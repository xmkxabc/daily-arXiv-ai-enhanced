{"id": "2506.14913", "title": "Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning", "authors": ["Wassim Bouaziz", "Mathurin Videau", "Nicolas Usunier", "El-Mahdi El-Mhamdi"], "summary": "The pre-training of large language models (LLMs) relies on massive text\ndatasets sourced from diverse and difficult-to-curate origins. Although\nmembership inference attacks and hidden canaries have been explored to trace\ndata usage, such methods rely on memorization of training data, which LM\nproviders try to limit. In this work, we demonstrate that indirect data\npoisoning (where the targeted behavior is absent from training data) is not\nonly feasible but also allow to effectively protect a dataset and trace its\nuse. Using gradient-based optimization prompt-tuning, we make a model learn\narbitrary secret sequences: secret responses to secret prompts that are absent\nfrom the training corpus. We validate our approach on language models\npre-trained from scratch and show that less than 0.005% of poisoned tokens are\nsufficient to covertly make a LM learn a secret and detect it with extremely\nhigh confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.\nCrucially, this occurs without performance degradation (on LM benchmarks) and\ndespite secrets never appearing in the training set.", "comment": "18 pages, 12 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14913v1", "AI": {"title_translation": "冬日战士：通过间接数据投毒在预训练阶段为语言模型植入后门", "tldr": "研究表明，通过间接数据投毒，可以在预训练阶段为大型语言模型植入秘密后门，以保护数据集并追踪其使用，且对模型性能无影响。", "motivation": "现有数据使用追踪方法（如成员推断攻击和隐藏金丝雀）依赖于模型对训练数据的记忆，而语言模型提供商正试图限制这种记忆。因此，需要一种不依赖于数据记忆的新方法来有效保护数据集并追踪其使用。", "method": "本文提出一种间接数据投毒方法。通过基于梯度的优化提示调优，使模型学习任意秘密序列（即对秘密提示的秘密响应），这些秘密序列在训练语料库中不存在。", "result": "实验证明，不到0.005%的投毒令牌就足以隐蔽地使语言模型学习秘密并以极高置信度（p < 10^-55）检测到它，且具有理论上可认证的方案。此过程不会导致语言模型在语言模型基准测试中性能下降，尽管秘密从未出现在训练集中。", "conclusion": "间接数据投毒是一种可行且有效的方法，可以在不影响模型性能的情况下，在预训练阶段为大型语言模型植入秘密后门，从而实现数据集的保护和使用追踪。", "translation": "大型语言模型（LLMs）的预训练依赖于海量文本数据集，这些数据来源多样且难以管理。尽管成员推断攻击和隐藏金丝雀等方法已被探索用于追踪数据使用，但这些方法依赖于训练数据的记忆，而语言模型提供商正试图限制这种记忆。在这项工作中，我们证明了间接数据投毒（目标行为在训练数据中不存在）不仅可行，而且能够有效地保护数据集并追踪其使用。通过基于梯度的优化提示调优，我们使模型学习任意秘密序列：即对秘密提示的秘密响应，这些响应在训练语料库中不存在。我们在从零开始预训练的语言模型上验证了我们的方法，并表明不到0.005%的投毒令牌足以隐蔽地使语言模型学习一个秘密，并通过理论上可认证的方案以极高置信度（p < 10^-55）检测到它。至关重要的是，这发生在不影响性能（在语言模型基准测试中）的情况下，尽管秘密从未出现在训练集中。", "summary": "本文提出一种新颖的间接数据投毒技术，用于在大型语言模型预训练阶段保护数据集和追踪其使用。与现有依赖模型记忆的方法不同，该方法通过基于梯度的提示调优，使模型学习训练数据中不存在的秘密响应。实验结果表明，极少量的投毒数据即可在不影响模型性能的前提下，隐蔽地植入可高置信度检测的秘密，有效解决了数据溯源和保护的难题。", "keywords": "间接数据投毒, 语言模型, 预训练, 数据保护, 后门", "comments": "这项工作具有显著的创新性，它提出了一种不依赖于模型记忆的间接数据投毒方法，以解决LLM数据溯源和保护的挑战。其关键在于能够在不影响模型性能的前提下，通过极少量的数据实现高置信度的秘密植入和检测。这为大型模型的数据治理和版权保护提供了新的思路，但同时也可能被恶意利用，为模型注入难以察觉的后门。"}}
{"id": "2506.14944", "title": "Fair Data Exchange with Constant-Time Proofs", "authors": ["Majid Khabbazian"], "summary": "The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic\npay-per-file transfers with constant-size proofs, but its prover and verifier\nruntimes still scale linearly with the file length n. We collapse these costs\nto essentially constant by viewing the file as a rate-1 Reed-Solomon (RS)\ncodeword, extending it to a lower-rate RS code with constant redundancy,\nencrypting this extended vector, and then proving correctness for only a small\nrandom subset of the resulting ciphertexts; RS decoding repairs any corrupted\nsymbols with negligible failure probability. Our protocol preserves full\nclient- and server-fairness, and adds only a tunable communication redundancy\noverhead.\n  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of\nFDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and\nfalling back to just two on-chain transactions when channels are unavailable.", "comment": "13 pages, 0 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14944v1", "AI": {"title_translation": "公平数据交换与常数时间证明", "tldr": "本文提出了一种新的公平数据交换（FDE）协议，通过将文件视为Reed-Solomon（RS）码字并结合zk-SNARK技术，将证明者和验证者的运行时间从文件长度的线性关系缩减到基本常数，并支持链下交易。", "motivation": "CCS 2024中引入的公平数据交换（FDE）协议的证明者和验证者运行时随文件长度n线性扩展，效率低下。", "method": "将文件视为速率为1的Reed-Solomon（RS）码字，并将其扩展为具有常数冗余的低速率RS码；加密此扩展向量；仅对结果密文的一个小随机子集证明正确性，并使用RS解码修复损坏符号。此外，使用紧凑的zk-SNARK修复FDE比特币实例化中的椭圆曲线不匹配，以实现链下交易。", "result": "证明者和验证者的运行时间缩减至基本常数；保留了完整的客户端和服务器公平性；仅增加了可调的通信冗余开销；使整个交换可以在链下运行，并在通道不可用时仅回退到两次链上交易。", "conclusion": "通过利用Reed-Solomon码和zk-SNARK技术，本论文显著提高了公平数据交换协议的效率，实现了常数时间复杂度，并支持链下操作，同时保持了公平性。", "translation": "在CCS 2024上引入的公平数据交换（FDE）协议提供了具有常数大小证明的原子按文件付费传输，但其证明者和验证者运行时仍随文件长度n线性扩展。我们通过将文件视为速率为1的Reed-Solomon（RS）码字，将其扩展为具有常数冗余的低速率RS码，加密此扩展向量，然后仅对结果密文的一个小随机子集证明正确性，将这些成本缩减至基本常数；RS解码以可忽略的失败概率修复任何损坏的符号。我们的协议保留了完整的客户端和服务器公平性，并且仅增加了可调的通信冗余开销。最后，我们使用紧凑的zk-SNARK修补了FDE的比特币实例化中的椭圆曲线不匹配问题，从而使整个交换可以在链下运行，并且在通道不可用时仅回退到两次链上交易。", "summary": "本文旨在解决CCS 2024公平数据交换（FDE）协议中证明者和验证者运行时随文件长度线性扩展的问题。作者提出了一种新颖的方法：将文件转换为Reed-Solomon码字，通过恒定冗余进行扩展并加密，然后仅对密文的一个小随机子集进行正确性证明，并利用RS解码进行错误修复。这种方法将证明者和验证者的运行时间缩减至基本常数。此外，该协议还集成了紧凑的zk-SNARK，以实现比特币FDE的链下交易，同时保持了完全的公平性并仅引入可调的通信冗余开销。", "keywords": "公平数据交换, 常数时间证明, Reed-Solomon码, zk-SNARK, 链下交易", "comments": "该论文的创新之处在于巧妙地将Reed-Solomon编码与选择性证明结合，实现了数据交换的常数时间复杂度，是对现有线性扩展协议的重大改进。同时，集成zk-SNARK以支持比特币链下交易，极大地提升了协议的实用性和效率，对于可扩展和公平的去中心化数据市场具有重要意义。"}}
{"id": "2506.14964", "title": "Narrowing the Gap between TEEs Threat Model and Deployment Strategies", "authors": ["Filip Rezabek", "Jonathan Passerat-Palmbach", "Moe Mahhouk", "Frieder Erdmann", "Andrew Miller"], "summary": "Confidential Virtual Machines (CVMs) provide isolation guarantees for data in\nuse, but their threat model does not include physical level protection and\nside-channel attacks. Therefore, current deployments rely on trusted cloud\nproviders to host the CVMs' underlying infrastructure. However, TEE\nattestations do not provide information about the operator hosting a CVM.\nWithout knowing whether a Trusted Execution Environment (TEE) runs within a\nprovider's infrastructure, a user cannot accurately assess the risks of\nphysical attacks. We observe a misalignment in the threat model where the\nworkloads are protected against other tenants but do not offer end-to-end\nsecurity assurances to external users without relying on cloud providers. The\nattestation should be extended to bind the CVM with the provider. A possible\nsolution can rely on the Protected Platform Identifier (PPID), a unique CPU\nidentifier. However, the implementation details of various TEE manufacturers,\nattestation flows, and providers vary. This makes verification of attestations,\nease of migration, and building applications without relying on a trusted party\nchallenging, highlighting a key limitation that must be addressed for the\nadoption of CVMs. We discuss two points focusing on hardening and extensions of\nTEEs' attestation.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14964v1", "AI": {"title_translation": "缩小TEE威胁模型与部署策略之间的差距", "tldr": "TEEs的威胁模型与实际部署存在差距，缺乏物理层保护和提供商绑定信息，本文讨论了解决此问题的认证扩展方案。", "motivation": "尽管机密虚拟机（CVMs）为使用中的数据提供隔离保证，但其威胁模型不包括物理层保护和侧信道攻击。当前部署依赖受信任的云提供商，但TEE证明不提供托管CVM的基础设施运营商信息，导致用户无法准确评估物理攻击风险，从而暴露出威胁模型与部署策略之间的错位。", "method": "论文观察到威胁模型与部署策略之间的错位，并提出将认证扩展以绑定CVM与提供商，可能依赖于受保护平台标识符（PPID）。同时，讨论了增强和扩展TEE认证的两个重点。", "result": "论文指出了机密虚拟机（CVMs）在威胁模型与部署策略之间存在的错位，即工作负载虽受其他租户保护，但缺乏对外部用户的端到端安全保证，除非依赖云提供商。此外，它还揭示了由于TEE制造商、认证流程和提供商实现细节的差异，导致认证验证、迁移便利性以及构建无需信任第三方的应用程序面临挑战，这成为CVMs普及的关键限制。", "conclusion": "为了促进机密虚拟机（CVMs）的广泛采用，必须解决当前TEE威胁模型与部署策略之间的差距，特别是通过扩展认证机制来绑定CVM与提供商，并克服不同实现细节带来的验证和迁移挑战。", "translation": "机密虚拟机（CVMs）为使用中的数据提供隔离保证，但其威胁模型不包括物理层保护和侧信道攻击。因此，当前的部署依赖受信任的云提供商来托管CVM的基础设施。然而，TEE认证不提供有关托管CVM的运营商信息。在不知道可信执行环境（TEE）是否在提供商基础设施内运行的情况下，用户无法准确评估物理攻击的风险。我们观察到威胁模型存在错位，即工作负载受到其他租户的保护，但如果没有依赖云提供商，则无法向外部用户提供端到端安全保证。认证应该扩展到将CVM与提供商绑定。一个可能的解决方案可以依赖于受保护平台标识符（PPID），一个独特的CPU标识符。然而，各种TEE制造商、认证流程和提供商的实现细节各不相同。这使得认证的验证、迁移的便利性以及在不依赖受信任方的情况下构建应用程序变得具有挑战性，突显了CVMs普及必须解决的一个关键限制。我们讨论了两个重点，侧重于TEE认证的强化和扩展。", "summary": "本文指出机密虚拟机（CVMs）的威胁模型与实际部署存在显著差距，其认证机制未能提供托管CVM的云提供商信息，导致用户无法全面评估物理攻击风险。为解决这一问题，论文提出应扩展认证以将CVM与提供商绑定，并探讨了使用受保护平台标识符（PPID）的可能性。文章强调了不同TEE实现细节带来的挑战，并讨论了强化和扩展TEE认证的重要性，以促进CVMs的广泛应用。", "keywords": "TEE, CVM, 威胁模型, 认证, 物理安全", "comments": "这篇论文识别了一个在可信执行环境（TEEs）和机密虚拟机（CVMs）实际部署中非常关键且常常被忽视的问题：威胁模型与实际信任边界的错位。它清晰地指出了当前认证机制的不足，即未能将CVM与实际的物理层提供商绑定。提出通过扩展认证和利用PPID来弥补这一差距，虽然面临实施复杂性，但对提升CVM的端到端安全性至关重要，为未来CVM的广泛采纳提供了重要的思考方向。"}}
{"id": "2506.15018", "title": "Private Continual Counting of Unbounded Streams", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "summary": "We study the problem of differentially private continual counting in the\nunbounded setting where the input size $n$ is not known in advance. Current\nstate-of-the-art algorithms based on optimal instantiations of the matrix\nmechanism cannot be directly applied here because their privacy guarantees only\nhold when key parameters are tuned to $n$. Using the common `doubling trick'\navoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve\nthis problem by introducing novel matrix factorizations based on logarithmic\nperturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works,\nwhich may be of independent interest. The resulting algorithm has smooth error,\nand for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum\nof the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It\nrequires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to\n$O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing\ntime for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA\n2023). Empirically, we find that our algorithm's performance is also comparable\nto theirs in absolute terms: our variance is less than $1.5\\times$ theirs for\n$t$ as large as $2^{24}$.", "comment": "12 pages, 2 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15018v1", "AI": {"title_translation": "无界流的私有持续计数", "tldr": "本文提出了一种针对无界数据流的差分隐私持续计数算法，解决了现有方法在输入规模未知情况下的局限性，并实现了平滑误差和竞争性性能。", "motivation": "当前基于矩阵机制的最新算法无法直接应用于无界数据流，因为它们的隐私保证依赖于预先已知的输入大小n。常用的“加倍技巧”虽然避免了对n的了解，但会导致次优和不平滑的误差。", "method": "通过引入基于先前工作中研究的函数 $\\frac{1}{\\sqrt{1-z}}$ 的对数扰动的新型矩阵分解来解决此问题。", "result": "所提出的算法具有平滑误差，并且对于任何 $\\alpha > 0$ 和 $t\\leq n$，它能够以 $O(\\log^{2+2\\alpha}(t))$ 的方差私有地估计前 $t$ 个数据点的总和。它需要 $O(t)$ 的空间和每轮平摊 $O(\\log t)$ 的时间。在经验上，该算法的性能也与Henzinger等人（SODA 2023）的算法在绝对值上相当：对于 $t$ 大到 $2^{24}$ 时，我们的方差小于他们的 $1.5$ 倍。", "conclusion": "本文通过引入新颖的矩阵分解，解决了无界设置中差分隐私持续计数的问题，从而得到了一种具有平滑误差且性能与最先进的有界输入算法相当的算法。", "translation": "我们研究了在输入大小 $n$ 未知的情况下，无界设置中差分隐私持续计数的问题。当前基于矩阵机制最优实例的最新算法不能直接应用于此，因为它们的隐私保证仅在关键参数针对 $n$ 调整时才成立。使用常见的“加倍技巧”可以避免对 $n$ 的了解，但会导致次优和不平滑的误差。我们通过引入基于先前工作中研究的函数 $\\frac{1}{\\sqrt{1-z}}$ 的对数扰动的新型矩阵分解来解决此问题，这可能具有独立的意义。由此产生的算法具有平滑误差，并且对于任何 $\\alpha > 0$ 和 $t\\leq n$，它能够以 $O(\\log^{2+2\\alpha}(t))$ 的方差私有地估计前 $t$ 个数据点的总和。它需要 $O(t)$ 的空间和每轮平摊 $O(\\log t)$ 的时间，而Henzinger等人（SODA 2023）的近似最优有界输入算法的方差为 $O(\\log(n)\\log(t))$，空间为 $O(n)$，预处理时间为 $O(n \\log n)$。在经验上，我们发现我们的算法性能在绝对值上也可与他们的算法相媲美：对于 $t$ 大到 $2^{24}$ 时，我们的方差小于他们的 $1.5$ 倍。", "summary": "本文解决了在输入规模未知的情况下，对无界数据流进行差分隐私持续计数的问题。针对现有基于矩阵机制的算法不适用于无界流的局限性以及“加倍技巧”导致的误差问题，作者提出了一种基于函数 $\\frac{1}{\\sqrt{1-z}}$ 对数扰动的新型矩阵分解算法。该算法实现了平滑误差，能够在 $O(\\log^{2+2\\alpha}(t))$ 的方差下私有地估计前 $t$ 个数据点的和，并具有 $O(t)$ 的空间复杂度和每轮平摊 $O(\\log t)$ 的时间复杂度。实验结果表明，其性能与最先进的有界输入算法相当甚至更优。", "keywords": "差分隐私, 持续计数, 无界流, 矩阵分解, 平滑误差", "comments": "该论文在差分隐私流处理领域取得了重要进展，解决了无界数据流这一实际挑战。其创新点在于引入了新颖的矩阵分解方法，这不仅提供了理论上的保证（平滑误差、改进的方差复杂度），而且在经验上表现出色。这项工作对于数据流本质上无界且隐私至关重要的实际应用具有重要意义。"}}
{"id": "2506.15066", "title": "ChatModel: Automating Reference Model Design and Verification with LLMs", "authors": ["Jianmin Ye", "Tianyang Liu", "Qi Tian", "Shengchu Su", "Zhe Jiang", "Xi Wang"], "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15066v1", "AI": {"title_translation": "ChatModel：利用大型语言模型自动化参考模型设计与验证", "tldr": "ChatModel是一个利用大型语言模型（LLMs）辅助的敏捷参考模型生成和验证平台，显著提高了复杂集成电路设计中参考模型的开发效率和质量。", "motivation": "随着集成电路设计复杂性增加，功能验证面临挑战，而参考模型的开发也变得复杂耗时。尽管LLMs在代码编程方面有潜力，但生成复杂的参考模型仍是难题。", "method": "ChatModel通过整合设计标准化和分层敏捷建模，实现了从设计规范到功能完善参考模型的流程简化。它采用构建块生成策略，增强了LLMs设计参考模型的能力，并提升了验证效率。", "result": "ChatModel在300个不同复杂度的设计上进行了评估，与替代方法相比，性能提升高达55.02%，生成稳定性显著增强，参考模型设计生产能力提高了9.18倍。与传统方法相比，参考模型设计和验证的迭代过程平均加速了5.90倍。", "conclusion": "ChatModel显著推动了参考模型生成和验证的自动化，展示了其巨大的潜力。", "translation": "随着集成电路设计复杂性持续升级，功能验证变得日益具有挑战性。参考模型对于加速验证过程至关重要，但其本身也变得越来越复杂且开发耗时。尽管大型语言模型（LLMs）在代码编程方面展现出潜力，但有效生成复杂参考模型仍然是一个重大障碍。为应对这些挑战，我们引入了ChatModel，这是首个由LLM辅助的敏捷参考模型生成和验证平台。ChatModel通过整合设计标准化和分层敏捷建模，简化了从设计规范到功能完善参考模型的过渡。它采用构建块生成策略，不仅增强了LLMs设计参考模型的能力，还显著提高了验证效率。我们对ChatModel在300个不同复杂度的设计上进行了评估，结果表明参考模型生成效率和质量均有显著提升。与替代方法相比，ChatModel实现了55.02%的峰值性能提升，生成稳定性显著增强，并且其生产参考模型设计的能力提高了9.18倍。此外，与传统方法相比，它将参考模型设计和验证的迭代过程平均加速了5.90倍。这些结果突出了ChatModel在显著推进参考模型生成和验证自动化方面的潜力。", "summary": "ChatModel是一个创新的LLM辅助平台，旨在自动化和加速复杂集成电路设计的参考模型生成与验证。它通过集成设计标准化和分层敏捷建模，并采用构建块生成策略，显著提升了LLMs生成参考模型的能力和验证效率。实验结果表明，ChatModel在效率、质量、生成稳定性及迭代速度方面均取得了显著提升，证明了其在自动化参考模型开发方面的巨大潜力。", "keywords": "参考模型, 大型语言模型, 自动化, 功能验证, 集成电路设计", "comments": "ChatModel的创新之处在于首次将大型语言模型应用于复杂集成电路的参考模型设计与验证，并通过构建块生成策略解决了LLMs在生成复杂参考模型方面的挑战。其重要性在于大幅提升了验证效率，降低了开发成本，对集成电路设计流程自动化具有里程碑意义。"}}
{"id": "2506.14866", "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "authors": ["Thomas Kuntz", "Agatha Duzan", "Hao Zhao", "Francesco Croce", "Zico Kolter", "Nicolas Flammarion", "Maksym Andriushchenko"], "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.14866v1", "AI": {"title_translation": "OS-Harm：一个衡量计算机使用代理安全性的基准", "tldr": "OS-Harm是一个新的基准，用于衡量基于LLM的计算机使用代理的安全性，涵盖了故意误用、提示注入攻击和模型不当行为等三类危害，并提出了一个自动评估器来评估准确性和安全性。研究发现，现有模型容易受到故意误用和提示注入攻击。", "motivation": "尽管基于大型语言模型的计算机使用代理日益普及，但其安全性却在很大程度上被忽视。评估和理解这些系统潜在的有害行为对于它们的广泛采用至关重要，因此需要一个专门的基准来解决这一安全评估空白。", "method": "引入了OS-Harm基准，该基准构建在OSWorld环境之上，旨在测试模型在三类危害（故意用户误用、提示注入攻击和模型不当行为）下的表现。为此，创建了150个涵盖多种安全违规类型（骚扰、版权侵犯、虚假信息、数据泄露等）的任务，并要求代理与各种操作系统应用程序（电子邮件客户端、代码编辑器、浏览器等）交互。此外，提出了一种自动评估器来评估代理的准确性和安全性，该评估器与人工标注达到了高一致性（F1分数分别为0.76和0.79）。研究评估了一系列前沿模型，如o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Pro。", "result": "所有被评估的模型都倾向于直接遵守许多故意的误用查询，相对容易受到静态提示注入的影响，并且偶尔会执行不安全的操作。", "conclusion": "OS-Harm基准揭示了当前计算机使用代理在安全性方面的显著不足，特别是在应对故意误用和提示注入攻击方面。这表明在确保这些代理广泛采用之前，需要进一步关注和改进其安全机制。", "translation": "计算机使用代理是基于大型语言模型的代理，它们可以通过处理屏幕截图或可访问性树来直接与图形用户界面交互。尽管这些系统越来越受欢迎，但它们的安全性却在很大程度上被忽视了，尽管评估和理解其潜在的有害行为对于广泛采用至关重要。为了弥补这一空白，我们引入了OS-Harm，这是一个衡量计算机使用代理安全性的新基准。OS-Harm建立在OSWorld环境之上，旨在测试模型在三类危害中的表现：故意用户误用、提示注入攻击和模型不当行为。为了涵盖这些情况，我们创建了150个任务，这些任务涵盖了几种类型的安全违规（骚扰、版权侵犯、虚假信息、数据泄露等），并要求代理与各种操作系统应用程序（电子邮件客户端、代码编辑器、浏览器等）交互。此外，我们提出了一种自动评估器，用于评估代理的准确性和安全性，该评估器与人工标注达到了高一致性（F1分数分别为0.76和0.79）。我们基于一系列前沿模型（如o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Pro）评估了计算机使用代理，并提供了对其安全性的见解。特别是，所有模型都倾向于直接遵守许多故意的误用查询，相对容易受到静态提示注入的影响，并且偶尔会执行不安全的操作。OS-Harm基准可在https://github.com/tml-epfl/os-harm获取。", "summary": "本研究引入了OS-Harm，一个用于评估基于大型语言模型的计算机使用代理安全性的新基准。该基准构建于OSWorld环境之上，包含150个任务，旨在测试代理在故意用户误用、提示注入攻击和模型不当行为三类危害下的表现，涵盖了多种安全违规类型和操作系统应用交互。研究还提出了一种高精度自动评估器。对前沿模型的评估结果显示，当前模型普遍容易遵守恶意指令、易受提示注入攻击，并偶尔执行不安全操作，凸显了计算机使用代理在安全方面的不足。", "keywords": "计算机使用代理, 安全性, 基准, OS-Harm, 大型语言模型, 提示注入", "comments": "OS-Harm基准的创新之处在于其专门针对计算机使用代理的安全问题，填补了现有评估的空白。它通过构建在OSWorld环境之上，并设计了涵盖多种危害类型和操作系统应用交互的150个任务，提供了全面且贴近实际的评估场景。自动评估器的提出提高了评估效率和可复现性。这项工作的重要性在于揭示了当前前沿LLM代理在安全性方面的脆弱性，为未来的研究和开发指明了方向，对于推动这些代理的负责任部署至关重要。"}}
{"id": "2506.14771", "title": "Empirical Studies of Large Scale Environment Scanning by Consumer Electronics", "authors": ["Mengyuan Wang", "Yang Liu", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "summary": "This paper presents an empirical evaluation of the Matterport Pro3, a\nconsumer-grade 3D scanning device, for large-scale environment reconstruction.\nWe conduct detailed scanning (1,099 scanning points) of a six-floor building\n(17,567 square meters) and assess the device's effectiveness, limitations, and\nperformance enhancements in diverse scenarios. Challenges encountered during\nthe scanning are addressed through proposed solutions, while we also explore\nadvanced methods to overcome them more effectively. Comparative analysis with\nanother consumer-grade device (iPhone) highlights the Pro3's balance between\ncost-effectiveness and performance. The Matterport Pro3 achieves a denser point\ncloud with 1,877,324 points compared to the iPhone's 506,961 points and higher\nalignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)\naverage distance error between the two point cloud models is 0.0408 meters,\nwith a standard deviation of 0.0715 meters. The study demonstrates the Pro3's\nability to generate high-quality 3D models suitable for large-scale\napplications, leveraging features such as LiDAR and advanced alignment\ntechniques.", "comment": "Accepted by IEEE Consumer Electronics Magazine", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14771v1", "AI": {"title_translation": "消费电子产品在大型环境扫描中的实证研究", "tldr": "本文对消费级3D扫描设备Matterport Pro3在大型环境重建中的性能进行了实证评估，发现其在成本效益和性能之间取得了良好平衡，能够生成适用于大型应用的高质量3D模型，优于iPhone。", "motivation": "本研究旨在对消费级3D扫描设备Matterport Pro3进行实证评估，以了解其在大规模环境重建中的有效性、局限性和性能提升，并与其他消费级设备进行比较。", "method": "研究对一栋六层楼（17,567平方米）进行了详细扫描（1,099个扫描点），评估了Matterport Pro3在不同场景下的表现。针对扫描中遇到的挑战提出了解决方案并探索了更先进的方法。同时，与另一款消费级设备iPhone进行了对比分析。", "result": "Matterport Pro3获得了更密集的点云（1,877,324个点，而iPhone为506,961个点）和更高的对齐精度（RMSE为0.0118米）。两个点云模型之间的云到云（C2C）平均距离误差为0.0408米，标准差为0.0715米。研究表明Pro3在成本效益和性能之间取得了平衡，适用于大型应用。", "conclusion": "Matterport Pro3能够利用LiDAR和先进的对齐技术生成适用于大型应用的高质量3D模型，在成本效益和性能方面表现出色。", "translation": "本文对消费级3D扫描设备Matterport Pro3在大型环境重建中的应用进行了实证评估。我们对一栋六层楼（17,567平方米）进行了详细扫描（1,099个扫描点），并评估了该设备在不同场景下的有效性、局限性和性能提升。论文提出了解决扫描过程中遇到的挑战的方案，并探索了更有效地克服这些挑战的先进方法。与另一款消费级设备（iPhone）的比较分析突出了Pro3在成本效益和性能之间的平衡。Matterport Pro3获得了更密集的点云（1,877,324个点，而iPhone为506,961个点）和更高的对齐精度（RMSE为0.0118米）。两个点云模型之间的云到云（C2C）平均距离误差为0.0408米，标准差为0.0715米。该研究表明，Pro3能够生成适用于大型应用的高质量3D模型，这得益于其LiDAR等功能和先进的对齐技术。", "summary": "本文对消费级3D扫描设备Matterport Pro3在大型环境重建中的性能进行了实证评估。通过对一栋六层楼的详细扫描并与iPhone进行对比，研究发现Pro3在点云密度和对齐精度方面表现更优，且在成本效益和性能之间取得了良好平衡，证明其适合生成高质量的大型3D模型。", "keywords": "3D扫描, Matterport Pro3, 大规模环境, 消费电子, 实证研究", "comments": "该论文对消费级3D扫描设备在大型环境中的实际应用进行了深入的实证研究，为建筑、工程和施工（AEC）等行业提供了有价值的参考。通过与iPhone的对比，结果更具说服力，突出了Matterport Pro3在性能和成本效益方面的优势。其创新之处在于详细的大规模实证数据和对实际挑战的解决，而非仅理论探讨，具有很强的实用价值。"}}
{"id": "2506.14775", "title": "See What I Mean? CUE: A Cognitive Model of Understanding Explanations", "authors": ["Tobias Labarta", "Nhi Hoang", "Katharina Weitz", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "summary": "As machine learning systems increasingly inform critical decisions, the need\nfor human-understandable explanations grows. Current evaluations of Explainable\nAI (XAI) often prioritize technical fidelity over cognitive accessibility which\ncritically affects users, in particular those with visual impairments. We\npropose CUE, a model for Cognitive Understanding of Explanations, linking\nexplanation properties to cognitive sub-processes: legibility (perception),\nreadability (comprehension), and interpretability (interpretation). In a study\n(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we\nfound comparable task performance but lower confidence/effort for visually\nimpaired users. Unlike expected, these gaps were not mitigated and sometimes\nworsened by accessibility-focused color maps like Cividis. These results\nchallenge assumptions about perceptual optimization and support the need for\nadaptive XAI interfaces. They also validate CUE by demonstrating that altering\nexplanation legibility affects understandability. We contribute: (1) a\nformalized cognitive model for explanation understanding, (2) an integrated\ndefinition of human-centered explanation properties, and (3) empirical evidence\nmotivating accessible, user-tailored XAI.", "comment": "10 pages, 5 figures (main text), 4 tables, 455-participant user study", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14775v1", "AI": {"title_translation": "你懂我的意思吗？CUE：一个理解解释的认知模型", "tldr": "本文提出了CUE模型，一个关于理解机器学习解释的认知模型，并指出当前XAI评估忽视认知可及性。研究通过实验发现，为视障用户设计的无障碍调色板效果不佳，反而可能加剧问题，强调了开发自适应XAI界面的重要性。", "motivation": "随着机器学习系统日益影响关键决策，对人类可理解解释的需求不断增长。然而，当前对可解释人工智能（XAI）的评估往往优先考虑技术保真度而非认知可及性，这严重影响了用户，特别是视障用户。", "method": "本文提出了CUE（解释认知理解）模型，该模型将解释属性（易读性、可读性、可解释性）与认知子过程（感知、理解、解读）关联起来。为验证CUE模型，研究进行了一项包含455名参与者的用户研究，测试了不同颜色映射（BWR、Cividis、Coolwarm）的热图，以评估其对用户（包括视障用户）认知理解的影响。", "result": "实验结果显示，在任务表现方面，视障用户与非视障用户相当，但视障用户的信心和努力程度较低。出乎意料的是，像Cividis这样注重可访问性的颜色映射并未能缓解这些差距，有时甚至使其恶化。", "conclusion": "研究结果挑战了关于感知优化的传统假设，并强调了开发自适应XAI界面的必要性。同时，通过证明改变解释的易读性会影响可理解性，研究也验证了CUE模型的有效性。", "translation": "随着机器学习系统日益影响关键决策，对人类可理解解释的需求不断增长。当前对可解释人工智能（XAI）的评估往往优先考虑技术保真度而非认知可及性，这严重影响了用户，特别是视障用户。我们提出了CUE，一个解释认知理解模型，将解释属性与认知子过程联系起来：易读性（感知）、可读性（理解）和可解释性（解读）。在一项测试不同颜色映射（BWR、Cividis、Coolwarm）热图的研究（N=455）中，我们发现任务表现相当，但视障用户的信心/努力程度较低。与预期不同的是，这些差距并未通过诸如Cividis等注重可访问性的颜色映射得到缓解，有时甚至恶化。这些结果挑战了关于感知优化的假设，并支持需要自适应的XAI界面。它们还通过证明改变解释的易读性会影响可理解性，从而验证了CUE模型。我们的贡献包括：（1）一个解释理解的形式化认知模型，（2）一个以人为中心的解释属性的综合定义，以及（3）促使开发可访问的、用户定制化XAI的实证证据。", "summary": "本文提出了CUE模型，这是一个关于理解机器学习解释的认知模型，旨在解决当前可解释人工智能（XAI）评估中认知可及性不足的问题，尤其关注对视障用户的影响。通过一项涉及455名参与者的实验，研究发现尽管任务表现相似，但视障用户在使用热图解释时信心和努力程度较低，且注重可访问性的颜色映射未能改善甚至恶化了这种状况。研究结果挑战了感知优化的传统观念，强调了开发自适应XAI界面的必要性，并验证了CUE模型中解释易读性对理解能力的影响。本文的贡献在于提出了形式化的认知模型、综合的人本解释属性定义以及支持可访问、用户定制化XAI的实证证据。", "keywords": "可解释人工智能, 认知模型, 解释理解, 视障用户, 可及性", "comments": "这篇论文的创新点在于提出了CUE这一形式化的认知模型，将解释的属性与认知过程相结合，为理解和设计可解释AI提供了新的理论框架。其重要性在于它关注了当前XAI研究中常被忽视的认知可及性问题，特别是对视障用户的关注，并通过实证研究揭示了传统无障碍设计可能存在的局限性。研究结果对未来XAI界面的设计，尤其是需要考虑用户个体差异和认知需求的设计，具有重要的指导意义。"}}
{"id": "2506.14826", "title": "Collaborative Interest-aware Graph Learning for Group Identification", "authors": ["Rui Zhao", "Beihong Jin", "Beibei Li", "Yiyuan Zheng"], "summary": "With the popularity of social media, an increasing number of users are\njoining group activities on online social platforms. This elicits the\nrequirement of group identification (GI), which is to recommend groups to\nusers. We reveal that users are influenced by both group-level and item-level\ninterests, and these dual-level interests have a collaborative evolution\nrelationship: joining a group expands the user's item interests, further\nprompting the user to join new groups. Ultimately, the two interests tend to\nalign dynamically. However, existing GI methods fail to fully model this\ncollaborative evolution relationship, ignoring the enhancement of group-level\ninterests on item-level interests, and suffering from false-negative samples\nwhen aligning cross-level interests. In order to fully model the collaborative\nevolution relationship between dual-level user interests, we propose CI4GI, a\nCollaborative Interest-aware model for Group Identification. Specifically, we\ndesign an interest enhancement strategy that identifies additional interests of\nusers from the items interacted with by the groups they have joined as a\nsupplement to item-level interests. In addition, we adopt the distance between\ninterest distributions of two users to optimize the identification of negative\nsamples for a user, mitigating the interference of false-negative samples\nduring cross-level interests alignment. The results of experiments on three\nreal-world datasets demonstrate that CI4GI significantly outperforms\nstate-of-the-art models.", "comment": "accepted by ECML PKDD 2025", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.14826v1", "AI": {"title_translation": "协作兴趣感知图学习用于群组识别", "tldr": "提出CI4GI模型，通过建模用户群组和物品双层兴趣的协作演化关系，并优化负样本识别，显著提升群组识别效果。", "motivation": "现有群组识别方法未能充分建模用户群组和物品双层兴趣的协作演化关系，忽略了群组兴趣对物品兴趣的增强作用，且在跨层兴趣对齐时存在假负样本问题。", "method": "提出CI4GI（Collaborative Interest-aware model for Group Identification）模型。该模型设计了兴趣增强策略，从用户已加入群组互动过的物品中识别额外兴趣以补充物品级兴趣；并采用用户间兴趣分布距离优化负样本识别，以缓解假负样本干扰。", "result": "在三个真实世界数据集上的实验结果表明，CI4GI显著优于现有最先进的模型。", "conclusion": "CI4GI模型通过全面建模用户双层兴趣的协作演化关系并优化负样本识别，有效解决了现有群组识别方法的不足，显著提升了群组识别的性能。", "translation": "随着社交媒体的普及，越来越多的用户加入在线社交平台上的群组活动。这引发了群组识别（GI）的需求，即向用户推荐群组。我们发现用户受到群组级和物品级兴趣的双重影响，并且这些双层兴趣具有协作演化关系：加入一个群组会扩展用户的物品兴趣，进一步促使用户加入新的群组。最终，这两种兴趣倾向于动态对齐。然而，现有群组识别方法未能充分建模这种协作演化关系，忽视了群组级兴趣对物品级兴趣的增强作用，并且在对齐跨层兴趣时存在假负样本问题。为了充分建模用户双层兴趣之间的协作演化关系，我们提出了CI4GI，一个协作兴趣感知群组识别模型。具体来说，我们设计了一种兴趣增强策略，从用户已加入群组互动过的物品中识别出用户的额外兴趣，作为物品级兴趣的补充。此外，我们采用两个用户兴趣分布之间的距离来优化用户负样本的识别，从而减轻跨层兴趣对齐过程中假负样本的干扰。在三个真实世界数据集上的实验结果表明，CI4GI显著优于现有最先进的模型。", "summary": "本文针对群组识别（GI）任务中现有方法未能充分建模用户群组和物品双层兴趣协作演化关系的问题，提出了CI4GI模型。该模型通过设计兴趣增强策略来补充物品级兴趣，并优化负样本识别以缓解假负样本干扰。实验结果表明，CI4GI在多个真实数据集上显著优于现有最先进的方法。", "keywords": "群组识别, 协作演化, 双层兴趣, 假负样本, 兴趣增强", "comments": "本文的创新点在于揭示并建模了用户群组级和物品级兴趣的协作演化关系，并针对性地提出了兴趣增强策略和负样本优化方法，有效解决了现有群组识别方法在处理复杂用户兴趣交互和负样本识别方面的不足，具有重要的理论和实践意义。"}}
{"id": "2506.14987", "title": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a\nCNN-based dynamic priority prediction mechanism for improved interference\ncoordination in multi-cell, multi-channel networks. Unlike LDP's static\npriorities, our approach uses a Convolutional Neural Network and graph coloring\nto adaptively assign link priorities based on real-time traffic, transmission\nopportunities, and network conditions. Assuming that first training phase is\nperformed offline, our approach introduced minimal overhead, while enabling\nmore efficient resource allocation, boosting network capacity, SINR, and\nschedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and\n49\\% over LDP across three network configurations, highlighting its\neffectiveness for complex URLLC scenarios.", "comment": "This paper has been submitted to IEEEGLOBE2025 on April 15, 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.14987v1", "AI": {"title_translation": "启用CNN的工业URLLC概率实时保障调度", "tldr": "该研究提出一种基于CNN的动态优先级预测机制，增强了LDP算法，以改善工业URLLC中多小区、多信道网络的干扰协调，显著提升了网络性能。", "motivation": "在大型工业无线网络中，确保超可靠、低延迟通信（URLLC）的包级通信质量至关重要。", "method": "该研究通过引入基于卷积神经网络（CNN）的动态优先级预测机制来增强局部截止日期划分（LDP）算法，以改善多小区、多信道网络中的干扰协调。与LDP的静态优先级不同，该方法利用CNN和图着色根据实时流量、传输机会和网络条件自适应地分配链路优先级。其训练阶段离线进行，引入了最小的开销。", "result": "仿真结果表明，该方法在三种网络配置下，相对于LDP算法，信噪比（SINR）增益分别高达113%、94%和49%。这表明它能够实现更高效的资源分配，提升网络容量、SINR和可调度性。", "conclusion": "该基于CNN的调度方法在复杂URLLC场景中表现出显著的有效性，通过动态优先级预测和高效资源分配，大幅提升了网络性能和可靠性。", "translation": "确保包级通信质量对于大规模工业无线网络中的超可靠、低延迟通信（URLLC）至关重要。我们通过引入基于CNN的动态优先级预测机制来增强局部截止日期划分（LDP）算法，以改善多小区、多信道网络中的干扰协调。与LDP的静态优先级不同，我们的方法使用卷积神经网络和图着色，根据实时流量、传输机会和网络条件自适应地分配链路优先级。假设第一阶段的训练是离线进行的，我们的方法引入了最小的开销，同时实现了更高效的资源分配，提升了网络容量、SINR和可调度性。仿真结果显示，在三种网络配置下，相对于LDP，SINR增益分别高达113%、94%和49%，突显了其在复杂URLLC场景中的有效性。", "summary": "本论文提出了一种基于卷积神经网络（CNN）的动态优先级预测机制，以增强现有的局部截止日期划分（LDP）算法，旨在改善大型工业无线网络中多小区、多信道URLLC的干扰协调。与LDP的静态优先级不同，该方法利用CNN和图着色根据实时网络条件自适应地分配链路优先级。该方法在离线训练后引入了最小的开销，并通过仿真结果证明，在提高网络容量、信噪比（SINR）和可调度性方面，相对于LDP取得了显著增益，尤其在复杂URLLC场景中表现出卓越的有效性。", "keywords": "URLLC, CNN, 调度, 干扰协调, 工业无线网络", "comments": "这项研究的创新之处在于将CNN应用于无线通信调度，实现了动态的优先级预测，从而克服了传统LDP算法静态优先级的局限性。其通过机器学习优化资源分配的能力，对于提升工业URLLC的可靠性和效率具有重要意义。高SINR增益的仿真结果令人印象深刻，表明了该方法在实际部署中的巨大潜力。"}}
{"id": "2506.14855", "title": "Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers", "authors": ["Tommaso Belvedere", "Michael Ziegltrum", "Giulio Turrisi", "Valerio Modugno"], "summary": "Model Predictive Path Integral control is a powerful sampling-based approach\nsuitable for complex robotic tasks due to its flexibility in handling nonlinear\ndynamics and non-convex costs. However, its applicability in real-time,\nhighfrequency robotic control scenarios is limited by computational demands.\nThis paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments\nstandard MPPI by computing local linear feedback gains derived from sensitivity\nanalysis inspired by Riccati-based feedback used in gradient-based MPC. These\ngains allow for rapid closed-loop corrections around the current state without\nrequiring full re-optimization at each timestep. We demonstrate the\neffectiveness of F-MPPI through simulations and real-world experiments on two\nrobotic platforms: a quadrupedal robot performing dynamic locomotion on uneven\nterrain and a quadrotor executing aggressive maneuvers with onboard\ncomputation. Results illustrate that incorporating local feedback significantly\nimproves control performance and stability, enabling robust, high-frequency\noperation suitable for complex robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14855v1", "AI": {"title_translation": "反馈-MPPI：通过展开微分实现快速基于采样的MPC——告别低级控制器", "tldr": "Feedback-MPPI (F-MPPI) 是一种新颖的控制框架，它通过计算局部线性反馈增益来增强标准MPPI，显著提高了计算效率和控制性能，使其适用于实时、高频的复杂机器人控制任务。", "motivation": "模型预测路径积分控制 (MPPI) 虽然强大，但由于计算需求高，限制了其在实时、高频机器人控制场景中的应用。", "method": "本文引入了反馈-MPPI (F-MPPI)，通过从敏感性分析中导出局部线性反馈增益来增强标准MPPI，这些增益灵感来源于基于Riccati的梯度MPC反馈。这些增益允许围绕当前状态进行快速闭环校正，而无需在每个时间步进行完全的重新优化。", "result": "F-MPPI 在四足机器人动态运动和四旋翼飞行器激进机动等模拟和真实世界实验中表现出有效性。结果表明，结合局部反馈显著提高了控制性能和稳定性，实现了鲁棒、高频操作。", "conclusion": "Feedback-MPPI 通过引入局部线性反馈增益，显著提升了模型预测路径积分控制的计算效率和控制性能，使其能够应用于实时、高频的复杂机器人系统。", "translation": "模型预测路径积分控制（MPPI）是一种强大的基于采样的方法，由于其在处理非线性动力学和非凸成本方面的灵活性，适用于复杂的机器人任务。然而，其在实时、高频机器人控制场景中的适用性受到计算需求的限制。本文介绍了反馈-MPPI（F-MPPI），这是一种新颖的框架，它通过计算源自敏感性分析的局部线性反馈增益来增强标准MPPI，这些增益的灵感来源于基于Riccati的梯度MPC中使用的反馈。这些增益允许围绕当前状态进行快速闭环校正，而无需在每个时间步进行完全的重新优化。我们通过在两个机器人平台上的模拟和真实世界实验证明了F-MPPI的有效性：一个在不平坦地形上进行动态运动的四足机器人和一个执行激进机动并进行板载计算的四旋翼飞行器。结果表明，结合局部反馈显著提高了控制性能和稳定性，实现了鲁棒、高频操作，适用于复杂的机器人系统。", "summary": "本文提出了Feedback-MPPI (F-MPPI)，一种改进型模型预测路径积分控制（MPPI）框架，旨在解决MPPI在实时高频机器人控制中面临的计算挑战。F-MPPI通过引入基于敏感性分析的局部线性反馈增益来增强MPPI，从而实现快速闭环校正，避免了每个时间步的完全重新优化。通过在四足机器人和四旋翼飞行器上的仿真和实际实验，证明了F-MPPI显著提升了控制性能和稳定性，使其能够为复杂机器人系统提供鲁棒、高频的控制。", "keywords": "MPPI, 反馈控制, 采样控制, 机器人控制, 实时控制", "comments": "这项工作通过将局部反馈机制融入到采样基线控制方法MPPI中，有效地解决了其计算效率低的问题，从而使其能够应用于对实时性要求极高的机器人系统。其创新点在于将梯度基线MPC中的反馈思想与采样基线MPC相结合，实现了性能与效率的平衡。这对于复杂非线性系统的实时控制具有重要意义。"}}
{"id": "2506.15045", "title": "An Integrated Sensing and Communication System for Time-Sensitive Targets with Random Arrivals", "authors": ["Homa Nikbakht", "Yonina C. Eldar", "H. Vincent Poor"], "summary": "In 6G networks, integrated sensing and communication (ISAC) is envisioned as\na key technology that enables wireless systems to perform joint sensing and\ncommunication using shared hardware, antennas and spectrum. ISAC designs\nfacilitate emerging applications such as smart cities and autonomous driving.\nSuch applications also demand ultra-reliable and low-latency communication\n(URLLC). Thus, an ISAC-enabled URLLC system can prioritize time-sensitive\ntargets and ensure information delivery under strict latency and reliability\nconstraints. We propose a bi-static MIMO ISAC system to detect the arrival of\nURLLC messages and prioritize their delivery. In this system, a base station\n(BS) communicates with a user equipment (UE) and a sensing receiver (SR) is\ndeployed to collect echo signals reflected from a target of interest. The BS\nregularly transmits messages of enhanced mobile broadband (eMBB) services to\nthe UE. During each eMBB transmission, if the SR senses the presence of a\ntarget of interest, it immediately triggers the transmission of an additional\nURLLC message. To reinforce URLLC transmissions, we propose a dirty-paper\ncoding (DPC)-based technique that mitigates the interference of both eMBB and\nsensing signals. To decode the eMBB message, we consider two approaches for\nhandling the URLLC interference: treating interference as noise and successive\ninterference cancellation. For this system, we formulate the\nrate-reliability-detection trade-off in the finite blocklength (FBL) regime by\nevaluating the communication rate of the eMBB transmissions, the reliability of\nthe URLLC transmissions and the probability of the target detection. Our\nnumerical analysis show that our proposed DPC-based ISAC scheme significantly\noutperforms power-sharing and traditional time-sharing schemes. In particular,\nit achieves higher eMBB transmission rate while satisfying both URLLC and\nsensing constraints.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15045v1", "AI": {"title_translation": "用于随机到达时间敏感目标的集成感知与通信系统", "tldr": "本文提出了一种基于脏纸编码（DPC）的双基地MIMO集成感知与通信（ISAC）系统，用于6G网络中优先处理时间敏感目标（URLLC消息），并在满足URLLC和感知约束的同时，显著提高了eMBB传输速率。", "motivation": "在6G网络中，集成感知与通信（ISAC）被视为一项关键技术，能够利用共享硬件、天线和频谱进行联合感知和通信。新兴应用如智慧城市和自动驾驶要求超可靠低延迟通信（URLLC）。因此，需要一个支持ISAC的URLLC系统来优先处理时间敏感目标，并在严格的延迟和可靠性约束下确保信息传输。", "method": "本文提出了一种双基地MIMO ISAC系统，用于检测URLLC消息的到达并优先处理其传输。在该系统中，基站（BS）与用户设备（UE）通信，并部署感知接收器（SR）以收集从目标反射的回波信号。BS定期传输增强型移动宽带（eMBB）服务消息给UE。在每次eMBB传输期间，如果SR感知到目标存在，它会立即触发额外URLLC消息的传输。为增强URLLC传输，提出了一种基于脏纸编码（DPC）的技术，以减轻eMBB和感知信号的干扰。对于eMBB消息的解码，考虑了两种URLLC干扰处理方法：将干扰视为噪声和连续干扰消除。系统在有限块长（FBL）状态下，通过评估eMBB传输的通信速率、URLLC传输的可靠性以及目标检测概率，建立了速率-可靠性-检测的权衡模型。", "result": "数值分析表明，所提出的基于DPC的ISAC方案显著优于功率共享和传统时分共享方案。具体而言，它在满足URLLC和感知约束的同时，实现了更高的eMBB传输速率。", "conclusion": "本文提出的基于DPC的ISAC方案在集成感知与通信系统中，通过有效管理干扰，能够显著提升eMBB传输速率，同时满足URLLC和感知性能要求，为6G网络中处理时间敏感目标提供了有效解决方案。", "translation": "在6G网络中，集成感知与通信（ISAC）被设想为一项关键技术，使无线系统能够使用共享硬件、天线和频谱执行联合感知和通信。ISAC设计促进了智慧城市和自动驾驶等新兴应用。此类应用也需要超可靠和低延迟通信（URLLC）。因此，一个支持ISAC的URLLC系统可以优先处理时间敏感目标，并在严格的延迟和可靠性约束下确保信息传输。我们提出了一种双基地MIMO ISAC系统，用于检测URLLC消息的到达并优先处理其传输。在该系统中，基站（BS）与用户设备（UE）通信，并部署感知接收器（SR）以收集从目标反射的回波信号。BS定期传输增强型移动宽带（eMBB）服务消息给UE。在每次eMBB传输期间，如果SR感知到目标存在，它会立即触发额外URLLC消息的传输。为增强URLLC传输，我们提出了一种基于脏纸编码（DPC）的技术，以减轻eMBB和感知信号的干扰。为解码eMBB消息，我们考虑了两种处理URLLC干扰的方法：将干扰视为噪声和连续干扰消除。对于该系统，我们通过评估eMBB传输的通信速率、URLLC传输的可靠性以及目标检测概率，在有限块长（FBL）状态下建立了速率-可靠性-检测的权衡模型。我们的数值分析表明，所提出的基于DPC的ISAC方案显著优于功率共享和传统时分共享方案。具体而言，它在满足URLLC和感知约束的同时，实现了更高的eMBB传输速率。", "summary": "本研究提出了一种用于6G网络的双基地MIMO集成感知与通信（ISAC）系统，旨在处理随机到达的时间敏感目标（URLLC消息）。该系统通过基站、用户设备和感知接收器的协作，在常规eMBB传输过程中，一旦感知到目标存在即触发URLLC消息传输。为有效管理干扰，文中引入了一种基于脏纸编码（DPC）的技术。研究还分析了eMBB和URLLC干扰处理方案，并在有限块长下建模了速率-可靠性-检测的权衡。数值结果表明，所提出的DPC-based ISAC方案在满足URLLC和感知约束的同时，显著优于现有方案，实现了更高的eMBB传输速率。", "keywords": "集成感知与通信, 超可靠低延迟通信, 脏纸编码, 有限块长, 6G网络", "comments": "本文的创新点在于提出了一个结合脏纸编码（DPC）的双基地MIMO ISAC系统，以应对6G网络中时间敏感目标带来的URLLC和感知需求。通过DPC有效减轻了eMBB和感知信号的干扰，这对于实现高eMBB速率同时满足严格的URLLC和感知约束至关重要。研究在有限块长下对速率-可靠性-检测的权衡进行建模，也体现了其理论深度。该方案在性能上显著优于传统方法，为未来ISAC系统设计提供了有价值的指导。"}}
{"id": "2506.15006", "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language Models", "authors": ["Jesmin Jahan Tithi", "Hanjiang Wu", "Avishaii Abuhatzera", "Fabrizio Petrini"], "summary": "The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.", "comment": "14 pages, submitted to SC25 for review", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15006v1", "AI": {"title_translation": "扩展智能：为下一代语言模型设计数据中心", "tldr": "本研究提出并评估了一种全面的协同设计框架和FullFlat网络架构，旨在为万亿参数级大型语言模型构建可扩展、高效且经济的数据中心，并提供了实用的设计路线图。", "motivation": "大型语言模型（LLMs）的爆炸式增长，如拥有1.8万亿参数的GPT-4，对数据中心架构提出了严峻挑战，需要彻底重新思考以确保可扩展性、效率和成本效益。", "method": "本研究提供了一个全面的协同设计框架，联合探索了FLOPS、HBM带宽和容量、多种网络拓扑（两层 vs. FullFlat 光学）、横向扩展域的大小以及LLM中常用的并行/优化策略。研究引入并评估了FullFlat网络架构，并通过详细的敏感性分析量化了计算与通信重叠、利用硬件加速集合通信、更宽的横向扩展域和更大内存容量的益处。此外，研究扩展并验证了一个性能建模工具，该工具能够预测LLM运行时，精度在真实测量值的10%以内。", "result": "研究引入并评估了FullFlat网络架构，证明了其对性能和可扩展性的变革性影响。通过敏感性分析，量化了计算与通信重叠、利用硬件加速集合通信、更宽的横向扩展域和更大内存容量的益处。研究涵盖了稀疏（专家混合）和密集型Transformer-based LLMs，揭示了系统设计选择如何影响模型FLOPS利用率（MFU）和整体吞吐量。开发的性能建模工具预测LLM运行时与真实测量值误差在10%以内。", "conclusion": "本研究的发现为设计能够高效支持万亿参数模型、降低优化复杂性并维持AI能力快速发展的AI数据中心提供了可操作的见解和实用的路线图。", "translation": "大型语言模型（LLMs）的爆炸式增长——例如拥有1.8万亿参数的GPT-4——要求彻底重新思考数据中心架构，以确保可扩展性、效率和成本效益。我们的工作提供了一个全面的协同设计框架，联合探索了FLOPS、HBM带宽和容量、多种网络拓扑（两层 vs. FullFlat 光学）、横向扩展域的大小以及LLMs中流行的并行/优化策略。我们引入并评估了FullFlat网络架构，该架构在所有节点之间提供统一的高带宽、低延迟连接，并展示了它们对性能和可扩展性的变革性影响。通过详细的敏感性分析，我们量化了计算与通信重叠、利用硬件加速集合通信、更宽的横向扩展域和更大内存容量的益处。我们的研究涵盖了稀疏（专家混合）和密集型基于Transformer的LLMs，揭示了系统设计选择如何影响模型FLOPS利用率（MFU = 每token的模型flops x 每秒观察到的token / 硬件峰值flops）和整体吞吐量。在协同设计研究中，我们扩展并验证了一个性能建模工具，该工具能够预测LLM运行时，精度在真实测量值的10%以内。我们的发现为设计能够高效支持万亿参数模型、降低优化复杂性并维持AI能力快速发展的AI数据中心提供了可操作的见解和实用的路线图。", "summary": "本论文针对大型语言模型（LLMs）的爆发式增长对数据中心架构提出的挑战，提出了一套全面的协同设计框架。该框架综合考量了计算能力（FLOPS）、内存带宽与容量（HBM）、多种网络拓扑（包括新型FullFlat架构）、横向扩展规模及LLM优化策略。研究通过引入和评估FullFlat网络，证明了其在提升性能和可扩展性方面的显著优势，并量化了计算通信重叠、硬件加速集合通信、更大扩展域和内存容量带来的效益。研究涵盖稀疏和密集型LLM，揭示了系统设计对模型FLOPS利用率和吞吐量的影响。此外，论文还验证了一个预测LLM运行时精度在10%内的性能建模工具。最终，本研究为构建高效支持万亿参数AI模型的数据中心提供了实用见解和设计路线图。", "keywords": "数据中心, 大型语言模型, 网络架构, 可扩展性, 协同设计", "comments": "这项工作在应对当前大型语言模型对数据中心基础设施的巨大需求方面具有重要意义。其创新之处在于提出了一个全面的协同设计框架，并引入了FullFlat网络架构，这对于解决LLM的扩展性、效率和成本问题提供了新思路。该研究不仅停留在理论层面，还通过性能建模工具的验证，提供了实用且可操作的见解，为未来AI数据中心的设计提供了宝贵的路线图。特别值得关注的是对稀疏和密集型LLM的考量，这使得其研究结果更具普适性。"}}
{"id": "2506.14851", "title": "Efficient Serving of LLM Applications with Probabilistic Demand Modeling", "authors": ["Yifei Liu", "Zuo Gan", "Zhenghao Gan", "Weiye Wang", "Chen Chen", "Yizhou Shan", "Xusheng Chen", "Zhenhua Han", "Yifei Zhu", "Shixuan Sun", "Minyi Guo"], "summary": "Applications based on Large Language Models (LLMs) contains a series of tasks\nto address real-world problems with boosted capability, which have dynamic\ndemand volumes on diverse backends. Existing serving systems treat the resource\ndemands of LLM applications as a blackbox, compromising end-to-end efficiency\ndue to improper queuing order and backend warm up latency. We find that the\nresource demands of LLM applications can be modeled in a general and accurate\nmanner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which\nleverages PDGraph for efficient serving of LLM applications. Confronting\nprobabilistic demand description, Hermes applies the Gittins policy to\ndetermine the scheduling order that can minimize the average application\ncompletion time. It also uses the PDGraph model to help prewarm cold backends\nat proper moments. Experiments with diverse LLM applications confirm that\nHermes can effectively improve the application serving efficiency, reducing the\naverage completion time by over 70% and the P95 completion time by over 80%.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14851v1", "AI": {"title_translation": "大语言模型应用的高效服务与概率需求建模", "tldr": "Hermes系统通过概率需求建模（PDGraph）和Gittins策略，显著提升了LLM应用的服务效率，大幅减少了应用完成时间。", "motivation": "现有的LLM应用服务系统将资源需求视为黑盒，导致排队顺序不当和后端预热延迟，从而损害了端到端效率。", "method": "本文提出了概率需求图（PDGraph）来通用且准确地建模LLM应用的资源需求。在此基础上，提出了Hermes系统，该系统利用PDGraph，并应用Gittins策略来确定最小化平均应用完成时间的调度顺序，同时利用PDGraph模型在适当的时机预热冷后端。", "result": "实验证明，Hermes能有效提高LLM应用的服务效率，将平均完成时间减少70%以上，将P95完成时间减少80%以上。", "conclusion": "通过对LLM应用概率需求的精确建模（PDGraph）并结合Gittins策略进行调度和预热，Hermes系统显著提升了LLM应用的服务效率。", "translation": "基于大型语言模型（LLM）的应用包含一系列任务，以增强能力解决现实世界问题，这些任务在不同的后端上具有动态的需求量。现有的服务系统将LLM应用的资源需求视为黑盒，由于不当的排队顺序和后端预热延迟，损害了端到端效率。我们发现LLM应用的资源需求可以通过概率需求图（PDGraph）以通用且准确的方式建模。然后我们提出了Hermes，它利用PDGraph高效服务LLM应用。面对概率需求描述，Hermes应用Gittins策略来确定可以最小化平均应用完成时间的调度顺序。它还使用PDGraph模型在适当的时机帮助预热冷后端。对各种LLM应用的实验证实，Hermes可以有效提高应用服务效率，将平均完成时间减少70%以上，将P95完成时间减少80%以上。", "summary": "本文针对现有LLM应用服务系统效率低下的问题，提出了概率需求图（PDGraph）来精确建模LLM应用的动态资源需求。在此基础上，开发了Hermes系统，该系统利用PDGraph结合Gittins策略优化调度顺序，并实现后端及时预热，从而显著提升了LLM应用的服务效率，实验结果显示平均完成时间减少了70%以上，P95完成时间减少了80%以上。", "keywords": "LLM服务, 概率需求建模, 调度, 后端预热, Hermes", "comments": "该论文的创新点在于提出了概率需求图（PDGraph）来精确建模LLM应用的动态资源需求，并在此基础上设计了Hermes系统，利用Gittins策略进行智能调度和后端预热。这种方法克服了现有系统将需求视为黑盒的局限性，显著提升了LLM应用的服务效率，具有重要的实践意义。"}}
{"id": "2506.14791", "title": "SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection", "authors": ["Jingxuan Zhou", "Yuehao Wu", "Yibo Zhang", "Yeyubei Zhang", "Yunchong Liu", "Bolin Huang", "Chunhong Yuan"], "summary": "Aiming at the problem of difficulty in accurately identifying graphical\nimplicit correlations in multimodal irony detection tasks, this paper proposes\na Semantic Irony Recognition Network (SemIRNet). The model contains three main\ninnovations: (1) The ConceptNet knowledge base is introduced for the first time\nto acquire conceptual knowledge, which enhances the model's common-sense\nreasoning ability; (2) Two cross-modal semantic similarity detection modules at\nthe word level and sample level are designed to model graphic-textual\ncorrelations at different granularities; and (3) A contrastive learning loss\nfunction is introduced to optimize the spatial distribution of the sample\nfeatures, which improves the separability of positive and negative samples.\nExperiments on a publicly available multimodal irony detection benchmark\ndataset show that the accuracy and F1 value of this model are improved by 1.64%\nand 2.88% to 88.87% and 86.33%, respectively, compared with the existing\noptimal methods. Further ablation experiments verify the important role of\nknowledge fusion and semantic similarity detection in improving the model\nperformance.", "comment": "5 pages, 3 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14791v1", "AI": {"title_translation": "SemIRNet：一种用于多模态讽刺检测的语义反讽识别网络", "tldr": "该论文提出SemIRNet，一个引入ConceptNet知识库、跨模态语义相似度检测模块和对比学习损失的多模态讽刺检测网络，显著提升了检测准确性和F1值。", "motivation": "在多模态反讽检测任务中，难以准确识别图形隐式关联是当前面临的问题。", "method": "本文提出语义反讽识别网络（SemIRNet），包含三项创新：1. 首次引入ConceptNet知识库以增强常识推理能力；2. 设计词级和样本级两种跨模态语义相似度检测模块，建模不同粒度的图文关联；3. 引入对比学习损失函数优化样本特征空间分布，提高正负样本的可分离性。", "result": "在公开多模态反讽检测基准数据集上的实验表明，与现有最优方法相比，SemIRNet的准确率和F1值分别提高1.64%和2.88%，达到88.87%和86.33%。进一步的消融实验验证了知识融合和语义相似度检测对模型性能提升的重要作用。", "conclusion": "SemIRNet通过引入知识库、设计跨模态相似度模块和采用对比学习，有效解决了多模态反讽检测中图形隐式关联识别的难题，并取得了显著的性能提升。", "translation": "针对多模态反讽检测任务中难以准确识别图形隐式关联的问题，本文提出了一种语义反讽识别网络（SemIRNet）。该模型包含三项主要创新：(1) 首次引入ConceptNet知识库以获取概念知识，增强了模型的常识推理能力；(2) 设计了词级和样本级两种跨模态语义相似度检测模块，以不同粒度建模图文关联；(3) 引入对比学习损失函数，优化样本特征的空间分布，提高了正负样本的可分离性。在公开的多模态反讽检测基准数据集上进行的实验表明，与现有最优方法相比，该模型的准确率和F1值分别提高了1.64%和2.88%，达到88.87%和86.33%。进一步的消融实验验证了知识融合和语义相似度检测在提高模型性能方面的重要作用。", "summary": "本文针对多模态反讽检测中图形隐式关联识别的难题，提出了一种语义反讽识别网络（SemIRNet）。该模型通过引入ConceptNet知识库增强常识推理，设计词级和样本级跨模态语义相似度检测模块建模图文关联，并利用对比学习优化特征空间分布。实验结果显示，SemIRNet在公开数据集上显著提升了准确率和F1值，验证了其创新方法的有效性。", "keywords": "多模态反讽检测, 语义反讽识别, ConceptNet, 跨模态相似度, 对比学习", "comments": "SemIRNet的创新点在于将ConceptNet知识库引入多模态反讽检测，以增强常识推理能力，这在同类研究中较为新颖。同时，通过设计多粒度跨模态相似度检测和引入对比学习，有效解决了图文信息融合和特征表示的挑战，其性能提升具有实际意义。"}}
{"id": "2506.14936", "title": "CALM: Contextual Analog Logic with Multimodality", "authors": ["Maxwell J. Jacobson", "Corey J. Maley", "Yexiang Xue"], "summary": "In this work, we introduce Contextual Analog Logic with Multimodality (CALM).\nCALM unites symbolic reasoning with neural generation, enabling systems to make\ncontext-sensitive decisions grounded in real-world multi-modal data.\n  Background: Classic bivalent logic systems cannot capture the nuance of human\ndecision-making. They also require human grounding in multi-modal environments,\nwhich can be ad-hoc, rigid, and brittle. Neural networks are good at extracting\nrich contextual information from multi-modal data, but lack interpretable\nstructures for reasoning.\n  Objectives: CALM aims to bridge the gap between logic and neural perception,\ncreating an analog logic that can reason over multi-modal inputs. Without this\nintegration, AI systems remain either brittle or unstructured, unable to\ngeneralize robustly to real-world tasks. In CALM, symbolic predicates evaluate\nto analog truth values computed by neural networks and constrained search.\n  Methods: CALM represents each predicate using a domain tree, which\niteratively refines its analog truth value when the contextual groundings of\nits entities are determined. The iterative refinement is predicted by neural\nnetworks capable of capturing multi-modal information and is filtered through a\nsymbolic reasoning module to ensure constraint satisfaction.\n  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%\naccuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It\nalso demonstrated spatial heatmap generation aligned with logical constraints\nand delicate human preferences, as shown by a human study.\n  Conclusions: CALM demonstrates the potential to reason with logic structure\nwhile aligning with preferences in multi-modal environments. It lays the\nfoundation for next-gen AI systems that require the precision and\ninterpretation of logic and the multimodal information processing of neural\nnetworks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.14936v1", "AI": {"title_translation": "CALM：多模态上下文模拟逻辑", "tldr": "CALM结合符号逻辑和神经网络，通过模拟逻辑处理多模态数据，实现上下文敏感的决策，并在对象放置任务中表现出色。", "motivation": "传统二值逻辑系统无法捕捉人类决策的细微差别，且在多模态环境中需要僵化的人工基础。神经网络擅长提取多模态信息但缺乏可解释的推理结构。CALM旨在弥合逻辑与神经感知之间的鸿沟，创建一种能够对多模态输入进行推理的模拟逻辑，以解决现有AI系统脆弱或非结构化、无法稳健泛化的问题。", "method": "CALM通过领域树表示每个谓词，该树在确定实体上下文基础时迭代地细化其模拟真值。这种迭代细化由能够捕获多模态信息的神经网络预测，并通过符号推理模块过滤以确保约束满足。", "result": "在填空式对象放置任务中，CALM达到了92.2%的准确率，优于经典逻辑（86.3%）和LLM（59.4%）基线。它还展示了与逻辑约束和微妙的人类偏好对齐的空间热图生成，这一点通过一项人类研究得到证实。", "conclusion": "CALM展示了在多模态环境中利用逻辑结构进行推理同时与偏好对齐的潜力。它为需要逻辑的精确性和解释性以及神经网络的多模态信息处理能力的下一代AI系统奠定了基础。", "translation": "在这项工作中，我们介绍了多模态上下文模拟逻辑（CALM）。CALM将符号推理与神经生成相结合，使系统能够根据真实世界的多模态数据做出上下文敏感的决策。背景：经典的二值逻辑系统无法捕捉人类决策的细微差别。它们还需要在多模态环境中进行人工基础，这可能是临时性的、僵化的和脆弱的。神经网络擅长从多模态数据中提取丰富的上下文信息，但缺乏可解释的推理结构。目标：CALM旨在弥合逻辑和神经感知之间的鸿沟，创建一种可以对多模态输入进行推理的模拟逻辑。如果没有这种集成，AI系统仍然要么脆弱，要么非结构化，无法稳健地泛化到现实世界的任务。在CALM中，符号谓词评估为由神经网络和约束搜索计算的模拟真值。方法：CALM使用领域树表示每个谓词，当其实体的上下文基础确定时，该领域树会迭代地细化其模拟真值。迭代细化由能够捕获多模态信息的神经网络预测，并通过符号推理模块过滤以确保约束满足。结果：在填空式对象放置任务中，CALM取得了92.2%的准确率，优于经典逻辑（86.3%）和LLM（59.4%）基线。它还展示了与逻辑约束和微妙的人类偏好对齐的空间热图生成，这一点通过一项人类研究得到证实。结论：CALM展示了在多模态环境中利用逻辑结构进行推理同时与偏好对齐的潜力。它为需要逻辑的精确性和解释性以及神经网络的多模态信息处理能力的下一代AI系统奠定了基础。", "summary": "本文提出了多模态上下文模拟逻辑（CALM），它将符号推理与神经网络的上下文感知能力相结合，旨在解决传统逻辑系统在多模态数据处理中的局限性以及神经网络缺乏可解释推理结构的问题。CALM通过领域树和迭代细化过程，利用神经网络计算模拟真值并结合符号推理确保约束满足。实验结果表明，CALM在对象放置任务中显著优于传统逻辑和大型语言模型，并能生成符合逻辑和人类偏好的空间热图。这表明CALM为构建结合逻辑精度和多模态处理能力的下一代AI系统奠定了基础。", "keywords": "上下文模拟逻辑,多模态,符号推理,神经网络,真值", "comments": "CALM的创新点在于其将符号逻辑与神经网络的优势结合起来，通过“模拟真值”的概念弥合了二者之间的鸿沟，实现了更具细微差别和上下文敏感的推理。其重要性在于提供了一种处理多模态数据并同时保持一定可解释性的方法，这对于提升AI系统在复杂现实世界任务中的泛化能力至关重要。通过在对象放置任务中的显著性能提升，验证了其方法的有效性。"}}
{"id": "2506.14933", "title": "Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection", "authors": ["Adriana Watson"], "summary": "The decentralized finance (DeFi) community has grown rapidly in recent years,\npushed forward by cryptocurrency enthusiasts interested in the vast untapped\npotential of new markets. The surge in popularity of cryptocurrency has ushered\nin a new era of financial crime. Unfortunately, the novelty of the technology\nmakes the task of catching and prosecuting offenders particularly challenging.\nThus, it is necessary to implement automated detection tools related to\npolicies to address the growing criminality in the cryptocurrency realm.", "comment": "6 pages, 4 figures. Code available at:\n  https://github.com/awatson246/crypto-anomaly-detection-policy", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.14933v1", "AI": {"title_translation": "先解释，后信任：基于图的加密货币异常检测中的大型语言模型增强解释", "tldr": "随着加密货币犯罪的增加，由于技术新颖性，打击犯罪变得困难，因此需要自动化检测工具。", "motivation": "去中心化金融（DeFi）和加密货币的快速发展带来了新的金融犯罪浪潮。由于技术的新颖性，抓捕和起诉犯罪分子极具挑战性。因此，迫切需要实施自动化检测工具来应对加密货币领域日益增长的犯罪活动。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "近年来，去中心化金融（DeFi）社区发展迅速，得益于加密货币爱好者对新市场巨大未开发潜力的兴趣推动。加密货币的普及带来了金融犯罪的新时代。不幸的是，这项技术的新颖性使得抓捕和起诉犯罪分子的任务尤其具有挑战性。因此，有必要实施与政策相关的自动化检测工具，以解决加密货币领域日益增长的犯罪活动。", "summary": "抽象指出，去中心化金融（DeFi）和加密货币的迅速兴起伴随着金融犯罪的增加。由于技术的新颖性，打击这些犯罪面临挑战，这凸显了在加密货币领域部署自动化检测工具以应对日益增长的犯罪活动的紧迫性。", "keywords": "DeFi, 加密货币, 金融犯罪, 自动化检测", "comments": "这篇论文的标题表明其创新之处在于利用大型语言模型（LLM）为基于图的加密货币异常检测提供解释，旨在解决信任问题。这对于在金融犯罪等高风险领域部署AI系统至关重要，因为可解释性可以增强用户对检测结果的信任和理解。"}}
{"id": "2506.14774", "title": "MedSyn: Enhancing Diagnostics with Human-AI Collaboration", "authors": ["Burcu Sayin", "Ipek Baris Schlicht", "Ngoc Vo Hong", "Sara Allievi", "Jacopo Staiano", "Pasquale Minervini", "Andrea Passerini"], "summary": "Clinical decision-making is inherently complex, often influenced by cognitive\nbiases, incomplete information, and case ambiguity. Large Language Models\n(LLMs) have shown promise as tools for supporting clinical decision-making, yet\ntheir typical one-shot or limited-interaction usage may overlook the\ncomplexities of real-world medical practice. In this work, we propose a hybrid\nhuman-AI framework, MedSyn, where physicians and LLMs engage in multi-step,\ninteractive dialogues to refine diagnoses and treatment decisions. Unlike\nstatic decision-support tools, MedSyn enables dynamic exchanges, allowing\nphysicians to challenge LLM suggestions while the LLM highlights alternative\nperspectives. Through simulated physician-LLM interactions, we assess the\npotential of open-source LLMs as physician assistants. Results show open-source\nLLMs are promising as physician assistants in the real world. Future work will\ninvolve real physician interactions to further validate MedSyn's usefulness in\ndiagnostic accuracy and patient outcomes.", "comment": "Accepted to the Trustworthy and Collaborative Artificial Intelligence\n  Workshop 2025 (TCAI 2025) in the 4th International Conference Series on\n  Hybrid Human-Artificial Intelligence (HHAI 2025)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14774v1", "AI": {"title_translation": "MedSyn：通过人机协作增强诊断", "tldr": "MedSyn是一个人机协作框架，通过多步交互对话，让医生和大型语言模型（LLMs）共同改进诊断和治疗决策，以应对临床决策的复杂性。", "motivation": "临床决策固有的复杂性，常受认知偏差、信息不完整和病例模糊性影响。尽管大型语言模型（LLMs）在支持临床决策方面显示出潜力，但其传统的单次或有限交互使用方式可能忽略了真实世界医疗实践的复杂性。", "method": "本文提出了一个混合人机智能框架MedSyn，医生和LLMs在该框架中进行多步交互对话，以完善诊断和治疗决策。与静态决策支持工具不同，MedSyn支持动态交流，允许医生质疑LLM的建议，同时LLM也能提出替代观点。研究通过模拟医生-LLM交互来评估开源LLM作为医生助手的潜力。", "result": "结果显示，开源LLMs作为医生助手在真实世界中具有前景。", "conclusion": "开源LLMs作为医生助手具有潜力。未来的工作将涉及真实的医生交互，以进一步验证MedSyn在诊断准确性和患者结果方面的有效性。", "translation": "临床决策本身就很复杂，常常受到认知偏差、信息不完整和病例模糊性的影响。大型语言模型（LLMs）作为支持临床决策的工具已显示出前景，但其典型的单次或有限交互使用可能忽略了真实世界医疗实践的复杂性。在这项工作中，我们提出了一个混合人机智能框架MedSyn，医生和LLMs在该框架中进行多步、交互式对话，以完善诊断和治疗决策。与静态决策支持工具不同，MedSyn支持动态交流，允许医生质疑LLM的建议，同时LLM也能提出替代观点。通过模拟医生-LLM交互，我们评估了开源LLM作为医生助手的潜力。结果显示开源LLMs作为医生助手在真实世界中具有前景。未来的工作将涉及真实的医生交互，以进一步验证MedSyn在诊断准确性和患者结果方面的有效性。", "summary": "MedSyn是一个创新的人机协作框架，旨在通过医生与大型语言模型（LLMs）之间的多步、交互式对话来优化临床诊断和治疗决策。该框架克服了传统LLM应用中交互不足的局限性，允许动态的质疑与反馈。通过模拟交互，研究发现开源LLMs作为医生助手具有显著潜力，预示着未来在真实医疗环境中提升诊断准确性和患者预后的可能性。", "keywords": "人机协作, 大型语言模型, 临床诊断, 医疗决策, MedSyn", "comments": "MedSyn框架的创新之处在于其强调人机之间的多步、动态交互，而非静态或单次决策支持。这更符合真实世界医疗实践的复杂性，允许医生和AI相互挑战和补充，从而可能显著提高诊断准确性。其重要性在于为AI在医疗领域的应用提供了一个更实用和有效的范式，超越了简单的辅助工具，迈向真正的协作伙伴。虽然初步结果令人鼓舞，但目前仅基于模拟交互，未来需要真实医生参与的验证才能充分证明其在临床实践中的价值和局限性。"}}
{"id": "2506.15532", "title": "Controller Synthesis for Parametric Timed Games", "authors": ["Mikael Bisgaard Dahlsen-Jensen", "Baptiste Fievet", "Laure Petrucci", "Jaco van de Pol"], "summary": "We present a (semi)-algorithm to compute winning strategies for parametric\ntimed games. Previous algorithms only synthesized constraints on the clock\nparameters for which the game is winning. A new definition of (winning)\nstrategies is proposed, and ways to compute them. A transformation of these\nstrategies to (parametric) timed automata allows for building a controller\nenforcing them. The feasibility of the method is demonstrated by an\nimplementation and experiments for the Production Cell case study.", "comment": "This is the full version of the paper under the same title accepted\n  to QEST+FORMATS 2025. 29 pages", "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.15532v1", "AI": {"title_translation": "参数化时间博弈的控制器合成", "tldr": "提出了一种计算参数化时间博弈制胜策略的半算法，并可转换为控制器。", "motivation": "先前的算法只能合成博弈获胜的时钟参数约束，而不能直接计算制胜策略。", "method": "提出了一种新的（制胜）策略定义及其计算方法。这些策略可以转换为（参数化）时间自动机，从而构建一个强制执行这些策略的控制器。", "result": "开发了一种计算参数化时间博弈制胜策略的（半）算法。该方法已通过在生产单元案例研究中的实现和实验证明了其可行性。", "conclusion": "该研究成功地提出了一种新的方法来计算参数化时间博弈的制胜策略，并能够将其转化为可执行的控制器，填补了现有算法的不足。", "translation": "我们提出了一种（半）算法，用于计算参数化时间博弈的制胜策略。以前的算法只合成博弈获胜的时钟参数约束。本文提出了一种新的（制胜）策略定义，以及计算它们的方法。将这些策略转换为（参数化）时间自动机，可以构建一个强制执行它们的控制器。该方法的可行性通过在生产单元案例研究中的实现和实验得到了证明。", "summary": "本文提出了一种计算参数化时间博弈制胜策略的（半）算法，旨在克服现有算法仅能合成参数约束的局限。研究中定义了新的制胜策略及其计算方法，并将其转换为参数化时间自动机以构建控制器。该方法已通过生产单元案例研究的实现和实验验证了其可行性。", "keywords": "参数化时间博弈, 制胜策略, 控制器合成, 时间自动机, 生产单元", "comments": "这项工作创新性地提出了直接计算参数化时间博弈制胜策略的方法，并能将其转化为实际可用的控制器，而非仅仅是参数约束，这对于实时系统控制具有重要意义。"}}
{"id": "2506.14863", "title": "Preparing for the Intelligence Explosion", "authors": ["William MacAskill", "Fin Moorhouse"], "summary": "AI that can accelerate research could drive a century of technological\nprogress over just a few years. During such a period, new technological or\npolitical developments will raise consequential and hard-to-reverse decisions,\nin rapid succession. We call these developments grand challenges. These\nchallenges include new weapons of mass destruction, AI-enabled autocracies,\nraces to grab offworld resources, and digital beings worthy of moral\nconsideration, as well as opportunities to dramatically improve quality of life\nand collective decision-making. We argue that these challenges cannot always be\ndelegated to future AI systems, and suggest things we can do today to\nmeaningfully improve our prospects. AGI preparedness is therefore not just\nabout ensuring that advanced AI systems are aligned: we should be preparing,\nnow, for the disorienting range of developments an intelligence explosion would\nbring.", "comment": "61 pages", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14863v1", "AI": {"title_translation": "准备应对智能爆炸", "tldr": "论文讨论了AI加速研究可能带来的“智能爆炸”时期，该时期将出现一系列重大的、难以逆转的挑战和机遇，并强调现在就需要为这些潜在的颠覆性发展做好准备，而不仅仅是关注AI对齐。", "motivation": "论文的动机是指出AI加速研究可能导致“智能爆炸”，这将带来一系列快速连续的、具有重大影响且难以逆转的挑战（如新型武器、AI独裁、资源争夺、数字生命体等）和机遇。作者认为这些挑战不能总是委托给未来的AI系统，因此需要现在就开始为这些潜在的颠覆性发展做准备。", "method": "论文通过论证和提出建议来阐述其观点。具体方法是识别并定义“重大挑战”，并强调提前准备的重要性。", "result": "论文的结果是提出了一系列“重大挑战”的分类，并强调了现在就进行准备的必要性，指出AGI准备不仅仅是关于AI对齐，更是关于应对智能爆炸带来的广泛发展。", "conclusion": "结论是，面对AI可能引发的智能爆炸，人类现在就需要为随之而来的各种颠覆性发展做好准备，而不仅仅是将注意力局限于确保高级AI系统对齐。", "translation": "人工智能加速研究可能在短短几年内推动一个世纪的技术进步。在此期间，新的技术或政治发展将接连带来重大且难以逆转的决策，我们称这些发展为“重大挑战”。这些挑战包括新型大规模杀伤性武器、人工智能驱动的独裁政权、对外星资源的争夺、以及值得道德考量的数字生命，同时也有大幅提升生活质量和集体决策的机会。我们认为，这些挑战并非总能委托给未来的AI系统，并提出我们今天可以做些什么来有意义地改善我们的前景。因此，通用人工智能（AGI）的准备工作不仅仅是确保高级AI系统对齐：我们现在就应该为智能爆炸将带来的令人迷失方向的各种发展做好准备。", "summary": "这篇论文探讨了人工智能加速研究可能引发的“智能爆炸”现象，指出它将在短时间内带来一系列重大的、难以逆转的技术和政治挑战，如新型武器、AI独裁和数字生命，同时也伴随改善生活质量的机遇。作者强调，这些挑战不能完全寄希望于未来的AI系统解决，因此，人类现在就必须采取行动，为智能爆炸可能带来的广泛而颠覆性的发展做好准备，而不仅仅是关注AI的对齐问题。", "keywords": "智能爆炸, 人工智能, 重大挑战, 准备, 技术进步", "comments": "这篇论文的创新之处在于其对“智能爆炸”背景下潜在“重大挑战”的系统性定义和分类，并强调了提前准备的紧迫性和必要性，超越了传统上对AI对齐的单一关注点。其重要性在于，它促使人们思考AI发展可能带来的深远社会和伦理影响，并呼吁在技术发展达到临界点之前采取预防性措施，具有前瞻性。"}}
{"id": "2506.15004", "title": "Mixed Traffic: A Perspective from Long Duration Autonomy", "authors": ["Filippos Tzortzoglou", "Logan E. Beaver"], "summary": "The rapid adoption of autonomous vehicle has established mixed traffic\nenvironments, comprising both autonomous and human-driven vehicles (HDVs), as\nessential components of next-generation mobility systems. Along these lines,\nconnectivity between autonomous vehicles and infrastructure (V2I) is also a\nsignificant factor that can effectively support higher-level decision-making.\nAt the same time, the integration of V2I within mixed traffic environments\nremains a timely and challenging problem. In this paper, we present a\nlong-duration autonomy controller for connected and automated vehicles (CAVs)\noperating in such environments, with a focus on intersections where right turns\non red are permitted. We begin by deriving the optimal control policy for CAVs\nunder free-flow traffic. Next, we analyze crossing time constraints imposed by\nsmart traffic lights and map these constraints to controller bounds using\nControl Barrier Functions (CBFs), with the aim to drive a CAV to cross the\nintersection on time. We also introduce criteria for identifying, in real-time,\nfeasible crossing intervals for each CAV. To ensure safety for the CAVs, we\npresent model-agnostic safety guarantees, and demonstrate their compatibility\nwith both CAVs and HDVs. Ultimately, the final control actions are enforced\nthrough a combination of CBF constraints, constraining CAVs to traverse the\nintersection within the designated time intervals while respecting other\nvehicles. Finally, we guarantee that our control policy yields always a\nfeasible solution and validate the proposed approach through extensive\nsimulations in MATLAB.", "comment": "14 pages, 12 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15004v1", "AI": {"title_translation": "混合交通：长时自主性的视角", "tldr": "本文提出了一种用于混合交通环境中联网自动驾驶车辆（CAVs）的长时自主控制器，特别关注允许红灯右转的交叉口，并通过控制屏障函数（CBFs）确保安全和按时通过。", "motivation": "自动驾驶汽车的快速普及使得混合交通环境成为下一代出行系统的重要组成部分。联网自动驾驶车辆（CAVs）与基础设施（V2I）的集成在混合交通环境中仍是一个及时且具有挑战性的问题，尤其是在需要高级决策支持的场景。", "method": "作者首先推导了自由流交通下CAVs的最优控制策略。然后，他们分析了智能交通灯施加的交叉口时间约束，并使用控制屏障函数（CBFs）将这些约束映射到控制器边界，以确保CAV按时通过交叉口。论文还引入了实时识别每个CAV可行交叉口间隔的标准，并提出了模型无关的安全保障，以确保CAVs和人工驾驶车辆（HDVs）的安全。最终，通过CBF约束来执行控制动作，确保CAV在指定时间间隔内通过交叉口并尊重其他车辆。", "result": "该控制策略总能产生一个可行的解决方案。通过MATLAB中的广泛仿真验证了所提出的方法。", "conclusion": "本研究成功开发并验证了一种长时自主控制器，该控制器能够使联网自动驾驶车辆（CAVs）在混合交通环境（特别是允许红灯右转的交叉口）中安全、有效地运行，并通过CBF确保了可行性和安全性。", "translation": "自动驾驶汽车的迅速普及使得由自动驾驶车辆和人工驾驶车辆（HDVs）组成的混合交通环境成为下一代出行系统的重要组成部分。与此同时，自动驾驶车辆与基础设施（V2I）之间的连接也是一个重要因素，能够有效支持更高层次的决策。然而，V2I在混合交通环境中的集成仍然是一个及时且具有挑战性的问题。在本文中，我们为在这种环境中运行的联网自动驾驶车辆（CAVs）提出了一种长时自主控制器，重点关注允许红灯右转的交叉口。我们首先推导了自由流交通下CAVs的最优控制策略。接下来，我们分析了智能交通灯施加的通过时间约束，并使用控制屏障函数（CBFs）将这些约束映射到控制器边界，旨在驱动CAV按时通过交叉口。我们还引入了实时识别每个CAV可行通过间隔的标准。为了确保CAVs的安全，我们提出了模型无关的安全保障，并证明了它们与CAVs和HDVs的兼容性。最终，通过CBF约束的组合来执行最终的控制动作，约束CAVs在指定的时间间隔内通过交叉口，同时尊重其他车辆。最后，我们保证我们的控制策略总能产生一个可行的解决方案，并通过MATLAB中的广泛仿真验证了所提出的方法。", "summary": "本文针对包含自动驾驶车辆和人工驾驶车辆的混合交通环境，提出了一种长时自主控制器，专注于允许红灯右转的交叉口。研究通过推导最优控制策略和利用控制屏障函数（CBFs）来处理智能交通灯的时间约束，并提出了实时识别可行通过间隔的准则。为确保安全，论文引入了模型无关的安全保障，并结合CBF约束来执行控制动作。该方法被证明能产生可行解，并通过MATLAB仿真验证。", "keywords": "混合交通, 联网自动驾驶车辆, 控制屏障函数, 交叉口, 长时自主性", "comments": "该论文在解决混合交通环境中联网自动驾驶车辆（CAVs）的实际挑战方面具有创新性，特别是在交叉口管理和红灯右转场景。其采用控制屏障函数（CBFs）来确保安全性和实时可行性是一个重要的贡献。模型无关的安全保障增加了其通用性。通过仿真验证了其有效性，但未来工作可能需要考虑真实世界的部署和更复杂的交通场景。"}}
{"id": "2506.14906", "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder", "authors": ["Robert Czupryniak", "Abhishek Chakraborty", "Andrew N. Jordan", "John C. Howell"], "summary": "We apply machine learning methods to demonstrate range superresolution in\nremote sensing radar detection. Specifically, we implement a denoising\nautoencoder to estimate the distance between two equal intensity scatterers in\nthe subwavelength regime. The machine learning models are trained on waveforms\nsubject to a bandlimit constraint such that ranges much smaller than the\ninverse bandlimit are optimized in their precision. The autoencoder achieves\neffective dimensionality reduction, with the bottleneck layer exhibiting a\nstrong and consistent correlation with the true scatterer separation. We\nconfirm reproducibility across different training sessions and network\ninitializations by analyzing the scaled encoder outputs and their robustness to\nnoise. We investigate the behavior of the bottleneck layer for the following\ntypes of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,\nand a theoretically near-optimal pulse created from a spherical Bessel function\nbasis. The Bessel signal performs best, followed by the triangle wave, with the\nsinc signal performing worst, highlighting the crucial role of signal design in\nthe success of machine-learning-based range resolution.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14906v1", "AI": {"title_translation": "使用去噪自编码器实现雷达测距超分辨率", "tldr": "本文使用去噪自编码器在雷达测距中实现了超分辨率，并发现信号设计对机器学习的测距分辨率至关重要。", "motivation": "在遥感雷达探测中展示距离超分辨率。", "method": "实现了去噪自编码器来估计亚波长区域中两个等强度散射体之间的距离。模型在受带限约束的波形上训练，以优化远小于逆带限的距离精度。研究了三种脉冲类型：传统sinc脉冲、带限三角形脉冲和理论上接近最优的球形贝塞尔函数基脉冲。", "result": "自编码器实现了有效的降维，瓶颈层与真实散射体分离具有强而一致的相关性。结果在不同训练会话和网络初始化中具有可重现性。贝塞尔信号表现最佳，其次是三角形波，sinc信号表现最差。", "conclusion": "信号设计在基于机器学习的测距分辨率成功中扮演着关键角色。", "translation": "我们应用机器学习方法来展示遥感雷达探测中的距离超分辨率。具体来说，我们实现了一个去噪自编码器来估计亚波长区域中两个等强度散射体之间的距离。机器学习模型在受带限约束的波形上进行训练，从而优化了远小于逆带限的距离精度。自编码器实现了有效的降维，其瓶颈层与真实的散射体分离表现出强烈且一致的相关性。我们通过分析缩放后的编码器输出及其对噪声的鲁棒性，确认了不同训练会话和网络初始化之间的可重现性。我们研究了瓶颈层对于以下类型脉冲的行为：传统的sinc脉冲、带限三角形类型脉冲，以及由球形贝塞尔函数基创建的理论上接近最优的脉冲。贝塞尔信号表现最佳，其次是三角形波，sinc信号表现最差，这突出显示了信号设计在基于机器学习的距离分辨率成功中的关键作用。", "summary": "本文利用去噪自编码器实现了雷达测距的超分辨率，特别是在亚波长范围内估计散射体距离。通过在受带限约束的波形上训练模型，研究发现自编码器能有效降维，其瓶颈层与真实距离高度相关，且结果具有可重现性。研究还比较了不同脉冲信号对性能的影响，揭示了信号设计在基于机器学习的测距分辨率中的关键作用，其中贝塞尔信号表现最优。", "keywords": "雷达测距, 超分辨率, 去噪自编码器, 机器学习, 信号设计", "comments": "这篇论文的创新点在于将去噪自编码器应用于雷达测距的超分辨率问题，尤其是在亚波长区域。它不仅展示了机器学习在此领域的潜力，还强调了信号设计对于模型性能的关键影响，为未来相关研究提供了重要指导。"}}
{"id": "2506.14974", "title": "Parallel Complexity of Depth-First-Search and Maximal path", "authors": ["Archit Chauhan", "Samir Datta", "M. Praveen"], "summary": "Constructing a Depth First Search (DFS) tree is a fundamental graph problem,\nwhose parallel complexity is still not settled. Reif showed parallel\nintractability of lex-first DFS. In contrast, randomized parallel algorithms\n(and more recently, deterministic quasipolynomial parallel algorithms) are\nknown for constructing a DFS tree in general (di)graphs. However a\ndeterministic parallel algorithm for DFS in general graphs remains an elusive\ngoal. Working towards this, a series of works gave deterministic NC algorithms\nfor DFS in planar graphs and digraphs. We further extend these results to more\ngeneral graph classes, by providing NC algorithms for (di)graphs of bounded\ngenus, and for undirected H-minor-free graphs where H is a fixed graph with at\nmost one crossing. For the case of (di)graphs of bounded tree-width, we further\nimprove the complexity to a Logspace bound. Constructing a maximal path is a\nsimpler problem (that reduces to DFS) for which no deterministic parallel\nbounds are known for general graphs. For planar graphs a bound of O(log n)\nparallel time on a CREW PRAM (thus in NC2) is known. We improve this bound to\nLogspace.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.14974v1", "AI": {"title_translation": "深度优先搜索和最长路径的并行复杂度", "tldr": "本文为有界亏格图和H-minor-free图提供了确定性并行DFS算法，并改进了有界树宽图DFS和平面图最大路径问题的并行复杂度。", "motivation": "深度优先搜索（DFS）树的并行复杂度问题尚未解决，尤其是在一般图上确定性并行算法的缺失。同时，最大路径问题在一般图上也没有确定性并行边界。", "method": "提出了针对有界亏格有向图/无向图以及H-minor-free无向图（H是固定图且至多有一个交叉）的NC算法。对于有界树宽的有向图/无向图，将复杂度提升至Logspace。对于最大路径问题，将平面图的并行时间界限改进至Logspace。", "result": "为有界亏格有向图/无向图和H-minor-free无向图（H是固定图且至多有一个交叉）提供了NC算法。将有界树宽有向图/无向图的DFS复杂度改进至Logspace。将平面图上最大路径问题的并行时间界限改进至Logspace。", "conclusion": "本文成功为更广泛的图类（如有限亏格图和H-minor-free图）提供了确定性并行DFS算法，并将有界树宽图的DFS和平面图的最大路径问题的并行复杂度提升至Logspace。", "translation": "构建深度优先搜索（DFS）树是一个基本的图问题，其并行复杂度尚未解决。Reif证明了词典序DFS的并行难处理性。相比之下，对于在一般有向图/无向图中构建DFS树，已存在随机并行算法（以及最近的确定性准多项式并行算法）。然而，针对一般图的确定性并行DFS算法仍然是一个难以实现的目标。为此，一系列工作为平面图和有向图的DFS提供了确定性NC算法。我们通过为有界亏格的有向图/无向图以及H-minor-free无向图（其中H是固定图且至多有一个交叉）提供NC算法，进一步扩展了这些结果。对于有界树宽的有向图/无向图，我们将复杂度进一步提升至Logspace界限。构建最大路径是一个更简单的问题（可归约到DFS），但对于一般图，尚无已知的确定性并行界限。对于平面图，已知在CREW PRAM上存在O(log n)的并行时间界限（因此在NC2中）。我们将此界限改进至Logspace。", "summary": "本文探讨了深度优先搜索（DFS）树和最大路径的并行复杂度问题。鉴于一般图上确定性并行DFS算法的挑战，作者提出了针对有界亏格有向图/无向图以及H-minor-free无向图的NC算法。此外，对于有界树宽的有向图/无向图，DFS的并行复杂度被进一步提升至Logspace。对于最大路径问题，作者将平面图的并行时间界限从O(log n)改进至Logspace。这些工作致力于在更广泛的图类上实现确定性并行算法。", "keywords": "并行复杂度, 深度优先搜索, 最大路径, NC算法, Logspace", "comments": "本文的创新之处在于将确定性NC算法应用于更普遍的图类别，如有限亏格图和H-minor-free图，并为特定图类（如有限树宽图）的DFS和最大路径问题提供了更优的并行复杂度界限，推动了确定性并行图算法的研究进展。"}}
{"id": "2506.15183", "title": "You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality", "authors": ["Xingyu Chen", "Xinmin Fang", "Shuting Zhang", "Xinyu Zhang", "Liang He", "Zhengxiong Li"], "summary": "Mobile Virtual Reality (VR) is essential to achieving convenient and\nimmersive human-computer interaction and realizing emerging applications such\nas Metaverse. However, existing VR technologies require two separate renderings\nof binocular images, causing a significant bottleneck for mobile devices with\nlimited computing capability and power supply. This paper proposes an approach\nto rendering optimization for mobile VR called EffVR. By utilizing the\nper-pixel attribute, EffVR can generate binocular VR images from the monocular\nimage through genuinely one rendering, saving half the computation over\nconventional approaches. Our evaluation indicates that, compared with the\nstate-of-art, EffVRcan save 27% power consumption on average while achieving\nhigh binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR\napplications. Additionally, EffVR can increase the frame rate by 115.2%. These\nresults corroborate EffVRsuperior computation/energy-saving performance, paving\nthe road to a sustainable mobile VR. The source code, demo video, android app,\nand more are released anonymously at https://yoro-vr.github.io/", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15183v1", "AI": {"title_translation": "你只渲染一次：提升移动虚拟现实的能源和计算效率", "tldr": "EffVR通过单次渲染生成双目VR图像，显著提升移动VR的能效和计算效率。", "motivation": "现有移动VR技术需要两次独立的双目图像渲染，对计算能力和电源有限的移动设备造成显著瓶颈。", "method": "本文提出EffVR方法，通过利用逐像素属性，仅通过一次渲染从单目图像生成双目VR图像，相比传统方法节省一半计算量。", "result": "与现有技术相比，EffVR平均节省27%的功耗，同时保持高双目图像质量（SSIM 0.9679，PSNR 34.09），并将帧率提高了115.2%。", "conclusion": "这些结果证实了EffVR卓越的计算/节能性能，为可持续的移动VR铺平了道路。", "translation": "移动虚拟现实（VR）对于实现便捷沉浸式人机交互和实现元宇宙等新兴应用至关重要。然而，现有VR技术需要对双目图像进行两次单独渲染，这给计算能力和电源有限的移动设备带来了显著的瓶颈。本文提出了一种名为EffVR的移动VR渲染优化方法。通过利用逐像素属性，EffVR可以通过真正的一次渲染从单目图像生成双目VR图像，从而比传统方法节省一半的计算量。我们的评估表明，与现有技术相比，EffVR在移动VR应用中平均可节省27%的功耗，同时实现高双目图像质量（SSIM 0.9679和PSNR 34.09）。此外，EffVR可以将帧率提高115.2%。这些结果证实了EffVR卓越的计算/节能性能，为可持续的移动VR铺平了道路。源代码、演示视频、安卓应用程序等已匿名发布在https://yoro-vr.github.io/", "summary": "本文提出EffVR，一种针对移动VR的渲染优化方法。通过利用逐像素属性，EffVR能够从单目图像一次性生成双目VR图像，将计算量减半。实验结果表明，EffVR在保持高图像质量的同时，平均可节省27%的功耗，并将帧率提升115.2%，显著提升了移动VR的能效和计算性能，为可持续移动VR发展奠定基础。", "keywords": "移动VR, 渲染优化, 能效, 计算效率, 单次渲染", "comments": "EffVR的创新之处在于通过单次渲染生成双目图像，而非传统的两次渲染，这在资源受限的移动VR设备上具有重大意义。它通过巧妙地利用逐像素属性，实现了计算量减半和显著的能效提升，同时保持了高质量的视觉体验。这项工作对于推动移动VR的普及和可持续发展具有重要价值，解决了当前移动VR面临的核心性能瓶颈。"}}
{"id": "2506.15120", "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study with a Rényi Divergence-Based Solution", "authors": ["Shengjia Zhang", "Jiawei Chen", "Changdong Li", "Sheng Zhou", "Qihao Shi", "Yan Feng", "Chun Chen", "Can Wang"], "summary": "Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.", "comment": "AAAI 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15120v1", "AI": {"title_translation": "推荐系统中损失函数的进展：基于Rényi散度的解决方案的比较研究", "tldr": "本文深入分析了推荐系统中常用的Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 的优缺点，并提出了一种基于Rényi散度的通用损失函数DrRL，该函数结合了SL和CCL的优点，有效解决了它们的局限性，并在实验中验证了其在准确性和鲁棒性方面的优越性。", "motivation": "损失函数在优化推荐模型中扮演关键角色，其中Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 尤其有效，但它们的理论联系和差异需深入探索。SL对假负例敏感，CCL数据利用率低，这些局限性需要被解决。", "method": "本文对Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 进行了全面分析，发现两者均可视为传统损失函数与分布鲁棒优化 (DRO) 的结合。在此基础上，提出了一种新的损失函数DrRL，该函数通过在DRO优化中利用Rényi散度来泛化SL和CCL，并结合了两者的优势结构。通过广泛的实验验证了DrRL的优越性。", "result": "研究发现Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 均可通过分布鲁棒优化 (DRO) 增强，但SL对假负例敏感，而CCL存在数据利用率低的问题。本文提出的DrRL能够有效缓解SL和CCL的局限性。实验结果验证了DrRL在推荐准确性和鲁棒性方面的优越性。", "conclusion": "Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 是有效的推荐系统损失函数，但各有局限。本文提出的DrRL通过Rényi散度泛化了SL和CCL，成功结合了两者的优点并克服了它们的缺点，在推荐准确性和鲁棒性方面表现出优越性。", "translation": "损失函数在优化推荐模型中扮演着关键角色。在各种损失函数中，Softmax损失（SL）和余弦对比损失（CCL）特别有效。它们的理论联系和差异值得深入探索。这项工作对这些损失函数进行了全面的分析，获得了重要的见解：1）共同优点——两者都可以视为传统损失函数通过分布鲁棒优化（DRO）的增强，从而增强了对分布变化的鲁棒性；2）各自的局限性——由于它们在DRO优化中使用了不同的分布距离度量，SL对假负例实例表现出高敏感性，而CCL则存在数据利用率低的问题。为了解决这些这些局限性，这项工作提出了一种新的损失函数DrRL，它通过在DRO优化中利用Rényi散度来泛化SL和CCL。DrRL结合了SL和CCL的有利结构，并被证明能有效缓解它们的局限性。已经进行了广泛的实验来验证DrRL在推荐准确性和鲁棒性方面的优越性。", "summary": "本文深入探讨了推荐系统中Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 这两种有效损失函数的理论联系与局限。研究发现两者均可视为传统损失函数结合分布鲁棒优化 (DRO) 的增强，但SL对假负例敏感，CCL数据利用率低。为解决这些问题，论文提出了一种基于Rényi散度的通用损失函数DrRL。DrRL整合了SL和CCL的优点，有效克服了它们的缺点，并通过实验验证了其在推荐准确性和鲁棒性上的卓越性能。", "keywords": "推荐系统, 损失函数, 分布鲁棒优化, Rényi散度, Softmax Loss", "comments": "该论文的创新点在于提出了一个通用框架DrRL，通过Rényi散度统一并改进了SL和CCL。其重要性在于提供了一种更鲁棒、更高效的推荐系统损失函数，有望提升推荐模型的性能和泛化能力。通过对现有主流损失函数的深入分析，揭示了其内在联系和各自的优缺点，为后续研究提供了理论基础。"}}
{"id": "2506.14973", "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition", "authors": ["Jiamin Xie", "Ju Lin", "Yiteng Huang", "Tyler Vuong", "Zhaojiang Lin", "Zhaojun Yang", "Peng Su", "Prashant Rawat", "Sangeeta Srivastava", "Ming Sun", "Florian Metze"], "summary": "Recent studies have demonstrated that prompting large language models (LLM)\nwith audio encodings enables effective speech recognition capabilities.\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\naudio with spatial cues remains a relatively uninvestigated area of research.\nIn this work, we present directional-SpeechLlama, a novel approach that\nleverages the microphone array of smart glasses to achieve directional speech\nrecognition, source localization, and bystander cross-talk suppression. To\nenhance the model's ability to understand directivity, we propose two key\ntechniques: serialized directional output training (S-DOT) and contrastive\ndirection data augmentation (CDDA). Experimental results show that our proposed\ndirectional-SpeechLlama effectively captures the relationship between textual\ncues and spatial audio, yielding strong performance in both speech recognition\nand source localization tasks.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.14973v1", "AI": {"title_translation": "定向思维：用于多说话者定向语音识别的语音大语言模型", "tldr": "本文提出了一种名为 directional-SpeechLlama 的新方法，利用智能眼镜的麦克风阵列实现定向语音识别、声源定位和旁听者串扰抑制，并通过 S-DOT 和 CDDA 技术增强模型对方向性的理解，在语音识别和声源定位任务中表现出色。", "motivation": "当前研究中，语音大语言模型（Speech LLMs）处理带有空间线索的多通道音频的能力仍是一个相对未被充分研究的领域。", "method": "本文提出了 directional-SpeechLlama，一种利用智能眼镜麦克风阵列实现定向语音识别、声源定位和旁听者串扰抑制的新方法。为增强模型对方向性的理解，提出了两种关键技术：序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）。", "result": "实验结果表明，所提出的 directional-SpeechLlama 有效地捕捉了文本线索与空间音频之间的关系，在语音识别和声源定位任务中均表现出强大的性能。", "conclusion": "directional-SpeechLlama 模型能够有效捕捉文本线索与空间音频之间的关系，并在语音识别和声源定位任务中取得良好性能。", "translation": "最近的研究表明，通过音频编码提示大语言模型（LLM）能够实现有效的语音识别能力。然而，语音大语言模型理解和处理带有空间线索的多通道音频的能力仍然是一个相对未被充分研究的领域。在这项工作中，我们提出了 directional-SpeechLlama，一种利用智能眼镜麦克风阵列实现定向语音识别、声源定位和旁听者串扰抑制的新方法。为了增强模型理解方向性的能力，我们提出了两种关键技术：序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）。实验结果表明，我们提出的 directional-SpeechLlama 有效地捕捉了文本线索与空间音频之间的关系，在语音识别和声源定位任务中均表现出强大的性能。", "summary": "本文介绍了一种名为 directional-SpeechLlama 的新型语音大语言模型，旨在解决多通道音频中空间线索处理的挑战。该模型利用智能眼镜的麦克风阵列，实现了定向语音识别、声源定位和旁听者串扰抑制。为提升模型对方向性的理解，研究者提出了序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）两种技术。实验证明，directional-SpeechLlama 能有效关联文本与空间音频，并在语音识别和声源定位任务中展现出卓越性能。", "keywords": "定向语音识别, 大语言模型, 智能眼镜, 声源定位, 空间音频", "comments": "本文的创新点在于将空间线索引入语音大语言模型，解决了多说话者场景下的定向语音识别和声源定位问题，并通过S-DOT和CDDA等独特训练方法提高了模型的方向性理解能力。这对于智能眼镜等可穿戴设备在复杂听觉环境下的应用具有重要意义。"}}
{"id": "2506.14984", "title": "Extending Spike-Timing Dependent Plasticity to Learning Synaptic Delays", "authors": ["Marissa Dominijanni", "Alexander Ororbia", "Kenneth W. Regan"], "summary": "Synaptic delays play a crucial role in biological neuronal networks, where\ntheir modulation has been observed in mammalian learning processes. In the\nrealm of neuromorphic computing, although spiking neural networks (SNNs) aim to\nemulate biology more closely than traditional artificial neural networks do,\nsynaptic delays are rarely incorporated into their simulation. We introduce a\nnovel learning rule for simultaneously learning synaptic connection strengths\nand delays, by extending spike-timing dependent plasticity (STDP), a Hebbian\nmethod commonly used for learning synaptic weights. We validate our approach by\nextending a widely-used SNN model for classification trained with unsupervised\nlearning. Then we demonstrate the effectiveness of our new method by comparing\nit against another existing methods for co-learning synaptic weights and delays\nas well as against STDP without synaptic delays. Results demonstrate that our\nproposed method consistently achieves superior performance across a variety of\ntest scenarios. Furthermore, our experimental results yield insight into the\ninterplay between synaptic efficacy and delay.", "comment": "Repository containing the source code used to generate the results is\n  available at: https://github.com/mdominijanni/dsstdp-results", "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.14984v1", "AI": {"title_translation": "将尖峰时间依赖可塑性扩展到学习突触延迟", "tldr": "本文提出了一种新的学习规则，通过扩展尖峰时间依赖可塑性（STDP），同时学习尖峰神经网络（SNNs）中的突触连接强度和延迟，并在分类任务中展示了优越的性能。", "motivation": "生物神经网络中突触延迟的调节在哺乳动物的学习过程中起着关键作用，但脉冲神经网络（SNNs）在模拟生物学时很少将突触延迟纳入其仿真中。", "method": "通过扩展尖峰时间依赖可塑性（STDP），提出了一种新的学习规则，用于同时学习突触连接强度和延迟。该方法通过扩展一个广泛使用的无监督学习分类SNN模型进行验证，并与现有的协同学习突触权重和延迟的方法以及不带突触延迟的STDP进行了比较。", "result": "所提出的方法在各种测试场景中始终实现卓越的性能。此外，实验结果揭示了突触效能和延迟之间相互作用的深刻见解。", "conclusion": "通过扩展STDP来同时学习突触连接强度和延迟的新学习规则，能够显著提高脉冲神经网络的性能，并为理解突触效能和延迟的相互作用提供了见解。", "translation": "突触延迟在生物神经网络中扮演着关键角色，在哺乳动物的学习过程中观察到它们的调节作用。在神经形态计算领域，尽管脉冲神经网络（SNNs）旨在比传统人工神经网络更紧密地模拟生物学，但突触延迟很少被纳入它们的仿真中。我们引入了一种新颖的学习规则，通过扩展尖峰时间依赖可塑性（STDP）（一种常用于学习突触权重的赫布方法），同时学习突触连接强度和延迟。我们通过扩展一个广泛使用的无监督学习分类SNN模型来验证我们的方法。然后，我们通过将其与另一种现有的协同学习突触权重和延迟的方法以及不带突触延迟的STDP进行比较，证明了我们新方法的有效性。结果表明，我们提出的方法在各种测试场景中始终实现卓越的性能。此外，我们的实验结果揭示了突触效能和延迟之间相互作用的深刻见解。", "summary": "本研究提出了一种新颖的学习规则，通过扩展尖峰时间依赖可塑性（STDP），使得脉冲神经网络（SNNs）能够同时学习突触连接强度和延迟。该方法旨在弥补当前SNNs在模拟生物真实性时对突触延迟的忽视。实验结果表明，与现有方法和不考虑延迟的STDP相比，该新规则在多种测试场景下均能实现卓越的性能，并为理解突触效能与延迟的相互作用提供了新的视角。", "keywords": "STDP, 突触延迟, 脉冲神经网络, 学习规则, 神经形态计算", "comments": "本文的创新点在于将STDP扩展到不仅学习突触权重，还学习突触延迟，这显著提升了SNN的生物真实性和性能。这种方法对于神经形态计算领域具有重要意义，因为它为构建更高效、更仿生的SNN模型提供了新的途径。该研究的局限性可能在于其验证主要集中在分类任务上，未来可以探索其在其他复杂任务中的表现。"}}
{"id": "2506.14864", "title": "pycnet-audio: A Python package to support bioacoustics data processing", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that\nleverages recent improvements in purpose-made automated recording units (ARUs).\nThe general approach is to deploy ARUs in the field to record on a programmed\nschedule for extended periods (weeks or months), after which the audio data are\nretrieved. These data must then be processed, typically either by measuring or\nanalyzing characteristics of the audio itself (e.g. calculating acoustic\nindices), or by searching for some signal of interest within the recordings,\ne.g. vocalizations or other sounds produced by some target species,\nanthropogenic or environmental noise, etc. In the latter case, some method is\nrequired to locate the signal(s) of interest within the audio. While very small\ndatasets can simply be searched manually, even modest projects can produce\naudio datasets on the order of 105 hours of recordings, making manual review\nimpractical and necessitating some form of automated detection. pycnet-audio\n(Ruff 2024) is intended to provide a practical processing workflow for acoustic\ndata, built around the PNW-Cnet model, which was initially developed by the\nU.S. Forest Service to support population monitoring of northern spotted owls\n(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins\n2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of\nca. 80 forest wildlife species and numerous forms of anthropogenic and\nenvironmental noise (Ruff et al. 2021, 2023).", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.14864v1", "AI": {"title_translation": "pycnet-audio：一个支持生物声学数据处理的Python包", "tldr": "pycnet-audio是一个Python包，它基于PNW-Cnet模型，旨在为野生动物研究中的大规模被动声学监测数据提供实用的自动化处理工作流程，解决了手动分析大量录音的难题。", "motivation": "被动声学监测在野生动物研究中日益普及，但即使是中等规模的项目也会产生数万小时的录音数据，使得手动审查变得不切实际，因此需要某种形式的自动化检测方法来处理这些数据。", "method": "pycnet-audio是一个Python包，它围绕PNW-Cnet模型构建。PNW-Cnet最初由美国林务局开发，用于支持北部斑点猫头鹰和其他森林猫头鹰的种群监测，现已扩展到可检测大约80种森林野生动物的叫声以及多种人为和环境噪声。", "result": "pycnet-audio旨在提供一个实用的声学数据处理工作流程。其核心PNW-Cnet模型已扩展到能够检测约80种森林野生动物的叫声以及多种人为和环境噪声。", "conclusion": "pycnet-audio包提供了一个基于PNW-Cnet模型的实用工作流程，旨在解决大规模生物声学数据处理的挑战，实现对野生动物叫声和环境噪声的自动化检测。", "translation": "被动声学监测是野生动物研究中一种新兴的方法，它利用了专用自动化录音单元（ARU）的最新改进。一般方法是在野外部署ARU，按预设时间表进行数周或数月的长时间录音，然后检索音频数据。这些数据随后必须进行处理，通常是通过测量或分析音频本身的特性（例如计算声学指数），或者在录音中搜索感兴趣的信号，例如某些目标物种发出的叫声或其他声音、人为或环境噪声等。在后一种情况下，需要某种方法来定位音频中感兴趣的信号。虽然非常小的数据集可以手动搜索，但即使是适度的项目也能产生大约10^5小时的录音数据，这使得手动审查变得不切实际，并需要某种形式的自动化检测。pycnet-audio（Ruff 2024）旨在为声学数据提供一个实用的处理工作流程，它围绕PNW-Cnet模型构建，该模型最初由美国林务局开发，用于支持北部斑点猫头鹰（Strix occidentalis caurina）和其他森林猫头鹰的种群监测（Lesmeister和Jenkins 2022；Ruff 等人 2020）。PNW-Cnet已扩展到可检测大约80种森林野生动物的叫声以及多种人为和环境噪声（Ruff 等人 2021，2023）。", "summary": "pycnet-audio是一个Python包，旨在为生物声学数据处理提供一个实用的自动化工作流程。该包围绕PNW-Cnet模型构建，该模型最初用于监测北部斑点猫头鹰，现已扩展到可检测约80种森林野生动物的叫声以及多种人为和环境噪声。它解决了被动声学监测中处理大规模录音数据（可达10^5小时）手动审查不切实际的问题，为野生动物研究提供了高效的数据分析工具。", "keywords": "生物声学, Python包, 被动声学监测, PNW-Cnet, 自动化检测", "comments": "该论文介绍的pycnet-audio包解决了生物声学领域的一个关键痛点：如何高效处理大规模的被动声学监测数据。其创新性在于将一个成熟的PNW-Cnet检测模型封装成易于使用的Python包，为研究人员提供了一个实用的自动化工作流程。这对于加速野生动物种群监测和生态研究具有重要意义，尤其是在数据量巨大的背景下，自动化处理是必不可少的。"}}
{"id": "2506.14900", "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "authors": ["Imane Guellil", "Salomé Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.", "comment": "Accepted and will be published at ACL2025 (main conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14900v1", "AI": {"title_translation": "出院总结中不良事件提取：新数据集、标注方案和初步发现", "tldr": "本文介绍了一个用于从老年患者出院总结中提取不良事件的手动标注语料库，解决了未充分代表人群和复杂标注的挑战，结果显示在粗粒度任务上表现良好，但在细粒度事件和罕见事件上仍面临挑战。", "motivation": "临床NLP资源中老年患者群体常被低估，且现有工作很少解决不连续和重叠实体等挑战。", "method": "构建了一个手动标注的老年患者出院总结不良事件语料库，包含14种不良事件及上下文属性。其标注方案支持不连续和重叠实体。使用FlairNLP评估了多种模型，涵盖细粒度、粗粒度及带否定词的粗粒度三种标注粒度。", "result": "变压器模型（如BERT-cased）在文档级粗粒度提取上表现出色（F1 = 0.943），但在细粒度实体级任务上性能显著下降（F1 = 0.675），尤其对于罕见事件和复杂属性。", "conclusion": "尽管在粗粒度任务上表现良好，但在检测未充分代表的不良事件和捕捉细微临床语言方面仍存在重大挑战。该数据集可作为不良事件提取方法的可靠基准，并支持未来的跨数据集泛化。", "translation": "在这项工作中，我们提出了一个用于从老年患者出院总结中提取不良事件（AE）的手动标注语料库，老年患者群体在临床自然语言处理资源中往往未被充分代表。该数据集包括14种具有临床意义的不良事件，如跌倒、谵妄和颅内出血，以及否定、诊断类型和住院发生等上下文属性。独特的是，该标注方案支持不连续和重叠实体，解决了以往工作中很少处理的挑战。我们使用FlairNLP评估了多个模型，涵盖三种标注粒度：细粒度、粗粒度以及带否定的粗粒度。尽管基于Transformer的模型（例如BERT-cased）在文档级粗粒度提取上取得了强大的性能（F1 = 0.943），但在细粒度实体级任务（例如F1 = 0.675）上性能显著下降，特别是对于罕见事件和复杂属性。这些结果表明，尽管取得了高分，但在检测未充分代表的不良事件和捕捉细微的临床语言方面仍然存在重大挑战。该数据集在可信研究环境（TRE）中开发，可通过DataLoch按需提供，可作为评估不良事件提取方法的强大基准，并支持未来的跨数据集泛化。", "summary": "本文介绍了一个新的手动标注数据集，用于从老年患者出院总结中提取不良事件（AE），该群体在临床NLP中通常未被充分代表。该数据集包含14种不良事件，并对不连续和重叠实体进行了独特标注。使用FlairNLP模型进行的实验表明，在粗粒度AE提取方面表现出色（F1=0.943），但细粒度任务和罕见事件仍面临重大挑战（F1=0.675）。该数据集可作为未来研究的基准。", "keywords": "不良事件提取, 临床NLP, 出院总结, 数据集, 标注方案", "comments": "本文的创新之处在于构建了一个针对临床NLP中未充分代表的老年患者群体的不良事件提取新数据集，并采用了支持不连续和重叠实体的独特标注方案。其重要性在于填补了临床不良事件提取领域的空白，并提供了一个可靠的基准。然而，研究也揭示了在细粒度任务、罕见事件和复杂属性方面提取性能显著下降的局限性。"}}
{"id": "2506.15028", "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices", "authors": ["Gargi Mitra", "Mohammadreza Hallajiyan", "Inji Kim", "Athish Pranav Dharmalingam", "Mohammed Elnawawy", "Shahrear Iqbal", "Karthik Pattabiraman", "Homa Alemzadeh"], "summary": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients.", "comment": "32 pages, 6 figures, 6 tables", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15028v1", "AI": {"title_translation": "机器学习医疗设备中的系统理论和数据驱动安全分析", "tldr": "论文关注机器学习医疗设备面临的网络安全风险，特别是对患者安全的影响，并提出一套工具和技术，帮助在上市前进行全面的风险评估，以确保设备设计阶段的安全性。", "motivation": "随着AI/ML在医疗设备中的广泛应用，随之而来的复杂模型、互联互通和底层技术漏洞带来了严重网络安全风险，可能导致ML模型错误预测，从而对患者构成重大安全威胁。因此，迫切需要在设备设计阶段（上市前）解决这些网络安全挑战。", "method": "首先，通过分析公开的设备召回、不良事件及已知漏洞数据，以了解AI/ML医疗设备的威胁态势及其对患者安全的影响。在此基础上，提出了一套由作者设计的工具和技术，旨在协助安全分析师进行全面的上市前风险评估。", "result": "论文分析了AI/ML医疗设备的威胁格局及其对患者安全的影响，并引入了一套工具和技术，旨在帮助安全分析师进行全面的上市前风险评估。", "conclusion": "论文强调了在AI/ML医疗设备设计初期融入网络安全作为核心原则的重要性，旨在赋能制造商确保这些设备对患者是安全的。", "translation": "人工智能/机器学习在医疗设备中的整合正在通过增强诊断和治疗设施迅速改变医疗保健。然而，这种进步也由于使用了复杂且通常不透明的模型、广泛的互联互通、与第三方外围设备的互操作性、互联网连接以及底层技术中的漏洞而带来了严重网络安全风险。这些因素导致了广泛的攻击面，并使威胁预防、检测和缓解变得具有挑战性。鉴于这些设备的高度安全关键性质，对这些设备的网络攻击可能导致机器学习模型错误预测，从而对患者构成重大安全风险。因此，从设计阶段开始确保这些设备的安全性至关重要。本文强调了在上市前阶段解决启用机器学习的医疗设备中网络安全挑战的紧迫性。我们首先分析了公开可用的设备召回和不良事件数据，以及已知漏洞，以了解人工智能/机器学习医疗设备的威胁态势及其对患者安全的影响。在此分析的基础上，我们引入了一套由我们设计的工具和技术，以协助安全分析师进行全面的上市前风险评估。我们的工作旨在赋能制造商将网络安全作为人工智能/机器学习医疗设备的核心设计原则嵌入，从而使其对患者安全。", "summary": "本文探讨了AI/ML在医疗设备应用中引入的网络安全风险及其对患者安全的潜在威胁。鉴于此类设备的高度安全关键性，作者强调了在设备上市前阶段解决网络安全问题的紧迫性。研究通过分析公开的设备召回、不良事件和已知漏洞数据来理解威胁格局，并在此基础上提出了一套新颖的工具和技术，旨在协助安全分析师进行全面的上市前风险评估，最终目标是帮助制造商将网络安全融入AI/ML医疗设备的设计中，确保患者安全。", "keywords": "机器学习医疗设备, 网络安全, 风险评估, 患者安全, 数据驱动", "comments": "这篇论文的创新点在于它结合了系统理论和数据驱动的方法来解决ML医疗设备的网络安全问题，并特别关注了上市前阶段的风险评估。其重要性体现在提出了具体的工具和技术来帮助制造商在设计初期嵌入网络安全，这对于确保患者安全至关重要。该研究通过分析真实世界的召回和漏洞数据，使得其提出的解决方案更具实践指导意义。"}}
{"id": "2506.15207", "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Jimmy Cao", "Ryszard Kowalczyk"], "summary": "The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised\nEarth Observation (EO) missions, addressing challenges in climate monitoring,\ndisaster management, and more. However, autonomous coordination in\nmulti-satellite systems remains a fundamental challenge. Traditional\noptimisation approaches struggle to handle the real-time decision-making\ndemands of dynamic EO missions, necessitating the use of Reinforcement Learning\n(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we\ninvestigate RL-based autonomous EO mission planning by modelling\nsingle-satellite operations and extending to multi-satellite constellations\nusing MARL frameworks. We address key challenges, including energy and data\nstorage limitations, uncertainties in satellite observations, and the\ncomplexities of decentralised coordination under partial observability. By\nleveraging a near-realistic satellite simulation environment, we evaluate the\ntraining stability and performance of state-of-the-art MARL algorithms,\nincluding PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can\neffectively balance imaging and resource management while addressing\nnon-stationarity and reward interdependency in multi-satellite coordination.\nThe insights gained from this study provide a foundation for autonomous\nsatellite operations, offering practical guidelines for improving policy\nlearning in decentralised EO missions.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15207v1", "AI": {"title_translation": "多智能体强化学习在自主多卫星对地观测中的应用：一个现实案例研究", "tldr": "多智能体强化学习（MARL）能够有效协调多卫星对地观测任务，在接近真实的模拟环境中平衡成像与资源管理，同时解决能量限制和部分可观测性等挑战。", "motivation": "传统的优化方法难以处理动态对地观测（EO）任务中多卫星系统的实时决策需求，因此需要强化学习（RL）和多智能体强化学习（MARL）来解决自主协调、能量/数据存储限制、观测不确定性以及部分可观测性下分散协调等关键挑战。", "method": "本文通过对单卫星操作进行建模，并使用MARL框架扩展到多卫星星座，研究了基于RL的自主EO任务规划。研究在接近真实的卫星模拟环境中评估了PPO、IPPO、MAPPO和HAPPO等最先进的MARL算法的训练稳定性和性能，解决了能量和数据存储限制、卫星观测不确定性以及部分可观测性下分散协调的复杂性。", "result": "结果表明，MARL能够有效地平衡成像和资源管理，同时解决多卫星协调中的非平稳性和奖励相互依赖性。", "conclusion": "这项研究为自主卫星操作奠定了基础，并为改进分散式对地观测任务中的策略学习提供了实用指导。", "translation": "低地球轨道 (LEO) 卫星的指数级增长彻底改变了对地观测 (EO) 任务，解决了气候监测、灾害管理等方面的挑战。然而，多卫星系统中的自主协调仍然是一个基本挑战。传统的优化方法难以处理动态 EO 任务的实时决策需求，因此需要使用强化学习 (RL) 和多智能体强化学习 (MARL)。在本文中，我们通过对单卫星操作进行建模并使用 MARL 框架扩展到多卫星星座，研究了基于 RL 的自主 EO 任务规划。我们解决了关键挑战，包括能量和数据存储限制、卫星观测的不确定性以及部分可观测性下分散协调的复杂性。通过利用近乎真实的卫星模拟环境，我们评估了最先进的 MARL 算法（包括 PPO、IPPO、MAPPO 和 HAPPO）的训练稳定性和性能。我们的结果表明，MARL 可以有效地平衡成像和资源管理，同时解决多卫星协调中的非平稳性和奖励相互依赖性。这项研究获得的见解为自主卫星操作奠定了基础，为改进分散式 EO 任务中的策略学习提供了实用指导。", "summary": "本文探讨了多智能体强化学习（MARL）在自主多卫星对地观测（EO）任务规划中的应用。针对传统优化方法在动态EO任务中的局限性，论文通过RL建模单卫星操作并扩展至MARL框架下的多卫星星座。研究解决了能量和数据存储限制、观测不确定性以及部分可观测性下分散协调等关键挑战。通过一个接近真实的卫星模拟环境，论文评估了多种MARL算法（PPO、IPPO、MAPPO、HAPPO）的性能，证明了MARL在平衡成像与资源管理以及处理复杂多卫星协调问题方面的有效性。研究结果为自主卫星操作和分散式EO任务中的策略学习提供了实用指导。", "keywords": "多智能体强化学习, 对地观测, 卫星协调, 自主系统, 资源管理", "comments": "该论文通过将先进的MARL技术应用于高度复杂且现实的自主多卫星对地观测问题，做出了重要贡献。其关注资源限制、不确定性和部分可观测性等实际挑战，并结合在接近真实模拟环境中的评估，增强了其实用相关性，为未来的自主空间任务奠定了坚实基础。对不同MARL算法的比较研究也具有重要价值。"}}
{"id": "2506.15084", "title": "An Empirical Study of Bugs in Data Visualization Libraries", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "summary": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.", "comment": "Proc. ACM Softw. Eng. 2, FSE", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15084v1", "AI": {"title_translation": "数据可视化库中错误的实证研究", "tldr": "首次对数据可视化库中的错误进行了全面分析，识别了普遍存在的图表不准确问题，并探讨了使用视觉语言模型（VLM）进行检测的可行性。", "motivation": "数据可视化库在数据呈现、分析和应用开发中至关重要，但错误的视觉呈现会误导用户并影响决策。视觉错误可能不会导致崩溃，但会隐蔽地误导用户。因此，了解数据可视化库中错误的独特特征对于研究人员和开发人员检测和修复错误至关重要。", "method": "本研究对从五个广泛使用的库中收集的564个数据可视化库错误进行了首次全面分析。研究系统地分析了这些错误的症状和根本原因，并提供了详细的分类。此外，还识别了触发此类错误的八个关键步骤和两个特定于数据可视化库的测试预言。研究还探索了应用视觉语言模型（VLM）检测不正确/不准确图表的可行性。", "result": "研究发现，不正确/不准确的图表在数据可视化库中普遍存在，且不正确的图形计算是主要根源，这表明需要进一步的自动化测试方法。研究识别了触发此类错误的八个关键步骤和两个特定于数据可视化库的测试预言，这可能启发未来设计有效的自动化测试技术。视觉语言模型在错误检测中的有效性在29%到57%之间，取决于提示，并且在提示中添加更多信息不一定能提高有效性。", "conclusion": "不正确的图形计算是数据可视化库中不准确图表的主要原因，这凸显了开发更先进的自动化测试方法的必要性。本研究识别的错误触发步骤和测试预言为未来设计有效的自动化测试技术提供了基础。尽管视觉语言模型在错误检测方面表现出一定的潜力，但其有效性受提示内容影响较大。", "translation": "数据可视化（DataViz）库在演示、数据分析和应用程序开发中发挥着关键作用，这突显了其在将数据转换为视觉表示时的准确性至关重要。不正确的可视化会对用户体验产生不利影响，扭曲信息传达，并影响用户的感知和决策过程。这些库中的视觉错误可能特别隐蔽，因为它们可能不会导致像崩溃这样的明显错误，而是通过图形方式误导用户对底层数据的理解，从而导致错误的决策。因此，深入了解DataViz库中错误的独特特征对于研究人员和开发人员检测和修复DataViz库中的错误至关重要。\n本研究首次对DataViz库中的错误进行了全面分析，检查了从五个广泛使用的库中收集的564个错误。我们的研究系统地分析了它们的症状和根本原因，并提供了详细的分类。我们发现不正确/不准确的图表在DataViz库中普遍存在，不正确的图形计算是主要根源，这使得DataViz库需要进一步的自动化测试方法。此外，我们识别了触发此类错误的八个关键步骤和两个特定于DataViz库的测试预言，这可能启发未来研究设计有效的自动化测试技术。此外，随着视觉语言模型（VLMs）的最新进展，我们探索了应用这些模型来检测不正确/不准确图表的可行性。结果表明，VLM在错误检测中的有效性从29%到57%不等，取决于提示，并且在提示中添加更多信息不一定能提高有效性。更多发现可在我们的手稿中找到。", "summary": "本研究首次对数据可视化库中的错误进行了全面实证分析，收集了来自五个常用库的564个错误样本。研究系统地分析了错误的症状和根本原因，并构建了详细的错误分类体系。结果显示，不正确/不准确的图表在数据可视化库中普遍存在，且主要根源在于不正确的图形计算。为此，论文提出了八个触发此类错误的关键步骤和两个特定于数据可视化库的测试预言，旨在为未来的自动化测试方法提供指导。此外，研究还探索了利用视觉语言模型（VLM）检测不准确图表的可行性，发现其有效性在29%至57%之间波动，且受提示内容影响。", "keywords": "数据可视化, 错误分析, 自动化测试, 视觉语言模型, 图表不准确", "comments": "本研究首次对数据可视化库中的错误进行了大规模的实证分析，填补了该领域研究的空白，具有重要的创新性。论文不仅识别了错误的普遍性和隐蔽性，还深入分析了其根本原因，并提出了触发步骤和测试预言，为未来开发更有效的自动化测试工具和修复策略奠定了基础。此外，探索视觉语言模型在错误检测中的应用是前瞻性的尝试，尽管目前效果有限，但为跨模态技术在软件测试中的应用提供了初步见解和研究方向。"}}
{"id": "2506.14928", "title": "On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing", "authors": ["Dyk Chung Nguyen", "Thomas Chetaille", "Yuan-Hang Zhang", "Yuriy V. Pershin", "Massimiliano Di Ventra"], "summary": "Digital memcomputing machines (DMMs) have been designed to solve complex\ncombinatorial optimization problems. Since DMMs are fundamentally classical\ndynamical systems, their ordinary differential equations (ODEs) can be\nefficiently simulated on modern computers. This provides a unique platform to\nstudy their performance under various conditions. An aspect that has received\nlittle attention so far is how their performance is affected by the numerical\nerrors in the solution of their ODEs and the physical noise they would be\nnaturally subject to if built in hardware. Here, we analyze these two aspects\nin detail by varying the integration time step (numerical noise) and adding\nstochastic perturbations (physical noise) into the equations of DMMs. We are\nparticularly interested in understanding how noise induces a chaotic transition\nthat marks the shift from successful problem-solving to failure in these\nsystems. Our study includes an analysis of power spectra and Lyapunov exponents\ndepending on the noise strength. The results reveal a correlation between the\ninstance solvability and the sign of the ensemble averaged mean largest\nLyapunov exponent. Interestingly, we find a regime in which DMMs with positive\nmean largest Lyapunov exponents still exhibit solvability. Furthermore, the\npower spectra provide additional information about our system by distinguishing\nbetween regular behavior (peaks) and chaotic behavior (broadband spectrum).\nTherefore, power spectra could be utilized to control whether a DMM operates in\nthe optimal dynamical regime. Overall, we find that the qualitative effects of\nnumerical and physical noise are mostly similar, despite their fundamentally\ndifferent origin.", "comment": null, "cate": "nlin.CD", "url": "http://arxiv.org/abs/2506.14928v1", "AI": {"title_translation": "数字忆阻计算中噪声诱导混沌导致的可解-不可解转变", "tldr": "本文研究了数值和物理噪声如何影响数字忆阻计算机器 (DMMs)，导致从问题解决到失败的混沌转变，并探讨了可解性与李雅普诺夫指数和功率谱之间的相关性。", "motivation": "数字忆阻计算机器 (DMMs) 被设计用于解决复杂的组合优化问题。然而，其在常微分方程 (ODEs) 求解中的数值误差以及在硬件中可能遇到的物理噪声对其性能的影响，迄今为止鲜有关注。本文旨在详细分析这两种噪声如何诱导混沌转变，从而导致系统从成功解决问题转向失败。", "method": "研究通过改变积分时间步长（模拟数值噪声）和在 DMMs 方程中添加随机扰动（模拟物理噪声）来分析噪声的影响。研究方法包括对功率谱和李雅普诺夫指数进行分析，以探究它们随噪声强度的变化。", "result": "1. 实例可解性与系综平均最大李雅普诺夫指数的符号之间存在相关性。\n2. 在特定范围内，即使 DMMs 具有正的平均最大李雅普诺夫指数，它们仍然表现出可解性。\n3. 功率谱能够区分规则行为（峰值）和混沌行为（宽带谱），这表明功率谱可用于控制 DMMs 是否在最佳动力学状态下运行。\n4. 数值噪声和物理噪声的定性影响大体相似，尽管它们的根本来源不同。", "conclusion": "噪声在数字忆阻计算机器 (DMMs) 中诱导混沌转变，标志着从成功解决问题到失败的转变。功率谱可以用于控制 DMMs 的最佳操作状态。数值噪声和物理噪声的影响在定性上是相似的。", "translation": "数字忆阻计算机器 (DMMs) 被设计用于解决复杂的组合优化问题。由于 DMMs 本质上是经典的动力学系统，它们的常微分方程 (ODEs) 可以在现代计算机上高效模拟。这提供了一个独特的平台来研究它们在各种条件下的性能。迄今为止，一个很少受到关注的方面是其 ODEs 求解中的数值误差以及如果构建为硬件时它们自然会受到的物理噪声如何影响其性能。在这里，我们通过改变积分时间步长（数值噪声）和在 DMMs 方程中添加随机扰动（物理噪声）来详细分析这两个方面。我们特别感兴趣的是了解噪声如何诱导混沌转变，这标志着这些系统中从成功解决问题到失败的转变。我们的研究包括分析功率谱和李雅普诺夫指数，具体取决于噪声强度。结果揭示了实例可解性与系综平均最大李雅普诺夫指数符号之间的相关性。有趣的是，我们发现了一个在正平均最大李雅普诺夫指数下 DMMs 仍然表现出可解性的区域。此外，功率谱通过区分规则行为（峰值）和混沌行为（宽带谱）提供了关于我们系统的额外信息。因此，功率谱可以用于控制 DMM 是否在最佳动力学状态下运行。总的来说，我们发现数值噪声和物理噪声的定性影响大体相似，尽管它们的根本来源不同。", "summary": "本文研究了数值和物理噪声对数字忆阻计算机器 (DMMs) 的影响，DMMs 旨在解决组合优化问题。通过分析不同噪声水平下的李雅普诺夫指数和功率谱，研究揭示了噪声如何诱导从问题解决到失败的混沌转变。结果发现可解性与最大李雅普诺夫指数之间存在相关性，并识别出即使在李雅普诺夫指数为正的情况下 DMMs 仍能保持可解性的区域。论文建议利用功率谱来控制 DMMs 的最佳操作状态，并指出两种噪声类型具有相似的定性影响。", "keywords": "数字忆阻计算机器, 噪声, 混沌, 李雅普诺夫指数, 功率谱, 可解性", "comments": "该论文强调了 DMMs 实际应用中对噪声鲁棒性的关键方面。发现 DMMs 即使在特定范围内具有正的李雅普诺夫指数仍然可解，这一点特别有趣，表明混沌与计算之间存在比直觉更复杂的关系。提出使用功率谱进行操作控制是一个实用且创新的贡献。"}}
{"id": "2506.14777", "title": "WebXAII: an open-source web framework to study human-XAI interaction", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "Sébastien Harispe"], "summary": "This article introduces WebXAII, an open-source web framework designed to\nfacilitate research on human interaction with eXplainable Artificial\nIntelligence (XAI) systems. The field of XAI is rapidly expanding, driven by\nthe growing societal implications of the widespread adoption of AI (and in\nparticular machine learning) across diverse applications. Researchers who study\nthe interaction between humans and XAI techniques typically develop ad hoc\ninterfaces in order to conduct their studies. These interfaces are usually not\nshared alongside the results of the studies, which limits their reusability and\nthe reproducibility of experiments. In response, we design and implement\nWebXAII, a web-based platform that can embody full experimental protocols,\nmeaning that it can present all aspects of the experiment to human participants\nand record their responses. The experimental protocols are translated into a\ncomposite architecture of generic views and modules, which offers a lot of\nflexibility. The architecture is defined in a structured configuration file, so\nthat protocols can be implemented with minimal programming skills. We\ndemonstrate that WebXAII can effectively embody relevant protocols, by\nreproducing the protocol of a state-of-the-art study of the literature. The\nframework is available at https://github.com/PAJEAN/WebXAII.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14777v1", "AI": {"title_translation": "WebXAII：一个用于研究人机交互的开源Web框架", "tldr": "WebXAII是一个开源Web框架，旨在促进人与可解释AI（XAI）系统交互的研究，解决现有研究中界面复用性差和实验再现性低的问题。", "motivation": "现有的XAI人机交互研究通常开发一次性接口，这些接口不共享，限制了其可重用性和实验的再现性。", "method": "作者设计并实现了一个名为WebXAII的开源Web框架。该平台能够承载完整的实验协议，通过通用视图和模块的复合架构实现灵活性，并通过结构化配置文件实现协议的低编程技能实现。", "result": "作者通过复现一篇最先进文献中的实验协议，证明了WebXAII能够有效地承载相关协议。", "conclusion": "WebXAII提供了一个开放源代码的Web框架，用于研究人类与可解释人工智能系统的互动，解决了实验接口复用性和再现性的问题。", "translation": "本文介绍了WebXAII，一个开源的Web框架，旨在促进人类与可解释人工智能（XAI）系统交互的研究。XAI领域正在迅速扩展，这得益于AI（特别是机器学习）在各种应用中广泛采用所带来的日益增长的社会影响。研究人类与XAI技术之间交互的研究人员通常会开发专门的接口来进行他们的研究。这些接口通常不会与研究结果一起共享，这限制了它们的复用性和实验的再现性。作为回应，我们设计并实现了WebXAII，一个基于Web的平台，可以承载完整的实验协议，这意味着它可以向参与者展示实验的所有方面并记录他们的响应。实验协议被转换为通用视图和模块的复合架构，这提供了很大的灵活性。该架构在结构化配置文件中定义，因此可以用最少的编程技能实现协议。我们通过复现一篇文献中最先进研究的协议，证明了WebXAII可以有效地承载相关协议。该框架可在https://github.com/PAJEAN/WebXAII获取。", "summary": "本文介绍了WebXAII，一个旨在促进人与可解释人工智能（XAI）系统交互研究的开源Web框架。针对当前XAI人机交互研究中，定制化接口复用性差和实验再现性低的问题，WebXAII提供了一个可承载完整实验协议的Web平台。该平台采用灵活的复合架构，并通过结构化配置文件简化协议实现。作者通过复现现有研究协议，证明了WebXAII的有效性，为XAI人机交互研究提供了可复用且易于操作的工具。", "keywords": "WebXAII, 开源框架, 人机交互, 可解释人工智能, 实验再现性", "comments": "WebXAII的创新之处在于它提供了一个标准化的、开源的Web框架，解决了XAI人机交互研究中长期存在的实验接口复用性低和实验再现性差的问题。通过提供一个灵活且低编程要求的平台，它有望显著降低研究门槛，加速该领域的研究进展。其开放源代码的特性也鼓励了社区协作和进一步开发。"}}
{"id": "2506.14836", "title": "Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse", "authors": ["Mark M. Bailey", "Mark I. Heiligman"], "summary": "How can we detect when global events fundamentally reshape public discourse?\nThis study introduces a topological framework for identifying structural change\nin media narratives using persistent homology. Drawing on international news\narticles surrounding major events - including the Russian invasion of Ukraine\n(Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol\ninsurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we\nconstruct daily co-occurrence graphs of noun phrases to trace evolving\ndiscourse. Each graph is embedded and transformed into a persistence diagram\nvia a Vietoris-Rips filtration. We then compute Wasserstein distances and\npersistence entropies across homological dimensions to capture semantic\ndisruption and narrative volatility over time. Our results show that major\ngeopolitical and social events align with sharp spikes in both H0 (connected\ncomponents) and H1 (loops), indicating sudden reorganization in narrative\nstructure and coherence. Cross-correlation analyses reveal a typical lag\npattern in which changes to component-level structure (H0) precede higher-order\nmotif shifts (H1), suggesting a bottom-up cascade of semantic change. An\nexception occurs during the Russian invasion of Ukraine, where H1 entropy leads\nH0, possibly reflecting top-down narrative framing before local discourse\nadjusts. Persistence entropy further distinguishes tightly focused from diffuse\nnarrative regimes. These findings demonstrate that persistent homology offers a\nmathematically principled, unsupervised method for detecting inflection points\nand directional shifts in public attention - without requiring prior knowledge\nof specific events. This topological approach advances computational social\nscience by enabling real-time detection of semantic restructuring during\ncrises, protests, and information shocks.", "comment": "23 pages", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.14836v1", "AI": {"title_translation": "通过持久结构检测叙事转变：对媒体话语的拓扑分析", "tldr": "本研究提出一种基于持久同调的拓扑框架，用于无监督地检测重大全球事件如何重塑媒体叙事结构和公共关注点。", "motivation": "如何检测全球事件何时根本性地重塑公共话语。", "method": "本研究引入一个拓扑框架，使用持久同调来识别媒体叙事中的结构变化。通过构建每日名词短语共现图，并将其嵌入转化为持久图，然后计算沃瑟斯坦距离和持久熵来捕捉语义中断和叙事波动。", "result": "重大的地缘政治和社会事件与H0（连接组件）和H1（循环）的急剧峰值一致，表明叙事结构和连贯性的突然重组。交叉关联分析揭示H0变化通常先于H1，但俄罗斯入侵乌克兰是一个例外，H1熵领先H0。持久熵还能区分焦点集中和分散的叙事模式。", "conclusion": "持久同调提供了一种数学上严谨、无监督的方法，用于检测公共关注的转折点和方向性转变，无需预先了解特定事件。这种拓扑方法通过在危机、抗议和信息冲击期间实现语义重组的实时检测，推动了计算社会科学的发展。", "translation": "如何检测全球事件何时根本性地重塑公共话语？本研究引入了一个拓扑框架，用于使用持久同调识别媒体叙事中的结构变化。我们以围绕重大事件——包括俄罗斯入侵乌克兰（2022年2月）、乔治·弗洛伊德谋杀案（2020年5月）、美国国会大厦骚乱（2021年1月）以及哈马斯领导的入侵以色列（2023年10月）——的国际新闻文章为基础，构建每日名词短语共现图，以追踪不断演变的话语。每个图都被嵌入并通过Vietoris-Rips过滤转换为持久图。然后，我们计算跨同调维度的沃瑟斯坦距离和持久熵，以捕捉随时间变化的语义中断和叙事波动。我们的结果表明，重大的地缘政治和社会事件与H0（连接组件）和H1（循环）的急剧峰值一致，表明叙事结构和连贯性的突然重组。交叉关联分析揭示了一种典型的滞后模式，其中组件级结构（H0）的变化先于更高阶主题（H1）的转变，这表明语义变化的自下而上级联。俄罗斯入侵乌克兰期间出现一个例外，H1熵领先H0，这可能反映了在局部话语调整之前自上而下的叙事框架。持久熵进一步区分了焦点集中和分散的叙事模式。这些发现表明，持久同调提供了一种数学上严谨、无监督的方法，用于检测公共关注的转折点和方向性转变——无需预先了解特定事件。这种拓扑方法通过在危机、抗议和信息冲击期间实现语义重组的实时检测，推动了计算社会科学的发展。", "summary": "本文提出一个基于持久同调的拓扑框架，用于无监督地检测媒体叙事中的结构性转变。研究利用国际新闻文章构建名词短语共现图，并通过计算沃瑟斯坦距离和持久熵来量化叙事波动。结果显示，重大事件与叙事结构（H0和H1）的急剧重组相关，且通常H0变化先于H1，但俄乌战争事件表现出H1领先的例外。该方法提供了一种无需先验知识即可实时检测公共话语中语义重构的有效工具。", "keywords": "持久同调, 叙事转变, 媒体话语, 拓扑分析, 语义重组", "comments": "该研究创新性地将拓扑数据分析（尤其是持久同调）应用于媒体话语分析，提供了一种数学上严谨且无监督的方法来识别叙事转变。其重要性在于能够实时检测危机、抗议和信息冲击中的语义重组，极大地推动了计算社会科学的发展。特别指出H0和H1的滞后模式以及俄乌战争的例外，为理解叙事演变提供了新的视角。"}}
{"id": "2506.15011", "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph\nConvolutional Network (GCN) integrated with a Deep Q-Network (DQN)\nreinforcement learning framework for improved interference coordination in\nmulti-cell, multi-channel networks. Unlike LDP's static priorities, our\napproach dynamically learns link priorities based on real-time traffic demand,\nnetwork topology, remaining transmission opportunities, and interference\npatterns. The GCN captures spatial dependencies, while the DQN enables adaptive\nscheduling decisions through reward-guided exploration. Simulation results show\nthat our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and\n175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN\nmodel demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over\nour previous CNN-based approach across the same configurations. These results\nunderscore the effectiveness of our GCN-DQN model in addressing complex URLLC\nrequirements with minimal overhead and superior network performance.", "comment": "This paper has been submitted to IEEE MASS 2025 on May 7, 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.15011v1", "AI": {"title_translation": "GCN驱动的强化学习在工业URLLC中实现概率实时保证", "tldr": "本文提出了一种结合GCN和DQN的强化学习框架，用于改进工业URLLC中的干扰协调，相较于传统LDP算法和之前的CNN方法，显著提升了SINR。", "motivation": "在大型工业无线网络中，确保超可靠、低延迟通信（URLLC）的包级通信质量至关重要。", "method": "本文通过引入一个结合图卷积网络（GCN）和深度Q网络（DQN）强化学习框架，增强了局部截止日期划分（LDP）算法。该方法根据实时流量需求、网络拓扑、剩余传输机会和干扰模式动态学习链路优先级。GCN用于捕获空间依赖性，而DQN通过奖励引导的探索实现自适应调度决策。", "result": "仿真结果显示，在三种网络配置下，GCN-DQN模型相对于LDP算法，平均SINR分别提高了179.6%、197.4%和175.2%。此外，GCN-DQN模型相对于之前基于CNN的方法，在相同配置下平均SINR分别提高了31.5%、53.0%和84.7%。", "conclusion": "这些结果强调了GCN-DQN模型在解决复杂URLLC需求方面的有效性，具有最小的开销和卓越的网络性能。", "translation": "确保包级通信质量对于大型工业无线网络中的超可靠、低延迟通信（URLLC）至关重要。我们通过引入一个结合图卷积网络（GCN）和深度Q网络（DQN）强化学习框架，增强了局部截止日期划分（LDP）算法，以改进多小区、多信道网络中的干扰协调。与LDP的静态优先级不同，我们的方法根据实时流量需求、网络拓扑、剩余传输机会和干扰模式动态学习链路优先级。GCN捕获空间依赖性，而DQN通过奖励引导的探索实现自适应调度决策。仿真结果显示，在三种网络配置下，我们的GCN-DQN模型相对于LDP算法，平均SINR分别提高了179.6%、197.4%和175.2%。此外，GCN-DQN模型相对于我们之前基于CNN的方法，在相同配置下平均SINR分别提高了31.5%、53.0%和84.7%。这些结果强调了GCN-DQN模型在解决复杂URLLC需求方面的有效性，具有最小的开销和卓越的网络性能。", "summary": "本文提出了一种基于GCN和DQN的强化学习框架，旨在解决工业URLLC中的干扰协调问题。该模型通过动态学习链路优先级，显著提升了多小区、多信道网络中的SINR性能，优于传统的LDP算法和基于CNN的先前方法，为复杂的URLLC需求提供了高效的解决方案。", "keywords": "URLLC, GCN, DQN, 强化学习, 干扰协调", "comments": "本文的创新点在于将GCN与DQN强化学习框架相结合，用于动态学习URLLC环境中的链路优先级，而非采用静态优先级。这种方法能够有效捕获空间依赖性，并通过奖励机制实现自适应调度，从而在复杂的工业无线网络中显著提升通信质量和网络性能，具有重要的实际应用价值。"}}
{"id": "2506.14857", "title": "Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired", "authors": ["Suman Raj", "Swapnil Padhi", "Ruchi Bhoot", "Prince Modi", "Yogesh Simmhan"], "summary": "Autonomous navigation by drones using onboard sensors combined with machine\nlearning and computer vision algorithms is impacting a number of domains,\nincluding agriculture, logistics, and disaster management. In this paper, we\nexamine the use of drones for assisting visually impaired people (VIPs) in\nnavigating through outdoor urban environments. Specifically, we present a\nperception-based path planning system for local planning around the\nneighborhood of the VIP, integrated with a global planner based on GPS and maps\nfor coarse planning. We represent the problem using a geometric formulation and\npropose a multi DNN based framework for obstacle avoidance of the UAV as well\nas the VIP. Our evaluations conducted on a drone human system in a university\ncampus environment verifies the feasibility of our algorithms in three\nscenarios; when the VIP walks on a footpath, near parked vehicles, and in a\ncrowded street.", "comment": "16 pages, 7 figures; Accepted as Late-Breaking Results at the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n  2023", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14857v1", "AI": {"title_translation": "面向视觉障碍者引导的无人机基于感知的避碰研究", "tldr": "该研究提出了一种基于感知的无人机避碰系统，用于辅助视障人士在户外导航。系统结合了局部感知规划和全局GPS规划，并通过多DNN框架实现无人机和视障者的避障。在大学校园环境中的评估验证了算法在多种场景下的可行性。", "motivation": "该研究旨在探索使用无人机辅助视障人士在户外城市环境中导航的可能性，特别是解决无人机在引导过程中与障碍物避碰的问题。", "method": "本文提出了一种基于感知的路径规划系统，用于视障者附近的局部规划，并与基于GPS和地图的全局规划器集成进行粗略规划。问题采用几何公式表示，并提出了一个基于多DNN的框架，用于无人机和视障者的障碍物避碰。", "result": "在大学校园环境中进行的无人机-人类系统评估验证了算法在三种场景下的可行性：视障者在人行道上行走、靠近停放车辆以及在拥挤街道中。", "conclusion": "该研究提出的算法在引导视障人士并在各种户外城市环境中避开碰撞方面是可行的。", "translation": "无人机利用机载传感器结合机器学习和计算机视觉算法进行自主导航，正在影响农业、物流和灾害管理等多个领域。在本文中，我们研究了无人机在辅助视障人士（VIPs）在户外城市环境中导航方面的应用。具体来说，我们提出了一种基于感知的路径规划系统，用于VIPs附近区域的局部规划，并与基于GPS和地图的全局规划器集成进行粗略规划。我们使用几何公式表示该问题，并提出了一个基于多DNN的框架，用于无人机以及VIPs的障碍物避碰。我们在大学校园环境中进行的无人机-人类系统评估验证了我们的算法在三种场景下的可行性：当VIPs在人行道上行走时，靠近停放的车辆时，以及在拥挤的街道中时。", "summary": "本论文探讨了使用无人机辅助视障人士在户外城市环境中导航的应用。研究提出了一种结合局部感知路径规划和全局GPS规划的系统，并采用多深度神经网络（DNN）框架实现无人机及其引导的视障人士的障碍物避碰。在大学校园进行的实验评估验证了该算法在人行道、停放车辆附近和拥挤街道等多种场景下的可行性。", "keywords": "无人机, 避碰, 视障者, 感知规划, 多DNN", "comments": "该研究的创新之处在于将基于感知的碰撞避免与全局规划相结合，以实现对视障人士的引导，并采用了多DNN方法。在多种实际场景中的评估突显了其实用潜力，为辅助视障人群的智能导航提供了新的方向。"}}
{"id": "2506.15052", "title": "MIMO Systems Aided by Microwave Linear Analog Computers: Capacity-Achieving Architectures with Reduced Circuit Complexity", "authors": ["Matteo Nerini", "Bruno Clerckx"], "summary": "To meet the demands of future wireless networks, antenna arrays must scale\nfrom massive multiple-input multiple-output (MIMO) to gigantic MIMO, involving\neven larger numbers of antennas. To address the hardware and computational cost\nof gigantic MIMO, several strategies are available that shift processing from\nthe digital to the analog domain. Among them, microwave linear analog computers\n(MiLACs) offer a compelling solution by enabling fully analog beamforming\nthrough reconfigurable microwave networks. Prior work has focused on\nfully-connected MiLACs, whose ports are all interconnected to each other via\ntunable impedance components. Although such MiLACs are capacity-achieving,\ntheir circuit complexity, given by the number of required impedance components,\nscales quadratically with the number of antennas, limiting their practicality.\nTo solve this issue, in this paper, we propose a graph theoretical model of\nMiLAC facilitating the systematic design of lower-complexity MiLAC\narchitectures. Leveraging this model, we propose stem-connected MiLACs as a\nfamily of MiLAC architectures maintaining capacity-achieving performance while\ndrastically reducing the circuit complexity. Besides, we optimize\nstem-connected MiLACs with a closed-form capacity-achieving solution. Our\ntheoretical analysis, confirmed by numerical simulations, shows that\nstem-connected MiLACs are capacity-achieving, but with circuit complexity that\nscales linearly with the number of antennas, enabling high-performance,\nscalable, gigantic MIMO.", "comment": "Submitted to IEEE for publication", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15052v1", "AI": {"title_translation": "微波线性模拟计算机辅助的MIMO系统：具有降低电路复杂度的容量实现架构", "tldr": "本文提出了一种名为“干连接MiLACs”的新型微波线性模拟计算机架构，旨在降低大规模MIMO系统的电路复杂度，同时保持容量实现性能，使其复杂度与天线数量呈线性关系，而非传统的二次方关系。", "motivation": "未来的无线网络需要从大规模MIMO扩展到巨型MIMO，涉及更多的天线。现有全连接MiLACs虽然能实现容量，但其电路复杂度随天线数量呈二次方增长，限制了其实用性。因此，需要找到一种方法来降低MiLAC架构的电路复杂度，以实现高性能、可扩展的巨型MIMO。", "method": "本文提出了一种MiLAC的图论模型，以系统地设计低复杂度的MiLAC架构。利用该模型，提出了一系列名为“干连接MiLACs”的MiLAC架构，这些架构在保持容量实现性能的同时，显著降低了电路复杂度。此外，还通过一个闭式容量实现解决方案对干连接MiLACs进行了优化。", "result": "理论分析和数值模拟证实，干连接MiLACs能够实现容量，但其电路复杂度与天线数量呈线性关系，而非全连接MiLACs的二次方关系。这使得高性能、可扩展的巨型MIMO成为可能。", "conclusion": "干连接MiLACs提供了一种有效的解决方案，可以在保持容量实现性能的同时，显著降低巨型MIMO系统的电路复杂度，使其复杂度与天线数量呈线性关系，从而促进了巨型MIMO的实际应用和可扩展性。", "translation": "为了满足未来无线网络的需求，天线阵列必须从大规模多输入多输出（MIMO）扩展到巨型MIMO，涉及更多数量的天线。为了解决巨型MIMO的硬件和计算成本问题，有几种策略可以将处理从数字域转移到模拟域。其中，微波线性模拟计算机（MiLACs）通过可重构的微波网络实现全模拟波束成形，提供了一种引人注目的解决方案。先前的研究主要集中在全连接MiLACs上，其所有端口通过可调阻抗组件相互连接。尽管此类MiLACs能够实现容量，但其电路复杂度（由所需阻抗组件的数量决定）随天线数量呈二次方增长，限制了其实用性。为了解决这个问题，本文提出了一种MiLAC的图论模型，以促进低复杂度MiLAC架构的系统设计。利用该模型，我们提出了一系列名为“干连接MiLACs”的MiLAC架构，这些架构在保持容量实现性能的同时，显著降低了电路复杂度。此外，我们通过一个闭式容量实现解决方案优化了干连接MiLACs。我们的理论分析得到了数值模拟的证实，表明干连接MiLACs能够实现容量，但其电路复杂度与天线数量呈线性关系，从而实现了高性能、可扩展的巨型MIMO。", "summary": "本文针对巨型MIMO系统全连接微波线性模拟计算机（MiLACs）存在的电路复杂度高的问题，提出了一种基于图论模型的新型MiLAC架构——干连接MiLACs。该架构在保持容量实现性能的同时，将电路复杂度从天线数量的二次方降低到线性关系。理论分析和仿真结果验证了其有效性，为实现高性能、可扩展的巨型MIMO提供了实用解决方案。", "keywords": "巨型MIMO, 微波线性模拟计算机, 电路复杂度, 容量实现, 图论模型", "comments": "本文的创新点在于提出了MiLAC的图论模型以及基于此模型设计的干连接MiLACs架构。其重要性在于解决了全连接MiLACs电路复杂度过高的问题，使得巨型MIMO系统在硬件实现上更具可行性。通过将复杂度从二次方降低到线性，极大地提升了MiLACs在未来大规模天线系统中的实用性和可扩展性。"}}
{"id": "2506.14852", "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching", "authors": ["Qizheng Zhang", "Michael Wornow", "Kunle Olukotun"], "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.", "comment": "23 pages", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14852v1", "AI": {"title_translation": "通过测试时计划缓存实现LLM代理的高效服务", "tldr": "LLM代理应用因广泛的规划和推理需求而成本高昂。现有LLM缓存技术（如上下文缓存和语义缓存）不足以满足代理应用的需求。本文提出“代理计划缓存”，一种在测试时提取、存储、适应和重用结构化计划模板的新方法，以降低服务成本。评估显示，该系统平均可降低成本46.62%并保持性能。", "motivation": "LLM代理应用因其广泛的规划和推理需求而产生高昂的成本。现有的LLM缓存技术主要为聊天机器人设计，无法有效处理依赖外部数据或环境上下文的代理应用所面临的成本问题。", "method": "本文提出“代理计划缓存”方法。该方法在测试时从已完成的代理执行中提取、存储、适应和重用结构化计划模板。它通过关键词提取将新的请求与缓存的计划进行匹配，并利用轻量级模型将这些模板适应于特定任务的上下文计划。", "result": "在多个真实世界代理应用中的评估表明，我们的系统平均可降低成本46.62%，同时保持性能。", "conclusion": "代理计划缓存为LLM代理服务提供了一种更高效的解决方案，并且可以有效补充现有的LLM服务基础设施。", "translation": "基于LLM的代理应用在复杂工作流中展现出越来越卓越的能力，但由于广泛的规划和推理需求而产生高昂的成本。现有的LLM缓存技术（如上下文缓存和语义缓存）主要为聊天机器人服务设计，不足以满足输出依赖于外部数据或环境上下文的代理应用。我们提出了一种新颖的方法——代理计划缓存，该方法从代理应用的规划阶段提取、存储、适应和重用结构化计划模板，以减少服务成本。与传统的语义缓存不同，我们的系统在测试时从已完成的代理执行中提取计划模板，采用关键词提取将新请求与缓存计划进行匹配，并利用轻量级模型将这些模板适应于具有上下文的任务特定计划。在多个真实世界代理应用中的评估表明，我们的系统平均可降低成本46.62%，同时保持性能，为LLM代理服务提供了一种更高效的解决方案，并补充了现有的LLM服务基础设施。", "summary": "本文提出“代理计划缓存”技术，旨在解决LLM代理应用因复杂规划和推理导致的高成本问题。该方法通过在测试时从已执行代理中提取、存储和复用结构化计划模板，并利用关键词匹配和轻量级模型进行适应，从而显著降低服务成本。实验证明，该系统平均能降低46.62%的成本，同时保持性能，为LLM代理服务提供了高效的补充方案。", "keywords": "LLM代理, 计划缓存, 成本效率, 测试时, 代理应用", "comments": "这篇论文提出了一种创新的缓存策略，专门针对LLM代理的特点，即其输出依赖于外部环境和复杂的规划过程。通过缓存“计划模板”而非简单的上下文或语义，它解决了现有缓存技术在代理场景下的不足。其“测试时”提取和“轻量级模型”适应的特点，使得方案既高效又灵活。成本降低46.62%是一个显著的改进，对LLM代理的实际部署具有重要意义。"}}
{"id": "2506.14805", "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14805v1", "AI": {"title_translation": "Argus Inspection: 多模态大型语言模型是否拥有潘诺普忒斯的眼睛？", "tldr": "本文介绍了Argus Inspection基准，用于评估多模态大语言模型在视觉细粒度感知和常识因果推理方面的能力，发现现有模型表现仍有很大提升空间。", "motivation": "多模态大型语言模型（MLLMs）在视觉细粒度感知和常识因果推理方面仍存在挑战，需要一个更全面、更细致的评估方法来衡量其真实能力。", "method": "本文引入了Argus Inspection，一个具有两个难度级别的多模态基准，重点评估详细的视觉识别和结合现实世界常识的因果推理能力。同时，提出了Eye of Panoptes框架，该框架整合了二元参数Sigmoid度量和指示函数，以更全面地评估MLLMs在基于意见的推理任务中的响应。研究在26个主流MLLMs上进行了实验。", "result": "对26个主流多模态大型语言模型进行的实验表明，在视觉细粒度推理方面的最高性能仅达到0.46，这突出显示了该领域仍有巨大的提升潜力。", "conclusion": "多模态大型语言模型在视觉细粒度感知和常识因果推理方面仍有显著的改进空间。本研究为MLLMs的持续优化和发展提供了有价值的视角和方向。", "translation": "随着多模态大型语言模型（MLLMs）的不断发展，它们的认知和推理能力取得了显著进展。然而，在视觉细粒度感知和常识因果推理方面仍然存在挑战。本文介绍了Argus Inspection，一个具有两个难度级别的多模态基准，强调详细的视觉识别，同时结合现实世界常识理解来评估因果推理能力。在此基础上，我们提出了“潘诺普忒斯之眼”（Eye of Panoptes）框架，该框架将二元参数Sigmoid度量与指示函数相结合，从而能够更全面地评估MLLMs在基于意见的推理任务中的响应。在26个主流MLLMs上进行的实验表明，视觉细粒度推理的最高性能仅达到0.46，突显了巨大的提升潜力。我们的研究为MLLMs的持续改进提供了宝贵的视角。", "summary": "本文提出了一个名为Argus Inspection的新型多模态基准，旨在深入评估多模态大型语言模型（MLLMs）在视觉细粒度感知和常识因果推理方面的能力。该基准包含两个难度级别，并引入了“潘诺普忒斯之眼”评估框架，以提供更全面的性能衡量。通过对26个主流MLLMs的实验，研究发现当前模型在视觉细粒度推理方面的表现仍有显著提升空间，最高性能仅为0.46，这为未来的MLLMs研究和改进指明了方向。", "keywords": "多模态大语言模型, 视觉细粒度感知, 常识因果推理, 基准测试, Argus Inspection", "comments": "本文的创新点在于提出了一个专门针对MLLMs视觉细粒度感知和常识因果推理的全新多模态基准Argus Inspection，并引入了独特的Eye of Panoptes评估框架。其重要性在于明确指出了当前MLLMs在这些关键能力上的不足，为社区提供了量化评估工具和未来模型优化的具体方向。"}}
{"id": "2506.14990", "title": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning", "authors": ["Tristan Tomilin", "Luka van den Boogaard", "Samuel Garcin", "Bram Grooten", "Meng Fang", "Mykola Pechenizkiy"], "summary": "Benchmarks play a crucial role in the development and analysis of\nreinforcement learning (RL) algorithms, with environment availability strongly\nimpacting research. One particularly underexplored intersection is continual\nlearning (CL) in cooperative multi-agent settings. To remedy this, we introduce\nMEAL (Multi-agent Environments for Adaptive Learning), the first benchmark\ntailored for continual multi-agent reinforcement learning (CMARL). Existing CL\nbenchmarks run environments on the CPU, leading to computational bottlenecks\nand limiting the length of task sequences. MEAL leverages JAX for GPU\nacceleration, enabling continual learning across sequences of 100 tasks on a\nstandard desktop PC in a few hours. We show that naively combining popular CL\nand MARL methods yields strong performance on simple environments, but fails to\nscale to more complex settings requiring sustained coordination and adaptation.\nOur ablation study identifies architectural and algorithmic features critical\nfor CMARL on MEAL.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.14990v1", "AI": {"title_translation": "MEAL：一个持续多智能体强化学习的基准", "tldr": "MEAL是首个专门针对持续多智能体强化学习（CMARL）的基准，利用JAX进行GPU加速，并揭示了现有方法在复杂CMARL任务上的局限性。", "motivation": "现有强化学习基准在持续学习与合作多智能体设置的交叉领域缺乏探索，导致该领域研究受限。为了弥补这一不足，研究者需要一个专门针对持续多智能体强化学习（CMARL）的基准。", "method": "研究引入了MEAL（多智能体自适应学习环境），这是首个专门为持续多智能体强化学习（CMARL）设计的基准。MEAL利用JAX进行GPU加速，能够在标准台式机上在数小时内完成100个任务序列的持续学习。研究还通过消融研究识别了CMARL的关键架构和算法特征。", "result": "研究表明，简单地结合流行的持续学习（CL）和多智能体强化学习（MARL）方法在简单环境中表现良好，但无法扩展到需要持续协调和适应的更复杂设置。消融研究识别了MEAL上CMARL的关键架构和算法特征。", "conclusion": "持续多智能体强化学习（CMARL）在复杂环境中仍面临挑战，简单的组合方法不足以应对。MEAL基准的引入有助于推动CMARL领域的研究，并揭示了需要进一步探索的关键算法和架构特性。", "translation": "基准在强化学习（RL）算法的开发和分析中起着至关重要的作用，环境的可用性强烈影响着研究。一个特别未被充分探索的交叉点是合作多智能体设置中的持续学习（CL）。为了弥补这一点，我们引入了MEAL（多智能体自适应学习环境），这是第一个专门为持续多智能体强化学习（CMARL）量身定制的基准。现有的持续学习基准在CPU上运行环境，导致计算瓶颈并限制了任务序列的长度。MEAL利用JAX进行GPU加速，使得在标准台式机上在几个小时内就能完成100个任务序列的持续学习。我们发现，简单地结合流行的持续学习和多智能体强化学习方法在简单环境中表现出强大的性能，但无法扩展到需要持续协调和适应的更复杂设置。我们的消融研究确定了MEAL上CMARL的关键架构和算法特征。", "summary": "MEAL是首个专门针对持续多智能体强化学习（CMARL）的基准。针对现有持续学习基准在多智能体合作设置中缺乏探索且存在CPU计算瓶颈的问题，MEAL利用JAX进行GPU加速，大大提高了训练效率。研究发现，简单结合现有持续学习和多智能体强化学习方法在复杂CMARL任务上表现不佳，并识别了对CMARL至关重要的架构和算法特征。", "keywords": "持续多智能体强化学习, 基准, JAX, 多智能体强化学习, 持续学习", "comments": "MEAL基准的创新之处在于它是首个专门为持续多智能体强化学习（CMARL）设计的基准，并引入了GPU加速，极大地提高了研究效率。这对于推动CMARL领域的发展具有重要意义，因为它提供了一个标准化的、高效的评估工具。研究结果也指出了现有方法在复杂CMARL任务上的局限性，为未来的研究指明了方向。"}}
{"id": "2506.15263", "title": "Minimizing Structural Vibrations via Guided Flow Matching Design Optimization", "authors": ["Jan van Delden", "Julius Schultz", "Sebastian Rothe", "Christian Libner", "Sabine C. Langer", "Timo Lüddecke"], "summary": "Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.15263v1", "AI": {"title_translation": "通过引导流匹配设计优化最小化结构振动", "tldr": "本研究提出了一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕来减少结构振动，该方法能够生成多样化且可制造的低振动设计。", "motivation": "结构振动是工程系统中（如汽车、火车或飞机）不必要的噪音来源，最小化这些振动对于提高乘客舒适度至关重要。", "method": "本研究提出了一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕来减少振动。该方法整合了一个生成式流匹配模型和一个预测结构振动的代理模型。在生成过程中，流匹配模型倾向于可制造性，而代理模型则倾向于低振动解决方案。流匹配模型及其训练数据隐式定义了设计空间，无需手动定义设计参数的优化，从而实现了对潜在解决方案的更广泛探索。该方法应用于一系列可微分优化目标，包括通过精心构建目标函数直接优化特定特征频率。", "result": "结果表明，与随机搜索、基于准则的设计启发式方法和遗传优化生成的设计相比，我们的方法能够生成多样化且可制造的板状设计，并显著降低了结构振动。", "conclusion": "该研究成功开发了一种通过引导流匹配设计优化来最小化结构振动的新方法，能够生成具有降低振动特性的多样化和可制造的设计。", "translation": "结构振动是汽车、火车或飞机等工程系统中不必要的噪音来源。最小化这些振动对于提高乘客舒适度至关重要。这项工作提出了一种基于引导流匹配的新颖设计优化方法，通过在板状结构中放置压痕（凹陷）来减少振动。我们的方法整合了一个生成式流匹配模型和一个经过训练用于预测结构振动的代理模型。在生成过程中，流匹配模型倾向于可制造性，而代理模型则倾向于低振动解决方案。流匹配模型及其训练数据隐式定义了设计空间，无需手动定义设计参数的优化，从而实现了对潜在解决方案的更广泛探索。我们将该方法应用于一系列可微分优化目标，包括通过精心构建目标函数直接优化特定特征频率。结果表明，与随机搜索、基于准则的设计启发式方法和遗传优化生成的设计相比，我们的方法能够生成多样化且可制造的板状设计，并显著降低了结构振动。代码和数据可在https://github.com/ecker-lab/Optimizing_Vibrating_Plates获取。", "summary": "本研究提出了一种基于引导流匹配的新型设计优化方法，旨在通过在板状结构中放置压痕来最小化结构振动。该方法结合了生成式流匹配模型和预测振动的代理模型，前者确保可制造性，后者追求低振动。通过隐式定义设计空间，该方法实现了对多样化且可制造的低振动设计的广泛探索，并在实验中表现优于传统优化方法。", "keywords": "结构振动, 流匹配, 设计优化, 压痕, 制造性", "comments": "该论文提出了一种新颖的基于引导流匹配的设计优化方法，其创新之处在于将生成式模型与代理模型相结合，并隐式定义设计空间，从而无需手动参数优化，能够探索更广泛的设计解决方案。这对于复杂工程系统的振动控制具有重要意义，尤其是在确保设计可制造性的同时实现性能优化方面。"}}
{"id": "2506.14781", "title": "Two-dimensional Parallel Tempering for Constrained Optimization", "authors": ["Corentin Delacour", "M Mahmudul Hasan Sajeeb", "Joao P. Hespanha", "Kerem Y. Camsari"], "summary": "Sampling Boltzmann probability distributions plays a key role in machine\nlearning and optimization, motivating the design of hardware accelerators such\nas Ising machines. While the Ising model can in principle encode arbitrary\noptimization problems, practical implementations are often hindered by soft\nconstraints that either slow down mixing when too strong, or fail to enforce\nfeasibility when too weak. We introduce a two-dimensional extension of the\npowerful parallel tempering algorithm (PT) that addresses this challenge by\nadding a second dimension of replicas interpolating the penalty strengths. This\nscheme ensures constraint satisfaction in the final replicas, analogous to\nlow-energy states at low temperature. The resulting two-dimensional parallel\ntempering algorithm (2D-PT) improves mixing in heavily constrained replicas and\neliminates the need to explicitly tune the penalty strength. In a\nrepresentative example of graph sparsification with copy constraints, 2D-PT\nachieves near-ideal mixing, with Kullback-Leibler divergence decaying as\nO(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of\nmagnitude speedup over conventional PT with the same number of replicas. The\nmethod applies broadly to constrained Ising problems and can be deployed on\nexisting Ising machines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14781v1", "AI": {"title_translation": "二维平行回火算法用于约束优化", "tldr": "本文介绍了2D-PT，一种平行回火算法的扩展，通过插值惩罚强度来改善约束优化问题中的采样，从而实现更好的混合并消除惩罚调整的需要，并显著加速。", "motivation": "在机器学习和优化中，采样玻尔兹曼概率分布至关重要，这促使了伊辛机等硬件加速器的设计。然而，伊辛模型在解决优化问题时，实际实现常受软约束阻碍：过强的约束会减慢混合，过弱的约束则无法保证可行性。", "method": "本文引入了一种强大的平行回火算法（PT）的二维扩展（2D-PT）。该方案通过添加第二个维度来插入惩罚强度，从而确保最终副本中的约束满足。", "result": "2D-PT改善了重度约束副本中的混合，并消除了明确调整惩罚强度的需要。在具有复制约束的图稀疏化示例中，2D-PT实现了接近理想的混合，库尔巴克-莱布勒散度以O(1/t)的速度衰减。当应用于稀疏化的Wishart实例时，2D-PT比相同数量副本的传统PT实现了数量级的加速。", "conclusion": "2D-PT方法广泛适用于受约束的伊辛问题，并且可以部署在现有的伊辛机上，它通过改善混合和消除惩罚调整来解决软约束的挑战。", "translation": "采样玻尔兹曼概率分布在机器学习和优化中扮演着关键角色，这促使了诸如伊辛机等硬件加速器的设计。尽管伊辛模型原则上可以编码任意优化问题，但实际实现常常受到软约束的阻碍，这些约束在过强时会减慢混合，在过弱时则无法强制执行可行性。我们引入了一种强大的平行回火算法（PT）的二维扩展，通过添加第二个维度来插入惩罚强度，从而解决了这一挑战。该方案确保了最终副本中的约束满足，类似于低温下的低能量状态。由此产生的二维平行回火算法（2D-PT）改善了重度约束副本中的混合，并消除了明确调整惩罚强度的需要。在一个具有复制约束的图稀疏化代表性示例中，2D-PT实现了接近理想的混合，库尔巴克-莱布勒散度以O(1/t)的速度衰减。当应用于稀疏化的Wishart实例时，2D-PT比相同数量副本的传统PT实现了数量级的加速。该方法广泛适用于受约束的伊辛问题，并且可以部署在现有的伊辛机上。", "summary": "本文提出了一种二维平行回火算法（2D-PT），作为对传统平行回火算法的扩展，旨在解决伊辛模型中受约束优化问题中软约束带来的挑战。通过在第二个维度上插值惩罚强度，2D-PT显著改善了重度约束系统中的混合效率，并消除了手动调整惩罚强度的必要性。实验结果表明，2D-PT在代表性问题上实现了接近理想的混合，并且比传统PT提高了数量级的速度，这使得它广泛适用于受约束的伊辛问题，并可部署在现有硬件上。", "keywords": "二维平行回火, 约束优化, 伊辛模型, 玻尔兹曼采样, 惩罚方法", "comments": "本文的创新之处在于将广为人知的平行回火算法扩展到二维，特别是为了处理伊辛模型中软约束这一众所周知的难题。通过插值惩罚强度，它为混合速度和约束满足之间的权衡提供了一个优雅的解决方案。这可能是在优化领域中伊辛机实际应用的重要一步。"}}
{"id": "2506.15543", "title": "Learning Algorithms in the Limit", "authors": ["Hristo Papazov", "Nicolas Flammarion"], "summary": "This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.", "comment": "Accepted at COLT 2025. This version matches the proceedings version", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15543v1", "AI": {"title_translation": "极限学习算法", "tldr": "本文通过引入计算观测和受限输入源，扩展了Gold的归纳推理框架，以研究在更现实约束下可计算函数的极限学习问题，并展示了新的观测类型如何克服学习障碍，同时也指出了某些函数类的局限性。", "motivation": "研究在更现实的约束下（如计算观测和受限输入源）可计算函数的学习问题，以克服传统输入-输出观测在学习一般递归函数方面的不足。", "method": "通过引入时间界限观测和策略-轨迹观测来扩展Gold的归纳推理框架，并围绕计算代理的观测构建了一个形式框架。", "result": "传统输入-输出观测不足以学习一般递归函数，但通过施加计算复杂性约束或补充近似时间界限观测可以克服这一障碍。从策略轨迹学习可计算函数可以简化为从输入和输出学习有理函数，这与有限状态传感器推理有联系。然而，对于线性时间可计算函数类，即使是策略轨迹观测，也不存在可计算或多项式质量的特征集。", "conclusion": "本文研究了在计算观测和受限输入源下可计算函数的极限学习，发现新的观测类型可以克服传统方法的局限性，使得一般递归函数的学习成为可能，但同时也存在某些特定函数类的固有学习限制。", "translation": "本文通过扩展Gold的归纳推理框架，纳入计算观测和受限输入源，研究了可计算函数在极限情况下的学习问题。作为传统输入-输出观测的补充，我们引入了时间界限观测和策略-轨迹观测，以在更现实的约束下研究一般递归函数的可学习性。尽管输入-输出观测不足以在极限情况下学习一般递归函数类，但我们通过施加计算复杂性约束或补充近似时间界限观测来克服了这一学习障碍。此外，我们围绕计算代理的观测构建了一个形式框架，并表明从策略轨迹学习可计算函数可以简化为从输入和输出学习有理函数，从而揭示了与有限状态传感器推理的有趣联系。不利的一面是，我们表明即使对于策略-轨迹观测，线性时间可计算函数类也不存在可计算或多项式质量的特征集。", "summary": "本文扩展了Gold的归纳推理框架，引入了时间界限和策略-轨迹等计算观测类型，以研究在更现实约束下可计算函数的学习。研究发现，虽然传统输入-输出观测不足以学习一般递归函数，但通过引入新的观测或复杂性约束可以克服这一限制。文章还建立了计算代理观测的形式框架，并将从策略轨迹学习与有限状态传感器推理联系起来，但同时也指出，对于线性时间可计算函数，某些理想的特征集是不存在的。", "keywords": "极限学习, 归纳推理, 可计算函数, 计算观测, 策略轨迹", "comments": "本文通过引入计算观测（如时间界限观测和策略-轨迹观测）创新性地扩展了Gold的归纳推理框架，这对于在更现实的计算约束下理解可学习性至关重要。它不仅展示了新观测类型如何克服传统输入-输出观测的局限性，使一般递归函数的学习成为可能，而且还揭示了与有限状态传感器推理的有趣联系。同时，论文也诚实地指出了某些函数类在特定条件下学习的负面结果，这提供了对极限学习算法能力和局限性的全面视角。"}}
{"id": "2506.14922", "title": "FORTRESS: Frontier Risk Evaluation for National Security and Public Safety", "authors": ["Christina Q. Knight", "Kaustubh Deshpande", "Ved Sirdeshmukh", "Meher Mankikar", "Scale Red Team", "SEAL Research Team", "Julian Michael"], "summary": "The rapid advancement of large language models (LLMs) introduces dual-use\ncapabilities that could both threaten and bolster national security and public\nsafety (NSPS). Models implement safeguards to protect against potential misuse\nrelevant to NSPS and allow for benign users to receive helpful information.\nHowever, current benchmarks often fail to test safeguard robustness to\npotential NSPS risks in an objective, robust way. We introduce FORTRESS: 500\nexpert-crafted adversarial prompts with instance-based rubrics of 4-7 binary\nquestions for automated evaluation across 3 domains (unclassified information\nonly): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE),\nPolitical Violence & Terrorism, and Criminal & Financial Illicit Activities,\nwith 10 total subcategories across these domains. Each prompt-rubric pair has a\ncorresponding benign version to test for model over-refusals. This evaluation\nof frontier LLMs' safeguard robustness reveals varying trade-offs between\npotential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low\naverage risk score (ARS) (14.09 out of 100) but the highest over-refusal score\n(ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but\na high average potential risk (66.29). Deepseek-R1 has the highest ARS at\n78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even\ntrade-off between potential risks and over-refusals (with an ARS of 21.69 and\nORS of 5.2). To provide policymakers and researchers with a clear understanding\nof models' potential risks, we publicly release FORTRESS at\nhttps://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a\nprivate set for evaluation.", "comment": "12 pages, 7 figures, submitted to NeurIPS", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14922v1", "AI": {"title_translation": "FORTRESS：国家安全和公共安全的前沿风险评估", "tldr": "FORTRESS引入了一个新的基准，包含500个对抗性提示，用于客观、稳健地评估前沿大型语言模型在国家安全和公共安全领域的安全防护能力，并揭示了风险与实用性之间的权衡。", "motivation": "大型语言模型（LLMs）具有双重用途能力，可能威胁或增强国家安全和公共安全（NSPS）。然而，现有基准未能客观、稳健地测试安全防护措施对潜在NSPS风险的鲁棒性。", "method": "引入了FORTRESS，一个包含500个专家精心设计的对抗性提示的基准，并附有4-7个二元问题的实例式评估标准，用于自动化评估。评估涵盖三个领域：化学、生物、放射、核与爆炸（CBRNE），政治暴力与恐怖主义，以及犯罪与金融非法活动，共10个子类别。每个对抗性提示-评估标准对都包含一个相应的良性版本，以测试模型过度拒绝的情况。", "result": "对前沿LLM安全防护鲁棒性的评估揭示了潜在风险和模型有用性之间存在不同权衡：Claude-3.5-Sonnet的平均风险得分（ARS）低（14.09），但过度拒绝得分（ORS）最高（21.8）；Gemini 2.5 Pro的ORS低（1.4），但平均潜在风险高（66.29）；Deepseek-R1的ARS最高（78.05），但ORS最低（0.06）；o1模型在潜在风险和过度拒绝之间表现出更均衡的权衡（ARS为21.69，ORS为5.2）。", "conclusion": "为了让政策制定者和研究人员清晰了解模型的潜在风险，作者公开了FORTRESS基准，并维护一个私人数据集用于评估。", "translation": "大型语言模型（LLMs）的快速发展带来了双重用途能力，这既可能威胁也可能增强国家安全和公共安全（NSPS）。模型实施安全防护措施，以防止与NSPS相关的潜在滥用，并允许良性用户接收有用的信息。然而，目前的基准往往未能客观、稳健地测试安全防护措施对潜在NSPS风险的鲁棒性。我们引入了FORTRESS：500个专家精心设计的对抗性提示，附带4-7个二元问题的实例式评估标准，用于在3个领域（仅限非机密信息）进行自动化评估：化学、生物、放射、核与爆炸（CBRNE）、政治暴力与恐怖主义，以及犯罪与金融非法活动，这些领域共有10个子类别。每个提示-评估标准对都有一个相应的良性版本，用于测试模型过度拒绝的情况。对前沿LLM安全防护鲁棒性的评估揭示了潜在风险和模型有用性之间存在不同的权衡：Claude-3.5-Sonnet的平均风险得分（ARS）低（14.09，满分100），但过度拒绝得分（ORS）最高（21.8，满分100），而Gemini 2.5 Pro显示出低过度拒绝（1.4），但平均潜在风险高（66.29）。Deepseek-R1的ARS最高，为78.05，但ORS最低，仅为0.06。像o1这样的模型在潜在风险和过度拒绝之间表现出更均衡的权衡（ARS为21.69，ORS为5.2）。为了让政策制定者和研究人员清晰了解模型的潜在风险，我们公开了FORTRESS，网址为https://huggingface.co/datasets/ScaleAI/fortress_public。我们还维护一个私人数据集用于评估。", "summary": "本文介绍了FORTRESS，一个用于评估大型语言模型（LLMs）在国家安全和公共安全（NSPS）领域潜在风险的新基准。该基准包含500个专家设计的对抗性提示和实例式评估标准，涵盖CBRNE、政治暴力与恐怖主义、犯罪与金融非法活动三个领域，并包含良性版本以测试过度拒绝。评估结果显示，不同前沿LLM在潜在风险和模型实用性之间存在不同的权衡，例如Claude-3.5-Sonnet风险低但过度拒绝高，而Gemini 2.5 Pro过度拒绝低但风险高。FORTRESS已公开，旨在帮助政策制定者和研究人员理解模型风险。", "keywords": "大型语言模型, 国家安全, 公共安全, 风险评估, 安全防护", "comments": "FORTRESS的创新之处在于其大规模、专家设计的对抗性提示集以及用于自动化评估的细致评估标准，这提供了一种更客观、鲁棒的方式来测试LLM在国家安全和公共安全关键领域的安全防护能力。该研究的重要性在于揭示了当前前沿LLM在风险规避和有用性之间的复杂权衡，并为政策制定者和研究人员提供了急需的工具来评估和理解这些风险。公开数据集的举措也值得称赞，有助于推动该领域的透明度和进一步研究。"}}
{"id": "2506.15026", "title": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles: Minimizing Lane Changes and Merging", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "summary": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced\nsafety and operational efficiency. However, frequent lane changes and merging\nmaneuvers continue to pose significant safety risks and disrupt traffic flow.\nThis paper introduces the Minimizing Lane Change Algorithm (MLCA), a\nstate-machine-based approach designed to reduce unnecessary lane changes,\nthereby enhancing both traffic safety and efficiency. The MLCA algorithm\nprioritizes maintaining lane stability unless safety-critical conditions\nnecessitate a lane change. The algorithm's effectiveness was evaluated through\nsimulations conducted on the SUMO platform, comparing its performance against\nestablished models, including LC2017 and MOBIL. Results demonstrate substantial\nreductions in lane changes and collisions, leading to smoother traffic flow and\nimproved safety metrics. Additionally, the study highlights the MLCA's\nadaptability to various traffic densities and roadway configurations,\nshowcasing its potential for wide-scale deployment in real-world AV systems.\nFuture work aims to validate these findings in more complex scenarios using the\nCARLA simulator, which will enable the testing of the algorithm under more\ndynamic and high-fidelity conditions, such as urban traffic environments with\ndiverse road users. Moreover, the integration of cybersecurity measures for\nvehicle-to-vehicle (V2V) communication will be explored to ensure robust and\nsecure data exchange, further enhancing the reliability and safety of AV\noperations. This research contributes to the broader goal of developing\nintelligent traffic systems that optimize both individual vehicle performance\nand overall traffic network efficiency.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15026v1", "AI": {"title_translation": "自动驾驶汽车安全增强的算法方法：最小化变道与合流", "tldr": "本文提出MLCA算法，通过减少不必要的变道来提高自动驾驶汽车的安全性和交通效率，并在SUMO仿真中验证了其有效性。", "motivation": "自动驾驶汽车中频繁的变道和合流操作会带来显著的安全风险并扰乱交通流，因此需要算法来减少这些风险。", "method": "本文提出了一种基于状态机的“最小化变道算法（MLCA）”，旨在减少不必要的变道。该算法优先保持车道稳定性，除非安全关键条件要求变道。通过在SUMO平台上进行仿真，并与LC2017和MOBIL等现有模型进行比较，评估了其性能。", "result": "仿真结果表明，MLCA显著减少了变道次数和碰撞，从而使交通流更平稳，并提高了安全指标。此外，MLCA对各种交通密度和道路配置具有良好的适应性。", "conclusion": "该研究通过开发MLCA算法，证明了减少自动驾驶汽车不必要变道对于提升交通安全和效率的有效性，并为未来在复杂真实世界场景中的部署和集成网络安全措施奠定了基础。", "translation": "自动驾驶汽车（AV）技术的快速发展预示着更高的安全性和运行效率。然而，频繁的变道和合流操作仍然带来显著的安全风险并扰乱交通流。本文引入了最小化变道算法（MLCA），这是一种基于状态机的方法，旨在减少不必要的变道，从而提高交通安全性和效率。MLCA算法优先保持车道稳定性，除非安全关键条件要求变道。该算法的有效性通过在SUMO平台上进行的仿真进行了评估，并将其性能与包括LC2017和MOBIL在内的既有模型进行了比较。结果表明，变道和碰撞显著减少，从而导致交通流更平稳并改善了安全指标。此外，研究强调了MLCA对各种交通密度和道路配置的适应性，展示了其在实际AV系统中大规模部署的潜力。未来的工作旨在利用CARLA模拟器在更复杂的场景中验证这些发现，这将使得在更动态和高保真条件下（例如具有不同道路使用者的城市交通环境）测试算法成为可能。此外，还将探索车辆到车辆（V2V）通信的网络安全措施集成，以确保健壮和安全的数据交换，进一步提高AV操作的可靠性和安全性。这项研究有助于实现开发智能交通系统的更广泛目标，该系统能够优化单个车辆性能和整体交通网络效率。", "summary": "本文提出了一种名为最小化变道算法（MLCA）的基于状态机方法，旨在减少自动驾驶汽车不必要的变道，从而提高交通安全和效率。该算法优先保持车道稳定性。通过在SUMO平台上的仿真，MLCA与现有模型相比，显著减少了变道和碰撞，改善了交通流和安全指标，并展现出对不同交通环境的适应性，具有广泛部署潜力。", "keywords": "自动驾驶汽车, 变道, 合流, 交通安全, MLCA", "comments": "这项研究通过提出MLCA算法，为解决自动驾驶汽车频繁变道和合流带来的安全及效率问题提供了一种新颖的解决方案。其创新性在于采用基于状态机的方法来优先保持车道稳定性，有效减少了不必要的变道。该方法在仿真中表现出显著的性能提升，对提高自动驾驶系统的实际安全性和交通流畅度具有重要意义。未来工作计划在更复杂的模拟器中进行验证，并考虑网络安全集成，这表明了该研究的全面性和前瞻性。"}}
{"id": "2506.14834", "title": "Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection", "authors": ["Akwasi Asare", "Dennis Agyemanh Nana Gookyi", "Derrick Boateng", "Fortunatus Aabangbio Wulnye"], "summary": "Diabetic Retinopathy (DR), a leading cause of vision impairment in\nindividuals with diabetes, affects approximately 34.6% of diabetes patients\nglobally, with the number of cases projected to reach 242 million by 2045.\nTraditional DR diagnosis relies on the manual examination of retinal fundus\nimages, which is both time-consuming and resource intensive. This study\npresents a novel solution using Edge Impulse to deploy multiple deep learning\nmodels for real-time DR detection on edge devices. A robust dataset of over\n3,662 retinal fundus images, sourced from the Kaggle EyePACS dataset, was\ncurated, and enhanced through preprocessing techniques, including augmentation\nand normalization. Using TensorFlow, various Convolutional Neural Networks\n(CNNs), such as MobileNet, ShuffleNet, SqueezeNet, and a custom Deep Neural\nNetwork (DNN), were designed, trained, and optimized for edge deployment. The\nmodels were converted to TensorFlowLite and quantized to 8-bit integers to\nreduce their size and enhance inference speed, with minimal trade-offs in\naccuracy. Performance evaluations across different edge hardware platforms,\nincluding smartphones and microcontrollers, highlighted key metrics such as\ninference speed, accuracy, precision, and resource utilization. MobileNet\nachieved an accuracy of 96.45%, while SqueezeNet demonstrated strong real-time\nperformance with a small model size of 176 KB and latency of just 17 ms on GPU.\nShuffleNet and the custom DNN achieved moderate accuracy but excelled in\nresource efficiency, making them suitable for lower-end devices. This\nintegration of edge AI technology into healthcare presents a scalable,\ncost-effective solution for early DR detection, providing timely and accurate\ndiagnosis, especially in resource-constrained and remote healthcare settings.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14834v1", "AI": {"title_translation": "在边缘设备上部署和评估多种深度学习模型用于糖尿病视网膜病变检测", "tldr": "该研究在边缘设备上部署并评估了多种深度学习模型，实现了对糖尿病视网膜病变的实时、高效检测，特别适用于资源受限的环境。", "motivation": "糖尿病视网膜病变是导致视力障碍的主要原因，影响全球大量糖尿病患者，且传统诊断方法耗时且资源密集。", "method": "研究利用Edge Impulse在边缘设备上部署深度学习模型进行实时糖尿病视网膜病变检测。使用了来自Kaggle EyePACS数据集的3662多张视网膜眼底图像，并通过数据增强和归一化进行预处理。使用TensorFlow设计、训练和优化了MobileNet、ShuffleNet、SqueezeNet以及自定义DNN等多种CNN模型，并将其转换为TensorFlowLite并量化为8位整数以减小模型大小并提高推理速度。在智能手机和微控制器等不同边缘硬件平台上评估了模型的推理速度、准确性、精度和资源利用率。", "result": "MobileNet实现了96.45%的准确率。SqueezeNet在GPU上表现出强大的实时性能，模型大小仅为176 KB，延迟仅为17 ms。ShuffleNet和自定义DNN虽然准确率适中，但在资源效率方面表现出色，适用于低端设备。", "conclusion": "将边缘AI技术整合到医疗保健中，为早期糖尿病视网膜病变检测提供了一种可扩展、经济高效的解决方案，能够及时、准确地诊断，特别适用于资源受限和偏远的医疗环境。", "translation": "糖尿病视网膜病变（DR）是糖尿病患者视力受损的主要原因，全球约34.6%的糖尿病患者受其影响，预计到2045年病例数将达到2.42亿。传统的糖尿病视网膜病变诊断依赖于视网膜眼底图像的人工检查，这既耗时又资源密集。本研究提出了一种新颖的解决方案，利用Edge Impulse在边缘设备上部署多个深度学习模型，用于实时糖尿病视网膜病变检测。从Kaggle EyePACS数据集中整理并增强了超过3,662张视网膜眼底图像的强大数据集，通过包括增强和归一化在内的预处理技术进行了处理。使用TensorFlow，设计、训练和优化了各种卷积神经网络（CNN），如MobileNet、ShuffleNet、SqueezeNet和自定义深度神经网络（DNN），以适应边缘部署。这些模型被转换为TensorFlowLite并量化为8位整数，以减小模型大小并提高推理速度，同时最大限度地减少准确性方面的权衡。在包括智能手机和微控制器在内的不同边缘硬件平台上进行的性能评估突出了关键指标，如推理速度、准确性、精度和资源利用率。MobileNet实现了96.45%的准确率，而SqueezeNet展示了强大的实时性能，模型大小仅为176 KB，在GPU上的延迟仅为17毫秒。ShuffleNet和自定义DNN实现了中等准确率，但在资源效率方面表现出色，使其适用于低端设备。将边缘AI技术整合到医疗保健中，为早期糖尿病视网膜病变检测提供了一种可扩展、经济高效的解决方案，能够及时、准确地诊断，特别是在资源受限和偏远的医疗环境中。", "summary": "本研究提出了一种在边缘设备上部署和评估多种深度学习模型以实现糖尿病视网膜病变（DR）实时检测的新方法。通过对Kaggle EyePACS数据集进行预处理和增强，并利用TensorFlow训练和优化了包括MobileNet、ShuffleNet、SqueezeNet和自定义DNN在内的多种CNN模型，将其转换为TensorFlowLite并进行8位量化以适应边缘部署。性能评估显示，MobileNet准确率达到96.45%，SqueezeNet在模型大小和延迟方面表现优异，而ShuffleNet和自定义DNN则在资源效率上突出。该方案为资源受限环境下的早期DR检测提供了一种可扩展且经济高效的解决方案。", "keywords": "糖尿病视网膜病变, 深度学习, 边缘计算, 模型部署, TensorFlowLite", "comments": "这项研究的创新之处在于成功地将多个深度学习模型部署到边缘设备上进行糖尿病视网膜病变检测，解决了传统诊断的效率问题。通过模型量化和针对不同边缘硬件的优化，实现了在保持较高准确性的同时显著降低了模型大小和推理延迟，使其在资源受限和偏远地区具有重要的实际应用价值。"}}
{"id": "2506.14985", "title": "Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization", "authors": ["Kuranage Roche Rayan Ranasinghe", "Hyeon Seok Rou", "Iván Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "George C. Alexandropoulos"], "summary": "The doubly-dispersive (DD) channel structure has played a pivotal role in\nwireless communications, particularly in high-mobility scenarios and integrated\nsensing and communications (ISAC), due to its ability to capture the key fading\neffects experienced by a transmitted signal as it propagates through a dynamic\nmedium. However, extending the DD framework to multiple-input multiple-output\n(MIMO) systems, especially in environments artificially enhanced by\nreconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces\n(SIM), remains a challenging open problem. In this chapter, a novel\nmetasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary\nnumber of RISs, while also incorporating SIM at both the transmitter and\nreceiver is introduced. Next, the application of this model to some key\nwaveforms optimized for DD environments -- namely orthogonal frequency division\nmultiplexing (OFDM), orthogonal time frequency space (OTFS), and affine\nfrequency division multiplexing (AFDM) -- is discussed. Finally, the\nprogrammability of the proposed model is highlighted through an illustrative\napplication, demonstrating its potential for enhancing waveform performance in\nSIM-assisted wireless systems.", "comment": "Author's version of chapter from forthcoming book \"Reconfigurable\n  Metasurfaces for Wireless Communications: Architectures, Modeling, and\n  Optimization\"", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14985v1", "AI": {"title_translation": "超表面集成双色散MIMO：信道建模与优化", "tldr": "本文提出了一种新型的超表面参数化双色散（MPDD）信道模型，用于集成可重构智能表面（RIS）和堆叠智能超表面（SIM）的MIMO系统，并讨论了其在优化波形（如OFDM、OTFS、AFDM）中的应用潜力。", "motivation": "在无线通信中，双色散（DD）信道对于高移动性场景和集成传感与通信（ISAC）至关重要。然而，将DD框架扩展到多输入多输出（MIMO）系统，尤其是在由可重构智能表面（RIS）和堆叠智能超表面（SIM）增强的环境中，仍然是一个具有挑战性的开放问题。", "method": "本文引入了一种新颖的超表面参数化双色散（MPDD）信道模型，该模型集成了任意数量的RIS，并在发射器和接收器处都包含了SIM。接着，讨论了该模型在针对DD环境优化的关键波形（如正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM））中的应用。", "result": "通过一个说明性应用，强调了所提出模型的可编程性，并展示了其在SIM辅助无线系统中增强波形性能的潜力。", "conclusion": "本研究提出的MPDD信道模型为在集成RIS和SIM的MIMO系统中捕捉双色散效应提供了一个解决方案，并展示了其在优化多种波形性能方面的有效性。", "translation": "双色散（DD）信道结构在无线通信中扮演着关键角色，尤其是在高移动性场景和集成传感与通信（ISAC）中，因为它能够捕捉传输信号在动态介质中传播时所经历的关键衰落效应。然而，将DD框架扩展到多输入多输出（MIMO）系统，特别是在由可重构智能表面（RIS）和堆叠智能超表面（SIM）人工增强的环境中，仍然是一个具有挑战性的开放问题。在本章中，引入了一种新颖的超表面参数化DD（MPDD）信道模型，该模型集成了任意数量的RIS，同时在发射器和接收器处都包含了SIM。接下来，讨论了该模型在一些针对DD环境优化的关键波形（即正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM））中的应用。最后，通过一个说明性应用突出了所提出模型的可编程性，展示了其在SIM辅助无线系统中增强波形性能的潜力。", "summary": "本文提出了一种新颖的超表面参数化双色散（MPDD）信道模型，旨在解决将双色散（DD）框架扩展到集成可重构智能表面（RIS）和堆叠智能超表面（SIM）的多输入多输出（MIMO）系统所面临的挑战。该模型集成了RIS和SIM，并探讨了其在优化OFDM、OTFS和AFDM等关键波形方面的应用。研究结果表明，该模型具有可编程性，并有望增强SIM辅助无线系统中的波形性能。", "keywords": "双色散, MIMO, 超表面, 信道建模, SIM", "comments": "本文的创新点在于提出了一个新颖的MPDD信道模型，它首次将RIS和SIM集成到双色散MIMO信道建模中，解决了现有框架在复杂环境下的局限性。该模型的可编程性及其在优化多种先进波形方面的潜力，对于未来高移动性或ISAC场景下的无线通信系统设计具有重要意义。"}}
{"id": "2506.15097", "title": "Efficient space reduction techniques by optimized majority rules for the Kemeny aggregation problem", "authors": ["Xuan Kien Phung", "Sylvie Hamel"], "summary": "The Kemeny aggregation problem consists of computing the consensus rankings\nof an election with respect to the Kemeny-Young voting method. These aggregated\nrankings are the geometric medians as well as the maximum likelihood estimators\nin the Mallows model of the rankings in the election under the Kendall-tau\ndistance which counts the number of pairwise disagreements. The problem admits\nfundamental applications in various domains such as computational social\nchoice, machine learning, operations research, and biology but its\ncomputational complexity is unfortunately expensive. In this paper, we\nestablish optimized quantitative extensions of the well-known 3/4-majority rule\nof Betzler et al. and the Major Order Theorem of Hamel and Milosz for the\nKemeny aggregation problem. By taking into account the extra information\navailable in the problem such as the number of candidates and by considering an\nadditional optimization of certain piecewise linear functions in one variable,\nour results achieve significantly more refined space reduction techniques as\nillustrated by experimental results on real and synthetic data without\nincreasing the time complexity of the algorithms.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.15097v1", "AI": {"title_translation": "基于优化多数规则的Kemeny聚合问题高效空间削减技术", "tldr": "本文通过优化多数规则和利用额外信息，为Kemeny聚合问题提出了更精细的空间削减技术，且不增加算法的时间复杂度。", "motivation": "Kemeny聚合问题在计算社会选择、机器学习、运筹学和生物学等多个领域有重要应用，但其计算复杂性很高，成本昂贵。", "method": "本文对Betzler等人的3/4多数规则和Hamel和Milosz的多数顺序定理进行了优化的量化扩展。通过考虑问题中可用的额外信息（如候选人数量），并对某些分段线性函数进行额外的优化，实现了空间削减。", "result": "我们的方法在不增加算法时间复杂度的情况下，实现了显著更精细的空间削减技术，这通过真实和合成数据上的实验结果得到了证实。", "conclusion": "通过优化现有多数规则并利用额外信息，可以为Kemeny聚合问题提供更高效且不增加时间复杂度的空间削减技术。", "translation": "Kemeny聚合问题在于计算选举中Kemeny-Young投票方法下的共识排序。这些聚合排序是Mallows模型中选举排序的几何中位数和最大似然估计，衡量标准是计算成对分歧数量的Kendall-tau距离。该问题在计算社会选择、机器学习、运筹学和生物学等各种领域都有基本应用，但其计算复杂性不幸地昂贵。在本文中，我们为Kemeny聚合问题建立了Betzler等人著名的3/4多数规则和Hamel和Milosz的多数顺序定理的优化定量扩展。通过考虑问题中可用的额外信息，例如候选人数量，并通过考虑对一个变量中的某些分段线性函数进行额外的优化，我们的结果实现了显著更精细的空间削减技术，这通过真实和合成数据上的实验结果得到了证实，且不增加算法的时间复杂度。", "summary": "本研究针对计算成本高昂的Kemeny聚合问题，提出了一种高效的空间削减技术。该方法通过优化扩展了现有的3/4多数规则和多数顺序定理，并利用了问题中的额外信息（如候选人数量）和分段线性函数优化。实验结果表明，这些改进在不增加算法时间复杂度的前提下，显著提升了空间削减的精细度。", "keywords": "Kemeny聚合问题, 空间削减, 多数规则, 计算复杂性, 共识排序", "comments": "本文的创新点在于将现有多数规则与额外信息和优化技术相结合，从而在不牺牲时间效率的前提下，显著提高了Kemeny聚合问题的空间效率。这对于处理大规模选举数据或在资源受限环境下解决相关问题具有重要意义。"}}
{"id": "2506.15290", "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models", "authors": ["Andela Ilic", "Jiaxi Jiang", "Paul Streli", "Xintong Liu", "Christian Holz"], "summary": "Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.", "comment": "Accepted by IJCAI 2025", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15290v1", "AI": {"title_translation": "基于宽松稀疏惯性传感器的服装感知扩散模型人体运动捕捉", "tldr": "该研究提出了一种使用宽松稀疏惯性传感器进行全身人体姿态估计的新方法，通过模拟数据和服装感知扩散模型，在定量和定性上优于现有技术。", "motivation": "现有基于惯性测量单元（IMU）的运动捕捉方法通常假设传感器紧密附着于人体，但这在实际场景中往往不成立，限制了其应用。本研究旨在解决使用宽松稀疏IMU传感器进行全身人体姿态估计的挑战。", "method": "本研究通过现有服装感知人体运动数据集模拟IMU记录，生成宽松IMU数据。开发了基于Transformer的扩散模型来合成宽松IMU数据并基于此数据估计人体姿态。此外，在模型训练时结合服装相关参数，以保持表达能力并增强捕捉由宽松或紧身服装引入的变化的能力。", "result": "实验表明，在模拟和合成数据上训练的所提出的扩散方法在定量和定性上均优于最先进的方法。", "conclusion": "本研究为未来使用宽松稀疏惯性传感器进行人体运动捕捉开辟了一个有前景的方向。", "translation": "使用稀疏惯性传感器进行运动捕捉因其便携性且与基于摄像头的追踪相比没有遮挡问题而显示出巨大的前景。现有方法通常假设IMU传感器紧密附着于人体。然而，这一假设在现实场景中往往不成立。在本文中，我们提出了一项使用稀疏、宽松附着的IMU传感器进行全身人体姿态估计的新任务。为了解决这项任务，我们从现有服装感知人体运动数据集中模拟了IMU记录。我们开发了基于Transformer的扩散模型来合成宽松IMU数据，并基于这种具有挑战性的宽松IMU数据估计人体姿态。此外，我们表明，在模拟宽松数据上训练模型时，结合服装相关参数可以有效地保持表达能力，并增强捕捉由更宽松或更紧身服装引入的变化的能力。实验表明，我们提出的在模拟和合成数据上训练的扩散方法在定量和定性上均优于最先进的方法，为未来的研究开辟了一个有前景的方向。", "summary": "本论文提出了一种使用稀疏、宽松惯性传感器进行全身人体姿态估计的新任务，以克服现有IMU运动捕捉方法中传感器紧密附着的限制。研究团队通过模拟服装感知的人体运动数据集生成宽松IMU数据，并开发了基于Transformer的服装感知扩散模型来合成数据并估计姿态。实验结果显示，该方法在处理宽松传感器数据方面表现出色，并超越了现有最先进的方法，为未来的研究方向提供了新的思路。", "keywords": "人体运动捕捉, 惯性传感器, 扩散模型, 服装感知, 姿态估计", "comments": "该论文的创新点在于解决了惯性传感器在实际应用中可能遇到的传感器佩戴宽松问题，并引入了“服装感知”的概念，通过扩散模型有效处理了由服装引起的姿态变化。这对于IMU运动捕捉在更广泛、非受控环境中的应用具有重要意义，克服了传统方法对传感器紧密附着假设的依赖。"}}
{"id": "2506.15267", "title": "Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative Next-User Modeling", "authors": ["Yu-Ting Lan", "Yang Huo", "Yi Shen", "Xiao Yang", "Zuotao Liu"], "summary": "The item cold-start problem is critical for online recommendation systems, as\nthe success of this phase determines whether high-quality new items can\ntransition to popular ones, receive essential feedback to inspire creators, and\nthus lead to the long-term retention of creators. However, modern\nrecommendation systems still struggle to address item cold-start challenges due\nto the heavy reliance on item and historical interactions, which are\nnon-trivial for cold-start items lacking sufficient exposure and feedback.\nLookalike algorithms provide a promising solution by extending feedback for new\nitems based on lookalike users. Traditional lookalike algorithms face such\nlimitations: (1) failing to effectively model the lookalike users and further\nimprove recommendations with the existing rule- or model-based methods; and (2)\nstruggling to utilize the interaction signals and incorporate diverse features\nin modern recommendation systems.\n  Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel\nframework for enhancing cold-start recommendations via generative next-user\nmodeling. Specifically, we employ a transformer-based model to capture the\nunidirectional relationships among recently interacted users and utilize these\nsequences to generate the next potential user who is most likely to interact\nwith the item. The additional item features are also integrated as prefix\nprompt embeddings to assist the next-user generation. The effectiveness of\nNext-User Retrieval is evaluated through both offline experiments and online\nA/B tests. Our method achieves significant improvements with increases of\n0.0142% in daily active users and +0.1144% in publications in Douyin,\nshowcasing its practical applicability and scalability.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15267v1", "AI": {"title_translation": "下一个用户检索：通过生成式下一个用户建模增强冷启动推荐", "tldr": "本文提出了一种名为Next-User Retrieval的新框架，通过生成式下一个用户建模来解决推荐系统中的物品冷启动问题，并在抖音的在线A/B测试中取得了显著效果。", "motivation": "在线推荐系统中，物品冷启动问题至关重要，因为它决定了新物品能否成为热门、获得反馈并留住创作者。然而，现代推荐系统由于严重依赖物品和历史互动，对于缺乏曝光和反馈的冷启动物品难以应对。传统的Lookalike算法存在局限性，未能有效建模Lookalike用户，也难以利用互动信号和整合多样的特征。", "method": "本文提出了一种名为Next-User Retrieval的新框架，通过生成式下一个用户建模来增强冷启动推荐。具体来说，该方法采用基于Transformer的模型来捕获最近互动用户之间的单向关系，并利用这些序列生成最有可能与物品互动的下一个潜在用户。此外，物品特征也被整合为前缀提示嵌入，以辅助下一个用户的生成。", "result": "通过离线实验和在线A/B测试，Next-User Retrieval方法取得了显著的改进。在抖音平台，日活跃用户增加了0.0142%，作品发布量增加了0.1144%。", "conclusion": "Next-User Retrieval框架通过生成式下一个用户建模有效解决了物品冷启动问题，并通过在线实验验证了其在实际应用中的可行性和可扩展性。", "translation": "物品冷启动问题对于在线推荐系统至关重要，因为这一阶段的成功决定了高质量的新物品能否转化为热门物品，获得必要的反馈以激励创作者，从而实现创作者的长期留存。然而，现代推荐系统仍然难以解决物品冷启动挑战，因为它们严重依赖物品和历史互动，而这对于缺乏足够曝光和反馈的冷启动物品来说并非易事。Lookalike算法通过基于相似用户扩展新物品的反馈提供了一种有前景的解决方案。传统的Lookalike算法面临以下局限性：(1)未能有效建模相似用户，并用现有的基于规则或模型的方法进一步改进推荐；(2)难以利用互动信号并整合现代推荐系统中的多样化特征。\n受Lookalike算法的启发，我们提出了Next-User Retrieval，一个通过生成式下一个用户建模来增强冷启动推荐的新颖框架。具体来说，我们采用基于Transformer的模型来捕获最近互动用户之间的单向关系，并利用这些序列生成最有可能与物品互动的下一个潜在用户。额外的物品特征也被整合为前缀提示嵌入，以辅助下一个用户的生成。Next-User Retrieval的有效性通过离线实验和在线A/B测试进行了评估。我们的方法在抖音平台实现了显著改进，日活跃用户增加了0.0142%，作品发布量增加了0.1144%，展示了其在实际应用中的可行性和可扩展性。", "summary": "本文针对在线推荐系统中的物品冷启动问题，提出了一种名为Next-User Retrieval的新型框架。该框架受Lookalike算法启发，利用基于Transformer的模型捕获用户互动序列，并通过生成式建模预测最有可能与冷启动物品互动的下一个用户，同时整合物品特征。通过离线实验和在抖音的在线A/B测试，该方法在日活跃用户和作品发布量方面均取得了显著提升，证明了其在实际应用中的有效性和可扩展性。", "keywords": "冷启动推荐, 生成式建模, 下一个用户检索, Transformer, Lookalike算法", "comments": "该论文提出了一种新颖的方法来解决推荐系统中的关键冷启动问题，这对于新内容和创作者的生态系统至关重要。其创新点在于将生成式建模应用于“下一个用户”预测，而非传统的基于物品或用户的协同过滤。通过Transformer模型捕捉用户序列关系，并结合物品特征作为提示，这种方法有效地弥补了冷启动物品数据稀疏的不足。在线A/B测试的结果表明了该方法在实际生产环境中的实用价值和可扩展性。"}}
{"id": "2506.15456", "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.15456v1", "AI": {"title_translation": "用于解耦语音分词的分解RVQ-GAN", "tldr": "本文提出了分层音频编解码器（HAC），一个统一的神经语音编解码器，通过将瓶颈分解为声学、语音和词汇三个语言级别，并利用知识蒸馏来生成解耦的语音令牌，这些令牌在下游任务中表现出优越的性能。", "motivation": "为了弥合声学细节和词汇意义之间的差距，并为下游语音生成和理解任务提供统一的离散语音表示。", "method": "本文提出了分层音频编解码器（HAC），一个统一的神经语音编解码器。HAC将其瓶颈分解为声学、语音和词汇三个语言级别。它利用两个知识蒸馏目标：一个来自预训练的语音编码器（HuBERT）用于音素级结构，另一个来自基于文本的编码器（LaBSE）用于词汇线索。", "result": "在英语和多语言数据上的实验表明，HAC的分解瓶颈产生了解耦的令牌集：一个与音素对齐，另一个捕获词级语义。定量评估证实，HAC令牌保留了自然度并提供了可解释的语言信息，在解耦和重建质量方面均优于单级基线。", "conclusion": "HAC作为一个统一的离散语音表示，能够弥合声学细节和词汇意义之间的差距，在下游语音生成和理解任务中具有巨大潜力。", "translation": "我们提出了分层音频编解码器（HAC），一个统一的神经语音编解码器，它在一个模型中将其瓶颈分解为三个语言级别——声学、语音和词汇。HAC利用两个知识蒸馏目标：一个来自预训练的语音编码器（HuBERT）用于音素级结构，另一个来自基于文本的编码器（LaBSE）用于词汇线索。在英语和多语言数据上的实验表明，HAC的分解瓶颈产生了解耦的令牌集：一个与音素对齐，而另一个捕获词级语义。定量评估证实，HAC令牌保留了自然度并提供了可解释的语言信息，在解耦和重建质量方面均优于单级基线。这些发现强调了HAC作为统一离散语音表示的潜力，它弥合了声学细节和词汇意义之间的差距，用于下游语音生成和理解任务。", "summary": "本文提出了一种名为分层音频编解码器（HAC）的统一神经语音编解码器。HAC通过将瓶颈分解为声学、语音和词汇三个语言级别，并利用HuBERT和LaBSE进行知识蒸馏，从而生成解耦的语音令牌。实验结果表明，HAC生成的令牌能有效捕捉音素和词汇语义，且在自然度、可解释性、解耦性和重建质量方面均优于现有基线。这表明HAC在构建统一离散语音表示方面具有显著潜力，有助于未来的语音生成和理解任务。", "keywords": "语音编解码器, 解耦, 语音令牌, 知识蒸馏, 分层表示", "comments": "本文的创新点在于提出了一个统一的神经语音编解码器HAC，通过分层分解瓶颈并结合多源知识蒸馏，实现了语音令牌在不同语言级别（声学、语音、词汇）的解耦，为下游任务提供了更丰富和可解释的语音表示。其重要性体现在能够弥合语音的物理特性与高层语义之间的鸿沟。"}}
{"id": "2506.15222", "title": "The Pitfalls and Potentials of Adding Gene-invariance to Optimal Mixing", "authors": ["Anton Bouter", "Dirk Thierens", "Peter A. N. Bosman"], "summary": "Optimal Mixing (OM) is a variation operator that integrates local search with\ngenetic recombination. EAs with OM are capable of state-of-the-art optimization\nin discrete spaces, offering significant advantages over classic\nrecombination-based EAs. This success is partly due to high selection pressure\nthat drives rapid convergence. However, this can also negatively impact\npopulation diversity, complicating the solving of hierarchical problems, which\nfeature multiple layers of complexity. While there have been attempts to\naddress this issue, these solutions are often complicated and prone to bias. To\novercome this, we propose a solution inspired by the Gene Invariant Genetic\nAlgorithm (GIGA), which preserves gene frequencies in the population throughout\nthe process. This technique is tailored to and integrated with the Gene-pool\nOptimal Mixing Evolutionary Algorithm (GOMEA), resulting in GI-GOMEA. The\nsimple, yet elegant changes are found to have striking potential: GI-GOMEA\noutperforms GOMEA on a range of well-known problems, even when these problems\nare adjusted for pitfalls - biases in much-used benchmark problems that can be\neasily exploited by maintaining gene invariance. Perhaps even more notably,\nGI-GOMEA is also found to be effective at solving hierarchical problems,\nincluding newly introduced asymmetric hierarchical trap functions.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.15222v1", "AI": {"title_translation": "将基因不变性引入最优混合的陷阱与潜力", "tldr": "最优混合（OM）算法在离散空间优化中表现优秀，但高选择压力导致多样性下降，难以解决分层问题。本文提出GI-GOMEA，通过保持基因频率解决此问题，并发现其在多种问题上优于GOMEA，尤其在分层问题上表现出色。", "motivation": "最佳混合（OM）算法在离散空间优化中表现出色，但其高选择压力会导致种群多样性下降，从而难以解决具有多层复杂性的分层问题。现有解决方案复杂且容易产生偏差。", "method": "本文提出了一种名为GI-GOMEA的新方法，该方法受到基因不变遗传算法（GIGA）的启发，旨在整个过程中保持种群中的基因频率。这项技术专门为基因池最优混合进化算法（GOMEA）量身定制并与之集成。", "result": "GI-GOMEA在多种知名问题上表现优于GOMEA，即使这些问题经过调整以消除基准测试中的偏差。更值得注意的是，GI-GOMEA在解决分层问题方面也表现出有效性，包括新引入的非对称分层陷阱函数。", "conclusion": "通过引入基因不变性，GI-GOMEA成功克服了传统最优混合算法在处理复杂分层问题时多样性不足的缺点，并在性能上超越了GOMEA，展现了其在优化和解决分层问题上的巨大潜力。", "translation": "最优混合（OM）是一种将局部搜索与遗传重组相结合的变异算子。采用OM的EA在离散空间中能够实现最先进的优化，与经典的基于重组的EA相比具有显著优势。这种成功部分归因于驱动快速收敛的高选择压力。然而，这也会对种群多样性产生负面影响，使解决具有多层复杂性的分层问题变得复杂。尽管已经尝试解决此问题，但这些解决方案通常很复杂且容易产生偏差。\n为了克服这个问题，我们提出了一种受基因不变遗传算法（GIGA）启发的解决方案，该方案在整个过程中保留了种群中的基因频率。这项技术是为基因池最优混合进化算法（GOMEA）量身定制并与之集成的，从而产生了GI-GOMEA。\n这些简单而优雅的改变被发现具有惊人的潜力：GI-GOMEA在许多知名问题上优于GOMEA，即使这些问题经过调整以消除陷阱——常用基准问题中可以通过保持基因不变性轻松利用的偏差。也许更值得注意的是，GI-GOMEA也被发现能有效解决分层问题，包括新引入的非对称分层陷阱函数。", "summary": "本文针对最优混合（OM）算法在高选择压力下导致种群多样性下降，难以解决分层问题的问题，提出了一种名为GI-GOMEA的新型进化算法。GI-GOMEA通过引入基因不变性来保持种群基因频率。实验结果表明，GI-GOMEA在多种优化问题上均优于GOMEA，并且在解决包括新型非对称分层陷阱函数在内的分层问题上表现出显著效果，展示了其在复杂优化领域的潜力。", "keywords": "基因不变性, 最优混合, 进化算法, 分层问题, 种群多样性", "comments": "这篇论文通过引入基因不变性，巧妙地解决了最优混合算法在高选择压力下多样性不足的固有问题，这是一种创新性的改进。GI-GOMEA不仅在标准基准测试中表现出色，更重要的是，它有效解决了传统方法难以处理的分层问题，这对于复杂优化领域具有重要意义。该方法的简单性和有效性是其主要亮点。"}}
{"id": "2506.15000", "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "summary": "Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15000v1", "AI": {"title_translation": "深度学习模型在真实世界噪声环境下语音增强的比较评估", "tldr": "本研究比较了Wave-U-Net、CMGAN和U-Net三种深度学习模型在真实噪声环境下的语音增强性能，发现U-Net在噪声抑制方面表现突出，CMGAN在感知质量上最佳，Wave-U-Net则在保留说话人特定特征方面表现良好。", "motivation": "语音增强对于提高语音信号的清晰度和质量至关重要，尤其是在嘈杂环境中。现有深度学习模型在噪声抑制、感知质量和说话人特定特征保留之间难以平衡，且缺乏对其比较性能的评估，这是一个关键的研究空白。", "method": "本研究在SpEAR、VPQAD和Clarkson等多样化数据集上，对Wave-U-Net、CMGAN和U-Net三种先进的深度学习模型进行了基准测试和比较评估。", "result": "U-Net在噪声抑制方面表现出色，在SpEAR、VPQAD和Clarkson数据集上信噪比分别提高了+71.96%、+64.83%和+364.2%。CMGAN在感知质量方面表现最佳，在SpEAR和VPQAD上获得了最高的PESQ分数（分别为4.04和1.46）。Wave-U-Net在保留说话人特定特征方面表现良好，在SpEAR和VPQAD上VeriSpeak分数分别提高了+10.84%和+27.38%。", "conclusion": "先进方法可以优化噪声抑制、感知质量和说话人识别之间的权衡。研究结果有助于推进语音生物识别、法医音频分析、电信和说话人验证在挑战性声学条件下的发展。", "translation": "语音增强，特别是去噪，对于提高真实世界应用中语音信号的清晰度和质量至关重要，尤其是在嘈杂环境中。虽然之前的研究已经为此目的引入了各种深度学习模型，但许多模型难以平衡噪声抑制、感知质量和说话人特定特征的保留，这在它们的比较性能评估中留下了一个关键的研究空白。本研究在SpEAR、VPQAD和Clarkson等多样化数据集上，对三种最先进的模型Wave-U-Net、CMGAN和U-Net进行了基准测试。选择这些模型是因为它们在文献中的相关性和代码可访问性。评估结果表明，U-Net在噪声抑制方面取得了高效果，在SpEAR上信噪比提高了+71.96%，在VPQAD上提高了+64.83%，在Clarkson数据集上提高了+364.2%。CMGAN在感知质量方面表现出色，在SpEAR和VPQAD上获得了最高的PESQ分数（分别为4.04和1.46），这使其非常适合优先考虑自然和清晰语音的应用。Wave-U-Net通过提高说话人特定特征的保留来平衡这些属性，SpEAR上VeriSpeak分数提高了+10.84%，VPQAD上提高了+27.38%。本研究表明先进方法如何优化噪声抑制、感知质量和说话人识别之间的权衡。这些发现可能有助于推进语音生物识别、法医音频分析、电信和具有挑战性声学条件下的说话人验证。", "summary": "本研究旨在解决现有深度学习模型在真实世界噪声环境中语音增强性能评估的空白，特别关注噪声抑制、感知质量和说话人特定特征保留之间的权衡。研究比较了Wave-U-Net、CMGAN和U-Net三种先进模型在SpEAR、VPQAD和Clarkson数据集上的表现。结果显示U-Net在噪声抑制方面效果显著，CMGAN在感知质量方面表现最佳，而Wave-U-Net在保留说话人特征方面表现良好。研究强调了优化这些权衡的重要性，并认为其发现对语音生物识别、法医音频分析和电信等领域具有潜在贡献。", "keywords": "语音增强, 深度学习, 噪声抑制, 感知质量, 说话人识别", "comments": "本文通过对现有SOTA深度学习模型在语音增强任务上的比较评估，填补了该领域的一个重要研究空白，即模型在不同性能维度（噪声抑制、感知质量、说话人特征保留）上的权衡。其详细的量化结果为未来模型选择和开发提供了宝贵的参考，特别是在需要平衡多重性能指标的应用场景中。"}}
{"id": "2506.14901", "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction", "authors": ["Marija Šakota", "Robert West"], "summary": "Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14901v1", "AI": {"title_translation": "结合约束与非约束解码的Boosting方法：BoostCD及其在信息抽取中的应用", "tldr": "BoostCD通过结合约束解码和非约束解码来提高结构化NLP任务中自回归语言模型的输出质量，尤其在信息抽取中表现优异。", "motivation": "当前许多结构化NLP任务中，自回归语言模型在训练时无需感知约束，这虽有利于动态约束，但可能导致测试时约束解码的输出质量低下。", "method": "BoostCD采用两阶段方法：第一阶段，基础模型M进行两次解码（约束和非约束模式），得到两个弱预测；第二阶段，一个学习到的自回归增强模型将这两个弱预测结合成一个最终预测。该方法利用了基础模型在有无约束下所犯错误的互补性。", "result": "将BoostCD应用于封闭信息抽取任务，提出的BoostIE模型在分布内和分布外均优于现有方法，并解决了这些方法中常见的几个错误。", "conclusion": "BoostCD通过有效结合约束和非约束解码，显著提升了结构化NLP任务中自回归语言模型的性能，尤其在信息抽取领域表现突出。", "translation": "许多近期处理结构化自然语言处理（NLP）任务的方法使用自回归语言模型M将非结构化输入文本x映射到表示结构化对象（如元组、列表、树、代码等）的输出文本y，其中所需的输出结构通过约束解码强制执行。在训练期间，这些方法不要求模型感知约束，约束仅隐含在训练输出y中。这虽然有利于动态约束而无需重新训练，但可能导致测试时约束解码的输出质量低下。我们通过增强约束解码（Boosted Constrained Decoding, BoostCD）克服了这个问题，它分两个阶段结合了约束解码和非约束解码：第一阶段，基础模型M进行两次解码，分别在约束和非约束模式下进行，获得两个弱预测。在第二阶段，一个学习到的自回归增强模型将这两个弱预测结合成一个最终预测。基础模型在有无约束下所犯的错误往往是互补的，增强模型学习利用这一点来提高性能。我们通过将其应用于封闭信息抽取来展示BoostCD的强大功能。我们的模型BoostIE在分布内和分布外均优于现有方法，解决了这些方法中识别出的几个常见错误。", "summary": "BoostCD是一种新的解码方法，旨在解决自回归语言模型在结构化NLP任务中，因训练时不感知约束而导致的测试时约束解码质量问题。它通过两阶段过程结合了约束和非约束解码：首先，基础模型生成两个弱预测，然后一个增强模型将它们结合。该方法利用了两种解码模式下错误互补的特点，并在信息抽取任务中展示了优越性能，超越了现有方法。", "keywords": "约束解码, 非约束解码, 信息抽取, 自回归模型, Boosting", "comments": "该论文提出了一种新颖的解码策略BoostCD，通过巧妙地结合约束和非约束解码的弱预测，有效解决了当前自回归模型在结构化NLP任务中面临的输出质量问题。其创新点在于利用了不同解码模式下错误的互补性，并通过一个学习到的增强模型进行整合，避免了模型在训练时对约束的显式依赖，同时提升了测试性能。在信息抽取领域的成功应用证明了其普适性和有效性，对未来结构化预测任务具有重要参考价值。"}}
{"id": "2506.15034", "title": "MECHA: Multithreaded and Efficient Cryptographic Hardware Access", "authors": ["Pratama Derry", "Laksmono Agus Mahardika Ari", "Iqbal Muhammad", "Howon Kim"], "summary": "This paper presents a multithread and efficient cryptographic hardware access\n(MECHA) for efficient and fast cryptographic operations that eliminates the\nneed for context switching. Utilizing a UNIX domain socket, MECHA manages\nmultiple requests from multiple applications simultaneously, resulting in\nfaster processing and improved efficiency. We comprise several key components,\nincluding the Server thread, Client thread, Transceiver thread, and a pair of\nSender and Receiver queues. MECHA design is portable and can be used with any\ncommunication protocol, with experimental results demonstrating a 83% increase\nin the speed of concurrent cryptographic requests compared to conventional\ninterface design. MECHA architecture has significant potential in the field of\nsecure communication applications ranging from cloud computing to the IoT,\noffering a faster and more efficient solution for managing multiple\ncryptographic operation requests concurrently.", "comment": "4 Page", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15034v1", "AI": {"title_translation": "MECHA：多线程高效密码硬件访问", "tldr": "MECHA是一种多线程高效密码硬件访问方案，通过消除上下文切换和使用UNIX域套接字，显著提升了并发密码操作的速度和效率。", "motivation": "为了实现高效快速的密码操作，并消除上下文切换的需要。传统的接口设计在处理并发密码请求时效率低下。", "method": "本文提出了MECHA（多线程高效密码硬件访问）方案。它利用UNIX域套接字同时管理来自多个应用程序的多个请求。MECHA由服务器线程、客户端线程、收发线程以及一对发送和接收队列等关键组件组成。其设计具有可移植性，可与任何通信协议配合使用。", "result": "实验结果表明，与传统接口设计相比，MECHA将并发密码请求的速度提高了83%。", "conclusion": "MECHA架构在从云计算到物联网的安全通信应用领域具有巨大潜力，为并发管理多个密码操作请求提供了更快、更高效的解决方案。", "translation": "本文提出了一种多线程高效密码硬件访问（MECHA）方案，旨在实现高效快速的密码操作并消除上下文切换的需要。MECHA利用UNIX域套接字同时管理来自多个应用程序的多个请求，从而提高处理速度和效率。该方案包含几个关键组件，包括服务器线程、客户端线程、收发线程以及一对发送和接收队列。MECHA的设计具有可移植性，可与任何通信协议配合使用。实验结果表明，与传统接口设计相比，并发密码请求的速度提高了83%。MECHA架构在从云计算到物联网的安全通信应用领域具有巨大潜力，为并发管理多个密码操作请求提供了更快、更高效的解决方案。", "summary": "本文介绍了一种名为MECHA的多线程高效密码硬件访问方案，旨在通过消除上下文切换来加速密码操作。MECHA利用UNIX域套接字同时处理来自多个应用程序的请求，从而提升处理速度和效率。该方案由服务器、客户端、收发线程及队列等核心组件构成，并具备良好的可移植性。实验证明，MECHA能将并发密码请求的速度提升83%，在云计算和物联网等安全通信领域展现出巨大应用潜力。", "keywords": "密码硬件访问, 多线程, 效率, UNIX域套接字, 安全通信", "comments": "MECHA的创新点在于其多线程设计和利用UNIX域套接字来消除上下文切换，从而显著提升了并发密码操作的效率。其通用性和可移植性也增加了其在多种安全通信应用场景中的实用价值和重要性。"}}
{"id": "2506.15672", "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence", "authors": ["Yao Zhang", "Chenyang Lin", "Shijie Tang", "Haokun Chen", "Shijie Zhou", "Yunpu Ma", "Volker Tresp"], "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.", "comment": "41 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15672v1", "AI": {"title_translation": "SwarmAgentic：迈向基于群体智能的全自动化智能体系统生成", "tldr": "SwarmAgentic是一个受粒子群优化启发的全自动化框架，用于从零开始生成和优化智能体系统，并在复杂任务中表现优异。", "motivation": "现有智能体系统生成框架缺乏完全的自主性，未能实现从零开始的智能体生成、自我优化的智能体功能以及协作，这限制了其适应性和可扩展性。", "method": "提出SwarmAgentic框架，通过语言驱动的探索，从零开始构建智能体系统，并联合优化智能体功能和协作。它维护候选系统群体，并通过反馈引导的更新进行演化，灵感来源于粒子群优化（PSO）。", "result": "SwarmAgentic在六个真实世界的开放式探索性任务中优于所有基线，在TravelPlanner基准测试上相对于ADAS实现了+261.8%的相对改进。", "conclusion": "SwarmAgentic通过将群体智能与全自动化多智能体系统生成相结合，标志着可扩展和自主智能体系统设计迈出了重要一步。", "translation": "大型语言模型的快速发展推动了智能体系统在决策、协调和任务执行方面的进步。然而，现有的智能体系统生成框架缺乏完全的自主性，未能实现从零开始的智能体生成、自我优化的智能体功能以及协作，这限制了其适应性和可扩展性。我们提出了SwarmAgentic，一个用于全自动化智能体系统生成的框架，它从零开始构建智能体系统，并通过语言驱动的探索，将智能体功能和协作作为相互依赖的组件进行联合优化。为了实现对系统级结构的高效搜索，SwarmAgentic维护一个候选系统群体，并通过反馈引导的更新对其进行演化，这借鉴了粒子群优化（PSO）的灵感。我们在六个涉及高级规划、系统级协调和创造性推理的真实世界、开放式和探索性任务上评估了我们的方法。在仅给定任务描述和目标函数的情况下，SwarmAgentic优于所有基线，在TravelPlanner基准测试上相对于ADAS实现了+261.8%的相对改进，这突显了在结构不受约束的任务中全自动化的有效性。该框架标志着可扩展和自主智能体系统设计的重要一步，将群体智能与全自动化系统多智能体生成相结合。我们的代码已在https://yaoz720.github.io/SwarmAgentic/ 公开。", "summary": "SwarmAgentic是一个创新的全自动化框架，旨在解决现有智能体系统生成框架在自主性、从零开始生成、自我优化和协作方面的不足。它通过结合语言驱动的探索和受粒子群优化启发的群体演化机制，实现了智能体功能和协作的联合优化。实验结果表明，SwarmAgentic在复杂、开放式任务中显著优于现有基线，证明了其在实现可扩展和自主智能体系统设计方面的巨大潜力。", "keywords": "智能体系统生成, 群体智能, 大型语言模型, 全自动化, 粒子群优化", "comments": "该论文提出了一种新颖的方法，将群体智能（特别是受PSO启发）与大型语言模型相结合，以实现智能体系统的全自动化生成和优化。其创新点在于从系统层面而非单个智能体层面进行优化，并解决了现有框架在自主性和可扩展性方面的局限性。在无结构约束任务上的显著性能提升，预示了其在未来复杂AI系统设计中的重要性。"}}
{"id": "2506.15088", "title": "Program Feature-based Fuzzing Benchmarking", "authors": ["Miao Miao"], "summary": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15088v1", "AI": {"title_translation": "基于程序特征的模糊测试基准", "tldr": "本文引入了一种新的模糊测试基准，通过考虑细粒度的程序特征来评估模糊测试器的性能。", "motivation": "传统的模糊测试评估通常侧重于模糊测试器在目标程序集上的整体性能，但很少有基准考虑细粒度的程序特征如何影响模糊测试的有效性。", "method": "作者回顾了25项近期灰盒模糊测试研究，提取了7个与控制流和数据流相关的、可能影响模糊测试器性能的程序特征。利用这些特征，他们生成了一个包含153个程序、由10个细粒度可配置参数控制的基准，并使用此基准评估了11个流行的模糊测试器。", "result": "结果表明，模糊测试器的性能根据程序特征及其强度而显著变化。", "conclusion": "将程序特性纳入模糊测试评估中非常重要。", "translation": "模糊测试是一种强大的软件测试技术，以其在识别软件漏洞方面的有效性而闻名。传统的模糊测试评估通常侧重于模糊测试器在目标程序集上的整体性能，但很少有基准考虑细粒度的程序特征如何影响模糊测试的有效性。为了弥补这一差距，我们引入了一个新颖的基准，旨在生成具有可配置的细粒度程序特征的程序，以增强模糊测试评估。我们回顾了25项近期灰盒模糊测试研究，提取了7个与控制流和数据流相关的、可能影响模糊测试器性能的程序特征。利用这些特征，我们生成了一个包含153个程序、由10个细粒度可配置参数控制的基准。我们使用此基准评估了11个流行的模糊测试器。结果表明，模糊测试器的性能根据程序特征及其强度而显著变化，这突显了将程序特性纳入模糊测试评估中的重要性。", "summary": "该论文提出了一个基于程序特征的新型模糊测试基准，旨在解决传统评估中忽视细粒度程序特性对模糊测试有效性影响的问题。通过分析25项灰盒模糊测试研究，提取了7个关键程序特征，并据此生成了一个包含153个程序的基准。对11个流行模糊测试器的评估表明，其性能受程序特征及其强度的显著影响，强调了在模糊测试评估中考虑程序特性的必要性。", "keywords": "模糊测试, 基准测试, 程序特征, 软件测试, 漏洞识别", "comments": "本文的创新之处在于提出了一个基于细粒度程序特征的模糊测试基准，填补了现有评估方法的空白。它为更深入、更准确地理解模糊测试器性能提供了新的视角，对于指导模糊测试工具的开发和改进具有重要意义。通过量化程序特征对模糊测试有效性的影响，该工作为未来的研究提供了坚实的基础。"}}
{"id": "2506.14799", "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "summary": "Recent advances in AI has enabled automated analysis of complex media content\nat scale and generate actionable insights regarding character representation\nalong such dimensions as gender and age. Past work focused on quantifying\nrepresentation from audio/video/text using various ML models, but without\nhaving the audience in the loop. We ask, even if character distribution along\ndemographic dimensions are available, how useful are they to the general\npublic? Do they actually trust the numbers generated by AI models? Our work\naddresses these questions through a user study, while proposing a new AI-based\ncharacter representation and visualization tool. Our tool based on the\nContrastive Language Image Pretraining (CLIP) foundation model to analyze\nvisual screen data to quantify character representation across dimensions of\nage and gender. We also designed effective visualizations suitable for\npresenting such analytics to lay audience. Next, we conducted a user study to\nseek empirical evidence on the usefulness and trustworthiness of the\nAI-generated results for carefully chosen movies presented in the form of our\nvisualizations. We note that participants were able to understand the analytics\nfrom our visualization, and deemed the tool `overall useful'. Participants also\nindicated a need for more detailed visualizations to include more demographic\ncategories and contextual information of the characters. Participants' trust in\nAI-based gender and age models is seen to be moderate to low, although they\nwere not against the use of AI in this context. Our tool including code,\nbenchmarking, and data from the user study can be found here:\nhttps://anonymous.4open.science/r/Character-Representation-Media-FF7B", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14799v1", "AI": {"title_translation": "使用多模态基础模型分析媒体内容中的角色表征：有效性与信任", "tldr": "本研究通过用户研究评估了使用多模态基础模型分析媒体内容中角色表征的有效性和信任度，并提出了一个新工具。", "motivation": "过去的媒体内容角色表征分析工作缺乏观众参与，因此本研究旨在探讨AI模型生成的角色人口统计数据对公众的实用性和信任度。", "method": "研究提出了一种基于CLIP多模态基础模型的新型AI角色表征和可视化工具，用于量化视觉屏幕数据中的年龄和性别表征。随后，通过用户研究评估了该工具生成结果的有用性和可信度。", "result": "参与者能够理解可视化分析结果，并认为该工具“总体有用”。然而，参与者也表示需要更详细的可视化，包含更多人口统计类别和上下文信息。参与者对AI性别和年龄模型的信任度为中低等，但他们并不反对在该情境下使用AI。", "conclusion": "本研究表明，使用AI工具分析媒体内容中的角色表征对用户是有效且有用的，但AI生成结果的信任度仍有待提高，且需要更详细和全面的可视化呈现。", "translation": "人工智能的最新进展使得大规模自动化分析复杂媒体内容并生成关于角色表征（如性别和年龄维度）的可操作洞察成为可能。过去的工作侧重于使用各种机器学习模型从音频/视频/文本中量化表征，但没有让观众参与其中。我们提出疑问，即使角色在人口统计维度的分布可用，它们对公众有多大用处？他们真的信任人工智能模型生成的数据吗？我们的工作通过一项用户研究来解决这些问题，同时提出了一种新的人工智能角色表征和可视化工具。我们的工具基于对比语言图像预训练（CLIP）基础模型，分析视觉屏幕数据以量化年龄和性别维度上的角色表征。我们还设计了适合向普通受众呈现此类分析的有效可视化。接下来，我们进行了一项用户研究，旨在为以我们可视化形式呈现的精心选择的电影中人工智能生成结果的有用性和可信度寻求经验证据。我们注意到参与者能够理解我们可视化中的分析结果，并认为该工具“总体有用”。参与者还表示需要更详细的可视化，以包含更多人口统计类别和角色的上下文信息。参与者对基于人工智能的性别和年龄模型的信任度为中低等，尽管他们不反对在此情境下使用人工智能。我们的工具，包括代码、基准测试和用户研究数据，可在此处找到：https://anonymous.4open.science/r/Character-Representation-Media-FF7B", "summary": "本研究提出并评估了一个基于CLIP多模态基础模型的AI工具，用于分析媒体内容中的角色表征（年龄和性别），并设计了相应的可视化界面。通过用户研究发现，该工具被认为是“总体有用”的，用户能够理解其分析结果。然而，研究也揭示了用户对AI生成结果的信任度中低，并提出了对更详细可视化和更多人口统计信息的需求。", "keywords": "角色表征, 多模态基础模型, 用户研究, 有效性, 信任", "comments": "该研究的创新之处在于将用户研究引入AI驱动的角色表征分析，直接探讨了AI分析结果对公众的实用性和信任度，填补了现有研究的空白。其提出的新工具和可视化设计具有实用价值。然而，研究也揭示了AI在公众信任度方面仍面临挑战，未来工作需关注如何提升AI结果的透明度和可信度。"}}
{"id": "2506.15168", "title": "Algorithmic resolution of crowd-sourced moderation on X in polarized settings across countries", "authors": ["Paul Bouchaud", "Pedro Ramaciotti"], "summary": "Social platforms increasingly transition from expert fact-checking to\ncrowd-sourced moderation, with X pioneering this shift through its Community\nNotes system, enabling users to collaboratively moderate misleading content. To\nresolve conflicting moderation, Community Notes learns a latent ideological\ndimension and selects notes garnering cross-partisan support. As this system,\ndesigned for and evaluated in the United States, is now deployed worldwide, we\nevaluate its operation across diverse polarization contexts. We analyze 1.9\nmillion moderation notes with 135 million ratings from 1.2 million users,\ncross-referencing ideological scaling data across 13 countries. Our results\nshow X's Community Notes effectively captures each country's main polarizing\ndimension but fails by design to moderate the most polarizing content, posing\npotential risks to civic discourse and electoral processes.", "comment": "46 pages", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.15168v1", "AI": {"title_translation": "跨国极化环境下X平台上众包审核的算法解析", "tldr": "研究X平台（推特）的社区笔记系统在不同国家极化环境下的运作，发现其能识别极化维度但无法有效审核最具极化内容，可能对公共讨论构成风险。", "motivation": "社交平台正从专家事实核查转向众包审核，X平台的社区笔记系统已在全球部署，因此需要评估其在不同极化背景下的运作效果。", "method": "分析了190万条审核笔记、1.35亿条评分和120万用户的数据，并交叉引用了13个国家的意识形态量表数据。", "result": "X的社区笔记系统能有效捕捉每个国家主要的极化维度，但设计上未能审核最具极化内容。", "conclusion": "X的社区笔记系统在审核最具极化内容方面存在不足，可能对公民话语和选举过程构成潜在风险。", "translation": "社交平台正日益从专家事实核查转向众包审核，X平台通过其社区笔记系统率先实现了这一转变，该系统使用户能够协作审核误导性内容。为了解决冲突的审核，社区笔记学习了一个潜在的意识形态维度，并选择获得跨党派支持的笔记。由于该系统最初是为美国设计和评估的，现在已在全球部署，我们评估了其在不同极化背景下的运作。我们分析了190万条审核笔记，包含来自120万用户的1.35亿条评分，并交叉引用了13个国家的意识形态量表数据。我们的结果显示，X的社区笔记系统能有效捕捉每个国家主要的极化维度，但设计上未能审核最具极化内容，这可能对公民话语和选举过程构成潜在风险。", "summary": "本文评估了X平台（推特）的社区笔记系统在全球不同极化环境下的表现。研究分析了大量用户审核数据，发现该系统虽然能识别各国的核心极化维度，但其设计缺陷导致无法有效审核最具争议和极化的内容，这给公民话语和选举过程带来了潜在风险。", "keywords": "众包审核, X平台, 社区笔记, 极化, 跨国评估", "comments": "该研究揭示了众包审核系统在全球部署时可能面临的局限性，特别是在处理高度极化内容方面的挑战。其重要性在于指出了社交媒体平台在推广去中心化审核机制时需关注的潜在风险，即可能无法有效应对社会分裂问题，反而可能加剧。"}}
{"id": "2506.14937", "title": "Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders", "authors": ["Luan Gonçalves Miranda", "Pedro Ivo da Cruz", "Murilo Bellezoni Loiola"], "summary": "Currently, digital security mechanisms like Anomaly Detection Systems using\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\nseparation threshold to classify the extracted reconstruction error, the\ndefinition of this threshold directly impacts the performance of the detection\nprocess. Thus, this work proposes the automatic definition of this threshold\nusing some machine learning algorithms. For this, three algorithms were\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.", "comment": "This work was accepted at SBrT 2022 (Brazilian Symposium on\n  Telecommunications and Signal Processing), though it was not included in the\n  official proceedings. in Portuguese language", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14937v1", "AI": {"title_translation": "使用自编码器自动确定计算机网络攻击检测阈值", "tldr": "本文提出了一种使用机器学习算法自动确定自编码器在网络攻击检测中重建误差分离阈值的方法，以解决传统方法中阈值定义对性能影响大的问题。", "motivation": "目前，使用自编码器（AE）的异常检测系统在处理数据不平衡等问题上显示出巨大潜力。然而，AE使用非平凡且非标准化的分离阈值来分类重建误差，该阈值的定义直接影响检测过程的性能。", "method": "本工作提出使用机器学习算法自动定义该阈值。为此，评估了三种算法：K近邻（K-Nearst Neighbors）、K均值（K-Means）和支持向量机（Support Vector Machine）。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "目前，使用自编码器（AE）的异常检测系统等数字安全机制在规避数据固有的问题（如数据不平衡）方面显示出巨大潜力。由于AE使用非平凡且非标准化的分离阈值来分类提取的重建误差，因此该阈值的定义直接影响检测过程的性能。因此，本工作提出使用一些机器学习算法自动定义该阈值。为此，评估了三种算法：K近邻、K均值和支持向量机。", "summary": "本文旨在解决使用自编码器（AE）进行网络异常检测时，重建误差分离阈值定义困难且影响性能的问题。研究提出并评估了K近邻、K均值和支持向量机三种机器学习算法，以实现该阈值的自动确定，从而提高检测过程的有效性。", "keywords": "自编码器, 异常检测, 阈值确定, 机器学习, 网络安全", "comments": "该论文关注自编码器在网络异常检测中的一个关键挑战：阈值设定。提出使用机器学习算法自动优化这一过程，具有一定的创新性，可以提高检测系统的实用性和性能。然而，抽象中未提供实验结果，无法评估其有效性。"}}
{"id": "2506.14865", "title": "Efficient and Real-Time Motion Planning for Robotics Using Projection-Based Optimization", "authors": ["Xuemin Chi", "Hakan Girgin", "Tobias Löw", "Yangyang Xie", "Teng Xue", "Jihao Huang", "Cheng Hu", "Zhitao Liu", "Sylvain Calinon"], "summary": "Generating motions for robots interacting with objects of various shapes is a\ncomplex challenge, further complicated by the robot geometry and multiple\ndesired behaviors. While current robot programming tools (such as inverse\nkinematics, collision avoidance, and manipulation planning) often treat these\nproblems as constrained optimization, many existing solvers focus on specific\nproblem domains or do not exploit geometric constraints effectively. We propose\nan efficient first-order method, Augmented Lagrangian Spectral Projected\nGradient Descent (ALSPG), which leverages geometric projections via Euclidean\nprojections, Minkowski sums, and basis functions. We show that by using\ngeometric constraints rather than full constraints and gradients, ALSPG\nsignificantly improves real-time performance. Compared to second-order methods\nlike iLQR, ALSPG remains competitive in the unconstrained case. We validate our\nmethod through toy examples and extensive simulations, and demonstrate its\neffectiveness on a 7-axis Franka robot, a 6-axis P-Rob robot and a 1:10 scale\ncar in real-world experiments. Source codes, experimental data and videos are\navailable on the project webpage: https://sites.google.com/view/alspg-oc", "comment": "submitted to IROS 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14865v1", "AI": {"title_translation": "基于投影优化的机器人高效实时运动规划", "tldr": "本文提出了一种名为ALSPG的有效一阶方法，通过利用几何投影显著提高了机器人运动规划的实时性能，并在多种机器人上进行了验证。", "motivation": "为与各种形状物体交互的机器人生成运动是一个复杂的挑战，现有许多求解器专注于特定问题领域或未能有效利用几何约束，导致实时性能不足。", "method": "本文提出了一种名为增强拉格朗日谱投影梯度下降（ALSPG）的有效一阶方法。该方法通过欧几里得投影、闵可夫斯基和以及基函数来利用几何投影，并使用几何约束而非完整约束和梯度。", "result": "ALSPG显著提高了实时性能。与iLQR等二阶方法相比，ALSPG在无约束情况下仍具有竞争力。该方法通过玩具示例和大量仿真进行了验证，并在7轴Franka机器人、6轴P-Rob机器人和1:10比例汽车的实际实验中展示了其有效性。", "conclusion": "通过利用几何约束而非完整约束和梯度，所提出的ALSPG方法显著提高了机器人运动规划的实时性能，并在实际应用中表现出有效性。", "translation": "为与各种形状物体交互的机器人生成运动是一个复杂的挑战，机器人几何形状和多种期望行为使问题进一步复杂化。虽然当前的机器人编程工具（如逆运动学、避撞和操作规划）通常将这些问题视为约束优化，但许多现有求解器专注于特定问题领域或未能有效利用几何约束。我们提出了一种高效的一阶方法，即增强拉格朗日谱投影梯度下降（ALSPG），该方法通过欧几里得投影、闵可夫斯基和以及基函数来利用几何投影。我们表明，通过使用几何约束而非完整约束和梯度，ALSPG显著提高了实时性能。与iLQR等二阶方法相比，ALSPG在无约束情况下仍具有竞争力。我们通过玩具示例和大量仿真验证了我们的方法，并在7轴Franka机器人、6轴P-Rob机器人和1:10比例汽车的实际实验中展示了其有效性。源代码、实验数据和视频可在项目网页上获取：https://sites.google.com/view/alspg-oc", "summary": "本文提出了一种名为增强拉格朗日谱投影梯度下降（ALSPG）的有效一阶方法，用于解决机器人与复杂物体交互时的运动规划挑战。ALSPG通过利用欧几里得投影、闵可夫斯基和及基函数等几何投影，并采用几何约束而非完整约束和梯度，显著提升了运动规划的实时性能。研究表明，该方法在无约束情况下与二阶方法iLQR保持竞争力。ALSPG已通过仿真和在多种实际机器人上的实验验证了其有效性。", "keywords": "机器人运动规划, 实时, 投影优化, ALSPG, 几何约束", "comments": "本文的创新点在于提出了一种高效的一阶优化方法ALSPG，通过巧妙地利用几何投影和几何约束，而非传统的完整约束和梯度，显著提升了机器人运动规划的实时性。这对于需要快速响应和复杂环境交互的机器人应用具有重要意义。该方法在实际机器人上的验证也增加了其说服力。"}}
{"id": "2506.15073", "title": "Linear and Numerical SDoF Bounds of Active RIS-Assisted MIMO Wiretap Interference Channel", "authors": ["Su Linfan", "Miao Yuhang", "Song Yuxuan", "Zheng Shuo", "Zhang Tong", "Xu Yinfei", "Wang Shuai", "Li Na"], "summary": "The multiple-input multiple-output (MIMO) wiretap interference channel (IC)\nserves as a canonical model for information-theoretic security, where a\nmultiple-antenna eavesdropper attempts to intercept communications in a\ntwo-user MIMO IC system. The secure degrees-of-freedom (SDoF) of an active\nreconfigurable intelligent surface (RIS)-assisted MIMO wiretap IC is with\npractical interests but remains unexplored. In this paper, we establish both\nsum-SDoF lower and upper bounds through linear beamforming conditions and\nnumerical methods. Specifically, our proposed lower bound is derived from\ntransmission scheme design and corresponding solutions to the sum-SDoF\nmaximization problem, formulated by linear integer programming. The solutions\nto this optimization problem addresses RIS element allocation for leakage and\ninterference cancellation. The proposed upper bound is obtained by solving a\nnuclear norm minimization problem, leveraging the fact that nuclear norm serves\nas a convex relaxation of the rank function. For symmetry antenna\nconfigurations, we derive a closed-form lower bound. Extensive numerical\nsimulations show that our proposed lower and upper bounds coincide across many\nantenna configurations, and our proposed lower bound outperforms the existing\nbenchmark.", "comment": "11 pages, 2 figures, submitted to IEEE OJCOMS", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15073v1", "AI": {"title_translation": "有源RIS辅助MIMO窃听干扰信道的线性和数值SDoF界", "tldr": "本文研究了有源RIS辅助MIMO窃听干扰信道的安全自由度（SDoF），并提出了线性和数值方法来建立SDoF的上下界。仿真结果表明所提出的界限在多种天线配置下吻合良好，并且下界优于现有基准。", "motivation": "多输入多输出（MIMO）窃听干扰信道是信息论安全性的一个典型模型，但有源可重构智能表面（RIS）辅助MIMO窃听干扰信道的安全自由度（SDoF）虽然具有实际意义但尚未被探索。", "method": "本文通过线束成形条件和数值方法建立了总SDoF的下界和上界。下界通过传输方案设计和线性整数规划公式化的SDoF最大化问题的解推导得出，该解解决了RIS元件分配以消除泄漏和干扰。上界通过解决核范数最小化问题获得，利用核范数是秩函数的凸松弛这一事实。对于对称天线配置，推导出了闭合形式的下界。", "result": "广泛的数值仿真表明，所提出的下界和上界在许多天线配置下吻合，并且所提出的下界优于现有基准。", "conclusion": "本文成功建立了有源RIS辅助MIMO窃听干扰信道的安全自由度（SDoF）的线性和数值上下界，并通过仿真验证了其有效性和优越性。", "translation": "多输入多输出（MIMO）窃听干扰信道是信息论安全性的一个典型模型，其中多天线窃听者试图截获双用户MIMO干扰信道系统中的通信。有源可重构智能表面（RIS）辅助MIMO窃听干扰信道的安全自由度（SDoF）具有实际意义，但尚未被探索。在本文中，我们通过线束成形条件和数值方法建立了总SDoF的下界和上界。具体而言，我们提出的下界是通过传输方案设计和线性整数规划公式化的总SDoF最大化问题的相应解推导出来的。该优化问题的解解决了RIS元件分配以消除泄漏和干扰。提出的上界是通过解决核范数最小化问题获得的，利用核范数是秩函数的凸松弛这一事实。对于对称天线配置，我们推导出了闭合形式的下界。广泛的数值仿真表明，我们提出的下界和上界在许多天线配置下吻合，并且我们提出的下界优于现有基准。", "summary": "本文研究了有源可重构智能表面（RIS）辅助的多输入多输出（MIMO）窃听干扰信道的安全自由度（SDoF）。针对这一未被探索的领域，作者通过线性波束成形和数值方法建立了总SDoF的上下界。下界通过线性整数规划解决的传输方案设计和SDoF最大化问题获得，旨在消除泄漏和干扰；上界则通过解决核范数最小化问题得到。研究还推导了对称天线配置下的闭合形式下界。仿真结果表明，所提出的界限在多种天线配置下吻合良好，并且下界性能优于现有基准。", "keywords": "安全自由度, 有源RIS, MIMO窃听信道, 上下界, 核范数最小化", "comments": "该论文创新性地将有源RIS引入到MIMO窃听干扰信道中，并首次对其SDoF进行探索。其贡献在于通过结合线性规划和凸优化技术，为SDoF的分析提供了严谨的数学框架，特别是提出了实用的上下界推导方法。数值仿真结果有力地支持了所提方法的有效性和优越性，对未来安全通信系统的设计具有重要指导意义。"}}
{"id": "2506.15316", "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor", "authors": ["Benoit Tain", "Raphael Millet", "Romain Lemaire", "Michal Szczepanski", "Laurent Alacoque", "Emmanuel Pluchart", "Sylvain Choisnet", "Rohit Prasad", "Jerome Chossat", "Pascal Pierunek", "Pascal Vivet", "Sebastien Thuries"], "summary": "This paper presents J3DAI, a tiny deep neural network-based hardware\naccelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial\nintelligence (AI) chip integrating a Deep Neural Network (DNN)-based\naccelerator. The DNN accelerator is designed to efficiently perform neural\nnetwork tasks such as image classification and segmentation. This paper focuses\non the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)\ncharacteristics and showcasing advanced edge AI capabilities on a CMOS image\nsensor. To support hardware, we utilized the Aidge comprehensive software\nframework, which enables the programming of both the host processor and the DNN\naccelerator. Aidge supports post-training quantization, significantly reducing\nmemory footprint and computational complexity, making it crucial for deploying\nmodels on resource-constrained hardware like J3DAI. Our experimental results\ndemonstrate the versatility and efficiency of this innovative design in the\nfield of edge AI, showcasing its potential to handle both simple and\ncomputationally intensive tasks. Future work will focus on further optimizing\nthe architecture and exploring new applications to fully leverage the\ncapabilities of J3DAI. As edge AI continues to grow in importance, innovations\nlike J3DAI will play a crucial role in enabling real-time, low-latency, and\nenergy-efficient AI processing at the edge.", "comment": "Preprint from ISLPED 2025. 979-8-3315-2710-5/25/$31.00\n  \\c{opyright}2025 IEEE", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15316v1", "AI": {"title_translation": "J3DAI：一种用于3D堆叠CMOS图像传感器的微型基于DNN的边缘AI加速器", "tldr": "J3DAI是一种微型DNN加速器，集成在3D堆叠CMOS图像传感器中，用于高效的边缘AI处理，支持图像分类和分割，并通过Aidge软件框架实现优化，展示了在资源受限硬件上的多功能性和效率。", "motivation": "随着边缘AI的重要性日益增长，需要在边缘设备上实现实时、低延迟和高能效的AI处理。该论文旨在解决在资源受限硬件（如3D堆叠CMOS图像传感器）上部署高效深度学习模型的需求。", "method": "本文提出了J3DAI，一个基于深度神经网络的硬件加速器，集成在三层3D堆叠CMOS图像传感器中。它专注于数字系统设计，并利用Aidge综合软件框架来支持主机处理器和DNN加速器的编程，Aidge还支持后训练量化以减少内存占用和计算复杂性。", "result": "实验结果表明，J3DAI在边缘AI领域具有多功能性和效率，能够处理简单和计算密集型任务。它展示了在CMOS图像传感器上先进的边缘AI能力。", "conclusion": "J3DAI作为一种创新的边缘AI加速器，在3D堆叠CMOS图像传感器中展现了在资源受限环境下进行高效AI处理的巨大潜力，将在未来的实时、低延迟和高能效边缘AI中发挥关键作用。", "translation": "本文介绍了J3DAI，一种用于三层3D堆叠CMOS图像传感器的微型深度神经网络硬件加速器，该传感器集成了一个基于深度神经网络（DNN）加速器的人工智能（AI）芯片。该DNN加速器旨在高效执行图像分类和分割等神经网络任务。本文重点介绍J3DAI的数字系统，强调其性能-功耗-面积（PPA）特性，并展示了在CMOS图像传感器上先进的边缘AI能力。为了支持硬件，我们利用了Aidge综合软件框架，该框架能够对主机处理器和DNN加速器进行编程。Aidge支持后训练量化，显著减少内存占用和计算复杂性，这对于在J3DAI等资源受限硬件上部署模型至关重要。我们的实验结果证明了这种创新设计在边缘AI领域的多功能性和效率，展示了其处理简单和计算密集型任务的潜力。未来的工作将侧重于进一步优化架构并探索新的应用，以充分利用J3DAI的能力。随着边缘AI的重要性持续增长，像J3DAI这样的创新将在实现边缘实时、低延迟和高能效AI处理方面发挥关键作用。", "summary": "本文介绍了J3DAI，一种专为3D堆叠CMOS图像传感器设计的微型DNN硬件加速器。J3DAI旨在高效执行图像分类和分割等边缘AI任务，并集成了一个AI芯片。该研究重点关注J3DAI的数字系统及其PPA特性，并展示了其先进的边缘AI能力。通过使用Aidge软件框架支持后训练量化，J3DAI显著减少了内存占用和计算复杂性，使其适用于资源受限的硬件。实验结果验证了J3DAI在边缘AI应用中的多功能性和效率。", "keywords": "边缘AI, DNN加速器, 3D堆叠CMOS图像传感器, 后训练量化, J3DAI", "comments": "J3DAI的创新之处在于其将微型DNN加速器直接集成到3D堆叠CMOS图像传感器中，实现了高度集成的边缘AI解决方案。这种设计对于资源受限的边缘设备至关重要，因为它通过硬件-软件协同优化（结合Aidge框架的量化）显著提升了能效和处理能力。其重要性在于推动了实时、低延迟和高能效的边缘AI应用，特别是在图像处理领域。"}}
{"id": "2506.14981", "title": "Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions", "authors": ["Hailiang Zhang", "Dieu My T. Nguyen", "Christine Smit", "Mahabal Hegde"], "summary": "Data analysis on massive multi-dimensional data, such as high-resolution\nlarge-region time averaging or area averaging for geospatial data, often\ninvolves calculations over a significant number of data points. While\nperforming calculations in scalable and flexible distributed or cloud\nenvironments is a viable option, a full scan of large data volumes still serves\nas a computationally intensive bottleneck, leading to significant cost. This\npaper introduces a generic and comprehensive method to address these\ncomputational challenges. This method generates a small, size-tunable\nsupplementary dataset that stores the cumulative sums along specific subset\ndimensions on top of the raw data. This minor addition unlocks rapid and cheap\nhigh-resolution large-region data analysis, making calculations over large\nnumbers of data points feasible with small instances or even microservices in\nthe cloud. This method is general-purpose, but is particularly well-suited for\ndata stored in chunked, cloud-optimized formats and for services running in\ndistributed or cloud environments. We present a Zarr extension proposal to\nintegrate the specifications of this method and facilitate its straightforward\nimplementation in general-purpose software applications. Benchmark tests\ndemonstrate that this method, implemented in Amazon Web services (AWS),\nsignificantly outperforms the brute-force approach used in on-premises\nservices. With just 5% supplemental storage, this method achieves a performance\nthat is 3-4 orders of magnitude (~10,000 times) faster than the brute-force\napproach, while incurring significantly reduced computational costs.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14981v1", "AI": {"title_translation": "基于Zarr的降维块级累积和", "tldr": "本文提出了一种基于Zarr的通用方法，通过生成一个小的辅助数据集来存储特定子维度上的累积和，从而显著加速大规模多维数据的分析，并在计算成本上实现大幅降低，性能比暴力方法快3-4个数量级。", "motivation": "对大规模多维数据进行分析（如高分辨率大区域时间平均或地理空间数据的区域平均）涉及大量数据点的计算。在可扩展和灵活的分布式或云环境中执行计算是可行的，但对大数据量进行全面扫描仍然是计算密集型瓶颈，导致显著成本。", "method": "该方法引入了一个通用且全面的方法，通过在原始数据之上生成一个小的、大小可调的辅助数据集，该数据集沿特定子维度存储累积和。该方法还提出了一个Zarr扩展提案，以集成其规范并促进在通用软件应用中的实现。", "result": "基准测试表明，该方法在Amazon Web Services (AWS)中实现时，显著优于本地服务中使用的暴力方法。仅需5%的额外存储，该方法即可实现比暴力方法快3-4个数量级（约10,000倍）的性能，同时显著降低计算成本。", "conclusion": "通过引入一个存储累积和的辅助数据集，本文提出的基于Zarr的方法极大地提高了大规模多维数据分析的速度和效率，解决了传统方法中计算密集型瓶颈和高成本问题，尤其适用于分块、云优化的数据格式和分布式/云环境。", "translation": "对大规模多维数据进行分析，例如高分辨率大区域时间平均或地理空间数据的区域平均，通常涉及大量数据点的计算。尽管在可扩展和灵活的分布式或云环境中执行计算是可行的选择，但对大量数据进行全面扫描仍然是计算密集型瓶颈，导致显著成本。本文介绍了一种通用且全面的方法来解决这些计算挑战。该方法在原始数据之上生成一个小的、大小可调的辅助数据集，该数据集沿特定子维度存储累积和。这一微小的补充解锁了快速且廉价的高分辨率大区域数据分析，使得在云中使用小型实例甚至微服务对大量数据点进行计算成为可能。该方法是通用目的的，但特别适用于以分块、云优化格式存储的数据以及在分布式或云环境中运行的服务。我们提出了一个Zarr扩展提案，以整合该方法的规范并促进其在通用软件应用程序中的直接实现。基准测试表明，该方法在Amazon Web Services (AWS)中实现时，显著优于本地服务中使用的暴力方法。仅需5%的额外存储，该方法即可实现比暴力方法快3-4个数量级（约10,000倍）的性能，同时显著降低计算成本。", "summary": "本文提出了一种基于Zarr的创新方法，旨在解决大规模多维数据分析中的计算瓶颈和高成本问题。通过生成一个小的辅助数据集来存储特定子维度上的累积和，该方法显著加速了高分辨率大区域数据分析。基准测试显示，在AWS中，该方法仅用5%的额外存储，就比传统暴力方法快3-4个数量级，并大幅降低了计算成本，特别适用于云环境和分块数据格式。", "keywords": "Zarr, 累积和, 多维数据, 云计算, 性能优化", "comments": "本文提出的方法通过引入一个小的、可调大小的辅助数据集来存储累积和，巧妙地解决了大规模多维数据分析中的计算效率瓶颈和高成本问题。其创新点在于利用Zarr等云优化格式的特性，实现了与原始数据分离但紧密关联的预计算，从而在分布式和云环境中获得了惊人的性能提升（10,000倍）。这种方法对于处理地球科学、大数据分析等领域的海量数据集具有重要意义，能够使得原本计算昂贵的操作变得可行且经济。该方法通用性强，且有Zarr扩展提案支持，预示着其在未来数据分析工具中的广泛应用潜力。"}}
{"id": "2506.14816", "title": "A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection", "authors": ["Alavikunhu Panthakkan", "Zubair Medammal", "S M Anzar", "Fatma Taher", "Hussain Al-Ahmad"], "summary": "Falconry, a revered tradition involving the training and hunting with\nfalcons, requires meticulous health surveillance to ensure the health and\nsafety of these prized birds, particularly in hunting scenarios. This paper\npresents an innovative method employing a hybrid of ConvNeXt and EfficientNet\nAI models for the classification of falcon diseases. The study focuses on\naccurately identifying three conditions: Normal, Liver Disease and\n'Aspergillosis'. A substantial dataset was utilized for training and validating\nthe model, with an emphasis on key performance metrics such as accuracy,\nprecision, recall, and F1-score. Extensive testing and analysis have shown that\nour concatenated AI model outperforms traditional diagnostic methods and\nindividual model architectures. The successful implementation of this hybrid AI\nmodel marks a significant step forward in precise falcon disease detection and\npaves the way for future developments in AI-powered avian healthcare solutions.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14816v1", "AI": {"title_translation": "一种用于精确猎鹰疾病检测的混合ConvNeXt-EfficientNet人工智能解决方案", "tldr": "本文提出了一种结合ConvNeXt和EfficientNet的混合AI模型，用于精确检测猎鹰疾病，并在性能上超越传统方法和单一模型。", "motivation": "猎鹰饲养传统要求对猎鹰进行细致的健康监测，以确保这些珍贵鸟类的健康和安全，尤其是在狩猎场景中。传统的诊断方法可能效率不足或准确性有限。", "method": "本研究采用ConvNeXt和EfficientNet AI模型的混合体来对猎鹰疾病进行分类。研究重点是准确识别三种疾病：正常、肝病和曲霉病。使用了大量数据集进行模型训练和验证，并强调了准确性、精确度、召回率和F1分数等关键性能指标。", "result": "广泛的测试和分析表明，所提出的混合AI模型在性能上优于传统的诊断方法和单独的模型架构。", "conclusion": "这种混合AI模型的成功实施标志着猎鹰疾病精确检测方面迈出了重要一步，并为未来AI驱动的禽类医疗解决方案的发展铺平了道路。", "translation": "猎鹰饲养是一种受人尊敬的传统，涉及训练和使用猎鹰进行狩猎，需要细致的健康监测，以确保这些珍贵鸟类的健康和安全，尤其是在狩猎场景中。本文提出了一种创新方法，采用ConvNeXt和EfficientNet AI模型的混合体来对猎鹰疾病进行分类。该研究侧重于准确识别三种疾病：正常、肝病和“曲霉病”。使用了大量数据集进行模型训练和验证，并强调了准确性、精确度、召回率和F1分数等关键性能指标。广泛的测试和分析表明，我们结合的AI模型优于传统的诊断方法和单独的模型架构。这种混合AI模型的成功实施标志着猎鹰疾病精确检测方面迈出了重要一步，并为未来AI驱动的禽类医疗解决方案的发展铺平了道路。", "summary": "本文提出了一种结合ConvNeXt和EfficientNet的混合人工智能模型，用于精确检测猎鹰的肝病和曲霉病。研究利用大量数据集进行训练和验证，结果显示该混合模型在准确性、精确度、召回率和F1分数等关键指标上均优于传统诊断方法和单一模型架构。这项工作为AI在禽类医疗领域的应用开辟了新途径。", "keywords": "猎鹰疾病检测, ConvNeXt, EfficientNet, 混合AI模型, 禽类医疗", "comments": "该论文的创新点在于结合了两种先进的深度学习架构ConvNeXt和EfficientNet，形成混合模型以提升疾病检测的准确性。其重要性体现在为猎鹰这种特殊且珍贵的动物提供了更精确、高效的疾病诊断方案，这对于保护动物健康和促进相关传统活动具有积极意义。该研究为AI在动物医疗领域的应用提供了有价值的案例。"}}
{"id": "2506.15050", "title": "Truncated Proximal Policy Optimization", "authors": ["Tiantian Fan", "Lingjun Liu", "Yu Yue", "Jiaze Chen", "Chengyi Wang", "Qiying Yu", "Chi Zhang", "Zhiqi Lin", "Ruofei Zhu", "Yufeng Yuan", "Xiaochen Zuo", "Bole Ma", "Mofan Zhang", "Gaohong Liu", "Ru Zhang", "Haotian Zhou", "Cong Xie", "Ruidong Zhu", "Zhi Zhang", "Xin Liu", "Mingxuan Wang", "Lin Yan", "Yonghui Wu"], "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15050v1", "AI": {"title_translation": "截断近端策略优化", "tldr": "T-PPO是一种新的PPO变体，通过简化策略更新和限制响应长度，显著提高了大型语言模型（LLMs）的训练效率，解决了PPO在长响应生成中耗时的问题。", "motivation": "现有的近端策略优化（PPO）及其变体在训练大型语言模型（LLMs）时，由于其固有的在线策略性质，尤其是在生成长响应时，会非常耗时，并且存在硬件利用率低的问题，即在等待完整Rollout期间资源闲置。", "method": "本文提出了截断近端策略优化（T-PPO）。其主要贡献有两点：1. 提出扩展广义优势估计（EGAE），用于从不完整响应中进行优势估计，同时保持策略学习的完整性。2. 设计了一种计算优化的机制，允许策略模型和价值模型独立优化，通过选择性过滤提示和截断的Tokens，减少冗余计算并加速训练过程。", "result": "在AIME 2024上使用32B基础模型进行实验，结果表明T-PPO将推理LLMs的训练效率提高了2.5倍，并且优于现有竞争对手。", "conclusion": "T-PPO通过优化策略更新和响应生成过程，显著提升了大型语言模型在推理任务上的训练效率，并展现出优越的性能。", "translation": "最近，在测试时扩展的大型语言模型（LLMs）通过生成长链式思维（CoT），在科学和专业任务中展示了卓越的推理能力。作为开发这些推理模型的关键组成部分，以近端策略优化（PPO）及其变体为代表的强化学习（RL），允许模型通过试错学习。然而，PPO由于其固有的在线策略性质，可能会非常耗时，而响应长度的增加进一步加剧了这一问题。在这项工作中，我们提出了截断近端策略优化（T-PPO），这是PPO的一种新颖扩展，通过简化策略更新和限制响应生成来提高训练效率。T-PPO缓解了硬件利用率低的问题，这是完全同步长生成过程的固有缺陷，其中资源在等待完整Rollout期间经常处于闲置状态。我们的贡献是双重的。首先，我们提出了扩展广义优势估计（EGAE），用于从不完整响应中进行优势估计，同时保持策略学习的完整性。其次，我们设计了一种计算优化的机制，允许策略和价值模型独立优化。通过选择性过滤提示和截断的Tokens，该机制减少了冗余计算，并在不牺牲收敛性能的情况下加速了训练过程。我们在AIME 2024上使用32B基础模型展示了T-PPO的有效性和功效。实验结果表明，T-PPO将推理LLMs的训练效率提高了2.5倍，并优于现有竞争对手。", "summary": "本文提出了一种名为截断近端策略优化（T-PPO）的新型强化学习算法，旨在解决传统近端策略优化（PPO）在训练大型语言模型（LLMs）时因长响应生成而导致的训练效率低下问题。T-PPO通过引入扩展广义优势估计（EGAE）来处理不完整响应的优势估计，并设计了一种计算优化的机制，允许策略和价值模型独立优化，从而减少冗余计算。实验结果表明，T-PPO能够将推理LLMs的训练效率提高高达2.5倍，并优于现有竞争算法。", "keywords": "近端策略优化, 大型语言模型, 训练效率, 强化学习, 截断策略优化", "comments": "T-PPO的创新点在于其提出了处理不完整响应的EGAE和策略与价值模型的独立优化机制，有效解决了PPO在长序列生成中的效率瓶颈，尤其对于LLMs的训练具有重要意义。其对训练效率的显著提升预示着在大型模型训练中具有广阔的应用前景。"}}
{"id": "2506.15405", "title": "Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks", "authors": ["Roshan Antony Gomez", "Julien Stöcker", "Barış Cansız", "Michael Kaliske"], "summary": "Physics-informed neural networks (PINNs) are extensively used to represent\nvarious physical systems across multiple scientific domains. The same can be\nsaid for cardiac electrophysiology, wherein fully-connected neural networks\n(FCNNs) have been employed to predict the evolution of an action potential in a\n2D space following the two-parameter phenomenological Aliev-Panfilov (AP)\nmodel. In this paper, the training behaviour of PINNs is investigated to\ndetermine optimal hyperparameters to predict the electrophysiological activity\nof the myocardium in 3D according to the AP model, with the inclusion of\nboundary and material parameters. An FCNN architecture is employed with the\ngoverning partial differential equations in their strong form, which are scaled\nconsistently with normalization of network inputs. The finite element (FE)\nmethod is used to generate training data for the network. Numerical examples\nwith varying spatial dimensions and parameterizations are generated using the\ntrained models. The network predicted fields for both the action potential and\nthe recovery variable are compared with the respective FE simulations. Network\nlosses are weighed with individual scalar values. Their effect on training and\nprediction is studied to arrive at a method of controlling losses during\ntraining.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.15405v1", "AI": {"title_translation": "使用物理信息神经网络模拟三维参数化心脏电生理", "tldr": "该研究使用物理信息神经网络（PINNs）在三维空间中模拟心脏电生理活动，并研究了训练行为和损失控制方法。", "motivation": "物理信息神经网络（PINNs）已被广泛应用于各种物理系统，包括心脏电生理学。此前，全连接神经网络（FCNNs）已被用于在二维空间中预测动作电位的演变。本文旨在研究PINNs的训练行为，以确定最佳超参数，从而根据Aliev-Panfilov (AP) 模型预测三维心肌的电生理活动，并包含边界和材料参数。", "method": "本文采用FCNN架构，使用强形式的偏微分方程，并与网络输入的归一化进行一致缩放。使用有限元（FE）方法生成网络训练数据。通过训练好的模型生成具有不同空间维度和参数化的数值示例。网络预测的动作电位和恢复变量的场与各自的FE模拟结果进行比较。研究了用单个标量值加权的损失对训练和预测的影响，以找到一种在训练过程中控制损失的方法。", "result": "研究生成了具有不同空间维度和参数化的数值示例。网络预测的动作电位和恢复变量的场与相应的有限元模拟结果进行了比较。通过研究损失权重对训练和预测的影响，得出了一种在训练过程中控制损失的方法。", "conclusion": "该研究成功地研究了PINNs的训练行为，并确定了预测三维心肌电生理活动的最佳超参数。通过研究损失权重，得出了一种在训练过程中控制损失的方法。", "translation": "物理信息神经网络（PINNs）被广泛用于表示跨多个科学领域的各种物理系统。心脏电生理学也不例外，其中全连接神经网络（FCNNs）已被用于根据双参数现象学Aliev-Panfilov（AP）模型预测二维空间中动作电位的演变。在本文中，研究了PINNs的训练行为，以确定最佳超参数，从而根据AP模型预测三维心肌的电生理活动，并包含边界和材料参数。采用了FCNN架构，使用强形式的控制偏微分方程，并与网络输入的归一化进行一致缩放。使用有限元（FE）方法为网络生成训练数据。使用训练好的模型生成了具有不同空间维度和参数化的数值示例。将网络预测的动作电位和恢复变量的场与各自的FE模拟结果进行了比较。网络损失用单个标量值加权。研究了它们对训练和预测的影响，以得出一种在训练过程中控制损失的方法。", "summary": "本研究利用物理信息神经网络（PINNs）模拟三维参数化心脏电生理活动。论文探讨了PINNs的训练行为，旨在确定最佳超参数，以根据Aliev-Panfilov模型预测包含边界和材料参数的三维心肌电生理活动。研究采用了全连接神经网络（FCNN）架构，结合强形式的偏微分方程，并利用有限元（FE）方法生成训练数据。通过比较网络预测结果与FE模拟结果，并研究不同损失权重对训练和预测的影响，最终提出了一种在训练过程中控制损失的方法。", "keywords": "物理信息神经网络, 心脏电生理学, 三维模拟, 有限元方法, 损失控制", "comments": "该论文的创新点在于将物理信息神经网络（PINNs）应用于三维心脏电生理学的复杂模拟，这在生物医学工程领域具有重要意义。通过结合有限元方法生成训练数据，并深入研究损失函数的权重对训练和预测行为的影响，为PINNs在复杂物理系统中的应用提供了宝贵的实践经验和优化策略。这种对损失控制的关注对于提高PINNs的训练稳定性和预测精度至关重要。"}}
{"id": "2506.14782", "title": "Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials", "authors": ["Joseph Geraci", "Bessi Qorri", "Christian Cumbaa", "Mike Tsay", "Paul Leonczyk", "Luca Pani"], "summary": "Artificial intelligence (AI) has evolved into an ecosystem of specialized\n\"species,\" each with unique strengths. We analyze two: DeepSeek-V3, a\n671-billion-parameter Mixture of Experts large language model (LLM)\nexemplifying scale-driven generality, and NetraAI, a dynamical system-based\nframework engineered for stability and interpretability on small clinical trial\ndatasets. We formalize NetraAI's foundations, combining contraction mappings,\ninformation geometry, and evolutionary algorithms to identify predictive\npatient cohorts. Features are embedded in a metric space and iteratively\ncontracted toward stable attractors that define latent subgroups. A\npseudo-temporal embedding and long-range memory enable exploration of\nhigher-order feature interactions, while an internal evolutionary loop selects\ncompact, explainable 2-4-variable bundles (\"Personas\").\n  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary\nlayer that observes Persona outputs, prioritizes promising variables, injects\ndomain knowledge, and assesses robustness. This two-tier architecture mirrors\nthe human scientific process: NetraAI as experimentalist, the LLM as theorist,\nforming a self-improving loop.\n  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI\nuncovered small, high-effect-size subpopulations that transformed weak baseline\nmodels (AUC ~0.50-0.68) into near-perfect classifiers using only a few\nfeatures. We position NetraAI at the intersection of dynamical systems,\ninformation geometry, and evolutionary learning, aligned with emerging\nconcept-level reasoning paradigms such as LeCun's Joint Embedding Predictive\nArchitecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI\noffers a new generation of adaptive, self-reflective AI to accelerate clinical\ndiscovery.", "comment": "27 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14782v1", "AI": {"title_translation": "整合动力系统学习与基础模型：用于临床试验的元进化人工智能框架", "tldr": "本文提出NetraAI，一个结合动力系统、信息几何和进化算法的框架，并与大型语言模型（LLM）结合，用于在小型临床试验数据集中发现可解释的高效应子群，显著提升了预测性能。", "motivation": "人工智能（AI）领域存在不同“物种”各有所长，例如大规模通用模型（如DeepSeek-V3）和专注于稳定性和可解释性的模型（如NetraAI）。本研究的动机是结合这些优势，创建一个能够在小型临床试验数据集中发现可靠、可解释知识的自适应、自反思AI，以加速临床发现。", "method": "研究提出了NetraAI框架，它结合了收缩映射、信息几何和进化算法来识别预测性患者队列。其核心是将特征嵌入度量空间，并迭代收缩至定义潜在子群的稳定吸引子。通过伪时间嵌入和长时记忆探索高阶特征交互，并通过内部进化循环选择紧凑、可解释的2-4变量组合（“Persona”）。此外，引入了一个LLM策略师作为元进化层，观察Persona输出，优先处理有前景的变量，注入领域知识并评估鲁棒性。这种双层架构模仿人类科学过程，NetraAI作为实验者，LLM作为理论家，形成一个自我改进的循环。", "result": "在精神分裂症、抑郁症和胰腺癌的案例研究中，NetraAI发现了小而效应量大的亚群，这些亚群将弱基线模型（AUC约0.50-0.68）转化为近乎完美的分类器，且仅使用少量特征。", "conclusion": "NetraAI将动力系统、信息几何和进化学习相结合，与新兴的概念级推理范式（如LeCun的JEPA）保持一致。通过优先考虑可靠、可解释的知识，NetraAI提供了一种新一代的自适应、自反思AI，以加速临床发现。", "translation": "人工智能（AI）已演变为一个由专业化“物种”组成的生态系统，每个物种都具有独特的优势。我们分析了两种：DeepSeek-V3，一个拥有6710亿参数的专家混合大型语言模型（LLM），代表了规模驱动的通用性；以及NetraAI，一个基于动力系统的框架，专为小型临床试验数据集的稳定性和可解释性而设计。我们形式化了NetraAI的基础，结合了收缩映射、信息几何和进化算法来识别预测性患者队列。特征被嵌入到度量空间中，并迭代收缩向定义潜在子群的稳定吸引子。伪时间嵌入和长时记忆使得探索高阶特征交互成为可能，同时内部进化循环选择紧凑、可解释的2-4变量组合（“Persona”）。\n为了指导发现，我们引入了一个LLM策略师作为元进化层，它观察Persona输出，优先处理有前景的变量，注入领域知识并评估鲁棒性。这种两层架构反映了人类科学过程：NetraAI作为实验者，LLM作为理论家，形成一个自我改进的循环。\n在案例研究（精神分裂症、抑郁症、胰腺癌）中，NetraAI发现了小而效应量大的亚群，这些亚群将弱基线模型（AUC约0.50-0.68）转化为使用少量特征的近乎完美的分类器。我们将NetraAI定位在动力系统、信息几何和进化学习的交叉点，与新兴的概念级推理范式（如LeCun的联合嵌入预测架构JEPA）保持一致。通过优先考虑可靠、可解释的知识，NetraAI提供了一种新一代的自适应、自反思AI，以加速临床发现。", "summary": "本文提出一个名为NetraAI的元进化人工智能框架，旨在整合动力系统学习与基础模型，特别应用于临床试验。NetraAI结合收缩映射、信息几何和进化算法，在小型临床试验数据集中识别具有预测性的患者子群，并通过迭代收缩和内部进化循环发现可解释的特征组合（“Persona”）。该框架引入一个LLM策略师作为元进化层，指导发现过程并注入领域知识。实验证明，NetraAI能将弱基线模型显著提升为高性能分类器，揭示了小而效应量高的亚群，为临床发现提供了一种新颖、可解释且自适应的AI方法。", "keywords": "动力系统, 临床试验, 元进化AI, 可解释性, 患者子群", "comments": "该论文提出了一个创新性的AI框架NetraAI，其核心亮点在于将动力系统、信息几何和进化算法有机结合，以处理小型临床试验数据并提取可解释的知识。更具创新性的是，引入LLM作为“元进化”策略师，模拟人类科学发现中的实验者与理论家角色，形成一个自改进的闭环，这在AI领域是前瞻性的探索。其强调“可解释性”和“可靠性”对于临床应用至关重要，能够将弱预测模型转化为高精度分类器，显示出巨大的应用潜力。该框架与新兴的概念级推理范式对齐，预示了未来AI在复杂科学问题解决上的发展方向。"}}
{"id": "2506.14997", "title": "Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings", "authors": ["Harbin Hong", "Sebastian Caldas", "Liu Leqi"], "summary": "As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14997v1", "AI": {"title_translation": "多项选择设置中量化大型语言模型与人类不一致性的假设检验", "tldr": "本文提出了一种基于假设检验的定量框架，用于评估大型语言模型（LLM）在多项选择调查中模拟人类行为的程度，并发现一个流行的LLM在模拟特定人群对有争议问题的意见时表现不佳。", "motivation": "随着大型语言模型（LLM）在社会科学研究中日益普及，评估这些模型复制人类行为的能力变得至关重要，因此需要一个量化框架来衡量LLM模拟行为与实际人类行为之间的不一致性。", "method": "本文提出了一种使用假设检验的定量框架，用于评估LLM模拟行为与多项选择调查中实际人类行为之间的不一致性。该框架应用于一个流行的大型语言模型，以模拟不同公共调查中人们的意见。", "result": "研究发现，所测试的流行大型语言模型在模拟特定子人群（例如，不同种族、年龄和收入）对有争议问题的意见时表现不佳，表明该模型与测试人群之间存在不一致性。", "conclusion": "该研究强调了在使用LLM进行社会科学研究时，需要超越简单模拟人类受试者，采用新的实践，因为LLM可能无法有效模拟特定人群对有争议问题的行为。", "translation": "随着大型语言模型（LLM）在社会科学研究（例如经济学和市场营销）中的日益普及，评估这些模型复制人类行为的程度变得至关重要。在这项工作中，我们利用假设检验，提出了一个定量框架来评估LLM模拟行为与多项选择调查设置中实际人类行为之间的不一致性。该框架允许我们以原则性的方式确定特定的语言模型是否能有效模拟通过多项选择选项表达的人类意见、决策和一般行为。我们将此框架应用于一个流行的大型语言模型，用于模拟各种公共调查中人们的意见，发现该模型不适合模拟所测试的子人群（例如，不同种族、年龄和收入）对有争议的问题。这引发了关于该语言模型与测试人群之间一致性的疑问，强调了在社会科学研究中使用LLM时需要超越简单模拟人类受试者的新实践。", "summary": "本文提出了一种基于假设检验的定量框架，旨在评估大型语言模型（LLM）在多项选择调查中模拟人类行为与真实人类行为之间的不一致性。研究人员将此框架应用于一个流行的LLM，以模拟公共调查中的人类意见，结果显示该模型在模拟特定子人群（如不同种族、年龄、收入）对有争议问题的意见时表现不佳。这表明在社会科学研究中，在使用LLM模拟人类行为时需要更加谨慎和采取新的实践。", "keywords": "大型语言模型, 假设检验, 人类行为模拟, 不一致性, 社会科学研究", "comments": "本文创新性地将假设检验引入LLM-人类行为一致性评估，提供了一个量化和原则性的框架，而非定性判断。其重要性在于揭示了当前LLM在模拟特定人群和处理复杂问题时的局限性，对未来社会科学研究中LLM的应用提出了警示，并呼吁更精细化的方法。"}}
{"id": "2506.15082", "title": "Make Your AUV Adaptive: An Environment-Aware Reinforcement Learning Framework For Underwater Tasks", "authors": ["Yimian Ding", "Jingzehua Xu", "Guanwen Xie", "Shuai Zhang", "Yi Li"], "summary": "This study presents a novel environment-aware reinforcement learning (RL)\nframework designed to augment the operational capabilities of autonomous\nunderwater vehicles (AUVs) in underwater environments. Departing from\ntraditional RL architectures, the proposed framework integrates an\nenvironment-aware network module that dynamically captures flow field data,\neffectively embedding this critical environmental information into the state\nspace. This integration facilitates real-time environmental adaptation,\nsignificantly enhancing the AUV's situational awareness and decision-making\ncapabilities. Furthermore, the framework incorporates AUV structure\ncharacteristics into the optimization process, employing a large language model\n(LLM)-based iterative refinement mechanism that leverages both environmental\nconditions and training outcomes to optimize task performance. Comprehensive\nexperimental evaluations demonstrate the framework's superior performance,\nrobustness and adaptability.", "comment": "This paper has been accepted by IROS 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15082v1", "AI": {"title_translation": "让你的AUV适应环境：一种面向水下任务的环境感知强化学习框架", "tldr": "提出一种环境感知强化学习框架，通过整合环境信息和AUV结构特性，显著提升AUV在水下任务中的性能、鲁棒性和适应性。", "motivation": "旨在增强自主水下航行器（AUV）在水下环境中的操作能力，通过克服传统强化学习架构的局限性，实现实时环境适应性。", "method": "提出了一种新颖的环境感知强化学习框架。该框架包含：1. 一个环境感知网络模块，动态捕获流场数据并将其嵌入到状态空间中，以实现实时环境适应。2. 将AUV结构特性纳入优化过程，并采用基于大型语言模型（LLM）的迭代优化机制，利用环境条件和训练结果来优化任务性能。", "result": "综合实验评估表明，该框架在性能、鲁棒性和适应性方面表现出卓越的优势。", "conclusion": "该环境感知强化学习框架能够显著提升AUV在复杂水下环境中的操作能力、决策能力和适应性。", "translation": "本研究提出了一种新颖的环境感知强化学习（RL）框架，旨在增强自主水下航行器（AUV）在水下环境中的操作能力。与传统强化学习架构不同，所提出的框架集成了一个环境感知网络模块，该模块动态捕获流场数据，有效地将这一关键环境信息嵌入到状态空间中。这种集成促进了实时环境适应，显著增强了AUV的态势感知和决策能力。此外，该框架将AUV结构特性纳入优化过程，采用基于大型语言模型（LLM）的迭代优化机制，利用环境条件和训练结果来优化任务性能。综合实验评估表明，该框架在性能、鲁棒性和适应性方面表现出卓越的优势。", "summary": "本文提出了一种新颖的环境感知强化学习框架，旨在提升自主水下航行器（AUV）在水下环境中的操作能力。该框架通过一个环境感知网络模块将流场数据集成到状态空间中，实现了AUV的实时环境适应。同时，它将AUV结构特性和基于大型语言模型（LLM）的迭代优化机制纳入训练过程，以优化任务表现。实验结果验证了该框架在性能、鲁棒性和适应性方面的优越性。", "keywords": "自主水下航行器, 强化学习, 环境感知, 大型语言模型, 水下任务", "comments": "这篇论文的创新点在于将环境感知能力（通过捕获流场数据）和AUV结构特性（通过LLM迭代优化）融入强化学习框架，从而显著提升AUV在复杂水下环境中的自适应能力。特别是引入LLM进行优化，为RL在机器人控制领域的应用提供了新的思路。"}}
{"id": "2506.14844", "title": "Improving Prostate Gland Segmenting Using Transformer based Architectures", "authors": ["Shatha Abudalou"], "summary": "Inter reader variability and cross site domain shift challenge the automatic\nsegmentation of prostate anatomy using T2 weighted MRI images. This study\ninvestigates whether transformer models can retain precision amid such\nheterogeneity. We compare the performance of UNETR and SwinUNETR in prostate\ngland segmentation against our previous 3D UNet model [1], based on 546 MRI\n(T2weighted) volumes annotated by two independent experts. Three training\nstrategies were analyzed: single cohort dataset, 5 fold cross validated mixed\ncohort, and gland size based dataset. Hyperparameters were tuned by Optuna. The\ntest set, from an independent population of readers, served as the evaluation\nendpoint (Dice Similarity Coefficient). In single reader training, SwinUNETR\nachieved an average dice score of 0.816 for Reader#1 and 0.860 for Reader#2,\nwhile UNETR scored 0.8 and 0.833 for Readers #1 and #2, respectively, compared\nto the baseline UNets 0.825 for Reader #1 and 0.851 for Reader #2. SwinUNETR\nhad an average dice score of 0.8583 for Reader#1 and 0.867 for Reader#2 in\ncross-validated mixed training. For the gland size-based dataset, SwinUNETR\nachieved an average dice score of 0.902 for Reader#1 subset and 0.894 for\nReader#2, using the five-fold mixed training strategy (Reader#1, n=53;\nReader#2, n=87) at larger gland size-based subsets, where UNETR performed\npoorly. Our findings demonstrate that global and shifted-window self-attention\neffectively reduces label noise and class imbalance sensitivity, resulting in\nimprovements in the Dice score over CNNs by up to five points while maintaining\ncomputational efficiency. This contributes to the high robustness of SwinUNETR\nfor clinical deployment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14844v1", "AI": {"title_translation": "使用基于Transformer的架构改进前列腺腺体分割", "tldr": "本研究比较了UNETR和SwinUNETR在T2加权MRI图像上前列腺腺体分割中的性能，发现SwinUNETR在处理阅读器间变异性和领域偏移方面表现出更高的鲁棒性和更好的Dice分数。", "motivation": "由于阅读器间变异性和跨站点领域偏移，使用T2加权MRI图像自动分割前列腺解剖结构面临挑战。本研究旨在调查Transformer模型是否能在这种异质性中保持精度。", "method": "本研究比较了UNETR和SwinUNETR与之前3D UNet模型在546个体素（T2加权MRI）上的前列腺腺体分割性能，这些体素由两位独立专家标注。分析了三种训练策略：单一队列数据集、5折交叉验证混合队列和基于腺体大小的数据集。使用Optuna调整超参数，并使用来自独立阅读器群体的测试集进行评估，评估指标为Dice相似系数。", "result": "在单一阅读器训练中，SwinUNETR在阅读器#1上平均Dice分数为0.816，在阅读器#2上为0.860。UNETR在阅读器#1上为0.8，在阅读器#2上为0.833。基线UNets在阅读器#1上为0.825，在阅读器#2上为0.851。在交叉验证混合训练中，SwinUNETR在阅读器#1上平均Dice分数为0.8583，在阅读器#2上为0.867。在基于腺体大小的数据集上，SwinUNETR在五折混合训练策略下，对于较大腺体大小的子集，阅读器#1子集达到0.902，阅读器#2达到0.894，而UNETR表现不佳。", "conclusion": "研究结果表明，全局和移位窗口自注意力机制能有效减少标签噪声和类别不平衡敏感性，使Dice分数比CNN提高多达5个百分点，同时保持计算效率。这有助于SwinUNETR在临床部署中展现出高鲁棒性。", "translation": "阅读器间变异性和跨站点领域偏移对使用T2加权MRI图像自动分割前列腺解剖结构提出了挑战。本研究旨在调查Transformer模型是否能在这种异质性中保持精度。我们比较了UNETR和SwinUNETR在前列腺腺体分割中的性能，并与我们之前的3D UNet模型[1]进行了对比，研究基于由两位独立专家标注的546个MRI（T2加权）体素。分析了三种训练策略：单一队列数据集、5折交叉验证混合队列和基于腺体大小的数据集。超参数通过Optuna进行调整。来自独立阅读器群体的测试集作为评估终点（Dice相似系数）。在单一阅读器训练中，SwinUNETR在阅读器#1上平均Dice分数为0.816，在阅读器#2上为0.860，而UNETR在阅读器#1上得分为0.8，在阅读器#2上得分为0.833，相比之下，基线UNet在阅读器#1上为0.825，在阅读器#2上为0.851。在交叉验证混合训练中，SwinUNETR在阅读器#1上平均Dice分数为0.8583，在阅读器#2上为0.867。对于基于腺体大小的数据集，SwinUNETR在五折混合训练策略下（阅读器#1，n=53；阅读器#2，n=87），对于较大腺体大小的子集，阅读器#1子集达到0.902，阅读器#2达到0.894，而UNETR表现不佳。我们的发现表明，全局和移位窗口自注意力机制能有效减少标签噪声和类别不平衡敏感性，使Dice分数比CNN提高多达5个百分点，同时保持计算效率。这有助于SwinUNETR在临床部署中展现出高鲁棒性。", "summary": "本研究旨在解决T2加权MRI图像上前列腺腺体自动分割面临的阅读器间变异性和领域偏移挑战。通过比较UNETR和SwinUNETR与3D UNet模型在546个MRI体素上的性能，并分析了单一队列、交叉验证混合和基于腺体大小的三种训练策略。结果显示，SwinUNETR在多种训练策略下均表现出优于UNETR和基线UNet的Dice分数，尤其是在处理较大腺体时。研究表明，Transformer模型，特别是SwinUNETR的自注意力机制，能有效降低标签噪声和类别不平衡敏感性，从而提高分割精度和模型鲁棒性，使其更适用于临床部署。", "keywords": "前列腺分割, Transformer, UNETR, SwinUNETR, 医学图像分割", "comments": "本研究的创新之处在于将Transformer架构（UNETR和SwinUNETR）应用于前列腺腺体分割，并系统地比较了其在处理医学图像分割中常见的阅读器间变异性和领域偏移问题上的性能。SwinUNETR表现出的优越性能和鲁棒性对于提高临床诊断的自动化和准确性具有重要意义。研究通过对比不同训练策略和数据集特性，深入分析了模型在实际应用中的潜力。"}}
{"id": "2506.14992", "title": "Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks", "authors": ["Zhihao Tao", "Athina P. Petropulu"], "summary": "We propose a novel directional modulation (DM) design for OFDM transmitters\naided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS\nis configured to preserve the integrity of transmitted signals toward multiple\nlegitimate users while scrambling the signal in all other directions. Existing\nTM-IRS design methods typically target a single user direction and follow\npredefined rule-based procedures, making them unsuitable for multi-user\nscenarios. Here, we propose a generative AI-based approach to design good sets\nof TM-IRS parameters out of a set of all possible quantized ranges of\nparameters. The design objective is to maximize the sum rate across the\nauthorized directions. We model the TM-IRS parameter selection as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a specific configuration of TM-IRS parameters. GFlowNets are\nemployed to learn a stochastic policy that samples TM-IRS parameter sets with\nprobability proportional to their associated sum rate reward. Experimental\nresults demonstrate that the proposed method effectively enhances the security\nof the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size\nof the TM-IRS configuration space, the GFlowNet is able to converge after\ntraining on fewer than 0.000001% of all possible configurations, demonstrating\nremarkable efficiency compared to exhaustive combinatorial search.\nImplementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to\nfacilitate reproducibility.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14992v1", "AI": {"title_translation": "基于生成流网络的时变智能反射面的安全设计", "tldr": "本文提出了一种新颖的基于生成流网络（GFlowNets）的AI方法，用于设计时变智能反射面（TM-IRS）参数，以在多用户OFDM系统中实现安全定向调制，并在效率上显著优于传统方法。", "motivation": "现有TM-IRS设计方法通常针对单个用户方向，并遵循预定义的基于规则的程序，这使得它们不适用于多用户场景。因此，需要一种新的方法来解决多用户环境下的TM-IRS设计问题。", "method": "本文提出了一种基于生成式AI的方法来设计TM-IRS参数，目标是最大化授权方向上的总速率。将TM-IRS参数选择建模为确定性马尔可夫决策过程（MDP），并采用GFlowNets学习一种随机策略，以与其关联的总速率奖励成比例的概率采样TM-IRS参数集。", "result": "所提出的方法有效增强了多用户TM-IRS辅助OFDM系统的安全性。尽管TM-IRS配置空间巨大，但GFlowNet在少于0.000001%的所有可能配置上进行训练后即可收敛，与穷举组合搜索相比，展现出卓越的效率。", "conclusion": "本文提出了一种基于GFlowNets的TM-IRS参数设计新方法，能够有效提高多用户OFDM系统的安全性，并在训练效率上表现出色。", "translation": "我们提出了一种由时变智能反射面（TM-IRS）辅助的OFDM发射机的新型定向调制（DM）设计。TM-IRS被配置为保持向多个合法用户传输信号的完整性，同时在所有其他方向上扰乱信号。现有的TM-IRS设计方法通常针对单个用户方向，并遵循预定义的基于规则的程序，这使得它们不适用于多用户场景。在此，我们提出了一种基于生成式AI的方法，从所有可能的量化参数范围中设计出良好的TM-IRS参数集。设计目标是最大化授权方向上的总速率。我们将TM-IRS参数选择建模为确定性马尔可夫决策过程（MDP），其中每个终端状态对应于TM-IRS参数的特定配置。GFlowNets被用于学习一种随机策略，以与其关联的总速率奖励成比例的概率采样TM-IRS参数集。实验结果表明，所提出的方法有效增强了多用户TM-IRS辅助OFDM系统的安全性。此外，尽管TM-IRS配置空间巨大，但GFlowNet在少于所有可能配置的0.000001%上进行训练后即可收敛，与穷举组合搜索相比，展现出卓越的效率。实现代码可在https://github.com/ZhihaoTao/GFN4TM-RIS获取，以方便复现。", "summary": "本文提出了一种基于生成流网络（GFlowNets）的新型AI方法，用于设计时变智能反射面（TM-IRS）参数，以实现多用户OFDM系统中的安全定向调制。该方法将TM-IRS参数选择建模为马尔可夫决策过程，并利用GFlowNets学习高效的参数采样策略，旨在最大化合法用户的总速率。实验结果表明，该方法不仅能有效提升多用户TM-IRS辅助系统的安全性，而且在极大的配置空间中展现出卓越的收敛效率，远超传统穷举搜索方法。", "keywords": "智能反射面, 生成流网络, 定向调制, 多用户通信, 安全通信", "comments": "本文的创新点在于首次将生成流网络（GFlowNets）应用于时变智能反射面（TM-IRS）的设计，以解决多用户场景下的安全定向调制问题。通过将参数选择建模为MDP，并利用GFlowNets进行高效探索，该方法显著提升了在大规模配置空间中的搜索效率，突破了传统规则方法的局限性。这种将强化学习范式引入TM-IRS设计的思路，为未来智能通信系统中的硬件配置优化提供了新的范式，具有重要的研究价值和潜在应用前景。"}}
{"id": "2506.15312", "title": "One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning", "authors": ["Han Wu", "Junyao Li", "Kangbo Zhao", "Sen Zhang", "Yukai Shi", "Liang Lin"], "summary": "Face sketch synthesis is a technique aimed at converting face photos into\nsketches. Existing face sketch synthesis research mainly relies on training\nwith numerous photo-sketch sample pairs from existing datasets. However, these\nlarge-scale discriminative learning methods will have to face problems such as\ndata scarcity and high human labor costs. Once the training data becomes\nscarce, their generative performance significantly degrades. In this paper, we\npropose a one-shot face sketch synthesis method based on diffusion models. We\noptimize text instructions on a diffusion model using face photo-sketch image\npairs. Then, the instructions derived through gradient-based optimization are\nused for inference. To simulate real-world scenarios more accurately and\nevaluate method effectiveness more comprehensively, we introduce a new\nbenchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark\nconsists of 400 pairs of face photo-sketch images, including sketches with\ndifferent styles and photos with different backgrounds, ages, sexes,\nexpressions, illumination, etc. For a solid out-of-distribution evaluation, we\nselect only one pair of images for training at each time, with the rest used\nfor inference. Extensive experiments demonstrate that the proposed method can\nconvert various photos into realistic and highly consistent sketches in a\none-shot context. Compared to other methods, our approach offers greater\nconvenience and broader applicability. The dataset will be available at:\nhttps://github.com/HanWu3125/OS-Sketch", "comment": "We propose a novel framework for face sketch synthesis, where merely\n  a single pair of samples suffices to enable in-the-wild face sketch synthesis", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15312v1", "AI": {"title_translation": "野外环境下的单样本人脸素描合成：基于生成扩散先验和指令微调", "tldr": "提出一种基于扩散模型和指令微调的单样本人脸素描合成方法，解决了现有方法对大量数据依赖的问题，并在新数据集上表现出良好效果。", "motivation": "现有的人脸素描合成方法主要依赖于大量光照-素描样本对进行训练，但这种判别式学习方法面临数据稀缺和高昂的人力成本问题，一旦训练数据稀缺，生成性能会显著下降。", "method": "本文提出一种基于扩散模型的单样本人脸素描合成方法。具体而言，该方法使用人脸光照-素描图像对来优化扩散模型上的文本指令，然后利用基于梯度的优化导出的指令进行推理。为了更准确地模拟真实世界场景并全面评估方法有效性，引入了一个名为One-shot Face Sketch Dataset (OS-Sketch)的新基准数据集，包含400对不同风格素描和不同背景、年龄、性别、表情、光照照片的图像。每次仅选择一对图像进行训练，其余用于推理，以进行域外评估。", "result": "广泛的实验证明，所提出的方法在单样本条件下能将各种照片转换为逼真且高度一致的素描。", "conclusion": "该方法与现有方法相比，提供了更大的便利性和更广泛的适用性，有效解决了数据稀缺问题，并在单样本学习范式下实现了高质量的人脸素描合成。", "translation": "人脸素描合成是一种旨在将人脸照片转换为素描的技术。现有的人脸素描合成研究主要依赖于使用现有数据集中的大量照片-素描样本对进行训练。然而，这些大规模判别式学习方法将不得不面对数据稀缺和高昂的人力成本等问题。一旦训练数据变得稀缺，它们的生成性能就会显著下降。在本文中，我们提出了一种基于扩散模型的单样本人脸素描合成方法。我们使用人脸照片-素描图像对来优化扩散模型上的文本指令。然后，通过基于梯度的优化导出的指令用于推理。为了更准确地模拟真实世界场景并更全面地评估方法有效性，我们引入了一个名为单样本人脸素描数据集（OS-Sketch）的新基准。该基准包含400对人脸照片-素描图像，其中包括不同风格的素描和不同背景、年龄、性别、表情、光照等的照片。为了进行可靠的域外评估，我们每次只选择一对图像进行训练，其余用于推理。广泛的实验表明，所提出的方法可以在单样本背景下将各种照片转换为逼真且高度一致的素描。与其他方法相比，我们的方法提供了更大的便利性和更广泛的适用性。数据集将在此处提供：https://github.com/HanWu3125/OS-Sketch", "summary": "本论文提出了一种基于生成扩散模型和指令微调的单样本人脸素描合成方法，旨在解决传统方法对大量数据依赖的问题。通过利用少量光照-素描图像对优化扩散模型的文本指令，实现高效的素描生成。为验证方法有效性，作者构建了新的单样本人脸素描数据集（OS-Sketch），并在其上进行广泛实验，结果表明该方法能将多种照片高质量地转换为逼真素描，具有更高的便利性和适用性。", "keywords": "人脸素描合成, 单样本学习, 扩散模型, 指令微调, 数据集", "comments": "该论文的创新点在于将扩散模型与指令微调相结合，实现了在数据极度稀缺（单样本）情况下的高质量人脸素描合成，这显著降低了对大规模标注数据的依赖和人力成本。引入新的OS-Sketch数据集也为未来研究提供了宝贵的基准，特别是在“野外”和域外泛化能力评估方面。该方法在实际应用中具有广阔前景，尤其是在数据获取困难或需要快速适应新风格的场景。"}}
{"id": "2506.15284", "title": "Multi-Interest Recommendation: A Survey", "authors": ["Zihao Li", "Qiang Chen", "Lixin Zou", "Aixin Sun", "Chenliang Li"], "summary": "Existing recommendation methods often struggle to model users' multifaceted\npreferences due to the diversity and volatility of user behavior, as well as\nthe inherent uncertainty and ambiguity of item attributes in practical\nscenarios. Multi-interest recommendation addresses this challenge by extracting\nmultiple interest representations from users' historical interactions, enabling\nfine-grained preference modeling and more accurate recommendations. It has\ndrawn broad interest in recommendation research. However, current\nrecommendation surveys have either specialized in frontier recommendation\nmethods or delved into specific tasks and downstream applications. In this\nwork, we systematically review the progress, solutions, challenges, and future\ndirections of multi-interest recommendation by answering the following three\nquestions: (1) Why is multi-interest modeling significantly important for\nrecommendation? (2) What aspects are focused on by multi-interest modeling in\nrecommendation? and (3) How can multi-interest modeling be applied, along with\nthe technical details of the representative modules? We hope that this survey\nestablishes a fundamental framework and delivers a preliminary overview for\nresearchers interested in this field and committed to further exploration. The\nimplementation of multi-interest recommendation summarized in this survey is\nmaintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15284v1", "AI": {"title_translation": "多兴趣推荐：一项综述", "tldr": "这是一篇关于多兴趣推荐的综述，系统回顾了其进展、解决方案、挑战和未来方向，旨在为研究人员提供基础框架和初步概述。", "motivation": "现有推荐方法由于用户行为的多样性和波动性，以及物品属性的固有不确定性，难以对用户多方面的偏好进行建模。多兴趣推荐旨在通过从用户历史交互中提取多个兴趣表示来解决此问题，从而实现细粒度偏好建模和更准确的推荐。", "method": "本文系统地回顾了多兴趣推荐的进展、解决方案、挑战和未来方向。具体通过回答以下三个问题进行论述：(1) 为何多兴趣建模对推荐如此重要？(2) 多兴趣建模在推荐中关注哪些方面？(3) 如何应用多兴趣建模及其代表性模块的技术细节？", "result": "本综述建立了多兴趣推荐的基础框架，并为该领域的研究人员提供了初步概述。", "conclusion": "作者希望本综述能为对多兴趣推荐感兴趣并致力于进一步探索的研究人员建立一个基础框架并提供初步概述。", "translation": "现有推荐方法由于用户行为的多样性和波动性，以及实际场景中物品属性固有的不确定性和模糊性，常常难以对用户多方面的偏好进行建模。多兴趣推荐通过从用户的历史交互中提取多个兴趣表示来解决这一挑战，从而实现细粒度的偏好建模和更准确的推荐。它在推荐研究中引起了广泛关注。然而，当前的推荐综述要么专注于前沿推荐方法，要么深入探讨特定任务和下游应用。在这项工作中，我们通过回答以下三个问题，系统地回顾了多兴趣推荐的进展、解决方案、挑战和未来方向：(1) 为什么多兴趣建模对推荐如此重要？(2) 多兴趣建模在推荐中关注哪些方面？(3) 如何应用多兴趣建模以及代表性模块的技术细节？我们希望这项综述能为对该领域感兴趣并致力于进一步探索的研究人员建立一个基础框架并提供初步概述。本综述总结的多兴趣推荐的实现维护在 https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey。", "summary": "这篇综述系统地回顾了多兴趣推荐领域的进展。它指出，传统推荐方法难以捕捉用户多面性偏好，而多兴趣推荐通过提取多个兴趣表示来解决此问题。综述通过回答多兴趣建模的重要性、关注点和应用方法等问题，为研究人员提供了该领域的基础框架和初步概述。", "keywords": "多兴趣推荐, 推荐系统, 用户偏好, 综述, 兴趣建模", "comments": "这是一篇重要的综述性论文，它系统地梳理了多兴趣推荐这一新兴且关键的研究方向。它填补了现有综述的空白，为研究人员提供了一个全面的视角，有助于理解该领域的现状、挑战和未来发展方向。其结构化的问答形式也使得内容更具条理性和指导性。"}}
{"id": "2506.15602", "title": "Estimate Hitting Time by Hitting Probability for Elitist Evolutionary Algorithms", "authors": ["Jun He", "Siang Yew Chong", "Xin Yao"], "summary": "Drift analysis is a powerful tool for analyzing the time complexity of\nevolutionary algorithms. However, it requires manual construction of drift\nfunctions to bound hitting time for each specific algorithm and problem. To\naddress this limitation, general linear drift functions were introduced for\nelitist evolutionary algorithms. But calculating linear bound coefficients\neffectively remains a problem. This paper proposes a new method called drift\nanalysis of hitting probability to compute these coefficients. Each coefficient\nis interpreted as a bound on the hitting probability of a fitness level,\ntransforming the task of estimating hitting time into estimating hitting\nprobability. A novel drift analysis method is then developed to estimate\nhitting probability, where paths are introduced to handle multimodal fitness\nlandscapes. Explicit expressions are constructed to compute hitting\nprobability, significantly simplifying the estimation process. One advantage of\nthe proposed method is its ability to estimate both the lower and upper bounds\nof hitting time and to compare the performance of two algorithms in terms of\nhitting time. To demonstrate this application, two algorithms for the knapsack\nproblem, each incorporating feasibility rules and greedy repair respectively,\nare compared. The analysis indicates that neither constraint handling technique\nconsistently outperforms the other.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.15602v1", "AI": {"title_translation": "通过命中概率估计精英演化算法的命中时间", "tldr": "本文提出了一种基于命中概率的漂移分析新方法，用于估计精英演化算法的命中时间，特别适用于处理多峰适应度景观，并能比较算法性能。", "motivation": "漂移分析是分析演化算法时间复杂度的强大工具，但它需要手动构建漂移函数来限定命中时间。尽管已引入精英演化算法的通用线性漂移函数，但有效计算线性边界系数仍是一个未解决的问题。", "method": "本文提出了一种名为“命中概率漂移分析”的新方法来计算线性边界系数。该方法将每个系数解释为适应度水平命中概率的边界，从而将估计命中时间转化为估计命中概率。为此，开发了一种新的漂移分析方法来估计命中概率，并引入了“路径”来处理多峰适应度景观。文章构建了显式表达式来计算命中概率，并通过比较两种解决背包问题的算法（分别采用可行性规则和贪婪修复）来演示其应用。", "result": "所提出的方法显著简化了命中时间的估计过程，并能够估计命中时间的下限和上限。此外，它还能用于比较两种算法在命中时间方面的性能。在背包问题的应用中，分析表明所比较的两种约束处理技术（可行性规则和贪婪修复）都没有持续优于对方。", "conclusion": "本文成功引入了一种基于命中概率的新型漂移分析方法，用于估计精英演化算法的命中时间。该方法简化了估计过程，并提供了估计上下限及算法比较的能力。通过背包问题案例，验证了其应用性，并发现两种约束处理技术并无明显优劣。", "translation": "漂移分析是分析演化算法时间复杂度的强大工具。然而，它需要为每个特定的算法和问题手动构建漂移函数来限定命中时间。为了解决这个限制，引入了精英演化算法的通用线性漂移函数。但有效计算线性边界系数仍然是一个问题。本文提出了一种名为命中概率漂移分析的新方法来计算这些系数。每个系数被解释为适应度水平命中概率的边界，将估计命中时间的任务转化为估计命中概率。然后开发了一种新的漂移分析方法来估计命中概率，其中引入了路径来处理多峰适应度景观。构建了显式表达式来计算命中概率，显著简化了估计过程。所提出方法的一个优点是它能够估计命中时间的下限和上限，并比较两种算法在命中时间方面的性能。为了演示这个应用，比较了两种解决背包问题的算法，它们分别结合了可行性规则和贪婪修复。分析表明，这两种约束处理技术都没有持续优于另一种。", "summary": "本文提出了一种名为“命中概率漂移分析”的新方法，旨在解决精英演化算法中线性边界系数难以有效计算的问题。该方法将命中时间的估计转化为命中概率的估计，通过引入“路径”来处理多峰适应度景观，并构建显式表达式来简化计算过程。新方法不仅能估计命中时间的上下限，还能用于比较不同算法的性能。文章通过对背包问题的两种算法进行比较，展示了其应用价值，并指出这两种约束处理技术并无持续的优劣之分。", "keywords": "漂移分析, 命中时间, 命中概率, 精英演化算法, 背包问题", "comments": "本文通过将命中时间估计转化为命中概率估计，为精英演化算法的漂移分析提供了一种创新且实用的方法。引入“路径”来处理多峰适应度景观是其重要贡献，显著扩展了漂移分析的适用范围。该方法能够提供命中时间的上下限并促进算法比较，具有显著的实际应用价值。尽管背包问题的比较结果显示两种技术无明显优劣，但这本身也是一个有价值的发现，为后续研究提供了方向。"}}
{"id": "2506.15029", "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "authors": ["Prateek Mehta", "Anasuya Patil"], "summary": "Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW).", "comment": "9 pages, 9 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15029v1", "AI": {"title_translation": "基于LabVIEW的准确修订版光学字符识别语音合成", "tldr": "开发了一个基于OCR和LabVIEW的语音合成系统，旨在帮助视障人士更便捷地获取信息。", "motivation": "视障人士在获取书籍信息方面存在局限性，现有方法（如盲文书和录音）不足以满足需求，而语音是更有效的交流方式。", "method": "使用Laboratory Virtual Instrument Engineering Workbench (LabVIEW) 实现了一个基于光学字符识别（OCR）的语音合成系统。", "result": "论文开发了一个准确、可靠、经济且用户友好的基于光学字符识别（OCR）的语音合成系统。", "conclusion": "Not mentioned in abstract", "translation": "通过声音提取知识是一种独特的特性。视障人士通常只依赖盲文书籍和非政府组织提供的录音。由于这些方法的局限性，盲人往往无法获取他们选择的书籍。对于盲人和视障人士来说，语音是一种比文本更有效的交流方式，因为他们可以轻松地对声音做出反应。本文介绍了开发一个准确、可靠、经济且用户友好的基于光学字符识别（OCR）的语音合成系统。该基于OCR的系统已使用Laboratory Virtual Instrument Engineering Workbench (LabVIEW) 实现。", "summary": "本文介绍了一个基于光学字符识别（OCR）和LabVIEW的语音合成系统的开发。该系统旨在为视障人士提供一个准确、可靠、经济且用户友好的信息获取方式，以克服当前盲文书籍和录音的局限性，使他们能够更便捷地访问所选书籍。", "keywords": "光学字符识别, 语音合成, LabVIEW, 视障人士, 信息获取", "comments": "该论文的创新点在于结合OCR技术和LabVIEW平台开发了针对视障人士的语音合成系统，具有较强的实用性和社会价值。然而，摘要中未提及系统的具体性能评估数据或实际应用效果。"}}
{"id": "2506.14912", "title": "CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision", "authors": ["Dyah Adila", "Shuai Zhang", "Boran Han", "Bonan Min", "Yuyang Wang"], "summary": "The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14912v1", "AI": {"title_translation": "CrEst：基于弱监督的大语言模型上下文可信度评估", "tldr": "CrEst是一个新的弱监督框架，用于评估大语言模型推理过程中上下文文档的可信度，无需手动标注，通过文档间语义一致性实现，并能显著提升模型性能和鲁棒性。", "motivation": "现有方法在集成上下文信息以提升大语言模型（LLMs）性能时，常忽略上下文文档的可信度差异，这可能导致不可靠信息的传播。", "method": "本研究提出了CrEst框架，一个无需手动标注的弱监督方法，用于在大语言模型推理期间评估上下文文档的可信度。其核心思想是可信文档与其他可信文档具有更高的语义一致性，从而通过文档间一致性进行自动化可信度评估。为将可信度整合到LLM推理中，提出了两种策略：一种是针对无法访问内部权重或激活的模型的黑盒方法，另一种是直接修改注意力机制的白盒方法。", "result": "在三种模型架构和五个数据集上的广泛实验表明，CrEst始终优于强基线模型，准确率最高提升26.86%，F1分数提高3.49%。进一步分析显示，CrEst即使在高噪声条件下也能保持鲁棒性能。", "conclusion": "CrEst框架有效解决了大语言模型中上下文信息可信度评估的挑战，通过弱监督方式显著提升了模型在知识密集型任务上的性能和鲁棒性，而无需人工标注。", "translation": "上下文信息的整合显著提升了大型语言模型（LLMs）在知识密集型任务上的表现。然而，现有方法常常忽视一个关键挑战：上下文文档的可信度差异很大，可能导致不可靠信息的传播。在本文中，我们引入了CrEst，一个新颖的弱监督框架，用于在LLM推理过程中评估上下文文档的可信度——无需人工标注。我们的方法基于一个洞察：可信文档倾向于与其他可信文档表现出更高的语义一致性，从而通过文档间一致性实现自动可信度评估。为了将可信度整合到LLM推理中，我们提出了两种集成策略：一种是针对无法访问内部权重或激活的模型的黑盒方法，另一种是直接修改注意力机制的白盒方法。在三种模型架构和五个数据集上的广泛实验表明，CrEst始终优于强基线模型，准确率最高提升26.86%，F1分数提高3.49%。进一步分析显示，CrEst即使在高噪声条件下也能保持鲁棒性能。", "summary": "CrEst是一个新颖的弱监督框架，旨在解决大语言模型（LLMs）在利用上下文信息时面临的可信度挑战。它通过识别可信文档之间更高的语义一致性，自动评估上下文文档的可信度，无需手动标注。该框架提出了黑盒和白盒两种集成策略，以将可信度信息纳入LLM推理过程。实验证明，CrEst在多项任务和数据集上显著优于现有基线，提高了准确性和F1分数，并在高噪声环境下展现出强大的鲁棒性。", "keywords": "可信度评估, 弱监督, 大语言模型, 上下文, 语义一致性", "comments": "CrEst的创新点在于其利用文档间语义一致性进行弱监督可信度评估，避免了耗时的人工标注。这对于提高LLMs在知识密集型任务中的可靠性至关重要。其提出的黑盒和白盒集成策略也增加了其适用性。这项工作对于提升LLM输出的准确性和减少错误信息传播具有重要意义。"}}
{"id": "2506.14780", "title": "Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes", "authors": ["Reda Chhaibi", "Serge Gratton", "Samuel Vaiter"], "summary": "In this paper, we propose an accelerated version for the Sinkhorn algorithm,\nwhich is the reference method for computing the solution to Entropic Optimal\nTransport.\n  Its main draw-back is the exponential slow-down of convergence as the\nregularization weakens $\\varepsilon \\rightarrow 0$.\n  Thanks to spectral insights on the behavior of the Hessian, we propose to\nmitigate the problem via an original spectral warm-start strategy. This leads\nto faster convergence compared to the reference method, as also demonstrated in\nour numerical experiments.", "comment": "22 pages, 5 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14780v1", "AI": {"title_translation": "通过稳定低频模式加速熵最优传输的计算", "tldr": "本文提出了一种加速Sinkhorn算法的方法，通过谱热启动策略解决了熵最优传输在弱正则化下收敛速度指数级减慢的问题，并实现了更快的收敛。", "motivation": "Sinkhorn算法在计算熵最优传输时，当正则化项$\\\\varepsilon \\\\rightarrow 0$时，收敛速度呈指数级减慢，这是其主要缺点。", "method": "通过对Hessian行为的谱洞察，提出了一种原创的谱热启动策略来缓解收敛速度慢的问题。", "result": "与参考方法相比，该方法实现了更快的收敛速度，并通过数值实验得到了验证。", "conclusion": "通过利用Hessian的谱特性提出的谱热启动策略，能够有效加速熵最优传输的Sinkhorn算法，尤其是在弱正则化条件下。", "translation": "在本文中，我们提出了一种加速版本的Sinkhorn算法，该算法是计算熵最优传输解的参考方法。\n其主要缺点是当正则化减弱（$\\\\varepsilon \\\\rightarrow 0$）时，收敛速度呈指数级减慢。\n得益于对Hessian行为的谱洞察，我们提出通过一种原创的谱热启动策略来缓解这个问题。这导致了与参考方法相比更快的收敛速度，正如我们的数值实验所证明的那样。", "summary": "本文提出了一种加速计算熵最优传输的Sinkhorn算法的新方法。针对该算法在弱正则化条件下收敛速度指数级下降的问题，作者利用对Hessian行为的谱洞察，设计了一种原创的谱热启动策略。数值实验结果表明，该策略能够显著提高收敛速度，优于现有参考方法。", "keywords": "熵最优传输, Sinkhorn算法, 谱热启动, 加速收敛, 弱正则化", "comments": "本文的创新点在于利用Hessian的谱特性来设计一种新颖的谱热启动策略，以解决Sinkhorn算法在弱正则化下收敛缓慢的挑战。这种方法具有重要的实际意义，可以显著提高熵最优传输算法的效率和适用性。"}}
{"id": "2506.15043", "title": "Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures", "authors": ["Amir Hossein Baradaran"], "summary": "Advancements in the defense industry are paramount for ensuring the safety\nand security of nations, providing robust protection against emerging threats.\nAmong these threats, hypersonic missiles pose a significant challenge due to\ntheir extreme speeds and maneuverability, making accurate trajectory prediction\na critical necessity for effective countermeasures. This paper addresses this\nchallenge by employing a novel hybrid deep learning approach, integrating\nConvolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks,\nand Gated Recurrent Units (GRUs). By leveraging the strengths of these\narchitectures, the proposed method successfully predicts the complex\ntrajectories of hypersonic missiles with high accuracy, offering a significant\ncontribution to defense strategies and missile interception technologies. This\nresearch demonstrates the potential of advanced machine learning techniques in\nenhancing the predictive capabilities of defense systems.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15043v1", "AI": {"title_translation": "采用CNN-LSTM-GRU架构的高超音速导弹轨迹高级预测", "tldr": "本文提出了一种结合CNN、LSTM和GRU的混合深度学习方法，用于高精度预测高超音速导弹的复杂轨迹。", "motivation": "高超音速导弹因其极端速度和机动性对国家安全构成重大挑战，因此准确的轨迹预测对于有效的反制措施至关重要。", "method": "本文采用了一种新颖的混合深度学习方法，集成了卷积神经网络（CNN）、长短期记忆网络（LSTM）和门控循环单元（GRU）。", "result": "所提出的方法成功地高精度预测了高超音速导弹的复杂轨迹。", "conclusion": "该研究为防御策略和导弹拦截技术做出了重大贡献，并展示了先进机器学习技术在增强防御系统预测能力方面的潜力。", "translation": "国防工业的进步对于确保国家安全至关重要，能够为新兴威胁提供强大的保护。在这些威胁中，高超音速导弹因其极端速度和机动性构成了重大挑战，使得准确的轨迹预测成为有效反制措施的关键必要条件。本文通过采用一种新颖的混合深度学习方法来解决这一挑战，该方法集成了卷积神经网络（CNN）、长短期记忆（LSTM）网络和门控循环单元（GRU）。通过利用这些架构的优势，所提出的方法成功地高精度预测了高超音速导弹的复杂轨迹，为防御策略和导弹拦截技术做出了重大贡献。这项研究展示了先进机器学习技术在增强防御系统预测能力方面的潜力。", "summary": "本文针对高超音速导弹因其极端速度和机动性带来的挑战，提出了一种新颖的混合深度学习方法，该方法集成了卷积神经网络（CNN）、长短期记忆网络（LSTM）和门控循环单元（GRU）。该方法成功地高精度预测了高超音速导弹的复杂轨迹，为防御策略和导弹拦截技术提供了重要支持，并展示了先进机器学习技术在增强防御系统预测能力方面的潜力。", "keywords": "高超音速导弹, 轨迹预测, 深度学习, CNN-LSTM-GRU, 防御系统", "comments": "该论文的创新点在于其结合了CNN、LSTM和GRU的混合深度学习架构，以应对高超音速导弹轨迹预测的复杂性。这种多模型融合的方法有望捕获数据中的空间和时间特征，从而提高预测精度。该研究对于提升国家防御能力和导弹拦截效率具有重要意义，尤其是在当前高超音速武器技术发展迅速的背景下。"}}
{"id": "2506.15098", "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15098v1", "AI": {"title_translation": "增强报告审批预测：大型语言模型的比较研究", "tldr": "本研究系统评估了大型语言模型（LLM）在增强报告审批预测（ERAP）中的表现，发现LLM（特别是经过LoRA微调的Llama 3.1 8B Instruct）显著优于传统方法，提高了准确性和召回率，并有效解决了类别不平衡问题。", "motivation": "增强报告（ERs）是用户和开发者之间捕获软件改进建议的关键沟通渠道，但手动处理这些报告耗费资源，导致延迟和潜在的宝贵洞察损失。为解决此挑战，增强报告审批预测（ERAP）应运而生，旨在利用机器学习技术自动化决策。鉴于大型语言模型（LLM）的最新进展，本研究旨在探索其在提高预测准确性方面的新机遇。", "method": "本研究系统评估了18种大型语言模型（LLM）变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA和XLNet等编码器模型；GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct和DeepSeek-V3等解码器模型），并将其与传统方法（CNN/LSTM-BERT/GloVe）进行了比较。实验中还探讨了整合创建者资料以及使用LoRA对模型进行微调的影响。", "result": "实验揭示了两项关键发现：(1) 整合创建者资料将未微调的仅解码器模型的整体准确率提高了10.8%，尽管可能引入偏差；(2) 经过LoRA微调的Llama 3.1 8B Instruct进一步提升了性能，准确率达到79%，并显著提高了已批准报告的召回率（76.1% 对比 LSTM-GLOVE的64.1%），在严格的时间顺序评估下，其性能比传统方法高出5%，并有效解决了类别不平衡问题。研究还调查并总结了大型模型表现不佳的ER案例。", "conclusion": "这些发现表明，大型语言模型（LLM）是增强报告审批预测（ERAP）的卓越解决方案，展现了其在简化软件维护工作流程和改善实际开发环境中决策的潜力。研究还为未来的研究方向提供了有价值的指引。", "translation": "增强报告（ERs）是用户和开发者之间至关重要的沟通渠道，捕获了对软件改进的宝贵建议。然而，手动处理这些报告资源密集，导致延迟和潜在的宝贵洞察损失。为解决这一挑战，增强报告审批预测（ERAP）已成为研究焦点，利用机器学习技术实现决策自动化。尽管传统方法已采用基于特征的分类器和深度学习模型，但大型语言模型（LLM）的最新进展为提高预测准确性带来了新机遇。本研究系统评估了18种LLM变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA和XLNet等编码器模型；GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct和DeepSeek-V3等解码器模型），并将其与传统方法（CNN/LSTM-BERT/GloVe）进行了比较。我们的实验揭示了两项关键发现：(1) 整合创建者资料将未微调的仅解码器模型的整体准确率提高了10.8%，尽管可能引入偏差；(2) 经过LoRA微调的Llama 3.1 8B Instruct进一步提升了性能，准确率达到79%，并显著提高了已批准报告的召回率（76.1% 对比 LSTM-GLOVE的64.1%），在严格的时间顺序评估下，其性能比传统方法高出5%，并有效解决了类别不平衡问题。这些发现表明，LLM是ERAP的卓越解决方案，展现了其在简化软件维护工作流程和改善实际开发环境中决策的潜力。我们还调查并总结了大型模型表现不佳的ER案例，为未来的研究提供了有价值的方向。", "summary": "本研究旨在解决软件增强报告手动处理耗时且低效的问题，通过系统评估大型语言模型（LLM）在增强报告审批预测（ERAP）中的应用。研究比较了18种LLM变体与传统方法，并发现整合创建者资料可提高未微调解码器模型的准确性。最重要的是，经过LoRA微调的Llama 3.1 8B Instruct模型表现出最佳性能，达到79%的准确率和76.1%的批准报告召回率，显著优于传统方法，并有效解决了类别不平衡问题。研究证实LLM是ERAP的优越解决方案，有望优化软件维护流程。", "keywords": "增强报告审批预测, 大型语言模型, 软件维护, 比较研究, LoRA", "comments": "本论文的创新之处在于对多种大型语言模型在增强报告审批预测任务中进行了全面且系统的比较研究，清晰地展示了LLM相对于传统方法的显著优势。特别值得注意的是，研究不仅验证了LLM的强大能力，还深入探讨了整合创建者资料和LoRA微调等策略对模型性能的具体影响，提供了宝贵的实践指导。此外，对LLM表现不佳案例的分析，为未来研究指明了方向，具有重要的启发意义。"}}
{"id": "2506.14809", "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model", "authors": ["Peng Jiang", "Vinicius Cezar Monteiro de Lira", "Antonio Maiorino"], "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating\nhigh-quality surveys remains labor-intensive, requiring both domain expertise\nand methodological rigor. With the evolution of large language models (LLMs),\nnew opportunities emerge to automate survey generation. This paper presents the\nreal-world deployment of an LLM-powered system designed to accelerate data\ncollection while maintaining survey quality. Deploying such systems in\nproduction introduces real-world complexity, including diverse user needs and\nquality control. We evaluate the system using the DeLone and McLean IS Success\nModel to understand how generative AI can reshape a core IS method. This study\nmakes three key contributions. To our knowledge, this is the first application\nof the IS Success Model to a generative AI system for survey creation. In\naddition, we propose a hybrid evaluation framework combining automated and\nhuman assessments. Finally, we implement safeguards that mitigate\npost-deployment risks and support responsible integration into IS workflows.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14809v1", "AI": {"title_translation": "部署的LLM问卷创建工具对信息系统成功模型的影响", "tldr": "本文评估了一个已部署的LLM驱动的问卷创建系统，使用信息系统成功模型来理解生成式AI如何改变信息系统方法，并提出了混合评估框架和风险防范措施。", "motivation": "问卷是信息系统（IS）研究的基石，但创建高质量问卷仍然是劳动密集型工作，需要领域专业知识和方法严谨性。大型语言模型（LLM）的演进为自动化问卷生成带来了新机遇。", "method": "本文部署了一个LLM驱动的系统以加速数据收集并保持问卷质量，并使用DeLone和McLean信息系统成功模型进行评估。研究还提出了结合自动化和人工评估的混合评估框架，并实施了减轻部署后风险的保障措施。", "result": "首次将信息系统成功模型应用于生成式AI问卷创建系统；提出了结合自动化和人工评估的混合评估框架；实施了减轻部署后风险并支持负责任地集成到信息系统工作流程中的保障措施。", "conclusion": "部署LLM驱动的问卷创建工具可以重塑信息系统核心方法，并通过系统性评估和风险缓解实现负责任的集成。", "translation": "问卷是信息系统（IS）研究的基石，然而创建高质量问卷仍然是劳动密集型工作，需要领域专业知识和方法严谨性。随着大型语言模型（LLM）的演进，自动化问卷生成的新机遇随之出现。本文介绍了一个LLM驱动系统的实际部署，该系统旨在加速数据收集同时保持问卷质量。在生产环境中部署此类系统会带来现实世界的复杂性，包括多样化的用户需求和质量控制。我们使用DeLone和McLean信息系统成功模型评估了该系统，以了解生成式AI如何重塑核心信息系统方法。这项研究做出了三项关键贡献。据我们所知，这是信息系统成功模型首次应用于生成式AI问卷创建系统。此外，我们提出了一种结合自动化和人工评估的混合评估框架。最后，我们实施了减轻部署后风险并支持负责任地集成到信息系统工作流程中的保障措施。", "summary": "本文研究了部署LLM驱动的问卷创建工具对信息系统研究的影响。通过部署一个旨在加速数据收集和保持问卷质量的LLM系统，并利用DeLone和McLean信息系统成功模型对其进行评估，探讨了生成式AI如何改变信息系统方法。研究提出了混合评估框架并实施了风险保障措施，首次将IS成功模型应用于生成式AI系统，为负责任地集成AI工具提供了见解。", "keywords": "LLM, 问卷创建, 信息系统成功模型, 生成式AI, 混合评估", "comments": "本文的创新之处在于首次将信息系统成功模型应用于评估生成式AI系统，特别是用于问卷创建的系统，这为理解AI工具在特定领域的影响提供了一个结构化的评估框架。此外，提出的混合评估框架和风险防范措施对于AI工具的实际部署和负责任的集成具有重要意义。"}}
{"id": "2506.15397", "title": "Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control", "authors": ["Sepehr Elahi", "Paula Mürmann", "Patrick Thiran"], "summary": "The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15397v1", "AI": {"title_translation": "学习接种疫苗：结合结构学习和有效疫苗接种用于流行病和疫情控制", "tldr": "本文提出了一种结合图结构学习和疫苗接种策略的方法，以在未知网络结构下最小化SIS模型中疾病的流行时间。", "motivation": "以前关于最佳疫苗接种的研究假设图结构是已知的，但这在实际中往往不成立。因此，在图结构未知且仅能观察到顶点感染状态的情况下，如何有效接种以最小化疾病消亡时间是一个亟待解决的问题。", "method": "作者将问题分解为图学习和疫苗接种策略制定两部分。他们提出了一种新颖的基于包含-排除的学习算法，并确定了其图恢复的样本复杂度。接着，他们详细介绍了一种针对谱半径最小化（SRM）问题的最优算法，并证明了其对于有界树宽的图的运行时间是多项式时间。此外，还为任意图提供了一种高效且有效的多项式时间贪婪启发式算法。", "result": "数值实验结果验证了所提出的学习和疫苗接种算法在合成数据和真实世界数据上的有效性。", "conclusion": "本文成功地解决了在图结构未知的情况下，通过结合图学习和有效的疫苗接种策略来最小化SIS模型中疾病流行时间的问题，并提供了理论保证和实验验证。", "translation": "易感-感染-易感（SIS）模型是一种广泛用于图上信息传播和传染病（特别是非免疫性疾病）传播的模型。对于一种高度传染性的疾病，一个自然的问题是如何最佳地接种个体以最大程度地缩短疾病的消亡时间。虽然以前的工作表明最佳疫苗接种问题与NP难的谱半径最小化（SRM）问题密切相关，但它们都假设图是已知的，这在实践中往往并非如此。在这项工作中，我们考虑了在图结构未知且仅能观察到顶点感染状态的情况下，如何最小化由SIS模型建模的疫情消亡时间的问题。为此，我们将问题分为两部分：学习图结构和确定有效的疫苗接种策略。我们提出了一种新颖的基于包含-排除的学习算法，并且与以前的方法不同，我们建立了其图恢复的样本复杂度。然后，我们详细介绍了一种针对SRM问题的最优算法，并证明了其对于具有有界树宽的图，运行时间是顶点数量的多项式。此外，我们还为任意图提供了一种高效且有效的多项式时间贪婪启发式算法。最后，我们展示了在合成数据和真实世界数据上的实验，这些实验数值验证了我们的学习和疫苗接种算法。", "summary": "本文研究了在图结构未知但可观察到顶点感染状态的情况下，如何通过有效疫苗接种来最小化SIS模型中疾病流行时间的问题。作者将该问题分解为图结构学习和疫苗接种策略制定两部分。他们提出了一种基于包含-排除的新型图学习算法，并分析了其样本复杂度。同时，针对NP难的谱半径最小化（SRM）问题，提出了一种对有界树宽图有效的最优多项式时间算法，并为任意图提供了一种高效的贪婪启发式算法。实验结果验证了所提算法的有效性。", "keywords": "SIS模型, 疫情控制, 图学习, 疫苗接种, 谱半径最小化", "comments": "这篇论文的创新点在于解决了在实际应用中常见的图结构未知情况下进行流行病控制的挑战。通过将问题分解为图学习和疫苗接种策略，并为每个子问题提供了理论上具有保证（样本复杂度、多项式时间）和实践上有效的解决方案（贪婪启发式），提升了实际应用价值。"}}
{"id": "2506.14968", "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization", "authors": ["Rajat Kumar Jenamani", "Tom Silver", "Ben Dodson", "Shiqin Tong", "Anthony Song", "Yuting Yang", "Ziang Liu", "Benjamin Howe", "Aimee Whitneck", "Tapomayukh Bhattacharjee"], "summary": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast", "comment": "RSS 2025 - Outstanding Paper Award & Outstanding Systems Paper Award\n  Finalist", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14968v1", "AI": {"title_translation": "FEAST：一种面向野外个性化的灵活进餐辅助系统", "tldr": "FEAST是一种灵活的机器人进餐辅助系统，通过模块化硬件、多样化交互和大型语言模型驱动的行为树，实现了野外个性化，成功帮助用户满足其独特的进餐需求。", "motivation": "现有的物理看护机器人虽然有潜力改善生活质量，但在家庭环境中提供进餐辅助仍面临挑战，因为部署过程中会出现活动多样性、情境多样性、食物种类和用户偏好等问题。", "method": "本文提出了FEAST系统，一个灵活的进餐辅助系统，旨在实现野外个性化。它基于三个关键原则：适应性、透明性和安全性。FEAST通过以下方式实现这些原则：(i) 模块化硬件，可在辅助喂食、饮水和擦嘴之间切换；(ii) 多样化的交互方法（网络界面、头部手势、物理按钮）；(iii) 可通过大型语言模型安全透明地调整的参数化行为树。系统通过评估个性化需求、与基线系统比较、进行家庭用户研究（与两位护理接受者进行三次进餐）以及由职业治疗师评估来验证。", "result": "FEAST系统提供了广泛的透明和安全适应性，并且优于仅限于固定定制的现有基线系统。在所有评估案例中，用户都成功地个性化FEAST以满足其个人需求和偏好。", "conclusion": "FEAST系统通过其灵活的设计和个性化能力，能够有效地在现实世界中满足护理接受者的独特进餐辅助需求。", "translation": "**FEAST: 一种面向野外个性化的灵活进餐辅助系统**\n物理看护机器人有望改善全球数百万需要喂食帮助的人的生活质量。然而，由于部署过程中出现的活动（例如，进食、饮水、擦嘴）、情境（例如，社交、看电视）、食物种类和用户偏好等多样性，家庭进餐辅助仍然充满挑战。在这项工作中，我们提出了FEAST，一个灵活的进餐辅助系统，可以在野外进行个性化，以满足个体护理接受者的独特需求。该系统是与两名社区研究人员合作开发，并参考了对不同护理接受者群体进行的形成性研究，其设计遵循野外个性化的三个关键原则：适应性、透明性和安全性。FEAST通过以下方式体现了这些原则：(i) 模块化硬件，可在辅助喂食、饮水和擦嘴之间切换；(ii) 多样化的交互方法，包括网络界面、头部手势和物理按钮，以适应不同的功能能力和偏好；(iii) 可使用大型语言模型安全透明地调整的参数化行为树。我们根据形成性研究中确定的个性化需求对系统进行了评估，结果表明FEAST提供了广泛的透明和安全适应性，并且优于受限于固定定制的现有基线系统。为了展示实际应用性，我们与两名护理接受者（他们是社区研究人员）进行了家庭用户研究，在三种不同的场景中为他们提供了三餐。我们还通过一位先前不熟悉该系统的职业治疗师的评估，进一步评估了FEAST的生态有效性。在所有情况下，用户都成功地个性化FEAST以满足他们的个人需求和偏好。网站：https://emprise.cs.cornell.edu/feast", "summary": "FEAST是一个创新的机器人进餐辅助系统，旨在克服现有家庭护理机器人面临的挑战，通过提供野外个性化能力来满足个体用户的独特需求。系统采用模块化硬件、多样化交互方式和LLM驱动的行为树，实现了适应性、透明性和安全性。评估结果表明，FEAST在提供个性化辅助方面表现出色，并优于传统基线系统，成功地在真实家庭环境中帮助用户。", "keywords": "进餐辅助系统, 个性化, 机器人护理, 行为树, 大型语言模型", "comments": "这篇论文的创新点在于其“野外个性化”的概念，通过结合模块化硬件、多模态交互和大型语言模型来动态适应用户的独特需求和不断变化的环境。系统对适应性、透明性和安全性的强调，以及与社区研究人员的合作和实际家庭用户研究，增加了其现实应用价值和可信度。FEAST超越了传统固定定制的限制，为未来护理机器人提供了新的设计范式。"}}
{"id": "2506.15127", "title": "New Constructions of Full Flag Codes Based on Partial Spreads", "authors": ["Xiang Han", "Xinran Li", "Gang Wang"], "summary": "Flag codes are a class of multishot network codes comprising sequences of\nnested subspaces (flags) within the vector space $\\mathbb{F}_q^n$, where $q$ is\na prime power. In this paper, we propose a family of constructions for full\nflag codes based on partial spreads. The distances of this family include\nmaximum distance (optimum distance flag codes), second-maximum distance\n(quasi-optimum distance flag codes), as well as other feasible values. The\nstructure of these flag codes resembles that of a \\textquotedblleft sandwich\",\nconsisting of one layer of companion matrix and two layers of partial spreads.\nFurthermore, we present an efficient decoding algorithm for these codes.", "comment": "24 pages", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15127v1", "AI": {"title_translation": "基于部分扩延的全旗码新构造", "tldr": "本文提出了一种基于部分扩延的全旗码新构造方法，其码距包括最大和次最大距离，并提供了一种高效的解码算法。", "motivation": "论文旨在提出一种新的全旗码构造方法，旗码是多射网络编码中的一类。", "method": "论文提出了一种基于部分扩延的全旗码构造家族。这些旗码的结构类似于“三明治”，由一层伴随矩阵和两层部分扩延组成。", "result": "构造的旗码的码距包括最大距离（最优距离旗码）、次最大距离（准最优距离旗码）以及其他可行值。此外，还提出了一种针对这些编码的高效解码算法。", "conclusion": "论文成功提出了基于部分扩延的全旗码新构造，并展示了其优越的码距特性和高效的解码能力。", "translation": "旗码是一类多射网络编码，由向量空间 $\\mathbb{F}_q^n$ 中的嵌套子空间序列（旗）组成，其中 $q$ 是素数幂。本文提出了一系列基于部分扩延的全旗码构造。该家族的码距包括最大距离（最优距离旗码）、次最大距离（准最优距离旗码）以及其他可行值。这些旗码的结构类似于“三明治”，由一层伴随矩阵和两层部分扩延组成。此外，我们还提出了一种针对这些编码的高效解码算法。", "summary": "本文提出了一种基于部分扩延的全旗码新构造方法。这种构造产生的旗码具有多种码距，包括最优和准最优距离，并且其结构被描述为“三明治”型，包含伴随矩阵和部分扩延层。论文还提供了一种高效的解码算法。", "keywords": "旗码, 部分扩延, 网络编码, 解码算法, 全旗码", "comments": "这项工作在网络编码领域，特别是旗码的构造方面，提供了新的视角。通过结合部分扩延和伴随矩阵，实现了具有优越距离特性（包括最优和准最优距离）的旗码，并且提出了高效的解码算法，这对于实际应用具有重要意义。"}}
{"id": "2506.15440", "title": "Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration", "authors": ["Omar Numan", "Gaurav Singh", "Kazybek Adam", "Jelin Leslin", "Aleksi Korsman", "Otto Simola", "Marko Kosunen", "Jussi Ryynänen", "Martin Andraud"], "summary": "Developing accurate and reliable Compute-In-Memory (CIM) architectures is\nbecoming a key research focus to accelerate Artificial Intelligence (AI) tasks\non hardware, particularly Deep Neural Networks (DNNs). In that regard, there\nhas been significant interest in analog and mixed-signal CIM architectures\naimed at increasing the efficiency of data storage and computation to handle\nthe massive amount of data needed by DNNs. Specifically, resistive mixed-signal\nCIM cores are pushed by recent progresses in emerging Non-Volatile Memory\n(eNVM) solutions. Yet, mixed-signal CIM computing cores still face several\nintegration and reliability challenges that hinder their large-scale adoption\ninto end-to-end AI computing systems. In terms of integration, resistive and\neNVM-based CIM cores need to be integrated with a control processor to realize\nend-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still\nmore efficient and easier to program than their eNVM counterparts. In terms of\nreliability, analog circuits are more susceptible to variations, leading to\ncomputation errors and degraded accuracy. This work addresses these two\nchallenges by proposing a self-calibrated mixed-signal CIM accelerator SoC,\nfabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the\nCIM architecture, combining the density and ease of SRAM-based weight storage\nwith multi-bit computation using linear resistors, and (2) an open-source\nprogramming and testing strategy for CIM systems. The accuracy and reliability\nare enabled through an automated RISC-V controlled on-chip calibration,\nallowing us to improve the compute SNR by 25 to 45% across multiple columns to\nreach 18-24 dB. To showcase further integration possibilities, we show how our\nproof-of-concept SoC can be extended to recent high-density linear resistor\ntechnologies for enhanced computing performance.", "comment": "This work has been submitted to the IEEE for possible publication. 12\n  pages, 10 figures, 2 tables", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15440v1", "AI": {"title_translation": "Acore-CIM：构建带RISC-V控制自校准的精确可靠混合信号CIM核心", "tldr": "Acore-CIM 提出了一种自校准混合信号CIM加速器SoC，结合SRAM和线性电阻，并利用RISC-V进行片上校准，以解决CIM核心的集成和可靠性挑战，提高计算信噪比。", "motivation": "开发精确可靠的存内计算（CIM）架构是加速人工智能（AI）任务，特别是深度神经网络（DNN）的关键研究焦点。混合信号CIM核心面临集成和可靠性挑战，阻碍其大规模应用。具体来说，电阻式CIM核心需要与控制处理器集成，且模拟电路易受变异影响导致计算误差和精度下降。", "method": "本工作提出了一种自校准混合信号CIM加速器SoC，采用22纳米FDSOI技术制造。通过以下方式促进集成：1) 结合SRAM的密度和易用性与线性电阻的多比特计算的CIM架构；2) 开源的CIM系统编程和测试策略。通过自动化的RISC-V控制片上校准实现精度和可靠性。", "result": "通过自动化的RISC-V控制片上校准，计算信噪比（SNR）在多个列上提高了25%至45%，达到18-24 dB。证明了该概念验证SoC可以扩展到最新的高密度线性电阻技术，以增强计算性能。", "conclusion": "Acore-CIM成功解决了混合信号CIM计算核心的集成和可靠性挑战，通过结合创新的CIM架构和RISC-V控制的自校准，显著提高了计算精度，并展示了未来与高密度电阻技术集成的潜力。", "translation": "开发精确可靠的存内计算（CIM）架构正成为加速硬件上人工智能（AI）任务，特别是深度神经网络（DNN）的关键研究焦点。在这方面，模拟和混合信号CIM架构引起了极大的兴趣，旨在提高数据存储和计算效率，以处理DNN所需的大量数据。具体来说，新兴非易失性存储器（eNVM）解决方案的最新进展推动了电阻式混合信号CIM核心的发展。然而，混合信号CIM计算核心仍面临若干集成和可靠性挑战，阻碍了其大规模应用于端到端AI计算系统。在集成方面，基于电阻和eNVM的CIM核心需要与控制处理器集成以实现端到端AI加速。此外，基于SRAM的CIM架构仍然比其eNVM对应物更高效且更易于编程。在可靠性方面，模拟电路更容易受到变化的影响，导致计算错误和精度下降。这项工作通过提出一种自校准混合信号CIM加速器SoC来解决这两个挑战，该SoC采用22纳米FDSOI技术制造。集成通过以下方式实现：（1）CIM架构，结合了基于SRAM的权重存储的密度和易用性与使用线性电阻的多比特计算；（2）用于CIM系统的开源编程和测试策略。通过自动化的RISC-V控制片上校准实现了精度和可靠性，使我们能够将多个列的计算信噪比提高25%至45%，达到18-24 dB。为了展示进一步的集成可能性，我们展示了我们的概念验证SoC如何扩展到最新的高密度线性电阻技术以增强计算性能。", "summary": "Acore-CIM提出了一种22纳米FDSOI工艺的自校准混合信号CIM加速器SoC，旨在解决当前CIM核心在集成和可靠性方面的挑战。该架构结合了SRAM的存储优势和线性电阻的多比特计算能力，并通过开源策略简化编程。核心创新在于利用RISC-V控制的自动化片上校准，显著提升了计算精度和可靠性，将计算信噪比提高了25-45%，达到18-24 dB。该工作还展示了其与高密度线性电阻技术的扩展集成潜力。", "keywords": "存内计算, 混合信号, 自校准, RISC-V, 人工智能加速", "comments": "该论文的创新点在于其提出了一种结合SRAM和线性电阻的混合信号CIM架构，并利用RISC-V处理器实现了片上自校准，有效解决了模拟CIM核心的集成和可靠性两大难题。这种自校准机制显著提高了计算精度和信噪比，对于推动CIM技术在AI硬件加速中的实际应用具有重要意义。同时，开源的编程和测试策略也降低了CIM系统的开发门槛。"}}
{"id": "2506.15114", "title": "Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library", "authors": ["Youjia Li", "Robert Latham", "Robert Ross", "Ankit Agrawal", "Alok Choudhary", "Wei-Keng Liao"], "summary": "High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by\nlarge-scale scientific applications to perform I/O tasks in parallel. These I/O\nlibraries store the metadata such as data types and dimensionality along with\nthe raw data in the same files. While these libraries are well-optimized for\nconcurrent access to the raw data, they are designed neither to handle a large\nnumber of data objects efficiently nor to create different data objects\nindependently by multiple processes, as they require applications to call data\nobject creation APIs collectively with consistent metadata among all processes.\nApplications that process data gathered from remote sensors, such as particle\ncollision experiments in high-energy physics, may generate data of different\nsizes from different sensors and desire to store them as separate data objects.\nFor such applications, the I/O library's requirement on collective data object\ncreation can become very expensive, as the cost of metadata consistency check\nincreases with the metadata volume as well as the number of processes. To\naddress this limitation, using PnetCDF as an experimental platform, we\ninvestigate solutions in this paper that abide the netCDF file format, as well\nas propose a new file header format that enables independent data object\ncreation. The proposed file header consists of two sections, an index table and\na list of metadata blocks. The index table contains the reference to the\nmetadata blocks and each block stores metadata of objects that can be created\ncollectively or independently. The new design achieves a scalable performance,\ncutting data object creation times by up to 582x when running on 4096 MPI\nprocesses to create 5,684,800 data objects in parallel. Additionally, the new\nmethod reduces the memory footprints, with each process requiring an amount of\nmemory space inversely proportional to the number of processes.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15114v1", "AI": {"title_translation": "并行数据对象创建：迈向高性能I/O库中可伸缩元数据管理", "tldr": "本文提出了一种新的文件头格式，以解决现有高性能I/O库在并行创建大量数据对象时元数据管理效率低下的问题，实现高达582倍的性能提升和内存占用减少。", "motivation": "现有的HDF5和PnetCDF等高性能I/O库在并行I/O任务中，要求应用程序集体调用数据对象创建API，并保持元数据一致性。这导致在处理来自不同传感器、大小不同的数据时，集体数据对象创建成本非常高，因为元数据一致性检查的成本随元数据量和进程数增加。", "method": "以PnetCDF作为实验平台，本文研究了符合netCDF文件格式的解决方案，并提出了一种新的文件头格式，以实现独立的数据对象创建。所提出的文件头包含两个部分：索引表和元数据块列表。索引表包含对元数据块的引用，每个块存储可以集体或独立创建的对象的元数据。", "result": "新的设计实现了可伸缩的性能，在4096个MPI进程并行创建5,684,800个数据对象时，数据对象创建时间缩短了高达582倍。此外，新方法减少了内存占用，每个进程所需的内存空间与进程数量成反比。", "conclusion": "通过引入新的文件头格式，该研究成功解决了高性能I/O库在并行数据对象创建中元数据管理的可伸缩性问题，显著提高了创建效率并降低了内存消耗。", "translation": "高级I/O库，如HDF5和PnetCDF，常被大型科学应用程序用于并行执行I/O任务。这些I/O库将数据类型和维度等元数据与原始数据存储在同一文件中。虽然这些库已针对原始数据的并发访问进行了优化，但它们并非旨在高效处理大量数据对象，也无法由多个进程独立创建不同的数据对象，因为它们要求应用程序在所有进程之间以一致的元数据集体调用数据对象创建API。处理来自远程传感器（如高能物理中的粒子碰撞实验）数据的应用程序可能会生成来自不同传感器的不同大小的数据，并希望将它们存储为独立的数据对象。对于此类应用程序，I/O库对集体数据对象创建的要求可能变得非常昂贵，因为元数据一致性检查的成本随元数据量和进程数的增加而增加。为解决此限制，本文以PnetCDF作为实验平台，研究了符合netCDF文件格式的解决方案，并提出了一种新的文件头格式，以实现独立的数据对象创建。所提出的文件头包含两个部分：索引表和元数据块列表。索引表包含对元数据块的引用，每个块存储可以集体或独立创建的对象的元数据。新设计实现了可伸缩的性能，在4096个MPI进程并行创建5,684,800个数据对象时，数据对象创建时间缩短了高达582倍。此外，新方法减少了内存占用，每个进程所需的内存空间与进程数量成反比。", "summary": "本文针对HDF5和PnetCDF等高性能I/O库在并行创建大量独立数据对象时元数据管理效率低下的问题，提出了一种新的文件头格式。该格式包含索引表和元数据块列表，允许独立或集体创建数据对象。实验结果表明，新方法在并行数据对象创建中将时间缩短了高达582倍，并显著降低了内存占用，有效提升了大规模科学应用中的I/O性能。", "keywords": "高性能I/O, 元数据管理, 并行数据创建, PnetCDF, 文件头格式", "comments": "该论文通过提出创新的文件头格式，有效解决了现有高性能I/O库在并行数据对象创建中元数据管理的可伸缩性瓶颈。其在性能上的显著提升（高达582倍）和内存效率的优化，对于处理大规模异构数据的科学应用具有重要意义。该方案在遵守现有文件格式标准的同时，实现了对独立数据对象创建的支持，展现了良好的工程实践。"}}
{"id": "2506.14823", "title": "ViLLa: A Neuro-Symbolic approach for Animal Monitoring", "authors": ["Harsha Koduri"], "summary": "Monitoring animal populations in natural environments requires systems that\ncan interpret both visual data and human language queries. This work introduces\nViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for\ninterpretable animal monitoring. ViLLa integrates three core components: a\nvisual detection module for identifying animals and their spatial locations in\nimages, a language parser for understanding natural language queries, and a\nsymbolic reasoning layer that applies logic-based inference to answer those\nqueries. Given an image and a question such as \"How many dogs are in the\nscene?\" or \"Where is the buffalo?\", the system grounds visual detections into\nsymbolic facts and uses predefined rules to compute accurate answers related to\ncount, presence, and location. Unlike end-to-end black-box models, ViLLa\nseparates perception, understanding, and reasoning, offering modularity and\ntransparency. The system was evaluated on a range of animal imagery tasks and\ndemonstrates the ability to bridge visual content with structured,\nhuman-interpretable queries.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14823v1", "AI": {"title_translation": "ViLLa：一种用于动物监测的神经符号方法", "tldr": "ViLLa是一个神经符号框架，通过结合视觉检测、语言解析和符号推理，实现可解释的动物监测，能够回答关于图像中动物数量、存在和位置的问题。", "motivation": "在自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。", "method": "ViLLa（Vision-Language-Logic Approach）是一个神经符号框架，集成了三个核心组件：用于识别图像中动物及其空间位置的视觉检测模块、用于理解自然语言查询的语言解析器，以及应用基于逻辑推理来回答这些查询的符号推理层。系统将视觉检测结果转化为符号事实，并使用预定义规则计算与计数、存在和位置相关的答案。", "result": "该系统在各种动物图像任务上进行了评估，并展示了将视觉内容与结构化、人类可解释查询连接起来的能力。", "conclusion": "ViLLa与端到端黑盒模型不同，它将感知、理解和推理分离，提供了模块化和透明度。", "translation": "在自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。这项工作引入了ViLLa（Vision-Language-Logic Approach），一个为可解释动物监测设计的神经符号框架。ViLLa集成了三个核心组件：用于识别图像中动物及其空间位置的视觉检测模块，用于理解自然语言查询的语言解析器，以及应用基于逻辑推理来回答这些查询的符号推理层。给定一张图像和一个问题，例如“场景中有多少只狗？”或“水牛在哪里？”，系统将视觉检测结果转化为符号事实，并使用预定义规则计算与计数、存在和位置相关的准确答案。与端到端黑盒模型不同，ViLLa将感知、理解和推理分离，提供了模块化和透明度。该系统在各种动物图像任务上进行了评估，并展示了将视觉内容与结构化、人类可解释查询连接起来的能力。", "summary": "ViLLa是一个神经符号框架，旨在实现可解释的动物监测。它结合了视觉检测、自然语言处理和符号推理，能够将图像中的动物信息转化为符号事实，并利用逻辑规则回答关于动物数量、位置和存在性的自然语言查询。该系统通过分离感知、理解和推理，提供了模块化和透明度，并在动物图像任务中表现出连接视觉内容与人类可解释查询的能力。", "keywords": "神经符号, 动物监测, 视觉语言, 符号推理, 可解释性", "comments": "ViLLa的创新在于其神经符号方法，它通过结合深度学习的感知能力和符号推理的解释性与透明度，克服了传统黑盒模型的局限性。这种模块化设计使其更易于理解和调试，对于需要高可解释性的动物监测等应用场景具有重要意义。"}}
{"id": "2506.15196", "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges", "authors": ["Xianliang Yang", "Ling Zhang", "Haolong Qian", "Lei Song", "Jiang Bian"], "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.", "comment": "27 pages,9 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15196v1", "AI": {"title_translation": "HeurAgenix：利用大型语言模型解决复杂组合优化挑战", "tldr": "HeurAgenix是一个由LLM驱动的两阶段超启发式框架，用于解决组合优化问题，它能进化启发式算法并自动选择最佳算法，表现优于现有LLM方法并媲美专业求解器。", "motivation": "传统启发式算法设计依赖人工专业知识，且难以泛化到多样实例。", "method": "HeurAgenix是一个两阶段的LLM驱动超启发式框架。在启发式进化阶段，LLM比较种子解和高质量解以提取可重用进化策略。在问题解决阶段，LLM动态选择最有前景的启发式算法，可选择使用SOTA LLM或轻量级微调模型。为解决监督数据稀缺问题，轻量级选择器通过双重奖励机制（结合选择偏好和状态感知信号）进行微调。", "result": "在经典基准测试中，HeurAgenix不仅超越了现有基于LLM的超启发式算法，而且达到了或超过了专业求解器的性能。", "conclusion": "HeurAgenix通过LLM驱动的超启发式方法，有效解决了组合优化问题中传统启发式算法的泛化和设计难题，并展示了超越现有LLM方法和媲美专业求解器的强大性能。", "translation": "启发式算法在解决组合优化（CO）问题中发挥着至关重要的作用，然而传统设计严重依赖人工专业知识，并且难以泛化到多样化的实例。我们引入了**HeurAgenix**，一个由大型语言模型（LLM）驱动的两阶段超启发式框架，它首先进化启发式算法，然后自动从中进行选择。在启发式进化阶段，HeurAgenix利用LLM比较种子启发式解与更高质量的解，并提取可重用的进化策略。在问题解决过程中，它在LLM感知能力的指导下，为每个问题状态动态选择最有前景的启发式算法。为了灵活性，这个选择器可以是最先进的LLM，也可以是经过微调的、推理成本较低的轻量级模型。为了缓解由CO复杂性引起的可靠监督稀缺问题，我们使用双重奖励机制微调轻量级启发式选择器，该机制共同利用选择偏好和状态感知信号，从而在噪声标注下实现稳健选择。在经典基准测试上的大量实验表明，HeurAgenix不仅优于现有基于LLM的超启发式算法，而且达到了或超过了专业求解器的性能。代码可在https://github.com/microsoft/HeurAgenix获取。", "summary": "HeurAgenix是一个创新的两阶段LLM驱动超启发式框架，旨在克服传统启发式算法在组合优化问题中依赖人工专业知识和泛化能力差的局限性。它通过LLM进化启发式策略并动态选择最佳策略，同时采用双重奖励机制微调轻量级模型以应对监督数据稀缺。实验证明其性能优于现有LLM方法并与专业求解器相当。", "keywords": "组合优化, 大型语言模型, 超启发式, 启发式进化, 动态选择", "comments": "这篇论文通过引入LLM来自动化启发式算法的进化和选择过程，为解决复杂组合优化问题提供了一个新颖且高效的方案。其创新之处在于利用LLM的感知能力进行策略提取和动态选择，并通过双重奖励机制解决了监督数据稀缺的问题，显著提升了泛化能力和性能。"}}
{"id": "2506.14783", "title": "ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification", "authors": ["Mohamed Masry", "Mohamed Amen", "Mohamed Elzyat", "Mohamed Hamed", "Norhan Magdy", "Maram Khaled"], "summary": "Decoding natural language from brain activity using non-invasive\nelectroencephalography (EEG) remains a significant challenge in neuroscience\nand machine learning, particularly for open-vocabulary scenarios where\ntraditional methods struggle with noise and variability. Previous studies have\nachieved high accuracy on small-closed vocabularies, but it still struggles on\nopen vocabularies. In this study, we propose ETS, a framework that integrates\nEEG with synchronized eye-tracking data to address two critical tasks: (1)\nopen-vocabulary text generation and (2) sentiment classification of perceived\nlanguage. Our model achieves a superior performance on BLEU and Rouge score for\nEEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment\nclassification, which significantly outperforms supervised baselines.\nFurthermore, we show that our proposed model can handle data from various\nsubjects and sources, showing great potential for high performance open\nvocabulary eeg-to-text system.", "comment": "Graduation project report submitted at Faculty of Computer Science\n  and Artificial Intelligence, Helwan University", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14783v1", "AI": {"title_translation": "ETS：开放词汇脑电图到文本解码和情感分类", "tldr": "ETS是一个结合脑电图和眼动追踪的框架，用于开放词汇文本生成和情感分类，性能优于基线。", "motivation": "从脑电活动中解码自然语言，特别是在开放词汇场景下，是一个重大挑战，传统方法难以处理噪声和变异性，且之前的研究在开放词汇上表现不佳。", "method": "提出了ETS框架，该框架将脑电图（EEG）与同步的眼动追踪数据相结合，以解决开放词汇文本生成和感知语言情感分类这两个任务。", "result": "该模型在EEG-To-Text解码方面取得了优异的BLEU和Rouge分数，在基于EEG的三元情感分类方面F1分数提高了10%，显著优于有监督的基线。此外，该模型能够处理来自不同受试者和来源的数据。", "conclusion": "该模型在开放词汇脑电图到文本解码和情感分类方面表现出色，并展示了在处理多源数据方面的巨大潜力，有望实现高性能的开放词汇脑电图到文本系统。", "translation": "从非侵入性脑电图（EEG）解码自然语言仍然是神经科学和机器学习领域的一个重大挑战，特别是在开放词汇场景下，传统方法难以处理噪声和变异性。以前的研究在小封闭词汇上取得了高准确率，但在开放词汇上仍然存在困难。在这项研究中，我们提出了ETS，一个将脑电图与同步眼动追踪数据相结合的框架，以解决两个关键任务：（1）开放词汇文本生成和（2）感知语言的情感分类。我们的模型在EEG到文本解码方面取得了优异的BLEU和Rouge分数，在基于EEG的三元情感分类方面F1分数提高了10%，显著优于有监督的基线。此外，我们表明我们提出的模型可以处理来自不同受试者和来源的数据，显示出高性能开放词汇脑电图到文本系统的巨大潜力。", "summary": "本文提出了ETS框架，将脑电图（EEG）与眼动追踪数据结合，解决了从脑电活动中进行开放词汇文本生成和情感分类的挑战。该模型在两项任务上均显著优于现有基线，尤其在开放词汇文本解码和三元情感分类方面表现突出，并能适应多源数据，预示着高性能脑电图到文本系统的巨大前景。", "keywords": "脑电图, 文本解码, 情感分类, 开放词汇, 眼动追踪", "comments": "该研究的创新点在于将眼动追踪数据与EEG结合，以克服开放词汇解码的挑战，并成功应用于文本生成和情感分类。其在开放词汇场景下的优异表现，特别是对多源数据的处理能力，为非侵入性脑机接口和辅助交流领域带来了新的突破。"}}
{"id": "2506.15113", "title": "Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning", "authors": ["Min Namgung", "JangHyeon Lee", "Fangyi Ding", "Yao-Yi Chiang"], "summary": "Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15113v1", "AI": {"title_translation": "交通普惠：利用区域表示学习绘制公平的自行车到地铁连接", "tldr": "研究提出TFA框架，利用区域表示学习预测自行车需求，评估交通可达性，并推荐新站点，以促进公平的自行车共享系统扩展，减少城市弱势群体的交通不平等。", "motivation": "确保公平的公共交通可达性仍具挑战，尤其是在人口稠密的城市，低收入和少数族裔社区常面临有限的交通可达性。自行车共享系统（BSS）虽能弥补差距，但向服务不足社区扩展困难，因新站点需求不确定且传统可达性指标有局限性。", "method": "引入Transit for All (TFA) 空间计算框架，包含三个部分：(1) 利用区域表示学习整合多模态地理空间数据，预测冷启动站点的自行车共享需求；(2) 结合预测的自行车共享需求和传统交通可达性指标，利用新型加权公共交通可达性水平（wPTAL）进行综合交通可达性评估；(3) 考虑潜在乘客量和公平性提升，提供新自行车站点放置的战略建议。", "result": "以纽约市为案例研究，识别出对历史服务不足社区中低收入和少数族裔群体影响尤为严重的可达性差距。结果表明，由wPTAL指导的战略性新站点放置显著减少了与经济和人口因素相关的交通可达性差异。", "conclusion": "本研究表明，TFA为城市规划者提供了实用指导，以促进公平交通并提升服务不足城市社区的生活质量。", "translation": "确保公平的公共交通可达性仍然充满挑战，特别是在纽约市等人口稠密的城市，低收入和少数族裔社区往往面临有限的交通可达性。自行车共享系统（BSS）可以通过提供经济实惠的第一英里和最后一英里连接来弥补这些公平差距。然而，由于新规划（“冷启动”）站点位置的自行车共享需求不确定性以及传统可达性指标可能忽略实际自行车使用潜力，战略性地将BSS扩展到服务不足的社区变得困难。我们引入了“交通普惠”（Transit for All, TFA），一个空间计算框架，旨在通过三个组件指导BSS的公平扩展：(1) 利用整合多模态地理空间数据的区域表示学习，预测冷启动站点的空间感知自行车共享需求；(2) 通过结合预测的自行车共享需求和传统交通可达性指标，利用我们新颖的加权公共交通可达性水平（wPTAL）进行综合交通可达性评估；(3) 考虑潜在乘客量和公平性提升，提供新自行车站点的战略性放置建议。我们以纽约市为案例研究，识别出对历史服务不足社区中低收入和少数族裔群体影响尤为严重的可达性差距。我们的结果表明，由wPTAL指导的战略性新站点放置显著减少了与经济和人口因素相关的交通可达性差异。通过我们的研究，我们证明TFA为城市规划者提供了实用指导，以促进公平交通并提升服务不足城市社区的生活质量。", "summary": "为解决城市中低收入和少数族裔社区公共交通可达性不公平问题，本研究提出了Transit for All (TFA) 空间计算框架，旨在指导自行车共享系统的公平扩展。TFA框架通过整合多模态地理空间数据，利用区域表示学习预测新站点的自行车需求，并结合新的加权公共交通可达性水平（wPTAL）评估综合可达性，进而提供战略性站点放置建议。以纽约市为例，研究发现TFA指导的新站点部署能有效减少交通可达性差距，提升弱势群体的交通公平性。", "keywords": "自行车共享系统, 交通可达性, 公平性, 区域表示学习, 空间计算", "comments": "该论文提出创新的TFA框架，通过结合区域表示学习和新型加权可达性指标（wPTAL），解决了自行车共享系统在服务不足社区公平扩展的难题。其重要性在于为城市规划者提供了实用的工具，以量化和解决交通不平等问题，从而提升城市弱势群体的生活质量。该方法对冷启动站点的需求预测具有独到之处。"}}
{"id": "2506.15105", "title": "Skew-Induced Insertion Loss Deviation (SILD) and FOM_SILD: Metrics for Quantifying P/N Skew Effects in High-Speed Channels", "authors": ["David Nozadze", "Zurab Kiguradze", "Amendra Koul", "Mike Sapozhnikov"], "summary": "The rise of AI workloads and growing data center demands have driven the need\nfor ultra-high-speed interconnects exceeding 200 Gb/s. As unit intervals (UI)\nshrink, even a few picoseconds of P/N skew can degrade serializer-deserializer\n(SerDes) performance. Traditional methods for quantifying skew fall short in\ncapturing its impact. We introduce two new metrics: 1) Skew-Induced Insertion\nLoss Deviation (SILD) and 2) its complementary Figure of Merit (FOM_SILD),\nanalytically developed to assess P/N skew effects. Measured S-parameters\nconfirm FOM_SILD reciprocity, while simulations of 224G PAM4 SerDes show strong\ncorrelation with bit error rate (BER) trends. This approach offers a robust\nframework for analyzing skew in next-generation ultra-high-speed interconnects.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15105v1", "AI": {"title_translation": "高速通道中量化P/N偏差效应的度量：偏斜引起的插入损耗偏差（SILD）和FOM_SILD", "tldr": "本文引入了偏斜引起的插入损耗偏差（SILD）及其品质因数（FOM_SILD）作为量化高速互连中P/N偏差效应的新指标，并证明它们与误码率（BER）趋势具有强相关性。", "motivation": "随着人工智能工作负载和数据中心需求推动对超过200 Gb/s的超高速互连的需求，单位间隔（UI）的缩小使得即使是几皮秒的P/N偏差也会降低串行器-解串器（SerDes）的性能。传统的偏差量化方法无法充分捕捉其影响。", "method": "本文引入了两个新的分析指标：偏斜引起的插入损耗偏差（SILD）及其互补的品质因数（FOM_SILD），用于评估P/N偏差效应。", "result": "测量的S参数证实了FOM_SILD的互易性。224G PAM4 SerDes的仿真显示，这些指标与误码率（BER）趋势有很强的相关性。", "conclusion": "所提出的方法为分析下一代超高速互连中的偏差提供了一个稳健的框架。", "translation": "人工智能工作负载的兴起和数据中心需求的增长推动了对超过200 Gb/s的超高速互连的需求。随着单位间隔（UI）的缩小，即使是几皮秒的P/N偏差也可能降低串行器-解串器（SerDes）的性能。传统的偏差量化方法未能充分捕捉其影响。我们引入了两个新指标：1）偏斜引起的插入损耗偏差（SILD）和2）其互补的品质因数（FOM_SILD），它们经过分析开发以评估P/N偏差效应。测量的S参数证实了FOM_SILD的互易性，而224G PAM4 SerDes的仿真显示与误码率（BER）趋势有很强的相关性。这种方法为分析下一代超高速互连中的偏差提供了一个稳健的框架。", "summary": "本文针对超高速互连（>200 Gb/s）中P/N偏差的挑战，引入了两个新颖的度量指标：偏斜引起的插入损耗偏差（SILD）及其品质因数（FOM_SILD）。这些经过分析开发的指标通过S参数测量（证实FOM_SILD互易性）和224G PAM4 SerDes仿真（显示与误码率强相关性）得到了验证。所提出的框架为量化和分析下一代高速通道中的偏差效应提供了一种稳健的方法。", "keywords": "P/N偏差, SILD, FOM_SILD, 高速互连, SerDes, BER", "comments": "本文引入了新颖的、经过分析开发的指标（SILD和FOM_SILD），与传统方法相比，它们为量化高速互连中的P/N偏差效应提供了一种更稳健的方式。通过S参数测量和SerDes仿真（特别是与BER的相关性）进行的验证突出了其实际适用性以及对设计下一代超高速系统的重要性。对P/N偏差这一关键但常被忽视的退化因素的关注，使得这项工作对于推动高速通信技术的发展尤其具有现实意义。"}}
{"id": "2506.14909", "title": "Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)", "authors": ["Fridolin Haugg", "Grace Lee", "John He", "Leonard Nürnberg", "Dennis Bontempi", "Danielle S. Bitterman", "Paul Catalano", "Vasco Prudente", "Dmitrii Glubokov", "Andrew Warrington", "Suraj Pai", "Dirk De Ruysscher", "Christian Guthier", "Benjamin H. Kann", "Vadim N. Gladyshev", "Hugo JWL Aerts", "Raymond H. Mak"], "summary": "Background: Facial appearance offers a noninvasive window into health. We\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\nand survival risk prediction (FAHR-FaceSurvival).\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\nand independence (saliency mapping) was tested extensively. Both models were\nclinically tested in two independent cancer patient datasets with survival\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\nfactors.\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\noutperformed a prior facial age estimation model in survival prognostication.\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\nhighest-risk quartile had more than triple the mortality of the lowest\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\nindependent cohort and both models showed generalizability across age, sex,\nrace and cancer subgroups. The two algorithms provided distinct, complementary\nprognostic information; saliency mapping revealed each model relied on distinct\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\nprognostic accuracy.\n  Interpretation: A single foundation model can generate inexpensive, scalable\nfacial biomarkers that capture both biological ageing and disease-related\nmortality risk. The foundation model enabled effective training using\nrelatively small clinical datasets.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14909v1", "AI": {"title_translation": "用于健康识别的面部照片基础人工智能模型 (FAHR-Face)", "tldr": "FAHR-Face是一个基于4000万张面部图像训练的基础AI模型，能通过面部照片准确估计生物年龄和预测生存风险，且在小规模临床数据集上表现良好。", "motivation": "面部外观可以作为一种无创的健康信息窗口。研究旨在构建一个基础AI模型，利用面部照片进行健康识别，具体应用于生物年龄估计和生存风险预测。", "method": "FAHR-Face模型在超过4000万张面部图像上进行训练。FAHR-FaceAge模型在749,935张公共图像上进行了两阶段、年龄平衡的微调。FAHR-FaceSurvival模型在34,389张癌症患者照片上进行了微调。模型鲁棒性（化妆、姿势、光照）和独立性（显著性映射）经过广泛测试。两个模型均在两个独立的癌症患者数据集中进行临床测试，生存分析采用多变量Cox模型并根据临床预后因素进行调整。", "result": "FAHR-FaceAge在公共数据集上的平均绝对误差最低，为5.1年，优于基准模型，并在整个人类寿命范围内保持准确性。在癌症患者中，FAHR-FaceAge在生存预后方面优于先前的面部年龄估计模型。FAHR-FaceSurvival展示了强大的死亡率预测能力，最高风险四分位数患者的死亡率是最低风险四分位数的3倍以上（校正后的风险比3.22；P<0.001）。这些发现已在独立队列中得到验证，并且两个模型在年龄、性别、种族和癌症亚组中均显示出泛化能力。两种算法提供了独特且互补的预后信息；显著性映射显示每个模型依赖于不同的面部区域。FAHR-FaceAge和FAHR-FaceSurvival的结合提高了预后准确性。", "conclusion": "一个单一的基础模型可以生成廉价、可扩展的面部生物标志物，捕捉生物衰老和疾病相关死亡风险。该基础模型使得使用相对较小的临床数据集进行有效训练成为可能。", "translation": "背景：面部外观为健康提供了一个无创的窗口。我们构建了FAHR-Face，一个在超过4000万张面部图像上训练的基础模型，并将其针对两个不同的任务进行了微调：生物年龄估计（FAHR-FaceAge）和生存风险预测（FAHR-FaceSurvival）。\n方法：FAHR-FaceAge在749,935张公共图像上进行了两阶段、年龄平衡的微调；FAHR-FaceSurvival在34,389张癌症患者照片上进行了微调。模型鲁棒性（整容手术、化妆、姿势、光照）和独立性（显著性映射）经过广泛测试。两个模型均在两个独立的癌症患者数据集中进行临床测试，生存分析采用多变量Cox模型并根据临床预后因素进行调整。\n发现：在年龄估计方面，FAHR-FaceAge在公共数据集上的平均绝对误差最低，为5.1年，优于基准模型，并在整个人类寿命范围内保持准确性。在癌症患者中，FAHR-FaceAge在生存预后方面优于先前的面部年龄估计模型。FAHR-FaceSurvival展示了强大的死亡率预测能力，最高风险四分位数患者的死亡率是最低风险四分位数的3倍以上（校正后的风险比3.22；P<0.001）。这些发现已在独立队列中得到验证，并且两个模型在年龄、性别、种族和癌症亚组中均显示出泛化能力。这两种算法提供了独特且互补的预后信息；显著性映射显示每个模型依赖于不同的面部区域。FAHR-FaceAge和FAHR-FaceSurvival的结合提高了预后准确性。\n解释：一个单一的基础模型可以生成廉价、可扩展的面部生物标志物，捕捉生物衰老和疾病相关死亡风险。该基础模型使得使用相对较小的临床数据集进行有效训练成为可能。", "summary": "该研究构建了一个名为FAHR-Face的基础人工智能模型，该模型在超过4000万张面部图像上进行训练，并针对生物年龄估计（FAHR-FaceAge）和生存风险预测（FAHR-FaceSurvival）两项任务进行微调。FAHR-FaceAge在年龄估计上表现出色，平均绝对误差为5.1年，并在癌症患者生存预后中优于现有模型。FAHR-FaceSurvival能有效预测死亡率，高风险组死亡率显著高于低风险组。两个模型在不同人群和癌症亚组中均具泛化能力，并提供互补的预后信息。研究表明，该基础模型能利用少量临床数据生成经济、可扩展的面部生物标志物，用于评估生物衰老和疾病相关死亡风险。", "keywords": "面部识别, 健康识别, 基础模型, 生物年龄, 生存风险预测", "comments": "这项研究的创新之处在于构建了一个大规模的基础AI模型（FAHR-Face），并成功将其微调应用于健康识别领域的两个重要任务：生物年龄估计和生存风险预测。其重要性在于证明了面部照片可以作为一种非侵入性且经济的生物标志物来源，为临床预后提供了新的工具。模型在小规模临床数据集上的有效训练能力，也展示了基础模型在医疗领域应用的巨大潜力。"}}
{"id": "2506.15125", "title": "Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks", "authors": ["Linlin Wang", "Wei Wang", "Dezhao Wang", "Shanwen Wang"], "summary": "With the applicability of optical fiber-based distributed acoustic sensing\n(DAS) systems, effective signal processing and analysis approaches are needed\nto promote its popularization in the field of intelligent transportation\nsystems (ITS). This paper presents a signal denoising algorithm using a hybrid\ndeep-learning network (HDLNet). Without annotated data and time-consuming\nlabeling, this self-supervised network runs in parallel, combining an\nautoencoder for denoising (DAE) and a long short-term memory (LSTM) for\nsequential processing. Additionally, a line-by-line matching algorithm for\nvehicle detection and tracking is introduced, thus realizing the complete\nprocessing of fiber signal denoising and feature extraction. Experiments were\ncarried out on a self-established real highway tunnel dataset, showing that our\nproposed hybrid network yields more satisfactory denoising performance than\nSpatial-domain DAE.", "comment": "15 pages, 10 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15125v1", "AI": {"title_translation": "基于混合深度学习网络的光纤信号去噪算法", "tldr": "本文提出了一种基于混合深度学习网络（HDLNet）的光纤信号去噪算法，该算法无需标注数据，结合了去噪自编码器（DAE）和长短期记忆网络（LSTM），并引入了车辆检测和跟踪的逐行匹配算法，在真实高速公路隧道数据集上表现出比空间域DAE更优的去噪性能。", "motivation": "为了促进光纤分布式声传感（DAS）系统在智能交通系统（ITS）领域的普及，需要有效的信号处理和分析方法。本文旨在解决光纤信号的去噪问题，以提高DAS系统的应用性。", "method": "本文提出了一种混合深度学习网络（HDLNet）用于信号去噪。该网络是自监督的，无需标注数据，并并行运行。它结合了用于去噪的自编码器（DAE）和用于序列处理的长短期记忆网络（LSTM）。此外，还引入了一种用于车辆检测和跟踪的逐行匹配算法，以实现光纤信号去噪和特征提取的完整处理。", "result": "在自建的真实高速公路隧道数据集上进行的实验表明，所提出的混合网络比空间域DAE产生了更令人满意的去噪性能。", "conclusion": "本文提出的基于混合深度学习网络（HDLNet）的光纤信号去噪算法，在无需标注数据的情况下，能够有效提升DAS系统的信号去噪性能，并且在真实数据集上表现出优于传统方法的优势，实现了完整的信号处理和特征提取流程。", "translation": "随着基于光纤的分布式声传感（DAS）系统的适用性增强，需要有效的信号处理和分析方法以促进其在智能交通系统（ITS）领域的普及。本文提出了一种使用混合深度学习网络（HDLNet）的信号去噪算法。该自监督网络无需标注数据和耗时的标记，并行运行，结合了用于去噪的自编码器（DAE）和用于序列处理的长短期记忆网络（LSTM）。此外，还引入了一种用于车辆检测和跟踪的逐行匹配算法，从而实现了光纤信号去噪和特征提取的完整处理。实验在自建的真实高速公路隧道数据集上进行，结果表明我们提出的混合网络比空间域DAE产生了更令人满意的去噪性能。", "summary": "本文针对光纤分布式声传感（DAS）系统在智能交通系统（ITS）中的应用需求，提出了一种基于混合深度学习网络（HDLNet）的光纤信号去噪算法。该算法采用自监督学习范式，无需人工标注数据，通过并行结合去噪自编码器（DAE）和长短期记忆网络（LSTM）实现高效去噪。为实现完整处理，该研究还引入了用于车辆检测和跟踪的逐行匹配算法。在真实高速公路隧道数据集上的实验结果表明，该混合网络在去噪性能上优于传统的空间域DAE。", "keywords": "光纤信号去噪, 混合深度学习, 自监督学习, 分布式声传感, 智能交通系统", "comments": "这项工作通过引入一种无需标注数据的自监督混合深度学习网络（HDLNet）来解决光纤信号去噪问题，具有创新性。其将DAE和LSTM结合的并行架构以及与车辆检测跟踪算法的集成，为智能交通系统中的DAS应用提供了一个完整的解决方案。该方法在实际数据集上的优越性能验证了其有效性和实用性。"}}
{"id": "2506.15684", "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards", "authors": ["Qingming Liu", "Zhen Liu", "Dinghuai Zhang", "Kui Jia"], "summary": "Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.", "comment": "Technical Report (21 pages, 21 figures)", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15684v1", "AI": {"title_translation": "Nabla-R2D3：基于2D奖励的有效高效3D扩散对齐", "tldr": "Nabla-R2D3是一个利用2D奖励信号对3D原生扩散模型进行对齐的强化学习框架，它能有效提高生成质量并减少遗忘。", "motivation": "生成高质量、逼真的3D资产是一个长期挑战。尽管最先进的3D生成扩散模型取得了进展，但它们在遵循指令、符合人类偏好或生成逼真纹理、几何和物理属性方面仍表现不足。", "method": "本文引入了Nabla-R2D3，一个高效且样本效率高的强化学习对齐框架，用于3D原生扩散模型，利用2D奖励。它基于Nabla-GFlowNet方法，以原则性方式将分数函数与奖励梯度匹配，从而实现奖励微调，并仅使用2D奖励信号有效适应3D扩散模型。", "result": "大量实验表明，与难以收敛或遭受奖励欺骗的普通微调基线不同，Nabla-R2D3在少量微调步骤内持续获得更高的奖励并减少先验遗忘。", "conclusion": "Nabla-R2D3是一种有效且高效的方法，能够使用2D奖励信号对3D扩散模型进行对齐，显著提升了3D资产的生成质量和逼真度。", "translation": "生成高质量和逼真的3D资产仍然是3D视觉和计算机图形学领域的一个长期挑战。尽管最先进的生成模型，如扩散模型，在3D生成方面取得了显著进展，但由于其遵循指令、符合人类偏好或生成逼真纹理、几何和物理属性的能力有限，它们往往无法达到人类设计内容的水平。在本文中，我们引入了Nabla-R2D3，一个高效且样本效率高的强化学习对齐框架，用于3D原生扩散模型，并利用2D奖励。Nabla-R2D3建立在最近提出的Nabla-GFlowNet方法之上，该方法以原则性方式将分数函数与奖励梯度匹配以进行奖励微调，从而使我们的Nabla-R2D3能够仅使用2D奖励信号有效适应3D扩散模型。大量实验表明，与难以收敛或遭受奖励欺骗的普通微调基线不同，Nabla-R2D3在少量微调步骤内持续获得更高的奖励并减少先验遗忘。", "summary": "Nabla-R2D3是一个新颖的强化学习框架，旨在解决3D扩散模型在生成高质量、逼真3D资产方面的不足。该方法利用2D奖励信号，基于Nabla-GFlowNet对3D原生扩散模型进行对齐和微调。实验证明，Nabla-R2D3相比传统微调基线，能更有效地实现收敛，获得更高奖励，并显著减少先验遗忘。", "keywords": "3D扩散, 强化学习, 2D奖励, 模型对齐, Nabla-R2D3", "comments": "Nabla-R2D3的创新之处在于其将2D奖励信号应用于3D扩散模型的强化学习对齐，提高了生成质量和效率。其基于Nabla-GFlowNet的方法，解决了传统微调中常见的收敛困难和奖励欺骗问题，展示了在3D内容生成领域的重要潜力。"}}
{"id": "2506.15576", "title": "DiscRec: Disentangled Semantic-Collaborative Modeling for Generative Recommendation", "authors": ["Chang Liu", "Yimeng Bai", "Xiaoyan Zhao", "Yang Zhang", "Fuli Feng", "Wenge Rong"], "summary": "Generative recommendation is emerging as a powerful paradigm that directly\ngenerates item predictions, moving beyond traditional matching-based\napproaches. However, current methods face two key challenges: token-item\nmisalignment, where uniform token-level modeling ignores item-level granularity\nthat is critical for collaborative signal learning, and semantic-collaborative\nsignal entanglement, where collaborative and semantic signals exhibit distinct\ndistributions yet are fused in a unified embedding space, leading to\nconflicting optimization objectives that limit the recommendation performance.\n  To address these issues, we propose DiscRec, a novel framework that enables\nDisentangled Semantic-Collaborative signal modeling with flexible fusion for\ngenerative Recommendation.First, DiscRec introduces item-level position\nembeddings, assigned based on indices within each semantic ID, enabling\nexplicit modeling of item structure in input token sequences.Second, DiscRec\nemploys a dual-branch module to disentangle the two signals at the embedding\nlayer: a semantic branch encodes semantic signals using original token\nembeddings, while a collaborative branch applies localized attention restricted\nto tokens within the same item to effectively capture collaborative signals. A\ngating mechanism subsequently fuses both branches while preserving the model's\nability to model sequential dependencies. Extensive experiments on four\nreal-world datasets demonstrate that DiscRec effectively decouples these\nsignals and consistently outperforms state-of-the-art baselines. Our codes are\navailable on https://github.com/Ten-Mao/DiscRec.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15576v1", "AI": {"title_translation": "DiscRec：生成式推荐中的解耦语义-协同建模", "tldr": "本文提出了DiscRec框架，通过引入项目级位置嵌入和双分支模块（语义分支和协同分支），解决生成式推荐中存在的令牌-项目错位和语义-协同信号纠缠问题，并在四个真实世界数据集上实现了SOTA性能。", "motivation": "当前生成式推荐方法面临两大挑战：一是令牌-项目错位，即统一的令牌级建模忽略了对协同信号学习至关重要的项目级粒度；二是语义-协同信号纠缠，即协同和语义信号分布不同却在统一嵌入空间中融合，导致优化目标冲突，限制了推荐性能。", "method": "本文提出了DiscRec框架，实现了生成式推荐中解耦的语义-协同信号建模与灵活融合。首先，DiscRec引入了项目级位置嵌入，根据每个语义ID内的索引分配，从而在输入令牌序列中显式建模项目结构。其次，DiscRec采用双分支模块在嵌入层解耦两种信号：语义分支使用原始令牌嵌入编码语义信号，而协同分支应用局部注意力，仅限于同一项目内的令牌，以有效捕获协同信号。随后，一个门控机制融合两个分支，同时保留模型建模序列依赖的能力。", "result": "在四个真实世界数据集上的大量实验表明，DiscRec能有效解耦这些信号，并始终优于最先进的基线方法。", "conclusion": "DiscRec通过解耦语义和协同信号，有效解决了现有生成式推荐方法中的关键挑战，并在实验中展现出卓越的性能，证明了其在生成式推荐领域的优越性。", "translation": "生成式推荐正成为一种强大的范式，它直接生成项目预测，超越了传统的基于匹配的方法。然而，当前方法面临两个关键挑战：令牌-项目错位，即统一的令牌级建模忽略了对协同信号学习至关重要的项目级粒度；以及语义-协同信号纠缠，即协同和语义信号表现出不同的分布，却在统一的嵌入空间中融合，导致优化目标冲突，从而限制了推荐性能。\n为了解决这些问题，我们提出了DiscRec，一个新颖的框架，它能够在生成式推荐中实现解耦的语义-协同信号建模与灵活融合。首先，DiscRec引入了项目级位置嵌入，根据每个语义ID内的索引进行分配，从而在输入令牌序列中显式建模项目结构。其次，DiscRec采用双分支模块在嵌入层解耦这两种信号：一个语义分支使用原始令牌嵌入编码语义信号，而一个协同分支应用局部注意力，仅限于同一项目内的令牌，以有效捕获协同信号。随后，一个门控机制融合两个分支，同时保留模型建模序列依赖的能力。在四个真实世界数据集上的大量实验表明，DiscRec能有效解耦这些信号，并始终优于最先进的基线方法。我们的代码可在https://github.com/Ten-Mao/DiscRec上获取。", "summary": "本文提出了一种名为DiscRec的新型生成式推荐框架，旨在解决现有方法中的令牌-项目错位和语义-协同信号纠缠问题。DiscRec通过引入项目级位置嵌入来显式建模项目结构，并设计了一个双分支模块（包含语义分支和协同分支）在嵌入层解耦两种信号。此外，一个门控机制用于灵活融合这两个分支。实验结果表明，DiscRec能有效解耦信号，并在多个真实世界数据集上显著优于现有最先进的基线模型。", "keywords": "生成式推荐, 解耦建模, 语义信号, 协同信号, 双分支", "comments": "DiscRec的创新之处在于其双分支解耦设计，能够有效分离语义和协同信号，这解决了现有生成式推荐模型中信号纠缠导致性能受限的关键问题。通过引入项目级位置嵌入，模型能够更好地理解项目内部结构，进一步提升了建模的精细度。这种解耦与灵活融合的策略为生成式推荐领域提供了一个新的视角和有效的解决方案，具有重要的研究价值和潜在的应用前景。"}}
{"id": "2506.14877", "title": "Beyond Universality: Cultural Diversity in Music and Its Implications for Sound Design and Sonification", "authors": ["Rubén García-Benito"], "summary": "The Audio Mostly (AM) conference has long been a platform for exploring the\nintersection of sound, technology, and culture. Despite growing interest in\nsonic cultures, discussions on the role of cultural diversity in sound design\nand sonification remain limited. This paper investigates the implicit biases\nand gaps within the discourse on music and sound aesthetics, challenging the\nnotion of music as a 'universal language'. Through a historical and\ncross-cultural analysis of musicology and ethnomusicology, the profound\ninfluence of cultural context on auditory perception and aesthetic appraisal is\nhighlighted. By drawing parallels between historical music practices and\ncontemporary sound design, the paper advocates for a more inclusive approach\nthat recognizes the diversity of sonic traditions. Using music as a case study,\nwe underscore broader implications for sound design and sonification,\nemphasizing the need to integrate cultural perspectives into auditory design\npractices. A reevaluation of existing frameworks in sound design and\nsonification is proposed, emphasizing the necessity of culturally informed\npractices that resonate with global audiences. Ultimately, embracing cultural\ndiversity in sound design is suggested to lead to richer, more meaningful\nauditory experiences and to foster greater inclusivity within the field.", "comment": "12 pages, 1 figure. Long paper accepted for publication at the Audio\n  Mostly & ICAD Joint Conference (AM.ICAD 2025). To appear in the ACM\n  International Conference Proceedings Series (ICPS)", "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.14877v1", "AI": {"title_translation": "超越普适性：音乐中的文化多样性及其对声音设计和声化技术的影响", "tldr": "本文探讨了音乐和声音美学中文化多样性的重要性，挑战了音乐是“通用语言”的观念，并主张在声音设计和声化中采用更具包容性的文化视角。", "motivation": "尽管对声音文化的兴趣日益增长，但在声音设计和声化中文化多样性的作用讨论仍然有限。本文旨在调查音乐和声音美学论述中隐含的偏见和空白，并挑战音乐是“通用语言”的观念，强调将文化视角融入听觉设计实践的必要性。", "method": "通过对音乐学和民族音乐学的历史和跨文化分析，本文突出了文化背景对听觉感知和审美评价的深刻影响。通过将历史音乐实践与当代声音设计进行类比，倡导一种更具包容性的方法。", "result": "研究结果表明，文化背景对听觉感知和审美评价有着深刻影响。论文呼吁采用一种更具包容性的方法，以识别声音传统的多样性，并强调将文化视角整合到听觉设计实践中的必要性。", "conclusion": "拥抱声音设计中的文化多样性将带来更丰富、更有意义的听觉体验，并促进该领域更大的包容性。因此，需要重新评估现有的声音设计和声化框架，强调文化知情的实践。", "translation": "Audio Mostly (AM) 会议长期以来一直是探索声音、技术和文化交汇点的平台。尽管对声音文化的兴趣日益增长，但在声音设计和声化中文化多样性的作用讨论仍然有限。本文调查了音乐和声音美学论述中隐含的偏见和空白，挑战了音乐是“通用语言”的观念。通过对音乐学和民族音乐学的历史和跨文化分析，突出了文化背景对听觉感知和审美评价的深刻影响。通过将历史音乐实践与当代声音设计进行类比，本文倡导一种更具包容性的方法，以识别声音传统的多样性。以音乐为案例研究，我们强调了对声音设计和声化的更广泛影响，强调了将文化视角整合到听觉设计实践中的必要性。本文提出了对现有声音设计和声化框架的重新评估，强调了与全球受众产生共鸣的文化知情实践的必要性。最终，本文认为在声音设计中拥抱文化多样性将带来更丰富、更有意义的听觉体验，并促进该领域更大的包容性。", "summary": "本文探讨了音乐中文化多样性的重要性，挑战了音乐是“通用语言”的传统观念。通过对音乐学和民族音乐学的历史及跨文化分析，论文揭示了文化背景对听觉感知和审美评价的深远影响。文章主张在声音设计和声化中采取更具包容性的方法，强调整合文化视角，以创造更丰富、更具意义的听觉体验，并促进领域的包容性。", "keywords": "文化多样性, 声音设计, 声化, 音乐学, 民族音乐学", "comments": "本文的创新之处在于它明确地挑战了音乐作为“通用语言”的普遍观念，并将这一批判性视角延伸到声音设计和声化领域。它强调了文化背景在听觉感知中的关键作用，并呼吁在声音设计实践中进行范式转变，以促进更大的全球包容性和更丰富的用户体验。其重要性在于为未来声音设计和人机交互中的跨文化研究奠定了基础。"}}
{"id": "2506.14951", "title": "Flat Channels to Infinity in Neural Loss Landscapes", "authors": ["Flavio Martinelli", "Alexander Van Meegen", "Berfin Şimşek", "Wulfram Gerstner", "Johanni Brea"], "summary": "The loss landscapes of neural networks contain minima and saddle points that\nmay be connected in flat regions or appear in isolation. We identify and\ncharacterize a special structure in the loss landscape: channels along which\nthe loss decreases extremely slowly, while the output weights of at least two\nneurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight\nvectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At\nconvergence, the two neurons implement a gated linear unit:\n$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot\n\\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot\n\\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these\nchannels to infinity are asymptotically parallel to symmetry-induced lines of\ncritical points. Gradient flow solvers, and related optimization methods like\nSGD or ADAM, reach the channels with high probability in diverse regression\nsettings, but without careful inspection they look like flat local minima with\nfinite parameter values. Our characterization provides a comprehensive picture\nof these quasi-flat regions in terms of gradient dynamics, geometry, and\nfunctional interpretation. The emergence of gated linear units at the end of\nthe channels highlights a surprising aspect of the computational capabilities\nof fully connected layers.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14951v1", "AI": {"title_translation": "神经网络损失景观中的平坦无限通道", "tldr": "本文识别并表征了神经网络损失景观中一种特殊的“平坦无限通道”，优化器可能收敛于此，这些通道导致输出权重发散，输入权重相等，最终形成门控线性单元。", "motivation": "研究神经网络损失景观中可能连接或孤立的最小值和鞍点，并识别和表征其中一种特殊的结构。", "method": "通过识别损失函数下降极慢，同时至少两个神经元的输出权重发散到正负无穷大，且其输入权重向量变得彼此相等的通道来表征这种结构。几何上，这些通道渐近平行于对称诱导的临界点线。并通过梯度流求解器及相关优化方法（如SGD或ADAM）来观察它们在不同回归设置中的表现。", "result": "识别了损失景观中一种特殊的“平坦无限通道”：损失沿这些通道下降极慢，同时至少两个神经元a_i和a_j的输出权重发散到±无穷大，它们的输入权重向量w_i和w_j变得彼此相等。在收敛时，这两个神经元实现了一个门控线性单元。梯度流求解器和相关优化方法（如SGD或ADAM）在不同的回归设置中以高概率达到这些通道，但它们看起来像具有有限参数值的平坦局部最小值。", "conclusion": "本文对神经网络损失景观中的准平坦区域提供了关于梯度动力学、几何学和功能解释的全面描述。门控线性单元在通道末端的出现揭示了全连接层计算能力的惊人一面。", "translation": "神经网络的损失景观包含可能在平坦区域中连接或孤立出现的最小值和鞍点。我们识别并表征了损失景观中的一种特殊结构：损失沿这些通道下降极慢，同时至少两个神经元a_i和a_j的输出权重发散到±无穷大，它们的输入权重向量w_i和w_j变得彼此相等。在收敛时，这两个神经元实现了一个门控线性单元：a_iσ(w_i·x) + a_jσ(w_j·x) → σ(w·x) + (v·x)σ'(w·x)。从几何上看，这些无限通道渐近平行于对称诱导的临界点线。梯度流求解器和相关的优化方法，如SGD或ADAM，在不同的回归设置中以高概率达到这些通道，但如果不仔细检查，它们看起来就像具有有限参数值的平坦局部最小值。我们的表征从梯度动力学、几何学和功能解释方面提供了这些准平坦区域的全面图景。门控线性单元在通道末端的出现突出了全连接层计算能力的一个令人惊讶的方面。", "summary": "本文研究了神经网络损失景观中的一种特殊结构——“平坦无限通道”。这些通道的特征是损失下降缓慢，同时特定神经元的输出权重趋于无穷，输入权重趋于相等，最终形成门控线性单元。研究发现，常见的优化器如SGD和ADAM在训练过程中有高概率进入这些通道，且这些通道在表面上类似于平坦的局部最小值。文章从梯度动力学、几何和功能角度全面描述了这些准平坦区域，并揭示了全连接层在这些通道末端形成门控线性单元的计算能力。", "keywords": "损失景观, 平坦通道, 神经网络, 优化, 门控线性单元", "comments": "本文创新性地识别并详细刻画了神经网络损失景观中一种此前未被充分理解的特殊结构——“平坦无限通道”。这一发现对于理解深度学习优化过程中为何会遇到“平坦”区域以及这些区域的实际性质具有重要意义。它揭示了即使参数发散，模型也可能收敛到有意义的功能单位（门控线性单元），这挑战了传统上对参数有限且稳定的局部最小值的理解。这一研究对于改进优化算法和解释神经网络的泛化能力提供了新的视角。"}}
{"id": "2506.15154", "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15154v1", "AI": {"title_translation": "SonicVerse：基于多任务学习的音乐特征增强字幕生成", "tldr": "SonicVerse是一个多任务音乐字幕生成模型，它通过整合音乐特征检测（如音调、人声）来增强字幕的质量和细节，并能为长音乐片段生成时间感知的描述。", "motivation": "详细、准确反映音乐特性的字幕能够丰富音乐数据库，并推动音乐AI领域的研究进展。", "method": "本文提出了一个多任务音乐字幕生成模型SonicVerse，它将字幕生成与辅助音乐特征检测任务（如音调检测、人声检测等）相结合，以直接捕捉低级声学细节和高级音乐属性。其核心贡献是一个基于投影的架构，将音频输入转换为语言标记，同时通过专用的辅助头部检测音乐特征。这些辅助头部的输出也被投影到语言标记中，以增强字幕生成的输入。该框架不仅能为短音乐片段生成丰富、描述性的字幕，还能通过使用大型语言模型链接输出，直接为长音乐作品生成详细的时间感知描述。为了训练模型，作者使用模块化音乐特征提取器MIMFLEX扩展了MusicBench数据集，添加了音乐特征标注，形成了配对的音频、字幕和音乐特征数据。", "result": "实验结果表明，以这种方式整合音乐特征显著提高了生成字幕的质量和细节。", "conclusion": "通过将音乐特征检测整合到多任务学习框架中，SonicVerse能够生成更高质量、更详细的音乐字幕，并为长音乐作品提供时间感知的描述，从而有效提升了音乐AI领域的能力。", "translation": "详细准确地反映音乐作品特征的字幕可以丰富音乐数据库，并推动音乐AI的研究。本文介绍了一个多任务音乐字幕生成模型SonicVerse，它将字幕生成与辅助音乐特征检测任务（如音调检测、人声检测等）相结合，以直接捕捉低级声学细节和高级音乐属性。其关键贡献是一个基于投影的架构，将音频输入转换为语言标记，同时通过专用的辅助头部检测音乐特征。这些头部的输出也被投影到语言标记中，以增强字幕输入的质量。该框架不仅能为短音乐片段生成丰富、描述性的字幕，还能通过使用大型语言模型链接输出，直接为更长的音乐作品生成详细的时间感知描述。为了训练模型，我们使用模块化音乐特征提取器MIMFLEX对MusicBench数据集进行了扩展标注，得到了配对的音频、字幕和音乐特征数据。实验结果表明，以这种方式整合特征提高了生成字幕的质量和细节。", "summary": "SonicVerse是一个创新的多任务音乐字幕生成模型，它通过将音乐特征检测（如音调、人声）与字幕生成相结合，克服了传统方法仅依赖低级声学特征的局限。该模型采用独特的投影架构，将音频输入和检测到的音乐特征都转换为语言标记，从而生成更丰富、更详细的音乐描述。此外，它还能结合大型语言模型为长音乐作品提供时间感知的描述。通过扩展MusicBench数据集并进行实验，研究证明了整合音乐特征能显著提升生成字幕的质量和细节。", "keywords": "多任务学习, 音乐字幕, 音乐特征, SonicVerse, 音频处理", "comments": "SonicVerse的创新之处在于其多任务学习方法和投影架构，它有效地将低级声学信息与高级音乐特征结合起来，极大地丰富了音乐字幕的生成。这种方法不仅提升了字幕的描述性和准确性，还为处理长音乐作品提供了可行方案。通过扩展数据集以包含更多音乐特征，该研究也为未来的音乐信息检索和AI应用奠定了基础，具有重要的实践意义。"}}
{"id": "2506.14927", "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "authors": ["Joseph J. Peper", "Wenzhao Qiu", "Ali Payani", "Lu Wang"], "summary": "Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14927v1", "AI": {"title_translation": "MDBench：一个知识引导生成的多文档推理基准", "tldr": "MDBench是一个新的多文档推理基准，通过知识引导的合成生成过程创建，旨在评估大型语言模型在处理长上下文输入时的推理能力，并发现现有LLM在此基准上仍面临显著挑战。", "motivation": "随着大型语言模型（LLMs）推理能力的快速扩展，对新的评估基准的需求日益增长。特别是，尽管多文档（MD）推理对于LLM处理长上下文输入至关重要，但现有基准稀少，且由于长输入标注成本高昂，多文档基准的创建历来具有挑战性。", "method": "本文介绍了MDBench，一个用于评估LLM多文档推理能力的新数据集。MDBench通过一种新颖的合成生成过程创建，该过程允许可控且高效地生成具有挑战性的文档集和相应的问答（QA）示例。该技术基于浓缩的结构化种子知识，通过LLM辅助编辑进行修改以引入MD特有的推理挑战，然后将结构化知识转换为自然文本形式，生成文档集和QA示例。", "result": "对流行LLM和提示技术进行分析后发现，即使对于相对较短的文档集，MDBench对所有方法都提出了显著挑战。此外，知识引导的生成技术（1）能够对MD特有的推理能力进行有针对性的分析，并且（2）可以快速适应新的挑战和未来的模型改进。", "conclusion": "MDBench提供了一个新颖的、高效的多文档推理基准，揭示了当前大型语言模型在该任务上的不足，并展示了其知识引导生成方法的灵活性和潜力，为未来LLM的评估和发展提供了工具。", "translation": "自然语言处理评估取得了显著进展，这在很大程度上得益于强大大型语言模型（LLM）的普及。随着LLM推理能力的快速扩展，新的评估基准变得越来越重要。特别是，尽管多文档（MD）推理是极度相关的领域，鉴于LLM处理长上下文输入的能力，但很少有基准能够严格检验模型在这种设置下的行为。此外，由于标注长输入的成本高昂，多文档设置历来对基准创建构成挑战。在这项工作中，我们引入了MDBench，一个用于评估LLM在多文档推理任务上的新数据集。值得注意的是，MDBench是通过一种新颖的合成生成过程创建的，这使我们能够可控且高效地生成具有挑战性的文档集和相应的问答（QA）示例。我们新颖的技术基于浓缩的结构化种子知识，通过LLM辅助编辑对其进行修改，以引入MD特有的推理挑战。然后，我们将这种结构化知识转换为自然文本表面形式，生成一个文档集和相应的QA示例。我们分析了流行的LLM和提示技术的行为，发现MDBench对所有方法都提出了显著挑战，即使是相对较短的文档集。我们还看到，我们的知识引导生成技术（1）使我们能够轻松地对MD特有的推理能力进行有针对性的分析，并且（2）可以快速适应新的挑战和未来的模型改进。", "summary": "MDBench是一个新颖的多文档推理基准，旨在解决现有基准稀缺和创建成本高昂的问题。它采用知识引导的合成生成方法，利用结构化种子知识和LLM辅助编辑来创建复杂的文档集和问答对，从而高效地生成具有挑战性的多文档推理任务。对现有大型语言模型在MDBench上的评估显示，即使是短文档集，模型仍面临显著挑战。该研究强调了其生成技术在支持针对性分析和未来扩展方面的潜力。", "keywords": "多文档推理, 基准测试, 大型语言模型, 知识引导生成, 自然语言处理", "comments": "MDBench的创新之处在于其知识引导的合成生成方法，这克服了传统多文档基准创建中高昂的人工标注成本问题。通过自动化和可控的生成过程，它能够高效地创建大量具有挑战性的多文档推理任务，这对于评估和推动LLM在处理长上下文和复杂推理方面的能力至关重要。其灵活性也使得该基准可以快速适应未来的模型进步和新的挑战。"}}
{"id": "2506.14785", "title": "Moment-enhanced shallow water equations for non-slip boundary conditions", "authors": ["Shiping Zhou", "Juntao Huang", "Andrew J. Christlieb"], "summary": "The shallow water equations often assume a constant velocity profile along\nthe vertical axis. However, this assumption does not hold in many practical\napplications. To better approximate the vertical velocity distribution, models\nsuch as the shallow water moment expansion models have been proposed.\nNevertheless, under non-slip bottom boundary conditions, both the standard\nshallow water equation and its moment-enhanced models struggle to accurately\ncapture the vertical velocity profile due to the stiff source terms. In this\nwork, we propose modified shallow water equations and corresponding\nmoment-enhanced models that perform well under both non-slip and slip boundary\nconditions. The primary difference between the modified and original models\nlies in the treatment of the source term, which allows our modified moment\nexpansion models to be readily generalized, while maintaining compatibility\nwith our previous analysis on the hyperbolicity of the model. To assess the\nperformance of both the standard and modified moment expansion models, we\nconduct a comprehensive numerical comparison with the incompressible\nNavier--Stokes equations -- a comparison that is absent from existing\nliterature.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14785v1", "AI": {"title_translation": "非滑移边界条件下动量增强型浅水方程", "tldr": "本文提出了改进的浅水方程和动量增强模型，以更好地处理非滑移边界条件，并通过与Navier-Stokes方程的比较进行了验证。", "motivation": "浅水方程通常假设垂直方向上的速度剖面恒定，但在许多实际应用中此假设不成立。现有的浅水方程和动量增强模型在非滑移底部边界条件下，由于僵硬的源项，难以准确捕捉垂直速度剖面。", "method": "提出改进的浅水方程和相应的动量增强模型。主要区别在于源项的处理方式。通过与不可压缩Navier–Stokes方程进行全面的数值比较来评估模型的性能。", "result": "所提出的改进模型在非滑移和滑移边界条件下均表现良好。改进的源项处理方式使得模型易于推广，同时保持了与之前双曲性分析的兼容性。", "conclusion": "改进的浅水方程和动量增强模型有效地解决了非滑移边界条件下的挑战，与Navier-Stokes方程相比表现出良好的性能。", "translation": "浅水方程通常假设沿垂直轴的速度剖面恒定。然而，这个假设在许多实际应用中并不成立。为了更好地近似垂直速度分布，已经提出了诸如浅水动量展开模型之类的模型。然而，在非滑移底部边界条件下，由于僵硬的源项，无论是标准浅水方程还是其动量增强模型都难以准确捕捉垂直速度剖面。在这项工作中，我们提出了改进的浅水方程和相应的动量增强模型，这些模型在非滑移和滑移边界条件下都表现良好。改进模型与原始模型之间的主要区别在于对源项的处理，这使得我们改进的动量展开模型易于推广，同时保持了与我们之前对模型双曲性分析的兼容性。为了评估标准和改进动量展开模型的性能，我们与不可压缩Navier-Stokes方程进行了全面的数值比较——这种比较在现有文献中是缺失的。", "summary": "本文针对标准浅水方程及其动量增强模型在非滑移边界条件下难以准确捕捉垂直速度剖面的问题，提出了改进的浅水方程和动量增强模型。通过改变源项的处理方式，新模型在非滑移和滑移边界条件下均表现出色，并易于推广且保持了与模型双曲性分析的兼容性。研究通过与不可压缩Navier-Stokes方程的全面数值比较验证了所提模型的有效性，填补了现有文献中的空白。", "keywords": "浅水方程, 动量增强模型, 非滑移边界条件, 源项, Navier-Stokes方程", "comments": "本文的创新点在于对浅水方程源项处理的改进，这使得模型在非滑移边界条件下能够更准确地捕捉垂直速度剖面，同时保持了理论上的双曲性兼容。此外，与不可压缩Navier-Stokes方程进行全面的数值比较，填补了现有文献的空白，大大增强了研究的可靠性和实用性。"}}
{"id": "2506.15070", "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine", "authors": ["Rasha Karakchi", "Rye Stahle-Smith", "Nishant Chinnasami", "Tiffany Yu"], "summary": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators.", "comment": "This is submitted to the ACM/IEEE Symposium on Edge Computing (SEC\n  2025)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15070v1", "AI": {"title_translation": "迈向轻量级、可扩展、并行安全加密引擎", "tldr": "SPiME是一种轻量级、可扩展的FPGA兼容内存内安全处理器加密架构，将AES-128集成到PiM中，实现高效边缘加密，解决了传统CPU加密的性能瓶颈。", "motivation": "物联网（IoT）应用的指数级增长，对边缘高效、高吞吐量和节能数据处理的需求日益增长。传统的以CPU为中心的加密方法在延迟敏感和资源受限的环境中存在性能瓶颈和过多的数据移动问题。", "method": "本文提出了SPiME，一种轻量级、可扩展、FPGA兼容的内存内安全处理器加密架构。SPiME将高级加密标准（AES-128）直接集成到内存内处理（PiM）框架中。该架构被设计为并行PiM单元的模块化阵列，每个单元包含一个AES核心和最小控制单元，以实现分布式就地加密，同时保持最小开销。该架构使用Verilog语言实现，并在多个AMD UltraScale和UltraScale+ FPGA上进行了测试。", "result": "评估结果显示，SPiME可以扩展到4,000多个并行单元，同时在高端设备上仅占用不到5%的关键FPGA资源。它能提供超过25 Gbps的持续加密吞吐量，并具有可预测的低延迟性能。", "conclusion": "SPiME设计所具备的可移植性、可配置性和资源效率，使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的有吸引力的解决方案。", "translation": "物联网（IoT）应用的指数级增长加剧了对边缘高效、高吞吐量和节能数据处理的需求。传统的以CPU为中心的加密方法存在性能瓶颈和过多的数据移动问题，尤其是在延迟敏感和资源受限的环境中。在本文中，我们提出了SPiME，一种轻量级、可扩展且与FPGA兼容的内存内安全处理器加密架构，它将高级加密标准（AES-128）直接集成到内存内处理（PiM）框架中。SPiME被设计为并行PiM单元的模块化阵列，每个单元将一个AES核心与一个最小控制单元相结合，以实现开销最小的分布式就地加密。该架构完全采用Verilog实现，并在多个AMD UltraScale和UltraScale+ FPGA上进行了测试。评估结果表明，SPiME可以扩展到4,000多个并行单元，同时在高端设备上保持不到5%的关键FPGA资源利用率。它提供超过25 Gbps的持续加密吞吐量，并具有可预测的低延迟性能。该设计的可移植性、可配置性和资源效率使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的引人注目的解决方案。", "summary": "本文介绍了一种名为SPiME的轻量级、可扩展、FPGA兼容的内存内安全处理器加密架构。该架构旨在解决物联网边缘计算中传统CPU加密方法面临的性能瓶颈和数据移动问题。SPiME通过将AES-128直接集成到内存内处理（PiM）框架中，并设计为并行PiM单元的模块化阵列，实现了高效、低开销的分布式就地加密。实验结果表明，SPiME在FPGA上表现出卓越的扩展性、高吞吐量（超过25 Gbps）和低延迟性能，使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的理想解决方案。", "keywords": "内存内处理, FPGA, AES-128, 边缘计算, 加密引擎", "comments": "这项工作通过将AES加密直接集成到内存内处理单元中，有效解决了边缘计算中数据加密的性能和资源限制问题，具有显著的创新性。其模块化和可扩展设计使其在资源受限设备上具有高度实用性，为未来安全边缘AI和IoT应用提供了强大的硬件加速基础。"}}
{"id": "2506.15135", "title": "Towards Bug-Free Distributed Go Programs", "authors": ["Zhengqun Koo"], "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.", "comment": "Version 1. B.Comp. Dissertation", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15135v1", "AI": {"title_translation": "迈向无Bug的分布式Go程序", "tldr": "本文描述了一个验证框架，该框架可以静态地证明使用Go语言子集的分布式程序中不存在通信竞态，通过扩展happens-before顺序来处理缓冲和非缓冲通道。", "motivation": "分布式系统中的并发推理非常困难，意外的竞态会导致Bug。虽然共享内存系统中的数据竞态检测已得到充分研究，但通信竞态（在共享通道上收发操作不满足happens-before关系）同样会引发严重问题，例如接收到错误消息或根本没有消息。因此，需要一种方法来证明分布式程序中通信竞态的缺失。", "method": "本文描述了一个验证框架，该框架能够证明使用Go编程语言子集的分布式程序中通信竞态的缺失。该框架通过静态推理分布式程序的执行方式，利用扩展到缓冲和非缓冲通道的happens-before顺序来实现同步主要通过消息传递的程序验证。", "result": "该验证框架能够证明分布式Go程序中不存在通信竞态。", "conclusion": "本文提出了一个验证框架，该框架能够静态地证明使用Go语言子集的分布式程序中通信竞态的缺失，从而有助于实现无Bug的分布式Go程序。", "translation": "分布式系统程序员需要对并发进行推理以避免竞态。然而，对并发进行推理是困难的，并且意外的竞态会表现为Bug。共享内存系统中的数据竞态检测已得到充分研究（动态数据竞态检测 [13]、行为类型 [15]、动态竞态检测 [31]）。类似于数据竞态由共享内存位置上不满足happens-before关系的读写组成，通信竞态由共享通道上不满足happens-before关系的接收和发送组成。通信竞态是有问题的：接收方期望从特定发送方接收特定消息，但如果存在通信竞态，接收方可能会收到为其他接收方准备的消息，或者根本收不到任何消息。在这项工作中，我们描述了一个验证框架，该框架可以证明使用Go编程语言子集的分布式程序中不存在通信竞态，其中同步主要通过消息传递实现。我们利用扩展到缓冲和非缓冲通道的happens-before顺序，静态地推理分布式程序的执行方式。", "summary": "本文提出了一个验证框架，旨在解决分布式Go程序中通信竞态的问题。不同于数据竞态，通信竞态发生在共享通道上，可能导致消息错位或丢失。该框架通过静态分析Go程序中消息传递的同步机制，并利用扩展的happens-before顺序（涵盖缓冲和非缓冲通道），来证明分布式程序中通信竞态的缺失，从而提高程序的可靠性。", "keywords": "通信竞态, Go, 分布式系统, 静态验证, happens-before", "comments": "这项工作的创新点在于将happens-before顺序的概念扩展并应用于Go语言的分布式消息传递场景，特别是考虑了缓冲和非缓冲通道。它提供了一种静态验证的方法来证明通信竞态的缺失，这对于开发高可靠性的分布式系统至关重要。其重要性在于，它直接解决了Go等通过消息传递实现并发的语言中一个常见的且难以调试的问题，有助于提升分布式程序的质量和可信度。"}}
{"id": "2506.14820", "title": "Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction", "authors": ["Hyeon Jeon", "Hyunwook Lee", "Yun-Hsin Kuo", "Taehyun Yang", "Daniel Archambault", "Sungahn Ko", "Takanori Fujiwara", "Kwan-Liu Ma", "Jinwook Seo"], "summary": "Visual analytics using dimensionality reduction (DR) can easily be unreliable\nfor various reasons, e.g., inherent distortions in representing the original\ndata. The literature has thus proposed a wide range of methodologies to make\nDR-based visual analytics reliable. However, the diversity and extensiveness of\nthe literature can leave novice analysts and researchers uncertain about where\nto begin and proceed. To address this problem, we propose a guide for reading\npapers for reliable visual analytics with DR. Relying on the previous\nclassification of the relevant literature, our guide helps both practitioners\nto (1) assess their current DR expertise and (2) identify papers that will\nfurther enhance their understanding. Interview studies with three experts in DR\nand data visualizations validate the significance, comprehensiveness, and\nusefulness of our guide.", "comment": "EG/VGTC EuroVis 2025 Short paper", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14820v1", "AI": {"title_translation": "驾驭高维幕后：一份探索文献以可靠使用降维的指南", "tldr": "本文提出了一份指南，帮助新手分析师和研究人员有效探索降维（DR）相关文献，以实现可靠的可视分析，并通过专家访谈验证了其有效性。", "motivation": "由于降维（DR）的可视分析可能因数据固有的扭曲等原因而不可靠，文献中提出了多种方法来提高其可靠性。然而，文献的广泛性和多样性使新手分析师和研究人员难以入门和深入。为了解决这个问题，本文旨在提供一个阅读指南。", "method": "本文提出了一份阅读指南，用于学习可靠的基于降维的可视分析论文。该指南基于先前对相关文献的分类，旨在帮助实践者评估其当前的降维专业知识，并识别能进一步提升其理解的论文。通过对三位降维和数据可视化专家的访谈研究，验证了该指南的重要性、全面性和实用性。", "result": "该指南能够帮助实践者（1）评估他们当前的降维专业知识，以及（2）识别将进一步增强他们理解的论文。对三位降维和数据可视化专家的访谈研究验证了该指南的重要性、全面性和实用性。", "conclusion": "本文提出的用于可靠使用降维的可视分析文献阅读指南，对帮助新手分析师和研究人员导航复杂多样的文献具有显著、全面且实用的价值。", "translation": "使用降维（DR）进行可视化分析，由于多种原因（例如，表示原始数据时固有的失真），很容易变得不可靠。因此，文献中提出了各种方法来使基于DR的可视分析变得可靠。然而，文献的多样性和广泛性可能使新手分析师和研究人员不确定从何开始和如何进行。为了解决这个问题，我们提出了一份阅读论文的指南，用于实现可靠的基于DR的可视分析。我们的指南依赖于先前对相关文献的分类，帮助实践者（1）评估他们当前的DR专业知识，并（2）识别将进一步增强他们理解的论文。对三位DR和数据可视化专家的访谈研究验证了我们指南的重要性、全面性和实用性。", "summary": "本文针对降维（DR）可视化分析中常见的不可靠性以及相关文献的庞杂性，提出了一份专门的阅读指南。该指南基于现有文献分类，旨在帮助新手分析师和研究人员评估自身DR专业水平，并高效筛选出有助于提升理解的文献。通过专家访谈，该指南的重要性、全面性和实用性得到了验证，为可靠使用DR提供了有效的学习路径。", "keywords": "降维, 可视分析, 文献指南, 可靠性, 专家访谈", "comments": "这篇论文的创新点在于它提供了一个结构化的指南来帮助新手理解和导航复杂的降维文献，这对于提高降维技术在可视化分析中的可靠应用具有重要意义。它不仅识别了问题（文献庞杂导致新手难以入手），还提供了一个实用的解决方案（一个基于现有分类的阅读指南），并通过专家访谈进行了验证，显示了其潜在的实用价值。"}}
{"id": "2506.15464", "title": "Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic Graphs", "authors": ["Le Vu Anh", "Mehmet Dik", "Nguyen Viet Anh"], "summary": "Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.", "comment": "5 pages, 5 figures", "cate": "math.MG", "url": "http://arxiv.org/abs/2506.15464v1", "AI": {"title_translation": "delta-双曲图上边界加权滤波器的谱收缩", "tldr": "针对传统图滤波器难以处理分层图的问题，本文提出了一种新的边界加权算子，用于delta-双曲图。该算子证明了谱收缩特性，提供了一种稳定、无参数的轻量级滤波器，适用于处理具有分层结构的数据。", "motivation": "分层图通常表现出树状分支模式，这种结构特性对传统图滤波器的设计提出了挑战。", "method": "引入了一个边界加权算子，该算子根据每条边的端点漂移到图的格罗莫夫边界的距离来重新调整其权重。通过在delta-双曲网络上使用Busemann函数，证明了该算子谱范数的闭式上限。", "result": "证明了该算子谱范数的闭式上限：每个信号在每次通过时都会损失其能量中由曲率控制的一部分。", "conclusion": "该结果提供了一种无参数、轻量级的滤波器，其稳定性直接源于几何第一原理，为具有密集或隐藏分层结构的数据上的图信号处理提供了一种新的分析工具。", "translation": "分层图通常表现出树状分支模式，这种结构特性对传统图滤波器的设计提出了挑战。我们引入了一种边界加权算子，根据每条边的端点漂移到图的格罗莫夫边界的距离来重新调整其权重。通过在delta-双曲网络上使用Busemann函数，我们证明了该算子谱范数的闭式上限：每个信号在每次通过时都会损失其能量中由曲率控制的一部分。该结果提供了一种无参数、轻量级的滤波器，其稳定性直接源于几何第一原理，为具有密集或隐藏分层结构的数据上的图信号处理提供了一种新的分析工具。", "summary": "本文针对传统图滤波器在处理分层图时面临的挑战，提出了一种新颖的边界加权算子。该算子根据边的端点向图的格罗莫夫边界的漂移距离重新调整边权重。通过在delta-双曲网络上应用Busemann函数，作者证明了该算子谱范数的闭式上限，表明信号在每次通过时都会损失由曲率控制的能量比例。这最终提供了一种稳定、无参数、轻量级的滤波器，为处理具有复杂分层结构的数据提供了新的图信号处理分析工具。", "keywords": "谱收缩, 边界加权滤波器, delta-双曲图, 图信号处理, 分层图", "comments": "该论文的创新之处在于利用了几何特性（格罗莫夫边界、delta-双曲图上的Busemann函数）来设计一种稳定且无参数的图滤波器，以解决分层图的滤波难题，这在图信号处理领域是一个重要的进展。"}}
{"id": "2506.14975", "title": "Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion", "authors": ["Jeffrey Mao", "Raghuram Cauligi Srinivas", "Steven Nogar", "Giuseppe Loianno"], "summary": "Quadrotors hold significant promise for several applications such as\nagriculture, search and rescue, and infrastructure inspection. Achieving\nautonomous operation requires systems to navigate safely through complex and\nunfamiliar environments. This level of autonomy is particularly challenging due\nto the complexity of such environments and the need for real-time decision\nmaking especially for platforms constrained by size, weight, and power (SWaP),\nwhich limits flight time and precludes the use of bulky sensors like Light\nDetection and Ranging (LiDAR) for mapping. Furthermore, computing globally\noptimal, collision-free paths and translating them into time-optimized, safe\ntrajectories in real time adds significant computational complexity. To address\nthese challenges, we present a fully onboard, real-time navigation system that\nrelies solely on lightweight onboard sensors. Our system constructs a dense 3D\nmap of the environment using a novel visual depth estimation approach that\nfuses stereo and monocular learning-based depth, yielding longer-range, denser,\nand less noisy depth maps than conventional stereo methods. Building on this\nmap, we introduce a novel planning and trajectory generation framework capable\nof rapidly computing time-optimal global trajectories. As the map is\nincrementally updated with new depth information, our system continuously\nrefines the trajectory to maintain safety and optimality. Both our planner and\ntrajectory generator outperforms state-of-the-art methods in terms of\ncomputational efficiency and guarantee obstacle-free trajectories. We validate\nour system through robust autonomous flight experiments in diverse indoor and\noutdoor environments, demonstrating its effectiveness for safe navigation in\npreviously unknown settings.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14975v1", "AI": {"title_translation": "基于学习深度补全的非结构化环境时间优化安全导航", "tldr": "本文提出了一种机载实时导航系统，通过融合立体和单目学习深度估计来构建密集的3D地图，并结合新颖的规划框架，使四旋翼飞行器能够在非结构化环境中实现时间优化且安全的自主导航。", "motivation": "四旋翼飞行器在农业、搜救和基础设施检查等领域具有巨大潜力，但其自主操作面临挑战。在复杂和陌生的环境中实现安全导航，特别是对于尺寸、重量和功耗（SWaP）受限的平台，需要实时决策，并且由于无法使用笨重传感器（如LiDAR）进行地图绘制，这变得尤为困难。此外，实时计算全局最优、无碰撞路径并将其转化为时间优化的安全轨迹会增加显著的计算复杂度。", "method": "为解决上述挑战，本文提出了一种完全机载、实时导航系统，该系统仅依赖轻量级机载传感器。系统通过一种新颖的视觉深度估计方法构建密集的3D环境地图，该方法融合了立体和单目学习深度，产生了比传统立体方法更远距离、更密集、噪声更低的深度图。在此地图基础上，引入了一种新颖的规划和轨迹生成框架，能够快速计算时间最优的全局轨迹。随着新深度信息的增量更新，系统持续优化轨迹以保持安全性和最优性。", "result": "与传统立体方法相比，本文提出的视觉深度估计方法生成的深度图具有更远的范围、更高的密度和更低的噪声。此外，所提出的规划器和轨迹生成器在计算效率方面优于最先进的方法，并能保证生成无障碍的轨迹。系统通过在多样化的室内和室外环境中的鲁棒自主飞行实验进行了验证，证明了其在未知环境中安全导航的有效性。", "conclusion": "本文成功开发并验证了一个完全机载、实时的四旋翼飞行器导航系统，该系统通过创新的学习深度补全技术和高效的时间优化轨迹规划，克服了在非结构化环境中自主导航的挑战，实现了在SWaP受限平台上的安全高效飞行。", "translation": "四旋翼飞行器在农业、搜救和基础设施检查等多个应用领域具有重要前景。实现自主操作要求系统能够安全地在复杂和陌生的环境中导航。这种程度的自主性尤其具有挑战性，原因在于此类环境的复杂性以及对实时决策的需求，特别是对于受尺寸、重量和功耗（SWaP）限制的平台，这限制了飞行时间并排除了使用笨重传感器（如激光雷达）进行地图绘制的可能性。此外，实时计算全局最优、无碰撞路径并将其转化为时间优化、安全的轨迹增加了显著的计算复杂性。为了解决这些挑战，我们提出了一种完全机载、实时导航系统，该系统仅依赖轻量级机载传感器。我们的系统利用一种新颖的视觉深度估计方法构建环境的密集3D地图，该方法融合了立体和单目学习深度，产生的深度图比传统立体方法更远、更密集、噪声更低。在此地图基础上，我们引入了一种新颖的规划和轨迹生成框架，能够快速计算时间最优的全局轨迹。随着地图通过新的深度信息增量更新，我们的系统不断完善轨迹以保持安全性和最优性。我们的规划器和轨迹生成器在计算效率方面均优于最先进的方法，并保证了无障碍轨迹。我们通过在多样化的室内和室外环境中的鲁棒自主飞行实验验证了我们的系统，证明了其在先前未知环境中安全导航的有效性。", "summary": "本文提出了一种面向四旋翼飞行器的机载实时导航系统，旨在解决其在非结构化环境中自主导航的挑战。该系统通过融合立体和单目学习深度估计，生成高精度、远距离的密集3D环境地图，有效克服了传统传感器（如LiDAR）的SWaP限制。在此基础上，系统引入了一个创新的规划和轨迹生成框架，能够快速计算并持续优化时间最优的无碰撞路径。实验证明，该系统在计算效率和导航安全性方面均优于现有技术，成功实现了在未知环境中的鲁棒自主飞行。", "keywords": "四旋翼飞行器, 自主导航, 深度补全, 实时规划, 非结构化环境", "comments": "本文的创新点在于其将立体和单目学习深度估计有效融合，为尺寸、重量和功耗受限的平台提供了高质量的3D环境感知能力，这对于轻量级无人机至关重要。同时，其结合的实时规划与轨迹生成框架，在保证安全性的前提下实现了时间最优，且计算效率高，解决了实际应用中的关键瓶颈。该研究对于推动小型无人机在复杂未知环境中的自主导航能力具有重要意义。"}}
{"id": "2506.15167", "title": "LLM Agent for Hyper-Parameter Optimization", "authors": ["Wanzhe Wang", "Jianqiu Peng", "Menghao Hu", "Weihuang Zhong", "Tong Zhang", "Shuai Wang", "Yixin Zhang", "Mingjie Shao", "Wanli Ni"], "summary": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters tuning methods for\nwarm-start particles swarm optimization with cross and mutation (WS-PSO-CM)\nalgortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and\ncommunication are primarily heuristic-based, exhibiting low levels of\nautomation and unsatisfactory performance. In this paper, we design an large\nlanguage model (LLM) agent for automatic hyper-parameters-tuning, where an\niterative framework and model context protocol (MCP) are applied. In\nparticular, the LLM agent is first setup via a profile, which specifies the\nmission, background, and output format. Then, the LLM agent is driven by the\nprompt requirement, and iteratively invokes WS-PSO-CM algorithm for\nexploration. Finally, the LLM agent autonomously terminates the loop and\nreturns a set of hyper-parameters. Our experiment results show that the minimal\nsum-rate achieved by hyper-parameters generated via our LLM agent is\nsignificantly higher than those by both human heuristics and random generation\nmethods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM\nalgorithm background is useful in finding high-performance hyper-parameters.", "comment": "6 pages, 6 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15167v1", "AI": {"title_translation": "LLM智能体用于超参数优化", "tldr": "本文设计了一个LLM智能体，用于自动调优WS-PSO-CM算法的超参数，实验结果表明其性能优于人工启发式和随机生成方法。", "motivation": "当前用于无线电地图赋能无人机轨迹和通信的WS-PSO-CM算法的超参数调优方法主要是基于启发式的，自动化程度低且性能不佳。", "method": "本文设计了一个大型语言模型（LLM）智能体用于自动超参数调优。该智能体应用了迭代框架和模型上下文协议（MCP）。具体地，LLM智能体首先通过指定任务、背景和输出格式的配置文件进行设置，然后由提示要求驱动，迭代调用WS-PSO-CM算法进行探索，最后自主终止循环并返回一组超参数。", "result": "实验结果表明，通过LLM智能体生成的超参数所实现的最小和速率显著高于人工启发式和随机生成方法。", "conclusion": "这表明具有PSO知识和WS-PSO-CM算法背景的LLM智能体在寻找高性能超参数方面是有用的。", "translation": "超参数对于通信算法的性能至关重要。然而，当前用于无线电地图赋能无人机（UAV）轨迹和通信的带有交叉和变异的温启动粒子群优化（WS-PSO-CM）算法的超参数调优方法主要是基于启发式的，自动化程度低且性能不佳。在本文中，我们设计了一个大型语言模型（LLM）智能体用于自动超参数调优，其中应用了迭代框架和模型上下文协议（MCP）。具体地，LLM智能体首先通过一个配置文件进行设置，该文件指定了任务、背景和输出格式。然后，LLM智能体由提示要求驱动，并迭代调用WS-PSO-CM算法进行探索。最后，LLM智能体自主终止循环并返回一组超参数。我们的实验结果表明，通过我们的LLM智能体生成的超参数所实现的最小和速率显著高于人工启发式和随机生成方法。这表明具有PSO知识和WS-PSO-CM算法背景的LLM智能体在寻找高性能超参数方面是有用的。", "summary": "本文提出了一种基于大型语言模型（LLM）智能体的超参数自动调优方法，旨在解决现有WS-PSO-CM算法超参数调优方法自动化程度低和性能不佳的问题。该LLM智能体通过迭代框架和模型上下文协议（MCP）进行操作，能够自主探索并生成高性能超参数。实验结果表明，与人工启发式和随机生成方法相比，该LLM智能体在提高通信算法性能方面表现出显著优势。", "keywords": "LLM智能体, 超参数优化, WS-PSO-CM, 粒子群优化, 无人机通信", "comments": "本文的创新点在于将LLM智能体应用于传统的超参数优化问题，特别是结合了迭代框架和模型上下文协议，实现了超参数调优过程的自动化和性能提升。这为未来基于LLM的优化方法提供了新的思路和潜力。"}}
{"id": "2506.15601", "title": "CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies", "authors": ["Donghyun Gouk", "Seungkwan Kang", "Seungjun Lee", "Jiseon Kim", "Kyungkuk Nam", "Eojin Ryu", "Sangwon Lee", "Dongpyung Kim", "Junhyeok Jang", "Hanyeoreum Bae", "Myoungsoo Jung"], "summary": "This work introduces a GPU storage expansion solution utilizing CXL,\nfeaturing a novel GPU system design with multiple CXL root ports for\nintegrating diverse storage media (DRAMs and/or SSDs). We developed and\nsiliconized a custom CXL controller integrated at the hardware RTL level,\nachieving two-digit nanosecond roundtrip latency, the first in the field. This\nstudy also includes speculative read and deterministic store mechanisms to\nefficiently manage read and write operations to hide the endpoint's backend\nmedia latency variation. Performance evaluations reveal our approach\nsignificantly outperforms existing methods, marking a substantial advancement\nin GPU storage technology.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15601v1", "AI": {"title_translation": "CXL-GPU：通过集成CXL技术拓展GPU内存边界", "tldr": "该工作介绍了一种利用CXL技术扩展GPU存储的解决方案，通过新颖的GPU系统设计和定制CXL控制器，显著提升了性能。", "motivation": "为了扩展GPU的存储能力，并解决现有GPU存储解决方案的局限性。", "method": "提出了一个利用CXL的多CXL根端口GPU系统设计，以集成多种存储介质（DRAM和/或SSD）。开发并硅化了一个定制的CXL控制器（硬件RTL级别集成），实现了两位数纳秒级的往返延迟。采用了推测性读取和确定性存储机制来有效管理读写操作，以隐藏后端介质的延迟变化。", "result": "定制CXL控制器实现了两位数纳秒级的往返延迟。性能评估显示，该方法显著优于现有方法。", "conclusion": "通过CXL技术集成和定制控制器，成功扩展了GPU存储能力，并在性能上取得了显著进步。", "translation": "这项工作介绍了一种利用CXL的GPU存储扩展解决方案，其特点是一种新颖的GPU系统设计，带有多个CXL根端口，用于集成各种存储介质（DRAM和/或SSD）。我们开发并硅化了一个在硬件RTL级别集成的定制CXL控制器，实现了两位数纳秒的往返延迟，这是该领域的首次。本研究还包括推测性读取和确定性存储机制，以有效管理读写操作，从而隐藏端点后端介质的延迟变化。性能评估表明，我们的方法显著优于现有方法，标志着GPU存储技术取得了实质性进展。", "summary": "本研究提出了一种名为CXL-GPU的GPU存储扩展方案，利用CXL技术设计了一个具有多个CXL根端口的GPU系统，以支持DRAM和/或SSD等多种存储介质。通过自主研发并硅化了定制的CXL控制器，实现了业内首次的两位数纳秒级往返延迟。此外，该方案引入了推测性读取和确定性存储机制来优化读写操作，有效应对后端介质延迟波动。性能评估结果显示，CXL-GPU相较于现有方法有显著的性能提升，推动了GPU存储技术的发展。", "keywords": "CXL, GPU存储, 存储扩展, CXL控制器, 内存边界", "comments": "该论文的创新点在于首次将定制的CXL控制器集成到GPU系统，并实现了极低的两位数纳秒级往返延迟，这对于提升GPU访问外部存储的效率至关重要。同时，引入的推测性读取和确定性存储机制也体现了对存储延迟管理的高效考虑。这项工作为GPU内存扩展和异构存储集成提供了一个高性能的解决方案，对未来高性能计算和数据中心架构具有重要意义。"}}
{"id": "2506.15155", "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving", "authors": ["Jiale Xu", "Rui Zhang", "Yi Xiong", "Cong Guo", "Zihan Liu", "Yangjie Zhou", "Weiming Hu", "Hao Wu", "Changxu Shao", "Ziqing Wang", "Yongjie Yuan", "Junping Zhao", "Minyi Guo", "Jingwen Leng"], "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15155v1", "AI": {"title_translation": "eLLM: 面向高效LLM服务的弹性内存管理框架", "tldr": "eLLM是一个弹性内存管理框架，通过统一内存池、弹性内存机制和轻量级调度策略，解决了现有LLM服务系统中内存管理效率低下的问题，显著提高了吞吐量并支持更大的批处理量。", "motivation": "现有的大型语言模型（LLM）服务系统在处理动态工作负载时，由于运行时内存和KV缓存管理的分离，导致内存利用率低下，吞吐量下降近20%。", "method": "本文提出了eLLM弹性内存管理框架，其核心组件包括：1) 虚拟张量抽象，将张量的虚拟地址空间与物理GPU内存解耦，创建统一灵活的内存池；2) 弹性内存机制，通过运行时内存膨胀和收缩动态调整内存分配，利用CPU内存作为可扩展缓冲区；3) 轻量级调度策略，采用SLO感知策略优化内存利用率，平衡性能权衡。", "result": "eLLM显著优于现有系统，解码吞吐量提高2.32倍，对于128K令牌输入支持3倍大的批处理量。", "conclusion": "eLLM通过创新的弹性内存管理框架，有效解决了LLM服务中的内存效率问题，显著提升了系统性能。", "translation": "大型语言模型越来越多地部署在数据中心。服务这些模型需要仔细的内存管理，因为它们的内存使用包括静态权重、动态激活和键值缓存。虽然静态权重是恒定且可预测的，但动态组件（如激活和KV缓存）在运行时频繁变化，给高效内存管理带来了巨大挑战。现代LLM服务系统通常在不同的抽象级别处理运行时内存和KV缓存：运行时内存管理依赖于静态张量抽象，而KV缓存则利用在张量抽象之上构建的基于页表的虚拟化层。这种虚拟化动态管理KV缓存以减轻内存碎片。然而，这种双层方法从根本上隔离了运行时内存和KV缓存管理，导致在动态工作负载下内存利用率不佳，这可能导致吞吐量下降近20%。\n为了解决这些限制，我们提出了eLLM，一个受操作系统中经典内存膨胀机制启发的弹性内存管理框架。eLLM的核心组件包括：(1) 虚拟张量抽象，它将张量的虚拟地址空间与物理GPU内存解耦，创建一个统一且灵活的内存池；(2) 弹性内存机制，通过运行时内存膨胀和收缩动态调整内存分配，利用CPU内存作为可扩展缓冲区；以及(3) 轻量级调度策略，采用SLO感知策略优化内存利用率，有效平衡严格SLO约束下的性能权衡。综合评估表明，eLLM显著优于最先进的系统，解码吞吐量提高了2.32倍，并支持128K令牌输入的3倍大批处理量。", "summary": "eLLM是一个为大型语言模型（LLM）服务设计的弹性内存管理框架，旨在解决现有系统中运行时内存和KV缓存管理分离导致的低效问题。它引入了虚拟张量抽象来统一内存池，通过弹性内存机制动态调整内存分配并利用CPU内存，并采用轻量级SLO感知调度策略。实验结果表明，eLLM在解码吞吐量上实现了2.32倍的提升，并能支持3倍大的批处理量。", "keywords": "LLM服务, 内存管理, 弹性内存, KV缓存, 虚拟张量", "comments": "eLLM的创新之处在于其将操作系统中的内存气球（memory ballooning）机制引入到LLM服务内存管理中，通过统一的虚拟张量抽象和弹性内存机制，有效解决了传统双层内存管理导致的碎片化和低效问题。利用CPU内存作为扩展缓冲区是一个实用的设计，有助于缓解GPU内存限制。该框架对于提升LLM部署效率和降低服务成本具有重要意义。"}}
{"id": "2506.14825", "title": "GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction", "authors": ["Ke Song", "Yunhe Wu", "Chunchit Siu", "Huiyuan Xiong"], "summary": "Addressing the task of 3D semantic occupancy prediction for autonomous\ndriving, we tackle two key issues in existing 3D Gaussian Splating (3DGS)\nmethods: (1) unified feature aggregation neglecting semantic correlations among\nsimilar categories and across regions, and (2) boundary ambiguities caused by\nthe lack of geometric constraints in MLP iterative optimization. We propose the\nGraphGSOcc model, a novel framework that combines semantic and geometric graph\nTransformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the\nDual Gaussians Graph Attenntion, which dynamically constructs dual graph\nstructures: a geometric graph adaptively calculating KNN search radii based on\nGaussian poses, enabling large-scale Gaussians to aggregate features from\nbroader neighborhoods while compact Gaussians focus on local geometric\nconsistency; a semantic graph retaining top-M highly correlated nodes via\ncosine similarity to explicitly encode semantic relationships within and across\ninstances. Coupled with the Multi-scale Graph Attention framework, fine-grained\nattention at lower layers optimizes boundary details, while coarse-grained\nattention at higher layers models object-level topology. Experiments on the\nSurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB,\ndemonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to\nGaussianWorld", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14825v1", "AI": {"title_translation": "GraphGSOcc：用于3D高斯溅射的语义和几何图Transformer的占用预测", "tldr": "GraphGSOcc使用语义和几何图Transformer改进了3D高斯溅射的占用预测，解决了特征聚合和边界模糊问题，提高了mIoU并减少了内存。", "motivation": "现有3D高斯溅射方法存在两个关键问题：1) 统一特征聚合忽略了相似类别和区域间的语义关联；2) MLP迭代优化中缺乏几何约束导致边界模糊。", "method": "提出GraphGSOcc模型，一个结合语义和几何图Transformer的3D高斯溅射占用预测框架。核心是“双高斯图注意力”机制：几何图根据高斯姿态自适应计算KNN搜索半径，使大规模高斯聚合更广阔特征，紧凑高斯关注局部几何一致性；语义图通过余弦相似度保留高相关节点，编码实例内和实例间的语义关系。结合多尺度图注意力框架，低层细粒度注意力优化边界细节，高层粗粒度注意力建模对象级拓扑。", "result": "在SurroundOcc数据集上，mIoU达到24.10%，GPU内存减少到6.1 GB。相比GaussianWorld，mIoU提升1.97%，内存减少13.7%。", "conclusion": "GraphGSOcc通过结合语义和几何图Transformer，有效解决了3D高斯溅射在3D语义占用预测中的挑战，显著提高了性能并优化了内存使用。", "translation": "针对自动驾驶中的3D语义占用预测任务，我们解决了现有3D高斯溅射（3DGS）方法中的两个关键问题：(1) 统一特征聚合忽略了相似类别和区域间的语义关联，以及 (2) MLP迭代优化中缺乏几何约束导致的边界模糊。我们提出了GraphGSOcc模型，这是一个结合语义和几何图Transformer的3D高斯溅射占用预测的新颖框架。我们提出了双高斯图注意力机制，它动态构建双图结构：一个几何图，根据高斯姿态自适应计算KNN搜索半径，使大规模高斯能够聚合来自更广泛邻域的特征，同时紧凑高斯专注于局部几何一致性；一个语义图，通过余弦相似度保留前M个高度相关的节点，以显式编码实例内部和实例之间的语义关系。结合多尺度图注意力框架，较低层的细粒度注意力优化边界细节，而较高层的粗粒度注意力建模对象级拓扑。在SurroundOcc数据集上的实验结果显示，mIoU达到24.10%，GPU内存减少到6.1 GB，与GaussianWorld相比，mIoU提高了1.97%，内存减少了13.7%。", "summary": "GraphGSOcc是一个用于自动驾驶3D语义占用预测的新框架，它通过引入语义和几何图Transformer来改进3D高斯溅射。该模型通过双高斯图注意力机制构建几何图和语义图，分别处理几何一致性和语义关联，并结合多尺度图注意力优化不同粒度的特征。实验结果表明，GraphGSOcc在SurroundOcc数据集上显著提高了mIoU并降低了内存消耗。", "keywords": "3D高斯溅射, 占用预测, 图Transformer, 语义图, 几何图", "comments": "该论文通过引入图Transformer，特别是创新性地设计了双高斯图注意力（几何图和语义图），有效解决了3D高斯溅射在占用预测中面临的特征聚合不足和边界模糊问题。其多尺度图注意力框架也进一步提升了模型的精细化处理能力。在性能提升的同时显著降低了内存占用，这对于实际应用，尤其是在资源受限的自动驾驶场景中，具有重要意义。"}}
{"id": "2506.14784", "title": "Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation", "authors": ["Emre Yilmaz", "Philipp Bekemeyer"], "summary": "Determining onflow parameters is crucial from the perspectives of wind tunnel\ntesting and regular flight and wind turbine operations. These parameters have\ntraditionally been predicted via direct measurements which might lead to\nchallenges in case of sensor faults. Alternatively, a data-driven prediction\nmodel based on surface pressure data can be used to determine these parameters.\nIt is essential that such predictors achieve close to real-time learning as\ndictated by practical applications such as monitoring wind tunnel operations or\nlearning the variations in aerodynamic performance of aerospace and wind energy\nsystems. To overcome the challenges caused by changes in the data distribution\nas well as in adapting to a new prediction task, we propose a transfer learning\nmethodology to predict the onflow parameters, specifically angle of attack and\nonflow speed. It requires first training a convolutional neural network\n(ConvNet) model offline for the core prediction task, then freezing the weights\nof this model except the selected layers preceding the output node, and finally\nexecuting transfer learning by retraining these layers. A demonstration of this\napproach is provided using steady CFD analysis data for an airfoil for i)\ndomain adaptation where transfer learning is performed with data from a target\ndomain having different data distribution than the source domain and ii) task\nadaptation where the prediction task is changed. Further exploration on the\ninfluence of noisy data, performance on an extended domain, and trade studies\nvarying sampling sizes and architectures are provided. Results successfully\ndemonstrate the potential of the approach for adaptation to changing data\ndistribution, domain extension, and task update while the application for noisy\ndata is concluded to be not as effective.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14784v1", "AI": {"title_translation": "使用迁移学习进行域和任务适应以预测迎流参数", "tldr": "该论文提出了一种基于迁移学习的卷积神经网络方法，用于预测迎流参数（攻角和迎流速度），旨在适应不断变化的数据分布和新任务，并在除噪声数据外的情况下取得了良好效果。", "motivation": "确定迎流参数至关重要，但直接测量可能面临传感器故障挑战。替代方案是使用基于表面压力数据的预测模型，但此类模型需要实现接近实时学习，并克服数据分布变化和适应新预测任务的挑战。", "method": "提出了一种迁移学习方法。首先离线训练一个卷积神经网络（ConvNet）模型用于核心预测任务，然后冻结该模型除输出节点前选定层以外的权重，最后通过重新训练这些层来执行迁移学习。通过翼型的稳态CFD分析数据进行了演示，包括域适应（目标域数据分布与源域不同）和任务适应（预测任务改变）。", "result": "结果成功证明了该方法在适应不断变化的数据分布、域扩展和任务更新方面的潜力。然而，对于噪声数据的应用效果不佳。", "conclusion": "所提出的迁移学习方法在预测迎流参数方面，对于适应不同的数据分布、扩展领域和更新任务是有效的，尽管其在噪声数据上的表现有限。", "translation": "确定迎流参数对于风洞试验、常规飞行和风力涡轮机运行至关重要。这些参数传统上通过直接测量来预测，但这可能在传感器故障时带来挑战。或者，可以使用基于表面压力数据的驱动数据预测模型来确定这些参数。至关重要的是，此类预测器应实现接近实时学习，以满足实际应用的需求，例如监测风洞运行或学习航空航天和风能系统气动性能的变化。为了克服数据分布变化以及适应新预测任务所带来的挑战，我们提出了一种迁移学习方法来预测迎流参数，特别是攻角和迎流速度。它首先需要离线训练一个卷积神经网络（ConvNet）模型来完成核心预测任务，然后冻结该模型的权重（输出节点前的选定层除外），最后通过重新训练这些层来执行迁移学习。通过翼型稳态CFD分析数据展示了该方法，包括：i）域适应，其中迁移学习是在目标域数据（与源域数据分布不同）上进行的；ii）任务适应，其中预测任务发生变化。还进一步探讨了噪声数据的影响、在扩展域上的性能以及不同采样大小和架构的权衡研究。结果成功地证明了该方法在适应不断变化的数据分布、域扩展和任务更新方面的潜力，但对于噪声数据的应用效果不佳。", "summary": "本文介绍了一种迁移学习方法，用于预测关键的迎流参数，如攻角和迎流速度，旨在解决直接测量的局限性以及数据驱动模型在实时适应方面的需求。该方法涉及预训练一个卷积神经网络，冻结大部分层，然后对特定层进行微调以实现域和任务适应。通过使用CFD数据进行演示，该方法显示出处理不同数据分布、扩展领域和更新任务的巨大潜力，尽管其在噪声数据上的有效性有限。", "keywords": "迁移学习, 迎流参数, 卷积神经网络, 域适应, 任务适应", "comments": "该论文通过提出一种数据驱动的解决方案，解决了航空航天和风能系统中的实际挑战。迁移学习的应用在不进行大量重新训练的情况下适应不断变化的条件和任务，这具有创新性。对域适应、任务适应以及噪声数据性能的明确评估提供了全面的评估。然而，其在噪声数据上的性能是其主要局限性之一，这在实际应用中至关重要。"}}
{"id": "2506.15278", "title": "Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing", "authors": ["Reuben Binns", "Jake Stein", "Siddhartha Datta", "Max Van Kleek", "Nigel Shadbolt"], "summary": "Ride-sharing platforms like Uber market themselves as enabling `flexibility'\nfor their workforce, meaning that drivers are expected to anticipate when and\nwhere the algorithm will allocate them jobs, and how well remunerated those\njobs will be. In this work we describe our process of participatory action\nresearch with drivers and trade union organisers, culminating in a\nparticipatory audit of Uber's algorithmic pay and work allocation, before and\nafter the introduction of dynamic pricing. Through longitudinal analysis of 1.5\nmillion trips from 258 drivers in the UK, we find that after dynamic pricing,\npay has decreased, Uber's cut has increased, job allocation and pay is less\npredictable, inequality between drivers is increased, and drivers spend more\ntime waiting for jobs. In addition to these findings, we provide methodological\nand theoretical contributions to algorithm auditing, gig work, and the emerging\npractice of worker data science.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15278v1", "AI": {"title_translation": "即使你能得到，也不是什么好活；对优步算法薪酬和定价的纵向研究", "tldr": "研究发现，优步引入动态定价后，司机收入下降，优步抽成增加，工作分配和收入可预测性降低，司机间不平等加剧，等待时间增加。", "motivation": "优步等网约车平台宣称其能为劳动力提供“灵活性”，但司机需要预测算法分配工作和报酬。本研究旨在通过参与式行动研究，审计优步的算法薪酬和工作分配，特别是在动态定价引入前后，以揭示其真实影响。", "method": "采用与司机和工会组织者合作的参与式行动研究，对优步的算法薪酬和工作分配进行参与式审计。通过对英国258名司机150万次行程的纵向分析，比较动态定价引入前后的变化。", "result": "动态定价后，司机收入下降，优步抽成增加，工作分配和薪酬可预测性降低，司机间不平等加剧，司机等待工作的时间增加。", "conclusion": "动态定价对优步司机造成了显著的负面影响，导致他们的收入减少、工作确定性降低以及不平等加剧。本研究也为算法审计、零工经济和工人数据科学提供了重要的方法论和理论贡献。", "translation": "网约车平台如优步将自己宣传为能为劳动力提供“灵活性”，这意味着司机需要预测算法何时何地会分配给他们工作，以及这些工作的报酬会如何。在这项工作中，我们描述了我们与司机和工会组织者进行参与式行动研究的过程，最终对优步的算法薪酬和工作分配进行了参与式审计，包括动态定价引入之前和之后。通过对英国258名司机150万次行程的纵向分析，我们发现，在动态定价之后，薪酬下降了，优步的抽成增加了，工作分配和薪酬的可预测性降低了，司机之间的不平等加剧了，并且司机花在等待工作上的时间更多了。除了这些发现之外，我们还为算法审计、零工工作以及新兴的工人数据科学实践提供了方法论和理论贡献。", "summary": "这项研究通过与英国258名优步司机合作，对150万次行程进行纵向分析，审计了优步算法薪酬和工作分配在引入动态定价前后的变化。结果显示，动态定价导致司机收入下降、优步抽成增加、工作分配和薪酬可预测性降低、司机间不平等加剧以及等待时间增加。该研究还为算法审计、零工经济和工人数据科学提供了方法论和理论贡献。", "keywords": "优步, 算法薪酬, 动态定价, 零工经济, 算法审计", "comments": "这项研究通过结合参与式行动研究和大规模数据分析，揭示了算法定价对零工经济中工人收入和工作条件产生的负面影响。其创新之处在于将工人视角纳入算法审计，并提供了具体的数据证据来反驳平台宣称的“灵活性”。这对于理解算法管理下的劳动关系以及推动工人权益保护具有重要意义。"}}
{"id": "2506.15106", "title": "Local Differential Privacy for Distributed Stochastic Aggregative Optimization with Guaranteed Optimality", "authors": ["Ziqin Chen", "Yongqiang Wang"], "summary": "Distributed aggregative optimization underpins many cooperative optimization\nand multi-agent control systems, where each agent's objective function depends\nboth on its local optimization variable and an aggregate of all agents'\noptimization variables. Existing distributed aggregative optimization\napproaches typically require access to accurate gradients of the objective\nfunctions, which, however, are often hard to obtain in real-world applications.\nFor example, in machine learning, gradients are commonly contaminated by two\nmain sources of noise: the randomness inherent in sampled data, and the\nadditional variability introduced by mini-batch computations. In addition to\nthe issue of relying on accurate gradients, existing distributed aggregative\noptimization approaches require agents to share explicit information, which\ncould breach the privacy of participating agents. We propose an algorithm that\ncan solve both problems with existing distributed aggregative optimization\napproaches: not only can the proposed algorithm guarantee mean-square\nconvergence to an exact optimal solution when the gradients are subject to\nnoise, it also simultaneously ensures rigorous differential privacy, with the\ncumulative privacy budget guaranteed to be finite even when the number of\niterations tends to infinity. To the best of our knowledge, this is the first\nalgorithm able to guarantee both accurate convergence and rigorous differential\nprivacy in distributed aggregative optimization. Besides characterizing the\nconvergence rates under nonconvex/convex/strongly convex conditions, we also\nrigorously quantify the cost of differential privacy in terms of convergence\nrates. Experimental results on personalized machine learning using benchmark\ndatasets confirm the efficacy of the proposed algorithm.", "comment": "21 pages, 6 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15106v1", "AI": {"title_translation": "具有保证最优性的分布式随机聚合优化中的局部差分隐私", "tldr": "本文提出一种新的分布式聚合优化算法，解决了现有方法在梯度噪声下的收敛性和隐私保护问题。该算法在梯度有噪声时能保证均方收敛到精确最优解，并同时提供严格的差分隐私，且累积隐私预算有限。", "motivation": "现有分布式聚合优化方法面临两大挑战：一是需要精确梯度，而实际应用中梯度常受采样数据和mini-batch计算噪声污染；二是要求代理间共享显式信息，可能侵犯参与者的隐私。", "method": "本文提出了一种新算法，该算法在梯度存在噪声的情况下，仍能保证均方收敛到精确最优解，并同时确保严格的差分隐私，即使迭代次数趋于无穷大，累积隐私预算也保证是有限的。", "result": "该算法在非凸/凸/强凸条件下表征了收敛速率，并严格量化了差分隐私对收敛速率的影响。在基准数据集上进行的个性化机器学习实验证实了所提出算法的有效性。", "conclusion": "据作者所知，这是第一个能够在分布式聚合优化中同时保证准确收敛和严格差分隐私的算法，成功解决了现有方法在噪声和隐私方面的挑战。", "translation": "分布式聚合优化是许多协作优化和多智能体控制系统的基础，其中每个智能体的目标函数既取决于其局部优化变量，也取决于所有智能体优化变量的聚合。现有的分布式聚合优化方法通常需要访问目标函数的精确梯度，然而在实际应用中往往难以获得。例如，在机器学习中，梯度通常受到两个主要噪声源的污染：采样数据固有的随机性，以及小批量计算引入的额外变异性。除了依赖精确梯度的问题外，现有的分布式聚合优化方法要求智能体共享显式信息，这可能侵犯参与智能体的隐私。我们提出了一种算法，可以解决现有分布式聚合优化方法的这两个问题：所提出的算法不仅可以在梯度受噪声影响时保证均方收敛到精确最优解，而且还同时确保严格的差分隐私，即使迭代次数趋于无穷大，累积隐私预算也保证是有限的。据我们所知，这是第一个能够在分布式聚合优化中同时保证准确收敛和严格差分隐私的算法。除了表征非凸/凸/强凸条件下的收敛速率外，我们还严格量化了差分隐私在收敛速率方面的成本。使用基准数据集进行个性化机器学习的实验结果证实了所提出算法的有效性。", "summary": "本文针对分布式聚合优化中对精确梯度的依赖和隐私泄露问题，提出了一种创新的算法。该算法在梯度存在噪声时，仍能保证均方收敛到精确最优解，并同时提供严格的局部差分隐私，即使迭代次数无限，累积隐私预算也有限。这是首个在分布式聚合优化中同时实现准确收敛和严格差分隐私的算法，并通过实验验证了其在个性化机器学习中的有效性。", "keywords": "分布式聚合优化, 局部差分隐私, 随机优化, 隐私保护, 梯度噪声", "comments": "这项工作具有显著的创新性，因为它首次在一个算法中同时解决了分布式聚合优化中的两大挑战：在梯度噪声下保证优化精度和提供严格的差分隐私。其对隐私预算有限性的保证，即使在无限迭代次数下，也极大地增强了其实用性和理论完整性。"}}
{"id": "2506.14914", "title": "Recursive Variational Autoencoders for 3D Blood Vessel Generative Modeling", "authors": ["Paula Feldman", "Miguel Fainstein", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "summary": "Anatomical trees play an important role in clinical diagnosis and treatment\nplanning. Yet, accurately representing these structures poses significant\nchallenges owing to their intricate and varied topology and geometry. Most\nexisting methods to synthesize vasculature are rule based, and despite\nproviding some degree of control and variation in the structures produced, they\nfail to capture the diversity and complexity of actual anatomical data. We\ndeveloped a Recursive variational Neural Network (RvNN) that fully exploits the\nhierarchical organization of the vessel and learns a low-dimensional manifold\nencoding branch connectivity along with geometry features describing the target\nsurface. After training, the RvNN latent space can be sampled to generate new\nvessel geometries. By leveraging the power of generative neural networks, we\ngenerate 3D models of blood vessels that are both accurate and diverse, which\nis crucial for medical and surgical training, hemodynamic simulations, and many\nother purposes. These results closely resemble real data, achieving high\nsimilarity in vessel radii, length, and tortuosity across various datasets,\nincluding those with aneurysms. To the best of our knowledge, this work is the\nfirst to utilize this technique for synthesizing blood vessels.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14914v1", "AI": {"title_translation": "用于3D血管生成建模的递归变分自编码器", "tldr": "该研究开发了一种递归变分神经网络（RvNN），用于生成准确且多样化的3D血管模型，解决了现有方法在捕捉血管复杂性和多样性方面的不足。", "motivation": "解剖树在临床诊断和治疗规划中至关重要，但其复杂的拓扑和几何结构使得准确表示面临挑战。现有的血管合成方法多基于规则，无法捕捉实际解剖数据的多样性和复杂性。", "method": "开发了一种递归变分神经网络（RvNN），该网络充分利用血管的层次结构，学习编码分支连接和描述目标表面几何特征的低维流形。训练后，可以通过采样RvNN的潜在空间来生成新的血管几何形状。", "result": "生成了既准确又多样化的3D血管模型，这些模型与真实数据非常相似，在各种数据集（包括动脉瘤数据集）中，血管半径、长度和弯曲度均表现出高度相似性。", "conclusion": "本工作首次将递归变分神经网络技术应用于血管合成，生成的模型对于医疗和手术训练、血流动力学模拟及其他许多目的至关重要。", "translation": "解剖树在临床诊断和治疗规划中发挥着重要作用。然而，由于其复杂多变的拓扑结构和几何形状，准确表示这些结构带来了巨大的挑战。大多数现有的血管合成方法都是基于规则的，尽管它们在一定程度上提供了对所生成结构的控制和变化，但未能捕捉到实际解剖数据的多样性和复杂性。我们开发了一种递归变分神经网络（RvNN），该网络充分利用了血管的层次组织结构，并学习了一个低维流形，该流形编码了分支连接以及描述目标表面的几何特征。训练后，可以对RvNN的潜在空间进行采样以生成新的血管几何形状。通过利用生成式神经网络的强大功能，我们生成了既准确又多样化的3D血管模型，这对于医疗和手术训练、血流动力学模拟以及许多其他目的至关重要。这些结果与真实数据非常相似，在各种数据集（包括动脉瘤）中，血管半径、长度和弯曲度均实现了高度相似性。据我们所知，这项工作是首次将该技术用于合成血管。", "summary": "该论文提出了一种递归变分神经网络（RvNN）用于3D血管生成建模。针对现有规则方法无法捕捉血管多样性和复杂性的问题，RvNN通过学习血管的层次结构和低维流形，能够生成准确且多样化的3D血管模型。这些模型在血管半径、长度和弯曲度上与真实数据高度相似，包括动脉瘤结构，为医疗训练和模拟提供了重要工具。这是首次将此类技术应用于血管合成。", "keywords": "递归变分神经网络, 3D血管建模, 生成模型, 解剖树, 医疗图像", "comments": "这项工作具有显著的创新性，首次将递归变分神经网络（RvNN）应用于3D血管的生成建模。它有效地解决了传统基于规则方法在生成血管结构时缺乏多样性和复杂性的局限，通过学习数据内在的层次结构，生成了高度逼真且多样化的血管模型，这在医学图像分析和模拟领域具有重要意义。"}}
{"id": "2506.15136", "title": "Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead", "authors": ["Kehui Li", "Binggui Zhou", "Jiajia Guo", "Feifei Gao", "Guanghua Yang", "Shaodan Ma"], "summary": "Multi-user millimeter-wave communication relies on narrow beams and dense\ncell deployments to ensure reliable connectivity. However, tracking optimal\nbeams for multiple mobile users across multiple base stations (BSs) results in\nsignificant signaling overhead. Recent works have explored the capability of\nout-of-band (OOB) modalities in obtaining spatial characteristics of wireless\nchannels and reducing pilot overhead in single-BS single-user/multi-user\nsystems. However, applying OOB modalities for multi-BS selection towards dense\ncell deployments leads to high coordination overhead, i.e, excessive computing\noverhead and high latency in data exchange. How to leverage OOB modalities to\neliminate pilot overhead and achieve efficient multi-BS coordination in\nmulti-BS systems remains largely unexplored. In this paper, we propose a novel\nOOB modality synergy (OMS) based mobility management scheme to realize\nmulti-user beam prediction and proactive BS selection by synergizing two OOB\nmodalities, i.e., vision and location. Specifically, mobile users are initially\nidentified via spatial alignment of visual sensing and location feedback, and\nthen tracked according to the temporal correlation in image sequence.\nSubsequently, a binary encoding map based gain and beam prediction network\n(BEM-GBPN) is designed to predict beamforming gains and optimal beams for\nmobile users at each BS, such that a central unit can control the BSs to\nperform user handoff and beam switching. Simulation results indicate that the\nproposed OMS-based mobility management scheme enhances beam prediction and BS\nselection accuracy and enables users to achieve 91% transmission rates of the\noptimal with zero pilot overhead and significantly improve multi-BS\ncoordination efficiency compared to existing methods.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15136v1", "AI": {"title_translation": "基于带外模态协同的多用户波束预测和主动基站选择，零导频开销", "tldr": "本文提出了一种基于带外模态协同（OMS）的移动性管理方案，通过融合视觉和定位两种带外模态，实现了多用户波束预测和主动基站选择，消除了导频开销，并显著提高了多基站协调效率。", "motivation": "多用户毫米波通信中，在多个基站（BS）之间跟踪多个移动用户的最佳波束会导致显著的信令开销。现有带外（OOB）模态方法在应用于多基站选择时，会带来高协调开销（计算开销和数据交换延迟）。因此，如何利用OOB模态消除导频开销并在多基站系统中实现高效的多基站协调仍是一个未充分探索的问题。", "method": "本文提出了一种新颖的基于带外模态协同（OMS）的移动性管理方案，通过协同视觉和定位两种带外模态，实现多用户波束预测和主动基站选择。具体而言，首先通过视觉感知和定位反馈的空间对齐来识别移动用户，然后根据图像序列的时间相关性进行跟踪。随后，设计了一个基于二进制编码图的增益和波束预测网络（BEM-GBPN），用于预测每个基站处移动用户的波束赋形增益和最佳波束，从而使中央单元能够控制基站执行用户切换和波束切换。", "result": "仿真结果表明，所提出的OMS移动性管理方案提高了波束预测和基站选择的准确性，使S用户在零导频开销下达到最佳传输速率的91%，并且与现有方法相比，显著提高了多基站协调效率。", "conclusion": "本文提出的基于带外模态协同的移动性管理方案，通过融合视觉和定位信息，有效解决了多用户毫米波通信中多基站场景下的波束预测和基站选择问题，成功消除了导频开销，并显著提升了系统性能和协调效率。", "translation": "多用户毫米波通信依赖窄波束和密集小区部署以确保可靠连接。然而，在多个基站（BS）之间跟踪多个移动用户的最佳波束会导致显著的信令开销。最近的研究探索了带外（OOB）模态在获取无线信道空间特性和减少单基站单用户/多用户系统中的导频开销方面的能力。然而，将OOB模态应用于密集小区部署的多基站选择会导致高协调开销，即过高的计算开销和高数据交换延迟。如何利用OOB模态消除导频开销并在多基站系统中实现高效的多基站协调仍然是一个未充分探索的问题。在本文中，我们提出了一种新颖的基于带外模态协同（OMS）的移动性管理方案，通过协同视觉和定位两种带外模态，实现多用户波束预测和主动基站选择。具体而言，移动用户首先通过视觉感知和定位反馈的空间对齐进行识别，然后根据图像序列的时间相关性进行跟踪。随后，设计了一个基于二进制编码图的增益和波束预测网络（BEM-GBPN），用于预测每个基站处移动用户的波束赋形增益和最佳波束，以便中央单元可以控制基站执行用户切换和波束切换。仿真结果表明，所提出的基于OMS的移动性管理方案提高了波束预测和基站选择的准确性，并使S用户在零导频开销下达到最佳传输速率的91%，与现有方法相比，显著提高了多基站协调效率。", "summary": "本文提出了一种名为带外模态协同（OMS）的移动性管理方案，旨在解决多用户毫米波通信中多基站场景下的波束预测和基站选择问题。该方案通过融合视觉和定位两种带外模态，实现了对移动用户的识别、跟踪，并利用设计的BEM-GBPN网络预测最佳波束和波束赋形增益。其核心优势在于能够实现零导频开销，并显著提升多基站协调效率和传输速率，有效克服了传统方法面临的高信令与协调开销挑战。", "keywords": "带外模态协同, 波束预测, 基站选择, 零导频开销, 毫米波通信", "comments": "该论文的创新点在于首次将视觉和定位这两种带外模态进行协同，以解决多用户、多基站毫米波通信中的波束预测和基站选择问题，并实现了零导频开销，这对于提升未来移动通信的效率和降低开销具有重要意义。提出的BEM-GBPN网络是实现这一目标的关键技术。然而，实际部署中，视觉感知和定位的精度受环境影响较大，且实时处理的计算复杂度可能带来挑战。"}}
{"id": "2506.15379", "title": "Tractable Graph Structures in EFX Orientation", "authors": ["Václav Blažej", "Sushmita Gupta", "M. S. Ramanujan", "Peter Strulo"], "summary": "Since its introduction, envy-freeness up to any good (EFX) has become a\nfundamental solution concept in fair division of indivisible goods. Its\nexistence remains elusive -- even for four agents with additive utility\nfunctions, it is unknown whether an EFX allocation always exists.\nUnsurprisingly, restricted settings to delineate tractable and intractable\ncases have been explored. Christadolou, Fiat et al.[EC'23] introduced the\nnotion of EFX-orientation, where the agents form the vertices of a graph and\nthe items correspond to edges, and an agent values only the items that are\nincident to it. The goal is to allocate items to one of the adjacent agents\nwhile satisfying the EFX condition.\n  Building on the work of Zeng and Mehta'24, which established a sharp\ncomplexity threshold based on the structure of the underlying graph --\npolynomial-time solvability for bipartite graphs and NP-hardness for graphs\nwith chromatic number at least three -- we further explore the algorithmic\nlandscape of EFX-orientation using parameterized graph algorithms.\n  Specifically, we show that bipartiteness is a surprisingly stringent\ncondition for tractability: EFX orientation is NP-complete even when the\nvaluations are symmetric, binary and the graph is at most two edge-removals\naway from being bipartite. Moreover, introducing a single non-binary value\nmakes the problem NP-hard even when the graph is only one edge removal away\nfrom being bipartite. We further perform a parameterized analysis to examine\nstructures of the underlying graph that enable tractability. In particular, we\nshow that the problem is solvable in linear time on graphs whose treewidth is\nbounded by a constant and that the complexity of an instance is closely tied to\nthe sizes of acyclic connected components on its one-valued edges.", "comment": null, "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.15379v1", "AI": {"title_translation": "EFX导向中的可处理图结构", "tldr": "该论文探讨了EFX导向在图结构上的可处理性，发现二分性是可处理性的严格条件，并识别了其他可处理的图结构。", "motivation": "EFX（envy-freeness up to any good）是公平分配中的一个基本概念，但其存在性难以捉摸。EFX-orientation作为一种受限设置被引入以探索可处理情况。本文旨在利用参数化图算法进一步探索EFX导向的算法前景，尤其是在先前工作已根据图结构（二分图与非二分图）确定了复杂性阈值的基础上。", "method": "本文采用参数化图算法来探索EFX导向的算法前景。具体分析了问题在特定图条件（例如，与二分图的接近程度、树宽）和估值类型（对称、二元、非二元）下的复杂性。", "result": "结果表明，二分性是可处理性的一个非常严格的条件：即使在估值对称、二元且图仅需移除最多两条边即可变为二分图时，EFX导向也是NP完全的。此外，引入单个非二元值会使问题变为NP-hard，即使图仅需移除一条边即可变为二分图。然而，该问题在树宽由常数限制的图上可以在线性时间内解决，并且实例的复杂性与单值边上无环连通分量的大小密切相关。", "conclusion": "本文表明，EFX导向的可处理性对图结构（例如，与二分性的微小偏差）和估值类型（引入非二元值）的微小变化高度敏感，同时识别了其他能够实现高效求解的结构属性（如有限树宽）。", "translation": "自引入以来，EFX（envy-freeness up to any good）已成为不可分割物品公平分配中的一个基本解决方案概念。它的存在性仍然难以捉摸——即使对于具有附加效用函数的四个代理，EFX分配是否总是存在仍然未知。毫不奇怪，人们已经探索了限制性设置以划定可处理和不可处理的情况。Christadolou, Fiat et al.[EC'23]引入了EFX-orientation的概念，其中代理构成图的顶点，物品对应边，并且代理仅重视与其相邻的物品。目标是在满足EFX条件的同时将物品分配给其中一个相邻代理。基于Zeng和Mehta'24的工作，该工作根据底层图的结构建立了尖锐的复杂性阈值——二分图的多项式时间可解性以及色数至少为三的图的NP-hard性——我们使用参数化图算法进一步探索EFX-orientation的算法前景。具体来说，我们表明二分性对于可处理性来说是一个令人惊讶的严格条件：即使当估值是对称的、二元的，并且图最多两次移除边即可变为二分图时，EFX方向也是NP完全的。此外，引入单个非二元值会使问题变为NP-hard，即使图仅一次移除边即可变为二分图。我们进一步进行了参数化分析，以检查能够实现可处理性的底层图结构。特别是，我们表明该问题在树宽由常数限制的图上可以在线性时间内解决，并且实例的复杂性与单值边上无环连通分量的大小密切相关。", "summary": "本论文研究了EFX导向在图结构上的算法可处理性，其中代理重视与其相邻的物品。在先前工作的基础上，该研究揭示二分性是实现可处理性的一个极其严格的条件，即使图结构仅有微小偏差或引入非二元估值，问题也会迅速变为NP完全/NP难。然而，论文也识别了新的可处理情况，例如具有有限树宽的图，表明问题的复杂性与特定的图结构属性紧密相关。", "keywords": "EFX, 公平分配, 图算法, NP-完全性, 树宽", "comments": "这篇论文显著推进了对EFX导向计算复杂度的理解。它精确地划定了可处理性的边界，揭示了多项式时间可解性在放宽如二分性等限制性条件时是如何脆弱的。使用参数化图算法是揭示这些细粒度复杂性洞察的有力方法，为公平分配和算法图论贡献了有价值的理论成果。"}}
{"id": "2506.15571", "title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Tu Nguyen Thi Ngoc"], "summary": "Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.", "comment": "9 pages, 8 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15571v1", "AI": {"title_translation": "MicroRicci：一种用于自适应网格平滑的贪婪局部Ricci流求解器", "tldr": "MicroRicci是一种自适应、局部且高效的Ricci流网格平滑器，通过结合编码理论和小型神经网络，实现了显著的性能提升和高质量结果。", "motivation": "实时大规模网格平滑是一个巨大挑战，传统Ricci流求解器需要昂贵的全局更新，而贪婪启发式方法则收敛缓慢或调优困难。", "method": "论文提出了MicroRicci，一个自适应的局部Ricci流求解器。它借鉴了编码理论，核心是一个贪婪的综合症解码步骤，能在O(E)时间内定位并纠正最大曲率误差，并辅以两个小型神经网络模块，用于动态选择顶点和步长。", "result": "在110个SJTU-TMQA网格上，MicroRicci将迭代次数从950±140次削减到400±80次（加速2.4倍），将曲率分布从0.19收紧到0.185，实现了UV畸变与MOS的相关性r=-0.93。每次迭代仅增加0.25毫秒（从0.80到1.05毫秒），端到端运行时比现有技术快1.8倍。", "conclusion": "MicroRicci结合了线性时间更新、自动超参数适应以及高质量的几何和感知结果，非常适合图形、模拟及相关领域中的实时、资源受限应用。", "translation": "大规模实时网格平滑仍然是一个艰巨的挑战：经典的Ricci流求解器需要耗费昂贵的全局更新，而贪婪启发式方法则收敛缓慢或调优困难。我们提出了MicroRicci，这是第一个真正的自适应、局部Ricci流求解器，它借鉴了编码理论的思想，并将其打包到仅1K + 200个参数中。其核心是一个贪婪的综合症解码步骤，能够在O(E)时间内精确定位并纠正最大的曲率误差，并辅以两个小型神经网络模块，用于动态选择顶点和步长。在110个SJTU-TMQA网格的多样化数据集上，MicroRicci将迭代次数从950±140次削减到400±80次（加速2.4倍），将曲率分布从0.19收紧到0.185，并实现了UV畸变与MOS（平均意见分数）之间r = -0.93的显著相关性。它每次迭代仅增加0.25毫秒（从0.80毫秒到1.05毫秒），与现有最先进的方法相比，端到端运行时加速了1.8倍。MicroRicci结合了线性时间更新、自动超参数适应以及高质量的几何和感知结果，使其非常适合图形、模拟及相关领域中的实时、资源受限应用。", "summary": "针对大规模实时网格平滑中传统Ricci流求解器全局更新成本高和贪婪启发式方法收敛慢、调优难的问题，本文提出了MicroRicci。这是一种首个真正的自适应局部Ricci流求解器，它借鉴了编码理论，通过一个贪婪的综合症解码步骤在O(E)时间内修正曲率误差，并辅以小型神经网络模块自适应选择顶点和步长。实验结果表明，MicroRicci在迭代次数、曲率分布和UV畸变方面均表现优异，且相比现有技术实现了显著的运行时加速，证明其非常适用于实时、资源受限的图形和模拟应用。", "keywords": "网格平滑, Ricci流, 自适应, 局部求解器, 编码理论", "comments": "MicroRicci的创新之处在于其将编码理论与小型神经网络相结合，实现了自适应、局部化且高效的网格平滑。其线性时间更新和自动超参数适应特性显著提升了实用性，使其在实时、资源受限的应用中具有重要价值。"}}
{"id": "2506.15064", "title": "HiPreNets: High-Precision Neural Networks through Progressive Training", "authors": ["Ethan Mulle", "Wei Kang", "Qi Gong"], "summary": "Deep neural networks are powerful tools for solving nonlinear problems in\nscience and engineering, but training highly accurate models becomes\nchallenging as problem complexity increases. Non-convex optimization and\nnumerous hyperparameters to tune make performance improvement difficult, and\ntraditional approaches often prioritize minimizing mean squared error (MSE)\nwhile overlooking $L^{\\infty}$ error, which is the critical focus in many\napplications. To address these challenges, we present a progressive framework\nfor training and tuning high-precision neural networks (HiPreNets). Our\napproach refines a previously explored staged training technique for neural\nnetworks that improves an existing fully connected neural network by\nsequentially learning its prediction residuals using additional networks,\nleading to improved overall accuracy. We discuss how to take advantage of the\nstructure of the residuals to guide the choice of loss function, number of\nparameters to use, and ways to introduce adaptive data sampling techniques. We\nvalidate our framework's effectiveness through several benchmark problems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15064v1", "AI": {"title_translation": "HiPreNets：通过渐进式训练实现高精度神经网络", "tldr": "本文提出了HiPreNets，一个通过顺序学习预测残差来提高神经网络精度的渐进式训练框架，以解决复杂问题中高精度模型训练的挑战。", "motivation": "深度神经网络在解决科学和工程中的非线性问题时面临挑战，尤其是在问题复杂性增加时，训练高精度模型变得困难。非凸优化和大量的超参数调整使得性能提升不易，并且传统方法通常优先考虑最小化均方误差（MSE），而忽略了在许多应用中至关重要的$L^{\\infty}$误差。", "method": "我们提出了一个用于训练和调整高精度神经网络（HiPreNets）的渐进式框架。该方法改进了一种分阶段训练技术，通过使用额外的网络顺序学习现有全连接神经网络的预测残差，从而提高整体精度。我们还讨论了如何利用残差结构来指导损失函数的选择、参数数量以及引入自适应数据采样技术。", "result": "通过几个基准问题验证了我们框架的有效性。", "conclusion": "本文提出的HiPreNets渐进式训练框架能够有效提高神经网络的精度，解决了复杂问题中高精度模型训练的挑战，并通过基准测试验证了其有效性。", "translation": "深度神经网络是解决科学和工程中非线性问题的强大工具，但随着问题复杂性的增加，训练高精度模型变得具有挑战性。非凸优化和大量的超参数调整使得性能提升变得困难，并且传统方法通常优先考虑最小化均方误差（MSE），而忽略了在许多应用中至关重要的$L^{\\infty}$误差。为了应对这些挑战，我们提出了一个用于训练和调整高精度神经网络（HiPreNets）的渐进式框架。我们的方法改进了先前探索的神经网络分阶段训练技术，通过使用额外的网络顺序学习其预测残差来改进现有全连接神经网络，从而提高整体精度。我们讨论了如何利用残差结构来指导损失函数的选择、要使用的参数数量以及引入自适应数据采样技术。我们通过几个基准问题验证了我们框架的有效性。", "summary": "本文提出了一种名为HiPreNets的渐进式框架，旨在解决复杂问题中高精度神经网络训练的挑战。该方法通过顺序学习现有神经网络的预测残差来提高整体精度，并利用残差结构指导损失函数选择、参数数量和自适应数据采样。该框架的有效性已通过多个基准问题得到验证。", "keywords": "高精度神经网络, 渐进式训练, 残差学习, HiPreNets, $L^{\\infty}$误差", "comments": "这项工作通过引入一种渐进式训练框架HiPreNets，创新性地解决了深度神经网络在复杂问题中训练高精度模型时面临的挑战，特别关注了$L^{\\infty}$误差而非仅仅是MSE。其通过学习残差来逐步提升精度的方法具有重要意义，为高精度模型训练提供了新的思路。"}}
{"id": "2506.15514", "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field\nof music information retrieval, despite great advances in automatic speech\nrecognition (ASR) brought about by transformer-based architectures in recent\nyears. One of the major challenges in ALT is the high amplitude of interfering\naudio signals relative to conventional ASR due to musical accompaniment. Recent\nadvances in music source separation have enabled automatic extraction of\nhigh-quality separated vocals, which could potentially improve ALT performance.\nHowever, the effect of source separation has not been systematically\ninvestigated in order to establish best practices for its use. This work\nexamines the impact of source separation on ALT using Whisper, a\nstate-of-the-art open source ASR model. We evaluate Whisper's performance on\noriginal audio, separated vocals, and vocal stems across short-form and\nlong-form transcription tasks. For short-form, we suggest a concatenation\nmethod that results in a consistent reduction in Word Error Rate (WER). For\nlong-form, we propose an algorithm using source separation as a vocal activity\ndetector to derive segment boundaries, which results in a consistent reduction\nin WER relative to Whisper's native long-form algorithm. Our approach achieves\nstate-of-the-art results for an open source system on the Jam-ALT long-form ALT\nbenchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the\nfirst dataset of long-form lyric transcripts following the Jam-ALT guidelines\nfor which vocal stems are publicly available.", "comment": "Accepted at 2025 ICME Workshop AI for Music", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15514v1", "AI": {"title_translation": "利用音乐源分离技术结合 Whisper 进行自动歌词转录", "tldr": "本文系统性地研究了音乐源分离技术对使用 Whisper 进行自动歌词转录（ALT）的影响，并提出了针对短文本和长文本转录的有效方法，在不进行训练或微调的情况下，在长文本ALT基准测试中取得了最先进的开源系统表现，并发布了新的长文本歌词转录数据集。", "motivation": "自动歌词转录（ALT）由于音乐伴奏带来的高振幅干扰信号，仍然是一项具有挑战性的任务。尽管自动语音识别（ASR）在Transformer架构的推动下取得了显著进展，但音乐源分离对ALT性能的影响尚未被系统性地研究，以建立其最佳实践。", "method": "本研究使用最先进的开源ASR模型 Whisper，评估了源分离对ALT的影响。我们评估了 Whisper 在原始音频、分离人声和人声干音上在短文本和长文本转录任务中的性能。对于短文本，我们提出了一种拼接方法；对于长文本，我们提出了一种利用源分离作为人声活动检测器来确定片段边界的算法。", "result": "提出的拼接方法在短文本转录中实现了词错误率（WER）的持续降低。长文本算法相对于 Whisper 的原生长文本算法，在WER上实现了持续降低。我们的方法在不进行任何训练或微调的情况下，在Jam-ALT长文本ALT基准测试中，为开源系统取得了最先进的结果。此外，我们还发布了MUSDB-ALT，这是第一个遵循Jam-ALT指南且人声干音公开可用的长文本歌词转录数据集。", "conclusion": "音乐源分离技术能够有效提升 Whisper 在自动歌词转录任务中的性能，尤其是在处理长文本时，通过合理利用源分离可以显著降低词错误率，并在不进行额外训练的情况下达到领先水平。", "translation": "尽管近年来Transformer架构在自动语音识别（ASR）方面取得了巨大进步，但自动歌词转录（ALT）在音乐信息检索领域仍是一项具有挑战性的任务。ALT的主要挑战之一是相对于传统ASR，由于音乐伴奏导致干扰音频信号的振幅较高。音乐源分离的最新进展使得高质量分离人声的自动提取成为可能，这可能提高ALT性能。然而，源分离的效果尚未被系统地研究，以建立其使用的最佳实践。这项工作利用最先进的开源ASR模型 Whisper，检验了源分离对ALT的影响。我们评估了 Whisper 在原始音频、分离人声和人声干音上在短文本和长文本转录任务中的性能。对于短文本，我们提出了一种拼接方法，该方法能持续降低词错误率（WER）。对于长文本，我们提出了一种利用源分离作为人声活动检测器来确定片段边界的算法，该算法相对于 Whisper 的原生长文本算法，能持续降低WER。我们的方法在不进行任何训练或微调的情况下，在Jam-ALT长文本ALT基准测试中，为开源系统取得了最先进的结果。我们还发布了MUSDB-ALT，这是第一个遵循Jam-ALT指南且人声干音公开可用的长文本歌词转录数据集。", "summary": "本研究系统地探讨了音乐源分离技术对使用最先进的ASR模型 Whisper 进行自动歌词转录（ALT）的影响。针对ALT中伴奏干扰的挑战，论文评估了 Whisper 在原始音频、分离人声和人声干音上的表现。研究提出了针对短文本转录的拼接方法和针对长文本转录的基于源分离的人声活动检测算法，两者均有效降低了词错误率。该方法在不进行训练或微调的情况下，在Jam-ALT长文本ALT基准测试中取得了开源系统的领先性能，并发布了新的长文本歌词转录数据集MUSDB-ALT。", "keywords": "自动歌词转录, 音乐源分离, Whisper, 词错误率, 数据集", "comments": "本文创新性地系统研究了音乐源分离技术在自动歌词转录（ALT）任务中与先进ASR模型 Whisper 的结合应用。其重要性在于，在不进行模型训练或微调的前提下，通过巧妙利用源分离技术，显著提升了ALT性能，特别是在处理复杂音乐伴奏下的长文本歌词转录时。这为ALT领域提供了一种高效且无需额外训练成本的解决方案，并贡献了一个宝贵的公开数据集。"}}
{"id": "2506.14949", "title": "From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?", "authors": ["Shadman Sakib", "Oishy Fatema Akhand", "Ajwad Abrar"], "summary": "While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.", "comment": "Accepted in 1st IEEE QPAIN 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14949v1", "AI": {"title_translation": "从对话到检查：大型语言模型能否辅助糖尿病预测？", "tldr": "本研究探讨了大型语言模型（LLMs）在糖尿病预测中的有效性，发现专有LLM表现优于开源LLM，且特定LLM（Gemma-2-27B）在某些指标上超越了传统机器学习模型，表明LLM在医疗预测任务中具有潜力。", "motivation": "机器学习（ML）和深度学习（DL）模型已被广泛应用于糖尿病预测，但大型语言模型（LLMs）在结构化数值数据方面的应用尚未得到充分探索。", "method": "本研究使用Pima Indian Diabetes Database (PIDD)数据集，采用零样本、单样本和三样本提示方法，测试了六种LLM（Gemma-2-27B、Mistral-7B、Llama-3.1-8B、Llama-3.2-2B、GPT-4o和Gemini Flash 2.0），并与三种传统机器学习模型（随机森林、逻辑回归和支持向量机）进行性能比较。评估指标包括准确率、精确率、召回率和F1分数。", "result": "结果显示，专有LLM的表现优于开源LLM，其中GPT-4o和Gemma-2-27B在少样本设置中准确率最高。值得注意的是，Gemma-2-27B在F1分数方面也优于传统ML模型。然而，仍存在提示策略间性能差异以及需要领域特定微调等问题。", "conclusion": "本研究表明，大型语言模型可用于医疗预测任务，并鼓励未来在提示工程和混合方法方面进行研究，以改进医疗预测。", "translation": "尽管机器学习（ML）和深度学习（DL）模型已广泛应用于糖尿病预测，但大型语言模型（LLMs）在结构化数值数据方面的应用仍未得到充分探索。在本研究中，我们测试了LLM在零样本、单样本和三样本提示方法下预测糖尿病的有效性。我们使用Pima Indian Diabetes Database (PIDD)进行了实证分析。我们评估了六个LLM，包括四个开源模型：Gemma-2-27B、Mistral-7B、Llama-3.1-8B和Llama-3.2-2B。我们还测试了两个专有模型：GPT-4o和Gemini Flash 2.0。此外，我们将其性能与三种传统机器学习模型（随机森林、逻辑回归和支持向量机SVM）进行了比较。我们使用准确率、精确率、召回率和F1分数作为评估指标。我们的结果表明，专有LLM的表现优于开源LLM，其中GPT-4o和Gemma-2-27B在少样本设置中达到了最高准确率。值得注意的是，Gemma-2-27B在F1分数方面也优于传统ML模型。然而，仍存在提示策略间性能差异以及需要领域特定微调等问题。本研究表明，LLM可用于医疗预测任务，并鼓励未来在提示工程和混合方法方面进行研究，以改进医疗预测。", "summary": "本研究探讨了大型语言模型（LLMs）在糖尿病预测中的应用潜力，特别是在结构化数值数据上的表现。研究人员使用Pima Indian Diabetes Database (PIDD)，通过零样本、单样本和三样本提示方法评估了六种LLMs（包括开源和专有模型），并与传统机器学习模型进行了对比。结果显示，专有LLMs通常优于开源LLMs，其中GPT-4o和Gemma-2-27B表现突出，后者在F1分数上甚至超越了传统ML模型。研究指出，尽管LLMs在医疗预测中具有前景，但仍面临提示策略性能差异和领域特定微调的需求。这项工作为未来通过提示工程和混合方法提升医疗预测的准确性奠定了基础。", "keywords": "大型语言模型, 糖尿病预测, 医疗预测, 少样本学习, 机器学习", "comments": "本研究的创新之处在于首次系统性地探索了LLMs在处理结构化数值数据进行医疗诊断预测方面的潜力，这与LLMs传统上处理文本数据的应用形成对比。其重要性在于为医疗领域引入了新的预测工具，并证明了某些LLMs（如Gemma-2-27B）在特定指标上甚至能超越传统ML模型。然而，研究也指出了局限性，即LLMs性能受提示策略影响较大，且仍需进行领域特定的微调，这提示了未来研究的方向。"}}
{"id": "2506.14788", "title": "Energy-consistent dynamic fracture phase field models: unilateral constraints and finite element simulations", "authors": ["Md Mamun Miah", "Ryuhei Wakida", "Masato Kimura"], "summary": "Phase field models have emerged as a powerful and flexible framework for\nsimulating complex interface-driven phenomena across a wide range of scientific\nand engineering applications. In fracture mechanics, the phase field\napproach--formulated as a gradient flow of the Griffith fracture energy with\nAmbrosio-Tortorelli regularization--has gained significant attention for its\nability to capture complex crack topologies. In this study, we propose a\ndynamic fracture phase field model (DF-PFM) based on the elastodynamic wave\nequation. We further extend this framework by incorporating a unilateral\ncontact condition, yielding a refined model suitable for simulating fault\nrupture under high pressure. For both models, we rigorously derive energy\ndissipation identities under mixed boundary conditions, ensuring energy\nconsistency of the formulations. To validate the proposed approach, we conduct\nnumerical experiments using linear implicit time discretization and finite\nelement methods. Our simulations demonstrate that the unilateral contact\ncondition is essential for accurately capturing shear-dominated crack\npropagation and preventing non-physical interpenetration, especially under\nhigh-compression loading scenarios relevant to seismic faulting.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14788v1", "AI": {"title_translation": "能量一致的动态断裂相场模型：单边约束和有限元模拟", "tldr": "本研究提出了一个基于弹动力学波动方程的动态断裂相场模型（DF-PFM），并引入单边接触条件以模拟高压下的断层破裂。通过严格推导能量耗散恒等式确保能量一致性，有限元模拟验证了单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理穿透的重要性。", "motivation": "相场模型在断裂力学中能够捕捉复杂的裂纹拓扑结构。本研究旨在提出一个动态断裂相场模型，并扩展其以模拟高压下的断层破裂，解决传统方法可能出现的非物理穿透问题。", "method": "本研究提出了一个基于弹动力学波动方程的动态断裂相场模型（DF-PFM）。通过引入单边接触条件进一步扩展了该模型，并严格推导了混合边界条件下的能量耗散恒等式以确保能量一致性。数值实验采用线性隐式时间离散化和有限元方法进行验证。", "result": "模拟结果表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理穿透至关重要，尤其是在与地震断层相关的、高压缩载荷情景下。", "conclusion": "单边接触条件对于动态断裂相场模型在模拟剪切主导的裂纹扩展和防止非物理穿透方面具有关键作用，特别是在高压缩载荷条件下。", "translation": "相场模型已成为模拟各种科学和工程应用中复杂界面驱动现象的强大而灵活的框架。在断裂力学中，相场方法——通过Ambrosio-Tortorelli正则化，作为Griffith断裂能量的梯度流——因其捕捉复杂裂纹拓扑结构的能力而受到广泛关注。在本研究中，我们提出了一个基于弹动力学波动方程的动态断裂相场模型（DF-PFM）。我们通过引入单边接触条件进一步扩展了该框架，从而得到一个适用于模拟高压下断层破裂的精细模型。对于这两种模型，我们严格推导了混合边界条件下的能量耗散恒等式，确保了公式的能量一致性。为了验证所提出的方法，我们使用线性隐式时间离散化和有限元方法进行了数值实验。我们的模拟表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理穿透至关重要，尤其是在与地震断层相关的高压缩载荷情景下。", "summary": "本研究提出了一个能量一致的动态断裂相场模型（DF-PFM），该模型基于弹动力学波动方程，并通过引入单边接触条件扩展，以更好地模拟高压下的断层破裂。通过严格的能量耗散恒等式推导确保了模型的能量一致性。有限元模拟验证了单边接触条件在准确捕捉剪切主导裂纹扩展和防止非物理穿透方面的关键作用，尤其是在高压缩载荷情景下。", "keywords": "相场模型, 动态断裂, 单边约束, 有限元, 能量一致性", "comments": "该论文的创新点在于提出了一个能量一致的动态断裂相场模型，并创造性地引入了单边接触条件，这对于模拟高压下如地震断层破裂等复杂现象至关重要。其通过严格推导能量耗散恒等式来确保模型能量守恒的严谨性也值得称赞。这项工作对于提升断裂力学中相场模型的应用范围和准确性具有重要意义，特别是在地质力学和材料科学领域。"}}
{"id": "2506.15075", "title": "CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets", "authors": ["Samhita Kuili", "Mohammadreza Amini", "Burak Kantarci"], "summary": "In the ever-expanding domain of 5G-NR wireless cellular networks,\nover-the-air jamming attacks are prevalent as security attacks, compromising\nthe quality of the received signal. We simulate a jamming environment by\nincorporating additive white Gaussian noise (AWGN) into the real-world In-phase\nand Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is\nexploited to implement a jamming detection over various characteristics such as\nheterogenous I/Q datasets; extracting relevant information on Synchronization\nSignal Blocks (SSBs), and fewer SSB observations with notable class imbalance.\nGiven the characteristics of datasets, balanced datasets are acquired by\nemploying a Conv1D conditional Wasserstein Generative Adversarial\nNetwork-Gradient Penalty(CWGAN-GP) on both majority and minority SSB\nobservations. Additionally, we compare the performance and detection ability of\nthe proposed CAE model on augmented datasets with benchmark models:\nConvolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder\n(CSAE). Despite the complexity of data heterogeneity involved across all\ndatasets, CAE depicts the robustness in detection performance of jammed signal\nby achieving average values of 97.33% precision, 91.33% recall, 94.08%\nF1-score, and 94.35% accuracy over CDAE and CSAE.", "comment": "6 pages, 5 figures, Accepted to IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15075v1", "AI": {"title_translation": "针对非独立同分布数据集中的5G-NR干扰检测，采用CWGAN-GP增强型CAE", "tldr": "本文提出了一种使用CWGAN-GP增强型卷积自编码器（CAE）的方法，用于在5G-NR非独立同分布数据集中检测干扰信号，并在性能上优于其他基准模型。", "motivation": "在5G-NR无线蜂窝网络中，空中干扰攻击普遍存在，会影响接收信号的质量，因此需要有效的干扰检测方法。", "method": "研究人员通过将加性高斯白噪声（AWGN）引入真实的同相/正交（I/Q）OFDM数据集来模拟干扰环境。他们利用卷积自编码器（CAE）进行干扰检测，尤其针对异构I/Q数据集、同步信号块（SSBs）信息提取以及类不平衡的少数SSB观测。为了解决数据集不平衡问题，他们对多数和少数SSB观测采用了Conv1D条件Wasserstein生成对抗网络-梯度惩罚（CWGAN-GP）来平衡数据集。最后，将所提出的CAE模型在增强型数据集上的性能和检测能力与卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE）等基准模型进行了比较。", "result": "尽管数据异构性复杂，CAE在干扰信号检测性能方面表现出鲁棒性，平均精度达到97.33%，召回率91.33%，F1分数94.08%，准确率94.35%，优于CDAE和CSAE。", "conclusion": "所提出的CWGAN-GP增强型CAE模型在处理5G-NR非独立同分布数据集中的干扰检测问题上表现出卓越的性能和鲁棒性，有效提升了干扰信号的检测能力。", "translation": "在不断扩展的5G-NR无线蜂窝网络领域中，空中干扰攻击作为安全攻击普遍存在，会损害接收信号的质量。我们通过将加性高斯白噪声（AWGN）引入真实世界的同相和正交（I/Q）OFDM数据集来模拟干扰环境。卷积自编码器（CAE）被用于实现干扰检测，涉及各种特征，例如异构I/Q数据集；提取同步信号块（SSBs）的相关信息；以及少数SSB观测的显著类不平衡。考虑到数据集的特性，通过在多数和少数SSB观测上采用Conv1D条件Wasserstein生成对抗网络-梯度惩罚（CWGAN-GP）获得了平衡数据集。此外，我们比较了所提出的CAE模型在增强型数据集上的性能和检测能力与基准模型：卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE）。尽管所有数据集都涉及复杂的数据异构性，CAE在干扰信号检测性能方面表现出鲁棒性，在CDAE和CSAE上实现了97.33%的平均精度、91.33%的召回率、94.08%的F1分数和94.35%的准确率。", "summary": "本文针对5G-NR网络中普遍存在的干扰攻击问题，提出了一种基于CWGAN-GP增强型卷积自编码器（CAE）的干扰检测方法。研究通过模拟干扰环境并利用CWGAN-GP对非独立同分布且存在类不平衡的I/Q OFDM数据集进行数据增强。实验结果表明，该CAE模型在检测干扰信号方面表现出优异的鲁棒性，其平均精度、召回率、F1分数和准确率均显著高于传统的卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE）。", "keywords": "5G-NR, 干扰检测, CWGAN-GP, 卷积自编码器, 非独立同分布数据集", "comments": "该论文的创新点在于结合CWGAN-GP进行数据增强，以解决5G-NR非独立同分布数据集中的类不平衡问题，并利用CAE进行干扰检测。这种方法提高了在复杂和异构数据环境下的检测性能，对于5G网络的安全性和信号质量保障具有重要意义。"}}
{"id": "2506.15172", "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "summary": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.", "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15172v1", "AI": {"title_translation": "敏捷/Scrum流程的先进方法：RetroAI++", "tldr": "RetroAI++是一个基于AI原型的工具，旨在自动化并改进敏捷/Scrum软件开发中的冲刺规划和回顾活动。", "motivation": "本研究旨在支持软件开发人员进行敏捷/Scrum软件开发中的冲刺规划和回顾分析，因为这些是项目管理的关键要素。", "method": "本文提出了一个名为RetroAI++的原型工具，该工具基于新兴的智能技术和AI洞察，旨在自动化和完善敏捷/Scrum开发项目在冲刺规划、开发和回顾阶段的多个流程。", "result": "RetroAI++原型工具能够自动化和完善敏捷/Scrum流程的实际应用，并为冲刺组织提供智能建议，为回顾反思提供有意义的见解。", "conclusion": "RetroAI++通过利用人工智能洞察，自动化并改进敏捷/Scrum流程中的冲刺规划和回顾活动，从而为软件开发人员提供智能支持。", "translation": "在敏捷/Scrum软件开发中，冲刺规划和回顾分析是项目管理的关键要素。我们工作的目标是在这些活动中支持软件开发人员。在本文中，我们提出了基于新兴智能技术的原型工具RetroAI++。在我们的RetroAI++原型中，我们旨在自动化和完善冲刺规划和回顾中敏捷/Scrum流程的实际应用。利用AI洞察，我们的原型旨在自动化和完善敏捷/Scrum开发项目在冲刺规划、开发和回顾阶段涉及的许多流程，为冲刺组织提供智能建议，并为回顾反思提供有意义的见解。", "summary": "本文介绍了一个名为RetroAI++的AI原型工具，旨在优化敏捷/Scrum软件开发中的冲刺规划和回顾流程。该工具利用智能技术和AI洞察，自动化并完善了这些关键的项目管理活动，提供智能建议以改进冲刺组织，并为回顾提供有价值的见解，从而提升开发效率和质量。", "keywords": "敏捷/Scrum, AI, RetroAI++, 冲刺规划, 回顾", "comments": "RetroAI++的创新之处在于将AI技术应用于敏捷/Scrum流程的核心环节（冲刺规划和回顾），以实现自动化和智能辅助。这对于提高软件开发效率、减少人为错误和提供更深入的洞察具有重要意义。该原型工具的提出，显示了AI在项目管理领域应用的潜力。"}}
{"id": "2506.14829", "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities", "authors": ["Aditya Majumdar", "Wenbo Zhang", "Kashvi Prawal", "Amulya Yadav"], "summary": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects\nfocus on harnessing AI to address societal issues in areas such as healthcare,\nsocial justice, etc. Unfortunately, despite growing interest in AI4SI,\nachieving tangible, on-the-ground impact remains a significant challenge. For\nexample, identifying and engaging motivated collaborators who are willing to\nco-design and deploy AI based solutions in real-world settings is often\ndifficult. Even when such partnerships are established, many AI4SI projects\n\"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to\ntransition to at-scale production-level solutions. Furthermore, the unique\nchallenges faced by AI4SI researchers are not always fully recognized within\nthe broader AI community, where such work is sometimes viewed as primarily\napplied and not aligning with the traditional criteria for novelty emphasized\nin core AI venues. This paper attempts to shine a light on the diverse\nchallenges faced in AI4SI research by diagnosing a multitude of factors that\nprevent AI4SI partnerships from achieving real-world impact on the ground.\nDrawing on semi-structured interviews with six leading AI4SI researchers -\ncomplemented by the authors' own lived experiences in conducting AI4SI research\n- this paper attempts to understand the day-to-day difficulties faced in\ndeveloping and deploying socially impactful AI solutions. Through thematic\nanalysis, we identify structural and organizational, communication,\ncollaboration, and operational challenges as key barriers to deployment. While\nthere are no easy fixes, we synthesize best practices and actionable strategies\ndrawn from these interviews and our own work in this space. In doing so, we\nhope this paper serves as a practical reference guide for AI4SI researchers and\npartner organizations seeking to engage more effectively in socially impactful\nAI collaborations.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14829v1", "AI": {"title_translation": "AI社会影响研究中实现影响力的难度：挑战与机遇的基层视角", "tldr": "AI社会影响力研究难以在实际中产生影响，本文通过访谈识别了主要挑战并提出了实践策略。", "motivation": "尽管AI社会影响力（AI4SI）项目日益受到关注，但其在实际中产生切实影响仍面临巨大挑战，例如难以找到合作者、项目停留在概念验证阶段，以及AI社区对AI4SI研究的认可度不足。本文旨在揭示AI4SI研究中面临的各种挑战。", "method": "本文通过对六位领先的AI4SI研究人员进行半结构化访谈，并结合作者自身进行AI4SI研究的经验，通过主题分析法识别了部署中的结构和组织、沟通、协作和操作挑战。", "result": "识别出AI4SI项目部署的主要障碍是结构和组织、沟通、协作以及操作方面的挑战。", "conclusion": "尽管没有简单的解决方案，本文综合了从访谈和自身工作中得出的最佳实践和可操作策略，旨在为AI4SI研究人员和合作组织提供实用参考指南，以更有效地参与具有社会影响力的AI合作。", "translation": "在努力应对联合国可持续发展目标（UN SDGs）的过程中，人工智能促进社会影响（AI4SI）项目致力于利用AI来解决医疗、社会正义等领域的社会问题。然而，尽管AI4SI的兴趣日益增长，但在实地实现切实的、落地的影响仍然是一个重大挑战。例如，识别和吸引有动力、愿意在现实世界环境中共同设计和部署基于AI解决方案的合作者往往很困难。即使建立了这种伙伴关系，许多AI4SI项目也“未能”超越概念验证阶段，因此无法过渡到大规模的生产级解决方案。此外，AI4SI研究人员面临的独特挑战在更广泛的AI社区中并未总是得到充分认可，在这些社区中，此类工作有时被视为主要是应用性的，不符合核心AI领域所强调的新颖性传统标准。本文试图通过诊断阻止AI4SI伙伴关系在实地实现实际影响的多种因素，来揭示AI4SI研究中面临的各种挑战。本文借鉴了对六位领先AI4SI研究人员的半结构化访谈——并辅以作者自身在进行AI4SI研究中的亲身经历——试图理解在开发和部署具有社会影响力的AI解决方案中面临的日常困难。通过主题分析，我们识别出结构和组织、沟通、协作和操作方面的挑战是部署的主要障碍。虽然没有简单的解决方案，但我们综合了从这些访谈和我们在此领域自身工作中得出的最佳实践和可操作策略。通过这样做，我们希望本文能为AI4SI研究人员和寻求更有效参与社会影响力AI合作的伙伴组织提供实用参考指南。", "summary": "本文探讨了人工智能促进社会影响（AI4SI）研究在实现实际影响力方面面临的挑战。通过对AI4SI研究人员的访谈和作者的经验，识别了结构、沟通、协作和操作方面的关键障碍，并提出了克服这些挑战的最佳实践和实用策略，旨在为AI4SI合作提供指导。", "keywords": "AI社会影响, 影响力, 挑战, 实践策略, 协作", "comments": "本文深入分析了AI4SI领域面临的实际落地困境，其价值在于揭示了技术之外的深层问题，如合作模式、社区认可和项目管理等。通过结合访谈和亲身经验，提供了宝贵的基层视角。其提出的实践策略对于推动AI4SI项目的实际影响具有重要的指导意义，有助于弥合AI研究与社会需求之间的差距。"}}
{"id": "2506.15009", "title": "Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots", "authors": ["Jinjie Li", "Jiaxuan Li", "Kotaro Kaneko", "Liming Shu", "Moju Zhao"], "summary": "Omnidirectional aerial robots offer full 6-DoF independent control over\nposition and orientation, making them popular for aerial manipulation. Although\nadvancements in robotic autonomy, operating by human remains essential in\ncomplex aerial environments. Existing teleoperation approaches for multirotors\nfail to fully leverage the additional DoFs provided by omnidirectional\nrotation. Additionally, the dexterity of human fingers should be exploited for\nmore engaged interaction. In this work, we propose an aerial teleoperation\nsystem that brings the omnidirectionality of human hands into the unbounded\naerial workspace. Our system includes two motion-tracking marker sets -- one on\nthe shoulder and one on the hand -- along with a data glove to capture hand\ngestures. Using these inputs, we design four interaction modes for different\ntasks, including Spherical Mode and Cartesian Mode for long-range moving as\nwell as Operation Mode and Locking Mode for precise manipulation, where the\nhand gestures are utilized for seamless mode switching. We evaluate our system\non a valve-turning task in real world, demonstrating how each mode contributes\nto effective aerial manipulation. This interaction framework bridges human\ndexterity with aerial robotics, paving the way for enhanced teleoperated aerial\nmanipulation in unstructured environments.", "comment": "7 pages, 9 figures. This work has been accepted to IROS 2025. The\n  video will be released soon", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15009v1", "AI": {"title_translation": "六自由度手部遥操作用于全向空中机器人", "tldr": "本文提出了一种基于手部追踪和数据手套的全向空中机器人遥操作系统，通过四种交互模式实现远程移动和精确操作，并在阀门转动任务中进行了验证。", "motivation": "全向空中机器人具备六自由度独立控制能力，适用于空中操作。然而，现有遥操作方法未能充分利用其额外的自由度，也未充分利用人类手指的灵巧性。因此，需要一种能将人类手部全向性引入空中工作空间的新型遥操作系统。", "method": "本研究提出了一种空中遥操作系统，利用肩部和手部的两个运动追踪标记集以及一个数据手套来捕捉手势。系统设计了四种交互模式以适应不同任务：用于长距离移动的球形模式和笛卡尔模式，以及用于精确操作的操作模式和锁定模式，手势用于无缝模式切换。", "result": "系统在现实世界的阀门转动任务中进行了评估，结果表明每种模式都有助于有效的空中操作。", "conclusion": "该交互框架将人类的灵巧性与空中机器人技术相结合，为在非结构化环境中增强遥控空中操作铺平了道路。", "translation": "全向空中机器人提供对位置和姿态的完全六自由度独立控制，使其在空中操作中广受欢迎。尽管机器人自主性取得了进步，但在复杂的空中环境中，人类操作仍然至关重要。现有的多旋翼遥操作方法未能充分利用全向旋转提供的额外自由度。此外，应利用人类手指的灵巧性进行更深入的交互。在这项工作中，我们提出了一种空中遥操作系统，将人类手的全向性带入无限的空中工作空间。我们的系统包括两组运动追踪标记——一组在肩部，一组在手部——以及一个数据手套来捕捉手势。利用这些输入，我们为不同任务设计了四种交互模式，包括用于长距离移动的球形模式和笛卡尔模式，以及用于精确操作的操作模式和锁定模式，其中手势用于无缝模式切换。我们在现实世界中的阀门转动任务中评估了我们的系统，展示了每种模式如何有助于有效的空中操作。这个交互框架将人类的灵巧性与空中机器人技术相结合，为在非结构化环境中增强遥控空中操作铺平了道路。", "summary": "本文提出了一种新颖的六自由度手部遥操作系统，专为全向空中机器人设计，以解决现有系统未能充分利用其额外自由度和人类手部灵巧性的问题。该系统结合了肩部和手部运动追踪标记以及数据手套，捕捉手部姿态和手势。为适应不同任务，系统设计了四种交互模式（球形、笛卡尔、操作和锁定模式），并通过手势实现模式切换。在阀门转动任务中的评估证明了该系统在实现有效空中操作方面的潜力，为非结构化环境下的远程操作提供了新的范例。", "keywords": "遥操作, 全向空中机器人, 六自由度, 手部追踪, 人机交互", "comments": "该论文的创新点在于提出了一个将人类手部全向性与全向空中机器人相结合的遥操作框架。通过结合运动追踪和数据手套，并设计多模式交互，它有效利用了人类的自然灵巧性，解决了传统遥操作的局限性。这对于复杂、非结构化环境中的空中操作具有重要意义。"}}
{"id": "2506.15176", "title": "In-Context Learning for Gradient-Free Receiver Adaptation: Principles, Applications, and Theory", "authors": ["Matteo Zecchin", "Tomer Raviv", "Dileep Kalathil", "Krishna Narayanan", "Nir Shlezinger", "Osvaldo Simeone"], "summary": "In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15176v1", "AI": {"title_translation": "上下文学习在免梯度接收机自适应中的应用：原理、应用与理论", "tldr": "本文提出了基于上下文学习（ICL）的免梯度自适应技术，用于无线接收机在无需在线再训练的情况下进行实时自适应，并探讨了其原理、架构和在蜂窝自由大规模MIMO网络中的应用。", "motivation": "现有的深度学习无线接收机自适应策略（如联合训练、超网络方法和元学习）要么灵活性有限，要么需要通过梯度下降进行显式优化，这限制了它们在动态信道环境中的实时应用。", "method": "本文提出了基于上下文学习（ICL）的免梯度自适应技术。具体方法包括：回顾基于Transformer模型和结构化状态空间模型（SSM）的ICL架构框架；深入探讨序列模型如何有效地从上下文信息中学习自适应的理论；将ICL应用于蜂窝自由大规模MIMO网络，并提供理论分析和经验证据。", "result": "研究结果表明，上下文学习（ICL）是一种基于导频信号和辅助上下文信息进行实时接收机自适应的原理性且高效的方法。", "conclusion": "上下文学习（ICL）为无线接收机提供了一种无需在线再训练即可实现实时自适应的有效途径。", "translation": "近年来，深度学习促进了无线接收机的创建，使其能够在传统基于模型设计面临挑战的条件下有效运行。利用可编程硬件架构，基于深度学习的接收机提供了动态适应不同信道环境的潜力。然而，当前的自适应策略，包括联合训练、基于超网络的方法和元学习，要么表现出有限的灵活性，要么需要通过梯度下降进行显式优化。本文提出了植根于新兴上下文学习（ICL）范式的免梯度自适应技术。我们回顾了基于Transformer模型和结构化状态空间模型（SSM）的ICL架构框架，以及关于序列模型如何有效地从上下文信息中学习自适应的理论见解。此外，我们探讨了ICL在蜂窝自由大规模MIMO网络中的应用，提供了理论分析和经验证据。我们的研究结果表明，ICL代表了一种使用导频信号和辅助上下文信息进行实时接收机自适应的原则性且高效的方法——无需在线再训练。", "summary": "本文提出了一种基于上下文学习（ICL）的免梯度自适应技术，旨在解决现有深度学习无线接收机自适应方法在灵活性和实时性方面的局限性。研究回顾了基于Transformer和结构化状态空间模型（SSM）的ICL架构，并提供了序列模型如何通过上下文信息进行自适应的理论见解。通过在蜂窝自由大规模MIMO网络中的应用验证，结果表明ICL是一种无需在线再训练，即可利用导频信号和上下文信息实现高效实时接收机自适应的有效方法。", "keywords": "上下文学习, 免梯度自适应, 无线接收机, Transformer, 结构化状态空间模型", "comments": "该论文的创新点在于将上下文学习（ICL）引入无线接收机的自适应领域，并提出了免梯度自适应的解决方案。这解决了传统深度学习方法需要显式梯度优化或在线再训练的痛点，对于实现无线通信中接收机的实时、高效自适应具有重要意义。特别是在动态变化的信道环境中，这种无需在线训练的能力极大地提升了系统的实用性。"}}
{"id": "2506.15613", "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation", "authors": ["Miryeong Kwon", "Donghyun Gouk", "Junhyeok Jang", "Jinwoo Baek", "Hyunwoo You", "Sangyoon Ji", "Hongjoo Jung", "Junseok Moon", "Seungkwan Kang", "Seungjun Lee", "Myoungsoo Jung"], "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15613v1", "AI": {"title_translation": "从块到字节：利用CXL内存协议和指令注解改造PCIe SSD", "tldr": "该论文探索了如何利用CXL协议和指令注解将PCIe SSDs转换为可扩展的字节寻址工作内存，并通过原型和模拟验证了其性能优势。", "motivation": "该研究旨在解决将基于块的存储适应CXL以内存为中心的模型所面临的挑战，并强调可缓存性作为关键促成因素，从而将PCIe SSD转换为可扩展、字节可寻址的工作内存。", "method": "作者通过在定制的FPGA平台上原型化CXL-SSD，并提出了确定性和可缓冲性两种注解机制来增强性能，同时保留数据持久性。他们通过基于模拟的评估来验证其方法。", "result": "模拟评估显示，CXL-SSD的性能比基于PCIe的内存扩展器高出10.9倍，并通过注解增强进一步将延迟降低了5.4倍。在高局部性工作负载中，由于高效的片上缓存，CXL-SSD的性能接近DRAM。", "conclusion": "这项工作强调了将块存储集成到CXL生态系统中的可行性，并为未来的内存-存储融合奠定了基础。", "translation": "本文探讨了计算快速连接（CXL）如何将基于PCIe的块存储转换为可扩展的字节寻址工作内存。我们通过强调可缓存性作为关键促成因素，并提倡使用Type 3端点设备（称之为CXL-SSD），来解决块存储适应CXL以内存为中心的模型所面临的挑战。为了验证我们的方法，我们在定制的FPGA平台上原型化了一个CXL-SSD，并提出了确定性和可缓冲性两种注解机制，以在保持数据持久性的同时提高性能。我们基于模拟的评估表明，CXL-SSD比基于PCIe的内存扩展器性能提高了10.9倍，并通过注解增强进一步将延迟降低了5.4倍。在高局部性工作负载中，由于高效的片上缓存，CXL-SSD的性能接近DRAM。这项工作突出了将块存储集成到CXL生态系统中的可行性，并为未来的内存-存储融合提供了基础。", "summary": "本文探讨了如何通过CXL内存协议和指令注解将PCIe SSDs改造为可扩展的字节寻址工作内存。研究人员在FPGA平台上原型化了CXL-SSD，并引入了确定性和可缓冲性注解机制以提升性能和保持数据持久性。模拟结果表明，CXL-SSD的性能显著优于传统PCIe内存扩展器，且通过注解可进一步降低延迟，在高局部性场景下接近DRAM性能。这证明了块存储与CXL生态系统融合的可行性，为内存-存储融合奠定了基础。", "keywords": "CXL, PCIe SSD, 字节寻址, 内存-存储融合, 指令注解", "comments": "这篇论文的创新点在于提出了将传统块存储（PCIe SSD）通过CXL协议转换为字节寻址内存的新范式，并引入了“确定性”和“可缓冲性”的指令注解机制来优化性能。其重要性在于为未来内存与存储的融合提供了切实可行的路径，有望显著提升数据中心和高性能计算的效率。该研究通过FPGA原型和模拟评估，验证了其方法的有效性和显著的性能提升，为构建更高效的异构内存系统提供了基础。"}}
{"id": "2506.15418", "title": "RISC-V for HPC: An update of where we are and main action points", "authors": ["Nick Brown"], "summary": "This extended abstract is submitted on behalf of the RISC-V HPC SIG who have\nbeen undertaking an analysis to explore the current state and limitations of\nthe RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has\nbeen great progress made in recent years, we also highlight limitations and\nwhere effort should be focussed.", "comment": "Extended abstract accepted to the RISC-V Summit Europe 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15418v1", "AI": {"title_translation": "RISC-V 在 HPC 中的应用：现状更新和主要行动点", "tldr": "RISC-V HPC SIG 分析了 RISC-V 生态系统在高性能计算领域的现状和局限性，并指出了未来的重点工作。", "motivation": "探索 RISC-V 生态系统在高性能计算 (HPC) 领域的当前状态和局限性。", "method": "RISC-V HPC SIG 进行了一项分析。", "result": "识别了 RISC-V 在 HPC 领域取得的进展，并强调了其局限性以及需要重点关注的领域。", "conclusion": "RISC-V 在 HPC 领域取得了显著进展，但仍存在局限性，需要明确的努力方向。", "translation": "本扩展摘要代表 RISC-V HPC SIG 提交，该组织一直在进行一项分析，以探索 RISC-V 生态系统在 HPC 领域的当前状态和局限性。虽然近年来取得了巨大进展值得庆祝，但我们也强调了局限性以及应重点关注的领域。", "summary": "本文是 RISC-V HPC SIG 的一份扩展摘要，旨在评估 RISC-V 生态系统在高性能计算 (HPC) 领域的现状和存在的局限性。该分析不仅肯定了近期取得的显著进展，也明确指出了目前面临的挑战以及未来需要集中精力解决的关键问题。", "keywords": "RISC-V, HPC, 生态系统, 局限性, 行动点", "comments": "这篇论文的创新点在于它提供了一个关于 RISC-V 在 HPC 领域现状的最新分析，并明确指出了未来的行动方向，这对于推动 RISC-V 在高性能计算领域的进一步发展具有重要指导意义。"}}
{"id": "2506.14827", "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning", "authors": ["Yifeng Gao", "Yifan Ding", "Hongyu Su", "Juncheng Li", "Yunhan Zhao", "Lin Luo", "Zixing Chen", "Li Wang", "Xin Wang", "Yixu Wang", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As AI-generated video becomes increasingly pervasive across media platforms,\nthe ability to reliably distinguish synthetic content from authentic footage\nhas become both urgent and essential. Existing approaches have primarily\ntreated this challenge as a binary classification task, offering limited\ninsight into where or why a model identifies a video as AI-generated. However,\nthe core challenge extends beyond simply detecting subtle artifacts; it\nrequires providing fine-grained, persuasive evidence that can convince auditors\nand end-users alike. To address this critical gap, we introduce DAVID-X, the\nfirst dataset to pair AI-generated videos with detailed defect-level,\ntemporal-spatial annotations and written rationales. Leveraging these rich\nannotations, we present DAVID-XR1, a video-language model designed to deliver\nan interpretable chain of visual reasoning-including defect categorization,\ntemporal-spatial localization, and natural language explanations. This approach\nfundamentally transforms AI-generated video detection from an opaque black-box\ndecision into a transparent and verifiable diagnostic process. We demonstrate\nthat a general-purpose backbone, fine-tuned on our compact dataset and enhanced\nwith chain-of-thought distillation, achieves strong generalization across a\nvariety of generators and generation modes. Our results highlight the promise\nof explainable detection methods for trustworthy identification of AI-generated\nvideo content.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14827v1", "AI": {"title_translation": "DAVID-XR1：通过可解释推理检测AI生成视频", "tldr": "鉴于AI生成视频的日益普及，可靠区分合成内容和真实素材变得至关重要。现有方法多为二元分类，缺乏解释性。本文引入DAVID-X数据集和DAVID-XR1模型，旨在提供可解释的视觉推理（包括缺陷分类、时空定位和自然语言解释），将检测从不透明的黑箱决策转变为透明可验证的诊断过程，并展示了其强大的泛化能力。", "motivation": "随着AI生成视频在媒体平台上的日益普及，可靠区分合成内容和真实素材变得迫切且必要。现有方法主要将此挑战视为二元分类任务，对模型为何识别视频为AI生成提供有限的洞察。然而，核心挑战不仅限于检测细微缺陷，还需要提供细粒度、有说服力的证据，以说服审计员和最终用户。", "method": "为解决现有方法的不足，本文引入了DAVID-X数据集，这是第一个将AI生成视频与详细的缺陷级别、时空注释和书面理由配对的数据集。利用这些丰富的注释，本文提出了DAVID-XR1，一个视频-语言模型，旨在提供可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。该方法通过在紧凑数据集上微调通用骨干网络，并辅以思维链蒸馏，实现了强大的泛化能力。", "result": "该模型在紧凑数据集上进行微调并增强了思维链蒸馏后，在各种生成器和生成模式下均表现出强大的泛化能力。", "conclusion": "可解释的检测方法对于可信地识别AI生成视频内容具有广阔前景。", "translation": "随着AI生成视频在媒体平台上的日益普及，可靠区分合成内容和真实素材变得迫切且必要。现有方法主要将此挑战视为二元分类任务，对模型为何识别视频为AI生成提供有限的洞察。然而，核心挑战不仅限于检测细微缺陷；它需要提供细粒度、有说服力的证据，以说服审计员和最终用户。为了解决这一关键空白，我们引入了DAVID-X，这是第一个将AI生成视频与详细的缺陷级别、时空注释和书面理由配对的数据集。利用这些丰富的注释，我们提出了DAVID-XR1，一个视频-语言模型，旨在提供可解释的视觉推理链——包括缺陷分类、时空定位和自然语言解释。这种方法从根本上将AI生成视频检测从不透明的黑箱决策转变为透明和可验证的诊断过程。我们证明，一个通用骨干网络，在我们的紧凑数据集上进行微调并增强了思维链蒸馏，在各种生成器和生成模式下均实现了强大的泛化能力。我们的结果突出了可解释检测方法在可信识别AI生成视频内容方面的潜力。", "summary": "本文针对AI生成视频检测中缺乏解释性的问题，提出了DAVID-X数据集和DAVID-XR1视频-语言模型。DAVID-X数据集首次为AI生成视频提供了详细的缺陷级别、时空注释和书面理由。基于此，DAVID-XR1模型能够提供可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释，从而将AI生成视频检测从不透明的黑箱决策转变为透明且可验证的诊断过程。实验证明，该模型在紧凑数据集上微调后，通过思维链蒸馏增强，在多种生成器和生成模式下均表现出强大的泛化能力，凸显了可解释检测方法在可信识别AI生成视频内容方面的潜力。", "keywords": "AI生成视频检测, 可解释人工智能, 视频语言模型, 深度伪造, 数据集", "comments": "本文的创新之处在于将AI生成视频检测从传统的二元分类提升到可解释的诊断过程。通过引入独特的DAVID-X数据集，该数据集包含详细的缺陷注释和理由，为训练能够提供透明推理的AI模型奠定了基础。DAVID-XR1模型利用视频-语言理解，不仅能检测出AI生成内容，还能解释检测的原因和位置，这对于建立用户信任和审计可信度至关重要。这项工作显著提升了AI生成视频检测的实用性和可靠性，为未来可解释的媒体取证研究开辟了道路。"}}
{"id": "2506.15225", "title": "Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels", "authors": ["Jiahao You", "Ziye Jia", "Chao Dong", "Qihui Wu", "Zhu Han"], "summary": "The computation demands from the maritime Internet of Things (MIoT) increase\nrapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels\nbased multi-access edge computing (MEC) can fulfill these MIoT requirements.\nHowever, the uncertain maritime tasks present significant challenges of\ninefficient computation offloading and resource allocation. In this paper, we\nfocus on the maritime computation offloading and resource allocation through\nthe cooperation of UAVs and vessels, with consideration of uncertain tasks.\nSpecifically, we propose a cooperative MEC framework for computation offloading\nand resource allocation, including MIoT devices, UAVs and vessels. Then, we\nformulate the optimization problem to minimize the total execution time. As for\nthe uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the\nunpredictable task arrivals and varying computational resource availability. By\nconverting the long-term constraints into short-term constraints, we obtain a\nset of small-scale optimization problems. Further, considering the\nheterogeneity of actions and resources of UAVs and vessels, we reformulate the\nsmall-scale optimization problem into a Markov game (MG). Moreover, a\nheterogeneous-agent soft actor-critic is proposed to sequentially update\nvarious neural networks and effectively solve the MG problem. Finally,\nsimulations are conducted to verify the effectiveness in addressing\ncomputational offloading and resource allocation.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15225v1", "AI": {"title_translation": "通过无人机和船只合作的不确定海上MEC联合计算卸载和资源分配", "tldr": "该研究提出了一种基于无人机和船只合作的MEC框架，利用Lyapunov优化和异构智能体软Actor-Critic解决不确定海上MIoT任务的计算卸载和资源分配问题，以最小化总执行时间。", "motivation": "海上物联网（MIoT）的计算需求迅速增长，而基于无人机和船只的多接入边缘计算（MEC）可以满足这些需求。然而，不确定的海上任务给低效的计算卸载和资源分配带来了挑战。", "method": "提出一个协作MEC框架，包括MIoT设备、无人机和船只。将优化问题建模为最小化总执行时间。利用Lyapunov优化处理不可预测的任务到达和变化的计算资源可用性，将长期约束转换为短期约束，得到一系列小规模优化问题。考虑到无人机和船只行动和资源的异构性，将小规模优化问题重构为马尔可夫博弈（MG）。提出一种异构智能体软Actor-Critic（HAC）方法来解决MG问题。", "result": "模拟验证了所提方法在解决计算卸载和资源分配方面的有效性。", "conclusion": "该论文成功地通过无人机和船只的合作，利用Lyapunov优化和异构智能体软Actor-Critic解决了不确定海上MEC环境下的计算卸载和资源分配问题，并有效最小化了总执行时间。", "translation": "近年来，海上物联网（MIoT）的计算需求迅速增长，而基于无人机（UAV）和船只的多接入边缘计算（MEC）可以满足这些MIoT需求。然而，不确定的海上任务给低效的计算卸载和资源分配带来了重大挑战。在本文中，我们关注通过无人机和船只合作进行的海上计算卸载和资源分配，并考虑不确定任务。具体来说，我们提出了一个用于计算卸载和资源分配的协作MEC框架，包括MIoT设备、无人机和船只。然后，我们制定了优化问题以最小化总执行时间。对于不确定的MIoT任务，我们利用Lyapunov优化来处理不可预测的任务到达和变化的计算资源可用性。通过将长期约束转换为短期约束，我们获得了一系列小规模优化问题。此外，考虑到无人机和船只行动和资源的异构性，我们将小规模优化问题重构为马尔可夫博弈（MG）。此外，提出了一种异构智能体软Actor-Critic（HAC）方法来顺序更新各种神经网络并有效解决MG问题。最后，通过仿真验证了其在解决计算卸载和资源分配方面的有效性。", "summary": "本文针对海上物联网（MIoT）日益增长的计算需求和不确定任务带来的挑战，提出了一种基于无人机和船只合作的MEC框架，以实现计算卸载和资源分配。该研究将问题建模为最小化总执行时间的优化问题，并利用Lyapunov优化处理任务不确定性。进一步，考虑到异构性，将问题重构为马尔可夫博弈，并提出异构智能体软Actor-Critic算法进行求解。仿真结果验证了所提方法的有效性。", "keywords": "海上MEC, 计算卸载, 资源分配, 无人机, 马尔可夫博弈", "comments": "本文的创新之处在于将无人机和船只的合作引入海上MEC环境，并有效结合Lyapunov优化处理任务不确定性，以及利用异构智能体软Actor-Critic解决复杂的多智能体优化问题。这种综合方法为不确定海上环境下的高效计算卸载和资源分配提供了新的解决方案，具有重要的实际应用价值。"}}
{"id": "2506.14786", "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series", "authors": ["Haobo Li", "Eunseo Jung", "Zixin Chen", "Zhaowei Wang", "Yueya Wang", "Huamin Qu", "Alexis Kai Hon Lau"], "summary": "Multimodal time series forecasting is foundational in various fields, such as\nutilizing satellite imagery and numerical data for predicting typhoons in\nclimate science. However, existing multimodal approaches primarily focus on\nutilizing text data to help time series forecasting, leaving the visual data in\nexisting time series datasets untouched. Furthermore, it is challenging for\nmodels to effectively capture the physical information embedded in visual data,\nsuch as satellite imagery's temporal and geospatial context, which extends\nbeyond images themselves. To address this gap, we propose physics-informed\npositional encoding (PIPE), a lightweight method that embeds physical\ninformation into vision language models (VLMs). PIPE introduces two key\ninnovations: (1) a physics-informed positional indexing scheme for mapping\nphysics to positional IDs, and (2) a variant-frequency positional encoding\nmechanism for encoding frequency information of physical variables and\nsequential order of tokens within the embedding space. By preserving both the\nphysical information and sequential order information, PIPE significantly\nimproves multimodal alignment and forecasting accuracy. Through the experiments\non the most representative and the largest open-sourced satellite image\ndataset, PIPE achieves state-of-the-art performance in both deep learning\nforecasting and climate domain methods, demonstrating superiority across\nbenchmarks, including a 12% improvement in typhoon intensity forecasting over\nprior works. Our code is provided in the supplementary material.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14786v1", "AI": {"title_translation": "PIPE：物理信息位置编码用于卫星图像和时间序列对齐", "tldr": "PIPE是一种轻量级方法，通过将物理信息编码到视觉语言模型中，显著改善了卫星图像和时间序列的多模态对齐和预测准确性，并在台风强度预测中取得了最先进的性能。", "motivation": "现有的多模态时间序列预测方法主要侧重于文本数据，忽视了视觉数据（如卫星图像）中嵌入的物理信息（如时间、地理空间上下文），导致模型难以有效捕获这些关键信息。", "method": "本文提出了物理信息位置编码（PIPE），这是一种轻量级方法，用于将物理信息嵌入到视觉语言模型（VLMs）中。PIPE包含两项关键创新：1) 一种物理信息位置索引方案，用于将物理信息映射到位置ID；2) 一种变频位置编码机制，用于编码物理变量的频率信息和嵌入空间中token的顺序。", "result": "在最具代表性和最大的开源卫星图像数据集上的实验表明，PIPE在深度学习预测和气候领域方法中均取得了最先进的性能。它在各项基准测试中表现出优越性，包括在台风强度预测方面比先前工作提高了12%。", "conclusion": "PIPE通过保留物理信息和顺序信息，显著改善了多模态对齐和预测准确性，并在卫星图像时间序列预测任务中实现了最先进的性能和卓越的基准表现。", "translation": "多模态时间序列预测在各个领域都是基础，例如在气候科学中利用卫星图像和数值数据预测台风。然而，现有的多模态方法主要侧重于利用文本数据来辅助时间序列预测，而忽略了现有时间序列数据集中未被触及的视觉数据。此外，模型难以有效捕获视觉数据中嵌入的物理信息，例如卫星图像的时间和地理空间上下文，这些信息超越了图像本身。为了弥补这一空白，我们提出了物理信息位置编码（PIPE），这是一种将物理信息嵌入到视觉语言模型（VLM）中的轻量级方法。PIPE引入了两项关键创新：（1）一种物理信息位置索引方案，用于将物理信息映射到位置ID；（2）一种变频位置编码机制，用于编码物理变量的频率信息和嵌入空间中token的顺序。通过保留物理信息和顺序信息，PIPE显著提高了多模态对齐和预测准确性。通过在最具代表性和最大的开源卫星图像数据集上的实验，PIPE在深度学习预测和气候领域方法中均取得了最先进的性能，在各项基准测试中表现出优越性，包括在台风强度预测方面比先前工作提高了12%。我们的代码在补充材料中提供。", "summary": "本文提出了一种名为PIPE的轻量级物理信息位置编码方法，旨在解决现有模型在多模态时间序列预测中忽视视觉数据中物理信息的问题。PIPE通过物理信息位置索引方案和变频位置编码机制，将物理信息嵌入到视觉语言模型中，从而有效捕获卫星图像等视觉数据中的时间、地理空间和物理上下文。实验结果表明，PIPE在卫星图像数据集上显著提高了多模态对齐和预测准确性，并在台风强度预测方面取得了12%的提升，实现了最先进的性能。", "keywords": "物理信息位置编码, 卫星图像, 时间序列, 多模态预测, 视觉语言模型", "comments": "PIPE通过将物理信息（如时间、地理空间上下文）创新性地编码到位置编码中，有效解决了视觉数据中物理信息难以被现有模型利用的挑战。这不仅是对传统位置编码方法的扩展，也为多模态时间序列预测，特别是涉及卫星图像和物理现象（如台风预测）的领域，提供了显著的性能提升和新的研究方向。"}}
{"id": "2506.15572", "title": "Misinformation by Omission: The Need for More Environmental Transparency in AI", "authors": ["Sasha Luccioni", "Boris Gamazaychikov", "Theo Alves da Costa", "Emma Strubell"], "summary": "In recent years, Artificial Intelligence (AI) models have grown in size and\ncomplexity, driving greater demand for computational power and natural\nresources. In parallel to this trend, transparency around the costs and impacts\nof these models has decreased, meaning that the users of these technologies\nhave little to no information about their resource demands and subsequent\nimpacts on the environment. Despite this dearth of adequate data, escalating\ndemand for figures quantifying AI's environmental impacts has led to numerous\ninstances of misinformation evolving from inaccurate or de-contextualized\nbest-effort estimates of greenhouse gas emissions. In this article, we explore\npervasive myths and misconceptions shaping public understanding of AI's\nenvironmental impacts, tracing their origins and their spread in both the media\nand scientific publications. We discuss the importance of data transparency in\nclarifying misconceptions and mitigating these harms, and conclude with a set\nof recommendations for how AI developers and policymakers can leverage this\ninformation to mitigate negative impacts in the future.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15572v1", "AI": {"title_translation": "AI中的遗漏信息：环境透明度的必要性", "tldr": "AI模型增长导致资源消耗增加，但相关透明度下降，引发环境影响的错误信息。本文探讨这些迷思并强调数据透明度的重要性，并提出建议。", "motivation": "近年来，人工智能（AI）模型的规模和复杂性不断增长，导致对计算能力和自然资源的需求剧增。然而，这些模型的成本和影响透明度却有所下降，使得用户对AI的资源需求及其对环境的影响知之甚少。这种数据匮乏导致了关于AI环境影响的错误信息泛滥。", "method": "本文探讨了影响公众对AI环境影响理解的普遍迷思和误解，追溯了它们的起源及其在媒体和科学出版物中的传播。文章还讨论了数据透明度在澄清误解和减轻危害方面的重要性。", "result": "文章揭示并探讨了关于AI环境影响的普遍迷思和误解，追溯了它们的起源和传播途径。强调了数据透明度对于纠正这些错误信息和减轻负面影响的关键作用。", "conclusion": "文章最后提出了一系列建议，指导AI开发者和政策制定者如何利用信息来减轻AI未来可能造成的负面环境影响。", "translation": "近年来，人工智能（AI）模型的规模和复杂性不断增长，推动了对计算能力和自然资源的更大需求。与此同时，围绕这些模型成本和影响的透明度却有所下降，这意味着这些技术的使用者对其资源需求以及随之而来的环境影响知之甚少。尽管缺乏足够的数据，但对量化AI环境影响数据的需求不断升级，导致了许多源于不准确或脱离语境的最佳估计温室气体排放的错误信息。在本文中，我们探讨了影响公众对AI环境影响理解的普遍迷思和误解，追溯了它们的起源及其在媒体和科学出版物中的传播。我们讨论了数据透明度在澄清误解和减轻这些危害方面的重要性，并以一套建议作为结论，说明AI开发者和政策制定者如何利用这些信息来减轻未来的负面影响。", "summary": "本研究指出，随着AI模型规模和复杂性的增长，其对计算能力和自然资源的消耗也随之增加，但相关环境影响的透明度却在下降。这种信息缺失导致了关于AI环境影响的错误信息和误解的广泛传播。文章深入探讨了这些普遍存在的迷思及其传播途径，并强调了数据透明度在澄清误解和减轻负面影响方面的重要性。最后，论文为AI开发者和政策制定者提供了如何利用信息来减轻未来环境影响的建议。", "keywords": "AI, 环境影响, 透明度, 错误信息, 资源消耗", "comments": "该论文及时地指出了AI快速发展背景下，环境影响透明度不足导致错误信息泛滥的关键问题，具有重要的现实意义。其创新之处在于将“遗漏信息”作为一种新型的虚假信息来源进行分析，并强调了数据透明度在应对这一挑战中的核心作用，为AI的可持续发展提供了新的视角和实用建议。"}}
{"id": "2506.15124", "title": "A Force Feedback Exoskeleton for Teleoperation Using Magnetorheological Clutches", "authors": ["Zhongyuan Kong", "Lei Li", "Erwin Ang Tien Yew", "Zirui Chen", "Wenbo Li", "Shiwu Zhang", "Jian Yang", "Shuaishuai Sun"], "summary": "This paper proposes an upper-limb exoskeleton teleoperation system based on\nmagnetorheological (MR) clutches, aiming to improve operational accuracy and\nenhance the immersive experience during lunar sampling tasks. Conventional\nexoskeleton teleoperation systems commonly employ active force feedback\nsolutions, such as servo motors, which typically suffer from high system\ncomplexity and increased energy consumption. Furthermore, force feedback\ndevices utilizing motors and gear reducers generally compromise backdrivability\nand pose safety risks to operators due to active force output. To address these\nlimitations, we propose a semi-active force feedback strategy based on MR\nclutches. Dynamic magnetic field control enables precise adjustment of joint\nstiffness and damping, thereby providing smooth and high-resolution force\nfeedback. The designed MR clutch exhibits outstanding performance across key\nmetrics, achieving a torque-to-mass ratio (TMR) of 93.6 Nm/kg, a\ntorque-to-volume ratio (TVR) of 4.05 x 10^5 Nm/m^3, and a torque-to-power ratio\n(TPR) of 4.15 Nm/W. Notably, the TMR represents an improvement of approximately\n246% over a representative design in prior work. Experimental results validate\nthe system's capability to deliver high-fidelity force feedback. Overall, the\nproposed system presents a promising solution for deep-space teleoperation with\nstrong potential for real-world deployment in future missions.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15124v1", "AI": {"title_translation": "基于磁流变离合器的力反馈外骨骼遥操作系统", "tldr": "本文提出了一种基于磁流变（MR）离合器的半主动力反馈上肢外骨骼遥操作系统，旨在提高月球采样任务的操作精度和沉浸感，同时克服传统主动系统在复杂性、能耗和安全性方面的局限性。", "motivation": "传统的力反馈外骨骼遥操作系统，如使用伺服电机的主动解决方案，通常存在系统复杂性高、能耗大、回驱性差以及对操作员存在安全风险等问题。本文旨在解决这些局限性，并提高月球采样任务的操作精度和沉浸式体验。", "method": "本文提出了一种基于磁流变（MR）离合器的半主动力反馈策略。通过动态磁场控制，该系统能够精确调节关节刚度和阻尼，从而提供平滑、高分辨率的力反馈。", "result": "所设计的磁流变离合器在关键指标上表现出色，实现了93.6 Nm/kg的扭矩质量比（TMR）、4.05 x 10^5 Nm/m^3的扭矩体积比（TVR）以及4.15 Nm/W的扭矩功率比（TPR）。值得注意的是，TMR比现有工作中的代表性设计提高了约246%。实验结果验证了该系统提供高保真力反馈的能力。", "conclusion": "所提出的系统为深空遥操作提供了一个有前景的解决方案，在未来的任务中具有强大的实际部署潜力。", "translation": "本文提出了一种基于磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在提高月球采样任务的操作精度和沉浸式体验。传统的力反馈外骨骼遥操作系统通常采用主动力反馈解决方案，例如伺服电机，这些方案通常存在系统复杂性高和能耗增加的问题。此外，利用电机和齿轮减速器实现的力反馈装置通常会损害回驱性，并且由于主动力输出，对操作员构成安全风险。为了解决这些局限性，我们提出了一种基于磁流变离合器的半主动力反馈策略。动态磁场控制能够精确调节关节刚度和阻尼，从而提供平滑、高分辨率的力反馈。所设计的磁流变离合器在关键指标上表现出色，实现了93.6 Nm/kg的扭矩质量比（TMR）、4.05 x 10^5 Nm/m^3的扭矩体积比（TVR）以及4.15 Nm/W的扭矩功率比（TPR）。值得注意的是，TMR比现有工作中的代表性设计提高了约246%。实验结果验证了该系统提供高保真力反馈的能力。总的来说，所提出的系统为深空遥操作提供了一个有前景的解决方案，在未来的任务中具有强大的实际部署潜力。", "summary": "本文介绍了一种利用磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在提升月球采样等任务的操作精度和沉浸感。该系统采用半主动力反馈策略，克服了传统主动系统（如伺服电机）在复杂性、高能耗、回驱性差和安全风险方面的局限性。通过动态磁场控制，MR离合器能精确调节关节刚度和阻尼，提供平滑、高分辨率的力反馈。所设计的MR离合器表现出卓越的性能，其扭矩质量比（TMR）相比现有设计显著提升了约246%。实验结果证实了系统提供高保真力反馈的能力，表明其是未来深空遥操作的有效解决方案。", "keywords": "磁流变离合器, 外骨骼, 力反馈, 遥操作, 半主动", "comments": "该论文的创新之处在于利用半主动磁流变离合器来克服主动力反馈系统的缺点，提供了一种更安全、高效且具有良好回驱性的解决方案。其在性能指标上，特别是扭矩质量比（TMR）的显著提升，突显了其在太空探索等严苛应用中的巨大实际潜力。"}}
{"id": "2506.14970", "title": "NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for Multi-Modal Neurological Disorder Classification", "authors": ["Wajih Hassan Raza", "Aamir Bader Shah", "Yu Wen", "Yidan Shen", "Juan Diego Martinez Lemus", "Mya Caryn Schiess", "Timothy Michael Ellmore", "Renjie Hu", "Xin Fu"], "summary": "The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical\ndata holds great promise for enhancing the diagnosis of neurological disorders\n(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged\nas a powerful tool for extracting meaningful patterns from medical data to aid\nin diagnosis. However, existing DL approaches struggle to effectively leverage\nmulti-modal MRI and clinical data, leading to suboptimal performance.\n  To address this challenge, we utilize a unique, proprietary multi-modal\nclinical dataset curated for ND research. Based on this dataset, we propose a\nnovel transformer-based Mixture-of-Experts (MoE) framework for ND\nclassification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion\nTensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our\nframework employs transformer encoders to capture spatial relationships within\nvolumetric MRI data while utilizing modality-specific experts for targeted\nfeature extraction. A gating mechanism with adaptive fusion dynamically\nintegrates expert outputs, ensuring optimal predictive performance.\nComprehensive experiments and comparisons with multiple baselines demonstrate\nthat our multi-modal approach significantly enhances diagnostic accuracy,\nparticularly in distinguishing overlapping disease states. Our framework\nachieves a validation accuracy of 82.47\\%, outperforming baseline methods by\nover 10\\%, highlighting its potential to improve ND diagnosis by applying\nmulti-modal learning to real-world clinical data.", "comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14970v1", "AI": {"title_translation": "NeuroMoE: 一种基于Transformer的专家混合框架，用于多模态神经疾病分类", "tldr": "提出NeuroMoE框架，利用Transformer和专家混合模型整合多模态MRI和临床数据，显著提升神经疾病诊断准确性。", "motivation": "现有深度学习方法难以有效利用多模态MRI和临床数据诊断神经疾病，导致性能不佳。", "method": "利用专有的多模态临床数据集，提出基于Transformer的MoE框架。该框架结合解剖MRI、DTI、fMRI和临床评估数据，使用Transformer编码器捕获MRI空间关系，模态特定专家进行特征提取，并通过自适应融合的门控机制动态整合专家输出。", "result": "多模态方法显著提高诊断准确性，尤其在区分重叠疾病状态方面。在验证集上达到82.47%的准确率，比基线方法高出10%以上。", "conclusion": "NeuroMoE框架通过将多模态学习应用于真实世界临床数据，具有改善神经疾病诊断的潜力。", "translation": "多模态磁共振成像（MRI）和临床数据的整合有望显著提高真实世界临床环境中神经疾病（NDs）的诊断。深度学习（DL）最近已成为从医学数据中提取有意义模式以辅助诊断的强大工具。然而，现有的DL方法难以有效利用多模态MRI和临床数据，导致性能不佳。\n为了解决这一挑战，我们利用了一个为ND研究而策划的独特专有多模态临床数据集。基于此数据集，我们提出了一种新颖的基于Transformer的专家混合（MoE）框架，用于ND分类，该框架利用多种MRI模态——解剖（aMRI）、弥散张量成像（DTI）和功能（fMRI）——以及临床评估。我们的框架采用Transformer编码器来捕获体积MRI数据内的空间关系，同时利用模态特定专家进行有针对性的特征提取。具有自适应融合的门控机制动态整合专家输出，确保最佳预测性能。\n与多个基线进行的全面实验和比较表明，我们的多模态方法显著提高了诊断准确性，特别是在区分重叠疾病状态方面。我们的框架实现了82.47%的验证准确率，比基线方法高出10%以上，突显了其通过将多模态学习应用于真实世界临床数据来改善ND诊断的潜力。", "summary": "本研究提出NeuroMoE，一个基于Transformer的专家混合框架，用于多模态神经疾病分类。针对现有深度学习方法难以有效整合多模态MRI和临床数据的问题，NeuroMoE利用Transformer编码器处理体积MRI数据，并通过模态特定专家和自适应融合门控机制整合多种MRI模态（aMRI、DTI、fMRI）和临床数据。实验结果表明，NeuroMoE显著提高了神经疾病诊断准确性，在验证集上达到82.47%的准确率，优于基线方法10%以上，展现了其在真实世界临床应用中的巨大潜力。", "keywords": "神经疾病分类, 多模态, Transformer, 专家混合, MRI", "comments": "该论文的创新点在于提出了一个结合Transformer和专家混合模型的多模态框架，有效地整合了异构的MRI和临床数据，解决了现有深度学习方法在多模态数据利用上的瓶颈。其自适应融合的门控机制是亮点，能够动态优化专家输出。研究结果显著的性能提升表明了该方法在神经疾病诊断领域的巨大潜力，尤其是在区分复杂疾病状态方面。该框架有望为临床诊断提供更精确的工具。"}}
{"id": "2506.15148", "title": "Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation", "authors": ["Yuxuan Xia", "Ángel F. García-Fernández", "Johan Karlsson", "Yu Ge", "Lennart Svensson", "Ting Yuan"], "summary": "This paper presents a generalization of the trajectory general optimal\nsub-pattern assignment (GOSPA) metric for evaluating multi-object tracking\nalgorithms that provide trajectory estimates with track-level uncertainties.\nThis metric builds on the recently introduced probabilistic GOSPA metric to\naccount for both the existence and state estimation uncertainties of individual\nobject states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a\nmultidimensional assignment problem, and its linear programming\nrelaxation--also a valid metric--is computable in polynomial time.\nAdditionally, this metric retains the interpretability of TGOSPA, and we show\nthat its decomposition yields intuitive costs terms associated to expected\nlocalization error and existence probability mismatch error for properly\ndetected objects, expected missed and false detection error, and track switch\nerror. The effectiveness of the proposed metric is demonstrated through a\nsimulation study.", "comment": "7 pages, 4 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15148v1", "AI": {"title_translation": "概率轨迹GOSPA：一种用于不确定性感知多目标跟踪性能评估的度量", "tldr": "本文提出了一种概率轨迹GOSPA度量，用于评估考虑轨迹级不确定性的多目标跟踪算法，并证明其有效性。", "motivation": "现有度量可能未能充分考虑多目标跟踪算法中轨迹估计的轨迹级不确定性。本文旨在提出一种能够评估提供具有轨迹级不确定性估计的追踪算法的度量。", "method": "本文提出了一种轨迹通用最优子模式分配（GOSPA）度量的泛化，即概率轨迹GOSPA。该度量基于近期提出的概率GOSPA度量，同时考虑了单个目标状态的存在和状态估计不确定性。它可以被表述为一个多维分配问题，并且其线性规划松弛（也是一个有效的度量）可以在多项式时间内计算。该度量保留了TGOSPA的可解释性，并且其分解产生了与预期定位误差、存在概率不匹配误差、预期漏检和虚警误差以及轨迹切换误差相关的直观成本项。", "result": "所提出的度量通过仿真研究证明了其有效性。它保留了TGOSPA的可解释性，并且其分解产生了与预期定位误差、存在概率不匹配误差、预期漏检和虚警误差以及轨迹切换误差相关的直观成本项。", "conclusion": "本文提出的概率轨迹GOSPA度量能够有效评估提供轨迹级不确定性估计的多目标跟踪算法。", "translation": "本文提出了一种轨迹通用最优子模式分配（GOSPA）度量的泛化，用于评估提供轨迹级不确定性估计的多目标跟踪算法。该度量建立在近期引入的概率GOSPA度量的基础上，以同时考虑单个目标状态的存在和状态估计不确定性。与轨迹GOSPA（TGOSPA）类似，它可以被表述为一个多维分配问题，并且其线性规划松弛——也是一个有效的度量——可以在多项式时间内计算。此外，该度量保留了TGOSPA的可解释性，并且我们展示了其分解产生了与正确检测到的目标的预期定位误差和存在概率不匹配误差、预期漏检和虚警误差以及轨迹切换误差相关的直观成本项。所提出的度量的有效性通过仿真研究得到证明。", "summary": "本文介绍了一种名为概率轨迹GOSPA的新度量，它是对轨迹GOSPA的泛化，专门用于评估那些提供轨迹级不确定性估计的多目标跟踪算法。该度量整合了概率GOSPA的优势，考虑了目标的存在性和状态估计不确定性。它可被建模为多维分配问题，并通过多项式时间可计算的线性规划松弛求解。该度量保持了原始TGOSPA的良好可解释性，其分解项能直观反映定位误差、存在概率不匹配、漏检、虚警和轨迹切换等错误。仿真研究验证了其有效性。", "keywords": "多目标跟踪, GOSPA, 轨迹评估, 不确定性, 性能度量", "comments": "本文的创新之处在于将GOSPA度量扩展到能够处理多目标跟踪中的轨迹级不确定性，这对于更准确地评估复杂跟踪系统至关重要。通过引入概率GOSPA的概念，并保持其可计算性和可解释性，该工作为不确定性感知跟踪算法的性能评估提供了一个有力的工具。"}}
{"id": "2506.15121", "title": "Generative thermodynamic computing", "authors": ["Stephen Whitelam"], "summary": "We introduce a generative modeling framework for thermodynamic computing, in\nwhich structured data is synthesized from noise by the natural time evolution\nof a physical system governed by Langevin dynamics. While conventional\ndiffusion models use neural networks to perform denoising, here the information\nneeded to generate structure from noise is encoded by the dynamics of a\nthermodynamic system. Training proceeds by maximizing the probability with\nwhich the computer generates the reverse of a noising trajectory, which ensures\nthat the computer generates data with minimal heat emission. We demonstrate\nthis framework within a digital simulation of a thermodynamic computer. If\nrealized in analog hardware, such a system would function as a generative model\nthat produces structured samples without the need for artificially-injected\nnoise or active control of denoising.", "comment": null, "cate": "cond-mat.stat-mech", "url": "http://arxiv.org/abs/2506.15121v1", "AI": {"title_translation": "生成式热力学计算", "tldr": "该论文介绍了一种生成式热力学计算框架，通过物理系统在朗之万动力学下的自然时间演化从噪声中合成结构化数据，实现数据生成且热量排放最小。", "motivation": "传统的扩散模型依赖神经网络进行去噪，而本文旨在通过热力学系统的动力学直接编码从噪声中生成结构所需的信息，以期在模拟硬件中实现无需人工噪声注入或主动去噪控制的生成模型。", "method": "本文引入了一个生成建模框架，通过受朗之万动力学支配的物理系统的自然时间演化，从噪声中合成结构化数据。训练过程通过最大化计算机生成噪声轨迹逆过程的概率来进行，以确保数据生成过程中的热量排放最小化。该框架已在热力学计算机的数字模拟中进行了演示。", "result": "该框架已在热力学计算机的数字模拟中得到验证。如果能以模拟硬件实现，该系统将作为一个生成模型，无需人工注入噪声或主动控制去噪即可生成结构化样本。", "conclusion": "该框架通过利用热力学系统的自然动力学，为生成建模提供了一种新颖的途径，有望在模拟硬件中实现更节能、更自主的生成系统。", "translation": "生成式热力学计算\n我们引入了一种用于热力学计算的生成建模框架，其中结构化数据通过受朗之万动力学支配的物理系统的自然时间演化从噪声中合成。虽然传统的扩散模型使用神经网络进行去噪，但在这里，从噪声中生成结构所需的信息通过热力学系统的动力学进行编码。训练通过最大化计算机生成噪声轨迹逆过程的概率进行，这确保了计算机以最小的热量排放生成数据。我们在热力学计算机的数字模拟中演示了该框架。如果以模拟硬件实现，这样的系统将作为一个生成模型，在不需要人工注入噪声或主动控制去噪的情况下生成结构化样本。", "summary": "本文提出了一种新颖的生成式热力学计算框架。与依赖神经网络去噪的传统扩散模型不同，该方法利用物理系统在朗之万动力学下的自然演化，将数据生成所需的信息编码在热力学系统动力学中。通过优化逆向噪声轨迹的生成概率，确保了最小的热量排放。该框架已在数字模拟中得到验证，并有望在模拟硬件中实现无需人工噪声和主动控制的自主生成模型。", "keywords": "生成建模, 热力学计算, 朗之万动力学, 扩散模型, 模拟硬件", "comments": "这篇论文通过将生成建模与热力学物理系统相结合，提出了一个非常创新的方法。它摆脱了当前流行的基于神经网络的扩散模型，转而利用自然物理过程来生成数据，这在节能和自主性方面具有重要潜力。特别是如果能在模拟硬件中实现，它将代表生成模型领域的一个重大突破，因为它消除了对人工噪声注入和主动控制的需求。"}}
{"id": "2506.15530", "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "summary": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15530v1", "AI": {"title_translation": "Diff-TONE：文本到音乐扩散模型中乐器编辑的时间步优化", "tldr": "Diff-TONE提出了一种在文本到音乐扩散模型中优化时间步的方法，用于在不额外训练或影响速度的情况下，对现有音频轨道进行乐器编辑，同时保留其内容。", "motivation": "当前的文本到音乐生成模型在控制生成过程以实现特定结果方面存在重大挑战，即使是微小的文本提示变化也可能导致作品的剧烈改变。本文旨在解决如何利用现有模型对音频中的乐器进行编辑，同时保留其底层内容。", "method": "该方法利用预训练的文本到音乐扩散模型进行乐器编辑。基于模型首先处理音频的整体结构/内容，然后添加乐器信息的洞察，通过乐器分类器选择一个最佳的中间时间步。此方法无需对扩散模型进行额外训练，也不会降低生成速度。", "result": "该方法在保留原始作品内容和实现所需音色之间取得了平衡。同时，它不需要额外的模型训练，也不会损害生成过程的速度。", "conclusion": "通过优化预训练文本到音乐扩散模型中的中间时间步，可以有效地在保留原始内容的同时实现乐器编辑，提供了一种高效且实用的生成音乐控制方法。", "translation": "文本到音乐生成模型的突破正在改变创作格局，为音乐家提供了前所未有的创新作曲和实验工具。然而，控制生成过程以实现特定期望结果仍然是一个重大挑战。即使文本提示的微小变化，结合相同的随机种子，也可能彻底改变生成的作品。在本文中，我们探索了现有文本到音乐扩散模型在乐器编辑方面的应用。具体来说，对于现有的音轨，我们旨在利用预训练的文本到音乐扩散模型来编辑乐器，同时保留底层内容。基于模型首先关注音频的整体结构或内容，然后添加乐器信息，最后优化质量的洞察，我们展示了通过乐器分类器选择一个精心选择的中间时间步，可以在保留原始作品内容和实现所需音色之间取得平衡。我们的方法不需要对文本到音乐扩散模型进行额外训练，也不会影响生成过程的速度。", "summary": "本文介绍了Diff-TONE，一种利用预训练文本到音乐扩散模型对现有音频进行乐器编辑的新方法。为解决文本到音乐生成中控制精度不足的问题，Diff-TONE利用扩散模型先处理内容再处理音色的特性，通过乐器分类器确定最佳中间时间步。该方法有效地平衡了保留原始音频内容与实现所需乐器音色，且无需额外模型训练，也不影响生成速度。", "keywords": "文本到音乐, 扩散模型, 乐器编辑, 时间步优化, 音频生成", "comments": "该论文的创新之处在于，它利用了扩散模型固有的生成过程，实现了无需再训练即可进行有针对性的编辑，效率极高。这种方法为生成式音乐中的精细控制提供了一个实用的解决方案，解决了当前文本到音乐模型的一个关键限制。"}}
{"id": "2506.15001", "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings", "authors": ["Ignacio Sastre", "Aiala Rosá"], "summary": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.", "comment": "This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15001v1", "AI": {"title_translation": "记忆令牌：大型语言模型可以生成可逆句子嵌入", "tldr": "大型语言模型（LLM）能够通过使用特殊记忆令牌生成可逆的句子嵌入，从而在不修改模型权重的情况下精确重建原始文本。", "motivation": "本研究观察到一个有趣的现象：大型语言模型能够生成可逆的句子嵌入，这突出了一种潜在的新能力，并暗示了其在内存检索、压缩和受控文本生成方面的应用前景。", "method": "研究通过引入一个特殊的“记忆令牌”来实现可逆嵌入，该令牌的嵌入通过对固定序列进行训练来优化。当LLM被这个嵌入提示时，它能够精确地重建原始序列。该方法在英语和西班牙语数据集、长达约240个令牌的序列以及从100M到8B参数的模型规模上进行了评估。", "result": "研究发现可以生成可逆的句子嵌入，使LLM能够在不修改模型权重的情况下精确重建原始文本。Llama 3.1 8B模型成功重建了所有测试序列。这一现象在不同数据集、序列长度和模型规模上均得到验证。", "conclusion": "本研究揭示了大型语言模型的一项有趣能力，即生成可逆句子嵌入，这预示着其在基于内存的检索、数据压缩和受控文本生成等领域具有潜在应用。", "translation": "在这项工作中，我们观察到一个有趣的现象：可以生成可逆的句子嵌入，允许大型语言模型（LLM）精确地重建原始文本，而无需修改模型的权重。这是通过引入一个特殊的记忆令牌来实现的，其嵌入通过对固定序列进行训练来优化。当用这个嵌入进行提示时，模型会精确地重建固定序列。我们在英语和西班牙语数据集、长达约240个令牌的序列以及从100M到8B参数的模型规模上评估了这一现象。值得注意的是，Llama 3.1 8B成功重建了所有测试序列。我们的发现突出了LLM的一个有趣能力，并提出了在基于内存的检索、压缩和受控文本生成方面的潜在应用。", "summary": "本文展示了大型语言模型（LLM）能够生成可逆句子嵌入。通过训练一个特殊的“记忆令牌”嵌入，LLM在被提示时可以精确地重建原始文本，而无需修改其权重。这一现象在多种数据集、序列长度和模型尺寸上得到验证，其中Llama 3.1 8B实现了完美的重建。此能力为基于内存的检索、数据压缩和精确文本生成等新应用提供了可能性。", "keywords": "记忆令牌, 大型语言模型, 可逆嵌入, 文本重建, 句子嵌入", "comments": "这篇论文揭示了LLM一项引人入胜的内在能力。利用一个简单的记忆令牌实现可逆句子嵌入，从而精确重建文本，这一概念极具创新性，因为它避免了对整个模型进行微调。这可能会显著改变LLM管理和检索信息的方式，为数据压缩和受控生成提供新颖的解决方案。研究发现即使是较小的模型也表现出这种行为，而像Llama 3.1这样的大型模型能够实现完美重建，这突显了其鲁棒性和潜在实用性。"}}
{"id": "2506.14792", "title": "Fast automated adjoints for spectral PDE solvers", "authors": ["Calum S. Skene", "Keaton J. Burns"], "summary": "We present a general and automated approach for computing model gradients for\nPDE solvers built on sparse spectral methods, and implement this capability in\nthe widely used open-source Dedalus framework. We apply reverse-mode automatic\ndifferentiation to symbolic graph representations of PDEs, efficiently\nconstructing adjoint solvers that retain the speed and memory efficiency of\nthis important class of modern numerical methods. This approach enables users\nto compute gradients and perform optimization for a wide range of\ntime-dependent and nonlinear systems without writing additional code. The\nframework supports a broad class of equations, geometries, and boundary\nconditions, and runs efficiently in parallel using MPI. We demonstrate the\nflexibility and capabilities of this system using canonical problems from the\nliterature, showing both strong performance and practical utility for a wide\nvariety of inverse problems. By integrating automatic adjoints into a flexible\nhigh-level solver, our approach enables researchers to perform gradient-based\noptimization and sensitivity analyses in spectral simulations with ease and\nefficiency.", "comment": "17 pages, 6 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14792v1", "AI": {"title_translation": "谱偏微分方程求解器的快速自动伴随", "tldr": "本文提出了一种在Dedalus框架中为基于稀疏谱方法的偏微分方程求解器计算模型梯度的通用自动化方法，通过逆向模式自动微分高效构建伴随求解器，从而实现广泛的时间依赖和非线性系统的梯度计算和优化，且无需额外编码。", "motivation": "在谱模拟中，研究人员需要一种能够轻松高效地进行基于梯度的优化和敏感性分析的方法。传统的梯度计算方法可能效率低下或需要大量手动编码。本文旨在提供一种通用且自动化的方法来解决这个问题，使计算模型梯度变得更加便捷。", "method": "本文提出了一种通用且自动化的方法，用于计算基于稀疏谱方法的偏微分方程求解器的模型梯度。具体而言，该方法在广泛使用的开源Dedalus框架中实现了这一功能，通过对偏微分方程的符号图表示应用逆向模式自动微分，高效地构建了伴随求解器。该框架支持多种方程、几何和边界条件，并能通过MPI高效并行运行。", "result": "该方法使得用户无需编写额外代码即可计算梯度并对各种时间依赖和非线性系统进行优化。该框架支持广泛的方程、几何和边界条件，并能通过MPI高效并行运行。通过使用文献中的典型问题进行演示，该系统展现了强大的性能和在各种逆问题中的实用性。", "conclusion": "通过将自动伴随集成到一个灵活的高级求解器中，本文提出的方法使研究人员能够轻松高效地在谱模拟中执行基于梯度的优化和敏感性分析。", "translation": "我们提出了一种通用且自动化的方法，用于计算基于稀疏谱方法的偏微分方程求解器的模型梯度，并在广泛使用的开源Dedalus框架中实现了这一功能。我们对偏微分方程的符号图表示应用逆向模式自动微分，高效地构建了伴随求解器，这些求解器保留了这种重要的现代数值方法的速度和内存效率。这种方法使用户无需编写额外代码即可计算梯度并对各种时间依赖和非线性系统进行优化。该框架支持广泛的方程、几何和边界条件，并能通过MPI高效并行运行。我们使用文献中的典型问题演示了该系统的灵活性和能力，展示了在各种逆问题中的强大性能和实用性。通过将自动伴随集成到一个灵活的高级求解器中，我们的方法使研究人员能够轻松高效地在谱模拟中执行基于梯度的优化和敏感性分析。", "summary": "本文介绍了一种在Dedalus框架中实现自动伴随计算的通用方法，用于基于稀疏谱方法的偏微分方程求解器。该方法利用逆向模式自动微分从偏微分方程的符号图表示中高效构建伴随求解器，从而实现对各种时间依赖和非线性系统的梯度计算和优化，而无需额外编码。该框架支持多种方程和条件，并能高效并行运行，其在逆问题中的实用性和性能已得到验证，极大地简化了谱模拟中的梯度优化和敏感性分析。", "keywords": "自动伴随, 谱方法, 偏微分方程, 自动微分, Dedalus", "comments": "本文提出了一种创新且实用的方法，将自动微分引入到谱偏微分方程求解器中，显著降低了研究人员进行梯度计算和优化的门槛。其在Dedalus框架中的实现以及对并行计算的支持，提升了方法的实用性和效率，对于需要高精度和复杂系统分析的领域具有重要意义。"}}
{"id": "2506.15093", "title": "Flexible Hardware-Enabled Guarantees for AI Compute", "authors": ["James Petrie", "Onni Aarne", "Nora Ammann", "David Dalrymple"], "summary": "As artificial intelligence systems become increasingly powerful, they pose\ngrowing risks to international security, creating urgent coordination\nchallenges that current governance approaches struggle to address without\ncompromising sensitive information or national security. We propose flexible\nhardware-enabled guarantees (flexHEGs), that could be integrated with AI\naccelerators to enable trustworthy, privacy-preserving verification and\nenforcement of claims about AI development. FlexHEGs consist of an auditable\nguarantee processor that monitors accelerator usage and a secure enclosure\nproviding physical tamper protection. The system would be fully open source\nwith flexible, updateable verification capabilities. FlexHEGs could enable\ndiverse governance mechanisms including privacy-preserving model evaluations,\ncontrolled deployment, compute limits for training, and automated safety\nprotocol enforcement. In this first part of a three part series, we provide a\ncomprehensive introduction of the flexHEG system, including an overview of the\ngovernance and security capabilities it offers, its potential development and\nadoption paths, and the remaining challenges and limitations it faces. While\ntechnically challenging, flexHEGs offer an approach to address emerging\nregulatory and international security challenges in frontier AI development.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15093v1", "AI": {"title_translation": "针对AI计算的灵活硬件保障", "tldr": "提出一种名为flexHEG的灵活硬件保障系统，通过集成到AI加速器中，实现对AI开发的可信、隐私保护的验证和执行，以应对AI带来的国际安全和治理挑战。", "motivation": "随着AI系统能力增强，其对国际安全构成日益增长的风险，导致现有治理方法在不泄露敏感信息或国家安全的情况下难以应对的紧急协调挑战。", "method": "提出灵活硬件保障（flexHEGs），可集成到AI加速器中。FlexHEGs包含一个可审计的保障处理器（监控加速器使用）和一个提供物理防篡改的安全外壳。系统将完全开源，并具有灵活、可更新的验证能力。", "result": "FlexHEGs能够实现多样化的治理机制，包括隐私保护的模型评估、受控部署、训练计算限制以及自动化安全协议的执行。", "conclusion": "尽管技术上具有挑战性，但flexHEGs为解决前沿AI开发中新兴的监管和国际安全挑战提供了一种方法。", "translation": "随着人工智能系统变得越来越强大，它们对国际安全构成日益增长的风险，带来了紧急的协调挑战，而当前的治理方法在不损害敏感信息或国家安全的情况下难以解决这些挑战。我们提出灵活的硬件保障（flexHEGs），可以将其与AI加速器集成，以实现对AI开发声明的可信、隐私保护的验证和执行。FlexHEGs由一个可审计的保障处理器（监控加速器使用）和一个提供物理防篡改的安全外壳组成。该系统将完全开源，并具有灵活、可更新的验证能力。FlexHEGs可以实现多样化的治理机制，包括隐私保护的模型评估、受控部署、训练计算限制以及自动化安全协议的执行。作为三部曲系列的第一部分，我们全面介绍了flexHEG系统，包括其提供的治理和安全能力概述、其潜在的开发和采用路径，以及它面临的剩余挑战和限制。尽管技术上具有挑战性，但flexHEGs为解决前沿AI开发中新兴的监管和国际安全挑战提供了一种方法。", "summary": "这篇论文提出了灵活硬件保障（flexHEGs），旨在通过集成到AI加速器中，提供可信且隐私保护的验证和执行能力，以应对AI系统日益增长的国际安全风险和治理挑战。FlexHEGs包含一个可审计的处理器和安全外壳，并支持开源和灵活更新的验证功能。它能实现多种治理机制，如隐私保护评估、受控部署和计算限制。论文介绍了该系统的概念、能力、发展路径及挑战，强调其是解决前沿AI监管与国际安全问题的新方法。", "keywords": "AI治理, 硬件保障, 国际安全, 隐私保护, AI加速器", "comments": "这篇论文提出了一种创新的硬件-软件协同方法来解决AI治理和国际安全问题，特别是在隐私保护和可信验证方面具有重要意义。其开源和可更新的设计理念增加了系统的灵活性和可信度。然而，硬件层面的实现复杂性和实际部署的挑战是其主要的限制。作为系列论文的第一部分，它为后续深入研究奠定了基础。"}}
{"id": "2506.15227", "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15227v1", "AI": {"title_translation": "大型语言模型在单元测试中的应用：一项系统性文献综述", "tldr": "本文是对大型语言模型（LLMs）在单元测试领域应用的首个系统性文献综述，旨在帮助研究人员理解现有成就、挑战及未来方向。", "motivation": "单元测试是现代软件工程的基础实践，而大型语言模型（LLMs）的最新进展已使其在自动化单元测试任务中展现出显著性能并大幅减少了人工工作。然而，由于该领域仍在探索中，研究人员难以全面理解现有成果、开放挑战和未来机遇，因此需要进行系统性的回顾。", "method": "本文进行了一项系统性文献综述，涵盖截至2025年3月关于LLMs在单元测试中应用的相关论文。研究从单元测试和LLMs两个角度分析论文，首先对受益于LLMs的现有单元测试任务进行分类（如测试生成和预言生成），然后讨论了将LLMs整合到单元测试研究中的关键方面（包括模型使用、适应策略和混合方法），最后总结了未解决的关键挑战并提出了未来研究方向。", "result": "本研究为单元测试社区提供了一个关于LLM应用于单元测试研究领域的系统性概览。它帮助研究人员全面理解了该领域的现有成就，并为未来的研究指明了方向。相关的研究成果已在GitHub上公开。", "conclusion": "本文通过提供对大型语言模型在单元测试中应用的系统性概览，帮助研究人员全面理解该领域的研究现状、成就和挑战，从而促进未来的研究发展。", "translation": "单元测试是现代软件工程中的一项基本实践，旨在确保单个软件组件的正确性、可维护性和可靠性。最近，随着大型语言模型（LLMs）的进步，越来越多的研究利用LLMs来自动化各种单元测试任务，展示了卓越的性能并显著减少了人工工作。然而，由于基于LLM的单元测试领域仍在持续探索中，研究人员难以理解现有的成就、开放的挑战和未来的机遇。本文首次对截至2025年3月LLMs在单元测试中的应用进行了系统性文献综述。我们从单元测试和LLMs的角度分析了相关论文。我们首先对受益于LLMs的现有单元测试任务进行分类，例如测试生成和预言生成。然后，我们讨论了将LLMs整合到单元测试研究中的几个关键方面，包括模型使用、适应策略和混合方法。我们进一步总结了仍未解决的关键挑战，并概述了有前景的方向以指导该领域的未来研究。总的来说，我们的论文为单元测试社区提供了研究现状的系统性概览，帮助研究人员全面理解成就并促进未来的研究。我们的成果已在GitHub存储库公开：https://github.com/iSEngLab/AwesomeLLM4UT。", "summary": "本文是首个针对大型语言模型（LLMs）在单元测试中应用的系统性文献综述，旨在解决该领域研究分散、难以全面理解的问题。文章系统分析了相关论文，对受益于LLMs的单元测试任务进行分类（如测试生成、预言生成），讨论了LLMs集成关键方面（模型使用、适应策略、混合方法），并总结了未解决的挑战及未来研究方向。该综述为单元测试社区提供了全面的研究概览，有助于促进未来研究。", "keywords": "大型语言模型, 单元测试, 系统性文献综述, 测试生成, 预言生成", "comments": "本文作为该领域内的首个系统性文献综述，具有重要的里程碑意义。它不仅系统梳理了大型语言模型在单元测试中的应用现状和成就，更重要的是，它明确指出了当前存在的挑战并展望了未来的研究方向，为后续研究者提供了宝贵的路线图。其公开的研究成果也体现了开放科学的精神，有利于社区的共同进步。"}}
{"id": "2506.14948", "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework", "authors": ["Mohna Chakraborty", "Lu Wang", "David Jurgens"], "summary": "Large language models (LLMs) are increasingly deployed in domains requiring\nmoral understanding, yet their reasoning often remains shallow, and misaligned\nwith human reasoning. Unlike humans, whose moral reasoning integrates\ncontextual trade-offs, value systems, and ethical theories, LLMs often rely on\nsurface patterns, leading to biased decisions in morally and ethically complex\nscenarios. To address this gap, we present a value-grounded framework for\nevaluating and distilling structured moral reasoning in LLMs. We benchmark 12\nopen-source models across four moral datasets using a taxonomy of prompts\ngrounded in value systems, ethical theories, and cognitive reasoning\nstrategies. Our evaluation is guided by four questions: (1) Does reasoning\nimprove LLM decision-making over direct prompting? (2) Which types of\nvalue/ethical frameworks most effectively guide LLM reasoning? (3) Which\ncognitive reasoning strategies lead to better moral performance? (4) Can\nsmall-sized LLMs acquire moral competence through distillation? We find that\nprompting with explicit moral structure consistently improves accuracy and\ncoherence, with first-principles reasoning and Schwartz's + care-ethics\nscaffolds yielding the strongest gains. Furthermore, our supervised\ndistillation approach transfers moral competence from large to small models\nwithout additional inference cost. Together, our results offer a scalable path\ntoward interpretable and value-grounded models.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14948v1", "AI": {"title_translation": "语言模型中的结构化道德推理：一个基于价值观的评估框架", "tldr": "开发了一个基于价值观的框架来评估和提升LLM的道德推理能力，发现结构化提示和蒸馏能显著提高模型性能和道德能力。", "motivation": "大型语言模型（LLMs）在需要道德理解的领域中应用日益增多，但它们的推理往往是肤浅的，与人类推理不一致，容易在复杂道德场景中做出有偏见的决策。", "method": "提出了一个基于价值观的框架来评估和蒸馏LLM中的结构化道德推理。使用基于价值观系统、伦理理论和认知推理策略的提示分类法，在四个道德数据集上对12个开源模型进行了基准测试。通过监督蒸馏方法将道德能力从大型模型转移到小型模型。", "result": "带有明确道德结构的提示一致性地提高了准确性和连贯性；第一性原理推理和Schwartz的+关怀伦理支架效果最好；监督蒸馏方法可以在不增加推理成本的情况下将道德能力从大型模型转移到小型模型。", "conclusion": "结果为实现可解释和基于价值观的模型提供了一条可扩展的路径。", "translation": "大型语言模型（LLMs）正越来越多地部署在需要道德理解的领域，然而它们的推理往往停留在表面，并且与人类推理不符。与人类不同，人类的道德推理整合了情境权衡、价值体系和伦理理论，而LLMs通常依赖于表面模式，导致在道德和伦理复杂的场景中做出有偏见的决策。为了解决这一差距，我们提出了一个基于价值观的框架，用于评估和蒸馏LLMs中的结构化道德推理。我们使用基于价值体系、伦理理论和认知推理策略的提示分类法，在四个道德数据集上对12个开源模型进行了基准测试。我们的评估由四个问题指导：(1) 推理是否比直接提示更能改善LLM的决策？(2) 哪种类型的价值观/伦理框架最有效地指导LLM推理？(3) 哪种认知推理策略能带来更好的道德表现？(4) 小型LLMs能否通过蒸馏获得道德能力？我们发现，使用明确道德结构进行提示一致地提高了准确性和连贯性，其中第一性原理推理和Schwartz的+关怀伦理支架带来了最显著的提升。此外，我们的监督蒸馏方法在不增加额外推理成本的情况下，将道德能力从大型模型转移到小型模型。总而言之，我们的结果为实现可解释和基于价值观的模型提供了一条可扩展的路径。", "summary": "本文提出了一个名为“基于价值观的评估框架”的方法，旨在解决大型语言模型（LLMs）在道德推理方面存在的肤浅和与人类不符的问题。该框架通过结合价值观、伦理理论和认知策略的结构化提示，对12个开源模型进行了基准测试，并探索了不同提示方式和认知策略对LLM道德决策的影响。研究发现，明确的道德结构提示能显著提升模型的准确性和连贯性，特别是第一性原理推理和关怀伦理支架表现最佳。此外，通过监督蒸馏，小型模型也能有效获得道德能力。这些发现为开发更具解释性和基于价值观的LLMs提供了可行途径。", "keywords": "语言模型, 道德推理, 评估框架, 知识蒸馏", "comments": "这篇论文的创新点在于提出了一个基于价值观的结构化评估框架，并首次系统地探讨了如何通过结构化提示和知识蒸馏来提升LLM的道德推理能力。其重要性在于为LLM在敏感领域的应用提供了更可靠的道德基础，有助于减少偏见。"}}
{"id": "2506.15012", "title": "Context Matters: Learning Generalizable Rewards via Calibrated Features", "authors": ["Alexandra Forsey-Smerek", "Julie Shah", "Andreea Bobu"], "summary": "A key challenge in reward learning from human input is that desired agent\nbehavior often changes based on context. Traditional methods typically treat\neach new context as a separate task with its own reward function. For example,\nif a previously ignored stove becomes too hot to be around, the robot must\nlearn a new reward from scratch, even though the underlying preference for\nprioritizing safety over efficiency remains unchanged. We observe that context\ninfluences not the underlying preference itself, but rather the\n$\\textit{saliency}$--or importance--of reward features. For instance, stove\nheat affects the importance of the robot's proximity, yet the human's safety\npreference stays the same. Existing multi-task and meta IRL methods learn\ncontext-dependent representations $\\textit{implicitly}$--without distinguishing\nbetween preferences and feature importance--resulting in substantial data\nrequirements. Instead, we propose $\\textit{explicitly}$ modeling\ncontext-invariant preferences separately from context-dependent feature\nsaliency, creating modular reward representations that adapt to new contexts.\nTo achieve this, we introduce $\\textit{calibrated features}$--representations\nthat capture contextual effects on feature saliency--and present specialized\npaired comparison queries that isolate saliency from preference for efficient\nlearning. Experiments with simulated users show our method significantly\nimproves sample efficiency, requiring 10x fewer preference queries than\nbaselines to achieve equivalent reward accuracy, with up to 15% better\nperformance in low-data regimes (5-10 queries). An in-person user study (N=12)\ndemonstrates that participants can effectively teach their unique personal\ncontextual preferences using our method, enabling more adaptable and\npersonalized reward learning.", "comment": "30 pages, 21 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15012v1", "AI": {"title_translation": "上下文至关重要：通过校准特征学习可泛化奖励", "tldr": "当前奖励学习方法难以应对上下文变化。本文提出通过“校准特征”显式分离上下文不变偏好和上下文依赖特征显著性，以提高样本效率和适应性。", "motivation": "从人类输入中学习奖励面临一个关键挑战：期望的智能体行为常常随上下文变化。传统方法通常将每个新上下文视为一个独立的任务，需要从头开始学习新的奖励函数，即使潜在偏好（例如，安全优先于效率）保持不变。现有的大多数多任务和元IRL方法隐式地学习上下文依赖的表示，导致大量数据需求。", "method": "本文观察到上下文影响的是奖励特征的显著性（重要性），而非底层偏好。因此，提出显式地将上下文不变偏好与上下文依赖的特征显著性分开建模，从而创建适应新上下文的模块化奖励表示。为此，引入了“校准特征”——捕获上下文对特征显著性影响的表示——并提出了专门的配对比较查询，以将显著性与偏好分离，从而实现高效学习。", "result": "与模拟用户进行的实验表明，该方法显著提高了样本效率，实现同等奖励精度所需的偏好查询数量比基线方法少10倍，在低数据量（5-10次查询）情况下性能提升高达15%。一项针对12名参与者的真人用户研究表明，参与者可以有效利用该方法教授其独特的个人上下文偏好，从而实现更具适应性和个性化的奖励学习。", "conclusion": "本文提出的方法通过显式分离上下文不变偏好和上下文依赖特征显著性，实现了更具适应性和个性化的奖励学习。这显著提高了样本效率，并使用户能够有效地教授独特的上下文偏好。", "translation": "从人类输入中学习奖励的一个关键挑战是，期望的智能体行为通常会根据上下文而变化。传统方法通常将每个新上下文视为一个具有其自身奖励函数的独立任务。例如，如果一个之前被忽略的炉子变得太热，机器人必须从头开始学习新的奖励，即使其将安全置于效率之上的潜在偏好保持不变。我们观察到，上下文影响的不是底层偏好本身，而是奖励特征的显著性——或重要性。例如，炉子的热度影响机器人接近度的重要性，但人类的安全偏好保持不变。现有的多任务和元IRL方法隐式地学习上下文依赖的表示——不区分偏好和特征重要性——导致大量数据需求。相反，我们建议显式地将上下文不变偏好与上下文依赖的特征显著性分开建模，从而创建适应新上下文的模块化奖励表示。为了实现这一点，我们引入了“校准特征”——捕获上下文对特征显著性影响的表示——并提出了专门的配对比较查询，以将显著性与偏好分离，从而实现高效学习。与模拟用户进行的实验表明，我们的方法显著提高了样本效率，实现同等奖励精度所需的偏好查询数量比基线方法少10倍，在低数据量（5-10次查询）情况下性能提升高达15%。一项真人用户研究（N=12）表明，参与者可以有效利用我们的方法教授其独特的个人上下文偏好，从而实现更具适应性和个性化的奖励学习。", "summary": "当期望的智能体行为随上下文变化时，从人类输入中学习奖励面临挑战。本文观察到上下文影响的是奖励特征的显著性而非底层偏好。与现有隐式学习上下文依赖表示的方法不同，本文提出显式分离上下文不变偏好与上下文依赖特征显著性，并引入“校准特征”和专门查询来构建模块化、可适应的奖励表示。实验证明，该方法显著提高了样本效率，所需的查询次数减少10倍，并使用户能有效教授个性化上下文偏好。", "keywords": "奖励学习, 上下文偏好, 特征显著性, 校准特征, 样本效率", "comments": "本文的创新之处在于明确区分了上下文不变偏好和上下文依赖的特征显著性，这与以往隐式学习的方法形成了鲜明对比。这种模块化表示显著提高了数据效率，解决了当前奖励学习方法的一个关键限制，为开发更具适应性和个性化的人工智能系统奠定了基础。"}}
{"id": "2506.15233", "title": "New Bounds and Constructions for Variable Packet-Error Coding", "authors": ["Xiangliang Kong", "Xin Wang", "Ron M. Roth", "Itzhak Tamo"], "summary": "In this paper, we consider the problem of variable packet-error coding, which\nemerges in network communication scenarios where a source transmits information\nto a destination through multiple disjoint paths. The objective is to design\ncodes with dynamic error-correcting capabilities that adapt to varying numbers\nof errors. Specifically, we first provide several bounds on the\nrate--distortion trade-off for general variable packet-error coding schemes.\nThen, we present two explicit constructions of variable packet-error coding\nschemes. The first construction uses higher-order MDS codes and provides a\ncoding scheme that achieves a better rate--distortion trade-off compared to\nknown results for general parameter regimes. The second construction is based\non a variant of the repetition code and yields a coding scheme with an optimal\nrate--distortion trade-off, with respect to our bound, for certain parameter\nregimes.", "comment": "20 pages, 3 figures, part of the work in this paper has been accepted\n  for presentation at the 2025 IEEE International Symposium on Information\n  Theory (ISIT)", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15233v1", "AI": {"title_translation": "可变分组差错编码的新界限和构造", "tldr": "本文研究可变分组差错编码，提出了新的速率-失真界限，并通过两种新构造实现了更好的或最优的编码方案，以适应网络通信中动态变化的错误数量。", "motivation": "在网络通信场景中，信息通过多条不相交路径传输，需要设计具有动态纠错能力以适应不同数量错误的编码方案。", "method": "首先，为通用可变分组差错编码方案的速率-失真权衡提供了几个界限。然后，提出了两种显式的可变分组差错编码方案构造：第一种使用高阶MDS码；第二种基于重复码的变体。", "result": "第一种构造（使用高阶MDS码）与已知结果相比，在通用参数范围内实现了更好的速率-失真权衡。第二种构造（基于重复码变体）在某些参数范围内，相对于本文提出的界限，实现了最优的速率-失真权衡。", "conclusion": "论文提出了可变分组差错编码的新界限和两种有效的构造方案，显著提升了动态纠错编码在网络通信中的性能。", "translation": "在本文中，我们考虑了可变分组差错编码问题，该问题出现在源通过多个不相交路径向目的地传输信息的网络通信场景中。目标是设计具有动态纠错能力的编码，以适应不同数量的错误。具体来说，我们首先为通用可变分组差错编码方案的速率-失真权衡提供了几个界限。然后，我们提出了两种显式的可变分组差错编码方案构造。第一种构造使用高阶MDS码，并提供了一种编码方案，与已知结果相比，在通用参数范围内实现了更好的速率-失真权衡。第二种构造基于重复码的一种变体，并针对某些参数范围，相对于我们提出的界限，产生了一种具有最优速率-失真权衡的编码方案。", "summary": "本文研究了网络通信中可变分组差错编码问题，旨在设计能适应动态错误数量的编码。作者首先为通用可变分组差错编码方案的速率-失真权衡提供了新的界限。接着，提出了两种具体的编码方案构造：一种基于高阶MDS码，在通用参数范围内实现了比现有技术更好的速率-失真权衡；另一种基于重复码变体，在特定参数范围内达到了最优的速率-失真权衡。", "keywords": "可变分组差错编码, 速率-失真权衡, MDS码, 重复码, 网络通信", "comments": "该论文在可变分组差错编码领域做出了重要贡献，通过提供新的理论界限和两种创新的构造方法，显著提升了动态纠错编码的性能。特别是第二种构造实现了最优性能，这对于实际网络通信具有重要意义。"}}
{"id": "2506.15634", "title": "SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy", "authors": ["Hasnain A. Ziad", "Alexander C. Bodoh", "Ashiq A. Sakib"], "summary": "Duplication-based redundancy schemes have proven to be effective in designing\nfully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The\ncomplete resiliency, however, is accompanied by significant energy, latency,\nand area overhead. This paper presents a novel error-tolerant Null Convention\nLogic (NCL) architecture based on selective redundancy. Results demonstrate the\nefficacy of the proposed method in terms of area and energy utilization as\ncompared to existing duplication-based NCL designs, targeting an image\nprocessing application.", "comment": "5 pages. Accepted for publication in the Proceedings of IEEE ISCAS\n  2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15634v1", "AI": {"title_translation": "SR-NCL：一种基于选择性冗余的面积/能效弹性NCL架构", "tldr": "提出一种基于选择性冗余的NCL架构，显著降低了传统冗余方案的面积和能耗开销。", "motivation": "现有的基于复制的冗余方案在设计完全弹性准延迟不敏感（QDI）异步电路时，虽然有效，但会带来显著的能量、延迟和面积开销。", "method": "本文提出了一种基于选择性冗余的新型容错空约定逻辑（NCL）架构。", "result": "结果表明，与现有的基于复制的NCL设计相比，所提出的方法在面积和能量利用方面更有效，并以图像处理应用为目标进行了验证。", "conclusion": "基于选择性冗余的NCL架构能够有效降低传统复制冗余方案的面积和能量开销，同时保持错误容忍能力。", "translation": "基于复制的冗余方案已被证明在设计完全弹性的准延迟不敏感（QDI）异步电路中是有效的。然而，完全的弹性伴随着显著的能量、延迟和面积开销。本文提出了一种基于选择性冗余的新型容错空约定逻辑（NCL）架构。结果表明，与现有的基于复制的NCL设计相比，所提出的方法在面积和能量利用方面是有效的，并以图像处理应用为目标进行了验证。", "summary": "本文针对传统基于复制的冗余方案在实现完全弹性QDI异步电路时面临的巨大面积和能耗开销问题，提出了一种名为SR-NCL的新型基于选择性冗余的容错空约定逻辑（NCL）架构。实验结果（以图像处理应用为例）表明，与现有基于复制的NCL设计相比，SR-NCL在面积和能量利用方面具有显著优势。", "keywords": "空约定逻辑 (NCL), 选择性冗余, 异步电路, 面积效率, 能量效率", "comments": "这项研究通过引入选择性冗余，有效解决了传统完全复制冗余方案在异步电路设计中带来的高昂面积和能耗成本。其创新点在于从“完全冗余”转向“选择性冗余”，这对于追求低功耗和小型化的异步电路设计具有重要意义。该方法在图像处理应用中的验证也显示了其实用潜力。"}}
{"id": "2506.15437", "title": "Exploring Fast Fourier Transforms on the Tenstorrent Wormhole", "authors": ["Nick Brown", "Jake Davies", "Felix LeClair"], "summary": "Whilst numerous areas of computing have adopted the RISC-V Instruction Set\nArchitecture (ISA) wholesale in recent years, it is yet to become widespread in\nHPC. RISC-V accelerators offer a compelling option where the HPC community can\nbenefit from the specialisation offered by the open nature of the standard but\nwithout the extensive ecosystem changes required when adopting RISC-V CPUs. In\nthis paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)\nalgorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon\nTenstorrent's Tensix architecture, this technology decouples the movement of\ndata from compute, potentially offering increased control to the programmer.\nExploring different optimisation techniques to address the bottlenecks inherent\nin data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is\nslower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around\n8 times less power and consumes around 2.8 times less energy than the CPU when\ncomputing the Fourier transform.", "comment": "Author accepted version of paper submitted to RISC-V for HPC ISC\n  workshop 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15437v1", "AI": {"title_translation": "探索在Tenstorrent Wormhole上实现快速傅里叶变换", "tldr": "本文探讨了在Tenstorrent Wormhole RISC-V加速器上实现快速傅里叶变换（FFT），发现其在计算FFT时比传统CPU功耗和能耗显著降低。", "motivation": "RISC-V指令集架构（ISA）在高性能计算（HPC）领域尚未普及，但RISC-V加速器提供了一个无需广泛生态系统变更即可利用开放标准专业化的方案。本文旨在探索将快速傅里叶变换（FFT）算法移植到这种RISC-V加速器上，以评估其潜力。", "method": "将Cooley-Tukey快速傅里叶变换（FFT）算法移植到基于Tenstorrent Wormhole PCIe RISC-V的加速器上，并探索不同的优化技术以解决数据移动固有的瓶颈。", "result": "对于二维FFT，Wormhole n300虽然比服务器级24核Xeon Platinum CPU慢，但在计算傅里叶变换时，Wormhole的功耗大约低8倍，能耗大约低2.8倍。", "conclusion": "Tenstorrent Wormhole RISC-V加速器在执行FFT时，尽管速度可能不如高性能CPU，但在功耗和能耗方面表现出显著的优势，这表明其在能源效率方面具有潜力。", "translation": "近年来，尽管许多计算领域已全面采用RISC-V指令集架构（ISA），但它尚未在高性能计算（HPC）领域广泛普及。RISC-V加速器提供了一个引人注目的选择，HPC社区可以从开放标准的专业化中受益，而无需在采用RISC-V CPU时进行广泛的生态系统变更。在本文中，我们探讨了将Cooley-Tukey快速傅里叶变换（FFT）算法移植到基于Tenstorrent Wormhole PCIe RISC-V的加速器上。这项技术建立在Tenstorrent的Tensix架构之上，将数据移动与计算解耦，可能为程序员提供更大的控制权。通过探索不同的优化技术来解决数据移动固有的瓶颈，我们证明了对于二维FFT，虽然Wormhole n300比服务器级24核Xeon Platinum CPU慢，但在计算傅里叶变换时，Wormhole的功耗大约低8倍，能耗大约低2.8倍。", "summary": "本文研究了在Tenstorrent Wormhole PCIe RISC-V加速器上实现Cooley-Tukey快速傅里叶变换（FFT）算法。该研究旨在利用RISC-V加速器在HPC中的潜力，通过优化数据移动瓶颈，结果显示Wormhole在执行2D FFT时，虽然速度慢于传统CPU，但在功耗和能耗方面表现出显著的效率优势。", "keywords": "快速傅里叶变换, RISC-V加速器, Tenstorrent Wormhole, 功耗效率, 高性能计算", "comments": "这篇论文的创新点在于将FFT算法移植到Tenstorrent Wormhole RISC-V加速器上，并评估其性能和能效。它强调了RISC-V加速器在HPC领域，特别是在能源效率方面的潜力，这对于追求绿色计算的应用非常重要。尽管其速度不如传统CPU，但显著的功耗和能耗优势使其在特定场景下具有吸引力。"}}
{"id": "2506.14831", "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review", "authors": ["Céline Finet", "Stephane Da Silva Martins", "Jean-Bernard Hayet", "Ioannis Karamouzas", "Javad Amirian", "Sylvie Le Hégarat-Mascle", "Julien Pettré", "Emanuel Aldea"], "summary": "With the emergence of powerful data-driven methods in human trajectory\nprediction (HTP), gaining a finer understanding of multi-agent interactions\nlies within hand's reach, with important implications in areas such as\nautonomous navigation and crowd modeling. This survey reviews some of the most\nrecent advancements in deep learning-based multi-agent trajectory prediction,\nfocusing on studies published between 2020 and 2024. We categorize the existing\nmethods based on their architectural design, their input representations, and\ntheir overall prediction strategies, placing a particular emphasis on models\nevaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges\nand future research directions in the field of multi-agent HTP.", "comment": "30 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14831v1", "AI": {"title_translation": "多智能体人类轨迹预测的最新进展：一项全面综述", "tldr": "该综述回顾了2020年至2024年间深度学习驱动的多智能体人类轨迹预测的最新进展，并分类了现有方法，指出了挑战和未来方向。", "motivation": "随着数据驱动方法在人类轨迹预测（HTP）中的兴起，更好地理解多智能体交互变得可能，这在自主导航和人群建模等领域具有重要意义。因此，需要对该领域的最新进展进行全面回顾。", "method": "本文对2020年至2024年间发表的基于深度学习的多智能体轨迹预测的最新进展进行了综述。研究人员根据架构设计、输入表示和整体预测策略对现有方法进行了分类，并特别强调了使用ETH/UCY基准进行评估的模型。", "result": "该综述详细阐述了2020年至2024年间深度学习驱动的多智能体人类轨迹预测领域的最新进展，并根据架构设计、输入表示和整体预测策略对现有方法进行了系统分类。此外，它还指出了该领域面临的关键挑战和未来的研究方向。", "conclusion": "该综述全面回顾了多智能体人类轨迹预测的最新进展，分类了现有方法，并指出了未来的研究挑战和方向，为领域发展提供了指导。", "translation": "随着数据驱动方法在人类轨迹预测（HTP）领域的兴起，对多智能体交互的更深入理解触手可及，这在自主导航和人群建模等领域具有重要意义。本调查综述了基于深度学习的多智能体轨迹预测的一些最新进展，重点关注2020年至2024年间发表的研究。我们根据其架构设计、输入表示和整体预测策略对现有方法进行了分类，并特别强调了使用ETH/UCY基准进行评估的模型。此外，我们还强调了多智能体HTP领域的关键挑战和未来的研究方向。", "summary": "本文对2020年至2024年间深度学习在多智能体人类轨迹预测（HTP）方面的最新进展进行了全面综述。该综述根据架构、输入表示和预测策略对现有方法进行了分类，并关注了使用ETH/UCY基准评估的模型。此外，文章还强调了该领域的关键挑战和未来研究方向，对自主导航和人群建模等应用具有重要意义。", "keywords": "人类轨迹预测, 多智能体交互, 深度学习, 综述, ETH/UCY基准", "comments": "该综述具有很高的时效性和实用性，它涵盖了多智能体人类轨迹预测领域2020-2024年的最新进展，并进行了系统分类，为研究人员提供了清晰的概览和未来方向。其创新之处在于对现有方法的细致分类和对ETH/UCY基准的特别关注，这有助于理解当前研究的焦点和评估标准。该工作对于推动自主导航和人群建模等领域的发展具有重要指导意义。"}}
{"id": "2506.15377", "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation", "authors": ["Ruoyu Wang", "Xinshu Li", "Chen Wang", "Lina Yao"], "summary": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate\ncomplex environments toward given objectives. Across diverse settings within\nNavigation tasks, many necessitate the modelling of sequential data accumulated\nfrom preceding time steps. While existing methods perform well, they typically\nprocess all historical observations simultaneously, overlooking the internal\nassociation structure within the data, which may limit the potential for\nfurther improvements in task performance. We address this by examining the\nunique characteristics of Navigation tasks through the lens of causality,\nintroducing a causal framework to highlight the limitations of conventional\nsequential methods. Leveraging this insight, we propose Causality-Aware\nNavigation (CAN), which incorporates a Causal Understanding Module to enhance\nthe agent's environmental understanding capability. Empirical evaluations show\nthat our approach consistently outperforms baselines across various tasks and\nsimulation environments. Extensive ablations studies attribute these gains to\nthe Causal Understanding Module, which generalizes effectively in both\nReinforcement and Supervised Learning settings without computational overhead.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15377v1", "AI": {"title_translation": "高效且可泛化的视觉导航环境理解", "tldr": "现有视觉导航方法同时处理历史数据，限制了性能。本文提出了因果感知导航（CAN），包含一个因果理解模块，以提高环境理解能力，实现在没有计算开销的情况下更好的泛化和性能。", "motivation": "现有视觉导航方法在处理从先前时间步积累的序列数据时，通常同时处理所有历史观测，忽视了数据内部的关联结构，这限制了任务性能进一步提升的潜力。", "method": "本文通过因果关系视角审视导航任务的独特特征，引入了一个因果框架来突出传统序列方法的局限性。在此基础上，提出了因果感知导航（CAN），其中包含一个因果理解模块（Causal Understanding Module）来增强智能体的环境理解能力。", "result": "实证评估表明，所提出的方法在各种任务和模拟环境中始终优于基线。广泛的消融研究表明，这些性能提升归因于因果理解模块，该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "conclusion": "本文提出的因果框架以及因果感知导航（CAN）及其因果理解模块，有效地增强了环境理解能力，从而在视觉导航任务中实现了性能和泛化能力的提升，并且在不同的学习范式下无需额外的计算成本。", "translation": "视觉导航是具身AI中的一项核心任务，使智能体能够在复杂环境中导航以达到给定目标。在导航任务的各种设置中，许多任务都需要对从先前时间步积累的序列数据进行建模。尽管现有方法表现良好，但它们通常同时处理所有历史观测数据，忽视了数据内部的关联结构，这可能会限制任务性能进一步提升的潜力。我们通过因果关系视角审视导航任务的独特特征来解决这个问题，引入了一个因果框架来突出传统序列方法的局限性。利用这一洞察，我们提出了因果感知导航（CAN），它包含一个因果理解模块，以增强智能体的环境理解能力。实证评估表明，我们的方法在各种任务和模拟环境中始终优于基线。广泛的消融研究将这些增益归因于因果理解模块，该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "summary": "本文针对视觉导航中现有方法未能有效利用序列数据内部结构的问题，引入了一个因果框架来分析导航任务，并提出了带有因果理解模块（CUM）的因果感知导航（CAN）。实证结果表明，CAN在各种任务和模拟环境中持续优于基线，并且在强化学习和监督学习两种学习范式下均能有效泛化，且没有计算开销，性能提升主要归因于CUM。", "keywords": "视觉导航, 具身AI, 因果理解, 序列数据, 泛化能力", "comments": "本文为视觉导航引入了新颖的因果视角，具有创新性。通过因果关系关注序列数据的内部结构，解决了当前方法的一个基本局限。所提出的因果感知导航（CAN）及其因果理解模块提供了一种计算高效的方式来改善环境理解和泛化能力，对具身AI领域做出了重要贡献。"}}
{"id": "2506.14787", "title": "Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems", "authors": ["Funing Li", "Yuan Tian", "Ruben Noortwyck", "Jifeng Zhou", "Liming Kuang", "Robert Schulz"], "summary": "In modern industrial and logistics environments, the rapid expansion of fast\ndelivery services has heightened the demand for storage systems that combine\nhigh efficiency with increased density. Multi-deep autonomous vehicle storage\nand retrieval systems (AVS/RS) present a viable solution for achieving greater\nstorage density. However, these systems encounter significant challenges during\nretrieval operations due to lane blockages. A conventional approach to mitigate\nthis issue involves storing items with homogeneous characteristics in a single\nlane, but this strategy restricts the flexibility and adaptability of\nmulti-deep storage systems.\n  In this study, we propose a deep reinforcement learning-based framework to\naddress the retrieval problem in multi-deep storage systems with heterogeneous\nitem configurations. Each item is associated with a specific due date, and the\nobjective is to minimize total tardiness. To effectively capture the system's\ntopology, we introduce a graph-based state representation that integrates both\nitem attributes and the local topological structure of the multi-deep\nwarehouse. To process this representation, we design a novel neural network\narchitecture that combines a Graph Neural Network (GNN) with a Transformer\nmodel. The GNN encodes topological and item-specific information into\nembeddings for all directly accessible items, while the Transformer maps these\nembeddings into global priority assignments. The Transformer's strong\ngeneralization capability further allows our approach to be applied to storage\nsystems with diverse layouts. Extensive numerical experiments, including\ncomparisons with heuristic methods, demonstrate the superiority of the proposed\nneural network architecture and the effectiveness of the trained agent in\noptimizing retrieval tardiness.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14787v1", "AI": {"title_translation": "面向多深存储系统中高效检索的拓扑感知和高度泛化深度强化学习", "tldr": "本研究提出了一种基于深度强化学习的框架，结合图神经网络和Transformer模型，用于解决多深存储系统中异构物品配置下的检索问题，以最小化总迟到时间，并证明了其优越性和泛化能力。", "motivation": "现代工业和物流环境中，快速配送服务的快速扩张提高了对高效率、高密度存储系统的需求。多深式自动存取系统（AVS/RS）是实现更高存储密度的可行方案，但其检索操作中存在巷道堵塞的重大挑战。传统方法限制了系统的灵活性和适应性。", "method": "提出了一种基于深度强化学习的框架来解决异构物品配置下的多深存储系统检索问题，目标是最小化总迟到时间。引入了结合物品属性和局部拓扑结构的图基状态表示。设计了一种结合图神经网络（GNN）和Transformer模型的新型神经网络架构。GNN编码拓扑和物品特定信息，Transformer将这些嵌入映射为全局优先级分配。", "result": "广泛的数值实验（包括与启发式方法的比较）证明了所提出的神经网络架构的优越性以及训练代理在优化检索迟到时间方面的有效性。", "conclusion": "本研究提出的基于深度强化学习的框架，通过结合图神经网络和Transformer模型，能够有效解决多深存储系统中异构物品配置下的检索问题，显著优化检索迟到时间，并展现出强大的泛化能力。", "translation": "在现代工业和物流环境中，快速配送服务的快速扩张提高了对高效率、高密度存储系统的需求。多深式自动存取系统（AVS/RS）是实现更高存储密度的可行方案。然而，这些系统在检索操作中由于巷道堵塞而面临重大挑战。缓解这一问题的传统方法是将具有同质特性的物品存储在单个巷道中，但这种策略限制了多深存储系统的灵活性和适应性。\n在本研究中，我们提出了一种基于深度强化学习的框架来解决具有异构物品配置的多深存储系统中的检索问题。每个物品都关联一个特定的到期日，目标是最小化总迟到时间。为了有效地捕获系统拓扑结构，我们引入了一种基于图的状态表示，该表示整合了物品属性和多深仓库的局部拓扑结构。为了处理这种表示，我们设计了一种结合图神经网络（GNN）和Transformer模型的新型神经网络架构。GNN将拓扑和物品特定信息编码为所有直接可访问物品的嵌入，而Transformer将这些嵌入映射为全局优先级分配。Transformer强大的泛化能力进一步使得我们的方法能够应用于具有不同布局的存储系统。广泛的数值实验，包括与启发式方法的比较，证明了所提出的神经网络架构的优越性以及训练代理在优化检索迟到时间方面的有效性。", "summary": "本研究提出了一种基于深度强化学习的创新框架，旨在解决多深存储系统中异构物品配置下的高效检索问题，以最小化总迟到时间。该框架引入了结合物品属性和局部拓扑结构的图基状态表示，并设计了一种结合图神经网络（GNN）和Transformer模型的新型神经网络架构。GNN负责编码拓扑和物品信息，而Transformer则负责全局优先级分配，并赋予模型强大的泛化能力，适用于不同布局的存储系统。实验结果表明，该方法在优化检索迟到时间方面优于传统启发式方法。", "keywords": "深度强化学习, 多深存储系统, 图神经网络, Transformer, 检索优化", "comments": "这项研究的创新之处在于将深度强化学习应用于多深存储系统的检索优化，并巧妙地结合了图神经网络（GNN）和Transformer模型。GNN用于捕获复杂的拓扑结构和物品属性，而Transformer则提供了强大的泛化能力，使其能够适应不同布局的仓库，这对于实际工业应用非常重要。这种结合解决了传统方法在处理异构物品和系统灵活性方面的局限性，为智能仓储的优化提供了新的思路。"}}
{"id": "2506.14817", "title": "Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning", "authors": ["Simon P. von der Maase"], "summary": "Forecasting violent conflict at high spatial and temporal resolution remains\na central challenge for both researchers and policymakers. This study presents\na novel neural network architecture for forecasting three distinct types of\nviolence -- state-based, non-state, and one-sided -- at the subnational\n(priogrid-month) level, up to 36 months in advance. The model jointly performs\nclassification and regression tasks, producing both probabilistic estimates and\nexpected magnitudes of future events. It achieves state-of-the-art performance\nacross all tasks and generates approximate predictive posterior distributions\nto quantify forecast uncertainty.\n  The architecture is built on a Monte Carlo Dropout Long Short-Term Memory\n(LSTM) U-Net, integrating convolutional layers to capture spatial dependencies\nwith recurrent structures to model temporal dynamics. Unlike many existing\napproaches, it requires no manual feature engineering and relies solely on\nhistorical conflict data. This design enables the model to autonomously learn\ncomplex spatiotemporal patterns underlying violent conflict.\n  Beyond achieving state-of-the-art predictive performance, the model is also\nhighly extensible: it can readily integrate additional data sources and jointly\nforecast auxiliary variables. These capabilities make it a promising tool for\nearly warning systems, humanitarian response planning, and evidence-based\npeacebuilding initiatives.", "comment": "33 pages, 9 figures, 3 tables. Presented at workshops hosted by PRIO,\n  AFK (German Association for Peace and Conflict Studies), CCEW (Bundeswehr\n  University Munich), Uppsala University, SODAS (University of Copenhagen) and\n  in briefings with UN agencies including UNIDIR, OCHA, and FAO", "cate": "stat.OT", "url": "http://arxiv.org/abs/2506.14817v1", "AI": {"title_translation": "下一代冲突预测：通过时空学习释放预测模式", "tldr": "本文提出了一种新颖的神经网络（Monte Carlo Dropout LSTM U-Net），用于最先进的次国家暴力冲突预测，通过历史数据学习时空模式，无需手动特征工程，并量化不确定性。", "motivation": "以高时空分辨率预测暴力冲突对于研究人员和政策制定者来说仍然是一个核心挑战。", "method": "本研究提出了一种基于Monte Carlo Dropout 长短期记忆（LSTM）U-Net的新型神经网络架构。它集成了卷积层以捕获空间依赖性，并使用循环结构来建模时间动态。该模型联合执行分类和回归任务，仅依赖历史冲突数据，无需手动特征工程。", "result": "该模型在所有任务中（提前36个月预测三种类型的次国家级暴力）均达到了最先进的性能，生成了概率估计和未来事件的预期规模，并产生了近似的预测后验分布以量化预测不确定性。", "conclusion": "该模型因其最先进的性能、可扩展性以及集成额外数据源和联合预测辅助变量的能力，成为预警系统、人道主义响应规划和循证和平建设倡议的一个有前景的工具。", "translation": "以高空间和时间分辨率预测暴力冲突仍然是研究人员和政策制定者面临的核心挑战。本研究提出了一种新颖的神经网络架构，用于在次国家（priogrid-month）级别，提前长达36个月预测三种不同类型的暴力——国家型、非国家型和单方面型。该模型联合执行分类和回归任务，生成未来事件的概率估计和预期规模。它在所有任务中均达到了最先进的性能，并生成近似的预测后验分布以量化预测不确定性。\n该架构建立在Monte Carlo Dropout 长短期记忆（LSTM）U-Net之上，集成了卷积层以捕获空间依赖性，并结合循环结构来建模时间动态。与许多现有方法不同，它不需要手动特征工程，并且仅依赖历史冲突数据。这种设计使模型能够自主学习暴力冲突背后复杂的时空模式。\n除了实现最先进的预测性能外，该模型还具有高度可扩展性：它可以轻松集成额外的数据源并联合预测辅助变量。这些能力使其成为预警系统、人道主义响应规划和循证和平建设倡议的一个有前景的工具。", "summary": "本文提出了一种新颖的Monte Carlo Dropout LSTM U-Net架构，用于预测次国家级暴力冲突（国家型、非国家型、单方面型），最长可提前36个月。该模型通过从历史数据中学习复杂的时空模式，无需手动特征工程，实现了最先进的性能，并提供概率估计、预期规模以及不确定性量化。其可扩展性使其在预警和和平建设方面具有重要价值。", "keywords": "冲突预测, 时空学习, 神经网络, LSTM U-Net, 不确定性量化", "comments": "该论文的创新之处在于其新颖的神经网络架构（Monte Carlo Dropout LSTM U-Net），它将空间和时间学习相结合，并能在无需手动特征工程的情况下达到最先进的性能。预测不确定性的量化也是一个重要的进步。该模型在预警系统方面的实际应用具有高度重要性。"}}
{"id": "2506.15191", "title": "Islanding Strategy for Smart Grids Oriented to Resilience Enhancement and Its Power Supply Range Optimization", "authors": ["Yanhong Luo", "Wenchao Meng", "Xi Zhu", "Andreas Elombo", "Hu Rong", "Bing Xie", "Tianwen Zhang"], "summary": "With the increasing prevalence of distributed generators, islanded operation\nbased on distributed generation is considered a vital means to enhance the\nreliability and resilience of smart grids. This paper investigates the main\nfactors in islanding partition of smart grids and establishes a mathematical\nmodel for islanding division. A method to determine the maximum power supply\nrange of distributed energy resources (DERs) based on the reachability matrix\nand power circle algorithm is proposed to improve computational efficiency. A\ndynamic programming method based on breadth-first search (BFS) is used to solve\nthe islanding partition scheme, and a region correction method is applied to\nmodify the maximum power supply area by considering controllable loads and\nprioritizing critical load restoration, thereby enhancing system resilience.\nFinally, simulation results verify the effectiveness of the proposed algorithm\nin improving smart grid resilience.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15191v1", "AI": {"title_translation": "智能电网面向韧性增强的孤岛策略及其供电范围优化", "tldr": "本文提出了一种基于可达矩阵和功率圆算法确定分布式能源最大供电范围，并结合BFS动态规划和区域修正方法解决智能电网孤岛划分问题，以增强系统韧性。", "motivation": "随着分布式发电机的日益普及，基于分布式发电的孤岛运行被认为是提高智能电网可靠性和韧性的重要手段。", "method": "本文调查了智能电网孤岛划分的主要因素，并建立了孤岛划分的数学模型。提出了一种基于可达矩阵和功率圆算法的分布式能源（DERs）最大供电范围确定方法，以提高计算效率。采用基于广度优先搜索（BFS）的动态规划方法求解孤岛划分方案，并应用区域修正方法，通过考虑可控负荷和优先恢复关键负荷来修正最大供电区域。", "result": "仿真结果验证了所提出算法在提高智能电网韧性方面的有效性。", "conclusion": "所提出的算法能够有效地提高智能电网的韧性，并通过优化孤岛划分和供电范围，增强系统在分布式发电背景下的可靠性。", "translation": "随着分布式发电机的日益普及，基于分布式发电的孤岛运行被认为是提高智能电网可靠性和韧性的重要手段。本文研究了智能电网孤岛划分的主要因素，并建立了孤岛划分的数学模型。提出了一种基于可达矩阵和功率圆算法的分布式能源（DERs）最大供电范围确定方法，以提高计算效率。采用基于广度优先搜索（BFS）的动态规划方法求解孤岛划分方案，并应用区域修正方法，通过考虑可控负荷和优先恢复关键负荷来修正最大供电区域，从而增强系统韧性。最后，仿真结果验证了所提出算法在提高智能电网韧性方面的有效性。", "summary": "本文针对智能电网的韧性增强，提出了一种孤岛策略及其供电范围优化方法。研究了孤岛划分的关键因素并建立了数学模型，提出了一种结合可达矩阵和功率圆算法的分布式能源最大供电范围确定方法以提高效率。通过基于BFS的动态规划解决孤岛划分方案，并引入区域修正方法以考虑可控负荷和优先恢复关键负荷，从而有效提升了智能电网的韧性。仿真结果验证了该算法的有效性。", "keywords": "智能电网, 孤岛策略, 韧性增强, 供电范围优化, 分布式能源", "comments": "该论文提出了一种综合性的孤岛策略，结合了多种算法来解决智能电网的韧性增强问题。其创新点在于将可达矩阵、功率圆算法、BFS动态规划和区域修正方法结合起来，以优化孤岛划分和供电范围，特别是考虑了可控负荷和关键负荷的优先恢复，这对于提高电网在故障情况下的自愈能力具有重要意义。"}}
{"id": "2506.15182", "title": "Classification of Multi-Parametric Body MRI Series Using Deep Learning", "authors": ["Boah Kim", "Tejas Sudharshan Mathai", "Kimberly Helm", "Peter A. Pinto", "Ronald M. Summers"], "summary": "Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15182v1", "AI": {"title_translation": "使用深度学习对多参数体部MRI序列进行分类", "tldr": "本文提出了一种基于深度学习的分类模型（DenseNet-121），用于准确分类8种多参数体部MRI序列，解决了DICOM头信息不准确的问题，并在内部和外部数据集上均表现出高准确性。", "motivation": "多参数磁共振成像（mpMRI）检查的序列类型多样，且DICOM头信息常因协议多样性和技术人员错误而出现不准确。为解决此问题，本研究旨在开发一种深度学习模型，以准确分类8种不同的体部mpMRI序列，从而帮助放射科医生高效阅片。", "method": "研究使用了来自不同机构的mpMRI数据，训练并比较了基于ResNet、EfficientNet和DenseNet的多个深度学习分类器，以对8种不同的MRI序列进行分类。确定了性能最佳的分类器后，研究探讨了其在不同训练数据量下的分类能力，并在训练分布外的数据集上进行了评估。此外，模型还采用了两种训练策略，使用来自不同扫描仪的mpMRI检查数据进行训练和性能测试。", "result": "实验结果表明，DenseNet-121模型在与其他分类模型比较时，取得了最高的F1分数（0.966）和准确率（0.972），p值小于0.05。当使用超过729个训练数据进行训练时，模型准确率超过0.95，且性能随训练数据量的增加而提升。在DLDS和CPTAC-UCEC外部数据集上，模型分别达到了0.872和0.810的准确率。", "conclusion": "这些结果表明，在内部和外部数据集上，DenseNet-121模型在分类8种体部MRI序列类型任务中均能达到高准确率。", "translation": "多参数磁共振成像（mpMRI）检查具有多种不同成像协议获取的序列类型。由于协议的多样性和偶尔的技术人员错误，这些序列的DICOM头信息常常不正确。为了解决这个问题，我们提出了一种基于深度学习的分类模型，用于分类8种不同的体部mpMRI序列类型，以便放射科医生高效地阅片。研究使用了来自不同机构的mpMRI数据，训练了多个基于ResNet、EfficientNet和DenseNet的深度学习分类器来分类8种不同的MRI序列，并比较了它们的性能。然后，确定了性能最佳的分类器，并研究了其在不同训练数据量设置下的分类能力。此外，该模型还在训练分布外的数据集上进行了评估。更进一步，该模型使用从不同扫描仪获得的mpMRI检查数据，通过两种训练策略进行训练，并测试了其性能。实验结果表明，DenseNet-121模型在与其他分类模型比较时，取得了最高的F1分数（0.966）和准确率（0.972），p值小于0.05。当使用超过729个训练数据进行训练时，模型准确率超过0.95，且性能随训练数据量的增加而提高。在DLDS和CPTAC-UCEC外部数据集上，模型分别达到了0.872和0.810的准确率。这些结果表明，在内部和外部数据集上，DenseNet-121模型在分类8种体部MRI序列类型任务中均能达到高准确率。", "summary": "本文针对多参数MRI序列中DICOM头信息不准确的问题，提出了一种基于深度学习的分类模型。研究利用来自不同机构的mpMRI数据，训练并比较了ResNet、EfficientNet和DenseNet模型，以对8种体部mpMRI序列类型进行分类。DenseNet-121模型表现最佳，在内部数据集上取得了0.966的F1分数和0.972的准确率。其性能随训练数据量的增加而提高，并在数据充足时保持超过0.95的准确率。该模型在外部数据集（DLDS和CPTAC-UCEC）上也表现出稳健的性能，准确率分别为0.872和0.810，表明其在体部MRI序列分类任务中具有高准确性和泛化能力。", "keywords": "深度学习, MRI分类, DenseNet, 多参数MRI, DICOM头", "comments": "该论文为MRI序列标注不准确这一常见临床问题提供了实用的解决方案。通过比较多种深度学习架构以及在分布外数据集和不同扫描仪上的评估，突出了所提出DenseNet-121模型的鲁棒性和泛化能力。这项工作有望通过自动化准确的序列分类，显著提高放射科医生的工作效率。"}}
{"id": "2506.15235", "title": "Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data", "authors": ["Taewon Kang", "Seunghyeon Park", "Pyo-Woong Son", "Jiwon Seo"], "summary": "The vulnerabilities of global navigation satellite systems (GNSS) to signal\ninterference have increased the demand for complementary positioning,\nnavigation, and timing (PNT) systems. To address this, South Korea has decided\nto deploy an enhanced long-range navigation (eLoran) system as a complementary\nPNT solution. Similar to GNSS, eLoran provides highly accurate timing\ninformation, which is essential for applications such as telecommunications,\nfinancial systems, and power distribution. However, the primary sources of\nerror for GNSS and eLoran differ. For eLoran, the main source of error is\nsignal propagation delay over land, known as the additional secondary factor\n(ASF). This delay, influenced by ground conductivity and weather conditions\nalong the signal path, is challenging to predict and mitigate. In this paper,\nwe measure the time difference (TD) between GPS and eLoran using a time\ninterval counter and analyze the correlations between eLoran/GPS TD and eleven\nmeteorological factors. Accurate estimation of eLoran/GPS TD could enable\neLoran to achieve timing accuracy comparable to that of GPS. We propose two\nestimation models for eLoran/GPS TD and compare their performance with existing\nTD estimation methods. The proposed WLR-AGRNN model captures the linear\nrelationships between meteorological factors and eLoran/GPS TD using weighted\nlinear regression (WLR) and models nonlinear relationships between outputs from\nexpert networks through an anisotropic general regression neural network\n(AGRNN). The model incorporates terrain elevation to appropriately weight\nmeteorological data, as elevation influences signal propagation delay.\nExperimental results based on four months of data demonstrate that the\nWLR-AGRNN model outperforms other models, highlighting its effectiveness in\nimproving eLoran/GPS TD estimation accuracy.", "comment": "Submitted to IEEE Access", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15235v1", "AI": {"title_translation": "通过机器学习结合气象和地形数据提高eLoran授时精度", "tldr": "本文提出WLR-AGRNN机器学习模型，利用气象和地形数据提高eLoran系统的授时精度，通过准确估计eLoran/GPS时差，实验证明其优于现有方法。", "motivation": "全球导航卫星系统 (GNSS) 易受信号干扰，导致对互补定位、导航和授时 (PNT) 系统的需求增加。韩国部署eLoran作为GNSS的补充。eLoran的主要误差源是陆地信号传播延迟（ASF），其受地面电导率和天气条件影响，难以预测和缓解。为了提高eLoran的授时精度，使其达到GPS的水平，需要准确估计eLoran/GPS时差。", "method": "本文测量了GPS和eLoran之间的时差(TD)，并分析了eLoran/GPS TD与十一个气象因素之间的相关性。提出并比较了两种eLoran/GPS TD估计模型，其中核心模型是WLR-AGRNN。该模型结合了加权线性回归(WLR)捕捉气象因素与TD之间的线性关系，以及各向异性广义回归神经网络(AGRNN)建模专家网络输出之间的非线性关系。模型中还纳入地形高程数据以适当加权气象数据，因为高程会影响信号传播延迟。", "result": "基于四个月数据的实验结果表明，WLR-AGRNN模型优于其他模型。", "conclusion": "WLR-AGRNN模型在提高eLoran/GPS时差估计精度方面表现出有效性，从而提升了eLoran系统的授时精度。", "translation": "全球导航卫星系统 (GNSS) 对信号干扰的脆弱性增加了对互补定位、导航和授时 (PNT) 系统的需求。为解决此问题，韩国已决定部署增强型远程导航 (eLoran) 系统作为互补 PNT 解决方案。与 GNSS 类似，eLoran 提供高精度的授时信息，这对于电信、金融系统和电力分配等应用至关重要。然而，GNSS 和 eLoran 的主要误差源不同。对于 eLoran，主要的误差源是陆地上的信号传播延迟，称为附加二次因子 (ASF)。这种延迟受信号路径上的地面电导率和天气条件影响，难以预测和缓解。在本文中，我们使用时间间隔计数器测量 GPS 和 eLoran 之间的时间差 (TD)，并分析 eLoran/GPS TD 与十一个气象因素之间的相关性。准确估计 eLoran/GPS TD 可以使 eLoran 实现与 GPS 相当的授时精度。我们提出了两种 eLoran/GPS TD 估计模型，并比较了它们与现有 TD 估计方法的性能。所提出的 WLR-AGRNN 模型使用加权线性回归 (WLR) 捕捉气象因素与 eLoran/GPS TD 之间的线性关系，并通过各向异性广义回归神经网络 (AGRNN) 建模专家网络输出之间的非线性关系。该模型纳入地形高程以适当加权气象数据，因为高程会影响信号传播延迟。基于四个月数据的实验结果表明，WLR-AGRNN 模型优于其他模型，突出了其在提高 eLoran/GPS TD 估计精度方面的有效性。", "summary": "本文旨在通过机器学习方法提高eLoran系统的授时精度，以应对GNSS的脆弱性。研究发现eLoran的主要误差源是受气象和地形影响的信号传播延迟（ASF）。为解决此问题，作者测量了eLoran与GPS之间的时差（TD），并分析了其与气象因素的相关性。文章提出了一种名为WLR-AGRNN的新模型，该模型结合了加权线性回归处理线性关系和各向异性广义回归神经网络处理非线性关系，并整合了地形高程数据。实验结果表明，该模型在估计eLoran/GPS TD方面优于现有方法，显著提升了eLoran的授时精度。", "keywords": "eLoran, 授时精度, 机器学习, 信号传播延迟, WLR-AGRNN", "comments": "这篇论文通过引入机器学习（WLR-AGRNN模型）来解决eLoran系统授时精度中的关键误差源——ASF，这是一个创新点。它将气象和地形数据结合起来，有效提升了TD的估计精度，对于eLoran作为GNSS补充PNT系统在实际应用中的性能提升具有重要意义。该方法通过数据驱动的方式克服了传统物理模型难以准确预测复杂传播延迟的挑战。"}}
{"id": "2506.15107", "title": "I Know You're Listening: Adaptive Voice for HRI", "authors": ["Paige Tuttösí"], "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs.", "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts IROS\n  23 Mmm whatcha say? Uncovering distal and proximal context effects in first\n  and second-language word perception using psychophysical reverse correlation\n  INTERSPEECH 24 Emojivoice: Towards long-term controllable expressivity in\n  robot speech RO-MAN 25", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15107v1", "AI": {"title_translation": "我知道你在听：用于人机交互的自适应语音", "tldr": "本文针对语言教学机器人语音不足的问题，提出了轻量级、富有表现力、能适应环境、并提高二语学习者清晰度的自适应语音系统。", "motivation": "现有研究在语言教学机器人中对任务特定合成语音的探索有限，特别是在二语教学领域，这可能严重影响机器人在语言教学任务中的有效性。", "method": "1. 开发了一种轻量级且富有表现力的机器人语音，通过微调Matcha-TTS并使用表情符号提示来创建。2. 研究了如何使机器人的语音适应物理和社会环境，通过在嘈杂和高能量环境中提高音高和音高率。3. 为二语学习者创建了一个英语TTS系统，通过数据驱动、基于感知的方法探讨元音时长对二语听者感知的影响，并开发了“L2清晰模式”来延长紧张元音。", "result": "1. 开发的语音被证明更具表现力、社会适宜性，并适合长时间的表达性语音（如讲故事），且能实时运行，计算资源需求有限。2. 发现在嘈杂和高能量环境中提高音高和音高率，能使机器人语音显得更合适，更具环境感知能力。3. L2清晰模式在尊重性、可理解性和鼓励性方面优于基础Matcha-TTS，并减少了在困难的紧/松元音最小对中的转录错误。", "conclusion": "本文通过开发轻量级、适应环境且对二语学习者更清晰的合成语音，显著提升了语言教学机器人的语音能力和有效性。", "translation": "尽管社交机器人在语言教学中的应用已被探索，但针对语言教学机器人任务特定合成语音的研究仍然有限。鉴于语言是一项口头任务，这一空白可能会严重影响机器人在语言教学任务中的有效性。我们通过三项贡献解决了二语教学机器人语音的不足：1. 我们解决了对轻量级和富有表现力的机器人语音的需求。通过对Matcha-TTS进行微调，我们使用表情符号提示创建了一种富有表现力的语音，它在时间上显示出一定的表达范围。该语音可以在有限的计算资源下实时运行。通过案例研究，我们发现这种语音更具表现力、社会适宜性，并且适合长时间的表达性语音，如讲故事。2. 我们探索了如何使机器人的语音适应物理和社会环境，以便在各种地点部署我们的语音。我们发现在嘈杂和高能量环境中提高音高和音高率，能使机器人的语音显得更合适，并使其似乎更了解当前环境。3. 我们创建了一个英语TTS系统，该系统利用二语学习者难以区分的已知元音语言特性，提高了对二语听者的清晰度。我们使用数据驱动、基于感知的方法来理解二语说话者如何使用时长线索来解释英语中带有最少紧张（长）和松弛（短）元音的挑战性单词。我们发现元音的时长强烈影响二语听者的感知，并为Matcha-TTS创建了一个“L2清晰模式”，该模式对紧张元音进行延长，同时保持松弛元音不变。我们的清晰模式被发现比基础Matcha-TTS更具尊重性、可理解性和鼓励性，同时减少了这些挑战性紧/松最小对中的转录错误。", "summary": "本文旨在解决语言教学机器人中任务特定合成语音的不足，特别是针对二语教学。作者提出了三项主要贡献：首先，开发了一种基于微调Matcha-TTS和表情符号提示的轻量级、富有表现力的实时机器人语音，并通过案例研究验证其表现力和适用性。其次，探索了语音如何适应物理和社会环境，发现在嘈杂环境中提高音高和音高率能增强语音的适宜性和环境感知。最后，为二语学习者创建了一个改进清晰度的英语TTS系统，通过延长紧张元音来提高感知度，该“L2清晰模式”被证明在可理解性和用户体验上优于基础系统，并减少了转录错误。", "keywords": "机器人语音, 语言教学, 自适应语音, TTS, 二语学习", "comments": "这篇论文通过多方面的创新，显著提升了语言教学机器人语音的实用性和有效性。其亮点在于不仅关注了语音的表达力与实时性，还深入考虑了语音对环境的适应性，以及针对二语学习者特定语言难点的优化。特别是“L2清晰模式”的提出，体现了对用户群体细致入微的理解和技术创新。这些贡献使机器人语音更自然、更易于理解，有望推动人机交互在教育领域的应用。"}}
{"id": "2506.15548", "title": "Versatile Symbolic Music-for-Music Modeling via Function Alignment", "authors": ["Junyan Jiang", "Daniel Chin", "Liwei Lin", "Xuanjie Liu", "Gus Xia"], "summary": "Many music AI models learn a map between music content and human-defined\nlabels. However, many annotations, such as chords, can be naturally expressed\nwithin the music modality itself, e.g., as sequences of symbolic notes. This\nobservation enables both understanding tasks (e.g., chord recognition) and\nconditional generation tasks (e.g., chord-conditioned melody generation) to be\nunified under a music-for-music sequence modeling paradigm. In this work, we\npropose parameter-efficient solutions for a variety of symbolic music-for-music\ntasks. The high-level idea is that (1) we utilize a pretrained Language Model\n(LM) for both the reference and the target sequence and (2) we link these two\nLMs via a lightweight adapter. Experiments show that our method achieves\nsuperior performance among different tasks such as chord recognition, melody\ngeneration, and drum track generation. All demos, code and model weights are\npublicly available.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15548v1", "AI": {"title_translation": "多功能符号音乐到音乐建模通过功能对齐", "tldr": "本文提出了一种参数高效的音乐到音乐序列建模方法，通过预训练语言模型和轻量级适配器，统一了多种音乐理解和生成任务，并取得了优越性能。", "motivation": "现有的音乐AI模型多依赖人工定义的标签，但许多音乐注释（如和弦）本身就可以用音乐形式表达。因此，研究旨在将音乐理解和条件生成任务统一到“音乐到音乐”序列建模范式下，并寻求参数高效的解决方案。", "method": "提出了一种参数高效的解决方案。核心思想是：1) 对参考序列和目标序列都使用预训练语言模型（LM）；2) 通过一个轻量级适配器连接这两个LM。", "result": "实验表明，该方法在和弦识别、旋律生成和鼓点生成等不同任务中均取得了优越的性能。", "conclusion": "该工作成功地将多种符号音乐到音乐任务统一在一个范式下，并通过创新的预训练LM和适配器方法实现了卓越的性能，证明了“音乐到音乐”建模的有效性和高效性。", "translation": "许多音乐AI模型学习音乐内容与人类定义标签之间的映射。然而，许多注释，例如和弦，可以在音乐模态本身中自然表达，例如作为符号音符序列。这一观察使得理解任务（例如和弦识别）和条件生成任务（例如和弦条件旋律生成）都可以在“音乐到音乐”序列建模范式下统一。在这项工作中，我们为各种符号音乐到音乐任务提出了参数高效的解决方案。其高级思想是 (1) 我们对参考序列和目标序列都利用预训练语言模型（LM），以及 (2) 我们通过一个轻量级适配器连接这两个LM。实验表明，我们的方法在和弦识别、旋律生成和鼓点生成等不同任务中均取得了优越的性能。所有演示、代码和模型权重均已公开。", "summary": "本文提出了一种创新的“音乐到音乐”序列建模范式，旨在统一多种符号音乐任务，包括理解和生成。该方法利用预训练语言模型处理参考和目标音乐序列，并通过一个轻量级适配器连接，实现了参数高效的解决方案。实验证明，该方法在和弦识别、旋律和鼓点生成等任务上表现出色，为音乐AI领域提供了一个多功能且高效的新途径。", "keywords": "音乐到音乐建模, 符号音乐, 语言模型, 参数高效, 序列生成", "comments": "这篇论文的创新点在于将传统上依赖人工标签的音乐AI任务重新定义为“音乐到音乐”的序列建模问题，这是一种更自然且统一的视角。通过利用预训练语言模型和引入轻量级适配器，实现了参数高效且性能优越的模型，这对于实际应用具有重要意义。代码和模型权重的公开也促进了研究的可复现性和进一步发展。"}}
{"id": "2506.15030", "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "summary": "Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.", "comment": "22 pages, 2 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15030v1", "AI": {"title_translation": "使用主题建模和文本分类方法识别NVDRS文本叙述中的社会隔离主题", "tldr": "本研究利用自然语言处理和机器学习方法，从美国国家暴力死亡报告系统（NVDRS）的文本叙述中识别社会隔离主题，并发现与自杀相关的风险因素。", "motivation": "近年来社会隔离和孤独感日益增加，对自杀率有显著影响。尽管美国国家暴力死亡报告系统（NVDRS）的结构化变量中未记录社会隔离和孤独感，但自然语言处理技术可用于从执法和法医叙述中识别这些概念。", "method": "研究利用主题建模进行词汇开发，并采用监督学习分类器来识别NVDRS文本叙述中的社会隔离主题。", "result": "开发出高质量的分类器（平均F1：0.86，准确率：0.82）。在评估2002年至2020年间超过30万例自杀事件后，识别出1198例提及慢性社会隔离。男性（OR = 1.44）、同性恋者（OR = 3.68）或离异者（OR = 3.34）被分类为慢性社会隔离的几率更高。还发现了其他社会隔离主题的重要预测因素，如近期或即将离婚、失去子女监护权、被驱逐或近期搬家以及分手。", "conclusion": "该方法可以改善美国社会隔离和孤独感的监测和预防。", "translation": "近年来，社会隔离和孤独感日益增加，并极大地导致自杀率上升。尽管美国国家暴力死亡报告系统（NVDRS）的结构化变量目前尚未记录社会隔离和孤独感，但自然语言处理（NLP）技术可用于在执法和法医叙述中识别这些概念。我们利用主题建模生成词汇，并使用监督学习分类器开发了高质量的分类器（平均F1：0.86，准确率：0.82）。在评估2002年至2020年间超过30万例自杀事件后，我们识别出1198例提及慢性社会隔离。如果死者是男性（OR = 1.44；CI：1.24，1.69，p<.0001）、同性恋者（OR = 3.68；1.97，6.33，p<.0001）或离异者（OR = 3.34；2.68，4.19，p<.0001），则其被分类为慢性社会隔离的几率更高。我们还发现了其他社会隔离主题的重要预测因素，包括近期或即将离婚、失去子女监护权、被驱逐或近期搬家以及分手。我们的方法可以改善美国社会隔离和孤独感的监测和预防。", "summary": "本研究利用自然语言处理技术，特别是主题建模和监督学习分类器，从美国国家暴力死亡报告系统（NVDRS）的文本叙述中识别与自杀相关的社会隔离主题。研究开发了高效的分类器，并在大量自杀案例中识别出慢性社会隔离的实例，同时揭示了与社会隔离相关的关键人口统计学和社会因素，如性别、性取向、婚姻状况以及其他生活事件。该方法有望改进对社会隔离和孤独感的监测和预防。", "keywords": "社会隔离, 自杀, 自然语言处理, 主题建模, 文本分类, NVDRS", "comments": "这项研究通过将先进的自然语言处理技术应用于非结构化文本数据，成功地从现有系统中提取了关键的公共卫生信息，具有重要的创新性。它弥补了NVDRS在社会隔离数据记录方面的不足，为自杀预防提供了新的监测工具和风险因素洞察。其局限性可能在于依赖文本叙述的完整性和准确性，以及模型泛化到其他数据集的能力。"}}
{"id": "2506.14800", "title": "A micromorphic-based artificial diffusion method for stabilized finite element approximation of convection-diffusion problems", "authors": ["Soheil Firooz", "B. Daya Reddy", "Paul Steinmann"], "summary": "We present a novel artificial diffusion method to circumvent the\ninstabilities associated with the standard finite element approximation of\nconvection-diffusion equations. Motivated by the micromorphic approach, we\nintroduce an auxiliary variable, which is related to the gradient of the field\nof interest, and which leads to a coupled problem. Conditions for\nwell-posedness of the resulting formulation are established. We carry out a\ncomprehensive numerical study to compare the proposed methodology against some\nwell-established approaches in one- and two-dimensional settings. The proposed\nmethod outperforms established approaches in general in approximating\naccurately the solutions to pertinent and challenging problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14800v1", "AI": {"title_translation": "一种基于微观形态的人工扩散方法，用于对流-扩散问题的稳定有限元近似", "tldr": "本文提出了一种新颖的基于微观形态的人工扩散方法，用于稳定对流-扩散方程的有限元近似，通过引入辅助变量解决不稳定性问题，并在数值研究中表现优于现有方法。", "motivation": "标准有限元近似对流-扩散方程时存在不稳定性问题，需要开发新的方法来规避这些不稳定性并准确近似解。", "method": "该方法基于微观形态方法，引入了一个与感兴趣场梯度相关的辅助变量，从而形成一个耦合问题。论文建立了所得公式的适定性条件，并通过全面的数值研究与现有方法进行比较。", "result": "所提出的方法在近似相关和具有挑战性问题的解时，总体上优于现有方法。", "conclusion": "基于微观形态的人工扩散方法能够有效解决对流-扩散方程有限元近似的不稳定性问题，并能更准确地近似解。", "translation": "我们提出了一种新颖的人工扩散方法，以规避与对流-扩散方程标准有限元近似相关的不稳定性。受微观形态方法的启发，我们引入了一个辅助变量，该变量与感兴趣场的梯度相关，并导致一个耦合问题。文中建立了所得公式的适定性条件。我们进行了一项全面的数值研究，以在一维和二维环境下将所提出的方法与一些成熟的方法进行比较。所提出的方法在准确近似相关和具有挑战性问题的解方面，总体上优于现有方法。", "summary": "本文提出了一种新颖的基于微观形态的人工扩散方法，旨在解决对流-扩散方程标准有限元近似中的不稳定性问题。该方法引入了一个与场梯度相关的辅助变量，形成一个耦合系统，并建立了其适定性条件。通过数值研究，该方法在准确性方面优于现有方法。", "keywords": "人工扩散, 有限元, 对流-扩散, 微观形态, 稳定性", "comments": "该论文提出了一种创新的方法来解决对流-扩散问题中的数值不稳定性，其基于微观形态的思路引入辅助变量是其核心创新点。数值研究结果表明其优越性，这对于计算流体力学和传热等领域具有重要意义。"}}
{"id": "2506.15100", "title": "International Security Applications of Flexible Hardware-Enabled Guarantees", "authors": ["Onni Aarne", "James Petrie"], "summary": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees\n(flexHEGs) offer opportunities to address international security challenges\nthrough comprehensive governance frameworks. This report examines how flexHEGs\ncould enable internationally trustworthy AI governance by establishing\nstandardized designs, robust ecosystem defenses, and clear operational\nparameters for AI-relevant chips. We analyze four critical international\nsecurity applications: limiting proliferation to address malicious use,\nimplementing safety norms to prevent loss of control, managing risks from\nmilitary AI systems, and supporting strategic stability through\nbalance-of-power mechanisms while respecting national sovereignty. The report\nexplores both targeted deployments for specific high-risk facilities and\ncomprehensive deployments covering all AI-relevant compute. We examine two\nprimary governance models: verification-based agreements that enable\ntransparent compliance monitoring, and ruleset-based agreements that\nautomatically enforce international rules through cryptographically-signed\nupdates. Through game-theoretic analysis, we demonstrate that comprehensive\nflexHEG agreements could remain stable under reasonable assumptions about state\npreferences and catastrophic risks. The report addresses critical\nimplementation challenges including technical thresholds for AI-relevant chips,\nmanagement of existing non-flexHEG hardware, and safeguards against abuse of\ngovernance power. While requiring significant international coordination,\nflexHEGs could provide a technical foundation for managing AI risks at the\nscale and speed necessary to address emerging threats to international security\nand stability.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15100v1", "AI": {"title_translation": "灵活硬件支持保障的国际安全应用", "tldr": "本报告探讨了灵活硬件支持保障（flexHEGs）如何通过标准化设计、强大的生态系统防御和明确的操作参数，为人工智能相关芯片建立国际可信赖的人工智能治理，以应对国际安全挑战。", "motivation": "随着人工智能能力的迅速发展，灵活硬件支持保障（flexHEGs）为通过全面的治理框架解决国际安全挑战提供了机会。", "method": "本报告分析了flexHEGs如何通过建立标准化设计、强大的生态系统防御和明确的操作参数来实现国际可信赖的人工智能治理。报告分析了四个关键的国际安全应用，并探讨了两种主要的治理模型：基于验证的协议和基于规则集的协议。通过博弈论分析，报告证明了全面的flexHEG协议在合理假设下可以保持稳定。报告还讨论了实施挑战。", "result": "报告分析了flexHEGs在限制恶意使用、实施安全规范、管理军事AI系统风险以及通过权力平衡机制支持战略稳定方面的应用。研究了两种治理模型（基于验证和基于规则集），并通过博弈论分析表明全面的flexHEG协议在合理假设下可以保持稳定。", "conclusion": "flexHEGs可以为管理人工智能风险提供技术基础，其规模和速度足以应对国际安全和稳定面临的新兴威胁，尽管这需要大量的国际协调。", "translation": "随着人工智能能力的迅速发展，灵活硬件支持保障（flexHEGs）为通过全面的治理框架解决国际安全挑战提供了机会。本报告探讨了flexHEGs如何通过建立标准化设计、强大的生态系统防御和明确的操作参数，为人工智能相关芯片建立国际可信赖的人工智能治理。我们分析了四个关键的国际安全应用：限制扩散以应对恶意使用、实施安全规范以防止失控、管理军事人工智能系统带来的风险，以及通过权力平衡机制支持战略稳定同时尊重国家主权。报告探讨了针对特定高风险设施的定向部署和涵盖所有人工智能相关计算的全面部署。我们研究了两种主要的治理模型：基于验证的协议，实现透明的合规性监控；以及基于规则集的协议，通过加密签名的更新自动执行国际规则。通过博弈论分析，我们证明了在对国家偏好和灾难性风险的合理假设下，全面的flexHEG协议可以保持稳定。报告讨论了关键的实施挑战，包括人工智能相关芯片的技术门槛、现有非flexHEG硬件的管理，以及防止治理权力滥用的保障措施。尽管需要大量的国际协调，但flexHEGs可以为管理人工智能风险提供技术基础，其规模和速度足以应对国际安全和稳定面临的新兴威胁。", "summary": "本报告探讨了灵活硬件支持保障（flexHEGs）在国际安全领域的应用，旨在通过为人工智能相关芯片建立标准化设计和操作参数，实现国际可信赖的人工智能治理。报告分析了flexHEGs在限制恶意AI、确保AI安全、管理军事AI风险和维持战略稳定方面的潜力，并提出了基于验证和基于规则集的两种治理模型。通过博弈论分析，研究表明全面的flexHEG协议在合理假设下是稳定的，并且能够应对新兴的国际安全威胁，尽管实施上存在挑战且需要国际协调。", "keywords": "灵活硬件支持保障, 国际安全, 人工智能治理, 战略稳定, 博弈论", "comments": "该论文提出了一种新颖的技术解决方案（flexHEGs）来应对人工智能带来的国际安全挑战，其创新点在于将硬件层面的保障与国际治理框架相结合。其重要性体现在为AI风险管理提供了一个可行的技术基础，特别是在军备控制和防止AI滥用方面。论文还考虑了实施中的挑战，显示了其务实性。"}}
{"id": "2506.15453", "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.", "comment": "6 pages, 3 figures, 4 tables, conference paper", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15453v1", "AI": {"title_translation": "通过大型语言模型驱动的代码片段描述生成来揭示意图", "tldr": "本研究探讨了开发人员常用的代码片段描述类型，并评估了LLM（Llama）在生成这些描述方面的能力。结果显示LLM能有效识别描述类型，但生成描述的相似度仍有提升空间。", "motivation": "代码片段文档对于开发者和用户理解关键区域至关重要，特别是第三方库。随着大型语言模型（LLMs）的兴起，研究开发者常用的描述类型以及LLM生成这些描述的能力成为关键目标。", "method": "研究使用了包含185,412个包和1,024,579个代码片段的NPM代码片段数据集，并从中抽取了400个代码片段及其描述作为样本。首先进行人工分类，然后使用Llama模型进行描述生成，并与原始描述进行对比分析。", "result": "1. 人工分类发现，大多数原始描述（55.5%）侧重于基于示例的用法。2. LLM正确识别出大多数原始描述为“示例”（79.75%），与人工分类结果一致。3. 生成的描述与原始描述的平均相似度得分为0.7173，表明相关性但仍有改进空间（低于0.9表示不相关）。", "conclusion": "研究结果表明，根据代码片段的任务不同，文档的意图可能有所差异，包括使用说明、安装指南或供库用户学习的描述性示例。", "translation": "文档化代码片段对于开发者和用户理解关键领域至关重要。示例包括用法示例和其他应用程序编程接口（API），这对于第三方库尤其重要。随着大型语言模型（LLM）的兴起，关键目标是调查开发者通常使用的描述类型，并评估LLM（在本例中是Llama）支持描述生成的能力。我们使用NPM代码片段数据集，该数据集包含185,412个包和1,024,579个代码片段。从中，我们使用400个代码片段（及其描述）作为样本。首先，我们的人工分类发现，大多数原始描述（55.5%）强调基于示例的用法。这一发现强调了清晰文档的重要性，因为某些描述缺乏足够的细节来传达意图。其次，LLM正确地将大多数原始描述识别为“示例”（79.75%），这与我们的人工发现相同，显示出泛化的倾向。第三，与原始描述相比，生成的描述的平均相似度得分为0.7173，表明相关性但仍有改进空间。低于0.9的分数表示存在一些不相关性。我们的结果表明，根据代码片段的任务不同，文档的意图可能与使用说明、安装或任何库用户的描述性学习示例不同。", "summary": "本研究旨在探究开发者常用的代码片段描述类型，并评估大型语言模型（LLM，具体为Llama）在生成这些描述方面的能力。研究使用了包含大量NPM代码片段的数据集，并抽取400个样本进行人工分类和LLM生成测试。结果发现，大多数原始描述侧重于示例用法，且LLM能有效识别描述类型。尽管LLM生成的描述与原始描述具有一定的相似性，但仍有提升空间，表明代码片段的意图可能因任务而异。", "keywords": "大型语言模型, 代码片段, 描述生成, 文档, 意图", "comments": "该论文探索了LLM在代码片段描述生成这一实用领域的应用，具有一定的创新性。其发现LLM在识别描述类型方面表现良好，但在生成内容相似度上仍有提升空间，这为未来研究指明了方向。研究强调了清晰文档的重要性，并揭示了不同代码片段任务可能对应的不同文档意图，对软件工程实践有指导意义。然而，相似度分数（0.7173）与理想值（0.9）之间仍存在差距，表明LLM在生成高质量、精确的意图描述方面仍面临挑战。"}}
{"id": "2506.15008", "title": "Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output", "authors": ["Richa Gupta", "Alexander Htet Kyaw"], "summary": "Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.", "comment": "15 Pages, 6 figures, CAAD Futures 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15008v1", "AI": {"title_translation": "洞察驱动的生成式AI设计：将真实世界数据融入文本到图像输出", "tldr": "本文提出了一种将DALL-E 3与材料数据集整合的新型流程，通过提供可持续性指标和材料使用洞察，丰富了AI生成的室内设计，旨在帮助设计师做出更环保的决策，并评估了其在用户测试中的效果。", "motivation": "尽管生成式AI（特别是文本到图像模型）在将概念转化为视觉表现方面具有革命性，但其生成的图像往往缺乏对设计师有用的可操作数据，尤其是在可持续性方面。", "method": "本文提出了一种将DALL-E 3与材料数据集整合的新型流程。模型生成室内设计图像后，后处理模块会识别出前十种存在的材料，并将其与通用材料字典中的二氧化碳当量（CO2e）值配对。通过三项用户测试评估系统：1) 在提示前不提可持续性；2) 在提示前沟通可持续性目标；3) 在提示前沟通可持续性目标并包含CO2e量化数据。", "result": "定性和定量分析表明，在第三项测试中引入可持续性指标，能促使设计师做出更明智的决策，但也可能引发决策疲劳并降低总体满意度。尽管如此，大多数参与者在第三项测试中表示将可持续性原则纳入了他们的工作流程。", "conclusion": "研究结果强调了平衡设计自由与实际约束的重要性，为AI辅助建筑设计中实现整体性、数据驱动的解决方案提供了清晰的路径，旨在引导更具生态责任感的设计实践。", "translation": "生成式AI，特别是文本到图像模型，通过将概念想法从简单的文本提示快速转化为视觉表现，彻底改变了室内建筑设计。尽管生成式AI可以生成视觉上吸引人的图像，但它们通常缺乏设计师可操作的数据。在这项工作中，我们提出了一种新颖的流程，将DALL-E 3与材料数据集集成，以可持续性指标和材料使用洞察来丰富AI生成的设计。在模型生成室内设计图像后，一个后处理模块会识别出前十种存在的材料，并将其与通用材料字典中的二氧化碳当量（CO2e）值配对。这种方法允许设计师立即评估环境影响并相应地调整提示。我们通过三项用户测试评估了该系统：（1）在与生成式AI的提示过程之前，未向用户提及可持续性；（2）在提示之前向用户传达可持续性目标；以及（3）在传达可持续性目标的同时，在生成式AI输出中包含量化的CO2e数据。我们的定性和定量分析显示，在第三项测试中引入可持续性指标会导致更明智的设计决策，但它也可能引发决策疲劳并降低总体满意度。尽管如此，大多数参与者表示在第三项测试中将可持续性原则纳入了他们的工作流程，这突显了集成指标在指导更具生态责任感的实践方面的潜力。我们的研究结果展示了平衡设计自由与实际约束的重要性，为AI辅助建筑设计中实现整体性、数据驱动的解决方案提供了清晰的路径。", "summary": "本文提出了一种创新的生成式AI设计流程，将DALL-E 3与真实世界材料数据集集成，旨在为室内设计提供可持续性指标和材料使用洞察。通过后处理模块，AI生成的图像能够显示材料的CO2e值，帮助设计师评估环境影响。用户测试表明，引入可持续性数据能促使更明智的决策，尽管可能导致决策疲劳，但大多数用户仍倾向于采纳可持续性原则，证明了该方法在平衡设计自由与实际约束、推动生态负责型设计方面的潜力。", "keywords": "生成式AI, 文本到图像, 可持续设计, 室内建筑, 材料数据", "comments": "该论文的创新点在于将真实世界的、可量化的可持续性数据（如CO2e值）整合到生成式AI的设计输出中，这超越了传统生成式AI仅关注视觉美学的局限。它为设计师提供了即时可操作的环境影响反馈，有助于推动更负责任的设计实践。然而，研究也揭示了潜在的局限性，即信息过载可能导致决策疲劳，这提示未来研究需探索更优化、更人性化的数据呈现方式，以在提供洞察与保持用户体验之间取得平衡。其重要性在于为AI在可持续设计领域的应用开辟了新途径。"}}
{"id": "2506.15032", "title": "Assigning Multi-Robot Tasks to Multitasking Robots", "authors": ["Winston Smith", "Andrew Boateng", "Taha Shaheen", "Yu Zhang"], "summary": "One simplifying assumption in existing and well-performing task allocation\nmethods is that the robots are single-tasking: each robot operates on a single\ntask at any given time. While this assumption is harmless to make in some\nsituations, it can be inefficient or even infeasible in others. In this paper,\nwe consider assigning multi-robot tasks to multitasking robots. The key\ncontribution is a novel task allocation framework that incorporates the\nconsideration of physical constraints introduced by multitasking. This is in\ncontrast to the existing work where such constraints are largely ignored. After\nformulating the problem, we propose a compilation to weighted MAX-SAT, which\nallows us to leverage existing solvers for a solution. A more efficient greedy\nheuristic is then introduced. For evaluation, we first compare our methods with\na modern baseline that is efficient for single-tasking robots to validate the\nbenefits of multitasking in synthetic domains. Then, using a site-clearing\nscenario in simulation, we further illustrate the complex task interaction\nconsidered by the multitasking robots in our approach to demonstrate its\nperformance. Finally, we demonstrate a physical experiment to show how\nmultitasking enabled by our approach can benefit task efficiency in a realistic\nsetting.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15032v1", "AI": {"title_translation": "将多机器人任务分配给多任务机器人", "tldr": "本文提出了一种新的任务分配框架，用于将多机器人任务分配给多任务机器人，并考虑了物理约束，通过编译为加权MAX-SAT和贪婪启发式算法求解，并在模拟和物理实验中验证了其有效性。", "motivation": "现有任务分配方法通常假设机器人是单任务的，这在某些情况下会导致效率低下或不可行。", "method": "提出了一种新的任务分配框架，该框架考虑了多任务处理引入的物理约束。将问题形式化为加权MAX-SAT，并利用现有求解器求解。此外，还引入了一种更高效的贪婪启发式算法。", "result": "通过与单任务机器人基线方法比较，在合成领域验证了多任务处理的优势。在现场清理模拟中，展示了多任务机器人在处理复杂任务交互方面的性能。物理实验也表明该方法使多任务处理提高了任务效率。", "conclusion": "本文成功提出了一种考虑物理约束的多机器人任务分配框架，并证明了多任务处理在提高任务效率方面的益处。", "translation": "现有且表现良好的任务分配方法的一个简化假设是机器人是单任务的：每个机器人在任何给定时间只执行一个任务。虽然这个假设在某些情况下是无害的，但在其他情况下可能效率低下甚至不可行。在本文中，我们考虑将多机器人任务分配给多任务机器人。关键贡献是一个新颖的任务分配框架，它纳入了多任务处理引入的物理约束的考虑。这与现有工作中很大程度上忽略此类约束的情况形成对比。在问题形式化之后，我们提出了一种编译到加权MAX-SAT的方法，这使我们能够利用现有求解器来获得解决方案。然后引入了一种更高效的贪婪启发式算法。为了评估，我们首先将我们的方法与一种对单任务机器人高效的现代基线进行比较，以验证多任务处理在合成领域中的优势。然后，在模拟中的现场清理场景中，我们进一步说明了我们方法中多任务机器人所考虑的复杂任务交互，以展示其性能。最后，我们演示了一个物理实验，以展示我们的方法实现的多任务处理如何在现实环境中提高任务效率。", "summary": "本文针对现有任务分配方法中机器人被假设为单任务的局限性，提出了一种新颖的多机器人任务分配框架，旨在将多机器人任务分配给能够同时处理多个任务的机器人。该框架创新性地考虑了多任务处理带来的物理约束，并通过将问题编译为加权MAX-SAT和引入贪婪启发式算法来求解。通过合成域、模拟和物理实验的评估，证明了所提方法在提高任务效率方面的优势和有效性。", "keywords": "多机器人任务分配, 多任务机器人, 物理约束, MAX-SAT, 贪婪启发式", "comments": "这篇论文的创新点在于打破了传统任务分配中机器人单任务的假设，引入了多任务机器人的概念，并首次将多任务处理带来的物理约束纳入到任务分配框架中。这对于提升多机器人系统的效率和灵活性具有重要意义。通过结合精确的MAX-SAT求解和高效的贪婪启发式算法，提供了理论和实践相结合的解决方案。"}}
{"id": "2506.15412", "title": "Golden Partition Zone: Rethinking Neural Network Partitioning Under Inversion Threats in Collaborative Inference", "authors": ["Rongke Liu", "Youwen Zhu"], "summary": "In collaborative inference, intermediate features transmitted from edge\ndevices can be exploited by adversaries to reconstruct original inputs via\nmodel inversion attacks (MIA). While existing defenses focus on shallow-layer\nprotection, they often incur significant utility loss. A key open question is\nhow to partition the edge-cloud model to maximize resistance to MIA while\nminimizing accuracy degradation. We first show that increasing model depth\nalone does not guarantee resistance. Through theoretical analysis, we\ndemonstrate that representational transitions in neural networks cause sharp\nchanges in conditional entropy $H(x\\mid z)$, with intra-class variance (denoted\n$R_c^2$) and feature dimensionality as critical factors. Experiments on three\nrepresentative deep vision models demonstrate that splitting at the\nrepresentational-transition or decision-level layers increases mean squared\nerror by more than four times compared to shallow splits, indicating\nsignificantly stronger resistance to MIA. Positive label smoothing further\nenhances robustness by compressing $R_c^2$ and improving generalization.\nFinally, we validate the resilience of decision-level features under enhanced\ninversion models and observe that the type of auxiliary data influences both\ntransition boundaries and reconstruction behavior.", "comment": "8 pages, 11 figures, 5 tables", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15412v1", "AI": {"title_translation": "黄金分割区：协作推理中反演威胁下神经网络划分的再思考", "tldr": "在协作推理中，为抵御模型反演攻击（MIA），本研究提出将神经网络在表征转换层或决策级层进行深度划分。理论分析和实验表明，这种划分能显著提高MIA抵抗力，且正标签平滑能进一步增强鲁棒性。", "motivation": "在协作推理中，边缘设备传输的中间特征易受模型反演攻击（MIA）威胁，导致原始输入被重建。现有防御措施主要集中于浅层保护，但往往造成显著的效用损失。因此，关键问题是如何划分边缘-云模型，以最大限度地抵抗MIA，同时最大限度地减少精度下降。", "method": "研究首先指出，仅增加模型深度并不能保证对MIA的抵抗力。通过理论分析，证明了神经网络中的表征转换导致条件熵$H(x\\mid z)$的急剧变化，其中类内方差（$R_c^2$）和特征维度是关键因素。在三个代表性深度视觉模型上进行实验，探究在表征转换层或决策级层进行分割的效果。此外，研究还探索了正标签平滑对鲁棒性的影响，并验证了决策级特征在增强型反演模型下的弹性，同时观察了辅助数据类型对转换边界和重建行为的影响。", "result": "仅仅增加模型深度并不能保证对MIA的抵抗力。理论分析表明，神经网络中的表征转换会导致条件熵$H(x\\mid z)$的急剧变化，其中类内方差($R_c^2$)和特征维度是关键因素。在表征转换层或决策级层进行分割，与浅层分割相比，均方误差增加了四倍以上，显著增强了对MIA的抵抗力。正标签平滑通过压缩$R_c^2$和提高泛化能力，进一步增强了鲁棒性。决策级特征在增强型反演模型下显示出弹性。辅助数据的类型会影响转换边界和重建行为。", "conclusion": "在协作推理中，将神经网络在表征转换层或决策级层进行深度划分，能够显著增强对模型反演攻击的抵抗力。正标签平滑进一步提高了模型的鲁棒性。这项研究强调了划分策略和辅助数据在缓解反演威胁方面的重要性。", "translation": "在协作推理中，从边缘设备传输的中间特征可能被攻击者利用，通过模型反演攻击（MIA）重建原始输入。虽然现有防御措施侧重于浅层保护，但它们通常会造成显著的效用损失。一个关键的开放问题是如何划分边缘-云模型，以最大限度地抵抗MIA，同时最大限度地减少精度下降。我们首先表明，仅仅增加模型深度并不能保证抵抗力。通过理论分析，我们证明了神经网络中的表征转换导致条件熵 $H(x\\mid z)$ 的急剧变化，其中类内方差（表示为 $R_c^2$）和特征维度是关键因素。在三个代表性深度视觉模型上的实验表明，在表征转换层或决策级层进行分割，与浅层分割相比，均方误差增加了四倍以上，这表明对MIA的抵抗力显著增强。正标签平滑通过压缩 $R_c^2$ 和提高泛化能力，进一步增强了鲁棒性。最后，我们验证了决策级特征在增强型反演模型下的弹性，并观察到辅助数据的类型会影响转换边界和重建行为。", "summary": "本论文旨在解决协作推理中模型反演攻击（MIA）的挑战，即中间特征可能泄露原始输入的问题。针对现有浅层防御措施的不足，作者提出了一种新颖的划分策略。通过理论分析和实验验证，研究表明在表征转换层或决策级层进行模型分割，能显著提高对MIA的抵抗力（均方误差增加四倍以上），优于浅层分割。此外，正标签平滑通过压缩类内方差，进一步增强了模型的鲁棒性。研究结果强调了深度层划分和辅助数据在保障协作推理安全中的重要性。", "keywords": "协作推理, 模型反演攻击, 神经网络划分, 表征转换, 正标签平滑", "comments": "该论文为协作推理中神经网络划分以抵御反演攻击提供了新颖的视角，超越了传统的浅层防御。其将条件熵与表征转换相结合的理论分析，以及经验验证，对保护AI隐私做出了重要贡献。发现正标签平滑能够增强鲁棒性也提供了一个实用的见解。"}}
{"id": "2506.14830", "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "summary": "Aiming at the critical role of SSD health state prediction in data\nreliability assurance, this study proposes a hybrid BiGRU-MHA model that\nincorporates a multi-head attention mechanism to enhance the accuracy and\nstability of storage device health classification. The model innovatively\nintegrates temporal feature extraction and key information focusing\ncapabilities. Specifically, it leverages the bidirectional timing modeling\nadvantages of the BiGRU network to capture both forward and backward\ndependencies of SSD degradation features. Simultaneously, the multi-head\nattention mechanism dynamically assigns feature weights, improving the model's\nsensitivity to critical health indicators. Experimental results show that the\nproposed model achieves classification accuracies of 92.70% on the training set\nand 92.44% on the test set, with a minimal performance gap of only 0.26%,\ndemonstrating excellent generalization ability. Further analysis using the\nreceiver operating characteristic (ROC) curve shows an area under the curve\n(AUC) of 0.94 on the test set, confirming the model's robust binary\nclassification performance. This work not only presents a new technical\napproach for SSD health prediction but also addresses the generalization\nbottleneck of traditional models, offering a verifiable method with practical\nvalue for preventive maintenance of industrial-grade storage systems. The\nresults show the model can significantly reduce data loss risks by providing\nearly failure warnings and help optimize maintenance costs, supporting\nintelligent decision-making in building reliable storage systems for cloud\ncomputing data centers and edge storage environments.", "comment": "Source code available; Accepted by 2025 6th International Conference\n  on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14830v1", "AI": {"title_translation": "基于多头注意力机制的双向门控循环单元SSD健康状态分类模型优化", "tldr": "本研究提出了一种混合BiGRU-MHA模型用于SSD健康状态分类，实现了高准确性和鲁棒的泛化能力。", "motivation": "为了解决固态硬盘（SSD）健康状态预测在数据可靠性保障中的关键作用，研究旨在提高存储设备健康分类的准确性和稳定性。", "method": "本研究提出了一种混合BiGRU-MHA模型，该模型结合了多头注意力机制，创新性地整合了时间特征提取和关键信息聚焦能力。具体而言，它利用BiGRU网络捕获SSD降级特征的前向和后向依赖关系，同时多头注意力机制动态分配特征权重，提高模型对关键健康指标的敏感性。", "result": "所提出的模型在训练集上实现了92.70%的分类准确率，在测试集上实现了92.44%的分类准确率，性能差距仅为0.26%，表现出卓越的泛化能力。ROC曲线分析显示，在测试集上的AUC为0.94，证实了模型强大的二元分类性能。", "conclusion": "该工作为SSD健康预测提供了一种新的技术方法，解决了传统模型的泛化瓶颈，为工业级存储系统的预防性维护提供了一种可验证且具有实际价值的方法。模型能显著降低数据丢失风险，优化维护成本，支持在构建可靠存储系统时的智能决策。", "translation": "针对固态硬盘（SSD）健康状态预测在数据可靠性保障中的关键作用，本研究提出了一种混合BiGRU-MHA模型，该模型结合了多头注意力机制，以提高存储设备健康分类的准确性和稳定性。该模型创新性地整合了时间特征提取和关键信息聚焦能力。具体而言，它利用BiGRU网络的双向时间建模优势来捕获SSD降级特征的前向和后向依赖关系。同时，多头注意力机制动态分配特征权重，提高了模型对关键健康指标的敏感性。实验结果表明，所提出的模型在训练集上实现了92.70%的分类准确率，在测试集上实现了92.44%的分类准确率，性能差距仅为0.26%，表现出卓越的泛化能力。使用接收者操作特征（ROC）曲线的进一步分析显示，在测试集上的曲线下面积（AUC）为0.94，证实了模型强大的二元分类性能。这项工作不仅为SSD健康预测提供了一种新的技术方法，还解决了传统模型的泛化瓶颈，为工业级存储系统的预防性维护提供了一种可验证且具有实际价值的方法。结果表明，该模型可以通过提供早期故障预警来显著降低数据丢失风险，并有助于优化维护成本，支持在构建云计算数据中心和边缘存储环境中可靠存储系统时的智能决策。", "summary": "本文提出了一种新颖的混合BiGRU-MHA模型，该模型结合了BiGRU用于时间特征提取和多头注意力机制用于聚焦关键信息，以增强SSD健康状态分类。该模型在测试集上实现了92.44%的高准确率，训练集与测试集之间仅有0.26%的性能差距，展现出强大的泛化能力，为存储系统的预防性维护和风险降低提供了实用的解决方案。", "keywords": "SSD健康预测, BiGRU, 多头注意力, 分类, 泛化", "comments": "本文提出了一种创新的混合模型（BiGRU-MHA），有效地结合了序列数据处理和注意力机制，用于SSD健康预测。其优势在于解决了传统模型常见的泛化问题，并展示了在工业级存储系统中的实际应用价值。训练集和测试集之间较低的性能差距突出了其鲁棒性以及在云计算和边缘环境中实际部署的潜力。"}}
{"id": "2506.15454", "title": "Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA", "authors": ["Nizar ALHafez", "Ahmad Kurdi"], "summary": "This paper presents a comprehensive comparison of three dominant parallel\nprogramming models in High Performance Computing (HPC): Message Passing\nInterface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device\nArchitecture (CUDA). Selecting optimal programming approaches for modern\nheterogeneous HPC architectures has become increasingly critical. We\nsystematically analyze these models across multiple dimensions: architectural\nfoundations, performance characteristics, domain-specific suitability,\nprogramming complexity, and recent advancements. We examine each model's\nstrengths, weaknesses, and optimization techniques. Our investigation\ndemonstrates that MPI excels in distributed memory environments with\nnear-linear scalability for communication-intensive applications, but faces\ncommunication overhead challenges. OpenMP provides strong performance and\nusability in shared-memory systems and loop-centric tasks, though it is limited\nby shared memory contention. CUDA offers substantial performance gains for\ndata-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires\nspecialized expertise. Performance evaluations across scientific simulations,\nmachine learning, and data analytics reveal that hybrid approaches combining\ntwo or more models often yield optimal results in heterogeneous environments.\nThe paper also discusses implementation challenges, optimization best\npractices, and emerging trends such as performance portability frameworks,\ntask-based programming, and the convergence of HPC and Big Data. This research\nhelps developers and researchers make informed decisions when selecting\nprogramming models for modern HPC applications, emphasizing that the best\nchoice depends on application requirements, hardware, and development\nconstraints.", "comment": "10 pages", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15454v1", "AI": {"title_translation": "现代高性能计算中的并行范式：MPI、OpenMP和CUDA的比较分析", "tldr": "本文对高性能计算中三种主流并行编程模型（MPI、OpenMP和CUDA）进行了全面比较，分析了它们的优缺点及优化技术，并指出在异构环境中混合方法常能获得最佳性能，旨在帮助开发者选择合适的编程模型。", "motivation": "为现代异构高性能计算（HPC）架构选择最优的编程方法变得越来越关键。", "method": "本文系统地分析了MPI、OpenMP和CUDA这三种并行编程模型，涵盖了它们的架构基础、性能特征、领域适用性、编程复杂性和最新进展。研究审查了每种模型的优缺点和优化技术，并通过科学模拟、机器学习和数据分析进行了性能评估。", "result": "MPI在分布式内存环境中表现出色，对于通信密集型应用具有近乎线性的可扩展性，但面临通信开销挑战。OpenMP在共享内存系统和以循环为中心的任务中提供强大的性能和可用性，但受限于共享内存争用。CUDA为数据并行GPU工作负载提供了显著的性能提升，但仅限于NVIDIA GPU并需要专业知识。性能评估表明，在异构环境中，结合两种或更多模型的混合方法通常能产生最佳结果。", "conclusion": "最佳的编程模型选择取决于应用程序需求、硬件和开发限制。", "translation": "本文对高性能计算（HPC）中三种主流并行编程模型：消息传递接口（MPI）、开放多处理（OpenMP）和统一计算设备架构（CUDA）进行了全面比较。为现代异构HPC架构选择最优的编程方法变得越来越关键。我们系统地从多个维度分析了这些模型：架构基础、性能特征、领域特定适用性、编程复杂性和最新进展。我们审查了每种模型的优点、缺点和优化技术。我们的研究表明，MPI在分布式内存环境中表现出色，对于通信密集型应用具有近乎线性的可扩展性，但面临通信开销挑战。OpenMP在共享内存系统和以循环为中心的任务中提供强大的性能和可用性，但受限于共享内存争用。CUDA为数据并行GPU工作负载提供了显著的性能提升，但仅限于NVIDIA GPU并需要专业知识。在科学模拟、机器学习和数据分析领域的性能评估表明，在异构环境中，结合两种或更多模型的混合方法通常能产生最佳结果。本文还讨论了实现挑战、优化最佳实践以及性能可移植性框架、基于任务的编程和HPC与大数据融合等新兴趋势。这项研究有助于开发人员和研究人员在为现代HPC应用程序选择编程模型时做出明智的决策，强调最佳选择取决于应用程序需求、硬件和开发限制。", "summary": "本文对高性能计算（HPC）中主流的MPI、OpenMP和CUDA并行编程模型进行了深入的比较分析。研究从架构、性能、适用性、复杂性等多维度评估了它们的优缺点和优化策略。结果显示，MPI擅长分布式内存，OpenMP适用于共享内存，CUDA则在GPU数据并行任务中表现卓越，但各有其局限。研究强调，在异构HPC环境中，结合多种模型的混合方法常能达到最优性能。论文还探讨了实现挑战、优化实践和新兴趋势，旨在为开发者在选择HPC编程模型时提供决策指导，指出最佳选择需综合考虑应用需求、硬件和开发限制。", "keywords": "HPC, 并行编程, MPI, OpenMP, CUDA", "comments": "本文通过对MPI、OpenMP和CUDA三大并行编程范式的全面比较，为HPC领域的开发者提供了宝贵的决策依据。其创新之处在于系统性地从多个维度进行分析，并明确指出混合编程模型在现代异构环境中的优越性，这对于指导实际应用开发具有重要意义。文章不仅罗列了各模型的特点，还深入探讨了其局限性、优化技巧以及未来趋势，体现了较高的实用价值。"}}
{"id": "2506.14832", "title": "ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Zixuan Dai", "Miao Zhang", "Shuai Lu"], "summary": "In contemporary architectural design, the growing complexity and diversity of\ndesign demands have made generative plugin tools essential for quickly\nproducing initial concepts and exploring novel 3D forms. However, objectively\nanalyzing the differences between human-designed and machine-generated 3D forms\nremains a challenge, limiting our understanding of their respective strengths\nand hindering the advancement of generative tools.\n  To address this, we built ArchForms-4000, a dataset containing 2,000\narchitect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet,\na 3D convolutional neural network tailored for classifying and analyzing\narchitectural forms, incorporating a saliency module to highlight key spatial\nfeatures aligned with architectural reasoning; And conducted comparative\nexperiments showing our model outperforms human experts in distinguishing form\norigins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.\n  This study not only highlights the distinctive advantages of human-designed\nforms in spatial organization, proportional harmony, and detail refinement but\nalso provides valuable insights for enhancing generative design tools in the\nfuture.", "comment": "22 pages, 8 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14832v1", "AI": {"title_translation": "ArchShapeNet：一个用于评估建筑形状的可解释3D-CNN框架", "tldr": "该研究提出了ArchShapeNet，一个可解释的3D卷积神经网络，用于区分人类设计和机器生成的建筑三维形式，并在性能上超越了人类专家。", "motivation": "当代建筑设计中，生成式插件工具虽然能快速生成概念和探索新颖的3D形式，但客观分析人类设计和机器生成3D形式之间的差异仍然是挑战，这限制了对它们各自优点的理解，并阻碍了生成工具的进步。", "method": "研究构建了ArchForms-4000数据集，包含2000个建筑师设计和2000个Evomass生成的3D形式；提出了ArchShapeNet，一个专门用于分类和分析建筑形式的3D卷积神经网络，并融入了显著性模块以突出与建筑推理相符的关键空间特征；进行了比较实验。", "result": "该模型在区分形式来源方面表现优于人类专家，准确率达到94.29%，精确率96.2%，召回率98.51%。研究还突出了人类设计形式在空间组织、比例协调和细节完善方面的独特优势。", "conclusion": "本研究不仅强调了人类设计形式在空间组织、比例和谐和细节细化方面的独特优势，而且为未来增强生成设计工具提供了宝贵的见解。", "translation": "在当代建筑设计中，设计需求的日益复杂性和多样性使得生成式插件工具对于快速产生初始概念和探索新颖的3D形式变得至关重要。然而，客观分析人类设计和机器生成的3D形式之间的差异仍然是一个挑战，这限制了我们对其各自优点的理解，并阻碍了生成工具的进步。\n为解决这一问题，我们构建了ArchForms-4000数据集，其中包含2000个建筑师设计和2000个Evomass生成的3D形式；提出了ArchShapeNet，一个专门用于分类和分析建筑形式的3D卷积神经网络，并融入了显著性模块以突出与建筑推理相符的关键空间特征；并进行了比较实验，结果表明我们的模型在区分形式来源方面优于人类专家，达到了94.29%的准确率、96.2%的精确率和98.51%的召回率。\n这项研究不仅突出了人类设计形式在空间组织、比例协调和细节完善方面的独特优势，而且为未来增强生成设计工具提供了宝贵的见解。", "summary": "本研究旨在解决建筑设计中人类设计与机器生成3D形式之间客观分析的难题。为此，研究构建了包含4000个建筑形式的ArchForms-4000数据集，并提出了ArchShapeNet，一个带有显著性模块的可解释3D卷积神经网络。该模型在区分形式来源上表现出色，准确率达94.29%，并超越了人类专家。研究结果不仅揭示了人类设计形式的独特优势，也为未来生成设计工具的改进提供了方向。", "keywords": "建筑形状, 3D卷积神经网络, 可解释性, 生成设计, 形式评估", "comments": "该论文通过构建特定数据集和提出可解释的3D-CNN框架，有效地解决了建筑设计领域中一个重要的实际问题，即如何客观评估和区分人类与机器生成的建筑形式。其创新性在于结合了深度学习与可解释性，为理解AI在创意设计中的作用提供了新的视角。模型的优异表现证明了其在自动化评估方面的潜力，同时其对人类设计优势的强调也为未来AI辅助设计指明了方向。"}}
{"id": "2506.15567", "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents", "authors": ["Aline Dobrovsky", "Konstantin Schekotihin", "Christian Burmer"], "summary": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15567v1", "AI": {"title_translation": "使用基于LLM的推理和行动代理管理复杂的故障分析工作流程", "tldr": "本文提出了一种基于大型语言模型（LLM）的规划代理（LPA），通过整合LLM、规划能力和外部工具，自动化并管理复杂的故障分析（FA）工作流程。", "motivation": "故障分析（FA）是一个高度复杂且知识密集的过程。尽管AI组件能够自动化FA中的各种任务（如图像检测、案例检索和报告生成），但随着部署的AI模型数量增加，如何将这些组件协调成与FA过程无缝集成的连贯高效的工作流程，是一个巨大的挑战。", "method": "本文研究并实现了基于大型语言模型（LLM）的规划代理（LPA）。该LPA将LLM与高级规划能力和外部工具利用相结合，使其能够自主处理复杂查询、从外部系统检索相关数据并生成人类可读的响应。", "result": "评估结果表明，该代理在支持故障分析（FA）任务方面具有操作有效性和可靠性。", "conclusion": "基于LLM的规划代理（LPA）能够有效且可靠地支持故障分析（FA）任务，展示了LLM驱动代理在管理复杂工作流程方面的潜力。", "translation": "故障分析 (FA) 是一个高度复杂且知识密集的过程。将 AI 组件整合到 FA 实验室的计算基础设施中，有潜力自动化各种任务，包括图像中不合格品的检测、从不同数据源检索类似案例以及从带注释的图像生成报告。然而，随着部署的 AI 模型数量的增加，挑战在于将这些组件协调成与 FA 过程无缝集成的连贯高效的工作流程。\n本文研究了基于大型语言模型 (LLM) 的规划代理 (LPA) 的设计和实现，以协助 FA 工程师解决其分析案例。LPA 将 LLM 与高级规划能力和外部工具利用相结合，能够自主处理复杂查询、从外部系统检索相关数据以及生成人类可读的响应。评估结果表明该代理在支持 FA 任务方面的操作有效性和可靠性。", "summary": "本文旨在解决故障分析（FA）中AI组件日益增多带来的工作流编排挑战。研究提出并实现了一种基于大型语言模型（LLM）的规划代理（LPA），该代理结合了LLM、高级规划能力和外部工具利用。LPA能够自主处理复杂查询、检索相关数据并生成响应，评估结果证实了其在支持FA任务中的操作有效性和可靠性。", "keywords": "故障分析, LLM, 规划代理, 工作流管理, AI自动化", "comments": "该论文解决了AI集成中的一个实际挑战，即复杂工作流程的编排。在故障分析这一特定领域中，利用LLM进行规划和工具使用是一种创新方法，有望管理复杂的、多组件的AI系统。这可能为其他领域中类似复杂工作流程的自动化树立先例。"}}
{"id": "2506.14789", "title": "AZT1D: A Real-World Dataset for Type 1 Diabetes", "authors": ["Saman Khamesian", "Asiful Arefeen", "Bithika M. Thompson", "Maria Adela Grando", "Hassan Ghasemzadeh"], "summary": "High quality real world datasets are essential for advancing data driven\napproaches in type 1 diabetes (T1D) management, including personalized therapy\ndesign, digital twin systems, and glucose prediction models. However, progress\nin this area has been limited by the scarcity of publicly available datasets\nthat offer detailed and comprehensive patient data. To address this gap, we\npresent AZT1D, a dataset containing data collected from 25 individuals with T1D\non automated insulin delivery (AID) systems. AZT1D includes continuous glucose\nmonitoring (CGM) data, insulin pump and insulin administration data,\ncarbohydrate intake, and device mode (regular, sleep, and exercise) obtained\nover 6 to 8 weeks for each patient. Notably, the dataset provides granular\ndetails on bolus insulin delivery (i.e., total dose, bolus type, correction\nspecific amounts) features that are rarely found in existing datasets. By\noffering rich, naturalistic data, AZT1D supports a wide range of artificial\nintelligence and machine learning applications aimed at improving clinical\ndecision making and individualized care in T1D.", "comment": "4 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14789v1", "AI": {"title_translation": "AZT1D：一个真实世界1型糖尿病数据集", "tldr": "AZT1D是一个新的真实世界1型糖尿病数据集，包含来自25名使用自动化胰岛素输送（AID）系统患者的详细数据，旨在弥补现有公开数据集的不足，支持AI/ML应用以改善1型糖尿病管理。", "motivation": "现有公开的1型糖尿病数据集缺乏详细和全面的患者数据，限制了数据驱动方法（如个性化治疗、数字孪生、血糖预测）在1型糖尿病管理中的进展。", "method": "本文收集了来自25名使用自动化胰岛素输送（AID）系统的1型糖尿病患者的数据，持续6到8周。数据集包括连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及设备模式（常规、睡眠、运动），并特别提供了现有数据集中罕见的推注胰岛素输送的详细信息（总剂量、推注类型、校正具体量）。", "result": "成果是AZT1D数据集，它提供了丰富、自然的数据，支持广泛的人工智能和机器学习应用，旨在改善1型糖尿病的临床决策和个体化护理。", "conclusion": "AZT1D数据集通过提供详细的真实世界数据，有助于改进1型糖尿病的临床决策和个体化护理。", "translation": "高质量的真实世界数据集对于推进1型糖尿病（T1D）管理中的数据驱动方法至关重要，包括个性化治疗设计、数字孪生系统和血糖预测模型。然而，由于缺乏提供详细和全面患者数据的公开可用数据集，该领域的进展受到了限制。为了弥补这一空白，我们提出了AZT1D，这是一个包含从25名使用自动化胰岛素输送（AID）系统的T1D患者收集的数据集。AZT1D包括连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及每位患者在6到8周内获得的设备模式（常规、睡眠和运动）。值得注意的是，该数据集提供了推注胰岛素输送的详细粒度信息（即总剂量、推注类型、校正具体量），这些特征在现有数据集中很少发现。通过提供丰富、自然的数据，AZT1D支持广泛的人工智能和机器学习应用，旨在改善T1D中的临床决策和个体化护理。", "summary": "本文介绍了AZT1D，一个为解决现有1型糖尿病真实世界数据集不足而创建的新数据集。该数据集包含了25名使用自动化胰岛素输送系统患者的连续血糖监测、胰岛素给药、碳水化合物摄入及设备模式等详细数据，尤其提供了现有数据集罕见的推注胰岛素输送的精细信息。AZT1D旨在支持AI/ML应用，以改进1型糖尿病的临床决策和个性化护理。", "keywords": "1型糖尿病, 真实世界数据, 数据集, 自动化胰岛素输送, 血糖监测", "comments": "该论文的创新之处在于创建了一个高质量、详细且全面的真实世界1型糖尿病数据集，尤其弥补了现有数据集中推注胰岛素信息不足的空白。其重要性在于为个性化治疗、数字孪生和血糖预测等数据驱动方法提供了急需的资源，有望显著推动1型糖尿病管理领域的AI和ML应用。"}}
{"id": "2506.15047", "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi", "Dong Whi Yoo", "Keran Wang", "Violeta J. Rodriguez", "Ravi Karkar", "Koustuv Saha"], "summary": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15047v1", "AI": {"title_translation": "将看护者需求映射到AI聊天机器人设计：阿尔茨海默病和痴呆症看护者心理健康支持的优势与不足", "tldr": "本研究开发并评估了一个基于GPT-4o的聊天机器人Carey，以了解阿尔茨海默病看护者对AI心理健康支持的需求和看法，并提出设计建议。", "motivation": "阿尔茨海默病及相关痴呆症（AD/ADRD）看护者面临巨大的情感和后勤挑战，导致高风险的压力、焦虑和抑郁。尽管生成式AI（特别是大型语言模型LLMs）为心理健康支持提供了新机会，但目前对看护者如何看待和使用此类技术知之甚少。", "method": "研究开发了基于GPT-4o的聊天机器人Carey，旨在为AD/ADRD看护者提供信息和情感支持。以Carey作为技术探测器，研究人员在基于常见看护压力源的情景驱动互动后，对16名家庭看护者进行了半结构化访谈。通过归纳编码和反思性主题分析，系统地理解了看护者的需求和期望，并识别了看护者愿望和担忧中的细微紧张关系。", "result": "研究揭示了看护者需求的六个主题：按需信息获取、情感支持、安全披露空间、危机管理、个性化和数据隐私。对于每个主题，都识别了看护者愿望和担忧之间的细微紧张关系。研究还提出了看护者需求、AI聊天机器人优势、不足和设计建议的映射。", "conclusion": "研究结果为设计主动、值得信赖和以看护者为中心的AI系统提供了理论和实践见解，这些系统能更好地支持AD/ADRD看护者不断变化的心理健康需求。", "translation": "家庭看护阿尔茨海默病及相关痴呆症（AD/ADRD）患者的看护者面临着巨大的情感和后勤挑战，使他们面临更高的压力、焦虑和抑郁风险。尽管生成式人工智能（特别是大型语言模型，LLMs）的最新进展为支持心理健康提供了新机会，但对看护者如何看待和使用此类技术知之甚少。为了弥补这一空白，我们开发了Carey，一个基于GPT-4o的聊天机器人，旨在为AD/ADRD看护者提供信息和情感支持。我们以Carey作为技术探测器，在以常见看护压力源为基础的情景驱动互动后，对16名家庭看护者进行了半结构化访谈。通过归纳编码和反思性主题分析，我们系统地理解了看护者在六个主题上的需求和期望——按需信息获取、情感支持、安全披露空间、危机管理、个性化和数据隐私。对于每个主题，我们还识别了看护者愿望和担忧中的细微紧张关系。我们提出了看护者需求、AI聊天机器人优势、不足和设计建议的映射。我们的发现为设计主动、值得信赖和以看护者为中心的AI系统提供了理论和实践见解，这些系统能更好地支持AD/ADRD看护者不断变化的心理健康需求。", "summary": "本研究开发并评估了一个基于GPT-4o的聊天机器人Carey，旨在为阿尔茨海默病及相关痴呆症看护者提供心理健康支持。通过对16名看护者的半结构化访谈，研究识别了看护者在信息获取、情感支持、隐私保护等方面的六大核心需求，并揭示了AI聊天机器人在满足这些需求方面的优势与不足。研究结果为未来设计更符合看护者需求、更值得信赖的AI系统提供了理论和实践指导。", "keywords": "看护者需求, AI聊天机器人, 阿尔茨海默病, 心理健康支持, GPT-4o", "comments": "这篇论文通过实证研究，将看护者的具体需求与AI聊天机器人设计相结合，具有重要的实践意义。它不仅识别了AI在心理健康支持中的潜力，也指出了需要改进的领域，特别是对数据隐私和个性化的关注，为未来AI辅助看护系统的发展提供了清晰的方向。其创新点在于将LLM技术应用于特定且高压的看护群体，并深入挖掘了用户感知和期望。"}}
{"id": "2506.15192", "title": "Microgrid Operation Control with Adaptable Droop Gains", "authors": ["E. D. Gomez Anccas", "C. A. Hans", "D. Schulz"], "summary": "Modern low-carbon power systems come with many challenges, such as increased\ninverter penetration and increased uncertainty from renewable sources and\nloads. In this context, the microgrid concept is a promising approach, which is\nbased on a segmentation of the grid into independent smaller cells that can run\neither in grid-connected or standalone mode.In microgrids, droop control is\nwidely used for primary control. It enables proportional power sharing,\ndepending on the droop gains. Operation control schemes considering droop\ncontrol often assume fixed droop gains. However, using adaptive droop gains for\ngrid-forming units allow to shape power sharing in presence of fluctuations,\nenhancing flexibility while maintaining a safe microgrid operation,\nparticularly under uncertainty. This work introduces a bilinear formulation for\nmicrogrid operation control that finds optimal power setpoints and droop gains\non a timescale of minutes by solving a finite horizon optimization problem. In\ndetail, a robust minmax model predictive control scheme is designed for a\nstandalone microgrid, comprising a fuel cell, a photovoltaic system and an\nenergy storage. Closed-loop simulations are performed with and without variable\ndroop gains. The results show an increase in renewable utilization of up to 7.5\n% while reducing the power output of the fuel cell by 6 %, when allowing\nvariable droop gains.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15192v1", "AI": {"title_translation": "具有自适应下垂增益的微电网运行控制", "tldr": "本文提出一种具有自适应下垂增益的双线性微电网运行控制方法，通过鲁棒最小最大模型预测控制，提高了可再生能源利用率并降低了燃料电池输出。", "motivation": "现代低碳电力系统面临逆变器渗透率增加、可再生能源和负荷不确定性增加的挑战。微电网概念是解决这些问题的一种有前途的方法，其中下垂控制被广泛用于一次控制。然而，传统的下垂控制方案通常假设固定的下垂增益，这限制了在波动存在时功率共享的灵活性。因此，需要一种能够利用自适应下垂增益来塑造功率共享并增强灵活性的操作控制方案。", "method": "本文引入了一种用于微电网运行控制的双线性公式，通过解决一个有限视界优化问题，在数分钟的时间尺度上找到最优功率设定点和下垂增益。具体来说，设计了一种针对独立微电网的鲁棒最小最大模型预测控制方案，该微电网包含燃料电池、光伏系统和储能设备。", "result": "闭环仿真结果表明，在允许可变下垂增益的情况下，可再生能源利用率提高了高达7.5%，同时燃料电池的功率输出降低了6%。", "conclusion": "自适应下垂增益的引入能够显著提高微电网中可再生能源的利用率，并优化传统能源的运行，从而增强了微电网在不确定性下的灵活性和安全性。", "translation": "现代低碳电力系统面临诸多挑战，例如逆变器渗透率增加以及可再生能源和负荷的不确定性增加。在此背景下，微电网概念是一种有前景的方法，它基于将电网划分为独立的更小单元，这些单元可以以并网或独立模式运行。在微电网中，下垂控制广泛用于一次控制。它根据下垂增益实现比例功率共享。考虑下垂控制的运行控制方案通常假定固定的下垂增益。然而，对并网形成单元使用自适应下垂增益允许在存在波动的情况下形成功率共享，从而增强灵活性，同时保持微电网的安全运行，尤其是在不确定性下。这项工作引入了一种用于微电网运行控制的双线性公式，通过解决一个有限视界优化问题，在数分钟的时间尺度上找到最优功率设定点和下垂增益。具体来说，为独立微电网设计了一种鲁棒最小最大模型预测控制方案，该微电网包括燃料电池、光伏系统和储能。在有和没有可变下垂增益的情况下进行了闭环仿真。结果表明，在允许可变下垂增益时，可再生能源利用率提高了高达7.5%，同时燃料电池的功率输出降低了6%。", "summary": "本文提出一种具有自适应下垂增益的微电网运行控制方法，旨在解决现代电力系统中的不确定性和灵活性挑战。研究引入了双线性公式，并设计了鲁棒最小最大模型预测控制方案，用于优化独立微电网的功率设定点和下垂增益。通过闭环仿真，结果显示该方法能显著提高可再生能源利用率，并降低燃料电池的功率输出，从而增强了微电网在不确定性下的运行性能。", "keywords": "微电网, 下垂控制, 自适应增益, 模型预测控制, 可再生能源", "comments": "本文的创新点在于引入了自适应下垂增益的概念，并将其融入到微电网运行控制的双线性公式和鲁棒模型预测控制框架中。这对于提高微电网在不确定性下的灵活性和可再生能源利用率具有重要意义。通过量化的仿真结果，论文清晰地展示了其方法的有效性。"}}
{"id": "2506.15228", "title": "ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression", "authors": ["Yufeng Zhang", "Wenrui Dai", "Hang Yu", "Shizhan Liu", "Junhui Hou", "Jianguo Li", "Weiyao Lin"], "summary": "Neural Image Compression (NIC) has revolutionized image compression with its\nsuperior rate-distortion performance and multi-task capabilities, supporting\nboth human visual perception and machine vision tasks. However, its widespread\nadoption is hindered by substantial computational demands. While existing\napproaches attempt to address this challenge through module-specific\noptimizations or pre-defined complexity levels, they lack comprehensive control\nover computational complexity. We present ABC (Adaptive BayesNet structure\nlearning for computational scalable multi-task image Compression), a novel,\ncomprehensive framework that achieves computational scalability across all NIC\ncomponents through Bayesian network (BayesNet) structure learning. ABC\nintroduces three key innovations: (i) a heterogeneous bipartite BayesNet\n(inter-node structure) for managing neural backbone computations; (ii) a\nhomogeneous multipartite BayesNet (intra-node structure) for optimizing\nautoregressive unit processing; and (iii) an adaptive control module that\ndynamically adjusts the BayesNet structure based on device capabilities, input\ndata complexity, and downstream task requirements. Experiments demonstrate that\nABC enables full computational scalability with better complexity adaptivity\nand broader complexity control span, while maintaining competitive compression\nperformance. Furthermore, the framework's versatility allows integration with\nvarious NIC architectures that employ BayesNet representations, making it a\nrobust solution for ensuring computational scalability in NIC applications.\nCode is available in https://github.com/worldlife123/cbench_BaSIC.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15228v1", "AI": {"title_translation": "ABC：面向计算可扩展多任务图像压缩的自适应贝叶斯网络结构学习", "tldr": "神经图像压缩（NIC）计算需求高，ABC通过贝叶斯网络结构学习实现NIC的计算可扩展性，同时保持性能。", "motivation": "神经图像压缩（NIC）虽然性能优越并支持多任务，但其广泛应用受限于巨大的计算需求。现有方法缺乏对计算复杂度的全面控制。", "method": "本文提出了ABC（自适应贝叶斯网络结构学习，用于计算可扩展多任务图像压缩）框架，通过贝叶斯网络（BayesNet）结构学习实现所有NIC组件的计算可扩展性。ABC包含三项关键创新：(i)一个异构二分贝叶斯网络（节点间结构）用于管理神经网络骨干计算；(ii)一个同构多部贝叶斯网络（节点内结构）用于优化自回归单元处理；(iii)一个自适应控制模块，根据设备能力、输入数据复杂性和下游任务需求动态调整贝叶斯网络结构。", "result": "实验表明，ABC实现了全面的计算可扩展性，具有更好的复杂度适应性和更广的复杂度控制范围，同时保持了有竞争力的压缩性能。", "conclusion": "ABC框架为确保神经图像压缩（NIC）应用的计算可扩展性提供了一个鲁棒的解决方案，并且其通用性使其能够与采用贝叶斯网络表示的各种NIC架构集成。", "translation": "神经图像压缩（NIC）以其卓越的码率失真性能和多任务能力（支持人类视觉感知和机器视觉任务）彻底改变了图像压缩领域。然而，其广泛采用受到巨大计算需求的阻碍。尽管现有方法试图通过特定模块优化或预定义复杂度级别来解决这一挑战，但它们缺乏对计算复杂度的全面控制。我们提出了ABC（自适应贝叶斯网络结构学习，用于计算可扩展多任务图像压缩），这是一个新颖、全面的框架，通过贝叶斯网络（BayesNet）结构学习，在所有NIC组件中实现计算可扩展性。ABC引入了三项关键创新：(i)用于管理神经网络骨干计算的异构二分贝叶斯网络（节点间结构）；(ii)用于优化自回归单元处理的同构多部贝叶斯网络（节点内结构）；以及(iii)一个自适应控制模块，根据设备能力、输入数据复杂性和下游任务需求动态调整贝叶斯网络结构。实验表明，ABC实现了全面的计算可扩展性，具有更好的复杂度适应性和更广的复杂度控制范围，同时保持了有竞争力的压缩性能。此外，该框架的通用性使其能够与采用贝叶斯网络表示的各种NIC架构集成，使其成为确保NIC应用计算可扩展性的一个鲁棒解决方案。代码可在https://github.com/worldlife123/cbench_BaSIC获取。", "summary": "本文提出ABC框架，旨在解决神经图像压缩（NIC）面临的巨大计算需求问题。通过引入贝叶斯网络结构学习，ABC实现了对NIC所有组件的计算可扩展性控制。该框架包含异构和同构贝叶斯网络分别用于管理神经网络骨干和自回归单元，并结合自适应控制模块动态调整结构以适应不同条件。实验证明，ABC在保持竞争性压缩性能的同时，显著提升了NIC的计算可扩展性、适应性和控制范围，并展现出与多种NIC架构的良好兼容性。", "keywords": "神经图像压缩, 计算可扩展性, 贝叶斯网络, 结构学习, 多任务", "comments": "本文的创新点在于首次将贝叶斯网络结构学习应用于神经图像压缩（NIC）的计算可扩展性控制，并提出了多层次的贝叶斯网络结构设计（节点间和节点内）以及一个自适应控制模块。这种方法实现了对NIC所有组件的全面计算复杂度管理，显著提高了模型的实用性和部署灵活性，为NIC在资源受限环境下的应用提供了重要的解决方案。"}}
{"id": "2506.15273", "title": "Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access", "authors": ["Anup Mishra", "Čedomir Stefanović", "Xiuqiang Xu", "Petar Popovski", "Israel Leyva-Mayorga"], "summary": "Flexible and efficient wireless resource sharing across heterogeneous\nservices is a key objective for future wireless networks. In this context, we\ninvestigate the performance of a system where latency-constrained\ninternet-of-things (IoT) devices coexist with a broadband user. The base\nstation adopts a grant-free access framework to manage resource allocation,\neither through orthogonal radio access network (RAN) slicing or by allowing\nshared access between services. For the IoT users, we propose a reinforcement\nlearning (RL) approach based on double Q-Learning (QL) to optimise their\nrepetition-based transmission strategy, allowing them to adapt to varying\nlevels of interference and meet a predefined latency target. We evaluate the\nsystem's performance in terms of the cumulative distribution function of IoT\nusers' latency, as well as the broadband user's throughput and energy\nefficiency (EE). Our results show that the proposed RL-based access policies\nsignificantly enhance the latency performance of IoT users in both RAN Slicing\nand RAN Sharing scenarios, while preserving desirable broadband throughput and\nEE. Furthermore, the proposed policies enable RAN Sharing to be\nenergy-efficient at low IoT traffic levels, and RAN Slicing to be favourable\nunder high IoT traffic.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15273v1", "AI": {"title_translation": "基于强化学习的异构无线接入策略优化", "tldr": "本文提出了一种基于强化学习的双Q学习方法，用于优化异构无线网络中物联网设备的传输策略，以提高其延迟性能并保持宽带用户的性能。", "motivation": "未来无线网络需要灵活高效地在异构服务间共享无线资源。本文研究了延迟受限的物联网设备与宽带用户共存的系统，旨在优化资源分配以满足物联网设备的延迟要求并保持宽带性能。", "method": "基站采用免授权接入框架进行资源分配，方式包括正交无线接入网络(RAN)切片或服务间共享接入。针对物联网用户，提出了一种基于双Q学习(QL)的强化学习(RL)方法来优化其基于重复的传输策略，使其能够适应不同干扰水平并满足预定义的延迟目标。", "result": "所提出的基于RL的接入策略显著提升了物联网用户在RAN切片和RAN共享场景下的延迟性能，同时保持了理想的宽带吞吐量和能效。此外，所提出的策略使得RAN共享在低物联网流量时能效高，而RAN切片在高物联网流量时更有利。", "conclusion": "本文提出的基于强化学习的策略能够有效优化异构无线网络中的资源分配，显著改善物联网设备的延迟性能，并能根据物联网流量水平灵活选择RAN切片或RAN共享方案，同时保持宽带用户的良好性能。", "translation": "未来无线网络的一个关键目标是跨异构服务实现灵活高效的无线资源共享。在此背景下，我们研究了一个延迟受限的物联网（IoT）设备与宽带用户共存的系统性能。基站采用免授权接入框架来管理资源分配，这可以通过正交无线接入网络（RAN）切片或允许服务间共享接入来实现。对于物联网用户，我们提出了一种基于双Q学习（QL）的强化学习（RL）方法，以优化其基于重复的传输策略，使其能够适应不同程度的干扰并满足预定义的延迟目标。我们通过物联网用户延迟的累积分布函数以及宽带用户的吞吐量和能效（EE）来评估系统性能。我们的结果表明，所提出的基于RL的接入策略显著增强了物联网用户在RAN切片和RAN共享场景下的延迟性能，同时保持了理想的宽带吞吐量和能效。此外，所提出的策略使得RAN共享在低物联网流量水平下具有能源效率，而RAN切片在高物联网流量下更为有利。", "summary": "本文研究了异构无线网络中延迟受限物联网设备与宽带用户共存的资源共享问题。基站采用免授权接入，通过RAN切片或共享接入进行资源分配。为优化物联网设备的传输策略以满足延迟要求，本文提出了一种基于双Q学习的强化学习方法。实验结果表明，该RL策略显著提升了物联网用户延迟性能，同时保持了宽带用户吞吐量和能效，并揭示了不同物联网流量下RAN切片和RAN共享的适用性。", "keywords": "强化学习, 异构无线接入, 物联网, 双Q学习, 资源分配", "comments": "本文的创新点在于将强化学习（特别是双Q学习）应用于异构无线网络中的资源分配和传输策略优化，以解决物联网设备的延迟要求和宽带用户的性能维护之间的冲突。这种自适应的策略优化方法对于未来复杂多样的无线通信场景具有重要意义，尤其是在动态干扰和流量变化的环境中。研究结果为RAN切片和RAN共享在不同流量条件下的应用提供了实用指导。"}}
{"id": "2506.15612", "title": "A survey of Chernoff and Hoeffding bounds", "authors": ["Alexandros V. Gerbessiotis"], "summary": "This is a survey paper that discusses the original bounds of the seminal\npapers by Chernoff and Hoeffding. Moreover, it includes a variety of derivative\nbounds in a variety of forms. Complete proofs are provided as needed. The\nintent is to provide a repository of reference bounds for the interested\nresearcher.", "comment": null, "cate": "cs.DM", "url": "http://arxiv.org/abs/2506.15612v1", "AI": {"title_translation": "Chernoff 和 Hoeffding 界限的综述", "tldr": "对 Chernoff 和 Hoeffding 界限及其衍生形式的综述，旨在提供参考。", "motivation": "为感兴趣的研究人员提供一个关于 Chernoff 和 Hoeffding 界限及其衍生形式的参考库。", "method": "通过讨论 Chernoff 和 Hoeffding 的原始界限，并包含各种形式的衍生界限，必要时提供完整的证明。", "result": "提供了一个关于 Chernoff 和 Hoeffding 界限及其衍生形式的参考集合，包含完整证明。", "conclusion": "该综述旨在为研究人员提供一个全面的 Chernoff 和 Hoeffding 界限的参考资源。", "translation": "这是一篇综述论文，讨论了 Chernoff 和 Hoeffding 经典论文中的原始界限。此外，它还包括各种形式的多种衍生界限。根据需要提供了完整的证明。目的是为感兴趣的研究人员提供一个参考界限的存储库。", "summary": "本文综述了 Chernoff 和 Hoeffding 的原始界限及其多种衍生形式，并提供了必要的完整证明，旨在为研究人员提供一个全面的参考资源库。", "keywords": "Chernoff 界限, Hoeffding 界限, 综述, 概率不等式, 统计学", "comments": "该论文作为综述性文章，其创新性在于系统性地整理和归纳了 Chernoff 和 Hoeffding 界限及其衍生形式，并提供了证明，对于需要快速查阅和理解这些重要概率不等式的研究人员具有很高的实用价值。其局限性在于作为综述，不涉及新的理论发现。"}}
{"id": "2506.15614", "title": "TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "summary": "This paper presents TTSOps, a fully automated closed-loop framework for\nconstructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated\nweb-scale speech data, often referred to as ``dark data,'' such as online\nvideos. Conventional TTS training pipelines require well-curated corpora with\nhigh acoustic quality and accurate text-speech alignment, which severely limits\nscalability, speaker diversity, and real-world applicability. While recent\nstudies have proposed acoustic-quality-based data selection techniques, they\noften overlook two critical aspects: (1) the inherent robustness of modern TTS\nmodels to noise, and (2) the potential contribution of perceptually low-quality\nyet informative samples. To address these issues, TTSOps introduces a\ndata-centric training pipeline that integrates three core components: (1)\nautomated data collection from dark data sources, (2) utterance-level dynamic\nselection of data cleansing methods based on training data quality, and (3)\nevaluation-in-the-loop data selection using automatically predicted mean\nopinion scores (MOS) to estimate each utterance's impact on model performance.\nFurthermore, TTSOps jointly optimizes the corpus and the TTS model in a\nclosed-loop framework by dynamically adapting both data selection and data\ncleansing processes to the characteristics of the target TTS model. Extensive\nexperiments on Japanese YouTube data demonstrate that TTSOps outperforms\nconventional acoustic-quality-based baselines in both the naturalness and\nspeaker diversity of synthesized speech.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15614v1", "AI": {"title_translation": "TTSOps：一个用于从“暗数据”训练多说话人TTS模型的闭环语料库优化框架", "tldr": "TTSOps是一个自动化闭环框架，用于从嘈杂的“暗数据”构建多说话人TTS系统，通过动态数据选择和清洗以及基于模型性能的评估来优化语料库，超越了传统方法。", "motivation": "传统TTS训练需要高质量、对齐良好的语料库，这限制了可扩展性、说话人多样性和实际应用。现有基于声学质量的数据选择方法忽略了现代TTS模型对噪声的鲁棒性以及感知上低质量但信息丰富的样本的潜在贡献。", "method": "TTSOps引入了一个以数据为中心的训练流程，包含三个核心组件：1) 从“暗数据”源自动收集数据；2) 基于训练数据质量的语料级动态数据清洗方法选择；3) 使用自动预测的MOS进行评估循环数据选择，以估计每个语料对模型性能的影响。它通过动态调整数据选择和清洗过程来联合优化语料库和TTS模型。", "result": "在日本YouTube数据上的大量实验表明，TTSOps在合成语音的自然度和说话人多样性方面均优于传统的基于声学质量的基线方法。", "conclusion": "TTSOps框架通过其数据中心训练流程、动态数据选择和清洗以及评估循环优化，有效地从嘈杂的“暗数据”中构建高质量的多说话人TTS系统，解决了传统方法的局限性。", "translation": "本文介绍了TTSOps，一个全自动闭环框架，用于从嘈杂、未经整理的网络规模语音数据（通常称为“暗数据”，如在线视频）构建多说话人文本到语音（TTS）系统。传统的TTS训练流程需要高质量声学和准确文本-语音对齐的精心整理语料库，这严重限制了可扩展性、说话人多样性和实际适用性。尽管最近的研究提出了基于声学质量的数据选择技术，但它们往往忽视了两个关键方面：（1）现代TTS模型固有的噪声鲁棒性，以及（2）感知上低质量但信息丰富的样本的潜在贡献。为了解决这些问题，TTSOps引入了一个以数据为中心的训练流程，该流程整合了三个核心组件：（1）从“暗数据”源自动收集数据，（2）基于训练数据质量的语料级动态数据清洗方法选择，以及（3）使用自动预测的平均意见得分（MOS）进行评估循环数据选择，以估计每个语料对模型性能的影响。此外，TTSOps通过动态调整数据选择和数据清洗过程以适应目标TTS模型的特性，在一个闭环框架中联合优化语料库和TTS模型。在日本YouTube数据上的大量实验表明，TTSOps在合成语音的自然度和说话人多样性方面均优于传统的基于声学质量的基线。", "summary": "TTSOps是一个创新的闭环框架，旨在解决从嘈杂的“暗数据”训练多说话人TTS模型所面临的挑战。它通过自动化数据收集、动态数据清洗选择以及基于模型性能评估的数据选择来优化语料库，从而克服了传统方法对高质量、精心整理数据的依赖。该框架能够联合优化语料库和TTS模型，并在实验中表现出优于传统方法的自然度和说话人多样性。", "keywords": "TTS, 暗数据, 语料库优化, 多说话人TTS, 闭环框架", "comments": "TTSOps的创新之处在于其全自动闭环设计，能够有效利用大量“暗数据”来训练TTS模型，这极大地扩展了TTS技术的应用范围和可扩展性。其对“感知上低质量但信息丰富样本”的重视以及将模型鲁棒性纳入考虑，是其优于传统数据选择方法的关键。通过将评估循环集成到数据选择中，确保了数据对模型性能的直接贡献，是其方法论上的亮点。"}}
{"id": "2506.15068", "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation", "authors": ["Zongxia Li", "Yapei Chang", "Yuhang Zhou", "Xiyang Wu", "Zichao Liang", "Yoo Yeon Sung", "Jordan Lee Boyd-Graber"], "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15068v1", "AI": {"title_translation": "自由形式生成中开放式R1训练的语义感知奖励", "tldr": "本文提出PrefBERT模型，一个语义感知的奖励模型，用于解决开放式长文本生成评估的挑战，并通过提供更好的语义奖励反馈来优化GRPO训练，使生成内容更符合人类偏好。", "motivation": "评估开放式长文本生成具有挑战性，因为很难明确区分好坏输出。现有方法常忽略连贯性、风格或相关性等关键方面，或者受到预训练数据的偏差影响，导致该问题研究不足。", "method": "本文提出了PrefBERT，一个评分模型，用于评估GRPO中的开放式长文本生成，并通过区分好坏输出的奖励来指导其训练。PrefBERT在两个包含多样长文本风格和Likert评级质量的响应评估数据集上进行训练，旨在提供比传统指标ROUGE-L和BERTScore更好的语义奖励反馈。", "result": "PrefBERT在多句和段落长度响应上训练后，在各种长篇段落中保持可靠性，并与GRPO所需的验证性奖励良好对齐。通过包括LLM-as-a-judge、人类评级和定性分析在内的全面评估，结果表明PrefBERT优于传统指标。人类评估证实，使用PrefBERT作为奖励信号来训练策略模型，产生的响应比使用传统指标训练的响应更符合人类偏好。", "conclusion": "PrefBERT作为一种语义感知的奖励模型，有效解决了开放式长文本生成评估的挑战，并能显著提升生成内容的质量和人类偏好对齐度。", "translation": "评估开放式长文本生成具有挑战性，因为很难明确区分好坏输出。现有方法常常忽略连贯性、风格或相关性等关键方面，或者受到预训练数据的偏见影响，使得开放式长文本评估成为一个尚未充分探索的问题。为了弥补这一空白，我们提出了PrefBERT，一个评分模型，用于评估GRPO中的开放式长文本生成，并通过对好坏输出的不同奖励来指导其训练。PrefBERT在两个包含多样长文本风格和Likert评级质量的响应评估数据集上进行训练，比传统的ROUGE-L和BERTScore指标提供了更好的语义奖励反馈，有效地支持了GRPO。通过包括LLM作为评判、人工评分和定性分析在内的全面评估，我们表明在多句和段落长度响应上训练的PrefBERT，在各种长篇段落中保持可靠性，并与GRPO所需的验证性奖励良好对齐。人类评估证实，使用PrefBERT作为奖励信号来训练策略模型，产生的响应比使用传统指标训练的响应更符合人类偏好。我们的代码可在https://github.com/zli12321/long_form_rl获取。", "summary": "本文针对开放式长文本生成评估的难题，提出了PrefBERT模型。PrefBERT是一个语义感知评分模型，通过提供区分好坏输出的奖励来指导GRPO训练。该模型在多样化的响应评估数据集上训练，并经LLM和人类评估证实，其提供的语义奖励反馈优于ROUGE-L和BERTScore等传统指标，能使生成的文本更符合人类偏好。", "keywords": "开放式生成, 语义奖励, PrefBERT, 强化学习, 长文本生成", "comments": "这篇论文通过引入语义感知的奖励模型PrefBERT，有效解决了开放式长文本生成中评估困难和现有指标不足的问题。其创新点在于利用区分好坏输出的奖励信号来优化强化学习训练，显著提升了生成内容的质量和人类偏好对齐度。该研究对于推动开放式生成任务的进展具有重要意义。"}}
{"id": "2506.14807", "title": "An explicit computational approach for a three-dimensional system of nonlinear elastodynamic sine-Gordon problem", "authors": ["Eric Ngondiep"], "summary": "This paper proposes an explicit computational method for solving a\nthree-dimensional system of nonlinear elastodynamic sine-Gordon equations\nsubject to appropriate initial and boundary conditions. The time derivative is\napproximated by interpolation technique whereas the finite element approach is\nused to approximate the space derivatives. The developed numerical scheme is\nso-called, high-order explicit computational technique. The new algorithm\nefficiently treats the time derivative term and provides a suitable time step\nrestriction for stability and convergence. Under this time step limitation,\nboth stability and error estimates of the proposed approach are deeply analyzed\nusing a constructed strong norm. The theoretical studies indicate that the\ndeveloped approach is temporal second-order convergent and spatially\nthird-order accurate. Some numerical examples are carried out to confirm the\ntheory, to validate the computational efficiency and to demonstrate the\npractical applicability of the new computational technique.", "comment": "21 pages, 20 figures, 6 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14807v1", "AI": {"title_translation": "三维非线性弹性动力学正弦-戈登问题的显式计算方法", "tldr": "本文提出一种求解三维非线性弹性动力学正弦-戈登方程组的高阶显式计算方法，并对其稳定性、收敛性和精度进行了理论和数值验证。", "motivation": "解决三维非线性弹性动力学正弦-戈登方程组的计算问题。", "method": "本文提出一种高阶显式计算方法，时间导数通过插值技术近似，空间导数通过有限元方法近似。该方法对时间导数项进行有效处理，并确定了适用于稳定性和收敛性的时间步长限制。", "result": "理论分析表明，所提出的方法在时间上是二阶收敛的，在空间上是三阶精确的。通过数值算例验证了该方法的计算效率和实际适用性，并证实了理论结果。", "conclusion": "所开发的高阶显式计算方法对三维非线性弹性动力学正弦-戈登问题有效，具有良好的稳定性、收敛性和精度，并通过数值验证了其理论分析和实际应用潜力。", "translation": "本文提出了一种求解三维非线性弹性动力学正弦-戈登方程组的显式计算方法，该方法适用于适当的初始和边界条件。时间导数通过插值技术近似，而空间导数则使用有限元方法进行近似。所开发的数值方案被称为高阶显式计算技术。新算法有效处理了时间导数项，并为稳定性和收敛性提供了合适的时间步长限制。在此时间步长限制下，使用构造的强范数深入分析了所提出方法的稳定性和误差估计。理论研究表明，所开发的方法在时间上是二阶收敛的，在空间上是三阶精确的。通过一些数值算例来证实理论，验证计算效率并展示新计算技术的实际适用性。", "summary": "本文提出一种新颖的高阶显式计算方法，用于求解三维非线性弹性动力学正弦-戈登方程组。该方法结合插值技术处理时间导数和有限元方法处理空间导数，并确定了确保稳定性和收敛性的时间步长限制。理论分析证明其在时间上二阶、空间上三阶精度，并通过数值算例验证了其高效性和实际应用潜力。", "keywords": "显式计算方法, 非线性弹性动力学, 正弦-戈登方程, 有限元方法, 高阶精度", "comments": "这篇论文的创新点在于提出了一个高阶显式计算方法来解决复杂的三维非线性弹性动力学正弦-戈登问题。显式方法通常在并行计算中具有优势。其对时间导数的有效处理以及对稳定性和误差的严格理论分析，特别是构造强范数来证明高阶精度，是其重要贡献。这为该类问题的数值模拟提供了可靠且高效的工具。"}}
{"id": "2506.15102", "title": "EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation", "authors": ["Shizhao Peng", "Shoumo Li", "Tianle Tao"], "summary": "Privacy-preserving neural network training in vertically partitioned\nscenarios is vital for secure collaborative modeling across institutions. This\npaper presents \\textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate\nSecure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale\noptimization for enhanced privacy and performance. To enable reliable\ncomputation under real-number domain, EVA-S2PMLP proposes a secure\ntransformation pipeline that maps scalar inputs to vector and matrix spaces\nwhile preserving correctness. The framework includes a suite of atomic\nprotocols for linear and non-linear secure computations, with modular support\nfor secure activation, matrix-vector operations, and loss evaluation.\nTheoretical analysis confirms the reliability, security, and asymptotic\ncomplexity of each protocol. Extensive experiments show that EVA-S2PMLP\nachieves high inference accuracy and significantly reduced communication\noverhead, with up to $12.3\\times$ improvement over baselines. Evaluation on\nbenchmark datasets demonstrates that the framework maintains model utility\nwhile ensuring strict data confidentiality, making it a practical solution for\nprivacy-preserving neural network training in finance, healthcare, and\ncross-organizational AI applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15102v1", "AI": {"title_translation": "EVA-S2PMLP: 通过空间变换实现安全可扩展的两方MLP", "tldr": "EVA-S2PMLP是一种安全高效的两方MLP框架，通过空间变换和优化协议，在垂直分区场景下实现隐私保护的神经网络训练，显著降低通信开销并保持高精度。", "motivation": "在垂直分区场景下，机构间安全协作建模需要保护隐私的神经网络训练。", "method": "本文提出了EVA-S2PMLP框架，引入空间尺度优化以增强隐私和性能。该框架通过一个安全变换管道将标量输入映射到向量和矩阵空间，并包含一套用于线性和非线性安全计算的原子协议，模块化支持安全激活、矩阵-向量操作和损失评估。", "result": "EVA-S2PMLP实现了高推理精度，显著降低了通信开销，比基线提高了高达12.3倍。在基准数据集上保持了模型效用，同时确保了严格的数据保密性。", "conclusion": "EVA-S2PMLP是用于金融、医疗保健和跨组织AI应用中隐私保护神经网络训练的实用解决方案。", "translation": "在垂直分区场景中，保护隐私的神经网络训练对于机构间的安全协作建模至关重要。本文提出了 \\textbf{EVA-S2PMLP}，一个高效、可验证且准确的安全两方多层感知器框架，该框架引入了空间尺度优化以增强隐私和性能。为了在实数域下实现可靠计算，EVA-S2PMLP 提出了一种安全转换管道，将标量输入映射到向量和矩阵空间，同时保持正确性。该框架包含一套用于线性和非线性安全计算的原子协议，模块化支持安全激活、矩阵-向量操作和损失评估。理论分析证实了每个协议的可靠性、安全性和渐近复杂度。大量实验表明，EVA-S2PMLP 实现了高推理精度并显著降低了通信开销，比基线提高了高达 12.3 倍。对基准数据集的评估表明，该框架在确保严格数据保密性的同时保持了模型效用，使其成为金融、医疗保健和跨组织 AI 应用中隐私保护神经网络训练的实用解决方案。", "summary": "本文提出了EVA-S2PMLP，一个针对垂直分区场景下隐私保护神经网络训练的安全两方多层感知器框架。该框架通过引入空间尺度优化和安全转换管道，将标量输入映射到向量和矩阵空间，以实现实数域下的可靠计算。EVA-S2PMLP包含一套模块化的原子协议，用于安全激活、矩阵-向量操作和损失评估。实验证明，EVA-S2PMLP在保持高推理精度的同时，显著降低了通信开销，使其成为跨组织AI应用中隐私保护训练的实用方案。", "keywords": "隐私保护, 两方计算, 多层感知器, 空间变换, 安全神经网络训练", "comments": "本文通过引入空间变换和原子协议，为垂直分区场景下的隐私保护神经网络训练提供了一个创新且实用的解决方案。其在通信开销上的显著改进（高达12.3倍）是重要的亮点，显示了其在实际应用中的潜力。该框架的可验证性、准确性和效率使其在金融和医疗等敏感数据领域具有重要价值。"}}
{"id": "2506.15655", "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15655v1", "AI": {"title_translation": "cAST: 通过抽象语法树进行结构化分块，增强代码检索增强生成", "tldr": "cAST通过抽象语法树（AST）进行结构化分块，解决了RAG中现有分块方法破坏代码语义结构的问题，显著提升了代码检索和生成性能。", "motivation": "现有基于行的分块启发式方法在检索增强生成（RAG）中常常破坏代码的语义结构，如分割函数或合并不相关的代码，从而降低生成质量。", "method": "提出cAST（Abstract Syntax Trees分块），这是一种结构感知的方法，它递归地将大型AST节点分解为较小的块，并在遵守大小限制的同时合并兄弟节点。这种方法生成跨编程语言和任务的自包含、语义连贯的单元。", "result": "在RepoEval检索上将Recall@5提高了4.3个百分点；在SWE-bench生成上将Pass@1提高了2.67个百分点。", "conclusion": "结构感知分块对于扩展检索增强的代码智能至关重要。", "translation": "检索增强生成（RAG）已成为大规模代码生成的关键，它将预测基于外部代码语料库，以提高真实性。然而，RAG管道中一个关键但尚未充分探索的方面是分块——将文档划分为可检索单元的过程。现有的基于行的分块启发式方法常常破坏语义结构，分割函数或合并不相关的代码，这会降低生成质量。我们提出了通过抽象语法树（cAST）进行分块，这是一种结构感知的方法，它递归地将大型AST节点分解为较小的块，并在遵守大小限制的同时合并兄弟节点。这种方法生成跨编程语言和任务的自包含、语义连贯的单元，提高了在各种代码生成任务上的性能，例如在RepoEval检索上将Recall@5提高了4.3个百分点，在SWE-bench生成上将Pass@1提高了2.67个百分点。我们的工作强调了结构感知分块对于扩展检索增强的代码智能的重要性。", "summary": "该论文提出了一种名为cAST的结构化分块方法，用于增强代码检索增强生成（RAG）。针对现有行级分块破坏代码语义结构的问题，cAST利用抽象语法树（AST）递归地将大型节点分解并合并兄弟节点，以生成语义连贯的代码块。实验结果表明，cAST显著提升了代码检索（Recall@5）和代码生成（Pass@1）的性能，突出了结构感知分块在代码智能中的重要性。", "keywords": "代码检索增强生成, 结构化分块, 抽象语法树, 代码生成, RAG", "comments": "这项工作创新性地将抽象语法树引入到RAG的分块策略中，解决了传统行级分块的结构破坏问题。其重要性在于，通过更精细、语义更强的代码单元划分，有效提升了代码检索和生成的质量，为大规模代码智能系统提供了更可靠的基础。"}}
{"id": "2506.15085", "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "authors": ["Paige Tuttösí", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "summary": "Humans vary their expressivity when speaking for extended periods to maintain\nengagement with their listener. Although social robots tend to be deployed with\n``expressive'' joyful voices, they lack this long-term variation found in human\nspeech. Foundation model text-to-speech systems are beginning to mimic the\nexpressivity in human speech, but they are difficult to deploy offline on\nrobots. We present EmojiVoice, a free, customizable text-to-speech (TTS)\ntoolkit that allows social roboticists to build temporally variable, expressive\nspeech on social robots. We introduce emoji-prompting to allow fine-grained\ncontrol of expressivity on a phase level and use the lightweight Matcha-TTS\nbackbone to generate speech in real-time. We explore three case studies: (1) a\nscripted conversation with a robot assistant, (2) a storytelling robot, and (3)\nan autonomous speech-to-speech interactive agent. We found that using varied\nemoji prompting improved the perception and expressivity of speech over a long\nperiod in a storytelling task, but expressive voice was not preferred in the\nassistant use case.", "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 :\n  https://dl.acm.org/doi/10.5555/3721488.3721774", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15085v1", "AI": {"title_translation": "EmojiVoice：迈向机器人语音中长期可控的表达能力", "tldr": "EmojiVoice是一个免费、可定制的TTS工具包，通过表情符号提示实现机器人语音的长期可变表达，并实时生成语音，改善了讲故事任务中的感知和表达，但在助手场景中未被偏好。", "motivation": "人类在长时间说话时会改变其表达方式以保持互动，但社交机器人缺乏这种长期可变的表达能力。现有基础模型文本转语音系统难以在机器人上离线部署，因此需要一种能让社交机器人实现长期可控且可变表达能力的解决方案。", "method": "本研究提出了EmojiVoice，一个免费、可定制的文本转语音（TTS）工具包。它引入了“表情符号提示”（emoji-prompting）以实现对表达能力在阶段层面的细粒度控制，并使用轻量级的Matcha-TTS骨干网络实时生成语音。研究通过三个案例研究进行了探索：一个与机器人助手的脚本对话、一个讲故事机器人和一个自主的语音到语音交互代理。", "result": "在讲故事任务中，使用多变的表情符号提示改善了长时间语音的感知和表达能力。然而，在助手用例中，富有表现力的声音并未受到偏好。", "conclusion": "EmojiVoice能够增强机器人语音的长期表达能力，尤其适用于讲故事等任务，但其效用可能因应用场景（例如，助手类任务中偏好较低）而异。", "translation": "人类在长时间说话时会改变其表达方式，以保持与听者的互动。尽管社交机器人通常部署有“富有表现力”的愉快声音，但它们缺乏人类语音中发现的这种长期变化。基础模型文本转语音系统开始模仿人类语音中的表达能力，但它们难以在机器人上离线部署。我们提出了EmojiVoice，一个免费、可定制的文本转语音（TTS）工具包，允许社交机器人专家在社交机器人上构建时间可变的、富有表现力的语音。我们引入了表情符号提示，以实现对表达能力在阶段层面的细粒度控制，并使用轻量级的Matcha-TTS骨干网络实时生成语音。我们探讨了三个案例研究：（1）与机器人助手的脚本对话，（2）一个讲故事机器人，以及（3）一个自主的语音到语音交互代理。我们发现，在讲故事任务中，使用多变的表情符号提示改善了长时间语音的感知和表达能力，但在助手用例中，富有表现力的声音并未受到偏好。", "summary": "EmojiVoice是一个免费、可定制的文本转语音（TTS）工具包，旨在解决社交机器人语音缺乏长期可变表达能力的问题。它通过引入“表情符号提示”实现对语音表达的细粒度控制，并利用轻量级Matcha-TTS骨干网络进行实时语音生成。研究通过三个案例研究验证了其效果，结果表明在讲故事任务中，表情符号提示显著提升了语音的感知和表达力，但在助手应用中，富有表现力的声音并未获得偏好。", "keywords": "机器人语音, 表达能力, 文本转语音, 表情符号提示, 社交机器人", "comments": "这篇论文解决了社交机器人领域的一个关键空白：机器人语音缺乏长期、动态变化的表达能力，而这对于维持人类互动至关重要。其创新点在于“表情符号提示”，提供了一种直观且细粒度的控制机制。使用轻量级骨干网络（Matcha-TTS）对于机器人离线部署具有实用性。研究结果强调了表达性语音的语境依赖性，表明更具表达力的声音有利于讲故事，但不一定适用于辅助任务。这种细致的理解对于未来的机器人语音设计具有宝贵的价值。"}}
{"id": "2506.15467", "title": "Towards Weight Distribution-Aware Polar Codes", "authors": ["Mohammad Rowshan", "Vlad-Florin Dragoi"], "summary": "Polar codes are constructed based on the reliability of sub-channels\nresulting from the polarization effect. However, this information-theoretic\nconstruction approach leads to a poor weight distribution. To address this\nissue, pre-transformed polar codes, such as CRC-polar codes and PAC codes, have\nbeen employed. In this paper, we focus on the structure of polar codes without\napplying any pre-transformations and explore methods, guided by the\nweight-contribution partial order, to design polar-like codes with enhanced\nweight distribution, notably without employing any search or optimization\nalgorithms. Numerical results demonstrate improvement over a range of codes\nboth with and without pre-transformation.", "comment": "Accepted and to be presented at IEEE ISIT'25", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15467v1", "AI": {"title_translation": "面向权值分布感知极化码", "tldr": "本文提出了一种基于权值贡献偏序关系的方法，用于设计具有更好权值分布的极化码，无需搜索或优化算法，并显示出性能提升。", "motivation": "传统的极化码构造方法（基于子信道可靠性）会导致较差的权值分布，即使是预变换的极化码（如CRC-极化码和PAC码）也未能完全解决此问题。本文旨在改进无预变换极化码的权值分布。", "method": "本文关注不进行任何预变换的极化码结构，并探索了由权值贡献偏序关系指导的方法，来设计具有增强权值分布的类极化码，且该方法不依赖于任何搜索或优化算法。", "result": "数值结果表明，与一系列有或无预变换的编码相比，所提出的方法在性能上有所改进。", "conclusion": "通过利用权值贡献偏序关系，可以在不使用搜索或优化算法的情况下，设计出具有更好权值分布的极化码，从而提高编码性能。", "translation": "极化码是根据极化效应产生的子信道可靠性来构造的。然而，这种信息论构造方法导致了较差的权值分布。为了解决这个问题，已经采用了预变换极化码，例如CRC-极化码和PAC码。在本文中，我们关注不应用任何预变换的极化码结构，并探索了在权值贡献偏序关系的指导下，设计具有增强权值分布的类极化码的方法，值得注意的是，该方法不采用任何搜索或优化算法。数值结果表明，与一系列有或无预变换的编码相比，该方法有所改进。", "summary": "本文研究了无预变换极化码的权值分布问题。针对传统极化码构造导致权值分布不佳的问题，提出了一种基于权值贡献偏序关系的设计方法，用于构造具有更优权值分布的类极化码。该方法的一大特点是不需要任何搜索或优化算法。实验结果表明，与现有有或无预变换的极化码相比，所设计的编码在性能上有所提升。", "keywords": "极化码, 权值分布, 权值贡献偏序, 无搜索算法, 编码设计", "comments": "本文的创新点在于提出了一种不依赖于复杂搜索或优化算法，而是通过利用权值贡献偏序关系来改进极化码权值分布的设计方法。这可能为极化码的实际应用提供更高效的构造途径，尤其是在资源受限的环境中。其贡献在于在理论指导下实现了性能提升，而非盲目搜索。"}}
{"id": "2506.15461", "title": "All is Not Lost: LLM Recovery without Checkpoints", "authors": ["Nikolay Blagoev", "Oğuzhan Ersoy", "Lydia Yiyu Chen"], "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15461v1", "AI": {"title_translation": "并非一无所有：LLM无检查点恢复", "tldr": "本文提出CheckFree及其扩展CheckFree+，一种无需检查点或冗余计算的LLM训练故障恢复方法，通过阶段平均或邻居模仿实现，显著提升去中心化训练的收敛速度。", "motivation": "在去中心化和计算能力有限的节点上训练大型语言模型（LLM）可以降低成本并实现模型普及，但节点故障导致的阶段丢失是一个挑战。传统的检查点或冗余计算方法会带来显著的通信和/或计算开销，在大模型设置下扩展性差。", "method": "本文提出了CheckFree，一种高效的恢复方法，通过将失败阶段替换为最近邻阶段的加权平均来实现，无需额外的计算或存储，但仅限于中间阶段。为了解决首尾阶段的故障，进一步提出了CheckFree+，它利用乱序流水线执行，通过邻居阶段模仿其行为并复制权重来恢复，对于(de)embedding层则通过复制到邻居阶段来恢复，仅需相对较小的存储开销。", "result": "在LLaMa模型上进行评估，对于低中等故障率（5-10%），CheckFree和CheckFree+在挂钟时间收敛方面比检查点和冗余计算方法性能提升超过12%。", "conclusion": "CheckFree和CheckFree+提供了一种高效的LLM训练故障恢复机制，无需传统检查点或冗余计算的开销，显著提高了去中心化训练的效率和鲁棒性。", "translation": "在去中心化和计算能力有限的节点（例如多个即时实例）上训练大型语言模型（LLM）可以降低训练成本并实现模型民主化。然而，这里不可避免的挑战是由于故障和操作员调度策略导致的节点流失，从而导致阶段（模型的一部分）的丢失。传统的故障恢复方法是使用检查点（定期将整个模型的副本发送到额外的存储）或冗余计算。这些方法即使在没有故障的情况下也会产生显著的通信和/或计算开销，并且在大模型设置下扩展性差。在本文中，我们提出了CheckFree，一种高效的恢复方法，其中失败的阶段被最近邻阶段的加权平均替代。与现有技术相比，CheckFree不需要额外的计算或存储。然而，由于平均相邻阶段的性质，它只能恢复中间阶段的故障。我们进一步将我们的方法扩展到CheckFree+，采用乱序流水线执行以容忍第一阶段和最后阶段的崩溃。由于乱序流水线，这些阶段的行为由其邻居模仿，这使得CheckFree+可以通过简单地从直接邻居复制权重来恢复它们。为了能够恢复（解）嵌入层，CheckFree+将这些层复制到相邻阶段，这需要相对较小的存储开销。我们广泛评估了我们的方法在LLaMa模型上的性能，模型大小从124M到1.5B，并具有不同的故障频率。在低和中等故障率（5-10%）的情况下，CheckFree和CheckFree+在挂钟时间收敛方面均优于检查点和冗余计算，性能提升超过12%。我们的两项提议都可以通过我们的代码运行，代码可在以下网址获取：https://github.com/gensyn-ai/CheckFree。", "summary": "本文提出CheckFree及其扩展CheckFree+，旨在解决去中心化LLM训练中节点故障导致的阶段丢失问题。CheckFree通过对失败的中间阶段进行邻居加权平均进行恢复，无需额外存储或计算。CheckFree+进一步通过乱序流水线执行，使首尾阶段能被邻居模仿并复制权重进行恢复，并为(de)embedding层提供轻微存储开销的复制机制。实验表明，在低中等故障率下，CheckFree和CheckFree+在LLaMa模型上能将训练收敛时间比传统方法缩短超过12%。", "keywords": "LLM恢复, 去中心化训练, 无检查点, 故障容忍, CheckFree", "comments": "这项工作非常创新，它挑战了传统上对检查点或冗余计算的依赖，为去中心化LLM训练提供了更高效、更轻量级的故障恢复方案。其核心思想——通过邻近阶段的信息来重建丢失阶段，尤其是在不增加显著开销的情况下，是其主要亮点。CheckFree+对首尾阶段和嵌入层的处理也显示了其方法的全面性。对于推动LLM训练的普及和降低成本具有重要意义。"}}
{"id": "2506.14833", "title": "Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices", "authors": ["Poojashree Chandrashekar Pankaj M Sajjanar"], "summary": "This paper describes a high-performance, low-latency video surveillance\nsystem designed for resource-constrained environments. We have proposed a\nformal entropy-based adaptive frame buffering algorithm and integrated that\nwith MobileNetV2 to achieve high throughput with low latency. The system is\ncapable of processing live streams of video with sub-50ms end-to-end inference\nlatency on resource-constrained devices (embedding platforms) such as Raspberry\nPi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection\naccuracy on standard datasets focused on video surveillance and exhibits\nrobustness to varying lighting, backgrounds, and speeds. A number of\ncomparative and ablation experiments validate the effectiveness of our design.\nFinally, our architecture is scalable, inexpensive, and compliant with stricter\ndata privacy regulations than common surveillance systems, so that the system\ncould coexist in a smart city or embedded security architecture.", "comment": "& pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14833v1", "AI": {"title_translation": "基于熵的自适应缓冲和MobileNetV2的边缘设备实时低延迟监控", "tldr": "该论文提出了一种在资源受限边缘设备上实现高吞吐量、低延迟（亚50毫秒）视频监控系统，结合了熵基自适应帧缓冲算法和MobileNetV2，同时保持高检测精度和数据隐私合规性。", "motivation": "在资源受限的环境中，需要一种高性能、低延迟的视频监控系统。", "method": "提出了一种形式化的基于熵的自适应帧缓冲算法，并将其与MobileNetV2集成，以在资源受限设备（如Raspberry Pi、Amazon和NVIDIA Jetson Nano）上实现高吞吐量和低延迟。该系统能够处理实时视频流。", "result": "系统在资源受限设备上实现了亚50毫秒的端到端推理延迟。在视频监控标准数据集上保持了超过92%的检测精度，并对不同的光照、背景和速度表现出鲁棒性。通过对比和消融实验验证了设计的有效性。", "conclusion": "该架构是可扩展的、廉价的，并且比常见的监控系统更符合严格的数据隐私法规，使其能够与智慧城市或嵌入式安全架构共存。", "translation": "本文描述了一种为资源受限环境设计的高性能、低延迟视频监控系统。我们提出了一种形式化的基于熵的自适应帧缓冲算法，并将其与MobileNetV2集成，以实现高吞吐量和低延迟。该系统能够在Raspberry Pi、Amazon和NVIDIA Jetson Nano等资源受限设备（嵌入式平台）上以亚50毫秒的端到端推理延迟处理实时视频流。我们的方法在专注于视频监控的标准数据集上保持了超过92%的检测精度，并对不同的光照、背景和速度表现出鲁棒性。大量的对比和消融实验验证了我们设计的有效性。最后，我们的架构是可扩展的、廉价的，并且比常见的监控系统更符合严格的数据隐私法规，因此该系统可以与智慧城市或嵌入式安全架构共存。", "summary": "本论文介绍了一种针对资源受限环境设计的高性能、低延迟视频监控系统。该系统结合了创新的熵基自适应帧缓冲算法和MobileNetV2模型，实现了在边缘设备（如Raspberry Pi、Jetson Nano）上亚50毫秒的实时视频推理延迟。它在保持超过92%检测精度的同时，展现了对多种环境条件的鲁棒性，并且具有可扩展性、成本效益和数据隐私合规性，适用于智慧城市等应用。", "keywords": "实时监控, 低延迟, 边缘计算, MobileNetV2, 熵自适应缓冲", "comments": "该论文的创新点在于结合了熵基自适应缓冲算法与MobileNetV2，有效解决了边缘设备上实时视频监控的性能瓶颈。其低延迟（亚50毫秒）和高精度（超过92%）的实测结果令人印象深刻，并且强调了数据隐私合规性，使其在实际部署中具有很高的实用价值和潜力。"}}
{"id": "2506.15624", "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": ["Lyle Goodyear", "Rachel Guo", "Ramesh Johari"], "summary": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.", "comment": "27 pages, 20 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15624v1", "AI": {"title_translation": "大型语言模型智能体在动态路径选择博弈中状态表示的影响", "tldr": "本文提出了一个统一框架，用于为LLM智能体在重复多智能体博弈中构建自然语言状态表示，并发现特定的状态表示（总结历史、遗憾信息、有限的他人行动信息）能使LLM智能体的行为更接近博弈论均衡，并更稳定。", "motivation": "大型语言模型（LLM）在动态环境中作为决策者展现出潜力，但其无状态特性需要创建历史的自然语言表示。以往关于LLM智能体博弈中历史编码的方法是临时性的，这不仅模糊了状态表示对智能体行为的影响，也限制了研究之间的可比性。", "method": "提出了一个统一框架，用于系统地构建LLM智能体在重复多智能体博弈中提示的自然语言“状态”表示。该框架从三个维度刻画了状态表示方法：行动信息量、奖励信息量和提示风格（或自然语言压缩）。将此框架应用于动态自私路径选择博弈。", "result": "在动态自私路径选择博弈中发现，LLM智能体行为对自然语言状态表示存在关键依赖。具体来说，提供（1）总结而非完整的历史、（2）关于遗憾而非原始收益的信息、（3）关于他人行动的有限信息的表示，能使智能体行为更接近博弈论均衡预测，并产生更稳定的博弈。相反，其他表示可能导致行为偏离均衡、动态博弈随时间变化更大，或两者兼有。", "conclusion": "状态表示对LLM智能体在动态博弈中的行为有显著影响，通过精心设计的自然语言状态表示，可以引导LLM智能体表现出更接近理论预测和更稳定的行为。", "translation": "大型语言模型（LLM）在动态环境中作为决策者展现出潜力，但其无状态特性需要创建历史的自然语言表示。我们提出了一个统一框架，用于系统地构建在重复多智能体博弈中提示LLM智能体的自然语言“状态”表示。以往关于LLM智能体博弈的研究在编码博弈历史时采用了临时性的方法，这不仅模糊了状态表示对智能体行为的影响，也限制了研究之间的可比性。我们的框架通过沿着三个轴线刻画状态表示方法来解决这些空白：行动信息量（即状态表示捕捉所玩行动的程度）；奖励信息量（即状态表示描述所获得奖励的程度）；以及提示风格（或自然语言压缩，即完整文本历史被总结的程度）。我们将此框架应用于一个动态自私路径选择博弈，选择该博弈是因为它在理论和人类受试者实验中都存在一个简单的均衡。尽管该博弈相对简单，我们发现LLM智能体行为对自然语言状态表示存在关键依赖。特别是，我们观察到，向智能体提供（1）总结而非完整的过去历史的自然语言表示；（2）关于遗憾而非原始收益的信息；以及（3）关于他人行动的有限信息的表示，能使行为更接近博弈论均衡预测，并使智能体进行更稳定的博弈。相比之下，其他表示可能表现出与均衡的较大偏差、动态博弈随时间变化更大，或两者兼有。", "summary": "本文针对大型语言模型（LLM）在动态决策中因无状态性而需历史表示的问题，提出了一个统一的自然语言状态表示框架。该框架从行动信息量、奖励信息量和提示风格三个维度对状态表示进行分类。通过在一个动态自私路径选择博弈中的应用，研究发现，提供总结的历史、遗憾信息以及有限的他人行动信息的表示，能使LLM智能体的行为更接近博弈论均衡，并提高博弈稳定性。", "keywords": "LLM智能体, 状态表示, 动态博弈, 路由博弈, 博弈论均衡", "comments": "这项工作创新性地提出了一个结构化的框架来分析和设计LLM在动态博弈中的状态表示，而非以往的临时性方法。其重要性在于揭示了自然语言状态表示对LLM智能体行为的关键影响，并为未来设计更有效、更稳定的LLM智能体提供了具体的指导原则。研究结果表明，并非信息越多越好，适当的抽象和聚焦关键信息（如遗憾而非原始收益）反而能提升LLM的决策质量，这对于理解LLM的认知机制和优化其在复杂环境中的应用具有重要意义。"}}
{"id": "2506.14790", "title": "Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting", "authors": ["Tianxiang Zhan", "Ming Jin", "Yuanpeng He", "Yuxuan Liang", "Yong Deng", "Shirui Pan"], "summary": "Recurring concept drift, a type of concept drift in which previously observed\ndata patterns reappear after some time, is one of the most prevalent types of\nconcept drift in time series. As time progresses, concept drift occurs and\npreviously encountered concepts are forgotten, thereby leading to a decline in\nthe accuracy of online predictions. Existing solutions employ parameter\nupdating techniques to delay forgetting; however, this may result in the loss\nof some previously learned knowledge while neglecting the exploration of\nknowledge retention mechanisms. To retain all conceptual knowledge and fully\nutilize it when the concepts recur, we propose the Continuous Evolution Pool\n(CEP), a pooling mechanism that stores different instances of forecasters for\ndifferent concepts. Our method first selects the forecaster nearest to the test\nsample and then learns the features from its neighboring samples - a process we\nrefer to as the retrieval. If there are insufficient neighboring samples, it\nindicates that a new concept has emerged, and a new model will evolve from the\ncurrent nearest sample to the pool to store the knowledge of the concept.\nSimultaneously, the elimination mechanism will enable outdated knowledge to be\ncleared to ensure the prediction effect of the forecasters. Experiments on\ndifferent architectural models and eight real datasets demonstrate that CEP\neffectively retains the knowledge of different concepts. In the scenario of\nonline forecasting with recurring concepts, CEP significantly enhances the\nprediction results.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14790v1", "AI": {"title_translation": "持续演化池：驯服在线时间序列预测中循环概念漂移", "tldr": "提出连续演化池（CEP）机制，通过存储不同预测器实例来应对时间序列中循环概念漂移，有效保留知识并提升在线预测精度。", "motivation": "循环概念漂移是时间序列中常见的概念漂移类型，导致在线预测精度下降。现有方法通过参数更新延迟遗忘，但可能丢失旧知识且忽视知识保留机制。", "method": "提出持续演化池（CEP），一种存储不同概念预测器实例的池化机制。该方法首先选择最接近测试样本的预测器（检索），若邻近样本不足则表示新概念出现，会从当前最近样本进化出新模型加入池中。同时，淘汰机制会清除过时知识。", "result": "在不同架构模型和八个真实数据集上的实验表明，CEP有效保留了不同概念的知识，并在循环概念的在线预测场景中显著提升了预测结果。", "conclusion": "CEP能有效解决时间序列在线预测中的循环概念漂移问题，通过知识保留和利用显著提升预测性能。", "translation": "循环概念漂移是一种概念漂移，其中先前观察到的数据模式在一段时间后重新出现，是时间序列中最普遍的概念漂移类型之一。随着时间的推移，概念漂移发生，先前遇到的概念被遗忘，从而导致在线预测精度下降。现有解决方案采用参数更新技术来延迟遗忘；然而，这可能导致一些先前学习到的知识丢失，同时忽略了知识保留机制的探索。为了保留所有概念知识并在概念重复出现时充分利用它们，我们提出了持续演化池（CEP），这是一种池化机制，用于存储不同概念的不同预测器实例。我们的方法首先选择最接近测试样本的预测样本，然后从其邻近样本中学习特征——我们称之为检索过程。如果邻近样本不足，则表示出现了新概念，一个新的模型将从当前最近的样本进化到池中，以存储该概念的知识。同时，淘汰机制将使过时的知识被清除，以确保预测器的预测效果。在不同架构模型和八个真实数据集上的实验表明，CEP有效地保留了不同概念的知识。在循环概念的在线预测场景中，CEP显著提高了预测结果。", "summary": "针对时间序列在线预测中循环概念漂移导致的预测精度下降问题，本文提出了持续演化池（CEP）机制。CEP通过存储不同概念的预测器实例，并在概念重复出现时进行检索利用，同时引入新模型进化和过时知识淘汰机制，有效保留了历史知识。实验证明CEP能显著提升循环概念在线预测的性能。", "keywords": "循环概念漂移, 在线时间序列预测, 持续演化池, 知识保留, 预测器集成", "comments": "该论文提出了一种创新的池化机制CEP来解决时间序列中循环概念漂移的挑战。其核心在于通过存储和检索不同概念的预测器实例，有效避免了传统方法中知识遗忘的问题。这种显式地知识保留和利用策略对于在线学习系统具有重要意义，尤其是在数据模式周期性变化的场景下。"}}
{"id": "2506.15384", "title": "Disruption of parkinsonian brain oscillations", "authors": ["Cédric Join", "Jakub Orłowski", "Antoine Chaillet", "Madeleine Lowery", "Hugues Mounier", "Michel Fliess"], "summary": "Deep brain stimulation (DBS) is an advanced surgical treatment for the\nsymptoms of Parkinson's disease (PD), involving electrical stimulation of\nneurons within the basal ganglia region of the brain. DBS is traditionally\ndelivered in an open-loop manner using fixed stimulation parameters, which may\nlead to suboptimal results. In an effort to overcome these limitations, closed\nloop DBS, using pathological subthalamic beta (13--30 Hz) activity as a\nfeedback signal, offers the potential to adapt DBS automatically in response to\nchanges in patient symptoms and side effects. However, clinically implemented\nclosed-loop techniques have been limited to date to simple control algorithms,\ndue to the inherent uncertainties in the dynamics involved. Model-free control,\nwhich has already seen successful applications in the field of bioengineering,\noffers a way to avoid this limitation and provides an alternative method to\napply modern control approach to selective suppression of pathological\noscillations.", "comment": "23rd Internat. Conf. Computational Methods in Systems Biology (CMSB\n  2025), 10-12 september 2025, Lyon, France", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15384v1", "AI": {"title_translation": "帕金森病脑部振荡的扰动", "tldr": "传统的开环深部脑刺激（DBS）效果不佳。闭环DBS虽然有潜力但受限于简单算法。本文提出模型无关控制作为一种替代方法，以克服这些限制并选择性抑制帕金森病的病理性脑振荡。", "motivation": "传统的开环深部脑刺激（DBS）采用固定参数，效果可能不佳。目前临床实施的闭环DBS技术受限于简单的控制算法，因为涉及的动力学存在固有的不确定性，这限制了其适应患者症状变化的能力。", "method": "本文提出采用模型无关控制（Model-free control）的方法，将其应用于闭环深部脑刺激（DBS），以选择性地抑制帕金森病的病理性脑振荡。这种方法旨在避免传统闭环DBS受限于简单算法的局限性。", "result": "Not mentioned in abstract", "conclusion": "模型无关控制为将现代控制方法应用于选择性抑制病理性振荡提供了一种替代方案，有望克服当前闭环深部脑刺激（DBS）技术的局限性。", "translation": "深部脑刺激 (DBS) 是一种治疗帕金森病 (PD) 症状的先进外科疗法，涉及对脑部基底神经节区域神经元的电刺激。DBS 传统上以开环方式进行，使用固定的刺激参数，这可能导致效果不佳。为了克服这些局限性，闭环 DBS 利用病理性的丘脑底核β波活动（13-30 Hz）作为反馈信号，有望根据患者症状和副作用的变化自动调整 DBS。然而，由于所涉及动力学固有的不确定性，目前临床实施的闭环技术仅限于简单的控制算法。模型无关控制（Model-free control）在生物工程领域已成功应用，它提供了一种避免这种局限性的方法，并提供了一种应用现代控制方法选择性抑制病理性振荡的替代方案。", "summary": "本文讨论了帕金森病传统开环深部脑刺激（DBS）的局限性，以及当前闭环DBS因动力学不确定性而受限于简单控制算法的挑战。为克服这些问题，文章提出采用在生物工程领域已成功应用的模型无关控制，作为一种替代方法来有效抑制病理性脑振荡，从而改善帕金森病的治疗效果。", "keywords": "深部脑刺激, 帕金森病, 闭环控制, 模型无关控制, 脑振荡", "comments": "该论文指出了当前闭环深部脑刺激（DBS）的一个重要局限性，并提出了一种新颖的控制方法——模型无关控制。这种方法在生物工程领域已有成功应用，预示着为改善帕金森病治疗提供了一个有前景的方向，具有潜在的创新性。"}}
{"id": "2506.15258", "title": "Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference", "authors": ["Jonghun Kim", "Gyeongdeok Jo", "Shinyoung Ra", "Hyunjin Park"], "summary": "Medical imaging data contain sensitive patient information requiring strong\nprivacy protection. Many analytical setups require data to be sent to a server\nfor inference purposes. Homomorphic encryption (HE) provides a solution by\nallowing computations to be performed on encrypted data without revealing the\noriginal information. However, HE inference is computationally expensive,\nparticularly for large images (e.g., chest X-rays). In this study, we propose\nan HE inference framework for medical images that uses VQGAN to compress images\ninto latent representations, thereby significantly reducing the computational\nburden while preserving image quality. We approximate the activation functions\nwith lower-degree polynomials to balance the accuracy and efficiency in\ncompliance with HE requirements. We observed that a downsampling factor of\neight for compression achieved an optimal balance between performance and\ncomputational cost. We further adapted the squeeze and excitation module, which\nis known to improve traditional CNNs, to enhance the HE framework. Our method\nwas tested on two chest X-ray datasets for multi-label classification tasks\nusing vanilla CNN backbones. Although HE inference remains relatively slow and\nintroduces minor performance differences compared with unencrypted inference,\nour approach shows strong potential for practical use in medical images", "comment": "11 pages, 5 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15258v1", "AI": {"title_translation": "潜在空间中同态加密神经网络推理的隐私保护胸部X光分类", "tldr": "提出一种新的同态加密（HE）推理框架，通过使用VQGAN将胸部X光图像压缩到潜在空间，并优化激活函数，以实现隐私保护的医学图像分类，同时显著降低计算成本。", "motivation": "医疗影像数据包含敏感患者信息，需要强隐私保护，但服务器端推理计算成本高，特别是同态加密（HE）在大型图像上计算昂贵。", "method": "提出一个HE推理框架，使用VQGAN将图像压缩到潜在表示以降低计算负担；使用低阶多项式近似激活函数以平衡精度和效率；并适配了Squeeze and Excitation模块来增强框架。", "result": "压缩的下采样因子为八时，在性能和计算成本之间达到最佳平衡。与未加密推理相比，HE推理仍然相对较慢并引入轻微的性能差异。", "conclusion": "尽管存在速度和性能差异，该方法在医学图像的实际应用中显示出巨大潜力。", "translation": "医疗影像数据包含敏感的患者信息，需要强大的隐私保护。许多分析设置要求将数据发送到服务器进行推理。同态加密（HE）通过允许在加密数据上进行计算而不泄露原始信息，提供了一种解决方案。然而，HE推理的计算成本很高，特别是对于大型图像（例如胸部X光）。在本研究中，我们提出了一种用于医学图像的HE推理框架，该框架使用VQGAN将图像压缩为潜在表示，从而在保持图像质量的同时显著降低计算负担。我们用低阶多项式近似激活函数，以平衡精度和效率，符合HE要求。我们观察到，压缩的下采样因子为八时，在性能和计算成本之间达到了最佳平衡。我们进一步适配了Squeeze and Excitation模块，该模块已知可改进传统CNN，以增强HE框架。我们的方法在两个胸部X光数据集上进行了多标签分类任务测试，使用了香草CNN骨干网络。尽管HE推理与未加密推理相比仍然相对较慢并引入轻微的性能差异，但我们的方法在医学图像的实际应用中显示出巨大潜力。", "summary": "本文提出一种新颖的同态加密（HE）推理框架，用于隐私保护的胸部X光图像分类。为了解决HE推理计算成本高昂的问题，该框架利用VQGAN将大尺寸图像压缩到潜在空间，并采用低阶多项式近似激活函数以平衡精度和效率。研究还引入了改进的Squeeze and Excitation模块。尽管HE推理仍存在速度和轻微性能损失，但实验结果表明，该方法在医学图像的实际应用中具有显著潜力。", "keywords": "同态加密, 隐私保护, 胸部X光, 潜在空间, VQGAN", "comments": "这项工作创新性地将VQGAN引入同态加密推理流程，有效解决了HE在处理大型医学图像时的计算瓶颈。通过在潜在空间进行加密推理，显著降低了计算成本，同时兼顾了隐私保护和图像质量。虽然仍有性能和速度的权衡，但该框架为实际部署隐私保护的医疗AI系统提供了有前景的方向。"}}
{"id": "2506.15338", "title": "Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry", "authors": ["Islam M. Tanash", "Ayush Kumar Dwivedi", "Taneli Riihonen"], "summary": "This paper studies a high-altitude platform (HAP) network supported by\nreconfigurable intelligent surfaces (RISs). The practical irregular placement\nof HAPs and RISs is modeled using homogeneous Poisson point processes, while\nbuildings that cause blockages in urban areas are modeled as a Boolean scheme\nof rectangles. We introduce a novel approach to characterize the statistical\nchannel based on generalized Beta prime distribution. Analytical expressions\nfor coverage probability and ergodic capacity in an interference-limited system\nare derived and validated through Monte Carlo simulations. The findings show\nnotable performance improvements and reveal the impact of various system\nparameters, including blockages effect which contribute in mitigating\ninterference from the other visible HAPs. This proposed system could enhance\nconnectivity and enable effective data offloading in urban environments.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15338v1", "AI": {"title_translation": "城市RIS辅助HAP网络：基于随机几何学的性能分析", "tldr": "本文研究了RIS辅助高空平台（HAP）网络，使用随机几何学建模，并通过分析推导和蒙特卡洛模拟验证了覆盖概率和遍历容量，结果显示性能显著提升，并揭示了系统参数的影响。", "motivation": "本文研究了由可重构智能表面（RIS）支持的高空平台（HAP）网络，旨在分析其在实际不规则部署和城市建筑物阻塞下的性能，以提升城市环境中的连接性并实现有效的数据分流。", "method": "本研究使用均匀泊松点过程建模HAP和RIS的实际不规则部署，并使用矩形布尔方案建模城市区域的建筑物阻塞。引入了一种基于广义Beta质数分布的新方法来表征统计信道。通过分析推导了干扰受限系统中的覆盖概率和遍历容量的表达式，并通过蒙特卡洛模拟进行了验证。", "result": "研究结果显示，RIS辅助HAP网络性能有显著提升，并揭示了各种系统参数的影响，包括阻塞效应有助于减轻来自其他可见HAP的干扰。", "conclusion": "该提出的系统可以增强城市环境中的连接性，并实现有效的数据分流。", "translation": "本文研究了由可重构智能表面（RIS）支持的高空平台（HAP）网络。HAP和RIS的实际不规则部署使用齐次泊松点过程建模，而城市区域中导致阻塞的建筑物则建模为矩形的布尔方案。我们引入了一种基于广义Beta质数分布的新方法来表征统计信道。在干扰受限系统中，推导了覆盖概率和遍历容量的解析表达式，并通过蒙特卡洛模拟进行了验证。研究结果显示了显著的性能改进，并揭示了各种系统参数的影响，包括有助于减轻来自其他可见HAP干扰的阻塞效应。所提出的系统可以增强城市环境中的连接性并实现有效的数据分流。", "summary": "本文研究了城市环境中由可重构智能表面（RIS）辅助的高空平台（HAP）网络。通过使用均匀泊松点过程和矩形布尔方案分别建模HAP/RIS部署和建筑物阻塞，并引入广义Beta质数分布来表征信道。研究推导并验证了覆盖概率和遍历容量的解析表达式，结果表明该系统能显著提升性能，并有效利用阻塞效应来减轻干扰，从而增强城市连接和数据分流能力。", "keywords": "RIS, HAP网络, 随机几何学, 覆盖概率, 遍历容量", "comments": "该论文的创新点在于结合了RIS辅助HAP网络与随机几何学模型，并引入了广义Beta质数分布来表征信道，这为分析城市复杂环境下的无线通信提供了新的工具。其重要性在于证明了RIS在城市HAP网络中增强连接性和数据卸载的潜力，特别是在考虑实际部署和建筑物阻塞的情况下。"}}
{"id": "2506.15076", "title": "Learning-Time Encoding Shapes Unlearning in LLMs", "authors": ["Ruihan Wu", "Konstantin Garov", "Kamalika Chaudhuri"], "summary": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15076v1", "AI": {"title_translation": "学习时编码塑造大型语言模型中的遗忘能力", "tldr": "本文实证研究了学习时知识编码如何影响大型语言模型的事实知识遗忘效果，发现使用释义描述学习可以提高遗忘性能，但从文本块中遗忘单个知识点很困难。", "motivation": "随着大型语言模型（LLMs）在现实世界中日益广泛部署，事后“遗忘”或删除特定知识的能力变得至关重要，原因包括隐私法规以及纠正过时或有害内容。", "method": "本文实证研究了学习时知识编码选择如何影响事实知识遗忘的有效性。", "result": "实验揭示了两个关键发现：(1) 使用释义描述进行学习可以提高遗忘性能；(2) 从文本块中遗忘单个知识点具有挑战性。", "conclusion": "研究结果表明，学习时的知识编码可能在实现可靠的事后遗忘中发挥核心作用。", "translation": "随着大型语言模型（LLMs）在现实世界中日益广泛部署，“遗忘”或事后删除特定知识的能力变得至关重要，原因包括隐私法规以及纠正过时或有害内容。先前的工作提出了遗忘基准和算法，并且通常假设训练过程和目标模型是固定的。在这项工作中，我们实证研究了学习时知识编码的选择如何影响事实知识遗忘的有效性。我们的实验揭示了两个关键发现：(1) 使用释义描述进行学习可以提高遗忘性能；(2) 从文本块中遗忘单个知识点具有挑战性。我们的结果表明，学习时的知识编码可能在实现可靠的事后遗忘中发挥核心作用。", "summary": "本文探讨了学习时知识编码对大型语言模型（LLMs）遗忘能力的影响。研究发现，使用释义描述进行学习能够提升遗忘性能，但从文本块中精确遗忘单个知识点仍面临挑战。这表明学习时编码在实现可靠的事后知识遗忘中扮演着关键角色。", "keywords": "大型语言模型, 知识遗忘, 学习时编码, 事实知识, 遗忘性能", "comments": "这项研究的创新之处在于，它将关注点从传统的遗忘算法和基准转移到学习阶段的知识编码方式，强调了“预防胜于治疗”的理念。其重要性在于为未来开发更有效、更可靠的LLM遗忘机制提供了新的研究方向，特别是在隐私保护和内容修正方面。该研究揭示的挑战性也为后续工作指明了方向，即如何更精细地控制知识的遗忘粒度。"}}
{"id": "2506.14812", "title": "Weak TransNet: A Petrov-Galerkin based neural network method for solving elliptic PDEs", "authors": ["Zhihang Xu", "Min Wang", "Zhu Wang"], "summary": "While deep learning has achieved remarkable success in solving partial\ndifferential equations (PDEs), it still faces significant challenges,\nparticularly when the PDE solutions have low regularity or singularities. To\naddress these issues, we propose the Weak TransNet (WTN) method, based on a\nPetrov-Galerkin formulation, for solving elliptic PDEs in this work, though its\nframework may extend to other classes of equations. Specifically, the neural\nfeature space defined by TransNet (Zhang et al., 2023) is used as the trial\nspace, while the test space is composed of radial basis functions. Since the\nsolution is expressed as a linear combination of trial functions, the\ncoefficients can be determined by minimizing the weak PDE residual via least\nsquares. Thus, this approach could help mitigate the challenges of\nnon-convexity and ill-conditioning that often arise in neural network training.\nFurthermore, the WTN method is extended to handle problems whose solutions\nexhibit multiscale features or possess sharp variations. Several numerical\nexperiments are presented to demonstrate the robustness and efficiency of the\nproposed methods.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14812v1", "AI": {"title_translation": "弱TransNet：一种基于Petrov-Galerkin的神经网络求解椭圆偏微分方程的方法", "tldr": "提出了一种名为Weak TransNet（WTN）的新型神经网络方法，结合Petrov-Galerkin公式，有效解决椭圆偏微分方程（PDEs），尤其适用于低正则性或奇异解，并能缓解神经网络训练中的非凸性和病态问题。", "motivation": "现有深度学习在解决偏微分方程（PDEs）时，当解具有低正则性或奇异性时面临显著挑战。", "method": "提出Weak TransNet（WTN）方法，基于Petrov-Galerkin公式求解椭圆PDEs。该方法使用TransNet定义的神经网络特征空间作为试空间，径向基函数组成测试空间。通过最小化弱PDE残差的最小二乘法来确定系数，从而缓解神经网络训练中常见的非凸性和病态问题。WTN方法还扩展到处理具有多尺度特征或剧烈变化的解。", "result": "数值实验表明所提出的方法具有鲁棒性和效率。", "conclusion": "Weak TransNet（WTN）方法能够有效解决椭圆偏微分方程，特别是当解具有低正则性、奇异性、多尺度特征或剧烈变化时，并有助于缓解神经网络训练中的非凸性和病态问题。", "translation": "尽管深度学习在解决偏微分方程（PDEs）方面取得了显著成功，但它仍然面临重大挑战，特别是当PDE解具有低正则性或奇异性时。为了解决这些问题，本研究提出了一种基于Petrov-Galerkin公式的弱TransNet（WTN）方法来求解椭圆PDEs，尽管其框架可能扩展到其他类型的方程。具体而言，TransNet（Zhang et al., 2023）定义的神经特征空间用作试空间，而测试空间由径向基函数组成。由于解表示为试函数的线性组合，系数可以通过最小化弱PDE残差的最小二乘法来确定。因此，这种方法可以帮助缓解神经网络训练中经常出现的非凸性和病态问题。此外，WTN方法还扩展到处理解表现出多尺度特征或具有剧烈变化的问题。本文通过几个数值实验来证明所提出方法的鲁棒性和效率。", "summary": "本文提出了一种名为Weak TransNet (WTN) 的新型神经网络方法，用于解决椭圆偏微分方程。该方法基于Petrov-Galerkin公式，利用TransNet作为试空间，径向基函数作为测试空间，并通过最小二乘法确定解的系数。WTN旨在克服传统深度学习在处理低正则性或奇异解时的挑战，并能有效缓解神经网络训练中的非凸性和病态问题。该方法还适用于具有多尺度或剧烈变化的解，并通过数值实验验证了其鲁棒性和效率。", "keywords": "偏微分方程, 神经网络, Petrov-Galerkin, 弱TransNet, 低正则性", "comments": "该论文的创新之处在于将Petrov-Galerkin弱形式与神经网络相结合，特别是利用TransNet作为试空间，并结合径向基函数作为测试空间。这种方法有效地解决了传统深度学习在处理低正则性或奇异PDE解时的痛点，并通过最小二乘法确定系数，显著缓解了神经网络训练中常见的非凸性和病态问题。其对多尺度和剧烈变化问题的处理能力也增强了方法的实用性。"}}
{"id": "2506.15112", "title": "PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning", "authors": ["Xiangman Li", "Xiaodong Wu", "Jianbing Ni", "Mohamed Mahmoud", "Maazen Alsabaan"], "summary": "Decentralized learning is vulnerable to poison attacks, where malicious\nclients manipulate local updates to degrade global model performance. Existing\ndefenses mainly detect and filter malicious models, aiming to prevent a limited\nnumber of attackers from corrupting the global model. However, restoring an\nalready compromised global model remains a challenge. A direct approach is to\nremove malicious clients and retrain the model using only the benign clients.\nYet, retraining is time-consuming, computationally expensive, and may\ncompromise model consistency and privacy.\n  We propose PDLRecover, a novel method to recover a poisoned global model\nefficiently by leveraging historical model information while preserving\nprivacy. The main challenge lies in protecting shared historical models while\nenabling parameter estimation for model recovery. By exploiting the linearity\nof approximate Hessian matrix computation, we apply secret sharing to protect\nhistorical updates, ensuring local models are not leaked during transmission or\nreconstruction. PDLRecover introduces client-side preparation, periodic\nrecovery updates, and a final exact update to ensure robustness and convergence\nof the recovered model. Periodic updates maintain accurate curvature\ninformation, and the final step ensures high-quality convergence. Experiments\nshow that the recovered global model achieves performance comparable to a fully\nretrained model but with significantly reduced computation and time cost.\nMoreover, PDLRecover effectively prevents leakage of local model parameters,\nensuring both accuracy and privacy in recovery.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15112v1", "AI": {"title_translation": "PDLRecover：隐私保护的去中心化模型恢复与机器遗忘", "tldr": "PDLRecover通过利用历史模型信息和秘密共享，以高效且隐私保护的方式恢复被投毒的去中心化学习模型。", "motivation": "去中心化学习易受投毒攻击，导致全局模型性能下降。现有防御主要检测和过滤恶意模型，但恢复已受损的全局模型仍是挑战。直接重训耗时、计算昂贵且可能损害模型一致性和隐私。", "method": "本文提出了PDLRecover，一种利用历史模型信息并保护隐私来高效恢复中毒全局模型的新方法。它通过利用近似Hessian矩阵计算的线性特性，应用秘密共享技术保护历史更新，确保本地模型在传输或重建过程中不泄露。PDLRecover引入了客户端准备、周期性恢复更新和最终精确更新，以确保恢复模型的鲁棒性和收敛性。", "result": "恢复的全局模型性能与完全重训的模型相当，但计算和时间成本显著降低。PDLRecover有效防止本地模型参数泄露，确保恢复的准确性和隐私性。", "conclusion": "PDLRecover提供了一种高效、隐私保护的去中心化模型恢复方法，在性能上与重训相当，同时显著降低了成本并保护了隐私。", "translation": "去中心化学习容易受到投毒攻击，恶意客户端会操纵本地更新以降低全局模型性能。现有的防御主要检测和过滤恶意模型，旨在防止有限数量的攻击者破坏全局模型。然而，恢复一个已经受损的全局模型仍然是一个挑战。一种直接的方法是移除恶意客户端并仅使用良性客户端重新训练模型。然而，重新训练耗时、计算成本高昂，并且可能会损害模型的一致性和隐私。\n我们提出了PDLRecover，这是一种通过利用历史模型信息同时保护隐私来有效恢复中毒全局模型的新方法。主要挑战在于保护共享的历史模型，同时实现模型恢复的参数估计。通过利用近似Hessian矩阵计算的线性特性，我们应用秘密共享来保护历史更新，确保本地模型在传输或重建过程中不会泄露。PDLRecover引入了客户端准备、周期性恢复更新和最终精确更新，以确保恢复模型的鲁棒性和收敛性。周期性更新保持准确的曲率信息，最后一步确保高质量的收敛。实验表明，恢复的全局模型性能与完全重新训练的模型相当，但计算和时间成本显著降低。此外，PDLRecover有效防止本地模型参数的泄露，确保恢复的准确性和隐私性。", "summary": "本文提出了PDLRecover，一种针对去中心化学习中模型投毒攻击的恢复方法。它通过利用历史模型信息和秘密共享技术，高效且隐私保护地恢复被破坏的全局模型。PDLRecover通过客户端准备、周期性更新和最终精确更新确保恢复模型的鲁棒性和收敛性。实验证明，PDLRecover在性能上可与完全重训模型媲美，同时显著降低了计算和时间成本，并有效保护了本地模型参数的隐私。", "keywords": "去中心化学习, 模型恢复, 隐私保护, 机器遗忘, 秘密共享", "comments": "该论文提出了一种创新的方法来解决去中心化学习中模型被投毒后的恢复问题，而不是仅仅关注预防。其核心贡献在于结合了历史模型信息、秘密共享和近似Hessian矩阵的线性特性，实现了高效且隐私保护的模型恢复。这对于提高去中心化学习系统的鲁棒性和实用性具有重要意义。"}}
{"id": "2506.15648", "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15648v1", "AI": {"title_translation": "deepSURF：通过模糊测试LLM增强的测试工具检测Rust中的内存安全漏洞", "tldr": "deepSURF是一款针对Rust不安全代码的内存安全漏洞检测工具，它结合了静态分析和LLM引导的模糊测试，有效解决了现有工具的局限性，并成功发现已知和未知的漏洞。", "motivation": "尽管Rust默认保证内存安全，但其允许使用不安全代码，这可能引入内存安全漏洞。现有Rust内存错误检测工具存在检测能力有限、处理Rust特定类型不足或严重依赖人工干预的局限性。", "method": "deepSURF集成静态分析与大型语言模型（LLM）引导的模糊测试工具生成。它通过将泛型替换为自定义类型并生成定制的特性实现来处理泛型。此外，deepSURF利用LLM动态增强模糊测试工具，以促进复杂API交互的探索。", "result": "在27个真实世界的Rust crate上进行评估，deepSURF成功重新发现了20个已知的内存安全错误，并发现了6个以前未知的漏洞，显示出比现有最先进工具的明显改进。", "conclusion": "deepSURF通过结合静态分析和LLM引导的模糊测试，能够有效识别Rust库中（特别是针对不安全代码）的内存安全漏洞，并优于现有工具。", "translation": "尽管Rust默认保证内存安全，但它也允许使用不安全代码，如果使用不当，可能会引入内存安全漏洞。不幸的是，现有的Rust内存错误检测工具通常检测能力有限，无法充分处理Rust特定类型，或者严重依赖人工干预。\n为了解决这些限制，我们提出了deepSURF，一个将静态分析与大型语言模型（LLM）引导的模糊测试工具生成相结合的工具，以有效识别Rust库中的内存安全漏洞，特别是针对不安全代码。deepSURF引入了一种处理泛型的新方法，通过将其替换为自定义类型并为所需的特性生成量身定制的实现，使模糊器能够模拟模糊测试库中的用户定义行为。此外，deepSURF采用LLM动态增强模糊测试工具，促进复杂API交互的探索，并显著增加暴露内存安全漏洞的可能性。我们在27个真实世界的Rust crate上评估了deepSURF，成功重新发现了20个已知的内存安全错误，并发现了6个以前未知的漏洞，表明比最先进的工具有了明显改进。", "summary": "deepSURF是一款针对Rust不安全代码的内存安全漏洞检测工具。它结合了静态分析和LLM引导的模糊测试工具生成，以克服现有工具的局限性。deepSURF通过自定义类型和LLM增强模糊测试工具来处理泛型和复杂API交互。实验结果表明，deepSURF在检测已知和未知内存安全漏洞方面优于现有技术。", "keywords": "内存安全, Rust, 模糊测试, LLM, 漏洞检测", "comments": "deepSURF的创新点在于结合了静态分析、LLM引导的模糊测试以及对Rust特有泛型的处理。LLM在动态增强模糊测试工具方面的应用，提高了复杂API交互的探索能力，是其能够发现新漏洞的关键。该工具对于提高Rust生态系统的安全性具有重要意义。"}}
{"id": "2506.15129", "title": "Data Verbalisation: What is Text Doing in a Data Visualisation?", "authors": ["Paul Murrell"], "summary": "This article discusses the role that text elements play in a data\nvisualisation. We argue that there is a need for a simple, coherent explanation\nof text elements similar to the understanding that already exists for non-text\nelements like bars, points, and lines. We explore examples of how text is used\nwithin a data visualisation and use existing knowledge and assessment\ntechniques to evaluate when text is effective and when it is not. The result is\na framework that aims to be easy to understand and easy to apply in order to\nunderstand the purpose and effectiveness of the text elements in any data\nvisualisation.", "comment": "43 pages (including appendix), 20 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15129v1", "AI": {"title_translation": "数据言语化：文本在数据可视化中扮演了什么角色？", "tldr": "本文讨论了文本元素在数据可视化中的作用，并提出了一个理解其目的和有效性的框架。", "motivation": "现有数据可视化对非文本元素（如条形、点、线）已有清晰的理解，但对文本元素缺乏简单、连贯的解释。因此，需要一个框架来理解文本在数据可视化中的作用和有效性。", "method": "文章探讨了文本在数据可视化中的使用示例，并利用现有知识和评估技术来评估文本何时有效何时无效。", "result": "研究提出了一个旨在易于理解和应用的框架，用于理解任何数据可视化中文本元素的用途和有效性。", "conclusion": "本文提出了一个框架，旨在帮助人们理解数据可视化中文本元素的用途和有效性，填补了现有理解上的空白。", "translation": "本文讨论了文本元素在数据可视化中的作用。我们认为，需要对文本元素进行简单、连贯的解释，类似于已经存在的对非文本元素（如条形、点和线）的理解。我们探讨了文本在数据可视化中如何使用的例子，并利用现有知识和评估技术来评估文本何时有效以及何时无效。结果是一个旨在易于理解和应用以理解任何数据可视化中文本元素的用途和有效性的框架。", "summary": "本文探讨了文本元素在数据可视化中的作用，指出目前缺乏对文本元素的系统理解。通过分析文本使用示例并运用现有评估技术，文章提出了一个易于理解和应用的框架，旨在帮助用户理解数据可视化中文本元素的用途和有效性。", "keywords": "数据可视化, 文本元素, 数据言语化, 框架, 可读性", "comments": "本文的创新之处在于填补了数据可视化领域对文本元素理解的空白，提供了一个结构化的框架来分析和评估文本在可视化中的作用，这对于提升数据可视化的整体效果和可读性具有重要意义。"}}
{"id": "2506.15087", "title": "3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception", "authors": ["Yuankai Lin", "Xiaofan Lu", "Jiahui Chen", "Hua Yang"], "summary": "To achieve human-like haptic perception in anthropomorphic grippers, the\ncompliant sensing surfaces of vision tactile sensor (VTS) must evolve from\nconventional planar configurations to biomimetically curved topographies with\ncontinuous surface gradients. However, planar VTSs have challenges when\nextended to curved surfaces, including insufficient lighting of surfaces,\nblurring in reconstruction, and complex spatial boundary conditions for surface\nstructures. With an end goal of constructing a human-like fingertip, our\nresearch (i) develops GelSplitter3D by expanding imaging channels with a prism\nand a near-infrared (NIR) camera, (ii) proposes a photometric stereo neural\nnetwork with a CAD-based normal ground truth generation method to calibrate\ntactile geometry, and (iii) devises a normal integration method with boundary\nconstraints of depth prior information to correcting the cumulative error of\nsurface integrals. We demonstrate better tactile sensing performance, a 40$\\%$\nimprovement in normal estimation accuracy, and the benefits of sensor shapes in\ngrasping and manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15087v1", "AI": {"title_translation": "机器人精细触觉感知的红外和可见光图像三维视觉-触觉重建", "tldr": "本文提出了一种结合红外和可见光图像的三维视觉-触觉重建方法，通过新型传感器GelSplitter3D、光度立体神经网络和法线积分方法，显著提高了机器人曲面触觉感知能力和法线估计精度。", "motivation": "为了使拟人化机械手实现类似人类的触觉感知能力，视觉触觉传感器（VTS）的柔顺传感表面需要从传统的平面配置发展到仿生曲面拓扑结构。然而，平面VTS在扩展到曲面时面临挑战，包括表面光照不足、重建模糊和表面结构复杂的空间边界条件。", "method": "本研究(i)通过使用棱镜和近红外（NIR）相机扩展成像通道，开发了GelSplitter3D；(ii)提出了一种光度立体神经网络，结合基于CAD的法线真值生成方法来校准触觉几何；(iii)设计了一种法线积分方法，利用深度先验信息的边界约束来纠正表面积分的累积误差。", "result": "研究结果表明，该方法实现了更好的触觉感知性能，法线估计精度提高了40%，并证明了传感器形状在抓取和操作任务中的优势。", "conclusion": "本文提出的GelSplitter3D传感器、光度立体神经网络和法线积分方法，有效解决了曲面视觉触觉重建的挑战，显著提升了机器人在抓取和操作任务中的精细触觉感知能力和法线估计精度。", "translation": "为了在拟人化机械手中实现类似人类的触觉感知能力，视觉触觉传感器（VTS）的柔顺传感表面必须从传统的平面配置演变为具有连续表面梯度的仿生曲面拓扑结构。然而，平面VTS在扩展到曲面时面临挑战，包括表面光照不足、重建模糊以及表面结构复杂的空间边界条件。为了构建一个类似人类指尖的最终目标，我们的研究（i）通过使用棱镜和近红外（NIR）相机扩展成像通道，开发了GelSplitter3D；（ii）提出了一种光度立体神经网络，结合基于CAD的法线真值生成方法来校准触觉几何；（iii）设计了一种法线积分方法，利用深度先验信息的边界约束来纠正表面积分的累积误差。我们展示了更好的触觉传感性能，法线估计精度提高了40%，以及传感器形状在抓取和操作任务中的优势。", "summary": "本文旨在解决平面视觉触觉传感器在应用于仿生曲面时面临的光照、模糊和边界条件复杂等挑战，以实现机器人类似人类的精细触觉感知。研究开发了GelSplitter3D传感器，通过扩展成像通道融入近红外光；提出了一种结合CAD真值生成的光度立体神经网络用于触觉几何校准；并设计了一种利用深度先验信息纠正累积误差的法线积分方法。实验结果表明，该方法显著提升了触觉感知性能，法线估计精度提高了40%，并验证了传感器形状在抓取和操作任务中的优势。", "keywords": "视觉触觉重建, 机器人触觉, GelSplitter3D, 光度立体, 曲面感知", "comments": "该论文通过引入红外成像和创新的重建算法，有效地解决了曲面视觉触觉传感器的关键挑战，是机器人精细操作和类人触觉感知领域的重要进展。GelSplitter3D的设计和结合CAD真值的校准方法具有新颖性，为未来仿生触觉传感器的发展提供了有价值的参考。"}}
{"id": "2506.15488", "title": "Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation", "authors": ["Hussam Al Daas", "Grey Ballard", "Laura Grigori", "Suraj Kumar", "Kathryn Rouse", "Mathieu Vérité"], "summary": "In this article, we focus on the parallel communication cost of multiplying\nthe same vector along two modes of a $3$-dimensional symmetric tensor. This is\na key computation in the higher-order power method for determining eigenpairs\nof a $3$-dimensional symmetric tensor and in gradient-based methods for\ncomputing a symmetric CP decomposition. We establish communication lower bounds\nthat determine how much data movement is required to perform the specified\ncomputation in parallel. The core idea of the proof relies on extending a key\ngeometric inequality for $3$-dimensional symmetric computations. We demonstrate\nthat the communication lower bounds are tight by presenting an optimal\nalgorithm where the data distribution is a natural extension of the triangle\nblock partition scheme for symmetric matrices to 3-dimensional symmetric\ntensors.", "comment": "19 pages, 1 figure", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15488v1", "AI": {"title_translation": "最小化并行对称张量与相同向量乘法的通信开销", "tldr": "本文研究了并行计算中3维对称张量与相同向量乘法的通信成本，建立了通信下界并提出了一个最优算法。", "motivation": "这种计算是高阶幂法确定3维对称张量特征对以及基于梯度的对称CP分解的关键计算。减少通信成本对于并行计算效率至关重要。", "method": "建立了通信下界，其证明核心是扩展了3维对称计算的关键几何不等式。通过提出一个最优算法来证明通信下界的紧致性，该算法的数据分布是三角形块分区方案向3维对称张量的自然扩展。", "result": "建立了并行对称张量与相同向量乘法的通信下界，并提出了一个达到这些下界的最优算法。", "conclusion": "本文成功地确定了并行计算3维对称张量与相同向量乘法所需的最小数据移动量，并通过提供一个最优算法实现了通信效率。", "translation": "在本文中，我们关注的是3维对称张量沿两个模态乘以相同向量的并行通信成本。这是确定3维对称张量特征对的高阶幂方法以及计算对称CP分解的基于梯度方法中的关键计算。我们建立了通信下界，这些下界确定了并行执行指定计算所需的数据移动量。证明的核心思想是扩展了3维对称计算的关键几何不等式。我们通过提出一个最优算法来证明通信下界的紧致性，该算法的数据分布是矩阵对称块分区方案向3维对称张量的自然扩展。", "summary": "本文研究了3维对称张量与相同向量乘法的并行通信成本，该计算在张量特征值问题和CP分解中至关重要。作者建立了通信下界，并通过扩展几何不等式来证明。此外，本文还提出了一个最优算法，该算法的数据分布方案是针对对称矩阵的三角形块分区方案的3维扩展，证明了所建立下界的紧致性。", "keywords": "对称张量, 并行计算, 通信下界, 最优算法, 张量分解", "comments": "本文通过建立通信下界并提出最优算法，为并行张量计算的通信优化提供了理论基础和实用方法，对于提高张量计算的效率具有重要意义。其创新之处在于将对称矩阵的块分区思想推广到高维对称张量，并利用几何不等式进行理论分析。"}}
{"id": "2506.14835", "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "summary": "Precisely localizing 3D objects from a single image constitutes a central\nchallenge in monocular 3D detection. While DETR-like architectures offer a\npowerful paradigm, their direct application in this domain encounters inherent\nlimitations, preventing optimal performance. Our work addresses these\nchallenges by introducing MonoVQD, a novel framework designed to fundamentally\nadvance DETR-based monocular 3D detection. We propose three main contributions.\nFirst, we propose the Mask Separated Self-Attention mechanism that enables the\nintegration of the denoising process into a DETR architecture. This improves\nthe stability of Hungarian matching to achieve a consistent optimization\nobjective. Second, we present the Variational Query Denoising technique to\naddress the gradient vanishing problem of conventional denoising methods, which\nseverely restricts the efficiency of the denoising process. This explicitly\nintroduces stochastic properties to mitigate this fundamental limitation and\nunlock substantial performance gains. Finally, we introduce a sophisticated\nself-distillation strategy, leveraging insights from later decoder layers to\nsynergistically improve query quality in earlier layers, thereby amplifying the\niterative refinement process. Rigorous experimentation demonstrates that\nMonoVQD achieves superior performance on the challenging KITTI monocular\nbenchmark. Highlighting its broad applicability, MonoVQD's core components\nseamlessly integrate into other architectures, delivering significant\nperformance gains even in multi-view 3D detection scenarios on the nuScenes\ndataset and underscoring its robust generalization capabilities.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14835v1", "AI": {"title_translation": "MonoVQD：基于变分查询去噪和自蒸馏的单目3D目标检测", "tldr": "MonoVQD是一种用于单目3D目标检测的新型框架，它通过引入掩码分离自注意力机制、变分查询去噪技术和自蒸馏策略，解决了DETR类架构在单目3D检测中的局限性，并在KITTI和nuScenes数据集上取得了显著性能提升。", "motivation": "在单目3D检测中，从单张图像精确地定位3D目标是一个核心挑战。DETR类架构虽然强大，但直接应用于该领域存在固有限制，阻碍了其最佳性能。", "method": "本文提出了MonoVQD框架，包含三项主要贡献：1. 提出掩码分离自注意力机制，将去噪过程整合到DETR架构中，以提高匈牙利匹配的稳定性并实现一致的优化目标。2. 引入变分查询去噪技术，解决传统去噪方法中梯度消失问题，通过引入随机特性来缓解这一限制并提升性能。3. 提出一种复杂的自蒸馏策略，利用后期解码器层的见解协同改进早期层中的查询质量，从而增强迭代细化过程。", "result": "MonoVQD在具有挑战性的KITTI单目基准测试中取得了卓越的性能。其核心组件可以无缝集成到其他架构中，即使在nuScenes数据集上的多视图3D检测场景中也能带来显著的性能提升，突显了其强大的泛化能力。", "conclusion": "MonoVQD通过其创新的组件（掩码分离自注意力、变分查询去噪和自蒸馏）有效地解决了DETR类架构在单目3D检测中的固有局限性，并在多个数据集上展示了卓越的性能和泛化能力，为单目和多视图3D目标检测提供了强大的解决方案。", "translation": "从单张图像精确地定位3D目标构成了单目3D检测中的核心挑战。虽然DETR类架构提供了一个强大的范式，但它们在该领域的直接应用遇到了固有限制，阻碍了最佳性能。我们的工作通过引入MonoVQD来解决这些挑战，这是一个旨在从根本上推进基于DETR的单目3D检测的新颖框架。我们提出了三项主要贡献。首先，我们提出了掩码分离自注意力机制，该机制能够将去噪过程集成到DETR架构中。这提高了匈牙利匹配的稳定性，以实现一致的优化目标。其次，我们提出了变分查询去噪技术，以解决传统去噪方法中梯度消失的问题，这严重限制了去噪过程的效率。这明确引入了随机特性，以缓解这一根本限制并解锁显著的性能增益。最后，我们引入了一种复杂的自蒸馏策略，利用后期解码器层的见解协同改进早期层中的查询质量，从而放大迭代细化过程。严格的实验表明，MonoVQD在具有挑战性的KITTI单目基准测试中取得了卓越的性能。MonoVQD的核心组件可以无缝集成到其他架构中，即使在nuScenes数据集上的多视图3D检测场景中也能带来显著的性能增益，这突显了其广泛的适用性和强大的泛化能力。", "summary": "MonoVQD是一个新颖的框架，旨在解决DETR类架构在单目3D目标检测中的局限性。该框架通过引入三项核心技术实现性能提升：掩码分离自注意力机制，用于稳定匈牙利匹配并整合去噪；变分查询去噪技术，用于克服传统去噪方法的梯度消失问题；以及自蒸馏策略，用于增强查询质量和迭代细化。实验证明，MonoVQD在KITTI单目基准测试上表现优异，并且其组件在nuScenes多视图3D检测中也展现出强大的泛化能力和显著的性能提升。", "keywords": "单目3D目标检测, DETR, 变分查询去噪, 自蒸馏, 掩码分离自注意力", "comments": "MonoVQD的创新之处在于其对DETR架构的深度改进，特别是通过变分查询去噪技术解决了传统去噪方法中的梯度消失问题，并引入了自蒸馏策略来优化查询质量。这些方法不仅提升了单目3D检测的精度，还展示了其在多视图场景下的泛化能力，为基于Transformer的3D目标检测提供了重要的发展方向。"}}
{"id": "2506.15639", "title": "The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy", "authors": ["James Weichert", "Daniel Dunlap", "Mohammed Farghally", "Hoda Eldardiry"], "summary": "As artificial intelligence (AI) further embeds itself into many settings\nacross personal and professional contexts, increasing attention must be paid\nnot only to AI ethics, but also to the governance and regulation of AI\ntechnologies through AI policy. However, the prevailing post-secondary\ncomputing curriculum is currently ill-equipped to prepare future AI\npractitioners to confront increasing demands to implement abstract ethical\nprinciples and normative policy preferences into the design and development of\nAI systems. We believe that familiarity with the 'AI policy landscape' and the\nability to translate ethical principles to practices will in the future\nconstitute an important responsibility for even the most technically-focused AI\nengineers.\n  Toward preparing current computer science (CS) students for these new\nexpectations, we developed an AI Policy Module to introduce discussions of AI\npolicy into the CS curriculum. Building on a successful pilot in fall 2024, in\nthis innovative practice full paper we present an updated and expanded version\nof the module, including a technical assignment on \"AI regulation\". We present\nthe findings from our pilot of the AI Policy Module 2.0, evaluating student\nattitudes towards AI ethics and policy through pre- and post-module surveys.\nFollowing the module, students reported increased concern about the ethical\nimpacts of AI technologies while also expressing greater confidence in their\nabilities to engage in discussions about AI regulation. Finally, we highlight\nthe AI Regulation Assignment as an effective and engaging tool for exploring\nthe limits of AI alignment and emphasizing the role of 'policy' in addressing\nethical challenges.", "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15639v1", "AI": {"title_translation": "AI政策模块：培养计算机科学学生在AI伦理与政策方面的能力", "tldr": "本文介绍了一个AI政策模块，旨在将AI伦理和政策讨论融入计算机科学课程，以培养学生应对AI伦理和监管挑战的能力。", "motivation": "随着人工智能日益融入个人和专业环境，不仅需要关注AI伦理，还需要通过AI政策对AI技术进行治理和监管。然而，当前的高等计算课程未能充分准备未来的AI从业者，使其能够将抽象的伦理原则和规范政策偏好转化为AI系统的设计和开发实践。作者认为，熟悉“AI政策格局”并将伦理原则转化为实践的能力将成为未来AI工程师的重要职责。", "method": "作者开发了一个AI政策模块，旨在将AI政策讨论引入计算机科学课程。他们基于2024年秋季的成功试点，提出了一个更新和扩展的模块版本，其中包括一项关于“AI监管”的技术作业。他们通过模块前和模块后的调查评估了学生对AI伦理和政策的态度，从而呈现了AI政策模块2.0试点结果。", "result": "模块结束后，学生报告了对AI技术伦理影响的担忧增加，同时也对参与AI监管讨论的能力表现出更大的信心。AI监管作业被证明是探索AI对齐限制和强调“政策”在解决伦理挑战中作用的有效且引人入胜的工具。", "conclusion": "AI政策模块有效地提高了计算机科学学生对AI伦理和政策的关注度及参与能力，证明了将AI政策教育融入现有课程的必要性和有效性。", "translation": "随着人工智能（AI）进一步融入个人和专业环境，不仅需要关注AI伦理，还需要通过AI政策对AI技术进行治理和监管。然而，当前的高等计算课程未能充分准备未来的AI从业者，使其能够应对将抽象伦理原则和规范政策偏好融入AI系统设计和开发日益增长的需求。我们相信，熟悉“AI政策格局”以及将伦理原则转化为实践的能力，在未来将成为即使是最专注于技术的AI工程师的一项重要职责。\n为了使当前的计算机科学（CS）学生为这些新期望做好准备，我们开发了一个AI政策模块，将AI政策的讨论引入CS课程。基于2024年秋季的成功试点，在这篇创新的实践型完整论文中，我们介绍了模块的更新和扩展版本，其中包括一项关于“AI监管”的技术作业。我们展示了AI政策模块2.0试点项目的研究结果，通过模块前和模块后的调查评估了学生对AI伦理和政策的态度。模块结束后，学生报告了对AI技术伦理影响的担忧增加，同时也对参与AI监管讨论的能力表现出更大的信心。最后，我们强调AI监管作业是探索AI对齐限制和强调“政策”在解决伦理挑战中作用的有效且引人入胜的工具。", "summary": "本文介绍了一个名为“AI政策模块”的创新教育工具，旨在解决当前计算机科学课程在培养学生应对AI伦理和政策挑战方面的不足。该模块将AI政策讨论融入CS课程，并通过一项关于“AI监管”的技术作业进行强化。通过对模块2.0版本的试点评估，研究发现学生在完成模块后，对AI伦理影响的关注度增加，并对参与AI监管讨论的能力更有信心。该研究强调了将AI政策教育融入CS课程的重要性，并提出了一个有效的教学实践。", "keywords": "AI伦理, AI政策, 计算机科学教育, 课程开发, AI监管", "comments": "本文提出的AI政策模块具有创新性，它直接弥补了当前计算机科学教育在AI伦理和政策方面的空白。其重要性在于，随着AI技术日益普及，培养负责任的AI开发者至关重要。通过实际的模块和作业，论文不仅提出了问题，还提供了一个可行的解决方案。该研究的局限性可能在于其试点规模和长期效果的评估。"}}
{"id": "2506.14793", "title": "Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout", "authors": ["Aditya Ravuri", "Neil D. Lawrence"], "summary": "Protein Language Models (PLMs) such as ESM2 have been shown to be capable of\nzero-shot prediction of critical scalar properties of proteins (fitness). In\nthis work, we show that injecting a dropout layer at inference time between a\nPLM's featurizer/embedding layer and its transformer, and averaging its output\nakin to Monte-Carlo dropout increases zero-shot performance on a subset of the\nProteinGym dataset. This is the case even when the model was not trained with\ndropouts to begin with, and does not require retraining or finetuning of the\nPLM. A dropout of 0.1 seems performant across all models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14793v1", "AI": {"title_translation": "蛋白质语言模型零样本适应度预测通过仅推理Dropout得到改进", "tldr": "在蛋白质语言模型（PLM）中，仅在推理时注入Dropout层并平均其输出，可以提高零样本适应度预测性能，且无需重新训练或微调。", "motivation": "蛋白质语言模型（PLM）如ESM2已被证明能够进行蛋白质关键标量属性（适应度）的零样本预测。本工作旨在探究如何在不重新训练或微调PLM的情况下，进一步提高其零样本预测性能。", "method": "在PLM的特征提取器/嵌入层与其Transformer之间注入一个Dropout层，并在推理时平均其输出，类似于蒙特卡洛Dropout。", "result": "即使模型最初没有使用Dropout进行训练，这种方法也能提高在ProteinGym数据集子集上的零样本性能。Dropout率为0.1在所有模型中表现良好。", "conclusion": "在蛋白质语言模型中，仅在推理时注入Dropout层并平均其输出，可以有效提高零样本适应度预测性能，且无需对模型进行重新训练或微调。", "translation": "蛋白质语言模型（PLM）如ESM2已被证明能够对蛋白质的关键标量属性（适应度）进行零样本预测。在这项工作中，我们展示了在推理时，在PLM的特征提取器/嵌入层与其Transformer之间注入一个Dropout层，并平均其输出（类似于蒙特卡洛Dropout），可以提高在ProteinGym数据集子集上的零样本性能。即使模型最初没有使用Dropout进行训练，这种情况也成立，并且不需要对PLM进行重新训练或微调。Dropout率为0.1似乎在所有模型中都表现良好。", "summary": "本研究提出一种在蛋白质语言模型（PLM）中提高零样本适应度预测性能的方法。通过在推理阶段，于PLM的特征提取器和Transformer之间注入一个Dropout层并对输出进行平均，即使模型未曾使用Dropout训练，也能显著提升其在ProteinGym数据集上的表现，且无需任何模型重训练或微调。实验表明，0.1的Dropout率在不同模型上均表现良好。", "keywords": "蛋白质语言模型, 零样本预测, Dropout, 适应度预测, ProteinGym", "comments": "本文的创新点在于提出了一种无需重新训练或微调模型即可提高蛋白质语言模型零样本预测性能的方法，这对于实际应用具有重要意义。通过在推理时引入Dropout，有效地利用了模型的内在不确定性来提升预测的鲁棒性和准确性。"}}
{"id": "2506.15366", "title": "Performative Validity of Recourse Explanations", "authors": ["Gunnar König", "Hidde Fokkema", "Timo Freiesleben", "Celestine Mendler-Dünner", "Ulrike on Luxburg"], "summary": "When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.", "comment": "34 pages, 3 figures, 1 table, Preprint", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15366v1", "AI": {"title_translation": "追索解释的表现性有效性", "tldr": "追索解释可能因其自身的影响而失效，当推荐操作涉及非因果变量时尤其如此。", "motivation": "追索解释存在一个关键但被忽视的现象：当大量申请人根据建议采取行动时，他们的集体行为可能会改变数据中的统计规律和决策边界，从而使追索算法自身的推荐失效。", "method": "本文正式刻画了在表现性下追索解释保持有效的条件。", "result": "一个关键发现是，如果追索行动受到非因果变量的影响或对其进行干预，它们可能会失效。", "conclusion": "基于分析，作者警告不要使用标准的反事实解释和因果追索方法，并提倡使用仅推荐针对因果变量行动的追索方法。", "translation": "当申请人被算法决策系统拒绝时，追索解释提供了可操作的建议，说明如何改变其输入特征以获得积极评估。一个关键但被忽视的现象是，追索解释是表现性的：当许多申请人根据其建议采取行动时，他们的集体行为可能会改变数据中的统计规律，并且一旦模型重新拟合，也会改变决策边界。因此，追索算法可能会使其自身的建议失效，导致那些努力实施建议的申请人再次申请时可能再次被拒绝。在这项工作中，我们正式刻画了追索解释在表现性下保持有效的条件。一个关键发现是，如果追索行动受到非因果变量的影响或对其进行干预，它们可能会失效。基于我们的分析，我们警告不要使用标准的反事实解释和因果追索方法，而是提倡使用仅推荐针对因果变量行动的追索方法。", "summary": "本文探讨了算法决策系统中追索解释的“表现性有效性”问题。当大量用户根据追索建议修改输入特征时，可能改变底层数据分布和模型决策边界，导致原有的追索建议失效。研究正式刻画了追索解释在表现性下保持有效的条件，并发现若建议涉及非因果变量，则更容易失效。因此，作者建议避免使用基于非因果变量的反事实解释和因果追索方法，而应专注于仅推荐针对因果变量的行动。", "keywords": "追索解释, 表现性, 算法公平性, 因果变量, 反事实解释", "comments": "这篇论文揭示了算法解释领域一个重要的、此前被忽视的问题——“表现性”对追索解释有效性的影响。其创新之处在于首次对这种现象进行了形式化刻画，并提出了关键发现，即非因果变量是导致解释失效的关键因素。这对于设计鲁棒和公平的算法解释系统具有重要指导意义，提醒研究者和实践者在提供建议时需考虑其长期和集体影响，并强调了因果推理在可解释AI中的重要性。"}}
{"id": "2506.15398", "title": "Multi-dimensional evaluation on a rural integrated energy system including solar, wind, biomass and geothermal energy", "authors": ["Ruonan Lia", "Chang Wena", "Mingyu Yan", "Congcong Wu", "Ahmed Lotfy Elrefai", "Xiaotong Zhang", "Sahban Wael Saeed Alnaser"], "summary": "This study focuses on the novel municipal-scale rural integrated energy\nsystem (RIES), which encompasses energy supply and application. By constructing\na seven-dimensional evaluation system including energy efficiency, energy\nsupply, low-carbon sustainability, environmental impact, energy economy, social\nbenefits, and integrated energy system development, this research combines the\nimproved analytic hierarchy process (IAHP) and entropy weight method (EWM) by\nsum of squares of deviations to balance expert experience and data objectivity.\nFurthermore, the cloud model is introduced to handle the fuzziness and\nrandomness in the evaluation. This method can quantify the differences in\nsystem performance before and after the planning implementation. The results\nindicate that after planning, the comprehensive score has increased from 83.12\nto 87.55, the entropy value has decreased from 6.931 to 5.336, indicating\nenhanced system stability. The hyper-entropy has dropped from 3.08 to 2.278,\nreflecting a reduction in uncertainty. The research findings provide a\nscientific basis for the planning optimization, policy-making, and sustainable\ndevelopment of rural integrated energy systems, possessing both theoretical\ninnovation and practical guiding value.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15398v1", "AI": {"title_translation": "农村综合能源系统多维度评价，包括太阳能、风能、生物质能和地热能", "tldr": "本研究对新型市级农村综合能源系统进行了七维度评价，结合改进层次分析法、熵权法和云模型，量化了规划实施前后系统性能差异，结果显示规划后系统综合得分提高，稳定性和不确定性降低。", "motivation": "旨在对新型市级农村综合能源系统（RIES）进行评估，该系统涵盖能源供应和应用。", "method": "本研究构建了一个包括能源效率、能源供应、低碳可持续性、环境影响、能源经济、社会效益和综合能源系统发展在内的七维度评价体系。结合改进的层次分析法（IAHP）和基于偏差平方和的熵权法（EWM）来平衡专家经验和数据客观性。此外，引入云模型来处理评估中的模糊性和随机性。", "result": "规划实施后，综合得分从83.12提高到87.55；熵值从6.931降低到5.336，表明系统稳定性增强；超熵从3.08下降到2.278，反映不确定性降低。", "conclusion": "研究结果为农村综合能源系统的规划优化、政策制定和可持续发展提供了科学依据，兼具理论创新和实践指导价值。", "translation": "本研究聚焦于新型市级农村综合能源系统（RIES），该系统涵盖能源供应和应用。通过构建包括能源效率、能源供应、低碳可持续性、环境影响、能源经济、社会效益和综合能源系统发展在内的七维度评价体系，本研究结合改进的层次分析法（IAHP）和基于偏差平方和的熵权法（EWM），以平衡专家经验和数据客观性。此外，引入云模型来处理评估中的模糊性和随机性。该方法可以量化规划实施前后系统性能的差异。结果表明，规划后，综合得分从83.12提高到87.55，熵值从6.931降低到5.336，表明系统稳定性增强。超熵从3.08下降到2.278，反映不确定性降低。研究结果为农村综合能源系统的规划优化、政策制定和可持续发展提供了科学依据，兼具理论创新和实践指导价值。", "summary": "本文针对新型市级农村综合能源系统（RIES）的能源供应与应用，构建了一个七维度评价体系。该研究结合改进的层次分析法和熵权法，并引入云模型来处理评估中的不确定性。研究结果表明，规划实施后系统综合性能显著提升，稳定性和确定性增强。这些发现为农村综合能源系统的规划、政策制定和可持续发展提供了科学依据。", "keywords": "农村综合能源系统, 多维度评价, 层次分析法, 熵权法, 云模型", "comments": "本文的创新点在于提出了一个包含七个维度的综合评价体系，并结合了改进的层次分析法、熵权法和云模型，有效处理了评估中的专家经验、数据客观性、模糊性和随机性。这种多方法融合的评价模型为农村综合能源系统的性能评估提供了一个全面且量化分析的框架，具有重要的理论和实践指导价值。"}}
{"id": "2506.15364", "title": "Brain Stroke Classification Using Wavelet Transform and MLP Neural Networks on DWI MRI Images", "authors": ["Mana Mohammadi", "Amirhesam Jafari Rad", "Ashkan Behrouzi"], "summary": "This paper presents a lightweight framework for classifying brain stroke\ntypes from Diffusion-Weighted Imaging (DWI) MRI scans, employing a Multi-Layer\nPerceptron (MLP) neural network with Wavelet Transform for feature extraction.\nAccurate and timely stroke detection is critical for effective treatment and\nimproved patient outcomes in neuroimaging. While Convolutional Neural Networks\n(CNNs) are widely used for medical image analysis, their computational\ncomplexity often hinders deployment in resource-constrained clinical settings.\nIn contrast, our approach combines Wavelet Transform with a compact MLP to\nachieve efficient and accurate stroke classification. Using the \"Brain Stroke\nMRI Images\" dataset, our method yields classification accuracies of 82.0% with\nthe \"db4\" wavelet (level 3 decomposition) and 86.00% with the \"Haar\" wavelet\n(level 2 decomposition). This analysis highlights a balance between diagnostic\naccuracy and computational efficiency, offering a practical solution for\nautomated stroke diagnosis. Future research will focus on enhancing model\nrobustness and integrating additional MRI modalities for comprehensive stroke\nassessment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15364v1", "AI": {"title_translation": "使用小波变换和MLP神经网络对DWI MRI图像进行脑卒中分类", "tldr": "本研究提出了一种结合小波变换和多层感知器（MLP）神经网络的轻量级框架，用于对DWI MRI图像进行脑卒中分类，实现了计算效率和诊断准确性的平衡。", "motivation": "准确及时的脑卒中检测对于有效的治疗和改善神经影像学中的患者预后至关重要。尽管卷积神经网络（CNN）广泛用于医学图像分析，但其计算复杂性常常阻碍在资源受限的临床环境中部署。", "method": "本研究提出了一种轻量级框架，通过使用小波变换进行特征提取，并结合多层感知器（MLP）神经网络对扩散加权成像（DWI）MRI扫描图像中的脑卒中类型进行分类。", "result": "使用“脑卒中MRI图像”数据集，该方法在使用“db4”小波（3级分解）时分类准确率达到82.0%，使用“Haar”小波（2级分解）时分类准确率达到86.00%。", "conclusion": "本分析强调了诊断准确性和计算效率之间的平衡，为自动化脑卒中诊断提供了一个实用的解决方案。", "translation": "本文提出了一种轻量级框架，利用多层感知器（MLP）神经网络和小波变换进行特征提取，对扩散加权成像（DWI）MRI扫描图像中的脑卒中类型进行分类。准确及时的脑卒中检测对于有效的治疗和改善神经影像学中的患者预后至关重要。尽管卷积神经网络（CNN）广泛用于医学图像分析，但其计算复杂性常常阻碍在资源受限的临床环境中部署。相比之下，我们的方法结合了小波变换和紧凑的MLP，以实现高效准确的脑卒中分类。使用“脑卒中MRI图像”数据集，我们的方法在使用“db4”小波（3级分解）时分类准确率达到82.0%，使用“Haar”小波（2级分解）时分类准确率达到86.00%。本分析强调了诊断准确性和计算效率之间的平衡，为自动化脑卒中诊断提供了一个实用的解决方案。未来的研究将侧重于增强模型鲁棒性并整合额外的MRI模态以进行全面的脑卒中评估。", "summary": "本论文介绍了一种用于DWI MRI图像脑卒中分类的轻量级框架，该框架结合了小波变换进行特征提取和多层感知器（MLP）神经网络。针对CNN计算复杂性高的问题，该方法旨在提供一个在资源受限临床环境中可部署的高效且准确的解决方案。实验结果显示，在使用“db4”和“Haar”小波时，分类准确率分别达到82.0%和86.00%，证明了其在诊断准确性和计算效率之间的平衡。", "keywords": "脑卒中分类, 小波变换, MLP神经网络, DWI MRI, 计算效率", "comments": "该论文的创新之处在于提出了一种轻量级的脑卒中分类框架，通过结合小波变换和MLP神经网络，有效解决了传统CNN模型在资源受限临床环境中的部署难题。其强调计算效率与诊断准确性的平衡，为实际应用提供了有前景的解决方案。"}}
{"id": "2506.15463", "title": "Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "summary": "In practical systems, recorded analog signals must be digitized for\nprocessing, introducing quantization as a critical aspect of data acquisition.\nWhile prior studies have examined quantization effects in various signal\nprocessing contexts, its impact on differential microphone arrays (DMAs),\nparticularly in one-dimensional (1D) first-order configurations, remains\nunexplored. This paper investigates the influence of signal quantization on\nperformance of first-order 1D DMAs across various beampatterns. An analytical\nexpression for quantized beamformed output for a first-order 1D DMA has been\nformulated. The effect of signal quantization has been studied on array\nperformance measures such as the Beampattern, Directivity Factor (DF),\nFront-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that\nbeampattern shape remains structurally invariant across quantization bit\ndepths, with quantization primarily affecting ND. DF and FBR remain constant\nwith the varying number of quantization bits. Additionally, ND is shown to be\nfrequency-independent; however, it increases with increasing quantization bit\ndepths, enhancing interference suppression. The study also examines the effect\nof steering nulls across the azimuthal range, showing that ND degrades as the\nnull moves closer to the source look direction, indicating reduced interference\nsuppression.", "comment": "5 Pages with 6 figures and 1 table", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15463v1", "AI": {"title_translation": "信号量化对一阶一维差分麦克风阵列性能测量的影响", "tldr": "本研究探讨了信号量化对一阶一维差分麦克风阵列（DMA）性能的影响，发现波束图形状保持不变，但零点深度随量化比特深度增加而改善，而指向性因子和前后比保持不变。", "motivation": "在实际系统中，模拟信号必须数字化处理，引入量化。尽管现有研究探讨了量化在各种信号处理中的影响，但其对差分麦克风阵列（DMA），尤其是一维一阶配置的影响尚未被探索。", "method": "本文研究了信号量化对一阶一维DMA在各种波束图下的性能影响。制定了一阶一维DMA量化波束形成输出的解析表达式。通过仿真研究了信号量化对阵列性能指标的影响，如波束图、指向性因子（DF）、前后比（FBR）和零点深度（ND）。", "result": "仿真结果表明，波束图形状在不同量化比特深度下结构保持不变，量化主要影响零点深度（ND）。指向性因子（DF）和前后比（FBR）随量化比特数的变化保持不变。零点深度（ND）与频率无关，但随量化比特深度增加而增加，从而增强了干扰抑制。当零点接近声源方向时，零点深度会降低，表明干扰抑制能力减弱。", "conclusion": "信号量化主要影响差分麦克风阵列的零点深度，提高量化比特深度可以增强干扰抑制能力，但零点位置会影响其性能。", "translation": "在实际系统中，记录的模拟信号必须数字化处理，这使得量化成为数据采集的关键方面。尽管先前的研究已经检查了量化在各种信号处理环境中的影响，但其对差分麦克风阵列（DMAs），特别是一维（1D）一阶配置的影响仍然未被探索。本文调查了信号量化对一阶一维DMAs在各种波束图下性能的影响。已经制定了一阶一维DMA量化波束形成输出的解析表达式。信号量化对阵列性能指标的影响，如波束图、指向性因子（DF）、前后比（FBR）和零点深度（ND）已被研究。仿真结果表明，波束图形状在不同量化比特深度下结构保持不变，量化主要影响零点深度（ND）。指向性因子（DF）和前后比（FBR）随量化比特数的变化保持不变。此外，零点深度（ND）被证明与频率无关；然而，它随着量化比特深度的增加而增加，从而增强了干扰抑制。该研究还检查了在方位角范围内转向零点的影响，表明当零点移近声源视线方向时，零点深度会降低，这表明干扰抑制能力减弱。", "summary": "本论文研究了信号量化对一阶一维差分麦克风阵列（DMA）性能的影响。研究发现，量化主要影响零点深度，随着量化比特深度的增加，零点深度增强，从而提高干扰抑制能力。而波束图形状、指向性因子和前后比则不受量化比特深度影响。此外，当零点接近声源方向时，干扰抑制能力会减弱。", "keywords": "信号量化, 差分麦克风阵列, 零点深度, 指向性因子, 波束图", "comments": "这项研究填补了量化对差分麦克风阵列影响的空白，特别是关注了一阶一维配置。其创新在于提出了量化波束形成输出的解析表达式，并通过仿真详细分析了量化对各项性能指标的影响。结果对于实际系统设计中选择合适的量化比特深度以平衡性能和资源具有指导意义。"}}
{"id": "2506.15081", "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification", "authors": ["Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "summary": "Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.", "comment": "Accepted by ACL2025(main conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15081v1", "AI": {"title_translation": "通过篇章感知的话语澄清改进对话篇章分析", "tldr": "本文提出了一种篇章感知澄清模块（DCM）和贡献感知偏好优化（CPO）方法，以解决对话篇章分析中的歧义问题，并显著优于现有SOTA基线。", "motivation": "对话篇章分析中，省略和习语等语言特征常引入歧义，模糊了意图篇章关系，对分析器构成挑战。", "method": "本文提出了一个篇章感知澄清模块（DCM）来增强对话篇章分析器的性能。DCM利用澄清类型推理和篇章目标推理两种推理过程。此外，引入了贡献感知偏好优化（CPO）来减轻错误澄清的风险，减少级联错误。CPO使分析器能够评估DCM澄清的贡献并提供反馈以优化DCM。", "result": "在STAC和Molweni数据集上的大量实验表明，所提出的方法有效解决了歧义，并显著优于现有最先进（SOTA）基线。", "conclusion": "本文提出的篇章感知澄清模块（DCM）和贡献感知偏好优化（CPO）方法能够有效解决对话篇章分析中的歧义问题，并显著提升了分析性能。", "translation": "对话篇章分析旨在识别和分析对话中话语之间的篇章关系。然而，对话中的语言特征，如省略和习语，经常引入歧义，模糊了预期的篇章关系，给分析器带来了重大挑战。为了解决这个问题，我们提出了一个篇章感知澄清模块（DCM）来增强对话篇章分析器的性能。DCM采用了两种不同的推理过程：澄清类型推理和篇章目标推理。前者分析语言特征，而后者区分意图关系和歧义关系。此外，我们引入了贡献感知偏好优化（CPO）来减轻错误澄清的风险，从而减少级联错误。CPO使分析器能够评估DCM澄清的贡献并提供反馈以优化DCM，从而增强其适应性并与分析器的要求保持一致。在STAC和Molweni数据集上的大量实验表明，我们的方法有效解决了歧义，并显著优于现有最先进（SOTA）基线。", "summary": "本文针对对话篇章分析中因语言特征（如省略和习语）导致的歧义问题，提出了一种篇章感知澄清模块（DCM）和贡献感知偏好优化（CPO）方法。DCM通过澄清类型推理和篇章目标推理来识别和解决歧义，而CPO则通过评估澄清贡献并提供反馈来优化DCM，从而减少错误并提高模型与分析器需求的对齐。实验证明，该方法在STAC和Molweni数据集上有效解决了歧义并显著优于现有SOTA基线。", "keywords": "对话篇章分析, 歧义解决, 篇章感知澄清模块, 贡献感知偏好优化, 自然语言处理", "comments": "本文的创新点在于提出了一个专门用于解决对话篇章分析中歧义问题的澄清模块（DCM），并通过贡献感知偏好优化（CPO）机制有效地降低了错误澄清的风险，提高了系统的鲁棒性。这种将澄清与优化相结合的方法对于提升对话理解的准确性具有重要意义。"}}
{"id": "2506.14814", "title": "Semi-orthogonal Tribonacci Wavelets and Numerical Solutions of Nonlinear Singular BVPs Arising in a Chemical Reaction", "authors": ["Ankita Yadav", "Amit K. Verma"], "summary": "In this article, we introduce a semi-orthogonal tribonacci wavelet and\ndevelop a semi-orthogonal tribonacci wavelet collocation method, offering an\neffective numerical method for solving a class of non-linear singular BVPs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14814v1", "AI": {"title_translation": "半正交Tribonacci小波与化学反应中非线性奇异边值问题的数值解", "tldr": "本文引入并开发了一种半正交Tribonacci小波配置方法，为解决一类非线性奇异边值问题提供了一种有效的数值方法。", "motivation": "为解决一类非线性奇异边值问题（BVPs）提供一种有效的数值方法。", "method": "引入半正交Tribonacci小波并开发了半正交Tribonacci小波配置方法。", "result": "Not mentioned in abstract", "conclusion": "开发的方法为解决一类非线性奇异边值问题提供了一种有效的数值方法。", "translation": "在本文中，我们引入了一种半正交Tribonacci小波，并开发了一种半正交Tribonacci小波配置方法，为解决一类非线性奇异边值问题提供了一种有效的数值方法。", "summary": "本文介绍了一种半正交Tribonacci小波，并基于此开发了一种半正交Tribonacci小波配置方法，旨在为解决非线性奇异边值问题提供一种有效的数值解决方案。", "keywords": "Tribonacci小波, 数值解, 非线性奇异边值问题, 边值问题, 小波配置方法", "comments": "本文的创新点在于引入了一种新型的半正交Tribonacci小波，并将其应用于构建一种新的配置方法，以解决传统方法在处理非线性奇异边值问题时可能遇到的困难。"}}
{"id": "2506.15117", "title": "CipherMind: The Longest Codebook in the World", "authors": ["Ming Nie", "Zhixiong Yang", "Bingsheng Wei"], "summary": "In recent years, the widespread application of large language models has\ninspired us to consider using inference for communication encryption. We\ntherefore propose CipherMind, which utilizes intermediate results from\ndeterministic fine-tuning of large model inferences as transmission content.\nThe semantic parameters of large models exhibit characteristics like opaque\nunderlying implementations and weak interpretability, thus enabling their use\nas an encryption method for data transmission. This communication paradigm can\nbe applied in scenarios like intra-gateway transmission, and theoretically, it\ncan be implemented using any large model as its foundation.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15117v1", "AI": {"title_translation": "CipherMind: 世界上最长的密码本", "tldr": "CipherMind提出了一种利用大型语言模型推理的中间结果进行通信加密的方法。", "motivation": "近年来大型语言模型的广泛应用启发了作者考虑使用推理进行通信加密。", "method": "CipherMind利用大型模型确定性微调推理的中间结果作为传输内容。大型模型的语义参数具有底层实现不透明和可解释性弱的特点，因此可作为数据传输的加密方法。该通信范式可应用于网关内传输等场景，理论上可基于任何大型模型实现。", "result": "Not mentioned in abstract", "conclusion": "该通信范式可应用于网关内传输等场景，理论上可基于任何大型模型实现。", "translation": "近年来，大型语言模型的广泛应用启发我们考虑使用推理进行通信加密。因此，我们提出了CipherMind，它利用大型模型确定性微调推理的中间结果作为传输内容。大型模型的语义参数表现出底层实现不透明和弱可解释性等特点，因此可以将其用作数据传输的加密方法。这种通信范式可以应用于网关内传输等场景，理论上可以使用任何大型模型作为其基础。", "summary": "CipherMind是一种基于大型语言模型推理中间结果的通信加密方法。该方法利用大型模型语义参数的不透明性和弱可解释性进行数据传输加密，适用于网关内传输等场景，并可理论上基于任何大型模型实现。", "keywords": "大型语言模型, 通信加密, 推理, CipherMind, 密码本", "comments": "这篇论文提出了一种新颖的加密方法，利用大型语言模型固有的复杂性和不透明性进行数据加密，这在概念上具有创新性。其重要性在于为特定场景（如网关内传输）提供了一种潜在的、基于AI的加密方案。然而，抽象中没有提及具体的加密强度、性能指标或安全性分析，这可能限制了对其实际应用潜力的评估。"}}
{"id": "2506.15189", "title": "Accessible Gesture-Driven Augmented Reality Interaction System", "authors": ["Yikan Wang"], "summary": "Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15189v1", "AI": {"title_translation": "无障碍手势驱动增强现实交互系统", "tldr": "本研究提出一种基于手势的增强现实交互系统，利用深度学习和可穿戴传感器提高运动障碍用户的AR可访问性，实验证明其在效率和满意度方面有显著提升。", "motivation": "增强现实（AR）虽然提供沉浸式交互，但由于依赖精确的输入方法，对运动障碍或手部灵活性受限的用户来说仍然难以使用。", "method": "该系统利用深度学习（包括ViTs、TCNs和GATs）从可穿戴传感器和摄像头识别手势和身体姿态，并根据用户能力调整界面。采用联邦学习进行隐私保护的模型训练，并通过强化学习优化菜单布局和交互模式等界面元素。", "result": "实验表明，与基线AR系统相比，该方法使运动障碍用户的任务完成效率提高了20%，用户满意度提高了25%。", "conclusion": "该方法显著增强了AR的可访问性和可扩展性。", "translation": "增强现实（AR）提供了沉浸式交互，但由于依赖精确的输入方法，对于运动障碍或手部灵活性有限的用户来说仍然难以使用。本研究提出了一种用于AR环境的基于手势的交互系统，利用深度学习从可穿戴传感器和摄像头识别手部和身体手势，并根据用户能力调整界面。该系统采用视觉Transformer（ViTs）、时间卷积网络（TCNs）和图注意力网络（GATs）进行手势处理，并通过联邦学习确保跨不同用户的隐私保护模型训练。强化学习优化了菜单布局和交互模式等界面元素。实验表明，与基线AR系统相比，该方法使运动障碍用户的任务完成效率提高了20%，用户满意度提高了25%。这种方法增强了AR的可访问性和可扩展性。", "summary": "本研究提出一种创新的手势驱动增强现实交互系统，旨在解决运动障碍用户在AR环境中的可访问性问题。该系统结合深度学习（ViTs、TCNs、GATs）、可穿戴传感器和摄像头识别用户手势，并通过联邦学习和强化学习优化模型训练和界面适应性。实验结果显示，该系统显著提升了运动障碍用户的任务效率和满意度，从而增强了AR的普适性和可扩展性。", "keywords": "深度学习, 联邦学习, 手势识别, 增强现实, 可访问性, 人机交互", "comments": "该论文的创新点在于结合多种先进的深度学习技术（ViTs, TCNs, GATs）进行手势识别，并引入联邦学习保障用户隐私，同时利用强化学习优化用户界面，为运动障碍用户提供了更友好的AR交互体验。其重要性在于推动了AR技术在无障碍领域的应用，拓宽了AR的用户群体。"}}
{"id": "2506.15096", "title": "DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory", "authors": ["Zihe Ji", "Huangxuan Lin", "Yue Gao"], "summary": "We present DyNaVLM, an end-to-end vision-language navigation framework using\nVision-Language Models (VLM). In contrast to prior methods constrained by fixed\nangular or distance intervals, our system empowers agents to freely select\nnavigation targets via visual-language reasoning. At its core lies a\nself-refining graph memory that 1) stores object locations as executable\ntopological relations, 2) enables cross-robot memory sharing through\ndistributed graph updates, and 3) enhances VLM's decision-making via retrieval\naugmentation. Operating without task-specific training or fine-tuning, DyNaVLM\ndemonstrates high performance on GOAT and ObjectNav benchmarks. Real-world\ntests further validate its robustness and generalization. The system's three\ninnovations: dynamic action space formulation, collaborative graph memory, and\ntraining-free deployment, establish a new paradigm for scalable embodied robot,\nbridging the gap between discrete VLN tasks and continuous real-world\nnavigation.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15096v1", "AI": {"title_translation": "DyNaVLM：具有动态视点和自优化图记忆的零样本视觉-语言导航系统", "tldr": "DyNaVLM是一个零样本视觉-语言导航系统，通过动态视点和自优化图记忆，使机器人能自由选择导航目标，并在真实世界中表现出色，无需任务特定训练。", "motivation": "现有的视觉-语言导航方法受限于固定的角度或距离间隔，限制了代理选择导航目标的能力。本文旨在开发一个能够通过视觉-语言推理自由选择导航目标并克服这些限制的系统。", "method": "DyNaVLM是一个使用视觉-语言模型（VLM）的端到端视觉-语言导航框架。其核心是一个自优化图记忆，该记忆能够将对象位置存储为可执行的拓扑关系、通过分布式图更新实现跨机器人记忆共享，并通过检索增强提升VLM的决策能力。该系统无需任务特定训练或微调。", "result": "DyNaVLM在GOAT和ObjectNav基准测试中表现出高性能。真实世界测试进一步验证了其鲁棒性和泛化能力。", "conclusion": "DyNaVLM通过引入动态动作空间公式、协作图记忆和免训练部署三项创新，为可扩展具身机器人建立了新范式，弥合了离散视觉-语言导航任务与连续真实世界导航之间的鸿沟。", "translation": "我们提出了DyNaVLM，一个使用视觉-语言模型（VLM）的端到端视觉-语言导航框架。与受限于固定角度或距离间隔的现有方法不同，我们的系统使代理能够通过视觉-语言推理自由选择导航目标。其核心是一个自优化图记忆，该记忆：1）将对象位置存储为可执行的拓扑关系；2）通过分布式图更新实现跨机器人记忆共享；3）通过检索增强提升VLM的决策能力。DyNaVLM无需任务特定训练或微调即可运行，在GOAT和ObjectNav基准测试中表现出高性能。真实世界测试进一步验证了其鲁棒性和泛化能力。该系统的三项创新：动态动作空间公式、协作图记忆和免训练部署，为可扩展具身机器人建立了新范式，弥合了离散视觉-语言导航任务与连续真实世界导航之间的鸿沟。", "summary": "DyNaVLM是一种零样本视觉-语言导航系统，通过引入动态视点和自优化图记忆，克服了传统方法固定导航间隔的限制。其核心的自优化图记忆能够存储拓扑关系、支持跨机器人记忆共享并增强VLM决策。该系统无需训练即可在GOAT和ObjectNav基准测试以及真实世界环境中展现出高鲁棒性和泛化能力，为具身机器人导航提供了新范式。", "keywords": "视觉-语言导航, 零样本, 动态视点, 图记忆, 具身机器人", "comments": "DyNaVLM的创新之处在于其动态动作空间、协作图记忆和免训练部署，这显著提升了视觉-语言导航的灵活性和实用性。特别是其零样本和跨机器人记忆共享能力，有望推动具身机器人在复杂真实世界环境中的广泛应用，为具身智能领域带来重要进展。"}}
{"id": "2506.15400", "title": "The maximum-average subtensor problem: equilibrium and out-of-equilibrium properties", "authors": ["Vittorio Erba", "Nathan Malo Kupferschmid", "Rodrigo Pérez Ortiz", "Lenka Zdeborová"], "summary": "In this paper we introduce and study the Maximum-Average Subtensor ($p$-MAS)\nproblem, in which one wants to find a subtensor of size $k$ of a given random\ntensor of size $N$, both of order $p$, with maximum sum of entries. We are\nmotivated by recent work on the matrix case of the problem in which several\nequilibrium and non-equilibrium properties have been characterized analytically\nin the asymptotic regime $1 \\ll k \\ll N$, and a puzzling phenomenon was\nobserved involving the coexistence of a clustered equilibrium phase and an\nefficient algorithm which produces submatrices in this phase. Here we extend\nprevious results on equilibrium and algorithmic properties for the matrix case\nto the tensor case. We show that the tensor case has a similar equilibrium\nphase diagram as the matrix case, and an overall similar phenomenology for the\nconsidered algorithms. Additionally, we consider out-of-equilibrium landscape\nproperties using Overlap Gap Properties and Franz-Parisi analysis, and discuss\nthe implications or lack-thereof for average-case algorithmic hardness.", "comment": null, "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.15400v1", "AI": {"title_translation": "最大平均子张量问题：平衡态和非平衡态性质", "tldr": "本文引入并研究了最大平均子张量（$p$-MAS）问题，旨在寻找给定随机张量的最大和子张量，并将其平衡态和非平衡态性质从矩阵情况扩展到张量情况，观察到类似的相图和算法现象，并探讨了平均情况算法硬度。", "motivation": "本研究的动机是近期关于最大平均子矩阵问题的研究，该研究在渐近状态下分析了矩阵的平衡态和非平衡态性质，并观察到集群平衡相与高效算法共存的现象。本文旨在将这些发现扩展到张量情况。", "method": "本文引入并研究了最大平均子张量（$p$-MAS）问题。研究方法包括将矩阵情况下的平衡态和算法性质的结果扩展到张量情况，并使用重叠间隙性质（Overlap Gap Properties）和Franz-Parisi分析来探究非平衡态景观性质。", "result": "研究结果表明，张量情况具有与矩阵情况相似的平衡态相图，并且对于所考虑的算法，整体现象学也相似。此外，还讨论了非平衡态景观性质对平均情况算法硬度的影响。", "conclusion": "结论是张量情况下的最大平均子张量问题在平衡态和非平衡态性质方面与矩阵情况表现出相似的现象，并且对平均情况算法硬度有特定的含义或缺乏含义。", "translation": "在本文中，我们引入并研究了最大平均子张量（$p$-MAS）问题，其中目标是在给定大小为$N$、阶数为$p$的随机张量中，找到一个大小为$k$、阶数为$p$的子张量，使其条目之和最大。我们的动机是近期关于该问题矩阵情况的工作，其中在渐近状态$1 \\ll k \\ll N$下，对几种平衡态和非平衡态性质进行了分析表征，并观察到了一种令人费解的现象，涉及集群平衡相和在该相中产生子矩阵的高效算法的共存。在此，我们将之前关于矩阵情况的平衡态和算法性质的结果扩展到张量情况。我们表明，张量情况具有与矩阵情况相似的平衡态相图，并且对于所考虑的算法，整体现象学也相似。此外，我们使用重叠间隙性质和Franz-Parisi分析考虑了非平衡态景观性质，并讨论了其对平均情况算法硬度的影响或缺乏影响。", "summary": "本文引入并研究了最大平均子张量（$p$-MAS）问题，旨在寻找随机张量中具有最大条目和的指定大小子张量。研究将先前在矩阵情况下的平衡态和算法性质的发现扩展到张量领域。结果显示，张量情况在平衡态相图和算法表现上与矩阵情况相似。此外，通过重叠间隙性质和Franz-Parisi分析，文章还探讨了非平衡态景观性质及其对平均情况算法硬度的影响。", "keywords": "最大平均子张量问题, 平衡态, 非平衡态, 张量, 算法硬度", "comments": "这篇论文将矩阵领域中已经研究过的最大平均子矩阵问题推广到了高阶张量领域，这是一个重要的进展。它不仅揭示了张量和矩阵情况在平衡态和算法行为上的相似性，还深入探讨了非平衡态性质，这对于理解复杂系统中的优化问题和算法的理论极限具有重要意义。特别地，对“集群平衡相”和“高效算法”的共存现象的进一步探讨，可能为设计更高效的张量处理算法提供新的视角。"}}
{"id": "2506.15537", "title": "Automatic Metadata Capture and Processing for High-Performance Workflows", "authors": ["Polina Shpilker", "Line Pouchard"], "summary": "Modern workflows run on increasingly heterogeneous computing architectures\nand with this heterogeneity comes additional complexity. We aim to apply the\nFAIR principles for research reproducibility by developing software to collect\nmetadata annotations for workflows run on HPC systems. We experiment with two\npossible formats to uniformly store these metadata, and reorganize the\ncollected metadata to be as easy to use as possible for researchers studying\ntheir workflow performance.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15537v1", "AI": {"title_translation": "高性能工作流的自动元数据捕获与处理", "tldr": "开发软件自动捕获和处理高性能工作流元数据，以提高研究可复现性并简化性能分析。", "motivation": "现代工作流在异构计算架构上运行，导致复杂性增加。研究旨在通过应用FAIR原则，提高研究可复现性。", "method": "开发软件来收集高性能计算系统上运行的工作流的元数据注释。实验了两种统一存储元数据的格式，并重新组织收集到的元数据，使其易于研究人员用于分析工作流性能。", "result": "实验了两种元数据存储格式，并成功重组了收集到的元数据，使其对研究工作流性能的科研人员而言易于使用。", "conclusion": "通过自动捕获、处理和优化元数据存储，可以有效支持高性能工作流的可复现性，并简化性能分析。", "translation": "现代工作流在日益异构的计算架构上运行，这种异构性带来了额外的复杂性。我们旨在通过开发软件来收集在高性能计算系统上运行的工作流的元数据注释，从而应用FAIR原则以实现研究可复现性。我们实验了两种可能的格式来统一存储这些元数据，并重新组织收集到的元数据，使其尽可能易于研究人员用于研究其工作流性能。", "summary": "本文针对现代异构计算架构下工作流的复杂性问题，提出通过开发软件自动捕获高性能计算系统工作流的元数据。研究旨在应用FAIR原则提高研究可复现性，并探讨了两种统一的元数据存储格式，同时优化元数据组织，以方便研究人员进行工作流性能分析。", "keywords": "元数据捕获, 高性能工作流, FAIR原则, 可复现性, 性能分析", "comments": "这篇论文的创新点在于将FAIR原则应用于高性能工作流的元数据管理，并关注元数据的可用性，这对于提高科学计算的可复现性和效率具有重要意义。"}}
{"id": "2506.14837", "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction", "authors": ["Chengzhi Xu", "Yuyang Wang", "Lai Wei", "Lichao Sun", "Weiran Huang"], "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14837v1", "AI": {"title_translation": "基于结构化指令的图表到代码生成改进迭代细化", "tldr": "{ChartIR}通过结构化指令和迭代细化改进了多模态大语言模型在图表到代码生成任务上的性能。", "motivation": "尽管多模态大语言模型（MLLMs）在视觉任务上表现出色，但它们在图表到代码生成任务上的性能仍不理想。这项任务需要精确的视觉理解和将视觉元素准确翻译成结构化代码的能力，而直接提示MLLMs往往效果不佳。", "method": "提出了一种名为{ChartIR}的基于结构化指令的迭代细化方法。该方法首先区分视觉理解和代码翻译两个任务。为了实现视觉理解，设计了两种结构化指令：描述指令（捕捉参考图表的视觉元素）和差异指令（描述参考图表与生成图表之间的差异），将视觉特征转化为语言表示。其次，将整个图表生成流程分解为初始代码生成和迭代细化两个阶段，以逐步增强最终输出。", "result": "实验结果表明，与其它方法相比，该方法在开源模型Qwen2-VL和闭源模型GPT-4o上都取得了优越的性能。", "conclusion": "该研究提出的{ChartIR}方法通过结构化指令和迭代细化，有效提升了多模态大语言模型在图表到代码生成任务上的准确性和性能。", "translation": "近期，多模态大语言模型（MLLMs）因其强大的视觉理解能力而受到越来越多的研究关注。尽管它们在各种视觉任务上取得了令人印象深刻的成果，但其在图表到代码生成方面的表现仍不尽如人意。这项任务要求MLLMs生成可重现给定图表的代码，这不仅需要精确的视觉理解，还需要将视觉元素准确翻译成结构化代码。直接提示MLLMs执行这项复杂任务往往会产生不令人满意的结果。为了解决这一挑战，我们提出了{ChartIR}，一种基于结构化指令的迭代细化方法。首先，我们区分了两个任务：视觉理解和代码翻译。为了完成视觉理解部分，我们设计了两种类型的结构化指令：描述和差异。描述指令捕捉参考图表的视觉元素，而差异指令则描述参考图表与生成图表之间的差异。这些指令有效地将视觉特征转化为语言表示，从而促进后续的代码翻译过程。其次，我们将整个图表生成流程分解为初始代码生成和迭代细化两个阶段，从而实现最终输出的逐步增强。实验结果表明，与其它方法相比，我们的方法在开源模型Qwen2-VL和闭源模型GPT-4o上都取得了优越的性能。", "summary": "本文提出了一种名为{ChartIR}的迭代细化方法，旨在解决多模态大语言模型（MLLMs）在图表到代码生成任务中表现不佳的问题。该方法通过引入结构化指令（描述和差异指令）来增强视觉理解，并将其转化为语言表示，同时将生成过程分解为初始代码生成和迭代细化两个阶段。实验证明，{ChartIR}在Qwen2-VL和GPT-4o模型上均优于现有方法，显著提高了图表到代码生成的准确性。", "keywords": "图表到代码生成, 迭代细化, 结构化指令, 多模态大语言模型, 视觉理解", "comments": "这篇论文通过引入结构化指令和迭代细化机制，有效地解决了MLLMs在图表到代码生成任务中面临的挑战，其创新点在于将复杂的任务分解为可管理的子任务，并通过精细化的指令设计提升了模型的理解和生成能力。"}}
{"id": "2506.15647", "title": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yang Deng", "Xingyu Sui", "Yulin Hu", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "summary": "Recent advancements in large reasoning models (LRMs) have significantly\nenhanced language models' capabilities in complex problem-solving by emulating\nhuman-like deliberative thinking. However, these models often exhibit\noverthinking (i.e., the generation of unnecessarily verbose and redundant\ncontent), which hinders efficiency and inflates inference cost. In this work,\nwe explore the representational and behavioral origins of this inefficiency,\nrevealing that LRMs inherently possess the capacity for more concise reasoning.\nEmpirical analyses show that correct reasoning paths vary significantly in\nlength, and the shortest correct responses often suffice, indicating untapped\nefficiency potential. Exploiting these findings, we propose two lightweight\nmethods to enhance LRM efficiency. First, we introduce Efficiency Steering, a\ntraining-free activation steering technique that modulates reasoning behavior\nvia a single direction in the model's representation space. Second, we develop\nSelf-Rewarded Efficiency RL, a reinforcement learning framework that\ndynamically balances task accuracy and brevity by rewarding concise correct\nsolutions. Extensive experiments on seven LRM backbones across multiple\nmathematical reasoning benchmarks demonstrate that our methods significantly\nreduce reasoning length while preserving or improving task performance. Our\nresults highlight that reasoning efficiency can be improved by leveraging and\nguiding the intrinsic capabilities of existing models in a self-guided manner.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15647v1", "AI": {"title_translation": "探索并利用大型推理模型内在效率以实现自我引导的效率提升", "tldr": "本文探讨了大型推理模型中“过度思考”导致的低效率问题，并提出了两种轻量级方法（效率引导和自奖励效率强化学习）来减少推理长度同时保持或提高任务性能。", "motivation": "大型推理模型（LRMs）在复杂问题解决方面取得了显著进展，但常出现“过度思考”现象，导致生成冗余内容，降低效率并增加推理成本。", "method": "首先，探索低效率的起源，揭示LRMs本身具备更简洁推理的能力，且最短的正确响应通常足够。其次，提出两种轻量级方法：1. 效率引导（Efficiency Steering），一种无需训练的激活引导技术，通过模型表示空间中的单一方向调节推理行为。2. 自奖励效率强化学习（Self-Rewarded Efficiency RL），一个强化学习框架，通过奖励简洁的正确解决方案来动态平衡任务准确性和简洁性。", "result": "在七个LRM骨干模型和多个数学推理基准上的广泛实验表明，所提出的方法显著减少了推理长度，同时保持或提升了任务性能。", "conclusion": "推理效率可以通过利用和引导现有模型内在能力以自我引导的方式得到提升。", "translation": "大型推理模型（LRMs）的最新进展通过模仿人类的审慎思维，显著增强了语言模型解决复杂问题的能力。然而，这些模型经常表现出“过度思考”（即生成不必要的冗长和冗余内容），这阻碍了效率并增加了推理成本。在这项工作中，我们探索了这种低效率的表示和行为起源，揭示了LRMs本身具有更简洁推理的能力。实证分析表明，正确的推理路径长度差异很大，最短的正确响应通常就足够了，这表明存在未开发的效率潜力。利用这些发现，我们提出了两种轻量级方法来提高LRM的效率。首先，我们引入了“效率引导”（Efficiency Steering），这是一种无需训练的激活引导技术，通过模型表示空间中的单一方向来调节推理行为。其次，我们开发了“自奖励效率强化学习”（Self-Rewarded Efficiency RL），这是一个强化学习框架，通过奖励简洁的正确解决方案来动态平衡任务准确性和简洁性。在多个数学推理基准上对七个LRM骨干模型进行的广泛实验表明，我们的方法显著减少了推理长度，同时保持或提高了任务性能。我们的结果突出表明，推理效率可以通过利用和引导现有模型的内在能力以自我引导的方式得到提升。", "summary": "本文研究大型推理模型（LRMs）中常见的“过度思考”导致的低效率问题。通过分析发现LRMs本身具有简洁推理的潜力。为此，作者提出了两种轻量级方法：基于激活引导的“效率引导”和基于强化学习的““自奖励效率强化学习”，旨在在不牺牲性能的前提下减少推理冗余。实验证明，这些方法能有效缩短推理长度并保持或提升任务表现，从而实现自我引导的效率提升。", "keywords": "大型推理模型, 效率提升, 过度思考, 激活引导, 强化学习", "comments": "这项工作针对大型推理模型中的“过度思考”这一实际问题，提出了创新的解决方案。其亮点在于揭示了模型内在的效率潜力，并提出了两种无需大量额外训练的轻量级方法。特别是“效率引导”通过激活引导调节推理行为，以及“自奖励效率强化学习”通过平衡准确性和简洁性，都体现了对模型内在机制的深刻理解和巧妙利用。这对于降低大型模型推理成本、提升实际应用效率具有重要意义。"}}
{"id": "2506.14794", "title": "Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors", "authors": ["Henrik Klagges", "Robert Dahlke", "Fabian Klemm", "Benjamin Merkel", "Daniel Klingmann", "David A. Reiss", "Dan Zecha"], "summary": "Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM\nduring pretraining is extremely expensive and seems inefficient. To better\nleverage the huge investments made into pretrained models, we develop the new\n\"Assembly-of-Experts\" (AoE) construction method to create capable child\nvariants of existing Mixture-of-Experts parent models in linear time. Model\nweight tensors get interpolated individually, allowing to enhance or suppress\nsemantic features of the parents.\n  Varying the proportion of weights taken from the parent models, we observe\nsome properties of the AoE child model changing gradually, while other\nbehavioral traits emerge with a sharp transition. Surprisingly, nearly every\ngenerated model is functional and capable, which makes searching the model\nspace straightforward.\n  We construct the DeepSeek R1T \"Chimera\", a 671B open-weights hybrid model\ncombining DeepSeek's V3-0324 and R1 model variants. The child inherits only the\nrouted expert tensors of R1, but still achieves about R1-level intelligence. At\nthe same time, it uses about 40\\% fewer output tokens, close to V3 speed.\nConstructed without any fine-tuning or distillation, the Chimera exhibits\nsurprisingly compact, orderly reasoning compared to its parent models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14794v1", "AI": {"title_translation": "专家组装：线性时间构建具有涌现和适应性行为的奇美拉大型语言模型变体", "tldr": "提出了一种线性时间构建大型语言模型（LLM）变体的“专家组装”（AoE）方法，通过插值现有模型权重来创建具有强大性能和效率的子模型，如奇美拉模型。", "motivation": "预训练大型语言模型（LLM）的计算成本极高且效率低下，需要$10^{13}$-$10^{15}$ FLOPs来计算一个8位权重。", "method": "开发了一种名为“专家组装”（AoE）的新型构建方法，可以在线性时间内从现有的专家混合（MoE）父模型创建出具有能力的子变体。该方法通过单独插值模型权重张量，从而增强或抑制父模型的语义特征。", "result": "通过调整从父模型获取的权重比例，AoE子模型的某些属性逐渐变化，而其他行为特征则发生急剧转变。几乎所有生成的模型都功能强大且有能力，使得模型空间搜索变得简单。成功构建了DeepSeek R1T“奇美拉”模型，这是一个671B的开源混合模型，结合了DeepSeek的V3-0324和R1模型变体。奇美拉仅继承了R1的路由专家张量，但仍达到了接近R1的智能水平，同时输出令牌减少约40%，接近V3的速度。奇美拉在未经任何微调或蒸馏的情况下，表现出比其父模型更紧凑、更有序的推理能力。", "conclusion": "“专家组装”（AoE）方法能够高效且有效地从现有模型构建出高性能的大型语言模型变体，例如奇美拉，它在保持高智能水平的同时显著提高了效率，并且无需额外的微调或蒸馏。", "translation": "预训练大型语言模型（LLM）中计算一个8位权重需要$10^{13}$-$10^{15}$ FLOPs，这极其昂贵且似乎效率低下。为了更好地利用在预训练模型上的巨大投资，我们开发了一种新的“专家组装”（AoE）构建方法，以线性时间创建现有专家混合（MoE）父模型的能力子变体。模型权重张量被单独插值，从而可以增强或抑制父模型的语义特征。通过改变从父模型获取的权重比例，我们观察到AoE子模型的某些属性逐渐变化，而其他行为特征则发生急剧转变。令人惊讶的是，几乎每个生成的模型都功能强大且有能力，这使得模型空间搜索变得简单。我们构建了DeepSeek R1T“奇美拉”，这是一个671B的开源混合模型，结合了DeepSeek的V3-0324和R1模型变体。该子模型仅继承了R1的路由专家张量，但仍达到了接近R1的智能水平。同时，它使用的输出令牌减少了约40%，接近V3的速度。奇美拉在未经任何微调或蒸馏的情况下，表现出比其父模型更紧凑、更有序的推理能力。", "summary": "本研究提出了一种名为“专家组装”（AoE）的新方法，旨在通过线性时间插值现有大型语言模型（LLM）的权重，来高效构建具有强大能力和适应性行为的子变体。该方法解决了LLM预训练成本高昂的问题。通过AoE，可以从专家混合（MoE）父模型创建子模型，并观察到其属性的渐变和行为特征的急剧涌现。研究成功构建了671B的“奇美拉”模型，该模型结合了DeepSeek V3-0324和R1，在无需微调或蒸馏的情况下，实现了接近R1的智能水平，同时显著提高效率（减少40%输出令牌，接近V3速度），并展现出更紧凑、有序的推理能力。", "keywords": "专家组装, 大型语言模型, 奇美拉, 模型构建, 线性时间", "comments": "这篇论文的创新点在于提出了“专家组装”（AoE）这一新颖且高效的模型构建方法，它通过插值而非传统的微调或蒸馏来创建新的LLM变体。这种方法大大降低了模型开发的成本和时间，并展示了从现有模型中“组装”出高性能新模型的潜力。特别令人印象深刻的是，即使是部分继承，也能达到与父模型相近的智能水平，且在效率上有所提升，这对于资源有限的研究者和开发者具有重要意义。"}}
{"id": "2506.15623", "title": "Minding the Politeness Gap in Cross-cultural Communication", "authors": ["Yuka Machino", "Matthias Hofer", "Max Siegel", "Joshua B. Tenenbaum", "Robert D. Hawkins"], "summary": "Misunderstandings in cross-cultural communication often arise from subtle\ndifferences in interpretation, but it is unclear whether these differences\narise from the literal meanings assigned to words or from more general\npragmatic factors such as norms around politeness and brevity. In this paper,\nwe report three experiments examining how speakers of British and American\nEnglish interpret intensifiers like \"quite\" and \"very.\" To better understand\nthese cross-cultural differences, we developed a computational cognitive model\nwhere listeners recursively reason about speakers who balance informativity,\npoliteness, and utterance cost. Our model comparisons suggested that\ncross-cultural differences in intensifier interpretation stem from a\ncombination of (1) different literal meanings, (2) different weights on\nutterance cost. These findings challenge accounts based purely on semantic\nvariation or politeness norms, demonstrating that cross-cultural differences in\ninterpretation emerge from an intricate interplay between the two.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15623v1", "AI": {"title_translation": "弥合跨文化交流中的礼貌差距", "tldr": "研究发现英美英语中对程度副词的跨文化理解差异源于字面意义和表达成本权重的结合，而非单纯的语义或礼貌规范。", "motivation": "跨文化交流中的误解通常源于解释上的细微差异，但尚不清楚这些差异是源于词语的字面意义还是更普遍的语用因素，如礼貌和简洁规范。", "method": "本文报告了三项实验，研究英式英语和美式英语使用者如何解释“quite”和“very”等程度副词。为理解这些跨文化差异，开发了一个计算认知模型，在该模型中，听者递归地推断说话者如何在信息量、礼貌和言语成本之间取得平衡。", "result": "模型比较表明，程度副词解释的跨文化差异源于(1)不同的字面意义和(2)不同的言语成本权重相结合。", "conclusion": "这些发现挑战了纯粹基于语义变异或礼貌规范的解释，表明解释中的跨文化差异是语义和语用之间复杂相互作用的结果。", "translation": "跨文化交流中的误解常源于解释上的细微差异，但尚不清楚这些差异是源于词语的字面意义，还是源于更普遍的语用因素，如礼貌和简洁规范。本文报告了三项实验，研究英式英语和美式英语使用者如何解释“quite”和“very”等程度副词。为了更好地理解这些跨文化差异，我们开发了一个计算认知模型，其中听者递归地推断说话者如何在信息量、礼貌和言语成本之间取得平衡。我们的模型比较表明，程度副词解释的跨文化差异源于(1)不同的字面意义和(2)不同的言语成本权重相结合。这些发现挑战了纯粹基于语义变异或礼貌规范的解释，表明解释中的跨文化差异是两者之间复杂相互作用的结果。", "summary": "本文通过三项实验和计算认知模型，探究了英美英语使用者在解释“quite”和“very”等程度副词时的跨文化差异。研究发现，这些差异不仅与词语的字面意义有关，也与表达成本的权重有关。这表明跨文化理解的差异是语义和语用因素复杂交互作用的结果，而非单一因素决定。", "keywords": "跨文化交流, 礼貌, 程度副词, 计算认知模型, 语用学", "comments": "这篇论文通过结合实验和计算模型，为理解跨文化交流中的语用差异提供了一个新颖的视角。它挑战了传统上将语义和语用（如礼貌）视为独立影响因素的观点，强调了它们之间的复杂相互作用，这对于语言学和跨文化研究具有重要意义。"}}
{"id": "2506.15447", "title": "Model Predictive Path-Following Control for a Quadrotor", "authors": ["David Leprich", "Mario Rosenfelder", "Mario Hermle", "Jingshan Chen", "Peter Eberhard"], "summary": "Automating drone-assisted processes is a complex task. Many solutions rely on\ntrajectory generation and tracking, whereas in contrast, path-following control\nis a particularly promising approach, offering an intuitive and natural\napproach to automate tasks for drones and other vehicles. While different\nsolutions to the path-following problem have been proposed, most of them lack\nthe capability to explicitly handle state and input constraints, are formulated\nin a conservative two-stage approach, or are only applicable to linear systems.\nTo address these challenges, the paper is built upon a Model Predictive\nControl-based path-following framework and extends its application to the\nCrazyflie quadrotor, which is investigated in hardware experiments. A cascaded\ncontrol structure including an underlying attitude controller is included in\nthe Model Predictive Path-Following Control formulation to meet the challenging\nreal-time demands of quadrotor control. The effectiveness of the proposed\nmethod is demonstrated through real-world experiments, representing, to the\nbest of the authors' knowledge, a novel application of this MPC-based\npath-following approach to the quadrotor. Additionally, as an extension to the\noriginal method, to allow for deviations of the path in cases where the precise\nfollowing of the path might be overly restrictive, a corridor path-following\napproach is presented.", "comment": "15 pages, 11 figures, submitted to PAMM 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15447v1", "AI": {"title_translation": "四旋翼飞行器的模型预测路径跟踪控制", "tldr": "本文提出了一种基于模型预测控制的路径跟踪框架，并将其应用于四旋翼飞行器，通过级联控制结构和走廊路径跟踪方法，在硬件实验中验证了其在处理约束和实时性方面的有效性。", "motivation": "现有的无人机自动化路径跟踪解决方案通常缺乏明确处理状态和输入约束的能力，采用保守的两阶段方法，或仅适用于线性系统。", "method": "该论文基于模型预测控制（MPC）的路径跟踪框架，并将其应用扩展到Crazyflie四旋翼飞行器，并通过硬件实验进行研究。为满足四旋翼控制的实时性要求，该方法中包含一个级联控制结构及底层的姿态控制器。此外，还提出了一个走廊路径跟踪方法，以允许路径偏差。", "result": "所提出的方法通过实际硬件实验证明了其有效性，据作者所知，这是MPC路径跟踪方法在四旋翼飞行器上的一个新颖应用。", "conclusion": "该研究成功地开发并验证了一种基于模型预测控制的路径跟踪方法，该方法能够有效处理四旋翼飞行器的状态和输入约束以及实时性挑战，并且引入了允许路径偏差的走廊路径跟踪功能。", "translation": "自动化无人机辅助过程是一项复杂的任务。许多解决方案依赖于轨迹生成和跟踪，而相比之下，路径跟踪控制是一种特别有前景的方法，它为无人机和其他车辆的自动化任务提供了一种直观而自然的方法。尽管已经提出了许多路径跟踪问题的解决方案，但它们大多数缺乏明确处理状态和输入约束的能力，或者采用保守的两阶段方法，或者只适用于线性系统。为了解决这些挑战，本文建立在基于模型预测控制的路径跟踪框架之上，并将其应用扩展到Crazyflie四旋翼飞行器，并通过硬件实验进行了研究。模型预测路径跟踪控制公式中包含一个级联控制结构，其中包括一个底层的姿态控制器，以满足四旋翼控制具有挑战性的实时需求。所提出方法的有效性通过实际实验得到证明，据作者所知，这是MPC路径跟踪方法在四旋翼飞行器上的一个新颖应用。此外，作为对原始方法的扩展，为了在精确跟踪路径可能过于严格的情况下允许路径偏差，本文提出了一种走廊路径跟踪方法。", "summary": "本文提出了一种针对四旋翼飞行器的模型预测路径跟踪控制方法，旨在解决现有方案在处理约束和实时性方面的不足。该方法基于MPC框架，并结合了级联控制结构和走廊路径跟踪扩展，通过在Crazyflie四旋翼飞行器上的硬件实验验证了其有效性和新颖性。", "keywords": "模型预测控制, 路径跟踪, 四旋翼飞行器, 约束处理, 实时控制", "comments": "该论文的创新点在于将MPC路径跟踪框架应用于四旋翼飞行器，并解决了传统方法在处理约束和实时性方面的局限性。引入的级联控制结构和走廊路径跟踪方法增强了其实用性和灵活性，对于无人机自动化任务具有重要意义。"}}
{"id": "2506.15365", "title": "FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation", "authors": ["Haolong Jin", "Shenglin Liu", "Cong Cong", "Qingmin Feng", "Yongzhi Liu", "Lina Huang", "Yingzi Hu"], "summary": "Federated learning (FL) has emerged as a promising approach for collaborative\nmedical image analysis, enabling multiple institutions to build robust\npredictive models while preserving sensitive patient data. In the context of\nWhole Slide Image (WSI) classification, FL faces significant challenges,\nincluding heterogeneous computational resources across participating medical\ninstitutes and privacy concerns. To address these challenges, we propose\nFedWSIDD, a novel FL paradigm that leverages dataset distillation (DD) to learn\nand transmit synthetic slides. On the server side, FedWSIDD aggregates\nsynthetic slides from participating centres and distributes them across all\ncentres. On the client side, we introduce a novel DD algorithm tailored to\nhistopathology datasets which incorporates stain normalisation into the\ndistillation process to generate a compact set of highly informative synthetic\nslides. These synthetic slides, rather than model parameters, are transmitted\nto the server. After communication, the received synthetic slides are combined\nwith original slides for local tasks. Extensive experiments on multiple WSI\nclassification tasks, including CAMELYON16 and CAMELYON17, demonstrate that\nFedWSIDD offers flexibility for heterogeneous local models, enhances local WSI\nclassification performance, and preserves patient privacy. This makes it a\nhighly effective solution for complex WSI classification tasks. The code is\navailable at FedWSIDD.", "comment": "MICCAI 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15365v1", "AI": {"title_translation": "FedWSIDD：基于数据集蒸馏的联邦全玻片图像分类", "tldr": "FedWSIDD是一种新的联邦学习方法，通过数据集蒸馏生成并传输合成玻片，以应对全玻片图像分类中资源异构和隐私问题，同时提高分类性能。", "motivation": "联邦学习在医疗图像分析中面临计算资源异构和隐私保护的挑战，尤其是在全玻片图像（WSI）分类的背景下。", "method": "本文提出了FedWSIDD，一种利用数据集蒸馏（DD）来学习和传输合成玻片的新型联邦学习范式。在服务器端，FedWSIDD聚合来自参与中心的合成玻片并分发到所有中心。在客户端，引入了一种针对组织病理学数据集的DD算法，该算法将染色归一化纳入蒸馏过程，以生成一组紧凑且信息丰富的合成玻片。这些合成玻片（而非模型参数）被传输到服务器。通信后，接收到的合成玻片与原始玻片结合用于局部任务。", "result": "在CAMELYON16和CAMELYON17等多个WSI分类任务上的大量实验表明，FedWSIDD为异构局部模型提供了灵活性，增强了局部WSI分类性能，并保护了患者隐私。", "conclusion": "FedWSIDD是一种高效的解决方案，适用于复杂的全玻片图像分类任务，解决了联邦学习在医疗图像领域面临的资源异构性和隐私保护问题。", "translation": "联邦学习（FL）已成为一种有前景的协作医疗图像分析方法，它使多个机构能够在保护敏感患者数据的同时构建鲁棒的预测模型。在全玻片图像（WSI）分类的背景下，联邦学习面临重大挑战，包括参与医疗机构之间计算资源的异构性以及隐私问题。为了解决这些挑战，我们提出了FedWSIDD，这是一种新颖的联邦学习范式，它利用数据集蒸馏（DD）来学习和传输合成玻片。在服务器端，FedWSIDD聚合来自参与中心的合成玻片并将其分发到所有中心。在客户端，我们引入了一种针对组织病理学数据集的新型DD算法，该算法将染色归一化纳入蒸馏过程，以生成一组紧凑且信息丰富的合成玻片。这些合成玻片（而非模型参数）被传输到服务器。通信后，接收到的合成玻片与原始玻片结合用于局部任务。在包括CAMELYON16和CAMELYON17在内的多个WSI分类任务上进行的大量实验表明，FedWSIDD为异构局部模型提供了灵活性，增强了局部WSI分类性能，并保护了患者隐私。这使其成为复杂WSI分类任务的高效解决方案。代码可在FedWSIDD获取。", "summary": "FedWSIDD提出了一种新的联邦学习（FL）方法，用于全玻片图像（WSI）分类，旨在解决传统FL在医疗图像分析中面临的资源异构性和隐私问题。该方法利用数据集蒸馏（DD）生成并传输合成玻片而非模型参数。客户端通过定制的DD算法，结合染色归一化，生成信息丰富的合成玻片；服务器端则聚合并分发这些合成玻片。实验证明，FedWSIDD在保持患者隐私的同时，提高了WSI分类性能，并适应了异构的本地模型。", "keywords": "联邦学习, 数据集蒸馏, 全玻片图像分类, 隐私保护, 异构性", "comments": "该论文的创新点在于将数据集蒸馏（DD）引入到联邦学习（FL）框架中，用于全玻片图像（WSI）分类。通过传输合成数据而非模型参数，有效解决了医疗领域FL中常见的隐私泄露和计算资源异构问题。其独特的客户端DD算法融入染色归一化，提高了合成数据的质量和实用性。这一方法为复杂医疗图像分析的联邦学习提供了新的思路和高效的解决方案。"}}
{"id": "2506.15470", "title": "Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "summary": "With the deployment of large antenna arrays at high frequency bands, future\nwireless communication systems are likely to operate in the radiative\nnear-field. Unlike far-field beam steering, near-field beams can be focused\nwithin a spatial region of finite depth, enabling spatial multiplexing in both\nthe angular and range dimensions. This paper derives the beamdepth for a\ngeneralized uniform rectangular array (URA) and investigates how array geometry\ninfluences the near-field beamdepth and the limits where near-field\nbeamfocusing is achievable. To characterize the near-field boundary in terms of\nbeamfocusing and spatial multiplexing gains, we define the effective\nbeamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis\nreveals that while a square URA achieves the narrowest beamdepth, the EBRD is\nmaximized for a wide or tall URA. However, despite its narrow beamdepth, a\nsquare URA may experience a reduction in multiuser sum rate due to its severely\nconstrained EBRD. Simulation results confirm that a wide or tall URA achieves a\nsum rate of 3.5 X more than that of a square URA, benefiting from the extended\nEBRD and improved spatial multiplexing capabilities.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15470v1", "AI": {"title_translation": "分析 URA 几何结构以增强空间复用和扩展近场覆盖", "tldr": "在近场通信中，方形均匀矩形阵列（URA）虽然波束深度最窄，但总和速率较低；宽或高的 URA 阵列能最大化有效波束聚焦瑞利距离（EBRD），从而显著提高总和速率和空间复用能力。", "motivation": "随着高频段大型天线阵列的部署，未来的无线通信系统很可能在辐射近场中运行。与远场波束控制不同，近场波束可以在有限深度的空间区域内聚焦，从而在角度和距离维度上实现空间复用。因此，研究阵列几何结构对近场波束深度和聚焦极限的影响，以增强空间复用和扩展近场覆盖，变得至关重要。", "method": "本文推导了广义均匀矩形阵列（URA）的波束深度，并研究了阵列几何结构如何影响近场波束深度和近场波束聚焦的可实现极限。为了表征波束聚焦和空间复用增益方面的近场边界，文中定义了广义 URA 的有效波束聚焦瑞利距离（EBRD）。", "result": "分析表明，方形 URA 实现了最窄的波束深度，但其有效波束聚焦瑞利距离（EBRD）被严重限制。相反，宽或高的 URA 阵列能够最大化 EBRD。尽管方形 URA 的波束深度较窄，但由于其受严格限制的 EBRD，可能导致多用户总和速率降低。仿真结果证实，宽或高的 URA 阵列的总和速率比方形 URA 高 3.5 倍，这得益于其扩展的 EBRD 和改进的空间复用能力。", "conclusion": "尽管方形 URA 实现了最窄的近场波束深度，但宽或高的均匀矩形阵列（URA）在最大化有效波束聚焦瑞利距离（EBRD）和在近场通信系统中实现更高的多用户总和速率方面更具优势，这归因于其增强的空间复用能力。", "translation": "随着高频段大型天线阵列的部署，未来的无线通信系统很可能在辐射近场中运行。与远场波束控制不同，近场波束可以在有限深度的空间区域内聚焦，从而在角度和距离维度上实现空间复用。本文推导了广义均匀矩形阵列（URA）的波束深度，并研究了阵列几何结构如何影响近场波束深度和近场波束聚焦的可实现极限。为了表征波束聚焦和空间复用增益方面的近场边界，我们定义了广义 URA 的有效波束聚焦瑞利距离（EBRD）。我们的分析表明，虽然方形 URA 实现了最窄的波束深度，但宽或高的 URA 阵列能最大化 EBRD。然而，尽管其波束深度较窄，方形 URA 可能因其受严格限制的 EBRD 而导致多用户总和速率降低。仿真结果证实，宽或高的 URA 阵列的总和速率比方形 URA 高 3.5 倍，这得益于其扩展的 EBRD 和改进的空间复用能力。", "summary": "本研究探讨了在未来无线通信系统近场运行背景下，均匀矩形阵列（URA）几何结构对空间复用和近场覆盖的影响。文章推导了广义 URA 的波束深度，并引入了有效波束聚焦瑞利距离（EBRD）来量化近场边界。研究发现，虽然方形 URA 具有最窄的波束深度，但其 EBRD 受限，导致多用户总和速率较低。相比之下，宽或高的 URA 能最大化 EBRD，并在仿真中显示出比方形 URA 高 3.5 倍的总和速率，从而显著提升了空间复用能力。", "keywords": "URA 几何, 近场通信, 空间复用, 波束聚焦, 瑞利距离", "comments": "这篇论文为近场通信中 URA 阵列的几何优化提供了宝贵的见解。EBRD 的引入是一个重要的创新点，它提供了一个量化近场边界和性能的新指标。研究结果揭示了方形 URA 在某些性能指标上可能不如非方形 URA 的反直觉发现，强调了在设计近场系统时需要全面考虑空间复用增益和覆盖范围，而不仅仅是波束深度。其对宽或高阵列优越性的论证具有重要的实际指导意义。"}}
{"id": "2506.15556", "title": "PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction", "authors": ["Shufan Li", "Aditya Grover"], "summary": "Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.", "comment": "16 pages,4 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15556v1", "AI": {"title_translation": "PredGen：通过输入时推测加速大型语言模型推理以实现实时语音交互", "tldr": "PredGen通过在用户说话时进行推测性解码来加速LLM的推理，从而将实时语音交互中的延迟降低约2倍。", "motivation": "大型语言模型（LLMs）在实时语音聊天应用中与文本转语音（TTS）系统结合使用时，用户输入结束到音频输出开始之间存在明显的延迟，尤其是在消费级硬件上作为单用户语音助手部署时。这种延迟主要由LLM生成TTS系统所需的第一个句子所需的时间决定，导致用户体验不佳。", "method": "本文提出了预测生成（PredGen）框架，通过在输入时进行推测性解码来缓解或消除延迟。PredGen在用户仍在说话时生成候选响应，从而使系统能够以最小的延迟开始TTS处理。", "result": "在Lmsys和MT-Bench数据集上的模拟实验表明，所提出的方法可以在广泛的使用场景中将延迟有效减少约2倍，同时在输入时仅产生最小的额外计算成本。", "conclusion": "PredGen通过在用户输入时进行推测性解码，显著降低了LLM在实时语音交互中的延迟，从而改善了用户体验，尤其是在资源受限的设备上。", "translation": "大型语言模型（LLMs）广泛应用于实时语音聊天应用中，通常与文本转语音（TTS）系统结合使用以生成音频响应。然而，其庞大的模型尺寸常常导致用户输入结束到音频输出开始之间出现明显的延迟，从而导致次优的用户体验。当LLM作为单用户语音助手部署在计算能力有限的消费级硬件上时，这种延迟尤为明显。我们发现，这种延迟主要由LLM生成第一个句子所需的时间决定，而TTS系统需要这个句子作为输入，并逐句合成音频响应。为了解决这一瓶颈，我们提出了预测生成（PredGen），一个通过在输入时进行推测性解码来减轻甚至消除这种延迟的新颖框架。PredGen在用户仍在说话时生成候选响应，使系统能够以最小的延迟开始TTS处理。在Lmsys和MT-Bench数据集上的模拟实验表明，所提出的方法可以在广泛的使用场景中将延迟有效减少约2倍，同时在输入时仅产生最小的额外计算成本——这些计算原本是未被利用的。", "summary": "本文提出PredGen框架，旨在解决大型语言模型（LLMs）在实时语音交互中结合文本转语音（TTS）系统时存在的显著延迟问题。该延迟主要源于LLM生成首个句子的时间。PredGen通过在用户输入时进行推测性解码，即在用户说话时提前生成候选响应，从而使TTS系统能够更快地开始处理。实验结果表明，PredGen能将延迟降低约2倍，且仅带来极小的额外计算开销，显著提升了用户体验。", "keywords": "大型语言模型, 实时语音交互, 推测性解码, 延迟优化, PredGen", "comments": "PredGen通过利用输入时未被利用的计算资源进行推测性解码，巧妙地解决了LLM在实时语音交互中“首句延迟”这一关键瓶颈。这种方法具有创新性，因为它将计算提前，从而在不增加太多额外成本的情况下显著提升了用户体验，对于在消费级硬件上部署LLM语音助手具有重要意义。"}}
{"id": "2506.15220", "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models", "authors": ["Changli Tang", "Yixuan Li", "Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15220v1", "AI": {"title_translation": "video-SALMONN 2: 增强字幕生成的音视频大语言模型", "tldr": "video-SALMONN 2是一个先进的音视频大语言模型，通过创新的多轮定向偏好优化（MrDPO）方法显著提升了视频字幕生成精度，并在同等规模模型中超越了领先模型。", "motivation": "视频包含丰富信息，生成详细准确的自然语言描述是视频理解的关键方面。", "method": "本文提出了video-SALMONN 2，一个结合低秩适应（LoRA）和定向偏好优化（DPO）的增强型音视频大语言模型，用于视频（含配对音频）字幕生成。提出了新的指标来评估视频描述的完整性和准确性，并使用DPO进行优化。为进一步改进训练，提出了一种新颖的多轮DPO（MrDPO）方法，该方法涉及周期性更新DPO参考模型，在每个训练轮次（1000步）后合并并重新初始化LoRA模块作为参数更新的代理，并结合真实视频字幕的指导来稳定训练过程。", "result": "实验结果表明，MrDPO显著增强了video-SALMONN 2的字幕生成精度，将字幕错误率降低了28%。最终的70亿参数video-SALMONN 2模型在视频字幕生成任务上超越了GPT-4o和Gemini-1.5-Pro等领先模型，同时在广泛使用的视频问答基准测试中与同等规模的最先进模型保持高度竞争力。", "conclusion": "video-SALMONN 2通过创新的多轮定向偏好优化（MrDPO）方法，在视频字幕生成和视频问答任务上取得了卓越性能，证明了其作为音视频大语言模型在视频理解领域的强大能力。", "translation": "视频包含丰富信息，在自然语言中生成详细准确的描述是视频理解的关键方面。在本文中，我们提出了video-SALMONN 2，一个先进的音视频大语言模型（LLM），它结合了低秩适应（LoRA）并通过定向偏好优化（DPO）设计用于增强视频（含配对音频）字幕生成。我们提出了新的指标来评估视频描述的完整性和准确性，并使用DPO进行优化。为了进一步改进训练，我们提出了一种新颖的多轮DPO（MrDPO）方法，该方法涉及周期性更新DPO参考模型，在每个训练轮次（1000步）后合并并重新初始化LoRA模块作为参数更新的代理，并结合真实视频字幕的指导来稳定训练过程。实验结果表明，MrDPO显著增强了video-SALMONN 2的字幕生成精度，将字幕错误率降低了28%。最终的70亿参数video-SALMONN 2模型在视频字幕生成任务上超越了GPT-4o和Gemini-1.5-Pro等领先模型，同时在广泛使用的视频问答基准测试中与同等规模的最先进模型保持高度竞争力。代码可在https://github.com/bytedance/video-SALMONN-2 获取。", "summary": "本论文介绍了video-SALMONN 2，一个为增强视频字幕生成而设计的音视频大语言模型。该模型通过结合低秩适应（LoRA）和创新的多轮定向偏好优化（MrDPO）方法进行训练，其中MrDPO通过周期性更新参考模型、重新初始化LoRA模块和整合真实字幕指导来稳定优化过程。实验结果显示，MrDPO将字幕错误率降低了28%，使得70亿参数的video-SALMONN 2在视频字幕生成任务上超越了GPT-4o和Gemini-1.5-Pro等大型模型，并在视频问答任务上保持了与同等规模SOTA模型的竞争力。", "keywords": "视频字幕生成, 音视频大语言模型, DPO, LoRA, MrDPO", "comments": "该论文提出了一种创新的多轮DPO（MrDPO）训练方法，有效提升了音视频大语言模型video-SALMONN 2的视频字幕生成能力。其亮点在于通过周期性更新参考模型和LoRA模块，并结合真实字幕指导，解决了DPO训练中的稳定性问题。尽管模型参数仅为70亿，但其在视频字幕生成方面超越了大型商业模型（如GPT-4o和Gemini-1.5-Pro），并在视频问答上保持竞争力，这展示了其高效性和强大的泛化能力。"}}
{"id": "2506.15118", "title": "CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records", "authors": ["Junke Wang", "Hongshun Ling", "Li Zhang", "Longqian Zhang", "Fang Wang", "Yuan Gao", "Zhi Li"], "summary": "Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.", "comment": "20 pages,5 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15118v1", "AI": {"title_translation": "CKD-EHR：面向电子健康记录的临床知识蒸馏", "tldr": "CKD-EHR框架通过知识蒸馏提升了EHR疾病预测模型的效率和准确性，解决了现有大模型在医疗领域知识表示不足和部署效率低的问题。", "motivation": "现有的基于EHR的疾病预测大语言模型面临两大挑战：医疗知识表示不足和临床部署效率低下。", "method": "本文提出了CKD-EHR框架，采用知识蒸馏技术实现高效准确的疾病风险预测。具体方法是：首先，将大语言模型Qwen2.5-7B在医学知识增强数据上微调作为教师模型；然后，通过多粒度注意力蒸馏机制生成可解释的软标签；最后，将蒸馏的知识转移到轻量级BERT学生模型。", "result": "在MIMIC-III数据集上，CKD-EHR显著优于基线模型：诊断准确率提高9%，F1分数提高27%，推理速度提升22.2倍。", "conclusion": "CKD-EHR解决方案不仅大大提高了资源利用效率，还显著增强了诊断的准确性和及时性，为临床环境中的资源优化提供了实用的技术方法。", "translation": "电子健康记录（EHR）疾病预测模型在促进精准医疗和实现早期干预方面展现出显著的临床价值。然而，现有的大语言模型面临两大主要挑战：医疗知识表示不足和临床部署效率低下。为了解决这些挑战，本研究提出了CKD-EHR（面向EHR的临床知识蒸馏）框架，该框架通过知识蒸馏技术实现了高效准确的疾病风险预测。具体而言，首先在大语言模型Qwen2.5-7B上对医学知识增强数据进行微调，使其作为教师模型。然后，通过多粒度注意力蒸馏机制生成可解释的软标签。最后，将蒸馏的知识转移到轻量级BERT学生模型。实验结果表明，在MIMIC-III数据集上，CKD-EHR显著优于基线模型：诊断准确率提高了9%，F1分数提高了27%，并实现了22.2倍的推理速度提升。这一创新解决方案不仅大大提高了资源利用效率，而且显著增强了诊断的准确性和及时性，为临床环境中的资源优化提供了实用的技术方法。本研究的代码和数据可在https://github.com/209506702/CKD_EHR获取。", "summary": "本文提出了CKD-EHR框架，通过知识蒸馏技术提升了基于电子健康记录（EHR）的疾病预测模型的效率和准确性。该框架使用微调后的Qwen2.5-7B作为教师模型，通过多粒度注意力蒸馏将知识转移到轻量级BERT学生模型。实验结果表明，CKD-EHR在MIMIC-III数据集上显著提高了诊断准确率和F1分数，并大幅提升了推理速度，为临床资源优化提供了实用方案。", "keywords": "电子健康记录, 知识蒸馏, 疾病预测, 大语言模型, 临床应用", "comments": "该研究提出了一种创新的知识蒸馏框架CKD-EHR，有效解决了大型语言模型在医疗领域应用中知识表示不足和部署效率低的痛点。通过将大型教师模型的医疗知识蒸馏到轻量级学生模型，实现了在保持甚至提升预测准确性的同时，大幅提高推理速度和资源利用效率，这对于临床实践中的快速、准确诊断具有重要意义。"}}
{"id": "2506.14916", "title": "Interpolation-based reproducing kernel particle method", "authors": ["Jennifer E. Fromm", "John A. Evans", "J. S. Chen"], "summary": "Meshfree methods, including the reproducing kernel particle method (RKPM),\nhave been widely used within the computational mechanics community to model\nphysical phenomena in materials undergoing large deformations or extreme\ntopology changes. RKPM shape functions and their derivatives cannot be\naccurately integrated with the Gauss-quadrature methods widely employed for the\nfinite element method (FEM) and typically require sophisticated nodal\nintegration techniques, preventing them from easily being implemented in\nexisting FEM software. Interpolation-based methods have been developed to\naddress similar problems with isogeometric and immersed boundary methods,\nallowing these techniques to be implemented within open-source finite element\nsoftware. With interpolation-based methods, background basis functions are\nrepresented as linear combinations of Lagrange polynomial foreground basis\nfunctions defined upon a boundary-conforming foreground mesh. This work extends\nthe applications of interpolation-based methods to implement RKPM within\nopen-source finite element software. Interpolation-based RKPM is applied to\nseveral PDEs, and error convergence rates are equivalent to classic RKPM\nintegrated using high-order Gauss-quadrature schemes. The interpolation-based\nmethod is able to exploit the continuity of the RKPM basis to solve\nhigher-order PDEs, demonstrated through the biharmonic problem. The method is\nextended to multi-material problems through Heaviside enrichment schemes, using\nlocal foreground refinement to reduce geometric integration error and achieve\nhigh-order accuracy. The computational cost of interpolation-based RKPM is\nsimilar to the smoothed gradient nodal integration schemes, offering\nsignificant savings over Gauss-quadrature-based meshfree methods while enabling\neasy implementation within existing finite element software.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14916v1", "AI": {"title_translation": "插值型再生核粒子法", "tldr": "开发了一种基于插值的方法，使再生核粒子法（RKPM）能更容易地集成到现有有限元软件中，同时保持精度并降低计算成本。", "motivation": "传统再生核粒子法（RKPM）的形函数及其导数难以用高斯积分法精确积分，通常需要复杂的节点积分技术，这阻碍了其在现有有限元软件中的方便实现。", "method": "本文将插值型方法应用于再生核粒子法（RKPM），通过将背景基函数表示为定义在边界一致前景网格上的拉格朗日多项式前景基函数的线性组合。该方法还通过Heaviside富集方案扩展到多材料问题，并使用局部前景细化来减少几何积分误差。", "result": "插值型RKPM应用于多个偏微分方程，其误差收敛率与使用高阶高斯积分方案的经典RKPM相当。该方法能够利用RKPM基函数的连续性来解决高阶偏微分方程（通过双调和问题演示）。其计算成本与平滑梯度节点积分方案相似，比基于高斯积分的无网格方法显著节省。", "conclusion": "插值型再生核粒子法提供了一种有效且高效的方法，可以在现有有限元软件中方便地实现RKPM，同时保持计算精度并降低成本，并能处理高阶和多材料问题。", "translation": "无网格方法，包括再生核粒子法（RKPM），在计算力学领域中被广泛用于模拟经历大变形或极端拓扑变化的材料中的物理现象。RKPM形函数及其导数无法通过有限元法（FEM）中广泛使用的高斯积分方法精确积分，通常需要复杂的节点积分技术，这阻碍了它们在现有FEM软件中的方便实现。为了解决等几何和浸入边界方法中的类似问题，已经开发了基于插值的方法，使得这些技术可以在开源有限元软件中实现。基于插值的方法中，背景基函数被表示为定义在与边界一致的前景网格上的拉格朗日多项式前景基函数的线性组合。这项工作扩展了基于插值方法的应用，以在开源有限元软件中实现RKPM。插值型RKPM应用于多个偏微分方程，其误差收敛率与使用高阶高斯积分方案的经典RKPM相当。基于插值的方法能够利用RKPM基函数的连续性来解决高阶偏微分方程，通过双调和问题进行了演示。该方法通过Heaviside富集方案扩展到多材料问题，利用局部前景细化来减少几何积分误差并实现高阶精度。插值型RKPM的计算成本与平滑梯度节点积分方案相似，与基于高斯积分的无网格方法相比显著节省，同时便于在现有有限元软件中实现。", "summary": "本文提出了一种插值型再生核粒子法（Interpolation-based RKPM），旨在解决传统RKPM在有限元软件中实施困难的问题。通过将背景基函数表示为拉格朗日多项式前景基函数的线性组合，该方法使得RKPM能够方便地集成到现有FEM软件中。研究表明，插值型RKPM在多个PDEs上的误差收敛率与经典RKPM相当，且能够解决高阶PDEs并扩展到多材料问题。此外，其计算成本显著低于基于高斯积分的无网格方法。", "keywords": "再生核粒子法, 无网格方法, 有限元方法, 插值, 计算力学", "comments": "这项工作通过引入插值型方法，有效地解决了再生核粒子法（RKPM）在现有有限元软件中集成困难的关键瓶颈。其创新之处在于将背景基函数与前景网格上的拉格朗日多项式结合，实现了RKPM的易用性，同时保持了计算精度。这对于促进RKPM在工程实践中的应用具有重要意义。"}}
{"id": "2506.15170", "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem", "authors": ["Yanxu Mao", "Tiehan Cui", "Peipei Liu", "Datao You", "Hongsong Zhu"], "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15170v1", "AI": {"title_translation": "从大型语言模型到多模态大型语言模型再到智能体：大型语言模型生态系统中越狱攻击与防御新兴范式的综述", "tldr": "本文综述了大型语言模型（LLMs）向多模态LLMs和智能体发展过程中，越狱攻击和防御机制日益增长的复杂性，并指出了现有研究的局限性及未来方向。", "motivation": "大型语言模型（LLMs）正迅速从单模态系统演变为多模态LLMs和智能体，这在显著扩展其能力的同时，也带来了日益严峻的安全风险。本研究的动机是系统性地调查在不断扩展的LLM生态系统中，越狱攻击和相应防御机制日益增长的复杂性。", "method": "本文采用系统性综述的方法。首先，追踪了从LLMs到MLLMs再到智能体的演变轨迹，并强调了每个阶段出现的核心安全挑战。其次，从攻击影响和可见性角度对主流越狱技术进行了分类，并分析了代表性攻击方法、相关数据集和评估指标。在防御方面，根据响应时间和技术方法组织了现有策略。此外，还识别了现有综述的局限性，并提供了最新工作的综合分析，同时概述了未来的研究方向。", "result": "本研究系统地综述了LLM生态系统中日益复杂的越狱攻击和防御机制。它揭示了从LLMs到MLLMs再到智能体发展过程中出现的核心安全挑战。研究对主流越狱技术进行了分类，并分析了代表性攻击方法、数据集和评估指标。在防御方面，组织了现有策略。同时，识别了现有综述的关键局限性，并提供了最新工作的综合分析，概述了数据集构建、评估框架优化和策略泛化等方面的未来研究方向。", "conclusion": "本研究旨在增进对越狱机制的理解，并促进在日益强大的LLMs背景下，开发出更具弹性和适应性的防御策略。", "translation": "大型语言模型（LLMs）正迅速从单模态系统演变为多模态LLMs和智能体，这在显著扩展其能力的同时，也带来了日益严峻的安全风险。本文系统性地综述了在不断扩展的LLM生态系统中，越狱攻击和相应防御机制日益增长的复杂性。我们首先追溯了从LLMs到MLLMs再到智能体的发展轨迹，强调了每个阶段出现的核心安全挑战。接下来，我们从攻击影响和可见性角度对主流越狱技术进行了分类，并对代表性攻击方法、相关数据集和评估指标进行了全面分析。在防御方面，我们根据响应时间和技术方法组织了现有策略，以提供对其适用性和实施的结构化理解。此外，我们指出了现有综述的关键局限性，例如对智能体特定安全问题关注不足、缺乏混合越狱方法的清晰分类、缺乏对实验设置的详细分析以及对最新进展的覆盖过时。为了解决这些局限性，我们提供了最新工作的更新综合分析，并概述了数据集构建、评估框架优化和策略泛化等领域的未来研究方向。我们的研究旨在增进对越狱机制的理解，并促进在日益强大的LLMs背景下，开发出更具弹性和适应性的防御策略。", "summary": "本文对大型语言模型（LLMs）生态系统中，从LLMs到多模态LLMs再到智能体演变过程中出现的越狱攻击和防御机制进行了全面的系统性综述。研究首先追溯了模型发展轨迹中的安全挑战，然后分类并分析了主流越狱技术（包括攻击方法、数据集和评估指标）以及相应的防御策略。此外，本文识别了现有综述的不足，并提供了最新研究的综合分析，同时提出了未来研究方向，旨在加深对越狱机制的理解并推动更具弹性的防御策略发展。", "keywords": "LLM安全, 越狱攻击, 多模态大模型, 智能体, 防御策略", "comments": "该论文的创新之处在于其对LLM生态系统演进（从LLM到MLLM再到Agent）中越狱攻击与防御的全面覆盖，填补了现有综述在智能体安全、混合攻击分类和最新进展方面的空白。其重要性体现在为理解不断演变的大模型安全威胁提供了结构化视角，并为未来研究指明了方向，有助于构建更鲁棒的AI系统。"}}
{"id": "2506.15293", "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces", "authors": ["Francesco Chiossi", "Julian Rasch", "Robin Welsch", "Albrecht Schmidt", "Florian Michahelles"], "summary": "As robots enter collaborative workspaces, ensuring mutual understanding\nbetween human workers and robotic systems becomes a prerequisite for trust,\nsafety, and efficiency. In this position paper, we draw on the cooperation\nscenario of the AIMotive project in which a human and a cobot jointly perform\nassembly tasks to argue for a structured approach to intent communication.\nBuilding on the Situation Awareness-based Agent Transparency (SAT) framework\nand the notion of task abstraction levels, we propose a multidimensional design\nspace that maps intent content (SAT1, SAT3), planning horizon (operational to\nstrategic), and modality (visual, auditory, haptic). We illustrate how this\nspace can guide the design of multimodal communication strategies tailored to\ndynamic collaborative work contexts. With this paper, we lay the conceptual\nfoundation for a future design toolkit aimed at supporting transparent\nhuman-robot interaction in the workplace. We highlight key open questions and\ndesign challenges, and propose a shared agenda for multimodal, adaptive, and\ntrustworthy robotic collaboration in hybrid work environments.", "comment": "9 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15293v1", "AI": {"title_translation": "意图设计：工业工作空间中人机协作的多模态框架", "tldr": "本论文提出了一种多模态框架，用于工业环境中人机协作中的意图通信，基于SAT和任务抽象，旨在增强信任、安全性和效率。", "motivation": "随着机器人进入协作工作空间，确保人类工人与机器人系统之间的相互理解成为信任、安全和效率的先决条件，因此需要研究如何实现有效的意图通信。", "method": "论文基于态势感知代理透明度（SAT）框架和任务抽象级别概念，提出了一个多维设计空间。该空间映射了意图内容（SAT1、SAT3）、规划范围（从操作到战略）和模态（视觉、听觉、触觉），以指导多模态通信策略的设计。", "result": "本论文为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。它阐述了所提出的设计空间如何指导多模态通信策略，并强调了关键的开放问题和设计挑战。", "conclusion": "本论文为支持透明人机交互的未来设计工具包奠定了概念基础，并为混合工作环境中多模态、自适应和值得信赖的机器人协作提出了一个共同议程。", "translation": "随着机器人进入协作工作空间，确保人类工人与机器人系统之间的相互理解成为信任、安全和效率的先决条件。在这篇立场论文中，我们借鉴了AIMotive项目中的协作场景（其中人类和协作机器人共同执行装配任务），以论证一种结构化的意图通信方法。基于态势感知代理透明度（SAT）框架和任务抽象级别概念，我们提出了一个多维设计空间，映射了意图内容（SAT1、SAT3）、规划范围（从操作到战略）和模态（视觉、听觉、触觉）。我们阐述了该空间如何指导多模态通信策略的设计，以适应动态的协作工作环境。凭借这篇论文，我们为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。我们强调了关键的开放问题和设计挑战，并为混合工作环境中多模态、自适应和值得信赖的机器人协作提出了一个共同议程。", "summary": "这篇立场论文提出了一种用于工业协作工作空间中人机意图通信的多模态框架。论文借鉴AIMotive项目，引入了一个基于SAT框架和任务抽象级别的多维设计空间。该空间映射了意图内容、规划范围和模态，以指导有效的通信策略。本文旨在为未来旨在增强透明人机交互的设计工具包奠定概念基础，并解决自适应和值得信赖的机器人协作的关键挑战。", "keywords": "人机协作, 多模态通信, 意图通信, 工业工作空间, 态势感知", "comments": "该论文作为一篇立场论文，其创新性在于为工业环境中的人机协作意图通信提出了一个结构化、多维的设计空间，并整合了现有的SAT和任务抽象等概念。它为未来开发提高人机交互透明度和信任度的设计工具包奠定了重要的概念基础。尽管本论文未提供实证验证，但其概念框架对于推动混合工作环境中的人机协作具有重要意义。"}}
{"id": "2506.15595", "title": "LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters", "authors": ["Kunming Zhang", "Hanlong Liao", "Guoming Tang"], "summary": "Parallel computing with multiple GPUs has become the dominant paradigm for\nmachine learning tasks, especially those of large language models (LLMs). To\nreduce the latency incurred by inter-GPU communication, a common practice for\nparallel tasks has been to allocate GPUs based on their physical proximity.\nHowever, this long-standing assumption has notable limitations, particularly in\nlarge-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs\nis irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU\ndispatching system based on global perspectives. To tackle the difficulty of\nstoring massive GPU topology information, LiteGD adopts a computation-aware\ndesign that leverages a lightweight Transformer network trained on sampled\ndata. Our customized design for network structure ensures both transferability\nand scalability. LiteGD also employs a bidirectional tree search approach to\nfind the optimal GPU dispatching in the data generated in the previous step,\nwhich can identify near-optimal solutions while reducing search overhead. We\nimplement and evaluate LiteGD in both real and simulated GPU clusters with\nhomogeneous and heterogeneous interconnects, respectively. Experimental results\ndemonstrate that LiteGD consistently achieves high GPU bandwidth efficacy\n(approximately 90\\%) across various cluster configurations and 80\\% in\nreal-world H100 cluster, significantly outperforming conventional default and\ninterconnect topology-aware dispatching methods, particularly in large-scale\nheterogeneous environments.", "comment": "12 pages, 19 figures,7 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15595v1", "AI": {"title_translation": "LiteGD: 轻量级动态GPU调度，适用于大规模异构集群", "tldr": "LiteGD是一种轻量级动态GPU调度系统，通过学习网络拓扑并采用双向树搜索，显著提升了大规模异构GPU集群中的带宽效率。", "motivation": "现有的GPU分配方法（基于物理邻近性）在处理大规模异构GPU集群中不规则的带宽分布时存在显著局限性，导致GPU间通信延迟。", "method": "本文引入了LiteGD，一个基于全局视角的轻量级动态GPU调度系统。它采用计算感知设计，利用在采样数据上训练的轻量级Transformer网络来处理大规模GPU拓扑信息，并定制网络结构以确保可迁移性和可扩展性。LiteGD还采用双向树搜索方法来找到近乎最优的GPU调度方案，同时减少搜索开销。", "result": "LiteGD在各种集群配置中始终实现高GPU带宽效率（约90%），在真实世界的H100集群中达到80%，显著优于传统的默认和互连拓扑感知调度方法，特别是在大规模异构环境中。", "conclusion": "LiteGD通过其创新的计算感知设计和双向树搜索方法，有效解决了大规模异构GPU集群中的调度挑战，显著提高了GPU带宽效率，优于现有方法。", "translation": "并行计算与多GPU已成为机器学习任务，特别是大型语言模型（LLMs）的主导范式。为了减少GPU间通信引起的延迟，并行任务的常见做法是根据其物理邻近性分配GPU。然而，这种长期存在的假设存在显著局限性，特别是在带宽分布不规则的大规模异构GPU集群中。在本文中，我们引入了LiteGD，一个基于全局视角的轻量级动态GPU调度系统。为了解决存储海量GPU拓扑信息的难题，LiteGD采用了一种计算感知设计，利用在采样数据上训练的轻量级Transformer网络。我们为网络结构定制的设计确保了可迁移性和可扩展性。LiteGD还采用双向树搜索方法，在前面步骤生成的数据中找到最优的GPU调度，这可以识别接近最优的解决方案，同时减少搜索开销。我们在真实和模拟的GPU集群中分别对LiteGD进行了实现和评估，这些集群具有同构和异构互连。实验结果表明，LiteGD在各种集群配置中始终实现高GPU带宽效率（约90%），在真实世界的H100集群中达到80%，显著优于传统的默认和互连拓扑感知调度方法，特别是在大规模异构环境中。", "summary": "本文提出了LiteGD，一个针对大规模异构GPU集群的轻量级动态GPU调度系统。针对现有基于物理邻近性的调度方法在复杂拓扑中的局限性，LiteGD引入了计算感知设计，利用轻量级Transformer网络学习GPU拓扑，并通过定制网络结构保证可迁移性和可扩展性。此外，它采用双向树搜索找到近乎最优的GPU调度方案，以减少搜索开销。实验证明，LiteGD在同构和异构集群中均能实现高GPU带宽效率（最高90%），并显著优于传统调度方法，特别是在大规模异构环境中表现更优。", "keywords": "GPU调度, 异构集群, Transformer, 带宽效率, 并行计算", "comments": "LiteGD的创新之处在于其结合了计算感知设计与轻量级Transformer网络来处理复杂的GPU拓扑信息，以及采用双向树搜索来高效寻找最优调度，这对于解决大规模异构集群中的性能瓶颈具有重要意义。其在真实H100集群中取得的高带宽效率验证了其实用价值。"}}
{"id": "2506.14842", "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers", "authors": ["Lukas Schiesser", "Cornelius Wolff", "Sophie Haas", "Simon Pukrop"], "summary": "Building image classification models remains cumbersome in data-scarce\ndomains, where collecting large labeled datasets is impractical. In-context\nlearning (ICL) has emerged as a promising paradigm for few-shot image\nclassification (FSIC), enabling models to generalize across domains without\ngradient-based adaptation. However, prior work has largely overlooked a\ncritical component of ICL-based FSIC pipelines: the role of image embeddings.\nIn this work, we present PictSure, an ICL framework that places the embedding\nmodel -- its architecture, pretraining, and training dynamics -- at the center\nof analysis. We systematically examine the effects of different visual encoder\ntypes, pretraining objectives, and fine-tuning strategies on downstream FSIC\nperformance. Our experiments show that the training success and the\nout-of-domain performance are highly dependent on how the embedding models are\npretrained. Consequently, PictSure manages to outperform existing ICL-based\nFSIC models on out-of-domain benchmarks that differ significantly from the\ntraining distribution, while maintaining comparable results on in-domain tasks.\nCode can be found at https://github.com/PictSure/pictsure-library.", "comment": "15 pages, 10 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14842v1", "AI": {"title_translation": "PictSure：预训练嵌入对上下文学习图像分类器至关重要", "tldr": "PictSure是一个关注图像嵌入预训练的上下文学习（ICL）框架，在少样本图像分类（FSIC）中，通过优化嵌入模型，显著提升了模型在域外基准测试上的性能。", "motivation": "在数据稀缺领域，构建图像分类模型由于难以收集大规模标注数据集而变得困难。上下文学习（ICL）作为一种无需基于梯度的适应即可在少样本图像分类（FSIC）中实现跨域泛化的范式，很有前景。然而，现有工作大多忽略了ICL-FSIC流程中的关键组成部分：图像嵌入的作用。", "method": "我们提出了PictSure，一个将嵌入模型（其架构、预训练和训练动态）置于分析中心的ICL框架。我们系统地检查了不同视觉编码器类型、预训练目标和微调策略对下游FSIC性能的影响。", "result": "实验表明，训练成功和域外性能高度依赖于嵌入模型的预训练方式。因此，PictSure在与训练分布显著不同的域外基准测试上超越了现有的基于ICL的FSIC模型，同时在域内任务上保持了可比的结果。", "conclusion": "预训练图像嵌入模型对于上下文学习图像分类器的性能，尤其是在域外泛畴化能力上，具有决定性影响。", "translation": "在数据稀缺领域，构建图像分类模型仍然很麻烦，因为收集大规模标注数据集是不切实际的。上下文学习（ICL）已成为少样本图像分类（FSIC）的一个有前景的范式，使模型无需基于梯度的适应即可在不同领域泛化。然而，先前的工作在很大程度上忽略了基于ICL的FSIC流程中的一个关键组成部分：图像嵌入的作用。在这项工作中，我们提出了PictSure，一个将嵌入模型——其架构、预训练和训练动态——置于分析中心的ICL框架。我们系统地检查了不同视觉编码器类型、预训练目标和微调策略对下游FSIC性能的影响。我们的实验表明，训练成功和域外性能高度依赖于嵌入模型的预训练方式。因此，PictSure在与训练分布显著不同的域外基准测试上成功地超越了现有的基于ICL的FSIC模型，同时在域内任务上保持了可比的结果。代码可在https://github.com/PictSure/pictsure-library找到。", "summary": "该论文提出了PictSure，一个专注于图像嵌入模型在上下文学习（ICL）中作用的框架，以解决数据稀缺领域少样本图像分类（FSIC）的挑战。研究系统地探究了视觉编码器类型、预训练目标和微调策略对FSIC性能的影响，发现嵌入模型的预训练方式对模型在域外泛化能力上起着决定性作用。PictSure在域外基准测试上表现优于现有ICL-FSIC模型，同时在域内任务上保持竞争力。", "keywords": "上下文学习, 图像分类, 少样本学习, 图像嵌入, 预训练", "comments": "该研究的创新之处在于其首次系统地将图像嵌入模型（包括其架构、预训练和训练动态）作为ICL-FSIC流程的核心分析对象，填补了现有研究的空白。其重要性在于揭示了预训练嵌入对模型域外泛化能力的关键影响，为未来在数据稀缺场景下提升FSIC性能提供了新的研究方向和实践指导。"}}
{"id": "2506.14797", "title": "Bound by semanticity: universal laws governing the generalization-identification tradeoff", "authors": ["Marco Nurisso", "Jesseba Fernando", "Raj Deshpande", "Alan Perotti", "Raja Marjieh", "Steven M. Frankland", "Richard L. Lewis", "Taylor W. Webb", "Declan Campbell", "Francesco Vaccarino", "Jonathan D. Cohen", "Giovanni Petri"], "summary": "Intelligent systems must deploy internal representations that are\nsimultaneously structured -- to support broad generalization -- and selective\n-- to preserve input identity. We expose a fundamental limit on this tradeoff.\nFor any model whose representational similarity between inputs decays with\nfinite semantic resolution $\\varepsilon$, we derive closed-form expressions\nthat pin its probability of correct generalization $p_S$ and identification\n$p_I$ to a universal Pareto front independent of input space geometry.\nExtending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs\npredicts a sharp $1/n$ collapse of multi-input processing capacity and a\nnon-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end\nreproduces these laws: during learning a resolution boundary self-organizes and\nempirical $(p_S,p_I)$ trajectories closely follow theoretical curves for\nlinearly decaying similarity. Finally, we demonstrate that the same limits\npersist in two markedly more complex settings -- a convolutional neural network\nand state-of-the-art vision-language models -- confirming that\nfinite-resolution similarity is a fundamental emergent informational\nconstraint, not merely a toy-model artifact. Together, these results provide an\nexact theory of the generalization-identification trade-off and clarify how\nsemantic resolution shapes the representational capacity of deep networks and\nbrains alike.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14797v1", "AI": {"title_translation": "受语义性约束：泛化-识别权衡的普适法则", "tldr": "智能系统在泛化和识别之间的权衡存在一个普遍限制，由语义分辨率决定，并适用于深度网络和大脑。", "motivation": "智能系统需要内部表示既能支持广泛泛化又能保持输入识别，但这两者之间存在权衡，本文旨在揭示这种权衡的根本限制。", "method": "作者推导了在有限语义分辨率下，正确泛化概率($p_S$)和识别概率($p_I$)的封闭形式表达式，并将其与一个普适的帕累托前沿关联。进一步将分析扩展到噪声、异构空间和多输入情况。通过训练一个最小的ReLU网络验证了这些定律，并证明了相同的限制在卷积神经网络和最先进的视觉-语言模型中也存在。", "result": "对于任何模型，其表示相似性随有限语义分辨率($\\varepsilon$)衰减，其正确泛化概率($p_S$)和识别概率($p_I$)被限制在一个普适的帕累托前沿上，与输入空间几何无关。分析预测多输入处理能力会急剧下降($1/n$)，并且$p_S$存在非单调最优值。实验发现，ReLU网络在学习过程中会自组织一个分辨率边界，且经验性的$(p_S,p_I)$轨迹与理论曲线密切吻合。这些限制在更复杂的卷积神经网络和视觉-语言模型中也持续存在。", "conclusion": "这些结果提供了一个关于泛化-识别权衡的精确理论，并阐明了语义分辨率如何塑造深度网络和大脑的表征能力。", "translation": "智能系统必须部署内部表示，这些表示既要结构化——以支持广泛的泛化——又要选择性——以保持输入识别。我们揭示了这种权衡的一个根本限制。对于任何模型的表征相似性随有限语义分辨率 $\\varepsilon$ 衰减的模型，我们推导出了封闭形式的表达式，将其正确泛化概率 $p_S$ 和识别概率 $p_I$ 限制在一个与输入空间几何无关的普适帕累托前沿上。将分析扩展到有噪声、异构空间以及 $n>2$ 个输入的情况，预测了多输入处理能力会急剧下降（$1/n$），并且 $p_S$ 存在一个非单调最优值。一个经过端到端训练的最小ReLU网络重现了这些定律：在学习过程中，一个分辨率边界自组织形成，并且经验性的 $(p_S,p_I)$ 轨迹与线性衰减相似性的理论曲线密切吻合。最后，我们证明了相同的限制在两个明显更复杂的设置中也持续存在——一个卷积神经网络和最先进的视觉-语言模型——证实了有限分辨率相似性是一种基本的涌现信息约束，而不仅仅是玩具模型的产物。总而言之，这些结果提供了一个关于泛化-识别权衡的精确理论，并阐明了语义分辨率如何塑造深度网络和大脑的表征能力。", "summary": "本文揭示了智能系统在泛化和识别之间权衡的根本限制，提出了一个普适理论。研究表明，在有限语义分辨率下，泛化和识别能力受限于一个普适的帕累托前沿。该理论预测了多输入处理能力的下降和泛化概率的非单调性，并通过最小ReLU网络、卷积神经网络和视觉-语言模型进行了验证，证明了语义分辨率是影响深度网络和大脑表征能力的关键信息约束。", "keywords": "泛化-识别权衡, 语义分辨率, 表征能力, 深度学习, 普适法则", "comments": "这篇论文的创新之处在于提出了一个关于泛化-识别权衡的精确理论，并通过引入“语义分辨率”这一概念，揭示了智能系统（包括人工神经网络和大脑）在表示能力上的根本限制。其重要性在于提供了一个统一的框架来理解深度学习模型在泛化和识别任务中的表现，并指出这种限制并非仅仅是玩具模型的产物，而是普遍存在的。"}}
{"id": "2506.15683", "title": "PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning", "authors": ["Yuhui Shi", "Yehan Yang", "Qiang Sheng", "Hao Mi", "Beizhe Hu", "Chaoxi Xu", "Juan Cao"], "summary": "With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.", "comment": "17 pages, 3 figures, 6 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15683v1", "AI": {"title_translation": "PhantomHunter：通过家族感知学习检测未见过的私有微调LLM生成文本", "tldr": "本文提出了PhantomHunter，一个专门用于检测由未见过的私有微调大型语言模型（LLM）生成的文本的检测器，通过家族感知学习框架，在多个LLM家族数据集上实现了超过96%的F1分数。", "motivation": "随着LLM的普及，误导信息生产和学术不端等社会问题日益严重，LLM生成文本检测变得至关重要。然而，现有方法在检测由私有微调LLM生成的文本时性能显著下降，因为用户可以轻松通过微调开源LLM来创建私有LLM。", "method": "本文提出了PhantomHunter，一个LLM生成文本检测器，专门用于检测未见过的私有微调LLM生成的文本。它采用家族感知学习框架，捕获基础模型及其衍生模型之间共享的家族级特征，而非记忆个体特征。", "result": "在LLaMA、Gemma和Mistral家族的数据集上进行的实验表明，PhantomHunter优于7个基线方法和3个工业服务，F1分数超过96%。", "conclusion": "PhantomHunter成功解决了检测未见过的私有微调LLM生成文本的挑战，并通过其家族感知学习框架展示了卓越的性能和泛化能力。", "translation": "随着大型语言模型（LLM）的普及，误导信息生产和学术不端等不良社会问题变得更加严重，使得LLM生成文本检测现在具有前所未有的重要性。尽管现有方法取得了显著进展，但由私有微调LLM生成的文本所带来的新挑战仍未得到充分探索。用户可以通过使用私有语料库对开源LLM进行微调，轻松拥有私有LLM，导致现有检测器在实践中性能显著下降。为了解决这个问题，我们提出了PhantomHunter，一个专门用于检测未见过的私有微调LLM生成文本的LLM生成文本检测器。其家族感知学习框架捕获了基础模型及其衍生模型共享的家族级特征，而不是记忆个体特征。在LLaMA、Gemma和Mistral家族的数据上进行的实验表明，其优于7个基线和3个工业服务，F1分数超过96%。", "summary": "本文针对现有LLM生成文本检测器在面对私有微调模型时性能下降的问题，提出了PhantomHunter。该检测器采用独特的家族感知学习框架，通过捕获LLM家族共享的普遍特征而非个体特性，有效识别由未见过的私有微调LLM生成的文本。实验结果表明，PhantomHunter在多个主流LLM家族数据集上表现出卓越的性能，F1分数超过96%，显著优于现有基线和工业服务。", "keywords": "LLM文本检测, 私有微调LLM, 家族感知学习, PhantomHunter, 文本检测", "comments": "本文的创新点在于提出了“家族感知学习”框架，有效解决了现有LLM文本检测器在面对私有微调模型时泛化能力不足的痛点。通过关注模型家族的共性特征，PhantomHunter提高了对未知私有模型的检测能力，这对于应对LLM带来的虚假信息和学术不端等社会问题具有重要实践意义。"}}
{"id": "2506.15465", "title": "DATA-DRIVEN PRONTO: a Model-free Solution for Numerical Optimal Control", "authors": ["Marco Borghesi", "Lorenzo Sforni", "Giuseppe Notarstefano"], "summary": "This article addresses the problem of data-driven numerical optimal control\nfor unknown nonlinear systems. In our scenario, we suppose to have the\npossibility of performing multiple experiments (or simulations) on the system.\nExperiments are performed by relying on a data-driven tracking controller able\nto steer the system towards a desired reference. Our proposed DATA-DRIVEN\nPRONTO algorithm iteratively refines a tentative solution of the optimal\ncontrol problem by computing an approximate descent direction via a local\ntrajectory perturbation. At each iteration, multiple trajectories are gathered\nby perturbing the current trajectory with a suitable dither signal, and then\nused to obtain a data-driven, time-varying linearization. The exploration is\nguided by the tracking controller, so that perturbed trajectories are obtained\nin closed loop. We show local convergence of DATA-DRIVEN PRONTO to a ball about\nan isolated optimal solution, whose radius depends on the amplitude of the\ndither signal. We corroborate the theoretical results by applying it to an\nunderactuated robot.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15465v1", "AI": {"title_translation": "数据驱动的PRONTO：一种用于数值最优控制的无模型解决方案", "tldr": "本文提出了一种名为DATA-DRIVEN PRONTO的无模型算法，用于未知非线性系统的数值最优控制，通过数据驱动的轨迹扰动和线性化实现局部收敛。", "motivation": "本文旨在解决未知非线性系统的数据驱动数值最优控制问题。", "method": "所提出的DATA-DRIVEN PRONTO算法通过计算局部轨迹扰动来迭代优化最优控制问题的尝试解。在每次迭代中，通过施加合适的抖动信号扰动当前轨迹来收集多条轨迹，然后用于获取数据驱动的时变线性化。探索过程由跟踪控制器引导，以闭环方式获得扰动轨迹。", "result": "DATA-DRIVEN PRONTO算法显示出局部收敛到一个孤立最优解附近的球体，该球体的半径取决于抖动信号的幅度。理论结果通过应用于一个欠驱动机器人得到了证实。", "conclusion": "DATA-DRIVEN PRONTO算法能够实现未知非线性系统数值最优控制的局部收敛。", "translation": "本文解决了未知非线性系统的数据驱动数值最优控制问题。在我们的场景中，我们假设能够对系统进行多次实验（或仿真）。实验通过依赖于数据驱动的跟踪控制器进行，该控制器能够将系统引导至所需的参考。我们提出的DATA-DRIVEN PRONTO算法通过计算局部轨迹扰动来获得近似下降方向，从而迭代地改进最优控制问题的试探性解决方案。在每次迭代中，通过使用合适的抖动信号扰动当前轨迹来收集多条轨迹，然后用于获得数据驱动的时变线性化。探索由跟踪控制器引导，因此在闭环中获得扰动轨迹。我们证明了DATA-DRIVEN PRONTO局部收敛到一个孤立最优解附近的球体，其半径取决于抖动信号的幅度。我们通过将其应用于欠驱动机器人来证实了理论结果。", "summary": "本文提出了一种名为DATA-DRIVEN PRONTO的无模型算法，用于解决未知非线性系统的数据驱动数值最优控制问题。该算法通过迭代地扰动当前轨迹并收集数据以获得时变线性化来细化最优控制解。在跟踪控制器的引导下，它能实现对最优解的局部收敛，其收敛半径与抖动信号幅度相关，并在欠驱动机器人上得到了验证。", "keywords": "数据驱动, 最优控制, 无模型, 非线性系统, 局部收敛", "comments": "这项工作提出了一种新颖的无模型数据驱动方法来解决未知非线性系统的最优控制问题，这在实际应用中具有重要意义。其创新之处在于利用局部轨迹扰动和数据驱动的线性化来迭代逼近最优解，并且在闭环设置下进行。方法的局部收敛性是其理论贡献，同时通过机器人应用展示了其可行性。"}}
{"id": "2506.15395", "title": "A Real-time Endoscopic Image Denoising System", "authors": ["Yu Xing", "Shishi Huang", "Meng Lv", "Guo Chen", "Huailiang Wang", "Lingzhi Sui"], "summary": "Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15395v1", "AI": {"title_translation": "实时内窥镜图像去噪系统", "tldr": "该研究开发了一种混合去噪系统，解决了小型内窥镜图像的噪声问题，并在FPGA上实现了实时性能，显著提高了图像质量。", "motivation": "微型化的一次性内窥镜虽然增强了操作灵活性和诊断能力，但其超紧凑模拟图像传感器由于光敏面积有限、动态范围受限以及电路设计简化，导致处理后的内窥镜图像存在显著的噪声问题。", "method": "本研究首先为医用内窥镜中的模拟图像传感器开发了一个全面的噪声模型，涵盖了固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。在此分析基础上，提出了一个混合去噪系统，该系统协同结合了传统图像处理算法和先进的基于学习的技术，用于处理从传感器捕获的原始帧。", "result": "该方法在不损失精细细节或颜色失真的情况下有效降低了图像噪声，同时在FPGA平台上实现了实时性能，并在测试数据集上将平均PSNR从21.16提高到33.05。", "conclusion": "所提出的混合去噪系统有效地解决了小型传感器内窥镜图像的实时噪声问题，提高了图像质量，并支持实际应用。", "translation": "具有小型化设计的内窥镜显著增强了操作灵活性、便携性和诊断能力，同时大幅降低了医疗程序的侵入性。最近，配备小于1毫米x1毫米超紧凑模拟图像传感器的一次性内窥镜为医学诊断带来了革命性进展。它们减少了与可重复使用设备相关的结构冗余和高昂的资本支出，消除了因消毒不足引起的患者感染风险，并减轻了患者痛苦。然而，有限的光敏面积导致每个像素的光子捕获量减少，需要更高的光子灵敏度设置以保持足够的亮度。在高对比度医疗成像场景中，小型传感器表现出受限的动态范围，使得难以同时捕获高光和阴影中的细节，并且需要额外的局部数字增益来补偿。此外，简化的电路设计和模拟信号传输引入了额外的噪声源。这些因素共同导致处理后的内窥镜图像中存在显著的噪声问题。在这项工作中，我们为医用内窥镜中的模拟图像传感器开发了一个全面的噪声模型，解决了三种主要的噪声类型：固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。基于这一分析，我们提出了一种混合去噪系统，该系统协同结合了传统图像处理算法和先进的基于学习的技术，用于处理从传感器捕获的原始帧。实验表明，我们的方法在不损失精细细节或颜色失真的情况下有效降低了图像噪声，同时在FPGA平台上实现了实时性能，并在我们的测试数据集上将平均PSNR从21.16提高到33.05。", "summary": "本文针对微型化一次性内窥镜图像中存在的显著噪声问题进行了研究，这些问题源于有限的传感器面积、受限的动态范围和简化的模拟电路。作者为这些传感器开发了一个全面的噪声模型，识别出固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。在此基础上，他们提出了一种结合传统图像处理和基于学习技术的混合去噪系统。实验结果表明，该系统能有效降低噪声，同时保持图像细节和色彩，并在FPGA上实现实时性能，显著提高了PSNR。", "keywords": "内窥镜图像, 去噪, 实时, 混合系统, 模拟传感器", "comments": "该研究的创新之处在于为紧凑型模拟内窥镜传感器开发了特定的噪声模型，并提出了一种结合传统和基于学习方法的混合去噪方案。这对于在FPGA等受限硬件上实现高质量和实时性能至关重要。这项工作对于提高下一代一次性内窥镜的诊断能力具有重要意义。"}}
{"id": "2506.15670", "title": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward", "authors": ["Özlem Tugfe Demir", "Mustafa Ozger", "Ferdi Kara", "Woong-Hee Lee", "Emil Björnson"], "summary": "This paper explores the integration of simultaneous wireless information and\npower transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)\ntechnology operating in the upper mid-band frequency range (7-24 GHz). The\nnear-field propagation achieved by gMIMO introduces unique opportunities for\nenergy-efficient, high-capacity communication systems that cater to the demands\nof 6G wireless networks. Exploiting spherical wave propagation, near-field\nSWIPT with gMIMO enables precise energy and data delivery, enhancing spectral\nefficiency through beamfocusing and massive spatial multiplexing. This paper\ndiscusses theoretical principles, design challenges, and enabling solutions,\nincluding advanced channel estimation techniques, precoding strategies, and\ndynamic array configurations such as sparse and modular arrays. Through\nanalytical insights and a case study, this paper demonstrates the feasibility\nof achieving optimized energy harvesting and data throughput in dense and\ndynamic environments. These findings contribute to advancing energy-autonomous\nInternet-of-Everything (IoE) deployments, smart factory networks, and other\nenergy-autonomous applications aligned with the goals of next-generation\nwireless technologies.", "comment": "7 pages, 5 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15670v1", "AI": {"title_translation": "近场SWIPT与gMIMO在中高频段的应用：机遇、挑战与未来发展", "tldr": "本文探讨了在中高频段将SWIPT与gMIMO结合，利用近场传播实现高效能、高容量的6G无线网络，并讨论了其机遇、挑战和解决方案。", "motivation": "探索将SWIPT与gMIMO技术在中高频段集成，以应对6G无线网络对节能、高容量通信系统的需求，并推动能量自主的物联网部署和智能工厂网络等应用。", "method": "本文通过探索SWIPT与gMIMO的集成，讨论了其理论原理、设计挑战和实现解决方案，包括先进的信道估计技术、预编码策略和动态阵列配置（如稀疏和模块化阵列）。通过分析性见解和案例研究，证明了在密集和动态环境中实现优化的能量收集和数据吞吐量的可行性。", "result": "结果表明，利用球形波传播，近场SWIPT与gMIMO能够实现精确的能量和数据传输，并通过波束聚焦和大规模空间复用提高频谱效率。通过分析性见解和案例研究，本文证明了在密集和动态环境中实现优化能量收集和数据吞吐量的可行性。", "conclusion": "近场SWIPT与gMIMO的结合为6G无线网络提供了实现高效能、高容量通信系统的独特机遇，并为能量自主的物联网部署、智能工厂网络及其他下一代无线技术应用贡献了重要进展。", "translation": "本文探讨了同时无线信息与功率传输（SWIPT）与在中高频段（7-24 GHz）运行的巨型多输入多输出（gMIMO）技术的集成。gMIMO实现的近场传播为满足6G无线网络需求的节能、高容量通信系统带来了独特机遇。利用球形波传播，近场SWIPT与gMIMO能够实现精确的能量和数据传输，通过波束聚焦和大规模空间复用提高频谱效率。本文讨论了理论原理、设计挑战和实现解决方案，包括先进的信道估计技术、预编码策略以及稀疏和模块化阵列等动态阵列配置。通过分析性见解和案例研究，本文证明了在密集和动态环境中实现优化能量收集和数据吞吐量的可行性。这些发现有助于推动能量自主物联网（IoE）部署、智能工厂网络以及其他符合下一代无线技术目标的能量自主应用。", "summary": "本文研究了在中高频段将SWIPT与gMIMO技术结合，以利用gMIMO带来的近场传播特性。这种结合为6G网络提供了节能、高容量的通信方案，通过精确的能量和数据传输、波束聚焦及大规模空间复用提升频谱效率。论文探讨了相关理论、挑战与解决方案，并通过案例研究验证了在复杂环境下优化能量收集和数据吞吐量的可行性，为能量自主的物联网和智能工厂等应用奠定了基础。", "keywords": "SWIPT, gMIMO, 近场通信, 能量收集, 6G网络", "comments": "这篇论文探讨了将SWIPT与gMIMO结合应用于中高频段的创新性，特别强调了近场传播在6G网络中的潜力。其重要性在于为未来能量自主的物联网和智能工厂等应用提供了新的技术路径，通过精确的能量和数据传输实现了效率提升。论文还全面讨论了技术挑战和解决方案，使其具有较强的实践指导意义。"}}
{"id": "2506.15131", "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "authors": ["Jing Yang Lee", "Kong-Aik Lee", "Woon-Seng Gan"], "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15131v1", "AI": {"title_translation": "使用大型语言模型建模开放域对话中的一对多属性", "tldr": "本文通过将开放域对话生成分解为多响应生成和基于偏好的选择两个任务，并引入o2mDial数据集，来建模大型语言模型中的一对多属性，显著提高了响应多样性和质量。", "motivation": "开放域对话具有一对多属性，即一个对话上下文可以有多个合适的响应。尽管先前的研究表明建模此属性可以提高响应多样性，但大多数现代基于大型语言模型的对话代理并未明确做到这一点。", "method": "本文通过将开放域对话（OD）生成分解为多响应生成（MRG）和基于偏好的选择（PS）两个关键任务来建模OD的一对多属性。为此，引入了一个名为o2mDial的对话语料库，该语料库专门设计用于捕获一对多属性，每个上下文包含多个合理的响应。利用o2mDial，提出了新的上下文学习和指令微调策略，以及针对MRG的新评估指标，同时为PS提供了一种基于模型的方法。", "result": "将所提出的两阶段框架应用于较小的LLM进行OD生成，可以增强整体响应多样性，同时保持上下文连贯性，并将响应质量提高多达90%，使其更接近大型模型的性能。", "conclusion": "通过将开放域对话生成分解为多响应生成和偏好选择，并引入专门的数据集和训练策略，可以有效地在较小的LLM中建模一对多属性，从而显著提高响应多样性和质量，使其性能接近大型模型。", "translation": "开放域对话（OD）展现出“一对多”（o2m）的特性，即对于单个对话上下文，存在多个合适的响应。尽管先前的研究表明，建模这一特性能够提升响应的多样性，但大多数现代基于大型语言模型的对话代理并未明确地这样做。在这项工作中，我们通过将OD生成分解为两个关键任务来建模大型语言模型中OD的o2m特性：多响应生成（MRG）和基于偏好的选择（PS）。这两个任务分别涉及为给定的对话上下文生成一组n个语义和词汇多样的高质量响应，然后根据人类偏好选择一个单一响应。为了促进MRG和PS，我们引入了o2mDial，这是一个专门设计用于捕捉o2m特性的对话语料库，其特点是每个上下文都包含多个合理的响应。利用o2mDial，我们提出了新的上下文学习和指令微调策略，以及针对MRG的新颖评估指标，同时为PS提供了一种基于模型的方法。实证结果表明，将所提出的两阶段框架应用于较小的LLM进行OD生成，可以增强整体响应多样性，同时保持上下文连贯性，并将响应质量提高多达90%，使其更接近大型模型的性能。", "summary": "本文提出了一种在大型语言模型（LLMs）中建模开放域对话（OD）一对多（o2m）属性的新方法。该方法将OD生成分解为多响应生成（MRG）和基于偏好的选择（PS）两个阶段。为了支持这一框架，研究者引入了o2mDial数据集，该数据集包含每个上下文的多个合理响应。通过利用o2mDial，提出了新的上下文学习、指令微调策略以及针对MRG的新评估指标和针对PS的基于模型的方法。实验结果表明，该两阶段框架能显著提高较小LLMs的响应多样性和质量，使其性能接近大型模型。", "keywords": "开放域对话, 一对多属性, 大型语言模型, 多响应生成, 偏好选择", "comments": "这项工作通过明确地建模开放域对话中的一对多属性，为提升LLM的对话能力提供了一个有价值的视角。其创新点在于将生成过程分解为多响应生成和偏好选择两个阶段，并构建了专门的数据集o2mDial来支持这一方法。对于小型LLM性能的显著提升，尤其是在多样性和质量方面的改进，显示了该方法的实用性和潜力，有助于缩小与大型模型之间的差距。"}}
{"id": "2506.14994", "title": "Optimal alignment of Lorentz orientation and generalization to matrix Lie groups", "authors": ["Congzhou M Sha"], "summary": "There exist elegant methods of aligning point clouds in $\\mathbb R^3$.\nUnfortunately, these methods rely on the positive definite property of the\nEuclidean metric, and do not easily extend to the indefinite Minkowski metric.\nIn this paper, we propose two solutions to the following problem: given\ninertial reference frames $A$ and $B$, and given (possibly noisy) measurements\nof a set of 4-vectors $\\{v_i\\}$ made in those reference frames with components\n$\\{v_{A,i}\\}$ and $\\{v_{B,i}\\}$, find the optimal Lorentz transformation\n$\\Lambda$ such that $\\Lambda v_{A,i}=v_{B,i}$. The method we outline is\nconceptually simple and easily extends to alignment problems in other matrix\nLie groups.", "comment": "8 pages, 2 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14994v1", "AI": {"title_translation": "洛伦兹方向的最佳对齐及其向矩阵李群的推广", "tldr": "提出了一种在洛伦兹群中对齐4-向量测量值的方法，并可推广到其他矩阵李群。", "motivation": "现有的欧几里得空间点云对齐方法依赖于正定度量，不适用于不定闵可夫斯基度量。", "method": "提出两种解决方案，用于找到最佳洛伦兹变换 $\\Lambda$ 来对齐在不同惯性参考系中测量的4-向量，使其满足 $\\Lambda v_{A,i}=v_{B,i}$。该方法概念简单且易于推广到其他矩阵李群。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在$\\mathbb R^3$中存在优雅的点云对齐方法。不幸的是，这些方法依赖于欧几里得度量的正定性，并且不容易扩展到不定闵可夫斯基度量。在本文中，我们针对以下问题提出了两种解决方案：给定惯性参考系A和B，以及在这些参考系中对一组4向量$\\{v_i\\}$进行的（可能存在噪声的）测量，其分量分别为$\\{v_{A,i}\\}$和$\\{v_{B,i}\\}$，找到最佳洛伦兹变换$\\Lambda$，使得$\\Lambda v_{A,i}=v_{B,i}$。我们概述的方法概念简单，并且易于扩展到其他矩阵李群中的对齐问题。", "summary": "本文解决了在洛伦兹群中对齐4-向量测量值的挑战，因为现有基于欧几里得度量的方法不适用。作者提出了两种概念简单的方法来找到最优的洛伦兹变换，以对齐不同惯性参考系中的4-向量测量值，并指出该方法易于推广到其他矩阵李群的对齐问题。", "keywords": "洛伦兹变换, 点云对齐, 矩阵李群, 闵可夫斯基度量, 4-向量", "comments": "本文的创新点在于解决了欧几里得空间点云对齐方法无法直接应用于洛伦兹群的问题，为处理闵可夫斯基空间中的数据对齐提供了新的思路。其方法的可推广性是其重要特点。"}}
{"id": "2506.15212", "title": "LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis", "authors": ["Madjid G. Tehrani", "Eldar Sultanow", "William J. Buchanan", "Mahkame Houmani", "Christel H. Djaha Fodja"], "summary": "With the rapid advancements in Natural Language Processing (NLP), large\nlanguage models (LLMs) like GPT-4 have gained significant traction in diverse\napplications, including security vulnerability scanning. This paper\ninvestigates the efficacy of GPT-4 in identifying software vulnerabilities\ncompared to traditional Static Application Security Testing (SAST) tools.\nDrawing from an array of security mistakes, our analysis underscores the potent\ncapabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that\nGPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in\ndetecting 32 types of exploitable vulnerabilities. This study also addresses\nthe potential security concerns surrounding LLMs, emphasising the imperative of\nsecurity by design/default and other security best practices for AI.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15212v1", "AI": {"title_translation": "LLM与SAST：关于检测GPT4高级数据分析中编码错误的技??术分析", "tldr": "本文对比了GPT-4（高级数据分析）与传统SAST工具在检测软件漏洞方面的效能，发现GPT-4在检测32种可利用漏洞时准确率达94%，优于SAST。", "motivation": "随着大型语言模型（LLM）在安全漏洞扫描等应用中的兴起，本文旨在调查GPT-4在识别软件漏洞方面相对于传统静态应用安全测试（SAST）工具的有效性。", "method": "本研究通过分析一系列安全错误，比较了GPT-4（高级数据分析）与传统SAST工具在检测软件漏洞方面的性能。", "result": "研究发现，GPT-4（高级数据分析）在检测32种可利用漏洞时，准确率达到94%，表现优于SAST工具。", "conclusion": "GPT-4在软件漏洞检测方面展现出强大的能力，并且在检测特定类型的漏洞时准确率显著高于传统SAST工具。此外，研究强调了LLM相关的潜在安全问题，并指出AI设计和默认安全以及其他安全最佳实践的重要性。", "translation": "随着自然语言处理（NLP）的快速发展，GPT-4等大型语言模型（LLM）在包括安全漏洞扫描在内的各种应用中获得了显著关注。本文调查了GPT-4在识别软件漏洞方面与传统静态应用安全测试（SAST）工具相比的有效性。通过分析一系列安全错误，我们的分析强调了GPT-4在LLM增强型漏洞扫描中的强大能力。我们发现GPT-4（高级数据分析）在检测32种可利用漏洞时，以94%的准确率超越了SAST。本研究还探讨了围绕LLM的潜在安全问题，强调了AI设计/默认安全以及其他安全最佳实践的必要性。", "summary": "本文对GPT-4（高级数据分析）在软件漏洞检测方面的效能进行了技术分析，并与传统静态应用安全测试（SAST）工具进行了比较。研究发现，GPT-4在检测32种可利用漏洞时的准确率高达94%，显著优于SAST。此外，论文还讨论了与LLM相关的潜在安全隐患，并强调了AI安全设计和最佳实践的重要性。", "keywords": "LLM, GPT-4, SAST, 漏洞检测, 软件安全", "comments": "这篇论文通过实证比较，突出了大型语言模型在软件安全领域，特别是在漏洞检测方面的巨大潜力。GPT-4在准确率上的显著优势表明了LLM在未来安全工具发展中的重要地位。同时，论文也关注到了LLM自身的安全问题，这对于AI安全领域是重要的提醒。"}}
{"id": "2506.15294", "title": "UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments", "authors": ["Jonas Lau", "Annie Tran"], "summary": "This paper discusses a popular UX research activity, feature prioritization,\nusing the User Experience Research Point of View (UXR PoV) Playbook framework.\nWe describe an application of multinomial logistic regression, frequently\nmarketed as MaxDiff, for prioritizing product features in consumer product\ndevelopment. It addresses challenges of traditional surveying techniques. We\npropose a solution using MaxDiff to generate a reliable preference list with a\nreasonable sample size. We also adapt the MaxDiff method to reduce the number\nof survey responses in half, making it less tedious from the survey takers'\nperspective. We present a case study using the adapted MaxDiff method for\ntablet feature prioritization research involving users with disabilities.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15294v1", "AI": {"title_translation": "用户体验研究视角下的产品功能优先级排序：在数百万工程投入之前的考量", "tldr": "本文讨论了使用MaxDiff方法进行产品功能优先级排序，解决了传统调查的挑战，并提出了一个改进的MaxDiff方法来减少调查工作量，通过一个案例研究进行了验证。", "motivation": "传统调查技术在产品功能优先级排序中存在挑战，需要一种能生成可靠偏好列表且样本量合理的方法。", "method": "本文描述了多项逻辑回归（MaxDiff）的应用，用于消费者产品开发中的功能优先级排序。此外，还提出了一种改进的MaxDiff方法，将调查响应数量减少一半，以减轻受访者的负担。通过一个针对残疾用户平板功能优先级研究的案例研究验证了该方法。", "result": "使用MaxDiff方法能够生成可靠的偏好列表，且样本量合理。改进后的MaxDiff方法成功将调查响应数量减少了一半，从而降低了受访者的疲劳度。该方法通过一个涉及残疾用户的平板功能优先级研究案例得到了验证。", "conclusion": "Not mentioned in abstract", "translation": "本文讨论了一项流行的用户体验研究活动——功能优先级排序，并使用了用户体验研究视角（UXR PoV）手册框架。我们描述了多项逻辑回归（通常被称为MaxDiff）在消费产品开发中用于产品功能优先级排序的应用。它解决了传统调查技术的挑战。我们提出了一种使用MaxDiff生成可靠偏好列表且样本量合理的解决方案。我们还改进了MaxDiff方法，将调查响应数量减少了一半，从而从受访者的角度来看，减轻了调查的繁琐性。我们提出了一个案例研究，使用改进后的MaxDiff方法进行针对残疾用户的平板功能优先级研究。", "summary": "本文探讨了在产品开发中进行功能优先级排序的UX研究活动，并引入了UXR PoV框架。文章详细介绍了MaxDiff（多项逻辑回归）的应用，旨在克服传统调查方法的局限性。研究提出了一种优化的MaxDiff方案，该方案不仅能以合理的样本量生成可靠的功能偏好列表，还能将受访者的调查负担减半。文章通过一项针对残疾用户的平板功能优先级研究案例，验证了改进后的MaxDiff方法的有效性。", "keywords": "功能优先级排序, MaxDiff, 用户体验研究, 多项逻辑回归, 调查方法", "comments": "本文创新性地将MaxDiff方法应用于产品功能优先级排序，并针对传统调查的痛点进行了改进，特别是通过减少调查量提升了用户体验。其在“数百万工程投入”前的应用，强调了在早期阶段进行精准优先级排序的重要性，这对于资源优化和避免后期返工具有重要价值。案例研究涉及残疾用户，也体现了研究的包容性。"}}
{"id": "2506.15126", "title": "VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments", "authors": ["Bingbing Zhang", "Huan Yin", "Shuo Liu", "Fumin Zhang", "Wen Xu"], "summary": "In this study, we present a novel simultaneous localization and mapping\n(SLAM) system, VIMS, designed for underwater navigation. Conventional\nvisual-inertial state estimators encounter significant practical challenges in\nperceptually degraded underwater environments, particularly in scale estimation\nand loop closing. To address these issues, we first propose leveraging a\nlow-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a\nhigh-sampling-rate magnetometer for place recognition by utilizing magnetic\nsignatures generated by an economical magnetic field coil. Building on this, a\nhierarchical scheme is developed for visual-magnetic place recognition,\nenabling robust loop closure. Furthermore, VIMS achieves a balance between\nlocal feature tracking and descriptor-based loop closing, avoiding additional\ncomputational burden on the front end. Experimental results highlight the\nefficacy of the proposed VIMS, demonstrating significant improvements in both\nthe robustness and accuracy of state estimation within underwater environments.", "comment": "This work has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15126v1", "AI": {"title_translation": "VIMS：一种水下环境中的视觉-惯性-磁-声纳同步定位与建图系统", "tldr": "VIMS是一个针对水下导航的SLAM系统，通过整合低成本单波束声纳改善尺度估计，并利用磁力计进行地点识别和鲁棒的闭环检测，显著提高了水下状态估计的鲁棒性和精度。", "motivation": "传统视觉-惯性状态估计器在感知退化的水下环境中面临显著的实际挑战，特别是在尺度估计和闭环检测方面。", "method": "VIMS系统首先利用低成本单波束声纳改进尺度估计。然后，通过经济的磁场线圈产生的磁特征，整合高采样率磁力计进行地点识别。在此基础上，开发了一种用于视觉-磁地点识别的分层方案，实现了鲁棒的闭环检测。此外，VIMS在局部特征跟踪和基于描述符的闭环检测之间取得了平衡，避免了前端的额外计算负担。", "result": "实验结果突出显示了所提出的VIMS的有效性，证明了水下环境中状态估计的鲁棒性和精度均有显著改善。", "conclusion": "VIMS系统通过结合声纳和磁力计，有效解决了水下视觉-惯性SLAM中的尺度估计和闭环检测挑战，显著提高了水下导航的性能。", "translation": "在这项研究中，我们提出了一种新颖的同步定位与建图（SLAM）系统VIMS，专为水下导航设计。传统的视觉-惯性状态估计器在感知退化的水下环境中遇到显著的实际挑战，特别是在尺度估计和闭环方面。为了解决这些问题，我们首先提出利用低成本单波束声纳来改善尺度估计。然后，VIMS通过利用经济型磁场线圈产生的磁特征，整合高采样率磁力计进行地点识别。在此基础上，开发了一种用于视觉-磁地点识别的分层方案，实现了鲁棒的闭环。此外，VIMS在局部特征跟踪和基于描述符的闭环之间取得了平衡，避免了前端的额外计算负担。实验结果突出显示了所提出的VIMS的有效性，证明了水下环境中状态估计的鲁棒性和精度均有显著改善。", "summary": "本研究提出了一种名为VIMS的新型水下SLAM系统，旨在解决传统视觉-惯性估计器在水下环境中的尺度估计和闭环挑战。VIMS通过整合低成本单波束声纳以改善尺度估计，并利用高采样率磁力计结合磁场线圈产生的磁特征进行地点识别，进而构建分层视觉-磁地点识别方案实现鲁棒闭环。该系统在保持前端计算效率的同时，平衡了局部特征跟踪和基于描述符的闭环检测。实验证明VIMS显著提升了水下状态估计的鲁棒性和精度。", "keywords": "水下SLAM, 视觉-惯性导航, 声纳, 磁力计, 闭环检测", "comments": "该论文的创新之处在于其多传感器融合方法，特别是将低成本声纳和磁力计集成到视觉-惯性SLAM框架中，以解决水下环境特有的挑战，如尺度漂移和闭环困难。这种方法为水下导航提供了一个更鲁棒和精确的解决方案，具有重要的实际应用价值。"}}
{"id": "2506.15636", "title": "Quantum Error Correction Exploiting Degeneracy to Approach the Hashing Bound", "authors": ["Kenta Kasai"], "summary": "Quantum error correction is essential for realizing scalable quantum\ncomputation. Among various approaches, low-density parity-check codes over\nhigher-order Galois fields have shown promising performance due to their\nstructured sparsity and compatibility with iterative decoding algorithms whose\ncomputational complexity scales linearly with the number of physical qubits. In\nthis work, we demonstrate that explicitly exploiting the degeneracy of quantum\nerrors can significantly enhance the decoding performance. Simulation results\nover the depolarizing channel indicate that the proposed method, at a coding\nrate of 1/3, achieves a frame error rate as low as $10^{-4}$ at a physical\nerror rate of 9.45% for a code with 104,000 logical qubits and 312,000 physical\nqubits, approaching the quantum hashing bound. These findings highlight the\ncritical role of degeneracy in closing the gap to the fundamental limits of\nquantum error correction.", "comment": "This work has been submitted to a journal for possible publication", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.15636v1", "AI": {"title_translation": "利用简并性逼近哈希界限的量子纠错", "tldr": "本文通过利用量子错误的简并性，显著提升了量子纠错的解码性能，在退极化信道上逼近了量子哈希界限。", "motivation": "实现可扩展的量子计算需要量子纠错。高阶伽罗瓦域上的低密度奇偶校验码因其结构稀疏性和与迭代解码算法的兼容性而显示出良好前景，但仍需进一步提升性能。", "method": "本文提出并演示了通过显式利用量子错误的简并性来显著增强解码性能的方法。", "result": "在编码率为1/3时，所提出的方法在退极化信道上，对于一个包含104,000个逻辑量子比特和312,000个物理量子比特的码，在9.45%的物理错误率下实现了低至$10^{-4}$的帧错误率，性能逼近量子哈希界限。", "conclusion": "研究结果强调了简并性在缩小与量子纠错基本极限差距方面的关键作用。", "translation": "量子纠错对于实现可扩展的量子计算至关重要。在各种方法中，高阶伽罗瓦域上的低密度奇偶校验码因其结构稀疏性和与迭代解码算法的兼容性而显示出良好的前景，其计算复杂度与物理量子比特数量呈线性关系。在这项工作中，我们证明了明确利用量子错误的简并性可以显著提高解码性能。在退极化信道上的仿真结果表明，所提出的方法在编码率为1/3时，对于一个包含104,000个逻辑量子比特和312,000个物理量子比特的码，在9.45%的物理错误率下实现了低至$10^{-4}$的帧错误率，逼近了量子哈希界限。这些发现突出了简并性在缩小与量子纠错基本极限差距方面的关键作用。", "summary": "本研究提出了一种量子纠错方法，通过显式利用量子错误的简并性来提升解码性能。该方法在低密度奇偶校验码的基础上，针对退极化信道进行了仿真，结果显示在特定编码率和物理错误率下，能够达到非常低的帧错误率，并逼近量子哈希界限，证明了简并性在量子纠错中的重要作用。", "keywords": "量子纠错, 简并性, 哈希界限, 低密度奇偶校验码, 解码性能", "comments": "该论文的创新点在于明确提出并利用了量子错误的简并性来提升量子纠错的性能，这为逼近量子纠错的理论极限提供了一条新的途径。其在大型码字上的仿真结果展示了该方法的潜力，对于实现容错量子计算具有重要意义。"}}
{"id": "2506.14911", "title": "Event-Driven Online Vertical Federated Learning", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "summary": "Online learning is more adaptable to real-world scenarios in Vertical\nFederated Learning (VFL) compared to offline learning. However, integrating\nonline learning into VFL presents challenges due to the unique nature of VFL,\nwhere clients possess non-intersecting feature sets for the same sample. In\nreal-world scenarios, the clients may not receive data streaming for the\ndisjoint features for the same entity synchronously. Instead, the data are\ntypically generated by an \\emph{event} relevant to only a subset of clients. We\nare the first to identify these challenges in online VFL, which have been\noverlooked by previous research. To address these challenges, we proposed an\nevent-driven online VFL framework. In this framework, only a subset of clients\nwere activated during each event, while the remaining clients passively\ncollaborated in the learning process. Furthermore, we incorporated\n\\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by\nonline learning problems with non-convex models within a non-stationary\nenvironment. We conducted a comprehensive regret analysis of our proposed\nframework, specifically examining the DLR under non-convex conditions with\nevent-driven online VFL. Extensive experiments demonstrated that our proposed\nframework was more stable than the existing online VFL framework under\nnon-stationary data conditions while also significantly reducing communication\nand computation costs.", "comment": "Published as a conference paper at ICLR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14911v1", "AI": {"title_translation": "事件驱动的在线垂直联邦学习", "tldr": "本文提出了一种事件驱动的在线垂直联邦学习框架，以解决现有在线VFL中数据异步和非凸模型下的挑战，并显著降低了通信和计算成本。", "motivation": "在线学习在垂直联邦学习（VFL）中更具适应性，但由于VFL中客户端拥有不相交的特征集，且实际场景中数据流通常由仅与部分客户端相关的“事件”异步生成，这给在线VFL带来了挑战。之前的研究忽略了这些挑战，本文旨在解决这些问题。", "method": "本文提出了一个事件驱动的在线VFL框架。在该框架中，每次事件发生时只激活部分客户端，其余客户端被动协作。此外，为了解决非平稳环境下非凸模型在线学习问题带来的挑战，本文将动态局部遗憾（Dynamic Local Regret, DLR）引入VFL，并对DLR在事件驱动的在线VFL非凸条件下的情况进行了全面的遗憾分析。", "result": "实验证明，本文提出的框架在非平稳数据条件下比现有的在线VFL框架更稳定，并且显著降低了通信和计算成本。", "conclusion": "本文首次识别了在线VFL中的挑战，并提出了一个事件驱动的在线VFL框架，该框架通过引入动态局部遗憾和部分客户端激活机制，有效解决了异步数据流和非凸模型在线学习的问题，并在稳定性、通信和计算成本方面表现出优越性。", "translation": "与离线学习相比，在线学习在垂直联邦学习（VFL）中更能适应现实场景。然而，由于VFL的独特性质，即客户端拥有相同样本的不相交特征集，将在线学习集成到VFL中带来了挑战。在现实场景中，客户端可能不会同步接收同一实体不相交特征的数据流。相反，数据通常由仅与部分客户端相关的“事件”生成。我们首次识别了在线VFL中的这些挑战，而这些挑战已被之前的研究忽视。为了解决这些挑战，我们提出了一个事件驱动的在线VFL框架。在这个框架中，每次事件发生时只激活部分客户端，而其余客户端被动地参与学习过程。此外，我们将动态局部遗憾（DLR）引入VFL，以解决非平稳环境下非凸模型在线学习问题所带来的挑战。我们对我们提出的框架进行了全面的遗憾分析，特别是研究了事件驱动的在线VFL在非凸条件下的DLR。广泛的实验表明，我们提出的框架在非平稳数据条件下比现有的在线VFL框架更稳定，同时也显著降低了通信和计算成本。", "summary": "本文针对在线垂直联邦学习（VFL）中数据异步生成和非凸模型学习的挑战，提出了一个事件驱动的在线VFL框架。该框架通过在事件发生时仅激活部分客户端并引入动态局部遗憾（DLR）来处理非平稳环境。实验结果表明，该框架在稳定性方面优于现有方法，并能有效降低通信和计算成本。", "keywords": "垂直联邦学习, 在线学习, 事件驱动, 动态局部遗憾, 非平稳环境", "comments": "本文的创新点在于首次明确指出了在线VFL中数据异步性和事件驱动的挑战，并提出了一个新颖的事件驱动框架来解决这些问题。通过引入动态局部遗憾和部分客户端激活机制，该方法在理论分析和实验验证上都展示了其在非平稳数据条件下优于现有在线VFL框架的性能，并有效降低了资源消耗，具有重要的实际应用价值。"}}
{"id": "2506.14846", "title": "Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach", "authors": ["Shreyas Rajeev", "B Sathish Babu"], "summary": "Kernel size selection in Convolutional Neural Networks (CNNs) is a critical\nbut often overlooked design decision that affects receptive field, feature\nextraction, computational cost, and model accuracy. This paper proposes the\nBest Kernel Size Estimation Function (BKSEF), a mathematically grounded and\nempirically validated framework for optimal, layer-wise kernel size\ndetermination. BKSEF balances information gain, computational efficiency, and\naccuracy improvements by integrating principles from information theory, signal\nprocessing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100,\nImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided\narchitectures achieve up to 3.1 percent accuracy improvement and 42.8 percent\nreduction in FLOPs compared to traditional models using uniform 3x3 kernels.\nTwo real-world case studies further validate the approach: one for medical\nimage classification in a cloud-based setup, and another for traffic sign\nrecognition on edge devices. The former achieved enhanced interpretability and\naccuracy, while the latter reduced latency and model size significantly, with\nminimal accuracy trade-off. These results show that kernel size can be an\nactive, optimizable parameter rather than a fixed heuristic. BKSEF provides\npractical heuristics and theoretical support for researchers and developers\nseeking efficient and application-aware CNN designs. It is suitable for\nintegration into neural architecture search pipelines and real-time systems,\noffering a new perspective on CNN optimization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14846v1", "AI": {"title_translation": "在卷积神经网络中寻找最优核大小和维度：一种架构优化方法", "tldr": "本文提出了最佳核大小估计函数（BKSEF），一个用于优化卷积神经网络（CNN）中核大小的框架。BKSEF通过平衡信息增益、计算效率和精度，实现了高达3.1%的精度提升和42.8%的FLOPs减少，并在多个数据集和实际案例中得到验证。", "motivation": "卷积神经网络（CNN）中的核大小选择是一个关键但经常被忽视的设计决策，它影响着感受野、特征提取、计算成本和模型精度。本研究旨在解决核大小选择问题，以实现更优的CNN架构。", "method": "本文提出了最佳核大小估计函数（BKSEF），这是一个基于数学原理并经过实证验证的框架，用于最佳的逐层核大小确定。BKSEF通过整合信息论、信号处理和学习理论的原理，平衡信息增益、计算效率和精度提升。", "result": "与使用统一3x3核的传统模型相比，BKSEF引导的架构在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上实现了高达3.1%的精度提升和42.8%的FLOPs减少。在医学图像分类和交通标志识别的真实世界案例研究中，BKSEF分别实现了增强的可解释性、精度以及显著降低的延迟和模型大小，同时保持最小的精度损失。", "conclusion": "核大小可以是一个主动的、可优化的参数，而不是一个固定的启发式方法。BKSEF为寻求高效和应用感知型CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持，并适用于集成到神经架构搜索流程和实时系统中。", "translation": "卷积神经网络（CNN）中的核大小选择是一个关键但经常被忽视的设计决策，它会影响感受野、特征提取、计算成本和模型精度。本文提出了最佳核大小估计函数（BKSEF），这是一个基于数学原理并经过实证验证的框架，用于最佳的逐层核大小确定。BKSEF通过整合信息论、信号处理和学习理论的原理，平衡信息增益、计算效率和精度提升。在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上进行的大量实验表明，与使用统一3x3核的传统模型相比，BKSEF引导的架构实现了高达3.1%的精度提升和42.8%的FLOPs减少。两个真实世界的案例研究进一步验证了该方法：一个用于基于云的医学图像分类，另一个用于边缘设备上的交通标志识别。前者实现了增强的可解释性和精度，而后者显著降低了延迟和模型大小，且精度损失最小。这些结果表明，核大小可以是一个主动的、可优化的参数，而不是一个固定的启发式方法。BKSEF为寻求高效和应用感知型CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持。它适用于集成到神经架构搜索流程和实时系统中，为CNN优化提供了新的视角。", "summary": "本文提出了最佳核大小估计函数（BKSEF），一个用于优化卷积神经网络（CNN）中核大小的新颖框架。BKSEF融合了信息论、信号处理和学习理论的原理，以确定最优的逐层核大小，从而平衡信息增益、计算效率和精度。在多个数据集上的实验表明，BKSEF引导的CNNs在精度和计算成本方面均优于传统模型。此外，在医学图像分类和交通标志识别的实际应用中，该方法也展现出增强的性能和效率。这项研究强调了核大小作为一个可优化参数的重要性，为CNN设计和优化提供了新视角。", "keywords": "卷积神经网络, 核大小优化, 架构优化, BKSEF, 神经架构搜索", "comments": "该论文的创新之处在于将核大小视为一个可优化参数而非固定启发式方法，并提出了一个基于数学原理的框架（BKSEF）来实现这一点。其重要性在于提升了CNN的效率和精度，并在各种实际应用中，尤其是在资源受限的环境下，具有显著的实用价值。"}}
{"id": "2506.15677", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15677v1", "AI": {"title_translation": "具身网络智能体：弥合物理-数字领域以实现集成智能体智能", "tldr": "引入具身网络智能体范式和基准，以整合物理和数字智能，解决当前AI智能体孤立的问题。", "motivation": "当前的AI智能体要么专注于数字信息推理，要么专注于物理世界交互，很少能两者兼顾。这种分离限制了它们解决需要集成物理和数字智能的任务的能力，例如根据在线食谱烹饪或使用网络知识导航。", "method": "引入了“具身网络智能体”这一新范式，并开发了“具身网络智能体任务环境”——一个统一的模拟平台，该平台紧密集成逼真的3D室内外环境与功能性网络界面。在此基础上，构建并发布了“具身网络智能体基准”，其中包含烹饪、导航、购物、旅游和地理定位等多样化任务，这些任务都要求在物理和数字领域进行协调推理。", "result": "实验结果揭示了最先进的AI系统与人类能力之间存在显著的性能差距。", "conclusion": "研究结果表明，在具身认知和网络规模知识访问的交叉点上，既存在挑战也存在机遇。", "translation": "当前的AI智能体大多是孤立的——它们要么检索并推理海量的在线数字信息和知识；要么通过具身感知、规划和行动与物理世界交互——但很少两者兼顾。这种分离限制了它们解决需要集成物理和数字智能的任务的能力，例如根据在线食谱烹饪、使用动态地图数据导航或使用网络知识解释现实世界地标。我们引入了具身网络智能体，这是一种新颖的AI智能体范式，它能够流畅地连接具身和网络规模推理。为了实现这一概念，我们首先开发了具身网络智能体任务环境，这是一个统一的模拟平台，紧密集成逼真的3D室内外环境与功能性网络界面。在此平台基础上，我们构建并发布了具身网络智能体基准，该基准包含一系列多样化的任务，包括烹饪、导航、购物、旅游和地理定位——所有这些都要求在物理和数字领域进行协调推理，以系统评估跨领域智能。实验结果揭示了最先进的AI系统与人类能力之间存在显著的性能差距，这在具身认知和网络规模知识访问的交叉点上既带来了挑战也提供了机遇。所有数据集、代码和网站均可在我们的项目页面 https://embodied-web-agent.github.io/ 公开获取。", "summary": "该论文提出了“具身网络智能体”这一新范式，旨在整合物理具身和网络规模推理，以克服当前AI智能体在处理需要跨领域智能任务时的局限性。为实现这一目标，作者开发了一个统一的模拟平台“具身网络智能体任务环境”，并发布了“具身网络智能体基准”，其中包含多项需要结合物理和数字智能的任务。实验结果显示，当前AI系统与人类能力之间存在显著差距，这揭示了具身认知与网络知识访问交叉领域中的挑战与机遇。", "keywords": "具身AI, 网络智能体, 集成智能, 模拟平台, 基准", "comments": "该论文通过提出具身网络智能体范式及其配套的统一模拟平台和综合基准，解决了当前AI智能体在物理和数字领域能力分离的关键限制。这种创新性方法和所提供的评估工具对于推动具身AI和网络规模知识整合领域的发展具有重要意义，为开发更具通用能力的智能体提供了具体的框架。"}}
{"id": "2506.14802", "title": "ss-Mamba: Semantic-Spline Selective State-Space Model", "authors": ["Zuochen Ye"], "summary": "We propose ss-Mamba, a novel foundation model that enhances time series\nforecasting by integrating semantic-aware embeddings and adaptive spline-based\ntemporal encoding within a selective state-space modeling framework. Building\nupon the recent success of Transformer architectures, ss-Mamba adopts the Mamba\nselective state space model as an efficient alternative that achieves\ncomparable performance while significantly reducing computational complexity\nfrom quadratic to linear time. Semantic index embeddings, initialized from\npretrained language models, allow effective generalization to previously unseen\nseries through meaningful semantic priors. Additionally, spline-based\nKolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex\nseasonalities and non-stationary temporal effects, providing a powerful\nenhancement over conventional temporal feature encodings. Extensive\nexperimental evaluations confirm that ss-Mamba delivers superior accuracy,\nrobustness, and interpretability, demonstrating its capability as a versatile\nand computationally efficient alternative to traditional Transformer-based\nmodels in time-series forecasting.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14802v1", "AI": {"title_translation": "ss-Mamba: 语义样条选择性状态空间模型", "tldr": "ss-Mamba是一个新的时间序列预测基础模型，它结合了语义嵌入和样条基时间编码，利用Mamba的选择性状态空间模型，实现了与Transformer相当的性能，但计算复杂度从二次降至线性，并提高了准确性、鲁棒性和可解释性。", "motivation": "传统Transformer模型在时间序列预测中计算复杂度高（二次方），且在处理复杂季节性和非平稳时间效应方面可能存在局限。该研究旨在开发一种更高效、更具泛化能力和可解释性的时间序列预测模型。", "method": "提出ss-Mamba模型，这是一个基于Mamba选择性状态空间模型的时序预测基础模型。它结合了：1. 语义感知嵌入（Semantic-aware embeddings）：从预训练语言模型初始化，用于有效泛化到未见序列。2. 自适应样条基时间编码（Adaptive spline-based temporal encoding）：利用样条基Kolmogorov-Arnold Networks (KAN) 动态且可解释地捕捉复杂季节性和非平稳时间效应。该模型旨在将计算复杂度从二次降低到线性。", "result": "广泛的实验评估表明，ss-Mamba在时间序列预测中提供了卓越的准确性、鲁棒性和可解释性。", "conclusion": "ss-Mamba是一个多功能且计算高效的替代方案，可替代传统基于Transformer的模型进行时间序列预测。", "translation": "我们提出了ss-Mamba，一个新颖的基础模型，它通过在选择性状态空间建模框架内集成语义感知嵌入和自适应样条基时间编码来增强时间序列预测。基于Transformer架构的最新成功，ss-Mamba采用Mamba选择性状态空间模型作为一种高效的替代方案，它实现了可比的性能，同时将计算复杂度从二次方显著降低到线性时间。从预训练语言模型初始化的语义索引嵌入，通过有意义的语义先验，允许有效地泛化到以前未见的序列。此外，基于样条的Kolmogorov-Arnold Networks (KAN) 动态且可解释地捕捉复杂的季节性和非平稳时间效应，为传统时间特征编码提供了强大的增强。广泛的实验评估证实，ss-Mamba提供了卓越的准确性、鲁棒性和可解释性，展示了其作为时间序列预测中传统基于Transformer模型的通用且计算高效的替代方案的能力。", "summary": "ss-Mamba是一个新颖的时间序列预测基础模型，它将语义感知嵌入和自适应样条基时间编码集成到Mamba选择性状态空间模型中。该模型旨在克服Transformer架构的二次计算复杂度，实现线性时间复杂度的同时保持可比性能。通过利用预训练语言模型的语义嵌入，ss-Mamba能够泛化到未见序列，而样条基KAN则增强了对复杂时间效应的捕获和可解释性。实验结果表明，ss-Mamba在准确性、鲁棒性和可解释性方面均优于传统Transformer模型。", "keywords": "时间序列预测, ss-Mamba, 选择性状态空间模型, 语义嵌入, 样条基KAN", "comments": "ss-Mamba的创新之处在于将Mamba选择性状态空间模型与语义感知嵌入和样条基KAN相结合，解决了时间序列预测中效率、泛化能力和可解释性的关键挑战。通过将计算复杂度从二次降低到线性，它为大规模时间序列预测提供了一个有前景的、更高效的替代方案，同时通过语义先验和可解释的时间编码提高了模型性能和泛用性。"}}
{"id": "2506.15665", "title": "A Data-Integrated Framework for Learning Fractional-Order Nonlinear Dynamical Systems", "authors": ["Bahram Yaghooti", "Chengyu Li", "Bruno Sinopoli"], "summary": "This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15665v1", "AI": {"title_translation": "用于学习分数阶非线性动力系统的数据集成框架", "tldr": "本文提出了一个数据集成框架，用于学习分数阶非线性动力系统在离散时间和连续时间设置下的动态。该框架通过设计实验生成数据集，然后利用这些数据和分数阶系统的记忆依赖性来估计分数阶并重建漂移和控制向量场。实验结果验证了该框架的有效性，并表明分数阶模型能更好地捕捉长程依赖性。", "motivation": "本文旨在提出一个数据集成框架，用于有效学习离散时间和连续时间设置下的分数阶非线性动力系统。", "method": "该框架包含两个主要步骤：首先，设计输入输出实验以生成学习系统动态所需的必要数据集（包括分数阶、漂移向量场和控制向量场）。其次，利用这些数据集以及分数阶系统的记忆依赖性来估计系统的分数阶，并使用正交基函数重建漂移和控制向量场。", "result": "将所提出的算法应用于四个基准分数阶系统，结果证实了该框架在准确学习系统动态方面的有效性。数值比较表明，分数阶模型能更好地捕捉长程依赖性，突出了整数阶表示的局限性。", "conclusion": "所提出的数据集成框架能够有效地学习分数阶非线性动力系统。与整数阶模型相比，分数阶模型在捕捉长程依赖性方面表现出优越性。", "translation": "本文提出了一个数据集成框架，用于学习离散时间域和连续时间域中的分数阶非线性系统动力学。所提出的框架包含两个主要步骤。在第一步中，设计输入输出实验以生成学习系统动力学所需的必要数据集，包括分数阶、漂移向量场和控制向量场。在第二步中，这些数据集以及分数阶系统的记忆依赖性被用于估计系统的分数阶。然后使用正交基函数重建漂移和控制向量场。为了验证所提出的方法，将该算法应用于四个基准分数阶系统。结果证实了所提出框架在准确学习系统动力学方面的有效性。最后，使用相同的数据集来学习等效的整数阶模型。数值比较表明，分数阶模型能更好地捕捉长程依赖性，突出了整数阶表示的局限性。", "summary": "本文介绍了一个数据集成框架，旨在学习离散和连续时间下的分数阶非线性动力系统。该框架通过设计实验获取数据集，然后利用这些数据和分数阶系统的记忆特性来估计分数阶，并重建系统的漂移和控制向量场。通过对四个基准系统的验证，证实了该框架在学习系统动态方面的准确性和有效性。研究还发现，与整数阶模型相比，分数阶模型在捕捉长程依赖性方面表现出显著优势。", "keywords": "分数阶系统, 非线性动力学, 数据集成, 系统辨识, 长程依赖", "comments": "该论文提出了一种新颖的数据集成框架，用于学习复杂的非线性分数阶动力系统，这在理论和应用上都具有重要意义。其创新之处在于结合了数据驱动方法和分数阶系统的固有记忆特性。此外，通过与整数阶模型的比较，明确指出了分数阶模型在处理长程依赖性方面的优越性，这对于系统建模和控制领域具有重要的启发作用。"}}
{"id": "2506.15489", "title": "Advanced cervical cancer classification: enhancing pap smear images with hybrid PMD Filter-CLAHE", "authors": ["Ach Khozaimi", "Isnani Darti", "Syaiful Anam", "Wuryansari Muharini Kusumawinahyu"], "summary": "Cervical cancer remains a significant health problem, especially in\ndeveloping countries. Early detection is critical for effective treatment.\nConvolutional neural networks (CNN) have shown promise in automated cervical\ncancer screening, but their performance depends on Pap smear image quality.\nThis study investigates the impact of various image preprocessing techniques on\nCNN performance for cervical cancer classification using the SIPaKMeD dataset.\nThree preprocessing techniques were evaluated: perona-malik diffusion (PMD)\nfilter for noise reduction, contrast-limited adaptive histogram equalization\n(CLAHE) for image contrast enhancement, and the proposed hybrid PMD\nfilter-CLAHE approach. The enhanced image datasets were evaluated on pretrained\nmodels, such as ResNet-34, ResNet-50, SqueezeNet-1.0, MobileNet-V2,\nEfficientNet-B0, EfficientNet-B1, DenseNet-121, and DenseNet-201. The results\nshow that hybrid preprocessing PMD filter-CLAHE can improve the Pap smear image\nquality and CNN architecture performance compared to the original images. The\nmaximum metric improvements are 13.62% for accuracy, 10.04% for precision,\n13.08% for recall, and 14.34% for F1-score. The proposed hybrid PMD\nfilter-CLAHE technique offers a new perspective in improving cervical cancer\nclassification performance using CNN architectures.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15489v1", "AI": {"title_translation": "宫颈癌高级分类：使用混合PMD滤波器-CLAHE增强巴氏涂片图像", "tldr": "混合PMD滤波器-CLAHE图像预处理显著提高了基于CNN的宫颈癌巴氏涂片图像分类性能。", "motivation": "宫颈癌是严重的全球健康问题，早期检测至关重要。虽然卷积神经网络（CNN）在自动宫颈癌筛查中前景广阔，但其性能受巴氏涂片图像质量影响。", "method": "本研究评估了三种图像预处理技术（Perona-Malik扩散（PMD）滤波器、对比度受限自适应直方图均衡化（CLAHE）以及提出的混合PMD滤波器-CLAHE方法）对CNN宫颈癌分类性能的影响。增强后的图像数据集在ResNet-34、ResNet-50、SqueezeNet-1.0、MobileNet-V2、EfficientNet-B0、EfficientNet-B1、DenseNet-121和DenseNet-201等预训练模型上进行了评估，使用SIPaKMeD数据集。", "result": "结果表明，混合PMD滤波器-CLAHE预处理方法能提高巴氏涂片图像质量和CNN架构性能，相比原始图像，准确率、精确率、召回率和F1分数分别最高提升13.62%、10.04%、13.08%和14.34%。", "conclusion": "提出的混合PMD滤波器-CLAHE技术为使用CNN架构改善宫颈癌分类性能提供了新视角。", "translation": "宫颈癌仍然是一个重要的健康问题，特别是在发展中国家。早期检测对于有效治疗至关重要。卷积神经网络（CNN）在自动化宫颈癌筛查中显示出前景，但其性能取决于巴氏涂片图像质量。本研究调查了各种图像预处理技术对使用SIPaKMeD数据集进行宫颈癌分类的CNN性能的影响。评估了三种预处理技术：用于降噪的Perona-Malik扩散（PMD）滤波器、用于图像对比度增强的对比度受限自适应直方图均衡化（CLAHE），以及提出的混合PMD滤波器-CLAHE方法。增强后的图像数据集在ResNet-34、ResNet-50、SqueezeNet-1.0、MobileNet-V2、EfficientNet-B0、EfficientNet-B1、DenseNet-121和DenseNet-201等预训练模型上进行了评估。结果表明，与原始图像相比，混合预处理PMD滤波器-CLAHE可以提高巴氏涂片图像质量和CNN架构性能。最大指标提升分别为：准确率13.62%，精确率10.04%，召回率13.08%，F1分数14.34%。提出的混合PMD滤波器-CLAHE技术为使用CNN架构提高宫颈癌分类性能提供了新视角。", "summary": "本研究旨在提高基于卷积神经网络（CNN）的宫颈癌分类性能，通过改善巴氏涂片图像质量。研究评估了PMD滤波器、CLAHE以及结合两者的混合PMD滤波器-CLAHE三种图像预处理技术。结果显示，混合PMD滤波器-CLAHE方法能显著提升图像质量和CNN模型（如ResNet、EfficientNet等）的分类性能，各项指标最高提升超过10%，为宫颈癌的早期自动化筛查提供了新的有效途径。", "keywords": "宫颈癌分类, 巴氏涂片, 图像预处理, CNN, PMD-CLAHE", "comments": "该论文通过引入混合PMD滤波器-CLAHE图像预处理方法，有效地解决了巴氏涂片图像质量对CNN分类性能的限制，其创新性在于结合了降噪和对比度增强的优点。研究结果量化地展示了预处理对深度学习模型性能的显著提升，强调了数据质量在医疗图像分析中的重要性。"}}
{"id": "2506.14795", "title": "Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations", "authors": ["Batuhan Hangun", "Emine Akpinar", "Oguz Altun", "Onder Eyecioglu"], "summary": "Quantum Machine Learning (QML) is an emerging field at the intersection of\nquantum computing and machine learning, aiming to enhance classical machine\nlearning methods by leveraging quantum mechanics principles such as\nentanglement and superposition. However, skepticism persists regarding the\npractical advantages of QML, mainly due to the current limitations of noisy\nintermediate-scale quantum (NISQ) devices. This study addresses these concerns\nby extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired\ncounterparts of Artificial Neural Networks (ANNs), demonstrating their\neffectiveness compared to classical methods. We systematically construct and\nevaluate twelve distinct QNN configurations, utilizing two unique quantum\nfeature maps combined with six different entanglement strategies for ansatz\ndesign. Experiments conducted on a wind energy dataset reveal that QNNs\nemploying the Z feature map achieve up to 93% prediction accuracy when\nforecasting wind power output using only four input parameters. Our findings\nshow that QNNs outperform classical methods in predictive tasks, underscoring\nthe potential of QML in real-world applications.", "comment": "6 pages, 2 figures", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14795v1", "AI": {"title_translation": "风力预测中QNN架构的比较分析：特征映射和Ansatz配置", "tldr": "本文评估了用于风力预测的不同量子神经网络 (QNN) 架构，展示了 QNN 能够实现高达 93% 的预测精度并优于经典方法，尽管目前量子设备存在局限性。", "motivation": "解决由于噪声中等规模量子 (NISQ) 设备的限制，人们对量子机器学习 (QML) 实际优势的持续怀疑，通过证明量子神经网络 (QNNs) 相对于经典方法的有效性。", "method": "系统地构建并评估了十二种不同的 QNN 配置，利用两种独特的量子特征映射与六种不同的纠缠策略进行 ansatz 设计。实验在风能数据集上进行。", "result": "采用 Z 特征映射的 QNN 在仅使用四个输入参数预测风力输出时，实现了高达 93% 的预测精度。QNN 在预测任务中优于经典方法。", "conclusion": "研究结果突显了量子机器学习 (QML) 在实际应用中的潜力。", "translation": "量子机器学习 (QML) 是量子计算和机器学习交叉领域的新兴学科，旨在通过利用纠缠和叠加等量子力学原理来增强经典机器学习方法。然而，由于目前噪声中等规模量子 (NISQ) 设备的限制，人们对 QML 的实际优势仍持怀疑态度。本研究通过广泛评估量子神经网络 (QNNs)——人工神经网络 (ANNs) 的量子启发对应物，来解决这些问题，证明了它们与经典方法相比的有效性。我们系统地构建并评估了十二种不同的 QNN 配置，利用两种独特的量子特征映射与六种不同的纠缠策略进行 ansatz 设计。在风能数据集上进行的实验表明，采用 Z 特征映射的 QNN 在仅使用四个输入参数预测风力输出时，实现了高达 93% 的预测精度。我们的研究结果表明，QNN 在预测任务中优于经典方法，突显了 QML 在实际应用中的潜力。", "summary": "本研究比较了用于风力预测的各种量子神经网络 (QNN) 架构，旨在尽管当前硬件存在局限性，仍能证明量子机器学习 (QML) 的实际优势。通过在风能数据集上评估十二种具有不同特征映射和纠缠策略的 QNN 配置，研究发现 QNN，特别是使用 Z 特征映射的 QNN，实现了高达 93% 的预测精度并优于经典方法，突出了 QML 在实际应用中的潜力。", "keywords": "量子机器学习, 量子神经网络, 风力预测, 特征映射, Ansatz配置", "comments": "这篇论文具有创新性，因为它通过在实际应用（风力预测）中直接比较 QNN 与经典方法，实证地解决了围绕 QML 实际效用的怀疑。其对不同 QNN 架构（特征映射和 ansatz 配置）的系统评估为在 NISQ 设备上设计有效的 QML 模型提供了宝贵的见解。所展示的在最小输入参数下实现风力预测的高精度（93%）是一个显著的结果，强调了 QML 超越理论讨论的潜力。"}}
{"id": "2506.15138", "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models", "authors": ["Gyeongje Cho", "Yeonkyoun So", "Chanwoo Park", "Sangmin Lee", "Sungmok Jung", "Jaejin Lee"], "summary": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15138v1", "AI": {"title_translation": "Thunder-Tok：最小化生成语言模型韩语文本分词中的每词标记数", "tldr": "Thunder-Tok是一种新的韩语分词器，通过基于规则的预分词和分支熵选择算法，将每词标记数减少了约10%，同时保持了模型性能。", "motivation": "旨在减少韩语生成语言模型中分词的标记冗余（token fertility），同时不损害模型性能，以提高推理速度。", "method": "该方法采用基于规则的预分词，与韩语语言结构对齐；创建包含语言单元的种子词汇；使用基于分支熵的选择算法。这些技术旨在增加平均标记长度，从而降低冗余并保留语言信息。", "result": "Thunder-Tok将标记冗余（即标记数量）减少了约10%，从而将推理速度提高了10%，且在各种下游任务中与BPE相比没有损害性能。", "conclusion": "这种语言学知情的方法对于设计高效的语言模型分词器是有效且实用的。", "translation": "这篇论文介绍了Thunder-Tok，一种新的韩语分词器，旨在在不损害模型性能的情况下减少标记冗余。我们的方法使用了一种基于规则的预分词方法，该方法与韩语的语言结构对齐。我们还创建了一个包含类似于语言单元的种子词汇，并采用了一种基于分支熵的选择算法。这些技术增加了平均标记长度，从而在保留语言信息的同时降低了冗余。实验结果表明，与BPE相比，Thunder-Tok将冗余减少了约10%（即减少了10%的标记数量，将推理速度提高了10%），同时在各种下游任务中没有损害性能。这些发现表明，我们这种语言学知情的方法对于设计高效的语言模型分词器是有效且实用的。", "summary": "本文提出了一种名为Thunder-Tok的新型韩语分词器，旨在优化生成语言模型的韩语文本分词效率。该分词器采用基于规则的预分词和基于分支熵的词汇选择算法，有效增加了平均标记长度，从而将每词标记数减少了约10%，并提升了10%的推理速度。实验证明，该方法在不牺牲模型在多种下游任务性能的前提下，显著降低了标记冗余，证明了其语言学知情设计的有效性和实用性。", "keywords": "韩语分词, 标记化, 生成语言模型, 标记冗余, 分支熵", "comments": "Thunder-Tok的创新之处在于其结合了语言学知识（基于规则的预分词）和统计方法（分支熵选择算法）来优化韩语分词，这对于处理韩语等形态复杂的语言具有重要意义。通过减少标记数量直接提升推理速度，为实际应用带来了显著的性能增益。"}}
{"id": "2506.15077", "title": "A Nonconforming Finite Element Method for Elliptic Interface Problems on Locally Anisotropic Meshes", "authors": ["Hua Wang", "Qichen Zhang"], "summary": "We propose a new nonconforming \\(P_1\\) finite element method for elliptic\ninterface problems. The method is constructed on a locally anisotropic mixed\nmesh, which is generated by fitting the interface through a simple connection\nof intersection points on an interface-unfitted background mesh, as introduced\nin \\cite{Hu2021optimal}. We first establish interpolation error estimates on\nquadrilateral elements satisfying the regular decomposition property (RDP).\nBuilding on this, the main contribution of this work is a novel consistency\nerror analysis for nonconforming elements, which removes the quasi-regularity\nassumption commonly required in existing approaches. Numerical results confirm\nthe theoretical convergence rates and demonstrate the robustness and accuracy\nof the proposed method.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15077v1", "AI": {"title_translation": "局部各向异性网格上椭圆界面问题的非协调有限元方法", "tldr": "本文提出了一种新的非协调P1有限元方法，用于解决椭圆界面问题，该方法在局部各向异性混合网格上构建，并首次去除了传统方法中常见的准正则性假设。", "motivation": "现有非协调有限元方法在处理椭圆界面问题时，通常需要准正则性假设，这限制了其应用范围。", "method": "本文提出一种新的非协调P1有限元方法，用于解决椭圆界面问题。该方法在局部各向异性混合网格上构建，这种网格通过将界面非拟合背景网格上的交点简单连接来拟合界面。首先建立了满足正则分解性质（RDP）的四边形单元上的插值误差估计，然后对非协调单元进行了新颖的一致性误差分析，去除了现有方法中通常需要的准正则性假设。", "result": "数值结果证实了理论收敛率，并证明了所提方法的鲁棒性和准确性。", "conclusion": "所提出的非协调有限元方法在局部各向异性网格上有效解决了椭圆界面问题，并且通过去除准正则性假设，提升了方法的普适性。", "translation": "我们提出了一种新的用于椭圆界面问题的非协调P1有限元方法。该方法构建在局部各向异性混合网格上，这种网格通过将界面非拟合背景网格上的交点进行简单连接来拟合界面，如文献[Hu2021optimal]中所介绍的。我们首先建立了满足正则分解性质（RDP）的四边形单元上的插值误差估计。在此基础上，这项工作的主要贡献是对非协调单元进行了一种新颖的一致性误差分析，该分析消除了现有方法中通常需要的准正则性假设。数值结果证实了理论收敛率，并证明了所提方法的鲁棒性和准确性。", "summary": "本文提出了一种在局部各向异性混合网格上求解椭圆界面问题的新型非协调P1有限元方法。该方法通过对非协调单元进行新颖的一致性误差分析，成功地去除了传统方法中常见的准正则性假设。数值实验验证了所提方法的理论收敛性、鲁棒性和准确性。", "keywords": "非协调有限元方法, 椭圆界面问题, 局部各向异性网格, 一致性误差分析, 准正则性假设", "comments": "这项工作的创新之处在于提出了新的非协调有限元方法，并通过一致性误差分析，成功移除了现有方法中常见的准正则性假设，这对于扩展非协调有限元方法的适用性具有重要意义。"}}
{"id": "2506.15224", "title": "Facility Location Problem under Local Differential Privacy without Super-set Assumption", "authors": ["Kevin Pfisterer", "Quentin Hillebrand", "Vorapong Suppakitpaisarn"], "summary": "In this paper, we introduce an adaptation of the facility location problem\nand analyze it within the framework of local differential privacy (LDP). Under\nthis model, we ensure the privacy of client presence at specific locations.\nWhen n is the number of points, Gupta et al. established a lower bound of\n$\\Omega(\\sqrt{n})$ on the approximation ratio for any differentially private\nalgorithm applied to the original facility location problem. As a result,\nsubsequent works have adopted the super-set assumption, which may, however,\ncompromise user privacy. We show that this lower bound does not apply to our\nadaptation by presenting an LDP algorithm that achieves a constant\napproximation ratio with a relatively small additive factor. Additionally, we\nprovide experimental results demonstrating that our algorithm outperforms the\nstraightforward approach on both synthetically generated and real-world\ndatasets.", "comment": "accepted at DBSec 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15224v1", "AI": {"title_translation": "局部差分隐私下不带超集假设的设施选址问题", "tldr": "本文在局部差分隐私框架下，提出了一种设施选址问题的改进算法，解决了现有方法在隐私保护和近似比上的不足，实现了常数近似比，并在实验中表现优异。", "motivation": "现有的差分隐私设施选址算法在原始问题上存在 $\\Omega(\\sqrt{n})$ 的近似比下限，导致后续工作不得不采用可能损害用户隐私的超集假设。本文旨在解决这一问题，在不使用超集假设的情况下，提高隐私保护下的近似比。", "method": "本文提出了一种设施选址问题的改进版本，并在局部差分隐私 (LDP) 框架下进行分析。设计了一个LDP算法，该算法在不使用超集假设的情况下，实现了常数近似比和一个相对较小的附加因子。通过实验验证了其性能。", "result": "本文提出的LDP算法在不依赖超集假设的情况下，达到了常数近似比，并带有较小的附加因子。实验结果表明，该算法在合成数据集和真实世界数据集上均优于直接方法。", "conclusion": "现有的差分隐私设施选址问题在隐私保护和近似比之间存在权衡。本文通过对设施选址问题的改进，并在LDP框架下设计新算法，成功地在不牺牲隐私的前提下，突破了现有算法的近似比下限，实现了更好的性能。", "translation": "在本文中，我们引入了设施选址问题的一种改进版本，并在局部差分隐私（LDP）框架内对其进行了分析。在此模型下，我们确保了客户端在特定位置的隐私。当n是点数时，Gupta等人为应用于原始设施选址问题的任何差分隐私算法建立了$\\Omega(\\sqrt{n})$的近似比下限。因此，后续工作采用了超集假设，但这可能会损害用户隐私。我们通过提出一种LDP算法来证明这个下限不适用于我们的改进版本，该算法以相对较小的附加因子实现了常数近似比。此外，我们提供了实验结果，表明我们的算法在合成生成和真实世界数据集上都优于直接方法。", "summary": "本文研究了局部差分隐私（LDP）下的设施选址问题，旨在解决现有方法在近似比和隐私保护方面的局限性。针对原始设施选址问题存在的 $\\Omega(\\sqrt{n})$ 近似比下限以及为规避此下限而引入的可能损害隐私的超集假设，作者提出了一种新的设施选址问题变体。在此变体下，本文设计了一个LDP算法，该算法在不依赖超集假设的情况下实现了常数近似比和较小的附加因子。实验结果证实了该算法在合成数据和真实数据上的优越性能。", "keywords": "设施选址问题, 局部差分隐私, 近似比, 超集假设, 隐私保护", "comments": "本文的创新点在于在局部差分隐私框架下，成功地对设施选址问题进行了适应性修改，并设计出了一种不依赖“超集假设”且能达到常数近似比的算法。这解决了现有研究在隐私保护和近似比之间难以平衡的难题，避免了可能损害用户隐私的假设，对于实际应用中的隐私保护设施选址具有重要意义。"}}
{"id": "2506.15314", "title": "Case Study for Developing a UXR Point of View for FinOps Product Innovation", "authors": ["Jason Dong", "Anna Wu"], "summary": "In the dynamic landscape of Cloud financial management, we are sharing a case\nstudy exploring the development of a User Experience Research (UXR) Point of\nView (PoV) to drive FinOps product innovation. We demonstrate how qualitative\nand quantitative research methods working together to navigate the challenges\nof understanding customer needs, aligning cross-functional teams, and\nprioritizing limited resources. Through a multi-phased research approach, the\nresearch team identifies opportunities, quantifies pain points, and segments\ndiverse customer cohorts. This culminated in a UXR PoV that informed the\ncreation of a differentiated product strategy, a 'one-stop shop' dashboard\nempowering FinOps practitioners with actionable insights and tools. This case\nstudy highlights the power of mixed-methods research in uncovering actionable\ninsights that drive impactful product innovation.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15314v1", "AI": {"title_translation": "针对FinOps产品创新开发用户体验研究（UXR）视角的案例研究", "tldr": "本案例研究展示了如何通过混合方法用户体验研究（UXR）开发FinOps产品创新的视角（PoV），从而创建了一个“一站式”仪表板以满足用户需求并推动产品创新。", "motivation": "在动态的云财务管理环境中，旨在通过开发用户体验研究（UXR）视角来驱动FinOps产品创新，以应对理解客户需求、协调跨职能团队和优先分配有限资源等挑战。", "method": "采用多阶段的混合方法研究方法，结合定性和定量研究，以识别机会、量化痛点并细分客户群体。", "result": "成功形成了一个用户体验研究（UXR）视角，该视角指导了差异化产品策略的制定，并促成了一个“一站式”仪表板的创建，为FinOps实践者提供了可操作的见解和工具。", "conclusion": "混合方法研究在发现可操作的见解以推动有影响力的产品创新方面具有强大作用。", "translation": "在动态的云财务管理环境中，我们分享了一个案例研究，探讨了如何开发用户体验研究（UXR）视角以推动FinOps产品创新。我们展示了定性和定量研究方法如何协同工作，以应对理解客户需求、协调跨职能团队和优先分配有限资源的挑战。通过多阶段的研究方法，研究团队识别了机会，量化了痛点，并对不同的客户群体进行了细分。这最终形成了一个用户体验研究（UXR）视角，为差异化产品策略的创建提供了信息，即一个“一站式”仪表板，为FinOps实践者提供可操作的见解和工具。本案例研究强调了混合方法研究在发现可操作的见解以推动有影响力的产品创新方面的力量。", "summary": "本案例研究探讨了在云财务管理领域，如何通过开发用户体验研究（UXR）视角来推动FinOps产品创新。研究采用了多阶段的混合方法，整合定性和定量研究，以深入理解客户需求、协调团队并优化资源。最终，研究成果形成了一个UXR视角，并指导开发了一个“一站式”仪表板，旨在为FinOps实践者提供实用工具和洞察，从而展示了混合方法研究在驱动产品创新方面的有效性。", "keywords": "用户体验研究, FinOps, 产品创新, 混合方法研究, 案例研究", "comments": "这篇案例研究强调了在产品开发中整合用户体验研究（UXR）的重要性，尤其是在复杂的FinOps领域。其创新点在于结合定性与定量研究，系统地识别用户痛点和需求，并将其转化为具体的、可操作的产品策略和工具（如“一站式”仪表板）。这对于推动以用户为中心的产品创新具有重要借鉴意义。"}}
{"id": "2506.15132", "title": "Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion", "authors": ["Yushi Wang", "Penghui Chen", "Xinyu Han", "Feng Wu", "Mingguo Zhao"], "summary": "Recent advancements in reinforcement learning (RL) have led to significant\nprogress in humanoid robot locomotion, simplifying the design and training of\nmotion policies in simulation. However, the numerous implementation details\nmake transferring these policies to real-world robots a challenging task. To\naddress this, we have developed a comprehensive code framework that covers the\nentire process from training to deployment, incorporating common RL training\nmethods, domain randomization, reward function design, and solutions for\nhandling parallel structures. This library is made available as a community\nresource, with detailed descriptions of its design and experimental results. We\nvalidate the framework on the Booster T1 robot, demonstrating that the trained\npolicies seamlessly transfer to the physical platform, enabling capabilities\nsuch as omnidirectional walking, disturbance resistance, and terrain\nadaptability. We hope this work provides a convenient tool for the robotics\ncommunity, accelerating the development of humanoid robots. The code can be\nfound in https://github.com/BoosterRobotics/booster_gym.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15132v1", "AI": {"title_translation": "Booster Gym：一个用于人形机器人运动的端到端强化学习框架", "tldr": "开发了一个名为Booster Gym的端到端强化学习框架，旨在简化人形机器人运动策略从训练到部署的流程，并实现了模拟到真实世界的无缝迁移。", "motivation": "现有的强化学习在人形机器人运动方面取得了进展，但将模拟中训练的策略转移到真实机器人上仍然具有挑战性，因为涉及大量的实施细节。", "method": "开发了一个全面的代码框架，涵盖从训练到部署的整个过程，整合了常见的RL训练方法、域随机化、奖励函数设计以及处理并行结构的解决方案。该库作为社区资源提供。", "result": "在Booster T1机器人上验证了该框架，证明训练的策略可以无缝迁移到物理平台，实现了全向行走、抗干扰和地形适应能力。", "conclusion": "该工作为机器人社区提供了一个便捷工具，加速了人形机器人的开发。", "translation": "近期强化学习（RL）的进展已使人形机器人运动取得显著进步，简化了模拟中运动策略的设计和训练。然而，大量的实施细节使得将这些策略转移到真实世界的机器人成为一项具有挑战性的任务。为了解决这个问题，我们开发了一个全面的代码框架，涵盖从训练到部署的整个过程，整合了常见的RL训练方法、域随机化、奖励函数设计以及处理并行结构的解决方案。该库作为社区资源提供，并附有其设计和实验结果的详细描述。我们在Booster T1机器人上验证了该框架，证明训练的策略可以无缝迁移到物理平台，实现了全向行走、抗干扰和地形适应能力。我们希望这项工作能为机器人社区提供一个便捷工具，加速人形机器人的开发。代码可在https://github.com/BoosterRobotics/booster_gym 找到。", "summary": "Booster Gym是一个为人形机器人运动设计的端到端强化学习框架，旨在解决模拟策略向真实机器人迁移的挑战。它整合了RL训练、域随机化、奖励设计和并行结构处理等功能。该框架已在Booster T1机器人上验证，成功实现了策略的无缝迁移，使机器人具备全向行走、抗干扰和地形适应能力。该框架作为开源社区资源，有望加速人形机器人的开发。", "keywords": "强化学习, 人形机器人, 运动控制, 迁移学习, 开源框架", "comments": "该论文的创新点在于提供了一个全面的、端到端的强化学习框架，显著简化了人形机器人运动策略从模拟到真实世界的部署过程。它通过整合常见RL训练方法、域随机化和奖励函数设计等关键技术，有效解决了现实世界迁移的复杂性。该框架作为开源社区资源发布，对于降低人形机器人RL研究的门槛和加速相关领域的实际应用具有重要意义。"}}
{"id": "2506.14854", "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "authors": ["Varun Mannam", "Zhenyu Shi"], "summary": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring.", "comment": "Submitting to ICCV 2025 workshop:\n  https://retailvisionworkshop.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14854v1", "AI": {"title_translation": "高效零售视频标注：一种用于产品和客户交互分析的鲁棒关键帧生成方法", "tldr": "本研究提出了一种基于深度学习的方法，用于自动化零售视频中的关键帧识别和产品与客户的自动标注，显著提高了标注效率并降低了成本。", "motivation": "传统的视频标注方法依赖耗时的人工标注，导致帧选择不鲁棒且运营成本高昂，尤其是在客户行为分析、产品交互检测和店内活动识别等零售应用中。", "method": "我们提出了一种基于深度学习的方法，利用深度神经网络学习判别性特征，通过嵌入视频帧并结合针对零售环境定制的基于对象检测的技术，实现零售视频中关键帧的自动化识别和产品与客户的自动标注。", "result": "实验结果表明，我们的方法优于传统方法，标注准确性与人工标注相当，同时显著提高了零售视频标注的整体效率。平均可节省2倍的视频标注成本，并且在不降低标注质量的情况下，人工标注者只需验证/调整不到5%的检测帧。", "conclusion": "本研究提出的自动化关键帧检测方法能够大幅节省零售视频标注任务的时间和精力，对购物者旅程分析、产品交互检测和店内安全监控等多种零售应用具有重要价值。", "translation": "精确的视频标注在现代零售应用中扮演着至关重要的角色，包括客户行为分析、产品交互检测和店内活动识别。然而，传统的标注方法严重依赖耗时的人工标注，引入了不鲁棒的帧选择并增加了运营成本。为了解决零售领域中的这些挑战，我们提出了一种基于深度学习的方法，该方法自动化了零售视频中的关键帧识别，并提供了产品和客户的自动标注。我们的方法利用深度神经网络通过嵌入视频帧和结合为零售环境量身定制的基于对象检测的技术来学习判别性特征。实验结果表明，我们的方法优于传统方法，实现了与人工标注相当的准确性，同时提高了零售视频标注的整体效率。值得注意的是，我们的方法使视频标注的成本平均节省了2倍。通过允许人工标注者验证/调整视频数据集中不到5%的检测帧，同时在不降低标注质量的情况下自动化其余帧的标注过程，零售商可以显著降低运营成本。关键帧检测的自动化在零售视频标注任务中节省了大量时间和精力，证明对购物者旅程分析、产品交互检测和店内安全监控等多种零售应用具有高度价值。", "summary": "本论文提出了一种基于深度学习的零售视频关键帧生成方法，旨在解决传统人工标注耗时且成本高的问题。该方法利用深度神经网络和目标检测技术，自动识别视频中的关键帧并对产品和客户进行标注。实验证明，该方法在保持与人工标注相当的准确性的同时，显著提高了标注效率，平均节省2倍成本，并且只需人工核验少于5%的帧，从而大幅降低了零售视频标注的运营成本和工作量，对零售分析和安全监控具有重要应用价值。", "keywords": "零售视频标注, 关键帧生成, 深度学习, 目标检测, 效率提升", "comments": "该论文的创新点在于将深度学习应用于零售视频的关键帧生成和自动化标注，有效解决了传统人工标注效率低、成本高的问题。其提出的方法不仅提高了标注效率，还显著降低了运营成本，对零售行业的数据分析和应用具有重要意义。该方法通过减少人工干预，提升了标注的鲁棒性。"}}
{"id": "2207.01732", "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition", "authors": ["Jiamin Xie", "John H. L. Hansen"], "summary": "Convolutional neural networks (CNN) have improved speech recognition\nperformance greatly by exploiting localized time-frequency patterns. But these\npatterns are assumed to appear in symmetric and rigid kernels by the\nconventional CNN operation. It motivates the question: What about asymmetric\nkernels? In this study, we illustrate adaptive views can discover local\nfeatures which couple better with attention than fixed views of the input. We\nreplace depthwise CNNs in the Conformer architecture with a deformable\ncounterpart, dubbed this \"Deformer\". By analyzing our best-performing model, we\nvisualize both local receptive fields and global attention maps learned by the\nDeformer and show increased feature associations on the utterance level. The\nstatistical analysis of learned kernel offsets provides an insight into the\nchange of information in features with the network depth. Finally, replacing\nonly half of the layers in the encoder, the Deformer improves +5.6% relative\nWER without a LM and +6.4% relative WER with a LM over the Conformer baseline\non the WSJ eval92 set.", "comment": "Accepted to Interspeech 2022", "cate": "eess.AS", "url": "http://arxiv.org/abs/2207.01732v2", "AI": {"title_translation": "DEFORMER：将形变的局部模式与全局上下文耦合以实现鲁棒的端到端语音识别", "tldr": "Deformer通过使用可变形卷积来捕捉非对称时频模式，从而改进语音识别，其性能优于Conformer基线模型。", "motivation": "传统的卷积神经网络（CNN）在语音识别中通过利用局部时频模式取得了显著进展，但它们假设这些模式出现在对称且刚性的核中。这引发了一个问题：非对称核该如何处理？", "method": "本研究提出了一种名为“Deformer”的新架构，它将Conformer架构中的深度卷积神经网络替换为可变形的对应部分。Deformer能够通过自适应视图发现与注意力机制更好耦合的局部特征。", "result": "通过可视化Deformer学习到的局部感受野和全局注意力图，研究显示其在话语层面增加了特征关联。对学习到的核偏移进行统计分析，揭示了特征信息随网络深度的变化。在WSJ eval92数据集上，仅替换编码器中一半的层，Deformer在没有语言模型的情况下相对WER提升了+5.6%，在使用语言模型的情况下相对WER提升了+6.4%，均优于Conformer基线。", "conclusion": "Deformer通过引入可变形卷积来捕捉更灵活的局部模式，显著提高了端到端语音识别的性能，超越了Conformer基线模型。", "translation": "卷积神经网络（CNN）通过利用局部时频模式极大地提高了语音识别性能。但这些模式被传统CNN操作假定出现在对称且刚性的核中。这引发了一个问题：非对称核该如何处理？在本研究中，我们阐明自适应视图可以发现与注意力机制比固定输入视图更好地耦合的局部特征。我们将Conformer架构中的深度卷积神经网络替换为可变形的对应部分，称之为“Deformer”。通过分析我们表现最佳的模型，我们可视化了Deformer学习到的局部感受野和全局注意力图，并展示了在话语层面增加的特征关联。对学习到的核偏移进行统计分析，提供了关于特征信息随网络深度变化的见解。最后，仅替换编码器中一半的层，Deformer在WSJ eval92数据集上，相比Conformer基线，在没有语言模型的情况下相对WER提高了+5.6%，在有语言模型的情况下相对WER提高了+6.4%。", "summary": "本论文引入了Deformer，一种改进Conformer架构的新模型，旨在解决传统CNN在语音识别中处理非对称时频模式的局限性。Deformer通过将Conformer中的深度卷积层替换为可变形的对应部分，使其能够自适应地捕获局部特征并更好地与全局上下文（注意力机制）耦合。实验结果表明，Deformer在WSJ eval92数据集上显著提升了语音识别性能，相对词错误率（WER）相比Conformer基线有显著降低，证明了其在端到端语音识别中的鲁棒性。", "keywords": "Deformer, 语音识别, Conformer, 可变形卷积, 时频模式", "comments": "Deformer的创新之处在于将可变形卷积引入到语音识别领域，解决了传统CNN在捕捉复杂、非刚性时频模式方面的限制。通过允许卷积核自适应地形变，模型能够更好地捕捉语音信号中细微的局部特征，并与全局注意力机制有效结合。其在WSJ数据集上显著的WER提升证明了该方法的有效性和重要性。"}}
{"id": "2506.14806", "title": "Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis", "authors": ["Bochen Lyu", "Xiaojing Zhang", "Fangyi Zheng", "He Wang", "Zheng Wang", "Zhanxing Zhu"], "summary": "This paper establishes a continuous time approximation, a piece-wise\ncontinuous differential equation, for the discrete Heavy-Ball (HB) momentum\nmethod with explicit discretization error. Investigating continuous\ndifferential equations has been a promising approach for studying the discrete\noptimization methods. Despite the crucial role of momentum in gradient-based\noptimization methods, the gap between the original discrete dynamics and the\ncontinuous time approximations due to the discretization error has not been\ncomprehensively bridged yet. In this work, we study the HB momentum method in\ncontinuous time while putting more focus on the discretization error to provide\nadditional theoretical tools to this area. In particular, we design a\nfirst-order piece-wise continuous differential equation, where we add a number\nof counter terms to account for the discretization error explicitly. As a\nresult, we provide a continuous time model for the HB momentum method that\nallows the control of discretization error to arbitrary order of the step size.\nAs an application, we leverage it to find a new implicit regularization of the\ndirectional smoothness and investigate the implicit bias of HB for diagonal\nlinear networks, indicating how our results can be used in deep learning. Our\ntheoretical findings are further supported by numerical experiments.", "comment": "32 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14806v1", "AI": {"title_translation": "连续时间下的重球动量法及离散化误差分析", "tldr": "本文为离散重球动量法建立了一个连续时间近似模型，通过引入补偿项明确处理了离散化误差，实现了对误差的任意阶控制，并应用于深度学习中的隐式正则化和偏置分析。", "motivation": "尽管动量在基于梯度的优化方法中至关重要，但由于离散化误差，原始离散动力学与连续时间近似之间的鸿沟尚未被全面弥合。", "method": "本文设计了一个一阶分段连续微分方程，其中加入了多个补偿项以明确解释离散化误差。", "result": "提出了一个重球动量法的连续时间模型，该模型允许对离散化误差进行任意阶步长的控制。作为应用，利用该模型发现了方向平滑度的一种新的隐式正则化，并研究了HB对对角线性网络的隐式偏置，表明了其在深度学习中的应用。理论结果得到了数值实验的支持。", "conclusion": "本文通过建立一个明确考虑离散化误差的连续时间模型，有效弥合了离散重球动量法与其连续时间近似之间的差距，并展示了其在深度学习中的潜在应用。", "translation": "本文为离散重球（HB）动量法建立了一个连续时间近似，即一个分段连续微分方程，并明确给出了离散化误差。研究连续微分方程一直是研究离散优化方法的一种有前景的方法。尽管动量在基于梯度的优化方法中起着关键作用，但由于离散化误差，原始离散动力学与连续时间近似之间的鸿沟尚未被全面弥合。在这项工作中，我们研究了连续时间下的HB动量法，同时更侧重于离散化误差，以期为该领域提供额外的理论工具。特别是，我们设计了一个一阶分段连续微分方程，其中我们添加了多个补偿项以明确解释离散化误差。结果是，我们为HB动量法提供了一个连续时间模型，该模型允许对离散化误差进行任意阶步长的控制。作为一项应用，我们利用它找到了方向平滑度的一种新的隐式正则化，并研究了HB对对角线性网络的隐式偏置，表明了我们的结果如何用于深度学习。我们的理论发现得到了数值实验的进一步支持。", "summary": "本文针对离散重球（HB）动量法与连续时间近似之间的离散化误差问题，提出了一种新的连续时间模型。通过设计一个包含显式补偿项的一阶分段连续微分方程，该模型能够将离散化误差控制到任意步长阶。研究还展示了其在发现隐式正则化和分析深度学习中隐式偏置方面的应用，并通过数值实验验证了理论结果。", "keywords": "重球动量法, 连续时间, 离散化误差, 优化, 深度学习", "comments": "本文的创新之处在于其明确地解决了重球动量法中离散动力学与连续时间近似之间的离散化误差问题。通过引入补偿项来精确控制误差，为理解和改进优化算法提供了新的理论工具，并展示了其在深度学习中的实际应用潜力，对于连接理论优化与实际深度学习实践具有重要意义。"}}
{"id": "2506.15150", "title": "Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation", "authors": ["Yuanlong Ji", "Xingbang Yang", "Ruoqi Zhao", "Qihan Ye", "Quan Zheng", "Yubo Fan"], "summary": "Gait phase estimation based on inertial measurement unit (IMU) signals\nfacilitates precise adaptation of exoskeletons to individual gait variations.\nHowever, challenges remain in achieving high accuracy and robustness,\nparticularly during periods of terrain changes. To address this, we develop a\ngait phase estimation neural network based on implicit modeling of human\nlocomotion, which combines temporal convolution for feature extraction with\ntransformer layers for multi-channel information fusion. A channel-wise masked\nreconstruction pre-training strategy is proposed, which first treats gait phase\nstate vectors and IMU signals as joint observations of human locomotion, thus\nenhancing model generalization. Experimental results demonstrate that the\nproposed method outperforms existing baseline approaches, achieving a gait\nphase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under\nstable terrain conditions with a look-back window of 2 seconds, and a phase\nRMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain\ntransitions. Hardware validation on a hip exoskeleton further confirms that the\nalgorithm can reliably identify gait cycles and key events, adapting to various\ncontinuous motion scenarios. This research paves the way for more intelligent\nand adaptive exoskeleton systems, enabling safer and more efficient human-robot\ninteraction across diverse real-world environments.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15150v1", "AI": {"title_translation": "人体运动隐式建模的实时步态相位估计", "tldr": "本文提出一种基于人体运动隐式建模的神经网络，用于实时步态相位估计，有效解决了IMU信号在地形变化下精度和鲁棒性不足的问题，并在外骨骼上进行了验证。", "motivation": "现有基于惯性测量单元（IMU）信号的步态相位估计方法在实现高精度和鲁棒性方面仍面临挑战，尤其是在地形变化期间。", "method": "本文开发了一个基于人体运动隐式建模的步态相位估计神经网络，该网络结合了用于特征提取的时间卷积层和用于多通道信息融合的Transformer层。此外，还提出了一种通道掩蔽重建预训练策略，将步态相位状态向量和IMU信号视为人体运动的联合观测，以增强模型泛化能力。", "result": "所提出的方法在稳定地形条件下，步态相位RMSE为$2.729 \\pm 1.071%$，相位率MAE为$0.037 \\pm 0.016%$；在地形转换条件下，相位RMSE为$3.215 \\pm 1.303%$，相位率MAE为$0.050 \\pm 0.023%$，均优于现有基线方法。在髋关节外骨骼上的硬件验证进一步证实该算法能可靠识别步态周期和关键事件，并适应各种连续运动场景。", "conclusion": "这项研究为更智能和自适应的外骨骼系统铺平了道路，从而在各种现实环境中实现更安全、更高效的人机交互。", "translation": "基于惯性测量单元（IMU）信号的步态相位估计有助于外骨骼精确适应个体步态变化。然而，在实现高精度和鲁棒性方面仍存在挑战，特别是在地形变化期间。为了解决这个问题，我们开发了一种基于人体运动隐式建模的步态相位估计神经网络，它结合了用于特征提取的时间卷积和用于多通道信息融合的Transformer层。提出了一种通道掩蔽重建预训练策略，该策略首先将步态相位状态向量和IMU信号视为人体运动的联合观测，从而增强了模型的泛化能力。实验结果表明，所提出的方法优于现有基线方法，在2秒回溯窗口的稳定地形条件下，步态相位RMSE为$2.729 \\pm 1.071%$，相位率MAE为$0.037 \\pm 0.016%$；在地形转换条件下，相位RMSE为$3.215 \\pm 1.303%$，相位率MAE为$0.050 \\pm 0.023%$。在髋关节外骨骼上的硬件验证进一步证实，该算法可以可靠地识别步态周期和关键事件，适应各种连续运动场景。这项研究为更智能和自适应的外骨骼系统铺平了道路，从而在各种现实环境中实现更安全、更高效的人机交互。", "summary": "本文提出一种基于人体运动隐式建模的神经网络，用于实时步态相位估计，旨在解决IMU信号在地形变化下精度和鲁棒性不足的问题。该网络结合了时间卷积和Transformer层，并引入了通道掩蔽重建预训练策略以增强泛化能力。实验证明，该方法在稳定和变化地形下均优于现有基线，并在外骨骼上进行了硬件验证，为智能自适应外骨骼系统提供了基础。", "keywords": "步态相位估计, 隐式建模, 神经网络, IMU, 外骨骼", "comments": "该研究的创新点在于将人体运动的隐式建模与神经网络相结合，特别是引入了通道掩蔽重建预训练策略，有效提升了模型在复杂地形下的泛化能力和鲁棒性。其在真实外骨骼上的硬件验证也证明了方法的实用性和前景，有望显著改善人机交互的安全性与效率。"}}
{"id": "2506.15557", "title": "Construction of an Organ Shape Atlas Using a Hierarchical Mesh Variational Autoencoder", "authors": ["Zijie Wang", "Ryuichi Umehara", "Mitsuhiro Nakamura", "Megumi Nakao"], "summary": "An organ shape atlas, which represents the shape and position of the organs\nand skeleton of a living body using a small number of parameters, is expected\nto have a wide range of clinical applications, including intraoperative\nguidance and radiotherapy. Because the shape and position of soft organs vary\ngreatly among patients, it is difficult for linear models to reconstruct shapes\nthat have large local variations. Because it is difficult for conventional\nnonlinear models to control and interpret the organ shapes obtained, deep\nlearning has been attracting attention in three-dimensional shape\nrepresentation. In this study, we propose an organ shape atlas based on a mesh\nvariational autoencoder (MeshVAE) with hierarchical latent variables. To\nrepresent the complex shapes of biological organs and nonlinear shape\ndifferences between individuals, the proposed method maintains the performance\nof organ shape reconstruction by hierarchizing latent variables and enables\nshape representation using lower-dimensional latent variables. Additionally,\ntemplates that define vertex correspondence between different resolutions\nenable hierarchical representation in mesh data and control the global and\nlocal features of the organ shape. We trained the model using liver and stomach\norgan meshes obtained from 124 cases and confirmed that the model reconstructed\nthe position and shape with an average distance between vertices of 1.5 mm and\nmean distance of 0.7 mm for the liver shape, and an average distance between\nvertices of 1.4 mm and mean distance of 0.8 mm for the stomach shape on test\ndata from 19 of cases. The proposed method continuously represented\ninterpolated shapes, and by changing latent variables at different hierarchical\nlevels, the proposed method hierarchically separated shape features compared\nwith PCA.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15557v1", "AI": {"title_translation": "使用分层网格变分自编码器构建器官形状图谱", "tldr": "本研究提出了一种基于分层网格变分自编码器（MeshVAE）的器官形状图谱，能够准确重建复杂器官形状，并实现形状特征的分层表示和控制，解决了传统模型在表示复杂生物器官形状方面的局限性。", "motivation": "器官形状图谱在临床应用中具有广泛前景（如术中引导和放射治疗）。然而，软器官形状的个体间差异大，线性模型难以处理局部大变异；传统非线性模型在控制和解释器官形状方面存在困难。", "method": "本研究提出了一种基于分层潜在变量的网格变分自编码器（MeshVAE）来构建器官形状图谱。该方法通过分层潜在变量来表示生物器官的复杂形状和个体间的非线性形状差异，同时保持形状重建性能并使用较低维度的潜在变量进行表示。此外，利用定义不同分辨率之间顶点对应关系的模板，实现了网格数据的分层表示，并能控制器官形状的全局和局部特征。", "result": "模型使用来自124个病例的肝脏和胃器官网格进行训练，并在19个测试病例数据上进行了验证。结果显示，肝脏形状重建的顶点平均距离为1.5毫米，平均距离为0.7毫米；胃形状重建的顶点平均距离为1.4毫米，平均距离为0.8毫米。所提出的方法能够连续表示插值形状，并且通过改变不同层次的潜在变量，与PCA相比，能够分层地分离形状特征。", "conclusion": "本研究提出的基于分层网格变分自编码器的器官形状图谱，能够有效且准确地重建复杂的器官形状，实现低维度的形状表示，并能分层控制形状特征，相比传统方法（如PCA）展现出优越的性能。", "translation": "器官形状图谱，即使用少量参数表示活体器官和骨骼的形状和位置，有望在术中引导和放射治疗等领域具有广泛的临床应用。由于软器官的形状和位置在患者之间差异很大，线性模型难以重建具有大局部变化的形状。由于传统非线性模型难以控制和解释所获得的器官形状，深度学习在三维形状表示方面受到了关注。在本研究中，我们提出了一种基于具有分层潜在变量的网格变分自编码器（MeshVAE）的器官形状图谱。为了表示生物器官的复杂形状和个体之间非线性形状差异，所提出的方法通过分层潜在变量来保持器官形状重建的性能，并能够使用较低维度的潜在变量进行形状表示。此外，定义不同分辨率之间顶点对应关系的模板实现了网格数据中的分层表示，并控制了器官形状的全局和局部特征。我们使用从124个病例获得的肝脏和胃器官网格训练了该模型，并证实该模型在19个测试病例的数据上，肝脏形状的顶点平均距离为1.5毫米，平均距离为0.7毫米；胃形状的顶点平均距离为1.4毫米，平均距离为0.8毫米，重建了位置和形状。所提出的方法连续地表示了插值形状，并且通过改变不同层次的潜在变量，与PCA相比，所提出的方法分层地分离了形状特征。", "summary": "本研究提出了一种基于分层网格变分自编码器（MeshVAE）的器官形状图谱，旨在解决传统模型在表示具有大局部变化的复杂器官形状方面的局限性。该方法通过引入分层潜在变量和多分辨率模板，实现了对生物器官形状的准确重建、低维度表示以及全局和局部特征的精细控制。在肝脏和胃器官数据集上的实验结果表明，该模型能够以高精度重建器官形状，并能有效分层分离形状特征，优于传统的PCA方法，为临床应用提供了新的工具。", "keywords": "器官形状图谱, 网格变分自编码器, 分层潜在变量, 三维形状表示, 深度学习", "comments": "该论文的创新点在于将分层潜在变量引入网格变分自编码器，从而有效解决了复杂器官形状表示中局部变异难以处理的问题，并实现了形状特征的层次化控制和解释。其提出的多分辨率模板进一步增强了模型的灵活性和控制力。该方法在临床应用中具有重要意义，尤其是在需要精确三维形状表示的领域，如术中指导和放射治疗。"}}
{"id": "2506.15156", "title": "Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View", "authors": ["Muhammad Cendekia Airlangga", "Hilal AlQuabeh", "Munachiso S Nwadike", "Kentaro Inui"], "summary": "We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15156v1", "AI": {"title_translation": "Mamba中首因效应和近因效应的出现：一个机制性视角", "tldr": "该研究通过首因和近因效应揭示了Mamba模型中信息保留和遗忘的机制，发现U形准确率曲线由长期记忆、短期记忆和动态记忆分配三种机制引起。", "motivation": "本研究旨在利用首因效应和近因效应作为行为工具，探究状态空间语言模型（特别是Mamba架构）如何随时间保留和遗忘信息，以理解其记忆机制。", "method": "研究通过对Mamba架构应用结构化回忆任务，并在两个大规模Mamba语言模型（1.4B和7B参数）上进行有针对性的消融实验和输入扰动来验证发现。", "result": "研究观察到Mamba模型在输入序列的开始和结束时表现出一致的U形准确率曲线。发现了三种机制：1. 长期记忆由模型选择性状态空间块内稀疏的通道子集支持，这些通道持续编码早期输入并与首因效应相关。2. 短期记忆受delta调制循环控制，近期输入因指数衰减获得更多权重，但当引入干扰项时，这种近因优势会消失。3. 记忆分配受语义规律动态调节，输入序列中的重复关系会改变delta门控行为，增加对中间项的遗忘倾向。", "conclusion": "本研究揭示了Mamba模型中首因效应和近因效应背后的具体机制，深入阐明了这些模型如何管理信息的保留和遗忘。", "translation": "我们利用首因效应和近因效应作为行为工具来研究状态空间语言模型中的记忆，以揭示信息如何随时间保留和遗忘。将结构化回忆任务应用于Mamba架构，我们观察到一致的U形准确率曲线，表明在输入序列的开始和结束时表现出强大的性能。我们确定了导致这种模式出现的三种机制。首先，长期记忆由模型选择性状态空间块内稀疏的通道子集支持，这些通道持续编码早期输入标记并与首因效应存在因果关系。其次，短期记忆受delta调制循环控制：近期输入由于指数衰减而获得更多权重，但当引入干扰项时，这种近因优势会消失，揭示了记忆深度的明确限制。第三，我们发现记忆分配受到语义规律的动态调节：输入序列中重复的关系会改变delta门控行为，增加遗忘中间项的倾向。我们通过对两个大型Mamba语言模型（一个具有1.4B参数，另一个具有7B参数）进行有针对性的消融实验和输入扰动来验证这些发现。", "summary": "本研究通过分析Mamba模型中的首因效应和近因效应，深入探讨了其信息保留和遗忘机制。研究发现Mamba在回忆任务中呈现U形准确率曲线，并揭示了三种核心机制：长期记忆由稀疏通道支持，短期记忆受delta调制循环控制，以及记忆分配受语义规律动态调节。这些发现通过在不同规模的Mamba模型上的实验得到验证，为理解状态空间模型如何处理序列信息提供了新的视角。", "keywords": "Mamba, 首因效应, 近因效应, 状态空间模型, 记忆机制", "comments": "这项研究通过将心理学中的记忆效应（首因和近因）引入到对Mamba架构的分析中，提供了一个新颖且深入的视角来理解其内部机制。它不仅揭示了Mamba如何处理长期和短期信息，还发现了语义规律对记忆分配的影响，这对于优化状态空间模型和提高其在长序列任务中的表现具有重要指导意义。其机制性的解释和大规模模型的验证增加了研究的可信度。"}}
{"id": "2506.15142", "title": "Fourth- and Higher-Order Semi-Lagrangian Finite Volume Methods for the Two-dimensional Advection Equation on Arbitrarily Complex Domains", "authors": ["Yunxia Sun", "Kaiyi Liang", "Yuke Zhu", "Zhi Lin", "Qinghai Zhang"], "summary": "To numerically solve the two-dimensional advection equation, we propose a\nfamily of fourth- and higher-order semi-Lagrangian finite volume (SLFV) methods\nthat feature (1) fourth-, sixth-, and eighth-order convergence rates, (2)\napplicability to both regular and irregular domains with arbitrarily complex\ntopology and geometry, (3) ease of handling both zero and nonzero source terms,\nand (4) the same algorithmic steps for both periodic and incoming penetration\nconditions. Test results confirm the analysis and demonstrate the accuracy,\nflexibility, robustness, and excellent conditioning of the proposed SLFV\nmethod.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15142v1", "AI": {"title_translation": "二维平流方程在任意复杂域上的四阶及更高阶半拉格朗日有限体积方法", "tldr": "本文提出了一种用于数值求解二维平流方程的四阶及更高阶半拉格朗日有限体积（SLFV）方法，该方法具有高收敛率、适用于复杂域、易于处理源项和边界条件，并通过测试验证了其准确性、灵活性和鲁棒性。", "motivation": "为了数值求解二维平流方程。", "method": "本文提出了一系列四阶及更高阶的半拉格朗日有限体积（SLFV）方法。该方法具有以下特点：1) 达到四阶、六阶和八阶收敛率；2) 适用于具有任意复杂拓扑和几何形状的规则和不规则域；3) 易于处理零和非零源项；4) 对周期和入射穿透条件采用相同的算法步骤。", "result": "测试结果证实了理论分析，并证明了所提出的SLFV方法的准确性、灵活性、鲁棒性和出色的条件性。", "conclusion": "所提出的四阶及更高阶半拉格朗日有限体积（SLFV）方法是求解二维平流方程的一种高效、灵活且鲁棒的数值方法，尤其适用于复杂域和多种边界条件。", "translation": "为了数值求解二维平流方程，我们提出了一系列四阶及更高阶的半拉格朗日有限体积（SLFV）方法，其特点包括：(1) 四阶、六阶和八阶收敛率；(2) 适用于具有任意复杂拓扑和几何形状的规则和不规则域；(3) 易于处理零和非零源项；以及(4) 对周期和入射穿透条件采用相同的算法步骤。测试结果证实了分析，并证明了所提出的SLFV方法的准确性、灵活性、鲁棒性和出色的条件性。", "summary": "本文提出了一系列用于数值求解二维平流方程的四阶及更高阶半拉格朗日有限体积（SLFV）方法。该方法具有高阶收敛率（四阶、六阶、八阶），能够适应任意复杂的规则和不规则域，并简化了源项和边界条件的处理。实验结果验证了该方法的准确性、灵活性、鲁棒性及良好的条件性。", "keywords": "半拉格朗日有限体积方法, 二维平流方程, 高阶收敛, 复杂域, 数值方法", "comments": "该研究通过开发高阶半拉格朗日有限体积方法，显著提升了二维平流方程数值求解的精度和适用性，尤其是在处理复杂几何域和不同边界条件方面的创新性，使其成为一个通用且高效的数值工具。其高阶收敛特性和对复杂域的普适性是主要亮点。"}}
{"id": "2506.15253", "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "summary": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.", "comment": "12 pages, 8 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15253v1", "AI": {"title_translation": "RAS-Eval：一个用于真实世界环境中LLM智能体安全评估的综合基准", "tldr": "RAS-Eval是一个新的安全基准，用于评估LLM智能体在真实世界环境中的安全性。研究发现当前LLM智能体存在显著漏洞，攻击平均降低了36.78%的任务完成率，并在学术环境中达到了85.65%的成功率，揭示了实际部署中的关键风险。", "motivation": "大型语言模型（LLM）智能体在医疗和金融等关键领域的快速部署，使得对鲁棒安全框架的需求变得紧迫。为了解决动态环境中LLM智能体缺乏标准化评估基准的问题，本研究引入了RAS-Eval。", "method": "本研究引入了RAS-Eval，一个支持模拟和真实世界工具执行的综合安全基准。RAS-Eval包含80个测试用例和3,802个攻击任务，这些任务映射到11个通用弱点枚举（CWE）类别。工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。研究评估了6个最先进的LLM在不同场景下的表现。", "result": "评估揭示了显著的漏洞：攻击平均降低了智能体任务完成率（TCR）36.78%，并在学术环境中实现了85.65%的成功率。值得注意的是，安全能力遵循缩放定律，更大的模型表现优于较小的模型。", "conclusion": "本研究的发现揭示了真实世界智能体部署中的关键风险，并为未来的安全研究提供了一个基础框架。", "translation": "大型语言模型（LLM）智能体在医疗和金融等关键领域的快速部署，使得对鲁棒安全框架的需求变得紧迫。为了解决动态环境中这些智能体缺乏标准化评估基准的问题，我们引入了RAS-Eval，一个支持模拟和真实世界工具执行的综合安全基准。RAS-Eval包含80个测试用例和3,802个攻击任务，这些任务映射到11个通用弱点枚举（CWE）类别，工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。我们评估了6个最先进的LLM在不同场景下的表现，揭示了显著的漏洞：攻击平均降低了智能体任务完成率（TCR）36.78%，并在学术环境中实现了85.65%的成功率。值得注意的是，安全能力遵循缩放定律，更大的模型表现优于较小的模型。我们的发现揭示了真实世界智能体部署中的关键风险，并为未来的安全研究提供了一个基础框架。代码和数据可在https://github.com/lanzer-tree/RAS-Eval获取。", "summary": "本研究介绍了RAS-Eval，一个旨在解决LLM智能体在真实世界环境中缺乏标准化安全评估基准的问题。该基准包含80个测试用例和3,802个攻击任务，覆盖11个CWE类别，并支持多种工具格式。通过对6个先进LLM的评估，研究发现现有智能体存在显著安全漏洞，攻击可大幅降低任务完成率并取得高成功率。研究还指出LLM的安全能力遵循缩放定律。这些发现强调了LLM智能体在实际部署中的风险，并为未来的安全研究奠定了基础。", "keywords": "LLM智能体, 安全评估, RAS-Eval, 漏洞, 基准测试", "comments": "RAS-Eval的创新之处在于其提供了首个支持模拟和真实世界工具执行的LLM智能体安全综合基准，填补了该领域评估标准的空白。其重要性在于通过量化展示现有LLM智能体的安全漏洞，包括任务完成率的显著下降和高攻击成功率，揭示了在医疗、金融等关键领域部署LLM智能体的潜在风险。此外，发现安全能力遵循缩放定律也为模型开发和部署提供了重要指导。这项工作为未来LLM智能体的安全研究和更安全的部署奠定了坚实基础。"}}
{"id": "2506.15325", "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook", "authors": ["Festus Adedoyin", "Huseyin Dogan"], "summary": "Advancements in Artificial Intelligence (AI) have significantly transformed\nthe financial industry, enabling the development of more personalised and\nadaptable financial products and services. This research paper explores various\ninstances where Human-Centred AI (HCAI) has facilitated these advancements,\ndrawing from contemporary studies and industry progress. The paper examines how\nthe application of HCAI-powered data analytics, machine learning, and natural\nlanguage processing enables financial institutions to gain a deeper\nunderstanding of their customers' unique needs, preferences, and behavioural\npatterns. This, in turn, allows for the creation of tailored financial\nsolutions that address individual consumer requirements, ultimately enhancing\noverall user experience and satisfaction. Additionally, the study highlights\nthe integration of AI-powered robo-advisory services, which offer customised\ninvestment recommendations and portfolio management tailored to diverse risk\nprofiles and investment goals. Moreover, the paper underscores the role of AI\nin strengthening fraud detection, risk assessment, and regulatory compliance,\nleading to a more secure and adaptable financial landscape. The findings of\nthis research demonstrate the substantial impact of Human-Centred AI on the\nfinancial industry, offering a strategic framework for financial institutions\nto leverage these technologies. By incorporating a User Experience Research\n(UXR) Point of View (PoV), financial institutions can ensure that AI-driven\nsolutions align with user needs and business objectives.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15325v1", "AI": {"title_translation": "金融科技中的以人为本人工智能：开发用户体验（UX）研究观点（PoV）手册", "tldr": "本研究探讨了以人为本的人工智能（HCAI）如何在金融科技中通过数据分析、机器学习和自然语言处理等技术，提升用户体验、个性化服务、风险管理和合规性，并强调结合用户体验研究（UXR）视角的重要性。", "motivation": "金融行业在AI推动下发展出更个性化和适应性的产品服务，本研究旨在探讨以人为本的人工智能（HCAI）如何促进这些进步，并为金融机构提供一个战略框架。", "method": "本研究通过探索当代研究和行业进展中以人为本的人工智能（HCAI）的应用实例，分析其如何通过数据分析、机器学习、自然语言处理以及集成AI驱动的机器人顾问服务等方式，增强金融机构的能力。", "result": "研究结果表明，以人为本的人工智能（HCAI）在金融行业产生了重大影响，通过实现对客户需求的深入理解、创建定制化金融解决方案、增强用户体验和满意度、提供定制化投资建议、强化欺诈检测、风险评估和监管合规性，从而提供了一个战略框架。", "conclusion": "结论是以人为本的人工智能（HCAI）对金融行业具有深远影响，金融机构应采纳用户体验研究（UXR）的视角来利用这些技术，确保AI驱动的解决方案符合用户需求和业务目标。", "translation": "人工智能（AI）的进步显著改变了金融行业，使得开发更个性化和适应性的金融产品与服务成为可能。本研究论文借鉴当代研究和行业进展，探讨了以人为本的人工智能（HCAI）在何种情况下促进了这些进步。论文审视了HCAI驱动的数据分析、机器学习和自然语言处理的应用如何使金融机构能够更深入地了解客户的独特需求、偏好和行为模式。这反过来又使得能够创建满足个体消费者需求的定制化金融解决方案，最终提升整体用户体验和满意度。此外，本研究还强调了AI驱动的机器人顾问服务的整合，这些服务根据不同的风险偏好和投资目标提供定制化的投资建议和投资组合管理。此外，论文强调了AI在加强欺诈检测、风险评估和监管合规性方面的作用，从而促成了一个更安全、更具适应性的金融格局。本研究的结果表明，以人为本的人工智能对金融行业产生了实质性影响，为金融机构提供了一个利用这些技术的战略框架。通过融入用户体验研究（UXR）的视角，金融机构可以确保AI驱动的解决方案与用户需求和业务目标保持一致。", "summary": "本文探讨了以人为本的人工智能（HCAI）在金融科技领域的应用及其对行业转型的作用。研究指出HCAI通过数据分析、机器学习和自然语言处理等技术，帮助金融机构深入理解客户需求，提供个性化金融产品、增强用户体验、优化机器人顾问服务、加强欺诈检测、风险评估和合规性。论文强调，金融机构应结合用户体验研究（UXR）视角，以确保AI解决方案与用户需求和业务目标一致，从而有效利用这些技术。", "keywords": "以人为本人工智能, 金融科技, 用户体验研究, 数据分析, 机器人顾问服务", "comments": "本文创新性地将“以人为本”的视角引入金融科技中的AI应用，并提出了一个用户体验研究（UXR）的“观点手册”概念，这对于指导金融机构在AI部署中平衡技术效率和用户中心性具有重要意义。其价值在于提供了一个战略框架，帮助金融机构更好地利用AI技术，同时确保用户满意度和业务目标的实现。"}}
{"id": "2506.15146", "title": "TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality", "authors": ["Masaki Murooka", "Takahiro Hoshi", "Kensuke Fukumitsu", "Shimpei Masuda", "Marwan Hamze", "Tomoya Sasaki", "Mitsuharu Morisawa", "Eiichi Yoshida"], "summary": "Manipulation with whole-body contact by humanoid robots offers distinct\nadvantages, including enhanced stability and reduced load. On the other hand,\nwe need to address challenges such as the increased computational cost of\nmotion generation and the difficulty of measuring broad-area contact. We\ntherefore have developed a humanoid control system that allows a humanoid robot\nequipped with tactile sensors on its upper body to learn a policy for\nwhole-body manipulation through imitation learning based on human teleoperation\ndata. This policy, named tactile-modality extended ACT (TACT), has a feature to\ntake multiple sensor modalities as input, including joint position, vision, and\ntactile measurements. Furthermore, by integrating this policy with retargeting\nand locomotion control based on a biped model, we demonstrate that the\nlife-size humanoid robot RHP7 Kaleido is capable of achieving whole-body\ncontact manipulation while maintaining balance and walking. Through detailed\nexperimental verification, we show that inputting both vision and tactile\nmodalities into the policy contributes to improving the robustness of\nmanipulation involving broad and delicate contact.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15146v1", "AI": {"title_translation": "TACT：基于触觉模态的深度模仿学习实现仿人机器人全身接触操作", "tldr": "本文介绍了TACT，一种通过深度模仿学习使仿人机器人能够进行全身接触操作的策略，该策略利用触觉和其他传感器模态，并展示了其操作鲁棒性的提高。", "motivation": "仿人机器人的全身接触操作具有增强稳定性、减轻负载等显著优势，但同时也面临运动生成计算成本高和难以测量大面积接触的挑战。本文旨在开发一种系统来利用这些优势并克服这些挑战。", "method": "开发了一个仿人机器人控制系统，该系统允许配备上身触觉传感器的仿人机器人通过基于人类远程操作数据的模仿学习来学习全身操作策略。该策略命名为TACT（触觉模态扩展ACT），能够接收关节位置、视觉和触觉测量等多种传感器模态作为输入。此外，该策略与基于双足模型的重定向和运动控制相结合，并在真人大小的仿人机器人RHP7 Kaleido上进行了演示。", "result": "真人大小的仿人机器人RHP7 Kaleido能够实现全身接触操作，同时保持平衡和行走。详细的实验验证表明，将视觉和触觉模态都输入到策略中有助于提高涉及广泛和精细接触的操作的鲁棒性。", "conclusion": "本文成功开发并验证了TACT策略，该策略通过深度模仿学习和触觉反馈，使仿人机器人能够进行鲁棒的全身接触操作，解决了相关技术挑战并提升了机器人交互能力。", "translation": "仿人机器人全身接触操作具有显著优势，包括增强稳定性和减轻负载。另一方面，我们需要解决运动生成计算成本增加和测量大面积接触困难等挑战。因此，我们开发了一种仿人机器人控制系统，该系统允许配备上身触觉传感器的仿人机器人通过基于人类远程操作数据的模仿学习来学习全身操作策略。这种策略名为触觉模态扩展ACT（TACT），其特点是能够将多种传感器模态作为输入，包括关节位置、视觉和触觉测量。此外，通过将该策略与基于双足模型的重定向和运动控制相结合，我们证明了真人大小的仿人机器人RHP7 Kaleido能够实现全身接触操作，同时保持平衡和行走。通过详细的实验验证，我们表明将视觉和触觉模态都输入到策略中有助于提高涉及广泛和精细接触的操作的鲁棒性。", "summary": "本文提出了一种名为TACT的深度模仿学习策略，旨在使仿人机器人能够进行全身接触操作。该策略通过人类远程操作数据进行学习，并整合了关节位置、视觉和触觉等多种传感器模态输入。通过将TACT与双足运动控制相结合，真人大小的RHP7 Kaleido机器人被证明能够实现稳定的全身接触操作。实验结果表明，同时利用视觉和触觉模态显著提升了机器人处理广泛而精细接触任务的鲁棒性。", "keywords": "全身接触操作, 深度模仿学习, 触觉感知, 仿人机器人, 鲁棒性", "comments": "本文的创新之处在于成功地将触觉感知整合到深度模仿学习框架中，以实现仿人机器人复杂的全身接触操作。这有效解决了大面积接触测量等挑战，并显著提高了操作的鲁棒性，对于提升机器人在非结构化环境中的灵巧性和稳定性具有重要意义。"}}
{"id": "2506.15264", "title": "Centroid Approximation for Byzantine-Tolerant Federated Learning", "authors": ["Mélanie Cambus", "Darya Melnyk", "Tijana Milentijević", "Stefan Schmid"], "summary": "Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.", "comment": "19 pages, 10 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15264v1", "AI": {"title_translation": "拜占庭容错联邦学习中的质心近似", "tldr": "该论文研究了联邦学习在拜占庭行为下的鲁棒性，并首次提出了质心近似的下界，以及一个新的算法来改进近似。", "motivation": "联邦学习在分布式设置中允许客户端本地保存数据进行模型训练。现有研究主要使用平均作为聚合规则，并关注输入需要满足的收敛条件。然而，对于联邦学习在拜占庭行为下的鲁棒性缺乏深入了解，且发现平均/质心与分布式计算中的有效性条件之间存在权衡。", "method": "研究了各种有效性条件对平均值近似的保证能力，并分析了在实验设置中良好近似效果不佳的原因（拜占庭异常值）。主要贡献是提出了在盒式有效性下质心近似的第一个下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ 和一个上界 $2\\min\\{n,\\sqrt{d}\\}$。此外，提出了一个在凸有效性下实现 $\\sqrt{2d}$-近似的新算法。通过联邦随机梯度下降和联邦平均设置中的经验评估来补充分析结果。", "result": "证明了单独的有效性条件不能保证良好的平均近似。发现在实验设置中，即使达到良好近似，由于拜占庭异常值，结果也不佳。提出了在盒式有效性下的质心近似的第一个下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$。提供了一个上界 $2\\min\\{n,\\sqrt{d}\\}$。提出了一个在凸有效性下实现 $\\sqrt{2d}$-近似的新算法，该算法也证明了现有文献中的下界是紧的。所有提出的界限都可以在分布式点对点设置中实现。", "conclusion": "本研究提出了拜占庭容错联邦学习中质心近似的分析界限，包括一个首次提出的下界和一个改进的上界。此外，引入了一个新的近似算法，该算法不仅在理论上达到良好近似，而且证明了现有文献中相关下界的紧性。所有提出的界限在分布式点对点设置中也适用，并通过实证评估验证了分析结果。", "translation": "联邦学习允许每个客户端在分布式环境中训练机器学习模型时，将其数据保留在本地。最近的大量研究确定了输入必须满足的要求，以保证训练循环的收敛性。这类工作使用平均作为训练模型的聚合规则。特别是，我们感兴趣的是联邦学习是否对拜占庭行为具有鲁棒性，并观察和研究了平均/质心与分布式计算中的有效性条件之间的权衡。我们表明，单独的各种有效性条件并不能保证对平均值的良好近似。此外，我们表明，由于可能存在的拜占庭异常值，在实验设置中达到良好近似并不能获得好的结果。我们的主要贡献是在文献中经常考虑的盒式有效性下，质心近似的第一个下界为 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$，其中 $n$ 是客户端数量，$t$ 是拜占庭故障数量的上限，$d$ 是机器学习模型的维度。我们通过对 $n<d$ 情况的新分析，补充了这个下界，给出了一个上界 $2\\min\\{n,\\sqrt{d}\\}$。此外，我们提出了一种在凸有效性下实现 $\\sqrt{2d}$-近似的新算法，这也证明了文献中现有的下界是紧的。我们表明，所有提出的界限也可以在分布式点对点设置中实现。我们通过在联邦随机梯度下降和联邦平均设置中的经验评估来补充我们的分析结果。", "summary": "该论文深入探讨了联邦学习在存在拜占庭行为时的鲁棒性问题。研究发现，传统的平均聚合和有效性条件并不能有效应对拜占庭攻击，甚至可能导致不佳的近似效果。为此，作者首次提出了在盒式有效性下质心近似的理论下界和上界，并针对 $n<d$ 的情况提供了新的分析。更重要的是，论文提出了一种新的算法，能够在凸有效性下实现更优的 $\\sqrt{2d}$-近似，并证明了现有文献中相关下界的紧性。这些理论发现得到了经验评估的验证，并且在分布式点对点环境中也成立。", "keywords": "联邦学习, 拜占庭容错, 质心近似, 下界, 新算法", "comments": "这篇论文的创新点在于首次为拜占庭容错联邦学习中的质心近似提供了明确的理论下界和上界，这对于理解此类系统在恶意攻击下的性能极限至关重要。此外，提出的新算法不仅在理论上实现了更好的近似效果，还证明了现有理论的紧性，这为该领域的研究提供了坚实的理论基础。论文结合了理论分析和实证评估，增强了研究结果的说服力。"}}
{"id": "2506.14856", "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction", "authors": ["Zhengquan Zhang", "Feng Xu", "Mengmi Zhang"], "summary": "Some perspectives naturally provide more information than others. How can an\nAI system determine which viewpoint offers the most valuable insight for\naccurate and efficient 3D object reconstruction? Active view selection (AVS)\nfor 3D reconstruction remains a fundamental challenge in computer vision. The\naim is to identify the minimal set of views that yields the most accurate 3D\nreconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian\nSplatting, from a current observation and computing uncertainty for each\ncandidate viewpoint, we introduce a novel AVS approach guided by neural\nuncertainty maps predicted by a lightweight feedforward deep neural network,\nnamed UPNet. UPNet takes a single input image of a 3D object and outputs a\npredicted uncertainty map, representing uncertainty values across all possible\ncandidate viewpoints. By leveraging heuristics derived from observing many\nnatural objects and their associated uncertainty patterns, we train UPNet to\nlearn a direct mapping from viewpoint appearance to uncertainty in the\nunderlying volumetric representations. Next, our approach aggregates all\npreviously predicted neural uncertainty maps to suppress redundant candidate\nviewpoints and effectively select the most informative one. Using these\nselected viewpoints, we train 3D neural rendering models and evaluate the\nquality of novel view synthesis against other competitive AVS methods.\nRemarkably, despite using half of the viewpoints than the upper bound, our\nmethod achieves comparable reconstruction accuracy. In addition, it\nsignificantly reduces computational overhead during AVS, achieving up to a 400\ntimes speedup along with over 50\\% reductions in CPU, RAM, and GPU usage\ncompared to baseline methods. Notably, our approach generalizes effectively to\nAVS tasks involving novel object categories, without requiring any additional\ntraining.", "comment": "9 pages, 3 figures in the main text. Under review for NeurIPS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14856v1", "AI": {"title_translation": "窥探未知：基于神经不确定性图的主动视角选择用于三维重建", "tldr": "本文提出了一种名为UPNet的轻量级前馈深度神经网络，通过预测神经不确定性图来指导主动视角选择，显著提高了三维重建的效率和准确性，同时大幅降低了计算开销。", "motivation": "在三维重建中，主动视角选择（AVS）是一个基本挑战，目标是识别最少量的视图以实现最准确的三维重建。传统的AVS方法通常需要从当前观测中学习辐射场并计算每个候选视角的不确定性，这计算成本高昂。", "method": "本文引入了一种新颖的AVS方法，该方法由UPNet预测的神经不确定性图指导。UPNet是一个轻量级前馈深度神经网络，它接收单个三维物体输入图像，并输出一个预测的不确定性图，表示所有可能候选视角的不确定性值。UPNet通过利用从观察许多自然物体及其相关不确定性模式中得出的启发式方法进行训练，以学习从视角外观到底层体积表示中不确定性的直接映射。随后，该方法聚合所有先前预测的神经不确定性图，以抑制冗余候选视角并有效选择信息量最大的视角。最后，利用这些选定的视角训练三维神经渲染模型。", "result": "尽管使用的视角数量是上限的一半，但本文方法实现了可比较的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提升高达400倍，CPU、RAM和GPU使用率降低了50%以上。", "conclusion": "本文提出的基于神经不确定性图的主动视角选择方法，在保证重建精度的同时，显著提高了三维重建的效率并降低了计算资源消耗，并且对新物体类别具有良好的泛化能力，无需额外训练。", "translation": "有些视角自然比其他视角提供更多的信息。AI系统如何确定哪个视点能为准确高效的三维物体重建提供最有价值的见解？用于三维重建的主动视角选择（AVS）仍然是计算机视觉中的一个基本挑战。其目标是识别最少量的视图，以产生最准确的三维重建。我们没有像NeRF或3D高斯飞溅那样从当前观测中学习辐射场并计算每个候选视点的不确定性，而是引入了一种由轻量级前馈深度神经网络UPNet预测的神经不确定性图引导的新颖AVS方法。UPNet接收单个三维物体输入图像，并输出一个预测的不确定性图，表示所有可能候选视点的不确定性值。通过利用从观察许多自然物体及其相关不确定性模式中得出的启发式方法，我们训练UPNet学习从视点外观到底层体积表示中不确定性的直接映射。接下来，我们的方法聚合所有先前预测的神经不确定性图，以抑制冗余候选视点并有效选择信息量最大的视点。使用这些选定的视点，我们训练三维神经渲染模型，并根据其他有竞争力的AVS方法评估新视图合成的质量。值得注意的是，尽管使用的视点数量是上限的一半，但我们的方法实现了可比较的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提升高达400倍，CPU、RAM和GPU使用率降低了50%以上。值得注意的是，我们的方法有效地泛化到涉及新物体类别的AVS任务，无需任何额外训练。", "summary": "本文提出了一种新颖的主动视角选择（AVS）方法，通过引入一个轻量级前馈深度神经网络UPNet来解决三维重建中的视角选择挑战。UPNet能够从单个输入图像预测神经不确定性图，指导选择最具信息量的视角。该方法通过聚合不确定性图来避免冗余视角选择，并显著提高了三维重建的效率和精度。实验结果表明，该方法在仅使用一半视角的情况下仍能达到可比的重建精度，同时计算开销大幅降低，并对新物体类别具有良好的泛化能力。", "keywords": "主动视角选择, 三维重建, 神经不确定性图, UPNet, 深度学习", "comments": "本文提出了一种创新的主动视角选择（AVS）方法，通过引入轻量级UPNet直接预测神经不确定性图，显著提升了三维重建的效率和资源利用率。其核心创新在于避免了传统方法中昂贵的辐射场学习和不确定性计算，而是通过学习从视角外观到不确定性的直接映射。这种方法不仅实现了计算上的巨大加速（最高400倍），同时保持了重建精度，并且展现出卓越的泛化能力，无需针对新物体进行额外训练，这在实际应用中具有重要价值。该研究为高效三维重建提供了一条有前景的新路径。"}}
{"id": "2309.13018", "title": "Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model", "authors": ["Jiamin Xie", "Ke Li", "Jinxi Guo", "Andros Tjandra", "Yuan Shangguan", "Leda Sari", "Chunyang Wu", "Junteng Jia", "Jay Mahadeokar", "Ozlem Kalinli"], "summary": "Neural network pruning offers an effective method for compressing a\nmultilingual automatic speech recognition (ASR) model with minimal performance\nloss. However, it entails several rounds of pruning and re-training needed to\nbe run for each language. In this work, we propose the use of an adaptive\nmasking approach in two scenarios for pruning a multilingual ASR model\nefficiently, each resulting in sparse monolingual models or a sparse\nmultilingual model (named as Dynamic ASR Pathways). Our approach dynamically\nadapts the sub-network, avoiding premature decisions about a fixed sub-network\nstructure. We show that our approach outperforms existing pruning methods when\ntargeting sparse monolingual models. Further, we illustrate that Dynamic ASR\nPathways jointly discovers and trains better sub-networks (pathways) of a\nsingle multilingual model by adapting from different sub-network\ninitializations, thereby reducing the need for language-specific pruning.", "comment": null, "cate": "eess.AS", "url": "http://arxiv.org/abs/2309.13018v2", "AI": {"title_translation": "动态ASR路径：一种用于多语言ASR模型高效剪枝的自适应掩码方法", "tldr": "本文提出了一种自适应掩码方法，用于高效剪枝多语言ASR模型，可生成稀疏的单语言模型或稀疏的多语言模型（动态ASR路径），其表现优于现有方法并减少了特定语言的剪枝需求。", "motivation": "现有神经网络剪枝方法在压缩多语言自动语音识别（ASR）模型时，需要为每种语言进行多轮剪枝和再训练，效率低下。", "method": "提出了一种自适应掩码方法，在两种场景下高效剪枝多语言ASR模型：生成稀疏的单语言模型或稀疏的多语言模型（命名为动态ASR路径）。该方法动态调整子网络，避免过早固定子网络结构。动态ASR路径通过适应不同的子网络初始化，共同发现并训练单个多语言模型中更优的子网络。", "result": "该方法在生成稀疏单语言模型时优于现有剪枝方法。此外，动态ASR路径减少了对特定语言剪枝的需求。", "conclusion": "自适应掩码方法，特别是动态ASR路径，为多语言ASR模型提供了一种高效且有效的剪枝方案，在稀疏单语言模型方面表现更优，并减少了特定语言的剪枝需求。", "translation": "神经网络剪枝为压缩多语言自动语音识别（ASR）模型提供了一种有效方法，且性能损失最小。然而，它需要为每种语言运行多轮剪枝和再训练。在这项工作中，我们提出了在两种场景中使用自适应掩码方法来高效剪枝多语言ASR模型，每种场景分别产生稀疏的单语言模型或稀疏的多语言模型（命名为动态ASR路径）。我们的方法动态地适应子网络，避免了对固定子网络结构做出过早的决定。我们表明，当目标是稀疏的单语言模型时，我们的方法优于现有的剪枝方法。此外，我们阐明了动态ASR路径通过适应不同的子网络初始化，共同发现和训练单个多语言模型中更好的子网络（路径），从而减少了对特定语言剪枝的需求。", "summary": "本文提出了一种名为“动态ASR路径”的自适应掩码方法，旨在高效剪枝多语言ASR模型。该方法能在两种场景下应用，分别生成稀疏的单语言模型或一个稀疏的多语言模型。其核心优势在于动态调整子网络结构，避免了传统剪枝中固定子网络带来的限制。实验结果表明，该方法在生成稀疏单语言模型时优于现有剪枝方法，并且能有效减少多语言模型对特定语言剪枝的需求。", "keywords": "ASR, 剪枝, 多语言, 自适应掩码, 神经网络", "comments": "该论文的创新点在于其提出的“自适应掩码方法”，通过动态调整子网络结构，显著提高了多语言ASR模型剪枝的效率，并减少了为每种语言单独剪枝的必要性。这对于多语言模型的部署和维护具有重要意义。"}}
{"id": "2506.14808", "title": "PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models", "authors": ["Jenny Schmalfuss", "Nadine Chang", "Vibashan VS", "Maying Shen", "Andres Bruhn", "Jose M. Alvarez"], "summary": "Vision language models (VLMs) respond to user-crafted text prompts and visual\ninputs, and are applied to numerous real-world problems. VLMs integrate visual\nmodalities with large language models (LLMs), which are well known to be\nprompt-sensitive. Hence, it is crucial to determine whether VLMs inherit this\ninstability to varying prompts. We therefore investigate which prompt\nvariations VLMs are most sensitive to and which VLMs are most agnostic to\nprompt variations. To this end, we introduce PARC (Prompt Analysis via\nReliability and Calibration), a VLM prompt sensitivity analysis framework built\non three pillars: (1) plausible prompt variations in both the language and\nvision domain, (2) a novel model reliability score with built-in guarantees,\nand (3) a calibration step that enables dataset- and prompt-spanning prompt\nvariation analysis. Regarding prompt variations, PARC's evaluation shows that\nVLMs mirror LLM language prompt sensitivity in the vision domain, and most\ndestructive variations change the expected answer. Regarding models,\noutstandingly robust VLMs among 22 evaluated models come from the InternVL2\nfamily. We further find indications that prompt sensitivity is linked to\ntraining data. The code will be at https://github.com/NVlabs/PARC.", "comment": "Accepted to CVPR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14808v1", "AI": {"title_translation": "PARC：一个揭示视觉语言模型内部对称性的量化框架", "tldr": "PARC是一个新的框架，用于量化分析视觉语言模型（VLMs）对提示变化的敏感性，发现VLMs像LLMs一样对提示敏感，并且InternVL2模型家族表现出卓越的鲁棒性。", "motivation": "视觉语言模型（VLMs）结合了视觉和大型语言模型（LLMs），而LLMs众所周知对提示敏感。因此，确定VLMs是否继承了这种对不同提示的不稳定性至关重要。本研究旨在找出VLMs对哪些提示变体最敏感，以及哪些VLMs对提示变体最不敏感。", "method": "引入了PARC（Prompt Analysis via Reliability and Calibration）框架，该框架建立在三个支柱之上：1) 语言和视觉领域中合理的提示变体；2) 具有内置保证的新颖模型可靠性分数；3) 实现数据集和提示跨度提示变体分析的校准步骤。", "result": "PARC的评估表明，VLMs在视觉领域反映了LLM在语言提示方面的敏感性，并且最具破坏性的变体改变了预期答案。在评估的22个模型中，InternVL2家族表现出卓越的鲁棒性。此外，研究发现提示敏感性与训练数据有关。", "conclusion": "VLMs继承了LLMs对提示的敏感性，并且这种敏感性在视觉领域同样存在。InternVL2模型家族在提示鲁棒性方面表现出色，且提示敏感性可能与训练数据相关。", "translation": "视觉语言模型（VLMs）响应用户制作的文本提示和视觉输入，并应用于众多现实世界问题。VLMs将视觉模态与大型语言模型（LLMs）集成，而LLMs众所周知对提示敏感。因此，确定VLMs是否继承了这种对不同提示的不稳定性至关重要。我们因此研究了VLMs对哪些提示变体最敏感，以及哪些VLMs对提示变体最不敏感。为此，我们引入了PARC（通过可靠性和校准进行的提示分析），这是一个建立在三个支柱上的VLM提示敏感性分析框架：(1) 语言和视觉领域中合理的提示变体，(2) 具有内置保证的新颖模型可靠性分数，以及 (3) 一个能够实现数据集和提示跨度提示变体分析的校准步骤。关于提示变体，PARC的评估表明，VLMs在视觉领域反映了LLM的语言提示敏感性，并且最具破坏性的变体改变了预期答案。关于模型，在评估的22个模型中，InternVL2家族表现出卓越的鲁棒性。我们进一步发现提示敏感性与训练数据有关的迹象。代码将在https://github.com/NVlabs/PARC提供。", "summary": "本研究提出了PARC框架，用于量化分析视觉语言模型（VLMs）对不同提示的敏感性。PARC通过分析语言和视觉领域的提示变体、引入模型可靠性分数和校准步骤，评估了VLMs的稳定性。研究发现VLMs确实继承了大型语言模型（LLMs）的提示敏感性，尤其是在视觉领域。在测试的22个模型中，InternVL2家族表现出最佳的提示鲁棒性，并且提示敏感性似乎与训练数据有关。", "keywords": "视觉语言模型, 提示敏感性, PARC, 模型鲁棒性, InternVL2", "comments": "PARC框架的创新之处在于其量化分析VLM提示敏感性的方法，特别是结合了视觉和语言领域的提示变体，并引入了模型可靠性分数和校准步骤。这项工作对于理解VLMs的鲁棒性及其在实际应用中的表现至关重要，有助于开发者选择更稳定的模型或改进模型训练策略。发现InternVL2家族的优异表现以及提示敏感性与训练数据的关联，为未来的VLM研究和开发提供了宝贵的见解。"}}
{"id": "2506.15376", "title": "Comparison of Innovative Strategies for the Coverage Problem: Path Planning, Search Optimization, and Applications in Underwater Robotics", "authors": ["Ahmed Ibrahim", "Francisco F. C. Rego", "Éric Busvelle"], "summary": "In many applications, including underwater robotics, the coverage problem\nrequires an autonomous vehicle to systematically explore a defined area while\nminimizing redundancy and avoiding obstacles. This paper investigates coverage\npath planning strategies to enhance the efficiency of underwater gliders,\nparticularly in maximizing the probability of detecting a radioactive source\nwhile ensuring safe navigation.\n  We evaluate three path-planning approaches: the Traveling Salesman Problem\n(TSP), Minimum Spanning Tree (MST), and Optimal Control Problem (OCP).\nSimulations were conducted in MATLAB, comparing processing time, uncovered\nareas, path length, and traversal time. Results indicate that OCP is preferable\nwhen traversal time is constrained, although it incurs significantly higher\ncomputational costs. Conversely, MST-based approaches provide faster but less\noptimal solutions. These findings offer insights into selecting appropriate\nalgorithms based on mission priorities, balancing efficiency and computational\nfeasibility.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15376v1", "AI": {"title_translation": "覆盖问题创新策略的比较：路径规划、搜索优化及在水下机器人中的应用", "tldr": "论文比较了三种路径规划策略（TSP、MST、OCP）在水下机器人覆盖问题中的性能，发现OCP在时间受限时最优但计算成本高，MST更快但效果次优。", "motivation": "解决水下机器人覆盖问题，提高水下滑翔机在探索特定区域（特别是探测放射源）时的效率，同时最小化冗余并避开障碍物。", "method": "评估了三种路径规划方法：旅行商问题 (TSP)、最小生成树 (MST) 和最优控制问题 (OCP)。在MATLAB中进行仿真，比较了处理时间、未覆盖区域、路径长度和遍历时间。", "result": "OCP在遍历时间受限时表现更优，但计算成本显著更高。MST方法更快但解决方案次优。", "conclusion": "研究结果为根据任务优先级选择合适的算法提供了见解，以平衡效率和计算可行性。", "translation": "在许多应用中，包括水下机器人领域，覆盖问题要求自主载具系统地探索一个限定区域，同时最小化冗余并避开障碍物。本文研究了覆盖路径规划策略，以提高水下滑翔机的效率，特别是在确保安全导航的同时最大化探测放射源的概率。\n我们评估了三种路径规划方法：旅行商问题 (TSP)、最小生成树 (MST) 和最优控制问题 (OCP)。仿真在MATLAB中进行，比较了处理时间、未覆盖区域、路径长度和遍历时间。结果表明，当遍历时间受限时，OCP更优，尽管其计算成本显著更高。相反，基于MST的方法更快但解决方案次优。这些发现为根据任务优先级选择合适的算法提供了见解，以平衡效率和计算可行性。", "summary": "本文比较了旅行商问题 (TSP)、最小生成树 (MST) 和最优控制问题 (OCP) 三种路径规划策略在水下机器人覆盖问题中的应用。研究旨在提高水下滑翔机探测效率并确保安全导航。通过MATLAB仿真，评估了这些策略在处理时间、未覆盖区域、路径长度和遍历时间方面的表现。结果显示，OCP在遍历时间受限时效果最佳但计算成本高，而MST方法速度快但优化程度较低。研究为根据任务优先级选择合适的算法提供了指导。", "keywords": "覆盖问题, 路径规划, 水下机器人, TSP, MST, OCP", "comments": "这篇论文通过比较三种经典的优化算法在水下机器人覆盖问题中的应用，为实际任务提供了实用的算法选择依据。其创新点在于将这些通用算法应用于水下机器人这一特定且复杂的场景，并量化比较了它们的性能权衡，特别是考虑了计算成本和遍历时间的限制。"}}
{"id": "2506.15562", "title": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and Efficient Attention", "authors": ["Syed Haider Ali", "Asrar Ahmad", "Muhammad Ali", "Asifullah Khan", "Muhammad Shahban", "Nadeem Shaukat"], "summary": "Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.", "comment": "16 pages, 5 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15562v1", "AI": {"title_translation": "自动化MRI肿瘤分割：基于混合U-Net结合Transformer和高效注意力机制", "tldr": "本研究针对本地化MRI肿瘤分割，提出了一种结合U-Net、Transformer和多种注意力机制的混合模型，并在有限的本地数据集上实现了有竞争力的分割性能，强调了特定站点模型开发的重要性。", "motivation": "肿瘤的精确自动分割对于放疗治疗计划优化至关重要；现有AI分割模型主要基于大型公共数据集训练，缺乏本地患者群体的异质性；需要针对本地数据集开发AI肿瘤分割模型，以便直接集成到医院软件中，实现高效准确的肿瘤治疗规划和执行。", "method": "使用来自当地医院的MRI数据集进行训练，并遵循严格的隐私保护；开发了鲁棒的数据管道，用于DICOM提取、预处理和广泛的图像增强，总计生成6080张图像用于训练；提出了一种新颖的混合架构，将基于U-Net的卷积神经网络与Transformer瓶颈和互补注意力模块（包括高效注意力、Squeeze-and-Excitation (SE) 块、卷积块注意力模块 (CBAM) 和ResNeXt 块）相结合；训练时使用最大批处理大小为8，编码器用ImageNet预训练权重初始化，通过检查点在双NVIDIA T4 GPU上训练模型。", "result": "在本地MRI数据集上的定量评估结果：Dice相似系数为0.764，Intersection over Union (IoU) 为0.736；表明在有限数据下仍具有竞争力的性能。", "conclusion": "本研究证明了在有限数据下，混合U-Net与Transformer及注意力机制的模型在本地MRI肿瘤分割上具有竞争力；强调了针对特定站点的模型开发对于临床部署的重要性。", "translation": "癌症是一种异常生长，有可能局部侵袭并转移到远处器官。为了优化放疗治疗计划，需要对肿瘤及周围正常组织进行精确的自动分割。最近基于AI的分割模型通常在大型公共数据集上进行训练，这些数据集缺乏本地患者群体的异质性。尽管这些研究推动了基于AI的医学图像分割，但为了将AI肿瘤分割模型直接开发并集成到医院软件中，以实现高效准确的肿瘤治疗规划和执行，对本地数据集的研究是必要的。本研究在从当地医院获取的严格隐私保护下的磁共振成像（MRI）数据集上，使用计算高效的混合UNet-Transformer模型来增强肿瘤分割。我们开发了一个鲁棒的数据管道，用于无缝的DICOM提取和预处理，随后进行了广泛的图像增强，以确保模型在不同临床环境中的泛化能力，最终用于训练的数据集总计6080张图像。我们新颖的架构将基于UNet的卷积神经网络与Transformer瓶颈和互补注意力模块（包括高效注意力、Squeeze-and-Excitation (SE) 块、卷积块注意力模块 (CBAM) 和ResNeXt 块）相结合。为了加速收敛并减少计算需求，我们使用了最大批处理大小为8，并用ImageNet预训练权重初始化编码器，通过检查点在双NVIDIA T4 GPU上训练模型，以克服Kaggle的运行时限制。在本地MRI数据集上的定量评估结果显示，Dice相似系数为0.764，交并比（IoU）为0.736，尽管数据有限，但仍展现出有竞争力的性能，并强调了特定站点模型开发对于临床部署的重要性。", "summary": "本研究旨在解决现有AI肿瘤分割模型在本地患者数据异质性上的不足，提出了一种基于混合U-Net、Transformer和多种注意力机制的新型架构，用于MRI肿瘤的自动化分割。该模型在从本地医院获取的6080张图像数据集上进行训练，并结合了鲁棒的数据预处理和增强管道。定量评估结果显示，在有限数据下，该模型在Dice相似系数和IoU方面均达到了有竞争力的性能，凸显了针对特定站点开发模型对临床应用的重要性。", "keywords": "MRI肿瘤分割, 混合U-Net, Transformer, 注意力机制, 医疗图像分割", "comments": "创新性在于结合了U-Net和Transformer，并通过引入多种注意力机制优化特征表示，旨在提升医学图像分割表现。重要性在于强调了在本地数据集上开发和验证AI模型对临床实践集成的关键作用。局限性可能在于Dice系数和IoU值仍有提升空间，未来可探索更先进的模型设计或联邦学习等方法。"}}
{"id": "2506.15208", "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals", "authors": ["Andrea Cadeddu", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Enrico Motta", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).", "comment": "Submitted to IEEE Access", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15208v1", "AI": {"title_translation": "大型语言模型任务适应技术在识别可持续发展目标中的比较研究", "tldr": "本研究比较了大型语言模型（LLMs）及其任务适应技术（零样本学习、少样本学习、微调）在识别可持续发展目标（SDGs）文本分类任务上的表现，发现通过提示工程优化后的小型模型能与大型模型媲美。", "motivation": "跟踪联合国可持续发展目标（SDGs）的进展因数据规模庞大和复杂性而困难。文本分类模型，特别是大型语言模型（LLMs），是自动化分析文本的有效工具。本研究旨在评估LLMs及其任务适应技术在SDG文本分类中的有效性。", "method": "本研究分析了多种专有和开源大型语言模型（LLMs），用于单标签、多类别文本分类任务，重点是识别可持续发展目标（SDGs）。同时评估了任务适应技术，包括零样本学习（Zero-Shot Learning）、少样本学习（Few-Shot Learning）和微调（Fine-Tuning）在该领域的有效性。", "result": "结果显示，通过提示工程优化后的小型模型可以与OpenAI的GPT等大型模型表现相当。", "conclusion": "通过有效的提示工程，小型大型语言模型在识别可持续发展目标文本分类任务上可以达到与大型模型相当的性能，这表明在资源受限的环境下，小型模型也能有效应用。", "translation": "2012年，联合国推出了17项可持续发展目标（SDGs），旨在到2030年创建一个更可持续、更美好的未来。然而，由于所涉数据的规模庞大和复杂性，追踪这些目标的进展十分困难。文本分类模型已成为该领域的关键工具，能够自动化分析来自各种来源的大量文本。此外，大型语言模型（LLMs）凭借其识别复杂语言模式和语义的能力，最近在包括文本分类在内的许多自然语言处理任务中被证明是不可或缺的。本研究分析了各种专有和开源LLMs，用于一项侧重于SDGs的单标签、多类别文本分类任务。然后，它还评估了任务适应技术（即上下文学习方法），即零样本学习（Zero-Shot Learning）和少样本学习（Few-Shot Learning），以及微调（Fine-Tuning）在该领域内的有效性。结果显示，经过提示工程优化后的小型模型可以与OpenAI的GPT（生成式预训练变换器）等大型模型表现相当。", "summary": "本研究比较了不同大型语言模型（LLMs）及其任务适应技术（零样本学习、少样本学习、微调）在识别联合国可持续发展目标（SDGs）的文本分类任务中的表现。研究发现，通过提示工程优化的小型模型在性能上可以与OpenAI的GPT等大型模型相媲美，这为SDGs的自动化跟踪提供了新的视角。", "keywords": "大型语言模型,可持续发展目标,文本分类,任务适应,提示工程", "comments": "这项研究的创新之处在于，它不仅比较了不同规模LLM在SDG分类任务上的表现，更强调了任务适应技术（特别是提示工程）对模型性能的提升作用。其重要性在于，为资源有限的环境下选择和优化LLM提供了实用指导，证明了小型模型在特定任务上通过优化可以达到大型模型的水平，降低了对计算资源的需求。"}}
{"id": "2506.15165", "title": "A time-frequency method for acoustic scattering with trapping", "authors": ["Heather Wilber", "Wietse Vaes", "Abinand Gopal", "Gunnar Martinsson"], "summary": "A Fourier transform method is introduced for a class of hybrid time-frequency\nmethods that solve the acoustic scattering problem in regimes where the\nsolution exhibits both highly oscillatory behavior and slow decay in time. This\nextends the applicability of hybrid time-frequency schemes to domains with\ntrapping regions. A fast sinc transform technique for managing highly\noscillatory behavior and long time horizons is combined with a contour\nintegration scheme that improves smoothness properties in the integrand.", "comment": "18 pages, 9 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15165v1", "AI": {"title_translation": "具有陷阱的声散射时频方法", "tldr": "本文介绍了一种新的傅里叶变换方法，用于混合时频方案，以解决在解具有高度振荡和慢衰减特性的区域中的声散射问题，并将其适用范围扩展到具有陷阱的区域。", "motivation": "解决声散射问题，特别是当解表现出高度振荡行为和时间上的慢衰减时，并扩展现有混合时频方案在具有陷阱区域的适用性。", "method": "引入了一种傅里叶变换方法，用于一类混合时频方法。该方法结合了用于处理高度振荡行为和长时间范围的快速sinc变换技术，以及改善被积函数平滑性的等高线积分方案。", "result": "该方法扩展了混合时频方案在具有陷阱区域的适用性。", "conclusion": "通过引入傅里叶变换方法并结合快速sinc变换和等高线积分，成功地将混合时频方案应用于具有陷阱的声散射问题，处理了高度振荡和慢衰减特性。", "translation": "引入了一种傅里叶变换方法，用于解决声散射问题的一类混合时频方法，该问题在解表现出高度振荡行为和时间上慢衰减的区域中存在。这扩展了混合时频方案在具有陷阱区域的适用性。一种用于管理高度振荡行为和长时间范围的快速sinc变换技术与一种改善被积函数平滑性的等高线积分方案相结合。", "summary": "本文提出了一种基于傅里叶变换的混合时频方法，旨在解决声散射问题中解具有强振荡和慢衰减特性的挑战。该方法通过结合快速sinc变换技术处理高度振荡和长时间范围，以及等高线积分方案改善积分平滑性，成功将混合时频方案的适用范围扩展到包含陷阱区域的声学散射问题。", "keywords": "声散射, 时频方法, 傅里叶变换, 陷阱区域, sinc变换", "comments": "这篇论文的创新点在于将傅里叶变换方法引入到混合时频方案中，并结合了快速sinc变换和等高线积分，从而有效处理了声散射问题中高度振荡和慢衰减的复杂特性。其重要性在于扩展了现有混合时频方案在更复杂（如具有陷阱区域）场景下的应用范围，为相关领域的数值模拟提供了新的工具。"}}
{"id": "2506.15388", "title": "Evaluation Pipeline for systematically searching for Anomaly Detection Systems", "authors": ["Florian Rokohl", "Alexander Lehnert", "Marc Reichenbach"], "summary": "Digitalization in the medical world provides major benefits while making it a\ntarget for attackers and thus hard to secure. To deal with network intruders we\npropose an anomaly detection system on hardware to detect malicious clients in\nreal-time. We meet real-time and power restrictions using FPGAs. Overall system\nperformance is achieved via the presented holistic system evaluation.", "comment": "Submitted to 18th HiPEAC Workshop on Reconfigurable Computing\n  (WRC'2024)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15388v1", "AI": {"title_translation": "系统化搜索异常检测系统的评估流程", "tldr": "本文提出了一种基于FPGA的硬件异常检测系统，用于实时检测医疗网络中的恶意客户端，并通过整体系统评估实现高性能。", "motivation": "医疗领域的数字化带来了巨大的好处，但也使其成为攻击者的目标，难以保障安全。需要处理网络入侵者。", "method": "提出一种基于FPGA的硬件异常检测系统，用于实时检测恶意客户端，并满足实时性和功耗限制。通过所提出的整体系统评估来实现整体系统性能。", "result": "通过所提出的整体系统评估，实现了整体系统性能。", "conclusion": "基于FPGA的硬件异常检测系统能够实时检测恶意客户端，并且通过整体评估方法可以实现高性能，满足医疗网络安全需求。", "translation": "医疗领域的数字化带来了巨大的好处，同时也使其成为攻击者的目标，因此难以保障安全。为了应对网络入侵者，我们提出了一种基于硬件的异常检测系统，用于实时检测恶意客户端。我们使用FPGA来满足实时性和功耗限制。通过所提出的整体系统评估，实现了整体系统性能。", "summary": "本文提出一种基于FPGA的硬件异常检测系统，旨在实时检测医疗网络中的恶意客户端，并满足实时性和功耗限制。同时，文章强调通过所提出的整体系统评估流程来确保和优化该系统的整体性能。", "keywords": "异常检测, FPGA, 医疗安全, 实时检测, 系统评估", "comments": "本文提出了一种创新的基于FPGA的硬件异常检测方法，旨在解决医疗数字化带来的安全挑战。其亮点在于利用FPGA满足实时性和功耗限制，并强调通过“整体系统评估”来确保性能。这表明其关注点不仅在于检测系统本身，还在于如何系统化地评估和优化这类系统。摘要中未提供具体的性能数据或评估流程的详细描述。"}}
{"id": "2506.15332", "title": "Building Blocks of a User Experience Research Point of View", "authors": ["Patricia Diaz"], "summary": "This paper presents three User Experience Research (UXR) perspectives based\non data, evidence and insights - known as Point of View (POV) - showcasing how\nthe strategies and methods of building a POV work in an enterprise setting. The\nPOV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in\nvideos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the\nlearner as it is the loop that requires the least effort for the highest\nimpact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at\nthe intersection of emergent technical capabilities that unlock novel\napproaches to critical user needs while addressing business strategic\npriorities (2019). They all seemed far-fetched and went against common\npractice. All were adopted and had long-lasting impact.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15332v1", "AI": {"title_translation": "用户体验研究视角的构成要素", "tldr": "本文介绍了三个用户体验研究（UXR）视角（POV），它们最初看似不切实际但最终被采纳并产生了长期影响，展示了在企业环境中构建POV的策略和方法。", "motivation": "本文旨在展示在企业环境中，如何基于数据、证据和洞察力构建用户体验研究（UXR）视角（POV）的策略和方法。", "method": "本文通过介绍三个具体的、基于数据、证据和洞察力的用户体验研究视角（POV）来展示其构建方法，这些视角包括：1. 智能视觉（利用AI从视频中提取和翻译文本）；2. 可评估代码编辑器（专注于AI直接反馈）；3. 机会图景（识别新兴技术与用户需求及商业战略交叉的高影响力机会）。", "result": "文中介绍的三个用户体验研究视角（POV）——智能视觉、可评估代码编辑器和机会图景——尽管最初看似不切实际且与常规做法相悖，但最终都被采纳并产生了长期影响。", "conclusion": "本文的结论是，即使是那些最初看起来不切实际并与传统做法相悖的用户体验研究视角，如果基于数据、证据和洞察力，也能在企业环境中成功被采纳并产生持久的影响。", "translation": "本文基于数据、证据和洞察力，提出了三个用户体验研究（UXR）视角——称为“观点”（POV）——展示了在企业环境中构建POV的策略和方法。这些POV包括：1. 智能视觉：利用AI从视频中的视觉内容中提取和翻译文本（2019年）。2. 可评估代码编辑器：专注于向学习者提供直接的AI反馈，因为这是实现最高影响所需努力最少的循环（2023年）。3. 机会格局：识别新兴技术能力与关键用户需求交叉点上的高影响力机会，同时解决业务战略优先事项（2019年）。它们最初都显得遥不可及，并与普遍做法相悖。但所有这些都被采纳并产生了长期影响。", "summary": "本文介绍了在企业环境中构建用户体验研究（UXR）视角的策略和方法，通过展示三个具体的、基于数据和洞察力的POV案例进行阐述。这些POV分别是智能视觉、可评估代码编辑器和机会图景，它们最初被认为是激进的，但最终都被成功采纳并证明具有长期影响力。", "keywords": "用户体验研究, 视角, 企业环境, AI反馈, 机会识别", "comments": "本文的创新之处在于其通过具体案例展示了如何将看似激进或反常规的用户体验研究视角（POV）成功引入企业环境并产生实际影响。它强调了基于数据、证据和洞察力的重要性，即使面对初期阻力，也能实现创新成果。论文的重要性在于为UXR专业人士提供了构建和推广其观点的实用框架，尤其是在技术驱动的创新领域。其局限性可能在于仅提供了三个案例，可能需要更多不同背景和规模企业的案例来进一步验证其普适性。"}}
{"id": "2506.15626", "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction", "authors": ["Vincent Roca", "Marc Tommasi", "Paul Andrey", "Aurélien Bellet", "Markus D. Schirmer", "Hilde Henon", "Laurent Puy", "Julien Ramon", "Grégory Kuchcinski", "Martin Bretzner", "Renaud Lopes"], "summary": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15626v1", "AI": {"title_translation": "联邦学习在基于MRI的脑龄预测中的应用：一项关于卒中后功能预后预测的多中心研究", "tldr": "联邦学习能在不集中敏感患者数据的情况下实现准确的脑龄预测，并显示出与卒中恢复的强关联。", "motivation": "训练鲁棒的脑龄（BrainAGE）模型需要大型数据集，但数据隐私问题限制了数据的集中。本研究旨在评估联邦学习（FL）在缺血性卒中患者中进行BrainAGE估计的性能，并探讨其与临床表型和功能预后的关联。", "method": "本研究使用了来自16个医院中心的1674名卒中患者的FLAIR脑图像。研究实施了标准机器学习和深度学习模型，在集中式学习、联邦学习（FL）和单站点学习三种数据管理策略下进行BrainAGE估计。研究报告了预测误差，并检查了BrainAGE与血管危险因素（如糖尿病、高血压、吸烟）以及卒中后三个月功能预后之间的关联。通过逻辑回归评估了BrainAGE对这些预后的预测价值，并调整了年龄、性别、血管危险因素、卒中严重程度、MRI与动脉穿刺之间的时间、既往静脉溶栓和再通结果等混杂因素。", "result": "尽管集中式学习产生了最准确的预测，但联邦学习（FL）始终优于单站点模型。在所有模型中，糖尿病患者的BrainAGE显著更高。良好和不良功能预后患者之间的比较，以及这些预后的多变量预测表明，BrainAGE与卒中后恢复之间存在显著关联。", "conclusion": "联邦学习（FL）能够在不集中数据的情况下实现准确的年龄预测。BrainAGE、血管危险因素和卒中后恢复之间的强关联突出了其在卒中护理中进行预后建模的潜力。", "translation": "**目标：**脑龄预测差（BrainAGE）是一种反映大脑健康的神经影像生物标志物。然而，训练鲁棒的BrainAGE模型需要大型数据集，但通常受到隐私问题的限制。本研究评估了联邦学习（FL）在接受机械取栓治疗的缺血性卒中患者中进行BrainAGE估计的性能，并调查了其与临床表型和功能预后的关联。\n**方法：**我们使用了来自16个医院中心的1674名卒中患者的FLAIR脑图像。我们实现了标准机器学习和深度学习模型，用于在三种数据管理策略下进行BrainAGE估计：集中式学习（数据池化）、FL（在每个站点进行本地训练）和单站点学习。我们报告了预测误差，并检查了BrainAGE与血管危险因素（例如糖尿病、高血压、吸烟）以及卒中后三个月功能预后之间的关联。逻辑回归评估了BrainAGE对这些预后的预测价值，并调整了年龄、性别、血管危险因素、卒中严重程度、MRI与动脉穿刺之间的时间、既往静脉溶栓和再通结果等因素。\n**结果：**尽管集中式学习产生了最准确的预测，但FL始终优于单站点模型。在所有模型中，糖尿病患者的BrainAGE显著更高。良好和不良功能预后患者之间的比较，以及这些预后的多变量预测表明，BrainAGE与卒中后恢复之间存在显著关联。\n**结论：**FL能够在不集中数据的情况下实现准确的年龄预测。BrainAGE、血管危险因素和卒中后恢复之间的强关联突出了其在卒中护理中进行预后建模的潜力。", "summary": "这项多中心研究评估了联邦学习（FL）在16个医院中心的1674名卒中患者中基于MRI的脑龄（BrainAGE）估计的性能。研究将FL与集中式学习和单站点学习进行了比较，发现FL始终优于单站点模型，尽管集中式学习最为准确。研究还揭示了BrainAGE与血管危险因素（如糖尿病）以及卒中后功能预后之间存在显著关联，表明BrainAGE作为卒中护理中预后生物标志物的潜力，尤其是在保护隐私的联邦学习模式下。", "keywords": "联邦学习, 脑龄, 卒中, 预后建模, 多中心研究", "comments": "这篇论文解决了医学人工智能领域的一个关键挑战：数据隐私和大型数据集的获取。通过证明联邦学习在脑龄估计方面的有效性，它为在不损害患者机密性的前提下进行协作研究提供了一个实用的解决方案。研究发现脑龄与血管危险因素和卒中后恢复相关，这凸显了其作为预后标志物的临床相关性。该研究的多中心设计增强了其发现的普遍性。"}}
{"id": "2506.14903", "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization", "authors": ["Renjith Prasad", "Abhilekh Borah", "Hasnat Md Abdullah", "Chathurangi Shyalika", "Gurpreet Singh", "Ritvik Garimella", "Rajarshi Roy", "Harshul Surana", "Nasrin Imanpour", "Suranjana Trivedy", "Amit Sheth", "Amitava Das"], "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated\nimages faithfully capture user intent while maintaining safety and fairness.\nDirect Preference Optimization (DPO), prominent in large language models\n(LLMs), is extending its influence to T2I systems. This paper introduces\nDPO-Kernels for T2I models, a novel extension enhancing alignment across three\ndimensions: (i) Hybrid Loss, integrating embedding-based objectives with\ntraditional probability-based loss for improved optimization; (ii) Kernelized\nRepresentations, employing Radial Basis Function (RBF), Polynomial, and Wavelet\nkernels for richer feature transformations and better separation between safe\nand unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's\ndefault Kullback-Leibler (KL) regularizer by incorporating Wasserstein and\nR'enyi divergences for enhanced stability and robustness. We introduce\nDETONATE, the first large-scale benchmark of its kind, comprising approximately\n100K curated image pairs categorized as chosen and rejected. DETONATE\nencapsulates three axes of social bias and discrimination: Race, Gender, and\nDisability. Prompts are sourced from hate speech datasets, with images\ngenerated by leading T2I models including Stable Diffusion 3.5 Large, Stable\nDiffusion XL, and Midjourney. Additionally, we propose the Alignment Quality\nIndex (AQI), a novel geometric measure quantifying latent-space separability of\nsafe/unsafe image activations, revealing hidden vulnerabilities. Empirically,\nwe demonstrate that DPO-Kernels maintain strong generalization bounds via\nHeavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are\npublicly released.", "comment": "59 pages, 10 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14903v1", "AI": {"title_translation": "DETONATE：文本到图像对齐与核化直接偏好优化基准", "tldr": "论文介绍了DPO-Kernels，一种增强T2I模型对齐的新方法，并发布了DETONATE，一个用于评估T2I模型偏见的大规模基准数据集，以及AQI，一个量化安全/不安全图像分离度的几何度量。", "motivation": "确保文本到图像（T2I）模型生成的图像能够忠实捕捉用户意图，同时保持安全性与公平性，对齐至关重要。直接偏好优化（DPO）在大型语言模型（LLM）中表现突出，其影响力正扩展到T2I系统。", "method": "提出了DPO-Kernels，通过混合损失、核化表示（径向基函数、多项式、小波核）和散度选择（Wasserstein、R'enyi散度）增强T2I模型对齐。引入了DETONATE，首个包含约10万个图像对的大规模T2I对齐基准，涵盖种族、性别和残疾等社会偏见维度。还提出了对齐质量指数（AQI），一种量化安全/不安全图像激活在潜在空间中可分离性的几何度量。", "result": "经验证明，DPO-Kernels通过重尾自正则化（HT-SR）保持了强大的泛化界限。", "conclusion": "本文引入了DPO-Kernels以增强T2I模型对齐，并发布了DETONATE基准数据集和AQI，为评估和改进T2I模型的安全性、公平性及对齐质量提供了新工具和资源。", "translation": "对齐对于文本到图像（T2I）模型至关重要，以确保生成的图像忠实地捕捉用户意图，同时保持安全性和公平性。直接偏好优化（DPO）在大型语言模型（LLM）中表现突出，其影响力正扩展到T2I系统。本文为T2I模型引入了DPO-Kernels，这是一种新颖的扩展，从三个维度增强了对齐：(i) 混合损失，将基于嵌入的目标与传统的基于概率的损失相结合，以改进优化；(ii) 核化表示，采用径向基函数（RBF）、多项式和小波核，以实现更丰富的特征变换，并更好地分离安全和不安全输入；以及(iii) 散度选择，通过引入Wasserstein和R'enyi散度，超越DPO默认的Kullback-Leibler（KL）正则化器，以增强稳定性和鲁棒性。我们引入了DETONATE，这是同类中第一个大规模基准，包含大约10万个经过整理的图像对，分为选择和拒绝两类。DETONATE包含了种族、性别和残疾这三个社会偏见和歧视轴。提示语来源于仇恨言论数据集，图像由领先的T2I模型生成，包括Stable Diffusion 3.5 Large、Stable Diffusion XL和Midjourney。此外，我们提出了对齐质量指数（AQI），一种新颖的几何度量，用于量化安全/不安全图像激活在潜在空间中的可分离性，揭示了隐藏的漏洞。经验证明，DPO-Kernels通过重尾自正则化（HT-SR）保持了强大的泛化界限。DETONATE和完整的代码已公开发布。", "summary": "本文提出了DPO-Kernels，一种针对文本到图像（T2I）模型的新型对齐增强方法，通过结合混合损失、核化表示和多种散度选择来优化对齐效果。为评估T2I模型的对齐质量和潜在偏见，作者还推出了DETONATE，一个包含约10万个图像对的大规模基准数据集，涵盖社会偏见维度，并引入了对齐质量指数（AQI）来量化安全/不安全图像的潜在空间可分离性。实验结果表明DPO-Kernels具有良好的泛化能力。所有代码和数据集均已公开。", "keywords": "文本到图像对齐, 直接偏好优化, DPO-Kernels, DETONATE, 社会偏见", "comments": "这篇论文的创新点在于将DPO扩展到T2I领域，并引入了DPO-Kernels，通过多维度优化提升了对齐效果。其重要性体现在构建了DETONATE这一大规模、关注社会偏见的T2I对齐基准，以及提出了AQI这一新颖的评估指标，为T2I模型的安全性和公平性研究提供了宝贵的资源和工具。这对于推动负责任的AI发展具有重要意义。"}}
{"id": "2310.18450", "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition", "authors": ["Jiamin Xie", "John H. L. Hansen"], "summary": "In this paper, we present MixRep, a simple and effective data augmentation\nstrategy based on mixup for low-resource ASR. MixRep interpolates the feature\ndimensions of hidden representations in the neural network that can be applied\nto both the acoustic feature input and the output of each layer, which\ngeneralizes the previous MixSpeech method. Further, we propose to combine the\nmixup with a regularization along the time axis of the input, which is shown as\ncomplementary. We apply MixRep to a Conformer encoder of an E2E LAS\narchitecture trained with a joint CTC loss. We experiment on the WSJ dataset\nand subsets of the SWB dataset, covering reading and telephony conversational\nspeech. Experimental results show that MixRep consistently outperforms other\nregularization methods for low-resource ASR. Compared to a strong SpecAugment\nbaseline, MixRep achieves a +6.5\\% and a +6.7\\% relative WER reduction on the\neval92 set and the Callhome part of the eval'2000 set.", "comment": "Accepted to Interspeech 2023", "cate": "eess.AS", "url": "http://arxiv.org/abs/2310.18450v1", "AI": {"title_translation": "MixRep：低资源语音识别的隐式表示混叠", "tldr": "MixRep是一种针对低资源ASR的简单有效的数据增强策略，通过混合隐藏表示来提高性能，优于其他正则化方法。", "motivation": "论文旨在解决低资源语音识别（ASR）中的数据稀缺问题，提出一种有效的数据增强方法。", "method": "提出MixRep，一种基于mixup的数据增强策略，通过在神经网络中对隐藏表示的特征维度进行插值，可应用于声学特征输入和每一层的输出，泛化了MixSpeech方法。此外，将mixup与输入的时间轴正则化相结合。将MixRep应用于一个采用联合CTC损失训练的E2E LAS架构的Conformer编码器。", "result": "MixRep在低资源ASR中持续优于其他正则化方法。与强大的SpecAugment基线相比，MixRep在eval92集和eval'2000集的Callhome部分分别实现了+6.5%和+6.7%的相对WER降低。", "conclusion": "MixRep是一种针对低资源ASR的有效数据增强策略，能够显著提升性能。", "translation": "在本文中，我们提出了MixRep，一种基于mixup的简单有效的数据增强策略，用于低资源ASR。MixRep在神经网络中对隐藏表示的特征维度进行插值，可以应用于声学特征输入和每一层的输出，这泛化了之前的MixSpeech方法。此外，我们提出将mixup与输入的时间轴正则化相结合，这被证明是互补的。我们将MixRep应用于一个采用联合CTC损失训练的E2E LAS架构的Conformer编码器。我们在WSJ数据集和SWB数据集的子集上进行了实验，涵盖了阅读和电话会话语音。实验结果表明，MixRep在低资源ASR中持续优于其他正则化方法。与强大的SpecAugment基线相比，MixRep在eval92集和eval'2000集的Callhome部分分别实现了+6.5%和+6.7%的相对WER降低。", "summary": "本文提出了MixRep，一种针对低资源自动语音识别（ASR）的创新数据增强方法。MixRep通过在神经网络中插值隐藏表示的特征维度实现mixup，并可应用于输入和各层输出，是对现有MixSpeech方法的泛化。该方法还结合了时间轴正则化以增强效果。实验结果表明，MixRep在低资源ASR任务上持续优于其他正则化方法，并在WSJ和SWB数据集上相较于SpecAugment基线取得了显著的相对词错误率（WER）降低。", "keywords": "MixRep, 低资源ASR, 数据增强, Mixup, 隐藏表示", "comments": "该论文的创新点在于提出了MixRep，一种基于隐藏表示mixup的数据增强策略，并将其泛化应用于神经网络的多个层面（输入和各层输出），同时结合了时间轴正则化。其重要性体现在对低资源ASR的显著性能提升，特别是与强基线的比较中展现出的有效性。"}}
{"id": "2506.14810", "title": "Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies", "authors": ["Qiwen Zhang"], "summary": "Sparse and intermittent demand forecasting in supply chains presents a\ncritical challenge, as frequent zero-demand periods hinder traditional model\naccuracy and impact inventory management. We propose and evaluate a\nModel-Router framework that dynamically selects the most suitable forecasting\nmodel-spanning classical, ML, and DL methods for each product based on its\nunique demand pattern. By comparing rule-based, LightGBM, and InceptionTime\nrouters, our approach learns to assign appropriate forecasting strategies,\neffectively differentiating between smooth, lumpy, or intermittent demand\nregimes to optimize predictions. Experiments on the large-scale Favorita\ndataset show our deep learning (Inception Time) router improves forecasting\naccuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with\n4.67x faster inference time. Ultimately, these gains in forecasting precision\nwill drive substantial reductions in both stockouts and wasteful excess\ninventory, underscoring the critical role of intelligent, adaptive Al in\noptimizing contemporary supply chain operations.", "comment": "7 pages, 4 figures, conference", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14810v1", "AI": {"title_translation": "稀疏需求预测的智能路由：选择策略的比较评估", "tldr": "该研究提出了一个模型路由框架，根据产品需求模式动态选择最佳预测模型，通过深度学习路由显著提高了稀疏需求预测的准确性，并加快了推理速度，从而优化了供应链库存管理。", "motivation": "稀疏和间歇性需求预测在供应链中是一个严峻的挑战，因为频繁的零需求期会影响传统模型的准确性并阻碍库存管理。", "method": "研究提出了一个模型路由框架，该框架根据每个产品的独特需求模式，动态选择最合适的预测模型（涵盖经典、机器学习和深度学习方法）。通过比较基于规则、LightGBM和InceptionTime的路由器，该方法学习分配适当的预测策略，有效区分平滑、块状或间歇性需求模式以优化预测。", "result": "在大型Favorita数据集上的实验表明，深度学习（Inception Time）路由器比强大的单一模型基准将预测准确性提高了高达11.8%（NWRMSLE），并且推理时间快了4.67倍。", "conclusion": "预测精度的提高将显著减少缺货和浪费的过剩库存，突显了智能、自适应人工智能在优化当代供应链运营中的关键作用。", "translation": "供应链中稀疏和间歇性需求预测是一个严峻的挑战，因为频繁的零需求期会阻碍传统模型的准确性并影响库存管理。我们提出并评估了一个模型路由框架，该框架根据每个产品独特的需*求模式，动态选择最合适的预测模型——涵盖经典、机器学习和深度学习方法。通过比较基于规则、LightGBM和InceptionTime的路由器，我们的方法学习分配适当的预测策略，有效区分平滑、块状或间歇性需求模式以优化预测。在大型Favorita数据集上的实验表明，我们的深度学习（Inception Time）路由器比强大的单一模型基准将预测准确性提高了高达11.8%（NWRMSLE），并且推理时间快了4.67倍。最终，这些预测精度的提升将显著减少缺货和浪费的过剩库存，突显了智能、自适应人工智能在优化当代供应链运营中的关键作用。", "summary": "本研究提出了一种名为Model-Router的框架，旨在解决供应链中稀疏和间歇性需求预测的难题。该框架通过动态选择最适合特定产品需求模式（包括平滑、块状或间歇性）的预测模型来提高预测准确性。研究比较了基于规则、LightGBM和InceptionTime三种路由器，并在Favorita大型数据集上进行了实验。结果显示，基于深度学习的InceptionTime路由器在预测准确性上比单一模型基准提高了11.8%，同时推理速度加快了4.67倍，证明了其在优化库存管理和供应链运营方面的潜力。", "keywords": "稀疏需求预测, 智能路由, 模型选择, 深度学习, 供应链", "comments": "该论文的创新点在于提出了一个动态模型路由框架，能够根据不同的需求模式自适应地选择最佳预测模型。特别是在稀疏和间歇性需求预测这一难题上，通过引入深度学习路由器显著提升了预测准确性和推理效率，为供应链管理带来了实际价值。其重要性在于提供了一种更智能、更高效的预测解决方案，有助于减少库存成本和提高服务水平。"}}
{"id": "2506.15527", "title": "On Exact Solutions to the Linear Bellman Equation", "authors": ["David Ohlin", "Richard Pates", "Murat Arcak"], "summary": "This paper presents sufficient conditions for optimal control of systems with\ndynamics given by a linear operator, in order to obtain an explicit solution to\nthe Bellman equation that can be calculated in a distributed fashion. Further,\nthe class of Linearly Solvable MDP is reformulated as a continuous-state\noptimal control problem. It is shown that this class naturally satisfies the\nconditions for explicit solution of the Bellman equation, motivating the\nextension of previous results to semilinear dynamics to account for input\nnonlinearities. The applicability of the given conditions is illustrated in\nscenarios with linear and quadratic cost, corresponding to the Stochastic\nShortest Path and Linear-Quadratic Regulator problems.", "comment": "Preprint to be published in the Control Systems Letters", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15527v1", "AI": {"title_translation": "关于线性贝尔曼方程精确解的研究", "tldr": "本文提出了线性贝尔曼方程精确解的充分条件，并将其应用于可线性求解的MDP，以实现分布式计算，并扩展到半线性动力学。", "motivation": "旨在为具有线性算子动力学的系统找到最优控制的充分条件，从而获得可分布式计算的贝尔曼方程的显式解。此外，激励将现有结果扩展到半线性动力学以处理输入非线性。", "method": "提出了获得线性贝尔曼方程显式解的充分条件。将可线性求解的MDP（马尔可夫决策过程）重新表述为连续状态最优控制问题。", "result": "找到了使线性贝尔曼方程具有显式解的充分条件。证明了可线性求解的MDP类别自然满足这些条件。这些条件适用于线性和二次成本场景，如随机最短路径和线性二次调节器问题。", "conclusion": "本文成功提出了线性贝尔曼方程显式解的充分条件，并证明了其在可线性求解MDP中的适用性，为处理输入非线性提供了基础。", "translation": "本文提出了具有线性算子动力学系统最优控制的充分条件，以获得可以分布式计算的贝尔曼方程的显式解。此外，将可线性求解的MDP（马尔可夫决策过程）类别重新表述为连续状态最优控制问题。结果表明，该类别自然满足贝尔曼方程显式解的条件，这促使将以前的结果扩展到半线性动力学，以解释输入非线性。所给条件的适用性通过线性和二次成本场景进行了说明，对应于随机最短路径和线性二次调节器问题。", "summary": "本文研究了线性贝尔曼方程的精确解，提出了在具有线性算子动力学系统中实现最优控制的充分条件，从而获得可分布式计算的显式解。研究将可线性求解的马尔可夫决策过程（MDP）重新定义为连续状态最优控制问题，并证明该类问题天然满足所提出的条件。这些发现为将现有成果扩展到包含输入非线性的半线性动力学提供了基础，并通过随机最短路径和线性二次调节器等具体应用场景验证了其适用性。", "keywords": "贝尔曼方程, 最优控制, 线性算子, 分布式计算, 可线性求解MDP", "comments": "这篇论文通过提供线性贝尔曼方程的显式解条件，在最优控制领域做出了贡献，特别是在分布式计算方面。将LSMDP重新表述为连续状态问题并证明其满足这些条件，为处理更复杂的非线性系统提供了理论基础和扩展潜力。"}}
{"id": "2506.15211", "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "authors": ["Feng He", "Zijun Chen", "Xinnian Liang", "Tingting Ma", "Yunqi Qiu", "Shuangzhi Wu", "Junchi Yan"], "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15211v1", "AI": {"title_translation": "ProtoReasoning：原型作为大型语言模型中可泛化推理的基础", "tldr": "ProtoReasoning是一个新框架，通过利用可扩展和可验证的原型表示（如Prolog和PDDL），显著提升了大型语言模型的推理能力和跨领域泛化性。", "motivation": "大型推理模型（LRMs）通过长链式思维（Long CoT）推理展现出显著的跨领域泛化能力，但其背后的机制尚不明确。本文假设跨领域泛化源于共享的抽象推理原型。", "method": "本文提出了ProtoReasoning框架，通过以下特性增强LLM的推理能力：1) 自动化原型构建流程，将问题转化为原型表示；2) 通过Prolog/PDDL解释器提供可靠反馈的综合验证系统；3) 在原型空间内任意合成问题的可扩展性并确保正确性。该框架利用Prolog进行逻辑推理，PDDL进行规划。", "result": "ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提升4.7%，在规划任务上提升6.3%，在通用推理（MMLU）上提升4.0%，在数学（AIME24）上提升1.0%。消融研究证实，在原型空间中学习比仅在自然语言表示上训练更能增强对结构相似问题的泛化能力。", "conclusion": "研究结果验证了推理原型是大型语言模型中可泛化推理基础的假设，ProtoReasoning通过利用原型表示显著提升了LLMs的推理能力和泛化性。", "translation": "大型推理模型（LRMs）最近在通过长链式思维（Long CoT）推理训练方面取得了进展，展示了卓越的跨领域泛化能力。然而，支持这种迁移的底层机制仍知之甚少。我们假设跨领域泛化源于共享的抽象推理原型——捕捉跨领域问题本质的基本推理模式。这些原型最大限度地减少了表示的细微差别，揭示了看似不同的任务都基于共享的推理结构。基于这一假设，我们提出了ProtoReasoning，一个通过利用可扩展和可验证的原型表示（Prolog用于逻辑推理，PDDL用于规划）来增强大型语言模型推理能力的框架。ProtoReasoning的特点包括：(1) 自动化原型构建流程，将问题转化为相应的原型表示；(2) 通过Prolog/PDDL解释器提供可靠反馈的综合验证系统；(3) 在原型空间内任意合成问题并确保正确性的可扩展性。广泛的实验表明，ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在通用推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。重要的是，我们的消融研究证实，与仅在自然语言表示上训练相比，在原型空间中学习也表现出对结构相似问题的增强泛化能力，验证了我们的假设，即推理原型是大型语言模型中可泛化推理的基础。", "summary": "本文提出了ProtoReasoning框架，旨在通过利用抽象推理原型来增强大型语言模型的通用推理能力。该框架基于跨领域泛化源于共享原型的假设，通过自动化原型构建、验证系统和问题合成能力，将问题转化为Prolog或PDDL等可验证的原型表示。实验证明，ProtoReasoning在逻辑推理、规划、通用推理和数学任务上均取得了显著改进，并验证了原型学习在提升LLM泛化能力方面的有效性。", "keywords": "大型语言模型, 推理, 原型, 泛化, Prolog, PDDL", "comments": "ProtoReasoning的创新之处在于其将抽象推理原型引入LLM的推理过程，并通过结构化表示（如Prolog和PDDL）实现可验证和可扩展的训练。这为理解LLM的泛化机制提供了新的视角，并为构建更鲁棒、更具通用性的推理模型开辟了道路。其强调原型空间中的学习，有助于模型捕获问题本质，减少对表面特征的依赖，从而提升跨领域和结构相似问题的泛化能力。"}}
{"id": "2506.15185", "title": "Heterogeneous and anisotropic elastic parameter estimation using a novel semi-analytical forward solver", "authors": ["Xiaopeng Zhu", "Zhongyi Huang"], "summary": "An efficient procedure using a novel semi-analytical forward solver for\nidentifying heterogeneous and anisotropic elastic parameters from only one\nfull-field measurement is proposed and explored. We formulate the inverse\nproblem as an special energy functional minimization with total variation(TV)\nregularization. The minimization problem is solved by Adam algorithm, which\nonly requires solving one forward problem and no adjoint problem in each\niteration. In order to deal with the irregularity of the elastic regions, the\nanisotropy and heterogeneity of parameters and potential singularities in\nforward-modeled issues, a novel semi-analytical forward solver named the direct\nmethod of lines is proposed, which discretizes angular variable while\npreserving analytical solutions along remaining coordinates. To validate the\nefficacy of our procedure, a series of numerical experiments are implemented\nsubsequently, achieving reliable performance in both forward modeling and the\nsix elastic arguments reconstruction scenarios.", "comment": "26 pages,9 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15185v1", "AI": {"title_translation": "使用新型半解析正向求解器进行非均匀各向异性弹性参数估计", "tldr": "本文提出了一种基于新型半解析正向求解器和Adam算法的高效方法，用于仅通过一次全场测量识别非均匀各向异性弹性参数。", "motivation": "为了解决从单次全场测量中识别非均匀和各向异性弹性参数的挑战，并处理弹性区域的不规则性、参数的各向异性和非均匀性以及正向建模问题中潜在的奇异性。", "method": "提出了一种高效的程序，该程序使用一种新型的半解析正向求解器。将逆问题表述为带有全变分(TV)正则化的特殊能量泛函最小化问题。最小化问题通过Adam算法求解，该算法在每次迭代中仅需解决一个正向问题，无需伴随问题。为处理弹性区域的不规则性、参数的各向异性和非均匀性以及正向建模问题中潜在的奇异性，提出了一种名为“直线直接法”的新型半解析正向求解器，该方法离散化了角度变量，同时保留了沿其余坐标的解析解。", "result": "通过一系列数值实验验证了该方法的有效性，在正向建模和六个弹性参数重建场景中均取得了可靠的性能。", "conclusion": "所提出的使用新型半解析正向求解器的方法在识别非均匀各向异性弹性参数方面表现出高效和可靠的性能。", "translation": "本文提出并探索了一种使用新型半解析正向求解器的高效程序，用于仅通过一次全场测量识别非均匀和各向异性弹性参数。我们将逆问题表述为带有全变分(TV)正则化的特殊能量泛函最小化问题。最小化问题通过Adam算法求解，该算法在每次迭代中仅需解决一个正向问题，无需伴随问题。为了处理弹性区域的不规则性、参数的各向异性和非均匀性以及正向建模问题中潜在的奇异性，提出了一种名为“直线直接法”的新型半解析正向求解器，该方法离散化了角度变量，同时保留了沿其余坐标的解析解。随后，实施了一系列数值实验来验证我们程序的有效性，在正向建模和六个弹性参数重建场景中均取得了可靠的性能。", "summary": "本研究提出了一种高效的方法，用于仅通过一次全场测量来估计非均匀和各向异性弹性参数。该方法将逆问题公式化为带全变分正则化的能量泛函最小化，并使用Adam算法求解，其特点是每次迭代只需解决一个正向问题。为应对复杂区域和参数特性，引入了一种新型的“直线直接法”半解析正向求解器。数值实验验证了该方法在正向建模和弹性参数重建中的可靠性。", "keywords": "弹性参数估计, 非均匀各向异性, 半解析求解器, Adam算法, 全变分正则化", "comments": "该论文的创新点在于提出了新型的“直线直接法”半解析正向求解器，该求解器能够有效处理非均匀、各向异性区域的复杂性及潜在奇异性。同时，采用Adam算法且无需伴随问题的设计显著提高了计算效率，使其在仅依赖单次全场测量的情况下也能实现可靠的参数估计，具有重要的实际应用价值。"}}
{"id": "2506.15417", "title": "Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules", "authors": ["Alessandro Palumbo", "Ruben Salvador"], "summary": "Software-exploitable Hardware Trojans (HTs) enable attackers to execute\nunauthorized software or gain illicit access to privileged operations. This\nmanuscript introduces a hardware-based methodology for detecting runtime HT\nactivations using Error Correction Codes (ECCs) on a RISC-V microprocessor.\nSpecifically, it focuses on HTs that inject malicious instructions, disrupting\nthe normal execution flow by triggering unauthorized programs. To counter this\nthreat, the manuscript introduces a Hardware Security Checker (HSC) leveraging\nHamming Single Error Correction (HSEC) architectures for effective HT\ndetection. Experimental results demonstrate that the proposed solution achieves\na 100% detection rate for potential HT activations, with no false positives or\nundetected attacks. The implementation incurs minimal overhead, requiring only\n72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's\noriginal operating frequency and introducing no additional time delay.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 5 figures,", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15417v1", "AI": {"title_translation": "基于硬件纠错码模块的微处理器硬件木马检测", "tldr": "本文提出了一种基于硬件纠错码（ECC）的硬件安全检查器（HSC），可在运行时以100%的检测率和极低开销检测微处理器中的软件可利用硬件木马。", "motivation": "软件可利用的硬件木马（HTs）允许攻击者执行未经授权的软件或非法访问特权操作，对微处理器的安全构成威胁。", "method": "本文引入了一种硬件安全检查器（HSC），利用汉明单错误校正（HSEC）架构在RISC-V微处理器上检测运行时硬件木马的激活。该方法专注于检测注入恶意指令并触发未经授权程序从而扰乱正常执行流程的硬件木马。", "result": "所提出的解决方案对潜在的硬件木马激活实现了100%的检测率，没有误报或未检测到的攻击。该实现仅需72个LUT、24个FF和0.5个BRAM，开销极小，同时保持了微处理器的原始工作频率，没有引入额外的时间延迟。", "conclusion": "基于硬件纠错码的HSC能够高效且有效地在微处理器中检测软件可利用的硬件木马，具有高检测率和低资源开销。", "translation": "软件可利用的硬件木马（HTs）使攻击者能够执行未经授权的软件或非法获取特权操作。本手稿介绍了一种基于硬件的方法，用于在RISC-V微处理器上使用纠错码（ECCs）检测运行时HT的激活。具体来说，它专注于注入恶意指令、通过触发未经授权程序来扰乱正常执行流程的HTs。为了应对这一威胁，本手稿引入了一个硬件安全检查器（HSC），利用汉明单错误校正（HSEC）架构进行有效的HT检测。实验结果表明，所提出的解决方案对潜在的HT激活实现了100%的检测率，没有误报或未检测到的攻击。该实现开销极小，仅需要72个#LUTs、24个#FFs和0.5个#BRAM，同时保持了微处理器的原始工作频率，并且没有引入额外的时间延迟。", "summary": "本文提出了一种基于硬件纠错码（ECC）的硬件安全检查器（HSC），用于在RISC-V微处理器中运行时检测软件可利用的硬件木马。该方法特别针对注入恶意指令的硬件木马，并利用汉明单错误校正（HSEC）架构实现。实验结果表明，该方案对硬件木马激活的检测率达到100%，无误报，且资源开销极低，不影响微处理器性能。", "keywords": "硬件木马, 纠错码, 微处理器, RISC-V, 硬件安全", "comments": "该论文提出了一种新颖且高效的硬件木马检测方法，其创新点在于将硬件纠错码应用于安全领域，实现了运行时检测。其重要性在于提供了一种高检测率、低开销且不影响性能的解决方案，对于提高微处理器安全性具有重要意义。"}}
{"id": "2506.15468", "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI", "authors": ["Ryota Okumura", "Tadahiro Taniguchi", "Akira Taniguchi", "Yoshinobu Hagiwara"], "summary": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15468v1", "AI": {"title_translation": "通过Metropolis-Hastings人机交互实现协同创造性学习", "tldr": "本文提出了一种新颖的协同创造性学习范式，其中人类和AI通过Metropolis-Hastings命名博弈相互整合信息，构建共享符号系统，并实验证明了其在提高分类准确性和促进共享符号系统收敛方面的有效性。", "motivation": "传统的AI教学基于单向知识转移，无法有效整合来自本质不同模态的信息。本文旨在解决人类与AI之间信息整合的挑战，提出协同创造性学习，以实现生物和人工代理共同构建共享外部表征。", "method": "本文提出协同创造性学习作为一种新范式，并使用基于Metropolis-Hastings命名博弈（MHNG）的人机交互模型进行经验测试。通过在线实验，69名参与者在部分可观察条件下，与三种计算机代理类型（基于MH、始终接受或始终拒绝）之一玩联合注意力命名博弈（JA-NG）。", "result": "实验结果表明，与基于MH代理的人机对通过交互显著提高了分类准确性，并实现了向共享符号系统更强的收敛。此外，人类的接受行为与MH导出的接受概率密切相关。", "conclusion": "这些发现首次为通过基于MHNG的交互在人机二元组中出现的协同创造性学习提供了实证证据。这表明了迈向共生AI系统的一条有前景的道路，即通过动态调整感知经验，与人类一起学习而不是从人类那里学习，为共生AI对齐开辟了新途径。", "translation": "我们提出协同创造性学习作为一种新颖的范式，其中人类和AI，即生物和人工代理，相互整合其部分感知信息和知识，以构建共享的外部表征，我们将这一过程解释为符号涌现。与基于单向知识转移的传统AI教学不同，这解决了整合来自本质不同模态信息的挑战。我们使用基于Metropolis-Hastings命名博弈（MHNG）（一种去中心化贝叶斯推理机制）的人机交互模型，通过经验测试了这一框架。在一项在线实验中，69名参与者在部分可观察条件下，与三种计算机代理类型（基于MH、始终接受或始终拒绝）之一玩联合注意力命名博弈（JA-NG）。结果显示，与基于MH代理的人机对通过交互显著提高了分类准确性，并实现了向共享符号系统更强的收敛。此外，人类的接受行为与MH导出的接受概率密切相关。这些发现首次为通过基于MHNG的交互在人机二元组中出现的协同创造性学习提供了实证证据。这表明了迈向共生AI系统的一条有前景的道路，即通过动态调整感知经验，与人类一起学习而不是从人类那里学习，为共生AI对齐开辟了新途径。", "summary": "本文提出了一种名为“协同创造性学习”的新型人机交互范式，旨在解决传统AI学习中单向知识转移的局限性，实现人类与AI之间双向的信息整合和共享表征构建。研究通过基于Metropolis-Hastings命名博弈的在线实验，验证了该范式的有效性。结果表明，与Metropolis-Hastings代理交互的人机对在分类准确性上显著提升，并能有效收敛到共享符号系统，人类接受行为也与模型预测一致。这为开发能够与人类共同学习而非仅从人类学习的共生AI系统提供了实证支持。", "keywords": "协同创造性学习, Metropolis-Hastings命名博弈, 人机交互, 符号涌现, 共生AI", "comments": "这项研究的创新之处在于提出了“协同创造性学习”这一新范式，并利用Metropolis-Hastings命名博弈机制来模拟人类与AI之间的双向信息整合。它突破了传统AI单向学习的限制，为实现真正意义上的共生AI系统提供了新的思路和实证证据。该研究的重要性在于为未来AI系统设计指明了方向，即AI应学习如何与人类共同创造和理解世界，而非仅仅作为知识的接收者。"}}
{"id": "2506.15157", "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation", "authors": ["Hanbit Oh", "Andrea M. Salcedo-Vázquez", "Ixchel G. Ramirez-Alpizar", "Yukiyasu Domae"], "summary": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2025 accepted", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15157v1", "AI": {"title_translation": "鲁棒即时策略：利用学生t回归模型实现机器人操作的鲁棒上下文模仿学习", "tldr": "提出了一种名为鲁棒即时策略 (RIP) 的新算法，利用学生t回归模型来提高机器人模仿学习中上下文即时策略的鲁棒性，有效应对幻觉问题。", "motivation": "现有的上下文模仿学习（In-Context IL）方法利用大型语言模型（LLMs）作为即时策略，但其可靠性因LLM产生的“幻觉”问题（即生成偏离示范的劣质轨迹）而在机器人领域受到损害。", "method": "提出了一种名为鲁棒即时策略（RIP）的新型鲁棒上下文模仿学习算法。RIP利用学生t回归模型来增强对即时策略幻觉轨迹的鲁棒性，从而实现可靠的轨迹生成。具体来说，RIP从LLM生成多个候选机器人轨迹以完成给定任务，并使用有利于忽略异常值（即幻觉）的学生t分布对它们进行聚合，从而生成对幻觉具有鲁棒性的轨迹。", "result": "实验结果表明，RIP显著优于最先进的模仿学习方法，任务成功率至少提高了26%，特别是在日常任务的低数据场景中。", "conclusion": "RIP通过利用学生t回归模型有效解决了上下文模仿学习中LLM幻觉问题，显著提高了机器人操作的模仿学习性能和可靠性。", "translation": "模仿学习 (IL) 旨在通过观察少量人类演示，使机器人能够自主执行任务。最近，一种称为上下文模仿学习 (In-Context IL) 的IL变体利用现成的LLMs作为即时策略，这些策略通过少量给定的演示来理解上下文，从而执行新任务，而不是通过大规模演示显式更新网络模型。然而，LLM基于即时策略的幻觉问题（例如，偶尔生成偏离给定演示的劣质轨迹）损害了其在机器人领域的可靠性。为了缓解这个问题，我们提出了一种新的鲁棒上下文模仿学习算法，称为鲁棒即时策略 (RIP)，它利用学生t回归模型来抵抗即时策略的幻觉轨迹，从而实现可靠的轨迹生成。具体来说，RIP从LLM生成多个候选机器人轨迹以完成给定任务，并使用学生t分布聚合它们，这有利于忽略异常值（即幻觉）；因此，生成了对幻觉具有鲁棒性的轨迹。我们在模拟和真实世界环境中进行的实验表明，RIP显著优于最先进的IL方法，任务成功率至少提高了26%，特别是在日常任务的低数据场景中。视频结果可在 https://sites.google.com/view/robustinstantpolicy 获取。", "summary": "本文提出了一种名为鲁棒即时策略（RIP）的新型上下文模仿学习算法，旨在解决大型语言模型（LLMs）在机器人模仿学习中因“幻觉”问题导致的不可靠性。RIP通过利用学生t回归模型聚合LLM生成的多个候选轨迹，并有效忽略异常值（即幻觉），从而生成对幻觉具有鲁棒性的可靠机器人轨迹。实验结果表明，RIP在任务成功率方面显著优于现有方法，尤其在低数据场景下表现突出。", "keywords": "模仿学习, 鲁棒即时策略, 学生t回归, 大型语言模型, 机器人操作", "comments": "这项工作通过引入学生t回归模型来处理LLM在模仿学习中产生的幻觉问题，提出了一种创新的方法。其重要性在于显著提高了机器人模仿学习的鲁棒性和可靠性，特别是在数据稀缺的实际应用场景中。这种方法为将LLM应用于高可靠性要求的机器人控制提供了有益的思路。"}}
{"id": "2506.14907", "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "authors": ["Yizhen Zhang", "Yang Ding", "Shuoshuo Zhang", "Xinchen Zhang", "Haoling Li", "Zhong-zhi Li", "Peijie Wang", "Jie Wu", "Lei Ji", "Yelong Shen", "Yujiu Yang", "Yeyun Gong"], "summary": "Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14907v1", "AI": {"title_translation": "PeRL：用于交错视觉-语言推理的置换增强强化学习", "tldr": "PeRL提出了一种新的强化学习方法，通过图像序列置换和rollout过滤机制，显著提升了多图像视觉-语言推理任务的性能，同时保持了单图像任务的竞争力。", "motivation": "现有的大多数多模态强化学习方法仅限于单图像空间推理，难以泛化到涉及多图像位置推理的复杂真实场景，在这些场景中理解图像间关系至关重要。", "method": "我们提出了一个通用的强化学习方法PeRL，专为交错多模态任务设计，并采用了一个多阶段策略来增强探索-利用权衡。具体来说，我们引入图像序列置换来模拟多样的位置关系以探索更多空间和位置多样性。此外，我们设计了一个rollout过滤机制用于重采样，以专注于对学习最优行为贡献最大的轨迹，从而有效利用学习到的策略。", "result": "PeRL模型在5个广泛使用的多图像基准测试和3个单图像基准测试上进行了评估。实验证实，PeRL训练的模型始终大幅超越R1相关和交错VLM基线，在多图像基准测试上取得了最先进的性能，同时在单图像任务上保持了可比的性能。", "conclusion": "PeRL通过引入图像序列置换和rollout过滤机制，成功解决了多图像视觉-语言推理的挑战，并在多图像基准测试上取得了最先进的性能，证明了其在复杂多模态推理任务中的有效性。", "translation": "受DeepSeek-R1等强化学习方法所展示的卓越推理能力的启发，近期新兴研究已开始探索使用强化学习（RL）来增强视觉-语言模型（VLM），以执行多模态推理任务。然而，大多数现有的多模态强化学习方法仍仅限于单图像上下文中的空间推理，并且难以泛化到涉及多图像位置推理的更复杂和真实世界的场景，在这些场景中理解图像之间的关系至关重要。为了应对这一挑战，我们提出了一种通用的强化学习方法PeRL，专为交错多模态任务量身定制，并设计了一个多阶段策略，旨在增强探索-利用权衡，从而提高学习效率和任务性能。具体来说，我们引入图像序列置换来模拟多样的位置关系，以探索更多空间和位置多样性。此外，我们设计了一个rollout过滤机制用于重采样，以专注于对学习最优行为贡献最大的轨迹，从而有效利用学习到的策略。我们在5个广泛使用的多图像基准测试和3个单图像基准测试上评估了我们的模型。我们的实验证实，经过PeRL训练的模型始终大幅超越R1相关和交错VLM基线，在多图像基准测试上取得了最先进的性能，同时在单图像任务上保持了可比的性能。", "summary": "本文提出了一种名为PeRL的强化学习方法，旨在解决现有视觉-语言模型在多图像推理任务中泛化能力不足的问题。PeRL通过引入图像序列置换来模拟多样化的空间关系，并设计了rollout过滤机制以优化策略学习效率。实验结果表明，PeRL在多个多图像基准测试上显著超越了现有基线，达到了最先进的性能，同时在单图像任务上保持了竞争力，证明了其在复杂交错多模态推理中的有效性。", "keywords": "强化学习, 视觉-语言推理, 多图像, 置换, PeRL", "comments": "PeRL的创新之处在于其针对多图像推理的强化学习方法，特别是引入图像序列置换来增强探索多样性，以及rollout过滤机制来优化利用效率。这有效地解决了现有VLM在处理复杂多图像情景时的局限性，为多模态推理领域带来了显著的性能提升。"}}
{"id": "2506.14811", "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning", "authors": ["Mikel Malagón", "Josu Ceberio", "Jose A. Lozano"], "summary": "This work introduces a growable and modular neural network architecture that\nnaturally avoids catastrophic forgetting and interference in continual\nreinforcement learning. The structure of each module allows the selective\ncombination of previous policies along with its internal policy, accelerating\nthe learning process on the current task. Unlike previous growing neural\nnetwork approaches, we show that the number of parameters of the proposed\napproach grows linearly with respect to the number of tasks, and does not\nsacrifice plasticity to scale. Experiments conducted in benchmark continuous\ncontrol and visual problems reveal that the proposed approach achieves greater\nknowledge transfer and performance than alternative methods.", "comment": "ICML 2024 (oral)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14811v1", "AI": {"title_translation": "可扩展持续强化学习的自组合策略", "tldr": "本文提出了一种可增长的模块化神经网络架构，用于持续强化学习，旨在避免灾难性遗忘和干扰，并加速学习过程。其参数数量随任务数量线性增长，且不牺牲可塑性，在实验中表现出优于现有方法的知识转移和性能。", "motivation": "解决持续强化学习中的灾难性遗忘和干扰问题，并加速当前任务的学习过程。", "method": "引入了一种可增长的模块化神经网络架构。该架构的每个模块允许选择性地组合先前的策略及其内部策略。与现有方法不同，其参数数量随任务数量线性增长，且不牺牲可塑性。", "result": "在基准连续控制和视觉问题上的实验表明，所提出的方法比替代方法实现了更大的知识转移和性能。参数数量随任务数量线性增长，且不牺牲可塑性。", "conclusion": "本文提出了一种可增长的模块化神经网络架构，能够有效地避免持续强化学习中的灾难性遗忘和干扰，加速学习过程，并实现高效扩展。", "translation": "这项工作引入了一种可增长的模块化神经网络架构，它自然地避免了持续强化学习中的灾难性遗忘和干扰。每个模块的结构允许选择性地组合先前的策略及其内部策略，从而加速当前任务的学习过程。与以前的增长型神经网络方法不同，我们表明所提出方法的参数数量随任务数量线性增长，并且不牺牲可塑性以实现扩展。在基准连续控制和视觉问题上进行的实验表明，所提出的方法比替代方法实现了更大的知识转移和性能。", "summary": "本文提出了一种新颖的可增长模块化神经网络架构，专为持续强化学习设计。该架构通过允许每个模块选择性地组合先前策略与自身策略，有效避免了灾难性遗忘和干扰。该架构具有可扩展性，参数数量随任务数量线性增长，并保持可塑性。实验结果表明，在连续控制和视觉任务中，该方法比现有方法展现出更优越的知识转移和性能。", "keywords": "持续强化学习, 灾难性遗忘, 模块化神经网络, 策略组合, 可扩展学习", "comments": "该论文的创新之处在于其“自组合策略”的模块化、可增长网络，直接解决了持续学习中常见的灾难性遗忘和可扩展性挑战。参数的线性增长是相对于先前方法的一项显著改进。"}}
{"id": "2506.15276", "title": "MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "JunHao Jiang"], "summary": "Implicit Neural representations (INRs) have emerged as a promising approach\nfor video compression, and have achieved comparable performance to the\nstate-of-the-art codecs such as H.266/VVC. However, existing INR-based methods\nstruggle to effectively represent detail-intensive and fast-changing video\ncontent. This limitation mainly stems from the underutilization of internal\nnetwork features and the absence of video-specific considerations in network\ndesign. To address these challenges, we propose a multi-scale feature fusion\nframework, MSNeRV, for neural video representation. In the encoding stage, we\nenhance temporal consistency by employing temporal windows, and divide the\nvideo into multiple Groups of Pictures (GoPs), where a GoP-level grid is used\nfor background representation. Additionally, we design a multi-scale spatial\ndecoder with a scale-adaptive loss function to integrate multi-resolution and\nmulti-frequency information. To further improve feature extraction, we\nintroduce a multi-scale feature block that fully leverages hidden features. We\nevaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and\ncompression. Experimental results demonstrate that our model exhibits superior\nrepresentation capability among INR-based approaches and surpasses VTM-23.7\n(Random Access) in dynamic scenarios in terms of compression efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15276v1", "AI": {"title_translation": "MSNeRV：基于多尺度特征融合的神经视频表示", "tldr": "MSNeRV通过多尺度特征融合改进了神经视频表示，尤其擅长处理细节丰富和快速变化的视频内容，并在压缩效率上超越了VTM-23.7。", "motivation": "现有的基于隐式神经表示（INR）的视频压缩方法难以有效表示细节丰富和快速变化的视频内容。这主要是因为内部网络特征利用不足以及网络设计中缺乏针对视频的特定考量。", "method": "我们提出了一个名为MSNeRV的多尺度特征融合框架。在编码阶段，通过使用时间窗口增强时间一致性，并将视频划分为多个图片组（GoPs），使用GoP级网格表示背景。此外，我们设计了一个带有尺度自适应损失函数的多尺度空间解码器，以整合多分辨率和多频率信息。为了进一步改进特征提取，我们引入了一个多尺度特征块来充分利用隐藏特征。", "result": "MSNeRV在HEVC ClassB和UVG数据集上进行了评估。实验结果表明，我们的模型在基于INR的方法中表现出卓越的表示能力，并且在动态场景下，其压缩效率超越了VTM-23.7（随机访问）。", "conclusion": "MSNeRV通过多尺度特征融合和视频特定设计，有效解决了现有INR方法在处理复杂视频内容时的局限性，并在视频表示和压缩方面取得了领先的性能，尤其适用于动态场景。", "translation": "隐式神经表示（INRs）已成为一种有前景的视频压缩方法，并已达到与H.266/VVC等最先进编解码器相当的性能。然而，现有的基于INR的方法难以有效表示细节丰富和快速变化的视频内容。这一局限性主要源于内部网络特征的利用不足以及网络设计中缺乏针对视频的特定考量。为了解决这些挑战，我们提出了一种用于神经视频表示的多尺度特征融合框架MSNeRV。在编码阶段，我们通过采用时间窗口来增强时间一致性，并将视频划分为多个图片组（GoP），其中GoP级网格用于背景表示。此外，我们设计了一个具有尺度自适应损失函数的多尺度空间解码器，以整合多分辨率和多频率信息。为了进一步改进特征提取，我们引入了一个多尺度特征块，该块充分利用了隐藏特征。我们在HEVC ClassB和UVG数据集上对MSNeRV进行了视频表示和压缩评估。实验结果表明，我们的模型在基于INR的方法中表现出卓越的表示能力，并且在动态场景下，其压缩效率超越了VTM-23.7（随机访问）。", "summary": "本文提出了MSNeRV，一个用于神经视频表示的多尺度特征融合框架，旨在解决现有隐式神经表示（INR）方法在处理细节丰富和快速变化的视频内容时的不足。MSNeRV通过引入时间窗口增强时间一致性，利用GoP级网格表示背景，并设计了一个带有尺度自适应损失函数的多尺度空间解码器来整合多分辨率和多频率信息。此外，它还包含一个多尺度特征块以充分利用隐藏特征。实验证明，MSNeRV在INR方法中具有卓越的表示能力，并在动态场景下的压缩效率超越了VTM-23.7。", "keywords": "神经视频表示, 多尺度特征融合, 隐式神经表示, 视频压缩, 动态场景", "comments": "MSNeRV的创新之处在于其多尺度特征融合框架和针对视频特性的设计，如时间窗口和GoP级网格，有效解决了INR在处理动态和细节丰富视频时的固有局限性。其超越VTM-23.7的性能表明了其在视频压缩领域的巨大潜力。"}}
{"id": "2506.15215", "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "authors": ["Yongqi Fan", "Yating Wang", "Guandong Wang", "Jie Zhai", "Jingping Liu", "Qi Ye", "Tong Ruan"], "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15215v1", "AI": {"title_translation": "MinosEval：针对开放式问答评估中事实型与非事实型问题的区分与定制化评估方法", "tldr": "MinosEval是一种新的开放式问答评估方法，它首先区分事实型和非事实型问题，然后采用不同的评估策略（事实型问题使用自适应关键点评分，非事实型问题使用实例感知列表式排名），以更好地对齐人类判断并提供更可解释的结果。", "motivation": "开放式问答（QA）是评估大型语言模型（LLM）能力的关键任务，但其评估面临挑战，包括答案陈述长、推理过程复杂和表达多样性。传统的ROUGE和BERTScore等指标难以捕捉语义相似性，而当前的基于LLM的评估方法（如成对或列表式比较）缺乏直观的可解释性，并且无法适应不同问题内容。最重要的是，现有方法忽略了事实型和非事实型问题之间的区别。", "method": "本文提出了MinosEval，一种新颖的评估方法，它首先区分开放式问题是事实型还是非事实型，然后采用不同的评估策略来对候选答案进行排名。对于事实型问题，它应用自适应关键点评分策略；对于非事实型问题，它使用实例感知列表式排名策略。", "result": "在多个开放式QA数据集（包括自建的补充社区资源的更多候选答案的数据集）上的实验表明，MinosEval与人类标注的对齐度更高，并提供了更可解释的结果。", "conclusion": "MinosEval通过区分事实型和非事实型问题并采用定制化的评估策略，显著提高了开放式问答评估的准确性和可解释性，使其更好地与人类判断对齐。", "translation": "开放式问答（QA）是评估大型语言模型（LLM）能力的关键任务。与封闭式QA相比，它要求更长的答案陈述、更细致的推理过程和多样化的表达，使得精细和可解释的自动化评估既关键又具有挑战性。传统的ROUGE和BERTScore等指标由于模型响应和参考答案之间存在不同的模式而难以捕捉语义相似性。当前基于LLM的评估方法，例如候选答案的成对或列表式比较，缺乏直观的可解释性。虽然对每个响应的点式评分提供了一些描述，但它无法适应不同的问题内容。最值得注意的是，现有方法忽略了事实型和非事实型问题之间的区别。为了解决这些挑战，我们提出了\\textbf{MinosEval}，一种新颖的评估方法，它首先区分开放式问题，然后使用不同的评估策略对候选答案进行排名。对于事实型问题，它应用自适应关键点评分策略，而对于非事实型问题，它使用实例感知列表式排名策略。在多个开放式QA数据集（包括自建的补充社区资源的更多候选答案的数据集）上的实验表明，MinosEval与人类标注的对齐度更高，并提供了更可解释的结果。", "summary": "本文提出MinosEval，一种针对开放式问答（QA）的新型评估方法，旨在解决现有LLM评估工具在语义捕捉、可解释性和问题类型区分方面的不足。MinosEval通过首先区分问题是事实型还是非事实型，然后分别采用自适应关键点评分和实例感知列表式排名策略进行评估。实验结果表明，MinosEval与人类标注的对齐度更高，且评估结果更具可解释性。", "keywords": "开放式问答, LLM评估, 事实型问题, 非事实型问题, MinosEval", "comments": "MinosEval的创新之处在于其对事实型和非事实型开放式问题的区分处理，这填补了现有评估方法的一个显著空白。通过为不同类型的问题定制评估策略，它提高了评估的准确性和可解释性，对于推动LLM在开放式QA领域的进步具有重要意义。"}}
{"id": "2506.15203", "title": "Reduced Particle in Cell method for the Vlasov-Poisson system using auto-encoder and Hamiltonian neural", "authors": ["Emmanuel Franck", "Laurent Navoret", "Vincent Vigon", "Raphaël Côte", "Guillaume Steimer"], "summary": "Hamiltonian particle-based simulations of plasma dynamics are inherently\ncomputationally intensive, primarily due to the large number of particles\nrequired to obtain accurate solutions. This challenge becomes even more acute\nin many-query contexts, where numerous simulations must be conducted across a\nrange of time and parameter values. Consequently, it is essential to construct\nreduced order models from such discretizations to significantly lower\ncomputational costs while ensuring validity across the specified time and\nparameter domains. Preserving the Hamiltonian structure in these reduced models\nis also crucial, as it helps maintain long-term stability. In this paper, we\nintroduce a nonlinear, non-intrusive, data-driven model order reduction method\nfor the 1D-1V Vlasov--Poisson system, discretized using a Hamiltonian\nParticle-In-Cell scheme. Our approach relies on a two-step projection\nframework: an initial linear projection based on the Proper Symplectic\nDecomposition, followed by a nonlinear projection learned via an autoencoder\nneural network. The reduced dynamics are then modeled using a Hamiltonian\nneural network. The offline phase of the method is split into two stages:\nfirst, constructing the linear projection using full-order model snapshots;\nsecond, jointly training the autoencoder and the Hamiltonian neural network to\nsimultaneously learn the encoder-decoder mappings and the reduced dynamics. We\nvalidate the proposed method on several benchmarks, including Landau damping\nand two-stream instability. The results show that our method has better\nreduction properties than standard linear Hamiltonian reduction methods.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15203v1", "AI": {"title_translation": "使用自编码器和哈密顿神经网络的Vlasov-Poisson系统降阶粒子在单元法", "tldr": "本文提出了一种结合自编码器和哈密顿神经网络的非线性、非侵入式、数据驱动的模型降阶方法，用于减少Vlasov-Poisson系统哈密顿粒子模拟的计算成本，同时保持哈密顿结构。", "motivation": "哈密顿粒子模拟等离子体动力学计算成本高，尤其是在多查询场景下。为了显著降低计算成本并保持长期稳定性，需要构建降阶模型并保留哈密顿结构。", "method": "提出了一种用于1D-1V Vlasov-Poisson系统的非线性、非侵入式、数据驱动的模型降阶方法。该方法采用两步投影框架：首先是基于Proper Symplectic Decomposition的线性投影，然后是通过自编码器神经网络学习的非线性投影。降阶动力学通过哈密顿神经网络建模。离线阶段分为两步：构建线性投影；联合训练自编码器和哈密顿神经网络，同时学习编解码器映射和降阶动力学。", "result": "在Landau阻尼和双流不稳定性等基准测试中验证了所提出的方法。结果表明，该方法比标准线性哈密顿降阶方法具有更好的降阶性能。", "conclusion": "该方法在Vlasov-Poisson系统哈密顿粒子模拟的降阶方面表现出优于传统线性方法的性能，能够有效降低计算成本并保持哈密顿结构。", "translation": "哈密顿粒子模拟的等离子体动力学本质上是计算密集型的，这主要是因为需要大量的粒子才能获得精确的解。在多查询场景中，当必须在一定的时间和参数范围内进行大量模拟时，这一挑战变得更加严峻。因此，从这种离散化中构建降阶模型至关重要，以显著降低计算成本，同时确保在指定的时间和参数域内的有效性。在这些降阶模型中保留哈密顿结构也至关重要，因为它有助于保持长期稳定性。在本文中，我们介绍了一种用于1D-1V Vlasov-Poisson系统的非线性、非侵入式、数据驱动的模型降阶方法，该系统使用哈密顿粒子在单元（PIC）方案进行离散化。我们的方法依赖于两步投影框架：基于Proper Symplectic Decomposition的初始线性投影，然后是通过自编码器神经网络学习的非线性投影。然后使用哈密顿神经网络对降阶动力学进行建模。该方法的离线阶段分为两个阶段：首先，使用全阶模型快照构建线性投影；其次，联合训练自编码器和哈密顿神经网络，以同时学习编解码器映射和降阶动力学。我们在几个基准测试中验证了所提出的方法，包括Landau阻尼和双流不稳定性。结果表明，我们的方法比标准线性哈密顿降阶方法具有更好的降阶性能。", "summary": "本文针对Vlasov-Poisson系统的哈密顿粒子模拟计算成本高的问题，提出了一种非线性、非侵入式、数据驱动的模型降阶方法。该方法结合了基于Proper Symplectic Decomposition的线性投影和自编码器学习的非线性投影，并通过哈密顿神经网络建模降阶动力学。离线阶段包括线性投影构建和自编码器与哈密顿神经网络的联合训练。实验结果表明，该方法在降低计算成本的同时，比现有线性方法具有更好的降阶性能并保持了哈密顿结构。", "keywords": "模型降阶, Vlasov-Poisson系统, 哈密顿神经网络, 自编码器, 粒子在单元法", "comments": "这项工作在等离子体模拟的计算效率方面具有重要意义。通过结合数据驱动的自编码器和哈密顿神经网络，它不仅实现了模型降阶，还巧妙地保留了物理系统的哈密顿结构，这对于长期模拟的稳定性至关重要。其创新点在于将线性与非线性投影相结合，并利用神经网络学习复杂动力学，有望在计算物理和工程领域得到广泛应用。"}}
{"id": "2506.15432", "title": "Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters", "authors": ["Guillaume Lomet", "Ruben Salvador", "Brice Colombier", "Vincent Grosso", "Olivier Sentieys", "Cedric Killian"], "summary": "Dataflow neural network accelerators efficiently process AI tasks on FPGAs,\nwith deployment simplified by ready-to-use frameworks and pre-trained models.\nHowever, this convenience makes them vulnerable to malicious actors seeking to\nreverse engineer valuable Intellectual Property (IP) through Side-Channel\nAttacks (SCA). This paper proposes a methodology to recover the hardware\nconfiguration of dataflow accelerators generated with the FINN framework.\nThrough unsupervised dimensionality reduction, we reduce the computational\noverhead compared to the state-of-the-art, enabling lightweight classifiers to\nrecover both folding and quantization parameters. We demonstrate an attack\nphase requiring only 337 ms to recover the hardware parameters with an accuracy\nof more than 95% and 421 ms to fully recover these parameters with an averaging\nof 4 traces for a FINN-based accelerator running a CNN, both using a random\nforest classifier on side-channel traces, even with the accelerator dataflow\nfully loaded. This approach offers a more realistic attack scenario than\nexisting methods, and compared to SoA attacks based on tsfresh, our method\nrequires 940x and 110x less time for preparation and attack phases,\nrespectively, and gives better results even without averaging traces.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 4 figures, 1 algorithm", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15432v1", "AI": {"title_translation": "侧信道提取数据流AI加速器硬件参数", "tldr": "本文提出一种侧信道攻击方法，能快速高效地恢复基于FINN框架的数据流AI加速器硬件参数，比现有方法更快更准确。", "motivation": "数据流AI加速器在FPGA上高效处理AI任务，但其便捷性使其容易受到恶意攻击者的侧信道攻击，从而逆向工程获取有价值的知识产权（IP）。", "method": "本文提出一种通过无监督降维减少计算开销的方法，并使用轻量级分类器（如随机森林）从侧信道迹线中恢复折叠和量化参数。", "result": "该方法能在337毫秒内以超过95%的准确率恢复硬件参数，在421毫秒内通过平均4条迹线完全恢复参数。与基于tsfresh的现有方法相比，准备阶段和攻击阶段分别减少了940倍和110倍的时间，即使不平均迹线也能获得更好的结果。", "conclusion": "本文提出的方法提供了一种比现有方法更真实的攻击场景，并且在恢复数据流AI加速器硬件参数方面具有显著的速度和效率优势。", "translation": "数据流神经网络加速器在FPGA上高效处理AI任务，借助即用型框架和预训练模型简化了部署。然而，这种便利性使其容易受到寻求通过侧信道攻击（SCA）逆向工程有价值知识产权（IP）的恶意行为者的攻击。本文提出一种恢复使用FINN框架生成的数据流加速器硬件配置的方法。通过无监督降维，我们与现有技术相比减少了计算开销，使得轻量级分类器能够恢复折叠和量化参数。我们展示了一个攻击阶段，仅需337毫秒即可恢复硬件参数，准确率超过95%；在421毫秒内通过对FINN加速器运行CNN的4条迹线进行平均，完全恢复这些参数，两者均使用侧信道迹线上的随机森林分类器，即使加速器数据流完全加载。这种方法提供了比现有方法更真实的攻击场景，与基于tsfresh的SoA攻击相比，我们的方法在准备和攻击阶段分别所需时间减少了940倍和110倍，即使不平均迹线也能获得更好的结果。", "summary": "本文提出了一种针对基于FINN框架的数据流AI加速器的侧信道攻击方法，旨在高效恢复其硬件配置参数。该方法利用无监督降维技术降低了计算复杂度，并结合轻量级分类器（如随机森林）分析侧信道迹线，成功提取了加速器的折叠和量化参数。实验证明，该方法在极短时间内（毫秒级）即可实现高精度（>95%）的参数恢复，且在速度上远超现有技术，为更真实的攻击场景提供了可能。", "keywords": "侧信道攻击, 数据流加速器, 硬件参数提取, FINN框架, 无监督降维", "comments": "这项工作在侧信道攻击领域具有重要意义，它揭示了数据流AI加速器在部署便利性背后存在的安全漏洞。其创新之处在于通过无监督降维显著提升了攻击效率和速度，使得对硬件参数的快速高精度恢复成为可能，这对于评估和增强AI加速器的安全性具有重要的参考价值。"}}
{"id": "2506.15497", "title": "Foundation of Affective Computing and Interaction", "authors": ["Changzeng Fu"], "summary": "This book provides a comprehensive exploration of affective computing and\nhuman-computer interaction technologies. It begins with the historical\ndevelopment and basic concepts of human-computer interaction, delving into the\ntechnical frameworks and practical applications of emotional computing, visual\ninteraction, voice interaction, brain-computer interfaces, physiological\nelectrical signal analysis, and social robotics. The book covers a wide range\nof topics, including the psychological and neuroscience foundations of emotion,\nmultimodal emotion recognition, emotional expression mechanisms, and the\nprinciples of brain-computer interfaces.\n  Key technologies such as affective computing based on discrete emotion theory\nand dimensional models, visual perception principles, speech recognition and\nsynthesis, EEG signal acquisition and processing, and multimodal emotion\nrecognition are explained in detail. This book also addresses the technical\nchallenges in the field, including multimodal data fusion, privacy and\nsecurity, and ethical considerations in human-machine relationships. It\ndiscusses the applications of these technologies across various domains such as\neducation, healthcare, entertainment, and intelligent assistance.\n  Looking to the future, the book anticipates trends such as the deep\nintegration of artificial intelligence with emotion recognition, the\nadvancement of multimodal interaction technologies, and the development of more\npersonalized and adaptive emotion recognition systems. It emphasizes the\nimportance of balancing technological innovation with ethical considerations to\nensure the responsible development and application of affective computing\ntechnologies.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15497v1", "AI": {"title_translation": "情感计算与交互的基础", "tldr": "本书全面探讨情感计算与人机交互技术，涵盖历史发展、技术框架、应用、关键技术、挑战及未来趋势，并强调伦理考量。", "motivation": "旨在全面探索情感计算与人机交互技术，提供其基础、应用和未来发展的综合性内容。", "method": "本书全面涵盖了人机交互的历史发展、基本概念、技术框架和实际应用，深入讲解了情感计算、视觉交互、语音交互、脑机接口、生理电信号分析以及社交机器人等关键技术，并探讨了多模态数据融合、隐私安全和伦理考量等技术挑战。", "result": "本书提供了对情感计算和人机交互的全面理解，详细解释了各种技术、其心理学和神经科学基础、跨领域的应用，并讨论了该领域面临的挑战和未来发展方向。", "conclusion": "本书展望了人工智能与情感识别的深度融合、多模态交互技术的进步以及更个性化和自适应情感识别系统的发展趋势，并强调了在技术创新与伦理考量之间取得平衡的重要性，以确保情感计算技术的负责任发展和应用。", "translation": "本书全面探讨了情感计算与人机交互技术。它从人机交互的历史发展和基本概念开始，深入探讨了情感计算、视觉交互、语音交互、脑机接口、生理电信号分析和社交机器人的技术框架和实际应用。本书涵盖了广泛的主题，包括情感的心理学和神经科学基础、多模态情感识别、情感表达机制以及脑机接口的原理。\n离散情感理论和维度模型基础上的情感计算、视觉感知原理、语音识别与合成、脑电图信号采集与处理以及多模态情感识别等关键技术都得到了详细解释。本书还探讨了该领域的技术挑战，包括多模态数据融合、隐私与安全以及人机关系中的伦理考量。它讨论了这些技术在教育、医疗保健、娱乐和智能辅助等各个领域的应用。\n展望未来，本书预测了人工智能与情感识别的深度融合、多模态交互技术的进步以及更个性化和自适应情感识别系统的发展趋势。它强调了平衡技术创新与伦理考量的重要性，以确保情感计算技术的负责任发展和应用。", "summary": "本书全面概述了情感计算与人机交互。它详细阐述了其历史背景、技术框架，包括多模态情感识别、视觉/语音交互和脑机接口，以及其心理学和神经科学基础。书中还探讨了关键技术、数据融合和伦理等挑战，以及在教育、医疗等领域的应用，并展望了未来趋势，强调了负责任的技术发展。", "keywords": "情感计算, 人机交互, 情感识别, 脑机接口, 多模态交互", "comments": "本书作为一本基础性著作，全面而深入地探讨了情感计算与人机交互领域。其优势在于覆盖范围广泛，从历史背景和基本概念，到先进技术、实际应用、面临的挑战以及伦理考量，为该领域的研究人员和实践者提供了宝贵的参考。"}}
{"id": "2506.15175", "title": "SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization", "authors": ["Hanjun Kim", "Minwoo Jung", "Wooseong Yang", "Ayoung Kim"], "summary": "Despite the growing adoption of radar in robotics, the majority of research\nhas been confined to homogeneous sensor types, overlooking the integration and\ncross-modality challenges inherent in heterogeneous radar technologies. This\nleads to significant difficulties in generalizing across diverse radar data\ntypes, with modality-aware approaches that could leverage the complementary\nstrengths of heterogeneous radar remaining unexplored. To bridge these gaps, we\npropose SHeRLoc, the first deep network tailored for heterogeneous radar, which\nutilizes RCS polar matching to align multimodal radar data. Our hierarchical\noptimal transport-based feature aggregation method generates rotationally\nrobust multi-scale descriptors. By employing FFT-similarity-based data mining\nand adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric\nlearning. SHeRLoc achieves an order of magnitude improvement in heterogeneous\nradar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public\ndataset and outperforming state of-the-art methods. Also applicable to LiDAR,\nSHeRLoc paves the way for cross-modal place recognition and heterogeneous\nsensor SLAM. The source code will be available upon acceptance.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15175v1", "AI": {"title_translation": "SHeRLoc: 同步异构雷达地点识别用于跨模态定位", "tldr": "SHeRLoc是一个针对异构雷达的深度网络，通过RCS极匹配和分层最优传输实现跨模态地点识别，显著提高了异构雷达地点识别的召回率。", "motivation": "现有雷达研究主要局限于同质传感器类型，忽视了异构雷达技术中的集成和跨模态挑战，导致数据泛化困难，且未充分利用异构雷达的互补优势。", "method": "提出SHeRLoc，第一个为异构雷达设计的深度网络。它利用RCS极匹配对多模态雷达数据进行对齐，并采用基于分层最优传输的特征聚合方法生成旋转鲁棒的多尺度描述符。通过基于FFT相似度的数据挖掘和自适应边界三元组损失，SHeRLoc实现了FOV感知的度量学习。", "result": "SHeRLoc在异构雷达地点识别方面实现了数量级的改进，在公开数据集上将recall@1从低于0.1提高到0.9，并优于现有最先进的方法。也适用于激光雷达，为跨模态地点识别和异构传感器SLAM铺平了道路。", "conclusion": "SHeRLoc为异构雷达地点识别提供了一个有效解决方案，并为跨模态地点识别和异构传感器SLAM铺平了道路。", "translation": "尽管雷达在机器人领域日益普及，但大多数研究仅限于同质传感器类型，忽视了异构雷达技术固有的集成和跨模态挑战。这导致在不同雷达数据类型之间进行泛化时面临巨大困难，而能够利用异构雷达互补优势的模态感知方法仍未被探索。为了弥补这些差距，我们提出了SHeRLoc，这是第一个专为异构雷达量身定制的深度网络，它利用RCS极匹配来对齐多模态雷达数据。我们基于分层最优传输的特征聚合方法生成了旋转鲁棒的多尺度描述符。通过采用基于FFT相似度的数据挖掘和自适应边界三元组损失，SHeRLoc实现了FOV感知的度量学习。SHeRLoc在异构雷达地点识别方面实现了数量级的改进，在公共数据集上将recall@1从低于0.1提高到0.9，并超越了最先进的方法。SHeRLoc也适用于激光雷达，为跨模态地点识别和异构传感器SLAM铺平了道路。源代码将在接受后提供。", "summary": "SHeRLoc是一个创新的深度学习框架，专为解决异构雷达数据集成和跨模态地点识别的挑战而设计。它通过RCS极匹配对齐多模态雷达数据，并使用分层最优传输生成旋转鲁棒的特征描述符。结合FFT相似度数据挖掘和自适应边界三元组损失，SHeRLoc实现了FOV感知的度量学习。实验结果表明，SHeRLoc在异构雷达地点识别方面表现出色，显著提高了召回率，并为跨模态定位和异构传感器SLAM奠定了基础。", "keywords": "异构雷达, 地点识别, 跨模态定位, 深度学习, 雷达SLAM", "comments": "SHeRLoc的创新之处在于它是首个专门为异构雷达设计的深度网络，通过独特的RCS极匹配和分层最优传输方法，有效解决了跨模态数据对齐和特征表示的难题。其在召回率上的数量级提升显示出其在实际应用中的巨大潜力，尤其是在多传感器融合的机器人领域。"}}
{"id": "2506.14919", "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models", "authors": ["Xinkai Zhao", "Yuta Tokuoka", "Junichiro Iwasawa", "Keita Oda"], "summary": "The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14919v1", "AI": {"title_translation": "基于频率校准的医学图像扩散模型成员推断攻击", "tldr": "本文提出了一种频率校准的重建误差（FCRE）方法，用于对医学图像扩散模型进行成员推断攻击，通过关注中频重建误差来提高攻击性能。", "motivation": "扩散模型在医学图像生成中的广泛应用引发了严重的隐私担忧。成员推断攻击（MIA）可用于量化隐私风险。然而，现有MIA方法直接应用于医学图像时面临挑战，因为重建误差受图像固有难度影响，且扩散模型难以重建高频细节。", "method": "我们提出了一种频率校准的重建误差（FCRE）方法。该方法通过关注特定中频范围内的重建误差，并排除高频（难以重建）和低频（信息量较少）区域，从而减轻了图像固有难度的混淆因素。具体来说，我们分析逆向扩散过程，获取中频重建误差，并计算重建图像与原始图像之间的结构相似性指数分数。通过将此分数与阈值比较来确定成员身份。", "result": "在多个医学图像数据集上的实验表明，我们的FCRE方法优于现有的MIA方法。", "conclusion": "通过引入频率校准的重建误差，本文提出的FCRE方法有效解决了现有成员推断攻击在医学图像扩散模型上的局限性，并提高了攻击性能，从而更好地量化了医学图像生成中的隐私风险。", "translation": "扩散模型在图像生成中，尤其是在医学成像等敏感领域的日益增长的使用，引发了显著的隐私担忧。成员推断攻击（MIA）已成为一种潜在的方法，用于确定特定图像是否曾用于训练扩散模型，从而量化隐私风险。现有的MIA方法通常依赖于扩散重建误差，其中成员图像的重建误差预计低于非成员图像。然而，将这些方法直接应用于医学图像面临挑战。重建误差受固有图像难度影响，并且扩散模型在高频细节重建方面存在困难。为了解决这些问题，我们提出了一种用于医学图像扩散模型的频率校准重建误差（FCRE）方法，用于MIA。通过关注特定中频范围内的重建误差，并排除高频（难以重建）和低频（信息量较少）区域，我们的频率选择性方法减轻了固有图像难度的混淆因素。具体来说，我们分析逆向扩散过程，获取中频重建误差，并计算重建图像与原始图像之间的结构相似性指数分数。通过将此分数与阈值比较来确定成员身份。在多个医学图像数据集上的实验表明，我们的FCRE方法优于现有的MIA方法。", "summary": "本文针对医学图像扩散模型中的隐私问题，提出了一种名为频率校准重建误差（FCRE）的新型成员推断攻击（MIA）方法。该方法通过专注于中频重建误差，并排除高频和低频区域，有效克服了现有MIA方法在医学图像上因图像固有难度和高频细节重建困难而面临的挑战。实验结果表明，FCRE方法在多个医学图像数据集上均优于现有方法，为量化扩散模型中的隐私风险提供了更有效手段。", "keywords": "成员推断攻击, 扩散模型, 医学图像, 隐私, 频率校准", "comments": "本文创新性地提出了频率校准的重建误差方法，通过频率选择性地分析图像重建误差，有效解决了现有成员推断攻击在医学图像领域面临的挑战，为评估医学图像扩散模型的隐私安全性提供了更精确的工具。"}}
{"id": "2506.14813", "title": "Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks", "authors": ["Yuxuan Jiang", "Ziming Zhou", "Boyu Xu", "Beijie Liu", "Runhui Xu", "Peng Huang"], "summary": "Training deep learning (DL) models is a complex process, making it prone to\nsilent errors that are challenging to detect and diagnose. This paper presents\nTRAINCHECK, a framework that takes a proactive checking approach to address\nsilent training errors. TRAINCHECK automatically infers invariants tailored for\nDL training. It uses these invariants to proactively detect silent errors\nduring the training process while providing debugging help. To evaluate\nTRAINCHECK, we reproduce 20 real-world silent training errors with diverse root\ncauses. TRAINCHECK successfully detects 18 errors within a single training\niteration. It also uncovers 6 unknown bugs in popular training libraries that\nlead to silent errors.", "comment": "19 pages, to appear in 19th USENIX Symposium on Operating Systems\n  Design and Implementation (OSDI '25)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14813v1", "AI": {"title_translation": "自信训练：通过自动化主动检查捕获深度学习训练中的无声错误", "tldr": "TRAINCHECK通过自动推断不变量，主动检测深度学习训练中的无声错误，并在一次迭代中成功捕获了大多数错误，甚至发现了流行库中的新bug。", "motivation": "训练深度学习（DL）模型是一个复杂的过程，容易出现难以检测和诊断的无声错误。", "method": "本文提出了TRAINCHECK框架，它采用主动检查方法。TRAINCHECK自动推断针对深度学习训练的不变量，并利用这些不变量在训练过程中主动检测无声错误，同时提供调试帮助。", "result": "TRAINCHECK成功检测到20个真实世界无声训练错误中的18个，且仅在一次训练迭代中完成。它还发现了流行训练库中导致无声错误的6个未知bug。", "conclusion": "TRAINCHECK能够有效且高效地检测深度学习训练中的无声错误，并有助于发现潜在的库级问题。", "translation": "训练深度学习（DL）模型是一个复杂的过程，这使得它容易出现难以检测和诊断的无声错误。本文提出了TRAINCHECK，一个采用主动检查方法来解决无声训练错误的框架。TRAINCHECK自动推断针对DL训练的定制不变量。它使用这些不变量在训练过程中主动检测无声错误，同时提供调试帮助。为了评估TRAINCHECK，我们重现了20个具有不同根本原因的真实世界无声训练错误。TRAINCHECK在单次训练迭代中成功检测到18个错误。它还在流行的训练库中发现了6个导致无声错误的未知bug。", "summary": "本文提出了TRAINCHECK框架，旨在解决深度学习训练中难以发现的无声错误。TRAINCHECK通过自动推断并利用训练不变量，主动检测这些错误并提供调试支持。实验表明，TRAINCHECK在单次迭代中成功检测了20个真实错误中的18个，并发现了流行训练库中的6个新bug，证明了其在提高深度学习训练可靠性方面的有效性。", "keywords": "深度学习训练, 无声错误, 自动化检查, 不变量, TRAINCHECK", "comments": "TRAINCHECK的创新点在于其主动检查方法和自动推断不变量的能力，这对于复杂且易出错的深度学习训练过程至关重要。它不仅能帮助开发者捕获难以发现的无声错误，还能揭示流行库中的潜在bug，对于提升深度学习系统的健壮性和可靠性具有重要意义。"}}
{"id": "2506.15239", "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15239v1", "AI": {"title_translation": "迷失在变体中？评估巴斯克语和西班牙语地理变体的NLI性能", "tldr": "研究发现，当前语言技术在处理巴斯克语和西班牙语的语言变体时，NLI性能会下降，尤其是在巴斯克语中。", "motivation": "评估当前语言技术理解巴斯克语和西班牙语变体的能力，并探究语言变体对NLI性能的影响。", "method": "使用自然语言推理（NLI）作为核心任务，引入了一个新的、人工整理的巴斯克语和西班牙语及其各自变体的平行数据集。通过使用仅编码器和基于解码器的大型语言模型（LLMs）进行跨语言和上下文学习实验，并进行了错误分析和消融实验。", "result": "处理语言变体时，性能会下降，尤其是在巴斯克语中。性能下降并非由于词汇重叠，而是由于语言变体本身。仅编码器模型在处理西部巴斯克语时表现尤其困难。", "conclusion": "当前语言技术在处理语言变体时存在显著挑战，特别是对于标准语体差异较大的方言（如西部巴斯克语）。", "translation": "本文评估了当前语言技术理解巴斯克语和西班牙语语言变体的能力。我们使用自然语言推理（NLI）作为核心任务，并引入了一个新颖的、人工整理的巴斯克语和西班牙语及其各自变体的平行数据集。我们对使用仅编码器和基于解码器的大型语言模型（LLM）进行的跨语言和上下文学习实验的实证分析表明，在处理语言变体时，性能会出现下降，尤其是在巴斯克语中。错误分析表明，这种下降并非由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在处理西部巴斯克语时尤其困难，这与语言学理论相符，即边缘方言（例如西部方言）与标准语体的距离更远。所有数据和代码均已公开。", "summary": "本文评估了现有语言技术在理解巴斯克语和西班牙语语言变体方面的能力，以自然语言推理（NLI）为核心任务。研究引入了一个新的巴斯克语和西班牙语变体平行数据集，并使用编码器和解码器LLM进行实验。结果显示，处理语言变体时性能显著下降，尤其是在巴斯克语中，且这种下降主要归因于语言变体本身而非词汇重叠。此外，编码器模型在处理西部巴斯克语时表现出明显困难。", "keywords": "语言变体, 自然语言推理, LLM, 巴斯克语, 西班牙语", "comments": "这项研究通过构建新的多变体数据集和深入的错误分析，揭示了当前LLMs在处理语言变体方面的局限性，特别是对于与标准语差异较大的方言。其贡献在于强调了语言多样性对模型性能的影响，并为未来多语言和多变体NLP研究提供了宝贵的数据集和见解。"}}
{"id": "2506.15259", "title": "Splitting-based randomised dynamical low-rank approximations for stiff matrix differential equations", "authors": ["Zi Wu", "Yong-Liang Zhao"], "summary": "In the fields of control theory and machine learning, the dynamic low-rank\napproximation for large-scale matrices has received substantial attention.\nConsidering the large-scale semilinear stiff matrix differential equations, we\npropose a dynamic numerical integrator for obtaining low-rank approximations of\nsolutions. We first decompose the differential equation into a stiff linear\ncomponent and a nonstiff nonlinear term, then employ an exponential integrator\nalong with a dynamic low-rank approach to resolve these subsystems,\nrespectively. Furthermore, the proposed framework naturally extends to\nrank-adaptation scenarios. Through rigorous validation on canonical stiff\nmatrix differential problems, including spatially discretized Allen-Cahn\nequations and differential Riccati equations, we demonstrate that the method\nachieves the theoretically predicted convergence orders. Numerical evidence\nconfirms the robustness and accuracy of the proposed methods.", "comment": "6 figures, 12 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15259v1", "AI": {"title_translation": "基于分裂的随机动态低秩逼近用于刚性矩阵微分方程", "tldr": "本文提出一种基于分裂的随机动态低秩近似方法，用于求解大规模半线性刚性矩阵微分方程，该方法在理论收敛阶数和数值鲁棒性、准确性方面表现良好。", "motivation": "在大规模矩阵的动态低秩近似在控制理论和机器学习领域受到广泛关注，尤其针对大规模半线性刚性矩阵微分方程的求解。", "method": "将微分方程分解为刚性线性分量和非刚性非线性项，然后分别采用指数积分器和动态低秩方法来解决这些子系统。该框架还支持秩自适应。", "result": "该方法在典型的刚性矩阵微分问题（包括空间离散化的Allen-Cahn方程和微分Riccati方程）上实现了理论预测的收敛阶数。数值证据证实了所提方法的鲁棒性和准确性。", "conclusion": "所提出的基于分裂的随机动态低秩近似方法对大规模半线性刚性矩阵微分方程有效且表现优异，能达到理论收敛阶数并具有良好的鲁棒性和准确性。", "translation": "在控制理论和机器学习领域，大规模矩阵的动态低秩近似受到了广泛关注。考虑到大规模半线性刚性矩阵微分方程，我们提出了一种用于获取解的低秩近似的动态数值积分器。我们首先将微分方程分解为刚性线性分量和非刚性非线性项，然后分别采用指数积分器和动态低秩方法来解决这些子系统。此外，所提出的框架自然地扩展到秩自适应场景。通过对典型的刚性矩阵微分问题（包括空间离散化的Allen-Cahn方程和微分Riccati方程）进行严格验证，我们证明了该方法达到了理论预测的收敛阶数。数值证据证实了所提方法的鲁棒性和准确性。", "summary": "本文提出一种新的动态数值积分器，用于求解大规模半线性刚性矩阵微分方程的低秩近似。该方法通过将方程分解为刚性线性部分和非刚性非线性部分，并分别采用指数积分器和动态低秩方法进行求解。研究表明，该方法在典型问题上能达到理论收敛阶数，并具有良好的鲁棒性和准确性，且支持秩自适应。", "keywords": "动态低秩近似, 刚性矩阵微分方程, 分裂方法, 指数积分器, 秩自适应", "comments": "该论文的创新点在于将分裂思想与动态低秩近似结合，并引入指数积分器处理刚性部分，有效解决了大规模刚性矩阵微分方程的求解难题。这种方法在控制理论和机器学习等领域具有潜在应用价值。"}}
{"id": "2506.15547", "title": "An efficient construction of Raz's two-source randomness extractor with improved parameters", "authors": ["Cameron Foreman", "Lewis Wooltorton", "Kevin Milner", "Florian J. Curchod"], "summary": "Randomness extractors are algorithms that distill weak random sources into\nnear-perfect random numbers. Two-source extractors enable this distillation\nprocess by combining two independent weak random sources. Raz's extractor (STOC\n'05) was the first to achieve this in a setting where one source has linear\nmin-entropy (i.e., proportional to its length), while the other has only\nlogarithmic min-entropy in its length. However, Raz's original construction is\nimpractical due to a polynomial computation time of at least degree 4. Our work\nsolves this problem by presenting an improved version of Raz's extractor with\nquasi-linear computation time, as well as a new analytic theorem with reduced\nentropy requirements. We provide comprehensive analytical and numerical\ncomparisons of our construction with others in the literature, and we derive\nstrong and quantum-proof versions of our efficient Raz extractor. Additionally,\nwe offer an easy-to-use, open-source code implementation of the extractor and a\nnumerical parameter calculation module.", "comment": "12 + 11 pages. Comments welcome!", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15547v1", "AI": {"title_translation": "Raz双源随机性提取器的改进参数高效构造", "tldr": "改进了Raz双源随机性提取器的效率和参数，使其更实用。", "motivation": "Raz的原始双源随机性提取器由于计算时间复杂度高（至少四次方）而不切实际。", "method": "提出了Raz提取器的改进版本，实现了准线性计算时间；提出了一个新的分析定理，降低了熵要求；提供了与现有文献的全面分析和数值比较；导出了高效Raz提取器的强版本和抗量子版本；提供了提取器的易用开源代码实现和数值参数计算模块。", "result": "实现了Raz提取器的准线性计算时间；降低了熵要求；提供了全面的比较数据；成功导出了强和抗量子版本；提供了开源实现。", "conclusion": "成功地将Raz的非实用性双源随机性提取器改进为高效且实用的版本，并降低了其熵要求，使其在实际应用中更具可行性。", "translation": "随机性提取器是能将弱随机源提炼成近乎完美随机数的算法。双源提取器通过结合两个独立的弱随机源来实现这一提炼过程。Raz的提取器（STOC '05）是第一个在其中一个源具有线性最小熵（即与其长度成比例），而另一个源只有对数最小熵的情况下实现这一目标的。然而，Raz的原始构造由于至少四次方的多项式计算时间而变得不切实际。我们的工作通过提出Raz提取器的改进版本来解决这个问题，该版本具有准线性计算时间，以及一个具有降低熵要求的新分析定理。我们提供了我们的构造与文献中其他构造的全面分析和数值比较，并且我们导出了我们高效Raz提取器的强版本和抗量子版本。此外，我们还提供了提取器的易于使用、开源的代码实现和数值参数计算模块。", "summary": "本文针对Raz双源随机性提取器计算效率低的问题，提出了一种改进的构造方法。该方法将计算时间从多项式级别降低到准线性级别，并通过新的分析定理降低了熵要求。研究还提供了与现有方法的比较、强和抗量子版本，并发布了开源代码，显著提升了Raz提取器的实用性。", "keywords": "随机性提取器, Raz提取器, 双源提取器, 计算效率, 准线性时间", "comments": "本文的创新之处在于显著提升了Raz双源随机性提取器的计算效率，使其从理论上可行变为实际可用的工具。通过引入准线性计算时间、降低熵要求以及提供抗量子版本和开源实现，极大地推动了双源随机性提取器在密码学和随机性生成领域的应用潜力。"}}
{"id": "2506.15512", "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach", "authors": ["Wenqi Guan", "Yang Fang"], "summary": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15512v1", "AI": {"title_translation": "在LangChain中集成GPT并采用CoT增强的提示工程方法优化基于Web的AI查询检索", "tldr": "本文提出了一种在LangChain框架中集成GPT模型并结合CoT推理和提示工程的方法，以优化远程学习资源检索的深度和上下文相关性，从而提高用户满意度和学习成果。", "motivation": "当前远程学习资源的检索缺乏上下文深度，无法为复杂的学生查询提供全面的信息。大型语言模型（LLMs）的出现为教育活动带来了变革，因此需要一种更有效的方式来检索远程学习资源。", "method": "本文提出了一种新颖的方法，通过在LangChain框架中集成基于GPT的模型来增强远程学习检索。该系统通过使用CoT（Chain-of-Thought）推理和提示工程以更直观和高效的方式实现。该框架旨在提高检索结果的准确性和相关性，以返回全面且上下文丰富的解释和资源。", "result": "该方法在与范式LLMs的评估中显示出有效性，并报告了用户满意度和学习成果的改进。", "conclusion": "通过在LangChain中集成GPT模型并结合CoT增强的提示工程，可以显著提高远程学习资源检索的精度和相关性，从而提升学生的用户满意度和学习成果。", "translation": "大型语言模型为远程学习学生以及教育活动的其他方面带来了根本性的改变。当前远程学习资源的检索在上下文意义上缺乏深度，无法为复杂的学生查询提供全面的信息。本工作提出了一种新颖的方法，通过在LangChain框架内集成基于GPT的模型来增强远程学习检索。我们通过使用CoT推理和提示工程，以更直观和高效的方式实现了这一系统。我们提出的框架非常强调提高检索结果的精度和相关性，以返回最适合每个学生需求的全面且上下文丰富的解释和资源。我们还评估了我们的方法与范式LLMs相比的有效性，并报告了用户满意度和学习成果的改进。", "summary": "本文提出了一种利用GPT模型在LangChain框架中进行集成的CoT增强提示工程方法，旨在优化基于Web的AI查询检索，尤其针对远程学习资源。该方法通过增加检索结果的上下文深度、精度和相关性，解决了当前远程学习资源检索中信息缺乏的问题，并通过评估证实了其在提升用户满意度和学习成果方面的有效性。", "keywords": "GPT集成, LangChain, CoT推理, 提示工程, 远程学习检索", "comments": "该论文通过将GPT模型与LangChain框架相结合，并引入CoT推理和提示工程，为远程学习资源的智能检索提供了一种创新方案。其强调上下文深度和个性化匹配，对提升教育资源的可用性和学生学习体验具有重要意义。该方法有望解决传统检索系统在处理复杂查询时的局限性。"}}
{"id": "2506.15249", "title": "Context-Aware Deep Lagrangian Networks for Model Predictive Control", "authors": ["Lucas Schulze", "Jan Peters", "Oleg Arenz"], "summary": "Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15249v1", "AI": {"title_translation": "用于模型预测控制的上下文感知深度拉格朗日网络", "tldr": "该论文扩展了深度拉格朗日网络（DeLaN），使其具有上下文感知能力，并将其与在线系统识别和模型预测控制（MPC）相结合，用于自适应、物理信息化的机器人控制，在7自由度机械臂上显示出改进的跟踪误差。", "motivation": "基于物理信息的动态模型（如深度拉格朗日网络DeLaN）可以提高机器人控制的泛化能力和可解释性。然而，在复杂环境中，潜在交互对象的数量庞大且其物理属性通常不确定，这使得采用单一全局模型不可行。因此，需要采用在线系统识别的上下文感知模型，这些模型仅捕获环境中当前相关的方面。尽管物理原理（如能量守恒）在不同上下文下可能不成立，但确保任何单个上下文感知模型的物理合理性仍然是非常可取的，尤其是在使用它进行诸如模型预测控制（MPC）之类的后退视界控制方法时。", "method": "本研究扩展了深度拉格朗日网络（DeLaN）使其具有上下文感知能力。将上下文感知的DeLaN与循环网络相结合，用于在线系统识别。将其与模型预测控制（MPC）集成，用于自适应、物理信息化的控制。还结合了DeLaN与残差动力学模型，以利用通常可用的机器人标称模型。该方法在7自由度机械臂上进行了评估，用于在不同负载下的轨迹跟踪。", "result": "与使用扩展卡尔曼滤波的基线方法相比，本方法将末端执行器跟踪误差减少了39%，而基线方法实现了21%的改进。", "conclusion": "本研究提出了一种上下文感知深度拉格朗日网络（DeLaN）与在线系统识别和模型预测控制（MPC）相结合的方法，并引入了残差动力学模型，以实现自适应、物理信息化的机器人控制。实验证明，该方法在复杂环境中显著降低了轨迹跟踪误差，表现优于现有基线方法。", "translation": "基于物理信息的动态模型（例如深度拉格朗日网络，DeLaN）控制机器人可以提高其行为的泛化能力和可解释性。然而，在复杂环境中，可能交互的物体数量庞大，且其物理属性通常不确定。这种复杂性使得采用单一全局模型不可行。因此，我们需要诉诸于在线系统识别的上下文感知模型，这些模型仅捕获环境中当前相关的方面。虽然物理原理（例如能量守恒）可能无法在不同上下文之间保持不变，但确保任何单个上下文感知模型的物理合理性仍然是非常可取的，特别是在将其用于模型预测控制（MPC）等后退视界控制方法时。因此，在这项工作中，我们扩展了DeLaN使其具有上下文感知能力，将其与用于在线系统识别的循环网络相结合，并将其与MPC集成，以实现自适应的、物理信息化的控制。我们还将DeLaN与残差动力学模型相结合，以利用通常可用的机器人标称模型。我们在一个7自由度机械臂上评估了我们的方法，用于在不同负载下的轨迹跟踪。与使用扩展卡尔曼滤波的基线方法相比，我们的方法将末端执行器跟踪误差减少了39%，而基线方法实现了21%的改进。", "summary": "本论文解决了在复杂不确定环境中机器人控制的挑战，其中单一全局动态模型不足。它提出扩展深度拉格朗日网络（DeLaN）使其具有上下文感知能力，并将其与循环网络结合进行在线系统识别，与模型预测控制（MPC）集成以实现自适应、物理信息化的控制。该方法还结合了残差动力学模型以利用现有的机器人标称模型。在7自由度机械臂上进行评估，所提出的方法将末端执行器跟踪误差减少了39%，显著优于使用扩展卡尔曼滤波的基线方法。", "keywords": "上下文感知, 深度拉格朗日网络, 模型预测控制, 在线系统识别, 机器人控制", "comments": "创新点：将DeLaN扩展为上下文感知模型以应对复杂环境，并结合了在线系统识别、MPC以及残差动力学模型，有效解决了机器人控制中的模型不确定性和环境可变性问题。重要性：提供了一种鲁棒的、物理信息化的控制方法，能够适应不同上下文，提高了机器人行为的泛化能力和可解释性。显著的跟踪误差降低证明了其在实际应用中的价值。局限性：摘要中未明确提及，但通常MPC与复杂模型结合时可能存在计算成本问题，或对训练中未见过的全新上下文的鲁棒性。"}}
{"id": "2506.14934", "title": "Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "Arian Rahman Aditta", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "summary": "Distinguishing between quark- and gluon-initiated jets is a critical and\nchallenging task in high-energy physics, pivotal for improving new physics\nsearches and precision measurements at the Large Hadron Collider. While deep\nlearning, particularly Convolutional Neural Networks (CNNs), has advanced jet\ntagging using image-based representations, the potential of Vision Transformer\n(ViT) architectures, renowned for modeling global contextual information,\nremains largely underexplored for direct calorimeter image analysis, especially\nunder realistic detector and pileup conditions. This paper presents a\nsystematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet\nclassification using simulated 2012 CMS Open Data. We construct multi-channel\njet-view images from detector-level energy deposits (ECAL, HCAL) and\nreconstructed tracks, enabling an end-to-end learning approach. Our\ncomprehensive benchmarking demonstrates that ViT-based models, notably\nViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN\nbaselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of\ncapturing long-range spatial correlations within jet substructure. This work\nestablishes the first systematic framework and robust performance baselines for\napplying ViT architectures to calorimeter image-based jet classification using\npublic collider data, alongside a structured dataset suitable for further deep\nlearning research in this domain.", "comment": "Accepted in Third International Workshop on Generalizing from Limited\n  Resources in the Open World Workshop at International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14934v1", "AI": {"title_translation": "用于从量能器图像进行端到端夸克-胶子喷注分类的视觉Transformer", "tldr": "本研究系统评估了视觉Transformer（ViT）及其混合模型在夸克-胶子喷注分类中的应用，并证明其性能优于传统CNN基线。", "motivation": "在高能物理中，区分夸克和胶子喷注是一项关键且具有挑战性的任务，对于改进大型强子对撞机的新物理搜索和精确测量至关重要。尽管深度学习（特别是CNN）在基于图像的喷注标记方面取得了进展，但视觉Transformer（ViT）架构在直接量能器图像分析（尤其是在真实探测器和堆积条件下）中的潜力仍未得到充分探索。", "method": "本研究使用模拟的2012年CMS开放数据，对ViT和ViT-CNN混合模型进行了夸克-胶子喷注分类的系统评估。研究人员从探测器级别的能量沉积（ECAL、HCAL）和重建径迹构建了多通道喷注视图图像，实现了端到端的学习方法。", "result": "全面的基准测试表明，基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和准确性方面始终优于已建立的CNN基线，这突显了捕获喷注子结构中长程空间相关性的优势。", "conclusion": "这项工作建立了第一个系统框架和稳健的性能基线，用于将ViT架构应用于使用公共对撞机数据的基于量能器图像的喷注分类，并提供了一个适合该领域进一步深度学习研究的结构化数据集。", "translation": "区分夸克和胶子引起的喷注是高能物理中一项关键且具有挑战性的任务，对于改进大型强子对撞机的新物理搜索和精确测量至关重要。尽管深度学习，特别是卷积神经网络（CNNs），在使用基于图像的表示进行喷注标记方面取得了进展，但视觉Transformer（ViT）架构在建模全局上下文信息方面的潜力，对于直接量能器图像分析，尤其是在真实探测器和堆积条件下，仍未得到充分探索。本文系统评估了ViT和ViT-CNN混合模型在夸克-胶子喷注分类中的应用，使用了模拟的2012年CMS开放数据。我们从探测器级别的能量沉积（ECAL、HCAL）和重建径迹构建多通道喷注视图图像，实现了端到端的学习方法。我们的全面基准测试表明，基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和准确性方面始终优于已建立的CNN基线，这突显了捕获喷注子结构中长程空间相关性的优势。这项工作建立了第一个系统框架和稳健的性能基线，用于将ViT架构应用于使用公共对撞机数据的基于量能器图像的喷注分类，同时提供了一个适合该领域进一步深度学习研究的结构化数据集。", "summary": "本文系统评估了视觉Transformer（ViT）及其与CNN的混合模型在从量能器图像进行端到端夸克-胶子喷注分类中的性能。研究通过构建多通道喷注视图图像，并使用模拟的CMS开放数据进行训练，发现ViT基模型（如ViT+MaxViT和ViT+ConvNeXt）在F1分数、ROC-AUC和准确性上均优于传统CNN基线，证明了ViT在捕获喷注子结构长程空间相关性方面的优势。该工作为基于量能器图像的喷注分类应用ViT架构建立了首个系统框架和性能基线，并提供了相关数据集。", "keywords": "夸克-胶子喷注分类, 视觉Transformer, 量能器图像, 深度学习, 高能物理", "comments": "这项研究的创新之处在于首次系统地将视觉Transformer架构应用于高能物理中夸克-胶子喷注的直接量能器图像分类任务，并建立了可靠的性能基线。ViT在捕获长程空间相关性方面的优势被有效利用，超越了传统CNN的性能。此外，公开数据集的构建也为该领域的后续深度学习研究奠定了基础，具有重要的推动作用。"}}
{"id": "2506.14815", "title": "Predicting Anthropometric Body Composition Variables Using 3D Optical Imaging and Machine Learning", "authors": ["Gyaneshwar Agrahari", "Kiran Bist", "Monika Pandey", "Jacob Kapita", "Zachary James", "Jackson Knox", "Steven Heymsfield", "Sophia Ramirez", "Peter Wolenski", "Nadejda Drenska"], "summary": "Accurate prediction of anthropometric body composition variables, such as\nAppendicular Lean Mass (ALM), Body Fat Percentage (BFP), and Bone Mineral\nDensity (BMD), is essential for early diagnosis of several chronic diseases.\nCurrently, researchers rely on Dual-Energy X-ray Absorptiometry (DXA) scans to\nmeasure these metrics; however, DXA scans are costly and time-consuming. This\nwork proposes an alternative to DXA scans by applying statistical and machine\nlearning models on biomarkers (height, volume, left calf circumference, etc)\nobtained from 3D optical images. The dataset consists of 847 patients and was\nsourced from Pennington Biomedical Research Center. Extracting patients' data\nin healthcare faces many technical challenges and legal restrictions. However,\nmost supervised machine learning algorithms are inherently data-intensive,\nrequiring a large amount of training data. To overcome these limitations, we\nimplemented a semi-supervised model, the $p$-Laplacian regression model. This\npaper is the first to demonstrate the application of a $p$-Laplacian model for\nregression. Our $p$-Laplacian model yielded errors of $\\sim13\\%$ for ALM,\n$\\sim10\\%$ for BMD, and $\\sim20\\%$ for BFP when the training data accounted for\n10 percent of all data. Among the supervised algorithms we implemented, Support\nVector Regression (SVR) performed the best for ALM and BMD, yielding errors of\n$\\sim 8\\%$ for both, while Least Squares SVR performed the best for BFP with\n$\\sim 11\\%$ error when trained on 80 percent of the data. Our findings position\nthe $p$-Laplacian model as a promising tool for healthcare applications,\nparticularly in a data-constrained environment.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14815v1", "AI": {"title_translation": "使用3D光学成像和机器学习预测人体测量身体成分变量", "tldr": "本研究提出使用3D光学成像和机器学习模型（包括半监督p-Laplacian回归）作为DXA扫描的替代方案，用于预测人体成分变量，并在数据受限环境下表现出潜力。", "motivation": "准确预测人体成分变量（如ALM、BFP、BMD）对于慢性病的早期诊断至关重要。目前，研究人员依赖昂贵且耗时的DXA扫描来测量这些指标。", "method": "本研究提出一种替代DXA扫描的方法，即对从3D光学图像中获得的生物标志物（如身高、体积、左小腿围等）应用统计和机器学习模型。数据集包含847名患者。为克服医疗数据获取的挑战和限制，研究实施了半监督p-Laplacian回归模型，并首次将其应用于回归任务。同时，也评估了监督学习算法。", "result": "当训练数据占总数据的10%时，p-Laplacian模型对ALM、BMD和BFP的误差分别约为13%、10%和20%。在监督算法中，当使用80%的数据进行训练时，支持向量回归（SVR）对ALM和BMD的表现最佳，误差均为约8%；而最小二乘SVR对BFP表现最佳，误差约为11%。", "conclusion": "p-Laplacian模型在医疗应用中，尤其是在数据受限的环境下，被定位为一种有前景的工具。", "translation": "准确预测人体测量身体成分变量，如四肢瘦体重（ALM）、体脂百分比（BFP）和骨矿物质密度（BMD），对于几种慢性病的早期诊断至关重要。目前，研究人员依赖双能X射线吸收仪（DXA）扫描来测量这些指标；然而，DXA扫描成本高昂且耗时。本研究提出一种替代DXA扫描的方法，即对从3D光学图像中获得的生物标志物（身高、体积、左小腿围等）应用统计和机器学习模型。数据集包含847名患者，来源于彭宁顿生物医学研究中心。在医疗保健领域提取患者数据面临许多技术挑战和法律限制。然而，大多数监督机器学习算法本质上是数据密集型的，需要大量的训练数据。为了克服这些限制，我们实现了一种半监督模型——p-Laplacian回归模型。本文首次展示了p-Laplacian模型在回归中的应用。当训练数据占总数据的10%时，我们的p-Laplacian模型对ALM、BMD和BFP的误差分别约为13%、10%和20%。在我们实现的监督算法中，支持向量回归（SVR）对ALM和BMD的表现最佳，误差均为约8%，而最小二乘SVR对BFP的表现最佳，误差约为11%（当使用80%的数据进行训练时）。我们的发现表明，p-Laplacian模型是一种有前景的医疗应用工具，特别是在数据受限的环境中。", "summary": "本研究旨在通过3D光学成像和机器学习模型预测人体成分变量，以替代昂贵且耗时的DXA扫描。针对医疗数据获取的挑战，研究引入并首次将半监督p-Laplacian回归模型应用于回归任务，并在仅使用10%训练数据时，对ALM、BMD和BFP取得了可接受的预测误差。同时，在数据充足的情况下，监督SVR和LS-SVR模型表现出更高的准确性。研究结果表明，p-Laplacian模型特别适用于数据受限的医疗场景。", "keywords": "3D光学成像, 机器学习, 人体成分, p-Laplacian回归, 半监督学习", "comments": "本文的创新点在于提出了使用3D光学成像结合机器学习模型来预测人体成分变量，作为DXA扫描的非侵入性替代方案。尤其值得关注的是，其首次将半监督p-Laplacian回归模型应用于回归问题，并证明了其在数据受限环境下（如医疗领域）的潜力，这对于实际应用具有重要意义。该方法有望降低成本和时间，提高诊断效率。"}}
{"id": "2506.15241", "title": "Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs", "authors": ["Yang Fan", "Zhang Qi", "Xing Wenqian", "Liu Chang", "Liu Liu"], "summary": "This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15241v1", "AI": {"title_translation": "基于历史文本知识图谱的图检索增强生成研究", "tldr": "本文提出了Graph RAG框架，结合思维链和自指令生成，构建了历史人物关系数据集，并引入知识图谱与检索增强生成的协同机制，有效缓解了大语言模型在历史文本分析中的领域知识鸿沟和幻觉问题，提高了关系抽取性能和可解释性。", "motivation": "解决通用大语言模型在计算人文和AIGC技术背景下进行历史文本分析时存在的领域知识鸿沟问题。", "method": "提出了Graph RAG框架，该框架结合了思维链提示、自指令生成和过程监督，以最少的人工标注创建了“前四史”人物关系数据集。在图增强生成阶段，引入了知识图谱与检索增强生成的协同机制，以提高通用模型与历史知识的对齐度。", "result": "领域特定模型Xunzi-Qwen1.5-14B在关系抽取中达到了F1=0.68的最佳性能。集成GraphRAG的DeepSeek模型在开放域C-CLUE关系抽取数据集上F1值提高了11%（从0.08到0.19），超过了Xunzi-Qwen1.5-14B的F1值（0.12），有效缓解了幻觉现象并提高了可解释性。", "conclusion": "该框架为古典文本知识抽取提供了一种低资源解决方案，推动了历史知识服务和人文研究的进步。", "translation": "本文旨在解决通用大语言模型在计算人文和AIGC技术背景下进行历史文本分析时存在的领域知识鸿沟问题。我们提出了Graph RAG框架，该框架结合了思维链提示、自指令生成和过程监督，以最少的人工标注创建了“前四史”人物关系数据集。该数据集支持自动化历史知识抽取，降低了人工成本。在图增强生成阶段，我们引入了知识图谱与检索增强生成的协同机制，以提高通用模型与历史知识的对齐度。实验表明，领域特定模型Xunzi-Qwen1.5-14B在简体中文输入和思维链提示下，在关系抽取中达到了最佳性能（F1 = 0.68）。集成GraphRAG的DeepSeek模型在开放域C-CLUE关系抽取数据集上F1值提高了11%（0.08-0.19），超过了Xunzi-Qwen1.5-14B的F1值（0.12），有效缓解了幻觉现象，并提高了可解释性。该框架为古典文本知识抽取提供了一种低资源解决方案，推动了历史知识服务和人文研究的进步。", "summary": "本研究提出Graph RAG框架，旨在弥补大语言模型在历史文本分析中的领域知识不足。通过结合思维链、自指令生成和过程监督，构建了低标注成本的“前四史”人物关系数据集，并创新性地引入了知识图谱与检索增强生成的协同机制。实验证明，该框架显著提升了模型在历史关系抽取上的性能，有效缓解了幻觉问题，并增强了模型的可解释性，为古典文本知识抽取提供了高效的低资源解决方案。", "keywords": "图检索增强生成, 历史文本, 知识图谱, 大语言模型, 关系抽取", "comments": "该论文的创新点在于提出了Graph RAG框架，通过结合知识图谱与检索增强生成，并利用思维链和自指令生成来构建高质量的历史知识数据集，有效解决了大语言模型在特定领域，尤其是历史文本分析中的知识空白和幻觉问题。其提出的低资源解决方案对于古典文本知识的自动化抽取和应用具有重要意义，对计算人文和历史知识服务领域的发展具有积极推动作用。"}}
{"id": "2506.15360", "title": "Stochastic Diagonal Estimation Based on Matrix Quadratic Form Oracles", "authors": ["Haishan Ye", "Xiangyu Chang"], "summary": "We study the problem of estimating the diagonal of an implicitly given matrix\n$\\Ab$. For such a matrix we have access to an oracle that allows us to evaluate\nthe matrix quadratic form $ \\ub^\\top \\Ab \\ub$. Based on this query oracle, we\npropose a stochastic diagonal estimation method with random variable $\\ub$\ndrawn from the standard Gaussian distribution. We provide the element-wise and\nnorm-wise sample complexities of the proposed method. Our numerical experiments\non different types and dimensions matrices demonstrate the effectiveness of our\nmethod and validate the tightness of theoretical results.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15360v1", "AI": {"title_translation": "基于矩阵二次型预言机的随机对角线估计", "tldr": "该论文提出了一种基于矩阵二次型预言机的随机对角线估计方法，并分析了其样本复杂度。", "motivation": "研究目标是估计一个隐式给定矩阵的对角线，该矩阵只能通过评估矩阵二次型预言机来访问。", "method": "提出了一种随机对角线估计方法，其中随机变量从标准高斯分布中抽取，并利用矩阵二次型预言机进行查询。", "result": "提供了所提出方法的元素级和范数级样本复杂度。数值实验证明了该方法的有效性，并验证了理论结果的紧密性。", "conclusion": "所提出的随机对角线估计方法是有效的，并且其理论复杂度得到了实验验证。", "translation": "我们研究了估计隐式给定矩阵$\\Ab$对角线的问题。对于这样的矩阵，我们有一个预言机，允许我们评估矩阵二次型$ \\ub^\\top \\Ab \\ub$。基于这个查询预言机，我们提出了一种随机对角线估计方法，其中随机变量$\\ub$从标准高斯分布中抽取。我们提供了所提出方法的元素级和范数级样本复杂度。我们对不同类型和维度矩阵的数值实验证明了我们方法的有效性，并验证了理论结果的紧密性。", "summary": "本文研究了在只能通过矩阵二次型预言机访问的情况下，估计隐式给定矩阵对角线的问题。作者提出了一种基于标准高斯分布随机变量的随机对角线估计方法。论文详细阐述了该方法的元素级和范数级样本复杂度，并通过数值实验验证了其有效性和理论结果的紧密性。", "keywords": "随机对角线估计, 矩阵二次型, 样本复杂度, 高斯分布, 隐式矩阵", "comments": "该论文的创新点在于利用随机方法（高斯随机变量）和二次型预言机来解决隐式矩阵的对角线估计问题，这在矩阵无法直接访问时尤其有用。论文提供了理论上的样本复杂度保证和实验验证，显示了其在矩阵估计领域的扎实贡献。"}}
{"id": "2506.15525", "title": "\"How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "summary": "As generative AI (GenAI) emerges as a transformative force, clear\nunderstanding of high school students' perspectives is essential for GenAI's\nmeaningful integration in high school environments. In this work, we draw\ninsights from a participatory design workshop where we engaged 17 high school\nstudents -- a group rarely involved in prior research in this area -- through\nthe design of novel GenAI tools and school policies addressing their key\nconcerns. Students identified challenges and developed solutions outlining\ntheir ideal features in GenAI tools, appropriate school use, and regulations.\nThese centered around the problem spaces of combating bias & misinformation,\ntackling crime & plagiarism, preventing over-reliance on AI, and handling false\naccusations of academic dishonesty. Building on our participants'\nunderrepresented perspectives, we propose new guidelines targeted at\neducational technology designers for development of GenAI technologies in high\nschools. We also argue for further incorporation of student voices in\ndevelopment of AI policies in their schools.", "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in\n  proceedings of 24th annual ACM Interaction Design and Children Conference\n  (IDC 2025)", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15525v1", "AI": {"title_translation": "我们如何才能同时学习和使用人工智能？:: 与高中生共同参与生成式人工智能的设计", "tldr": "本研究通过与17名高中生共同参与设计工作坊，深入了解他们对生成式AI的看法，并提出针对教育技术设计者和学校AI政策的新指导方针。", "motivation": "为了将生成式人工智能（GenAI）有意义地整合到高中环境中，了解高中生对GenAI的看法至关重要。", "method": "通过一个参与式设计工作坊，与17名高中生（该领域先前研究中很少涉及的群体）进行互动，共同设计新颖的GenAI工具和解决他们主要担忧的学校政策。", "result": "学生们识别了挑战并提出了解决方案，概述了他们理想的GenAI工具特性、适当的学校使用和规定。这些解决方案集中在打击偏见和错误信息、解决犯罪和抄袭、防止过度依赖AI以及处理学术不端行为的虚假指控等问题空间。", "conclusion": "作者基于参与者的未被充分代表的观点，为教育技术设计师提出了开发高中GenAI技术的新指导方针，并主张在学校AI政策的制定中进一步纳入学生的声音。", "translation": "随着生成式人工智能（GenAI）作为一股变革力量的出现，清晰地理解高中生的视角对于GenAI在高中环境中有意义的整合至关重要。在这项工作中，我们从一个参与式设计工作坊中汲取见解，该工作坊吸引了17名高中生——一个在该领域先前研究中很少涉及的群体——通过设计新颖的GenAI工具和解决他们主要担忧的学校政策。学生们识别了挑战并提出了解决方案，概述了他们理想的GenAI工具特性、适当的学校使用和规定。这些解决方案集中在打击偏见和错误信息、解决犯罪和抄袭、防止过度依赖AI以及处理学术不端行为的虚假指控等问题空间。基于我们参与者未被充分代表的视角，我们为教育技术设计师提出了开发高中GenAI技术的新指导方针。我们还主张在学校AI政策的制定中进一步纳入学生的声音。", "summary": "本研究通过与17名高中生共同参与设计工作坊，探讨了生成式AI在高中环境中的整合问题。学生们识别了使用GenAI的挑战，如偏见、抄袭和过度依赖，并提出了理想的工具特性和学校政策。基于这些见解，研究为教育技术设计师提出了新的指导方针，并强调了在AI政策制定中纳入学生声音的重要性。", "keywords": "生成式AI, 参与式设计, 高中生, 教育技术, AI政策", "comments": "这项研究的创新之处在于其独特的参与式设计方法，它将高中生作为GenAI工具和政策设计的核心参与者。这对于确保AI技术在教育环境中得到负责任和有意义的整合至关重要，弥补了以往研究中对青少年声音的忽视。该研究强调了从最终用户视角出发进行设计的重要性，特别是在快速发展的AI领域。"}}
{"id": "2506.15343", "title": "Offensive Robot Cybersecurity", "authors": ["Víctor Mayoral-Vilches"], "summary": "Offensive Robot Cybersecurity introduces a groundbreaking approach by\nadvocating for offensive security methods empowered by means of automation. It\nemphasizes the necessity of understanding attackers' tactics and identifying\nvulnerabilities in advance to develop effective defenses, thereby improving\nrobots' security posture. This thesis leverages a decade of robotics\nexperience, employing Machine Learning and Game Theory to streamline the\nvulnerability identification and exploitation process. Intrinsically, the\nthesis uncovers a profound connection between robotic architecture and\ncybersecurity, highlighting that the design and creation aspect of robotics\ndeeply intertwines with its protection against attacks. This duality -- whereby\nthe architecture that shapes robot behavior and capabilities also necessitates\na defense mechanism through offensive and defensive cybersecurity strategies --\ncreates a unique equilibrium. Approaching cybersecurity with a dual perspective\nof defense and attack, rooted in an understanding of systems architecture, has\nbeen pivotal. Through comprehensive analysis, including ethical considerations,\nthe development of security tools, and executing cyber attacks on robot\nsoftware, hardware, and industry deployments, this thesis proposes a novel\narchitecture for cybersecurity cognitive engines. These engines, powered by\nadvanced game theory and machine learning, pave the way for autonomous\noffensive cybersecurity strategies for robots, marking a significant shift\ntowards self-defending robotic systems. This research not only underscores the\nimportance of offensive measures in enhancing robot cybersecurity but also sets\nthe stage for future advancements where robots are not just resilient to cyber\nthreats but are equipped to autonomously safeguard themselves.", "comment": "Doctoral thesis", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15343v1", "AI": {"title_translation": "进攻性机器人网络安全", "tldr": "该论文提出了一种开创性的方法，即通过自动化、机器学习和博弈论赋能的进攻性安全方法，以提前识别漏洞并开发有效的防御措施，从而显著提高机器人的网络安全态势并实现自主防御。", "motivation": "该论文强调了理解攻击者策略并提前识别漏洞以开发有效防御措施的必要性，从而提高机器人的安全态势。它还揭示了机器人架构与网络安全之间深刻的联系，指出机器人设计和创建方面与其抵抗攻击的保护措施是深度交织的。", "method": "本论文利用十年的机器人经验，运用机器学习和博弈论来简化漏洞识别和利用过程。它通过全面的分析，包括伦理考量、安全工具的开发以及对机器人软件、硬件和工业部署进行网络攻击。最终，它提出了一种用于网络安全认知引擎的新颖架构，这些引擎由先进的博弈论和机器学习驱动。", "result": "该研究揭示了机器人架构与网络安全之间深刻的联系，强调机器人设计和创建方面与其抵抗攻击的保护措施是深度交织的。它提出了一种新颖的网络安全认知引擎架构，为机器人的自主进攻性网络安全策略铺平了道路，标志着向自防御机器人系统迈出了重要一步。", "conclusion": "该研究不仅强调了进攻性措施在增强机器人网络安全方面的重要性，也为未来的发展奠定了基础，使机器人不仅能抵御网络威胁，还能自主保护自己。", "translation": "进攻性机器人网络安全引入了一种开创性的方法，倡导通过自动化手段赋能进攻性安全方法。它强调了理解攻击者策略并提前识别漏洞的必要性，以开发有效的防御措施，从而提高机器人的安全态势。本论文利用十年的机器人经验，运用机器学习和博弈论来简化漏洞识别和利用过程。从本质上讲，本论文揭示了机器人架构与网络安全之间深刻的联系，强调机器人设计和创建方面与其抵抗攻击的保护措施是深度交织的。这种二元性——即塑造机器人行为和能力的架构也需要通过进攻性和防御性网络安全策略来建立防御机制——创造了一种独特的平衡。以防御和攻击的双重视角来处理网络安全，并植根于对系统架构的理解，这一点至关重要。通过全面的分析，包括伦理考量、安全工具的开发以及对机器人软件、硬件和工业部署进行网络攻击，本论文提出了一种用于网络安全认知引擎的新颖架构。这些引擎由先进的博弈论和机器学习驱动，为机器人的自主进攻性网络安全策略铺平了道路，标志着向自防御机器人系统迈出了重要一步。这项研究不仅强调了进攻性措施在增强机器人网络安全方面的重要性，也为未来的发展奠定了基础，使机器人不仅能抵御网络威胁，还能自主保护自己。", "summary": "本论文引入了一种进攻性机器人网络安全方法，倡导通过自动化、机器学习和博弈论来识别和利用漏洞。它强调了机器人架构与网络安全之间深刻的联系，并提出了一种新颖的网络安全认知引擎架构，旨在实现机器人的自主进攻性安全策略，从而迈向自防御机器人系统。", "keywords": "进攻性网络安全, 机器人安全, 机器学习, 博弈论, 漏洞识别", "comments": "该论文的创新之处在于倡导将进攻性安全和自主策略应用于机器人，这与传统的防御方法相比是一个重大转变。它关注机器人架构与网络安全之间的相互作用，并结合机器学习和博弈论，是其独特的贡献。"}}
{"id": "2506.14980", "title": "Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors", "authors": ["Ziteng Li", "Malte Kuhlmann", "Ilana Nisky", "Nicolás Navarro-Guerrero"], "summary": "Compliance is a critical parameter for describing objects in engineering,\nagriculture, and biomedical applications. Traditional compliance detection\nmethods are limited by their lack of portability and scalability, rely on\nspecialized, often expensive equipment, and are unsuitable for robotic\napplications. Moreover, existing neural network-based approaches using\nvision-based tactile sensors still suffer from insufficient prediction\naccuracy. In this paper, we propose two models based on Long-term Recurrent\nConvolutional Networks (LRCNs) and Transformer architectures that leverage RGB\ntactile images and other information captured by the vision-based sensor\nGelSight to predict compliance metrics accurately. We validate the performance\nof these models using multiple metrics and demonstrate their effectiveness in\naccurately estimating compliance. The proposed models exhibit significant\nperformance improvement over the baseline. Additionally, we investigated the\ncorrelation between sensor compliance and object compliance estimation, which\nrevealed that objects that are harder than the sensor are more challenging to\nestimate.", "comment": "Accepted in the IEEE International Conference on Development and\n  Learning (ICDL). The paper contains 8 pages and 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14980v1", "AI": {"title_translation": "柔顺度检测的进展：使用基于视觉的触觉传感器的新型模型", "tldr": "本文提出了两种基于LRCNs和Transformer的新模型，利用视觉触觉传感器GelSight的数据，显著提高了柔顺度预测的准确性，解决了传统方法和现有神经网络方法的局限性。", "motivation": "传统的柔顺度检测方法存在便携性差、可扩展性差、设备昂贵且不适用于机器人应用的问题。此外，现有使用基于视觉的触觉传感器的神经网络方法预测精度不足。", "method": "本文提出了两种基于长短期循环卷积网络（LRCNs）和Transformer架构的模型。这些模型利用RGB触觉图像和视觉传感器GelSight捕获的其他信息来准确预测柔顺度指标。", "result": "所提出的模型在多项指标上验证了其性能，并展示了在准确估计柔顺度方面的有效性。这些模型比基线模型表现出显著的性能改进。此外，研究发现比传感器更硬的物体更难以估计。", "conclusion": "本文提出的基于LRCNs和Transformer的模型显著提高了柔顺度检测的准确性，克服了传统方法和现有神经网络方法的局限性，并揭示了传感器柔顺度与物体柔顺度估计之间的相关性。", "translation": "柔顺度是描述工程、农业和生物医学应用中物体的一个关键参数。传统的柔顺度检测方法受到其缺乏便携性和可扩展性的限制，依赖于专业的、通常昂贵的设备，并且不适用于机器人应用。此外，现有使用基于视觉的触觉传感器的神经网络方法仍然存在预测精度不足的问题。在本文中，我们提出了两种基于长短期循环卷积网络（LRCNs）和Transformer架构的模型，它们利用RGB触觉图像和视觉传感器GelSight捕获的其他信息来准确预测柔顺度指标。我们使用多个指标验证了这些模型的性能，并证明了它们在准确估计柔顺度方面的有效性。所提出的模型比基线模型表现出显著的性能改进。此外，我们研究了传感器柔顺度与物体柔顺度估计之间的相关性，结果表明比传感器更硬的物体更难以估计。", "summary": "本文针对传统柔顺度检测方法和现有神经网络方法在便携性、可扩展性及预测精度上的不足，提出了两种基于长短期循环卷积网络（LRCNs）和Transformer架构的新型模型。这些模型利用GelSight视觉触觉传感器捕获的RGB图像及其他信息，显著提高了柔顺度预测的准确性，并在多项指标上优于基线模型。研究还发现，估计比传感器更硬的物体柔顺度更具挑战性。", "keywords": "柔顺度检测, 视觉触觉传感器, LRCNs, Transformer, GelSight", "comments": "本文的创新之处在于将LRCNs和Transformer架构应用于视觉触觉传感器数据，以解决柔顺度检测中的精度问题。其重要性在于为机器人应用提供了更准确、更可扩展的柔顺度估计方法，有望推动触觉感知和机器人操作领域的发展。研究中关于传感器与物体硬度相关性的发现也提供了有价值的见解。"}}
{"id": "2506.14821", "title": "Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints", "authors": ["Sunil Kumar", "Bowen Zhao", "Leo Dirac", "Paulina Varshavskaya"], "summary": "Despite tremendous recent advances in large model reasoning ability,\nvision-language models (VLMs) still struggle with detailed visual reasoning,\nespecially when compute resources are limited. To address this challenge, we\ndraw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale\nmodels with Group Relative Policy Optimization (GRPO) to use external tools\nsuch as zoom. The greatest benefit is obtained with a combination of GRPO\nlearning, a simple reward structure, a simplified tool-calling interface,\nallocating additional tokens to the result of the tool call, and a training\ndata mix that over-represents visually difficult examples. Compared to\nsimilarly-sized baseline models, our method achieves better performance on some\nvisual question-answering (VQA) tasks, thanks to the detailed visual\ninformation gathered from the external tool.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14821v1", "AI": {"title_translation": "在资源受限下增强VLM使用工具进行详细视觉推理", "tldr": "VLMs在资源受限下进行详细视觉推理困难。本文提出用GRPO训练小型模型使用外部工具（如缩放），通过结合GRPO、简化奖励、工具接口、额外token分配和数据混合，在VQA任务上取得更好表现。", "motivation": "尽管大型模型推理能力最近取得了巨大进步，但视觉-语言模型（VLMs）在详细视觉推理方面仍然存在困难，尤其是在计算资源有限的情况下。", "method": "本文借鉴Deepseek-r1等方法，使用组相对策略优化（GRPO）训练小规模VLMs使用外部工具（如缩放）。该方法通过结合GRPO学习、简单的奖励结构、简化的工具调用接口、为工具调用结果分配额外token，以及过采样视觉困难示例的训练数据混合来获得最佳效果。", "result": "与同等大小的基线模型相比，我们的方法在一些视觉问答（VQA）任务上取得了更好的性能，这得益于从外部工具收集到的详细视觉信息。", "conclusion": "通过结合GRPO学习、优化奖励结构、简化工具接口、合理分配token以及使用特殊训练数据混合，可以有效增强小型VLM在资源受限下进行详细视觉推理的能力，并在VQA任务上取得更好表现。", "translation": "尽管大型模型推理能力最近取得了巨大进步，但视觉-语言模型（VLMs）在详细视觉推理方面仍然存在困难，尤其是在计算资源有限的情况下。为了解决这一挑战，我们借鉴了Deepseek-r1等VLM方法，并使用组相对策略优化（GRPO）训练小规模模型使用外部工具，例如缩放。通过结合GRPO学习、简单的奖励结构、简化的工具调用接口、为工具调用结果分配额外token，以及过采样视觉困难示例的训练数据混合，获得了最大的益处。与同等大小的基线模型相比，我们的方法在一些视觉问答（VQA）任务上取得了更好的性能，这得益于从外部工具收集到的详细视觉信息。", "summary": "本文旨在解决视觉-语言模型（VLMs）在资源受限下进行详细视觉推理的难题。研究人员受Deepseek-r1启发，利用组相对策略优化（GRPO）训练小型VLM以调用外部工具（如缩放）。通过整合GRPO、简化奖励机制、优化工具接口、分配额外token及采用特殊数据混合策略，该方法在视觉问答（VQA）任务上显著优于同等规模的基线模型，证明了工具辅助对提升视觉推理能力的有效性。", "keywords": "视觉-语言模型, 工具使用, 视觉推理, 资源受限, GRPO, VQA", "comments": "这项工作创新性地将强化学习（GRPO）应用于小型VLMs，使其能在资源受限下有效利用外部工具进行详细视觉推理。其重要性在于为部署于边缘设备或计算资源有限环境下的VLM提供了一条可行路径，通过工具调用弥补模型本身的局限性。方法结合了多种优化策略，展现了系统性优化的潜力。"}}
{"id": "2506.15246", "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge", "authors": ["Juli Bakagianni", "John Pavlopoulos", "Aristidis Likas"], "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15246v1", "AI": {"title_translation": "TopClustRAG在SIGIR 2025 LiveRAG挑战赛中的表现", "tldr": "TopClustRAG是一个RAG系统，结合了混合检索和K-Means聚类，用于大规模网络语料库上的问答，并在LiveRAG挑战赛中取得了不错的成绩。", "motivation": "该系统是为了LiveRAG挑战赛而开发的，旨在评估在大规模网络语料库上进行端到端问答的能力。", "method": "TopClustRAG系统采用了混合检索策略（结合稀疏和密集索引），随后使用K-Means聚类对语义相似的段落进行分组。从每个集群中选择代表性段落构建集群特定的提示，供大型语言模型（LLM）生成中间答案。这些答案经过过滤、重新排序，最终合成为一个全面响应。", "result": "在FineWeb Sample-10BT数据集上进行评估，TopClustRAG在官方排行榜上忠实度排名第2，正确性排名第7。", "conclusion": "该研究证明了基于聚类的上下文过滤和提示聚合在大规模RAG系统中是有效的。", "translation": "我们介绍了TopClustRAG，一个为LiveRAG挑战赛开发的检索增强生成（RAG）系统，该挑战赛评估在大规模网络语料库上进行端到端问答的能力。我们的系统采用混合检索策略，结合了稀疏和密集索引，随后通过K-Means聚类对语义相似的段落进行分组。每个聚类中的代表性段落用于为大型语言模型（LLM）构建特定于聚类的提示，生成中间答案，这些答案经过过滤、重新排序，最终合成一个单一的、全面的响应。这种多阶段管道增强了答案的多样性、相关性和对检索证据的忠实度。在FineWeb Sample-10BT数据集上进行评估，TopClustRAG在官方排行榜上忠实度排名第2，正确性排名第7，证明了在大规模RAG系统中基于聚类的上下文过滤和提示聚合的有效性。", "summary": "TopClustRAG是一个为LiveRAG挑战赛设计的检索增强生成（RAG）系统。它通过结合稀疏和密集检索、K-Means聚类来组织检索到的内容，并利用集群代表性段落为LLM生成答案。该多阶段方法旨在提高答案的多样性、相关性和忠实度。在FineWeb Sample-10BT数据集上的表现显示，TopClustRAG在忠实度方面排名第2，在正确性方面排名第7，验证了其在处理大规模RAG任务中的聚类和提示聚合策略的有效性。", "keywords": "检索增强生成, K-Means聚类, 大规模问答, 混合检索, LiveRAG挑战赛", "comments": "TopClustRAG的创新之处在于其将K-Means聚类引入RAG流程中，用于对检索到的上下文进行过滤和聚合，从而有效地管理大规模数据并提高生成答案的质量。这种方法对于处理海量网络语料库的问答系统具有重要意义，尤其是在确保答案忠实度和相关性方面。"}}
{"id": "2506.15481", "title": "A deep shotgun method for solving high-dimensional parabolic partial differential equations", "authors": ["Wenjun Xu", "Wenzhong Zhang"], "summary": "Recent advances in deep learning makes solving parabolic partial differential\nequations (PDEs) in high dimensional spaces possible via forward-backward\nstochastic differential equation (FBSDE) formulations. The implementation of\nmost existing methods requires simulating multiple trajectories of stochastic\nprocesses with a small step size of time discretization to ensure accuracy,\nhence having limited performance, especially when solving on a large time\ninterval. To address such issue, we propose a deep \"shotgun method\" that does\nnot exploit full trajectories, but only utilizes the data distribution of them.\nNumerical results including examples with dimensionality up to 10000\ndemonstrate the competitiveness of the proposed shotgun method in both\nperformance and accuracy.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15481v1", "AI": {"title_translation": "一种求解高维抛物型偏微分方程的深度“散弹枪”方法", "tldr": "本文提出了一种深度“散弹枪”方法，通过利用随机过程的数据分布而非完整轨迹来高效准确地求解高维抛物型偏微分方程。", "motivation": "现有方法在求解高维抛物型偏微分方程时，需要模拟大量小时间步长的随机过程轨迹以确保准确性，导致性能受限，尤其是在大时间间隔上求解时。", "method": "本文提出了一种深度“散弹枪”方法，该方法不利用完整的随机过程轨迹，而只利用它们的数据分布来求解高维抛物型偏微分方程。", "result": "数值结果表明，所提出的“散弹枪”方法在性能和准确性方面都具有竞争力，包括维度高达10000的例子。", "conclusion": "深度“散弹枪”方法通过利用数据分布而非完整轨迹，有效解决了现有方法在求解高维抛物型偏微分方程时的性能限制，并在高维度问题上展现出竞争力和准确性。", "translation": "深度学习的最新进展使得通过前向-后向随机微分方程（FBSDE）公式求解高维空间中的抛物型偏微分方程（PDEs）成为可能。大多数现有方法的实现需要模拟多个随机过程轨迹，并采用小的时间离散化步长以确保准确性，因此性能受限，特别是在大时间间隔上求解时。为了解决这个问题，我们提出了一种深度“散弹枪方法”，该方法不利用完整的轨迹，而只利用它们的数据分布。包括维度高达10000的示例的数值结果证明了所提出的“散弹枪方法”在性能和准确性方面的竞争力。", "summary": "本文提出了一种名为深度“散弹枪”的新方法，用于高效准确地求解高维抛物型偏微分方程。与依赖于模拟完整随机过程轨迹的现有方法不同，该方法仅利用轨迹的数据分布。数值实验（包括高达10000维的案例）表明，所提出的方法在性能和准确性方面均具有竞争力。", "keywords": "高维偏微分方程, 深度学习, 随机微分方程, 散弹枪方法, 轨迹数据分布", "comments": "这项工作提出了一种新颖的方法来解决高维PDEs的计算效率问题，通过避免模拟完整的随机过程轨迹，转而利用其数据分布。这在处理大规模和长时间间隔问题时具有显著的创新性，有望克服现有方法的性能瓶颈。"}}
{"id": "2506.15656", "title": "PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "summary": "Phishing websites continue to pose a significant cybersecurity threat, often\nleveraging deceptive structures, brand impersonation, and social engineering\ntactics to evade detection. While recent advances in large language models\n(LLMs) have enabled improved phishing detection through contextual\nunderstanding, most existing approaches rely on single-agent classification\nfacing the risks of hallucination and lack interpretability or robustness. To\naddress these limitations, we propose PhishDebate, a modular multi-agent\nLLM-based debate framework for phishing website detection. PhishDebate employs\nfour specialized agents to independently analyze different textual aspects of a\nwebpage--URL structure, HTML composition, semantic content, and brand\nimpersonation--under the coordination of a Moderator and a final Judge. Through\nstructured debate and divergent thinking, the framework delivers more accurate\nand interpretable decisions. Extensive evaluations on commercial LLMs\ndemonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate\n(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain\nof Thought (CoT) baselines. Additionally, its modular design allows agent-level\nconfigurability, enabling adaptation to varying resource and application\nrequirements.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15656v1", "AI": {"title_translation": "PhishDebate：一个基于LLM的多智能体网络钓鱼网站检测框架", "tldr": "PhishDebate是一个基于LLM的多智能体辩论框架，通过协调四个专业智能体分析网页文本特征来提高网络钓鱼网站检测的准确性和可解释性，优于现有方法。", "motivation": "现有的网络钓鱼网站检测方法，尤其是基于LLM的单智能体分类方法，面临幻觉、缺乏可解释性和鲁棒性的风险，无法有效应对利用欺骗性结构、品牌冒充和社会工程策略的网络钓鱼威胁。", "method": "提出PhishDebate，一个模块化的多智能体LLM辩论框架。它采用四个专门的智能体（分别分析URL结构、HTML组成、语义内容和品牌冒充）在仲裁者和最终评判的协调下独立分析网页的不同文本方面。通过结构化辩论和发散思维，该框架做出更准确和可解释的决策。", "result": "在真实世界网络钓鱼数据集上，PhishDebate实现了98.2%的召回率和98.2%的真阳性率（TPR），并且优于单智能体和思维链（CoT）基线方法。", "conclusion": "PhishDebate通过其模块化多智能体辩论框架，有效解决了现有LLM方法的局限性，显著提高了网络钓鱼网站检测的准确性和可解释性，并具有良好的可配置性以适应不同的资源和应用需求。", "translation": "网络钓鱼网站持续构成重大的网络安全威胁，它们经常利用欺骗性结构、品牌冒充和社会工程策略来逃避检测。尽管大型语言模型（LLM）的最新进展通过上下文理解改进了网络钓鱼检测，但大多数现有方法依赖于单智能体分类，面临幻觉、缺乏可解释性或鲁棒性的风险。为了解决这些局限性，我们提出了PhishDebate，一个模块化的基于LLM的多智能体辩论框架，用于网络钓鱼网站检测。PhishDebate采用四个专门的智能体，在仲裁者和最终评判的协调下，独立分析网页的不同文本方面——URL结构、HTML组成、语义内容和品牌冒充。通过结构化辩论和发散思维，该框架提供了更准确和可解释的决策。对商业LLM的广泛评估表明，PhishDebate在真实世界的网络钓鱼数据集上实现了98.2%的召回率和98.2%的真阳性率（TPR），并且优于单智能体和思维链（CoT）基线。此外，其模块化设计允许智能体级别的可配置性，从而能够适应不同的资源和应用需求。", "summary": "本文提出了PhishDebate，一个基于LLM的多智能体辩论框架，用于解决现有网络钓鱼网站检测方法中单智能体LLM面临的幻觉、可解释性差和鲁棒性不足的问题。PhishDebate包含四个专门的智能体，分别分析URL、HTML、语义内容和品牌冒充，并在仲裁者和评判的协调下进行结构化辩论。实验表明，PhishDebate在真实数据集上表现出色，召回率和真阳性率均达到98.2%，并优于单智能体和CoT基线，同时具有良好的模块化和可配置性。", "keywords": "网络钓鱼检测, 大型语言模型, 多智能体系统, 辩论框架, 网络安全", "comments": "这篇论文的创新点在于引入了多智能体辩论框架来增强LLM在网络钓鱼检测中的应用，有效解决了单智能体LLM的局限性。通过模拟人类辩论过程，提高了决策的准确性和可解释性，并提供了模块化的设计，使其具有很高的实用性和适应性。其在真实世界数据集上的高性能表现也凸显了其重要性。"}}
{"id": "2506.13776", "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations", "authors": ["Kevin L. Wei", "Patricia Paskov", "Sunishchal Dev", "Michael J. Byun", "Anka Reuel", "Xavier Roberts-Gaal", "Rachel Calcott", "Evie Coxon", "Chinmay Deshpande"], "summary": "In this position paper, we argue that human baselines in foundation model\nevaluations must be more rigorous and more transparent to enable meaningful\ncomparisons of human vs. AI performance, and we provide recommendations and a\nreporting checklist towards this end. Human performance baselines are vital for\nthe machine learning community, downstream users, and policymakers to interpret\nAI evaluations. Models are often claimed to achieve \"super-human\" performance,\nbut existing baselining methods are neither sufficiently rigorous nor\nsufficiently well-documented to robustly measure and assess performance\ndifferences. Based on a meta-review of the measurement theory and AI evaluation\nliteratures, we derive a framework with recommendations for designing,\nexecuting, and reporting human baselines. We synthesize our recommendations\ninto a checklist that we use to systematically review 115 human baselines\n(studies) in foundation model evaluations and thus identify shortcomings in\nexisting baselining methods; our checklist can also assist researchers in\nconducting human baselines and reporting results. We hope our work can advance\nmore rigorous AI evaluation practices that can better serve both the research\ncommunity and policymakers. Data is available at:\nhttps://github.com/kevinlwei/human-baselines", "comment": "A version of this paper has been accepted to ICML 2025 as a position\n  paper (spotlight), with the title: \"Position: Human Baselines in Model\n  Evaluations Need Rigor and Transparency (With Recommendations & Reporting\n  Checklist).\"", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.13776v1", "AI": {"title_translation": "模型评估中严谨透明的人类基线推荐和报告清单", "tldr": "本立场论文提出，在基础模型评估中，人类基线需要更加严谨和透明，以实现有意义的人机性能比较，并为此提供了建议和报告清单。", "motivation": "现有的人类基线方法不够严谨和透明，导致无法对人类与AI的性能进行有意义的比较。尽管经常声称模型达到了“超人类”表现，但缺乏健壮的测量和评估方法来衡量性能差异。严谨透明的人类表现基线对于机器学习社区、下游用户和政策制定者解释AI评估结果至关重要。", "method": "作者基于对测量理论和AI评估文献的元审查，提出了一个用于设计、执行和报告人类基线的框架和建议。他们将这些建议综合成一个清单，并使用该清单系统地审查了115项基础模型评估中的人类基线研究，以识别现有基线方法的不足之处。", "result": "通过对115项人类基线研究的系统审查，发现了现有基线方法的不足之处。作者提出的清单可以帮助研究人员进行人类基线研究并报告结果。", "conclusion": "作者希望他们的工作能够推动更严谨的AI评估实践，从而更好地服务于研究社区和政策制定者。", "translation": "在这篇立场论文中，我们认为基础模型评估中的人类基线必须更加严谨和透明，以实现人类与AI性能的有意义比较，并为此提供了建议和报告清单。人类性能基线对于机器学习社区、下游用户和政策制定者解释AI评估至关重要。模型经常被声称达到“超人类”性能，但现有的基线方法既不够严谨，也缺乏足够的文档记录，无法稳健地测量和评估性能差异。基于对测量理论和AI评估文献的元审查，我们提出了一个框架，其中包含设计、执行和报告人类基线的建议。我们将建议综合成一个清单，并用它系统地审查了115项基础模型评估中的人类基线（研究），从而识别出现有基线方法的不足；我们的清单也可以帮助研究人员进行人类基线并报告结果。我们希望我们的工作能够推动更严谨的AI评估实践，从而更好地服务于研究社区和政策制定者。数据可在以下网址获取：https://github.com/kevinlwei/human-baselines", "summary": "本立场论文强调了在基础模型评估中建立更严谨和透明的人类基线的重要性，以准确比较人类与AI的性能。论文指出，当前的人类基线方法不足以稳健评估“超人类”性能的主张。作者通过对测量理论和AI评估文献的元审查，提出了一个框架、建议和一个报告清单，旨在指导人类基线的规范设计、执行和报告。他们使用该清单审查了115项现有研究，揭示了当前方法的缺陷，并期望其工作能促进更可靠的AI评估实践，服务于学术界和政策制定者。", "keywords": "人类基线, 模型评估, 严谨性, 透明度, 报告清单", "comments": "这篇论文的创新点在于提出了一个系统性的框架和可操作的清单，以提高AI模型评估中人类基线的严谨性和透明度。在AI性能被广泛讨论，甚至出现“超人类”性能说法的背景下，该工作的重要性不言而喻，它为确保AI评估的科学性和可信度提供了关键工具。其局限性可能在于，作为一个立场论文，它更多是提出建议和框架，而非提供新的实验数据或模型，但其对评估方法论的贡献是显著的。"}}
{"id": "2506.15010", "title": "Hyper-Local Deformable Transformers for Text Spotting on Historical Maps", "authors": ["Yijun Lin", "Yao-Yi Chiang"], "summary": "Text on historical maps contains valuable information providing georeferenced\nhistorical, political, and cultural contexts. However, text extraction from\nhistorical maps is challenging due to the lack of (1) effective methods and (2)\ntraining data. Previous approaches use ad-hoc steps tailored to only specific\nmap styles. Recent machine learning-based text spotters (e.g., for scene\nimages) have the potential to solve these challenges because of their\nflexibility in supporting various types of text instances. However, these\nmethods remain challenges in extracting precise image features for predicting\nevery sub-component (boundary points and characters) in a text instance. This\nis critical because map text can be lengthy and highly rotated with complex\nbackgrounds, posing difficulties in detecting relevant image features from a\nrough text region. This paper proposes PALETTE, an end-to-end text spotter for\nscanned historical maps of a wide variety. PALETTE introduces a novel\nhyper-local sampling module to explicitly learn localized image features around\nthe target boundary points and characters of a text instance for detection and\nrecognition. PALETTE also enables hyper-local positional embeddings to learn\nspatial interactions between boundary points and characters within and across\ntext instances. In addition, this paper presents a novel approach to\nautomatically generate synthetic map images, SynthMap+, for training text\nspotters for historical maps. The experiment shows that PALETTE with SynthMap+\noutperforms SOTA text spotters on two new benchmark datasets of historical\nmaps, particularly for long and angled text. We have deployed PALETTE with\nSynthMap+ to process over 60,000 maps in the David Rumsey Historical Map\ncollection and generated over 100 million text labels to support map searching.\nThe project is released at\nhttps://github.com/kartta-foundation/mapkurator-palette-doc.", "comment": "Published in KDD2024", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15010v1", "AI": {"title_translation": "用于历史地图文本识别的超局部可变形Transformer", "tldr": "PALETTE是一个端到端的历史地图文本识别器，引入超局部采样模块和位置嵌入来处理复杂文本，并利用SynthMap+自动生成合成训练数据。它在两个新基准数据集上超越了SOTA方法，并已应用于处理超过6万张地图。", "motivation": "历史地图上的文本包含宝贵信息，但由于缺乏有效方法和训练数据，文本提取极具挑战性。现有方法针对特定地图风格，而最新的机器学习文本识别器虽有潜力，但在提取精确图像特征以预测文本实例的子组件（边界点和字符）方面仍存在挑战，尤其对于长、高度旋转且背景复杂的地图文本。", "method": "本文提出了PALETTE，一个端到端历史地图文本识别器。PALETTE引入了一个新颖的超局部采样模块，用于显式学习文本实例目标边界点和字符周围的局部图像特征，以进行检测和识别。PALETTE还启用了超局部位置嵌入，以学习文本实例内部和之间的边界点与字符的空间交互。此外，本文提出了一种新颖的方法，SynthMap+，用于自动生成合成地图图像，以训练历史地图文本识别器。", "result": "PALETTE结合SynthMap+在两个新的历史地图基准数据集上优于SOTA文本识别器，特别是在处理长文本和倾斜文本方面。该系统已部署到David Rumsey历史地图收藏中，处理了超过6万张地图，并生成了超过1亿个文本标签以支持地图搜索。", "conclusion": "PALETTE结合SynthMap+为历史地图上的复杂文本识别提供了一个高效且鲁棒的解决方案，并在实际应用中取得了显著成果，证明了其在处理大规模历史地图数据方面的有效性。", "translation": "历史地图上的文本包含有价值的信息，提供了地理参考的历史、政治和文化背景。然而，由于缺乏（1）有效方法和（2）训练数据，从历史地图中提取文本具有挑战性。以往的方法使用仅针对特定地图风格的临时步骤。最近基于机器学习的文本识别器（例如，用于场景图像的识别器）有潜力解决这些挑战，因为它们在支持各种文本实例类型方面具有灵活性。然而，这些方法在提取精确图像特征以预测文本实例中的每个子组件（边界点和字符）方面仍然存在挑战。这至关重要，因为地图文本可能很长、高度旋转且背景复杂，这给从粗略文本区域中检测相关图像特征带来了困难。本文提出了PALETTE，一个用于各种扫描历史地图的端到端文本识别器。PALETTE引入了一个新颖的超局部采样模块，用于显式学习文本实例目标边界点和字符周围的局部图像特征，以进行检测和识别。PALETTE还启用了超局部位置嵌入，以学习文本实例内部和之间的边界点与字符的空间交互。此外，本文提出了一种新颖的方法，SynthMap+，用于自动生成合成地图图像，以训练历史地图文本识别器。实验表明，PALETTE结合SynthMap+在两个新的历史地图基准数据集上优于SOTA文本识别器，特别是对于长文本和倾斜文本。我们已将PALETTE结合SynthMap+部署到David Rumsey历史地图收藏中，处理了超过6万张地图，并生成了超过1亿个文本标签以支持地图搜索。该项目已在https://github.com/kartta-foundation/mapkurator-palette-doc发布。", "summary": "本文提出了PALETTE，一个专为历史地图设计的端到端文本识别器，旨在解决复杂背景下长、旋转文本的提取难题。PALETTE的核心在于其新颖的超局部采样模块和超局部位置嵌入，用于精确捕获文本实例的边界点和字符特征及其空间关系。为克服数据稀缺问题，研究还引入了SynthMap+自动生成合成训练数据。实验证明，PALETTE结合SynthMap+在新的历史地图基准数据集上表现优异，尤其擅长处理长文本和倾斜文本。该系统已成功应用于David Rumsey历史地图收藏，处理了大量地图并生成了海量文本标签，极大地提升了地图搜索能力。", "keywords": "历史地图, 文本识别, 可变形Transformer, 超局部采样, 合成数据", "comments": "本文针对历史地图文本识别这一独特且具有挑战性的任务，提出了创新的解决方案。其核心创新点在于超局部采样模块和超局部位置嵌入，这些设计能够有效应对历史地图文本特有的长度、旋转和复杂背景问题。此外，SynthMap+的引入解决了历史地图领域训练数据稀缺的关键瓶颈，这对于深度学习方法的应用至关重要。该研究不仅在基准测试中取得了SOTA性能，更重要的是，其在大规模历史地图数据集上的成功部署和实际应用，充分验证了PALETTE的实用性和鲁棒性，具有显著的社会和文化价值。"}}
{"id": "2506.14824", "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "authors": ["Yao Zhang", "Hewei Gao", "Haokun Chen", "Weiguo Li", "Yunpu Ma", "Volker Tresp"], "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.", "comment": "12 pages, 3 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14824v1", "AI": {"title_translation": "联邦纳米：迈向预训练多模态大型语言模型的轻量级联邦微调", "tldr": "FedNano 是一种轻量级联邦学习框架，通过在客户端部署小型 NanoEdge 模块而非整个多模态大模型，解决了多模态大模型联邦学习中的计算和通信挑战，并实现了优于现有方法的性能。", "motivation": "多模态大型语言模型（MLLMs）在多模态推理和跨模态检索方面表现出色，但由于分布式数据、严格的隐私要求、高计算需求、有限的客户端容量、高通信成本以及异构客户端数据，在实际部署中面临挑战。现有联邦学习方法假设在客户端部署完整模型，但这对于大规模 MLLMs 不可行。", "method": "提出 FedNano 框架，将大型语言模型（LLM）集中部署在服务器端。在客户端引入轻量级模块 NanoEdge 进行客户端特定适应。NanoEdge 采用模态特定的编码器、连接器和可训练的基于低秩适应的 NanoAdapters。客户端仅传输紧凑的 NanoAdapter 更新。", "result": "FedNano 在实验中优于先前的联邦学习基线方法。它将客户端存储减少了95%，并将通信开销限制在模型参数的0.01%。", "conclusion": "FedNano 弥合了 MLLM 规模与联邦学习可行性之间的差距，实现了可扩展、去中心化的多模态 AI 系统。", "translation": "多模态大型语言模型（MLLMs）在多模态推理和跨模态检索等任务中表现出色，但由于分布式多模态数据和严格的隐私要求，在实际场景中面临部署挑战。联邦学习（FL）通过实现在不集中数据的情况下进行协作模型训练，提供了一个解决方案。然而，为 MLLMs 实现 FL 带来了重大挑战，包括高计算需求、有限的客户端容量、巨大的通信成本以及异构客户端数据。现有的 FL 方法假设在客户端部署完整模型，但这一假设对于大规模 MLLMs 因其庞大的尺寸和通信需求而失效。为了解决这些限制，我们提出了 FedNano，这是第一个将 LLM 集中在服务器上，同时引入 NanoEdge（一个用于客户端特定适应的轻量级模块）的 FL 框架。NanoEdge 采用模态特定的编码器、连接器和具有低秩适应的可训练 NanoAdapters。这种设计消除了在客户端部署 LLM 的需要，将客户端存储减少了 95%，并将通信开销限制在仅占模型参数的 0.01%。通过仅传输紧凑的 NanoAdapter 更新，FedNano 在保留隐私的同时处理异构客户端数据和资源限制。实验表明，FedNano 优于先前的 FL 基线，弥合了 MLLM 规模与 FL 可行性之间的差距，并实现了可扩展、去中心化的多模态 AI 系统。", "summary": "本文提出了 FedNano，一种针对预训练多模态大型语言模型（MLLMs）的轻量级联邦学习（FL）框架。为解决 MLLMs 在 FL 中面临的计算、通信和部署挑战，FedNano 将 LLM 核心保留在服务器端，而客户端仅部署轻量级的 NanoEdge 模块，该模块包含模态特定组件和基于低秩适应的 NanoAdapters。此设计显著减少了客户端存储和通信开销，并能处理数据异构性。实验证明 FedNano 优于现有 FL 基线，实现了可扩展的去中心化多模态 AI 系统。", "keywords": "联邦学习, 多模态大型语言模型, 轻量级微调, 低秩适应, 隐私保护", "comments": "FedNano 通过将大型模型的核心置于服务器端，并在客户端使用轻量级适配器进行微调，为大规模多模态大模型在联邦学习场景下的部署提供了创新性解决方案。它有效地解决了 MLLMs 面临的计算资源、通信带宽和隐私保护等核心挑战，具有重要的实践意义。"}}
{"id": "2506.15266", "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments", "authors": ["Sungen Hahm", "Heejin Kim", "Gyuseong Lee", "Hyunji Park", "Jaejin Lee"], "summary": "To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15266v1", "AI": {"title_translation": "Thunder-DeID：韩国法院判决的准确高效去标识化框架", "tldr": "提出Thunder-DeID框架，通过构建数据集、系统分类PII和开发深度神经网络，实现韩国法院判决的SOTA去标识化。", "motivation": "为了平衡司法公开和个人数据保护，韩国法院判决需去标识化，但现有流程无法大规模处理并遵守严格法律要求，且个人标识符的法律定义模糊，不适用于技术解决方案。", "method": "1. 构建并发布首个包含标注判决及相应实体提及列表的韩国法律数据集。2. 引入个人可识别信息（PII）的系统分类。3. 开发端到端深度神经网络（DNN）去标识化管道。", "result": "所提出的模型在法院判决去标识化方面取得了最先进的性能。", "conclusion": "该框架有效解决了韩国法院判决大规模去标识化的挑战，并实现了卓越的性能。", "translation": "为了确保司法公开和个人数据保护之间的平衡，韩国司法机构要求在公开法院判决前对其进行去标识化处理。然而，目前的去标识化流程不足以大规模处理法院判决，同时又难以遵守严格的法律要求。此外，个人标识符的法律定义和分类模糊不清，不适合技术解决方案。为了解决这些挑战，我们提出了一个名为 Thunder-DeID 的去标识化框架，该框架符合相关法律和实践。具体来说，我们 (i) 构建并发布了第一个包含标注判决及相应实体提及列表的韩国法律数据集，(ii) 引入了个人可识别信息 (PII) 的系统分类，以及 (iii) 开发了一个端到端的基于深度神经网络 (DNN) 的去标识化管道。我们的实验结果表明，我们的模型在法院判决的去标识化方面取得了最先进的性能。", "summary": "本文针对韩国法院判决大规模去标识化面临的挑战，提出了一个名为Thunder-DeID的框架。该框架通过构建首个韩国法律标注数据集、系统化PII分类以及开发基于深度神经网络的端到端去标识化管道，有效解决了现有流程效率低下和法律定义模糊的问题。实验结果表明，Thunder-DeID在法院判决去标识化方面达到了最先进的性能。", "keywords": "去标识化, 法院判决, 个人可识别信息, 深度神经网络, 韩国法律", "comments": "这项工作具有重要的实际意义，因为它解决了韩国司法系统中大规模公开法院判决的实际需求，同时兼顾了数据隐私。其创新点在于首次构建了韩国法律领域的标注数据集，并提出了系统化的PII分类，为后续研究和应用奠定了基础。端到端DNN管道的应用也展示了深度学习在法律文本处理中的潜力。"}}
{"id": "2506.15541", "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity", "authors": ["Oluwadamilola Fasina", "Ruben V. C. Pohle", "Pei-Chun Su", "Ronald R. Coifman"], "summary": "We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.", "comment": "16 pages, 6 figures, 2 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15541v1", "AI": {"title_translation": "内在和外在组织注意力：Softmax不变性和网络稀疏性", "tldr": "该论文研究了Transformer中自注意力机制的内在和外在结构，通过拟微分计算证明了Softmax不变性，并利用分层张量组织分析网络稀疏性，展示了其在可解释性和模型剪枝方面的应用价值。", "motivation": "为了深入理解Transformer中自注意力机制的内在（注意力头内部）和外在（注意力头之间）结构，并探索其对模型可解释性和网络应用（如模型剪枝）的潜在影响。", "method": "论文采用了多种方法：1. 运用拟微分计算获得了自注意力机制对softmax激活不变性的理论证据，并通过计算示例进行支持。2. 采用现有的张量分层组织方法，通过构建基于查询、键和头轴的网络3-张量分层分区树来检查网络结构。3. 通过可视化注意力头和扩散图嵌入的层次组织进行定性说明。4. 通过使用查询、键和头空间上的双和三哈尔基的扩展系数，定量研究了网络稀疏性。5. 使用视觉和语言Transformer提供了计算示例以展示理论和方法学发现的实用性。", "result": "主要成果包括：1. 获得了自注意力机制对softmax激活不变性的理论证据。2. 证明了网络3-张量的分层组织能够有效地执行信号处理任务。3. 定性可视化了注意力头的分层组织，并定量研究了网络稀疏性。4. 揭示了这些发现对解释性分析（理论上可行且可经验利用）和经验性网络应用（如模型剪枝和网络架构比较）的双重影响。", "conclusion": "本研究的发现为自注意力机制的解释性分析提供了理论基础，并为利用网络3-张量组织进行模型剪枝和网络架构比较等经验性网络应用提供了方法学框架，从而提升了模型的理解和优化能力。", "translation": "我们研究了Transformer中自注意力机制的内在（注意力头内部）和外在（注意力头之间）结构。通过引用拟微分计算，获得了自注意力机制对softmax激活不变性的理论证据（并得到计算示例的支持），这依赖于注意力头的内在组织。此外，我们利用现有的一种张量分层组织方法，通过构建关于网络3-张量的查询、键和头轴的分层分区树来检查网络结构。这种组织是重要的，因为它允许人们在组织好的网络3-张量表现出规律性的几何结构上有效地执行常见的信号处理任务。我们通过可视化由注意力头和扩散图嵌入组成的树的分层组织来定性地举例说明，并通过使用查询、键和头空间上的双和三哈尔基（分别为）的各个注意力头和整个网络的扩展系数来定量地研究网络稀疏性。为了展示我们理论和方法学发现的实用性，我们提供了使用视觉和语言Transformer的计算示例。这些发现的影响是双重的：（1）理论上允许解释性分析的后续步骤，并且可以凭经验用于下游解释性任务；（2）可以利用网络3-张量组织进行经验性的网络应用，例如模型剪枝（凭借网络稀疏性）和网络架构比较。", "summary": "本论文深入探讨了Transformer中自注意力机制的内在和外在组织。它通过拟微分计算为自注意力机制的Softmax不变性提供了理论依据，并提出了一种分层张量组织方法来分析网络结构和稀疏性。研究结果为自注意力机制的解释性分析开辟了新途径，并支持模型剪枝和网络架构比较等实际网络应用。", "keywords": "自注意力, Softmax不变性, 网络稀疏性, Transformer, 张量组织", "comments": "该论文对自注意力机制的Softmax不变性提供了新颖的理论视角，这对于理解Transformer的行为至关重要。所提出的分层张量组织及其在网络稀疏性分析中的应用，为模型分析和优化提供了一种系统方法，有望带来更高效和可解释的Transformer模型。其同时关注理论可解释性和剪枝等实际应用，是该研究的一个显著优点。"}}
{"id": "2506.15380", "title": "Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning", "authors": ["Taegeun Yang", "Jiwoo Hwang", "Jeil Jeong", "Minsung Yoon", "Sung-Eui Yoon"], "summary": "We propose a hierarchical reinforcement learning (HRL) framework for\nefficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.\nOur approach combines interaction-based obstacle property estimation with\nstructured pushing strategies, facilitating the dynamic manipulation of\nunforeseen obstacles while adhering to a pre-planned global path. The\nhigh-level policy generates pushing commands that consider environmental\nconstraints and path-tracking objectives, while the low-level policy precisely\nand stably executes these commands through coordinated whole-body movements.\nComprehensive simulation-based experiments demonstrate improvements in\nperforming NAMO tasks, including higher success rates, shortened traversed path\nlength, and reduced goal-reaching times, compared to baselines. Additionally,\nablation studies assess the efficacy of each component, while a qualitative\nanalysis further validates the accuracy and reliability of the real-time\nobstacle property estimation.", "comment": "8 pages, 6 figures, Accepted to IROS 2025. Supplementary Video:\n  https://youtu.be/sZ8_z7sYVP0", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15380v1", "AI": {"title_translation": "通过分层策略学习实现移动操纵器在可移动障碍物中的高效导航", "tldr": "本文提出了一种分层强化学习框架，用于移动操纵器在可移动障碍物（NAMO）中进行高效导航，结合了基于交互的障碍物属性估计和结构化推动策略，以提高成功率、缩短路径长度和减少到达目标时间。", "motivation": "在可移动障碍物中进行导航（NAMO）是一个挑战，需要移动操纵器能够动态操纵意外障碍物并同时遵循预设的全局路径。", "method": "本文提出了一种分层强化学习（HRL）框架。该框架结合了基于交互的障碍物属性估计和结构化推动策略。高层策略生成考虑环境约束和路径跟踪目标的推动指令，而低层策略通过协调全身运动精确稳定地执行这些指令。", "result": "与基线相比，在模拟实验中，NAMO任务的执行得到了改进，包括更高的成功率、更短的遍历路径长度和更少的到达目标时间。此外，消融研究评估了每个组件的功效，定性分析进一步验证了实时障碍物属性估计的准确性和可靠性。", "conclusion": "该分层强化学习框架能够使移动操纵器在可移动障碍物环境中进行高效导航，并通过实验证明了其在成功率、路径效率和时间效率方面的优越性，以及各组件的有效性和障碍物估计的准确性。", "translation": "我们提出了一种分层强化学习（HRL）框架，用于移动操纵器在可移动障碍物（NAMO）中的高效导航。我们的方法结合了基于交互的障碍物属性估计和结构化推动策略，从而促进了对意外障碍物的动态操纵，同时遵循预先规划的全局路径。高层策略生成考虑环境约束和路径跟踪目标的推动指令，而低层策略通过协调全身运动精确稳定地执行这些指令。全面的基于模拟的实验表明，与基线相比，执行NAMO任务的性能有所提高，包括更高的成功率、更短的遍历路径长度和更少的到达目标时间。此外，消融研究评估了每个组件的功效，而定性分析进一步验证了实时障碍物属性估计的准确性和可靠性。", "summary": "本文提出了一种用于移动操纵器在可移动障碍物中高效导航（NAMO）的分层强化学习（HRL）框架。该方法结合了基于交互的障碍物属性估计和结构化推动策略，实现了对未知障碍物的动态操作，同时遵循预设路径。高层策略负责生成推动指令，低层策略负责精确执行。模拟实验结果表明，该方法在成功率、路径长度和目标达成时间方面均优于基线，并且验证了障碍物属性估计的准确性。", "keywords": "分层强化学习, 移动操纵器, 可移动障碍物导航, 障碍物属性估计, 推动策略", "comments": "本文的创新点在于结合了分层强化学习、交互式障碍物属性估计和结构化推动策略，为移动操纵器在动态、未知的可移动障碍物环境中导航提供了有效解决方案。其分层策略设计有效解耦了高层决策和低层执行，提高了系统的鲁棒性和效率。"}}
{"id": "2506.15033", "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "authors": ["Gary Song Yan", "Yusen Zhang", "Jinyu Zhao", "Hao Zhang", "Zhangping Yang", "Guanye Xiong", "Yanfei Liu", "Tao Zhang", "Yujie He", "Siyuan Tian", "Yao Gou", "Min Li"], "summary": "In this pioneering study, we introduce StyleWallfacer, a groundbreaking\nunified training and inference framework, which not only addresses various\nissues encountered in the style transfer process of traditional methods but\nalso unifies the framework for different tasks. This framework is designed to\nrevolutionize the field by enabling artist level style transfer and text driven\nstylization. First, we propose a semantic-based style injection method that\nuses BLIP to generate text descriptions strictly aligned with the semantics of\nthe style image in CLIP space. By leveraging a large language model to remove\nstyle-related descriptions from these descriptions, we create a semantic gap.\nThis gap is then used to fine-tune the model, enabling efficient and drift-free\ninjection of style knowledge. Second, we propose a data augmentation strategy\nbased on human feedback, incorporating high-quality samples generated early in\nthe fine-tuning process into the training set to facilitate progressive\nlearning and significantly reduce its overfitting. Finally, we design a\ntraining-free triple diffusion process using the fine-tuned model, which\nmanipulates the features of self-attention layers in a manner similar to the\ncross-attention mechanism. Specifically, in the generation process, the key and\nvalue of the content-related process are replaced with those of the\nstyle-related process to inject style while maintaining text control over the\nmodel. We also introduce query preservation to mitigate disruptions to the\noriginal content. Under such a design, we have achieved high-quality\nimage-driven style transfer and text-driven stylization, delivering\nartist-level style transfer results while preserving the original image\ncontent. Moreover, we achieve image color editing during the style transfer\nprocess for the first time.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15033v1", "AI": {"title_translation": "打破风格索福：我们是否真的注定要在风格迁移中限制想象力？", "tldr": "提出StyleWallfacer统一框架，通过语义注入、数据增强和无训练扩散过程实现艺术家级别图像和文本驱动的风格迁移，首次支持图像色彩编辑并保持内容。", "motivation": "传统风格迁移方法存在各种问题，并且缺乏统一的框架来处理不同任务。本文旨在通过革新性的方法实现艺术家级别的风格迁移和文本驱动的风格化。", "method": "本文引入了StyleWallfacer统一框架。首先，提出了一种基于语义的风格注入方法，利用BLIP生成与风格图像语义严格对齐的文本描述，并通过大型语言模型创建语义间隙，用于微调模型以高效、无漂移地注入风格知识。其次，提出了一种基于人类反馈的数据增强策略，将微调过程早期生成的高质量样本纳入训练集，以促进渐进学习并显著减少过拟合。最后，设计了一个使用微调模型的无训练三元扩散过程，通过替换自注意力层的关键和值来注入风格，同时通过查询保留保持文本控制和原始内容。", "result": "实现了高质量的图像驱动风格迁移和文本驱动风格化，达到了艺术家级别的风格迁移效果，同时保留了原始图像内容。此外，首次在风格迁移过程中实现了图像色彩编辑。", "conclusion": "通过所提出的统一框架和创新方法，本文成功解决了传统风格迁移的挑战，实现了艺术家级别的风格迁移和文本驱动的风格化，并首次引入了图像色彩编辑功能，同时有效保留了原始内容。", "translation": "在这项开创性研究中，我们引入了StyleWallfacer，一个突破性的统一训练和推理框架，它不仅解决了传统方法在风格迁移过程中遇到的各种问题，而且统一了不同任务的框架。该框架旨在通过实现艺术家级别的风格迁移和文本驱动的风格化来彻底改变该领域。首先，我们提出了一种基于语义的风格注入方法，该方法使用BLIP生成与CLIP空间中风格图像语义严格对齐的文本描述。通过利用大型语言模型从这些描述中删除与风格相关的描述，我们创建了一个语义间隙。然后，该间隙用于微调模型，从而能够高效且无漂移地注入风格知识。其次，我们提出了一种基于人类反馈的数据增强策略，将微调过程早期生成的高质量样本纳入训练集，以促进渐进学习并显著减少其过拟合。最后，我们设计了一个使用微调模型的无训练三元扩散过程，该过程以类似于交叉注意力机制的方式操作自注意力层的特征。具体来说，在生成过程中，内容相关过程的键和值被替换为风格相关过程的键和值，以在注入风格的同时保持对模型的文本控制。我们还引入了查询保留以减轻对原始内容的干扰。在这种设计下，我们实现了高质量的图像驱动风格迁移和文本驱动风格化，提供了艺术家级别的风格迁移结果，同时保留了原始图像内容。此外，我们首次在风格迁移过程中实现了图像色彩编辑。", "summary": "本文提出了StyleWallfacer，一个统一的训练和推理框架，旨在解决传统风格迁移问题并实现艺术家级别及文本驱动的风格化。该框架包含三个核心创新：基于BLIP和大型语言模型的语义风格注入方法以创建语义间隙进行高效风格知识注入；基于人类反馈的数据增强策略以减少过拟合；以及一个无训练的三元扩散过程，通过操纵自注意力特征实现风格注入和文本控制。实验结果表明，该方法实现了高质量的图像和文本驱动风格迁移，达到了艺术家级别效果，并首次在风格迁移中实现了图像色彩编辑，同时有效保留了原始内容。", "keywords": "风格迁移, StyleWallfacer, 扩散模型, 文本驱动风格化, 语义注入", "comments": "这项研究通过引入StyleWallfacer框架，在风格迁移领域取得了显著进展，特别是在实现艺术家级别效果和文本驱动风格化方面。其创新点在于结合了语义间隙注入、人类反馈数据增强和无训练扩散过程，这为高效、高质量的风格迁移提供了新的范式。首次实现图像色彩编辑是其另一重要贡献，拓宽了风格迁移的应用范围。该统一框架也提高了不同任务间的通用性。"}}
{"id": "2506.14828", "title": "Accurate and Uncertainty-Aware Multi-Task Prediction of HEA Properties Using Prior-Guided Deep Gaussian Processes", "authors": ["Sk Md Ahnaf Akif Alvi", "Mrinalini Mulukutla", "Nicolas Flores", "Danial Khatamsaz", "Jan Janssen", "Danny Perez", "Douglas Allaire", "Vahid Attari", "Raymundo Arroyave"], "summary": "Surrogate modeling techniques have become indispensable in accelerating the\ndiscovery and optimization of high-entropy alloys(HEAs), especially when\nintegrating computational predictions with sparse experimental observations.\nThis study systematically evaluates the fitting performance of four prominent\nsurrogate models conventional Gaussian Processes(cGP), Deep Gaussian\nProcesses(DGP), encoder-decoder neural networks for multi-output regression and\nXGBoost applied to a hybrid dataset of experimental and computational\nproperties in the AlCoCrCuFeMnNiV HEA system. We specifically assess their\ncapabilities in predicting correlated material properties, including yield\nstrength, hardness, modulus, ultimate tensile strength, elongation, and average\nhardness under dynamic and quasi-static conditions, alongside auxiliary\ncomputational properties. The comparison highlights the strengths of\nhierarchical and deep modeling approaches in handling heteroscedastic,\nheterotopic, and incomplete data commonly encountered in materials informatics.\nOur findings illustrate that DGP infused with machine learning-based prior\noutperform other surrogates by effectively capturing inter-property\ncorrelations and input-dependent uncertainty. This enhanced predictive accuracy\npositions advanced surrogate models as powerful tools for robust and\ndata-efficient materials design.", "comment": "Deep Gaussian Processes Multi-task Gaussian Processes High Entropy\n  Alloys", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14828v1", "AI": {"title_translation": "基于先验引导的深度高斯过程的HEA性能精确且不确定性感知多任务预测", "tldr": "本研究评估了用于高熵合金（HEA）性能预测的替代模型，发现结合机器学习先验的深度高斯过程（DGP）在捕获属性间相关性和输入依赖性不确定性方面表现最佳，为材料设计提供了强大工具。", "motivation": "替代建模技术在加速高熵合金（HEAs）的发现和优化方面不可或缺，尤其是在将计算预测与稀疏实验观察相结合时。本研究旨在系统评估并比较不同替代模型在预测HEAs相关材料属性方面的能力。", "method": "本研究系统评估了四种主要替代模型（传统高斯过程、深度高斯过程、用于多输出回归的编码器-解码器神经网络和XGBoost），应用于AlCoCrCuFeMnNiV HEA系统中实验和计算属性的混合数据集。研究评估了这些模型在预测包括屈服强度、硬度、模量、极限拉伸强度、伸长率以及动态和准静态条件下的平均硬度等相关材料属性方面的能力。", "result": "研究发现，注入了基于机器学习先验的深度高斯过程（DGP）通过有效捕获属性间相关性和输入依赖性不确定性，优于其他替代模型。比较突出了分层和深度建模方法在处理材料信息学中常见的异方差、异位和不完整数据方面的优势。", "conclusion": "先进的替代模型，特别是结合先验引导的深度高斯过程，由于其增强的预测精度和不确定性感知能力，是实现稳健和数据高效材料设计的强大工具。", "translation": "替代建模技术在加速高熵合金（HEAs）的发现和优化方面变得不可或缺，尤其是在将计算预测与稀疏实验观察相结合时。本研究系统地评估了四种主要替代模型的拟合性能：传统高斯过程（cGP）、深度高斯过程（DGP）、用于多输出回归的编码器-解码器神经网络和XGBoost，这些模型应用于AlCoCrCuFeMnNiV HEA系统中实验和计算属性的混合数据集。我们特别评估了它们在预测相关材料属性方面的能力，包括屈服强度、硬度、模量、极限拉伸强度、伸长率以及动态和准静态条件下的平均硬度，以及辅助计算属性。比较突出了分层和深度建模方法在处理材料信息学中常见的异方差、异位和不完整数据方面的优势。我们的研究结果表明，注入了基于机器学习先验的DGP通过有效捕获属性间相关性和输入依赖性不确定性，优于其他替代模型。这种增强的预测精度使先进的替代模型成为稳健和数据高效的材料设计的强大工具。", "summary": "本论文系统比较了四种替代模型，用于使用混合数据集对高熵合金（HEA）性能进行多任务预测。研究表明，结合机器学习先验引导的深度高斯过程（DGP）在捕获属性间相关性和输入依赖性不确定性方面表现出色，优于其他模型。该研究强调了深度建模方法在处理复杂和不完整材料数据方面的有效性，将先进的替代模型定位为有价值的材料设计工具。", "keywords": "高熵合金, 深度高斯过程, 替代建模, 多任务预测, 不确定性量化", "comments": "该论文解决了材料科学领域中利用有限数据进行高效性能预测的关键需求。结合先验引导的深度高斯过程的使用是创新的，特别适用于处理相关且不确定的多输出数据。这种方法为加速高熵合金的发现和优化提供了一个稳健的解决方案。"}}
{"id": "2506.15301", "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment", "authors": ["Shrestha Ghosh", "Moritz Schneider", "Carina Reinicke", "Carsten Eickhoff"], "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15301v1", "AI": {"title_translation": "队列发现：LLM辅助临床试验招募综述", "tldr": "本文综述了大型语言模型（LLM）在临床试验患者招募中的应用，分析了其潜力、现有方法的局限性、面临的挑战及未来方向。", "motivation": "尽管大型语言模型（LLMs）在通用NLP任务中取得了显著进展，但在临床试验招募等关键领域的应用仍有限。临床试验设计和患者数据（结构化和非结构化文本）的特性使得LLMs的知识聚合和推理能力对试验-患者匹配任务非常有益。与试验特定的传统方法相比，LLMs有望提供更通用的解决方案。", "method": "本文首次对试验-患者匹配任务进行了分析，并对临床试验招募中新兴的基于LLM的方法进行了背景化处理。通过一项综述，作者批判性地审查了现有的基准、方法、评估框架、在临床研究中采用LLM技术面临的挑战以及未来发展方向。", "result": "综述分析了试验-患者匹配任务，并对LLM辅助的临床试验招募方法进行了背景化。它批判性地审查了现有的基准、方法和评估框架，指出了在临床研究中采用LLM技术面临的挑战以及未来的发展方向。同时，也指出当前LLM辅助方法依赖专有模型和弱评估基准的问题。", "conclusion": "LLMs在临床试验招募中具有巨大潜力，能提供比传统方法更通用的解决方案，但目前其应用仍面临专有模型和评估基准薄弱的挑战。未来需要关注克服这些挑战并探索新的研究方向。", "translation": "大型语言模型（LLMs）的最新进展极大地改善了通用领域的自然语言处理（NLP）任务。然而，它们在临床试验招募等关键领域的应用仍然有限。由于临床试验以自然语言设计，患者数据以结构化和非结构化文本表示，因此匹配试验和患者的任务受益于LLM的知识聚合和推理能力。传统方法是针对特定试验的，而LLMs凭借其整合分布式知识的能力，有望构建一个更通用的解决方案。然而，近期LLM辅助方法的应用依赖于专有模型和薄弱的评估基准。在本综述中，我们首次分析了试验-患者匹配任务，并对临床试验招募中新兴的基于LLM的方法进行了背景化处理。我们批判性地审查了现有的基准、方法和评估框架，以及在临床研究中采用LLM技术所面临的挑战和令人兴奋的未来方向。", "summary": "本文是一篇关于大型语言模型（LLMs）在临床试验招募中应用的综述。它指出，尽管LLMs在通用NLP任务中表现出色，但在临床试验患者匹配这一关键领域应用有限。文章分析了LLMs如何利用其知识聚合和推理能力来优化试验与患者的匹配，并指出LLMs有望提供比传统方法更通用的解决方案。同时，该综述也批判性地审视了当前LLM辅助方法所面临的挑战，如依赖专有模型和评估基准薄弱，并展望了未来的研究方向。", "keywords": "LLM, 临床试验招募, 队列发现, 患者匹配, 综述", "comments": "这篇综述的重要性在于它首次系统地分析了LLM在临床试验招募这一关键且复杂领域的应用潜力与挑战。它强调了LLM作为通用解决方案的优势，同时也明确指出了当前方法依赖专有模型和评估基准不足的局限性，为未来的研究指明了方向。其创新之处在于将LLM技术与临床试验的实际需求相结合，并对其应用现状进行了全面审视。"}}
{"id": "2506.15627", "title": "Pathwise convergence of a novel numerical scheme based on semi-implicit method for stochastic differential-algebraic equations with non-global Lipschitz coefficients", "authors": ["Guy Tsafack", "Antoine Tambue"], "summary": "This paper delves into the well-posedness and the numerical approximation of\nnon-autonomous stochastic differential algebraic equations (SDAEs) with\nnonlinear local Lipschitz coefficients that satisfy the more general\nmonotonicity condition called Khasminskii condition. The key challenge is the\npresence of a singular matrix which makes the numerical integration hard and\nheavy. To address this challenge, we propose a novel numerical scheme based on\nsemi-implicit method for the drift component of the SDAEs. More precisely we\nsplit the drift term as the sum of a linear term and a nonlinear term. The\nlinear part is approximated implicitly, while the nonlinear part is\napproximated explicitly. The linear component's role is to handle the\nsingularity issues during the numerical integration without the resolution of\nnonlinear algebraic equations in the constraint equations. This novel scheme is\ntherefore very efficient for SDAEs in high dimension that come after the\nspatial discretisation of stochastic partial differential algebraic equations\n(SPDAEs). To prove the pathwise convergence of our novel scheme, we first\nderive a equivalent scheme called dual scheme, suitable for mathematical\nanalysis and linked to the inherent stochastic differential equation resulting\nfrom the elimination of constraints in the initial SDAEs. We prove that our\nnovel scheme converges to the exact solution with rate $\\frac{1}{2}-\\epsilon$,\nfor arbitrary $\\epsilon>0$ in the pathwise sense. Numerical simulations are\nperformed to demonstrate the efficiency of the scheme in high dimension and to\nshow that our theoretical results are in agreement with numerical experiments.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15627v1", "AI": {"title_translation": "基于半隐式方法求解非全局Lipschitz系数随机微分代数方程的路径收敛性新数值格式", "tldr": "本文提出了一种用于具有非全局Lipschitz系数的随机微分代数方程（SDAEs）的新型半隐式数值格式，证明了其路径收敛性并展示了其效率。", "motivation": "核心挑战在于具有满足Khasminskii条件的非线性局部Lipschitz系数的非自治随机微分代数方程（SDAEs）中存在奇异矩阵，这使得数值积分变得困难和繁重。", "method": "提出了一种基于半隐式方法的新型数值格式，用于SDAEs的漂移分量。具体来说，将漂移项分解为线性和非线性项之和。线性部分采用隐式近似，而非线性部分采用显式近似。线性分量的作用是在数值积分过程中处理奇异性问题，而无需求解约束方程中的非线性代数方程。为了证明该新方案的路径收敛性，首先推导了一个等效方案，称为对偶方案，适用于数学分析并与初始SDAEs中消除约束后产生的固有随机微分方程相关联。", "result": "本文证明了所提出的新方案以 $\\frac{1}{2}-\\epsilon$ 的速率（对于任意 $\\epsilon>0$）在路径意义上收敛到精确解。通过数值模拟证明了该方案在高维情况下的效率，并表明理论结果与数值实验结果吻合。", "conclusion": "本文成功提出了一种高效的半隐式数值格式，用于解决具有非全局Lipschitz系数的随机微分代数方程，克服了奇异性问题并证明了路径收敛性。", "translation": "本文深入研究了具有满足更一般单调性条件（称为Khasminskii条件）的非线性局部Lipschitz系数的非自治随机微分代数方程（SDAEs）的适定性和数值逼近。主要挑战在于存在一个奇异矩阵，这使得数值积分变得困难和繁重。为了解决这一挑战，我们提出了一种基于半隐式方法的新型数值格式，用于SDAEs的漂移分量。更准确地说，我们将漂移项分解为线性和非线性项之和。线性部分采用隐式近似，而非线性部分采用显式近似。线性分量的作用是在数值积分过程中处理奇异性问题，而无需求解约束方程中的非线性代数方程。因此，这种新型方案对于随机偏微分代数方程（SPDAEs）空间离散化后产生的高维SDAEs非常高效。为了证明我们新型方案的路径收敛性，我们首先推导了一个等效方案，称为对偶方案，适用于数学分析并与初始SDAEs中消除约束后产生的固有随机微分方程相关联。我们证明了我们的新型方案以 $\\frac{1}{2}-\\epsilon$ 的速率，对于任意 $\\epsilon>0$ 在路径意义上收敛到精确解。进行了数值模拟以证明该方案在高维情况下的效率，并表明我们的理论结果与数值实验结果吻合。", "summary": "本文深入研究了具有满足Khasminskii条件的非线性局部Lipschitz系数的非自治随机微分代数方程（SDAEs）的适定性和数值逼近。针对奇异矩阵导致数值积分困难的挑战，提出了一种基于半隐式方法的新型数值格式。该方案将漂移项分解为隐式处理的线性部分和显式处理的非线性部分，以有效解决奇异性问题。文章证明了该新方案以 $\\frac{1}{2}-\\epsilon$ 的速率在路径意义上收敛到精确解。数值模拟验证了该方案在高维情况下的效率，并与理论结果一致。", "keywords": "随机微分代数方程, 半隐式方法, 路径收敛性, 非全局Lipschitz, 奇异矩阵", "comments": "该论文的创新之处在于提出了一种新颖的半隐式数值格式，通过对漂移项的巧妙分解，有效地解决了随机微分代数方程（SDAEs）中由于奇异矩阵带来的数值积分难题，特别适用于高维问题。其在路径意义上证明了收敛性，并给出了具体的收敛速率，这在理论上具有重要意义。该方案对于由随机偏微分代数方程（SPDAEs）空间离散化产生的高维SDAEs具有潜在的应用价值。"}}
{"id": "2506.15402", "title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System", "authors": ["Miaoxin Pan", "Jinnan Li", "Yaowen Zhang", "Yi Yang", "Yufeng Yue"], "summary": "Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15402v1", "AI": {"title_translation": "MCOO-SLAM：一种多相机全向物体SLAM系统", "tldr": "MCOO-SLAM是一种多相机全向物体SLAM系统，通过利用环视相机配置和语义-几何-时间融合策略，解决了现有SLAM系统在大型或室外环境中视野窄、易受遮挡、深度感知有限等问题，实现了鲁棒、一致且语义丰富的物体级建图和定位。", "motivation": "大多数现有物体级SLAM方法依赖于RGB-D传感器或单目视图，它们存在视野窄、对遮挡敏感以及深度感知有限的问题，尤其是在大型或室外环境中。这些限制导致系统只能从有限视角观察物体，从而造成不准确的物体建模和不可靠的数据关联。", "method": "MCOO-SLAM利用环视相机配置，整合了点特征和增强了开放词汇语义的物体级地标。它引入了语义-几何-时间融合策略以实现跨多视图的鲁棒物体关联，并设计了一个全向闭环模块以实现视点不变的地点识别。此外，构建的地图被抽象为分层3D场景图以支持下游推理任务。", "result": "在真实世界实验中，MCOO-SLAM实现了准确的定位和可扩展的物体级建图，并提高了对遮挡、姿态变化和环境复杂性的鲁棒性。", "conclusion": "MCOO-SLAM通过创新的多相机全向方法，有效解决了现有物体级SLAM在复杂室外环境中的局限性，实现了更鲁棒、一致且语义丰富的建图和定位，为高级机器人任务提供了支持。", "translation": "物体级SLAM提供结构化且具有语义意义的环境表示，使其更具可解释性并适用于高级机器人任务。然而，大多数现有方法依赖于RGB-D传感器或单目视图，它们存在视野窄、对遮挡敏感以及深度感知有限的问题——尤其是在大型或室外环境中。这些限制通常使系统只能从有限视角观察物体，从而导致不准确的物体建模和不可靠的数据关联。在这项工作中，我们提出了MCOO-SLAM，一种新颖的多相机全向物体SLAM系统，它充分利用环视相机配置，在复杂的室外场景中实现鲁棒、一致且语义丰富的建图。我们的方法整合了点特征和通过开放词汇语义增强的物体级地标。引入了一种语义-几何-时间融合策略，用于在多个视图之间进行鲁棒的物体关联，从而提高了一致性和准确的物体建模，并且设计了一个全向闭环模块，以使用场景级描述符实现视点不变的地点识别。此外，构建的地图被抽象为分层3D场景图以支持下游推理任务。在真实世界中的大量实验表明，MCOO-SLAM实现了准确的定位和可扩展的物体级建图，并提高了对遮挡、姿态变化和环境复杂性的鲁棒性。", "summary": "MCOO-SLAM是一种新型的多相机全向物体SLAM系统，旨在解决现有方法在复杂室外环境中视野窄、遮挡敏感和深度感知不足的问题。该系统通过利用环视相机配置，结合点特征和语义增强的物体级地标，并采用语义-几何-时间融合策略进行物体关联，以及全向闭环模块进行地点识别。构建的地图被抽象为分层3D场景图。实验证明MCOO-SLAM在定位和物体级建图方面具有高精度和鲁棒性。", "keywords": "物体SLAM, 多相机, 全向, 语义建图, 场景图", "comments": "MCOO-SLAM的创新之处在于其充分利用多相机全向配置，并结合了语义增强的物体级地标和多层次的融合策略，有效克服了传统SLAM在复杂户外场景中的局限性。其提出的语义-几何-时间融合和全向闭环模块对于提高物体关联和地点识别的鲁棒性具有重要意义。分层3D场景图的构建也为高级推理任务提供了良好的基础。"}}
{"id": "2506.15078", "title": "Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study", "authors": ["Xianghong Fang", "Litao Guo", "Hengchao Chen", "Yuxuan Zhang", "XiaofanXia", "Dingjie Song", "Yexin Liu", "Hao Wang", "Harry Yang", "Yuan Yuan", "Qiang Sun"], "summary": "The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15078v1", "AI": {"title_translation": "通过分布匹配增强向量量化：一项理论与实证研究", "tldr": "本文通过使用Wasserstein距离对齐特征和码本向量的分布，解决了向量量化中的训练不稳定和码本崩溃问题，显著提高了码本利用率并减少了量化误差。", "motivation": "现有向量量化方法存在训练不稳定性和码本崩溃问题，这主要是由于特征和码本向量的分布不匹配导致的，导致码本向量缺乏代表性并造成数据信息损失。", "method": "采用Wasserstein距离来对齐特征和码本向量的分布。", "result": "实现了接近100%的码本利用率，并显著降低了量化误差。", "conclusion": "理论和实证分析均验证了所提出方法的有效性。", "translation": "自回归模型的成功在很大程度上取决于向量量化的有效性，这是一种通过将连续特征映射到可学习码本中最近的码向量来离散化连续特征的技术。现有向量量化方法的两个关键问题是训练不稳定性和码本崩溃。训练不稳定性源于直通估计器引入的梯度差异，尤其是在存在显著量化误差的情况下，而码本崩溃则发生在训练期间仅使用一小部分码向量时。对这些问题的仔细检查表明，它们主要是由特征和码向量之间分布不匹配引起的，导致码向量缺乏代表性并在压缩过程中造成显著的数据信息损失。为了解决这个问题，我们采用Wasserstein距离来对齐这两个分布，实现了接近100%的码本利用率并显著降低了量化误差。理论和实证分析均验证了所提出方法的有效性。", "summary": "向量量化对于自回归模型至关重要，但现有方法存在训练不稳定和码本崩溃问题，这源于特征与码本向量的分布不匹配。本文提出利用Wasserstein距离对齐这些分布，从而实现了近100%的码本利用率并显著降低了量化误差，该方法通过理论和实证分析得到了验证。", "keywords": "向量量化, 分布匹配, Wasserstein距离, 码本崩溃, 量化误差", "comments": "本文通过识别向量量化中训练不稳定和码本崩溃的根本原因（分布不匹配），提出了创新的解决方案。利用Wasserstein距离进行分布对齐在理论上是合理的，并且取得了显著的实际效果（高码本利用率和低量化误差）。"}}
{"id": "2506.15304", "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification", "authors": ["Negar Foroutan", "Jakhongir Saydaliev", "Ye Eun Kim", "Antoine Bosselut"], "summary": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.", "comment": "Submitted to EMNLP", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15304v1", "AI": {"title_translation": "ConLID：低资源语言识别的监督对比学习", "tldr": "提出了一种监督对比学习方法ConLID，以提高低资源语言在域外数据上的语言识别性能。", "motivation": "语言识别（LID）是构建多语言LLM预训练语料库的关键步骤，但低资源语言由于训练数据有限且通常为单一领域，导致性能不佳，存在类别不平衡和偏差问题。", "method": "提出了一种新颖的监督对比学习（SCL）方法，旨在学习低资源语言的领域不变表示，以解决类别不平衡和偏差问题。", "result": "该方法使低资源语言在域外数据上的LID性能提高了3.2%。", "conclusion": "该方法有效提升了LID模型对低资源语言的识别能力。", "translation": "语言识别（LID）是整理网络爬取的多语言大型语言模型（LLM）预训练语料库的关键一步。尽管许多关于LID模型训练的研究侧重于收集多样化的训练数据以提高性能，但低资源语言——通常局限于单一领域数据，例如圣经——的性能仍然很差。为了解决这些类别不平衡和偏差问题，我们提出了一种新颖的监督对比学习（SCL）方法来学习低资源语言的领域不变表示。通过广泛的分析，我们表明我们的方法将低资源语言在域外数据上的LID性能提高了3.2%，证明了其在增强LID模型方面的有效性。", "summary": "本文提出了一种名为ConLID的监督对比学习（SCL）方法，旨在解决低资源语言识别（LID）中因数据稀缺和领域限制导致的性能不佳问题。通过学习领域不变表示，ConLID显著提升了低资源语言在域外数据上的LID性能，提高了3.2%，证明了其在增强LID模型方面的有效性。", "keywords": "语言识别, 低资源语言, 监督对比学习, 领域不变表示, ConLID", "comments": "该研究通过引入监督对比学习来解决低资源语言识别中的领域适应性问题，具有创新性。其关注点在于学习领域不变表示，这对于处理数据稀疏和领域偏差导致的性能瓶颈非常重要，为多语言LLM语料库的构建提供了有价值的改进。"}}
{"id": "2506.15630", "title": "Non-uniform finite-element meshes defined by ray dynamics for Helmholtz problems", "authors": ["Martin Averseng", "Jeffrey Galkowski", "Euan A. Spence"], "summary": "The $h$-version of the finite-element method ($h$-FEM) applied to the\nhigh-frequency Helmholtz equation has been a classic topic in numerical\nanalysis since the 1990s. It is now rigorously understood that (using piecewise\npolynomials of degree $p$ on a mesh of a maximal width $h$) the conditions\n\"$(hk)^p \\rho$ sufficiently small\" and \"$(hk)^{2p} \\rho$ sufficiently small\"\nguarantee, respectively, $k$-uniform quasioptimality (QO) and bounded relative\nerror (BRE), where $\\rho$ is the norm of the solution operator with $\\rho\\sim\nk$ for non-trapping problems. Empirically, these conditions are observed to be\noptimal in the context of $h$-FEM with a uniform mesh. This paper demonstrates\nthat QO and BRE can be achieved using certain non-uniform meshes that violate\nthe conditions above on $h$ and involve coarser meshes away from trapping and\nin the perfectly matched layer (PML). The main theorem details how varying the\nmeshwidth in one region affects errors both in that region and elsewhere. One\nnotable consequence is that, for any scattering problem (trapping or\nnontrapping), in the PML one only needs $hk$ to be sufficiently small; i.e.\nthere is no pollution in the PML.\n  The motivating idea for the analysis is that the Helmholtz data-to-solution\nmap behaves differently depending on the locations of both the measurement and\ndata, in particular, on the properties of billiards trajectories (i.e. rays)\nthrough these sets. Because of this, it is natural that the approximation\nrequirements for finite-element spaces in a subset should depend on the\nproperties of billiard rays through that set. Inserting this behaviour into the\nlatest duality arguments for the FEM applied to the high-frequency Helmholtz\nequation allows us to retain detailed information about the influence of\n$\\textit{both}$ the mesh structure $\\textit{and}$ the behaviour of the true\nsolution on local errors in FEM.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15630v1", "AI": {"title_translation": "非均匀有限元网格，由射线动力学定义，用于亥姆霍兹问题", "tldr": "本文提出一种基于射线动力学定义的非均匀网格，用于高频亥姆霍兹方程的有限元方法，证明其在保持准最优性与有界相对误差的同时，可以放宽网格限制，特别是在PML区域。", "motivation": "传统均匀网格下高频亥姆霍兹方程的有限元方法（h-FEM）的误差条件在实践中被认为是最佳的。然而，亥姆霍兹数据到解的映射行为因测量和数据位置的不同而不同，特别是与通过这些区域的射线轨迹特性有关。因此，研究人员认为子区域的有限元空间近似要求应取决于通过该区域的射线特性。", "method": "本文通过将射线动力学行为（即亥姆霍兹数据到解映射的局部特性，取决于射线轨迹）整合到最新应用于高频亥姆霍兹方程的有限元对偶论证中，来构建非均匀网格。这种网格在远离陷阱和PML区域使用更粗的网格，从而放宽了传统的网格条件。", "result": "结果表明，使用这种由射线动力学定义的非均匀网格，可以实现在$h$-FEM中达到准最优性（QO）和有界相对误差（BRE），即使这些网格违反了传统均匀网格的条件。一个显著的后果是，对于任何散射问题（无论是否存在陷阱），在完美匹配层（PML）中，仅需$hk$足够小即可，即PML中没有污染。", "conclusion": "论文证明了在亥姆霍兹问题中，通过考虑射线动力学来定义非均匀有限元网格，可以有效提高计算效率，同时保持解的准确性，尤其是在PML区域能够消除污染，这挑战了传统均匀网格的限制。", "translation": "高频亥姆霍兹方程的有限元方法（h-FEM）自20世纪90年代以来一直是数值分析中的经典课题。现在已经严格理解，（在最大宽度为$h$的网格上使用$p$次分段多项式）条件\"$(hk)^p \\rho$足够小\"和\"$(hk)^{2p} \\rho$足够小\"分别保证了$k$-均匀准最优性（QO）和有界相对误差（BRE），其中$\\rho$是解算子范数，对于非陷阱问题$\\rho\\sim k$。根据经验，在均匀网格的h-FEM背景下，这些条件被认为是最佳的。本文证明了使用某些非均匀网格也可以实现QO和BRE，这些网格违反了上述关于$h$的条件，并且在远离陷阱区域和完美匹配层（PML）中使用更粗的网格。主要定理详细说明了在一个区域内改变网格宽度如何影响该区域和其他地方的误差。一个显著的后果是，对于任何散射问题（无论是否存在陷阱），在PML中仅需$hk$足够小即可；即PML中没有污染。\n分析的动机是，亥姆霍兹数据到解的映射行为因测量和数据位置的不同而不同，特别是取决于通过这些集合的弹子轨迹（即射线）的特性。因此，子集中有限元空间的近似要求自然应取决于通过该集合的弹子射线的特性。将这种行为插入到应用于高频亥姆霍兹方程的有限元最新对偶论证中，使我们能够保留关于网格结构和真实解行为对有限元局部误差影响的详细信息。", "summary": "本文探讨了应用于高频亥姆霍兹方程的h-FEM，提出并证明了一种基于射线动力学定义的非均匀有限元网格。与传统均匀网格下严格的误差条件不同，这种非均匀网格可以在保持准最优性和有界相对误差的同时，在远离陷阱和完美匹配层（PML）的区域使用更粗的网格。研究发现，在PML区域，仅需$hk$足够小即可避免污染。这一方法通过将局部射线轨迹特性整合到有限元对偶论证中，揭示了网格结构与真实解行为对局部误差的共同影响。", "keywords": "亥姆霍兹方程, 有限元方法, 非均匀网格, 射线动力学, 完美匹配层 (PML)", "comments": "这篇论文的创新点在于提出了基于射线动力学定义非均匀网格的方法，挑战了传统h-FEM对高频亥姆霍兹方程均匀网格的严格要求。通过允许在某些区域使用更粗的网格，该方法有望显著提高计算效率，同时保持精度，尤其是在PML中消除了污染，这对于实际应用具有重要意义。它深入理解了局部误差如何受网格和解行为双重影响。"}}
{"id": "2506.15295", "title": "A theory of Lending Protocols in DeFi", "authors": ["Massimo Bartoletti", "Enrico Lipparini"], "summary": "Lending protocols are one of the main applications of Decentralized Finance\n(DeFi), enabling crypto-assets loan markets with a total value estimated in the\ntens of billions of dollars. Unlike traditional lending systems, these\nprotocols operate without relying on trusted authorities or off-chain\nenforcement mechanisms. To achieve key economic goals such as stability of the\nloan market, they devise instead trustless on-chain mechanisms, such as\nrewarding liquidators who repay the loans of under-collateralized borrowers by\nawarding them part of the borrower's collateral. The complexity of these\nincentive mechanisms, combined with their entanglement in low-level\nimplementation details, makes it challenging to precisely assess the structural\nand economic properties of lending protocols, as well as to analyze user\nstrategies and attacks. Crucially, since participation is open to anyone, any\nweaknesses in the incentive mechanism may give rise to unintended emergent\nbehaviours, or even enable adversarial strategies aimed at making profits to\nthe detriment of legit users, or at undermining the stability of the protocol.\nIn this work, we propose a formal model of lending protocols that captures the\nessential features of mainstream platforms, enabling us to identify and prove\nkey properties related to their economic and strategic dynamics.", "comment": null, "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.15295v1", "AI": {"title_translation": "DeFi中借贷协议的理论", "tldr": "本文提出了一种DeFi借贷协议的形式化模型，旨在分析其复杂的经济和战略特性，并识别潜在的弱点。", "motivation": "由于DeFi借贷协议中链上激励机制的复杂性及其与底层实现细节的纠缠，导致难以精确评估其结构和经济特性，也难以分析用户策略和潜在的攻击。此外，由于参与是开放的，激励机制的任何弱点都可能导致意想不到的行为或对抗性策略，从而损害合法用户或破坏协议的稳定性。", "method": "本文提出了一个借贷协议的形式化模型，该模型捕获了主流平台的关键特征。", "result": "该形式化模型能够识别并证明与借贷协议的经济和战略动态相关的关键属性。", "conclusion": "本文提出的形式化模型为理解和分析DeFi借贷协议中复杂的经济和战略行为提供了一个框架，有助于识别和证明其关键属性。", "translation": "借贷协议是去中心化金融（DeFi）的主要应用之一，支持加密资产贷款市场，总价值估计达数百亿美元。与传统借贷系统不同，这些协议的运作不依赖于受信任的机构或链下执行机制。为了实现贷款市场稳定性等关键经济目标，它们转而设计了无需信任的链上机制，例如通过奖励清算人（清偿抵押不足借款人贷款并获得部分抵押品）来激励。这些激励机制的复杂性，加上它们与底层实现细节的纠缠，使得精确评估借贷协议的结构和经济特性、以及分析用户策略和攻击变得具有挑战性。至关重要的是，由于参与对所有人开放，激励机制中的任何弱点都可能导致意想不到的涌现行为，甚至促成旨在损害合法用户利益或破坏协议稳定性的对抗性策略。在这项工作中，我们提出了一个借贷协议的形式化模型，该模型捕获了主流平台的关键特征，使我们能够识别并证明与其经济和战略动态相关的关键属性。", "summary": "本文针对DeFi借贷协议这一复杂的链上系统，提出了一种形式化模型。鉴于这些系统在分析其结构、经济和战略特性方面面临的挑战，尤其是由于开放参与和复杂的激励机制可能导致意想不到的行为或对抗性攻击的风险，该模型旨在捕获主流平台的关键特征，从而有助于识别和证明管理这些协议经济和战略动态的关键属性。", "keywords": "DeFi, 借贷协议, 形式化模型, 经济特性, 激励机制", "comments": "这篇论文解决了快速发展的DeFi领域的一个关键需求。通过提出一个形式化模型，它旨在对目前由于其复杂设计和去信任性质而缺乏全面理解的复杂、高价值系统进行严格分析。这种方法可以显著有助于识别漏洞、提高协议稳定性，并为DeFi中更好的设计实践提供信息。"}}
{"id": "2506.15450", "title": "SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle", "authors": ["Kun Liu", "Junhao Xiao", "Hao Lin", "Yue Cao", "Hui Peng", "Kaihong Huang", "Huimin Lu"], "summary": "Despite significant advancements in the research of aquatic-aerial robots,\nexisting configurations struggle to efficiently perform underwater, surface,\nand aerial movement simultaneously. In this paper, we propose a novel\nmultimodal surfing aquatic-aerial vehicle, SurfAAV, which efficiently\nintegrates underwater navigation, surface gliding, and aerial flying\ncapabilities. Thanks to the design of the novel differential thrust vectoring\nhydrofoil, SurfAAV can achieve efficient surface gliding and underwater\nnavigation without the need for a buoyancy adjustment system. This design\nprovides flexible operational capabilities for both surface and underwater\ntasks, enabling the robot to quickly carry out underwater monitoring\nactivities. Additionally, when it is necessary to reach another water body,\nSurfAAV can switch to aerial mode through a gliding takeoff, flying to the\ntarget water area to perform corresponding tasks. The main contribution of this\nletter lies in proposing a new solution for underwater, surface, and aerial\nmovement, designing a novel hybrid prototype concept, developing the required\ncontrol laws, and validating the robot's ability to successfully perform\nsurface gliding and gliding takeoff. SurfAAV achieves a maximum surface gliding\nspeed of 7.96 m/s and a maximum underwater speed of 3.1 m/s. The prototype's\nsurface gliding maneuverability and underwater cruising maneuverability both\nexceed those of existing aquatic-aerial vehicles.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15450v1", "AI": {"title_translation": "SurfAAV：一种新型多模态水空两栖冲浪飞行器的设计与实现", "tldr": "提出并实现了SurfAAV，一种新型水空两栖机器人，通过差分推力矢量水翼设计，实现高效水下、水面和空中移动，无需浮力调节系统，并展示了优越的冲浪滑行和水下巡航能力。", "motivation": "尽管水空两栖机器人的研究取得了显著进展，但现有配置难以同时高效地执行水下、水面和空中移动任务。", "method": "提出新型多模态水空两栖飞行器SurfAAV，它有效地整合了水下导航、水面滑行和空中飞行能力。核心设计是新颖的差分推力矢量水翼，无需浮力调节系统即可实现高效水面滑行和水下导航。当需要到达另一个水域时，SurfAAV可以通过滑翔起飞切换到空中模式。本文还开发了所需的控制律。", "result": "SurfAAV实现了7.96米/秒的最大水面滑行速度和3.1米/秒的最大水下速度。原型机的表面滑行机动性和水下巡航机动性均超过现有水空两栖飞行器。", "conclusion": "本文提出了一种新的水下、水面和空中运动解决方案，设计了一种新型混合原型概念，开发了所需的控制律，并验证了机器人成功执行水面滑行和滑翔起飞的能力。SurfAAV在水面滑行和水下巡航方面表现出优越性能。", "translation": "尽管水空两栖机器人的研究取得了显著进展，但现有配置难以同时高效地执行水下、水面和空中移动。在本文中，我们提出了一种新型多模态冲浪水空两栖飞行器SurfAAV，它有效地整合了水下导航、水面滑行和空中飞行能力。得益于新型差分推力矢量水翼的设计，SurfAAV无需浮力调节系统即可实现高效的水面滑行和水下导航。这种设计为水面和水下任务提供了灵活的操作能力，使机器人能够快速开展水下监测活动。此外，当需要到达另一个水域时，SurfAAV可以通过滑翔起飞切换到空中模式，飞往目标水域执行相应的任务。本文的主要贡献在于提出了一种新的水下、水面和空中运动解决方案，设计了一种新型混合原型概念，开发了所需的控制律，并验证了机器人成功执行水面滑行和滑翔起飞的能力。SurfAAV实现了7.96米/秒的最大水面滑行速度和3.1米/秒的最大水下速度。该原型机的表面滑行机动性和水下巡航机动性均超过现有水空两栖飞行器。", "summary": "本文提出了一种名为SurfAAV的新型多模态水空两栖飞行器，旨在解决现有水空机器人难以同时高效执行水下、水面和空中移动的问题。SurfAAV通过创新性地设计差分推力矢量水翼，实现了无需浮力调节系统的高效水面滑行和水下导航。它能够进行水下监测，并通过滑翔起飞切换到空中模式，飞往不同水域执行任务。研究验证了其水面滑行和滑翔起飞能力，并展示了其在水面滑行（7.96 m/s）和水下（3.1 m/s）速度以及机动性方面均优于现有水空两栖机器人的性能。", "keywords": "水空两栖机器人, 多模态, 差分推力矢量水翼, SurfAAV, 水面滑行", "comments": "本文的创新点在于提出了新型的差分推力矢量水翼设计，使得水空两栖机器人无需复杂的浮力调节系统即可高效地在水下和水面移动，并能实现空中飞行。这种设计显著提升了机器人的任务灵活性和效率，特别是在需要快速跨水域移动和进行水下监测的场景。其提出的混合原型概念和验证的优越性能，为未来多模态机器人设计提供了有价值的思路。"}}
{"id": "2506.15153", "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts", "authors": ["Yufei Liu", "Haoke Xiao", "Jiaxing Chai", "Yongcun Zhang", "Rong Wang", "Zijie Meng", "Zhiming Luo"], "summary": "The advent of Large Vision Models (LVMs) offers new opportunities for\nfew-shot medical image segmentation. However, existing training-free methods\nbased on LVMs fail to effectively utilize negative prompts, leading to poor\nperformance on low-contrast medical images. To address this issue, we propose\nSynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core\ninsight: improving the quality of negative prompts. To select point prompts in\na more reliable confidence map, we design a novel Confidence Map Synergy Module\nby combining the strengths of DINOv2 and SAM. Based on the confidence map, we\nselect the top-k pixels as the positive points set and choose the negative\npoints set using a Gaussian distribution, followed by independent K-means\nclustering for both sets. Then, these selected points are leveraged as\nhigh-quality prompts for SAM to get the segmentation results. Extensive\nexperiments demonstrate that SynPo achieves performance comparable to\nstate-of-the-art training-based few-shot methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15153v1", "AI": {"title_translation": "SynPo：通过高质量负提示提升免训练少样本医学图像分割", "tldr": "SynPo提出一种免训练少样本医学图像分割方法，通过结合DINOv2和SAM生成高质量正负提示，解决了现有方法在低对比度图像上负提示利用不足的问题。", "motivation": "现有基于大型视觉模型（LVMs）的免训练少样本医学图像分割方法未能有效利用负提示，导致在低对比度医学图像上表现不佳。", "method": "提出SynPo，一种基于LVMs（如SAM）的免训练少样本方法，其核心在于提高负提示的质量。设计了置信度图协同模块，结合DINOv2和SAM的优势来选择更可靠的置信度图中的点提示。基于此置信度图，选择top-k像素作为正点集，并使用高斯分布选择负点集，随后对两组点进行独立的K-means聚类。最后，利用这些高质量的点作为SAM的提示来获取分割结果。", "result": "SynPo的性能与最先进的基于训练的少样本方法相当。", "conclusion": "SynPo通过优化负提示的质量，显著提升了免训练少样本医学图像分割在低对比度图像上的性能，达到了与SOTA训练方法相当的水平。", "translation": "大型视觉模型（LVMs）的出现为少样本医学图像分割提供了新的机遇。然而，现有基于LVMs的免训练方法未能有效利用负提示，导致在低对比度医学图像上表现不佳。为解决此问题，我们提出了SynPo，一种基于LVMs（例如SAM）的免训练少样本方法，其核心洞察在于：提高负提示的质量。为了在更可靠的置信度图中选择点提示，我们设计了一个新颖的置信度图协同模块，结合了DINOv2和SAM的优势。基于该置信度图，我们选择top-k像素作为正点集，并使用高斯分布选择负点集，随后对两组点进行独立的K-means聚类。然后，这些选定的点被用作SAM的高质量提示以获得分割结果。大量实验表明，SynPo实现了与最先进的基于训练的少样本方法相当的性能。", "summary": "SynPo是一种针对免训练少样本医学图像分割的新方法，旨在解决现有LVMs方法在低对比度图像上负提示利用不足的问题。该方法通过结合DINOv2和SAM设计了一个置信度图协同模块，以生成高质量的正负点提示，并将其输入SAM进行分割。实验证明，SynPo的性能可媲美最先进的基于训练的少样本方法。", "keywords": "医学图像分割, 少样本学习, 免训练, 大型视觉模型, 负提示", "comments": "SynPo的创新点在于其免训练的特性以及对高质量负提示的有效利用，特别是在处理低对比度医学图像方面的提升。通过结合DINOv2和SAM的优势来生成更可靠的置信度图，并在此基础上进行精细的点提示选择，是其核心贡献。这为医学图像分割领域提供了一种无需大量训练数据和计算资源的有效途径。"}}
{"id": "2506.14796", "title": "PFMBench: Protein Foundation Model Benchmark", "authors": ["Zhangyang Gao", "Hao Wang", "Cheng Tan", "Chenrui Xu", "Mengdi Liu", "Bozhen Hu", "Linlin Chao", "Xiaoming Zhang", "Stan Z. Li"], "summary": "This study investigates the current landscape and future directions of\nprotein foundation model research. While recent advancements have transformed\nprotein science and engineering, the field lacks a comprehensive benchmark for\nfair evaluation and in-depth understanding. Since ESM-1B, numerous protein\nfoundation models have emerged, each with unique datasets and methodologies.\nHowever, evaluations often focus on limited tasks tailored to specific models,\nhindering insights into broader generalization and limitations. Specifically,\nresearchers struggle to understand the relationships between tasks, assess how\nwell current models perform across them, and determine the criteria in\ndeveloping new foundation models. To fill this gap, we present PFMBench, a\ncomprehensive benchmark evaluating protein foundation models across 38 tasks\nspanning 8 key areas of protein science. Through hundreds of experiments on 17\nstate-of-the-art models across 38 tasks, PFMBench reveals the inherent\ncorrelations between tasks, identifies top-performing models, and provides a\nstreamlined evaluation protocol. Code is available at\n\\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}.", "comment": null, "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.14796v1", "AI": {"title_translation": "PFMBench：蛋白质基础模型基准", "tldr": "PFMBench是一个全面评估蛋白质基础模型在38项任务上表现的基准，旨在解决当前缺乏统一评估标准的问题，揭示任务间关联并识别高性能模型。", "motivation": "当前蛋白质基础模型研究缺乏一个全面的基准，无法进行公平评估和深入理解。研究人员难以理解任务之间的关系、评估模型在不同任务上的表现，以及确定开发新基础模型的标准。", "method": "本研究提出了PFMBench，一个全面的基准，用于评估蛋白质基础模型在涵盖蛋白质科学8个关键领域的38项任务上的表现。通过在17个最先进模型上进行数百次实验，PFMBench揭示了任务固有的相关性，识别了表现最佳的模型，并提供了一个简化的评估协议。", "result": "PFMBench揭示了任务之间固有的相关性，识别了表现最佳的模型，并提供了一个简化的评估协议。", "conclusion": "PFMBench填补了蛋白质基础模型评估的空白，为公平评估和深入理解模型性能提供了全面的基准和简化的评估协议。", "translation": "本研究调查了蛋白质基础模型研究的当前格局和未来方向。尽管最近的进展已经改变了蛋白质科学和工程，但该领域缺乏一个用于公平评估和深入理解的全面基准。自ESM-1B以来，许多蛋白质基础模型已经出现，每个模型都有独特的数据集和方法。然而，评估通常只关注针对特定模型的有限任务，阻碍了对更广泛泛化和局限性的深入了解。具体而言，研究人员难以理解任务之间的关系，评估当前模型在这些任务上的表现，并确定开发新基础模型的标准。为了填补这一空白，我们提出了PFMBench，一个在蛋白质科学8个关键领域的38项任务上评估蛋白质基础模型的全面基准。通过在17个最先进模型上进行数百次实验，PFMBench揭示了任务之间固有的相关性，识别了表现最佳的模型，并提供了一个简化的评估协议。代码可在GitHub上获取。", "summary": "本研究介绍了PFMBench，一个针对蛋白质基础模型的综合性基准测试平台。鉴于当前蛋白质科学领域缺乏统一的评估标准，PFMBench旨在通过在38项任务上评估17个最先进的模型，揭示任务间的关联性，识别高性能模型，并提供标准化的评估流程，从而促进蛋白质基础模型的公平评估和深入理解。", "keywords": "蛋白质基础模型, 基准测试, PFMBench, 模型评估, 蛋白质科学", "comments": "PFMBench的创新之处在于其构建了一个全面且多任务的蛋白质基础模型评估基准，填补了当前领域缺乏统一评估标准的空白。这对于推动蛋白质科学和工程领域的发展至关重要，因为它提供了一个标准化的框架来比较和理解不同模型的性能和泛化能力。"}}
{"id": "2506.14843", "title": "CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration", "authors": ["Luca Gherardini", "Imre Lengyel", "Tunde Peto", "Caroline C. W. Klaverd", "Magda A. Meester-Smoord", "Johanna Maria Colijnd", "EYE-RISK Consortium", "E3 Consortium", "Jose Sousa"], "summary": "Machine Learning (ML) is used to tackle various tasks, such as disease\nclassification and prediction. The effectiveness of ML models relies heavily on\nhaving large amounts of complete data. However, healthcare data is often\nlimited or incomplete, which can hinder model performance. Additionally, issues\nlike the trustworthiness of solutions vary with the datasets used. The lack of\ntransparency in some ML models further complicates their understanding and use.\nIn healthcare, particularly in the case of Age-related Macular Degeneration\n(AMD), which affects millions of older adults, early diagnosis is crucial due\nto the absence of effective treatments for reversing progression. Diagnosing\nAMD involves assessing retinal images along with patients' symptom reports.\nThere is a need for classification approaches that consider genetic, dietary,\nclinical, and demographic factors. Recently, we introduced the -Comprehensive\nAbstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed\nat improving AMD stage classification. CACTUS offers explainability and\nflexibility, outperforming standard ML models. It enhances decision-making by\nidentifying key factors and providing confidence in its results. The important\nfeatures identified by CACTUS allow us to compare with existing medical\nknowledge. By eliminating less relevant or biased data, we created a clinical\nscenario for clinicians to offer feedback and address biases.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14843v1", "AI": {"title_translation": "CACTUS作为一种可靠的工具，用于早期分类年龄相关性黄斑变性", "tldr": "CACTUS是一种可解释、灵活的机器学习工具，用于早期分类年龄相关性黄斑变性(AMD)，它能克服医疗数据限制，并优于传统模型。", "motivation": "机器学习在医疗领域面临数据有限、不完整、缺乏信任度和透明度的问题。特别是对于年龄相关性黄斑变性(AMD)，由于缺乏有效治疗，早期诊断至关重要，但现有诊断方法需要更全面的考量因素。因此，需要一个能够有效分类AMD并提供可解释性的工具。", "method": "本文引入了“结构揭示的综合抽象和分类工具”(CACTUS)，旨在改进AMD阶段分类。CACTUS提供可解释性和灵活性，通过识别关键因素来增强决策制定，并消除不相关或有偏见的数据，以便临床医生提供反馈和解决偏见。", "result": "CACTUS在AMD阶段分类方面优于标准机器学习模型。它能识别出与现有医学知识相符的重要特征，并通过消除不相关数据来创建临床场景，从而帮助临床医生解决偏见，增强决策制定并提供结果置信度。", "conclusion": "CACTUS是一种可靠、可解释且灵活的工具，能够有效地进行年龄相关性黄斑变性的早期分类，并克服了医疗数据有限和模型透明度方面的挑战。", "translation": "机器学习(ML)被用于解决各种任务，例如疾病分类和预测。ML模型的有效性在很大程度上依赖于拥有大量完整数据。然而，医疗保健数据通常是有限或不完整的，这会阻碍模型性能。此外，解决方案的可靠性等问题也因所使用的数据集而异。一些ML模型缺乏透明度，这进一步使其理解和使用复杂化。在医疗保健领域，特别是在影响数百万老年人的年龄相关性黄斑变性(AMD)病例中，由于缺乏逆转疾病进展的有效治疗方法，早期诊断至关重要。诊断AMD涉及评估视网膜图像以及患者的症状报告。需要考虑遗传、饮食、临床和人口统计学因素的分类方法。最近，我们引入了——结构揭示的综合抽象和分类工具——(CACTUS)，旨在改善AMD阶段分类。CACTUS提供可解释性和灵活性，优于标准ML模型。它通过识别关键因素并提供结果置信度来增强决策制定。CACTUS识别出的重要特征使我们能够与现有医学知识进行比较。通过消除不那么相关或有偏见的数据，我们为临床医生创建了一个临床场景，以便他们提供反馈并解决偏见。", "summary": "本文介绍了CACTUS（结构揭示的综合抽象和分类工具），旨在解决医疗数据有限、机器学习模型缺乏透明度以及年龄相关性黄斑变性（AMD）早期诊断的需求。CACTUS能够改进AMD阶段分类，提供可解释性和灵活性，并且性能优于传统机器学习模型。它通过识别关键特征、提供置信度并帮助消除数据偏差，为临床决策提供支持。", "keywords": "年龄相关性黄斑变性, 机器学习, 早期诊断, 可解释性, CACTUS", "comments": "CACTUS的创新之处在于其在医疗领域，特别是AMD早期诊断中的应用，它不仅提升了分类性能，更强调了模型的“可解释性”和“灵活性”，这对于医疗决策至关重要。通过识别关键特征并与现有医学知识对比，以及处理数据偏差的能力，CACTUS在解决真实世界医疗数据挑战方面具有重要意义。"}}
{"id": "2506.15339", "title": "DeVisE: Behavioral Testing of Medical Large Language Models", "authors": ["Camila Zurdo Tagliabue", "Heloisa Oss Boll", "Aykut Erdem", "Erkut Erdem", "Iacer Calixto"], "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15339v1", "AI": {"title_translation": "DeVisE：医用大型语言模型的行为测试", "tldr": "DeVisE框架通过行为测试评估医用LLM的临床理解能力，发现零样本模型推理更连贯，而微调模型更稳定但对临床变化不敏感，强调公平性评估的重要性。", "motivation": "当前用于临床决策支持的大型语言模型（LLMs）评估方法未能有效区分真正的医学推理与表面模式，因此需要一种方法来探测细粒度的临床理解能力。", "method": "引入了DeVisE（人口统计学和生命体征评估）行为测试框架。构建了一个包含原始和基于模板的合成ICU出院记录数据集（来自MIMIC-IV），其中包含针对人口统计学（年龄、性别、种族）和生命体征属性的受控单变量反事实。在零样本和微调设置下评估了五种LLM（通用型和医学微调型）。通过输入级敏感性（反事实如何改变记录的可能性）和下游推理（它们如何影响预测的住院时间）来评估模型行为。", "result": "零样本模型表现出更连贯的反事实推理模式。微调模型倾向于更稳定，但对临床有意义的变化响应较差。人口统计学因素微妙但持续地影响模型输出。", "conclusion": "行为测试有助于揭示临床LLM的推理策略，并为设计更安全、更透明的医疗AI系统提供信息。鉴于人口统计学因素的影响，公平性评估至关重要。", "translation": "大型语言模型（LLMs）在临床决策支持中的应用日益增多，然而当前的评估方法往往无法区分真正的医学推理与表面模式。我们引入了DeVisE（人口统计学和生命体征评估），这是一个用于探测细粒度临床理解的行为测试框架。我们从MIMIC-IV构建了一个ICU出院记录数据集，生成了原始（真实世界）和基于模板（合成）版本，并带有针对人口统计学（年龄、性别、种族）和生命体征属性的受控单变量反事实。我们在零样本和微调设置下评估了五种LLM，涵盖了通用型和医学微调型变体。我们通过（1）输入级敏感性——反事实如何改变记录的可能性；以及（2）下游推理——它们如何影响预测的住院时间，来评估模型行为。我们的结果表明，零样本模型表现出更连贯的反事实推理模式，而微调模型倾向于更稳定但对临床有意义的变化响应较差。值得注意的是，人口统计学因素微妙但持续地影响输出，强调了公平性评估的重要性。这项工作突出了行为测试在揭示临床LLM推理策略和指导设计更安全、更透明的医疗AI系统方面的效用。", "summary": "本论文引入了DeVisE，一个用于评估医用大型语言模型（LLMs）细粒度临床理解的行为测试框架。该框架利用从MIMIC-IV派生的反事实数据集，评估LLM对人口统计学和生命体征变化的敏感性及其对下游推理的影响。研究结果表明，零样本模型比稳定但响应较差的微调模型表现出更连贯的推理模式，同时人口统计学因素持续影响模型输出，强调了在开发更安全的医疗AI时进行公平性评估的必要性。", "keywords": "行为测试, 医疗LLM, 反事实推理, 临床理解, 公平性", "comments": "本文的创新之处在于提出了一个带有受控反事实的行为测试框架（DeVisE），它超越了传统的评估指标，深入探究了医疗LLM的“推理”而非仅仅是性能。研究发现人口统计学因素会微妙地影响输出，这对于确保临床AI的公平性和减轻偏见至关重要，使得这项工作对负责任地开发医疗LLM具有高度重要性。"}}
{"id": "2506.15660", "title": "On the Upper Bounds for the Matrix Spectral Norm", "authors": ["Alexey Naumov", "Maxim Rakhuba", "Denis Ryapolov", "Sergey Samsonov"], "summary": "We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15660v1", "AI": {"title_translation": "关于矩阵谱范数上界的研究", "tldr": "本文提出了一种新的“平衡估计器”（Counterbalance estimator），仅通过矩阵-向量积即可提供矩阵谱范数更紧密、更可靠的上界，特别适用于谱衰减快的矩阵。", "motivation": "该研究旨在解决仅使用矩阵-向量积估计矩阵谱范数的问题，并寻求比标准方法（如幂法）更紧密、更可靠的上界。", "method": "本文提出了一种新的“平衡估计器”（Counterbalance estimator），用于提供矩阵谱范数的上界，并推导了其低估的概率保证。", "result": "所提出的估计器在合成和实际设置中都能产生比幂法等标准方法明显更紧密的上界。该方法对于谱衰减快的矩阵（如深度学习和逆问题中的矩阵）特别有效。", "conclusion": "“平衡估计器”提供了一种优越的矩阵谱范数估计方法，能够提供更紧密、更可靠的上界，尤其适用于谱衰减快的矩阵。", "translation": "我们考虑了仅使用矩阵-向量积来估计矩阵谱范数的问题。我们提出了一种新的平衡估计器（Counterbalance estimator），它提供了范数的上界，并推导了其低估的概率保证。与幂法等标准方法相比，所提出的估计器在合成和实际设置中都能产生明显更紧的上界。我们的方法对于谱衰减快的矩阵特别有效，例如深度学习和逆问题中出现的矩阵。", "summary": "本文提出了一种名为“平衡估计器”（Counterbalance estimator）的新方法，用于仅通过矩阵-向量积估计矩阵的谱范数。该估计器不仅能提供范数的上界，还附带了其低估的概率保证。与传统方法（如幂法）相比，该方法在合成和实际场景中均能产生显著更紧密、更精确的上界。它对于谱衰减快的矩阵尤其有效，这类矩阵常见于深度学习和逆问题中。", "keywords": "矩阵谱范数, 平衡估计器, 上界, 矩阵-向量积, 谱衰减", "comments": "该论文的创新之处在于提出了“平衡估计器”，该估计器在仅使用矩阵-向量积的情况下，能够提供比现有方法（如幂法）更紧密的矩阵谱范数上界，并提供了概率保证。其在处理深度学习和逆问题中常见的谱衰减快矩阵方面的有效性，显示了其在实际应用中的重要性和潜力。"}}
{"id": "2506.15518", "title": "Real-Time Initialization of Unknown Anchors for UWB-aided Navigation", "authors": ["Giulio Delama", "Igor Borowski", "Roland Jung", "Stephan Weiss"], "summary": "This paper presents a framework for the real-time initialization of unknown\nUltra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is\ndesigned for localization solutions where UWB modules act as supplementary\nsensors. Our approach enables the automatic detection and calibration of\npreviously unknown anchors during operation, removing the need for manual\nsetup. By combining an online Positional Dilution of Precision (PDOP)\nestimation, a lightweight outlier detection method, and an adaptive robust\nkernel for non-linear optimization, our approach significantly improves\nrobustness and suitability for real-world applications compared to\nstate-of-the-art. In particular, we show that our metric which triggers an\ninitialization decision is more conservative than current ones commonly based\non initial linear or non-linear initialization guesses. This allows for better\ninitialization geometry and subsequently lower initialization errors. We\ndemonstrate the proposed approach on two different mobile robots: an autonomous\nforklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry\n(VIO) framework. The results highlight the effectiveness of the proposed method\nwith robust initialization and low positioning error. We open-source our code\nin a C++ library including a ROS wrapper.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15518v1", "AI": {"title_translation": "UWB辅助导航中未知锚点的实时初始化", "tldr": "本文提出了一种实时框架，用于在UWB辅助导航系统中自动初始化未知UWB锚点，从而提高了实际应用的鲁棒性和精度。", "motivation": "现有UWB辅助导航系统需要手动设置锚点。本文旨在通过在运行期间自动检测和校准以前未知的锚点来消除这种需求，使UWB成为辅助传感器。", "method": "所提出的方法结合了在线位置精度稀释度（PDOP）估计、轻量级异常值检测方法和用于非线性优化的自适应鲁棒核。它使用比现有常用方法更保守的度量来触发初始化决策。", "result": "该方法显示出鲁棒的初始化和低定位误差。它在自主叉车和配备UWB辅助视觉惯性里程计（VIO）框架的四旋翼无人机上进行了演示。代码已开源。", "conclusion": "所提出的未知UWB锚点实时初始化框架显著提高了UWB辅助导航系统在实际应用中的鲁棒性和适用性，实现了更好的初始化几何形状和更低的误差。", "translation": "本文提出了一种用于UWB辅助导航系统中未知超宽带（UWB）锚点实时初始化的框架。该方法专为UWB模块作为辅助传感器的定位解决方案而设计。我们的方法能够在运行期间自动检测和校准以前未知的锚点，从而消除了手动设置的需要。通过结合在线位置精度稀释度（PDOP）估计、轻量级异常值检测方法和用于非线性优化的自适应鲁棒核，与现有技术相比，我们的方法显著提高了鲁棒性和对实际应用的适用性。特别是，我们表明，我们的触发初始化决策的度量比当前通常基于初始线性或非线性初始化猜测的度量更为保守。这允许更好的初始化几何形状和随后的较低初始化误差。我们在两种不同的移动机器人上演示了所提出的方法：一辆自主叉车和一架配备UWB辅助视觉惯性里程计（VIO）框架的四旋翼无人机。结果突出了所提出方法的有效性，具有鲁棒的初始化和低定位误差。我们将在C++库中开源我们的代码，包括一个ROS封装器。", "summary": "本文介绍了一种用于UWB辅助导航系统中未知超宽带（UWB）锚点初始化的新型实时框架。该方法实现了锚点的自动检测和校准，消除了手动设置的需要。它集成了在线PDOP估计、异常值检测和用于非线性优化的自适应鲁棒核，与现有技术相比，提高了鲁棒性并降低了定位误差。该方法已在移动机器人上进行了演示，证明了其有效性，并且其代码已开源。", "keywords": "UWB, 锚点初始化, 实时导航, 定位, 鲁棒优化", "comments": "本文通过自动化未知UWB锚点的初始化，提出了一个重要的创新，解决了UWB辅助导航系统的一个关键限制，并增强了其实际部署。多种技术的结合（PDOP、异常值检测、鲁棒优化）有助于其鲁棒性。代码的开源进一步增加了其潜在影响。"}}
{"id": "2506.15160", "title": "Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation", "authors": ["Jiaqi Shi", "Jin Xiao", "Xiaoguang Hu", "Boyang Song", "Hao Jiang", "Tianyou Chen", "Baochang Zhang"], "summary": "Point cloud analysis is the cornerstone of many downstream tasks, among which\naggregating local structures is the basis for understanding point cloud data.\nWhile numerous works aggregate neighbor using three-dimensional relative\ncoordinates, there are irrelevant point interference and feature hierarchy gap\nproblems due to the limitation of local coordinates. Although some works\naddress this limitation by refining spatial description though explicit\nmodeling of cross-stage structure, these enhancement methods based on direct\ngeometric structure encoding have problems of high computational overhead and\nnoise sensitivity. To overcome these problems, we propose the Point\nDistribution Set Abstraction module (PDSA) that utilizes the correlation in the\nhigh-dimensional space to correct the feature distribution during aggregation,\nwhich improves the computational efficiency and robustness. PDSA distinguishes\nthe point correlation based on a lightweight cross-stage structural descriptor,\nand enhances structural homogeneity by reducing the variance of the neighbor\nfeature matrix and increasing classes separability though long-distance\nmodeling. Additionally, we introducing a key point mechanism to optimize the\ncomputational overhead. The experimental result on semantic segmentation and\nclassification tasks based on different baselines verify the generalization of\nthe method we proposed, and achieve significant performance improvement with\nless parameter cost. The corresponding ablation and visualization results\ndemonstrate the effectiveness and rationality of our method. The code and\ntraining weight is available at: https://github.com/AGENT9717/PointDistribution", "comment": "17 papes, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15160v1", "AI": {"title_translation": "增强点云分析：基于跨阶段结构相关性的邻域聚合校正", "tldr": "提出PDSA模块，通过高维空间相关性校正特征分布，提高点云分析的计算效率和鲁棒性，并在语义分割和分类任务中表现优异。", "motivation": "现有点云分析方法在邻域聚合时存在不相关点干扰和特征层次差距问题（基于三维相对坐标），以及直接几何结构编码方法计算开销大和对噪声敏感。", "method": "提出Point Distribution Set Abstraction (PDSA) 模块，利用高维空间中的相关性在聚合过程中校正特征分布。PDSA通过轻量级跨阶段结构描述符区分点相关性，通过减少邻域特征矩阵方差和长距离建模增加类别可分性来增强结构同质性。引入关键点机制优化计算开销。", "result": "在语义分割和分类任务上，基于不同基线的实验结果验证了所提方法的泛化性，以更少的参数成本实现了显著的性能提升。消融实验和可视化结果证明了方法的有效性和合理性。", "conclusion": "所提出的PDSA模块通过有效解决现有方法的局限性，显著提高了点云分析的效率、鲁棒性和性能，证明了其在点云处理任务中的有效性和实用性。", "translation": "点云分析是许多下游任务的基石，其中聚合局部结构是理解点云数据的基础。虽然许多工作使用三维相对坐标聚合邻域，但由于局部坐标的限制，存在不相关点干扰和特征层次差距问题。尽管一些工作通过显式建模跨阶段结构来细化空间描述以解决这一限制，但这些基于直接几何结构编码的增强方法存在计算开销高和噪声敏感性问题。为了克服这些问题，我们提出了点分布集抽象模块（PDSA），它利用高维空间中的相关性来校正聚合过程中的特征分布，从而提高计算效率和鲁棒性。PDSA基于轻量级跨阶段结构描述符区分点相关性，并通过减少邻域特征矩阵的方差和通过长距离建模增加类别可分性来增强结构同质性。此外，我们引入了关键点机制来优化计算开销。基于不同基线在语义分割和分类任务上的实验结果验证了我们提出的方法的泛化性，并以更少的参数成本实现了显著的性能提升。相应的消融实验和可视化结果证明了我们方法的有效性和合理性。代码和训练权重可在：https://github.com/AGENT9717/PointDistribution 获取。", "summary": "本文针对点云分析中邻域聚合的挑战，包括不相关点干扰、特征层次差距、高计算开销和噪声敏感性，提出了一种名为点分布集抽象（PDSA）的新模块。PDSA通过在高维空间中校正特征分布来提高计算效率和鲁棒性，并利用轻量级跨阶段结构描述符区分点相关性，同时通过方差减少和长距离建模增强结构同质性。实验证明，PDSA在语义分割和分类任务上实现了显著的性能提升和更低的参数成本。", "keywords": "点云分析, 邻域聚合, 特征校正, 跨阶段结构, PDSA", "comments": "该论文提出了一种创新的邻域聚合校正方法PDSA，通过在高维空间中处理特征分布和引入跨阶段结构相关性，有效解决了传统点云分析方法在效率、鲁棒性和噪声敏感性方面的局限。其引入的关键点机制优化了计算开销，使得方法在性能提升的同时也具有较高的实用价值。"}}
{"id": "2506.14895", "title": "Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification", "authors": ["Jenni Raitoharju"], "summary": "This paper focuses on small-scale one-class classification with some negative\nsamples available. We propose Generalized Reference Kernel with Negative\nSamples (GRKneg) for One-class Support Vector Machine (OC-SVM). We study\ndifferent ways to select/generate the reference vectors and recommend an\napproach for the problem at hand. It is worth noting that the proposed method\ndoes not use any labels in the model optimization but uses the original OC-SVM\nimplementation. Only the kernel used in the process is improved using the\nnegative data. We compare our method with the standard OC-SVM and with the\nbinary Support Vector Machine (SVM) using different amounts of negative\nsamples. Our approach consistently outperforms the standard OC-SVM using Radial\nBasis Function kernel. When there are plenty of negative samples, the binary\nSVM outperforms the one-class approaches as expected, but we show that for the\nlowest numbers of negative samples the proposed approach clearly outperforms\nthe binary SVM.", "comment": "Accepted to EUSIPCO2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14895v1", "AI": {"title_translation": "广义参考核与负样本支持向量一类分类", "tldr": "针对小规模一类分类问题，本文提出一种使用负样本的广义参考核（GRKneg）来改进单类支持向量机（OC-SVM），在负样本量少时表现优于标准OC-SVM和二分类SVM。", "motivation": "解决小规模一类分类问题，尤其是在有少量负样本可用的情况下，希望改进现有的一类支持向量机（OC-SVM）的性能。", "method": "提出广义参考核与负样本（GRKneg）方法用于一类支持向量机（OC-SVM）。该方法通过使用负数据改进核函数，但在模型优化时不使用任何标签，并研究了不同的参考向量选择/生成方法。", "result": "提出的方法始终优于使用径向基函数核的标准OC-SVM。在负样本数量最少时，该方法明显优于二分类SVM，但在负样本充足时，二分类SVM表现更好。", "conclusion": "GRKneg在负样本稀缺的小规模一类分类问题中，能够有效提升OC-SVM的性能，并优于二分类SVM。", "translation": "本文关注小规模一类分类问题，尤其是在有少量负样本可用的情况下。我们提出了一种用于一类支持向量机（OC-SVM）的带有负样本的广义参考核（GRKneg）。我们研究了选择/生成参考向量的不同方法，并推荐了一种适用于当前问题的方法。值得注意的是，所提出的方法在模型优化中不使用任何标签，但使用了原始的OC-SVM实现。在此过程中仅使用负数据改进了核函数。我们将我们的方法与标准OC-SVM以及使用不同数量负样本的二分类支持向量机（SVM）进行了比较。我们的方法始终优于使用径向基函数核的标准OC-SVM。当负样本充足时，二分类SVM表现优于一类方法，这符合预期，但我们表明，在负样本数量最少时，所提出的方法明显优于二分类SVM。", "summary": "本文针对存在少量负样本的小规模一类分类问题，提出了一种名为GRKneg的广义参考核方法，用于改进传统的一类支持向量机（OC-SVM）。该方法通过负样本优化核函数，但在模型优化时不依赖标签。实验结果表明，与标准OC-SVM相比，GRKneg表现出一致的性能提升。特别是在负样本数量较少的情况下，GRKneg显著优于二分类支持向量机（SVM）。", "keywords": "一类分类, 支持向量机, 负样本, 广义参考核, 小规模分类", "comments": "该论文的创新点在于提出了一种在OC-SVM中有效利用少量负样本的核函数改进方法，避免了在优化过程中使用标签，这对于数据稀疏或标签获取困难的场景具有重要意义。其贡献在于证明了在负样本有限的情况下，改进的一类分类器可以优于传统的二分类方法。"}}
{"id": "2506.15355", "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Sriparna Saha"], "summary": "Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15355v1", "AI": {"title_translation": "SANSKRITI：一个评估语言模型印度文化知识的综合基准", "tldr": "SANSKRITI是一个大型基准数据集，用于评估语言模型对印度文化的理解，发现现有模型在处理文化细微差别方面存在显著差异。", "motivation": "语言模型（LMs）的全球有效性依赖于对当地社会文化背景的理解。为了解决现有模型在印度文化理解方面的不足，本文引入了一个新的基准。", "method": "本文引入了SANSKRITI，一个包含21,853个精心策划的问答对的基准数据集，涵盖印度28个邦和8个联邦属地的16个关键文化属性。研究者使用SANSKRITI评估了领先的大型语言模型（LLMs）、印度语言模型（ILMs）和小型语言模型（SLMs）。", "result": "评估结果显示，现有语言模型在处理具有文化细微差别的查询方面存在显著差异，许多模型在特定区域背景下表现不佳。", "conclusion": "SANSKRITI通过提供一个广泛、文化丰富且多样化的数据集，为评估和改进语言模型的文化理解设定了新标准。", "translation": "语言模型（LMs）是塑造现代工作流程不可或缺的工具，但其全球有效性取决于对当地社会文化背景的理解。为了解决这个问题，我们引入了SANSKRITI，一个旨在评估语言模型对印度丰富文化多样性理解的基准。SANSKRITI包含21,853个精心策划的问答对，涵盖28个邦和8个联邦属地，是测试印度文化知识的最大数据集。它涵盖了印度文化的16个关键属性：仪式典礼、历史、旅游、美食、舞蹈和音乐、服饰、语言、艺术、节日、宗教、医药、交通、体育、夜生活和人物，全面展现了印度的文化图景。我们使用SANSKRITI评估了领先的大型语言模型（LLMs）、印度语言模型（ILMs）和小型语言模型（SLMs），揭示了它们在处理具有文化细微差别的查询方面的显著差异，许多模型在特定区域背景下表现不佳。通过提供一个广泛、文化丰富且多样化的数据集，SANSKRITI为评估和改进语言模型的文化理解设定了新标准。", "summary": "本文介绍了SANSKRITI，一个专门用于评估语言模型对印度文化理解的综合基准。该基准包含21,853个问答对，覆盖印度28个邦和8个联邦属地的16个文化领域，是目前最大的印度文化知识数据集。通过对主流语言模型进行评估，研究发现现有模型在处理文化细节和区域特定背景时存在明显不足。SANSKRITI的推出旨在为提升语言模型的文化理解能力设立新的评估标准。", "keywords": "语言模型, 印度文化, 基准测试, SANSKRITI, 文化理解", "comments": "SANSKRITI的创新之处在于其作为首个大规模、综合性、专门针对印度文化知识的基准数据集。它填补了现有语言模型在非西方文化背景下知识评估的空白，对于促进语言模型的全球适用性和文化公平性具有重要意义。该数据集的细致分类和广泛覆盖范围，使其成为评估和改进模型文化理解能力的宝贵资源。"}}
{"id": "2506.14822", "title": "Analysis and conditional optimization of projection estimates for the distribution of random variable using Legendre polynomials", "authors": ["Tatyana A. Averina", "Konstantin A. Rybakov"], "summary": "Algorithms for jointly obtaining projection estimates of the density and\ndistribution function of a random variable using the Legendre polynomials are\nproposed. For these algorithms, a problem of the conditional optimization is\nsolved. Such an optimization allows one increasing the approximation accuracy\nwith a minimum computational costs. The proposed algorithms are tested on\nexamples with different degree of smoothness of the density.", "comment": null, "cate": "stat.CO", "url": "http://arxiv.org/abs/2506.14822v1", "AI": {"title_translation": "随机变量分布投影估计的勒让德多项式分析与条件优化", "tldr": "提出了使用勒让德多项式对随机变量密度和分布函数进行投影估计的算法，并通过条件优化提高了精度并降低了计算成本。", "motivation": "旨在提高使用勒让德多项式进行随机变量密度和分布函数投影估计的近似精度，同时保持最低的计算成本。", "method": "提出了使用勒让德多项式联合获取随机变量密度和分布函数投影估计的算法，并通过解决条件优化问题来提高近似精度。", "result": "提出的算法通过条件优化，能够在最小计算成本下提高近似精度。这些算法在不同平滑度密度的例子上进行了测试。", "conclusion": "提出的基于勒让德多项式的随机变量密度和分布函数投影估计算法，通过条件优化能够有效提高近似精度并控制计算成本。", "translation": "提出了使用勒让德多项式联合获取随机变量密度和分布函数投影估计的算法。针对这些算法，解决了条件优化问题。这种优化能够以最小的计算成本提高近似精度。所提出的算法在具有不同密度平滑度的示例上进行了测试。", "summary": "本文提出了一系列利用勒让德多项式对随机变量的密度和分布函数进行投影估计的算法。为了提升估计的准确性并控制计算开销，研究解决了相关的条件优化问题。实验结果表明，这些算法能够在不同平滑度的密度函数上有效地提高近似精度。", "keywords": "勒让德多项式, 投影估计, 条件优化, 随机变量分布, 密度估计", "comments": "这项研究通过引入条件优化，有效解决了基于勒让德多项式的投影估计在提高精度和降低计算成本之间的平衡问题，具有一定的实用价值。"}}
{"id": "2506.15539", "title": "Aerial Grasping via Maximizing Delta-Arm Workspace Utilization", "authors": ["Haoran Chen", "Weiliang Deng", "Biyu Ye", "Yifan Xiong", "Ximin Lyu"], "summary": "The workspace limits the operational capabilities and range of motion for the\nsystems with robotic arms. Maximizing workspace utilization has the potential\nto provide more optimal solutions for aerial manipulation tasks, increasing the\nsystem's flexibility and operational efficiency. In this paper, we introduce a\nnovel planning framework for aerial grasping that maximizes workspace\nutilization. We formulate an optimization problem to optimize the aerial\nmanipulator's trajectory, incorporating task constraints to achieve efficient\nmanipulation. To address the challenge of incorporating the delta arm's\nnon-convex workspace into optimization constraints, we leverage a Multilayer\nPerceptron (MLP) to map position points to feasibility\nprobabilities.Furthermore, we employ Reversible Residual Networks (RevNet) to\napproximate the complex forward kinematics of the delta arm, utilizing\nefficient model gradients to eliminate workspace constraints. We validate our\nmethods in simulations and real-world experiments to demonstrate their\neffectiveness.", "comment": "8 pages, 7 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15539v1", "AI": {"title_translation": "通过最大化Delta臂工作空间利用率的空中抓取", "tldr": "本文提出了一种新的空中抓取规划框架，通过使用MLP处理非凸工作空间和RevNet近似正向运动学来最大化Delta臂的工作空间利用率。", "motivation": "机器人手臂系统的工作空间限制了其操作能力和运动范围。最大化工作空间利用率可以为空中操作任务提供更优的解决方案，从而提高系统的灵活性和操作效率。", "method": "本文引入了一种新颖的空中抓取规划框架，旨在最大化工作空间利用率。该框架通过制定一个优化问题来优化空中机械手的轨迹，并结合任务约束以实现高效操作。为解决Delta臂非凸工作空间纳入优化约束的挑战，研究人员利用多层感知器（MLP）将位置点映射到可行性概率。此外，还采用可逆残差网络（RevNet）来近似Delta臂复杂的正向运动学，并利用高效的模型梯度消除工作空间约束。", "result": "该方法在仿真和实际实验中得到了验证，证明了其有效性。", "conclusion": "本文提出的通过最大化Delta臂工作空间利用率的空中抓取方法是有效的。", "translation": "工作空间限制了机器人手臂系统的操作能力和运动范围。最大化工作空间利用率有可能为空中操作任务提供更优的解决方案，从而提高系统的灵活性和操作效率。在本文中，我们引入了一种新颖的空中抓取规划框架，该框架最大化了工作空间利用率。我们制定了一个优化问题，以优化空中机械手的轨迹，并结合任务约束以实现高效操作。为了解决将Delta臂的非凸工作空间纳入优化约束的挑战，我们利用多层感知器（MLP）将位置点映射到可行性概率。此外，我们采用可逆残差网络（RevNet）来近似Delta臂复杂的正向运动学，利用高效的模型梯度来消除工作空间约束。我们在仿真和实际实验中验证了我们的方法，以证明其有效性。", "summary": "本文提出了一种新颖的空中抓取规划框架，旨在最大化Delta臂的工作空间利用率。该框架通过优化空中机械手轨迹并结合任务约束来实现高效操作。为解决Delta臂非凸工作空间的挑战，研究利用多层感知器（MLP）进行可行性映射，并使用可逆残差网络（RevNet）近似正向运动学以消除工作空间约束。该方法已在仿真和实际实验中得到验证。", "keywords": "空中抓取, 工作空间利用, Delta臂, 优化, 机器学习", "comments": "该论文创新性地利用MLP和RevNet来解决Delta臂非凸工作空间与优化问题结合的挑战，这对于提高空中操作的效率和灵活性至关重要。"}}
{"id": "2506.15166", "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography", "authors": ["Abdur Rahman", "Keerthiveena Balraj", "Manojkumar Ramteke", "Anurag Singh Rathore"], "summary": "Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND", "comment": "Version of record published in Discover Applied Sciences (Springer\n  Nature). The definitive article is available at\n  https://doi.org/10.1007/s42452-025-07055-5", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15166v1", "AI": {"title_translation": "Echo-DND：一种用于超声心动图左心室分割的鲁棒精确双噪声扩散模型", "tldr": "Echo-DND是一种新的双噪声扩散模型，通过结合高斯和伯努利噪声、多尺度融合以及空间一致性校准，显著提高了超声心动图中左心室分割的精度和鲁棒性。", "motivation": "超声心动图中的左心室（LV）准确分割对于诊断和治疗至关重要，但超声图像固有的噪声大、对比度低、LV边界模糊等问题使得分割过程复杂化。", "method": "本文引入了Echo-DND，这是一种新颖的双噪声扩散模型，专门用于左心室分割。它结合了高斯噪声和伯努利噪声，并融入了多尺度融合条件模块以提高分割精度，同时利用空间相干性校准来保持分割掩膜的空间完整性。", "result": "Echo-DND模型在CAMUS和EchoNet-Dynamic数据集上进行了严格验证，分别取得了0.962和0.939的高Dice分数，性能优于现有的SOTA模型。", "conclusion": "Echo-DND模型在超声心动图分割领域树立了新标准，其架构有望在其他医学成像任务中得到更广泛的应用，从而可能提高各种医学领域的诊断准确性。", "translation": "扩散概率模型（DPMs）的最新进展彻底改变了图像处理，在医疗应用中展现出巨大潜力。超声心动图中左心室（LV）的精确分割对于诊断程序和必要的治疗至关重要。然而，超声图像以其高噪声、低对比度和模糊的LV边界而闻名，从而使分割过程复杂化。为了应对这些挑战，本文引入了Echo-DND，这是一种专门为此任务设计的新型双噪声扩散模型。Echo-DND利用高斯噪声和伯努利噪声的独特组合。它还结合了多尺度融合条件模块以提高分割精度。此外，它利用空间相干性校准来保持分割掩膜的空间完整性。该模型的性能在CAMUS和EchoNet-Dynamic数据集上得到了严格验证。广泛的评估表明，所提出的框架优于现有的SOTA模型。它在这些数据集上分别获得了0.962和0.939的高Dice分数。所提出的Echo-DND模型在超声心动图分割方面建立了新标准，其架构有望在其他医学成像任务中得到更广泛的应用，可能提高各种医学领域的诊断准确性。项目页面：https://abdur75648.github.io/Echo-DND", "summary": "Echo-DND是一种新型双噪声扩散模型，专为解决超声心动图左心室（LV）分割中高噪声、低对比度及模糊边界的挑战而设计。该模型独特地结合了高斯和伯努利噪声，并整合了多尺度融合条件模块以提升精度，同时通过空间相干性校准保持分割掩膜的空间完整性。在CAMUS和EchoNet-Dynamic数据集上的验证结果显示，Echo-DND在Dice分数上表现出色（分别为0.962和0.939），超越了现有最先进模型，为超声心动图分割树立了新标准，并有望扩展至其他医学影像应用。", "keywords": "双噪声扩散模型, 左心室分割, 超声心动图, 图像处理, 医学影像", "comments": "Echo-DND的创新之处在于其双噪声扩散模型的设计，结合高斯和伯努利噪声，以及多尺度融合和空间相干性校准，有效解决了超声图像固有的挑战。其在医学图像分割领域的卓越性能，特别是超越现有SOTA模型，显示了其重要性。该模型为未来在其他医学影像任务中的应用提供了广阔前景。"}}
{"id": "2506.14798", "title": "MODS: Multi-source Observations Conditional Diffusion Model for Meteorological State Downscaling", "authors": ["Siwei Tu", "Jingyi Xu", "Weidong Yang", "Lei Bai", "Ben Fei"], "summary": "Accurate acquisition of high-resolution surface meteorological conditions is\ncritical for forecasting and simulating meteorological variables. Directly\napplying spatial interpolation methods to derive meteorological values at\nspecific locations from low-resolution grid fields often yields results that\ndeviate significantly from the actual conditions. Existing downscaling methods\nprimarily rely on the coupling relationship between geostationary satellites\nand ERA5 variables as a condition. However, using brightness temperature data\nfrom geostationary satellites alone fails to comprehensively capture all the\nchanges in meteorological variables in ERA5 maps. To address this limitation,\nwe can use a wider range of satellite data to make more full use of its\ninversion effects on various meteorological variables, thus producing more\nrealistic results across different meteorological variables. To further improve\nthe accuracy of downscaling meteorological variables at any location, we\npropose the Multi-source Observation Down-Scaling Model (MODS). It is a\nconditional diffusion model that fuses data from multiple geostationary\nsatellites GridSat, polar-orbiting satellites (AMSU-A, HIRS, and MHS), and\ntopographic data (GEBCO), as conditions, and is pre-trained on the ERA5\nreanalysis dataset. During training, latent features from diverse conditional\ninputs are extracted separately and fused into ERA5 maps via a multi-source\ncross-attention module. By exploiting the inversion relationships between\nreanalysis data and multi-source atmospheric variables, MODS generates\natmospheric states that align more closely with real-world conditions. During\nsampling, MODS enhances downscaling consistency by incorporating low-resolution\nERA5 maps and station-level meteorological data as guidance. Experimental\nresults demonstrate that MODS achieves higher fidelity when downscaling ERA5\nmaps to a 6.25 km resolution.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.14798v1", "AI": {"title_translation": "MODS：多源观测条件扩散模型用于气象状态降尺度", "tldr": "MODS是一个条件扩散模型，它融合了多源卫星和地形数据，用于将气象状态从低分辨率ERA5地图降尺度到高分辨率，显著提高了气象变量的准确性和真实性。", "motivation": "准确获取高分辨率地表气象条件对气象预报和模拟至关重要。直接空间插值低分辨率网格数据往往与实际情况偏差大。现有降尺度方法主要依赖静止卫星和ERA5变量的耦合关系，但仅使用静止卫星亮度温度数据无法全面捕捉ERA5图中气象变量的所有变化。因此，需要利用更广泛的卫星数据来更充分地利用其对各种气象变量的反演效果，以产生更真实的结果。", "method": "本文提出了多源观测降尺度模型（MODS），这是一个条件扩散模型。它融合了来自多个地球静止卫星（GridSat）、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）作为条件，并在ERA5再分析数据集上进行预训练。在训练过程中，从不同的条件输入中单独提取潜在特征，并通过一个多源交叉注意力模块融合到ERA5地图中。通过利用再分析数据和多源大气变量之间的反演关系，MODS生成与真实世界条件更接近的大气状态。在采样过程中，MODS通过结合低分辨率ERA5地图和站点级气象数据作为指导，增强了降尺度的一致性。", "result": "实验结果表明，MODS在将ERA5地图降尺度到6.25公里分辨率时，实现了更高的保真度。", "conclusion": "MODS通过融合多源观测数据和采用条件扩散模型，能够将ERA5地图有效降尺度至更高分辨率，生成更接近真实条件的气象状态，从而显著提高了气象变量降尺度的准确性和保真度。", "translation": "准确获取高分辨率地表气象条件对于预报和模拟气象变量至关重要。直接将空间插值方法应用于从低分辨率网格场推导特定位置的气象值，通常会产生与实际情况显著偏离的结果。现有的降尺度方法主要依赖于地球静止卫星和ERA5变量之间的耦合关系作为条件。然而，仅使用地球静止卫星的亮度温度数据无法全面捕捉ERA5图中气象变量的所有变化。为了解决这一限制，我们可以使用更广泛的卫星数据，以更充分地利用其对各种气象变量的反演效果，从而在不同的气象变量中产生更真实的结果。为了进一步提高任意位置气象变量降尺度的准确性，我们提出了多源观测降尺度模型（MODS）。它是一个条件扩散模型，融合了来自多个地球静止卫星GridSat、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）作为条件，并在ERA5再分析数据集上进行预训练。在训练过程中，来自不同条件输入的潜在特征被单独提取，并通过多源交叉注意力模块融合到ERA5地图中。通过利用再分析数据和多源大气变量之间的反演关系，MODS生成与真实世界条件更接近的大气状态。在采样过程中，MODS通过结合低分辨率ERA5地图和站点级气象数据作为指导，增强了降尺度的一致性。实验结果表明，MODS在将ERA5地图降尺度到6.25公里分辨率时，实现了更高的保真度。", "summary": "本研究提出了一种名为MODS（多源观测降尺度模型）的条件扩散模型，旨在解决现有气象变量降尺度方法中分辨率不足和数据利用不全面的问题。针对现有方法仅依赖单一静止卫星数据导致无法全面捕捉气象变量变化的局限性，MODS创新性地融合了多源地球静止卫星、极轨卫星以及地形数据作为条件输入。该模型在ERA5再分析数据集上进行预训练，并通过多源交叉注意力模块将不同条件输入的潜在特征融合到ERA5地图中，以生成更接近真实情况的气象状态。此外，MODS在采样阶段引入低分辨率ERA5地图和站点级气象数据作为指导，进一步提升了降尺度的一致性。实验结果验证了MODS在将ERA5地图降尺度到6.25公里分辨率时具有更高的保真度。", "keywords": "多源观测, 条件扩散模型, 气象降尺度, ERA5, 卫星数据", "comments": "MODS模型通过融合多源卫星和地形数据作为条件输入，显著提升了气象变量降尺度的准确性，这在现有方法中是重要的创新点。其采用的条件扩散模型和多源交叉注意力模块，有效解决了单一数据源无法全面捕捉气象变量变化的局限性。此外，在采样阶段引入低分辨率ERA5地图和站点级气象数据进行指导，进一步增强了模型在实际应用中的鲁棒性和一致性。这项工作对于高分辨率气象预报和模拟具有重要意义。"}}
{"id": "2506.15372", "title": "COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation", "authors": ["Raghvendra Kumar", "S. A. Mohammed Salman", "Aryan Sahu", "Tridib Nandi", "Pragathi Y. P.", "Sriparna Saha", "Jose G. Moreno"], "summary": "Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.", "comment": "ACL 2025 MAINs", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15372v1", "AI": {"title_translation": "COSMMIC：面向摘要和标题生成的评论敏感多模态多语言印度语语料库", "tldr": "COSMMIC是一个新的多模态、多语言印度语语料库，包含文章、图片和用户评论，用于改进印度语的摘要和标题生成任务。", "motivation": "尽管在英语和中文的评论感知多模态和多语言摘要方面取得了进展，但印度语言的研究仍然有限。本研究旨在通过引入COSMMIC数据集来弥补这一空白。", "method": "本研究引入了COSMMIC数据集，这是一个评论敏感的多模态、多语言数据集，涵盖九种主要的印度语言。该数据集包含4,959对文章-图像和24,484条读者评论。研究探索了四种配置下的摘要和标题生成：仅使用文章文本、结合用户评论、利用图像，以及结合文本、评论和图像。为评估数据集的有效性，研究采用了LLama3和GPT-4等先进语言模型，并进行了全面研究以评估不同组件组合，包括识别支持性评论、使用IndicBERT的评论分类器过滤噪音，以及使用基于CLIP的多语言分类器从图像中提取有价值的见解。", "result": "本研究创建并引入了COSMMIC数据集，一个独特地整合了文本、图像和用户反馈的多模态、多语言印度语语料库。通过对不同组件组合的评估，确定了对自然语言生成（NLG）任务最有效的配置。", "conclusion": "COSMMIC数据集独特地整合了文本、图像和用户反馈，这弥补了印度语言资源方面的空白，推动了自然语言处理（NLP）研究并促进了包容性。", "translation": "尽管在评论感知多模态和多语言摘要方面取得了进展，但针对英语和中文的研究仍在进行中，而印度语言的研究仍然有限。本研究通过引入COSMMIC来弥补这一空白，这是一个开创性的评论敏感、多模态、多语言数据集，包含九种主要的印度语言。COSMMIC包含4,959对文章-图像和24,484条读者评论，所有包含的语言都提供了真实摘要。我们的方法通过整合读者见解和反馈来增强摘要。我们探索了四种配置下的摘要和标题生成：(1) 仅使用文章文本，(2) 结合用户评论，(3) 利用图像，以及 (4) 结合文本、评论和图像。为了评估数据集的有效性，我们采用了LLama3和GPT-4等最先进的语言模型。我们进行了一项全面的研究，以评估不同的组件组合，包括识别支持性评论，使用专门的IndicBERT评论分类器过滤噪音，以及使用基于CLIP的多语言分类器从图像中提取有价值的见解。这有助于确定自然语言生成（NLG）任务最有效的配置。与许多现有数据集不同，这些数据集要么仅限于文本，要么在多模态设置中缺乏用户评论，COSMMIC独特地整合了文本、图像和用户反馈。这种全面的方法弥补了印度语言资源方面的空白，推动了NLP研究并促进了包容性。", "summary": "本研究介绍了COSMMIC，一个创新的评论敏感多模态多语言印度语语料库，旨在解决印度语言在摘要和标题生成方面研究的不足。该数据集包含文章、图像和用户评论，涵盖九种印度主要语言。研究通过结合文本、评论和图像等多种输入配置，利用LLama3、GPT-4等先进模型，并结合评论分类和图像特征提取技术，评估了不同配置对NLG任务的有效性。COSMMIC的独特之处在于其全面整合了文本、图像和用户反馈，为印度语言的NLP研究提供了宝贵的资源。", "keywords": "COSMMIC, 印度语, 摘要生成, 标题生成, 多模态语料库", "comments": "本论文的创新之处在于构建了一个独特的、评论敏感的多模态多语言印度语语料库COSMMIC，填补了印度语言在这一领域的研究空白。它不仅包含了文本和图像，还创造性地整合了用户评论，为自然语言生成任务提供了更丰富的上下文信息。这种方法有望显著提升印度语言摘要和标题生成的质量和鲁棒性，对促进印度语言的NLP发展具有重要意义。"}}
{"id": "2506.15349", "title": "Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference", "authors": ["Terrance Liu", "Matteo Boglioni", "Yiwei Fu", "Shengyuan Hu", "Pratiksha Thaker", "Zhiwei Steven Wu"], "summary": "Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15349v1", "AI": {"title_translation": "使用分位数回归的成员推断增强单次运行隐私审计", "tldr": "本文提出了一种利用分位数回归进行成员推断攻击的方法，以改进黑盒设置下的单次运行隐私审计，从而在保持计算效率的同时获得更紧密的隐私下限。", "motivation": "现有的差分隐私（DP）审计技术在黑盒设置下，经验下限与理论上限之间仍存在较大差距，且一些方法需要昂贵的多次训练运行。尽管近期有计算高效的单次运行审计方法，但在实际的黑盒设置中仍有改进空间。", "method": "本文研究了如何结合更强的成员推断攻击（MIA）方法来改进黑盒设置下的单次运行隐私审计。具体地，提出了一种利用分位数回归进行MIA的方法。", "result": "在CIFAR-10上训练的图像分类模型上使用DP-SGD进行评估，所提出的方法实现了更紧密的隐私下限，同时关键地保持了单次运行方法的计算效率。", "conclusion": "通过将分位数回归应用于成员推断攻击，可以有效提升黑盒设置下差分隐私机制的单次运行审计效果，获得更精确的隐私下限，同时不牺牲计算效率。", "translation": "差分隐私（DP）审计旨在为DP-SGD等DP机制的隐私保障提供经验下限。虽然一些现有技术需要大量训练运行，成本过高，但最近的工作引入了单次运行审计方法，可以在白盒设置下有效地审计DP-SGD，同时保持计算效率。然而，在更实际的黑盒设置中，训练期间无法操纵梯度，并且只观察到最后一个模型迭代，先前的研究表明，经验下限和理论上限之间仍然存在很大差距。因此，在这项工作中，我们研究了如何结合更强的成员推断攻击（MIA）方法来改进黑盒设置下的单次运行审计。在CIFAR-10上使用DP-SGD训练的图像分类模型上进行评估，我们证明了我们提出的方法（利用分位数回归进行MIA）在保持单次运行方法计算效率的关键同时，实现了更紧密的界限。", "summary": "本文针对差分隐私（DP）审计在黑盒设置下经验下限与理论上限存在较大差距的问题，提出了一种改进单次运行审计的方法。该方法通过引入基于分位数回归的更强成员推断攻击（MIA），在不牺牲计算效率的前提下，显著收紧了DP机制的隐私下限。实验证明，该方法在CIFAR-10数据集上训练的图像分类模型上表现出优越性。", "keywords": "差分隐私审计, 成员推断攻击, 分位数回归, 黑盒设置, 隐私保障", "comments": "这项工作的创新之处在于将分位数回归应用于成员推断攻击，以提升黑盒DP审计的精确性。其重要性在于，它在维持计算效率的同时，缩小了经验隐私下限与理论上限之间的差距，这对于实际应用中评估DP机制的隐私保障具有重要意义。该方法对于那些需要精确隐私评估但又受限于计算资源的场景尤其有价值。"}}
{"id": "2506.15607", "title": "GRIM: Task-Oriented Grasping with Conditioning on Generative Examples", "authors": ["Shailesh", "Alok Raj", "Nayan Kumar", "Priya Shukla", "Andrew Melnik", "Micheal Beetz", "Gora Chand Nandi"], "summary": "Task-Oriented Grasping (TOG) presents a significant challenge, requiring a\nnuanced understanding of task semantics, object affordances, and the functional\nconstraints dictating how an object should be grasped for a specific task. To\naddress these challenges, we introduce GRIM (Grasp Re-alignment via Iterative\nMatching), a novel training-free framework for task-oriented grasping.\nInitially, a coarse alignment strategy is developed using a combination of\ngeometric cues and principal component analysis (PCA)-reduced DINO features for\nsimilarity scoring. Subsequently, the full grasp pose associated with the\nretrieved memory instance is transferred to the aligned scene object and\nfurther refined against a set of task-agnostic, geometrically stable grasps\ngenerated for the scene object, prioritizing task compatibility. In contrast to\nexisting learning-based methods, GRIM demonstrates strong generalization\ncapabilities, achieving robust performance with only a small number of\nconditioning examples.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15607v1", "AI": {"title_translation": "GRIM：基于生成示例条件化的任务导向抓取", "tldr": "GRIM是一个无需训练的任务导向抓取框架，通过几何线索和DINO特征进行粗略对齐，然后将记忆实例的抓取姿态转移并精炼，实现强大的泛化能力，只需少量条件示例。", "motivation": "任务导向抓取（TOG）面临巨大挑战，需要对任务语义、物体功能和抓取的功能约束有细致的理解。", "method": "GRIM（通过迭代匹配进行抓取重新对齐）是一个新颖的、无需训练的任务导向抓取框架。它首先使用几何线索和PCA降维的DINO特征组合进行相似度评分，以实现粗略对齐。随后，将检索到的记忆实例的完整抓取姿态转移到对齐的场景物体上，并针对一组任务无关的、几何稳定的抓取进行进一步精炼，优先考虑任务兼容性。", "result": "GRIM展现出强大的泛化能力，仅需少量条件示例即可实现鲁棒性能。", "conclusion": "GRIM是一个新颖的、无需训练的任务导向抓取框架，通过生成示例条件化，解决了现有学习方法的泛化问题，实现了鲁棒且高效的抓取。", "translation": "任务导向抓取（TOG）提出了一个重大挑战，需要对任务语义、物体功能以及决定物体如何被抓取以完成特定任务的功能约束有细致的理解。为了应对这些挑战，我们引入了GRIM（Grasp Re-alignment via Iterative Matching），一个新颖的、无需训练的任务导向抓取框架。最初，开发了一种粗略对齐策略，利用几何线索和主成分分析（PCA）降维的DINO特征组合进行相似度评分。随后，与检索到的记忆实例相关的完整抓取姿态被转移到对齐的场景物体上，并针对一组为场景物体生成的任务无关的、几何稳定的抓取进行进一步精炼，优先考虑任务兼容性。与现有基于学习的方法相比，GRIM展示了强大的泛化能力，仅需少量条件示例即可实现鲁棒性能。", "summary": "本文介绍了一个名为GRIM（Grasp Re-alignment via Iterative Matching）的无需训练框架，用于解决任务导向抓取（TOG）的挑战。GRIM通过结合几何线索和PCA降维的DINO特征进行粗略对齐，随后将检索到的记忆实例的抓取姿态转移并精炼，以实现任务兼容性。该方法与现有学习方法不同，展现出强大的泛化能力，仅需少量条件示例即可实现鲁棒的抓取性能。", "keywords": "任务导向抓取, 无需训练, 抓取姿态, 泛化能力, DINO特征", "comments": "GRIM的创新之处在于其无需训练的框架，这与现有的学习型方法形成对比，并显著提升了泛化能力。它通过巧妙结合几何特征和DINO特征进行对齐，并通过记忆实例和精炼过程实现抓取，解决了任务导向抓取中对大量训练数据的依赖问题，只需少量条件示例即可表现出色。"}}
{"id": "2506.15180", "title": "ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections", "authors": ["Ziling Huang", "Yidan Zhang", "Shin'ichi Satoh"], "summary": "Large-scale visual search engines are expected to solve a dual problem at\nonce: (i) locate every image that truly contains the object described by a\nsentence and (ii) identify the object's bounding box or exact pixels within\neach hit. Existing techniques address only one side of this challenge. Visual\ngrounding yields tight boxes and masks but rests on the unrealistic assumption\nthat the object is present in every test image, producing a flood of false\nalarms when applied to web-scale collections. Text-to-image retrieval excels at\nsifting through massive databases to rank relevant images, yet it stops at\nwhole-image matches and offers no fine-grained localization. We introduce\nReferring Search and Discovery (ReSeDis), the first task that unifies\ncorpus-level retrieval with pixel-level grounding. Given a free-form\ndescription, a ReSeDis model must decide whether the queried object appears in\neach image and, if so, where it is, returning bounding boxes or segmentation\nmasks. To enable rigorous study, we curate a benchmark in which every\ndescription maps uniquely to object instances scattered across a large, diverse\ncorpus, eliminating unintended matches. We further design a task-specific\nmetric that jointly scores retrieval recall and localization precision.\nFinally, we provide a straightforward zero-shot baseline using a frozen\nvision-language model, revealing significant headroom for future study. ReSeDis\noffers a realistic, end-to-end testbed for building the next generation of\nrobust and scalable multimodal search systems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15180v1", "AI": {"title_translation": "ReSeDis：一个用于大规模图像集合中基于指称的对象搜索数据集", "tldr": "ReSeDis引入了一项新的任务和数据集，用于统一大规模图像检索和精确对象定位，解决了现有方法的局限性。", "motivation": "现有的大规模视觉搜索引擎要么只能定位图像（文本到图像检索），要么只能在图像内识别对象（视觉定位），但没有一个能将两者统一起来以应对大规模集合，这导致了误报或缺乏细粒度定位的问题。", "method": "本文提出了ReSeDis，这是一项统一语料库级检索和像素级定位的新任务。为此，他们构建了一个基准数据集，其中每个描述都唯一地映射到分散在大规模多样化语料库中的对象实例。此外，他们还设计了一个结合检索召回率和定位精度的任务特定度量标准，并提供了一个使用冻结视觉-语言模型的零样本基线。", "result": "论文提供了一个基准数据集、一个任务特定的度量标准以及一个基线模型，表明未来的研究仍有显著的改进空间。", "conclusion": "ReSeDis为构建下一代鲁棒且可扩展的多模态搜索系统提供了一个现实的端到端测试平台。", "translation": "大规模视觉搜索引擎有望同时解决双重问题：(i) 找到真正包含句子描述对象的每张图像，以及 (ii) 在每个命中图像中识别对象的边界框或精确像素。现有技术只解决了其中一个挑战。视觉定位能够产生紧密的边界框和掩码，但它建立在不切实际的假设之上，即对象存在于每个测试图像中，当应用于网络规模的集合时会产生大量误报。文本到图像检索擅长筛选海量数据库以对相关图像进行排名，但它止步于整图匹配，不提供细粒度定位。我们引入了指称搜索与发现 (ReSeDis)，这是第一个统一语料库级检索和像素级定位的任务。给定一个自由形式的描述，ReSeDis 模型必须决定查询对象是否出现在每张图像中，如果出现，则返回其位置，即边界框或分割掩码。为了实现严谨的研究，我们策划了一个基准，其中每个描述都唯一地映射到分散在大规模、多样化语料库中的对象实例，从而消除了意外匹配。我们还设计了一个任务特定的度量标准，它联合评估检索召回率和定位精度。最后，我们提供了一个使用冻结视觉-语言模型的直接零样本基线，揭示了未来研究的巨大进步空间。ReSeDis 为构建下一代鲁棒且可扩展的多模态搜索系统提供了一个现实的端到端测试平台。", "summary": "本文介绍了ReSeDis，一项新颖的任务，它将大规模图像检索与精确的对象定位相结合，解决了当前视觉搜索引擎只能处理其中一个方面的局限性。为了促进研究，作者策划了一个独特的、描述与特定对象实例唯一映射的数据集，提出了一个结合检索和定位的综合度量标准，并建立了一个零样本基线，为未来开发鲁棒的多模态搜索系统指明了研究方向。", "keywords": "指称搜索, 对象定位, 图像检索, 多模态搜索, 数据集", "comments": "这项工作的创新之处在于将语料库级别的检索与像素级别的定位相结合，这是迈向更实用、更鲁棒的大规模视觉搜索的关键一步。创建具有唯一映射的专用数据集和组合评估指标对于严谨的研究也极具价值。这项工作为开发下一代多模态搜索系统提供了急需的基准。"}}
{"id": "2506.14929", "title": "FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning", "authors": ["Ganyu Wang", "Jinjie Fang", "Maxwell J. Ying", "Bin Gu", "Xi Chen", "Boyu Wang", "Charles Ling"], "summary": "Black-Box Discrete Prompt Learning is a prompt-tuning method that optimizes\ndiscrete prompts without accessing model parameters or gradients, making the\nprompt tuning on a cloud-based Large Language Model (LLM) feasible. Adapting\nfederated learning to BDPL could further enhance prompt tuning performance by\nleveraging data from diverse sources. However, all previous research on\nfederated black-box prompt tuning had neglected the substantial query cost\nassociated with the cloud-based LLM service. To address this gap, we conducted\na theoretical analysis of query efficiency within the context of federated\nblack-box prompt tuning. Our findings revealed that degrading FedAvg to\nactivate only one client per round, a strategy we called \\textit{FedOne},\nenabled optimal query efficiency in federated black-box prompt learning.\nBuilding on this insight, we proposed the FedOne framework, a federated\nblack-box discrete prompt learning method designed to maximize query efficiency\nwhen interacting with cloud-based LLMs. We conducted numerical experiments on\nvarious aspects of our framework, demonstrating a significant improvement in\nquery efficiency, which aligns with our theoretical results.", "comment": "Published in Proceedings of the 42nd International Conference on\n  Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14929v1", "AI": {"title_translation": "FedOne：面向黑盒离散提示学习的查询高效联邦学习", "tldr": "本文提出了FedOne框架，通过每轮仅激活一个客户端的策略，显著提高了联邦黑盒离散提示学习的查询效率，解决了高昂的LLM查询成本问题。", "motivation": "所有先前关于联邦黑盒提示调优的研究都忽略了与基于云的大型语言模型（LLM）服务相关的巨大查询成本。", "method": "通过对联邦黑盒提示调优背景下的查询效率进行理论分析，发现将FedAvg降级为每轮仅激活一个客户端（即FedOne策略）可以在联邦黑盒提示学习中实现最优查询效率。基于此洞察，提出了FedOne框架。", "result": "数值实验表明，FedOne框架显著提高了查询效率，与理论结果一致。", "conclusion": "FedOne框架通过优化客户端激活策略，有效解决了联邦黑盒离散提示学习中与云端LLM交互时的高昂查询效率问题，实现了最优的查询效率。", "translation": "黑盒离散提示学习是一种提示调优方法，它无需访问模型参数或梯度即可优化离散提示，这使得在基于云的大型语言模型（LLM）上进行提示调优成为可能。将联邦学习应用于BDPL可以利用来自不同来源的数据进一步提高提示调优性能。然而，所有先前关于联邦黑盒提示调优的研究都忽略了与基于云的LLM服务相关的巨大查询成本。为了解决这一空白，我们对联邦黑盒提示调优背景下的查询效率进行了理论分析。我们的发现表明，将FedAvg降级为每轮仅激活一个客户端（我们称之为FedOne的策略）可以在联邦黑盒提示学习中实现最优查询效率。基于这一洞察，我们提出了FedOne框架，这是一种联邦黑盒离散提示学习方法，旨在与基于云的LLM交互时最大限度地提高查询效率。我们对框架的各个方面进行了数值实验，结果表明查询效率显著提高，这与我们的理论结果一致。", "summary": "本文提出了FedOne框架，旨在解决联邦黑盒离散提示学习中高昂的云端LLM查询成本问题。通过理论分析，发现每轮仅激活一个客户端的FedOne策略能够实现最优查询效率。实验结果验证了FedOne在提高查询效率方面的显著效果。", "keywords": "联邦学习, 黑盒离散提示学习, 查询效率, 大型语言模型, FedOne", "comments": "FedOne的创新之处在于其对联邦学习中客户端激活策略的重新思考，以应对黑盒提示学习中LLM查询成本的挑战。通过理论分析指导实践，提出了一种简单而有效的优化方案，对于降低基于云LLM的联邦学习成本具有重要意义。"}}
{"id": "2506.15415", "title": "Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning", "authors": ["Stanley Ngugi"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.", "comment": "11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15415v1", "AI": {"title_translation": "目标词汇注入：通过早期层LoRA微调解锁Lugha-Llama中的潜在跨语言对齐", "tldr": "大型语言模型（LLMs）在低资源语言（LRLs）中的性能受限于数据稀缺和预训练不足。本文提出了一种名为“目标词汇注入”（TLI）的新型高效微调方法，通过LoRA和对比学习目标，针对LLM早期层（如Lugha-Llama-8B-wura的第2层）中已存在的潜在跨语言词汇对齐进行微调，显著提高了斯瓦希里语-英语的词汇对齐，并能很好地泛化到未见词对。", "motivation": "大型语言模型（LLMs）在斯瓦希里语等低资源语言（LRLs）中的表现因数据稀缺和预训练不足而滞后。关键挑战在于实现强大的跨语言词汇对齐，这对于翻译和跨语言信息检索等任务至关重要。研究发现Lugha-Llama-8B-wura在其早期内部层（如第2层）对斯瓦希里语-英语词对表现出接近完美的词汇对齐，但这种能力并未完全体现在其最终输出表示中。", "method": "本文引入了目标词汇注入（TLI）方法。TLI利用低秩适应（LoRA）和对比学习目标对模型进行微调，特别针对从经验上确定的最佳早期层（如第2层）提取的嵌入。该方法旨在利用Lugha-Llama-8B-wura中已观察到的强大潜在跨语言对齐。", "result": "TLI显著改善了623对已训练的斯瓦希里语-英语词对的输出级别词汇对齐，平均余弦相似度从0.3211提高到0.4113（+28.08%）。更重要的是，这些改进在63对未见的对照词对上表现出显著的泛化能力，相似度从0.3143提高到0.4033（+28.32%）。", "conclusion": "这些发现表明，TLI增强了模型保留和传播其固有的早期层跨语言知识的能力，为改善以LRL为重点的LLM中的词汇对齐提供了一种参数高效且有效的策略。", "translation": "大型语言模型（LLMs）展现出卓越的能力，然而，由于数据稀缺和预训练中的代表性不足，它们在斯瓦希里语等低资源语言（LRLs）中的性能往往滞后。实现鲁棒的跨语言词汇对齐是一个关键挑战，这对于翻译和跨语言信息检索等任务至关重要。本文引入了目标词汇注入（TLI），一种新颖高效的微调方法。我们首先证明，以斯瓦希里语为中心的LLM Lugha-Llama-8B-wura在其早期内部层（特别是第2层，根据初步研究，平均余弦相似度约为0.99998）对斯瓦希里语-英语词对表现出强大的、近乎完美的词汇对齐，但这种能力并未完全体现在其最终输出表示中（基线在我们的评估集上相似度约为0.32）。TLI利用这一洞察，通过低秩适应（LoRA）和对比学习目标对模型进行微调，专门针对从这个经验确定的最佳早期层提取的嵌入。我们的实验表明，TLI显著改善了623对已训练的斯瓦希里语-英语词对的输出级别词汇对齐，平均余弦相似度从0.3211提高到0.4113（+28.08%，p < 1.33 x 10^-240）。更重要的是，这些改进在63对未见的对照词对上表现出显著的泛化能力，相似度从0.3143提高到0.4033（+28.32%，p < 7.17 x 10^-27）。这些发现表明，TLI增强了模型保留和传播其固有的早期层跨语言知识的能力，为改善以LRL为重点的LLM中的词汇对齐提供了一种参数高效且有效的策略。", "summary": "为解决大型语言模型（LLMs）在低资源语言（LRLs）中跨语言对齐不佳的问题，本文提出了一种名为目标词汇注入（TLI）的新型微调方法。TLI利用了Lugha-Llama-8B-wura模型在早期层中已存在强大的斯瓦希里语-英语潜在词汇对齐的发现。通过对这些早期层应用结合对比学习目标的低秩适应（LoRA）微调，TLI显著提高了已训练和未见词对的输出级别词汇对齐，展示了一种参数高效的方式来解锁和传播LRL-focused LLM中固有的跨语言知识。", "keywords": "词汇对齐, 低资源语言, LoRA, 微调, 跨语言", "comments": "TLI方法通过识别和利用LLM早期层中潜在的跨语言对齐，为改善低资源语言性能提供了一个创新途径。其参数高效性（LoRA）以及对未见数据的良好泛化能力是显著的优势。该方法有潜力应用于其他低资源语言和大型语言模型。"}}
{"id": "2506.15419", "title": "Density estimation via periodic scaled Korobov kernel method with exponential decay condition", "authors": ["Ziyang Ye", "Haoyuan Tan", "Xiaoqun Wang", "Zhijian He"], "summary": "We propose the periodic scaled Korobov kernel (PSKK) method for nonparametric\ndensity estimation on $\\mathbb{R}^d$. By first wrapping the target density into\na periodic version through modulo operation and subsequently applying kernel\nridge regression in scaled Korobov spaces, we extend the kernel approach\nproposed by Kazashi and Nobile (SIAM J. Numer. Anal., 2023) and eliminate its\nrequirement for inherent periodicity of the density function. This key\nmodification enables effective estimation of densities defined on unbounded\ndomains. We establish rigorous mean integrated squared error (MISE) bounds,\nproving that for densities with smoothness of order $\\alpha$ and exponential\ndecay, our method achieves the $\\mathcal{O}(M^{-1/(1+1/(2\\alpha)+\\epsilon)})$\nMISE convergence rate with an arbitrarily small $\\epsilon>0$. While matching\nthe convergence rate of the previous kernel approach, our approach applies to a\nbroader class of non-periodic distributions. Numerical experiments confirm the\ntheoretical results and demonstrate significant improvement over traditional\nkernel density estimation in large-sample regimes.", "comment": "26 pages, 6 figures", "cate": "math.ST", "url": "http://arxiv.org/abs/2506.15419v1", "AI": {"title_translation": "采用周期尺度Korobov核方法和指数衰减条件进行密度估计", "tldr": "本文提出一种新的周期尺度Korobov核（PSKK）方法用于非参数密度估计，通过消除对密度函数固有周期性的要求，使其能有效估计无界域上的密度，并在收敛速度上与现有方法匹配，同时适用于更广泛的非周期性分布。", "motivation": "现有的核密度估计方法（如Kazashi和Nobile提出的方法）要求密度函数具有固有的周期性，这限制了其在无界域上非周期性分布的应用。本文旨在消除这一限制，以实现对这类更广泛分布的有效估计。", "method": "本文提出周期尺度Korobov核（PSKK）方法。该方法通过首先利用模运算将目标密度包装成周期版本，然后将核岭回归应用于尺度Korobov空间。这种关键修改消除了对密度函数固有周期性的要求，从而能够处理定义在无界域上的密度。", "result": "研究建立了严格的均方积分误差（MISE）界限，证明对于平滑度为$\\\\alpha$且呈指数衰减的密度，该方法实现了$\\\\mathcal{O}(M^{-1/(1+1/(2\\\\alpha)+\\\\\\\\epsilon)})$的MISE收敛速度。该方法在收敛速度上与现有核方法匹配，但适用于更广泛的非周期性分布。数值实验证实了理论结果，并显示在大样本情况下比传统核密度估计有显著改进。", "conclusion": "所提出的PSKK方法成功扩展了核密度估计的应用范围，使其能够处理非周期性分布。该方法在理论上证明了与现有方法相当的收敛速度，并通过数值实验验证了其有效性和在大样本下的优越性，展现了更广泛的适用性。", "translation": "我们提出周期尺度Korobov核（PSKK）方法用于$\\\\mathbb{R}^d$上的非参数密度估计。通过首先通过模运算将目标密度包装成周期版本，然后将核岭回归应用于尺度Korobov空间，我们扩展了Kazashi和Nobile（SIAM J. Numer. Anal., 2023）提出的核方法，并消除了其对密度函数固有周期性的要求。这一关键修改使得能够有效估计定义在无界域上的密度。我们建立了严格的均方积分误差（MISE）界限，证明对于平滑度为$\\\\alpha$且呈指数衰减的密度，我们的方法实现了$\\\\mathcal{O}(M^{-1/(1+1/(2\\\\alpha)+\\\\\\\\epsilon)})$的MISE收敛速度，其中$\\\\\\\\epsilon>0$可以任意小。虽然匹配了先前核方法的收敛速度，但我们的方法适用于更广泛的非周期性分布。数值实验证实了理论结果，并显示在大样本情况下比传统核密度估计有显著改进。", "summary": "本文提出了一种新的非参数密度估计方法——周期尺度Korobov核（PSKK）方法。该方法通过将目标密度转换为周期形式并应用核岭回归，成功扩展了现有核方法，消除了对密度函数固有周期性的限制，从而能够有效估计无界域上的非周期性密度。研究建立了严格的MISE收敛界限，证明其在收敛速度上与现有方法相当，但适用范围更广，并通过数值实验验证了其理论优势和在大样本下的优越性。", "keywords": "密度估计, Korobov核, 非参数估计, 指数衰减, MISE收敛", "comments": "该论文的创新之处在于通过巧妙地将非周期性密度转换为周期性版本，并结合尺度Korobov核方法，成功克服了传统核方法对数据周期性假设的限制。这极大地扩展了核密度估计的应用范围，使其能够处理更普遍的无界域上的非周期性分布。在保持相同收敛速度的同时，提升了方法的普适性，具有重要的理论和实际意义。"}}
{"id": "2506.15674", "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers", "authors": ["Tommaso Green", "Martin Gubri", "Haritz Puerto", "Sangdoo Yun", "Seong Joon Oh"], "summary": "We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15674v1", "AI": {"title_translation": "泄露的思想：大型推理模型并非隐私思考者", "tldr": "大型推理模型的内部思考（推理痕迹）会泄露敏感用户数据，特别是当推理步骤增加时，这造成了隐私与效用之间的冲突。", "motivation": "研究大型推理模型作为个人代理时，其推理痕迹中的隐私泄露问题，挑战这些痕迹被认为是内部和安全的假设。", "method": "通过探查（probing）和代理评估（agentic evaluations）的方法，证明了测试时计算方法（特别是增加推理步骤）会放大隐私泄露。", "result": "推理痕迹频繁包含敏感用户数据，可通过提示注入或意外泄露到输出中。增加推理步骤会放大这种泄露。虽然增加计算预算使模型在最终答案中更谨慎，但也会导致其推理更冗长，并在思考过程中泄露更多信息。", "conclusion": "推理提高了效用，但也扩大了隐私攻击面，这是一个核心矛盾。安全工作必须扩展到模型的内部思考，而不仅仅是其输出。", "translation": "我们研究了作为个人代理使用的大型推理模型的推理痕迹中的隐私泄露问题。与最终输出不同，推理痕迹通常被认为是内部和安全的。我们通过展示推理痕迹频繁包含敏感用户数据来挑战这一假设，这些数据可以通过提示注入提取或意外泄露到输出中。通过探查和代理评估，我们证明了测试时计算方法，特别是增加推理步骤，会放大这种泄露。虽然增加这些测试时计算方法的预算会使模型在最终答案中更加谨慎，但也会导致它们推理更加冗长，并在自己的思考中泄露更多信息。这揭示了一个核心矛盾：推理提高了效用，但扩大了隐私攻击面。我们认为安全工作必须扩展到模型的内部思考，而不仅仅是其输出。", "summary": "本文研究了大型推理模型作为个人代理时，其推理痕迹中的隐私泄露问题，挑战了这些痕迹被认为是内部和安全的假设。研究表明，敏感用户数据可以从这些痕迹中提取，并且这种泄露常因增加推理步骤而加剧。研究揭示了一个核心矛盾：推理虽然提高了模型效用，却也扩大了隐私攻击面。因此，安全工作必须扩展到模型的内部思考，而不仅仅是其输出。", "keywords": "隐私泄露, 推理痕迹, 大型推理模型, 个人代理, 提示注入", "comments": "这篇论文具有创新性，因为它揭示了大型语言模型内部推理过程中一个此前被忽视的隐私漏洞。其重要性在于强调了模型效用（通过推理提升）与隐私之间存在的关键张力，并促使我们重新评估超越最终输出的安全协议。这表明，如果当前的安全措施未能考虑到内部思考的泄露，它们可能是不够的。"}}
{"id": "2506.15666", "title": "Vision in Action: Learning Active Perception from Human Demonstrations", "authors": ["Haoyu Xiong", "Xiaomeng Xu", "Jimmy Wu", "Yifan Hou", "Jeannette Bohg", "Shuran Song"], "summary": "We present Vision in Action (ViA), an active perception system for bimanual\nrobot manipulation. ViA learns task-relevant active perceptual strategies\n(e.g., searching, tracking, and focusing) directly from human demonstrations.\nOn the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to\nenable flexible, human-like head movements. To capture human active perception\nstrategies, we design a VR-based teleoperation interface that creates a shared\nobservation space between the robot and the human operator. To mitigate VR\nmotion sickness caused by latency in the robot's physical movements, the\ninterface uses an intermediate 3D scene representation, enabling real-time view\nrendering on the operator side while asynchronously updating the scene with the\nrobot's latest observations. Together, these design elements enable the\nlearning of robust visuomotor policies for three complex, multi-stage bimanual\nmanipulation tasks involving visual occlusions, significantly outperforming\nbaseline systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15666v1", "AI": {"title_translation": "动作中的视觉：从人类演示中学习主动感知", "tldr": "本文提出了Vision in Action (ViA) 系统，通过VR遥操作从人类演示中学习机器人双臂操作的主动感知策略，并在复杂、有遮挡的任务中表现优异。", "motivation": "该研究旨在使双臂机器人能够学习和执行复杂操作任务中的主动感知策略，通过模仿人类的感知行为来提高机器人的操作能力，尤其是在存在视觉遮挡的情况下。", "method": "本文提出了Vision in Action (ViA) 系统，该系统利用一个6自由度的机器人颈部实现类似人类的头部运动。为了捕捉人类的主动感知策略，研究人员设计了一个基于VR的遥操作界面，该界面在机器人和人类操作员之间创建共享观察空间。为缓解VR晕动症，该界面采用中间3D场景表示，实现实时视图渲染和异步场景更新，从而有效地从人类演示中学习任务相关的感知策略。", "result": "ViA系统成功学习了针对三个复杂、多阶段双臂操作任务的鲁棒视觉运动策略，这些任务涉及视觉遮挡。实验结果表明，ViA系统在这些任务上的性能显著优于基线系统。", "conclusion": "Vision in Action (ViA) 系统能够有效地从人类演示中学习主动感知策略，并在复杂的双臂机器人操作任务中实现卓越的性能，尤其是在处理视觉遮挡方面。", "translation": "我们提出了“动作中的视觉”（ViA），一个用于双臂机器人操作的主动感知系统。ViA直接从人类演示中学习与任务相关的主动感知策略（例如，搜索、跟踪和聚焦）。在硬件方面，ViA采用了一个简单但有效的6自由度机器人颈部，以实现灵活的、类似人类的头部运动。为了捕捉人类的主动感知策略，我们设计了一个基于VR的遥操作界面，该界面在机器人和人类操作员之间创建了一个共享的观察空间。为了缓解由机器人物理运动延迟引起的VR晕动症，该界面使用了一个中间3D场景表示，从而在操作员端实现实时视图渲染，同时异步更新机器人最新观察到的场景。这些设计元素共同使得能够学习针对三个复杂、多阶段双臂操作任务的鲁棒视觉运动策略，这些任务涉及视觉遮挡，并且显著优于基线系统。", "summary": "本文介绍了Vision in Action (ViA) 系统，一个用于双臂机器人操作的主动感知框架。ViA通过创新的VR遥操作界面，能够直接从人类演示中学习任务相关的感知策略，如搜索、跟踪和聚焦，并结合一个灵活的6自由度机器人颈部。该系统通过使用中间3D场景表示来有效缓解VR遥操作中因延迟引起的晕动症。实验证明，ViA在处理涉及视觉遮挡的复杂多阶段双臂操作任务时，能够学习到鲁棒的视觉运动策略，并显著超越了现有基线系统。", "keywords": "主动感知, 双臂机器人, 人类演示, 视觉运动, VR遥操作", "comments": "本文的创新点在于其ViA系统，通过独特设计的VR遥操作界面，成功地从人类演示中学习了主动感知策略，这对于机器人处理复杂、动态环境中的操作任务至关重要。尤其是解决了VR遥操作中的延迟和晕动症问题，使得数据采集更为有效和舒适，为机器人学习高效的人类感知行为提供了新途径。"}}
{"id": "2506.15200", "title": "Conquering the Retina: Bringing Visual in-Context Learning to OCT", "authors": ["Alessio Negrini", "Simon Reiß"], "summary": "Recent advancements in medical image analysis have led to the development of\nhighly specialized models tailored to specific clinical tasks. These models\nhave demonstrated exceptional performance and remain a crucial research\ndirection. Yet, their applicability is limited to predefined tasks, requiring\nexpertise and extensive resources for development and adaptation. In contrast,\ngeneralist models offer a different form of utility: allowing medical\npractitioners to define tasks on the fly without the need for task-specific\nmodel development. In this work, we explore how to train generalist models for\nthe domain of retinal optical coherence tomography using visual in-context\nlearning (VICL), i.e., training models to generalize across tasks based on a\nfew examples provided at inference time. To facilitate rigorous assessment, we\npropose a broad evaluation protocol tailored to VICL in OCT. We extensively\nevaluate a state-of-the-art medical VICL approach on multiple retinal OCT\ndatasets, establishing a first baseline to highlight the potential and current\nlimitations of in-context learning for OCT. To foster further research and\npractical adoption, we openly release our code.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15200v1", "AI": {"title_translation": "征服视网膜：将视觉情境学习引入眼科光学相干断层扫描（OCT）", "tldr": "本文探索了如何将视觉情境学习（VICL）应用于视网膜光学相干断层扫描（OCT）领域，以训练通用模型，并提出了一个评估协议，建立了VICL在OCT领域的首个基线。", "motivation": "现有的医疗图像分析模型虽然性能卓越，但其应用受限于预定义任务，开发和适应需要大量专业知识和资源。与此相反，通用模型允许医疗从业者即时定义任务，无需开发特定任务的模型，因此本文旨在探索如何为视网膜OCT领域训练通用模型。", "method": "本文通过视觉情境学习（VICL）来训练通用模型，使其能够根据推理时提供的少量示例来泛化到不同任务。为严格评估，提出了一个针对OCT中VICL的广泛评估协议。研究人员在多个视网膜OCT数据集上广泛评估了一种最先进的医疗VICL方法。", "result": "研究结果建立了情境学习在OCT领域的首个基线，突出了其潜力与当前的局限性。", "conclusion": "本研究为视网膜OCT领域的情境学习建立了初步基线，展示了其潜力与局限性，旨在促进未来的研究和实际应用。", "translation": "近年来，医学图像分析领域的最新进展促使了针对特定临床任务的高度专业化模型的开发。这些模型展示了卓越的性能，并仍然是一个重要的研究方向。然而，它们的适用性仅限于预定义任务，需要专业知识和大量资源进行开发和适应。相比之下，通用模型提供了另一种形式的实用性：允许医疗从业者即时定义任务，而无需开发特定任务的模型。在这项工作中，我们探讨了如何利用视觉情境学习（VICL），即训练模型根据推理时提供的少量示例进行跨任务泛化，来训练视网膜光学相干断层扫描（OCT）领域的通用模型。为了促进严格评估，我们提出了一个针对OCT中VICL的广泛评估协议。我们广泛评估了一种最先进的医疗VICL方法在多个视网膜OCT数据集上的表现，建立了情境学习在OCT领域的首个基线，以突显其潜力和当前局限性。为了促进进一步的研究和实际应用，我们公开了我们的代码。", "summary": "本文旨在将视觉情境学习（VICL）引入视网膜光学相干断层扫描（OCT）领域，以开发能够即时适应不同任务的通用模型，克服了传统专业模型在应用上的局限性。研究提出了一项针对OCT中VICL的评估协议，并在多个数据集上对先进的VICL方法进行了广泛评估，成功建立了该领域首个性能基线，揭示了VICL在OCT中的潜力与局限性。为促进后续研究和实际应用，作者已公开代码。", "keywords": "视觉情境学习, OCT, 通用模型, 视网膜, 医疗图像分析", "comments": "本文的创新点在于首次将视觉情境学习（VICL）应用于视网膜光学相干断层扫描（OCT）领域，并为此建立了首个基线。这对于推动医疗图像分析从特定任务模型向更灵活的通用模型发展具有重要意义，尤其是在资源受限或需要快速适应新任务的临床环境中。通过公开代码，作者也为未来的研究和实际应用奠定了基础。"}}
{"id": "2506.15425", "title": "Understanding GUI Agent Localization Biases through Logit Sharpness", "authors": ["Xingjian Tao", "Yiwei Wang", "Yujun Cai", "Zhicheng Yang", "Jing Tang"], "summary": "Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15425v1", "AI": {"title_translation": "通过Logit锐度理解GUI代理定位偏差", "tldr": "本文提出了一个细粒度评估框架、一个量化模型不确定性的PSS指标以及一个训练无关的上下文感知裁剪技术，以理解并改进GUI代理的定位偏差和幻觉问题，提高其可靠性、可解释性和鲁棒性。", "motivation": "尽管多模态大型语言模型（MLLMs）使GUI代理能够通过将语言映射到空间动作来与操作系统交互并表现出良好的性能，但它们经常出现幻觉，即系统性的定位错误，这损害了其可靠性。", "method": "本文提出了一个细粒度评估框架，将模型预测分为四种不同类型，以揭示传统准确性指标之外的细微失败模式。为了更好地量化模型不确定性，引入了峰值锐度分数（PSS），该指标评估坐标预测中语义连续性与Logits分布之间的一致性。在此基础上，进一步提出了上下文感知裁剪（Context-Aware Cropping），这是一种无需训练的技术，通过自适应地优化输入上下文来提高模型性能。", "result": "广泛的实验表明，本文提出的框架和方法提供了可操作的见解，并增强了GUI代理行为的可解释性和鲁棒性。", "conclusion": "本文通过引入细粒度评估框架、峰值锐度分数（PSS）和上下文感知裁剪技术，有效揭示并改进了GUI代理的定位偏差和幻觉问题，显著提升了模型的可靠性、可解释性和鲁棒性。", "translation": "多模态大型语言模型（MLLMs）已使GUI代理能够通过将语言映射到空间动作来与操作系统交互。尽管它们表现出良好的性能，但这些模型经常出现幻觉——系统性的定位错误，这损害了可靠性。我们提出了一个细粒度评估框架，将模型预测分为四种不同类型，揭示了超越传统准确性指标的细微失败模式。为了更好地量化模型不确定性，我们引入了峰值锐度分数（PSS），该指标评估坐标预测中语义连续性与Logits分布之间的一致性。在此基础上，我们进一步提出了上下文感知裁剪，这是一种无需训练的技术，通过自适应地优化输入上下文来提高模型性能。广泛的实验表明，我们的框架和方法提供了可操作的见解，并增强了GUI代理行为的可解释性和鲁棒性。", "summary": "本文针对多模态大型语言模型（MLLMs）驱动的GUI代理在与操作系统交互中常出现的定位幻觉问题，提出了一系列改进方法。研究引入了一个细粒度评估框架以识别更深层次的失败模式，并设计了峰值锐度分数（PSS）来量化模型的不确定性。此外，还提出了一种无需训练的上下文感知裁剪技术来优化输入上下文，从而提升模型性能。实验结果验证了这些方法能够提供有价值的洞察，并增强GUI代理行为的可解释性和鲁棒性。", "keywords": "GUI代理, 定位偏差, 幻觉, Logit锐度, 多模态大型语言模型", "comments": "这篇论文通过深入分析GUI代理的定位偏差，提出了创新的评估指标（PSS）和实用的改进技术（上下文感知裁剪），尤其后者无需训练，具有很高的应用价值。它不仅揭示了MLLMs在GUI交互中的局限性，还为提升其可靠性和可解释性提供了具体路径，对Agent领域的研究具有重要意义。"}}
{"id": "2506.15680", "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos", "authors": ["Kaifeng Zhang", "Baoyu Li", "Kris Hauser", "Yunzhu Li"], "summary": "Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .", "comment": "Project page: https://kywind.github.io/pgnd", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15680v1", "AI": {"title_translation": "用于从RGB-D视频中学习可变形物体模型的粒子-网格神经动力学", "tldr": "本文提出了一种结合粒子和空间网格的神经动力学框架，用于从RGB-D视频中学习可变形物体的数字孪生模型，并实现了在稀疏视图下的高效学习和泛化，优于现有方法。", "motivation": "由于可变形物体多样的物理特性以及从有限视觉信息中估计状态的困难，对可变形物体动力学建模具有挑战性。", "method": "本文提出了一种结合物体粒子和空间网格的混合表示神经动力学框架。粒子表示物体形状，空间网格离散化3D空间以确保空间连续性和提高学习效率。该框架与高斯溅射（Gaussian Splattings）相结合用于视觉渲染，实现了可变形物体的全学习型数字孪生体。", "result": "该模型能够从机器人-物体交互的稀疏视图RGB-D记录中学习各种（如绳索、布料、毛绒玩具、纸袋）物体的动力学。它在类别级别上泛化到未见实例，并且在有限摄像机视图场景中优于最先进的基于学习和基于物理的模拟器。此外，学习到的模型可用于模型规划，实现目标导向的物体操作。", "conclusion": "本文提出的粒子-网格神经动力学框架成功地为可变形物体构建了一个全学习型的数字孪生模型，能够从有限视觉信息中学习其复杂动力学，并支持模型驱动的规划和操作。", "translation": "由于可变形物体多样的物理特性以及从有限视觉信息中估计状态的困难，对可变形物体动力学建模具有挑战性。我们通过一个结合物体粒子和空间网格的混合表示神经动力学框架来解决这些挑战。我们的粒子-网格模型捕获全局形状和运动信息，同时预测密集的粒子运动，从而能够对具有不同形状和材料的物体进行建模。粒子表示物体形状，而空间网格离散化3D空间以确保空间连续性并提高学习效率。结合高斯溅射（Gaussian Splattings）进行视觉渲染，我们的框架实现了可变形物体的全学习型数字孪生体，并生成3D动作条件视频。通过实验，我们证明了我们的模型能够从机器人-物体交互的稀疏视图RGB-D记录中学习各种物体——如绳索、布料、毛绒玩具和纸袋——的动力学，同时在类别级别上泛化到未见实例。我们的方法优于最先进的基于学习和基于物理的模拟器，特别是在有限摄像机视图的场景中。此外，我们展示了学习模型在基于模型的规划中的实用性，能够在一系列任务中实现目标导向的物体操作。项目页面可在https://kywind.github.io/pgnd上找到。", "summary": "本文提出了一种新颖的粒子-网格神经动力学框架，用于从RGB-D视频中学习可变形物体的动力学模型。该框架采用混合表示，结合粒子捕捉物体形状和密集运动，以及空间网格确保连续性和提高学习效率。通过与高斯溅射结合，它创建了可变形物体的全学习型数字孪生体，并能生成动作条件视频。实验证明，该模型能从稀疏视图数据中有效学习多种物体的动力学，泛化能力强，并优于现有模拟器，同时支持模型驱动的物体操作规划。", "keywords": "可变形物体建模, 神经动力学, 粒子-网格, RGB-D视频, 数字孪生", "comments": "该论文的创新点在于提出了粒子与空间网格相结合的混合表示，有效解决了可变形物体建模中形状多样性和状态估计的挑战。其通过全学习方式构建数字孪生，并结合高斯溅射进行渲染，使其在稀疏视图下表现出色，并能泛化到未见实例，这对于机器人操作等领域具有重要意义。此外，其在模型规划中的应用也展示了其实用价值。"}}
{"id": "2506.15201", "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models", "authors": ["Xuelin Shen", "Jiayin Xu", "Kangsheng Yin", "Wenhan Yang"], "summary": "The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.", "comment": "11 pages, 6 figures, publised to ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15201v1", "AI": {"title_translation": "隐私保护图像压缩：防御视觉-语言预训练模型的利用", "tldr": "本文提出一种名为隐私保护图像压缩（PSIC）的新方法，通过在图像压缩阶段引入防御机制，使图像在保持良好感知质量的同时，阻止视觉-语言预训练（VLP）模型对图像语义的理解，从而保护用户隐私。", "motivation": "视觉-语言预训练（VLP）模型日益增强的语义理解能力使得公开图像容易被搜索引擎等工具利用，从而难以保护用户隐私。因此，本文旨在通过在图像压缩阶段实施防御来防止这种利用。", "method": "提出了一种灵活的隐私保护图像压缩（PSIC）方法，能够生成具有多种解码选项的比特流。默认情况下，解码后的图像能保持满意的感知质量，同时阻止VLP模型进行语义解释。该方法还保留了原始图像压缩功能，并可通过可定制的输入条件重建保留完整语义信息的图像。为此，设计了一个条件潜在触发生成（CLTG）模块来生成偏差信息以指导解码过程，并提出了一个不确定性感知加密导向（UAEO）优化函数，利用目标VLP模型在训练数据上的不确定性推断出的软标签。此外，还采用自适应多目标优化策略，在统一的训练过程中同时提高加密性能和感知质量。该方案是即插即用的，可集成到现有大多数学习型图像压缩（LIC）模型中。", "result": "大量在多个下游任务上的实验证明了所提设计的有效性。", "conclusion": "本文提出的PSIC方法通过在图像压缩阶段引入防御机制，成功地在保持图像感知质量的同时，有效阻止了VLP模型对图像语义信息的利用，为用户隐私保护提供了一种可集成且有效的解决方案。", "translation": "视觉-语言预训练（VLP）模型语义理解能力的提升使得保护公开图像不被搜索引擎和其他类似工具利用变得越来越困难。在此背景下，本文旨在通过在图像压缩阶段实施防御来防止这种利用，从而保护用户的隐私。具体来说，我们提出了一种灵活的编码方法，称为隐私保护图像压缩（PSIC），它可以生成具有多个解码选项的比特流。默认情况下，比特流解码后图像能保持令人满意的感知质量，同时阻止VLP模型进行解释。我们的方法还保留了原始图像压缩功能。通过可定制的输入条件，所提出的方案可以重建保留其完整语义信息的图像。为此，提出了一种条件潜在触发生成（CLTG）模块，根据可定制的条件生成偏差信息，以引导解码过程生成不同的重建版本；并设计了一种不确定性感知加密导向（UAEO）优化函数，利用从目标VLP模型在训练数据上的不确定性推断出的软标签。本文进一步结合了一种自适应多目标优化策略，以在统一的训练过程中同时获得改进的加密性能和感知质量。所提出的方案是即插即用的，可以无缝集成到大多数现有的学习型图像压缩（LIC）模型中。在多个下游任务中进行的大量实验证明了我们设计的有效性。", "summary": "本文提出了一种名为隐私保护图像压缩（PSIC）的新型灵活编码方法，旨在通过在图像压缩阶段引入防御机制来保护用户隐私，防止视觉-语言预训练（VLP）模型对公开图像的语义信息进行利用。PSIC能够生成多解码选项的比特流，默认输出保持良好感知质量但阻止VLP模型理解的图像，同时可通过特定条件恢复完整语义信息。该方法结合了条件潜在触发生成（CLTG）模块和不确定性感知加密导向（UAEO）优化函数，并采用自适应多目标优化策略，可无缝集成到现有学习型图像压缩模型中，并通过实验验证了其有效性。", "keywords": "隐私保护, 图像压缩, 视觉-语言预训练模型, 多目标优化, 语义防御", "comments": "这篇论文提出了一种新颖的方法，在图像压缩层面解决VLP模型带来的隐私泄露问题，具有重要的实际意义。其创新点在于引入了多解码选项和条件潜在触发机制，允许图像在保持视觉可用的同时，对VLP模型呈现“模糊”的语义信息。这种在数据源头进行防御的思路，相对于后期处理，可能更为高效和彻底。其即插即用的特性也增加了其应用潜力。"}}
{"id": "2506.15451", "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need", "authors": ["Zhouhong Gu", "Xiaoxuan Zhu", "Yin Cai", "Hao Shen", "Xingzhou Chen", "Qingyi Wang", "Jialin Li", "Xiaoran Shi", "Haoran Guo", "Wenxuan Huang", "Hongwei Feng", "Yanghua Xiao", "Zheyu Ye", "Yao Hu", "Shaosheng Cao"], "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15451v1", "AI": {"title_translation": "AgentGroupChat-V2：分而治之是基于大型语言模型的多智能体系统所需要的", "tldr": "AgentGroupChat-V2是一个新颖的框架，通过引入分而治之的完全并行架构、自适应协作引擎和智能体组织优化策略，显著提升了基于大型语言模型的多智能体系统在复杂任务处理和泛化能力方面的性能。", "motivation": "当前基于大型语言模型的多智能体系统在系统架构设计、跨领域泛化能力和性能保证方面面临严峻挑战，尤其是在任务复杂度和智能体数量增加时。", "method": "本文提出了AgentGroupChat-V2框架，通过三项核心创新来解决现有挑战：1) 分而治之的完全并行架构，将用户查询分解为分层任务森林结构，实现依赖管理和分布式并发处理；2) 自适应协作引擎，根据任务特性动态选择异构LLM组合和交互模式；3) 结合分而治之方法的智能体组织优化策略，实现高效问题分解。", "result": "实验证明AgentGroupChat-V2在多领域表现出色，在GSM8K上达到91.50%的准确率（超越最佳基线5.6个百分点），在竞赛级AIME上达到30.4%的准确率（几乎是其他方法的两倍），在HumanEval上达到79.20%的pass@1。性能优势随任务难度增加而愈发显著，尤其是在Level 5 MATH问题上，相比现有最佳基线提升超过11个百分点。", "conclusion": "这些结果证实AgentGroupChat-V2为构建高效、通用的大型语言模型多智能体系统提供了一个全面的解决方案，在复杂推理场景中具有显著优势。", "translation": "基于大型语言模型的多智能体系统在社会模拟和复杂任务解决领域展现出巨大潜力。然而，当前框架在系统架构设计、跨领域泛化能力和性能保证方面面临严峻挑战，尤其是在任务复杂度和智能体数量增加时。我们引入了AgentGroupChat-V2，一个新颖的框架，通过三项核心创新来解决这些挑战：(1) 分而治之的完全并行架构，将用户查询分解为分层任务森林结构，实现依赖管理和分布式并发处理。(2) 自适应协作引擎，根据任务特性动态选择异构LLM组合和交互模式。(3) 结合分而治之方法的智能体组织优化策略，实现高效问题分解。广泛的实验证明AgentGroupChat-V2在不同领域均表现出卓越性能，在GSM8K上达到91.50%的准确率（超越最佳基线5.6个百分点），在竞赛级AIME上达到30.4%的准确率（几乎是其他方法的两倍），在HumanEval上达到79.20%的pass@1。性能优势随任务难度增加而愈发显著，尤其是在Level 5 MATH问题上，相比现有最佳基线提升超过11个百分点。这些结果证实AgentGroupChat-V2为构建高效、通用的大型语言模型多智能体系统提供了一个全面的解决方案，在复杂推理场景中具有显著优势。代码可在https://github.com/MikeGu721/AgentGroupChat-V2获取。", "summary": "AgentGroupChat-V2是一个针对大型语言模型（LLM）多智能体系统的新型框架，旨在解决现有系统在架构设计、泛化能力和性能方面的挑战。该框架通过引入分而治之的完全并行架构、自适应协作引擎以及优化的智能体组织策略，实现了用户查询的层级分解和分布式处理。实验结果表明，AgentGroupChat-V2在GSM8K、AIME和HumanEval等多个基准测试中均显著超越了现有最佳方法，尤其是在高难度任务上表现出更强的优势，证明了其在复杂推理场景下构建高效、通用LLM多智能体系统的潜力。", "keywords": "LLM多智能体系统, 分而治之, AgentGroupChat-V2, 并行架构, 复杂任务解决", "comments": "本文提出的AgentGroupChat-V2框架创新性地将“分而治之”思想应用于LLM多智能体系统的设计中，通过构建完全并行的架构和自适应协作机制，有效解决了现有系统在处理复杂任务时的可扩展性和性能瓶颈。其在多个难度级别任务上的显著性能提升，特别是对高难度数学问题的解决能力，突显了其在复杂推理领域的重要性和实用性。"}}
{"id": "2506.15218", "title": "DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder", "authors": ["Dan He", "Weisheng Li", "Guofen Wang", "Yuping Huang", "Shiqiang Liu"], "summary": "Multimodal medical image fusion (MMIF) extracts the most meaningful\ninformation from multiple source images, enabling a more comprehensive and\naccurate diagnosis. Achieving high-quality fusion results requires a careful\nbalance of brightness, color, contrast, and detail; this ensures that the fused\nimages effectively display relevant anatomical structures and reflect the\nfunctional status of the tissues. However, existing MMIF methods have limited\ncapacity to capture detailed features during conventional training and suffer\nfrom insufficient cross-modal feature interaction, leading to suboptimal fused\nimage quality. To address these issues, this study proposes a two-stage\ndiffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In\nStage I, a diffusion process trains UNet for image reconstruction. UNet\ncaptures detailed information through progressive denoising and represents\nmultilevel data, providing a rich set of feature representations for the\nsubsequent fusion network. In Stage II, noisy images at various steps are input\ninto the fusion network to enhance the model's feature recognition capability.\nThree key fusion modules are also integrated to process medical images from\ndifferent modalities adaptively. Ultimately, the robust network structure and a\nhybrid loss function are integrated to harmonize the fused image's brightness,\ncolor, contrast, and detail, enhancing its quality and information density. The\nexperimental results across various medical image types demonstrate that the\nproposed method performs exceptionally well regarding objective evaluation\nmetrics. The fused image preserves appropriate brightness, a comprehensive\ndistribution of radioactive tracers, rich textures, and clear edges. The code\nis available at https://github.com/HeDan-11/DM-FNet.", "comment": "This paper has been accepted by IEEE Transactions on Multimedia (TMM)\n  in March 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15218v1", "AI": {"title_translation": "DM-FNet：基于扩散过程训练的编解码器实现统一多模态医学图像融合", "tldr": "DM-FNet提出了一种基于扩散模型的两阶段网络，用于高质量多模态医学图像融合，解决了现有方法的局限性，并在客观评估指标上表现出色。", "motivation": "现有多模态医学图像融合（MMIF）方法在传统训练中捕获详细特征的能力有限，并且存在跨模态特征交互不足的问题，导致融合图像质量不佳。", "method": "本研究提出了一种两阶段的基于扩散模型的融合网络（DM-FNet）。第一阶段，通过扩散过程训练UNet进行图像重建，以捕获详细信息并提供丰富的特征表示。第二阶段，将不同步骤的噪声图像输入融合网络，以增强模型的特征识别能力。该方法还集成了三个关键的融合模块，以自适应地处理来自不同模态的医学图像，并结合鲁棒的网络结构和混合损失函数来协调融合图像的亮度、颜色、对比度和细节。", "result": "在各种医学图像类型上的实验结果表明，所提出的方法在客观评估指标方面表现出色。融合图像保留了适当的亮度、放射性示踪剂的全面分布、丰富的纹理和清晰的边缘。", "conclusion": "所提出的DM-FNet通过利用扩散过程和鲁棒的网络结构，有效地解决了现有MMIF方法的局限性，生成了高质量的融合图像，有助于更全面的诊断。", "translation": "多模态医学图像融合（MMIF）从多源图像中提取最有意义的信息，从而实现更全面、更准确的诊断。实现高质量的融合结果需要对亮度、颜色、对比度和细节进行仔细的平衡；这确保了融合图像有效地显示相关的解剖结构并反映组织的功​​能状态。然而，现有的MMIF方法在传统训练中捕获详细特征的能力有限，并且存在跨模态特征交互不足的问题，导致融合图像质量不佳。为了解决这些问题，本研究提出了一种两阶段的基于扩散模型的融合网络（DM-FNet），以实现统一的MMIF。在第一阶段，扩散过程训练UNet进行图像重建。UNet通过渐进式去噪捕获详细信息并表示多级数据，为后续的融合网络提供了丰富的特征表示。在第二阶段，将不同步骤的噪声图像输入融合网络，以增强模型的特征识别能力。还集成了三个关键的融合模块，以自适应地处理来自不同模态的医学图像。最终，鲁棒的网络结构和混合损失函数被整合，以协调融合图像的亮度、颜色、对比度和细节，从而提高其质量和信息密度。在各种医学图像类型上的实验结果表明，所提出的方法在客观评估指标方面表现出色。融合图像保留了适当的亮度、放射性示踪剂的全面分布、丰富的纹理和清晰的边缘。代码可在https://github.com/HeDan-11/DM-FNet获取。", "summary": "本文介绍了DM-FNet，一种新颖的两阶段基于扩散模型的网络，用于统一多模态医学图像融合（MMIF）。它解决了现有方法在细节捕获和跨模态交互方面的局限性。第一阶段利用扩散过程训练UNet进行详细特征提取，而第二阶段通过将噪声图像输入融合网络来增强特征识别。DM-FNet集成了三个自适应融合模块和一个混合损失函数，以确保融合图像具有平衡的亮度、颜色、对比度和细节，并在实验中展现出卓越的性能。", "keywords": "多模态医学图像融合, 扩散模型, UNet, 图像重建, 特征交互", "comments": "该研究的创新之处在于将扩散模型应用于多模态医学图像融合的特征提取，这与传统训练方法相比是一种新颖的途径。其两阶段设计和自适应融合模块增强了处理各种医学图像模态和保留关键诊断信息的能力。这有望显著提高诊断准确性。"}}
{"id": "2506.14965", "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "authors": ["Zhoujun Cheng", "Shibo Hao", "Tianyang Liu", "Fan Zhou", "Yutao Xie", "Feng Yao", "Yuexin Bian", "Yonghao Zhuang", "Nilabjo Dey", "Yuheng Zha", "Yi Gu", "Kun Zhou", "Yuqi Wang", "Yuan Li", "Richard Fan", "Jianshu She", "Chengqian Gao", "Abulhair Saparov", "Haonan Li", "Taylor W. Killian", "Mikhail Yurochkin", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu"], "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360", "comment": "38 pages, 9 figures. Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14965v1", "AI": {"title_translation": "从跨领域视角重新审视LLM推理的强化学习", "tldr": "本文引入了跨领域强化学习推理语料库Guru，并发现RL在不同推理领域中对LLM的知识利用和技能习得有不同影响，训练出的Guru模型在多领域推理任务上取得了SOTA性能。", "motivation": "现有的LLM推理强化学习研究主要集中在数学和代码领域，限制了对其在通用推理中适用性的理解。主要挑战在于缺乏可靠、可扩展的跨领域RL奖励信号。", "method": "1. 构建了一个包含92K个可验证示例的精选RL推理语料库Guru，涵盖数学、代码、科学、逻辑、模拟和表格六个推理领域，并通过领域特定奖励设计、去重和过滤确保可靠性。2. 基于Guru，系统性地重新审视了LLM推理中RL的既定发现。3. 训练并提出了Guru-7B和Guru-32B两个模型。", "result": "1. RL对LLM推理的影响在不同领域存在显著差异。2. 在预训练中频繁出现的领域（数学、代码、科学）容易受益于跨领域RL训练，而预训练中接触较少的领域（逻辑、模拟、表格）需要领域内训练才能获得显著性能提升，这表明RL可能促进真正的技能习得。3. Guru-7B和Guru-32B模型在公开模型中取得了SOTA性能，在17项任务评估套件上比最佳基线分别高出7.9%和6.7%。4. 模型有效提高了其基础模型的Pass@k性能，尤其是在预训练数据中较少出现的复杂任务上。", "conclusion": "强化学习对LLM推理的影响因领域而异，它不仅能激发现有知识，还能促进真实技能的习得，特别是在预训练中接触较少的领域。本文提出的Guru语料库和Guru模型为通用推理提供了SOTA解决方案。", "translation": "强化学习（RL）已成为改善大型语言模型（LLM）推理的一种有前景的方法，但大多数开放性研究都狭隘地集中在数学和代码领域，这限制了我们对其在通用推理中更广泛适用性的理解。一个关键挑战在于缺乏跨不同推理领域的可靠、可扩展的RL奖励信号。我们引入了Guru，一个精选的RL推理语料库，包含9.2万个可验证的示例，涵盖六个推理领域——数学、代码、科学、逻辑、模拟和表格——每个领域都通过领域特定的奖励设计、去重和过滤构建，以确保RL训练的可靠性和有效性。基于Guru，我们系统地重新审视了LLM推理中RL的既定发现，并观察到跨领域的显著差异。例如，虽然先前的工作表明RL主要从预训练模型中激发现有知识，但我们的结果揭示了一种更细致的模式：在预训练中频繁出现的领域（数学、代码、科学）很容易受益于跨领域RL训练，而预训练中接触有限的领域（逻辑、模拟和表格）需要领域内训练才能获得有意义的性能提升，这表明RL可能促进真正的技能习得。最后，我们提出了Guru-7B和Guru-32B，这两个模型在公开可用的数据上经过RL训练后，在开放模型中取得了最先进的性能，在我们的涵盖六个推理领域的17项任务评估套件上，分别比最佳基线高出7.9%和6.7%。我们还表明，我们的模型有效提高了其基础模型的Pass@k性能，特别是在预训练数据中不太可能出现的复杂任务上。我们发布了数据、模型、训练和评估代码，以促进通用推理。", "summary": "本文提出了一种名为Guru的跨领域强化学习推理语料库，旨在解决现有RL对LLM推理研究领域狭窄的问题。Guru包含六个推理领域的9.2万个示例，通过领域特定奖励设计构建。基于此语料库，研究发现RL对LLM推理的影响因领域而异，对于预训练中常见的领域，跨领域RL训练有效；而对于预训练中较少见的领域，则需要领域内训练以促进真正的技能习得。研究还推出了Guru-7B和Guru-32B模型，这些模型在多领域推理任务上达到了当前开放模型的最佳性能，并显著提升了基础模型在复杂任务上的表现。", "keywords": "强化学习, LLM推理, 跨领域, 技能习得, Guru语料库", "comments": "本文通过构建一个大规模、多领域的RL推理语料库Guru，弥补了现有RL在LLM推理应用中领域局限性的不足。其创新之处在于揭示了RL对LLM推理的复杂影响，即在不同领域，RL可能分别侧重于知识提取和新技能习得。这一发现对于理解RL在LLM中的作用具有重要意义。同时，发布的SOTA模型和数据集对推动通用LLM推理研究具有重要贡献。"}}
{"id": "2506.15455", "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation", "authors": ["Xinnuo Xu", "Rachel Lawrence", "Kshitij Dubey", "Atharva Pandey", "Risa Ueno", "Fabian Falck", "Aditya V. Nori", "Rahul Sharma", "Amit Sharma", "Javier Gonzalez"], "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15455v1", "AI": {"title_translation": "RE-IMAGINE：用于推理评估的符号基准合成", "tldr": "RE-IMAGINE是一个框架，通过生成无法通过记忆解决的问题变体，评估大型语言模型是真正推理还是统计性回忆。", "motivation": "当前大型语言模型在推理基准上表现出高准确性，但尚不清楚这是否源于真正的推理能力，还是仅仅对训练集的统计性回忆。", "method": "本文引入了RE-IMAGINE框架，该框架受因果关系阶梯（Pearl, 2009）启发，旨在表征大型语言模型推理能力的层次结构。它通过一个自动化管道，在中间符号表示中改变问题，生成任意数量的、无法仅通过记忆解决的问题变体。该框架通用，可应用于数学、代码和逻辑等推理领域。", "result": "在四个广泛使用的基准测试上评估了多种大型语言模型后，观察到当模型面对RE-IMAGINE生成的变体问题时，其性能有所下降。", "conclusion": "这些评估结果表明，大型语言模型过去的性能在一定程度上依赖于统计性回忆，并为针对推理层次结构中各项技能的进一步研究开辟了道路。", "translation": "最近的大型语言模型（LLMs）在推理基准测试中报告了高准确率。然而，目前尚不清楚观察到的结果是源于真正的推理能力，还是源于训练集的统计性回忆。受因果关系阶梯（Pearl, 2009）及其三个层次（关联、干预和反事实）的启发，本文引入了RE-IMAGINE，这是一个用于表征LLMs推理能力层次结构的框架，同时还提供了一个自动化管道，用于生成不同层次的问题变体。通过在中间符号表示中改变问题，RE-IMAGINE生成了任意数量的、无法仅通过记忆解决的问题。此外，该框架具有通用性，可应用于数学、代码和逻辑等推理领域。我们在一系列广泛使用的基准测试上展示了我们的框架，以评估多种LLMs家族，并观察到当模型被查询问题变体时，性能有所下降。这些评估表明，过去的性能在一定程度上依赖于统计性回忆，并为针对推理层次结构中各项技能的进一步研究开辟了道路。", "summary": "本文提出了RE-IMAGINE框架，旨在区分大型语言模型在推理任务上的表现是源于真正的推理能力还是统计性回忆。该框架受因果关系阶梯启发，通过改变问题的符号表示来生成大量新的、无法通过记忆解决的问题变体，并可应用于数学、代码和逻辑等多个推理领域。实验结果表明，当模型面对这些变体问题时，性能显著下降，这表明它们在一定程度上依赖于对训练数据的回忆。", "keywords": "大型语言模型, 推理评估, 符号基准合成, RE-IMAGINE, 记忆与推理", "comments": "RE-IMAGINE框架的创新之处在于其通过符号表示生成问题变体的能力，有效避免了模型通过记忆而非推理来解决问题的可能性。这对于评估大型语言模型的真实推理能力至关重要，为未来的模型开发提供了更清晰的评估标准。"}}
{"id": "2506.14978", "title": "ODD: Overlap-aware Estimation of Model Performance under Distribution Shift", "authors": ["Aayush Mishra", "Anqi Liu"], "summary": "Reliable and accurate estimation of the error of an ML model in unseen test\ndomains is an important problem for safe intelligent systems. Prior work uses\ndisagreement discrepancy (DIS^2) to derive practical error bounds under\ndistribution shifts. It optimizes for a maximally disagreeing classifier on the\ntarget domain to bound the error of a given source classifier. Although this\napproach offers a reliable and competitively accurate estimate of the target\nerror, we identify a problem in this approach which causes the disagreement\ndiscrepancy objective to compete in the overlapping region between source and\ntarget domains. With an intuitive assumption that the target disagreement\nshould be no more than the source disagreement in the overlapping region due to\nhigh enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).\nMaximizing ODD only requires disagreement in the non-overlapping target domain,\nremoving the competition. Our ODD-based bound uses domain-classifiers to\nestimate domain-overlap and better predicts target performance than DIS^2. We\nconduct experiments on a wide array of benchmarks to show that our method\nimproves the overall performance-estimation error while remaining valid and\nreliable. Our code and results are available on GitHub.", "comment": "Accepted to the 41st Conference on Uncertainty in Artificial\n  Intelligence, 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14978v1", "AI": {"title_translation": "ODD：分布偏移下模型性能的重叠感知估计", "tldr": "ODD提出了一种新的重叠感知不一致性差异方法，用于在存在分布偏移的情况下更准确地估计机器学习模型的性能，解决了现有DIS^2方法在重叠区域的竞争问题。", "motivation": "在未见过的测试域中可靠且准确地估计机器学习模型的误差对于安全的智能系统至关重要。现有方法DIS^2在处理源域和目标域之间的重叠区域时存在问题，导致不一致性差异目标在该区域内产生竞争。", "method": "本研究提出了重叠感知不一致性差异（ODD）方法。ODD基于一个直观的假设，即在重叠区域中，目标域的不一致性不应超过源域的不一致性。通过最大化ODD，仅需关注非重叠目标域中的不一致性，从而消除了竞争。ODD利用域分类器来估计域重叠。", "result": "实验结果表明，ODD方法在广泛的基准测试中改进了整体性能估计误差，并且保持了有效性和可靠性。与DIS^2相比，ODD能更好地预测目标性能。", "conclusion": "ODD通过引入重叠感知机制，有效解决了现有DIS^2方法在分布偏移下模型性能估计中的重叠区域竞争问题，从而提供了一种更准确、更可靠的误差估计方法。", "translation": "在未见过的测试域中可靠且准确地估计机器学习模型的误差对于安全的智能系统来说是一个重要问题。先前的研究使用不一致性差异（DIS^2）来推导分布偏移下的实际误差界限。它在目标域上优化了一个最大不一致分类器，以限制给定源分类器的误差。尽管这种方法提供了可靠且具有竞争力的目标误差估计，但我们发现这种方法存在一个问题，即不一致性差异目标在源域和目标域之间的重叠区域中产生竞争。我们基于一个直观的假设，即由于足够高的支持度，目标域的不一致性在重叠区域中不应超过源域的不一致性，从而设计了重叠感知不一致性差异（ODD）。最大化ODD仅需要在非重叠目标域中存在不一致性，从而消除了竞争。我们基于ODD的界限使用域分类器来估计域重叠，并且比DIS^2能更好地预测目标性能。我们在广泛的基准测试上进行了实验，结果表明我们的方法在保持有效性和可靠性的同时，提高了整体性能估计误差。我们的代码和结果可在GitHub上获取。", "summary": "该论文提出了一种名为ODD（重叠感知不一致性差异）的新方法，旨在解决机器学习模型在分布偏移下性能估计的准确性问题。现有DIS^2方法在源域和目标域的重叠区域中存在竞争问题，导致误差估计不准确。ODD通过引入重叠感知机制，并利用域分类器来估计域重叠，仅关注非重叠目标域中的不一致性，从而消除了这种竞争。实验证明，ODD比DIS^2能更准确、更可靠地预测模型在目标域上的性能。", "keywords": "分布偏移, 模型性能估计, 不一致性差异, 重叠感知, 域适应", "comments": "这篇论文的创新点在于提出了“重叠感知”的概念，并将其应用于模型性能估计的不一致性差异方法中。通过识别并解决现有DIS^2方法在重叠区域的竞争问题，ODD提供了一种更精确的误差估计方法，对于提高智能系统的安全性和可靠性具有重要意义。其方法直观且实验结果表明了其优越性，是领域适应和泛化研究的一个有价值的贡献。"}}
{"id": "2506.15480", "title": "Context-Informed Grounding Supervision", "authors": ["Hyunji Lee", "Seunghyun Yoon", "Yunjae Won", "Hanseok Oh", "Geewook Kim", "Trung Bui", "Franck Dernoncourt", "Elias Stengel-Eskin", "Mohit Bansal", "Minjoon Seo"], "summary": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15480v1", "AI": {"title_translation": "上下文感知的基础监督", "tldr": "本文提出了CINGS，一种后训练监督方法，通过在响应前添加相关上下文并仅对响应令牌计算损失，显著增强了大型语言模型在文本和视觉领域对外部信息的依赖和接地能力，同时不影响其通用性能。", "motivation": "大型语言模型（LLMs）通常需要补充外部知识以提供未编码的信息或减少幻觉。然而，现有工作表明，仅仅在推理时附加上下文并不能确保生成的内容是基于上下文的。", "method": "本文提出了上下文感知的基础监督（Context-INformed Grounding Supervision, CINGS），这是一种后训练监督方法。在训练过程中，模型会预先添加相关上下文到响应中，但仅对响应令牌计算损失，并屏蔽掉上下文。", "result": "通过CINGS训练的模型在文本和视觉领域都表现出更强的接地能力。在文本领域，CINGS在11个信息检索数据集上优于其他训练方法，并与推理时接地技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干，在四个基准测试中减少了幻觉并保持了事实一致性。这种改进的接地能力没有降低通用下游性能。", "conclusion": "CINGS通过改变模型的先验知识和行为，隐式地鼓励模型更多地依赖外部上下文，从而显著增强了大型语言模型的接地能力。", "translation": "大型语言模型（LLM）通常需要补充外部知识，以提供其参数中未编码的信息或减少幻觉。在这种情况下，我们期望模型通过将其响应基于所提供的外部上下文来生成响应。然而，先前的研究表明，仅仅在推理时附加上下文并不能确保生成的内容是基于上下文的。为了解决这个问题，我们提出了上下文感知的基础监督（CINGS），这是一种后训练监督方法，其中模型在训练时将相关上下文预置到响应中，同时仅对响应令牌计算损失并屏蔽掉上下文。我们的实验表明，与标准的指令调优模型相比，使用CINGS训练的模型在文本和视觉领域都表现出更强的接地能力。在文本领域，CINGS在11个信息检索数据集上优于其他训练方法，并与推理时接地技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干，在四个基准测试中减少了幻觉并保持了生成响应的事实一致性。这种改进的接地能力并未导致通用下游性能的下降。最后，我们分析了CINGS中增强接地能力的潜在机制，发现它诱导了模型先验知识和行为的转变，隐式地鼓励模型更多地依赖外部上下文。", "summary": "本文提出了一种名为上下文感知的基础监督（CINGS）的后训练方法，旨在解决大型语言模型（LLMs）在利用外部知识时接地能力不足的问题。CINGS通过在训练时将相关上下文预置到响应中，并仅对响应令牌计算损失，从而使模型更有效地依赖外部信息。实验证明，CINGS显著增强了模型在文本和视觉领域（包括视觉-语言模型）的接地能力，减少了幻觉，同时不影响模型的通用性能。研究还发现CINGS通过改变模型的内部机制，使其更倾向于利用外部上下文。", "keywords": "大型语言模型, 接地能力, 幻觉, 后训练监督, 上下文感知", "comments": "CINGS的创新之处在于其独特的后训练监督机制，通过选择性地计算损失来强制模型在生成时更依赖于预置的上下文，而不是仅仅将其作为输入。这种方法有效地解决了LLM在利用外部知识时常出现的“幻觉”问题，并且在文本和视觉-语言领域都显示出显著的性能提升。其重要性在于为构建更可靠、更少幻觉的LLM提供了一条有效的途径，特别是在需要事实一致性的应用中。此外，该方法与推理时接地技术互补，显示了其普适性。"}}
{"id": "2506.15065", "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Xiaopan Zhang", "Fahim Faisal Niloy", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "summary": "Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15065v1", "AI": {"title_translation": "HEAL：一项关于大型语言模型驱动的具身智能体幻觉的实证研究", "tldr": "本研究系统地调查了大型语言模型驱动的具身智能体在场景-任务不一致下的幻觉问题，发现现有模型在处理不可行任务时存在根本性局限，并提供了改进策略的见解。", "motivation": "大型语言模型（LLMs）作为具身智能体的认知核心被广泛采用，但其固有的幻觉问题（源于未能将用户指令与观察到的物理环境相结合）会导致导航错误，例如搜索不存在的物体。本研究旨在系统地理解这种幻觉的发生程度、触发因素以及当前模型的响应方式。", "method": "构建了一个幻觉探测集，基于现有基准，能将幻觉率提高40倍。在两个模拟环境中评估了12个模型。", "result": "模型展现出推理能力，但未能解决场景-任务不一致问题，突显了在处理不可行任务方面的根本性局限。", "conclusion": "研究提供了关于理想模型行为的可行见解，为开发更稳健可靠的规划策略提供了指导。", "translation": "大型语言模型（LLM）正越来越多地被采纳为具身智能体的认知核心。然而，源于未能将用户指令与观察到的物理环境相结合的固有幻觉，可能导致导航错误，例如搜索一个不存在的冰箱。在本文中，我们首次对基于LLM的具身智能体在场景-任务不一致下执行长时任务时的幻觉进行了系统研究。我们的目标是了解幻觉发生的程度、哪些类型的不一致会触发它们，以及当前模型如何响应。为了实现这些目标，我们通过在现有基准上构建了一个幻觉探测集，该探测集能够将幻觉率提高到基础提示的40倍。在两个模拟环境中评估了12个模型后，我们发现，尽管模型表现出推理能力，但它们未能解决场景-任务不一致问题——这突显了在处理不可行任务方面的根本性局限。我们还针对每种情况下的理想模型行为提供了可行的见解，为开发更稳健和可靠的规划策略提供了指导。", "summary": "本研究首次系统地探讨了大型语言模型（LLM）驱动的具身智能体在执行长时任务时，因场景与任务不一致而产生的幻觉问题。通过构建一个能够显著提高幻觉率的探测集，并在两个模拟环境中评估了12个模型，研究发现LLM虽然具有推理能力，但在处理不可行任务时存在根本性缺陷，无法有效解决场景-任务不一致。研究结果为开发更鲁棒的具身智能体规划策略提供了具体指导和见解。", "keywords": "具身智能体, 大型语言模型, 幻觉, 场景-任务不一致, 经验研究", "comments": "这项研究是首次系统性地探究LLM驱动的具身智能体幻觉问题，其创新之处在于构建了一个高效的幻觉探测集，能够显著放大幻觉现象，从而更清晰地揭示了LLM在处理现实世界不一致性时的核心局限。研究结果对于具身AI领域具有重要意义，它不仅揭示了当前LLM的不足，更提供了未来模型改进的方向和实用建议，对于提升具身智能体的可靠性和鲁棒性至关重要。"}}
{"id": "2506.15231", "title": "Convolutional Feature Enhancement and Attention Fusion BiFPN for Ship Detection in SAR Images", "authors": ["Liangjie Meng", "Danxia Li", "Jinrong He", "Lili Ma", "Zhixin Li"], "summary": "Synthetic Aperture Radar (SAR) enables submeter-resolution imaging and\nall-weather monitoring via active microwave and advanced signal processing.\nCurrently, SAR has found extensive applications in critical maritime domains\nsuch as ship detection. However, SAR ship detection faces several challenges,\nincluding significant scale variations among ships, the presence of small\noffshore vessels mixed with noise, and complex backgrounds for large nearshore\nships. To address these issues, this paper proposes a novel feature enhancement\nand fusion framework named C-AFBiFPN. C-AFBiFPN constructs a Convolutional\nFeature Enhancement (CFE) module following the backbone network, aiming to\nenrich feature representation and enhance the ability to capture and represent\nlocal details and contextual information. Furthermore, C-AFBiFPN innovatively\nintegrates BiFormer attention within the fusion strategy of BiFPN, creating the\nAFBiFPN network. AFBiFPN improves the global modeling capability of cross-scale\nfeature fusion and can adaptively focus on critical feature regions. The\nexperimental results on SAR Ship Detection Dataset (SSDD) indicate that the\nproposed approach substantially enhances detection accuracy for small targets,\nrobustness against occlusions, and adaptability to multi-scale features.", "comment": "5 pages, 4 figures, 2 tables. Code available at\n  https://github.com/mlj666219/C-AFBiFPN/tree/master", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15231v1", "AI": {"title_translation": "卷积特征增强与注意力融合BiFPN用于SAR图像舰船检测", "tldr": "本文提出C-AFBiFPN框架，结合卷积特征增强（CFE）和注意力融合BiFPN（AFBiFPN），旨在解决SAR图像舰船检测中多尺度、小目标和复杂背景等挑战，并在实验中显著提升了检测性能。", "motivation": "SAR舰船检测面临多项挑战，包括舰船之间显著的尺度变化、混杂噪声的小型近海船只，以及大型近岸舰船的复杂背景。", "method": "本文提出了一种新颖的特征增强和融合框架C-AFBiFPN。该框架在骨干网络之后构建了一个卷积特征增强（CFE）模块，旨在丰富特征表示并增强捕获和表示局部细节及上下文信息的能力。此外，C-AFBiFPN创新性地将BiFormer注意力整合到BiFPN的融合策略中，创建了AFBiFPN网络，以改善跨尺度特征融合的全局建模能力，并能自适应地关注关键特征区域。", "result": "在SAR舰船检测数据集（SSDD）上的实验结果表明，所提出的方法显著提高了小目标检测精度、对遮挡的鲁棒性以及对多尺度特征的适应性。", "conclusion": "C-AFBiFPN框架通过卷积特征增强和注意力融合BiFPN，有效解决了SAR图像舰船检测中的多尺度、小目标和复杂背景等挑战，显著提升了检测性能。", "translation": "合成孔径雷达（SAR）通过主动微波和先进信号处理实现亚米级分辨率成像和全天候监测。目前，SAR已在舰船检测等关键海洋领域得到广泛应用。然而，SAR舰船检测面临多项挑战，包括舰船之间显著的尺度变化、混杂噪声的小型近海船只，以及大型近岸舰船的复杂背景。为了解决这些问题，本文提出了一种新颖的特征增强和融合框架，名为C-AFBiFPN。C-AFBiFPN在骨干网络之后构建了一个卷积特征增强（CFE）模块，旨在丰富特征表示并增强捕获和表示局部细节及上下文信息的能力。此外，C-AFBiFPN创新性地将BiFormer注意力整合到BiFPN的融合策略中，创建了AFBiFPN网络。AFBiFPN改善了跨尺度特征融合的全局建模能力，并能自适应地关注关键特征区域。在SAR舰船检测数据集（SSDD）上的实验结果表明，所提出的方法显著提高了小目标检测精度、对遮挡的鲁棒性以及对多尺度特征的适应性。", "summary": "本文提出了一种名为C-AFBiFPN的特征增强与融合框架，旨在解决SAR图像舰船检测中存在的尺度变化大、小目标识别困难及复杂背景等问题。该框架通过引入卷积特征增强（CFE）模块来丰富特征表示，并创新性地将BiFormer注意力机制集成到BiFPN中，构建AFBiFPN网络，以提升跨尺度特征融合的全局建模能力和关键区域的自适应聚焦能力。实验结果表明，C-AFBiFPN在小目标检测精度、抗遮挡鲁棒性以及多尺度特征适应性方面均取得了显著提升。", "keywords": "SAR图像, 舰船检测, 卷积特征增强, BiFPN, 注意力融合", "comments": "该论文的创新点在于提出了C-AFBiFPN框架，特别是结合了卷积特征增强（CFE）和融入BiFormer注意力的AFBiFPN。CFE旨在提升局部细节和上下文信息的捕获能力，而AFBiFPN则通过注意力机制增强了跨尺度特征融合的全局建模和关键区域聚焦能力。这对于解决SAR图像中舰船检测面临的多尺度、小目标和复杂背景等挑战具有重要意义，显示出较强的实用性和潜在应用价值。"}}
{"id": "2506.14986", "title": "Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks", "authors": ["Maxime Usdin", "Lito Kriara", "Licinio Craveiro"], "summary": "Early multiple sclerosis (MS) disability progression prediction is\nchallenging due to disease heterogeneity. This work predicts 48- and 72-week\ndisability using sparse baseline clinical data and 12 weeks of daily digital\nFloodlight data from the CONSONANCE clinical trial. We employed\nstate-of-the-art tabular and time-series foundation models (FMs), a custom\nmultimodal attention-based transformer, and machine learning methods. Despite\nthe difficulty of early prediction (AUROC 0.63), integrating digital data via\nadvanced models improved performance over clinical data alone. A transformer\nmodel using unimodal embeddings from the Moment FM yielded the best result, but\nour multimodal transformer consistently outperformed its unimodal counterpart,\nconfirming the advantages of combining clinical with digital data. Our findings\ndemonstrate the promise of FMs and multimodal approaches to extract predictive\nsignals from complex and diverse clinical and digital life sciences data (e.g.,\nimaging, omics), enabling more accurate prognostics for MS and potentially\nother complex diseases.", "comment": "Accepted to IJCAI 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14986v1", "AI": {"title_translation": "多发性硬化症残疾进展的早期预测：基于多模态基础模型基准", "tldr": "该研究利用多模态基础模型，结合临床和数字数据，旨在早期预测多发性硬化症的残疾进展，并证明了多模态方法的优势。", "motivation": "由于疾病异质性，早期预测多发性硬化症（MS）残疾进展具有挑战性。本研究旨在通过整合稀疏基线临床数据和每日数字数据来解决这一难题，以提高预测准确性。", "method": "研究利用CONSONANCE临床试验中的稀疏基线临床数据和12周的每日数字Floodlight数据，预测48周和72周的残疾进展。采用了先进的表格和时间序列基础模型（FMs）、定制的多模态注意力Transformer模型以及其他机器学习方法。", "result": "尽管早期预测难度大（AUROC 0.63），但整合数字数据通过先进模型提高了性能。使用Moment FM单模态嵌入的Transformer模型取得了最佳结果，但多模态Transformer模型持续优于其单模态对应物，证实了结合临床与数字数据的优势。", "conclusion": "研究结果表明，基础模型和多模态方法在从复杂多样的临床和数字生命科学数据中提取预测信号方面具有潜力，能够为MS及其他复杂疾病提供更准确的预后。", "translation": "由于疾病异质性，早期预测多发性硬化症（MS）残疾进展具有挑战性。本工作利用CONSONANCE临床试验中的稀疏基线临床数据和12周的每日数字Floodlight数据，预测48周和72周的残疾情况。我们采用了最先进的表格和时间序列基础模型（FMs）、定制的多模态注意力Transformer模型以及机器学习方法。尽管早期预测存在难度（AUROC 0.63），但通过先进模型整合数字数据比单独使用临床数据提高了性能。使用Moment FM单模态嵌入的Transformer模型取得了最佳结果，但我们的多模态Transformer模型持续优于其单模态对应物，证实了结合临床与数字数据的优势。我们的研究结果表明，基础模型和多模态方法在从复杂多样的临床和数字生命科学数据（例如，影像、组学）中提取预测信号方面具有前景，从而能够为MS以及潜在的其他复杂疾病提供更准确的预后。", "summary": "本研究旨在解决多发性硬化症（MS）残疾进展早期预测的挑战，通过结合稀疏基线临床数据和每日数字数据，利用先进的基础模型和多模态Transformer模型进行预测。结果显示，整合数字数据显著提高了预测性能，多模态方法优于单模态方法，证明了其在从复杂生命科学数据中提取预测信号的潜力，有望提高MS及其他复杂疾病的预后准确性。", "keywords": "多发性硬化症, 残疾进展预测, 多模态, 基础模型, 数字健康数据", "comments": "该研究的创新之处在于首次将多模态基础模型应用于多发性硬化症的残疾进展预测，并成功整合了临床与数字数据。其重要性在于证明了多模态方法在处理复杂异质性疾病数据方面的潜力，为未来更准确的疾病预后提供了新的思路。虽然早期预测的AUROC值不高，但其方法学上的突破值得肯定。"}}
{"id": "2506.15498", "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling", "authors": ["Md Imbesat Hassan Rizvi", "Xiaodan Zhu", "Iryna Gurevych"], "summary": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.", "comment": "8 pages main content, 4 figures, 4 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15498v1", "AI": {"title_translation": "SPARE：单次通过参考引导评估的自动过程监督和奖励建模", "tldr": "SPARE是一种新的单次通过、参考引导的自动过程标注框架，显著提高了LLM的推理性能和效率，用于微调和奖励模型训练。", "motivation": "高效、高质量的自动化过程标注仍然是大型语言模型（LLMs）复杂多步推理能力提升面临的重大挑战。", "method": "引入了单次通过参考引导评估（SPARE），这是一个新颖的结构化框架，通过将每个解决方案步骤与参考解决方案中的一个或多个步骤对齐，并伴随明确的评估推理，实现单次通过、每步标注。", "result": "参考引导的步骤级评估有效促进了数学推理、多跳组合问答和空间推理三个领域中四个数据集上的过程监督。SPARE在用于离线强化学习设置中的模型微调和奖励模型训练时，提高了LLM的推理性能。在具有挑战性的数学数据集上取得了竞争性性能，并提供2.6倍的更高效率，仅需38%的运行时间，相比基于树搜索的自动标注。", "conclusion": "SPARE是一种有效且高效的自动化过程标注方法，能够显著提升大型语言模型的推理性能和效率。其代码库和训练模型已公开发布，以促进进一步研究和可复现性。", "translation": "过程或分步监督在提升大型语言模型（LLMs）复杂多步推理能力方面发挥了关键作用。然而，高效、高质量的自动化过程标注仍然是一个重大挑战。为了解决这个问题，我们引入了单次通过参考引导评估（SPARE），这是一个新颖的结构化框架，通过将每个解决方案步骤与参考解决方案中的一个或多个步骤对齐，并伴随明确的评估推理，实现单次通过、每步标注。我们展示了参考引导的步骤级评估有效促进了数学推理、多跳组合问答和空间推理三个领域中四个数据集上的过程监督。我们证明，与基线相比，SPARE在以下方面提高了推理性能：（1）在离线强化学习设置中用于推理时贪婪解码的模型微调，以及（2）用于对多个LLM生成输出进行排序/聚合的奖励模型训练。此外，SPARE在具有挑战性的数学数据集上取得了具有竞争力的性能，同时提供了2.6倍的更高效率，与基于树搜索的自动标注相比，仅需38%的运行时间。代码库以及经过训练的SPARE-PRM模型已公开发布，以促进进一步的研究和可复现性。", "summary": "SPARE是一种新颖的单次通过标注框架，通过将解决方案步骤与参考对齐并进行显式推理评估，解决了大型语言模型（LLMs）高效高质量自动化过程标注的挑战。该方法在数学推理、多跳问答和空间推理等多个领域的数据集上有效促进了过程监督。实验证明，SPARE在用于模型微调和奖励模型训练时，能提高LLM的推理性能，并在数学数据集上实现竞争性表现，同时比基于树搜索的自动标注效率提高2.6倍。该框架的代码库和训练模型已公开发布。", "keywords": "过程监督, 参考引导评估, 大型语言模型, 自动标注, 奖励建模", "comments": "SPARE的创新之处在于其“单次通过”和“参考引导评估”的结构化框架，显著提高了自动过程标注的效率和质量。它不仅提升了LLM在复杂推理任务上的性能，还通过开源代码促进了该领域的研究和复现，对于LLM的过程监督和奖励建模具有重要意义。"}}
{"id": "2506.15242", "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories", "authors": ["Qingsong Yan", "Qiang Wang", "Kaiyong Zhao", "Jie Chen", "Bo Li", "Xiaowen Chu", "Fei Deng"], "summary": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged\nas powerful tools for 3D reconstruction and SLAM tasks. However, their\nperformance depends heavily on accurate camera pose priors. Existing approaches\nattempt to address this issue by introducing external constraints but fall\nshort of achieving satisfactory accuracy, particularly when camera trajectories\nare complex. In this paper, we propose a novel method, RA-NeRF, capable of\npredicting highly accurate camera poses even with complex camera trajectories.\nFollowing the incremental pipeline, RA-NeRF reconstructs the scene using NeRF\nwith photometric consistency and incorporates flow-driven pose regulation to\nenhance robustness during initialization and localization. Additionally,\nRA-NeRF employs an implicit pose filter to capture the camera movement pattern\nand eliminate the noise for pose estimation. To validate our method, we conduct\nextensive experiments on the Tanks\\&Temple dataset for standard evaluation, as\nwell as the NeRFBuster dataset, which presents challenging camera pose\ntrajectories. On both datasets, RA-NeRF achieves state-of-the-art results in\nboth camera pose estimation and visual quality, demonstrating its effectiveness\nand robustness in scene reconstruction under complex pose trajectories.", "comment": "IROS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15242v1", "AI": {"title_translation": "RA-NeRF：复杂轨迹下高精度相机姿态估计的鲁棒神经辐射场重建", "tldr": "RA-NeRF提出了一种在复杂相机轨迹下，通过结合光度一致性、流驱动姿态调节和隐式姿态滤波器，实现高精度相机姿态估计和鲁棒神经辐射场重建的方法。", "motivation": "现有的神经辐射场（NeRF）和3D高斯泼溅（3DGS）方法在3D重建和SLAM任务中严重依赖精确的相机姿态先验，但在相机轨迹复杂时，未能达到令人满意的精度。", "method": "RA-NeRF采用增量式流程，使用NeRF和光度一致性重建场景。它引入流驱动姿态调节以增强初始化和定位的鲁棒性，并采用隐式姿态滤波器来捕捉相机运动模式并消除姿态估计中的噪声。", "result": "RA-NeRF在Tanks&Temple和NeRFBuster数据集上，在相机姿态估计和视觉质量方面均达到了最先进的结果。", "conclusion": "RA-NeRF在复杂姿态轨迹下的场景重建中表现出有效性和鲁棒性，能够预测高精度的相机姿态。", "translation": "神经辐射场（NeRF）和3D高斯泼溅（3DGS）已成为3D重建和SLAM任务的强大工具。然而，它们的性能严重依赖于精确的相机姿态先验。现有方法试图通过引入外部约束来解决这个问题，但在相机轨迹复杂时，未能达到令人满意的精度。在本文中，我们提出了一种新颖的方法RA-NeRF，即使在复杂的相机轨迹下也能预测高度精确的相机姿态。RA-NeRF遵循增量式管道，使用NeRF和光度一致性重建场景，并结合流驱动的姿态调节以增强初始化和定位期间的鲁棒性。此外，RA-NeRF采用隐式姿态滤波器来捕捉相机运动模式并消除姿态估计中的噪声。为了验证我们的方法，我们对Tanks&Temple数据集进行了标准评估，以及对NeRFBuster数据集（该数据集呈现具有挑战性的相机姿态轨迹）进行了广泛实验。在这两个数据集上，RA-NeRF在相机姿态估计和视觉质量方面都取得了最先进的结果，证明了其在复杂姿态轨迹下场景重建的有效性和鲁棒性。", "summary": "本文提出了RA-NeRF，一种针对复杂相机轨迹下的神经辐射场重建方法，旨在解决现有NeRF和3DGS方法对精确相机姿态先验的过度依赖问题。RA-NeRF通过结合光度一致性、流驱动姿态调节和隐式姿态滤波器，实现了高精度的相机姿态估计和鲁棒的场景重建，并在多个挑战性数据集上达到了最先进的性能。", "keywords": "神经辐射场, 相机姿态估计, 3D重建, 复杂轨迹, 鲁棒性", "comments": "RA-NeRF的创新点在于其结合了光度一致性、流驱动姿态调节和隐式姿态滤波器，有效解决了复杂相机轨迹下NeRF和3DGS重建中相机姿态估计精度不足的问题。这对于提高NeRF在实际应用中的鲁棒性和泛化能力具有重要意义。"}}
{"id": "2506.14988", "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits", "authors": ["Tianyi Xu", "Jiaxin Liu", "Zizhan Zheng"], "summary": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14988v1", "AI": {"title_translation": "多智能体多臂老虎机公平探测算法", "tldr": "本文提出了一个多智能体多臂老虎机(MA-MAB)框架，通过引入新颖的探测机制，在保证公平性的同时最大化系统性能，并在离线和在线设置中设计了高效算法，实验证明其优于基线方法。", "motivation": "在多智能体多臂老虎机(MA-MAB)框架中，如何在信息有限的情况下，在最大化整体系统性能的同时，确保智能体之间的公平结果是一个关键挑战。", "method": "本文提出了一个多智能体多臂老虎机(MA-MAB)框架。为解决信息有限的决策挑战，引入了一个新颖的探测框架，在分配前策略性地收集关于选定臂的信息。在奖励分布已知的离线设置中，利用次模性质设计了一个具有可证明性能界限的贪婪探测算法。对于更复杂的在线设置，开发了一种在保持公平性的同时实现次线性遗憾的算法。", "result": "在合成数据集和真实世界数据集上进行的广泛实验表明，所提出的方法优于基线方法，实现了更好的公平性和效率。", "conclusion": "本文提出的多智能体多臂老虎机(MA-MAB)框架及其探测算法，在确保公平性的同时最大化系统性能方面表现出色，并在离线和在线设置中均取得了显著效果。", "translation": "我们提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在确保智能体之间的公平结果，同时最大化整体系统性能。在这种设置中，关键挑战是在臂奖励信息有限的情况下进行决策。为了解决这个问题，我们引入了一个新颖的探测框架，在分配之前策略性地收集关于选定臂的信息。在奖励分布已知的离线设置中，我们利用次模性质设计了一个具有可证明性能界限的贪婪探测算法。对于更复杂的在线设置，我们开发了一种在保持公平性的同时实现次线性遗憾的算法。在合成数据集和真实世界数据集上进行的广泛实验表明，我们的方法优于基线方法，实现了更好的公平性和效率。", "summary": "本文提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在解决在信息有限环境下，如何在最大化整体系统性能的同时确保智能体间公平性的问题。核心创新在于引入了一个新颖的探测机制，用于在决策前策略性地收集信息。针对离线设置，设计了基于次模性质的贪婪探测算法；针对在线设置，开发了能实现次线性遗憾并保持公平性的算法。实验结果表明，该方法在公平性和效率上均优于现有基线。", "keywords": "多智能体多臂老虎机, 公平算法, 探测框架, 在线学习, 次模优化", "comments": "本文的创新点在于将“探测”机制引入多智能体多臂老虎机问题，以应对信息有限下的公平性与性能权衡。通过在分配前主动收集信息，提高了决策质量。离线和在线设置下的算法设计，以及实验验证，都显示了该方法的有效性和优越性。这对于实际应用中需要平衡性能和公平性的多智能体系统具有重要意义。"}}
{"id": "2506.15504", "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge", "authors": ["Li Zheng", "Sihang Wang", "Hao Fei", "Zuquan Peng", "Fei Li", "Jianming Fu", "Chong Teng", "Donghong Ji"], "summary": "Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.", "comment": "Accepted by ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15504v1", "AI": {"title_translation": "基于双向动态交互和情感知识的夸张与隐喻检测增强", "tldr": "提出EmoBi框架，利用双向动态交互和情感知识显著提升夸张和隐喻检测的性能。", "motivation": "夸张和隐喻检测对自然语言处理（NLP）任务具有重要意义，但由于其语义模糊性和表达多样性，识别它们极具挑战性。现有方法大多关注表面文本特征，忽略了夸张和隐喻的关联以及隐式情感对感知这些修辞手法的影响。", "method": "提出情感引导的夸张和隐喻检测框架EmoBi，包含情感分析模块（深入挖掘情感内涵）、基于情感的领域映射模块（识别目标和源领域以更深入理解隐式含义）和双向动态交互模块（实现夸张和隐喻的相互促进）。同时设计了验证机制以确保检测准确性和可靠性。", "result": "EmoBi在四个数据集上的表现均优于所有基线方法。与当前最新技术（SoTA）相比，在TroFi数据集上夸张检测的F1分数提高了28.1%，在HYPO-L数据集上隐喻检测的F1分数提高了23.1%。", "conclusion": "深入分析表明，所提出的方法在推进夸张和隐喻检测方面具有有效性和潜力。", "translation": "基于文本的夸张和隐喻检测对自然语言处理（NLP）任务具有重要意义。然而，由于其语义模糊性和表达多样性，识别它们相当具有挑战性。现有方法大多关注表面文本特征，忽略了夸张和隐喻的关联以及隐式情感对感知这些修辞手法的影响。为了实现这些假设，我们提出了一个基于双向动态交互的情感引导夸张和隐喻检测框架（EmoBi）。首先，情感分析模块深入挖掘夸张和隐喻背后的情感内涵。其次，基于情感的领域映射模块识别目标和源领域，以更深入地理解夸张和隐喻的隐含意义。最后，双向动态交互模块使夸张和隐喻之间能够相互促进。同时，设计了一个验证机制以确保检测的准确性和可靠性。实验表明，EmoBi在四个数据集上的表现均优于所有基线方法。具体而言，与当前的最新技术（SoTA）相比，在TroFi数据集上夸张检测的F1分数提高了28.1%，在HYPO-L数据集上隐喻检测的F1分数提高了23.1%。这些结果在深入分析的支持下，强调了我们方法在推进夸张和隐喻检测方面的有效性和潜力。", "summary": "该论文提出了一个名为EmoBi的情感引导框架，用于增强夸张和隐喻检测。该框架通过情感分析、基于情感的领域映射以及夸张与隐喻之间的双向动态交互来解决现有方法忽视语义关联和情感影响的问题。实验结果表明，EmoBi在多个数据集上显著优于现有基线方法，F1分数在特定数据集上分别提升了28.1%和23.1%，证明了其在提高检测准确性方面的有效性。", "keywords": "夸张检测, 隐喻检测, 情感分析, 双向交互, 自然语言处理", "comments": "该论文的创新点在于引入了情感知识和夸张与隐喻之间的双向动态交互，解决了现有方法仅关注表面特征的局限性。其提出的EmoBi框架在实验中取得了显著的性能提升，尤其是在F1分数上的大幅提高，表明了该方法在自然语言处理中修辞手法检测领域的巨大潜力。这对于理解文本深层含义和情感表达具有重要意义。"}}
{"id": "2506.15244", "title": "Retrospective Memory for Camouflaged Object Detection", "authors": ["Chenxi Zhang", "Jiayun Wu", "Qing Zhang", "Yazhe Zhai", "Youwei Pang"], "summary": "Camouflaged object detection (COD) primarily focuses on learning subtle yet\ndiscriminative representations from complex scenes. Existing methods\npredominantly follow the parametric feedforward architecture based on static\nvisual representation modeling. However, they lack explicit mechanisms for\nacquiring historical context, limiting their adaptation and effectiveness in\nhandling challenging camouflage scenes. In this paper, we propose a\nrecall-augmented COD architecture, namely RetroMem, which dynamically modulates\ncamouflage pattern perception and inference by integrating relevant historical\nknowledge into the process. Specifically, RetroMem employs a two-stage training\nparadigm consisting of a learning stage and a recall stage to construct,\nupdate, and utilize memory representations effectively. During the learning\nstage, we design a dense multi-scale adapter (DMA) to improve the pretrained\nencoder's capability to capture rich multi-scale visual information with very\nfew trainable parameters, thereby providing foundational inferences. In the\nrecall stage, we propose a dynamic memory mechanism (DMM) and an inference\npattern reconstruction (IPR). These components fully leverage the latent\nrelationships between learned knowledge and current sample context to\nreconstruct the inference of camouflage patterns, thereby significantly\nimproving the model's understanding of camouflage scenes. Extensive experiments\non several widely used datasets demonstrate that our RetroMem significantly\noutperforms existing state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15244v1", "AI": {"title_translation": "用于伪装目标检测的回溯记忆", "tldr": "本文提出了一种名为RetroMem的召回增强型伪装目标检测架构，通过集成历史知识来动态调制伪装模式感知和推理，显著优于现有SOTA方法。", "motivation": "现有伪装目标检测方法主要基于静态视觉表示建模的参数前馈架构，但缺乏获取历史上下文的显式机制，限制了它们在处理复杂伪装场景时的适应性和有效性。", "method": "本文提出了一种召回增强型伪装目标检测架构RetroMem。它采用两阶段训练范式：学习阶段和召回阶段。在学习阶段，设计了密集多尺度适配器（DMA）以提高预训练编码器捕获丰富多尺度视觉信息的能力。在召回阶段，提出了动态记忆机制（DMM）和推理模式重建（IPR），以利用学习到的知识和当前样本上下文之间的潜在关系来重建伪装模式的推理。", "result": "在多个广泛使用的数据集上进行的广泛实验表明，RetroMem显著优于现有最先进的方法。", "conclusion": "RetroMem通过整合历史知识，有效解决了现有伪装目标检测方法缺乏历史上下文的问题，显著提升了模型在复杂伪装场景下的检测性能。", "translation": "伪装目标检测（COD）主要侧重于从复杂场景中学习细微但具有区分性的表示。现有方法主要遵循基于静态视觉表示建模的参数前馈架构。然而，它们缺乏获取历史上下文的显式机制，限制了它们在处理具有挑战性的伪装场景时的适应性和有效性。在本文中，我们提出了一种召回增强型COD架构，即RetroMem，它通过将相关的历史知识整合到过程中，动态地调节伪装模式的感知和推理。具体来说，RetroMem采用两阶段训练范式，包括学习阶段和召回阶段，以有效地构建、更新和利用记忆表示。在学习阶段，我们设计了一个密集多尺度适配器（DMA），以提高预训练编码器捕获丰富多尺度视觉信息的能力，且只需极少的训练参数，从而提供基础推理。在召回阶段，我们提出了动态记忆机制（DMM）和推理模式重建（IPR）。这些组件充分利用学习到的知识和当前样本上下文之间的潜在关系来重建伪装模式的推理，从而显著提高了模型对伪装场景的理解。在几个广泛使用的数据集上进行的广泛实验表明，我们的RetroMem显著优于现有最先进的方法。", "summary": "本文提出了一种名为RetroMem的召回增强型伪装目标检测（COD）架构，旨在解决现有COD方法缺乏历史上下文的问题。RetroMem采用双阶段训练范式，包括学习阶段和召回阶段。在学习阶段，通过密集多尺度适配器（DMA）增强编码器捕获多尺度信息的能力。在召回阶段，利用动态记忆机制（DMM）和推理模式重建（IPR）整合历史知识，动态调制伪装模式感知和推理。实验证明，RetroMem在多个数据集上显著优于现有SOTA方法。", "keywords": "伪装目标检测, 回溯记忆, 召回增强, 动态记忆机制, 多尺度适配器", "comments": "RetroMem的创新之处在于引入了“回溯记忆”的概念，通过动态记忆机制和推理模式重建，使得模型能够利用历史上下文信息来提升伪装目标检测的性能，这突破了传统静态视觉表示的局限性。其两阶段训练范式也为如何有效构建和利用记忆表示提供了新思路。"}}
{"id": "2506.15019", "title": "Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment", "authors": ["Yue Gao"], "summary": "Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.", "comment": "Accepted to IJCAI2025 AI4TS", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15019v1", "AI": {"title_translation": "用于败血症治疗离线强化学习的稳定CDE自编码器与敏锐度正则化", "tldr": "本研究提出并验证了稳定的受控微分方程（CDE）自编码器结合敏锐度正则化，可以为败血症治疗中的离线强化学习提供高质量的状态表示和优越的策略性能，强调了训练稳定性的重要性。", "motivation": "有效的败血症治疗强化学习依赖于从不规则ICU时间序列中学习稳定的、有临床意义的状态表示。先前的研究忽视了序列表示中训练不稳定性的关键挑战及其对策略性能的有害影响。", "method": "本研究通过两种关键因素实现了强大的强化学习策略：1) 通过早期停止或稳定化方法确保训练稳定性；2) 通过与临床评分（SOFA、SAPS-II、OASIS）进行相关性正则化，强制执行敏锐度感知的表示。", "result": "在MIMIC-III败血症队列上的实验表明，稳定的CDE自编码器产生的表示与敏锐度评分高度相关，并使强化学习策略具有卓越的性能（WIS回报 > 0.9）。相反，不稳定的CDE表示导致表示质量下降和策略失败（WIS回报 ~ 0）。潜在空间的可视化显示，稳定的CDE不仅能区分幸存者和非幸存者的轨迹，还能揭示清晰的敏锐度评分梯度，而不稳定的训练未能捕捉到这两种模式。", "conclusion": "这些发现为在临床强化学习中使用CDE编码不规则医疗时间序列提供了实用指导，强调了序列表示学习中训练稳定性的必要性。", "translation": "有效的败血症治疗强化学习取决于从不规则ICU时间序列中学习稳定的、有临床意义的状态表示。虽然以前的工作已经探索了针对这项任务的表示学习，但序列表示中训练不稳定性的关键挑战及其对策略性能的有害影响却被忽视了。这项工作表明，当满足两个关键因素时，受控微分方程（CDE）状态表示可以实现强大的强化学习策略：(1) 通过早期停止或稳定方法确保训练稳定性，以及 (2) 通过与临床评分（SOFA、SAPS-II、OASIS）进行相关性正则化来强制执行敏锐度感知的表示。在MIMIC-III败血症队列上的实验表明，稳定的CDE自编码器产生的表示与敏锐度评分高度相关，并使强化学习策略具有卓越的性能（WIS回报 > 0.9）。相反，不稳定的CDE表示导致表示质量下降和策略失败（WIS回报 ~ 0）。潜在空间的可视化显示，稳定的CDE不仅能区分幸存者和非幸存者的轨迹，还能揭示清晰的敏锐度评分梯度，而不稳定的训练未能捕捉到这两种模式。这些发现为在临床强化学习中使用CDE编码不规则医疗时间序列提供了实用指导，强调了序列表示学习中训练稳定性的必要性。", "summary": "本研究提出了一种用于败血症治疗离线强化学习的稳定CDE自编码器，通过结合训练稳定性方法和与临床评分相关的敏锐度正则化，解决了传统序列表示学习中训练不稳定的问题。实验结果表明，该方法能够学习到与临床敏锐度高度相关的稳定状态表示，并显著提升了强化学习策略的性能，为临床强化学习中不规则医疗时间序列的编码提供了实用指导。", "keywords": "CDE自编码器, 强化学习, 败血症治疗, 状态表示, 训练稳定性", "comments": "这项研究的创新之处在于明确指出了训练稳定性在基于CDE的序列表示学习中的关键作用，并提出了结合敏锐度正则化的方法来提高表示的临床意义和策略性能。其重要性在于为处理不规则医疗时间序列和提升临床强化学习的可靠性提供了具体且有效的指导方针。"}}
{"id": "2506.15522", "title": "Lessons from Training Grounded LLMs with Verifiable Rewards", "authors": ["Shang Hong Sim", "Tej Deep Pala", "Vernon Toh", "Hai Leong Chieu", "Amir Zadeh", "Chuan Li", "Navonil Majumder", "Soujanya Poria"], "summary": "Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15522v1", "AI": {"title_translation": "使用可验证奖励训练接地大型语言模型的经验教训", "tldr": "本文通过强化学习和内部推理，并使用GRPO方法和可验证的基于结果的奖励，显著提高了大型语言模型在生成可信、有根据的回复方面的表现，尤其是在处理无法回答的查询和生成良好引用的回复方面。", "motivation": "大型语言模型（LLMs）生成有依据且值得信赖的回复仍然是一个关键挑战。尽管基于引用的检索增强生成（RAG）有前景，但经过指令微调的模型在简单场景中也常失败，例如遗漏明确答案、错误引用或在有证据时拒绝回答。", "method": "本文通过强化学习（RL）和内部推理来增强LLMs的接地能力。研究使用了GRPO（Group Relative Policy Optimization）方法，利用可验证的、基于结果的奖励来训练模型，这些奖励针对答案的正确性、引用的充分性和拒绝的质量，而无需金标准推理轨迹或昂贵的标注。此外，研究还探索了两阶段训练设置，首先优化答案和引用行为，然后是拒绝行为。同时，也重新审视了通过GPT-4蒸馏进行的指令微调，并将其与GRPO结合。", "result": "推理增强的模型显著优于仅指令微调的版本，特别是在处理无法回答的查询和生成良好引用的回复方面。两阶段训练设置（先优化答案和引用行为，然后是拒绝）通过稳定学习信号进一步改善了接地能力。将GPT-4蒸馏与GRPO结合，提高了在长篇、生成式问答任务上的性能。", "conclusion": "研究结果强调了推理、阶段性优化和结果驱动型强化学习对于构建更可验证和更可靠的大型语言模型的价值。", "translation": "生成有依据且值得信赖的回复仍然是大型语言模型（LLMs）面临的一个关键挑战。虽然基于引用的检索增强生成（RAG）具有前景，但经过指令微调的模型即使在简单的场景中也常常失败：遗漏明确陈述的答案、错误引用或在有证据时拒绝回答。在这项工作中，我们探索了强化学习（RL）和内部推理如何增强LLMs的接地能力。我们使用GRPO（Group Relative Policy Optimization）方法，利用可验证的、基于结果的奖励来训练模型，这些奖励针对答案的正确性、引用的充分性和拒绝的质量，而无需金标准推理轨迹或昂贵的标注。通过在ASQA、QAMPARI、ELI5和ExpertQA上的全面实验，我们表明推理增强的模型显著优于仅指令微调的版本，尤其是在处理无法回答的查询和生成良好引用的回复方面。一个两阶段的训练设置，首先优化答案和引用行为，然后是拒绝，通过稳定学习信号进一步改善了接地能力。此外，我们重新审视了通过GPT-4蒸馏进行的指令微调，发现将其与GRPO结合可以提高在长篇、生成式问答任务上的性能。总的来说，我们的发现强调了推理、阶段性优化和结果驱动型强化学习对于构建更可验证和更可靠的LLMs的价值。", "summary": "本文探讨了如何通过强化学习（RL）和内部推理来提升大型语言模型（LLMs）生成有依据且值得信赖回复的能力。研究采用GRPO方法，利用可验证的成果导向奖励（包括答案正确性、引用充分性和拒绝质量）来训练模型，无需昂贵的标注。实验证明，推理增强模型在处理不可回答查询和生成良好引用回复方面显著优于仅指令微调的模型。此外，两阶段训练和结合GPT-4蒸馏与GRPO进一步提升了模型性能。研究强调了推理、分阶段优化和结果驱动型RL对于构建更可靠LLMs的重要性。", "keywords": "大型语言模型, 强化学习, 接地, 可验证奖励, GRPO", "comments": "本文的创新点在于将强化学习（GRPO）与内部推理相结合，以解决大型语言模型在生成有依据和可信回复方面的核心挑战，特别是在没有昂贵标注的情况下实现这一点。其提出的两阶段训练和结合GPT-4蒸馏的方法，为提升模型在复杂问答任务上的性能提供了有效途径，对于提高LLMs的可靠性和可验证性具有重要意义。"}}
{"id": "2506.15260", "title": "Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing", "authors": ["Adrian Poniatowski", "Natalie Gentner", "Manuel Barusco", "Davide Dalle Pezze", "Samuele Salti", "Gian Antonio Susto"], "summary": "In the semiconductor sector, due to high demand but also strong and\nincreasing competition, time to market and quality are key factors in securing\nsignificant market share in various application areas. Thanks to the success of\ndeep learning methods in recent years in the computer vision domain, Industry\n4.0 and 5.0 applications, such as defect classification, have achieved\nremarkable success. In particular, Domain Adaptation (DA) has proven highly\neffective since it focuses on using the knowledge learned on a (source) domain\nto adapt and perform effectively on a different but related (target) domain. By\nimproving robustness and scalability, DA minimizes the need for extensive\nmanual re-labeling or re-training of models. This not only reduces\ncomputational and resource costs but also allows human experts to focus on\nhigh-value tasks. Therefore, we tested the efficacy of DA techniques in\nsemi-supervised and unsupervised settings within the context of the\nsemiconductor field. Moreover, we propose the DBACS approach, a\nCycleGAN-inspired model enhanced with additional loss terms to improve\nperformance. All the approaches are studied and validated on real-world\nElectron Microscope images considering the unsupervised and semi-supervised\nsettings, proving the usefulness of our method in advancing DA techniques for\nthe semiconductor field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15260v1", "AI": {"title_translation": "半导体制造中缺陷图像分类的领域适应", "tldr": "本文研究了半导体制造缺陷图像分类中的领域适应技术，并提出了一种名为 DBACS 的新方法，证明了其在无监督和半监督设置下的有效性。", "motivation": "半导体行业对上市时间和质量要求高，缺陷分类是关键。深度学习在计算机视觉领域取得成功，特别是领域适应（DA）能有效利用源域知识适应目标域，减少手动标注和训练成本，提高鲁棒性和可扩展性。因此，需要测试和改进DA技术在半导体缺陷分类中的应用。", "method": "本文在半监督和无监督设置下测试了领域适应（DA）技术在半导体领域的有效性。此外，提出了一种名为 DBACS 的方法，该方法受 CycleGAN 启发，并增加了额外的损失项以提高性能。所有方法均在真实的电子显微镜图像上进行研究和验证。", "result": "领域适应技术在半导体制造缺陷分类的半监督和无监督设置中被证明是有效的。提出的 DBACS 方法在实际电子显微镜图像上的验证结果表明其在推动半导体领域领域适应技术方面具有实用性。", "conclusion": "领域适应技术，特别是本文提出的 DBACS 方法，在半导体制造的缺陷图像分类中具有显著的应用价值，能够有效提高模型的鲁棒性和可扩展性，并减少资源成本。", "translation": "在半导体领域，由于高需求以及日益激烈的竞争，上市时间和质量是确保在各个应用领域获得显著市场份额的关键因素。近年来，得益于深度学习方法在计算机视觉领域的成功，工业4.0和5.0应用，例如缺陷分类，取得了显著成就。特别是，领域适应（DA）已被证明高效，因为它侧重于利用在（源）领域学到的知识，以适应并在不同但相关的（目标）领域有效执行。通过提高鲁棒性和可扩展性，DA 最大限度地减少了对大量手动重新标注或模型重新训练的需求。这不仅降低了计算和资源成本，还使人类专家能够专注于高价值任务。因此，我们在半导体领域背景下，在半监督和无监督设置中测试了 DA 技术的有效性。此外，我们提出了一种受 CycleGAN 启发并增加了额外损失项以提高性能的 DBACS 方法。所有方法都在考虑无监督和半监督设置的真实电子显微镜图像上进行了研究和验证，证明了我们方法在推动半导体领域 DA 技术方面的实用性。", "summary": "本文探讨了领域适应（DA）技术在半导体制造缺陷图像分类中的应用，以应对行业对上市时间和质量的高要求。研究测试了DA在半监督和无监督环境下的有效性，并提出了一种受CycleGAN启发的DBACS方法，该方法通过额外的损失项增强了性能。在真实电子显微镜图像上的验证表明，所提出的方法在提高半导体领域DA技术的鲁棒性和可扩展性方面具有显著价值。", "keywords": "领域适应, 缺陷分类, 半导体制造, 深度学习, DBACS", "comments": "本文关注了半导体制造中一个重要的实际问题，即缺陷分类。通过引入领域适应技术，特别是提出的DBACS方法，有效地解决了数据标注成本高、模型泛化能力差的痛点。其创新点在于结合CycleGAN思想并加入额外损失项，提高了在无监督和半监督场景下的性能。这项工作对于推动工业界深度学习应用的落地具有重要意义。"}}
{"id": "2506.15021", "title": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models", "authors": ["Gyuhak Kim", "Sumiran Singh Thakur", "Su Min Park", "Wei Wei", "Yujia Bao"], "summary": "Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15021v1", "AI": {"title_translation": "SFT-GO：基于分组优化的LLMs监督微调", "tldr": "SFT-GO是一种新的监督微调方法，通过对重要性不同的token组进行差异化优化，显著提升了LLMs在各种基准测试上的性能。", "motivation": "现有监督微调（SFT）方法将每个训练实例视为统一序列，对所有token一视同仁，忽略了只有部分token包含关键任务特定信息的事实，导致优化效率不高。", "method": "SFT-GO（Supervised Fine-Tuning with Group Optimization）根据token的重要性对其进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化LLM。该机制自适应地强调最具挑战性的token组，并引导模型更好地处理不同的组分布。", "result": "SFT-GO在流行的LLM基准测试上始终优于基线方法。这些改进在各种数据集和基础模型上都成立，证明了该方法的鲁棒性和有效性。", "conclusion": "SFT-GO通过引入分组优化机制，有效解决了传统SFT方法中token重要性差异被忽视的问题，显著提升了大型语言模型的性能和泛化能力。", "translation": "监督微调（SFT）已成为调整大型语言模型（LLM）以符合人类期望和特定下游任务的重要步骤。然而，现有的SFT方法通常将每个训练实例视为一个统一序列，对所有token一视同仁，无论其相关性如何。这忽视了通常只有一部分token包含关键的、任务特定的信息这一事实。为了解决这一局限性，我们引入了SFT-GO（Supervised Fine-Tuning with Group Optimization），这是一种根据token重要性对其进行不同处理的新方法。SFT-GO根据每个样本中token的重要性值对其进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化LLM。这种机制自适应地强调最具挑战性的token组，并引导模型更好地处理不同的组分布，从而改善整体学习动态。我们提供了SFT-GO收敛速度的理论分析，证明了其效率。在经验上，我们应用SFT-GO与三种不同的token分组策略，结果表明，使用SFT-GO训练的模型在流行的LLM基准测试中始终优于基线方法。这些改进在各种数据集和基础模型上都成立，证明了我们方法的鲁棒性和有效性。", "summary": "本文提出了SFT-GO，一种新的监督微调方法，旨在解决现有SFT方法中对所有token一视同仁的局限性。SFT-GO根据token的重要性对其进行分组，并通过加权组合最差组损失和标准交叉熵损失来优化模型，从而更关注挑战性强的token组。理论分析证明了其收敛效率，实验结果显示，SFT-GO在多个LLM基准测试上持续超越现有基线方法，展现了其在不同数据集和基础模型上的鲁棒性和有效性。", "keywords": "监督微调, 大语言模型, 分组优化, SFT-GO, token重要性", "comments": "SFT-GO的创新点在于引入了“分组优化”的概念，解决了传统SFT中对所有token同等对待的不足。通过差异化处理不同重要性的token组，尤其是强调最难的组，它能够更有效地引导模型学习，提升了微调的精度和效率。这种思想对于优化大型模型训练具有重要意义，尤其是在处理长序列或信息密度不均的任务中。"}}
{"id": "2506.15545", "title": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models", "authors": ["Bailin Wang", "Chang Lan", "Chong Wang", "Ruoming Pang"], "summary": "Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.", "comment": "9 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15545v1", "AI": {"title_translation": "RATTENTION：迈向局部-全局注意力模型中的最小滑动窗口大小", "tldr": "RATTENTION是一种新的局部-全局注意力模型，通过结合线性注意力机制捕获窗口外信息，实现了在保持性能的同时大幅减小滑动窗口大小，从而提升了短上下文和长上下文场景的效率。", "motivation": "当前局部-全局注意力模型在窗口大小选择上存在帕累托权衡：大窗口效率提升不明显，小窗口性能下降。核心动机是解决局部注意力固有的局限性——完全忽略定义窗口外的token，旨在即使在短上下文场景中也能实现效率提升。", "method": "本文提出了RATTENTION，它是局部注意力的一种变体，集成了专门的线性注意力机制，旨在捕获来自窗口外token的信息。通过专门的内核实现和减小的窗口大小，RATTENTION保持了与现有最先进方法相当的训练速度。", "result": "在3B和12B规模的预训练实验表明，RATTENTION在性能和效率之间实现了卓越的帕累托权衡。最佳情况下，RATTENTION在窗口大小仅为512时，在各种设置下均能与全注意力模型保持一致的性能。此外，RATTENTION中线性注意力组件固有的递归特性有助于增强长上下文性能，这在RULER基准测试中得到了验证。这些改进并未影响训练效率。", "conclusion": "RATTENTION通过引入线性注意力机制来处理窗口外信息，成功地在局部-全局注意力模型中实现了更小的滑动窗口大小，同时保持甚至提升了性能（包括长上下文能力）和训练效率，为高效的Transformer替代方案提供了一个有前景的方向。", "translation": "局部-全局注意力模型最近成为标准Transformer的引人注目的替代品，有望提高训练和推理效率。然而，窗口大小的关键选择呈现出帕累托权衡：较大的窗口保持与全注意力相似的性能，但在短上下文场景中效率提升最小，而较小的窗口可能导致性能下降。当前的模型，如Gemma2和Mistral，采用保守的窗口大小（例如，8192预训练长度中的4096）以保持性能。这项工作研究了改变这种帕累托边界的策略，使局部-全局模型即使在短上下文条件下也能实现效率增益。我们的核心动机是解决局部注意力的内在局限性——它完全忽略了定义窗口之外的token。我们探索了RATTENTION，这是一种局部注意力的变体，与专门的线性注意力机制相结合，旨在捕获来自这些窗口外token的信息。在3B和12B规模的预训练实验表明，RATTENTION在性能和效率之间实现了卓越的帕累托权衡。作为最佳选择，窗口大小仅为512的RATTENTION在各种设置下始终与全注意力模型的性能相匹配。此外，RATTENTION中线性注意力组件固有的递归特性有助于增强长上下文性能，这在RULER基准测试中得到了验证。至关重要的是，这些改进并未损害训练效率；得益于专门的内核实现和减小的窗口大小，RATTENTION保持了与现有最先进方法相当的训练速度。", "summary": "RATTENTION是一种新型的局部-全局注意力模型，旨在解决传统局部注意力模型中窗口大小选择的帕累托权衡问题。通过将局部注意力与专门的线性注意力机制相结合，RATTENTION能够捕获窗口外的上下文信息。实验证明，RATTENTION在保持或超越全注意力模型性能的同时，显著减小了滑动窗口大小（例如，降至512），从而在短上下文和长上下文场景中均实现了更高的效率。此外，其训练速度与现有先进方法相当，显示出在效率和性能之间更优越的平衡。", "keywords": "RATTENTION, 局部-全局注意力, 滑动窗口, 线性注意力, 效率", "comments": "RATTENTION的创新之处在于其将局部注意力与线性注意力结合，有效解决了局部注意力对窗口外信息忽略的问题。这使得模型能够在保持高性能的同时，大幅减小滑动窗口大小，从而在计算效率上取得显著突破，特别是在短上下文和长上下文场景中。该工作对于推动高效Transformer架构的发展具有重要意义，为未来的大模型训练和部署提供了新的思路。"}}
{"id": "2506.15025", "title": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size", "authors": ["Soufiane Hayou", "Liyuan Liu"], "summary": "Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.", "comment": "TD,LR: How to set the learning rate for emebdding layer in LLMs?", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15025v1", "AI": {"title_translation": "LLM中最优嵌入学习率：词汇量大小的影响", "tldr": "本文理论分析了词汇量大小对LLM训练动态的影响，发现当词汇量增大时，训练动态介于$\\mu$P和“大词汇量（LV）”两种机制之间，LV机制下最优嵌入学习率与隐藏层学习率之比应大致按$\\Theta(\\sqrt{width})$缩放，这与$\\mu$P预测的$\\Theta(width)$不同，并通过实验验证了该理论。", "motivation": "预训练大型语言模型成本高昂，而现有的参数化方法如$\\mu$P在应用于LLMs时出现了相互矛盾的观察结果。$\\mu$P理论的一个局限性在于其假设输入维度（词汇量大小）在宽度趋于无穷时是固定的，这在实践中并不现实，因为词汇量通常远大于宽度。因此，需要研究词汇量大小对训练动态的影响。", "method": "本文通过理论分析词汇量大小对训练动态的影响，并提出了一种新的机制（大词汇量LV机制）。随后，通过进行多项实验来验证该理论，并从头预训练一个10亿参数模型以展示所提出嵌入学习率缩放规则的益处。", "result": "研究发现，随着词汇量的增加，训练动态在$\\mu$P机制和我们称之为“大词汇量（LV）机制”之间插值。在LV机制中，最优的嵌入学习率与隐藏学习率之比应大致按$\\Theta(\\sqrt{width})$缩放，这与$\\mu$P预测的$\\Theta(width)$不同，但与先前文献中报道的经验结果惊人地接近。实验验证了理论的正确性，并展示了新缩放规则对嵌入学习率的益处。", "conclusion": "当词汇量增大时，LLM的训练动态会从$\\mu$P机制转向大词汇量（LV）机制，在此机制下，最优的嵌入学习率与隐藏学习率之比应大致按$\\Theta(\\sqrt{width})$缩放，这与$\\mu$P的预测不同，但更符合实际经验。这一发现为LLM的有效训练提供了新的指导。", "translation": "预训练大型语言模型是一个成本高昂的过程。为了使这一过程更高效，已经提出了几种方法来优化模型架构/参数化和硬件使用。在参数化方面，$\\mu$P（最大更新参数化）以一种使超参数（HPs）能够随宽度（嵌入维度）转移的方式参数化模型权重和学习率（LR）：超参数可以针对小型模型进行调整，并用于大型模型而无需额外调整。尽管$\\mu$P在实践中表现出了令人印象深刻的结果，但最近的经验研究在应用于LLMs时报告了相互矛盾的观察结果。$\\mu$P理论的一个局限性在于，在宽度趋于无穷大时，输入维度（LLMs中的词汇量大小）被认为是固定的。这不符合实际，因为在实践中词汇量大小通常远大于宽度。在这项工作中，我们提供了词汇量大小对训练动态影响的理论分析，并随后表明，随着词汇量大小的增加，训练动态在$\\mu$P机制和我们称之为“大词汇量（LV）机制”之间插值，其中最优缩放规则与$\\mu$P预测的不同。我们的分析揭示，在LV机制中，最优的嵌入学习率与隐藏学习率之比应大致按$\\Theta(\\sqrt{width})$缩放，这与先前文献中报道的经验结果惊人地接近，并且与$\\mu$P预测的$\\Theta(width)$比率不同。我们进行了多项实验来验证我们的理论，并从头预训练了一个10亿参数模型，以展示我们建议的嵌入学习率缩放规则的益处。", "summary": "本文研究了词汇量大小对大型语言模型（LLMs）训练动态中最优嵌入学习率的影响。针对$\\mu$P理论在处理大词汇量LLMs时存在的局限性，作者进行了理论分析，发现随着词汇量的增加，训练动态在$\\mu$P机制和一种新的“大词汇量（LV）机制”之间转变。在LV机制下，最优的嵌入学习率与隐藏层学习率之比应大致按$\\Theta(\\sqrt{width})$缩放，这与$\\mu$P预测的$\\Theta(width)$不同，但与经验观察吻合。通过实验验证了这一理论，并展示了新缩放规则的有效性，为LLM的预训练提供了更准确的学习率优化指导。", "keywords": "大型语言模型, 嵌入学习率, 词汇量大小, $\\mu$P, 训练动态", "comments": "本文的创新之处在于识别并理论分析了“大词汇量（LV）机制”，揭示了当词汇量远大于模型宽度时，LLM中嵌入学习率的最优缩放规则与传统的$\\mu$P理论预测不同。这一发现解决了$\\mu$P在LLM应用中出现矛盾观察的问题，为LLM的高效预训练提供了更精确的超参数调优指导，具有重要的实践意义。其理论分析与实验验证相结合，增强了研究结果的说服力。"}}
{"id": "2506.15553", "title": "Approximating Language Model Training Data from Weights", "authors": ["John X. Morris", "Junjie Oscar Yin", "Woojeong Kim", "Vitaly Shmatikov", "Alexander M. Rush"], "summary": "Modern language models often have open weights but closed training data. We\nformalize the problem of data approximation from model weights and propose\nseveral baselines and metrics. We develop a gradient-based approach that\nselects the highest-matching data from a large public text corpus and show its\neffectiveness at recovering useful data given only weights of the original and\nfinetuned models. Even when none of the true training data is known, our method\nis able to locate a small subset of public Web documents can be used to train a\nmodel to close to the original model performance given models trained for both\nclassification and supervised-finetuning. On the AG News classification task,\nour method improves performance from 65% (using randomly selected data) to 80%,\napproaching the expert benchmark of 88%. When applied to a model trained with\nSFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,\ncompared to an expert LLAMA model's perplexity of 2.0.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15553v1", "AI": {"title_translation": "从模型权重近似语言模型训练数据", "tldr": "本文提出一种基于梯度的方法，仅通过模型权重来近似语言模型的训练数据，并证明其能有效恢复有用数据，用于训练出接近原始模型性能的新模型。", "motivation": "现代语言模型通常权重公开但训练数据不公开，因此需要解决从模型权重近似训练数据的问题。", "method": "本文将从模型权重近似数据的过程形式化，并提出基线和度量标准。开发了一种基于梯度的方法，从大型公共文本语料库中选择最高匹配的数据。", "result": "该方法能有效恢复有用数据，即使在不知道真实训练数据的情况下，也能找到一小部分公共网络文档，用于训练出接近原始模型性能的模型。在AG News分类任务上，性能从随机选择数据的65%提高到80%。在MSMARCO网络文档上，困惑度从3.3降低到2.3。", "conclusion": "该方法能够从公开的模型权重中有效近似并恢复有用的训练数据，从而训练出性能接近原始模型的模型。", "translation": "现代语言模型通常权重公开但训练数据不公开。我们形式化了从模型权重近似数据的任务，并提出了几种基线和度量方法。我们开发了一种基于梯度的方法，从大型公共文本语料库中选择最高匹配的数据，并展示了其在仅给定原始模型和微调模型权重的情况下恢复有用数据的有效性。即使在不知道真实训练数据的情况下，我们的方法也能够找到一小部分公共网络文档，这些文档可以用于训练模型，使其性能接近原始模型，适用于分类和监督微调模型。在AG News分类任务上，我们的方法将性能从65%（使用随机选择数据）提高到80%，接近专家基准的88%。当应用于在MSMARCO网络文档上通过SFT训练的模型时，我们的方法将困惑度从3.3降低到2.3，而专家LLAMA模型的困惑度为2.0。", "summary": "本文研究了从公开的语言模型权重中近似其封闭训练数据的问题。作者提出了一种基于梯度的方法，该方法能从大型公共文本语料库中识别出与模型训练数据高度匹配的子集。实验证明，该方法即使在不知道原始训练数据的情况下，也能有效恢复出可用于训练新模型的数据，并且新模型在分类和监督微调任务上的表现能接近原始模型，例如在AG News任务上性能从65%提升到80%，在MSMARCO任务上困惑度从3.3降至2.3。", "keywords": "语言模型, 训练数据近似, 模型权重, 梯度方法, 数据恢复", "comments": "这项工作具有重要的意义，因为它解决了语言模型训练数据不透明的问题，为理解和复现模型行为提供了新途径。其创新之处在于提出了一种基于梯度的方法来从模型权重反向推导训练数据。这对于模型审计、偏见检测以及在数据受限情况下复现模型性能具有潜在价值。"}}
{"id": "2506.15279", "title": "BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via Bezier Curve Refinement", "authors": ["Qian Li", "Feng Liu", "Shuojue Yang", "Daiyun Shen", "Yueming Jin"], "summary": "Laparoscopic liver surgery, while minimally invasive, poses significant\nchallenges in accurately identifying critical anatomical structures. Augmented\nreality (AR) systems, integrating MRI/CT with laparoscopic images based on\n2D-3D registration, offer a promising solution for enhancing surgical\nnavigation. A vital aspect of the registration progress is the precise\ndetection of curvilinear anatomical landmarks in laparoscopic images. In this\npaper, we propose BCRNet (Bezier Curve Refinement Net), a novel framework that\nsignificantly enhances landmark detection in laparoscopic liver surgery\nprimarily via the Bezier curve refinement strategy. The framework starts with a\nMulti-modal Feature Extraction (MFE) module designed to robustly capture\nsemantic features. Then we propose Adaptive Curve Proposal Initialization\n(ACPI) to generate pixel-aligned Bezier curves and confidence scores for\nreliable initial proposals. Additionally, we design the Hierarchical Curve\nRefinement (HCR) mechanism to enhance these proposals iteratively through a\nmulti-stage process, capturing fine-grained contextual details from multi-scale\npixel-level features for precise Bezier curve adjustment. Extensive evaluations\non the L3D and P2ILF datasets demonstrate that BCRNet outperforms\nstate-of-the-art methods, achieving significant performance improvements. Code\nwill be available.", "comment": "Accepted at MICCAI 2025, 11 pages, 2 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15279v1", "AI": {"title_translation": "BCRNet：通过贝塞尔曲线细化增强腹腔镜肝脏手术中的地标检测", "tldr": "BCRNet通过贝塞尔曲线细化显著提升腹腔镜肝脏手术中的曲线地标检测精度，在L3D和P2ILF数据集上超越现有方法。", "motivation": "腹腔镜肝脏手术中准确识别关键解剖结构具有挑战性。增强现实(AR)系统结合MRI/CT与腹腔镜图像进行2D-3D配准，是增强手术导航的有前景的解决方案。其中，精确检测腹腔镜图像中的曲线解剖地标是配准过程的关键环节。", "method": "本文提出了BCRNet（Bezier Curve Refinement Net）框架，通过贝塞尔曲线细化策略显著增强腹腔镜肝脏手术中的地标检测。该框架主要包括：1. 多模态特征提取(MFE)模块，用于鲁棒捕获语义特征。2. 自适应曲线提议初始化(ACPI)，用于生成像素对齐的贝塞尔曲线和置信度分数作为可靠的初始提议。3. 分层曲线细化(HCR)机制，通过多阶段过程迭代增强这些提议，从多尺度像素级特征中捕获细粒度上下文细节，以实现精确的贝塞尔曲线调整。", "result": "在L3D和P2ILF数据集上的广泛评估表明，BCRNet优于现有最先进的方法，取得了显著的性能改进。", "conclusion": "BCRNet通过其新颖的贝塞尔曲线细化策略，显著提高了腹腔镜肝脏手术中曲线解剖地标的检测精度，为增强手术导航提供了有效且优越的解决方案。", "translation": "腹腔镜肝脏手术虽然是微创手术，但在准确识别关键解剖结构方面提出了重大挑战。增强现实（AR）系统将MRI/CT与腹腔镜图像基于2D-3D配准相结合，为增强手术导航提供了一个有前景的解决方案。配准过程的一个重要方面是精确检测腹腔镜图像中的曲线解剖地标。在本文中，我们提出了BCRNet（Bezier Curve Refinement Net），这是一个新颖的框架，主要通过贝塞尔曲线细化策略显著增强腹腔镜肝脏手术中的地标检测。该框架首先通过一个多模态特征提取（MFE）模块，旨在鲁棒地捕获语义特征。然后，我们提出了自适应曲线提议初始化（ACPI）来生成像素对齐的贝塞尔曲线和置信度分数，以获得可靠的初始提议。此外，我们设计了分层曲线细化（HCR）机制，通过多阶段过程迭代增强这些提议，从多尺度像素级特征中捕获细粒度上下文细节，以进行精确的贝塞尔曲线调整。在L3D和P2ILF数据集上的广泛评估表明，BCRNet优于现有最先进的方法，取得了显著的性能改进。代码将可用。", "summary": "BCRNet是一个新颖的深度学习框架，旨在通过贝塞尔曲线细化策略显著提升腹腔镜肝脏手术中曲线解剖地标的检测精度。它包含多模态特征提取模块、自适应曲线提议初始化机制和分层曲线细化机制，以鲁棒地捕获特征、生成初始曲线提议并迭代地精确调整。在L3D和P2ILF数据集上的评估证明，BCRNet性能优于现有最先进的方法，为增强手术导航提供了有效途径。", "keywords": "地标检测, 腹腔镜肝脏手术, 贝塞尔曲线, 增强现实, 手术导航", "comments": "BCRNet的创新点在于其独特的贝塞尔曲线细化策略，这对于精确表示和调整复杂的曲线地标至关重要，尤其适用于腹腔镜肝脏手术中关键解剖结构的识别。该方法通过引入多阶段的细化机制，能够从多尺度特征中捕获细粒度细节，从而实现高精度的地标检测。在两个真实数据集上的显著性能提升，凸显了其在实际手术导航应用中的潜力和重要性。"}}
{"id": "2506.15051", "title": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization", "authors": ["Zheng Li", "Jerry Cheng", "Huanying Helen Gu"], "summary": "Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.", "comment": "10 pages, 2 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15051v1", "AI": {"title_translation": "用于自适应超参数优化的序列策略梯度", "tldr": "传统强化学习用于超参数优化成本高昂。本文提出SPG（序列策略梯度建模），一种受DeepSeek-V3启发的新型轨迹生成范式，用于轻量级在线超参数优化。SPG能在单次前向传播中生成轨迹，并在CV、NLP和音频等多种任务上实现了0.2%~7%的性能提升，同时显著降低了计算成本。", "motivation": "强化学习在神经网络架构搜索和超参数优化中应用广泛，但传统方法由于时间与计算成本过高，阻碍了其普及。", "method": "本文提出了序列策略梯度建模（SPG），这是一种受DeepSeek-V3多令牌预测架构启发的新型轨迹生成范式，用于轻量级在线超参数优化。与传统策略梯度方法不同，SPG通过临时模块扩展了基础模型，使其能够在单次前向传播中生成状态-动作（填充）轨迹。", "result": "实验表明，使用SPG在其原始数据集上重新训练的模型性能有所提升，并且优于标准迁移微调。该方法在广泛采用的模型上表现出持续的改进，性能提升达到+0.2%~7%，同时计算成本显著降低。SPG在计算机视觉（ImageNet, COCO）、自然语言处理（GLUE, SQuAD）和音频（SUPERB）等五个数据集上进行了评估，验证了其工业适用性。", "conclusion": "SPG为自适应超参数优化提供了一种计算高效且有效的方法，在不同领域均能带来持续的性能改进。", "translation": "强化学习对于神经网络架构搜索和超参数优化至关重要，但传统方法由于高昂的时间和计算成本而阻碍了其广泛应用。受DeepSeek-V3多令牌预测架构的启发，我们提出了序列策略梯度建模（SPG），这是一种用于轻量级在线超参数优化的新型轨迹生成范式。与传统策略梯度方法不同，SPG通过临时模块扩展了基础模型，使其能够在单次前向传播中生成状态-动作（填充）轨迹。我们的实验表明，使用SPG在其原始数据集上重新训练的模型性能有所提升，并且优于标准迁移微调。我们在涵盖计算机视觉（ImageNet、COCO）、自然语言处理（GLUE、SQuAD）和音频（SUPERB）的五个数据集上进行评估，以评估SPG的工业适用性。所提出的方法在广泛采用的模型上表现出持续的改进，实现了+0.2~7%的性能提升，同时计算成本显著降低。完全可复现的代码和预训练模型：https://huggingface.co/UniversalAlgorithmic/SPG。", "summary": "本文提出了一种名为序列策略梯度建模（SPG）的新型轨迹生成范式，用于轻量级在线超参数优化，其灵感来源于DeepSeek-V3的多令牌预测架构。SPG通过临时模块扩展基础模型，使其能够在单次前向传播中生成状态-动作轨迹，从而克服了传统强化学习在超参数优化中高昂的计算成本问题。实验结果表明，SPG在多个领域（计算机视觉、自然语言处理、音频）的五个数据集上，使模型性能提升0.2%至7%，且优于标准迁移微调，同时显著降低了计算开销。", "keywords": "序列策略梯度, 超参数优化, 强化学习, 轻量级, DeepSeek-V3", "comments": "SPG的创新之处在于其单次前向传播生成轨迹的能力，有效解决了传统强化学习在超参数优化中面临的计算瓶颈。该方法在不同领域展现出的广泛适用性、持续的性能提升以及显著的低成本，使其成为一种极具前景的实用超参数优化方案。"}}
{"id": "2506.15560", "title": "RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation", "authors": ["Xingrui Qin", "Wentao Zhao", "Chuan Cao", "Yihe Niu", "Houcheng Jiang", "Jingchuan Wang"], "summary": "Dense metric depth estimation using millimeter-wave radar typically requires\ndense LiDAR supervision, generated via multi-frame projection and\ninterpolation, to guide the learning of accurate depth from sparse radar\nmeasurements and RGB images. However, this paradigm is both costly and\ndata-intensive. To address this, we propose RaCalNet, a novel framework that\neliminates the need for dense supervision by using sparse LiDAR to supervise\nthe learning of refined radar measurements, resulting in a supervision density\nof merely around 1% compared to dense-supervised methods. Unlike previous\napproaches that associate radar points with broad image regions and rely\nheavily on dense labels, RaCalNet first recalibrates and refines sparse radar\npoints to construct accurate depth priors. These priors then serve as reliable\nanchors to guide monocular depth prediction, enabling metric-scale estimation\nwithout resorting to dense supervision. This design improves structural\nconsistency and preserves fine details. Despite relying solely on sparse\nsupervision, RaCalNet surpasses state-of-the-art dense-supervised methods,\nproducing depth maps with clear object contours and fine-grained textures.\nExtensive experiments on the ZJU-4DRadarCam dataset and real-world deployment\nscenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%,\nrespectively.", "comment": "9 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15560v1", "AI": {"title_translation": "RaCalNet：用于稀疏监督度量深度估计的雷达校准网络", "tldr": "RaCalNet提出一种新框架，通过稀疏LiDAR监督（仅1%密度）精炼雷达测量，实现度量深度估计，无需密集监督，性能超越现有密集监督方法。", "motivation": "现有的毫米波雷达密集度量深度估计方法需要通过多帧投影和插值生成密集的LiDAR监督，成本高昂且数据密集，限制了其实用性。", "method": "RaCalNet通过使用稀疏LiDAR监督精炼雷达测量，从而消除了对密集监督的需求。它首先重新校准和精炼稀疏雷达点以构建准确的深度先验，然后这些先验作为可靠的锚点来指导单目深度预测，实现在没有密集监督的情况下进行度量尺度的估计。", "result": "尽管仅依赖稀疏监督，RaCalNet在ZJU-4DRadarCam数据集和实际部署场景中均超越了最先进的密集监督方法，分别将RMSE降低了35.30%和34.89%。它生成的深度图具有清晰的物体轮廓和细粒度纹理。", "conclusion": "RaCalNet通过利用稀疏监督实现了高效且准确的度量深度估计，克服了传统方法对密集监督的依赖，并在性能上超越了现有技术。", "translation": "毫米波雷达的密集度量深度估计通常需要密集的激光雷达监督，通过多帧投影和插值生成，以指导从稀疏雷达测量和RGB图像中学习精确深度。然而，这种范式既昂贵又数据密集。为了解决这个问题，我们提出了RaCalNet，一个新颖的框架，它通过使用稀疏激光雷达监督精炼雷达测量来消除对密集监督的需求，与密集监督方法相比，其监督密度仅为1%左右。与以往将雷达点与广阔图像区域关联并严重依赖密集标签的方法不同，RaCalNet首先重新校准和精炼稀疏雷达点以构建准确的深度先验。这些先验随后作为可靠的锚点来指导单目深度预测，从而无需依赖密集监督即可实现度量尺度的估计。这种设计提高了结构一致性并保留了精细细节。尽管仅依赖稀疏监督，RaCalNet超越了最先进的密集监督方法，生成的深度图具有清晰的物体轮廓和细粒度纹理。在ZJU-4DRadarCam数据集和实际部署场景中的大量实验证明了其有效性，RMSE分别降低了35.30%和34.89%。", "summary": "RaCalNet是一种创新的框架，用于毫米波雷达的度量深度估计，旨在解决传统方法对昂贵且数据密集型密集LiDAR监督的依赖。它通过利用稀疏LiDAR监督（仅约1%密度）来校准和精炼稀疏雷达点，生成准确的深度先验，进而指导单目深度预测。该方法在没有密集监督的情况下实现了度量尺度的估计，并显著提高了深度图的结构一致性和细节保留。实验证明，RaCalNet在性能上超越了现有密集监督方法，显著降低了RMSE。", "keywords": "雷达校准, 稀疏监督, 深度估计, 度量深度, RaCalNet", "comments": "RaCalNet的创新点在于其通过稀疏监督实现高精度度量深度估计的能力，这极大地降低了数据采集和标注成本，使其在实际应用中更具可行性。其将雷达点精炼为深度先验的策略是关键，有效利用了稀疏信息。这项工作对于自动驾驶和机器人领域的低成本、高精度感知具有重要意义。"}}
{"id": "2506.15285", "title": "AI-driven visual monitoring of industrial assembly tasks", "authors": ["Mattia Nardon", "Stefano Messelodi", "Antonio Granata", "Fabio Poiesi", "Alberto Danese", "Davide Boscaini"], "summary": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15285v1", "AI": {"title_translation": "AI驱动的工业装配任务视觉监控", "tldr": "ViMAT是一个新型的AI驱动视觉监控系统，无需额外设置，能实时监控工业装配任务，并通过感知和推理模块确保操作正确，已在实际任务中验证其有效性。", "motivation": "现有的工业装配任务视觉监控商业解决方案通常需要严格的工作空间设置或视觉标记，这限制了其应用范围。本文旨在提出一个无需这些限制的AI驱动系统。", "method": "ViMAT系统结合了一个感知模块和一个推理模块。感知模块从多视角视频流中提取视觉观测结果，推理模块则根据观察到的装配状态和先前的任务知识推断最可能执行的动作。", "result": "ViMAT在两项装配任务中得到了验证：乐高组件的更换和液压机模具的重新配置。通过定量和定性分析，ViMAT在部分和不确定视觉观测的挑战性真实世界场景中展现了其有效性。", "conclusion": "ViMAT提供了一个无需严格限制的、有效的AI驱动实时工业装配任务视觉监控解决方案，并在挑战性的真实世界条件下表现出鲁棒性。", "translation": "工业装配任务的视觉监控对于防止因程序错误造成的设备损坏和确保工人安全至关重要。尽管存在商业解决方案，但它们通常需要严格的工作空间设置或应用视觉标记来简化问题。我们引入了ViMAT，这是一种新型的AI驱动系统，用于实时视觉监控装配任务，并且无需这些限制。ViMAT结合了一个感知模块，该模块从多视角视频流中提取视觉观测结果，以及一个推理模块，该模块根据观察到的装配状态和先前的任务知识推断最可能执行的动作。我们在两项装配任务中验证了ViMAT，包括乐高组件的更换和液压机模具的重新配置，通过在以部分和不确定的视觉观测为特征的挑战性真实世界场景中的定量和定性分析，证明了其有效性。项目页面：https://tev-fbk.github.io/ViMAT", "summary": "本文介绍了ViMAT，一个创新的AI驱动系统，用于实时视觉监控工业装配任务，克服了现有商业解决方案对严格工作空间或视觉标记的依赖。ViMAT整合了感知和推理模块，能从多视角视频流中提取信息并推断当前操作。该系统已在乐高组件更换和液压机模具配置等实际任务中得到验证，展现了在复杂视觉条件下监控的有效性和鲁棒性。", "keywords": "工业装配, 视觉监控, AI驱动, 实时系统, 多视角视频", "comments": "ViMAT的创新之处在于其无需预设标记或刚性环境的实时视觉监控能力，这大大提高了工业应用的灵活性和实用性。其结合感知和推理模块的设计使其能够处理复杂和不确定的视觉数据，是工业自动化和安全领域的重要进展。"}}
{"id": "2506.15054", "title": "Muon Optimizes Under Spectral Norm Constraints", "authors": ["Lizhang Chen", "Jonathan Li", "Qiang Liu"], "summary": "The pursuit of faster optimization algorithms remains an active and important\nresearch direction in deep learning. Recently, the Muon optimizer [JJB+24] has\ndemonstrated promising empirical performance, but its theoretical foundation\nremains less understood. In this paper, we bridge this gap and provide a\ntheoretical analysis of Muon by placing it within the Lion-$\\mathcal{K}$ family\nof optimizers [CLLL24]. Specifically, we show that Muon corresponds to\nLion-$\\mathcal{K}$ when equipped with the nuclear norm, and we leverage the\ntheoretical results of Lion-$\\mathcal{K}$ to establish that Muon (with\ndecoupled weight decay) implicitly solves an optimization problem that enforces\na constraint on the spectral norm of weight matrices. This perspective not only\ndemystifies the implicit regularization effects of Muon but also leads to\nnatural generalizations through varying the choice of convex map $\\mathcal{K}$,\nallowing for the exploration of a broader class of implicitly regularized and\nconstrained optimization algorithms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15054v1", "AI": {"title_translation": "Muon在谱范数约束下进行优化", "tldr": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$族优化器中，揭示了其隐式正则化效应，并证明Muon在解耦权重衰减下隐式地解决了谱范数约束优化问题。", "motivation": "深度学习中对更快优化算法的追求是一个活跃且重要的研究方向。Muon优化器已显示出良好的经验性能，但其理论基础尚不明确，本文旨在弥补这一理论空白。", "method": "本文将Muon优化器置于Lion-$\\mathcal{K}$族优化器[CLLL24]中进行理论分析。具体来说，证明了当配备核范数时，Muon对应于Lion-$\\mathcal{K}$，并利用Lion-$\\mathcal{K}$的理论结果来确立Muon（带解耦权重衰减）隐式地解决了对权重矩阵的谱范数施加约束的优化问题。", "result": "研究结果表明，Muon（带解耦权重衰减）隐式地解决了对权重矩阵的谱范数施加约束的优化问题。这一发现揭示了Muon的隐式正则化效应。", "conclusion": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$族优化器中，成功地提供了其理论基础，揭示了其隐式正则化效应，并为探索更广泛的隐式正则化和约束优化算法提供了新的视角和推广可能性。", "translation": "对更快优化算法的追求仍然是深度学习中一个活跃而重要的研究方向。最近，Muon优化器[JJB+24]已展现出良好的经验性能，但其理论基础仍不甚明朗。在本文中，我们弥补了这一空白，通过将Muon置于Lion-$\\mathcal{K}$族优化器[CLLL24]中，对其进行了理论分析。具体来说，我们证明了当配备核范数时，Muon对应于Lion-$\\mathcal{K}$，并且我们利用Lion-$\\mathcal{K}$的理论结果来确立Muon（带有解耦权重衰减）隐式地解决了一个强制约束权重矩阵谱范数的优化问题。这一视角不仅揭示了Muon的隐式正则化效应，而且通过改变凸映射$\\mathcal{K}$的选择，自然地导向了更广泛的推广，从而允许探索更广泛的隐式正则化和约束优化算法。", "summary": "本文旨在为Muon优化器提供理论基础。通过将其定位在Lion-$\\mathcal{K}$优化器家族中，研究发现Muon在配备核范数时对应于Lion-$\\mathcal{K}$。进一步的理论分析表明，Muon（带有解耦权重衰减）隐式地解决了对权重矩阵谱范数施加约束的优化问题。这一发现不仅解释了Muon的隐式正则化效果，也为开发更广泛的隐式正则化和约束优化算法提供了新的方向。", "keywords": "Muon优化器, 谱范数, 隐式正则化, Lion-K, 深度学习优化", "comments": "本文的创新之处在于它为Muon优化器提供了一个急需的理论框架，将其与已有的Lion-$\\mathcal{K}$族优化器联系起来。通过揭示Muon的隐式谱范数约束，该研究不仅解密了其经验上的成功，还为设计新的、具有特定正则化属性的优化算法打开了大门。其重要性在于它将经验有效的优化器置于坚实的理论基础之上，这对于深度学习算法的进一步发展至关重要。"}}
{"id": "2506.15568", "title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models", "authors": ["Zhengyang Shan", "Emily Ruth Diana", "Jiawei Zhou"], "summary": "We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.", "comment": "Accepted by ACL 2025 Main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15568v1", "AI": {"title_translation": "性别包容性公平指数 (GIFI)：一个评估大型语言模型性别多样性的多级框架", "tldr": "引入GIFI，一个评估大型语言模型性别包容性（包括二元和非二元性别）的新指标，并发现大型语言模型在性别包容性方面存在显著差异。", "motivation": "现有研究主要关注二元性别区分，但大型语言模型需要处理二元和非二元性别。因此，需要一个超越二元区分的全面性别公平性评估方法。", "method": "提出了性别包容性公平指数（GIFI），这是一个新颖且全面的度量标准，用于量化大型语言模型的性别多样性包容性。GIFI包含从探测性别代词到测试模型生成和认知行为等不同层面的广泛评估。使用GIFI对22个主流开源和专有大型语言模型进行了广泛评估。", "result": "发现大型语言模型在性别包容性方面存在显著差异。", "conclusion": "强调了提高大型语言模型包容性的重要性，为未来生成模型在性别公平性方面的进展提供了关键基准。", "translation": "我们对大型语言模型（LLMs）的性别公平性进行了全面评估，重点关注它们处理二元和非二元性别的能力。虽然以往的研究主要关注二元性别区分，但我们引入了性别包容性公平指数（GIFI），这是一个新颖且全面的指标，用于量化LLMs的多元性别包容性。GIFI包含不同层面的广泛评估，从简单地使用提供的性别代词探测模型，到测试不同性别假设下模型生成和认知行为的各个方面，揭示了与不同性别标识符相关的偏见。我们使用GIFI对22个不同规模和能力的主流开源和专有LLMs进行了广泛评估，发现LLMs在性别包容性方面存在显著差异。我们的研究强调了提高LLMs包容性的重要性，为未来生成模型在性别公平性方面的进展提供了关键基准。", "summary": "本研究提出性别包容性公平指数（GIFI），这是一个针对大型语言模型（LLMs）的创新性多级评估框架，旨在全面衡量其在处理二元和非二元性别方面的公平性与包容性。GIFI通过探测模型对代词的响应以及在不同性别假设下的生成和认知行为，揭示了潜在的性别偏见。研究人员使用GIFI对22个主流LLMs进行了广泛测试，结果显示这些模型在性别包容性方面存在显著差异。该工作强调了提升LLMs包容性的重要性，并为未来生成模型在性别公平性方面的研究提供了重要的基准。", "keywords": "性别包容性, 公平性, 大型语言模型, GIFI, 偏见", "comments": "该研究的创新之处在于超越了传统的二元性别评估，引入了GIFI这一多层次、全面的性别包容性评估框架，特别关注了非二元性别。它为大型语言模型的性别公平性研究提供了一个重要的新基准，对未来生成模型的发展具有指导意义，有助于推动AI伦理和负责任AI的进步。"}}
{"id": "2506.15635", "title": "FindingDory: A Benchmark to Evaluate Memory in Embodied Agents", "authors": ["Karmesh Yadav", "Yusuf Ali", "Gunshi Gupta", "Yarin Gal", "Zsolt Kira"], "summary": "Large vision-language models have recently demonstrated impressive\nperformance in planning and control tasks, driving interest in their\napplication to real-world robotics. However, deploying these models for\nreasoning in embodied contexts is limited by their ability to incorporate\nlong-term experience collected across multiple days and represented by vast\ncollections of images. Current VLMs typically struggle to process more than a\nfew hundred images concurrently, highlighting the need for more efficient\nmechanisms to handle long-term memory in embodied settings. To effectively\nevaluate these models for long-horizon control, a benchmark must specifically\ntarget scenarios where memory is crucial for success. Existing long-video QA\nbenchmarks overlook embodied challenges like object manipulation and\nnavigation, which demand low-level skills and fine-grained reasoning over past\ninteractions. Moreover, effective memory integration in embodied agents\ninvolves both recalling relevant historical information and executing actions\nbased on that information, making it essential to study these aspects together\nrather than in isolation. In this work, we introduce a new benchmark for\nlong-range embodied tasks in the Habitat simulator. This benchmark evaluates\nmemory-based capabilities across 60 tasks requiring sustained engagement and\ncontextual awareness in an environment. The tasks can also be procedurally\nextended to longer and more challenging versions, enabling scalable evaluation\nof memory and reasoning. We also present baselines that integrate\nstate-of-the-art VLMs with low level navigation policies, assessing their\nperformance on these memory-intensive tasks and highlight areas for\nimprovement.", "comment": "Our dataset and code will be made available at:\n  https://findingdory-benchmark.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15635v1", "AI": {"title_translation": "FindingDory：一个评估具身智能体记忆能力的基准", "tldr": "该论文介绍了FindingDory，一个在Habitat模拟器中评估具身智能体长期记忆能力的基准，旨在解决现有视觉-语言模型在处理长期经验和具身任务中的记忆挑战。", "motivation": "当前的视觉-语言模型（VLMs）在处理大量图像以整合长期经验方面存在局限性，导致它们难以在具身环境中进行有效的长期推理。现有基准忽略了具身任务中对记忆至关重要的低级技能和精细推理需求，且未能将信息回忆与行动执行结合评估。", "method": "本文引入了一个名为FindingDory的新基准，用于在Habitat模拟器中评估具身智能体的长距离任务记忆能力。该基准包含60项任务，要求智能体持续参与并具备环境语境意识，且任务可以程序化扩展以增加长度和挑战性。此外，论文还提出了将最先进的VLMs与低级导航策略相结合的基线模型，并评估它们在这些记忆密集型任务上的表现。", "result": "FindingDory基准能够评估具身智能体在60项需要长期参与和语境意识的记忆密集型任务中的能力。通过该基准，论文评估了集成VLMs的基线模型在这些任务上的表现，并指出了需要改进的领域。", "conclusion": "该研究通过引入FindingDory基准，为评估具身智能体在长期具身任务中的记忆和推理能力提供了一个重要工具，并揭示了当前视觉-语言模型在处理长期记忆方面的局限性及未来改进方向。", "translation": "大型视觉-语言模型最近在规划和控制任务中表现出色，这推动了它们在现实世界机器人技术中的应用兴趣。然而，在具身环境中部署这些模型进行推理，受限于它们整合跨多天收集并由大量图像表示的长期经验的能力。当前的VLM通常难以同时处理数百张以上的图像，这凸显了在具身环境中处理长期记忆需要更高效的机制。为了有效评估这些模型进行长周期控制，基准必须专门针对记忆对成功至关重要的场景。现有的长视频问答基准忽略了具身挑战，如物体操作和导航，这些挑战需要低级技能和对过去交互的精细推理。此外，具身智能体中有效的记忆整合涉及回忆相关历史信息和基于该信息执行动作，这使得同时研究这些方面而非孤立研究变得至关重要。在这项工作中，我们引入了一个用于Habitat模拟器中长距离具身任务的新基准。该基准评估了在需要持续参与和环境语境意识的60项任务中的基于记忆的能力。这些任务也可以通过程序化扩展为更长、更具挑战性的版本，从而实现对记忆和推理的可扩展评估。我们还提出了将最先进的VLM与低级导航策略相结合的基线，评估它们在这些记忆密集型任务上的表现，并强调了需要改进的领域。", "summary": "本文提出了FindingDory，一个在Habitat模拟器中评估具身智能体长期记忆能力的基准。鉴于当前视觉-语言模型在处理大量历史图像和具身任务中的记忆限制，该基准设计了60项可扩展的记忆密集型任务，旨在专门衡量智能体在具身环境中回忆和利用长期经验的能力。研究还提供了结合先进VLMs的基线模型表现，以揭示现有方法的不足并指导未来研究。", "keywords": "具身智能体, 记忆, 基准, 视觉-语言模型, 长期记忆", "comments": "FindingDory基准的创新之处在于其专注于评估具身智能体在长期、记忆密集型任务中的表现，填补了现有基准对具身挑战和记忆-行动整合不足的空白。其可扩展的任务设计允许对记忆和推理能力进行更全面的评估。该工作对于推动具身智能体在复杂真实世界场景中的应用具有重要意义，因为它直接解决了现有VLMs在处理长期经验方面的核心限制。"}}
{"id": "2506.15298", "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering", "authors": ["Xinqi Fan", "Jingting Li", "John See", "Moi Hoon Yap", "Wen-Huang Cheng", "Xiaobai Li", "Xiaopeng Hong", "Su-Jing Wang", "Adrian K. Davision"], "summary": "Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.", "comment": "Micro-Expression Grand Challenge (MEGC) at ACM MM 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15298v1", "AI": {"title_translation": "MEGC2025：微表情发现与识别及视觉问答挑战赛", "tldr": "MEGC 2025挑战赛引入了两项新任务：集成微表情发现与识别（ME-STR）以及基于多模态大模型的微表情视觉问答（ME-VQA）。", "motivation": "传统上将微表情发现和识别视为独立任务的方法次优，尤其是在分析长时间视频时。新兴的多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）为增强微表情分析提供了新途径。", "method": "本文介绍了MEGC 2025挑战赛，包含两项任务：1) 微表情发现后识别（ME-STR），将微表情发现和识别整合到统一的顺序流程中；2) 微表情视觉问答（ME-VQA），利用MLLMs或LVLMs通过视觉问答探索微表情理解。所有参与算法需在指定测试集上运行并提交结果至排行榜。", "result": "Not mentioned in abstract", "conclusion": "MEGC 2025挑战赛旨在通过推广集成式的发现与识别方法以及利用多模态模型进行视觉问答，来解决当前微表情分析的局限性，从而推动该领域的发展。", "translation": "面部微表情（MEs）是当一个人经历情绪但试图抑制或压抑面部表情时，自发产生的面部非自愿运动，通常出现在高风险环境中。近年来，在微表情识别、发现和生成领域取得了实质性进展。然而，将发现和识别视为独立任务的传统方法是次优的，特别是对于分析现实环境中长时间视频而言。同时，多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）的出现为通过其强大的多模态推理能力增强微表情分析提供了有前景的新途径。微表情大挑战（MEGC）2025引入了两项任务，反映了这些不断发展的研究方向：(1) 微表情发现后识别（ME-STR），它将微表情发现和随后的识别整合到一个统一的顺序管道中；(2) 微表情视觉问答（ME-VQA），它通过视觉问答探索微表情理解，利用MLLMs或LVLMs解决与微表情相关的各种问题类型。所有参与算法都要求在此测试集上运行并将其结果提交到排行榜。更多详情请访问 https://megc2025.github.io。", "summary": "MEGC 2025大挑战旨在解决微表情（ME）分析的局限性。它提出了两项新任务：ME发现后识别（ME-STR），整合了ME发现和识别；以及ME视觉问答（ME-VQA），利用多模态大语言/视觉语言模型进行ME理解。此挑战旨在推动集成式和多模态ME分析的进步。", "keywords": "微表情, 挑战赛, 发现与识别, 视觉问答, 多模态模型", "comments": "本文不是一篇研究论文，而是对一项大型挑战赛的公告。其创新之处在于为微表情分析定义了新的、更具挑战性的任务，特别是整合了发现和识别过程，并探索了多模态大语言模型在此领域中的潜力。这项挑战对推动微表情研究的下一代发展至关重要。"}}
{"id": "2506.15569", "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification", "authors": ["Chengye Wang", "Yifei Shen", "Zexi Kuang", "Arman Cohan", "Yilun Zhao"], "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15569v1", "AI": {"title_translation": "SciVer：评估多模态科学声明验证中的基础模型", "tldr": "引入SciVer，首个多模态科学声明验证基准，发现现有基础模型与人类专家之间存在显著性能差距，并揭示了模型局限性。", "motivation": "现有基础模型在多模态科学背景下的声明验证能力缺乏专门的评估基准。", "method": "本文引入了SciVer数据集，包含3000个专家标注的例子，覆盖1113篇科学论文，包含四种推理类型，并提供专家标注的支持证据。研究评估了21个最先进的多模态基础模型（如o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, Qwen2.5-VL）。通过对检索增强生成（RAG）的深入分析和人工错误评估来识别模型局限性。", "result": "实验表明，在SciVer基准上，现有基础模型与人类专家之间存在显著的性能差距。", "conclusion": "识别出当前开源模型在多模态科学文献任务中理解和推理方面的关键局限性，并为模型进步提供了重要见解。", "translation": "我们引入了SciVer，这是第一个专门设计用于评估基础模型在多模态科学背景下验证声明能力的基准。SciVer包含来自1113篇科学论文的3000个专家标注示例，涵盖四种子集，每种代表多模态科学声明验证中常见的一种推理类型。为了实现细粒度评估，每个示例都包含专家标注的支持证据。我们评估了21个最先进的多模态基础模型，包括o4-mini、Gemini-2.5-Flash、Llama-3.2-Vision和Qwen2.5-VL的性能。我们的实验揭示了这些模型在SciVer上与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）的深入分析和人工进行的错误评估，我们识别出当前开源模型的关键局限性，为提高模型在多模态科学文献任务中的理解和推理能力提供了重要见解。", "summary": "本文介绍了SciVer，一个用于评估基础模型在多模态科学背景下进行声明验证的首个基准数据集。SciVer包含3000个专家标注的示例，覆盖多种推理类型，并提供详细的支持证据。研究者使用该基准评估了21个主流多模态基础模型，发现它们与人类专家之间存在显著的性能差距。通过深入分析和错误评估，论文揭示了当前模型在科学文献理解和推理方面的局限性，并为未来的模型改进提供了方向。", "keywords": "多模态科学声明验证, 基础模型, SciVer, 基准评估, 性能差距", "comments": "SciVer作为首个专门针对多模态科学声明验证的基准，填补了现有评估工具的空白，具有重要的创新性。它通过大规模、专家标注的数据集和细粒度评估，为深入理解基础模型在复杂科学推理任务中的能力和局限性提供了宝贵工具。研究结果揭示了当前SOTA模型与人类专家之间的巨大差距，凸显了未来研究的挑战和方向，特别是对模型在科学情境下理解和推理能力的提升。"}}
{"id": "2506.15313", "title": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning", "authors": ["Leonid Ivanov", "Vasily Yuryev", "Dmitry Yudin"], "summary": "In autonomous driving, high-definition (HD) maps and semantic maps in\nbird's-eye view (BEV) are essential for accurate localization, planning, and\ndecision-making. This paper introduces an enhanced End-to-End model named MapFM\nfor online vectorized HD map generation. We show significantly boost feature\nrepresentation quality by incorporating powerful foundation model for encoding\ncamera images. To further enrich the model's understanding of the environment\nand improve prediction quality, we integrate auxiliary prediction heads for\nsemantic segmentation in the BEV representation. This multi-task learning\napproach provides richer contextual supervision, leading to a more\ncomprehensive scene representation and ultimately resulting in higher accuracy\nand improved quality of the predicted vectorized HD maps. The source code is\navailable at https://github.com/LIvanoff/MapFM.", "comment": "Preprint. Submitted. 12 pages, 4 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15313v1", "AI": {"title_translation": "MapFM：基于基础模型的HD地图生成与多任务上下文学习", "tldr": "MapFM是一个端到端模型，通过结合基础模型和多任务学习（包括BEV语义分割）来在线生成高精度矢量化高清地图，显著提升了地图生成质量和准确性。", "motivation": "在自动驾驶中，高清（HD）地图和鸟瞰图（BEV）中的语义地图对于准确的定位、规划和决策至关重要。", "method": "本文提出了一个名为MapFM的增强型端到端模型，用于在线矢量化高清地图生成。该模型通过整合强大的基础模型来编码相机图像以显著提升特征表示质量；同时，为丰富模型对环境的理解和提高预测质量，集成了用于鸟瞰图（BEV）表示中语义分割的辅助预测头；这种多任务学习方法提供了更丰富的上下文监督。", "result": "该方法显著提升了特征表示质量，形成了更全面的场景表示，并最终获得了更高精度和更高质量的预测矢量化高清地图。", "conclusion": "MapFM通过结合基础模型和多任务上下文学习（特别是BEV语义分割），成功地提升了在线矢量化高清地图的生成质量和准确性，为自动驾驶提供了关键支持。", "translation": "在自动驾驶中，高清（HD）地图和鸟瞰图（BEV）中的语义地图对于准确的定位、规划和决策至关重要。本文介绍了一个名为MapFM的增强型端到端模型，用于在线矢量化高清地图生成。我们展示了通过整合强大的基础模型来编码相机图像，可以显著提升特征表示质量。为了进一步丰富模型对环境的理解并提高预测质量，我们集成了用于鸟瞰图（BEV）表示中语义分割的辅助预测头。这种多任务学习方法提供了更丰富的上下文监督，从而形成更全面的场景表示，并最终带来更高精度和更高质量的预测矢量化高清地图。源代码可在 https://github.com/LIvanoff/MapFM 获取。", "summary": "MapFM是一个为自动驾驶设计的新型端到端模型，旨在在线生成高质量的矢量化高清地图。它通过整合强大的基础模型来增强相机图像的特征表示，并引入BEV语义分割的辅助任务进行多任务学习，从而提供更丰富的上下文信息，显著提升了地图预测的准确性和质量。", "keywords": "高清地图, 基础模型, 多任务学习, 自动驾驶, 矢量化地图", "comments": "该论文的创新点在于将基础模型引入到高清地图生成中，以提升特征表示能力，并结合多任务学习（特别是BEV语义分割）来丰富环境理解和提高预测质量。这种方法为自动驾驶中的在线高清地图生成提供了一个高效且高精度的新范式，具有重要的实践意义。"}}
{"id": "2506.15583", "title": "DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement", "authors": ["Shaoqing Lin", "Chong Teng", "Fei Li", "Donghong Ji", "Lizhen Qu", "Zhuang Li"], "summary": "Vision-Language Models (VLMs) now generate discourse-level, multi-sentence\nvisual descriptions, challenging text scene graph parsers originally designed\nfor single-sentence caption-to-graph mapping. Current approaches typically\nmerge sentence-level parsing outputs for discourse input, often missing\nphenomena like cross-sentence coreference, resulting in fragmented graphs and\ndegraded downstream VLM task performance. To address this, we introduce a new\ntask, Discourse-level text Scene Graph parsing (DiscoSG), supported by our\ndataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised\nmulti-sentence caption-graph pairs for images. Each caption averages 9\nsentences, and each graph contains at least 3 times more triples than those in\nexisting datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS\nimproves SPICE by approximately 48% over the best sentence-merging baseline,\nhigh inference cost and restrictive licensing hinder its open-source use, and\nsmaller fine-tuned PLMs struggle with complex graphs. We propose\nDiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a\nsecond PLM to iteratively propose graph edits, reducing full-graph generation\noverhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE\nby approximately 30% over the best baseline while achieving 86 times faster\ninference than GPT-4. It also consistently improves downstream VLM tasks like\ndiscourse-level caption evaluation and hallucination detection. Code and data\nare available at: https://github.com/ShaoqLin/DiscoSG", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15583v1", "AI": {"title_translation": "DiscoSG：通过迭代图细化实现篇章级文本场景图解析", "tldr": "本文提出DiscoSG任务和DiscoSG-Refiner模型，用于高效解析篇章级文本场景图，显著提升性能并降低推理成本。", "motivation": "现有的文本场景图解析器主要为单句设计，难以处理视觉-语言模型（VLM）生成的篇章级多句视觉描述，导致图碎片化和下游VLM任务性能下降，因为它们通常合并句级解析输出而忽略跨句指代等现象。", "method": "本文引入了新的篇章级文本场景图解析（DiscoSG）任务，并构建了数据集DiscoSG-DS，其中包含400个专家标注和8,430个合成的多句标题-图对。为解决大型模型的高成本问题，提出DiscoSG-Refiner模型，该模型使用一个小型预训练语言模型（PLM）生成基础图，然后利用第二个PLM迭代地提出图编辑，以减少完整图生成的开销。", "result": "在DiscoSG-DS上微调大型PLM（如GPT-4）使SPICE比最佳句级合并基线提高了约48%。使用两个Flan-T5-Base模型的DiscoSG-Refiner模型，SPICE比最佳基线提高了约30%，同时推理速度比GPT-4快86倍。此外，它还持续改善了下游VLM任务，如篇章级标题评估和幻觉检测。", "conclusion": "DiscoSG-Refiner通过迭代图细化，有效解决了篇章级文本场景图解析的挑战，在保持高性能的同时显著降低了推理成本，并提升了下游视觉-语言模型的表现。", "translation": "视觉-语言模型（VLM）现在能生成篇章级的多句视觉描述，这挑战了最初为单句标题到图映射设计的文本场景图解析器。当前方法通常合并句级解析输出以处理篇章输入，但常常忽略跨句指代等现象，导致图碎片化和下游VLM任务性能下降。为解决此问题，我们引入了一项新任务——篇章级文本场景图解析（DiscoSG），并构建了数据集DiscoSG-DS，该数据集包含400个专家标注和8,430个合成的图像多句标题-图对。每个标题平均有9个句子，每个图包含的谓词三元组数量至少是现有数据集的3倍。虽然在DiscoSG-DS上微调大型预训练语言模型（即GPT-4）使SPICE比最佳句级合并基线提高了约48%，但高昂的推理成本和限制性许可阻碍了其开源使用，且较小的微调PLM难以处理复杂图。我们提出了DiscoSG-Refiner，它使用一个小型PLM起草基础图，然后利用第二个PLM迭代地提出图编辑，从而减少完整图生成开销。使用两个Flan-T5-Base模型，DiscoSG-Refiner在SPICE上仍比最佳基线提高了约30%，同时推理速度比GPT-4快86倍。它还持续改善了下游VLM任务，如篇章级标题评估和幻觉检测。代码和数据可在以下网址获取：https://github.com/ShaoqLin/DiscoSG", "summary": "本文提出DiscoSG，一项新的篇章级文本场景图解析任务，并发布了大规模数据集DiscoSG-DS，以应对视觉-语言模型生成多句描述带来的挑战。针对大型模型的高成本问题，作者开发了DiscoSG-Refiner，一种高效的迭代图细化方法，利用两个小型PLM生成和优化场景图。实验表明，DiscoSG-Refiner在保持高性能（SPICE提升30%）的同时，显著降低了推理成本（比GPT-4快86倍），并提升了下游VLM任务的表现。", "keywords": "文本场景图解析, 篇章级, 迭代图细化, 视觉-语言模型, DiscoSG", "comments": "本文的创新点在于明确提出并解决了篇章级文本场景图解析的挑战，通过构建大规模数据集DiscoSG-DS为该领域提供了宝贵的资源。DiscoSG-Refiner模型的迭代细化策略是一个重要创新，它在保证性能的同时显著降低了对大型模型的依赖和推理成本，使其更具实用性。该研究对于提升视觉-语言模型在理解复杂、多句描述方面的能力具有重要意义。"}}
{"id": "2506.15318", "title": "OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models", "authors": ["Lanfeng Zhong", "Xin Liao", "Shichuan Zhang", "Shaoting Zhang", "Guotai Wang"], "summary": "Pathology image classification plays a crucial role in accurate medical\ndiagnosis and treatment planning. Training high-performance models for this\ntask typically requires large-scale annotated datasets, which are both\nexpensive and time-consuming to acquire. Active Learning (AL) offers a solution\nby iteratively selecting the most informative samples for annotation, thereby\nreducing the labeling effort. However, most AL methods are designed under the\nassumption of a closed-set scenario, where all the unannotated images belong to\ntarget classes. In real-world clinical environments, the unlabeled pool often\ncontains a substantial amount of Out-Of-Distribution (OOD) data, leading to low\nefficiency of annotation in traditional AL methods. Furthermore, most existing\nAL methods start with random selection in the first query round, leading to a\nsignificant waste of labeling costs in open-set scenarios. To address these\nchallenges, we propose OpenPath, a novel open-set active learning approach for\npathological image classification leveraging a pre-trained Vision-Language\nModel (VLM). In the first query, we propose task-specific prompts that combine\ntarget and relevant non-target class prompts to effectively select\nIn-Distribution (ID) and informative samples from the unlabeled pool. In\nsubsequent queries, Diverse Informative ID Sampling (DIS) that includes\nPrototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic\nSampling (EGSS) is proposed to ensure both purity and informativeness in a\nquery, avoiding the selection of OOD samples. Experiments on two public\npathology image datasets show that OpenPath significantly enhances the model's\nperformance due to its high purity of selected samples, and outperforms several\nstate-of-the-art open-set AL methods. The code is available at\n\\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..", "comment": "MICCAI 2025 early accept", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15318v1", "AI": {"title_translation": "OpenPath：基于预训练视觉语言模型的病理图像开放集主动学习分类", "tldr": "OpenPath 是一种新的开放集主动学习方法，利用预训练的视觉语言模型，通过改进的样本选择策略，有效解决了病理图像分类中大规模数据集标注的挑战和传统主动学习方法在开放集场景中的效率低下问题。", "motivation": "病理图像分类需要大量标注数据，获取成本高昂且耗时。传统主动学习（AL）方法假设所有未标注图像都属于目标类别（闭集场景），但在真实的临床环境中，未标注池中常包含大量分布外（OOD）数据，导致传统AL方法效率低下。此外，大多数现有AL方法在首次查询时随机选择，在开放集场景中造成标注成本的显著浪费。", "method": "本文提出了OpenPath，一种新颖的开放集主动学习方法，用于病理图像分类，利用预训练的视觉语言模型（VLM）。在首次查询中，提出结合目标和相关非目标类别提示的任务特定提示，以有效从未标注池中选择分布内（ID）和信息丰富的样本。在后续查询中，提出了多样化信息ID采样（DIS），包括基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保查询的纯度和信息量，避免选择OOD样本。", "result": "在两个公共病理图像数据集上的实验表明，OpenPath 由于其选择样本的高纯度，显著提升了模型性能，并优于几种最先进的开放集主动学习方法。", "conclusion": "OpenPath通过结合预训练视觉语言模型和创新的样本选择策略，有效解决了病理图像分类中开放集主动学习的挑战，显著提高了标注效率和模型性能。", "translation": "病理图像分类在准确的医学诊断和治疗规划中起着至关重要的作用。为这项任务训练高性能模型通常需要大规模的标注数据集，这既昂贵又耗时。主动学习（AL）通过迭代选择最具信息量的样本进行标注来提供解决方案，从而减少了标注工作量。然而，大多数AL方法是在闭集场景的假设下设计的，即所有未标注图像都属于目标类别。在真实世界的临床环境中，未标注池通常包含大量分布外（OOD）数据，导致传统AL方法的标注效率低下。此外，大多数现有AL方法在首次查询轮次中从随机选择开始，导致在开放集场景中浪费了大量的标注成本。为了解决这些挑战，我们提出了OpenPath，一种新颖的开放集主动学习方法，用于病理图像分类，它利用了预训练的视觉语言模型（VLM）。在首次查询中，我们提出了任务特定提示，结合了目标和相关的非目标类别提示，以有效从未标注池中选择分布内（ID）和信息丰富的样本。在随后的查询中，提出了多样化信息ID采样（DIS），包括基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保查询的纯度和信息量，避免选择OOD样本。在两个公共病理图像数据集上的实验表明，OpenPath 由于其选择样本的高纯度，显著提升了模型的性能，并优于几种最先进的开放集AL方法。代码可在https://github.com/HiLab-git/OpenPath 获取。", "summary": "OpenPath提出了一种新颖的开放集主动学习方法，用于病理图像分类，旨在解决传统主动学习在真实临床环境中面临的挑战，即存在大量分布外（OOD）数据和首次查询的随机性导致的标注效率低下。该方法利用预训练的视觉语言模型（VLM），在首次查询中采用结合目标和非目标类别提示的任务特定提示来选择分布内（ID）和信息丰富的样本。在后续查询中，引入了多样化信息ID采样（DIS），包含基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保所选样本的纯度和信息量。实验证明，OpenPath显著提升了模型性能，并优于现有最先进的开放集主动学习方法。", "keywords": "开放集主动学习, 病理图像分类, 视觉语言模型, OOD检测, 样本选择", "comments": "OpenPath的创新点在于将预训练的视觉语言模型引入开放集主动学习，并设计了针对开放集场景的样本选择策略，特别是首次查询的非随机选择和后续查询中对ID样本纯度与信息量的兼顾。这对于解决医疗图像领域中数据标注成本高昂且数据分布复杂的问题具有重要意义，有望提高诊断模型的训练效率和鲁棒性。"}}
{"id": "2506.15079", "title": "Neural Canonical Polyadic Factorization for Traffic Analysis", "authors": ["Yikai Hou", "Peng Tang"], "summary": "Modern intelligent transportation systems rely on accurate spatiotemporal\ntraffic analysis to optimize urban mobility and infrastructure resilience.\nHowever, pervasive missing data caused by sensor failures and heterogeneous\nsensing gaps fundamentally hinders reliable traffic modeling. This paper\nproposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes\nlow-rank tensor algebra with deep representation learning for robust traffic\ndata imputation. The model innovatively embeds CP decomposition into neural\narchitecture through learnable embedding projections, where sparse traffic\ntensors are encoded into dense latent factors across road segments, time\nintervals, and mobility metrics. A hierarchical feature fusion mechanism\nemploys Hadamard products to explicitly model multilinear interactions, while\nstacked multilayer perceptron layers nonlinearly refine these representations\nto capture complex spatiotemporal couplings. Extensive evaluations on six urban\ntraffic datasets demonstrate NCPF's superiority over six state-of-the-art\nbaselines. By unifying CP decomposition's interpretable factor analysis with\nneural network's nonlinear expressive power, NCPF provides a principled yet\nflexible approaches for high-dimensional traffic data imputation, offering\ncritical support for next-generation transportation digital twins and adaptive\ntraffic control systems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15079v1", "AI": {"title_translation": "用于交通分析的神经规范多重分解", "tldr": "NCPF模型结合张量分解和深度学习，有效解决交通数据缺失问题，提升交通分析的准确性。", "motivation": "现代智能交通系统依赖准确的时空交通分析来优化城市出行和基础设施弹性，但传感器故障和异构感知间隙导致的数据缺失严重阻碍了可靠的交通建模。", "method": "本文提出了一种神经规范多重分解（NCPF）模型，该模型将低秩张量代数与深度表示学习相结合，用于鲁棒的交通数据插补。它创新性地通过可学习的嵌入投影将CP分解嵌入到神经网络架构中，将稀疏交通张量编码为跨路段、时间间隔和移动性指标的密集潜在因子。一个分层特征融合机制采用Hadamard积来明确建模多线性交互，而堆叠的多层感知器层非线性地细化这些表示以捕获复杂的时空耦合。", "result": "在六个城市交通数据集上的广泛评估表明，NCPF优于六个最先进的基线模型。", "conclusion": "NCPF通过统一CP分解的可解释因子分析与神经网络的非线性表达能力，为高维交通数据插补提供了一种原则性而灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了关键支持。", "translation": "现代智能交通系统依赖准确的时空交通分析来优化城市出行和基础设施弹性。然而，传感器故障和异构感知间隙导致的普遍数据缺失从根本上阻碍了可靠的交通建模。本文提出了一种神经规范多重分解（NCPF）模型，该模型将低秩张量代数与深度表示学习相结合，用于鲁棒的交通数据插补。该模型创新性地通过可学习的嵌入投影将CP分解嵌入到神经网络架构中，其中稀疏交通张量被编码为跨路段、时间间隔和移动性指标的密集潜在因子。一个分层特征融合机制采用Hadamard积来明确建模多线性交互，而堆叠的多层感知器层非线性地细化这些表示以捕获复杂的时空耦合。在六个城市交通数据集上的广泛评估表明NCPF优于六个最先进的基线。通过统一CP分解的可解释因子分析与神经网络的非线性表达能力，NCPF为高维交通数据插补提供了一种原则性而灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了关键支持。", "summary": "本文提出了一种名为神经规范多重分解（NCPF）的新模型，旨在解决智能交通系统中普遍存在的交通数据缺失问题。NCPF模型创新性地将CP分解嵌入到神经网络架构中，通过学习嵌入投影将稀疏交通张量编码为密集潜在因子，并利用分层特征融合机制和多层感知器来捕获复杂的时空耦合。实验结果表明，NCPF在多个城市交通数据集上表现优于现有先进方法，为高维交通数据插补提供了一种有效且灵活的解决方案，支持未来交通系统的发展。", "keywords": "神经规范多重分解, 交通分析, 数据插补, 张量分解, 时空耦合", "comments": "该论文的创新点在于将可解释的CP分解与神经网络的非线性表达能力相结合，为解决交通数据缺失问题提供了一种新颖且有效的方法。这种结合使得模型既能进行因子分析，又能处理复杂的时空关系。其重要性在于为智能交通系统中的数据插补提供了关键支持，有助于构建更可靠的交通模型和下一代交通数字孪生。"}}
{"id": "2506.15594", "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts", "authors": ["Negar Foroutan", "Angelika Romanou", "Matin Ansaripour", "Julian Martin Eisenschlos", "Karl Aberer", "Rémi Lebret"], "summary": "Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.", "comment": "ACL 2025 (Findings)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15594v1", "AI": {"title_translation": "WikiMixQA：一个用于表格和图表问答的多模态基准", "tldr": "引入了WikiMixQA，一个用于评估视觉语言模型在长文本多模态文档理解中对表格和图表进行复杂推理的新基准，并发现现有模型在此任务上表现不佳。", "motivation": "自动文档理解（DU）面临复杂布局、表格和图表的挑战。尽管视觉语言大模型（VLLMs）在多项任务中有所改进，但它们处理长上下文视觉输入的有效性尚不明确。", "method": "引入了WikiMixQA，一个包含1,000个多项选择题（MCQs）的基准，旨在评估对从4,000个维基百科页面中提取的表格和图表进行的跨模态推理。它强调复杂推理，要求模型综合来自多种模态的信息。评估了12个最先进的视觉语言模型。", "result": "当提供直接上下文时，专有模型达到约70%的准确率。当需要从长文档中检索时，性能显著下降。GPT-4-o是唯一在此设置下准确率超过50%的模型。开源模型表现差得多，最高准确率为27%。", "conclusion": "这些发现强调了长上下文、多模态推理的挑战，并确立WikiMixQA作为推进文档理解研究的关键基准。", "translation": "文档是保存和传播信息的基础，通常包含复杂的布局、表格和图表，这对自动文档理解（DU）构成了重大挑战。尽管视觉语言大模型（VLLMs）在各种任务中都表现出改进，但它们在处理长上下文视觉输入方面的有效性仍不清楚。本文介绍了WikiMixQA，一个包含1,000个多项选择题（MCQs）的基准，旨在评估对从4,000个维基百科页面中提取的、涵盖七个不同主题的表格和图表进行的跨模态推理。与现有基准不同，WikiMixQA通过要求模型综合来自多种模态的信息来强调复杂推理。我们评估了12个最先进的视觉语言模型，结果显示，虽然专有模型在提供直接上下文时能达到约70%的准确率，但当需要从长文档中检索时，它们的性能显著下降。其中，GPT-4-o是唯一在此设置下准确率超过50%的模型，而开源模型表现则差得多，最高准确率为27%。这些发现强调了长上下文、多模态推理的挑战，并确立WikiMixQA作为推进文档理解研究的关键基准。", "summary": "本文介绍了WikiMixQA，一个用于评估视觉语言模型在文档理解中对表格和图表进行复杂、跨模态推理的新基准。该基准包含1000个多项选择题，数据来源于维基百科。研究发现，尽管专有模型在直接上下文下表现尚可，但在长文档检索场景下，所有模型（包括最先进的VLLMs）的性能都显著下降，突显了长上下文多模态推理的挑战。", "keywords": "多模态问答, 文档理解, 表格, 图表, 视觉语言模型, WikiMixQA", "comments": "该论文创新性地提出了一个专注于长上下文、多模态复杂推理的问答基准，填补了现有基准的空白。它揭示了当前视觉语言模型在处理真实世界复杂文档，尤其是在需要从长文档中检索信息时，所面临的显著局限性。WikiMixQA的建立对于推动文档理解和多模态人工智能领域的研究具有重要意义。"}}
{"id": "2506.15368", "title": "Open-World Object Counting in Videos", "authors": ["Niki Amini-Naieni", "Andrew Zisserman"], "summary": "We introduce a new task of open-world object counting in videos: given a text\ndescription, or an image example, that specifies the target object, the\nobjective is to enumerate all the unique instances of the target objects in the\nvideo. This task is especially challenging in crowded scenes with occlusions\nand similar objects, where avoiding double counting and identifying\nreappearances is crucial. To this end, we make the following contributions: we\nintroduce a model, CountVid, for this task. It leverages an image-based\ncounting model, and a promptable video segmentation and tracking model to\nenable automated, open-world object counting across video frames. To evaluate\nits performance, we introduce VideoCount, a new dataset for our novel task\nbuilt from the TAO and MOT20 tracking datasets, as well as from videos of\npenguins and metal alloy crystallization captured by x-rays. Using this\ndataset, we demonstrate that CountVid provides accurate object counts, and\nsignificantly outperforms strong baselines. The VideoCount dataset, the\nCountVid model, and all the code are available at\nhttps://github.com/niki-amini-naieni/CountVid/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15368v1", "AI": {"title_translation": "视频中的开放世界目标计数", "tldr": "本文提出了一种在视频中进行开放世界目标计数的新任务，并介绍了模型CountVid和数据集VideoCount，以准确识别和计数视频中指定的目标实例。", "motivation": "在视频中进行开放世界目标计数是一项新任务，尤其在拥挤、有遮挡和相似物体的场景中，避免重复计数和识别再出现的目标至关重要，因此需要一种新的方法来解决这些挑战。", "method": "本文提出了CountVid模型，该模型结合了基于图像的计数模型和可提示的视频分割与跟踪模型，以实现跨视频帧的自动化开放世界目标计数。同时，还引入了VideoCount数据集，该数据集基于TAO、MOT20以及企鹅和金属合金结晶的X射线视频构建，用于评估模型性能。", "result": "CountVid模型在VideoCount数据集上提供了准确的目标计数，并显著优于强大的基线模型。", "conclusion": "本文成功引入了视频中开放世界目标计数的新任务、CountVid模型和VideoCount数据集，并证明了CountVid在准确计数方面的优越性能。", "translation": "我们引入了一项视频中开放世界目标计数的新任务：给定文本描述或图像示例来指定目标对象，目标是枚举视频中目标对象的所有唯一实例。这项任务在拥挤、有遮挡和相似物体的场景中尤其具有挑战性，其中避免重复计数和识别重新出现的目标至关重要。为此，我们做出了以下贡献：我们为这项任务引入了一个模型CountVid。它利用基于图像的计数模型和可提示的视频分割和跟踪模型，以实现跨视频帧的自动化、开放世界目标计数。为了评估其性能，我们引入了VideoCount，这是一个为我们新任务而构建的新数据集，其数据来源于TAO和MOT20跟踪数据集，以及通过X射线捕获的企鹅和金属合金结晶视频。使用这个数据集，我们证明了CountVid提供了准确的目标计数，并显著优于强大的基线。VideoCount数据集、CountVid模型和所有代码均可在https://github.com/niki-amini-naieni/CountVid/获取。", "summary": "本文提出了一项名为“视频中开放世界目标计数”的新任务，旨在根据文本或图像示例在视频中识别并枚举所有唯一的目标实例。为了应对拥挤、遮挡和相似目标等挑战，研究人员开发了CountVid模型，该模型结合了图像计数和视频分割跟踪技术。为评估CountVid，还创建了VideoCount数据集。实验结果表明，CountVid能够提供准确的计数，并显著优于现有基线。", "keywords": "开放世界目标计数, 视频分析, CountVid, VideoCount, 目标跟踪", "comments": "这项工作通过引入“开放世界目标计数”这一新任务，填补了视频分析领域的一个空白，该任务具有很高的实用价值。CountVid模型结合了现有技术，并通过新的数据集进行验证，显示出良好的性能。其创新性在于任务的定义以及解决方案的整合，为未来相关研究奠定了基础。"}}
{"id": "2506.15115", "title": "Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions", "authors": ["Lulu Xue", "Shengshan Hu", "Wei Lu", "Yan Shen", "Dongxu Li", "Peijin Guo", "Ziqi Zhou", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "summary": "With growing demands for privacy protection, security, and legal compliance\n(e.g., GDPR), machine unlearning has emerged as a critical technique for\nensuring the controllability and regulatory alignment of machine learning\nmodels. However, a fundamental challenge in this field lies in effectively\nverifying whether unlearning operations have been successfully and thoroughly\nexecuted. Despite a growing body of work on unlearning techniques, verification\nmethodologies remain comparatively underexplored and often fragmented. Existing\napproaches lack a unified taxonomy and a systematic framework for evaluation.\nTo bridge this gap, this paper presents the first structured survey of machine\nunlearning verification methods. We propose a taxonomy that organizes current\ntechniques into two principal categories -- behavioral verification and\nparametric verification -- based on the type of evidence used to assess\nunlearning fidelity. We examine representative methods within each category,\nanalyze their underlying assumptions, strengths, and limitations, and identify\npotential vulnerabilities in practical deployment. In closing, we articulate a\nset of open problems in current verification research, aiming to provide a\nfoundation for developing more robust, efficient, and theoretically grounded\nunlearning verification mechanisms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15115v1", "AI": {"title_translation": "迈向可靠遗忘：机器学习遗忘验证、挑战与未来方向综述", "tldr": "该论文首次对机器学习遗忘验证方法进行了结构化综述，提出了一个统一的分类法，并探讨了现有方法的挑战和未来研究方向，以解决遗忘操作验证的根本挑战。", "motivation": "随着对隐私保护、安全性及法律合规（如GDPR）的需求日益增长，机器学习遗忘已成为确保机器学习模型可控性和法规一致性的关键技术。然而，该领域的一个根本挑战在于如何有效验证遗忘操作是否成功且彻底地执行。尽管关于遗忘技术的工作不断增多，但验证方法的研究相对不足且分散，现有方法缺乏统一的分类法和系统的评估框架。", "method": "本文首次对机器学习遗忘验证方法进行了结构化综述。作者提出了一种分类法，将当前技术分为行为验证和参数验证两大类，并在此基础上检查了每类中的代表性方法，分析了其基本假设、优点和局限性，并指出了实际部署中潜在的漏洞。", "result": "本研究提出了首个机器学习遗忘验证方法的结构化综述，并提出了一个统一的分类法，将现有技术组织为行为验证和参数验证两大主要类别。研究分析了各类代表性方法的假设、优缺点，并识别了潜在的部署漏洞。", "conclusion": "本文阐明了当前验证研究中的一系列开放问题，旨在为开发更鲁棒、高效和有理论基础的遗忘验证机制提供基础。", "translation": "随着对隐私保护、安全性及法律合规（如GDPR）的需求日益增长，机器学习遗忘已成为确保机器学习模型可控性和法规一致性的关键技术。然而，该领域的一个根本挑战在于如何有效验证遗忘操作是否成功且彻底地执行。尽管关于遗忘技术的工作不断增多，但验证方法的研究相对不足且分散。现有方法缺乏统一的分类法和系统的评估框架。为了弥合这一差距，本文首次对机器学习遗忘验证方法进行了结构化综述。我们提出了一种分类法，将当前技术根据用于评估遗忘保真度的证据类型，分为行为验证和参数验证两大主要类别。我们研究了每类中的代表性方法，分析了它们的基本假设、优点和局限性，并指出了实际部署中潜在的漏洞。最后，我们阐明了当前验证研究中的一系列开放问题，旨在为开发更鲁棒、高效和有理论基础的遗忘验证机制提供基础。", "summary": "本论文旨在解决机器学习遗忘领域中验证操作有效性的核心挑战。作为首次结构化综述，它提出了一个统一的分类法，将现有的遗忘验证方法分为行为验证和参数验证两大类。论文深入分析了每类方法的假设、优缺点及潜在漏洞，并提出了未来研究的开放性问题，为构建更可靠的遗忘验证机制奠定基础。", "keywords": "机器学习遗忘, 验证, 综述, 隐私保护, GDPR", "comments": "本文的创新之处在于它是首个对机器学习遗忘验证方法进行结构化综述的论文，并提出了一个统一的分类法，填补了现有研究中验证方法分散且缺乏系统性框架的空白。其重要性在于为机器学习遗忘的可靠性提供了理论基础和方向，对于隐私保护和合规性具有重要意义。通过识别开放问题，为未来研究指明了清晰的路径。"}}
{"id": "2506.15598", "title": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso", "Pedro Pinto", "Abel Ferreira", "Luís Abreu", "Isabel Rangel", "Sandra Monteiro"], "summary": "While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.", "comment": "This is a preprint version of the manuscript currently under review\n  at an international journal", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15598v1", "AI": {"title_translation": "从模型到课堂：评估葡萄牙语生成式多项选择题的叙事性和难度", "tldr": "本研究评估了生成式AI模型在葡萄牙语阅读理解多项选择题（MCQs）生成方面的能力，重点关注叙事元素和难度。结果显示模型可以生成与人工题目质量相当的MCQs，但仍存在语义清晰度和干扰项生成方面的挑战。", "motivation": "多项选择题（MCQs）对学习和评估很有价值，但手动创建不同难度和阅读技能的MCQs既耗时又昂贵。尽管生成式AI在自动化MCQ生成方面取得了进展，但对其质量和可靠性的评估（尤其是在生成失败的情况下）关注有限，且大多数研究集中在英语，忽略了其他语言（如葡萄牙语）。", "method": "本研究调查了当前生成模型在为葡萄牙语阅读理解生成MCQs方面的能力，特别关注与课程相关的叙事元素和不同难度级别。通过专家评审和分析学生回答中提取的心理测量属性来评估这些MCQs，以评估它们对小学生的适用性。", "result": "结果表明，当前模型可以生成与人工编写的MCQs质量相当的题目。然而，研究也发现了与语义清晰度和可回答性相关的问题。此外，在生成能够吸引学生并符合高质量MCQ选项设计标准的干扰项方面仍然存在挑战。", "conclusion": "当前模型在生成葡萄牙语MCQs方面显示出潜力，能够达到与人工题目相当的质量，但在语义清晰度、可回答性以及创建高质量干扰项方面仍需改进，以更好地适用于实际教学环境。", "translation": "虽然多项选择题（MCQs）对学习和评估很有价值，但手动创建不同难度级别和针对性阅读技能的MCQs仍然是一项耗时且成本高昂的任务。生成式AI的最新进展为高效自动化MCQ生成提供了机会。然而，对生成式MCQs的实际质量和可靠性的评估受到的关注有限——特别是在生成失败的情况下。当生成的MCQs旨在应用于真实世界环境时，这方面变得尤为重要。此外，大多数MCQ生成研究都集中在英语，而其他语言则未得到充分探索。本文研究了当前生成模型在为葡萄牙语（一种形态丰富的语言）阅读理解生成MCQs方面的能力。我们的研究重点是生成与课程相关的叙事元素对齐并涵盖不同难度级别的MCQs。我们通过专家评审和分析从学生回答中提取的心理测量属性来评估这些MCQs，以评估它们对小学生的适用性。我们的结果表明，当前模型可以生成与人工编写的MCQs质量相当的题目。然而，我们发现了一些与语义清晰度和可回答性相关的问题。此外，在生成能够吸引学生并符合高质量MCQ选项设计标准的干扰项方面仍然存在挑战。", "summary": "本研究旨在评估生成式AI模型在为葡萄牙语阅读理解生成多项选择题（MCQs）方面的能力，以解决手动创建题目耗时且昂贵、现有研究多集中于英语的痛点。研究侧重于生成符合课程叙事元素且难度各异的MCQs，并通过专家评审和学生响应的心理测量分析进行评估。结果显示，模型能生成与人工题目质量相当的MCQs，但仍需在语义清晰度、可回答性和高质量干扰项生成方面进行改进，以更好地应用于实际教学。", "keywords": "MCQ生成, 葡萄牙语, 生成式AI, 阅读理解, 心理测量", "comments": "该论文的创新之处在于其专注于葡萄牙语这一形态丰富的语言，并结合了叙事元素和难度控制来生成MCQs，这对于非英语语言的自动化教育评估具有重要意义。通过专家评审和学生心理测量分析相结合的评估方法也增强了研究的实用性。然而，论文指出的语义清晰度、可回答性以及干扰项生成方面的挑战，揭示了当前生成模型在产生高质量、可教学内容方面仍需克服的关键局限性。"}}
{"id": "2506.15369", "title": "Unsupervised Pelage Pattern Unwrapping for Animal Re-identification", "authors": ["Aleksandr Algasov", "Ekaterina Nepovinnykh", "Fedor Zolotarev", "Tuomas Eerola", "Heikki Kälviäinen", "Pavel Zemčík", "Charles V. Stewart"], "summary": "Existing individual re-identification methods often struggle with the\ndeformable nature of animal fur or skin patterns which undergo geometric\ndistortions due to body movement and posture changes. In this paper, we propose\na geometry-aware texture mapping approach that unwarps pelage patterns, the\nunique markings found on an animal's skin or fur, into a canonical UV space,\nenabling more robust feature matching. Our method uses surface normal\nestimation to guide the unwrapping process while preserving the geometric\nconsistency between the 3D surface and the 2D texture space. We focus on two\nchallenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards\n(Panthera pardus). Both species have distinctive yet highly deformable fur\npatterns. By integrating our pattern-preserving UV mapping with existing\nre-identification techniques, we demonstrate improved accuracy across diverse\nposes and viewing angles. Our framework does not require ground truth UV\nannotations and can be trained in a self-supervised manner. Experiments on seal\nand leopard datasets show up to a 5.4% improvement in re-identification\naccuracy.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15369v1", "AI": {"title_translation": "无监督动物皮毛图案展开用于动物重识别", "tldr": "本文提出一种无监督的几何感知纹理映射方法，将动物皮毛图案展开到标准UV空间，以解决动物重识别中因身体运动导致的图案变形问题。该方法在海豹和豹子数据集上将重识别准确率提高了5.4%。", "motivation": "现有的动物个体重识别方法难以处理动物皮毛或皮肤图案的形变性，这些图案会因身体运动和姿势变化而发生几何畸变。", "method": "本文提出一种几何感知的纹理映射方法，将动物特有的皮毛图案展开到规范的UV空间，以实现更稳健的特征匹配。该方法利用表面法线估计来指导展开过程，同时保持3D表面和2D纹理空间之间的几何一致性。该框架不需要真值UV标注，可以以自监督方式训练。", "result": "在海豹和豹子数据集上的实验表明，重识别准确率提高了5.4%。该方法在不同姿势和视角下均表现出更高的准确性。", "conclusion": "本文提出的无监督皮毛图案展开方法，通过将图案映射到规范UV空间并保持几何一致性，有效解决了动物重识别中皮毛图案形变的问题，显著提升了识别准确率。", "translation": "现有的个体重识别方法常常难以处理动物皮毛或皮肤图案的可变形性质，这些图案会因身体运动和姿势变化而发生几何畸变。在本文中，我们提出了一种几何感知的纹理映射方法，将动物皮肤或皮毛上独特的皮毛图案展开到规范的UV空间，从而实现更稳健的特征匹配。我们的方法利用表面法线估计来指导展开过程，同时保持3D表面和2D纹理空间之间的几何一致性。我们专注于两种具有挑战性的物种：塞马环斑海豹（Pusa hispida saimensis）和豹子（Panthera pardus）。这两种物种都具有独特但高度可变形的皮毛图案。通过将我们保留图案的UV映射与现有重识别技术相结合，我们证明了在不同姿势和视角下准确性的提高。我们的框架不需要真值UV标注，并且可以以自监督方式进行训练。在海豹和豹子数据集上的实验表明，重识别准确率提高了5.4%。", "summary": "本文提出了一种无监督的几何感知纹理映射方法，旨在解决动物重识别中因身体运动和姿势变化导致的皮毛图案变形问题。该方法将独特的皮毛图案展开到规范的UV空间，并通过表面法线估计指导展开过程，同时保持几何一致性。该框架无需真值UV标注，可自监督训练。在塞马环斑海豹和豹子数据集上的实验表明，该方法与现有重识别技术结合后，在不同姿势和视角下均能提高重识别准确率，最高可达5.4%。", "keywords": "动物重识别, 皮毛图案, 无监督学习, 纹理映射, UV展开", "comments": "该论文的创新点在于提出了一个无监督的几何感知纹理映射方法，用于解决动物重识别中皮毛图案形变这一核心难题。通过将图案展开到规范的UV空间并利用表面法线估计来保持几何一致性，该方法显著提升了在具有挑战性物种（如海豹和豹子）上的重识别准确率。其自监督训练的特性也降低了对标注数据的依赖，具有重要的实用价值。"}}
{"id": "2506.15181", "title": "ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed Machine Learning", "authors": ["Bing Liu", "Chengcheng Zhao", "Li Chai", "Peng Cheng", "Yaonan Wang"], "summary": "Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15181v1", "AI": {"title_translation": "ImprovDML: 改进的私有拜占庭容错分布式机器学习中的权衡", "tldr": "ImprovDML是一种去中心化分布式机器学习框架，通过结合弹性向量共识算法和多变量高斯噪声，在保证隐私和拜占庭容错的同时，显著提高了模型精度，并提供比现有工作更严格的理论保证。", "motivation": "在分布式机器学习（DML）中，同时处理拜占庭攻击和隐私泄露是一个重要问题。将拜占庭容错聚合规则与差分隐私机制相结合的常见策略通常会导致模型精度显著下降。", "method": "提出了一种名为ImprovDML的去中心化DML框架。该框架利用一种弹性向量共识算法，在每次迭代中计算正常（非拜占庭）代理的凸包内的一个点进行弹性聚合。然后，引入多变量高斯噪声到梯度中以保护隐私。隐私分析采用集中地理隐私（concentrated geo-privacy）的概念。", "result": "该框架在确保隐私保护和拜占庭攻击弹性的同时，实现了高模型精度。在非凸设置下，提供了比现有工作更严格的收敛保证和渐近学习误差界。与差分隐私相比，它在隐私保护和模型精度之间实现了更好的权衡。数值模拟验证了理论结果。", "conclusion": "ImprovDML框架在分布式机器学习中有效地解决了拜占庭攻击、隐私泄露和模型精度之间的权衡问题，通过采用创新的聚合和隐私机制，实现了优于现有方法的性能和更严格的理论保证。", "translation": "联合解决分布式机器学习（DML）中的拜占庭攻击和隐私泄露已成为一个重要问题。一种常见的策略涉及将拜占庭容错聚合规则与差分隐私机制相结合。然而，这些技术的结合通常会导致模型精度显著下降。为了解决这个问题，我们提出了一种名为ImprovDML的去中心化DML框架，该框架在确保隐私保护和拜占庭攻击弹性的同时，实现了高模型精度。该框架利用一种弹性向量共识算法，可以在每次迭代中计算正常（非拜占庭）代理的凸包内的一个点进行弹性聚合。然后，引入多变量高斯噪声到梯度中以保护隐私。我们在非凸设置下提供了收敛保证，并推导了渐近学习误差界，这些界限比现有工作中报告的更严格。对于隐私分析，我们采用了集中地理隐私（concentrated geo-privacy）的概念，该概念根据输入之间的欧几里得距离量化隐私保护。我们证明，与差分隐私相比，它在隐私保护和模型精度之间实现了改进的权衡。最后，数值模拟验证了我们的理论结果。", "summary": "ImprovDML是一个去中心化的分布式机器学习框架，旨在解决拜占庭攻击和隐私泄露导致模型精度下降的问题。它通过利用弹性向量共识算法进行聚合，并在梯度中引入多变量高斯噪声来保护隐私。该框架采用集中地理隐私概念进行隐私分析，并实现了高模型精度、隐私保护和拜占庭攻击弹性之间的改进权衡。研究提供了在非凸设置下更严格的收敛保证和学习误差界，并通过数值模拟验证了理论结果，表明其在性能上优于现有方法。", "keywords": "分布式机器学习, 拜占庭容错, 隐私保护, 权衡, 集中地理隐私", "comments": "这篇论文的创新点在于提出了ImprovDML框架，它有效地解决了分布式机器学习中拜占庭容错、隐私保护和模型精度之间的关键权衡问题。通过引入弹性向量共识算法和利用集中地理隐私概念，该工作在理论上提供了比现有方法更严格的收敛保证和误差界限，并在实践中展示了更好的性能，这对于安全且高效的分布式学习具有重要意义。"}}
{"id": "2506.15617", "title": "The Compositional Architecture of Regret in Large Language Models", "authors": ["Xiangxiang Cui", "Shu Yang", "Tianjin Huang", "Wanyu Lin", "Lijie Hu", "Di Wang"], "summary": "Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.", "comment": "23 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15617v1", "AI": {"title_translation": "大型语言模型中“后悔”的构成架构", "tldr": "研究大型语言模型中“后悔”表达的内部机制，提出了构建数据集和识别后悔表示层及后悔神经元的指标，并成功识别了相关层和神经元类型。", "motivation": "研究大型语言模型中的“后悔”机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。目前面临缺乏专门数据集、缺乏识别最优后悔表示层和分析后悔神经元的指标等挑战。", "method": "提出了一个构建后悔数据集的工作流；引入了监督压缩-解耦指数（S-CDI）指标来识别最优后悔表示层；提出了后悔主导分数（RDS）指标来识别后悔神经元，以及群组影响系数（GIC）来分析激活模式。", "result": "成功使用S-CDI指标识别了最优后悔表示层，显著提高了探针分类实验的性能；发现了模型层间存在M形解耦模式，揭示了信息处理如何在耦合和解耦阶段交替进行；通过RDS指标将神经元分为后悔神经元、非后悔神经元和双重神经元三类。", "conclusion": "Not mentioned in abstract", "translation": "大型语言模型中的“后悔”是指当模型遇到与其先前生成的错误信息相矛盾的证据时，其所表达的明确后悔。研究后悔机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。为了理解这一机制，我们首先需要识别模型输出中的后悔表达，然后分析其内部表示。这种分析需要检查模型隐藏状态，即信息在神经元层面进行处理的地方。然而，这面临三个关键挑战：（1）缺乏捕捉后悔表达的专门数据集，（2）缺乏寻找最优后悔表示层的指标，以及（3）缺乏识别和分析后悔神经元的指标。为了解决这些局限性，我们提出：（1）一个通过策略性设计的提示场景构建全面后悔数据集的工作流，（2）监督压缩-解耦指数（S-CDI）指标来识别最优后悔表示层，以及（3）后悔主导分数（RDS）指标来识别后悔神经元和群组影响系数（GIC）来分析激活模式。我们的实验结果成功地使用S-CDI指标识别了最优后悔表示层，这显著提高了探针分类实验的性能。此外，我们发现在模型层中存在M形解耦模式，揭示了信息处理如何在耦合和解耦阶段交替进行。通过RDS指标，我们将神经元分为三个不同的功能组：后悔神经元、非后悔神经元和双重神经元。", "summary": "这项研究探讨了大型语言模型中“后悔”机制的构成架构，即模型在面对与其先前错误信息相悖的证据时所表现出的明确后悔。研究旨在提高模型可靠性并理解认知编码。为克服数据和度量缺乏的挑战，研究提出了构建后悔数据集的工作流，以及S-CDI、RDS和GIC等新指标。实验成功识别了最优后悔表示层，发现了M形解耦模式，并将神经元分为后悔、非后悔和双重三类，为理解LLM内部认知过程提供了新见解。", "keywords": "大型语言模型, 后悔机制, 内部表示, 神经元分析, 可解释性", "comments": "这篇论文通过提出一套新颖的方法和指标（如S-CDI, RDS, GIC），创新性地解决了大型语言模型中“后悔”机制研究的关键挑战，包括数据集缺乏和缺乏量化分析工具。其发现的M形解耦模式和神经元分类对于理解LLM的内部信息处理和认知编码具有重要意义，有助于提升模型的可解释性和可靠性。"}}
{"id": "2506.15381", "title": "When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class", "authors": ["Yujin Kim", "Hyunsoo Kim", "Hyunwoo J. Kim", "Suhyun Kim"], "summary": "Open-source pre-trained models hold great potential for diverse applications,\nbut their utility declines when their training data is unavailable. Data-Free\nImage Synthesis (DFIS) aims to generate images that approximate the learned\ndata distribution of a pre-trained model without accessing the original data.\nHowever, existing DFIS meth ods produce samples that deviate from the training\ndata distribution due to the lack of prior knowl edge about natural images. To\novercome this limitation, we propose DDIS, the first Diffusion-assisted\nData-free Image Synthesis method that leverages a text-to-image diffusion model\nas a powerful image prior, improving synthetic image quality. DDIS extracts\nknowledge about the learned distribution from the given model and uses it to\nguide the diffusion model, enabling the generation of images that accurately\nalign with the training data distribution. To achieve this, we introduce Domain\nAlignment Guidance (DAG) that aligns the synthetic data domain with the\ntraining data domain during the diffusion sampling process. Furthermore, we\noptimize a single Class Alignment Token (CAT) embedding to effectively capture\nclass-specific attributes in the training dataset. Experiments on PACS and Ima\ngeNet demonstrate that DDIS outperforms prior DFIS methods by generating\nsamples that better reflect the training data distribution, achieving SOTA\nperformance in data-free applications.", "comment": "Published at ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15381v1", "AI": {"title_translation": "当模型知识遇到扩散模型：扩散辅助的无数据图像合成，结合域与类对齐", "tldr": "DDIS是一种新的扩散辅助无数据图像合成方法，它利用文本到图像扩散模型作为强大的图像先验，并通过域对齐和类对齐来提高合成图像的质量，超越了现有方法。", "motivation": "现有无数据图像合成（DFIS）方法在缺乏自然图像先验知识的情况下，生成的样本偏离训练数据分布，导致合成图像质量不佳。", "method": "本文提出了DDIS，首个扩散辅助的无数据图像合成方法。它利用文本到图像扩散模型作为强大的图像先验，并从给定模型中提取知识来指导扩散模型。具体地，DDIS引入了域对齐指导（DAG）以在扩散采样过程中将合成数据域与训练数据域对齐，并优化了一个单一的类对齐令牌（CAT）嵌入以有效捕获训练数据集中的类特定属性。", "result": "在PACS和ImageNet上的实验表明，DDIS生成了更能反映训练数据分布的样本，性能优于先前的无数据图像合成方法，在无数据应用中达到了最先进（SOTA）的性能。", "conclusion": "DDIS通过结合扩散模型作为强大的图像先验，并引入域对齐和类对齐机制，显著提高了无数据图像合成的质量和与训练数据分布的匹配度，实现了SOTA性能。", "translation": "开源预训练模型在各种应用中具有巨大潜力，但当其训练数据不可用时，其效用会下降。无数据图像合成（DFIS）旨在生成近似预训练模型学习到的数据分布的图像，而无需访问原始数据。然而，现有DFIS方法由于缺乏自然图像的先验知识，生成的样本偏离训练数据分布。为了克服这一限制，我们提出了DDIS，这是第一个扩散辅助的无数据图像合成方法，它利用文本到图像扩散模型作为强大的图像先验，提高了合成图像的质量。DDIS从给定模型中提取关于学习分布的知识，并用它来指导扩散模型，从而能够生成与训练数据分布精确对齐的图像。为了实现这一点，我们引入了域对齐指导（DAG），在扩散采样过程中将合成数据域与训练数据域对齐。此外，我们优化了一个单一的类对齐令牌（CAT）嵌入，以有效捕获训练数据集中的类特定属性。在PACS和ImageNet上的实验表明，DDIS通过生成更能反映训练数据分布的样本，性能优于先前的DFIS方法，在无数据应用中达到了最先进的性能。", "summary": "本文提出了DDIS，一种新颖的扩散辅助无数据图像合成方法，旨在解决现有DFIS方法因缺乏自然图像先验知识导致生成图像偏离训练数据分布的问题。DDIS利用文本到图像扩散模型作为强大的图像先验，并通过引入域对齐指导（DAG）和优化类对齐令牌（CAT）嵌入来确保合成图像与原始训练数据分布的高度对齐。实验证明，DDIS在PACS和ImageNet数据集上均超越了现有DFIS方法，在无数据应用中取得了最先进的性能。", "keywords": "无数据图像合成, 扩散模型, 域对齐, 类对齐, 图像生成", "comments": "DDIS的创新之处在于首次将强大的文本到图像扩散模型引入无数据图像合成领域，并提出了一套有效的对齐机制（DAG和CAT）来弥补现有方法在图像先验知识方面的不足。这极大地提高了在无数据限制下合成图像的质量和真实性，对于保护数据隐私和利用预训练模型具有重要意义。"}}
{"id": "2506.15190", "title": "Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors", "authors": ["Jiyi Wang", "Jingyang Ke", "Bo Dai", "Anqi Wu"], "summary": "Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.", "comment": "9 pages and 4 figures for the main text", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15190v1", "AI": {"title_translation": "学习任务无关的技能基础以揭示动物行为中的运动原语", "tldr": "本文提出了一种基于强化学习的模仿框架SKIL，用于理解动物行为，通过学习可解释的技能集并将其动态混合以生成复杂的行为，超越了传统离散模型的局限性。", "motivation": "现有的行为分割方法通过在限制性生成假设下施加离散的音节，从而过度简化了动物灵活重组有限核心运动原语以满足多样任务需求的过程。", "method": "我们引入了基于技能的模仿学习（SKIL），这是一种基于强化学习的模仿框架，用于行为理解。它通过利用转换概率上的表征学习来推断可解释的技能集（即行为的潜在基函数），并将策略参数化为这些技能的动态混合。", "result": "该方法在简单的网格世界、离散迷宫和自由移动动物的无约束视频中得到了验证。在各项任务中，它能够识别可重用的技能组件，学习持续演变的组合策略，并生成超出传统离散模型能力范围的真实轨迹。", "conclusion": "通过利用具有组合表示的生成行为建模，我们的方法为复杂动物行为如何从基本运动原语的动态组合中产生提供了一个简洁、有原则的解释。", "translation": "动物灵活地重组有限的核心运动原语以满足多样化的任务需求，但现有的行为分割方法通过在限制性生成假设下施加离散的音节，从而过度简化了这一过程。为了反映动物行为的生成过程，我们引入了基于技能的模仿学习（SKIL）用于行为理解，这是一个基于强化学习的模仿框架，它（1）通过利用转换概率上的表征学习来推断可解释的技能集，即行为的潜在基函数，并且（2）将策略参数化为这些技能的动态混合。我们在一个简单的网格世界、一个离散迷宫和自由移动动物的无约束视频上验证了我们的方法。在各项任务中，它识别出可重用的技能组件，学习持续演变的组合策略，并生成超出传统离散模型能力范围的真实轨迹。通过利用具有组合表示的生成行为建模，我们的方法为复杂动物行为如何从基本运动原语的动态组合中产生提供了一个简洁、有原则的解释。", "summary": "本文提出了一种名为SKIL（基于技能的模仿学习）的强化学习框架，旨在克服现有行为分割方法对动物运动原语过度简化的不足。SKIL通过表征学习推断可解释的技能集，并将行为策略参数化为这些技能的动态组合。该方法在不同任务中表现出色，能够识别可复用技能、学习组合策略并生成真实轨迹，为理解动物复杂行为的生成机制提供了新的视角。", "keywords": "运动原语, 技能学习, 强化学习, 模仿学习, 动物行为", "comments": "本文的创新之处在于提出了一种新颖的基于强化学习的模仿学习框架SKIL，它通过学习任务无关的技能基础来揭示动物行为中的运动原语。与传统依赖离散音节的模型不同，SKIL能够学习连续演变的组合策略并生成更真实的轨迹，这对于深入理解动物行为的复杂性和灵活性具有重要意义。该方法通过引入动态混合的技能和利用转换概率上的表征学习，提供了一种更符合生物学实际的行为生成模型。"}}
{"id": "2506.15404", "title": "NERO: Explainable Out-of-Distribution Detection with Neuron-level Relevance", "authors": ["Anju Chhetri", "Jari Korhonen", "Prashnna Gyawali", "Binod Bhattarai"], "summary": "Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15404v1", "AI": {"title_translation": "NERO：基于神经元级别相关性的可解释性分布外检测", "tldr": "NERO是一种新颖的分布外检测方法，通过利用神经元级别的相关性来提高医疗影像中OOD样本的可解释性和区分能力，并在基准测试中优于现有SOTA方法。", "motivation": "深度学习的可靠性至关重要，尤其是在医疗影像领域，因为诊断决策依赖于模型输出。OOD样本检测是衡量模型可靠性的重要指标，在医疗影像中可用于发现异常。现有许多OOD检测方法依赖于特征或logit空间表示，但可能无法完全捕获OOD多样性。", "method": "提出了一种名为NERO的新型OOD评分机制，该机制利用特征层的神经元级别相关性。具体方法是：为每个分布内（ID）类别聚类神经元级别相关性以形成代表性中心，并引入相关性距离度量来量化新样本与这些中心的偏差，从而增强OOD可分离性。此外，通过在偏差项中纳入缩放相关性并结合特征范数来优化性能。该框架还支持可解释的OOD检测。", "result": "在胃肠道影像基准数据集Kvasir和GastroVision上，NERO在多种深度学习架构上验证了其有效性，并实现了优于现有最先进OOD检测方法的改进。", "conclusion": "NERO通过利用神经元级别的相关性，成功地提高了OOD检测的性能和可解释性，尤其在医疗影像领域表现出色，优于现有SOTA方法，增强了深度学习模型的可靠性。", "translation": "确保深度学习的可靠性至关重要，尤其是在医疗影像领域，诊断决策常常依赖于模型输出。区分分布外（OOD）样本的能力已被证明是衡量模型可靠性的一个有价值的指标。在医疗影像中，这一点尤为关键，因为识别OOD输入有助于标记可能未被发现的潜在异常。虽然许多OOD检测方法依赖于特征或logit空间表示，但最近的研究表明这些方法可能无法完全捕获OOD的多样性。为了解决这个问题，我们提出了一种新颖的OOD评分机制，称为NERO，它利用特征层的神经元级别相关性。具体来说，我们为每个分布内（ID）类别聚类神经元级别相关性以形成代表性中心，并引入一个相关性距离度量来量化新样本与这些中心的偏差，从而增强OOD的可分离性。此外，我们通过在偏差项中纳入缩放相关性并结合特征范数来优化性能。我们的框架还支持可解释的OOD检测。我们在胃肠道影像基准数据集Kvasir和GastroVision上，在多种深度学习架构上验证了其有效性，并实现了优于现有最先进OOD检测方法的改进。", "summary": "本文提出了一种名为NERO的新型分布外（OOD）检测方法，旨在解决现有方法未能充分捕获OOD多样性的问题。NERO通过利用特征层的神经元级别相关性，为每个ID类别形成代表性中心，并引入相关性距离度量来增强OOD样本的可分离性。该方法还在偏差项中结合了缩放相关性和特征范数以优化性能，并提供了可解释的OOD检测能力。在医疗影像基准数据集上的实验结果表明，NERO在多种深度学习架构上均优于现有最先进的OOD检测方法。", "keywords": "分布外检测, 神经元相关性, 可解释性AI, 医疗影像, 深度学习可靠性", "comments": "这篇论文的创新点在于引入了神经元级别的相关性来提升OOD检测的性能和可解释性，这与传统依赖特征或logit空间的方法有所不同。在医疗影像这种高风险领域，可解释的OOD检测具有重要意义，有助于提高模型决策的透明度和可靠性。NERO通过量化新样本与ID类别神经元相关性中心的偏差，提供了一种新颖且有效的OOD识别机制。"}}
{"id": "2506.15199", "title": "Interpretability and Generalization Bounds for Learning Spatial Physics", "authors": ["Alejandro Francisco Queiruga", "Theo Gutman-Solo", "Shuai Jiang"], "summary": "While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15199v1", "AI": {"title_translation": "学习空间物理的解释性与泛化界限", "tldr": "本文量化了机器学习在科学应用中（以1D泊松方程为例）的准确性与泛化能力，证明了泛化界限，强调了数据函数空间的重要性，并提出了一种新的可解释性方法和泛化测量技术。", "motivation": "机器学习在科学问题中应用前景广阔，但视觉效果可能具有欺骗性，实际的定量准确性至关重要；需要将数值分析的严谨性应用于机器学习，量化不同ML技术在微分方程上的准确性；识别影响模型泛化的关键因素。", "method": "将数值分析的严谨性应用于机器学习，量化不同ML技术在基本一维泊松微分方程上的应用准确性；分析训练动态并推导白盒微分方程发现方法和黑盒线性模型的最佳参数；在有限数据离散化和受限训练数据子空间下证明泛化界限和收敛速度；通过经验验证复制分析导出的泛化界限；对深度线性模型、浅层神经网络以及物理专用DeepONets和神经算子进行经验性泛化演示；理论和经验性地展示了泛化到真实物理方程的挑战；基于理论分析，展示了一种新的机制可解释性视角，可从黑盒模型权重中提取格林函数表示。", "result": "在有限数据离散化和受限训练数据子空间下证明了泛化界限和收敛速度；分析导出的泛化界限得到了经验验证；经验性地证明了深度线性模型、浅层神经网络以及物理专用模型同样缺乏泛化能力；理论和经验性地表明在每个探索的案例中，泛化到真实物理方程都无法得到保证；发现不同类别的模型可以表现出相反的泛化行为；展示了一种新的机制可解释性视角，可以通过从黑盒模型权重中提取格林函数表示来应用于科学模型。", "conclusion": "在所探索的每种情况下，泛化到真实的物理方程都无法得到保证，并且不同类别的模型可能表现出相反的泛化行为。本文提出了一种新的机制可解释性方法，通过从黑盒模型权重中提取格林函数表示来实现；并提出了一种新的交叉验证技术来衡量物理系统中的泛化能力，建议将其应用于泊松方程作为未来方法的评估基准。", "translation": "虽然机器学习在科学问题中有许多看似有前景的应用，但视觉效果可能具有欺骗性。对于科学应用而言，实际的定量准确性至关重要。本研究通过量化不同机器学习技术应用于基本一维泊松微分方程的准确性，将数值分析的严谨性应用于机器学习。除了数据的数量和离散化，我们还发现数据的函数空间对模型的泛化至关重要。我们通过分析训练动态并推导白盒微分方程发现方法和黑盒线性模型的最佳参数，证明了在有限数据离散化和受限训练数据子空间下的泛化界限和收敛速度。分析导出的泛化界限得到了经验复制。类似的泛化不足在深度线性模型、浅层神经网络以及物理专用DeepONets和神经算子中也得到了经验证明。我们从理论和经验上证明，在每个探索的案例中，泛化到真实的物理方程都无法得到保证。令人惊讶的是，我们发现不同类别的模型可能表现出相反的泛化行为。基于我们的理论分析，我们还展示了一种新的机制可解释性视角，即可以从黑盒模型的权重中提取格林函数表示。我们的结果为衡量物理系统中泛化能力的新型交叉验证技术提供了信息。我们建议将其应用于泊松方程作为未来方法的评估基准。", "summary": "本文深入探讨了机器学习在科学问题中应用的定量准确性和泛化能力。研究以一维泊松微分方程为切入点，借鉴数值分析的严谨性，量化了不同机器学习技术在处理该方程时的准确性，并强调了数据函数空间对模型泛化的关键影响。作者不仅在有限数据离散化和受限训练数据子空间下证明了泛化界限和收敛速度，还发现即使是物理专用模型也可能无法有效泛化到真实的物理方程。此外，研究还提出了一种创新性的机制可解释性方法，即通过从黑盒模型的权重中提取格林函数表示，以及一种衡量物理系统泛化能力的新型交叉验证技术，并建议将泊松方程作为未来方法的评估基准。", "keywords": "泛化界限, 解释性, 空间物理, 泊松方程, 机器学习", "comments": "本文的创新之处在于将数值分析的严谨性引入机器学习在物理系统中的泛化和可解释性研究。它不仅提供了关于模型泛化能力的理论和经验证据，还提出了从黑盒模型中提取物理洞察力的新方法（格林函数表示），并为物理系统中的模型评估提供了一个新的交叉验证框架和基准。这对于推动机器学习在科学应用中从“看似有希望”到“真正准确可靠”具有重要意义。"}}
{"id": "2506.15629", "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability", "authors": ["Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "summary": "In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.", "comment": "ACL 2025 Main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15629v1", "AI": {"title_translation": "重新审视大型语言模型在考虑指令遵循能力时的组合泛化能力", "tldr": "本研究提出了Ordered CommonGen基准，用于评估大型语言模型在指令遵循下的组合泛化能力。研究发现，LLMs虽然理解指令意图，但存在偏向特定概念顺序模式的偏差，导致输出多样性低，且即使是表现最好的模型，其有序覆盖率也仅为75%，表明LLMs在指令遵循和组合泛化方面仍需改进。", "motivation": "在生成式常识推理任务中，大型语言模型在遵循特定概念顺序的指令时存在不足。为了解决这一问题，需要一个能同时评估组合泛化和指令遵循能力的基准。", "method": "提出了Ordered CommonGen基准，通过测量“有序覆盖率”来评估大型语言模型在生成句子时是否遵循指定的概念顺序，从而同时评估其组合泛化和指令遵循能力。对36个LLM进行了综合分析。", "result": "研究发现，大型语言模型虽然普遍理解指令意图，但对特定概念顺序模式存在偏见，导致输出多样性低，甚至在概念顺序改变时也产生相同结果。即使是指令遵循能力最好的LLM，其有序覆盖率也仅达到约75%。", "conclusion": "大型语言模型在指令遵循和组合泛化能力方面仍需显著改进，尤其是在处理概念顺序指令时。", "translation": "在CommonGen等生成式常识推理任务中，生成式大型语言模型（LLMs）会组合包含所有给定概念的句子。然而，当侧重于指令遵循能力时，如果提示指定了概念顺序，LLMs必须生成遵循指定顺序的句子。为了解决这个问题，我们提出了Ordered CommonGen，这是一个旨在评估LLMs组合泛化和指令遵循能力的基准。该基准通过测量有序覆盖率来评估概念是否按指定顺序生成，从而能够同时评估这两种能力。我们使用36个LLM进行了全面分析，发现LLMs虽然普遍理解指令意图，但对特定概念顺序模式的偏见常常导致输出多样性低或即使概念顺序改变也产生相同结果。此外，即使是指令遵循能力最强的LLM也仅达到了约75%的有序覆盖率，这突出表明指令遵循和组合泛化能力都需要改进。", "summary": "本研究提出了一个名为Ordered CommonGen的新基准，旨在同时评估大型语言模型（LLMs）的组合泛化能力和指令遵循能力，特别是在生成遵循特定概念顺序的句子方面。通过对36个LLMs的分析发现，尽管LLMs能理解指令意图，但它们普遍存在对特定概念顺序模式的偏见，导致输出缺乏多样性且未能完全遵循指令。即使是表现最佳的模型，其有序覆盖率也仅达到75%，这强调了LLMs在这两方面仍需大幅提升。", "keywords": "组合泛化, 指令遵循, 大型语言模型, Ordered CommonGen, 有序覆盖率", "comments": "该论文通过提出Ordered CommonGen基准，创新性地结合了组合泛化和指令遵循能力的评估，填补了现有评估方法的空白。其通过“有序覆盖率”这一指标，能够更细致地揭示LLMs在处理复杂指令时的深层问题，而非仅仅关注概念的包含。研究结果揭示了当前LLMs在指令遵循和输出多样性上的显著局限性，对未来LLM的研究方向具有重要指导意义。"}}
{"id": "2506.15442", "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material", "authors": ["Team Hunyuan3D", "Shuhui Yang", "Mingxin Yang", "Yifei Feng", "Xin Huang", "Sheng Zhang", "Zebin He", "Di Luo", "Haolin Liu", "Yunfei Zhao", "Qingxiang Lin", "Zeqiang Lai", "Xianghui Yang", "Huiwen Shi", "Zibo Zhao", "Bowen Zhang", "Hongyu Yan", "Lifu Wang", "Sicong Liu", "Jihong Zhang", "Meng Chen", "Liang Dong", "Yiwen Jia", "Yulin Cai", "Jiaao Yu", "Yixuan Tang", "Dongyuan Guo", "Junlin Yu", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Shida Wei", "Chao Zhang", "Yonghao Tan", "Yifu Sun", "Lin Niu", "Shirui Huang", "Bojian Zheng", "Shu Liu", "Shilin Chen", "Xiang Yuan", "Xiaofeng Yang", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Tian Liu", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Jingwei Huang", "Chunchao Guo"], "summary": "3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.", "comment": "Github link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15442v1", "AI": {"title_translation": "Hunyuan3D 2.1：从图像到高保真3D资产，支持生产级PBR材质", "tldr": "Hunyuan3D 2.1是一个教程，旨在通过其双组件系统（Hunyuan3D-DiT和Hunyuan3D-Paint）降低3D AIGC的复杂性，帮助用户生成高保真3D资产。", "motivation": "3D AI生成内容（AIGC）领域因数据收集、处理和模型训练的复杂性，主要局限于研究人员、开发人员和设计师。本教程旨在解决这些挑战，使该领域更易于访问。", "method": "本文以Hunyuan3D 2.1为例，提供了一个关于处理3D数据、训练3D生成模型和评估其性能的综合分步指南。Hunyuan3D 2.1系统包含两个核心组件：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint。教程涵盖了数据准备、模型架构、训练策略、评估指标和部署的整个工作流程。", "result": "通过使用Hunyuan3D 2.1系统，用户能够生成高分辨率、带纹理的3D资产。该教程旨在让用户掌握微调或开发强大的3D生成模型的知识。", "conclusion": "本教程旨在使学习者能够掌握开发或微调适用于游戏、虚拟现实和工业设计等领域的强大3D生成模型的知识。", "translation": "3D AI生成内容（AIGC）是一个充满激情的领域，它极大地加速了游戏、电影和设计中3D模型的创建。尽管已经开发出一些革命性的突破性模型，但由于收集、处理和训练3D模型所涉及的复杂性，该领域在很大程度上仍仅限于研究人员、开发人员和设计师。为了应对这些挑战，我们在此教程中以Hunyuan3D 2.1为例。本教程提供了关于处理3D数据、训练3D生成模型以及使用Hunyuan3D 2.1（一个用于生成高分辨率纹理3D资产的先进系统）评估其性能的全面分步指南。该系统包含两个核心组件：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint。我们将探讨整个工作流程，包括数据准备、模型架构、训练策略、评估指标和部署。通过本教程的学习，您将掌握微调或开发适用于游戏、虚拟现实和工业设计应用的强大3D生成模型的知识。", "summary": "Hunyuan3D 2.1是一个旨在降低3D AI生成内容（AIGC）领域复杂性的教程。它详细介绍了如何利用Hunyuan3D 2.1系统（由Hunyuan3D-DiT负责形状生成和Hunyuan3D-Paint负责纹理合成）来处理3D数据、训练和评估3D生成模型，最终实现从图像到高保真、生产级3D资产的创建，适用于游戏、VR和工业设计等领域。", "keywords": "3D AIGC, Hunyuan3D 2.1, 3D生成, PBR材质, 教程", "comments": "本文以教程的形式，通过Hunyuan3D 2.1系统，有效降低了3D AIGC的准入门槛，使其不仅限于专业研究者。其创新性在于提供了一个完整的、生产就绪的PBR材质3D资产生成工作流，并明确区分了形状生成和纹理合成的组件，具有很强的实用价值和推广意义。"}}
{"id": "2506.15251", "title": "Singular Value Decomposition on Kronecker Adaptation for Large Language Model", "authors": ["Yee Hin Chong", "Peng Qu"], "summary": "Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15251v1", "AI": {"title_translation": "大型语言模型中基于Kronecker自适应的奇异值分解", "tldr": "SoKA是一种新的PEFT方法，结合Kronecker分解和SVD初始化，动态选择秩，显著减少参数并保持或超越LLaMA2-7B在推理和代码任务上的性能。", "motivation": "大型预训练Transformer模型的完全微调会带来高昂的存储、内存和计算开销。现有参数高效微调（PEFT）方法存在不足，包括：适配器模块引入推理时延迟；随机初始化的低秩更新收敛不佳；基于Kronecker的分解依赖固定秩选择，可能不匹配任务复杂度。", "method": "本文提出SoKA（SVD on Kronecker Adaptation），一种新颖的PEFT策略，它将Kronecker积张量分解与SVD驱动的初始化和频谱感知的动态秩选择相结合。其中，Kronecker-Product SVD（KPSVD）过程将完整权重更新的主成分提取为紧凑的Kronecker因子，并使用自适应秩选择算法，通过能量阈值和肘点标准来修剪可忽略的成分。", "result": "在LLaMA2-7B模型上，SoKA在算术推理（GSM8K）、形式数学（MATH）和代码生成（MBPP）任务中，仅需0.99M可训练参数，比LoRA/PiSSA少25%，同时达到或超越基线性能。此外，SoKA表现出更快的收敛速度和更稳定的梯度。", "conclusion": "SoKA是一种鲁棒且高效的大规模模型自适应方法，通过结合Kronecker分解和SVD驱动的动态秩选择，显著减少了可训练参数，同时保持或提升了性能和收敛稳定性。", "translation": "大型预训练Transformer模型在各种语言和推理任务中取得了最先进的结果，但完全微调会带来大量的存储、内存和计算开销。参数高效微调（PEFT）方法通过仅学习一小部分任务特定参数来减轻这些成本，但现有方法要么引入推理时延迟（适配器模块），要么收敛不佳（随机初始化的低秩更新），要么依赖固定秩选择，可能不匹配任务复杂度（基于Kronecker的分解）。\n我们提出SoKA（SVD on Kronecker Adaptation），一种新颖的PEFT策略，它将Kronecker积张量分解与SVD驱动的初始化和频谱感知的动态秩选择相结合。我们的Kronecker-Product SVD（KPSVD）过程将完整权重更新的主成分提取为紧凑的Kronecker因子，而自适应秩选择算法使用能量阈值和肘点标准来修剪可忽略的成分。\n在LLaMA2-7B模型上对算术推理（GSM8K）、形式数学（MATH）和代码生成（MBPP）的实证评估表明，SoKA仅需0.99M可训练参数，比LoRA/PiSSA少25%，同时达到或超越基线性能。此外，SoKA表现出更快的收敛速度和更稳定的梯度，凸显了其在大规模模型自适应方面的鲁棒性和效率。", "summary": "本文提出SoKA，一种新的参数高效微调策略，通过结合Kronecker积张量分解和SVD驱动的初始化以及频谱感知的动态秩选择，解决了现有PEFT方法在推理延迟、收敛性和固定秩选择上的不足。实验证明，SoKA在LLaMA2-7B模型上显著减少了可训练参数（比LoRA/PiSSA少25%），同时在多项任务上保持或超越了基线性能，并展现出更快的收敛速度和更稳定的梯度，证明了其在大规模语言模型适应中的高效性和鲁棒性。", "keywords": "参数高效微调, Kronecker分解, 奇异值分解, 动态秩选择, 大型语言模型", "comments": "SoKA的创新点在于将Kronecker分解与SVD初始化和动态秩选择相结合，有效地解决了PEFT方法中秩选择的难题，并显著减少了参数量。其在大规模LLM上的优异表现，特别是参数效率和收敛稳定性，使其成为一个有前景的PEFT方案。"}}
{"id": "2506.15650", "title": "Oldies but Goldies: The Potential of Character N-grams for Romanian Texts", "authors": ["Dana Lupsa", "Sanda-Maria Avram"], "summary": "This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15650v1", "AI": {"title_translation": "经典永流传：字符N-gram在罗马尼亚语文本中的潜力", "tldr": "本研究表明，简单的字符N-gram特征结合ANN模型在罗马尼亚语作者归属识别上表现出色，甚至达到最先进（SOTA）水平。", "motivation": "本研究旨在解决罗马尼亚语文本的作者归属识别问题，并评估轻量级、可解释的字符N-gram特征在这一任务上的潜力，尤其是在资源受限或研究不足的语言环境中。", "method": "研究使用了ROST语料库，系统评估了六种机器学习技术：支持向量机（SVM）、逻辑回归（LR）、k近邻（k-NN）、决策树（DT）、随机森林（RF）和人工神经网络（ANN），并采用字符n-gram作为分类特征。", "result": "ANN模型表现最佳，在使用5-gram特征时，在十五次运行中有四次实现了完美分类。这些结果表明，轻量级、可解释的字符n-gram方法可以为罗马尼亚语作者归属提供最先进的准确性，与更复杂的方法相媲美。", "conclusion": "简单的文体学特征（如字符N-gram）在资源受限或研究不足的语言环境中具有巨大潜力，能够实现高精度的作者归属识别。", "translation": "本研究旨在解决使用ROST语料库（该领域的标准基准）对罗马尼亚语文本进行作者归属识别的问题。我们系统地评估了六种机器学习技术：支持向量机（SVM）、逻辑回归（LR）、k近邻（k-NN）、决策树（DT）、随机森林（RF）和人工神经网络（ANN），并采用字符n-gram特征进行分类。其中，ANN模型取得了最高性能，在使用5-gram特征时，在十五次运行中有四次实现了完美分类。这些结果表明，轻量级、可解释的字符n-gram方法可以为罗马尼亚语作者归属识别提供最先进的准确性，与更复杂的方法相媲美。我们的发现突出了简单文体特征在资源受限或研究不足的语言环境中的潜力。", "summary": "本研究探讨了使用字符N-gram特征对罗马尼亚语文本进行作者归属识别的有效性。研究人员在ROST语料库上系统评估了多种机器学习模型，发现人工神经网络（ANN）结合5-gram特征表现最佳，实现了高精度分类，甚至在部分情况下达到完美识别。这表明简单、可解释的字符N-gram方法在罗马尼亚语作者归属任务上具有与复杂方法相媲美的最先进性能，并突显了其在资源受限语言环境中的应用潜力。", "keywords": "作者归属, 字符N-gram, 罗马尼亚语, 机器学习, 人工神经网络", "comments": "这篇论文的创新之处在于强调了“老”方法（字符N-gram）在特定任务（罗马尼亚语作者归属）上的“金子”潜力。它挑战了“越复杂越好”的普遍观念，证明了简单、可解释的特征工程在低资源语言环境下也能达到最先进的性能，这对于计算资源有限或语言学研究不足的领域具有重要意义。"}}
{"id": "2506.15477", "title": "Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning", "authors": ["Chunlei Li", "Jingyang Hou", "Yilei Shi", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "summary": "Medical report generation from imaging data remains a challenging task in\nclinical practice. While large language models (LLMs) show great promise in\naddressing this challenge, their effective integration with medical imaging\ndata still deserves in-depth exploration. In this paper, we present MRG-LLM, a\nnovel multimodal large language model (MLLM) that combines a frozen LLM with a\nlearnable visual encoder and introduces a dynamic prompt customization\nmechanism. Our key innovation lies in generating instance-specific prompts\ntailored to individual medical images through conditional affine\ntransformations derived from visual features. We propose two implementations:\nprompt-wise and promptbook-wise customization, enabling precise and targeted\nreport generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets\ndemonstrate that MRG-LLM achieves state-of-the-art performance in medical\nreport generation. Our code will be made publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15477v1", "AI": {"title_translation": "多模态大语言模型用于通过定制化提示微调进行医疗报告生成", "tldr": "MRG-LLM是一个新的多模态大语言模型，通过定制化提示微调，在医疗报告生成任务上取得了最先进的性能。", "motivation": "从医学影像数据生成医疗报告在临床实践中仍然是一项具有挑战性的任务。虽然大型语言模型（LLMs）在这方面显示出巨大潜力，但它们与医学影像数据的有效整合仍需深入探索。", "method": "本文提出了MRG-LLM，一个新颖的多模态大语言模型（MLLM）。它结合了一个冻结的LLM和一个可学习的视觉编码器，并引入了动态提示定制机制。其关键创新在于通过从视觉特征导出的条件仿射变换生成针对个体医学图像的实例特定提示。提出了两种实现方式：提示级和提示簿级定制。", "result": "在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医疗报告生成方面取得了最先进的性能。", "conclusion": "MRG-LLM通过其新颖的多模态架构和动态提示定制机制，有效解决了从医学影像生成报告的挑战，并达到了最先进的性能。", "translation": "从影像数据生成医疗报告在临床实践中仍然是一项具有挑战性的任务。虽然大型语言模型（LLMs）在解决这一挑战方面显示出巨大的前景，但它们与医学影像数据的有效整合仍值得深入探索。在本文中，我们提出了MRG-LLM，一个新颖的多模态大语言模型（MLLM），它将一个冻结的LLM与一个可学习的视觉编码器结合起来，并引入了一种动态提示定制机制。我们的关键创新在于通过从视觉特征导出的条件仿射变换，生成针对个体医学图像的实例特定提示。我们提出了两种实现方式：提示级和提示簿级定制，从而实现精确和有针对性的报告生成。在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医疗报告生成方面取得了最先进的性能。我们的代码将公开可用。", "summary": "本文提出了MRG-LLM，一个用于医疗报告生成的新型多模态大语言模型。该模型结合了冻结的LLM和可学习的视觉编码器，并引入了动态提示定制机制，通过条件仿射变换生成实例特定的提示。实验证明MRG-LLM在IU X-ray和MIMIC-CXR数据集上达到了医疗报告生成的最新SOTA性能。", "keywords": "医疗报告生成, 多模态大语言模型, 提示微调, 视觉编码器, 动态提示定制", "comments": "该论文的创新点在于提出了动态提示定制机制，通过视觉特征生成实例特定的提示，有效桥接了医学影像与LLM之间的鸿沟，提高了医疗报告生成的精度和针对性。这是一个对多模态LLM在医疗领域应用的重要贡献。"}}
{"id": "2506.15662", "title": "CC-LEARN: Cohort-based Consistency Learning", "authors": ["Xiao Ye", "Shaswat Shrivastava", "Zhaonan Li", "Jacob Dineen", "Shijie Lu", "Avneet Ahuja", "Ming Shen", "Zhikun Xu", "Ben Zhou"], "summary": "Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15662v1", "AI": {"title_translation": "CC-LEARN：基于群组的一致性学习", "tldr": "CC-Learn是一个强化学习框架，通过在相似问题群组上训练，提高了大型语言模型推理的一致性和鲁棒性。", "motivation": "大型语言模型在许多任务中表现出色，但在一致、鲁棒的推理方面仍然存在困难。", "method": "本文引入了基于群组的一致性学习（CC-Learn），这是一个强化学习框架，通过在源自共享程序抽象的相似问题群组上进行训练，提高LLM推理的可靠性。为了强制执行群组级别的一致性，定义了一个复合目标，结合了群组准确性、有效问题分解的检索奖励，以及对琐碎或无效查找的拒绝惩罚，这些都可以通过强化学习直接优化。", "result": "在挑战性推理基准（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn比预训练和SFT基线提高了准确性和推理稳定性。", "conclusion": "这些结果表明，群组级别的强化学习有效地增强了大型语言模型（LLMs）的推理一致性。", "translation": "大型语言模型在许多任务中表现出色，但在一致、鲁棒的推理方面仍然存在困难。我们引入了基于群组的一致性学习（CC-Learn），这是一个强化学习框架，通过在源自共享程序抽象的相似问题群组上进行训练，提高了LLM推理的可靠性。为了强制执行群组级别的一致性，我们定义了一个复合目标，结合了群组准确性、有效问题分解的检索奖励，以及对琐碎或无效查找的拒绝惩罚，这些都可以通过强化学习直接优化，这与监督微调不同。优化这个奖励引导模型在所有群组成员中采用统一的推理模式。在挑战性推理基准（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn比预训练和SFT基线提高了准确性和推理稳定性。这些结果表明，群组级别的强化学习有效地增强了大型语言模型（LLMs）的推理一致性。", "summary": "CC-Learn是一个新颖的强化学习框架，旨在解决大型语言模型在推理一致性方面的不足。该方法通过在基于共享抽象的相似问题群组上进行训练，并优化一个结合了群组准确性、检索奖励和拒绝惩罚的复合目标，来引导模型形成统一的推理模式。实验证明，CC-Learn在多个推理基准上显著提升了LLM的准确性和推理稳定性。", "keywords": "大型语言模型, 强化学习, 推理一致性, 群组学习, CC-Learn", "comments": "CC-Learn的创新之处在于将强化学习应用于“群组”层面的推理一致性优化，通过设计一个复合奖励函数，有效解决了LLM在复杂推理任务中缺乏鲁棒性的问题。这种方法为提高LLM的可靠性提供了一个新的视角。"}}
{"id": "2506.15483", "title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects", "authors": ["Shujia Li", "Haiyu Zhang", "Xinyuan Chen", "Yaohui Wang", "Yutong Ban"], "summary": "While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15483v1", "AI": {"title_translation": "GenHOI：泛化文本驱动的未见物体4D人机交互合成", "tldr": "GenHOI是一个两阶段框架，用于泛化文本驱动的4D人机交互合成，能够处理未见物体并生成高保真序列，解决了大规模4D HOI数据集的稀缺性问题。", "motivation": "尽管扩散模型和大规模运动数据集推动了文本驱动的人体运动合成，但将其扩展到4D人机交互（HOI）仍然具有挑战性，主要是因为缺乏大规模的4D HOI数据集。", "method": "本研究提出了GenHOI，一个新颖的两阶段框架。第一阶段，使用Object-AnchorNet仅从3D HOI数据集中学习，重建未见物体的稀疏3D HOI关键帧，从而减少对大规模4D HOI数据集的依赖。第二阶段，引入一个Contact-Aware Diffusion Model (ContactDM)，将稀疏3D HOI关键帧无缝插值成密集的时间连贯4D HOI序列。为了提高生成质量，ContactDM中提出了一个新颖的Contact-Aware Encoder来提取人机接触模式，以及一个新颖的Contact-Aware HOI Attention来有效整合接触信号到扩散模型中。", "result": "实验结果表明，该方法在公开可用的OMOMO和3D-FUTURE数据集上取得了最先进的结果，展示了对未见物体的强大泛化能力，同时实现了高保真4D HOI生成。", "conclusion": "GenHOI框架通过其两阶段设计，成功解决了4D人机交互合成中未见物体泛化和高保真生成的问题，并在现有数据集上取得了领先性能。", "translation": "尽管扩散模型和大规模运动数据集推动了文本驱动的人体运动合成，但将其扩展到4D人机交互（HOI）仍然具有挑战性，主要是因为缺乏大规模的4D HOI数据集。在本研究中，我们引入了GenHOI，一个新颖的两阶段框架，旨在实现两个关键目标：1）泛化到未见物体，以及2）合成高保真4D HOI序列。在我们框架的初始阶段，我们采用Object-AnchorNet来重建未见物体的稀疏3D HOI关键帧，仅从3D HOI数据集中学习，从而减轻了对大规模4D HOI数据集的依赖。随后，我们在第二阶段引入了一个接触感知扩散模型（ContactDM），将稀疏3D HOI关键帧无缝插值成密集的时间连贯4D HOI序列。为了提高生成的4D HOI序列的质量，我们在ContactDM中提出了一个新颖的接触感知编码器来提取人机接触模式，以及一个新颖的接触感知HOI注意力机制来有效地将接触信号整合到扩散模型中。实验结果表明，我们在公开可用的OMOMO和3D-FUTURE数据集上取得了最先进的结果，展示了对未见物体的强大泛化能力，同时实现了高保真4D HOI生成。", "summary": "本论文提出了GenHOI，一个新颖的两阶段框架，用于解决文本驱动的4D人机交互（HOI）合成中对未见物体的泛化能力和高保真生成问题。针对大规模4D HOI数据集稀缺的挑战，GenHOI首先利用Object-AnchorNet从3D HOI数据中重建稀疏3D HOI关键帧，然后通过Contact-Aware Diffusion Model（ContactDM）将这些关键帧插值成密集的4D HOI序列。ContactDM引入了接触感知编码器和接触感知HOI注意力机制以增强生成质量。实验证明，GenHOI在OMOMO和3D-FUTURE数据集上达到了最先进水平，展现了对未见物体的强大泛化能力和高保真生成效果。", "keywords": "4D人机交互, 文本驱动合成, 扩散模型, 未见物体, 泛化", "comments": "GenHOI通过其创新的两阶段方法有效解决了4D人机交互合成中的核心挑战，即数据稀缺性和未见物体泛化。其亮点在于利用3D数据重建稀疏关键帧以减少对4D数据的依赖，并引入接触感知机制来提高序列质量。这对于推动通用型人机交互合成具有重要意义。"}}
{"id": "2506.15271", "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data", "authors": ["Bihe Zhao", "Pratyush Maini", "Franziska Boenisch", "Adam Dziedzic"], "summary": "The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.", "comment": "Accepted at ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15271v1", "AI": {"title_translation": "利用合成数据解锁事后数据集推断", "tldr": "现有的数据集推断（DI）方法需要难以获取的分布内私有数据集。本文通过合成生成高质量的私有数据集来克服这一限制，使得DI能够高置信度地检测到未经授权的数据集使用，从而帮助版权所有者进行维权。", "motivation": "大型语言模型（LLMs）的训练数据集通常未经数据所有者许可从互联网抓取，侵犯了知识产权。数据集推断（DI）可以识别某个可疑数据集是否被用于训练，从而帮助数据所有者验证未经授权的使用。然而，现有的DI方法需要一个与受损数据集分布紧密匹配的私有集合（已知未用于训练），这种数据在实践中很少可用，严重限制了DI的适用性。", "method": "本文通过合成生成所需的私有集合来解决现有问题。该方法解决了两个关键障碍：1) 创建高质量、多样化且准确反映原始分布的合成数据，这通过一个在精心设计的基于后缀的完成任务上训练的数据生成器实现；2) 通过事后校准弥合真实数据和合成数据之间的似然差距。", "result": "对各种文本数据集进行的广泛实验表明，使用本文生成的数据作为私有集合，使得DI能够高置信度地检测到原始训练集，同时保持较低的误报率。", "conclusion": "该研究结果使版权所有者能够对其数据使用提出合法主张，并证明了该方法在实际诉讼中的可靠性。", "translation": "大型语言模型（LLMs）的卓越能力主要归因于其庞大的训练数据集，这些数据集通常未经数据所有者知识产权许可从互联网上抓取。数据集推断（DI）通过识别可疑数据集是否被用于训练，提供了一种潜在的补救措施，从而使数据所有者能够验证未经授权的使用。然而，现有的DI方法需要一个私有集合——已知未用于训练——且该集合与受损数据集的分布紧密匹配。这种分布内、保留的数据在实践中很少可用，严重限制了DI的适用性。在这项工作中，我们通过合成生成所需的保留集合来解决这一挑战。我们的方法解决了两个关键障碍：(1) 创建高质量、多样化的合成数据，以准确反映原始分布，我们通过在精心设计的基于后缀的完成任务上训练的数据生成器来实现这一点；(2) 弥合真实数据和合成数据之间的似然差距，这通过事后校准实现。对各种文本数据集进行的广泛实验表明，使用我们生成的数据作为保留集合，使得DI能够高置信度地检测到原始训练集，同时保持较低的误报率。这一结果使版权所有者能够对其数据使用提出合法主张，并证明了我们方法在实际诉讼中的可靠性。我们的代码可在https://github.com/sprintml/PostHocDatasetInference 获取。", "summary": "本文提出了一种新方法，通过合成生成高质量的私有数据集来解决现有数据集推断（DI）方法在实际应用中面临的挑战。由于现有DI方法需要难以获取的、与训练数据同分布的私有数据，本文设计了一个数据生成器，通过后缀完成任务生成多样化合成数据，并采用事后校准来弥合真实与合成数据间的差异。实验结果表明，该方法使DI能够有效且高置信度地检测未经授权的数据集使用，为版权所有者提供了合法维权的工具。", "keywords": "数据集推断, 合成数据, 大语言模型, 版权保护, 数据隐私", "comments": "该论文创新性地解决了数据集推断（DI）在实际应用中受限于缺乏可用私有数据的痛点。通过引入合成数据生成和事后校准，极大地拓宽了DI方法的适用范围，使其能够真正应用于版权保护和数据使用追溯等实际场景。这项工作对于LLM时代的数据知识产权保护具有重要意义，其可靠性在真实诉讼中的潜力值得关注。"}}
{"id": "2506.15524", "title": "NTIRE 2025 Image Shadow Removal Challenge Report", "authors": ["Florin-Alexandru Vasluianu", "Tim Seizinger", "Zhuyun Zhou", "Cailian Chen", "Zongwei Wu", "Radu Timofte", "Mingjia Li", "Jin Hu", "Hainuo Wang", "Hengxing Liu", "Jiarui Wang", "Qiming Hu", "Xiaojie Guo", "Xin Lu", "Jiarong Yang", "Yuanfei Bao", "Anya Hu", "Zihao Fan", "Kunyu Wang", "Jie Xiao", "Xi Wang", "Xueyang Fu", "Zheng-Jun Zha", "Yu-Fan Lin", "Chia-Ming Lee", "Chih-Chung Hsu", "Xingbo Wang", "Dong Li", "Yuxu Chen", "Bin Chen", "Yuanbo Zhou", "Yuanbin Chen", "Hongwei Wang", "Jiannan Lin", "Qinquan Gao", "Tong Tong", "Zhao Zhang", "Yanyan Wei", "Wei Dong", "Han Zhou", "Seyed Amirreza Mousavi", "Jun Chen", "Haobo Liang", "Jiajie Jing", "Junyu Li", "Yan Yang", "Seoyeon Lee", "Chaewon Kim", "Ziyu Feng", "Shidi Chen", "Bowen Luan", "Zewen Chen", "Vijayalaxmi Ashok Aralikatti", "G Gyaneshwar Rao", "Nikhil Akalwadi", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Anas M. Ali", "Bilel Benjdira", "Wadii Boulila", "Alexandru Brateanu", "Cosmin Ancuti", "Tanmay Chaturvedi", "Manish Kumar", "Anmol Srivastav", "Daksh Trivedi", "Shashwat Thakur", "Kishor Upla", "Zeyu Xiao", "Zhuoyuan Li", "Boda Zhou", "Shashank Shekhar", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Suiyi Zhao", "Bo Wang", "Yan Luo", "Mingshen Wang", "Yilin Zhang"], "summary": "This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A\ntotal of 306 participants have registered, with 17 teams successfully\nsubmitting their solutions during the final evaluation phase. Following the\nlast two editions, this challenge had two evaluation tracks: one focusing on\nreconstruction fidelity and the other on visual perception through a user\nstudy. Both tracks were evaluated with images from the WSRD+ dataset,\nsimulating interactions between self- and cast-shadows with a large number of\ndiverse objects, textures, and materials.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15524v1", "AI": {"title_translation": "NTIRE 2025 图像去阴影挑战报告", "tldr": "NTIRE 2025 图像去阴影挑战报告，共有306名注册者，17支团队提交了解决方案，通过重建保真度和用户感知两方面进行了评估。", "motivation": "报告旨在总结NTIRE 2025图像去阴影挑战的结果和发现。", "method": "挑战设置了两个评估赛道：重建保真度评估和通过用户研究进行的视觉感知评估。所有评估均使用WSRD+数据集的图像进行，该数据集模拟了自阴影和投射阴影与大量多样对象、纹理和材料的相互作用。", "result": "共有306名参与者注册，17支团队在最终评估阶段成功提交了解决方案。", "conclusion": "报告了NTIRE 2025图像去阴影挑战的发现和结果。", "translation": "这项工作审视了NTIRE 2025去阴影挑战的发现。共有306名参与者注册，其中17支团队在最终评估阶段成功提交了解决方案。继前两届之后，本次挑战赛有两个评估赛道：一个侧重于重建保真度，另一个通过用户研究侧重于视觉感知。两个赛道都使用WSRD+数据集的图像进行评估，该数据集模拟了自阴影和投射阴影与大量多样对象、纹理和材料之间的相互作用。", "summary": "本报告总结了NTIRE 2025图像去阴影挑战的结果。此次挑战吸引了306名注册参与者，并有17支团队成功提交了解决方案。挑战设立了两个评估赛道：重建保真度与通过用户研究进行的视觉感知评估，均基于WSRD+数据集进行，该数据集包含模拟自阴影和投射阴影与多样化对象、纹理和材料相互作用的图像。", "keywords": "图像去阴影, NTIRE 2025, 挑战赛, 视觉感知, 重建保真度", "comments": "这篇报告是对一项计算机视觉挑战的总结，展示了在图像去阴影领域社区的参与度。挑战设置了重建保真度和视觉感知两个评估维度，这对于全面评估去阴影算法的性能至关重要。使用WSRD+数据集也表明了对真实世界复杂阴影场景的关注。"}}
{"id": "2506.15289", "title": "DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for Optimizing Large-Scale EV Charging Infrastructure", "authors": ["Chuan Li", "Shunyu Zhao", "Vincent Gauthier", "Hassine Moungla"], "summary": "The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15289v1", "AI": {"title_translation": "DOVA-PATBM：一个用于优化大规模电动汽车充电基础设施的智能、自适应和可扩展框架", "tldr": "DOVA-PATBM是一个智能、自适应且可扩展的地理计算框架，用于优化大规模电动汽车充电基础设施，通过整合多源数据和先进模型，显著提升覆盖率、公平性并满足电力限制。", "motivation": "随着电池电动汽车的普及，需要既数据丰富又地理上可扩展的基础设施规划工具。现有研究多集中于单一城市，而州级和国家级网络需协调大都市区、依赖汽车的郊区和电力受限的乡村走廊的冲突需求。", "method": "DOVA-PATBM是一个地理计算框架，它将异构数据（道路、人口、夜间灯光、POI和馈线）栅格化到分层H3网格上，使用区域归一化图神经网络中心性模型推断交叉口重要性，并叠加Voronoi细分以确保每30公里半径内至少有一个五端口直流快速充电器。通过循环探测器和浮动车轨迹学习的小时到达配置文件，输入有限的M/M/c排队模型以在馈线容量和停电风险约束下确定端口大小。然后，使用带有收入加权惩罚的贪婪最大覆盖启发式算法选择满足覆盖和公平目标的最小站点数量。", "result": "应用于美国佐治亚州，DOVA-PATBM (i) 将30公里瓦片覆盖率提高了12个百分点，(ii) 将低收入居民到最近充电器的平均距离减半，以及 (iii) 在任何地方都满足次输电净空——同时保持国家级部署的计算可行性。", "conclusion": "这些结果表明，一个紧密集成的、GNN驱动的多分辨率方法可以弥合学术优化和可部署基础设施政策之间的差距。", "translation": "电池电动汽车的加速普及要求基础设施规划工具既数据丰富又地理上可扩展。虽然大多数现有研究仅优化单一城市的充电位置，但州级和国家级网络必须协调人口密集的都市核心区、依赖汽车的郊区以及电力受限的乡村走廊之间的冲突需求。\n我们提出了DOVA-PATBM（基于Voronoi、自适应、POI感知时间行为模型的部署优化），这是一个将这些情境统一在一个管道中的地理计算框架。该方法将异构数据（道路、人口、夜间灯光、POI和馈线）栅格化到分层H3网格上，利用区域归一化图神经网络中心性模型推断交叉口的重要性，并叠加Voronoi细分，以确保在每30公里半径内至少有一个五端口直流快速充电器。从循环探测器和浮动车轨迹中学习到的每小时到达配置文件，输入一个有限的M/M/c排队模型，以在馈线容量和停电风险约束下确定端口大小。然后，采用带有收入加权惩罚的贪婪最大覆盖启发式算法，选择满足覆盖率和公平性目标的最小站点数量。\n应用于美国佐治亚州，DOVA-PATBM (i) 将30公里瓦片覆盖率提高了12个百分点，(ii) 将低收入居民到最近充电器的平均距离减半，以及 (iii) 在任何地方都满足次输电净空——所有这些都在国家级推广中保持计算可行性。这些结果表明，一个紧密集成的、GNN驱动的多分辨率方法可以弥合学术优化和可部署基础设施政策之间的差距。", "summary": "DOVA-PATBM是一个创新的地理计算框架，旨在解决大规模电动汽车充电基础设施规划的复杂性。它通过整合多源异构数据（如道路、人口、POI等）到分层H3网格，并利用图神经网络、Voronoi细分和排队理论来识别最佳充电站点。该框架还考虑了电力容量、停电风险和公平性等因素。在佐治亚州的应用表明，DOVA-PATBM显著提升了充电覆盖率，减少了低收入居民的出行距离，并确保了电网兼容性，同时保持了计算效率，为实际部署提供了可行的解决方案。", "keywords": "电动汽车充电, 基础设施优化, 地理计算, 图神经网络, Voronoi细分", "comments": "该论文的创新之处在于其提出了一个紧密集成的、GNN驱动的多分辨率地理计算框架DOVA-PATBM，能够有效处理大规模电动汽车充电基础设施规划中的异构数据和复杂约束。它不仅关注覆盖率，还兼顾了社会公平性和电网容量，弥合了理论优化与实际部署之间的差距，对于推动全国范围内的电动汽车基础设施建设具有重要意义。"}}
{"id": "2506.15676", "title": "Gender-Neutral Machine Translation Strategies in Practice", "authors": ["Hillary Dawkins", "Isar Nejadgholi", "Chi-kiu Lo"], "summary": "Gender-inclusive machine translation (MT) should preserve gender ambiguity in\nthe source to avoid misgendering and representational harms. While gender\nambiguity often occurs naturally in notional gender languages such as English,\nmaintaining that gender neutrality in grammatical gender languages is a\nchallenge. Here we assess the sensitivity of 21 MT systems to the need for\ngender neutrality in response to gender ambiguity in three translation\ndirections of varying difficulty. The specific gender-neutral strategies that\nare observed in practice are categorized and discussed. Additionally, we\nexamine the effect of binary gender stereotypes on the use of gender-neutral\ntranslation. In general, we report a disappointing absence of gender-neutral\ntranslations in response to gender ambiguity. However, we observe a small\nhandful of MT systems that switch to gender neutral translation using specific\nstrategies, depending on the target language.", "comment": "to appear at GITT 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15676v1", "AI": {"title_translation": "实践中的性别中立机器翻译策略", "tldr": "本文评估了21个机器翻译系统在处理性别模糊性时的性别中立表现，发现大多数系统缺乏性别中立翻译，但少数系统会根据目标语言采用特定策略。", "motivation": "性别包容的机器翻译（MT）应保留源语言中的性别模糊性，以避免性别错判和代表性伤害。在语法性别语言中保持性别中立是一个挑战，因此需要评估现有系统的表现。", "method": "评估了21个机器翻译系统在三种不同难度翻译方向上对性别模糊性中立处理的敏感性。对实践中观察到的具体性别中立策略进行了分类和讨论。此外，还研究了二元性别刻板印象对性别中立翻译使用的影响。", "result": "普遍存在令人失望的性别模糊性中立翻译缺失。然而，观察到少数机器翻译系统根据目标语言使用特定策略切换到性别中立翻译。", "conclusion": "大多数现有机器翻译系统在处理性别模糊性时未能实现性别中立，但少数系统展示了采取特定策略的能力，这表明在构建更具包容性的机器翻译方面仍有改进空间。", "translation": "性别包容的机器翻译（MT）应保留源语言中的性别模糊性，以避免性别错判和代表性伤害。虽然性别模糊性在英语等概念性别语言中自然存在，但在语法性别语言中保持性别中立是一个挑战。本文评估了21个机器翻译系统在三种不同难度翻译方向上对性别模糊性中立处理的需求敏感性。对实践中观察到的具体性别中立策略进行了分类和讨论。此外，我们还研究了二元性别刻板印象对性别中立翻译使用的影响。总的来说，我们报告了在处理性别模糊性时令人失望的性别中立翻译缺失。然而，我们观察到少数机器翻译系统根据目标语言使用特定策略切换到性别中立翻译。", "summary": "本文评估了21个机器翻译系统在处理性别模糊性时的性别中立表现。研究发现，尽管性别包容的机器翻译应避免性别错判，但大多数系统在响应性别模糊性时未能提供性别中立翻译。然而，少数系统展现了根据目标语言采用特定性别中立策略的能力。文章还分类讨论了观察到的策略，并探讨了二元性别刻板印象对性别中立翻译使用的影响。", "keywords": "机器翻译, 性别中立, 性别模糊性, 翻译策略, 性别刻板印象", "comments": "这篇论文揭示了当前机器翻译系统在处理性别中立性方面的普遍不足，强调了避免性别错判和代表性伤害的重要性。其创新之处在于系统性评估了大量MT系统，并分类了实践中出现的性别中立策略。这对于推动更具包容性的机器翻译发展具有重要意义，也指出了未来研究需要克服的挑战。"}}
{"id": "2506.15549", "title": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation", "authors": ["Farheen Ramzan", "Yusuf Kiberu", "Nikesh Jathanna", "Shahnaz Jamil-Copley", "Richard H. Clayton", "Chen", "Chen"], "summary": "Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.", "comment": "14 Pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15549v1", "AI": {"title_translation": "CLAIM: 临床指导的LGE增强用于真实多样化的心肌瘢痕合成与分割", "tldr": "提出CLAIM框架，通过临床指导的LGE增强来合成真实多样的心肌瘢痕图像，并同时优化瘢痕分割模型，以解决高质量LGE图像和瘢痕标签稀缺的问题。", "motivation": "高质量LGE图像和瘢痕标签的有限可用性和变异性限制了鲁棒的心肌瘢痕分割深度学习模型的发展。", "method": "本文提出了CLAIM框架，核心是SMILE模块（由临床知识指导的瘢痕掩膜生成），它以临床采用的AHA 17段模型为条件，驱动扩散生成器合成具有解剖学一致性和空间多样性的瘢痕模式图像。此外，CLAIM采用联合训练策略，同时优化瘢痕分割网络和生成器。", "result": "实验结果表明，CLAIM生成了与解剖学一致的瘢痕模式，并且与基线模型相比，在真实瘢痕分布上获得了更高的Dice相似度。", "conclusion": "CLAIM方法能够实现可控且真实的心肌瘢痕合成，并已证明对下游医学图像任务有用。", "translation": "基于深度学习的晚期钆增强（LGE）心脏MRI心肌瘢痕分割在结构性心脏疾病的准确及时诊断和治疗规划方面显示出巨大潜力。然而，高质量瘢痕标签的LGE图像的有限可用性和变异性限制了鲁棒分割模型的发展。为了解决这个问题，我们引入了CLAIM：临床指导的LGE增强用于真实多样化的心肌瘢痕合成与分割框架，这是一个用于解剖学基础瘢痕生成和分割的框架。其核心是SMILE模块（由临床知识指导的瘢痕掩膜生成），它以临床采用的AHA 17段模型为条件，驱动扩散生成器合成具有解剖学一致性和空间多样性的瘢痕模式图像。此外，CLAIM采用联合训练策略，其中瘢痕分割网络与生成器一起优化，旨在增强合成瘢痕的真实性和瘢痕分割性能的准确性。实验结果表明，与基线模型相比，CLAIM生成了与解剖学一致的瘢痕模式，并实现了与真实瘢痕分布更高的Dice相似度。我们的方法能够实现可控且真实的心肌瘢痕合成，并已证明对下游医学图像任务有用。", "summary": "本文提出了一个名为CLAIM的框架，旨在解决深度学习心肌瘢痕分割中高质量LGE图像和标签稀缺的问题。CLAIM通过SMILE模块利用临床知识（AHA 17段模型）指导扩散生成器合成真实且多样化的心肌瘢痕图像，并采用联合训练策略同时优化瘢痕合成和分割网络。实验证明CLAIM能生成解剖学上连贯的瘢痕，并提高了分割准确性，对下游医学图像任务具有实用价值。", "keywords": "心肌瘢痕分割, LGE MRI, 数据增强, 扩散模型, 临床指导", "comments": "CLAIM的创新点在于其利用临床知识（AHA 17段模型）指导合成瘢痕的策略，这使得生成的图像更具解剖学合理性，从而提高了合成数据的实用性。联合训练方法也有效地将合成与分割目标结合起来。该方法对解决医学图像领域数据稀缺问题具有重要意义，有助于开发更鲁棒的深度学习模型。"}}
{"id": "2506.15305", "title": "Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance", "authors": ["Qingkai Zhang", "L. Jeff Hong", "Houmin Yan"], "summary": "The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15305v1", "AI": {"title_translation": "供应链金融中用于增强信用风险管理的条件生成建模", "tldr": "针对跨境电商供应链金融中中小企业融资难问题，本文提出一种基于条件生成建模（QRGMM与DeepFM结合）的信用风险管理框架，有效评估风险并确定贷款规模，是生成式AI在该领域的首次应用。", "motivation": "跨境电商迅速发展为中小企业带来机遇，但其信用历史有限导致融资困难。第三方物流主导的供应链金融利用在途库存作为抵押，成为有前景的解决方案，但仍需解决信用风险评估和贷款规模确定的双重挑战。", "method": "提出一个先进的信用风险管理框架，针对第三方物流主导的供应链金融。具体方法是利用基于分位数回归的生成式元建模（QRGMM）进行销售分布的条件生成建模，作为风险估计的基础。该框架能够灵活估计多种风险度量，并引入功能性风险度量公式，系统性地捕捉风险度量与不同贷款水平之间的关系，并提供理论保证。为捕捉电商销售数据中复杂的协变量交互，将QRGMM与深度因子分解机（DeepFM）集成。", "result": "在合成数据和真实世界数据上的广泛实验验证了模型在信用风险评估和贷款规模确定方面的有效性。", "conclusion": "本研究代表了生成式AI在跨境电商供应链金融风险管理领域的开创性应用，为增强信用实践和改善中小企业获取资本提供了坚实基础。", "translation": "跨境电商的快速扩张为中小企业（SMEs）创造了巨大的机会，然而，由于中小企业有限的信用历史，融资仍然是一个关键挑战。第三方物流（3PL）主导的供应链金融（SCF）已成为一个有前景的解决方案，它利用在途库存作为抵押品。我们提出了一个专门为3PL主导的SCF设计的先进信用风险管理框架，解决了信用风险评估和贷款规模确定的双重挑战。具体而言，我们利用基于分位数回归的生成式元建模（QRGMM）对销售分布进行条件生成建模，作为风险估计的基础。我们提出了一个统一的框架，该框架能够灵活地估计多种风险度量，同时引入了功能性风险度量公式，系统地捕捉这些风险度量与不同贷款水平之间的关系，并得到了理论保证。为了捕捉电商销售数据中复杂的协变量交互，我们将QRGMM与深度因子分解机（DeepFM）相结合。在合成数据和真实世界数据上的大量实验验证了我们模型在信用风险评估和贷款规模确定方面的有效性。本研究代表了生成式AI在跨境电商供应链金融风险管理领域的开创性应用，为增强信用实践和改善中小企业获取资本提供了坚实基础。", "summary": "本文针对跨境电商供应链金融中中小企业融资难和信用历史有限的问题，提出一个先进的信用风险管理框架。该框架利用基于分位数回归的生成式元建模（QRGMM）对销售分布进行条件生成建模，并结合深度因子分解机（DeepFM）处理复杂数据交互，以实现灵活的信用风险评估和贷款规模确定。实验结果验证了模型的有效性，并强调了其作为生成式AI在供应链金融风险管理中开创性应用的意义。", "keywords": "供应链金融, 信用风险管理, 条件生成建模, 分位数回归生成式元建模, 深度因子分解机", "comments": "本文的创新之处在于首次将条件生成建模（QRGMM与DeepFM结合）应用于跨境电商供应链金融的信用风险管理，特别是在解决中小企业融资的信用评估和贷款规模确定问题上。其提出的功能性风险度量公式具有理论支撑，并能系统性地捕捉风险与贷款水平的关系，为实际应用提供了坚实基础。该研究填补了生成式AI在该领域应用的空白，对提升中小企业融资可及性具有重要意义。"}}
{"id": "2506.15681", "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.", "comment": "Project page: https://byungkwanlee.github.io/GenRecal-page/", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15681v1", "AI": {"title_translation": "GenRecal：从大型到小型视觉-语言模型的重新校准后生成", "tldr": "GenRecal是一个新颖的通用蒸馏框架，通过引入一个校准器来对齐和适应异构视觉-语言模型（VLM）之间的特征表示，从而将知识从大型VLM有效迁移到小型VLM，解决了不同VLM架构的兼容性挑战，并在多个基准测试中显著提升了性能。", "motivation": "当前视觉-语言模型（VLM）虽然性能强大，但由于计算需求高，难以部署到资源受限设备。将大型VLM的知识蒸馏到小型模型中面临挑战，特别是VLM架构多样性（不同的LLM、token类型、词汇量、token分割和索引顺序）导致难以实现特定VLM类型的限制。", "method": "本文提出了一个名为GenRecal（Generation after Recalibration）的新型通用蒸馏框架，用于视觉-语言模型。GenRecal包含一个“重新校准器”（Recalibrator），该校准器旨在对齐和适应异构VLM之间的特征表示，从而实现跨不同类型VLM的有效知识迁移。", "result": "通过在多个具有挑战性的基准测试中进行大量实验，GenRecal显著提升了基线性能，并最终超越了大型开源和闭源视觉-语言模型。", "conclusion": "GenRecal是一个新颖且通用的蒸馏框架，通过其重新校准器有效解决了异构视觉-语言模型之间的知识迁移挑战，使得小型模型能够获得与大型模型相当甚至超越的性能，为资源受限设备上的VLM部署提供了解决方案。", "translation": "最近视觉-语言模型（VLM）的进步利用大型语言模型（LLM）实现了与GPT-4V等闭源系统相当的性能。然而，由于其巨大的计算需求，在实际场景中部署这些模型，特别是在资源受限的设备上，仍然具有挑战性。这激发了人们对将知识从大型VLM蒸馏到更小、更高效的对应模型中的兴趣。这里出现的一个关键挑战来自于VLM架构的多样性，它们基于不同的LLM并采用不同的token类型——在词汇量、token分割和token索引顺序上有所不同。为了解决这种对特定VLM类型的限制挑战，我们提出了重新校准后生成（GenRecal），一个新颖的通用VLM蒸馏框架。GenRecal包含一个重新校准器，它对齐并适应异构VLM之间的特征表示，从而实现跨不同类型VLM的有效知识迁移。通过在多个具有挑战性的基准测试中进行大量实验，我们证明了GenRecal显著提高了基线性能，最终超越了大型开源和闭源VLM。", "summary": "本文提出了GenRecal，一个新颖的通用视觉-语言模型（VLM）蒸馏框架。针对大型VLM部署困难及VLM架构多样性导致的知识迁移挑战，GenRecal引入了一个“重新校准器”，能够对齐并适应异构VLM间的特征表示，从而实现从大型VLM到小型VLM的有效知识迁移。实验结果表明，GenRecal显著提升了性能，并超越了现有的开源和闭源大型VLM。", "keywords": "视觉-语言模型蒸馏, 知识迁移, 异构模型对齐, 模型压缩, GenRecal", "comments": "GenRecal的创新之处在于提出了一个通用的蒸馏框架，特别是其“重新校准器”解决了异构VLM之间特征对齐的难题，这对于VLM在多样化硬件环境中的部署具有重要意义。该方法通过知识蒸馏，有效降低了VLM的计算成本，同时保持甚至超越了大型模型的性能，极大地推动了VLM在资源受限设备上的应用潜力。"}}
{"id": "2506.14861", "title": "BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic Foundation Models", "authors": ["Bharath Dandala", "Michael M. Danziger", "Ella Barkan", "Tanwi Biswas", "Viatcheslav Gurev", "Jianying Hu", "Matthew Madgwick", "Akira Koseki", "Tal Kozlovski", "Michal Rosen-Zvi", "Yishai Shimoni", "Ching-Huei Tsou"], "summary": "Transcriptomic foundation models (TFMs) have recently emerged as powerful\ntools for analyzing gene expression in cells and tissues, supporting key tasks\nsuch as cell-type annotation, batch correction, and perturbation prediction.\nHowever, the diversity of model implementations and training strategies across\nrecent TFMs, though promising, makes it challenging to isolate the contribution\nof individual design choices or evaluate their potential synergies. This\nhinders the field's ability to converge on best practices and limits the\nreproducibility of insights across studies. We present BMFM-RNA, an\nopen-source, modular software package that unifies diverse TFM pretraining and\nfine-tuning objectives within a single framework. Leveraging this capability,\nwe introduce a novel training objective, whole cell expression decoder (WCED),\nwhich captures global expression patterns using an autoencoder-like CLS\nbottleneck representation. In this paper, we describe the framework, supported\ninput representations, and training objectives. We evaluated four model\ncheckpoints pretrained on CELLxGENE using combinations of masked language\nmodeling (MLM), WCED and multitask learning. Using the benchmarking\ncapabilities of BMFM-RNA, we show that WCED-based models achieve performance\nthat matches or exceeds state-of-the-art approaches like scGPT across more than\na dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available\nas part of the biomed-multi-omics project (\nhttps://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible\nfoundation for systematic benchmarking and community-driven exploration of\noptimal TFM training strategies, enabling the development of more effective\ntools to leverage the latest advances in AI for understanding cell biology.", "comment": null, "cate": "q-bio.GN", "url": "http://arxiv.org/abs/2506.14861v1", "AI": {"title_translation": "BMFM-RNA：一个用于构建和评估转录组基础模型的开放框架", "tldr": "BMFM-RNA是一个开放的、模块化的框架，用于统一和评估转录组基础模型（TFMs）的预训练和微调目标，并引入了新的WCED训练目标，其性能媲美或超越现有最先进方法。", "motivation": "转录组基础模型（TFMs）虽然强大且多样，但其实现和训练策略的差异性使得难以评估个体设计选择的贡献或其潜在的协同作用，这阻碍了领域内最佳实践的形成和研究结果的可重复性。", "method": "本文提出了BMFM-RNA，一个开源、模块化的软件包，它在一个统一的框架内整合了多种TFM预训练和微调目标。在此基础上，引入了一种新颖的训练目标——全细胞表达解码器（WCED），它使用类似自编码器的CLS瓶颈表示来捕获全局表达模式。作者描述了该框架、支持的输入表示和训练目标，并评估了在CELLxGENE上预训练的四种模型检查点，这些模型结合了掩码语言建模（MLM）、WCED和多任务学习。", "result": "通过BMFM-RNA的基准测试能力，研究表明基于WCED的模型在十多个数据集的零样本和微调任务中，其性能与scGPT等最先进方法相当或更优。", "conclusion": "BMFM-RNA为系统性基准测试和社区驱动的转录组基础模型优化训练策略探索提供了一个可复现的基础，从而能够开发更有效的工具，利用人工智能的最新进展来理解细胞生物学。", "translation": "转录组基础模型（TFMs）最近已成为分析细胞和组织中基因表达的强大工具，支持细胞类型注释、批次校正和扰动预测等关键任务。然而，近期TFMs在模型实现和训练策略上的多样性，尽管前景广阔，但使得难以分离个体设计选择的贡献或评估它们的潜在协同作用。这阻碍了该领域在最佳实践上达成共识，并限制了研究结果在不同研究间的可重复性。我们提出了BMFM-RNA，一个开源、模块化的软件包，它在一个单一框架内统一了各种TFM预训练和微调目标。利用这一能力，我们引入了一种新颖的训练目标，即全细胞表达解码器（WCED），它使用类似自编码器的CLS瓶颈表示来捕获全局表达模式。在本文中，我们描述了该框架、支持的输入表示和训练目标。我们评估了在CELLxGENE上预训练的四个模型检查点，这些检查点结合了掩码语言建模（MLM）、WCED和多任务学习。利用BMFM-RNA的基准测试能力，我们展示了基于WCED的模型在十多个数据集的零样本和微调任务中，其性能与scGPT等最先进方法相当或更优。BMFM-RNA作为biomed-multi-omics项目的一部分（https://github.com/BiomedSciAI/biomed-multi-omic），为系统性基准测试和社区驱动的优化TFM训练策略探索提供了可复现的基础，从而能够开发更有效的工具，利用人工智能的最新进展来理解细胞生物学。", "summary": "BMFM-RNA是一个旨在解决转录组基础模型（TFMs）多样性导致的可重复性和评估挑战的开放框架。它统一了TFMs的预训练和微调目标，并引入了新型的WCED训练目标，该目标能有效捕获全局表达模式。实验结果表明，基于WCED的模型在多项任务上表现优异，达到或超越了现有SOTA方法。BMFM-RNA为TFMs的系统性基准测试和社区协作探索提供了可复现平台，有助于推动细胞生物学领域AI应用的发展。", "keywords": "转录组基础模型, BMFM-RNA, WCED, 基因表达, 基准测试", "comments": "BMFM-RNA的创新之处在于提供了一个开放、模块化且统一的框架，有效解决了转录组基础模型（TFMs）在实现和训练策略上多样性所带来的评估和可重复性挑战。其引入的WCED训练目标，通过捕获全局表达模式，提升了模型的性能。该框架的重要性在于它为TFMs的系统性基准测试和社区协作探索提供了坚实的基础，有望标准化研究流程，加速转录组学领域AI工具的开发和应用。"}}
{"id": "2506.15307", "title": "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language Models Using Forward-Only Passes", "authors": ["Jinglong Luo", "Zhuo Zhang", "Yehong Zhang", "Shiyu Liu", "Ye Dong", "Xun Zhou", "Hui Wang", "Yue Yu", "Zenglin Xu"], "summary": "Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15307v1", "AI": {"title_translation": "SecFwT：使用仅前向传播高效隐私保护微调大型语言模型", "tldr": "SecFwT是一个基于安全多方计算（MPC）的框架，通过引入仅前向调优范式和MPC友好的随机特征注意力，实现了高效、隐私保护的大型语言模型微调。", "motivation": "大型语言模型（LLMs）在隐私敏感领域（如医疗保健和金融）的应用受到训练数据稀缺的限制，因为严格的隐私要求使得数据难以获取。虽然安全多方计算（MPC）可以保护模型参数和用户数据，但其在LLMs上的应用主要限于推理，因为微调，特别是隐私保护的反向传播和优化器操作，引入了显著的计算挑战。本文旨在解决MPC基隐私保护LLM微调中反向和优化器过程的计算开销过大以及基于softmax的注意力机制在MPC设置中效率低下这两个主要障碍。", "method": "本文提出了SecFwT，这是第一个基于MPC的框架，用于高效、隐私保护的LLM微调。SecFwT引入了一种仅前向调优范式，以消除反向传播和优化器计算。此外，它采用MPC友好的随机特征注意力来近似softmax注意力，从而显著减少了昂贵的非线性操作和计算复杂性。", "result": "实验结果表明，SecFwT在效率和隐私保护方面取得了显著提升，从而能够对LLMs进行可扩展且安全的微调，以应用于隐私关键型应用程序。", "conclusion": "SecFwT成功地解决了MPC基隐私保护LLM微调中的计算挑战，通过创新的仅前向调优和MPC友好的注意力机制，实现了在隐私敏感领域对LLMs进行高效、安全和可扩展的微调。", "translation": "大型语言模型（LLMs）已经改变了许多领域，但它们在医疗保健和金融等隐私敏感领域适应专业任务的能力受到严格隐私要求导致可访问训练数据稀缺的限制。基于安全多方计算（MPC）的隐私保护机器学习提供了一种强大的方法来保护模型参数和用户数据，但其在LLMs上的应用主要限于推理，因为微调会带来显著的计算挑战，尤其是在隐私保护的反向传播和优化器操作中。本文指出了基于MPC的隐私保护LLM微调的两个主要障碍：（1）反向和优化器过程的巨大计算开销，以及（2）基于softmax的注意力机制在MPC设置中的低效率。为了解决这些挑战，我们提出了SecFwT，这是第一个基于MPC的框架，旨在实现高效、隐私保护的LLM微调。SecFwT引入了一种仅前向调优范式，以消除反向和优化器计算，并采用MPC友好的随机特征注意力来近似softmax注意力，从而显著减少了昂贵的非线性操作和计算复杂性。实验结果表明，SecFwT在效率和隐私保护方面取得了显著提升，从而能够对LLMs进行可扩展且安全的微调，以应用于隐私关键型应用程序。", "summary": "本文提出SecFwT，一个创新的基于安全多方计算（MPC）的框架，旨在解决大型语言模型（LLMs）在隐私敏感领域微调时面临的计算和隐私挑战。针对传统MPC微调中反向传播和优化器操作的巨大开销以及softmax注意力机制的低效性，SecFwT引入了“仅前向调优”范式以消除反向计算，并采用MPC友好的“随机特征注意力”来替代softmax，从而大幅降低了非线性操作和计算复杂性。实验证明，SecFwT显著提升了效率和隐私保护能力，使得LLMs能够在隐私关键型应用中进行可扩展且安全的微调。", "keywords": "隐私保护, 大型语言模型, 微调, 安全多方计算, 仅前向调优", "comments": "SecFwT的创新性在于其提出了“仅前向调优”范式，这极大地简化了MPC环境下的LLM微调过程，避免了复杂的反向传播。同时，采用随机特征注意力来近似softmax是一个巧妙的设计，它解决了MPC中非线性操作效率低下的问题。这项工作对于推动LLMs在医疗、金融等强隐私保护领域的大规模应用具有重要意义，因为它提供了一个既高效又安全的解决方案。"}}
{"id": "2506.15563", "title": "Control and Realism: Best of Both Worlds in Layout-to-Image without Training", "authors": ["Bonan Li", "Yinhan Hu", "Songhua Liu", "Xinchao Wang"], "summary": "Layout-to-Image generation aims to create complex scenes with precise control\nover the placement and arrangement of subjects. Existing works have\ndemonstrated that pre-trained Text-to-Image diffusion models can achieve this\ngoal without training on any specific data; however, they often face challenges\nwith imprecise localization and unrealistic artifacts. Focusing on these\ndrawbacks, we propose a novel training-free method, WinWinLay. At its core,\nWinWinLay presents two key strategies, Non-local Attention Energy Function and\nAdaptive Update, that collaboratively enhance control precision and realism. On\none hand, we theoretically demonstrate that the commonly used attention energy\nfunction introduces inherent spatial distribution biases, hindering objects\nfrom being uniformly aligned with layout instructions. To overcome this issue,\nnon-local attention prior is explored to redistribute attention scores,\nfacilitating objects to better conform to the specified spatial conditions. On\nthe other hand, we identify that the vanilla backpropagation update rule can\ncause deviations from the pre-trained domain, leading to out-of-distribution\nartifacts. We accordingly introduce a Langevin dynamics-based adaptive update\nscheme as a remedy that promotes in-domain updating while respecting layout\nconstraints. Extensive experiments demonstrate that WinWinLay excels in\ncontrolling element placement and achieving photorealistic visual fidelity,\noutperforming the current state-of-the-art methods.", "comment": "Accepted by ICML2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15563v1", "AI": {"title_translation": "控制与真实感：无需训练的布局到图像生成中的两全其美", "tldr": "WinWinLay是一种无需训练的布局到图像生成方法，通过解决现有方法的定位不精确和不真实伪影问题，提升了控制精度和真实感。", "motivation": "现有的预训练文本到图像扩散模型在布局到图像生成中存在定位不精确和产生不真实伪影的问题，这阻碍了对主体放置和排列的精确控制以及生成图像的真实感。", "method": "本文提出了一种名为WinWinLay的无需训练方法。其核心包含两个关键策略：1. 非局部注意力能量函数：理论上证明常用注意力函数存在空间分布偏差，通过探索非局部注意力先验来重新分配注意力分数，使对象更好地符合空间条件。2. 自适应更新：针对香草反向传播更新规则导致的域外伪影，引入基于Langevin动力学的自适应更新方案，促进域内更新同时尊重布局约束。", "result": "广泛的实验表明，WinWinLay在控制元素放置和实现逼真视觉保真度方面表现出色，性能超越了当前最先进的方法。", "conclusion": "WinWinLay通过其新颖的无需训练策略（非局部注意力能量函数和自适应更新），成功地解决了布局到图像生成中控制精度和图像真实感面临的挑战，实现了两全其美。", "translation": "布局到图像生成旨在创建具有精确控制主体放置和排列的复杂场景。现有工作表明，预训练的文本到图像扩散模型无需在任何特定数据上训练即可实现此目标；然而，它们经常面临定位不精确和不真实伪影的挑战。针对这些缺点，我们提出了一种新颖的无需训练的方法——WinWinLay。WinWinLay的核心是提出了两个关键策略：非局部注意力能量函数和自适应更新，它们协同增强控制精度和真实感。一方面，我们从理论上证明了常用的注意力能量函数引入了固有的空间分布偏差，阻碍了对象与布局指令的均匀对齐。为了克服这个问题，我们探索了非局部注意力先验来重新分配注意力分数，促进对象更好地符合指定的空间条件。另一方面，我们发现香草反向传播更新规则可能导致偏离预训练领域，从而导致域外伪影。因此，我们引入了一种基于朗之万动力学的自适应更新方案作为补救措施，该方案在尊重布局约束的同时促进域内更新。大量的实验表明，WinWinLay在控制元素放置和实现逼真视觉保真度方面表现出色，超越了当前最先进的方法。", "summary": "本文提出了一种名为WinWinLay的无需训练的布局到图像生成方法，旨在解决现有预训练文本到图像扩散模型在控制精度和图像真实感方面的不足。WinWinLay通过引入非局部注意力能量函数来纠正空间分布偏差，并采用基于Langevin动力学的自适应更新方案来避免域外伪影，从而显著提升了对象定位的精确性和生成图像的逼真度。实验证明其性能优于现有SOTA方法。", "keywords": "布局到图像生成, 无需训练, 扩散模型, 控制精度, 图像真实感", "comments": "该论文的创新之处在于提出了一个无需训练的解决方案来解决布局到图像生成中长期存在的控制精度和真实感问题。通过深入分析现有方法的缺陷（注意力偏差和域外更新），并提出理论支持的非局部注意力先验和Langevin动力学自适应更新，WinWinLay有效地平衡了控制性和生成质量，为该领域提供了一个高效且实用的新范式。"}}
{"id": "2506.14862", "title": "Identifiability by common backdoor in summary causal graphs of time series", "authors": ["Clément Yvernes", "Charles K. Assaad", "Emilie Devijver", "Eric Gaussier"], "summary": "The identifiability problem for interventions aims at assessing whether the\ntotal effect of some given interventions can be written with a do-free formula,\nand thus be computed from observational data only. We study this problem,\nconsidering multiple interventions and multiple effects, in the context of time\nseries when only abstractions of the true causal graph in the form of summary\ncausal graphs are available. We focus in this study on identifiability by a\ncommon backdoor set, and establish, for time series with and without\nconsistency throughout time, conditions under which such a set exists. We also\nprovide algorithms of limited complexity to decide whether the problem is\nidentifiable or not.", "comment": null, "cate": "math.ST", "url": "http://arxiv.org/abs/2506.14862v1", "AI": {"title_translation": "时间序列摘要因果图中的通用后门可识别性", "tldr": "研究了在时间序列的摘要因果图中，通过通用后门集识别干预效果的问题，并提供了判断算法。", "motivation": "评估干预的总效应是否可以通过无do公式计算，即仅从观测数据中计算，尤其是在时间序列的摘要因果图中，处理多重干预和多重效应。", "method": "专注于通过通用后门集的可识别性，为具有和不具有时间一致性的时间序列建立了此类集合存在的条件，并提供了复杂度有限的算法来判断问题是否可识别。", "result": "建立了在时间序列中存在通用后门集的条件，并提供了判断可识别性的算法。", "conclusion": "论文成功地为时间序列的摘要因果图中的干预可识别性问题提供了通过通用后门集的解决方案，包括存在条件和判断算法。", "translation": "干预的可识别性问题旨在评估某些给定干预的总效应是否可以用无do公式表示，从而仅能从观测数据中计算。我们研究了这个问题，在时间序列的背景下，当只有真实因果图的抽象形式——摘要因果图可用时，考虑了多重干预和多重效应。本研究侧重于通过通用后门集的可识别性，并为具有和不具有时间一致性的时间序列，建立了此类集合存在的条件。我们还提供了复杂度有限的算法来判断问题是否可识别。", "summary": "本文研究了在时间序列的摘要因果图中，多重干预和多重效应的可识别性问题，特别是通过通用后门集。作者为具有和不具有时间一致性的时间序列，建立了通用后门集存在的条件，并提供了复杂度有限的算法来判断干预效果是否可识别。", "keywords": "因果推断, 可识别性, 通用后门, 时间序列, 摘要因果图", "comments": "这篇论文解决了时间序列因果推断中的一个核心问题——可识别性，尤其是在信息不完全（摘要因果图）的情况下。提出通用后门集的存在条件和算法具有实际意义，有助于从观测数据中进行因果效应估计。"}}
{"id": "2506.15309", "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation", "authors": ["Júlia Vilalta-Mor", "Alexis Molina", "Laura Ortega Varga", "Isaac Filella-Merce", "Victor Guallar"], "summary": "Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.", "comment": "16 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15309v1", "AI": {"title_translation": "主动学习引导的Seq2Seq变分自编码器用于多靶点抑制剂生成", "tldr": "该研究提出了一种将序列到序列变分自编码器（Seq2Seq VAE）集成到迭代循环中的结构化主动学习范式，用于有效地生成多靶点抑制剂，解决了药物发现中同时优化多个治疗靶点的挑战。", "motivation": "在药物发现中，同时针对多个治疗靶点优化分子仍然是一个巨大的挑战，特别是由于稀疏奖励和相互冲突的设计约束。本文旨在解决这一问题。", "method": "本研究提出了一种结构化主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）整合到迭代循环中，旨在平衡化学多样性、分子质量和多靶点亲和力。该方法交替进行扩展潜在空间的化学可行区域和根据日益严格的多靶点对接阈值逐步限制分子。", "result": "在一项针对三种相关冠状病毒主要蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的概念验证研究中，该方法有效地生成了一组结构多样化的泛抑制剂候选物。研究表明，在主动学习流程中，化学过滤的仔细时机和策略性放置显著增强了对有益化学空间的探索。", "conclusion": "该框架将稀疏奖励、多目标药物设计问题转化为可行的计算任务，并为高效探索复杂多药理学景观提供了通用路线图。", "translation": "同时优化分子以针对多个治疗靶点在药物发现中仍然是一个深刻的挑战，特别是由于奖励稀疏和设计约束冲突。我们提出了一种结构化主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）集成到迭代循环中，旨在平衡化学多样性、分子质量和多靶点亲和力。我们的方法在扩展潜在空间的化学可行区域和根据日益严格的多靶点对接阈值逐步限制分子之间交替进行。在一项针对三种相关冠状病毒主要蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的概念验证研究中，我们的方法有效地生成了一组结构多样化的泛抑制剂候选物。我们证明，在此主动学习流程中，化学过滤的仔细时机和策略性放置显著增强了对有益化学空间的探索，将稀疏奖励、多目标药物设计问题转化为可行的计算任务。因此，我们的框架为高效探索复杂多药理学景观提供了通用路线图。", "summary": "本研究提出了一种结合主动学习和序列到序列变分自编码器（Seq2Seq VAE）的新范式，旨在解决药物发现中多靶点分子优化的挑战。该方法通过迭代过程平衡化学多样性、分子质量和多靶点亲和力，并在冠状病毒蛋白酶抑制剂的生成中展示了其有效性，成功生成了多样化的泛抑制剂候选物，证明了其在复杂药物设计问题中的潜力。", "keywords": "主动学习, 变分自编码器, 多靶点抑制剂, 药物发现, 序列到序列模型", "comments": "该论文的创新之处在于将主动学习与Seq2Seq VAE结合，用于解决药物发现中多靶点优化的复杂问题。通过迭代地平衡化学多样性和靶点亲和力，并将稀疏奖励问题转化为可行的计算任务，为高效探索药物化学空间提供了一个通用且有前景的框架。其在生成泛抑制剂方面的概念验证研究也展示了其潜在的应用价值。"}}
{"id": "2506.15564", "title": "Show-o2: Improved Native Unified Multimodal Models", "authors": ["Jinheng Xie", "Zhenheng Yang", "Mike Zheng Shou"], "summary": "This paper presents improved native unified multimodal models, \\emph{i.e.,}\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15564v1", "AI": {"title_translation": "Show-o2：改进的本地统一多模态模型", "tldr": "Show-o2是一种改进的统一多模态模型，结合自回归建模和流匹配，在3D因果变分自编码器空间中实现图像、视频和文本的理解与生成。", "motivation": "旨在开发改进的、原生的统一多模态模型，以实现跨图像、视频和文本模态的有效多模态理解和生成。", "method": "本文提出了Show-o2模型，该模型结合自回归建模和流匹配。它基于3D因果变分自编码器空间，通过空间(-时间)融合的双路径构建统一视觉表示，实现图像和视频模态的扩展性。在语言模型基础上，自回归建模和流匹配分别应用于语言头和流头，用于文本标记预测和图像/视频生成。此外，还设计了两阶段训练方案以有效学习和扩展模型。", "result": "Show-o2模型在处理广泛的多模态理解和生成任务方面表现出多功能性，支持包括文本、图像和视频在内的多种模态。", "conclusion": "Show-o2通过结合自回归建模和流匹配，并在3D因果变分自编码器空间中构建统一视觉表示，成功实现了跨多种模态（文本、图像、视频）的有效多模态理解和生成，并具有良好的可扩展性。", "translation": "本文介绍了改进的本地统一多模态模型，即Show-o2，该模型利用自回归建模和流匹配。基于3D因果变分自编码器空间，通过空间（-时间）融合的双路径构建统一的视觉表示，从而实现图像和视频模态的扩展性，同时确保有效的多模态理解和生成。基于语言模型，自回归建模和流匹配分别原生应用于语言头和流头，以促进文本标记预测和图像/视频生成。设计了两阶段训练方案，以有效地学习并扩展到更大的模型。最终的Show-o2模型在处理包括文本、图像和视频在内的多种模态的广泛多模态理解和生成任务方面表现出多功能性。代码和模型已在https://github.com/showlab/Show-o发布。", "summary": "本文提出了Show-o2，一种改进的本地统一多模态模型，它结合了自回归建模和流匹配。该模型基于3D因果变分自编码器空间构建统一视觉表示，并通过双路径空间融合实现图像和视频的跨模态扩展。Show-o2利用语言模型，将自回归建模和流匹配应用于不同的头部以实现文本预测和图像/视频生成。通过两阶段训练，Show-o2在广泛的多模态理解和生成任务中展现出强大的通用性，涵盖文本、图像和视频。", "keywords": "多模态模型, 自回归建模, 流匹配, 统一表示, 图像视频生成", "comments": "Show-o2的创新之处在于其统一的多模态处理方法，通过将自回归建模和流匹配结合在一个3D因果变分自编码器空间中，实现了对图像、视频和文本的理解与生成。双路径空间融合和两阶段训练方案也增强了其可扩展性和性能。其对多种模态的通用性使其在多模态AI领域具有重要意义。"}}
{"id": "2506.15329", "title": "When and How Unlabeled Data Provably Improve In-Context Learning", "authors": ["Yingcong Li", "Xiangyu Chang", "Muti Kara", "Xiaofeng Liu", "Amit Roy-Chowdhury", "Samet Oymak"], "summary": "Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15329v1", "AI": {"title_translation": "无标签数据何时以及如何能被证明地改进上下文学习", "tldr": "研究表明，多层或循环Transformer能有效利用无标签数据改进上下文学习，而单层线性注意力模型则不能。", "motivation": "为了阐明上下文学习（ICL）即使在演示数据标签缺失或不正确时也能有效的能力，特别是其利用无标签数据的机制。", "method": "研究在一个规范设置下进行，其中演示数据遵循二元高斯混合模型（GMM），且部分数据标签缺失。通过对单层线性注意力模型和多层/循环Transformer进行全面的理论研究，分析它们如何利用无标签数据，并将其与期望最大化（EM）算法建立联系。", "result": "1) 单层线性注意力模型的损失平面能恢复最优的全监督估计器，但完全无法利用无标签数据；2) 多层或循环Transformer能通过隐式构建特定形式的估计器（$\\\\sum_{i\\\\ge 0} a_i (X^\\\\top X)^iX^\\\\top y$）有效利用无标签数据；3) 可表达为深度函数的类多项式，其领先的多项式幂随深度呈指数增长，因此适度的深度或循环次数就足够；4) 将理论应用于表格基础模型，通过循环显著提高了半监督表格学习的性能，优于标准的单次推理。", "conclusion": "多层或循环Transformer能够有效利用无标签数据来增强上下文学习的能力，这与其隐式构建的估计器形式以及与期望最大化等半监督学习算法的联系有关。在实际应用中，这种方法能显著提升半监督表格学习的性能。", "translation": "最近的研究表明，即使演示数据存在缺失或不正确的标签，上下文学习（ICL）也能有效。为了阐明这种能力，我们研究了一个典型设置，其中演示数据根据二元高斯混合模型（GMM）抽取，并且一定比例的演示数据标签缺失。我们提供了一项全面的理论研究，表明：(1) 单层线性注意力模型的损失平面能够恢复最优的全监督估计器，但完全未能利用无标签数据；(2) 相比之下，多层或循环Transformer可以通过隐式构建形式为 $\\\\sum_{i\\\\ge 0} a_i (X^\\\\top X)^iX^\\\\top y$ 的估计器来有效利用无标签数据，其中 $X$ 和 $y$ 分别表示特征和部分观测到的标签（缺失条目设置为零）。我们刻画了可以表示为深度函数的类多项式，并将其与半监督学习中常用的迭代伪标签算法——期望最大化（Expectation Maximization）建立了联系。重要的是，领先的多项式幂随深度呈指数增长，因此适度的深度/循环次数就足够了。作为理论的应用，我们提出对现成的表格基础模型进行循环，以增强其半监督能力。对真实世界数据集的广泛评估表明，我们的方法显著提高了半监督表格学习的性能，优于标准的单次推理。", "summary": "这项研究理论分析了上下文学习（ICL）中无标签数据的利用问题。研究发现，单层线性注意力模型无法利用无标签数据，而多层或循环Transformer可以通过隐式构建特定形式的估计器来有效利用它们，并与期望最大化算法相关联。理论表明，适度的深度或循环次数即可实现显著效果。实践中，将此方法应用于表格基础模型，显著提升了半监督学习性能。", "keywords": "上下文学习, 无标签数据, Transformer, 半监督学习, 期望最大化", "comments": "这篇论文通过深入的理论分析，解释了多层或循环Transformer在上下文学习中利用无标签数据的机制，填补了现有研究的空白。它不仅提供了数学上的证明，还将其与经典的半监督学习算法（EM）联系起来，并展示了在实际表格数据上的有效应用，具有重要的理论和实践意义。"}}
{"id": "2506.15565", "title": "Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification", "authors": ["Junhao Wu", "Aboagye-Ntow Stephen", "Chuyuan Wang", "Gang Chen", "Xin Huang"], "summary": "Ultra-high Spatial Resolution Land Cover Classification is essential for\nfine-grained land cover analysis, yet it remains challenging due to the high\ncost of pixel-level annotations, significant scale variation, and the limited\nadaptability of large-scale vision models. Existing methods typically focus on\n1-meter spatial resolution imagery and rely heavily on annotated data, whereas\npractical applications often require processing higher-resolution imagery under\nweak supervision. To address this, we propose a parameter-efficient\nsemi-supervised segmentation framework for 0.3 m spatial resolution imagery,\nwhich leverages the knowledge of SAM2 and introduces a remote sensing-specific\nFreqWeaver Adapter to enhance fine-grained detail modeling while maintaining a\nlightweight design at only 5.96% of the total model parameters. By effectively\nleveraging unlabeled data and maintaining minimal parameter overhead, the\nproposed method delivers robust segmentation results with superior structural\nconsistency, achieving a 1.78% improvement over existing parameter-efficient\ntuning strategies and a 3.44% gain compared to state-of-the-art high-resolution\nremote sensing segmentation approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15565v1", "AI": {"title_translation": "巴尔的摩地图集：用于半监督超高空间分辨率土地覆盖分类的 FreqWeaver 适配器", "tldr": "提出一种参数高效的半监督框架 Baltimore Atlas，通过 FreqWeaver 适配器在 0.3 米超高分辨率图像上实现优越的土地覆盖分类，解决了标注成本高和弱监督下的挑战。", "motivation": "超高空间分辨率土地覆盖分类面临像素级标注成本高、尺度变化大以及大型视觉模型适应性有限的挑战。现有方法主要关注 1 米分辨率并高度依赖标注数据，而实际应用需要处理更高分辨率（0.3 米）且在弱监督条件下进行。", "method": "提出一个参数高效的半监督分割框架，专门用于 0.3 米空间分辨率图像。该框架利用 SAM2 的知识，并引入了一个针对遥感领域的 FreqWeaver 适配器，以增强细粒度细节建模，同时保持轻量化设计（仅占总模型参数的 5.96%）。", "result": "该方法有效利用了未标注数据，保持了最小的参数开销，提供了鲁棒的分割结果，并具有卓越的结构一致性。与现有参数高效微调策略相比，性能提升了 1.78%；与最先进的高分辨率遥感分割方法相比，性能提升了 3.44%。", "conclusion": "该研究成功开发了一个参数高效的半监督框架，显著提升了在超高空间分辨率图像（0.3 米）上的土地覆盖分类性能，有效解决了弱监督条件下的标注成本和模型适应性问题。", "translation": "超高空间分辨率土地覆盖分类对于细粒度土地覆盖分析至关重要，但由于像素级标注成本高、显著的尺度变化以及大型视觉模型适应性有限，它仍然充满挑战。现有方法通常专注于 1 米空间分辨率图像，并严重依赖标注数据，而实际应用通常需要在弱监督下处理更高分辨率的图像。为了解决这个问题，我们提出了一个针对 0.3 米空间分辨率图像的参数高效半监督分割框架，该框架利用了 SAM2 的知识，并引入了一个遥感专用的 FreqWeaver 适配器，以在保持轻量化设计（仅占总模型参数的 5.96%）的同时增强细粒度细节建模。通过有效利用未标注数据并保持最小的参数开销，所提出的方法提供了鲁棒的分割结果，具有卓越的结构一致性，比现有参数高效微调策略提高了 1.78%，比最先进的高分辨率遥感分割方法提高了 3.44%。", "summary": "本文提出了 Baltimore Atlas，一个针对 0.3 米超高空间分辨率图像的参数高效半监督土地覆盖分类框架。该方法通过整合 SAM2 的知识和一个轻量级的 FreqWeaver 遥感适配器，有效解决了高昂的标注成本和弱监督下的挑战。实验结果表明，该框架在结构一致性和分割性能上均优于现有方法，实现了显著的性能提升。", "keywords": "土地覆盖分类, 半监督学习, 超高空间分辨率, FreqWeaver Adapter, 遥感", "comments": "这篇论文的创新点在于提出了一个参数高效的半监督框架，专门针对超高空间分辨率遥感图像的土地覆盖分类。通过引入 FreqWeaver 适配器并利用 SAM2 知识，它有效地解决了像素级标注成本高和弱监督下模型适应性差的问题。其轻量化设计（仅5.96%参数）和在0.3米分辨率上的显著性能提升，使其在实际应用中具有重要价值，特别是在数据标注受限的场景下。"}}
{"id": "2506.15330", "title": "Universal Laboratory Model: prognosis of abnormal clinical outcomes based on routine tests", "authors": ["Pavel Karpov", "Ilya Petrenkov", "Ruslan Raiman"], "summary": "Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.", "comment": "7 pages, 2 figues", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15330v1", "AI": {"title_translation": "通用实验室模型：基于常规检测的异常临床结果预测", "tldr": "本文提出一种新的表格建模方法，将其视为集合翻译问题，用于根据常规实验室检测结果预测异常临床结果，并在联合预测中显示出AUC的提升。", "motivation": "为了使早期诊断普及化，通过根据现有常规实验室检测结果（特别是广泛使用的全血细胞计数和常规生化指标）预测未开具处方检测的异常值。这些数据通常表现为带有缺失值的表格。", "method": "将表格建模问题表述为一个集合翻译问题。源集包含类似GPT的标签列嵌入及其对应值对，目标集仅包含相同类型的嵌入。该方法无需隐式估计即可有效处理缺失值，并连接了大型语言模型（LLM）与表格领域。", "result": "将此方法应用于临床实验室数据，在高尿酸、葡萄糖、胆固醇和低铁蛋白水平的联合预测中实现了高达8%的AUC提升。", "conclusion": "所提出的集合翻译方法能够有效预测常规实验室检测中的异常临床结果，处理缺失数据，并对各种生物标志物表现出有前景的预测性能。", "translation": "临床实验室结果在任何诊断中都无处不在。根据已进行的检测结果预测未开具处方检测的异常值看起来很有趣，因为这将使早期诊断人人可及。全血细胞计数（CBC）检测占据了特殊地位，因为它是使用最广泛的临床程序。将常规生化指标与CBC结合起来，呈现出一组因患者而异的检测值对，或者在常见设置中，一个带有缺失值的表格。在这里，我们将表格建模问题表述为一个集合翻译问题，其中源集包含类似GPT的标签列嵌入及其对应值对，而目标集仅包含相同类型的嵌入。所提出的方法可以有效地处理缺失值而无需隐式估计它们，并连接了大型语言模型（LLM）与表格领域。将此方法应用于临床实验室数据，我们在高尿酸、葡萄糖、胆固醇和低铁蛋白水平的联合预测中实现了高达8%的AUC提升。", "summary": "本文介绍了一种“通用实验室模型”，它将表格临床数据预测重新定义为集合翻译问题，以根据包括全血细胞计数（CBC）和生化指标在内的常规实验室检测来预测异常临床结果。该方法通过使用类似GPT的标签嵌入，无需估算即可有效处理缺失值。应用于临床数据时，它在几种关键生物标志物的联合预测中显示出高达8%的AUC提升，从而将大型语言模型（LLM）的能力与表格数据分析相结合，以实现早期诊断。", "keywords": "临床结果预测, 常规检测, 集合翻译, 缺失值处理, AUC提升", "comments": "该论文的创新之处在于将表格数据预测重新构建为集合翻译问题，并整合了类似GPT的嵌入，这是一种处理缺失数据和利用LLM概念分析结构化临床数据的新颖方式。这种方法对于早期诊断和个性化医疗具有重要意义，尤其是在实验室结果不完整的情况下。"}}
{"id": "2506.15577", "title": "A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and Non-Destructive Biomass Estimation from Point Clouds", "authors": ["Di Wang", "Shi Li"], "summary": "Estimating forest above-ground biomass (AGB) is crucial for assessing carbon\nstorage and supporting sustainable forest management. Quantitative Structural\nModel (QSM) offers a non-destructive approach to AGB estimation through 3D tree\nstructural reconstruction. However, current QSM methods face significant\nlimitations, as they are primarily designed for individual trees,depend on\nhigh-quality point cloud data from terrestrial laser scanning (TLS), and also\nrequire multiple pre-processing steps that hinder scalability and practical\ndeployment. This study presents a novel unified framework that enables\nend-to-end processing of large-scale point clouds using an innovative\ngraph-based pipeline. The proposed approach seamlessly integrates tree\nsegmentation,leaf-wood separation and 3D skeletal reconstruction through\ndedicated graph operations including pathing and abstracting for tree topology\nreasoning. Comprehensive validation was conducted on datasets with varying leaf\nconditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and\ndata sources (TLS and UAV-based laser scanning, ULS). Experimental results\ndemonstrate strong performance under challenging conditions, particularly in\nleaf-on scenarios (~20% relative error) and low-density ULS datasets with\npartial coverage (~30% relative error). These findings indicate that the\nproposed framework provides a robust and scalable solution for large-scale,\nnon-destructive AGB estimation. It significantly reduces dependency on\nspecialized pre-processing tools and establishes ULS as a viable alternative to\nTLS. To our knowledge, this is the first method capable of enabling seamless,\nend-to-end 3D tree reconstruction at operational scales. This advancement\nsubstantially improves the feasibility of QSM-based AGB estimation, paving the\nway for broader applications in forest inventory and climate change research.", "comment": "17 pages,19 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15577v1", "AI": {"title_translation": "一种基于图的统一框架，用于可扩展的三维树木重建和点云非破坏性生物量估算", "tldr": "该研究提出了一种新的统一的、基于图的框架，用于从点云中进行大规模、端到端的3D树木重建和非破坏性生物量估算，解决了现有方法的局限性。", "motivation": "估算森林地上生物量（AGB）对于评估碳储量和支持可持续森林管理至关重要。现有的定量结构模型（QSM）方法主要针对单棵树，依赖高质量的地面激光扫描（TLS）点云数据，并需要多个预处理步骤，这些限制阻碍了其可扩展性和实际部署。", "method": "本研究提出了一种新颖的统一框架，通过创新的基于图的管道实现大规模点云的端到端处理。该方法通过专用的图操作（包括路径规划和抽象化以进行树拓扑推理）无缝集成了树木分割、叶木分离和3D骨架重建。", "result": "在不同叶片条件（有叶和无叶）、空间尺度（树和样地级别）以及数据源（TLS和无人机激光扫描，ULS）的数据集上进行了全面验证。实验结果表明，在挑战性条件下表现出色，特别是在有叶场景中相对误差约为20%，在低密度、部分覆盖的ULS数据集中相对误差约为30%。", "conclusion": "所提出的框架为大规模、非破坏性AGB估算提供了一个鲁棒且可扩展的解决方案。它显著减少了对专业预处理工具的依赖，并确立了ULS作为TLS的可行替代方案。这是首个能够实现操作规模无缝、端到端3D树木重建的方法，大大提高了基于QSM的AGB估算的S可行性，为森林清查和气候变化研究的更广泛应用铺平了道路。", "translation": "估算森林地上生物量（AGB）对于评估碳储量和支持可持续森林管理至关重要。定量结构模型（QSM）通过三维树木结构重建提供了一种非破坏性的AGB估算方法。然而，当前的QSM方法面临显著局限性，因为它们主要针对单棵树，依赖于地面激光扫描（TLS）产生的高质量点云数据，并且还需要多个预处理步骤，这些都阻碍了其可扩展性和实际部署。本研究提出了一种新颖的统一框架，通过创新的基于图的管道实现大规模点云的端到端处理。所提出的方法通过专用的图操作（包括路径规划和抽象化以进行树拓扑推理）无缝集成了树木分割、叶木分离和三维骨架重建。在不同叶片条件（有叶和无叶）、空间尺度（树和样地级别）以及数据源（TLS和无人机激光扫描，ULS）的数据集上进行了全面验证。实验结果表明，在挑战性条件下表现出色，特别是在有叶场景中（相对误差约20%）和低密度、部分覆盖的ULS数据集中（相对误差约30%）。这些发现表明，所提出的框架为大规模、非破坏性AGB估算提供了一个鲁棒且可扩展的解决方案。它显著减少了对专业预处理工具的依赖，并确立了ULS作为TLS的可行替代方案。据我们所知，这是第一个能够实现操作规模无缝、端到端三维树木重建的方法。这一进步大大提高了基于QSM的AGB估算的S可行性，为森林清查和气候变化研究的更广泛应用铺平了道路。", "summary": "本研究提出了一个统一的、基于图的框架，旨在解决现有定量结构模型（QSM）在三维树木重建和地上生物量（AGB）估算中面临的可扩展性和预处理依赖问题。该框架通过创新的图操作，实现了从大规模点云数据中进行端到端的树木分割、叶木分离和三维骨架重建。实验结果表明，该方法在有叶和低密度无人机激光扫描（ULS）数据等挑战性条件下表现良好，验证了其鲁棒性和可扩展性。这项工作显著降低了对预处理工具的依赖，并确立了ULS作为地面激光扫描（TLS）的有效替代方案，为大规模森林清查和气候变化研究提供了可行方案。", "keywords": "3D树木重建, 生物量估算, 点云, 图框架, 无人机激光扫描", "comments": "该论文的创新之处在于提出了一个统一的、基于图的框架，实现了从点云到3D树木重建和生物量估算的端到端处理，显著提高了可扩展性。其重要性在于克服了现有QSM方法对高质量TLS数据和复杂预处理的依赖，特别是通过集成ULS数据，使其在实际应用中更具可行性。这是首次实现操作规模无缝3D树木重建的方法，对于森林管理和气候变化研究具有重要意义。"}}
{"id": "2506.15337", "title": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations", "authors": ["Naoki Matsumura", "Yuta Yoshimoto", "Yuto Iwasaki", "Meguru Yamazaki", "Yasufumi Sakai"], "summary": "Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15337v1", "AI": {"title_translation": "用于加速高精度神经网络分子动力学模拟的知识蒸馏框架", "tldr": "本文提出了一种新的知识蒸馏框架，通过使用未经微调的预训练神经网络作为教师模型，解决了传统知识蒸馏方法在生成高能结构训练数据方面的困难，从而显著减少了高精度密度泛函理论计算量，同时保持或提高了分子动力学模拟的准确性。", "motivation": "传统的知识蒸馏方法在针对特定材料模型时，通过微调预训练的神经网络作为教师模型会增加能量势垒，导致难以生成包含高能结构的训练数据，而高能结构对于准确稳定的分子动力学模拟至关重要。", "method": "我们提出了一种新的知识蒸馏（KD）框架。该框架利用未经微调的现成预训练神经网络作为教师模型，其更平缓的能量景观有助于探索更广泛的结构，包括高能结构。该框架采用两阶段训练过程：首先，学生神经网络使用由现成教师模型生成的数据集进行训练；然后，使用较小的高精度密度泛函理论（DFT）数据集对其进行微调。", "result": "我们的框架在有机（聚乙二醇）和无机（L$_{10}$GeP$_{2}$S$_{12}$）材料上都展现出有效性，在重现物理性质方面达到了与现有方法相当或更优的精度。重要的是，与现有神经网络生成方法相比，我们的方法将昂贵的DFT计算量减少了10倍，同时没有牺牲精度。", "conclusion": "所提出的知识蒸馏框架能够有效加速高精度神经网络分子动力学模拟，显著减少了对昂贵DFT计算的需求，同时保持或提高了预测精度，为材料科学研究提供了高效的工具。", "translation": "神经网络势（NNPs）为分子动力学（MD）模拟提供了传统力场的强大替代方案。准确和稳定的MD模拟对于评估材料性质至关重要，这需要包含低能稳定结构和高能结构的训练数据。传统的知识蒸馏（KD）方法将预训练的NNP微调为教师模型，以生成学生模型的训练数据。然而，在特定材料模型中，这种微调过程会增加能量势垒，使得难以创建包含高能结构的训练数据。为了解决这个问题，我们提出了一种新颖的KD框架，该框架利用未经微调的现成预训练NNP作为教师。其更平缓的能量景观有助于探索更广泛的结构，包括对稳定MD模拟至关重要的高能结构。我们的框架采用两阶段训练过程：首先，学生NNP使用由现成教师生成的数据集进行训练；然后，使用较小的高精度密度泛函理论（DFT）数据集对其进行微调。我们通过将其应用于有机（聚乙二醇）和无机（L$_{10}$GeP$_{2}$S$_{12}$）材料来证明我们框架的有效性，与现有方法相比，在重现物理性质方面达到了相当或更优的精度。重要的是，与现有NNP生成方法相比，我们的方法将昂贵的DFT计算量减少了10倍，同时没有牺牲精度。", "summary": "本文提出了一种新颖的知识蒸馏（KD）框架，旨在加速高精度神经网络分子动力学模拟。针对传统KD方法在生成包含高能结构的训练数据时遇到的能量势垒问题，该框架利用未经微调的现成预训练神经网络作为教师模型，其平缓的能量景观有利于探索更广泛的结构。通过两阶段训练过程——首先由教师生成数据训练学生网络，然后用少量高精度DFT数据微调——该方法在有机和无机材料上均实现了与现有方法相当或更优的精度，并且将昂贵的DFT计算量减少了10倍，显著提高了效率。", "keywords": "知识蒸馏, 神经网络势, 分子动力学模拟, 密度泛函理论, 高能结构", "comments": "这项工作的主要创新在于提出了使用“未经微调的现成预训练NNP”作为知识蒸馏的教师模型，巧妙地解决了传统KD方法在生成高能结构训练数据时的难题。这种方法不仅提高了模拟的稳定性和准确性，更重要的是，它极大地降低了对昂贵DFT计算的依赖，加速了材料科学领域中基于神经网络的分子动力学模拟的效率，具有重要的实际应用价值。"}}
{"id": "2506.15591", "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution", "authors": ["Yujing Sun", "Lingchen Sun", "Shuaizheng Liu", "Rongyuan Wu", "Zhengqiang Zhang", "Lei Zhang"], "summary": "It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15591v1", "AI": {"title_translation": "一步扩散实现细节丰富和时间一致的视频超分辨率", "tldr": "DLoRAL是一种新颖的一步扩散方法，采用双LoRA学习范式，通过分别学习时间一致性和细节增强，并将其合并以实现高效推理，从而实现细节丰富且时间一致的视频超分辨率。", "motivation": "在真实世界视频超分辨率（Real-VSR）中，重现丰富的空间细节同时保持时间一致性是一个具有挑战性的问题，尤其是在利用预训练生成模型（如稳定扩散（SD））合成逼真细节时。现有的基于SD的Real-VSR方法通常会为了时间连贯性而牺牲空间细节，导致视觉质量不佳。", "method": "本文提出了一种双LoRA学习（DLoRAL）范式来训练一个有效的一步扩散模型，以同时实现逼真的帧细节和时间一致性。具体来说，引入了一个跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一个一致性LoRA（C-LoRA）来从降质输入中学习鲁棒的时间表示。在一致性学习之后，固定CFR和C-LoRA模块，并训练一个细节LoRA（D-LoRA）以增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。这两个阶段交替迭代优化，协同提供一致且细节丰富的输出。在推理过程中，两个LoRA分支被合并到SD模型中，从而实现单次扩散步骤中高效高质量的视频恢复。", "result": "实验表明，DLoRAL在准确性和速度方面都取得了强大的性能。", "conclusion": "本文提出的DLoRAL方法成功解决了在真实世界视频超分辨率中同时实现细节丰富和时间一致性的挑战。", "translation": "在真实世界视频超分辨率（Real-VSR）中，重现丰富的空间细节同时保持时间一致性是一个具有挑战性的问题，尤其是在我们利用预训练生成模型（如稳定扩散（SD））合成逼真细节时。现有的基于SD的Real-VSR方法通常会为了时间连贯性而牺牲空间细节，导致视觉质量不佳。我们认为，关键在于如何有效地从低质量（LQ）输入视频中提取对降质鲁棒的时间一致性先验，并在保持提取的一致性先验的同时增强视频细节。为了实现这一点，我们提出了一种双LoRA学习（DLoRAL）范式来训练一个有效的一步扩散模型，以同时实现逼真的帧细节和时间一致性。具体来说，我们引入了一个跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一个一致性LoRA（C-LoRA）来从降质输入中学习鲁棒的时间表示。在一致性学习之后，我们固定CFR和C-LoRA模块，并训练一个细节LoRA（D-LoRA）以增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。这两个阶段交替迭代优化，协同提供一致且细节丰富的输出。在推理过程中，两个LoRA分支被合并到SD模型中，从而实现单次扩散步骤中高效高质量的视频恢复。实验表明，DLoRAL在准确性和速度方面都取得了强大的性能。代码和模型可在https://github.com/yjsunnn/DLoRAL获取。", "summary": "本文介绍了DLoRAL，一种利用双LoRA学习范式的新型一步扩散模型，用于真实世界视频超分辨率。为解决空间细节与时间一致性之间的平衡挑战，DLoRAL采用了跨帧检索模块和两个独立的LoRA分支：用于鲁棒时间先验的一致性LoRA（C-LoRA）和用于空间细节增强的细节LoRA（D-LoRA），两者交替迭代训练。合并后的LoRA分支使得视频能够在单次扩散步骤中高效且高质量地恢复，实验证明其在准确性和速度上均表现出色。", "keywords": "视频超分辨率, 扩散模型, LoRA, 时间一致性, 空间细节", "comments": "该论文的创新之处在于其双LoRA学习范式，它有效地解耦了时间一致性和空间细节的学习，然后在一个一步扩散框架内进行整合。这种方法，特别是C-LoRA和D-LoRA的迭代训练及其在推理时的合并，为真实世界视频超分辨率领域长期存在的挑战提供了一个优雅的解决方案，实现了高质量和高效率的双重优势。"}}
{"id": "2506.14923", "title": "Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning", "authors": ["Jaehong Chung", "Michael Manga", "Timothy Kneafsey", "Tapan Mukerji", "Mengsu Hu"], "summary": "Microearthquakes (MEQs) generated by subsurface fluid injection record the\nevolving stress state and permeability of reservoirs. Forecasting their full\nspatiotemporal evolution is therefore critical for applications such as\nenhanced geothermal systems (EGS), CO$_2$ sequestration and other\ngeo-engineering applications. We present a transformer-based deep learning\nmodel that ingests hydraulic stimulation history and prior MEQ observations to\nforecast four key quantities: cumulative MEQ count, cumulative logarithmic\nseismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of\nthe MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model\nachieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the\n15-second forecast horizon across all targets, and supplies uncertainty\nestimates through a learned standard deviation term. These accurate,\nuncertainty-quantified forecasts enable real-time inference of fracture\npropagation and permeability evolution, demonstrating the strong potential of\ndeep-learning approaches to improve seismic-risk assessment and guide\nmitigation strategies in future fluid-injection operations.", "comment": null, "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.14923v1", "AI": {"title_translation": "利用深度学习预测流体诱发微地震的时空演化", "tldr": "该研究提出一个基于Transformer的深度学习模型，用于预测流体诱发微地震的时空演化，在EGS Collab数据集上实现了高精度预测并提供不确定性估计，有助于地震风险评估和地质工程应用。", "motivation": "预测微地震（MEQ）的完整时空演化对于增强型地热系统（EGS）、二氧化碳封存及其他地球工程应用至关重要，因为微地震记录了储层的应力状态和渗透率演变。", "method": "本文提出了一个基于Transformer的深度学习模型，该模型接收水力压裂历史和先前的微地震观测数据，以预测四个关键量：累积微地震计数、累积对数地震矩以及微地震云的第50和第95百分位范围（P50, P95）。该模型应用于EGS Collab实验1数据集，并提供通过学习的标准差项获得的不确定性估计。", "result": "该模型在所有目标上，1秒预测时限的R²值大于0.98，15秒预测时限的R²值大于0.88，并提供了不确定性估计。", "conclusion": "这些准确的、量化不确定性的预测能够实时推断裂缝扩展和渗透率演变，展示了深度学习方法在改善地震风险评估和指导未来流体注入操作中的缓解策略方面的巨大潜力。", "translation": "地下流体注入产生的微地震（MEQ）记录了储层不断演变的应力状态和渗透率。因此，预测其完整的时空演化对于增强型地热系统（EGS）、二氧化碳封存和其他地球工程应用至关重要。我们提出了一个基于Transformer的深度学习模型，该模型输入水力压裂历史和先前的微地震观测数据，以预测四个关键量：累积微地震计数、累积对数地震矩以及微地震云的第50和第95百分位范围（P50, P95）。该模型应用于EGS Collab实验1数据集，在所有目标上，1秒预测时限的R²值大于0.98，15秒预测时限的R²值大于0.88，并通过学习的标准差项提供不确定性估计。这些准确的、量化不确定性的预测能够实时推断裂缝扩展和渗透率演变，展示了深度学习方法在改善地震风险评估和指导未来流体注入操作中的缓解策略方面的巨大潜力。", "summary": "本文提出了一种基于Transformer的深度学习模型，用于预测流体诱发微地震（MEQs）的时空演化。该模型结合水力压裂历史和先前MEQ观测数据，预测累积MEQ计数、累积对数地震矩以及MEQ云的50%和95%范围。在EGS Collab实验1数据集上，模型在1秒预测时限内R²达到0.98以上，在15秒预测时限内R²达到0.88以上，并提供不确定性估计。这些高精度、量化不确定性的预测有望实现裂缝扩展和渗透率演变的实时推断，从而改善地震风险评估并指导未来的流体注入操作。", "keywords": "微地震, 深度学习, 时空预测, 流体注入, Transformer模型", "comments": "该论文的创新点在于首次将基于Transformer的深度学习模型应用于流体诱发微地震的时空演化预测，并成功地结合了高精度预测与不确定性量化。这对于地热系统、CO2封存等关键地球工程应用的实时监测和风险缓解具有重要意义。"}}
{"id": "2506.15346", "title": "Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges", "authors": ["A. S. Stankevich", "I. B. Petrov"], "summary": "Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.", "comment": "Submitted to \"Computational Mathematics And Mathematical Physics\",\n  ISSN 1555-6662, issue 8, August 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15346v1", "AI": {"title_translation": "声波全波形反演与图像到图像薛定谔桥", "tldr": "本文提出一种条件图像到图像薛定谔桥（cI2SB）框架，用于声波全波形反演，解决了扩散模型在FWI中存在的迭代、随机采样和输出控制的局限性，实现了更高分辨率和效率的反演结果。", "motivation": "现有的深度学习模型（如扩散模型）在声波全波形反演（FWI）中虽然能生成高分辨率样本，但其迭代、随机采样性质以及启发式的输出控制限制了适用性。特别是，如何将近似速度模型有效纳入基于扩散的反演方案中仍不清楚。", "method": "本文通过使用薛定谔桥来解决上述问题，该桥在真实（ground truth）速度模型分布与平滑速度模型分布之间进行插值。为了学习样本在分布间转移的非线性漂移，作者将图像到图像薛定谔桥（I2SB）的概念扩展到条件采样，从而提出了条件图像到图像薛定谔桥（cI2SB）框架。", "result": "实验证明，所提出的cI2SB解决方案在从平滑近似模型和固定形状的观测地震信号中重建参考速度模型方面，优于先前工作中提出的条件扩散模型的重新实现。同时，它仅需少量神经网络函数评估（NFEs）即可达到优于基于监督学习方法获得的样本保真度。", "conclusion": "本文成功开发并验证了条件图像到图像薛定谔桥（cI2SB）框架在声波全波形反演中的应用，有效克服了传统扩散模型的局限性，实现了更高效率和更高质量的速度模型重建。", "translation": "深度学习模型应用于声波全波形反演（FWI）的最新进展以使用扩散模型作为贝叶斯类推理过程的先验分布为标志。这些方法的优点是能够生成高分辨率样本，这是经典反演方法或其他基于深度学习的解决方案无法实现的。然而，从扩散模型采样的迭代和随机性质以及输出控制的启发式性质仍然是其适用性的限制因素。例如，如何将近似速度模型以最佳方式纳入基于扩散的反演方案中仍不清楚，尽管它被认为是FWI管道的重要组成部分。我们通过采用薛定谔桥来解决这个问题，该桥在真实（ground truth）速度模型和平滑速度模型的分布之间进行插值。为了促进学习在分布之间传输样本的非线性漂移，我们将图像到图像薛定谔桥（I2SB）的概念扩展到条件采样，从而产生了条件图像到图像薛定谔桥（cI2SB）框架。为了验证我们的方法，我们评估了它在结合固定形状的观测地震信号的情况下，从平滑近似模型重建参考速度模型的有效性。我们的实验表明，所提出的解决方案优于我们对早期工作中建议的条件扩散模型的重新实现，同时仅需要少量神经网络函数评估（NFEs）即可达到优于基于监督学习方法所获得的样本保真度。实现本文所述算法的补充代码可在存储库 https://github.com/stankevich-mipt/seismic_inversion_via_I2SB 中找到。", "summary": "本文提出一种新的条件图像到图像薛定谔桥（cI2SB）框架，旨在改进声波全波形反演（FWI）中深度学习模型的应用。针对现有扩散模型在FWI中迭代、随机采样和输出控制的局限性，特别是近似速度模型整合的挑战，cI2SB通过在真实和平滑速度模型分布之间插值来解决。实验结果表明，cI2SB在重建参考速度模型方面优于条件扩散模型，且仅需少量函数评估即可达到更高的样本保真度。", "keywords": "声波全波形反演, 薛定谔桥, 图像到图像, 深度学习, 地震反演", "comments": "这篇论文的创新点在于将薛定谔桥，特别是其条件图像到图像版本（cI2SB），引入到声波全波形反演中。这有效地解决了传统扩散模型在FWI中面临的迭代性、随机性和输出控制等挑战，并提高了反演效率和结果质量。该方法为地球物理反演领域提供了一个新的、更高效的深度学习解决方案。"}}
{"id": "2506.15596", "title": "Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image Registration", "authors": ["Kyobin Choo", "Hyunkyung Han", "Jinyeong Kim", "Chanyong Yoon", "Seong Jae Hwang"], "summary": "In clinical practice, imaging modalities with functional characteristics,\nsuch as positron emission tomography (PET) and fractional anisotropy (FA), are\noften aligned with a structural reference (e.g., MRI, CT) for accurate\ninterpretation or group analysis, necessitating multi-modal deformable image\nregistration (DIR). However, due to the extreme heterogeneity of these\nmodalities compared to standard structural scans, conventional unsupervised DIR\nmethods struggle to learn reliable spatial mappings and often distort images.\nWe find that the similarity metrics guiding these models fail to capture\nalignment between highly disparate modalities. To address this, we propose\nM2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal\nDIR models using only mono-modal similarity while preserving the established\narchitectural paradigm for seamless integration into existing models. We also\nintroduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training\nscheme to promote diffeomorphism. Furthermore, our framework naturally extends\nto a semi-supervised setting, integrating pre-aligned and unaligned pairs only,\nwithout requiring ground-truth transformations or segmentation masks.\nExperiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\ndemonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for\nPET-MRI and FA-MRI registration, highlighting its effectiveness in handling\nhighly heterogeneous multi-modal DIR. Our code is available at\nhttps://github.com/MICV-yonsei/M2M-Reg.", "comment": "11 pages, 3 figures, 2 tables, Accepted at Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15596v1", "AI": {"title_translation": "单模态化极端异构多模态医学图像配准", "tldr": "M2M-Reg 框架通过使用单模态相似性和新型正则化器，改进了多模态医学图像配准，并在高度异构数据上实现了更好的性能。", "motivation": "在临床实践中，由于功能性成像模态（如 PET 和 FA）与结构性参考（如 MRI 和 CT）之间存在极高的异质性，导致传统无监督多模态可变形图像配准 (DIR) 方法难以学习可靠的空间映射并经常扭曲图像，因为其相似性度量无法捕获高度不同模态之间的对齐。", "method": "提出 M2M-Reg（多对单配准）框架，该框架仅使用单模态相似性训练多模态 DIR 模型，同时保持现有架构范式以便无缝集成。引入 GradCyCon 正则化器，利用 M2M-Reg 的循环训练方案促进微分同胚。该框架还可自然扩展到半监督设置，仅集成预对齐和未对齐的图像对，无需真实变换或分割掩膜。", "result": "在阿尔茨海默病神经影像学倡议 (ADNI) 数据集上的实验表明，M2M-Reg 在 PET-MRI 和 FA-MRI 配准方面比现有方法实现了高达 2 倍的 DSC 提升。", "conclusion": "M2M-Reg 通过解决相似性度量的局限性并提高映射可靠性，有效处理了高度异构的多模态可变形图像配准问题。", "translation": "在临床实践中，具有功能特性的成像模态，如正电子发射断层扫描 (PET) 和分数各向异性 (FA)，通常与结构参考（例如 MRI、CT）对齐，以实现准确的解释或组分析，这需要多模态可变形图像配准 (DIR)。然而，由于这些模态与标准结构扫描相比具有极高的异质性，传统的无监督 DIR 方法难以学习可靠的空间映射，并且经常扭曲图像。我们发现指导这些模型的相似性度量无法捕获高度不同的模态之间的对齐。为了解决这个问题，我们提出了 M2M-Reg（多对单配准），这是一个新颖的框架，它仅使用单模态相似性来训练多模态 DIR 模型，同时保留已建立的架构范式，以便无缝集成到现有模型中。我们还引入了 GradCyCon，这是一种利用 M2M-Reg 的循环训练方案来促进微分同胚的正则化器。此外，我们的框架自然地扩展到半监督设置，仅集成预对齐和未对齐的对，而无需真实变换或分割掩膜。在阿尔茨海默病神经影像学倡议 (ADNI) 数据集上的实验表明，M2M-Reg 在 PET-MRI 和 FA-MRI 配准方面比以前的方法实现了高达 2 倍的 DSC，突出了其在处理高度异构多模态 DIR 方面的有效性。我们的代码可在 https://github.com/MICV-yonsei/M2M-Reg 获取。", "summary": "本文提出 M2M-Reg，一个用于多模态可变形图像配准（DIR）的新颖框架，旨在解决模态间（如 PET/FA 与 MRI/CT）极端异质性的挑战。与传统方法在相似性度量上的困境不同，M2M-Reg 仅利用单模态相似性训练 DIR 模型，并能与现有架构无缝集成。它还引入了 GradCyCon，一个通过循环训练促进微分同胚的正则化器。该框架支持半监督设置，无需真实变换。ADNI 数据集上的实验表明，M2M-Reg 在 PET-MRI 和 FA-MRI 配准上实现了高达 2 倍的 DSC 提升，证明了其在处理高度异构多模态 DIR 方面的有效性。", "keywords": "多模态图像配准, 可变形图像配准, 异构模态, 单模态相似性, 微分同胚", "comments": "本文的创新之处在于通过在循环训练方案中利用单模态相似性，解决了高度异构多模态配准中相似性度量的根本局限性。引入 GradCyCon 以促进微分同胚，以及无需真实变换的半监督扩展，都是重要的实际优势。这种方法为解决具有挑战性的临床问题提供了一个强大的解决方案。"}}
{"id": "2506.15610", "title": "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion", "authors": ["Yuqing Lan", "Chenyang Zhu", "Zhirui Gao", "Jiazhao Zhang", "Yihan Cao", "Renjiao Yi", "Yijie Wang", "Kai Xu"], "summary": "Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.", "comment": "11 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15610v1", "AI": {"title_translation": "BoxFusion：通过实时多视图包围盒融合实现免重建的开放词汇3D目标检测", "tldr": "BoxFusion提出了一种无需重建的在线框架，利用预训练模型和多视图包围盒融合技术，实现了内存高效和实时的开放词汇3D目标检测，并在ScanNetV2和CA-1M数据集上达到了最先进的性能。", "motivation": "现有的3D目标检测方法通常依赖于密集的点云重建，这带来了巨大的计算开销和内存限制，阻碍了在下游任务中的实时部署。为了解决这个问题，本文提出了一种新的免重建在线框架。", "method": "BoxFusion框架针对内存高效和实时3D检测而设计。它利用Cubify Anything作为预训练的视觉基础模型（VFM），通过包围盒进行单视图3D目标检测，并结合CLIP捕获检测到的对象的开放词汇语义。为了将不同视图中检测到的所有包围盒融合为一个统一的包围盒，该方法采用了一个关联模块来处理多视图对应关系，并使用一个优化模块来融合在多视图中预测的同一实例的3D包围盒。关联模块使用3D非最大抑制（NMS）和包围盒对应匹配模块，优化模块则使用基于粒子滤波的IoU引导高效随机优化技术，以强制执行3D包围盒的多视图一致性，同时最小化计算复杂度。", "result": "在ScanNetV2和CA-1M数据集上的大量实验表明，该方法在在线方法中取得了最先进的性能。受益于这种新颖的免重建3D目标检测范式，该方法在各种场景中表现出强大的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。", "conclusion": "BoxFusion提出了一种新颖的免重建在线框架，通过实时多视图包围盒融合实现了内存高效和实时的开放词汇3D目标检测，并在实验中证明了其最先进的性能和强大的泛化能力。", "translation": "开放词汇3D目标检测因其在自动驾驶和具身AI中的关键应用而获得了广泛关注。现有的检测方法，无论是离线还是在线，通常依赖于密集的点云重建，这带来了巨大的计算开销和内存限制，阻碍了在下游任务中的实时部署。为了解决这个问题，我们提出了一种新颖的免重建在线框架，专为内存高效和实时3D检测而设计。具体来说，给定流式姿态RGB-D视频输入，我们利用Cubify Anything作为预训练的视觉基础模型（VFM），通过包围盒进行单视图3D目标检测，并结合CLIP捕获检测到的对象的开放词汇语义。为了将不同视图中检测到的所有包围盒融合为一个统一的包围盒，我们采用了一个关联模块来处理多视图对应关系，并使用一个优化模块来融合在多视图中预测的同一实例的3D包围盒。关联模块使用3D非最大抑制（NMS）和包围盒对应匹配模块，而优化模块则使用基于粒子滤波的IoU引导高效随机优化技术，以强制执行3D包围盒的多视图一致性，同时最小化计算复杂度。在ScanNetV2和CA-1M数据集上的大量实验表明，我们的方法在在线方法中取得了最先进的性能。受益于这种新颖的免重建3D目标检测范式，我们的方法在各种场景中表现出强大的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。", "summary": "BoxFusion提出了一种无需重建的在线框架，用于开放词汇3D目标检测。该方法通过利用预训练的视觉基础模型（Cubify Anything）进行单视图检测，并结合CLIP实现开放词汇语义理解。其核心创新在于设计了多视图包围盒融合机制，包括一个关联模块（使用3D NMS和包围盒对应匹配）和一个优化模块（基于IoU引导的粒子滤波），以高效地融合来自不同视图的3D包围盒。实验结果表明，该方法在ScanNetV2和CA-1M数据集上实现了最先进的在线检测性能，并展现出在各种大尺度场景下进行实时感知的泛化能力。", "keywords": "3D目标检测, 开放词汇, 免重建, 多视图融合, 实时感知", "comments": "该论文提出了一种创新的免重建范式，解决了现有3D目标检测方法中重建带来的计算和内存瓶颈。其结合预训练视觉基础模型和CLIP进行开放词汇检测，并通过精巧的多视图包围盒融合机制实现高效和准确的3D检测，这对于自动驾驶和具身AI等实时应用具有重要意义。该方法的实时性和在大尺度环境下的泛化能力是其亮点。"}}
{"id": "2506.15378", "title": "Sampling 3D Molecular Conformers with Diffusion Transformers", "authors": ["J. Thorben Frank", "Winfried Ripken", "Gregor Lied", "Klaus-Robert Müller", "Oliver T. Unke", "Stefan Chmiela"], "summary": "Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15378v1", "AI": {"title_translation": "使用扩散变换器采样三维分子构象", "tldr": "DiTMC框架通过模块化架构和图基条件策略，成功将扩散变换器应用于分子构象生成，实现了最先进的精度和物理有效性。", "motivation": "扩散变换器（DiTs）在生成建模中表现出色，尤其是在图像合成方面，使其成为分子构象生成的有力选择。然而，将DiTs应用于分子面临新挑战，包括整合离散分子图信息与连续三维几何、处理欧几里得对称性以及设计可泛化到不同大小和结构分子的条件机制。", "method": "我们提出了DiTMC框架，通过模块化架构来解决这些挑战，该架构将三维坐标的处理与原子连接的条件处理分离。为此，我们引入了两种与DiT架构无缝集成的互补图基条件策略。这些策略与不同的注意力机制相结合，包括标准非等变和SO(3)等变公式，从而灵活控制精度和计算效率之间的权衡。", "result": "在标准构象生成基准（GEOM-QM9、-DRUGS、-XL）上的实验表明，DiTMC实现了最先进的精度和物理有效性。", "conclusion": "我们的结果强调了架构选择和对称先验如何影响样本质量和效率，为分子结构的大规模生成建模提供了有前景的方向。", "translation": "扩散变换器（DiTs）在生成建模中表现出强大的性能，特别是在图像合成方面，使其成为分子构象生成的有力选择。然而，将DiTs应用于分子引入了新的挑战，例如将离散分子图信息与连续三维几何相结合，处理欧几里得对称性，以及设计能够泛化到不同大小和结构分子的条件机制。我们提出了DiTMC，一个通过模块化架构适应DiTs以解决这些挑战的框架，该架构将三维坐标的处理与原子连接的条件处理分离。为此，我们引入了两种与DiT架构无缝集成的互补图基条件策略。这些策略与不同的注意力机制相结合，包括标准非等变和SO(3)等变公式，从而灵活控制精度和计算效率之间的权衡。在标准构象生成基准（GEOM-QM9、-DRUGS、-XL）上的实验表明，DiTMC实现了最先进的精度和物理有效性。我们的结果强调了架构选择和对称先验如何影响样本质量和效率，为分子结构的大规模生成建模提供了有前景的方向。代码可在https://github.com/ML4MolSim/dit_mc获取。", "summary": "该论文提出了DiTMC框架，旨在将扩散变换器（DiTs）应用于三维分子构象生成。面对整合离散图信息与连续几何、处理欧几里得对称性以及泛化不同分子大小的挑战，DiTMC采用模块化架构，分离三维坐标处理与原子连接条件化。通过引入两种图基条件策略和灵活的注意力机制（包括SO(3)等变形式），DiTMC在标准基准测试中取得了最先进的精度和物理有效性，并揭示了架构选择和对称先验对生成质量和效率的影响。", "keywords": "扩散变换器, 分子构象生成, 三维分子, DiTMC, SO(3)等变", "comments": "DiTMC的创新之处在于其模块化架构和图基条件策略，成功将DiTs应用于复杂的三维分子构象生成，克服了数据异构性和对称性挑战。它为大规模分子结构生成建模开辟了新途径，对药物发现和材料科学等领域具有重要意义。"}}
{"id": "2506.15625", "title": "HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization", "authors": ["Roey Ron", "Guy Tevet", "Haim Sawdayee", "Amit H. Bermano"], "summary": "We present HOIDiNi, a text-driven diffusion framework for synthesizing\nrealistic and plausible human-object interaction (HOI). HOI generation is\nextremely challenging since it induces strict contact accuracies alongside a\ndiverse motion manifold. While current literature trades off between realism\nand physical correctness, HOIDiNi optimizes directly in the noise space of a\npretrained diffusion model using Diffusion Noise Optimization (DNO), achieving\nboth. This is made feasible thanks to our observation that the problem can be\nseparated into two phases: an object-centric phase, primarily making discrete\nchoices of hand-object contact locations, and a human-centric phase that\nrefines the full-body motion to realize this blueprint. This structured\napproach allows for precise hand-object contact without compromising motion\nnaturalness. Quantitative, qualitative, and subjective evaluations on the GRAB\ndataset alone clearly indicate HOIDiNi outperforms prior works and baselines in\ncontact accuracy, physical validity, and overall quality. Our results\ndemonstrate the ability to generate complex, controllable interactions,\nincluding grasping, placing, and full-body coordination, driven solely by\ntextual prompts. https://hoidini.github.io.", "comment": "Project page: https://hoidini.github.io", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15625v1", "AI": {"title_translation": "HOIDiNi：通过扩散噪声优化实现人-物交互", "tldr": "HOIDiNi是一个文本驱动的扩散框架，通过在预训练扩散模型的噪声空间中进行优化，生成逼真且物理正确的人-物交互，解决了现有方法在真实性和物理正确性之间权衡的问题。", "motivation": "人-物交互（HOI）生成极具挑战性，因为它需要严格的接触精度和多样化的运动流形。现有方法在真实性和物理正确性之间存在权衡，无法同时实现两者。", "method": "HOIDiNi是一个文本驱动的扩散框架，通过扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化。该方法将问题分为两个阶段：一个以物体为中心的阶段，主要进行手-物体接触位置的离散选择；以及一个以人为中心的阶段，细化全身运动以实现该蓝图。这种结构化方法允许精确的手-物体接触，同时不损害运动的自然性。", "result": "在GRAB数据集上的定量、定性和主观评估表明，HOIDiNi在接触精度、物理有效性和整体质量方面明显优于现有工作和基线。结果证明了其能够仅通过文本提示生成复杂、可控的交互，包括抓取、放置和全身协调。", "conclusion": "HOIDiNi成功地通过创新的扩散噪声优化方法，实现了逼真且物理正确的人-物交互生成，克服了现有方法的局限性，并展示了通过文本提示生成复杂交互的能力。", "translation": "我们提出了HOIDiNi，一个文本驱动的扩散框架，用于合成逼真且合理的人-物交互（HOI）。HOI生成极具挑战性，因为它需要严格的接触精度以及多样化的运动流形。虽然现有文献在真实性和物理正确性之间进行权衡，但HOIDiNi通过使用扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化，从而同时实现这两者。这得益于我们的观察，即该问题可以分为两个阶段：一个以物体为中心的阶段，主要进行手-物体接触位置的离散选择；以及一个以人为中心的阶段，细化全身运动以实现该蓝图。这种结构化方法允许精确的手-物体接触，同时不损害运动的自然性。仅在GRAB数据集上的定量、定性和主观评估清楚地表明，HOIDiNi在接触精度、物理有效性和整体质量方面优于现有工作和基线。我们的结果展示了其能够仅通过文本提示生成复杂、可控的交互，包括抓取、放置和全身协调。https://hoidini.github.io。", "summary": "HOIDiNi是一个创新的文本驱动扩散框架，旨在解决人-物交互（HOI）生成中真实性和物理正确性难以兼顾的挑战。它通过扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行操作，并采用两阶段方法（物体中心和人体中心）实现精确接触和自然运动。实验结果表明，HOIDiNi在接触精度、物理有效性和整体质量方面优于现有方法，能够生成复杂的、由文本提示驱动的可控交互。", "keywords": "人-物交互, 扩散模型, 噪声优化, 文本驱动, 动作生成", "comments": "这篇论文的创新点在于提出了HOIDiNi框架，特别是在扩散模型的噪声空间中进行优化（DNO），以及将HOI生成问题分解为物体中心和人体中心的两阶段方法。这种方法有效地解决了人-物交互生成中长期存在的真实性和物理正确性之间的矛盾。其文本驱动和生成复杂可控交互的能力也显示了广阔的应用前景。"}}
{"id": "2506.15383", "title": "Global Ground Metric Learning with Applications to scRNA data", "authors": ["Damin Kühn", "Michael T. Schaub"], "summary": "Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.", "comment": "This method is provided as a Python package on PyPI, see\n  https://github.com/DaminK/ggml-ot", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15383v1", "AI": {"title_translation": "全局地面度量学习及其在单细胞RNA数据中的应用", "tldr": "提出一种新的全局地面度量学习方法，仅需分布级标签即可学习适用于任意分布的度量，提高最优传输距离准确性，并在scRNA数据上验证了其在嵌入、聚类和分类任务中的性能。", "motivation": "传统最优传输的地面度量要么预定义（如欧氏距离），无法反映数据固有结构和特征重要性；要么监督学习方法泛化性差或受限于共享支持集，难以适用于任意分布。", "method": "提出一种新的方法，用于学习共享度量空间中任意分布的度量。该方法提供个体点之间的距离，类似全局度量，但训练时仅需分布级别的类别标签。", "result": "学习到的全局地面度量能够实现更精确的最优传输距离，从而改进了嵌入、聚类和分类任务的性能。在跨多种疾病的患者级scRNA-seq数据上验证了其有效性和可解释性。", "conclusion": "本文提出的全局地面度量学习方法克服了传统方法的局限性，通过仅使用分布级标签学习度量，显著提升了最优传输距离的准确性，并在多项任务中表现出优越性。", "translation": "最优传输提供了一个比较概率分布的鲁棒框架。其有效性受到底层地面度量选择的显著影响。传统上，地面度量要么是(i)预定义的，例如欧氏距离，要么是(ii)通过利用标记数据以监督方式学习的，以学习适合增强任务特定性能的地面度量。然而，预定义的度量通常无法解释数据中固有的结构和不同特征的不同重要性，并且现有的地面度量学习监督方法通常不能跨多个类别泛化或仅限于具有共享支持的分布。为了解决这些限制，我们提出了一种新颖的方法，用于学习共享度量空间中任意分布的度量。我们的方法提供了个体点之间的距离，就像一个全局度量，但训练时仅需要分布级别的类别标签。学习到的全局地面度量能够实现更精确的最优传输距离，从而在嵌入、聚类和分类任务中带来改进的性能。我们使用跨多种疾病的患者级scRNA-seq数据证明了我们方法的有效性和可解释性。", "summary": "本文提出了一种新颖的全局地面度量学习方法，旨在克服现有最优传输地面度量方法的局限性。该方法能够在共享度量空间中为任意分布学习度量，仅需分布级别的类别标签进行训练。通过学习到的全局度量，能够获得更准确的最优传输距离，从而显著提升了在嵌入、聚类和分类任务中的性能。研究通过患者级单细胞RNA测序数据验证了该方法的有效性和可解释性。", "keywords": "全局地面度量学习, 最优传输, scRNA数据, 概率分布比较, 分布级标签", "comments": "该研究通过提出一种创新的全局地面度量学习方法，解决了最优传输中地面度量选择的长期挑战。其主要创新在于仅需分布级标签即可学习全局度量，这显著降低了对精细标记数据的需求，并提高了模型的泛化能力。在单细胞RNA数据上的应用展示了其在生物医学数据分析中的巨大潜力，尤其是在处理高维、复杂数据时的性能提升。"}}
{"id": "2506.15385", "title": "Provable Maximum Entropy Manifold Exploration via Diffusion Models", "authors": ["Riccardo De Santi", "Marin Vlastelica", "Ya-Ping Hsieh", "Zebang Shen", "Niao He", "Andreas Krause"], "summary": "Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15385v1", "AI": {"title_translation": "通过扩散模型实现可证明的最大熵流形探索", "tldr": "本文提出了一种利用预训练扩散模型进行探索的新框架，通过最大化其隐式数据流形上的熵，并基于密度估计和分数函数连接开发了一种可扩展的镜像下降算法，该算法被证明收敛并显示出有希望的经验结果。", "motivation": "探索对于解决现实世界的决策问题（例如科学发现）至关重要，其目标是生成真正新颖的设计，而不是模仿现有数据分布。本文旨在解决如何在不依赖显式不确定性量化的情况下，利用生成模型的表示能力进行探索的挑战。", "method": "本文将探索铸造为在预训练扩散模型隐式定义的近似数据流形上的熵最大化问题。提出了一种基于密度估计的探索新原理，并利用扩散模型诱导的密度熵与其分数函数之间的基本联系来克服密度估计的挑战。开发了一种基于镜像下降的算法，通过对预训练扩散模型进行顺序微调来解决探索问题。", "result": "在合成数据和高维文本到图像扩散任务上对所提出的方法进行了经验评估，并展示了有希望的结果。", "conclusion": "本文提出了一种新的探索框架，通过最大化扩散模型隐式流形上的熵实现新颖设计生成。所提出的基于镜像下降的算法被证明在现实假设下收敛到最优探索扩散模型，并在实验中取得了有希望的结果。", "translation": "探索对于解决现实世界的决策问题（例如科学发现）至关重要，其目标是生成真正新颖的设计，而不是模仿现有数据分布。在这项工作中，我们解决了如何在不依赖显式不确定性量化的情况下，利用生成模型的表示能力进行探索的挑战。我们引入了一个新颖的框架，将探索铸造为在预训练扩散模型隐式定义的近似数据流形上的熵最大化问题。然后，我们提出了一种基于密度估计的探索新原理，这是一个在实践中众所周知的难题。为了克服这个问题并使该方法真正可扩展，我们利用了扩散模型诱导的密度熵与其分数函数之间的基本联系。在此基础上，我们开发了一种基于镜像下降的算法，通过对预训练扩散模型进行顺序微调来解决探索问题。我们通过利用对镜像流的最新理解，证明了其在现实假设下收敛到最优探索扩散模型。最后，我们在合成数据和高维文本到图像扩散任务上对我们的方法进行了经验评估，展示了有希望的结果。", "summary": "该论文提出了一种新的探索框架，通过最大化预训练扩散模型隐式定义的近似数据流形上的熵，旨在生成新颖设计。为解决密度估计的挑战，该方法利用扩散模型密度熵与其分数函数之间的联系，并开发了一种基于镜像下降的算法，通过顺序微调扩散模型实现探索。该算法被证明在现实假设下收敛到最优探索模型，并在合成数据和高维文本到图像扩散任务上显示出有希望的经验结果。", "keywords": "扩散模型, 熵最大化, 流形探索, 镜像下降, 密度估计", "comments": "本文提出了一种新颖的探索方法，其创新之处在于将探索问题转化为扩散模型隐式流形上的熵最大化，并巧妙地利用了扩散模型分数函数与密度熵的联系，从而避免了显式的不确定性量化。更重要的是，该方法提供了可证明的收敛性保证，这在生成模型领域是难得的。其在科学发现等需要生成新颖设计的场景中具有重要潜力。"}}
{"id": "2506.15645", "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models", "authors": ["Shuo Xing", "Lanqing Guo", "Hongyuan Hua", "Seoyoung Lee", "Peiran Li", "Yufei Wang", "Zhangyang Wang", "Zhengzhong Tu"], "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.", "comment": "18 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15645v1", "AI": {"title_translation": "揭秘多模态大语言模型中的视觉质量悖论", "tldr": "多模态大语言模型（MLLM）有时在视觉质量较低的图像上表现更好；论文提出了一种名为VQ-TTT的新方法，用于自适应地调整图像以提高MLLM性能。", "motivation": "当前对多模态大语言模型（MLLM）的输入视觉质量如何影响其响应知之甚少，且尚不清楚图像的更高感知质量是否能转化为更好的MLLM理解。本研究旨在弥补这一认知空白并探究视觉质量与MLLM性能之间的关系。", "method": "研究团队首先对主流多模态大语言模型（MLLM）和一系列视觉-语言基准进行了首次系统性研究，对每张图像应用了受控的降级和风格转变。为解决发现的视觉质量悖论，论文引入了视觉质量测试时调优（VQ-TTT）模块，这是一种轻量级的自适应方法。VQ-TTT通过在冻结的视觉编码器前插入一个可学习的低秩核来调节频率内容，并通过LoRA仅微调浅层视觉编码器层，实现在单次前向传播中动态调整每个输入图像，使其与任务特定的模型偏好对齐。", "result": "研究发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至单个实例的性能都可能提高。现成的图像修复管道无法解决这些特异性偏好。所提出的VQ-TTT方法在所有评估的MLLM和所有数据集上显著提升了平均准确性，且无需外部模型、缓存特征或额外训练数据。", "conclusion": "本研究的发现重新定义了多模态大语言模型（MLLM）的“更好”视觉输入，并强调在AI作为主要数据客户的新时代，需要自适应而非普遍“干净”的图像。", "translation": "最近的多模态大语言模型（MLLM）在基准视觉-语言任务上表现出色，但对于输入视觉质量如何影响它们的响应却知之甚少。图像的更高感知质量是否已经转化为更好的MLLM理解？我们进行了首次系统性研究，涵盖了主流MLLM和一系列视觉-语言基准测试，对每张图像应用了受控的降级和风格转变。令人惊讶的是，我们发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至单个实例的性能都可能提高。现成的修复管道无法调和这些特异性偏好。为了弥合这一差距，我们引入了视觉质量测试时调优（VQ-TTT）——一个轻量级自适应模块，它：（1）在冻结的视觉编码器之前插入一个可学习的低秩核来调节频率内容；（2）通过LoRA仅微调浅层视觉编码器层。VQ-TTT在单次前向传播中动态调整每个输入图像，使其与任务特定的模型偏好对齐。在所有评估的MLLM和所有数据集上，VQ-TTT显著提升了平均准确性，且无需外部模型、缓存特征或额外训练数据。这些发现重新定义了MLLM的“更好”视觉输入，并强调在AI成为主要数据客户的新时代，需要自适应而非普遍“干净”的图像。", "summary": "本论文研究了视觉质量对多模态大语言模型（MLLM）性能的影响，揭示了一个“视觉质量悖论”，即MLLM在图像偏离人类感知保真度时表现反而可能更好。传统的图像修复方法无法解决这一问题。为此，作者提出了一种名为视觉质量测试时调优（VQ-TTT）的轻量级模块，它通过调节频率内容和微调浅层视觉编码器层来动态调整输入图像。VQ-TTT显著提高了MLLM在多个模型和数据集上的准确性，表明对MLLM而言，适应性而非普遍“干净”的图像输入至关重要。", "keywords": "多模态大语言模型, 视觉质量, 视觉质量悖论, 测试时调优, VQ-TTT", "comments": "这篇论文的创新之处在于它挑战了“视觉质量越高AI表现越好”的传统假设，尤其是在多模态大语言模型领域。发现“视觉质量悖论”是一个重要突破。VQ-TTT提供了一种高效实用的解决方案，用于根据MLLM的偏好调整视觉输入，这对于未来MLLM的开发和部署，尤其是在输入数据质量多变或需要针对特定任务进行优化的情况下，具有重要意义。它预示着AI中心化数据准备范式的转变。"}}
{"id": "2506.15649", "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "authors": ["Ankan Deria", "Adinath Madhavrao Dukre", "Feilong Tang", "Sara Atito", "Sudipta Roy", "Muhammad Awais", "Muhammad Haris Khan", "Imran Razzak"], "summary": "Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15649v1", "AI": {"title_translation": "双阶段价值引导推理与基于边际奖励调整，实现快速忠实的VLM字幕生成", "tldr": "本文提出了ViMaR，一个双阶段推理框架，结合时间差值模型和基于边际的奖励调整，显著提高了视觉语言模型（VLM）字幕生成的效率和忠实度，减少了幻觉，并实现了跨模型泛化。", "motivation": "现有视觉语言模型（VLM）的推理时搜索方法计算成本高昂，且容易产生未受惩罚的低置信度生成，这通常导致持续的幻觉问题。", "method": "本文引入了“基于边际奖励的价值引导推理”（ViMaR），这是一个双阶段推理框架。第一阶段，通过单次遍历从多样化候选字幕中识别最高价值的字幕。第二阶段，仅选择性地细化那些被忽略或视觉基础薄弱的片段，从而消除频繁奖励的评估。通过校准的基于边际的惩罚来阻止低置信度的延续，同时保留描述的丰富性。", "result": "ViMaR生成的字幕显著更可靠、事实更准确、更详细、更具解释性，并且与现有价值引导方法相比，速度提升了4倍以上。ViMaR在LLaVA Mistral-7B上训练后，能有效泛化以指导更强的未见模型（如LLaVA-OneVision-Qwen2-7B）的解码，展现出强大的跨模型指导能力。此外，当使用ViMaR生成的字幕进行自训练时，底层模型在各种视觉理解基准上取得了显著提升。", "conclusion": "ViMaR是一个灵活、模块化、可扩展和可转移的推理时解码策略，能够实现快速、准确和自改进的VLM管道。", "translation": "尽管视觉语言模型（VLM）在推理时搜索方面取得了显著进展，但现有方法仍然计算成本高昂，并且容易产生未受惩罚的低置信度生成，这通常导致持续的幻觉。我们引入了“基于边际奖励的价值引导推理”（ViMaR），这是一个双阶段推理框架，通过将时间差值价值模型与基于边际感知的奖励调整相结合，提高了效率和输出忠实度。在第一阶段，我们执行单次遍历以识别多样化候选字幕中价值最高的字幕。在第二阶段，我们选择性地细化那些被忽略或视觉基础薄弱的片段，从而消除频繁奖励的评估。校准的基于边际的惩罚阻止了低置信度的延续，同时保留了描述的丰富性。在多种VLM架构上的广泛实验表明，ViMaR生成的字幕显著更可靠、事实更准确、更详细、更具解释性，同时与现有价值引导方法相比，速度提升了4倍以上。具体来说，我们展示了仅在LLaVA Mistral-7B上训练的ViMaR能够有效地泛化以指导更强的未见模型的解码。为了进一步验证这一点，我们调整了ViMaR以引导LLaVA-OneVision-Qwen2-7B中的生成，从而持续提高字幕质量并展示了强大的跨模型指导能力。这种跨模型泛化突出了ViMaR的灵活性和模块化，使其成为一种可扩展和可转移的推理时解码策略。此外，当使用ViMaR生成的字幕进行自训练时，底层模型在广泛的视觉理解基准上取得了显著提升，这突显了快速、准确和自改进的VLM管道的潜力。", "summary": "本文提出了一种名为ViMaR的双阶段推理框架，旨在解决现有视觉语言模型（VLM）字幕生成中效率低和幻觉问题。ViMaR结合了时间差值价值模型和基于边际的奖励调整，通过两阶段过程优化字幕生成：首先快速识别高价值候选，然后选择性地细化薄弱或被忽略的部分。实验证明，ViMaR生成的字幕在可靠性、准确性、细节和解释性方面显著优于现有方法，并实现了超过4倍的速度提升。此外，ViMaR展现出强大的跨模型泛化能力，其生成的字幕还能用于自训练以进一步提升底层模型的性能，为构建高效、准确且能自我改进的VLM系统提供了新途径。", "keywords": "价值引导推理, VLM字幕生成, 基于边际奖励, 双阶段推理, 幻觉减少", "comments": "ViMaR的创新之处在于其独特的双阶段推理架构，它巧妙地结合了价值引导和边际奖励调整，有效解决了VLM字幕生成中的效率瓶颈和幻觉问题。其在推理速度上的显著提升（4倍以上）和在字幕质量上的改善（更可靠、准确、详细）具有重要意义。特别值得注意的是，ViMaR展现出的强大跨模型泛化能力和通过自训练进一步提升模型性能的潜力，使其成为VLM领域一个极具前景且可扩展的通用解码策略。"}}
{"id": "2506.15408", "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI", "authors": ["David Dembinsky", "Adriano Lucieri", "Stanislav Frolov", "Hiba Najjar", "Ko Watanabe", "Andreas Dengel"], "summary": "Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.", "comment": "Submitted to TMLR, under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15408v1", "AI": {"title_translation": "统一VXAI：可解释人工智能评估的系统综述与框架", "tldr": "该研究通过系统综述提出了一个统一的可解释人工智能（XAI）评估框架（VXAI），解决了XAI评估缺乏标准化协议和共识的问题。", "motivation": "现代AI系统（特别是深度神经网络）的复杂性导致其缺乏透明度，从而影响了可信度。可解释人工智能（XAI）旨在提供人类可理解的模型行为解释，但目前XAI评估领域缺乏标准化的评估协议和合适的度量标准共识。", "method": "本研究遵循PRISMA指南进行了系统文献综述，识别了362篇相关出版物，并将其贡献整合为41个功能相似的度量组。在此基础上，提出了一个统一的XAI评估框架（VXAI），并设计了一个涵盖解释类型、评估情境性和解释质量要求的三个维度分类方案。", "result": "识别出362篇相关出版物，并将其贡献聚合为41个功能相似的度量组。提出了一个统一的XAI评估框架（VXAI）和一个三维分类方案，涵盖解释类型、评估情境性和解释质量要求。", "conclusion": "本框架提供了迄今为止最全面、结构化的VXAI概览，支持系统化的度量选择，促进了不同方法之间的可比性，并为未来的扩展提供了灵活的基础。", "translation": "现代AI系统通常依赖不透明的黑盒模型，最著名的是深度神经网络，其性能源于具有数百万学习参数的复杂架构。尽管功能强大，但它们的复杂性对可信度构成了重大挑战，特别是由于缺乏透明度。可解释人工智能（XAI）通过提供人类可理解的模型行为解释来解决这个问题。然而，为了确保其有用性和可信度，这些解释必须经过严格评估。尽管XAI方法数量不断增长，但该领域缺乏标准化的评估协议和对适当度量标准的共识。为了弥补这一空白，我们遵循系统评价和荟萃分析优先报告项目（PRISMA）指南进行了一项系统文献综述，并引入了一个统一的XAI评估框架（VXAI）。我们识别了362篇相关出版物，并将其贡献整合为41个功能相似的度量组。此外，我们提出了一个涵盖解释类型、评估情境性和解释质量要求的三个维度分类方案。我们的框架提供了迄今为止最全面、结构化的VXAI概览。它支持系统化的度量选择，促进了方法之间的可比性，并为未来的扩展提供了灵活的基础。", "summary": "本研究旨在解决可解释人工智能（XAI）评估中缺乏标准化协议和共识的问题。通过遵循PRISMA指南的系统文献综述，作者识别了362篇相关文献，并将其贡献整合为41个度量组。在此基础上，提出了一个统一的XAI评估框架（VXAI），并引入了一个基于解释类型、评估情境性和解释质量要求的三维分类方案。该框架被认为是迄今为止最全面、结构化的VXAI概览，旨在支持系统化的度量选择，提高方法间的可比性，并为未来研究奠定基础。", "keywords": "可解释人工智能, XAI, 评估框架, 系统综述, 透明度", "comments": "该论文通过对现有文献进行系统综述，提出了一个统一的XAI评估框架，这对于标准化和规范XAI方法的评估具有重要意义。其创新之处在于聚合了大量评估指标并提出了多维度分类方案，有助于解决当前领域内评估标准不统一的痛点。该框架的提出有望促进XAI领域研究的严谨性和可比性。"}}
{"id": "2506.15041", "title": "Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models", "authors": ["Tobias Schmidt", "Kai-Robin Lange", "Matthias Reccius", "Henrik Müller", "Michael Roos", "Carsten Jentsch"], "summary": "As interest in economic narratives has grown in recent years, so has the\nnumber of pipelines dedicated to extracting such narratives from texts.\nPipelines often employ a mix of state-of-the-art natural language processing\ntechniques, such as BERT, to tackle this task. While effective on foundational\nlinguistic operations essential for narrative extraction, such models lack the\ndeeper semantic understanding required to distinguish extracting economic\nnarratives from merely conducting classic tasks like Semantic Role Labeling.\nInstead of relying on complex model pipelines, we evaluate the benefits of\nLarge Language Models (LLMs) by analyzing a corpus of Wall Street Journal and\nNew York Times newspaper articles about inflation. We apply a rigorous\nnarrative definition and compare GPT-4o outputs to gold-standard narratives\nproduced by expert annotators. Our results suggests that GPT-4o is capable of\nextracting valid economic narratives in a structured format, but still falls\nshort of expert-level performance when handling complex documents and\nnarratives. Given the novelty of LLMs in economic research, we also provide\nguidance for future work in economics and the social sciences that employs LLMs\nto pursue similar objectives.", "comment": "53 pages, 5 figures", "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.15041v1", "AI": {"title_translation": "在大规模文本语料库中识别经济叙事——一种使用大型语言模型的集成方法", "tldr": "本研究评估了大型语言模型（LLMs），特别是GPT-4o，在从文本中提取经济叙事方面的能力。结果显示GPT-4o能提取有效叙事，但在复杂文档处理上仍不如专家。", "motivation": "随着对经济叙事兴趣的增长，现有用于从文本中提取经济叙事的NLP管道（如BERT）虽然在基础语言操作上有效，但缺乏更深层次的语义理解能力，无法区分经济叙事提取与经典任务（如语义角色标注）。因此，需要评估大型语言模型（LLMs）的潜力。", "method": "研究评估了大型语言模型（LLMs）的优势，通过分析华尔街日报和纽约时报关于通货膨胀的新闻文章语料库。研究应用了严格的叙事定义，并将GPT-4o的输出与专家标注的黄金标准叙事进行比较。", "result": "结果表明GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时，其表现仍未达到专家水平。", "conclusion": "GPT-4o能够提取经济叙事，但在复杂任务上仍有不足。鉴于LLMs在经济研究中的新颖性，本研究为未来在经济学和社会科学中利用LLMs实现类似目标提供了指导。", "translation": "近年来，随着对经济叙事兴趣的增长，专门从文本中提取此类叙事的管道数量也随之增加。这些管道通常采用最先进的自然语言处理技术组合，例如BERT，来处理这项任务。虽然这些模型在叙事提取所必需的基础语言操作上是有效的，但它们缺乏区分经济叙事提取与仅仅执行语义角色标注等经典任务所需的更深层次的语义理解。本研究没有依赖复杂的模型管道，而是通过分析《华尔街日报》和《纽约时报》关于通货膨胀的新闻文章语料库来评估大型语言模型（LLMs）的优势。我们应用了严格的叙事定义，并将GPT-4o的输出与专家标注的黄金标准叙事进行比较。我们的结果表明，GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时，其表现仍未达到专家水平。鉴于LLMs在经济研究中的新颖性，我们还为未来在经济学和社会科学中利用LLMs实现类似目标的工作提供了指导。", "summary": "本研究旨在评估大型语言模型（LLMs）在从文本中识别经济叙事方面的能力，以解决现有自然语言处理技术在深层语义理解方面的不足。通过使用GPT-4o分析关于通货膨胀的新闻文章语料库，并与专家标注的黄金标准进行比较，研究发现GPT-4o能够有效提取结构化的经济叙事。然而，在处理复杂文档和叙事时，其性能仍未达到专家水平。鉴于LLMs在经济研究中的新兴性，本研究也为未来在经济学和社会科学领域应用LLMs提供了指导。", "keywords": "经济叙事, 大型语言模型, GPT-4o, 自然语言处理, 通货膨胀", "comments": "该论文的创新之处在于首次系统地评估了大型语言模型（LLMs）在经济叙事提取这一特定且复杂的任务上的表现，而非仅仅依赖传统的NLP管道。其重要性在于为经济学和社会科学领域利用LLMs进行研究提供了初步的实证证据和实践指导。局限性在于指出当前LLMs（如GPT-4o）在处理复杂经济叙事时仍无法完全媲美人类专家，这为未来模型改进和研究方向提供了清晰的指引。"}}
{"id": "2506.15673", "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting", "authors": ["Kai He", "Ruofan Liang", "Jacob Munkberg", "Jon Hasselgren", "Nandita Vijaykumar", "Alexander Keller", "Sanja Fidler", "Igor Gilitschenski", "Zan Gojcic", "Zian Wang"], "summary": "We address the challenge of relighting a single image or video, a task that\ndemands precise scene intrinsic understanding and high-quality light transport\nsynthesis. Existing end-to-end relighting models are often limited by the\nscarcity of paired multi-illumination data, restricting their ability to\ngeneralize across diverse scenes. Conversely, two-stage pipelines that combine\ninverse and forward rendering can mitigate data requirements but are\nsusceptible to error accumulation and often fail to produce realistic outputs\nunder complex lighting conditions or with sophisticated materials. In this\nwork, we introduce a general-purpose approach that jointly estimates albedo and\nsynthesizes relit outputs in a single pass, harnessing the generative\ncapabilities of video diffusion models. This joint formulation enhances\nimplicit scene comprehension and facilitates the creation of realistic lighting\neffects and intricate material interactions, such as shadows, reflections, and\ntransparency. Trained on synthetic multi-illumination data and extensive\nautomatically labeled real-world videos, our model demonstrates strong\ngeneralization across diverse domains and surpasses previous methods in both\nvisual fidelity and temporal consistency.", "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/UniRelight/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15673v1", "AI": {"title_translation": "UniRelight：学习视频重新照明的联合分解与合成", "tldr": "UniRelight提出了一种新的联合分解与合成方法，利用视频扩散模型实现单次通过的图像/视频重照明，解决了现有方法的泛化性和真实性问题，并在视觉保真度和时间一致性上超越了现有方法。", "motivation": "现有端到端重照明模型受限于多照明配对数据稀缺，导致泛化能力差。两阶段流水线虽然数据需求低，但易受误差累积影响，难以在复杂光照和材质下生成真实效果。因此，需要一种更通用、更鲁彻的重照明方法。", "method": "本文引入了一种名为UniRelight的通用方法，它在单次通过中联合估计反照率并合成重照明输出。该方法利用了视频扩散模型的生成能力，增强了隐式场景理解，并能生成逼真的光照效果和复杂的材质交互（如阴影、反射和透明度）。模型在合成多照明数据和大量自动标注的真实世界视频上进行训练。", "result": "UniRelight模型在不同领域展现出强大的泛化能力，并在视觉保真度和时间一致性方面超越了以往的方法。", "conclusion": "通过联合估计反照率和合成重照明输出的单次通过方法，结合视频扩散模型的生成能力和多样化的训练数据，UniRelight成功克服了现有重照明方法的局限性，实现了在视觉保真度和时间一致性上的显著提升，并展现出强大的泛化能力。", "translation": "我们解决了对单张图像或视频进行重照明的挑战，这项任务需要精确的场景内在理解和高质量的光传输合成。现有的端到端重照明模型往往受限于配对的多照明数据稀缺，这限制了它们在不同场景中的泛化能力。相反，结合逆向和正向渲染的两阶段流水线可以减少数据需求，但容易受到误差累积的影响，并且在复杂光照条件或复杂材质下通常无法产生真实感输出。在这项工作中，我们引入了一种通用方法，它在单次通过中联合估计反照率并合成重照明输出，利用了视频扩散模型的生成能力。这种联合公式增强了隐式场景理解，并促进了逼真光照效果和复杂材质交互（如阴影、反射和透明度）的创建。我们的模型在合成多照明数据和大量自动标注的真实世界视频上进行训练，在不同领域展现出强大的泛化能力，并在视觉保真度和时间一致性方面超越了以往的方法。", "summary": "UniRelight提出了一种创新的视频重照明方法，通过在单次通过中联合进行反照率估计和重照明合成，解决了当前方法在数据稀缺和误差累积方面的限制。该方法利用视频扩散模型的强大生成能力，显著提升了场景理解、光照效果和材质交互的真实性。在合成和真实视频数据上训练后，UniRelight在泛化能力、视觉保真度和时间一致性上均超越了现有技术。", "keywords": "视频重照明, 联合分解, 视频扩散模型, 反照率估计, 光传输", "comments": "UniRelight的创新点在于其“联合分解与合成”的单次通过方法，有效结合了逆向和正向渲染的优点，同时避免了传统两阶段方法的误差累积问题。利用视频扩散模型是其成功的关键，赋予了模型强大的生成能力和对复杂光照及材质的鲁棒性。其在多源数据（合成与自动标注真实数据）上的训练策略也增强了模型的泛化能力，使其在实际应用中更具潜力。"}}
{"id": "2506.15421", "title": "Reward Models in Deep Reinforcement Learning: A Survey", "authors": ["Rui Yu", "Shenghua Wan", "Yucen Wang", "Chen-Xiao Gao", "Le Gan", "Zongzhang Zhang", "De-Chuan Zhan"], "summary": "In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.", "comment": "IJCAI 2025 Survey Track (To Appear)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15421v1", "AI": {"title_translation": "深度强化学习中的奖励模型：一项综述", "tldr": "本综述全面回顾了深度强化学习中奖励建模技术，填补了该领域系统性综述的空白，并指出了未来研究方向。", "motivation": "奖励模型在强化学习中作为期望目标的代理，对于指导策略优化至关重要。鉴于学术界和工业界对开发与真实目标对齐且促进策略优化的奖励模型的高度关注，以及当前文献中缺乏对奖励模型的系统性综述，本研究旨在填补这一空白。", "method": "本综述首先概述了奖励建模的背景和预备知识，然后根据来源、机制和学习范式对近期奖励建模方法进行了分类回顾。在此基础上，讨论了这些技术的各种应用，并审查了评估奖励模型的方法。", "result": "本综述提供了一个涵盖已建立和新兴方法的奖励建模技术全面回顾，系统地组织了现有知识，并识别了奖励建模领域的有前景的研究方向。", "conclusion": "本综述通过提供对深度强化学习中奖励模型的系统性回顾，填补了当前文献的空白，并强调了奖励建模领域未来有前景的研究方向。", "translation": "在强化学习（RL）中，智能体持续与环境交互并利用反馈来改进其行为。为了指导策略优化，奖励模型被引入作为期望目标的代理，以便当智能体最大化累积奖励时，也能实现任务设计者的意图。最近，学术界和工业界的研究人员都将大量注意力集中在开发不仅与真实目标紧密对齐，而且能促进策略优化的奖励模型上。在本综述中，我们对深度强化学习文献中的奖励建模技术进行了全面回顾。我们首先概述了奖励建模的背景和预备知识。接下来，我们对近期奖励建模方法进行了概述，并根据其来源、机制和学习范式进行了分类。在此理解的基础上，我们讨论了这些奖励建模技术的各种应用，并回顾了评估奖励模型的方法。最后，我们通过强调奖励建模中有前景的研究方向进行总结。总而言之，本综述包含了已建立和新兴的方法，填补了当前文献中奖励模型系统性回顾的空白。", "summary": "本综述全面审视了深度强化学习中的奖励模型。文章首先介绍了奖励建模的基础知识，随后根据来源、机制和学习范式对当前主流的奖励建模方法进行了分类和概述。此外，综述还探讨了奖励模型的应用场景及其评估方法，并展望了未来的研究方向。这项工作旨在弥补现有文献中缺乏系统性奖励模型综述的不足。", "keywords": "奖励模型, 深度强化学习, 策略优化, 综述, 奖励建模", "comments": "这篇综述的重要性在于它系统地整理了深度强化学习中奖励建模这一关键且快速发展的领域。它不仅为研究人员提供了全面的背景知识和现有方法的概览，还通过分类帮助读者理解不同方法的特点和适用性。更重要的是，它指出了未来的研究方向，对推动该领域的进步具有指导意义。"}}
{"id": "2506.15675", "title": "Sekai: A Video Dataset towards World Exploration", "authors": ["Zhen Li", "Chuanhao Li", "Xiaofeng Mao", "Shaoheng Lin", "Ming Li", "Shitian Zhao", "Zhaopan Xu", "Xinyue Li", "Yukang Feng", "Jianwen Sun", "Zizhen Li", "Fanrui Zhang", "Jiaxin Ai", "Zhixiang Wang", "Yuwei Wu", "Tong He", "Jiangmiao Pang", "Yu Qiao", "Yunde Jia", "Kaipeng Zhang"], "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.", "comment": "12 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15675v1", "AI": {"title_translation": "Sekai：一个面向世界探索的视频数据集", "tldr": "Sekai是一个大规模的第一人称视角全球视频数据集，旨在解决现有视频生成数据集在世界探索训练方面的局限性。", "motivation": "现有视频生成数据集在世界探索训练方面存在局限性，包括地点有限、持续时间短、场景静态以及缺乏探索和世界相关注释。", "method": "本文介绍了Sekai数据集，一个高质量的第一人称视角全球视频数据集，包含超过5,000小时的步行或无人机视角视频，来自100多个国家和地区的750个城市。开发了一个高效的工具箱来收集、预处理和注释视频，包括地点、场景、天气、人群密度、字幕和摄像机轨迹等信息。", "result": "实验证明了数据集的质量。使用数据集的一个子集训练了一个名为YUME的交互式视频世界探索模型。", "conclusion": "Sekai数据集将有益于视频生成和世界探索领域，并激发有价值的应用。", "translation": "视频生成技术取得了显著进展，有望成为交互式世界探索的基础。然而，现有的视频生成数据集不适合世界探索训练，因为它们存在一些局限性：地点有限、持续时间短、场景静态以及缺乏关于探索和世界的注释。在本文中，我们介绍了Sekai（日语中意为“世界”），一个高质量的第一人称视角全球视频数据集，带有丰富的世界探索注释。它包含来自100多个国家和地区750个城市的5,000多小时的步行或无人机视角（FPV和UVA）视频。我们开发了一个高效且有效的工具箱，用于收集、预处理和注释视频，包括地点、场景、天气、人群密度、字幕和摄像机轨迹。实验证明了数据集的质量。此外，我们使用一个子集训练了一个名为YUME（日语中意为“梦想”）的交互式视频世界探索模型。我们相信Sekai将有益于视频生成和世界探索领域，并激发有价值的应用。", "summary": "本文介绍了Sekai数据集，一个大规模高质量的第一人称视角全球视频数据集，旨在解决现有视频生成数据集在世界探索训练中的局限性。Sekai包含来自全球各地的5,000多小时视频，并附带丰富的地点、场景、天气、人群密度、字幕和摄像机轨迹等注释。研究人员开发了一个高效的工具箱来收集和处理数据，并通过实验证明了数据集的质量。此外，他们还利用Sekai的一个子集训练了一个名为YUME的交互式视频世界探索模型，预期该数据集将推动视频生成和世界探索领域的发展。", "keywords": "视频数据集, 世界探索, 第一人称视角, 视频生成, Sekai", "comments": "Sekai数据集的创新之处在于其前所未有的规模和丰富度，特别是在第一人称视角和全球覆盖方面。它解决了现有数据集在地点、持续时间、场景和注释方面的不足，为交互式世界探索和视频生成提供了急需的基础资源。其附带的工具箱也体现了实用性。该数据集有望成为推动相关研究领域突破的关键。"}}
{"id": "2506.15446", "title": "Zero-Shot Reinforcement Learning Under Partial Observability", "authors": ["Scott Jeen", "Tom Bewley", "Jonathan M. Cullen"], "summary": "Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.", "comment": "Reinforcement Learning Conference 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15446v1", "AI": {"title_translation": "零样本强化学习在部分可观测性下的表现", "tldr": "本文探讨了零样本强化学习在部分可观测环境下的性能退化问题，并提出基于记忆的架构能有效提升其表现。", "motivation": "现有零样本强化学习方法的一个关键假设是能够访问完全的马尔可夫状态，然而在许多实际应用中，环境状态通常是部分可观测的，这导致标准零样本强化学习方法的性能可能显著下降。", "method": "本文首先探究了标准零样本强化学习方法在部分可观测性下的性能如何退化，随后提出并评估了基于记忆的架构作为解决方案。研究在状态、奖励和动态变化均部分可观测的领域中对这些基于记忆的零样本强化学习方法进行了实验验证。", "result": "基于记忆的零样本强化学习方法在部分可观测领域中表现出比无记忆基线方法更优的性能。", "conclusion": "基于记忆的架构是有效提升零样本强化学习在部分可观测环境下性能的补救措施。", "translation": "近期工作表明，在特定假设下，零样本强化学习（RL）方法在无奖励预训练后可以泛化到环境中任何未见过的任务。访问马尔可夫状态是其中一个假设，然而，在许多现实世界应用中，马尔可夫状态仅是部分可观测的。本文探讨了标准零样本RL方法在部分可观测性下性能如何下降，并表明与单任务RL一样，基于记忆的架构是一种有效的补救措施。我们在状态、奖励和动态变化都部分可观测的领域中评估了我们基于记忆的零样本RL方法，并显示出比无记忆基线方法更好的性能。我们的代码已通过以下链接开源：https://enjeeneer.io/projects/bfms-with-memory/。", "summary": "本文研究了零样本强化学习（RL）在部分可观测环境下的性能问题，指出其在现实应用中因马尔可夫状态部分可观测而面临挑战。研究发现，在这种情况下，标准零样本RL方法的性能会下降。为解决此问题，作者提出并评估了基于记忆的架构，实验结果表明，在状态、奖励和动态变化均部分可观测的领域中，基于记忆的零样本RL方法比无记忆基线方法表现更优，证明了记忆机制在处理部分可观测性问题上的有效性。", "keywords": "零样本强化学习, 部分可观测性, 记忆网络, 泛化, 强化学习", "comments": "这篇论文通过引入记忆机制，有效地解决了零样本强化学习在现实世界部分可观测环境下的泛化挑战，增强了零样本RL方法的实用性。其创新点在于将记忆架构引入零样本RL，弥补了现有方法在非完全可观测条件下的不足。"}}
{"id": "2506.15682", "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model", "authors": ["Anirud Aggarwal", "Abhinav Shrivastava", "Matthew Gwilliam"], "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.", "comment": "29 pages, 22 figures, 9 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15682v1", "AI": {"title_translation": "进化缓存加速现成扩散模型", "tldr": "ECAD是一种基于遗传算法的进化缓存方法，能为扩散模型学习高效的缓存策略，显著加速推理并保持生成质量，且无需修改模型参数。", "motivation": "扩散模型生成高质量图像的能力很强，但推理速度慢且计算成本高。现有通过缓存和重用特征来加速的方法往往依赖于僵化的启发式规则，导致加速有限或泛化能力差。", "method": "本文提出进化缓存加速扩散模型（ECAD），这是一种遗传算法，它通过少量校准提示学习高效的、针对特定模型的缓存策略，形成帕累托前沿。ECAD不需要修改网络参数或参考图像。", "result": "ECAD显著提升了推理速度，实现了对质量-延迟权衡的精细控制，并能无缝适应不同的扩散模型。其学习到的调度方案能够有效泛化到校准期间未见过的分辨率和模型变体。在PixArt-alpha、PixArt-Sigma和FLUX-1.dev上，通过多项指标（FID、CLIP、Image Reward）和多个基准（COCO、MJHQ-30k、PartiPrompts）进行评估，ECAD始终优于现有方法。在PixArt-alpha上，ECAD找到的调度方案在COCO FID上优于现有最佳方法4.47，同时将推理加速从2.35倍提高到2.58倍。", "conclusion": "ECAD是一种可扩展且通用的加速扩散推理的方法。", "translation": "基于扩散的图像生成模型在生成高质量合成内容方面表现出色，但推理速度慢且计算成本高。先前的工作试图通过在推理步骤中缓存和重用扩散Transformer中的特征来缓解这一问题。然而，这些方法通常依赖于僵化的启发式规则，导致加速有限或在不同架构上的泛化能力差。我们提出了进化缓存加速扩散模型（ECAD），这是一种遗传算法，它仅使用一小组校准提示，学习高效的、针对每个模型的缓存调度，形成帕累托前沿。ECAD不需要对网络参数或参考图像进行任何修改。它提供了显著的推理加速，能够对质量-延迟权衡进行精细控制，并无缝适应不同的扩散模型。值得注意的是，ECAD学习到的调度方案可以有效地泛化到校准期间未见过的分辨率和模型变体。我们在PixArt-alpha、PixArt-Sigma和FLUX-1.dev上使用多项指标（FID、CLIP、Image Reward）和不同的基准（COCO、MJHQ-30k、PartiPrompts）评估了ECAD，展示了其相对于先前方法的持续改进。在PixArt-alpha上，ECAD识别出一个调度方案，其COCO FID比先前的最新方法提高了4.47，同时推理加速从2.35倍提高到2.58倍。我们的结果确立了ECAD是一种可扩展且通用的加速扩散推理的方法。我们的项目网站可在https://aniaggarwal.github.io/ecad访问，我们的代码可在https://github.com/aniaggarwal/ecad访问。", "summary": "本文提出ECAD，一种基于遗传算法的进化缓存方法，旨在解决扩散模型推理速度慢且计算成本高的问题。ECAD通过学习高效的、针对特定模型的缓存策略，形成帕累托前沿，无需修改网络参数，并能提供显著的推理加速，同时保持图像质量。实验结果表明，ECAD在多个扩散模型和基准测试中均优于现有方法，并能有效泛化到未见过的场景。", "keywords": "进化缓存, 扩散模型, 推理加速, 遗传算法, 图像生成", "comments": "ECAD的创新之处在于利用遗传算法动态学习最优缓存策略，而非依赖固定启发式规则，这使其具有更强的适应性和泛化能力。其无需修改模型参数的特性也大大降低了应用门槛。该方法对于提升扩散模型在实际应用中的效率具有重要意义，尤其是在对实时性有要求的场景。"}}
{"id": "2506.14995", "title": "Improved Image Reconstruction and Diffusion Parameter Estimation Using a Temporal Convolutional Network Model of Gradient Trajectory Errors", "authors": ["Jonathan B. Martin", "Hannah E. Alderson", "John C. Gore", "Mark D. Does", "Kevin D. Harkins"], "summary": "Summary: Errors in gradient trajectories introduce significant artifacts and\ndistortions in magnetic resonance images, particularly in non-Cartesian imaging\nsequences, where imperfect gradient waveforms can greatly reduce image quality.\nPurpose: Our objective is to develop a general, nonlinear gradient system model\nthat can accurately predict gradient distortions using convolutional networks.\nMethods: A set of training gradient waveforms were measured on a small animal\nimaging system, and used to train a temporal convolutional network to predict\nthe gradient waveforms produced by the imaging system. Results: The trained\nnetwork was able to accurately predict nonlinear distortions produced by the\ngradient system. Network prediction of gradient waveforms was incorporated into\nthe image reconstruction pipeline and provided improvements in image quality\nand diffusion parameter mapping compared to both the nominal gradient waveform\nand the gradient impulse response function. Conclusion: Temporal convolutional\nnetworks can more accurately model gradient system behavior than existing\nlinear methods and may be used to retrospectively correct gradient errors.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.14995v1", "AI": {"title_translation": "使用时间卷积网络模型梯度轨迹误差改进图像重建和扩散参数估计", "tldr": "本文提出了一种时间卷积网络模型，用于精确预测梯度系统失真，从而改进磁共振图像重建和扩散参数估计。", "motivation": "梯度轨迹误差会在磁共振图像中引入显著伪影和失真，尤其是在非笛卡尔成像序列中。本研究旨在开发一个通用的非线性梯度系统模型，能够使用卷积网络准确预测梯度失真。", "method": "在一小型动物成像系统上测量了一组训练梯度波形，并用于训练一个时间卷积网络，以预测成像系统产生的梯度波形。训练后的网络预测的梯度波形被整合到图像重建流程中。", "result": "训练后的网络能够准确预测梯度系统产生的非线性失真。与名义梯度波形和梯度脉冲响应函数相比，网络预测的梯度波形在图像质量和扩散参数映射方面提供了改进。", "conclusion": "时间卷积网络可以比现有线性方法更准确地建模梯度系统行为，并可用于回顾性校正梯度误差。", "translation": "摘要：梯度轨迹误差会在磁共振图像中引入显著的伪影和失真，特别是在非笛卡尔成像序列中，不完美的梯度波形会大大降低图像质量。目的：我们的目标是开发一个通用的非线性梯度系统模型，能够使用卷积网络准确预测梯度失真。方法：在一小型动物成像系统上测量了一组训练梯度波形，并用于训练一个时间卷积网络，以预测成像系统产生的梯度波形。结果：训练后的网络能够准确预测梯度系统产生的非线性失真。与名义梯度波形和梯度脉冲响应函数相比，网络预测的梯度波形被整合到图像重建流程中，并在图像质量和扩散参数映射方面提供了改进。结论：时间卷积网络可以比现有线性方法更准确地建模梯度系统行为，并可用于回顾性校正梯度误差。", "summary": "本研究提出了一种基于时间卷积网络（TCN）的通用非线性梯度系统模型，旨在准确预测磁共振成像中的梯度轨迹误差引起的失真。通过在小型动物成像系统上训练TCN，该模型能够精确预测梯度波形，并将其整合到图像重建流程中。结果显示，与传统方法相比，该TCN模型显著提升了图像质量和扩散参数映射的准确性，证明了TCN在纠正梯度误差方面的潜力。", "keywords": "时间卷积网络, 梯度轨迹误差, 图像重建, 扩散参数估计, 磁共振成像", "comments": "该论文的创新点在于将时间卷积网络应用于磁共振成像中的梯度轨迹误差校正，这是一种新颖的非线性建模方法。其重要性在于能够显著提高非笛卡尔成像序列的图像质量和扩散参数估计的准确性，对临床和科研应用具有潜在价值。相较于传统线性方法，TCN在建模复杂非线性行为方面表现出优越性。"}}
{"id": "2506.15448", "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning", "authors": ["Guoguo Ai", "Hezhe Qiao", "Hui Yan", "Guansong Pang"], "summary": "Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.", "comment": "18 pages, 11 figures, 3 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15448v1", "AI": {"title_translation": "半监督图异常检测通过鲁棒同质性学习", "tldr": "提出RHO，一个半监督图异常检测新方法，通过自适应学习多样化的同质性模式，显著优于现有SOTA方法。", "motivation": "现有半监督图异常检测方法假设正常节点同质性水平相似且标记正常节点能很好代表正常类的同质性模式，但在真实世界数据集中，正常节点可能表现出多样化的同质性，导致这些假设不成立。", "method": "提出RHO（Robust Homophily Learning），包含两个模块：自适应频率响应滤波器（AdaFreq）和图正常性对齐（GNA）。AdaFreq学习一组自适应谱滤波器，从节点属性的通道维度和跨通道维度捕获标记正常节点在不同同质性下的不同频率分量。GNA强制通道维度和跨通道同质性表示之间的一致性，以增强滤波器学习到的正常性的鲁棒性。", "result": "在八个真实世界GAD数据集上的实验表明，RHO能有效学习小正常节点集中多样化、常被低估的同质性，并显著优于最先进的竞争方法。", "conclusion": "RHO通过自适应学习多样化的同质性模式，有效解决了现有半监督图异常检测方法的局限性，并在真实世界数据集上取得了优异性能。", "translation": "半监督图异常检测（GAD）利用一小部分标记的正常节点，从图中大量未标记节点中识别异常节点。当前该领域的方法假设：1）正常节点具有相似的同质性水平；2）标记的正常节点能很好地代表正常类中的同质性模式。然而，由于图中的正常节点在真实世界的GAD数据集中可能表现出多样化的同质性，这一假设往往不成立。在本文中，我们提出了RHO，即鲁棒同质性学习，以自适应地学习这种同质性模式。RHO由两个新颖的模块组成：自适应频率响应滤波器（AdaFreq）和图正常性对齐（GNA）。AdaFreq学习一组自适应谱滤波器，这些滤波器从节点属性的通道维度和跨通道维度捕获具有不同同质性的标记正常节点的不同频率分量。引入GNA是为了强制通道维度和跨通道同质性表示之间的一致性，以增强滤波器在两个视图中学到的正常性的鲁棒性。在八个真实世界GAD数据集上的实验表明，RHO能有效学习小正常节点集中多样化、通常被低估的同质性，并显著优于最先进的竞争方法。代码可在https://github.com/mala-lab/RHO 获取。", "summary": "本文提出了RHO（Robust Homophily Learning），一种用于半监督图异常检测的新方法，旨在解决现有方法在处理正常节点多样化同质性时的局限性。RHO包含自适应频率响应滤波器（AdaFreq）和图正常性对齐（GNA）两个核心模块，通过自适应学习和增强不同视角下正常节点的同质性表示来提高异常检测的鲁棒性。实验证明，RHO在多个真实世界数据集上显著优于现有先进方法。", "keywords": "半监督图异常检测, 同质性学习, 鲁棒性, 异常检测, 图神经网络", "comments": "本文的创新点在于认识到真实世界图中正常节点同质性的多样性，并提出了一种自适应学习和鲁棒化这种同质性模式的方法。AdaFreq和GNA模块的设计巧妙地解决了这一挑战，显著提升了半监督图异常检测的性能。其重要性在于为处理复杂图数据中的异常检测提供了一个更实际、更有效的新范式。"}}
{"id": "2506.15452", "title": "Warping and Matching Subsequences Between Time Series", "authors": ["Simiao Lin", "Wannes Meert", "Pieter Robberechts", "Hendrik Blockeel"], "summary": "Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15452v1", "AI": {"title_translation": "时间序列之间子序列的扭曲与匹配", "tldr": "提出了一种新的技术，通过简化扭曲路径来突出、量化和可视化关键变换（如位移、压缩、振幅差异），以增强时间序列比较的可解释性。", "motivation": "现有的弹性距离度量虽然能提供鲁棒的定量比较，但缺乏定性比较。传统的可视化方法侧重于点对点对齐，未能传达子序列层面的更广泛结构关系，导致难以理解时间序列如何以及在哪里发生偏移、加速或减速。", "method": "提出了一种新颖的技术，通过简化扭曲路径来突出、量化和可视化关键变换（位移、压缩、振幅差异）。", "result": "通过提供时间序列间子序列匹配的更清晰表示，增强了时间序列比较的可解释性。", "conclusion": "该方法通过简化扭曲路径并突出关键变换，显著提高了时间序列比较的定性理解和可解释性。", "translation": "比较时间序列在聚类和分类等各种任务中至关重要。虽然允许扭曲的弹性距离度量提供了鲁棒的定量比较，但缺乏基于它们的定性比较。传统的可视化方法侧重于点对点对齐，未能传达子序列层面的更广泛结构关系。这种局限性使得难以理解一个时间序列相对于另一个时间序列如何以及在哪里发生偏移、加速或减速。为了解决这个问题，我们提出了一种新颖的技术，该技术简化了扭曲路径，以突出、量化和可视化关键变换（位移、压缩、振幅差异）。通过提供时间序列之间子序列如何匹配的更清晰表示，我们的方法增强了时间序列比较中的可解释性。", "summary": "本文提出了一种新颖的时间序列比较技术，旨在解决现有弹性距离度量在定性比较和传统可视化在结构关系表示上的不足。该方法通过简化扭曲路径，能够突出、量化并可视化时间序列间的关键变换，如位移、压缩和振幅差异。这使得子序列间的匹配关系更加清晰，从而显著增强了时间序列比较的解释性。", "keywords": "时间序列, 扭曲, 子序列, 比较, 可解释性", "comments": "该论文的创新点在于其提出了一种简化扭曲路径的方法，从而能够直观地可视化并量化时间序列之间的关键变换。这解决了现有方法在提供时间序列间定性比较和更深层次结构理解方面的不足，对于提高时间序列分析的可解释性具有重要意义。"}}
{"id": "2506.15479", "title": "Creating User-steerable Projections with Interactive Semantic Mapping", "authors": ["Artur André Oliveira", "Mateus Espadoto", "Roberto Hirata Jr.", "Roberto M. Cesar Jr.", "Alex C. Telea"], "summary": "Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15479v1", "AI": {"title_translation": "创建用户可控的交互式语义映射投影", "tldr": "该论文提出一个用户引导的投影框架，利用多模态大语言模型和自然语言提示，实现可定制且可解释的数据可视化，将降维技术转化为交互式用户驱动过程。", "motivation": "当前降维技术无法探索非变量或类别标签形式的语义结构，且缺乏用户可控性。", "method": "引入了一个新颖的用户引导投影框架，用于图像和文本数据。该框架通过多模态大语言模型（MLLMs）的零样本分类，结合自然语言引导提示，使用户能够动态地操纵投影以指定数据维度中未明确存在的高级语义关系。", "result": "在多个数据集上评估了该方法，结果显示它不仅增强了聚类分离，还将降维技术转变为交互式、用户驱动的过程。", "conclusion": "该方法弥合了全自动化降维技术与以人为中心的数据探索之间的鸿沟，为根据特定分析需求定制投影提供了灵活自适应的方式。", "translation": "降维（DR）技术将高维数据映射到低维空间。然而，当前的降维技术并非旨在探索不以变量或类别标签形式直接提供的语义结构。我们引入了一种新颖的用户引导投影框架，用于图像和文本数据，通过多模态大语言模型（MLLMs）的零样本分类，实现可定制、可解释的数据可视化。我们使用户能够通过自然语言引导提示动态地操纵投影，以指定用户感兴趣的、数据维度中未明确存在的高级语义关系。我们在多个数据集上评估了我们的方法，结果表明它不仅增强了聚类分离，还将降维技术转变为一个交互式、用户驱动的过程。我们的方法弥合了全自动化降维技术与以人为中心的数据探索之间的鸿沟，为根据特定分析需求定制投影提供了一种灵活自适应的方式。", "summary": "本论文提出一个新颖的用户引导投影框架，利用多模态大语言模型和自然语言提示，使用户能够动态地操纵图像和文本数据的降维投影，以探索数据中未明确存在的语义结构。该方法通过零样本分类实现可定制和可解释的可视化，并在实验中证明能增强聚类分离，将降维过程转化为交互式和用户驱动的探索，从而弥合了自动化降维与人机交互数据探索之间的差距。", "keywords": "降维, 用户引导, 语义映射, 多模态大语言模型, 数据可视化", "comments": "该论文的创新点在于将多模态大语言模型与降维技术结合，实现了用户通过自然语言对数据投影进行语义层面的引导和控制，极大地提升了数据可视化的交互性和可解释性。这在弥合自动化工具与人类直觉需求之间提供了一个重要的新方向，对于需要深入探索复杂数据语义的用户来说具有重要意义。"}}
{"id": "2506.15538", "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework", "authors": ["Laura Kopf", "Nils Feldhus", "Kirill Bykov", "Philine Lou Bommer", "Anna Hedström", "Marina M. -C. Höhne", "Oliver Eberle"], "summary": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15538v1", "AI": {"title_translation": "使用PRISM捕捉多义性：一种多概念特征描述框架", "tldr": "PRISM是一个新的框架，通过提供更细致的描述来准确捕捉神经网络特征的多义性，解决了现有方法假设神经元单义性的局限性。", "motivation": "现有特征描述方法面临鲁棒性有限和假设每个神经元只编码一个单一概念（单义性）的挑战，尽管有越来越多的证据表明神经元通常是多义的。这种假设限制了特征描述的表达能力，并限制了它们捕捉模型内部编码的全部行为的能力。", "method": "我们引入了多义性特征识别和评分方法（PRISM），这是一个新颖的框架，能够捕捉神经网络特征固有的复杂性。与先前为每个特征分配单一描述的方法不同，PRISM为多义和单义特征提供更细致的描述。", "result": "我们将PRISM应用于语言模型，并通过与现有方法的广泛基准测试，证明我们的方法能生成更准确和忠实的特征描述，提高了整体描述质量（通过描述分数）和在存在多义性时捕捉不同概念的能力（通过多义性分数）。", "conclusion": "PRISM通过提供更准确和忠实的特征描述，并有效捕捉多义性，显著提升了神经网络特征的解释能力。", "translation": "自动化可解释性研究旨在识别神经网络特征中编码的概念，以增强人类对模型行为的理解。当前的特征描述方法面临两个关键挑战：鲁棒性有限和每个神经元只编码一个单一概念（单义性）的错误假设，尽管越来越多的证据表明神经元通常是多义的。这种假设限制了特征描述的表达能力，并限制了它们捕捉模型内部编码的全部行为的能力。为了解决这个问题，我们引入了多义性特征识别和评分方法（PRISM），这是一个新颖的框架，能够捕捉神经网络特征固有的复杂性。与先前为每个特征分配单一描述的方法不同，PRISM为多义和单义特征提供更细致的描述。我们将PRISM应用于语言模型，并通过与现有方法的广泛基准测试，证明我们的方法能生成更准确和忠实的特征描述，提高了整体描述质量（通过描述分数）和在存在多义性时捕捉不同概念的能力（通过多义性分数）。", "summary": "该论文提出了PRISM（多义性特征识别和评分方法），一个旨在解决现有神经网络特征描述方法中鲁棒性不足和单义性假设问题的框架。PRISM能够为神经网络中的多义和单义特征提供更细致和准确的描述。通过在语言模型上的应用和基准测试，研究表明PRISM在描述质量和捕捉多义性方面优于现有方法，从而提升了模型行为的可解释性。", "keywords": "多义性, 神经网络可解释性, 特征描述, PRISM, 语言模型", "comments": "PRISM的创新之处在于它明确地解决了神经网络特征的“多义性”问题，打破了传统上对神经元“单义性”的错误假设。这对于更准确、更全面地理解复杂模型（特别是语言模型）的内部工作原理至关重要，是可解释性研究领域的一个重要进展。"}}
{"id": "2506.14803", "title": "Omnidirectional Video Super-Resolution using Deep Learning", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter W. Eklund", "Sunil Aryal"], "summary": "Omnidirectional Videos (or 360{\\deg} videos) are widely used in Virtual\nReality (VR) to facilitate immersive and interactive viewing experiences.\nHowever, the limited spatial resolution in 360{\\deg} videos does not allow for\neach degree of view to be represented with adequate pixels, limiting the visual\nquality offered in the immersive experience. Deep learning Video\nSuper-Resolution (VSR) techniques used for conventional videos could provide a\npromising software-based solution; however, these techniques do not tackle the\ndistortion present in equirectangular projections of 360{\\deg} video signals.\nAn additional obstacle is the limited availability of 360{\\deg} video datasets\nfor study. To address these issues, this paper creates a novel 360{\\deg} Video\nDataset (360VDS) with a study of the extensibility of conventional VSR models\nto 360{\\deg} videos. This paper further proposes a novel deep learning model\nfor 360{\\deg} Video Super-Resolution (360{\\deg} VSR), called Spherical Signal\nSuper-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent\nmodelling with an attention mechanism, unbound from conventional VSR techniques\nlike alignment. With a purpose-built feature extractor and a novel loss\nfunction addressing spherical distortion, S3PO outperforms most\nstate-of-the-art conventional VSR models and 360{\\deg}~specific\nsuper-resolution models on 360{\\deg} video datasets. A step-wise ablation study\nis presented to understand and demonstrate the impact of the chosen\narchitectural sub-components, targeted training and optimisation.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.14803v1", "AI": {"title_translation": "基于深度学习的全向视频超分辨率", "tldr": "本文针对全向（360度）视频分辨率低且现有超分辨率技术不适用于其独特畸变的问题，提出了一个新的360度视频数据集和一种名为S3PO的深度学习模型，该模型通过专门的特征提取器和损失函数有效提升了360度视频的视觉质量。", "motivation": "全向（360度）视频在虚拟现实（VR）中广泛使用，但其空间分辨率有限，导致沉浸式体验的视觉质量不足。传统的视频超分辨率（VSR）技术无法处理360度视频等距柱状投影中存在的畸变。此外，用于研究的360度视频数据集也十分有限。", "method": "本文创建了一个新颖的360度视频数据集（360VDS），并研究了传统VSR模型对360度视频的扩展性。在此基础上，提出了一种名为“比例优化球面信号超分辨率”（S3PO）的新型深度学习模型，用于360度视频超分辨率。S3PO采用带有注意力机制的循环建模，不依赖于传统的对齐技术，并配备了专门构建的特征提取器和解决球面畸变的新型损失函数。", "result": "S3PO模型在360度视频数据集上表现优于大多数最先进的传统VSR模型和360度专用超分辨率模型。文章还进行了逐步的消融研究，以理解和证明所选架构子组件、目标训练和优化的影响。", "conclusion": "本文提出的S3PO模型通过创新的架构和损失函数，有效解决了360度视频超分辨率中的畸变问题，显著提升了360度视频的视觉质量，并超越了现有主流方法。", "translation": "全向视频（或360度视频）在虚拟现实（VR）中被广泛用于提供沉浸式和交互式的观看体验。然而，360度视频有限的空间分辨率导致每个视角无法用足够的像素表示，从而限制了沉浸式体验所提供的视觉质量。用于传统视频的深度学习视频超分辨率（VSR）技术可以提供一种有前景的基于软件的解决方案；然而，这些技术未能解决360度视频信号等距柱状投影中存在的畸变。另一个障碍是用于研究的360度视频数据集的可用性有限。为了解决这些问题，本文创建了一个新颖的360度视频数据集（360VDS），并研究了传统VSR模型对360度视频的扩展性。本文进一步提出了一种用于360度视频超分辨率（360度VSR）的新型深度学习模型，名为“比例优化球面信号超分辨率”（S3PO）。S3PO采用带有注意力机制的循环建模，不受传统VSR技术（如对齐）的限制。通过专门构建的特征提取器和解决球面畸变的新型损失函数，S3PO在360度视频数据集上表现优于大多数最先进的传统VSR模型和360度专用超分辨率模型。本文还进行了逐步的消融研究，以理解和证明所选架构子组件、目标训练和优化的影响。", "summary": "本文针对360度视频在VR应用中存在的低分辨率和等距柱状投影畸变问题，以及数据集缺乏的挑战，提出了创新的解决方案。研究团队首先构建了一个新的360度视频数据集（360VDS），并在此基础上，开发了一种名为S3PO的深度学习模型用于360度视频超分辨率。S3PO模型通过结合循环建模、注意力机制、专门的特征提取器和针对球面畸变设计的损失函数，有效克服了传统VSR方法的局限性，并在性能上超越了现有先进的传统和360度专用超分辨率模型。文章还通过消融研究验证了模型各组件的有效性。", "keywords": "全向视频, 360度视频, 超分辨率, 深度学习, S3PO", "comments": "本文的创新点在于针对360度视频的独特属性（如等距柱状投影畸变）提出了专用的超分辨率解决方案，而不是简单地应用传统VSR技术。通过创建新的数据集和设计具有针对性损失函数的S3PO模型，该研究有效填补了360度视频高质量处理领域的空白，对VR和沉浸式媒体的发展具有重要意义。其提出的模型和数据集有望成为未来研究的基础。"}}
{"id": "2506.15492", "title": "LIT-LVM: Structured Regularization for Interaction Terms in Linear Predictors using Latent Variable Models", "authors": ["Mohammadreza Nemati", "Zhipeng Huang", "Kevin S. Xu"], "summary": "Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15492v1", "AI": {"title_translation": "LIT-LVM：使用潜在变量模型对线性预测器中交互项的结构化正则化", "tldr": "LIT-LVM是一种利用低维潜在变量模型对线性预测器中的交互项进行结构化正则化的方法，旨在提高高维设置下的预测精度并缓解过拟合，尤其在交互项数量远超样本数时表现更优。", "motivation": "在线性预测器中，精确估计交互项的系数是一个挑战，特别是在高维设置下容易出现过拟合。本研究旨在通过引入一种结构化正则化方法来解决这个问题，以提高预测精度并缓解过拟合。", "method": "本研究提出LIT-LVM方法，该方法假设不同交互项的系数具有近似的低维结构，并通过在低维空间中用潜在向量表示每个特征来实现。这种低维表示被视为一种结构化正则化方法，能进一步缓解高维设置下的过拟合。", "result": "LIT-LVM在各种模拟和真实数据上，特别是当交互项数量远高于样本数量时，与elastic net和因子分解机相比，实现了卓越的预测精度。LIT-LVM还为特征提供了低维潜在表示，这对于可视化和分析它们之间的关系非常有用。", "conclusion": "LIT-LVM通过对线性预测器中的交互项进行结构化正则化，在高维设置下显著提高了预测精度并缓解了过拟合，同时提供了有用的特征低维表示。", "translation": "统计学和机器学习中一些最简单但最常用的预测器使用特征的加权线性组合。此类线性预测器可以通过添加对应于所有特征对乘积的交互项来建模特征之间的非线性关系。我们考虑了准确估计线性预测器中交互项系数的问题。我们假设不同交互项的系数具有近似的低维结构，并通过在低维空间中用潜在向量表示每个特征。这种低维表示可以被视为一种结构化正则化方法，在高维设置下，除了lasso和elastic net等标准正则化器之外，还能进一步缓解过拟合。我们证明了我们的方法，称为LIT-LVM，在各种模拟和真实数据上，与elastic net和因子分解机相比，实现了卓越的预测精度，特别是当交互项的数量相对于样本数量较高时。LIT-LVM还为特征提供了低维潜在表示，这对于可视化和分析它们之间的关系非常有用。", "summary": "LIT-LVM是一种用于线性预测器中交互项的结构化正则化方法，它通过将每个特征表示为低维空间中的潜在向量来解决高维设置下的过拟合问题。该方法假设交互项系数具有低维结构，并被证明在预测精度上优于现有的正则化方法（如elastic net和因子分解机），尤其是在交互项数量远超样本数的情况下。此外，LIT-LVM还提供了有益的特征低维表示，有助于数据可视化和关系分析。", "keywords": "结构化正则化, 交互项, 线性预测器, 潜在变量模型, 过拟合", "comments": "LIT-LVM的创新之处在于其将潜在变量模型引入到线性预测器的交互项正则化中，这是一种新颖的结构化正则化方法。它有效地解决了高维数据中交互项系数估计的过拟合问题，并提供了可解释的特征低维表示，这对于理解特征关系具有重要意义。该方法在实际应用中，特别是在处理大量交互项时，有望显著提高模型的预测性能和可解释性。"}}
{"id": "2506.15606", "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning", "authors": ["Gabrel J. Perin", "Runjin Chen", "Xuxi Chen", "Nina S. T. Hirata", "Zhangyang Wang", "Junyuan Hong"], "summary": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15606v1", "AI": {"title_translation": "LoX：低秩外推增强大型语言模型对抗微调的安全性", "tldr": "LoX提出了一种无训练的低秩外推方法，通过外推安全子空间来增强LLM的安全性，使其在面对良性或恶意微调时更具鲁棒性。", "motivation": "大型语言模型（LLMs）在实际应用中日益普及，但其安全性令人担忧，尤其是在回应有害问题方面。尽管通过对齐努力提升了模型安全性，但后续的微调（即使数据看似无害）仍可能破坏其安全保护。本文实证表明，这种脆弱性源于LLM参数中安全关键的低秩子空间对微调的敏感性。", "method": "本文提出了一种名为低秩外推（LoX）的新型无训练方法，通过外推对齐LLM的安全子空间来增强其安全鲁棒性。", "result": "实验结果证实了LoX的有效性，它显著提升了模型对抗良性微调和恶意微调攻击的鲁棒性，同时保留了模型适应新任务的能力。例如，LoX使良性或恶意微调攻击的攻击成功率（ASR）绝对降低了11%至54%。通过研究参数的ASR图景，作者将LoX的成功归因于外推将LLM参数移动到一个更平坦的区域，从而对扰动不那么敏感。", "conclusion": "LoX通过将LLM参数移动到更平坦的区域来增强其安全性，使其对微调扰动不那么敏感，从而显著降低了攻击成功率，提升了模型对良性及恶意微调攻击的鲁棒性。", "translation": "大型语言模型（LLMs）在现实世界应用中已变得不可或缺。然而，它们的广泛采用引发了重大的安全担忧，尤其是在回应社会有害问题方面。尽管为通过对齐改进模型安全性付出了巨大努力，但对齐的模型仍然可能在后续微调后其安全保护被破坏——即使额外的训练数据看起来是良性的。在本文中，我们通过实证证明，这种脆弱性源于LLM参数中安全关键的低秩子空间对微调的敏感性。基于这一洞察，我们提出了一种新颖的无训练方法，称为低秩外推（LoX），通过外推对齐LLM的安全子空间来增强安全鲁棒性。我们的实验结果证实了LoX的有效性，展示了在对抗良性微调和恶意微调攻击时鲁棒性的显著改进，同时保留了模型适应新任务的能力。例如，LoX使面对良性或恶意微调攻击的攻击成功率（ASR）绝对降低了11%至54%。通过研究参数的ASR图景，我们将LoX的成功归因于外推将LLM参数移动到一个更平坦的区域，从而对扰动不那么敏感。代码可在github.com/VITA-Group/LoX获取。", "summary": "本文针对大型语言模型（LLMs）在微调后易受安全攻击的问题，提出了一种名为低秩外推（LoX）的无训练方法。研究发现LLM参数中安全关键的低秩子空间对微调敏感是导致安全漏洞的原因。LoX通过外推对齐LLM的安全子空间来增强模型的鲁棒性。实验证明，LoX能有效降低攻击成功率（ASR），在对抗良性或恶意微调攻击时，ASR绝对降低11%至54%，同时保持模型适应新任务的能力。LoX的成功在于它将LLM参数移动到对扰动不那么敏感的更平坦区域。", "keywords": "大型语言模型, 安全性, 微调, 低秩外推, 鲁棒性", "comments": "LoX方法通过洞察LLM参数中安全关键的低秩子空间对微调的敏感性，提出了一种创新性的无训练解决方案。其通过外推安全子空间来增强模型鲁棒性的思路具有新颖性，且实验结果表明其在降低攻击成功率方面表现出色。特别值得注意的是，该方法在提高安全性的同时，还能保持模型对新任务的适应性，这在实际应用中非常重要。将成功归因于“更平坦的区域”也提供了对其机制的合理解释。"}}
{"id": "2506.15499", "title": "Pixel-level Certified Explanations via Randomized Smoothing", "authors": ["Alaa Anani", "Tobias Lorenz", "Mario Fritz", "Bernt Schiele"], "summary": "Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15499v1", "AI": {"title_translation": "像素级认证解释通过随机平滑", "tldr": "提出首个通过随机平滑保证像素级归因方法鲁棒性的认证框架，解决现有归因解释非鲁棒性问题。", "motivation": "现有事后归因方法（解释深度学习预测）的解释高度非鲁棒，微小输入扰动可显著改变归因图，同时保持相同预测，这损害了其可信度，需要像素级归因分数的严格鲁棒性保证。", "method": "引入首个认证框架，利用随机平滑为任何黑盒归因方法提供像素级鲁棒性保证。通过稀疏化和平滑归因图，将任务重新表述为分割问题，并针对$\\ell_2$-有界扰动认证每个像素的重要性。提出三个评估指标来评估认证鲁棒性、定位和忠实度。", "result": "对5个ImageNet模型上的12种归因方法进行广泛评估，结果显示其认证归因是鲁棒的、可解释的、忠实的，可以在下游任务中可靠使用。", "conclusion": "该研究成功引入了首个像素级认证解释框架，并通过实验证明了其方法的有效性、鲁棒性、可解释性和忠实度，从而提高了深度学习模型解释的可信度。", "translation": "事后归因方法旨在通过突出有影响力的输入像素来解释深度学习预测。然而，这些解释高度非鲁棒：微小、难以察觉的输入扰动可以显著改变归因图，同时保持相同的预测。这种脆弱性削弱了它们的可信度，并要求对像素级归因分数提供严格的鲁棒性保证。我们引入了第一个认证框架，该框架利用随机平滑为任何黑盒归因方法提供像素级鲁棒性保证。通过稀疏化和平滑归因图，我们将任务重新表述为分割问题，并针对 $\\ell_2$-有界扰动认证每个像素的重要性。我们进一步提出了三个评估指标来评估认证鲁棒性、定位和忠实度。对5个ImageNet模型上的12种归因方法进行广泛评估，结果显示我们的认证归因是鲁棒的、可解释的、忠实的，可以在下游任务中可靠使用。我们的代码位于 https://github.com/AlaaAnani/certified-attributions。", "summary": "本文针对现有深度学习模型事后归因解释非鲁棒性问题，提出了首个像素级认证框架。该框架利用随机平滑技术，将归因任务重构为分割问题，并能对任何黑盒归因方法提供像素级鲁棒性保证。通过对多种归因方法和ImageNet模型的广泛评估，证明了其认证归因的鲁棒性、可解释性和忠实度，使其能够可靠地应用于下游任务。", "keywords": "像素级解释, 随机平滑, 认证鲁棒性, 归因方法, 深度学习解释", "comments": "这篇论文通过引入随机平滑技术来解决深度学习模型归因解释的鲁棒性问题，具有重要的创新性。它首次为像素级归因提供了严格的认证框架，显著提高了归因解释的可信度和实用性，对于确保AI系统在关键应用中的透明度和可靠性具有重要意义。"}}
{"id": "2506.15651", "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning", "authors": ["Tevin Wang", "Chenyan Xiong"], "summary": "Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15651v1", "AI": {"title_translation": "AutoRule：推理思维链提取的基于规则的奖励改进偏好学习", "tldr": "AutoRule是一个全自动方法，它从偏好反馈中提取规则并将其转化为基于规则的奖励，显著提升了从人类反馈中强化学习（RLHF）的性能，并减少了奖励作弊。", "motivation": "当前基于规则的奖励方法在改进从人类反馈中强化学习（RLHF）时，通常依赖于手动规则工程，这限制了其应用和效率。", "method": "AutoRule方法分三个阶段提取规则：首先，利用推理模型解释用户偏好；其次，从这些解释的推理链中识别候选规则；最后，将它们合成为一个统一的规则集。利用最终确定的规则集，我们采用语言模型验证器计算每个输出满足规则的比例，并将此指标作为辅助奖励，与学习到的奖励模型一同用于策略优化。", "result": "使用AutoRule训练Llama-3-8B模型，在AlpacaEval2.0上，长度控制的胜率相对提高了28.6%；在MT-Bench的保留子集上，第二轮表现相对提高了6.1%。分析证实，提取的规则与数据集偏好具有良好的一致性。与学习到的奖励模型相比，AutoRule在运行两集后显示出更低的奖励作弊。案例研究表明，提取的规则捕捉了不同数据集中所重视的独特品质。", "conclusion": "AutoRule成功地实现了从偏好反馈中自动提取规则并将其应用于基于规则的奖励，有效提升了RLHF的性能，减少了奖励作弊，并证明了自动提取规则的价值。", "translation": "基于规则的奖励为改进从人类反馈中强化学习（RLHF）提供了一种有前景的策略，但当前方法通常依赖于手动规则工程。我们提出了AutoRule，一个全自动方法，用于从偏好反馈中提取规则并将其转化为基于规则的奖励。AutoRule的提取分三个阶段进行：它利用推理模型解释用户偏好，从这些解释的推理链中识别候选规则，并将它们合成为一个统一的规则集。利用最终确定的规则集，我们采用语言模型验证器计算每个输出满足规则的比例，并将此指标作为辅助奖励，与学习到的奖励模型一同用于策略优化。使用AutoRule训练Llama-3-8B模型，与使用相同学习奖励模型但没有基于规则的辅助奖励的GRPO基线相比，在AlpacaEval2.0上，长度控制的胜率相对提高了28.6%，在MT-Bench的保留子集上，第二轮表现相对提高了6.1%。我们的分析证实，提取的规则与数据集偏好具有良好的一致性。我们发现，与学习到的奖励模型相比，AutoRule在运行两集后显示出更低的奖励作弊。最后，我们的案例研究表明，提取的规则捕捉了不同数据集中所重视的独特品质。提取的规则附在附录中，代码已在https://github.com/cxcscmu/AutoRule开源。", "summary": "AutoRule是一种创新的自动化方法，旨在从人类偏好反馈中提取规则，并将其转化为基于规则的辅助奖励，以改进从人类反馈中强化学习（RLHF）。该方法通过推理模型解释用户偏好，从推理链中识别并合成规则，然后使用语言模型验证器计算规则满足度作为辅助奖励。实验结果表明，AutoRule显著提升了Llama-3-8B模型在AlpacaEval2.0和MT-Bench上的性能，并有效减少了奖励作弊，证明了其在自动化规则提取和RLHF应用中的有效性。", "keywords": "基于规则的奖励, 强化学习, 人类反馈, 自动化规则提取, 偏好学习", "comments": "AutoRule的创新之处在于其全自动的规则提取流程，摆脱了传统手动规则工程的限制，极大地提高了RLHF的效率和可扩展性。通过利用推理模型的思维链来识别和合成规则，该方法能够捕捉到人类偏好中独特的、细微的品质。其在减少奖励作弊方面的表现也凸显了其在构建更鲁棒、更符合人类意图的AI系统方面的重要性。该研究为未来RLHF领域自动化和解释性规则的应用提供了新的方向。"}}
{"id": "2506.15506", "title": "Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review", "authors": ["Salijona Dyrmishi", "Mohamed Djilani", "Thibault Simonetto", "Salah Ghamizi", "Maxime Cordy"], "summary": "Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.", "comment": "This paper is currently under review at ACM Computing Surveys", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15506v1", "AI": {"title_translation": "通过系统文献综述深入了解表格机器学习的对抗性攻击", "tldr": "本文首次对针对表格机器学习模型的对抗性攻击进行了系统文献综述，总结了关键趋势、攻击策略、实际考虑因素，并指出了挑战和开放问题，旨在指导未来研究。", "motivation": "尽管计算机视觉和自然语言处理领域的对抗性攻击研究已得到广泛综述，但针对表格数据的研究仍然分散，缺乏系统性整理。", "method": "本文采用系统文献综述的方法，对针对表格机器学习模型的对抗性攻击进行了全面的回顾和分析。", "result": "该综述突出了关键趋势，对攻击策略进行了分类，分析了它们如何解决实际应用中的考虑因素，并概述了当前的挑战和开放研究问题。", "conclusion": "通过提供清晰和结构化的概述，本综述旨在指导未来理解和解决表格机器学习中对抗性漏洞的工作。", "translation": "机器学习中的对抗性攻击已在计算机视觉和自然语言处理等领域得到广泛综述，但对表格数据的研究仍然分散。本文首次对针对表格机器学习模型的对抗性攻击进行了系统文献综述。我们强调了关键趋势，对攻击策略进行了分类，并分析了它们如何解决实际应用中的实际考虑因素。此外，我们还概述了当前的挑战和开放研究问题。通过提供清晰和结构化的概述，本综述旨在指导未来理解和解决表格机器学习中对抗性漏洞的工作。", "summary": "本文是首篇针对表格机器学习模型对抗性攻击的系统文献综述。它系统地总结了该领域的研究现状，包括关键趋势、攻击策略的分类、实际应用考量以及当前面临的挑战和未解决的研究问题，旨在为未来研究提供清晰的指导。", "keywords": "对抗性攻击, 表格机器学习, 系统文献综述, 漏洞, 鲁棒性", "comments": "本文的创新之处在于它是首个专门针对表格机器学习对抗性攻击的系统文献综述，填补了该领域系统性研究的空白。其重要性在于为研究人员提供了结构化的知识体系和未来研究方向，对于推动表格数据对抗性鲁棒性研究具有指导意义。"}}
{"id": "2506.15507", "title": "Over-squashing in Spatiotemporal Graph Neural Networks", "authors": ["Ivan Marisca", "Jacob Bamberger", "Cesare Alippi", "Michael M. Bronstein"], "summary": "Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15507v1", "AI": {"title_translation": "时空图神经网络中的过挤压问题", "tldr": "本文首次形式化了时空图神经网络（STGNNs）中的过挤压问题，证明了其与静态GNNs不同，并发现卷积STGNNs反直觉地倾向于传播时间上远而非近的信息，且不同的处理范式同样受影响，为更有效的设计提供了指导。", "motivation": "静态图神经网络（GNNs）中的“过挤压”问题（即远距离节点信息交换失效）已得到广泛研究，但在处理序列数据的时空图神经网络（STGNNs）中尚未被探索，且时间维度会加剧这一挑战。", "method": "本文形式化了时空过挤压问题，并与静态情况进行对比分析，揭示其独特特征。通过理论证明，分析了不同时空处理范式（时间-空间或时间-然后-空间）受此问题影响的程度。最后，在合成和真实世界数据集上验证了理论发现。", "result": "1. 形式化了时空过挤压问题，并展示了其与静态情况的独特特征。2. 发现卷积STGNNs反直觉地倾向于传播时间上远而非近的信息。3. 证明了遵循时间-空间或时间-然后-空间处理范式的架构同样受到过挤压现象的影响。4. 在合成和真实世界数据集上验证了这些发现，提供了深入见解。", "conclusion": "本文深入理解了时空图神经网络（STGNNs）的运作动态，并为设计更有效的STGNN模型提供了原则性指导。", "translation": "图神经网络（GNNs）在各个领域取得了显著成功。然而，最近的理论进展已经确定了其信息传播能力方面的根本局限性，例如过挤压（over-squashing），即远距离节点无法有效交换信息。虽然这个问题在静态环境中得到了广泛研究，但在处理与图节点相关序列的时空图神经网络（STGNNs）中，这个问题仍未被探索。尽管如此，时间维度通过增加必须传播的信息量而加剧了这一挑战。在这项工作中，我们形式化了时空过挤压问题，并展示了其与静态情况相比的独特特征。我们的分析揭示，反直觉的是，卷积STGNNs倾向于传播时间上远而非近的信息。此外，我们证明了遵循时间-空间或先时间后空间处理范式的架构同样受到这种现象的影响，这为计算效率高的实现提供了理论依据。我们在合成和真实世界数据集上验证了我们的发现，为它们的运行动态提供了更深入的见解，并为更有效的设计提供了原则性指导。", "summary": "本文首次形式化并深入探讨了时空图神经网络（STGNNs）中的“过挤压”问题，该问题在静态GNNs中已广受关注，但在STGNNs中因时间维度而更显复杂。研究揭示了时空过挤压的独特属性，并反直觉地发现卷积STGNNs更倾向于传播时间上远而非近的信息。此外，本文证明了不同的时空处理范式（时间-空间或时间-然后-空间）均受此现象影响，并提供了理论依据和实验验证，旨在为STGNNs的有效设计提供指导。", "keywords": "时空图神经网络, 过挤压, 信息传播, 卷积STGNN, 理论分析", "comments": "本文首次将“过挤压”这一在静态GNNs中存在的关键问题扩展到时空GNNs领域，填补了该领域的空白。其创新之处在于形式化了时空背景下的过挤压问题，并揭示了卷积STGNNs中反直觉的信息传播特性。研究结果对STGNNs的设计和优化具有重要指导意义，有助于开发更有效、信息传播更远的STGNN模型。"}}
{"id": "2506.15679", "title": "Dense SAE Latents Are Features, Not Bugs", "authors": ["Xiaoqing Sun", "Alessandro Stolfo", "Joshua Engels", "Ben Wu", "Senthooran Rajamanoharan", "Mrinmaya Sachan", "Max Tegmark"], "summary": "Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15679v1", "AI": {"title_translation": "密集型SAE潜在变量是特征，而非缺陷", "tldr": "本研究系统地调查了稀疏自编码器（SAE）中的密集型潜在变量，并证明它们并非训练过程中的无用产物，而是反映了语言模型中有意义的内部表征，并发挥着重要的功能作用。", "motivation": "稀疏自编码器（SAE）旨在通过强制稀疏性从语言模型中提取可解释的特征。然而，许多SAE的潜在变量表现出高激活频率（即“密集”），这引发了人们对其可能只是训练过程中的不良伪影的担忧。本研究的动机在于系统地调查这些密集型潜在变量的几何结构、功能和起源，以证明它们实际上是有意义的模型表示。", "method": "本研究系统地调查了密集型潜在变量的几何结构、功能和起源。首先，研究人员证明了密集型潜在变量倾向于形成对跖对，并重建残差流中的特定方向；通过消融其子空间，可以抑制重新训练的SAE中新密集特征的出现，这表明高密度特征是残差空间的内在属性。其次，研究人员引入了密集型潜在变量的分类法，识别了与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重建相关的类别。最后，研究人员分析了这些特征在不同层中的演变，揭示了从早期层的结构特征，到中间层的语义特征，再到最后层中面向输出的信号的转变。", "result": "研究发现，密集型潜在变量不仅持续存在，而且通常反映了有意义的模型表征。它们倾向于形成对跖对，并重建残差流中的特定方向。消融其子空间可以抑制重新训练的SAE中新密集特征的出现，这表明高密度特征是残差空间的内在属性。研究还识别了密集型潜在变量的多种类别，包括与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重建相关的特征。此外，分析揭示了这些特征在模型层级中的演变：早期层主要包含结构特征，中间层包含语义特征，而最后层则侧重于面向输出的信号。", "conclusion": "本研究的发现表明，密集型潜在变量在语言模型计算中发挥着功能性作用，不应被视为训练噪声而予以忽视。", "translation": "稀疏自编码器（SAE）旨在通过强制稀疏性从语言模型中提取可解释的特征。理想情况下，SAE的训练会产生既稀疏又具有语义意义的潜在变量。然而，许多SAE的潜在变量激活频繁（即“密集”），这引发了人们对其可能只是训练过程中不良伪影的担忧。在这项工作中，我们系统地调查了密集型潜在变量的几何结构、功能和起源，并表明它们不仅持续存在，而且往往反映了有意义的模型表示。我们首先证明，密集型潜在变量倾向于形成对跖对，并重建残差流中的特定方向，并且消融其子空间会抑制重新训练的SAE中新密集特征的出现——这表明高密度特征是残差空间的内在属性。然后，我们引入了密集型潜在变量的分类法，识别了与位置跟踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重建相关的类别。最后，我们分析了这些特征在不同层中的演变，揭示了从早期层的结构特征，到中间层的语义特征，再到模型最后层中面向输出的信号的转变。我们的发现表明，密集型潜在变量在语言模型计算中发挥着功能性作用，不应被视为训练噪声而予以忽视。", "summary": "本研究系统地探讨了稀疏自编码器（SAE）中看似“密集”的潜在变量，挑战了它们是训练伪影的普遍观念。研究发现，这些密集型潜在变量并非缺陷，而是语言模型中具有实际功能和意义的表征。通过分析其几何结构、功能和起源，论文揭示了它们形成对跖对并重建残差流中特定方向的特性，并通过消融实验证明其内在性。此外，研究构建了密集型潜在变量的分类法，并分析了它们在模型不同层级（从结构到语义再到输出导向）的演变。最终结论是，密集型潜在变量在语言模型计算中扮演着重要角色，不应被误认为是噪声。", "keywords": "稀疏自编码器, 密集型潜在变量, 语言模型, 可解释性, 特征学习", "comments": "这项研究对于理解稀疏自编码器（SAE）的内部工作机制及其在语言模型可解释性方面的应用具有重要意义。它挑战了关于“密集”特征是缺陷的传统观点，揭示了这些特征的潜在功能性和重要性，这对于未来SAE的设计和可解释AI的研究方向可能产生积极影响。识别出这些特征的分类及其在模型层级间的演变，为深入分析语言模型内部表征提供了新的视角和工具。"}}
{"id": "2506.15513", "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Luong Van Nghia"], "summary": "Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.", "comment": "11 pages, 7 figures, 5 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15513v1", "AI": {"title_translation": "RePCS：诊断LLM驱动的检索增强生成中的数据记忆", "tldr": "RePCS是一种轻量级、模型无关的诊断方法，通过比较两种推理路径的输出分布差异，检测LLM驱动的检索增强生成（RAG）系统中模型是否依赖记忆数据而非检索证据。", "motivation": "检索增强生成（RAG）中的大型语言模型（LLM）可能依赖记忆的训练数据，绕过检索到的证据，并产生受污染的输出，因此需要一种方法来诊断这种行为。", "method": "本文提出了检索路径污染评分（RePCS），这是一种诊断方法，通过计算 Kullback-Leibler（KL）散度来比较两条推理路径的输出分布：(i) 仅使用查询的参数路径，和 (ii) 使用查询和检索上下文的检索增强路径。低散度表示检索上下文影响最小，可能存在记忆化。该方法模型无关，无需梯度或内部状态访问，仅增加一次额外的前向传播。此外，还推导了将KL阈值与用户定义的假阳性率和假阴性率相关联的PAC式保证。", "result": "在Prompt-WNQA基准测试上，RePCS实现了0.918的ROC-AUC，比现有最强方法高出6.5个百分点，同时在NVIDIA T4 GPU上的延迟开销低于4.7%。", "conclusion": "RePCS提供了一种轻量级、黑盒的保障，用于验证RAG系统是否有效利用检索，使其在安全关键应用中特别有价值。", "translation": "检索增强生成（RAG）已成为使用当前外部信息更新大型语言模型（LLM）响应的常见策略。然而，模型可能仍然依赖记忆的训练数据，绕过检索到的证据，并产生受污染的输出。我们引入了检索路径污染评分（RePCS），这是一种诊断方法，无需模型访问或重新训练即可检测此类行为。RePCS通过计算其输出分布之间的Kullback-Leibler（KL）散度来比较两条推理路径：(i) 仅使用查询的参数路径，和 (ii) 使用查询和检索上下文的检索增强路径。低散度表明检索上下文影响最小，表明可能存在记忆化。此过程与模型无关，不需要梯度或内部状态访问，并且仅增加一次额外的前向传播。我们进一步推导了将KL阈值与用户定义的假阳性率和假阴性率相关联的PAC式保证。在Prompt-WNQA基准测试上，RePCS实现了0.918的ROC-AUC。此结果比现有最强方法高出6.5个百分点，同时在NVIDIA T4 GPU上的延迟开销低于4.7%。RePCS提供了一种轻量级的黑盒保障，用于验证RAG系统是否有效利用检索，使其在安全关键应用中特别有价值。", "summary": "本论文介绍了RePCS，一种诊断LLM驱动的检索增强生成（RAG）系统中数据记忆化行为的方法。RePCS通过比较仅使用查询的参数路径和使用检索上下文的检索增强路径的输出分布（使用KL散度）来检测模型是否有效利用了检索到的信息。如果散度较低，则表明模型可能依赖记忆数据。该方法无需模型访问或重训练，且模型无关。实验结果显示，RePCS在Prompt-WNQA基准测试上表现优异，ROC-AUC达到0.918，超过现有最佳方法，并保持低延迟。RePCS为RAG系统提供了一种轻量级、黑盒的验证机制，尤其适用于安全关键应用。", "keywords": "检索增强生成, 大型语言模型, 数据记忆化, RePCS, 黑盒诊断", "comments": "RePCS的创新之处在于其黑盒、模型无关的诊断能力，无需访问模型内部状态或进行重训练，这大大降低了其应用门槛。通过比较两种推理路径的输出分布来检测数据记忆化是一个巧妙且高效的方法。其PAC-style保证增加了方法的理论严谨性，而优异的实验结果和低延迟则证明了其实用性，特别是在需要确保RAG系统可靠性的安全关键场景中具有重要价值。"}}
{"id": "2506.15535", "title": "A Simplified Analysis of SGD for Linear Regression with Weight Averaging", "authors": ["Alexandru Meterez", "Depen Morwani", "Costin-Andrei Oncescu", "Jingfeng Wu", "Cengiz Pehlevan", "Sham Kakade"], "summary": "Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15535v1", "AI": {"title_translation": "带权重平均的线性回归SGD简化分析", "tldr": "本文提供了一种使用简单线性代数工具分析线性回归中SGD偏差和方差边界的简化方法，避免了对半正定矩阵操作的需求。", "motivation": "现有的理论工作虽然提供了线性回归中SGD优化的精确速率，但其分析方法涉及复杂的半正定矩阵操作，使得分析不易理解。本研究旨在简化这一分析过程。", "method": "本文基于简单的线性代数工具，对线性回归中SGD的分析进行了简化，成功地避免了对半正定（PSD）矩阵算子进行复杂操作的要求。", "result": "本研究成功恢复了与~\textcite{zou2021benign}工作中提供的相同偏差和方差界限。", "conclusion": "本文的工作使得线性回归中SGD的分析变得非常易于理解，并将有助于进一步分析小批量训练和学习率调度，从而改进实际模型的训练。", "translation": "在过参数化模型中理论理解随机梯度下降（SGD）促使了当今实践中广泛使用的几种优化算法的发展。~\textcite{zou2021benign}最近的工作基于风险的偏差-方差分解，为线性回归中使用恒定学习率的SGD优化提供了精确的速率，包括有尾部迭代平均和无尾部迭代平均的情况。在我们的工作中，我们提供了一种简化的分析方法，该方法基于简单的线性代数工具，恢复了~\textcite{zou2021benign}中提供的相同偏差和方差界限，从而绕过了对半正定（PSD）矩阵算子进行操作的要求。我们相信我们的工作使得线性回归中SGD的分析非常易于理解，并将有助于进一步分析小批量处理和学习率调度，从而改进实际模型的训练。", "summary": "本文提出了一种简化分析随机梯度下降（SGD）在线性回归中偏差和方差界限的方法。该方法利用简单的线性代数工具，成功地恢复了现有复杂分析所得到的相同界限，同时避免了对半正定矩阵算子的复杂操作。这项工作旨在提高SGD分析的可访问性，并为未来在小批量训练和学习率调度方面的研究提供便利，从而有望改进实际模型的训练效果。", "keywords": "随机梯度下降, 线性回归, 权重平均, 简化分析, 偏差-方差", "comments": "本文的主要创新在于提供了一种更易于理解和操作的SGD分析方法，通过避免复杂的矩阵操作，降低了理论分析的门槛。这对于推动SGD在实际应用中的进一步优化（如小批量和学习率调度）具有重要意义，因为它使得研究人员能够更专注于算法的设计而非复杂的数学推导。"}}
{"id": "2506.15544", "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning", "authors": ["Roger Creus Castanyer", "Johan Obando-Ceron", "Lu Li", "Pierre-Luc Bacon", "Glen Berseth", "Aaron Courville", "Pablo Samuel Castro"], "summary": "Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15544v1", "AI": {"title_translation": "深度强化学习中大规模稳定学习的稳定梯度", "tldr": "本文通过实证分析发现深度强化学习网络大规模扩展时性能下降的原因是非平稳性与梯度病理，并提出了一系列简单有效的梯度稳定干预措施，成功解决了该问题，实现了大规模下的稳健性能。", "motivation": "深度强化学习网络在扩展时性能会下降，且其根本原因尚不明确；现有解决方案通常复杂且未能揭示根本原因。", "method": "通过一系列实证分析，揭示了非平稳性与次优架构选择导致的梯度病理是深度强化学习规模化挑战的根本原因。在此基础上，提出了一系列直接干预措施来稳定梯度流。", "result": "所提出的干预措施简单易实现，且兼容现有算法，能够在不同网络深度和宽度下实现稳健的性能，即使在大规模下也能达到强大的性能。研究结果在多种智能体和环境套件上得到了验证。", "conclusion": "通过稳定梯度流，可以有效解决深度强化学习网络大规模扩展时性能下降的问题，实现大规模下的稳健学习。", "translation": "扩展深度强化学习网络具有挑战性，并且通常会导致性能下降，但这种失败模式的根本原因仍然知之甚少。最近的一些工作提出了解决这一问题的机制，但它们往往很复杂，未能突出导致这种困难的原因。在这项工作中，我们进行了一系列实证分析，这些分析表明，非平稳性与梯度病理（由于次优的架构选择）相结合是规模化挑战的根本原因。我们提出了一系列直接干预措施来稳定梯度流，从而在各种网络深度和宽度下实现稳健的性能。我们的干预措施易于实施，与成熟的算法兼容，并形成了一种有效的机制，即使在大规模下也能实现强大的性能。我们在各种智能体和环境套件上验证了我们的发现。", "summary": "本文通过实证分析指出，深度强化学习网络大规模扩展时性能下降的原因在于非平稳性与次优架构选择导致的梯度病理。为解决此问题，作者提出了一系列简单易实现的梯度稳定干预措施，这些措施能有效稳定梯度流，从而在不同网络规模下实现强劲且稳健的性能，并在多种智能体和环境中得到验证。", "keywords": "深度强化学习, 梯度稳定, 性能扩展, 非平稳性, 梯度病理", "comments": "本文通过深入分析揭示了深度强化学习大规模扩展时的核心障碍——梯度病理与非平稳性，并提出了直接且通用的解决方案。其创新之处在于明确指出了问题根源，并提供了简单有效的干预措施，而非复杂的架构调整，这对于实际应用具有重要意义。"}}
{"id": "2506.15554", "title": "DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones", "authors": ["Akhil Singampalli", "Danish Gufran", "Sudeep Pasricha"], "summary": "Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15554v1", "AI": {"title_translation": "DAILOC：使用智能手机的室内定位域增量学习", "tldr": "DAILOC是一种新颖的域增量学习框架，用于解决Wi-Fi指纹室内定位中因设备异构性和时间变化引起的域偏移及灾难性遗忘问题，实验证明其性能显著优于现有方法。", "motivation": "基于Wi-Fi指纹的室内定位在实际部署中面临重大挑战，这源于设备异构性和室内环境中的时间变化所导致的域偏移。现有方法通常独立地解决这些问题，导致泛化能力差并随着时间的推移易受灾难性遗忘的影响。", "method": "本文提出了DAILOC，一个新颖的域增量学习框架，共同解决了时间性和设备引起的域偏移。DAILOC引入了一种新颖的解耦策略，使用多级变分自编码器将域偏移与位置相关特征分离。此外，还引入了一种新颖的记忆引导的类潜在对齐机制来解决随着时间的推移发生的灾难性遗忘效应。", "result": "实验表明，DAILOC显著优于现有最先进的方法，平均误差降低高达2.74倍，最差情况误差降低4.6倍。", "conclusion": "DAILOC通过联合解决设备和时间引起的域偏移以及灾难性遗忘问题，显著提升了Wi-Fi指纹室内定位的性能和鲁棒性。", "translation": "基于Wi-Fi指纹的室内定位在实际部署中面临重大挑战，这源于设备异构性和室内环境中的时间变化所导致的域偏移。现有方法通常独立地解决这些问题，导致泛化能力差并随着时间的推移易受灾难性遗忘的影响。在这项工作中，我们提出了DAILOC，一个新颖的域增量学习框架，它共同解决了时间性和设备引起的域偏移。DAILOC引入了一种新颖的解耦策略，使用多级变分自编码器将域偏移与位置相关特征分离。此外，我们引入了一种新颖的记忆引导的类潜在对齐机制来解决随着时间的推移发生的灾难性遗忘效应。跨多个智能手机、建筑物和时间实例的实验表明，DAILOC显著优于现有最先进的方法，平均误差降低高达2.74倍，最差情况误差降低4.6倍。", "summary": "DAILOC是一个针对Wi-Fi指纹室内定位中设备异构性、时间变化引起的域偏移和灾难性遗忘问题提出的域增量学习框架。它通过多级变分自编码器实现域偏移与位置特征的解耦，并利用记忆引导的类潜在对齐机制对抗遗忘。实验证明，DAILOC在多设备、多建筑和多时间场景下，性能远超现有技术，显著降低了定位误差。", "keywords": "室内定位, 域增量学习, Wi-Fi指纹, 域偏移, 灾难性遗忘", "comments": "DAILOC的创新之处在于其联合解决了Wi-Fi指纹室内定位中的设备异构性、时间变化引起的域偏移以及灾难性遗忘问题。通过引入解耦策略和记忆引导机制，提高了系统的泛化能力和长期稳定性，对于实际部署具有重要意义。"}}
{"id": "2506.15559", "title": "Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates", "authors": ["Danish Gufran", "Sudeep Pasricha"], "summary": "Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15559v1", "AI": {"title_translation": "迈向可解释的室内定位：使用逻辑门解释Wi-Fi指纹上的神经网络学习", "tldr": "LogNet框架使用逻辑门解释并改进了基于深度学习的Wi-Fi室内定位，使其更具可解释性、准确性、模型更小、延迟更低，并能应对环境变化。", "motivation": "现有的深度学习（DL）室内定位模型是黑箱，缺乏可解释性，难以理解预测机制及模型如何响应真实世界噪声，尤其是在处理环境动态引起的时变性时，这阻碍了模型的长期可靠性。", "method": "引入了LogNet，一个新颖的基于逻辑门的框架，旨在解释和增强基于DL的室内定位。LogNet通过识别哪些接入点（AP）对每个参考点（RP）最具影响力，并揭示环境噪声如何干扰DL驱动的定位决策，从而实现透明推理。", "result": "LogNet不仅能够解释DL模型的内部行为，还显著提高了性能：与先前的DL模型相比，定位误差降低了1.1倍至2.8倍，模型尺寸缩小了3.4倍至43.3倍，延迟降低了1.5倍至3.6倍。这些改进在多个真实世界建筑平面图和两年时间变化的数据上得到了验证。", "conclusion": "LogNet框架通过引入可解释性，成功解决了深度学习室内定位的黑箱问题，显著提升了模型的性能、效率和长期稳定性，使其更能适应真实世界的环境变化。", "translation": "深度学习（DL）在将Wi-Fi RSS指纹映射到物理位置的室内定位方面表现出强大的准确性；然而，大多数现有的DL框架作为黑箱模型运行，对预测如何做出或模型如何随时间响应真实世界噪声提供有限的洞察。这种缺乏可解释性阻碍了我们理解由环境动态引起的时间变化的影响，以及调整模型以实现长期可靠性的能力。为了解决这个问题，我们引入了LogNet，一个新颖的基于逻辑门的框架，旨在解释和增强基于DL的室内定位。LogNet通过识别哪些接入点（AP）对每个参考点（RP）最具影响力，并揭示环境噪声如何干扰DL驱动的定位决策，从而实现透明推理。这种可解释性使我们能够追踪和诊断模型故障，并调整DL系统以实现更稳定的长期部署。在多个真实世界建筑平面图和两年时间变化的数据上进行的评估表明，LogNet不仅解释了DL模型的内部行为，而且还提高了性能——与之前的基于DL的模型相比，定位误差降低了1.1倍至2.8倍，模型尺寸缩小了3.4倍至43.3倍，延迟降低了1.5倍至3.6倍。", "summary": "这篇论文介绍了LogNet，一个基于逻辑门的新型框架，旨在解决深度学习在Wi-Fi指纹室内定位中的黑箱问题。LogNet通过提供模型内部决策的透明推理，揭示了接入点的影响力和环境噪声对定位的影响，从而提高了模型的长期可靠性。实验结果表明，LogNet不仅能解释深度学习模型的行为，还能显著提升定位精度，并大幅减小模型尺寸和降低延迟。", "keywords": "室内定位, 深度学习, 可解释性AI, Wi-Fi指纹, 逻辑门", "comments": "这篇论文的创新点在于将逻辑门引入深度学习模型，以提高其在室内定位领域的可解释性，同时不牺牲甚至提升性能。它解决了深度学习模型在实际部署中面临的“黑箱”问题，特别是在处理环境动态和长期稳定性方面。这种方法对于需要高可靠性和可诊断性的应用场景具有重要意义，因为它允许用户理解模型为何做出特定预测，并在出现故障时进行诊断和调整。"}}
{"id": "2506.15566", "title": "Task-Agnostic Experts Composition for Continual Learning", "authors": ["Luigi Quarantiello", "Andrea Cossu", "Vincenzo Lomonaco"], "summary": "Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15566v1", "AI": {"title_translation": "面向持续学习的任务无关专家组合", "tldr": "提出了一种专家组合方法，用于持续学习，实现了更高的准确性和更低的计算资源消耗。", "motivation": "组合性是人类推理的基本能力，对神经网络也至关重要，尤其在追求更高效、可持续的AI框架时。", "method": "提出了一种组合式方法，通过零样本集成一组专家模型，并使用一个旨在测试组合性能力的挑战性基准来评估该方法。", "result": "专家组合方法比基线算法获得了更高的准确性，同时需要更少的计算资源，因此更高效。", "conclusion": "该研究表明，所提出的专家组合方法在持续学习中表现出优越的性能，能够以更高的效率和更少的资源实现更好的准确性。", "translation": "组合性是人类推理过程的基本能力之一，它允许将复杂问题分解为更简单的元素。这种特性对于神经网络也至关重要，尤其是在旨在实现更高效和可持续的AI框架时。我们提出了一种通过零样本集成一组专家模型的组合方法，并使用一个旨在测试组合性能力的挑战性基准来评估我们的方法。我们展示了我们的专家组合方法能够比基线算法实现更高的准确性，同时需要更少的计算资源，因此更高效。", "summary": "本论文提出了一种名为“专家组合”的零样本集成方法，旨在解决持续学习中的组合性问题。通过在一个具有挑战性的基准上进行评估，该方法在实现比现有基线算法更高准确性的同时，显著减少了计算资源消耗，从而提升了AI框架的效率和可持续性。", "keywords": "持续学习, 专家组合, 零样本学习, 组合性, 效率", "comments": "该论文的创新点在于提出了一个任务无关的专家组合方法，通过零样本集成实现了高效且高性能的持续学习。其重要性在于提升了AI的组合性能力，为构建更高效、可持续的AI系统提供了新的思路。该方法在减少计算资源消耗方面的表现也具有实际应用价值。"}}
{"id": "2506.15588", "title": "Memory-Efficient Differentially Private Training with Gradient Random Projection", "authors": ["Alex Mulrooney", "Devansh Gupta", "James Flemings", "Huanyu Zhang", "Murali Annavaram", "Meisam Razaviyayn", "Xinwei Zhang"], "summary": "Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15588v1", "AI": {"title_translation": "基于梯度随机投影的内存高效差分隐私训练", "tldr": "DP-GRAPE是一种新的差分隐私训练方法，通过梯度随机投影显著减少内存使用，同时保持与现有方法相当的效用和准确性，尤其适用于大规模模型。", "motivation": "标准的差分隐私（DP）训练方法（如DP-Adam）由于需要进行逐样本梯度裁剪，导致内存开销高昂，限制了可扩展性。", "method": "本文提出了DP-GRAPE（Gradient RAndom ProjEction）方法，它不直接将DP应用于GaLore，而是进行了三项关键修改：1) 在投影后对梯度进行隐私化；2) 使用随机高斯矩阵代替基于SVD的子空间；3) 在反向传播过程中应用投影。这些修改消除了昂贵的SVD计算，实现了显著的内存节省。", "result": "DP-GRAPE在保持与一阶DP方法相当的效用和DP-SGD相当的隐私-效用权衡的同时，显著降低了内存使用。与DP-Adam相比，DP-GRAPE在预训练Vision Transformers时内存使用减少了63%以上，在微调RoBERTa-Large时减少了70%以上，同时性能相似。DP-GRAPE还能扩展到微调OPT等大型模型，参数量高达67亿。", "conclusion": "DP-GRAPE成功地解决了差分隐私训练中的内存效率问题，在不牺牲准确性和训练时间的前提下，实现了显著的内存节省，并能扩展到大规模模型。", "translation": "差分隐私（DP）在神经网络训练过程中保护敏感数据，但像DP-Adam这样的标准方法由于逐样本梯度裁剪而导致内存开销高昂，限制了可扩展性。我们引入了DP-GRAPE（梯度随机投影），一种DP训练方法，它显著减少了内存使用，同时保持了与一阶DP方法相当的效用。DP-GRAPE没有直接将DP应用于GaLore，而是引入了三个关键修改：(1) 在投影后对梯度进行隐私化，(2) 随机高斯矩阵取代了基于SVD的子空间，以及 (3) 在反向传播过程中应用投影。这些贡献消除了对昂贵SVD计算的需求，实现了显著的内存节省，并提高了效用。尽管在低维子空间中操作，我们的理论分析表明DP-GRAPE实现了与DP-SGD相当的隐私-效用权衡。我们广泛的实证实验表明，DP-GRAPE可以在不牺牲准确性或训练时间的情况下，减少DP训练的内存占用。特别是，与DP-Adam相比，DP-GRAPE在预训练Vision Transformers时内存使用减少了63%以上，在微调RoBERTa-Large时减少了70%以上，同时实现了相似的性能。我们进一步证明DP-GRAPE可以扩展到微调大型模型，例如参数量高达67亿的OPT。", "summary": "本文提出了一种名为DP-GRAPE的差分隐私训练新方法，旨在解决现有DP方法（如DP-Adam）因逐样本梯度裁剪导致的内存开销过高问题。DP-GRAPE通过在梯度投影后进行隐私化、使用随机高斯矩阵以及在反向传播中应用投影，显著降低了内存消耗，同时保持了与主流DP方法相当的隐私-效用权衡和模型性能。实验证明，DP-GRAPE在Vision Transformers和RoBERTa-Large上内存使用分别减少了63%和70%以上，并成功扩展到数十亿参数的大型模型。", "keywords": "差分隐私, 梯度投影, 内存效率, 神经网络训练, 大规模模型", "comments": "该论文在差分隐私训练领域取得了重要进展，通过引入梯度随机投影技术，有效解决了困扰现有方法的内存效率瓶颈，尤其在大规模模型训练中展现出巨大潜力。其创新点在于将隐私化与投影相结合，并避免了昂贵的SVD计算，这对于推动差分隐私技术的实际应用具有重要意义。该方法在保持性能的同时显著降低了资源消耗，为未来隐私保护机器学习的研究提供了新思路。"}}
{"id": "2506.15620", "title": "GFLC: Graph-based Fairness-aware Label Correction for Fair Classification", "authors": ["Modar Sulaiman", "Kallol Roy"], "summary": "Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.", "comment": "25 pages, 6 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15620v1", "AI": {"title_translation": "GFLC：基于图的公平性感知标签校正，用于公平分类", "tldr": "GFLC是一种新的基于图的方法，用于纠正机器学习中的标签噪声，同时保持人口统计学平等，以解决训练数据中的标签偏差问题，并在性能和公平性之间取得显著改进。", "motivation": "机器学习中的公平性至关重要，因为人工智能系统日益影响社会各方面，且现有研究表明机器学习存在不公平结果。此外，用于训练去偏技术的数据常包含有偏和噪声标签，导致模型性能受损并在测试时误导分类器公平性。", "method": "本文提出了基于图的公平性感知标签校正（GFLC）方法，旨在纠正标签噪声同时保持数据集中的人口统计学平等。该方法结合了三个关键组件：预测置信度测量、通过Ricci流优化图拉普拉斯算子的基于图的正则化，以及明确的人口统计学平等激励。", "result": "实验结果表明，所提出的方法是有效的，并且与基线相比，在性能和公平性指标之间的权衡方面显示出显著改进。", "conclusion": "GFLC方法能够有效纠正标签噪声并保持人口统计学平等，从而在性能和公平性之间实现更好的平衡，有助于构建更值得信赖的机器学习系统。", "translation": "机器学习（ML）中的公平性对于构建值得信赖的机器学习系统至关重要，因为人工智能（AI）系统日益影响社会各个方面，包括医疗保健决策和法律判决。此外，大量研究表明ML中存在不公平结果，需要更鲁棒的公平性感知方法。然而，我们用于训练和开发去偏技术的数据通常包含有偏和噪声标签。因此，训练数据中的标签偏差会影响模型性能，并在测试期间错误地表示分类器的公平性。为了解决这个问题，本文提出了基于图的公平性感知标签校正（GFLC），这是一种在纠正标签噪声的同时保持数据集中人口统计学平等的有效方法。特别是，我们的方法结合了三个关键组件：预测置信度测量、通过Ricci流优化图拉普拉斯算子的基于图的正则化，以及明确的人口统计学平等激励。我们的实验结果表明了我们提出方法的有效性，并与基线相比，在性能和公平性指标之间的权衡方面显示出显著改进。", "summary": "本文提出了GFLC（基于图的公平性感知标签校正）方法，旨在解决机器学习训练数据中普遍存在的标签偏差和噪声问题，这些问题会影响模型性能和公平性表现。GFLC通过结合预测置信度、基于Ricci流优化的图拉普拉斯算子的图正则化以及明确的人口统计学平等激励来纠正标签噪声，同时确保数据集的人口统计学平等。实验结果表明，GFLC能有效改善性能与公平性指标之间的权衡。", "keywords": "公平性, 标签校正, 基于图, 人口统计学平等, 机器学习", "comments": "GFLC的创新之处在于其结合了图论（Ricci流优化的图拉普拉斯算子）与公平性激励来解决标签噪声问题，并在保持人口统计学平等方面表现出色，这对于构建更值得信赖的AI系统具有重要意义。"}}
{"id": "2506.15654", "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization", "authors": ["Ranting Hu"], "summary": "Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.", "comment": "23 pages, 14 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15654v1", "AI": {"title_translation": "CAWR: 抵抗损坏的优势加权回归用于鲁棒策略优化", "tldr": "本文提出CAWR，通过鲁棒损失函数和经验回放解决离线RL中AWRs因数据损坏导致的过保守策略问题，并在D4RL上表现优异。", "motivation": "离线强化学习（offline RL）算法常需额外约束或惩罚项以解决分布漂移问题。优势加权回归（AWRs）家族存在一个局限性，即由于数据损坏（特别是次优离线数据中的不良探索）可能导致学习到过保守的策略。", "method": "本文提出了抵抗损坏的优势加权回归（CAWR）。该方法在策略优化过程中结合了一组鲁棒损失函数，并采用基于优势的优先级经验回放方法来过滤掉不良探索。", "result": "在D4RL基准上的数值实验表明，我们的方法可以从次优离线数据中学习到更优的策略，显著提升了策略优化的性能。", "conclusion": "过保守策略主要由策略优化损失函数对不良探索的敏感性以及离线数据集中不良探索的比例引起。CAWR通过鲁棒损失函数和过滤不良探索有效解决了这一问题，从而学习到更优的策略。", "translation": "离线强化学习（offline RL）算法通常需要额外的约束或惩罚项来解决分布漂移问题，例如在策略优化期间添加隐式或显式策略约束以减少函数估计偏差。本文关注优势加权回归（AWRs）家族的一个局限性，即由于数据损坏（特别是次优离线数据中的不良探索）可能导致学习到过保守的策略。我们从两个角度研究了这个问题：(1) 不良探索如何影响基于KL散度的理论最优策略，以及 (2) 这种不良探索如何影响理论最优策略的近似。我们证明了这种过保守性主要由策略优化损失函数对不良探索的敏感性以及离线数据集中不良探索的比例引起。为了解决这个问题，我们提出了抵抗损坏的优势加权回归（CAWR），它在策略优化过程中结合了一组鲁棒损失函数，并采用基于优势的优先级经验回放方法来过滤掉不良探索。在D4RL基准上的数值实验表明，我们的方法可以从次优离线数据中学习到更优的策略，显著提升了策略优化的性能。", "summary": "本文针对离线强化学习中优势加权回归（AWRs）因次优数据中不良探索导致策略过保守的问题，深入分析了其理论原因。为此，作者提出了CAWR（抵抗损坏的优势加权回归），该方法引入了鲁棒损失函数和基于优势的优先级经验回放机制来过滤不良探索。实验结果表明，CAWR能从次优数据中学习到更优策略，显著提升性能。", "keywords": "离线强化学习, 优势加权回归, 数据损坏, 鲁棒策略优化, 经验回放", "comments": "这篇论文通过理论分析揭示了AWRs在离线RL中因不良探索导致过保守策略的根本原因，并提出了一种新颖的解决方案CAWR。其创新点在于结合了鲁棒损失函数和基于优势的经验回放机制来主动应对数据损坏。该方法在解决离线RL中的实际问题方面具有重要意义，尤其是在数据质量受限的场景下。"}}
{"id": "2506.14772", "title": "SimBank: from Simulation to Solution in Prescriptive Process Monitoring", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "summary": "Prescriptive Process Monitoring (PresPM) is an emerging area within Process\nMining, focused on optimizing processes through real-time interventions for\neffective decision-making. PresPM holds significant promise for organizations\nseeking enhanced operational performance. However, the current literature faces\ntwo key limitations: a lack of extensive comparisons between techniques and\ninsufficient evaluation approaches. To address these gaps, we introduce\nSimBank: a simulator designed for accurate benchmarking of PresPM methods.\nModeled after a bank's loan application process, SimBank enables extensive\ncomparisons of both online and offline PresPM methods. It incorporates a\nvariety of intervention optimization problems with differing levels of\ncomplexity and supports experiments on key causal machine learning challenges,\nsuch as assessing a method's robustness to confounding in data. SimBank\nadditionally offers a comprehensive evaluation capability: for each test case,\nit can generate the true outcome under each intervention action, which is not\npossible using recorded datasets. The simulator incorporates parallel\nactivities and loops, drawing from common logs to generate cases that closely\nresemble real-life process instances. Our proof of concept demonstrates\nSimBank's benchmarking capabilities through experiments with various PresPM\nmethods across different interventions, highlighting its value as a publicly\navailable simulator for advancing research and practice in PresPM.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.14772v1", "AI": {"title_translation": "SimBank：从模拟到规范性过程监控的解决方案", "tldr": "本文介绍了 SimBank，一个用于规范性过程监控 (PresPM) 方法基准测试的模拟器，旨在解决当前文献中技术间缺乏广泛比较和评估方法不足的问题。", "motivation": "当前规范性过程监控 (PresPM) 文献中存在两个主要限制：技术之间缺乏广泛的比较以及评估方法不足。虽然 PresPM 对寻求提升运营绩效的组织具有重要前景，但这些限制阻碍了其发展。", "method": "作者引入了 SimBank，一个模拟器，其模型基于银行的贷款申请流程。SimBank 能够对在线和离线 PresPM 方法进行广泛比较，包含各种复杂程度不同的干预优化问题，并支持对关键因果机器学习挑战（如评估方法对数据中混淆的鲁棒性）进行实验。与使用记录数据集不同，SimBank 可以为每个测试用例生成每种干预措施下的真实结果。此外，该模拟器还结合了并行活动和循环，并借鉴常见日志来生成与现实生活过程实例高度相似的案例。", "result": "通过对不同干预措施下的各种 PresPM 方法进行实验，概念验证展示了 SimBank 的基准测试能力。", "conclusion": "SimBank 是一个有价值的、可公开获取的模拟器，能够推动规范性过程监控的研究和实践。", "translation": "规范性过程监控 (PresPM) 是过程挖掘领域的一个新兴方向，专注于通过实时干预来优化流程以实现有效的决策。PresPM 对寻求提升运营绩效的组织具有重要前景。然而，目前的文献面临两个关键限制：技术之间缺乏广泛的比较以及评估方法不足。为了弥补这些空白，我们引入了 SimBank：一个专为准确基准测试 PresPM 方法而设计的模拟器。SimBank 模仿银行的贷款申请流程，能够对在线和离线 PresPM 方法进行广泛比较。它包含了各种复杂程度不同的干预优化问题，并支持对关键因果机器学习挑战进行实验，例如评估方法对数据中混淆的鲁棒性。SimBank 还提供全面的评估能力：对于每个测试用例，它可以在每种干预措施下生成真实结果，这在使用记录数据集时是不可能实现的。该模拟器结合了并行活动和循环，借鉴常见日志生成与现实生活过程实例高度相似的案例。我们的概念验证通过对不同干预措施下的各种 PresPM 方法进行实验，展示了 SimBank 的基准测试能力，突出了其作为公共可用模拟器在推进 PresPM 研究和实践方面的价值。", "summary": "规范性过程监控 (PresPM) 领域目前面临技术比较不足和评估方法欠缺的问题。为解决这些空白，本文提出了 SimBank，一个基于银行贷款申请流程的模拟器，旨在对在线和离线 PresPM 方法进行全面的基准测试。SimBank 支持多种干预优化问题和因果机器学习实验，并能生成真实结果，弥补了传统数据集的局限性。概念验证实验展示了 SimBank 的有效性及其在推进 PresPM 研究中的重要价值。", "keywords": "规范性过程监控, 过程挖掘, 模拟, 基准测试, 因果机器学习", "comments": "SimBank 通过提供一个强大的基准测试平台，解决了 PresPM 研究中的关键空白。其生成真实结果和模拟真实过程实例的能力，使其成为评估和比较 PresPM 方法（尤其是在因果机器学习背景下）的一项重要创新。其可公开获取的特性有望加速该领域的研究。"}}
{"id": "2506.14850", "title": "Beyond Force Metrics: Pre-Training MLFFs for Stable MD Simulations", "authors": ["Shagun Maheshwari", "Janghoon Ock", "Adeesh Kolluru", "Amir Barati Farimani", "John R. Kitchin"], "summary": "Machine-learning force fields (MLFFs) have emerged as a promising solution\nfor speeding up ab initio molecular dynamics (MD) simulations, where accurate\nforce predictions are critical but often computationally expensive. In this\nwork, we employ GemNet-T, a graph neural network model, as an MLFF and\ninvestigate two training strategies: (1) direct training on MD17 (10K samples)\nwithout pre-training, and (2) pre-training on the large-scale OC20 dataset\nfollowed by fine-tuning on MD17 (10K). While both approaches achieve low force\nmean absolute errors (MAEs), reaching 5 meV/A per atom, we find that lower\nforce errors do not necessarily guarantee stable MD simulations. Notably, the\npre-trained GemNet-T model yields significantly improved simulation stability,\nsustaining trajectories up to three times longer than the model trained from\nscratch. These findings underscore the value of pre-training on large, diverse\ndatasets to capture complex molecular interactions and highlight that force MAE\nalone is not always a sufficient metric of MD simulation stability.", "comment": null, "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2506.14850v1", "AI": {"title_translation": "超越力学指标：预训练机器学习力场实现稳定分子动力学模拟", "tldr": "预训练的机器学习力场（MLFFs）在分子动力学（MD）模拟中能显著提高稳定性，即使力误差（MAE）相似，也优于未预训练模型。", "motivation": "机器学习力场（MLFFs）有望加速从头算分子动力学（MD）模拟，但准确的力预测计算成本高昂。现有研究发现，仅追求低力平均绝对误差（MAE）并不能保证MD模拟的稳定性。", "method": "本研究采用图神经网络模型GemNet-T作为机器学习力场（MLFF），并比较了两种训练策略：1) 在MD17数据集（1万样本）上直接训练；2) 在大规模OC20数据集上进行预训练，然后在MD17上进行微调。", "result": "两种方法都达到了5 meV/A的低力平均绝对误差（MAE）。然而，预训练的GemNet-T模型显著提高了模拟稳定性，其轨迹持续时间比从头开始训练的模型长达三倍。这表明，较低的力误差不一定能保证稳定的MD模拟。", "conclusion": "在大规模、多样化数据集上进行预训练对于捕捉复杂的分子相互作用具有重要价值。同时，仅凭力平均绝对误差（MAE）不足以作为衡量分子动力学（MD）模拟稳定性的充分指标。", "translation": "机器学习力场（MLFFs）已成为加速从头算分子动力学（MD）模拟的一种有前景的解决方案，其中准确的力预测至关重要但通常计算成本高昂。在这项工作中，我们采用图神经网络模型GemNet-T作为MLFF，并研究了两种训练策略：(1) 在MD17（1万样本）上直接训练，不进行预训练；(2) 在大规模OC20数据集上进行预训练，然后在MD17（1万样本）上进行微调。虽然两种方法都达到了较低的力平均绝对误差（MAE），每原子5 meV/A，但我们发现较低的力误差不一定能保证稳定的MD模拟。值得注意的是，预训练的GemNet-T模型显著提高了模拟稳定性，其轨迹持续时间比从头开始训练的模型长达三倍。这些发现强调了在大规模、多样化数据集上进行预训练对于捕捉复杂分子相互作用的价值，并突出表明力MAE本身并不总是MD模拟稳定性的充分衡量标准。", "summary": "本研究探讨了机器学习力场（MLFFs）在加速分子动力学（MD）模拟中的应用。通过使用GemNet-T模型，作者比较了直接训练与在大规模数据集OC20上预训练后在MD17上微调两种策略。结果显示，尽管两种方法都能达到相似的低力误差，但预训练模型显著提升了MD模拟的稳定性，轨迹持续时间延长了三倍。这表明，力误差并非衡量MD模拟稳定性的唯一标准，预训练对于捕捉复杂分子相互作用至关重要。", "keywords": "机器学习力场, 分子动力学模拟, 预训练, GemNet-T, 模拟稳定性", "comments": "这篇论文的创新点在于强调了预训练在机器学习力场（MLFFs）中对于提升分子动力学（MD）模拟稳定性的关键作用，而非仅仅关注力预测精度。它挑战了传统上将低力平均绝对误差（MAE）视为唯一或主要成功指标的观念，提出了一个更全面的评估标准，即模拟的长期稳定性。这一发现对于开发更可靠、更高效的MLFFs具有重要指导意义。"}}
{"id": "2506.14853", "title": "DisProtEdit: Exploring Disentangled Representations for Multi-Attribute Protein Editing", "authors": ["Max Ku", "Sun Sun", "Hongyu Guo", "Wenhu Chen"], "summary": "We introduce DisProtEdit, a controllable protein editing framework that\nleverages dual-channel natural language supervision to learn disentangled\nrepresentations of structural and functional properties. Unlike prior\napproaches that rely on joint holistic embeddings, DisProtEdit explicitly\nseparates semantic factors, enabling modular and interpretable control. To\nsupport this, we construct SwissProtDis, a large-scale multimodal dataset where\neach protein sequence is paired with two textual descriptions, one for\nstructure and one for function, automatically decomposed using a large language\nmodel. DisProtEdit aligns protein and text embeddings using alignment and\nuniformity objectives, while a disentanglement loss promotes independence\nbetween structural and functional semantics. At inference time, protein editing\nis performed by modifying one or both text inputs and decoding from the updated\nlatent representation. Experiments on protein editing and representation\nlearning benchmarks demonstrate that DisProtEdit performs competitively with\nexisting methods while providing improved interpretability and controllability.\nOn a newly constructed multi-attribute editing benchmark, the model achieves a\nboth-hit success rate of up to 61.7%, highlighting its effectiveness in\ncoordinating simultaneous structural and functional edits.", "comment": "Accepted to ICMLW (GenBio) 2025 and ICMLW (FM4LS) 2025", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.14853v1", "AI": {"title_translation": "DisProtEdit：探索多属性蛋白质编辑的解耦表示", "tldr": "DisProtEdit是一个可控的蛋白质编辑框架，它利用双通道自然语言监督来学习结构和功能属性的解耦表示，与现有方法相比，它提供了更高的可解释性和可控性，并在多属性编辑基准上表现出色。", "motivation": "现有的蛋白质编辑方法依赖于联合的整体嵌入，这限制了其模块化和可解释的控制。因此，需要一种能够明确分离语义因素的蛋白质编辑框架。", "method": "该论文引入了DisProtEdit，一个可控的蛋白质编辑框架，利用双通道自然语言监督来学习结构和功能属性的解耦表示。它构建了一个大型多模态数据集SwissProtDis，其中每个蛋白质序列与两个文本描述（一个用于结构，一个用于功能）配对，这些描述是使用大型语言模型自动分解的。DisProtEdit通过对齐和统一目标来对齐蛋白质和文本嵌入，同时使用解耦损失来促进结构和功能语义之间的独立性。在推理时，通过修改一个或两个文本输入并从更新的潜在表示中解码来进行蛋白质编辑。", "result": "DisProtEdit在蛋白质编辑和表示学习基准上与现有方法相比表现出竞争力，并提供了更高的可解释性和可控性。在一个新构建的多属性编辑基准上，该模型实现了高达61.7%的双命中成功率，这突出了其在协调同步结构和功能编辑方面的有效性。", "conclusion": "DisProtEdit通过学习蛋白质结构和功能属性的解耦表示，实现了可控且可解释的多属性蛋白质编辑，并在相关基准上取得了优异的性能。", "translation": "我们引入了 DisProtEdit，这是一个可控的蛋白质编辑框架，它利用双通道自然语言监督来学习结构和功能属性的解耦表示。与依赖联合整体嵌入的现有方法不同，DisProtEdit 明确分离了语义因素，从而实现了模块化和可解释的控制。为了支持这一点，我们构建了 SwissProtDis，一个大规模多模态数据集，其中每个蛋白质序列都与两个文本描述配对，一个用于结构，一个用于功能，这些描述是使用大型语言模型自动分解的。DisProtEdit 使用对齐和统一目标来对齐蛋白质和文本嵌入，而解耦损失则促进结构和功能语义之间的独立性。在推理时，通过修改一个或两个文本输入并从更新的潜在表示中解码来执行蛋白质编辑。在蛋白质编辑和表示学习基准上的实验表明，DisProtEdit 与现有方法相比具有竞争力，同时提供了更高的可解释性和可控性。在一个新构建的多属性编辑基准上，该模型实现了高达 61.7% 的双命中成功率，这突出了其在协调同步结构和功能编辑方面的有效性。", "summary": "DisProtEdit是一个新颖的可控蛋白质编辑框架，它通过双通道自然语言监督学习蛋白质结构和功能属性的解耦表示。该框架通过构建大规模多模态数据集SwissProtDis，并利用对齐和解耦损失来明确分离语义因素。实验证明，DisProtEdit在蛋白质编辑和表示学习任务上表现出色，并显著提升了多属性蛋白质编辑的可解释性和可控性，尤其在同时进行结构和功能编辑时表现出高成功率。", "keywords": "蛋白质编辑, 解耦表示, 自然语言监督, 多模态数据集, 可控性", "comments": "DisProtEdit的创新之处在于其引入了双通道自然语言监督和解耦表示学习，以实现对蛋白质多属性编辑的精细控制。与以往的整体嵌入方法相比，其明确分离结构和功能语义的能力显著提升了模型的可解释性和模块化。此外，构建大规模多模态数据集SwissProtDis也是一项重要贡献，为未来的研究提供了宝贵资源。该方法在实现高成功率的同时兼顾了可控性和可解释性，为蛋白质设计和工程领域开辟了新的途径。"}}
{"id": "2506.14858", "title": "CutReg: A loss regularizer for enhancing the scalability of QML via adaptive circuit cutting", "authors": ["Maniraman Periyasamy", "Christian Ufrecht", "Daniel D. Scherer", "Wolfgang Mauerer"], "summary": "Whether QML can offer a transformative advantage remains an open question.\nThe severe constraints of NISQ hardware, particularly in circuit depth and\nconnectivity, hinder both the validation of quantum advantage and the empirical\ninvestigation of major obstacles like barren plateaus. Circuit cutting\ntechniques have emerged as a strategy to execute larger quantum circuits on\nsmaller, less connected hardware by dividing them into subcircuits. However,\nthis partitioning increases the number of samples needed to estimate the\nexpectation value accurately through classical post-processing compared to\nestimating it directly from the full circuit. This work introduces a novel\nregularization term into the QML optimization process, directly penalizing the\noverhead associated with sampling. We demonstrate that this approach enables\nthe optimizer to balance the advantages of gate cutting against the\noptimization of the typical ML cost function. Specifically, it navigates the\ntrade-off between minimizing the cutting overhead and maintaining the overall\naccuracy of the QML model, paving the way to study larger complex problems in\npursuit of quantum advantage.", "comment": "This work has been submitted to the QML@QCE workshop for possible\n  publication", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14858v1", "AI": {"title_translation": "CutReg：一种通过自适应电路切割增强QML可扩展性的损失正则化器", "tldr": "CutReg引入了一种新的损失正则化项，用于量化机器学习（QML）优化过程，以直接惩罚电路切割带来的采样开销，从而在NISQ硬件上平衡切割优势和模型准确性，提升QML的可扩展性。", "motivation": "NISQ硬件的严格限制（电路深度和连接性）阻碍了量子优势的验证以及对荒漠高原等主要障碍的实证研究。尽管电路切割技术能够将大电路分解为子电路以在受限硬件上运行，但这会显著增加估计期望值所需的采样开销，从而阻碍QML的实际应用和量子优势的探索。", "method": "本研究在QML优化过程中引入了一个新颖的正则化项，命名为CutReg，旨在直接惩罚与电路切割相关的采样开销。该方法使得优化器能够在利用门切割优势的同时，平衡对典型机器学习成本函数的优化，从而在最小化切割开销和保持QML模型整体准确性之间进行权衡。", "result": "该方法使优化器能够平衡门切割的优势与典型机器学习成本函数的优化。具体而言，它成功地在最小化切割开销和保持QML模型整体准确性之间取得了权衡。", "conclusion": "通过引入CutReg，本研究为在追求量子优势的过程中研究更大的复杂问题铺平了道路，因为它有效地管理了电路切割带来的采样开销与模型准确性之间的权衡。", "translation": "量子机器学习（QML）是否能提供变革性优势仍是一个悬而未决的问题。NISQ硬件的严格限制，特别是在电路深度和连接性方面，阻碍了量子优势的验证以及对荒漠高原等主要障碍的实证研究。电路切割技术已成为一种策略，通过将量子电路分割成子电路，在更小、连接性较差的硬件上执行更大的量子电路。然而，与直接从完整电路估计相比，这种分区增加了通过经典后处理准确估计期望值所需的样本数量。这项工作在QML优化过程中引入了一个新颖的正则化项，直接惩罚与采样相关的开销。我们证明了这种方法使优化器能够平衡门切割的优势与典型ML成本函数的优化。具体来说，它在最小化切割开销和保持QML模型整体准确性之间取得了权衡，为研究更大的复杂问题以追求量子优势铺平了道路。", "summary": "本论文旨在解决嘈杂中等规模量子（NISQ）硬件上量子机器学习（QML）的可扩展性挑战，这些挑战主要源于电路深度和连接性的限制。虽然电路切割技术允许在较小硬件上执行更大的量子电路，但它在期望值估计中引入了显著的采样开销。为缓解此问题，作者提出了“CutReg”，一种新颖的损失正则化器，它在QML优化过程中直接惩罚采样开销。这使得优化器能够有效地平衡电路切割的优势与优化机器学习成本函数的主要目标，从而在减少切割开销和保持模型准确性之间取得权衡。该方法有助于研究更复杂的量子问题，推动量子优势的实现。", "keywords": "QML, 电路切割, 正则化, 可扩展性, NISQ硬件", "comments": "这篇论文为QML可扩展性中的一个关键问题提供了一个创新性解决方案。通过引入一个明确针对电路切割所引入的采样开销的损失正则化器，它提供了一种实用方法来管理电路分解与计算成本之间的权衡。这种自适应方法有望显著增强QML在现有硬件上的适用性，是对该领域的重要贡献。"}}
{"id": "2506.14899", "title": "Optimal Convergence Rates of Deep Neural Network Classifiers", "authors": ["Zihan Zhang", "Lei Shi", "Ding-Xuan Zhou"], "summary": "In this paper, we study the binary classification problem on $[0,1]^d$ under\nthe Tsybakov noise condition (with exponent $s \\in [0,\\infty]$) and the\ncompositional assumption. This assumption requires the conditional class\nprobability function of the data distribution to be the composition of $q+1$\nvector-valued multivariate functions, where each component function is either a\nmaximum value function or a H\\\"{o}lder-$\\beta$ smooth function that depends\nonly on $d_*$ of its input variables. Notably, $d_*$ can be significantly\nsmaller than the input dimension $d$. We prove that, under these conditions,\nthe optimal convergence rate for the excess 0-1 risk of classifiers is $$\n\\left( \\frac{1}{n}\n\\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}\\;\\;\\;,\n$$ which is independent of the input dimension $d$. Additionally, we\ndemonstrate that ReLU deep neural networks (DNNs) trained with hinge loss can\nachieve this optimal convergence rate up to a logarithmic factor. This result\nprovides theoretical justification for the excellent performance of ReLU DNNs\nin practical classification tasks, particularly in high-dimensional settings.\nThe technique used to establish these results extends the oracle inequality\npresented in our previous work. The generalized approach is of independent\ninterest.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14899v1", "AI": {"title_translation": "深度神经网络分类器的最优收敛速度", "tldr": "本文研究了在特定噪声和组合假设下二分类问题的最优收敛速度，并证明ReLU深度神经网络可以达到此最优速度，解释了其在高维任务中的优秀表现。", "motivation": "旨在为ReLU深度神经网络在实际分类任务，特别是在高维设置中的优异性能提供理论依据。", "method": "在Tsybakov噪声条件和组合假设下，通过理论证明推导了分类器0-1风险的最优收敛速度。此外，还证明了使用铰链损失训练的ReLU深度神经网络可以达到此最优收敛速度（至多相差一个对数因子）。研究中使用的技术扩展了之前的预言不等式方法。", "result": "在给定条件下，分类器0-1风险的最优收敛速度公式为 $$ \\left( \\frac{1}{n} \\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}\\;\\;\\; $$ 且此速度与输入维度 $d$ 无关。此外，ReLU深度神经网络（使用铰链损失训练）可以达到此最优收敛速度，至多相差一个对数因子。", "conclusion": "该研究为ReLU深度神经网络在实际分类任务，尤其是在高维环境中的出色表现提供了坚实的理论依据。", "translation": "在本文中，我们研究了在Tsybakov噪声条件（指数 $s \\in [0,\\infty]$）和组合假设下 $[0,1]^d$ 上的二分类问题。该假设要求数据分布的条件类别概率函数是 $q+1$ 个向量值多元函数的组合，其中每个分量函数要么是最大值函数，要么是仅依赖于其 $d_*$ 个输入变量的H\"{o}lder-$\\\\beta$ 光滑函数。值得注意的是，$d_*$ 可以显著小于输入维度 $d$。我们证明，在这些条件下，分类器0-1风险的最优收敛速度为 $$ \\left( \\frac{1}{n} \\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}\\;\\;\\; $$ 该速度与输入维度 $d$ 无关。此外，我们证明了使用铰链损失训练的ReLU深度神经网络（DNNs）可以达到此最优收敛速度，至多相差一个对数因子。这一结果为ReLU深度神经网络在实际分类任务中，特别是在高维设置下的优异性能提供了理论依据。用于建立这些结果的技术扩展了我们之前工作中提出的预言不等式。这种广义方法本身也具有独立的价值。", "summary": "本文研究了在Tsybakov噪声条件和组合假设下二分类问题的最优收敛速度。研究证明，在特定条件下，分类器0-1风险的最优收敛速度与输入维度无关，并给出了具体公式。更重要的是，作者证明了使用铰链损失训练的ReLU深度神经网络能够达到这一最优收敛速度（至多相差一个对数因子），从而为ReLU DNNs在高维分类任务中的卓越性能提供了强有力的理论支持。", "keywords": "深度神经网络, 收敛速度, 二分类, Tsybakov噪声, 组合函数", "comments": "这篇论文通过建立严格的理论框架，量化了深度神经网络在特定条件下的学习效率。其创新之处在于证明了ReLU DNNs在高维设置下可以实现与维度无关的最优收敛速度，这为解释深度学习的成功提供了重要的理论依据。扩展的预言不等式方法也具有独立的理论价值。"}}
{"id": "2506.14920", "title": "Q2SAR: A Quantum Multiple Kernel Learning Approach for Drug Discovery", "authors": ["Alejandro Giraldo", "Daniel Ruiz", "Mariano Caruso", "Javier Mancilla", "Guido Bellomo"], "summary": "Quantitative Structure-Activity Relationship (QSAR) modeling is a cornerstone\nof computational drug discovery. This research demonstrates the successful\napplication of a Quantum Multiple Kernel Learning (QMKL) framework to enhance\nQSAR classification, showing a notable performance improvement over classical\nmethods. We apply this methodology to a dataset for identifying DYRK1A kinase\ninhibitors. The workflow involves converting SMILES representations into\nnumerical molecular descriptors, reducing dimensionality via Principal\nComponent Analysis (PCA), and employing a Support Vector Machine (SVM) trained\non an optimized combination of multiple quantum and classical kernels. By\nbenchmarking the QMKL-SVM against a classical Gradient Boosting model, we show\nthat the quantum-enhanced approach achieves a superior AUC score, highlighting\nits potential to provide a quantum advantage in challenging cheminformatics\nclassification tasks.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14920v1", "AI": {"title_translation": "Q2SAR：一种用于药物发现的量子多核学习方法", "tldr": "该研究成功应用量子多核学习（QMKL）框架改进QSAR分类，在识别DYRK1A激酶抑制剂的任务中，相较于经典方法显著提升了性能。", "motivation": "QSAR建模是计算药物发现的基石，研究旨在通过应用量子多核学习（QMKL）框架来增强QSAR分类，以期超越经典方法的性能。", "method": "该研究应用了量子多核学习（QMKL）框架。具体流程包括：将SMILES表示转换为数值分子描述符，通过主成分分析（PCA）降低维度，并使用在优化的多个量子和经典核组合上训练的支持向量机（SVM）。通过将QMKL-SVM与经典的梯度提升模型进行基准测试来评估性能。", "result": "量子增强方法（QMKL-SVM）在识别DYRK1A激酶抑制剂的数据集上，相较于经典的梯度提升模型，取得了更优的AUC分数。", "conclusion": "该研究表明，量子增强方法在具有挑战性的化学信息学分类任务中具有提供量子优势的潜力，并成功地将QMKL应用于QSAR分类以提高性能。", "translation": "定量构效关系（QSAR）建模是计算药物发现的基石。本研究展示了量子多核学习（QMKL）框架在增强QSAR分类方面的成功应用，显示出相对于经典方法的显著性能提升。我们将此方法应用于一个用于识别DYRK1A激酶抑制剂的数据集。其工作流程包括将SMILES表示转换为数值分子描述符，通过主成分分析（PCA）降低维度，并采用在优化的多个量子和经典核组合上训练的支持向量机（SVM）。通过将QMKL-SVM与经典的梯度提升模型进行基准测试，我们表明量子增强方法取得了更优的AUC分数，突出了其在具有挑战性的化学信息学分类任务中提供量子优势的潜力。", "summary": "本研究提出了一种名为Q2SAR的量子多核学习（QMKL）框架，旨在增强药物发现中的定量构效关系（QSAR）分类。该方法将SMILES分子表示转换为数值描述符，通过PCA降维，并利用结合量子和经典核的SVM进行训练。在识别DYRK1A激酶抑制剂的任务中，与传统梯度提升模型相比，QMKL-SVM展现出卓越的AUC性能，证明了其在复杂化学信息学分类中提供量子优势的潜力。", "keywords": "量子多核学习, QSAR, 药物发现, 化学信息学, SVM", "comments": "该研究的创新之处在于将量子多核学习（QMKL）引入到药物发现的QSAR建模中，并展示了其相对于经典方法的性能优势。这为化学信息学领域带来了新的视角，并可能在未来推动药物发现的效率。其重要性在于探索了量子计算在实际药物研发应用中的潜力，特别是在处理复杂分子数据分类任务方面。"}}
{"id": "2506.14925", "title": "Digital twin for virtual sensing of ferry quays via a Gaussian Process Latent Force Model", "authors": ["Luigi Sibille", "Torodd Skjerve Nord", "Alice Cicirello"], "summary": "Ferry quays experience rapid deterioration due to their exposure to harsh\nmaritime environments and ferry impacts. Vibration-based structural health\nmonitoring offers a valuable approach to assessing structural integrity and\nunderstanding the structural implications of these impacts. However, practical\nlimitations often restrict sensor placement at critical locations.\nConsequently, virtual sensing techniques become essential for establishing a\nDigital Twin and estimating the structural response. This study investigates\nthe application of the Gaussian Process Latent Force Model (GPLFM) for virtual\nsensing on the Magerholm ferry quay, combining in-operation experimental data\ncollected during a ferry impact with a detailed physics-based model. The\nproposed Physics-Encoded Machine Learning model integrates a reduced-order\nstructural model with a data-driven GPLFM representing the unknown impact\nforces via their modal contributions. Significant challenges are addressed for\nthe development of the Digital Twin of the ferry quay, including unknown impact\ncharacteristics (location, direction, intensity), time-varying boundary\nconditions, and sparse sensor configurations. Results show that the GPLFM\nprovides accurate acceleration response estimates at most locations, even under\nsimplifying modeling assumptions such as linear time-invariant behavior during\nthe impact phase. Lower accuracy was observed at locations in the impact zone.\nA numerical study was conducted to explore an optimal real-world sensor\nplacement strategy using a Backward Sequential Sensor Placement approach.\nSensitivity analyses were conducted to examine the influence of sensor types,\nsampling frequencies, and incorrectly assumed damping ratios. The results\nsuggest that the GP latent forces can help accommodate modeling and measurement\nuncertainties, maintaining acceptable estimation accuracy across scenarios.", "comment": "14 Figures, 1 Table", "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.14925v1", "AI": {"title_translation": "基于高斯过程潜在力模型的轮渡码头虚拟传感数字孪生", "tldr": "本研究利用高斯过程潜在力模型（GPLFM）为轮渡码头构建数字孪生，实现虚拟传感，以应对恶劣环境和传感器布设限制，并证明其在估计结构响应和处理不确定性方面的有效性。", "motivation": "轮渡码头因暴露于恶劣海洋环境和轮渡冲击而迅速恶化。振动结构健康监测是一种评估结构完整性的有效方法，但实际限制常导致关键位置传感器布设不足。因此，虚拟传感技术对于建立数字孪生和估计结构响应至关重要。", "method": "本研究将高斯过程潜在力模型（GPLFM）应用于轮渡码头的虚拟传感。该方法结合了轮渡冲击期间收集的在役实验数据和详细的基于物理的模型。提出的物理编码机器学习模型将降阶结构模型与数据驱动的GPLFM相结合，通过模态贡献来表示未知的冲击力。研究还解决了数字孪生开发中的关键挑战，包括未知冲击特性、时变边界条件和稀疏传感器配置。此外，还进行了数值研究以探索最优传感器布设策略，并进行了敏感性分析以检验传感器类型、采样频率和错误假设阻尼比的影响。", "result": "结果表明，即使在简化建模假设下，GPLFM也能在大多数位置提供准确的加速度响应估计。但在冲击区域的位置观察到较低的准确性。数值研究探索了最优的传感器布设策略。敏感性分析表明，高斯过程潜在力可以帮助适应建模和测量不确定性，在不同场景下保持可接受的估计精度。", "conclusion": "本研究表明，基于高斯过程潜在力模型（GPLFM）的虚拟传感方法能够有效应对轮渡码头数字孪生开发中的挑战，并在存在建模和测量不确定性时提供准确的结构响应估计。", "translation": "轮渡码头因暴露于恶劣海洋环境和轮渡冲击而迅速恶化。基于振动的结构健康监测为评估结构完整性和理解这些冲击对结构的影响提供了一种有价值的方法。然而，实际限制常常限制了关键位置的传感器布设。因此，虚拟传感技术对于建立数字孪生和估计结构响应变得至关重要。本研究通过将轮渡冲击期间收集的在役实验数据与详细的基于物理的模型相结合，研究了高斯过程潜在力模型（GPLFM）在Magerholm轮渡码头虚拟传感中的应用。所提出的物理编码机器学习模型将降阶结构模型与数据驱动的GPLFM集成，通过模态贡献表示未知的冲击力。在轮渡码头数字孪生开发中解决了重大挑战，包括未知冲击特性（位置、方向、强度）、时变边界条件和稀疏传感器配置。结果表明，即使在简化建模假设（如冲击阶段的线性时不变行为）下，GPLFM也能在大多数位置提供准确的加速度响应估计。在冲击区域的位置观察到较低的准确性。进行了一项数值研究，以使用逆向顺序传感器布设方法探索最佳的实际传感器布设策略。进行了敏感性分析，以检查传感器类型、采样频率和错误假设阻尼比的影响。结果表明，高斯过程潜在力有助于适应建模和测量不确定性，在各种情景下保持可接受的估计精度。", "summary": "本研究提出了一种基于高斯过程潜在力模型（GPLFM）的虚拟传感方法，用于构建轮渡码头的数字孪生。针对码头在恶劣环境下的快速老化和传感器布设限制问题，该方法结合了实验数据与物理模型，通过物理编码机器学习模型来估计结构响应，并解决未知冲击特性、时变边界和稀疏传感器等挑战。研究结果表明，GPLFM能准确估计大部分位置的加速度响应，并有效应对建模与测量不确定性。", "keywords": "数字孪生, 虚拟传感, 高斯过程潜在力模型, 结构健康监测, 轮渡码头", "comments": "该论文的创新点在于将高斯过程潜在力模型（GPLFM）与物理编码机器学习模型相结合，用于轮渡码头的虚拟传感，以克服传统传感器布设的局限性。其重要性体现在为恶劣环境下基础设施的结构健康监测提供了一种鲁棒且高效的解决方案，尤其是在数据稀疏和不确定性高的场景下。该方法能够有效处理未知冲击力等复杂因素，具有实际应用价值。"}}
{"id": "2506.14950", "title": "Double Machine Learning for Conditional Moment Restrictions: IV regression, Proximal Causal Learning and Beyond", "authors": ["Daqian Shao", "Ashkan Soleymani", "Francesco Quinzan", "Marta Kwiatkowska"], "summary": "Solving conditional moment restrictions (CMRs) is a key problem considered in\nstatistics, causal inference, and econometrics, where the aim is to solve for a\nfunction of interest that satisfies some conditional moment equalities.\nSpecifically, many techniques for causal inference, such as instrumental\nvariable (IV) regression and proximal causal learning (PCL), are CMR problems.\nMost CMR estimators use a two-stage approach, where the first-stage estimation\nis directly plugged into the second stage to estimate the function of interest.\nHowever, naively plugging in the first-stage estimator can cause heavy bias in\nthe second stage. This is particularly the case for recently proposed CMR\nestimators that use deep neural network (DNN) estimators for both stages, where\nregularisation and overfitting bias is present. We propose DML-CMR, a two-stage\nCMR estimator that provides an unbiased estimate with fast convergence rate\nguarantees. We derive a novel learning objective to reduce bias and develop the\nDML-CMR algorithm following the double/debiased machine learning (DML)\nframework. We show that our DML-CMR estimator can achieve the minimax optimal\nconvergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity\nconditions, where $N$ is the sample size. We apply DML-CMR to a range of\nproblems using DNN estimators, including IV regression and proximal causal\nlearning on real-world datasets, demonstrating state-of-the-art performance\nagainst existing CMR estimators and algorithms tailored to those problems.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14950v1", "AI": {"title_translation": "条件矩限制的双重机器学习：工具变量回归、近端因果学习及其他", "tldr": "提出DML-CMR，一个基于双重机器学习框架的两阶段条件矩限制估计器，解决了传统方法中的偏差问题，并实现了更快的收敛速度和最先进的性能。", "motivation": "解决条件矩限制（CMRs）是统计学、因果推断和计量经济学中的关键问题。传统的两阶段CMR估计器，特别是使用深度神经网络（DNN）的，在第一阶段估计直接插入第二阶段时会导致严重的偏差，尤其存在正则化和过拟合偏差。", "method": "提出DML-CMR，一个两阶段CMR估计器。它通过推导一个新的学习目标来减少偏差，并遵循双重/去偏机器学习（DML）框架开发了DML-CMR算法。", "result": "DML-CMR估计器在参数化和温和正则化条件下，可以达到$O(N^{-1/2})$的最小最大最优收敛速度。在工具变量回归和近端因果学习等实际数据集问题上应用DNN估计器，DML-CMR表现出优于现有CMR估计器和针对这些问题量身定制算法的最先进性能。", "conclusion": "DML-CMR提供了一种有效且高效的方法来解决条件矩限制问题，克服了传统两阶段估计器的偏差问题，并实现了理论最优的收敛速度和实际应用中的卓越性能。", "translation": "条件矩限制（CMRs）的求解是统计学、因果推断和计量经济学中考虑的一个关键问题，其目的是求解满足某些条件矩等式的目标函数。具体来说，许多因果推断技术，如工具变量（IV）回归和近端因果学习（PCL），都是CMR问题。大多数CMR估计器采用两阶段方法，其中第一阶段的估计直接插入第二阶段以估计目标函数。然而，简单地插入第一阶段估计器可能导致第二阶段的严重偏差。对于最近提出的使用深度神经网络（DNN）作为两阶段估计器的CMR估计器尤其如此，其中存在正则化和过拟合偏差。我们提出了DML-CMR，一个两阶段的CMR估计器，它提供无偏估计和快速收敛速度保证。我们推导了一个新颖的学习目标来减少偏差，并遵循双重/去偏机器学习（DML）框架开发了DML-CMR算法。我们证明了我们的DML-CMR估计器在参数化和温和正则化条件下可以达到$O(N^{-1/2})$的最小最大最优收敛速度，其中N是样本量。我们将DML-CMR应用于一系列使用DNN估计器的问题，包括在真实世界数据集上的工具变量回归和近端因果学习，展示了相对于现有CMR估计器和针对这些问题量身定制的算法的最先进性能。", "summary": "本文提出DML-CMR，一个用于解决条件矩限制（CMRs）的两阶段估计器，旨在克服传统两阶段方法中因简单插入第一阶段估计结果而导致的偏差问题。DML-CMR基于双重/去偏机器学习（DML）框架，通过引入新颖的学习目标来减少偏差并保证快速收敛。实验证明，DML-CMR在工具变量回归和近端因果学习等任务上，使用深度神经网络时能达到理论最优收敛速度，并在实际数据集中展现出最先进的性能。", "keywords": "条件矩限制, 双重机器学习, 因果推断, 工具变量回归, 深度神经网络", "comments": "这篇论文的创新点在于将双重机器学习（DML）框架应用于条件矩限制（CMRs）的求解，有效解决了传统两阶段估计器，特别是结合深度神经网络时，存在的严重偏差问题。其重要性体现在为因果推断和计量经济学中的关键问题提供了一种更稳健、更高效的估计方法，并提供了理论上的收敛速度保证。"}}
{"id": "2506.14952", "title": "An Observation on Lloyd's k-Means Algorithm in High Dimensions", "authors": ["David Silva-Sánchez", "Roy R. Lederman"], "summary": "Clustering and estimating cluster means are core problems in statistics and\nmachine learning, with k-means and Expectation Maximization (EM) being two\nwidely used algorithms. In this work, we provide a theoretical explanation for\nthe failure of k-means in high-dimensional settings with high noise and limited\nsample sizes, using a simple Gaussian Mixture Model (GMM). We identify regimes\nwhere, with high probability, almost every partition of the data becomes a\nfixed point of the k-means algorithm. This study is motivated by challenges in\nthe analysis of more complex cases, such as masked GMMs, and those arising from\napplications in Cryo-Electron Microscopy.", "comment": "27 pages, 3 figures, 4 supplemental figures", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14952v1", "AI": {"title_translation": "关于高维空间中Lloyd's k-均值算法的观察", "tldr": "本文理论解释了在高噪声和有限样本量的高维设置下，k-均值算法的失效原因：在这种情况下，几乎每个数据划分都可能成为k-均值算法的固定点。", "motivation": "聚类和估计聚类均值是统计学和机器学习中的核心问题。本研究的动机是分析更复杂情况（如掩蔽高斯混合模型）以及冷冻电镜应用中遇到的挑战。", "method": "作者使用一个简单的高斯混合模型（GMM），为k-均值算法在高维、高噪声和有限样本量设置下的失效提供了理论解释。", "result": "研究识别出在特定条件下，高概率下几乎每个数据划分都成为k-均值算法的固定点。", "conclusion": "在高维、高噪声和有限样本量的情况下，k-均值算法会失效，因为几乎所有数据划分都可能成为其固定点。", "translation": "聚类和估计聚类均值是统计学和机器学习中的核心问题，其中k-均值和期望最大化（EM）是两种广泛使用的算法。在这项工作中，我们利用一个简单的高斯混合模型（GMM），为k-均值算法在高噪声和有限样本量的高维设置下的失效提供了一个理论解释。我们识别出在这种情况下，几乎每个数据划分都以高概率成为k-均值算法的固定点。这项研究的动机是分析更复杂情况（如掩蔽高斯混合模型）以及冷冻电镜应用中遇到的挑战。", "summary": "本文探讨了在高维、高噪声和有限样本量环境下，k-均值算法失效的理论原因。研究利用一个简单的高斯混合模型，发现当维度、噪声和样本量满足特定条件时，几乎所有数据划分都可能成为k-均值算法的固定点，从而解释了其性能下降的原因。这项工作旨在解决更复杂模型（如掩蔽高斯混合模型）和冷冻电镜应用中遇到的分析挑战。", "keywords": "k-均值, 高维, 高斯混合模型, 固定点, 聚类", "comments": "本文通过理论分析，揭示了在高维、高噪声和有限样本量情境下k-均值算法失效的深层原因，即大量分区成为固定点。这一发现对于理解k-均值算法的局限性及其在实际应用中的表现具有重要意义，尤其是在处理高维数据时。其创新点在于为算法失效提供了一个清晰的数学解释，而非仅仅观察到现象。"}}
{"id": "2506.14957", "title": "POCO: Scalable Neural Forecasting through Population Conditioning", "authors": ["Yu Duan", "Hamza Tahir Chaudhry", "Misha B. Ahrens", "Christopher D Harvey", "Matthew G Perich", "Karl Deisseroth", "Kanaka Rajan"], "summary": "Predicting future neural activity is a core challenge in modeling brain\ndynamics, with applications ranging from scientific investigation to\nclosed-loop neurotechnology. While recent models of population activity\nemphasize interpretability and behavioral decoding, neural\nforecasting-particularly across multi-session, spontaneous recordings-remains\nunderexplored. We introduce POCO, a unified forecasting model that combines a\nlightweight univariate forecaster with a population-level encoder to capture\nboth neuron-specific and brain-wide dynamics. Trained across five calcium\nimaging datasets spanning zebrafish, mice, and C. elegans, POCO achieves\nstate-of-the-art accuracy at cellular resolution in spontaneous behaviors.\nAfter pre-training, POCO rapidly adapts to new recordings with minimal\nfine-tuning. Notably, POCO's learned unit embeddings recover biologically\nmeaningful structure-such as brain region clustering-without any anatomical\nlabels. Our comprehensive analysis reveals several key factors influencing\nperformance, including context length, session diversity, and preprocessing.\nTogether, these results position POCO as a scalable and adaptable approach for\ncross-session neural forecasting and offer actionable insights for future model\ndesign. By enabling accurate, generalizable forecasting models of neural\ndynamics across individuals and species, POCO lays the groundwork for adaptive\nneurotechnologies and large-scale efforts for neural foundation models.", "comment": null, "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.14957v1", "AI": {"title_translation": "POCO：通过群体条件实现可扩展的神经预测", "tldr": "POCO是一种新的神经预测模型，它结合了单元级和群体级动态，在多个数据集上实现了最先进的细胞分辨率预测，并且可以快速适应新数据，其学习到的嵌入能够揭示生物学意义的结构。", "motivation": "预测未来的神经活动是建模大脑动力学的核心挑战，现有模型主要侧重于可解释性和行为解码，但神经预测，特别是跨多会话、自发记录的预测，仍未得到充分探索。", "method": "论文引入了POCO，一个统一的预测模型，它结合了一个轻量级的单变量预测器和一个群体级编码器，以捕捉神经元特异性和全脑动态。", "result": "POCO在斑马鱼、小鼠和秀丽隐杆线虫的五个钙成像数据集上进行训练，在自发行为中实现了细胞分辨率的最先进精度。预训练后，POCO能以最小的微调快速适应新记录。POCO学习到的单元嵌入能够恢复具有生物学意义的结构（如脑区聚类），而无需任何解剖学标签。", "conclusion": "POCO是一种可扩展且适应性强的跨会话神经预测方法，并为未来的模型设计提供了可行的见解。通过实现准确、可泛化的跨个体和物种神经动力学预测模型，POCO为自适应神经技术和大规模的神经基础模型工作奠定了基础。", "translation": "预测未来的神经活动是建模大脑动力学的核心挑战，其应用范围从科学研究到闭环神经技术。虽然最近的群体活动模型强调可解释性和行为解码，但神经预测——尤其是在多会话、自发记录中——仍未得到充分探索。我们引入了POCO，一个统一的预测模型，它结合了一个轻量级的单变量预测器和一个群体级编码器，以捕捉神经元特异性和全脑动态。POCO在斑马鱼、小鼠和秀丽隐杆线虫的五个钙成像数据集上进行训练，在自发行为中实现了细胞分辨率的最先进精度。预训练后，POCO能以最小的微调快速适应新记录。值得注意的是，POCO学习到的单元嵌入能够恢复具有生物学意义的结构——例如脑区聚类——而无需任何解剖学标签。我们的综合分析揭示了影响性能的几个关键因素，包括上下文长度、会话多样性和预处理。总而言之，这些结果将POCO定位为一种可扩展且适应性强的跨会话神经预测方法，并为未来的模型设计提供了可行的见解。通过实现准确、可泛化的跨个体和物种神经动力学预测模型，POCO为自适应神经技术和大规模的神经基础模型工作奠定了基础。", "summary": "本文提出POCO模型，旨在解决跨会话、自发神经活动预测的挑战。POCO结合了轻量级单变量预测器和群体级编码器，能够捕捉神经元特异性和全脑动态。该模型在多物种钙成像数据集上实现了最先进的细胞分辨率预测，并展示了快速适应新数据和发现生物学结构（如脑区聚类）的能力。POCO被定位为一种可扩展、适应性强的神经预测方法，为未来的神经技术和基础模型奠定基础。", "keywords": "神经预测, 群体条件, 钙成像, 脑动力学, 神经技术", "comments": "POCO的创新之处在于其结合了单元级和群体级信息的双重编码器结构，实现了神经预测的SOTA性能，并在不依赖解剖学标签的情况下，通过学习到的嵌入恢复了生物学意义的结构，这对于理解大脑功能具有重要意义。其跨物种、多会话的泛化能力和快速适应性也使其在神经技术应用中具有巨大潜力。"}}
{"id": "2506.15020", "title": "Data analysis using discrete cubical homology", "authors": ["Chris Kapulkin", "Nathan Kershaw"], "summary": "We present a new tool for data analysis: persistence discrete homology, which\nis well-suited to analyze filtrations of graphs. In particular, we provide a\nnovel way of representing high-dimensional data as a filtration of graphs using\npairwise correlations. We discuss several applications of these tools, e.g., in\nweather and financial data, comparing them to the standard methods used in the\nrespective fields.", "comment": "17 pages; comments welcome", "cate": "math.AT", "url": "http://arxiv.org/abs/2506.15020v1", "AI": {"title_translation": "使用离散立方同调进行数据分析", "tldr": "本文提出了一种新的数据分析工具——持久离散同调，特别适用于分析图的过滤，并提供了一种通过成对相关性将高维数据表示为图过滤的新颖方法，该工具已应用于天气和金融数据。", "motivation": "本文旨在提出一种新的数据分析工具，特别是用于分析图的过滤和表示高维数据。", "method": "本文提出了一种名为“持久离散同调”的新工具，并提供了一种使用成对相关性将高维数据表示为图过滤的新颖方法。", "result": "本文讨论了该工具在天气和金融数据等领域的多种应用，并将其与各自领域的标准方法进行了比较。", "conclusion": "本文提出了一种新的数据分析工具（持久离散同调），尤其适用于高维数据，并通过在天气和金融数据中的应用展示了其潜力。", "translation": "我们提出了一种新的数据分析工具：持久离散同调，它非常适合分析图的过滤。特别是，我们提供了一种使用成对相关性将高维数据表示为图过滤的新颖方法。我们讨论了这些工具的几种应用，例如在天气和金融数据中，并将它们与各自领域中使用的标准方法进行比较。", "summary": "本文介绍了一种新的数据分析工具——持久离散同调。它提出了一种使用成对相关性将高维数据表示为图过滤的新颖方法。作者讨论了该工具在天气和金融数据等领域的多种应用，并将其与各自领域的标准方法进行了比较。", "keywords": "持久离散同调, 数据分析, 图过滤, 高维数据, 成对相关性", "comments": "本文的创新之处在于将离散同调应用于数据分析，特别是通过图过滤和成对相关性来表示高维数据。其重要性在于为处理复杂数据集提供了一种新的拓扑方法。"}}
{"id": "2506.15315", "title": "Proximal Operators of Sorted Nonconvex Penalties", "authors": ["Anne Gagneux", "Mathurin Massias", "Emmanuel Soubies"], "summary": "This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15315v1", "AI": {"title_translation": "排序非凸惩罚的近端算子", "tldr": "本文研究了排序非凸惩罚的近端算子的计算方法，旨在解决稀疏信号恢复和变量自动分组问题，并针对弱凸和非凸情况提出了基于PAV算法的高效解决方案。", "motivation": "该研究旨在解决稀疏信号恢复中变量自动分组的问题，通过引入排序非凸惩罚来促进变量聚类并减少系数收缩。核心挑战是为这些惩罚提供高效的近端算子计算方法，以便将其应用于流行的近端算法以解决复合优化问题。", "method": "本文研究了作为广义线性模型正则化方法的排序非光滑惩罚，特别是推广了SLOPE的排序非凸惩罚。区分了弱凸和非凸两种情况：对于弱凸情况（如排序MCP和SCAD），利用Pool Adjacent Violators (PAV) 算法精确计算近端算子；对于非凸情况（如q在]0,1[中的排序Lq），通过对PAV算法进行修改实现了高效计算。此外，还提出了关于非凸近端问题最小化器的新理论见解。", "result": "研究结果表明，PAV算法能够精确计算弱凸排序惩罚的近端算子。对于非凸情况，PAV算法的轻微修改被证明在计算近端算子方面非常高效。此外，本文还提供了关于非凸近端问题最小化器的新理论见解，并通过多项实验验证了使用此类惩罚的实际价值。", "conclusion": "本文为排序非凸惩罚的近端算子提供了高效的计算方法，并针对弱凸和非凸问题提出了基于PAV算法的解决方案。这些方法使得这些惩罚能够应用于复合优化问题，并在稀疏信号恢复和变量分组中展现出实用性。", "translation": "这项工作研究了具有变量自动分组的稀疏信号恢复问题。为此，我们研究了排序非光滑惩罚作为广义线性模型的正则化方法。我们专注于一类排序非凸惩罚，它推广了排序L1范数（SLOPE）。这些惩罚旨在通过其排序性质促进变量聚类，同时非凸性减少了系数的收缩。我们的目标是提供有效的方法来计算它们的近端算子，从而使得流行近端算法能够使用这种排序惩罚来解决复合优化问题。我们区分了两类问题：弱凸情况，其中计算近端算子仍然是凸问题；以及非凸情况，其中计算近端算子成为一个具有挑战性的非凸组合问题。对于弱凸情况（例如排序MCP和SCAD），我们解释了Pool Adjacent Violators (PAV) 算法如何精确计算近端算子。对于非凸情况（例如q在]0,1[中的排序Lq），我们表明该算法的轻微修改在处理近端算子的计算方面非常有效。我们还提出了关于非凸近端问题最小化器的新理论见解。我们在多项实验中展示了使用此类惩罚的实际兴趣。", "summary": "本文研究了用于稀疏信号恢复和变量自动分组的排序非凸惩罚，旨在提供其近端算子的高效计算方法。文章区分了弱凸和非凸两种情况：对于弱凸情况，提出了基于PAV算法的精确计算方法；对于非凸情况，通过修改PAV算法实现了高效计算。研究还提供了非凸近端问题最小化器的理论见解，并通过实验验证了这些惩罚的实用性。", "keywords": "稀疏信号恢复, 排序非凸惩罚, 近端算子, PAV算法, 变量分组", "comments": "该论文的创新点在于提出了计算排序非凸惩罚近端算子的有效方法，特别是对PAV算法的修改使其能够处理复杂的非凸情况。这对于稀疏信号恢复和变量分组等领域具有重要意义，因为它允许将这些推广的非凸惩罚应用于更广泛的优化问题中，同时利用了非凸性来减少系数收缩。"}}
{"id": "2506.15387", "title": "Multi-Timescale Gradient Sliding for Distributed Optimization", "authors": ["Junhui Zhang", "Patrick Jaillet"], "summary": "We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15387v1", "AI": {"title_translation": "分布式优化中的多时间尺度梯度滑动", "tldr": "提出了两种用于分布式非光滑凸优化的新型梯度滑动算法（MT-GS和AMT-GS），它们通过利用局部目标相似性、灵活的通信速率和多时间尺度更新，实现了最优的通信和梯度步数复杂度，并解决了非光滑目标函数通信依赖的开放问题。", "motivation": "解决凸、非光滑、分布式优化问题，旨在减少通信轮次，提高算法灵活性，并实现最优的计算复杂度。特别是，它旨在回答关于非光滑目标函数是否能实现通信轮次对局部函数相似性A的线性依赖的开放问题。", "method": "提出了两种一阶方法：多时间尺度梯度滑动（MT-GS）及其加速变体（AMT-GS）。这些方法通过块可分解的原始-对偶公式和Lan等人（2020）、Lan（2016）引入的滑动方法的多时间尺度变体来实现，其中不同的对偶块可以以不同的速率更新。", "result": "算法在寻找$\\epsilon$-次优解时，对$\\epsilon$的依赖性达到最优。对于Lipchitz目标函数，MT-GS需要$O(\\overline{r}A/\\epsilon)$次通信和$O(\\overline{r}/\\epsilon^2)$次次梯度步。如果目标函数是$\\mu$-强凸的，AMT-GS需要$O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$次通信和$O(\\overline{r}/(\\epsilon\\mu))$次次梯度步。通信轮次对$A$的线性依赖是最佳的，解决了开放问题。", "conclusion": "本文提出的MT-GS和AMT-GS算法在处理凸、非光滑、分布式优化问题时，不仅在通信轮次和次梯度步数上实现了对$\\epsilon$的最优依赖，而且通过其灵活的多时间尺度更新机制和对局部目标相似性的利用，有效减少了通信开销，并正面回答了非光滑目标函数通信依赖的开放问题。", "translation": "我们提出了两种用于凸、非光滑、分布式优化问题的一阶方法，分别称为多时间尺度梯度滑动（MT-GS）及其加速变体（AMT-GS）。我们的MT-GS和AMT-GS可以利用（局部）目标之间的相似性来减少通信轮次，并且具有灵活性，使得（代理）的不同子集可以以不同的、用户选择的速率进行通信，并且是完全确定性的。这三个理想的特性是通过块可分解的原始-对偶公式和Lan等人（2020）、Lan（2016）引入的滑动方法的多时间尺度变体实现的，其中不同的对偶块可能以不同的速率更新。\n为了找到一个$\\epsilon$-次优解，我们算法的复杂度实现了对$\\epsilon$的最优依赖：对于Lipchitz目标函数，MT-GS需要$O(\\overline{r}A/\\epsilon)$次通信和$O(\\overline{r}/\\epsilon^2)$次次梯度步；如果目标函数也是$\\mu$-强凸的，AMT-GS需要$O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$次通信和$O(\\overline{r}/(\\epsilon\\mu))$次次梯度步。这里，$\\overline{r}$衡量了对偶块的“平均更新速率”，而$A$衡量了局部函数（的次梯度）之间的相似性。此外，通信轮次对$A$的线性依赖是最佳的（Arjevani和Shamir 2015），从而为非光滑目标函数是否可以实现这种依赖的开放问题（Arjevani和Shamir 2015）提供了肯定的答案。", "summary": "本文提出了两种针对凸、非光滑分布式优化问题的一阶算法：多时间尺度梯度滑动（MT-GS）及其加速版本（AMT-GS）。这些算法通过块可分解的原始-对偶公式和多时间尺度更新策略，利用局部目标相似性减少通信，并允许灵活的通信速率。它们在实现$\\epsilon$-次优解时，其通信轮次和次梯度步数复杂度对$\\epsilon$具有最优依赖性，特别地，解决了非光滑目标函数通信轮次对局部函数相似性线性依赖的开放问题。", "keywords": "分布式优化, 梯度滑动, 非光滑优化, 通信复杂度, 多时间尺度", "comments": "这篇论文的创新点在于提出了多时间尺度梯度滑动方法，并将其应用于分布式优化，有效地解决了非光滑目标函数下的通信效率问题。通过引入灵活的通信速率和利用局部目标相似性，该方法显著减少了通信轮次，并达到了理论最优的复杂度，对分布式优化领域具有重要意义。"}}
{"id": "2506.15505", "title": "Time-dependent density estimation using binary classifiers", "authors": ["Agnimitra Dasgupta", "Javier Murgoitio-Esandi", "Ali Fardisi", "Assad A Oberai"], "summary": "We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15505v1", "AI": {"title_translation": "使用二元分类器进行时间依赖密度估计", "tldr": "本文提出了一种数据驱动的方法，利用新颖的时间依赖二元分类器来学习多元随机过程的时间依赖概率密度，该方法能够准确重建复杂密度、有效扩展并可靠检测异常事件。", "motivation": "学习多元随机过程的时间依赖概率密度，假设初始概率密度已知且可评估。", "method": "提出了一种数据驱动的方法，使用一种新颖的时间依赖二元分类器，通过基于对比估计的目标进行训练，以区分随机过程在两个相邻时间点的实现。该方法明确建模时间依赖概率分布，并且分类器中最终激活前的输入是对数密度关于时间偏导数的二阶近似。应用于由随机激励驱动的系统的时间依赖概率密度函数近似，并用于从给定实现集合成随机向量的新样本。训练样本路径通过随机插值器生成，新样本通过基于梯度的马尔可夫链蒙特卡罗方法生成。", "result": "所提出的方法能够准确重建复杂、时间依赖、多模态和近简并的密度，有效扩展到中等高维问题，并可靠检测真实世界数据中的稀有事件（异常值）。", "conclusion": "该方法提供了一种有效且准确的手段来估计时间依赖概率密度函数，并应用于样本合成和无监督异常值检测等领域。", "translation": "我们提出了一种数据驱动的方法，用于从样本路径中学习多元随机过程的时间依赖概率密度，假设初始概率密度已知且可以评估。我们的方法使用一种新颖的时间依赖二元分类器，通过基于对比估计的目标进行训练，该目标训练分类器区分随机过程在两个相邻时间点的实现。重要的是，所提出的方法明确地建模了时间依赖概率分布，这意味着可以在感兴趣的时间范围内获得概率密度的值。此外，时间依赖分类器中最终激活之前的输入是对数密度关于时间偏导数的二阶近似。我们将所提出的方法应用于近似由随机激励驱动的系统的时间依赖概率密度函数。我们还使用所提出的方法从给定的一组实现中合成随机向量的新样本。在此类应用中，我们使用随机插值器生成训练所需的样本路径。随后，由于自动微分可以有效地提供必要的梯度，因此使用基于梯度的马尔可夫链蒙特卡罗方法生成新样本。此外，我们通过在无监督异常值检测中的应用，展示了时间依赖概率密度函数的显式近似的实用性。通过几个数值实验，我们表明所提出的方法准确地重建了复杂的时间依赖、多模态和近简并密度，有效地扩展到中等高维问题，并可靠地检测真实世界数据中的稀有事件。", "summary": "本文提出了一种数据驱动方法，利用新颖的时间依赖二元分类器来估计多元随机过程的时间依赖概率密度。该方法通过对比估计训练分类器区分相邻时间点的过程实现，并能显式建模概率分布。它将对数密度偏导数的二阶近似作为输入。该方法成功应用于随机激励系统的密度近似、新样本合成（结合随机插值器和MCMC），以及无监督异常值检测。实验证明，该方法能准确重建复杂、多模态密度，有效处理中等高维问题，并可靠检测稀有事件。", "keywords": "时间依赖密度估计,二元分类器,对比估计,随机过程,异常值检测", "comments": "该论文的创新点在于提出了一种基于时间依赖二元分类器和对比估计的数据驱动方法来估计时间依赖概率密度，并将其与对数密度偏导数的二阶近似相结合。其重要性在于能够显式建模和重建复杂、时间依赖的密度，并有效应用于样本合成和异常值检测等实际问题，展示了良好的可扩展性和鲁棒性。"}}
{"id": "2506.15643", "title": "Revisiting Randomization in Greedy Model Search", "authors": ["Xin Chen", "Jason M. Klusowski", "Yan Shuo Tan", "Chang Yu"], "summary": "Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15643v1", "AI": {"title_translation": "重新审视贪婪模型搜索中的随机化", "tldr": "该论文提出了一种高效的随机贪婪前向选择方法用于稀疏线性回归，通过特征子采样实现随机化并利用动态规划提高效率。研究表明，该方法优于Lasso和Elastic Net，并能同时减少训练误差和自由度，改变偏差-方差权衡曲线，加深了对随机森林和隐式正则化的理解。", "motivation": "组合随机估计器（如随机森林）计算成本高昂，且其改善预测性能的机制尚不清楚。", "method": "提出一种用于稀疏线性回归的贪婪前向选择估计器集成，通过特征子采样进行随机化（每次迭代从随机子集中选择最佳特征）。设计了基于动态规划的新颖实现以提高计算效率。在正交特征设置下，严格证明了集成估计器通过两参数族的逻辑权重重新缩放普通最小二乘系数。", "result": "计算效率显著提高。在广泛设置下，性能优于Lasso和Elastic Net等流行方法。能够同时减少训练误差和自由度，从而改变基础估计器的偏差-方差权衡曲线。在正交特征情况下，集成估计器通过逻辑权重重新缩放OLS系数，扩大了模型搜索空间。", "conclusion": "研究结果增强了对随机森林的理解，并表明一般的隐式正则化可能比显式正则化具有更复杂的影响。", "translation": "将随机估计器组合成集成，例如通过随机森林，已成为现代数据科学中的一项基本技术，但计算成本可能很高。此外，其提高预测性能的机制尚不清楚。我们通过提出和分析一种贪婪前向选择估计器的集成来解决稀疏线性回归背景下的这些问题，该估计器通过特征子采样进行随机化——在每次迭代中，从随机子集中选择最佳特征。我们设计了一种基于动态规划的新颖实现，大大提高了其计算效率。此外，我们通过仔细的数值实验表明，我们的方法在各种设置下都可以优于Lasso和Elastic Net等流行方法。接下来，与普遍认为随机集成类似于收缩的观点相反，我们通过数值实验表明，它可以在同时减少训练误差和自由度，从而改变基础估计器的整个偏差-方差权衡曲线。我们在正交特征的设置中严格证明了这一事实，在这种情况下，集成估计器使用两参数族的逻辑权重重新缩放普通最小二乘系数，从而扩大了模型搜索空间。这些结果增强了我们对随机森林的理解，并表明一般的隐式正则化可能比显式正则化具有更复杂的效果。", "summary": "该论文针对稀疏线性回归，提出了一种通过特征子采样随机化的贪婪前向选择集成估计器，以解决随机集成计算成本高和作用机制不清的问题。通过动态规划实现，显著提高了计算效率。数值实验表明，该方法在多种设置下优于Lasso和Elastic Net，并且能够同时降低训练误差和自由度，改变偏差-方差权衡曲线。理论上，对于正交特征，该集成通过逻辑权重重新缩放OLS系数，扩大了模型搜索空间。这些发现加深了对随机森林的理解，并揭示了隐式正则化可能比显式正则化具有更复杂的影响。", "keywords": "稀疏线性回归, 随机化, 贪婪前向选择, 集成学习, 偏差-方差权衡", "comments": "这项工作通过提出一种新颖且高效的随机贪婪模型搜索方法，并对其作用机制进行了深入分析，对随机集成方法（特别是随机森林）的理解做出了重要贡献。它不仅在实践中提供了性能更优且计算效率更高的方法，而且在理论上挑战了关于随机化等同于收缩的普遍观点，揭示了随机化可能通过更复杂的方式影响模型表现，例如同时减少训练误差和自由度，并扩大模型搜索空间。这对于理解和设计更有效的机器学习模型具有重要意义。"}}
