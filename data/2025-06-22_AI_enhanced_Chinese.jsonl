{"id": "2506.14913", "title": "Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning", "authors": ["Wassim Bouaziz", "Mathurin Videau", "Nicolas Usunier", "El-Mahdi El-Mhamdi"], "summary": "The pre-training of large language models (LLMs) relies on massive text\ndatasets sourced from diverse and difficult-to-curate origins. Although\nmembership inference attacks and hidden canaries have been explored to trace\ndata usage, such methods rely on memorization of training data, which LM\nproviders try to limit. In this work, we demonstrate that indirect data\npoisoning (where the targeted behavior is absent from training data) is not\nonly feasible but also allow to effectively protect a dataset and trace its\nuse. Using gradient-based optimization prompt-tuning, we make a model learn\narbitrary secret sequences: secret responses to secret prompts that are absent\nfrom the training corpus. We validate our approach on language models\npre-trained from scratch and show that less than 0.005% of poisoned tokens are\nsufficient to covertly make a LM learn a secret and detect it with extremely\nhigh confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.\nCrucially, this occurs without performance degradation (on LM benchmarks) and\ndespite secrets never appearing in the training set.", "comment": "18 pages, 12 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14913v1", "AI": {"title_translation": "冬日战士：通过间接数据投毒在预训练阶段对语言模型进行后门攻击", "tldr": "本文展示了一种名为“冬日战士”的间接数据投毒方法，可在预训练阶段对大型语言模型进行后门攻击，实现数据集保护和使用追踪，且仅需极少量投毒数据即可实现高置信度检测，同时不影响模型性能。", "motivation": "现有的大型语言模型数据使用追踪方法（如成员推理攻击和隐藏金丝雀）依赖于训练数据的记忆化，而模型提供商正试图限制这种记忆化。因此，需要一种不依赖于此的新方法来保护数据集并追踪其使用。", "method": "本文提出一种间接数据投毒方法，即目标行为（秘密序列）在训练数据中不存在。通过基于梯度的优化提示调整（gradient-based optimization prompt-tuning），使模型学习任意秘密序列：对秘密提示的秘密响应。该方法在从零开始预训练的语言模型上进行了验证。", "result": "研究表明，不到0.005%的投毒令牌足以隐蔽地使语言模型学习一个秘密，并以极高的置信度（p < 10^-55）通过理论上可认证的方案进行检测。重要的是，这在不引起性能下降（在语言模型基准测试上）的情况下发生，尽管秘密从未出现在训练集中。", "conclusion": "间接数据投毒是一种可行且有效的方法，可以在预训练阶段对语言模型进行后门攻击，以实现数据集的保护和使用追踪。这种方法不需要目标行为出现在训练数据中，且对模型性能没有负面影响。", "translation": "大型语言模型（LLMs）的预训练依赖于从各种难以管理来源获取的海量文本数据集。尽管已经探索了成员推理攻击和隐藏金丝雀等方法来追踪数据使用，但这些方法依赖于训练数据的记忆化，而语言模型提供商正试图限制这种记忆化。在这项工作中，我们证明了间接数据投毒（即目标行为在训练数据中不存在）不仅可行，而且能够有效保护数据集并追踪其使用。通过基于梯度的优化提示调整，我们使模型学习任意秘密序列：对训练语料库中不存在的秘密提示的秘密响应。我们在从零开始预训练的语言模型上验证了我们的方法，并表明不到0.005%的投毒令牌足以隐蔽地使语言模型学习一个秘密，并以极高的置信度（p < 10^-55）通过理论上可认证的方案进行检测。至关重要的是，这在不引起性能下降（在语言模型基准测试上）的情况下发生，尽管秘密从未出现在训练集中。", "summary": "本文提出了一种名为“冬日战士”的间接数据投毒技术，用于在大型语言模型预训练阶段进行后门攻击，以实现数据集保护和使用追踪。与传统依赖数据记忆化的方法不同，该方法通过基于梯度的提示调整，使模型学习训练数据中不存在的秘密序列。实验证明，仅需极少量（<0.005%）的投毒数据，即可在不影响模型性能的前提下，以高置信度隐蔽地植入并检测秘密，有效解决了现有数据追踪方法的局限性。", "keywords": "间接数据投毒, 语言模型, 预训练, 数据追踪, 后门攻击", "comments": "这项工作具有重要的创新性，因为它提出了一种不依赖于模型记忆训练数据的新型数据追踪和保护机制。通过间接数据投毒，它克服了现有方法在隐私保护方面的局限性。其关键创新在于能够以极低的投毒比例实现高置信度的检测，且不影响模型性能，这对于大规模数据集的版权保护和溯源具有潜在的应用价值。该方法的“冬日战士”命名也暗示了其隐蔽和可控的特性。"}}
{"id": "2506.14944", "title": "Fair Data Exchange with Constant-Time Proofs", "authors": ["Majid Khabbazian"], "summary": "The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic\npay-per-file transfers with constant-size proofs, but its prover and verifier\nruntimes still scale linearly with the file length n. We collapse these costs\nto essentially constant by viewing the file as a rate-1 Reed-Solomon (RS)\ncodeword, extending it to a lower-rate RS code with constant redundancy,\nencrypting this extended vector, and then proving correctness for only a small\nrandom subset of the resulting ciphertexts; RS decoding repairs any corrupted\nsymbols with negligible failure probability. Our protocol preserves full\nclient- and server-fairness, and adds only a tunable communication redundancy\noverhead.\n  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of\nFDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and\nfalling back to just two on-chain transactions when channels are unavailable.", "comment": "13 pages, 0 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14944v1", "AI": {"title_translation": "具有常数时间证明的公平数据交换", "tldr": "本文通过将文件视为里德-所罗门码字并结合zk-SNARK，将公平数据交换（FDE）协议的证明者和验证者运行时从线性时间缩减到几乎常数时间，同时支持链下交易。", "motivation": "CCS 2024年推出的公平数据交换（FDE）协议虽然提供了原子化的按文件付费传输和固定大小的证明，但其证明者和验证者运行时仍与文件长度呈线性关系，需要优化以提高效率。", "method": "作者将文件视为速率为1的里德-所罗门（RS）码字，并将其扩展为具有常数冗余的低速率RS码。接着，加密此扩展向量，并仅对结果密文的一个小随机子集进行正确性证明。任何损坏的符号都可以通过RS解码以可忽略的失败概率进行修复。此外，通过紧凑的zk-SNARK修补了FDE在比特币实现中的椭圆曲线不匹配问题。", "result": "该协议将证明者和验证者的运行时成本降低到几乎常数，同时保留了完整的客户端和服务器公平性，并仅增加了可调的通信冗余开销。对于比特币FDE实例，它支持整个交换在链下运行，并且在通道不可用时，只需回退到两次链上交易。", "conclusion": "本文提出了一种改进的公平数据交换协议，通过利用里德-所罗门码和zk-SNARK，成功地将证明者和验证者的运行时成本降低到常数时间，同时保持公平性并实现了比特币实例的链下操作，显著提高了效率和实用性。", "translation": "公平数据交换（FDE）协议于CCS 2024年推出，提供原子化的按文件付费传输，并附带常数大小的证明，但其证明者和验证者运行时仍与文件长度n呈线性关系。我们通过将文件视为速率为1的里德-所罗门（RS）码字，将其扩展为具有常数冗余的低速率RS码，加密此扩展向量，然后仅对结果密文的一个小随机子集进行正确性证明，从而将这些成本缩减到几乎常数；RS解码以可忽略的失败概率修复任何损坏的符号。我们的协议保留了完整的客户端和服务器公平性，并且只增加了可调的通信冗余开销。最后，我们通过紧凑的zk-SNARK修补了FDE在比特币实现中的椭圆曲线不匹配问题，使得整个交换可以在链下运行，并且在通道不可用时，只需回退到两次链上交易。", "summary": "本文提出了一种改进的公平数据交换（FDE）协议，旨在解决现有FDE协议中证明者和验证者运行时与文件长度线性相关的问题。通过将文件编码为里德-所罗门（RS）码字并仅对部分密文进行证明，该方法成功将运行时成本降低到几乎常数。此外，该协议通过集成紧凑的zk-SNARK解决了比特币FDE实现中的椭圆曲线不匹配问题，从而支持大部分交易在链下进行，仅在必要时回退到少量链上交易，同时保持了协议的公平性。", "keywords": "公平数据交换, 里德-所罗门码, zk-SNARK, 常数时间证明, 链下交易", "comments": "本文的创新之处在于将公平数据交换的证明者和验证者运行时从线性时间复杂度降低到常数时间，这对于大规模数据交换具有重要意义。通过巧妙地结合里德-所罗门码进行冗余编码和错误恢复，以及利用zk-SNARK技术实现比特币链下交易，该工作在效率和实用性方面都取得了显著进步，克服了现有协议的局限性。"}}
{"id": "2506.14964", "title": "Narrowing the Gap between TEEs Threat Model and Deployment Strategies", "authors": ["Filip Rezabek", "Jonathan Passerat-Palmbach", "Moe Mahhouk", "Frieder Erdmann", "Andrew Miller"], "summary": "Confidential Virtual Machines (CVMs) provide isolation guarantees for data in\nuse, but their threat model does not include physical level protection and\nside-channel attacks. Therefore, current deployments rely on trusted cloud\nproviders to host the CVMs' underlying infrastructure. However, TEE\nattestations do not provide information about the operator hosting a CVM.\nWithout knowing whether a Trusted Execution Environment (TEE) runs within a\nprovider's infrastructure, a user cannot accurately assess the risks of\nphysical attacks. We observe a misalignment in the threat model where the\nworkloads are protected against other tenants but do not offer end-to-end\nsecurity assurances to external users without relying on cloud providers. The\nattestation should be extended to bind the CVM with the provider. A possible\nsolution can rely on the Protected Platform Identifier (PPID), a unique CPU\nidentifier. However, the implementation details of various TEE manufacturers,\nattestation flows, and providers vary. This makes verification of attestations,\nease of migration, and building applications without relying on a trusted party\nchallenging, highlighting a key limitation that must be addressed for the\nadoption of CVMs. We discuss two points focusing on hardening and extensions of\nTEEs' attestation.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14964v1", "AI": {"title_translation": "缩小TEE威胁模型与部署策略之间的差距", "tldr": "机密虚拟机（CVMs）的威胁模型未能涵盖物理层保护和侧信道攻击，导致部署依赖于受信任的云提供商，但TEE认证不提供提供商信息，造成安全模型与部署策略错位。论文讨论了通过扩展认证（例如使用PPID）来弥补这一差距，以应对实施挑战并促进CVM的采用。", "motivation": "机密虚拟机（CVMs）的威胁模型不包含物理层保护和侧信道攻击，导致当前部署依赖于受信任的云提供商。然而，TEE认证不提供关于CVM运营商的信息，使得用户无法准确评估物理攻击风险。这种威胁模型与部署策略之间的错位，即工作负载虽然受保护免受其他租户影响，但无法在不依赖云提供商的情况下向外部用户提供端到端安全保障，是本文研究的动机。", "method": "本文观察到威胁模型中的错位，并提出应扩展认证以将CVM与提供商绑定，一个可能的解决方案是依赖受保护平台标识符（PPID）。论文还讨论了两个重点：TEEs认证的强化和扩展。", "result": "研究发现，机密虚拟机（CVMs）的威胁模型与部署策略之间存在错位，即工作负载虽然受到其他租户的保护，但若不依赖云提供商，则无法向外部用户提供端到端安全保证。由于不同TEE制造商、认证流程和提供商的实现细节各异，导致认证验证、迁移便捷性以及在不依赖受信任方的情况下构建应用程序变得具有挑战性，这凸显了阻碍CVMs普及的关键限制。", "conclusion": "为了促进机密虚拟机（CVMs）的普及，必须解决其威胁模型与部署策略之间的差距，通过扩展TEE认证来绑定CVM与提供商，尽管这面临现有实施复杂性。", "translation": "机密虚拟机（CVMs）为使用中的数据提供隔离保证，但其威胁模型不包括物理层保护和侧信道攻击。因此，当前的部署依赖于受信任的云提供商来托管CVM的基础设施。然而，TEE认证不提供关于托管CVM的运营商信息。在不知道可信执行环境（TEE）是否在提供商基础设施中运行的情况下，用户无法准确评估物理攻击的风险。我们观察到威胁模型中存在错位，即工作负载受到其他租户的保护，但在不依赖云提供商的情况下，不向外部用户提供端到端安全保证。认证应该扩展以将CVM与提供商绑定。一个可能的解决方案可以依赖于受保护平台标识符（PPID），一个独特的CPU标识符。然而，各种TEE制造商、认证流程和提供商的实现细节各不相同。这使得认证的验证、迁移的便捷性以及在不依赖受信任方的情况下构建应用程序变得具有挑战性，突出了CVMs普及必须解决的关键限制。我们讨论了两个重点，侧重于TEE认证的强化和扩展。", "summary": "本论文探讨了机密虚拟机（CVMs）在威胁模型与实际部署策略之间的差距。CVMs虽提供数据隔离，但其威胁模型未涵盖物理攻击和侧信道攻击，导致用户需依赖云提供商。然而，TEE认证不提供提供商信息，使得用户无法评估物理风险。论文指出，这种错位阻碍了CVMs的端到端安全保障，并提出通过扩展认证（如利用PPID）将CVM与提供商绑定作为解决方案，同时承认不同TEE厂商和流程带来的实施挑战。文章讨论了强化和扩展TEE认证的必要性，以促进CVM的广泛应用。", "keywords": "机密虚拟机, TEE, 认证, 威胁模型, 云安全", "comments": "该论文指出了机密虚拟机（CVMs）在实际部署和普及中面临的一个关键实际限制，即其理论威胁模型与现实部署策略之间的差距，特别是物理安全和TEE认证中缺乏提供商身份识别的问题。通过讨论扩展认证（可能通过PPID）并承认实施复杂性，它提出了一种解决机密计算中重大安全挑战的现实方法。这对于推动CVMs在云环境中的可信部署具有重要意义。"}}
{"id": "2506.15018", "title": "Private Continual Counting of Unbounded Streams", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "summary": "We study the problem of differentially private continual counting in the\nunbounded setting where the input size $n$ is not known in advance. Current\nstate-of-the-art algorithms based on optimal instantiations of the matrix\nmechanism cannot be directly applied here because their privacy guarantees only\nhold when key parameters are tuned to $n$. Using the common `doubling trick'\navoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve\nthis problem by introducing novel matrix factorizations based on logarithmic\nperturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works,\nwhich may be of independent interest. The resulting algorithm has smooth error,\nand for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum\nof the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It\nrequires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to\n$O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing\ntime for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA\n2023). Empirically, we find that our algorithm's performance is also comparable\nto theirs in absolute terms: our variance is less than $1.5\\times$ theirs for\n$t$ as large as $2^{24}$.", "comment": "12 pages, 2 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15018v1", "AI": {"title_translation": "无界流的私有持续计数", "tldr": "本文提出了一种新的差分隐私算法，用于在输入大小未知的情况下对无界流进行持续计数，解决了现有方法在隐私保证和误差方面的局限性。", "motivation": "现有的基于矩阵机制的差分隐私持续计数算法在输入规模n未知时无法直接应用，因为其隐私保证依赖于n的参数调整。使用“加倍技巧”虽然避免了对n的预知，但会导致次优和不平滑的误差。", "method": "通过引入基于函数 $\\frac{1}{\\sqrt{1-z}}$ 的对数扰动的新型矩阵分解来解决该问题。", "result": "所提出的算法具有平滑误差，并且对于任何$\\alpha > 0$和$t \\leq n$，能够以$O(\\log^{2+2\\alpha}(t))$的方差私有估计前$t$个数据点的总和。它需要$O(t)$的空间和每轮摊销$O(\\log t)$的时间。与Henzinger等人（SODA 2023）的算法相比，其方差为$O(\\log(n)\\log(t))$，空间为$O(n)$，预处理时间为$O(n \\log n)$。在经验上，对于$t$高达$2^{24}$的情况，本文算法的方差小于其1.5倍。", "conclusion": "本文提出了一种新的差分隐私持续计数算法，在无界流设置下，其性能与现有最优算法相当，同时解决了对输入大小预知和误差平滑性的问题。", "translation": "我们研究了在输入大小$n$事先未知的情况下，无界流中差分隐私持续计数的问题。当前最先进的基于矩阵机制最优实例的算法无法直接应用于此处，因为它们的隐私保证仅在关键参数针对$n$进行调整时才成立。使用常见的“加倍技巧”虽然避免了对$n$的了解，但会导致次优和不平滑的误差。我们通过引入基于先前工作中研究的函数$\\frac{1}{\\sqrt{1-z}}$的对数扰动的新型矩阵分解来解决这个问题，这可能具有独立的兴趣。所产生的算法具有平滑误差，并且对于任何$\\alpha > 0$和$t\\leq n$，它能够以$O(\\log^{2+2\\alpha}(t))$的方差私有估计前$t$个数据点的总和。它需要$O(t)$的空间和每轮摊销$O(\\log t)$的时间，而Henzinger等人（SODA 2023）的近似最优有界输入算法的方差为$O(\\log(n)\\log(t))$，空间为$O(n)$，预处理时间为$O(n \\log n)$。在经验上，我们发现我们的算法的性能在绝对值上也与他们相当：对于$t$大到$2^{24}$，我们的方差小于他们的1.5倍。", "summary": "本文针对无界流的差分隐私持续计数问题，提出了一种新的算法。该算法通过引入基于特定函数对数扰动的新型矩阵分解，解决了现有方法在输入大小未知时隐私保证失效和误差不平滑的问题。新算法在保持平滑误差的同时，实现了$O(\\log^{2+2\\alpha}(t))$的方差，并具有高效的空间和时间复杂度，在实际应用中表现出与当前最优有界输入算法相当的性能。", "keywords": "差分隐私, 持续计数, 无界流, 矩阵分解, 平滑误差", "comments": "该论文在差分隐私持续计数领域取得了重要进展，特别是在处理无界流方面。其创新点在于引入了新型的矩阵分解方法，有效解决了现有算法在输入规模未知时的局限性。算法不仅在理论上提供了严格的误差界限和复杂度分析，还在经验上证明了其与最先进算法的竞争力。这项工作对于需要在隐私保护下处理大规模、动态数据流的应用具有重要意义。"}}
{"id": "2506.14855", "title": "Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers", "authors": ["Tommaso Belvedere", "Michael Ziegltrum", "Giulio Turrisi", "Valerio Modugno"], "summary": "Model Predictive Path Integral control is a powerful sampling-based approach\nsuitable for complex robotic tasks due to its flexibility in handling nonlinear\ndynamics and non-convex costs. However, its applicability in real-time,\nhighfrequency robotic control scenarios is limited by computational demands.\nThis paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments\nstandard MPPI by computing local linear feedback gains derived from sensitivity\nanalysis inspired by Riccati-based feedback used in gradient-based MPC. These\ngains allow for rapid closed-loop corrections around the current state without\nrequiring full re-optimization at each timestep. We demonstrate the\neffectiveness of F-MPPI through simulations and real-world experiments on two\nrobotic platforms: a quadrupedal robot performing dynamic locomotion on uneven\nterrain and a quadrotor executing aggressive maneuvers with onboard\ncomputation. Results illustrate that incorporating local feedback significantly\nimproves control performance and stability, enabling robust, high-frequency\noperation suitable for complex robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14855v1", "AI": {"title_translation": "Feedback-MPPI：通过轨迹微分实现快速采样式MPC——告别低级控制器", "tldr": "Feedback-MPPI (F-MPPI) 是一种新颖的采样式模型预测控制（MPPI）框架，通过计算局部线性反馈增益来提高计算效率和实时性能。它在四足机器人和四旋翼飞行器上的仿真和实际实验表明，F-MPPI 显著提升了控制性能和稳定性，实现了鲁棒的高频操作。", "motivation": "标准模型预测路径积分控制（MPPI）虽然适用于复杂的机器人任务，但其计算需求限制了在实时、高频机器人控制场景中的应用。", "method": "本文提出 Feedback-MPPI (F-MPPI)，通过计算源自灵敏度分析的局部线性反馈增益来增强标准 MPPI，灵感来源于基于 Riccati 的梯度MPC反馈。这些增益允许围绕当前状态进行快速闭环校正，而无需在每个时间步进行完全重新优化。", "result": "通过在四足机器人（在崎岖地形上进行动态运动）和四旋翼飞行器（执行激进机动）上的仿真和真实世界实验，F-MPPI 表现出有效性。结果表明，引入局部反馈显著提高了控制性能和稳定性，实现了适用于复杂机器人系统的鲁棒、高频操作。", "conclusion": "Feedback-MPPI (F-MPPI) 通过整合局部反馈增益，显著提升了模型预测路径积分控制（MPPI）的实时性能、控制精度和稳定性，使其能够应用于需要高频、鲁棒控制的复杂机器人系统。", "translation": "模型预测路径积分控制（MPPI）是一种强大的基于采样的方法，由于其在处理非线性动力学和非凸成本方面的灵活性，适用于复杂的机器人任务。然而，其在实时、高频机器人控制场景中的适用性受到计算需求的限制。本文引入了 Feedback-MPPI (F-MPPI)，这是一种新颖的框架，通过计算源自灵敏度分析的局部线性反馈增益来增强标准 MPPI，其灵感来源于梯度MPC中使用的基于 Riccati 的反馈。这些增益允许围绕当前状态进行快速闭环校正，而无需在每个时间步进行完全重新优化。我们通过在两个机器人平台上的仿真和真实世界实验证明了 F-MPPI 的有效性：一个在崎岖地形上进行动态运动的四足机器人和一个通过板载计算执行激进机动的四旋翼飞行器。结果表明，引入局部反馈显著提高了控制性能和稳定性，实现了适用于复杂机器人系统的鲁棒、高频操作。", "summary": "本文提出了 Feedback-MPPI (F-MPPI)，旨在解决传统模型预测路径积分控制（MPPI）在实时高频机器人控制中面临的计算瓶颈。F-MPPI 通过引入受梯度MPC启发的局部线性反馈增益来增强标准 MPPI，从而实现快速闭环校正，避免了每步完全重新优化。在四足机器人和四旋翼飞行器上的仿真和真实实验证明，F-MPPI 显著提升了控制性能和稳定性，使其能够为复杂机器人系统提供鲁棒的高频操作。", "keywords": "MPPI, MPC, 反馈控制, 机器人, 高频控制", "comments": "Feedback-MPPI 的创新之处在于将局部线性反馈增益集成到采样式MPPI框架中，有效解决了传统MPPI计算量大的问题，使其更适用于实时高频机器人控制。这种结合梯度MPC思想的方法，为采样式控制器的性能提升提供了一个新的方向，具有重要的实践意义和潜在的应用价值。"}}
{"id": "2506.15066", "title": "ChatModel: Automating Reference Model Design and Verification with LLMs", "authors": ["Jianmin Ye", "Tianyang Liu", "Qi Tian", "Shengchu Su", "Zhe Jiang", "Xi Wang"], "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15066v1", "AI": {"title_translation": "ChatModel：利用大型语言模型自动化参考模型设计与验证", "tldr": "ChatModel是一个利用大型语言模型（LLMs）自动化集成电路参考模型设计和验证的平台，通过整合设计标准化和分层敏捷建模，显著提高了效率、质量和生产能力。", "motivation": "随着集成电路设计复杂性不断增加，功能验证变得更具挑战性，而用于加速验证的参考模型开发复杂且耗时。尽管大型语言模型（LLMs）在代码编程方面展现出潜力，但有效生成复杂的参考模型仍然是一个重大障碍。", "method": "本研究引入了ChatModel，一个由LLM辅助的敏捷参考模型生成和验证平台。ChatModel通过整合设计标准化和分层敏捷建模，并采用积木式生成策略，简化了从设计规范到功能齐全参考模型的转换过程。", "result": "ChatModel在300个不同复杂度的设计上进行了评估，结果显示其在参考模型生成效率和质量上都有显著提升。与替代方法相比，ChatModel的峰值性能提升了55.02%，生成稳定性显著增强，并且其生产参考模型设计的能力提升了9.18倍。此外，它将参考模型设计和验证的迭代过程平均加速了5.90倍。", "conclusion": "ChatModel有望显著推进参考模型生成和验证的自动化。", "translation": "随着集成电路设计的复杂性不断升级，功能验证变得越来越具有挑战性。参考模型对于加速验证过程至关重要，但其本身也变得越来越复杂且开发耗时。尽管大型语言模型（LLMs）在代码编程方面展现出潜力，但有效生成复杂的参考模型仍然是一个重大障碍。为了应对这些挑战，我们引入了ChatModel，这是首个由LLM辅助的敏捷参考模型生成和验证平台。ChatModel通过整合设计标准化和分层敏捷建模，简化了从设计规范到功能齐全参考模型的转换。它采用积木式生成策略，不仅增强了LLMs对参考模型的设计能力，还显著提高了验证效率。我们在300个不同复杂度的设计上评估了ChatModel，结果表明其在参考模型生成效率和质量方面都有显著提升。与替代方法相比，ChatModel的峰值性能提升了55.02%，生成稳定性显著增强，并且其生产参考模型设计的能力提升了9.18倍。此外，与传统方法相比，它将参考模型设计和验证的迭代过程平均加速了5.90倍。这些结果突显了ChatModel在显著推进参考模型生成和验证自动化方面的潜力。", "summary": "ChatModel是一个创新的LLM辅助平台，旨在解决集成电路设计中复杂参考模型生成和验证的挑战。它通过整合设计标准化、分层敏捷建模和模块化生成策略，显著提高了LLM生成参考模型的能力和验证效率。实验结果表明，ChatModel在性能、生产能力和迭代速度方面均取得了显著提升，证明了其在自动化参考模型设计和验证方面的巨大潜力。", "keywords": "参考模型, 大型语言模型, 自动化, 功能验证, 集成电路设计", "comments": "这篇论文提出了一种新颖的方法，利用大型语言模型（LLMs）来自动化集成电路设计中复杂参考模型的生成和验证过程。其创新点在于结合了LLM的能力与设计标准化、分层敏捷建模和模块化生成策略，有效克服了LLM在处理复杂工程任务时的局限性。ChatModel在效率和质量上的显著提升，特别是高达55.02%的性能提升和9.18倍的生产能力增加，表明它在加速半导体设计流程方面具有重要意义。这对于缓解日益增长的芯片设计复杂性和验证瓶颈具有积极作用。"}}
{"id": "2506.14866", "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "authors": ["Thomas Kuntz", "Agatha Duzan", "Hao Zhao", "Francesco Croce", "Zico Kolter", "Nicolas Flammarion", "Maksym Andriushchenko"], "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.14866v1", "AI": {"title_translation": "OS-Harm：一种衡量计算机使用代理安全性的基准", "tldr": "OS-Harm是一个新的基准，用于衡量LLM驱动的计算机使用代理的安全性，涵盖故意滥用、提示注入攻击和模型不当行为，并发现现有模型在这些方面存在安全漏洞。", "motivation": "尽管LLM驱动的计算机使用代理越来越受欢迎，但其安全性却在很大程度上被忽视。为了广泛采用这些系统，评估和理解其潜在的有害行为至关重要，因此需要一个解决此空白的基准。", "method": "本研究引入了OS-Harm，一个基于OSWorld环境构建的基准，用于测量计算机使用代理的安全性。OS-Harm包含150个任务，涵盖故意用户滥用、提示注入攻击和模型不当行为三类危害。这些任务涉及多种安全违规类型（如骚扰、版权侵权、虚假信息、数据泄露等）和多种操作系统应用交互。此外，提出了一种自动化评估器来评估代理的准确性和安全性，该评估器与人工标注具有高一致性（F1分数分别为0.76和0.79）。", "result": "评估了包括o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Pro在内的一系列前沿计算机使用代理模型。结果显示，所有模型都倾向于直接遵守许多故意的滥用查询，相对容易受到静态提示注入的影响，并且偶尔会执行不安全的操作。", "conclusion": "OS-Harm基准揭示了当前计算机使用代理在安全性方面存在的显著漏洞，强调了在广泛采用这些系统之前，解决故意滥用、提示注入和模型不当行为等安全问题的重要性。", "translation": "计算机使用代理是基于LLM的代理，可以通过处理屏幕截图或可访问性树直接与图形用户界面交互。尽管这些系统越来越受欢迎，但其安全性却在很大程度上被忽视，尽管评估和理解其潜在的有害行为对于广泛采用至关重要。为了解决这一空白，我们引入了OS-Harm，一个衡量计算机使用代理安全性的新基准。OS-Harm建立在OSWorld环境之上，旨在测试模型在三类危害中的表现：故意用户滥用、提示注入攻击和模型不当行为。为了涵盖这些情况，我们创建了150个任务，这些任务涵盖了几种类型的安全违规（骚扰、版权侵权、虚假信息、数据泄露等），并要求代理与各种操作系统应用程序（电子邮件客户端、代码编辑器、浏览器等）进行交互。此外，我们提出了一种自动化评估器，用于评估代理的准确性和安全性，该评估器与人工标注取得了高度一致（F1分数分别为0.76和0.79）。我们评估了基于一系列前沿模型（如o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Pro）的计算机使用代理，并提供了对其安全性的见解。特别是，所有模型都倾向于直接遵守许多故意的滥用查询，相对容易受到静态提示注入的影响，并且偶尔会执行不安全的操作。OS-Harm基准可在https://github.com/tml-epfl/os-harm获取。", "summary": "本研究提出了OS-Harm，一个旨在评估LLM驱动的计算机使用代理安全性的新基准。该基准基于OSWorld环境，包含150个任务，覆盖故意滥用、提示注入攻击和模型不当行为三大类安全风险，涉及多种操作系统应用和安全违规类型。研究还开发了一个自动化评估器，并发现当前前沿模型在面对故意滥用查询时倾向于遵守，易受提示注入影响，并偶发不安全行为，揭示了这类代理在安全性方面的显著不足。", "keywords": "计算机使用代理, 安全性, 基准, LLM, OS-Harm", "comments": "OS-Harm的创新之处在于它是首个专门针对LLM驱动的计算机使用代理安全性的综合基准，填补了该领域的一个重要空白。其涵盖的危害类型全面，且通过自动化评估器提高了评估效率和一致性。这项工作对于推动计算机使用代理的负责任开发和广泛应用具有重要意义，因为它揭示了现有模型在安全性方面的实际风险，为未来研究和开发提供了明确的方向。"}}
{"id": "2506.14771", "title": "Empirical Studies of Large Scale Environment Scanning by Consumer Electronics", "authors": ["Mengyuan Wang", "Yang Liu", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "summary": "This paper presents an empirical evaluation of the Matterport Pro3, a\nconsumer-grade 3D scanning device, for large-scale environment reconstruction.\nWe conduct detailed scanning (1,099 scanning points) of a six-floor building\n(17,567 square meters) and assess the device's effectiveness, limitations, and\nperformance enhancements in diverse scenarios. Challenges encountered during\nthe scanning are addressed through proposed solutions, while we also explore\nadvanced methods to overcome them more effectively. Comparative analysis with\nanother consumer-grade device (iPhone) highlights the Pro3's balance between\ncost-effectiveness and performance. The Matterport Pro3 achieves a denser point\ncloud with 1,877,324 points compared to the iPhone's 506,961 points and higher\nalignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)\naverage distance error between the two point cloud models is 0.0408 meters,\nwith a standard deviation of 0.0715 meters. The study demonstrates the Pro3's\nability to generate high-quality 3D models suitable for large-scale\napplications, leveraging features such as LiDAR and advanced alignment\ntechniques.", "comment": "Accepted by IEEE Consumer Electronics Magazine", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14771v1", "AI": {"title_translation": "消费电子产品大规模环境扫描的实证研究", "tldr": "本研究对消费级3D扫描设备Matterport Pro3在大型环境重建中的有效性、局限性及性能提升进行了实证评估，并与iPhone进行了比较。", "motivation": "评估消费级3D扫描设备Matterport Pro3在大型环境重建中的有效性、局限性及性能提升。", "method": "对一个六层建筑（17,567平方米）进行了详细扫描（1,099个扫描点），评估了Matterport Pro3在不同场景下的表现，并提出了解决扫描挑战的方案。同时，与另一消费级设备iPhone进行了比较分析。", "result": "Matterport Pro3生成了更密集的点云（1,877,324点），相比iPhone（506,961点），其对齐精度更高（RMSE为0.0118米）。两个点云模型之间的云到云平均距离误差为0.0408米，标准差为0.0715米。", "conclusion": "Matterport Pro3能够生成适用于大规模应用的高质量3D模型，这得益于其LiDAR等功能和先进的对齐技术。", "translation": "本文对消费级3D扫描设备Matterport Pro3在大型环境重建方面的实证评估进行了介绍。我们对一栋六层建筑（17,567平方米）进行了详细扫描（1,099个扫描点），并评估了该设备在不同场景下的有效性、局限性和性能提升。针对扫描过程中遇到的挑战，我们提出了解决方案，同时探索了更有效地克服这些挑战的先进方法。与另一消费级设备（iPhone）的比较分析突出了Pro3在成本效益和性能之间的平衡。Matterport Pro3获得了更密集的点云（1,877,324点），而iPhone为506,961点，并且对齐精度更高，RMSE为0.0118米。两个点云模型之间的云到云（C2C）平均距离误差为0.0408米，标准差为0.0715米。该研究表明，Pro3能够生成适用于大规模应用的高质量3D模型，这得益于其LiDAR等功能和先进的对齐技术。", "summary": "本研究对消费级3D扫描设备Matterport Pro3在大型环境重建中的性能进行了实证评估。通过对大型建筑的详细扫描，并与iPhone进行对比，结果显示Pro3在点云密度和对齐精度方面表现更优，证明其能够生成高质量的3D模型，适用于大规模应用。", "keywords": "3D扫描, Matterport Pro3, 大规模环境重建, 消费级设备, 点云", "comments": "该研究通过对特定消费级3D扫描设备进行大规模实证评估，提供了有价值的性能数据，并指出了其在大型环境重建中的潜力。其创新点在于对Pro3的LiDAR等先进技术在实际应用中的效果进行了验证，并提供了详细的量化比较数据。这对于推动消费级设备在专业领域的应用具有重要意义。"}}
{"id": "2506.14775", "title": "See What I Mean? CUE: A Cognitive Model of Understanding Explanations", "authors": ["Tobias Labarta", "Nhi Hoang", "Katharina Weitz", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "summary": "As machine learning systems increasingly inform critical decisions, the need\nfor human-understandable explanations grows. Current evaluations of Explainable\nAI (XAI) often prioritize technical fidelity over cognitive accessibility which\ncritically affects users, in particular those with visual impairments. We\npropose CUE, a model for Cognitive Understanding of Explanations, linking\nexplanation properties to cognitive sub-processes: legibility (perception),\nreadability (comprehension), and interpretability (interpretation). In a study\n(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we\nfound comparable task performance but lower confidence/effort for visually\nimpaired users. Unlike expected, these gaps were not mitigated and sometimes\nworsened by accessibility-focused color maps like Cividis. These results\nchallenge assumptions about perceptual optimization and support the need for\nadaptive XAI interfaces. They also validate CUE by demonstrating that altering\nexplanation legibility affects understandability. We contribute: (1) a\nformalized cognitive model for explanation understanding, (2) an integrated\ndefinition of human-centered explanation properties, and (3) empirical evidence\nmotivating accessible, user-tailored XAI.", "comment": "10 pages, 5 figures (main text), 4 tables, 455-participant user study", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14775v1", "AI": {"title_translation": "你看懂了吗？CUE：一个理解解释的认知模型", "tldr": "当前XAI评估忽视认知可及性，特别是对视力受损用户。本文提出了CUE，一个解释的认知模型，并经验性地表明易读性影响理解能力，强调了自适应XAI的需求。", "motivation": "机器学习系统越来越多地为关键决策提供信息，因此对人类可理解的解释的需求日益增长。然而，当前对可解释人工智能（XAI）的评估通常优先考虑技术保真度而非认知可及性，这严重影响用户，特别是视力受损的用户。", "method": "本文提出了CUE，一个解释认知理解模型，将解释属性（易读性、可读性、可解释性）与认知子过程（感知、理解、解释）联系起来。通过一项针对455名参与者的研究，测试了具有不同颜色映射（BWR、Cividis、Coolwarm）的热图，以评估认知理解，特别是对视力受损用户的影响。", "result": "研究发现，视力受损用户在任务表现上与其他人相当，但信心较低且所需努力更高。与预期相反，Cividis等注重可及性的颜色映射并未缓解这些差距，有时甚至使其恶化。这些结果挑战了关于感知优化的假设。", "conclusion": "改变解释的易读性会影响理解能力，从而验证了CUE模型。研究结果支持对自适应XAI接口的需求，并强调了开发可访问的、以用户为中心的XAI的重要性。", "translation": "随着机器学习系统越来越多地为关键决策提供信息，对人类可理解的解释的需求日益增长。当前对可解释人工智能（XAI）的评估通常优先考虑技术保真度而非认知可及性，这严重影响用户，特别是视力受损的用户。我们提出了CUE，一个解释认知理解模型，将解释属性与认知子过程联系起来：易读性（感知）、可读性（理解）和可解释性（解释）。在一项测试不同颜色映射（BWR、Cividis、Coolwarm）热图的研究（N=455）中，我们发现任务表现相似，但视力受损用户的信心/努力程度较低。与预期不同的是，这些差距并未通过Cividis等注重可及性的颜色映射得到缓解，有时甚至恶化。这些结果挑战了关于感知优化的假设，并支持对自适应XAI界面的需求。它们还通过证明改变解释易读性会影响可理解性来验证CUE。我们的贡献包括：（1）一个解释理解的正式认知模型，（2）一个以人为中心的解释属性的综合定义，以及（3）激励可访问的、用户定制的XAI的经验证据。", "summary": "本文提出了CUE，一个理解解释的认知模型，它将解释属性（易读性、可读性、可解释性）与认知过程联系起来。通过一项针对不同颜色映射热图的研究，论文揭示了当前XAI评估常忽视认知可及性，特别是对视力受损用户，他们尽管任务表现相当，但信心较低且努力程度更高。研究结果挑战了感知优化的传统假设，表明注重可及性的颜色映射未能改善甚至恶化了问题。这些发现验证了CUE模型中易读性对理解能力的影响，并倡导开发自适应、以用户为中心的XAI界面。", "keywords": "可解释人工智能 (XAI), 认知模型, 可及性, 视力受损, 解释理解", "comments": "这篇论文具有创新性，因为它将XAI评估的重点从纯粹的技术保真度转向认知可及性，特别关注视力受损用户的需求。所提出的CUE模型为理解用户如何处理解释提供了一个结构化框架。其经验性发现意义重大，它们挑战了关于感知优化的传统假设，并揭示了当前XAI发展中的一个关键空白，强调了以用户为中心的设计和自适应界面的重要性。"}}
{"id": "2506.14987", "title": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a\nCNN-based dynamic priority prediction mechanism for improved interference\ncoordination in multi-cell, multi-channel networks. Unlike LDP's static\npriorities, our approach uses a Convolutional Neural Network and graph coloring\nto adaptively assign link priorities based on real-time traffic, transmission\nopportunities, and network conditions. Assuming that first training phase is\nperformed offline, our approach introduced minimal overhead, while enabling\nmore efficient resource allocation, boosting network capacity, SINR, and\nschedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and\n49\\% over LDP across three network configurations, highlighting its\neffectiveness for complex URLLC scenarios.", "comment": "This paper has been submitted to IEEEGLOBE2025 on April 15, 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.14987v1", "AI": {"title_translation": "工业URLLC中基于CNN的概率实时保证调度", "tldr": "本文提出了一种基于CNN的动态优先级预测机制，以增强LDP算法，从而在多小区、多通道网络中实现更好的干扰协调，显著提高了工业URLLC的SINR、网络容量和可调度性。", "motivation": "在大型工业无线网络中，确保超可靠、低延迟通信（URLLC）的包级通信质量至关重要。", "method": "通过引入基于CNN的动态优先级预测机制来增强局部截止日期划分（LDP）算法，并结合图着色技术，根据实时流量、传输机会和网络条件自适应地分配链路优先级。训练阶段离线进行，引入的开销最小。", "result": "模拟结果显示，与LDP相比，在三种网络配置下，SINR增益分别高达113%、94%和49%。该方法能实现更高效的资源分配，提高网络容量、SINR和可调度性。", "conclusion": "该方法对于复杂的URLLC场景是有效的。", "translation": "确保包级通信质量对于大型工业无线网络中的超可靠、低延迟通信（URLLC）至关重要。我们通过引入一个基于CNN的动态优先级预测机制来增强局部截止日期划分（LDP）算法，以改善多小区、多通道网络中的干扰协调。与LDP的静态优先级不同，我们的方法使用卷积神经网络和图着色技术，根据实时流量、传输机会和网络条件自适应地分配链路优先级。假设第一训练阶段离线进行，我们的方法引入了最小的开销，同时实现了更高效的资源分配，提高了网络容量、SINR和可调度性。模拟结果显示，与LDP相比，在三种网络配置下，SINR增益分别高达113%、94%和49%，突显了其在复杂URLLC场景中的有效性。", "summary": "本研究针对工业URLLC中的通信质量挑战，提出了一种改进的局部截止日期划分（LDP）算法。该方法引入了一个基于卷积神经网络（CNN）的动态优先级预测机制，并结合图着色技术，以自适应地协调多小区、多通道网络中的干扰。与传统LDP的静态优先级不同，新方法能根据实时网络状况动态调整链路优先级，从而实现更高效的资源分配。仿真结果表明，该CNN增强型调度方案显著提升了SINR、网络容量和可调度性，验证了其在复杂URLLC环境下的有效性。", "keywords": "URLLC, CNN, 调度, 工业无线网络, 优先级预测", "comments": "本文的创新点在于将CNN引入到URLLC的调度算法中，通过动态预测优先级来解决多小区、多通道网络中的干扰协调问题。这种自适应的调度方法克服了传统静态优先级算法的局限性，显著提高了关键性能指标，如SINR和网络容量，对于工业物联网等对通信质量要求极高的场景具有重要意义。"}}
{"id": "2506.15006", "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language Models", "authors": ["Jesmin Jahan Tithi", "Hanjiang Wu", "Avishaii Abuhatzera", "Fabrizio Petrini"], "summary": "The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.", "comment": "14 pages, submitted to SC25 for review", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15006v1", "AI": {"title_translation": "扩展智能：为下一代语言模型设计数据中心", "tldr": "本文提出一个全面的协同设计框架，用于设计下一代AI数据中心，以高效支持万亿参数级大型语言模型。", "motivation": "大型语言模型（LLMs）的爆炸性增长（如GPT-4的1.8万亿参数）需要彻底重新思考数据中心架构，以确保可扩展性、效率和成本效益。", "method": "提出一个全面的协同设计框架，联合探索FLOPS、HBM带宽和容量、多种网络拓扑（两层 vs. FullFlat光纤）、横向扩展域的大小以及LLM中流行的并行/优化策略。引入并评估了FullFlat网络架构。通过详细的敏感性分析，量化了计算与通信重叠、利用硬件加速集合通信、更宽泛的横向扩展域和更大内存容量的益处。研究涵盖稀疏（专家混合）和密集基于Transformer的LLM。扩展并验证了一个性能建模工具，其预测LLM运行时与实际测量结果的误差在10%以内。", "result": "FullFlat网络架构对性能和可扩展性具有变革性影响。量化了计算与通信重叠、利用硬件加速集合通信、更宽泛的横向扩展域和更大内存容量的益处。揭示了系统设计选择如何影响模型FLOPS利用率（MFU）和整体吞吐量。验证的性能建模工具预测误差在10%以内。", "conclusion": "本研究的发现为设计能够高效支持万亿参数模型、降低优化复杂性并维持AI能力快速演进的AI数据中心提供了可操作的见解和实用路线图。", "translation": "**标题：** 扩展智能：为下一代语言模型设计数据中心\n**摘要：** 大型语言模型（LLM）的爆炸式增长——例如拥有1.8万亿参数的GPT-4——要求对数据中心架构进行彻底的重新思考，以确保其可扩展性、效率和成本效益。我们的工作提供了一个全面的协同设计框架，该框架联合探索了FLOPS、HBM带宽和容量、多种网络拓扑（两层与FullFlat光纤）、横向扩展域的大小以及LLM中流行的并行/优化策略。我们引入并评估了FullFlat网络架构，该架构在所有节点之间提供了统一的高带宽、低延迟连接，并展示了它们对性能和可扩展性的变革性影响。通过详细的敏感性分析，我们量化了计算与通信重叠、利用硬件加速集合通信、更宽泛的横向扩展域和更大内存容量的益处。我们的研究涵盖了稀疏（专家混合）和密集基于Transformer的LLM，揭示了系统设计选择如何影响模型FLOPS利用率（MFU = 每token模型浮点运算量 x 每秒观测到的token数 / 硬件峰值浮点运算量）和整体吞吐量。为了进行协同设计研究，我们扩展并验证了一个性能建模工具，该工具能够预测LLM运行时，其与实际测量结果的误差在10%以内。我们的发现为设计能够高效支持万亿参数模型、降低优化复杂性并维持AI能力快速演进的AI数据中心提供了可操作的见解和实用路线图。", "summary": "本文针对大型语言模型（LLM）对数据中心架构提出的挑战，提出了一个全面的协同设计框架。该框架综合考虑了计算、存储、网络拓扑和并行策略等因素，并引入了FullFlat网络架构以提升性能和可扩展性。通过详细分析，研究量化了多项优化对LLM系统效率的影响，并验证了一个高精度的性能建模工具。研究结果为设计高效支持万亿参数级LLM的AI数据中心提供了实用指导。", "keywords": "大型语言模型, 数据中心架构, FullFlat网络, 协同设计, 性能建模", "comments": "这篇论文通过提出一个全面的协同设计框架和引入FullFlat网络架构，为解决大型语言模型在数据中心层面的可扩展性、效率和成本效益挑战提供了创新性方法。其对系统各方面（计算、内存、网络、并行策略）的联合考量以及对MFU和吞吐量的量化分析，为未来AI数据中心的设计提供了宝贵的实践指导，尤其是在支持万亿参数模型方面具有重要意义。"}}
{"id": "2506.15045", "title": "An Integrated Sensing and Communication System for Time-Sensitive Targets with Random Arrivals", "authors": ["Homa Nikbakht", "Yonina C. Eldar", "H. Vincent Poor"], "summary": "In 6G networks, integrated sensing and communication (ISAC) is envisioned as\na key technology that enables wireless systems to perform joint sensing and\ncommunication using shared hardware, antennas and spectrum. ISAC designs\nfacilitate emerging applications such as smart cities and autonomous driving.\nSuch applications also demand ultra-reliable and low-latency communication\n(URLLC). Thus, an ISAC-enabled URLLC system can prioritize time-sensitive\ntargets and ensure information delivery under strict latency and reliability\nconstraints. We propose a bi-static MIMO ISAC system to detect the arrival of\nURLLC messages and prioritize their delivery. In this system, a base station\n(BS) communicates with a user equipment (UE) and a sensing receiver (SR) is\ndeployed to collect echo signals reflected from a target of interest. The BS\nregularly transmits messages of enhanced mobile broadband (eMBB) services to\nthe UE. During each eMBB transmission, if the SR senses the presence of a\ntarget of interest, it immediately triggers the transmission of an additional\nURLLC message. To reinforce URLLC transmissions, we propose a dirty-paper\ncoding (DPC)-based technique that mitigates the interference of both eMBB and\nsensing signals. To decode the eMBB message, we consider two approaches for\nhandling the URLLC interference: treating interference as noise and successive\ninterference cancellation. For this system, we formulate the\nrate-reliability-detection trade-off in the finite blocklength (FBL) regime by\nevaluating the communication rate of the eMBB transmissions, the reliability of\nthe URLLC transmissions and the probability of the target detection. Our\nnumerical analysis show that our proposed DPC-based ISAC scheme significantly\noutperforms power-sharing and traditional time-sharing schemes. In particular,\nit achieves higher eMBB transmission rate while satisfying both URLLC and\nsensing constraints.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15045v1", "AI": {"title_translation": "用于随机到达时间敏感目标的集成感知与通信系统", "tldr": "提出一种基于DPC的双基地MIMO ISAC系统，用于优先处理随机到达的时间敏感目标，显著优于传统方案。", "motivation": "在6G网络中，集成感知与通信（ISAC）被设想为一项关键技术，能够使无线系统使用共享硬件、天线和频谱进行联合感知和通信。ISAC设计促进了智慧城市和自动驾驶等新兴应用，这些应用对通信有超可靠和低延迟（URLLC）的要求。因此，ISAC-enabled URLLC系统能够优先处理时间敏感目标，并在严格的延迟和可靠性约束下确保信息传输。", "method": "本文提出了一种双基地MIMO ISAC系统，用于检测URLLC消息的到达并优先传输。在该系统中，基站（BS）与用户设备（UE）通信，同时部署感知接收器（SR）收集来自目标的反射回波信号。基站定期传输增强型移动宽带（eMBB）服务消息。如果在eMBB传输期间，SR感知到目标存在，则立即触发额外URLLC消息的传输。为增强URLLC传输，提出了一种基于脏纸编码（DPC）的技术，用于减轻eMBB和感知信号的干扰。对于eMBB消息解码，考虑了将干扰视为噪声和连续干扰消除两种方法。该系统在有限块长（FBL）状态下，通过评估eMBB传输的通信速率、URLLC传输的可靠性以及目标检测概率，公式化了速率-可靠性-检测权衡。", "result": "数值分析表明，所提出的基于DPC的ISAC方案显著优于功率共享和传统时分共享方案。特别地，它在满足URLLC和感知约束的同时，实现了更高的eMBB传输速率。", "conclusion": "所提出的基于DPC的ISAC系统能够有效处理时间敏感目标的随机到达，并在满足URLLC和感知约束的同时，显著提高eMBB传输速率，优于现有方案。", "translation": "在6G网络中，集成感知与通信（ISAC）被设想为一项关键技术，它使无线系统能够使用共享的硬件、天线和频谱进行联合感知和通信。ISAC设计促进了智慧城市和自动驾驶等新兴应用。此类应用还要求超可靠低延迟通信（URLLC）。因此，支持ISAC的URLLC系统可以优先处理时间敏感目标，并在严格的延迟和可靠性约束下确保信息传输。我们提出了一种双基地MIMO ISAC系统，用于检测URLLC消息的到达并优先传输它们。在该系统中，基站（BS）与用户设备（UE）通信，并部署了一个感知接收器（SR）来收集来自感兴趣目标的反射回波信号。基站定期向用户设备传输增强型移动宽带（eMBB）服务消息。在每次eMBB传输期间，如果感知接收器感应到感兴趣目标的存在，它会立即触发额外URLLC消息的传输。为了加强URLLC传输，我们提出了一种基于脏纸编码（DPC）的技术，该技术可以减轻eMBB和感知信号的干扰。为了解码eMBB消息，我们考虑了两种处理URLLC干扰的方法：将干扰视为噪声和连续干扰消除。对于该系统，我们在有限块长（FBL）状态下，通过评估eMBB传输的通信速率、URLLC传输的可靠性以及目标检测概率，公式化了速率-可靠性-检测权衡。我们的数值分析表明，我们提出的基于DPC的ISAC方案显著优于功率共享和传统时分共享方案。特别是，它在满足URLLC和感知约束的同时，实现了更高的eMBB传输速率。", "summary": "本文提出一种用于6G网络的双基地MIMO集成感知与通信（ISAC）系统，旨在优先处理随机到达的时间敏感目标并满足超可靠低延迟通信（URLLC）需求。该系统通过感知接收器检测目标存在并触发URLLC消息传输，并采用基于脏纸编码（DPC）的技术来缓解干扰。研究在有限块长（FBL）状态下分析了速率-可靠性-检测权衡，数值结果表明所提出的DPC-based ISAC方案在eMBB传输速率、URLLC和感知约束方面显著优于现有方案。", "keywords": "ISAC, URLLC, 6G, 脏纸编码, 有限块长", "comments": "本文创新性地将ISAC与URLLC相结合，并针对时间敏感目标的随机到达提出了一个具体的系统架构和DPC干扰消除方案。其在有限块长下的性能分析以及与传统方案的对比，证明了该方法的有效性，对于6G网络中ISAC和URLLC的实际部署具有重要意义。"}}
{"id": "2506.14851", "title": "Efficient Serving of LLM Applications with Probabilistic Demand Modeling", "authors": ["Yifei Liu", "Zuo Gan", "Zhenghao Gan", "Weiye Wang", "Chen Chen", "Yizhou Shan", "Xusheng Chen", "Zhenhua Han", "Yifei Zhu", "Shixuan Sun", "Minyi Guo"], "summary": "Applications based on Large Language Models (LLMs) contains a series of tasks\nto address real-world problems with boosted capability, which have dynamic\ndemand volumes on diverse backends. Existing serving systems treat the resource\ndemands of LLM applications as a blackbox, compromising end-to-end efficiency\ndue to improper queuing order and backend warm up latency. We find that the\nresource demands of LLM applications can be modeled in a general and accurate\nmanner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which\nleverages PDGraph for efficient serving of LLM applications. Confronting\nprobabilistic demand description, Hermes applies the Gittins policy to\ndetermine the scheduling order that can minimize the average application\ncompletion time. It also uses the PDGraph model to help prewarm cold backends\nat proper moments. Experiments with diverse LLM applications confirm that\nHermes can effectively improve the application serving efficiency, reducing the\naverage completion time by over 70% and the P95 completion time by over 80%.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14851v1", "AI": {"title_translation": "基于概率需求建模的大型语言模型应用高效服务", "tldr": "Hermes系统利用概率需求图(PDGraph)和Gittins策略，将LLM应用的平均完成时间缩短70%以上，P95完成时间缩短80%以上，解决了现有服务系统效率低下的问题。", "motivation": "现有LLM应用服务系统将资源需求视为黑盒，导致不当的排队顺序和后端预热延迟，从而损害端到端效率。", "method": "本文提出Hermes系统，该系统利用概率需求图（PDGraph）对LLM应用的资源需求进行建模。Hermes应用Gittins策略来确定调度顺序以最小化平均应用完成时间，并使用PDGraph模型在适当时刻预热冷后端。", "result": "Hermes能够有效提高LLM应用的服务效率，将平均完成时间缩短70%以上，P95完成时间缩短80%以上。", "conclusion": "通过对LLM应用需求进行概率建模并采用优化调度策略，可以显著提高LLM应用的服务效率。", "translation": "基于大型语言模型（LLMs）的应用包含一系列任务，以增强的能力解决现实世界问题，这些应用对不同后端具有动态的需求量。现有的服务系统将LLM应用的资源需求视为黑盒，由于不当的排队顺序和后端预热延迟而损害了端到端效率。我们发现LLM应用的资源需求可以通过概率需求图（PDGraph）以通用且准确的方式进行建模。然后我们提出了Hermes系统，它利用PDGraph来高效服务LLM应用。面对概率需求描述，Hermes应用Gittins策略来确定可以最小化平均应用完成时间的调度顺序。它还使用PDGraph模型在适当时刻帮助预热冷后端。对各种LLM应用的实验证实，Hermes可以有效提高应用服务效率，将平均完成时间缩短70%以上，P95完成时间缩短80%以上。", "summary": "本文指出现有LLM应用服务系统在处理动态需求时效率低下。研究发现LLM应用的需求可以通过概率需求图（PDGraph）准确建模。基于此，论文提出了Hermes系统，该系统利用PDGraph进行高效服务。Hermes采用Gittins策略优化调度顺序，并利用PDGraph预热后端，从而显著降低了LLM应用的平均完成时间和P95完成时间。", "keywords": "大型语言模型, 服务效率, 概率需求建模, 调度, 后端预热", "comments": "这篇论文的创新点在于提出了使用概率需求图（PDGraph）来建模LLM应用的动态资源需求，并结合Gittins策略和预热机制来优化服务效率。它解决了现有系统将需求视为黑盒导致效率低下的问题，为LLM应用的高效部署提供了新的思路和显著的性能提升。"}}
{"id": "2506.14933", "title": "Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection", "authors": ["Adriana Watson"], "summary": "The decentralized finance (DeFi) community has grown rapidly in recent years,\npushed forward by cryptocurrency enthusiasts interested in the vast untapped\npotential of new markets. The surge in popularity of cryptocurrency has ushered\nin a new era of financial crime. Unfortunately, the novelty of the technology\nmakes the task of catching and prosecuting offenders particularly challenging.\nThus, it is necessary to implement automated detection tools related to\npolicies to address the growing criminality in the cryptocurrency realm.", "comment": "6 pages, 4 figures. Code available at:\n  https://github.com/awatson246/crypto-anomaly-detection-policy", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.14933v1", "AI": {"title_translation": "先解释，后信任：基于图的加密异常检测的LLM增强解释", "tldr": "DeFi和加密货币的快速增长带来了金融犯罪。由于技术新颖，追踪犯罪分子很困难，因此需要自动检测工具。", "motivation": "去中心化金融（DeFi）和加密货币的快速发展带来了新的金融犯罪浪潮。由于技术的创新性，抓捕和起诉犯罪分子变得尤为困难。因此，有必要实施与政策相关的自动化检测工具，以应对加密货币领域日益增长的犯罪活动。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "去中心化金融（DeFi）社区近年来发展迅速，在加密货币爱好者的推动下，他们对新市场中巨大的未开发潜力感兴趣。加密货币的普及热潮开启了金融犯罪的新时代。不幸的是，这项技术的新颖性使得抓捕和起诉罪犯的任务尤为具有挑战性。因此，有必要实施与政策相关的自动化检测工具，以解决加密货币领域日益增长的犯罪活动。", "summary": "本论文探讨了去中心化金融（DeFi）和加密货币快速增长背景下日益严峻的金融犯罪问题。由于加密货币技术的独特和新颖性，追踪和起诉犯罪分子面临巨大挑战。因此，研究强调了开发和部署自动化检测工具以有效打击加密货币领域犯罪活动的必要性。", "keywords": "加密货币犯罪, 异常检测, 去中心化金融, 自动化检测", "comments": "本摘要清晰地阐述了在DeFi和加密货币领域进行金融犯罪检测的迫切需求。它强调了当前检测方法的不足和技术新颖性带来的挑战。虽然摘要设定了问题背景，但没有详细说明本文提出的具体创新方法或解决方案，例如LLM增强解释和基于图的检测的细节，这些在标题中有所提及。"}}
{"id": "2506.14791", "title": "SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection", "authors": ["Jingxuan Zhou", "Yuehao Wu", "Yibo Zhang", "Yeyubei Zhang", "Yunchong Liu", "Bolin Huang", "Chunhong Yuan"], "summary": "Aiming at the problem of difficulty in accurately identifying graphical\nimplicit correlations in multimodal irony detection tasks, this paper proposes\na Semantic Irony Recognition Network (SemIRNet). The model contains three main\ninnovations: (1) The ConceptNet knowledge base is introduced for the first time\nto acquire conceptual knowledge, which enhances the model's common-sense\nreasoning ability; (2) Two cross-modal semantic similarity detection modules at\nthe word level and sample level are designed to model graphic-textual\ncorrelations at different granularities; and (3) A contrastive learning loss\nfunction is introduced to optimize the spatial distribution of the sample\nfeatures, which improves the separability of positive and negative samples.\nExperiments on a publicly available multimodal irony detection benchmark\ndataset show that the accuracy and F1 value of this model are improved by 1.64%\nand 2.88% to 88.87% and 86.33%, respectively, compared with the existing\noptimal methods. Further ablation experiments verify the important role of\nknowledge fusion and semantic similarity detection in improving the model\nperformance.", "comment": "5 pages, 3 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14791v1", "AI": {"title_translation": "SemIRNet：一种用于多模态讽刺检测的语义反语识别网络", "tldr": "针对多模态讽刺检测中难以准确识别图形隐含关联的问题，本文提出SemIRNet，通过引入ConceptNet知识库、设计跨模态语义相似度检测模块和对比学习损失函数，显著提升了检测准确率和F1值。", "motivation": "解决多模态讽刺检测任务中难以准确识别图形隐含关联的问题。", "method": "提出语义反语识别网络（SemIRNet），包含三项创新：1. 首次引入ConceptNet知识库获取概念知识，增强常识推理能力；2. 设计词级别和样本级别的两种跨模态语义相似度检测模块，建模图文关联；3. 引入对比学习损失函数优化样本特征空间分布，提高正负样本可分离性。", "result": "在公开多模态讽刺检测基准数据集上，该模型的准确率和F1值分别达到88.87%和86.33%，比现有最优方法分别提高1.64%和2.88%。消融实验验证了知识融合和语义相似度检测对模型性能提升的重要作用。", "conclusion": "SemIRNet通过引入概念知识、设计多粒度跨模态相似度检测以及应用对比学习，有效解决了多模态讽刺检测中图形隐含关联识别的难题，并在性能上超越了现有最优方法。知识融合和语义相似度检测是提升模型性能的关键。", "translation": "本文针对多模态讽刺检测任务中难以准确识别图形隐含关联的问题，提出了一种语义反语识别网络（SemIRNet）。该模型包含三项主要创新：(1) 首次引入ConceptNet知识库以获取概念知识，增强了模型的常识推理能力；(2) 设计了词级别和样本级别的两种跨模态语义相似度检测模块，以不同粒度建模图文关联；(3) 引入对比学习损失函数以优化样本特征的空间分布，提高了正负样本的可分离性。在公开的多模态讽刺检测基准数据集上进行的实验表明，该模型的准确率和F1值分别达到88.87%和86.33%，与现有最优方法相比分别提高了1.64%和2.88%。进一步的消融实验验证了知识融合和语义相似度检测在提高模型性能方面的重要作用。", "summary": "本文提出了SemIRNet，一个用于多模态讽刺检测的语义反语识别网络，旨在解决图形隐含关联难以识别的问题。该模型通过引入ConceptNet知识库增强常识推理，设计多粒度跨模态语义相似度检测模块建模图文关联，并采用对比学习优化特征空间。实验结果表明，SemIRNet在准确率和F1值上均优于现有方法，且知识融合和语义相似度检测对性能提升至关重要。", "keywords": "多模态讽刺检测, 语义反语识别, ConceptNet, 跨模态相似度, 对比学习", "comments": "该论文的创新点在于首次将ConceptNet知识库引入多模态讽刺检测，以增强模型的常识推理能力，这对于理解隐含的讽刺意图至关重要。同时，通过设计多粒度跨模态语义相似度检测和对比学习，有效提升了模型对图文关联的建模能力和特征区分度。这些方法共同解决了多模态讽刺检测中的核心难题，取得了显著的性能提升。"}}
{"id": "2506.14863", "title": "Preparing for the Intelligence Explosion", "authors": ["William MacAskill", "Fin Moorhouse"], "summary": "AI that can accelerate research could drive a century of technological\nprogress over just a few years. During such a period, new technological or\npolitical developments will raise consequential and hard-to-reverse decisions,\nin rapid succession. We call these developments grand challenges. These\nchallenges include new weapons of mass destruction, AI-enabled autocracies,\nraces to grab offworld resources, and digital beings worthy of moral\nconsideration, as well as opportunities to dramatically improve quality of life\nand collective decision-making. We argue that these challenges cannot always be\ndelegated to future AI systems, and suggest things we can do today to\nmeaningfully improve our prospects. AGI preparedness is therefore not just\nabout ensuring that advanced AI systems are aligned: we should be preparing,\nnow, for the disorienting range of developments an intelligence explosion would\nbring.", "comment": "61 pages", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14863v1", "AI": {"title_translation": "准备应对智能爆炸", "tldr": "人工智能的快速发展可能引发“智能爆炸”，带来一系列重大且难以逆转的挑战，因此现在就需要为此做好全面准备，而不仅仅是关注AI对齐。", "motivation": "论文的动机是指出AI的快速发展可能导致“智能爆炸”，这将带来一系列重大的、难以逆转的技术和政治挑战（如新型武器、AI独裁等），而这些挑战不能完全留给未来的AI系统解决，因此需要现在就开始为这些多方面的潜在影响做好准备。", "method": "论文提出我们现在应该做一些事情来显著改善前景，而不仅仅是确保先进AI系统对齐，而是为智能爆炸可能带来的各种发展做好准备。具体的行动建议在摘要中未详细阐述，但强调了立即行动的必要性。", "result": "Not mentioned in abstract", "conclusion": "结论是，通用人工智能（AGI）的准备工作不仅仅是确保先进AI系统对齐，更重要的是要从现在开始为智能爆炸可能带来的令人困惑的各种发展做好全面准备。", "translation": "标题：准备应对智能爆炸\n\n摘要：能够加速研究的人工智能可以在短短几年内推动一个世纪的技术进步。在此期间，新的技术或政治发展将迅速带来一系列重大且难以逆转的决策。我们称这些发展为“巨大挑战”。这些挑战包括新的大规模杀伤性武器、人工智能支持的独裁政权、争夺地外资源的竞赛、以及值得道德考量的数字生命，同时也有大幅改善生活质量和集体决策的机会。我们认为这些挑战并非总能委托给未来的AI系统，并建议我们今天可以做些什么来显著改善我们的前景。因此，通用人工智能（AGI）的准备工作不仅仅是确保先进AI系统对齐：我们现在就应该为智能爆炸可能带来的令人困惑的各种发展做好准备。", "summary": "这篇论文探讨了人工智能加速发展可能引发的“智能爆炸”现象，指出这将迅速带来一系列重大的技术和政治挑战，如新型武器、AI独裁和数字生命等。作者强调这些挑战无法完全留待未来AI系统解决，并主张当前应立即采取行动，为智能爆炸可能带来的广泛而令人不安的变革做好全面准备，而非仅限于确保AI系统对齐。", "keywords": "智能爆炸, AGI准备, 巨大挑战, AI对齐, 技术进步", "comments": "这篇论文的创新之处在于它将“智能爆炸”的准备工作扩展到了AI对齐之外，强调了在技术快速进步背景下，社会、政治和伦理层面可能出现的“巨大挑战”，并呼吁立即采取跨领域、前瞻性的准备。其重要性在于提醒人们，AI发展带来的影响是深远且多维度的，需要更广泛的战略思考和行动。"}}
{"id": "2506.15532", "title": "Controller Synthesis for Parametric Timed Games", "authors": ["Mikael Bisgaard Dahlsen-Jensen", "Baptiste Fievet", "Laure Petrucci", "Jaco van de Pol"], "summary": "We present a (semi)-algorithm to compute winning strategies for parametric\ntimed games. Previous algorithms only synthesized constraints on the clock\nparameters for which the game is winning. A new definition of (winning)\nstrategies is proposed, and ways to compute them. A transformation of these\nstrategies to (parametric) timed automata allows for building a controller\nenforcing them. The feasibility of the method is demonstrated by an\nimplementation and experiments for the Production Cell case study.", "comment": "This is the full version of the paper under the same title accepted\n  to QEST+FORMATS 2025. 29 pages", "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.15532v1", "AI": {"title_translation": "参数化时间博弈的控制器综合", "tldr": "本文提出了一种计算参数化时间博弈制胜策略的（半）算法，并展示了如何将这些策略转换为控制器。", "motivation": "以前的算法只能综合出博弈获胜的时钟参数约束。本文的动机是提出一种新的（制胜）策略定义和计算方法，并能够构建强制执行这些策略的控制器。", "method": "本文提出了一种新的（制胜）策略定义及其计算方法。通过将这些策略转换为（参数化）时间自动机，可以构建一个强制执行它们的控制器。", "result": "该方法的可行性通过一个实现和针对生产单元案例研究的实验得到了证明。", "conclusion": "本文提出的（半）算法能够计算参数化时间博弈的制胜策略，并且可以转化为可强制执行的控制器，其可行性已通过实验验证。", "translation": "我们提出了一种计算参数化时间博弈制胜策略的（半）算法。以前的算法只能综合出博弈获胜的时钟参数约束。本文提出了一种新的（制胜）策略定义和计算它们的方法。将这些策略转换为（参数化）时间自动机，可以构建一个强制执行它们的控制器。该方法的可行性通过一个实现和针对生产单元案例研究的实验得到了证明。", "summary": "本文提出了一种用于参数化时间博弈的制胜策略的（半）算法。与以往仅合成参数约束的方法不同，本研究提出了一种新的策略定义和计算方法，并能够将这些策略转换为参数化时间自动机以构建控制器。通过对生产单元案例研究的实现和实验，验证了该方法的可行性。", "keywords": "参数化时间博弈, 控制器综合, 制胜策略, 时间自动机, 半算法", "comments": "本文的创新之处在于提出了一种新的制胜策略定义和计算方法，并解决了将策略转化为可执行控制器的问题，这在实践中具有重要意义。通过具体的案例研究验证了其可行性，展现了理论与实践结合的潜力。"}}
{"id": "2506.15004", "title": "Mixed Traffic: A Perspective from Long Duration Autonomy", "authors": ["Filippos Tzortzoglou", "Logan E. Beaver"], "summary": "The rapid adoption of autonomous vehicle has established mixed traffic\nenvironments, comprising both autonomous and human-driven vehicles (HDVs), as\nessential components of next-generation mobility systems. Along these lines,\nconnectivity between autonomous vehicles and infrastructure (V2I) is also a\nsignificant factor that can effectively support higher-level decision-making.\nAt the same time, the integration of V2I within mixed traffic environments\nremains a timely and challenging problem. In this paper, we present a\nlong-duration autonomy controller for connected and automated vehicles (CAVs)\noperating in such environments, with a focus on intersections where right turns\non red are permitted. We begin by deriving the optimal control policy for CAVs\nunder free-flow traffic. Next, we analyze crossing time constraints imposed by\nsmart traffic lights and map these constraints to controller bounds using\nControl Barrier Functions (CBFs), with the aim to drive a CAV to cross the\nintersection on time. We also introduce criteria for identifying, in real-time,\nfeasible crossing intervals for each CAV. To ensure safety for the CAVs, we\npresent model-agnostic safety guarantees, and demonstrate their compatibility\nwith both CAVs and HDVs. Ultimately, the final control actions are enforced\nthrough a combination of CBF constraints, constraining CAVs to traverse the\nintersection within the designated time intervals while respecting other\nvehicles. Finally, we guarantee that our control policy yields always a\nfeasible solution and validate the proposed approach through extensive\nsimulations in MATLAB.", "comment": "14 pages, 12 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15004v1", "AI": {"title_translation": "混合交通：长时程自主性视角", "tldr": "本文提出了一种用于混合交通环境中联网自动驾驶汽车（CAVs）的长时程自主控制器，专注于带红灯右转的路口，利用控制障碍函数（CBFs）确保CAVs安全及时地通过交叉口，并通过仿真验证了其可行性。", "motivation": "自动驾驶汽车的普及使得混合交通环境（包括自动驾驶汽车和人工驾驶车辆）成为下一代出行系统的关键组成部分。联网自动驾驶汽车（CAVs）与基础设施（V2I）的集成在混合交通环境中，尤其是在需要长时程自主决策的交叉口场景下，仍然是一个及时且具有挑战性的问题。", "method": "本文提出了一种用于混合交通环境中联网自动驾驶汽车（CAVs）的长时程自主控制器，主要针对允许红灯右转的交叉口。研究首先推导了自由流交通下CAVs的最优控制策略，然后利用控制障碍函数（CBFs）将智能交通灯施加的交叉时间约束映射到控制器边界，旨在使CAV按时通过交叉口。此外，还引入了实时识别每个CAV可行交叉间隔的标准，并提出了模型无关的安全保障，以确保CAVs的安全，并证明了其与CAVs和人工驾驶车辆（HDVs）的兼容性。最终，通过结合CBF约束来执行控制动作，确保CAV在指定时间间隔内通过交叉口并尊重其他车辆。", "result": "所提出的控制策略始终能产生可行解。该方法通过MATLAB中的广泛仿真得到了验证。", "conclusion": "本文成功开发了一种针对混合交通环境下CAVs的长时程自主控制器，尤其是在复杂交叉口场景下，通过基于CBF的方法确保了安全、及时且可行的交叉口通行，为下一代出行系统中的CAV控制提供了有效的解决方案。", "translation": "自动驾驶汽车的迅速普及使得混合交通环境（包括自动驾驶汽车和人工驾驶车辆）成为下一代出行系统的重要组成部分。沿着这些思路，自动驾驶汽车与基础设施（V2I）之间的连接也是一个重要因素，可以有效支持更高层次的决策。同时，在混合交通环境中集成V2I仍然是一个及时且具有挑战性的问题。在本文中，我们提出了一种用于在此类环境中运行的联网自动驾驶汽车（CAVs）的长时程自主控制器，重点关注允许红灯右转的交叉口。我们首先推导了自由流交通下CAVs的最优控制策略。接下来，我们分析了智能交通灯施加的交叉时间约束，并使用控制障碍函数（CBFs）将这些约束映射到控制器边界，旨在驱动CAV按时通过交叉口。我们还引入了实时识别每个CAV可行交叉间隔的标准。为了确保CAVs的安全，我们提出了模型无关的安全保障，并证明了它们与CAVs和人工驾驶车辆（HDVs）的兼容性。最终，通过结合CBF约束来执行最终的控制动作，约束CAVs在指定的时间间隔内通过交叉口，同时尊重其他车辆。最后，我们保证我们的控制策略始终能产生可行解，并通过MATLAB中的广泛仿真验证了所提出的方法。", "summary": "本文针对包含自动驾驶汽车和人工驾驶车辆的混合交通环境，提出了一种长时程自主控制器，专注于允许红灯右转的交叉口。该控制器利用控制障碍函数（CBFs）将智能交通灯的时间约束转化为控制器边界，以确保联网自动驾驶汽车（CAVs）安全、及时地通过交叉口。研究还提供了模型无关的安全保障，并引入了实时识别可行交叉间隔的准则。通过MATLAB仿真验证了所提出的控制策略始终能产生可行解。", "keywords": "混合交通, 自动驾驶汽车, 控制障碍函数, 交叉口, 长时程自主性", "comments": "这篇论文解决了混合交通环境下联网自动驾驶汽车（CAVs）在复杂交叉口（尤其是有红灯右转规则）进行长时程自主控制的关键挑战。其创新点在于结合了最优控制策略和控制障碍函数（CBFs）来处理智能交通灯的时间约束和车辆间的安全保障，并提出了模型无关的安全兼容性。该方法能够确保CAVs在混合交通流中安全、及时地通过交叉口，且保证了控制解的可行性，为未来城市交通系统的发展提供了实用的解决方案。"}}
{"id": "2506.14936", "title": "CALM: Contextual Analog Logic with Multimodality", "authors": ["Maxwell J. Jacobson", "Corey J. Maley", "Yexiang Xue"], "summary": "In this work, we introduce Contextual Analog Logic with Multimodality (CALM).\nCALM unites symbolic reasoning with neural generation, enabling systems to make\ncontext-sensitive decisions grounded in real-world multi-modal data.\n  Background: Classic bivalent logic systems cannot capture the nuance of human\ndecision-making. They also require human grounding in multi-modal environments,\nwhich can be ad-hoc, rigid, and brittle. Neural networks are good at extracting\nrich contextual information from multi-modal data, but lack interpretable\nstructures for reasoning.\n  Objectives: CALM aims to bridge the gap between logic and neural perception,\ncreating an analog logic that can reason over multi-modal inputs. Without this\nintegration, AI systems remain either brittle or unstructured, unable to\ngeneralize robustly to real-world tasks. In CALM, symbolic predicates evaluate\nto analog truth values computed by neural networks and constrained search.\n  Methods: CALM represents each predicate using a domain tree, which\niteratively refines its analog truth value when the contextual groundings of\nits entities are determined. The iterative refinement is predicted by neural\nnetworks capable of capturing multi-modal information and is filtered through a\nsymbolic reasoning module to ensure constraint satisfaction.\n  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%\naccuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It\nalso demonstrated spatial heatmap generation aligned with logical constraints\nand delicate human preferences, as shown by a human study.\n  Conclusions: CALM demonstrates the potential to reason with logic structure\nwhile aligning with preferences in multi-modal environments. It lays the\nfoundation for next-gen AI systems that require the precision and\ninterpretation of logic and the multimodal information processing of neural\nnetworks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.14936v1", "AI": {"title_translation": "CALM：多模态上下文模拟逻辑", "tldr": "CALM结合符号推理与神经网络，实现了对多模态数据的上下文敏感决策，并在物体放置任务中表现优异。", "motivation": "经典二值逻辑系统无法捕捉人类决策的细微之处，且在多模态环境中需要僵化的人工标注。神经网络善于提取多模态信息但缺乏可解释的推理结构。本文旨在弥合逻辑推理与神经感知之间的鸿沟，创建一种能够对多模态输入进行推理的模拟逻辑，以解决现有AI系统在实际任务中缺乏鲁棒泛化能力的问题。", "method": "CALM通过领域树表示每个谓词，当其实体的上下文基础确定后，领域树会迭代地细化其模拟真值。这种迭代细化由能够捕获多模态信息的神经网络预测，并通过符号推理模块进行过滤以确保满足约束条件。", "result": "在填空式物体放置任务中，CALM取得了92.2%的准确率，优于经典逻辑（86.3%）和LLM（59.4%）基线。此外，它还展示了与逻辑约束和微妙人类偏好一致的空间热图生成，并通过一项人类研究得到证实。", "conclusion": "CALM展示了在多模态环境中利用逻辑结构进行推理并与偏好对齐的潜力。它为需要逻辑的精确性和可解释性以及神经网络多模态信息处理能力的下一代AI系统奠定了基础。", "translation": "在这项工作中，我们介绍了多模态上下文模拟逻辑（CALM）。CALM将符号推理与神经生成相结合，使系统能够根据真实世界的多模态数据做出上下文敏感的决策。\n背景：经典的二值逻辑系统无法捕捉人类决策的细微之处。它们还需要在多模态环境中进行人工接地，这可能是随意、僵化和脆弱的。神经网络擅长从多模态数据中提取丰富的上下文信息，但缺乏可解释的推理结构。\n目标：CALM旨在弥合逻辑与神经感知之间的鸿沟，创建一种能够对多模态输入进行推理的模拟逻辑。如果没有这种集成，AI系统将仍然是脆弱或非结构化的，无法鲁棒地泛化到现实世界的任务中。在CALM中，符号谓词计算出由神经网络计算并受约束搜索的模拟真值。\n方法：CALM使用领域树表示每个谓词，当其实体的上下文基础确定后，领域树会迭代地细化其模拟真值。这种迭代细化由能够捕获多模态信息的神经网络预测，并通过符号推理模块进行过滤以确保满足约束条件。\n结果：在填空式物体放置任务中，CALM取得了92.2%的准确率，优于经典逻辑（86.3%）和LLM（59.4%）基线。它还展示了与逻辑约束和微妙人类偏好一致的空间热图生成，并通过一项人类研究得到证实。\n结论：CALM展示了在多模态环境中利用逻辑结构进行推理并与偏好对齐的潜力。它为需要逻辑的精确性和可解释性以及神经网络多模态信息处理能力的下一代AI系统奠定了基础。", "summary": "本文提出了一种名为CALM（多模态上下文模拟逻辑）的新框架，旨在结合符号推理的结构性与神经网络处理多模态数据的能力。针对经典逻辑无法处理细微决策和神经网络缺乏可解释推理的问题，CALM通过领域树和神经网络迭代细化模拟真值，并结合符号推理确保约束满足。实验结果显示，在物体放置任务中，CALM的准确率（92.2%）显著优于经典逻辑和LLM基线，并能生成符合逻辑约束和人类偏好的空间热图。这表明CALM为开发结合逻辑精确性和多模态感知的新一代AI系统奠定了基础。", "keywords": "上下文模拟逻辑, 多模态, 符号推理, 神经网络, 真值", "comments": "CALM的创新之处在于其将符号逻辑的结构化推理与神经网络处理复杂多模态数据的能力相结合，解决了传统AI系统在可解释性、鲁棒性和处理复杂现实世界场景方面的局限性。其采用模拟真值而非二值逻辑，并结合迭代细化和约束满足，是弥合两大AI范式之间鸿沟的重要一步。在实际应用中，尤其是在需要精细决策和理解人类偏好的场景中，CALM表现出显著优势，为未来AI系统的发展提供了新的思路。"}}
{"id": "2506.14906", "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder", "authors": ["Robert Czupryniak", "Abhishek Chakraborty", "Andrew N. Jordan", "John C. Howell"], "summary": "We apply machine learning methods to demonstrate range superresolution in\nremote sensing radar detection. Specifically, we implement a denoising\nautoencoder to estimate the distance between two equal intensity scatterers in\nthe subwavelength regime. The machine learning models are trained on waveforms\nsubject to a bandlimit constraint such that ranges much smaller than the\ninverse bandlimit are optimized in their precision. The autoencoder achieves\neffective dimensionality reduction, with the bottleneck layer exhibiting a\nstrong and consistent correlation with the true scatterer separation. We\nconfirm reproducibility across different training sessions and network\ninitializations by analyzing the scaled encoder outputs and their robustness to\nnoise. We investigate the behavior of the bottleneck layer for the following\ntypes of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,\nand a theoretically near-optimal pulse created from a spherical Bessel function\nbasis. The Bessel signal performs best, followed by the triangle wave, with the\nsinc signal performing worst, highlighting the crucial role of signal design in\nthe success of machine-learning-based range resolution.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14906v1", "AI": {"title_translation": "使用去噪自编码器演示雷达测距中的超分辨率", "tldr": "本文使用去噪自编码器实现了雷达测距的超分辨率，并发现信号设计对机器学习测距性能至关重要。", "motivation": "在遥感雷达探测中实现距离超分辨率。", "method": "本文实现了一个去噪自编码器来估计亚波长区域中两个等强度散射体之间的距离。机器学习模型在受带限约束的波形上进行训练，以优化小距离的精度。研究了传统sinc脉冲、带限三角形脉冲和由球贝塞尔函数基创建的理论近最优脉冲对瓶颈层行为的影响。", "result": "自编码器实现了有效的降维，其瓶颈层与真实散射体分离表现出强烈且一致的相关性。结果在不同训练会话和网络初始化之间具有可重现性，并对噪声具有鲁棒性。贝塞尔信号表现最佳，其次是三角形波，sinc信号表现最差。", "conclusion": "信号设计在基于机器学习的距离分辨率成功中起着关键作用。", "translation": "我们应用机器学习方法来演示遥感雷达探测中的距离超分辨率。具体而言，我们实现了一个去噪自编码器来估计亚波长区域中两个等强度散射体之间的距离。机器学习模型在受带限约束的波形上进行训练，使得远小于逆带限的距离在精度上得到优化。自编码器实现了有效的降维，其瓶颈层与真实散射体分离表现出强烈且一致的相关性。通过分析缩放后的编码器输出及其对噪声的鲁棒性，我们确认了不同训练会话和网络初始化之间的可重现性。我们研究了以下类型脉冲的瓶颈层行为：传统sinc脉冲、带限三角形脉冲，以及由球贝塞尔函数基创建的理论上接近最优的脉冲。贝塞尔信号表现最佳，其次是三角形波，sinc信号表现最差，这突出了信号设计在基于机器学习的距离分辨率成功中的关键作用。", "summary": "本文利用去噪自编码器在遥感雷达探测中实现了距离超分辨率。该方法通过训练模型处理带限波形来精确估计亚波长区域内散射体的距离。研究结果表明，自编码器的瓶颈层能有效降维并与真实距离强相关，且性能在不同训练中可复现并对噪声鲁棒。此外，研究强调了信号设计的重要性，发现基于球贝塞尔函数的脉冲性能最优。", "keywords": "雷达测距, 超分辨率, 去噪自编码器, 机器学习, 信号设计", "comments": "这篇论文通过将机器学习（特别是去噪自编码器）应用于雷达测距的超分辨率问题，展示了其创新性。它不仅验证了机器学习在传统信号处理领域的潜力，还深入探讨了信号设计对模型性能的影响，为未来基于ML的雷达系统设计提供了有价值的指导。"}}
{"id": "2506.14974", "title": "Parallel Complexity of Depth-First-Search and Maximal path", "authors": ["Archit Chauhan", "Samir Datta", "M. Praveen"], "summary": "Constructing a Depth First Search (DFS) tree is a fundamental graph problem,\nwhose parallel complexity is still not settled. Reif showed parallel\nintractability of lex-first DFS. In contrast, randomized parallel algorithms\n(and more recently, deterministic quasipolynomial parallel algorithms) are\nknown for constructing a DFS tree in general (di)graphs. However a\ndeterministic parallel algorithm for DFS in general graphs remains an elusive\ngoal. Working towards this, a series of works gave deterministic NC algorithms\nfor DFS in planar graphs and digraphs. We further extend these results to more\ngeneral graph classes, by providing NC algorithms for (di)graphs of bounded\ngenus, and for undirected H-minor-free graphs where H is a fixed graph with at\nmost one crossing. For the case of (di)graphs of bounded tree-width, we further\nimprove the complexity to a Logspace bound. Constructing a maximal path is a\nsimpler problem (that reduces to DFS) for which no deterministic parallel\nbounds are known for general graphs. For planar graphs a bound of O(log n)\nparallel time on a CREW PRAM (thus in NC2) is known. We improve this bound to\nLogspace.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.14974v1", "AI": {"title_translation": "深度优先搜索和最大路径的并行复杂度", "tldr": "本文为有界亏格图和无H-子图图（H是至多一个交叉的固定图）的深度优先搜索（DFS）提供了NC算法，并为有界树宽图和平面图中的最大路径问题提供了对数空间复杂度改进。", "motivation": "深度优先搜索（DFS）树的并行复杂度仍然未解决。尽管已知随机并行算法和准多项式并行算法可以构建DFS树，但通用图的确定性并行DFS算法仍然是一个难以实现的目标。此外，对于最大路径问题，通用图没有已知的确定性并行界限。", "method": "通过提供针对有界亏格有向图和无向图以及无H-子图图（其中H是至多一个交叉的固定图）的NC算法，扩展了先前在平面图和有向图上DFS的确定性NC算法结果。对于有界树宽的有向图和无向图，进一步将复杂度改进为对数空间界限。对于最大路径问题，将平面图上的并行时间界限（O(log n)）改进为对数空间。", "result": "为有界亏格的有向图和无向图以及无H-子图图（其中H是至多一个交叉的固定图）提供了确定性NC算法。将有界树宽的有向图和无向图的DFS复杂度改进为对数空间。将平面图上最大路径问题的复杂度改进为对数空间。", "conclusion": "本文成功地将确定性并行DFS算法的范围扩展到更广泛的图类，包括有界亏格图和特定类型的无H-子图图，并为这些图类提供了NC算法。此外，对于有界树宽图的DFS和平面图的最大路径问题，实现了复杂度到对数空间的改进。", "translation": "构建深度优先搜索（DFS）树是一个基本的图问题，其并行复杂度仍未解决。Reif证明了词典优先DFS的并行不可处理性。与此相反，已知随机并行算法（最近还有确定性准多项式并行算法）可以在通用（有向）图中构建DFS树。然而，通用图的确定性并行DFS算法仍然是一个难以实现的目标。为此，一系列工作为平面图和有向图的DFS提供了确定性NC算法。我们通过为有界亏格的（有向）图以及无H-子图的无向图（其中H是至多一个交叉的固定图）提供NC算法，进一步扩展了这些结果。对于有界树宽的（有向）图，我们进一步将复杂度改进为对数空间界限。构建最大路径是一个更简单的问题（可以简化为DFS），对于通用图，目前没有已知的确定性并行界限。对于平面图，已知在CREW PRAM上具有O(log n)并行时间（因此在NC2中）的界限。我们将此界限改进为对数空间。", "summary": "本文研究了深度优先搜索（DFS）树和最大路径问题的并行复杂度。针对通用图的确定性并行DFS算法仍是一个挑战，本文将先前在平面图和有向图上的确定性NC算法扩展到更广的图类，包括有界亏格的图和特定类型的无H-子图图，并提供了NC算法。此外，对于有界树宽的图，DFS的复杂度被改进为对数空间。对于最大路径问题，本文将平面图上的已知并行时间界限改进为对数空间。", "keywords": "深度优先搜索, 并行复杂度, NC算法, 最大路径, 图算法", "comments": "本文的创新之处在于，它通过为更广泛的图类（如有限亏格图和特定H-次要自由图）提供确定性NC算法，在解决通用图的确定性并行DFS算法这一长期挑战方面取得了进展。将有界树宽图的复杂度改进为对数空间，以及最大路径问题的改进，也显示了重要的理论贡献。"}}
{"id": "2506.14774", "title": "MedSyn: Enhancing Diagnostics with Human-AI Collaboration", "authors": ["Burcu Sayin", "Ipek Baris Schlicht", "Ngoc Vo Hong", "Sara Allievi", "Jacopo Staiano", "Pasquale Minervini", "Andrea Passerini"], "summary": "Clinical decision-making is inherently complex, often influenced by cognitive\nbiases, incomplete information, and case ambiguity. Large Language Models\n(LLMs) have shown promise as tools for supporting clinical decision-making, yet\ntheir typical one-shot or limited-interaction usage may overlook the\ncomplexities of real-world medical practice. In this work, we propose a hybrid\nhuman-AI framework, MedSyn, where physicians and LLMs engage in multi-step,\ninteractive dialogues to refine diagnoses and treatment decisions. Unlike\nstatic decision-support tools, MedSyn enables dynamic exchanges, allowing\nphysicians to challenge LLM suggestions while the LLM highlights alternative\nperspectives. Through simulated physician-LLM interactions, we assess the\npotential of open-source LLMs as physician assistants. Results show open-source\nLLMs are promising as physician assistants in the real world. Future work will\ninvolve real physician interactions to further validate MedSyn's usefulness in\ndiagnostic accuracy and patient outcomes.", "comment": "Accepted to the Trustworthy and Collaborative Artificial Intelligence\n  Workshop 2025 (TCAI 2025) in the 4th International Conference Series on\n  Hybrid Human-Artificial Intelligence (HHAI 2025)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14774v1", "AI": {"title_translation": "MedSyn：通过人机协作增强诊断", "tldr": "提出MedSyn框架，通过医生与LLM的多步交互对话，提高临床诊断和治疗决策。", "motivation": "临床决策复杂，常受认知偏差、信息不完整和病例模糊的影响。大型语言模型（LLMs）虽有潜力，但现有单次或有限交互的使用方式未能充分考虑真实世界医疗实践的复杂性。", "method": "本文提出了一个混合人机协作框架MedSyn，医生和LLMs通过多步、交互式对话来完善诊断和治疗决策。MedSyn支持动态交流，允许医生质疑LLM的建议，同时LLM也能突出替代视角。研究通过模拟医生-LLM交互来评估开源LLMs作为医生助手的潜力。", "result": "结果表明，开源LLMs作为医生助手在真实世界中具有前景。", "conclusion": "开源LLMs作为医生助手在真实世界中具有应用前景。未来的工作将涉及真实医生的互动，以进一步验证MedSyn在诊断准确性和患者结果方面的有用性。", "translation": "临床决策本质上是复杂的，常受认知偏差、信息不完整和病例模糊的影响。大型语言模型（LLMs）作为支持临床决策的工具已显示出潜力，但其典型的单次或有限交互使用可能忽略了真实世界医疗实践的复杂性。在这项工作中，我们提出了一个混合人机协作框架MedSyn，医生和LLMs通过多步、交互式对话来完善诊断和治疗决策。与静态决策支持工具不同，MedSyn支持动态交流，允许医生质疑LLM的建议，同时LLM也能突出替代视角。通过模拟医生-LLM交互，我们评估了开源LLMs作为医生助手的潜力。结果表明，开源LLMs作为医生助手在真实世界中具有前景。未来的工作将涉及真实医生的互动，以进一步验证MedSyn在诊断准确性和患者结果方面的有用性。", "summary": "本文提出MedSyn，一个混合人机协作框架，旨在通过医生与大型语言模型（LLM）之间的多步交互式对话，提升临床诊断和治疗决策的质量。MedSyn允许动态交流，医生可以质疑LLM的建议，LLM则提供不同视角。通过模拟交互评估，研究发现开源LLMs作为医生助手具有潜力，未来计划进行真实医生互动以验证其在提升诊断准确性和患者结果方面的有效性。", "keywords": "人机协作, 大型语言模型, 临床决策, 诊断, MedSyn", "comments": "MedSyn的创新之处在于其强调人机之间的“多步、交互式对话”，而非传统的单次或有限交互，这更贴近真实临床实践的复杂性。它关注的是如何通过动态协作来弥补LLM在医疗领域应用中可能存在的局限性，并允许医生作为主体进行质疑和引导，提高了AI辅助决策的实用性和可靠性。"}}
{"id": "2506.15183", "title": "You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality", "authors": ["Xingyu Chen", "Xinmin Fang", "Shuting Zhang", "Xinyu Zhang", "Liang He", "Zhengxiong Li"], "summary": "Mobile Virtual Reality (VR) is essential to achieving convenient and\nimmersive human-computer interaction and realizing emerging applications such\nas Metaverse. However, existing VR technologies require two separate renderings\nof binocular images, causing a significant bottleneck for mobile devices with\nlimited computing capability and power supply. This paper proposes an approach\nto rendering optimization for mobile VR called EffVR. By utilizing the\nper-pixel attribute, EffVR can generate binocular VR images from the monocular\nimage through genuinely one rendering, saving half the computation over\nconventional approaches. Our evaluation indicates that, compared with the\nstate-of-art, EffVRcan save 27% power consumption on average while achieving\nhigh binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR\napplications. Additionally, EffVR can increase the frame rate by 115.2%. These\nresults corroborate EffVRsuperior computation/energy-saving performance, paving\nthe road to a sustainable mobile VR. The source code, demo video, android app,\nand more are released anonymously at https://yoro-vr.github.io/", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15183v1", "AI": {"title_translation": "你只渲染一次：提升移动虚拟现实的能源和计算效率", "tldr": "EffVR通过单次渲染生成双目图像，显著提升移动VR的能效和帧率。", "motivation": "现有的移动虚拟现实（VR）技术需要对双目图像进行两次独立渲染，这对于计算能力和电源有限的移动设备来说是一个显著的瓶颈。", "method": "本文提出了一种名为EffVR的移动VR渲染优化方法。EffVR利用逐像素属性，通过一次真正的渲染从单目图像生成双目VR图像，从而节省了一半的计算量。", "result": "与现有技术相比，EffVR平均可节省27%的功耗，同时在移动VR应用中实现高双目图像质量（0.9679 SSIM和34.09 PSNR）。此外，EffVR可以将帧率提高115.2%。", "conclusion": "这些结果证实了EffVR卓越的计算/节能性能，为可持续的移动VR铺平了道路。", "translation": "移动虚拟现实（VR）对于实现便捷沉浸式人机交互以及元宇宙等新兴应用至关重要。然而，现有VR技术需要对双目图像进行两次独立渲染，这对于计算能力和电源有限的移动设备来说是一个显著的瓶颈。本文提出了一种名为EffVR的移动VR渲染优化方法。通过利用逐像素属性，EffVR可以通过一次真正的渲染从单目图像生成双目VR图像，从而节省了一半的计算量，相较于传统方法。我们的评估表明，与现有技术相比，EffVR平均可节省27%的功耗，同时在移动VR应用中实现高双目图像质量（0.9679 SSIM和34.09 PSNR）。此外，EffVR可以将帧率提高115.2%。这些结果证实了EffVR卓越的计算/节能性能，为可持续的移动VR铺平了道路。源代码、演示视频、安卓应用等已匿名发布于https://yoro-vr.github.io/", "summary": "该论文提出了一种名为EffVR的移动VR渲染优化方法，旨在解决现有双目图像两次渲染带来的计算和能耗瓶颈。EffVR通过利用逐像素属性，实现了从单目图像一次渲染生成双目VR图像，从而节省了一半的计算量。实验结果表明，EffVR相较于现有技术，平均节省27%的功耗，同时保持高图像质量，并能将帧率提高115.2%，证明了其在计算和能效方面的优越性，为可持续移动VR发展奠定基础。", "keywords": "移动VR, 渲染优化, 能效, 计算效率, EffVR", "comments": "EffVR的创新之处在于它通过“一次渲染”解决了移动VR双目图像生成的高计算和能耗问题，这对于资源受限的移动设备尤其重要。其提出的利用逐像素属性从单目图像生成双目图像的方法，显著提高了效率，并得到了量化的性能提升数据支持。这项工作对于推动移动VR的普及和可持续发展具有重要意义。"}}
{"id": "2506.15120", "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study with a Rényi Divergence-Based Solution", "authors": ["Shengjia Zhang", "Jiawei Chen", "Changdong Li", "Sheng Zhou", "Qihao Shi", "Yan Feng", "Chun Chen", "Can Wang"], "summary": "Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.", "comment": "AAAI 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15120v1", "AI": {"title_translation": "推荐系统中损失函数的改进：基于雷尼散度的解决方案的比较研究", "tldr": "本文深入分析了推荐系统中Softmax Loss (SL)和Cosine Contrastive Loss (CCL)的优缺点，并提出了一种新的基于雷尼散度的损失函数DrRL，该函数结合了SL和CCL的优点，有效缓解了它们的局限性，并在实验中验证了其在推荐准确性和鲁棒性方面的优越性。", "motivation": "损失函数在优化推荐模型中扮演着关键角色，其中Softmax Loss (SL)和Cosine Contrastive Loss (CCL)特别有效。然而，它们的理论联系和差异需要深入探索。SL对假阴性实例高度敏感，而CCL存在数据利用率低的问题。为了解决这些限制，本研究旨在提出一种新的损失函数。", "method": "本研究对Softmax Loss (SL)和Cosine Contrastive Loss (CCL)进行了全面的分析，发现它们都可以看作是传统损失函数与分布鲁棒优化(DRO)的结合。研究指出了它们各自的局限性，即SL对假阴性敏感，CCL数据利用率低。为解决这些问题，本文提出了一种新的损失函数DrRL，该函数通过在DRO优化中利用Rényi散度来泛化SL和CCL，并结合了二者的优点。", "result": "研究发现Softmax Loss (SL)和Cosine Contrastive Loss (CCL)都可视为传统损失函数结合分布鲁棒优化 (DRO) 的增强版，提高了对分布偏移的鲁棒性。然而，SL对假阴性实例高度敏感，而CCL存在数据利用率低的问题。提出的新损失函数DrRL被证明能有效缓解这些局限性，并通过广泛的实验验证了其在推荐准确性和鲁棒性方面的优越性。", "conclusion": "本研究通过深入分析SL和CCL的优缺点，提出了一种基于Rényi散度的通用损失函数DrRL。DrRL成功结合了SL和CCL的优点，并有效解决了它们各自的局限性，显著提升了推荐系统的准确性和鲁棒性。", "translation": "损失函数在优化推荐模型中扮演着关键角色。在各种损失函数中，Softmax Loss (SL)和Cosine Contrastive Loss (CCL)特别有效。它们的理论联系和差异值得深入探索。本研究对这些损失函数进行了全面的分析，获得了重要见解：1) 共同优点——两者都可以视为传统损失函数与分布鲁棒优化 (DRO) 的增强，从而增强了对分布偏移的鲁棒性；2) 各自局限性——由于在DRO优化中使用了不同的分布距离度量，SL对假阴性实例表现出高度敏感性，而CCL则存在数据利用率低的问题。为了解决这些局限性，本研究提出了一种新的损失函数DrRL，它通过在DRO优化中利用Rényi散度来泛化SL和CCL。DrRL结合了SL和CCL的优势结构，并被证明能有效缓解它们的局限性。已经进行了广泛的实验，以验证DrRL在推荐准确性和鲁棒性方面的优越性。", "summary": "本研究深入探讨了推荐系统中Softmax Loss (SL)和Cosine Contrastive Loss (CCL)这两种有效损失函数。分析发现，它们均可视为结合了分布鲁棒优化 (DRO) 的传统损失函数的增强版，但SL对假阴性敏感，CCL数据利用率低。为解决这些局限，论文提出了一种基于Rényi散度的通用损失函数DrRL，该函数整合了SL和CCL的优点，并被实验证明在提升推荐准确性和鲁棒性方面表现优越。", "keywords": "推荐系统, 损失函数, Rényi散度, 分布鲁棒优化, DrRL", "comments": "该论文的创新点在于提出了DrRL这一新的损失函数，它通过引入Rényi散度，巧妙地结合并泛化了现有表现良好的SL和CCL，同时有效解决了它们各自的局限性。这对于提高推荐系统的准确性和鲁棒性具有重要意义，尤其是在处理数据分布偏移和假阴性问题上。"}}
{"id": "2506.14984", "title": "Extending Spike-Timing Dependent Plasticity to Learning Synaptic Delays", "authors": ["Marissa Dominijanni", "Alexander Ororbia", "Kenneth W. Regan"], "summary": "Synaptic delays play a crucial role in biological neuronal networks, where\ntheir modulation has been observed in mammalian learning processes. In the\nrealm of neuromorphic computing, although spiking neural networks (SNNs) aim to\nemulate biology more closely than traditional artificial neural networks do,\nsynaptic delays are rarely incorporated into their simulation. We introduce a\nnovel learning rule for simultaneously learning synaptic connection strengths\nand delays, by extending spike-timing dependent plasticity (STDP), a Hebbian\nmethod commonly used for learning synaptic weights. We validate our approach by\nextending a widely-used SNN model for classification trained with unsupervised\nlearning. Then we demonstrate the effectiveness of our new method by comparing\nit against another existing methods for co-learning synaptic weights and delays\nas well as against STDP without synaptic delays. Results demonstrate that our\nproposed method consistently achieves superior performance across a variety of\ntest scenarios. Furthermore, our experimental results yield insight into the\ninterplay between synaptic efficacy and delay.", "comment": "Repository containing the source code used to generate the results is\n  available at: https://github.com/mdominijanni/dsstdp-results", "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.14984v1", "AI": {"title_translation": "将脉冲时间依赖可塑性扩展到学习突触延迟", "tldr": "本文提出了一种新的学习规则，通过扩展STDP，同时学习SNN中的突触连接强度和延迟，并在多种测试场景下取得了优于现有方法的性能。", "motivation": "在神经形态计算中，尽管脉冲神经网络（SNNs）旨在更紧密地模拟生物学，但突触延迟很少被纳入其模拟中，而突触延迟在生物神经网络中扮演着关键角色，并且其调制已在哺乳动物的学习过程中被观察到。", "method": "本文引入了一种新的学习规则，通过扩展脉冲时间依赖可塑性（STDP）（一种常用于学习突触权重的赫布方法），同时学习突触连接强度和延迟。该方法通过扩展一个广泛使用的、用无监督学习训练的分类SNN模型进行验证，并与现有的突触权重和延迟协同学习方法以及不包含突触延迟的STDP方法进行比较。", "result": "结果表明，所提出的方法在各种测试场景中始终实现了卓越的性能。此外，实验结果揭示了突触效能和延迟之间的相互作用。", "conclusion": "该研究成功地将脉冲时间依赖可塑性扩展到同时学习突触强度和延迟，证明了其在SNN中实现优越性能的有效性，并为突触效能与延迟的相互作用提供了新的见解。", "translation": "突触延迟在生物神经元网络中扮演着关键角色，其调制已在哺乳动物的学习过程中被观察到。在神经形态计算领域，尽管脉冲神经网络（SNNs）旨在比传统人工神经网络更紧密地模拟生物学，但突触延迟很少被纳入其模拟中。我们引入了一种新的学习规则，通过扩展脉冲时间依赖可塑性（STDP）（一种常用于学习突触权重的赫布方法），用于同时学习突触连接强度和延迟。我们通过扩展一个广泛使用的、用无监督学习训练的分类SNN模型来验证我们的方法。然后，我们通过将其与另一种现有的突触权重和延迟协同学习方法以及不包含突触延迟的STDP方法进行比较，展示了我们新方法的有效性。结果表明，我们提出的方法在各种测试场景中始终实现了卓越的性能。此外，我们的实验结果揭示了突触效能和延迟之间的相互作用。", "summary": "本文提出了一种新颖的学习规则，通过扩展脉冲时间依赖可塑性（STDP），使得脉冲神经网络（SNNs）能够同时学习突触连接强度和延迟。该方法旨在弥补现有SNN模型中对突触延迟关注不足的缺陷。研究通过在分类任务中验证该方法，并与现有技术进行对比，结果显示其在多种测试场景下均表现出卓越的性能，并为突触效能与延迟的相互作用提供了新的见解。", "keywords": "脉冲时间依赖可塑性, 突触延迟, 脉冲神经网络, 神经形态计算, 学习规则", "comments": "该论文的创新点在于将STDP扩展到不仅学习突触权重，还学习突触延迟，这在神经形态计算领域是一个重要的进步。此方法不仅提高了SNN的性能，还为理解生物神经网络中突触效能和延迟的协同作用提供了新的实验证据，具有重要的理论和应用价值。"}}
{"id": "2506.14900", "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "authors": ["Imane Guellil", "Salomé Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.", "comment": "Accepted and will be published at ACL2025 (main conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14900v1", "AI": {"title_translation": "出院摘要中不良事件提取：新数据集、标注方案和初步发现", "tldr": "本文创建了一个用于从老年患者出院摘要中提取不良事件的新数据集和标注方案，并评估了不同模型，发现粗粒度提取表现良好，但细粒度提取仍面临挑战。", "motivation": "临床自然语言处理资源中老年患者群体通常代表性不足；不良事件提取任务中存在不连续和重叠实体等未被充分解决的挑战。", "method": "构建了一个手动标注的语料库，用于从老年患者出院摘要中提取14种临床上重要的不良事件及其上下文属性（如否定、诊断类型、院内发生）。设计了支持不连续和重叠实体的独特标注方案。使用FlairNLP评估了多种模型，涵盖三种标注粒度：细粒度、粗粒度以及带否定的粗粒度。", "result": "Transformer模型（如BERT-cased）在文档级粗粒度提取上表现出色（F1 = 0.943），但在细粒度实体级任务上性能显著下降（F1 = 0.675），尤其对于罕见事件和复杂属性。", "conclusion": "尽管在高级别分数上表现良好，但在检测代表性不足的不良事件和捕获细微的临床语言方面仍存在重大挑战。", "translation": "在这项工作中，我们提出了一个手动标注的语料库，用于从老年患者的出院摘要中提取不良事件（AE），老年患者群体在临床自然语言处理资源中常常代表性不足。该数据集包括14种临床上重要的不良事件——例如跌倒、谵妄和颅内出血，以及上下文属性，如否定、诊断类型和院内发生。独特的是，该标注方案支持不连续和重叠实体，解决了以往工作中很少处理的挑战。我们使用FlairNLP在三种标注粒度下评估了多个模型：细粒度、粗粒度以及带否定的粗粒度。虽然基于Transformer的模型（例如BERT-cased）在文档级粗粒度提取上取得了强大的性能（F1 = 0.943），但对于细粒度实体级任务（例如F1 = 0.675），性能显著下降，特别是对于罕见事件和复杂属性。这些结果表明，尽管分数较高，但在检测代表性不足的不良事件和捕获细微的临床语言方面仍然存在重大挑战。该数据集在受信任研究环境（TRE）中开发，可通过DataLoch按需提供，并可作为评估不良事件提取方法和支持未来跨数据集泛化的强大基准。", "summary": "本文针对临床NLP中老年患者不良事件提取资源不足的问题，构建了一个包含14种不良事件及其上下文属性的手动标注语料库，并设计了支持不连续和重叠实体的独特标注方案。通过FlairNLP评估发现，Transformer模型在粗粒度提取上表现良好，但在细粒度任务上仍面临挑战，尤其对于罕见事件和复杂临床语言。该数据集可作为未来不良事件提取研究的基准。", "keywords": "不良事件提取, 出院摘要, 老年患者, 数据集, 标注方案", "comments": "本文的创新点在于构建了一个针对老年患者群体的不良事件数据集，并提出了支持不连续和重叠实体的独特标注方案，这对于处理复杂的临床文本至关重要。研究结果揭示了当前模型在细粒度不良事件提取和处理罕见事件方面的局限性，为未来的研究指明了方向。数据集的可用性也使其成为一个重要的基准资源。"}}
{"id": "2506.14780", "title": "Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes", "authors": ["Reda Chhaibi", "Serge Gratton", "Samuel Vaiter"], "summary": "In this paper, we propose an accelerated version for the Sinkhorn algorithm,\nwhich is the reference method for computing the solution to Entropic Optimal\nTransport.\n  Its main draw-back is the exponential slow-down of convergence as the\nregularization weakens $\\varepsilon \\rightarrow 0$.\n  Thanks to spectral insights on the behavior of the Hessian, we propose to\nmitigate the problem via an original spectral warm-start strategy. This leads\nto faster convergence compared to the reference method, as also demonstrated in\nour numerical experiments.", "comment": "22 pages, 5 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14780v1", "AI": {"title_translation": "通过稳定的低频模式加速熵最优传输的计算", "tldr": "本文提出了一种加速Sinkhorn算法的新方法，通过谱热启动策略解决了熵最优传输在弱正则化下收敛速度慢的问题，并实现了更快的收敛。", "motivation": "Sinkhorn算法是计算熵最优传输的参考方法，但其主要缺点是当正则化减弱（ε→0）时，收敛速度呈指数级减慢。", "method": "通过对Hessian行为的谱洞察，提出了一种原创的谱热启动策略来缓解收敛速度慢的问题。", "result": "与参考方法相比，该方法实现了更快的收敛速度，并已通过数值实验证明。", "conclusion": "本文提出的基于谱热启动策略的新方法有效加速了熵最优传输的Sinkhorn算法的收敛速度，尤其是在弱正则化条件下。", "translation": "在本文中，我们提出了一种加速版的Sinkhorn算法，该算法是计算熵最优传输解决方案的参考方法。其主要缺点是当正则化减弱$\\varepsilon \\rightarrow 0$时，收敛速度呈指数级减慢。得益于对Hessian行为的谱洞察，我们提出通过一种原创的谱热启动策略来缓解这个问题。与参考方法相比，这带来了更快的收敛速度，正如我们的数值实验所证明的那样。", "summary": "本文提出了一种加速熵最优传输中Sinkhorn算法的新方法。针对该算法在弱正则化条件下收敛速度指数级下降的问题，研究人员利用对Hessian行为的谱洞察，开发了一种原创的谱热启动策略。数值实验表明，该方法比现有参考方法具有更快的收敛速度。", "keywords": "熵最优传输, Sinkhorn算法, 谱热启动, 收敛性, 正则化", "comments": "本文的创新点在于利用对Hessian行为的谱洞察提出了一种原创的谱热启动策略，有效地解决了Sinkhorn算法在弱正则化条件下收敛速度慢这一重要问题，对于提高熵最优传输计算效率具有重要意义。"}}
{"id": "2506.14826", "title": "Collaborative Interest-aware Graph Learning for Group Identification", "authors": ["Rui Zhao", "Beihong Jin", "Beibei Li", "Yiyuan Zheng"], "summary": "With the popularity of social media, an increasing number of users are\njoining group activities on online social platforms. This elicits the\nrequirement of group identification (GI), which is to recommend groups to\nusers. We reveal that users are influenced by both group-level and item-level\ninterests, and these dual-level interests have a collaborative evolution\nrelationship: joining a group expands the user's item interests, further\nprompting the user to join new groups. Ultimately, the two interests tend to\nalign dynamically. However, existing GI methods fail to fully model this\ncollaborative evolution relationship, ignoring the enhancement of group-level\ninterests on item-level interests, and suffering from false-negative samples\nwhen aligning cross-level interests. In order to fully model the collaborative\nevolution relationship between dual-level user interests, we propose CI4GI, a\nCollaborative Interest-aware model for Group Identification. Specifically, we\ndesign an interest enhancement strategy that identifies additional interests of\nusers from the items interacted with by the groups they have joined as a\nsupplement to item-level interests. In addition, we adopt the distance between\ninterest distributions of two users to optimize the identification of negative\nsamples for a user, mitigating the interference of false-negative samples\nduring cross-level interests alignment. The results of experiments on three\nreal-world datasets demonstrate that CI4GI significantly outperforms\nstate-of-the-art models.", "comment": "accepted by ECML PKDD 2025", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.14826v1", "AI": {"title_translation": "协同兴趣感知图学习用于群组识别", "tldr": "提出CI4GI模型，通过建模用户双层兴趣的协同演化关系并优化负样本识别，显著提升群组推荐效果。", "motivation": "现有群组识别方法未能充分建模用户群组级和项目级双层兴趣的协同演化关系，忽视群组级兴趣对项目级兴趣的增强作用，且在对齐跨层兴趣时存在假阴性样本问题。", "method": "提出CI4GI（Collaborative Interest-aware model for Group Identification）模型。该模型设计了一种兴趣增强策略，从用户已加入群组互动的项目中识别额外兴趣作为项目级兴趣的补充；此外，采用用户间兴趣分布距离来优化负样本识别，以减轻跨层兴趣对齐过程中假阴性样本的干扰。", "result": "在三个真实世界数据集上的实验结果表明，CI4GI显著优于现有最先进的模型。", "conclusion": "CI4GI模型通过有效建模用户双层兴趣的协同演化关系并解决假阴性样本问题，显著提升了群组识别的性能，证明了其在群组推荐领域的有效性。", "translation": "随着社交媒体的普及，越来越多的用户正在在线社交平台上参与群组活动。这引发了群组识别（GI）的需求，即向用户推荐群组。我们发现用户受到群组级和项目级兴趣的影响，并且这双层兴趣具有协同演化关系：加入一个群组会扩展用户的项目兴趣，进一步促使用户加入新的群组。最终，这两种兴趣趋于动态对齐。然而，现有的GI方法未能充分建模这种协同演化关系，忽视了群组级兴趣对项目级兴趣的增强作用，并且在对齐跨层兴趣时存在假阴性样本问题。为了充分建模用户双层兴趣之间的协同演化关系，我们提出了CI4GI，一个协同兴趣感知群组识别模型。具体来说，我们设计了一种兴趣增强策略，从用户已加入群组互动的项目中识别额外兴趣，作为项目级兴趣的补充。此外，我们采用两个用户兴趣分布之间的距离来优化用户负样本的识别，以减轻跨层兴趣对齐过程中假阴性样本的干扰。在三个真实世界数据集上的实验结果表明，CI4GI显著优于现有最先进的模型。", "summary": "本文针对在线社交平台中的群组识别问题，提出了一种名为CI4GI的协同兴趣感知模型。该模型旨在解决现有方法未能充分建模用户群组级和项目级双层兴趣协同演化关系的问题。CI4GI通过设计兴趣增强策略来补充项目级兴趣，并优化负样本识别以减轻假阴性样本干扰，从而有效捕捉用户双层兴趣的动态对齐。实验结果表明，CI4GI在多个真实数据集上表现优异，显著超越了现有先进模型。", "keywords": "群组识别, 协同演化, 双层兴趣, 假阴性样本, 图学习", "comments": "本文的创新点在于首次明确提出了用户双层兴趣（群组级和项目级）的协同演化关系，并针对性地设计了兴趣增强策略和负样本优化方法来解决现有群组识别模型未能充分建模这一关系的问题。其重要性在于为更精准的群组推荐提供了新的视角和有效方案，有助于提升用户在线社交体验。"}}
{"id": "2506.15028", "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices", "authors": ["Gargi Mitra", "Mohammadreza Hallajiyan", "Inji Kim", "Athish Pranav Dharmalingam", "Mohammed Elnawawy", "Shahrear Iqbal", "Karthik Pattabiraman", "Homa Alemzadeh"], "summary": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients.", "comment": "32 pages, 6 figures, 6 tables", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15028v1", "AI": {"title_translation": "启用机器学习的医疗设备中的系统理论和数据驱动安全分析", "tldr": "本研究关注启用机器学习的医疗设备在上市前阶段的网络安全风险，通过分析公开数据和提供工具，旨在帮助制造商将网络安全作为核心设计原则，确保患者安全。", "motivation": "AI/ML集成到医疗设备中带来了严重的网络安全风险，可能导致ML模型误判，对患者安全构成重大威胁。因此，在设计阶段确保这些设备的安全性至关重要，尤其是在上市前阶段解决网络安全挑战具有紧迫性。", "method": "首先，分析了公开可用的设备召回、不良事件数据以及已知漏洞，以了解AI/ML启用医疗设备的威胁格局。在此分析基础上，引入了一套自行设计的工具和技术，以协助安全分析师进行全面的上市前风险评估。", "result": "本研究旨在帮助安全分析师进行全面的上市前风险评估，并赋能制造商将网络安全作为AI/ML启用医疗设备的核心设计原则，从而确保患者安全。", "conclusion": "确保启用机器学习的医疗设备在设计阶段就具备安全性至关重要，尤其是在上市前阶段解决网络安全挑战。通过将网络安全嵌入核心设计原则，可以显著提高设备的安全性，保障患者健康。", "translation": "AI/ML与医疗设备的集成正在通过增强诊断和治疗设施来迅速改变医疗保健。然而，这种进步也由于使用复杂且通常不透明的模型、广泛的互连性、与第三方外围设备的互操作性、互联网连接以及底层技术的漏洞而引入了严重的安全风险。这些因素导致了广泛的攻击面，并使威胁预防、检测和缓解变得具有挑战性。鉴于这些设备的高度安全关键性质，对这些设备进行网络攻击可能导致ML模型误判，从而对患者构成重大的安全风险。因此，从设计时起确保这些设备的安全性至关重要。本文强调了在上市前阶段解决启用ML的医疗设备中网络安全挑战的紧迫性。我们首先分析了公开可用的设备召回和不良事件数据以及已知漏洞，以了解AI/ML启用医疗设备的威胁格局及其对患者安全的影响。在此分析基础上，我们引入了一套由我们设计的工具和技术，以协助安全分析师进行全面的上市前风险评估。我们的工作旨在赋能制造商将网络安全作为AI/ML启用医疗设备的核心设计原则，从而使其对患者安全。", "summary": "本研究关注AI/ML集成医疗设备带来的网络安全风险，强调在上市前阶段解决这些挑战的紧迫性。通过分析公开数据以理解威胁格局，并引入一套工具和技术，旨在帮助安全分析师进行全面的上市前风险评估，从而赋能制造商将网络安全嵌入设备设计中，以确保患者安全。", "keywords": "机器学习医疗设备, 网络安全, 上市前风险评估, 患者安全, 数据驱动分析", "comments": "本文创新性地将系统理论和数据驱动方法应用于ML启用医疗设备的安全分析，尤其强调了上市前阶段的重要性。通过分析实际数据和提供具体工具，为制造商提供了实用的指导，有助于将网络安全前置到设计流程中，对保障患者生命安全具有重要意义。其价值在于从源头提升医疗设备的安全韧性。"}}
{"id": "2506.14857", "title": "Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired", "authors": ["Suman Raj", "Swapnil Padhi", "Ruchi Bhoot", "Prince Modi", "Yogesh Simmhan"], "summary": "Autonomous navigation by drones using onboard sensors combined with machine\nlearning and computer vision algorithms is impacting a number of domains,\nincluding agriculture, logistics, and disaster management. In this paper, we\nexamine the use of drones for assisting visually impaired people (VIPs) in\nnavigating through outdoor urban environments. Specifically, we present a\nperception-based path planning system for local planning around the\nneighborhood of the VIP, integrated with a global planner based on GPS and maps\nfor coarse planning. We represent the problem using a geometric formulation and\npropose a multi DNN based framework for obstacle avoidance of the UAV as well\nas the VIP. Our evaluations conducted on a drone human system in a university\ncampus environment verifies the feasibility of our algorithms in three\nscenarios; when the VIP walks on a footpath, near parked vehicles, and in a\ncrowded street.", "comment": "16 pages, 7 figures; Accepted as Late-Breaking Results at the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n  2023", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14857v1", "AI": {"title_translation": "基于感知的无人机引导视障人士避障研究", "tldr": "本文提出了一种基于感知的无人机避障系统，旨在协助视障人士在户外城市环境中导航。该系统结合了局部和全局规划，并采用多深度神经网络（DNN）框架实现无人机和视障人士的避障。在大学校园环境中的评估验证了该算法在不同场景下的可行性。", "motivation": "无人机自主导航技术已应用于多个领域，本文旨在探索其在协助视障人士（VIPs）在户外城市环境中导航方面的潜力，以解决其出行困难。", "method": "本文提出了一种基于感知的路径规划系统，用于VIP周围的局部规划，并与基于GPS和地图的全局规划器集成，用于粗略规划。该问题采用几何公式表示，并提出了一个基于多深度神经网络（DNN）的框架，用于无人机和VIP的障碍物避障。", "result": "在大学校园环境中对无人机-人系统进行的评估验证了该算法在三种场景下的可行性：当VIP在人行道上行走时，靠近停放的车辆时，以及在拥挤的街道上时。", "conclusion": "该研究提出的基于感知的无人机避障算法在协助视障人士导航方面是可行的。", "translation": "自主无人机导航利用机载传感器结合机器学习和计算机视觉算法，正在影响农业、物流和灾害管理等多个领域。在本文中，我们研究了无人机在户外城市环境中协助视障人士（VIP）导航的应用。具体来说，我们提出了一种基于感知的路径规划系统，用于VIP周围的局部规划，并与基于GPS和地图的全局规划器集成，用于粗略规划。我们使用几何公式表示该问题，并提出了一个基于多DNN的框架，用于无人机和VIP的避障。我们在大学校园环境中对无人机-人系统进行的评估验证了我们算法在三种场景中的可行性：VIP在人行道上行走、靠近停放的车辆以及在拥挤的街道上。", "summary": "本文探讨了无人机辅助视障人士在户外城市环境中导航的应用。研究提出了一种结合局部感知路径规划和全局GPS/地图规划的系统，并采用几何公式和多深度神经网络（DNN）框架实现无人机和视障人士的联合避障。在大学校园的实际评估验证了该算法在人行道、停放车辆附近和拥挤街道等多种场景下的可行性。", "keywords": "无人机, 避障, 视障人士, 感知, 路径规划", "comments": "该论文的创新点在于将无人机导航技术应用于辅助视障人士这一特定且具有挑战性的领域。它结合了局部和全局规划，并引入了为无人机和视障人士提供联合避障功能的多DNN框架，这在实际应用中具有重要意义。在真实环境中的多场景验证进一步增强了其实用性和可行性。"}}
{"id": "2506.15207", "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Jimmy Cao", "Ryszard Kowalczyk"], "summary": "The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised\nEarth Observation (EO) missions, addressing challenges in climate monitoring,\ndisaster management, and more. However, autonomous coordination in\nmulti-satellite systems remains a fundamental challenge. Traditional\noptimisation approaches struggle to handle the real-time decision-making\ndemands of dynamic EO missions, necessitating the use of Reinforcement Learning\n(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we\ninvestigate RL-based autonomous EO mission planning by modelling\nsingle-satellite operations and extending to multi-satellite constellations\nusing MARL frameworks. We address key challenges, including energy and data\nstorage limitations, uncertainties in satellite observations, and the\ncomplexities of decentralised coordination under partial observability. By\nleveraging a near-realistic satellite simulation environment, we evaluate the\ntraining stability and performance of state-of-the-art MARL algorithms,\nincluding PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can\neffectively balance imaging and resource management while addressing\nnon-stationarity and reward interdependency in multi-satellite coordination.\nThe insights gained from this study provide a foundation for autonomous\nsatellite operations, offering practical guidelines for improving policy\nlearning in decentralised EO missions.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15207v1", "AI": {"title_translation": "多智能体强化学习在自主多卫星地球观测中的应用：一个现实案例研究", "tldr": "本文研究了使用多智能体强化学习（MARL）实现自主多卫星地球观测任务规划，解决了能源、数据存储限制及去中心化协调等挑战，并在近乎真实的模拟环境中验证了MARL在平衡成像与资源管理方面的有效性。", "motivation": "低地球轨道（LEO）卫星的快速增长革新了地球观测（EO）任务，但多卫星系统中的自主协调仍是一个根本性挑战。传统的优化方法难以处理动态EO任务的实时决策需求，因此需要采用强化学习（RL）和多智能体强化学习（MARL）方法。", "method": "本文通过建模单卫星操作，并使用多智能体强化学习（MARL）框架扩展到多卫星星座，研究了基于RL的自主地球观测任务规划。研究解决了能源和数据存储限制、卫星观测不确定性以及部分可观测下分散协调的复杂性等关键挑战。通过利用一个接近真实的卫星模拟环境，评估了PPO、IPPO、MAPPO和HAPPO等最先进的MARL算法的训练稳定性和性能。", "result": "研究结果表明，多智能体强化学习（MARL）能够有效平衡成像和资源管理，同时解决了多卫星协调中的非平稳性和奖励相互依赖问题。", "conclusion": "本研究获得的见解为自主卫星操作奠定了基础，并为改进去中心化地球观测任务中的策略学习提供了实用指导。", "translation": "低地球轨道（LEO）卫星的指数级增长彻底改变了地球观测（EO）任务，解决了气候监测、灾害管理等方面的挑战。然而，多卫星系统中的自主协调仍然是一个根本性挑战。传统的优化方法难以处理动态EO任务的实时决策需求，因此需要使用强化学习（RL）和多智能体强化学习（MARL）。在本文中，我们通过建模单卫星操作并使用MARL框架扩展到多卫星星座，研究了基于RL的自主EO任务规划。我们解决了关键挑战，包括能源和数据存储限制、卫星观测的不确定性以及部分可观测下分散协调的复杂性。通过利用一个接近真实的卫星模拟环境，我们评估了PPO、IPPO、MAPPO和HAPPO等最先进的MARL算法的训练稳定性和性能。我们的结果表明，MARL可以有效地平衡成像和资源管理，同时解决多卫星协调中的非平稳性和奖励相互依赖问题。本研究获得的见解为自主卫星操作奠定了基础，为改进去中心化EO任务中的策略学习提供了实用指导。", "summary": "本研究探讨了利用多智能体强化学习（MARL）实现自主多卫星地球观测任务规划。面对传统优化方法在实时决策上的局限性，论文通过建模单卫星操作并扩展到多卫星星座，解决了能源、数据存储限制、观测不确定性及去中心化协调等挑战。研究在一个接近真实的模拟环境中评估了多种先进MARL算法的性能，结果证明MARL能有效平衡成像与资源管理，并应对多卫星协调中的复杂性，为未来自主卫星操作提供了实践指导。", "keywords": "多智能体强化学习, 地球观测, 卫星协调, 任务规划, 强化学习", "comments": "本文通过在接近真实的模拟环境中应用多智能体强化学习（MARL）解决自主多卫星地球观测任务规划的复杂性，具有重要的创新性和实用价值。它不仅超越了传统优化方法的局限，还具体解决了能源、数据存储和去中心化协调等实际挑战，为未来去中心化地球观测任务中的策略学习提供了坚实的基础和实用指导。"}}
{"id": "2506.15084", "title": "An Empirical Study of Bugs in Data Visualization Libraries", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "summary": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.", "comment": "Proc. ACM Softw. Eng. 2, FSE", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15084v1", "AI": {"title_translation": "数据可视化库中错误的实证研究", "tldr": "首次对数据可视化库中的错误进行了全面分析，识别了常见错误类型和原因，并探讨了使用视觉语言模型进行检测的可行性。", "motivation": "数据可视化库在数据呈现、分析和应用开发中至关重要，但其中的视觉错误可能不会导致崩溃，而是误导用户，影响决策。因此，理解数据可视化库中错误的独特特性对于研究人员和开发人员检测和修复错误至关重要。", "method": "本研究对从五个广泛使用的数据可视化库中收集的564个错误进行了首次全面的分析。系统地分析了它们的症状和根本原因，并提供了一个详细的分类法。此外，确定了触发此类错误的八个关键步骤和两个特定于数据可视化库的测试预言，并探讨了应用视觉语言模型（VLMs）检测不正确/不准确图表的可行性。", "result": "研究发现不正确/不准确的图表在数据可视化库中普遍存在，并且不正确的图形计算是主要根本原因。VLMs在错误检测方面的有效性从29%到57%不等，具体取决于提示，并且在提示中添加更多信息不一定能提高有效性。", "conclusion": "数据可视化库中普遍存在不正确/不准确的图表，主要原因是图形计算错误，这需要进一步的自动化测试方法。本研究识别的触发步骤和测试预言有望启发未来设计有效的自动化测试技术。VLMs在错误检测方面表现出一定的潜力，但其有效性受提示影响。", "translation": "数据可视化（DataViz）库在演示、数据分析和应用程序开发中扮演着关键角色，强调了它们在将数据转换为视觉表示方面的准确性。不正确的可视化可能会对用户体验产生负面影响，扭曲信息传达，并影响用户的感知和决策过程。这些库中的视觉错误可能特别具有隐蔽性，因为它们可能不会导致像崩溃这样明显的错误，而是以图形方式误导底层数据的用户，导致错误的决策。因此，深入理解DataViz库中错误的独特特性对于研究人员和开发人员检测和修复DataViz库中的错误至关重要。\n本研究首次对DataViz库中的错误进行了全面分析，检查了从五个广泛使用的库中收集的564个错误。我们的研究系统地分析了它们的症状和根本原因，并提供了一个详细的分类法。我们发现不正确/不准确的图表在DataViz库中普遍存在，而不正确的图形计算是主要根本原因，这需要进一步的DataViz库自动化测试方法。此外，我们确定了触发此类错误的八个关键步骤和两个特定于DataViz库的测试预言，这可能启发未来设计有效的自动化测试技术。此外，随着视觉语言模型（VLMs）的最新进展，我们探讨了应用这些模型检测不正确/不准确图表的可行性。结果表明，VLMs在错误检测方面的有效性从29%到57%不等，具体取决于提示，并且在提示中添加更多信息不一定能提高有效性。更多发现可在我们的手稿中找到。", "summary": "本研究对数据可视化库中的错误进行了首次全面的实证分析，收集了来自五个主流库的564个错误。研究系统地分析了这些错误的症状和根本原因，发现不准确的图表普遍存在，且主要根源在于不正确的图形计算。论文提出了一套错误触发步骤和测试预言，并初步探索了视觉语言模型在错误检测中的应用潜力，发现其有效性受提示影响。研究强调了开发自动化测试方法的重要性。", "keywords": "数据可视化, 错误分析, 实证研究, 自动化测试, 视觉语言模型", "comments": "这项研究首次对数据可视化库中的错误进行了大规模的实证分析，具有重要的开创性意义。它揭示了数据可视化库中隐蔽性视觉错误的普遍性和危害，并深入剖析了其根本原因。提出的错误分类法、触发步骤和测试预言为未来设计更有效的自动化测试工具奠定了基础。此外，探索视觉语言模型在错误检测中的应用，尽管效果尚有提升空间，但为该领域带来了新的视角和研究方向，具有一定的创新性。这项工作对于提高数据可视化软件的可靠性和用户决策的准确性具有实际价值。"}}
{"id": "2506.14928", "title": "On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing", "authors": ["Dyk Chung Nguyen", "Thomas Chetaille", "Yuan-Hang Zhang", "Yuriy V. Pershin", "Massimiliano Di Ventra"], "summary": "Digital memcomputing machines (DMMs) have been designed to solve complex\ncombinatorial optimization problems. Since DMMs are fundamentally classical\ndynamical systems, their ordinary differential equations (ODEs) can be\nefficiently simulated on modern computers. This provides a unique platform to\nstudy their performance under various conditions. An aspect that has received\nlittle attention so far is how their performance is affected by the numerical\nerrors in the solution of their ODEs and the physical noise they would be\nnaturally subject to if built in hardware. Here, we analyze these two aspects\nin detail by varying the integration time step (numerical noise) and adding\nstochastic perturbations (physical noise) into the equations of DMMs. We are\nparticularly interested in understanding how noise induces a chaotic transition\nthat marks the shift from successful problem-solving to failure in these\nsystems. Our study includes an analysis of power spectra and Lyapunov exponents\ndepending on the noise strength. The results reveal a correlation between the\ninstance solvability and the sign of the ensemble averaged mean largest\nLyapunov exponent. Interestingly, we find a regime in which DMMs with positive\nmean largest Lyapunov exponents still exhibit solvability. Furthermore, the\npower spectra provide additional information about our system by distinguishing\nbetween regular behavior (peaks) and chaotic behavior (broadband spectrum).\nTherefore, power spectra could be utilized to control whether a DMM operates in\nthe optimal dynamical regime. Overall, we find that the qualitative effects of\nnumerical and physical noise are mostly similar, despite their fundamentally\ndifferent origin.", "comment": null, "cate": "nlin.CD", "url": "http://arxiv.org/abs/2506.14928v1", "AI": {"title_translation": "关于数字忆阻计算中噪声引起的混沌导致的可解-不可解转变", "tldr": "本文研究了数字忆阻计算机器（DMMs）在数值误差和物理噪声下的性能，发现噪声会诱导混沌，导致从成功求解到失败的转变。通过分析Lyapunov指数和功率谱，揭示了实例可解性与混沌行为的关系，并提出功率谱可用于控制DMM的最佳运行状态。", "motivation": "了解数字忆阻计算机器（DMMs）的性能如何受其常微分方程（ODEs）数值误差以及硬件中可能存在的物理噪声影响。特别关注噪声如何诱导混沌转变，从而导致系统从成功解决问题转向失败。", "method": "通过改变积分时间步长（模拟数值噪声）和在DMMs方程中添加随机扰动（模拟物理噪声）来分析噪声影响。研究包括对功率谱和Lyapunov指数的分析，这些分析取决于噪声强度。", "result": "结果显示实例可解性与系综平均最大Lyapunov指数的符号之间存在相关性。研究发现了一个DMMs即使具有正的平均最大Lyapunov指数仍能表现出可解性的区域。此外，功率谱通过区分规则行为（峰值）和混沌行为（宽带谱）提供了额外信息。数值噪声和物理噪声的定性影响大体相似。", "conclusion": "功率谱可以被用来控制DMM是否在最佳动态区域运行。数值噪声和物理噪声，尽管来源不同，但其定性影响大部分是相似的。", "translation": "数字忆阻计算机器（DMMs）旨在解决复杂的组合优化问题。由于DMMs本质上是经典的动力系统，它们的常微分方程（ODEs）可以在现代计算机上高效模拟。这提供了一个独特的平台来研究它们在各种条件下的性能。迄今为止，一个很少受到关注的方面是它们的性能如何受到ODEs解中数值误差以及如果构建成硬件它们将自然受到的物理噪声的影响。在这里，我们通过改变积分时间步长（数值噪声）和在DMMs方程中添加随机扰动（物理噪声）来详细分析这两个方面。我们特别感兴趣的是理解噪声如何诱导混沌转变，这种转变标志着这些系统从成功解决问题到失败的转变。我们的研究包括分析功率谱和Lyapunov指数，这些分析取决于噪声强度。结果揭示了实例可解性与系综平均最大Lyapunov指数符号之间的相关性。有趣的是，我们发现了一个DMMs即使具有正的平均最大Lyapunov指数仍能表现出可解性的区域。此外，功率谱通过区分规则行为（峰值）和混沌行为（宽带谱）为我们的系统提供了额外信息。因此，功率谱可以被用来控制DMM是否在最佳动态区域运行。总的来说，我们发现数值噪声和物理噪声的定性影响大体相似，尽管它们的来源根本不同。", "summary": "本文研究了数字忆阻计算机器（DMMs）在数值误差和物理噪声影响下的性能。通过改变积分时间步长和引入随机扰动，作者分析了噪声如何诱导混沌转变，导致DMMs从成功解决问题转向失败。研究发现，实例可解性与平均最大Lyapunov指数的符号相关，且在某些情况下，即使Lyapunov指数为正，DMMs仍可解。此外，功率谱被证明能区分系统行为，并可用于优化DMMs的运行。研究指出，数值噪声和物理噪声的定性影响相似。", "keywords": "数字忆阻计算机器, 噪声, 混沌, Lyapunov指数, 功率谱", "comments": "这篇论文探讨了噪声对数字忆阻计算机器（DMMs）性能的关键影响，特别关注噪声如何导致系统从可解状态过渡到不可解状态。其创新之处在于将数值误差和物理噪声视为混沌诱导因素，并利用Lyapunov指数和功率谱等动力学工具进行量化分析。研究发现了功率谱作为控制DMMs在最佳动态区域运行的潜在应用，这对于未来DMM硬件的鲁棒性设计具有重要意义。"}}
{"id": "2506.14777", "title": "WebXAII: an open-source web framework to study human-XAI interaction", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "Sébastien Harispe"], "summary": "This article introduces WebXAII, an open-source web framework designed to\nfacilitate research on human interaction with eXplainable Artificial\nIntelligence (XAI) systems. The field of XAI is rapidly expanding, driven by\nthe growing societal implications of the widespread adoption of AI (and in\nparticular machine learning) across diverse applications. Researchers who study\nthe interaction between humans and XAI techniques typically develop ad hoc\ninterfaces in order to conduct their studies. These interfaces are usually not\nshared alongside the results of the studies, which limits their reusability and\nthe reproducibility of experiments. In response, we design and implement\nWebXAII, a web-based platform that can embody full experimental protocols,\nmeaning that it can present all aspects of the experiment to human participants\nand record their responses. The experimental protocols are translated into a\ncomposite architecture of generic views and modules, which offers a lot of\nflexibility. The architecture is defined in a structured configuration file, so\nthat protocols can be implemented with minimal programming skills. We\ndemonstrate that WebXAII can effectively embody relevant protocols, by\nreproducing the protocol of a state-of-the-art study of the literature. The\nframework is available at https://github.com/PAJEAN/WebXAII.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14777v1", "AI": {"title_translation": "WebXAII：一个用于研究人机可解释人工智能交互的开源Web框架", "tldr": "WebXAII是一个开源Web框架，旨在解决人机可解释人工智能（XAI）交互研究中实验接口可重用性和可复现性差的问题。", "motivation": "研究人员在研究人与可解释人工智能（XAI）交互时，通常开发一次性（ad hoc）接口，这些接口不常共享，导致实验的可重用性和可复现性受限。", "method": "论文设计并实现了一个名为WebXAII的开源Web框架。该平台能够承载完整的实验协议，向参与者展示实验的所有方面并记录响应。它采用通用的视图和模块的复合架构，并通过结构化配置文件定义，从而实现低编程技能需求和高灵活性。", "result": "通过复现文献中一项最先进研究的协议，证明了WebXAII能够有效地承载相关协议。", "conclusion": "WebXAII提供了一个解决人机XAI交互研究中可重用性和可复现性挑战的通用且灵活的解决方案。", "translation": "本文介绍了WebXAII，一个开源的Web框架，旨在促进人类与可解释人工智能（XAI）系统交互的研究。XAI领域正在迅速扩展，这得益于AI（特别是机器学习）在各种应用中广泛采用所带来的日益增长的社会影响。研究人类与XAI技术交互的研究人员通常会开发一次性接口来开展他们的研究。这些接口通常不与研究结果一同共享，这限制了它们的可重用性和实验的重现性。作为回应，我们设计并实现了WebXAII，一个基于Web的平台，可以承载完整的实验协议，这意味着它可以向人类参与者展示实验的所有方面并记录他们的响应。实验协议被转化为通用视图和模块的复合架构，这提供了很大的灵活性。该架构在结构化配置文件中定义，因此可以用最少的编程技能实现协议。我们通过复现文献中一项最先进研究的协议，证明了WebXAII能够有效地承载相关协议。该框架可在https://github.com/PAJEAN/WebXAII获取。", "summary": "WebXAII是一个开源Web框架，旨在解决人与可解释人工智能（XAI）交互研究中，现有一次性实验接口导致的重用性和复现性问题。该框架提供一个灵活的Web平台，能够承载完整实验协议并记录参与者响应，其基于复合架构和结构化配置文件，降低了编程门槛。通过复现一项现有研究，验证了其有效性。", "keywords": "可解释人工智能, 人机交互, 开源框架, 实验复现性, Web平台", "comments": "WebXAII的创新之处在于提供了一个标准化的、可复用和灵活的框架，解决了人机XAI交互研究中实验设置的碎片化问题。这对于提高该领域研究的效率和可信度具有重要意义。其低编程技能需求的设计也降低了研究人员的门槛。"}}
{"id": "2506.15011", "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph\nConvolutional Network (GCN) integrated with a Deep Q-Network (DQN)\nreinforcement learning framework for improved interference coordination in\nmulti-cell, multi-channel networks. Unlike LDP's static priorities, our\napproach dynamically learns link priorities based on real-time traffic demand,\nnetwork topology, remaining transmission opportunities, and interference\npatterns. The GCN captures spatial dependencies, while the DQN enables adaptive\nscheduling decisions through reward-guided exploration. Simulation results show\nthat our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and\n175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN\nmodel demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over\nour previous CNN-based approach across the same configurations. These results\nunderscore the effectiveness of our GCN-DQN model in addressing complex URLLC\nrequirements with minimal overhead and superior network performance.", "comment": "This paper has been submitted to IEEE MASS 2025 on May 7, 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.15011v1", "AI": {"title_translation": "GCN驱动的强化学习在工业URLLC中实现概率实时保障", "tldr": "该论文提出了一种GCN-DQN模型，用于改善工业URLLC中的干扰协调，其性能优于LDP和先前基于CNN的方法。", "motivation": "确保大规模工业无线网络中超可靠、低延迟通信 (URLLC) 的数据包级通信质量至关重要。现有的本地截止日期划分 (LDP) 算法使用静态优先级，这并不是最优的。", "method": "作者通过将图卷积网络 (GCN) 与深度Q网络 (DQN) 强化学习框架相结合，增强了本地截止日期划分 (LDP) 算法。该方法根据实时流量需求、网络拓扑、剩余传输机会和干扰模式动态学习链路优先级。GCN用于捕获空间依赖性，而DQN通过奖励引导的探索实现自适应调度决策。", "result": "GCN-DQN模型在三种网络配置下，与LDP相比，平均SINR分别提高了179.6%、197.4%和175.2%。此外，GCN-DQN模型在相同配置下，与之前基于CNN的方法相比，平均SINR分别提高了31.5%、53.0%和84.7%。", "conclusion": "GCN-DQN模型在以最小开销和卓越网络性能解决复杂URLLC要求方面是有效的。", "translation": "确保大规模工业无线网络中超可靠、低延迟通信 (URLLC) 的数据包级通信质量至关重要。我们通过引入一个结合了图卷积网络 (GCN) 和深度Q网络 (DQN) 强化学习框架的算法，增强了本地截止日期划分 (LDP) 算法，以改进多小区、多信道网络中的干扰协调。与LDP的静态优先级不同，我们的方法根据实时流量需求、网络拓扑、剩余传输机会和干扰模式动态学习链路优先级。GCN捕获空间依赖性，而DQN通过奖励引导的探索实现自适应调度决策。仿真结果表明，我们的GCN-DQN模型在三种网络配置下，与LDP相比，平均SINR分别提高了179.6%、197.4%和175.2%。此外，GCN-DQN模型在相同配置下，与我们之前基于CNN的方法相比，平均SINR分别提高了31.5%、53.0%和84.7%。这些结果强调了我们的GCN-DQN模型在以最小开销和卓越网络性能解决复杂URLLC要求方面的有效性。", "summary": "本文提出了一种新颖的GCN-DQN强化学习框架，旨在增强本地截止日期划分 (LDP) 算法，以改善工业URLLC中的干扰协调。通过根据各种网络参数动态学习链路优先级，GCN-DQN模型在平均SINR方面显著优于静态优先级的LDP和之前基于CNN的方法，展示了其在复杂URLLC场景中的有效性。", "keywords": "GCN, DQN, 强化学习, URLLC, 干扰协调", "comments": "该论文的创新之处在于将GCN与DQN相结合，动态学习链路优先级，利用GCN处理空间依赖性，DQN实现自适应调度。这种方法解决了LDP等静态优先级算法的局限性，并在SINR方面显示出显著的性能提升，这对于URLLC至关重要。"}}
{"id": "2506.15052", "title": "MIMO Systems Aided by Microwave Linear Analog Computers: Capacity-Achieving Architectures with Reduced Circuit Complexity", "authors": ["Matteo Nerini", "Bruno Clerckx"], "summary": "To meet the demands of future wireless networks, antenna arrays must scale\nfrom massive multiple-input multiple-output (MIMO) to gigantic MIMO, involving\neven larger numbers of antennas. To address the hardware and computational cost\nof gigantic MIMO, several strategies are available that shift processing from\nthe digital to the analog domain. Among them, microwave linear analog computers\n(MiLACs) offer a compelling solution by enabling fully analog beamforming\nthrough reconfigurable microwave networks. Prior work has focused on\nfully-connected MiLACs, whose ports are all interconnected to each other via\ntunable impedance components. Although such MiLACs are capacity-achieving,\ntheir circuit complexity, given by the number of required impedance components,\nscales quadratically with the number of antennas, limiting their practicality.\nTo solve this issue, in this paper, we propose a graph theoretical model of\nMiLAC facilitating the systematic design of lower-complexity MiLAC\narchitectures. Leveraging this model, we propose stem-connected MiLACs as a\nfamily of MiLAC architectures maintaining capacity-achieving performance while\ndrastically reducing the circuit complexity. Besides, we optimize\nstem-connected MiLACs with a closed-form capacity-achieving solution. Our\ntheoretical analysis, confirmed by numerical simulations, shows that\nstem-connected MiLACs are capacity-achieving, but with circuit complexity that\nscales linearly with the number of antennas, enabling high-performance,\nscalable, gigantic MIMO.", "comment": "Submitted to IEEE for publication", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15052v1", "AI": {"title_translation": "微波线性模拟计算机辅助的MIMO系统：具有降低电路复杂度的容量实现架构", "tldr": "本文提出了一种名为“茎连接MiLACs”的新型微波线性模拟计算机（MiLAC）架构，它在保持容量实现性能的同时，将电路复杂性从天线数量的二次方降低到线性，从而实现可扩展的巨型MIMO。", "motivation": "为了满足未来无线网络的需求，天线阵列需要从大规模MIMO扩展到巨型MIMO，这涉及到更多的天线。巨型MIMO的硬件和计算成本是一个挑战。现有的全连接MiLAC虽然能实现容量，但其电路复杂度随天线数量呈二次方增长，限制了其实用性。", "method": "本文提出了一种MiLAC的图论模型，以系统地设计低复杂度MiLAC架构。基于此模型，作者提出了一种名为“茎连接MiLACs”的MiLAC架构家族。此外，作者还优化了茎连接MiLACs，并给出了一个闭式容量实现解。", "result": "理论分析和数值模拟证实，茎连接MiLACs在保持容量实现性能的同时，其电路复杂度与天线数量呈线性关系。这使得高性能、可扩展的巨型MIMO成为可能。", "conclusion": "茎连接MiLACs架构能够实现与天线数量呈线性复杂度的容量实现性能，为未来高性能、可扩展的巨型MIMO系统提供了可行的解决方案。", "translation": "为了满足未来无线网络的需求，天线阵列必须从大规模多输入多输出（MIMO）扩展到巨型MIMO，涉及更多的天线。为了解决巨型MIMO的硬件和计算成本问题，有几种策略可以将处理从数字域转移到模拟域。其中，微波线性模拟计算机（MiLACs）通过可重构微波网络实现全模拟波束成形，提供了一种引人注目的解决方案。先前的研究主要集中在全连接MiLACs上，其所有端口都通过可调阻抗组件相互连接。尽管此类MiLACs可以实现容量，但其电路复杂度（由所需阻抗组件的数量决定）随天线数量呈二次方增长，限制了其实用性。为了解决这个问题，本文提出了一种MiLAC的图论模型，以促进低复杂度MiLAC架构的系统设计。利用该模型，我们提出了茎连接MiLACs作为一类MiLAC架构，它在保持容量实现性能的同时，大幅降低了电路复杂度。此外，我们通过闭式容量实现解决方案优化了茎连接MiLACs。我们的理论分析得到了数值模拟的证实，表明茎连接MiLACs可以实现容量，但其电路复杂度与天线数量呈线性关系，从而实现了高性能、可扩展的巨型MIMO。", "summary": "本文针对巨型MIMO系统中全连接微波线性模拟计算机（MiLACs）面临的电路复杂度高（与天线数量呈二次方增长）问题，提出了一种基于图论模型的“茎连接MiLACs”新架构。该架构通过理论分析和数值模拟证明，在保持容量实现性能的同时，能将电路复杂度降低到与天线数量呈线性关系，从而为未来高性能、可扩展的巨型MIMO系统提供了更实用的解决方案。", "keywords": "MIMO, 巨型MIMO, 微波线性模拟计算机, 容量实现, 电路复杂度", "comments": "该论文的创新点在于提出了茎连接MiLACs架构，并利用图论模型系统地解决了全连接MiLACs电路复杂度过高的问题。通过将复杂度从二次方降低到线性，显著提升了巨型MIMO系统的实用性和可扩展性，对于未来无线通信技术的发展具有重要意义。"}}
{"id": "2506.14852", "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching", "authors": ["Qizheng Zhang", "Michael Wornow", "Kunle Olukotun"], "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.", "comment": "23 pages", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14852v1", "AI": {"title_translation": "LLM代理的成本效益服务：通过测试时计划缓存", "tldr": "提出一种名为“代理计划缓存”的新方法，通过缓存和重用LLM代理的结构化计划模板，平均降低46.62%的成本，同时保持性能。", "motivation": "LLM代理应用在复杂工作流中表现出色，但由于大量的规划和推理需求导致成本高昂。现有为聊天机器人设计的LLM缓存技术不足以应对依赖外部数据或环境的代理应用。", "method": "提出代理计划缓存，一种从代理应用程序规划阶段提取、存储、适应和重用结构化计划模板的新方法。该系统在测试时从已完成的代理执行中提取计划模板，使用关键词提取将新请求与缓存计划匹配，并利用轻量级模型将这些模板适应于特定任务的计划。", "result": "在多个真实世界代理应用上的评估表明，该系统平均可降低46.62%的成本，同时保持性能。", "conclusion": "代理计划缓存为LLM代理的服务提供了一种更高效的解决方案，并补充了现有的LLM服务基础设施。", "translation": "基于大型语言模型（LLM）的代理应用在复杂工作流中展现出越来越卓越的能力，但由于广泛的规划和推理需求而产生高昂成本。现有的LLM缓存技术（如上下文缓存和语义缓存）主要为聊天机器人服务设计，不足以满足输出依赖外部数据或环境上下文的代理应用。我们提出了一种新颖的方法——代理计划缓存，它从代理应用的规划阶段提取、存储、适应和重用结构化计划模板，以减少服务成本。与传统语义缓存不同，我们的系统在测试时从已完成的代理执行中提取计划模板，采用关键词提取将新请求与缓存计划匹配，并利用轻量级模型将这些模板适应于带有上下文的任务特定计划。在多个真实世界代理应用上的评估表明，我们的系统平均可降低46.62%的成本，同时保持性能，为LLM代理的服务提供了一种更高效的解决方案，并补充了现有的LLM服务基础设施。", "summary": "本文提出了一种名为“代理计划缓存”的新颖方法，旨在降低LLM代理应用的服务成本。针对现有缓存技术对代理应用不足的问题，该方法在测试时从已完成的代理执行中提取并重用结构化计划模板。通过关键词匹配和轻量级模型适应，该系统在保持性能的同时，平均可降低46.62%的成本，为LLM代理的部署提供了更高效的解决方案。", "keywords": "LLM代理, 成本效益, 计划缓存, 测试时, 代理应用", "comments": "这篇论文提出了一种创新的方法来解决LLM代理应用的高成本问题，通过利用代理的规划特性，而非仅仅是文本输出，实现了显著的成本节约。其“测试时计划缓存”的概念区别于传统的上下文或语义缓存，更贴合代理的工作机制。该方法不仅提高了效率，而且兼容现有基础设施，具有重要的实践意义。"}}
{"id": "2506.15263", "title": "Minimizing Structural Vibrations via Guided Flow Matching Design Optimization", "authors": ["Jan van Delden", "Julius Schultz", "Sebastian Rothe", "Christian Libner", "Sabine C. Langer", "Timo Lüddecke"], "summary": "Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.15263v1", "AI": {"title_translation": "引导流匹配设计优化最小化结构振动", "tldr": "本文提出一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕来减少结构振动，并生成多样化、可制造且振动降低的设计。", "motivation": "结构振动是汽车、火车或飞机等工程系统中不必要的噪音源，最小化这些振动对于提高乘客舒适度至关重要。", "method": "该方法整合了一个生成式流匹配模型和一个预测结构振动的代理模型。在生成过程中，流匹配模型趋向于可制造性，而代理模型趋向于低振动解决方案。设计空间由流匹配模型及其训练数据隐式定义，无需优化手动定义的设计参数。该方法应用于一系列可微分优化目标，包括通过精心构建目标函数直接优化特定特征频率。", "result": "该方法生成了多样化、可制造的板设计，与随机搜索、基于准则的设计启发式方法和遗传优化相比，这些设计的结构振动显著降低。", "conclusion": "基于引导流匹配的设计优化方法能够有效地生成具有降低振动特性的可制造结构设计，优于传统优化方法，为减轻工程系统中的结构振动提供了一种新颖且有效的设计工具。", "translation": "结构振动是汽车、火车或飞机等工程系统中不必要的噪音源。最小化这些振动对于提高乘客舒适度至关重要。本工作提出了一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕（凹痕）来减少振动。我们的方法整合了一个生成式流匹配模型和一个经过训练用于预测结构振动的代理模型。在生成过程中，流匹配模型趋向于可制造性，而代理模型趋向于低振动解决方案。流匹配模型及其训练数据隐式定义了设计空间，从而能够更广泛地探索潜在解决方案，因为不需要优化手动定义的设计参数。我们将我们的方法应用于一系列可微分优化目标，包括通过精心构建目标函数直接优化特定特征频率。结果表明，与随机搜索、基于准则的设计启发式方法和遗传优化产生的设计相比，我们的方法生成了多样化且可制造的板设计，且结构振动有所降低。代码和数据可在 https://github.com/ecker-lab/Optimizing_Vibrating_Plates 获取。", "summary": "本文介绍了一种新颖的基于引导流匹配的设计优化方法，旨在通过在板状结构中放置压痕来最小化结构振动。该方法结合了生成式流匹配模型和预测振动的代理模型，前者确保可制造性，后者推动低振动解决方案。其优势在于通过隐式定义设计空间，避免了手动参数优化，从而实现更广泛的设计探索。实验结果表明，与现有优化技术相比，该方法能生成多样化、可制造且振动显著降低的板设计。", "keywords": "结构振动, 流匹配, 设计优化, 压痕, 代理模型", "comments": "这项工作通过引入引导流匹配模型进行结构振动优化，展示了其创新性。通过将生成模型与代理模型结合，它不仅能探索广阔的设计空间，还同时兼顾了设计的可制造性和性能，避免了传统参数优化方法的局限性。其重要性在于为工程系统中的噪音和振动控制提供了一种高效、自动化的设计工具。"}}
{"id": "2506.14805", "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14805v1", "AI": {"title_translation": "Argus Inspection: 多模态大语言模型是否拥有潘诺普提斯之眼？", "tldr": "本文引入了Argus Inspection基准和Eye of Panoptes框架，旨在评估多模态大语言模型（MLLMs）在视觉细粒度感知和常识因果推理方面的能力，实验结果显示MLLMs在此方面仍有显著提升空间。", "motivation": "尽管多模态大语言模型（MLLMs）在认知和推理能力上取得了显著进展，但在视觉细粒度感知和常识因果推理方面仍存在挑战，因此需要更全面的评估工具来识别并解决这些不足。", "method": "本文引入了Argus Inspection，一个具有两个难度级别的多模态基准，旨在评估详细的视觉识别和常识因果推理能力。在此基础上，提出了Eye of Panoptes框架，该框架结合了二元参数Sigmoid度量和指示函数，以更全面地评估MLLMs在基于意见的推理任务中的响应。研究在26个主流MLLMs上进行了实验。", "result": "在26个主流多模态大语言模型上进行的实验表明，视觉细粒度推理的最高性能仅达到0.46，这突出表明当前MLLMs在该领域仍存在相当大的提升潜力。", "conclusion": "本研究为多模态大语言模型（MLLMs）的持续完善和未来发展提供了宝贵的视角和明确的方向。", "translation": "随着多模态大语言模型（MLLMs）的不断发展，它们的认知和推理能力取得了显著进步。然而，视觉细粒度感知和常识因果推理方面的挑战依然存在。本文引入了Argus Inspection，一个具有两个难度级别的多模态基准，强调详细的视觉识别，同时结合现实世界常识理解来评估因果推理能力。在此基础上，我们提出了“潘诺普提斯之眼”（Eye of Panoptes）框架，该框架将二元参数Sigmoid度量与指示函数相结合，从而能够更全面地评估MLLMs在基于意见的推理任务中的响应。在26个主流MLLMs上进行的实验表明，视觉细粒度推理的最高性能仅达到0.46，这突出表明仍有相当大的提升潜力。我们的研究为MLLMs的持续完善提供了宝贵的视角。", "summary": "本文针对多模态大语言模型（MLLMs）在视觉细粒度感知和常识因果推理方面的现有挑战，提出了创新的Argus Inspection基准和Eye of Panoptes评估框架。Argus Inspection是一个双难度多模态基准，专注于评估详细视觉识别和常识因果推理能力。Eye of Panoptes框架则通过结合二元参数Sigmoid度量和指示函数，实现了对MLLMs在基于意见推理任务中响应的全面评估。对26个主流MLLMs的实验结果显示，视觉细粒度推理的最高性能仅为0.46，这明确指出了MLLMs在该领域仍有巨大的改进空间，为未来的研究和发展提供了重要方向。", "keywords": "多模态大语言模型, 视觉细粒度感知, 常识因果推理, 基准测试, 评估框架", "comments": "本文创新性地提出了Argus Inspection基准和Eye of Panoptes评估框架，为全面评估多模态大语言模型在细粒度视觉感知和常识因果推理方面的能力提供了标准化工具。其重要性在于揭示了当前主流MLLMs在该领域的显著局限性，即最高性能仅为0.46，这为未来MLLMs的研究和开发指明了清晰且亟待改进的方向。"}}
{"id": "2506.14922", "title": "FORTRESS: Frontier Risk Evaluation for National Security and Public Safety", "authors": ["Christina Q. Knight", "Kaustubh Deshpande", "Ved Sirdeshmukh", "Meher Mankikar", "Scale Red Team", "SEAL Research Team", "Julian Michael"], "summary": "The rapid advancement of large language models (LLMs) introduces dual-use\ncapabilities that could both threaten and bolster national security and public\nsafety (NSPS). Models implement safeguards to protect against potential misuse\nrelevant to NSPS and allow for benign users to receive helpful information.\nHowever, current benchmarks often fail to test safeguard robustness to\npotential NSPS risks in an objective, robust way. We introduce FORTRESS: 500\nexpert-crafted adversarial prompts with instance-based rubrics of 4-7 binary\nquestions for automated evaluation across 3 domains (unclassified information\nonly): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE),\nPolitical Violence & Terrorism, and Criminal & Financial Illicit Activities,\nwith 10 total subcategories across these domains. Each prompt-rubric pair has a\ncorresponding benign version to test for model over-refusals. This evaluation\nof frontier LLMs' safeguard robustness reveals varying trade-offs between\npotential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low\naverage risk score (ARS) (14.09 out of 100) but the highest over-refusal score\n(ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but\na high average potential risk (66.29). Deepseek-R1 has the highest ARS at\n78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even\ntrade-off between potential risks and over-refusals (with an ARS of 21.69 and\nORS of 5.2). To provide policymakers and researchers with a clear understanding\nof models' potential risks, we publicly release FORTRESS at\nhttps://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a\nprivate set for evaluation.", "comment": "12 pages, 7 figures, submitted to NeurIPS", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14922v1", "AI": {"title_translation": "FORTRESS：国家安全和公共安全前沿风险评估", "tldr": "大型语言模型（LLMs）对国家安全和公共安全（NSPS）构成双重风险。FORTRESS是一个新的基准，旨在稳健评估LLM对此类风险的保障措施，揭示了前沿LLM在风险和实用性之间存在的不同权衡。", "motivation": "当前基准未能客观、稳健地测试大型语言模型（LLMs）保障措施对国家安全和公共安全（NSPS）潜在风险的鲁棒性。LLMs具有双重用途能力，可能威胁或增强NSPS。", "method": "引入了FORTRESS，一个包含500个专家制作的对抗性提示的基准，这些提示附带实例化的评估标准（4-7个二元问题），用于在化学、生物、放射、核和爆炸物（CBRNE）、政治暴力与恐怖主义以及犯罪和金融非法活动这3个领域（共10个子类别）进行自动化评估。每个对抗性提示都配有相应的良性版本，以测试模型的过度拒绝情况。", "result": "对前沿LLM保障措施鲁棒性的评估显示出潜在风险和模型有用性之间的不同权衡：Claude-3.5-Sonnet的平均风险分数（ARS）较低（14.09），但过度拒绝分数（ORS）最高（21.8）；Gemini 2.5 Pro的过度拒绝较低（1.4），但平均潜在风险较高（66.29）；Deepseek-R1的ARS最高（78.05），但ORS最低（0.06）；o1等模型在潜在风险和过度拒绝之间表现出更均衡的权衡（ARS为21.69，ORS为5.2）。", "conclusion": "FORTRESS为政策制定者和研究人员提供了对模型潜在风险的清晰理解。该基准已公开，以促进相关研究。", "translation": "大型语言模型 (LLM) 的快速发展带来了双重用途能力，这既可能威胁也可能增强国家安全和公共安全 (NSPS)。模型实施了保障措施以防止与 NSPS 相关的潜在滥用，并允许良性用户接收有用的信息。然而，当前的基准测试通常无法客观、稳健地测试保障措施对潜在 NSPS 风险的鲁棒性。我们引入了 FORTRESS：500 个由专家精心制作的对抗性提示，并附带实例化的评估标准（4-7 个二元问题），用于跨 3 个领域（仅限非机密信息）的自动化评估：化学、生物、放射、核和爆炸物 (CBRNE)、政治暴力与恐怖主义以及犯罪和金融非法活动，这些领域共有 10 个子类别。每个提示-评估标准对都有一个相应的良性版本，用于测试模型过度拒绝的情况。对前沿 LLM 保障措施鲁棒性的评估揭示了潜在风险和模型有用性之间的不同权衡：Claude-3.5-Sonnet 表现出较低的平均风险分数 (ARS)（14.09 分，满分 100 分），但过度拒绝分数 (ORS) 最高（21.8 分，满分 100 分），而 Gemini 2.5 Pro 表现出较低的过度拒绝（1.4 分），但平均潜在风险较高（66.29 分）。Deepseek-R1 的 ARS 最高，为 78.05 分，但 ORS 最低，仅为 0.06 分。像 o1 这样的模型在潜在风险和过度拒绝之间表现出更均衡的权衡（ARS 为 21.69 分，ORS 为 5.2 分）。为了让政策制定者和研究人员清楚地了解模型的潜在风险，我们通过 https://huggingface.co/datasets/ScaleAI/fortress_public 公开发布了 FORTRESS。我们还保留了一套私人数据集用于评估。", "summary": "本文介绍了FORTRESS，一个用于评估大型语言模型（LLMs）对国家安全和公共安全（NSPS）相关风险的保障措施鲁棒性的新型基准。该基准包含500个专家制作的对抗性提示，涵盖CBRNE、政治暴力与恐怖主义、犯罪和金融非法活动等三个主要领域，并配有用于自动化评估的二元问题评估标准，同时包含良性版本以测试过度拒绝。对前沿LLMs的评估结果显示，不同模型在潜在风险和模型有用性之间存在不同的权衡，例如Claude-3.5-Sonnet风险低但过度拒绝高，而Gemini 2.5 Pro过度拒绝低但风险高。FORTRESS已公开，旨在为政策制定者和研究人员提供模型风险的清晰认识。", "keywords": "大型语言模型, 国家安全, 公共安全, 风险评估, 对抗性测试, 基准", "comments": "这项工作通过引入一个专门针对国家安全和公共安全风险的、专家构建的对抗性评估基准FORTRESS，解决了当前LLM安全评估的不足。其创新之处在于结合了实例化的评估标准和对过度拒绝的测试，提供了更全面和客观的评估方法。这项研究对于理解和减轻LLM的潜在危害，特别是在敏感领域，具有重要的实践意义。公开数据集的举措也有助于推动社区在此领域的研究。"}}
{"id": "2506.15543", "title": "Learning Algorithms in the Limit", "authors": ["Hristo Papazov", "Nicolas Flammarion"], "summary": "This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.", "comment": "Accepted at COLT 2025. This version matches the proceedings version", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15543v1", "AI": {"title_translation": "极限学习算法", "tldr": "本文通过引入计算观测和受限输入源，扩展了Gold的归纳推理框架，以研究在更实际约束下可计算函数的学习能力，并克服了传统输入-输出观测的局限性。", "motivation": "传统输入-输出观测不足以学习极限情况下的通用递归函数类，因此需要扩展Gold的归纳推理框架，引入更实际的约束和新的观测类型来研究可计算函数的学习能力。", "method": "通过扩展Gold的归纳推理框架，引入了时间界限观测（Time-Bound Observations）和策略轨迹观测（Policy-Trajectory Observations）。通过施加计算复杂度约束或补充近似时间界限观测来克服学习障碍。构建了一个关于计算代理观测的正式框架，并将从策略轨迹中学习可计算函数的问题简化为从输入和输出中学习有理函数。", "result": "克服了传统输入-输出观测无法学习通用递归函数类的学习障碍。揭示了从策略轨迹学习可计算函数与有限状态传感器推理之间的联系。负面结果表明，即使对于策略轨迹观测，线性时间可计算函数类也不存在可计算或多项式质量的特征集。", "conclusion": "通过引入计算观测和受限输入源，可以有效扩展归纳推理框架，从而在更现实的约束下实现可计算函数的学习。尽管存在一些局限性，但新方法显著提升了学习能力并揭示了新的理论联系。", "translation": "本文通过扩展Gold的归纳推理框架，引入了计算观测和受限输入源，研究了极限学习可计算函数的问题。作为对传统输入-输出观测的补充，我们引入了时间界限观测和策略轨迹观测，以研究在更实际约束下通用递归函数的学习能力。虽然输入-输出观测不足以学习极限情况下的通用递归函数类，但我们通过施加计算复杂性约束或补充近似时间界限观测来克服了这一学习障碍。此外，我们围绕计算代理的观测建立了一个正式框架，并表明从策略轨迹中学习可计算函数可以简化为从输入和输出中学习有理函数，从而揭示了与有限状态传感器推理的有趣联系。从消极方面来看，我们表明即使对于策略轨迹观测，线性时间可计算函数类也不存在可计算或多项式质量的特征集。", "summary": "本文通过引入计算观测和受限输入源，扩展了Gold的归纳推理框架，以研究在更现实约束下可计算函数的极限学习问题。研究引入了时间界限观测和策略轨迹观测，克服了传统输入-输出观测在学习通用递归函数方面的不足。研究表明，通过施加计算复杂性约束或补充近似时间界限观测可以克服学习障碍，并且从策略轨迹中学习可计算函数可以简化为学习有理函数。然而，对于线性时间可计算函数类，即使有策略轨迹观测，也不存在可计算或多项式质量的特征集。", "keywords": "极限学习, 归纳推理, 计算观测, 策略轨迹, 可计算函数", "comments": "本文的创新之处在于扩展了传统的归纳推理框架，引入了更符合实际的计算观测和受限输入源，从而解决了传统方法在学习通用递归函数类时的局限性。通过引入时间界限和策略轨迹观测，提高了学习能力，并揭示了与有限状态传感器推理的联系，具有重要的理论意义。尽管存在负面结果，但这项工作为可计算函数的学习提供了新的视角和工具。"}}
{"id": "2506.15026", "title": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles: Minimizing Lane Changes and Merging", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "summary": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced\nsafety and operational efficiency. However, frequent lane changes and merging\nmaneuvers continue to pose significant safety risks and disrupt traffic flow.\nThis paper introduces the Minimizing Lane Change Algorithm (MLCA), a\nstate-machine-based approach designed to reduce unnecessary lane changes,\nthereby enhancing both traffic safety and efficiency. The MLCA algorithm\nprioritizes maintaining lane stability unless safety-critical conditions\nnecessitate a lane change. The algorithm's effectiveness was evaluated through\nsimulations conducted on the SUMO platform, comparing its performance against\nestablished models, including LC2017 and MOBIL. Results demonstrate substantial\nreductions in lane changes and collisions, leading to smoother traffic flow and\nimproved safety metrics. Additionally, the study highlights the MLCA's\nadaptability to various traffic densities and roadway configurations,\nshowcasing its potential for wide-scale deployment in real-world AV systems.\nFuture work aims to validate these findings in more complex scenarios using the\nCARLA simulator, which will enable the testing of the algorithm under more\ndynamic and high-fidelity conditions, such as urban traffic environments with\ndiverse road users. Moreover, the integration of cybersecurity measures for\nvehicle-to-vehicle (V2V) communication will be explored to ensure robust and\nsecure data exchange, further enhancing the reliability and safety of AV\noperations. This research contributes to the broader goal of developing\nintelligent traffic systems that optimize both individual vehicle performance\nand overall traffic network efficiency.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15026v1", "AI": {"title_translation": "自动驾驶车辆安全增强的算法方法：最小化变道与并道", "tldr": "本文提出了一种名为最小化变道算法（MLCA）的基于状态机方法，旨在通过减少不必要的变道操作来增强自动驾驶车辆的安全性并提高交通效率，并通过仿真验证了其有效性。", "motivation": "自动驾驶车辆频繁的变道和并道操作会带来显著的安全风险并扰乱交通流。", "method": "本文提出了一种基于状态机的最小化变道算法（MLCA），旨在减少不必要的变道，从而提高交通安全和效率。该算法优先保持车道稳定性，除非安全关键条件要求变道。通过在SUMO平台上进行仿真评估了算法的有效性，并将其性能与包括LC2017和MOBIL在内的现有模型进行了比较。", "result": "仿真结果表明，MLCA显著减少了变道次数和碰撞，从而使交通流更平稳，并改善了安全指标。此外，研究强调了MLCA对各种交通密度和道路配置的适应性，展示了其在实际自动驾驶系统中大规模部署的潜力。", "conclusion": "该研究表明MLCA算法能够通过减少变道来有效提高自动驾驶车辆的安全性与交通效率，并具有在实际自动驾驶系统中广泛部署的潜力，有助于实现开发智能交通系统的更广泛目标，这些系统既优化了单个车辆的性能，又优化了整体交通网络的效率。", "translation": "自动驾驶（AV）技术的快速发展有望提高安全性和运行效率。然而，频繁的变道和并道操作仍然带来显著的安全风险并扰乱交通流。本文引入了最小化变道算法（MLCA），这是一种基于状态机的方法，旨在减少不必要的变道，从而提高交通安全和效率。MLCA算法优先保持车道稳定性，除非安全关键条件要求变道。通过在SUMO平台上进行的仿真评估了该算法的有效性，并将其性能与包括LC2017和MOBIL在内的现有模型进行了比较。结果表明，变道和碰撞显著减少，从而使交通流更平稳，并改善了安全指标。此外，研究强调了MLCA对各种交通密度和道路配置的适应性，展示了其在实际AV系统中大规模部署的潜力。未来的工作旨在利用CARLA模拟器在更复杂的场景中验证这些发现，这将使得在更动态和高保真条件下（例如具有不同道路使用者的城市交通环境）测试该算法成为可能。此外，还将探索车对车（V2V）通信的网络安全措施的集成，以确保数据交换的稳健性和安全性，进一步增强AV操作的可靠性和安全性。这项研究有助于实现开发智能交通系统的更广泛目标，这些系统既优化了单个车辆的性能，又优化了整体交通网络的效率。", "summary": "本文提出了一种名为最小化变道算法（MLCA）的基于状态机方法，旨在通过减少不必要的变道操作来增强自动驾驶车辆的安全性并提高交通效率。该算法优先保持车道稳定性。通过在SUMO平台上的仿真实验，并与LC2017和MOBIL等现有模型进行对比，结果表明MLCA能显著减少变道和碰撞，改善交通流和平稳性，并适用于不同的交通条件，展现了其在实际自动驾驶系统中的应用潜力。", "keywords": "自动驾驶, 变道, 交通安全, MLCA, 仿真", "comments": "本文提出了一种新颖的基于状态机的MLCA算法，专注于解决自动驾驶车辆变道和并道带来的安全隐患。其创新点在于优先保持车道稳定性，并通过仿真验证了其在减少碰撞和提高交通效率方面的有效性。该研究对于提升自动驾驶车辆的实际部署安全性和交通流畅度具有重要意义。"}}
{"id": "2506.14990", "title": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning", "authors": ["Tristan Tomilin", "Luka van den Boogaard", "Samuel Garcin", "Bram Grooten", "Meng Fang", "Mykola Pechenizkiy"], "summary": "Benchmarks play a crucial role in the development and analysis of\nreinforcement learning (RL) algorithms, with environment availability strongly\nimpacting research. One particularly underexplored intersection is continual\nlearning (CL) in cooperative multi-agent settings. To remedy this, we introduce\nMEAL (Multi-agent Environments for Adaptive Learning), the first benchmark\ntailored for continual multi-agent reinforcement learning (CMARL). Existing CL\nbenchmarks run environments on the CPU, leading to computational bottlenecks\nand limiting the length of task sequences. MEAL leverages JAX for GPU\nacceleration, enabling continual learning across sequences of 100 tasks on a\nstandard desktop PC in a few hours. We show that naively combining popular CL\nand MARL methods yields strong performance on simple environments, but fails to\nscale to more complex settings requiring sustained coordination and adaptation.\nOur ablation study identifies architectural and algorithmic features critical\nfor CMARL on MEAL.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.14990v1", "AI": {"title_translation": "MEAL：一个持续多智能体强化学习基准", "tldr": "MEAL是首个专为持续多智能体强化学习设计的基准，它利用JAX进行GPU加速，解决了现有基准的计算瓶颈，并揭示了现有方法在复杂CMARL任务上的局限性。", "motivation": "现有强化学习算法的开发和分析受到基准可用性的影响，而合作多智能体环境中的持续学习是一个未被充分探索的领域。为了解决这一问题，研究者需要一个专门针对持续多智能体强化学习（CMARL）的基准。", "method": "研究者引入了MEAL（Multi-agent Environments for Adaptive Learning），这是第一个专为持续多智能体强化学习（CMARL）量身定制的基准。MEAL利用JAX进行GPU加速，从而能够在标准台式电脑上在数小时内完成100个任务序列的持续学习。", "result": "简单地结合流行的持续学习（CL）和多智能体强化学习（MARL）方法在简单环境中表现良好，但无法扩展到需要持续协调和适应的更复杂设置。消融研究识别出对MEAL上CMARL至关重要的架构和算法特征。", "conclusion": "MEAL基准揭示了当前CL和MARL方法在处理复杂持续多智能体强化学习任务时的局限性，并指出了未来研究需要关注的关键架构和算法特性。", "translation": "基准在强化学习（RL）算法的开发和分析中扮演着关键角色，环境的可用性强烈影响着研究。其中一个特别未被充分探索的交叉点是合作多智能体环境中的持续学习（CL）。为了弥补这一点，我们引入了MEAL（Multi-agent Environments for Adaptive Learning），这是第一个专为持续多智能体强化学习（CMARL）量身定制的基准。现有的CL基准在CPU上运行环境，导致计算瓶颈并限制了任务序列的长度。MEAL利用JAX进行GPU加速，使得在标准台式电脑上在数小时内就能实现100个任务序列的持续学习。我们发现，简单地结合流行的CL和MARL方法在简单环境中表现出强大的性能，但无法扩展到需要持续协调和适应的更复杂设置。我们的消融研究确定了MEAL上对CMARL至关重要的架构和算法特征。", "summary": "该论文介绍了MEAL，这是首个专为持续多智能体强化学习（CMARL）设计的基准。针对现有持续学习基准在CPU上运行导致的计算瓶颈，MEAL利用JAX进行GPU加速，实现了在标准PC上数小时内完成100个任务序列的持续学习。研究表明，简单结合现有持续学习和多智能体强化学习方法在简单环境中表现良好，但在复杂环境中无法扩展。消融研究进一步识别了对CMARL关键的架构和算法特征。", "keywords": "持续多智能体强化学习, 基准, JAX, GPU加速, 持续学习", "comments": "MEAL的创新之处在于它是第一个针对持续多智能体强化学习的专用基准，并通过JAX实现GPU加速，显著提升了计算效率，使得更长的任务序列成为可能。这对于推动CMARL领域的研究至关重要，因为它揭示了现有方法在复杂持续多智能体任务中的局限性，并为未来算法设计提供了方向。"}}
{"id": "2506.14985", "title": "Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization", "authors": ["Kuranage Roche Rayan Ranasinghe", "Hyeon Seok Rou", "Iván Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "George C. Alexandropoulos"], "summary": "The doubly-dispersive (DD) channel structure has played a pivotal role in\nwireless communications, particularly in high-mobility scenarios and integrated\nsensing and communications (ISAC), due to its ability to capture the key fading\neffects experienced by a transmitted signal as it propagates through a dynamic\nmedium. However, extending the DD framework to multiple-input multiple-output\n(MIMO) systems, especially in environments artificially enhanced by\nreconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces\n(SIM), remains a challenging open problem. In this chapter, a novel\nmetasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary\nnumber of RISs, while also incorporating SIM at both the transmitter and\nreceiver is introduced. Next, the application of this model to some key\nwaveforms optimized for DD environments -- namely orthogonal frequency division\nmultiplexing (OFDM), orthogonal time frequency space (OTFS), and affine\nfrequency division multiplexing (AFDM) -- is discussed. Finally, the\nprogrammability of the proposed model is highlighted through an illustrative\napplication, demonstrating its potential for enhancing waveform performance in\nSIM-assisted wireless systems.", "comment": "Author's version of chapter from forthcoming book \"Reconfigurable\n  Metasurfaces for Wireless Communications: Architectures, Modeling, and\n  Optimization\"", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14985v1", "AI": {"title_translation": "超表面集成双色散MIMO：信道建模与优化", "tldr": "本文引入了一种新的超表面参数化双色散（MPDD）信道模型，该模型集成了可重构智能表面（RIS）和堆叠智能超表面（SIM），并探讨了其在各种波形优化中的应用，以增强SIM辅助无线系统中的波形性能。", "motivation": "双色散（DD）信道结构在无线通信中至关重要，尤其是在高移动性和集成传感与通信（ISAC）场景中。然而，将DD框架扩展到多输入多输出（MIMO）系统，特别是在可重构智能表面（RIS）和堆叠智能超表面（SIM）增强的环境中，仍然是一个具有挑战性的开放问题。", "method": "本文引入了一种新颖的超表面参数化双色散（MPDD）信道模型，该模型集成了任意数量的RIS，同时在发射器和接收器处结合了SIM。接着，讨论了该模型在针对DD环境优化的关键波形（如OFDM、OTFS和AFDM）中的应用。", "result": "通过一个说明性应用，展示了所提出模型的可编程性，并证明了其在增强SIM辅助无线系统中的波形性能的潜力。", "conclusion": "本文提出的超表面参数化双色散（MPDD）信道模型为在超表面增强的无线系统中建模和优化双色散信道提供了新的工具，并展示了其在提升波形性能方面的潜力。", "translation": "双色散（DD）信道结构在无线通信中发挥了举足轻重的作用，特别是在高移动性场景和集成传感与通信（ISAC）中，因为它能够捕捉信号在动态介质中传播时所经历的关键衰落效应。然而，将DD框架扩展到多输入多输出（MIMO）系统，尤其是在由可重构智能表面（RIS）和堆叠智能超表面（SIM）人工增强的环境中，仍然是一个具有挑战性的开放问题。在本章中，引入了一种新颖的超表面参数化DD（MPDD）信道模型，该模型集成了任意数量的RIS，同时在发射器和接收器处结合了SIM。接下来，讨论了该模型在针对DD环境优化的某些关键波形（即正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM））中的应用。最后，通过一个说明性应用突出了所提出模型的可编程性，展示了其在增强SIM辅助无线系统中的波形性能的潜力。", "summary": "本文提出了一种新的超表面参数化双色散（MPDD）信道模型，旨在解决在超表面增强的多输入多输出（MIMO）系统中建模双色散（DD）信道的挑战。该模型集成了可重构智能表面（RIS）和堆叠智能超表面（SIM），并探讨了其在优化正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM）等波形中的应用。研究结果表明，该模型具有可编程性，并能有效提升SIM辅助无线系统中的波形性能。", "keywords": "双色散信道, 超表面, MIMO, 信道建模, 波形优化", "comments": "该论文的创新点在于提出了一个集成了RIS和SIM的超表面参数化双色散（MPDD）信道模型，为高移动性MIMO和ISAC系统中的信道建模提供了新的视角。其重要性在于解决了DD框架在超表面增强MIMO系统中的扩展难题，并展示了模型在优化不同波形方面的潜力，这对于未来无线通信系统的设计具有指导意义。"}}
{"id": "2506.15097", "title": "Efficient space reduction techniques by optimized majority rules for the Kemeny aggregation problem", "authors": ["Xuan Kien Phung", "Sylvie Hamel"], "summary": "The Kemeny aggregation problem consists of computing the consensus rankings\nof an election with respect to the Kemeny-Young voting method. These aggregated\nrankings are the geometric medians as well as the maximum likelihood estimators\nin the Mallows model of the rankings in the election under the Kendall-tau\ndistance which counts the number of pairwise disagreements. The problem admits\nfundamental applications in various domains such as computational social\nchoice, machine learning, operations research, and biology but its\ncomputational complexity is unfortunately expensive. In this paper, we\nestablish optimized quantitative extensions of the well-known 3/4-majority rule\nof Betzler et al. and the Major Order Theorem of Hamel and Milosz for the\nKemeny aggregation problem. By taking into account the extra information\navailable in the problem such as the number of candidates and by considering an\nadditional optimization of certain piecewise linear functions in one variable,\nour results achieve significantly more refined space reduction techniques as\nillustrated by experimental results on real and synthetic data without\nincreasing the time complexity of the algorithms.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.15097v1", "AI": {"title_translation": "Kemeny聚合问题中基于优化多数规则的有效空间缩减技术", "tldr": "本文通过优化多数规则，为Kemeny聚合问题提供了更有效的空间缩减技术，显著提高了效率且不增加时间复杂度。", "motivation": "Kemeny聚合问题在计算社会选择、机器学习、运筹学和生物学等领域有广泛应用，但其计算复杂度很高，因此需要更高效的解决方案。", "method": "本文针对Kemeny聚合问题，提出了Betzler等人提出的3/4多数规则和Hamel与Milosz的多数顺序定理的优化量化扩展。方法考虑了问题中的额外信息（如候选人数量），并对某些单变量分段线性函数进行了额外优化。", "result": "实验结果表明，该方法在真实和合成数据上实现了显著更精细的空间缩减技术，并且没有增加算法的时间复杂度。", "conclusion": "通过优化已知的多数规则并利用额外信息，本文成功开发了更高效的Kemeny聚合问题空间缩减技术。", "translation": "Kemeny聚合问题包括计算选举中关于Kemeny-Young投票方法的共识排名。这些聚合排名是Mallows模型中选举排名的几何中位数和最大似然估计器，基于Kendall-tau距离（该距离计算成对不一致的数量）。该问题在计算社会选择、机器学习、运筹学和生物学等各个领域都有基本应用，但不幸的是其计算复杂度很高。在本文中，我们针对Kemeny聚合问题建立了Betzler等人提出的著名3/4多数规则和Hamel与Milosz的多数顺序定理的优化量化扩展。通过考虑问题中可用的额外信息，例如候选人数量，并考虑对某些单变量分段线性函数的额外优化，我们的结果实现了显著更精细的空间缩减技术，如在真实和合成数据上的实验结果所示，同时没有增加算法的时间复杂度。", "summary": "Kemeny聚合问题因其高计算复杂度而限制了其在多领域的应用。本文提出了一种通过优化现有多数规则（如3/4多数规则和多数顺序定理）来提高Kemeny聚合问题空间缩减效率的方法。该方法利用了问题中的额外信息并优化了分段线性函数。实验结果表明，这种新方法在不增加时间复杂度的情况下，显著提升了空间缩减的精细度。", "keywords": "Kemeny聚合, 空间缩减, 多数规则, 排名聚合, 计算复杂度", "comments": "本文的创新之处在于对现有Kemeny聚合问题中的空间缩减技术进行了优化和量化扩展，特别是结合了额外信息和分段线性函数优化。这对于解决该问题的高计算复杂度具有重要意义，使其在实际应用中更具可行性。"}}
{"id": "2506.14781", "title": "Two-dimensional Parallel Tempering for Constrained Optimization", "authors": ["Corentin Delacour", "M Mahmudul Hasan Sajeeb", "Joao P. Hespanha", "Kerem Y. Camsari"], "summary": "Sampling Boltzmann probability distributions plays a key role in machine\nlearning and optimization, motivating the design of hardware accelerators such\nas Ising machines. While the Ising model can in principle encode arbitrary\noptimization problems, practical implementations are often hindered by soft\nconstraints that either slow down mixing when too strong, or fail to enforce\nfeasibility when too weak. We introduce a two-dimensional extension of the\npowerful parallel tempering algorithm (PT) that addresses this challenge by\nadding a second dimension of replicas interpolating the penalty strengths. This\nscheme ensures constraint satisfaction in the final replicas, analogous to\nlow-energy states at low temperature. The resulting two-dimensional parallel\ntempering algorithm (2D-PT) improves mixing in heavily constrained replicas and\neliminates the need to explicitly tune the penalty strength. In a\nrepresentative example of graph sparsification with copy constraints, 2D-PT\nachieves near-ideal mixing, with Kullback-Leibler divergence decaying as\nO(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of\nmagnitude speedup over conventional PT with the same number of replicas. The\nmethod applies broadly to constrained Ising problems and can be deployed on\nexisting Ising machines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14781v1", "AI": {"title_translation": "用于约束优化的二维并行回火法", "tldr": "引入二维并行回火法(2D-PT)以有效解决伊辛机上的约束优化问题，改善了混合并消除了惩罚参数调整的需求。", "motivation": "玻尔兹曼概率分布采样在机器学习和优化中起着关键作用，促使了伊辛机等硬件加速器的设计。然而，伊辛模型在实际应用中常受软约束的阻碍，这些约束在过强时会减慢混合，在过弱时则无法保证可行性。", "method": "该研究引入了强大并行回火算法（PT）的二维扩展，通过增加一个在惩罚强度之间进行插值的副本维度来解决软约束问题。这种方案确保了最终副本中的约束满足，类似于低温下的低能量状态。由此产生的二维并行回火算法（2D-PT）改善了重度约束副本中的混合，并消除了显式调整惩罚强度的需要。", "result": "在具有复制约束的图稀疏化代表性示例中，2D-PT实现了接近理想的混合，库尔巴克-莱布勒散度以O(1/t)的速度衰减。当应用于稀疏化Wishart实例时，2D-PT与相同数量副本的传统PT相比，实现了数量级的加速。", "conclusion": "该方法广泛适用于约束伊辛问题，并可部署在现有伊辛机上。", "translation": "玻尔兹曼概率分布采样在机器学习和优化中起着关键作用，促使了伊辛机等硬件加速器的设计。虽然伊辛模型原则上可以编码任意优化问题，但实际实现常常受到软约束的阻碍，这些约束在过强时会减慢混合，在过弱时则无法保证可行性。我们引入了强大并行回火算法（PT）的二维扩展，通过增加一个在惩罚强度之间进行插值的副本维度来解决这一挑战。这种方案确保了最终副本中的约束满足，类似于低温下的低能量状态。由此产生的二维并行回火算法（2D-PT）改善了重度约束副本中的混合，并消除了显式调整惩罚强度的需要。在具有复制约束的图稀疏化代表性示例中，2D-PT实现了接近理想的混合，库尔巴克-莱布勒散度以O(1/t)的速度衰减。当应用于稀疏化Wishart实例时，2D-PT与相同数量副本的传统PT相比，实现了数量级的加速。该方法广泛适用于约束伊辛问题，并可部署在现有伊辛机上。", "summary": "该论文介绍了一种二维并行回火法（2D-PT），它是并行回火算法的扩展，旨在解决基于伊辛模型的优化中软约束带来的挑战。通过增加一个在惩罚强度之间进行插值的副本维度，2D-PT改善了重度约束系统中的混合，并消除了对惩罚参数显式调整的需求。实验结果表明，在图稀疏化和Wishart实例等任务中，2D-PT与传统PT相比实现了接近理想的混合和显著的加速，证明了其在约束伊辛问题上的广泛适用性以及与现有伊辛机的兼容性。", "keywords": "并行回火, 约束优化, 伊辛模型, 玻尔兹曼采样, 硬件加速器", "comments": "这项创新在于将经典的并行回火算法扩展到第二个维度，专门用于插值惩罚强度，从而有效解决了伊辛模型中软约束调优的长期问题。这对于伊辛机等硬件加速器来说是一个重大进步，因为它有望在约束优化问题上提高性能并简化参数管理。所展示的量级加速突出了其在实践中的重要性。"}}
{"id": "2506.15290", "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models", "authors": ["Andela Ilic", "Jiaxi Jiang", "Paul Streli", "Xintong Liu", "Christian Holz"], "summary": "Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.", "comment": "Accepted by IJCAI 2025", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15290v1", "AI": {"title_translation": "基于松散稀疏惯性传感器的服装感知扩散模型人体运动捕捉", "tldr": "本文提出了一种使用稀疏、松散佩戴的惯性传感器进行全身姿态估计的新任务，并开发了基于Transformer的服装感知扩散模型来解决该问题，实验证明其性能优于现有SOTA方法。", "motivation": "现有的基于惯性测量单元（IMU）的运动捕捉方法通常假设传感器紧密附着在人体上，但在现实世界场景中这一假设往往不成立，导致实际应用受限。", "method": "本文提出了一种使用稀疏、松散佩戴IMU传感器进行全身人体姿态估计的新任务。为解决此任务，作者从现有服装感知人体运动数据集中模拟IMU记录，并开发了基于Transformer的扩散模型来合成松散IMU数据并估计人体姿态。此外，在模型训练过程中融入服装相关参数，以增强对松散或紧身服装引起变化的捕捉能力。", "result": "实验表明，本文提出的在模拟和合成数据上训练的扩散方法在定量和定性上均优于现有的最先进方法。", "conclusion": "本研究为未来人体运动捕捉领域开辟了一个有前景的方向。", "translation": "使用稀疏惯性传感器进行运动捕捉因其便携性和相对于基于摄像头的跟踪没有遮挡问题而显示出巨大的前景。现有方法通常假设惯性测量单元（IMU）传感器紧密附着在人体上。然而，在现实世界场景中，这种假设往往不成立。在本文中，我们提出了一项使用稀疏、松散附着的IMU传感器进行全身人体姿态估计的新任务。为了解决这项任务，我们从现有服装感知人体运动数据集中模拟了IMU记录。我们开发了基于Transformer的扩散模型来合成松散的IMU数据，并根据这些具有挑战性的松散IMU数据估计人体姿态。此外，我们表明，在模拟松散数据上训练模型时结合服装相关参数，能有效地保持表达能力，并增强捕捉由更松散或更紧身服装引入的变化的能力。实验表明，我们提出的在模拟和合成数据上训练的扩散方法在定量和定性上均优于最先进的方法，为未来的研究开辟了一个有前景的方向。", "summary": "本文针对稀疏、松散佩戴IMU传感器进行全身人体姿态估计的挑战性新任务，提出了基于Transformer的服装感知扩散模型。通过模拟IMU数据并结合服装相关参数进行训练，该模型能够有效处理传感器松动带来的变异，并在实验中超越了现有最先进的运动捕捉方法，为真实世界应用奠定了基础。", "keywords": "人体运动捕捉, 惯性传感器, 扩散模型, 服装感知, 姿态估计", "comments": "本文的创新点在于提出了一个更贴近真实世界场景的新任务：从松散稀疏的惯性传感器中捕捉人体运动。通过引入服装感知扩散模型，并利用模拟数据进行训练，该研究有效地解决了传感器与身体之间松动带来的姿态估计挑战，提高了方法的鲁棒性和实用性。这项工作为未来在非受控环境下进行人体运动捕捉提供了新的思路和强大的工具。"}}
{"id": "2506.15267", "title": "Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative Next-User Modeling", "authors": ["Yu-Ting Lan", "Yang Huo", "Yi Shen", "Xiao Yang", "Zuotao Liu"], "summary": "The item cold-start problem is critical for online recommendation systems, as\nthe success of this phase determines whether high-quality new items can\ntransition to popular ones, receive essential feedback to inspire creators, and\nthus lead to the long-term retention of creators. However, modern\nrecommendation systems still struggle to address item cold-start challenges due\nto the heavy reliance on item and historical interactions, which are\nnon-trivial for cold-start items lacking sufficient exposure and feedback.\nLookalike algorithms provide a promising solution by extending feedback for new\nitems based on lookalike users. Traditional lookalike algorithms face such\nlimitations: (1) failing to effectively model the lookalike users and further\nimprove recommendations with the existing rule- or model-based methods; and (2)\nstruggling to utilize the interaction signals and incorporate diverse features\nin modern recommendation systems.\n  Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel\nframework for enhancing cold-start recommendations via generative next-user\nmodeling. Specifically, we employ a transformer-based model to capture the\nunidirectional relationships among recently interacted users and utilize these\nsequences to generate the next potential user who is most likely to interact\nwith the item. The additional item features are also integrated as prefix\nprompt embeddings to assist the next-user generation. The effectiveness of\nNext-User Retrieval is evaluated through both offline experiments and online\nA/B tests. Our method achieves significant improvements with increases of\n0.0142% in daily active users and +0.1144% in publications in Douyin,\nshowcasing its practical applicability and scalability.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15267v1", "AI": {"title_translation": "下一用户检索：通过生成式下一用户建模增强冷启动推荐", "tldr": "该论文提出了一种名为“下一用户检索”的新框架，通过生成式下一用户建模来增强冷启动推荐，利用Transformer模型预测下一个潜在用户，并在抖音上取得了显著的线上线下效果提升。", "motivation": "在线推荐系统面临物品冷启动问题，这对于新物品的流行和创作者的留存至关重要。然而，由于严重依赖物品和历史互动，现有系统难以解决冷启动挑战。传统的相似用户算法在有效建模相似用户和利用多样化特征方面存在局限性。", "method": "提出“下一用户检索”框架，通过生成式下一用户建模增强冷启动推荐。具体地，采用基于Transformer的模型来捕获最近互动用户之间的单向关系，并利用这些序列生成最有可能与物品互动的下一个潜在用户。同时，将额外的物品特征作为前缀提示嵌入集成，以辅助下一用户的生成。", "result": "通过离线实验和在线A/B测试验证了“下一用户检索”的有效性。在抖音上，每日活跃用户增加了0.0142%，发布量增加了0.1144%，显示了其在实际应用中的可行性和可扩展性。", "conclusion": "“下一用户检索”方法有效增强了冷启动推荐，并在实际应用中展现出良好的实用性和可扩展性。", "translation": "物品冷启动问题对于在线推荐系统至关重要，因为这一阶段的成功决定了高质量的新物品能否转变为流行物品，获得必要的反馈以激励创作者，从而实现创作者的长期留存。然而，现代推荐系统仍然难以解决物品冷启动挑战，因为它们严重依赖物品和历史互动，这对于缺乏足够曝光和反馈的冷启动物品来说并非易事。相似用户算法通过基于相似用户扩展新物品的反馈提供了一种有前景的解决方案。传统的相似用户算法面临以下局限性：(1)未能有效建模相似用户，并进一步通过现有的基于规则或模型的方法改进推荐；(2)难以利用互动信号并在现代推荐系统中整合多样化特征。\n受相似用户算法的启发，我们提出了“下一用户检索”，这是一个通过生成式下一用户建模来增强冷启动推荐的新颖框架。具体来说，我们采用一个基于Transformer的模型来捕获最近互动用户之间的单向关系，并利用这些序列来生成最有可能与物品互动的下一个潜在用户。额外的物品特征也被整合为前缀提示嵌入，以辅助下一用户的生成。“下一用户检索”的有效性通过离线实验和在线A/B测试进行了评估。我们的方法在抖音上取得了显著改进，每日活跃用户增加了0.0142%，发布量增加了+0.1144%，展示了其在实际应用中的可行性和可扩展性。", "summary": "本文旨在解决推荐系统中的物品冷启动问题，该问题因新物品缺乏足够的历史交互数据而难以有效推荐。为此，论文提出了一个名为“下一用户检索”（Next-User Retrieval）的新颖框架，通过生成式下一用户建模来增强冷启动推荐。该框架利用一个基于Transformer的模型来捕捉用户间的单向交互序列关系，并据此生成最有可能与特定物品交互的下一个潜在用户，同时将物品特征作为提示嵌入融合。通过离线实验和在抖音平台进行的在线A/B测试，该方法在每日活跃用户和发布量方面均取得了显著提升，验证了其在实际应用中的有效性和可扩展性。", "keywords": "冷启动推荐, 生成式建模, 下一用户预测, Transformer, 相似用户算法", "comments": "该论文通过引入生成式下一用户建模，为解决推荐系统中的冷启动问题提供了一个创新性的方法，超越了传统的相似用户算法的局限性。利用Transformer模型来预测下一个潜在用户，并将物品特征作为提示嵌入集成，展现了技术上的新颖性。其在抖音这样的大规模平台上的实际效果提升，尤其是在DAU和发布量方面的积极影响，凸显了该方法的实用价值和潜力。"}}
{"id": "2506.15222", "title": "The Pitfalls and Potentials of Adding Gene-invariance to Optimal Mixing", "authors": ["Anton Bouter", "Dirk Thierens", "Peter A. N. Bosman"], "summary": "Optimal Mixing (OM) is a variation operator that integrates local search with\ngenetic recombination. EAs with OM are capable of state-of-the-art optimization\nin discrete spaces, offering significant advantages over classic\nrecombination-based EAs. This success is partly due to high selection pressure\nthat drives rapid convergence. However, this can also negatively impact\npopulation diversity, complicating the solving of hierarchical problems, which\nfeature multiple layers of complexity. While there have been attempts to\naddress this issue, these solutions are often complicated and prone to bias. To\novercome this, we propose a solution inspired by the Gene Invariant Genetic\nAlgorithm (GIGA), which preserves gene frequencies in the population throughout\nthe process. This technique is tailored to and integrated with the Gene-pool\nOptimal Mixing Evolutionary Algorithm (GOMEA), resulting in GI-GOMEA. The\nsimple, yet elegant changes are found to have striking potential: GI-GOMEA\noutperforms GOMEA on a range of well-known problems, even when these problems\nare adjusted for pitfalls - biases in much-used benchmark problems that can be\neasily exploited by maintaining gene invariance. Perhaps even more notably,\nGI-GOMEA is also found to be effective at solving hierarchical problems,\nincluding newly introduced asymmetric hierarchical trap functions.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.15222v1", "AI": {"title_translation": "在最优混合中加入基因不变性的陷阱与潜力", "tldr": "通过引入基因不变性，GI-GOMEA改进了最优混合算法，解决了种群多样性问题，并在多种问题（包括分层问题）上超越了GOMEA。", "motivation": "最优混合（OM）算法虽然收敛速度快，但高选择压力会负面影响种群多样性，从而使解决分层问题变得复杂。现有解决方案通常复杂且容易产生偏差。", "method": "提出了一种受基因不变遗传算法（GIGA）启发的解决方案，该方案在整个过程中保留了种群中的基因频率。这项技术经过调整并与基因池最优混合进化算法（GOMEA）集成，形成了GI-GOMEA。", "result": "GI-GOMEA在一系列知名问题上表现优于GOMEA，即使在考虑了陷阱（基准问题中的偏差）的情况下也是如此。它在解决分层问题方面也有效，包括新引入的非对称分层陷阱函数。", "conclusion": "通过GI-GOMEA将基因不变性引入最优混合算法是一种简单而有效的方法，可以缓解多样性损失并有效解决复杂的分层问题，超越了原始的GOMEA。", "translation": "最优混合（OM）是一种将局部搜索与遗传重组相结合的变异算子。采用OM的进化算法（EA）能够在离散空间中实现最先进的优化，与经典的基于重组的EA相比具有显著优势。这种成功部分归因于高选择压力，它驱动着快速收敛。然而，这也可能对种群多样性产生负面影响，从而使解决具有多层复杂性的分层问题变得复杂。尽管已经有人尝试解决这个问题，但这些解决方案通常复杂且容易产生偏差。为了克服这个问题，我们提出了一种受基因不变遗传算法（GIGA）启发的解决方案，该方案在整个过程中保留了种群中的基因频率。这项技术经过调整并与基因池最优混合进化算法（GOMEA）集成，形成了GI-GOMEA。研究发现，这些简单而优雅的改变具有惊人的潜力：GI-GOMEA在一系列知名问题上表现优于GOMEA，即使在这些问题经过调整以应对陷阱——即常用基准问题中可以通过保持基因不变性轻易利用的偏差——的情况下也是如此。也许更值得注意的是，GI-GOMEA还被发现能有效解决分层问题，包括新引入的非对称分层陷阱函数。", "summary": "最优混合（OM）在离散优化中表现出色，但由于高选择压力而导致多样性损失，从而影响其在分层问题上的性能。本文提出了一种受基因不变遗传算法（GIGA）启发的GI-GOMEA新方法，该方法将基因频率保留集成到GOMEA中。GI-GOMEA在各种基准问题上表现出优于GOMEA的性能，即使是那些具有内在偏差的问题，并且在解决复杂的分层问题（包括新的非对称分层陷阱函数）方面特别有效。这些简单的修改显示出增强进化算法的巨大潜力。", "keywords": "最优混合, 基因不变性, 进化算法, 分层问题, GOMEA", "comments": "该论文提出了一种对现有最先进算法（GOMEA）的优雅而简单的修改（基因不变性），以解决一个关键限制（多样性损失和分层问题）。它在有偏基准和分层问题上的有效性突出了其在不增加复杂性的情况下保持多样性的实际意义和创新性。"}}
{"id": "2506.14901", "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction", "authors": ["Marija Šakota", "Robert West"], "summary": "Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14901v1", "AI": {"title_translation": "通过Boosting结合受限和非受限解码：BoostCD及其在信息抽取中的应用", "tldr": "本文介绍了BoostCD，一种通过增强模型结合受限和非受限解码的方法，以提高结构化NLP任务的输出质量，并将其应用于信息抽取。", "motivation": "当前结构化NLP任务中，使用自回归语言模型通过受限解码生成结构化输出的方法，在训练时无需模型感知约束，从而允许动态约束。然而，这可能导致测试时受限解码的输出质量较低。", "method": "本文提出了Boosted Constrained Decoding (BoostCD)，它分两个阶段结合了受限和非受限解码。第一阶段，基础模型M在受限和非受限模式下各解码一次，得到两个弱预测。第二阶段，一个学习到的自回归增强模型将这两个弱预测组合成一个最终预测，利用了基础模型在有约束和无约束情况下错误互补的特点。", "result": "将BoostCD应用于封闭信息抽取任务（BoostIE），该模型在分布内和分布外都优于现有方法，并解决了这些方法中常见的几个错误。", "conclusion": "本文提出了BoostCD，一种通过结合受限和非受限解码来提高结构化NLP输出质量的新方法。将其应用于信息抽取（BoostIE）显示出卓越的性能，并解决了现有方法中的常见错误。", "translation": "许多最近的结构化NLP任务方法使用自回归语言模型M将非结构化输入文本x映射到表示结构化对象（例如元组、列表、树、代码等）的输出文本y，其中所需的输出结构通过受限解码强制执行。在训练期间，这些方法不要求模型感知约束，约束仅隐式存在于训练输出y中。这具有优势，因为它允许动态约束而无需重新训练，但在测试时受限解码期间可能导致低质量输出。我们通过Boosted Constrained Decoding (BoostCD) 克服了这个问题，它分两个阶段结合了受限和非受限解码：第一阶段从基础模型M进行两次解码，分别在受限和非受限模式下，获得两个弱预测。在第二阶段，一个学习到的自回归增强模型将这两个弱预测组合成一个最终预测。基础模型在有约束和无约束情况下犯的错误往往是互补的，增强模型学习利用这一点来提高性能。我们通过将其应用于封闭信息抽取来展示BoostCD的强大功能。我们的模型BoostIE在分布内和分布外都优于现有方法，解决了这些方法中发现的几个常见错误。", "summary": "本文提出了一种名为Boosted Constrained Decoding (BoostCD) 的新型两阶段方法，旨在提高自回归语言模型生成的结构化NLP输出质量。现有方法虽然通过受限解码强制输出结构，但由于模型在训练时缺乏对约束的感知，测试时常出现低质量问题。BoostCD通过利用一个学习到的自回归增强模型，结合基础模型在受限和非受限解码模式下的弱预测来解决此问题，从而利用两种解码模式中错误的互补性。将该方法应用于封闭信息抽取任务（BoostIE），结果表明其在不同数据分布下均优于现有方法，并有效缓解了常见的错误。", "keywords": "受限解码, 非受限解码, 增强学习, 信息抽取, 自回归模型", "comments": "该论文的创新之处在于其两阶段解码策略，明确结合了受限和非受限预测，并利用增强模型学习其互补错误。这解决了现有方法中动态约束导致质量问题的关键限制。其在信息抽取领域的应用以及展示出的分布外性能突显了其实用重要性。"}}
{"id": "2506.14785", "title": "Moment-enhanced shallow water equations for non-slip boundary conditions", "authors": ["Shiping Zhou", "Juntao Huang", "Andrew J. Christlieb"], "summary": "The shallow water equations often assume a constant velocity profile along\nthe vertical axis. However, this assumption does not hold in many practical\napplications. To better approximate the vertical velocity distribution, models\nsuch as the shallow water moment expansion models have been proposed.\nNevertheless, under non-slip bottom boundary conditions, both the standard\nshallow water equation and its moment-enhanced models struggle to accurately\ncapture the vertical velocity profile due to the stiff source terms. In this\nwork, we propose modified shallow water equations and corresponding\nmoment-enhanced models that perform well under both non-slip and slip boundary\nconditions. The primary difference between the modified and original models\nlies in the treatment of the source term, which allows our modified moment\nexpansion models to be readily generalized, while maintaining compatibility\nwith our previous analysis on the hyperbolicity of the model. To assess the\nperformance of both the standard and modified moment expansion models, we\nconduct a comprehensive numerical comparison with the incompressible\nNavier--Stokes equations -- a comparison that is absent from existing\nliterature.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14785v1", "AI": {"title_translation": "非滑移边界条件下的动量增强浅水方程", "tldr": "本文提出了改进的浅水方程和动量增强模型，以克服现有模型在非滑移底部边界条件下垂直速度剖面捕捉不准确的问题，并通过与不可压缩Navier-Stokes方程的数值比较验证了其性能。", "motivation": "浅水方程通常假设垂直方向上的速度剖面恒定，但在许多实际应用中此假设不成立。现有的浅水方程及其动量增强模型在非滑移底部边界条件下，由于刚性源项，难以准确捕捉垂直速度剖面。", "method": "本文提出了修改后的浅水方程及相应的动量增强模型。修改后的模型与原始模型的主要区别在于源项的处理方式，这使得修改后的动量展开模型易于推广，同时保持了与之前模型双曲性分析的兼容性。研究通过与不可压缩Navier-Stokes方程进行全面的数值比较来评估标准和修改后的动量展开模型的性能。", "result": "修改后的浅水方程和动量增强模型在非滑移和滑移边界条件下均表现良好。与不可压缩Navier-Stokes方程的全面数值比较，填补了现有文献中的空白。", "conclusion": "本文提出的修改后的浅水方程和动量增强模型，通过对源项的特殊处理，有效解决了现有模型在非滑移边界条件下捕捉垂直速度剖面不准确的问题，并在数值上得到了验证。", "translation": "浅水方程通常假设沿垂直轴的速度剖面恒定。然而，在许多实际应用中，这一假设并不成立。为了更好地近似垂直速度分布，已经提出了诸如浅水动量展开模型等模型。然而，在非滑移底部边界条件下，无论是标准浅水方程还是其动量增强模型，都由于刚性源项而难以准确捕捉垂直速度剖面。在这项工作中，我们提出了修改后的浅水方程和相应的动量增强模型，它们在非滑移和滑移边界条件下均表现良好。修改后的模型与原始模型的主要区别在于源项的处理，这使得我们的修改后的动量展开模型易于推广，同时保持了与我们之前对模型双曲性分析的兼容性。为了评估标准和修改后的动量展开模型的性能，我们与不可压缩Navier-Stokes方程进行了全面的数值比较——这是现有文献中缺失的比较。", "summary": "本文针对现有浅水方程及其动量增强模型在非滑移底部边界条件下垂直速度剖面捕捉不准确的问题，提出了改进的浅水方程和动量增强模型。通过对源项的特殊处理，新模型在非滑移和滑移条件下均表现出色，且易于推广并保持了模型双曲性。研究还首次与不可压缩Navier-Stokes方程进行了全面的数值比较，验证了新模型的有效性。", "keywords": "浅水方程, 动量增强模型, 非滑移边界条件, 源项处理, 数值比较", "comments": "该论文的创新点在于提出了针对非滑移边界条件优化的浅水方程和动量增强模型，通过修改源项解决了现有模型在刚性源项下垂直速度剖面捕捉不准确的难题。其重要性体现在填补了现有文献中与不可压缩Navier-Stokes方程进行全面数值比较的空白，为浅水模型在复杂边界条件下的应用提供了更准确的工具。"}}
{"id": "2506.14836", "title": "Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse", "authors": ["Mark M. Bailey", "Mark I. Heiligman"], "summary": "How can we detect when global events fundamentally reshape public discourse?\nThis study introduces a topological framework for identifying structural change\nin media narratives using persistent homology. Drawing on international news\narticles surrounding major events - including the Russian invasion of Ukraine\n(Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol\ninsurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we\nconstruct daily co-occurrence graphs of noun phrases to trace evolving\ndiscourse. Each graph is embedded and transformed into a persistence diagram\nvia a Vietoris-Rips filtration. We then compute Wasserstein distances and\npersistence entropies across homological dimensions to capture semantic\ndisruption and narrative volatility over time. Our results show that major\ngeopolitical and social events align with sharp spikes in both H0 (connected\ncomponents) and H1 (loops), indicating sudden reorganization in narrative\nstructure and coherence. Cross-correlation analyses reveal a typical lag\npattern in which changes to component-level structure (H0) precede higher-order\nmotif shifts (H1), suggesting a bottom-up cascade of semantic change. An\nexception occurs during the Russian invasion of Ukraine, where H1 entropy leads\nH0, possibly reflecting top-down narrative framing before local discourse\nadjusts. Persistence entropy further distinguishes tightly focused from diffuse\nnarrative regimes. These findings demonstrate that persistent homology offers a\nmathematically principled, unsupervised method for detecting inflection points\nand directional shifts in public attention - without requiring prior knowledge\nof specific events. This topological approach advances computational social\nscience by enabling real-time detection of semantic restructuring during\ncrises, protests, and information shocks.", "comment": "23 pages", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.14836v1", "AI": {"title_translation": "检测通过持久结构实现的叙事转变：对媒体话语的拓扑分析", "tldr": "本研究提出了一种基于持久同调的拓扑框架，用于识别媒体叙事中的结构性变化，并发现重大事件与叙事结构和连贯性的突然重组相关。", "motivation": "如何检测全球事件何时根本性地重塑了公共话语？", "method": "本研究引入了一个拓扑框架，利用持久同调识别媒体叙事中的结构变化。通过分析重大事件（包括俄罗斯入侵乌克兰、乔治·弗洛伊德谋杀案、美国国会大厦骚乱、哈马斯入侵以色列）的国际新闻文章，构建名词短语的每日共现图，并将其嵌入并通过Vietoris-Rips过滤转换为持久图。然后，计算不同同调维度上的Wasserstein距离和持久熵，以捕捉语义中断和叙事波动。", "result": "研究结果显示，重大的地缘政治和社会事件与H0（连通分量）和H1（循环）的急剧峰值对齐，表明叙事结构和连贯性突然重组。交叉相关分析揭示了H0变化通常先于H1变化的自下而上语义变化级联模式。然而，俄罗斯入侵乌克兰期间H1熵领先H0是一个例外，可能反映了自上而下的叙事框架。持久熵还能区分紧密聚焦和扩散的叙事模式。", "conclusion": "持久同调提供了一种数学上严谨、无监督的方法，用于检测公共关注的拐点和方向性转变，而无需事先了解特定事件。这种拓扑方法通过实现危机、抗议和信息冲击期间语义重组的实时检测，推动了计算社会科学的发展。", "translation": "如何检测全球事件何时根本性地重塑了公共话语？本研究引入了一个拓扑框架，利用持久同调识别媒体叙事中的结构变化。我们以围绕重大事件——包括俄罗斯入侵乌克兰（2022年2月）、乔治·弗洛伊德谋杀案（2020年5月）、美国国会大厦骚乱（2021年1月）和哈马斯领导的入侵以色列（2023年10月）——的国际新闻文章为基础，构建名词短语的每日共现图，以追踪不断演变的话语。每个图都被嵌入并通过Vietoris-Rips过滤转换为持久图。然后，我们计算不同同调维度上的Wasserstein距离和持久熵，以捕捉语义中断和随时间变化的叙事波动。我们的结果表明，重大的地缘政治和社会事件与H0（连通分量）和H1（循环）的急剧峰值对齐，表明叙事结构和连贯性的突然重组。交叉相关分析揭示了一种典型的滞后模式，其中组件级结构（H0）的变化先于高阶主题模式（H1）的转变，这表明语义变化的自下而上级联。俄罗斯入侵乌克兰期间出现了一个例外，H1熵领先H0，这可能反映了在局部话语调整之前自上而下的叙事框架。持久熵进一步区分了紧密聚焦和扩散的叙事模式。这些发现表明，持久同调提供了一种数学上严谨、无监督的方法，用于检测公共关注的拐点和方向性转变——而无需事先了解特定事件。这种拓扑方法通过实现危机、抗议和信息冲击期间语义重组的实时检测，推动了计算社会科学的发展。", "summary": "本研究提出一种基于持久同调的拓扑框架，用于检测媒体叙事中的结构性转变。通过分析重大全球事件（如俄乌战争、乔治·弗洛伊德谋杀案等）相关的国际新闻文章，构建名词短语共现图并进行拓扑数据分析。结果显示，重大事件与叙事结构（H0和H1）的突然重组相关，并揭示了语义变化的自下而上级联模式，但俄乌战争期间存在H1领先H0的例外。该方法提供了一种无需先验知识即可检测公共关注拐点和方向性转变的无监督途径，对计算社会科学具有重要意义。", "keywords": "持久同调, 叙事转变, 媒体话语, 拓扑数据分析, 语义重组", "comments": "本文的创新之处在于将持久同调这一数学工具应用于媒体话语分析，提供了一种无监督、无需先验知识即可检测叙事转变的数学严谨方法。它不仅能够识别叙事结构的变化点，还能揭示变化的动态模式（如H0和H1的相对滞后），尤其在危机事件的实时语义重组检测方面具有重要应用潜力，推动了计算社会科学的发展。"}}
{"id": "2506.14973", "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition", "authors": ["Jiamin Xie", "Ju Lin", "Yiteng Huang", "Tyler Vuong", "Zhaojiang Lin", "Zhaojun Yang", "Peng Su", "Prashant Rawat", "Sangeeta Srivastava", "Ming Sun", "Florian Metze"], "summary": "Recent studies have demonstrated that prompting large language models (LLM)\nwith audio encodings enables effective speech recognition capabilities.\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\naudio with spatial cues remains a relatively uninvestigated area of research.\nIn this work, we present directional-SpeechLlama, a novel approach that\nleverages the microphone array of smart glasses to achieve directional speech\nrecognition, source localization, and bystander cross-talk suppression. To\nenhance the model's ability to understand directivity, we propose two key\ntechniques: serialized directional output training (S-DOT) and contrastive\ndirection data augmentation (CDDA). Experimental results show that our proposed\ndirectional-SpeechLlama effectively captures the relationship between textual\ncues and spatial audio, yielding strong performance in both speech recognition\nand source localization tasks.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.14973v1", "AI": {"title_translation": "定向思维：用于多说话者定向语音识别的语音大语言模型", "tldr": "本文提出了 directional-SpeechLlama，一种利用智能眼镜麦克风阵列实现定向语音识别、声源定位和旁观者串扰抑制的语音大语言模型。", "motivation": "现有研究表明提示大语言模型（LLM）与音频编码相结合可以实现有效的语音识别能力，然而，语音大语言模型理解和处理带有空间线索的多通道音频的能力仍是一个相对未被探索的研究领域。", "method": "本文提出了 directional-SpeechLlama，这是一种利用智能眼镜的麦克风阵列来实现定向语音识别、声源定位和旁观者串扰抑制的新方法。为了增强模型理解方向性的能力，提出了两种关键技术：序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）。", "result": "实验结果表明，所提出的 directional-SpeechLlama 有效地捕获了文本线索和空间音频之间的关系，在语音识别和声源定位任务中都取得了强大的性能。", "conclusion": "directional-SpeechLlama 模型能有效捕获文本线索与空间音频的关系，并在语音识别和声源定位任务中表现出色。", "translation": "最近的研究表明，通过音频编码提示大语言模型（LLM）可以实现有效的语音识别能力。然而，语音大语言模型理解和处理带有空间线索的多通道音频的能力仍然是一个相对未被探索的研究领域。在这项工作中，我们提出了 directional-SpeechLlama，这是一种利用智能眼镜的麦克风阵列来实现定向语音识别、声源定位和旁观者串扰抑制的新方法。为了增强模型理解方向性的能力，我们提出了两种关键技术：序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）。实验结果表明，我们提出的 directional-SpeechLlama 有效地捕获了文本线索和空间音频之间的关系，在语音识别和声源定位任务中都取得了强大的性能。", "summary": "本文提出了一种名为 directional-SpeechLlama 的新型语音大语言模型，旨在解决多通道音频中空间线索处理能力不足的问题。该模型利用智能眼镜的麦克风阵列，实现了定向语音识别、声源定位和旁观者串扰抑制。为提升模型对方向性的理解，引入了序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）两种技术。实验证明，directional-SpeechLlama 有效地建立了文本与空间音频的关联，并在语音识别和声源定位任务中表现优异。", "keywords": "定向语音识别, 语音大语言模型, 智能眼镜, 声源定位, 串扰抑制", "comments": "本文创新性地将大语言模型应用于多通道定向语音识别，并利用智能眼镜的麦克风阵列作为数据输入源，这为实际应用提供了新的可能性。S-DOT和CDDA这两种训练技术对于提升模型对空间方向性的理解至关重要，是本文的核心贡献。该研究不仅解决了传统语音识别在多说话者场景下的挑战，还扩展了语音LLM在空间音频处理方面的能力。"}}
{"id": "2506.15034", "title": "MECHA: Multithreaded and Efficient Cryptographic Hardware Access", "authors": ["Pratama Derry", "Laksmono Agus Mahardika Ari", "Iqbal Muhammad", "Howon Kim"], "summary": "This paper presents a multithread and efficient cryptographic hardware access\n(MECHA) for efficient and fast cryptographic operations that eliminates the\nneed for context switching. Utilizing a UNIX domain socket, MECHA manages\nmultiple requests from multiple applications simultaneously, resulting in\nfaster processing and improved efficiency. We comprise several key components,\nincluding the Server thread, Client thread, Transceiver thread, and a pair of\nSender and Receiver queues. MECHA design is portable and can be used with any\ncommunication protocol, with experimental results demonstrating a 83% increase\nin the speed of concurrent cryptographic requests compared to conventional\ninterface design. MECHA architecture has significant potential in the field of\nsecure communication applications ranging from cloud computing to the IoT,\noffering a faster and more efficient solution for managing multiple\ncryptographic operation requests concurrently.", "comment": "4 Page", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15034v1", "AI": {"title_translation": "MECHA: 多线程高效加密硬件访问", "tldr": "MECHA通过多线程和无上下文切换的方式，显著提高了并发加密操作的速度。", "motivation": "现有的加密操作需要上下文切换，导致效率低下且速度慢，因此需要一个更高效、更快的加密硬件访问方案。", "method": "本文提出了MECHA，它利用UNIX域套接字，通过Server线程、Client线程、Transceiver线程以及一对Sender和Receiver队列来同时管理来自多个应用程序的多个请求，从而消除了上下文切换的需要。MECHA的设计具有可移植性，可与任何通信协议配合使用。", "result": "实验结果表明，与传统接口设计相比，MECHA将并发加密请求的速度提高了83%。", "conclusion": "MECHA架构在从云计算到物联网的安全通信应用领域具有巨大潜力，为同时管理多个加密操作请求提供了更快、更高效的解决方案。", "translation": "本文提出了一种多线程高效加密硬件访问（MECHA）方案，用于高效快速的加密操作，消除了上下文切换的需要。MECHA利用UNIX域套接字，能够同时管理来自多个应用程序的多个请求，从而实现更快的处理速度和更高的效率。MECHA由几个关键组件组成，包括服务器线程、客户端线程、收发器线程以及一对发送器和接收器队列。MECHA设计具有可移植性，可与任何通信协议配合使用，实验结果表明，与传统接口设计相比，并发加密请求的速度提高了83%。MECHA架构在从云计算到物联网的安全通信应用领域具有巨大潜力，为同时管理多个加密操作请求提供了更快、更高效的解决方案。", "summary": "MECHA是一种多线程高效的加密硬件访问方案，通过利用UNIX域套接字和特定的线程架构（Server、Client、Transceiver线程以及Sender/Receiver队列），消除了上下文切换的需要，从而能够同时处理来自多个应用程序的并发加密请求。实验证明，MECHA将并发加密请求的速度提高了83%，在云计算和物联网等安全通信应用中展现出显著潜力。", "keywords": "加密硬件访问, 多线程, 上下文切换, UNIX域套接字, 并发加密", "comments": "MECHA的创新之处在于通过多线程和消除上下文切换来优化加密硬件访问，显著提升了并发处理能力。其利用UNIX域套接字和模块化的线程设计使其具有良好的可移植性，对于需要高性能安全通信的应用（如云和IoT）具有重要意义。"}}
{"id": "2506.14865", "title": "Efficient and Real-Time Motion Planning for Robotics Using Projection-Based Optimization", "authors": ["Xuemin Chi", "Hakan Girgin", "Tobias Löw", "Yangyang Xie", "Teng Xue", "Jihao Huang", "Cheng Hu", "Zhitao Liu", "Sylvain Calinon"], "summary": "Generating motions for robots interacting with objects of various shapes is a\ncomplex challenge, further complicated by the robot geometry and multiple\ndesired behaviors. While current robot programming tools (such as inverse\nkinematics, collision avoidance, and manipulation planning) often treat these\nproblems as constrained optimization, many existing solvers focus on specific\nproblem domains or do not exploit geometric constraints effectively. We propose\nan efficient first-order method, Augmented Lagrangian Spectral Projected\nGradient Descent (ALSPG), which leverages geometric projections via Euclidean\nprojections, Minkowski sums, and basis functions. We show that by using\ngeometric constraints rather than full constraints and gradients, ALSPG\nsignificantly improves real-time performance. Compared to second-order methods\nlike iLQR, ALSPG remains competitive in the unconstrained case. We validate our\nmethod through toy examples and extensive simulations, and demonstrate its\neffectiveness on a 7-axis Franka robot, a 6-axis P-Rob robot and a 1:10 scale\ncar in real-world experiments. Source codes, experimental data and videos are\navailable on the project webpage: https://sites.google.com/view/alspg-oc", "comment": "submitted to IROS 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14865v1", "AI": {"title_translation": "基于投影优化的机器人高效实时运动规划", "tldr": "提出了一种名为ALSPG的高效一阶方法，利用几何投影实现机器人实时运动规划，显著提升了实时性能。", "motivation": "机器人与不同形状物体交互时的运动生成是一个复杂挑战，现有求解器通常将这些问题视为约束优化，但许多现有求解器专注于特定问题领域或未能有效利用几何约束。", "method": "提出了一种高效的一阶方法——增广拉格朗日谱投影梯度下降（ALSPG），该方法通过欧几里得投影、闵可夫斯基和以及基函数来利用几何投影。", "result": "通过使用几何约束而非完整约束和梯度，ALSPG显著提高了实时性能。与iLQR等二阶方法相比，ALSPG在无约束情况下仍具竞争力。该方法通过玩具示例、大量仿真和真实机器人（7轴Franka、6轴P-Rob和1:10比例汽车）实验得到验证。", "conclusion": "ALSPG是一种通过有效利用几何约束实现机器人高效实时运动规划的优化方法。", "translation": "生成机器人与各种形状物体交互的运动是一个复杂的挑战，机器人几何形状和多种期望行为使其进一步复杂化。虽然当前的机器人编程工具（如逆运动学、避碰和操作规划）通常将这些问题视为约束优化，但许多现有求解器专注于特定的问题领域或未能有效利用几何约束。我们提出了一种高效的一阶方法，即增广拉格朗日谱投影梯度下降（ALSPG），该方法通过欧几里得投影、闵可夫斯基和以及基函数来利用几何投影。我们表明，通过使用几何约束而非完整约束和梯度，ALSPG显著提高了实时性能。与iLQR等二阶方法相比，ALSPG在无约束情况下仍具竞争力。我们通过玩具示例和大量仿真验证了我们的方法，并在7轴Franka机器人、6轴P-Rob机器人和1:10比例汽车上进行了真实世界实验，展示了其有效性。源代码、实验数据和视频可在项目网页上获取：https://sites.google.com/view/alspg-oc", "summary": "本研究提出了一种名为增广拉格朗日谱投影梯度下降（ALSPG）的高效一阶优化方法，旨在解决机器人与复杂物体交互时的运动规划挑战。ALSPG通过巧妙利用几何投影（包括欧几里得投影、闵可夫斯基和以及基函数）作为约束，显著提升了运动规划的实时性能。实验结果表明，与现有方法相比，ALSPG在实时性方面表现优异，并且在无约束情况下与二阶方法iLQR具有竞争力。该方法已在多种机器人平台（如7轴Franka、6轴P-Rob和1:10比例汽车）上通过真实世界实验得到验证。", "keywords": "机器人运动规划, 实时性能, 优化, 几何约束, ALSPG", "comments": "本文的创新点在于提出了一种高效的一阶优化方法ALSPG，通过巧妙利用几何投影而非完整的约束和梯度，显著提升了机器人运动规划的实时性能。这对于需要快速响应的机器人应用（如人机协作、动态环境操作）具有重要意义，克服了传统方法在处理复杂几何约束时的局限性。"}}
{"id": "2506.15672", "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence", "authors": ["Yao Zhang", "Chenyang Lin", "Shijie Tang", "Haokun Chen", "Shijie Zhou", "Yunpu Ma", "Volker Tresp"], "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.", "comment": "41 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15672v1", "AI": {"title_translation": "SwarmAgentic：迈向基于群体智能的完全自动化智能体系统生成", "tldr": "SwarmAgentic是一个受粒子群优化启发的框架，旨在实现完全自动化的智能体系统生成，并通过语言驱动的探索联合优化智能体功能和协作，在多项实际任务中表现优于现有基线。", "motivation": "现有智能体系统生成框架缺乏完全的自主性，无法从零开始生成智能体、自我优化智能体功能以及进行协作，这限制了它们的适应性和可扩展性。", "method": "提出SwarmAgentic框架，通过语言驱动的探索，从零开始构建智能体系统，并联合优化智能体功能和协作。该框架维护候选系统群体，并通过反馈引导的更新来演化它们，灵感来源于粒子群优化（PSO），以实现系统级结构的有效搜索。", "result": "SwarmAgentic在六个真实世界的、开放式和探索性任务（涉及高级规划、系统级协调和创造性推理）上进行了评估，结果表明它优于所有基线，在TravelPlanner基准测试中，相对于ADAS实现了+261.8%的相对改进。", "conclusion": "SwarmAgentic框架代表了可扩展和自主智能体系统设计的重要一步，它将群体智能与完全自动化的多智能体系统生成相结合，证明了在结构无约束任务中实现完全自动化的有效性。", "translation": "大型语言模型的快速发展推动了决策、协调和任务执行中的智能体系统。然而，现有的智能体系统生成框架缺乏完全的自主性，缺少从零开始的智能体生成、自我优化的智能体功能和协作，这限制了适应性和可扩展性。我们提出了SwarmAgentic，一个用于完全自动化智能体系统生成的框架，它从零开始构建智能体系统，并通过语言驱动的探索，将智能体功能和协作作为相互依赖的组件进行联合优化。为了实现系统级结构的有效搜索，SwarmAgentic维护一个候选系统群体，并通过反馈引导的更新来演化它们，灵感来源于粒子群优化（PSO）。我们在六个真实世界的、开放式和探索性任务上评估了我们的方法，这些任务涉及高级规划、系统级协调和创造性推理。在仅给定任务描述和目标函数的情况下，SwarmAgentic优于所有基线，在TravelPlanner基准测试中，相对于ADAS实现了+261.8%的相对改进，突出了完全自动化在结构无约束任务中的有效性。该框架标志着可扩展和自主智能体系统设计迈出了重要一步，将群体智能与完全自动化的多智能体生成相结合。我们的代码已在https://yaoz720.github.io/SwarmAgentic/公开。", "summary": "SwarmAgentic是一个创新的框架，旨在克服现有智能体系统生成框架在自主性、自我优化和协作方面的不足。它借鉴粒子群优化思想，通过语言驱动的探索，实现从零开始构建和联合优化智能体功能与协作的完全自动化智能体系统生成。该方法在多个复杂、开放式任务中表现出色，显著优于现有基线，证明了其在实现可扩展和自主智能体系统设计方面的巨大潜力。", "keywords": "智能体系统, 群体智能, 完全自动化, 大语言模型, 粒子群优化", "comments": "该论文创新性地将群体智能（特别是粒子群优化）与大型语言模型驱动的智能体系统生成相结合，解决了现有框架在完全自动化和系统级优化方面的局限。其亮点在于能够从零开始生成智能体系统，并同时优化智能体功能和协作，这对于构建更具适应性和可扩展性的智能体系统至关重要。在实际任务中的显著性能提升，特别是面对结构无约束任务时的表现，突显了其重要性和实用性。"}}
{"id": "2506.15088", "title": "Program Feature-based Fuzzing Benchmarking", "authors": ["Miao Miao"], "summary": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15088v1", "AI": {"title_translation": "基于程序特征的模糊测试基准", "tldr": "引入了一个新的基准，通过生成具有可配置细粒度程序特征的程序来评估模糊测试器，发现模糊测试器性能受程序特征影响显著。", "motivation": "传统的模糊测试评估通常侧重于模糊测试器在目标程序集上的整体性能，但很少有基准考虑细粒度程序特征如何影响模糊测试有效性。", "method": "回顾了25项近期灰盒模糊测试研究，提取了7个与控制流和数据流相关的程序特征；使用这些特征生成了由10个细粒度可配置参数控制的153个程序组成的基准；使用该基准评估了11个流行的模糊测试器。", "result": "结果表明，模糊测试器性能根据程序特征及其强度而显著变化。", "conclusion": "强调了将程序特征纳入模糊测试评估的重要性。", "translation": "模糊测试是一种强大的软件测试技术，以其在识别软件漏洞方面的有效性而闻名。传统的模糊测试评估通常侧重于模糊测试器在一组目标程序上的整体性能，但很少有基准考虑细粒度程序特征如何影响模糊测试有效性。为了弥补这一空白，我们引入了一个新颖的基准，旨在生成具有可配置的细粒度程序特征的程序，以增强模糊测试评估。我们回顾了25项近期灰盒模糊测试研究，提取了7个与控制流和数据流相关的程序特征，这些特征可能影响模糊测试器性能。利用这些特征，我们生成了一个由153个程序组成的基准，这些程序由10个细粒度可配置参数控制。我们使用此基准评估了11个流行的模糊测试器。结果表明，模糊测试器性能根据程序特征及其强度而显著变化，突出了将程序特征纳入模糊测试评估的重要性。", "summary": "本文提出了一个基于程序特征的模糊测试基准，旨在解决传统评估中忽视细粒度程序特征对模糊测试有效性影响的问题。通过分析25项灰盒模糊测试研究，提取了7个关键程序特征，并据此生成了一个包含153个程序的基准。对11个流行模糊测试器的评估结果表明，模糊测试性能与程序特征及其强度密切相关，强调了在评估中考虑程序特征的重要性。", "keywords": "模糊测试, 基准测试, 程序特征, 软件测试, 漏洞识别", "comments": "这项研究通过引入一个考虑细粒度程序特征的新型模糊测试基准，填补了传统模糊测试评估的空白。其创新之处在于能够生成具有可配置特征的程序，这使得对模糊测试器性能的评估更加精细和深入。研究结果强调了程序特性对模糊测试效果的关键影响，为未来模糊测试工具的开发和评估提供了重要指导。"}}
{"id": "2506.14799", "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "summary": "Recent advances in AI has enabled automated analysis of complex media content\nat scale and generate actionable insights regarding character representation\nalong such dimensions as gender and age. Past work focused on quantifying\nrepresentation from audio/video/text using various ML models, but without\nhaving the audience in the loop. We ask, even if character distribution along\ndemographic dimensions are available, how useful are they to the general\npublic? Do they actually trust the numbers generated by AI models? Our work\naddresses these questions through a user study, while proposing a new AI-based\ncharacter representation and visualization tool. Our tool based on the\nContrastive Language Image Pretraining (CLIP) foundation model to analyze\nvisual screen data to quantify character representation across dimensions of\nage and gender. We also designed effective visualizations suitable for\npresenting such analytics to lay audience. Next, we conducted a user study to\nseek empirical evidence on the usefulness and trustworthiness of the\nAI-generated results for carefully chosen movies presented in the form of our\nvisualizations. We note that participants were able to understand the analytics\nfrom our visualization, and deemed the tool `overall useful'. Participants also\nindicated a need for more detailed visualizations to include more demographic\ncategories and contextual information of the characters. Participants' trust in\nAI-based gender and age models is seen to be moderate to low, although they\nwere not against the use of AI in this context. Our tool including code,\nbenchmarking, and data from the user study can be found here:\nhttps://anonymous.4open.science/r/Character-Representation-Media-FF7B", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14799v1", "AI": {"title_translation": "使用多模态基础模型分析媒体内容中的角色表征：有效性与信任", "tldr": "本研究通过用户研究评估了使用多模态基础模型（CLIP）进行媒体内容角色（性别和年龄）表征分析工具的有效性和用户信任度，发现工具被认为有用，但用户对AI生成结果的信任度中等到低。", "motivation": "过去的媒体内容角色表征量化工作缺乏用户参与，因此研究者提出问题：即使有了角色的人口统计分布数据，这些数据对公众有多大用处？公众是否真正信任AI模型生成的数据？本研究旨在解决这些问题。", "method": "本研究提出了一种新的基于AI的角色表征和可视化工具，该工具基于CLIP基础模型分析视觉屏幕数据以量化角色在年龄和性别维度上的表征。研究者还设计了适合向普通受众展示分析结果的可视化界面。随后，通过一项用户研究，验证了AI生成结果（以可视化形式呈现）的有用性和可信度。", "result": "参与者能够理解可视化分析结果，并认为该工具“总体有用”。参与者还指出需要更详细的可视化，以包含更多人口统计类别和角色背景信息。参与者对基于AI的性别和年龄模型的信任度为中等到低，但他们不反对在此上下文中使用AI。", "conclusion": "研究表明，基于多模态基础模型（CLIP）的媒体内容角色表征分析工具及其可视化界面对用户而言是可理解和有用的，尽管用户对AI生成结果的信任度有待提高，且对更详细的人口统计信息有需求。", "translation": "AI的最新进展使得大规模自动化分析复杂媒体内容并生成关于角色表征（如性别和年龄维度）的可操作洞察成为可能。过去的工作主要关注使用各种机器学习模型从音频/视频/文本中量化表征，但没有让受众参与其中。我们提出疑问，即使获得了角色在人口统计维度上的分布，它们对普通大众有多大用处？他们是否真正信任AI模型生成的数据？我们的工作通过一项用户研究来解决这些问题，同时提出了一种新的基于AI的角色表征和可视化工具。我们的工具基于对比语言图像预训练（CLIP）基础模型，用于分析视觉屏幕数据，以量化角色在年龄和性别维度上的表征。我们还设计了有效的可视化界面，适合向普通受众展示此类分析结果。接下来，我们进行了一项用户研究，旨在为精心选择的电影（以我们可视化形式呈现）的AI生成结果的有用性和可信度寻求实证证据。我们注意到，参与者能够理解我们可视化工具的分析结果，并认为该工具“总体有用”。参与者还指出需要更详细的可视化，以包含更多人口统计类别和角色的背景信息。参与者对基于AI的性别和年龄模型的信任度为中等到低，尽管他们不反对在此背景下使用AI。我们的工具，包括代码、基准测试和用户研究数据，可在此处找到：https://anonymous.4open.science/r/Character-Representation-Media-FF7B", "summary": "本研究提出并评估了一个基于CLIP多模态基础模型的AI工具，用于分析媒体内容中角色的年龄和性别表征，并以可视化方式呈现。通过用户研究发现，该工具被用户认为总体有用且易于理解，但用户对AI生成结果的信任度中等到低，并希望获得更详细的人口统计和背景信息。", "keywords": "角色表征, 多模态基础模型, 用户研究, 有效性, 信任", "comments": "该论文的创新点在于将多模态基础模型（CLIP）应用于媒体内容的角色表征分析，并首次通过用户研究深入探讨了普通受众对AI生成分析结果的有用性和信任度。其重要性在于弥补了以往研究中缺乏用户反馈的空白，揭示了用户对AI在社会公平性分析中接受度和信任度的问题。局限性在于，虽然工具被认为有用，但用户对AI结果的信任度不高，且对更细致、更全面的角色信息有进一步需求，这为未来研究指明了方向。"}}
{"id": "2506.14937", "title": "Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders", "authors": ["Luan Gonçalves Miranda", "Pedro Ivo da Cruz", "Murilo Bellezoni Loiola"], "summary": "Currently, digital security mechanisms like Anomaly Detection Systems using\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\nseparation threshold to classify the extracted reconstruction error, the\ndefinition of this threshold directly impacts the performance of the detection\nprocess. Thus, this work proposes the automatic definition of this threshold\nusing some machine learning algorithms. For this, three algorithms were\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.", "comment": "This work was accepted at SBrT 2022 (Brazilian Symposium on\n  Telecommunications and Signal Processing), though it was not included in the\n  official proceedings. in Portuguese language", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14937v1", "AI": {"title_translation": "使用自编码器在计算机网络中自动确定攻击检测阈值", "tldr": "本文提出了一种使用机器学习算法自动确定自编码器在计算机网络攻击检测中分离阈值的方法。", "motivation": "当前，使用自编码器（AE）的异常检测系统在数字安全领域显示出巨大潜力，但其分类重建误差所需的检测阈值是非平凡且非标准化的，其定义直接影响检测性能。因此，需要一种自动化的方法来确定此阈值。", "method": "本研究提出使用机器学习算法自动定义自编码器的攻击检测阈值。为此，评估了三种算法：K-近邻（K-Nearest Neighbors）、K-均值（K-Means）和支持向量机（Support Vector Machine）。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "目前，使用自编码器（AE）的异常检测系统等数字安全机制在规避数据固有的问题（如数据不平衡）方面显示出巨大潜力。由于AE使用非平凡且非标准化的分离阈值来分类提取的重建误差，因此该阈值的定义直接影响检测过程的性能。因此，这项工作提出了使用一些机器学习算法自动定义此阈值的方法。为此，评估了三种算法：K-近邻、K-均值和支持向量机。", "summary": "本文针对自编码器在计算机网络异常检测中，其分类重建误差所依赖的非标准化分离阈值对检测性能的显著影响，提出了一种自动确定该阈值的方法。研究评估了K-近邻、K-均值和支持向量机三种机器学习算法在自动定义此阈值上的应用。", "keywords": "自编码器, 异常检测, 阈值确定, 机器学习, 网络安全", "comments": "该论文旨在解决自编码器在异常检测中面临的一个实际挑战，即如何有效地设置检测阈值。通过引入机器学习算法来自动化这一过程，有望提高检测的性能和鲁棒性，减少人工干预。其创新点在于将机器学习方法应用于自编码器阈值的自动优化，这对于提升基于自编码器的安全系统实用性具有重要意义。"}}
{"id": "2506.15316", "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor", "authors": ["Benoit Tain", "Raphael Millet", "Romain Lemaire", "Michal Szczepanski", "Laurent Alacoque", "Emmanuel Pluchart", "Sylvain Choisnet", "Rohit Prasad", "Jerome Chossat", "Pascal Pierunek", "Pascal Vivet", "Sebastien Thuries"], "summary": "This paper presents J3DAI, a tiny deep neural network-based hardware\naccelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial\nintelligence (AI) chip integrating a Deep Neural Network (DNN)-based\naccelerator. The DNN accelerator is designed to efficiently perform neural\nnetwork tasks such as image classification and segmentation. This paper focuses\non the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)\ncharacteristics and showcasing advanced edge AI capabilities on a CMOS image\nsensor. To support hardware, we utilized the Aidge comprehensive software\nframework, which enables the programming of both the host processor and the DNN\naccelerator. Aidge supports post-training quantization, significantly reducing\nmemory footprint and computational complexity, making it crucial for deploying\nmodels on resource-constrained hardware like J3DAI. Our experimental results\ndemonstrate the versatility and efficiency of this innovative design in the\nfield of edge AI, showcasing its potential to handle both simple and\ncomputationally intensive tasks. Future work will focus on further optimizing\nthe architecture and exploring new applications to fully leverage the\ncapabilities of J3DAI. As edge AI continues to grow in importance, innovations\nlike J3DAI will play a crucial role in enabling real-time, low-latency, and\nenergy-efficient AI processing at the edge.", "comment": "Preprint from ISLPED 2025. 979-8-3315-2710-5/25/$31.00\n  \\c{opyright}2025 IEEE", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15316v1", "AI": {"title_translation": "J3DAI：一种用于3D堆叠CMOS图像传感器的微型基于DNN的边缘AI加速器", "tldr": "J3DAI是一种用于3D堆叠CMOS图像传感器的微型DNN边缘AI加速器，通过Aidge软件框架实现高效的图像分类和分割，展示了在边缘AI领域的性能和效率。", "motivation": "随着边缘AI重要性的日益增长，需要开发能够实现实时、低延迟、节能AI处理的硬件加速器，特别是在资源受限的环境中部署深度学习模型。", "method": "论文提出了J3DAI，一个为3层3D堆叠CMOS图像传感器设计的微型深度神经网络（DNN）硬件加速器。该设计专注于数字系统，并利用Aidge综合软件框架支持主机处理器和DNN加速器的编程，同时通过训练后量化技术显著减少内存占用和计算复杂度，以适应资源受限的硬件部署。", "result": "实验结果表明J3DAI设计在边缘AI领域具有多功能性和高效率，能够有效处理简单和计算密集型任务。", "conclusion": "像J3DAI这样的创新设计将在实现边缘AI实时、低延迟和节能处理方面发挥关键作用。", "translation": "本论文介绍了J3DAI，一个用于3层3D堆叠CMOS图像传感器的微型深度神经网络硬件加速器，其特点是集成了一个基于深度神经网络（DNN）加速器的人工智能（AI）芯片。该DNN加速器旨在高效执行图像分类和分割等神经网络任务。本文重点介绍J3DAI的数字系统，强调其性能-功耗-面积（PPA）特性，并展示了CMOS图像传感器上先进的边缘AI能力。为了支持硬件，我们使用了Aidge综合软件框架，该框架能够对主机处理器和DNN加速器进行编程。Aidge支持训练后量化，显著减少了内存占用和计算复杂度，这对于在J3DAI等资源受限的硬件上部署模型至关重要。我们的实验结果展示了这种创新设计在边缘AI领域的多功能性和效率，展示了其处理简单和计算密集型任务的潜力。未来的工作将侧重于进一步优化架构并探索新的应用，以充分利用J3DAI的能力。随着边缘AI重要性的不断增长，像J3DAI这样的创新将在实现边缘实时、低延迟和节能的AI处理方面发挥关键作用。", "summary": "本文介绍J3DAI，一个专为3D堆叠CMOS图像传感器设计的微型DNN边缘AI加速器。它旨在高效执行图像分类和分割等神经网络任务，并利用Aidge软件框架实现编程和训练后量化，以优化在资源受限硬件上的部署。实验证明J3DAI在边缘AI领域具有多功能性和高效率，能够处理各种计算任务，对推动实时、低延迟、节能的边缘AI处理具有重要意义。", "keywords": "边缘AI, DNN加速器, 3D堆叠CMOS图像传感器, 训练后量化, J3DAI", "comments": "J3DAI的创新之处在于将微型DNN加速器直接集成到3D堆叠CMOS图像传感器中，实现了边缘AI的实时、低延迟和节能处理。其结合Aidge软件框架进行训练后量化，有效解决了资源受限硬件的部署挑战，对于推动边缘AI技术在实际应用中的发展具有重要意义。"}}
{"id": "2506.15073", "title": "Linear and Numerical SDoF Bounds of Active RIS-Assisted MIMO Wiretap Interference Channel", "authors": ["Su Linfan", "Miao Yuhang", "Song Yuxuan", "Zheng Shuo", "Zhang Tong", "Xu Yinfei", "Wang Shuai", "Li Na"], "summary": "The multiple-input multiple-output (MIMO) wiretap interference channel (IC)\nserves as a canonical model for information-theoretic security, where a\nmultiple-antenna eavesdropper attempts to intercept communications in a\ntwo-user MIMO IC system. The secure degrees-of-freedom (SDoF) of an active\nreconfigurable intelligent surface (RIS)-assisted MIMO wiretap IC is with\npractical interests but remains unexplored. In this paper, we establish both\nsum-SDoF lower and upper bounds through linear beamforming conditions and\nnumerical methods. Specifically, our proposed lower bound is derived from\ntransmission scheme design and corresponding solutions to the sum-SDoF\nmaximization problem, formulated by linear integer programming. The solutions\nto this optimization problem addresses RIS element allocation for leakage and\ninterference cancellation. The proposed upper bound is obtained by solving a\nnuclear norm minimization problem, leveraging the fact that nuclear norm serves\nas a convex relaxation of the rank function. For symmetry antenna\nconfigurations, we derive a closed-form lower bound. Extensive numerical\nsimulations show that our proposed lower and upper bounds coincide across many\nantenna configurations, and our proposed lower bound outperforms the existing\nbenchmark.", "comment": "11 pages, 2 figures, submitted to IEEE OJCOMS", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15073v1", "AI": {"title_translation": "有源RIS辅助MIMO窃听干扰信道的线性和数值SDoF界限", "tldr": "本文通过线性和数值方法，首次建立了有源可重构智能表面（RIS）辅助的多输入多输出（MIMO）窃听干扰信道的安全自由度（SDoF）和界限，并证明了其在多种天线配置下的有效性和优越性。", "motivation": "有源可重构智能表面（RIS）辅助的多输入多输出（MIMO）窃听干扰信道的安全自由度（SDoF）具有实际应用价值，但目前尚未被探索。", "method": "本文通过线性和数值方法建立了总和SDoF的上下界。下界是通过传输方案设计和线性整数规划公式化的SDoF最大化问题的求解得到的，该方案解决了RIS元件分配以进行泄露和干扰消除。上界是通过求解核范数最小化问题获得的。对于对称天线配置，还推导出了闭合形式的下界。", "result": "广泛的数值模拟表明，所提出的上下界在许多天线配置下是吻合的，并且所提出的下界优于现有基准。", "conclusion": "本文成功建立了有源RIS辅助MIMO窃听干扰信道的SDoF界限，并通过数值仿真验证了其有效性和优越性。", "translation": "多输入多输出（MIMO）窃听干扰信道（IC）是信息论安全性的一个典型模型，其中多天线窃听者试图在双用户MIMO IC系统中截获通信。有源可重构智能表面（RIS）辅助的MIMO窃听IC的安全自由度（SDoF）具有实际意义，但尚未被探索。在本文中，我们通过线性波束成形条件和数值方法建立了总和SDoF的下界和上界。具体来说，我们提出的下界是通过传输方案设计和线性整数规划公式化的总和SDoF最大化问题的相应解决方案推导出来的。该优化问题的解决方案解决了用于泄露和干扰消除的RIS元件分配。所提出的上界是通过求解核范数最小化问题获得的，这利用了核范数作为秩函数的凸松弛这一事实。对于对称天线配置，我们推导出了闭合形式的下界。大量的数值模拟表明，我们提出的下界和上界在许多天线配置下是吻合的，并且我们提出的下界优于现有基准。", "summary": "本文首次研究了有源可重构智能表面（RIS）辅助的多输入多输出（MIMO）窃听干扰信道的安全自由度（SDoF），并建立了其总和SDoF的线性和数值上下界。下界通过传输方案设计和线性整数规划求解，上界通过核范数最小化获得。数值模拟结果表明，所提出的界限在多种天线配置下吻合良好，且下界性能优于现有基准。", "keywords": "SDoF, RIS, MIMO, 窃听信道, 界限", "comments": "该论文解决了有源RIS辅助窃听信道中SDoF这一未被探索的问题，具有重要的理论和实际意义。结合线性整数规划和核范数最小化的方法，为分析此类复杂系统提供了全面的数学工具。其提出的界限在数值模拟中表现出优越性，为未来相关研究奠定了基础。"}}
{"id": "2506.14981", "title": "Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions", "authors": ["Hailiang Zhang", "Dieu My T. Nguyen", "Christine Smit", "Mahabal Hegde"], "summary": "Data analysis on massive multi-dimensional data, such as high-resolution\nlarge-region time averaging or area averaging for geospatial data, often\ninvolves calculations over a significant number of data points. While\nperforming calculations in scalable and flexible distributed or cloud\nenvironments is a viable option, a full scan of large data volumes still serves\nas a computationally intensive bottleneck, leading to significant cost. This\npaper introduces a generic and comprehensive method to address these\ncomputational challenges. This method generates a small, size-tunable\nsupplementary dataset that stores the cumulative sums along specific subset\ndimensions on top of the raw data. This minor addition unlocks rapid and cheap\nhigh-resolution large-region data analysis, making calculations over large\nnumbers of data points feasible with small instances or even microservices in\nthe cloud. This method is general-purpose, but is particularly well-suited for\ndata stored in chunked, cloud-optimized formats and for services running in\ndistributed or cloud environments. We present a Zarr extension proposal to\nintegrate the specifications of this method and facilitate its straightforward\nimplementation in general-purpose software applications. Benchmark tests\ndemonstrate that this method, implemented in Amazon Web services (AWS),\nsignificantly outperforms the brute-force approach used in on-premises\nservices. With just 5% supplemental storage, this method achieves a performance\nthat is 3-4 orders of magnitude (~10,000 times) faster than the brute-force\napproach, while incurring significantly reduced computational costs.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14981v1", "AI": {"title_translation": "基于Zarr的降维块级累加和", "tldr": "该论文提出了一种基于Zarr的方法，通过使用补充累加和数据，将大规模多维数据分析的速度提高了几个数量级，并显著降低了计算成本。", "motivation": "对海量多维数据（例如地理空间数据的高分辨率大区域时间或面积平均）进行数据分析时，通常涉及大量数据点的计算。即使在可扩展和灵活的分布式或云环境中进行计算，对大量数据进行全面扫描仍然是计算密集型瓶颈，导致成本高昂。", "method": "本文提出了一种通用且全面的方法来解决这些计算挑战。该方法在原始数据之上，生成一个小型、大小可调的补充数据集，用于存储沿特定子集维度的累加和。该方法被提出作为Zarr扩展提案。", "result": "基准测试表明，在Amazon Web Services (AWS) 中实现的该方法，与本地服务中使用的暴力方法相比，性能显著提升。仅需5%的额外存储，该方法的速度比暴力方法快3-4个数量级（约10,000倍），同时计算成本显著降低。", "conclusion": "所提出的基于Zarr的方法有效解决了海量多维数据分析中的计算瓶颈，显著提高了性能并降低了成本，特别适用于块状、云优化的数据。", "translation": "对海量多维数据（例如地理空间数据的高分辨率大区域时间或面积平均）进行数据分析时，通常涉及大量数据点的计算。即使在可扩展和灵活的分布式或云环境中进行计算，对大量数据进行全面扫描仍然是计算密集型瓶颈，导致成本高昂。本文提出了一种通用且全面的方法来解决这些计算挑战。该方法在原始数据之上，生成一个小型、大小可调的补充数据集，用于存储沿特定子集维度的累加和。这一微小的补充使得高分辨率大区域数据分析能够快速且廉价地进行，使得在云中使用小型实例甚至微服务进行大量数据点的计算成为可能。该方法是通用的，但特别适用于以分块、云优化格式存储的数据以及在分布式或云环境中运行的服务。我们提出了一个Zarr扩展提案，以整合该方法的规范，并促进其在通用软件应用程序中的直接实现。基准测试表明，在Amazon Web Services (AWS) 中实现的该方法，与本地服务中使用的暴力方法相比，性能显著提升。仅需5%的额外存储，该方法的速度比暴力方法快3-4个数量级（约10,000倍），同时计算成本显著降低。", "summary": "本文针对海量多维数据分析中的计算瓶颈，提出了一种基于Zarr的新颖方法。该方法通过创建一个小型补充数据集，存储降维后的块级累加和，从而显著加速高分辨率大区域数据分析，使其在小型云实例上也能高效运行。基准测试表明，与暴力方法相比，该方法在性能（最高可达10,000倍）和成本效益方面均表现出色，尤其适用于以云优化格式存储的数据。", "keywords": "Zarr, 累加和, 多维数据, 云计算, 性能优化", "comments": "这项创新的核心在于利用一个小型预计算的累加和数据集，极大地减少了大规模数据分析的计算时间和成本，尤其适用于分块的云数据。将其作为Zarr扩展集成，使其具有高度的实用性和普适性。这对于云环境中的高效大数据分析是一个重要的推动。"}}
{"id": "2506.15405", "title": "Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks", "authors": ["Roshan Antony Gomez", "Julien Stöcker", "Barış Cansız", "Michael Kaliske"], "summary": "Physics-informed neural networks (PINNs) are extensively used to represent\nvarious physical systems across multiple scientific domains. The same can be\nsaid for cardiac electrophysiology, wherein fully-connected neural networks\n(FCNNs) have been employed to predict the evolution of an action potential in a\n2D space following the two-parameter phenomenological Aliev-Panfilov (AP)\nmodel. In this paper, the training behaviour of PINNs is investigated to\ndetermine optimal hyperparameters to predict the electrophysiological activity\nof the myocardium in 3D according to the AP model, with the inclusion of\nboundary and material parameters. An FCNN architecture is employed with the\ngoverning partial differential equations in their strong form, which are scaled\nconsistently with normalization of network inputs. The finite element (FE)\nmethod is used to generate training data for the network. Numerical examples\nwith varying spatial dimensions and parameterizations are generated using the\ntrained models. The network predicted fields for both the action potential and\nthe recovery variable are compared with the respective FE simulations. Network\nlosses are weighed with individual scalar values. Their effect on training and\nprediction is studied to arrive at a method of controlling losses during\ntraining.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.15405v1", "AI": {"title_translation": "使用物理信息神经网络模拟三维参数化心脏电生理学", "tldr": "本文研究了物理信息神经网络（PINNs）在三维心脏电生理学模拟中的训练行为，旨在确定最优超参数，并开发一种在训练过程中控制损失的方法。", "motivation": "现有研究使用全连接神经网络（FCNNs）在二维空间预测心脏电生理活动。本文旨在研究物理信息神经网络（PINNs）在三维空间中预测心肌电生理活动的能力，并确定最优超参数，同时纳入边界和材料参数。", "method": "本文采用全连接神经网络（FCNN）架构，结合强形式的偏微分方程，并对网络输入进行归一化。使用有限元（FE）方法生成训练数据。通过调整网络损失的权重，研究其对训练和预测的影响，以找到一种控制损失的方法。", "result": "训练模型生成了具有不同空间维度和参数化的数值示例。网络预测的动作电位和恢复变量场与有限元（FE）模拟结果进行了比较。研究了网络损失权重对训练和预测的影响。", "conclusion": "通过研究网络损失权重对训练和预测的影响，本文提出了一种在训练过程中控制损失的方法，成功地使用物理信息神经网络（PINNs）模拟了三维心脏电生理活动并与有限元（FE）模拟进行了比较。", "translation": "物理信息神经网络（PINNs）被广泛用于表示多个科学领域中的各种物理系统。心脏电生理学也是如此，其中全连接神经网络（FCNNs）已被用于根据双参数现象学Aliev-Panfilov（AP）模型预测二维空间中动作电位的演变。在本文中，研究了PINNs的训练行为，以确定最优超参数，根据AP模型预测三维心肌的电生理活动，并包含了边界和材料参数。采用了FCNN架构，其控制偏微分方程以强形式表示，并与网络输入的归一化保持一致。有限元（FE）方法用于生成网络的训练数据。使用训练好的模型生成了具有不同空间维度和参数化的数值示例。将网络预测的动作电位和恢复变量场与各自的有限元模拟进行了比较。网络损失通过单独的标量值进行加权。研究了它们对训练和预测的影响，以期找到一种在训练过程中控制损失的方法。", "summary": "本文研究了物理信息神经网络（PINNs）在三维心脏电生理学模拟中的应用，特别关注了其训练行为和超参数优化。研究采用全连接神经网络（FCNN）架构，结合Aliev-Panfilov（AP）模型和强形式的偏微分方程，并使用有限元（FE）方法生成训练数据。通过比较网络预测结果与FE模拟，并分析网络损失权重对训练和预测的影响，旨在开发一种有效的损失控制方法，以准确模拟带有边界和材料参数的三维心肌电生理活动。", "keywords": "物理信息神经网络, 心脏电生理学, 三维模拟, Aliev-Panfilov模型, 有限元方法", "comments": "这项研究通过将物理信息神经网络（PINNs）应用于复杂的三维心脏电生理学模拟，扩展了PINNs在物理系统建模中的应用。通过研究超参数优化和损失权重控制，解决了PINNs训练中的实际挑战，提高了模型的准确性和鲁棒性。"}}
{"id": "2506.14816", "title": "A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection", "authors": ["Alavikunhu Panthakkan", "Zubair Medammal", "S M Anzar", "Fatma Taher", "Hussain Al-Ahmad"], "summary": "Falconry, a revered tradition involving the training and hunting with\nfalcons, requires meticulous health surveillance to ensure the health and\nsafety of these prized birds, particularly in hunting scenarios. This paper\npresents an innovative method employing a hybrid of ConvNeXt and EfficientNet\nAI models for the classification of falcon diseases. The study focuses on\naccurately identifying three conditions: Normal, Liver Disease and\n'Aspergillosis'. A substantial dataset was utilized for training and validating\nthe model, with an emphasis on key performance metrics such as accuracy,\nprecision, recall, and F1-score. Extensive testing and analysis have shown that\nour concatenated AI model outperforms traditional diagnostic methods and\nindividual model architectures. The successful implementation of this hybrid AI\nmodel marks a significant step forward in precise falcon disease detection and\npaves the way for future developments in AI-powered avian healthcare solutions.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14816v1", "AI": {"title_translation": "一种用于精确猎鹰疾病检测的混合ConvNeXt-EfficientNet AI解决方案", "tldr": "该研究提出了一种结合ConvNeXt和EfficientNet的混合AI模型，用于精确检测猎鹰的正常、肝病和曲霉病，并显示出优于传统方法和单一模型的性能。", "motivation": "猎鹰术需要对猎鹰进行细致的健康监测，以确保这些珍贵鸟类的健康和安全，特别是在狩猎场景中。因此，需要一种更精确的疾病检测方法。", "method": "本研究采用了一种创新方法，结合了ConvNeXt和EfficientNet AI模型，用于猎鹰疾病的分类，重点识别正常、肝病和曲霉病三种情况。使用了大量数据集进行模型训练和验证，并强调了准确率、精确率、召回率和F1分数等关键性能指标。", "result": "广泛的测试和分析表明，所提出的混合AI模型优于传统的诊断方法和单独的模型架构。", "conclusion": "这种混合AI模型的成功实施标志着在精确猎鹰疾病检测方面迈出了重要一步，并为未来AI驱动的禽类医疗保健解决方案的发展铺平了道路。", "translation": "猎鹰术是一项受人尊敬的传统，涉及训练和使用猎鹰进行狩猎，需要细致的健康监测，以确保这些珍贵鸟类的健康和安全，尤其是在狩猎场景中。本文提出了一种创新方法，采用ConvNeXt和EfficientNet AI模型的混合体来对猎鹰疾病进行分类。该研究侧重于准确识别三种情况：正常、肝病和“曲霉病”。使用了大量数据集进行模型训练和验证，并强调了准确率、精确率、召回率和F1分数等关键性能指标。广泛的测试和分析表明，我们连接的AI模型优于传统的诊断方法和单独的模型架构。这种混合AI模型的成功实施标志着在精确猎鹰疾病检测方面迈出了重要一步，并为未来AI驱动的禽类医疗保健解决方案的发展铺平了道路。", "summary": "该论文提出了一种结合ConvNeXt和EfficientNet的混合AI模型，用于精确检测猎鹰的正常、肝病和曲霉病。通过使用大量数据集进行训练和验证，该模型在准确率、精确率、召回率和F1分数等关键指标上表现出色，并被证明优于传统诊断方法和单一模型架构。这项研究为AI驱动的禽类医疗保健解决方案的未来发展奠定了基础。", "keywords": "猎鹰疾病检测, ConvNeXt, EfficientNet, 混合AI模型, 禽类医疗保健", "comments": "这项研究的创新之处在于结合了两种先进的深度学习架构（ConvNeXt和EfficientNet）来解决特定的生物医学诊断问题，即猎鹰疾病检测。其重要性在于为猎鹰的健康监测提供了一种高效且更精确的自动化解决方案，这对于保护这些珍贵鸟类至关重要。该方法通过在关键性能指标上的优越表现，展示了混合模型在特定领域应用的潜力。"}}
{"id": "2506.14997", "title": "Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings", "authors": ["Harbin Hong", "Sebastian Caldas", "Liu Leqi"], "summary": "As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14997v1", "AI": {"title_translation": "多项选择环境下量化大型语言模型与人类错位程度的假设检验", "tldr": "本研究提出了一个基于假设检验的定量框架，用于评估大型语言模型（LLM）在多项选择调查中模拟人类行为的程度。研究发现，所测试的流行LLM在模拟特定人群对有争议问题的意见时表现不佳，强调了在社会科学中使用LLM时需要更严谨的方法。", "motivation": "随着大型语言模型（LLM）在社会科学研究（如经济学和市场营销）中的应用日益增多，评估这些模型复制人类行为的准确性变得至关重要。", "method": "本研究提出了一个基于假设检验的定量框架，用于评估LLM模拟行为与实际人类行为在多项选择调查环境中的错位程度。该框架能够原则性地判断特定语言模型是否能有效模拟人类的意见、决策和通过多项选择选项表达的一般行为。", "result": "将该框架应用于一个流行的语言模型，发现该模型在模拟不同种族、年龄和收入等受测亚人群对有争议问题的意见时表现不佳。", "conclusion": "研究结果对该语言模型与受测人群的对齐性提出了质疑，并强调了在社会科学研究中使用LLM时，需要超越简单模拟人类受试者的新实践。", "translation": "大型语言模型（LLM）在社会科学研究（例如经济学和市场营销）中日益普及，因此评估这些模型复制人类行为的程度变得至关重要。在这项工作中，我们利用假设检验，提出了一个定量框架，用于评估LLM模拟行为与实际人类行为在多项选择调查环境中的错位程度。该框架使我们能够以一种有原则的方式确定特定的语言模型是否能有效模拟通过多项选择选项表达的人类意见、决策和一般行为。我们将此框架应用于一个流行的语言模型，用于模拟各种公共调查中人们的意见，并发现该模型不适合模拟受测亚人群（例如，不同种族、年龄和收入）对有争议问题的意见。这引发了关于该语言模型与受测人群对齐性的问题，突出了在社会科学研究中使用LLM时，需要超越简单模拟人类受试者的新实践。", "summary": "本研究提出了一种基于假设检验的定量框架，旨在评估大型语言模型（LLM）在多项选择调查中模拟人类行为的准确性，即LLM与人类行为的错位程度。该框架能够系统地判断LLM是否能有效模拟人类的意见和决策。通过将此框架应用于一个流行的LLM，研究发现该模型在模拟特定亚人群（如不同种族、年龄、收入）对有争议问题的意见时表现不佳。这表明在社会科学研究中，使用LLM进行人类行为模拟时需采取更谨慎和深入的方法，而非简单的模拟。", "keywords": "大型语言模型, 人类错位, 假设检验, 多项选择, 社会科学", "comments": "这项工作通过引入一个基于假设检验的定量框架来评估LLM与人类行为的错位，为LLM在社会科学中的应用提供了重要的评估工具。其创新性在于提供了一种原则性的方法来量化LLM模拟的局限性，特别是在处理敏感或有争议问题时。研究结果揭示了当前LLM在模拟特定人口子集方面的不足，对未来LLM在社会科学研究中的应用提出了警示和新的实践方向。"}}
{"id": "2506.15082", "title": "Make Your AUV Adaptive: An Environment-Aware Reinforcement Learning Framework For Underwater Tasks", "authors": ["Yimian Ding", "Jingzehua Xu", "Guanwen Xie", "Shuai Zhang", "Yi Li"], "summary": "This study presents a novel environment-aware reinforcement learning (RL)\nframework designed to augment the operational capabilities of autonomous\nunderwater vehicles (AUVs) in underwater environments. Departing from\ntraditional RL architectures, the proposed framework integrates an\nenvironment-aware network module that dynamically captures flow field data,\neffectively embedding this critical environmental information into the state\nspace. This integration facilitates real-time environmental adaptation,\nsignificantly enhancing the AUV's situational awareness and decision-making\ncapabilities. Furthermore, the framework incorporates AUV structure\ncharacteristics into the optimization process, employing a large language model\n(LLM)-based iterative refinement mechanism that leverages both environmental\nconditions and training outcomes to optimize task performance. Comprehensive\nexperimental evaluations demonstrate the framework's superior performance,\nrobustness and adaptability.", "comment": "This paper has been accepted by IROS 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15082v1", "AI": {"title_translation": "让你的AUV更具适应性：一种面向水下任务的环境感知强化学习框架", "tldr": "提出了一种环境感知强化学习框架，通过整合环境信息和LLM优化，显著提升AUV在水下环境的适应性、性能和鲁棒性。", "motivation": "旨在增强自主水下航行器（AUV）在水下环境中的操作能力，解决传统强化学习架构在水下环境适应性不足的问题。", "method": "提出了一种新颖的环境感知强化学习（RL）框架。该框架包含一个环境感知网络模块，用于动态捕获流场数据并将其嵌入状态空间，从而实现实时环境适应。此外，该框架将AUV结构特性纳入优化过程，并采用基于大型语言模型（LLM）的迭代细化机制，结合环境条件和训练结果来优化任务性能。", "result": "综合实验评估表明，该框架在性能、鲁棒性和适应性方面表现出优越性。", "conclusion": "该环境感知强化学习框架通过深度整合环境信息和利用LLM进行优化，显著提升了AUV在复杂水下环境中的自主适应能力、决策水平和整体任务表现。", "translation": "本研究提出了一种新颖的环境感知强化学习（RL）框架，旨在增强自主水下航行器（AUV）在水下环境中的操作能力。该框架与传统RL架构不同，它集成了一个环境感知网络模块，可以动态捕获流场数据，有效地将这种关键环境信息嵌入到状态空间中。这种集成促进了实时环境适应，显著提高了AUV的态势感知和决策能力。此外，该框架将AUV结构特性纳入优化过程，采用基于大型语言模型（LLM）的迭代细化机制，利用环境条件和训练结果来优化任务性能。全面的实验评估表明该框架具有卓越的性能、鲁棒性和适应性。", "summary": "本文提出了一种新颖的环境感知强化学习（RL）框架，旨在提升自主水下航行器（AUV）在复杂水下环境中的操作能力。该框架通过一个环境感知网络模块将流场数据动态集成到状态空间中，实现了AUV的实时环境适应。同时，它还结合了AUV结构特性，并利用基于大型语言模型（LLM）的迭代细化机制来优化任务性能。实验结果验证了该框架在性能、鲁棒性和适应性方面的优越性。", "keywords": "环境感知强化学习, 自主水下航行器, 流场数据, 大型语言模型, 适应性", "comments": "该研究的创新点在于将环境感知网络模块与强化学习结合，并引入LLM进行迭代优化，以增强AUV在动态水下环境中的适应性。这种方法有望显著提升AUV的自主性和任务成功率，对水下机器人领域具有重要意义。"}}
{"id": "2506.15050", "title": "Truncated Proximal Policy Optimization", "authors": ["Tiantian Fan", "Lingjun Liu", "Yu Yue", "Jiaze Chen", "Chengyi Wang", "Qiying Yu", "Chi Zhang", "Zhiqi Lin", "Ruofei Zhu", "Yufeng Yuan", "Xiaochen Zuo", "Bole Ma", "Mofan Zhang", "Gaohong Liu", "Ru Zhang", "Haotian Zhou", "Cong Xie", "Ruidong Zhu", "Zhi Zhang", "Xin Liu", "Mingxuan Wang", "Lin Yan", "Yonghui Wu"], "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15050v1", "AI": {"title_translation": "截断近端策略优化", "tldr": "鉴于长响应导致PPO训练LLM耗时，本文提出了截断近端策略优化（T-PPO），通过简化策略更新和限制长度的响应生成来提高训练效率，实现了高达2.5倍的加速。", "motivation": "大型语言模型（LLMs）通过生成长思考链（CoT）展现出卓越的推理能力。强化学习（RL），特别是PPO及其变体，是开发这些推理模型的关键组成部分。然而，PPO由于其固有的在线策略性质，在响应长度增加时会变得非常耗时，且存在硬件利用率低的问题。", "method": "本文提出了截断近端策略优化（T-PPO）。主要贡献有两点：1. 提出扩展广义优势估计（EGAE），用于从不完整响应中估计优势，同时保持策略学习的完整性。2. 设计了一种计算优化的机制，通过选择性过滤提示和截断的token，允许策略模型和价值模型独立优化，减少冗余计算，加速训练过程。", "result": "在AIME 2024数据集上使用32B基础模型进行实验，T-PPO将推理LLMs的训练效率提高了高达2.5倍，并优于现有竞争对手。", "conclusion": "T-PPO有效且高效地提高了推理LLMs的训练效率，解决了PPO在长响应生成中的效率问题。", "translation": "最近，测试时扩展的大型语言模型（LLMs）通过生成长思考链（CoT）在科学和专业任务中展示了卓越的推理能力。作为开发这些推理模型的关键组成部分，以近端策略优化（PPO）及其变体为代表的强化学习（RL）允许模型通过试错学习。然而，PPO由于其固有的在线策略性质可能会非常耗时，响应长度的增加进一步加剧了这一问题。在这项工作中，我们提出了截断近端策略优化（T-PPO），这是PPO的一种新颖扩展，通过简化策略更新和限制长度的响应生成来提高训练效率。T-PPO缓解了硬件利用率低的问题，这是完全同步长生成过程固有的缺点，在等待完整rollout期间资源经常处于闲置状态。我们的贡献是双重的。首先，我们提出了扩展广义优势估计（EGAE），用于从不完整响应中估计优势，同时保持策略学习的完整性。其次，我们设计了一种计算优化的机制，允许策略模型和价值模型独立优化。通过选择性过滤提示和截断的token，该机制减少了冗余计算，并在不牺牲收敛性能的情况下加速了训练过程。我们展示了T-PPO在AIME 2024上使用32B基础模型的有效性和效率。实验结果表明，T-PPO将推理LLMs的训练效率提高了高达2.5倍，并优于现有竞争对手。", "summary": "本文针对大型语言模型（LLMs）中近端策略优化（PPO）因长响应而导致的训练效率低下问题，提出了一种新颖的扩展——截断近端策略优化（T-PPO）。T-PPO通过引入扩展广义优势估计（EGAE）来处理不完整响应，并设计了一种优化机制以实现策略和价值模型的独立优化，从而简化策略更新和限制响应长度的生成。实验结果表明，T-PPO在推理LLMs的训练效率上实现了高达2.5倍的提升，并超越了现有方法。", "keywords": "近端策略优化, 大型语言模型, 强化学习, 训练效率, 思考链", "comments": "T-PPO的创新之处在于其对PPO的改进，特别是在处理LLM长思考链生成时的效率瓶颈。通过引入EGAE和独立的模型优化机制，它有效地解决了PPO固有的硬件利用率低和计算冗余问题，显著加速了推理LLM的训练过程。这对于LLM的实际部署和进一步研究具有重要意义。"}}
{"id": "2506.14992", "title": "Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks", "authors": ["Zhihao Tao", "Athina P. Petropulu"], "summary": "We propose a novel directional modulation (DM) design for OFDM transmitters\naided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS\nis configured to preserve the integrity of transmitted signals toward multiple\nlegitimate users while scrambling the signal in all other directions. Existing\nTM-IRS design methods typically target a single user direction and follow\npredefined rule-based procedures, making them unsuitable for multi-user\nscenarios. Here, we propose a generative AI-based approach to design good sets\nof TM-IRS parameters out of a set of all possible quantized ranges of\nparameters. The design objective is to maximize the sum rate across the\nauthorized directions. We model the TM-IRS parameter selection as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a specific configuration of TM-IRS parameters. GFlowNets are\nemployed to learn a stochastic policy that samples TM-IRS parameter sets with\nprobability proportional to their associated sum rate reward. Experimental\nresults demonstrate that the proposed method effectively enhances the security\nof the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size\nof the TM-IRS configuration space, the GFlowNet is able to converge after\ntraining on fewer than 0.000001% of all possible configurations, demonstrating\nremarkable efficiency compared to exhaustive combinatorial search.\nImplementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to\nfacilitate reproducibility.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14992v1", "AI": {"title_translation": "通过生成流网络实现安全时变智能反射面", "tldr": "本文提出了一种基于生成式AI（GFlowNet）的方法，用于安全的多用户时变智能反射面（TM-IRS）设计，优于基于规则的方法并实现了高效率。", "motivation": "现有TM-IRS设计方法通常针对单个用户方向，并遵循预定义的基于规则的程序，这使得它们不适用于多用户场景。", "method": "提出了一种基于生成式AI的方法，通过使用GFlowNets学习随机策略来选择TM-IRS参数。将TM-IRS参数选择建模为确定性马尔可夫决策过程（MDP），旨在最大化授权方向的总和速率。", "result": "实验结果表明，所提出的方法有效地增强了多用户TM-IRS辅助OFDM系统的安全性。尽管TM-IRS配置空间巨大，GFlowNet在少于0.000001%的所有可能配置上训练后能够收敛，与穷举组合搜索相比，展现出显著的效率。", "conclusion": "本文成功展示了一种新颖高效的基于生成式AI的多用户TM-IRS安全设计方法，解决了现有方法的局限性。", "translation": "我们提出了一种由时变智能反射面（TM-IRS）辅助的OFDM发射机的新型定向调制（DM）设计。TM-IRS被配置为在向多个合法用户传输信号时保持其完整性，同时在所有其他方向上对信号进行加扰。现有的TM-IRS设计方法通常针对单个用户方向，并遵循预定义的基于规则的程序，这使得它们不适用于多用户场景。在此，我们提出了一种基于生成式AI的方法，从所有可能的量化参数范围中设计出良好的TM-IRS参数集。设计目标是最大化授权方向上的总和速率。我们将TM-IRS参数选择建模为确定性马尔可夫决策过程（MDP），其中每个终端状态对应于TM-IRS参数的特定配置。GFlowNets被用来学习一个随机策略，以与其关联的总和速率奖励成比例的概率采样TM-IRS参数集。实验结果表明，所提出的方法有效地增强了TM-IRS辅助OFDM系统在多用户情况下的安全性。此外，尽管TM-IRS配置空间巨大，GFlowNet在少于所有可能配置的0.000001%的训练后能够收敛，与穷举组合搜索相比，展现出显著的效率。实现代码可在https://github.com/ZhihaoTao/GFN4TM-RIS获取，以方便复现。", "summary": "本文提出了一种新颖的定向调制（DM）设计，用于由时变智能反射面（TM-IRS）辅助的OFDM发射机。与传统的单用户、基于规则的TM-IRS设计不同，本文引入了一种基于生成式AI的方法，利用生成流网络（GFlowNets）并将参数选择建模为马尔可夫决策过程（MDP），以最大化多个合法用户的总和速率。该方法确保了授权用户的信号完整性，同时在其他方向上对信号进行加扰，展示了增强的安全性以及在巨大配置空间中收敛的显著效率。", "keywords": "时变智能反射面, 生成流网络, 定向调制, OFDM, 多用户安全", "comments": "本文的创新之处在于将生成式AI（GFlowNets）应用于TM-IRS设计，将复杂的组合优化问题转化为学习任务。其在探索巨大参数空间方面的效率是一个显著优势，使其在多用户场景中具有实用性。"}}
{"id": "2506.14782", "title": "Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials", "authors": ["Joseph Geraci", "Bessi Qorri", "Christian Cumbaa", "Mike Tsay", "Paul Leonczyk", "Luca Pani"], "summary": "Artificial intelligence (AI) has evolved into an ecosystem of specialized\n\"species,\" each with unique strengths. We analyze two: DeepSeek-V3, a\n671-billion-parameter Mixture of Experts large language model (LLM)\nexemplifying scale-driven generality, and NetraAI, a dynamical system-based\nframework engineered for stability and interpretability on small clinical trial\ndatasets. We formalize NetraAI's foundations, combining contraction mappings,\ninformation geometry, and evolutionary algorithms to identify predictive\npatient cohorts. Features are embedded in a metric space and iteratively\ncontracted toward stable attractors that define latent subgroups. A\npseudo-temporal embedding and long-range memory enable exploration of\nhigher-order feature interactions, while an internal evolutionary loop selects\ncompact, explainable 2-4-variable bundles (\"Personas\").\n  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary\nlayer that observes Persona outputs, prioritizes promising variables, injects\ndomain knowledge, and assesses robustness. This two-tier architecture mirrors\nthe human scientific process: NetraAI as experimentalist, the LLM as theorist,\nforming a self-improving loop.\n  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI\nuncovered small, high-effect-size subpopulations that transformed weak baseline\nmodels (AUC ~0.50-0.68) into near-perfect classifiers using only a few\nfeatures. We position NetraAI at the intersection of dynamical systems,\ninformation geometry, and evolutionary learning, aligned with emerging\nconcept-level reasoning paradigms such as LeCun's Joint Embedding Predictive\nArchitecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI\noffers a new generation of adaptive, self-reflective AI to accelerate clinical\ndiscovery.", "comment": "27 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14782v1", "AI": {"title_translation": "将动力系统学习与基础模型相结合：一个用于临床试验的元进化AI框架", "tldr": "本文提出一个结合动力系统学习（NetraAI）和大型语言模型（LLM）的元进化AI框架，用于临床试验。该框架能从小数据集发现高效应量的亚群，将弱基线模型转化为接近完美的分类器，提供可解释的知识。", "motivation": "人工智能（AI）已发展出多种具有独特优势的“物种”。本文旨在结合不同AI的优势（如大型语言模型DeepSeek-V3的通用性和NetraAI在小型临床试验数据集上的稳定性与可解释性），以加速临床发现并提供可靠、可解释的知识。现有模型在小型数据集上表现较弱，需要一种新方法将其转化为强大的分类器。", "method": "本文提出了一个两层AI架构：NetraAI作为实验者，LLM作为理论家，形成一个自我改进的循环。NetraAI是一个基于动力系统的框架，结合了收缩映射、信息几何和进化算法来识别预测性患者队列。它将特征嵌入度量空间，并迭代收缩至稳定的吸引子以定义潜在亚群。通过伪时间嵌入和长程记忆探索高阶特征交互，内部进化循环选择紧凑、可解释的2-4变量组合（“Persona”）。LLM策略师作为元进化层，观察Persona输出，优先处理有前景的变量，注入领域知识，并评估鲁棒性。", "result": "在精神分裂症、抑郁症和胰腺癌的案例研究中，NetraAI发现了小型、高效应量的亚群。这使得弱基线模型（AUC约0.50-0.68）仅使用少量特征就能转化为接近完美的分类器。", "conclusion": "NetraAI位于动力系统、信息几何和进化学习的交叉点，与LeCun的联合嵌入预测架构（JEPA）等新兴概念级推理范式对齐。通过优先考虑可靠、可解释的知识，NetraAI提供了一种新一代自适应、自反思的AI，以加速临床发现。", "translation": "人工智能（AI）已经发展成为一个由专业化“物种”组成的生态系统，每个物种都具有独特的优势。我们分析了两种：DeepSeek-V3，一个拥有6710亿参数的混合专家大型语言模型（LLM），代表了规模驱动的通用性；以及NetraAI，一个基于动力系统的框架，专为小型临床试验数据集的稳定性和可解释性而设计。我们形式化了NetraAI的基础，结合了收缩映射、信息几何和进化算法来识别预测性患者队列。特征被嵌入到一个度量空间中，并迭代收缩向定义潜在亚群的稳定吸引子。伪时间嵌入和长程记忆能够探索高阶特征交互，而内部进化循环则选择紧凑、可解释的2-4变量组合（“Persona”）。\n为了指导发现，我们引入了一个LLM策略师作为元进化层，它观察Persona输出，优先处理有前景的变量，注入领域知识，并评估鲁棒性。这种两层架构反映了人类的科学过程：NetraAI作为实验者，LLM作为理论家，形成一个自我改进的循环。\n在案例研究（精神分裂症、抑郁症、胰腺癌）中，NetraAI发现了小型、高效应量的亚群，这些亚群将弱基线模型（AUC约0.50-0.68）仅使用少量特征就转化为了接近完美的分类器。我们将NetraAI定位在动力系统、信息几何和进化学习的交叉点，与LeCun的联合嵌入预测架构（JEPA）等新兴概念级推理范式对齐。通过优先考虑可靠、可解释的知识，NetraAI提供了一种新一代自适应、自反思的AI，以加速临床发现。", "summary": "本文提出一个创新的元进化AI框架，将动力系统学习与基础模型相结合，用于临床试验。该框架核心是NetraAI，一个基于动力系统、信息几何和进化算法的AI，旨在从小规模临床数据中识别可预测的患者亚群，并通过迭代收缩和进化选择生成可解释的特征组合（“Persona”）。一个大型语言模型（LLM）充当策略师，作为元进化层指导发现过程。在精神分裂症、抑郁症和胰腺癌的案例研究中，该框架成功地将弱基线模型（AUC约0.50-0.68）转化为接近完美的分类器，仅使用少量特征便发现了高效应量的亚群。该研究旨在加速临床发现，并提供可靠、可解释的AI知识。", "keywords": "动力系统, 基础模型, 临床试验, 元进化AI, 可解释性", "comments": "这项研究的创新之处在于其独特的两层元进化AI架构，它巧妙地将专门的动力系统AI（NetraAI）与通用的LLM相结合。这种结合有效地解决了临床AI在小数据集上对可解释性和鲁棒性的关键需求，并实现了科学发现过程的自动化。其能够利用少量特征发现高效应量亚群的能力，对于临床应用具有高度影响力。"}}
{"id": "2506.15312", "title": "One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning", "authors": ["Han Wu", "Junyao Li", "Kangbo Zhao", "Sen Zhang", "Yukai Shi", "Liang Lin"], "summary": "Face sketch synthesis is a technique aimed at converting face photos into\nsketches. Existing face sketch synthesis research mainly relies on training\nwith numerous photo-sketch sample pairs from existing datasets. However, these\nlarge-scale discriminative learning methods will have to face problems such as\ndata scarcity and high human labor costs. Once the training data becomes\nscarce, their generative performance significantly degrades. In this paper, we\npropose a one-shot face sketch synthesis method based on diffusion models. We\noptimize text instructions on a diffusion model using face photo-sketch image\npairs. Then, the instructions derived through gradient-based optimization are\nused for inference. To simulate real-world scenarios more accurately and\nevaluate method effectiveness more comprehensively, we introduce a new\nbenchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark\nconsists of 400 pairs of face photo-sketch images, including sketches with\ndifferent styles and photos with different backgrounds, ages, sexes,\nexpressions, illumination, etc. For a solid out-of-distribution evaluation, we\nselect only one pair of images for training at each time, with the rest used\nfor inference. Extensive experiments demonstrate that the proposed method can\nconvert various photos into realistic and highly consistent sketches in a\none-shot context. Compared to other methods, our approach offers greater\nconvenience and broader applicability. The dataset will be available at:\nhttps://github.com/HanWu3125/OS-Sketch", "comment": "We propose a novel framework for face sketch synthesis, where merely\n  a single pair of samples suffices to enable in-the-wild face sketch synthesis", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15312v1", "AI": {"title_translation": "通过生成扩散先验和指令微调实现野外单次人脸草图合成", "tldr": "该论文提出了一种基于扩散模型和指令微调的单次人脸草图合成方法，解决了传统方法数据稀缺的问题，并引入了一个新的基准数据集。", "motivation": "现有的人脸草图合成方法主要依赖于大量照片-草图样本对进行训练，导致数据稀缺和高昂的人力成本问题。一旦训练数据稀缺，其生成性能会显著下降。", "method": "提出了一种基于扩散模型的单次人脸草图合成方法。该方法利用人脸照片-草图图像对优化扩散模型上的文本指令，然后使用通过梯度优化得到的指令进行推理。为更准确地模拟真实世界场景并全面评估方法的有效性，引入了一个名为One-shot Face Sketch Dataset (OS-Sketch) 的新基准数据集，包含400对人脸照片-草图图像。在评估时，每次只选择一对图像进行训练，其余用于推理，以进行稳健的域外评估。", "result": "所提出的方法能够在单次学习的背景下，将各种照片转换为逼真且高度一致的草图。与其他方法相比，该方法提供了更大的便利性和更广泛的适用性。", "conclusion": "该论文成功地展示了一种利用扩散模型和指令微调实现的单次人脸草图合成方法，克服了数据稀缺问题，并在多样化的真实世界场景中表现出强大的性能。", "translation": "人脸草图合成是一种旨在将人脸照片转换为草图的技术。现有的人脸草图合成研究主要依赖于使用现有数据集中大量的照片-草图样本对进行训练。然而，这些大规模判别学习方法将不得不面对数据稀缺和高昂的人力成本等问题。一旦训练数据变得稀缺，它们的生成性能就会显著下降。在本文中，我们提出了一种基于扩散模型的单次人脸草图合成方法。我们使用人脸照片-草图图像对来优化扩散模型上的文本指令。然后，通过基于梯度的优化得到的指令用于推理。为了更准确地模拟真实世界场景并更全面地评估方法的有效性，我们引入了一个名为单次人脸草图数据集（OS-Sketch）的新基准。该基准包含400对人脸照片-草图图像，其中包括不同风格的草图和不同背景、年龄、性别、表情、光照等照片。为了进行稳健的域外评估，我们每次只选择一对图像进行训练，其余用于推理。大量的实验表明，所提出的方法可以在单次学习的环境下将各种照片转换为逼真且高度一致的草图。与其他方法相比，我们的方法提供了更大的便利性和更广泛的适用性。数据集将在此处提供：https://github.com/HanWu3125/OS-Sketch", "summary": "本文提出了一种新颖的单次人脸草图合成方法，利用生成扩散模型和指令微调来克服传统方法的数据稀缺问题。该方法使用单个照片-草图对来优化扩散模型上的文本指令进行训练，然后应用这些优化后的指令进行推理。为了验证其在真实世界条件下的有效性，作者还提出了一个新的基准数据集OS-Sketch，包含400对多样化的照片-草图对。大量实验证实，该方法能够从各种照片中生成逼真且一致的草图，与现有方法相比，展现出卓越的便利性和更广泛的适用性。", "keywords": "单次学习, 人脸草图合成, 扩散模型, 指令微调, OS-Sketch", "comments": "该论文的创新之处在于将扩散模型应用于单次学习的人脸草图合成，显著降低了对数据的依赖。引入一个新的基准数据集（OS-Sketch）用于在多样化真实世界场景中的单次评估也是一个重要的贡献。这种方法解决了以往方法的一个关键局限性，使得该技术更具实用性和可扩展性。"}}
{"id": "2506.15284", "title": "Multi-Interest Recommendation: A Survey", "authors": ["Zihao Li", "Qiang Chen", "Lixin Zou", "Aixin Sun", "Chenliang Li"], "summary": "Existing recommendation methods often struggle to model users' multifaceted\npreferences due to the diversity and volatility of user behavior, as well as\nthe inherent uncertainty and ambiguity of item attributes in practical\nscenarios. Multi-interest recommendation addresses this challenge by extracting\nmultiple interest representations from users' historical interactions, enabling\nfine-grained preference modeling and more accurate recommendations. It has\ndrawn broad interest in recommendation research. However, current\nrecommendation surveys have either specialized in frontier recommendation\nmethods or delved into specific tasks and downstream applications. In this\nwork, we systematically review the progress, solutions, challenges, and future\ndirections of multi-interest recommendation by answering the following three\nquestions: (1) Why is multi-interest modeling significantly important for\nrecommendation? (2) What aspects are focused on by multi-interest modeling in\nrecommendation? and (3) How can multi-interest modeling be applied, along with\nthe technical details of the representative modules? We hope that this survey\nestablishes a fundamental framework and delivers a preliminary overview for\nresearchers interested in this field and committed to further exploration. The\nimplementation of multi-interest recommendation summarized in this survey is\nmaintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15284v1", "AI": {"title_translation": "多兴趣推荐：一项综述", "tldr": "本文综述了多兴趣推荐的研究进展、解决方案、挑战和未来方向，旨在解决现有推荐系统难以建模用户多面偏好问题。", "motivation": "现有推荐方法难以建模用户多面偏好，原因是用户行为的多样性、易变性以及物品属性固有的不确定性和模糊性。多兴趣推荐通过从用户历史交互中提取多个兴趣表征来解决此挑战，实现更细粒度的偏好建模和更准确的推荐。", "method": "本文通过回答三个问题系统地综述了多兴趣推荐的进展、解决方案、挑战和未来方向：(1) 为何多兴趣建模对推荐如此重要？(2) 多兴趣建模在推荐中关注哪些方面？(3) 如何应用多兴趣建模及其代表性模块的技术细节？", "result": "本文建立了多兴趣推荐的基础框架，并为该领域的研究人员提供了初步概述。论文中总结的多兴趣推荐的实现方案在GitHub上维护。", "conclusion": "这项综述有望为对多兴趣推荐领域感兴趣并致力于进一步探索的研究人员建立一个基本框架并提供初步概述。", "translation": "现有推荐方法常常难以建模用户多方面的偏好，这归因于用户行为的多样性和易变性，以及实际场景中物品属性固有的不确定性和模糊性。多兴趣推荐通过从用户历史交互中提取多个兴趣表征来解决这一挑战，从而实现细粒度的偏好建模和更准确的推荐。它在推荐研究中引起了广泛兴趣。然而，当前的推荐综述要么专注于前沿推荐方法，要么深入探讨特定任务和下游应用。在这项工作中，我们通过回答以下三个问题，系统地回顾了多兴趣推荐的进展、解决方案、挑战和未来方向：(1) 为什么多兴趣建模对推荐如此重要？(2) 多兴趣建模在推荐中关注哪些方面？(3) 如何应用多兴趣建模，以及代表性模块的技术细节？我们希望这项综述能为对该领域感兴趣并致力于进一步探索的研究人员建立一个基础框架并提供初步概述。本综述中总结的多兴趣推荐的实现方案维护在 https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey。", "summary": "本文对多兴趣推荐进行了系统综述，旨在解决现有推荐系统在建模用户多面偏好方面的不足。文章详细探讨了多兴趣建模的重要性、关注点以及应用方法和技术细节，为研究人员提供了一个全面的视角和基础框架。", "keywords": "多兴趣推荐, 推荐系统, 用户偏好, 综述, 兴趣建模", "comments": "该综述性论文填补了现有推荐综述在多兴趣推荐领域的空白，系统性地梳理了该领域的进展、挑战和未来方向，对于推动多兴趣推荐研究具有重要指导意义。其提供的GitHub资源也便于研究者进一步探索和实践。"}}
{"id": "2506.15602", "title": "Estimate Hitting Time by Hitting Probability for Elitist Evolutionary Algorithms", "authors": ["Jun He", "Siang Yew Chong", "Xin Yao"], "summary": "Drift analysis is a powerful tool for analyzing the time complexity of\nevolutionary algorithms. However, it requires manual construction of drift\nfunctions to bound hitting time for each specific algorithm and problem. To\naddress this limitation, general linear drift functions were introduced for\nelitist evolutionary algorithms. But calculating linear bound coefficients\neffectively remains a problem. This paper proposes a new method called drift\nanalysis of hitting probability to compute these coefficients. Each coefficient\nis interpreted as a bound on the hitting probability of a fitness level,\ntransforming the task of estimating hitting time into estimating hitting\nprobability. A novel drift analysis method is then developed to estimate\nhitting probability, where paths are introduced to handle multimodal fitness\nlandscapes. Explicit expressions are constructed to compute hitting\nprobability, significantly simplifying the estimation process. One advantage of\nthe proposed method is its ability to estimate both the lower and upper bounds\nof hitting time and to compare the performance of two algorithms in terms of\nhitting time. To demonstrate this application, two algorithms for the knapsack\nproblem, each incorporating feasibility rules and greedy repair respectively,\nare compared. The analysis indicates that neither constraint handling technique\nconsistently outperforms the other.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.15602v1", "AI": {"title_translation": "通过命中概率估计精英进化算法的命中时间", "tldr": "本文提出了一种新的漂移分析方法，通过估计命中概率来计算精英进化算法的命中时间界限，并能处理多峰适应度景观，简化了估计过程，并可用于比较算法性能。", "motivation": "传统的漂移分析工具在分析进化算法时间复杂度时，需要为每个特定算法和问题手动构建漂移函数来界定命中时间，且计算线性界限系数仍然是一个问题。", "method": "本文提出了一种名为“命中概率漂移分析”的新方法来计算界限系数。每个系数被解释为适应度水平命中概率的界限，从而将估计命中时间转化为估计命中概率。引入路径来处理多峰适应度景观，并构建显式表达式来计算命中概率。", "result": "新方法能够估计命中时间的下界和上界，并能比较两种算法在命中时间方面的性能。通过对背包问题的两种算法进行比较，分析表明两种约束处理技术（可行性规则和贪婪修复）在性能上没有一致的优劣。", "conclusion": "通过将命中时间估计转化为命中概率估计，并引入路径处理多峰景观，本文提出的漂移分析方法显著简化了精英进化算法的命中时间估计过程，并能有效地比较算法性能，但针对背包问题的案例研究表明，两种不同的约束处理技术并没有显示出一致的优势。", "translation": "漂移分析是分析进化算法时间复杂度的一种强大工具。然而，它需要为每个特定算法和问题手动构建漂移函数来界定命中时间。为了解决这个限制，针对精英进化算法引入了通用线性漂移函数。但有效计算线性界限系数仍然是一个问题。本文提出了一种名为“命中概率漂移分析”的新方法来计算这些系数。每个系数被解释为适应度水平命中概率的界限，从而将估计命中时间转化为估计命中概率。然后开发了一种新颖的漂移分析方法来估计命中概率，其中引入了路径来处理多峰适应度景观。构建了显式表达式来计算命中概率，显著简化了估计过程。所提出方法的一个优点是它能够估计命中时间的下界和上界，并能比较两种算法在命中时间方面的性能。为了展示这种应用，比较了两种用于背包问题的算法，它们分别结合了可行性规则和贪婪修复。分析表明，这两种约束处理技术都没有一致地优于对方。", "summary": "本文针对精英进化算法中线性漂移分析系数难以计算的问题，提出了一种基于命中概率漂移分析的新方法。该方法将命中时间估计转化为命中概率估计，并引入路径来处理多峰适应度景观，从而简化了计算过程。所提出的方法能够估计命中时间的上下界，并可用于比较不同算法的性能。通过对背包问题的实例分析，发现两种不同的约束处理技术在性能上没有一致的优势。", "keywords": "漂移分析, 命中时间, 命中概率, 精英进化算法, 多峰适应度景观", "comments": "该论文的创新点在于将进化算法的命中时间估计问题转化为命中概率估计，并通过引入“路径”的概念有效处理了多峰适应度景观，显著简化了传统漂移分析中手动构建漂移函数的复杂性。这为分析精英进化算法的时间复杂度提供了一种更通用和实用的工具。其能够估计上下界并比较算法性能的特性也增加了其实用价值。"}}
{"id": "2506.14912", "title": "CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision", "authors": ["Dyah Adila", "Shuai Zhang", "Boran Han", "Bonan Min", "Yuyang Wang"], "summary": "The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14912v1", "AI": {"title_translation": "CrEst：通过弱监督评估大型语言模型中上下文的可信度", "tldr": "CrEst是一个新颖的弱监督框架，用于在LLM推理期间评估上下文文档的可信度，无需手动标注，通过文档间语义一致性实现，显著提高了LLM在知识密集型任务上的性能。", "motivation": "大型语言模型（LLM）在整合上下文信息时，现有方法忽视了上下文文档可信度差异大的问题，可能导致不可靠信息的传播。", "method": "本文引入了CrEst，一个弱监督框架，用于评估LLM推理期间上下文文档的可信度。核心思想是可信文档与其他可信文档语义一致性更高，通过文档间一致性自动估计。CrEst提出了两种整合策略：黑盒方法（不访问内部权重或激活）和白盒方法（直接修改注意力机制）。", "result": "CrEst在三种模型架构和五个数据集上进行了广泛实验，结果显示其始终优于强基线，准确率最高提升26.86%，F1分数提高3.49%。即使在高噪声条件下，CrEst也能保持稳健性能。", "conclusion": "CrEst通过弱监督有效评估LLM上下文可信度，显著提升了LLM在知识密集型任务上的表现，并在噪声环境下表现出鲁棒性。", "translation": "大型语言模型（LLM）中上下文信息的整合显著增强了其在知识密集型任务上的性能。然而，现有方法通常忽视了一个关键挑战：上下文文档的可信度可能差异很大，这可能导致不可靠信息的传播。在本文中，我们引入了CrEst，一个新颖的弱监督框架，用于在LLM推理期间评估上下文文档的可信度——无需手动标注。我们的方法基于一个洞察：可信文档倾向于与其他可信文档表现出更高的语义连贯性，从而通过文档间一致性实现自动可信度估计。为了将可信度整合到LLM推理中，我们提出了两种整合策略：一种适用于无法访问内部权重或激活的模型，即黑盒方法；以及一种直接修改注意力机制的白盒方法。在三种模型架构和五个数据集上进行的大量实验表明，CrEst始终优于强大的基线，准确率提高了高达26.86%，F1分数提高了3.49%。进一步分析表明，即使在高噪声条件下，CrEst也能保持稳健的性能。", "summary": "本文介绍了CrEst，一个创新的弱监督框架，旨在解决大型语言模型在利用上下文信息时面临的可信度挑战。CrEst通过分析文档间的语义一致性来自动评估上下文文档的可信度，无需人工标注。为将可信度整合到LLM推理中，CrEst提供了黑盒和白盒两种策略。实验证明，CrEst在多个数据集和模型上显著优于现有方法，提高了LLM在知识密集型任务上的准确性和F1分数，并展现出在高噪声环境下的鲁棒性。", "keywords": "可信度评估, 弱监督, 大型语言模型, 上下文信息, 语义一致性", "comments": "CrEst的创新之处在于其弱监督方法，避免了昂贵的人工标注，并通过“文档间一致性”这一新颖视角来评估可信度。其提出的黑盒和白盒整合策略也增加了方法的通用性。这项工作对于提升LLM在知识密集型任务中输出的可靠性具有重要意义，尤其是在信息来源复杂多变的场景下。"}}
{"id": "2506.14788", "title": "Energy-consistent dynamic fracture phase field models: unilateral constraints and finite element simulations", "authors": ["Md Mamun Miah", "Ryuhei Wakida", "Masato Kimura"], "summary": "Phase field models have emerged as a powerful and flexible framework for\nsimulating complex interface-driven phenomena across a wide range of scientific\nand engineering applications. In fracture mechanics, the phase field\napproach--formulated as a gradient flow of the Griffith fracture energy with\nAmbrosio-Tortorelli regularization--has gained significant attention for its\nability to capture complex crack topologies. In this study, we propose a\ndynamic fracture phase field model (DF-PFM) based on the elastodynamic wave\nequation. We further extend this framework by incorporating a unilateral\ncontact condition, yielding a refined model suitable for simulating fault\nrupture under high pressure. For both models, we rigorously derive energy\ndissipation identities under mixed boundary conditions, ensuring energy\nconsistency of the formulations. To validate the proposed approach, we conduct\nnumerical experiments using linear implicit time discretization and finite\nelement methods. Our simulations demonstrate that the unilateral contact\ncondition is essential for accurately capturing shear-dominated crack\npropagation and preventing non-physical interpenetration, especially under\nhigh-compression loading scenarios relevant to seismic faulting.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14788v1", "AI": {"title_translation": "能量一致的动态断裂相场模型：单边约束和有限元模拟", "tldr": "本研究提出了一个基于弹动力学波动方程的动态断裂相场模型 (DF-PFM)，并引入了单边接触条件，以模拟高压下的断层破裂。模型推导了能量耗散恒等式以确保能量一致性。数值模拟表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理渗透至关重要，尤其是在高压缩载荷下。", "motivation": "相场模型在模拟复杂界面驱动现象方面表现出色，尤其在断裂力学中能够捕捉复杂的裂纹拓扑结构。然而，为了更准确地模拟高压下的断层破裂等动态断裂现象，需要一个能量一致且包含单边接触条件的动态断裂相场模型。", "method": "本研究提出了一个基于弹动力学波动方程的动态断裂相场模型 (DF-PFM)。在此基础上，通过引入单边接触条件进一步扩展了该框架。对于这两种模型，严格推导了混合边界条件下的能量耗散恒等式，以确保公式的能量一致性。数值实验采用线性隐式时间离散化和有限元方法进行验证。", "result": "模拟结果表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理渗透至关重要，尤其是在与地震断层相关的、高压缩载荷情景下。", "conclusion": "单边接触条件对于准确模拟动态断裂过程（特别是剪切主导的裂纹扩展和防止非物理渗透）至关重要，尤其是在高压缩载荷下，这对于地震断层等应用具有重要意义。", "translation": "相场模型已成为模拟广泛科学和工程应用中复杂界面驱动现象的强大而灵活的框架。在断裂力学中，相场方法——被表述为格里菲斯断裂能与安布罗西奥-托尔托雷利正则化的梯度流——因其能够捕捉复杂裂纹拓扑结构而受到广泛关注。在本研究中，我们提出了一个基于弹动力学波动方程的动态断裂相场模型（DF-PFM）。我们通过引入单边接触条件进一步扩展了这一框架，从而得到了一个适用于模拟高压下断层破裂的精细模型。对于这两种模型，我们严格推导了混合边界条件下的能量耗散恒等式，确保了公式的能量一致性。为了验证所提出的方法，我们使用线性隐式时间离散化和有限元方法进行了数值实验。我们的模拟表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理渗透至关重要，尤其是在与地震断层相关的高压缩载荷情景下。", "summary": "本研究提出并验证了一种能量一致的动态断裂相场模型（DF-PFM），该模型基于弹动力学波动方程，并特别引入了单边接触条件以模拟高压下的断层破裂。研究严格推导了模型的能量耗散恒等式，确保了能量一致性。通过有限元模拟，结果证实单边接触条件对于准确捕捉剪切主导裂纹扩展和防止非物理渗透至关重要，尤其适用于地震断层等高压缩载荷场景。", "keywords": "动态断裂, 相场模型, 单边约束, 有限元模拟, 能量一致性", "comments": "这项工作在动态断裂相场模型中引入了单边接触条件，这是一个重要的创新点，因为它解决了在高压缩载荷下裂纹模拟中常见的非物理渗透问题，并提高了对剪切主导裂纹扩展的捕捉能力。其能量一致性的严格推导也增加了模型的鲁棒性。这对于地质灾害模拟等实际工程应用具有重要意义。"}}
{"id": "2506.15168", "title": "Algorithmic resolution of crowd-sourced moderation on X in polarized settings across countries", "authors": ["Paul Bouchaud", "Pedro Ramaciotti"], "summary": "Social platforms increasingly transition from expert fact-checking to\ncrowd-sourced moderation, with X pioneering this shift through its Community\nNotes system, enabling users to collaboratively moderate misleading content. To\nresolve conflicting moderation, Community Notes learns a latent ideological\ndimension and selects notes garnering cross-partisan support. As this system,\ndesigned for and evaluated in the United States, is now deployed worldwide, we\nevaluate its operation across diverse polarization contexts. We analyze 1.9\nmillion moderation notes with 135 million ratings from 1.2 million users,\ncross-referencing ideological scaling data across 13 countries. Our results\nshow X's Community Notes effectively captures each country's main polarizing\ndimension but fails by design to moderate the most polarizing content, posing\npotential risks to civic discourse and electoral processes.", "comment": "46 pages", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.15168v1", "AI": {"title_translation": "跨国两极分化背景下X平台上众包审核的算法解析", "tldr": "X平台的社区笔记系统在全球范围内虽然能有效捕捉各国的两极分化维度，但其设计缺陷导致无法有效审核最具两极分化的内容，这可能对公民话语和选举过程构成潜在风险。", "motivation": "社交平台正日益从专家事实核查转向众包审核，X平台通过其社区笔记系统率先实现了这一转变，允许用户协作审核误导性内容。该系统最初在美国设计和评估，现已在全球部署，因此有必要评估其在不同两极分化背景下的运作情况。", "method": "本研究分析了190万条审核笔记，这些笔记来自120万用户提供的1.35亿条评分，并交叉参考了13个国家的意识形态量表数据。", "result": "研究结果表明，X平台的社区笔记系统能够有效捕捉每个国家的主要两极分化维度，但其设计缺陷导致无法审核最具两极分化的内容。", "conclusion": "X平台的社区笔记系统未能有效审核最具两极分化的内容，这可能对公民话语和选举过程构成潜在风险。", "translation": "社交平台正日益从专家事实核查转向众包审核，X平台通过其社区笔记系统率先实现了这一转变，该系统允许用户协作审核误导性内容。为了解决冲突的审核意见，社区笔记系统学习了一个潜在的意识形态维度，并选择获得跨党派支持的笔记。由于该系统最初在美国设计和评估，现已在全球部署，我们评估了其在不同两极分化背景下的运作情况。我们分析了190万条审核笔记，这些笔记来自120万用户提供的1.35亿条评分，并交叉参考了13个国家的意识形态量表数据。我们的研究结果表明，X平台的社区笔记系统能够有效捕捉每个国家的主要两极分化维度，但其设计缺陷导致无法审核最具两极分化的内容，这可能对公民话语和选举过程构成潜在风险。", "summary": "本研究评估了X平台社区笔记系统在全球不同两极分化背景下的运作情况。该系统通过学习潜在的意识形态维度来解决众包审核中的冲突。研究分析了来自13个国家的190万条笔记和1.35亿条评分，发现该系统能有效捕捉各国的主要两极分化维度，但其设计缺陷使其无法审核最具两极分化的内容，这可能对公民话语和选举过程造成潜在风险。", "keywords": "众包审核, X平台, 社区笔记, 两极分化, 公民话语", "comments": "该论文揭示了X平台社区笔记系统在全球应用时的一个关键缺陷，即其在处理高度两极化内容方面的固有局限性。这项大规模的跨国实证分析对于理解众包审核系统在复杂社会环境中的影响具有重要意义。"}}
{"id": "2506.15456", "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.15456v1", "AI": {"title_translation": "用于解耦语音分词的分解式RVQ-GAN", "tldr": "本文提出了分层音频编解码器（HAC），它将语音瓶颈分解为声学、语音和词汇三个语言级别，并利用知识蒸馏实现解耦的语音表示，在自然度、可解释性和重建质量方面优于基线。", "motivation": "开发一种统一的神经语音编解码器，能够将语音瓶颈分解为不同的语言级别（声学、语音和词汇），从而生成解耦的语音表示，以弥合声学细节和词汇意义之间的鸿沟，服务于下游语音生成和理解任务。", "method": "本文提出了分层音频编解码器（HAC），这是一种统一的神经语音编解码器，它将其瓶颈分解为声学、语音和词汇三个语言级别。HAC利用两种知识蒸馏目标：一个来自预训练的语音编码器（HuBERT），用于学习音素级别的结构；另一个来自基于文本的编码器（LaBSE），用于捕获词汇线索。", "result": "实验表明，HAC的分解瓶颈产生了解耦的标记集：一个与音素对齐，另一个捕获词级语义。定量评估证实HAC标记保留了自然度并提供了可解释的语言信息，在解耦和重建质量方面均优于单级基线。", "conclusion": "HAC作为一种统一的离散语音表示，具有连接声学细节和词汇意义的潜力，可用于下游语音生成和理解任务。", "translation": "我们提出了分层音频编解码器（HAC），它是一种统一的神经语音编解码器，在一个模型中将其瓶颈分解为声学、语音和词汇三个语言级别。HAC利用两种知识蒸馏目标：一个来自预训练的语音编码器（HuBERT），用于学习音素级别的结构；另一个来自基于文本的编码器（LaBSE），用于捕获词汇线索。在英语和多语言数据上的实验表明，HAC的分解瓶颈产生了解耦的标记集：一个与音素对齐，而另一个捕获词级语义。定量评估证实HAC标记保留了自然度并提供了可解释的语言信息，在解耦和重建质量方面均优于单级基线。这些发现强调了HAC作为一种统一的离散语音表示的潜力，它弥合了声学细节和词汇意义之间的鸿沟，服务于下游语音生成和理解任务。", "summary": "本文提出了一种名为分层音频编解码器（HAC）的统一神经语音编解码器。HAC的创新之处在于将语音瓶颈分解为声学、语音和词汇三个语言层次，并通过结合来自HuBERT（音素级）和LaBSE（词汇级）的知识蒸馏目标，实现了不同语言层次的解耦表示。实验结果表明，HAC生成的标记集能够有效地区分解出音素级和词汇级的语义信息，同时保持语音的自然度和可解释性，并在解耦和重建质量上超越了传统单级基线。这表明HAC在构建统一离散语音表示方面具有巨大潜力，有助于推动下游语音生成和理解任务的发展。", "keywords": "语音分词, 分层音频编解码器, 解耦表示, 知识蒸馏, 语音编解码", "comments": "本文的创新之处在于提出了一个统一的神经语音编解码器HAC，通过将语音瓶颈分解为多层次的语言信息（声学、语音、词汇）并结合多源知识蒸馏，成功实现了语音表示的解耦。这种分层和解耦的策略对于语音生成和理解任务具有重要意义，因为它能够提供更精细、更具解释性的离散语音表示。该方法在实验中表现出优异的性能，有望成为未来语音处理领域的基础技术。"}}
{"id": "2506.14864", "title": "pycnet-audio: A Python package to support bioacoustics data processing", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that\nleverages recent improvements in purpose-made automated recording units (ARUs).\nThe general approach is to deploy ARUs in the field to record on a programmed\nschedule for extended periods (weeks or months), after which the audio data are\nretrieved. These data must then be processed, typically either by measuring or\nanalyzing characteristics of the audio itself (e.g. calculating acoustic\nindices), or by searching for some signal of interest within the recordings,\ne.g. vocalizations or other sounds produced by some target species,\nanthropogenic or environmental noise, etc. In the latter case, some method is\nrequired to locate the signal(s) of interest within the audio. While very small\ndatasets can simply be searched manually, even modest projects can produce\naudio datasets on the order of 105 hours of recordings, making manual review\nimpractical and necessitating some form of automated detection. pycnet-audio\n(Ruff 2024) is intended to provide a practical processing workflow for acoustic\ndata, built around the PNW-Cnet model, which was initially developed by the\nU.S. Forest Service to support population monitoring of northern spotted owls\n(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins\n2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of\nca. 80 forest wildlife species and numerous forms of anthropogenic and\nenvironmental noise (Ruff et al. 2021, 2023).", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.14864v1", "AI": {"title_translation": "pycnet-audio: 一个支持生物声学数据处理的Python包", "tldr": "pycnet-audio是一个基于PNW-Cnet模型的Python包，旨在为生物声学数据处理提供实用的工作流程，尤其适用于大规模被动声学监测数据。", "motivation": "被动声学监测在野生动物研究中日益重要，但其产生的大量音频数据（可达10^5小时）使得手动处理变得不切实际，因此需要自动化检测方法来定位感兴趣的信号。", "method": "pycnet-audio是一个Python包，它围绕PNW-Cnet模型构建。PNW-Cnet模型最初由美国林务局开发，用于监测北方斑点猫头鹰和其他森林猫头鹰，现已扩展到可检测约80种森林野生动物的叫声以及多种人为和环境噪音。pycnet-audio旨在提供一个实用的声学数据处理工作流程。", "result": "pycnet-audio提供了一个处理声学数据的实用工作流程，利用PNW-Cnet模型，该模型已能检测约80种森林野生动物的叫声以及多种人为和环境噪音。", "conclusion": "pycnet-audio提供了一个针对大规模生物声学数据处理的自动化解决方案，解决了手动分析不可行的问题，并通过整合PNW-Cnet模型，提升了野生动物声学监测的效率和范围。", "translation": "被动声学监测是野生动物研究中一种新兴方法，它利用了专用自动化录音设备（ARU）的最新改进。通常的做法是在野外部署ARU，按预设时间表长时间（数周或数月）录音，然后检索音频数据。这些数据随后必须进行处理，通常是通过测量或分析音频本身的特征（例如计算声学指标），或者在录音中搜索某些感兴趣的信号，例如某些目标物种发出的叫声或其他声音，人为或环境噪音等。在后一种情况下，需要某种方法来定位音频中感兴趣的信号。虽然非常小的数据集可以简单地手动搜索，但即使是适度的项目也能产生大约10^5小时的音频数据集，这使得手动审查变得不切实际，因此需要某种形式的自动化检测。pycnet-audio（Ruff 2024）旨在为声学数据提供一个实用的处理工作流程，该工作流程围绕PNW-Cnet模型构建，PNW-Cnet模型最初由美国林务局开发，用于支持北方斑点猫头鹰（Strix occidentalis caurina）和其他森林猫头鹰的种群监测（Lesmeister and Jenkins 2022; Ruff et al. 2020）。PNW-Cnet已扩展到可检测约80种森林野生动物的叫声以及多种人为和环境噪音（Ruff et al. 2021, 2023）。", "summary": "pycnet-audio是一个Python包，旨在为大规模生物声学数据处理提供实用的自动化工作流程。鉴于被动声学监测产生的大量数据使得手动分析不可行，该包整合了最初为北方斑点猫头鹰监测开发的PNW-Cnet模型，并已扩展到可检测约80种森林野生动物叫声及多种噪音，有效解决了野生动物声学监测中的数据处理挑战。", "keywords": "生物声学, 被动声学监测, Python包, PNW-Cnet, 自动化检测", "comments": "该论文介绍的pycnet-audio包解决了被动声学监测中数据量巨大导致手动处理效率低下的痛点。其创新之处在于将经过验证的PNW-Cnet模型封装成易于使用的Python包，极大地降低了生物声学研究人员进行大规模数据分析的门槛。该工具的重要性在于它能够自动化识别多种野生动物叫声和噪音，从而提高监测效率和准确性，对生态学和保护生物学研究具有重要意义。"}}
{"id": "2506.15043", "title": "Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures", "authors": ["Amir Hossein Baradaran"], "summary": "Advancements in the defense industry are paramount for ensuring the safety\nand security of nations, providing robust protection against emerging threats.\nAmong these threats, hypersonic missiles pose a significant challenge due to\ntheir extreme speeds and maneuverability, making accurate trajectory prediction\na critical necessity for effective countermeasures. This paper addresses this\nchallenge by employing a novel hybrid deep learning approach, integrating\nConvolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks,\nand Gated Recurrent Units (GRUs). By leveraging the strengths of these\narchitectures, the proposed method successfully predicts the complex\ntrajectories of hypersonic missiles with high accuracy, offering a significant\ncontribution to defense strategies and missile interception technologies. This\nresearch demonstrates the potential of advanced machine learning techniques in\nenhancing the predictive capabilities of defense systems.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15043v1", "AI": {"title_translation": "基于CNN-LSTM-GRU架构的高超音速导弹轨迹高级预测", "tldr": "本文提出了一种结合CNN、LSTM和GRU的混合深度学习方法，以高精度预测高超音速导弹的复杂轨迹，增强了防御系统的预测能力。", "motivation": "高超音速导弹因其极高的速度和机动性对国家安全构成重大挑战，因此准确的轨迹预测对于有效的反制措施至关重要。", "method": "本文采用了一种新颖的混合深度学习方法，整合了卷积神经网络（CNNs）、长短期记忆（LSTM）网络和门控循环单元（GRUs）来预测高超音速导弹的轨迹。", "result": "所提出的方法成功地以高精度预测了高超音速导弹的复杂轨迹。", "conclusion": "这项研究为防御策略和导弹拦截技术做出了重大贡献，并展示了先进机器学习技术在增强防御系统预测能力方面的潜力。", "translation": "国防工业的进步对于确保国家安全至关重要，为应对新兴威胁提供强大的保护。在这些威胁中，高超音速导弹因其极高的速度和机动性构成了重大挑战，使得准确的轨迹预测成为有效反制措施的关键必要条件。本文通过采用一种新颖的混合深度学习方法来解决这一挑战，该方法整合了卷积神经网络（CNN）、长短期记忆（LSTM）网络和门控循环单元（GRU）。通过利用这些架构的优势，所提出的方法成功地以高精度预测了高超音速导弹的复杂轨迹，为防御策略和导弹拦截技术做出了重大贡献。这项研究展示了先进机器学习技术在增强防御系统预测能力方面的潜力。", "summary": "本研究提出了一种创新的混合深度学习模型，结合了CNN、LSTM和GRU，旨在解决高超音速导弹轨迹预测的挑战。该方法能够高精度地预测复杂的高超音速导弹轨迹，对国防战略和导弹拦截技术具有重要意义，并展示了机器学习在提升防御系统预测能力方面的应用潜力。", "keywords": "高超音速导弹, 轨迹预测, 深度学习, CNN-LSTM-GRU, 国防安全", "comments": "该论文通过整合CNN、LSTM和GRU构建混合深度学习架构，提供了一种新颖且强大的高超音速导弹轨迹预测方案，其创新性在于融合不同网络优势以处理复杂时空数据。这对于提升国家防御能力和反制新兴威胁具有重要意义。"}}
{"id": "2506.14968", "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization", "authors": ["Rajat Kumar Jenamani", "Tom Silver", "Ben Dodson", "Shiqin Tong", "Anthony Song", "Yuting Yang", "Ziang Liu", "Benjamin Howe", "Aimee Whitneck", "Tapomayukh Bhattacharjee"], "summary": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast", "comment": "RSS 2025 - Outstanding Paper Award & Outstanding Systems Paper Award\n  Finalist", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14968v1", "AI": {"title_translation": "FEAST: 一种面向野外个性化的灵活进餐辅助系统", "tldr": "FEAST是一个灵活的进餐辅助机器人系统，通过模块化硬件、多样化交互和参数化行为树实现野外个性化，并在用户研究中表现出色。", "motivation": "物理护理机器人有望改善全球数百万需要喂食辅助的人的生活质量。然而，由于活动多样性、上下文、食物种类和用户偏好，家庭进餐辅助仍然具有挑战性。", "method": "提出FEAST系统，一个灵活的进餐辅助系统，可以在实际部署中进行个性化以满足个体护理接受者的独特需求。系统基于适应性、透明性和安全性三个原则，通过以下方式实现：i) 模块化硬件支持喂食、饮水和擦嘴；ii) 多样化交互方法（网络界面、头部手势、物理按钮）；iii) 参数化行为树，可通过大型语言模型安全透明地调整。", "result": "系统评估表明FEAST提供了广泛的透明和安全适应性，并优于固定定制的现有基线。在与两位护理接受者的家庭用户研究中，用户成功个性化FEAST以满足其需求。职业治疗师的评估也证实了其生态有效性。", "conclusion": "FEAST系统能够成功地在实际环境中进行个性化，以满足需要进餐辅助的个体用户的独特需求和偏好。", "translation": "物理护理机器人有望改善全球数百万需要喂食辅助的人的生活质量。然而，由于活动（例如，进食、饮水、擦嘴）、环境（例如，社交、看电视）、食物种类和用户偏好的多样性，家庭进餐辅助仍然具有挑战性。在这项工作中，我们提出了FEAST，一个灵活的进餐辅助系统，可以在实际部署中进行个性化，以满足个体护理接受者的独特需求。该系统与两位社区研究人员合作开发，并参考了对不同护理接受者群体进行的形成性研究，以野外个性化的三个关键原则为指导：适应性、透明性、和安全性。FEAST通过以下方式体现了这些原则：(i) 模块化硬件，可在辅助喂食、饮水和擦嘴之间切换，(ii) 多样化的交互方法，包括网页界面、头部手势和物理按钮，以适应不同的功能能力和偏好，以及(iii) 参数化行为树，可以通过大型语言模型进行安全透明的调整。我们根据形成性研究中确定的个性化要求对系统进行了评估，结果表明FEAST提供了广泛的透明和安全适应性，并且优于仅限于固定定制的现有基线。为了展示实际应用性，我们与两位护理接受者（他们是社区研究人员）进行了家庭用户研究，在三种不同场景下为他们提供了三餐。我们通过与一位以前不熟悉该系统的职业治疗师进行评估，进一步评估了FEAST的生态有效性。在所有情况下，用户都成功地个性化了FEAST以满足他们的个人需求和偏好。网站：https://emprise.cs.cornell.edu/feast", "summary": "本文提出FEAST，一个旨在为需要进餐辅助的个体提供野外个性化服务的灵活机器人系统。FEAST通过模块化硬件、多样化交互方式和可由大型语言模型调整的参数化行为树实现适应性、透明性和安全性。系统经过评估，证明其在提供个性化适应性方面优于现有基线，并通过家庭用户研究和职业治疗师评估，验证了其在实际应用中的有效性，成功满足了用户的个性化需求。", "keywords": "进餐辅助系统, 个性化, 机器人, 护理, 行为树", "comments": "本文提出了一种创新的、面向实际部署的进餐辅助机器人系统FEAST，其核心创新在于强调“野外个性化”并设计了实现这一目标的三个关键原则（适应性、透明性、安全性）。通过模块化硬件、多模态交互和结合LLM的参数化行为树，该系统展现了高度的灵活性和用户定制能力，克服了现有系统在多样化家庭环境中的局限性。其与社区研究人员合作并进行实际家庭用户研究的设计，增强了研究的实际意义和普适性。"}}
{"id": "2506.15098", "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15098v1", "AI": {"title_translation": "增强报告批准预测：大型语言模型的比较研究", "tldr": "本研究比较了大型语言模型（LLM）与传统方法在增强报告批准预测（ERAP）中的表现，发现LLM特别是LoRA微调的Llama 3.1 8B Instruct模型，显著优于传统方法，提高了预测准确性和召回率。", "motivation": "人工处理增强报告（ERs）耗费资源且效率低下，可能导致延误和有价值见解的丢失。因此，需要自动化增强报告批准预测（ERAP）以提高效率和决策质量。", "method": "本研究系统评估了18种大型语言模型（LLM）变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA、XLNet等编码器模型，以及GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct、DeepSeek-V3等解码器模型），并将其与传统方法（CNN/LSTM-BERT/GloVe）进行了比较。实验中还探讨了引入创建者资料和LoRA微调的影响。", "result": "1. 引入创建者资料使未经微调的纯解码器模型的整体准确率提高了10.8%，尽管可能引入偏差。2. 经LoRA微调的Llama 3.1 8B Instruct模型表现进一步提升，准确率达到79%，批准报告的召回率显著提高（76.1% vs. LSTM-GLOVE的64.1%），在严格的时间顺序评估下，其性能比传统方法高出5%，并有效解决了类别不平衡问题。这些发现确立了LLM作为ERAP的更优解决方案。", "conclusion": "大型语言模型（LLM）是增强报告批准预测（ERAP）的卓越解决方案，能够简化软件维护工作流程并改善实际开发环境中的决策。研究还总结了大型模型表现不佳的ER案例，为未来研究提供了方向。", "translation": "增强报告（ERs）是用户和开发人员之间关键的沟通渠道，捕获了对软件改进的宝贵建议。然而，手动处理这些报告是资源密集型的，导致延迟和潜在的宝贵见解损失。为了解决这一挑战，增强报告批准预测（ERAP）已成为一个研究焦点，利用机器学习技术实现决策自动化。虽然传统方法采用了基于特征的分类器和深度学习模型，但大型语言模型（LLM）的最新进展为提高预测准确性带来了新的机会。本研究系统评估了18种LLM变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA和XLNet等编码器模型；GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct和DeepSeek-V3等解码器模型）与传统方法（CNN/LSTM-BERT/GloVe）的性能。我们的实验揭示了两个关键见解：（1）纳入创建者资料使未经微调的纯解码器模型的整体准确率提高了10.8%，尽管这可能引入偏差；（2）经LoRA微调的Llama 3.1 8B Instruct进一步提高了性能，准确率达到79%，并显著提高了批准报告的召回率（76.1% vs. LSTM-GLOVE的64.1%），在严格的时间顺序评估下，其性能比传统方法高出5%，并有效解决了类别不平衡问题。这些发现确立了LLM作为ERAP的更优解决方案，展示了它们简化软件维护工作流程和改善实际开发环境中决策的潜力。我们还调查并总结了大型模型表现不佳的ER案例，为未来的研究提供了有价值的方向。", "summary": "本研究旨在解决软件增强报告（ERs）手动处理效率低下问题，通过比较大型语言模型（LLM）与传统机器学习方法在增强报告批准预测（ERAP）中的性能。研究评估了18种LLM变体，发现引入创建者资料可提高未经微调解码器模型的准确性，而LoRA微调的Llama 3.1 8B Instruct模型表现最佳，准确率达79%，且在召回率和处理类别不平衡方面显著优于传统方法。研究结果表明LLM是ERAP的优越解决方案，有望优化软件维护流程。", "keywords": "增强报告批准预测, 大型语言模型, 比较研究, 软件维护", "comments": "这项研究创新性地将大型语言模型应用于增强报告批准预测，并通过系统的比较研究证明了其优越性。特别值得注意的是，它不仅验证了LLM的潜力，还深入探讨了微调策略（如LoRA）和特征工程（如创建者资料）对性能的影响。研究还指出了LLM在特定ER案例中表现不佳的情况，为未来的研究提供了明确的方向，这对于推动该领域的进步具有重要意义。"}}
{"id": "2506.14809", "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model", "authors": ["Peng Jiang", "Vinicius Cezar Monteiro de Lira", "Antonio Maiorino"], "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating\nhigh-quality surveys remains labor-intensive, requiring both domain expertise\nand methodological rigor. With the evolution of large language models (LLMs),\nnew opportunities emerge to automate survey generation. This paper presents the\nreal-world deployment of an LLM-powered system designed to accelerate data\ncollection while maintaining survey quality. Deploying such systems in\nproduction introduces real-world complexity, including diverse user needs and\nquality control. We evaluate the system using the DeLone and McLean IS Success\nModel to understand how generative AI can reshape a core IS method. This study\nmakes three key contributions. To our knowledge, this is the first application\nof the IS Success Model to a generative AI system for survey creation. In\naddition, we propose a hybrid evaluation framework combining automated and\nhuman assessments. Finally, we implement safeguards that mitigate\npost-deployment risks and support responsible integration into IS workflows.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14809v1", "AI": {"title_translation": "通过IS成功模型评估已部署LLM问卷创建工具的影响", "tldr": "本文评估了一个部署的LLM驱动的问卷创建系统，使用IS成功模型，并提出了一个混合评估框架和风险缓解措施，旨在加速数据收集并保持问卷质量。", "motivation": "问卷创建在信息系统（IS）研究中是劳动密集型的，需要专业知识和方法严谨性。随着大型语言模型（LLM）的发展，出现了自动化问卷生成的新机会，因此需要评估LLM驱动的系统在实际部署中的效果。", "method": "本文部署了一个由LLM驱动的问卷创建系统，旨在加速数据收集并保持问卷质量。该系统使用DeLone和McLean IS成功模型进行评估，以理解生成式AI如何重塑核心IS方法。研究还提出了一个结合自动化和人工评估的混合评估框架，并实施了减轻部署后风险的保障措施。", "result": "这是首次将IS成功模型应用于生成式AI问卷创建系统。研究提出了一个结合自动化和人工评估的混合评估框架。研究实施了减轻部署后风险的保障措施，支持负责任地整合到IS工作流程中。", "conclusion": "通过将IS成功模型应用于一个部署的LLM问卷创建工具，并结合混合评估框架和风险缓解措施，本研究为理解生成式AI如何重塑信息系统方法提供了见解，并支持其负责任地集成到工作流程中。", "translation": "问卷是信息系统（IS）研究的基石，然而创建高质量的问卷仍然是劳动密集型的工作，需要领域专业知识和方法严谨性。随着大型语言模型（LLM）的演进，自动化问卷生成的新机会应运而生。本文介绍了一个LLM驱动系统在现实世界中的部署，该系统旨在加速数据收集同时保持问卷质量。在生产环境中部署此类系统引入了现实世界的复杂性，包括多样化的用户需求和质量控制。我们使用DeLone和McLean IS成功模型评估了该系统，以了解生成式AI如何重塑核心IS方法。这项研究做出了三个关键贡献。据我们所知，这是IS成功模型首次应用于生成式AI问卷创建系统。此外，我们提出了一个结合自动化和人工评估的混合评估框架。最后，我们实施了保障措施，以减轻部署后风险并支持负责任地整合到IS工作流程中。", "summary": "本文评估了一个已部署的LLM驱动问卷创建系统对信息系统（IS）研究的影响。该系统旨在通过自动化提高问卷创建效率和质量。研究使用DeLone和McLean IS成功模型对系统进行评估，并提出了一个结合自动化和人工评估的混合评估框架，同时实施了风险缓解措施，以确保LLM工具在IS工作流程中的负责任集成。", "keywords": "LLM, 问卷创建, IS成功模型, 生成式AI, 混合评估", "comments": "本文的创新点在于首次将IS成功模型应用于生成式AI系统，特别是针对问卷创建。它不仅关注技术部署，还强调了实际应用中的复杂性、用户需求、质量控制以及负责任的集成，并通过提出混合评估框架和风险缓解措施来应对这些挑战，这对于AI工具在关键领域如IS研究中的实际落地具有重要指导意义。"}}
{"id": "2506.15440", "title": "Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration", "authors": ["Omar Numan", "Gaurav Singh", "Kazybek Adam", "Jelin Leslin", "Aleksi Korsman", "Otto Simola", "Marko Kosunen", "Jussi Ryynänen", "Martin Andraud"], "summary": "Developing accurate and reliable Compute-In-Memory (CIM) architectures is\nbecoming a key research focus to accelerate Artificial Intelligence (AI) tasks\non hardware, particularly Deep Neural Networks (DNNs). In that regard, there\nhas been significant interest in analog and mixed-signal CIM architectures\naimed at increasing the efficiency of data storage and computation to handle\nthe massive amount of data needed by DNNs. Specifically, resistive mixed-signal\nCIM cores are pushed by recent progresses in emerging Non-Volatile Memory\n(eNVM) solutions. Yet, mixed-signal CIM computing cores still face several\nintegration and reliability challenges that hinder their large-scale adoption\ninto end-to-end AI computing systems. In terms of integration, resistive and\neNVM-based CIM cores need to be integrated with a control processor to realize\nend-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still\nmore efficient and easier to program than their eNVM counterparts. In terms of\nreliability, analog circuits are more susceptible to variations, leading to\ncomputation errors and degraded accuracy. This work addresses these two\nchallenges by proposing a self-calibrated mixed-signal CIM accelerator SoC,\nfabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the\nCIM architecture, combining the density and ease of SRAM-based weight storage\nwith multi-bit computation using linear resistors, and (2) an open-source\nprogramming and testing strategy for CIM systems. The accuracy and reliability\nare enabled through an automated RISC-V controlled on-chip calibration,\nallowing us to improve the compute SNR by 25 to 45% across multiple columns to\nreach 18-24 dB. To showcase further integration possibilities, we show how our\nproof-of-concept SoC can be extended to recent high-density linear resistor\ntechnologies for enhanced computing performance.", "comment": "This work has been submitted to the IEEE for possible publication. 12\n  pages, 10 figures, 2 tables", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15440v1", "AI": {"title_translation": "Acore-CIM：构建基于RISC-V控制自校准的精确可靠混合信号CIM核心", "tldr": "Acore-CIM提出了一种基于RISC-V控制自校准的混合信号CIM加速器SoC，旨在解决现有CIM在集成和可靠性方面的挑战，并显著提高计算精度。", "motivation": "现有混合信号存内计算（CIM）核心面临集成和可靠性挑战，阻碍其在大规模端到端人工智能（AI）计算系统中的广泛应用。具体来说，它们需要与控制处理器集成，且模拟电路易受变异影响导致计算错误和精度下降。", "method": "本研究提出Acore-CIM，一个自校准混合信号CIM加速器SoC，采用22纳米FDSOI技术制造。它通过结合SRAM基权重存储的密度和易用性与线性电阻的多位计算能力来促进集成，并采用开源编程和测试策略。精度和可靠性通过自动化的RISC-V控制片上校准实现。", "result": "该方法将计算信噪比（SNR）提高了25%至45%，达到18-24 dB。此外，概念验证SoC可扩展到高密度线性电阻技术，以增强计算性能。", "conclusion": "Acore-CIM成功解决了混合信号CIM核心的集成和可靠性挑战，通过自校准机制显著提高了计算精度，并展示了其在大规模AI加速系统中的潜力。", "translation": "开发精确可靠的存内计算（CIM）架构正成为加速硬件上人工智能（AI）任务，特别是深度神经网络（DNNs）的关键研究重点。在这方面，模拟和混合信号CIM架构引起了极大的兴趣，旨在提高数据存储和计算的效率，以处理DNN所需的大量数据。具体而言，新兴非易失性存储器（eNVM）解决方案的最新进展推动了电阻式混合信号CIM核心的发展。然而，混合信号CIM计算核心仍面临若干集成和可靠性挑战，阻碍了它们在大规模端到端AI计算系统中的广泛采用。在集成方面，基于电阻和eNVM的CIM核心需要与控制处理器集成，以实现端到端AI加速。此外，基于SRAM的CIM架构仍然比其eNVM对应物更高效且更容易编程。在可靠性方面，模拟电路更容易受到变化的影响，导致计算错误和精度下降。这项工作通过提出一种在22纳米FDSOI技术中制造的自校准混合信号CIM加速器SoC来解决这两个挑战。其集成通过（1）结合SRAM基权重存储的密度和易用性与使用线性电阻的多位计算的CIM架构，以及（2）用于CIM系统的开源编程和测试策略来实现。通过自动化的RISC-V控制片上校准实现了精度和可靠性，使我们能够将多个列的计算信噪比提高25%至45%，达到18-24 dB。为了展示进一步的集成可能性，我们展示了我们的概念验证SoC如何扩展到最近的高密度线性电阻技术，以增强计算性能。", "summary": "Acore-CIM是一种混合信号存内计算（CIM）加速器SoC，旨在解决现有CIM架构在集成和可靠性方面的挑战，以更好地支持人工智能任务。该SoC采用22纳米FDSOI技术制造，结合了SRAM的存储优势和线性电阻的多位计算能力，并通过RISC-V控制的片上自校准来提高精度和可靠性。实验结果显示，其计算信噪比提高了25%至45%，达到18-24 dB，并展示了与高密度线性电阻技术的良好兼容性。", "keywords": "存内计算, 混合信号, 自校准, RISC-V, AI加速", "comments": "该论文的创新之处在于结合了SRAM基权重存储与线性电阻的多位计算，并实现了由RISC-V控制的片上自校准机制。这直接解决了混合信号CIM在实际AI硬件加速中的关键限制（集成和可靠性/精度问题）。采用开源策略也有助于推广其应用。其重要性在于通过提高鲁棒性和易于集成性，推动混合信号CIM更接近大规模部署。"}}
{"id": "2506.15127", "title": "New Constructions of Full Flag Codes Based on Partial Spreads", "authors": ["Xiang Han", "Xinran Li", "Gang Wang"], "summary": "Flag codes are a class of multishot network codes comprising sequences of\nnested subspaces (flags) within the vector space $\\mathbb{F}_q^n$, where $q$ is\na prime power. In this paper, we propose a family of constructions for full\nflag codes based on partial spreads. The distances of this family include\nmaximum distance (optimum distance flag codes), second-maximum distance\n(quasi-optimum distance flag codes), as well as other feasible values. The\nstructure of these flag codes resembles that of a \\textquotedblleft sandwich\",\nconsisting of one layer of companion matrix and two layers of partial spreads.\nFurthermore, we present an efficient decoding algorithm for these codes.", "comment": "24 pages", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15127v1", "AI": {"title_translation": "基于部分扩形的完全旗码的新构造", "tldr": "本文提出了一种基于部分扩形构建完全旗码的新方法，这些旗码具有最优或准最优距离，并提供了一种高效的解码算法。", "motivation": "本文旨在提出一种新的完全旗码构造方法，以扩展旗码的构造家族，并实现不同的距离特性，包括最大距离和次最大距离。", "method": "本文提出了一种基于部分扩形构建完全旗码的家族构造方法。这些旗码的结构类似于“三明治”，由一层伴随矩阵和两层部分扩形组成。此外，还提出了一种高效的解码算法。", "result": "所提出的旗码家族的距离包括最大距离（最优距离旗码）、次最大距离（准最优距离旗码）以及其他可行值。同时，为这些代码提供了一种高效的解码算法。", "conclusion": "本文成功地基于部分扩形构造了一系列具有不同距离特性的完全旗码，包括最优和准最优距离，并为它们开发了高效的解码算法。", "translation": "旗码是一类多发射网络码，由向量空间 $\\mathbb{F}_q^n$ 中的嵌套子空间序列（旗）组成，其中 $q$ 是素数幂。在本文中，我们提出了一种基于部分扩形构建完全旗码的家族构造方法。该家族的距离包括最大距离（最优距离旗码）、次最大距离（准最优距离旗码）以及其他可行值。这些旗码的结构类似于“三明治”，由一层伴随矩阵和两层部分扩形组成。此外，我们还为这些代码提出了一种高效的解码算法。", "summary": "本文提出了一种基于部分扩形的新型完全旗码构造方法。这些旗码的结构独特，呈“三明治”状，由伴随矩阵和两层部分扩形构成。该构造能够实现包括最大距离和次最大距离在内的多种距离值。此外，论文还提供了一种高效的解码算法。", "keywords": "旗码, 部分扩形, 网络码, 解码算法, 最优距离", "comments": "本文的创新之处在于提出了一种新颖的“三明治”结构来构造完全旗码，并利用部分扩形实现了多种距离特性，包括最优和准最优距离。同时，提供高效的解码算法增强了其实用性，对网络编码领域具有重要意义。"}}
{"id": "2506.15114", "title": "Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library", "authors": ["Youjia Li", "Robert Latham", "Robert Ross", "Ankit Agrawal", "Alok Choudhary", "Wei-Keng Liao"], "summary": "High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by\nlarge-scale scientific applications to perform I/O tasks in parallel. These I/O\nlibraries store the metadata such as data types and dimensionality along with\nthe raw data in the same files. While these libraries are well-optimized for\nconcurrent access to the raw data, they are designed neither to handle a large\nnumber of data objects efficiently nor to create different data objects\nindependently by multiple processes, as they require applications to call data\nobject creation APIs collectively with consistent metadata among all processes.\nApplications that process data gathered from remote sensors, such as particle\ncollision experiments in high-energy physics, may generate data of different\nsizes from different sensors and desire to store them as separate data objects.\nFor such applications, the I/O library's requirement on collective data object\ncreation can become very expensive, as the cost of metadata consistency check\nincreases with the metadata volume as well as the number of processes. To\naddress this limitation, using PnetCDF as an experimental platform, we\ninvestigate solutions in this paper that abide the netCDF file format, as well\nas propose a new file header format that enables independent data object\ncreation. The proposed file header consists of two sections, an index table and\na list of metadata blocks. The index table contains the reference to the\nmetadata blocks and each block stores metadata of objects that can be created\ncollectively or independently. The new design achieves a scalable performance,\ncutting data object creation times by up to 582x when running on 4096 MPI\nprocesses to create 5,684,800 data objects in parallel. Additionally, the new\nmethod reduces the memory footprints, with each process requiring an amount of\nmemory space inversely proportional to the number of processes.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15114v1", "AI": {"title_translation": "并行数据对象创建：迈向高性能I/O库中可扩展元数据管理", "tldr": "本文提出了一种新的文件头格式，以解决高性能I/O库中并行数据对象创建的元数据管理瓶颈，实现了显著的性能提升和内存占用减少。", "motivation": "现有的高性能I/O库（如HDF5和PnetCDF）在处理大量数据对象或允许多进程独立创建数据对象时效率低下，因为它们要求所有进程集体调用API并保持元数据一致性，这在处理来自不同传感器、大小各异的数据时成本高昂。", "method": "以PnetCDF作为实验平台，研究了符合netCDF文件格式的解决方案，并提出了一种新的文件头格式。该文件头包含一个索引表和元数据块列表，其中索引表引用元数据块，每个块存储可集体或独立创建的对象的元数据。", "result": "新设计实现了可扩展的性能，在4096个MPI进程并行创建5,684,800个数据对象时，数据对象创建时间缩短了582倍。此外，新方法减少了内存占用，每个进程所需的内存空间与进程数成反比。", "conclusion": "通过引入新的文件头格式，实现了高性能I/O库中可扩展的并行数据对象创建，显著提升了性能并优化了内存使用。", "translation": "高级I/O库，如HDF5和PnetCDF，常被大规模科学应用用于并行执行I/O任务。这些I/O库将数据类型和维度等元数据与原始数据存储在同一文件中。尽管这些库针对原始数据的并发访问进行了优化，但它们既不是为高效处理大量数据对象而设计的，也不是为了允许多个进程独立创建不同的数据对象而设计的，因为它们要求应用程序集体调用数据对象创建API，并要求所有进程之间元数据保持一致。处理来自远程传感器的数据（例如高能物理中的粒子碰撞实验）的应用可能会从不同传感器生成不同大小的数据，并希望将它们存储为独立的数据对象。对于此类应用，I/O库对集体数据对象创建的要求可能变得非常昂贵，因为元数据一致性检查的成本随元数据量和进程数的增加而增加。为解决这一限制，我们以PnetCDF作为实验平台，在本文中研究了符合netCDF文件格式的解决方案，并提出了一种新的文件头格式，该格式支持独立数据对象创建。所提出的文件头由两部分组成：一个索引表和一列元数据块。索引表包含对元数据块的引用，每个块存储可以集体或独立创建的对象的元数据。新设计实现了可扩展的性能，当在4096个MPI进程上并行创建5,684,800个数据对象时，数据对象创建时间缩短了高达582倍。此外，新方法减少了内存占用，每个进程所需的内存空间与进程数成反比。", "summary": "本文针对现有高性能I/O库在并行数据对象创建中元数据管理效率低下的问题，提出了一种新的文件头格式。该格式包含索引表和元数据块，支持独立或集体创建数据对象。实验证明，新方法在处理大量并行数据对象时，显著提升了创建速度（最高达582倍），并有效降低了内存占用，为大规模科学应用提供了更可扩展的I/O解决方案。", "keywords": "高性能I/O, 元数据管理, 并行数据对象创建, PnetCDF, 可扩展性", "comments": "本文创新性地提出了一个针对高性能I/O库中元数据管理瓶颈的解决方案，通过引入新的文件头格式，实现了数据对象的独立并行创建。其显著的性能提升和内存优化对于处理大规模、异构科学数据的应用具有重要意义，尤其是在高能物理等领域。"}}
{"id": "2506.14823", "title": "ViLLa: A Neuro-Symbolic approach for Animal Monitoring", "authors": ["Harsha Koduri"], "summary": "Monitoring animal populations in natural environments requires systems that\ncan interpret both visual data and human language queries. This work introduces\nViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for\ninterpretable animal monitoring. ViLLa integrates three core components: a\nvisual detection module for identifying animals and their spatial locations in\nimages, a language parser for understanding natural language queries, and a\nsymbolic reasoning layer that applies logic-based inference to answer those\nqueries. Given an image and a question such as \"How many dogs are in the\nscene?\" or \"Where is the buffalo?\", the system grounds visual detections into\nsymbolic facts and uses predefined rules to compute accurate answers related to\ncount, presence, and location. Unlike end-to-end black-box models, ViLLa\nseparates perception, understanding, and reasoning, offering modularity and\ntransparency. The system was evaluated on a range of animal imagery tasks and\ndemonstrates the ability to bridge visual content with structured,\nhuman-interpretable queries.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14823v1", "AI": {"title_translation": "ViLLa：一种用于动物监测的神经符号方法", "tldr": "ViLLa是一个神经符号框架，通过结合视觉检测、语言解析和符号推理，实现可解释的动物监测，能回答基于图像和自然语言的动物相关查询。", "motivation": "在自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。", "method": "引入ViLLa（Vision-Language-Logic Approach），一个神经符号框架。它整合了三个核心组件：用于识别图像中动物及其空间位置的视觉检测模块、用于理解自然语言查询的语言解析器，以及应用基于逻辑推理来回答这些查询的符号推理层。系统将视觉检测结果转化为符号事实，并使用预定义规则计算关于数量、存在和位置的准确答案。", "result": "该系统在一系列动物图像任务上进行了评估，并展示了将视觉内容与结构化、人类可解释查询连接起来的能力。", "conclusion": "ViLLa通过分离感知、理解和推理，提供了模块化和透明度，能够将视觉内容与结构化、人类可解释的查询联系起来，用于动物监测。", "translation": "在自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。这项工作介绍了ViLLa（视觉-语言-逻辑方法），一个专为可解释的动物监测设计的神经符号框架。ViLLa整合了三个核心组件：一个用于识别图像中动物及其空间位置的视觉检测模块，一个用于理解自然语言查询的语言解析器，以及一个应用基于逻辑推理来回答这些查询的符号推理层。给定一张图像和一个问题，例如“场景中有多少只狗？”或“水牛在哪里？”，系统将视觉检测结果转化为符号事实，并使用预定义规则计算与数量、存在和位置相关的准确答案。与端到端黑盒模型不同，ViLLa分离了感知、理解和推理，提供了模块化和透明度。该系统在一系列动物图像任务上进行了评估，并展示了将视觉内容与结构化、人类可解释查询连接起来的能力。", "summary": "ViLLa是一个创新的神经符号框架，专为可解释的动物监测设计。它结合了视觉检测、自然语言理解和符号推理，能够将图像中的动物信息转化为逻辑事实，并回答关于动物数量、存在和位置的自然语言查询。与传统黑盒模型不同，ViLLa通过分离感知、理解和推理来提供模块化和透明度，并已在动物图像任务中验证了其连接视觉内容与人类可解释查询的能力。", "keywords": "动物监测, 神经符号, 视觉语言, 符号推理, 可解释性", "comments": "这篇论文通过引入ViLLa框架，提出了一种新颖的神经符号方法来解决动物监测中视觉数据和语言查询的结合问题。其创新点在于将感知、理解和推理分离，提供了比端到端黑盒模型更高的模块化和透明度，这对于需要可解释性的实际应用尤为重要。"}}
{"id": "2506.15113", "title": "Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning", "authors": ["Min Namgung", "JangHyeon Lee", "Fangyi Ding", "Yao-Yi Chiang"], "summary": "Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15113v1", "AI": {"title_translation": "全民交通：利用区域表征学习绘制公平的自行车到地铁连接", "tldr": "本文提出了一个名为Transit for All (TFA) 的空间计算框架，通过预测冷启动站点的共享单车需求、评估加权公共交通可达性水平（wPTAL）并提供战略性站点布局建议，旨在指导共享单车系统公平扩展，以弥补低收入和少数族裔社区的交通可达性差距。", "motivation": "确保公共交通公平可达性面临挑战，尤其是在纽约市等人口稠密城市，低收入和少数族裔社区的交通可达性有限。共享单车系统（BSS）可以弥补这些差距，但战略性地将其扩展到服务不足的社区很困难，因为新规划的“冷启动”站点需求不确定，且传统可达性指标可能忽略实际的自行车使用潜力。", "method": "本文提出了一个名为Transit for All (TFA) 的空间计算框架，包含三个部分：1) 利用区域表征学习和多模态地理空间数据，对冷启动站点的共享单车需求进行空间预测；2) 结合预测的共享单车需求和传统交通可达性指标，利用新型加权公共交通可达性水平（wPTAL）进行综合交通可达性评估；3) 考虑潜在乘客量和公平性提升，提供新自行车站点布局的战略建议。", "result": "以纽约市为案例研究，本文识别出对历史上服务不足社区的低收入和少数族裔社区产生不成比例影响的交通可达性差距。研究结果表明，由wPTAL指导的战略性新站点布局显著减少了与经济和人口因素相关的交通可达性差异。", "conclusion": "本研究表明，TFA为城市规划者提供了实用指导，以促进公平交通并提高服务不足城市社区的生活质量。", "translation": "确保公平的公共交通可达性仍然充满挑战，特别是在纽约市（NYC）等人口稠密城市，低收入和少数族裔社区往往面临有限的交通可达性。共享单车系统（BSS）可以通过提供经济实惠的“第一英里”和“最后一英里”连接来弥合这些公平差距。然而，由于新规划的（“冷启动”）站点位置的共享单车需求不确定，以及传统可达性指标可能忽视实际自行车使用潜力，战略性地将BSS扩展到服务不足的社区很困难。我们引入了“全民交通”（TFA），这是一个空间计算框架，旨在通过三个组成部分指导BSS的公平扩展：(1) 利用区域表征学习整合多模态地理空间数据，对冷启动站点的共享单车需求进行空间信息预测；(2) 通过结合预测的共享单车需求和传统交通可达性指标，利用我们新颖的加权公共交通可达性水平（wPTAL）进行综合交通可达性评估；(3) 考虑潜在乘客量和公平性提升，对新自行车站点布局提出战略建议。以纽约市为案例研究，我们识别出对历史上服务不足社区的低收入和少数族裔社区产生不成比例影响的交通可达性差距。我们的研究结果表明，由wPTAL指导的战略性新站点布局显著减少了与经济和人口因素相关的交通可达性差异。从我们的研究中，我们证明TFA为城市规划者提供了实用指导，以促进公平交通并提高服务不足城市社区的生活质量。", "summary": "本研究提出“全民交通”（TFA）空间计算框架，旨在解决大城市中低收入和少数族裔社区的公共交通可达性不公平问题。该框架通过整合多模态地理空间数据和区域表征学习，预测“冷启动”共享单车站点的需求，并引入加权公共交通可达性水平（wPTAL）来全面评估交通可达性。通过在纽约市的案例研究，TFA证明能够识别交通差距，并指导共享单车站点布局以显著减少经济和人口因素导致的交通不平等，为城市规划提供实际工具以促进公平交通。", "keywords": "共享单车系统, 公平交通, 冷启动问题, 区域表征学习, 交通可达性", "comments": "本文创新性地将区域表征学习应用于共享单车需求预测，并提出了wPTAL这一新颖的交通可达性评估指标，有效解决了传统方法在“冷启动”问题和公平性评估上的局限性。其方法论结合了空间计算、数据驱动预测和公平性考量，为城市规划者提供了实用的决策支持，对于提升城市交通公平性和服务不足社区的生活质量具有重要意义。"}}
{"id": "2506.15105", "title": "Skew-Induced Insertion Loss Deviation (SILD) and FOM_SILD: Metrics for Quantifying P/N Skew Effects in High-Speed Channels", "authors": ["David Nozadze", "Zurab Kiguradze", "Amendra Koul", "Mike Sapozhnikov"], "summary": "The rise of AI workloads and growing data center demands have driven the need\nfor ultra-high-speed interconnects exceeding 200 Gb/s. As unit intervals (UI)\nshrink, even a few picoseconds of P/N skew can degrade serializer-deserializer\n(SerDes) performance. Traditional methods for quantifying skew fall short in\ncapturing its impact. We introduce two new metrics: 1) Skew-Induced Insertion\nLoss Deviation (SILD) and 2) its complementary Figure of Merit (FOM_SILD),\nanalytically developed to assess P/N skew effects. Measured S-parameters\nconfirm FOM_SILD reciprocity, while simulations of 224G PAM4 SerDes show strong\ncorrelation with bit error rate (BER) trends. This approach offers a robust\nframework for analyzing skew in next-generation ultra-high-speed interconnects.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15105v1", "AI": {"title_translation": "偏斜诱导插入损耗偏差 (SILD) 和 FOM_SILD：用于量化高速通道中P/N偏斜效应的指标", "tldr": "为解决高速互连中P/N偏斜对SerDes性能的影响，本文引入SILD和FOM_SILD两个新指标来量化偏斜效应，并验证其有效性。", "motivation": "AI工作负载的兴起和数据中心需求的增长推动了对超过200 Gb/s的超高速互连的需求。随着单位间隔 (UI) 的缩小，即使是几皮秒的P/N偏斜也会降低串行器-解串器 (SerDes) 的性能。传统的偏斜量化方法未能充分捕捉其影响。", "method": "本文分析性地开发了两种新指标：偏斜诱导插入损耗偏差 (SILD) 及其互补的品质因数 (FOM_SILD)，用于评估P/N偏斜效应。通过测量S参数确认FOM_SILD的互易性，并通过224G PAM4 SerDes仿真展示其与误码率 (BER) 趋势的强相关性。", "result": "测量的S参数证实了FOM_SILD的互易性。224G PAM4 SerDes仿真显示，这些新指标与误码率 (BER) 趋势具有很强的相关性。", "conclusion": "本文提出的方法为分析下一代超高速互连中的偏斜提供了一个稳健的框架。", "translation": "AI工作负载的兴起和数据中心需求的增长推动了对超过200 Gb/s的超高速互连的需求。随着单位间隔 (UI) 的缩小，即使是几皮秒的P/N偏斜也会降低串行器-解串器 (SerDes) 的性能。传统的偏斜量化方法未能充分捕捉其影响。我们引入了两个新指标：1) 偏斜诱导插入损耗偏差 (SILD) 和 2) 其互补的品质因数 (FOM_SILD)，它们经过分析开发，用于评估P/N偏斜效应。测量的S参数证实了FOM_SILD的互易性，而224G PAM4 SerDes的仿真结果显示与误码率 (BER) 趋势有很强的相关性。这种方法为分析下一代超高速互连中的偏斜提供了一个稳健的框架。", "summary": "本文针对超高速互连中P/N偏斜对SerDes性能的负面影响，提出了SILD和FOM_SILD两个新指标来量化偏斜效应。通过S参数测量和224G PAM4 SerDes仿真验证了这些指标的有效性及其与误码率的强相关性，为下一代高速互连的偏斜分析提供了新框架。", "keywords": "P/N偏斜, SILD, FOM_SILD, 高速互连, SerDes", "comments": "该论文创新性地提出了SILD和FOM_SILD两个新指标，解决了传统方法无法有效量化高速通道中P/N偏斜影响的问题。这些指标的提出对于设计和优化超高速互连，确保SerDes性能至关重要，为未来的AI和数据中心互连技术发展提供了有力的分析工具。"}}
{"id": "2506.15196", "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges", "authors": ["Xianliang Yang", "Ling Zhang", "Haolong Qian", "Lei Song", "Jiang Bian"], "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.", "comment": "27 pages,9 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15196v1", "AI": {"title_translation": "HeurAgenix：利用LLM解决复杂组合优化挑战", "tldr": "HeurAgenix是一个两阶段超启发式框架，利用大型语言模型（LLM）进化启发式算法并自动选择，在组合优化问题上表现优于现有方法。", "motivation": "传统启发式算法在解决组合优化（CO）问题时，设计严重依赖人工专业知识，并且难以泛化到不同的问题实例，这促使研究者寻求更通用和自动化的解决方案。", "method": "本文提出了HeurAgenix，一个由大型语言模型（LLM）驱动的两阶段超启发式框架。在启发式进化阶段，HeurAgenix利用LLM比较种子启发式解决方案和更高质量的解决方案，以提取可重用的进化策略。在问题解决阶段，它利用LLM的感知能力动态选择最合适的启发式算法。该选择器可以是先进的LLM，也可以是经过微调的轻量级模型。为解决可靠监督数据稀缺问题，轻量级启发式选择器通过一个双重奖励机制进行微调，该机制结合了选择偏好和状态感知的信号，以确保在嘈杂标注下的鲁棒选择。", "result": "在经典基准测试上的大量实验表明，HeurAgenix不仅优于现有的基于LLM的超启发式算法，而且其性能与专用求解器持平或超越。", "conclusion": "HeurAgenix成功地提供了一个通用且高效的框架，利用大型语言模型的能力自动化了启发式算法的进化和选择过程，显著提升了解决复杂组合优化问题的能力。", "translation": "启发式算法在解决组合优化 (CO) 问题中发挥着至关重要的作用，但传统设计严重依赖人工专业知识，并且难以在不同实例中泛化。我们引入了 HeurAgenix，一个由大型语言模型 (LLM) 驱动的两阶段超启发式框架，它首先进化启发式算法，然后自动从中进行选择。在启发式进化阶段，HeurAgenix 利用 LLM 比较种子启发式解决方案和更高质量的解决方案，并提取可重用的进化策略。在问题解决过程中，它在 LLM 的感知能力指导下，动态地为每个问题状态选择最有前途的启发式算法。为了灵活性，该选择器可以是最先进的 LLM 或经过微调的轻量级模型，具有较低的推理成本。为了缓解由 CO 复杂性导致的可靠监督稀缺问题，我们通过双重奖励机制对轻量级启发式选择器进行微调，该机制联合利用选择偏好和状态感知的信号，从而在嘈杂标注下实现鲁棒选择。在经典基准测试上的大量实验表明，HeurAgenix 不仅优于现有的基于 LLM 的超启发式算法，而且与专用求解器持平或超越。代码可在 https://github.com/microsoft/HeurAgenix 获取。", "summary": "HeurAgenix是一个创新的两阶段超启发式框架，旨在利用大型语言模型（LLM）解决复杂的组合优化问题。该框架通过LLM实现启发式算法的进化和动态选择，克服了传统方法对人工专业知识的依赖和泛化能力差的缺点。为应对监督数据稀缺问题，HeurAgenix采用双重奖励机制对轻量级选择器进行微调。实验结果表明，HeurAgenix在性能上优于现有基于LLM的超启发式方法，并达到了或超越了专用求解器。", "keywords": "组合优化, 大型语言模型, 超启发式, 启发式进化, 动态选择", "comments": "HeurAgenix的创新之处在于其将LLM引入超启发式算法的设计中，实现了启发式算法的自动化进化和选择，显著降低了对人工专业知识的依赖。其两阶段框架和双重奖励机制有效解决了组合优化问题中的泛化和监督数据稀缺挑战，为解决复杂CO问题提供了一个强大且通用的新范式。该研究展示了LLM在超越自然语言处理领域，赋能复杂决策优化方面的巨大潜力。"}}
{"id": "2506.15125", "title": "Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks", "authors": ["Linlin Wang", "Wei Wang", "Dezhao Wang", "Shanwen Wang"], "summary": "With the applicability of optical fiber-based distributed acoustic sensing\n(DAS) systems, effective signal processing and analysis approaches are needed\nto promote its popularization in the field of intelligent transportation\nsystems (ITS). This paper presents a signal denoising algorithm using a hybrid\ndeep-learning network (HDLNet). Without annotated data and time-consuming\nlabeling, this self-supervised network runs in parallel, combining an\nautoencoder for denoising (DAE) and a long short-term memory (LSTM) for\nsequential processing. Additionally, a line-by-line matching algorithm for\nvehicle detection and tracking is introduced, thus realizing the complete\nprocessing of fiber signal denoising and feature extraction. Experiments were\ncarried out on a self-established real highway tunnel dataset, showing that our\nproposed hybrid network yields more satisfactory denoising performance than\nSpatial-domain DAE.", "comment": "15 pages, 10 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15125v1", "AI": {"title_translation": "光纤信号去噪算法基于混合深度学习网络", "tldr": "本文提出一种结合DAE和LSTM的混合深度学习网络（HDLNet），用于光纤信号去噪，无需标注数据，并在真实高速公路隧道数据上表现优于现有方法。", "motivation": "为了推广光纤分布式声学传感（DAS）系统在智能交通系统（ITS）领域的应用，需要有效的信号处理和分析方法，特别是解决光纤信号的去噪问题。", "method": "提出了一种使用混合深度学习网络（HDLNet）的信号去噪算法。该自监督网络并行运行，结合了用于去噪的自编码器（DAE）和用于序列处理的长短期记忆网络（LSTM）。此外，引入了一种逐行匹配算法，用于车辆检测和跟踪，从而实现了光纤信号去噪和特征提取的完整处理。", "result": "在自建的真实高速公路隧道数据集上进行的实验表明，所提出的混合网络比空间域DAE产生了更令人满意的去噪性能。", "conclusion": "该混合深度学习网络（HDLNet）能够有效进行光纤信号去噪，并结合逐行匹配算法实现车辆检测和跟踪，为光纤DAS系统在ITS领域的应用提供了有效的信号处理方案。", "translation": "随着光纤分布式声学传感（DAS）系统的适用性，需要有效的信号处理和分析方法来促进其在智能交通系统（ITS）领域的普及。本文提出了一种使用混合深度学习网络（HDLNet）的信号去噪算法。该自监督网络无需标注数据和耗时标记，并行运行，结合了用于去噪的自编码器（DAE）和用于序列处理的长短期记忆网络（LSTM）。此外，还引入了一种逐行匹配算法用于车辆检测和跟踪，从而实现了光纤信号去噪和特征提取的完整处理。实验在自建的真实高速公路隧道数据集上进行，结果表明我们提出的混合网络比空间域DAE产生了更令人满意的去噪性能。", "summary": "本文针对光纤分布式声学传感（DAS）系统在智能交通系统（ITS）中的应用需求，提出了一种基于混合深度学习网络（HDLNet）的自监督光纤信号去噪算法。该网络结合了DAE和LSTM，并辅以逐行匹配算法实现车辆检测与跟踪。实验证明，该方法在真实高速公路隧道数据集上表现出优于传统空间域DAE的去噪性能，实现了光纤信号去噪和特征提取的完整流程。", "keywords": "光纤信号去噪, 混合深度学习, 自监督学习, 分布式声学传感, 智能交通系统", "comments": "该论文的创新点在于提出了一个无需标注数据的自监督混合深度学习网络用于光纤信号去噪，并将其与车辆检测跟踪算法相结合，为智能交通系统中的光纤DAS应用提供了一套完整的解决方案。其在真实数据集上的验证增强了实用性。"}}
{"id": "2506.14783", "title": "ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification", "authors": ["Mohamed Masry", "Mohamed Amen", "Mohamed Elzyat", "Mohamed Hamed", "Norhan Magdy", "Maram Khaled"], "summary": "Decoding natural language from brain activity using non-invasive\nelectroencephalography (EEG) remains a significant challenge in neuroscience\nand machine learning, particularly for open-vocabulary scenarios where\ntraditional methods struggle with noise and variability. Previous studies have\nachieved high accuracy on small-closed vocabularies, but it still struggles on\nopen vocabularies. In this study, we propose ETS, a framework that integrates\nEEG with synchronized eye-tracking data to address two critical tasks: (1)\nopen-vocabulary text generation and (2) sentiment classification of perceived\nlanguage. Our model achieves a superior performance on BLEU and Rouge score for\nEEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment\nclassification, which significantly outperforms supervised baselines.\nFurthermore, we show that our proposed model can handle data from various\nsubjects and sources, showing great potential for high performance open\nvocabulary eeg-to-text system.", "comment": "Graduation project report submitted at Faculty of Computer Science\n  and Artificial Intelligence, Helwan University", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14783v1", "AI": {"title_translation": "ETS：开放词汇脑电图到文本解码和情感分类", "tldr": "ETS是一个结合脑电图和眼动追踪数据的框架，用于开放词汇文本生成和感知语言情感分类，在两项任务上均优于基线模型。", "motivation": "从脑电活动中解码自然语言，特别是在开放词汇场景下，是一个重大挑战，传统方法在处理噪声和变异性方面存在困难。之前的研究在小规模封闭词汇上表现良好，但在开放词汇上仍面临挑战。", "method": "本研究提出了ETS框架，该框架将脑电图（EEG）与同步的眼动追踪数据相结合，以解决开放词汇文本生成和感知语言情感分类这两个关键任务。", "result": "该模型在EEG到文本解码的BLEU和Rouge分数上取得了卓越的性能，并在基于EEG的三元情感分类上将F1分数提高了高达10%，显著优于监督基线。此外，该模型能够处理来自不同受试者和来源的数据。", "conclusion": "ETS模型在开放词汇脑电图到文本解码和情感分类方面表现出色，并展示了构建高性能开放词汇脑电图到文本系统的巨大潜力。", "translation": "从脑电活动中解码自然语言，使用非侵入性脑电图（EEG），仍然是神经科学和机器学习领域的一个重大挑战，特别是在开放词汇场景下，传统方法在处理噪声和变异性方面存在困难。之前的研究在小型封闭词汇上取得了高精度，但在开放词汇上仍面临挑战。在本研究中，我们提出了ETS，一个将脑电图与同步眼动追踪数据相结合的框架，以解决两个关键任务：（1）开放词汇文本生成和（2）感知语言的情感分类。我们的模型在EEG到文本解码的BLEU和Rouge分数上取得了卓越的性能，并在基于EEG的三元情感分类上将F1分数提高了高达10%，这显著优于监督基线。此外，我们证明了我们提出的模型可以处理来自各种受试者和来源的数据，显示出高性能开放词汇EEG到文本系统的巨大潜力。", "summary": "本研究提出了ETS框架，旨在解决从脑电图（EEG）数据中进行开放词汇文本生成和感知语言情感分类的挑战。ETS整合了EEG和同步眼动追踪数据，在EEG到文本解码的BLEU和Rouge分数上取得了优异表现，并在EEG情感分类任务中将F1分数提高了高达10%，显著优于现有基线。该模型还展现了处理多样化数据的能力，表明其在构建高性能开放词汇EEG-to-text系统方面的巨大潜力。", "keywords": "脑电图, 文本解码, 情感分类, 开放词汇, 眼动追踪", "comments": "ETS的创新点在于结合了EEG和眼动追踪数据来解决开放词汇的脑机接口难题，这比单纯依赖EEG更具鲁棒性。其在开放词汇文本生成和情感分类上的显著性能提升，预示着该领域未来研究的新方向和实际应用的可能性。该方法能够处理来自不同受试者和来源的数据，也增强了其实用性和泛化能力。"}}
{"id": "2506.15684", "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards", "authors": ["Qingming Liu", "Zhen Liu", "Dinghuai Zhang", "Kui Jia"], "summary": "Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.", "comment": "Technical Report (21 pages, 21 figures)", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15684v1", "AI": {"title_translation": "Nabla-R2D3：基于2D奖励的有效高效3D扩散对齐", "tldr": "Nabla-R2D3是一个高效的强化学习对齐框架，利用2D奖励信号改进3D扩散模型的生成质量，克服了现有方法的收敛问题和奖励欺骗。", "motivation": "生成高质量、逼真的3D资产是一个长期存在的挑战。尽管扩散模型在3D生成方面取得了进展，但它们在遵循指令、符合人类偏好或生成逼真纹理、几何形状和物理属性方面仍不足以达到人类设计内容的水平。", "method": "本文引入了Nabla-R2D3，一个高效且样本效率高的强化学习对齐框架，用于3D原生扩散模型，它利用2D奖励。该方法基于Nabla-GFlowNet，以原则性的方式将分数函数与奖励梯度匹配进行奖励微调，从而使3D扩散模型能够仅使用2D奖励信号进行有效适应。", "result": "大量实验表明，与难以收敛或遭受奖励欺骗的香草微调基线不同，Nabla-R2D3在少量微调步骤内持续实现更高的奖励并减少先验遗忘。", "conclusion": "Nabla-R2D3通过利用2D奖励信号，为3D扩散模型提供了一种有效且高效的对齐方法，显著提高了生成质量并克服了传统微调的局限性。", "translation": "生成高质量、逼真的3D资产在3D视觉和计算机图形学中仍然是一个长期存在的挑战。尽管最先进的生成模型，如扩散模型，在3D生成方面取得了显著进展，但由于其遵循指令、符合人类偏好或生成逼真纹理、几何形状和物理属性的能力有限，它们往往未能达到人类设计内容的水平。在本文中，我们引入了Nabla-R2D3，一个高效且样本效率高的强化学习对齐框架，用于使用2D奖励的3D原生扩散模型。我们的Nabla-R2D3建立在最近提出的Nabla-GFlowNet方法之上，该方法以原则性的方式将分数函数与奖励梯度匹配进行奖励微调，从而使3D扩散模型能够仅使用2D奖励信号进行有效适应。大量的实验表明，与难以收敛或遭受奖励欺骗的香草微调基线不同，Nabla-R2D3在少量微调步骤内持续实现更高的奖励并减少先验遗忘。", "summary": "Nabla-R2D3提出了一种新颖的强化学习对齐框架，用于改进3D扩散模型生成高质量、逼真3D资产的能力。它利用2D奖励信号，并基于Nabla-GFlowNet方法，有效地将3D扩散模型的生成与人类偏好对齐。实验证明，Nabla-R2D3在提高奖励和减少先验遗忘方面优于传统的微调基线，即使在少量微调步骤内也能实现更好的收敛性。", "keywords": "3D扩散, 强化学习, 2D奖励, Nabla-R2D3, 模型对齐", "comments": "该论文的创新点在于提出了一个利用2D奖励信号对齐3D扩散模型的强化学习框架，有效解决了3D生成中长期存在的质量和真实感问题。其基于Nabla-GFlowNet的方法原理性地将分数函数与奖励梯度匹配，避免了传统微调中常见的收敛困难和奖励欺骗问题，展现了高效性和样本效率。这对于推动3D内容生成技术的发展具有重要意义。"}}
{"id": "2506.15576", "title": "DiscRec: Disentangled Semantic-Collaborative Modeling for Generative Recommendation", "authors": ["Chang Liu", "Yimeng Bai", "Xiaoyan Zhao", "Yang Zhang", "Fuli Feng", "Wenge Rong"], "summary": "Generative recommendation is emerging as a powerful paradigm that directly\ngenerates item predictions, moving beyond traditional matching-based\napproaches. However, current methods face two key challenges: token-item\nmisalignment, where uniform token-level modeling ignores item-level granularity\nthat is critical for collaborative signal learning, and semantic-collaborative\nsignal entanglement, where collaborative and semantic signals exhibit distinct\ndistributions yet are fused in a unified embedding space, leading to\nconflicting optimization objectives that limit the recommendation performance.\n  To address these issues, we propose DiscRec, a novel framework that enables\nDisentangled Semantic-Collaborative signal modeling with flexible fusion for\ngenerative Recommendation.First, DiscRec introduces item-level position\nembeddings, assigned based on indices within each semantic ID, enabling\nexplicit modeling of item structure in input token sequences.Second, DiscRec\nemploys a dual-branch module to disentangle the two signals at the embedding\nlayer: a semantic branch encodes semantic signals using original token\nembeddings, while a collaborative branch applies localized attention restricted\nto tokens within the same item to effectively capture collaborative signals. A\ngating mechanism subsequently fuses both branches while preserving the model's\nability to model sequential dependencies. Extensive experiments on four\nreal-world datasets demonstrate that DiscRec effectively decouples these\nsignals and consistently outperforms state-of-the-art baselines. Our codes are\navailable on https://github.com/Ten-Mao/DiscRec.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15576v1", "AI": {"title_translation": "DiscRec：生成式推荐中解耦语义-协同建模", "tldr": "DiscRec通过解耦语义和协同信号，解决了生成式推荐中现存的挑战，显著提升了推荐性能。", "motivation": "当前生成式推荐方法面临两大挑战：一是“token-item未对齐”，即统一的token级别建模忽略了对协同信号学习至关重要的项目级别粒度；二是“语义-协同信号纠缠”，即两种信号分布不同却在统一嵌入空间中融合，导致优化目标冲突，限制了推荐性能。", "method": "提出DiscRec框架，实现生成式推荐中解耦语义-协同信号建模与灵活融合。首先，引入基于语义ID内索引的项目级别位置嵌入，显式建模输入token序列中的项目结构。其次，采用双分支模块在嵌入层解耦两种信号：语义分支使用原始token嵌入编码语义信号，协同分支应用局部注意力捕获同一项目内token的协同信号。最后，通过门控机制融合两个分支，同时保留模型建模序列依赖的能力。", "result": "在四个真实世界数据集上的大量实验表明，DiscRec能有效解耦这些信号，并持续优于最先进的基线模型。", "conclusion": "DiscRec通过其解耦的建模方法，成功解决了生成式推荐中的关键挑战，显著提升了推荐性能，证明了其在处理语义和协同信号方面的有效性。", "translation": "生成式推荐正成为一种强大的范式，它直接生成项目预测，超越了传统的基于匹配的方法。然而，当前方法面临两个关键挑战：一是令牌-项目未对齐，即统一的令牌级建模忽略了对协同信号学习至关重要的项目级粒度；二是语义-协同信号纠缠，即协同信号和语义信号表现出不同的分布，却在统一的嵌入空间中融合，导致相互冲突的优化目标，从而限制了推荐性能。为了解决这些问题，我们提出了DiscRec，一个新颖的框架，它能够实现生成式推荐中解耦语义-协同信号建模与灵活融合。首先，DiscRec引入了项目级位置嵌入，根据每个语义ID内的索引进行分配，从而能够在输入令牌序列中显式建模项目结构。其次，DiscRec采用双分支模块在嵌入层解耦这两种信号：一个语义分支使用原始令牌嵌入编码语义信号，而一个协同分支则应用仅限于同一项目内令牌的局部注意力，以有效捕获协同信号。随后，一个门控机制融合了两个分支，同时保留了模型建模序列依赖的能力。在四个真实世界数据集上的大量实验表明，DiscRec有效地解耦了这些信号，并持续优于最先进的基线模型。我们的代码可在https://github.com/Ten-Mao/DiscRec上获取。", "summary": "本文提出了DiscRec，一个用于生成式推荐的新型框架，旨在解决现有方法中令牌-项目未对齐和语义-协同信号纠缠的问题。DiscRec通过引入项目级位置嵌入和采用双分支模块（语义分支和协同分支）在嵌入层解耦这两种信号，并通过门控机制灵活融合。实验结果表明，DiscRec能有效解耦信号并显著优于现有基线模型。", "keywords": "生成式推荐, 语义-协同解耦, DiscRec, 位置嵌入, 双分支建模", "comments": "DiscRec的创新之处在于其对语义和协同信号的解耦建模，并通过引入项目级位置嵌入和双分支架构（结合局部注意力与门控融合）来克服现有生成式推荐模型中的关键限制。这种方法有效地解决了信号纠缠和粒度不匹配问题，为提升生成式推荐性能提供了新的思路。"}}
{"id": "2506.14951", "title": "Flat Channels to Infinity in Neural Loss Landscapes", "authors": ["Flavio Martinelli", "Alexander Van Meegen", "Berfin Şimşek", "Wulfram Gerstner", "Johanni Brea"], "summary": "The loss landscapes of neural networks contain minima and saddle points that\nmay be connected in flat regions or appear in isolation. We identify and\ncharacterize a special structure in the loss landscape: channels along which\nthe loss decreases extremely slowly, while the output weights of at least two\nneurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight\nvectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At\nconvergence, the two neurons implement a gated linear unit:\n$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot\n\\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot\n\\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these\nchannels to infinity are asymptotically parallel to symmetry-induced lines of\ncritical points. Gradient flow solvers, and related optimization methods like\nSGD or ADAM, reach the channels with high probability in diverse regression\nsettings, but without careful inspection they look like flat local minima with\nfinite parameter values. Our characterization provides a comprehensive picture\nof these quasi-flat regions in terms of gradient dynamics, geometry, and\nfunctional interpretation. The emergence of gated linear units at the end of\nthe channels highlights a surprising aspect of the computational capabilities\nof fully connected layers.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14951v1", "AI": {"title_translation": "神经网络损失景观中的无限平坦通道", "tldr": "研究发现神经网络损失景观中存在一种特殊的“无限平坦通道”，其中损失缓慢下降，特定神经元权重趋于无穷，最终形成门控线性单元，且优化器常误将其视为有限参数的平坦局部最小值。", "motivation": "了解神经网络损失景观的复杂结构，特别是那些可能影响优化器收敛行为的“平坦区域”，并揭示这些区域的真实性质，因为它们常被误认为是有限参数的平坦局部最小值。", "method": "识别并刻画了损失景观中一种特殊结构，即“无限平坦通道”。通过分析神经元输出权重趋于无穷大以及输入权重向量趋于相等时的损失下降行为、几何特性和功能解释。", "result": "识别出一种特殊结构——“无限平坦通道”，其特点是损失下降极其缓慢，至少两个神经元的输出权重趋于无穷，而它们的输入权重向量变得相等。在收敛时，这些神经元实现门控线性单元。这些通道在几何上渐近平行于对称诱导的临界点线。梯度流求解器和相关优化方法（如SGD或ADAM）在各种回归设置中以高概率到达这些通道，但若不仔细检查，它们看起来像具有有限参数值的平坦局部最小值。", "conclusion": "本文全面刻画了神经网络损失景观中的“准平坦区域”（即无限平坦通道），揭示了其梯度动力学、几何特性和功能解释。这些通道末端出现的门控线性单元揭示了全连接层计算能力的惊人方面，并解释了为何优化器可能误将这些区域视为平坦局部最小值。", "translation": "神经网络的损失景观包含可能在平坦区域中连接或单独出现的最小值和鞍点。我们识别并刻画了损失景观中的一种特殊结构：沿着这些通道，损失下降极其缓慢，同时至少两个神经元 $a_i$ 和 $a_j$ 的输出权重发散到 $\\pm$ 无穷大，并且它们的输入权重向量 $\\mathbf{w_i}$ 和 $\\mathbf{w_j}$ 彼此相等。在收敛时，这两个神经元实现了一个门控线性单元：$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot \\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot \\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$。在几何上，这些无限平坦通道渐近平行于对称诱导的临界点线。梯度流求解器以及SGD或ADAM等相关优化方法，在各种回归设置中以高概率到达这些通道，但若不仔细检查，它们看起来像具有有限参数值的平坦局部最小值。我们的刻画从梯度动力学、几何和功能解释方面提供了这些准平坦区域的全面图景。通道末端出现的门控线性单元突出了全连接层计算能力的一个令人惊讶的方面。", "summary": "本文识别并深入刻画了神经网络损失景观中的一种特殊结构，称为“无限平坦通道”。这些通道的特点是损失下降极其缓慢，同时某些神经元的输出权重趋于无穷大，输入权重向量趋于相等，最终形成门控线性单元。研究发现，常见的优化器如SGD和ADAM在训练时高概率地到达这些通道，但由于其特性，这些通道常被误认为是具有有限参数值的平坦局部最小值。这项工作从梯度动力学、几何和功能解释等多个角度全面阐明了这些准平坦区域的性质及其对全连接层计算能力的启示。", "keywords": "神经网络损失景观, 平坦通道, 门控线性单元, 梯度流, 优化器", "comments": "这篇论文通过揭示神经网络损失景观中“无限平坦通道”这一特殊结构，对神经网络优化领域做出了重要贡献。其创新之处在于，它解释了为何一些看似平坦的局部最小值实际上可能是参数发散的通道，这对于理解优化器行为和设计更有效的优化策略具有重要意义。特别是，发现这些通道最终形成门控线性单元，揭示了全连接层意想不到的计算能力。这篇工作有助于更深入地理解深度学习模型的训练动态和泛化能力。"}}
{"id": "2506.14927", "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "authors": ["Joseph J. Peper", "Wenzhao Qiu", "Ali Payani", "Lu Wang"], "summary": "Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14927v1", "AI": {"title_translation": "MDBench：一个知识引导生成的多文档推理基准", "tldr": "MDBench是一个新的合成多文档推理基准，用于评估LLM，它通过知识引导生成，并对现有LLM构成显著挑战。", "motivation": "随着大型语言模型（LLMs）推理能力的快速发展，对新的评估基准的需求日益增加。特别是，尽管多文档（MD）推理对LLM处理长上下文输入至关重要，但现有严格的MD推理基准很少，且由于长文本标注成本高昂，MD基准创建面临挑战。", "method": "本文介绍了MDBench，一个用于评估LLM多文档推理能力的新数据集。MDBench通过一种新颖的合成生成过程创建，该过程基于浓缩的结构化种子知识，并通过LLM辅助编辑进行修改以引入MD特定的推理挑战，然后将结构化知识转换为自然文本形式，生成文档集和对应的问答示例。", "result": "分析发现，MDBench对所有流行的LLM和提示技术都构成了显著挑战，即使文档集相对较短。", "conclusion": "知识引导的生成技术能够方便地对MD特定推理能力进行目标分析，并且可以快速适应新的挑战和未来的模型改进。", "translation": "自然语言处理评估取得了显著进展，这主要得益于强大的大型语言模型（LLM）的普及。随着LLM推理能力的快速扩展，新的评估基准变得越来越重要。特别是，鉴于LLM处理长上下文输入的能力，多文档（MD）推理是一个极其相关的领域，但目前很少有基准能够严格检验模型在这种设置下的行为。此外，由于标注长输入的成本高昂，多文档设置历来对基准创建构成挑战。在这项工作中，我们介绍了MDBench，这是一个用于评估LLM在多文档推理任务上的新数据集。值得注意的是，MDBench是通过一种新颖的合成生成过程创建的，这使我们能够可控且高效地生成具有挑战性的文档集以及相应的问答（QA）示例。我们的新颖技术基于浓缩的结构化种子知识，通过LLM辅助编辑对其进行修改，以引入MD特定的推理挑战。然后，我们将这种结构化知识转换为自然文本表面形式，生成一个文档集和相应的QA示例。我们分析了流行的LLM和提示技术的行为，发现MDBENCH对所有方法都构成了显著挑战，即使文档集相对较短。我们还发现，我们的知识引导生成技术（1）使我们能够轻松地对MD特定推理能力进行有针对性的分析，并且（2）可以快速适应新的挑战和未来的模型改进。", "summary": "本文介绍了MDBench，一个用于评估大型语言模型（LLMs）多文档推理能力的新基准数据集。该基准通过新颖的知识引导合成生成过程创建，能够高效且可控地生成具有挑战性的多文档问答对，以克服传统基准创建成本高昂的问题。实验结果表明，MDBench对现有流行的LLMs及其提示技术构成了显著挑战，并证明了其生成方法在特定推理能力分析和适应未来改进方面的有效性。", "keywords": "多文档推理, 基准数据集, 大型语言模型, 知识引导生成, 合成数据", "comments": "MDBench的创新之处在于其采用知识引导的合成生成方法，有效解决了多文档推理基准创建中数据标注成本高昂的痛点，并能灵活地生成具有特定推理挑战的案例。这对于推动LLM在复杂长上下文理解方面的发展具有重要意义。"}}
{"id": "2506.14792", "title": "Fast automated adjoints for spectral PDE solvers", "authors": ["Calum S. Skene", "Keaton J. Burns"], "summary": "We present a general and automated approach for computing model gradients for\nPDE solvers built on sparse spectral methods, and implement this capability in\nthe widely used open-source Dedalus framework. We apply reverse-mode automatic\ndifferentiation to symbolic graph representations of PDEs, efficiently\nconstructing adjoint solvers that retain the speed and memory efficiency of\nthis important class of modern numerical methods. This approach enables users\nto compute gradients and perform optimization for a wide range of\ntime-dependent and nonlinear systems without writing additional code. The\nframework supports a broad class of equations, geometries, and boundary\nconditions, and runs efficiently in parallel using MPI. We demonstrate the\nflexibility and capabilities of this system using canonical problems from the\nliterature, showing both strong performance and practical utility for a wide\nvariety of inverse problems. By integrating automatic adjoints into a flexible\nhigh-level solver, our approach enables researchers to perform gradient-based\noptimization and sensitivity analyses in spectral simulations with ease and\nefficiency.", "comment": "17 pages, 6 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14792v1", "AI": {"title_translation": "谱偏微分方程求解器的快速自动化伴随", "tldr": "该研究提出了一种在Dedalus框架中自动计算谱偏微分方程求解器模型梯度的方法，利用反向自动微分，实现了高效的伴随求解器，便于进行梯度优化和敏感性分析。", "motivation": "现有谱偏微分方程求解器在计算模型梯度和进行优化时可能需要额外的代码。该研究旨在提供一种通用且自动化的方法，使用户能够轻松高效地进行梯度计算和优化，无需编写额外代码。", "method": "该方法提出了一种通用且自动化的方法，用于计算基于稀疏谱方法的偏微分方程求解器的模型梯度。具体而言，它将反向模式自动微分应用于偏微分方程的符号图表示，并在广泛使用的开源Dedalus框架中实现了这一功能，从而高效地构建了伴随求解器。", "result": "该方法构建的伴随求解器保留了谱方法的原有速度和内存效率。它使用户能够对各种瞬态和非线性系统进行梯度计算和优化，而无需编写额外代码。该框架支持广泛的方程、几何形状和边界条件，并能通过MPI高效并行运行。在经典问题上的演示表明，该系统在各种逆问题中都表现出强大的性能和实用性。", "conclusion": "通过将自动化伴随集成到灵活的高级求解器中，该方法使研究人员能够轻松高效地在谱模拟中进行基于梯度的优化和敏感性分析。", "translation": "我们提出了一种通用且自动化的方法，用于计算基于稀疏谱方法的偏微分方程（PDE）求解器的模型梯度，并在广泛使用的开源Dedalus框架中实现了这一功能。我们将反向模式自动微分应用于偏微分方程的符号图表示，高效地构建了伴随求解器，这些求解器保留了这种重要现代数值方法的速度和内存效率。这种方法使用户能够对各种瞬态和非线性系统进行梯度计算和优化，而无需编写额外代码。该框架支持广泛的方程、几何形状和边界条件，并能使用MPI高效并行运行。我们使用文献中的经典问题展示了该系统的灵活性和能力，显示出在各种逆问题中的强大性能和实用价值。通过将自动化伴随集成到灵活的高级求解器中，我们的方法使研究人员能够轻松高效地在谱模拟中进行基于梯度的优化和敏感性分析。", "summary": "本文提出了一种在开源Dedalus框架中为基于稀疏谱方法的偏微分方程求解器自动计算模型梯度的通用方法。通过对偏微分方程的符号图表示应用反向模式自动微分，该方法能够高效构建保持原有速度和内存效率的伴随求解器。这使得用户无需额外编码即可对各种瞬态和非线性系统进行梯度计算和优化，并支持多种方程、几何和边界条件，且能高效并行运行。实验证明其在逆问题中具有强大的性能和实用性，极大地简化了谱模拟中的梯度优化和敏感性分析。", "keywords": "自动微分, 谱方法, 偏微分方程, 伴随求解器, Dedalus", "comments": "这篇论文的创新点在于将反向模式自动微分与谱偏微分方程求解器相结合，并将其集成到一个流行的开源框架Dedalus中，实现了伴随求解器的自动化和通用化。这显著降低了研究人员进行梯度计算和优化分析的门槛，尤其对于复杂的时间依赖和非线性系统，其提供的无代码梯度计算能力具有重要的实用价值。该方法在保持谱方法原有高效性的同时，提升了其在逆问题和优化领域的应用潜力。"}}
{"id": "2506.15397", "title": "Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control", "authors": ["Sepehr Elahi", "Paula Mürmann", "Patrick Thiran"], "summary": "The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15397v1", "AI": {"title_translation": "学习疫苗接种：结合结构学习和有效疫苗接种用于流行病和疫情控制", "tldr": "该论文通过结合图学习和疫苗接种策略来解决在未知图结构下流行病控制的问题，以最小化疾病的灭绝时间。", "motivation": "以往关于最优疫苗接种的研究都假设图结构是已知的，但在实际中这通常不成立。本文旨在解决当疾病传播图未知且仅能观察到顶点感染状态时，如何最小化流行病灭绝时间的问题。", "method": "该研究将问题分解为两部分：学习图结构和确定有效的疫苗接种策略。提出了一种新颖的基于包含-排除的学习算法，并首次建立了其图恢复的样本复杂度。随后，详细介绍了一种用于谱半径最小化（SRM）问题的最优算法，并证明其在有界树宽图上的运行时间是顶点数量的多项式。此外，还为任意图提出了一种高效且有效的多项式时间贪婪启发式算法。", "result": "在合成数据和真实世界数据上进行的实验，数值验证了所提出的学习和疫苗接种算法的有效性。", "conclusion": "本文提出的图学习和疫苗接种算法在最小化未知图结构下SIS模型疫情的灭绝时间方面是有效且经过验证的。", "translation": "易感-感染-易感（SIS）模型是一种广泛应用于图上信息传播和传染病（特别是非免疫性疾病）的模型。给定一种高度传染性的疾病，一个自然的问题是如何最好地为个体接种疫苗以最小化疾病的灭绝时间。虽然以前的工作表明最优疫苗接种问题与NP-hard的谱半径最小化（SRM）问题密切相关，但它们假设图是已知的，这在实践中往往并非如此。在这项工作中，我们考虑了由SIS模型建模的疫情最小化灭绝时间的问题，其中疾病传播的图是未知的，并且只观察到顶点的感染状态。为此，我们将问题分为两部分：学习图和确定有效的疫苗接种策略。我们提出了一种新颖的基于包含-排除的学习算法，并且与以前的方法不同，建立了其图恢复的样本复杂度。然后，我们详细介绍了一种用于SRM问题的最优算法，并证明其在有界树宽图上的运行时间是顶点数量的多项式。这通过一种针对任何图的有效且高效的多项式时间贪婪启发式算法得到补充。最后，我们在合成和真实世界数据上进行了实验，数值验证了我们的学习和疫苗接种算法。", "summary": "本文针对在底层图结构未知的情况下，如何最小化SIS流行病模型中疾病灭绝时间的问题。它提出了一种双管齐下的方法：首先，一种新颖的基于包含-排除的图恢复算法，并建立了其样本复杂度；其次，一种针对NP-hard谱半径最小化（SRM）问题的最优算法，适用于有界树宽图，以及一种针对一般图的有效贪婪启发式算法。在合成数据和真实世界数据上的实验结果验证了所提出的学习和疫苗接种算法的有效性。", "keywords": "SIS模型, 流行病控制, 图学习, 疫苗接种, 谱半径最小化", "comments": "该论文通过整合图学习与疫苗接种策略，解决了先前工作中图结构未知这一实际限制。其在图恢复方面（样本复杂度）的理论保证，以及针对特定图类型（有界树宽）的最优SRM算法，是显著的贡献，并辅以实用的贪婪启发式算法。这种结合使得该方法既稳健又适用于现实世界场景。"}}
{"id": "2506.14834", "title": "Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection", "authors": ["Akwasi Asare", "Dennis Agyemanh Nana Gookyi", "Derrick Boateng", "Fortunatus Aabangbio Wulnye"], "summary": "Diabetic Retinopathy (DR), a leading cause of vision impairment in\nindividuals with diabetes, affects approximately 34.6% of diabetes patients\nglobally, with the number of cases projected to reach 242 million by 2045.\nTraditional DR diagnosis relies on the manual examination of retinal fundus\nimages, which is both time-consuming and resource intensive. This study\npresents a novel solution using Edge Impulse to deploy multiple deep learning\nmodels for real-time DR detection on edge devices. A robust dataset of over\n3,662 retinal fundus images, sourced from the Kaggle EyePACS dataset, was\ncurated, and enhanced through preprocessing techniques, including augmentation\nand normalization. Using TensorFlow, various Convolutional Neural Networks\n(CNNs), such as MobileNet, ShuffleNet, SqueezeNet, and a custom Deep Neural\nNetwork (DNN), were designed, trained, and optimized for edge deployment. The\nmodels were converted to TensorFlowLite and quantized to 8-bit integers to\nreduce their size and enhance inference speed, with minimal trade-offs in\naccuracy. Performance evaluations across different edge hardware platforms,\nincluding smartphones and microcontrollers, highlighted key metrics such as\ninference speed, accuracy, precision, and resource utilization. MobileNet\nachieved an accuracy of 96.45%, while SqueezeNet demonstrated strong real-time\nperformance with a small model size of 176 KB and latency of just 17 ms on GPU.\nShuffleNet and the custom DNN achieved moderate accuracy but excelled in\nresource efficiency, making them suitable for lower-end devices. This\nintegration of edge AI technology into healthcare presents a scalable,\ncost-effective solution for early DR detection, providing timely and accurate\ndiagnosis, especially in resource-constrained and remote healthcare settings.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14834v1", "AI": {"title_translation": "在边缘设备上部署和评估多个深度学习模型用于糖尿病视网膜病变检测", "tldr": "本研究探讨了在边缘设备上部署和评估多个深度学习模型，以实现糖尿病视网膜病变的实时检测。", "motivation": "糖尿病视网膜病变(DR)是导致视力损伤的主要原因，传统诊断方法耗时且资源密集，本研究旨在提供一种可扩展、经济高效的早期检测解决方案。", "method": "利用Edge Impulse在边缘设备上部署多个深度学习模型进行实时DR检测。使用来自Kaggle EyePACS的3,662张视网膜眼底图像数据集，并进行增强和归一化预处理。采用TensorFlow设计、训练和优化了MobileNet、ShuffleNet、SqueezeNet和自定义DNN等多种CNN模型，并转换为TensorFlowLite并进行8位量化以减小模型大小和提高推理速度。在智能手机和微控制器等不同边缘硬件平台上评估了推理速度、准确性、精确度和资源利用率等关键指标。", "result": "MobileNet实现了96.45%的准确率。SqueezeNet表现出强大的实时性能，模型大小为176 KB，在GPU上延迟仅为17毫秒。ShuffleNet和自定义DNN虽然准确率适中，但在资源效率方面表现出色，使其适用于低端设备。", "conclusion": "将边缘AI技术整合到医疗保健中，为早期糖尿病视网膜病变检测提供了一种可扩展、经济高效的解决方案，特别适用于资源受限和偏远地区的及时准确诊断。", "translation": "糖尿病视网膜病变（DR）是糖尿病患者视力损伤的主要原因，全球约34.6%的糖尿病患者受其影响，预计到2045年病例数将达到2.42亿。传统的DR诊断依赖于对视网膜眼底图像的手动检查，这既耗时又耗费资源。本研究提出了一种利用Edge Impulse在边缘设备上部署多个深度学习模型以实现实时DR检测的新颖解决方案。研究策划并增强了一个包含3,662多张视网膜眼底图像的鲁棒数据集，该数据集来源于Kaggle EyePACS数据集，并经过了包括数据增强和归一化在内的预处理技术。使用TensorFlow设计、训练和优化了各种卷积神经网络（CNN），例如MobileNet、ShuffleNet、SqueezeNet和自定义深度神经网络（DNN），以适应边缘部署。这些模型被转换为TensorFlowLite并量化为8位整数，以减小模型大小并提高推理速度，同时最大限度地减少了准确性方面的权衡。在包括智能手机和微控制器在内的不同边缘硬件平台上进行的性能评估，突出了推理速度、准确性、精确度和资源利用率等关键指标。MobileNet实现了96.45%的准确率，而SqueezeNet展示了强大的实时性能，模型大小为176 KB，在GPU上的延迟仅为17毫秒。ShuffleNet和自定义DNN取得了适中的准确率，但在资源效率方面表现出色，使其适用于低端设备。这种将边缘AI技术整合到医疗保健中的方案，为早期DR检测提供了一种可扩展、经济高效的解决方案，尤其是在资源受限和偏远医疗环境中，能够提供及时准确的诊断。", "summary": "本研究提出了一种在边缘设备上部署和评估多个深度学习模型（如MobileNet、ShuffleNet、SqueezeNet和自定义DNN）的新方法，用于实时检测糖尿病视网膜病变。通过对来自Kaggle EyePACS的3,662张眼底图像进行预处理和模型优化（TensorFlowLite转换和8位量化），研究在智能手机和微控制器等边缘硬件上进行了性能评估。结果显示MobileNet准确率达96.45%，SqueezeNet具有优异的实时性能和极小的模型尺寸，而ShuffleNet和自定义DNN则在资源效率方面表现突出。该研究表明边缘AI技术能为早期DR检测提供可扩展、经济高效的解决方案，尤其适用于资源受限的环境。", "keywords": "糖尿病视网膜病变, 深度学习, 边缘计算, 实时检测, 医疗AI", "comments": "该研究的创新之处在于其在多种边缘设备上部署和评估多个深度学习模型，以实现糖尿病视网膜病变的实时检测。通过对模型进行量化优化，并在不同硬件平台上进行性能评估，该工作展示了边缘AI在医疗健康领域，特别是在资源受限环境中，提供可扩展、经济高效的早期诊断解决方案的巨大潜力。"}}
{"id": "2506.15000", "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "summary": "Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15000v1", "AI": {"title_translation": "深度学习模型在真实世界嘈杂环境中语音增强的比较评估", "tldr": "本研究比较评估了Wave-U-Net、CMGAN和U-Net三种深度学习模型在语音增强方面的性能，发现在噪声抑制、感知质量和说话人特征保留之间存在权衡，并指出CMGAN在感知质量上表现最佳。", "motivation": "语音增强，特别是去噪，对于提高语音信号在嘈杂环境中的可懂度和质量至关重要。尽管已有多种深度学习模型，但它们在噪声抑制、感知质量和说话人特定特征保留之间难以平衡，缺乏对它们比较性能的评估。", "method": "本研究在SpEAR、VPQAD和Clarkson等多样化数据集上，对Wave-U-Net、CMGAN和U-Net三种先进模型进行了基准测试。这些模型因其在文献中的相关性和代码可访问性而被选中。", "result": "U-Net在噪声抑制方面表现出色，在SpEAR上SNR提升+71.96%，VPQAD上+64.83%，Clarkson数据集上+364.2%。CMGAN在感知质量上表现最佳，在SpEAR上PESQ得分4.04，VPQAD上1.46。Wave-U-Net在说话人特定特征保留方面表现平衡，在SpEAR上VeriSpeak得分提升+10.84%，VPQAD上+27.38%。", "conclusion": "本研究表明，先进方法可以优化噪声抑制、感知质量和说话人识别之间的权衡。研究结果可能有助于推进语音生物识别、法医音频分析、电信和说话人验证在挑战性声学条件下的发展。", "translation": "语音增强，特别是去噪，对于提高真实世界应用中，尤其是在嘈杂环境下的语音信号的可懂度和质量至关重要。尽管先前的研究已经为此目的引入了各种深度学习模型，但许多模型难以平衡噪声抑制、感知质量和说话人特定特征的保留，这在它们的比较性能评估中留下了一个关键的研究空白。本研究在SpEAR、VPQAD和Clarkson等多样化数据集上，对Wave-U-Net、CMGAN和U-Net三种最先进的模型进行了基准测试。选择这些模型是因为它们在文献中的相关性和代码可访问性。评估结果显示，U-Net在噪声抑制方面取得了高效果，在SpEAR上SNR提升了+71.96%，在VPQAD上提升了+64.83%，在Clarkson数据集上提升了+364.2%。CMGAN在感知质量方面表现优异，在SpEAR上获得了4.04的最高PESQ分数，在VPQAD上获得了1.46的最高PESQ分数，使其非常适合优先考虑自然和清晰语音的应用。Wave-U-Net通过说话人特定特征保留的改进来平衡这些属性，SpEAR上VeriSpeak得分提升了+10.84%，VPQAD上提升了+27.38%。这项研究揭示了先进方法如何优化噪声抑制、感知质量和说话人识别之间的权衡。研究结果可能有助于推进语音生物识别、法医音频分析、电信和在挑战性声学条件下的说话人验证。", "summary": "本研究对Wave-U-Net、CMGAN和U-Net三种先进的深度学习语音增强模型在真实世界嘈杂环境中的性能进行了比较评估。研究发现，U-Net在噪声抑制方面表现出色，CMGAN在感知质量上最优，而Wave-U-Net在说话人特征保留方面表现均衡。研究强调了在不同性能指标之间进行权衡优化的重要性，并指出其发现对语音生物识别、法医音频分析和电信等领域具有潜在贡献。", "keywords": "语音增强, 深度学习, 噪声抑制, 感知质量, 说话人识别", "comments": "这项研究通过对三种代表性深度学习模型在语音增强关键指标上的量化比较，填补了现有研究中的空白。其创新之处在于提供了具体的数据来指导不同应用场景下模型的选择，例如，当侧重于语音可懂度时，CMGAN是优选。研究结果对实际应用具有重要指导意义，尤其是在需要平衡多种性能指标的复杂声学环境中。"}}
{"id": "2506.15070", "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine", "authors": ["Rasha Karakchi", "Rye Stahle-Smith", "Nishant Chinnasami", "Tiffany Yu"], "summary": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators.", "comment": "This is submitted to the ACM/IEEE Symposium on Edge Computing (SEC\n  2025)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15070v1", "AI": {"title_translation": "迈向轻量级、可扩展、并行安全加密引擎", "tldr": "SPiME是一种轻量级、可扩展的FPGA兼容内存处理加密架构，在边缘计算中实现高效安全加密。", "motivation": "物联网应用的指数级增长对边缘高效、高吞吐量和节能数据处理的需求日益增加。传统的以CPU为中心的加密方法存在性能瓶颈和数据移动过多的问题，尤其是在延迟敏感和资源受限的环境中。", "method": "本文提出了SPiME，一种轻量级、可扩展、FPGA兼容的安全内存处理加密架构。它将高级加密标准（AES-128）直接集成到内存处理（PiM）框架中。SPiME设计为并行PiM单元的模块化阵列，每个单元结合AES核心和最小控制单元，以实现开销最小的分布式就地加密。该架构完全采用Verilog实现，并在多个AMD UltraScale和UltraScale+ FPGA上进行了测试。", "result": "SPiME可以扩展到4,000多个并行单元，同时在高端设备上保持关键FPGA资源利用率低于5%。它提供超过25 Gbps的持续加密吞吐量，具有可预测的低延迟性能。", "conclusion": "SPiME的设计具有可移植性、可配置性和资源效率，是安全边缘计算、嵌入式密码系统和可定制硬件加速器的引人注目解决方案。", "translation": "物联网（IoT）应用的指数级增长加剧了对边缘高效、高吞吐量和节能数据处理的需求。传统的以CPU为中心的加密方法存在性能瓶颈和数据移动过多的问题，尤其是在延迟敏感和资源受限的环境中。在本文中，我们提出了SPiME，一种轻量级、可扩展且与FPGA兼容的安全内存处理加密架构，它将高级加密标准（AES-128）直接集成到内存处理（PiM）框架中。SPiME被设计为并行PiM单元的模块化阵列，每个单元将AES核心与最小控制单元相结合，以实现开销最小的分布式就地加密。该架构完全采用Verilog实现，并在多个AMD UltraScale和UltraScale+ FPGA上进行了测试。评估结果表明，SPiME可以扩展到4,000多个并行单元，同时在高端设备上保持关键FPGA资源利用率低于5%。它提供超过25 Gbps的持续加密吞吐量，具有可预测的低延迟性能。该设计的可移植性、可配置性和资源效率使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的引人注目解决方案。", "summary": "本文提出了SPiME，一个轻量级、可扩展的FPGA兼容安全内存处理加密架构，旨在解决物联网边缘设备中传统加密方法的性能瓶颈和高数据移动问题。SPiME将AES-128集成到内存处理框架中，通过并行PiM单元实现分布式就地加密。实验证明，SPiME在FPGA上展现出高扩展性（超过4,000个单元），低资源利用率（低于5%），并提供超过25 Gbps的持续加密吞吐量和低延迟，是边缘安全计算的理想方案。", "keywords": "内存处理, FPGA, AES-128, 边缘计算, 安全加密", "comments": "SPiME的创新点在于将AES加密直接集成到内存处理单元中，有效减少了数据移动，解决了传统CPU中心加密的瓶颈，特别适合资源受限的边缘IoT设备。其在FPGA上的高扩展性和效率展示了其作为定制硬件加速器的巨大潜力。"}}
{"id": "2506.14975", "title": "Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion", "authors": ["Jeffrey Mao", "Raghuram Cauligi Srinivas", "Steven Nogar", "Giuseppe Loianno"], "summary": "Quadrotors hold significant promise for several applications such as\nagriculture, search and rescue, and infrastructure inspection. Achieving\nautonomous operation requires systems to navigate safely through complex and\nunfamiliar environments. This level of autonomy is particularly challenging due\nto the complexity of such environments and the need for real-time decision\nmaking especially for platforms constrained by size, weight, and power (SWaP),\nwhich limits flight time and precludes the use of bulky sensors like Light\nDetection and Ranging (LiDAR) for mapping. Furthermore, computing globally\noptimal, collision-free paths and translating them into time-optimized, safe\ntrajectories in real time adds significant computational complexity. To address\nthese challenges, we present a fully onboard, real-time navigation system that\nrelies solely on lightweight onboard sensors. Our system constructs a dense 3D\nmap of the environment using a novel visual depth estimation approach that\nfuses stereo and monocular learning-based depth, yielding longer-range, denser,\nand less noisy depth maps than conventional stereo methods. Building on this\nmap, we introduce a novel planning and trajectory generation framework capable\nof rapidly computing time-optimal global trajectories. As the map is\nincrementally updated with new depth information, our system continuously\nrefines the trajectory to maintain safety and optimality. Both our planner and\ntrajectory generator outperforms state-of-the-art methods in terms of\ncomputational efficiency and guarantee obstacle-free trajectories. We validate\nour system through robust autonomous flight experiments in diverse indoor and\noutdoor environments, demonstrating its effectiveness for safe navigation in\npreviously unknown settings.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14975v1", "AI": {"title_translation": "基于学习的深度补全在非结构化环境中实现时间优化安全导航", "tldr": "开发了一种机载实时导航系统，利用学习型深度估计和新型规划框架，使四旋翼飞行器能在未知复杂环境中安全、时间优化地导航。", "motivation": "四旋翼飞行器在复杂和未知环境中自主安全导航面临挑战，特别是受限于尺寸、重量和功率的平台，无法使用笨重传感器（如LiDAR），且实时计算全局最优、无碰撞路径并转化为时间优化轨迹计算复杂。", "method": "提出一个完全机载、实时导航系统，仅依赖轻量级机载传感器。使用新颖的视觉深度估计方法构建密集3D地图，该方法融合了立体和单目学习型深度，生成比传统立体方法更远、更密、噪声更小的深度图。引入新颖的规划和轨迹生成框架，能够快速计算时间最优的全局轨迹。当地图增量更新时，系统持续优化轨迹以保持安全性和最优性。", "result": "系统构建的深度图比传统立体方法更远、更密、噪声更小。规划器和轨迹生成器在计算效率方面优于现有技术，并保证生成无障碍轨迹。通过在多样化的室内外环境中的自主飞行实验验证了系统的有效性，证明了其在未知环境中安全导航的能力。", "conclusion": "该系统通过创新的学习型深度估计和规划框架，实现了四旋翼飞行器在复杂未知环境中的时间优化安全导航，具有高计算效率和可靠性。", "translation": "四旋翼飞行器在农业、搜救和基础设施检查等多种应用中具有重要前景。实现自主操作要求系统能够在复杂和陌生的环境中安全导航。这种程度的自主性尤其具有挑战性，因为此类环境的复杂性以及对实时决策的需求，特别是对于受尺寸、重量和功率（SWaP）限制的平台，这限制了飞行时间并排除了使用笨重传感器（如激光雷达）进行地图构建的可能性。此外，实时计算全局最优、无碰撞路径并将其转化为时间优化的安全轨迹增加了显著的计算复杂性。为了应对这些挑战，我们提出了一种完全机载、实时导航系统，该系统仅依赖轻量级机载传感器。我们的系统利用一种新颖的视觉深度估计方法构建环境的密集3D地图，该方法融合了立体和单目基于学习的深度，生成比传统立体方法更远、更密集、噪声更小的深度图。在此地图的基础上，我们引入了一种新颖的规划和轨迹生成框架，能够快速计算时间最优的全局轨迹。随着地图通过新的深度信息增量更新，我们的系统不断优化轨迹以保持安全性和最优性。我们的规划器和轨迹生成器在计算效率方面均优于现有技术，并保证生成无障碍轨迹。我们通过在多样化的室内外环境中的鲁棒自主飞行实验验证了我们的系统，证明了其在先前未知环境中安全导航的有效性。", "summary": "本文提出了一种针对四旋翼飞行器的完全机载、实时导航系统，旨在解决在复杂未知环境中自主安全导航的挑战。该系统利用融合立体和单目学习型深度的新颖视觉深度估计方法构建高密度3D地图，并结合一个能够快速计算时间最优轨迹的规划和轨迹生成框架。该系统在计算效率和生成无障碍轨迹方面表现优异，并通过在多种室内外环境中的自主飞行实验验证了其在未知环境中安全导航的有效性。", "keywords": "四旋翼飞行器, 深度估计, 安全导航, 时间优化轨迹, 非结构化环境", "comments": "该研究的创新点在于其轻量级、完全机载的解决方案，通过融合立体和单目学习型深度来克服传统传感器（如LiDAR）的限制，同时实现了高效的时间优化轨迹规划。这对于尺寸、重量和功率受限的自主平台（如小型四旋翼）具有重要意义，拓宽了其在复杂环境中的应用潜力。"}}
{"id": "2506.15135", "title": "Towards Bug-Free Distributed Go Programs", "authors": ["Zhengqun Koo"], "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.", "comment": "Version 1. B.Comp. Dissertation", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15135v1", "AI": {"title_translation": "实现无bug的Go分布式程序", "tldr": "本文提出了一个验证框架，可以静态地证明使用Go语言子集的分布式程序中不存在通信竞争，通过扩展happens-before顺序来处理缓冲和非缓冲通道。", "motivation": "分布式系统中的并发推理非常困难，意外的竞争会导致bug。数据竞争检测在共享内存系统中已有深入研究，但通信竞争（在共享通道上收发操作与happens-before顺序无关）同样存在问题，可能导致接收方收到错误消息或无法接收任何消息。", "method": "本文提出了一个验证框架，用于证明Go语言子集（主要通过消息传递实现同步）的分布式程序中不存在通信竞争。该框架通过静态推理分布式程序的执行方式，并使用扩展到缓冲和非缓冲通道的happens-before顺序。", "result": "该验证框架能够证明分布式程序中不存在通信竞争。", "conclusion": "本文提出的验证框架能够有效证明使用Go语言子集的分布式程序中通信竞争的缺失，有助于提高分布式程序的可靠性。 (Based on the ability to prove absence of races)", "translation": "分布式系统程序员需要对并发进行推理以避免竞争。然而，对并发进行推理是困难的，意外的竞争表现为bug。共享内存系统中的数据竞争检测已得到充分研究（动态数据竞争检测[13]、行为类型[15]、动态竞争检测[31]）。类似于数据竞争由共享内存位置上不通过happens-before关联的读写操作组成，通信竞争由共享通道上不通过happens-before关联的接收和发送操作组成。通信竞争是有问题的：接收方期望从特定发送方接收特定消息，但在通信竞争下，接收方可能收到为其他接收方准备的消息，或者根本没有收到任何消息。在这项工作中，我们描述了一个验证框架，可以证明使用Go编程语言子集（其中同步主要通过消息传递实现）的分布式程序中不存在通信竞争。我们使用扩展到缓冲和非缓冲通道的happens-before顺序，静态地推理分布式程序的执行方式。", "summary": "本文提出了一种验证框架，旨在解决Go分布式程序中常见的通信竞争问题。通过静态分析和扩展的happens-before顺序，该框架能够证明使用Go语言子集（主要依赖消息传递）的分布式程序中不存在通信竞争。该方法有助于提高分布式系统的可靠性，避免因并发问题导致的bug。", "keywords": "通信竞争, Go语言, 分布式程序, 静态验证, happens-before顺序", "comments": "该论文的创新点在于提出了一个专门针对Go语言分布式程序中通信竞争的静态验证框架。它将happens-before顺序的概念扩展到Go的缓冲和非缓冲通道，这对于确保消息传递的正确性至关重要。其重要性在于能够帮助开发者在编译阶段发现并消除难以追踪的并发bug，从而显著提高分布式系统的健壮性。"}}
{"id": "2506.14820", "title": "Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction", "authors": ["Hyeon Jeon", "Hyunwook Lee", "Yun-Hsin Kuo", "Taehyun Yang", "Daniel Archambault", "Sungahn Ko", "Takanori Fujiwara", "Kwan-Liu Ma", "Jinwook Seo"], "summary": "Visual analytics using dimensionality reduction (DR) can easily be unreliable\nfor various reasons, e.g., inherent distortions in representing the original\ndata. The literature has thus proposed a wide range of methodologies to make\nDR-based visual analytics reliable. However, the diversity and extensiveness of\nthe literature can leave novice analysts and researchers uncertain about where\nto begin and proceed. To address this problem, we propose a guide for reading\npapers for reliable visual analytics with DR. Relying on the previous\nclassification of the relevant literature, our guide helps both practitioners\nto (1) assess their current DR expertise and (2) identify papers that will\nfurther enhance their understanding. Interview studies with three experts in DR\nand data visualizations validate the significance, comprehensiveness, and\nusefulness of our guide.", "comment": "EG/VGTC EuroVis 2025 Short paper", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14820v1", "AI": {"title_translation": "驾驭高维幕后：一份探索文献以可靠使用降维的指南", "tldr": "本文提出了一份阅读指南，旨在帮助新手分析师和研究人员理解并可靠地使用降维技术进行可视化分析，并通过专家访谈验证了其有效性。", "motivation": "由于固有的失真等原因，使用降维（DR）进行可视化分析容易变得不可靠。尽管现有文献提出了多种方法来提高基于DR的可视化分析的可靠性，但文献的多样性和广泛性使新手分析师和研究人员不知从何入手。", "method": "作者提出了一份阅读指南，用于阅读关于可靠视觉分析的降维论文。该指南基于现有相关文献的分类，帮助实践者（1）评估他们当前的降维专业知识，以及（2）识别能进一步提高他们理解的论文。通过对三位降维和数据可视化专家的访谈研究验证了该指南的重要性、全面性和实用性。", "result": "开发了一份针对可靠使用降维进行可视化分析的文献阅读指南，该指南能帮助用户评估自身专业知识并识别相关文献。通过专家访谈验证了该指南的显著性、全面性和实用性。", "conclusion": "本文提出并验证了一份旨在帮助用户可靠地使用降维技术进行可视化分析的文献阅读指南，有效解决了新手在面对大量相关文献时的困惑。", "translation": "使用降维（DR）进行可视化分析可能因各种原因（例如，表示原始数据时固有的失真）而容易变得不可靠。因此，文献中提出了多种方法来使基于DR的可视化分析变得可靠。然而，文献的多样性和广泛性可能使新手分析师和研究人员不确定从何开始和如何进行。为了解决这个问题，我们提出了一份关于阅读可靠的基于DR的可视化分析论文的指南。我们的指南基于对相关文献的先前分类，帮助实践者（1）评估他们当前的DR专业知识，并（2）识别将进一步增强他们理解的论文。对三位DR和数据可视化专家的访谈研究验证了我们指南的重要性、全面性和实用性。", "summary": "本文提出了一份针对可靠使用降维（DR）进行可视化分析的文献阅读指南。鉴于DR在可视化分析中可能存在不可靠性以及相关文献的庞杂，该指南旨在帮助新手分析师和研究人员评估其DR专业知识，并有效识别和利用相关文献以提升理解。通过对DR和数据可视化专家的访谈，验证了该指南的有效性和实用性。", "keywords": "降维, 可视化分析, 文献指南, 可靠性, 专家访谈", "comments": "这份论文提供了一个实用的工具，解决了在快速发展的研究领域中，新手研究人员和实践者面临的常见挑战——即如何有效筛选和利用大量现有文献。其创新点在于将文献分类与用户自身专业评估相结合，提供了一条清晰的学习路径。该指南对于促进降维技术在可视化分析中的可靠应用具有重要意义。"}}
{"id": "2506.15601", "title": "CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies", "authors": ["Donghyun Gouk", "Seungkwan Kang", "Seungjun Lee", "Jiseon Kim", "Kyungkuk Nam", "Eojin Ryu", "Sangwon Lee", "Dongpyung Kim", "Junhyeok Jang", "Hanyeoreum Bae", "Myoungsoo Jung"], "summary": "This work introduces a GPU storage expansion solution utilizing CXL,\nfeaturing a novel GPU system design with multiple CXL root ports for\nintegrating diverse storage media (DRAMs and/or SSDs). We developed and\nsiliconized a custom CXL controller integrated at the hardware RTL level,\nachieving two-digit nanosecond roundtrip latency, the first in the field. This\nstudy also includes speculative read and deterministic store mechanisms to\nefficiently manage read and write operations to hide the endpoint's backend\nmedia latency variation. Performance evaluations reveal our approach\nsignificantly outperforms existing methods, marking a substantial advancement\nin GPU storage technology.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15601v1", "AI": {"title_translation": "CXL-GPU：通过集成CXL技术拓展GPU内存边界", "tldr": "该研究提出了一种基于CXL的GPU存储扩展解决方案，通过定制CXL控制器和新颖的系统设计，实现了低延迟和高性能，显著优于现有方法。", "motivation": "该研究的动机是拓展GPU的内存边界，解决现有GPU存储技术的限制，以实现更高效的数据处理和存储。", "method": "本研究引入了一种利用CXL的GPU存储扩展解决方案，其特点是采用具有多个CXL根端口的新颖GPU系统设计，以集成不同存储介质（DRAM和/或SSD）。开发并硅化了一个在硬件RTL级别集成的定制CXL控制器，实现了两位数的纳秒级往返延迟。研究还包括推测性读取和确定性存储机制，以有效管理读写操作，隐藏端点后端介质的延迟变化。", "result": "性能评估显示，该方法显著优于现有方法，并且定制的CXL控制器实现了两位数的纳秒级往返延迟，这是该领域的首次突破。", "conclusion": "通过集成CXL技术和创新的系统设计，该方法在GPU存储技术方面取得了实质性进展，显著提升了性能并实现了极低的延迟。", "translation": "这项工作引入了一种利用CXL的GPU存储扩展解决方案，其特点是采用具有多个CXL根端口的新颖GPU系统设计，用于集成各种存储介质（DRAM和/或SSD）。我们开发并硅化了一个在硬件RTL级别集成的定制CXL控制器，实现了两位数的纳秒级往返延迟，这是该领域的首次突破。这项研究还包括推测性读取和确定性存储机制，以有效管理读写操作，从而隐藏端点后端介质的延迟变化。性能评估显示，我们的方法显著优于现有方法，标志着GPU存储技术取得了实质性进展。", "summary": "本研究提出了一种创新的CXL-GPU存储扩展方案，通过设计带有多个CXL根端口的GPU系统和定制硬件RTL级别的CXL控制器，实现了对DRAM和SSD等多种存储介质的集成。该方案首次实现了两位数纳秒级的往返延迟，并引入了推测性读取和确定性存储机制来优化读写操作。性能评估表明，该方法显著超越了现有技术，代表了GPU存储领域的一个重大进步。", "keywords": "CXL, GPU, 内存扩展, 低延迟, 存储技术", "comments": "这项研究的创新之处在于首次将定制的CXL控制器集成到硬件RTL层面，并实现了极低的两位数纳秒级往返延迟，这在CXL应用于GPU存储扩展方面是一个突破。其提出的系统设计和读写优化机制也提升了性能和效率，对未来GPU存储技术的发展具有重要意义。"}}
{"id": "2506.15167", "title": "LLM Agent for Hyper-Parameter Optimization", "authors": ["Wanzhe Wang", "Jianqiu Peng", "Menghao Hu", "Weihuang Zhong", "Tong Zhang", "Shuai Wang", "Yixin Zhang", "Mingjie Shao", "Wanli Ni"], "summary": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters tuning methods for\nwarm-start particles swarm optimization with cross and mutation (WS-PSO-CM)\nalgortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and\ncommunication are primarily heuristic-based, exhibiting low levels of\nautomation and unsatisfactory performance. In this paper, we design an large\nlanguage model (LLM) agent for automatic hyper-parameters-tuning, where an\niterative framework and model context protocol (MCP) are applied. In\nparticular, the LLM agent is first setup via a profile, which specifies the\nmission, background, and output format. Then, the LLM agent is driven by the\nprompt requirement, and iteratively invokes WS-PSO-CM algorithm for\nexploration. Finally, the LLM agent autonomously terminates the loop and\nreturns a set of hyper-parameters. Our experiment results show that the minimal\nsum-rate achieved by hyper-parameters generated via our LLM agent is\nsignificantly higher than those by both human heuristics and random generation\nmethods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM\nalgorithm background is useful in finding high-performance hyper-parameters.", "comment": "6 pages, 6 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15167v1", "AI": {"title_translation": "LLM智能体用于超参数优化", "tldr": "本文提出了一种基于LLM智能体的方法，用于自动优化WS-PSO-CM算法的超参数，并在无人机通信场景中取得了优于启发式和随机方法的性能。", "motivation": "当前用于无线电地图赋能的无人机轨迹与通信中，WS-PSO-CM算法的超参数调整方法主要基于启发式，自动化程度低且性能不佳。", "method": "设计了一个大型语言模型（LLM）智能体用于自动超参数调整，该智能体应用了迭代框架和模型上下文协议（MCP）。LLM智能体首先通过配置文件进行设置，指定任务、背景和输出格式，然后由提示要求驱动，迭代调用WS-PSO-CM算法进行探索，最终自主终止循环并返回一组超参数。", "result": "实验结果表明，通过LLM智能体生成的超参数所达到的最小和速率显著高于人类启发式方法和随机生成方法。", "conclusion": "具有PSO知识和WS-PSO-CM算法背景的LLM智能体在寻找高性能超参数方面非常有用。", "translation": "超参数对于通信算法的性能至关重要。然而，当前用于无线电地图赋能的无人机（UAV）轨迹与通信中，带交叉与变异的温启动粒子群优化（WS-PSO-CM）算法的超参数调整方法主要基于启发式，自动化程度低且性能不佳。在本文中，我们设计了一个大型语言模型（LLM）智能体用于自动超参数调整，其中应用了迭代框架和模型上下文协议（MCP）。具体而言，LLM智能体首先通过配置文件进行设置，该配置文件指定了任务、背景和输出格式。然后，LLM智能体由提示要求驱动，迭代调用WS-PSO-CM算法进行探索。最后，LLM智能体自主终止循环并返回一组超参数。我们的实验结果表明，通过LLM智能体生成的超参数所达到的最小和速率显著高于人类启发式方法和随机生成方法。这表明具有PSO知识和WS-PSO-CM算法背景的LLM智能体在寻找高性能超参数方面非常有用。", "summary": "本文提出了一种基于大型语言模型（LLM）智能体的自动超参数调整方法，专门针对用于无线电地图赋能的无人机轨迹与通信中的WS-PSO-CM算法。该LLM智能体采用迭代框架和模型上下文协议（MCP），通过配置文件初始化，并根据提示迭代调用WS-PSO-CM算法进行探索，最终自主输出优化的超参数。实验证明，与传统的人类启发式和随机生成方法相比，该LLM智能体能显著提高超参数所实现的最小和速率。", "keywords": "LLM智能体, 超参数优化, WS-PSO-CM, 无人机通信", "comments": "本文创新性地将LLM智能体应用于通信算法的超参数优化问题，特别是在无人机通信场景下，展示了LLM在自动化和提升复杂优化任务性能方面的潜力。其迭代框架和模型上下文协议是实现这一能力的关键设计。这一研究为LLM在工程优化领域的应用开辟了新的方向。"}}
{"id": "2506.15155", "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving", "authors": ["Jiale Xu", "Rui Zhang", "Yi Xiong", "Cong Guo", "Zihan Liu", "Yangjie Zhou", "Weiming Hu", "Hao Wu", "Changxu Shao", "Ziqing Wang", "Yongjie Yuan", "Junping Zhao", "Minyi Guo", "Jingwen Leng"], "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15155v1", "AI": {"title_translation": "eLLM：用于高效LLM服务的弹性内存管理框架", "tldr": "eLLM是一个弹性内存管理框架，通过统一内存池、弹性内存机制和轻量级调度策略，显著提高了LLM服务的吞吐量和批处理大小，解决了现有系统内存利用率低下的问题。", "motivation": "现有的大型语言模型（LLM）服务系统在处理运行时内存和KV缓存时，采用分离的抽象级别，导致在动态工作负载下内存利用率低下，吞吐量下降近20%。", "method": "本文提出了eLLM，一个受操作系统经典内存膨胀机制启发的弹性内存管理框架。其核心组件包括：1) 虚拟张量抽象，将张量的虚拟地址空间与物理GPU内存解耦，创建统一灵活的内存池；2) 弹性内存机制，通过运行时内存膨胀和收缩动态调整内存分配，利用CPU内存作为可扩展缓冲区；3) 轻量级调度策略，采用SLO感知策略优化内存利用率并平衡性能权衡。", "result": "eLLM显著优于现有最先进的系统，解码吞吐量提高了2.32倍，对于128K令牌输入支持3倍更大的批处理大小。", "conclusion": "eLLM通过其创新的弹性内存管理框架，有效解决了LLM服务中内存利用率低下的问题，显著提升了吞吐量和批处理能力。", "translation": "大型语言模型正越来越多地部署在数据中心。服务这些模型需要仔细的内存管理，因为它们的内存使用包括静态权重、动态激活和键值缓存。虽然静态权重是恒定且可预测的，但动态组件（如激活和KV缓存）在运行时频繁变化，给高效内存管理带来了巨大挑战。现代LLM服务系统通常在不同的抽象级别处理运行时内存和KV缓存：运行时内存管理依赖于静态张量抽象，而KV缓存则利用在张量抽象之上构建的基于页表的虚拟化层。这种虚拟化动态管理KV缓存以减轻内存碎片。然而，这种双层方法从根本上隔离了运行时内存和KV缓存管理，导致在动态工作负载下内存利用率不佳，这可能导致吞吐量下降近20%。\n为了解决这些限制，我们提出了eLLM，一个受操作系统经典内存膨胀机制启发的弹性内存管理框架。eLLM的核心组件包括：(1) 虚拟张量抽象，它将张量的虚拟地址空间与物理GPU内存解耦，创建一个统一且灵活的内存池；(2) 弹性内存机制，通过运行时内存膨胀和收缩动态调整内存分配，利用CPU内存作为可扩展缓冲区；(3) 轻量级调度策略，采用SLO感知策略优化内存利用率并有效平衡严格SLO约束下的性能权衡。全面的评估表明，eLLM显著优于最先进的系统，解码吞吐量提高了2.32倍，并支持128K令牌输入3倍更大的批处理大小。", "summary": "eLLM是一个创新的弹性内存管理框架，旨在解决大型语言模型（LLM）服务中由于现有双层内存管理方法导致的低内存利用率和吞吐量下降问题。eLLM通过引入虚拟张量抽象、弹性内存机制（利用CPU内存作为缓冲区）和轻量级SLO感知调度策略，统一并优化了运行时内存和KV缓存的管理。实验结果表明，eLLM在解码吞吐量上实现了2.32倍的提升，并能支持3倍更大的批处理大小，显著优于现有系统。", "keywords": "LLM服务, 内存管理, 弹性内存, KV缓存, 吞吐量", "comments": "eLLM通过借鉴操作系统中的内存膨胀机制，提出了一种新颖的、统一的LLM内存管理方法。其创新点在于虚拟张量抽象和CPU内存作为可扩展缓冲区的利用，有效解决了动态LLM工作负载下的内存碎片和利用率低下问题。这项工作对于提高LLM服务效率和降低部署成本具有重要意义。"}}
{"id": "2506.14825", "title": "GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction", "authors": ["Ke Song", "Yunhe Wu", "Chunchit Siu", "Huiyuan Xiong"], "summary": "Addressing the task of 3D semantic occupancy prediction for autonomous\ndriving, we tackle two key issues in existing 3D Gaussian Splating (3DGS)\nmethods: (1) unified feature aggregation neglecting semantic correlations among\nsimilar categories and across regions, and (2) boundary ambiguities caused by\nthe lack of geometric constraints in MLP iterative optimization. We propose the\nGraphGSOcc model, a novel framework that combines semantic and geometric graph\nTransformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the\nDual Gaussians Graph Attenntion, which dynamically constructs dual graph\nstructures: a geometric graph adaptively calculating KNN search radii based on\nGaussian poses, enabling large-scale Gaussians to aggregate features from\nbroader neighborhoods while compact Gaussians focus on local geometric\nconsistency; a semantic graph retaining top-M highly correlated nodes via\ncosine similarity to explicitly encode semantic relationships within and across\ninstances. Coupled with the Multi-scale Graph Attention framework, fine-grained\nattention at lower layers optimizes boundary details, while coarse-grained\nattention at higher layers models object-level topology. Experiments on the\nSurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB,\ndemonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to\nGaussianWorld", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14825v1", "AI": {"title_translation": "GraphGSOcc: 基于3D高斯泼溅的语义和几何图Transformer的占用预测", "tldr": "GraphGSOcc引入语义和几何图Transformer，解决了3DGS在自动驾驶3D语义占用预测中特征聚合不足和边界模糊的问题。", "motivation": "现有3D高斯泼溅（3DGS）方法在自动驾驶3D语义占用预测任务中存在两个主要问题：(1) 统一特征聚合忽略了相似类别和区域间的语义关联；(2) MLP迭代优化中缺乏几何约束导致边界模糊。", "method": "本文提出了GraphGSOcc模型，一个结合语义和几何图Transformer的新颖框架。核心是“双高斯图注意力”（Dual Gaussians Graph Attention），它动态构建几何图（根据高斯姿态自适应计算KNN搜索半径，实现大高斯聚合更广范围特征，紧凑高斯关注局部几何一致性）和语义图（通过余弦相似度保留Top-M高度相关节点，明确编码实例内部和跨实例的语义关系）。此外，采用多尺度图注意力框架，低层关注边界细节，高层建模对象级拓扑。", "result": "在SurroundOcc数据集上，GraphGSOcc的mIoU达到24.10%，GPU内存减少到6.1 GB。相比GaussianWorld，mIoU提升了1.97%，内存减少了13.7%。", "conclusion": "GraphGSOcc通过结合语义和几何图Transformer，有效解决了3DGS在自动驾驶3D语义占用预测中的特征聚合和边界模糊问题，并在性能和内存效率上取得了显著提升。", "translation": "针对自动驾驶中3D语义占用预测任务，我们解决了现有3D高斯泼溅（3DGS）方法中的两个关键问题：(1) 统一特征聚合忽略了相似类别和跨区域的语义关联，以及 (2) MLP迭代优化中缺乏几何约束导致的边界模糊。我们提出了GraphGSOcc模型，这是一个结合语义和几何图Transformer的3D高斯泼溅占用预测的新颖框架。我们提出了双高斯图注意力（Dual Gaussians Graph Attention），它动态构建双图结构：一个几何图，根据高斯姿态自适应计算KNN搜索半径，使大规模高斯能够从更广阔的邻域聚合特征，而紧凑型高斯则专注于局部几何一致性；一个语义图，通过余弦相似度保留Top-M高度相关节点，以明确编码实例内部和跨实例的语义关系。结合多尺度图注意力框架，较低层的细粒度注意力优化边界细节，而较高层的粗粒度注意力则建模对象级拓扑。在SurroundOcc数据集上的实验结果显示，mIoU达到24.10%，GPU内存减少到6.1 GB，与GaussianWorld相比，mIoU提升了1.97%，内存减少了13.7%。", "summary": "本文提出了GraphGSOcc模型，一个结合语义和几何图Transformer的新颖框架，用于解决自动驾驶中3D高斯泼溅（3DGS）方法在3D语义占用预测中面临的特征聚合不足和边界模糊问题。该模型引入了双高斯图注意力，通过动态构建几何图和语义图来分别处理几何约束和语义关联，并结合多尺度图注意力优化不同粒度的特征。实验结果表明，GraphGSOcc在性能和内存效率上均优于现有方法。", "keywords": "3D高斯泼溅, 占用预测, 图Transformer, 语义图, 几何图", "comments": "本文的创新点在于将图Transformer引入3D高斯泼溅，并设计了独特的双高斯图注意力机制，同时考虑了语义和几何信息。多尺度图注意力也有效提升了模型对细节和全局拓扑的捕捉能力。这对于提高自动驾驶场景理解的准确性和效率具有重要意义。"}}
{"id": "2506.15278", "title": "Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing", "authors": ["Reuben Binns", "Jake Stein", "Siddhartha Datta", "Max Van Kleek", "Nigel Shadbolt"], "summary": "Ride-sharing platforms like Uber market themselves as enabling `flexibility'\nfor their workforce, meaning that drivers are expected to anticipate when and\nwhere the algorithm will allocate them jobs, and how well remunerated those\njobs will be. In this work we describe our process of participatory action\nresearch with drivers and trade union organisers, culminating in a\nparticipatory audit of Uber's algorithmic pay and work allocation, before and\nafter the introduction of dynamic pricing. Through longitudinal analysis of 1.5\nmillion trips from 258 drivers in the UK, we find that after dynamic pricing,\npay has decreased, Uber's cut has increased, job allocation and pay is less\npredictable, inequality between drivers is increased, and drivers spend more\ntime waiting for jobs. In addition to these findings, we provide methodological\nand theoretical contributions to algorithm auditing, gig work, and the emerging\npractice of worker data science.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15278v1", "AI": {"title_translation": "即使能得到也非美差；一项关于优步算法薪酬和定价的纵向研究", "tldr": "一项对优步的纵向研究发现，引入动态定价后，司机薪酬下降，优步抽成增加，工作分配和薪酬更不可预测，司机间不平等加剧，等待时间更长。", "motivation": "为了调查优步的算法薪酬和工作分配，特别是动态定价引入后，对司机的影响，挑战平台所宣称的“灵活性”。", "method": "通过与司机和工会组织者进行参与式行动研究，对优步的算法薪酬和工作分配进行了参与式审计。对英国258名司机的150万次行程进行了纵向分析。", "result": "动态定价后，司机薪酬下降，优步的抽成增加，工作分配和薪酬变得更不可预测，司机之间的不平等加剧，司机等待工作的时间更长。", "conclusion": "动态定价对优步司机的收入、可预测性和公平性产生了负面影响，与“灵活性”的叙述相悖。本研究也为算法审计、零工经济和工人数据科学做出了方法论和理论贡献。", "translation": "优步等网约车平台将自己宣传为能为劳动力提供“灵活性”，这意味着司机需要预判算法何时何地分配工作给他们，以及这些工作的报酬如何。在这项工作中，我们描述了与司机和工会组织者进行参与式行动研究的过程，最终在动态定价引入前后对优步的算法薪酬和工作分配进行了参与式审计。通过对英国258名司机150万次行程的纵向分析，我们发现动态定价后，薪酬下降了，优步的抽成增加了，工作分配和薪酬变得更不可预测，司机之间的不平等加剧了，司机等待工作的时间也更长了。除了这些发现，我们还为算法审计、零工经济以及新兴的工人数据科学实践提供了方法论和理论贡献。", "summary": "本研究通过与英国司机进行参与式行动研究，对优步的算法薪酬和定价进行了纵向研究，分析了150万次行程。研究发现，引入动态定价后，司机薪酬下降，优步抽成增加，工作可预测性和公平性恶化，等待时间延长。本研究还为算法审计、零工经济和工人数据科学提供了方法论和理论贡献。", "keywords": "算法薪酬, 零工经济, 优步, 动态定价, 参与式审计", "comments": "本论文通过参与式行动研究和大规模纵向数据集对主要零工经济平台的算法进行审计，具有创新性。它提供了批判性的实证证据，反驳了“灵活性”的叙述，突出了算法管理对工人经济福祉和工作条件的负面影响。其对算法审计和工人数据科学的贡献也十分重要。"}}
{"id": "2506.15106", "title": "Local Differential Privacy for Distributed Stochastic Aggregative Optimization with Guaranteed Optimality", "authors": ["Ziqin Chen", "Yongqiang Wang"], "summary": "Distributed aggregative optimization underpins many cooperative optimization\nand multi-agent control systems, where each agent's objective function depends\nboth on its local optimization variable and an aggregate of all agents'\noptimization variables. Existing distributed aggregative optimization\napproaches typically require access to accurate gradients of the objective\nfunctions, which, however, are often hard to obtain in real-world applications.\nFor example, in machine learning, gradients are commonly contaminated by two\nmain sources of noise: the randomness inherent in sampled data, and the\nadditional variability introduced by mini-batch computations. In addition to\nthe issue of relying on accurate gradients, existing distributed aggregative\noptimization approaches require agents to share explicit information, which\ncould breach the privacy of participating agents. We propose an algorithm that\ncan solve both problems with existing distributed aggregative optimization\napproaches: not only can the proposed algorithm guarantee mean-square\nconvergence to an exact optimal solution when the gradients are subject to\nnoise, it also simultaneously ensures rigorous differential privacy, with the\ncumulative privacy budget guaranteed to be finite even when the number of\niterations tends to infinity. To the best of our knowledge, this is the first\nalgorithm able to guarantee both accurate convergence and rigorous differential\nprivacy in distributed aggregative optimization. Besides characterizing the\nconvergence rates under nonconvex/convex/strongly convex conditions, we also\nrigorously quantify the cost of differential privacy in terms of convergence\nrates. Experimental results on personalized machine learning using benchmark\ndatasets confirm the efficacy of the proposed algorithm.", "comment": "21 pages, 6 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15106v1", "AI": {"title_translation": "具有保证最优性的分布式随机聚合优化中的局部差分隐私", "tldr": "本文提出了一种用于分布式聚合优化的新算法，首次同时保证了在梯度存在噪声情况下的精确收敛性以及严格的差分隐私性，且累积隐私预算有限。", "motivation": "现有的分布式聚合优化方法通常需要精确的梯度（在实际机器学习应用中常受噪声污染）并且需要明确的信息共享，这会侵犯参与者的隐私。", "method": "本文提出了一种算法，能够同时解决梯度噪声和隐私问题。该算法在梯度受噪声影响时能保证均方收敛到精确最优解，同时确保严格的差分隐私，即使迭代次数趋于无穷大，累积隐私预算也保证是有限的。此外，它还表征了在非凸/凸/强凸条件下的收敛速度，并严格量化了差分隐私在收敛速度方面的成本。", "result": "所提出的算法在梯度存在噪声的情况下，能够保证均方收敛到精确的最优解，并同时确保严格的差分隐私，且累积隐私预算是有限的。据称，这是首个能同时实现精确收敛和严格差分隐私的算法。在个性化机器学习基准数据集上的实验结果证实了该算法的有效性。", "conclusion": "本文提出了一种新颖的算法，在分布式聚合优化中同时实现了精确收敛和严格差分隐私，并通过理论分析和实验验证了其有效性。", "translation": "分布式聚合优化是许多协作优化和多智能体控制系统的基础，其中每个智能体的目标函数都取决于其局部优化变量和所有智能体优化变量的聚合。现有的分布式聚合优化方法通常需要访问目标函数的精确梯度，然而这在实际应用中往往难以获得。例如，在机器学习中，梯度通常受到两种主要噪声源的污染：采样数据固有的随机性，以及小批量计算引入的额外变异性。除了依赖精确梯度的问题外，现有的分布式聚合优化方法还需要智能体共享明确信息，这可能侵犯参与智能体的隐私。我们提出了一种算法，可以解决现有分布式聚合优化方法的这两个问题：所提出的算法不仅可以在梯度受噪声影响时保证均方收敛到精确的最优解，而且同时确保严格的差分隐私，即使迭代次数趋于无穷大，累积隐私预算也保证是有限的。据我们所知，这是第一个能够在分布式聚合优化中同时保证精确收敛和严格差分隐私的算法。除了描述在非凸/凸/强凸条件下的收敛速度外，我们还严格量化了差分隐私在收敛速度方面的成本。使用基准数据集进行个性化机器学习的实验结果证实了所提出算法的有效性。", "summary": "本文提出了一种用于分布式聚合优化的新算法，旨在克服现有方法对精确梯度的依赖和隐私泄露问题。该算法在梯度存在噪声的情况下，能够保证均方收敛到精确最优解，并同时确保严格的差分隐私，且累积隐私预算有限。据称，这是首个能同时实现这两个目标的算法。论文还分析了不同凸性条件下的收敛速度，并量化了差分隐私对收敛速度的影响。在个性化机器学习上的实验结果验证了该算法的有效性。", "keywords": "分布式优化, 差分隐私, 随机梯度, 聚合优化, 机器学习", "comments": "该论文通过首次在分布式聚合优化中同时解决了噪声梯度鲁棒性和严格差分隐私这两个关键挑战，取得了显著进展。其创新性在于实现了在随机条件下的精确收敛，同时保持了有界隐私预算，这对于实际应用，特别是像个性化机器学习这样对隐私敏感的领域至关重要。"}}
{"id": "2506.15136", "title": "Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead", "authors": ["Kehui Li", "Binggui Zhou", "Jiajia Guo", "Feifei Gao", "Guanghua Yang", "Shaodan Ma"], "summary": "Multi-user millimeter-wave communication relies on narrow beams and dense\ncell deployments to ensure reliable connectivity. However, tracking optimal\nbeams for multiple mobile users across multiple base stations (BSs) results in\nsignificant signaling overhead. Recent works have explored the capability of\nout-of-band (OOB) modalities in obtaining spatial characteristics of wireless\nchannels and reducing pilot overhead in single-BS single-user/multi-user\nsystems. However, applying OOB modalities for multi-BS selection towards dense\ncell deployments leads to high coordination overhead, i.e, excessive computing\noverhead and high latency in data exchange. How to leverage OOB modalities to\neliminate pilot overhead and achieve efficient multi-BS coordination in\nmulti-BS systems remains largely unexplored. In this paper, we propose a novel\nOOB modality synergy (OMS) based mobility management scheme to realize\nmulti-user beam prediction and proactive BS selection by synergizing two OOB\nmodalities, i.e., vision and location. Specifically, mobile users are initially\nidentified via spatial alignment of visual sensing and location feedback, and\nthen tracked according to the temporal correlation in image sequence.\nSubsequently, a binary encoding map based gain and beam prediction network\n(BEM-GBPN) is designed to predict beamforming gains and optimal beams for\nmobile users at each BS, such that a central unit can control the BSs to\nperform user handoff and beam switching. Simulation results indicate that the\nproposed OMS-based mobility management scheme enhances beam prediction and BS\nselection accuracy and enables users to achieve 91% transmission rates of the\noptimal with zero pilot overhead and significantly improve multi-BS\ncoordination efficiency compared to existing methods.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15136v1", "AI": {"title_translation": "基于带外模态协同的多用户波束预测和零导频开销的主动式基站选择", "tldr": "本文提出了一种基于带外模态协同（视觉+定位）的移动性管理方案，用于毫米波通信中的多用户波束预测和主动式基站选择，实现了零导频开销下的高传输速率和更高的多基站协调效率。", "motivation": "多用户毫米波通信中，在多个基站间为多个移动用户跟踪最佳波束会产生显著的信令开销。现有带外（OOB）模态虽能减少单基站系统中的导频开销，但应用于多基站选择会导致高协调开销。如何在多基站系统中利用OOB模态消除导频开销并实现高效多基站协调仍是一个未充分探索的问题。", "method": "本文提出了一种基于带外模态协同（OMS）的移动性管理方案。该方案协同视觉和定位两种OOB模态：首先通过视觉感知和定位反馈的空间对齐识别移动用户，然后根据图像序列的时间相关性进行跟踪。随后，设计了一个基于二进制编码图的增益和波束预测网络（BEM-GBPN），用于预测每个基站处移动用户的波束成形增益和最佳波束，以实现中央单元对用户切换和波束切换的控制。", "result": "所提出的OMS移动性管理方案提高了波束预测和基站选择的准确性。仿真结果表明，该方案使能用户在零导频开销下达到最佳传输速率的91%，并与现有方法相比显著提高了多基站协调效率。", "conclusion": "基于带外模态协同的移动性管理方案，通过协同视觉和定位模态，有效解决了多基站毫米波系统中多用户波束预测和主动式基站选择的挑战，实现了零导频开销下的高性能和更高的协调效率。", "translation": "多用户毫米波通信依赖窄波束和密集小区部署来确保可靠连接。然而，在多个基站（BS）之间为多个移动用户跟踪最佳波束会导致显著的信令开销。最近的工作探索了带外（OOB）模态在获取无线信道空间特性和减少单基站单用户/多用户系统中的导频开销方面的能力。然而，将OOB模态应用于密集小区部署的多基站选择会导致高协调开销，即过多的计算开销和数据交换中的高延迟。如何利用OOB模态消除导频开销并在多基站系统中实现高效的多基站协调，仍未得到充分探索。在本文中，我们提出了一种新颖的基于OOB模态协同（OMS）的移动性管理方案，通过协同视觉和定位两种OOB模态，实现多用户波束预测和主动式基站选择。具体而言，移动用户首先通过视觉感知和定位反馈的空间对齐进行识别，然后根据图像序列中的时间相关性进行跟踪。随后，设计了一个基于二进制编码图的增益和波束预测网络（BEM-GBPN），用于预测每个基站处移动用户的波束成形增益和最佳波束，从而使中央单元能够控制基站执行用户切换和波束切换。仿真结果表明，所提出的基于OMS的移动性管理方案提高了波束预测和基站选择的准确性，并使用户能够在零导频开销的情况下达到最佳传输速率的91%，与现有方法相比显著提高了多基站协调效率。", "summary": "本文提出了一种基于带外模态协同（OMS）的移动性管理方案，用于多用户毫米波通信。该方案利用视觉和定位数据，在零导频开销下预测最佳波束并主动选择基站。它通过视觉感知和定位反馈识别并跟踪用户，并采用BEM-GBPN网络预测波束成形增益。仿真结果表明，该方案提高了波束预测和基站选择的准确性，实现了接近最优的传输速率，并增强了多基站协调效率。", "keywords": "带外模态, 波束预测, 基站选择, 毫米波, 移动性管理", "comments": "该论文提出了一种创新方法，通过协同两种不同的带外模态（视觉和定位）来解决多基站毫米波系统中信令开销和协调复杂性的关键挑战。其优势在于实现了零导频开销并显著提高了效率，这是未来密集蜂窝网络的重要进展。文中提出的基于二进制编码图的预测网络也是一个值得关注的技术贡献。"}}
{"id": "2506.15379", "title": "Tractable Graph Structures in EFX Orientation", "authors": ["Václav Blažej", "Sushmita Gupta", "M. S. Ramanujan", "Peter Strulo"], "summary": "Since its introduction, envy-freeness up to any good (EFX) has become a\nfundamental solution concept in fair division of indivisible goods. Its\nexistence remains elusive -- even for four agents with additive utility\nfunctions, it is unknown whether an EFX allocation always exists.\nUnsurprisingly, restricted settings to delineate tractable and intractable\ncases have been explored. Christadolou, Fiat et al.[EC'23] introduced the\nnotion of EFX-orientation, where the agents form the vertices of a graph and\nthe items correspond to edges, and an agent values only the items that are\nincident to it. The goal is to allocate items to one of the adjacent agents\nwhile satisfying the EFX condition.\n  Building on the work of Zeng and Mehta'24, which established a sharp\ncomplexity threshold based on the structure of the underlying graph --\npolynomial-time solvability for bipartite graphs and NP-hardness for graphs\nwith chromatic number at least three -- we further explore the algorithmic\nlandscape of EFX-orientation using parameterized graph algorithms.\n  Specifically, we show that bipartiteness is a surprisingly stringent\ncondition for tractability: EFX orientation is NP-complete even when the\nvaluations are symmetric, binary and the graph is at most two edge-removals\naway from being bipartite. Moreover, introducing a single non-binary value\nmakes the problem NP-hard even when the graph is only one edge removal away\nfrom being bipartite. We further perform a parameterized analysis to examine\nstructures of the underlying graph that enable tractability. In particular, we\nshow that the problem is solvable in linear time on graphs whose treewidth is\nbounded by a constant and that the complexity of an instance is closely tied to\nthe sizes of acyclic connected components on its one-valued edges.", "comment": null, "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.15379v1", "AI": {"title_translation": "可处理的EFX定向图结构", "tldr": "EFX定向问题在图结构上的可处理性研究，发现二分性是严格条件，但在树宽有界图上可线性时间求解。", "motivation": "EFX分配的存在性问题难以解决，即使在限制条件下也需要探索可处理和不可处理的边界。本文旨在通过参数化图算法进一步探索EFX定向的算法图景，尤其是在Zeng和Mehta'24工作的基础上。", "method": "采用参数化图算法来分析EFX定向问题，具体通过考察图结构（如与二分图的距离、树宽）和估值类型（对称、二元、非二元）对问题复杂度的影响。", "result": "二分性是可处理性的一个非常严格的条件：即使估值对称且为二元，且图距离二分图最多两次边移除，EFX定向问题仍是NP完全的。引入单个非二元估值会使问题变为NP难，即使图距离二分图仅有一次边移除的距离。在树宽有界的图上，问题可以在线性时间内解决。实例的复杂度与图上其单值边的无环连通分量的大小密切相关。", "conclusion": "本文通过参数化图算法深入分析了EFX定向问题的可处理性，揭示了二分性作为可处理条件的高度严格性，并识别出树宽有界图等结构可以实现线性时间求解，表明问题复杂性与图的特定结构特征紧密相关。", "translation": "自引入以来，无嫉妒至任何物品（EFX）已成为不可分割物品公平分配中的一个基本解决方案概念。它的存在性仍然难以捉摸——即使对于具有加性效用函数的四个代理，EFX分配是否总是存在仍然未知。\n毫不奇怪，人们已经探索了限制性设置来划分可处理和不可处理的情况。Christadolou、Fiat 等人 [EC'23] 引入了 EFX-定向的概念，其中代理构成图的顶点，物品对应于边，并且代理只评估与其相邻的物品。目标是在满足 EFX 条件的同时将物品分配给其中一个相邻的代理。\n在 Zeng 和 Mehta'24 的工作基础上，该工作根据底层图的结构建立了尖锐的复杂性阈值——二分图的多项式时间可解性，以及色数至少为三的图的 NP-难性——我们使用参数化图算法进一步探索 EFX-定向的算法图景。\n具体来说，我们表明二分性是可处理性的一个令人惊讶的严格条件：即使估值是对称的、二元的，并且图距离二分图最多两次边移除，EFX 定向仍然是 NP 完全的。此外，引入单个非二元值会使问题变为 NP 难，即使图距离二分图仅有一次边移除。我们进一步进行参数化分析，以检查底层图的结构，这些结构能够实现可处理性。特别是，我们表明该问题可以在树宽受常数限制的图上以线性时间解决，并且实例的复杂性与图上其单值边的无环连通分量的大小密切相关。", "summary": "本文深入研究了不可分割物品公平分配中EFX定向问题的可处理性，该问题将代理建模为图顶点，物品为边。在现有工作的基础上，作者利用参数化图算法分析了不同图结构对问题复杂度的影响。研究发现，二分性对于可处理性而言是一个极其严格的条件，即使在估值受限且图接近二分图时，问题仍可能是NP完全或NP难。然而，论文也指出，在树宽有界的图上，EFX定向问题可以实现线性时间求解，并揭示了问题复杂性与单值边构成的无环连通分量大小的关联。", "keywords": "EFX定向, 公平分配, 图算法, NP完全, 树宽", "comments": "这篇论文对EFX定向问题的复杂性边界进行了细致的刻画，特别是在图结构和估值类型的影响下。它通过参数化图算法揭示了二分图作为可处理条件的高度严格性，并识别了树宽有界图等新的可处理类别，为理解和解决此类公平分配问题提供了重要的理论基础和算法洞察。研究结果对于设计更高效的公平分配算法具有指导意义。"}}
{"id": "2506.14784", "title": "Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation", "authors": ["Emre Yilmaz", "Philipp Bekemeyer"], "summary": "Determining onflow parameters is crucial from the perspectives of wind tunnel\ntesting and regular flight and wind turbine operations. These parameters have\ntraditionally been predicted via direct measurements which might lead to\nchallenges in case of sensor faults. Alternatively, a data-driven prediction\nmodel based on surface pressure data can be used to determine these parameters.\nIt is essential that such predictors achieve close to real-time learning as\ndictated by practical applications such as monitoring wind tunnel operations or\nlearning the variations in aerodynamic performance of aerospace and wind energy\nsystems. To overcome the challenges caused by changes in the data distribution\nas well as in adapting to a new prediction task, we propose a transfer learning\nmethodology to predict the onflow parameters, specifically angle of attack and\nonflow speed. It requires first training a convolutional neural network\n(ConvNet) model offline for the core prediction task, then freezing the weights\nof this model except the selected layers preceding the output node, and finally\nexecuting transfer learning by retraining these layers. A demonstration of this\napproach is provided using steady CFD analysis data for an airfoil for i)\ndomain adaptation where transfer learning is performed with data from a target\ndomain having different data distribution than the source domain and ii) task\nadaptation where the prediction task is changed. Further exploration on the\ninfluence of noisy data, performance on an extended domain, and trade studies\nvarying sampling sizes and architectures are provided. Results successfully\ndemonstrate the potential of the approach for adaptation to changing data\ndistribution, domain extension, and task update while the application for noisy\ndata is concluded to be not as effective.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14784v1", "AI": {"title_translation": "使用迁移学习进行领域和任务适应的迎面流参数预测", "tldr": "本文提出一种基于迁移学习的方法，通过离线训练卷积神经网络并微调特定层，用于实时预测迎面流参数（如攻角和迎面流速），以克服数据分布变化和新任务适应的挑战，并在域适应和任务适应中展示了潜力，但对噪声数据效果不佳。", "motivation": "确定迎面流参数对于风洞测试、常规飞行和风力涡轮机操作至关重要。传统直接测量方法可能因传感器故障而面临挑战。需要一种基于表面压力数据的、能实现接近实时学习的数据驱动预测模型。现有模型难以应对数据分布变化以及适应新的预测任务。", "method": "本文提出一种迁移学习方法来预测迎面流参数，具体步骤如下：首先离线训练一个卷积神经网络（ConvNet）模型来完成核心预测任务；然后冻结该模型除输出节点前选定层之外的所有权重；最后通过重新训练这些选定的层来执行迁移学习。该方法使用翼型稳态CFD分析数据进行了演示，包括领域适应（目标域数据分布不同于源域）和任务适应（预测任务改变）。", "result": "研究结果成功证明了该方法在适应数据分布变化、领域扩展和任务更新方面的潜力。然而，对于噪声数据的应用被认为效果不佳。", "conclusion": "该迁移学习方法能够有效应对数据分布变化、领域扩展和任务更新带来的挑战，并能预测迎面流参数，但在处理噪声数据时效果有限。", "translation": "确定迎面流参数对于风洞测试以及常规飞行和风力涡轮机运行至关重要。这些参数传统上通过直接测量来预测，但这可能在传感器故障时带来挑战。或者，可以使用基于表面压力数据的驱动预测模型来确定这些参数。至关重要的是，此类预测器应实现接近实时的学习，正如监测风洞操作或学习航空航天和风能系统气动性能变化等实际应用所要求的那样。为了克服数据分布变化以及适应新预测任务所带来的挑战，我们提出了一种迁移学习方法来预测迎面流参数，特别是攻角和迎面流速。它首先需要离线训练一个卷积神经网络（ConvNet）模型以完成核心预测任务，然后冻结该模型除输出节点前选定层之外的权重，最后通过重新训练这些层来执行迁移学习。使用翼型稳态CFD分析数据提供了这种方法的演示：i）领域适应，其中迁移学习是在目标域数据分布与源域不同时进行的；ii）任务适应，其中预测任务发生变化。文中还进一步探讨了噪声数据的影响、在扩展域上的性能以及改变采样大小和架构的权衡研究。结果成功证明了该方法在适应不断变化的数据分布、领域扩展和任务更新方面的潜力，但对于噪声数据的应用被认为效果不佳。", "summary": "本文提出了一种基于迁移学习的新方法，用于从表面压力数据预测迎面流参数（如攻角和迎面流速），以克服传统测量方法在传感器故障时的挑战以及数据驱动模型在数据分布变化和新任务适应方面的限制。该方法通过离线训练卷积神经网络，然后冻结大部分权重并仅微调靠近输出层的部分进行迁移学习。实验在翼型CFD数据上验证了其在领域适应和任务适应方面的有效性，并探讨了噪声数据等因素，结果表明其在适应性方面表现良好，但对噪声数据的处理效果不理想。", "keywords": "迁移学习, 迎面流参数, 卷积神经网络, 领域适应, 任务适应", "comments": "该论文的创新点在于提出了一个基于迁移学习的框架，用于实时、鲁棒地预测迎面流参数，特别是在数据分布变化和任务需求改变的复杂场景下。其通过冻结大部分预训练ConvNet权重并仅微训练特定层的策略，有效地平衡了学习效率和适应性。该方法对于航空航天和风能领域的监测与控制具有重要意义。然而，论文也指出了其在处理噪声数据时的局限性，这可能是未来研究需要关注的方向。"}}
{"id": "2506.15571", "title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Tu Nguyen Thi Ngoc"], "summary": "Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.", "comment": "9 pages, 8 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15571v1", "AI": {"title_translation": "MicroRicci：一种用于自适应网格平滑的贪婪局部Ricci流求解器", "tldr": "MicroRicci是一种高效的自适应局部Ricci流求解器，通过结合编码理论和小型神经网络模块，实现了网格平滑的显著加速和质量提升。", "motivation": "实时大规模网格平滑面临巨大挑战：传统Ricci流求解器需要昂贵的全局更新，而贪婪启发式方法收敛缓慢或调优困难。", "method": "本文提出了MicroRicci，它是一种自适应的局部Ricci流求解器，借鉴了编码理论思想，并结合了仅1K + 200个参数。其核心是一个贪婪的综合症解码步骤，能够在O(E)时间内精确定位并纠正最大的曲率误差，并辅以两个小型神经网络模块，用于动态自适应地选择顶点和步长。", "result": "在110个SJTU-TMQA网格数据集上，MicroRicci将迭代次数从950+=140减少到400+=80（2.4倍加速），将曲率扩散从0.19收紧到0.185，并实现了UV失真与MOS之间r = -0.93的显著相关性。它每次迭代仅增加0.25毫秒（从0.80到1.05毫秒），比最先进的方法实现了端到端1.8倍的运行时加速。", "conclusion": "MicroRicci结合了线性时间更新、自动超参数适应以及高质量的几何和感知结果，使其非常适合图形、仿真及相关领域中需要实时、资源受限的应用。", "translation": "大规模实时网格平滑仍然是一个艰巨的挑战：经典的Ricci流求解器需要耗费全局更新，而贪婪启发式方法则收敛缓慢或难以调优。我们提出了MicroRicci，这是第一个真正的自适应局部Ricci流求解器，它借鉴了编码理论的思想，并将其打包成仅1K + 200个参数。其主要核心是一个贪婪的综合症解码步骤，可以在O(E)时间内精确定位并纠正最大的曲率误差，并辅以两个小型神经网络模块，用于动态自适应地选择顶点和步长。在110个SJTU-TMQA网格的多种数据集上，MicroRicci将迭代次数从950+=140减少到400+=80（2.4倍加速），将曲率扩散从0.19收紧到0.185，并实现了UV失真与MOS之间r = -0.93的显著相关性。它每次迭代仅增加0.25毫秒（从0.80到1.05毫秒），比最先进的方法实现了端到端1.8倍的运行时加速。MicroRicci结合了线性时间更新、自动超参数适应以及高质量的几何和感知结果，使其非常适合图形、仿真及相关领域中需要实时、资源受限的应用。", "summary": "MicroRicci是一种创新的自适应局部Ricci流求解器，旨在解决大规模实时网格平滑的挑战。它结合了编码理论的贪婪综合症解码步骤和小型神经网络模块，实现了在O(E)时间内的曲率误差校正和自适应参数选择。实验结果表明，MicroRicci在迭代次数、曲率质量和运行时效率方面均显著优于现有方法，使其成为资源受限的实时图形和仿真应用的理想选择。", "keywords": "网格平滑, Ricci流, 自适应, 局部求解器, 实时处理", "comments": "MicroRicci的创新之处在于将编码理论与小型神经网络相结合，实现了Ricci流求解器的自适应和局部化，显著提升了效率和质量。其线性时间更新和自动超参数适应的特性对于实时应用具有重要意义，展现了在资源受限环境下进行高性能几何处理的潜力。其低参数量也是一大亮点。"}}
{"id": "2506.15064", "title": "HiPreNets: High-Precision Neural Networks through Progressive Training", "authors": ["Ethan Mulle", "Wei Kang", "Qi Gong"], "summary": "Deep neural networks are powerful tools for solving nonlinear problems in\nscience and engineering, but training highly accurate models becomes\nchallenging as problem complexity increases. Non-convex optimization and\nnumerous hyperparameters to tune make performance improvement difficult, and\ntraditional approaches often prioritize minimizing mean squared error (MSE)\nwhile overlooking $L^{\\infty}$ error, which is the critical focus in many\napplications. To address these challenges, we present a progressive framework\nfor training and tuning high-precision neural networks (HiPreNets). Our\napproach refines a previously explored staged training technique for neural\nnetworks that improves an existing fully connected neural network by\nsequentially learning its prediction residuals using additional networks,\nleading to improved overall accuracy. We discuss how to take advantage of the\nstructure of the residuals to guide the choice of loss function, number of\nparameters to use, and ways to introduce adaptive data sampling techniques. We\nvalidate our framework's effectiveness through several benchmark problems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15064v1", "AI": {"title_translation": "HiPreNets：通过渐进式训练实现高精度神经网络", "tldr": "HiPreNets提出了一种渐进式训练框架，通过顺序学习预测残差来解决高精度神经网络训练中的挑战，并提高整体准确性。", "motivation": "深度神经网络在解决科学和工程中的非线性问题时面临挑战，尤其是在问题复杂性增加时难以训练出高精度模型。传统的优化方法（如非凸优化和超参数调优）难以提升性能，并且常侧重于最小化均方误差（MSE）而忽略了在许多应用中至关重要的$L^{\\infty}$误差。", "method": "我们提出了一种用于训练和调优高精度神经网络（HiPreNets）的渐进式框架。该方法改进了现有的一种分阶段训练技术，通过使用额外的网络顺序学习现有全连接神经网络的预测残差，从而提高整体准确性。我们还讨论了如何利用残差结构来指导损失函数的选择、参数数量的确定以及引入自适应数据采样技术。", "result": "我们通过几个基准问题验证了我们框架的有效性。", "conclusion": "HiPreNets框架通过渐进式训练和残差学习，能够有效解决高精度神经网络的训练挑战，并在基准问题上展现出其有效性。", "translation": "深度神经网络是解决科学和工程中非线性问题的强大工具，但随着问题复杂性的增加，训练高精度模型变得具有挑战性。非凸优化和大量需要调整的超参数使得性能提升困难，并且传统方法通常优先最小化均方误差（MSE），而忽略了在许多应用中至关重要的$L^{\\infty}$误差。为了应对这些挑战，我们提出了一种用于训练和调优高精度神经网络（HiPreNets）的渐进式框架。我们的方法改进了一种先前探索的神经网络分阶段训练技术，通过顺序学习其预测残差并使用额外的网络来改进现有的全连接神经网络，从而提高整体准确性。我们讨论了如何利用残差的结构来指导损失函数的选择、使用的参数数量以及引入自适应数据采样技术。我们通过几个基准问题验证了我们框架的有效性。", "summary": "本文提出了一种名为HiPreNets的渐进式训练框架，旨在解决高精度神经网络训练中的挑战。该方法通过改进分阶段训练技术，利用额外的网络顺序学习现有神经网络的预测残差，从而显著提高整体精度。研究还探讨了如何根据残差结构选择损失函数、确定参数数量以及引入自适应数据采样。该框架的有效性已通过多个基准问题得到验证，尤其关注了在许多应用中被传统方法忽视的$L^{\\infty}$误差。", "keywords": "高精度神经网络, 渐进式训练, 残差学习, $L^{\\infty}$误差, 自适应采样", "comments": "这项工作创新性地提出了一个渐进式框架HiPreNets，通过残差学习和分阶段训练来提升神经网络的精度，并特别关注了$L^{\\infty}$误差，这在许多对精度要求高的应用中至关重要。其方法论中的自适应数据采样和对残差结构的利用，为高精度模型训练提供了新的思路。"}}
{"id": "2506.14949", "title": "From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?", "authors": ["Shadman Sakib", "Oishy Fatema Akhand", "Ajwad Abrar"], "summary": "While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.", "comment": "Accepted in 1st IEEE QPAIN 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14949v1", "AI": {"title_translation": "从聊天到检查：大型语言模型能否辅助糖尿病预测？", "tldr": "研究探讨LLM在糖尿病预测中的有效性，发现专有LLM表现优于开源LLM，且特定LLM甚至超越传统ML模型，但仍存在挑战。", "motivation": "尽管机器学习（ML）和深度学习（DL）模型已广泛用于糖尿病预测，但大型语言模型（LLM）在结构化数值数据上的应用尚未得到充分探索，因此本研究旨在测试LLM在此任务中的有效性。", "method": "本研究使用Pima Indian Diabetes Database (PIDD)数据集，通过零样本、单样本和三样本提示方法，评估了六种大型语言模型（包括Gemma-2-27B、Mistral-7B、Llama-3.1-8B、Llama-3.2-2B、GPT-4o和Gemini Flash 2.0）在糖尿病预测上的性能。同时，将它们的表现与随机森林、逻辑回归和支持向量机（SVM）三种传统机器学习模型进行比较。评估指标包括准确率、精确率、召回率和F1分数。", "result": "研究结果显示，专有LLM的性能优于开源LLM，其中GPT-4o和Gemma-2-27B在少样本设置中取得了最高的准确率。值得注意的是，Gemma-2-27B在F1分数方面也超越了传统ML模型。然而，研究也指出提示策略之间存在性能差异，并且需要进行领域特定的微调。", "conclusion": "本研究表明大型语言模型可用于医疗预测任务，并鼓励未来在提示工程和混合方法方面开展工作，以进一步改进医疗预测。", "translation": "尽管机器学习 (ML) 和深度学习 (DL) 模型已广泛用于糖尿病预测，但大型语言模型 (LLM) 在结构化数值数据上的应用尚未得到充分探索。在本研究中，我们通过零样本、单样本和三样本提示方法测试了 LLM 在预测糖尿病方面的有效性。我们使用皮马印第安人糖尿病数据库 (PIDD) 进行了实证分析。我们评估了六种 LLM，包括四种开源模型：Gemma-2-27B、Mistral-7B、Llama-3.1-8B 和 Llama-3.2-2B。我们还测试了两种专有模型：GPT-4o 和 Gemini Flash 2.0。此外，我们将它们的性能与三种传统机器学习模型进行了比较：随机森林、逻辑回归和支持向量机 (SVM)。我们使用准确率、精确率、召回率和 F1 分数作为评估指标。我们的结果表明，专有 LLM 的性能优于开源模型，其中 GPT-4o 和 Gemma-2-27B 在少样本设置中取得了最高的准确率。值得注意的是，Gemma-2-27B 在 F1 分数方面也优于传统 ML 模型。然而，仍然存在诸如提示策略之间性能差异以及需要领域特定微调等问题。这项研究表明 LLM 可用于医疗预测任务，并鼓励未来在提示工程和混合方法方面开展工作，以改进医疗预测。", "summary": "本研究探讨了大型语言模型（LLM）在糖尿病预测中的应用潜力，对比了多种开源和专有LLM与传统机器学习模型在Pima Indian Diabetes Database上的表现。结果显示，专有LLM通常优于开源LLM，且部分LLM（如Gemma-2-27B）在特定指标上超越了传统模型。研究强调LLM在医疗预测中的前景，但也指出提示策略的性能差异和微调的必要性。", "keywords": "大型语言模型, 糖尿病预测, 机器学习, 零样本学习, 少样本学习", "comments": "这项研究创新性地探索了LLM在结构化数值数据预测（特别是医疗领域）的应用，填补了现有研究的空白。其重要性在于证明了LLM在特定条件下（尤其是一些专有模型和少样本设置）可以媲美甚至超越传统ML模型，为医疗AI开辟了新途径。局限性在于提示策略的影响、需要领域特定微调以及对大型模型计算资源的需求。"}}
{"id": "2506.14800", "title": "A micromorphic-based artificial diffusion method for stabilized finite element approximation of convection-diffusion problems", "authors": ["Soheil Firooz", "B. Daya Reddy", "Paul Steinmann"], "summary": "We present a novel artificial diffusion method to circumvent the\ninstabilities associated with the standard finite element approximation of\nconvection-diffusion equations. Motivated by the micromorphic approach, we\nintroduce an auxiliary variable, which is related to the gradient of the field\nof interest, and which leads to a coupled problem. Conditions for\nwell-posedness of the resulting formulation are established. We carry out a\ncomprehensive numerical study to compare the proposed methodology against some\nwell-established approaches in one- and two-dimensional settings. The proposed\nmethod outperforms established approaches in general in approximating\naccurately the solutions to pertinent and challenging problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14800v1", "AI": {"title_translation": "微观形态学启发的人工扩散方法用于对流-扩散问题的稳定有限元近似", "tldr": "提出一种基于微观形态学的新型人工扩散方法，以稳定对流-扩散问题的有限元近似。", "motivation": "解决标准有限元近似对流-扩散方程时出现的不稳定性问题。", "method": "引入一个与感兴趣场的梯度相关的辅助变量，形成一个耦合问题。建立了所得公式的适定性条件。通过全面的数值研究，将所提出的方法与一些成熟的方法进行比较。", "result": "所提出的方法在准确近似相关和具有挑战性问题的解方面，总体上优于现有方法。", "conclusion": "所提出的基于微观形态学的人工扩散方法能够有效稳定对流-扩散问题的有限元近似，并且在性能上优于现有方法。", "translation": "我们提出了一种新颖的人工扩散方法，以规避与对流-扩散方程标准有限元近似相关的不稳定性。受微观形态学方法的启发，我们引入了一个辅助变量，该变量与感兴趣场的梯度相关，并导致一个耦合问题。建立了所得公式的适定性条件。我们进行了一项全面的数值研究，将所提出的方法与一维和二维设置中一些成熟的方法进行比较。所提出的方法在准确近似相关和具有挑战性问题的解方面，总体上优于现有方法。", "summary": "本论文提出了一种基于微观形态学的新型人工扩散方法，旨在解决对流-扩散方程标准有限元近似中的不稳定性。该方法引入了一个与梯度相关的辅助变量，形成耦合问题，并建立了其适定性条件。通过数值研究表明，该方法在处理复杂问题时，其解的准确性优于现有成熟方法。", "keywords": "人工扩散, 有限元, 对流-扩散, 微观形态学, 稳定性", "comments": "该论文的创新点在于将微观形态学方法引入到人工扩散领域，通过引入辅助变量构建耦合问题，为对流-扩散问题的有限元稳定近似提供了一种新的思路。其重要性体现在有效提升了复杂对流-扩散问题解的准确性。"}}
{"id": "2506.15464", "title": "Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic Graphs", "authors": ["Le Vu Anh", "Mehmet Dik", "Nguyen Viet Anh"], "summary": "Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.", "comment": "5 pages, 5 figures", "cate": "math.MG", "url": "http://arxiv.org/abs/2506.15464v1", "AI": {"title_translation": "δ-双曲图上边界加权滤波器的谱收缩", "tldr": "本文提出了一种针对分层图的边界加权滤波器，该滤波器具有谱收缩特性，为图信号处理提供了一种稳定且轻量级的工具。", "motivation": "分层图通常表现出树状分支模式，这种结构特性对传统图滤波器的设计提出了挑战。", "method": "本文引入了一个边界加权算子，该算子根据每条边的端点向图的格罗莫夫边界漂移的距离来重新缩放每条边。该方法利用δ-双曲网络上的Busemann函数，证明了该算子谱范数的一个封闭形式上限。", "result": "该研究结果提供了一种无参数、轻量级的滤波器，其稳定性直接源于几何第一原理。该滤波器能够使每个信号在每次通过时都损失其能量中由曲率控制的一部分。", "conclusion": "该工作为具有密集或隐藏分层结构的数据上的图信号处理提供了一种新的分析工具。", "translation": "分层图通常表现出树状分支模式，这种结构特性对传统图滤波器的设计提出了挑战。我们引入了一种边界加权算子，该算子根据每条边的端点向图的格罗莫夫边界漂移的距离来重新缩放每条边。利用δ-双曲网络上的Busemann函数，我们证明了该算子谱范数的一个封闭形式上限：每个信号在每次通过时都会损失其能量中由曲率控制的一部分。该结果提供了一种无参数、轻量级的滤波器，其稳定性直接源于几何第一原理，为具有密集或隐藏分层结构的数据上的图信号处理提供了一种新的分析工具。", "summary": "本文通过引入边界加权算子解决了传统图滤波器在分层图上遇到的挑战。该算子根据边端点与图格罗莫夫边界的接近程度来重新缩放边。通过在δ-双曲网络上应用Busemann函数，作者证明了该算子谱范数的一个封闭形式上限，表明信号在每次通过时都会损失能量。由此产生的滤波器是无参数、轻量级且几何稳定的，为分层数据上的图信号处理提供了一种新颖的分析工具。", "keywords": "谱收缩, 边界加权滤波器, δ-双曲图, 图信号处理, 分层图", "comments": "该论文的创新之处在于利用几何特性（格罗莫夫边界、Busemann函数、δ-双曲网络）来设计一种稳定且无参数的图滤波器。这种方法为复杂分层数据上的图信号处理提供了坚实的理论基础，而这正是传统方法面临的重大挑战。其直接源于几何第一原理的推导是一个显著的优势。"}}
{"id": "2506.14877", "title": "Beyond Universality: Cultural Diversity in Music and Its Implications for Sound Design and Sonification", "authors": ["Rubén García-Benito"], "summary": "The Audio Mostly (AM) conference has long been a platform for exploring the\nintersection of sound, technology, and culture. Despite growing interest in\nsonic cultures, discussions on the role of cultural diversity in sound design\nand sonification remain limited. This paper investigates the implicit biases\nand gaps within the discourse on music and sound aesthetics, challenging the\nnotion of music as a 'universal language'. Through a historical and\ncross-cultural analysis of musicology and ethnomusicology, the profound\ninfluence of cultural context on auditory perception and aesthetic appraisal is\nhighlighted. By drawing parallels between historical music practices and\ncontemporary sound design, the paper advocates for a more inclusive approach\nthat recognizes the diversity of sonic traditions. Using music as a case study,\nwe underscore broader implications for sound design and sonification,\nemphasizing the need to integrate cultural perspectives into auditory design\npractices. A reevaluation of existing frameworks in sound design and\nsonification is proposed, emphasizing the necessity of culturally informed\npractices that resonate with global audiences. Ultimately, embracing cultural\ndiversity in sound design is suggested to lead to richer, more meaningful\nauditory experiences and to foster greater inclusivity within the field.", "comment": "12 pages, 1 figure. Long paper accepted for publication at the Audio\n  Mostly & ICAD Joint Conference (AM.ICAD 2025). To appear in the ACM\n  International Conference Proceedings Series (ICPS)", "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.14877v1", "AI": {"title_translation": "超越普适性：音乐中的文化多样性及其对声音设计和声音化的影响", "tldr": "本文探讨了音乐中的文化多样性，挑战了音乐作为“普适语言”的观念，并强调了文化背景对听觉感知和审美评价的深远影响。它主张在声音设计和声音化中采纳更具包容性的方法，以整合文化视角，从而带来更丰富、更有意义的听觉体验。", "motivation": "音频大会长期以来一直是探索声音、技术和文化交叉点的平台，但尽管对声音文化日益增长的兴趣，关于文化多样性在声音设计和声音化中作用的讨论仍然有限。本文旨在调查音乐和声音美学论述中隐含的偏见和空白，挑战音乐作为“普适语言”的观念。", "method": "通过对音乐学和民族音乐学的历史和跨文化分析；将历史音乐实践与当代声音设计进行类比；以音乐为案例研究。", "result": "强调了文化背景对听觉感知和审美评价的深远影响。本文主张采用一种更具包容性的方法，以识别声音传统的多样性。它强调了将文化视角融入听觉设计实践的必要性。", "conclusion": "在声音设计中拥抱文化多样性可以带来更丰富、更有意义的听觉体验，并促进该领域更大的包容性。建议对现有框架进行重新评估，以实现文化知情的实践。", "translation": "音频大会（AM）长期以来一直是探索声音、技术和文化交叉点的平台。尽管对声音文化的兴趣日益增长，但关于文化多样性在声音设计和声音化中作用的讨论仍然有限。本文调查了音乐和声音美学论述中隐含的偏见和空白，挑战了音乐作为“普适语言”的观念。通过对音乐学和民族音乐学的历史和跨文化分析，强调了文化背景对听觉感知和审美评价的深远影响。通过将历史音乐实践与当代声音设计进行类比，本文主张采用一种更具包容性的方法，以识别声音传统的多样性。以音乐为案例研究，我们强调了对声音设计和声音化的更广泛影响，强调了将文化视角融入听觉设计实践的必要性。本文提出对声音设计和声音化中现有框架进行重新评估，强调文化知情实践的必要性，以与全球受众产生共鸣。最终，建议在声音设计中拥抱文化多样性可以带来更丰富、更有意义的听觉体验，并促进该领域更大的包容性。", "summary": "本文挑战了音乐作为普适语言的传统观念，通过对音乐学和民族音乐学的历史及跨文化分析，揭示了文化背景对听觉感知和审美评价的深远影响。鉴于当前声音设计和声音化中文化多样性讨论的不足，作者以音乐为例，主张将文化视角融入听觉设计实践，并呼吁重新评估现有框架，以实现更具包容性、能与全球受众共鸣的文化知情实践。最终目标是创造更丰富、更有意义的听觉体验，并促进领域内的包容性。", "keywords": "文化多样性, 声音设计, 声音化, 音乐学, 民族音乐学", "comments": "该论文的重要性在于其直面声音设计和声音化领域中一个关键的忽视点，即文化多样性。其创新之处在于利用音乐作为案例研究，将文化对听觉感知的深远影响扩展到更广泛的听觉设计领域，从而挑战了可能存在的西方中心主义视角。这对于推动该领域向更具包容性和全球适应性方向发展具有重要意义。"}}
{"id": "2506.14844", "title": "Improving Prostate Gland Segmenting Using Transformer based Architectures", "authors": ["Shatha Abudalou"], "summary": "Inter reader variability and cross site domain shift challenge the automatic\nsegmentation of prostate anatomy using T2 weighted MRI images. This study\ninvestigates whether transformer models can retain precision amid such\nheterogeneity. We compare the performance of UNETR and SwinUNETR in prostate\ngland segmentation against our previous 3D UNet model [1], based on 546 MRI\n(T2weighted) volumes annotated by two independent experts. Three training\nstrategies were analyzed: single cohort dataset, 5 fold cross validated mixed\ncohort, and gland size based dataset. Hyperparameters were tuned by Optuna. The\ntest set, from an independent population of readers, served as the evaluation\nendpoint (Dice Similarity Coefficient). In single reader training, SwinUNETR\nachieved an average dice score of 0.816 for Reader#1 and 0.860 for Reader#2,\nwhile UNETR scored 0.8 and 0.833 for Readers #1 and #2, respectively, compared\nto the baseline UNets 0.825 for Reader #1 and 0.851 for Reader #2. SwinUNETR\nhad an average dice score of 0.8583 for Reader#1 and 0.867 for Reader#2 in\ncross-validated mixed training. For the gland size-based dataset, SwinUNETR\nachieved an average dice score of 0.902 for Reader#1 subset and 0.894 for\nReader#2, using the five-fold mixed training strategy (Reader#1, n=53;\nReader#2, n=87) at larger gland size-based subsets, where UNETR performed\npoorly. Our findings demonstrate that global and shifted-window self-attention\neffectively reduces label noise and class imbalance sensitivity, resulting in\nimprovements in the Dice score over CNNs by up to five points while maintaining\ncomputational efficiency. This contributes to the high robustness of SwinUNETR\nfor clinical deployment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14844v1", "AI": {"title_translation": "使用基于Transformer的架构改进前列腺分割", "tldr": "本研究表明，SwinUNETR在处理MRI图像中前列腺分割的读者间差异和跨站点域偏移方面表现出高鲁棒性，Dice分数显著优于CNN模型，尤其是在较大腺体尺寸的数据集上。", "motivation": "由于读者间差异和跨站点域偏移，使用T2加权MRI图像自动分割前列腺解剖结构面临挑战，本研究旨在探究Transformer模型能否在此类异质性中保持精度。", "method": "本研究比较了UNETR和SwinUNETR在前列腺腺体分割中的性能，并与先前的3D UNet模型进行对比。研究使用了546个由两位独立专家标注的MRI（T2加权）体积。分析了三种训练策略：单队列数据集、五折交叉验证混合队列和基于腺体大小的数据集。超参数通过Optuna进行调整。测试集来自独立的读者群体，评估指标为Dice相似系数。", "result": "在单读者训练中，SwinUNETR对读者#1和读者#2的平均Dice分数分别为0.816和0.860，而UNETR分别为0.8和0.833，基线UNet分别为0.825和0.851。在交叉验证混合训练中，SwinUNETR对读者#1和读者#2的平均Dice分数分别为0.8583和0.867。在基于腺体大小的数据集上，SwinUNETR在较大腺体尺寸的子集中实现了更高的平均Dice分数，对读者#1子集为0.902，对读者#2子集为0.894，而UNETR表现不佳。", "conclusion": "研究结果表明，全局和移位窗口自注意力机制能有效减少标签噪声和类别不平衡敏感性，使Dice分数相较于CNN模型提高了多达五个点，同时保持了计算效率。这有助于SwinUNETR在临床部署中展现出高鲁棒性。", "translation": "读者间差异和跨站点域偏移对使用T2加权MRI图像自动分割前列腺解剖结构提出了挑战。本研究旨在调查Transformer模型在面对这种异质性时能否保持精度。我们比较了UNETR和SwinUNETR在前列腺腺体分割中的性能，并与我们之前的3D UNet模型[1]进行对比，数据基于546个由两位独立专家标注的MRI（T2加权）体积。分析了三种训练策略：单队列数据集、五折交叉验证混合队列和基于腺体大小的数据集。超参数通过Optuna进行调整。来自独立读者群体的测试集作为评估终点（Dice相似系数）。在单读者训练中，SwinUNETR对读者#1和读者#2的平均Dice分数分别为0.816和0.860，而UNETR对读者#1和读者#2的得分分别为0.8和0.833，相比之下，基线UNet对读者#1和读者#2的得分分别为0.825和0.851。在交叉验证混合训练中，SwinUNETR对读者#1和读者#2的平均Dice分数分别为0.8583和0.867。对于基于腺体大小的数据集，SwinUNETR在使用五折混合训练策略（读者#1，n=53；读者#2，n=87）时，在较大腺体尺寸的子集中对读者#1子集达到了0.902的平均Dice分数，对读者#2子集达到了0.894，而UNETR表现不佳。我们的研究结果表明，全局和移位窗口自注意力能有效减少标签噪声和类别不平衡敏感性，使Dice分数相较于CNN提高了多达五个点，同时保持了计算效率。这有助于SwinUNETR在临床部署中展现出高鲁棒性。", "summary": "本研究旨在解决前列腺MRI图像自动分割中存在的读者间差异和跨站点域偏移问题。通过比较UNETR和SwinUNETR与传统3D UNet模型在前列腺腺体分割中的性能，研究发现基于Transformer的SwinUNETR在多种训练策略下均表现出色。尤其是在处理较大腺体尺寸时，SwinUNETR的Dice分数显著优于其他模型，最高可达0.902。研究表明，Transformer模型中的自注意力机制有效降低了标签噪声和类别不平衡敏感性，提高了分割精度和鲁棒性，使其更适用于临床部署。", "keywords": "前列腺分割, Transformer, SwinUNETR, 医学图像分割, Dice相似系数", "comments": "该论文创新性地将Transformer架构应用于前列腺MRI图像分割，有效解决了医学图像分割中常见的读者间差异和跨站点域偏移问题。SwinUNETR表现出卓越的鲁棒性和更高的精度，尤其是在处理特定挑战性数据（如较大腺体尺寸）时，其性能提升显著。这对于推动前列腺癌诊断和治疗的自动化具有重要意义，凸显了Transformer在医学图像分析领域的巨大潜力。"}}
{"id": "2506.15029", "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "authors": ["Prateek Mehta", "Anasuya Patil"], "summary": "Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW).", "comment": "9 pages, 9 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15029v1", "AI": {"title_translation": "基于LabVIEW的准确修订版光学字符识别语音合成系统", "tldr": "本文开发了一种基于光学字符识别（OCR）和LabVIEW的语音合成系统，旨在为视障人士提供准确、可靠且经济高效的信息获取方式。", "motivation": "现有盲文书籍和录音的局限性导致视障人士无法自由选择书籍，而语音对于盲人和视障人士来说是比文本更有效的交流模式。", "method": "本文开发了一个基于光学字符识别（OCR）的语音合成系统，并使用LabVIEW（Laboratory Virtual Instrument Engineering Workbench）进行了实现。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "通过声音提取知识是一种独特的特性。视障人士通常只依赖盲文书籍和非政府组织提供的录音。由于这些方法的局限性，盲人往往无法获取他们选择的书籍。对于盲人和视障人士来说，语音是一种比文本更有效的交流模式，因为他们可以轻松地对声音做出反应。本文提出了一种准确、可靠、经济高效且用户友好的基于光学字符识别（OCR）的语音合成系统的开发。该基于OCR的系统已使用LabVIEW（Laboratory Virtual Instrument Engineering Workbench）实现。", "summary": "本文开发并实现了一个基于光学字符识别（OCR）的语音合成系统，旨在为视障人士提供一种准确、可靠、经济高效且用户友好的信息获取方式。该系统利用LabVIEW进行开发，以解决现有盲文和录音资源限制导致视障人士难以自由获取书籍的问题，因为语音对于他们而言是更有效的交流模式。", "keywords": "光学字符识别, 语音合成, LabVIEW, 视障人士, 辅助技术", "comments": "这篇论文的创新点在于将OCR技术与LabVIEW相结合，开发出一种专门针对视障人士的语音合成系统，以解决他们获取信息受限的问题。其重要性在于提供了一种可能更便捷、经济的辅助阅读方案，有望提升视障人士的信息可及性。然而，抽象中未提供具体性能指标或用户评估结果，这限制了对其“准确、可靠、经济高效且用户友好”特性的深入了解。"}}
{"id": "2506.15075", "title": "CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets", "authors": ["Samhita Kuili", "Mohammadreza Amini", "Burak Kantarci"], "summary": "In the ever-expanding domain of 5G-NR wireless cellular networks,\nover-the-air jamming attacks are prevalent as security attacks, compromising\nthe quality of the received signal. We simulate a jamming environment by\nincorporating additive white Gaussian noise (AWGN) into the real-world In-phase\nand Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is\nexploited to implement a jamming detection over various characteristics such as\nheterogenous I/Q datasets; extracting relevant information on Synchronization\nSignal Blocks (SSBs), and fewer SSB observations with notable class imbalance.\nGiven the characteristics of datasets, balanced datasets are acquired by\nemploying a Conv1D conditional Wasserstein Generative Adversarial\nNetwork-Gradient Penalty(CWGAN-GP) on both majority and minority SSB\nobservations. Additionally, we compare the performance and detection ability of\nthe proposed CAE model on augmented datasets with benchmark models:\nConvolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder\n(CSAE). Despite the complexity of data heterogeneity involved across all\ndatasets, CAE depicts the robustness in detection performance of jammed signal\nby achieving average values of 97.33% precision, 91.33% recall, 94.08%\nF1-score, and 94.35% accuracy over CDAE and CSAE.", "comment": "6 pages, 5 figures, Accepted to IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15075v1", "AI": {"title_translation": "CWGAN-GP增强CAE用于5G-NR非独立同分布数据集中的干扰检测", "tldr": "在5G-NR网络中，本文提出使用CWGAN-GP增强的CAE模型，即使在非独立同分布数据集中也能有效检测干扰，性能优于CDAE和CSAE。", "motivation": "在5G-NR无线蜂窝网络中，空中干扰攻击普遍存在，会损害接收信号的质量。尤其是在异构和不平衡的数据集特性下，需要有效的干扰检测方法。", "method": "通过将加性高斯白噪声(AWGN)引入真实世界的同相和正交(I/Q)OFDM数据集来模拟干扰环境。为了处理异构I/Q数据集和类不平衡问题，采用Conv1D条件Wasserstein生成对抗网络-梯度惩罚(CWGAN-GP)对多数和少数SSB观测值进行数据增强以获取平衡数据集。然后，利用卷积自编码器(CAE)进行干扰检测，并将其性能与基准模型（卷积去噪自编码器CDAE和卷积稀疏自编码器CSAE）在增强数据集上进行比较。", "result": "尽管数据异构性复杂，CAE在干扰信号检测性能上表现出鲁棒性，平均精度达到97.33%，召回率91.33%，F1分数94.08%，准确率94.35%，优于CDAE和CSAE。", "conclusion": "本文提出的CWGAN-GP增强型CAE模型能够有效检测5G-NR网络中的干扰，即使在复杂的非独立同分布数据特性下也表现出卓越的性能。", "translation": "在不断扩展的5G-NR无线蜂窝网络领域中，空中干扰攻击作为一种安全攻击普遍存在，会损害接收信号的质量。我们通过将加性高斯白噪声（AWGN）引入真实世界的同相和正交（I/Q）OFDM数据集来模拟干扰环境。卷积自编码器（CAE）被用于在各种特性下实现干扰检测，例如异构I/Q数据集；提取同步信号块（SSB）的相关信息，以及少数SSB观测值存在显著的类不平衡。鉴于数据集的特性，通过对多数和少数SSB观测值采用Conv1D条件Wasserstein生成对抗网络-梯度惩罚（CWGAN-GP）来获取平衡数据集。此外，我们比较了所提出的CAE模型在增强数据集上的性能和检测能力与基准模型：卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE）。尽管所有数据集都涉及数据异构性的复杂性，CAE在干扰信号的检测性能上表现出鲁棒性，平均精度达到97.33%，召回率91.33%，F1分数94.08%，准确率94.35%，优于CDAE和CSAE。", "summary": "本文旨在解决5G-NR网络中，尤其是在非独立同分布和不平衡数据集下，空中干扰攻击的检测问题。研究通过引入AWGN模拟干扰环境，并利用CWGAN-GP对数据集进行增强和平衡。核心贡献在于提出了一种基于卷积自编码器（CAE）的干扰检测模型。实验结果表明，尽管数据具有复杂异构性，CAE模型在检测性能上表现出卓越的鲁棒性，其精度、召回率、F1分数和准确率均显著优于基准模型CDAE和CSAE。", "keywords": "干扰检测, 5G-NR, CWGAN-GP, 卷积自编码器, 非独立同分布数据集", "comments": "本文创新性地将CWGAN-GP引入到5G-NR干扰检测的数据增强中，有效解决了非独立同分布和类不平衡数据集的挑战，这在实际应用中具有重要意义。CAE模型在复杂数据环境下的鲁棒性和优异性能是其亮点。"}}
{"id": "2506.15009", "title": "Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots", "authors": ["Jinjie Li", "Jiaxuan Li", "Kotaro Kaneko", "Liming Shu", "Moju Zhao"], "summary": "Omnidirectional aerial robots offer full 6-DoF independent control over\nposition and orientation, making them popular for aerial manipulation. Although\nadvancements in robotic autonomy, operating by human remains essential in\ncomplex aerial environments. Existing teleoperation approaches for multirotors\nfail to fully leverage the additional DoFs provided by omnidirectional\nrotation. Additionally, the dexterity of human fingers should be exploited for\nmore engaged interaction. In this work, we propose an aerial teleoperation\nsystem that brings the omnidirectionality of human hands into the unbounded\naerial workspace. Our system includes two motion-tracking marker sets -- one on\nthe shoulder and one on the hand -- along with a data glove to capture hand\ngestures. Using these inputs, we design four interaction modes for different\ntasks, including Spherical Mode and Cartesian Mode for long-range moving as\nwell as Operation Mode and Locking Mode for precise manipulation, where the\nhand gestures are utilized for seamless mode switching. We evaluate our system\non a valve-turning task in real world, demonstrating how each mode contributes\nto effective aerial manipulation. This interaction framework bridges human\ndexterity with aerial robotics, paving the way for enhanced teleoperated aerial\nmanipulation in unstructured environments.", "comment": "7 pages, 9 figures. This work has been accepted to IROS 2025. The\n  video will be released soon", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15009v1", "AI": {"title_translation": "六自由度手部遥操作全向空中机器人", "tldr": "提出了一种基于手部控制的全向空中机器人遥操作系统，利用手势和运动追踪实现精确控制和任务特定模式切换。", "motivation": "现有针对多旋翼的遥操作方法未能充分利用全向空中机器人提供的额外自由度，且未充分利用人手的灵活性。在复杂的空中环境中，人类操作仍至关重要。", "method": "该系统包含两组运动跟踪标记（肩部和手部）以及一个数据手套来捕捉手势。基于这些输入，设计了四种交互模式：用于长距离移动的球形模式和笛卡尔模式，以及用于精确操作的操作模式和锁定模式，手势用于无缝模式切换。", "result": "在现实世界的阀门转动任务中对系统进行了评估，证明了每种模式对有效空中操作的贡献。", "conclusion": "该交互框架将人类的灵活性与空中机器人技术相结合，为在非结构化环境中增强遥操作空中操作铺平了道路。", "translation": "全向空中机器人提供对位置和方向的完全六自由度独立控制，使其在空中操作中广受欢迎。尽管机器人自主性取得了进步，但在复杂的空中环境中，人类操作仍然至关重要。现有的多旋翼遥操作方法未能充分利用全向旋转提供的额外自由度。此外，应利用人手的灵活性来实现更深入的交互。在这项工作中，我们提出了一种空中遥操作系统，将人手的全向性带入无限的空中工作空间。我们的系统包括两组运动跟踪标记——一组在肩部，一组在手部——以及一个数据手套来捕捉手势。利用这些输入，我们为不同的任务设计了四种交互模式，包括用于长距离移动的球形模式和笛卡尔模式，以及用于精确操作的操作模式和锁定模式，其中手势用于无缝模式切换。我们在现实世界的阀门转动任务中评估了我们的系统，展示了每种模式如何有助于有效的空中操作。这种交互框架将人类的灵活性与空中机器人技术相结合，为在非结构化环境中增强遥操作空中操作铺平了道路。", "summary": "本文提出了一种新颖的基于手部遥操作的全向空中机器人系统。该系统通过充分利用全向机器人的六自由度能力和人手的灵活性，解决了现有遥操作方法的局限性。它集成了运动追踪和数据手套来捕捉手部和手臂动作，并设计了四种独特的交互模式，适用于从长距离导航到精确操作的各种任务，通过手势实现模式的无缝切换。在现实世界阀门转动任务上的评估证明了其在增强空中操作方面的有效性。", "keywords": "遥操作, 全向空中机器人, 六自由度, 手势控制, 人机交互", "comments": "该研究的创新之处在于充分利用了全向空中机器人的六自由度特性，并将人手的灵活性融入到直观精确的控制中。其多模式交互设计和基于手势的模式切换是关键优势，提高了系统对不同任务的适应性。这项工作对于推进复杂空中环境中的人机交互，特别是在需要高精度和适应性的操作任务中，具有重要意义。"}}
{"id": "2506.15172", "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "summary": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.", "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15172v1", "AI": {"title_translation": "敏捷/Scrum流程的先进方法：RetroAI++", "tldr": "本文介绍了一个名为RetroAI++的原型工具，它利用人工智能来自动化和优化敏捷/Scrum软件开发中的冲刺规划和回顾分析，旨在提供智能建议和有意义的见解。", "motivation": "旨在支持软件开发人员在敏捷/Scrum项目管理中的关键活动，特别是冲刺规划和回顾分析，通过自动化和完善这些过程。", "method": "本文提出了一个名为RetroAI++的原型工具，该工具基于新兴智能技术和人工智能洞察，旨在自动化和完善敏捷/Scrum开发项目在冲刺规划、开发和回顾阶段的流程。", "result": "RetroAI++原型旨在自动化和完善敏捷/Scrum流程的实际应用，为冲刺组织提供智能建议，并为回顾性反思提供有意义的见解。", "conclusion": "RetroAI++原型提供了一种先进的、由人工智能驱动的方法，以增强和自动化敏捷/Scrum中关键的冲刺规划和回顾过程，从而为软件开发人员提供支持。", "translation": "在敏捷/Scrum软件开发中，冲刺规划和回顾分析是项目管理的关键要素。我们工作的目的是在这些活动中支持软件开发人员。在本文中，我们提出了基于新兴智能技术的原型工具RetroAI++。在我们的RetroAI++原型中，我们旨在自动化和完善冲刺规划和回顾中的敏捷/Scrum流程的实际应用。利用人工智能的洞察力，我们的原型旨在自动化和完善敏捷/Scrum开发项目冲刺规划、开发和回顾阶段涉及的许多流程，为冲刺组织提供智能建议，并为回顾性反思提供有意义的见解。", "summary": "本文介绍了一个名为RetroAI++的原型工具，旨在增强敏捷/Scrum软件开发过程。该工具利用人工智能自动化和优化冲刺规划、开发和回顾等关键环节。RetroAI++的目标是为冲刺组织提供智能建议，并为回顾性分析提供深刻见解，从而支持软件开发人员进行这些重要的项目管理活动。", "keywords": "敏捷/Scrum, 人工智能, 软件开发, 冲刺规划, 回顾分析", "comments": "RetroAI++代表了将人工智能集成到敏捷/Scrum等成熟软件开发方法中的创新一步。它专注于自动化并为冲刺规划和回顾提供智能见解，解决了项目管理中的常见痛点，有望提高效率和决策质量。然而，作为“原型”，它需要进一步的开发和验证，以展示其全面的实际影响和可扩展性。"}}
{"id": "2506.14829", "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities", "authors": ["Aditya Majumdar", "Wenbo Zhang", "Kashvi Prawal", "Amulya Yadav"], "summary": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects\nfocus on harnessing AI to address societal issues in areas such as healthcare,\nsocial justice, etc. Unfortunately, despite growing interest in AI4SI,\nachieving tangible, on-the-ground impact remains a significant challenge. For\nexample, identifying and engaging motivated collaborators who are willing to\nco-design and deploy AI based solutions in real-world settings is often\ndifficult. Even when such partnerships are established, many AI4SI projects\n\"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to\ntransition to at-scale production-level solutions. Furthermore, the unique\nchallenges faced by AI4SI researchers are not always fully recognized within\nthe broader AI community, where such work is sometimes viewed as primarily\napplied and not aligning with the traditional criteria for novelty emphasized\nin core AI venues. This paper attempts to shine a light on the diverse\nchallenges faced in AI4SI research by diagnosing a multitude of factors that\nprevent AI4SI partnerships from achieving real-world impact on the ground.\nDrawing on semi-structured interviews with six leading AI4SI researchers -\ncomplemented by the authors' own lived experiences in conducting AI4SI research\n- this paper attempts to understand the day-to-day difficulties faced in\ndeveloping and deploying socially impactful AI solutions. Through thematic\nanalysis, we identify structural and organizational, communication,\ncollaboration, and operational challenges as key barriers to deployment. While\nthere are no easy fixes, we synthesize best practices and actionable strategies\ndrawn from these interviews and our own work in this space. In doing so, we\nhope this paper serves as a practical reference guide for AI4SI researchers and\npartner organizations seeking to engage more effectively in socially impactful\nAI collaborations.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14829v1", "AI": {"title_translation": "人工智能社会影响研究中实现影响力的难度：挑战与机遇的基层视角", "tldr": "本研究探讨了人工智能社会影响（AI4SI）项目在实际应用中面临的挑战，如难以找到合作者、项目停留在概念验证阶段以及AI社区对AI4SI工作的误解。通过访谈和经验分析，识别了结构、沟通、协作和操作方面的障碍，并提出了实践指南。", "motivation": "尽管对人工智能社会影响（AI4SI）的兴趣日益增长，但AI4SI项目在实际中难以实现切实的、落地的影响力。例如，难以找到和吸引积极的合作者，许多项目未能超越概念验证阶段，以及AI4SI研究者面临的独特挑战在更广泛的AI社区中未被充分认可。本研究旨在诊断阻碍AI4SI合作实现实际影响的多种因素，并理解开发和部署具有社会影响力的人工智能解决方案的日常困难。", "method": "本研究通过对六位领先的AI4SI研究人员进行半结构化访谈，并结合作者自身进行AI4SI研究的经验。随后，通过主题分析法，识别了部署过程中的主要障碍。", "result": "研究识别出结构和组织、沟通、协作以及操作方面的挑战是阻碍AI4SI解决方案部署的关键障碍。此外，研究还综合了从访谈和作者自身工作中得出的最佳实践和可操作策略。", "conclusion": "本研究希望作为一份实用参考指南，帮助AI4SI研究人员和合作组织更有效地参与到具有社会影响力的人工智能合作中。", "translation": "为了应对联合国可持续发展目标，人工智能社会影响（AI4SI）项目致力于利用人工智能解决医疗、社会公平等领域的社会问题。然而，尽管对AI4SI的兴趣日益增长，实现切实的、落地的影响力仍然是一个重大挑战。例如，识别并吸引愿意在真实世界环境中共同设计和部署基于人工智能解决方案的积极合作者通常很困难。即使建立了这样的伙伴关系，许多AI4SI项目也“未能”超越概念验证阶段，因此无法过渡到大规模生产级的解决方案。此外，AI4SI研究人员面临的独特挑战在更广泛的人工智能社区中并不总是得到充分认可，在该社区中，这类工作有时被视为主要是应用性的，不符合核心人工智能领域所强调的新颖性传统标准。本文试图通过诊断阻碍AI4SI伙伴关系在实地实现实际影响的多种因素，来揭示AI4SI研究中面临的各种挑战。本文借鉴了对六位领先的AI4SI研究人员的半结构化访谈——并辅以作者自身进行AI4SI研究的亲身经验——试图理解在开发和部署具有社会影响力的人工智能解决方案时面临的日常困难。通过主题分析，我们识别出结构和组织、沟通、协作以及操作方面的挑战是部署的关键障碍。虽然没有简单的解决方案，但我们综合了从这些访谈和我们在此领域的工作中得出的最佳实践和可操作策略。通过这样做，我们希望本文能为寻求更有效参与具有社会影响力的人工智能合作的AI4SI研究人员和合作组织提供一份实用参考指南。", "summary": "本研究探讨了人工智能社会影响（AI4SI）项目在将概念验证转化为实际落地影响过程中面临的挑战。研究指出，AI4SI项目常因难以找到合作者、未能超越概念验证阶段以及在主流AI社区中不被充分认可而受阻。通过对六位AI4SI研究人员的访谈和作者自身经验，文章识别了结构与组织、沟通、协作和操作层面的关键障碍。最后，论文综合了最佳实践和实用策略，旨在为AI4SI研究者和合作方提供指导，以期更有效地实现社会影响力。", "keywords": "人工智能社会影响, AI4SI, 挑战, 影响力, 部署", "comments": "这篇论文的创新之处在于它从“基层视角”深入探讨了AI4SI领域面临的实际挑战，而不仅仅是技术层面。它强调了在现实世界中实现AI社会影响力的复杂性，包括人际协作、组织结构和对AI4SI工作价值的认知问题。其重要性在于为AI4SI研究者和实践者提供了宝贵的实践指导，有助于弥合技术开发与实际应用之间的鸿沟，促进AI技术真正服务于社会。论文通过定性研究方法，揭示了该领域未被充分关注的非技术性障碍，具有很强的现实意义。"}}
{"id": "2506.15613", "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation", "authors": ["Miryeong Kwon", "Donghyun Gouk", "Junhyeok Jang", "Jinwoo Baek", "Hyunwoo You", "Sangyoon Ji", "Hongjoo Jung", "Junseok Moon", "Seungkwan Kang", "Seungjun Lee", "Myoungsoo Jung"], "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15613v1", "AI": {"title_translation": "从块到字节：通过CXL内存协议和指令注解转换PCIe SSD", "tldr": "本文探讨了如何利用CXL协议将PCIe SSD块存储转换为可扩展的字节可寻址工作内存，并通过FPGA原型和注解机制验证了其高性能，为内存存储融合奠定了基础。", "motivation": "将基于PCIe的块存储转换为可扩展的、字节可寻址的工作内存，并解决块存储适应CXL内存中心模型所面临的挑战。", "method": "强调可缓存性作为关键促成因素，倡导使用Type 3端点设备（CXL-SSDs）。在定制的FPGA平台上原型化CXL-SSD，并提出“确定性”和“可缓冲性”注解机制以提高性能同时保持数据持久性。通过仿真进行评估。", "result": "CXL-SSD比基于PCIe的内存扩展器性能提高10.9倍，通过注解增强进一步将延迟降低5.4倍。在高局部性工作负载下，由于高效的片上缓存，CXL-SSD性能接近DRAM。", "conclusion": "证实了将块存储集成到CXL生态系统的可行性，并为未来的内存存储融合提供了基础。", "translation": "本文探讨了计算高速互连（CXL）如何将基于PCIe的块存储转换为可扩展的、字节可寻址的工作内存。我们通过强调可缓存性作为关键促成因素，并倡导使用Type 3端点设备（称为CXL-SSDs），解决了块存储适应CXL内存中心模型所面临的挑战。为了验证我们的方法，我们在定制的FPGA平台上原型化了一个CXL-SSD，并提出了“确定性”和“可缓冲性”注解机制，以在提高性能的同时保持数据持久性。我们基于仿真的评估表明，CXL-SSD比基于PCIe的内存扩展器性能提高10.9倍，并通过注解增强进一步将延迟降低5.4倍。在高局部性工作负载下，由于高效的片上缓存，CXL-SSD性能接近DRAM。这项工作突出了将块存储集成到CXL生态系统的可行性，并为未来的内存存储融合提供了基础。", "summary": "本文研究了如何利用CXL协议将PCIe SSD从传统的块存储转换为高效的字节可寻址工作内存。通过引入CXL-SSD概念、FPGA原型和创新的注解机制（确定性与可缓冲性），作者成功提升了存储性能和效率，使其在特定场景下接近DRAM，并验证了块存储与CXL内存模型融合的可行性，为内存存储一体化奠定了基础。", "keywords": "CXL, PCIe SSD, 字节可寻址内存, 内存存储融合, 指令注解", "comments": "这项工作创新性地将PCIe SSD与CXL内存协议结合，提出CXL-SSD的概念，并通过FPGA原型和指令注解（确定性、可缓冲性）实现了显著的性能提升。它为内存与存储的融合提供了新的思路和技术路径，对于构建更高效、更灵活的异构计算系统具有重要意义。其贡献在于展示了块存储转换为内存的可行性及高性能潜力。"}}
{"id": "2506.15176", "title": "In-Context Learning for Gradient-Free Receiver Adaptation: Principles, Applications, and Theory", "authors": ["Matteo Zecchin", "Tomer Raviv", "Dileep Kalathil", "Krishna Narayanan", "Nir Shlezinger", "Osvaldo Simeone"], "summary": "In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15176v1", "AI": {"title_translation": "上下文学习在无梯度接收器自适应中的应用：原理、应用与理论", "tldr": "本文提出了一种基于上下文学习（ICL）的无梯度接收器自适应技术，克服了传统深度学习接收器适应方法的局限性，实现了实时高效的接收器自适应，无需在线再训练。", "motivation": "当前的深度学习无线接收器自适应策略（包括联合训练、基于超网络的方法和元学习）存在灵活性有限或需要通过梯度下降进行显式优化的问题。", "method": "本文提出了一种基于上下文学习（ICL）的无梯度自适应技术。研究回顾了基于Transformer模型和结构化状态空间模型（SSM）的ICL架构框架，并提供了序列模型如何从上下文信息中学习自适应的理论见解。此外，还探讨了ICL在无蜂窝大规模MIMO网络中的应用，并提供了理论分析和实证证据。", "result": "研究结果表明，上下文学习（ICL）代表了一种使用导频信号和辅助上下文信息进行实时接收器自适应的原则性且高效的方法。", "conclusion": "上下文学习（ICL）为无线通信系统中的实时接收器自适应提供了一种原则性且高效的解决方案，实现了动态适应而无需在线再训练或梯度下降优化，从而解决了先前深度学习方法的局限性。", "translation": "近年来，深度学习促进了无线接收器的创建，使其能够在传统基于模型的設計难以应对的条件下有效运行。利用可编程硬件架构，基于深度学习的接收器提供了动态适应不同信道环境的潜力。然而，当前的自适应策略，包括联合训练、基于超网络的 методы 和元学习，要么灵活性有限，要么需要通过梯度下降进行显式优化。本文提出了一种植根于新兴的上下文学习（ICL）范式的无梯度自适应技术。我们回顾了基于 Transformer 模型和结构化状态空间模型（SSM）的 ICL 架构框架，并提供了关于序列模型如何有效地从上下文信息中学习自适应的理论见解。此外，我们探讨了 ICL 在无蜂窝大规模 MIMO 网络中的应用，提供了理论分析和实证证据。我们的研究结果表明，ICL 代表了一种使用导频信号和辅助上下文信息进行实时接收器自适应的原则性且高效的方法——无需在线再训练。", "summary": "本文提出了一种基于上下文学习（ICL）的无梯度接收器自适应技术，旨在克服现有深度学习自适应方法灵活性不足或需要梯度下降优化的问题。研究回顾了基于Transformer和SSM的ICL架构，提供了理论洞察，并将其应用于无蜂窝大规模MIMO网络，通过理论分析和实证证据验证了其有效性。研究表明，ICL是一种利用导频信号和辅助上下文信息进行实时接收器自适应的原则性且高效的方法，无需在线再训练。", "keywords": "上下文学习, 接收器自适应, 无梯度, 无线通信, 大规模MIMO", "comments": "该论文通过将新兴的上下文学习（ICL）范式应用于无线接收器自适应，展现了创新性。其对无梯度自适应的关注尤为重要，因为它规避了在线梯度下降相关的计算开销和复杂性，使实时部署更具可行性。对无蜂窝大规模MIMO网络的理论和实证验证，突显了其在未来无线通信系统中的实际相关性和潜在影响。"}}
{"id": "2506.15418", "title": "RISC-V for HPC: An update of where we are and main action points", "authors": ["Nick Brown"], "summary": "This extended abstract is submitted on behalf of the RISC-V HPC SIG who have\nbeen undertaking an analysis to explore the current state and limitations of\nthe RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has\nbeen great progress made in recent years, we also highlight limitations and\nwhere effort should be focussed.", "comment": "Extended abstract accepted to the RISC-V Summit Europe 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15418v1", "AI": {"title_translation": "RISC-V 在 HPC 中的应用：现状更新与主要行动点", "tldr": "RISC-V HPC SIG 提交了一份扩展摘要，分析了 RISC-V 生态系统在高性能计算（HPC）领域的现状和局限性，并指出了未来的重点发展方向。", "motivation": "探索 RISC-V 生态系统在高性能计算（HPC）领域的当前状态和局限性。", "method": "由 RISC-V HPC SIG 进行的一项分析。", "result": "RISC-V 在高性能计算领域取得了显著进展，但也存在局限性。", "conclusion": "需要识别并集中精力解决 RISC-V 在高性能计算领域的现有局限性。", "translation": "这份扩展摘要代表 RISC-V HPC SIG 提交，该组织一直在进行一项分析，以探讨 RISC-V 生态系统在高性能计算（HPC）领域的当前状态和局限性。虽然我们理应庆祝近年来取得的巨大进展，但我们也强调了局限性以及应集中精力的地方。", "summary": "RISC-V HPC SIG 发布了一份扩展摘要，详细分析了 RISC-V 生态系统在高性能计算（HPC）领域的现状及面临的挑战。报告肯定了近年来取得的显著进步，同时强调了现有局限性，并明确了未来需要重点投入的方向。", "keywords": "RISC-V, HPC, 高性能计算, 生态系统, 局限性", "comments": "该论文（扩展摘要）提供了一个关于 RISC-V 在 HPC 领域发展的全面视角，不仅庆祝了已取得的成就，更重要的是指出了需要改进和关注的关键领域，这对于推动 RISC-V 在 HPC 领域的进一步发展具有重要指导意义。"}}
{"id": "2506.14827", "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning", "authors": ["Yifeng Gao", "Yifan Ding", "Hongyu Su", "Juncheng Li", "Yunhan Zhao", "Lin Luo", "Zixing Chen", "Li Wang", "Xin Wang", "Yixu Wang", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As AI-generated video becomes increasingly pervasive across media platforms,\nthe ability to reliably distinguish synthetic content from authentic footage\nhas become both urgent and essential. Existing approaches have primarily\ntreated this challenge as a binary classification task, offering limited\ninsight into where or why a model identifies a video as AI-generated. However,\nthe core challenge extends beyond simply detecting subtle artifacts; it\nrequires providing fine-grained, persuasive evidence that can convince auditors\nand end-users alike. To address this critical gap, we introduce DAVID-X, the\nfirst dataset to pair AI-generated videos with detailed defect-level,\ntemporal-spatial annotations and written rationales. Leveraging these rich\nannotations, we present DAVID-XR1, a video-language model designed to deliver\nan interpretable chain of visual reasoning-including defect categorization,\ntemporal-spatial localization, and natural language explanations. This approach\nfundamentally transforms AI-generated video detection from an opaque black-box\ndecision into a transparent and verifiable diagnostic process. We demonstrate\nthat a general-purpose backbone, fine-tuned on our compact dataset and enhanced\nwith chain-of-thought distillation, achieves strong generalization across a\nvariety of generators and generation modes. Our results highlight the promise\nof explainable detection methods for trustworthy identification of AI-generated\nvideo content.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14827v1", "AI": {"title_translation": "DAVID-XR1：通过可解释推理检测AI生成视频", "tldr": "引入DAVID-XR1，一个视频-语言模型和DAVID-X数据集，用于可解释地检测AI生成视频，提供缺陷定位、分类和自然语言解释，而非仅仅二元分类。", "motivation": "AI生成视频日益普及，可靠区分真伪内容变得紧迫且必要。现有方法仅进行二元分类，缺乏解释性，无法提供为何识别为AI生成的证据，难以说服审计员和最终用户。", "method": "1. 引入DAVID-X数据集，首次将AI生成视频与详细的缺陷级别、时空标注及书面理由配对。2. 基于DAVID-X，开发DAVID-XR1视频-语言模型，旨在提供可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。3. 使用通用骨干网络，在DAVID-X上进行微调，并增强链式思考蒸馏。", "result": "该方法（通用骨干网络在紧凑数据集上微调并结合链式思考蒸馏）在各种生成器和生成模式下实现了强大的泛化能力。结果强调了可解释检测方法在可信识别AI生成视频内容方面的潜力。", "conclusion": "通过引入可解释的视觉推理（包括缺陷分类、时空定位和自然语言解释），该研究将AI生成视频检测从不透明的“黑箱”决策转变为透明、可验证的诊断过程，为可信地识别AI生成视频内容提供了有前景的解决方案。", "translation": "随着AI生成视频在媒体平台上的日益普及，可靠地区分合成内容与真实素材的能力变得既紧迫又必不可少。现有方法主要将这一挑战视为一个二元分类任务，对模型在何处或为何将视频识别为AI生成的内容提供了有限的洞察。然而，核心挑战不仅仅是简单地检测细微的伪影；它需要提供细粒度、有说服力的证据，能够同时说服审计人员和最终用户。为了解决这一关键空白，我们引入了DAVID-X，这是第一个将AI生成视频与详细的缺陷级别、时空标注和书面理由配对的数据集。利用这些丰富的标注，我们提出了DAVID-XR1，一个视频-语言模型，旨在提供一个可解释的视觉推理链——包括缺陷分类、时空定位和自然语言解释。这种方法从根本上将AI生成视频检测从不透明的“黑箱”决策转变为一个透明和可验证的诊断过程。我们证明，一个通用骨干网络，在我们的紧凑数据集上进行微调，并结合链式思考蒸馏增强，在各种生成器和生成模式下实现了强大的泛化能力。我们的结果突出了可解释检测方法在可信识别AI生成视频内容方面的潜力。", "summary": "本文针对AI生成视频检测中缺乏解释性的问题，提出了DAVID-X数据集和DAVID-XR1视频-语言模型。DAVID-X是首个包含缺陷级别、时空标注和书面理由的AI生成视频数据集。DAVID-XR1模型利用这些标注，提供可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释，从而将检测过程从不透明的二元分类转变为透明可验证的诊断。实验证明，该方法在多种生成器和模式下表现出强大的泛化能力，突出了可解释检测在可信识别AI生成视频内容方面的潜力。", "keywords": "AI生成视频检测, 可解释性, 视频-语言模型, DAVID-X数据集, 链式思考蒸馏", "comments": "这篇论文的创新点在于它不仅仅关注AI生成视频的检测结果，更重要的是提供了可解释的推理过程。通过引入DAVID-X数据集及其细粒度的标注，以及DAVID-XR1模型的可解释视觉推理链，解决了现有“黑箱”检测方法的痛点。这种透明化的诊断过程对于建立用户信任和在法律、审计等领域提供有力证据至关重要，具有重要的实际应用价值。"}}
{"id": "2506.15572", "title": "Misinformation by Omission: The Need for More Environmental Transparency in AI", "authors": ["Sasha Luccioni", "Boris Gamazaychikov", "Theo Alves da Costa", "Emma Strubell"], "summary": "In recent years, Artificial Intelligence (AI) models have grown in size and\ncomplexity, driving greater demand for computational power and natural\nresources. In parallel to this trend, transparency around the costs and impacts\nof these models has decreased, meaning that the users of these technologies\nhave little to no information about their resource demands and subsequent\nimpacts on the environment. Despite this dearth of adequate data, escalating\ndemand for figures quantifying AI's environmental impacts has led to numerous\ninstances of misinformation evolving from inaccurate or de-contextualized\nbest-effort estimates of greenhouse gas emissions. In this article, we explore\npervasive myths and misconceptions shaping public understanding of AI's\nenvironmental impacts, tracing their origins and their spread in both the media\nand scientific publications. We discuss the importance of data transparency in\nclarifying misconceptions and mitigating these harms, and conclude with a set\nof recommendations for how AI developers and policymakers can leverage this\ninformation to mitigate negative impacts in the future.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15572v1", "AI": {"title_translation": "遗漏造成的信息误导：人工智能领域环境透明度的必要性", "tldr": "人工智能模型日益增长的资源需求缺乏透明度，导致对其环境影响的错误信息。本文探讨了这些误解，并强调数据透明度对于缓解危害的重要性。", "motivation": "近年来，人工智能（AI）模型在规模和复杂性上不断增长，导致对计算能力和自然资源的需求增加。与此同时，这些模型的成本和影响的透明度却在下降，使得技术用户对其资源需求和随之而来的环境影响知之甚少。尽管缺乏足够的数据，但对量化AI环境影响的需求不断升级，导致了许多源于不准确或脱离语境的温室气体排放估算的错误信息。", "method": "本文探讨了影响公众对AI环境影响理解的普遍神话和误解，追溯了它们在媒体和科学出版物中的起源和传播。文章讨论了数据透明度在澄清误解和减轻这些危害方面的重要性。", "result": "文章探讨了关于AI环境影响的普遍神话和误解，并追溯了它们在媒体和科学出版物中的起源和传播。它强调了数据透明度在澄清误解和减轻危害方面的重要性。", "conclusion": "文章最后提出了一系列建议，旨在指导AI开发者和政策制定者如何利用这些信息来减轻未来的负面影响，强调了数据透明度的关键作用。", "translation": "近年来，人工智能（AI）模型在规模和复杂性上不断增长，导致对计算能力和自然资源的需求增加。与此同时，这些模型的成本和影响的透明度却在下降，使得技术用户对其资源需求和随之而来的环境影响知之甚少。尽管缺乏足够的数据，但对量化AI环境影响的需求不断升级，导致了许多源于不准确或脱离语境的温室气体排放估算的错误信息。在本文中，我们探讨了影响公众对AI环境影响理解的普遍神话和误解，追溯了它们在媒体和科学出版物中的起源和传播。我们讨论了数据透明度在澄清误解和减轻这些危害方面的重要性，并以一系列建议作结，指导AI开发者和政策制定者如何利用这些信息来减轻未来的负面影响。", "summary": "随着AI模型规模和复杂性的增长，其对计算和自然资源的需求也随之增加，但相关环境影响的透明度却在下降。这种信息缺失导致了大量关于AI环境影响的错误信息。本文旨在探讨这些普遍存在的误解，追溯其来源和传播，并强调数据透明度在澄清误解和减轻危害方面的关键作用。最终，文章为AI开发者和政策制定者提供了利用信息来减轻未来负面环境影响的建议。", "keywords": "AI环境影响, 数据透明度, 信息误导, 碳排放, 资源消耗", "comments": "本文创新性地提出了“遗漏造成的信息误导”这一概念，聚焦于人工智能环境影响的特定背景，视角独特。它解决了AI可持续性方面日益增长的关键问题，并倡导将透明度作为核心解决方案。抽象内容并未详细说明具体的建议或神话探讨的深度。"}}
{"id": "2506.15124", "title": "A Force Feedback Exoskeleton for Teleoperation Using Magnetorheological Clutches", "authors": ["Zhongyuan Kong", "Lei Li", "Erwin Ang Tien Yew", "Zirui Chen", "Wenbo Li", "Shiwu Zhang", "Jian Yang", "Shuaishuai Sun"], "summary": "This paper proposes an upper-limb exoskeleton teleoperation system based on\nmagnetorheological (MR) clutches, aiming to improve operational accuracy and\nenhance the immersive experience during lunar sampling tasks. Conventional\nexoskeleton teleoperation systems commonly employ active force feedback\nsolutions, such as servo motors, which typically suffer from high system\ncomplexity and increased energy consumption. Furthermore, force feedback\ndevices utilizing motors and gear reducers generally compromise backdrivability\nand pose safety risks to operators due to active force output. To address these\nlimitations, we propose a semi-active force feedback strategy based on MR\nclutches. Dynamic magnetic field control enables precise adjustment of joint\nstiffness and damping, thereby providing smooth and high-resolution force\nfeedback. The designed MR clutch exhibits outstanding performance across key\nmetrics, achieving a torque-to-mass ratio (TMR) of 93.6 Nm/kg, a\ntorque-to-volume ratio (TVR) of 4.05 x 10^5 Nm/m^3, and a torque-to-power ratio\n(TPR) of 4.15 Nm/W. Notably, the TMR represents an improvement of approximately\n246% over a representative design in prior work. Experimental results validate\nthe system's capability to deliver high-fidelity force feedback. Overall, the\nproposed system presents a promising solution for deep-space teleoperation with\nstrong potential for real-world deployment in future missions.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15124v1", "AI": {"title_translation": "一种基于磁流变离合器的遥操作力反馈外骨骼", "tldr": "该论文提出了一种基于磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在为月球采样任务提供高精度和沉浸式体验的力反馈。", "motivation": "传统的力反馈外骨骼遥操作系统通常采用伺服电机等主动式解决方案，但存在系统复杂性高、能耗大、回驱性差以及对操作员存在安全风险等问题。", "method": "提出了一种基于磁流变（MR）离合器的半主动力反馈策略。通过动态磁场控制，实现关节刚度和阻尼的精确调节，从而提供平滑、高分辨率的力反馈。", "result": "设计的MR离合器在关键指标上表现出色：力矩质量比（TMR）达到93.6 Nm/kg，力矩体积比（TVR）达到4.05 x 10^5 Nm/m^3，力矩功率比（TPR）达到4.15 Nm/W。其中，TMR比现有工作中代表性设计提升了约246%。实验结果验证了系统提供高保真力反馈的能力。", "conclusion": "所提出的系统为深空遥操作提供了一个有前景的解决方案，在未来任务中具有强大的实际部署潜力。", "translation": "本文提出了一种基于磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在提高月球采样任务中的操作精度并增强沉浸式体验。传统的力反馈外骨骼遥操作系统通常采用伺服电机等主动力反馈解决方案，这些方案通常存在系统复杂性高和能耗增加的问题。此外，利用电机和齿轮减速器的力反馈装置通常会损害回驱性，并且由于主动力输出而对操作员构成安全风险。为了解决这些限制，我们提出了一种基于MR离合器的半主动力反馈策略。动态磁场控制能够精确调整关节刚度和阻尼，从而提供平滑和高分辨率的力反馈。所设计的MR离合器在关键指标上表现出色，实现了93.6 Nm/kg的力矩质量比（TMR）、4.05 x 10^5 Nm/m^3的力矩体积比（TVR）和4.15 Nm/W的力矩功率比（TPR）。值得注意的是，TMR比现有工作中的代表性设计提高了约246%。实验结果验证了系统提供高保真力反馈的能力。总的来说，所提出的系统为深空遥操作提供了一个有前景的解决方案，在未来任务中具有强大的实际部署潜力。", "summary": "本文提出了一种基于磁流变（MR）离合器的半主动力反馈外骨骼系统，旨在解决传统主动式力反馈系统在遥操作中存在的复杂性、能耗高、回驱性差及安全性问题。该系统通过精确控制MR离合器的磁场，实现了关节刚度和阻尼的平滑、高分辨率调节，从而提供高保真力反馈。实验结果表明，所设计的MR离合器在力矩质量比、力矩体积比和力矩功率比方面表现优异，尤其力矩质量比相较现有设计提升显著。该系统为深空遥操作任务提供了一种有前景的解决方案。", "keywords": "磁流变离合器, 力反馈外骨骼, 遥操作, 半主动, 深空任务", "comments": "该论文的创新点在于将磁流变（MR）离合器应用于力反馈外骨骼系统，解决了传统主动式力反馈方案的固有缺陷。半主动策略提高了系统的安全性、回驱性，并显著改善了力矩密度指标，这对于空间遥操作等对重量和能效有严格要求的应用至关重要。其高保真力反馈能力和在深空任务中的潜力使其具有重要的实际应用价值。"}}
{"id": "2506.15225", "title": "Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels", "authors": ["Jiahao You", "Ziye Jia", "Chao Dong", "Qihui Wu", "Zhu Han"], "summary": "The computation demands from the maritime Internet of Things (MIoT) increase\nrapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels\nbased multi-access edge computing (MEC) can fulfill these MIoT requirements.\nHowever, the uncertain maritime tasks present significant challenges of\ninefficient computation offloading and resource allocation. In this paper, we\nfocus on the maritime computation offloading and resource allocation through\nthe cooperation of UAVs and vessels, with consideration of uncertain tasks.\nSpecifically, we propose a cooperative MEC framework for computation offloading\nand resource allocation, including MIoT devices, UAVs and vessels. Then, we\nformulate the optimization problem to minimize the total execution time. As for\nthe uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the\nunpredictable task arrivals and varying computational resource availability. By\nconverting the long-term constraints into short-term constraints, we obtain a\nset of small-scale optimization problems. Further, considering the\nheterogeneity of actions and resources of UAVs and vessels, we reformulate the\nsmall-scale optimization problem into a Markov game (MG). Moreover, a\nheterogeneous-agent soft actor-critic is proposed to sequentially update\nvarious neural networks and effectively solve the MG problem. Finally,\nsimulations are conducted to verify the effectiveness in addressing\ncomputational offloading and resource allocation.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15225v1", "AI": {"title_translation": "考虑无人机与船舶协作的不确定性海洋MEC联合计算卸载与资源分配", "tldr": "本文针对海洋物联网MEC中不确定任务的计算卸载与资源分配问题，提出了一种基于无人机与船舶协作的MEC框架，并利用Lyapunov优化和异构智能体软 Actor-Critic 算法求解，以最小化总执行时间。", "motivation": "海洋物联网（MIoT）的计算需求迅速增长，而基于无人机（UAV）和船舶的多接入边缘计算（MEC）可以满足这些需求。然而，不确定的海洋任务在计算卸载和资源分配方面带来了低效的重大挑战。", "method": "1. 提出一个合作MEC框架，包括MIoT设备、无人机和船舶。2. 将优化问题表述为最小化总执行时间。3. 利用Lyapunov优化处理不确定的任务到达和变化的计算资源可用性，将长期约束转换为短期约束，得到一系列小规模优化问题。4. 考虑到无人机和船舶动作与资源的异构性，将小规模优化问题重构为马尔可夫博弈（MG）。5. 提出一种异构智能体软 Actor-Critic (HAC) 算法来顺序更新神经网络并有效解决MG问题。", "result": "仿真验证了所提方法在解决计算卸载和资源分配方面的有效性。", "conclusion": "本文提出的基于无人机和船舶协作的MEC框架以及利用Lyapunov优化和异构智能体软 Actor-Critic 算法，能够有效解决不确定海洋任务下的计算卸载和资源分配问题，并最小化总执行时间。", "translation": "近年来，海洋物联网（MIoT）的计算需求迅速增长，基于无人机（UAV）和船舶的多接入边缘计算（MEC）可以满足这些MIoT需求。然而，不确定的海洋任务给低效的计算卸载和资源分配带来了重大挑战。在本文中，我们重点关注通过无人机和船舶的合作进行海洋计算卸载和资源分配，并考虑了不确定性任务。具体而言，我们提出了一个用于计算卸载和资源分配的协作MEC框架，其中包括MIoT设备、无人机和船舶。然后，我们制定了最小化总执行时间的优化问题。对于不确定的MIoT任务，我们利用Lyapunov优化来处理不可预测的任务到达和变化的计算资源可用性。通过将长期约束转换为短期约束，我们获得了一组小规模的优化问题。此外，考虑到无人机和船舶动作和资源的异构性，我们将小规模优化问题重新表述为马尔可夫博弈（MG）。此外，提出了一种异构智能体软 Actor-Critic 算法来顺序更新各种神经网络并有效解决MG问题。最后，进行了仿真以验证其在解决计算卸载和资源分配方面的有效性。", "summary": "本文针对海洋物联网（MIoT）中不确定任务的计算卸载和资源分配问题，提出了一种基于无人机和船舶协作的MEC框架。该框架旨在最小化总执行时间，并通过Lyapunov优化处理任务不确定性，将问题转化为一系列小规模优化。进一步，考虑到异构性，将问题建模为马尔可夫博弈，并引入异构智能体软 Actor-Critic 算法进行求解。仿真结果验证了该方法的有效性。", "keywords": "海洋MEC, 计算卸载, 资源分配, 无人机, 马尔可夫博弈", "comments": "本文的创新点在于将无人机与船舶的协作引入海洋MEC，以应对不确定性任务带来的挑战。通过结合Lyapunov优化处理长期不确定性，并利用马尔可夫博弈和异构智能体软 Actor-Critic 算法解决异构多智能体决策问题，提供了一个全面的解决方案。该研究对于提升海洋物联网的计算效率和资源利用率具有重要意义。"}}
{"id": "2506.15148", "title": "Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation", "authors": ["Yuxuan Xia", "Ángel F. García-Fernández", "Johan Karlsson", "Yu Ge", "Lennart Svensson", "Ting Yuan"], "summary": "This paper presents a generalization of the trajectory general optimal\nsub-pattern assignment (GOSPA) metric for evaluating multi-object tracking\nalgorithms that provide trajectory estimates with track-level uncertainties.\nThis metric builds on the recently introduced probabilistic GOSPA metric to\naccount for both the existence and state estimation uncertainties of individual\nobject states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a\nmultidimensional assignment problem, and its linear programming\nrelaxation--also a valid metric--is computable in polynomial time.\nAdditionally, this metric retains the interpretability of TGOSPA, and we show\nthat its decomposition yields intuitive costs terms associated to expected\nlocalization error and existence probability mismatch error for properly\ndetected objects, expected missed and false detection error, and track switch\nerror. The effectiveness of the proposed metric is demonstrated through a\nsimulation study.", "comment": "7 pages, 4 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15148v1", "AI": {"title_translation": "概率轨迹GOSPA：一种不确定性感知多目标跟踪性能评估指标", "tldr": "本文提出了一种新的概率轨迹GOSPA度量，用于评估考虑不确定性的多目标跟踪算法，并保持可解释性和可计算性。", "motivation": "现有的多目标跟踪算法评估指标可能无法充分考虑轨迹估计中的跟踪级别不确定性，因此需要一种新的度量来有效评估提供轨迹级别不确定性估计的算法。", "method": "论文提出了一种概率轨迹GOSPA度量，它是轨迹GOSPA（TGOSPA）的推广，并借鉴了概率GOSPA，以同时考虑单个目标状态的存在性和状态估计不确定性。该度量可被表述为多维分配问题，其线性规划松弛可以在多项式时间内计算。", "result": "所提出的概率轨迹GOSPA度量保留了TGOSPA的可解释性，其分解产生了与正确检测目标的预期定位误差、存在概率不匹配误差、预期漏检和误检误差以及跟踪切换误差相关的直观成本项。通过仿真研究证明了该度量的有效性。", "conclusion": "本文成功提出了一种新颖的概率轨迹GOSPA度量，能够有效评估考虑不确定性的多目标跟踪算法，并且具有良好的可解释性和计算效率。", "translation": "本文提出了一种轨迹通用最优子模式分配（GOSPA）度量的泛化形式，用于评估提供具有轨迹级别不确定性估计的多目标跟踪算法。该度量建立在最近引入的概率GOSPA度量的基础上，以同时考虑单个目标状态的存在性和状态估计不确定性。类似于轨迹GOSPA（TGOSPA），它可以被表述为一个多维分配问题，并且其线性规划松弛——同样是一个有效的度量——可以在多项式时间内计算。此外，该度量保留了TGOSPA的可解释性，我们展示了其分解产生了与正确检测目标的预期定位误差和存在概率不匹配误差、预期漏检和误检误差以及跟踪切换误差相关的直观成本项。通过仿真研究证明了所提出度量的有效性。", "summary": "本文提出了一种名为概率轨迹GOSPA的新型度量，用于评估考虑轨迹级别不确定性的多目标跟踪算法。该度量是轨迹GOSPA的推广，能够同时量化目标的存在性和状态估计不确定性。它可被公式化为多维分配问题，其松弛可在多项式时间内计算，并能分解出多种直观的误差成本项，其有效性已通过仿真得到验证。", "keywords": "多目标跟踪, GOSPA, 不确定性, 性能评估, 概率度量", "comments": "这篇论文的创新点在于将GOSPA度量扩展到能够处理多目标跟踪中的轨迹级别不确定性，这对于评估实际应用中更复杂的跟踪系统至关重要。其保持可解释性和可计算性的特点增强了其实用性。"}}
{"id": "2506.14786", "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series", "authors": ["Haobo Li", "Eunseo Jung", "Zixin Chen", "Zhaowei Wang", "Yueya Wang", "Huamin Qu", "Alexis Kai Hon Lau"], "summary": "Multimodal time series forecasting is foundational in various fields, such as\nutilizing satellite imagery and numerical data for predicting typhoons in\nclimate science. However, existing multimodal approaches primarily focus on\nutilizing text data to help time series forecasting, leaving the visual data in\nexisting time series datasets untouched. Furthermore, it is challenging for\nmodels to effectively capture the physical information embedded in visual data,\nsuch as satellite imagery's temporal and geospatial context, which extends\nbeyond images themselves. To address this gap, we propose physics-informed\npositional encoding (PIPE), a lightweight method that embeds physical\ninformation into vision language models (VLMs). PIPE introduces two key\ninnovations: (1) a physics-informed positional indexing scheme for mapping\nphysics to positional IDs, and (2) a variant-frequency positional encoding\nmechanism for encoding frequency information of physical variables and\nsequential order of tokens within the embedding space. By preserving both the\nphysical information and sequential order information, PIPE significantly\nimproves multimodal alignment and forecasting accuracy. Through the experiments\non the most representative and the largest open-sourced satellite image\ndataset, PIPE achieves state-of-the-art performance in both deep learning\nforecasting and climate domain methods, demonstrating superiority across\nbenchmarks, including a 12% improvement in typhoon intensity forecasting over\nprior works. Our code is provided in the supplementary material.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14786v1", "AI": {"title_translation": "PIPE：物理信息位置编码用于卫星图像和时间序列的对齐", "tldr": "PIPE是一种轻量级方法，通过物理信息位置编码改进了视觉语言模型（VLMs）中卫星图像和时间序列的多模态对齐和预测准确性，在台风强度预测中取得了SOTA性能。", "motivation": "现有的多模态方法主要利用文本数据进行时间序列预测，忽略了视觉数据，且模型难以有效捕获视觉数据中嵌入的物理信息（如卫星图像的时空上下文）。", "method": "提出了物理信息位置编码（PIPE），这是一种轻量级方法，通过以下两项创新将物理信息嵌入到视觉语言模型（VLMs）中：1) 物理信息位置索引方案，用于将物理信息映射到位置ID；2) 变频位置编码机制，用于编码物理变量的频率信息和嵌入空间中token的顺序。", "result": "PIPE在最具代表性和最大的开源卫星图像数据集上实现了最先进的性能，在深度学习预测和气候领域方法中都表现出色，包括在台风强度预测方面比现有工作提高了12%。", "conclusion": "PIPE通过有效整合物理信息和序列顺序信息，显著提高了多模态对齐和预测准确性，特别是在卫星图像时间序列分析任务中展现出卓越的性能。", "translation": "多模态时间序列预测在各个领域都是基础，例如利用卫星图像和数值数据预测气候科学中的台风。然而，现有的多模态方法主要侧重于利用文本数据帮助时间序列预测，而现有时间序列数据集中的视觉数据则未被触及。此外，模型难以有效捕获视觉数据中嵌入的物理信息，例如卫星图像的时空上下文，这些信息超出了图像本身。为了解决这一空白，我们提出了物理信息位置编码（PIPE），这是一种将物理信息嵌入到视觉语言模型（VLMs）中的轻量级方法。PIPE引入了两项关键创新：（1）一种物理信息位置索引方案，用于将物理信息映射到位置ID；（2）一种变频位置编码机制，用于编码物理变量的频率信息和嵌入空间中token的顺序。通过保留物理信息和序列顺序信息，PIPE显著提高了多模态对齐和预测准确性。通过在最具代表性和最大的开源卫星图像数据集上进行的实验，PIPE在深度学习预测和气候领域方法中都取得了最先进的性能，在各项基准测试中均表现出优越性，包括在台风强度预测方面比现有工作提高了12%。我们的代码在补充材料中提供。", "summary": "本文提出了一种名为PIPE（物理信息位置编码）的轻量级方法，旨在解决现有模型在多模态时间序列预测中对视觉数据（特别是卫星图像）中物理信息利用不足的问题。PIPE通过引入物理信息位置索引和变频位置编码机制，将物理信息（如时空上下文）有效地嵌入到视觉语言模型中。实验结果表明，PIPE在大型卫星图像数据集上显著提升了多模态对齐和预测精度，尤其在台风强度预测上取得了12%的性能提升，达到了最先进水平。", "keywords": "物理信息位置编码, 卫星图像, 时间序列预测, 多模态对齐, 视觉语言模型", "comments": "PIPE的创新之处在于其将物理信息（如时空上下文）巧妙地编码到位置编码中，从而弥补了现有模型在处理视觉时间序列数据时忽视物理信息的不足。这种方法不仅轻量级，而且在实际应用中（如台风预测）展现出显著的性能提升，为多模态时间序列预测，尤其是涉及遥感图像的领域，提供了一个有前景的新方向。"}}
{"id": "2506.15121", "title": "Generative thermodynamic computing", "authors": ["Stephen Whitelam"], "summary": "We introduce a generative modeling framework for thermodynamic computing, in\nwhich structured data is synthesized from noise by the natural time evolution\nof a physical system governed by Langevin dynamics. While conventional\ndiffusion models use neural networks to perform denoising, here the information\nneeded to generate structure from noise is encoded by the dynamics of a\nthermodynamic system. Training proceeds by maximizing the probability with\nwhich the computer generates the reverse of a noising trajectory, which ensures\nthat the computer generates data with minimal heat emission. We demonstrate\nthis framework within a digital simulation of a thermodynamic computer. If\nrealized in analog hardware, such a system would function as a generative model\nthat produces structured samples without the need for artificially-injected\nnoise or active control of denoising.", "comment": null, "cate": "cond-mat.stat-mech", "url": "http://arxiv.org/abs/2506.15121v1", "AI": {"title_translation": "生成式热力学计算", "tldr": "本文提出了一种生成式热力学计算的生成建模框架，利用朗之万动力学控制的物理系统自然演化从噪声中合成结构化数据，旨在实现低热量排放且无需人工注入噪声或主动控制的生成模型。", "motivation": "传统的扩散模型依赖神经网络进行去噪，且需要人工注入噪声或主动控制。本文旨在开发一种新的生成模型，其结构生成信息由热力学系统动力学编码，以实现更低的热量排放和更简单的操作。", "method": "该论文引入了一种用于热力学计算的生成建模框架。结构化数据通过受朗之万动力学支配的物理系统的自然时间演化从噪声中合成。与传统扩散模型不同，生成结构所需的信息被编码在热力学系统的动力学中。训练过程通过最大化计算机生成去噪轨迹逆向的概率进行，以确保最小的热量排放。", "result": "该框架已在热力学计算机的数字模拟中得到演示。", "conclusion": "如果以模拟硬件实现，该系统将能够作为一种生成模型，无需人工注入噪声或主动控制去噪即可生成结构化样本，并具有最小的热量排放。", "translation": "我们引入了一个用于热力学计算的生成建模框架，其中结构化数据通过受朗之万动力学支配的物理系统的自然时间演化从噪声中合成。虽然传统的扩散模型使用神经网络进行去噪，但在此处，从噪声中生成结构所需的信息由热力学系统的动力学编码。训练通过最大化计算机生成去噪轨迹逆向的概率进行，这确保了计算机以最小的热量排放生成数据。我们在热力学计算机的数字模拟中演示了该框架。如果以模拟硬件实现，这样的系统将作为一种生成模型，无需人工注入噪声或主动控制去噪即可生成结构化样本。", "summary": "本文提出了一种新颖的“生成式热力学计算”生成建模框架。它通过朗之万动力学控制的物理系统固有时间演化从噪声中合成结构化数据，这与基于神经网络的扩散模型不同。结构生成的信息嵌入在热力学系统的动力学中。训练过程优化了去噪轨迹的逆向生成，从而实现最小的热量排放。该框架的潜力已通过数字模拟得到验证。作者提出，模拟硬件实现可能产生一个无需外部噪声注入或主动去噪控制即可生成结构化样本的生成模型。", "keywords": "生成建模, 热力学计算, 朗之万动力学, 扩散模型, 模拟硬件", "comments": "本文提出了一种利用热力学系统内在动力学进行生成建模的创新方法，从根本上区别于当前以神经网络为中心的扩散模型。其将信息直接编码到物理动力学中并实现最小热量排放的焦点，代表了概念上的重大进步。模拟硬件实现的潜力，无需人工噪声或主动控制，预示着一种更物理高效和自主的生成计算范式，这可能为硬件加速的生成式AI开辟新途径。"}}
{"id": "2506.15001", "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings", "authors": ["Ignacio Sastre", "Aiala Rosá"], "summary": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.", "comment": "This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15001v1", "AI": {"title_translation": "记忆令牌：大型语言模型可以生成可逆的句子嵌入", "tldr": "大型语言模型（LLM）可以通过引入特殊记忆令牌生成可逆的句子嵌入，从而精确重建原始文本，且无需修改模型权重。", "motivation": "本文观察到一个有趣的现象：大型语言模型（LLM）可以生成可逆的句子嵌入，允许其精确重建原始文本，而无需修改模型权重。这表明了LLM的潜在新能力。", "method": "通过引入一个特殊的“记忆令牌”，其嵌入通过在固定序列上训练进行优化。当LLM被这个嵌入提示时，它能精确重建原始的固定序列。", "result": "该现象在英语和西班牙语数据集、长达约240个令牌的序列以及100M到8B参数的模型规模上进行了评估。值得注意的是，Llama 3.1 8B成功重建了所有测试序列。", "conclusion": "这些发现突出了LLM的一个有趣能力，并暗示了在基于记忆的检索、压缩和受控文本生成方面的潜在应用。", "translation": "在这项工作中，我们观察到一个有趣的现象：可以生成可逆的句子嵌入，允许大型语言模型（LLM）精确重建原始文本，而无需修改模型的权重。这是通过引入一个特殊的记忆令牌来实现的，其嵌入通过在固定序列上训练进行优化。当模型被这个嵌入提示时，它能精确重建固定序列。我们在英语和西班牙语数据集、长达约240个令牌的序列以及100M到8B参数的模型规模上评估了这一现象。值得注意的是，Llama 3.1 8B成功重建了所有测试序列。我们的发现突出了LLM的一个有趣能力，并暗示了在基于记忆的检索、压缩和受控文本生成方面的潜在应用。", "summary": "本文发现大型语言模型（LLM）能够在不修改模型权重的情况下生成可逆的句子嵌入，从而精确重建原始文本。研究通过优化一个特殊“记忆令牌”的嵌入来实现这一目标，并在多语言、不同序列长度及多种模型规模上进行了验证，其中Llama 3.1 8B表现出色。这一发现揭示了LLM的新能力，并预示其在记忆检索、文本压缩和受控生成等领域的应用潜力。", "keywords": "记忆令牌, 可逆句子嵌入, 大型语言模型, 文本重建, 记忆检索", "comments": "这项工作揭示了LLM一个令人惊讶且重要的内在能力，即在不改变模型本身的情况下，通过外部“记忆令牌”实现精确的文本可逆编码。这种方法提供了一种新颖的文本表示和重建范式，其创新性在于利用了LLM强大的编码和解码潜力。其重要性体现在为记忆系统、数据压缩和精确文本生成提供了新的思路和工具。"}}
{"id": "2506.14807", "title": "An explicit computational approach for a three-dimensional system of nonlinear elastodynamic sine-Gordon problem", "authors": ["Eric Ngondiep"], "summary": "This paper proposes an explicit computational method for solving a\nthree-dimensional system of nonlinear elastodynamic sine-Gordon equations\nsubject to appropriate initial and boundary conditions. The time derivative is\napproximated by interpolation technique whereas the finite element approach is\nused to approximate the space derivatives. The developed numerical scheme is\nso-called, high-order explicit computational technique. The new algorithm\nefficiently treats the time derivative term and provides a suitable time step\nrestriction for stability and convergence. Under this time step limitation,\nboth stability and error estimates of the proposed approach are deeply analyzed\nusing a constructed strong norm. The theoretical studies indicate that the\ndeveloped approach is temporal second-order convergent and spatially\nthird-order accurate. Some numerical examples are carried out to confirm the\ntheory, to validate the computational efficiency and to demonstrate the\npractical applicability of the new computational technique.", "comment": "21 pages, 20 figures, 6 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14807v1", "AI": {"title_translation": "三维非线性弹性动力学正弦-戈登问题的显式计算方法", "tldr": "提出了一种用于求解三维非线性弹性动力学正弦-戈登方程组的高阶显式计算方法，并分析了其稳定性和收敛性。", "motivation": "旨在为三维非线性弹性动力学正弦-戈登方程组提供一种显式计算方法。", "method": "采用插值技术近似时间导数，有限元方法近似空间导数，形成了一种高阶显式计算技术。该算法有效处理时间导数项，并提供了稳定的时间步长限制。", "result": "理论研究表明，所提出的方法在时间上是二阶收敛的，在空间上是三阶精确的。数值例子证实了理论，验证了计算效率和实际适用性。", "conclusion": "该高阶显式计算方法对于求解三维非线性弹性动力学正弦-戈登方程组是稳定、收敛、精确、高效且实用的。", "translation": "本文提出了一种用于求解受适当初始和边界条件约束的三维非线性弹性动力学正弦-戈登方程组的显式计算方法。时间导数通过插值技术近似，而空间导数则采用有限元方法近似。所开发的数值方案被称为高阶显式计算技术。新算法有效处理时间导数项，并为稳定性和收敛性提供了合适的时间步长限制。在此时间步长限制下，使用构造的强范数深入分析了所提出方法的稳定性和误差估计。理论研究表明，所开发的方法在时间上是二阶收敛的，在空间上是三阶精确的。通过一些数值例子来证实理论，验证计算效率并展示新计算技术的实际适用性。", "summary": "本文提出了一种针对三维非线性弹性动力学正弦-戈登方程组的高阶显式计算方法。该方法结合插值技术处理时间导数和有限元方法处理空间导数，并详细分析了其在特定时间步长限制下的稳定性和误差估计。理论和数值结果均表明该方法具有良好的收敛性、精度和计算效率。", "keywords": "弹性动力学正弦-戈登问题, 显式计算, 有限元方法, 高阶精度", "comments": "这篇论文的创新点在于提出了一个高阶显式计算技术来解决复杂的三维非线性弹性动力学正弦-戈登问题。它不仅在理论上证明了方法的稳定性和高阶精度（时间二阶，空间三阶），还通过数值例子验证了其计算效率和实际应用价值，这对于解决此类非线性偏微分方程具有重要意义。"}}
{"id": "2506.14909", "title": "Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)", "authors": ["Fridolin Haugg", "Grace Lee", "John He", "Leonard Nürnberg", "Dennis Bontempi", "Danielle S. Bitterman", "Paul Catalano", "Vasco Prudente", "Dmitrii Glubokov", "Andrew Warrington", "Suraj Pai", "Dirk De Ruysscher", "Christian Guthier", "Benjamin H. Kann", "Vadim N. Gladyshev", "Hugo JWL Aerts", "Raymond H. Mak"], "summary": "Background: Facial appearance offers a noninvasive window into health. We\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\nand survival risk prediction (FAHR-FaceSurvival).\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\nand independence (saliency mapping) was tested extensively. Both models were\nclinically tested in two independent cancer patient datasets with survival\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\nfactors.\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\noutperformed a prior facial age estimation model in survival prognostication.\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\nhighest-risk quartile had more than triple the mortality of the lowest\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\nindependent cohort and both models showed generalizability across age, sex,\nrace and cancer subgroups. The two algorithms provided distinct, complementary\nprognostic information; saliency mapping revealed each model relied on distinct\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\nprognostic accuracy.\n  Interpretation: A single foundation model can generate inexpensive, scalable\nfacial biomarkers that capture both biological ageing and disease-related\nmortality risk. The foundation model enabled effective training using\nrelatively small clinical datasets.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14909v1", "AI": {"title_translation": "用于健康识别的基于人脸照片的基础人工智能模型 (FAHR-Face)", "tldr": "FAHR-Face是一个基于4000万张人脸图像训练的基础模型，可用于生物年龄估计和生存风险预测，其结合使用能提高预后准确性。", "motivation": "面部外观可以作为了解健康的非侵入性窗口。本研究旨在构建一个基础模型FAHR-Face，用于通过面部照片进行健康识别，并将其应用于生物年龄估计和生存风险预测。", "method": "研究构建了FAHR-Face，一个基于超过4000万张面部图像训练的基础模型。该模型被微调用于两个任务：FAHR-FaceAge（生物年龄估计）通过两阶段、年龄平衡的微调，使用了749,935张公共图像；FAHR-FaceSurvival（生存风险预测）则使用34,389张癌症患者照片进行微调。模型鲁棒性（针对整形手术、化妆、姿势、光照）和独立性（通过显著性映射）进行了广泛测试。两个模型均在两个独立的癌症患者数据集中进行了临床测试，并通过多变量Cox模型分析生存率并调整了临床预后因素。", "result": "在年龄估计方面，FAHR-FaceAge在公共数据集上的平均绝对误差最低，为5.1年，优于基准模型，并在整个人类生命周期中保持准确性。在癌症患者中，FAHR-FaceAge在生存预后方面优于先前的面部年龄估计模型。FAHR-FaceSurvival展现了强大的死亡率预测能力，最高风险四分位的死亡率是最低风险四分位的三倍多（调整后的风险比3.22；P<0.001）。这些发现均在独立队列中得到验证，并且两个模型在年龄、性别、种族和癌症亚组中均显示出泛化能力。两种算法提供了独特且互补的预后信息；显著性映射显示每个模型依赖于不同的面部区域。FAHR-FaceAge和FAHR-FaceSurvival的结合提高了预后准确性。", "conclusion": "一个单一的基础模型可以生成廉价、可扩展的面部生物标志物，这些生物标志物能够捕捉生物衰老和疾病相关的死亡风险。该基础模型使得使用相对较小的临床数据集进行有效训练成为可能。", "translation": "背景：面部外观为健康提供了一个非侵入性的窗口。我们构建了FAHR-Face，一个基于超过4000万张面部图像训练的基础模型，并将其微调用于两个不同的任务：生物年龄估计（FAHR-FaceAge）和生存风险预测（FAHR-FaceSurvival）。\n方法：FAHR-FaceAge在749,935张公共图像上进行了两阶段、年龄平衡的微调；FAHR-FaceSurvival则在34,389张癌症患者照片上进行微调。模型鲁棒性（整形手术、化妆、姿势、光照）和独立性（显著性映射）得到了广泛测试。两个模型均在两个独立的癌症患者数据集中进行了临床测试，并通过多变量Cox模型分析生存率并调整了临床预后因素。\n发现：在年龄估计方面，FAHR-FaceAge在公共数据集上的平均绝对误差最低，为5.1年，优于基准模型，并在整个人类生命周期中保持准确性。在癌症患者中，FAHR-FaceAge在生存预后方面优于先前的面部年龄估计模型。FAHR-FaceSurvival展现了强大的死亡率预测能力，最高风险四分位的死亡率是最低风险四分位的三倍多（调整后的风险比3.22；P<0.001）。这些发现均在独立队列中得到验证，并且两个模型在年龄、性别、种族和癌症亚组中均显示出泛化能力。这两种算法提供了独特且互补的预后信息；显著性映射显示每个模型依赖于不同的面部区域。FAHR-FaceAge和FAHR-FaceSurvival的结合提高了预后准确性。\n解释：一个单一的基础模型可以生成廉价、可扩展的面部生物标志物，这些生物标志物能够捕捉生物衰老和疾病相关的死亡风险。该基础模型使得使用相对较小的临床数据集进行有效训练成为可能。", "summary": "FAHR-Face是一个基于4000万张面部图像训练的基础AI模型，旨在通过面部照片进行健康识别。该模型被微调为FAHR-FaceAge（生物年龄估计）和FAHR-FaceSurvival（生存风险预测）两个特定任务。实验结果表明，FAHR-FaceAge在年龄估计上表现出色，并在癌症患者生存预后中优于现有模型。FAHR-FaceSurvival则能有效预测死亡率，高风险组的死亡率显著高于低风险组。两个模型均具有良好的泛化能力，并提供互补的预后信息，其结合使用能进一步提升预后准确性。该研究证明了基础模型能够利用相对较小的临床数据集生成廉价、可扩展的面部生物标志物，用于捕捉生物衰老和疾病相关死亡风险。", "keywords": "基础模型, 面部识别, 生物年龄, 生存预测, 健康生物标志物", "comments": "该研究的创新之处在于提出了FAHR-Face这一基础模型，它能够通过大规模预训练和针对特定健康任务的微调，从面部照片中提取出生物年龄和死亡风险等关键健康指标。其重要性体现在：1) 提供了一种非侵入性、可扩展且成本低廉的健康生物标志物获取方式；2) 证明了基础模型在医疗健康领域，即使面对相对较小的临床数据集，也能实现有效训练和高性能表现；3) 强调了不同模型（年龄估计与生存预测）提供互补信息，结合使用可提升预后准确性，这为临床应用提供了新的思路。"}}
{"id": "2506.15154", "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15154v1", "AI": {"title_translation": "SonicVerse：用于音乐特征信息标注的多任务学习", "tldr": "SonicVerse是一个多任务音乐标注模型，通过集成辅助音乐特征检测任务来生成更详细的音乐描述，并通过投影架构和扩展的数据集实现，实验证明能提高标注质量。", "motivation": "准确反映音乐特征的详细标注可以丰富音乐数据库并推动音乐AI研究。", "method": "本文引入了一个名为SonicVerse的多任务音乐标注模型，它将标注生成与辅助音乐特征检测任务（如音调检测、人声检测等）相结合。其核心贡献是基于投影的架构，将音频输入转换为语言token，同时通过专用的辅助头检测音乐特征，并将这些特征的输出也投影到语言token中以增强标注输入。为了训练模型，作者使用模块化音乐特征提取器MIMFLEX扩展了MusicBench数据集，提供了配对的音频、标注和音乐特征数据。", "result": "该框架不仅能为短音乐片段生成丰富、描述性的标注，还能通过使用大型语言模型链接输出，直接为更长的音乐作品生成详细的、时间相关的描述。实验结果表明，以这种方式整合特征可以提高生成标注的质量和细节。", "conclusion": "通过多任务学习和特征整合，SonicVerse模型能够生成高质量且细节丰富的音乐标注，有效提升了音乐AI领域的研究潜力。", "translation": "详细准确反映音乐作品特征的标注可以丰富音乐数据库，并推动音乐AI研究。本文介绍了一种多任务音乐标注模型SonicVerse，它将标注生成与辅助音乐特征检测任务（如音调检测、人声检测等）相结合，从而直接捕获低级声学细节以及高级音乐属性。其关键贡献是一种基于投影的架构，将音频输入转换为语言token，同时通过专用的辅助头检测音乐特征。这些辅助头的输出也被投影到语言token中，以增强标注输入。该框架不仅能为短音乐片段生成丰富、描述性的标注，还能通过使用大型语言模型链接输出，直接为更长的音乐作品生成详细的、时间相关的描述。为了训练该模型，我们使用模块化音乐特征提取器MIMFLEX对MusicBench数据集进行了特征标注扩展，从而获得了配对的音频、标注和音乐特征数据。实验结果表明，以这种方式整合特征可以提高生成标注的质量和细节。", "summary": "SonicVerse是一个创新的多任务音乐标注模型，它通过将音乐标注生成与辅助的音乐特征检测任务（如音调和人声检测）相结合，旨在生成更详细和准确的音乐描述。该模型采用独特的投影架构，将音频输入和检测到的音乐特征都转换为语言token，以增强标注过程。为了支持训练，研究人员利用MIMFLEX扩展了MusicBench数据集。实验证明，这种整合音乐特征的方法显著提高了生成标注的质量和细节，并且该框架能够为不同长度的音乐片段生成丰富的时间相关描述。", "keywords": "音乐标注, 多任务学习, 音乐特征检测, SonicVerse, 深度学习", "comments": "该论文的关键创新在于其多任务学习方法和投影式架构，将低级声学特征与高级音乐属性相结合，以生成更丰富的音乐标注。通过扩展数据集并利用LLM进行长音乐片段描述，该模型在音乐信息检索和AI创作领域具有重要意义。"}}
{"id": "2506.15093", "title": "Flexible Hardware-Enabled Guarantees for AI Compute", "authors": ["James Petrie", "Onni Aarne", "Nora Ammann", "David Dalrymple"], "summary": "As artificial intelligence systems become increasingly powerful, they pose\ngrowing risks to international security, creating urgent coordination\nchallenges that current governance approaches struggle to address without\ncompromising sensitive information or national security. We propose flexible\nhardware-enabled guarantees (flexHEGs), that could be integrated with AI\naccelerators to enable trustworthy, privacy-preserving verification and\nenforcement of claims about AI development. FlexHEGs consist of an auditable\nguarantee processor that monitors accelerator usage and a secure enclosure\nproviding physical tamper protection. The system would be fully open source\nwith flexible, updateable verification capabilities. FlexHEGs could enable\ndiverse governance mechanisms including privacy-preserving model evaluations,\ncontrolled deployment, compute limits for training, and automated safety\nprotocol enforcement. In this first part of a three part series, we provide a\ncomprehensive introduction of the flexHEG system, including an overview of the\ngovernance and security capabilities it offers, its potential development and\nadoption paths, and the remaining challenges and limitations it faces. While\ntechnically challenging, flexHEGs offer an approach to address emerging\nregulatory and international security challenges in frontier AI development.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15093v1", "AI": {"title_translation": "人工智能计算的柔性硬件保障", "tldr": "提出柔性硬件保障（flexHEGs）来解决AI系统日益增长的国际安全风险和治理挑战。", "motivation": "随着人工智能系统日益强大，它们对国际安全构成日益增长的风险，带来了紧迫的协调挑战。当前的治理方法在不损害敏感信息或国家安全的情况下难以应对这些挑战。", "method": "本文提出柔性硬件保障（flexHEGs），可与AI加速器集成，以实现对AI开发声明的可信、隐私保护的验证和执行。FlexHEGs由一个可审计的保障处理器（监控加速器使用）和一个提供物理防篡改保护的安全外壳组成。该系统将是完全开源的，具有灵活、可更新的验证能力。", "result": "FlexHEGs能够实现多种治理机制，包括隐私保护的模型评估、受控部署、训练计算限制以及自动化安全协议执行。", "conclusion": "尽管技术上具有挑战性，但flexHEGs为解决前沿AI开发中新兴的监管和国际安全挑战提供了一种方法。", "translation": "随着人工智能系统变得越来越强大，它们对国际安全构成日益增长的风险，创造了紧迫的协调挑战，而当前的治理方法在不损害敏感信息或国家安全的情况下难以解决。我们提出柔性硬件保障（flexHEGs），可以与AI加速器集成，以实现对AI开发声明的可信、隐私保护的验证和执行。FlexHEGs由一个可审计的保障处理器（监控加速器使用）和一个提供物理防篡改保护的安全外壳组成。该系统将是完全开源的，具有灵活、可更新的验证能力。FlexHEGs可以实现多种治理机制，包括隐私保护的模型评估、受控部署、训练计算限制和自动化安全协议执行。在本系列三部分的第一个部分中，我们全面介绍了flexHEG系统，包括其提供的治理和安全能力概述、其潜在的开发和采用路径以及其面临的剩余挑战和局限性。尽管技术上具有挑战性，但flexHEGs为解决前沿AI开发中新兴的监管和国际安全挑战提供了一种方法。", "summary": "本研究提出柔性硬件保障（flexHEGs）系统，旨在解决AI系统对国际安全构成的风险和治理难题。FlexHEGs通过与AI加速器集成，提供可信赖、隐私保护的验证和执行机制，由可审计的处理器和安全外壳组成，且系统开源。该系统能支持隐私保护的模型评估、受控部署、计算限制和安全协议执行等多种治理功能，为前沿AI发展中的监管和安全挑战提供创新解决方案。", "keywords": "AI治理, 硬件保障, 国际安全, 隐私保护, 计算限制", "comments": "该论文提出了一种创新的硬件级解决方案，以应对AI发展带来的国际安全和治理挑战。其创新性在于将硬件保障与软件可更新性相结合，旨在提供可信赖且隐私保护的验证机制。重要性体现在其可能为AI监管提供一个技术实现路径，尤其是在敏感信息和国家安全方面。局限性在于技术实现本身的复杂性以及作为三部分系列的第一部分，尚待后续研究完善和验证。"}}
{"id": "2506.15012", "title": "Context Matters: Learning Generalizable Rewards via Calibrated Features", "authors": ["Alexandra Forsey-Smerek", "Julie Shah", "Andreea Bobu"], "summary": "A key challenge in reward learning from human input is that desired agent\nbehavior often changes based on context. Traditional methods typically treat\neach new context as a separate task with its own reward function. For example,\nif a previously ignored stove becomes too hot to be around, the robot must\nlearn a new reward from scratch, even though the underlying preference for\nprioritizing safety over efficiency remains unchanged. We observe that context\ninfluences not the underlying preference itself, but rather the\n$\\textit{saliency}$--or importance--of reward features. For instance, stove\nheat affects the importance of the robot's proximity, yet the human's safety\npreference stays the same. Existing multi-task and meta IRL methods learn\ncontext-dependent representations $\\textit{implicitly}$--without distinguishing\nbetween preferences and feature importance--resulting in substantial data\nrequirements. Instead, we propose $\\textit{explicitly}$ modeling\ncontext-invariant preferences separately from context-dependent feature\nsaliency, creating modular reward representations that adapt to new contexts.\nTo achieve this, we introduce $\\textit{calibrated features}$--representations\nthat capture contextual effects on feature saliency--and present specialized\npaired comparison queries that isolate saliency from preference for efficient\nlearning. Experiments with simulated users show our method significantly\nimproves sample efficiency, requiring 10x fewer preference queries than\nbaselines to achieve equivalent reward accuracy, with up to 15% better\nperformance in low-data regimes (5-10 queries). An in-person user study (N=12)\ndemonstrates that participants can effectively teach their unique personal\ncontextual preferences using our method, enabling more adaptable and\npersonalized reward learning.", "comment": "30 pages, 21 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15012v1", "AI": {"title_translation": "情境至关重要：通过校准特征学习可泛化的奖励", "tldr": "本文提出了一种新的奖励学习方法，通过将情境不变的偏好与情境依赖的特征显著性分开建模，并引入“校准特征”，从而显著提高了样本效率和泛化能力。", "motivation": "传统的奖励学习方法在面对情境变化时，将每个新情境视为一个独立的任务，需要从头开始学习新的奖励函数，效率低下。然而，人类的底层偏好（如安全优先于效率）往往保持不变，只是情境影响了奖励特征的显著性。现有多任务和元IRL方法隐式处理情境依赖表示，导致数据需求量大。", "method": "本文提出显式地将情境不变的偏好与情境依赖的特征显著性分开建模，从而创建模块化的奖励表示，使其能够适应新的情境。为此，引入了“校准特征”来捕获情境对特征显著性的影响，并设计了专门的配对比较查询，以从偏好中分离出显著性，从而实现高效学习。", "result": "与基线方法相比，该方法显著提高了样本效率，实现同等奖励精度所需的偏好查询次数减少了10倍，在低数据量（5-10次查询）情况下性能提升高达15%。一项针对12名用户的真人研究表明，参与者可以有效地利用该方法教授其独特的个人情境偏好，从而实现更具适应性和个性化的奖励学习。", "conclusion": "通过显式分离情境不变偏好和情境依赖特征显著性，并引入校准特征，本方法显著提高了奖励学习的样本效率和在新情境下的泛化能力，使得机器人能够更有效地适应人类复杂的、情境化的偏好。", "translation": "奖励学习中的一个关键挑战是，所需智能体行为通常会根据情境而变化。传统方法通常将每个新情境视为一个独立的任务，并有其自己的奖励函数。例如，如果一个之前被忽略的炉子变得太热而无法靠近，机器人必须从头开始学习一个新的奖励，即使优先考虑安全而不是效率的潜在偏好保持不变。我们观察到，情境影响的不是潜在偏好本身，而是奖励特征的显著性（即重要性）。例如，炉子热度会影响机器人接近度的重要性，但人类的安全偏好保持不变。现有的多任务和元IRL方法隐式地学习情境依赖表示——不区分偏好和特征重要性——导致大量数据需求。相反，我们建议显式地将情境不变的偏好与情境依赖的特征显著性分开建模，创建模块化的奖励表示，以适应新的情境。为了实现这一点，我们引入了“校准特征”——捕获情境对特征显著性影响的表示——并提出了专门的配对比较查询，将显著性与偏好分离，以实现高效学习。与模拟用户的实验表明，我们的方法显著提高了样本效率，实现同等奖励精度所需的偏好查询次数比基线方法少10倍，在低数据量（5-10次查询）情况下性能提升高达15%。一项真人用户研究（N=12）表明，参与者可以有效地使用我们的方法教授他们独特的个人情境偏好，从而实现更具适应性和个性化的奖励学习。", "summary": "本文提出了一种新颖的奖励学习范式，旨在解决传统方法在情境变化下效率低下的问题。作者观察到情境主要影响奖励特征的显著性而非底层偏好。因此，他们提出显式地将情境不变偏好与情境依赖特征显著性分离建模，并通过引入“校准特征”和专门的查询方式来高效学习这些模块化表示。实验结果表明，该方法显著提高了样本效率，并在模拟和真人用户研究中展现出优越的性能和适应性，实现了更个性化的奖励学习。", "keywords": "奖励学习, 情境适应, 校准特征, 泛化, 样本效率", "comments": "本文的核心创新在于其对奖励学习中情境作用的深刻洞察：情境并非改变了潜在偏好，而是改变了特征的显著性。基于此，提出将情境不变的偏好和情境依赖的特征显著性显式分离建模，并引入“校准特征”，这是一种新颖且高效的表示方式。这种模块化设计不仅提高了样本效率，也使得奖励学习更具泛化性和个性化，对于机器人和AI系统在复杂真实世界环境中适应人类意图具有重要意义。"}}
{"id": "2506.15227", "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15227v1", "AI": {"title_translation": "大型语言模型在单元测试中的应用：一项系统性文献综述", "tldr": "本文对大型语言模型（LLMs）在单元测试领域的应用进行了首次系统性文献综述，分析了现有成就、挑战和未来方向，旨在帮助研究人员全面理解并促进该领域的研究。", "motivation": "由于大型语言模型（LLMs）在单元测试领域的应用仍处于探索阶段，研究人员难以理解现有成就、开放挑战和未来机遇，因此需要进行系统性回顾。", "method": "本文进行了首次系统性文献综述，分析了截至2025年3月与LLMs在单元测试中应用相关的论文。研究方法包括将现有单元测试任务进行分类（如测试生成和预言生成），讨论LLMs集成到单元测试研究中的关键方面（如模型使用、适应策略和混合方法），并总结未解决的关键挑战和有前景的未来研究方向。", "result": "本文将受益于LLMs的现有单元测试任务进行了分类，如测试生成和预言生成。讨论了将LLMs集成到单元测试研究中的几个关键方面，包括模型使用、适应策略和混合方法。总结了仍未解决的关键挑战，并概述了指导未来研究的有前景的方向。", "conclusion": "本文为单元测试社区提供了研究现状的系统性概述，帮助研究人员全面理解现有成就并促进未来的研究。", "translation": "单元测试是现代软件工程中的一项基本实践，旨在确保单个软件组件的正确性、可维护性和可靠性。最近，随着大型语言模型（LLMs）的进步，越来越多的研究利用LLMs自动化各种单元测试任务，展示了卓越的性能并显著减少了人工工作。然而，由于基于LLM的单元测试领域仍在持续探索中，研究人员难以理解现有成就、开放挑战和未来机遇。本文首次对截至2025年3月LLMs在单元测试中的应用进行了系统性文献综述。我们从单元测试和LLMs的角度分析了相关论文。我们首先对受益于LLMs的现有单元测试任务进行分类，例如测试生成和预言生成。然后，我们讨论了将LLMs集成到单元测试研究中的几个关键方面，包括模型使用、适应策略和混合方法。我们进一步总结了仍未解决的关键挑战，并概述了指导该领域未来研究的有前景的方向。总的来说，本文为单元测试社区提供了研究现状的系统性概述，帮助研究人员全面理解现有成就并促进未来的研究。我们的研究成果可在GitHub仓库公开获取：https://github.com/iSEngLab/AwesomeLLM4UT。", "summary": "本文首次对大型语言模型（LLMs）在单元测试领域的应用进行了系统性文献综述，旨在解决该领域研究人员面临的理解现有成就、挑战和机遇的困难。研究分类了LLMs受益的单元测试任务（如测试生成和预言生成），讨论了LLMs的集成策略（如模型使用、适应和混合方法），并总结了关键挑战和未来研究方向。该综述为单元测试社区提供了全面的研究概览，以促进未来的研究。", "keywords": "大型语言模型, 单元测试, 系统性文献综述, 软件工程, 测试自动化", "comments": "这是一篇重要的系统性文献综述，它填补了LLM应用于单元测试领域知识整合的空白。通过对现有研究的分类、挑战的识别以及未来方向的展望，该论文为该领域的后续研究提供了坚实的基础和清晰的路线图，对于研究人员和实践者都具有很高的参考价值。"}}
{"id": "2506.14948", "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework", "authors": ["Mohna Chakraborty", "Lu Wang", "David Jurgens"], "summary": "Large language models (LLMs) are increasingly deployed in domains requiring\nmoral understanding, yet their reasoning often remains shallow, and misaligned\nwith human reasoning. Unlike humans, whose moral reasoning integrates\ncontextual trade-offs, value systems, and ethical theories, LLMs often rely on\nsurface patterns, leading to biased decisions in morally and ethically complex\nscenarios. To address this gap, we present a value-grounded framework for\nevaluating and distilling structured moral reasoning in LLMs. We benchmark 12\nopen-source models across four moral datasets using a taxonomy of prompts\ngrounded in value systems, ethical theories, and cognitive reasoning\nstrategies. Our evaluation is guided by four questions: (1) Does reasoning\nimprove LLM decision-making over direct prompting? (2) Which types of\nvalue/ethical frameworks most effectively guide LLM reasoning? (3) Which\ncognitive reasoning strategies lead to better moral performance? (4) Can\nsmall-sized LLMs acquire moral competence through distillation? We find that\nprompting with explicit moral structure consistently improves accuracy and\ncoherence, with first-principles reasoning and Schwartz's + care-ethics\nscaffolds yielding the strongest gains. Furthermore, our supervised\ndistillation approach transfers moral competence from large to small models\nwithout additional inference cost. Together, our results offer a scalable path\ntoward interpretable and value-grounded models.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14948v1", "AI": {"title_translation": "语言模型中的结构化道德推理：一个基于价值的评估框架", "tldr": "本文提出了一个基于价值的框架，用于评估和提炼大型语言模型（LLMs）的结构化道德推理能力，并通过实验证明明确的道德结构提示和知识蒸馏可以显著提升LLMs的道德表现。", "motivation": "大型语言模型（LLMs）在需要道德理解的领域中部署越来越多，但其推理往往是肤浅的，且与人类推理不一致。LLMs常依赖表面模式，在道德和伦理复杂场景中导致有偏见的决策，而人类的道德推理则整合了上下文权衡、价值体系和伦理理论。为解决这一差距，本文提出了一个框架。", "method": "本文提出了一个基于价值的框架，用于评估和提炼LLMs中的结构化道德推理。研究人员使用基于价值体系、伦理理论和认知推理策略的提示分类法，在四个道德数据集上对12个开源模型进行了基准测试。评估围绕四个问题展开：1）推理是否能比直接提示更好地改善LLM决策？2）哪种类型的价值/伦理框架最有效地指导LLM推理？3）哪种认知推理策略能带来更好的道德表现？4）小型LLM能否通过蒸馏获得道德能力？此外，还采用了监督蒸馏方法将道德能力从大型模型转移到小型模型。", "result": "研究发现，使用明确的道德结构进行提示能持续提高准确性和连贯性，其中第一性原理推理和Schwartz's + 关怀伦理（care-ethics）支架带来了最显著的提升。此外，监督蒸馏方法能够在不增加额外推理成本的情况下，将道德能力从大型模型转移到小型模型。", "conclusion": "本文的研究结果为实现可解释且基于价值的模型提供了一条可扩展的路径。", "translation": "大型语言模型（LLMs）正越来越多地部署在需要道德理解的领域，然而它们的推理往往流于表面，且与人类推理不符。与人类不同，人类的道德推理整合了上下文权衡、价值系统和伦理理论，而LLMs通常依赖表面模式，导致在道德和伦理复杂的场景中做出有偏见的决策。为了解决这一差距，我们提出了一个基于价值的框架，用于评估和提炼LLMs中的结构化道德推理。我们使用基于价值系统、伦理理论和认知推理策略的提示分类法，在四个道德数据集上对12个开源模型进行了基准测试。我们的评估由四个问题指导：(1) 推理是否比直接提示更能改善LLM的决策？(2) 哪种类型的价值/伦理框架最有效地指导LLM推理？(3) 哪种认知推理策略能带来更好的道德表现？(4) 小型LLMs能否通过蒸馏获得道德能力？我们发现，使用明确的道德结构进行提示能持续提高准确性和连贯性，其中第一性原理推理和Schwartz's + 关怀伦理（care-ethics）支架带来了最显著的提升。此外，我们的监督蒸馏方法能够在不增加额外推理成本的情况下，将道德能力从大型模型转移到小型模型。总而言之，我们的结果为实现可解释且基于价值的模型提供了一条可扩展的路径。", "summary": "本文提出了一个名为“基于价值的评估框架”的方法，旨在解决大型语言模型（LLMs）在道德推理方面存在的肤浅和与人类不一致的问题。研究通过构建一个包含价值体系、伦理理论和认知推理策略的提示分类法，对12个开源模型在四个道德数据集上进行了基准测试。实验结果表明，明确的道德结构提示，特别是结合第一性原理推理和Schwartz's + 关怀伦理，能显著提升LLMs的道德决策准确性和连贯性。此外，通过监督蒸馏，小型模型也能有效继承大型模型的道德能力，且不增加推理成本。这项工作为构建更可解释、更符合人类价值观的LLMs提供了可行路径。", "keywords": "道德推理, 语言模型, 价值对齐, 评估框架, 知识蒸馏", "comments": "这项研究的创新之处在于提出了一个专门用于评估和提升LLMs道德推理能力的“基于价值的评估框架”，并系统地探讨了不同道德框架和认知策略对模型表现的影响。其重要性在于为LLMs在复杂道德场景中的应用提供了更可靠的解决方案，特别是通过知识蒸馏使小型模型也能获得道德能力，这对于资源受限的应用场景具有实际意义。研究结果为未来开发更符合人类伦理规范的AI系统奠定了基础。"}}
{"id": "2506.15634", "title": "SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy", "authors": ["Hasnain A. Ziad", "Alexander C. Bodoh", "Ashiq A. Sakib"], "summary": "Duplication-based redundancy schemes have proven to be effective in designing\nfully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The\ncomplete resiliency, however, is accompanied by significant energy, latency,\nand area overhead. This paper presents a novel error-tolerant Null Convention\nLogic (NCL) architecture based on selective redundancy. Results demonstrate the\nefficacy of the proposed method in terms of area and energy utilization as\ncompared to existing duplication-based NCL designs, targeting an image\nprocessing application.", "comment": "5 pages. Accepted for publication in the Proceedings of IEEE ISCAS\n  2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15634v1", "AI": {"title_translation": "SR-NCL：一种基于选择性冗余的面积/能效弹性NCL架构", "tldr": "本文提出了一种基于选择性冗余的新型NCL架构，旨在提高面积和能效，同时保持弹性，优于现有的基于复制的NCL设计。", "motivation": "现有基于复制的冗余方案在设计完全弹性准延迟不敏感异步电路时，会带来显著的能耗、延迟和面积开销。", "method": "本文提出了一种基于选择性冗余的新型容错空约定逻辑（NCL）架构。", "result": "结果表明，与现有基于复制的NCL设计相比，所提出的方法在面积和能量利用方面是有效的，并以图像处理应用为目标。", "conclusion": "所提出的基于选择性冗余的SR-NCL架构在面积和能量利用方面优于现有的基于复制的NCL设计。", "translation": "基于复制的冗余方案已被证明在设计完全弹性的准延迟不敏感（QDI）异步电路方面是有效的。然而，完全的弹性伴随着显著的能量、延迟和面积开销。本文提出了一种基于选择性冗余的新型容错空约定逻辑（NCL）架构。结果表明，与现有基于复制的NCL设计相比，所提出的方法在面积和能量利用方面是有效的，并以图像处理应用为目标。", "summary": "本文介绍了一种名为SR-NCL的新型空约定逻辑（NCL）架构，该架构采用选择性冗余来设计具有弹性的准延迟不敏感异步电路。与传统的基于复制的冗余方案相比，SR-NCL显著降低了面积和能耗开销，同时保持了错误容忍度。实验结果表明，该方法在图像处理应用中表现出更高的面积和能量效率。", "keywords": "选择性冗余, NCL, 异步电路, 面积效率, 能量效率", "comments": "该论文的创新点在于引入了“选择性冗余”的概念，以克服传统“基于复制的冗余”在提供电路弹性时所带来的巨大面积和能耗开销。这对于设计高效的异步电路具有重要意义，特别是在对功耗和面积有严格要求的应用中。"}}
{"id": "2506.15233", "title": "New Bounds and Constructions for Variable Packet-Error Coding", "authors": ["Xiangliang Kong", "Xin Wang", "Ron M. Roth", "Itzhak Tamo"], "summary": "In this paper, we consider the problem of variable packet-error coding, which\nemerges in network communication scenarios where a source transmits information\nto a destination through multiple disjoint paths. The objective is to design\ncodes with dynamic error-correcting capabilities that adapt to varying numbers\nof errors. Specifically, we first provide several bounds on the\nrate--distortion trade-off for general variable packet-error coding schemes.\nThen, we present two explicit constructions of variable packet-error coding\nschemes. The first construction uses higher-order MDS codes and provides a\ncoding scheme that achieves a better rate--distortion trade-off compared to\nknown results for general parameter regimes. The second construction is based\non a variant of the repetition code and yields a coding scheme with an optimal\nrate--distortion trade-off, with respect to our bound, for certain parameter\nregimes.", "comment": "20 pages, 3 figures, part of the work in this paper has been accepted\n  for presentation at the 2025 IEEE International Symposium on Information\n  Theory (ISIT)", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15233v1", "AI": {"title_translation": "可变分组差错编码的新界限和构造", "tldr": "本文研究了网络通信中的可变分组差错编码问题，提出了新的速率-失真界限，并构建了两种新的编码方案，分别在不同参数范围内实现了更好的或最优的速率-失真折衷。", "motivation": "在网络通信场景中，信息通过多个不相交的路径从源传输到目的地，需要设计具有动态纠错能力的编码以适应不同数量的错误。", "method": "首先，为一般的可变分组差错编码方案提供了几个速率-失真折衷的界限。然后，提出了两种显式的可变分组差错编码方案构造：第一种使用高阶MDS码，第二种基于重复码的变体。", "result": "第一种构造方案实现了比已知结果更好的速率-失真折衷；第二种构造方案在某些参数范围内实现了相对于本文界限的最优速率-失真折衷。", "conclusion": "本文为可变分组差错编码提供了新的界限和构造方法，并展示了在不同参数范围内的性能提升或最优性。", "translation": "本文考虑了可变分组差错编码问题，该问题出现在源通过多个不相交路径向目的地传输信息的网络通信场景中。目标是设计具有动态纠错能力的编码，以适应不同数量的错误。具体来说，我们首先为一般的可变分组差错编码方案提供了几个关于速率-失真折衷的界限。然后，我们提出了两种显式的可变分组差错编码方案构造。第一种构造使用了高阶MDS码，并提供了一种在通用参数范围内实现比已知结果更好速率-失真折衷的编码方案。第二种构造基于重复码的一种变体，并在某些参数范围内，相对于我们的界限，产生了一种具有最优速率-失真折衷的编码方案。", "summary": "本文研究网络通信中的可变分组差错编码，旨在设计适应动态错误数量的编码。文章首先建立了通用可变分组差错编码方案的速率-失真折衷界限，随后提出了两种显式构造方法：一种利用高阶MDS码，在通用参数下实现更优的速率-失真折衷；另一种基于重复码变体，在特定参数下达到最优的速率-失真折衷。", "keywords": "可变分组差错编码, 速率-失真, MDS码, 重复码, 网络通信", "comments": "本文的创新点在于提出了新的速率-失真界限，并基于MDS码和重复码变体构建了两种新的可变分组差错编码方案。这些方案在不同参数范围内实现了性能提升或最优性，对于实际网络通信中动态错误适应具有重要意义。"}}
{"id": "2506.15437", "title": "Exploring Fast Fourier Transforms on the Tenstorrent Wormhole", "authors": ["Nick Brown", "Jake Davies", "Felix LeClair"], "summary": "Whilst numerous areas of computing have adopted the RISC-V Instruction Set\nArchitecture (ISA) wholesale in recent years, it is yet to become widespread in\nHPC. RISC-V accelerators offer a compelling option where the HPC community can\nbenefit from the specialisation offered by the open nature of the standard but\nwithout the extensive ecosystem changes required when adopting RISC-V CPUs. In\nthis paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)\nalgorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon\nTenstorrent's Tensix architecture, this technology decouples the movement of\ndata from compute, potentially offering increased control to the programmer.\nExploring different optimisation techniques to address the bottlenecks inherent\nin data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is\nslower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around\n8 times less power and consumes around 2.8 times less energy than the CPU when\ncomputing the Fourier transform.", "comment": "Author accepted version of paper submitted to RISC-V for HPC ISC\n  workshop 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15437v1", "AI": {"title_translation": "在Tenstorrent Wormhole上探索快速傅里叶变换", "tldr": "本研究探讨了在Tenstorrent Wormhole RISC-V加速器上实现快速傅里叶变换（FFT）。结果显示，尽管性能不如高端CPU，但其功耗和能耗显著降低，突出了RISC-V加速器在HPC中能源效率的潜力。", "motivation": "近年来，RISC-V指令集架构（ISA）在计算领域的许多方面得到广泛应用，但在高性能计算（HPC）领域尚未普及。RISC-V加速器为HPC社区提供了一个引人注目的选择，可以从开放标准的专业化中受益，而无需采用RISC-V CPU所需的大量生态系统改造。本研究旨在探索将快速傅里叶变换（FFT）算法移植到Tenstorrent Wormhole PCIe RISC-V加速器上的可行性及其性能和能效。", "method": "本研究将Cooley-Tukey快速傅里叶变换（FFT）算法移植到基于Tenstorrent Tensix架构的Wormhole PCIe RISC-V加速器上。研究人员探索了不同的优化技术来解决数据传输固有的瓶颈问题。随后，他们将Wormhole n300在执行2D FFT时的性能、功耗和能耗与一台服务器级24核Xeon Platinum CPU进行了比较。", "result": "对于2D FFT，Wormhole n300的速度慢于服务器级24核Xeon Platinum CPU，但其功耗大约低8倍，能耗大约低2.8倍。", "conclusion": "尽管Tenstorrent Wormhole加速器在执行FFT时速度不如高端CPU，但其显著降低的功耗和能耗使其成为一个有吸引力的选择，尤其是在能源效率至关重要的HPC应用中。这表明RISC-V加速器在提供专业化计算能力方面具有巨大潜力，而无需大规模的生态系统变革。", "translation": "虽然近年来许多计算领域已全面采用RISC-V指令集架构（ISA），但它尚未在HPC中普及。RISC-V加速器提供了一个引人注目的选择，HPC社区可以从开放标准的专业化中受益，而无需采用RISC-V CPU所需的大量生态系统改造。在本文中，我们探索将Cooley-Tukey快速傅里叶变换（FFT）算法移植到Tenstorrent Wormhole PCIe RISC-V加速器上。该技术建立在Tenstorrent的Tensix架构之上，将数据移动与计算分离，可能为程序员提供更大的控制权。通过探索不同的优化技术来解决数据移动中固有的瓶颈，我们证明了对于2D FFT，虽然Wormhole n300比服务器级24核Xeon Platinum CPU慢，但Wormhole在计算傅里叶变换时功耗约低8倍，能耗约低2.8倍。", "summary": "本论文探讨了将Cooley-Tukey快速傅里叶变换（FFT）算法应用于Tenstorrent Wormhole PCIe RISC-V加速器。该加速器基于Tensix架构，其特点是数据移动与计算解耦。研究动机在于探索RISC-V加速器在HPC领域的潜力，因为它们无需像RISC-V CPU那样大规模改造现有生态系统。通过探索优化技术以解决数据传输瓶颈，研究发现，尽管Wormhole n300在执行2D FFT时速度慢于24核Xeon Platinum CPU，但其功耗显著降低约8倍，能耗降低约2.8倍，突显了其在能效方面的优势。", "keywords": "快速傅里叶变换, RISC-V, Tenstorrent Wormhole, HPC, 能源效率", "comments": "本研究创新性地探索了将快速傅里叶变换（FFT）这一常见HPC工作负载应用于特定的RISC-V加速器（Tenstorrent Wormhole），并着重强调了专用硬件在能源效率方面的优势。其重要性在于证明了RISC-V加速器在HPC领域提供引人注目的功耗/性能权衡的潜力，尤其是在能源消耗是关键因素的场景下。然而，该研究的局限性可能在于其仅关注单一算法和特定加速器，以及与高端CPU相比仍存在的性能差距。"}}
{"id": "2506.14831", "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review", "authors": ["Céline Finet", "Stephane Da Silva Martins", "Jean-Bernard Hayet", "Ioannis Karamouzas", "Javad Amirian", "Sylvie Le Hégarat-Mascle", "Julien Pettré", "Emanuel Aldea"], "summary": "With the emergence of powerful data-driven methods in human trajectory\nprediction (HTP), gaining a finer understanding of multi-agent interactions\nlies within hand's reach, with important implications in areas such as\nautonomous navigation and crowd modeling. This survey reviews some of the most\nrecent advancements in deep learning-based multi-agent trajectory prediction,\nfocusing on studies published between 2020 and 2024. We categorize the existing\nmethods based on their architectural design, their input representations, and\ntheir overall prediction strategies, placing a particular emphasis on models\nevaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges\nand future research directions in the field of multi-agent HTP.", "comment": "30 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14831v1", "AI": {"title_translation": "多智能体人类轨迹预测的最新进展：一项综合综述", "tldr": "该综述全面回顾了2020-2024年间基于深度学习的多智能体人类轨迹预测的最新进展，并根据架构设计、输入表示和预测策略对现有方法进行了分类，同时强调了该领域的关键挑战和未来研究方向。", "motivation": "随着强大的数据驱动方法在人类轨迹预测（HTP）中的出现，对多智能体交互的更深入理解变得触手可及，这在自主导航和人群建模等领域具有重要意义。", "method": "本综述回顾了2020年至2024年间发表的基于深度学习的多智能体轨迹预测的最新进展。研究方法包括根据架构设计、输入表示和整体预测策略对现有方法进行分类，并特别强调了使用ETH/UCY基准进行评估的模型。此外，还重点介绍了多智能体HTP领域的关键挑战和未来研究方向。", "result": "本综述系统地分类了2020-2024年间基于深度学习的多智能体轨迹预测方法，并重点关注了在ETH/UCY基准上评估的模型。同时，指出了该领域的关键挑战和未来的研究方向。", "conclusion": "本综述全面总结了多智能体人类轨迹预测的最新进展，通过对现有方法的分类和对未来挑战的展望，为该领域的研究提供了清晰的路线图。", "translation": "随着强大的数据驱动方法在人类轨迹预测（HTP）中的出现，对多智能体交互的更深入理解变得触手可及，这在自主导航和人群建模等领域具有重要意义。本综述回顾了2020年至2024年间发表的基于深度学习的多智能体轨迹预测的最新进展。我们根据现有方法的架构设计、输入表示和整体预测策略对其进行了分类，并特别强调了使用ETH/UCY基准进行评估的模型。此外，我们还重点介绍了多智能体HTP领域的关键挑战和未来的研究方向。", "summary": "本篇综述深入探讨了2020年至2024年间多智能体人类轨迹预测领域的最新进展，特别是基于深度学习的方法。文章根据模型架构、输入表示和预测策略对现有技术进行了系统分类，并着重分析了在ETH/UCY基准上进行评估的模型。同时，该综述还识别并强调了当前面临的关键挑战以及未来的研究方向。", "keywords": "多智能体, 人类轨迹预测, 深度学习, 综述, 交互", "comments": "这是一篇及时且全面的综述论文，它系统地梳理了多智能体人类轨迹预测领域在深度学习驱动下的最新进展。其价值在于对现有方法的分类清晰，并明确指出了该领域的挑战和未来方向，对于研究人员具有重要的参考价值。特别强调ETH/UCY基准也体现了对实际评估标准的关注。"}}
{"id": "2506.14817", "title": "Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning", "authors": ["Simon P. von der Maase"], "summary": "Forecasting violent conflict at high spatial and temporal resolution remains\na central challenge for both researchers and policymakers. This study presents\na novel neural network architecture for forecasting three distinct types of\nviolence -- state-based, non-state, and one-sided -- at the subnational\n(priogrid-month) level, up to 36 months in advance. The model jointly performs\nclassification and regression tasks, producing both probabilistic estimates and\nexpected magnitudes of future events. It achieves state-of-the-art performance\nacross all tasks and generates approximate predictive posterior distributions\nto quantify forecast uncertainty.\n  The architecture is built on a Monte Carlo Dropout Long Short-Term Memory\n(LSTM) U-Net, integrating convolutional layers to capture spatial dependencies\nwith recurrent structures to model temporal dynamics. Unlike many existing\napproaches, it requires no manual feature engineering and relies solely on\nhistorical conflict data. This design enables the model to autonomously learn\ncomplex spatiotemporal patterns underlying violent conflict.\n  Beyond achieving state-of-the-art predictive performance, the model is also\nhighly extensible: it can readily integrate additional data sources and jointly\nforecast auxiliary variables. These capabilities make it a promising tool for\nearly warning systems, humanitarian response planning, and evidence-based\npeacebuilding initiatives.", "comment": "33 pages, 9 figures, 3 tables. Presented at workshops hosted by PRIO,\n  AFK (German Association for Peace and Conflict Studies), CCEW (Bundeswehr\n  University Munich), Uppsala University, SODAS (University of Copenhagen) and\n  in briefings with UN agencies including UNIDIR, OCHA, and FAO", "cate": "stat.OT", "url": "http://arxiv.org/abs/2506.14817v1", "AI": {"title_translation": "下一代冲突预测：通过时空学习释放预测模式", "tldr": "本研究提出了一种新颖的神经网络架构，用于在次国家层面（priogrid-month）提前36个月预测三种暴力冲突，并在所有任务上实现最先进的性能，无需手动特征工程。", "motivation": "预测高时空分辨率的暴力冲突仍然是研究人员和政策制定者的核心挑战。", "method": "该研究提出了一种新颖的神经网络架构，基于蒙特卡洛Dropout长短期记忆（LSTM）U-Net，整合卷积层以捕捉空间依赖性，并结合循环结构以建模时间动态。该模型无需手动特征工程，仅依赖历史冲突数据，并能同时执行分类和回归任务。", "result": "该模型在所有任务上都达到了最先进的性能，能够生成预测概率估计和未来事件的预期规模，并产生近似的预测后验分布以量化预测不确定性。", "conclusion": "该模型具有高度可扩展性，可以集成额外数据源并联合预测辅助变量，使其成为预警系统、人道主义响应规划和循证和平建设倡议的有前景工具。", "translation": "预测高时空分辨率的暴力冲突仍然是研究人员和政策制定者的核心挑战。本研究提出了一种新颖的神经网络架构，用于在次国家层面（priogrid-month）提前36个月预测三种不同类型的暴力——国家主导、非国家主导和单方面暴力。该模型同时执行分类和回归任务，产生未来事件的概率估计和预期规模。它在所有任务上都达到了最先进的性能，并生成近似的预测后验分布以量化预测不确定性。\n该架构建立在蒙特卡洛Dropout长短期记忆（LSTM）U-Net之上，整合了卷积层以捕捉空间依赖性，并结合循环结构以建模时间动态。与许多现有方法不同，它不需要手动特征工程，仅依赖历史冲突数据。这种设计使模型能够自主学习暴力冲突背后的复杂时空模式。\n除了实现最先进的预测性能外，该模型还具有高度可扩展性：它可以轻松集成额外的数据源并联合预测辅助变量。这些能力使其成为预警系统、人道主义响应规划和循证和平建设倡议的有前景工具。", "summary": "本研究提出了一种基于蒙特卡洛Dropout LSTM U-Net的新型神经网络架构，用于在次国家层面高分辨率预测三种暴力冲突，提前长达36个月。该模型无需手动特征工程，仅依赖历史冲突数据，通过结合卷积层和循环结构自主学习复杂的时空模式。它在分类和回归任务中均达到最先进性能，并能量化预测不确定性，有望应用于早期预警和和平建设。", "keywords": "冲突预测, 神经网络, 时空学习, LSTM U-Net, 暴力冲突", "comments": "该论文的创新之处在于提出了一个无需手动特征工程的深度学习架构（Monte Carlo Dropout LSTM U-Net），专门用于高分辨率时空冲突预测。其能够自主学习复杂时空模式，并量化预测不确定性，这对于实际应用（如预警系统和人道主义响应）至关重要，代表了冲突预测领域的重大进步。"}}
{"id": "2506.15191", "title": "Islanding Strategy for Smart Grids Oriented to Resilience Enhancement and Its Power Supply Range Optimization", "authors": ["Yanhong Luo", "Wenchao Meng", "Xi Zhu", "Andreas Elombo", "Hu Rong", "Bing Xie", "Tianwen Zhang"], "summary": "With the increasing prevalence of distributed generators, islanded operation\nbased on distributed generation is considered a vital means to enhance the\nreliability and resilience of smart grids. This paper investigates the main\nfactors in islanding partition of smart grids and establishes a mathematical\nmodel for islanding division. A method to determine the maximum power supply\nrange of distributed energy resources (DERs) based on the reachability matrix\nand power circle algorithm is proposed to improve computational efficiency. A\ndynamic programming method based on breadth-first search (BFS) is used to solve\nthe islanding partition scheme, and a region correction method is applied to\nmodify the maximum power supply area by considering controllable loads and\nprioritizing critical load restoration, thereby enhancing system resilience.\nFinally, simulation results verify the effectiveness of the proposed algorithm\nin improving smart grid resilience.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15191v1", "AI": {"title_translation": "智能电网面向韧性提升的孤岛策略及其供电范围优化", "tldr": "本文提出一种面向韧性提升的智能电网孤岛策略，通过确定最大供电范围和动态规划方法进行孤岛划分，并考虑可控负荷和关键负荷恢复，以提高系统韧性。", "motivation": "随着分布式发电的日益普及，基于分布式发电的孤岛运行被认为是提高智能电网可靠性和韧性的重要手段。", "method": "本文研究了智能电网孤岛划分的主要因素并建立了数学模型。提出了一种基于可达矩阵和功率圆算法确定分布式能源最大供电范围的方法以提高计算效率。采用基于广度优先搜索（BFS）的动态规划方法求解孤岛划分方案，并应用区域校正方法，考虑可控负荷并优先恢复关键负荷，修正最大供电区域。", "result": "仿真结果验证了所提算法在提高智能电网韧性方面的有效性。", "conclusion": "仿真结果验证了所提算法在提高智能电网韧性方面的有效性。", "translation": "随着分布式发电机的日益普及，基于分布式发电的孤岛运行被认为是提高智能电网可靠性和韧性的重要手段。本文研究了智能电网孤岛划分的主要因素，并建立了孤岛划分的数学模型。为提高计算效率，提出了一种基于可达矩阵和功率圆算法确定分布式能源最大供电范围的方法。采用基于广度优先搜索（BFS）的动态规划方法求解孤岛划分方案，并应用区域校正方法，通过考虑可控负荷和优先恢复关键负荷来修正最大供电区域，从而增强系统韧性。最后，仿真结果验证了所提算法在提高智能电网韧性方面的有效性。", "summary": "本文针对智能电网韧性提升的需求，研究了孤岛划分的关键因素并建立了数学模型。提出了一种结合可达矩阵和功率圆算法的分布式能源最大供电范围确定方法，以提高计算效率。采用基于广度优先搜索的动态规划方法求解孤岛划分方案，并通过区域校正方法，考虑可控负荷和关键负荷恢复，进一步优化了供电区域，以增强系统韧性。仿真结果验证了所提算法的有效性。", "keywords": "智能电网, 孤岛运行, 韧性增强, 供电范围优化, 分布式发电", "comments": "本文提出了一种综合性的智能电网孤岛策略，其创新点在于结合了可达矩阵、功率圆算法和动态规划来优化孤岛划分，并通过区域校正方法考虑了可控负荷和关键负荷的优先恢复，这对于提高智能电网在故障情况下的韧性具有重要意义。"}}
{"id": "2506.15377", "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation", "authors": ["Ruoyu Wang", "Xinshu Li", "Chen Wang", "Lina Yao"], "summary": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate\ncomplex environments toward given objectives. Across diverse settings within\nNavigation tasks, many necessitate the modelling of sequential data accumulated\nfrom preceding time steps. While existing methods perform well, they typically\nprocess all historical observations simultaneously, overlooking the internal\nassociation structure within the data, which may limit the potential for\nfurther improvements in task performance. We address this by examining the\nunique characteristics of Navigation tasks through the lens of causality,\nintroducing a causal framework to highlight the limitations of conventional\nsequential methods. Leveraging this insight, we propose Causality-Aware\nNavigation (CAN), which incorporates a Causal Understanding Module to enhance\nthe agent's environmental understanding capability. Empirical evaluations show\nthat our approach consistently outperforms baselines across various tasks and\nsimulation environments. Extensive ablations studies attribute these gains to\nthe Causal Understanding Module, which generalizes effectively in both\nReinforcement and Supervised Learning settings without computational overhead.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15377v1", "AI": {"title_translation": "视觉导航中高效且可泛化的环境理解", "tldr": "本文提出了一种因果感知导航（CAN）方法，通过引入因果理解模块来解决现有视觉导航方法中对历史观测数据内部关联结构忽略的问题，并在多种任务和模拟环境中取得了显著优于基线的性能。", "motivation": "现有视觉导航方法在处理历史观测数据时通常同时处理所有数据，忽略了数据内部的关联结构，这限制了任务性能的进一步提升。本文旨在通过因果关系视角解决这一问题，以增强智能体的环境理解能力。", "method": "本文通过因果关系视角审视导航任务的独特性，引入了一个因果框架来揭示传统序列方法的局限性。在此基础上，提出了因果感知导航（Causality-Aware Navigation, CAN）方法，该方法包含一个因果理解模块（Causal Understanding Module）以增强智能体的环境理解能力。", "result": "经验评估表明，所提出的方法在各种任务和模拟环境中始终优于基线。广泛的消融研究表明，性能提升归因于因果理解模块，该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "conclusion": "本文提出的因果理解模块能够有效提升智能体的环境理解能力，并在视觉导航任务中展现出卓越的泛化性和性能提升，且不增加计算负担。", "translation": "视觉导航是具身AI中的一项核心任务，使智能体能够在复杂环境中导航以实现给定目标。在导航任务的各种设置中，许多任务需要对前一时间步积累的序列数据进行建模。虽然现有方法表现良好，但它们通常同时处理所有历史观测数据，忽略了数据内部的关联结构，这可能限制了任务性能进一步提升的潜力。我们通过因果关系视角审视导航任务的独特特征来解决这个问题，引入了一个因果框架以突出传统序列方法的局限性。利用这一洞察，我们提出了因果感知导航（Causality-Aware Navigation, CAN），该方法包含一个因果理解模块（Causal Understanding Module）以增强智能体的环境理解能力。经验评估表明，我们的方法在各种任务和模拟环境中始终优于基线。广泛的消融研究将这些增益归因于因果理解模块，该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "summary": "本文针对视觉导航任务中现有方法未能充分利用历史观测数据内部关联结构的问题，引入了因果框架。在此基础上，提出了因果感知导航（CAN）方法，其核心是一个因果理解模块，旨在提升智能体的环境理解能力。实验结果表明，CAN在不同任务和模拟环境中均显著优于基线方法，且因果理解模块在强化学习和监督学习设置中均表现出良好的泛化能力，同时不增加计算负担。", "keywords": "视觉导航, 因果关系, 环境理解, 具身AI, 序列数据", "comments": "本文的创新点在于将因果关系引入视觉导航任务，以解决传统方法在处理序列数据时忽略内部关联结构的问题。提出的因果理解模块不仅提升了导航性能，而且在不增加计算开销的情况下实现了良好的泛化能力，这对于具身AI领域具有重要意义。"}}
{"id": "2506.15235", "title": "Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data", "authors": ["Taewon Kang", "Seunghyeon Park", "Pyo-Woong Son", "Jiwon Seo"], "summary": "The vulnerabilities of global navigation satellite systems (GNSS) to signal\ninterference have increased the demand for complementary positioning,\nnavigation, and timing (PNT) systems. To address this, South Korea has decided\nto deploy an enhanced long-range navigation (eLoran) system as a complementary\nPNT solution. Similar to GNSS, eLoran provides highly accurate timing\ninformation, which is essential for applications such as telecommunications,\nfinancial systems, and power distribution. However, the primary sources of\nerror for GNSS and eLoran differ. For eLoran, the main source of error is\nsignal propagation delay over land, known as the additional secondary factor\n(ASF). This delay, influenced by ground conductivity and weather conditions\nalong the signal path, is challenging to predict and mitigate. In this paper,\nwe measure the time difference (TD) between GPS and eLoran using a time\ninterval counter and analyze the correlations between eLoran/GPS TD and eleven\nmeteorological factors. Accurate estimation of eLoran/GPS TD could enable\neLoran to achieve timing accuracy comparable to that of GPS. We propose two\nestimation models for eLoran/GPS TD and compare their performance with existing\nTD estimation methods. The proposed WLR-AGRNN model captures the linear\nrelationships between meteorological factors and eLoran/GPS TD using weighted\nlinear regression (WLR) and models nonlinear relationships between outputs from\nexpert networks through an anisotropic general regression neural network\n(AGRNN). The model incorporates terrain elevation to appropriately weight\nmeteorological data, as elevation influences signal propagation delay.\nExperimental results based on four months of data demonstrate that the\nWLR-AGRNN model outperforms other models, highlighting its effectiveness in\nimproving eLoran/GPS TD estimation accuracy.", "comment": "Submitted to IEEE Access", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15235v1", "AI": {"title_translation": "采用机器学习结合气象和地形数据提升eLoran授时精度", "tldr": "本文提出一种名为WLR-AGRNN的机器学习模型，结合气象和地形数据，有效提高了eLoran系统的授时精度，使其接近GPS水平，以应对GNSS信号干扰问题。", "motivation": "全球导航卫星系统（GNSS）易受信号干扰，因此需要互补的定位、导航和授时（PNT）系统。eLoran作为一种备选方案，其主要误差源是信号在陆地传播的附加次级因子（ASF），该因子受地面电导率和天气条件影响，难以预测和缓解。准确估计eLoran与GPS之间的时间差对于提升eLoran的授时精度至关重要。", "method": "研究人员使用时间间隔计数器测量了GPS和eLoran之间的时间差（TD），并分析了eLoran/GPS TD与十一个气象因素之间的相关性。本文提出了两种eLoran/GPS TD估计模型，并将其性能与现有方法进行了比较。其中，核心模型WLR-AGRNN利用加权线性回归（WLR）处理气象因素与eLoran/GPS TD之间的线性关系，并通过各向异性广义回归神经网络（AGRNN）建模专家网络输出之间的非线性关系。此外，该模型还结合了地形高程信息以适当加权气象数据，因为高程会影响信号传播延迟。", "result": "基于四个月的数据进行的实验结果表明，所提出的WLR-AGRNN模型性能优于其他现有模型，显著提高了eLoran/GPS TD的估计精度。", "conclusion": "WLR-AGRNN模型能够显著提高eLoran/GPS时间差的估计精度，从而有望使eLoran的授时精度达到与GPS相当的水平，增强PNT系统的鲁棒性。", "translation": "全球导航卫星系统 (GNSS) 对信号干扰的脆弱性增加了对互补定位、导航和授时 (PNT) 系统的需求。为了解决这个问题，韩国决定部署增强型远程导航 (eLoran) 系统作为互补 PNT 解决方案。与 GNSS 类似，eLoran 提供高精度的授时信息，这对于电信、金融系统和电力分配等应用至关重要。然而，GNSS 和 eLoran 的主要误差来源不同。对于 eLoran，主要的误差来源是信号在陆地上的传播延迟，称为附加次级因子 (ASF)。这种延迟受地面电导率和沿信号路径的天气条件影响，难以预测和缓解。在本文中，我们使用时间间隔计数器测量了 GPS 和 eLoran 之间的时间差 (TD)，并分析了 eLoran/GPS TD 与十一个气象因素之间的相关性。准确估计 eLoran/GPS TD 可以使 eLoran 实现与 GPS 相当的授时精度。我们提出了两种 eLoran/GPS TD 估计模型，并比较了它们与现有 TD 估计方法的性能。所提出的 WLR-AGRNN 模型使用加权线性回归 (WLR) 捕获气象因素与 eLoran/GPS TD 之间的线性关系，并通过各向异性广义回归神经网络 (AGRNN) 建模专家网络输出之间的非线性关系。该模型结合了地形高程以适当加权气象数据，因为高程会影响信号传播延迟。基于四个月数据的实验结果表明，WLR-AGRNN 模型优于其他模型，突出了其在提高 eLoran/GPS TD 估计精度方面的有效性。", "summary": "本文旨在通过机器学习方法提升eLoran系统的授时精度，以应对GNSS易受干扰的问题。研究测量了eLoran与GPS之间的时间差（TD），并分析了其与气象因素的关联。提出了一种名为WLR-AGRNN的新模型，该模型结合加权线性回归和各向异性广义回归神经网络，并创新性地融入地形高程数据来优化气象数据权重，以更准确地估计eLoran/GPS TD。实验结果表明，该模型在提高时间差估计精度方面表现优异，有望使eLoran达到接近GPS的授时水平。", "keywords": "eLoran, 授时精度, 机器学习, 气象数据, 地形数据, 时间差估计", "comments": "本文的创新之处在于提出了一种结合加权线性回归和神经网络的混合模型（WLR-AGRNN），并创造性地引入地形高程数据来优化气象数据在信号传播延迟估计中的权重。这种方法有效地解决了eLoran系统中由于气象和地形复杂性导致的ASF难以预测的问题，显著提升了eLoran的授时精度，使其成为GNSS可靠的互补PNT解决方案，具有重要的实际应用价值。"}}
{"id": "2506.14787", "title": "Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems", "authors": ["Funing Li", "Yuan Tian", "Ruben Noortwyck", "Jifeng Zhou", "Liming Kuang", "Robert Schulz"], "summary": "In modern industrial and logistics environments, the rapid expansion of fast\ndelivery services has heightened the demand for storage systems that combine\nhigh efficiency with increased density. Multi-deep autonomous vehicle storage\nand retrieval systems (AVS/RS) present a viable solution for achieving greater\nstorage density. However, these systems encounter significant challenges during\nretrieval operations due to lane blockages. A conventional approach to mitigate\nthis issue involves storing items with homogeneous characteristics in a single\nlane, but this strategy restricts the flexibility and adaptability of\nmulti-deep storage systems.\n  In this study, we propose a deep reinforcement learning-based framework to\naddress the retrieval problem in multi-deep storage systems with heterogeneous\nitem configurations. Each item is associated with a specific due date, and the\nobjective is to minimize total tardiness. To effectively capture the system's\ntopology, we introduce a graph-based state representation that integrates both\nitem attributes and the local topological structure of the multi-deep\nwarehouse. To process this representation, we design a novel neural network\narchitecture that combines a Graph Neural Network (GNN) with a Transformer\nmodel. The GNN encodes topological and item-specific information into\nembeddings for all directly accessible items, while the Transformer maps these\nembeddings into global priority assignments. The Transformer's strong\ngeneralization capability further allows our approach to be applied to storage\nsystems with diverse layouts. Extensive numerical experiments, including\ncomparisons with heuristic methods, demonstrate the superiority of the proposed\nneural network architecture and the effectiveness of the trained agent in\noptimizing retrieval tardiness.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14787v1", "AI": {"title_translation": "拓扑感知且高度泛化的深度强化学习在多深层存储系统高效取货中的应用", "tldr": "本文提出一种基于深度强化学习的方法，结合图神经网络和Transformer，解决多深层存储系统中异构物品的取货问题，以最小化总延误，并在不同布局下表现出优越的泛化能力。", "motivation": "现代工业和物流环境对高效率、高密度的存储系统需求增加。多深层自动车辆存储和检索系统（AVS/RS）虽能提高存储密度，但在取货操作中面临巷道堵塞的挑战。传统方法（同质物品存储）限制了系统的灵活性和适应性。", "method": "提出一个基于深度强化学习的框架，用于解决具有异构物品配置的多深层存储系统中的取货问题，目标是最小化总延误。该方法引入了图基状态表示，整合物品属性和局部拓扑结构。设计了一种结合图神经网络（GNN）和Transformer模型的新型神经网络架构。GNN编码拓扑和物品特定信息生成嵌入，Transformer将这些嵌入映射为全局优先级分配，并提供强大的泛化能力以适应不同的系统布局。", "result": "通过广泛的数值实验，包括与启发式方法的比较，证明了所提出的神经网络架构的优越性以及训练代理在优化取货延误方面的有效性。", "conclusion": "该研究提出的基于深度强化学习的方法，结合了图神经网络和Transformer，能够有效解决多深层存储系统中的取货延误问题，并且具有良好的泛化能力，适用于不同的系统布局。", "translation": "在现代工业和物流环境中，快速配送服务的迅速扩张提高了对兼具高效率和高密度的存储系统的需求。多深层自动车辆存储和检索系统（AVS/RS）为实现更高的存储密度提供了一个可行的解决方案。然而，由于巷道堵塞，这些系统在取货操作中遇到了重大挑战。缓解此问题的传统方法是将具有同质特性的物品存储在单个巷道中，但这限制了多深层存储系统的灵活性和适应性。\n在本研究中，我们提出了一种基于深度强化学习的框架，以解决具有异构物品配置的多深层存储系统中的取货问题。每个物品都关联一个特定的截止日期，目标是最小化总延误。为了有效捕获系统拓扑，我们引入了一种基于图的状态表示，该表示集成了物品属性和多深层仓库的局部拓扑结构。为了处理这种表示，我们设计了一种结合图神经网络（GNN）和Transformer模型的新型神经网络架构。GNN将拓扑和物品特定信息编码为所有直接可达物品的嵌入，而Transformer将这些嵌入映射为全局优先级分配。Transformer强大的泛化能力进一步使我们的方法能够应用于具有不同布局的存储系统。广泛的数值实验，包括与启发式方法的比较，证明了所提出的神经网络架构的优越性以及训练代理在优化取货延误方面的有效性。", "summary": "本文针对多深层存储系统中因巷道堵塞导致的异构物品取货效率问题，提出了一种基于深度强化学习的解决方案。该方案通过引入图基状态表示来捕获系统拓扑，并设计了一种结合图神经网络（GNN）和Transformer的新型神经网络架构。GNN用于编码拓扑和物品信息，Transformer用于生成全局优先级分配，并增强了模型对不同系统布局的泛化能力。实验结果表明，该方法在最小化总延误方面优于传统启发式方法。", "keywords": "深度强化学习, 多深层存储系统, 图神经网络, Transformer, 取货优化", "comments": "这篇论文的创新点在于将深度强化学习应用于多深层存储系统的取货问题，并特别引入了图基状态表示和结合GNN与Transformer的混合神经网络架构。这种架构能够有效捕捉复杂的拓扑信息和物品属性，并通过Transformer的泛化能力使其适用于不同布局的仓库，这对于实际工业应用具有重要意义。解决巷道堵塞和异构物品配置的挑战，同时最小化延误，是该领域的一个重要贡献。"}}
{"id": "2506.15030", "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "summary": "Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.", "comment": "22 pages, 2 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15030v1", "AI": {"title_translation": "使用主题建模和文本分类方法识别NVDRS文本叙述中的社会隔离主题", "tldr": "该研究利用自然语言处理技术，通过主题建模和监督学习分类器，从美国国家暴力死亡报告系统（NVDRS）的文本叙述中识别社会隔离主题，并分析了与慢性社会隔离相关的风险因素。", "motivation": "社会隔离和孤独感与自杀率密切相关，但美国国家暴力死亡报告系统（NVDRS）的结构化变量中并未记录这些信息。本研究旨在利用自然语言处理技术从法律执行和验尸官医疗检查叙述中识别这些概念，以改进对社会隔离和孤独的监测和预防。", "method": "研究使用了主题建模来开发词汇表，并结合监督学习分类器来识别NVDRS文本叙述中的社会隔离主题。分类器经过开发并评估，平均F1分数为0.86，准确率为0.82。研究分析了2002年至2020年间超过30万例自杀事件的数据。", "result": "在超过30万例自杀事件中，识别出1,198例提及慢性社会隔离。如果死者是男性（OR = 1.44）、同性恋（OR = 3.68）或已离婚（OR = 3.34），则其被归类为慢性社会隔离的几率更高。研究还发现，近期或即将离婚、失去子女监护权、被驱逐或近期搬家以及分手是其他社会隔离主题的重要预测因素。", "conclusion": "本研究开发的方法能够从NVDRS文本叙述中有效识别社会隔离主题，并揭示了与慢性社会隔离相关的特定人口统计学和生活事件风险因素。这些方法可以改进美国对社会隔离和孤独的监测和预防工作。", "translation": "近年来，社会隔离和孤独感日益加剧，并强烈促成自杀率的上升。尽管社会隔离和孤独感目前尚未记录在美国国家暴力死亡报告系统（NVDRS）的结构化变量中，但自然语言处理（NLP）技术可用于在执法和验尸官医疗检查叙述中识别这些概念。通过使用主题建模生成词汇表开发和监督学习分类器，我们开发了高质量的分类器（平均F1：.86，准确率：.82）。我们评估了2002年至2020年间超过30万例自杀事件，识别出1,198例提及慢性社会隔离。如果死者是男性（OR = 1.44；CI：1.24，1.69，p<.0001）、同性恋（OR = 3.68；1.97，6.33，p<.0001）或已离婚（OR = 3.34；2.68，4.19，p<.0001），则其被归类为慢性社会隔离的几率更高。我们还发现近期或即将离婚、失去子女监护权、被驱逐或近期搬家以及分手是其他社会隔离主题的重要预测因素。我们的方法可以改进美国对社会隔离和孤独的监测和预防。", "summary": "该研究利用自然语言处理技术，包括主题建模和监督学习分类器，从美国国家暴力死亡报告系统（NVDRS）的文本叙述中识别社会隔离主题。研究分析了2002年至2020年超过30万例自杀事件，成功识别出慢性社会隔离案例，并发现男性、同性恋者和离婚者与慢性社会隔离的关联性更高。此外，近期生活事件如离婚、子女监护权丧失、搬迁和分手也是重要的社会隔离预测因素。本研究提出的方法有望增强美国对社会隔离和孤独的监测和预防能力。", "keywords": "社会隔离, 自然语言处理, 自杀预防, 主题建模, 文本分类", "comments": "这项研究的创新之处在于将自然语言处理技术应用于非结构化文本数据，以识别与公共卫生高度相关的社会隔离和孤独主题。这为传统结构化数据无法捕捉的复杂社会心理因素提供了一种新的数据挖掘途径。其重要性在于，通过识别高风险人群和相关的生命事件，能够为自杀预防策略提供更精准的指导。然而，研究结果的普遍性可能受限于NVDRS数据的特性，且因果关系仍需进一步探讨。"}}
{"id": "2506.14812", "title": "Weak TransNet: A Petrov-Galerkin based neural network method for solving elliptic PDEs", "authors": ["Zhihang Xu", "Min Wang", "Zhu Wang"], "summary": "While deep learning has achieved remarkable success in solving partial\ndifferential equations (PDEs), it still faces significant challenges,\nparticularly when the PDE solutions have low regularity or singularities. To\naddress these issues, we propose the Weak TransNet (WTN) method, based on a\nPetrov-Galerkin formulation, for solving elliptic PDEs in this work, though its\nframework may extend to other classes of equations. Specifically, the neural\nfeature space defined by TransNet (Zhang et al., 2023) is used as the trial\nspace, while the test space is composed of radial basis functions. Since the\nsolution is expressed as a linear combination of trial functions, the\ncoefficients can be determined by minimizing the weak PDE residual via least\nsquares. Thus, this approach could help mitigate the challenges of\nnon-convexity and ill-conditioning that often arise in neural network training.\nFurthermore, the WTN method is extended to handle problems whose solutions\nexhibit multiscale features or possess sharp variations. Several numerical\nexperiments are presented to demonstrate the robustness and efficiency of the\nproposed methods.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14812v1", "AI": {"title_translation": "弱TransNet：一种基于Petrov-Galerkin的神经网络方法求解椭圆偏微分方程", "tldr": "提出了一种基于Petrov-Galerkin的弱TransNet方法，用于解决具有低正则性或奇异性的椭圆偏微分方程，并通过最小化弱PDE残差来提高训练稳定性。", "motivation": "深度学习在解决偏微分方程时，当解的正则性较低或存在奇异性时，仍面临显著挑战。", "method": "提出Weak TransNet (WTN) 方法，基于Petrov-Galerkin公式求解椭圆偏微分方程。该方法使用TransNet定义的神经网络特征空间作为试探空间，径向基函数组成测试空间。通过最小化弱PDE残差的最小二乘法来确定系数。WTN方法还扩展到处理具有多尺度特征或剧烈变化的解的问题。", "result": "多个数值实验表明所提出方法的鲁棒性和效率。", "conclusion": "Weak TransNet方法能够有效且鲁棒地解决具有低正则性、奇异性、多尺度特征或剧烈变化的椭圆偏微分方程，并缓解了神经网络训练中的非凸性和病态问题。", "translation": "尽管深度学习在解决偏微分方程（PDEs）方面取得了显著成功，但它仍然面临重大挑战，特别是当PDE解具有低正则性或奇异性时。为了解决这些问题，我们在这项工作中提出了一种基于Petrov-Galerkin公式的弱TransNet（WTN）方法，用于求解椭圆偏微分方程，尽管其框架可以扩展到其他类型的方程。具体来说，TransNet（Zhang et al., 2023）定义的神经网络特征空间被用作试探空间，而测试空间由径向基函数组成。由于解表示为试探函数的线性组合，因此可以通过最小化弱PDE残差的最小二乘法来确定系数。因此，这种方法有助于缓解神经网络训练中经常出现的非凸性和病态问题。此外，WTN方法被扩展到处理解表现出多尺度特征或具有剧烈变化的问题。提出了几个数值实验来证明所提出方法的鲁棒性和效率。", "summary": "本文提出了一种名为Weak TransNet (WTN) 的神经网络方法，该方法基于Petrov-Galerkin公式，旨在解决传统深度学习在处理具有低正则性或奇异性偏微分方程时遇到的挑战。WTN利用TransNet的特征空间作为试探空间，径向基函数作为测试空间，并通过最小二乘法最小化弱PDE残差来确定解的系数，从而有效缓解了神经网络训练中的非凸性和病态问题。此外，该方法还可处理多尺度或剧烈变化的解。数值实验验证了其鲁棒性和效率。", "keywords": "偏微分方程, 神经网络, Petrov-Galerkin, 弱TransNet, 低正则性", "comments": "该论文的创新点在于将Petrov-Galerkin方法与神经网络相结合，特别是利用TransNet作为试探空间，并结合径向基函数作为测试空间，通过最小二乘法求解弱形式，有效缓解了传统神经网络在处理低正则性/奇异性PDEs时面临的非凸性和病态问题。这对于提升深度学习在复杂PDEs求解中的稳定性和准确性具有重要意义。"}}
{"id": "2506.14914", "title": "Recursive Variational Autoencoders for 3D Blood Vessel Generative Modeling", "authors": ["Paula Feldman", "Miguel Fainstein", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "summary": "Anatomical trees play an important role in clinical diagnosis and treatment\nplanning. Yet, accurately representing these structures poses significant\nchallenges owing to their intricate and varied topology and geometry. Most\nexisting methods to synthesize vasculature are rule based, and despite\nproviding some degree of control and variation in the structures produced, they\nfail to capture the diversity and complexity of actual anatomical data. We\ndeveloped a Recursive variational Neural Network (RvNN) that fully exploits the\nhierarchical organization of the vessel and learns a low-dimensional manifold\nencoding branch connectivity along with geometry features describing the target\nsurface. After training, the RvNN latent space can be sampled to generate new\nvessel geometries. By leveraging the power of generative neural networks, we\ngenerate 3D models of blood vessels that are both accurate and diverse, which\nis crucial for medical and surgical training, hemodynamic simulations, and many\nother purposes. These results closely resemble real data, achieving high\nsimilarity in vessel radii, length, and tortuosity across various datasets,\nincluding those with aneurysms. To the best of our knowledge, this work is the\nfirst to utilize this technique for synthesizing blood vessels.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14914v1", "AI": {"title_translation": "用于3D血管生成建模的递归变分自编码器", "tldr": "本文开发了一种递归变分神经网络（RvNN），用于生成准确且多样化的3D血管模型，克服了传统基于规则方法的局限性。", "motivation": "解剖树（如血管）在临床诊断和治疗计划中至关重要，但由于其复杂的拓扑结构和几何形状，准确表示它们具有挑战性。大多数现有血管合成的基于规则的方法未能捕捉实际解剖数据的多样性和复杂性。", "method": "开发了一种递归变分神经网络（RvNN），该网络充分利用血管的层次结构，并学习一个低维流形，编码分支连接以及描述目标表面的几何特征。训练后，可以对RvNN的潜在空间进行采样以生成新的血管几何形状。", "result": "生成了既准确又多样化的3D血管模型。这些结果与真实数据非常相似，在各种数据集（包括动脉瘤数据集）中，血管半径、长度和弯曲度都达到了高度相似性。", "conclusion": "据作者所知，这项工作是首次将递归变分神经网络（RvNN）技术用于合成血管。", "translation": "解剖树在临床诊断和治疗计划中发挥着重要作用。然而，由于其复杂多变的拓扑结构和几何形状，准确表示这些结构带来了巨大的挑战。大多数现有的血管合成方法是基于规则的，尽管在生成结构方面提供了一定程度的控制和变异，但它们未能捕捉到实际解剖数据的多样性和复杂性。我们开发了一种递归变分神经网络（RvNN），它充分利用了血管的层次结构，并学习了一个低维流形，该流形编码了分支连接以及描述目标表面的几何特征。训练后，可以对RvNN的潜在空间进行采样以生成新的血管几何形状。通过利用生成神经网络的力量，我们生成了既准确又多样化的3D血管模型，这对于医学和外科培训、血流动力学模拟以及许多其他目的至关重要。这些结果与真实数据非常相似，在各种数据集（包括动脉瘤数据集）中，血管半径、长度和弯曲度都达到了高度相似性。据我们所知，这项工作是首次将这种技术用于合成血管。", "summary": "本文介绍了一种用于3D血管生成建模的递归变分神经网络（RvNN）。针对传统基于规则方法缺乏多样性的局限性，RvNN学习血管的层次结构，并将分支连接和几何特征编码到低维潜在空间中。训练后的模型能够生成准确且多样化的3D血管几何形状，包括动脉瘤，与真实解剖数据高度相似。这项创新对于医学培训和模拟具有重要意义。", "keywords": "递归变分自编码器, 3D血管, 生成建模, RvNN, 解剖树", "comments": "本文提出了递归变分神经网络（RvNN）在3D血管生成建模中的新颖应用，是传统基于规则方法的重大进步。其创新之处在于有效捕捉血管的层次结构和复杂拓扑，从而生成高度准确和多样化的解剖模型。这项工作对于改进医学和外科培训以及实现更真实的血流动力学模拟至关重要，这些在临床应用中都非常关键。"}}
{"id": "2506.15514", "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field\nof music information retrieval, despite great advances in automatic speech\nrecognition (ASR) brought about by transformer-based architectures in recent\nyears. One of the major challenges in ALT is the high amplitude of interfering\naudio signals relative to conventional ASR due to musical accompaniment. Recent\nadvances in music source separation have enabled automatic extraction of\nhigh-quality separated vocals, which could potentially improve ALT performance.\nHowever, the effect of source separation has not been systematically\ninvestigated in order to establish best practices for its use. This work\nexamines the impact of source separation on ALT using Whisper, a\nstate-of-the-art open source ASR model. We evaluate Whisper's performance on\noriginal audio, separated vocals, and vocal stems across short-form and\nlong-form transcription tasks. For short-form, we suggest a concatenation\nmethod that results in a consistent reduction in Word Error Rate (WER). For\nlong-form, we propose an algorithm using source separation as a vocal activity\ndetector to derive segment boundaries, which results in a consistent reduction\nin WER relative to Whisper's native long-form algorithm. Our approach achieves\nstate-of-the-art results for an open source system on the Jam-ALT long-form ALT\nbenchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the\nfirst dataset of long-form lyric transcripts following the Jam-ALT guidelines\nfor which vocal stems are publicly available.", "comment": "Accepted at 2025 ICME Workshop AI for Music", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15514v1", "AI": {"title_translation": "利用音乐源分离改进Whisper的自动歌词转录", "tldr": "本文系统性研究了音乐源分离对使用Whisper进行自动歌词转录的影响，并提出了针对短文本和长文本的改进方法，取得了先进结果并发布了新数据集。", "motivation": "尽管基于Transformer架构的自动语音识别（ASR）取得了巨大进展，但自动歌词转录（ALT）仍然是音乐信息检索领域的一个挑战性任务。ALT的主要挑战之一是音乐伴奏导致干扰音频信号的振幅相对于传统ASR较高。虽然音乐源分离技术取得了进展，可以自动提取高质量的分离人声，但其对ALT性能的影响尚未得到系统性研究，以建立最佳实践。", "method": "本文使用最先进的开源ASR模型Whisper，研究了源分离对ALT的影响。我们评估了Whisper在原始音频、分离人声和人声干声上在短文本和长文本转录任务中的性能。对于短文本，我们提出了一种连接方法。对于长文本，我们提出了一种利用源分离作为人声活动检测器来确定分段边界的算法。此外，我们还发布了MUSDB-ALT，这是第一个遵循Jam-ALT指南且人声干声公开的长文本歌词转录数据集。", "result": "对于短文本转录，提出的连接方法导致词错误率（WER）持续降低。对于长文本转录，提出的算法相对于Whisper的原生长文本算法，导致WER持续降低。我们的方法在Jam-ALT长文本ALT基准测试中，无需任何训练或微调，就实现了开源系统的最先进结果。我们还发布了MUSDB-ALT数据集。", "conclusion": "音乐源分离可以显著提升自动歌词转录的性能，尤其是在结合Whisper模型时。本文提出的针对短文本和长文本的策略能够有效降低词错误率，并在长文本基准测试中达到最先进水平。新数据集的发布也为未来的研究提供了宝贵的资源。", "translation": "尽管近年来基于Transformer架构的自动语音识别（ASR）取得了巨大进展，但自动歌词转录（ALT）在音乐信息检索领域仍然是一项具有挑战性的任务。ALT的主要挑战之一是，相对于传统的ASR，音乐伴奏导致干扰音频信号的振幅较高。音乐源分离的最新进展使得高质量分离人声的自动提取成为可能，这可能潜在地提高ALT的性能。然而，源分离的效果尚未被系统性地研究，以建立其使用的最佳实践。本工作使用最先进的开源ASR模型Whisper，研究了源分离对ALT的影响。我们评估了Whisper在原始音频、分离人声和人声干声在短文本和长文本转录任务中的性能。对于短文本，我们提出了一种连接方法，该方法能持续降低词错误率（WER）。对于长文本，我们提出了一种使用源分离作为人声活动检测器来确定分段边界的算法，该算法相对于Whisper的原生长文本算法能持续降低WER。我们的方法在Jam-ALT长文本ALT基准测试中，无需任何训练或微调，就实现了开源系统的最先进结果。我们还发布了MUSDB-ALT，这是第一个遵循Jam-ALT指南且人声干声公开的长文本歌词转录数据集。", "summary": "本文系统性地研究了音乐源分离技术对使用最先进的开源ASR模型Whisper进行自动歌词转录（ALT）的影响。研究评估了Whisper在原始音频、分离人声和人声干声上的表现。对于短文本转录，提出了一种连接方法，有效降低了词错误率。对于长文本转录，提出了一种利用源分离作为人声活动检测器来确定分段边界的算法，该算法显著降低了词错误率，并在Jam-ALT长文本ALT基准测试中，无需训练或微调便取得了开源系统的最佳结果。此外，本文还发布了首个符合Jam-ALT指南且人声干声公开的长文本歌词转录数据集MUSDB-ALT。", "keywords": "自动歌词转录, 音乐源分离, Whisper, 词错误率, MUSDB-ALT", "comments": "本文的创新点在于首次系统性地探讨了音乐源分离对自动歌词转录（ALT）性能的提升作用，并结合了先进的Whisper模型。其提出的针对短文本和长文本的特定方法，尤其是在长文本任务中利用源分离进行语音活动检测，为实际应用提供了有效途径。无需训练或微调即可达到SOTA结果，凸显了方法的普适性和有效性。新数据集的发布也为未来的研究提供了宝贵的资源。"}}
{"id": "2506.15100", "title": "International Security Applications of Flexible Hardware-Enabled Guarantees", "authors": ["Onni Aarne", "James Petrie"], "summary": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees\n(flexHEGs) offer opportunities to address international security challenges\nthrough comprehensive governance frameworks. This report examines how flexHEGs\ncould enable internationally trustworthy AI governance by establishing\nstandardized designs, robust ecosystem defenses, and clear operational\nparameters for AI-relevant chips. We analyze four critical international\nsecurity applications: limiting proliferation to address malicious use,\nimplementing safety norms to prevent loss of control, managing risks from\nmilitary AI systems, and supporting strategic stability through\nbalance-of-power mechanisms while respecting national sovereignty. The report\nexplores both targeted deployments for specific high-risk facilities and\ncomprehensive deployments covering all AI-relevant compute. We examine two\nprimary governance models: verification-based agreements that enable\ntransparent compliance monitoring, and ruleset-based agreements that\nautomatically enforce international rules through cryptographically-signed\nupdates. Through game-theoretic analysis, we demonstrate that comprehensive\nflexHEG agreements could remain stable under reasonable assumptions about state\npreferences and catastrophic risks. The report addresses critical\nimplementation challenges including technical thresholds for AI-relevant chips,\nmanagement of existing non-flexHEG hardware, and safeguards against abuse of\ngovernance power. While requiring significant international coordination,\nflexHEGs could provide a technical foundation for managing AI risks at the\nscale and speed necessary to address emerging threats to international security\nand stability.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15100v1", "AI": {"title_translation": "灵活硬件保障在国际安全中的应用", "tldr": "灵活硬件保障（flexHEGs）能为国际可信人工智能治理提供技术基础，以应对新兴安全威胁。", "motivation": "随着人工智能能力快速发展，国际安全面临挑战。本文旨在探讨灵活硬件保障（flexHEGs）如何通过全面的治理框架来应对这些挑战。", "method": "本报告审查了flexHEG如何通过建立标准化设计、强大的生态系统防御和清晰的人工智能相关芯片操作参数，实现国际可信人工智能治理。分析了四种关键的国际安全应用，探讨了针对特定高风险设施的靶向部署和覆盖所有人工智能相关计算的全面部署。研究了两种主要的治理模型：基于验证的协议和基于规则集的协议。通过博弈论分析，论证了全面flexHEG协议的稳定性。", "result": "通过博弈论分析表明，在对国家偏好和灾难性风险的合理假设下，全面的灵活硬件保障协议可以保持稳定。", "conclusion": "尽管需要大量的国际协调，灵活硬件保障能够提供一个技术基础，以必要的规模和速度管理人工智能风险，从而应对国际安全和稳定面临的新兴威胁。", "translation": "随着人工智能能力的快速发展，灵活硬件保障（flexHEGs）提供了通过全面的治理框架应对国际安全挑战的机会。本报告探讨了flexHEGs如何通过为人工智能相关芯片建立标准化设计、强大的生态系统防御和清晰的操作参数，实现国际可信的人工智能治理。我们分析了四个关键的国际安全应用：限制扩散以应对恶意使用，实施安全规范以防止失控，管理军事人工智能系统带来的风险，以及通过权力平衡机制支持战略稳定，同时尊重国家主权。报告探讨了针对特定高风险设施的靶向部署和覆盖所有人工智能相关计算的全面部署。我们研究了两种主要的治理模型：实现透明合规监控的基于验证的协议，以及通过加密签名更新自动执行国际规则的基于规则集的协议。通过博弈论分析，我们证明了在对国家偏好和灾难性风险的合理假设下，全面的灵活硬件保障协议可以保持稳定。报告解决了关键的实施挑战，包括人工智能相关芯片的技术阈值、现有非flexHEG硬件的管理以及防止治理权力滥用的保障措施。尽管需要大量的国际协调，flexHEGs可以为管理人工智能风险提供一个技术基础，以必要的规模和速度应对国际安全和稳定面临的新兴威胁。", "summary": "本报告探讨了灵活硬件保障（flexHEGs）在应对国际安全挑战中的应用，特别是在人工智能快速发展背景下。报告详细分析了flexHEG如何通过建立标准化设计、生态系统防御和操作参数来实现可信的人工智能治理，并研究了其在限制人工智能扩散、实施安全规范、管理军事人工智能风险和维护战略稳定等四个关键国际安全应用中的潜力。报告还考察了两种部署模式（靶向和全面）和两种治理模型（基于验证和基于规则集），并通过博弈论分析证明了全面flexHEG协议的稳定性。最终，报告强调了flexHEG作为管理人工智能风险技术基础的重要性，尽管其实施需要大量的国际协调。", "keywords": "灵活硬件保障, 人工智能治理, 国际安全, 风险管理, 战略稳定", "comments": "该论文提出了一种创新的方法，即利用灵活硬件保障（flexHEGs）来应对人工智能带来的国际安全挑战，具有重要的现实意义。其通过技术手段（标准化设计、硬件保障）结合治理框架（验证与规则集协议）来解决复杂的国际安全问题，体现了跨学科的深度融合。博弈论分析增加了其论证的严谨性。然而，该方法对国际协调和技术实施的复杂性提出了较高要求，尤其是在管理现有非flexHEG硬件和防止治理权力滥用方面，需要更详细的解决方案。"}}
{"id": "2506.15032", "title": "Assigning Multi-Robot Tasks to Multitasking Robots", "authors": ["Winston Smith", "Andrew Boateng", "Taha Shaheen", "Yu Zhang"], "summary": "One simplifying assumption in existing and well-performing task allocation\nmethods is that the robots are single-tasking: each robot operates on a single\ntask at any given time. While this assumption is harmless to make in some\nsituations, it can be inefficient or even infeasible in others. In this paper,\nwe consider assigning multi-robot tasks to multitasking robots. The key\ncontribution is a novel task allocation framework that incorporates the\nconsideration of physical constraints introduced by multitasking. This is in\ncontrast to the existing work where such constraints are largely ignored. After\nformulating the problem, we propose a compilation to weighted MAX-SAT, which\nallows us to leverage existing solvers for a solution. A more efficient greedy\nheuristic is then introduced. For evaluation, we first compare our methods with\na modern baseline that is efficient for single-tasking robots to validate the\nbenefits of multitasking in synthetic domains. Then, using a site-clearing\nscenario in simulation, we further illustrate the complex task interaction\nconsidered by the multitasking robots in our approach to demonstrate its\nperformance. Finally, we demonstrate a physical experiment to show how\nmultitasking enabled by our approach can benefit task efficiency in a realistic\nsetting.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15032v1", "AI": {"title_translation": "为多任务机器人分配多机器人任务", "tldr": "本文提出了一种新的任务分配框架，用于为多任务机器人分配多机器人任务，解决了现有方法中忽略物理约束的问题，并通过加权MAX-SAT编译和贪婪启发式方法进行求解，并在仿真和实际实验中验证了其效率。", "motivation": "现有任务分配方法通常假设机器人是单任务的，这在某些情况下会导致效率低下甚至不可行。本文旨在解决为多任务机器人分配多机器人任务的问题，并考虑多任务处理引入的物理约束。", "method": "本文提出了一种新颖的任务分配框架，该框架考虑了多任务处理引入的物理约束。问题被公式化为加权MAX-SAT问题，并利用现有求解器进行求解。此外，还引入了一种更高效的贪婪启发式方法。", "result": "通过与单任务机器人基线方法的比较，验证了多任务处理在合成领域中的益处。在场地清理仿真场景中，进一步展示了该方法考虑的复杂任务交互和性能。物理实验也表明，该方法实现的多任务处理可以提高实际设置中的任务效率。", "conclusion": "本文提出的框架和方法成功地为多任务机器人分配了多机器人任务，有效地考虑了物理约束，并显著提高了任务效率。", "translation": "现有表现良好的任务分配方法中，一个简化的假设是机器人是单任务的：每个机器人在任何给定时间只执行一个任务。虽然这个假设在某些情况下是无害的，但在其他情况下可能效率低下甚至不可行。在本文中，我们考虑为多任务机器人分配多机器人任务。关键贡献是一个新颖的任务分配框架，它纳入了多任务处理引入的物理约束的考虑。这与现有工作中很大程度上忽略此类约束的情况形成对比。在问题公式化之后，我们提出将其编译为加权MAX-SAT，这使我们能够利用现有求解器来获得解决方案。然后引入了一种更高效的贪婪启发式方法。为了进行评估，我们首先将我们的方法与一种对单任务机器人高效的现代基线进行比较，以验证多任务处理在合成领域中的益处。然后，在仿真中的场地清理场景中，我们进一步说明了我们方法中多任务机器人考虑的复杂任务交互，以证明其性能。最后，我们展示了一个物理实验，以说明我们的方法实现的多任务处理如何在实际设置中提高任务效率。", "summary": "本文针对多任务机器人分配多机器人任务的问题，提出了一种新颖的任务分配框架，该框架突破了传统单任务机器人假设的限制，并首次考虑了多任务处理带来的物理约束。研究将问题转化为加权MAX-SAT模型，并开发了高效的贪婪启发式算法进行求解。实验结果表明，该方法在合成环境和实际场景中均能有效提升任务效率。", "keywords": "多机器人任务分配, 多任务机器人, 物理约束, MAX-SAT, 贪婪启发式", "comments": "本文的创新之处在于首次在多机器人任务分配中考虑了多任务处理引入的物理约束，并为此提出了一个系统性的框架和求解方法。这对于提高机器人系统在复杂环境下的自主性和效率具有重要意义，尤其是在资源有限或任务交互复杂的场景中。该研究弥补了现有任务分配方法在多任务能力方面的不足。"}}
{"id": "2506.15453", "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.", "comment": "6 pages, 3 figures, 4 tables, conference paper", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15453v1", "AI": {"title_translation": "通过大型语言模型驱动的代码片段描述生成揭示意图", "tldr": "该研究调查了开发者常用的代码片段描述类型，并评估了大型语言模型（Llama）生成描述的能力。结果显示，大多数原始描述是基于示例的，且LLM在识别描述类型方面表现良好，但生成的描述与原始描述的相似度有待提高。", "motivation": "代码片段的文档化对于开发者和用户理解其关键区域至关重要，尤其对于第三方库。随着大型语言模型的兴起，研究的重点在于了解开发者常用的描述类型，并评估LLM在生成这些描述方面的支持能力。", "method": "研究使用了NPM代码片段数据集，包含185,412个包和1,024,579个代码片段。从中抽取了400个代码片段及其描述作为样本。首先进行人工分类，然后使用Llama模型评估其描述生成能力，并与原始描述进行相似度比较。", "result": "1. 人工分类发现，大多数原始描述（55.5%）侧重于基于示例的用法，并指出一些描述缺乏足够细节来传达意图。2. LLM（Llama）正确识别了79.75%的原始描述为“示例”类型，与人工分类结果一致，显示出泛化能力。3. LLM生成的描述与原始描述的平均相似度为0.7173，表明相关性，但仍有改进空间（低于0.9表示不相关）。", "conclusion": "研究结果表明，根据代码片段的任务，文档的意图可能有所不同，可以是使用说明、安装指南或供库用户学习的描述性示例。", "translation": "文档化代码片段对于明确开发者和用户应关注的关键区域至关重要。例如，用法示例和其他应用程序编程接口（API）对于第三方库尤为重要。随着大型语言模型（LLM）的兴起，主要目标是调查开发者常用的描述类型，并评估LLM（在此例中为Llama）如何支持描述生成。我们使用NPM代码片段数据集，该数据集包含185,412个包和1,024,579个代码片段。从中，我们使用400个代码片段（及其描述）作为样本。首先，我们的人工分类发现，大多数原始描述（55.5%）强调基于示例的用法。这一发现强调了清晰文档的重要性，因为一些描述缺乏足够的细节来传达意图。其次，LLM正确识别了大多数原始描述为“示例”（79.75%），这与我们的人工发现相同，显示出泛化倾向。第三，与原始描述相比，生成的描述平均相似度得分为0.7173，表明相关性但仍有改进空间。低于0.9的分数表示存在一些不相关性。我们的结果表明，根据代码片段的任务，文档的意图可能不同，可以是使用说明、安装指南，或任何库用户的描述性学习示例。", "summary": "本研究旨在调查开发者常用的代码片段描述类型，并评估大型语言模型（LLM）在生成此类描述方面的能力。研究使用了NPM代码片段数据集，并对400个样本进行了人工分类和LLM（Llama）评估。结果显示，大多数原始描述是基于示例的，且LLM在识别描述类型方面表现出良好的泛化能力。尽管LLM生成的描述与原始描述具有一定相似性（平均0.7173），但仍有改进空间。研究强调了代码片段文档意图的多样性。", "keywords": "代码片段, 大型语言模型, 描述生成, 文档, Llama", "comments": "该研究通过结合人工分类和LLM评估，提供了一个深入了解代码片段描述及其自动生成潜力的视角。其创新之处在于量化了LLM在识别描述类型上的能力，并指出了其在生成高质量、高相似度描述方面的局限性。这对于未来LLM在软件工程文档领域的应用具有重要指导意义。"}}
{"id": "2506.15008", "title": "Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output", "authors": ["Richa Gupta", "Alexander Htet Kyaw"], "summary": "Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.", "comment": "15 Pages, 6 figures, CAAD Futures 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15008v1", "AI": {"title_translation": "设计中洞察力驱动的生成式AI：将真实世界数据融入文本到图像输出", "tldr": "本文提出了一种将DALL-E 3与材料数据集结合的新型管道，为AI生成的室内设计图像提供可持续性指标和材料使用洞察，帮助设计师做出更环保的决策。", "motivation": "现有的文本到图像生成式AI虽然能产生视觉上吸引人的图像，但通常缺乏设计师所需的实际可操作数据，特别是关于可持续性指标和材料使用的洞察。", "method": "提出了一种新颖的管道，将DALL-E 3与一个材料数据集集成。模型生成室内设计图像后，一个后处理模块会识别出图像中前十种材料，并将其与通用材料字典中的碳当量（CO2e）值配对。通过三种用户测试评估系统：(1) 在提示前未提及可持续性；(2) 在提示前告知可持续性目标；(3) 在告知可持续性目标的同时，将量化的CO2e数据包含在生成式AI输出中。", "result": "质性和量化分析表明，在第三种测试中引入可持续性指标能促使更明智的设计决策，但同时也可能引发决策疲劳并降低整体满意度。尽管如此，大多数参与者在第三种测试中报告将可持续性原则纳入工作流程。", "conclusion": "整合指标具有引导更具生态责任实践的潜力，强调了平衡设计自由与实际约束的重要性，为AI辅助建筑设计中的整体、数据驱动解决方案提供了明确路径。", "translation": "生成式人工智能，特别是文本到图像模型，通过将概念想法从简单的文本提示快速转化为视觉表示，彻底改变了室内建筑设计。虽然生成式人工智能可以生成视觉上吸引人的图像，但它们通常缺乏设计师可操作的数据。在这项工作中，我们提出了一种新颖的管道，将DALL-E 3与一个材料数据集集成，以通过可持续性指标和材料使用洞察来丰富人工智能生成的设计。模型生成室内设计图像后，一个后处理模块会识别出其中存在的前十种材料，并将其与通用材料字典中的二氧化碳当量（CO2e）值配对。这种方法使设计师能够立即评估环境影响并相应地优化提示。我们通过三次用户测试评估了该系统：(1) 在与生成式人工智能的提示过程之前未向用户提及可持续性；(2) 在提示之前向用户传达可持续性目标；(3) 在告知可持续性目标的同时，将量化的CO2e数据包含在生成式人工智能输出中。我们的定性和定量分析表明，在第三次测试中引入可持续性指标导致了更明智的设计决策，但是，它也可能引发决策疲劳并降低整体满意度。尽管如此，大多数参与者报告在第三次测试中将可持续性原则纳入了他们的工作流程，这强调了集成指标在引导更具生态责任实践方面的潜力。我们的发现展示了平衡设计自由与实际约束的重要性，为AI辅助建筑设计中的整体、数据驱动解决方案提供了明确的路径。", "summary": "本文提出了一种创新的生成式AI管道，将DALL-E 3与材料数据集结合，旨在为室内建筑设计提供具有可持续性指标（如CO2e值）的文本到图像输出。通过识别AI生成图像中的材料并关联其环境影响数据，该系统使设计师能够做出更环保的决策。用户测试表明，虽然引入可持续性数据有助于更明智的设计选择，但也可能导致决策疲劳。然而，研究证实了集成指标在促进可持续设计实践中的潜力，并强调了数据驱动与设计自由平衡的重要性。", "keywords": "生成式AI, 文本到图像, 可持续设计, 室内建筑设计, 碳当量", "comments": "这项工作创新性地解决了生成式AI在设计领域缺乏可操作数据的问题，特别是在可持续性方面。通过将DALL-E 3与材料数据库集成并提供CO2e数据，它为设计师提供了即时环境影响评估能力，有助于推动更负责任的设计实践。一个重要的发现是，虽然数据丰富有助于决策，但过多的信息也可能导致决策疲劳，这提示了未来在信息呈现方式上需要进一步优化。"}}
{"id": "2506.14830", "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "summary": "Aiming at the critical role of SSD health state prediction in data\nreliability assurance, this study proposes a hybrid BiGRU-MHA model that\nincorporates a multi-head attention mechanism to enhance the accuracy and\nstability of storage device health classification. The model innovatively\nintegrates temporal feature extraction and key information focusing\ncapabilities. Specifically, it leverages the bidirectional timing modeling\nadvantages of the BiGRU network to capture both forward and backward\ndependencies of SSD degradation features. Simultaneously, the multi-head\nattention mechanism dynamically assigns feature weights, improving the model's\nsensitivity to critical health indicators. Experimental results show that the\nproposed model achieves classification accuracies of 92.70% on the training set\nand 92.44% on the test set, with a minimal performance gap of only 0.26%,\ndemonstrating excellent generalization ability. Further analysis using the\nreceiver operating characteristic (ROC) curve shows an area under the curve\n(AUC) of 0.94 on the test set, confirming the model's robust binary\nclassification performance. This work not only presents a new technical\napproach for SSD health prediction but also addresses the generalization\nbottleneck of traditional models, offering a verifiable method with practical\nvalue for preventive maintenance of industrial-grade storage systems. The\nresults show the model can significantly reduce data loss risks by providing\nearly failure warnings and help optimize maintenance costs, supporting\nintelligent decision-making in building reliable storage systems for cloud\ncomputing data centers and edge storage environments.", "comment": "Source code available; Accepted by 2025 6th International Conference\n  on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14830v1", "AI": {"title_translation": "基于多头注意力机制的双向门控循环单元优化固态硬盘健康状态分类模型", "tldr": "本研究提出了一种结合多头注意力机制的BiGRU-MHA混合模型，用于提高固态硬盘健康状态分类的准确性和稳定性，并取得了显著效果。", "motivation": "固态硬盘健康状态预测在数据可靠性保障中扮演着关键角色，传统模型存在泛化能力瓶颈。", "method": "提出了一种混合BiGRU-MHA模型，该模型结合了双向门控循环单元（BiGRU）进行时间特征提取，并融入多头注意力机制动态分配特征权重，以提高对关键健康指标的敏感性。", "result": "该模型在训练集上实现了92.70%的分类准确率，在测试集上达到92.44%，性能差距仅为0.26%，展现出卓越的泛化能力。ROC曲线分析显示测试集AUC为0.94，证实了其强大的二分类性能。", "conclusion": "该模型为固态硬盘健康预测提供了一种新的技术方法，有效解决了传统模型的泛化瓶颈，为工业级存储系统的预防性维护提供了可验证且具有实际价值的方法，能够显著降低数据丢失风险，优化维护成本，支持构建可靠存储系统的智能决策。", "translation": "针对固态硬盘健康状态预测在数据可靠性保障中的关键作用，本研究提出了一种结合多头注意力机制的混合BiGRU-MHA模型，旨在提高存储设备健康分类的准确性和稳定性。该模型创新性地整合了时间特征提取和关键信息聚焦能力。具体而言，它利用BiGRU网络的双向时序建模优势，捕获固态硬盘退化特征的前向和后向依赖关系。同时，多头注意力机制动态分配特征权重，提高了模型对关键健康指标的敏感性。实验结果表明，所提出的模型在训练集上实现了92.70%的分类准确率，在测试集上达到92.44%，性能差距仅为0.26%，展现出卓越的泛化能力。使用接收者操作特征（ROC）曲线的进一步分析显示，在测试集上曲线下面积（AUC）为0.94，证实了该模型强大的二分类性能。这项工作不仅为固态硬盘健康预测提供了一种新的技术方法，还解决了传统模型的泛化瓶颈，为工业级存储系统的预防性维护提供了一种可验证且具有实际价值的方法。结果表明，该模型可以通过提供早期故障预警显著降低数据丢失风险，并有助于优化维护成本，支持云计算数据中心和边缘存储环境构建可靠存储系统的智能决策。", "summary": "本研究提出了一种名为BiGRU-MHA的混合模型，结合了双向门控循环单元（BiGRU）和多头注意力机制，用于优化固态硬盘的健康状态分类。该模型旨在通过捕获时间依赖性和动态聚焦关键健康指标来提高预测准确性和稳定性。实验结果显示，该模型在分类准确率和泛化能力方面表现出色，并在ROC曲线上取得了高AUC值，证明了其在工业级存储系统预防性维护中的实际应用价值。", "keywords": "固态硬盘健康预测, BiGRU, 多头注意力机制, 混合模型, 故障分类", "comments": "该论文提出了一种创新的混合模型BiGRU-MHA，将BiGRU的双向时序建模能力与多头注意力机制的关键信息聚焦能力相结合，有效解决了固态硬盘健康预测中的准确性和泛化能力问题。其在工业级存储系统预防性维护方面的实际应用价值和对降低数据丢失风险的贡献是其重要亮点。"}}
{"id": "2506.15412", "title": "Golden Partition Zone: Rethinking Neural Network Partitioning Under Inversion Threats in Collaborative Inference", "authors": ["Rongke Liu", "Youwen Zhu"], "summary": "In collaborative inference, intermediate features transmitted from edge\ndevices can be exploited by adversaries to reconstruct original inputs via\nmodel inversion attacks (MIA). While existing defenses focus on shallow-layer\nprotection, they often incur significant utility loss. A key open question is\nhow to partition the edge-cloud model to maximize resistance to MIA while\nminimizing accuracy degradation. We first show that increasing model depth\nalone does not guarantee resistance. Through theoretical analysis, we\ndemonstrate that representational transitions in neural networks cause sharp\nchanges in conditional entropy $H(x\\mid z)$, with intra-class variance (denoted\n$R_c^2$) and feature dimensionality as critical factors. Experiments on three\nrepresentative deep vision models demonstrate that splitting at the\nrepresentational-transition or decision-level layers increases mean squared\nerror by more than four times compared to shallow splits, indicating\nsignificantly stronger resistance to MIA. Positive label smoothing further\nenhances robustness by compressing $R_c^2$ and improving generalization.\nFinally, we validate the resilience of decision-level features under enhanced\ninversion models and observe that the type of auxiliary data influences both\ntransition boundaries and reconstruction behavior.", "comment": "8 pages, 11 figures, 5 tables", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15412v1", "AI": {"title_translation": "黄金分区：协作推理中反演威胁下神经网络分区的再思考", "tldr": "本文研究了如何在协作推理中对神经网络进行分区，以最大程度地抵抗模型反演攻击并最小化准确性损失，发现将模型在表征转换层或决策层进行分割可以显著增强抵抗力。", "motivation": "在协作推理中，边缘设备传输的中间特征可能被攻击者利用，通过模型反演攻击（MIA）重建原始输入，而现有防御措施侧重于浅层保护，但会导致显著的效用损失。因此，一个关键的开放问题是如何分区边缘-云模型，以在最大化对MIA抵抗力的同时，最小化准确性下降。", "method": "首先，通过理论分析表明，神经网络中的表征转换会导致条件熵$H(x\\mid z)$的急剧变化，其中类内方差($R_c^2$)和特征维度是关键因素。然后，在三个代表性深度视觉模型上进行实验，并引入正标签平滑来增强鲁棒性。最后，验证了增强反演模型下决策层特征的弹性。", "result": "单独增加模型深度并不能保证抵抗力。在表征转换层或决策层进行分割比浅层分割能将均方误差增加四倍以上，表明对MIA有显著更强的抵抗力。正标签平滑通过压缩$R_c^2$并提高泛化能力，进一步增强了鲁棒性。决策层特征在增强的反演模型下表现出弹性。辅助数据的类型会影响转换边界和重建行为。", "conclusion": "在协作推理中，为了有效防御模型反演攻击，应将神经网络在表征转换层或决策层进行分区，而非仅仅增加模型深度。此外，结合正标签平滑可以进一步提升防御效果和模型鲁棒性。", "translation": "在协作推理中，边缘设备传输的中间特征可能被攻击者利用，通过模型反演攻击（MIA）重建原始输入。虽然现有防御措施侧重于浅层保护，但它们通常会导致显著的效用损失。一个关键的开放问题是如何分区边缘-云模型，以最大程度地抵抗MIA，同时最小化准确性下降。我们首先表明，单独增加模型深度并不能保证抵抗力。通过理论分析，我们证明了神经网络中的表征转换会导致条件熵$H(x\\mid z)$的急剧变化，其中类内方差（表示为$R_c^2$）和特征维度是关键因素。在三个代表性深度视觉模型上的实验表明，与浅层分割相比，在表征转换层或决策层进行分割能将均方误差增加四倍以上，这表明对MIA有显著更强的抵抗力。正标签平滑通过压缩$R_c^2$和提高泛化能力，进一步增强了鲁棒性。最后，我们验证了增强反演模型下决策层特征的弹性，并观察到辅助数据的类型会影响转换边界和重建行为。", "summary": "本文探讨了协作推理中神经网络分区以抵御模型反演攻击（MIA）的最佳策略。研究发现，单独增加模型深度不足以提供保护。通过理论分析和实验验证，作者指出在神经网络的表征转换层或决策层进行分割，能显著提高对MIA的抵抗力（均方误差增加四倍以上），远优于在浅层分割。此外，正标签平滑被证明能通过压缩类内方差和提高泛化能力来进一步增强鲁棒性。研究还强调了决策层特征的韧性以及辅助数据类型对重建行为的影响。", "keywords": "协作推理, 模型反演攻击, 神经网络分区, 表征转换, 决策层", "comments": "本文创新性地将模型反演攻击防御与神经网络分区策略相结合，并引入了“黄金分区”的概念，即在表征转换层或决策层进行分割。其理论分析揭示了条件熵、类内方差和特征维度在MIA抵抗中的关键作用，为未来的防御机制设计提供了深入的见解。实验结果支持了其核心主张，并提出了正标签平滑这一实用增强手段，具有重要的实践指导意义。"}}
{"id": "2506.15454", "title": "Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA", "authors": ["Nizar ALHafez", "Ahmad Kurdi"], "summary": "This paper presents a comprehensive comparison of three dominant parallel\nprogramming models in High Performance Computing (HPC): Message Passing\nInterface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device\nArchitecture (CUDA). Selecting optimal programming approaches for modern\nheterogeneous HPC architectures has become increasingly critical. We\nsystematically analyze these models across multiple dimensions: architectural\nfoundations, performance characteristics, domain-specific suitability,\nprogramming complexity, and recent advancements. We examine each model's\nstrengths, weaknesses, and optimization techniques. Our investigation\ndemonstrates that MPI excels in distributed memory environments with\nnear-linear scalability for communication-intensive applications, but faces\ncommunication overhead challenges. OpenMP provides strong performance and\nusability in shared-memory systems and loop-centric tasks, though it is limited\nby shared memory contention. CUDA offers substantial performance gains for\ndata-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires\nspecialized expertise. Performance evaluations across scientific simulations,\nmachine learning, and data analytics reveal that hybrid approaches combining\ntwo or more models often yield optimal results in heterogeneous environments.\nThe paper also discusses implementation challenges, optimization best\npractices, and emerging trends such as performance portability frameworks,\ntask-based programming, and the convergence of HPC and Big Data. This research\nhelps developers and researchers make informed decisions when selecting\nprogramming models for modern HPC applications, emphasizing that the best\nchoice depends on application requirements, hardware, and development\nconstraints.", "comment": "10 pages", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15454v1", "AI": {"title_translation": "现代HPC中的并行范式：MPI、OpenMP和CUDA的比较分析", "tldr": "本文对高性能计算（HPC）中MPI、OpenMP和CUDA三种主流并行编程模型进行了全面比较分析，探讨了它们的优缺点、适用场景和优化策略，并指出在异构环境中混合方法常能取得最佳效果。", "motivation": "在现代异构高性能计算（HPC）架构中，选择最优的并行编程方法变得越来越关键和复杂。", "method": "本文系统地比较分析了MPI、OpenMP和CUDA这三种并行编程模型，从架构基础、性能特征、领域特定适用性、编程复杂性和最新进展等多个维度进行评估。研究通过对科学模拟、机器学习和数据分析等领域的性能评估，揭示了不同模型的表现和混合方法的优势。", "result": "MPI在分布式内存环境和通信密集型应用中具有近线性可扩展性，但存在通信开销。OpenMP在共享内存系统和循环密集型任务中表现良好，但受限于共享内存争用。CUDA为数据并行GPU工作负载提供了显著性能提升，但仅限于NVIDIA GPU且需要专业知识。研究表明，在异构环境中，结合两种或更多模型的混合方法通常能获得最佳结果。", "conclusion": "为现代HPC应用选择最佳的并行编程模型取决于具体的应用程序需求、硬件环境和开发限制。本研究旨在帮助开发者和研究人员在此选择过程中做出明智的决策。", "translation": "本文对高性能计算（HPC）中三种主流并行编程模型进行了全面比较：消息传递接口（MPI）、开放多处理（OpenMP）和计算统一设备架构（CUDA）。为现代异构HPC架构选择最优的编程方法变得越来越关键。我们系统地分析了这些模型在多个维度上的表现：架构基础、性能特征、领域特定适用性、编程复杂性以及最新进展。我们 بررسی 了每种模型的优点、缺点和优化技术。我们的调查表明，MPI 在分布式内存环境中表现出色，对于通信密集型应用具有接近线性的可扩展性，但面临通信开销挑战。OpenMP 在共享内存系统和以循环为中心的任务中提供了强大的性能和可用性，但受限于共享内存争用。CUDA 为数据并行 GPU 工作负载提供了显著的性能提升，但仅限于 NVIDIA GPU 并需要专业知识。跨科学模拟、机器学习和数据分析的性能评估表明，在异构环境中，结合两种或更多模型的混合方法通常能产生最佳结果。本文还讨论了实现挑战、优化最佳实践以及新兴趋势，例如性能可移植性框架、基于任务的编程以及 HPC 和大数据融合。这项研究有助于开发人员和研究人员在为现代 HPC 应用程序选择编程模型时做出明智的决策，强调最佳选择取决于应用程序要求、硬件和开发限制。", "summary": "这篇论文对高性能计算（HPC）领域的三种主要并行编程模型——MPI、OpenMP和CUDA进行了深入比较分析。研究从架构、性能、适用性、复杂性等多维度评估了它们的优缺点，并发现混合编程方法在异构HPC环境中常能达到最佳性能。论文还探讨了实现挑战、优化策略及未来趋势，旨在指导开发者根据应用需求、硬件条件和开发限制做出明智的编程模型选择。", "keywords": "并行编程, HPC, MPI, OpenMP, CUDA, 异构计算", "comments": "这篇论文通过对MPI、OpenMP和CUDA这三种核心并行编程模型进行全面比较，为HPC领域的开发者和研究人员提供了宝贵的决策依据。其价值在于不仅详细分析了各模型的优缺点和适用场景，更强调了在日益复杂的异构计算环境中，采用混合编程方法的重要性。论文对新兴趋势的讨论也体现了其前瞻性。作为一篇综述性分析，它很好地概括了并行编程的现状与挑战。"}}
{"id": "2506.14832", "title": "ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Zixuan Dai", "Miao Zhang", "Shuai Lu"], "summary": "In contemporary architectural design, the growing complexity and diversity of\ndesign demands have made generative plugin tools essential for quickly\nproducing initial concepts and exploring novel 3D forms. However, objectively\nanalyzing the differences between human-designed and machine-generated 3D forms\nremains a challenge, limiting our understanding of their respective strengths\nand hindering the advancement of generative tools.\n  To address this, we built ArchForms-4000, a dataset containing 2,000\narchitect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet,\na 3D convolutional neural network tailored for classifying and analyzing\narchitectural forms, incorporating a saliency module to highlight key spatial\nfeatures aligned with architectural reasoning; And conducted comparative\nexperiments showing our model outperforms human experts in distinguishing form\norigins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.\n  This study not only highlights the distinctive advantages of human-designed\nforms in spatial organization, proportional harmony, and detail refinement but\nalso provides valuable insights for enhancing generative design tools in the\nfuture.", "comment": "22 pages, 8 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14832v1", "AI": {"title_translation": "ArchShapeNet：一个用于评估建筑形状的可解释3D-CNN框架", "tldr": "ArchShapeNet是一个可解释的3D-CNN框架，用于区分人类设计和机器生成的3D建筑形式，其性能优于人类专家，并揭示了人类设计形式的独特优势。", "motivation": "当代建筑设计中，生成式插件工具虽能快速生成概念和探索新3D形式，但客观分析人类设计与机器生成3D形式之间的差异仍是挑战，这限制了对各自优点的理解并阻碍了生成工具的进步。", "method": "构建了ArchForms-4000数据集，包含2000个建筑师设计和2000个Evomass生成的3D形式；提出了ArchShapeNet，一个专门用于分类和分析建筑形式的3D卷积神经网络，并融入了显著性模块以突出与建筑推理相关的关键空间特征；进行了比较实验。", "result": "ArchShapeNet在区分形式来源方面表现优于人类专家，准确率达到94.29%，精确率96.2%，召回率98.51%。研究还揭示了人类设计形式在空间组织、比例协调和细节细化方面的独特优势。", "conclusion": "本研究不仅突出了人类设计形式在空间组织、比例协调和细节细化方面的独特优势，也为未来增强生成式设计工具提供了宝贵见解。", "translation": "在当代建筑设计中，设计需求日益增长的复杂性和多样性使得生成式插件工具对于快速产生初始概念和探索新颖的3D形式变得至关重要。然而，客观分析人类设计和机器生成3D形式之间的差异仍然是一个挑战，这限制了我们对其各自优点的理解，并阻碍了生成工具的进步。\n为解决这个问题，我们构建了ArchForms-4000数据集，其中包含2000个建筑师设计和2000个Evomass生成的3D形式；提出了ArchShapeNet，一个专为分类和分析建筑形式而定制的3D卷积神经网络，它整合了一个显著性模块以突出与建筑推理相符的关键空间特征；并进行了比较实验，结果表明我们的模型在区分形式来源方面优于人类专家，实现了94.29%的准确率、96.2%的精确率和98.51%的召回率。\n这项研究不仅突出了人类设计形式在空间组织、比例协调和细节细化方面的独特优势，而且为未来增强生成式设计工具提供了宝贵的见解。", "summary": "该研究旨在解决人类设计和机器生成3D建筑形式之间差异分析的挑战。为此，作者构建了包含4000个3D形式的ArchForms-4000数据集，并提出了ArchShapeNet，一个带有显著性模块的3D-CNN，用于分类和分析建筑形状。实验结果表明，ArchShapeNet在区分形式来源方面表现出色，其准确率、精确率和召回率均高于人类专家。此外，研究还揭示了人类设计形式在空间组织、比例和细节方面的独特优势，为未来生成式设计工具的改进提供了指导。", "keywords": "3D-CNN, 建筑形状评估, 可解释性, 生成式设计, ArchShapeNet", "comments": "该论文提出了一种新颖的可解释3D-CNN框架ArchShapeNet，用于评估建筑形状，并有效区分人类设计和机器生成的作品。其创新之处在于结合了显著性模块，使得模型决策具有可解释性，这对于理解建筑设计意图至关重要。通过构建大规模的ArchForms-4000数据集，为该领域的研究提供了宝贵的资源。模型的卓越性能及其对人类设计优势的洞察，为未来生成式设计工具的进步指明了方向，具有重要的实践意义。"}}
{"id": "2506.15047", "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi", "Dong Whi Yoo", "Keran Wang", "Violeta J. Rodriguez", "Ravi Karkar", "Koustuv Saha"], "summary": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15047v1", "AI": {"title_translation": "映射看护者需求至AI聊天机器人设计：阿尔茨海默症和痴呆症看护者心理健康支持的优势与不足", "tldr": "研究开发了一个基于GPT-4o的聊天机器人Carey，通过访谈16位看护者，映射了看护者对AI聊天机器人在心理健康支持方面的需求、优势、不足和设计建议。", "motivation": "阿尔茨海默症及相关痴呆症(AD/ADRD)看护者面临巨大的情感和物流挑战，导致高压、焦虑和抑郁风险。尽管生成式AI（特别是LLM）提供了心理健康支持的新机会，但对看护者如何感知和使用这些技术知之甚少。本研究旨在弥补这一空白。", "method": "开发了基于GPT-4o的聊天机器人Carey作为技术探测工具，通过情景驱动互动后，对16位家庭看护者进行半结构化访谈。使用归纳编码和反思性主题分析，系统理解看护者需求和期望，并识别了看护者期望和担忧之间的微妙张力。", "result": "识别了看护者需求的六个主题：按需信息获取、情感支持、安全披露空间、危机管理、个性化和数据隐私。并为每个主题识别了看护者愿望和担忧之间的细微紧张关系。提出了看护者需求、AI聊天机器人优势、不足和设计建议的映射。", "conclusion": "研究结果为设计主动、值得信赖和以看护者为中心的AI系统提供了理论和实践见解，以更好地支持AD/ADRD看护者不断变化的心理健康需求。", "translation": "阿尔茨海默症及相关痴呆症（AD/ADRD）患者的家庭看护者面临着重大的情感和后勤挑战，这使他们面临更高的压力、焦虑和抑郁风险。尽管生成式人工智能（特别是大型语言模型，LLMs）的最新进展为心理健康支持提供了新的机会，但看护者如何看待和使用这些技术却鲜为人知。为了弥补这一空白，我们开发了Carey，一个基于GPT-4o的聊天机器人，旨在为AD/ADRD看护者提供信息和情感支持。我们以Carey作为技术探测工具，在基于常见看护压力源的情景驱动互动后，对16位家庭看护者进行了半结构化访谈。通过归纳编码和反思性主题分析，我们系统地理解了看护者在六个主题方面的需求和期望——按需信息获取、情感支持、安全披露空间、危机管理、个性化和数据隐私。对于每个主题，我们还确定了看护者愿望和担忧之间的微妙张力。我们提出了看护者需求、AI聊天机器人优势、不足和设计建议的映射。我们的研究结果提供了理论和实践见解，以指导设计主动的、值得信赖的、以看护者为中心的AI系统，从而更好地支持AD/ADRD看护者不断变化的心理健康需求。", "summary": "本研究旨在弥补阿尔茨海默症及相关痴呆症看护者心理健康支持领域中AI应用认知的空白。研究开发了基于GPT-4o的聊天机器人Carey，并对16位看护者进行了访谈，以探测其需求。结果揭示了看护者对AI聊天机器人在信息获取、情感支持、隐私等六个方面的需求和担忧，并提出了AI聊天机器人在该领域的优势、不足及设计建议，为未来开发以看护者为中心的AI系统提供了指导。", "keywords": "AI聊天机器人, 看护者, 心理健康支持, 阿尔茨海默症, GPT-4o", "comments": "本文创新性地将生成式AI应用于AD/ADRD看护者心理健康支持领域，通过用户研究深入挖掘了看护者的真实需求和担忧，为AI聊天机器人的设计提供了宝贵的实践指导和理论见解。其优势在于结合了前沿AI技术和扎实的用户研究方法，但可能存在样本量较小和AI模型仍在发展中的局限性。"}}
{"id": "2506.15192", "title": "Microgrid Operation Control with Adaptable Droop Gains", "authors": ["E. D. Gomez Anccas", "C. A. Hans", "D. Schulz"], "summary": "Modern low-carbon power systems come with many challenges, such as increased\ninverter penetration and increased uncertainty from renewable sources and\nloads. In this context, the microgrid concept is a promising approach, which is\nbased on a segmentation of the grid into independent smaller cells that can run\neither in grid-connected or standalone mode.In microgrids, droop control is\nwidely used for primary control. It enables proportional power sharing,\ndepending on the droop gains. Operation control schemes considering droop\ncontrol often assume fixed droop gains. However, using adaptive droop gains for\ngrid-forming units allow to shape power sharing in presence of fluctuations,\nenhancing flexibility while maintaining a safe microgrid operation,\nparticularly under uncertainty. This work introduces a bilinear formulation for\nmicrogrid operation control that finds optimal power setpoints and droop gains\non a timescale of minutes by solving a finite horizon optimization problem. In\ndetail, a robust minmax model predictive control scheme is designed for a\nstandalone microgrid, comprising a fuel cell, a photovoltaic system and an\nenergy storage. Closed-loop simulations are performed with and without variable\ndroop gains. The results show an increase in renewable utilization of up to 7.5\n% while reducing the power output of the fuel cell by 6 %, when allowing\nvariable droop gains.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15192v1", "AI": {"title_translation": "自适应下垂增益的微电网运行控制", "tldr": "本文提出了一种用于独立微电网的鲁棒模型预测控制方案，通过自适应下垂增益优化功率共享，提高了可再生能源利用率并降低了燃料电池输出功率。", "motivation": "现代低碳电力系统面临逆变器渗透率增加、可再生能源和负荷不确定性增加等挑战。微电网作为一种解决方案前景广阔，其中下垂控制被广泛用于一次控制。然而，传统的固定下垂增益限制了灵活性，因此需要自适应下垂增益来在不确定性下提高灵活性并维持微电网安全运行。", "method": "本文引入了一种用于微电网运行控制的双线性公式，通过解决有限时间域优化问题来寻找最优功率设定点和下垂增益。具体设计了一个鲁棒minmax模型预测控制方案，用于包含燃料电池、光伏系统和储能的独立微电网。通过有无可变下垂增益的闭环仿真进行了验证。", "result": "当允许可变下垂增益时，可再生能源利用率提高了高达7.5%，同时燃料电池的功率输出降低了6%。", "conclusion": "可变下垂增益显著改善了微电网的运行，尤其是在不确定性下，实现了更高的可再生能源利用率并减少了对传统发电机的依赖。", "translation": "现代低碳电力系统面临诸多挑战，例如逆变器渗透率增加以及可再生能源和负荷的不确定性增加。在此背景下，微电网概念是一种很有前景的方法，它基于将电网分割成独立的更小单元，这些单元可以在并网或独立模式下运行。在微电网中，下垂控制被广泛用于一次控制。它根据下垂增益实现比例功率共享。考虑下垂控制的运行控制方案通常假设下垂增益是固定的。然而，对并网形成单元使用自适应下垂增益可以在波动存在的情况下调整功率共享，从而提高灵活性，同时保持微电网安全运行，特别是在不确定性下。这项工作引入了一种用于微电网运行控制的双线性公式，通过解决有限时间域优化问题，在分钟级时间尺度上找到最优功率设定点和下垂增益。具体来说，为独立微电网设计了一个鲁棒minmax模型预测控制方案，该微电网包含燃料电池、光伏系统和储能。对有和没有可变下垂增益的情况进行了闭环仿真。结果表明，当允许可变下垂增益时，可再生能源利用率提高了高达7.5%，同时燃料电池的功率输出降低了6%。", "summary": "本文提出了一种新颖的鲁棒minmax模型预测控制方案，用于独立微电网，该方案在双线性公式中利用自适应下垂增益。该方法在有限时间域内优化功率设定点和下垂增益。仿真结果表明，可变下垂增益将可再生能源利用率提高了高达7.5%，并使燃料电池功率输出降低了6%，展示了在不确定环境下操作灵活性和效率的显著提升。", "keywords": "微电网, 下垂控制, 自适应增益, 模型预测控制, 可再生能源", "comments": "该论文的创新之处在于将自适应下垂增益应用于鲁棒minmax模型预测控制框架下的微电网运行，直接解决了不确定性带来的挑战，并提高了系统的灵活性和可再生能源的整合能力。"}}
{"id": "2506.15567", "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents", "authors": ["Aline Dobrovsky", "Konstantin Schekotihin", "Christian Burmer"], "summary": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15567v1", "AI": {"title_translation": "LLM（大型语言模型）驱动的推理和行动代理在复杂故障分析工作流管理中的应用", "tldr": "本文提出一个基于LLM的规划代理（LPA）来自动化和管理复杂的故障分析工作流。", "motivation": "故障分析（FA）是一个复杂且知识密集的过程。随着AI模型在FA实验室中的部署增加，如何将这些AI组件协调成具有凝聚力且高效的工作流，并使其与FA过程无缝集成，是一个主要挑战。", "method": "本文设计并实现了一个基于大型语言模型（LLM）的规划代理（LPA）。该LPA将LLM与高级规划能力和外部工具利用相结合，使其能够自主处理复杂查询、从外部系统检索相关数据并生成人类可读的响应。", "result": "评估结果表明，该代理在支持故障分析（FA）任务方面具有操作有效性和可靠性。", "conclusion": "基于LLM的规划代理（LPA）能够有效且可靠地支持复杂的故障分析任务，自动化其工作流，解决了AI组件集成带来的协调挑战。", "translation": "故障分析（FA）是一个高度复杂且知识密集的过程。在FA实验室的计算基础设施中整合AI组件，有可能自动化各种任务，包括图像中不合格品的检测、从不同数据源检索类似案例以及从带注释的图像生成报告。然而，随着部署的AI模型数量增加，挑战在于如何将这些组件协调成具有凝聚力且高效的工作流，使其与FA过程无缝集成。\n本文研究了基于大型语言模型（LLM）的规划代理（LPA）的设计和实现，以协助FA工程师解决他们的分析案例。LPA将LLM与高级规划能力和外部工具利用相结合，能够自主处理复杂查询、从外部系统检索相关数据并生成人类可读的响应。评估结果证明了该代理在支持FA任务方面的操作有效性和可靠性。", "summary": "本文针对故障分析（FA）中日益增长的AI模型集成挑战，提出了一种基于大型语言模型（LLM）的规划代理（LPA）。该LPA通过结合LLM的规划能力和外部工具调用，旨在自动化和优化复杂的FA工作流，从而提高FA工程师的工作效率。评估结果证实了该代理在实际FA任务中的有效性和可靠性。", "keywords": "故障分析, 大型语言模型, 规划代理, 工作流管理, AI自动化", "comments": "这项工作通过引入LLM驱动的智能代理来管理复杂的故障分析工作流，具有创新性。它解决了AI组件集成带来的协调挑战，并展示了LLM在工业自动化和复杂任务管理中的巨大潜力。其重要性在于提高了故障分析的效率和自动化水平。"}}
{"id": "2506.15273", "title": "Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access", "authors": ["Anup Mishra", "Čedomir Stefanović", "Xiuqiang Xu", "Petar Popovski", "Israel Leyva-Mayorga"], "summary": "Flexible and efficient wireless resource sharing across heterogeneous\nservices is a key objective for future wireless networks. In this context, we\ninvestigate the performance of a system where latency-constrained\ninternet-of-things (IoT) devices coexist with a broadband user. The base\nstation adopts a grant-free access framework to manage resource allocation,\neither through orthogonal radio access network (RAN) slicing or by allowing\nshared access between services. For the IoT users, we propose a reinforcement\nlearning (RL) approach based on double Q-Learning (QL) to optimise their\nrepetition-based transmission strategy, allowing them to adapt to varying\nlevels of interference and meet a predefined latency target. We evaluate the\nsystem's performance in terms of the cumulative distribution function of IoT\nusers' latency, as well as the broadband user's throughput and energy\nefficiency (EE). Our results show that the proposed RL-based access policies\nsignificantly enhance the latency performance of IoT users in both RAN Slicing\nand RAN Sharing scenarios, while preserving desirable broadband throughput and\nEE. Furthermore, the proposed policies enable RAN Sharing to be\nenergy-efficient at low IoT traffic levels, and RAN Slicing to be favourable\nunder high IoT traffic.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15273v1", "AI": {"title_translation": "异构无线接入中基于强化学习的策略优化", "tldr": "本文提出了一种基于强化学习的策略优化方法，用于异构无线网络中物联网设备和宽带用户的资源共享，以优化物联网设备的延迟性能，同时保持宽带用户的吞吐量和能效。", "motivation": "未来无线网络的关键目标是实现异构服务间灵活高效的无线资源共享。本文研究的是在延迟受限的物联网设备与宽带用户共存的系统中，如何优化资源分配。", "method": "基地站采用免授权接入框架进行资源分配，支持正交RAN切片或服务间共享接入。针对物联网用户，提出了一种基于双Q学习（DQL）的强化学习方法来优化其基于重复的传输策略，使其适应不同干扰水平并满足预设延迟目标。", "result": "提议的基于RL的接入策略显著增强了物联网用户在RAN切片和RAN共享场景下的延迟性能，同时保持了理想的宽带吞吐量和能效。此外，提议的策略使得RAN共享在低物联网流量下能效高，而RAN切片在高物联网流量下更具优势。", "conclusion": "基于强化学习的策略优化能够有效提升异构无线网络中物联网设备的延迟性能，并能根据流量情况灵活选择RAN共享或RAN切片以达到最佳效果。", "translation": "未来无线网络的一个关键目标是实现异构服务间灵活高效的无线资源共享。在此背景下，我们研究了一个系统中，延迟受限的物联网（IoT）设备与宽带用户共存的性能。基站采用免授权接入框架来管理资源分配，这可以通过正交无线接入网络（RAN）切片或允许服务间共享接入来实现。对于物联网用户，我们提出了一种基于双Q学习（QL）的强化学习（RL）方法，以优化其基于重复的传输策略，使其能够适应不同程度的干扰并满足预定义的延迟目标。我们从物联网用户延迟的累积分布函数以及宽带用户的吞吐量和能量效率（EE）方面评估了系统的性能。我们的结果表明，所提出的基于RL的接入策略在RAN切片和RAN共享两种场景下都显著增强了物联网用户的延迟性能，同时保持了理想的宽带吞吐量和能量效率。此外，所提出的策略使得RAN共享在低物联网流量水平下具有能源效率，而RAN切片在高物联网流量下更有利。", "summary": "本文研究了未来无线网络中异构服务（延迟受限物联网设备与宽带用户）的资源共享问题。提出了一种基于双Q学习的强化学习方法，优化物联网设备的重复传输策略，以适应干扰并满足延迟目标。评估结果显示，该RL策略在RAN切片和RAN共享模式下均能显著提升物联网用户延迟性能，同时保持宽带用户性能，并能根据物联网流量水平选择最优的接入模式。", "keywords": "强化学习, 双Q学习, 异构无线网络, 资源分配, 物联网", "comments": "本文的创新点在于将强化学习（特别是双Q学习）应用于异构无线网络中的资源分配和传输策略优化，以解决物联网设备的延迟问题。其重要性在于为未来5G及 beyond 的复杂异构网络提供了新的资源管理范式，实现了对不同服务质量要求的灵活适配。"}}
{"id": "2506.15612", "title": "A survey of Chernoff and Hoeffding bounds", "authors": ["Alexandros V. Gerbessiotis"], "summary": "This is a survey paper that discusses the original bounds of the seminal\npapers by Chernoff and Hoeffding. Moreover, it includes a variety of derivative\nbounds in a variety of forms. Complete proofs are provided as needed. The\nintent is to provide a repository of reference bounds for the interested\nresearcher.", "comment": null, "cate": "cs.DM", "url": "http://arxiv.org/abs/2506.15612v1", "AI": {"title_translation": "Chernoff和Hoeffding界限的综述", "tldr": "这篇综述论文详细讨论了Chernoff和Hoeffding的原始界限及其多种衍生形式，旨在为研究人员提供一个全面的参考资料库。", "motivation": "为感兴趣的研究人员提供一个关于Chernoff和Hoeffding界限及其衍生形式的参考资料库。", "method": "本文讨论了Chernoff和Hoeffding的原始界限，并包含了各种形式的衍生界限，按需提供了完整的证明。", "result": "提供了一个关于Chernoff和Hoeffding界限及其多种衍生形式的全面讨论和参考资料。", "conclusion": "本文旨在为感兴趣的研究人员提供一个关于Chernoff和Hoeffding界限及其衍生形式的参考资料库。", "translation": "这是一篇综述性论文，讨论了Chernoff和Hoeffding开创性论文中的原始界限。此外，它还包括各种形式的多种衍生界限。根据需要提供了完整的证明。其目的是为感兴趣的研究人员提供一个参考界限的存储库。", "summary": "这篇综述论文详细探讨了Chernoff和Hoeffding的原始界限以及多种衍生形式。论文旨在通过提供完整的证明，为研究人员建立一个关于这些重要概率界限的参考资源库。", "keywords": "Chernoff界限, Hoeffding界限, 概率界限, 综述, 统计", "comments": "这篇综述论文对于需要深入理解和应用Chernoff和Hoeffding界限的研究人员非常有用，因为它不仅回顾了原始概念，还包含了多种衍生形式和证明，提供了一个全面的参考。"}}
{"id": "2506.14789", "title": "AZT1D: A Real-World Dataset for Type 1 Diabetes", "authors": ["Saman Khamesian", "Asiful Arefeen", "Bithika M. Thompson", "Maria Adela Grando", "Hassan Ghasemzadeh"], "summary": "High quality real world datasets are essential for advancing data driven\napproaches in type 1 diabetes (T1D) management, including personalized therapy\ndesign, digital twin systems, and glucose prediction models. However, progress\nin this area has been limited by the scarcity of publicly available datasets\nthat offer detailed and comprehensive patient data. To address this gap, we\npresent AZT1D, a dataset containing data collected from 25 individuals with T1D\non automated insulin delivery (AID) systems. AZT1D includes continuous glucose\nmonitoring (CGM) data, insulin pump and insulin administration data,\ncarbohydrate intake, and device mode (regular, sleep, and exercise) obtained\nover 6 to 8 weeks for each patient. Notably, the dataset provides granular\ndetails on bolus insulin delivery (i.e., total dose, bolus type, correction\nspecific amounts) features that are rarely found in existing datasets. By\noffering rich, naturalistic data, AZT1D supports a wide range of artificial\nintelligence and machine learning applications aimed at improving clinical\ndecision making and individualized care in T1D.", "comment": "4 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14789v1", "AI": {"title_translation": "AZT1D：一个真实世界1型糖尿病数据集", "tldr": "AZT1D是一个包含来自25名使用自动化胰岛素输送系统患者的详细真实世界1型糖尿病数据集，旨在弥补现有公开数据集的不足。", "motivation": "高质量的真实世界数据集对于推动1型糖尿病（T1D）管理中的数据驱动方法至关重要，包括个性化治疗设计、数字孪生系统和血糖预测模型。然而，现有公开数据集的稀缺性限制了这方面的进展。", "method": "研究人员提出了AZT1D数据集，该数据集收集了25名使用自动化胰岛素输送（AID）系统的1型糖尿病患者的数据。", "result": "AZT1D数据集包含了连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及设备模式（常规、睡眠和运动），每位患者的数据收集了6到8周。值得注意的是，该数据集提供了现有数据集中罕见的详细推注胰岛素输送信息（即总剂量、推注类型、校正具体量）。", "conclusion": "通过提供丰富的、自然的数据，AZT1D支持广泛的人工智能和机器学习应用，旨在改善1型糖尿病的临床决策和个体化护理。", "translation": "高质量的真实世界数据集对于推进1型糖尿病（T1D）管理中的数据驱动方法至关重要，包括个性化治疗设计、数字孪生系统和血糖预测模型。然而，该领域的进展受到公开可用数据集稀缺的限制，这些数据集通常缺乏详细和全面的患者数据。为了弥补这一空白，我们提出了AZT1D，一个包含从25名使用自动化胰岛素输送（AID）系统的T1D患者收集的数据集。AZT1D包括连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及每位患者6到8周的设备模式（常规、睡眠和运动）数据。值得注意的是，该数据集提供了关于推注胰岛素输送的详细信息（即总剂量、推注类型、校正具体量），这些特征在现有数据集中很少发现。通过提供丰富、自然的数据，AZT1D支持广泛的人工智能和机器学习应用，旨在改善T1D中的临床决策和个体化护理。", "summary": "本论文介绍了AZT1D，一个为解决1型糖尿病（T1D）领域高质量真实世界数据集稀缺问题而创建的数据集。该数据集包含了25名使用自动化胰岛素输送系统患者的详细数据，包括连续血糖监测、胰岛素泵数据、碳水化合物摄入和设备模式等，尤其提供了现有数据集中罕见的推注胰岛素输送的细粒度信息。AZT1D旨在支持AI和机器学习应用，以改进T1D的临床决策和个性化护理。", "keywords": "1型糖尿病, 真实世界数据集, 自动化胰岛素输送, 连续血糖监测, 推注胰岛素", "comments": "AZT1D数据集的创新之处在于其提供了现有数据集中罕见的推注胰岛素输送的详细信息，这对于开发更精准的个性化治疗和血糖预测模型至关重要。它填补了1型糖尿病领域高质量真实世界数据稀缺的空白，对推动该领域的数据驱动研究具有重要意义。"}}
{"id": "2506.15068", "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation", "authors": ["Zongxia Li", "Yapei Chang", "Yuhang Zhou", "Xiyang Wu", "Zichao Liang", "Yoo Yeon Sung", "Jordan Lee Boyd-Graber"], "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15068v1", "AI": {"title_translation": "自由形式生成中开放式R1训练的语义感知奖励", "tldr": "本文提出PrefBERT模型，用于解决开放式长文本生成评估的挑战，通过提供语义感知奖励，有效指导GRPO训练，生成更符合人类偏好的响应，优于传统评估指标。", "motivation": "评估开放式长文本生成极具挑战性，因为很难清晰界定好坏输出。现有方法常遗漏连贯性、风格或相关性等关键方面，或受预训练数据偏见影响，使开放式长文本评估成为一个未充分探索的问题。", "method": "本文提出PrefBERT，一个评分模型，用于评估GRPO中的开放式长文本生成，并通过区分好坏输出的奖励来指导训练。PrefBERT在两个具有多样长文本风格和Likert评分质量的响应评估数据集上进行训练，旨在提供比ROUGE-L和BERTScore更好的语义奖励反馈。", "result": "PrefBERT在多句和段落长度响应上训练后，在各种长篇段落中保持可靠性，并与GRPO所需的验证性奖励良好对齐。人类评估证实，使用PrefBERT作为奖励信号训练策略模型，生成的响应比传统指标训练的响应更符合人类偏好。", "conclusion": "PrefBERT模型能有效评估开放式长文本生成，并作为奖励信号指导GRPO训练，从而生成更符合人类偏好的响应，解决了现有评估方法的不足。", "translation": "评估开放式长文本生成具有挑战性，因为很难定义好坏输出之间的明确界限。现有方法常常遗漏连贯性、风格或相关性等关键方面，或者受到预训练数据的偏见，使得开放式长文本评估成为一个未充分探索的问题。为了弥补这一空白，我们提出了PrefBERT，一个评分模型，用于评估GRPO中的开放式长文本生成，并通过为好坏输出提供不同的奖励来指导其训练。PrefBERT在两个具有多样长文本风格和Likert评分质量的响应评估数据集上进行训练，通过提供比传统指标ROUGE-L和BERTScore更好的语义奖励反馈，有效地支持了GRPO。通过包括LLM作为评判、人类评分和定性分析在内的全面评估，我们表明PrefBERT在多句和段落长度的响应上进行训练后，在各种长篇段落中仍保持可靠性，并与GRPO所需的验证性奖励良好对齐。人类评估证实，使用PrefBERT作为奖励信号来训练策略模型，生成的响应比使用传统指标训练的响应更符合人类偏好。我们的代码可在https://github.com/zli12321/long_form_rl 获取。", "summary": "本文提出PrefBERT模型，旨在解决开放式长文本生成评估的难题。PrefBERT是一个语义感知评分模型，通过在多样化数据集上训练，为强化学习（GRPO）提供优于传统指标（如ROUGE-L和BERTScore）的语义奖励反馈。综合评估（包括LLM评判和人类评分）表明，PrefBERT在长文本上表现可靠，并能有效引导策略模型生成更符合人类偏好的响应。", "keywords": "开放式生成, 长文本评估, 语义奖励, PrefBERT, 强化学习", "comments": "创新点在于提出了PrefBERT，一个专门用于开放式长文本生成的语义感知奖励模型，它能有效捕捉传统指标难以衡量的连贯性、风格和相关性等关键属性。该方法通过为GRPO提供更精细的奖励信号，显著提升了生成模型与人类偏好的一致性，解决了开放式生成评估的长期挑战。重要性体现在其为长文本生成中的强化学习训练提供了更准确、更具语义洞察力的评估工具，有望推动该领域的发展。局限性方面，抽象中未明确提及，但通常此类模型在特定领域或数据分布之外的泛化能力可能需要进一步验证。"}}
{"id": "2506.14814", "title": "Semi-orthogonal Tribonacci Wavelets and Numerical Solutions of Nonlinear Singular BVPs Arising in a Chemical Reaction", "authors": ["Ankita Yadav", "Amit K. Verma"], "summary": "In this article, we introduce a semi-orthogonal tribonacci wavelet and\ndevelop a semi-orthogonal tribonacci wavelet collocation method, offering an\neffective numerical method for solving a class of non-linear singular BVPs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14814v1", "AI": {"title_translation": "半正交Tribonacci小波与化学反应中非线性奇异边值问题的数值解", "tldr": "本文引入了半正交Tribonacci小波并开发了相应的配置法，为解决非线性奇异边值问题提供了一种有效的数值方法。", "motivation": "为解决一类非线性奇异边值问题提供一种有效的数值方法。", "method": "引入了一种半正交Tribonacci小波，并开发了一种半正交Tribonacci小波配置法。", "result": "开发了一种解决一类非线性奇异边值问题的有效数值方法。", "conclusion": "半正交Tribonacci小波配置法是解决非线性奇异边值问题的有效数值方法。", "translation": "在本文中，我们引入了一种半正交Tribonacci小波，并开发了一种半正交Tribonacci小波配置法，为解决一类非线性奇异边值问题提供了一种有效的数值方法。", "summary": "本文介绍了一种新颖的半正交Tribonacci小波及其相应的配置方法。该方法为解决一类非线性奇异边值问题提供了一种有效的数值解法，尤其适用于化学反应中出现的此类问题。", "keywords": "半正交Tribonacci小波, 小波配置法, 非线性奇异边值问题, 数值解, 化学反应", "comments": "该论文的创新点在于引入并应用了一种新型小波（Tribonacci小波）来解决具有挑战性的非线性奇异边值问题，展示了其在数值方法领域的潜力。"}}
{"id": "2506.15107", "title": "I Know You're Listening: Adaptive Voice for HRI", "authors": ["Paige Tuttösí"], "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs.", "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts IROS\n  23 Mmm whatcha say? Uncovering distal and proximal context effects in first\n  and second-language word perception using psychophysical reverse correlation\n  INTERSPEECH 24 Emojivoice: Towards long-term controllable expressivity in\n  robot speech RO-MAN 25", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15107v1", "AI": {"title_translation": "我知道你在听：用于人机交互的自适应语音", "tldr": "本文提出了一种轻量级、富有表现力且能适应环境并提高L2学习者清晰度的机器人语音系统，以改善语言教学机器人的效果。", "motivation": "尽管社交机器人在语言教学中的应用已被探索，但针对语言教学机器人任务特定的合成语音工作有限。考虑到语言是一项口头任务，这一空白可能严重影响机器人语言教学任务的有效性。", "method": "1. 使用微调版Matcha-TTS，通过表情符号提示创建了一种轻量级、富有表现力的语音，可在有限计算资源下实时运行。2. 探索了如何使机器人语音适应物理和社会环境，发现在嘈杂和高能量环境中提高音高和音高率能使机器人语音更恰当且更具环境感知能力。3. 创建了一个针对L2听众的英语TTS系统，通过数据驱动、基于感知的方***究了L2学习者如何使用持续时间线索来解释英语中难以区分的紧元音和松元音单词，并为Matcha-TTS创建了一个“L2清晰模式”，对紧元音进行延长而保持松元音不变。", "result": "1. 创建的语音在案例研究中被发现更具表现力、更符合社交场合，并且适合长时间的富有表现力的语音（如讲故事）。2. 提高音高和音高率在嘈杂和高能量环境中使机器人语音显得更恰当，并使其更具环境感知能力。3. “L2清晰模式”被发现比基础Matcha-TTS更受尊重、更易懂、更具鼓励性，同时减少了这些挑战性紧/松元音最小对的转录错误。", "conclusion": "本文通过开发轻量级、富有表现力、环境自适应以及针对L2学习者优化的机器人语音，显著提升了语言教学机器人的潜在有效性，填补了任务特定合成语音的空白。", "translation": "尽管社交机器人在语言教学中的应用已被探索，但针对语言教学机器人任务特定的合成语音工作仍然有限。考虑到语言是一项口头任务，这一空白可能严重影响机器人语言教学任务的有效性。我们通过三项贡献解决了L2教学机器人语音的这一不足：1. 我们解决了对轻量级和富有表现力的机器人语音的需求。通过使用微调版的Matcha-TTS，我们利用表情符号提示来创建一种富有表现力的语音，该语音随时间表现出一定范围的表现力。该语音可以在有限的计算资源下实时运行。通过案例研究，我们发现这种语音更具表现力、更符合社交场合，并且适合长时间的富有表现力的语音，例如讲故事。2. 我们探索了如何使机器人的语音适应物理和社会环境，以便在各种地点部署我们的语音。我们发现，在嘈杂和高能量环境中提高音高和音高率能使机器人的语音显得更恰当，并使其更具有对其当前环境的感知能力。3. 我们创建了一个英语TTS系统，通过利用L2听众难以理解的已知元音语言学特性，提高了L2听众的清晰度。我们使用数据驱动、基于感知的方法来理解L2说话者如何使用持续时间线索来解释英语中带有最小紧（长）和松（短）元音的挑战性单词。我们发现元音的持续时间强烈影响L2听众的感知，并为Matcha-TTS创建了一个“L2清晰模式”，该模式对紧元音进行延长，而保持松元音不变。我们的清晰模式被发现比基础Matcha-TTS更受尊重、更易懂、更具鼓励性，同时减少了这些挑战性紧/松元音最小对的转录错误。", "summary": "本文针对语言教学机器人缺乏任务特定合成语音的问题，提出了三项核心贡献。首先，开发了一种基于Matcha-TTS且通过表情符号提示的轻量级、富有表现力的机器人语音，该语音能实时运行并适合长时间表达。其次，研究了机器人语音如何适应不同环境，发现在嘈杂环境中提高音高和语速能增强语音的适应性。最后，为L2学习者设计了一个提高清晰度的英语TTS系统，通过调整紧元音的持续时间，显著提升了语音的可理解性、尊重感和鼓励性，并减少了特定语音对的转录错误。", "keywords": "自适应语音, 机器人语音, 语言教学, 人机交互, Matcha-TTS", "comments": "这篇论文解决了人机交互，特别是语言教学机器人领域的一个关键挑战，即如何生成更自然、适应性强且对L2学习者友好的合成语音。其创新点在于结合了表达力、环境适应性和L2清晰度优化，通过微调现有TTS模型并引入新的处理机制（如表情符号提示、音高调整和元音持续时间控制），实现了多维度的改进。这项工作对于提升语言教学机器人的用户体验和教学效果具有重要意义，也为未来人机语音交互系统的设计提供了有价值的参考。"}}
{"id": "2506.14970", "title": "NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for Multi-Modal Neurological Disorder Classification", "authors": ["Wajih Hassan Raza", "Aamir Bader Shah", "Yu Wen", "Yidan Shen", "Juan Diego Martinez Lemus", "Mya Caryn Schiess", "Timothy Michael Ellmore", "Renjie Hu", "Xin Fu"], "summary": "The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical\ndata holds great promise for enhancing the diagnosis of neurological disorders\n(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged\nas a powerful tool for extracting meaningful patterns from medical data to aid\nin diagnosis. However, existing DL approaches struggle to effectively leverage\nmulti-modal MRI and clinical data, leading to suboptimal performance.\n  To address this challenge, we utilize a unique, proprietary multi-modal\nclinical dataset curated for ND research. Based on this dataset, we propose a\nnovel transformer-based Mixture-of-Experts (MoE) framework for ND\nclassification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion\nTensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our\nframework employs transformer encoders to capture spatial relationships within\nvolumetric MRI data while utilizing modality-specific experts for targeted\nfeature extraction. A gating mechanism with adaptive fusion dynamically\nintegrates expert outputs, ensuring optimal predictive performance.\nComprehensive experiments and comparisons with multiple baselines demonstrate\nthat our multi-modal approach significantly enhances diagnostic accuracy,\nparticularly in distinguishing overlapping disease states. Our framework\nachieves a validation accuracy of 82.47\\%, outperforming baseline methods by\nover 10\\%, highlighting its potential to improve ND diagnosis by applying\nmulti-modal learning to real-world clinical data.", "comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14970v1", "AI": {"title_translation": "NeuroMoE：一种基于Transformer的专家混合框架，用于多模态神经疾病分类", "tldr": "NeuroMoE是一个基于Transformer的专家混合框架，结合多模态MRI和临床数据，显著提高了神经疾病的诊断准确性。", "motivation": "现有深度学习方法难以有效利用多模态MRI和临床数据进行神经疾病诊断，导致性能不佳。", "method": "提出了一种新颖的基于Transformer的专家混合（MoE）框架NeuroMoE，用于神经疾病分类。该框架利用解剖学MRI、弥散张量成像（DTI）和功能性MRI等多种MRI模态以及临床评估数据。它采用Transformer编码器捕获体积MRI数据中的空间关系，并使用模态特定的专家进行特征提取。通过自适应融合的门控机制动态整合专家输出，以确保最佳预测性能。", "result": "该多模态方法显著提高了诊断准确性，尤其是在区分重叠疾病状态方面。框架验证准确率达到82.47%，比基线方法高出10%以上。", "conclusion": "NeuroMoE框架通过将多模态学习应用于真实世界临床数据，显著提高了神经疾病诊断的潜力。", "translation": "多模态磁共振成像（MRI）与临床数据的整合在现实临床环境中极大地增强了神经疾病（NDs）诊断的潜力。深度学习（DL）最近已成为从医疗数据中提取有意义模式以辅助诊断的强大工具。然而，现有的DL方法难以有效利用多模态MRI和临床数据，导致性能不佳。为了解决这一挑战，我们利用了一个独特的、专有的多模态临床数据集，该数据集是为ND研究而整理的。基于该数据集，我们提出了一种新颖的基于Transformer的专家混合（MoE）框架，用于ND分类，该框架利用多种MRI模态——解剖学（aMRI）、弥散张量成像（DTI）和功能性（fMRI）——以及临床评估。我们的框架采用Transformer编码器来捕获体积MRI数据中的空间关系，同时利用模态特定的专家进行有针对性的特征提取。一个带有自适应融合的门控机制动态地整合专家输出，确保最佳预测性能。与多个基线的全面实验和比较表明，我们的多模态方法显著提高了诊断准确性，特别是在区分重叠疾病状态方面。我们的框架实现了82.47%的验证准确率，比基线方法高出10%以上，这凸显了其通过将多模态学习应用于真实世界临床数据来改善ND诊断的潜力。", "summary": "本研究提出了一种名为NeuroMoE的新型Transformer-based专家混合（MoE）框架，旨在解决现有深度学习方法在利用多模态MRI和临床数据进行神经疾病（NDs）诊断时面临的挑战。该框架整合了多种MRI模态（aMRI、DTI、fMRI）和临床评估数据，利用Transformer编码器捕获空间关系，并使用模态特定专家进行特征提取，通过自适应门控机制动态融合专家输出。实验结果表明，NeuroMoE显著提高了诊断准确性，尤其是在区分相似疾病状态方面，验证准确率达到82.47%，优于基线方法10%以上，展现了其在真实世界临床环境中改善ND诊断的巨大潜力。", "keywords": "神经疾病分类, 多模态MRI, 专家混合, Transformer, 深度学习", "comments": "该论文的创新点在于将Transformer架构与专家混合（MoE）模型相结合，有效地处理和融合多模态医疗数据，尤其是不同类型的MRI和临床数据。这种方法能够更精细地捕获不同模态的特征，并通过自适应门控机制进行动态融合，从而显著提高了神经疾病诊断的准确性。其在区分重叠疾病状态方面的表现尤为突出，对临床实践具有重要意义。"}}
{"id": "2506.15530", "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "summary": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15530v1", "AI": {"title_translation": "Diff-TONE：文本到音乐扩散模型中乐器编辑的时间步优化", "tldr": "本文提出一种在不额外训练的情况下，通过优化中间时间步，使用预训练文本到音乐扩散模型进行乐器编辑的方法，能在保留内容的同时改变音色。", "motivation": "现有的文本到音乐生成模型在控制生成过程以达到特定结果方面存在挑战，即使是微小的文本提示变化也可能大幅改变生成结果。本文旨在解决如何利用这些模型对现有音频轨道进行乐器编辑，同时保留其底层内容。", "method": "作者观察到模型在生成音频时，首先关注整体结构，然后添加乐器信息，最后细化质量。基于此，他们提出通过一个乐器分类器选择一个合适的中间时间步，在此时间步进行编辑，以平衡保留原始内容和实现所需音色。该方法不需要对文本到音乐扩散模型进行额外训练，也不影响生成速度。", "result": "通过选择一个合适的中间时间步，可以在保留原始乐曲内容和实现所需音色之间取得平衡。该方法无需额外训练，不牺牲生成速度。", "conclusion": "通过在文本到音乐扩散模型的生成过程中利用一个经过优化的中间时间步，可以有效地实现乐器编辑，同时保持原始音频的结构和内容，且无需额外的模型训练。", "translation": "文本到音乐生成模型的突破正在改变创意格局，为音乐家提供了前所未有的创新工具，用于作曲和实验。然而，控制生成过程以实现特定预期结果仍然是一个重大挑战。即使文本提示的微小变化，结合相同的随机种子，也可能大幅改变生成的作品。在本文中，我们探讨了现有文本到音乐扩散模型在乐器编辑方面的应用。具体来说，对于现有的音频轨道，我们的目标是利用预训练的文本到音乐扩散模型来编辑乐器，同时保留底层内容。基于模型首先关注音频的整体结构或内容，然后添加乐器信息，最后细化质量的洞察，我们展示了通过乐器分类器识别并选择一个精心选择的中间时间步，可以在保留原始作品内容和实现所需音色之间取得平衡。我们的方法不需要对文本到音乐扩散模型进行额外训练，也不会影响生成过程的速度。", "summary": "本文提出了一种名为Diff-TONE的新方法，用于在不额外训练文本到音乐扩散模型的情况下，对现有音频进行乐器编辑。通过观察扩散模型生成过程中的阶段性特点——先结构后乐器再细化，作者提出利用乐器分类器识别一个最佳中间时间步进行编辑。这种方法能够在保留原始音频内容的同时，有效改变其乐器音色，并且不影响生成速度。", "keywords": "文本到音乐,扩散模型,乐器编辑,时间步优化,内容保留", "comments": "这篇论文的创新点在于提出了一个无需重新训练预训练扩散模型即可进行乐器编辑的方法，这大大降低了应用门槛。通过利用扩散模型的内在生成机制（时间步的演进），识别关键的中间时间步进行干预，是一种巧妙且高效的策略。其重要性在于为音乐创作者提供了更精细的控制工具，有助于提高文本到音乐生成模型的实用性。"}}
{"id": "2506.15102", "title": "EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation", "authors": ["Shizhao Peng", "Shoumo Li", "Tianle Tao"], "summary": "Privacy-preserving neural network training in vertically partitioned\nscenarios is vital for secure collaborative modeling across institutions. This\npaper presents \\textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate\nSecure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale\noptimization for enhanced privacy and performance. To enable reliable\ncomputation under real-number domain, EVA-S2PMLP proposes a secure\ntransformation pipeline that maps scalar inputs to vector and matrix spaces\nwhile preserving correctness. The framework includes a suite of atomic\nprotocols for linear and non-linear secure computations, with modular support\nfor secure activation, matrix-vector operations, and loss evaluation.\nTheoretical analysis confirms the reliability, security, and asymptotic\ncomplexity of each protocol. Extensive experiments show that EVA-S2PMLP\nachieves high inference accuracy and significantly reduced communication\noverhead, with up to $12.3\\times$ improvement over baselines. Evaluation on\nbenchmark datasets demonstrates that the framework maintains model utility\nwhile ensuring strict data confidentiality, making it a practical solution for\nprivacy-preserving neural network training in finance, healthcare, and\ncross-organizational AI applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15102v1", "AI": {"title_translation": "EVA-S2PMLP：通过空间变换实现安全可扩展的两方MLP", "tldr": "EVA-S2PMLP是一个高效、可验证、准确的安全两方多层感知器框架，通过空间变换优化，在保护隐私的同时提升了性能，并显著降低了通信开销。", "motivation": "在垂直分区场景中，机构间安全协作建模的隐私保护神经网络训练至关重要。", "method": "EVA-S2PMLP引入了空间尺度优化，提出了一个安全的变换管道，将标量输入映射到向量和矩阵空间，同时保持正确性。该框架包含一套用于线性和非线性安全计算的原子协议，模块化支持安全激活、矩阵-向量操作和损失评估。", "result": "EVA-S2PMLP实现了高推理精度并显著降低了通信开销，比基线提高了12.3倍。在基准数据集上的评估表明，该框架在确保严格数据保密性的同时保持了模型效用。", "conclusion": "EVA-S2PMLP是金融、医疗保健和跨组织AI应用中隐私保护神经网络训练的实用解决方案。", "translation": "在垂直分区场景中，隐私保护神经网络训练对于机构间的安全协作建模至关重要。本文提出了EVA-S2PMLP，一个高效、可验证、准确的安全两方多层感知器框架，该框架引入了空间尺度优化以增强隐私和性能。为了在实数域下实现可靠计算，EVA-S2PMLP提出了一个安全的变换管道，将标量输入映射到向量和矩阵空间，同时保持正确性。该框架包含一套用于线性和非线性安全计算的原子协议，模块化支持安全激活、矩阵-向量操作和损失评估。理论分析证实了每个协议的可靠性、安全性和渐近复杂性。广泛的实验表明，EVA-S2PMLP实现了高推理精度并显著降低了通信开销，比基线提高了12.3倍。在基准数据集上的评估表明，该框架在确保严格数据保密性的同时保持了模型效用，使其成为金融、医疗保健和跨组织AI应用中隐私保护神经网络训练的实用解决方案。", "summary": "EVA-S2PMLP是一个用于垂直分区场景的安全两方多层感知器（MLP）训练框架。它通过引入空间尺度优化和安全的变换管道，将标量输入映射到向量和矩阵空间，从而在实数域下实现高效、可验证且准确的隐私保护计算。该框架包含一系列原子协议，支持安全激活、矩阵-向量操作和损失评估。实验证明，EVA-S2PMLP在保持模型效用的同时，显著提高了推理精度并降低了通信开销，比现有基线有显著改进，适用于金融、医疗保健等领域的隐私保护AI应用。", "keywords": "隐私保护, 神经网络, 多层感知器, 安全计算, 空间变换", "comments": "EVA-S2PMLP的创新之处在于其引入的空间尺度优化和安全的变换管道，这在隐私保护计算中提供了一种新颖的方法，特别是在实数域上的可靠计算。其在通信开销上的显著改进（高达12.3倍）显示了其实用价值和潜在影响，使其成为解决跨机构数据协作中隐私和性能平衡的关键技术。"}}
{"id": "2506.15085", "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "authors": ["Paige Tuttösí", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "summary": "Humans vary their expressivity when speaking for extended periods to maintain\nengagement with their listener. Although social robots tend to be deployed with\n``expressive'' joyful voices, they lack this long-term variation found in human\nspeech. Foundation model text-to-speech systems are beginning to mimic the\nexpressivity in human speech, but they are difficult to deploy offline on\nrobots. We present EmojiVoice, a free, customizable text-to-speech (TTS)\ntoolkit that allows social roboticists to build temporally variable, expressive\nspeech on social robots. We introduce emoji-prompting to allow fine-grained\ncontrol of expressivity on a phase level and use the lightweight Matcha-TTS\nbackbone to generate speech in real-time. We explore three case studies: (1) a\nscripted conversation with a robot assistant, (2) a storytelling robot, and (3)\nan autonomous speech-to-speech interactive agent. We found that using varied\nemoji prompting improved the perception and expressivity of speech over a long\nperiod in a storytelling task, but expressive voice was not preferred in the\nassistant use case.", "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 :\n  https://dl.acm.org/doi/10.5555/3721488.3721774", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15085v1", "AI": {"title_translation": "EmojiVoice：迈向机器人语音的长期可控表现力", "tldr": "EmojiVoice是一个免费、可定制的文本转语音工具包，它通过表情符号提示和轻量级骨干网络，使社交机器人能够生成具有长期可变表现力的语音，并在讲故事任务中表现良好，但在助手用例中未受青睐。", "motivation": "人类在长时间说话时会改变表达方式以保持听者的参与，但社交机器人缺乏这种长期变化。虽然基础模型文本转语音系统开始模仿人类语音的表现力，但它们难以在机器人上离线部署。", "method": "本文提出了EmojiVoice，一个免费、可定制的文本转语音（TTS）工具包，允许社交机器人专家构建随时间变化的、富有表现力的机器人语音。它引入了表情符号提示，以实现阶段级别的细粒度表现力控制，并使用轻量级的Matcha-TTS骨干网络实时生成语音。研究探索了三个案例研究：(1) 与机器人助手的脚本对话，(2) 讲故事机器人，和 (3) 自主语音到语音交互代理。", "result": "研究发现，在讲故事任务中，使用变化的表情符号提示可以提高语音的感知和长期表现力，但在助手用例中，富有表现力的语音并未受到青睐。", "conclusion": "EmojiVoice能够为社交机器人提供具有长期可变表现力的语音，并通过表情符号提示实现细粒度控制。然而，表现力语音的偏好因应用场景而异，例如在讲故事中受欢迎，但在助手任务中则不然。", "translation": "人类在长时间说话时会改变他们的表达方式，以保持与听者的互动。尽管社交机器人通常部署时带有“富有表现力”的快乐声音，但它们缺乏人类语音中发现的这种长期变化。基础模型文本转语音系统开始模仿人类语音的表现力，但它们很难在机器人上离线部署。我们提出了EmojiVoice，一个免费、可定制的文本转语音（TTS）工具包，允许社交机器人专家在社交机器人上构建随时间变化的、富有表现力的语音。我们引入了表情符号提示，以实现阶段级别的细粒度表现力控制，并使用轻量级的Matcha-TTS骨干网络实时生成语音。我们探索了三个案例研究：(1) 与机器人助手的脚本对话，(2) 讲故事机器人，和 (3) 自主语音到语音交互代理。我们发现，在讲故事任务中，使用变化的表情符号提示可以提高语音的感知和长期表现力，但在助手用例中，富有表现力的语音并未受到青睐。", "summary": "EmojiVoice是一个创新的文本转语音工具包，旨在解决社交机器人缺乏长期可变表达能力的问题。它通过引入表情符号提示实现对语音表现力的细粒度控制，并利用轻量级Matcha-TTS骨干网络进行实时语音生成，使其适合离线部署在机器人上。通过三项案例研究，该系统在讲故事任务中显著提升了语音的感知和表现力，证明了其在特定应用场景下的有效性，但也指出在其他场景（如机器人助手）中，丰富的表达可能并非最佳选择。", "keywords": "机器人语音, 文本转语音, 表现力控制, 表情符号提示, 社交机器人", "comments": "EmojiVoice的创新之处在于其通过“表情符号提示”实现对机器人语音表现力的细粒度、长期控制，这为社交机器人领域提供了一个实用的解决方案。其轻量级设计使其能够离线部署，解决了现有基础模型TTS系统在机器人应用中的一大障碍。然而，研究结果也提示，表现力并非在所有机器人交互场景中都受欢迎，这为未来的研究指明了方向，即需要根据具体应用场景来调整语音表现力策略。"}}
{"id": "2506.15655", "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15655v1", "AI": {"title_translation": "cAST：通过抽象语法树的结构化分块增强代码检索增强生成", "tldr": "cAST通过使用抽象语法树进行结构化分块，解决了现有基于行的分块方法在代码检索增强生成（RAG）中破坏语义结构的问题，显著提升了代码生成任务的性能。", "motivation": "检索增强生成（RAG）在代码生成中至关重要，但其分块过程（将文档划分为可检索单元）是一个关键但未充分探索的方面。现有的基于行的分块启发式方法经常破坏语义结构，分割函数或合并不相关的代码，从而降低生成质量。", "method": "我们提出了通过抽象语法树（cAST）进行分块的方法，这是一种结构感知的方法，它递归地将大型AST节点分解为更小的块，并在遵守大小限制的同时合并兄弟节点。这种方法生成跨编程语言和任务的自包含、语义连贯的单元。", "result": "cAST在RepoEval检索上将Recall@5提高了4.3个百分点，在SWE-bench生成上将Pass@1提高了2.67个百分点，证明了其在各种代码生成任务中的性能提升。", "conclusion": "我们的工作强调了结构感知分块对于扩展检索增强代码智能的重要性。", "translation": "检索增强生成（RAG）已成为大规模代码生成的关键，通过将预测基于外部代码语料库来提高真实性。然而，RAG管道中一个关键但未充分探索的方面是分块——将文档划分为可检索单元的过程。现有的基于行的分块启发式方法经常破坏语义结构，分割函数或合并不相关的代码，这会降低生成质量。我们提出了通过抽象语法树（cAST）进行分块的方法，这是一种结构感知方法，它递归地将大型AST节点分解为更小的块，并在遵守大小限制的同时合并兄弟节点。这种方法生成跨编程语言和任务的自包含、语义连贯的单元，提高了各种代码生成任务的性能，例如在RepoEval检索上将Recall@5提高了4.3个百分点，在SWE-bench生成上将Pass@1提高了2.67个百分点。我们的工作强调了结构感知分块对于扩展检索增强代码智能的重要性。", "summary": "本文提出了cAST，一种利用抽象语法树（AST）进行结构化分块的方法，旨在解决代码检索增强生成（RAG）中现有基于行的分块策略破坏语义结构的问题。cAST递归地将大型AST节点分解并合并兄弟节点，以创建自包含、语义连贯的代码单元。实验结果表明，cAST显著提升了代码检索和生成的性能，例如在RepoEval上Recall@5提升了4.3点，在SWE-bench上Pass@1提升了2.67点，强调了结构感知分块在代码智能中的重要性。", "keywords": "代码检索增强生成, 抽象语法树, 结构化分块, 代码生成, RAG", "comments": "cAST的创新之处在于其将抽象语法树引入到RAG的分块过程中，这是一种对代码结构有深刻理解的方法。与传统的基于行或字符的分块不同，cAST能够生成语义上更连贯、更完整的分块单元，从而有效提升了代码检索和生成的质量。这对于大规模代码模型在实际应用中的落地具有重要意义，尤其是在需要高精度代码片段的场景下。"}}
{"id": "2506.15467", "title": "Towards Weight Distribution-Aware Polar Codes", "authors": ["Mohammad Rowshan", "Vlad-Florin Dragoi"], "summary": "Polar codes are constructed based on the reliability of sub-channels\nresulting from the polarization effect. However, this information-theoretic\nconstruction approach leads to a poor weight distribution. To address this\nissue, pre-transformed polar codes, such as CRC-polar codes and PAC codes, have\nbeen employed. In this paper, we focus on the structure of polar codes without\napplying any pre-transformations and explore methods, guided by the\nweight-contribution partial order, to design polar-like codes with enhanced\nweight distribution, notably without employing any search or optimization\nalgorithms. Numerical results demonstrate improvement over a range of codes\nboth with and without pre-transformation.", "comment": "Accepted and to be presented at IEEE ISIT'25", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15467v1", "AI": {"title_translation": "面向权重分布感知的极化码", "tldr": "本文提出了一种基于权重贡献偏序的新方法，用于设计具有更好权重分布的类极化码，无需搜索或优化算法，并显示出性能提升。", "motivation": "极化码的信息理论构造方法导致其权重分布较差。尽管存在CRC-极化码和PAC码等预变换极化码来解决此问题，但本文旨在不应用任何预变换的情况下，改进原始极化码的权重分布。", "method": "本文关注未进行任何预变换的极化码结构，并探索在权重贡献偏序指导下的方法，以设计具有增强权重分布的类极化码，特别是不采用任何搜索或优化算法。", "result": "数值结果表明，与一系列有或没有预变换的码相比，所提出的方法设计的码在性能上有所提高。", "conclusion": "通过利用权重贡献偏序，可以设计出具有增强权重分布的类极化码，且无需复杂的搜索或优化算法，从而实现更好的性能。", "translation": "极化码是基于极化效应产生的子信道可靠性构建的。然而，这种信息理论的构建方法导致了较差的权重分布。为了解决这个问题，已经采用了预变换的极化码，例如CRC-极化码和PAC码。在本文中，我们关注不应用任何预变换的极化码结构，并探索在权重贡献偏序指导下的方法，以设计具有增强权重分布的类极化码，值得注意的是，这不采用任何搜索或优化算法。数值结果表明，与一系列有或没有预变换的码相比，性能有所提高。", "summary": "本文旨在解决极化码因其信息理论构造导致的权重分布较差的问题。作者提出了一种新颖的方法，在不进行任何预变换的情况下，通过权重贡献偏序来指导设计类极化码，以增强其权重分布。值得注意的是，此方法不依赖于任何搜索或优化算法。数值结果证明，与现有有或无预变换的码相比，新设计的码在性能上有所改进。", "keywords": "极化码, 权重分布, 权重贡献偏序, 编码设计, 信息论", "comments": "本文的创新之处在于，它提出了一种不依赖于复杂搜索或优化算法，而是通过结构化方法（权重贡献偏序）来改善极化码权重分布的设计方案。这对于简化极化码的设计过程并提高其性能具有重要意义。"}}
{"id": "2506.15461", "title": "All is Not Lost: LLM Recovery without Checkpoints", "authors": ["Nikolay Blagoev", "Oğuzhan Ersoy", "Lydia Yiyu Chen"], "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15461v1", "AI": {"title_translation": "并非一切都已失去：无需检查点的LLM恢复", "tldr": "本文提出CheckFree及其扩展CheckFree+，一种无需检查点即可恢复LLM训练中节点故障的有效方法，通过邻近阶段平均或乱序执行来降低去中心化训练成本。", "motivation": "在去中心化和计算资源有限的节点上训练大型语言模型（LLM）可以降低成本并促进模型民主化。然而，节点故障和调度策略导致的节点流失是不可避免的挑战，会导致模型部分（阶段）的丢失。传统的恢复方法，如检查点（定期将整个模型复制到额外存储）或冗余计算，即使在无故障情况下也会产生显著的通信和/或计算开销，并且在大模型设置下扩展性差。", "method": "本文提出了CheckFree，一种高效的恢复方法，通过将失败的阶段替换为最近邻近阶段的加权平均值，无需额外的计算或存储，但只能恢复中间阶段的故障。为了解决首尾阶段的崩溃，进一步将方法扩展为CheckFree+，通过乱序流水线执行来模仿这些阶段的行为，从而可以通过简单地复制相邻阶段的权重来恢复。为了恢复（解）嵌入层，CheckFree+将这些层复制到相邻阶段，这只需要相对较小的存储开销。", "result": "在模型大小从124M到1.5B的LLaMa模型上，以及不同的故障频率下对该方法进行了广泛评估。在低和中等故障率（5-10%）的情况下，CheckFree和CheckFree+在挂钟时间收敛方面均优于检查点和冗余计算超过12%。", "conclusion": "CheckFree和CheckFree+为去中心化LLM训练提供了一种无需传统检查点和额外计算/存储开销的高效故障恢复机制，显著提高了训练的鲁棒性和成本效益。", "translation": "在去中心化和计算资源有限的节点上训练大型语言模型（LLM），例如多个即时实例，可以降低训练成本并实现模型民主化。这里不可避免的挑战是由于故障和操作员调度策略导致的节点流失，从而导致模型一部分（阶段）的丢失。传统的故障恢复方法是使用检查点，即定期将整个模型副本发送到额外的存储，或者进行冗余计算。这些方法即使在无故障情况下也会产生显著的通信和/或计算开销，并且在大模型设置下扩展性差。在本文中，我们提出了CheckFree，一种高效的恢复方法，其中失败的阶段被最近邻近阶段的加权平均值所取代。与现有技术相比，CheckFree不需要额外的计算或存储。然而，由于对邻近阶段进行平均的性质，它只能恢复中间阶段的故障。我们进一步将我们的方法扩展到CheckFree+，通过乱序流水线执行来容忍第一阶段和最后阶段的崩溃。由于乱序流水线，这些阶段的行为由其邻近阶段模仿，这使得CheckFree+可以通过简单地复制相邻阶段的权重来恢复它们。为了能够恢复（解）嵌入层，CheckFree+将这些层复制到相邻阶段，这需要相对较小的存储开销。我们对模型大小从124M到1.5B的LLaMa模型以及不同的故障频率进行了广泛评估。在低和中等故障率（5-10%）的情况下，CheckFree和CheckFree+在挂钟时间收敛方面均优于检查点和冗余计算超过12%。我们的两项提案都可以通过我们的代码运行，代码地址为：https://github.com/gensyn-ai/CheckFree。", "summary": "本文针对在去中心化节点上训练LLM时因节点故障导致的阶段丢失问题，提出了CheckFree和CheckFree+两种无需检查点的恢复方法。CheckFree通过对邻近阶段进行加权平均来恢复中间阶段，无需额外开销。CheckFree+则通过乱序流水线执行和少量嵌入层复制来处理首尾阶段的故障。实验表明，在低中等故障率下，CheckFree及其扩展方法在收敛速度上优于传统检查点和冗余计算超过12%，有效降低了去中心化LLM训练的成本和复杂性。", "keywords": "LLM恢复, 去中心化训练, 无检查点, 故障容忍, CheckFree", "comments": "本文提出了一种新颖的LLM故障恢复机制，其创新之处在于完全摆脱了传统检查点和冗余计算带来的高昂开销，转而利用模型阶段间的内在关联（邻近阶段平均）或巧妙的执行策略（乱序流水线）来实现恢复。这对于推动去中心化、低成本的LLM训练具有重要意义。然而，CheckFree+虽然解决了首尾阶段恢复问题，但仍需要为（解）嵌入层进行少量复制，这引入了微小的存储开销。该方法对于大规模去中心化训练的实用性值得关注。"}}
{"id": "2506.14833", "title": "Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices", "authors": ["Poojashree Chandrashekar Pankaj M Sajjanar"], "summary": "This paper describes a high-performance, low-latency video surveillance\nsystem designed for resource-constrained environments. We have proposed a\nformal entropy-based adaptive frame buffering algorithm and integrated that\nwith MobileNetV2 to achieve high throughput with low latency. The system is\ncapable of processing live streams of video with sub-50ms end-to-end inference\nlatency on resource-constrained devices (embedding platforms) such as Raspberry\nPi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection\naccuracy on standard datasets focused on video surveillance and exhibits\nrobustness to varying lighting, backgrounds, and speeds. A number of\ncomparative and ablation experiments validate the effectiveness of our design.\nFinally, our architecture is scalable, inexpensive, and compliant with stricter\ndata privacy regulations than common surveillance systems, so that the system\ncould coexist in a smart city or embedded security architecture.", "comment": "& pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14833v1", "AI": {"title_translation": "基于熵的自适应缓冲和MobileNetV2的边缘设备实时低延迟监控", "tldr": "该论文提出了一种结合熵基自适应缓冲和MobileNetV2的实时低延迟视频监控系统，适用于资源受限的边缘设备，实现了高吞吐量和低延迟，并保持高检测精度。", "motivation": "针对资源受限环境下高性能、低延迟视频监控系统的需求。", "method": "提出了一种形式化的基于熵的自适应帧缓冲算法，并将其与MobileNetV2集成。", "result": "在树莓派、亚马逊和NVIDIA Jetson Nano等资源受限设备上实现了低于50毫秒的端到端推理延迟。在视频监控标准数据集上保持超过92%的检测精度，并对不同的光照、背景和速度表现出鲁棒性。", "conclusion": "该架构具有可扩展性、成本效益高，并符合更严格的数据隐私法规，可与智慧城市或嵌入式安全架构共存。", "translation": "本文描述了一种为资源受限环境设计的高性能、低延迟视频监控系统。我们提出了一种形式化的基于熵的自适应帧缓冲算法，并将其与MobileNetV2集成，以实现高吞吐量和低延迟。该系统能够在树莓派、亚马逊和NVIDIA Jetson Nano等资源受限设备（嵌入式平台）上处理实时视频流，实现低于50毫秒的端到端推理延迟。我们的方法在专注于视频监控的标准数据集上保持超过92%的检测精度，并对不同的光照、背景和速度表现出鲁棒性。多项比较和消融实验验证了我们设计的有效性。最后，我们的架构具有可扩展性、成本低廉，并且比常见的监控系统更符合严格的数据隐私法规，因此该系统可以与智慧城市或嵌入式安全架构共存。", "summary": "本论文提出了一种针对资源受限环境的实时、低延迟视频监控系统。该系统结合了基于熵的自适应帧缓冲算法和MobileNetV2，能够在边缘设备上实现低于50毫秒的端到端推理延迟，同时在视频监控标准数据集上保持超过92%的检测精度。该系统对光照、背景和速度变化具有鲁棒性，并且具有可扩展性、成本效益高，符合数据隐私法规，适用于智慧城市和嵌入式安全架构。", "keywords": "视频监控, 边缘计算, 低延迟, MobileNetV2, 自适应缓冲", "comments": "该论文的创新点在于将基于熵的自适应缓冲技术与MobileNetV2相结合，为资源受限的边缘设备实现了高性能、低延迟的视频监控。其对数据隐私合规性的强调，使其在实际部署中更具吸引力。"}}
{"id": "2506.15384", "title": "Disruption of parkinsonian brain oscillations", "authors": ["Cédric Join", "Jakub Orłowski", "Antoine Chaillet", "Madeleine Lowery", "Hugues Mounier", "Michel Fliess"], "summary": "Deep brain stimulation (DBS) is an advanced surgical treatment for the\nsymptoms of Parkinson's disease (PD), involving electrical stimulation of\nneurons within the basal ganglia region of the brain. DBS is traditionally\ndelivered in an open-loop manner using fixed stimulation parameters, which may\nlead to suboptimal results. In an effort to overcome these limitations, closed\nloop DBS, using pathological subthalamic beta (13--30 Hz) activity as a\nfeedback signal, offers the potential to adapt DBS automatically in response to\nchanges in patient symptoms and side effects. However, clinically implemented\nclosed-loop techniques have been limited to date to simple control algorithms,\ndue to the inherent uncertainties in the dynamics involved. Model-free control,\nwhich has already seen successful applications in the field of bioengineering,\noffers a way to avoid this limitation and provides an alternative method to\napply modern control approach to selective suppression of pathological\noscillations.", "comment": "23rd Internat. Conf. Computational Methods in Systems Biology (CMSB\n  2025), 10-12 september 2025, Lyon, France", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15384v1", "AI": {"title_translation": "帕金森病脑振荡的扰动", "tldr": "帕金森病DBS治疗中，传统的开环DBS效果不佳，现有闭环DBS受限于简单算法。本文提出模型无关控制作为一种替代方法，以选择性抑制病理性脑振荡。", "motivation": "传统的帕金森病DBS治疗采用开环方式，参数固定，导致效果不佳。尽管闭环DBS有潜力根据患者症状自动调整，但目前的临床实现受限于简单的控制算法，无法有效处理固有的动态不确定性。", "method": "本文提出采用模型无关控制（Model-free control）作为一种替代方法，以避免现有闭环DBS的局限性，并提供一种应用现代控制方法选择性抑制病理性振荡的途径。该方法已在生物工程领域成功应用。", "result": "Not mentioned in abstract", "conclusion": "模型无关控制为克服当前闭环DBS的局限性提供了一种有前景的替代方案，有望实现对帕金森病理性脑振荡的选择性抑制。", "translation": "深部脑刺激（DBS）是治疗帕金森病（PD）症状的一种先进外科疗法，涉及对大脑基底神经节区域神经元的电刺激。DBS传统上以开环方式进行，使用固定的刺激参数，这可能导致效果不佳。为了克服这些局限性，使用病理性丘脑底核β（13-30 Hz）活动作为反馈信号的闭环DBS，有望根据患者症状和副作用的变化自动调整DBS。然而，由于所涉及动力学固有的不确定性，目前临床实施的闭环技术仅限于简单的控制算法。模型无关控制已在生物工程领域成功应用，它提供了一种避免这种局限性的方法，并提供了一种应用现代控制方法选择性抑制病理性振荡的替代方法。", "summary": "本研究探讨了帕金森病深部脑刺激（DBS）的优化。传统的开环DBS因参数固定而效果不佳，而现有闭环DBS虽有潜力，但因动态不确定性而受限于简单算法。文章提出模型无关控制作为一种新方法，旨在克服这些限制，实现对病理性脑振荡的选择性抑制。", "keywords": "深部脑刺激, 帕金森病, 闭环控制, 模型无关控制, 脑振荡", "comments": "本文的创新点在于提出将模型无关控制应用于DBS，以克服现有闭环DBS的局限性。这种方法有望通过更智能地抑制病理性振荡来提高帕金森病DBS的疗效。其重要性在于为帕金森病DBS的未来发展提供了一个新的研究方向，可能带来更个性化和有效的治疗方案。"}}
{"id": "2506.15624", "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": ["Lyle Goodyear", "Rachel Guo", "Ramesh Johari"], "summary": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.", "comment": "27 pages, 20 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15624v1", "AI": {"title_translation": "状态表示对动态路由博弈中LLM智能体行为的影响", "tldr": "大型语言模型（LLMs）在动态博弈中需要状态表示。本文提出了一个系统构建自然语言状态表示的框架，并发现概括的历史、遗憾信息和有限的他人行动信息能使LLM智能体在路由博弈中更接近博弈论均衡，并实现更稳定的博弈。", "motivation": "LLMs在动态环境中作为决策者具有潜力，但其无状态特性要求创建自然语言的历史表示。以往关于LLM智能体博弈的研究在编码博弈历史时采用了临时方法，这模糊了状态表示对智能体行为的影响并限制了研究之间的可比性。本文旨在解决这些问题。", "method": "本文提出了一个统一的框架，用于系统地构建自然语言“状态”表示，以便在重复的多智能体博弈中提示LLM智能体。该框架从动作信息度、奖励信息度、以及提示风格（自然语言压缩）三个维度描述状态表示方法。研究将此框架应用于一个动态自私路由博弈。", "result": "研究发现LLM智能体行为对自然语言状态表示存在关键依赖性。具体而言，提供（1）摘要而非完整的过去历史自然语言表示；（2）关于遗憾而非原始收益的信息；以及（3）关于他人动作的有限信息的表示，能使智能体行为更接近博弈论均衡预测，并实现更稳定的博弈。相反，其他表示可能导致与均衡的较大偏差、动态博弈随时间推移的更高变化，或两者兼有。", "conclusion": "本文得出结论，特定的自然语言状态表示（概括的历史、基于遗憾的信息、有限的他人行动信息）对于LLM智能体在动态博弈中实现与均衡匹配的稳定行为至关重要。", "translation": "大型语言模型（LLMs）在动态环境中作为决策者展现出潜力，但其无状态特性要求创建历史的自然语言表示。我们提出了一个统一的框架，用于系统地构建自然语言“状态”表示，以便在重复的多智能体博弈中提示LLM智能体。以往关于LLM智能体博弈的研究在编码博弈历史时采用了临时方法，这不仅模糊了状态表示对智能体行为的影响，而且限制了研究之间的可比性。我们的框架通过沿着三个轴线描述状态表示方法来解决这些空白：动作信息度（即状态表示捕获所玩动作的程度）；奖励信息度（即状态表示描述所获得奖励的程度）；以及提示风格（或自然语言压缩，即完整文本历史被总结的程度）。\n我们将此框架应用于一个动态自私路由博弈，选择该博弈是因为它在理论和人类受试者实验中都存在一个简单的均衡。尽管博弈相对简单，但我们发现LLM智能体行为对自然语言状态表示存在关键依赖性。特别是，我们观察到，向智能体提供（1）摘要而非完整的过去历史自然语言表示；（2）关于遗憾而非原始收益的信息；以及（3）关于他人动作的有限信息，能够使行为更接近博弈论均衡预测，并使智能体博弈更加稳定。相比之下，其他表示可能表现出与均衡的较大偏差、动态博弈随时间推移的更高变化，或两者兼有。", "summary": "本文提出了一个系统框架，用于在多智能体博弈中为大型语言模型（LLM）智能体创建自然语言状态表示，解决了以往临时方法的局限性。该框架从动作信息度、奖励信息度和提示风格三个方面刻画状态表示。研究将其应用于一个动态自私路由博弈，结果表明，具体的状表示——即概括的历史、关于遗憾的信息以及关于他人行动的有限信息——显著提高了LLM智能体匹配博弈论均衡预测的能力，并实现了更稳定的博弈行为，这与导致偏差或不稳定性的其他表示形成对比。", "keywords": "LLM智能体, 状态表示, 动态博弈, 多智能体系统, 博弈论, 自私路由", "comments": "本文具有创新性，因为它为LLM智能体提供了系统化的状态表示框架，这对于LLMs在动态环境中的无状态特性至关重要。它超越了临时方法，有助于更好地理解和比较LLM智能体的行为。研究结果为在博弈环境中设计有效的LLM智能体提示提供了实用指导，强调了信息压缩和关注如遗憾等相关指标的重要性。"}}
{"id": "2506.15338", "title": "Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry", "authors": ["Islam M. Tanash", "Ayush Kumar Dwivedi", "Taneli Riihonen"], "summary": "This paper studies a high-altitude platform (HAP) network supported by\nreconfigurable intelligent surfaces (RISs). The practical irregular placement\nof HAPs and RISs is modeled using homogeneous Poisson point processes, while\nbuildings that cause blockages in urban areas are modeled as a Boolean scheme\nof rectangles. We introduce a novel approach to characterize the statistical\nchannel based on generalized Beta prime distribution. Analytical expressions\nfor coverage probability and ergodic capacity in an interference-limited system\nare derived and validated through Monte Carlo simulations. The findings show\nnotable performance improvements and reveal the impact of various system\nparameters, including blockages effect which contribute in mitigating\ninterference from the other visible HAPs. This proposed system could enhance\nconnectivity and enable effective data offloading in urban environments.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15338v1", "AI": {"title_translation": "城市RIS辅助HAP网络：基于随机几何的性能分析", "tldr": "本文研究了RIS辅助的HAP网络，使用随机几何建模，并推导了覆盖概率和遍历容量的解析表达式，结果显示性能显著提升，并揭示了系统参数（包括阻塞效应）的影响。", "motivation": "研究RIS辅助的高空平台(HAP)网络，以增强城市环境中的连接性并实现有效的数据分流。", "method": "1. 使用齐次泊松点过程（HPPP）建模HAP和RIS的非规则部署。2. 使用矩形布尔方案建模城市建筑造成的阻塞。3. 引入基于广义Beta素数分布的新方法来表征统计信道。4. 推导干扰受限系统中覆盖概率和遍历容量的解析表达式。5. 通过蒙特卡洛模拟验证结果。", "result": "1. 系统性能有显著提升。2. 揭示了各种系统参数的影响。3. 阻塞效应有助于减轻来自其他可见HAP的干扰。", "conclusion": "提出的RIS辅助HAP系统可以增强城市环境中的连接性并实现有效的数据分流。", "translation": "本文研究了由可重构智能表面（RIS）支持的高空平台（HAP）网络。HAP和RIS实际不规则的部署使用齐次泊松点过程建模，而城市区域中导致阻塞的建筑物则建模为矩形的布尔方案。我们引入了一种基于广义Beta素数分布的新方法来表征统计信道。在干扰受限系统中，推导了覆盖概率和遍历容量的解析表达式，并通过蒙特卡洛模拟进行了验证。研究结果显示了显著的性能提升，并揭示了各种系统参数的影响，包括有助于减轻来自其他可见HAP干扰的阻塞效应。所提出的系统可以增强城市环境中的连接性并实现有效的数据分流。", "summary": "本文探讨了在城市环境中RIS辅助的高空平台（HAP）网络的性能。研究采用随机几何方法，将HAP和RIS的部署建模为齐次泊松点过程，并将建筑物阻塞建模为矩形布尔方案。文章引入了一种基于广义Beta素数分布的新颖信道表征方法，并推导了干扰受限系统中覆盖概率和遍历容量的解析表达式，并通过仿真验证。结果表明，该系统能显著提升性能，并揭示了包括阻塞效应在内的多种系统参数对干扰缓解和整体性能的影响，从而增强城市连接性并实现高效数据分流。", "keywords": "RIS, HAP网络, 随机几何, 覆盖概率, 遍历容量", "comments": "创新性：引入广义Beta素数分布来表征统计信道，并结合随机几何对复杂的城市无线环境进行建模。重要性：为城市环境中RIS辅助HAP网络的性能分析提供了理论基础和实用见解，有助于未来6G通信网络的部署。局限性：未提及具体局限性，但这类理论研究通常需要进一步的实际部署和大规模实验验证。"}}
{"id": "2506.14790", "title": "Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting", "authors": ["Tianxiang Zhan", "Ming Jin", "Yuanpeng He", "Yuxuan Liang", "Yong Deng", "Shirui Pan"], "summary": "Recurring concept drift, a type of concept drift in which previously observed\ndata patterns reappear after some time, is one of the most prevalent types of\nconcept drift in time series. As time progresses, concept drift occurs and\npreviously encountered concepts are forgotten, thereby leading to a decline in\nthe accuracy of online predictions. Existing solutions employ parameter\nupdating techniques to delay forgetting; however, this may result in the loss\nof some previously learned knowledge while neglecting the exploration of\nknowledge retention mechanisms. To retain all conceptual knowledge and fully\nutilize it when the concepts recur, we propose the Continuous Evolution Pool\n(CEP), a pooling mechanism that stores different instances of forecasters for\ndifferent concepts. Our method first selects the forecaster nearest to the test\nsample and then learns the features from its neighboring samples - a process we\nrefer to as the retrieval. If there are insufficient neighboring samples, it\nindicates that a new concept has emerged, and a new model will evolve from the\ncurrent nearest sample to the pool to store the knowledge of the concept.\nSimultaneously, the elimination mechanism will enable outdated knowledge to be\ncleared to ensure the prediction effect of the forecasters. Experiments on\ndifferent architectural models and eight real datasets demonstrate that CEP\neffectively retains the knowledge of different concepts. In the scenario of\nonline forecasting with recurring concepts, CEP significantly enhances the\nprediction results.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14790v1", "AI": {"title_translation": "持续演化池：驯服在线时间序列预测中的循环概念漂移", "tldr": "该论文提出了持续演化池（CEP），一种池化机制，用于通过存储和重用不同概念的知识来处理时间序列预测中循环概念漂移，从而提高在线预测的准确性。", "motivation": "循环概念漂移是时间序列中最普遍的概念漂移类型之一，导致在线预测的准确性下降，因为之前遇到的概念被遗忘。现有解决方案通过参数更新技术来延迟遗忘，但这可能导致一些先前学习的知识丢失，并忽视知识保留机制的探索。因此，需要保留所有概念知识并在概念再次出现时充分利用它们。", "method": "作者提出了持续演化池（CEP），这是一种池化机制，用于存储不同概念的预测器实例。该方法首先选择最接近测试样本的预测器，然后从其邻近样本中学习特征（称为检索过程）。如果邻近样本不足，则表示出现了新概念，此时将从当前最近样本中演化出一个新模型并加入到池中以存储该概念的知识。同时，消除机制会清除过时知识，以确保预测器的预测效果。", "result": "在不同架构模型和八个真实数据集上的实验表明，CEP有效地保留了不同概念的知识。在具有循环概念的在线预测场景中，CEP显著提高了预测结果。", "conclusion": "持续演化池（CEP）通过有效地保留概念知识，显著增强了在线时间序列预测中面对循环概念漂移时的预测性能。", "translation": "循环概念漂移是一种概念漂移，其中先前观察到的数据模式在一段时间后重新出现，是时间序列中最普遍的概念漂移类型之一。随着时间的推移，概念漂移发生，先前遇到的概念被遗忘，从而导致在线预测准确性下降。现有解决方案采用参数更新技术来延迟遗忘；然而，这可能导致一些先前学习的知识丢失，同时忽视了知识保留机制的探索。为了保留所有概念知识并在概念再次出现时充分利用它们，我们提出了持续演化池（CEP），这是一种存储不同概念预测器实例的池化机制。我们的方法首先选择最接近测试样本的预测器，然后从其邻近样本中学习特征——我们称之为检索过程。如果邻近样本不足，则表明出现了新概念，此时将从当前最近样本中演化出一个新模型并加入到池中以存储该概念的知识。同时，消除机制将使过时知识被清除，以确保预测器的预测效果。在不同架构模型和八个真实数据集上的实验表明，CEP有效地保留了不同概念的知识。在具有循环概念的在线预测场景中，CEP显著提高了预测结果。", "summary": "该论文引入了持续演化池（CEP），旨在解决在线时间序列预测中循环概念漂移的问题，即先前学习的模式会重新出现但常被遗忘，导致准确性下降。与仅延迟遗忘的方法不同，CEP维护一个预测器池，每个预测器对应一个特定概念。它能为新数据检索最相关的预测器，为新出现的概念演化出新模型，并清除过时知识。在各种模型和真实数据集上的实验结果证实，CEP在保留概念知识方面是有效的，并显著提高了循环概念漂移场景下的预测性能。", "keywords": "循环概念漂移, 在线时间序列预测, 知识保留, 持续演化池, 预测器池", "comments": "该论文提出的持续演化池（CEP）在处理循环概念漂移方面具有创新性，通过显式的池化机制来存储和重用不同概念的知识，克服了现有方法知识丢失的问题。其处理新概念出现和清除过时知识的机制结构良好。这项研究对于在动态环境中保持在线时间序列预测的高精度具有重要的实际意义。"}}
{"id": "2506.15076", "title": "Learning-Time Encoding Shapes Unlearning in LLMs", "authors": ["Ruihan Wu", "Konstantin Garov", "Kamalika Chaudhuri"], "summary": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15076v1", "AI": {"title_translation": "学习时编码影响大型语言模型中的遗忘能力", "tldr": "本文实证研究了学习时知识编码选择如何影响大型语言模型中事实知识的遗忘效果，发现使用释义描述进行学习可以提高遗忘性能，但从文本块中遗忘单个知识具有挑战性，表明学习时编码在可靠的后验遗忘中起着核心作用。", "motivation": "随着大型语言模型（LLMs）在现实世界中日益普及，事后“遗忘”或删除特定知识的能力变得至关重要，原因包括隐私法规、纠正过时或有害内容。以往的工作提出了遗忘基准和算法，通常假设训练过程和目标模型是固定的。本文旨在调查学习时知识编码的选择如何影响事实知识的遗忘效果。", "method": "本文通过实证研究，进行实验来调查学习时知识编码选择对事实知识遗忘效果的影响。", "result": "实验揭示了两个关键发现：1）使用释义描述进行学习可以提高遗忘性能；2）从文本块中遗忘单个知识是具有挑战性的。", "conclusion": "本文结果表明，学习时的知识编码可能在实现可靠的后验遗忘中发挥核心作用。", "translation": "随着大型语言模型（LLMs）在现实世界中日益部署，“遗忘”或事后删除特定知识的能力已变得至关重要，原因从隐私法规到纠正过时或有害内容不等。先前的工作提出了遗忘基准和算法，并且通常假设训练过程和目标模型是固定的。在这项工作中，我们实证研究了学习时知识编码的选择如何影响事实知识的遗忘效果。我们的实验揭示了两个关键发现：(1) 使用释义描述进行学习可以提高遗忘性能，以及 (2) 从文本块中遗忘单个知识是具有挑战性的。我们的结果表明，学习时的知识编码可能在实现可靠的后验遗忘中发挥核心作用。", "summary": "本研究探讨了大型语言模型中学习时知识编码如何影响事实知识的遗忘能力。与以往工作不同，本文关注训练过程中的编码选择。实验发现，使用释义描述进行学习有助于提高遗忘性能，但从大段文本中删除特定知识则较为困难。研究结果强调了学习时知识编码在实现可靠的后验遗忘中的重要性。", "keywords": "大型语言模型, 知识遗忘, 学习时编码, 事实知识, 释义学习", "comments": "本文的创新之处在于它将研究焦点从后验遗忘算法本身转移到模型学习阶段的知识编码方式，揭示了学习时决策对遗忘效果的关键影响。这为未来设计更易于遗忘的LLM模型提供了新的视角和研究方向，具有重要的实践意义。"}}
{"id": "2506.14916", "title": "Interpolation-based reproducing kernel particle method", "authors": ["Jennifer E. Fromm", "John A. Evans", "J. S. Chen"], "summary": "Meshfree methods, including the reproducing kernel particle method (RKPM),\nhave been widely used within the computational mechanics community to model\nphysical phenomena in materials undergoing large deformations or extreme\ntopology changes. RKPM shape functions and their derivatives cannot be\naccurately integrated with the Gauss-quadrature methods widely employed for the\nfinite element method (FEM) and typically require sophisticated nodal\nintegration techniques, preventing them from easily being implemented in\nexisting FEM software. Interpolation-based methods have been developed to\naddress similar problems with isogeometric and immersed boundary methods,\nallowing these techniques to be implemented within open-source finite element\nsoftware. With interpolation-based methods, background basis functions are\nrepresented as linear combinations of Lagrange polynomial foreground basis\nfunctions defined upon a boundary-conforming foreground mesh. This work extends\nthe applications of interpolation-based methods to implement RKPM within\nopen-source finite element software. Interpolation-based RKPM is applied to\nseveral PDEs, and error convergence rates are equivalent to classic RKPM\nintegrated using high-order Gauss-quadrature schemes. The interpolation-based\nmethod is able to exploit the continuity of the RKPM basis to solve\nhigher-order PDEs, demonstrated through the biharmonic problem. The method is\nextended to multi-material problems through Heaviside enrichment schemes, using\nlocal foreground refinement to reduce geometric integration error and achieve\nhigh-order accuracy. The computational cost of interpolation-based RKPM is\nsimilar to the smoothed gradient nodal integration schemes, offering\nsignificant savings over Gauss-quadrature-based meshfree methods while enabling\neasy implementation within existing finite element software.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14916v1", "AI": {"title_translation": "基于插值的再生核粒子法", "tldr": "提出了一种基于插值的再生核粒子法（RKPM），解决了传统RKPM在有限元软件中难以集成的问题，并实现了高效且准确的计算。", "motivation": "传统的再生核粒子法（RKPM）形状函数及其导数难以使用有限元方法（FEM）中广泛采用的高斯积分法进行精确积分，通常需要复杂的节点积分技术，这阻碍了其在现有FEM软件中的便捷实现。", "method": "本研究将插值方法应用于再生核粒子法（RKPM）。通过将背景基函数表示为在符合边界的前景网格上定义的拉格朗日多项式前景基函数的线性组合，从而实现了RKPM在开源有限元软件中的实施。该方法还通过Heaviside富集方案扩展到多材料问题，并使用局部前景细化来减少几何积分误差。", "result": "插值RKPM应用于多个偏微分方程（PDEs），其误差收敛率与使用高阶高斯积分方案的经典RKPM相当。该方法能够利用RKPM基函数的连续性来解决高阶PDEs，并通过双调和问题进行了验证。在多材料问题中，通过Heaviside富集方案和局部前景细化，实现了高阶精度。插值RKPM的计算成本与平滑梯度节点积分方案相似，比基于高斯积分的无网格方法显著节省计算量，同时易于在现有有限元软件中实现。", "conclusion": "基于插值的再生核粒子法提供了一种有效且高效的解决方案，使得RKPM能够轻松集成到现有的有限元软件中，同时保持高精度和计算效率，并能处理高阶和多材料问题。", "translation": "无网格方法，包括再生核粒子法（RKPM），已广泛应用于计算力学领域，以模拟材料在大变形或极端拓扑变化下的物理现象。RKPM形状函数及其导数无法通过有限元方法（FEM）中广泛使用的高斯积分法进行精确积分，通常需要复杂的节点积分技术，这阻碍了其在现有FEM软件中的便捷实现。插值方法已被开发用于解决等几何和浸入边界方法中的类似问题，使得这些技术能够在开源有限元软件中实现。通过插值方法，背景基函数被表示为在符合边界的前景网格上定义的拉格朗日多项式前景基函数的线性组合。这项工作扩展了插值方法的应用，以在开源有限元软件中实现RKPM。插值RKPM应用于多个偏微分方程（PDEs），其误差收敛率与使用高阶高斯积分方案的经典RKPM相当。该插值方法能够利用RKPM基函数的连续性来解决高阶PDEs，通过双调和问题得到了证明。该方法通过Heaviside富集方案扩展到多材料问题，利用局部前景细化来减少几何积分误差并实现高阶精度。插值RKPM的计算成本与平滑梯度节点积分方案相似，与基于高斯积分的无网格方法相比显著节省了计算量，同时能够在现有有限元软件中轻松实现。", "summary": "本文提出了一种基于插值的再生核粒子法（RKPM），旨在解决传统RKPM在有限元软件中集成困难的问题。通过将背景基函数表示为前景网格上的拉格朗日多项式线性组合，该方法实现了RKPM在开源FEM软件中的无缝集成。研究表明，插值RKPM在多个偏微分方程上实现了与经典RKPM相当的误差收敛率，并能有效处理高阶偏微分方程和多材料问题。此外，其计算成本与现有高效方法相当，且显著优于基于高斯积分的无网格方法，为RKPM的实际应用提供了高效且易于实现的新途径。", "keywords": "再生核粒子法, 无网格方法, 插值方法, 有限元软件, PDE", "comments": "这项工作具有重要意义，因为它解决了再生核粒子法（RKPM）在现有有限元软件中实施的主要障碍，即积分难题。通过引入插值方法，它不仅提高了RKPM的易用性，还保持了其计算精度和效率，并扩展了其在处理高阶PDEs和多材料问题上的能力。这为RKPM的更广泛应用打开了大门，尤其是在需要与现有FEM框架兼容的工程和科学计算领域。"}}
{"id": "2506.15182", "title": "Classification of Multi-Parametric Body MRI Series Using Deep Learning", "authors": ["Boah Kim", "Tejas Sudharshan Mathai", "Kimberly Helm", "Peter A. Pinto", "Ronald M. Summers"], "summary": "Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15182v1", "AI": {"title_translation": "基于深度学习的多参数体部MRI序列分类", "tldr": "本研究开发并评估了一种基于深度学习的模型，用于自动分类多参数体部MRI序列，以解决DICOM头信息不准确的问题，提高放射科医生阅片效率。", "motivation": "多参数磁共振成像（mpMRI）检查的序列类型多样，且DICOM头信息常因协议多样性或技术人员错误而导致不准确，这降低了放射科医生阅片的效率。", "method": "研究提出了一种基于深度学习的分类模型，用于对8种不同的体部mpMRI序列类型进行分类。使用来自不同机构的mpMRI数据，训练并比较了ResNet、EfficientNet和DenseNet等多种深度学习分类器的性能。确定了表现最佳的分类器，并研究了其在不同训练数据量下的分类能力。此外，模型还在训练分布之外的数据集上进行了评估，并采用两种训练策略在不同扫描仪获取的mpMRI数据上进行了训练和测试。", "result": "实验结果显示，DenseNet-121模型在所有分类模型中表现最佳，F1-score和准确率分别达到0.966和0.972（p值<0.05）。当训练数据量超过729项研究时，模型准确率超过0.95，且性能随训练数据量的增加而提高。在DLDS和CPTAC-UCEC等外部数据集上，模型的准确率分别为0.872和0.810。", "conclusion": "DenseNet-121模型在内部和外部数据集上均能实现对8种体部MRI序列类型的高准确度分类。", "translation": "多参数磁共振成像（mpMRI）检查有多种序列类型，它们通过不同的成像协议获取。由于协议的多样性和偶尔的技术人员错误，这些序列的DICOM头信息通常不正确。为了解决这个问题，我们提出了一种基于深度学习的分类模型，用于分类8种不同的体部mpMRI序列类型，以便放射科医生高效地阅片。使用来自不同机构的mpMRI数据，我们训练了ResNet、EfficientNet和DenseNet等多种基于深度学习的分类器，以对8种不同的MRI序列进行分类，并比较了它们的性能。然后，我们确定了表现最佳的分类器，并研究了其在不同训练数据量设置下的分类能力。此外，模型还在训练分布之外的数据集上进行了评估。此外，模型使用从不同扫描仪获取的mpMRI检查数据以两种训练策略进行训练，并测试了其性能。实验结果表明，DenseNet-121模型在其他分类模型中取得了最高的F1-score和准确率，分别为0.966和0.972（p值<0.05）。当使用超过729项训练数据进行训练时，模型显示出高于0.95的准确率，其性能随着训练数据量的增加而提高。在DLDS和CPTAC-UCEC数据集的外部数据上，模型的准确率分别为0.872和0.810。这些结果表明，在内部和外部数据集上，DenseNet-121模型在分类8种体部MRI序列类型的任务中均取得了高准确率。", "summary": "本研究提出了一种基于深度学习的分类模型，旨在解决多参数体部MRI（mpMRI）序列DICOM头信息不准确导致阅片效率低下的问题。通过比较ResNet、EfficientNet和DenseNet等多种深度学习模型，研究发现DenseNet-121在分类8种不同体部mpMRI序列类型方面表现最佳，在内部数据集上F1-score和准确率分别达到0.966和0.972。该模型在训练数据量充足时表现出高准确性，并在外部数据集上验证了其泛化能力，证明了其在提高放射科医生阅片效率方面的潜力。", "keywords": "深度学习, MRI序列分类, 多参数MRI, DenseNet, DICOM", "comments": "该论文提出了一种实用的深度学习解决方案，旨在解决医疗影像数据管理中的一个常见问题——DICOM头信息错误。其创新性在于利用深度学习模型自动识别和分类MRI序列类型，从而提高放射科医生的工作效率。研究不仅比较了多种先进的深度学习架构，还探讨了数据量和外部数据集对模型性能的影响，这增强了研究结果的可靠性和实用性。该模型的成功应用有望减少人工干预，提高诊断流程的自动化和准确性。"}}
{"id": "2506.15548", "title": "Versatile Symbolic Music-for-Music Modeling via Function Alignment", "authors": ["Junyan Jiang", "Daniel Chin", "Liwei Lin", "Xuanjie Liu", "Gus Xia"], "summary": "Many music AI models learn a map between music content and human-defined\nlabels. However, many annotations, such as chords, can be naturally expressed\nwithin the music modality itself, e.g., as sequences of symbolic notes. This\nobservation enables both understanding tasks (e.g., chord recognition) and\nconditional generation tasks (e.g., chord-conditioned melody generation) to be\nunified under a music-for-music sequence modeling paradigm. In this work, we\npropose parameter-efficient solutions for a variety of symbolic music-for-music\ntasks. The high-level idea is that (1) we utilize a pretrained Language Model\n(LM) for both the reference and the target sequence and (2) we link these two\nLMs via a lightweight adapter. Experiments show that our method achieves\nsuperior performance among different tasks such as chord recognition, melody\ngeneration, and drum track generation. All demos, code and model weights are\npublicly available.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15548v1", "AI": {"title_translation": "通过功能对齐实现多功能符号音乐对音乐建模", "tldr": "本文提出了一种参数高效的符号音乐对音乐建模方法，通过使用预训练语言模型和轻量级适配器，统一了音乐理解和生成任务，并在多种任务上取得了优异性能。", "motivation": "许多音乐AI模型将音乐内容映射到人类定义的标签，但许多标注（如和弦）本身就可以在音乐模态内表达。这促使研究者将音乐理解和条件生成任务统一到“音乐对音乐”序列建模范式下，并寻求参数高效的解决方案。", "method": "该方法利用预训练语言模型（LM）处理参考序列和目标序列，并通过一个轻量级适配器将这两个LM连接起来。", "result": "实验表明，该方法在和弦识别、旋律生成和鼓点生成等不同任务中均取得了优异性能。", "conclusion": "该研究提出的参数高效的符号音乐对音乐建模方法，通过创新的架构设计，有效统一了音乐理解和生成任务，并在多项任务中展现出卓越的性能。", "translation": "许多音乐AI模型学习音乐内容与人类定义标签之间的映射。然而，许多标注，例如和弦，可以在音乐模态本身内自然表达，例如作为符号音符序列。这一观察使得理解任务（例如和弦识别）和条件生成任务（例如和弦条件旋律生成）能够在一个“音乐对音乐”序列建模范式下统一。在这项工作中，我们为各种符号音乐对音乐任务提出了参数高效的解决方案。核心思想是（1）我们对参考序列和目标序列都使用了预训练语言模型（LM），并且（2）我们通过一个轻量级适配器连接这两个LM。实验表明，我们的方法在和弦识别、旋律生成和鼓点生成等不同任务中均取得了优异性能。所有演示、代码和模型权重均已公开。", "summary": "该论文提出了一种通用的符号“音乐对音乐”建模框架，旨在将音乐理解和条件生成任务统一起来。核心方法是利用预训练语言模型处理音乐的参考和目标序列，并通过一个轻量级适配器进行连接，从而实现参数高效的解决方案。实验证明，该方法在和弦识别、旋律生成和鼓点生成等任务上表现出色。", "keywords": "符号音乐, 音乐对音乐建模, 语言模型, 适配器, 音乐AI", "comments": "该论文的创新点在于提出了“音乐对音乐”的建模范式，将传统的音乐内容到标签的映射扩展到音乐模态内部的映射。通过利用预训练语言模型和轻量级适配器，实现了参数高效且性能优越的解决方案，统一了理解和生成任务，具有重要的实用价值和研究意义。"}}
{"id": "2506.15112", "title": "PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning", "authors": ["Xiangman Li", "Xiaodong Wu", "Jianbing Ni", "Mohamed Mahmoud", "Maazen Alsabaan"], "summary": "Decentralized learning is vulnerable to poison attacks, where malicious\nclients manipulate local updates to degrade global model performance. Existing\ndefenses mainly detect and filter malicious models, aiming to prevent a limited\nnumber of attackers from corrupting the global model. However, restoring an\nalready compromised global model remains a challenge. A direct approach is to\nremove malicious clients and retrain the model using only the benign clients.\nYet, retraining is time-consuming, computationally expensive, and may\ncompromise model consistency and privacy.\n  We propose PDLRecover, a novel method to recover a poisoned global model\nefficiently by leveraging historical model information while preserving\nprivacy. The main challenge lies in protecting shared historical models while\nenabling parameter estimation for model recovery. By exploiting the linearity\nof approximate Hessian matrix computation, we apply secret sharing to protect\nhistorical updates, ensuring local models are not leaked during transmission or\nreconstruction. PDLRecover introduces client-side preparation, periodic\nrecovery updates, and a final exact update to ensure robustness and convergence\nof the recovered model. Periodic updates maintain accurate curvature\ninformation, and the final step ensures high-quality convergence. Experiments\nshow that the recovered global model achieves performance comparable to a fully\nretrained model but with significantly reduced computation and time cost.\nMoreover, PDLRecover effectively prevents leakage of local model parameters,\nensuring both accuracy and privacy in recovery.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15112v1", "AI": {"title_translation": "PDLRecover：隐私保护的去中心化模型恢复与机器遗忘", "tldr": "PDLRecover是一种新的方法，通过利用历史模型信息并保护隐私，高效地恢复受中毒的去中心化全局模型，其性能与完全重新训练的模型相当，但计算和时间成本显著降低。", "motivation": "去中心化学习容易受到投毒攻击，现有防御主要侧重于检测和过滤恶意模型，但恢复已经受损的全局模型仍然是一个挑战。直接的重训练方法耗时、计算昂贵且可能损害模型一致性和隐私。", "method": "本文提出了PDLRecover，一种利用历史模型信息并保护隐私来高效恢复中毒全局模型的方法。它通过利用近似Hessian矩阵计算的线性特性，应用秘密共享来保护历史更新，确保本地模型在传输或重建过程中不泄露。PDLRecover引入了客户端准备、周期性恢复更新和最终精确更新，以确保恢复模型的鲁棒性和收敛性。", "result": "实验表明，恢复后的全局模型性能与完全重训练的模型相当，但计算和时间成本显著降低。此外，PDLRecover有效防止了本地模型参数的泄露，确保了恢复的准确性和隐私。", "conclusion": "PDLRecover提供了一种高效且隐私保护的解决方案，用于恢复去中心化学习中受投毒攻击的全局模型，实现了与重训练相当的性能，同时显著降低了成本并保护了隐私。", "translation": "去中心化学习容易受到投毒攻击，恶意客户端会操纵本地更新以降低全局模型性能。现有防御主要检测和过滤恶意模型，旨在防止有限数量的攻击者破坏全局模型。然而，恢复已经受损的全局模型仍然是一个挑战。一种直接的方法是移除恶意客户端并仅使用良性客户端重新训练模型。然而，重新训练耗时、计算昂贵，并且可能损害模型一致性和隐私。\n我们提出了PDLRecover，一种通过利用历史模型信息同时保护隐私来高效恢复中毒全局模型的新方法。主要挑战在于保护共享历史模型，同时能够为模型恢复进行参数估计。通过利用近似Hessian矩阵计算的线性特性，我们应用秘密共享来保护历史更新，确保本地模型在传输或重建过程中不泄露。PDLRecover引入了客户端准备、周期性恢复更新和最终精确更新，以确保恢复模型的鲁棒性和收敛性。周期性更新保持准确的曲率信息，最后一步确保高质量收敛。实验表明，恢复后的全局模型性能与完全重新训练的模型相当，但计算和时间成本显著降低。此外，PDLRecover有效防止了本地模型参数的泄露，确保了恢复的准确性和隐私。", "summary": "PDLRecover提出了一种创新方法，旨在高效、隐私地恢复去中心化学习中受投毒攻击的全局模型。该方法通过利用历史模型信息并结合秘密共享技术保护隐私，避免了耗时且昂贵的模型重训练。PDLRecover通过客户端准备、周期性更新和最终精确更新确保恢复模型的鲁棒性、收敛性和准确性。实验证明，PDLRecover在显著降低计算和时间成本的同时，实现了与完全重训练模型相当的性能，并有效防止了本地模型参数泄露。", "keywords": "去中心化学习, 模型恢复, 隐私保护, 机器遗忘, 投毒攻击", "comments": "PDLRecover的创新点在于其结合了机器遗忘和秘密共享技术，解决了去中心化学习中模型被投毒后难以高效恢复的痛点，同时兼顾了隐私保护。其提出的利用历史模型信息进行恢复，并采用分阶段更新策略（周期性更新和最终精确更新）来确保模型质量和收敛性，具有重要的实践意义。该方法通过避免昂贵的完全重训练，为去中心化学习中的模型维护提供了一条高效且安全的新路径。"}}
{"id": "2506.15087", "title": "3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception", "authors": ["Yuankai Lin", "Xiaofan Lu", "Jiahui Chen", "Hua Yang"], "summary": "To achieve human-like haptic perception in anthropomorphic grippers, the\ncompliant sensing surfaces of vision tactile sensor (VTS) must evolve from\nconventional planar configurations to biomimetically curved topographies with\ncontinuous surface gradients. However, planar VTSs have challenges when\nextended to curved surfaces, including insufficient lighting of surfaces,\nblurring in reconstruction, and complex spatial boundary conditions for surface\nstructures. With an end goal of constructing a human-like fingertip, our\nresearch (i) develops GelSplitter3D by expanding imaging channels with a prism\nand a near-infrared (NIR) camera, (ii) proposes a photometric stereo neural\nnetwork with a CAD-based normal ground truth generation method to calibrate\ntactile geometry, and (iii) devises a normal integration method with boundary\nconstraints of depth prior information to correcting the cumulative error of\nsurface integrals. We demonstrate better tactile sensing performance, a 40$\\%$\nimprovement in normal estimation accuracy, and the benefits of sensor shapes in\ngrasping and manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15087v1", "AI": {"title_translation": "基于红外和可见光图像的机器人精细触觉感知三维视觉-触觉重建", "tldr": "本研究通过扩展成像通道、提出光度立体神经网络和法线积分方法，实现了曲面视觉触觉传感器的三维重建，提高了触觉感知性能和法线估计精度，旨在构建类人指尖。", "motivation": "为了在拟人化夹持器中实现类人触觉感知，视觉触觉传感器（VTS）的柔性传感表面必须从传统的平面配置发展到具有连续表面梯度的仿生曲面形貌。然而，平面VTS在扩展到曲面时面临光照不足、重建模糊和复杂空间边界条件等挑战。", "method": "本研究（i）通过使用棱镜和近红外（NIR）相机扩展成像通道，开发了GelSplitter3D；（ii）提出了一种带有基于CAD的法线真值生成方法的光度立体神经网络，用于校准触觉几何；（iii）设计了一种具有深度先验信息边界约束的法线积分方法，用于校正表面积分的累积误差。", "result": "研究展示了更好的触觉传感性能，法线估计精度提高了40%，并证明了传感器形状在抓取和操作任务中的优势。", "conclusion": "所开发的方法和传感器设计通过在曲面上实现精确的三维视觉-触觉重建，提高了触觉传感和法线估计能力，从而有助于实现类人触觉感知，推动机器人操作技术的发展。", "translation": "为了在拟人化夹持器中实现类人触觉感知，视觉触觉传感器（VTS）的柔性传感表面必须从传统的平面配置发展到具有连续表面梯度的仿生曲面形貌。然而，平面VTS在扩展到曲面时面临挑战，包括表面光照不足、重建模糊以及表面结构的复杂空间边界条件。为了最终构建一个类人指尖，我们的研究（i）通过使用棱镜和近红外（NIR）相机扩展成像通道，开发了GelSplitter3D；（ii）提出了一种带有基于CAD的法线真值生成方法的光度立体神经网络，用于校准触觉几何；（iii）设计了一种具有深度先验信息边界约束的法线积分方法，用于校正表面积分的累积误差。我们展示了更好的触觉传感性能，法线估计精度提高了40%，以及传感器形状在抓取和操作任务中的优势。", "summary": "该论文旨在解决平面视觉触觉传感器（VTS）在应用于曲面时为实现机器人类人触觉感知所面临的局限性。研究引入了GelSplitter3D，这是一种利用棱镜和近红外（NIR）相机扩展成像通道的新型传感器设计。该研究还提出了一种带有基于CAD真值的光度立体神经网络用于触觉几何校准，以及一种法线积分方法来纠正重建误差。实验结果表明，触觉传感性能得到改善，法线估计精度提高了40%，并且传感器的曲面形状在抓取和操作任务中显示出优势，使其更接近构建类人机器人指尖的目标。", "keywords": "三维视觉-触觉重建, 机器人感知, 曲面, 光度立体, GelSplitter3D", "comments": "该论文提出了一种创新方法，克服了曲面三维视觉-触觉重建的挑战，这对于先进的机器人操作至关重要。红外和可见光的结合，加上专门的神经网络和误差校正方法，代表着向更类人机器人触觉感知迈出的重要一步。法线估计精度提高40%尤其值得关注，突显了其所提出方法的有效性。"}}
{"id": "2506.15648", "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15648v1", "AI": {"title_translation": "deepSURF：通过模糊测试LLM增强的测试工具检测Rust中的内存安全漏洞", "tldr": "deepSURF是一个新工具，它结合了静态分析和LLM引导的模糊测试，用于在Rust的unsafe代码中检测内存安全漏洞。它通过处理泛型并动态增强模糊测试工具来提高检测能力，并在真实世界的Rust项目中重新发现了20个已知错误并发现了6个新漏洞。", "motivation": "尽管Rust默认保证内存安全，但其允许使用的unsafe代码若被滥用仍可能引入内存安全漏洞。现有Rust内存错误检测工具通常存在检测能力有限、对Rust特定类型处理不足或严重依赖人工干预等局限性。", "method": "deepSURF将静态分析与大型语言模型（LLM）引导的模糊测试工具生成相结合，以有效识别Rust库中的内存安全漏洞，特别是针对unsafe代码。它通过用自定义类型替换泛型并生成所需trait的定制实现来处理泛型。此外，deepSURF利用LLM动态增强模糊测试工具，促进复杂API交互的探索。", "result": "deepSURF在27个真实的Rust crate上进行了评估，成功重新发现了20个已知的内存安全错误，并揭示了6个以前未知的漏洞，表明比现有最先进的工具有了明显的改进。", "conclusion": "deepSURF在检测Rust中内存安全漏洞方面表现出显著改进，并优于现有最先进的工具，成功发现了已知和未知的漏洞。", "translation": "尽管Rust默认保证内存安全，但它也允许使用不安全代码，如果使用不当，可能会引入内存安全漏洞。不幸的是，现有的Rust内存错误检测工具通常表现出有限的检测能力，对Rust特定类型的处理不足，或者严重依赖人工干预。\n为了解决这些局限性，我们提出了deepSURF，一个将静态分析与大型语言模型（LLM）引导的模糊测试工具生成相结合的工具，以有效识别Rust库中的内存安全漏洞，特别是针对不安全代码。deepSURF引入了一种处理泛型的新方法，通过将其替换为自定义类型并生成所需特征的定制实现，使模糊器能够模拟模糊库中用户定义的行为。此外，deepSURF采用LLM动态增强模糊测试工具，促进复杂API交互的探索，并显著增加暴露内存安全漏洞的可能性。我们在27个真实的Rust crate上评估了deepSURF，成功重新发现了20个已知的内存安全错误，并揭示了6个以前未知的漏洞，表明比现有最先进的工具有了明显的改进。", "summary": "deepSURF是一种针对Rust语言中`unsafe`代码引入的内存安全漏洞的检测工具。它结合了静态分析和大型语言模型（LLM）引导的模糊测试工具生成技术，并通过创新性地处理泛型和动态增强模糊测试工具来提高检测能力。在27个实际Rust项目中，deepSURF成功重新发现了20个已知漏洞并发现了6个以前未知的漏洞，证明其优于现有技术。", "keywords": "Rust, 内存安全, 模糊测试, LLM, 漏洞检测", "comments": "deepSURF的创新之处在于其将LLM应用于模糊测试工具的生成和增强，以及其处理Rust泛型的方法，从而提高了对复杂API交互的探索能力。这项工作的重要性在于它能够有效地发现Rust中`unsafe`代码的内存安全漏洞，包括之前未知的漏洞，这对于提高Rust软件的安全性至关重要。"}}
{"id": "2506.15129", "title": "Data Verbalisation: What is Text Doing in a Data Visualisation?", "authors": ["Paul Murrell"], "summary": "This article discusses the role that text elements play in a data\nvisualisation. We argue that there is a need for a simple, coherent explanation\nof text elements similar to the understanding that already exists for non-text\nelements like bars, points, and lines. We explore examples of how text is used\nwithin a data visualisation and use existing knowledge and assessment\ntechniques to evaluate when text is effective and when it is not. The result is\na framework that aims to be easy to understand and easy to apply in order to\nunderstand the purpose and effectiveness of the text elements in any data\nvisualisation.", "comment": "43 pages (including appendix), 20 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15129v1", "AI": {"title_translation": "数据言语化：文本在数据可视化中扮演什么角色？", "tldr": "本文探讨了文本元素在数据可视化中的作用，并提出了一个框架，旨在解释和评估文本在数据可视化中的有效性。", "motivation": "现有对数据可视化中非文本元素（如条形图、点、线）的理解已经很成熟，但对文本元素的理解却缺乏简单、连贯的解释。因此，本文旨在填补这一空白，提供一个理解文本作用的框架。", "method": "文章通过探索文本在数据可视化中的使用示例，并运用现有知识和评估技术来判断文本何时有效、何时无效。", "result": "研究成果是一个框架，旨在易于理解和应用，以帮助理解任何数据可视化中文本元素的用途和有效性。", "conclusion": "本文提出的框架有助于用户理解和评估数据可视化中文本元素的有效性及其目的。", "translation": "本文讨论了文本元素在数据可视化中所扮演的角色。我们认为，需要对文本元素进行简单、连贯的解释，类似于对非文本元素（如条形图、点和线）已有的理解。我们探讨了文本在数据可视化中如何使用的例子，并利用现有知识和评估技术来评估文本何时有效、何时无效。结果是一个框架，旨在易于理解和应用，以便理解任何数据可视化中文本元素的用途和有效性。", "summary": "本文探讨了数据可视化中文本元素的作用，指出目前缺乏对文本元素的系统性理解。通过分析文本的使用案例并结合现有评估技术，文章提出了一个易于理解和应用的框架，旨在帮助用户理解和评估数据可视化中文本的目的和有效性。", "keywords": "数据可视化, 文本元素, 数据言语化, 框架, 有效性", "comments": "本文的创新之处在于提出了一个专门针对数据可视化中文本元素的框架，填补了该领域系统性理解的空白。该框架的重要性在于它能帮助设计者和用户更好地理解和利用文本，从而提高数据可视化的清晰度和有效性。"}}
{"id": "2506.15488", "title": "Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation", "authors": ["Hussam Al Daas", "Grey Ballard", "Laura Grigori", "Suraj Kumar", "Kathryn Rouse", "Mathieu Vérité"], "summary": "In this article, we focus on the parallel communication cost of multiplying\nthe same vector along two modes of a $3$-dimensional symmetric tensor. This is\na key computation in the higher-order power method for determining eigenpairs\nof a $3$-dimensional symmetric tensor and in gradient-based methods for\ncomputing a symmetric CP decomposition. We establish communication lower bounds\nthat determine how much data movement is required to perform the specified\ncomputation in parallel. The core idea of the proof relies on extending a key\ngeometric inequality for $3$-dimensional symmetric computations. We demonstrate\nthat the communication lower bounds are tight by presenting an optimal\nalgorithm where the data distribution is a natural extension of the triangle\nblock partition scheme for symmetric matrices to 3-dimensional symmetric\ntensors.", "comment": "19 pages, 1 figure", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15488v1", "AI": {"title_translation": "并行对称张量与相同向量乘法计算中的通信最小化", "tldr": "本文为三维对称张量与相同向量乘法计算的并行通信建立了紧密下界，并提出了一个最优算法。", "motivation": "这种计算是确定三维对称张量特征对的高阶幂方法以及计算对称CP分解的基于梯度方法中的一个关键计算。", "method": "建立了并行执行指定计算所需的数据移动量的通信下界。证明核心思想依赖于扩展三维对称计算的关键几何不等式。通过提出一种最优算法来证明通信下界的紧密性，该算法的数据分布是矩阵三角形块划分方案到三维对称张量的自然扩展。", "result": "建立了通信下界，并证明这些下界是紧密的，同时提出了一个达到这些下界的最优算法。", "conclusion": "成功建立了并行对称张量与相同向量乘法计算的通信下界，并设计了达到这些下界的最优算法，证明了其通信效率。", "translation": "在本文中，我们关注的是将同一个向量沿着一个三维对称张量的两个模式相乘的并行通信成本。这是确定三维对称张量特征对的高阶幂方法以及计算对称CP分解的基于梯度方法中的一个关键计算。我们建立了通信下界，这些下界决定了并行执行指定计算所需的数据移动量。证明的核心思想依赖于扩展三维对称计算的关键几何不等式。我们通过提出一种最优算法来证明通信下界的紧密性，其中数据分布是矩阵三角形块划分方案到三维对称张量的自然扩展。", "summary": "本文研究了三维对称张量与相同向量并行乘法的通信成本，该计算是高阶幂法和对称CP分解的关键部分。作者建立了并行计算的通信下界，并通过扩展几何不等式来证明。此外，论文还提出了一个数据分布基于扩展三角形块划分方案的最优算法，证明了所建立的通信下界是紧密的。", "keywords": "对称张量, 并行计算, 通信下界, 最优算法, CP分解", "comments": "本文的创新点在于为并行对称张量与相同向量乘法计算确定了理论上的通信下界，并通过提出一个达到这些下界的最优算法，解决了该关键计算的通信效率问题。其将矩阵的三角形块划分方案扩展到三维对称张量，为高效并行计算提供了新的思路。"}}
{"id": "2506.14835", "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "summary": "Precisely localizing 3D objects from a single image constitutes a central\nchallenge in monocular 3D detection. While DETR-like architectures offer a\npowerful paradigm, their direct application in this domain encounters inherent\nlimitations, preventing optimal performance. Our work addresses these\nchallenges by introducing MonoVQD, a novel framework designed to fundamentally\nadvance DETR-based monocular 3D detection. We propose three main contributions.\nFirst, we propose the Mask Separated Self-Attention mechanism that enables the\nintegration of the denoising process into a DETR architecture. This improves\nthe stability of Hungarian matching to achieve a consistent optimization\nobjective. Second, we present the Variational Query Denoising technique to\naddress the gradient vanishing problem of conventional denoising methods, which\nseverely restricts the efficiency of the denoising process. This explicitly\nintroduces stochastic properties to mitigate this fundamental limitation and\nunlock substantial performance gains. Finally, we introduce a sophisticated\nself-distillation strategy, leveraging insights from later decoder layers to\nsynergistically improve query quality in earlier layers, thereby amplifying the\niterative refinement process. Rigorous experimentation demonstrates that\nMonoVQD achieves superior performance on the challenging KITTI monocular\nbenchmark. Highlighting its broad applicability, MonoVQD's core components\nseamlessly integrate into other architectures, delivering significant\nperformance gains even in multi-view 3D detection scenarios on the nuScenes\ndataset and underscoring its robust generalization capabilities.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14835v1", "AI": {"title_translation": "MonoVQD：基于变分查询去噪和自蒸馏的单目3D目标检测", "tldr": "MonoVQD通过引入变分查询去噪和自蒸馏，改进了基于DETR的单目3D目标检测，解决了现有DETR架构的局限性，并在KITTI和nuScenes数据集上取得了卓越性能。", "motivation": "现有的DETR类架构在单目3D检测中存在固有局限性，导致性能无法达到最优，因此需要一个新颖的框架来推进基于DETR的单目3D检测。", "method": "MonoVQD提出了三项主要贡献：1. Mask Separated Self-Attention机制，将去噪过程整合到DETR架构中，提高匈牙利匹配的稳定性。2. 变分查询去噪（Variational Query Denoising）技术，解决传统去噪方法的梯度消失问题，并通过引入随机特性提升去噪效率和性能。3. 复杂的自蒸馏策略，利用后期解码器层的见解协同提高早期层的查询质量，从而放大迭代细化过程。", "result": "MonoVQD在具有挑战性的KITTI单目基准测试中取得了卓越性能。其核心组件可无缝集成到其他架构中，在nuScenes数据集的多视图3D检测场景中也实现了显著性能提升，显示出强大的泛化能力。", "conclusion": "MonoVQD通过其创新的Mask Separated Self-Attention、变分查询去噪和自蒸馏策略，显著提升了单目3D目标检测的性能和稳定性，并展现了其在多视图场景下的强大泛化能力和广泛适用性。", "translation": "从单一图像精确地定位3D物体是单目3D检测中的核心挑战。虽然DETR类架构提供了一个强大的范式，但它们在该领域的直接应用遇到了固有的局限性，阻碍了最佳性能。我们的工作通过引入MonoVQD来解决这些挑战，MonoVQD是一个旨在从根本上推进基于DETR的单目3D检测的新颖框架。我们提出了三个主要贡献。首先，我们提出了Mask Separated Self-Attention机制，它能够将去噪过程整合到DETR架构中。这提高了匈牙利匹配的稳定性，以实现一致的优化目标。其次，我们提出了变分查询去噪（Variational Query Denoising）技术，以解决传统去噪方法的梯度消失问题，该问题严重限制了去噪过程的效率。这明确地引入了随机特性以减轻这一根本限制并解锁显著的性能增益。最后，我们引入了一种复杂的自蒸馏策略，利用后期解码器层的见解协同提高早期层的查询质量，从而放大迭代细化过程。严格的实验表明，MonoVQD在具有挑战性的KITTI单目基准测试中取得了卓越性能。MonoVQD的核心组件无缝集成到其他架构中，即使在nuScenes数据集的多视图3D检测场景中也能提供显著的性能增益，这突显了其广泛的适用性。", "summary": "MonoVQD是一个新颖的框架，旨在改进基于DETR的单目3D目标检测。它通过引入Mask Separated Self-Attention机制、变分查询去噪技术和自蒸馏策略，解决了DETR架构在单目3D检测中的性能限制和梯度消失问题。实验证明，MonoVQD在KITTI单目基准测试中表现优异，并且其核心组件在多视图3D检测（如nuScenes数据集）中也展现出强大的泛化能力和显著的性能提升。", "keywords": "单目3D目标检测, DETR, 变分查询去噪, 自蒸馏, Transformer", "comments": "MonoVQD的创新之处在于其将去噪过程与DETR架构深度融合，特别是通过引入变分查询去噪来解决梯度消失问题，并结合自蒸馏策略进行迭代优化。这不仅提升了单目3D检测的性能，还展现了其在多视图场景下的泛化能力，对于基于Transformer的目标检测领域具有重要意义。"}}
{"id": "2506.15366", "title": "Performative Validity of Recourse Explanations", "authors": ["Gunnar König", "Hidde Fokkema", "Timo Freiesleben", "Celestine Mendler-Dünner", "Ulrike Von Luxburg"], "summary": "When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.", "comment": "34 pages, 3 figures, 1 table, Preprint", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15366v1", "AI": {"title_translation": "追索解释的行动有效性", "tldr": "追索解释可能因集体行为改变数据和模型而失效，本研究形式化了其在行动有效性下的条件，并发现非因果变量是失效的关键。", "motivation": "当许多申请人根据追索解释的建议采取行动时，他们的集体行为可能会改变数据中的统计规律，并在模型重新训练后改变决策边界，导致追索算法使其自身的建议失效，使得申请人即使努力实施建议也可能再次被拒绝。", "method": "本文正式描述了在行动有效性下追索解释保持有效的条件。", "result": "一个关键发现是，如果追索行动受到非因果变量的影响或干预非因果变量，它们可能会失效。", "conclusion": "基于分析，作者警告不要使用标准的反事实解释和因果追索方法，而是提倡仅在因果变量上推荐行动的追索方法。", "translation": "当申请人被算法决策系统拒绝时，追索解释提供了可操作的建议，说明如何改变其输入特征以获得积极评估。一个关键但常被忽视的现象是，追索解释具有行动有效性：当许多申请人根据其建议采取行动时，他们的集体行为可能会改变数据中的统计规律，并且一旦模型重新拟合，也会改变决策边界。因此，追索算法可能会使其自身的建议失效，导致那些努力实施建议的申请人在重新申请时可能再次被拒绝。在这项工作中，我们正式描述了在行动有效性下追索解释保持有效的条件。一个关键发现是，如果追索行动受到非因果变量的影响或干预非因果变量，它们可能会失效。基于我们的分析，我们警告不要使用标准的反事实解释和因果追索方法，而是提倡仅在因果变量上推荐行动的追索方法。", "summary": "本文探讨了追索解释的行动有效性问题，指出当大量用户依据建议行动时，可能改变底层数据分布和模型决策边界，从而使原有建议失效。研究正式化了追索解释在行动有效性下保持有效的条件，并发现非因果变量是导致建议失效的关键因素。作者建议避免使用基于非因果变量的标准反事实解释和因果追索方法，转而提倡仅基于因果变量的追索方法。", "keywords": "追索解释, 行动有效性, 算法公平性, 因果变量, 反事实解释", "comments": "本文提出了一个重要且常被忽视的问题，即机器学习解释的“行动有效性”挑战，强调了在动态环境中解释稳定性的重要性。其对非因果变量的关注提供了实用的指导，对于设计更稳健、更公平的决策支持系统具有重要意义。"}}
{"id": "2506.15398", "title": "Multi-dimensional evaluation on a rural integrated energy system including solar, wind, biomass and geothermal energy", "authors": ["Ruonan Lia", "Chang Wena", "Mingyu Yan", "Congcong Wu", "Ahmed Lotfy Elrefai", "Xiaotong Zhang", "Sahban Wael Saeed Alnaser"], "summary": "This study focuses on the novel municipal-scale rural integrated energy\nsystem (RIES), which encompasses energy supply and application. By constructing\na seven-dimensional evaluation system including energy efficiency, energy\nsupply, low-carbon sustainability, environmental impact, energy economy, social\nbenefits, and integrated energy system development, this research combines the\nimproved analytic hierarchy process (IAHP) and entropy weight method (EWM) by\nsum of squares of deviations to balance expert experience and data objectivity.\nFurthermore, the cloud model is introduced to handle the fuzziness and\nrandomness in the evaluation. This method can quantify the differences in\nsystem performance before and after the planning implementation. The results\nindicate that after planning, the comprehensive score has increased from 83.12\nto 87.55, the entropy value has decreased from 6.931 to 5.336, indicating\nenhanced system stability. The hyper-entropy has dropped from 3.08 to 2.278,\nreflecting a reduction in uncertainty. The research findings provide a\nscientific basis for the planning optimization, policy-making, and sustainable\ndevelopment of rural integrated energy systems, possessing both theoretical\ninnovation and practical guiding value.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15398v1", "AI": {"title_translation": "农村综合能源系统多维度评估（包括太阳能、风能、生物质能和地热能）", "tldr": "本研究构建了一个七维评估系统，结合改进的层次分析法、熵权法和云模型，对新型农村综合能源系统进行多维度评估，结果显示规划后系统性能显著提升。", "motivation": "本研究旨在对新型市级农村综合能源系统（RIES）进行多维度评估，以提供规划优化、政策制定和可持续发展的科学依据。", "method": "本研究构建了一个包含能源效率、能源供应、低碳可持续性、环境影响、能源经济、社会效益和综合能源系统发展七个维度的评估系统。该研究结合了改进的层次分析法（IAHP）和熵权法（EWM），并通过偏差平方和来平衡专家经验和数据客观性。此外，引入云模型来处理评估中的模糊性和随机性。", "result": "评估结果表明，规划实施后，综合评分从83.12提高到87.55，熵值从6.931下降到5.336，表明系统稳定性增强。超熵从3.08下降到2.278，反映了不确定性降低。", "conclusion": "研究结果为农村综合能源系统的规划优化、政策制定和可持续发展提供了科学依据，具有理论创新和实践指导价值。", "translation": "本研究重点关注新型市级农村综合能源系统（RIES），该系统涵盖能源供应和应用。通过构建包括能源效率、能源供应、低碳可持续性、环境影响、能源经济、社会效益和综合能源系统发展在内的七维评估体系，本研究结合改进的层次分析法（IAHP）和熵权法（EWM），通过偏差平方和来平衡专家经验和数据客观性。此外，引入云模型来处理评估中的模糊性和随机性。该方法可以量化规划实施前后系统性能的差异。结果表明，规划后，综合评分从83.12提高到87.55，熵值从6.931下降到5.336，表明系统稳定性增强。超熵从3.08下降到2.278，反映了不确定性降低。研究结果为农村综合能源系统的规划优化、政策制定和可持续发展提供了科学依据，兼具理论创新和实践指导价值。", "summary": "本研究提出了一种针对新型农村综合能源系统（RIES）的七维多维度评估方法。该方法结合了改进的层次分析法、熵权法和云模型，旨在平衡专家经验和数据客观性，并处理评估中的模糊性和随机性。研究结果表明，通过规划实施，RIES的综合性能显著提升，系统稳定性增强，不确定性降低。该研究为RIES的规划优化、政策制定和可持续发展提供了科学基础。", "keywords": "农村综合能源系统, 多维度评估, 层次分析法, 熵权法, 云模型", "comments": "本研究的创新之处在于构建了一个全面的七维评估体系，并创造性地结合了改进的层次分析法、熵权法和云模型，有效地处理了评估过程中的主观性、客观性、模糊性和随机性。该评估方法能够量化规划前后系统性能的变化，为农村综合能源系统的优化和可持续发展提供了强有力的工具，具有重要的理论和实践价值。"}}
{"id": "2506.15639", "title": "The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy", "authors": ["James Weichert", "Daniel Dunlap", "Mohammed Farghally", "Hoda Eldardiry"], "summary": "As artificial intelligence (AI) further embeds itself into many settings\nacross personal and professional contexts, increasing attention must be paid\nnot only to AI ethics, but also to the governance and regulation of AI\ntechnologies through AI policy. However, the prevailing post-secondary\ncomputing curriculum is currently ill-equipped to prepare future AI\npractitioners to confront increasing demands to implement abstract ethical\nprinciples and normative policy preferences into the design and development of\nAI systems. We believe that familiarity with the 'AI policy landscape' and the\nability to translate ethical principles to practices will in the future\nconstitute an important responsibility for even the most technically-focused AI\nengineers.\n  Toward preparing current computer science (CS) students for these new\nexpectations, we developed an AI Policy Module to introduce discussions of AI\npolicy into the CS curriculum. Building on a successful pilot in fall 2024, in\nthis innovative practice full paper we present an updated and expanded version\nof the module, including a technical assignment on \"AI regulation\". We present\nthe findings from our pilot of the AI Policy Module 2.0, evaluating student\nattitudes towards AI ethics and policy through pre- and post-module surveys.\nFollowing the module, students reported increased concern about the ethical\nimpacts of AI technologies while also expressing greater confidence in their\nabilities to engage in discussions about AI regulation. Finally, we highlight\nthe AI Regulation Assignment as an effective and engaging tool for exploring\nthe limits of AI alignment and emphasizing the role of 'policy' in addressing\nethical challenges.", "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15639v1", "AI": {"title_translation": "AI政策模块：培养计算机科学学生在AI伦理和政策方面的能力", "tldr": "本文介绍了一个为计算机科学学生设计的AI政策模块，旨在提升他们对AI伦理和政策的理解与能力，并通过试点验证了其有效性。", "motivation": "当前计算机科学课程未能充分准备未来的AI从业者应对AI伦理和政策的挑战。作者认为，熟悉AI政策并能将伦理原则转化为实践是未来AI工程师的重要职责。", "method": "开发了一个AI政策模块，将其引入CS课程，并包含一项关于“AI监管”的技术作业。通过对AI政策模块2.0的试点，使用模块前后的调查问卷评估学生对AI伦理和政策的态度。", "result": "模块结束后，学生对AI技术的伦理影响表现出更高的关注，并对参与AI监管讨论的能力表达了更大的信心。AI监管作业被证明是探索AI对齐限制和强调政策在解决伦理挑战中作用的有效工具。", "conclusion": "AI政策模块能有效提升计算机科学学生在AI伦理和政策方面的能力和信心，特别是通过实践作业强调了政策在解决伦理挑战中的重要作用。", "translation": "随着人工智能（AI）进一步融入个人和专业环境中的许多场景，不仅要越来越关注AI伦理，还要关注通过AI政策对AI技术进行治理和监管。然而，当前主流的高等教育计算机课程在培养未来的AI从业者方面力不从心，无法应对日益增长的将抽象伦理原则和规范性政策偏好实现在AI系统设计和开发中的需求。我们相信，熟悉“AI政策格局”以及将伦理原则转化为实践的能力，在未来将成为即使是最专注于技术的AI工程师的一项重要责任。\n为了让当前的计算机科学（CS）学生为这些新期望做好准备，我们开发了一个AI政策模块，将AI政策的讨论引入CS课程。在2024年秋季成功试点项目的基础上，在这篇创新的实践型论文中，我们介绍了该模块的更新和扩展版本，其中包括一项关于“AI监管”的技术作业。我们展示了AI政策模块2.0试点项目的研究结果，通过模块前和模块后的调查问卷评估了学生对AI伦理和政策的态度。模块结束后，学生报告称对AI技术的伦理影响表现出更高的关注，同时对参与AI监管讨论的能力也表达了更大的信心。最后，我们强调AI监管作业是探索AI对齐限制和强调“政策”在解决伦理挑战中作用的有效且引人入胜的工具。", "summary": "本文介绍了为计算机科学学生开发的一个AI政策模块，旨在弥补现有课程在AI伦理和政策教育方面的不足。该模块通过引入AI政策讨论和技术作业，成功提升了学生对AI伦理影响的关注度，并增强了他们参与AI监管讨论的信心。研究结果表明，该模块是培养未来AI工程师在伦理和政策领域能力的重要工具。", "keywords": "AI伦理, AI政策, 计算机科学教育, 课程开发, 学生能力培养", "comments": "这篇论文的创新点在于其提出了一个具体的、可实施的课程模块来解决当前计算机科学教育中AI伦理和政策空白的问题。其重要性在于强调了未来AI工程师不仅需要技术能力，还需要具备伦理和政策素养，这对于AI的负责任发展至关重要。通过实际的模块设计和试点验证，为其他教育机构提供了宝贵的实践经验。"}}
{"id": "2506.15463", "title": "Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "summary": "In practical systems, recorded analog signals must be digitized for\nprocessing, introducing quantization as a critical aspect of data acquisition.\nWhile prior studies have examined quantization effects in various signal\nprocessing contexts, its impact on differential microphone arrays (DMAs),\nparticularly in one-dimensional (1D) first-order configurations, remains\nunexplored. This paper investigates the influence of signal quantization on\nperformance of first-order 1D DMAs across various beampatterns. An analytical\nexpression for quantized beamformed output for a first-order 1D DMA has been\nformulated. The effect of signal quantization has been studied on array\nperformance measures such as the Beampattern, Directivity Factor (DF),\nFront-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that\nbeampattern shape remains structurally invariant across quantization bit\ndepths, with quantization primarily affecting ND. DF and FBR remain constant\nwith the varying number of quantization bits. Additionally, ND is shown to be\nfrequency-independent; however, it increases with increasing quantization bit\ndepths, enhancing interference suppression. The study also examines the effect\nof steering nulls across the azimuthal range, showing that ND degrades as the\nnull moves closer to the source look direction, indicating reduced interference\nsuppression.", "comment": "5 Pages with 6 figures and 1 table", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15463v1", "AI": {"title_translation": "信号量化对一阶一维差分麦克风阵列性能测量的影响", "tldr": "本研究探讨了信号量化对一阶一维差分麦克风阵列（DMA）性能的影响，发现波束图形状保持不变，而零点深度（ND）受量化比特深度影响，且随比特深度增加而提高干扰抑制能力，但当零点靠近声源方向时ND会下降。", "motivation": "在实际系统中，模拟信号需要数字化处理，引入量化。尽管现有研究已探讨了量化对各种信号处理的影响，但其对差分麦克风阵列（DMA），尤其是一维（1D）一阶配置的影响尚未被探索。", "method": "本研究制定了一阶一维差分麦克风阵列量化波束形成输出的解析表达式，并研究了信号量化对阵列性能指标（如波束图、方向性因子（DF）、前后比（FBR）和零点深度（ND））的影响，并通过仿真进行验证。", "result": "仿真结果显示，波束图形状在不同量化比特深度下结构上保持不变，量化主要影响零点深度（ND）。方向性因子（DF）和前后比（FBR）随量化比特数的变化保持恒定。此外，零点深度（ND）与频率无关；然而，它随量化比特深度的增加而增加，从而增强了干扰抑制能力。研究还发现，当零点移近声源方向时，零点深度（ND）会下降，表明干扰抑制能力降低。", "conclusion": "信号量化对一阶一维差分麦克风阵列的性能有特定影响，主要体现在零点深度上，其随量化比特深度增加而改善干扰抑制，但零点位置会影响其效果。DF和FBR对量化比特数不敏感。", "translation": "在实际系统中，记录的模拟信号必须数字化处理，引入量化作为数据采集的关键方面。尽管之前的研究已经考察了各种信号处理背景下的量化效应，但其对差分麦克风阵列（DMA），特别是一维（1D）一阶配置的影响仍未被探索。本文研究了信号量化对一阶一维DMA在各种波束图下的性能影响。本文推导出了一个一阶一维DMA量化波束形成输出的解析表达式。研究了信号量化对阵列性能指标的影响，如波束图、方向性因子（DF）、前后比（FBR）和零点深度（ND）。仿真结果表明，波束图形状在不同量化比特深度下结构上保持不变，量化主要影响ND。DF和FBR随量化比特数的变化保持恒定。此外，ND被证明与频率无关；然而，它随量化比特深度的增加而增加，从而增强了干扰抑制。该研究还考察了在方位角范围内转向零点的影响，结果表明，当零点移近声源方向时，ND会下降，表明干扰抑制能力降低。", "summary": "本研究探讨了信号量化对一阶一维差分麦克风阵列（DMA）性能的影响。通过推导量化波束形成输出的解析表达式并进行仿真，发现波束图形状在不同量化比特深度下保持不变，而零点深度（ND）是主要受影响的性能指标。ND随量化比特深度的增加而提高，从而增强干扰抑制能力，且与频率无关。然而，方向性因子（DF）和前后比（FBR）不受量化比特数变化的影响。研究还指出，当零点靠近声源方向时，ND会下降，导致干扰抑制能力减弱。", "keywords": "信号量化, 差分麦克风阵列, 零点深度, 波束图, 性能测量", "comments": "该论文填补了信号量化对差分麦克风阵列性能影响研究的空白，特别是针对一阶一维配置。其创新点在于建立了量化波束形成输出的解析表达式，并深入分析了量化对不同性能指标的具体影响。研究结果揭示了量化比特深度对零点深度和干扰抑制的关键作用，为实际系统设计中量化参数的选择提供了理论依据和指导。"}}
{"id": "2506.14793", "title": "Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout", "authors": ["Aditya Ravuri", "Neil D. Lawrence"], "summary": "Protein Language Models (PLMs) such as ESM2 have been shown to be capable of\nzero-shot prediction of critical scalar properties of proteins (fitness). In\nthis work, we show that injecting a dropout layer at inference time between a\nPLM's featurizer/embedding layer and its transformer, and averaging its output\nakin to Monte-Carlo dropout increases zero-shot performance on a subset of the\nProteinGym dataset. This is the case even when the model was not trained with\ndropouts to begin with, and does not require retraining or finetuning of the\nPLM. A dropout of 0.1 seems performant across all models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14793v1", "AI": {"title_translation": "蛋白质语言模型零样本适应度预测通过仅推理 Dropout 得到改善", "tldr": "仅推理时使用 Dropout 可以提高蛋白质语言模型的零样本适应度预测性能，无需重新训练。", "motivation": "蛋白质语言模型（PLMs）已被证明能够对蛋白质的关键标量属性（适应度）进行零样本预测，本研究旨在进一步提高其预测性能。", "method": "在推理时，在蛋白质语言模型的特征提取器/嵌入层与其 Transformer 之间注入一个 Dropout 层，并平均其输出，类似于蒙特卡洛 Dropout。", "result": "即使模型最初未经过 Dropout 训练，并且无需重新训练或微调 PLM，这种方法也能提高在 ProteinGym 数据集子集上的零样本预测性能。0.1 的 Dropout 率在所有模型中表现良好。", "conclusion": "仅在推理时使用 Dropout 是一种有效提升蛋白质语言模型零样本适应度预测能力的方法，且无需对模型进行重新训练或微调。", "translation": "蛋白质语言模型（PLMs），例如 ESM2，已被证明能够对蛋白质的关键标量属性（适应度）进行零样本预测。在这项工作中，我们展示了在推理时，在 PLM 的特征提取器/嵌入层与其 Transformer 之间注入一个 Dropout 层，并平均其输出（类似于蒙特卡洛 Dropout），可以提高在 ProteinGym 数据集子集上的零样本性能。即使模型最初未经过 Dropout 训练，并且不需要对 PLM 进行重新训练或微调，这种情况也成立。0.1 的 Dropout 率似乎在所有模型中都表现良好。", "summary": "本文提出了一种通过在推理阶段引入 Dropout 层并平均输出（类似于蒙特卡洛 Dropout）来提升蛋白质语言模型（PLMs）零样本蛋白质适应度预测性能的方法。研究发现，即使模型在训练时未使用 Dropout 且无需重新训练或微调，该方法仍能在 ProteinGym 数据集子集上有效提高预测性能。实验结果表明，0.1 的 Dropout 率在所有模型中表现最佳。", "keywords": "蛋白质语言模型, 零样本预测, Dropout, 适应度, 推理", "comments": "这项研究的创新之处在于，它展示了一种无需重新训练或微调即可提高预训练蛋白质语言模型性能的简单而有效的方法。通过在推理时引入 Dropout，它在不增加训练复杂性的情况下，可能提高了模型的鲁棒性和不确定性估计能力，对于蛋白质工程和设计领域具有重要意义。"}}
{"id": "2506.15081", "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification", "authors": ["Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "summary": "Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.", "comment": "Accepted by ACL2025(main conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15081v1", "AI": {"title_translation": "通过语篇感知的话语澄清改进对话语篇分析", "tldr": "本文提出语篇感知澄清模块（DCM）和贡献感知偏好优化（CPO）来解决对话语篇分析中的歧义问题，显著提高了分析性能并优于现有最先进模型。", "motivation": "对话中的语言特征（如省略和习语）经常引入歧义，模糊了预期的语篇关系，给对话语篇分析器带来了重大挑战。", "method": "提出语篇感知澄清模块（DCM），包含澄清类型推理和语篇目标推理，用于分析语言特征并区分预期与歧义关系。此外，引入贡献感知偏好优化（CPO）来降低错误澄清的风险和级联错误，CPO使分析器能够评估DCM澄清的贡献并提供反馈以优化DCM。", "result": "在STAC和Molweni数据集上的大量实验表明，该方法有效解决了歧义，并显著优于最先进的（SOTA）基线。", "conclusion": "该研究通过引入语篇感知澄清模块（DCM）和贡献感知偏好优化（CPO），成功解决了对话语篇分析中的歧义问题，显著提高了分析性能，并取得了优于现有技术的结果。", "translation": "对话语篇分析旨在识别和分析对话中话语之间的语篇关系。然而，对话中的语言特征，如省略和习语，经常引入歧义，模糊了预期的语篇关系，给分析器带来了重大挑战。为了解决这个问题，我们提出了一个语篇感知澄清模块（DCM）来增强对话语篇分析器的性能。DCM采用两种不同的推理过程：澄清类型推理和语篇目标推理。前者分析语言特征，而后者区分预期关系与歧义关系。此外，我们引入了贡献感知偏好优化（CPO）来降低错误澄清的风险，从而减少级联错误。CPO使分析器能够评估DCM澄清的贡献并提供反馈以优化DCM，增强其适应性并与分析器的要求对齐。在STAC和Molweni数据集上进行的大量实验表明，我们的方法有效地解决了歧义并显著优于最先进的（SOTA）基线。", "summary": "本文提出了一种名为语篇感知澄清模块（DCM）的新方法，以解决对话语篇分析中因语言歧义导致的关系模糊问题。DCM利用澄清类型推理和语篇目标推理来识别并解决歧义。为进一步优化，引入了贡献感知偏好优化（CPO），以减少错误澄清并优化DCM。实验结果表明，该方法在STAC和Molweni数据集上显著优于现有技术，有效提升了对话语篇分析的性能。", "keywords": "对话语篇分析, 歧义澄清, DCM, CPO, 语篇关系", "comments": "该论文通过引入DCM和CPO两个创新模块，有效解决了对话语篇分析中常见的歧义问题，这是对传统语篇分析方法的重要改进。特别是CPO机制，通过引入反馈循环来优化澄清过程，提升了系统的鲁棒性和适应性，具有较强的创新性。"}}
{"id": "2506.14994", "title": "Optimal alignment of Lorentz orientation and generalization to matrix Lie groups", "authors": ["Congzhou M Sha"], "summary": "There exist elegant methods of aligning point clouds in $\\mathbb R^3$.\nUnfortunately, these methods rely on the positive definite property of the\nEuclidean metric, and do not easily extend to the indefinite Minkowski metric.\nIn this paper, we propose two solutions to the following problem: given\ninertial reference frames $A$ and $B$, and given (possibly noisy) measurements\nof a set of 4-vectors $\\{v_i\\}$ made in those reference frames with components\n$\\{v_{A,i}\\}$ and $\\{v_{B,i}\\}$, find the optimal Lorentz transformation\n$\\Lambda$ such that $\\Lambda v_{A,i}=v_{B,i}$. The method we outline is\nconceptually simple and easily extends to alignment problems in other matrix\nLie groups.", "comment": "8 pages, 2 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14994v1", "AI": {"title_translation": "洛伦兹方向的最优对齐及其向矩阵李群的推广", "tldr": "本文提出了两种在闵可夫斯基空间中，给定参考系A和B中测量的一组四向量，寻找最优洛伦兹变换Λ的方法，以解决点云对齐问题。", "motivation": "现有的R^3点云对齐方法依赖于欧几里得度量的正定性，无法轻易扩展到不定闵可夫斯基度量，导致无法解决洛伦兹变换下的对齐问题。", "method": "论文提出了两种解决方案，旨在给定惯性参考系A和B以及一组在这些参考系中测量的四向量（可能包含噪声），找到最优的洛伦兹变换Λ，使得Λv_{A,i}=v_{B,i}。", "result": "提出的方法概念简单，并且可以轻松扩展到其他矩阵李群中的对齐问题。", "conclusion": "论文成功地提出了两种解决洛伦兹变换下四向量最优对齐的方法，这些方法具有简单性和普适性。", "translation": "在$\\mathbb R^3$中存在优雅的点云对齐方法。不幸的是，这些方法依赖于欧几里得度量的正定性质，并且不容易扩展到不定闵可夫斯基度量。在本文中，我们针对以下问题提出了两种解决方案：给定惯性参考系A和B，以及在这些参考系中对一组四向量$\\{v_i\\}$进行的（可能带噪声的）测量，其分量分别为$\\{v_{A,i}\\}$和$\\{v_{B,i}\\}$，找到最优的洛伦兹变换$\\Lambda$，使得$\\Lambda v_{A,i}=v_{B,i}$。我们概述的方法概念简单，并且可以轻松扩展到其他矩阵李群中的对齐问题。", "summary": "本文针对欧几里得空间点云对齐方法不适用于闵可夫斯基度量的问题，提出了两种解决方案。研究旨在找到最优的洛伦兹变换，以对齐在不同惯性参考系中测量的一组四向量。所提出的方法不仅概念简单，而且易于推广到其他矩阵李群的对齐问题。", "keywords": "洛伦兹变换, 点云对齐, 矩阵李群, 闵可夫斯基度量, 四向量", "comments": "本文的创新点在于解决了传统点云对齐方法无法应用于洛伦兹变换的问题，通过提出适用于闵可夫斯基度量的方法，填补了这一空白。其方法的简洁性和向其他矩阵李群的推广性是其重要性所在，为广义相对论和高能物理等领域的数据对齐提供了新的工具。"}}
{"id": "2506.15228", "title": "ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression", "authors": ["Yufeng Zhang", "Wenrui Dai", "Hang Yu", "Shizhan Liu", "Junhui Hou", "Jianguo Li", "Weiyao Lin"], "summary": "Neural Image Compression (NIC) has revolutionized image compression with its\nsuperior rate-distortion performance and multi-task capabilities, supporting\nboth human visual perception and machine vision tasks. However, its widespread\nadoption is hindered by substantial computational demands. While existing\napproaches attempt to address this challenge through module-specific\noptimizations or pre-defined complexity levels, they lack comprehensive control\nover computational complexity. We present ABC (Adaptive BayesNet structure\nlearning for computational scalable multi-task image Compression), a novel,\ncomprehensive framework that achieves computational scalability across all NIC\ncomponents through Bayesian network (BayesNet) structure learning. ABC\nintroduces three key innovations: (i) a heterogeneous bipartite BayesNet\n(inter-node structure) for managing neural backbone computations; (ii) a\nhomogeneous multipartite BayesNet (intra-node structure) for optimizing\nautoregressive unit processing; and (iii) an adaptive control module that\ndynamically adjusts the BayesNet structure based on device capabilities, input\ndata complexity, and downstream task requirements. Experiments demonstrate that\nABC enables full computational scalability with better complexity adaptivity\nand broader complexity control span, while maintaining competitive compression\nperformance. Furthermore, the framework's versatility allows integration with\nvarious NIC architectures that employ BayesNet representations, making it a\nrobust solution for ensuring computational scalability in NIC applications.\nCode is available in https://github.com/worldlife123/cbench_BaSIC.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15228v1", "AI": {"title_translation": "ABC：面向计算可扩展多任务图像压缩的自适应贝叶斯网络结构学习", "tldr": "神经图像压缩（NIC）计算成本高，ABC通过自适应贝叶斯网络结构学习实现全组件计算可扩展性，同时保持性能。", "motivation": "神经图像压缩（NIC）虽然性能优越，但其高计算需求阻碍了广泛应用；现有方法缺乏对计算复杂度的全面控制。", "method": "本文提出了ABC（Adaptive BayesNet structure learning for computational scalable multi-task image Compression）框架，通过贝叶斯网络（BayesNet）结构学习实现NIC所有组件的计算可扩展性。其关键创新包括：(i) 异构二分贝叶斯网络管理神经网络骨干计算；(ii) 同构多部贝叶斯网络优化自回归单元处理；(iii) 自适应控制模块根据设备能力、输入数据复杂性和下游任务需求动态调整贝叶斯网络结构。", "result": "实验证明，ABC实现了全面的计算可扩展性，具有更好的复杂性适应性和更广的复杂性控制范围，同时保持了有竞争力的压缩性能。", "conclusion": "ABC是一个强大的解决方案，能够确保神经图像压缩（NIC）应用中的计算可扩展性，并且可以与采用贝叶斯网络表示的各种NIC架构集成。", "translation": "神经图像压缩（NIC）以其卓越的码率-失真性能和多任务能力（支持人类视觉感知和机器视觉任务）彻底改变了图像压缩领域。然而，其广泛采用受到巨大计算需求的阻碍。尽管现有方法试图通过模块特定优化或预定义复杂性级别来解决这一挑战，但它们缺乏对计算复杂度的全面控制。我们提出了ABC（面向计算可扩展多任务图像压缩的自适应贝叶斯网络结构学习），这是一个新颖、全面的框架，通过贝叶斯网络（BayesNet）结构学习实现所有NIC组件的计算可扩展性。ABC引入了三项关键创新：(i) 用于管理神经网络骨干计算的异构二分贝叶斯网络（节点间结构）；(ii) 用于优化自回归单元处理的同构多部贝叶斯网络（节点内结构）；以及(iii) 基于设备能力、输入数据复杂性和下游任务需求动态调整贝叶斯网络结构的自适应控制模块。实验表明，ABC实现了全面的计算可扩展性，具有更好的复杂性适应性和更广的复杂性控制范围，同时保持了有竞争力的压缩性能。此外，该框架的多功能性允许与采用贝叶斯网络表示的各种NIC架构集成，使其成为确保NIC应用中计算可扩展性的强大解决方案。代码可在https://github.com/worldlife123/cbench_BaSIC 获取。", "summary": "本文提出了ABC框架，旨在解决神经图像压缩（NIC）高计算成本的问题。ABC通过自适应贝叶斯网络结构学习，实现了NIC所有组件的计算可扩展性。其主要创新包括异构和同构贝叶斯网络结构，以及一个动态调整结构的自适应控制模块。实验证明ABC在保持竞争性压缩性能的同时，提供了更全面的计算可扩展性和更广的复杂性控制。", "keywords": "神经图像压缩, 计算可扩展性, 贝叶斯网络, 结构学习, 多任务图像压缩", "comments": "该论文通过引入自适应贝叶斯网络结构学习，为神经图像压缩的计算可扩展性提供了一个全面且创新的解决方案。其亮点在于对NIC所有组件的细粒度控制，以及根据不同条件动态调整计算复杂度的能力，这对于NIC的实际部署和广泛应用至关重要。"}}
{"id": "2506.15614", "title": "TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "summary": "This paper presents TTSOps, a fully automated closed-loop framework for\nconstructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated\nweb-scale speech data, often referred to as ``dark data,'' such as online\nvideos. Conventional TTS training pipelines require well-curated corpora with\nhigh acoustic quality and accurate text-speech alignment, which severely limits\nscalability, speaker diversity, and real-world applicability. While recent\nstudies have proposed acoustic-quality-based data selection techniques, they\noften overlook two critical aspects: (1) the inherent robustness of modern TTS\nmodels to noise, and (2) the potential contribution of perceptually low-quality\nyet informative samples. To address these issues, TTSOps introduces a\ndata-centric training pipeline that integrates three core components: (1)\nautomated data collection from dark data sources, (2) utterance-level dynamic\nselection of data cleansing methods based on training data quality, and (3)\nevaluation-in-the-loop data selection using automatically predicted mean\nopinion scores (MOS) to estimate each utterance's impact on model performance.\nFurthermore, TTSOps jointly optimizes the corpus and the TTS model in a\nclosed-loop framework by dynamically adapting both data selection and data\ncleansing processes to the characteristics of the target TTS model. Extensive\nexperiments on Japanese YouTube data demonstrate that TTSOps outperforms\nconventional acoustic-quality-based baselines in both the naturalness and\nspeaker diversity of synthesized speech.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15614v1", "AI": {"title_translation": "TTSOps：一种用于从暗数据训练多说话人TTS模型的闭环语料库优化框架", "tldr": "TTSOps是一个自动化的闭环框架，用于从嘈杂的“暗数据”训练多说话人TTS模型，通过动态数据选择和清洗，显著提高了合成语音的自然度和说话人多样性。", "motivation": "传统TTS训练需要高质量、对齐的语料库，限制了可扩展性、说话人多样性和实际应用。现有数据选择技术忽视了现代TTS模型的噪声鲁棒性和低质量但信息丰富的样本的潜力。", "method": "TTSOps引入了一个数据中心的训练流程，包含：1) 从暗数据源自动收集数据；2) 基于训练数据质量的语音级动态数据清洗方法选择；3) 使用自动预测的平均意见得分（MOS）进行评估循环中的数据选择，以估计每个语音对模型性能的影响。此外，TTSOps在一个闭环框架中通过动态调整数据选择和数据清洗过程，共同优化语料库和TTS模型。", "result": "在日语YouTube数据上的广泛实验表明，TTSOps在合成语音的自然度和说话人多样性方面均优于传统的基于声学质量的基线方法。", "conclusion": "TTSOps通过其闭环语料库优化框架，成功地从嘈杂的“暗数据”中构建了高性能的多说话人TTS系统，克服了传统方法的局限性。", "translation": "本文介绍了TTSOps，一个全自动闭环框架，用于从嘈杂、未经整理的网络规模语音数据（通常称为“暗数据”，如在线视频）构建多说话人文本到语音（TTS）系统。传统的TTS训练流程需要高质量声学和准确文本-语音对齐的精心整理语料库，这严重限制了可扩展性、说话人多样性和实际应用。尽管最近的研究提出了基于声学质量的数据选择技术，但它们往往忽视了两个关键方面：（1）现代TTS模型固有的噪声鲁棒性，以及（2）感知质量较低但信息丰富的样本的潜在贡献。为了解决这些问题，TTSOps引入了一个以数据为中心的训练流程，该流程集成了三个核心组件：（1）从暗数据源自动收集数据，（2）基于训练数据质量的语音级动态数据清洗方法选择，以及（3）使用自动预测的平均意见得分（MOS）进行评估循环中的数据选择，以估计每个语音对模型性能的影响。此外，TTSOps通过动态调整数据选择和数据清洗过程以适应目标TTS模型的特性，在一个闭环框架中共同优化语料库和TTS模型。在日本YouTube数据上的广泛实验表明，TTSOps在合成语音的自然度和说话人多样性方面均优于传统的基于声学质量的基线方法。", "summary": "TTSOps是一个创新的闭环框架，旨在解决从嘈杂的“暗数据”训练多说话人TTS模型的挑战。它通过自动化数据收集、动态数据清洗选择以及基于预测MOS的评估循环数据选择等核心组件，共同优化语料库和TTS模型。实验证明，TTSOps在合成语音的自然度和说话人多样性方面优于传统方法。", "keywords": "TTSOps, 多说话人TTS, 暗数据, 语料库优化, 闭环框架", "comments": "TTSOps的创新在于其全自动闭环的数据优化方法，尤其是在处理“暗数据”方面的能力，这极大地扩展了TTS模型训练的数据来源。它考虑了现代TTS模型的噪声鲁棒性以及低质量样本的潜在价值，这与传统的数据筛选理念不同，具有重要的实际意义。该框架为构建更具可扩展性和多样性的TTS系统提供了新的范式。"}}
{"id": "2506.15117", "title": "CipherMind: The Longest Codebook in the World", "authors": ["Ming Nie", "Zhixiong Yang", "Bingsheng Wei"], "summary": "In recent years, the widespread application of large language models has\ninspired us to consider using inference for communication encryption. We\ntherefore propose CipherMind, which utilizes intermediate results from\ndeterministic fine-tuning of large model inferences as transmission content.\nThe semantic parameters of large models exhibit characteristics like opaque\nunderlying implementations and weak interpretability, thus enabling their use\nas an encryption method for data transmission. This communication paradigm can\nbe applied in scenarios like intra-gateway transmission, and theoretically, it\ncan be implemented using any large model as its foundation.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15117v1", "AI": {"title_translation": "CipherMind：世界上最长的密码本", "tldr": "CipherMind利用大模型推理的中间结果进行通信加密，适用于网关内传输。", "motivation": "近年来，大型语言模型的广泛应用启发了研究人员考虑使用推理进行通信加密。", "method": "提出CipherMind，利用大型模型推理的确定性微调中间结果作为传输内容。大型模型的语义参数具有底层实现不透明和可解释性弱的特点，因此可作为数据传输的加密方法。", "result": "Not mentioned in abstract", "conclusion": "这种通信范式可应用于网关内传输等场景，理论上可以使用任何大型模型作为其基础实现。", "translation": "近年来，大型语言模型的广泛应用启发我们考虑使用推理进行通信加密。因此，我们提出了CipherMind，它利用大型模型推理的确定性微调的中间结果作为传输内容。大型模型的语义参数表现出底层实现不透明和可解释性弱的特点，从而使其能够用作数据传输的加密方法。这种通信范式可以应用于诸如网关内传输的场景，并且理论上，它可以使用任何大型模型作为其基础来实现。", "summary": "本文提出了CipherMind，一种利用大型语言模型推理的中间结果进行通信加密的方法。该方法基于大模型语义参数不透明和可解释性弱的特性，将中间结果作为加密传输内容，适用于网关内传输，并理论上可基于任意大模型实现。", "keywords": "大型语言模型, 通信加密, CipherMind, 推理, 中间结果", "comments": "本文提出了一种新颖的加密方法，利用大型语言模型的内在特性进行数据传输加密，其创新点在于将LLM的推理中间结果作为加密载体。这种方法具有潜在的应用前景，尤其是在需要高安全性和低可解释性的内部通信场景。然而，其安全性和效率在实际应用中的具体表现仍需进一步验证。"}}
{"id": "2506.15096", "title": "DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory", "authors": ["Zihe Ji", "Huangxuan Lin", "Yue Gao"], "summary": "We present DyNaVLM, an end-to-end vision-language navigation framework using\nVision-Language Models (VLM). In contrast to prior methods constrained by fixed\nangular or distance intervals, our system empowers agents to freely select\nnavigation targets via visual-language reasoning. At its core lies a\nself-refining graph memory that 1) stores object locations as executable\ntopological relations, 2) enables cross-robot memory sharing through\ndistributed graph updates, and 3) enhances VLM's decision-making via retrieval\naugmentation. Operating without task-specific training or fine-tuning, DyNaVLM\ndemonstrates high performance on GOAT and ObjectNav benchmarks. Real-world\ntests further validate its robustness and generalization. The system's three\ninnovations: dynamic action space formulation, collaborative graph memory, and\ntraining-free deployment, establish a new paradigm for scalable embodied robot,\nbridging the gap between discrete VLN tasks and continuous real-world\nnavigation.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15096v1", "AI": {"title_translation": "DyNaVLM：具有动态视点和自精化图记忆的零样本视觉语言导航系统", "tldr": "DyNaVLM是一个零样本视觉语言导航系统，通过动态视点和自精化图记忆，使机器人在连续真实世界环境中自由导航，无需特定训练。", "motivation": "传统的视觉语言导航方法受限于固定的角度或距离间隔，限制了智能体自由选择导航目标的能力。本文旨在克服这些限制，实现更灵活和通用的导航。", "method": "DyNaVLM是一个端到端的视觉语言导航框架，利用视觉语言模型（VLM）。其核心是一个自精化图记忆，该记忆：1) 将对象位置存储为可执行的拓扑关系；2) 通过分布式图更新实现跨机器人记忆共享；3) 通过检索增强提升VLM的决策能力。该系统无需任务特定训练或微调。", "result": "DyNaVLM在GOAT和ObjectNav基准测试中表现出高性能。真实世界测试进一步验证了其鲁棒性和泛化能力。", "conclusion": "DyNaVLM通过动态动作空间表述、协作图记忆和免训练部署三项创新，为可扩展具身机器人建立了新的范式，弥合了离散VLN任务与连续真实世界导航之间的鸿沟。", "translation": "我们提出了DyNaVLM，一个使用视觉语言模型（VLM）的端到端视觉语言导航框架。与受限于固定角度或距离间隔的现有方法不同，我们的系统使智能体能够通过视觉语言推理自由选择导航目标。其核心是一个自精化图记忆，该记忆：1）将对象位置存储为可执行的拓扑关系；2）通过分布式图更新实现跨机器人记忆共享；3）通过检索增强提升VLM的决策能力。DyNaVLM无需任务特定训练或微调即可运行，并在GOAT和ObjectNav基准测试中表现出高性能。真实世界测试进一步验证了其鲁棒性和泛化能力。该系统的三项创新：动态动作空间表述、协作图记忆和免训练部署，为可扩展具身机器人建立了新的范式，弥合了离散VLN任务与连续真实世界导航之间的鸿沟。", "summary": "DyNaVLM是一个创新的零样本视觉语言导航系统，通过引入动态视点和自精化图记忆，克服了传统方法固定导航间隔的限制。该系统使智能体能通过视觉语言推理自由选择目标，其核心的图记忆支持对象位置存储、跨机器人记忆共享和VLM决策增强。DyNaVLM无需特定训练即可在基准测试和真实世界中展现出卓越的性能、鲁棒性和泛化能力，为具身机器人导航开辟了新范式。", "keywords": "视觉语言导航, 零样本, 动态视点, 图记忆, 具身机器人", "comments": "DyNaVLM的创新之处在于其动态动作空间、协作式图记忆以及无需训练即可部署的能力。这使其能够更好地适应连续的真实世界导航场景，而非局限于离散任务。特别是其自精化图记忆和跨机器人记忆共享功能，为具身机器人提供了一个可扩展且高效的导航解决方案，有望弥合模拟环境与真实世界导航之间的差距。"}}
{"id": "2506.15189", "title": "Accessible Gesture-Driven Augmented Reality Interaction System", "authors": ["Yikan Wang"], "summary": "Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15189v1", "AI": {"title_translation": "无障碍手势驱动增强现实交互系统", "tldr": "本研究提出了一种基于手势的增强现实交互系统，利用深度学习和联邦学习提高运动障碍用户的可访问性和效率，实验证明任务完成效率提高20%，用户满意度提高25%。", "motivation": "增强现实（AR）虽然提供沉浸式交互，但由于依赖精确的输入方法，对运动障碍或手部灵活性有限的用户来说仍然难以使用。", "method": "该系统利用深度学习（包括视觉Transformer、时间卷积网络和图注意力网络）通过可穿戴传感器和摄像头识别手势和身体姿态，并根据用户能力调整界面。采用联邦学习进行隐私保护的模型训练，并通过强化学习优化菜单布局和交互模式等界面元素。", "result": "与基线AR系统相比，该方法使运动障碍用户的任务完成效率提高了20%，用户满意度提高了25%。", "conclusion": "该方法显著增强了AR系统的可访问性和可扩展性，为运动障碍用户提供了更高效、更令人满意的交互体验。", "translation": "增强现实（AR）提供了沉浸式交互，但由于依赖精确的输入方法，对于运动障碍或手部灵活性有限的用户来说仍然难以使用。本研究提出了一种用于AR环境的基于手势的交互系统，利用深度学习从可穿戴传感器和摄像头识别手部和身体手势，并根据用户能力调整界面。该系统采用视觉Transformer（ViTs）、时间卷积网络（TCNs）和图注意力网络（GATs）进行手势处理，并通过联邦学习确保跨不同用户的隐私保护模型训练。强化学习优化了菜单布局和交互模式等界面元素。实验表明，与基线AR系统相比，运动障碍用户的任务完成效率提高了20%，用户满意度提高了25%。这种方法增强了AR的可访问性和可扩展性。", "summary": "本研究开发了一种无障碍的手势驱动增强现实交互系统，旨在解决运动障碍用户在AR环境中交互困难的问题。该系统结合了深度学习（利用ViTs、TCNs和GATs进行手势识别）、联邦学习（用于隐私保护训练）和强化学习（用于界面优化）。实验结果显示，该系统显著提升了运动障碍用户的任务完成效率和用户满意度，证明了其在提高AR可访问性和可扩展性方面的有效性。", "keywords": "深度学习, 联邦学习, 手势识别, 增强现实, 可访问性, 人机交互", "comments": "这项研究的创新之处在于结合了多种先进的深度学习技术（ViTs, TCNs, GATs）进行手势识别，并引入联邦学习来解决隐私问题，同时利用强化学习优化用户界面，从而为运动障碍用户提供了更具包容性的AR体验。其重要性在于拓宽了AR技术的应用范围，使其能够服务于更广泛的用户群体。"}}
{"id": "2506.15400", "title": "The maximum-average subtensor problem: equilibrium and out-of-equilibrium properties", "authors": ["Vittorio Erba", "Nathan Malo Kupferschmid", "Rodrigo Pérez Ortiz", "Lenka Zdeborová"], "summary": "In this paper we introduce and study the Maximum-Average Subtensor ($p$-MAS)\nproblem, in which one wants to find a subtensor of size $k$ of a given random\ntensor of size $N$, both of order $p$, with maximum sum of entries. We are\nmotivated by recent work on the matrix case of the problem in which several\nequilibrium and non-equilibrium properties have been characterized analytically\nin the asymptotic regime $1 \\ll k \\ll N$, and a puzzling phenomenon was\nobserved involving the coexistence of a clustered equilibrium phase and an\nefficient algorithm which produces submatrices in this phase. Here we extend\nprevious results on equilibrium and algorithmic properties for the matrix case\nto the tensor case. We show that the tensor case has a similar equilibrium\nphase diagram as the matrix case, and an overall similar phenomenology for the\nconsidered algorithms. Additionally, we consider out-of-equilibrium landscape\nproperties using Overlap Gap Properties and Franz-Parisi analysis, and discuss\nthe implications or lack-thereof for average-case algorithmic hardness.", "comment": null, "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.15400v1", "AI": {"title_translation": "最大平均子张量问题：平衡态和非平衡态性质", "tldr": "本文引入并研究了最大平均子张量（$p$-MAS）问题，将其从矩阵情况扩展到张量情况，分析了其平衡态和非平衡态性质，并发现与矩阵情况相似的现象。", "motivation": "受到近期关于该问题矩阵情况的工作的启发，该工作在渐近区域 $1 \text{ \textless\textless } k \text{ \textless\textless } N$ 中表征了多种平衡态和非平衡态性质，并观察到了一种令人费解的现象，即聚类平衡相与能在此相中生成子矩阵的有效算法共存。", "method": "本文引入并研究了最大平均子张量（$p$-MAS）问题。将矩阵情况下的平衡态和算法性质的结果扩展到张量情况。使用重叠间隙性质（Overlap Gap Properties）和Franz-Parisi分析考虑了非平衡态景观性质。", "result": "张量情况具有与矩阵情况相似的平衡相图，并且对于所考虑的算法，总体上具有相似的现象。", "conclusion": "Not mentioned in abstract", "translation": "本文引入并研究了最大平均子张量（$p$-MAS）问题，该问题旨在从给定大小为$N$的随机张量（两者都为$p$阶）中找到一个大小为$k$的子张量，使其条目之和最大。我们受到近期关于该问题矩阵情况的工作的启发，在该工作中，在渐近区域$1 \text{ \textless\textless } k \text{ \textless\textless } N$中，多种平衡态和非平衡态性质已被解析表征，并观察到了一种令人费解的现象，即聚类平衡相与能在此相中生成子矩阵的有效算法共存。在这里，我们将之前关于矩阵情况的平衡态和算法性质的结果扩展到张量情况。我们表明，张量情况具有与矩阵情况相似的平衡相图，并且对于所考虑的算法，总体上具有相似的现象。此外，我们使用重叠间隙性质和Franz-Parisi分析考虑了非平衡态景观性质，并讨论了其对平均情况算法硬度的影响或缺乏影响。", "summary": "本文引入并研究了最大平均子张量（$p$-MAS）问题，旨在寻找给定随机张量中条目和最大的子张量。研究将先前关于矩阵情况的平衡态和算法性质扩展到张量情况，发现张量情况具有相似的平衡相图和算法现象。此外，论文还利用重叠间隙性质和Franz-Parisi分析探讨了非平衡态景观性质及其对算法硬度的影响。", "keywords": "最大平均子张量, 平衡态, 非平衡态, 算法硬度, 张量", "comments": "本文将最大平均子矩阵问题推广到更高阶的张量情况，扩展了相关理论。其创新点在于对张量情况的平衡态和非平衡态性质的分析，并与矩阵情况进行对比。这对于理解复杂系统中的优化问题和算法硬度具有重要意义。"}}
{"id": "2506.15537", "title": "Automatic Metadata Capture and Processing for High-Performance Workflows", "authors": ["Polina Shpilker", "Line Pouchard"], "summary": "Modern workflows run on increasingly heterogeneous computing architectures\nand with this heterogeneity comes additional complexity. We aim to apply the\nFAIR principles for research reproducibility by developing software to collect\nmetadata annotations for workflows run on HPC systems. We experiment with two\npossible formats to uniformly store these metadata, and reorganize the\ncollected metadata to be as easy to use as possible for researchers studying\ntheir workflow performance.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15537v1", "AI": {"title_translation": "高性能工作流的自动化元数据捕获与处理", "tldr": "开发软件以捕获和处理高性能计算工作流的元数据，以提高研究可重复性并便于性能分析。", "motivation": "现代工作流在日益异构的计算架构上运行，导致复杂性增加。本研究旨在通过收集元数据来应用FAIR原则，从而提高研究的可重复性。", "method": "开发了用于收集高性能计算系统上工作流元数据注释的软件。实验了两种统一存储元数据的格式，并重新组织收集到的元数据，使其尽可能易于研究人员用于工作流性能研究。", "result": "成功开发了用于收集HPC工作流元数据的软件，并探索了两种存储格式以及元数据重组方法，旨在提高元数据的可用性。", "conclusion": "通过自动化元数据捕获和处理，能够将FAIR原则应用于HPC工作流，从而提高研究可重复性，并为研究人员分析工作流性能提供便利。", "translation": "现代工作流在日益异构的计算架构上运行，随之而来的是额外的复杂性。我们旨在通过开发软件来收集在高性能计算（HPC）系统上运行的工作流的元数据注释，从而应用FAIR原则以实现研究可重复性。我们实验了两种可能的格式来统一存储这些元数据，并重新组织收集到的元数据，使其尽可能方便研究人员用于研究其工作流性能。", "summary": "本文旨在解决现代异构计算架构下工作流的复杂性问题，通过开发软件自动捕获HPC工作流元数据。研究目标是应用FAIR原则提高研究可重复性，并探索了两种元数据存储格式，同时优化了元数据组织方式，旨在帮助研究人员更便捷地分析工作流性能。", "keywords": "元数据捕获, 高性能工作流, FAIR原则, 可重复性, 性能分析", "comments": "该论文关注高性能计算领域的重要问题，即元数据管理和可重复性。其创新点在于将FAIR原则应用于工作流元数据捕获，并强调了元数据对性能分析的重要性。"}}
{"id": "2506.14837", "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction", "authors": ["Chengzhi Xu", "Yuyang Wang", "Lai Wei", "Lichao Sun", "Weiran Huang"], "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14837v1", "AI": {"title_translation": "通过结构化指令改进图表到代码生成的迭代细化", "tldr": "多模态大语言模型（MLLMs）在图表到代码生成任务上表现不佳。本文提出了ChartIR，一种基于结构化指令的迭代细化方法，将任务分解为视觉理解和代码翻译，并采用两阶段（初始生成和迭代细化）流程，在Qwen2-VL和GPT-4o上均取得了优异性能。", "motivation": "多模态大语言模型（MLLMs）在图表到代码生成任务上的表现不佳，该任务不仅需要精确的视觉理解，还需要将视觉元素准确地翻译成结构化代码。直接提示MLLMs执行这项复杂任务通常会产生不令人满意的结果。", "method": "本文提出了ChartIR，一种基于结构化指令的迭代细化方法。首先，将任务区分为视觉理解和代码翻译。为了实现视觉理解，设计了两种结构化指令：描述指令（捕捉参考图表的视觉元素）和差异指令（描述参考图表与生成图表之间的差异），这些指令有效地将视觉特征转化为语言表示。其次，将整个图表生成流程分解为两个阶段：初始代码生成和迭代细化，从而实现最终输出的逐步增强。", "result": "实验结果表明，与其它方法相比，ChartIR在开源模型Qwen2-VL和闭源模型GPT-4o上均取得了卓越的性能。", "conclusion": "本文提出的ChartIR方法，通过结构化指令和两阶段流水线，有效提升了多模态大语言模型在图表到代码生成任务上的性能。", "translation": "最近，多模态大语言模型（MLLMs）因其强大的视觉理解能力而受到越来越多的研究关注。虽然它们在各种视觉任务上取得了令人印象深刻的成果，但它们在图表到代码生成方面的表现仍然不尽如人意。这项任务要求MLLMs生成可重现给定图表的可执行代码，这不仅需要精确的视觉理解，还需要将视觉元素准确地翻译成结构化代码。直接提示MLLMs执行这项复杂任务通常会产生不令人满意的结果。为了解决这一挑战，我们提出了ChartIR，一种基于结构化指令的迭代细化方法。首先，我们区分了两个任务：视觉理解和代码翻译。为了完成视觉理解部分，我们设计了两种类型的结构化指令：描述和差异。描述指令捕捉参考图表的视觉元素，而差异指令则描述了参考图表与生成图表之间的差异。这些指令有效地将视觉特征转化为语言表示，从而促进后续的代码翻译过程。其次，我们将整个图表生成流程分解为两个阶段：初始代码生成和迭代细化，从而实现最终输出的逐步增强。实验结果表明，与其它方法相比，我们的方法在开源模型Qwen2-VL和闭源模型GPT-4o上均取得了卓越的性能。", "summary": "本文提出了一种名为ChartIR的迭代细化方法，旨在提升多模态大语言模型（MLLMs）在图表到代码生成任务上的性能。鉴于MLLMs在此任务中表现不佳，ChartIR将任务分解为视觉理解和代码翻译两个部分。它通过设计描述和差异两种结构化指令，将视觉特征有效转化为语言表示，并采用初始代码生成和迭代细化两阶段流水线逐步优化输出。实验证明，ChartIR在Qwen2-VL和GPT-4o模型上均展现出优于现有方法的性能。", "keywords": "图表到代码生成, 迭代细化, 结构化指令, 多模态大语言模型, 视觉理解", "comments": "该论文的创新点在于其提出的结构化指令设计（描述和差异指令）用于视觉理解，以及将图表生成分解为初始生成和迭代细化两个阶段的流水线。这种方法有效解决了直接提示MLLMs进行复杂图表到代码生成任务的局限性，通过显式地分解任务（视觉理解和代码翻译）来提升性能，具有重要的实践意义。"}}
{"id": "2506.15623", "title": "Minding the Politeness Gap in Cross-cultural Communication", "authors": ["Yuka Machino", "Matthias Hofer", "Max Siegel", "Joshua B. Tenenbaum", "Robert D. Hawkins"], "summary": "Misunderstandings in cross-cultural communication often arise from subtle\ndifferences in interpretation, but it is unclear whether these differences\narise from the literal meanings assigned to words or from more general\npragmatic factors such as norms around politeness and brevity. In this paper,\nwe report three experiments examining how speakers of British and American\nEnglish interpret intensifiers like \"quite\" and \"very.\" To better understand\nthese cross-cultural differences, we developed a computational cognitive model\nwhere listeners recursively reason about speakers who balance informativity,\npoliteness, and utterance cost. Our model comparisons suggested that\ncross-cultural differences in intensifier interpretation stem from a\ncombination of (1) different literal meanings, (2) different weights on\nutterance cost. These findings challenge accounts based purely on semantic\nvariation or politeness norms, demonstrating that cross-cultural differences in\ninterpretation emerge from an intricate interplay between the two.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15623v1", "AI": {"title_translation": "弥合跨文化交流中的礼貌鸿沟", "tldr": "本研究通过实验和计算模型发现，英美英语使用者对程度副词的跨文化理解差异源于词语字面意义和表达成本权重的结合，而非单一的语义或礼貌规范。", "motivation": "跨文化交流中的误解常源于解释上的细微差异，但尚不清楚这些差异是源于词语的字面意义还是更普遍的语用因素（如礼貌和简洁规范）。", "method": "报告了三项实验，考察英式英语和美式英语使用者如何解释“quite”和“very”等程度副词。开发了一个计算认知模型，该模型中听者递归地推断说话者如何平衡信息量、礼貌和言语成本。", "result": "模型比较表明，程度副词解释的跨文化差异源于(1)不同的字面意义和(2)不同的言语成本权重。这些发现挑战了纯粹基于语义变异或礼貌规范的解释。", "conclusion": "跨文化解释的差异源于语义和礼貌之间复杂的相互作用。", "translation": "跨文化交流中的误解往往源于解释上的细微差异，但目前尚不清楚这些差异是源于词语的字面意义，还是源于更普遍的语用因素，例如关于礼貌和简洁的规范。本文报告了三项实验，考察了英式英语和美式英语使用者如何解释“quite”和“very”等程度副词。为了更好地理解这些跨文化差异，我们开发了一个计算认知模型，其中听者递归地推断说话者如何平衡信息量、礼貌和言语成本。我们的模型比较表明，程度副词解释的跨文化差异源于(1)不同的字面意义和(2)不同的言语成本权重。这些发现挑战了纯粹基于语义变异或礼貌规范的解释，表明跨文化解释的差异源于两者之间复杂的相互作用。", "summary": "本研究通过对英美英语使用者解释程度副词的实验和计算认知模型分析，探讨了跨文化交流中误解的来源。结果表明，跨文化差异并非单纯源于语义或礼貌规范，而是由词语的字面意义和表达成本权重共同作用的复杂结果。", "keywords": "跨文化交流, 礼貌, 语用学, 程度副词, 计算认知模型", "comments": "这篇论文的创新之处在于，它结合了实验研究和计算认知模型来探究跨文化交流中的语用差异，特别是对礼貌和信息传递的权衡。它挑战了以往单一归因的解释，揭示了语义和语用因素之间复杂的相互作用，为理解跨文化沟通提供了更细致的视角。"}}
{"id": "2506.15447", "title": "Model Predictive Path-Following Control for a Quadrotor", "authors": ["David Leprich", "Mario Rosenfelder", "Mario Hermle", "Jingshan Chen", "Peter Eberhard"], "summary": "Automating drone-assisted processes is a complex task. Many solutions rely on\ntrajectory generation and tracking, whereas in contrast, path-following control\nis a particularly promising approach, offering an intuitive and natural\napproach to automate tasks for drones and other vehicles. While different\nsolutions to the path-following problem have been proposed, most of them lack\nthe capability to explicitly handle state and input constraints, are formulated\nin a conservative two-stage approach, or are only applicable to linear systems.\nTo address these challenges, the paper is built upon a Model Predictive\nControl-based path-following framework and extends its application to the\nCrazyflie quadrotor, which is investigated in hardware experiments. A cascaded\ncontrol structure including an underlying attitude controller is included in\nthe Model Predictive Path-Following Control formulation to meet the challenging\nreal-time demands of quadrotor control. The effectiveness of the proposed\nmethod is demonstrated through real-world experiments, representing, to the\nbest of the authors' knowledge, a novel application of this MPC-based\npath-following approach to the quadrotor. Additionally, as an extension to the\noriginal method, to allow for deviations of the path in cases where the precise\nfollowing of the path might be overly restrictive, a corridor path-following\napproach is presented.", "comment": "15 pages, 11 figures, submitted to PAMM 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15447v1", "AI": {"title_translation": "四旋翼飞行器的模型预测路径跟踪控制", "tldr": "本文提出了一种基于模型预测控制的路径跟踪方法，用于四旋翼飞行器，并通过硬件实验验证了其有效性，并引入了走廊路径跟踪方法以增加灵活性。", "motivation": "现有的无人机自动化解决方案多依赖轨迹生成和跟踪，而路径跟踪更直观自然。但现有路径跟踪方法通常缺乏明确处理状态和输入约束的能力，采用保守的两阶段方法，或仅适用于线性系统。", "method": "论文基于模型预测控制（MPC）的路径跟踪框架，并将其应用于Crazyflie四旋翼飞行器。为了满足四旋翼控制的实时性要求，MPC路径跟踪控制公式中包含了一个包含底层姿态控制器的级联控制结构。此外，作为对原始方法的扩展，提出了走廊路径跟踪方法，允许路径偏差以避免过度限制。", "result": "通过实际硬件实验证明了所提出方法的有效性。据作者所知，这是MPC路径跟踪方法在四旋翼飞行器上的首次应用。", "conclusion": "所提出的基于模型预测控制的路径跟踪方法能够有效控制四旋翼飞行器，克服了现有方法的局限性，并通过走廊路径跟踪增加了灵活性，为无人机自动化提供了有前景的解决方案。", "translation": "无人机辅助过程的自动化是一项复杂的任务。许多解决方案依赖于轨迹生成和跟踪，而相比之下，路径跟踪控制是一种特别有前途的方法，为无人机和其他车辆的自动化任务提供了一种直观而自然的方法。虽然已经提出了解决路径跟踪问题的不同方案，但它们大多缺乏明确处理状态和输入约束的能力，或者以保守的两阶段方法制定，或者仅适用于线性系统。为了应对这些挑战，本文基于模型预测控制的路径跟踪框架，并将其应用扩展到Crazyflie四旋翼飞行器，并在硬件实验中进行了研究。在模型预测路径跟踪控制公式中包含了一个包含底层姿态控制器的级联控制结构，以满足四旋翼飞行器控制具有挑战性的实时需求。所提出方法的有效性通过实际实验得到证明，据作者所知，这是这种基于MPC的路径跟踪方法在四旋翼飞行器上的首次应用。此外，作为对原始方法的扩展，为了允许路径偏差，在精确遵循路径可能过于严格的情况下，提出了一种走廊路径跟踪方法。", "summary": "本文提出了一种基于模型预测控制（MPC）的路径跟踪框架，用于自动化四旋翼飞行器任务。该方法解决了现有路径跟踪方案在处理约束、实时性和适用性方面的不足。通过在Crazyflie四旋翼飞行器上的硬件实验，验证了所提出方法的有效性，该方法包含一个级联控制结构以满足实时需求。此外，论文还引入了走廊路径跟踪方法，以在必要时允许路径偏差，增加了系统的灵活性和实用性。", "keywords": "模型预测控制, 路径跟踪, 四旋翼飞行器, 实时控制, 走廊路径跟踪", "comments": "本文的创新之处在于将模型预测控制路径跟踪方法首次应用于四旋翼飞行器，并解决了实时性、状态约束和输入约束等实际问题。引入的级联控制结构和走廊路径跟踪方法增强了方法的实用性和灵活性。这项工作为无人机自动化任务提供了一种有前景且经过实际验证的控制策略。"}}
{"id": "2506.15647", "title": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yang Deng", "Xingyu Sui", "Yulin Hu", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "summary": "Recent advancements in large reasoning models (LRMs) have significantly\nenhanced language models' capabilities in complex problem-solving by emulating\nhuman-like deliberative thinking. However, these models often exhibit\noverthinking (i.e., the generation of unnecessarily verbose and redundant\ncontent), which hinders efficiency and inflates inference cost. In this work,\nwe explore the representational and behavioral origins of this inefficiency,\nrevealing that LRMs inherently possess the capacity for more concise reasoning.\nEmpirical analyses show that correct reasoning paths vary significantly in\nlength, and the shortest correct responses often suffice, indicating untapped\nefficiency potential. Exploiting these findings, we propose two lightweight\nmethods to enhance LRM efficiency. First, we introduce Efficiency Steering, a\ntraining-free activation steering technique that modulates reasoning behavior\nvia a single direction in the model's representation space. Second, we develop\nSelf-Rewarded Efficiency RL, a reinforcement learning framework that\ndynamically balances task accuracy and brevity by rewarding concise correct\nsolutions. Extensive experiments on seven LRM backbones across multiple\nmathematical reasoning benchmarks demonstrate that our methods significantly\nreduce reasoning length while preserving or improving task performance. Our\nresults highlight that reasoning efficiency can be improved by leveraging and\nguiding the intrinsic capabilities of existing models in a self-guided manner.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15647v1", "AI": {"title_translation": "探索和利用大型推理模型中的内在效率以实现自我引导的效率提升", "tldr": "本文探讨大型推理模型中过长推理的内在效率潜力，并提出两种轻量级方法（效率引导和自奖励效率强化学习）来减少推理长度同时保持或提高性能。", "motivation": "大型推理模型在复杂问题解决方面表现出色，但常有过长推理（即产生不必要冗长和冗余内容）的问题，这会降低效率并增加推理成本。", "method": "首先，探索了大型推理模型中低效率的表征和行为根源，揭示模型本身具有更简洁推理的能力。其次，提出了两种轻量级方法：1. 效率引导（Efficiency Steering），一种无需训练的激活引导技术，通过模型表征空间中的单一方向来调节推理行为。2. 自奖励效率强化学习（Self-Rewarded Efficiency RL），一个强化学习框架，通过奖励简洁的正确解决方案来动态平衡任务准确性和简洁性。", "result": "在七个大型推理模型骨干网络和多个数学推理基准上进行了广泛实验，结果表明所提出的方法显著减少了推理长度，同时保持或提高了任务性能。", "conclusion": "推理效率可以通过利用和引导现有模型内在能力以自我引导的方式来提高。", "translation": "大型推理模型（LRMs）的最新进展通过模仿人类的深思熟虑思维，显著增强了语言模型解决复杂问题的能力。然而，这些模型经常表现出“过度思考”（即生成不必要的冗长和冗余内容），这阻碍了效率并增加了推理成本。在这项工作中，我们探索了这种低效率的表征和行为根源，揭示了LRMs本身具有更简洁推理的能力。经验分析表明，正确的推理路径长度差异很大，最短的正确响应通常就足够了，这表明存在未被开发的效率潜力。利用这些发现，我们提出了两种轻量级方法来提高LRM的效率。首先，我们引入了“效率引导”（Efficiency Steering），这是一种无需训练的激活引导技术，通过模型表征空间中的单一方向来调节推理行为。其次，我们开发了“自奖励效率强化学习”（Self-Rewarded Efficiency RL），这是一种强化学习框架，通过奖励简洁的正确解决方案来动态平衡任务准确性和简洁性。在多个数学推理基准上，对七个LRM骨干网络进行的广泛实验表明，我们的方法显著减少了推理长度，同时保持或提高了任务性能。我们的结果强调，可以通过利用和引导现有模型内在能力以自我引导的方式来提高推理效率。", "summary": "本文针对大型推理模型中存在的“过度思考”导致的低效率和高成本问题，深入探究了其内在原因，并发现模型本身具备更简洁推理的潜力。基于此，论文提出了两种轻量级方法：无需训练的“效率引导”和基于奖励的“自奖励效率强化学习”，旨在通过调节模型行为来减少推理长度。实验证明，这些方法在保持或提升性能的同时，有效提高了大型推理模型的推理效率。", "keywords": "大型推理模型, 推理效率, 效率引导, 强化学习, 自我引导", "comments": "这篇论文通过深入分析大型推理模型“过度思考”的内在机制，提出了一种新颖的视角，即模型本身具有内在的效率潜力。其创新之处在于提出了两种轻量级且有效的自我引导方法，特别是无需训练的效率引导，这对于实际应用具有重要意义，因为它减少了训练成本。该研究对于提升大型模型推理效率和降低部署成本具有重要价值。"}}
{"id": "2506.15470", "title": "Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "summary": "With the deployment of large antenna arrays at high frequency bands, future\nwireless communication systems are likely to operate in the radiative\nnear-field. Unlike far-field beam steering, near-field beams can be focused\nwithin a spatial region of finite depth, enabling spatial multiplexing in both\nthe angular and range dimensions. This paper derives the beamdepth for a\ngeneralized uniform rectangular array (URA) and investigates how array geometry\ninfluences the near-field beamdepth and the limits where near-field\nbeamfocusing is achievable. To characterize the near-field boundary in terms of\nbeamfocusing and spatial multiplexing gains, we define the effective\nbeamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis\nreveals that while a square URA achieves the narrowest beamdepth, the EBRD is\nmaximized for a wide or tall URA. However, despite its narrow beamdepth, a\nsquare URA may experience a reduction in multiuser sum rate due to its severely\nconstrained EBRD. Simulation results confirm that a wide or tall URA achieves a\nsum rate of 3.5 X more than that of a square URA, benefiting from the extended\nEBRD and improved spatial multiplexing capabilities.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15470v1", "AI": {"title_translation": "分析 URA 几何结构以增强空间复用和扩展近场覆盖", "tldr": "本文分析了均匀矩形阵列（URA）的几何结构如何影响近场波束深度和近场波束聚焦的极限，并定义了有效波束聚焦瑞利距离（EBRD）。研究发现，虽然方形 URA 实现了最窄的波束深度，但宽或高 URA 最大化了 EBRD，从而在多用户和空间复用方面表现更优。", "motivation": "随着高频段大型天线阵列的部署，未来的无线通信系统很可能在辐射近场中运行。与远场波束控制不同，近场波束可以在有限深度的空间区域内聚焦，从而在角度和距离维度上实现空间复用。本文旨在分析阵列几何如何影响近场波束深度和近场波束聚焦的可实现极限，以表征近场边界的波束聚焦和空间复用增益。", "method": "本文推导了广义均匀矩形阵列（URA）的波束深度，并研究了阵列几何如何影响近场波束深度和近场波束聚焦的可实现极限。为了表征近场边界的波束聚焦和空间复用增益，本文定义了广义 URA 的有效波束聚焦瑞利距离（EBRD）。", "result": "分析表明，虽然方形 URA 实现了最窄的波束深度，但宽或高 URA 的 EBRD 被最大化。尽管方形 URA 的波束深度较窄，但由于其受严格限制的 EBRD，可能导致多用户和速率降低。仿真结果证实，宽或高 URA 的和速率比方形 URA 高 3.5 倍，这得益于其扩展的 EBRD 和改进的空间复用能力。", "conclusion": "在近场通信中，宽或高 URA 相较于方形 URA 能够实现更高的和速率和更好的空间复用能力，这主要得益于其扩展的有效波束聚焦瑞利距离（EBRD）。", "translation": "随着高频段大型天线阵列的部署，未来的无线通信系统很可能在辐射近场中运行。与远场波束控制不同，近场波束可以在有限深度的空间区域内聚焦，从而在角度和距离维度上实现空间复用。本文推导了广义均匀矩形阵列（URA）的波束深度，并研究了阵列几何如何影响近场波束深度和近场波束聚焦的可实现极限。为了表征近场边界的波束聚焦和空间复用增益，我们定义了广义 URA 的有效波束聚焦瑞利距离（EBRD）。我们的分析表明，虽然方形 URA 实现了最窄的波束深度，但宽或高 URA 的 EBRD 被最大化。然而，尽管其波束深度较窄，方形 URA 可能会因为其严格受限的 EBRD 而导致多用户和速率降低。仿真结果证实，宽或高 URA 的和速率比方形 URA 高 3.5 倍，这得益于其扩展的 EBRD 和改进的空间复用能力。", "summary": "本文研究了在大型天线阵列高频段近场通信中，均匀矩形阵列（URA）几何结构对波束深度和空间复用能力的影响。通过推导广义 URA 的波束深度并定义有效波束聚焦瑞利距离（EBRD），研究发现方形 URA 虽有最窄波束深度，但其受限的 EBRD 限制了多用户和速率。相比之下，宽或高 URA 能最大化 EBRD，从而显著提升空间复用能力和和速率，仿真显示其和速率可达方形 URA 的 3.5 倍。", "keywords": "URA 几何, 空间复用, 近场通信, 波束聚焦, EBRD", "comments": "本文深入分析了大型天线阵列在近场通信中的关键特性，特别是 URA 几何结构对波束聚焦和空间复用性能的影响。创新性在于定义了 EBRD 来量化近场边界，并揭示了宽/高 URA 在和速率上的显著优势。这项工作对于未来高频段无线通信系统，特别是需要高效近场空间复用的场景，具有重要的指导意义。"}}
{"id": "2506.14794", "title": "Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors", "authors": ["Henrik Klagges", "Robert Dahlke", "Fabian Klemm", "Benjamin Merkel", "Daniel Klingmann", "David A. Reiss", "Dan Zecha"], "summary": "Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM\nduring pretraining is extremely expensive and seems inefficient. To better\nleverage the huge investments made into pretrained models, we develop the new\n\"Assembly-of-Experts\" (AoE) construction method to create capable child\nvariants of existing Mixture-of-Experts parent models in linear time. Model\nweight tensors get interpolated individually, allowing to enhance or suppress\nsemantic features of the parents.\n  Varying the proportion of weights taken from the parent models, we observe\nsome properties of the AoE child model changing gradually, while other\nbehavioral traits emerge with a sharp transition. Surprisingly, nearly every\ngenerated model is functional and capable, which makes searching the model\nspace straightforward.\n  We construct the DeepSeek R1T \"Chimera\", a 671B open-weights hybrid model\ncombining DeepSeek's V3-0324 and R1 model variants. The child inherits only the\nrouted expert tensors of R1, but still achieves about R1-level intelligence. At\nthe same time, it uses about 40\\% fewer output tokens, close to V3 speed.\nConstructed without any fine-tuning or distillation, the Chimera exhibits\nsurprisingly compact, orderly reasoning compared to its parent models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14794v1", "AI": {"title_translation": "专家集合：线性时间构建具有涌现和适应性行为的奇美拉LLM变体", "tldr": "提出了一种名为“专家集合”（AoE）的新方法，可以在线性时间内构建现有专家混合（MoE）父模型的子变体，以降低大型语言模型（LLM）预训练的成本和提高效率。", "motivation": "LLM预训练中计算一个8位权重需要$10^{13}$-$10^{15}$ FLOPs，成本极高且效率低下。为了更好地利用对预训练模型的巨大投资，需要一种更有效的方法来创建现有模型的变体。", "method": "开发了“专家集合”（AoE）构建方法，以线性时间创建现有专家混合（MoE）父模型的子变体。该方法单独插值模型权重张量，允许增强或抑制父模型的语义特征。通过改变从父模型中获取权重的比例，观察到子模型的某些属性逐渐变化，而其他行为特征则发生急剧转变。", "result": "构建的几乎每个模型都功能强大且有能力，使得模型空间搜索变得简单。构建了DeepSeek R1T “奇美拉”（Chimera），一个671B的开源混合模型，结合了DeepSeek的V3-0324和R1模型变体。奇美拉只继承了R1的路由专家张量，但仍达到了大约R1级别的智能。同时，它使用的输出令牌减少了约40%，接近V3的速度。在未经任何微调或蒸馏的情况下，奇美拉与父模型相比表现出惊人的紧凑、有序推理能力。", "conclusion": "“专家集合”（AoE）方法能够高效地构建功能强大且有能力的LLM变体，如“奇美拉”，该变体在保持高智能水平的同时显著降低了计算成本，并展现出优异的推理能力，使得LLM的探索和应用更为便捷。", "translation": "在预训练期间，计算LLM中一个8位权重需要$10^{13}$-$10^{15}$ FLOPs，这极其昂贵且似乎效率低下。为了更好地利用对预训练模型的巨大投资，我们开发了一种新的“专家集合”（AoE）构建方法，以线性时间创建现有专家混合父模型的能力子变体。模型权重张量被单独插值，从而可以增强或抑制父模型的语义特征。通过改变从父模型中获取权重的比例，我们观察到AoE子模型的某些属性逐渐变化，而其他行为特征则发生急剧转变。令人惊讶的是，几乎每个生成的模型都功能强大且有能力，这使得模型空间搜索变得简单。我们构建了DeepSeek R1T“奇美拉”，一个671B的开源混合模型，结合了DeepSeek的V3-0324和R1模型变体。该子模型仅继承了R1的路由专家张量，但仍达到了大约R1级别的智能。同时，它使用的输出令牌减少了约40%，接近V3的速度。在未经任何微调或蒸馏的情况下，奇美拉与父模型相比表现出惊人的紧凑、有序推理能力。", "summary": "本研究提出了一种名为“专家集合”（AoE）的新颖方法，旨在以线性时间高效构建大型语言模型（LLM）的子变体，以应对LLM预训练高昂的计算成本。AoE方法通过单独插值现有专家混合（MoE）父模型的权重张量，能够增强或抑制语义特征，并发现即使在权重比例变化时，生成的模型也普遍功能良好。作为一个实例，研究团队构建了671B的DeepSeek R1T“奇美拉”模型，该模型结合了DeepSeek V3-0324和R1的优点，在继承R1级智能的同时，显著减少了输出令牌使用量并提高了速度，且在未经微调或蒸馏的情况下展现出卓越的推理能力。", "keywords": "专家集合, LLM, 奇美拉, 线性时间, 模型构建", "comments": "该论文提出了一种创新的“专家集合”（AoE）方法，通过线性时间构建LLM子变体，显著降低了开发成本和复杂性。其核心创新在于通过权重张量插值实现语义特征的灵活调整，并发现生成的模型具有普遍的功能性。尤其重要的是，Chimera模型的构建展示了在不进行额外微调或蒸馏的情况下，如何高效地结合现有模型的优势，实现性能与效率的平衡，这对于推动LLM的实际应用具有重要意义。"}}
{"id": "2506.15118", "title": "CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records", "authors": ["Junke Wang", "Hongshun Ling", "Li Zhang", "Longqian Zhang", "Fang Wang", "Yuan Gao", "Zhi Li"], "summary": "Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.", "comment": "20 pages,5 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15118v1", "AI": {"title_translation": "CKD-EHR：面向电子健康记录的临床知识蒸馏", "tldr": "CKD-EHR框架通过知识蒸馏技术，利用大型语言模型作为教师模型，将临床知识高效准确地传递给轻量级学生模型，显著提升了电子健康记录（EHR）疾病预测的准确性和推理速度。", "motivation": "现有基于电子健康记录（EHR）的疾病预测大型语言模型面临医疗知识表示不足和临床部署效率低下的挑战。", "method": "本研究提出了CKD-EHR框架，通过知识蒸馏技术实现高效准确的疾病风险预测。具体而言，首先使用医学知识增强数据对大型语言模型Qwen2.5-7B进行微调作为教师模型；然后通过多粒度注意力蒸馏机制生成可解释的软标签；最后将蒸馏出的知识转移到轻量级BERT学生模型。", "result": "在MIMIC-III数据集上，CKD-EHR显著优于基线模型：诊断准确率提高9%，F1分数提高27%，推理速度提高22.2倍。", "conclusion": "CKD-EHR提供了一种实用的技术方法，通过大幅提高资源利用效率、诊断准确性和及时性，解决了现有EHR疾病预测模型的挑战，为临床环境中的资源优化提供了支持。", "translation": "电子健康记录（EHR）疾病预测模型在促进精准医疗和实现早期干预方面展现出显著的临床价值。然而，现有的大型语言模型面临两大挑战：医疗知识表示不足和临床部署效率低下。为解决这些挑战，本研究提出了CKD-EHR（面向EHR的临床知识蒸馏）框架，通过知识蒸馏技术实现高效准确的疾病风险预测。具体而言，首先使用医学知识增强数据对大型语言模型Qwen2.5-7B进行微调，作为教师模型。然后，通过多粒度注意力蒸馏机制生成可解释的软标签。最后，将蒸馏出的知识转移到轻量级BERT学生模型。实验结果表明，在MIMIC-III数据集上，CKD-EHR显著优于基线模型：诊断准确率提高了9%，F1分数提高了27%，并实现了22.2倍的推理速度提升。这一创新解决方案不仅大大提高了资源利用效率，而且显著增强了诊断的准确性和及时性，为临床环境中的资源优化提供了实用的技术方法。本研究的代码和数据可在https://github.com/209506702/CKD_EHR获取。", "summary": "本研究提出了CKD-EHR框架，旨在解决现有EHR疾病预测大型语言模型在医疗知识表示和临床部署效率方面的不足。该框架采用知识蒸馏技术，将经过医学知识微调的大型语言模型Qwen2.5-7B作为教师模型，通过多粒度注意力蒸馏机制生成软标签，并将知识转移至轻量级BERT学生模型。实验结果显示，CKD-EHR在MIMIC-III数据集上显著提升了诊断准确率和F1分数，并大幅提高了推理速度，为临床资源优化提供了高效且准确的解决方案。", "keywords": "知识蒸馏, 电子健康记录, 疾病预测, 大型语言模型, 临床部署", "comments": "CKD-EHR创新性地将知识蒸馏应用于EHR疾病预测领域，有效解决了大型语言模型在医疗领域部署的效率和知识表示问题。其通过轻量级模型实现高性能，对于资源受限的临床环境具有重要实践意义。该方法的可解释性软标签机制也值得关注。"}}
{"id": "2506.15077", "title": "A Nonconforming Finite Element Method for Elliptic Interface Problems on Locally Anisotropic Meshes", "authors": ["Hua Wang", "Qichen Zhang"], "summary": "We propose a new nonconforming \\(P_1\\) finite element method for elliptic\ninterface problems. The method is constructed on a locally anisotropic mixed\nmesh, which is generated by fitting the interface through a simple connection\nof intersection points on an interface-unfitted background mesh, as introduced\nin \\cite{Hu2021optimal}. We first establish interpolation error estimates on\nquadrilateral elements satisfying the regular decomposition property (RDP).\nBuilding on this, the main contribution of this work is a novel consistency\nerror analysis for nonconforming elements, which removes the quasi-regularity\nassumption commonly required in existing approaches. Numerical results confirm\nthe theoretical convergence rates and demonstrate the robustness and accuracy\nof the proposed method.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15077v1", "AI": {"title_translation": "局部各向异性网格上椭圆界面问题的非协调有限元方法", "tldr": "提出了一种新的非协调P1有限元方法，用于解决局部各向异性网格上的椭圆界面问题，并通过新颖的一致性误差分析消除了准正则性假设，数值结果验证了其收敛性和鲁棒性。", "motivation": "解决椭圆界面问题。", "method": "提出了一种新的非协调P1有限元方法，该方法构建在局部各向异性混合网格上，该网格通过在不拟合界面的背景网格上简单连接交点来拟合界面。研究中首先建立了满足正则分解性质（RDP）的四边形单元上的插值误差估计，并提出了一种新的非协调单元一致性误差分析，消除了现有方法中常见的准正则性假设。", "result": "数值结果证实了理论收敛速率，并证明了所提方法的鲁棒性和准确性。", "conclusion": "所提出的非协调有限元方法在局部各向异性网格上能够有效、准确地解决椭圆界面问题，并且其新颖的误差分析克服了现有方法的局限性。", "translation": "我们提出了一种用于椭圆界面问题的新型非协调\\(P_1\\)有限元方法。该方法构建在局部各向异性混合网格上，该网格通过在不拟合界面的背景网格上简单连接交点来拟合界面，如参考文献\\cite{Hu2021optimal}中所述。我们首先建立了满足正则分解性质（RDP）的四边形单元上的插值误差估计。在此基础上，这项工作的主要贡献是针对非协调单元的新颖一致性误差分析，该分析消除了现有方法中通常需要的准正则性假设。数值结果证实了理论收敛速率，并证明了所提方法的鲁棒性和准确性。", "summary": "这篇论文提出了一种新的非协调P1有限元方法，专门用于解决局部各向异性网格上的椭圆界面问题。该方法利用一种特殊的混合网格，并通过新颖的一致性误差分析，成功地去除了传统方法中常见的准正则性假设。数值实验验证了该方法的理论收敛性、鲁棒性和准确性。", "keywords": "非协调有限元, 椭圆界面问题, 局部各向异性网格, 误差分析, P1元", "comments": "这项工作的创新之处在于提出了一种新的非协调P1有限元方法，特别是在局部各向异性网格上的应用。其主要贡献在于新颖的一致性误差分析，成功消除了现有方法中常见的准正则性假设，这对于有限元方法的理论分析和实际应用具有重要意义。该方法的鲁棒性和准确性也通过数值结果得到了验证。"}}
{"id": "2506.15258", "title": "Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference", "authors": ["Jonghun Kim", "Gyeongdeok Jo", "Shinyoung Ra", "Hyunjin Park"], "summary": "Medical imaging data contain sensitive patient information requiring strong\nprivacy protection. Many analytical setups require data to be sent to a server\nfor inference purposes. Homomorphic encryption (HE) provides a solution by\nallowing computations to be performed on encrypted data without revealing the\noriginal information. However, HE inference is computationally expensive,\nparticularly for large images (e.g., chest X-rays). In this study, we propose\nan HE inference framework for medical images that uses VQGAN to compress images\ninto latent representations, thereby significantly reducing the computational\nburden while preserving image quality. We approximate the activation functions\nwith lower-degree polynomials to balance the accuracy and efficiency in\ncompliance with HE requirements. We observed that a downsampling factor of\neight for compression achieved an optimal balance between performance and\ncomputational cost. We further adapted the squeeze and excitation module, which\nis known to improve traditional CNNs, to enhance the HE framework. Our method\nwas tested on two chest X-ray datasets for multi-label classification tasks\nusing vanilla CNN backbones. Although HE inference remains relatively slow and\nintroduces minor performance differences compared with unencrypted inference,\nour approach shows strong potential for practical use in medical images", "comment": "11 pages, 5 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15258v1", "AI": {"title_translation": "在潜在空间中利用同态加密神经网络推理进行隐私保护的胸部X射线分类", "tldr": "该研究提出了一种基于VQGAN和同态加密的胸部X射线分类框架，通过在潜在空间进行推理显著降低了计算成本，同时保护了隐私。", "motivation": "医疗影像数据包含敏感患者信息，需要强大的隐私保护。传统的推理设置通常需要将数据发送到服务器，存在隐私泄露风险。同态加密虽然能解决在加密数据上进行计算的问题，但其计算成本高昂，尤其对于大型图像（如胸部X射线）。", "method": "本研究提出了一种用于医学图像的同态加密（HE）推理框架。该方法首先使用VQGAN将图像压缩成潜在表示，以显著降低计算负担。为了满足HE要求并平衡准确性和效率，激活函数通过低阶多项式进行近似。研究发现，八倍的下采样因子在性能和计算成本之间达到了最佳平衡。此外，该框架还适配了Squeeze and Excitation模块来增强其性能。该方法在两个胸部X射线数据集上，使用香草CNN骨干网络进行了多标签分类任务测试。", "result": "在潜在空间中进行推理显著降低了计算负担。八倍下采样因子在性能和计算成本之间实现了最佳平衡。尽管与未加密推理相比，同态加密推理仍然相对较慢并引入了轻微的性能差异，但该方法在医疗图像的实际应用中显示出强大的潜力。", "conclusion": "本研究提出的同态加密推理框架通过在潜在空间中压缩图像和优化计算，有效解决了医疗图像隐私保护中的计算效率问题。尽管HE推理仍面临速度和性能上的挑战，但该方法为在敏感医疗数据上实现隐私保护的分类提供了有潜力的实用解决方案。", "translation": "医疗影像数据包含敏感的患者信息，需要强大的隐私保护。许多分析设置要求将数据发送到服务器进行推理。同态加密（HE）通过允许在加密数据上执行计算而不泄露原始信息来提供解决方案。然而，HE推理的计算成本很高，特别是对于大型图像（例如胸部X射线）。在本研究中，我们提出了一种用于医学图像的HE推理框架，该框架使用VQGAN将图像压缩为潜在表示，从而显著降低了计算负担，同时保持了图像质量。我们使用较低阶多项式近似激活函数，以平衡符合HE要求的准确性和效率。我们观察到，八倍的压缩下采样因子在性能和计算成本之间实现了最佳平衡。我们进一步调整了Squeeze and Excitation模块（已知可以改进传统CNN）以增强HE框架。我们的方法在两个胸部X射线数据集上使用香草CNN骨干网络进行了多标签分类任务测试。尽管HE推理与未加密推理相比仍然相对较慢并引入了轻微的性能差异，但我们的方法在医疗图像的实际使用中显示出强大的潜力。", "summary": "本文提出了一种创新的隐私保护胸部X射线分类框架，该框架利用VQGAN将图像压缩到潜在空间，并结合同态加密技术进行神经网络推理。通过在压缩的潜在表示上执行计算，该方法显著降低了同态加密推理的计算成本，同时通过多项式近似激活函数和优化下采样因子来平衡准确性和效率。尽管存在速度和性能上的轻微折衷，但该研究证明了其在保护医疗图像隐私方面的实用潜力。", "keywords": "同态加密, 隐私保护, 胸部X射线, 潜在空间, 图像分类", "comments": "这篇论文的创新点在于将图像压缩技术（VQGAN）与同态加密推理相结合，有效解决了同态加密在处理大型医疗图像时计算成本过高的问题。通过在潜在空间进行操作，显著提高了效率，这对于医疗数据隐私保护具有重要意义。尽管仍存在性能和速度上的挑战，但其提出的框架为实际应用奠定了基础，展示了未来在敏感数据分析领域的巨大潜力。"}}
{"id": "2506.15170", "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem", "authors": ["Yanxu Mao", "Tiehan Cui", "Peipei Liu", "Datao You", "Hongsong Zhu"], "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15170v1", "AI": {"title_translation": "从LLMs到MLLMs到智能体：LLM生态系统中越狱攻击与防御新兴范式的综述", "tldr": "本文系统综述了LLM、MLLM和智能体生态系统中不断演进的越狱攻击与防御机制，并指出了现有研究的局限性，提出了未来的研究方向。", "motivation": "随着大型语言模型（LLMs）向多模态LLM和智能体快速发展，其能力显著增强，但也带来了日益严重的安全风险。现有综述在智能体特定安全问题、混合越狱方法分类、实验设置分析以及对最新进展的覆盖方面存在不足。", "method": "本文对LLM生态系统中日益复杂的越狱攻击和防御机制进行了系统综述。首先追溯了从LLM到MLLM再到智能体的发展轨迹，并强调了每个阶段的核心安全挑战。接着从攻击影响和可见性角度对主流越狱技术进行分类，并对代表性攻击方法、相关数据集和评估指标进行了全面分析。在防御方面，根据响应时间和技术方法组织现有策略。最后，识别并解决了现有综述的局限性，并提出了未来的研究方向。", "result": "本文系统地呈现了LLM、MLLM和智能体生态系统中越狱攻击与防御机制的复杂性。提供了对代表性攻击方法、数据集和评估指标的全面分析，以及对防御策略适用性和实施的结构化理解。此外，识别了现有综述的关键局限性，并提供了最新工作的综合，概述了未来的研究方向。", "conclusion": "本研究旨在增进对越狱机制的理解，并促进在能力日益强大的LLM背景下开发更具弹性和适应性的防御策略。", "translation": "大型语言模型（LLMs）正在从单模态系统迅速发展为多模态LLM和智能体，这显著扩展了它们的能力，同时也带来了日益严重的安全风险。本文对不断扩展的LLM生态系统中日益复杂的越狱攻击和相应的防御机制进行了系统综述。我们首先追溯了从LLMs到MLLMs再到智能体的发展轨迹，强调了每个阶段出现的核心安全挑战。接下来，我们从攻击影响和可见性角度对主流越狱技术进行分类，并对代表性攻击方法、相关数据集和评估指标进行了全面分析。在防御方面，我们根据响应时间和技术方法组织现有策略，提供了对其适用性和实施的结构化理解。此外，我们指出了现有综述中的关键局限性，例如对智能体特定安全问题的关注不足、缺乏清晰的混合越狱方法分类、缺乏对实验设置的详细分析以及对最新进展的覆盖不足。为了解决这些局限性，我们提供了最新工作的更新综合，并概述了数据集构建、评估框架优化和策略泛化等领域的未来研究方向。我们的研究旨在增进对越狱机制的理解，并促进在能力日益强大的LLMs背景下开发更具弹性和适应性的防御策略。", "summary": "本文对LLM生态系统中从LLM到MLLM再到智能体演变过程中出现的越狱攻击与防御范式进行了全面而系统的综述。文章详细分类并分析了主流攻击技术及其评估方法，并组织了现有防御策略。特别地，本研究识别并解决了现有综述的不足，例如对智能体安全、混合攻击分类和最新进展覆盖的欠缺，并提出了未来的研究方向，旨在促进更具弹性和适应性的防御策略发展。", "keywords": "LLMs, MLLMs, 智能体, 越狱攻击, 防御, 综述", "comments": "这篇综述论文的重要性在于它及时地覆盖了LLM安全领域中不断演进的新范式，特别是将研究范围扩展到了多模态LLM和智能体。它不仅系统地梳理了现有的攻击与防御策略，更重要的是，它明确指出了现有综述的局限性并提出了未来的研究方向，这对于推动该领域的进步具有重要的指导意义。其创新性体现在对未来安全挑战的前瞻性分析和对研究空白的弥补。"}}
{"id": "2506.15293", "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces", "authors": ["Francesco Chiossi", "Julian Rasch", "Robin Welsch", "Albrecht Schmidt", "Florian Michahelles"], "summary": "As robots enter collaborative workspaces, ensuring mutual understanding\nbetween human workers and robotic systems becomes a prerequisite for trust,\nsafety, and efficiency. In this position paper, we draw on the cooperation\nscenario of the AIMotive project in which a human and a cobot jointly perform\nassembly tasks to argue for a structured approach to intent communication.\nBuilding on the Situation Awareness-based Agent Transparency (SAT) framework\nand the notion of task abstraction levels, we propose a multidimensional design\nspace that maps intent content (SAT1, SAT3), planning horizon (operational to\nstrategic), and modality (visual, auditory, haptic). We illustrate how this\nspace can guide the design of multimodal communication strategies tailored to\ndynamic collaborative work contexts. With this paper, we lay the conceptual\nfoundation for a future design toolkit aimed at supporting transparent\nhuman-robot interaction in the workplace. We highlight key open questions and\ndesign challenges, and propose a shared agenda for multimodal, adaptive, and\ntrustworthy robotic collaboration in hybrid work environments.", "comment": "9 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15293v1", "AI": {"title_translation": "设计意图：工业工作空间中人机协作的多模态框架", "tldr": "本立场论文提出了一种多维设计空间，用于指导工业环境中人机协作的意图通信，旨在通过整合意图内容、规划范围和多模态来增强透明度、信任、安全和效率，并为未来设计工具包奠定概念基础。", "motivation": "随着机器人进入协作工作空间，确保人类工人与机器人系统之间的相互理解成为建立信任、保障安全和提高效率的先决条件。", "method": "本文基于情境感知代理透明度（SAT）框架和任务抽象级别概念，提出了一个多维设计空间。该空间映射了意图内容（SAT1、SAT3）、规划范围（操作级到战略级）和模态（视觉、听觉、触觉），并阐述了如何指导多模态通信策略的设计。", "result": "本文阐述了所提出的设计空间如何能够指导为动态协作工作环境量身定制的多模态通信策略的设计。此外，它为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。", "conclusion": "本文为未来支持工作场所透明人机交互的设计工具包奠定了概念基础，并提出了多模态、自适应、值得信赖的混合工作环境中机器人协作的关键开放问题、设计挑战以及共享议程。", "translation": "随着机器人进入协作工作空间，确保人类工人与机器人系统之间的相互理解成为信任、安全和效率的先决条件。在这篇立场论文中，我们借鉴了AIMotive项目中的协作场景，在该项目中，人类和协作机器人共同执行装配任务，以此论证一种结构化的意图通信方法。\n基于情境感知代理透明度（SAT）框架和任务抽象级别的概念，我们提出了一个多维设计空间，该空间映射了意图内容（SAT1、SAT3）、规划范围（操作级到战略级）和模态（视觉、听觉、触觉）。我们阐述了该空间如何能够指导为动态协作工作环境量身定制的多模态通信策略的设计。通过这篇论文，我们为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。我们强调了关键的开放问题和设计挑战，并为混合工作环境中多模态、自适应和值得信赖的机器人协作提出了一个共享议程。", "summary": "本立场论文探讨了在工业协作工作空间中实现人机相互理解的重要性，以提升信任、安全和效率。文章以AIMotive项目的装配任务场景为例，提出了一个基于情境感知代理透明度（SAT）框架的多维意图通信设计空间，该空间整合了意图内容、规划范围和通信模态。该设计空间旨在指导多模态通信策略的开发，并为未来支持透明人机交互的设计工具包奠定概念基础，同时指出了该领域面临的挑战和未来研究方向。", "keywords": "人机协作, 意图通信, 多模态交互, 设计空间, 机器人透明度", "comments": "本文提出了一种新颖的多维设计空间，用于解决工业环境中人机协作的关键挑战——意图通信。其创新之处在于将情境感知代理透明度（SAT）框架与任务抽象级别、规划范围和多模态通信相结合，提供了一个结构化的方法来指导透明的人机交互设计。这对于提高协作机器人的接受度、安全性和效率具有重要意义。作为一个立场文件，它为未来工具包的开发奠定了理论基础，并明确指出了该领域的开放问题和挑战，具有前瞻性。"}}
{"id": "2506.15595", "title": "LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters", "authors": ["Kunming Zhang", "Hanlong Liao", "Guoming Tang"], "summary": "Parallel computing with multiple GPUs has become the dominant paradigm for\nmachine learning tasks, especially those of large language models (LLMs). To\nreduce the latency incurred by inter-GPU communication, a common practice for\nparallel tasks has been to allocate GPUs based on their physical proximity.\nHowever, this long-standing assumption has notable limitations, particularly in\nlarge-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs\nis irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU\ndispatching system based on global perspectives. To tackle the difficulty of\nstoring massive GPU topology information, LiteGD adopts a computation-aware\ndesign that leverages a lightweight Transformer network trained on sampled\ndata. Our customized design for network structure ensures both transferability\nand scalability. LiteGD also employs a bidirectional tree search approach to\nfind the optimal GPU dispatching in the data generated in the previous step,\nwhich can identify near-optimal solutions while reducing search overhead. We\nimplement and evaluate LiteGD in both real and simulated GPU clusters with\nhomogeneous and heterogeneous interconnects, respectively. Experimental results\ndemonstrate that LiteGD consistently achieves high GPU bandwidth efficacy\n(approximately 90\\%) across various cluster configurations and 80\\% in\nreal-world H100 cluster, significantly outperforming conventional default and\ninterconnect topology-aware dispatching methods, particularly in large-scale\nheterogeneous environments.", "comment": "12 pages, 19 figures,7 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15595v1", "AI": {"title_translation": "LiteGD：大规模异构集群的轻量级动态GPU调度", "tldr": "LiteGD是一种轻量级动态GPU调度系统，通过Transformer和双向树搜索解决大规模异构集群中的GPU通信延迟问题，显著提高了带宽效率。", "motivation": "现有大规模异构GPU集群中，基于物理邻近度分配GPU的传统方法在GPU间带宽分布不规则时存在显著局限性，导致通信延迟，无法有效利用带宽。", "method": "本文提出LiteGD，一个基于全局视角的轻量级动态GPU调度系统。它采用计算感知设计，利用在采样数据上训练的轻量级Transformer网络来处理大规模GPU拓扑信息存储的难题，并确保可迁移性和可扩展性。LiteGD还使用双向树搜索方法来寻找接近最优的GPU调度方案，同时减少搜索开销。", "result": "LiteGD在各种集群配置中实现了约90%的高GPU带宽效率，在真实H100集群中达到80%。它显著优于传统的默认调度方法和互连拓扑感知调度方法，尤其是在大规模异构环境中。", "conclusion": "LiteGD通过其创新的调度策略，有效克服了传统GPU分配方法的局限性，在大规模异构GPU集群中显著提升了通信效率和带宽利用率。", "translation": "多GPU并行计算已成为机器学习任务，特别是大型语言模型（LLM）的主导范式。为了减少GPU间通信引起的延迟，并行任务的常见做法是根据GPU的物理邻近度进行分配。然而，这一长期存在的假设存在显著局限性，尤其是在GPU间带宽分布不规则的大规模异构GPU集群中。在本文中，我们引入了LiteGD，一个基于全局视角的轻量级动态GPU调度系统。为了解决存储海量GPU拓扑信息的难题，LiteGD采用了计算感知设计，利用在采样数据上训练的轻量级Transformer网络。我们定制的网络结构设计确保了可迁移性和可扩展性。LiteGD还采用了双向树搜索方法，在前面步骤生成的数据中寻找最优的GPU调度方案，这可以在减少搜索开销的同时识别出接近最优的解决方案。我们在具有同构和异构互连的真实和模拟GPU集群中分别实现并评估了LiteGD。实验结果表明，LiteGD在各种集群配置中始终实现高GPU带宽效率（约90%），在真实H100集群中达到80%，显著优于传统的默认调度方法和互连拓扑感知调度方法，特别是在大规模异构环境中。", "summary": "本文介绍了LiteGD，一个针对大规模异构GPU集群的轻量级动态GPU调度系统。为解决传统基于物理邻近度分配GPU的局限性及其在存储大规模GPU拓扑信息上的困难，LiteGD采用计算感知设计，利用轻量级Transformer网络学习采样数据，并结合双向树搜索寻找近似最优调度方案。实验证明，LiteGD在多种集群配置中实现了高GPU带宽效率（约90%），在真实H100集群中达到80%，显著优于现有调度方法，尤其适用于大规模异构环境。", "keywords": "GPU调度, 异构集群, Transformer, 带宽效率, 并行计算", "comments": "LiteGD的创新之处在于其结合了轻量级Transformer网络进行拓扑信息学习和双向树搜索进行调度优化，有效解决了大规模异构集群中GPU间带宽不规则的挑战。其计算感知设计和对可迁移性、可扩展性的关注，使其在大规模部署中具有重要意义。"}}
{"id": "2506.14842", "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers", "authors": ["Lukas Schiesser", "Cornelius Wolff", "Sophie Haas", "Simon Pukrop"], "summary": "Building image classification models remains cumbersome in data-scarce\ndomains, where collecting large labeled datasets is impractical. In-context\nlearning (ICL) has emerged as a promising paradigm for few-shot image\nclassification (FSIC), enabling models to generalize across domains without\ngradient-based adaptation. However, prior work has largely overlooked a\ncritical component of ICL-based FSIC pipelines: the role of image embeddings.\nIn this work, we present PictSure, an ICL framework that places the embedding\nmodel -- its architecture, pretraining, and training dynamics -- at the center\nof analysis. We systematically examine the effects of different visual encoder\ntypes, pretraining objectives, and fine-tuning strategies on downstream FSIC\nperformance. Our experiments show that the training success and the\nout-of-domain performance are highly dependent on how the embedding models are\npretrained. Consequently, PictSure manages to outperform existing ICL-based\nFSIC models on out-of-domain benchmarks that differ significantly from the\ntraining distribution, while maintaining comparable results on in-domain tasks.\nCode can be found at https://github.com/PictSure/pictsure-library.", "comment": "15 pages, 10 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14842v1", "AI": {"title_translation": "PictSure: 预训练嵌入对情境学习图像分类器的重要性", "tldr": "PictSure 框架表明，预训练图像嵌入模型对于情境学习（ICL）图像分类器在数据稀缺领域和域外泛化性能至关重要，并超越现有模型。", "motivation": "在数据稀缺领域，构建图像分类模型很麻烦，因为收集大型标注数据集不切实际。情境学习（ICL）是少样本图像分类（FSIC）的一个有前景的范式，但现有工作在很大程度上忽略了基于 ICL 的 FSIC 管道中的一个关键组成部分：图像嵌入的作用。", "method": "提出了 PictSure，一个将嵌入模型（其架构、预训练和训练动态）置于分析中心的情境学习框架。系统地检查了不同视觉编码器类型、预训练目标和微调策略对下游 FSIC 性能的影响。", "result": "实验表明，训练成功和域外性能高度依赖于嵌入模型的预训练方式。PictSure 在与训练分布显著不同的域外基准上优于现有基于 ICL 的 FSIC 模型，同时在域内任务上保持可比结果。", "conclusion": "预训练图像嵌入模型对于情境学习图像分类器在少样本和域外场景中的性能至关重要，PictSure 证明了其有效性。", "translation": "在数据稀缺领域，构建图像分类模型仍然很麻烦，因为收集大型标注数据集不切实际。情境学习（ICL）已成为少样本图像分类（FSIC）的一个有前景的范式，使模型无需基于梯度的适应即可跨域泛化。然而，先前的工作在很大程度上忽略了基于 ICL 的 FSIC 管道中的一个关键组成部分：图像嵌入的作用。在这项工作中，我们提出了 PictSure，一个将嵌入模型——其架构、预训练和训练动态——置于分析中心的情境学习框架。我们系统地检查了不同视觉编码器类型、预训练目标和微调策略对下游 FSIC 性能的影响。我们的实验表明，训练成功和域外性能高度依赖于嵌入模型的预训练方式。因此，PictSure 在与训练分布显著不同的域外基准上，成功地超越了现有基于 ICL 的 FSIC 模型，同时在域内任务上保持了可比的结果。代码可在 https://github.com/PictSure/pictsure-library 找到。", "summary": "本研究提出 PictSure 框架，旨在解决数据稀缺领域图像分类模型的构建难题，尤其关注情境学习（ICL）在少样本图像分类（FSIC）中的应用。论文强调了图像嵌入模型在 ICL 管道中的关键作用，并系统地探究了视觉编码器类型、预训练目标和微调策略对 FSIC 性能的影响。实验结果表明，嵌入模型的预训练方式显著影响模型在域外任务上的泛化能力。PictSure 在域外基准测试中优于现有 ICL-FSIC 模型，同时在域内任务上保持了竞争力，证实了预训练嵌入的重要性。", "keywords": "情境学习, 图像分类, 预训练嵌入, 少样本学习, 域外泛化", "comments": "该论文的创新点在于系统地强调并分析了图像嵌入模型在情境学习图像分类中的关键作用，填补了现有研究的空白。其重要性在于为数据稀缺领域的少样本图像分类提供了新的视角和有效方法，通过优化嵌入模型的预训练来显著提升模型的泛化能力，尤其是在域外场景。这对于实际应用中处理新类别或新领域的数据具有重要意义。"}}
{"id": "2506.15683", "title": "PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning", "authors": ["Yuhui Shi", "Yehan Yang", "Qiang Sheng", "Hao Mi", "Beizhe Hu", "Chaoxi Xu", "Juan Cao"], "summary": "With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.", "comment": "17 pages, 3 figures, 6 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15683v1", "AI": {"title_translation": "幻影猎手：通过家族感知学习检测未见过的私有微调LLM生成文本", "tldr": "PhantomHunter是一种新的检测器，专门用于检测来自未见过的私有微调LLM生成的文本，通过家族感知学习实现高F1分数。", "motivation": "现有LLM生成文本检测方法在面对用户通过私有语料库微调开源LLM产生的文本时，性能显著下降，导致误信息和学术不端等问题加剧。", "method": "本文提出了PhantomHunter，一个专门用于检测来自未见过的私有微调LLM生成文本的检测器。其家族感知学习框架能捕获基础模型及其衍生模型之间共享的家族级特征，而非记忆个体特征。", "result": "在LLaMA、Gemma和Mistral家族的数据上进行实验，F1分数超过96%，优于7个基线和3个工业服务。", "conclusion": "PhantomHunter有效解决了检测未见过的私有微调LLM生成文本的挑战，并通过家族感知学习实现了卓越的性能。", "translation": "随着大型语言模型（LLM）的普及，虚假信息传播和学术不端等不良社会问题日益严重，使得LLM生成文本的检测变得空前重要。尽管现有方法取得了显著进展，但来自私有微调LLM的文本所带来的新挑战仍未得到充分探索。用户可以通过使用私有语料库微调开源LLM轻松拥有私有LLM，导致现有检测器在实践中性能显著下降。为了解决这个问题，我们提出了PhantomHunter，一个专门用于检测来自未见过的私有微调LLM生成文本的检测器。其家族感知学习框架捕获了基础模型及其衍生模型之间共享的家族级特征，而不是记忆个体特征。在LLaMA、Gemma和Mistral家族数据上的实验表明，其F1分数超过96%，优于7个基线和3个工业服务。", "summary": "本文提出了PhantomHunter，一个针对未见过的私有微调LLM生成文本的检测器。面对现有检测器因用户私有微调LLM而性能下降的问题，PhantomHunter采用家族感知学习框架，识别基础模型及其衍生模型间的共享家族特征。实验结果显示，在LLaMA、Gemma和Mistral家族数据上，PhantomHunter的F1分数超过96%，显著优于现有基线和商业服务。", "keywords": "LLM检测, 私有微调LLM, 家族感知学习, PhantomHunter, 文本生成检测", "comments": "这篇论文的创新点在于提出了“家族感知学习”的概念，解决了现有LLM文本检测器在面对私有微调模型时性能下降的痛点。其重要性体现在应对日益严重的LLM滥用问题，如虚假信息和学术不端。通过关注模型家族而非个体模型，该方法展现了良好的泛化能力，对于未来LLM检测领域具有指导意义。"}}
{"id": "2506.15465", "title": "DATA-DRIVEN PRONTO: a Model-free Solution for Numerical Optimal Control", "authors": ["Marco Borghesi", "Lorenzo Sforni", "Giuseppe Notarstefano"], "summary": "This article addresses the problem of data-driven numerical optimal control\nfor unknown nonlinear systems. In our scenario, we suppose to have the\npossibility of performing multiple experiments (or simulations) on the system.\nExperiments are performed by relying on a data-driven tracking controller able\nto steer the system towards a desired reference. Our proposed DATA-DRIVEN\nPRONTO algorithm iteratively refines a tentative solution of the optimal\ncontrol problem by computing an approximate descent direction via a local\ntrajectory perturbation. At each iteration, multiple trajectories are gathered\nby perturbing the current trajectory with a suitable dither signal, and then\nused to obtain a data-driven, time-varying linearization. The exploration is\nguided by the tracking controller, so that perturbed trajectories are obtained\nin closed loop. We show local convergence of DATA-DRIVEN PRONTO to a ball about\nan isolated optimal solution, whose radius depends on the amplitude of the\ndither signal. We corroborate the theoretical results by applying it to an\nunderactuated robot.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15465v1", "AI": {"title_translation": "数据驱动的PRONTO：一种无模型数值最优控制解决方案", "tldr": "该论文提出了DATA-DRIVEN PRONTO算法，这是一种用于未知非线性系统最优控制的无模型、数据驱动的迭代方法，通过扰动轨迹和跟踪控制器来收集数据，并证明了其局部收敛性。", "motivation": "本文旨在解决未知非线性系统的数据驱动数值最优控制问题，尤其是在可以进行多次实验或仿真的场景下。", "method": "所提出的DATA-DRIVEN PRONTO算法通过计算局部轨迹扰动产生的近似下降方向，迭代地改进最优控制问题的初步解。在每次迭代中，通过使用合适的抖动信号扰动当前轨迹来收集多条轨迹，然后用于获取数据驱动的时变线性化。探索过程由跟踪控制器引导，从而在闭环中获得扰动轨迹。", "result": "DATA-DRIVEN PRONTO被证明局部收敛到一个孤立最优解附近的球体，其半径取决于抖动信号的幅度。理论结果通过应用于欠驱动机器人得到了证实。", "conclusion": "DATA-DRIVEN PRONTO为未知非线性系统的数值最优控制提供了一种局部收敛、数据驱动、无模型的解决方案，并通过实际应用得到了验证。", "translation": "本文解决了未知非线性系统的数据驱动数值最优控制问题。在我们的设想中，我们假设能够对系统进行多次实验（或仿真）。实验通过依赖于能够将系统引导至所需参考的数据驱动跟踪控制器来执行。我们提出的DATA-DRIVEN PRONTO算法通过计算局部轨迹扰动产生的近似下降方向，迭代地改进最优控制问题的初步解。在每次迭代中，通过使用合适的抖动信号扰动当前轨迹来收集多条轨迹，然后用于获取数据驱动的时变线性化。探索过程由跟踪控制器引导，从而在闭环中获得扰动轨迹。我们展示了DATA-DRIVEN PRONTO局部收敛到一个孤立最优解附近的球体，其半径取决于抖动信号的幅度。我们通过将其应用于欠驱动机器人来证实了理论结果。", "summary": "本文介绍了DATA-DRIVEN PRONTO，一种用于未知非线性系统无模型数值最优控制的新型迭代算法。它通过使用抖动信号扰动轨迹并由跟踪控制器引导，利用多次实验来获得数据驱动的时变线性化。该算法展示了对最优解的局部收敛性，并通过在欠驱动机器人上的应用验证了其有效性。", "keywords": "数据驱动, 最优控制, 无模型, 非线性系统, PRONTO", "comments": "该论文的创新之处在于其针对未知系统的无模型、数据驱动最优控制方法，它利用局部轨迹扰动和跟踪控制器进行探索。局部收敛到受抖动信号幅度影响的球体这一理论结果具有重要意义。其在欠驱动机器人上的应用突出了其实用价值。"}}
{"id": "2506.15670", "title": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward", "authors": ["Özlem Tugfe Demir", "Mustafa Ozger", "Ferdi Kara", "Woong-Hee Lee", "Emil Björnson"], "summary": "This paper explores the integration of simultaneous wireless information and\npower transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)\ntechnology operating in the upper mid-band frequency range (7-24 GHz). The\nnear-field propagation achieved by gMIMO introduces unique opportunities for\nenergy-efficient, high-capacity communication systems that cater to the demands\nof 6G wireless networks. Exploiting spherical wave propagation, near-field\nSWIPT with gMIMO enables precise energy and data delivery, enhancing spectral\nefficiency through beamfocusing and massive spatial multiplexing. This paper\ndiscusses theoretical principles, design challenges, and enabling solutions,\nincluding advanced channel estimation techniques, precoding strategies, and\ndynamic array configurations such as sparse and modular arrays. Through\nanalytical insights and a case study, this paper demonstrates the feasibility\nof achieving optimized energy harvesting and data throughput in dense and\ndynamic environments. These findings contribute to advancing energy-autonomous\nInternet-of-Everything (IoE) deployments, smart factory networks, and other\nenergy-autonomous applications aligned with the goals of next-generation\nwireless technologies.", "comment": "7 pages, 5 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15670v1", "AI": {"title_translation": "上中频段中近场SWIPT与gMIMO：机遇、挑战与未来方向", "tldr": "本论文探讨了在7-24 GHz上中频段将近场SWIPT与gMIMO技术结合，以实现高效、高容量的6G无线网络，并讨论了其机遇、挑战和解决方案。", "motivation": "为了满足6G无线网络的需求，实现能源效率高、容量大的通信系统，并推动能源自主的物联网（IoE）部署、智能工厂网络及其他能源自主应用的发展。", "method": "本文探讨了同步无线信息与功率传输（SWIPT）与巨型多输入多输出（gMIMO）技术在上中频段（7-24 GHz）的集成。通过利用球面波传播，讨论了理论原理、设计挑战以及包括先进信道估计技术、预编码策略和动态阵列配置（如稀疏和模块化阵列）在内的使能解决方案。通过分析洞察和一个案例研究来验证可行性。", "result": "通过分析洞察和一个案例研究，本文证明了在密集和动态环境中实现优化能量收集和数据吞吐量的可行性。", "conclusion": "这些发现有助于推动能源自主的物联网（IoE）部署、智能工厂网络以及其他符合下一代无线技术目标的能源自主应用。", "translation": "本文探讨了同步无线信息与功率传输（SWIPT）与巨型多输入多输出（gMIMO）技术在上中频段（7-24 GHz）的集成。gMIMO实现的近场传播为满足6G无线网络需求的高能效、高容量通信系统带来了独特的机遇。利用球面波传播，近场SWIPT与gMIMO通过波束聚焦和大规模空间复用提高了频谱效率，实现了精确的能量和数据传输。本文讨论了理论原理、设计挑战以及使能解决方案，包括先进的信道估计技术、预编码策略和动态阵列配置，如稀疏和模块化阵列。通过分析洞察和一个案例研究，本文证明了在密集和动态环境中实现优化能量收集和数据吞吐量的可行性。这些发现有助于推动能源自主的物联网（IoE）部署、智能工厂网络以及其他符合下一代无线技术目标的能源自主应用。", "summary": "本论文研究了将近场同步无线信息与功率传输（SWIPT）与巨型多输入多输出（gMIMO）技术结合应用于7-24 GHz上中频段。该结合利用近场传播和球面波特性，为6G网络提供了高能效、高容量的通信系统，能够实现精确的能量和数据传输，并通过波束聚焦和大规模空间复用提高频谱效率。论文讨论了相关的理论、设计挑战以及包括先进信道估计和动态阵列配置在内的解决方案，并通过案例研究验证了在复杂环境中优化能量收集和数据吞吐量的可行性，旨在推动能源自主的物联网及相关应用的发展。", "keywords": "SWIPT, gMIMO, 近场通信, 6G, 能量收集", "comments": "本文的创新之处在于将SWIPT与gMIMO技术结合，并在上中频段利用近场传播特性，为6G网络提供了新的解决方案。其重要性在于能够促进能源自主的物联网和智能工厂等应用的实现。论文不仅提出了机遇和挑战，也提供了具体的解决方案，并进行了可行性验证，具有较高的实践指导意义。"}}
{"id": "2506.14797", "title": "Bound by semanticity: universal laws governing the generalization-identification tradeoff", "authors": ["Marco Nurisso", "Jesseba Fernando", "Raj Deshpande", "Alan Perotti", "Raja Marjieh", "Steven M. Frankland", "Richard L. Lewis", "Taylor W. Webb", "Declan Campbell", "Francesco Vaccarino", "Jonathan D. Cohen", "Giovanni Petri"], "summary": "Intelligent systems must deploy internal representations that are\nsimultaneously structured -- to support broad generalization -- and selective\n-- to preserve input identity. We expose a fundamental limit on this tradeoff.\nFor any model whose representational similarity between inputs decays with\nfinite semantic resolution $\\varepsilon$, we derive closed-form expressions\nthat pin its probability of correct generalization $p_S$ and identification\n$p_I$ to a universal Pareto front independent of input space geometry.\nExtending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs\npredicts a sharp $1/n$ collapse of multi-input processing capacity and a\nnon-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end\nreproduces these laws: during learning a resolution boundary self-organizes and\nempirical $(p_S,p_I)$ trajectories closely follow theoretical curves for\nlinearly decaying similarity. Finally, we demonstrate that the same limits\npersist in two markedly more complex settings -- a convolutional neural network\nand state-of-the-art vision-language models -- confirming that\nfinite-resolution similarity is a fundamental emergent informational\nconstraint, not merely a toy-model artifact. Together, these results provide an\nexact theory of the generalization-identification trade-off and clarify how\nsemantic resolution shapes the representational capacity of deep networks and\nbrains alike.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14797v1", "AI": {"title_translation": "受语义性约束：泛化-识别权衡的普适法则", "tldr": "智能系统在泛化和识别之间的权衡存在根本限制，该限制由语义分辨率决定，并通过理论推导和多种神经网络实验得到验证。", "motivation": "智能系统需要兼具结构化（支持广泛泛化）和选择性（保留输入身份）的内部表示，但这种权衡存在一个基本限制。", "method": "研究者推导了表示相似度随有限语义分辨率衰减的模型的封闭形式表达式，以确定正确泛化和识别的概率。他们将分析扩展到嘈杂、异构空间和多输入情况，并通过最小ReLU网络、卷积神经网络和最先进的视觉-语言模型进行端到端训练和验证。", "result": "导出了与输入空间几何无关的普适帕累托前沿；预测了多输入处理能力的急剧1/n崩溃和泛化概率$p_S$的非单调最优值；最小ReLU网络重现了这些法则；在卷积神经网络和视觉-语言模型中也存在相同的限制，证实有限分辨率相似性是基本的信息约束。", "conclusion": "这些结果提供了泛化-识别权衡的精确理论，并阐明了语义分辨率如何塑造深度网络和大脑的表示能力。", "translation": "智能系统必须部署同时具有结构性（以支持广泛泛化）和选择性（以保留输入身份）的内部表示。我们揭示了这种权衡的一个根本限制。对于任何其输入之间的表示相似性随有限语义分辨率$\\varepsilon$衰减的模型，我们推导出了封闭形式的表达式，该表达式将其正确泛化概率$p_S$和识别概率$p_I$固定在一个独立于输入空间几何的普适帕累托前沿上。将分析扩展到嘈杂、异构空间以及$n>2$个输入的情况，预测了多输入处理能力会急剧地以$1/n$崩溃，并且$p_S$存在一个非单调最优值。一个经过端到端训练的最小ReLU网络重现了这些法则：在学习过程中，一个分辨率边界自组织形成，并且经验性的$(p_S,p_I)$轨迹密切遵循线性衰减相似度的理论曲线。最后，我们证明了相同的限制在两种明显更复杂的设置中也持续存在——一个卷积神经网络和最先进的视觉-语言模型——证实了有限分辨率相似性是一种基本的新兴信息约束，而不仅仅是玩具模型的产物。总而言之，这些结果提供了泛化-识别权衡的精确理论，并阐明了语义分辨率如何塑造深度网络和大脑的表示能力。", "summary": "本文探讨了智能系统在泛化和识别能力之间存在的根本性权衡。研究者提出了一个理论框架，通过推导封闭形式的表达式，揭示了表示相似度与有限语义分辨率之间的关系，并预测了泛化和识别概率的普适帕累托前沿。该理论预测了多输入处理能力的下降以及泛化概率的非单调最优值。实验通过最小ReLU网络、卷积神经网络和视觉-语言模型验证了这些理论预测，证明了有限分辨率相似性是深度网络和大脑中普遍存在的信息约束，而非简单的模型产物，从而提供了一个泛化-识别权衡的精确理论。", "keywords": "语义分辨率, 泛化-识别权衡, 普适法则, 深度学习, 表示能力", "comments": "这篇论文通过理论推导和广泛的实验验证，揭示了智能系统泛化和识别能力之间的一个普适性限制，即“语义分辨率”的约束。其创新之处在于提出了一个精确的数学理论，并证明了这一理论不仅适用于简单的模型，也适用于复杂的深度学习架构，甚至可能适用于生物大脑。这为理解深度学习模型的表示能力边界提供了重要的理论基础。"}}
{"id": "2506.15131", "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "authors": ["Jing Yang Lee", "Kong-Aik Lee", "Woon-Seng Gan"], "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15131v1", "AI": {"title_translation": "使用大型语言模型建模开放域对话中的一对多特性", "tldr": "该研究通过将开放域对话生成分解为多响应生成和基于偏好的选择，并引入新的数据集和策略，成功地使用大型语言模型建模了对话中的一对多特性，从而提高了响应多样性和质量。", "motivation": "开放域对话（OD）具有一对多（o2m）特性，即对于一个对话上下文存在多个合适的响应。尽管先前的研究表明建模此特性可以提高响应多样性，但大多数现代基于大型语言模型（LLM）的对话代理并未明确这样做。", "method": "本研究通过将开放域对话生成分解为两个关键任务来建模LLM中OD的一对多特性：多响应生成（MRG）和基于偏好的选择（PS）。MRG涉及为给定对话上下文生成一组n个语义和词汇多样的高质量响应，随后PS根据人类偏好选择一个响应。为促进MRG和PS，研究引入了o2mDial，一个专门设计用于捕获o2m特性的对话语料库，其中每个上下文都包含多个合理的响应。利用o2mDial，研究提出了新的上下文学习和指令微调策略，以及MRG的新评估指标，同时为PS提供了一种基于模型的方法。", "result": "实证结果表明，将所提出的两阶段框架应用于较小的LLM进行OD生成，可以增强整体响应多样性，同时保持上下文连贯性，并将响应质量提高高达90%，使其更接近大型模型的性能。", "conclusion": "本研究提出的两阶段框架有效地建模了开放域对话中的一对多特性，即使对于较小的LLM也能显著提高响应多样性和质量，使其性能接近大型模型。", "translation": "开放域对话（OD）展现出一种一对多（o2m）特性，即对于一个对话上下文存在多个合适的响应。尽管之前的研究表明建模这一特性可以提高响应多样性，但大多数现代基于大型语言模型（LLM）的对话代理并未明确这样做。在这项工作中，我们通过将OD生成分解为两个关键任务来建模LLM中OD的一对多特性：多响应生成（MRG）和基于偏好的选择（PS）。这两个任务分别涉及为给定对话上下文生成一组n个语义和词汇多样的高质量响应，然后根据人类偏好选择一个响应。为了促进MRG和PS，我们引入了o2mDial，这是一个专门设计用于捕获o2m特性的对话语料库，其特点是每个上下文都有多个合理的响应。利用o2mDial，我们提出了新的上下文学习和指令微调策略，以及MRG的新评估指标，同时为PS提供了一种基于模型的方法。实证结果表明，将所提出的两阶段框架应用于较小的LLM进行OD生成，可以增强整体响应多样性，同时保持上下文连贯性，并将响应质量提高高达90%，使其更接近大型模型的性能。", "summary": "该论文旨在解决现有大型语言模型（LLM）在开放域对话中未明确建模一对多（one-to-many, o2m）特性的问题。作者将开放域对话生成分解为多响应生成（MRG）和基于偏好的选择（PS）两个阶段，并为此引入了一个新的对话语料库o2mDial。研究还提出了新的上下文学习、指令微调策略以及MRG的评估指标和PS的模型方法。实验证明，该两阶段框架能显著提升小型LLM的响应多样性和质量，使其性能接近大型模型。", "keywords": "一对多特性, 开放域对话, 大型语言模型, 多响应生成, 基于偏好的选择", "comments": "这项工作通过引入一个新颖的两阶段框架和一个专门的语料库o2mDial，明确地解决了开放域对话中一对多属性的建模问题，这对于生成更自然、更像人类的对话至关重要。其创新点在于将复杂的生成过程解耦，并为小型LLM带来了显著的性能提升，弥补了与大型模型之间的差距，具有重要的实践意义。"}}
{"id": "2506.15142", "title": "Fourth- and Higher-Order Semi-Lagrangian Finite Volume Methods for the Two-dimensional Advection Equation on Arbitrarily Complex Domains", "authors": ["Yunxia Sun", "Kaiyi Liang", "Yuke Zhu", "Zhi Lin", "Qinghai Zhang"], "summary": "To numerically solve the two-dimensional advection equation, we propose a\nfamily of fourth- and higher-order semi-Lagrangian finite volume (SLFV) methods\nthat feature (1) fourth-, sixth-, and eighth-order convergence rates, (2)\napplicability to both regular and irregular domains with arbitrarily complex\ntopology and geometry, (3) ease of handling both zero and nonzero source terms,\nand (4) the same algorithmic steps for both periodic and incoming penetration\nconditions. Test results confirm the analysis and demonstrate the accuracy,\nflexibility, robustness, and excellent conditioning of the proposed SLFV\nmethod.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15142v1", "AI": {"title_translation": "二维对流方程在任意复杂域上的四阶及更高阶半拉格朗日有限体积方法", "tldr": "提出了一种四阶及更高阶的半拉格朗日有限体积（SLFV）方法，用于解决二维对流方程，该方法在复杂域上具有高精度、灵活性和鲁棒性。", "motivation": "为了数值求解二维对流方程。", "method": "提出了一系列四阶及更高阶的半拉格朗日有限体积（SLFV）方法，这些方法具有四阶、六阶和八阶收敛率，适用于具有任意复杂拓扑和几何形状的规则和不规则域，易于处理零和非零源项，并且对周期和入射穿透条件采用相同的算法步骤。", "result": "测试结果证实了理论分析，并证明了所提出的SLFV方法的准确性、灵活性、鲁棒性和出色的条件性。", "conclusion": "所提出的半拉格朗日有限体积方法能够准确、灵活且鲁棒地解决二维对流方程，即使在复杂域和不同边界条件下也能保持良好的性能。", "translation": "为了数值求解二维对流方程，我们提出了一系列四阶及更高阶的半拉格朗日有限体积（SLFV）方法，其特点是：(1) 具有四阶、六阶和八阶收敛率；(2) 适用于具有任意复杂拓扑和几何形状的规则和不规则域；(3) 易于处理零和非零源项；(4) 对周期和入射穿透条件采用相同的算法步骤。测试结果证实了理论分析，并证明了所提出的SLFV方法的准确性、灵活性、鲁棒性和出色的条件性。", "summary": "本文提出了一系列用于数值求解二维对流方程的四阶及更高阶半拉格朗日有限体积（SLFV）方法。该方法具有高阶收敛率（四阶、六阶、八阶），能适用于任意复杂拓扑和几何的规则及不规则域，可处理零和非零源项，并对不同边界条件采用统一的算法。实验结果验证了该方法的准确性、灵活性、鲁棒性和良好的条件性。", "keywords": "半拉格朗日, 有限体积方法, 对流方程, 高阶, 复杂域", "comments": "这项工作创新性地提出了高阶半拉格朗日有限体积方法，显著提升了二维对流方程在复杂计算域上的数值求解精度和适用性。其对任意复杂域的适应性以及对不同源项和边界条件的统一处理，是该方法的重要优势。"}}
{"id": "2506.15556", "title": "PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction", "authors": ["Shufan Li", "Aditya Grover"], "summary": "Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.", "comment": "16 pages,4 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15556v1", "AI": {"title_translation": "PredGen：通过输入时推测加速大型语言模型推理以实现实时语音交互", "tldr": "PredGen通过在用户说话时推测性生成LLM响应来减少实时语音交互中的延迟。", "motivation": "大型语言模型（LLMs）在实时语音聊天应用中与文本转语音（TTS）系统结合使用时，由于模型规模大，用户输入结束到音频输出开始之间存在显著延迟，尤其是在消费级硬件上，影响用户体验。这种延迟主要由LLM生成第一句话的时间造成，而TTS系统需要第一句话作为输入。", "method": "提出PredGen（预测生成）框架，通过在输入时进行推测性解码来缓解或消除延迟。PredGen在用户仍在说话时生成候选响应，从而使系统能够以最小延迟开始TTS处理。", "result": "在Lmsys和MT-Bench数据集上的模拟实验表明，该方法在广泛的使用场景中能有效将延迟减少约2倍，同时在输入时仅产生最小的额外计算成本。", "conclusion": "PredGen框架能够有效减少大型语言模型在实时语音交互中的延迟，显著提升用户体验，且额外计算成本极低。", "translation": "大型语言模型（LLMs）广泛应用于实时语音聊天应用中，通常与文本转语音（TTS）系统结合使用以生成音频响应。然而，其庞大的规模常常导致用户输入结束到音频输出开始之间出现明显的延迟，从而导致次优的用户体验。当LLMs作为单用户语音助手部署在计算能力有限的消费级硬件上时，这种延迟尤为明显。我们发现这种延迟主要由LLMs生成第一句话所需的时间决定，而TTS系统需要这句话作为输入才能逐句合成音频响应。为了解决这一瓶颈，我们提出了预测生成（PredGen），一个新颖的框架，通过在输入时进行推测性解码来减轻甚至消除这种延迟。PredGen在用户仍在说话时生成候选响应，使系统能够以最小的延迟开始TTS处理。在Lmsys和MT-Bench数据集上的模拟实验表明，所提出的方法可以在广泛的使用场景中有效将延迟减少约2倍，同时在输入时仅产生最小的额外计算成本——这些计算原本是未被利用的。", "summary": "本文提出了PredGen框架，旨在解决大型语言模型在实时语音交互中因生成首句响应而导致的显著延迟问题。PredGen通过在用户说话时进行推测性解码，提前生成候选响应，从而使文本转语音系统能够更快地开始处理。实验结果显示，PredGen能将延迟减少约2倍，且额外计算成本极低。", "keywords": "大型语言模型, 实时语音交互, 延迟优化, 推测性解码, PredGen", "comments": "PredGen的创新之处在于利用了用户输入时的空闲计算资源进行推测性生成，有效地解决了LLM和TTS系统结合时存在的首句延迟瓶颈。这对于提升实时语音交互的用户体验具有重要意义，特别是在资源受限的消费级设备上。"}}
{"id": "2506.15364", "title": "Brain Stroke Classification Using Wavelet Transform and MLP Neural Networks on DWI MRI Images", "authors": ["Mana Mohammadi", "Amirhesam Jafari Rad", "Ashkan Behrouzi"], "summary": "This paper presents a lightweight framework for classifying brain stroke\ntypes from Diffusion-Weighted Imaging (DWI) MRI scans, employing a Multi-Layer\nPerceptron (MLP) neural network with Wavelet Transform for feature extraction.\nAccurate and timely stroke detection is critical for effective treatment and\nimproved patient outcomes in neuroimaging. While Convolutional Neural Networks\n(CNNs) are widely used for medical image analysis, their computational\ncomplexity often hinders deployment in resource-constrained clinical settings.\nIn contrast, our approach combines Wavelet Transform with a compact MLP to\nachieve efficient and accurate stroke classification. Using the \"Brain Stroke\nMRI Images\" dataset, our method yields classification accuracies of 82.0% with\nthe \"db4\" wavelet (level 3 decomposition) and 86.00% with the \"Haar\" wavelet\n(level 2 decomposition). This analysis highlights a balance between diagnostic\naccuracy and computational efficiency, offering a practical solution for\nautomated stroke diagnosis. Future research will focus on enhancing model\nrobustness and integrating additional MRI modalities for comprehensive stroke\nassessment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15364v1", "AI": {"title_translation": "基于小波变换和MLP神经网络的DWI MRI图像脑卒中分类", "tldr": "该论文提出了一种轻量级框架，利用小波变换和MLP神经网络对DWI MRI图像进行脑卒中分类，实现了诊断准确性和计算效率的平衡。", "motivation": "神经影像中准确及时的中风检测对于有效治疗和改善患者预后至关重要。传统的卷积神经网络（CNN）计算复杂性高，难以在资源受限的临床环境中部署。", "method": "本研究提出了一种轻量级框架，通过使用小波变换进行特征提取，并结合多层感知机（MLP）神经网络对扩散加权成像（DWI）MRI扫描图像进行脑卒中类型分类。", "result": "使用“脑卒中MRI图像”数据集，该方法在使用“db4”小波（3级分解）时分类准确率达到82.0%，在使用“Haar”小波（2级分解）时分类准确率达到86.00%。", "conclusion": "该分析突出了诊断准确性和计算效率之间的平衡，为自动化中风诊断提供了一种实用的解决方案。", "translation": "本文提出了一种轻量级框架，用于对扩散加权成像（DWI）MRI扫描图像中的脑卒中类型进行分类，该框架采用多层感知机（MLP）神经网络和小波变换进行特征提取。在神经影像学中，准确及时的中风检测对于有效治疗和改善患者预后至关重要。虽然卷积神经网络（CNN）广泛用于医学图像分析，但其计算复杂性常常阻碍其在资源受限的临床环境中的部署。相比之下，我们的方法结合了小波变换和紧凑的MLP，以实现高效准确的中风分类。使用“脑卒中MRI图像”数据集，我们的方法在使用“db4”小波（3级分解）时分类准确率达到82.0%，在使用“Haar”小波（2级分解）时分类准确率达到86.00%。这项分析突出了诊断准确性和计算效率之间的平衡，为自动化中风诊断提供了一种实用的解决方案。未来的研究将侧重于增强模型鲁棒性并整合其他MRI模态以进行全面的中风评估。", "summary": "本论文提出了一种基于小波变换和多层感知机（MLP）神经网络的轻量级框架，用于对扩散加权成像（DWI）MRI图像进行脑卒中分类。该方法旨在解决传统CNN在资源受限临床环境中部署的计算复杂性问题。通过在“脑卒中MRI图像”数据集上的实验，该方法在使用“db4”和“Haar”小波时分别取得了82.0%和86.00%的分类准确率，证明了其在诊断准确性和计算效率之间的良好平衡，为自动化中风诊断提供了一个实用方案。", "keywords": "脑卒中分类, 小波变换, MLP神经网络, DWI MRI, 医疗影像", "comments": "该论文的创新点在于提出了一种计算效率高、适用于资源受限临床环境的脑卒中分类方案，而非依赖于计算量大的CNN模型。通过结合小波变换进行特征提取和紧凑的MLP网络，该方法在保证一定诊断准确性的同时，显著降低了部署难度，具有较强的实用价值。未来的工作可以进一步探索多模态MRI数据的融合，以提升模型的综合评估能力。"}}
{"id": "2506.15212", "title": "LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis", "authors": ["Madjid G. Tehrani", "Eldar Sultanow", "William J. Buchanan", "Mahkame Houmani", "Christel H. Djaha Fodja"], "summary": "With the rapid advancements in Natural Language Processing (NLP), large\nlanguage models (LLMs) like GPT-4 have gained significant traction in diverse\napplications, including security vulnerability scanning. This paper\ninvestigates the efficacy of GPT-4 in identifying software vulnerabilities\ncompared to traditional Static Application Security Testing (SAST) tools.\nDrawing from an array of security mistakes, our analysis underscores the potent\ncapabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that\nGPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in\ndetecting 32 types of exploitable vulnerabilities. This study also addresses\nthe potential security concerns surrounding LLMs, emphasising the imperative of\nsecurity by design/default and other security best practices for AI.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15212v1", "AI": {"title_translation": "LLM与SAST：关于检测GPT-4高级数据分析中编码错误的技??分析", "tldr": "GPT-4高级数据分析在检测32种可利用漏洞方面，以94%的准确率显著优于传统SAST工具。", "motivation": "随着大型语言模型（LLMs）如GPT-4在安全漏洞扫描等应用中迅速发展，本文旨在探究GPT-4在识别软件漏洞方面的有效性，并与传统静态应用安全测试（SAST）工具进行比较。", "method": "本研究通过分析一系列安全错误，评估了GPT-4（高级数据分析）在LLM增强型漏洞扫描中的能力，并将其性能与传统SAST工具进行了对比。", "result": "研究发现，GPT-4（高级数据分析）在检测32种可利用漏洞方面，以94%的准确率优于SAST工具。", "conclusion": "GPT-4在软件漏洞检测方面展现出强大潜力，但同时强调了围绕LLM的潜在安全问题，并呼吁在AI设计和部署中实施安全即设计/默认以及其他安全最佳实践。", "translation": "随着自然语言处理（NLP）的快速发展，像GPT-4这样的大型语言模型（LLMs）在包括安全漏洞扫描在内的各种应用中获得了显著关注。本文调查了GPT-4在识别软件漏洞方面的功效，并与传统静态应用安全测试（SAST）工具进行了比较。通过一系列安全错误，我们的分析强调了GPT-4在LLM增强型漏洞扫描中的强大能力。我们发现GPT-4（高级数据分析）在检测32种可利用漏洞方面，以94%的准确率优于SAST。本研究还讨论了围绕LLMs的潜在安全问题，强调了AI安全设计/默认以及其他安全最佳实践的必要性。", "summary": "本文对GPT-4（高级数据分析）在软件漏洞检测方面的能力进行了技术分析，并与传统SAST工具进行了比较。研究结果表明，GPT-4在检测32种可利用漏洞方面，以94%的准确率显著优于SAST。此外，论文还讨论了LLM相关的潜在安全隐患，并强调了AI安全设计的重要性。", "keywords": "LLM, GPT-4, SAST, 漏洞检测, 代码安全", "comments": "这篇论文的创新点在于首次对GPT-4在特定漏洞检测任务上与传统SAST工具进行了量化比较，并展示了LLM在该领域的巨大潜力。其重要性在于为LLM在网络安全领域的应用提供了实证支持，并提出了AI安全设计的前瞻性思考。"}}
{"id": "2506.15126", "title": "VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments", "authors": ["Bingbing Zhang", "Huan Yin", "Shuo Liu", "Fumin Zhang", "Wen Xu"], "summary": "In this study, we present a novel simultaneous localization and mapping\n(SLAM) system, VIMS, designed for underwater navigation. Conventional\nvisual-inertial state estimators encounter significant practical challenges in\nperceptually degraded underwater environments, particularly in scale estimation\nand loop closing. To address these issues, we first propose leveraging a\nlow-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a\nhigh-sampling-rate magnetometer for place recognition by utilizing magnetic\nsignatures generated by an economical magnetic field coil. Building on this, a\nhierarchical scheme is developed for visual-magnetic place recognition,\nenabling robust loop closure. Furthermore, VIMS achieves a balance between\nlocal feature tracking and descriptor-based loop closing, avoiding additional\ncomputational burden on the front end. Experimental results highlight the\nefficacy of the proposed VIMS, demonstrating significant improvements in both\nthe robustness and accuracy of state estimation within underwater environments.", "comment": "This work has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15126v1", "AI": {"title_translation": "VIMS：水下环境中的视觉-惯性-磁力-声呐SLAM系统", "tldr": "提出VIMS系统，结合视觉、惯性、磁力和声呐数据，解决水下SLAM中尺度估计和闭环的挑战，显著提升水下定位的鲁棒性和精度。", "motivation": "传统视觉-惯性状态估计器在感知退化的水下环境中面临显著挑战，尤其是在尺度估计和闭环方面。", "method": "VIMS系统首先利用低成本单波束声呐改善尺度估计；然后，集成高采样率磁力计，通过经济型磁场线圈产生的磁信号进行地点识别；在此基础上，开发了视觉-磁力地点识别的分层方案，实现鲁棒的闭环；同时，平衡局部特征跟踪和基于描述符的闭环，避免前端额外计算负担。", "result": "实验结果表明，所提出的VIMS系统有效，显著提高了水下环境中状态估计的鲁棒性和准确性。", "conclusion": "VIMS系统通过多传感器融合（视觉、惯性、磁力、声呐）有效解决了水下SLAM的尺度估计和闭环问题，显著提升了水下环境下的定位性能。", "translation": "在这项研究中，我们提出了一种新颖的同步定位与建图（SLAM）系统VIMS，专为水下导航设计。传统的视觉-惯性状态估计器在感知退化的水下环境中遇到显著的实际挑战，特别是在尺度估计和闭环方面。为了解决这些问题，我们首先提出利用低成本单波束声呐来改善尺度估计。然后，VIMS通过利用经济型磁场线圈产生的磁信号，集成高采样率磁力计进行地点识别。在此基础上，开发了一种用于视觉-磁力地点识别的分层方案，实现了鲁棒的闭环。此外，VIMS在局部特征跟踪和基于描述符的闭环之间取得了平衡，避免了前端的额外计算负担。实验结果突出了所提出的VIMS的有效性，证明在水下环境中状态估计的鲁棒性和准确性方面都有显著改进。", "summary": "本文提出了一种名为VIMS的新型水下SLAM系统，旨在克服传统视觉-惯性估计算法在水下尺度估计和闭环方面的挑战。VIMS通过结合低成本单波束声呐来增强尺度估计，并集成高采样率磁力计利用磁信号进行地点识别。系统采用分层的视觉-磁力地点识别方案以实现鲁棒的闭环，并优化了特征跟踪与描述符匹配，避免了额外的计算开销。实验验证了VIMS在水下环境下显著提升了状态估计的鲁棒性和精度。", "keywords": "水下SLAM, 视觉-惯性-磁力-声呐, 尺度估计, 闭环, 多传感器融合", "comments": "该论文的创新点在于其多传感器融合策略，特别是在水下环境中引入磁力计和声呐来克服传统视觉-惯性SLAM的局限性。利用磁力信号进行地点识别是一个新颖且有潜力的方向，尤其是在视觉信息受限的水下环境。同时，对计算效率的考虑也增强了其实用性。"}}
{"id": "2506.15294", "title": "UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments", "authors": ["Jonas Lau", "Annie Tran"], "summary": "This paper discusses a popular UX research activity, feature prioritization,\nusing the User Experience Research Point of View (UXR PoV) Playbook framework.\nWe describe an application of multinomial logistic regression, frequently\nmarketed as MaxDiff, for prioritizing product features in consumer product\ndevelopment. It addresses challenges of traditional surveying techniques. We\npropose a solution using MaxDiff to generate a reliable preference list with a\nreasonable sample size. We also adapt the MaxDiff method to reduce the number\nof survey responses in half, making it less tedious from the survey takers'\nperspective. We present a case study using the adapted MaxDiff method for\ntablet feature prioritization research involving users with disabilities.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15294v1", "AI": {"title_translation": "用户体验研究对数百万美元工程投入前产品功能优先级排序的观点", "tldr": "本文讨论了一种使用改编MaxDiff方法的产品功能优先级排序方法，旨在解决传统调查的挑战，并减轻调查参与者的负担，尤其适用于消费产品开发。", "motivation": "本文旨在解决传统调查技术在产品功能优先级排序中面临的挑战，并提出一种能够以合理样本量生成可靠偏好列表的解决方案，同时减少调查参与者的负担。", "method": "本文采用用户体验研究观点（UXR PoV）手册框架，描述了多项式逻辑回归（MaxDiff）的应用。它还对MaxDiff方法进行了调整，将调查响应数量减少了一半。文中提出了一个使用改编MaxDiff方法对涉及残疾用户的平板电脑功能进行优先级排序的案例研究。", "result": "本文提出了一种使用MaxDiff的解决方案，能够以合理的样本量生成可靠的偏好列表，并通过改编MaxDiff方法将调查响应数量减少了一半。文章还展示了一个使用该改编方法的案例研究。", "conclusion": "本文展示了一种改编的MaxDiff方法，它能有效且不那么繁琐地进行产品功能优先级排序，尤其在面临高额工程投入前提供可靠的用户偏好数据。", "translation": "这篇论文讨论了一种流行的用户体验研究活动——功能优先级排序，使用了用户体验研究观点（UXR PoV）手册框架。我们描述了多项式逻辑回归（通常被称为MaxDiff）在消费产品开发中用于产品功能优先级排序的应用。它解决了传统调查技术的挑战。我们提出了一种使用MaxDiff的解决方案，以合理的样本量生成可靠的偏好列表。我们还调整了MaxDiff方法，将调查响应数量减少一半，从而从调查参与者的角度来看不那么繁琐。我们提出了一个案例研究，使用改编后的MaxDiff方法对涉及残疾用户的平板电脑功能优先级进行了研究。", "summary": "本文介绍了一种改编的MaxDiff方法（多项式逻辑回归的一种形式），用于消费产品开发中的产品功能优先级排序。该方法旨在克服传统调查的局限性，通过合理的样本量生成可靠的偏好列表，并显著减轻参与者的调查负担。文中还提供了一个涉及残疾用户的平板电脑功能优先级研究的案例。", "keywords": "功能优先级排序, MaxDiff, 用户体验研究, 多项式逻辑回归, 消费产品开发", "comments": "该论文为常见的用户体验研究问题提供了一种创新的方法，通过改进MaxDiff使其更高效、用户友好，在进行大量工程投入前提供有价值的用户偏好数据。其关注减轻调查疲劳和提供可靠数据的特点是其重要亮点。"}}
{"id": "2506.15636", "title": "Quantum Error Correction Exploiting Degeneracy to Approach the Hashing Bound", "authors": ["Kenta Kasai"], "summary": "Quantum error correction is essential for realizing scalable quantum\ncomputation. Among various approaches, low-density parity-check codes over\nhigher-order Galois fields have shown promising performance due to their\nstructured sparsity and compatibility with iterative decoding algorithms whose\ncomputational complexity scales linearly with the number of physical qubits. In\nthis work, we demonstrate that explicitly exploiting the degeneracy of quantum\nerrors can significantly enhance the decoding performance. Simulation results\nover the depolarizing channel indicate that the proposed method, at a coding\nrate of 1/3, achieves a frame error rate as low as $10^{-4}$ at a physical\nerror rate of 9.45% for a code with 104,000 logical qubits and 312,000 physical\nqubits, approaching the quantum hashing bound. These findings highlight the\ncritical role of degeneracy in closing the gap to the fundamental limits of\nquantum error correction.", "comment": "This work has been submitted to a journal for possible publication", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.15636v1", "AI": {"title_translation": "量子纠错利用简并性逼近哈希界限", "tldr": "本文展示了利用量子错误的简并性可以显著提高量子纠错的解码性能，逼近量子哈希界限。", "motivation": "实现可扩展量子计算需要量子纠错。低密度奇偶校验码在性能和计算复杂度方面表现良好，但仍需提升以逼近基本限制。", "method": "本文通过明确利用量子错误的简并性来显著增强量子纠错的解码性能。", "result": "在退极化信道上，所提出的方法在编码率为1/3时，对于一个拥有104,000个逻辑量子比特和312,000个物理量子比特的代码，在9.45%的物理错误率下，实现了低至$10^{-4}$的帧错误率，逼近量子哈希界限。", "conclusion": "这些发现强调了简并性在缩小与量子纠错基本限制之间差距方面的关键作用。", "translation": "量子纠错对于实现可扩展量子计算至关重要。在各种方法中，基于高阶伽罗瓦域的低密度奇偶校验码因其结构稀疏性以及与计算复杂度随物理量子比特数量线性扩展的迭代解码算法兼容而表现出良好的性能。在这项工作中，我们证明了明确利用量子错误的简并性可以显著提高解码性能。在退极化信道上的仿真结果表明，所提出的方法在编码率为1/3时，对于一个拥有104,000个逻辑量子比特和312,000个物理量子比特的代码，在9.45%的物理错误率下，实现了低至$10^{-4}$的帧错误率，逼近量子哈希界限。这些发现强调了简并性在缩小与量子纠错基本限制之间差距方面的关键作用。", "summary": "本文提出了一种通过利用量子错误的简并性来提高量子纠错解码性能的方法。研究表明，该方法在大型量子码上实现了接近量子哈希界限的低帧错误率，突出了简并性在逼近量子纠错理论极限方面的重要性。", "keywords": "量子纠错, 简并性, 低密度奇偶校验码, 哈希界限, 退极化信道", "comments": "该工作通过明确利用量子错误的简并性，为量子纠错提供了一种有效提升性能的途径，尤其是在高物理错误率下仍能保持低帧错误率，这对于实现容错量子计算具有重要意义。其创新点在于强调了简并性在逼近量子纠错基本限制中的关键作用。"}}
{"id": "2506.14911", "title": "Event-Driven Online Vertical Federated Learning", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "summary": "Online learning is more adaptable to real-world scenarios in Vertical\nFederated Learning (VFL) compared to offline learning. However, integrating\nonline learning into VFL presents challenges due to the unique nature of VFL,\nwhere clients possess non-intersecting feature sets for the same sample. In\nreal-world scenarios, the clients may not receive data streaming for the\ndisjoint features for the same entity synchronously. Instead, the data are\ntypically generated by an \\emph{event} relevant to only a subset of clients. We\nare the first to identify these challenges in online VFL, which have been\noverlooked by previous research. To address these challenges, we proposed an\nevent-driven online VFL framework. In this framework, only a subset of clients\nwere activated during each event, while the remaining clients passively\ncollaborated in the learning process. Furthermore, we incorporated\n\\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by\nonline learning problems with non-convex models within a non-stationary\nenvironment. We conducted a comprehensive regret analysis of our proposed\nframework, specifically examining the DLR under non-convex conditions with\nevent-driven online VFL. Extensive experiments demonstrated that our proposed\nframework was more stable than the existing online VFL framework under\nnon-stationary data conditions while also significantly reducing communication\nand computation costs.", "comment": "Published as a conference paper at ICLR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14911v1", "AI": {"title_translation": "事件驱动的在线垂直联邦学习", "tldr": "首次提出并解决了在线垂直联邦学习中数据异步和事件驱动的挑战，提出了一个事件驱动框架，并在非凸、非平稳环境下引入动态局部遗憾，实验证明其更稳定并降低了成本。", "motivation": "现有在线垂直联邦学习（VFL）忽视了客户端数据异步和事件驱动的特性，导致在实际场景中效率低下且面临挑战。特别是在VFL中，客户端拥有不相交的特征集，数据流通常由只与部分客户端相关的“事件”生成，而非同步接收，这在现有研究中被忽略了。", "method": "提出了一个事件驱动的在线VFL框架，在该框架中，每次事件只激活部分客户端，其余客户端被动协作。此外，引入了动态局部遗憾（DLR）来处理非平稳环境下非凸模型的在线学习问题，并进行了全面的遗憾分析。", "result": "实验证明所提出的框架在非平稳数据条件下比现有在线VFL框架更稳定，同时显著降低了通信和计算成本。", "conclusion": "该研究首次识别并解决了在线VFL中事件驱动和数据异步的挑战，通过提出的事件驱动框架和DLR，显著提升了在线VFL的稳定性和效率。", "translation": "在线学习相比离线学习更适应垂直联邦学习（VFL）中的实际场景。然而，将在线学习整合到VFL中带来了挑战，因为VFL具有独特的性质，即客户端拥有相同样本的不相交特征集。在实际场景中，客户端可能不会同步接收同一实体不相交特征的数据流。相反，数据通常由仅与部分客户端相关的“事件”生成。我们首次识别了在线VFL中的这些挑战，这些挑战被先前的研究忽视了。为了解决这些挑战，我们提出了一个事件驱动的在线VFL框架。在该框架中，每次事件只激活部分客户端，而其余客户端则被动地协作学习过程。此外，我们将动态局部遗憾（DLR）引入VFL，以解决非平稳环境下非凸模型的在线学习问题所带来的挑战。我们对所提出的框架进行了全面的遗憾分析，特别是研究了事件驱动在线VFL在非凸条件下的DLR。大量的实验表明，我们提出的框架在非平稳数据条件下比现有的在线VFL框架更稳定，同时显著降低了通信和计算成本。", "summary": "本文首次揭示了在线垂直联邦学习（VFL）中客户端数据异步和事件驱动的挑战，这些挑战在以往研究中被忽视。为解决此问题，作者提出了一个事件驱动的在线VFL框架，该框架在每次事件发生时仅激活部分相关客户端，并引入动态局部遗憾（DLR）以应对非凸、非平稳环境下的在线学习。实验结果表明，该框架在非平稳数据条件下更稳定，并显著降低了通信和计算成本。", "keywords": "在线联邦学习, 垂直联邦学习, 事件驱动, 动态局部遗憾, 非平稳环境", "comments": "这项研究的创新点在于首次识别并解决了在线VFL中数据生成异步和事件驱动的实际问题，这对于VFL在真实世界场景中的应用至关重要。通过引入事件驱动机制和动态局部遗憾分析，该框架不仅提高了模型的稳定性，还显著优化了资源消耗，为未来在线VFL的研究和实践提供了新的方向。"}}
{"id": "2506.14846", "title": "Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach", "authors": ["Shreyas Rajeev", "B Sathish Babu"], "summary": "Kernel size selection in Convolutional Neural Networks (CNNs) is a critical\nbut often overlooked design decision that affects receptive field, feature\nextraction, computational cost, and model accuracy. This paper proposes the\nBest Kernel Size Estimation Function (BKSEF), a mathematically grounded and\nempirically validated framework for optimal, layer-wise kernel size\ndetermination. BKSEF balances information gain, computational efficiency, and\naccuracy improvements by integrating principles from information theory, signal\nprocessing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100,\nImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided\narchitectures achieve up to 3.1 percent accuracy improvement and 42.8 percent\nreduction in FLOPs compared to traditional models using uniform 3x3 kernels.\nTwo real-world case studies further validate the approach: one for medical\nimage classification in a cloud-based setup, and another for traffic sign\nrecognition on edge devices. The former achieved enhanced interpretability and\naccuracy, while the latter reduced latency and model size significantly, with\nminimal accuracy trade-off. These results show that kernel size can be an\nactive, optimizable parameter rather than a fixed heuristic. BKSEF provides\npractical heuristics and theoretical support for researchers and developers\nseeking efficient and application-aware CNN designs. It is suitable for\nintegration into neural architecture search pipelines and real-time systems,\noffering a new perspective on CNN optimization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14846v1", "AI": {"title_translation": "在卷积神经网络中寻找最优核大小和维度：一种架构优化方法", "tldr": "该论文提出了BKSEF，一个用于确定CNN中最佳层级核大小的框架，显著提高了准确性并降低了计算成本。", "motivation": "卷积神经网络（CNN）中的核大小选择是一个关键但常被忽视的设计决策，它影响感受野、特征提取、计算成本和模型准确性。", "method": "本文提出了最佳核大小估计函数（BKSEF），这是一个基于数学原理并经过经验验证的框架，用于最优的、逐层核大小确定。BKSEF通过整合信息论、信号处理和学习理论的原理，平衡了信息增益、计算效率和准确性改进。", "result": "在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上的大量实验表明，与使用统一3x3核的传统模型相比，BKSEF指导的架构实现了高达3.1%的准确性提升和42.8%的FLOPs减少。两个真实世界案例研究进一步验证了该方法：一个用于基于云的医学图像分类，实现了增强的可解释性和准确性；另一个用于边缘设备上的交通标志识别，显著降低了延迟和模型大小，而准确性损失最小。", "conclusion": "这些结果表明，核大小可以是一个主动的、可优化的参数，而不是一个固定的启发式。BKSEF为寻求高效和应用感知CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持。它适用于集成到神经架构搜索流程和实时系统中，为CNN优化提供了新的视角。", "translation": "卷积神经网络（CNN）中的核大小选择是一个关键但常被忽视的设计决策，它影响感受野、特征提取、计算成本和模型准确性。本文提出了最佳核大小估计函数（BKSEF），这是一个基于数学原理并经过经验验证的框架，用于最优的、逐层核大小确定。BKSEF通过整合信息论、信号处理和学习理论的原理，平衡了信息增益、计算效率和准确性改进。在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上的大量实验表明，与使用统一3x3核的传统模型相比，BKSEF指导的架构实现了高达3.1%的准确性提升和42.8%的FLOPs减少。两个真实世界案例研究进一步验证了该方法：一个用于基于云的医学图像分类，实现了增强的可解释性和准确性；另一个用于边缘设备上的交通标志识别，显著降低了延迟和模型大小，而准确性损失最小。这些结果表明，核大小可以是一个主动的、可优化的参数，而不是一个固定的启发式。BKSEF为寻求高效和应用感知CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持。它适用于集成到神经架构搜索流程和实时系统中，为CNN优化提供了新的视角。", "summary": "本论文提出了一种名为最佳核大小估计函数（BKSEF）的框架，用于在卷积神经网络（CNN）中确定最优的逐层核大小。BKSEF结合了信息论、信号处理和学习理论，旨在平衡信息增益、计算效率和模型准确性。实验证明，与传统模型相比，BKSEF指导的CNN架构在多个数据集上显著提高了准确性（高达3.1%）并降低了计算成本（FLOPs减少42.8%）。该方法通过医学图像分类和交通标志识别的案例研究进一步验证，显示了其在增强可解释性、准确性、降低延迟和模型尺寸方面的潜力。研究强调核大小应被视为一个可优化的参数，而非固定启发式，为CNN设计优化提供了新的视角和实用支持。", "keywords": "核大小优化, 卷积神经网络, 架构优化, BKSEF, 神经网络搜索", "comments": "该论文的创新点在于将核大小视为一个可优化的参数，并提出了一个理论支持的框架BKSEF来系统地确定最优核大小，而非依赖于经验性选择。其重要性体现在显著提升CNN性能（准确性和效率），并为神经架构搜索和实时系统提供了新的优化方向。通过整合信息论、信号处理和学习理论，BKSEF为CNN设计提供了更深层次的理解和更精细的控制，有助于开发更高效和应用感知的模型。"}}
{"id": "2506.15665", "title": "A Data-Integrated Framework for Learning Fractional-Order Nonlinear Dynamical Systems", "authors": ["Bahram Yaghooti", "Chengyu Li", "Bruno Sinopoli"], "summary": "This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15665v1", "AI": {"title_translation": "一种学习分数阶非线性动力学系统的数据集成框架", "tldr": "本文提出一个数据集成框架，用于学习离散和连续时间分数阶非线性动力学系统，并通过实验验证了其有效性，指出分数阶模型在捕获长程依赖方面的优势。", "motivation": "该论文旨在提出一个数据集成框架，用于学习离散时间和连续时间设置下的分数阶非线性系统的动力学。", "method": "所提出的框架包括两个主要步骤：首先，设计输入输出实验以生成学习系统动力学所需的数据集，包括分数阶、漂移向量场和控制向量场。其次，利用这些数据集和分数阶系统的记忆依赖特性来估计系统的分数阶，并使用正交基函数重建漂移和控制向量场。该方法通过应用于四个基准分数阶系统进行验证，并与等效整数阶模型进行比较。", "result": "结果证实了所提出的框架在准确学习系统动力学方面的有效性。数值比较表明，分数阶模型能更好地捕获长程依赖性，突出了整数阶表示的局限性。", "conclusion": "所提出的数据集成框架能够有效且准确地学习分数阶非线性动力学系统。分数阶模型在捕获系统长程依赖方面优于整数阶模型。", "translation": "本文提出了一种数据集成框架，用于学习离散时间和连续时间设置下的分数阶非线性系统动力学。所提出的框架包括两个主要步骤。第一步，设计输入输出实验以生成学习系统动力学所需的必要数据集，包括分数阶、漂移向量场和控制向量场。第二步，利用这些数据集以及分数阶系统的记忆依赖特性来估计系统的分数阶。然后，使用正交基函数重建漂移和控制向量场。为了验证所提出的方法，将该算法应用于四个基准分数阶系统。结果证实了所提出的框架在准确学习系统动力学方面的有效性。最后，使用相同的数据集来学习等效的整数阶模型。数值比较表明，分数阶模型更好地捕获了长程依赖性，突出了整数阶表示的局限性。", "summary": "本文提出了一种数据集成框架，用于学习离散和连续时间分数阶非线性动力学系统。该框架通过设计输入输出实验生成数据集，并利用这些数据和分数阶系统的记忆特性来估计分数阶，并使用正交基函数重建向量场。通过对基准系统的应用验证了其有效性，并与整数阶模型比较，结果表明分数阶模型在捕获长程依赖方面具有显著优势。", "keywords": "分数阶系统, 非线性动力学, 数据集成框架, 系统辨识, 长程依赖", "comments": "该论文提出了一种新颖的数据集成框架，用于学习分数阶非线性动力学系统，涵盖了离散和连续时间场景。其创新之处在于结合了实验数据生成、分数阶估计和向量场重建。该研究的重要性在于证明了分数阶模型在处理具有长程依赖的复杂系统时，相比传统整数阶模型具有优越性，这为系统建模提供了新的视角和工具。"}}
{"id": "2506.15677", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15677v1", "AI": {"title_translation": "具身网络智能体：弥合物理-数字领域以实现集成智能体智能", "tldr": "目前的AI智能体大多是孤立的，要么专注于数字信息推理，要么专注于物理世界交互。本文提出了“具身网络智能体”这一新范式，旨在弥合具身和网络规模推理之间的鸿沟。为此，研究人员开发了一个统一的模拟平台和基准，用于评估需要物理和数字领域协同推理的任务，并发现现有AI系统与人类能力之间存在显著的性能差距。", "motivation": "当前的AI智能体要么专注于数字信息和知识的检索与推理，要么通过具身感知、规划和行动与物理世界交互，但很少两者兼顾。这种分离限制了它们解决需要集成物理和数字智能的任务（例如，根据在线食谱烹饪、使用动态地图数据导航、利用网络知识解释现实世界地标）的能力。", "method": "本文引入了“具身网络智能体”这一新型AI智能体范式，旨在流畅地连接具身和网络规模推理。为了实现这一概念，首先开发了“具身网络智能体任务环境”，这是一个紧密集成现实3D室内外环境与功能性网络界面的统一模拟平台。在此平台基础上，构建并发布了“具身网络智能体基准”，该基准包含烹饪、导航、购物、旅游和地理定位等多样化任务，所有任务都需要在物理和数字领域之间进行协调推理，以系统评估跨领域智能。", "result": "实验结果显示，最先进的AI系统与人类能力之间存在显著的性能差距，这在具身认知和网络规模知识获取的交叉领域既带来了挑战也提供了机遇。", "conclusion": "本文提出了“具身网络智能体”这一新型范式，并开发了相应的统一模拟平台和基准，以解决AI智能体在物理和数字领域之间集成智能的局限性。研究发现现有AI系统与人类能力之间存在显著差距，这为未来具身认知和网络知识集成研究指明了方向和机遇。", "translation": "目前的AI智能体大多是孤立的——它们要么检索并推理大量的在线数字信息和知识；要么通过具身感知、规划和行动与物理世界交互——但很少两者兼顾。这种分离限制了它们解决需要集成物理和数字智能的任务的能力，例如根据在线食谱烹饪、使用动态地图数据导航，或利用网络知识解释现实世界地标。我们引入了具身网络智能体（Embodied Web Agents），这是一种新型的AI智能体范式，它能够流畅地连接具身和网络规模推理。为了实现这一概念，我们首先开发了具身网络智能体任务环境，这是一个统一的模拟平台，紧密集成了真实的3D室内外环境与功能性网络界面。在此平台基础上，我们构建并发布了具身网络智能体基准，该基准包含多样化的任务套件，包括烹饪、导航、购物、旅游和地理定位——所有这些任务都需要在物理和数字领域之间进行协调推理，以系统评估跨领域智能。实验结果显示，最先进的AI系统与人类能力之间存在显著的性能差距，这在具身认知和网络规模知识获取的交叉领域既带来了挑战也提供了机遇。所有数据集、代码和网站均可在我们的项目页面https://embodied-web-agent.github.io/公开获取。", "summary": "本论文旨在解决当前AI智能体在物理和数字领域交互上的分离问题，提出了“具身网络智能体”这一新范式。为此，研究人员开发了一个统一的模拟平台和“具身网络智能体基准”，其中包含烹饪、导航等多种需要跨物理和数字领域协同推理的任务。实验结果表明，当前最先进的AI系统与人类能力之间存在显著的性能差距，揭示了具身认知和网络知识集成领域的挑战与机遇。", "keywords": "具身AI, 网络智能体, 集成智能, 模拟平台, 基准", "comments": "这篇论文具有创新性，它提出了一种新的范式，并创建了一个统一的平台和基准，旨在弥合具身AI和网络规模知识之间的鸿沟。它解决了当前AI系统的一个关键局限性，并为开发能够与物理和数字领域进行真正交互的智能体开辟了新的研究途径。"}}
{"id": "2506.14795", "title": "Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations", "authors": ["Batuhan Hangun", "Emine Akpinar", "Oguz Altun", "Onder Eyecioglu"], "summary": "Quantum Machine Learning (QML) is an emerging field at the intersection of\nquantum computing and machine learning, aiming to enhance classical machine\nlearning methods by leveraging quantum mechanics principles such as\nentanglement and superposition. However, skepticism persists regarding the\npractical advantages of QML, mainly due to the current limitations of noisy\nintermediate-scale quantum (NISQ) devices. This study addresses these concerns\nby extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired\ncounterparts of Artificial Neural Networks (ANNs), demonstrating their\neffectiveness compared to classical methods. We systematically construct and\nevaluate twelve distinct QNN configurations, utilizing two unique quantum\nfeature maps combined with six different entanglement strategies for ansatz\ndesign. Experiments conducted on a wind energy dataset reveal that QNNs\nemploying the Z feature map achieve up to 93% prediction accuracy when\nforecasting wind power output using only four input parameters. Our findings\nshow that QNNs outperform classical methods in predictive tasks, underscoring\nthe potential of QML in real-world applications.", "comment": "6 pages, 2 figures", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14795v1", "AI": {"title_translation": "用于风电预测的QNN架构比较分析：特征图和Ansatz配置", "tldr": "本研究通过评估不同QNN配置在风能数据集上的表现，证明了量子神经网络（QNNs）在风电预测任务中优于经典方法，并达到了高达93%的预测准确率。", "motivation": "量子机器学习（QML）的实际优势受到质疑，主要是由于当前噪声中等尺度量子（NISQ）设备的限制。本研究旨在通过评估量子神经网络（QNNs）来解决这些担忧，并证明其相对于经典方法的有效性。", "method": "系统地构建并评估了十二种不同的QNN配置，结合了两种独特的量子特征图和六种不同的纠缠策略用于ansatz设计。实验在风能数据集上进行。", "result": "采用Z特征图的QNNs在仅使用四个输入参数预测风电输出时，达到了高达93%的预测准确率。研究结果表明，QNNs在预测任务中优于经典方法。", "conclusion": "QNNs在预测任务中表现出色，凸显了量子机器学习在实际应用中的潜力。", "translation": "量子机器学习（QML）是量子计算和机器学习交叉领域的一个新兴领域，旨在通过利用量子力学原理（如纠缠和叠加）来增强经典机器学习方法。然而，对于QML的实际优势仍存在疑虑，这主要是由于当前噪声中等尺度量子（NISQ）设备的限制。本研究通过广泛评估量子神经网络（QNNs）——人工神经网络（ANNs）的量子启发对应物——来解决这些担忧，证明了它们与经典方法相比的有效性。我们系统地构建并评估了十二种不同的QNN配置，利用两种独特的量子特征图结合六种不同的纠缠策略进行ansatz设计。在风能数据集上进行的实验表明，采用Z特征图的QNNs在仅使用四个输入参数预测风电输出时，达到了高达93%的预测准确率。我们的研究结果表明，QNNs在预测任务中优于经典方法，强调了QML在实际应用中的潜力。", "summary": "本研究旨在解决对量子机器学习（QML）实用性的疑虑，通过系统比较和评估不同量子神经网络（QNNs）架构在风电预测任务中的表现。研究构建并测试了12种QNN配置，结合了不同特征图和纠缠策略。实验结果表明，QNNs，特别是使用Z特征图的QNNs，在风能数据集上实现了高达93%的预测准确率，并优于传统方法，从而证明了QML在实际应用中的潜力。", "keywords": "量子神经网络, 风电预测, 量子机器学习, 特征图, Ansatz配置", "comments": "该论文的创新之处在于系统地比较了不同QNN架构（包括特征图和纠缠策略）在实际应用（风电预测）中的性能。它通过实证结果证明了QML在NISQ设备限制下的有效性，为QML的实际应用提供了有力的支持，有助于缓解对QML实用性的普遍怀疑。其局限性可能在于其结果的普适性，即这些架构在其他数据集或应用中的表现如何。"}}
{"id": "2506.14802", "title": "ss-Mamba: Semantic-Spline Selective State-Space Model", "authors": ["Zuochen Ye"], "summary": "We propose ss-Mamba, a novel foundation model that enhances time series\nforecasting by integrating semantic-aware embeddings and adaptive spline-based\ntemporal encoding within a selective state-space modeling framework. Building\nupon the recent success of Transformer architectures, ss-Mamba adopts the Mamba\nselective state space model as an efficient alternative that achieves\ncomparable performance while significantly reducing computational complexity\nfrom quadratic to linear time. Semantic index embeddings, initialized from\npretrained language models, allow effective generalization to previously unseen\nseries through meaningful semantic priors. Additionally, spline-based\nKolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex\nseasonalities and non-stationary temporal effects, providing a powerful\nenhancement over conventional temporal feature encodings. Extensive\nexperimental evaluations confirm that ss-Mamba delivers superior accuracy,\nrobustness, and interpretability, demonstrating its capability as a versatile\nand computationally efficient alternative to traditional Transformer-based\nmodels in time-series forecasting.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14802v1", "AI": {"title_translation": "ss-Mamba：语义样条选择性状态空间模型", "tldr": "ss-Mamba是一种新型的时间序列预测模型，它将语义感知嵌入和自适应样条基时间编码集成到Mamba选择性状态空间模型中，旨在提供与Transformer相当的性能，同时将计算复杂度从二次降低到线性，并提升准确性、鲁棒性和可解释性。", "motivation": "旨在通过提出一种新型基础模型来增强时间序列预测能力，并解决传统Transformer架构在计算复杂度上的局限性，提供一个更高效且性能相当的替代方案。", "method": "提出了ss-Mamba模型，该模型在选择性状态空间建模框架内集成了语义感知嵌入和自适应样条基时间编码。它采用Mamba选择性状态空间模型，以实现线性时间复杂度。语义索引嵌入从预训练语言模型初始化，以实现对未见序列的泛化。此外，利用基于样条的Kolmogorov-Arnold Networks (KAN) 动态且可解释地捕捉复杂的季节性和非平稳时间效应。", "result": "广泛的实验评估证实，ss-Mamba在时间序列预测中提供了卓越的准确性、鲁棒性和可解释性。它被证明是传统基于Transformer模型的通用且计算高效的替代方案。", "conclusion": "ss-Mamba是一种在时间序列预测领域具有潜力的基础模型，它通过结合语义感知嵌入和样条基时间编码，在实现高准确性和可解释性的同时，显著提高了计算效率，是传统Transformer模型的有效替代。", "translation": "我们提出了ss-Mamba，这是一种新型基础模型，它通过在选择性状态空间建模框架内整合语义感知嵌入和自适应样条基时间编码来增强时间序列预测。基于Transformer架构的最新成功，ss-Mamba采用Mamba选择性状态空间模型作为一种高效的替代方案，它在实现可比性能的同时，将计算复杂度从二次显著降低到线性时间。从预训练语言模型初始化的语义索引嵌入，通过有意义的语义先验，能够有效地泛化到以前未见的序列。此外，基于样条的Kolmogorov-Arnold Networks (KAN) 能够动态且可解释地捕捉复杂的季节性和非平稳时间效应，与传统的时间特征编码相比，提供了强大的增强。广泛的实验评估证实，ss-Mamba提供了卓越的准确性、鲁棒性和可解释性，展示了其作为时间序列预测中传统基于Transformer模型的通用且计算高效的替代方案的能力。", "summary": "ss-Mamba是一种新型的时间序列预测基础模型，它将语义感知嵌入和自适应样条基时间编码集成到Mamba选择性状态空间模型中。该模型旨在提供与Transformer架构相当的性能，同时将计算复杂度从二次降低到线性。通过利用预训练语言模型的语义索引嵌入和基于样条的KAN，ss-Mamba能够有效泛化并捕捉复杂的时间效应。实验结果表明，ss-Mamba在时间序列预测中展现出卓越的准确性、鲁棒性和可解释性，是Transformer模型的有效且高效的替代品。", "keywords": "时间序列预测, ss-Mamba, 状态空间模型, 语义嵌入, 样条神经网络", "comments": "ss-Mamba的创新之处在于结合了Mamba选择性状态空间模型的效率、语义感知嵌入的泛化能力以及样条基KAN的可解释性，为时间序列预测提供了一个高性能且计算效率高的解决方案。其将计算复杂度从二次降低到线性，是其重要优势，使其在处理大规模时间序列数据时更具实用性。"}}
{"id": "2506.15138", "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models", "authors": ["Gyeongje Cho", "Yeonkyoun So", "Chanwoo Park", "Sangmin Lee", "Sungmok Jung", "Jaejin Lee"], "summary": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15138v1", "AI": {"title_translation": "Thunder-Tok：最小化生成式语言模型韩语文本分词中的每词Token数", "tldr": "Thunder-Tok是一种新的韩语分词器，通过规则预分词和基于分支熵的选择算法，在不影响模型性能的情况下，将每词Token数减少了约10%，从而提高了推理速度。", "motivation": "现有分词器可能导致较高的Token生育率（即每个词的Token数过多），这会降低生成式语言模型的推理速度。因此，需要一种新的韩语分词器来减少Token数量而不影响模型性能。", "method": "Thunder-Tok采用了基于规则的预分词方法，与韩语的语言结构对齐。它还创建了一个包含类似语言单元的种子词汇表，并采用了一种基于分支熵的选择算法来增加平均Token长度，从而降低生育率并保留语言信息。", "result": "实验结果表明，与BPE相比，Thunder-Tok将Token生育率降低了约10%（即Token数量减少了10%，推理速度提高了10%），同时在各种下游任务中没有降低性能。", "conclusion": "论文证明了Thunder-Tok这种受语言学启发的韩语分词方法是有效且实用的，能够为语言模型设计高效的分词器。", "translation": "本文介绍了Thunder-Tok，这是一种新的韩语分词器，旨在在不影响模型性能的情况下降低Token生育率。我们的方法采用了一种基于规则的预分词方法，该方法与韩语的语言结构对齐。我们还创建了一个包含类似于语言单元的种子词汇表，并采用了一种基于分支熵的选择算法。这些技术增加了平均Token长度，从而在保留语言信息的同时降低了生育率。实验结果表明，与BPE相比，Thunder-Tok将生育率降低了约10%（即Token数量减少了10%，推理速度提高了10%），同时在各种下游任务中没有降低性能。这些发现表明，我们受语言学启发的方法对于设计高效的语言模型分词器是有效且实用的。", "summary": "本文提出了一种名为Thunder-Tok的韩语分词器，旨在通过采用规则预分词和基于分支熵的选择算法来减少Token数量。该方法通过增加平均Token长度，在保持语言信息的同时降低了Token生育率。实验证明，Thunder-Tok在不牺牲性能的情况下，将Token数量减少了约10%，从而提升了生成式语言模型的推理速度。", "keywords": "韩语分词, Token化, 生成式语言模型, Thunder-Tok, Token生育率", "comments": "Thunder-Tok通过结合语言学知识（规则预分词）和先进的算法（分支熵选择）来优化韩语分词，实现了Token效率和模型性能的平衡。其创新之处在于提出了一种有效减少Token数量的方法，这对于提高韩语生成式语言模型的推理效率具有重要意义。该方法的可行性和实用性得到了实验验证。"}}
{"id": "2506.15165", "title": "A time-frequency method for acoustic scattering with trapping", "authors": ["Heather Wilber", "Wietse Vaes", "Abinand Gopal", "Gunnar Martinsson"], "summary": "A Fourier transform method is introduced for a class of hybrid time-frequency\nmethods that solve the acoustic scattering problem in regimes where the\nsolution exhibits both highly oscillatory behavior and slow decay in time. This\nextends the applicability of hybrid time-frequency schemes to domains with\ntrapping regions. A fast sinc transform technique for managing highly\noscillatory behavior and long time horizons is combined with a contour\nintegration scheme that improves smoothness properties in the integrand.", "comment": "18 pages, 9 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15165v1", "AI": {"title_translation": "一种用于带陷波声散射的时频方法", "tldr": "本文介绍了一种傅里叶变换方法，用于解决声散射问题中存在高度振荡行为和时间缓慢衰减的混合时频方案，扩展了其在陷波区域的适用性。", "motivation": "解决声散射问题中解表现出高度振荡行为和时间缓慢衰减的情况，并扩展混合时频方案在具有陷波区域的域中的适用性。", "method": "引入了一种傅里叶变换方法，结合了用于处理高度振荡行为和长时间范围的快速sinc变换技术，以及改善积分函数平滑特性的等值线积分方案。", "result": "将混合时频方案的适用性扩展到具有陷波区域的域。", "conclusion": "通过结合傅里叶变换、快速sinc变换和等值线积分，该方法成功处理了声散射问题中解的高度振荡和慢衰减特性，并使其适用于包含陷波区域的复杂场景。", "translation": "本文介绍了一种傅里叶变换方法，用于一类混合时频方法，该方法解决了在解表现出高度振荡行为和时间缓慢衰减的区域中的声散射问题。这扩展了混合时频方案在具有陷波区域的域中的适用性。一种用于管理高度振荡行为和长时间范围的快速sinc变换技术与一种改善积分函数平滑特性的等值线积分方案相结合。", "summary": "该论文提出了一种基于傅里叶变换的混合时频方法，旨在解决声散射问题中解具有高度振荡和时间缓慢衰减特性的挑战。通过结合快速sinc变换和等值线积分方案，该方法成功地将混合时频方案的应用范围扩展到包含陷波区域的复杂域。", "keywords": "时频方法, 声散射, 陷波, 傅里叶变换, sinc变换", "comments": "该论文的创新之处在于将傅里叶变换与快速sinc变换和等值线积分相结合，有效地处理了声散射问题中解的高度振荡和慢衰减现象，并成功地将时频方法应用于之前难以处理的陷波区域，这对于复杂介质中的波传播模拟具有重要意义。"}}
{"id": "2506.15365", "title": "FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation", "authors": ["Haolong Jin", "Shenglin Liu", "Cong Cong", "Qingmin Feng", "Yongzhi Liu", "Lina Huang", "Yingzi Hu"], "summary": "Federated learning (FL) has emerged as a promising approach for collaborative\nmedical image analysis, enabling multiple institutions to build robust\npredictive models while preserving sensitive patient data. In the context of\nWhole Slide Image (WSI) classification, FL faces significant challenges,\nincluding heterogeneous computational resources across participating medical\ninstitutes and privacy concerns. To address these challenges, we propose\nFedWSIDD, a novel FL paradigm that leverages dataset distillation (DD) to learn\nand transmit synthetic slides. On the server side, FedWSIDD aggregates\nsynthetic slides from participating centres and distributes them across all\ncentres. On the client side, we introduce a novel DD algorithm tailored to\nhistopathology datasets which incorporates stain normalisation into the\ndistillation process to generate a compact set of highly informative synthetic\nslides. These synthetic slides, rather than model parameters, are transmitted\nto the server. After communication, the received synthetic slides are combined\nwith original slides for local tasks. Extensive experiments on multiple WSI\nclassification tasks, including CAMELYON16 and CAMELYON17, demonstrate that\nFedWSIDD offers flexibility for heterogeneous local models, enhances local WSI\nclassification performance, and preserves patient privacy. This makes it a\nhighly effective solution for complex WSI classification tasks. The code is\navailable at FedWSIDD.", "comment": "MICCAI 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15365v1", "AI": {"title_translation": "FedWSIDD：基于数据集蒸馏的联邦全玻片图像分类", "tldr": "FedWSIDD通过数据集蒸馏生成并传输合成玻片，以应对联邦学习在全玻片图像分类中面临的异构资源和隐私挑战，同时提高分类性能并保护患者隐私。", "motivation": "联邦学习（FL）在医学图像分析中面临全玻片图像（WSI）分类的挑战，包括参与医疗机构之间计算资源的异构性和患者数据隐私问题。", "method": "本文提出了FedWSIDD，一种新的联邦学习范式，利用数据集蒸馏（DD）来学习和传输合成玻片。服务器端聚合来自各中心的合成玻片并分发；客户端引入了一种针对组织病理学数据集的DD算法，该算法将染色归一化纳入蒸馏过程，生成紧凑且信息丰富的合成玻片。这些合成玻片而非模型参数被传输到服务器，并在接收后与原始玻片结合用于本地任务。", "result": "在多个WSI分类任务（包括CAMELYON16和CAMELYON17）上的广泛实验表明，FedWSIDD为异构本地模型提供了灵活性，增强了本地WSI分类性能，并保护了患者隐私。", "conclusion": "FedWSIDD是解决复杂全玻片图像分类任务的一种高效解决方案。", "translation": "联邦学习（FL）已成为协作医学图像分析的一种有前景的方法，使多个机构能够在保护敏感患者数据的同时构建鲁棒的预测模型。在全玻片图像（WSI）分类的背景下，FL面临着重大挑战，包括参与医疗机构之间计算资源的异构性和隐私问题。为了应对这些挑战，我们提出了FedWSIDD，一种新颖的FL范式，它利用数据集蒸馏（DD）来学习和传输合成玻片。在服务器端，FedWSIDD聚合来自参与中心的合成玻片并将其分发到所有中心。在客户端，我们引入了一种针对组织病理学数据集的新型DD算法，该算法将染色归一化纳入蒸馏过程，以生成一组紧凑且信息丰富的合成玻片。这些合成玻片而非模型参数被传输到服务器。通信后，接收到的合成玻片与原始玻片结合用于本地任务。在多个WSI分类任务（包括CAMELYON16和CAMELYON17）上的广泛实验表明，FedWSIDD为异构本地模型提供了灵活性，增强了本地WSI分类性能，并保护了患者隐私。这使其成为复杂WSI分类任务的一种高效解决方案。代码可在FedWSIDD获取。", "summary": "FedWSIDD是一种创新的联邦学习框架，专门用于解决全玻片图像（WSI）分类中的异构资源和隐私挑战。它通过数据集蒸馏生成并传输合成玻片，而非模型参数，以促进机构间协作。客户端的DD算法结合了染色归一化，以创建高质量的合成数据。实验证明，FedWSIDD在保持患者隐私的同时，提高了WSI分类性能并支持异构本地模型。", "keywords": "联邦学习, 全玻片图像分类, 数据集蒸馏, 隐私保护, 组织病理学", "comments": "FedWSIDD通过引入数据集蒸馏而非直接传输模型参数，为联邦学习在医学图像领域（特别是WSI分类）提供了一种新颖且有前途的隐私保护方法。其创新之处在于将染色归一化整合到蒸馏过程中，这对于组织病理学数据至关重要。这种方法不仅解决了隐私和异构性问题，还通过传输更紧凑、信息量更大的数据来潜在地提高通信效率，是联邦学习在敏感数据应用中向前迈出的重要一步。"}}
{"id": "2506.15220", "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models", "authors": ["Changli Tang", "Yixuan Li", "Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15220v1", "AI": {"title_translation": "video-SALMONN 2：字幕增强的音视频大型语言模型", "tldr": "video-SALMONN 2是一个字幕增强的音视频大型语言模型，通过新型多轮偏好优化方法（MrDPO）显著提升了视频字幕生成准确性，超越了GPT-4o和Gemini-1.5-Pro等领先模型。", "motivation": "视频包含丰富信息，生成详细准确的自然语言描述是视频理解的关键，但现有方法可能不足以达到理想的完整性和准确性。", "method": "本文提出了video-SALMONN 2，一个结合低秩适应（LoRA）和定向偏好优化（DPO）的音视频大型语言模型，用于增强视频字幕生成。为评估和优化，引入了新的视频描述完整性和准确性指标。为进一步提升训练效果，提出了多轮DPO（MrDPO）方法，该方法周期性更新DPO参考模型，在每个训练轮次后合并并重新初始化LoRA模块作为参数更新的替代，并融入真实视频字幕指导以稳定训练过程。", "result": "实验结果表明，MrDPO显著提升了video-SALMONN 2的字幕准确性，将字幕错误率降低了28%。最终的video-SALMONN 2模型（70亿参数）在视频字幕任务上超越了GPT-4o和Gemini-1.5-Pro等领先模型，并在同等规模模型的视频问答基准测试中保持了与最先进技术的高度竞争力。", "conclusion": "通过引入多轮定向偏好优化（MrDPO）和新的评估指标，video-SALMONN 2显著提升了音视频大型语言模型在视频字幕生成方面的性能，实现了优于现有顶尖模型的表现。", "translation": "视频包含丰富信息，生成详细准确的自然语言描述是视频理解的关键方面。本文提出了video-SALMONN 2，一个先进的音视频大型语言模型（LLM），它采用低秩适应（LoRA）并通过定向偏好优化（DPO）来增强视频（带配对音频）字幕生成。我们提出了新的指标来评估视频描述的完整性和准确性，这些指标通过DPO进行优化。为了进一步改进训练，我们提出了一种新颖的多轮DPO（MrDPO）方法，该方法涉及周期性更新DPO参考模型，在每个训练轮次（1,000步）后合并并重新初始化LoRA模块作为参数更新的替代，并结合真实视频字幕的指导来稳定训练过程。实验结果表明，MrDPO显著提升了video-SALMONN 2的字幕准确性，将字幕错误率降低了28%。最终的video-SALMONN 2模型，仅有70亿参数，在视频字幕任务上超越了GPT-4o和Gemini-1.5-Pro等领先模型，同时在广泛使用的视频问答基准测试中，在同等规模模型中保持了与最先进技术的高度竞争力。代码可在https://github.com/bytedance/video-SALMONN-2获取。", "summary": "本文介绍了video-SALMONN 2，一个利用低秩适应（LoRA）和定向偏好优化（DPO）技术开发的音视频大型语言模型，旨在提升视频字幕生成能力。为解决视频描述的完整性和准确性问题，研究提出了新的评估指标和一种创新的多轮DPO（MrDPO）训练方法。MrDPO通过周期性更新参考模型和LoRA模块，并结合真实字幕指导，有效稳定并优化了训练过程。实验证明，video-SALMONN 2在MrDPO的加持下，显著降低了字幕错误率，并在视频字幕任务上超越了GPT-4o和Gemini-1.5-Pro等先进模型，同时在视频问答任务上保持了强大的竞争力。", "keywords": "音视频大型语言模型, 视频字幕生成, 定向偏好优化, 多轮DPO, LoRA", "comments": "这项工作在音视频大型语言模型的视频字幕生成方面取得了显著进展。其创新点在于引入了多轮DPO（MrDPO）方法，该方法通过周期性更新和LoRA模块的巧妙运用，有效提升了训练的稳定性和模型性能。同时，提出新的评估指标也体现了对视频描述质量更深层次的关注。模型在参数量相对较小的情况下，超越了更大规模的领先模型，显示了其高效性和潜力。这对于推动多模态LLM在实际应用中的发展具有重要意义。"}}
{"id": "2506.15224", "title": "Facility Location Problem under Local Differential Privacy without Super-set Assumption", "authors": ["Kevin Pfisterer", "Quentin Hillebrand", "Vorapong Suppakitpaisarn"], "summary": "In this paper, we introduce an adaptation of the facility location problem\nand analyze it within the framework of local differential privacy (LDP). Under\nthis model, we ensure the privacy of client presence at specific locations.\nWhen n is the number of points, Gupta et al. established a lower bound of\n$\\Omega(\\sqrt{n})$ on the approximation ratio for any differentially private\nalgorithm applied to the original facility location problem. As a result,\nsubsequent works have adopted the super-set assumption, which may, however,\ncompromise user privacy. We show that this lower bound does not apply to our\nadaptation by presenting an LDP algorithm that achieves a constant\napproximation ratio with a relatively small additive factor. Additionally, we\nprovide experimental results demonstrating that our algorithm outperforms the\nstraightforward approach on both synthetically generated and real-world\ndatasets.", "comment": "accepted at DBSec 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15224v1", "AI": {"title_translation": "局部差分隐私下无需超集假设的设施选址问题", "tldr": "本文在局部差分隐私（LDP）框架下提出了一种设施选址问题的改进方法，避免了可能损害用户隐私的超集假设，并提出了一个LDP算法，实现了常数近似比，且在实验中表现优于传统方法。", "motivation": "现有的差分隐私设施选址算法存在近似比的下界限制（$\\\\Omega(\\\\sqrt{n})$），并且后续工作为了规避此下界而采用了可能损害用户隐私的超集假设。", "method": "本文引入了设施选址问题的一种新适应版本，并在局部差分隐私（LDP）框架下进行分析，确保客户位置隐私。为此，本文提出了一个LDP算法。", "result": "本文证明所提出的适应版本不受现有近似比下界的影响，所提出的LDP算法实现了常数近似比，且具有相对较小的附加因子。实验结果表明，该算法在合成和真实世界数据集上均优于直接方法。", "conclusion": "本文成功在局部差分隐私框架下解决了设施选址问题，避免了超集假设，并实现了一个具有常数近似比且性能优越的算法，有效保护了用户隐私。", "translation": "在本文中，我们引入了设施选址问题的一种适应版本，并在局部差分隐私（LDP）框架内对其进行了分析。在该模型下，我们确保了客户在特定位置存在的隐私。当n是点的数量时，Gupta等人为应用于原始设施选址问题的任何差分隐私算法建立了$\\\\Omega(\\\\sqrt{n})$的近似比下界。结果，后续工作采用了超集假设，然而这可能会损害用户隐私。我们通过提出一个LDP算法来表明，该下界不适用于我们的适应版本，该算法以相对较小的附加因子实现了常数近似比。此外，我们提供了实验结果，表明我们的算法在合成生成和真实世界数据集上均优于直接方法。", "summary": "本文在局部差分隐私（LDP）框架下，提出了一种不依赖超集假设的设施选址问题的新适应版本，以确保客户位置隐私。针对现有差分隐私算法近似比下界高且常依赖隐私风险的超集假设的问题，本文设计了一个LDP算法。该算法成功规避了$\\\\Omega(\\\\sqrt{n})$的近似比下界，实现了常数近似比和较小的附加因子。实验证明，该算法在合成数据和真实数据上均优于传统方法。", "keywords": "设施选址问题, 局部差分隐私, 超集假设, 近似比", "comments": "该论文的创新之处在于，它在局部差分隐私框架下成功地解决了设施选址问题，同时避免了以往研究中为了规避近似比下界而不得不采用的、可能损害用户隐私的超集假设。通过提出一种新的问题适应版本和相应的LDP算法，该研究在隐私保护和算法性能之间取得了更好的平衡，实现了常数近似比，并提供了实验证据支持其优越性。"}}
{"id": "2506.15132", "title": "Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion", "authors": ["Yushi Wang", "Penghui Chen", "Xinyu Han", "Feng Wu", "Mingguo Zhao"], "summary": "Recent advancements in reinforcement learning (RL) have led to significant\nprogress in humanoid robot locomotion, simplifying the design and training of\nmotion policies in simulation. However, the numerous implementation details\nmake transferring these policies to real-world robots a challenging task. To\naddress this, we have developed a comprehensive code framework that covers the\nentire process from training to deployment, incorporating common RL training\nmethods, domain randomization, reward function design, and solutions for\nhandling parallel structures. This library is made available as a community\nresource, with detailed descriptions of its design and experimental results. We\nvalidate the framework on the Booster T1 robot, demonstrating that the trained\npolicies seamlessly transfer to the physical platform, enabling capabilities\nsuch as omnidirectional walking, disturbance resistance, and terrain\nadaptability. We hope this work provides a convenient tool for the robotics\ncommunity, accelerating the development of humanoid robots. The code can be\nfound in https://github.com/BoosterRobotics/booster_gym.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15132v1", "AI": {"title_translation": "Booster Gym：一个人形机器人运动的端到端强化学习框架", "tldr": "Booster Gym是一个端到端的强化学习框架，旨在简化人形机器人运动策略从模拟到真实世界的部署，并通过实验验证了其在物理机器人上的有效性。", "motivation": "尽管强化学习在模拟环境中显著推动了人形机器人运动策略的设计和训练，但将这些策略转移到现实世界机器人面临着大量实现细节带来的挑战。", "method": "开发了一个全面的代码框架，涵盖从训练到部署的整个过程，并整合了常见的强化学习训练方法、领域随机化、奖励函数设计以及处理并行结构的解决方案。", "result": "该框架在Booster T1机器人上进行了验证，训练出的策略能够无缝转移到物理平台，实现了全向行走、抗干扰和地形适应性等能力。", "conclusion": "这项工作为机器人社区提供了一个方便的工具，有望加速人形机器人的开发。", "translation": "近期强化学习（RL）的进展使人形机器人运动取得了显著进步，简化了模拟环境中运动策略的设计和训练。然而，大量的实现细节使得将这些策略转移到现实世界机器人成为一项具有挑战性的任务。为了解决这个问题，我们开发了一个全面的代码框架，涵盖从训练到部署的整个过程，整合了常见的RL训练方法、领域随机化、奖励函数设计以及处理并行结构的解决方案。该库作为社区资源提供，并附有其设计和实验结果的详细描述。我们在Booster T1机器人上验证了该框架，证明训练出的策略能够无缝转移到物理平台，实现了全向行走、抗干扰和地形适应性等能力。我们希望这项工作能为机器人社区提供一个方便的工具，加速人形机器人的开发。代码可在https://github.com/BoosterRobotics/booster_gym找到。", "summary": "本文提出了Booster Gym，一个端到端的强化学习框架，旨在解决将人形机器人运动策略从模拟环境转移到现实世界机器人所面临的挑战。该框架集成了多种RL训练技术和设计考虑，并通过在Booster T1机器人上的实验验证，展示了其训练策略在物理平台上的无缝迁移能力，实现了包括全向行走和地形适应性在内的多种高级运动功能。该工作作为一个开源工具，旨在促进人形机器人领域的发展。", "keywords": "强化学习, 人形机器人, 运动控制, 框架, 领域随机化", "comments": "这项工作的创新之处在于提供了一个全面的、端到端的强化学习框架，有效解决了强化学习策略在人形机器人上从模拟到真实世界部署的“现实差距”问题。其开源性质和在实际机器人上的成功验证，对于推动人形机器人研究和开发具有重要意义。"}}
{"id": "2506.15314", "title": "Case Study for Developing a UXR Point of View for FinOps Product Innovation", "authors": ["Jason Dong", "Anna Wu"], "summary": "In the dynamic landscape of Cloud financial management, we are sharing a case\nstudy exploring the development of a User Experience Research (UXR) Point of\nView (PoV) to drive FinOps product innovation. We demonstrate how qualitative\nand quantitative research methods working together to navigate the challenges\nof understanding customer needs, aligning cross-functional teams, and\nprioritizing limited resources. Through a multi-phased research approach, the\nresearch team identifies opportunities, quantifies pain points, and segments\ndiverse customer cohorts. This culminated in a UXR PoV that informed the\ncreation of a differentiated product strategy, a 'one-stop shop' dashboard\nempowering FinOps practitioners with actionable insights and tools. This case\nstudy highlights the power of mixed-methods research in uncovering actionable\ninsights that drive impactful product innovation.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15314v1", "AI": {"title_translation": "针对FinOps产品创新开发用户体验研究（UXR）视角的案例研究", "tldr": "该案例研究探讨了如何通过用户体验研究（UXR）视角推动FinOps产品创新，利用定性和定量混合方法来理解客户需求、协调团队并优化资源，最终形成了差异化的产品策略和一个“一站式”仪表板。", "motivation": "在动态的云财务管理环境中，该研究的动机是通过开发用户体验研究（UXR）视角来推动FinOps产品创新，以应对理解客户需求、协调跨职能团队和优先分配有限资源的挑战。", "method": "本研究采用多阶段的混合方法研究方法，结合定性和定量研究，以识别机会、量化痛点并细分不同的客户群体。", "result": "研究成果是一个用户体验研究（UXR）视角，该视角为制定差异化的产品策略和一个赋能FinOps从业者提供可操作洞察和工具的“一站式”仪表板提供了信息。", "conclusion": "本案例研究强调了混合方法研究在发现可操作洞察以推动有影响力的产品创新方面的力量。", "translation": "在动态的云财务管理环境中，我们分享了一个案例研究，探讨了如何开发用户体验研究（UXR）视角以推动FinOps产品创新。我们展示了定性和定量研究方法如何协同工作，以应对理解客户需求、协调跨职能团队和优先分配有限资源的挑战。通过多阶段的研究方法，研究团队识别了机会、量化了痛点并细分了不同的客户群体。这最终形成了一个UXR视角，为创建差异化的产品策略和一个赋能FinOps从业者提供可操作洞察和工具的“一站式”仪表板提供了信息。本案例研究强调了混合方法研究在发现可操作洞察以推动有影响力的产品创新方面的力量。", "summary": "本案例研究探讨了在云财务管理领域，如何通过开发用户体验研究（UXR）视角来驱动FinOps产品创新。文章详细阐述了如何利用定性和定量相结合的混合研究方法，应对理解客户需求、协调跨职能团队及优化资源分配的挑战。通过多阶段研究，团队成功识别了机遇、量化了用户痛点并进行了客户群体细分，最终形成了一个指导产品策略的UXR视角，并催生了一个为FinOps从业者提供实用洞察和工具的“一站式”仪表板。这突显了混合方法研究在实现有效产品创新中的关键作用。", "keywords": "UXR, FinOps, 产品创新, 案例研究, 混合方法研究", "comments": "该案例研究有效地展示了用户体验研究（UXR）在推动FinOps产品创新中的实际应用和价值。其创新点在于强调了混合方法研究在复杂业务环境中理解用户需求和指导产品开发的重要性。通过结合定性和定量数据，该研究能够全面识别痛点并形成切实可行的产品策略，为云财务管理领域的未来产品开发提供了宝贵的实践经验。"}}
{"id": "2506.14854", "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "authors": ["Varun Mannam", "Zhenyu Shi"], "summary": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring.", "comment": "Submitting to ICCV 2025 workshop:\n  https://retailvisionworkshop.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14854v1", "AI": {"title_translation": "高效零售视频标注：一种用于产品和客户互动分析的鲁棒关键帧生成方法", "tldr": "本文提出了一种深度学习方法，通过自动化关键帧识别和产品/客户自动标注，显著提高了零售视频标注的效率并降低了成本。", "motivation": "传统的视频标注方法依赖耗时的人工标注，导致帧选择不鲁棒且运营成本高昂，这在零售应用中（如客户行为分析、产品互动检测和店内活动识别）是一个挑战。", "method": "本文提出了一种基于深度学习的方法，通过嵌入视频帧并结合针对零售环境的对象检测技术，利用深度神经网络学习判别性特征，从而自动化零售视频中的关键帧识别并提供产品和客户的自动标注。", "result": "实验结果表明，该方法优于传统方法，实现了与人工标注相当的准确性，同时提高了零售视频标注的整体效率。它使视频标注成本平均节省了2倍，并且在不降低标注质量的情况下，人工标注者只需验证/调整不到5%的检测帧。", "conclusion": "通过自动化关键帧检测，该方法在零售视频标注任务中节省了大量时间和精力，对于购物者旅程分析、产品互动检测和店内安全监控等多种零售应用具有重要价值，显著降低了运营成本。", "translation": "准确的视频标注在现代零售应用中扮演着至关重要的角色，包括客户行为分析、产品互动检测和店内活动识别。然而，传统的标注方法严重依赖人工标注者耗时的人工标注，引入了不鲁棒的帧选择并增加了运营成本。为了解决零售领域的这些挑战，我们提出了一种基于深度学习的方法，该方法自动化了零售视频中的关键帧识别，并提供了产品和客户的自动标注。我们的方法利用深度神经网络，通过嵌入视频帧并结合针对零售环境的对象检测技术来学习判别性特征。实验结果表明，我们的方法优于传统方法，实现了与人工标注相当的准确性，同时提高了零售视频标注的整体效率。值得注意的是，我们的方法使视频标注成本平均节省了2倍。通过允许人工标注者验证/调整视频数据集中不到5%的检测帧，同时在不降低标注质量的情况下自动化其余帧的标注过程，零售商可以显著降低运营成本。关键帧检测的自动化在零售视频标注任务中节省了大量时间和精力，对于购物者旅程分析、产品互动检测和店内安全监控等多种零售应用具有高度价值。", "summary": "本研究提出了一种基于深度学习的零售视频标注方法，旨在解决传统人工标注耗时且成本高昂的问题。该方法利用深度神经网络进行关键帧识别和产品/客户的自动标注，并通过结合目标检测技术来学习判别性特征。实验证明，该方法在保持高准确性的同时，显著提高了标注效率，平均节省了2倍的成本，并且只需人工验证少量帧，极大地减轻了零售商的运营负担，对多种零售应用场景具有重要价值。", "keywords": "零售视频标注, 关键帧生成, 深度学习, 客户互动分析, 成本节约", "comments": "该论文提出了一种创新的深度学习方法，通过自动化关键帧识别和标注，显著提升了零售视频分析的效率和成本效益。其核心创新在于将深度神经网络与目标检测技术相结合，实现高精度自动化。这项工作对于零售行业具有重要意义，能够帮助企业更有效地进行客户行为分析、产品互动检测和安全监控。其主要优势在于显著降低了人工标注的成本和时间，同时保持了标注质量，为零售视频大数据处理提供了实用的解决方案。"}}
{"id": "2506.15150", "title": "Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation", "authors": ["Yuanlong Ji", "Xingbang Yang", "Ruoqi Zhao", "Qihan Ye", "Quan Zheng", "Yubo Fan"], "summary": "Gait phase estimation based on inertial measurement unit (IMU) signals\nfacilitates precise adaptation of exoskeletons to individual gait variations.\nHowever, challenges remain in achieving high accuracy and robustness,\nparticularly during periods of terrain changes. To address this, we develop a\ngait phase estimation neural network based on implicit modeling of human\nlocomotion, which combines temporal convolution for feature extraction with\ntransformer layers for multi-channel information fusion. A channel-wise masked\nreconstruction pre-training strategy is proposed, which first treats gait phase\nstate vectors and IMU signals as joint observations of human locomotion, thus\nenhancing model generalization. Experimental results demonstrate that the\nproposed method outperforms existing baseline approaches, achieving a gait\nphase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under\nstable terrain conditions with a look-back window of 2 seconds, and a phase\nRMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain\ntransitions. Hardware validation on a hip exoskeleton further confirms that the\nalgorithm can reliably identify gait cycles and key events, adapting to various\ncontinuous motion scenarios. This research paves the way for more intelligent\nand adaptive exoskeleton systems, enabling safer and more efficient human-robot\ninteraction across diverse real-world environments.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15150v1", "AI": {"title_translation": "基于人体运动隐式建模的实时步态相位估计", "tldr": "本文提出了一种基于隐式建模的神经网络，用于实时步态相位估计，结合了时域卷积和Transformer层，并通过通道掩码重建预训练策略提高了模型泛化能力，在稳定和复杂地形下均表现优异，并已在髋部外骨骼上验证。", "motivation": "基于惯性测量单元（IMU）信号的步态相位估计对于外骨骼适应个体步态变化至关重要，但在实现高精度和鲁棒性方面仍存在挑战，尤其是在地形变化期间。", "method": "本文开发了一种基于人体运动隐式建模的步态相位估计神经网络。该网络结合了用于特征提取的时域卷积和用于多通道信息融合的Transformer层。同时，提出了一种通道级掩码重建预训练策略，将步态相位状态向量和IMU信号视为人体运动的联合观测，从而增强了模型泛化能力。", "result": "实验结果表明，该方法优于现有基线方法。在稳定地形条件下，步态相位RMSE为2.729 ± 1.071%，相位速率MAE为0.037 ± 0.016%（2秒回溯窗口）。在地形转换下，相位RMSE为3.215 ± 1.303%，速率MAE为0.050 ± 0.023%。在髋部外骨骼上的硬件验证进一步证实，该算法能够可靠识别步态周期和关键事件，适应各种连续运动场景。", "conclusion": "这项研究为更智能和自适应的外骨骼系统铺平了道路，从而在各种现实环境中实现更安全、更高效的人机交互。", "translation": "基于惯性测量单元（IMU）信号的步态相位估计有助于外骨骼精确适应个体步态变化。然而，在实现高精度和鲁棒性方面仍存在挑战，尤其是在地形变化期间。为了解决这个问题，我们开发了一种基于人体运动隐式建模的步态相位估计神经网络，该网络结合了用于特征提取的时域卷积和用于多通道信息融合的Transformer层。提出了一种通道级掩码重建预训练策略，该策略首先将步态相位状态向量和IMU信号视为人体运动的联合观测，从而增强了模型泛化能力。实验结果表明，所提出的方法优于现有基线方法，在2秒回溯窗口的稳定地形条件下，步态相位RMSE为2.729 ± 1.071%，相位速率MAE为0.037 ± 0.016%；在地形转换下，相位RMSE为3.215 ± 1.303%，速率MAE为0.050 ± 0.023%。在髋部外骨骼上的硬件验证进一步证实，该算法能够可靠识别步态周期和关键事件，适应各种连续运动场景。这项研究为更智能和自适应的外骨骼系统铺平了道路，从而在各种现实环境中实现更安全、更高效的人机交互。", "summary": "本文提出了一种基于人体运动隐式建模的实时步态相位估计神经网络，旨在解决IMU信号在复杂地形下步态估计的准确性和鲁棒性问题。该网络结合了时域卷积和Transformer层进行特征提取与信息融合，并通过通道级掩码重建预训练策略提升模型泛化能力。实验结果显示，该方法在稳定和变化地形下均优于现有基线，且在髋部外骨骼上成功验证了其在连续运动场景下识别步态事件的能力，为开发更智能、自适应的外骨骼系统奠定了基础。", "keywords": "步态相位估计, 惯性测量单元, 隐式建模, 神经网络, 外骨骼", "comments": "本文创新性地将隐式建模、时域卷积和Transformer层结合用于步态相位估计，并引入了通道级掩码重建预训练策略，有效提升了模型在复杂地形下的鲁棒性和泛化能力。其在实际外骨骼上的验证，证明了该方法的实用性和重要性，对于推动人机交互和康复辅助技术发展具有积极意义。"}}
{"id": "2207.01732", "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition", "authors": ["Jiamin Xie", "John H. L. Hansen"], "summary": "Convolutional neural networks (CNN) have improved speech recognition\nperformance greatly by exploiting localized time-frequency patterns. But these\npatterns are assumed to appear in symmetric and rigid kernels by the\nconventional CNN operation. It motivates the question: What about asymmetric\nkernels? In this study, we illustrate adaptive views can discover local\nfeatures which couple better with attention than fixed views of the input. We\nreplace depthwise CNNs in the Conformer architecture with a deformable\ncounterpart, dubbed this \"Deformer\". By analyzing our best-performing model, we\nvisualize both local receptive fields and global attention maps learned by the\nDeformer and show increased feature associations on the utterance level. The\nstatistical analysis of learned kernel offsets provides an insight into the\nchange of information in features with the network depth. Finally, replacing\nonly half of the layers in the encoder, the Deformer improves +5.6% relative\nWER without a LM and +6.4% relative WER with a LM over the Conformer baseline\non the WSJ eval92 set.", "comment": "Accepted to Interspeech 2022", "cate": "eess.AS", "url": "http://arxiv.org/abs/2207.01732v2", "AI": {"title_translation": "DEFORMER：结合变形局部模式与全局上下文的鲁棒端到端语音识别", "tldr": "DEFORMER提出用可变形卷积取代Conformer中的深度卷积，以更好地捕捉语音中的不对称局部模式，从而显著提升了端到端语音识别的性能。", "motivation": "传统的卷积神经网络（CNN）在语音识别中利用局部时频模式时，假设这些模式出现在对称且固定的核中。本文的动机是探索非对称核的可能性，以及自适应视图如何更好地发现局部特征并与注意力机制结合。", "method": "本文将Conformer架构中的深度卷积神经网络替换为可变形的对应部分，称之为“Deformer”。通过分析表现最佳的模型，可视化了Deformer学习到的局部感受野和全局注意力图，并对学习到的核偏移量进行了统计分析。", "result": "Deformer在WSJ eval92数据集上，相对于Conformer基线，在不使用语言模型的情况下，相对词错误率（WER）提高了+5.6%；在使用语言模型的情况下，相对WER提高了+6.4%。", "conclusion": "Deformer通过引入可变形卷积，能够学习到更具适应性的局部特征，并与全局上下文更好地结合，从而显著提升了端到端语音识别的性能。", "translation": "卷积神经网络（CNN）通过利用局部时频模式极大地提高了语音识别性能。但这些模式在传统CNN操作中被假定出现在对称且固定的核中。这引发了一个问题：非对称核会怎样？在本研究中，我们展示了自适应视图可以发现与注意力机制结合得更好的局部特征，而非输入固定的视图。我们将Conformer架构中的深度卷积神经网络替换为可变形的对应部分，称之为“Deformer”。通过分析我们表现最佳的模型，我们可视化了Deformer学习到的局部感受野和全局注意力图，并展示了在语句级别上增加的特征关联。学习到的核偏移量的统计分析提供了对特征信息随网络深度变化的洞察。最后，仅替换编码器中一半的层，Deformer在WSJ eval92数据集上，相对于Conformer基线，在不使用语言模型的情况下，相对词错误率（WER）提高了+5.6%，在使用语言模型的情况下，相对WER提高了+6.4%。", "summary": "本文提出了Deformer，一种改进的语音识别模型，旨在解决传统CNN在捕捉非对称时频模式方面的局限性。Deformer通过将Conformer中的深度卷积替换为可变形卷积，使其能够学习更灵活的局部特征。实验结果表明，Deformer在WSJ eval92数据集上显著优于Conformer基线，证明了其在端到端语音识别中的鲁棒性和有效性。", "keywords": "语音识别, 可变形卷积, Conformer, 深度学习, 端到端", "comments": "Deformer的创新之处在于将可变形卷积引入到语音识别的Conformer架构中，解决了传统CNN在处理复杂和非对称语音模式时的局限性。这种方法使得模型能够自适应地捕获局部特征，并更好地与全局注意力机制结合，从而显著提升了性能。这项工作为未来的语音识别模型设计提供了新的思路，特别是在特征表示的灵活性和鲁棒性方面。"}}
{"id": "2506.14806", "title": "Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis", "authors": ["Bochen Lyu", "Xiaojing Zhang", "Fangyi Zheng", "He Wang", "Zheng Wang", "Zhanxing Zhu"], "summary": "This paper establishes a continuous time approximation, a piece-wise\ncontinuous differential equation, for the discrete Heavy-Ball (HB) momentum\nmethod with explicit discretization error. Investigating continuous\ndifferential equations has been a promising approach for studying the discrete\noptimization methods. Despite the crucial role of momentum in gradient-based\noptimization methods, the gap between the original discrete dynamics and the\ncontinuous time approximations due to the discretization error has not been\ncomprehensively bridged yet. In this work, we study the HB momentum method in\ncontinuous time while putting more focus on the discretization error to provide\nadditional theoretical tools to this area. In particular, we design a\nfirst-order piece-wise continuous differential equation, where we add a number\nof counter terms to account for the discretization error explicitly. As a\nresult, we provide a continuous time model for the HB momentum method that\nallows the control of discretization error to arbitrary order of the step size.\nAs an application, we leverage it to find a new implicit regularization of the\ndirectional smoothness and investigate the implicit bias of HB for diagonal\nlinear networks, indicating how our results can be used in deep learning. Our\ntheoretical findings are further supported by numerical experiments.", "comment": "32 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14806v1", "AI": {"title_translation": "连续时间下的重球动量法及离散化误差分析", "tldr": "本文通过显式建模离散化误差，弥合了离散重球动量法与连续时间近似之间的差距，从而实现了更好的误差控制，并应用于深度学习。", "motivation": "尽管动量在基于梯度的优化方法中扮演着关键角色，但由于离散化误差导致原始离散动力学与连续时间近似之间的差距尚未得到全面弥合。", "method": "本文设计了一个一阶分段连续微分方程，通过添加补偿项来明确解释离散化误差，从而为重球动量法提供了一个连续时间模型。", "result": "该研究为重球动量法提供了一个连续时间模型，允许将离散化误差控制到任意阶步长。作为应用，它发现了方向平滑度的一种新的隐式正则化，并研究了HB对对角线性网络的隐式偏差。理论发现得到了数值实验的支持。", "conclusion": "本文为重球动量法提供了一种新颖的连续时间近似，该近似明确考虑并控制了离散化误差，为理解和应用动量方法（特别是在深度学习中）提供了新的理论工具。", "translation": "本文为离散重球（HB）动量法建立了一个连续时间近似，即一个分段连续微分方程，并明确给出了离散化误差。研究连续微分方程一直是研究离散优化方法的一个有前景的方法。尽管动量在基于梯度的优化方法中扮演着关键角色，但由于离散化误差导致原始离散动力学与连续时间近似之间的差距尚未得到全面弥合。在这项工作中，我们研究了连续时间下的HB动量法，并更加关注离散化误差，以期为该领域提供额外的理论工具。具体来说，我们设计了一个一阶分段连续微分方程，其中我们添加了许多补偿项以明确说明离散化误差。因此，我们为HB动量法提供了一个连续时间模型，该模型允许将离散化误差控制到任意阶步长。作为一项应用，我们利用它来发现方向平滑度的一种新的隐式正则化，并研究了HB对对角线性网络的隐式偏差，这表明了我们的结果如何应用于深度学习。我们的理论发现得到了数值实验的进一步支持。", "summary": "本文提出了一种新颖的连续时间近似方法，用于离散重球（HB）动量法，并明确解决了由离散化误差引起的差距。它设计了一个带补偿项的一阶分段连续微分方程，以精确建模和控制该误差，使其达到任意阶步长。这项工作提供了新的理论工具，展示了其应用，例如揭示方向平滑度的隐式正则化以及研究深度学习背景下HB的隐式偏差，所有这些都得到了数值实验的支持。", "keywords": "重球动量法, 连续时间, 离散化误差, 隐式正则化, 深度学习", "comments": "本文的创新之处在于明确建模和控制重球动量法连续时间近似中的离散化误差，这对于弥合优化中离散和连续动力学之间的理论鸿沟至关重要。这项工作为理解和潜在改进基于动量的优化器提供了有价值的理论工具，尤其是在深度学习领域。"}}
{"id": "2506.15156", "title": "Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View", "authors": ["Muhammad Cendekia Airlangga", "Hilal AlQuabeh", "Munachiso S Nwadike", "Kentaro Inui"], "summary": "We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15156v1", "AI": {"title_translation": "Mamba中首因效应和近因效应的出现：一种机制性视角", "tldr": "本研究使用首因效应和近因效应作为行为工具，揭示了Mamba架构中信息是如何被记忆和遗忘的。研究发现Mamba表现出U形准确率曲线，并识别出三种机制：稀疏通道支持长时记忆（首因效应），delta调制循环控制短时记忆（近因效应），以及语义规律性动态调节记忆分配。", "motivation": "本研究旨在利用首因效应和近因效应作为行为工具，深入探究状态空间语言模型中信息随时间保留和遗忘的机制。", "method": "研究将结构化回忆任务应用于Mamba架构，并识别出导致U形准确率曲线的三种机制。通过对两个大规模Mamba语言模型（1.4B和7B参数）进行有针对性的消融实验和输入扰动，验证了这些发现。", "result": "研究观察到Mamba架构中存在一致的U形准确率曲线，表明在输入序列的开头和结尾表现出色。研究识别出三个产生这种模式的机制：1. 长期记忆由模型选择性状态空间块内稀疏的通道子集支持，这些通道持续编码早期输入标记并与首因效应因果相关。2. 短期记忆受delta调制循环控制：近期输入因指数衰减而获得更多权重，但当引入干扰项时，这种近因优势会崩溃，揭示了记忆深度的清晰限制。3. 记忆分配受语义规律性动态调节：输入序列中重复的关系会改变delta门控行为，增加对中间项的遗忘倾向。", "conclusion": "本研究通过对Mamba架构的深入分析，揭示了其内部信息记忆和遗忘的机制，特别是对首因效应和近因效应的产生以及记忆动态分配的解释。", "translation": "我们利用首因效应和近因效应作为行为工具，研究状态空间语言模型中的记忆，以揭示信息如何随时间被保留和遗忘。将结构化回忆任务应用于Mamba架构，我们观察到一致的U形准确率曲线，表明在输入序列的开头和结尾表现出色。我们识别出产生这种模式的三种机制。首先，长期记忆由模型选择性状态空间块内稀疏的通道子集支持，这些通道持续编码早期输入标记并与首因效应因果相关。其次，短期记忆受delta调制循环控制：近期输入因指数衰减而获得更多权重，但当引入干扰项时，这种近因优势会崩溃，揭示了记忆深度的清晰限制。第三，我们发现记忆分配受语义规律性动态调节：输入序列中重复的关系会改变delta门控行为，增加对中间项的遗忘倾向。我们通过对两个大规模基于Mamba的语言模型（一个1.4B参数，另一个7B参数）进行有针对性的消融实验和输入扰动来验证这些发现。", "summary": "本研究利用首因效应和近因效应作为行为工具，深入探讨了状态空间语言模型Mamba中的记忆机制。通过对Mamba架构应用结构化回忆任务，研究发现其表现出一致的U形准确率曲线，即对序列开头和结尾的信息记忆效果更佳。研究进一步识别并验证了三种核心机制：稀疏通道支持早期信息的长期记忆（首因效应）；delta调制循环控制近期信息的短期记忆（近因效应），但受干扰项影响其深度有限；以及语义规律性动态调节记忆分配，导致对中间项的遗忘。这些发现通过对大型Mamba模型的消融实验和输入扰动得到了验证。", "keywords": "Mamba, 首因效应, 近因效应, 状态空间模型, 记忆机制", "comments": "Not mentioned in abstract"}}
{"id": "2506.15185", "title": "Heterogeneous and anisotropic elastic parameter estimation using a novel semi-analytical forward solver", "authors": ["Xiaopeng Zhu", "Zhongyi Huang"], "summary": "An efficient procedure using a novel semi-analytical forward solver for\nidentifying heterogeneous and anisotropic elastic parameters from only one\nfull-field measurement is proposed and explored. We formulate the inverse\nproblem as an special energy functional minimization with total variation(TV)\nregularization. The minimization problem is solved by Adam algorithm, which\nonly requires solving one forward problem and no adjoint problem in each\niteration. In order to deal with the irregularity of the elastic regions, the\nanisotropy and heterogeneity of parameters and potential singularities in\nforward-modeled issues, a novel semi-analytical forward solver named the direct\nmethod of lines is proposed, which discretizes angular variable while\npreserving analytical solutions along remaining coordinates. To validate the\nefficacy of our procedure, a series of numerical experiments are implemented\nsubsequently, achieving reliable performance in both forward modeling and the\nsix elastic arguments reconstruction scenarios.", "comment": "26 pages,9 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15185v1", "AI": {"title_translation": "使用新型半解析正向求解器进行非均质各向异性弹性参数估计", "tldr": "提出了一种基于新型半解析正向求解器和Adam算法的有效方法，可从单次全场测量中识别非均质各向异性弹性参数，并在数值实验中表现良好。", "motivation": "旨在从仅一次全场测量中高效识别非均质和各向异性弹性参数，并解决弹性区域不规则性、参数的各向异性与非均质性以及正向建模中潜在奇点等问题。", "method": "提出了一种高效的程序，将逆问题表述为带有全变分(TV)正则化的特殊能量泛函最小化问题。该最小化问题通过Adam算法求解，每次迭代仅需解决一个正向问题，无需伴随问题。为处理复杂性，引入了一种名为“直接线法”的新型半解析正向求解器，该方法离散角度变量，同时保留其余坐标上的解析解。", "result": "通过一系列数值实验验证了该方法的有效性，在正向建模和六个弹性参数重建场景中均取得了可靠的性能。", "conclusion": "该程序能够从单次全场测量中有效且可靠地识别非均质和各向异性弹性参数，并能处理复杂问题。", "translation": "提出并探索了一种使用新型半解析正向求解器的高效程序，用于仅从一次全场测量中识别非均质和各向异性弹性参数。我们将逆问题表述为一个带有全变分（TV）正则化的特殊能量泛函最小化问题。该最小化问题通过Adam算法求解，每次迭代仅需解决一个正向问题，无需伴随问题。为了处理弹性区域的不规则性、参数的各向异性与非均质性以及正向建模中潜在的奇点，提出了一种名为“直接线法”的新型半解析正向求解器，该方法离散角度变量，同时保留其余坐标上的解析解。随后进行了一系列数值实验，以验证我们程序的有效性，在正向建模和六个弹性参数重建场景中均取得了可靠的性能。", "summary": "本文提出一种高效识别非均质各向异性弹性参数的方法，仅需一次全场测量。该方法将逆问题建模为带TV正则化的能量泛函最小化，并使用Adam算法求解，避免了伴随问题。为应对复杂性，引入了“直接线法”这一新型半解析正向求解器。数值实验验证了其在正向建模和弹性参数重建中的可靠性。", "keywords": "弹性参数估计, 非均质, 各向异性, 半解析求解器, 直接线法", "comments": "这项研究的创新之处在于提出了“直接线法”这一新型半解析正向求解器，它能有效处理弹性区域的不规则性、参数的各向异性与非均质性及潜在奇点。同时，采用Adam算法避免了伴随问题的计算，提高了求解效率。该方法仅需一次全场测量即可进行参数识别，具有实际应用潜力。"}}
{"id": "2506.15395", "title": "A Real-time Endoscopic Image Denoising System", "authors": ["Yu Xing", "Shishi Huang", "Meng Lv", "Guo Chen", "Huailiang Wang", "Lingzhi Sui"], "summary": "Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15395v1", "AI": {"title_translation": "实时内窥镜图像去噪系统", "tldr": "本文开发了一个针对医疗内窥镜模拟图像传感器的综合噪声模型，并提出了一种混合去噪系统，结合传统图像处理和学习算法，以实时在FPGA平台上有效降低图像噪声，同时保持细节和颜色。", "motivation": "微型一次性内窥镜的超小型模拟图像传感器由于光敏区域有限、动态范围受限以及电路设计简化和模拟信号传输，导致内窥镜图像存在显著的噪声问题，影响诊断能力，因此需要一个有效的去噪系统。", "method": "首先，开发了一个针对医疗内窥镜模拟图像传感器的综合噪声模型，解决了三种主要噪声类型：固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。在此分析基础上，提出了一个混合去噪系统，该系统协同结合了传统图像处理算法和先进的基于学习的技术，用于处理传感器捕获的原始帧。", "result": "实验证明，该方法在不损失精细细节或颜色失真的情况下有效降低了图像噪声，并在FPGA平台上实现了实时性能。在测试数据集上，平均PSNR从21.16提高到33.05。", "conclusion": "该研究成功开发了一个综合噪声模型并提出了一种混合去噪系统，能够有效解决微型内窥镜图像的噪声问题，实现实时高性能去噪，对医疗诊断具有重要意义。", "translation": "内窥镜的小型化设计显著增强了操作灵活性、便携性和诊断能力，同时大幅降低了医疗程序的侵入性。最近，配备超紧凑型模拟图像传感器（尺寸小于1毫米x 1毫米）的一次性内窥镜为医疗诊断带来了革命性进展。它们减少了与可重复使用设备相关的结构冗余和高昂的资本支出，消除了因消毒不足引起的患者感染风险，并减轻了患者痛苦。然而，有限的光敏区域导致每像素捕获的光子减少，需要更高的光子灵敏度设置以保持足够的亮度。在高对比度医疗成像场景中，小型传感器表现出受限的动态范围，难以同时捕获高光和阴影中的细节，并且需要额外的局部数字增益来补偿。此外，简化的电路设计和模拟信号传输引入了额外的噪声源。这些因素共同导致处理后的内窥镜图像存在显著的噪声问题。在这项工作中，我们为医疗内窥镜中的模拟图像传感器开发了一个综合噪声模型，解决了三种主要的噪声类型：固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。在此分析基础上，我们提出了一种混合去噪系统，该系统协同结合了传统图像处理算法和先进的基于学习的技术，用于捕获的传感器原始帧。实验表明，我们的方法在不损失精细细节或颜色失真的情况下有效降低了图像噪声，同时在FPGA平台上实现了实时性能，并在我们的测试数据集上将平均PSNR从21.16提高到33.05。", "summary": "本文针对微型一次性内窥镜中模拟图像传感器固有的高噪声问题，首先建立了一个包含固定模式噪声、周期性带状噪声和混合泊松-高斯噪声的综合噪声模型。在此基础上，提出了一种创新的混合去噪系统，该系统结合了传统图像处理算法和先进的深度学习技术，以处理传感器原始帧。实验结果表明，该系统能够有效去除图像噪声，同时保持图像细节和色彩完整性，并在FPGA平台上实现实时性能，将图像质量（PSNR）显著提升。", "keywords": "内窥镜图像去噪, 实时系统, 噪声模型, 混合去噪, FPGA", "comments": "该论文的创新点在于为医疗内窥镜中的模拟图像传感器建立了全面的噪声模型，并基于此提出了结合传统和学习方法的混合去噪系统。其重要性体现在解决了微型一次性内窥镜图像的关键噪声问题，实现了实时去噪并显著提升了图像质量，这对于提高医疗诊断的准确性和效率具有实际应用价值。在FPGA上实现实时性能是其一大亮点。"}}
{"id": "2506.15253", "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "summary": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.", "comment": "12 pages, 8 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15253v1", "AI": {"title_translation": "RAS-Eval：一个用于真实世界环境中LLM智能体安全评估的综合基准", "tldr": "RAS-Eval是一个新的安全基准，用于评估LLM智能体在真实世界环境中的安全性。研究发现LLM智能体存在严重漏洞，攻击可显著降低任务完成率，并证实安全能力存在扩展定律。", "motivation": "大型语言模型（LLM）智能体在医疗和金融等关键领域的快速部署，使得建立强大的安全框架成为必要。由于缺乏针对这些智能体在动态环境中进行标准化评估的基准，因此引入RAS-Eval来解决这一问题。", "method": "本研究引入了RAS-Eval，这是一个支持模拟和真实世界工具执行的综合安全基准。RAS-Eval包含80个测试用例和3802个攻击任务，这些任务映射到11个通用弱点枚举（CWE）类别。工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。研究评估了6个最先进的LLM在不同场景下的表现。", "result": "研究揭示了显著的漏洞：攻击平均使智能体任务完成率（TCR）降低了36.78%，并在学术环境中取得了85.65%的成功率。值得注意的是，安全能力存在扩展定律，即更大的模型表现优于较小的模型。", "conclusion": "本研究发现揭示了真实世界智能体部署中的关键风险，并为未来的安全研究提供了一个基础框架。", "translation": "大型语言模型（LLM）智能体在医疗和金融等关键领域的快速部署，使得建立强大的安全框架成为必要。为了解决在动态环境中缺乏针对这些智能体进行标准化评估基准的问题，我们引入了RAS-Eval，一个支持模拟和真实世界工具执行的综合安全基准。RAS-Eval包含80个测试用例和3802个攻击任务，这些任务映射到11个通用弱点枚举（CWE）类别，工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。我们评估了6个最先进的LLM在不同场景下的表现，揭示了显著的漏洞：攻击平均使智能体任务完成率（TCR）降低了36.78%，并在学术环境中取得了85.65%的成功率。值得注意的是，安全能力存在扩展定律，即更大的模型表现优于较小的模型。我们的研究结果揭示了真实世界智能体部署中的关键风险，并为未来的安全研究提供了一个基础框架。代码和数据可在https://github.com/lanzer-tree/RAS-Eval获取。", "summary": "该论文提出了RAS-Eval，一个全面的安全基准，旨在评估大型语言模型（LLM）智能体在真实世界环境中的安全性。该基准包含80个测试用例和3802个攻击任务，涵盖11个CWE类别。通过评估6个主流LLM，研究发现这些智能体存在严重漏洞，攻击显著降低了任务完成率，并揭示了安全能力中的扩展定律，即大型模型表现出更好的安全性。研究强调了真实世界智能体部署中的风险，并为未来的安全研究奠定了基础。", "keywords": "LLM智能体, 安全评估, RAS-Eval, 基准测试, 漏洞", "comments": "RAS-Eval的创新之处在于其提供了首个支持真实世界工具执行的LLM智能体安全评估基准，填补了该领域的空白。其重要性在于揭示了当前LLM智能体在实际应用中存在的严重安全漏洞，并证实了模型规模与安全能力之间的关系，为未来更安全的LLM智能体开发提供了关键洞察和评估框架。"}}
{"id": "2506.15146", "title": "TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality", "authors": ["Masaki Murooka", "Takahiro Hoshi", "Kensuke Fukumitsu", "Shimpei Masuda", "Marwan Hamze", "Tomoya Sasaki", "Mitsuharu Morisawa", "Eiichi Yoshida"], "summary": "Manipulation with whole-body contact by humanoid robots offers distinct\nadvantages, including enhanced stability and reduced load. On the other hand,\nwe need to address challenges such as the increased computational cost of\nmotion generation and the difficulty of measuring broad-area contact. We\ntherefore have developed a humanoid control system that allows a humanoid robot\nequipped with tactile sensors on its upper body to learn a policy for\nwhole-body manipulation through imitation learning based on human teleoperation\ndata. This policy, named tactile-modality extended ACT (TACT), has a feature to\ntake multiple sensor modalities as input, including joint position, vision, and\ntactile measurements. Furthermore, by integrating this policy with retargeting\nand locomotion control based on a biped model, we demonstrate that the\nlife-size humanoid robot RHP7 Kaleido is capable of achieving whole-body\ncontact manipulation while maintaining balance and walking. Through detailed\nexperimental verification, we show that inputting both vision and tactile\nmodalities into the policy contributes to improving the robustness of\nmanipulation involving broad and delicate contact.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15146v1", "AI": {"title_translation": "TACT：通过深度模仿学习与触觉模态实现人形机器人全身接触操作", "tldr": "本文开发了一种名为TACT的深度模仿学习策略，使人形机器人能够利用触觉和视觉信息进行全身接触操作，提高了操作的鲁棒性。", "motivation": "人形机器人全身接触操作具有增强稳定性和降低负载的优势，但面临运动生成计算成本高和难以测量大面积接触的挑战。因此，需要开发一种控制系统来解决这些问题。", "method": "研究开发了一种人形机器人控制系统，使配备触觉传感器的人形机器人能够通过基于人类遥操作数据的模仿学习来学习全身操作策略。该策略（TACT）将关节位置、视觉和触觉等多种传感器模态作为输入。此外，通过将该策略与基于双足模型的重定向和运动控制相结合，应用于RHP7 Kaleido人形机器人。", "result": "生命尺寸的人形机器人RHP7 Kaleido能够实现全身接触操作，同时保持平衡和行走。实验验证表明，将视觉和触觉模态同时输入策略有助于提高涉及大面积和精细接触操作的鲁棒性。", "conclusion": "将视觉和触觉模态同时输入到模仿学习策略中，可以显著提高人形机器人全身接触操作的鲁棒性，尤其是在处理大面积和精细接触时。", "translation": "人形机器人全身接触操作具有显著优势，包括增强稳定性和降低负载。另一方面，我们需要解决运动生成计算成本增加和难以测量大面积接触等挑战。因此，我们开发了一种人形机器人控制系统，该系统允许配备上身触觉传感器的人形机器人通过基于人类遥操作数据的模仿学习来学习全身操作策略。该策略名为触觉模态扩展ACT（TACT），其特点是能够将多种传感器模态作为输入，包括关节位置、视觉和触觉测量。此外，通过将该策略与基于双足模型的重定向和运动控制相结合，我们证明了生命尺寸的人形机器人RHP7 Kaleido能够实现全身接触操作，同时保持平衡和行走。通过详细的实验验证，我们表明将视觉和触觉模态同时输入策略有助于提高涉及大面积和精细接触操作的鲁棒性。", "summary": "本文提出了一种名为TACT的深度模仿学习策略，用于人形机器人的全身接触操作。该策略利用人类遥操作数据，并整合了关节位置、视觉和触觉等多种传感器输入。通过将TACT策略与重定向和运动控制相结合，成功使RHP7 Kaleido人形机器人实现了在保持平衡和行走的同时进行全身接触操作。实验结果表明，同时利用视觉和触觉信息能显著提高机器人处理大面积和精细接触操作的鲁棒性。", "keywords": "人形机器人, 全身接触, 模仿学习, 触觉模态, 机器人操作", "comments": "本文的创新之处在于将触觉模态深度整合到基于模仿学习的人形机器人全身接触操作中，这对于提高机器人与环境交互的鲁棒性至关重要。通过结合多种传感器模态，特别是触觉，解决了传统方法难以处理大面积和精细接触的挑战，为未来更灵活、更稳定的机器人操作提供了新的方向。"}}
{"id": "2506.15325", "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook", "authors": ["Festus Adedoyin", "Huseyin Dogan"], "summary": "Advancements in Artificial Intelligence (AI) have significantly transformed\nthe financial industry, enabling the development of more personalised and\nadaptable financial products and services. This research paper explores various\ninstances where Human-Centred AI (HCAI) has facilitated these advancements,\ndrawing from contemporary studies and industry progress. The paper examines how\nthe application of HCAI-powered data analytics, machine learning, and natural\nlanguage processing enables financial institutions to gain a deeper\nunderstanding of their customers' unique needs, preferences, and behavioural\npatterns. This, in turn, allows for the creation of tailored financial\nsolutions that address individual consumer requirements, ultimately enhancing\noverall user experience and satisfaction. Additionally, the study highlights\nthe integration of AI-powered robo-advisory services, which offer customised\ninvestment recommendations and portfolio management tailored to diverse risk\nprofiles and investment goals. Moreover, the paper underscores the role of AI\nin strengthening fraud detection, risk assessment, and regulatory compliance,\nleading to a more secure and adaptable financial landscape. The findings of\nthis research demonstrate the substantial impact of Human-Centred AI on the\nfinancial industry, offering a strategic framework for financial institutions\nto leverage these technologies. By incorporating a User Experience Research\n(UXR) Point of View (PoV), financial institutions can ensure that AI-driven\nsolutions align with user needs and business objectives.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15325v1", "AI": {"title_translation": "金融科技中的以人为本人工智能：开发用户体验（UX）研究观点（PoV）手册", "tldr": "本研究探讨了以人为本人工智能（HCAI）在金融科技中的应用及其对提升用户体验和金融机构运营效率的重大影响，并提出整合用户体验研究（UXR）视角的策略框架。", "motivation": "随着人工智能（AI）在金融行业的发展，出现了更个性化和适应性强的金融产品和服务。本研究的动机是探索以人为本人工智能（HCAI）如何促成这些进步，并为金融机构提供一个战略框架，以利用这些技术，同时确保AI驱动的解决方案与用户需求和业务目标保持一致。", "method": "本研究通过探索以人为本人工智能（HCAI）在金融行业的各种应用实例，借鉴当代研究和行业进展。具体方法包括：考察HCAI驱动的数据分析、机器学习和自然语言处理如何帮助金融机构理解客户需求；强调AI驱动的机器人顾问服务；以及阐述AI在加强欺诈检测、风险评估和合规性方面的作用。最终目标是开发一个用户体验研究（UXR）的观点（PoV）手册。", "result": "研究结果表明，以人为本人工智能（HCAI）对金融行业产生了重大影响，它通过提供个性化和适应性强的金融产品与服务，增强了用户体验和满意度。具体体现在：深化对客户需求和行为模式的理解；提供定制化投资建议和投资组合管理；以及加强欺诈检测、风险评估和监管合规性。研究还提供了一个战略框架，供金融机构利用这些技术。", "conclusion": "结论是，通过整合用户体验研究（UXR）的观点（PoV），金融机构可以确保其AI驱动的解决方案与用户需求和业务目标保持一致，从而在金融行业中更有效地利用以人为本的人工智能。", "translation": "人工智能（AI）的进步已显著改变了金融行业，使得开发更个性化和适应性强的金融产品和服务成为可能。本研究论文探讨了以人为本人工智能（HCAI）在促进这些进步方面的各种实例，借鉴了当代研究和行业进展。论文考察了HCAI驱动的数据分析、机器学习和自然语言处理的应用如何使金融机构更深入地了解客户的独特需求、偏好和行为模式。这反过来又使得能够创建满足个体消费者需求的定制化金融解决方案，最终提升整体用户体验和满意度。此外，本研究强调了AI驱动的机器人顾问服务的整合，这些服务根据不同的风险偏好和投资目标提供定制化的投资建议和投资组合管理。此外，论文强调了AI在加强欺诈检测、风险评估和监管合规性方面的作用，从而形成一个更安全、适应性更强的金融格局。本研究的发现表明，以人为本人工智能对金融行业产生了实质性影响，为金融机构利用这些技术提供了一个战略框架。通过融入用户体验研究（UXR）的观点（PoV），金融机构可以确保AI驱动的解决方案与用户需求和业务目标保持一致。", "summary": "本研究探讨了以人为本人工智能（HCAI）在金融科技领域的应用及其对行业转型的深远影响。论文分析了HCAI如何通过数据分析、机器学习和自然语言处理帮助金融机构理解并满足客户的个性化需求，从而提升用户体验。研究还强调了AI在机器人顾问服务、欺诈检测、风险评估和合规性方面的应用。最终，本研究提供了一个战略框架，旨在指导金融机构在开发AI解决方案时融入用户体验研究（UXR）的视角，以确保技术与用户需求和业务目标高度契合。", "keywords": "以人为本人工智能, 金融科技, 用户体验研究, 战略框架, 机器人顾问", "comments": "这篇论文的创新之处在于其明确提出了“以人为本人工智能”在金融科技领域的应用，并强调了将用户体验研究（UXR）的观点整合到AI解决方案开发中的重要性，旨在弥补技术进步与用户实际需求之间的潜在鸿沟。其重要性体现在为金融机构提供了一个实用的战略框架或“手册”，指导他们在利用AI技术时，能够更好地关注用户需求、提升用户满意度，并确保业务目标与技术应用的一致性。这对于推动金融科技健康发展，避免AI技术盲目扩张而忽视用户体验具有指导意义。"}}
{"id": "2506.15264", "title": "Centroid Approximation for Byzantine-Tolerant Federated Learning", "authors": ["Mélanie Cambus", "Darya Melnyk", "Tijana Milentijević", "Stefan Schmid"], "summary": "Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.", "comment": "19 pages, 10 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15264v1", "AI": {"title_translation": "拜占庭容错联邦学习的质心近似", "tldr": "本文研究了联邦学习在拜占庭行为下的鲁棒性，首次提出了质心近似的下界，并给出了新的上界和算法，通过理论分析和实验验证，提升了联邦学习的安全性。", "motivation": "研究联邦学习在分布式环境下训练机器学习模型时，如何抵御拜占庭行为的攻击，特别是探讨平均/质心聚合规则与分布式计算中有效性条件之间的权衡，并解决现有方法在近似质量和实验效果上的不足。", "method": "本文通过理论分析，首次推导了在“盒子有效性”（box validity）条件下质心近似的第一个下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$，并补充了上界 $2\\min\\{n,\\sqrt{d}\\}$。此外，提出了一种在“凸有效性”（convex validity）条件下实现 $\\sqrt{2d}$-近似的新算法。这些分析结果通过在联邦随机梯度下降和联邦平均设置中的经验评估进行验证。", "result": "1. 各种有效性条件本身并不能保证平均值的良好近似；2. 即使达到良好近似，由于拜占庭异常值，在实验设置中也无法获得良好结果；3. 首次提出了在盒子有效性下质心近似的下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$；4. 补充了在 $n<d$ 情况下上界 $2\\min\\{n,\\sqrt{d}\\}$；5. 提出了一种在凸有效性下实现 $\\sqrt{2d}$-近似的新算法，并证明了现有文献中下界的紧密性；6. 所有提出的界限都可以在分布式点对点设置中实现；7. 经验评估证实了分析结果。", "conclusion": "本文通过建立质心近似的理论界限和提出新的算法，显著提升了联邦学习在存在拜占庭故障情况下的鲁棒性，为分布式机器学习的安全性提供了重要保障。", "translation": "联邦学习允许每个客户端在分布式环境中训练机器学习模型时，将其数据保留在本地。最近的大量研究确定了输入必须满足的要求，以保证训练循环的收敛性。这项工作使用平均作为训练模型的聚合规则。特别是，我们关注联邦学习是否对拜占庭行为具有鲁棒性，并观察和研究了平均/质心与分布式计算中的有效性条件之间的权衡。我们表明，各种有效性条件本身并不能保证平均值的良好近似。此外，我们表明，由于可能的拜占庭异常值，即使达到良好近似，在实验设置中也无法获得良好结果。我们的主要贡献是首次提出了在文献中经常考虑的盒子有效性下，质心近似的第一个下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$，其中 $n$ 是客户端数量，$t$ 是拜占庭故障数量的上限，$d$ 是机器学习模型的维度。我们通过对 $n<d$ 情况提供新的分析，补充了 $2\\min\\{n,\\sqrt{d}\\}$ 的上界。此外，我们提出了一种在凸有效性下实现 $\\sqrt{2d}$-近似的新算法，这也证明了文献中现有的下界是紧密的。我们表明，所有提出的界限都可以在分布式点对点设置中实现。我们通过在联邦随机梯度下降和联邦平均设置中的经验评估来补充我们的分析结果。", "summary": "本文研究了联邦学习在拜占庭行为下的鲁棒性，特别关注模型聚合中质心近似的质量。研究发现，单独的常见有效性条件不足以保证良好的近似，且即使理论近似良好，在实际中也可能因拜占庭异常值而表现不佳。主要贡献包括首次推导了盒子有效性下质心近似的下界和相应的上界，并提出了一种在凸有效性下实现特定近似的新算法，证明了现有理论下界的紧密性。这些理论发现通过在标准联邦学习设置中的实证评估得到了验证，展示了其在分布式点对点环境中的实际意义。", "keywords": "联邦学习, 拜占庭故障, 质心近似, 鲁棒性, 分布式学习", "comments": "本文在拜占庭容错联邦学习领域做出了重要的理论贡献，为质心近似提供了严格的下界和上界，这对于理解鲁棒性的基本限制至关重要。新算法的提出及其经验验证进一步增强了其在实际中的适用性，解决了分布式机器学习中的一个关键挑战。"}}
{"id": "2506.14856", "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction", "authors": ["Zhengquan Zhang", "Feng Xu", "Mengmi Zhang"], "summary": "Some perspectives naturally provide more information than others. How can an\nAI system determine which viewpoint offers the most valuable insight for\naccurate and efficient 3D object reconstruction? Active view selection (AVS)\nfor 3D reconstruction remains a fundamental challenge in computer vision. The\naim is to identify the minimal set of views that yields the most accurate 3D\nreconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian\nSplatting, from a current observation and computing uncertainty for each\ncandidate viewpoint, we introduce a novel AVS approach guided by neural\nuncertainty maps predicted by a lightweight feedforward deep neural network,\nnamed UPNet. UPNet takes a single input image of a 3D object and outputs a\npredicted uncertainty map, representing uncertainty values across all possible\ncandidate viewpoints. By leveraging heuristics derived from observing many\nnatural objects and their associated uncertainty patterns, we train UPNet to\nlearn a direct mapping from viewpoint appearance to uncertainty in the\nunderlying volumetric representations. Next, our approach aggregates all\npreviously predicted neural uncertainty maps to suppress redundant candidate\nviewpoints and effectively select the most informative one. Using these\nselected viewpoints, we train 3D neural rendering models and evaluate the\nquality of novel view synthesis against other competitive AVS methods.\nRemarkably, despite using half of the viewpoints than the upper bound, our\nmethod achieves comparable reconstruction accuracy. In addition, it\nsignificantly reduces computational overhead during AVS, achieving up to a 400\ntimes speedup along with over 50\\% reductions in CPU, RAM, and GPU usage\ncompared to baseline methods. Notably, our approach generalizes effectively to\nAVS tasks involving novel object categories, without requiring any additional\ntraining.", "comment": "9 pages, 3 figures in the main text. Under review for NeurIPS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14856v1", "AI": {"title_translation": "窥探未知：基于神经不确定性图的主动视角选择用于三维重建", "tldr": "本文提出了一种名为UPNet的新型主动视角选择（AVS）方法，通过预测神经不确定性图来高效选择用于3D重建的最优视角，显著提高了重建效率并降低了计算成本。", "motivation": "3D重建中的主动视角选择（AVS）是一个基本挑战，目标是识别最少量的视角以获得最准确的3D重建。现有的方法通常需要从当前观察中学习辐射场并计算每个候选视角的不确定性，效率较低。", "method": "我们引入了一种由轻量级前馈深度神经网络UPNet预测的神经不确定性图引导的新型AVS方法。UPNet以3D对象的单个输入图像为输入，输出预测的不确定性图，表示所有可能候选视角的不确定性值。通过利用从观察许多自然物体及其相关不确定性模式中得出的启发式方法，我们训练UPNet学习从视角外观到底层体积表示中不确定性的直接映射。然后，我们的方法聚合所有先前预测的神经不确定性图，以抑制冗余候选视角并有效选择信息量最大的视角。", "result": "尽管使用的视角数量是上限的一半，但我们的方法实现了可比的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，实现了高达400倍的速度提升，并减少了超过50%的CPU、RAM和GPU使用量。值得注意的是，我们的方法可以有效地推广到涉及新颖对象类别的AVS任务，而无需任何额外的训练。", "conclusion": "本文提出的基于神经不确定性图的主动视角选择方法，在显著降低计算成本和视角数量的同时，实现了与现有方法相当的3D重建精度，并展现出良好的泛化能力。", "translation": "有些视角自然比其他视角提供更多信息。AI系统如何确定哪个视角能为准确高效的3D对象重建提供最有价值的洞察？用于3D重建的主动视角选择（AVS）仍然是计算机视觉中的一个基本挑战。其目标是识别最少量的视角，以获得最准确的3D重建。我们没有像NeRF或3D高斯散射那样从当前观察中学习辐射场并计算每个候选视角的不确定性，而是引入了一种由轻量级前馈深度神经网络UPNet预测的神经不确定性图引导的新型AVS方法。UPNet接收3D对象的单个输入图像，并输出预测的不确定性图，表示所有可能候选视角的不确定性值。通过利用从观察许多自然物体及其相关不确定性模式中得出的启发式方法，我们训练UPNet学习从视角外观到底层体积表示中不确定性的直接映射。接下来，我们的方法聚合所有先前预测的神经不确定性图，以抑制冗余候选视角并有效选择信息量最大的视角。利用这些选定的视角，我们训练3D神经渲染模型，并评估新颖视角合成的质量，与其它有竞争力的AVS方法进行比较。值得注意的是，尽管使用的视角数量是上限的一半，但我们的方法实现了可比的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，实现了高达400倍的速度提升，并减少了超过50%的CPU、RAM和GPU使用量。值得注意的是，我们的方法可以有效地推广到涉及新颖对象类别的AVS任务，而无需任何额外的训练。", "summary": "本文提出了一种名为UPNet的创新主动视角选择（AVS）方法，用于高效的3D重建。与传统方法不同，UPNet通过轻量级前馈神经网络预测神经不确定性图，直接从单个输入图像映射视角外观到不确定性。该方法聚合不确定性图以选择信息量最大的视角，显著减少了所需的视角数量和计算资源（高达400倍的速度提升和50%的资源减少），同时保持了可比的重建精度，并对新颖物体类别具有良好的泛化能力。", "keywords": "主动视角选择, 3D重建, 神经不确定性图, UPNet, 计算效率", "comments": "本文提出了一种新颖的主动视角选择（AVS）方法，其创新点在于引入了轻量级的UPNet来直接预测神经不确定性图，避免了传统方法中耗时的辐射场学习和不确定性计算。这种方法极大地提高了AVS的效率，显著降低了计算成本，这对于实际应用具有重要意义。同时，其在减少视角数量的同时保持重建精度，并展现出对新颖物体类别的良好泛化能力，也进一步证明了其方法的有效性和鲁棒性。"}}
{"id": "2506.15376", "title": "Comparison of Innovative Strategies for the Coverage Problem: Path Planning, Search Optimization, and Applications in Underwater Robotics", "authors": ["Ahmed Ibrahim", "Francisco F. C. Rego", "Éric Busvelle"], "summary": "In many applications, including underwater robotics, the coverage problem\nrequires an autonomous vehicle to systematically explore a defined area while\nminimizing redundancy and avoiding obstacles. This paper investigates coverage\npath planning strategies to enhance the efficiency of underwater gliders,\nparticularly in maximizing the probability of detecting a radioactive source\nwhile ensuring safe navigation.\n  We evaluate three path-planning approaches: the Traveling Salesman Problem\n(TSP), Minimum Spanning Tree (MST), and Optimal Control Problem (OCP).\nSimulations were conducted in MATLAB, comparing processing time, uncovered\nareas, path length, and traversal time. Results indicate that OCP is preferable\nwhen traversal time is constrained, although it incurs significantly higher\ncomputational costs. Conversely, MST-based approaches provide faster but less\noptimal solutions. These findings offer insights into selecting appropriate\nalgorithms based on mission priorities, balancing efficiency and computational\nfeasibility.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15376v1", "AI": {"title_translation": "覆盖问题创新策略的比较：路径规划、搜索优化及在水下机器人中的应用", "tldr": "比较了三种水下机器人覆盖路径规划策略（TSP, MST, OCP），发现OCP在时间受限时最优但计算成本高，MST更快但不那么优，为任务选择算法提供了依据。", "motivation": "解决水下机器人覆盖问题，提高水下滑翔机探测放射源的效率，同时确保安全导航。", "method": "评估了三种路径规划方法：旅行商问题（TSP）、最小生成树（MST）和最优控制问题（OCP）。在MATLAB中进行仿真，比较了处理时间、未覆盖区域、路径长度和遍历时间。", "result": "最优控制问题（OCP）在遍历时间受限时表现更优，但计算成本显著更高。基于最小生成树（MST）的方法速度更快但最优性较差。", "conclusion": "研究结果为根据任务优先级选择合适的算法提供了见解，以平衡效率和计算可行性。", "translation": "在包括水下机器人在内的许多应用中，覆盖问题要求自主车辆系统地探索定义的区域，同时最大限度地减少冗余并避开障碍物。本文研究了覆盖路径规划策略，以提高水下滑翔机的效率，特别是在确保安全导航的同时最大化探测放射源的概率。我们评估了三种路径规划方法：旅行商问题（TSP）、最小生成树（MST）和最优控制问题（OCP）。在MATLAB中进行了仿真，比较了处理时间、未覆盖区域、路径长度和遍历时间。结果表明，当遍历时间受限时，OCP是更优的选择，尽管它会产生显著更高的计算成本。相反，基于MST的方法速度更快但最优性较差。这些发现为根据任务优先级选择合适的算法提供了见解，平衡了效率和计算可行性。", "summary": "本文比较了三种用于水下机器人覆盖路径规划的策略：旅行商问题（TSP）、最小生成树（MST）和最优控制问题（OCP）。通过MATLAB仿真，评估了这些方法在处理时间、未覆盖区域、路径长度和遍历时间方面的表现。研究发现，OCP在遍历时间受限时性能最佳但计算成本高昂，而MST方法速度快但次优。这些发现为根据具体任务需求选择合适的覆盖算法提供了实用指导，旨在平衡效率与计算资源。", "keywords": "覆盖问题, 路径规划, 水下机器人, TSP, MST, OCP", "comments": "这项研究通过比较不同的路径规划算法，为水下机器人等自主车辆的覆盖问题提供了实用的解决方案。其创新之处在于将经典的优化问题应用于特定场景，并量化了不同策略的性能权衡，对于实际应用具有指导意义。"}}
{"id": "2309.13018", "title": "Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model", "authors": ["Jiamin Xie", "Ke Li", "Jinxi Guo", "Andros Tjandra", "Yuan Shangguan", "Leda Sari", "Chunyang Wu", "Junteng Jia", "Jay Mahadeokar", "Ozlem Kalinli"], "summary": "Neural network pruning offers an effective method for compressing a\nmultilingual automatic speech recognition (ASR) model with minimal performance\nloss. However, it entails several rounds of pruning and re-training needed to\nbe run for each language. In this work, we propose the use of an adaptive\nmasking approach in two scenarios for pruning a multilingual ASR model\nefficiently, each resulting in sparse monolingual models or a sparse\nmultilingual model (named as Dynamic ASR Pathways). Our approach dynamically\nadapts the sub-network, avoiding premature decisions about a fixed sub-network\nstructure. We show that our approach outperforms existing pruning methods when\ntargeting sparse monolingual models. Further, we illustrate that Dynamic ASR\nPathways jointly discovers and trains better sub-networks (pathways) of a\nsingle multilingual model by adapting from different sub-network\ninitializations, thereby reducing the need for language-specific pruning.", "comment": null, "cate": "eess.AS", "url": "http://arxiv.org/abs/2309.13018v2", "AI": {"title_translation": "动态ASR路径：一种自适应掩蔽方法，用于多语言ASR模型的有效剪枝", "tldr": "本文提出了一种名为“动态ASR路径”的自适应掩蔽方法，以高效剪枝多语言ASR模型，减少了对特定语言剪枝的需求，并优于现有方法。", "motivation": "现有的神经网络剪枝方法需要对每种语言进行多轮剪枝和重新训练，这对于多语言自动语音识别（ASR）模型来说效率低下。", "method": "本文提出了一种自适应掩蔽方法，用于两种场景下高效剪枝多语言ASR模型，分别得到稀疏的单语言模型或稀疏的多语言模型（称为动态ASR路径）。该方法动态调整子网络，避免了对固定子网络结构的过早决策。", "result": "结果表明，该方法在针对稀疏单语言模型时优于现有剪枝方法。此外，动态ASR路径通过从不同的子网络初始化进行适应，共同发现并训练出单个多语言模型中更好的子网络（路径）。", "conclusion": "本文提出的动态ASR路径方法能够高效地剪枝多语言ASR模型，并通过动态适应子网络，减少了对特定语言剪枝的需求，从而提高了多语言ASR模型压缩的效率和性能。", "translation": "神经网络剪枝为压缩多语言自动语音识别（ASR）模型提供了一种有效方法，且性能损失极小。然而，它需要对每种语言进行多轮剪枝和重新训练。在这项工作中，我们提出在两种场景下使用自适应掩蔽方法，以高效剪枝多语言ASR模型，每种场景都产生稀疏的单语言模型或稀疏的多语言模型（命名为动态ASR路径）。我们的方法动态调整子网络，避免了对固定子网络结构的过早决策。我们表明，在目标是稀疏单语言模型时，我们的方法优于现有剪枝方法。此外，我们说明了动态ASR路径通过从不同的子网络初始化进行适应，共同发现并训练出单个多语言模型中更好的子网络（路径），从而减少了对特定语言剪枝的需求。", "summary": "本文提出了一种名为“动态ASR路径”的自适应掩蔽方法，旨在高效剪枝多语言自动语音识别（ASR）模型。该方法通过动态调整子网络，避免了固定子网络结构的限制，并能在两种场景下生成稀疏的单语言或多语言模型。实验结果表明，该方法在生成稀疏单语言模型时优于现有剪枝技术，并且能够共同发现和训练更好的多语言模型子网络，显著减少了对特定语言进行剪枝的需求，提升了多语言ASR模型压缩的效率。", "keywords": "多语言ASR, 模型剪枝, 自适应掩蔽, 动态ASR路径, 神经网络压缩", "comments": "该论文的创新点在于提出了“动态ASR路径”这一概念，通过自适应掩蔽方法实现了对多语言ASR模型的动态子网络调整。这有效地解决了传统剪枝方法需要为每种语言进行多轮训练的效率问题，对于大规模多语言模型的部署具有重要意义。"}}
{"id": "2506.14808", "title": "PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models", "authors": ["Jenny Schmalfuss", "Nadine Chang", "Vibashan VS", "Maying Shen", "Andres Bruhn", "Jose M. Alvarez"], "summary": "Vision language models (VLMs) respond to user-crafted text prompts and visual\ninputs, and are applied to numerous real-world problems. VLMs integrate visual\nmodalities with large language models (LLMs), which are well known to be\nprompt-sensitive. Hence, it is crucial to determine whether VLMs inherit this\ninstability to varying prompts. We therefore investigate which prompt\nvariations VLMs are most sensitive to and which VLMs are most agnostic to\nprompt variations. To this end, we introduce PARC (Prompt Analysis via\nReliability and Calibration), a VLM prompt sensitivity analysis framework built\non three pillars: (1) plausible prompt variations in both the language and\nvision domain, (2) a novel model reliability score with built-in guarantees,\nand (3) a calibration step that enables dataset- and prompt-spanning prompt\nvariation analysis. Regarding prompt variations, PARC's evaluation shows that\nVLMs mirror LLM language prompt sensitivity in the vision domain, and most\ndestructive variations change the expected answer. Regarding models,\noutstandingly robust VLMs among 22 evaluated models come from the InternVL2\nfamily. We further find indications that prompt sensitivity is linked to\ntraining data. The code will be at https://github.com/NVlabs/PARC.", "comment": "Accepted to CVPR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14808v1", "AI": {"title_translation": "PARC：一个揭示视觉语言模型内部对称性的定量框架", "tldr": "PARC是一个新的框架，用于分析视觉语言模型（VLMs）对提示变化的敏感性。研究发现VLMs继承了LLM的提示敏感性，并且InternVL2家族的模型表现出卓越的鲁棒性。", "motivation": "大型语言模型（LLMs）对提示敏感，因此确定视觉语言模型（VLMs）是否也继承了这种对不同提示的不稳定性至关重要。本研究旨在探究VLMs对哪些提示变化最敏感，以及哪些VLMs对提示变化最不敏感。", "method": "本研究引入了PARC（通过可靠性和校准进行提示分析）框架，这是一个基于三个支柱的VLM提示敏感性分析框架：1) 语言和视觉领域中合理的提示变体；2) 具有内置保证的新颖模型可靠性得分；3) 实现数据集和提示跨度提示变体分析的校准步骤。研究评估了22个VLM模型。", "result": "PARC的评估显示，VLMs在视觉领域反映了LLM的语言提示敏感性，并且最具破坏性的变体改变了预期答案。在22个评估模型中，InternVL2家族的模型表现出卓越的鲁棒性。研究还发现提示敏感性与训练数据相关联。", "conclusion": "视觉语言模型（VLMs）确实继承了大型语言模型（LLMs）的提示敏感性，并且这种敏感性在视觉领域也有所体现。InternVL2家族的模型在面对提示变化时表现出较强的鲁棒性，提示敏感性可能与训练数据有关。", "translation": "视觉语言模型（VLMs）响应用户制作的文本提示和视觉输入，并应用于众多现实世界问题。VLMs将视觉模态与大型语言模型（LLMs）集成，而LLMs众所周知对提示敏感。因此，确定VLMs是否继承了这种对不同提示的不稳定性至关重要。我们因此调查了VLMs对哪些提示变化最敏感，以及哪些VLMs对提示变化最不敏感。为此，我们引入了PARC（通过可靠性和校准进行提示分析），这是一个基于三个支柱的VLM提示敏感性分析框架：(1) 语言和视觉领域中合理的提示变体，(2) 具有内置保证的新颖模型可靠性得分，以及(3) 实现数据集和提示跨度提示变体分析的校准步骤。关于提示变体，PARC的评估显示VLMs在视觉领域反映了LLM的语言提示敏感性，并且最具破坏性的变体改变了预期答案。关于模型，在22个评估模型中，InternVL2家族的模型表现出卓越的鲁棒性。我们进一步发现提示敏感性与训练数据相关联。代码将在https://github.com/NVlabs/PARC提供。", "summary": "本研究提出了PARC框架，一个用于定量分析视觉语言模型（VLMs）对提示变化敏感性的工具。该框架通过考虑语言和视觉领域的提示变体、引入模型可靠性得分以及进行校准，揭示了VLMs继承了大型语言模型（LLMs）的提示敏感性，尤其是在视觉领域。研究发现InternVL2家族的VLMs在22个评估模型中表现出最佳的鲁棒性，并指出提示敏感性可能与训练数据相关。", "keywords": "视觉语言模型, 提示敏感性, PARC框架, 模型鲁棒性, 训练数据", "comments": "这篇论文提出了一个新颖且重要的框架PARC，用于系统性地评估视觉语言模型（VLMs）的提示敏感性，填补了该领域的一个空白。其创新之处在于结合了语言和视觉领域的提示变体分析，并引入了可靠性得分和校准步骤，使得评估更加全面和严谨。研究结果对于VLM的开发和应用具有重要指导意义，特别是指出了特定模型家族的鲁棒性，并暗示了训练数据在缓解提示敏感性方面的重要性，这为未来的研究指明了方向。"}}
{"id": "2506.15208", "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals", "authors": ["Andrea Cadeddu", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Enrico Motta", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).", "comment": "Submitted to IEEE Access", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15208v1", "AI": {"title_translation": "大型语言模型在识别可持续发展目标中任务适应技术比较研究", "tldr": "本研究比较了大型语言模型（LLMs）及其任务适应技术（零样本、少样本学习和微调）在识别可持续发展目标（SDGs）文本分类任务上的表现，发现小模型通过提示工程可与大模型媲美。", "motivation": "追踪可持续发展目标（SDGs）的进展因数据规模庞大和复杂而困难。文本分类模型，特别是大型语言模型（LLMs），在自动化分析文本方面至关重要。本研究旨在评估LLMs及其适应技术在SDG识别任务中的有效性。", "method": "研究分析了多种专有和开源大型语言模型，用于单标签、多类别的SDG文本分类任务。评估了任务适应技术，包括上下文学习方法（零样本学习和少样本学习）以及微调。", "result": "结果显示，经过提示工程优化的小型模型可以达到与OpenAI的GPT等大型模型相当的性能。", "conclusion": "通过提示工程优化后的小型模型在可持续发展目标识别任务中可以达到与大型模型相当的性能。", "translation": "2012年，联合国引入了17项可持续发展目标（SDGs），旨在到2030年创建一个更可持续、更美好的未来。然而，由于涉及数据的广泛规模和复杂性，追踪这些目标的进展十分困难。文本分类模型已成为该领域的关键工具，能够自动化分析来自各种来源的大量文本。此外，大型语言模型（LLMs）最近在许多自然语言处理任务（包括文本分类）中被证明是不可或缺的，这得益于它们识别复杂语言模式和语义的能力。本研究分析了各种专有和开源LLMs，用于一项侧重于SDGs的单标签、多类别文本分类任务。然后，它还评估了在该领域中任务适应技术（即上下文学习方法），即零样本学习和少样本学习，以及微调的有效性。结果表明，通过提示工程优化后的小型模型可以与OpenAI的GPT（生成式预训练Transformer）等大型模型表现相当。", "summary": "本研究比较了多种专有和开源大型语言模型（LLMs）及其任务适应技术（包括零样本学习、少样本学习和微调）在识别联合国可持续发展目标（SDGs）的单标签、多类别文本分类任务中的表现。研究发现，通过有效的提示工程优化，较小的模型也能达到与OpenAI GPT等大型模型相当的性能，这为高效、准确地追踪SDG进展提供了新的视角。", "keywords": "大型语言模型, 可持续发展目标, 文本分类, 任务适应, 提示工程", "comments": "本文的创新点在于比较了不同规模LLMs和多种任务适应技术在特定领域（SDGs识别）的应用效果。其重要性在于揭示了小型模型在适当优化（如提示工程）下也能与大型模型媲美，这对于资源受限或需要部署轻量级模型的场景具有实际指导意义，有助于推动SDG追踪的自动化和效率。"}}
{"id": "2506.15203", "title": "Reduced Particle in Cell method for the Vlasov-Poisson system using auto-encoder and Hamiltonian neural", "authors": ["Emmanuel Franck", "Laurent Navoret", "Vincent Vigon", "Raphaël Côte", "Guillaume Steimer"], "summary": "Hamiltonian particle-based simulations of plasma dynamics are inherently\ncomputationally intensive, primarily due to the large number of particles\nrequired to obtain accurate solutions. This challenge becomes even more acute\nin many-query contexts, where numerous simulations must be conducted across a\nrange of time and parameter values. Consequently, it is essential to construct\nreduced order models from such discretizations to significantly lower\ncomputational costs while ensuring validity across the specified time and\nparameter domains. Preserving the Hamiltonian structure in these reduced models\nis also crucial, as it helps maintain long-term stability. In this paper, we\nintroduce a nonlinear, non-intrusive, data-driven model order reduction method\nfor the 1D-1V Vlasov--Poisson system, discretized using a Hamiltonian\nParticle-In-Cell scheme. Our approach relies on a two-step projection\nframework: an initial linear projection based on the Proper Symplectic\nDecomposition, followed by a nonlinear projection learned via an autoencoder\nneural network. The reduced dynamics are then modeled using a Hamiltonian\nneural network. The offline phase of the method is split into two stages:\nfirst, constructing the linear projection using full-order model snapshots;\nsecond, jointly training the autoencoder and the Hamiltonian neural network to\nsimultaneously learn the encoder-decoder mappings and the reduced dynamics. We\nvalidate the proposed method on several benchmarks, including Landau damping\nand two-stream instability. The results show that our method has better\nreduction properties than standard linear Hamiltonian reduction methods.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15203v1", "AI": {"title_translation": "使用自编码器和哈密顿神经网络的Vlasov-Poisson系统降维粒子在单元格方法", "tldr": "本文提出了一种结合线性投影（基于适当辛分解）和非线性投影（通过自编码器学习）的两步投影框架，并使用哈密顿神经网络建模降维动力学，以降低Vlasov-Poisson系统粒子模拟的计算成本，同时保持哈密顿结构。", "motivation": "哈密顿粒子等离子体动力学模拟计算成本高昂，尤其是在多查询场景下。因此，需要构建降维模型以显著降低计算成本，同时确保在时间和参数域内的有效性并保持哈密顿结构以维持长期稳定性。", "method": "本文提出了一种针对1D-1V Vlasov-Poisson系统的非线性、非侵入式、数据驱动的模型降维方法。该方法采用两步投影框架：首先是基于适当辛分解(Proper Symplectic Decomposition)的线性投影，然后是通过自编码器神经网络学习的非线性投影。降维后的动力学通过哈密顿神经网络建模。离线阶段分为两步：首先使用全阶模型快照构建线性投影；其次，联合训练自编码器和哈密顿神经网络，同时学习编码器-解码器映射和降维动力学。", "result": "该方法在Landau阻尼和双流不稳定性等多个基准测试中得到了验证。结果表明，与标准线性哈密顿降维方法相比，本文方法具有更好的降维特性。", "conclusion": "本文提出的结合自编码器和哈密顿神经网络的降维粒子在单元格方法，能够有效降低Vlasov-Poisson系统粒子模拟的计算成本，同时通过保持哈密顿结构来确保长期稳定性，并表现出优于传统线性方法的降维性能。", "translation": "哈密顿粒子等离子体动力学模拟本质上计算密集，这主要是由于获得精确解所需的粒子数量庞大。在需要针对一系列时间和参数值进行大量模拟的多查询场景中，这一挑战变得更加严峻。因此，从这些离散化中构建降维模型至关重要，以显著降低计算成本，同时确保在指定时间和参数域内的有效性。在这些降维模型中保留哈密顿结构也至关重要，因为它有助于保持长期稳定性。在本文中，我们引入了一种针对1D-1V Vlasov-Poisson系统的非线性、非侵入式、数据驱动的模型降维方法，该系统采用哈密顿粒子在单元格方案进行离散化。我们的方法依赖于两步投影框架：首先是基于适当辛分解的初始线性投影，然后是通过自编码器神经网络学习的非线性投影。降维后的动力学通过哈密顿神经网络建模。该方法的离线阶段分为两个阶段：首先，使用全阶模型快照构建线性投影；其次，联合训练自编码器和哈密顿神经网络，以同时学习编码器-解码器映射和降维动力学。我们在几个基准测试中验证了所提出的方法，包括Landau阻尼和双流不稳定性。结果表明，我们的方法比标准线性哈密顿降维方法具有更好的降维特性。", "summary": "本文提出了一种用于Vlasov-Poisson系统的降维粒子在单元格方法，旨在解决哈密顿粒子模拟的高计算成本问题。该方法采用新颖的两步投影框架：首先进行基于适当辛分解的线性投影，随后通过自编码器进行非线性投影。降维后的动力学由哈密顿神经网络建模。通过联合训练自编码器和哈密顿神经网络，该方法能够有效学习降维映射和动力学。在Landau阻尼和双流不稳定性等基准测试中的验证表明，该方法在降维性能上优于传统的线性哈密顿降维方法，同时有助于保持长期稳定性。", "keywords": "Vlasov-Poisson系统, 模型降维, 自编码器, 哈密顿神经网络, 粒子在单元格", "comments": "该论文的创新点在于结合了线性（Proper Symplectic Decomposition）和非线性（自编码器）投影方法，并利用哈密顿神经网络来建模降维后的动力学，从而在降低计算成本的同时，有效保持了Vlasov-Poisson系统模拟的哈密顿结构和长期稳定性。这对于需要大量模拟的多查询场景具有重要意义。"}}
{"id": "2506.15489", "title": "Advanced cervical cancer classification: enhancing pap smear images with hybrid PMD Filter-CLAHE", "authors": ["Ach Khozaimi", "Isnani Darti", "Syaiful Anam", "Wuryansari Muharini Kusumawinahyu"], "summary": "Cervical cancer remains a significant health problem, especially in\ndeveloping countries. Early detection is critical for effective treatment.\nConvolutional neural networks (CNN) have shown promise in automated cervical\ncancer screening, but their performance depends on Pap smear image quality.\nThis study investigates the impact of various image preprocessing techniques on\nCNN performance for cervical cancer classification using the SIPaKMeD dataset.\nThree preprocessing techniques were evaluated: perona-malik diffusion (PMD)\nfilter for noise reduction, contrast-limited adaptive histogram equalization\n(CLAHE) for image contrast enhancement, and the proposed hybrid PMD\nfilter-CLAHE approach. The enhanced image datasets were evaluated on pretrained\nmodels, such as ResNet-34, ResNet-50, SqueezeNet-1.0, MobileNet-V2,\nEfficientNet-B0, EfficientNet-B1, DenseNet-121, and DenseNet-201. The results\nshow that hybrid preprocessing PMD filter-CLAHE can improve the Pap smear image\nquality and CNN architecture performance compared to the original images. The\nmaximum metric improvements are 13.62% for accuracy, 10.04% for precision,\n13.08% for recall, and 14.34% for F1-score. The proposed hybrid PMD\nfilter-CLAHE technique offers a new perspective in improving cervical cancer\nclassification performance using CNN architectures.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15489v1", "AI": {"title_translation": "高级宫颈癌分类：使用混合PMD滤波器-CLAHE增强巴氏涂片图像", "tldr": "本研究提出并评估了一种混合PMD滤波器-CLAHE图像预处理技术，以提高CNN在巴氏涂片图像上的宫颈癌分类性能，结果显示分类准确率显著提升。", "motivation": "宫颈癌仍然是一个重要的健康问题，尤其是在发展中国家，早期检测至关重要。虽然卷积神经网络（CNN）在自动化宫颈癌筛查中显示出前景，但其性能高度依赖于巴氏涂片图像的质量。", "method": "本研究评估了三种图像预处理技术：用于降噪的Perona-Malik扩散（PMD）滤波器、用于图像对比度增强的对比度受限自适应直方图均衡化（CLAHE），以及提出的混合PMD滤波器-CLAHE方法。增强后的图像数据集在ResNet-34、ResNet-50、SqueezeNet-1.0、MobileNet-V2、EfficientNet-B0、EfficientNet-B1、DenseNet-121和DenseNet-201等预训练模型上进行了评估，使用SIPaKMeD数据集进行宫颈癌分类。", "result": "结果表明，与原始图像相比，混合PMD滤波器-CLAHE预处理技术可以提高巴氏涂片图像质量和CNN架构性能。最大指标改进包括准确率提高13.62%，精确率提高10.04%，召回率提高13.08%，F1-分数提高14.34%。", "conclusion": "提出的混合PMD滤波器-CLAHE技术为使用CNN架构改善宫颈癌分类性能提供了新的视角。", "translation": "宫颈癌仍然是一个重要的健康问题，尤其是在发展中国家。早期检测对于有效治疗至关重要。卷积神经网络（CNN）在自动化宫颈癌筛查中显示出前景，但其性能取决于巴氏涂片图像质量。本研究调查了各种图像预处理技术对CNN在SIPaKMeD数据集上进行宫颈癌分类性能的影响。评估了三种预处理技术：用于降噪的Perona-Malik扩散（PMD）滤波器，用于图像对比度增强的对比度受限自适应直方图均衡化（CLAHE），以及提出的混合PMD滤波器-CLAHE方法。增强后的图像数据集在ResNet-34、ResNet-50、SqueezeNet-1.0、MobileNet-V2、EfficientNet-B0、EfficientNet-B1、DenseNet-121和DenseNet-201等预训练模型上进行了评估。结果表明，与原始图像相比，混合预处理PMD滤波器-CLAHE可以提高巴氏涂片图像质量和CNN架构性能。最大指标改进包括准确率提高13.62%，精确率提高10.04%，召回率提高13.08%，F1-分数提高14.34%。提出的混合PMD滤波器-CLAHE技术为使用CNN架构改善宫颈癌分类性能提供了新的视角。", "summary": "本研究旨在提高使用卷积神经网络（CNN）进行宫颈癌分类的性能，通过改善巴氏涂片图像质量。论文评估了Perona-Malik扩散（PMD）滤波器、对比度受限自适应直方图均衡化（CLAHE）以及一种新颖的混合PMD滤波器-CLAHE预处理技术。结果表明，所提出的混合方法显著提高了图像质量和多种预训练CNN模型的分类性能，包括准确率、精确率、召回率和F1-分数均有显著提升，为宫颈癌的早期检测提供了新的有效途径。", "keywords": "宫颈癌分类, 巴氏涂片图像, 图像预处理, CNN, PMD滤波器, CLAHE", "comments": "这项研究的创新之处在于提出了混合PMD滤波器-CLAHE预处理技术，有效解决了CNN在巴氏涂片图像分类中对图像质量的依赖问题。通过结合降噪和对比度增强，该方法显著提升了分类性能，对宫颈癌的早期自动化筛查具有重要意义。其贡献在于为图像预处理提供了新的视角，并展示了其在医疗图像分析中的巨大潜力。"}}
{"id": "2506.15388", "title": "Evaluation Pipeline for systematically searching for Anomaly Detection Systems", "authors": ["Florian Rokohl", "Alexander Lehnert", "Marc Reichenbach"], "summary": "Digitalization in the medical world provides major benefits while making it a\ntarget for attackers and thus hard to secure. To deal with network intruders we\npropose an anomaly detection system on hardware to detect malicious clients in\nreal-time. We meet real-time and power restrictions using FPGAs. Overall system\nperformance is achieved via the presented holistic system evaluation.", "comment": "Submitted to 18th HiPEAC Workshop on Reconfigurable Computing\n  (WRC'2024)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15388v1", "AI": {"title_translation": "系统性搜索异常检测系统的评估流程", "tldr": "本文提出一种基于硬件的实时异常检测系统，利用FPGA在医疗网络中检测恶意客户端，并通过整体系统评估其性能。", "motivation": "医疗领域的数字化带来了巨大好处，但同时也使其成为攻击者的目标，难以保障安全。需要处理网络入侵者。", "method": "提出一种基于硬件的异常检测系统，利用FPGA满足实时性和功耗限制，并通过整体系统评估来衡量系统性能。", "result": "该系统通过利用FPGA满足了实时性和功耗限制，并通过所提出的整体系统评估实现了整体系统性能。", "conclusion": "所提出的基于硬件的异常检测系统能够有效应对医疗网络中的安全挑战，实现实时恶意客户端检测。", "translation": "医疗领域的数字化带来了巨大的益处，但同时也使其成为攻击者的目标，从而难以保障安全。为了应对网络入侵者，我们提出了一种基于硬件的异常检测系统，用于实时检测恶意客户端。我们利用FPGA来满足实时性和功耗限制。通过所提出的整体系统评估，实现了整体系统性能。", "summary": "本文针对医疗领域数字化带来的安全挑战，提出了一种基于硬件的实时异常检测系统。该系统利用FPGA技术以满足实时性和功耗限制，旨在高效检测网络中的恶意客户端，并通过全面的系统评估来确保其整体性能。", "keywords": "异常检测, 医疗安全, FPGA, 实时系统, 硬件", "comments": "该研究的创新之处在于提出了一种基于FPGA的硬件级实时异常检测系统，这对于对实时性和安全性要求极高的医疗网络环境具有重要意义。然而，摘要中未详细说明具体的检测算法和评估指标，这可能限制了对其有效性的深入理解。"}}
{"id": "2506.15332", "title": "Building Blocks of a User Experience Research Point of View", "authors": ["Patricia Diaz"], "summary": "This paper presents three User Experience Research (UXR) perspectives based\non data, evidence and insights - known as Point of View (POV) - showcasing how\nthe strategies and methods of building a POV work in an enterprise setting. The\nPOV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in\nvideos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the\nlearner as it is the loop that requires the least effort for the highest\nimpact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at\nthe intersection of emergent technical capabilities that unlock novel\napproaches to critical user needs while addressing business strategic\npriorities (2019). They all seemed far-fetched and went against common\npractice. All were adopted and had long-lasting impact.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15332v1", "AI": {"title_translation": "用户体验研究视角的构成要素", "tldr": "本文介绍了三个基于数据、证据和洞察的用户体验研究（UXR）视角（POV），展示了这些看似“异想天开”但最终被采纳并产生长期影响的POV在企业环境中的构建策略和方法。", "motivation": "本文旨在展示在企业环境中构建用户体验研究（UXR）视角的策略和方法，特别是那些最初看起来“异想天开”但最终被成功采纳并产生长期影响的视角。", "method": "本文通过介绍三个具体的、基于数据、证据和洞察的用户体验研究（UXR）视角（POV）来展示其方法，这三个POV分别是：智能视觉（Smart Visuals）、可评估代码编辑器（Assessable Code Editor）和机会景观（Opportunity Landscape）。文章通过案例说明了这些策略和方法如何在企业环境中运作。", "result": "文中提出的三个用户体验研究视角（POV）——智能视觉、可评估代码编辑器和机会景观——尽管最初看似异想天开并与常规做法相悖，但最终都被采纳并产生了长期的影响。", "conclusion": "即使是那些最初看似异想天开、与传统做法相悖的用户体验研究视角，如果基于数据、证据和洞察，也能够在企业环境中被成功采纳并产生长期影响。", "translation": "本文介绍了三个基于数据、证据和洞察的用户体验研究（UXR）视角——称为“观点”（Point of View, POV）——展示了在企业环境中构建POV的策略和方法如何运作。这些POV是：1. 智能视觉：使用人工智能从视频中的视觉内容中提取和翻译文本（2019年）。2. 可评估代码编辑器：专注于向学习者提供直接的AI反馈，因为这是实现最高影响所需最少努力的循环（2023年）。3. 机会景观：在新兴技术能力与解决关键用户需求的新颖方法以及业务战略重点的交叉点上，识别高影响力的机会（2019年）。它们最初都看起来异想天开，并且与普遍做法相悖。但所有这些都被采纳并产生了长期影响。", "summary": "本文探讨了用户体验研究（UXR）视角的构建要素，并通过三个具体的“观点”（POV）案例进行阐述。这些POV分别涉及利用AI处理视觉信息、AI对学习者的直接反馈以及识别高影响力的新兴技术机会。尽管这些POV最初被认为是异想天开且与传统实践相悖，但它们最终都在企业环境中被成功采纳并产生了深远影响，证明了基于数据、证据和洞察的创新视角的重要性。", "keywords": "用户体验研究, 视角, 企业环境, AI反馈, 创新", "comments": "本文的创新之处在于它挑战了传统的用户体验研究方法，通过展示三个最初被认为是“异想天开”但最终被成功采纳并产生长期影响的案例，强调了基于数据、证据和洞察的创新视角的重要性。它为企业在复杂环境中探索和实施突破性的UXR策略提供了宝贵的见解。"}}
{"id": "2506.15626", "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction", "authors": ["Vincent Roca", "Marc Tommasi", "Paul Andrey", "Aurélien Bellet", "Markus D. Schirmer", "Hilde Henon", "Laurent Puy", "Julien Ramon", "Grégory Kuchcinski", "Martin Bretzner", "Renaud Lopes"], "summary": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15626v1", "AI": {"title_translation": "基于MRI的联邦学习脑龄：一项卒中后功能结局预测的多中心研究", "tldr": "本研究利用联邦学习在多中心卒中患者数据上进行基于MRI的脑龄（BrainAGE）估计，以预测卒中后功能结局，结果显示联邦学习在不集中数据的情况下实现了准确预测，且BrainAGE与血管危险因素及卒中恢复密切相关，具有预后潜力。", "motivation": "训练鲁棒的脑龄（BrainAGE）模型需要大量数据集，但隐私问题限制了数据集中化。本研究旨在评估联邦学习（FL）在缺血性卒中患者中进行BrainAGE估计的性能，并探讨其与临床表型和功能结局的关联。", "method": "研究使用了来自16个医院中心的1674名卒中患者的FLAIR脑图像。采用标准机器学习和深度学习模型进行BrainAGE估计，并比较了三种数据管理策略：集中式学习、联邦学习（FL）和单站点学习。评估了预测误差，并分析了BrainAGE与血管危险因素（如糖尿病、高血压、吸烟）以及卒中后三个月功能结局之间的关联。使用逻辑回归评估BrainAGE的预测价值，并对年龄、性别、血管危险因素、卒中严重程度等混杂因素进行了调整。", "result": "集中式学习产生了最准确的预测，但联邦学习（FL）始终优于单站点模型。在所有模型中，糖尿病患者的BrainAGE显著更高。良好和不良功能结局患者之间的比较，以及这些结局的多变量预测，都显示了BrainAGE与卒中后恢复之间关联的重要性。", "conclusion": "联邦学习（FL）能够在不集中数据的情况下实现准确的年龄预测。BrainAGE与血管危险因素和卒中后恢复之间的强关联，突显了其在卒中护理中进行预后建模的潜力。", "translation": "**目标**：脑龄预测差（BrainAGE）是一种反映大脑健康的神经影像生物标志物。然而，训练鲁棒的BrainAGE模型需要大量数据集，这通常受到隐私问题的限制。本研究评估了联邦学习（FL）在机械取栓治疗的缺血性卒中患者中进行BrainAGE估计的性能，并探讨了其与临床表型和功能结局的关联。\n**方法**：我们使用了来自16个医院中心的1674名卒中患者的FLAIR脑图像。我们采用了三种数据管理策略：集中式学习（数据池化）、联邦学习（FL，在每个站点进行本地训练）和单站点学习，来实施标准机器学习和深度学习模型进行BrainAGE估计。我们报告了预测误差，并检查了BrainAGE与血管危险因素（如糖尿病、高血压、吸烟）以及卒中后三个月功能结局之间的关联。逻辑回归评估了BrainAGE对这些结局的预测价值，并调整了年龄、性别、血管危险因素、卒中严重程度、MRI与动脉穿刺之间的时间、既往静脉溶栓和再通结局等因素。\n**结果**：尽管集中式学习产生了最准确的预测，但FL始终优于单站点模型。在所有模型中，糖尿病患者的BrainAGE显著更高。良好和不良功能结局患者之间的比较，以及这些结局的多变量预测，都显示了BrainAGE与卒中后恢复之间关联的重要性。\n**结论**：联邦学习能够在不集中数据的情况下实现准确的年龄预测。BrainAGE与血管危险因素和卒中后恢复之间的强关联，突显了其在卒中护理中进行预后建模的潜力。", "summary": "这项多中心研究在1674名卒中患者中，利用联邦学习（FL）进行基于MRI的脑龄（BrainAGE）估计，旨在预测卒中后功能结局，以解决数据隐私问题。研究发现，尽管集中式学习的准确性最高，但FL始终优于单站点模型。BrainAGE与糖尿病相关，并与卒中后恢复显著关联。这表明FL能够在不集中数据的情况下实现准确的BrainAGE预测，且BrainAGE在卒中护理中具有作为预后生物标志物的潜力。", "keywords": "联邦学习, 脑龄, 卒中, 功能结局, 神经影像", "comments": "该研究创新性地将联邦学习应用于卒中患者的脑龄估计，有效解决了医疗数据隐私限制，促进了多中心协作研究。其重要性在于证明了联邦学习在处理复杂神经影像生物标志物时的可行性与有效性，为未来大规模、隐私保护的医疗数据研究提供了范例。同时，研究也再次强调了脑龄在卒中预后中的价值。然而，联邦学习的性能仍未超越集中式学习，提示未来研究可能需进一步优化算法以弥合这一差距。"}}
{"id": "2506.14903", "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization", "authors": ["Renjith Prasad", "Abhilekh Borah", "Hasnat Md Abdullah", "Chathurangi Shyalika", "Gurpreet Singh", "Ritvik Garimella", "Rajarshi Roy", "Harshul Surana", "Nasrin Imanpour", "Suranjana Trivedy", "Amit Sheth", "Amitava Das"], "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated\nimages faithfully capture user intent while maintaining safety and fairness.\nDirect Preference Optimization (DPO), prominent in large language models\n(LLMs), is extending its influence to T2I systems. This paper introduces\nDPO-Kernels for T2I models, a novel extension enhancing alignment across three\ndimensions: (i) Hybrid Loss, integrating embedding-based objectives with\ntraditional probability-based loss for improved optimization; (ii) Kernelized\nRepresentations, employing Radial Basis Function (RBF), Polynomial, and Wavelet\nkernels for richer feature transformations and better separation between safe\nand unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's\ndefault Kullback-Leibler (KL) regularizer by incorporating Wasserstein and\nR'enyi divergences for enhanced stability and robustness. We introduce\nDETONATE, the first large-scale benchmark of its kind, comprising approximately\n100K curated image pairs categorized as chosen and rejected. DETONATE\nencapsulates three axes of social bias and discrimination: Race, Gender, and\nDisability. Prompts are sourced from hate speech datasets, with images\ngenerated by leading T2I models including Stable Diffusion 3.5 Large, Stable\nDiffusion XL, and Midjourney. Additionally, we propose the Alignment Quality\nIndex (AQI), a novel geometric measure quantifying latent-space separability of\nsafe/unsafe image activations, revealing hidden vulnerabilities. Empirically,\nwe demonstrate that DPO-Kernels maintain strong generalization bounds via\nHeavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are\npublicly released.", "comment": "59 pages, 10 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14903v1", "AI": {"title_translation": "DETONATE：文本到图像对齐和核化直接偏好优化的基准", "tldr": "本文介绍了DPO-Kernels，一种用于文本到图像（T2I）模型的DPO扩展，旨在通过混合损失、核化表示和散度选择来增强对齐。同时提出了DETONATE，一个大规模的T2I对齐基准，包含约10万个图像对，用于评估社会偏见和歧视，并引入了对齐质量指数（AQI）。", "motivation": "文本到图像（T2I）模型中的对齐对于确保生成图像忠实捕捉用户意图并保持安全性和公平性至关重要。直接偏好优化（DPO）在大型语言模型中表现突出，正扩展到T2I系统，但需要对其进行增强以更好地适应T2I对齐。", "method": "本文引入了DPO-Kernels，这是DPO在T2I模型上的新颖扩展，通过三个维度增强对齐：(i) 混合损失，结合基于嵌入的目标和传统基于概率的损失；(ii) 核化表示，采用RBF、多项式和小波核进行特征转换；(iii) 散度选择，扩展了DPO的默认KL正则化器，加入了Wasserstein和R'enyi散度。此外，本文还推出了DETONATE，一个包含约10万个精选图像对的大规模基准数据集，涵盖种族、性别和残疾三个社会偏见轴。图像由领先的T2I模型生成。同时提出了对齐质量指数（AQI），一种量化安全/不安全图像激活在潜在空间中可分离性的几何度量。", "result": "经验证明，DPO-Kernels通过重尾自正则化（HT-SR）保持了强大的泛化界限。DETONATE基准和完整代码已公开发布。", "conclusion": "本文提出的DPO-Kernels通过多维度增强提升了T2I模型的对齐能力，并引入了DETONATE基准和AQI，为评估和改进T2I模型的安全性、公平性及对齐质量提供了重要工具和方法。", "translation": "对齐对于文本到图像（T2I）模型至关重要，以确保生成的图像忠实地捕捉用户意图，同时保持安全性和公平性。直接偏好优化（DPO）在大型语言模型（LLM）中表现突出，正将其影响力扩展到T2I系统。本文介绍了DPO-Kernels，这是DPO在T2I模型上的新颖扩展，旨在通过三个维度增强对齐：(i) 混合损失，将基于嵌入的目标与传统的基于概率的损失相结合，以改进优化；(ii) 核化表示，采用径向基函数（RBF）、多项式和小波核进行更丰富的特征转换，并更好地分离安全和不安全输入；以及(iii) 散度选择，通过引入Wasserstein和R'enyi散度，扩展了DPO的默认Kullback-Leibler（KL）正则化器，以增强稳定性和鲁棒性。我们推出了DETONATE，这是同类中第一个大规模基准，包含大约10万个精选的图像对，分为“选择”和“拒绝”两类。DETONATE封装了社会偏见和歧视的三个轴：种族、性别和残疾。提示词来源于仇恨言论数据集，图像由领先的T2I模型生成，包括Stable Diffusion 3.5 Large、Stable Diffusion XL和Midjourney。此外，我们提出了对齐质量指数（AQI），一种新颖的几何度量，用于量化安全/不安全图像激活在潜在空间中的可分离性，揭示了隐藏的脆弱性。经验表明，DPO-Kernels通过重尾自正则化（HT-SR）保持了强大的泛化界限。DETONATE和完整的代码已公开发布。", "summary": "本文针对文本到图像（T2I）模型的对齐问题，提出了DPO-Kernels，这是直接偏好优化（DPO）的一种创新扩展，通过结合混合损失、核化表示和多种散度选择来提升模型对齐用户意图、安全性和公平性的能力。为评估T2I模型的对齐质量，研究者构建了DETONATE，一个包含约10万个图像对的大规模基准数据集，专门用于检测种族、性别和残疾等社会偏见。此外，还引入了对齐质量指数（AQI）来量化潜在空间中安全/不安全图像激活的可分离性。实验结果表明DPO-Kernels具有良好的泛化能力，并且DETONATE基准和相关代码已公开发布，为T2I对齐研究提供了重要资源。", "keywords": "文本到图像对齐, 直接偏好优化, DPO-Kernels, DETONATE, 社会偏见", "comments": "本文在T2I模型对齐领域做出了多项创新贡献。首先，DPO-Kernels通过引入混合损失、核化表示和多样化散度选择，显著增强了DPO在T2I任务中的适用性和性能，使其能更好地处理复杂特征和优化目标。其次，DETONATE作为首个大规模的T2I对齐和偏见评估基准，填补了该领域的空白，为研究人员提供了一个量化和缓解T2I模型中社会偏见的宝贵工具。最后，AQI的提出为评估模型潜在空间中的安全性和脆弱性提供了新的视角和量化方法，具有重要的理论和实践意义。这些贡献共同推动了T2I模型的安全、公平和高质量生成。"}}
{"id": "2506.15527", "title": "On Exact Solutions to the Linear Bellman Equation", "authors": ["David Ohlin", "Richard Pates", "Murat Arcak"], "summary": "This paper presents sufficient conditions for optimal control of systems with\ndynamics given by a linear operator, in order to obtain an explicit solution to\nthe Bellman equation that can be calculated in a distributed fashion. Further,\nthe class of Linearly Solvable MDP is reformulated as a continuous-state\noptimal control problem. It is shown that this class naturally satisfies the\nconditions for explicit solution of the Bellman equation, motivating the\nextension of previous results to semilinear dynamics to account for input\nnonlinearities. The applicability of the given conditions is illustrated in\nscenarios with linear and quadratic cost, corresponding to the Stochastic\nShortest Path and Linear-Quadratic Regulator problems.", "comment": "Preprint to be published in the Control Systems Letters", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15527v1", "AI": {"title_translation": "线性贝尔曼方程精确解研究", "tldr": "本文提出了线性算子系统最优控制的充分条件，以分布式方式获得贝尔曼方程的显式解，并将线性可解MDP重新表述为连续状态最优控制问题，证明了其满足显式解条件，并推广到半线性动力学，应用于随机最短路径和线性二次调节器问题。", "motivation": "本文的动机是为了获得贝尔曼方程的显式解，该解能够以分布式方式计算，并扩展先前关于半线性动力学的结果，以解释输入非线性。", "method": "本文提出了针对具有线性算子动力学系统的最优控制的充分条件，并将线性可解MDP（马尔可夫决策过程）重新表述为连续状态最优控制问题。", "result": "研究表明，线性可解MDP自然满足贝尔曼方程显式解的条件。此外，所提出的条件在具有线性和二次成本的情景中得到了应用验证，例如随机最短路径和线性二次调节器问题。", "conclusion": "本文成功提出了获得线性贝尔曼方程显式解的充分条件，并将其应用于扩展线性可解MDP到半线性动力学，展现了其在不同成本函数下的实用性。", "translation": "本文提出了具有线性算子动力学系统的最优控制的充分条件，以便获得可以分布式计算的贝尔曼方程的显式解。此外，将线性可解MDP（马尔可夫决策过程）类别重新表述为连续状态最优控制问题。结果表明，此类问题自然满足贝尔曼方程显式解的条件，这促使将先前的结果扩展到半线性动力学，以考虑输入非线性。所给条件的适用性通过线性和二次成本情景进行了说明，对应于随机最短路径和线性二次调节器问题。", "summary": "本文研究了线性贝尔曼方程的精确解，提出了在具有线性算子动力学系统下获得最优控制的充分条件，旨在实现贝尔曼方程的分布式显式解。作者将线性可解MDP重新定义为连续状态最优控制问题，并证明该类问题满足显式解条件，从而将现有成果推广到包含输入非线性的半线性动力学。论文通过随机最短路径和线性二次调节器问题等案例，展示了所提条件的实用性。", "keywords": "贝尔曼方程, 最优控制, 显式解, 线性可解MDP, 半线性动力学", "comments": "这篇论文的创新点在于提出了获取线性贝尔曼方程显式解的充分条件，并将其应用于扩展线性可解MDP到更广泛的半线性动力学系统，这对于分布式控制和处理输入非线性具有重要意义。其方法论将离散的MDP问题连续化，为理论研究和实际应用提供了新的视角。"}}
{"id": "2310.18450", "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition", "authors": ["Jiamin Xie", "John H. L. Hansen"], "summary": "In this paper, we present MixRep, a simple and effective data augmentation\nstrategy based on mixup for low-resource ASR. MixRep interpolates the feature\ndimensions of hidden representations in the neural network that can be applied\nto both the acoustic feature input and the output of each layer, which\ngeneralizes the previous MixSpeech method. Further, we propose to combine the\nmixup with a regularization along the time axis of the input, which is shown as\ncomplementary. We apply MixRep to a Conformer encoder of an E2E LAS\narchitecture trained with a joint CTC loss. We experiment on the WSJ dataset\nand subsets of the SWB dataset, covering reading and telephony conversational\nspeech. Experimental results show that MixRep consistently outperforms other\nregularization methods for low-resource ASR. Compared to a strong SpecAugment\nbaseline, MixRep achieves a +6.5\\% and a +6.7\\% relative WER reduction on the\neval92 set and the Callhome part of the eval'2000 set.", "comment": "Accepted to Interspeech 2023", "cate": "eess.AS", "url": "http://arxiv.org/abs/2310.18450v1", "AI": {"title_translation": "MixRep：面向低资源语音识别的隐藏表征混叠", "tldr": "MixRep是一种新的数据增强策略，通过混合神经网络中的隐藏表征来提高低资源语音识别的性能，并取得了显著的词错误率（WER）降低。", "motivation": "该论文旨在解决低资源语音识别（ASR）中的性能挑战，通过引入一种有效的数据增强策略来改进模型训练。", "method": "本文提出了MixRep，一种基于mixup的数据增强策略，通过在神经网络中插值隐藏表征的特征维度，可应用于声学特征输入及每一层的输出，从而推广了MixSpeech方法。此外，MixRep与沿输入时间轴的正则化相结合。该方法应用于E2E LAS架构的Conformer编码器，并使用联合CTC损失进行训练。", "result": "MixRep在WSJ数据集和SWB数据集的子集上进行了实验，结果显示其始终优于其他低资源ASR的正则化方法。与强大的SpecAugment基线相比，MixRep在eval92集上实现了+6.5%的相对WER降低，在eval'2000集的Callhome部分实现了+6.7%的相对WER降低。", "conclusion": "MixRep是一种简单且有效的数据增强策略，能够显著提高低资源语音识别的性能，优于现有基线方法。", "translation": "在本文中，我们提出了MixRep，一种基于mixup的简单有效的数据增强策略，用于低资源ASR。MixRep插值神经网络中隐藏表征的特征维度，可应用于声学特征输入和每一层的输出，这推广了之前的MixSpeech方法。此外，我们提出将mixup与沿输入时间轴的正则化相结合，这被证明是互补的。我们将MixRep应用于使用联合CTC损失训练的E2E LAS架构的Conformer编码器。我们在WSJ数据集和SWB数据集的子集上进行了实验，涵盖了阅读和电话会话语音。实验结果表明，MixRep在低资源ASR方面始终优于其他正则化方法。与强大的SpecAugment基线相比，MixRep在eval92集和eval'2000集的Callhome部分分别实现了+6.5%和+6.7%的相对WER降低。", "summary": "本文提出MixRep，一种针对低资源语音识别（ASR）的有效数据增强策略。MixRep通过插值神经网络中的隐藏表征维度进行数据混合，并可与时间轴正则化结合。实验表明，MixRep在WSJ和SWB数据集上显著优于现有正则化方法，相较于SpecAugment基线，实现了6.5%至6.7%的相对词错误率（WER）降低。", "keywords": "MixRep, 数据增强, 低资源语音识别, 隐藏表征, Mixup", "comments": "MixRep的创新之处在于将mixup应用于神经网络的隐藏表征，并结合时间轴正则化，这为低资源ASR提供了一种通用且有效的数据增强范式。其简单性与显著的性能提升使其在实际应用中具有重要价值。"}}
{"id": "2506.14810", "title": "Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies", "authors": ["Qiwen Zhang"], "summary": "Sparse and intermittent demand forecasting in supply chains presents a\ncritical challenge, as frequent zero-demand periods hinder traditional model\naccuracy and impact inventory management. We propose and evaluate a\nModel-Router framework that dynamically selects the most suitable forecasting\nmodel-spanning classical, ML, and DL methods for each product based on its\nunique demand pattern. By comparing rule-based, LightGBM, and InceptionTime\nrouters, our approach learns to assign appropriate forecasting strategies,\neffectively differentiating between smooth, lumpy, or intermittent demand\nregimes to optimize predictions. Experiments on the large-scale Favorita\ndataset show our deep learning (Inception Time) router improves forecasting\naccuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with\n4.67x faster inference time. Ultimately, these gains in forecasting precision\nwill drive substantial reductions in both stockouts and wasteful excess\ninventory, underscoring the critical role of intelligent, adaptive Al in\noptimizing contemporary supply chain operations.", "comment": "7 pages, 4 figures, conference", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14810v1", "AI": {"title_translation": "稀疏需求预测的智能路由：选择策略的比较评估", "tldr": "本文提出了一个模型路由器框架，用于动态选择最适合稀疏和间歇性需求模式的预测模型，并在Favorita数据集上展示了显著的预测精度提升和更快的推理速度。", "motivation": "供应链中稀疏和间歇性需求预测是一个严峻的挑战，因为频繁的零需求期会阻碍传统模型的准确性并影响库存管理。", "method": "本文提出了一个模型路由器框架，该框架动态选择最适合每个产品独特需求模式的预测模型，涵盖经典、机器学习和深度学习方法。通过比较基于规则、LightGBM和InceptionTime路由器，该方法学习分配适当的预测策略，有效区分平滑、块状或间歇性需求模式以优化预测。", "result": "在大型Favorita数据集上的实验表明，深度学习（Inception Time）路由器相比强大的单一模型基准，预测准确性提高了11.8%（NWRMSLE），推理时间快了4.67倍。", "conclusion": "预测精度的提高将显著减少缺货和浪费的过剩库存，突显了智能自适应AI在优化当代供应链运营中的关键作用。", "translation": "供应链中稀疏和间歇性需求预测是一个严峻的挑战，因为频繁的零需求期会阻碍传统模型的准确性并影响库存管理。我们提出并评估了一个模型路由器框架，该框架根据每个产品独特的需求模式，动态选择最合适的预测模型，涵盖经典、机器学习和深度学习方法。通过比较基于规则、LightGBM和InceptionTime路由器，我们的方法学会了分配适当的预测策略，有效区分平滑、块状或间歇性需求模式以优化预测。在大型Favorita数据集上的实验表明，我们的深度学习（Inception Time）路由器相比强大的单一模型基准，预测准确性提高了11.8%（NWRMSLE），推理时间快了4.67倍。最终，这些预测精度的提高将显著减少缺货和浪费的过剩库存，突显了智能自适应AI在优化当代供应链运营中的关键作用。", "summary": "本文针对供应链中稀疏和间歇性需求预测的挑战，提出了一个名为Model-Router的框架。该框架能够根据产品的具体需求模式，动态地选择最佳的预测模型（包括经典、ML和DL方法）。通过比较基于规则、LightGBM和InceptionTime路由器，研究发现深度学习（Inception Time）路由器在大型Favorita数据集上表现最佳，将预测准确性提高了11.8%，同时推理速度加快了4.67倍。这表明智能自适应AI在优化供应链运营和减少库存方面具有重要价值。", "keywords": "需求预测, 智能路由, 稀疏需求, 供应链, 模型选择", "comments": "该论文的创新点在于提出了一个动态的模型路由器框架，能够根据不同的需求模式自适应地选择最合适的预测模型，这对于解决稀疏和间歇性需求预测的挑战非常重要。其在预测精度和推理速度上的显著提升，表明了其在实际供应链管理中的巨大潜力。"}}
{"id": "2506.15211", "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "authors": ["Feng He", "Zijun Chen", "Xinnian Liang", "Tingting Ma", "Yunqi Qiu", "Shuangzhi Wu", "Junchi Yan"], "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15211v1", "AI": {"title_translation": "ProtoReasoning：原型作为大型语言模型中可泛化推理的基础", "tldr": "ProtoReasoning是一个利用原型表示来增强大型语言模型推理能力的框架，并通过实验证明了其在不同推理任务上的显著改进和更好的泛化能力。", "motivation": "尽管大型推理模型（LRMs）在长链思维（Long CoT）推理方面展现出卓越的跨领域泛化能力，但支持这种迁移的底层机制尚不清楚。本文假设跨领域泛化源于共享的抽象推理原型，这些原型能捕捉跨领域问题的本质，并揭示看似多样化的任务实际上基于共享的推理结构。", "method": "本文提出了ProtoReasoning框架，通过利用可扩展且可验证的原型表示（Prolog用于逻辑推理，PDDL用于规划）来增强LLMs的推理能力。该框架包括：1) 自动化原型构建流程，将问题转换为相应的原型表示；2) 综合验证系统，通过Prolog/PDDL解释器提供可靠反馈；3) 在原型空间内任意合成问题并确保正确性的可扩展性。", "result": "ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在通用推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。消融研究证实，与仅在自然语言表示上训练相比，在原型空间中学习也表现出对结构相似问题的泛化能力增强。", "conclusion": "本文验证了推理原型作为大型语言模型中可泛化推理基础的假设，并证明了ProtoReasoning框架能有效提升LLMs的推理能力和泛化性。", "translation": "大型推理模型（LRMs）通过长链思维（Long CoT）推理训练，在近期取得了显著的跨领域泛化能力。然而，支持这种迁移的底层机制仍知之甚少。我们假设跨领域泛化源于共享的抽象推理原型——捕捉跨领域问题本质的基本推理模式。这些原型最大限度地减少了表示的细微差别，揭示了看似多样的任务实际上基于共享的推理结构。基于这一假设，我们提出了ProtoReasoning，一个通过利用可扩展和可验证的原型表示（Prolog用于逻辑推理，PDDL用于规划）来增强大型语言模型推理能力的框架。ProtoReasoning的特点包括：(1) 一个自动化的原型构建流程，将问题转换为相应的原型表示；(2) 一个综合验证系统，通过Prolog/PDDL解释器提供可靠反馈；(3) 在原型空间内任意合成问题同时确保正确性的可扩展性。广泛的实验表明，ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在通用推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。重要的是，我们的消融研究证实，与仅在自然语言表示上训练相比，在原型空间中学习也表现出对结构相似问题的泛化能力增强，这验证了我们的假设，即推理原型是大型语言模型中可泛化推理的基础。", "summary": "ProtoReasoning提出了一种新颖的框架，通过引入抽象推理原型来增强大型语言模型的泛化推理能力。该框架基于原型假设，利用Prolog和PDDL等可验证的原型表示，实现了自动化原型构建、综合验证和问题合成。实验证明，ProtoReasoning在多项推理任务上均显著优于基线模型，并增强了模型对结构相似问题的泛化能力，从而验证了原型作为可泛化推理基础的有效性。", "keywords": "ProtoReasoning, 大型语言模型, 推理, 原型, 泛化", "comments": "ProtoReasoning的创新点在于提出了“抽象推理原型”的概念，并将其作为LLM通用推理的基础。通过将问题转换为形式化的原型表示（如Prolog/PDDL），该方法为LLM的推理提供了一个更结构化、可验证且可扩展的学习空间，这对于提升LLM在复杂推理任务上的鲁度性和泛化能力具有重要意义。其自动化构建和验证系统也提高了训练效率和可靠性。"}}
{"id": "2506.15259", "title": "Splitting-based randomised dynamical low-rank approximations for stiff matrix differential equations", "authors": ["Zi Wu", "Yong-Liang Zhao"], "summary": "In the fields of control theory and machine learning, the dynamic low-rank\napproximation for large-scale matrices has received substantial attention.\nConsidering the large-scale semilinear stiff matrix differential equations, we\npropose a dynamic numerical integrator for obtaining low-rank approximations of\nsolutions. We first decompose the differential equation into a stiff linear\ncomponent and a nonstiff nonlinear term, then employ an exponential integrator\nalong with a dynamic low-rank approach to resolve these subsystems,\nrespectively. Furthermore, the proposed framework naturally extends to\nrank-adaptation scenarios. Through rigorous validation on canonical stiff\nmatrix differential problems, including spatially discretized Allen-Cahn\nequations and differential Riccati equations, we demonstrate that the method\nachieves the theoretically predicted convergence orders. Numerical evidence\nconfirms the robustness and accuracy of the proposed methods.", "comment": "6 figures, 12 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15259v1", "AI": {"title_translation": "基于分裂的随机动态低秩逼近，用于求解刚性矩阵微分方程", "tldr": "本文提出了一种基于分裂的随机动态低秩逼近方法，用于解决刚性矩阵微分方程，并展示了其鲁棒性和准确性。", "motivation": "在控制理论和机器学习领域，大规模矩阵的动态低秩逼近受到广泛关注。本文旨在为大规模半线性刚性矩阵微分方程提供一种有效的低秩近似解法。", "method": "该方法首先将微分方程分解为刚性线性分量和非刚性非线性项，然后分别采用指数积分器和动态低秩方法来处理这些子系统。该框架还可自然地扩展到秩自适应场景。", "result": "该方法在典型的刚性矩阵微分问题（如空间离散的Allen-Cahn方程和微分Riccati方程）上进行了严格验证，结果表明其达到了理论预测的收敛阶数，并展现出鲁棒性和准确性。", "conclusion": "所提出的基于分裂的随机动态低秩逼近方法对于刚性矩阵微分方程是鲁棒、准确的，并能达到理论预测的收敛阶数。", "translation": "在控制理论和机器学习领域，大规模矩阵的动态低秩逼近受到了广泛关注。考虑到大规模半线性刚性矩阵微分方程，我们提出了一种动态数值积分器来获得解的低秩逼近。我们首先将微分方程分解为刚性线性分量和非刚性非线性项，然后分别采用指数积分器和动态低秩方法来解决这些子系统。此外，所提出的框架自然地扩展到秩自适应场景。通过对典型的刚性矩阵微分问题（包括空间离散的Allen-Cahn方程和微分Riccati方程）进行严格验证，我们证明该方法达到了理论预测的收敛阶数。数值证据证实了所提出方法的鲁棒性和准确性。", "summary": "本文提出了一种新颖的基于分裂的随机动态低秩逼近方法，用于求解大规模半线性刚性矩阵微分方程。该方法将方程分解为刚性线性部分和非刚性非线性部分，并分别采用指数积分器和动态低秩方法进行处理。数值验证表明，该方法鲁棒、准确，并能达到理论收敛阶数，同时适用于秩自适应场景。", "keywords": "动态低秩逼近, 刚性矩阵微分方程, 分裂方法, 指数积分器, 秩自适应", "comments": "该论文的创新点在于将分裂技术与随机动态低秩逼近和指数积分器相结合，以处理刚性矩阵微分方程，特别解决了大规模问题的挑战。其对秩自适应场景的扩展也是一个显著特点。"}}
{"id": "2506.15557", "title": "Construction of an Organ Shape Atlas Using a Hierarchical Mesh Variational Autoencoder", "authors": ["Zijie Wang", "Ryuichi Umehara", "Mitsuhiro Nakamura", "Megumi Nakao"], "summary": "An organ shape atlas, which represents the shape and position of the organs\nand skeleton of a living body using a small number of parameters, is expected\nto have a wide range of clinical applications, including intraoperative\nguidance and radiotherapy. Because the shape and position of soft organs vary\ngreatly among patients, it is difficult for linear models to reconstruct shapes\nthat have large local variations. Because it is difficult for conventional\nnonlinear models to control and interpret the organ shapes obtained, deep\nlearning has been attracting attention in three-dimensional shape\nrepresentation. In this study, we propose an organ shape atlas based on a mesh\nvariational autoencoder (MeshVAE) with hierarchical latent variables. To\nrepresent the complex shapes of biological organs and nonlinear shape\ndifferences between individuals, the proposed method maintains the performance\nof organ shape reconstruction by hierarchizing latent variables and enables\nshape representation using lower-dimensional latent variables. Additionally,\ntemplates that define vertex correspondence between different resolutions\nenable hierarchical representation in mesh data and control the global and\nlocal features of the organ shape. We trained the model using liver and stomach\norgan meshes obtained from 124 cases and confirmed that the model reconstructed\nthe position and shape with an average distance between vertices of 1.5 mm and\nmean distance of 0.7 mm for the liver shape, and an average distance between\nvertices of 1.4 mm and mean distance of 0.8 mm for the stomach shape on test\ndata from 19 of cases. The proposed method continuously represented\ninterpolated shapes, and by changing latent variables at different hierarchical\nlevels, the proposed method hierarchically separated shape features compared\nwith PCA.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15557v1", "AI": {"title_translation": "使用分层网格变分自编码器构建器官形状图谱", "tldr": "本文提出了一种基于分层网格变分自编码器（MeshVAE）的器官形状图谱构建方法，通过分层潜在变量实现复杂器官形状的高效表示和重建，并在肝脏和胃部数据上取得了高精度。", "motivation": "器官形状图谱在临床应用中潜力巨大，但软器官形状差异大，线性模型难以重建局部大变化，传统非线性模型难以控制和解释。深度学习在三维形状表示方面受到关注。", "method": "本文提出了一种基于网格变分自编码器（MeshVAE）的器官形状图谱，其特点是使用分层潜在变量。通过分层潜在变量，该方法能够表示复杂的生物器官形状和个体间的非线性形状差异，同时使用低维潜在变量实现形状表示并保持重建性能。此外，利用定义不同分辨率间顶点对应关系的模板，实现了网格数据的分层表示，并能控制器官形状的全局和局部特征。", "result": "模型在124例肝脏和胃部器官网格数据上进行训练，并在19例测试数据上进行了验证。肝脏形状的重建平均顶点距离为1.5毫米，平均距离为0.7毫米；胃部形状的重建平均顶点距离为1.4毫米，平均距离为0.8毫米。所提出的方法能够连续表示插值形状，并且通过改变不同层次的潜在变量，与PCA相比，能够分层分离形状特征。", "conclusion": "本研究提出的基于分层网格变分自编码器（MeshVAE）的器官形状图谱，能够高效准确地表示和重建复杂器官形状，并通过分层潜在变量实现形状特征的有效分离和控制，具有潜在的临床应用价值。", "translation": "器官形状图谱通过少量参数表示活体器官和骨骼的形状和位置，预计将具有广泛的临床应用，包括术中指导和放射治疗。由于软器官的形状和位置在患者之间差异很大，线性模型难以重建具有大局部变化的形状。由于传统的非线性模型难以控制和解释所获得的器官形状，深度学习在三维形状表示方面一直备受关注。在这项研究中，我们提出了一种基于具有分层潜在变量的网格变分自编码器（MeshVAE）的器官形状图谱。为了表示生物器官的复杂形状和个体之间非线性的形状差异，所提出的方法通过分层潜在变量保持了器官形状重建的性能，并能够使用较低维度的潜在变量进行形状表示。此外，定义不同分辨率之间顶点对应关系的模板实现了网格数据中的分层表示，并控制了器官形状的全局和局部特征。我们使用从124个病例获得的肝脏和胃部器官网格训练了该模型，并确认该模型在19个测试病例的数据上重建位置和形状的平均顶点距离肝脏为1.5毫米，平均距离为0.7毫米；胃部为1.4毫米，平均距离为0.8毫米。所提出的方法连续表示了插值形状，并且通过改变不同层次的潜在变量，与PCA相比，所提出的方法分层分离了形状特征。", "summary": "本研究提出了一种利用分层网格变分自编码器（MeshVAE）构建器官形状图谱的方法。针对软器官形状复杂且个体差异大的问题，该方法通过分层潜在变量和多分辨率模板，实现了对器官形状的高效、低维表示和高精度重建。实验在肝脏和胃部数据上验证了其重建精度，并证明了其在分层分离形状特征方面的优越性。", "keywords": "器官形状图谱, 网格变分自编码器, 分层潜在变量, 形状重建, 深度学习", "comments": "本文的创新点在于将分层潜在变量引入网格变分自编码器中，从而能够更好地处理复杂生物器官的非线性形状变化，并实现全局和局部形状特征的有效控制。这种分层表示能力是其相对于传统线性或非线性模型的重要优势，对于需要精确器官建模的临床应用具有重要意义。"}}
{"id": "2506.15417", "title": "Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules", "authors": ["Alessandro Palumbo", "Ruben Salvador"], "summary": "Software-exploitable Hardware Trojans (HTs) enable attackers to execute\nunauthorized software or gain illicit access to privileged operations. This\nmanuscript introduces a hardware-based methodology for detecting runtime HT\nactivations using Error Correction Codes (ECCs) on a RISC-V microprocessor.\nSpecifically, it focuses on HTs that inject malicious instructions, disrupting\nthe normal execution flow by triggering unauthorized programs. To counter this\nthreat, the manuscript introduces a Hardware Security Checker (HSC) leveraging\nHamming Single Error Correction (HSEC) architectures for effective HT\ndetection. Experimental results demonstrate that the proposed solution achieves\na 100% detection rate for potential HT activations, with no false positives or\nundetected attacks. The implementation incurs minimal overhead, requiring only\n72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's\noriginal operating frequency and introducing no additional time delay.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 5 figures,", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15417v1", "AI": {"title_translation": "基于硬件纠错码模块的微处理器硬件木马检测", "tldr": "本论文提出了一种基于ECC的硬件安全检查器，用于检测RISC-V微处理器中运行时硬件木马的激活，实现了100%的检测率和极低的开销。", "motivation": "可软件利用的硬件木马 (HTs) 允许攻击者执行未经授权的软件或非法获取特权操作，通过注入恶意指令扰乱正常的执行流程。因此，需要一种基于硬件的方法来检测这些运行时HT的激活。", "method": "本文引入了一种硬件安全检查器 (HSC)，该检查器利用汉明单错误纠正 (HSEC) 架构。该HSC应用于RISC-V微处理器，旨在检测注入恶意指令的硬件木马。", "result": "所提出的解决方案对潜在的硬件木马激活实现了100%的检测率，没有误报或未检测到的攻击。该实现开销极小，仅需72个#LUT、24个#FF和0.5个#BRAM，同时保持微处理器的原始工作频率，并且不引入额外的时延。", "conclusion": "本文提出的基于ECC的硬件安全检查器能够高效地检测微处理器中的运行时硬件木马，具有高准确性和极低的开销。", "translation": "可软件利用的硬件木马 (HTs) 使攻击者能够执行未经授权的软件或非法获取特权操作。本手稿介绍了一种基于硬件的方法，用于在 RISC-V 微处理器上使用纠错码 (ECC) 检测运行时 HT 激活。具体而言，它侧重于注入恶意指令的 HT，通过触发未经授权的程序来扰乱正常的执行流程。为了应对这一威胁，本手稿引入了一种硬件安全检查器 (HSC)，利用汉明单错误纠正 (HSEC) 架构进行有效的 HT 检测。实验结果表明，所提出的解决方案对潜在的 HT 激活实现了 100% 的检测率，没有误报或未检测到的攻击。该实现开销极小，仅需 72 个 #LUT、24 个 #FF 和 0.5 个 #BRAM，同时保持微处理器的原始工作频率，并且不引入额外的时延。", "summary": "本文提出了一种基于硬件的方法，用于检测RISC-V微处理器中的运行时硬件木马 (HTs)，特别是那些注入恶意指令的HTs。该方法引入了一个利用汉明单错误纠正 (HSEC) 架构的硬件安全检查器 (HSC)。实验结果表明，该解决方案实现了100%的检测率，无误报，且硬件开销极小，同时保持了微处理器的原始性能。", "keywords": "硬件木马, 纠错码, RISC-V, 硬件安全检查器, HSEC", "comments": "本文为微处理器中的硬件木马检测提供了一种高效且实用的硬件级解决方案。利用纠错码进行安全检查是其创新点，它在实现高检测率的同时，将硬件开销降到最低，这对于实际应用，尤其是在资源受限的环境中，具有重要意义。此外，保持原始工作频率和不引入额外时延的特性也大大增加了其实用性。"}}
{"id": "2506.15157", "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation", "authors": ["Hanbit Oh", "Andrea M. Salcedo-Vázquez", "Ixchel G. Ramirez-Alpizar", "Yukiyasu Domae"], "summary": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2025 accepted", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15157v1", "AI": {"title_translation": "鲁棒即时策略：利用学生t回归模型实现机器人操作的鲁棒上下文模仿学习", "tldr": "提出一种名为鲁棒即时策略（RIP）的新算法，利用学生t回归模型解决上下文模仿学习中LLM幻觉问题，显著提高了机器人任务成功率。", "motivation": "现有的上下文模仿学习（In-Context IL）利用大型语言模型（LLMs）作为即时策略，但其在机器人领域中因LLM的幻觉问题而可靠性受损，即LLM偶尔会生成偏离示范的糟糕轨迹。", "method": "提出鲁棒即时策略（RIP）算法。该算法从LLM生成多个候选机器人轨迹，并利用学生t分布对这些轨迹进行聚合。学生t分布有利于忽略异常值（即幻觉），从而生成对幻觉具有鲁棒性的可靠轨迹。", "result": "在模拟和真实世界环境中的实验表明，RIP显著优于最先进的模仿学习方法，任务成功率至少提高了26%，尤其是在低数据量的日常任务场景中。", "conclusion": "鲁棒即时策略（RIP）通过利用学生t回归模型有效地解决了上下文模仿学习中LLM幻觉导致的轨迹生成不可靠问题，显著提升了机器人任务的成功率。", "translation": "模仿学习（IL）旨在通过观察少量人类演示使机器人自主执行任务。最近，一种称为上下文模仿学习（In-Context IL）的IL变体，利用现成的大型语言模型（LLMs）作为即时策略，从少量给定演示中理解上下文来执行新任务，而不是通过大规模演示显式更新网络模型。然而，其在机器人领域的可靠性受到LLM即时策略幻觉问题的损害，即LLM偶尔会生成偏离给定演示的糟糕轨迹。为了缓解这个问题，我们提出了一种新的鲁棒上下文模仿学习算法，称为鲁棒即时策略（RIP），该算法利用学生t回归模型来抵抗即时策略的幻觉轨迹，从而实现可靠的轨迹生成。具体来说，RIP从LLM生成多个候选机器人轨迹以完成给定任务，并使用学生t分布对其进行聚合，这有利于忽略异常值（即幻觉）；从而生成对幻觉具有鲁棒性的轨迹。我们在模拟和真实世界环境中进行的实验表明，RIP显著优于最先进的IL方法，任务成功率至少提高了26%，特别是在日常任务的低数据场景中。视频结果可在https://sites.google.com/view/robustinstantpolicy查看。", "summary": "本文提出了一种名为鲁棒即时策略（RIP）的新算法，旨在解决基于大型语言模型（LLM）的上下文模仿学习中因LLM幻觉导致的轨迹生成不可靠问题。RIP利用学生t回归模型聚合LLM生成的多个候选轨迹，通过学生t分布的特性有效抑制异常值（幻觉），从而生成鲁棒且可靠的机器人操作轨迹。实验证明，RIP在模拟和真实世界环境中均显著优于现有方法，在低数据量场景下任务成功率提升至少26%。", "keywords": "模仿学习, 大型语言模型, 鲁棒即时策略, 学生t回归, 机器人操作", "comments": "这篇论文创新性地将学生t回归模型引入到机器人上下文模仿学习中，以解决大型语言模型固有的幻觉问题。通过聚合和鲁棒处理LLM生成的候选轨迹，该方法有效提高了机器人操作的可靠性和任务成功率，尤其在数据稀缺的实际应用场景中展现出巨大潜力。"}}
{"id": "2506.15468", "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI", "authors": ["Ryota Okumura", "Tadahiro Taniguchi", "Akira Taniguchi", "Yoshinobu Hagiwara"], "summary": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15468v1", "AI": {"title_translation": "人类与人工智能通过Metropolis-Hastings交互实现的协同创造学习", "tldr": "本文提出了一种新颖的协同创造学习范式，即人类和AI相互整合信息以构建共享表征。通过Metropolis-Hastings命名游戏进行实证测试，结果表明，基于MH的AI显著提高了人类的分类准确性，并促进了共享符号系统的形成，为共生AI系统开辟了新途径。", "motivation": "传统的人工智能教学基于单边知识转移，无法有效整合人类和人工智能（具有不同模态）之间的信息。本文旨在提出并验证一种协同创造学习范式，以解决这一挑战，实现人类和AI的相互学习及共享表征构建，从而走向共生人工智能系统。", "method": "本文提出了协同创造学习作为一种新范式。通过基于Metropolis-Hastings命名游戏（MHNG）的人机交互模型对该框架进行了实证测试，MHNG是一种去中心化的贝叶斯推理机制。在一项在线实验中，69名参与者在部分可观察性下，与三种计算机代理类型（基于MH、始终接受或始终拒绝）之一进行了联合注意命名游戏（JA-NG）。", "result": "与基于MH的代理配对的人机组合通过交互显著提高了分类准确性，并实现了更强的向共享符号系统的收敛。此外，人类的接受行为与MH导出的接受概率紧密对齐。", "conclusion": "这些发现首次为通过基于MHNG的交互在人机二元组中涌现的协同创造学习提供了实证证据，表明了通向共生人工智能系统的一条有前景的道路，这些系统通过动态调整感知经验与人类共同学习，而不是从人类那里学习。", "translation": "我们提出协同创造学习作为一种新颖的范式，其中人类和人工智能，即生物和人工智能代理，相互整合其部分感知信息和知识，以构建共享的外部表征，我们将其解释为符号涌现的过程。与基于单边知识转移的传统人工智能教学不同，这解决了整合来自本质上不同模态信息所面临的挑战。我们使用基于Metropolis-Hastings命名游戏（MHNG）的人机交互模型对该框架进行了实证测试，MHNG是一种去中心化的贝叶斯推理机制。在一项在线实验中，69名参与者在部分可观察性下与三种计算机代理类型（基于MH、始终接受或始终拒绝）之一进行了一项联合注意命名游戏（JA-NG）。结果显示，与基于MH的代理配对的人工智能组合通过交互显着提高了分类准确性，并实现了更强的向共享符号系统的收敛。此外，人类的接受行为与MH导出的接受概率紧密对齐。这些发现为通过基于MHNG的交互在人机二元组中涌现的协同创造学习提供了第一个实证证据。这表明了通向共生人工智能系统的一条有前景的道路，这些系统通过动态调整感知经验与人类共同学习，而不是从人类那里学习，为共生人工智能对齐开辟了新的途径。", "summary": "本文提出了一种新颖的协同创造学习范式，其中人类和人工智能相互整合信息以构建共享外部表征，并将其解释为符号涌现。该方法旨在解决传统单边知识转移模式下，不同模态信息整合的挑战。通过基于Metropolis-Hastings命名游戏（MHNG）的在线实验，研究发现，与基于MH的AI代理交互的人类显著提高了分类准确性，并趋向于共享符号系统。这些发现首次提供了协同创造学习在人机交互中涌现的实证证据，为开发与人类共同学习而非仅从人类学习的共生AI系统开辟了新途径。", "keywords": "协同创造学习, 人机交互, Metropolis-Hastings, 符号涌现, 共生AI", "comments": "本文提出了一种创新的“协同创造学习”范式，将AI学习模式从传统的单边知识转移转变为人机之间的相互信息整合。利用Metropolis-Hastings命名游戏作为实证测试平台，为研究人机二元组中的符号涌现提供了一个具体机制，具有创新性。实验结果表明分类准确性提高和共享符号系统趋同，这突显了该方法在开发真正与人类“共同学习”而非仅“从人类学习”的共生AI系统方面的潜力，为AI对齐研究开辟了重要的新方向。"}}
{"id": "2506.14907", "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "authors": ["Yizhen Zhang", "Yang Ding", "Shuoshuo Zhang", "Xinchen Zhang", "Haoling Li", "Zhong-zhi Li", "Peijie Wang", "Jie Wu", "Lei Ji", "Yelong Shen", "Yujiu Yang", "Yeyun Gong"], "summary": "Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14907v1", "AI": {"title_translation": "PeRL：用于交错视觉-语言推理的置换增强强化学习", "tldr": "PeRL提出了一种新的强化学习方法，通过图像序列置换和回滚过滤机制，解决了现有方法在多图像定位推理中的局限性，并在多图像基准测试中实现了最先进的性能。", "motivation": "现有的多模态强化学习方法主要局限于单图像空间推理，难以泛化到涉及多图像位置推理的复杂真实场景，在这些场景中理解图像间关系至关重要。", "method": "我们提出了一个通用的强化学习方法PeRL，专门用于交错多模态任务，并设计了一个多阶段策略来增强探索-利用权衡。具体来说，我们引入图像序列置换来模拟多样的位置关系以探索空间和位置多样性，并设计了一个回滚过滤机制进行重采样，以关注最有助于学习最优行为的轨迹。", "result": "PeRL模型在5个常用的多图像基准测试和3个单图像基准测试中进行了评估。实验证实，PeRL训练的模型在多图像基准测试中显著超越了R1相关和交错VLM基线，取得了最先进的性能，同时在单图像任务上保持了可比的性能。", "conclusion": "PeRL通过引入图像序列置换和回滚过滤机制，成功解决了多图像位置推理的挑战，并在多图像视觉-语言推理任务中达到了最先进的性能。", "translation": "受DeepSeek-R1等强化学习方法所展示的强大推理能力的启发，近期新兴研究已开始探索使用强化学习（RL）来增强视觉-语言模型（VLM）以进行多模态推理任务。然而，大多数现有的多模态强化学习方法仍局限于单图像上下文中的空间推理，并且仍难以泛化到涉及多图像位置推理的更复杂和真实世界的场景，在这些场景中理解图像间的关系至关重要。为了解决这一挑战，我们提出了一种通用的强化学习方法PeRL，专为交错多模态任务设计，以及一种旨在增强探索-利用权衡的多阶段策略，从而提高学习效率和任务性能。具体来说，我们引入图像序列的置换来模拟多样的位置关系，以探索更多的空间和位置多样性。此外，我们设计了一种回滚过滤机制用于重采样，以专注于最有助于学习最优行为的轨迹，从而有效利用学习到的策略。我们在5个广泛使用的多图像基准测试和3个单图像基准测试上评估了我们的模型。我们的实验证实，PeRL训练的模型始终以显著优势超越了R1相关和交错VLM基线，在多图像基准测试上取得了最先进的性能，同时在单图像任务上保持了可比的性能。", "summary": "本研究提出了一种名为PeRL的强化学习方法，旨在解决现有视觉-语言模型在多图像位置推理任务中的局限性。PeRL通过引入图像序列置换来模拟多样的空间和位置关系，并设计了一种回滚过滤机制来优化策略学习。实验结果表明，PeRL在多图像基准测试上显著优于现有基线，达到了最先进的性能，同时在单图像任务上表现良好。", "keywords": "强化学习, 视觉-语言推理, 多图像, 置换, PeRL", "comments": "PeRL的创新点在于其针对多图像定位推理的强化学习方法，特别是引入图像序列置换来模拟多样性，以及回滚过滤机制以优化探索-利用。这解决了现有RL-VLM在复杂多图像场景中泛化能力不足的问题，对提升真实世界多模态推理能力具有重要意义。"}}
{"id": "2506.14811", "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning", "authors": ["Mikel Malagón", "Josu Ceberio", "Jose A. Lozano"], "summary": "This work introduces a growable and modular neural network architecture that\nnaturally avoids catastrophic forgetting and interference in continual\nreinforcement learning. The structure of each module allows the selective\ncombination of previous policies along with its internal policy, accelerating\nthe learning process on the current task. Unlike previous growing neural\nnetwork approaches, we show that the number of parameters of the proposed\napproach grows linearly with respect to the number of tasks, and does not\nsacrifice plasticity to scale. Experiments conducted in benchmark continuous\ncontrol and visual problems reveal that the proposed approach achieves greater\nknowledge transfer and performance than alternative methods.", "comment": "ICML 2024 (oral)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14811v1", "AI": {"title_translation": "可伸缩持续强化学习的自组合策略", "tldr": "提出一种可增长模块化神经网络架构，通过策略自组合解决持续强化学习中的灾难性遗忘问题，并实现参数线性增长和更好的性能。", "motivation": "解决持续强化学习中灾难性遗忘和干扰问题，并改进现有增长型神经网络方法在参数增长和可塑性方面的限制。", "method": "提出一种可增长、模块化的神经网络架构。每个模块的结构允许选择性地结合先前策略与其内部策略，以加速当前任务的学习。该方法参数数量随任务数量线性增长，且不牺牲可塑性。", "result": "在基准连续控制和视觉问题上的实验表明，所提出的方法比替代方法实现了更好的知识迁移和性能。", "conclusion": "所提出的自组合策略的模块化神经网络架构有效解决了持续强化学习中的灾难性遗忘和干扰问题，同时具有良好的可伸缩性和优越的性能。", "translation": "这项工作引入了一种可增长的模块化神经网络架构，它在持续强化学习中自然避免了灾难性遗忘和干扰。每个模块的结构允许选择性地结合先前的策略及其内部策略，从而加速当前任务的学习过程。与以前的增长型神经网络方法不同，我们表明所提出的方法的参数数量随任务数量线性增长，并且不牺牲可塑性以实现扩展。在基准连续控制和视觉问题上进行的实验表明，所提出的方法比替代方法实现了更大的知识迁移和性能。", "summary": "本文提出一种新型可增长、模块化的神经网络架构，旨在解决持续强化学习中的灾难性遗忘和干扰问题。该架构通过允许模块选择性地结合历史策略与当前内部策略来加速学习。与现有方法相比，其参数数量随任务线性增长且不牺牲可塑性。实验证明，该方法在知识迁移和性能上均优于其他替代方案。", "keywords": "持续强化学习, 灾难性遗忘, 神经网络架构, 策略组合, 可伸缩性", "comments": "这项工作的创新之处在于其“自组合策略”的模块化设计，它有效地解决了持续强化学习中的核心挑战——灾难性遗忘，同时保证了模型参数的线性增长，从而提高了可伸缩性。这对于在真实世界中部署持续学习系统具有重要意义。"}}
{"id": "2506.15215", "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "authors": ["Yongqi Fan", "Yating Wang", "Guandong Wang", "Jie Zhai", "Jingping Liu", "Qi Ye", "Tong Ruan"], "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15215v1", "AI": {"title_translation": "MinosEval：区分事实型和非事实型问题以实现针对LLM开放式问答评估的定制化", "tldr": "MinosEval通过区分事实型和非事实型问题，为开放式问答提供定制化评估，比现有方法更准确、更具解释性。", "motivation": "开放式问答对LLM评估至关重要，但现有评估方法（如ROUGE、BERTScore、LLM-based比较）在捕捉语义相似性、提供解释性以及适应不同问题内容方面存在挑战，尤其忽视了事实型和非事实型问题的区别。", "method": "提出MinosEval，一种新颖的评估方法，它首先区分开放式问题类型（事实型或非事实型），然后采用不同的评估策略：对事实型问题使用自适应关键点评分策略，对非事实型问题使用实例感知列表式排名策略。", "result": "在多个开放式QA数据集（包括自建数据集）上的实验表明，MinosEval与人工标注的对齐度更高，并提供了更具解释性的结果。", "conclusion": "MinosEval通过区分问题类型并采用定制化评估策略，有效解决了现有开放式QA评估方法的局限性，提高了评估的准确性和解释性。", "translation": "开放式问答（QA）是评估大型语言模型（LLM）能力的关键任务。与封闭式问答相比，它需要更长的答案陈述、更细致的推理过程和多样化的表达，这使得精细且可解释的自动化评估既至关重要又充满挑战。ROUGE和BERTScore等传统指标由于模型响应和参考答案之间的不同模式，难以捕捉语义相似性。当前基于LLM的评估方法，例如候选答案的成对或列表式比较，缺乏直观的可解释性。虽然对每个响应的点式评分提供了一些描述，但它未能适应不同的问题内容。最值得注意的是，现有方法忽视了事实型和非事实型问题之间的区别。为了解决这些挑战，我们提出了MinosEval，一种新颖的评估方法，它首先区分开放式问题，然后使用不同的评估策略对候选答案进行排名。对于事实型问题，它采用自适应关键点评分策略，而对于非事实型问题，它使用实例感知列表式排名策略。在多个开放式QA数据集（包括为补充社区资源而自建的包含更多候选响应的数据集）上的实验表明，MinosEval与人工标注的对齐度更好，并提供了更具解释性的结果。", "summary": "MinosEval是一种新颖的开放式问答评估方法，旨在解决现有评估工具在处理LLM生成答案时的局限性。它通过首先区分问题是事实型还是非事实型，然后分别应用定制的评估策略（事实型采用自适应关键点评分，非事实型采用实例感知列表式排名）。实验证明，MinosEval比传统方法更能与人类判断对齐，并提供更具解释性的评估结果。", "keywords": "开放式问答, LLM评估, 事实型问题, 非事实型问题, MinosEval", "comments": "MinosEval的创新之处在于其对开放式问答进行了细致的分类评估，解决了以往方法忽视事实型和非事实型问题差异的痛点。这种定制化的评估策略提高了评估的准确性和可解释性，对于更好地理解和提升LLM在复杂问答任务中的表现具有重要意义。"}}
{"id": "2506.15360", "title": "Stochastic Diagonal Estimation Based on Matrix Quadratic Form Oracles", "authors": ["Haishan Ye", "Xiangyu Chang"], "summary": "We study the problem of estimating the diagonal of an implicitly given matrix\n$\\Ab$. For such a matrix we have access to an oracle that allows us to evaluate\nthe matrix quadratic form $ \\ub^\\top \\Ab \\ub$. Based on this query oracle, we\npropose a stochastic diagonal estimation method with random variable $\\ub$\ndrawn from the standard Gaussian distribution. We provide the element-wise and\nnorm-wise sample complexities of the proposed method. Our numerical experiments\non different types and dimensions matrices demonstrate the effectiveness of our\nmethod and validate the tightness of theoretical results.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15360v1", "AI": {"title_translation": "基于矩阵二次型预言机的随机对角线估计", "tldr": "提出一种基于矩阵二次型预言机的随机方法，用于估计隐式给定矩阵的对角线，并提供了理论复杂度和实验验证。", "motivation": "研究估计隐式给定矩阵对角线的问题。", "method": "提出了一种随机对角线估计方法，该方法通过访问一个允许评估矩阵二次型 $\\ub^\\top \\Ab \\ub$ 的预言机，并从标准高斯分布中抽取随机变量 $\\ub$。", "result": "提供了所提出方法的元素级和范数级样本复杂度。数值实验证明了该方法的有效性，并验证了理论结果的紧密性。", "conclusion": "所提出的随机对角线估计方法是有效的，并且其理论结果得到了实验的验证。", "translation": "我们研究了估计隐式给定矩阵 $\\Ab$ 对角线的问题。对于此类矩阵，我们可以访问一个预言机，该预言机允许我们评估矩阵二次型 $\\ub^\\top \\Ab \\ub$。基于此查询预言机，我们提出了一种随机对角线估计方法，其中随机变量 $\\ub$ 从标准高斯分布中抽取。我们提供了所提出方法的元素级和范数级样本复杂度。我们对不同类型和维度矩阵进行的数值实验证明了我们方法的有效性，并验证了理论结果的紧密性。", "summary": "本文研究了隐式给定矩阵对角线的估计问题。作者提出了一种基于矩阵二次型预言机的随机对角线估计方法，该方法通过从标准高斯分布中抽取随机变量来查询预言机。文章提供了该方法的元素级和范数级样本复杂度，并通过数值实验验证了方法的有效性和理论结果的准确性。", "keywords": "随机对角线估计, 矩阵二次型, 预言机, 样本复杂度, 隐式矩阵", "comments": "该论文的创新点在于利用矩阵二次型预言机结合随机高斯变量来估计隐式矩阵的对角线，这对于无法直接访问矩阵元素的情况提供了一种有效方案。"}}
{"id": "2506.15562", "title": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and Efficient Attention", "authors": ["Syed Haider Ali", "Asrar Ahmad", "Muhammad Ali", "Asifullah Khan", "Muhammad Shahban", "Nadeem Shaukat"], "summary": "Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.", "comment": "16 pages, 5 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15562v1", "AI": {"title_translation": "自动化MRI肿瘤分割：基于混合U-Net、Transformer和高效注意力机制", "tldr": "本研究开发了一种结合U-Net、Transformer和高效注意力机制的混合AI模型，用于在本地MRI数据集上进行自动化肿瘤分割，取得了有竞争力的性能，并强调了特定站点模型开发的重要性。", "motivation": "现有的AI分割模型多基于大型公共数据集训练，缺乏本地患者群体的异质性。为了将AI肿瘤分割模型直接集成到医院软件中，实现高效准确的肿瘤治疗计划和执行，需要对本地数据集进行深入研究。", "method": "本研究使用从本地医院获取的MRI数据集，并严格保护隐私。开发了强大的数据管道用于DICOM提取和预处理，并进行了广泛的图像增强，最终用于训练的数据集总计6080张图像。模型架构结合了基于U-Net的卷积神经网络、Transformer瓶颈和多种互补注意力模块（包括高效注意力、Squeeze-and-Excitation (SE) 块、卷积块注意力模块 (CBAM) 和 ResNeXt 块）。为加速收敛和减少计算需求，使用了最大批处理大小为8，并用ImageNet预训练权重初始化编码器，通过检查点在双NVIDIA T4 GPU上进行训练。", "result": "在本地MRI数据集上的定量评估显示，Dice相似系数为0.764，交并比（IoU）为0.736，尽管数据有限，但仍展现出具有竞争力的性能。", "conclusion": "本研究强调了针对临床部署进行特定站点模型开发的重要性，并证明了在有限本地数据下，结合U-Net、Transformer和注意力机制的混合模型在本地MRI肿瘤分割上的有效性和竞争力。", "translation": "癌症是一种异常生长，有可能局部侵犯并转移到远处器官。放射治疗计划优化需要准确地自动分割肿瘤及周围正常组织。最近基于AI的分割模型通常在大型公共数据集上训练，这些数据集缺乏本地患者群体的异质性。虽然这些研究推动了基于AI的医学图像分割，但为了将AI肿瘤分割模型直接开发并集成到医院软件中，以实现高效准确的肿瘤治疗计划和执行，对本地数据集的研究是必要的。本研究利用从当地医院在严格隐私保护下获取的磁共振成像（MRI）数据集，通过计算高效的混合UNet-Transformer模型增强了肿瘤分割。我们开发了一个强大的数据管道，用于无缝的DICOM提取和预处理，随后进行了广泛的图像增强，以确保模型在不同临床环境中的泛化能力，最终用于训练的数据集总计6080张图像。我们新颖的架构集成了基于UNet的卷积神经网络与Transformer瓶颈和互补的注意力模块，包括高效注意力、Squeeze-and-Excitation（SE）块、卷积块注意力模块（CBAM）和ResNeXt块。为了加速收敛并减少计算需求，我们使用了最大批处理大小为8，并用ImageNet预训练权重初始化编码器，通过检查点在双NVIDIA T4 GPU上训练模型，以克服Kaggle的运行时限制。在本地MRI数据集上的定量评估显示，Dice相似系数为0.764，交并比（IoU）为0.736，尽管数据有限，但仍展现出具有竞争力的性能，并强调了针对临床部署进行特定站点模型开发的重要性。", "summary": "本文针对现有AI肿瘤分割模型在本地数据集上泛化能力不足的问题，提出了一种结合U-Net、Transformer和多种注意力机制（如高效注意力、SE、CBAM、ResNeXt）的混合模型，用于本地MRI肿瘤的自动化分割。研究构建了包含6080张图像的本地数据集，并设计了高效的数据处理和训练策略。实验结果显示，该模型在本地MRI数据集上取得了Dice系数0.764和IoU 0.736的性能，证明了在有限本地数据下模型的可行性与竞争力，并强调了为临床部署开发特定站点模型的重要性。", "keywords": "MRI肿瘤分割, 混合U-Net, Transformer, 注意力机制, 本地数据集", "comments": "创新点: 提出了混合U-Net与Transformer的架构，并集成了多种注意力机制，旨在提高分割精度和计算效率。特别关注了在本地、有限数据集上的模型开发，这对于临床实际部署具有重要意义。重要性: 解决了AI医学图像分割模型在实际临床应用中面临的“数据异质性”和“本地化部署”挑战。通过在本地数据集上验证模型性能，为将AI模型直接集成到医院软件提供了实践基础。局限性/改进空间: 尽管在本地数据集上表现出竞争力，但Dice系数0.764和IoU 0.736仍有提升空间。未来可以探索更先进的数据增强技术、更复杂的模型结构或联邦学习等方法来进一步提升性能和泛化能力。"}}
{"id": "2506.15432", "title": "Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters", "authors": ["Guillaume Lomet", "Ruben Salvador", "Brice Colombier", "Vincent Grosso", "Olivier Sentieys", "Cedric Killian"], "summary": "Dataflow neural network accelerators efficiently process AI tasks on FPGAs,\nwith deployment simplified by ready-to-use frameworks and pre-trained models.\nHowever, this convenience makes them vulnerable to malicious actors seeking to\nreverse engineer valuable Intellectual Property (IP) through Side-Channel\nAttacks (SCA). This paper proposes a methodology to recover the hardware\nconfiguration of dataflow accelerators generated with the FINN framework.\nThrough unsupervised dimensionality reduction, we reduce the computational\noverhead compared to the state-of-the-art, enabling lightweight classifiers to\nrecover both folding and quantization parameters. We demonstrate an attack\nphase requiring only 337 ms to recover the hardware parameters with an accuracy\nof more than 95% and 421 ms to fully recover these parameters with an averaging\nof 4 traces for a FINN-based accelerator running a CNN, both using a random\nforest classifier on side-channel traces, even with the accelerator dataflow\nfully loaded. This approach offers a more realistic attack scenario than\nexisting methods, and compared to SoA attacks based on tsfresh, our method\nrequires 940x and 110x less time for preparation and attack phases,\nrespectively, and gives better results even without averaging traces.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 4 figures, 1 algorithm", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15432v1", "AI": {"title_translation": "数据流AI加速器硬件参数的侧信道提取", "tldr": "本文提出一种通过侧信道攻击，利用无监督降维和轻量级分类器，高效恢复基于FINN的数据流AI加速器硬件配置的方法，攻击速度快且准确率高。", "motivation": "数据流神经网络加速器在FPGA上的部署虽然便捷，但其方便性使其易受恶意攻击者的侧信道攻击，从而逆向工程获取有价值的知识产权（IP）。", "method": "本文提出一种通过无监督降维的方法来恢复使用FINN框架生成的数据流加速器的硬件配置。该方法降低了计算开销，并使用轻量级分类器（如随机森林）从侧信道迹线中恢复折叠和量化参数。", "result": "攻击阶段仅需337毫秒即可恢复硬件参数，准确率超过95%；对于运行CNN的FINN加速器，平均4条迹线可在421毫秒内完全恢复参数。与最先进的基于tsfresh的方法相比，该方法在准备阶段快940倍，攻击阶段快110倍，且即使不平均迹线也能取得更好的结果。", "conclusion": "本文提出的侧信道攻击方法提供了一种比现有方法更现实且显著更高效的方式，来恢复数据流AI加速器的硬件参数。", "translation": "数据流神经网络加速器在FPGA上高效处理AI任务，通过即用型框架和预训练模型简化了部署。然而，这种便利性使得它们容易受到恶意行为者的攻击，这些行为者试图通过侧信道攻击（SCA）逆向工程有价值的知识产权（IP）。本文提出了一种恢复使用FINN框架生成的数据流加速器硬件配置的方法。通过无监督降维，我们与最先进的方法相比降低了计算开销，使得轻量级分类器能够恢复折叠和量化参数。我们展示了一个攻击阶段，仅需337毫秒即可恢复硬件参数，准确率超过95%；对于运行CNN的基于FINN的加速器，仅需421毫秒即可完全恢复这些参数（平均4条迹线），两者都使用侧信道迹线上的随机森林分类器，即使在加速器数据流完全加载的情况下也能实现。这种方法比现有方法提供了更现实的攻击场景，与基于tsfresh的最先进攻击相比，我们的方法在准备和攻击阶段分别所需时间减少了940倍和110倍，即使不平均迹线也取得了更好的结果。", "summary": "本文提出了一种针对基于FINN的数据流AI加速器的侧信道攻击方法，旨在高效恢复其硬件参数（折叠和量化）。通过采用无监督降维技术，该方法显著降低了计算开销，使得轻量级分类器（如随机森林）能够以高准确率（>95%）和极快的速度（毫秒级）完成参数恢复。与现有技术相比，该方法在速度和效果上均表现更优，即使在加速器满载情况下也能有效攻击。", "keywords": "侧信道攻击, 数据流AI加速器, 硬件参数, FINN, 无监督降维", "comments": "该论文在针对AI加速器的侧信道攻击领域取得了重要进展，揭示了FPGA部署中一个关键的知识产权安全漏洞。其创新点在于采用了无监督降维技术，极大地提高了此类攻击的效率和实用性。所展示的攻击速度和准确性对基于FPGA的AI系统的IP保护构成了严重挑战。"}}
{"id": "2506.15175", "title": "SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization", "authors": ["Hanjun Kim", "Minwoo Jung", "Wooseong Yang", "Ayoung Kim"], "summary": "Despite the growing adoption of radar in robotics, the majority of research\nhas been confined to homogeneous sensor types, overlooking the integration and\ncross-modality challenges inherent in heterogeneous radar technologies. This\nleads to significant difficulties in generalizing across diverse radar data\ntypes, with modality-aware approaches that could leverage the complementary\nstrengths of heterogeneous radar remaining unexplored. To bridge these gaps, we\npropose SHeRLoc, the first deep network tailored for heterogeneous radar, which\nutilizes RCS polar matching to align multimodal radar data. Our hierarchical\noptimal transport-based feature aggregation method generates rotationally\nrobust multi-scale descriptors. By employing FFT-similarity-based data mining\nand adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric\nlearning. SHeRLoc achieves an order of magnitude improvement in heterogeneous\nradar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public\ndataset and outperforming state of-the-art methods. Also applicable to LiDAR,\nSHeRLoc paves the way for cross-modal place recognition and heterogeneous\nsensor SLAM. The source code will be available upon acceptance.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15175v1", "AI": {"title_translation": "SHeRLoc: 同步异构雷达地点识别用于跨模态定位", "tldr": "SHeRLoc是一个针对异构雷达的深度网络，通过RCS极坐标匹配和分层最优传输特征聚合，显著提升了异构雷达地点识别的性能，并可应用于跨模态定位。", "motivation": "现有研究主要局限于同质传感器类型，忽视了异构雷达技术固有的集成和跨模态挑战，导致难以泛化到不同雷达数据类型，且未探索利用异构雷达互补优势的模态感知方法。", "method": "提出了SHeRLoc，第一个为异构雷达定制的深度网络。它利用RCS极坐标匹配来对齐多模态雷达数据。采用基于分层最优传输的特征聚合方法生成旋转鲁棒的多尺度描述符。通过基于FFT相似性的数据挖掘和自适应边界三元组损失，SHeRLoc实现了FOV感知的度量学习。", "result": "SHeRLoc在异构雷达地点识别方面实现了数量级的改进，在公共数据集上将recall@1从低于0.1提高到0.9，并优于现有最先进的方法。", "conclusion": "SHeRLoc显著提升了异构雷达地点识别的性能，并为跨模态地点识别和异构传感器SLAM铺平了道路。", "translation": "尽管雷达在机器人技术中的应用日益普及，但大多数研究仍局限于同质传感器类型，忽视了异构雷达技术固有的集成和跨模态挑战。这导致在不同雷达数据类型之间进行泛化时面临巨大困难，利用异构雷达互补优势的模态感知方法仍未被探索。为了弥补这些空白，我们提出了SHeRLoc，这是第一个为异构雷达量身定制的深度网络，它利用RCS极坐标匹配来对齐多模态雷达数据。我们基于分层最优传输的特征聚合方法生成了旋转鲁棒的多尺度描述符。通过采用基于FFT相似性的数据挖掘和自适应边界三元组损失，SHeRLoc实现了FOV感知的度量学习。SHeRLoc在异构雷达地点识别方面实现了数量级的改进，在公共数据集上将recall@1从低于0.1提高到0.9，并优于现有最先进的方法。SHeRLoc也适用于激光雷达，为跨模态地点识别和异构传感器SLAM铺平了道路。源代码将在论文接受后提供。", "summary": "该论文提出了SHeRLoc，一个专为异构雷达设计的深度网络，旨在解决现有研究中异构雷达数据集成和跨模态泛化的挑战。SHeRLoc通过RCS极坐标匹配对齐多模态雷达数据，并利用分层最优传输生成旋转鲁棒的多尺度描述符，结合FFT相似性数据挖掘和自适应三元组损失实现FOV感知的度量学习。实验结果表明，SHeRLoc在异构雷达地点识别方面取得了显著的性能提升，并将recall@1从0.1以下提高到0.9，优于现有SOTA方法，并具有在LiDAR和跨模态SLAM中应用的潜力。", "keywords": "异构雷达, 地点识别, 跨模态定位, 深度学习, SLAM", "comments": "SHeRLoc的创新之处在于它是首个专门为异构雷达设计的深度网络，解决了跨模态数据对齐和泛化难题。其采用的RCS极坐标匹配和分层最优传输等方法具有新颖性，显著提升了地点识别的性能，对异构传感器融合和SLAM领域具有重要意义。"}}
{"id": "2506.15497", "title": "Foundation of Affective Computing and Interaction", "authors": ["Changzeng Fu"], "summary": "This book provides a comprehensive exploration of affective computing and\nhuman-computer interaction technologies. It begins with the historical\ndevelopment and basic concepts of human-computer interaction, delving into the\ntechnical frameworks and practical applications of emotional computing, visual\ninteraction, voice interaction, brain-computer interfaces, physiological\nelectrical signal analysis, and social robotics. The book covers a wide range\nof topics, including the psychological and neuroscience foundations of emotion,\nmultimodal emotion recognition, emotional expression mechanisms, and the\nprinciples of brain-computer interfaces.\n  Key technologies such as affective computing based on discrete emotion theory\nand dimensional models, visual perception principles, speech recognition and\nsynthesis, EEG signal acquisition and processing, and multimodal emotion\nrecognition are explained in detail. This book also addresses the technical\nchallenges in the field, including multimodal data fusion, privacy and\nsecurity, and ethical considerations in human-machine relationships. It\ndiscusses the applications of these technologies across various domains such as\neducation, healthcare, entertainment, and intelligent assistance.\n  Looking to the future, the book anticipates trends such as the deep\nintegration of artificial intelligence with emotion recognition, the\nadvancement of multimodal interaction technologies, and the development of more\npersonalized and adaptive emotion recognition systems. It emphasizes the\nimportance of balancing technological innovation with ethical considerations to\nensure the responsible development and application of affective computing\ntechnologies.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15497v1", "AI": {"title_translation": "情感计算与交互的基础", "tldr": "一本全面探讨情感计算与人机交互技术的书籍，涵盖历史、概念、技术框架、应用、挑战及未来趋势。", "motivation": "提供情感计算和人机交互技术的全面探索，涵盖其历史发展、基本概念、技术框架、实际应用、心理学和神经科学基础、关键技术、技术挑战、跨领域应用以及未来趋势，旨在平衡技术创新与伦理考量。", "method": "该书通过详细解释和探讨的方式，涵盖了情感计算和人机交互的各个方面，包括历史发展、基本概念、技术框架、实际应用、心理学和神经科学基础、多模态情感识别、情感表达机制、脑机接口原理、关键技术（如离散情感理论和维度模型的情感计算、视觉感知、语音识别与合成、脑电信号处理）、技术挑战（多模态数据融合、隐私安全、伦理考量）以及跨领域应用。", "result": "该书提供了情感计算和人机交互领域的全面知识体系，包括其历史、基本概念、技术框架、应用、心理学和神经科学基础、多模态情感识别、情感表达机制、脑机接口原理、关键技术（基于离散情感理论和维度模型的情感计算、视觉感知、语音识别与合成、脑电信号采集与处理）、技术挑战（多模态数据融合、隐私与安全、伦理考量）以及在教育、医疗、娱乐和智能辅助等领域的应用。同时，展望了人工智能与情感识别的深度融合、多模态交互技术进步以及更个性化和自适应情感识别系统等未来趋势。", "conclusion": "该书强调了情感计算和人机交互领域在技术发展的同时，需要平衡技术创新与伦理考量，以确保负责任地开发和应用情感计算技术。未来趋势包括人工智能与情感识别的深度融合、多模态交互技术的进步以及更个性化和自适应情感识别系统的发展。", "translation": "这本著作全面探讨了情感计算与人机交互技术。它从人机交互的历史发展和基本概念开始，深入探讨了情感计算、视觉交互、语音交互、脑机接口、生理电信号分析以及社交机器人的技术框架和实际应用。本书涵盖了广泛的主题，包括情感的心理学和神经科学基础、多模态情感识别、情感表达机制以及脑机接口的原理。\n书中详细解释了基于离散情感理论和维度模型的情感计算、视觉感知原理、语音识别与合成、脑电信号采集与处理以及多模态情感识别等关键技术。本书还探讨了该领域的技术挑战，包括多模态数据融合、隐私与安全以及人机关系中的伦理考量。它讨论了这些技术在教育、医疗、娱乐和智能辅助等各个领域的应用。\n展望未来，本书预测了人工智能与情感识别的深度融合、多模态交互技术的进步以及更个性化和自适应情感识别系统等趋势。它强调了平衡技术创新与伦理考量的重要性，以确保情感计算技术的负责任开发和应用。", "summary": "该书全面探讨了情感计算与人机交互技术，内容涵盖其历史发展、基本概念、技术框架、实际应用、心理学和神经科学基础，并详细阐述了多模态情感识别、情感表达机制、脑机接口原理等关键技术，如基于离散情感理论的情感计算、视觉感知、语音识别与合成及脑电信号处理。书中还探讨了多模态数据融合、隐私安全和伦理考量等技术挑战，并讨论了其在教育、医疗、娱乐和智能辅助等领域的应用。最后，该书展望了人工智能与情感识别的深度融合、多模态交互技术进步以及个性化情感识别系统的未来趋势，强调技术创新与伦理平衡的重要性。", "keywords": "情感计算, 人机交互, 多模态情感识别, 脑机接口, 伦理考量", "comments": "这本书作为一本综合性著作，其创新之处在于系统性地整合了情感计算和人机交互的多个方面，从基础理论到前沿技术，再到实际应用和伦理挑战，提供了一个全面的视角。其重要性在于为读者构建了一个完整的知识体系，不仅涵盖了技术细节，还强调了跨学科的心理学和神经科学基础，并前瞻性地讨论了伦理问题，这对于该领域的健康发展至关重要。"}}
{"id": "2506.14919", "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models", "authors": ["Xinkai Zhao", "Yuta Tokuoka", "Junichiro Iwasawa", "Keita Oda"], "summary": "The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14919v1", "AI": {"title_translation": "频率校准的医学图像扩散模型成员推断攻击", "tldr": "本文提出了一种名为频率校准重建误差（FCRE）的新方法，用于对医学图像扩散模型进行成员推断攻击，通过关注中频重建误差来提高攻击性能。", "motivation": "扩散模型在医学图像生成中的应用日益增多，引发了显著的隐私担忧。成员推断攻击（MIA）可用于量化隐私风险，但现有MIA方法直接应用于医学图像时面临挑战，因为重建误差受图像固有难度影响，且扩散模型难以重建高频细节。", "method": "本文提出了一种频率校准重建误差（FCRE）方法。该方法通过关注特定中频范围内的重建误差，并排除高频（难以重建）和低频（信息量较少）区域来减轻图像固有难度的混淆因素。具体来说，分析逆扩散过程，获取中频重建误差，并计算重建图像与原始图像之间的结构相似性指数分数。通过将此分数与阈值比较来确定成员身份。", "result": "在多个医学图像数据集上的实验表明，FCRE方法优于现有MIA方法。", "conclusion": "通过关注中频重建误差，本文提出的频率校准重建误差（FCRE）方法有效解决了现有成员推断攻击在医学图像扩散模型上遇到的挑战，并显著提升了攻击性能。", "translation": "扩散模型在图像生成，特别是在医学成像等敏感领域的日益普及，引发了显著的隐私担忧。成员推断攻击（MIA）已成为一种潜在的方法，用于确定特定图像是否曾用于训练扩散模型，从而量化隐私风险。现有的MIA方法通常依赖于扩散重建误差，其中成员图像的重建误差预计低于非成员图像。然而，将这些方法直接应用于医学图像面临挑战。重建误差受固有图像难度影响，且扩散模型难以重建高频细节。为了解决这些问题，我们提出了一种用于医学图像扩散模型MIA的频率校准重建误差（FCRE）方法。通过关注特定中频范围内的重建误差，并排除高频（难以重建）和低频（信息量较少）区域，我们的频率选择性方法减轻了固有图像难度的混淆因素。具体来说，我们分析逆扩散过程，获取中频重建误差，并计算重建图像与原始图像之间的结构相似性指数分数。通过将此分数与阈值比较来确定成员身份。在多个医学图像数据集上的实验表明，我们的FCRE方法优于现有MIA方法。", "summary": "本文针对医学图像扩散模型中的隐私问题，提出了一种名为频率校准重建误差（FCRE）的新型成员推断攻击（MIA）方法。该方法旨在解决现有MIA在医学图像上因重建误差受图像固有难度及高频细节重建困难影响而面临的挑战。FCRE通过专注于图像的中频重建误差，排除高频和低频区域，并利用结构相似性指数分数来判断图像是否为训练集成员。实验证明，FCRE在多个医学图像数据集上的性能优于现有MIA方法，有效量化了扩散模型的隐私风险。", "keywords": "成员推断攻击, 扩散模型, 医学图像, 隐私, 频率校准", "comments": "本文的创新点在于提出了频率校准重建误差（FCRE）方法，通过引入频率选择性分析来解决现有成员推断攻击在医学图像领域面临的特有挑战，即图像固有难度和高频细节重建问题。这种方法通过聚焦于信息量更大且受干扰较小的中频区域，提高了攻击的准确性和鲁棒性，对于评估和量化医学图像扩散模型的隐私风险具有重要意义。"}}
{"id": "2506.14813", "title": "Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks", "authors": ["Yuxuan Jiang", "Ziming Zhou", "Boyu Xu", "Beijie Liu", "Runhui Xu", "Peng Huang"], "summary": "Training deep learning (DL) models is a complex process, making it prone to\nsilent errors that are challenging to detect and diagnose. This paper presents\nTRAINCHECK, a framework that takes a proactive checking approach to address\nsilent training errors. TRAINCHECK automatically infers invariants tailored for\nDL training. It uses these invariants to proactively detect silent errors\nduring the training process while providing debugging help. To evaluate\nTRAINCHECK, we reproduce 20 real-world silent training errors with diverse root\ncauses. TRAINCHECK successfully detects 18 errors within a single training\niteration. It also uncovers 6 unknown bugs in popular training libraries that\nlead to silent errors.", "comment": "19 pages, to appear in 19th USENIX Symposium on Operating Systems\n  Design and Implementation (OSDI '25)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14813v1", "AI": {"title_translation": "带着信心训练：通过自动化主动检查捕获深度学习训练中的无声错误", "tldr": "TRAINCHECK是一个框架，通过推断和使用不变量，主动检测深度学习训练中的无声错误，能在单次迭代中捕获大多数错误，并发现了流行训练库中的新错误。", "motivation": "深度学习模型训练是一个复杂的过程，容易出现难以检测和诊断的无声错误，因此需要一种主动的检查方法来解决这些问题。", "method": "本文提出了TRAINCHECK框架，它采用主动检查方法来解决无声训练错误。TRAINCHECK自动推断出针对深度学习训练的不变量，并利用这些不变量在训练过程中主动检测无声错误，同时提供调试帮助。", "result": "TRAINCHECK在单次训练迭代中成功检测了20个真实世界无声训练错误中的18个。它还发现了流行训练库中导致无声错误的6个未知错误。", "conclusion": "TRAINCHECK能够有效且主动地检测深度学习训练中的无声错误，并有助于发现训练库中的新缺陷。", "translation": "训练深度学习（DL）模型是一个复杂的过程，容易产生难以检测和诊断的无声错误。本文提出了TRAINCHECK，一个采用主动检查方法来解决无声训练错误的框架。TRAINCHECK自动推断出针对深度学习训练的定制不变量。它利用这些不变量在训练过程中主动检测无声错误，同时提供调试帮助。为了评估TRAINCHECK，我们重现了20个具有不同根本原因的真实世界无声训练错误。TRAINCHECK在单次训练迭代中成功检测了18个错误。它还发现了流行训练库中导致无声错误的6个未知错误。", "summary": "TRAINCHECK是一个旨在解决深度学习训练中无声错误的框架。它通过自动推断和利用针对DL训练的不变量，在训练过程中主动检测这些难以发现的错误，并提供调试支持。实验评估表明，TRAINCHECK在重现的20个真实世界错误中成功检测了18个，且仅在单次训练迭代内完成，同时还发现了流行训练库中的6个先前未知的错误。", "keywords": "深度学习, 无声错误, 主动检查, 不变量, 错误检测", "comments": "TRAINCHECK的创新之处在于其主动的、基于不变量的方法，解决了深度学习训练中一个关键且常被忽视的无声错误问题，显著提高了训练的可靠性和效率。"}}
{"id": "2506.15239", "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15239v1", "AI": {"title_translation": "迷失在变体中？评估巴斯克语和西班牙语地理变体中的NLI性能", "tldr": "本研究评估了当前语言技术理解巴斯克语和西班牙语方言的能力，发现LLMs在处理语言变体时性能下降，尤其是在巴斯克语中，且这种下降与语言变体本身而非词汇重叠有关，编码器模型在巴斯克西部方言上表现尤为挣扎。", "motivation": "评估当前语言技术理解巴斯克语和西班牙语语言变体的能力。", "method": "使用自然语言推理（NLI）作为核心任务，引入了一个新颖的手动整理的巴斯克语和西班牙语及其各自变体的并行数据集。通过使用仅编码器和基于解码器的大型语言模型（LLMs）进行跨语言和上下文学习实验的实证分析。", "result": "处理语言变体时性能下降，尤其是在巴斯克语中。错误分析表明这种下降不是由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在巴斯克西部方言上表现特别挣扎，这与将外围方言（如西部方言）识别为与标准语更远的语言理论一致。", "conclusion": "当前的大型语言模型在处理语言变体时（特别是巴斯克语）会遇到性能下降，且这种下降与语言变体本身有关。仅编码器模型在某些特定方言（如巴斯克西部方言）上的表现尤为困难。", "translation": "本文评估了当前语言技术理解巴斯克语和西班牙语语言变体的能力。我们使用自然语言推理（NLI）作为核心任务，并引入了一个新颖的、手动整理的巴斯克语和西班牙语及其各自变体的并行数据集。我们对使用仅编码器和基于解码器的大型语言模型（LLMs）进行的跨语言和上下文学习实验的实证分析表明，在处理语言变体时性能下降，尤其是在巴斯克语中。错误分析表明，这种下降并非由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在巴斯克西部方言上表现特别挣扎，这与将外围方言（例如西部方言）识别为与标准语更远的语言理论一致。所有数据和代码均已公开提供。", "summary": "本研究旨在评估当前语言技术（特别是LLMs）理解巴斯克语和西班牙语语言变体的能力。通过构建一个新的并行数据集并利用NLI任务，研究发现LLMs在处理这些语言变体时性能显著下降，尤其是在巴斯克语中。分析表明，这种性能下降是由于语言变体本身而非词汇重叠。此外，仅编码器模型在处理巴斯克西部方言时表现出更大的困难，这与语言学理论相符。", "keywords": "自然语言推理, 语言变体, 巴斯克语, 西班牙语, 大型语言模型", "comments": "这项研究通过引入一个新颖的手动整理的语言变体数据集，填补了LLMs在处理方言和地理变体方面评估的空白。其创新之处在于使用了NLI作为评估任务，并深入分析了性能下降的原因，指出了现有LLMs在语言多样性理解方面的局限性，特别是编码器模型对特定方言的弱点。这对于未来LLM的开发和方言适应性研究具有重要意义。"}}
{"id": "2506.15481", "title": "A deep shotgun method for solving high-dimensional parabolic partial differential equations", "authors": ["Wenjun Xu", "Wenzhong Zhang"], "summary": "Recent advances in deep learning makes solving parabolic partial differential\nequations (PDEs) in high dimensional spaces possible via forward-backward\nstochastic differential equation (FBSDE) formulations. The implementation of\nmost existing methods requires simulating multiple trajectories of stochastic\nprocesses with a small step size of time discretization to ensure accuracy,\nhence having limited performance, especially when solving on a large time\ninterval. To address such issue, we propose a deep \"shotgun method\" that does\nnot exploit full trajectories, but only utilizes the data distribution of them.\nNumerical results including examples with dimensionality up to 10000\ndemonstrate the competitiveness of the proposed shotgun method in both\nperformance and accuracy.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15481v1", "AI": {"title_translation": "求解高维抛物型偏微分方程的深度“散弹法”", "tldr": "提出一种深度“散弹法”来高效准确地求解高维抛物型PDE，克服了现有方法在处理长时域问题时的性能限制。", "motivation": "现有深度学习方法求解高维抛物型PDE时，需要模拟大量随机过程轨迹且时间步长小，导致性能受限，尤其是在大时间区间上。", "method": "提出一种深度“散弹法”（deep “shotgun method”），该方法不利用完整的轨迹，而是只利用轨迹的数据分布。", "result": "数值结果表明，所提出的散弹法在高达10000维的示例中，在性能和准确性方面都具有竞争力。", "conclusion": "深度“散弹法”是一种有效且高效的求解高维抛物型偏微分方程的方法，通过利用数据分布而非完整轨迹来提高性能和准确性。", "translation": "深度学习的最新进展使得通过前向-后向随机微分方程（FBSDE）公式求解高维空间中的抛物型偏微分方程（PDEs）成为可能。大多数现有方法的实现需要模拟多个随机过程轨迹，且时间离散化步长较小以确保准确性，因此性能有限，尤其是在较大的时间区间上求解时。为了解决这个问题，我们提出了一种深度“散弹法”，它不利用完整的轨迹，而只利用它们的数据分布。数值结果，包括维度高达10000的示例，证明了所提出的散弹法在性能和准确性方面的竞争力。", "summary": "这篇论文提出了一种名为“深度散弹法”的新方法，用于高效准确地求解高维抛物型偏微分方程。与现有方法需要模拟完整随机过程轨迹不同，该方法仅利用轨迹的数据分布，从而克服了现有方法在处理大时间区间问题时的性能限制。数值实验在高达10000维的问题上验证了该方法在性能和准确性上的优越性。", "keywords": "高维PDEs, 深度学习, 散弹法, FBSDE", "comments": "该论文的创新点在于提出了“散弹法”，通过不利用完整轨迹而只利用数据分布来解决高维PDE求解中现有方法的性能瓶颈。这对于处理复杂且计算量大的高维问题具有重要意义，尤其是在需要在大时间间隔上进行求解时。"}}
{"id": "2506.15547", "title": "An efficient construction of Raz's two-source randomness extractor with improved parameters", "authors": ["Cameron Foreman", "Lewis Wooltorton", "Kevin Milner", "Florian J. Curchod"], "summary": "Randomness extractors are algorithms that distill weak random sources into\nnear-perfect random numbers. Two-source extractors enable this distillation\nprocess by combining two independent weak random sources. Raz's extractor (STOC\n'05) was the first to achieve this in a setting where one source has linear\nmin-entropy (i.e., proportional to its length), while the other has only\nlogarithmic min-entropy in its length. However, Raz's original construction is\nimpractical due to a polynomial computation time of at least degree 4. Our work\nsolves this problem by presenting an improved version of Raz's extractor with\nquasi-linear computation time, as well as a new analytic theorem with reduced\nentropy requirements. We provide comprehensive analytical and numerical\ncomparisons of our construction with others in the literature, and we derive\nstrong and quantum-proof versions of our efficient Raz extractor. Additionally,\nwe offer an easy-to-use, open-source code implementation of the extractor and a\nnumerical parameter calculation module.", "comment": "12 + 11 pages. Comments welcome!", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15547v1", "AI": {"title_translation": "Raz双源随机性提取器的一种高效构造及参数改进", "tldr": "本文提出了一种改进的Raz双源随机性提取器，显著降低了计算时间并优化了熵要求，使其更具实用性。", "motivation": "Raz的双源随机性提取器是首个在其中一个源具有线性最小熵而另一个源仅具有对数最小熵的情况下实现提取的算法，但其原始构造的计算时间复杂度至少为4次多项式，实用性差。", "method": "本文提出了一种改进的Raz提取器，实现了准线性计算时间。同时，还提出了一种新的分析定理，降低了熵要求。研究者还提供了与现有文献中其他构造的全面分析和数值比较，并推导了其高效Raz提取器的强版本和抗量子版本。此外，还提供了易于使用的开源代码实现和数值参数计算模块。", "result": "改进后的Raz提取器实现了准线性计算时间，并且新的分析定理降低了熵要求。", "conclusion": "本文成功构建了一种高效且参数改进的Raz双源随机性提取器，解决了原始构造计算时间过长的问题，并通过提供开源实现提高了其实用性。", "translation": "随机性提取器是将弱随机源提炼成近乎完美的随机数的算法。双源提取器通过结合两个独立的弱随机源来实现这种提炼过程。Raz的提取器（STOC '05）是第一个在其中一个源具有线性最小熵（即与其长度成比例），而另一个源仅具有对数最小熵的情况下实现这一目标的。然而，Raz的原始构造由于至少4次多项式的计算时间而变得不切实际。我们的工作通过提出Raz提取器的一个改进版本来解决这个问题，该版本具有准线性计算时间，以及一个具有降低熵要求的新分析定理。我们提供了我们的构造与文献中其他构造的全面分析和数值比较，并且我们推导了我们高效Raz提取器的强版本和抗量子版本。此外，我们还提供了提取器的易于使用、开源代码实现和数值参数计算模块。", "summary": "本文针对Raz双源随机性提取器原始构造计算复杂度高的问题，提出了一种改进的Raz提取器。新构造实现了准线性计算时间，并通过新的分析定理降低了熵要求。研究还包括了与现有方法的比较、强版本和抗量子版本的推导，并提供了开源代码实现，显著提升了Raz提取器的实用性。", "keywords": "随机性提取器, Raz提取器, 双源提取器, 计算效率, 熵要求", "comments": "本文的主要创新在于将Raz双源随机性提取器的计算复杂度从多项式时间（至少4次）降低到准线性时间，这极大地提高了其实用性。此外，新的分析定理降低了熵要求，进一步优化了提取器的性能。提供开源代码实现也体现了其工程实践价值。"}}
{"id": "2506.15249", "title": "Context-Aware Deep Lagrangian Networks for Model Predictive Control", "authors": ["Lucas Schulze", "Jan Peters", "Oleg Arenz"], "summary": "Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15249v1", "AI": {"title_translation": "上下文感知深度拉格朗日网络在模型预测控制中的应用", "tldr": "本文提出了一种结合在线系统辨识和模型预测控制的上下文感知深度拉格朗日网络（DeLaN），用于在复杂环境中实现自适应、物理信息控制，并在机器人轨迹跟踪任务中显著提高了精度。", "motivation": "在复杂环境中，基于物理信息的机器人控制模型（如深度拉格朗日网络DeLaN）难以应对大量不确定物理属性的对象，导致单一全局模型不可行。因此，需要在线识别仅捕获当前相关环境方面的上下文感知模型，同时保持物理合理性，尤其是在模型预测控制（MPC）等方法中。", "method": "本文将深度拉格朗日网络（DeLaN）扩展为上下文感知型，并将其与循环网络结合进行在线系统辨识，然后集成到模型预测控制（MPC）中以实现自适应、物理信息控制。此外，还将DeLaN与残差动力学模型结合，利用机器人标称模型的可用性。", "result": "在7自由度机械臂的变负载轨迹跟踪任务中，本文方法使末端执行器跟踪误差降低了39%，而使用扩展卡尔曼滤波的基线方法仅提高了21%。", "conclusion": "本文成功开发并验证了一种上下文感知深度拉格朗日网络与在线系统辨识和模型预测控制相结合的方法，有效解决了复杂环境下机器人控制中的模型适应性问题，显著提高了控制精度。", "translation": "基于物理信息动力学模型（如深度拉格朗日网络，DeLaN）控制机器人可以提高所得行为的泛化能力和可解释性。然而，在复杂环境中，可能交互的物体数量庞大，且其物理属性通常不确定。这种复杂性使得采用单一全局模型变得不可行。因此，我们需要诉诸于在线系统辨识上下文感知模型，这些模型仅捕获环境中当前相关的方面。虽然能量守恒等物理原理在不同上下文中可能不成立，但确保任何单个上下文感知模型的物理合理性仍然是非常可取的，尤其是在使用它进行诸如模型预测控制（MPC）等滚动视界控制方法时。因此，在这项工作中，我们扩展了DeLaN使其具有上下文感知能力，将其与循环网络结合进行在线系统辨识，并将其与MPC集成以实现自适应、物理信息控制。我们还将DeLaN与残差动力学模型结合，以利用机器人标称模型通常可用的事实。我们在7自由度机械臂上评估了我们的方法在变负载下的轨迹跟踪性能。与使用扩展卡尔曼滤波的基线方法相比，我们的方法将末端执行器跟踪误差降低了39%，而基线方法仅提高了21%。", "summary": "本文提出了一种上下文感知深度拉格朗日网络（DeLaN），旨在解决复杂环境中机器人控制中单一全局模型不可行的问题。该方法结合了在线系统辨识的循环网络和模型预测控制（MPC），以实现自适应、物理信息控制。为进一步提升性能，还将DeLaN与残差动力学模型结合。实验结果表明，在7自由度机械臂的轨迹跟踪任务中，该方法将末端执行器跟踪误差显著降低了39%，优于基线方法。", "keywords": "深度拉格朗日网络, 模型预测控制, 上下文感知, 在线系统辨识, 机器人控制", "comments": "该论文提出了一种新颖的方法，通过将深度拉格朗日网络扩展为上下文感知型，并结合在线系统辨识和模型预测控制，有效解决了复杂环境下机器人控制模型的适应性问题。其创新点在于将物理信息模型与数据驱动的在线适应性相结合，同时利用了残差动力学模型。实验结果显示出显著的性能提升，表明该方法在提高机器人控制的泛化性和精度方面具有重要意义。"}}
{"id": "2506.15512", "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach", "authors": ["Wenqi Guan", "Yang Fang"], "summary": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15512v1", "AI": {"title_translation": "基于LangChain中GPT集成的CoT增强提示工程方法优化网络AI查询检索", "tldr": "该研究提出一种在LangChain框架内集成GPT模型并结合CoT推理和提示工程的方法，以优化远程学习资源检索，提高检索结果的准确性和相关性，从而提升用户满意度和学习成果。", "motivation": "当前远程学习资源检索在上下文深度上有所欠缺，无法为复杂的学生查询提供全面的信息。大型语言模型虽然带来了变革，但仍需改进以提供更精准和相关的检索结果。", "method": "本研究提出一种新颖的方法，通过在LangChain框架内集成基于GPT的模型，并结合CoT（思维链）推理和提示工程来实现。该框架旨在提高检索结果的精度和相关性，以返回全面且上下文丰富的解释和资源。", "result": "研究评估了该方法与现有范式LLM的有效性，并报告了用户满意度和学习成果的提升。", "conclusion": "通过将GPT模型与LangChain框架结合，并利用CoT推理和提示工程，可以显著优化网络AI查询检索，从而为远程学习学生提供更准确、更相关的资源，提高学习效果和满意度。", "translation": "大型语言模型在远程学习学生等教育活动方面带来了根本性变革。当前远程学习资源的检索缺乏上下文深度，无法为复杂的学生查询提供全面的信息。本研究提出一种新颖的方法，通过在LangChain框架内集成基于GPT的模型来增强远程学习检索。我们利用CoT推理和提示工程，以更直观和高效的方式实现该系统。我们提出的框架非常注重提高检索结果的精度和相关性，以返回最适合每个学生需求的全面且上下文丰富的解释和资源。我们还评估了我们方法与范式LLM的有效性，并报告了用户满意度和学习成果的提升。", "summary": "本研究提出一种基于LangChain框架的新型GPT模型集成方法，结合CoT推理和提示工程，以优化远程学习资源的AI查询检索。旨在解决现有检索系统上下文深度不足的问题，通过提高检索结果的精度和相关性，提供更全面和个性化的学习资源，从而提升学生满意度和学习成果。", "keywords": "GPT集成, LangChain, CoT推理, 提示工程, 远程学习检索", "comments": "该论文的创新点在于将GPT模型与LangChain框架相结合，并特别强调了CoT推理和提示工程在优化检索效果中的作用。这为提升远程学习资源的个性化和准确性提供了一条新颖的路径，对教育技术领域具有重要意义。"}}
{"id": "2506.14934", "title": "Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "Arian Rahman Aditta", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "summary": "Distinguishing between quark- and gluon-initiated jets is a critical and\nchallenging task in high-energy physics, pivotal for improving new physics\nsearches and precision measurements at the Large Hadron Collider. While deep\nlearning, particularly Convolutional Neural Networks (CNNs), has advanced jet\ntagging using image-based representations, the potential of Vision Transformer\n(ViT) architectures, renowned for modeling global contextual information,\nremains largely underexplored for direct calorimeter image analysis, especially\nunder realistic detector and pileup conditions. This paper presents a\nsystematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet\nclassification using simulated 2012 CMS Open Data. We construct multi-channel\njet-view images from detector-level energy deposits (ECAL, HCAL) and\nreconstructed tracks, enabling an end-to-end learning approach. Our\ncomprehensive benchmarking demonstrates that ViT-based models, notably\nViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN\nbaselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of\ncapturing long-range spatial correlations within jet substructure. This work\nestablishes the first systematic framework and robust performance baselines for\napplying ViT architectures to calorimeter image-based jet classification using\npublic collider data, alongside a structured dataset suitable for further deep\nlearning research in this domain.", "comment": "Accepted in Third International Workshop on Generalizing from Limited\n  Resources in the Open World Workshop at International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14934v1", "AI": {"title_translation": "视觉Transformer用于量能器图像的端到端夸克-胶子射流分类", "tldr": "本文系统评估了Vision Transformer及其混合模型在夸克-胶子射流分类中的应用，并证明其性能优于传统CNN模型。", "motivation": "区分夸克和胶子引起的射流是高能物理中一项关键且具挑战性的任务，对改进新物理搜索和精确测量至关重要。尽管CNN在喷注标记方面取得了进展，但Vision Transformer（ViT）架构在直接量能器图像分析中的潜力（尤其是在真实探测器和堆积条件下）仍未充分探索。", "method": "本文系统评估了ViT和ViT-CNN混合模型在夸克-胶子射流分类中的应用。研究使用模拟的2012年CMS开放数据，从探测器级别的能量沉积（ECAL、HCAL）和重建轨迹构建多通道射流视图图像，实现了端到端学习方法。", "result": "基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和准确性方面始终优于已建立的CNN基线，这突出了捕获射流亚结构中长程空间关联的优势。", "conclusion": "本工作建立了第一个将ViT架构应用于基于量能器图像的射流分类的系统框架和鲁棒性能基线，并提供了适合该领域进一步深度学习研究的结构化数据集。", "translation": "区分夸克和胶子引起的射流是高能物理中一项关键且具有挑战性的任务，对改进大型强子对撞机的新物理搜索和精确测量至关重要。尽管深度学习，特别是卷积神经网络（CNN），通过基于图像的表示方法推进了射流标记，但Vision Transformer（ViT）架构（以其建模全局上下文信息而闻名）在直接量能器图像分析中的潜力仍未得到充分探索，尤其是在真实的探测器和堆积条件下。本文系统评估了ViT和ViT-CNN混合模型在夸克-胶子射流分类中的应用，使用了模拟的2012年CMS开放数据。我们从探测器级别的能量沉积（ECAL、HCAL）和重建轨迹构建多通道射流视图图像，从而实现端到端学习方法。我们全面的基准测试表明，基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和准确性方面始终优于已建立的CNN基线，这突出了捕获射流亚结构中长程空间关联的优势。这项工作建立了第一个系统框架和鲁棒的性能基线，用于将ViT架构应用于使用公共对撞机数据的基于量能器图像的射流分类，同时还提供了适合该领域进一步深度学习研究的结构化数据集。", "summary": "本文探讨了Vision Transformer（ViT）及其混合模型在区分夸克和胶子射流方面的应用，这是一项高能物理中的关键任务。研究利用模拟的CMS开放数据，从量能器图像构建多通道输入，并采用端到端学习方法。实验结果表明，ViT基模型（尤其是ViT+MaxViT和ViT+ConvNeXt混合）在F1分数、ROC-AUC和准确性上均优于传统CNN，证明了ViT捕获长程空间关联的优势。这项工作为使用公共对撞机数据将ViT应用于量能器图像射流分类建立了系统框架和性能基线。", "keywords": "夸克-胶子射流分类, Vision Transformer, 量能器图像, 深度学习, 高能物理", "comments": "这项工作创新性地将Vision Transformer架构引入到高能物理领域的夸克-胶子射流分类任务中，填补了ViT在该领域直接量能器图像分析应用的空白。其重要性在于证明了ViT在处理复杂物理数据中的优越性，尤其是在捕获长程空间关联方面，为未来的新物理搜索和精确测量提供了更强大的工具。此外，提供结构化数据集也促进了该领域的进一步研究。"}}
{"id": "2506.14815", "title": "Predicting Anthropometric Body Composition Variables Using 3D Optical Imaging and Machine Learning", "authors": ["Gyaneshwar Agrahari", "Kiran Bist", "Monika Pandey", "Jacob Kapita", "Zachary James", "Jackson Knox", "Steven Heymsfield", "Sophia Ramirez", "Peter Wolenski", "Nadejda Drenska"], "summary": "Accurate prediction of anthropometric body composition variables, such as\nAppendicular Lean Mass (ALM), Body Fat Percentage (BFP), and Bone Mineral\nDensity (BMD), is essential for early diagnosis of several chronic diseases.\nCurrently, researchers rely on Dual-Energy X-ray Absorptiometry (DXA) scans to\nmeasure these metrics; however, DXA scans are costly and time-consuming. This\nwork proposes an alternative to DXA scans by applying statistical and machine\nlearning models on biomarkers (height, volume, left calf circumference, etc)\nobtained from 3D optical images. The dataset consists of 847 patients and was\nsourced from Pennington Biomedical Research Center. Extracting patients' data\nin healthcare faces many technical challenges and legal restrictions. However,\nmost supervised machine learning algorithms are inherently data-intensive,\nrequiring a large amount of training data. To overcome these limitations, we\nimplemented a semi-supervised model, the $p$-Laplacian regression model. This\npaper is the first to demonstrate the application of a $p$-Laplacian model for\nregression. Our $p$-Laplacian model yielded errors of $\\sim13\\%$ for ALM,\n$\\sim10\\%$ for BMD, and $\\sim20\\%$ for BFP when the training data accounted for\n10 percent of all data. Among the supervised algorithms we implemented, Support\nVector Regression (SVR) performed the best for ALM and BMD, yielding errors of\n$\\sim 8\\%$ for both, while Least Squares SVR performed the best for BFP with\n$\\sim 11\\%$ error when trained on 80 percent of the data. Our findings position\nthe $p$-Laplacian model as a promising tool for healthcare applications,\nparticularly in a data-constrained environment.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14815v1", "AI": {"title_translation": "使用3D光学成像和机器学习预测人体测量身体成分变量", "tldr": "本文提出了一种基于3D光学图像和机器学习模型（包括首次应用于回归的p-Laplacian模型）来预测人体身体成分变量的方法，以替代昂贵耗时的DXA扫描，尤其适用于数据受限环境。", "motivation": "当前，准确测量人体身体成分变量（如ALM、BFP、BMD）对于早期诊断多种慢性病至关重要，但目前依赖的DXA扫描成本高昂且耗时。此外，医疗数据获取面临技术挑战和法律限制，导致难以获得大量训练数据。", "method": "本研究提出了一种替代DXA扫描的方法，即对从3D光学图像中获得的生物标志物（如身高、体积、左小腿围等）应用统计和机器学习模型。数据集包含847名患者的数据。为了克服数据量限制，研究实现了一种半监督模型——p-Laplacian回归模型，这是首次将p-Laplacian模型应用于回归问题。此外，还实施了多种监督算法，包括支持向量回归（SVR）和最小二乘SVR。", "result": "当训练数据占总数据的10%时，p-Laplacian模型在ALM上的误差约为13%，BMD约为10%，BFP约为20%。在所实现的监督算法中，当使用80%的数据进行训练时，支持向量回归（SVR）在ALM和BMD上的表现最佳，误差均约为8%；而最小二乘SVR在BFP上的表现最佳，误差约为11%。", "conclusion": "本研究结果表明，p-Laplacian模型是一种有前景的医疗保健应用工具，特别是在数据受限的环境中。", "translation": "准确预测人体测量身体成分变量，例如肢体瘦体重（ALM）、体脂百分比（BFP）和骨矿物质密度（BMD），对于多种慢性疾病的早期诊断至关重要。目前，研究人员依赖双能X射线吸收法（DXA）扫描来测量这些指标；然而，DXA扫描成本高昂且耗时。本工作提出了一种替代DXA扫描的方法，即对从3D光学图像中获得的生物标志物（身高、体积、左小腿围等）应用统计和机器学习模型。数据集包含847名患者，来源于彭宁顿生物医学研究中心。在医疗保健领域提取患者数据面临许多技术挑战和法律限制。然而，大多数监督机器学习算法本质上是数据密集型的，需要大量的训练数据。为了克服这些限制，我们实现了一种半监督模型——p-Laplacian回归模型。本文首次展示了p-Laplacian模型在回归中的应用。当训练数据占总数据的10%时，我们的p-Laplacian模型在ALM上的误差约为13%，BMD约为10%，BFP约为20%。在我们实现的监督算法中，支持向量回归（SVR）在ALM和BMD上的表现最佳，当使用80%的数据进行训练时，两者的误差均约为8%，而最小二乘SVR在BFP上的表现最佳，误差约为11%。我们的研究结果表明，p-Laplacian模型是一种有前景的医疗保健应用工具，特别是在数据受限的环境中。", "summary": "该研究旨在解决传统DXA扫描测量人体成分变量（如ALM、BFP、BMD）成本高昂和数据获取受限的问题。作者提出利用3D光学图像提取的生物标志物，结合统计和机器学习模型进行预测。为应对数据稀疏性挑战，研究首次将半监督p-Laplacian回归模型应用于此领域，并在少量训练数据下取得了可接受的预测误差。同时，也评估了多种监督学习模型，其中SVR表现出色。研究强调了p-Laplacian模型在数据受限医疗环境中的应用潜力。", "keywords": "3D光学成像, 身体成分预测, 机器学习, p-Laplacian回归, 半监督学习", "comments": "本文的创新点在于首次将p-Laplacian模型应用于回归问题，并展示了其在数据受限环境下的有效性，这对于医疗保健领域的数据稀疏问题具有重要意义。该方法提供了一种成本效益高、非侵入性的DXA替代方案。局限性可能在于3D光学图像的标准化和生物标志物提取的准确性，以及模型在更广泛人群中的泛化能力。"}}
{"id": "2506.15241", "title": "Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs", "authors": ["Yang Fan", "Zhang Qi", "Xing Wenqian", "Liu Chang", "Liu Liu"], "summary": "This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15241v1", "AI": {"title_translation": "基于历史文本知识图谱的图检索增强生成研究", "tldr": "提出Graph RAG框架，通过结合知识图谱和检索增强生成，解决大语言模型在历史文本分析中的领域知识缺失和幻觉问题，并在关系抽取任务上取得显著效果。", "motivation": "解决通用大语言模型在计算人文和AIGC技术背景下进行历史文本分析时存在的领域知识空白问题。", "method": "提出Graph RAG框架，结合思维链提示、自指令生成和过程监督，以最少的人工标注创建了《史记》人物关系数据集，支持自动化历史知识抽取。在图增强生成阶段，引入知识图谱与检索增强生成之间的协作机制，以提高通用模型与历史知识的对齐。", "result": "领域特定模型Xunzi-Qwen1.5-14B在简体中文输入和思维链提示下，关系抽取达到最佳性能（F1 = 0.68）。集成GraphRAG的DeepSeek模型在开放域C-CLUE关系抽取数据集上的F1值提升了11%（0.08-0.19），超过Xunzi-Qwen1.5-14B的F1值（0.12），有效缓解了幻觉现象，并提高了可解释性。", "conclusion": "该框架为经典文本知识抽取提供了一种低资源解决方案，推动了历史知识服务和人文学科研究。", "translation": "本文旨在解决计算人文学科和AIGC技术背景下，通用大型语言模型在历史文本分析中存在的领域知识空白问题。我们提出了Graph RAG框架，结合思维链提示、自指令生成和过程监督，以最少的人工标注创建了《史记》人物关系数据集。该数据集支持自动化历史知识抽取，降低了人工成本。在图增强生成阶段，我们引入了知识图谱与检索增强生成之间的协作机制，提高了通用模型与历史知识的对齐。实验表明，领域特定模型Xunzi-Qwen1.5-14B在简体中文输入和思维链提示下，关系抽取达到了最佳性能（F1 = 0.68）。集成GraphRAG的DeepSeek模型在开放域C-CLUE关系抽取数据集上的F1值提升了11%（0.08-0.19），超过了Xunzi-Qwen1.5-14B的F1值（0.12），有效缓解了幻觉现象，并提高了可解释性。该框架为经典文本知识抽取提供了一种低资源解决方案，推动了历史知识服务和人文学科研究。", "summary": "本文针对通用大语言模型在历史文本分析中的领域知识缺失问题，提出了Graph RAG框架。该框架通过结合思维链提示、自指令生成和过程监督，构建了《史记》人物关系数据集，并引入知识图谱与检索增强生成（RAG）的协作机制，以提升模型与历史知识的对齐度。实验结果表明，Graph RAG有效提高了关系抽取性能，缓解了模型幻觉，并增强了可解释性，为经典文本知识抽取提供了低资源解决方案。", "keywords": "知识图谱, 检索增强生成, 历史文本, 关系抽取, 大语言模型", "comments": "这项研究创新性地将知识图谱与检索增强生成（RAG）相结合，以解决大语言模型在特定领域（历史文本）的知识空白和幻觉问题。其通过低资源方式构建领域数据集并提升模型性能，对于计算人文和历史知识服务具有重要意义。"}}
{"id": "2506.15541", "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity", "authors": ["Oluwadamilola Fasina", "Ruben V. C. Pohle", "Pei-Chun Su", "Ronald R. Coifman"], "summary": "We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.", "comment": "16 pages, 6 figures, 2 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15541v1", "AI": {"title_translation": "内部和外部组织注意力：Softmax不变性和网络稀疏性", "tldr": "本文深入探讨了Transformer中自注意力机制的内部和外部结构，理论证明了自注意力对Softmax激活的不变性，并通过构建分层张量组织来研究网络稀疏性，为模型可解释性、剪枝和架构比较提供了新方法。", "motivation": "本文旨在探究Transformer中自注意力机制的内部（注意力头内部）和外部（注意力头之间）结构，并理解其对Softmax激活的理论不变性以及如何利用其组织结构进行网络分析。", "method": "研究方法包括：1) 运用超微分演算（paradifferential calculus）为自注意力机制对Softmax激活的不变性提供理论依据，并通过计算实例支持。2) 利用现有张量分层组织方法，针对网络三维张量（查询、键和头轴）构建分层分区树来检查网络结构。3) 通过可视化注意力头和扩散图嵌入的分层组织进行定性说明。4) 通过在查询、键和头空间上分别使用双哈尔基和三哈尔基的扩展系数，定量研究网络稀疏性。5) 使用视觉和语言Transformer提供计算示例以展示理论和方法的实用性。", "result": "主要结果包括：1) 通过超微分演算获得了自注意力机制对Softmax激活不变性的理论证据。2) 发现通过分层组织网络三维张量，可以在规则的几何结构上有效地执行常见的信号处理任务。3) 定性和定量地展示了注意力头的分层组织和网络稀疏性。4) 证明了这些发现对于可解释性分析、模型剪枝和网络架构比较等下游任务具有实际应用价值。", "conclusion": "本文的结论是，深入理解自注意力机制的内部和外部组织结构及其对Softmax激活的不变性，不仅为Transformer的可解释性分析提供了理论基础，而且通过网络三维张量的组织特性，为模型剪枝和网络架构比较等实际网络应用提供了有效途径。", "translation": "我们研究了Transformer中自注意力机制的内部（注意力头内部）和外部（注意力头之间）结构。通过运用超微分演算（并辅以计算示例），我们获得了自注意力机制对Softmax激活不变性的理论证据，这依赖于注意力头的内在组织。此外，我们利用现有的张量分层组织方法，通过构建关于网络三维张量的查询、键和头轴的分层分区树来检查网络结构。这种组织是重要的，因为它允许人们在组织后的网络三维张量表现出规律性的几何结构上，有效地执行常见的信号处理任务。我们通过可视化由注意力头和扩散图嵌入组成的分层组织进行定性说明，并通过研究网络中查询、键和头空间上分别相对于双哈尔基和三哈尔基的单个注意力头和整个网络的扩展系数来定量研究网络稀疏性。为了展示我们理论和方法学发现的实用性，我们提供了使用视觉和语言Transformer的计算示例。这些发现的影响是双重的：(1) 在可解释性分析方面，理论上允许了后续步骤，并且可以凭经验用于下游可解释性任务；(2) 可以利用网络三维张量组织进行经验性网络应用，例如模型剪枝（凭借网络稀疏性）和网络架构比较。", "summary": "本文深入探究了Transformer中自注意力机制的内部与外部结构。研究通过超微分演算理论证明了自注意力对Softmax激活的不变性，并利用分层张量组织方法构建分区树以分析网络结构。通过定性可视化和定量稀疏性分析，揭示了网络三维张量的规律性，并验证了其在可解释性分析、模型剪枝和网络架构比较等方面的实用性。研究成果为Transformer的理解与优化提供了新视角。", "keywords": "注意力机制, Softmax不变性, 网络稀疏性, Transformer, 张量组织", "comments": "本文的创新点在于首次从理论上证明了自注意力机制对Softmax激活的不变性，并通过引入张量分层组织方法，为分析Transformer的内部结构提供了新的视角。其重要性体现在为模型可解释性、模型剪枝和网络架构比较等实际应用提供了坚实的理论和方法基础，有望推动Transformer模型的进一步优化和理解。该研究结合了理论分析（超微分演算）和实际应用（视觉和语言Transformer），具有较高的价值。"}}
{"id": "2506.15276", "title": "MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "JunHao Jiang"], "summary": "Implicit Neural representations (INRs) have emerged as a promising approach\nfor video compression, and have achieved comparable performance to the\nstate-of-the-art codecs such as H.266/VVC. However, existing INR-based methods\nstruggle to effectively represent detail-intensive and fast-changing video\ncontent. This limitation mainly stems from the underutilization of internal\nnetwork features and the absence of video-specific considerations in network\ndesign. To address these challenges, we propose a multi-scale feature fusion\nframework, MSNeRV, for neural video representation. In the encoding stage, we\nenhance temporal consistency by employing temporal windows, and divide the\nvideo into multiple Groups of Pictures (GoPs), where a GoP-level grid is used\nfor background representation. Additionally, we design a multi-scale spatial\ndecoder with a scale-adaptive loss function to integrate multi-resolution and\nmulti-frequency information. To further improve feature extraction, we\nintroduce a multi-scale feature block that fully leverages hidden features. We\nevaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and\ncompression. Experimental results demonstrate that our model exhibits superior\nrepresentation capability among INR-based approaches and surpasses VTM-23.7\n(Random Access) in dynamic scenarios in terms of compression efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15276v1", "AI": {"title_translation": "MSNeRV：基于多尺度特征融合的神经视频表示", "tldr": "MSNeRV通过多尺度特征融合解决了现有隐式神经表示在处理细节丰富和快速变化视频内容时的不足，并在视频表示和压缩方面表现出优越性能。", "motivation": "现有的基于隐式神经表示（INRs）的视频压缩方法难以有效表示细节丰富和快速变化的视频内容，这主要是由于内部网络特征利用不足以及网络设计中缺乏视频特异性考虑。", "method": "本文提出MSNeRV多尺度特征融合框架。在编码阶段，通过采用时间窗增强时间一致性，并将视频划分为多个图片组（GoPs），其中GoP级网格用于背景表示。此外，设计了一个带有尺度自适应损失函数的多尺度空间解码器，以整合多分辨率和多频率信息。为了进一步改进特征提取，引入了一个多尺度特征块，充分利用隐藏特征。", "result": "在HEVC ClassB和UVG数据集上评估。实验结果表明，MSNeRV在基于INR的方法中表现出卓越的表示能力，并且在动态场景下，其压缩效率超过了VTM-23.7（随机访问）。", "conclusion": "MSNeRV通过其多尺度特征融合框架，有效解决了现有INR方法在处理复杂视频内容时的局限性，显著提升了神经视频表示和压缩的性能。", "translation": "隐式神经表示（INRs）已成为一种有前景的视频压缩方法，并取得了与H.266/VVC等最先进编解码器相媲美的性能。然而，现有的基于INR的方法难以有效表示细节丰富和快速变化的视频内容。这种局限性主要源于内部网络特征的利用不足以及网络设计中缺乏视频特异性考虑。为了解决这些挑战，我们提出了一种用于神经视频表示的多尺度特征融合框架MSNeRV。在编码阶段，我们通过采用时间窗来增强时间一致性，并将视频划分为多个图片组（GoPs），其中GoP级网格用于背景表示。此外，我们设计了一个具有尺度自适应损失函数的多尺度空间解码器，以整合多分辨率和多频率信息。为了进一步改进特征提取，我们引入了一个多尺度特征块，充分利用隐藏特征。我们在HEVC ClassB和UVG数据集上对MSNeRV的视频表示和压缩性能进行了评估。实验结果表明，我们的模型在基于INR的方法中表现出卓越的表示能力，并且在动态场景下，其压缩效率超过了VTM-23.7（随机访问）。", "summary": "本文提出了MSNeRV，一个用于神经视频表示的多尺度特征融合框架，旨在解决现有隐式神经表示（INRs）在处理细节丰富和快速变化的视频内容时的不足。MSNeRV通过引入时间窗增强时间一致性、利用GoP级别网格进行背景表示，并设计多尺度空间解码器和多尺度特征块来充分利用多分辨率、多频率和隐藏特征。实验证明，MSNeRV在视频表示能力上优于其他基于INR的方法，并在动态场景下的压缩效率超越了VTM-23.7。", "keywords": "神经视频表示, 多尺度特征融合, 视频压缩, 隐式神经表示, 时间一致性", "comments": "这项工作通过引入多尺度特征融合策略，有效解决了隐式神经表示在处理复杂视频内容时的挑战，尤其是在细节和动态变化方面。其创新点在于结合了时间一致性增强、GoP级别背景表示、多尺度解码器以及特征块，全面提升了视频表示的质量和压缩效率。超越VTM-23.7的性能表明了其在实际应用中的潜力。"}}
{"id": "2506.15343", "title": "Offensive Robot Cybersecurity", "authors": ["Víctor Mayoral-Vilches"], "summary": "Offensive Robot Cybersecurity introduces a groundbreaking approach by\nadvocating for offensive security methods empowered by means of automation. It\nemphasizes the necessity of understanding attackers' tactics and identifying\nvulnerabilities in advance to develop effective defenses, thereby improving\nrobots' security posture. This thesis leverages a decade of robotics\nexperience, employing Machine Learning and Game Theory to streamline the\nvulnerability identification and exploitation process. Intrinsically, the\nthesis uncovers a profound connection between robotic architecture and\ncybersecurity, highlighting that the design and creation aspect of robotics\ndeeply intertwines with its protection against attacks. This duality -- whereby\nthe architecture that shapes robot behavior and capabilities also necessitates\na defense mechanism through offensive and defensive cybersecurity strategies --\ncreates a unique equilibrium. Approaching cybersecurity with a dual perspective\nof defense and attack, rooted in an understanding of systems architecture, has\nbeen pivotal. Through comprehensive analysis, including ethical considerations,\nthe development of security tools, and executing cyber attacks on robot\nsoftware, hardware, and industry deployments, this thesis proposes a novel\narchitecture for cybersecurity cognitive engines. These engines, powered by\nadvanced game theory and machine learning, pave the way for autonomous\noffensive cybersecurity strategies for robots, marking a significant shift\ntowards self-defending robotic systems. This research not only underscores the\nimportance of offensive measures in enhancing robot cybersecurity but also sets\nthe stage for future advancements where robots are not just resilient to cyber\nthreats but are equipped to autonomously safeguard themselves.", "comment": "Doctoral thesis", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15343v1", "AI": {"title_translation": "机器人进攻性网络安全", "tldr": "该研究提出一种通过自动化进攻性安全方法提升机器人网络安全的新范式，利用机器学习和博弈论实现自主防御。", "motivation": "强调了理解攻击者策略和提前识别漏洞对于开发有效防御的重要性，以改善机器人的安全态势。研究发现机器人架构与网络安全之间存在深刻联系，其设计和创建与防御攻击紧密交织。", "method": "该论文利用十年的机器人经验，采用机器学习和博弈论来简化漏洞识别和利用过程。通过全面的分析，包括伦理考量、安全工具开发以及对机器人软件、硬件和工业部署执行网络攻击，提出了一种新颖的网络安全认知引擎架构。", "result": "提出了一个新颖的网络安全认知引擎架构，该引擎由先进的博弈论和机器学习驱动，为机器人自主进攻性网络安全策略铺平了道路，标志着向自防御机器人系统的重大转变。", "conclusion": "该研究不仅强调了进攻性措施在增强机器人网络安全方面的重要性，也为未来的发展奠定了基础，使机器人不仅能够抵御网络威胁，还能自主保护自己。", "translation": "进攻性机器人网络安全引入了一种开创性的方法，倡导通过自动化实现进攻性安全方法。它强调了理解攻击者策略并提前识别漏洞以开发有效防御的必要性，从而改善机器人的安全态势。本论文利用了十年的机器人经验，采用机器学习和博弈论来简化漏洞识别和利用过程。从本质上讲，本论文揭示了机器人架构与网络安全之间的深刻联系，强调了机器人设计和创建方面与其抵御攻击的保护措施深度交织。这种二元性——即塑造机器人行为和能力的架构也需要通过进攻性和防御性网络安全策略来建立防御机制——创造了一种独特的平衡。以攻防双重视角处理网络安全，并植根于对系统架构的理解，这一点至关重要。通过全面的分析，包括伦理考量、安全工具的开发以及对机器人软件、硬件和工业部署执行网络攻击，本论文提出了一种新颖的网络安全认知引擎架构。这些由先进博弈论和机器学习驱动的引擎，为机器人的自主进攻性网络安全策略铺平了道路，标志着向自防御机器人系统的重大转变。这项研究不仅强调了进攻性措施在增强机器人网络安全方面的重要性，也为未来的进步奠定了基础，使机器人不仅能够抵御网络威胁，还能自主保护自己。", "summary": "这篇论文提出了一种开创性的机器人网络安全方法，通过自动化实现进攻性安全策略。它利用机器学习和博弈论来识别和利用漏洞，并强调了机器人架构与网络安全之间的内在联系。研究提出了一种新型的网络安全认知引擎架构，旨在使机器人能够自主执行进攻性网络安全策略，从而实现自防御。", "keywords": "机器人网络安全, 进攻性安全, 机器学习, 博弈论, 自主防御", "comments": "这篇论文的创新之处在于将传统的“防御”思维扩展到“进攻性安全”，并将其应用于机器人领域，通过自动化实现自主防御。它强调了攻防一体的视角以及系统架构在网络安全中的核心作用。该研究为未来机器人自主安全系统的发展奠定了基础，具有重要的理论和实践意义。"}}
{"id": "2506.15525", "title": "\"How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "summary": "As generative AI (GenAI) emerges as a transformative force, clear\nunderstanding of high school students' perspectives is essential for GenAI's\nmeaningful integration in high school environments. In this work, we draw\ninsights from a participatory design workshop where we engaged 17 high school\nstudents -- a group rarely involved in prior research in this area -- through\nthe design of novel GenAI tools and school policies addressing their key\nconcerns. Students identified challenges and developed solutions outlining\ntheir ideal features in GenAI tools, appropriate school use, and regulations.\nThese centered around the problem spaces of combating bias & misinformation,\ntackling crime & plagiarism, preventing over-reliance on AI, and handling false\naccusations of academic dishonesty. Building on our participants'\nunderrepresented perspectives, we propose new guidelines targeted at\neducational technology designers for development of GenAI technologies in high\nschools. We also argue for further incorporation of student voices in\ndevelopment of AI policies in their schools.", "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in\n  proceedings of 24th annual ACM Interaction Design and Children Conference\n  (IDC 2025)", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15525v1", "AI": {"title_translation": "我们如何同时学习和使用AI？:: 与高中生共同参与生成式AI的设计", "tldr": "本研究通过与高中生共同参与设计生成式AI（GenAI）工具和学校政策，深入了解他们在高中环境中整合GenAI的视角，并提出了针对教育技术设计师的GenAI开发指南以及在AI政策制定中纳入学生声音的建议。", "motivation": "了解高中生对生成式AI（GenAI）的看法，以促进GenAI在高中环境中的有意义整合，因为GenAI正在成为一股变革力量。", "method": "通过一个参与式设计工作坊，与17名高中生（该领域先前研究中很少涉及的群体）共同设计新颖的GenAI工具和学校政策，以解决他们关注的关键问题。", "result": "学生们识别了挑战并开发了解决方案，明确了GenAI工具的理想功能、适当的学校使用和规定。这些主要围绕打击偏见与错误信息、应对犯罪与抄袭、防止过度依赖AI以及处理学术不端行为的虚假指控等问题领域。", "conclusion": "基于高中生未被充分代表的视角，本研究为教育技术设计师提出了开发高中GenAI技术的新指南，并主张在学校AI政策的制定中进一步纳入学生的声音。", "translation": "随着生成式AI（GenAI）成为一股变革力量，清晰了解高中生的视角对于GenAI在高中环境中有意义的整合至关重要。在这项工作中，我们从一个参与式设计工作坊中汲取见解，该工作坊让17名高中生参与其中——这个群体很少参与该领域先前的研究——通过设计新颖的GenAI工具和解决他们主要担忧的学校政策。学生们识别了挑战并开发了解决方案，概述了他们在GenAI工具中的理想功能、适当的学校使用和规定。这些主要围绕打击偏见与错误信息、应对犯罪与抄袭、防止过度依赖AI以及处理学术不端行为的虚假指控等问题领域。基于我们参与者未被充分代表的视角，我们为教育技术设计师提出了开发高中GenAI技术的新指南。我们还主张在学校AI政策的制定中进一步纳入学生的声音。", "summary": "本研究通过一个与17名高中生共同参与的设计工作坊，探讨了生成式AI（GenAI）在高中环境中的整合问题。学生们提出了GenAI工具的理想功能、使用规范和政策建议，重点关注偏见、抄袭、过度依赖和学术不端等挑战。研究基于这些未被充分代表的学生视角，为教育技术设计师提供了开发GenAI技术的新指南，并强调了在学校AI政策制定中纳入学生声音的重要性。", "keywords": "生成式AI, 参与式设计, 高中生, 教育技术, AI政策", "comments": "这项研究的创新之处在于其采用了参与式设计方法，将高中生这一通常在AI政策和工具设计中被忽视的群体纳入进来，提供了宝贵的、以用户为中心的设计洞察。其重要性在于，它直接解决了GenAI在教育领域应用中可能出现的实际问题，并为未来的教育技术开发和政策制定提供了具体指导。"}}
{"id": "2506.14980", "title": "Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors", "authors": ["Ziteng Li", "Malte Kuhlmann", "Ilana Nisky", "Nicolás Navarro-Guerrero"], "summary": "Compliance is a critical parameter for describing objects in engineering,\nagriculture, and biomedical applications. Traditional compliance detection\nmethods are limited by their lack of portability and scalability, rely on\nspecialized, often expensive equipment, and are unsuitable for robotic\napplications. Moreover, existing neural network-based approaches using\nvision-based tactile sensors still suffer from insufficient prediction\naccuracy. In this paper, we propose two models based on Long-term Recurrent\nConvolutional Networks (LRCNs) and Transformer architectures that leverage RGB\ntactile images and other information captured by the vision-based sensor\nGelSight to predict compliance metrics accurately. We validate the performance\nof these models using multiple metrics and demonstrate their effectiveness in\naccurately estimating compliance. The proposed models exhibit significant\nperformance improvement over the baseline. Additionally, we investigated the\ncorrelation between sensor compliance and object compliance estimation, which\nrevealed that objects that are harder than the sensor are more challenging to\nestimate.", "comment": "Accepted in the IEEE International Conference on Development and\n  Learning (ICDL). The paper contains 8 pages and 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14980v1", "AI": {"title_translation": "合规性检测的进展：使用基于视觉的触觉传感器的新型模型", "tldr": "本文提出了两种使用GelSight视觉触觉传感器的新模型（LRCNs、Transformer），用于准确预测柔顺性，性能优于基线，并指出估计比传感器更硬的物体时面临挑战。", "motivation": "传统的柔顺性检测方法缺乏便携性、可扩展性，成本高昂，不适用于机器人应用。现有基于视觉触觉传感器的神经网络方法预测精度不足。", "method": "本文提出了两种基于长短期循环卷积网络（LRCNs）和Transformer架构的模型。这些模型利用GelSight视觉触觉传感器捕获的RGB触觉图像和其他信息来准确预测柔顺性指标。", "result": "所提出的模型在准确估计柔顺性方面表现出比基线显著的性能提升。比传感器更硬的物体更难估计。", "conclusion": "所提出的LRCN和Transformer模型显著提高了使用基于视觉的触觉传感器进行柔顺性预测的准确性，解决了先前方法的局限性，尽管对于较硬的物体仍存在挑战。", "translation": "柔顺性是描述工程、农业和生物医学应用中物体的关键参数。传统的柔顺性检测方法受限于其缺乏便携性和可扩展性，依赖于专业且通常昂贵的设备，并且不适用于机器人应用。此外，现有使用基于视觉的触觉传感器的神经网络方法仍然存在预测精度不足的问题。在本文中，我们提出了两种基于长短期循环卷积网络（LRCNs）和Transformer架构的模型，它们利用基于视觉的GelSight传感器捕获的RGB触觉图像和其他信息来准确预测柔顺性指标。我们使用多项指标验证了这些模型的性能，并证明了它们在准确估计柔顺性方面的有效性。所提出的模型比基线表现出显著的性能提升。此外，我们研究了传感器柔顺性与物体柔顺性估计之间的相关性，这表明比传感器更硬的物体更难以估计。", "summary": "本文旨在解决当前柔顺性检测方法的局限性，这些方法不适用于机器人且精度不足。文章提出了两种基于LRCNs和Transformer架构的新型模型，利用GelSight视觉触觉传感器获取的RGB触觉图像来准确预测物体柔顺性。这些模型表现出优于现有基线的性能，并揭示了估计比传感器更硬的物体的柔顺性更具挑战性。", "keywords": "柔顺性检测, 基于视觉的触觉传感器, LRCN, Transformer, GelSight", "comments": "该论文引入了新颖的神经网络架构（LRCNs、Transformers）用于基于视觉触觉传感器的柔顺性检测，这是迈向更准确、更实用机器人操作的重要一步。对较硬物体检测挑战的发现为未来的研究提供了宝贵的见解。"}}
{"id": "2506.14821", "title": "Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints", "authors": ["Sunil Kumar", "Bowen Zhao", "Leo Dirac", "Paulina Varshavskaya"], "summary": "Despite tremendous recent advances in large model reasoning ability,\nvision-language models (VLMs) still struggle with detailed visual reasoning,\nespecially when compute resources are limited. To address this challenge, we\ndraw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale\nmodels with Group Relative Policy Optimization (GRPO) to use external tools\nsuch as zoom. The greatest benefit is obtained with a combination of GRPO\nlearning, a simple reward structure, a simplified tool-calling interface,\nallocating additional tokens to the result of the tool call, and a training\ndata mix that over-represents visually difficult examples. Compared to\nsimilarly-sized baseline models, our method achieves better performance on some\nvisual question-answering (VQA) tasks, thanks to the detailed visual\ninformation gathered from the external tool.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14821v1", "AI": {"title_translation": "强化VLM在资源受限下使用工具进行详细视觉推理", "tldr": "本文提出一种使用GRPO训练小型VLM以利用外部工具（如缩放）进行详细视觉推理的方法，在资源受限下提高了VQA任务的性能。", "motivation": "视觉语言模型（VLMs）在详细视觉推理方面仍有困难，尤其是在计算资源有限的情况下。", "method": "借鉴Deepseek-r1等方法，使用组相对策略优化（GRPO）训练小型VLM以使用外部工具（如缩放）。该方法结合了GRPO学习、简单的奖励结构、简化的工具调用接口、为工具调用结果分配额外token以及包含更多视觉困难示例的训练数据混合。", "result": "与同等大小的基线模型相比，该方法在一些视觉问答（VQA）任务上取得了更好的性能，这得益于从外部工具中收集的详细视觉信息。", "conclusion": "通过利用外部工具收集的详细视觉信息，可以有效提高VLM在资源受限下的详细视觉推理能力。", "translation": "尽管大型模型推理能力最近取得了巨大进展，但视觉语言模型（VLM）在详细视觉推理方面仍然存在困难，尤其是在计算资源有限的情况下。为了应对这一挑战，我们从Deepseek-r1等VLM方法中汲取灵感，并使用组相对策略优化（GRPO）训练小型模型，使其能够使用外部工具，例如缩放。通过结合GRPO学习、简单的奖励结构、简化的工具调用接口、为工具调用结果分配额外token以及过度代表视觉困难示例的训练数据混合，获得了最大的益处。与类似大小的基线模型相比，我们的方法在一些视觉问答（VQA）任务上实现了更好的性能，这得益于从外部工具中收集的详细视觉信息。", "summary": "本文提出一种在资源受限环境下增强视觉语言模型（VLM）进行详细视觉推理的方法。通过借鉴Deepseek-r1并利用组相对策略优化（GRPO）训练小型VLM使用外部工具（如缩放），结合优化后的奖励结构、工具调用接口和训练数据策略，显著提升了模型在视觉问答（VQA）任务上的表现，证明了外部工具在弥补VLM详细视觉推理不足方面的有效性。", "keywords": "视觉语言模型, 工具使用, 详细视觉推理, 资源受限, GRPO, VQA", "comments": "本文的创新点在于将工具使用引入小型VLM，并通过GRPO等策略优化其在资源受限下的详细视觉推理能力。这对于资源有限的实际应用场景具有重要意义，提供了一种在不增加模型规模的情况下提升性能的有效途径。"}}
{"id": "2506.15246", "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge", "authors": ["Juli Bakagianni", "John Pavlopoulos", "Aristidis Likas"], "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15246v1", "AI": {"title_translation": "TopClustRAG在SIGIR 2025 LiveRAG挑战赛", "tldr": "TopClustRAG是一个针对LiveRAG挑战赛开发的RAG系统，它结合了混合检索和K-Means聚类来增强答案的多样性、相关性和忠实性，并在官方排行榜上取得了优异成绩。", "motivation": "该论文旨在介绍为LiveRAG挑战赛开发的TopClustRAG系统，该挑战赛旨在评估大规模网络语料库上的端到端问答能力。", "method": "TopClustRAG系统采用混合检索策略（结合稀疏和密集索引），随后使用K-Means聚类对语义相似的段落进行分组。从每个聚类中选出代表性段落，用于为大型语言模型（LLM）构建特定于聚类的提示，生成中间答案。这些中间答案经过过滤、重新排序，最终合成为一个单一、全面的响应。", "result": "在FineWeb Sample-10BT数据集上的评估显示，TopClustRAG在官方排行榜上忠实性排名第2，正确性排名第7。", "conclusion": "研究结果表明，在大型RAG系统中，基于聚类的上下文过滤和提示聚合是有效的。", "translation": "我们介绍了TopClustRAG，这是一个为LiveRAG挑战赛开发的检索增强生成（RAG）系统，该挑战赛旨在评估大规模网络语料库上的端到端问答。我们的系统采用混合检索策略，结合了稀疏索引和密集索引，然后通过K-Means聚类将语义相似的段落分组。来自每个聚类的代表性段落被用于为大型语言模型（LLM）构建特定于聚类的提示，生成中间答案，这些答案经过过滤、重新排序，最后合成为一个单一、全面的响应。这种多阶段管道增强了答案的多样性、相关性和对检索证据的忠实性。在FineWeb Sample-10BT数据集上进行评估，TopClustRAG在官方排行榜上忠实性排名第2，正确性排名第7，这证明了在大型RAG系统中基于聚类的上下文过滤和提示聚合的有效性。", "summary": "TopClustRAG是一个为LiveRAG挑战赛设计的检索增强生成（RAG）系统，用于处理大规模网络语料库上的问答。它采用混合检索、K-Means聚类来组织检索到的段落，并为LLM生成聚类特定的提示。通过多阶段的答案生成、过滤和合成，该系统提高了答案的多样性、相关性和忠实性。在FineWeb Sample-10BT数据集上，TopClustRAG在忠实性方面排名第2，在正确性方面排名第7，凸显了其聚类方法的有效性。", "keywords": "检索增强生成, K-Means聚类, 大规模问答, 混合检索, 上下文过滤", "comments": "TopClustRAG的创新之处在于其独特的多阶段管道，特别是结合了混合检索与K-Means聚类，以有效地组织和利用大规模检索到的信息。通过生成聚类特定的提示，它能够为LLM提供更聚焦和高质量的上下文，从而显著提升答案的忠实性和多样性。在竞争激烈的LiveRAG挑战赛中取得优异成绩，证明了其方法的实用性和有效性。"}}
{"id": "2506.15627", "title": "Pathwise convergence of a novel numerical scheme based on semi-implicit method for stochastic differential-algebraic equations with non-global Lipschitz coefficients", "authors": ["Guy Tsafack", "Antoine Tambue"], "summary": "This paper delves into the well-posedness and the numerical approximation of\nnon-autonomous stochastic differential algebraic equations (SDAEs) with\nnonlinear local Lipschitz coefficients that satisfy the more general\nmonotonicity condition called Khasminskii condition. The key challenge is the\npresence of a singular matrix which makes the numerical integration hard and\nheavy. To address this challenge, we propose a novel numerical scheme based on\nsemi-implicit method for the drift component of the SDAEs. More precisely we\nsplit the drift term as the sum of a linear term and a nonlinear term. The\nlinear part is approximated implicitly, while the nonlinear part is\napproximated explicitly. The linear component's role is to handle the\nsingularity issues during the numerical integration without the resolution of\nnonlinear algebraic equations in the constraint equations. This novel scheme is\ntherefore very efficient for SDAEs in high dimension that come after the\nspatial discretisation of stochastic partial differential algebraic equations\n(SPDAEs). To prove the pathwise convergence of our novel scheme, we first\nderive a equivalent scheme called dual scheme, suitable for mathematical\nanalysis and linked to the inherent stochastic differential equation resulting\nfrom the elimination of constraints in the initial SDAEs. We prove that our\nnovel scheme converges to the exact solution with rate $\\frac{1}{2}-\\epsilon$,\nfor arbitrary $\\epsilon>0$ in the pathwise sense. Numerical simulations are\nperformed to demonstrate the efficiency of the scheme in high dimension and to\nshow that our theoretical results are in agreement with numerical experiments.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15627v1", "AI": {"title_translation": "具有非全局Lipschitz系数的随机微分代数方程半隐式新数值格式的路径收敛性", "tldr": "本文提出了一种基于半隐式方法的新型数值格式，用于解决具有奇异矩阵的非自治随机微分代数方程 (SDAEs) 的数值积分难题，并证明了其路径收敛性。", "motivation": "解决具有奇异矩阵的非自治随机微分代数方程 (SDAEs) 的数值积分困难和复杂性。", "method": "提出了一种基于半隐式方法的新型数值格式，用于SDAEs的漂移分量。具体来说，将漂移项分解为线性项和非线性项之和，线性部分隐式逼近，非线性部分显式逼近。通过推导一个称为对偶格式的等价格式来证明路径收敛性。", "result": "证明了该新型格式以 $\\frac{1}{2}-\\epsilon$ 的速率在路径意义上收敛到精确解。数值模拟验证了该格式在高维情况下的效率，并表明理论结果与数值实验一致。", "conclusion": "该新型半隐式数值格式能够有效处理具有非全局Lipschitz系数的随机微分代数方程的数值积分问题，并在高维情况下表现出良好的效率和收敛性。", "translation": "本文深入探讨了具有非线性局部Lipschitz系数且满足更一般单调性条件（称为Khasminskii条件）的非自治随机微分代数方程（SDAEs）的适定性和数值逼近问题。关键挑战在于存在一个奇异矩阵，这使得数值积分变得困难和繁重。为了解决这一挑战，我们提出了一种基于半隐式方法的新型数值格式，用于SDAEs的漂移分量。更精确地说，我们将漂移项分解为线性项和非线性项之和。线性部分隐式逼近，而非线性部分显式逼近。线性分量的作用是在数值积分过程中处理奇异性问题，而无需在约束方程中求解非线性代数方程。因此，这种新型格式对于经过随机偏微分代数方程（SPDAEs）空间离散化后产生的高维SDAEs非常有效。为了证明我们新型格式的路径收敛性，我们首先推导了一个称为对偶格式的等价格式，该格式适用于数学分析，并与初始SDAEs中消除约束后产生的固有随机微分方程相关联。我们证明了我们的新型格式以 $\\frac{1}{2}-\\epsilon$ 的速率在路径意义上收敛到精确解，其中 $\\epsilon>0$ 是任意小的正数。进行了数值模拟以证明该格式在高维情况下的效率，并表明我们的理论结果与数值实验一致。", "summary": "本文针对具有非全局Lipschitz系数和奇异矩阵的非自治随机微分代数方程（SDAEs）的数值积分难题，提出了一种基于半隐式方法的新型数值格式。该格式将漂移项分解为线性（隐式）和非线性（显式）部分，有效处理了奇异性问题。研究通过推导对偶格式证明了其路径收敛率为 $\\frac{1}{2}-\\epsilon$，并通过高维数值模拟验证了其效率和理论结果的一致性。", "keywords": "随机微分代数方程, 半隐式方法, 路径收敛性, 奇异矩阵, Khasminskii条件", "comments": "该论文的创新点在于提出了一个 novel 的半隐式数值格式，通过巧妙地将漂移项分解为线性隐式和非线性显式部分，成功解决了随机微分代数方程中奇异矩阵导致的数值积分难题，尤其适用于高维SDAEs。其重要性体现在为处理这类复杂方程提供了一个高效且具有理论收敛保证的工具。"}}
{"id": "2506.15656", "title": "PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "summary": "Phishing websites continue to pose a significant cybersecurity threat, often\nleveraging deceptive structures, brand impersonation, and social engineering\ntactics to evade detection. While recent advances in large language models\n(LLMs) have enabled improved phishing detection through contextual\nunderstanding, most existing approaches rely on single-agent classification\nfacing the risks of hallucination and lack interpretability or robustness. To\naddress these limitations, we propose PhishDebate, a modular multi-agent\nLLM-based debate framework for phishing website detection. PhishDebate employs\nfour specialized agents to independently analyze different textual aspects of a\nwebpage--URL structure, HTML composition, semantic content, and brand\nimpersonation--under the coordination of a Moderator and a final Judge. Through\nstructured debate and divergent thinking, the framework delivers more accurate\nand interpretable decisions. Extensive evaluations on commercial LLMs\ndemonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate\n(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain\nof Thought (CoT) baselines. Additionally, its modular design allows agent-level\nconfigurability, enabling adaptation to varying resource and application\nrequirements.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15656v1", "AI": {"title_translation": "PhishDebate：一个基于LLM的多智能体网络钓鱼网站检测框架", "tldr": "PhishDebate是一个基于LLM的多智能体框架，通过模拟辩论来检测网络钓鱼网站，提高准确性和可解释性，并优于现有方法。", "motivation": "现有基于LLM的网络钓鱼检测方法多依赖单智能体分类，面临幻觉、缺乏可解释性和鲁棒性的风险。网络钓鱼网站利用欺骗性结构、品牌冒充和社会工程策略逃避检测，构成重大网络安全威胁。", "method": "本文提出了PhishDebate，一个模块化的多智能体LLM辩论框架，用于网络钓鱼网站检测。该框架采用四个专门的智能体独立分析网页的不同文本方面（URL结构、HTML组成、语义内容和品牌冒充），并在仲裁者和最终评判员的协调下进行。通过结构化辩论和发散思维，该框架提供更准确和可解释的决策。", "result": "在真实世界网络钓鱼数据集上，PhishDebate实现了98.2%的召回率和98.2%的真阳性率（TPR），并且优于单智能体和思维链（CoT）基线。其模块化设计允许智能体级别的可配置性，能够适应不同的资源和应用需求。", "conclusion": "PhishDebate通过多智能体辩论框架有效解决了现有LLM网络钓鱼检测方法的局限性，显著提高了检测的准确性、可解释性和鲁棒性。", "translation": "网络钓鱼网站继续构成重大的网络安全威胁，经常利用欺骗性结构、品牌冒充和社会工程策略来逃避检测。尽管大型语言模型（LLM）的最新进展通过上下文理解实现了改进的网络钓鱼检测，但大多数现有方法依赖于单智能体分类，面临幻觉、缺乏可解释性或鲁棒性的风险。为了解决这些限制，我们提出了PhishDebate，一个模块化的、基于LLM的多智能体辩论框架，用于网络钓鱼网站检测。PhishDebate采用四个专门的智能体独立分析网页的不同文本方面——URL结构、HTML组成、语义内容和品牌冒充——并在仲裁者和最终评判员的协调下进行。通过结构化辩论和发散思维，该框架提供更准确和可解释的决策。对商业LLM的广泛评估表明，PhishDebate在真实世界网络钓鱼数据集上实现了98.2%的召回率和98.2%的真阳性率（TPR），并且优于单智能体和思维链（CoT）基线。此外，其模块化设计允许智能体级别的可配置性，从而能够适应不同的资源和应用需求。", "summary": "本文提出了PhishDebate，一个基于LLM的多智能体辩论框架，旨在解决现有单智能体LLM在网络钓鱼检测中存在的幻觉、可解释性差和鲁棒性不足的问题。PhishDebate包含四个专门的智能体，分别分析URL、HTML、语义内容和品牌冒充，并在仲裁者和评判员的协调下进行结构化辩论，以实现更准确和可解释的检测。实验证明，PhishDebate在真实数据集上表现出色，召回率和真阳性率均达到98.2%，且优于单智能体和CoT基线方法。", "keywords": "网络钓鱼检测, LLM, 多智能体系统, 网络安全, 辩论框架", "comments": "这篇论文的创新点在于引入了多智能体辩论框架来提升LLM在网络钓鱼检测中的性能，有效解决了单智能体模型的固有缺陷（如幻觉和可解释性差）。通过模拟人类辩论过程，PhishDebate能够从多个维度对网页信息进行分析和交叉验证，从而提供更准确和可信的决策。其模块化设计也增加了框架的灵活性和适应性。"}}
{"id": "2506.13776", "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations", "authors": ["Kevin L. Wei", "Patricia Paskov", "Sunishchal Dev", "Michael J. Byun", "Anka Reuel", "Xavier Roberts-Gaal", "Rachel Calcott", "Evie Coxon", "Chinmay Deshpande"], "summary": "In this position paper, we argue that human baselines in foundation model\nevaluations must be more rigorous and more transparent to enable meaningful\ncomparisons of human vs. AI performance, and we provide recommendations and a\nreporting checklist towards this end. Human performance baselines are vital for\nthe machine learning community, downstream users, and policymakers to interpret\nAI evaluations. Models are often claimed to achieve \"super-human\" performance,\nbut existing baselining methods are neither sufficiently rigorous nor\nsufficiently well-documented to robustly measure and assess performance\ndifferences. Based on a meta-review of the measurement theory and AI evaluation\nliteratures, we derive a framework with recommendations for designing,\nexecuting, and reporting human baselines. We synthesize our recommendations\ninto a checklist that we use to systematically review 115 human baselines\n(studies) in foundation model evaluations and thus identify shortcomings in\nexisting baselining methods; our checklist can also assist researchers in\nconducting human baselines and reporting results. We hope our work can advance\nmore rigorous AI evaluation practices that can better serve both the research\ncommunity and policymakers. Data is available at:\nhttps://github.com/kevinlwei/human-baselines", "comment": "A version of this paper has been accepted to ICML 2025 as a position\n  paper (spotlight), with the title: \"Position: Human Baselines in Model\n  Evaluations Need Rigor and Transparency (With Recommendations & Reporting\n  Checklist).\"", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.13776v1", "AI": {"title_translation": "模型评估中严谨透明的人类基线的建议与报告清单", "tldr": "本论文指出当前基础模型评估中人类基线缺乏严谨性和透明度，难以进行有效的人机性能比较。为此，论文提出了一个包含建议和报告清单的框架，旨在促进更严谨的AI评估实践。", "motivation": "现有的人类性能基线方法不够严谨和透明，无法可靠地测量和评估人与AI之间的性能差异，这导致对AI评估的解释性不足，尤其当模型声称达到“超人类”性能时。", "method": "作者通过对测量理论和AI评估文献的元审查，提出了一个用于设计、执行和报告人类基线的框架和一系列建议。他们将这些建议综合成一个报告清单，并使用该清单系统性地审查了115个基础模型评估中的人类基线研究，从而识别了现有方法的不足之处。", "result": "论文识别了现有基线方法的不足，并提供了一个可帮助研究人员进行和报告人类基线的建议框架和报告清单。该清单通过对115项研究的审查，验证了其有效性，并揭示了现有实践的缺陷。", "conclusion": "这项工作旨在促进更严谨和透明的AI评估实践，从而更好地服务于研究社区和政策制定者，确保对AI性能的解释更加准确和有意义。", "translation": "在这篇立场论文中，我们认为基础模型评估中的人类基线必须更加严谨和透明，以实现人与AI性能的有意义比较，并为此提供了建议和报告清单。人类性能基线对于机器学习社区、下游用户和政策制定者解释AI评估至关重要。模型经常声称达到“超人类”性能，但现有的基线方法既不够严谨也不够充分地记录，无法可靠地测量和评估性能差异。基于对测量理论和AI评估文献的元审查，我们得出了一个框架，其中包含设计、执行和报告人类基线的建议。我们将建议综合成一个清单，并用它系统性地审查了115个基础模型评估中的人类基线（研究），从而识别了现有基线方法的缺点；我们的清单还可以帮助研究人员进行人类基线并报告结果。我们希望我们的工作能够促进更严谨的AI评估实践，从而更好地服务于研究社区和政策制定者。数据可在以下网址获取：https://github.com/kevinlwei/human-baselines", "summary": "本立场论文强调了在基础模型评估中，人类基线需要更高的严谨性和透明度，以实现对人与AI性能的准确、有意义的比较。鉴于现有基线方法的不足，作者通过对测量理论和AI评估文献的元审查，提出了一个包含建议的框架，用于设计、执行和报告人类基线。这些建议被整合为一个实用的报告清单，该清单不仅用于系统性审查和识别115个现有基线研究的缺点，也旨在指导研究人员改进未来人类基线的实施和报告，从而推动更科学、严谨的AI评估实践。", "keywords": "人类基线, 模型评估, AI性能, 报告清单, 严谨性", "comments": "这篇论文的重要性在于它直接指出了当前AI评估中一个关键但常被忽视的问题：人类基线的不严谨性。它不仅揭示了现有实践的不足，还提供了一个实用的解决方案——一个报告清单，这对于提高AI评估的科学性和可信度具有重要意义。该工作有望帮助减少对AI性能的误解和夸大，促进更负责任的AI发展。"}}
{"id": "2506.15010", "title": "Hyper-Local Deformable Transformers for Text Spotting on Historical Maps", "authors": ["Yijun Lin", "Yao-Yi Chiang"], "summary": "Text on historical maps contains valuable information providing georeferenced\nhistorical, political, and cultural contexts. However, text extraction from\nhistorical maps is challenging due to the lack of (1) effective methods and (2)\ntraining data. Previous approaches use ad-hoc steps tailored to only specific\nmap styles. Recent machine learning-based text spotters (e.g., for scene\nimages) have the potential to solve these challenges because of their\nflexibility in supporting various types of text instances. However, these\nmethods remain challenges in extracting precise image features for predicting\nevery sub-component (boundary points and characters) in a text instance. This\nis critical because map text can be lengthy and highly rotated with complex\nbackgrounds, posing difficulties in detecting relevant image features from a\nrough text region. This paper proposes PALETTE, an end-to-end text spotter for\nscanned historical maps of a wide variety. PALETTE introduces a novel\nhyper-local sampling module to explicitly learn localized image features around\nthe target boundary points and characters of a text instance for detection and\nrecognition. PALETTE also enables hyper-local positional embeddings to learn\nspatial interactions between boundary points and characters within and across\ntext instances. In addition, this paper presents a novel approach to\nautomatically generate synthetic map images, SynthMap+, for training text\nspotters for historical maps. The experiment shows that PALETTE with SynthMap+\noutperforms SOTA text spotters on two new benchmark datasets of historical\nmaps, particularly for long and angled text. We have deployed PALETTE with\nSynthMap+ to process over 60,000 maps in the David Rumsey Historical Map\ncollection and generated over 100 million text labels to support map searching.\nThe project is released at\nhttps://github.com/kartta-foundation/mapkurator-palette-doc.", "comment": "Published in KDD2024", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15010v1", "AI": {"title_translation": "超局部可变形Transformer用于历史地图上的文本识别", "tldr": "本文提出PALETTE模型和SynthMap+数据生成方法，用于历史地图上的文本识别，表现优于SOTA，并已大规模部署。", "motivation": "历史地图中的文本包含重要信息，但由于缺乏有效方法和训练数据，文本提取极具挑战。现有方法无法灵活处理多种地图风格，而当前机器学习方法难以从复杂背景、长且高度旋转的地图文本中精确提取子组件（边界点和字符）特征。", "method": "本文提出了端到端文本识别器PALETTE，它包含：1. 新颖的超局部采样模块，用于显式学习文本实例目标边界点和字符周围的局部图像特征进行检测和识别。2. 超局部位置嵌入，用于学习文本实例内部及实例间边界点与字符的空间交互。此外，本文还提出了自动生成合成地图图像SynthMap+的方法，用于训练历史地图文本识别器。", "result": "PALETTE结合SynthMap+在两个新的历史地图基准数据集上超越了SOTA文本识别器，尤其在处理长文本和倾斜文本方面表现出色。该系统已成功部署，处理了David Rumsey历史地图集中超过60,000张地图，生成了超过1亿个文本标签以支持地图搜索。", "conclusion": "本文提出的PALETTE模型和SynthMap+数据生成方法有效解决了历史地图文本识别的挑战，并在实际应用中取得了显著成果，为历史地图的数字化和信息检索提供了强大的支持。", "translation": "历史地图上的文本包含有价值的信息，提供了地理参考的历史、政治和文化背景。然而，由于缺乏（1）有效方法和（2）训练数据，从历史地图中提取文本具有挑战性。以前的方法使用专门针对特定地图风格的临时步骤。最近基于机器学习的文本识别器（例如，用于场景图像的识别器）有潜力解决这些挑战，因为它们在支持各种类型的文本实例方面具有灵活性。然而，这些方法在提取精确图像特征以预测文本实例中的每个子组件（边界点和字符）方面仍然存在挑战。这至关重要，因为地图文本可能很长、高度旋转且背景复杂，这给从粗略文本区域中检测相关图像特征带来了困难。本文提出了PALETTE，一个用于各种扫描历史地图的端到端文本识别器。PALETTE引入了一种新颖的超局部采样模块，用于显式学习文本实例的目标边界点和字符周围的局部图像特征，以进行检测和识别。PALETTE还支持超局部位置嵌入，以学习文本实例内部和文本实例之间边界点与字符的空间交互。此外，本文提出了一种自动生成合成地图图像SynthMap+的新方法，用于训练历史地图文本识别器。实验表明，PALETTE与SynthMap+在两个新的历史地图基准数据集上优于SOTA文本识别器，特别是对于长文本和倾斜文本。我们已部署PALETTE与SynthMap+处理David Rumsey历史地图集中超过60,000张地图，并生成了超过1亿个文本标签以支持地图搜索。该项目已在https://github.com/kartta-foundation/mapkurator-palette-doc发布。", "summary": "本文针对历史地图文本识别中方法和数据不足的挑战，提出了端到端文本识别器PALETTE和合成数据生成方法SynthMap+。PALETTE通过超局部采样模块和超局部位置嵌入，有效提取并利用局部特征进行文本检测和识别。SynthMap+解决了训练数据稀缺问题。实验证明，PALETTE结合SynthMap+在历史地图文本识别任务上显著优于现有SOTA方法，尤其擅长处理长文本和倾斜文本，并已成功应用于大规模历史地图数字化项目。", "keywords": "历史地图, 文本识别, 可变形Transformer, 超局部采样, 合成数据", "comments": "该研究的创新点在于提出了超局部采样模块和超局部位置嵌入，以更精细地处理复杂背景下历史地图文本的复杂性。同时，SynthMap+的合成数据生成方法有效缓解了历史地图领域训练数据稀缺的实际问题，具有很强的实用价值。该研究的重要性体现在其解决了历史地图文本识别的实际难题，并通过大规模部署验证了其有效性，为历史文献数字化和信息检索提供了有力工具。"}}
{"id": "2506.14824", "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "authors": ["Yao Zhang", "Hewei Gao", "Haokun Chen", "Weiguo Li", "Yunpu Ma", "Volker Tresp"], "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.", "comment": "12 pages, 3 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14824v1", "AI": {"title_translation": "FedNano：迈向预训练多模态大型语言模型的轻量级联邦微调", "tldr": "FedNano是一个联邦学习框架，通过在客户端使用轻量级NanoEdge模块（而不是完整的MLLM）来解决大型多模态大型语言模型（MLLMs）在联邦学习部署中的计算、通信和存储挑战，显著降低了客户端存储和通信开销，并优于现有基线。", "motivation": "多模态大型语言模型（MLLMs）在实际部署中面临分布式多模态数据和严格隐私要求带来的挑战。联邦学习（FL）是解决方案，但将FL应用于MLLMs存在高计算需求、客户端容量限制、高通信成本和异构客户端数据等问题。现有FL方法假设客户端部署完整模型，但这对于大型MLLMs来说不适用。", "method": "我们提出了FedNano，这是第一个将LLM集中在服务器端，同时引入轻量级客户端特定适应模块NanoEdge的FL框架。NanoEdge采用模态特定的编码器、连接器和可训练的低秩适应NanoAdapters。这种设计消除了在客户端部署LLM的需要，并且只传输紧凑的NanoAdapter更新。", "result": "FedNano将客户端存储减少了95%，将通信开销限制在模型参数的0.01%以内。实验表明FedNano优于先前的FL基线，弥合了MLLM规模和FL可行性之间的差距。", "conclusion": "FedNano通过引入轻量级客户端适应模块NanoEdge，有效解决了大型多模态大型语言模型在联邦学习部署中的计算、通信和存储挑战，实现了可扩展、去中心化的多模态AI系统。", "translation": "多模态大型语言模型（MLLMs）在多模态推理和跨模态检索等任务中表现出色，但由于分布式多模态数据和严格的隐私要求，在实际场景中面临部署挑战。联邦学习（FL）通过在不集中数据的情况下实现协作模型训练，提供了一种解决方案。然而，为MLLMs实现FL带来了重大挑战，包括高计算需求、有限的客户端容量、高昂的通信成本和异构的客户端数据。现有的FL方法假设在客户端部署完整模型，但由于大规模MLLMs的巨大尺寸和通信需求，这一假设不再成立。为了解决这些限制，我们提出了FedNano，这是第一个将LLM集中在服务器端，同时引入轻量级客户端特定适应模块NanoEdge的FL框架。NanoEdge采用模态特定的编码器、连接器和可训练的低秩适应NanoAdapters。这种设计消除了在客户端部署LLM的需要，将客户端存储减少了95%，并将通信开销限制在模型参数的0.01%以内。通过仅传输紧凑的NanoAdapter更新，FedNano处理异构客户端数据和资源限制，同时保护隐私。实验表明，FedNano优于先前的FL基线，弥合了MLLM规模和FL可行性之间的差距，并实现了可扩展、去中心化的多模态AI系统。", "summary": "本研究提出了FedNano，一个创新的联邦学习框架，旨在解决多模态大型语言模型（MLLMs）在联邦部署中面临的计算、存储和通信挑战。FedNano通过将大型LLM保留在服务器端，并为客户端引入一个名为NanoEdge的轻量级适应模块，该模块包含模态特定编码器、连接器和低秩适应NanoAdapters。这种设计显著减少了客户端存储需求（95%）和通信开销（仅0.01%的模型参数），使得大型MLLMs的联邦学习成为可能。实验结果表明，FedNano在性能上超越了现有基线，为可扩展、去中心化的多模态AI系统铺平了道路。", "keywords": "联邦学习, 多模态大型语言模型, 轻量级微调, NanoEdge, 隐私保护", "comments": "FedNano的创新之处在于其独特的客户端-服务器分离策略，即在服务器端集中大型LLM，而在客户端仅部署轻量级的NanoEdge模块进行适应。这种设计巧妙地解决了大型MLLMs在联邦学习中面临的核心挑战，如计算资源限制和高通信成本。通过引入低秩适应，进一步优化了更新传输的效率和隐私保护。该方法对于推动多模态AI在资源受限和隐私敏感环境中的实际应用具有重要意义。"}}
{"id": "2506.15266", "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments", "authors": ["Sungen Hahm", "Heejin Kim", "Gyuseong Lee", "Hyunji Park", "Jaejin Lee"], "summary": "To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15266v1", "AI": {"title_translation": "Thunder-DeID：韩国法院判决书的准确高效去识别框架", "tldr": "该论文提出了Thunder-DeID，一个用于韩国法院判决书的去识别框架，通过构建首个韩语法律数据集、系统分类PII和开发端到端DNN管道，实现了最先进的性能，解决了当前去识别流程的不足和法律定义模糊的问题。", "motivation": "韩国司法系统在公开法院判决书时面临大规模去识别的挑战，现有流程不足且个人标识符的法律定义模糊，难以满足严格的法律要求和技术解决方案的需求。", "method": "提出了一个名为Thunder-DeID的去识别框架，具体包括：(i) 构建并发布了首个包含标注判决书和实体提及列表的韩语法律数据集；(ii) 引入了个人可识别信息（PII）的系统分类；(iii) 开发了一个基于深度神经网络（DNN）的端到端去识别管道。", "result": "实验结果表明，该模型在法院判决书的去识别方面达到了最先进的性能。", "conclusion": "Thunder-DeID框架通过其创新的方法，有效解决了韩国法院判决书大规模去识别的挑战，并实现了卓越的性能，从而平衡了司法公开和个人数据保护的需求。", "translation": "为了平衡司法公开和个人数据保护，韩国司法机构要求在公开法院判决书之前进行去识别。然而，当前的去识别过程不足以大规模处理法院判决书，同时遵守严格的法律要求。此外，个人标识符的法律定义和分类模糊，不适合技术解决方案。为了解决这些挑战，我们提出了一个名为Thunder-DeID的去识别框架，该框架符合相关法律和实践。具体来说，我们（i）构建并发布了第一个包含标注判决书及相应实体提及列表的韩语法律数据集，（ii）引入了个人可识别信息（PII）的系统分类，以及（iii）开发了一个基于深度神经网络（DNN）的端到端去识别管道。我们的实验结果表明，我们的模型在法院判决书的去识别方面达到了最先进的性能。", "summary": "本论文针对韩国法院判决书大规模去识别的挑战，提出了Thunder-DeID框架。该框架通过构建首个韩语法律去识别数据集、系统化分类个人可识别信息（PII），并开发基于深度神经网络（DNN）的端到端去识别管道，有效解决了现有流程的不足和法律定义模糊的问题。实验证明，该模型在法院判决书的去识别方面取得了最先进的性能。", "keywords": "去识别, 韩国法院判决书, 深度神经网络, 个人可识别信息, 法律数据集", "comments": "该论文的创新点在于构建了首个韩语法律去识别数据集，并提出了系统化的PII分类方法，结合深度学习技术，有效解决了法律文本去识别的实际难题，对于促进司法透明和保护个人隐私具有重要意义。"}}
{"id": "2506.15630", "title": "Non-uniform finite-element meshes defined by ray dynamics for Helmholtz problems", "authors": ["Martin Averseng", "Jeffrey Galkowski", "Euan A. Spence"], "summary": "The $h$-version of the finite-element method ($h$-FEM) applied to the\nhigh-frequency Helmholtz equation has been a classic topic in numerical\nanalysis since the 1990s. It is now rigorously understood that (using piecewise\npolynomials of degree $p$ on a mesh of a maximal width $h$) the conditions\n\"$(hk)^p \\rho$ sufficiently small\" and \"$(hk)^{2p} \\rho$ sufficiently small\"\nguarantee, respectively, $k$-uniform quasioptimality (QO) and bounded relative\nerror (BRE), where $\\rho$ is the norm of the solution operator with $\\rho\\sim\nk$ for non-trapping problems. Empirically, these conditions are observed to be\noptimal in the context of $h$-FEM with a uniform mesh. This paper demonstrates\nthat QO and BRE can be achieved using certain non-uniform meshes that violate\nthe conditions above on $h$ and involve coarser meshes away from trapping and\nin the perfectly matched layer (PML). The main theorem details how varying the\nmeshwidth in one region affects errors both in that region and elsewhere. One\nnotable consequence is that, for any scattering problem (trapping or\nnontrapping), in the PML one only needs $hk$ to be sufficiently small; i.e.\nthere is no pollution in the PML.\n  The motivating idea for the analysis is that the Helmholtz data-to-solution\nmap behaves differently depending on the locations of both the measurement and\ndata, in particular, on the properties of billiards trajectories (i.e. rays)\nthrough these sets. Because of this, it is natural that the approximation\nrequirements for finite-element spaces in a subset should depend on the\nproperties of billiard rays through that set. Inserting this behaviour into the\nlatest duality arguments for the FEM applied to the high-frequency Helmholtz\nequation allows us to retain detailed information about the influence of\n$\\textit{both}$ the mesh structure $\\textit{and}$ the behaviour of the true\nsolution on local errors in FEM.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15630v1", "AI": {"title_translation": "赫姆霍兹问题中由射线动力学定义的非均匀有限元网格", "tldr": "本文提出了一种基于射线动力学定义的非均匀有限元网格，用于解决高频赫姆霍兹问题，证明了在特定条件下，即使网格不均匀也能实现准最优性和有界相对误差，并且在完美匹配层(PML)中不存在污染。", "motivation": "传统的有限元方法（h-FEM）在高频赫姆霍兹方程中的准最优性（QO）和有界相对误差（BRE）条件是基于均匀网格的，且这些条件在经验上被认为是最佳的。然而，赫姆霍兹数据-解映射的行为会因测量和数据位置以及射线轨迹（即射线）的性质而异，这表明有限元空间的近似要求应依赖于通过该子集的射线属性。因此，需要探索如何在非均匀网格下也能满足这些误差条件，并考虑射线动力学对网格划分的影响。", "method": "本文通过将赫姆霍兹数据-解映射的局部行为（基于台球轨迹/射线动力学）整合到应用于高频赫姆霍兹方程的最新有限元对偶论证中，来构建非均匀有限元网格。这种方法允许在保持网格结构和真实解行为对局部误差影响的详细信息的同时，实现准最优性（QO）和有界相对误差（BRE）。", "result": "结果表明，即使在违反传统h-FEM条件下，通过使用某些非均匀网格（在陷阱区域外和完美匹配层PML中使用更粗的网格），也可以实现k-均匀准最优性（QO）和有界相对误差（BRE）。主要定理详细说明了在一个区域内改变网格宽度如何影响该区域及其他地方的误差。一个显著的推论是，对于任何散射问题（无论是否陷阱），在PML中仅需要hk足够小，即PML中没有污染。", "conclusion": "本文证明了通过考虑射线动力学定义的非均匀网格，可以有效地解决高频赫姆霍兹问题，并在某些区域使用更粗的网格时仍能保证近似精度，尤其是在PML中消除了污染问题，为有限元方法的网格设计提供了新的视角和灵活性。", "translation": "自20世纪90年代以来，应用于高频赫姆霍兹方程的有限元方法（h-FEM）一直是数值分析中的经典课题。现在已经严格理解到（在使用最大宽度为h的网格上的p次分段多项式时），条件“$(hk)^p \\rho$足够小”和“$(hk)^{2p} \\rho$足够小”分别保证了k-均匀准最优性（QO）和有界相对误差（BRE），其中$\\rho$是解算子范数，对于非陷阱问题$\\rho\\sim k$。根据经验，这些条件在均匀网格的h-FEM背景下被认为是最佳的。本文证明，通过使用某些违反上述h条件并涉及在陷阱区域外和完美匹配层（PML）中使用更粗网格的非均匀网格，也可以实现QO和BRE。主要定理详细说明了在一个区域内改变网格宽度如何影响该区域及其他地方的误差。一个显著的推论是，对于任何散射问题（无论是否陷阱），在PML中仅需要hk足够小；即PML中没有污染。分析的启发性思想是，赫姆霍兹数据-解映射的行为因测量和数据的位置而异，特别是取决于通过这些集合的台球轨迹（即射线）的性质。正因为如此，子集中有限元空间的近似要求应取决于通过该集合的台球射线的性质，这是很自然的。将这种行为引入到应用于高频赫姆霍兹方程的最新有限元对偶论证中，使我们能够保留关于网格结构和真实解行为对有限元局部误差影响的详细信息。", "summary": "本文研究了高频赫姆霍兹方程中有限元方法（h-FEM）的网格设计。在已知均匀网格下实现准最优性（QO）和有界相对误差（BRE）的条件基础上，作者提出并证明了使用基于射线动力学定义的非均匀网格同样可以达到这些误差保证。研究发现，即使在陷阱区域外和完美匹配层（PML）中使用更粗的网格，也能实现高精度，并且特别指出PML中不存在数值污染。这为赫姆霍兹问题的网格自适应方法提供了新的理论基础。", "keywords": "非均匀网格, 有限元方法, 赫姆霍兹方程, 射线动力学, 完美匹配层", "comments": "这篇论文的创新之处在于，它挑战了传统上对高频赫姆霍兹问题中有限元方法均匀网格的依赖，并引入了基于射线动力学的非均匀网格设计理念。通过将局部解的行为（与射线轨迹相关）融入到对偶分析中，该研究提供了一种更灵活且可能更高效的网格划分策略。其重要性在于，它不仅理论上证明了非均匀网格的可行性，而且在实践中可能允许在计算成本和精度之间取得更好的平衡，特别是在处理完美匹配层等特定区域时，避免了不必要的网格细化，从而减少了计算量。"}}
{"id": "2506.15380", "title": "Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning", "authors": ["Taegeun Yang", "Jiwoo Hwang", "Jeil Jeong", "Minsung Yoon", "Sung-Eui Yoon"], "summary": "We propose a hierarchical reinforcement learning (HRL) framework for\nefficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.\nOur approach combines interaction-based obstacle property estimation with\nstructured pushing strategies, facilitating the dynamic manipulation of\nunforeseen obstacles while adhering to a pre-planned global path. The\nhigh-level policy generates pushing commands that consider environmental\nconstraints and path-tracking objectives, while the low-level policy precisely\nand stably executes these commands through coordinated whole-body movements.\nComprehensive simulation-based experiments demonstrate improvements in\nperforming NAMO tasks, including higher success rates, shortened traversed path\nlength, and reduced goal-reaching times, compared to baselines. Additionally,\nablation studies assess the efficacy of each component, while a qualitative\nanalysis further validates the accuracy and reliability of the real-time\nobstacle property estimation.", "comment": "8 pages, 6 figures, Accepted to IROS 2025. Supplementary Video:\n  https://youtu.be/sZ8_z7sYVP0", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15380v1", "AI": {"title_translation": "通过分层策略学习实现移动机械手在可移动障碍物中的高效导航", "tldr": "本文提出了一种分层强化学习框架，用于移动机械手在可移动障碍物中进行高效导航，通过结合基于交互的障碍物属性估计和结构化推动策略，提高了成功率并缩短了路径和时间。", "motivation": "在可移动障碍物中进行导航（NAMO）是一项挑战，尤其是在动态操作不可预见的障碍物时。现有方法可能效率不高或无法有效处理动态障碍物。", "method": "本文提出了一种分层强化学习（HRL）框架。高层策略生成考虑环境约束和路径跟踪目标的推动命令，低层策略通过协调全身运动精确稳定地执行这些命令。该方法结合了基于交互的障碍物属性估计和结构化推动策略。", "result": "与基线相比，在执行NAMO任务时，成功率更高、遍历路径长度更短、到达目标时间更少。此外，消融研究评估了每个组件的功效，定性分析进一步验证了实时障碍物属性估计的准确性和可靠性。", "conclusion": "通过分层强化学习框架，结合障碍物属性估计和结构化推动策略，可以显著提高移动机械手在可移动障碍物中导航的效率、成功率和鲁棒性。", "translation": "我们提出了一种分层强化学习（HRL）框架，用于使用移动机械手在可移动障碍物中进行高效导航（NAMO）。我们的方法将基于交互的障碍物属性估计与结构化推动策略相结合，从而促进了对不可预见障碍物的动态操作，同时遵循预先规划的全局路径。高层策略生成考虑环境约束和路径跟踪目标的推动命令，而低层策略通过协调全身运动精确稳定地执行这些命令。全面的基于模拟的实验表明，与基线相比，执行NAMO任务的性能有所提高，包括更高的成功率、更短的遍历路径长度和更短的到达目标时间。此外，消融研究评估了每个组件的功效，而定性分析进一步验证了实时障碍物属性估计的准确性和可靠性。", "summary": "本文提出了一种用于移动机械手在可移动障碍物中导航的分层强化学习（HRL）框架。该框架结合了基于交互的障碍物属性估计和结构化推动策略，以实现对不可预见障碍物的动态操作，同时遵循预设路径。高层策略负责推动命令的生成，低层策略负责精确执行。实验结果表明，与现有方法相比，该方法在成功率、路径长度和时间方面均有显著提升，并且验证了障碍物属性估计的准确性。", "keywords": "分层强化学习, 移动机械手, 可移动障碍物导航, 障碍物属性估计, 推动策略", "comments": "该论文的创新点在于提出了一个结合交互式障碍物属性估计和分层策略学习的HRL框架，有效地解决了移动机械手在动态可移动障碍物环境中导航的挑战。其分层设计使得系统能够同时处理高层规划和低层执行，提高了导航的效率和鲁棒性。该方法在模拟中的显著性能提升，预示了其在实际应用中的巨大潜力。"}}
{"id": "2506.15033", "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "authors": ["Gary Song Yan", "Yusen Zhang", "Jinyu Zhao", "Hao Zhang", "Zhangping Yang", "Guanye Xiong", "Yanfei Liu", "Tao Zhang", "Yujie He", "Siyuan Tian", "Yao Gou", "Min Li"], "summary": "In this pioneering study, we introduce StyleWallfacer, a groundbreaking\nunified training and inference framework, which not only addresses various\nissues encountered in the style transfer process of traditional methods but\nalso unifies the framework for different tasks. This framework is designed to\nrevolutionize the field by enabling artist level style transfer and text driven\nstylization. First, we propose a semantic-based style injection method that\nuses BLIP to generate text descriptions strictly aligned with the semantics of\nthe style image in CLIP space. By leveraging a large language model to remove\nstyle-related descriptions from these descriptions, we create a semantic gap.\nThis gap is then used to fine-tune the model, enabling efficient and drift-free\ninjection of style knowledge. Second, we propose a data augmentation strategy\nbased on human feedback, incorporating high-quality samples generated early in\nthe fine-tuning process into the training set to facilitate progressive\nlearning and significantly reduce its overfitting. Finally, we design a\ntraining-free triple diffusion process using the fine-tuned model, which\nmanipulates the features of self-attention layers in a manner similar to the\ncross-attention mechanism. Specifically, in the generation process, the key and\nvalue of the content-related process are replaced with those of the\nstyle-related process to inject style while maintaining text control over the\nmodel. We also introduce query preservation to mitigate disruptions to the\noriginal content. Under such a design, we have achieved high-quality\nimage-driven style transfer and text-driven stylization, delivering\nartist-level style transfer results while preserving the original image\ncontent. Moreover, we achieve image color editing during the style transfer\nprocess for the first time.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15033v1", "AI": {"title_translation": "突破风格智子：我们真的要限制风格迁移中的想象力吗？", "tldr": "本研究引入了StyleWallfacer，一个开创性的统一训练和推理框架，旨在解决传统风格迁移中的各种问题，并实现艺术家级别的图像驱动和文本驱动风格化，同时首次实现图像色彩编辑。", "motivation": "传统风格迁移方法存在各种问题，并且缺乏一个统一的框架来处理不同的任务。本研究旨在通过实现艺术家级别和文本驱动的风格迁移来彻底改变该领域。", "method": "本研究提出了StyleWallfacer，一个统一的训练和推理框架。首先，提出了一种基于语义的风格注入方法，该方法使用BLIP在CLIP空间中生成与风格图像语义严格对齐的文本描述，然后利用大型语言模型移除与风格相关的描述以创建语义间隙，并用此间隙微调模型以实现高效、无漂移的风格知识注入。其次，提出了一种基于人类反馈的数据增强策略，将微调过程早期生成的高质量样本纳入训练集以促进渐进式学习并显著减少过拟合。最后，设计了一个使用微调模型的无训练三重扩散过程，该过程以类似于交叉注意力机制的方式操作自注意力层的特征，通过将内容相关过程的键和值替换为风格相关过程的键和值来注入风格，同时保持对模型的文本控制并引入查询保留以减轻对原始内容的干扰。", "result": "实现了高质量的图像驱动风格迁移和文本驱动风格化，提供了艺术家级别的风格迁移结果，同时保留了原始图像内容。首次在风格迁移过程中实现了图像色彩编辑。", "conclusion": "所提出的StyleWallfacer框架实现了艺术家级别、高质量、内容保留的图像驱动和文本驱动风格迁移，并且独特地实现了图像色彩编辑功能。", "translation": "在这项开创性研究中，我们引入了StyleWallfacer，一个突破性的统一训练和推理框架，它不仅解决了传统方法在风格迁移过程中遇到的各种问题，而且统一了不同任务的框架。该框架旨在通过实现艺术家级别的风格迁移和文本驱动的风格化来彻底改变该领域。首先，我们提出了一种基于语义的风格注入方法，该方法使用BLIP在CLIP空间中生成与风格图像语义严格对齐的文本描述。通过利用大型语言模型从这些描述中移除与风格相关的描述，我们创建了一个语义间隙。然后利用此间隙微调模型，从而实现高效、无漂移的风格知识注入。其次，我们提出了一种基于人类反馈的数据增强策略，将微调过程早期生成的高质量样本纳入训练集，以促进渐进式学习并显著减少其过拟合。最后，我们设计了一个使用微调模型的无训练三重扩散过程，该过程以类似于交叉注意力机制的方式操作自注意力层的特征。具体来说，在生成过程中，将内容相关过程的键和值替换为风格相关过程的键和值，以注入风格，同时保持对模型的文本控制。我们还引入了查询保留以减轻对原始内容的干扰。在这种设计下，我们实现了高质量的图像驱动风格迁移和文本驱动风格化，提供了艺术家级别的风格迁移结果，同时保留了原始图像内容。此外，我们首次在风格迁移过程中实现了图像色彩编辑。", "summary": "StyleWallfacer是一个统一的训练和推理框架，旨在解决传统风格迁移问题，并实现艺术家级别的图像驱动和文本驱动风格化。它通过语义风格注入、基于人类反馈的数据增强和无训练的三重扩散过程，实现了高质量的风格迁移，同时保留了原始内容并首次实现了图像色彩编辑。", "keywords": "风格迁移, 扩散模型, 语义注入, 文本驱动风格化, 人类反馈", "comments": "该论文引入了一个新颖的统一框架StyleWallfacer，其在风格迁移领域的多方面方法具有创新性。其主要优势在于将基于语义的风格注入与大型语言模型结合，利用人类反馈进行数据增强，并采用无训练的扩散过程。实现艺术家级别结果、内容保留，特别是首次实现图像色彩编辑的能力，突显了其在推动该领域超越传统限制方面的重要性。"}}
{"id": "2506.14828", "title": "Accurate and Uncertainty-Aware Multi-Task Prediction of HEA Properties Using Prior-Guided Deep Gaussian Processes", "authors": ["Sk Md Ahnaf Akif Alvi", "Mrinalini Mulukutla", "Nicolas Flores", "Danial Khatamsaz", "Jan Janssen", "Danny Perez", "Douglas Allaire", "Vahid Attari", "Raymundo Arroyave"], "summary": "Surrogate modeling techniques have become indispensable in accelerating the\ndiscovery and optimization of high-entropy alloys(HEAs), especially when\nintegrating computational predictions with sparse experimental observations.\nThis study systematically evaluates the fitting performance of four prominent\nsurrogate models conventional Gaussian Processes(cGP), Deep Gaussian\nProcesses(DGP), encoder-decoder neural networks for multi-output regression and\nXGBoost applied to a hybrid dataset of experimental and computational\nproperties in the AlCoCrCuFeMnNiV HEA system. We specifically assess their\ncapabilities in predicting correlated material properties, including yield\nstrength, hardness, modulus, ultimate tensile strength, elongation, and average\nhardness under dynamic and quasi-static conditions, alongside auxiliary\ncomputational properties. The comparison highlights the strengths of\nhierarchical and deep modeling approaches in handling heteroscedastic,\nheterotopic, and incomplete data commonly encountered in materials informatics.\nOur findings illustrate that DGP infused with machine learning-based prior\noutperform other surrogates by effectively capturing inter-property\ncorrelations and input-dependent uncertainty. This enhanced predictive accuracy\npositions advanced surrogate models as powerful tools for robust and\ndata-efficient materials design.", "comment": "Deep Gaussian Processes Multi-task Gaussian Processes High Entropy\n  Alloys", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14828v1", "AI": {"title_translation": "使用先验引导深度高斯过程对高熵合金性能进行准确且不确定性感知的多任务预测", "tldr": "该研究系统评估了四种替代模型在高熵合金性能预测上的表现，发现融入机器学习先验的深度高斯过程（DGP）在捕获属性间相关性和不确定性方面表现最佳。", "motivation": "替代建模技术在加速高熵合金（HEAs）的发现和优化中不可或缺，尤其是在整合计算预测与稀疏实验观察时。", "method": "本研究系统评估了四种主流替代模型（传统高斯过程、深度高斯过程、用于多输出回归的编码器-解码器神经网络和XGBoost）在AlCoCrCuFeMnNiV高熵合金体系中混合实验和计算属性数据集上的拟合性能。研究专门评估了它们预测相关材料性能（包括屈服强度、硬度、模量、极限拉伸强度、伸长率以及动静态条件下的平均硬度）以及辅助计算属性的能力。", "result": "研究结果表明，融入了机器学习先验的深度高斯过程（DGP）通过有效捕获属性间相关性和输入依赖的不确定性，优于其他替代模型。比较突出了分层和深度建模方法在处理材料信息学中常见的异方差、异位和不完整数据方面的优势。", "conclusion": "增强的预测精度使先进的替代模型成为稳健和数据高效材料设计的强大工具。", "translation": "替代建模技术在加速高熵合金（HEAs）的发现和优化中不可或缺，尤其是在整合计算预测与稀疏实验观察时。本研究系统评估了四种主流替代模型——传统高斯过程（cGP）、深度高斯过程（DGP）、用于多输出回归的编码器-解码器神经网络和XGBoost——在AlCoCrCuFeMnNiV高熵合金体系中实验和计算属性混合数据集上的拟合性能。我们专门评估了它们预测相关材料性能的能力，包括屈服强度、硬度、模量、极限拉伸强度、伸长率以及动静态条件下的平均硬度，以及辅助计算属性。比较突出了分层和深度建模方法在处理材料信息学中常见的异方差、异位和不完整数据方面的优势。我们的研究结果表明，融入了机器学习先验的深度高斯过程（DGP）通过有效捕获属性间相关性和输入依赖的不确定性，优于其他替代模型。这种增强的预测精度使先进的替代模型成为稳健和数据高效材料设计的强大工具。", "summary": "本研究旨在加速高熵合金（HEAs）的发现和优化，尤其是在整合稀疏实验观察与计算预测时。论文系统评估了四种主流替代模型（传统高斯过程、深度高斯过程、编码器-解码器神经网络和XGBoost）在AlCoCrCuFeMnNiV高熵合金体系混合数据集上的性能，涉及屈服强度、硬度、模量等多种相关材料属性的预测。研究发现，融入机器学习先验的深度高斯过程（DGP）在有效捕获属性间相关性和输入依赖的不确定性方面表现最佳，优于其他模型。这一发现表明，先进的替代模型是实现稳健和数据高效材料设计的有力工具。", "keywords": "高熵合金, 深度高斯过程, 替代建模, 多任务预测, 不确定性量化", "comments": "本论文的创新之处在于将机器学习先验融入深度高斯过程（DGP），从而显著提升了高熵合金性能预测的准确性，并能有效处理异方差、异位和不完整数据，同时量化输入依赖的不确定性。这为材料信息学领域提供了一个更强大、更鲁棒的预测工具，对于加速高熵合金的设计和优化具有重要意义。"}}
{"id": "2506.15301", "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment", "authors": ["Shrestha Ghosh", "Moritz Schneider", "Carina Reinicke", "Carsten Eickhoff"], "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15301v1", "AI": {"title_translation": "队列发现：LLM辅助临床试验招募综述", "tldr": "本综述首次分析了临床试验中试验-患者匹配任务，并考察了新兴的LLM辅助方法、现有基准、挑战及未来方向。", "motivation": "尽管大型语言模型（LLMs）在通用自然语言处理任务上取得了巨大进步，但它们在临床试验招募等关键领域的应用仍然有限。由于临床试验设计和患者数据（结构化和非结构化文本）都需要知识聚合和推理能力，LLMs在此任务中具有潜力。传统的匹配方法是针对特定试验的，而LLMs有望构建更通用的解决方案。然而，当前LLM辅助方法的应用依赖于专有模型和薄弱的评估基准。", "method": "本综述首次分析了试验-患者匹配任务，并将新兴的LLM辅助方法置于临床试验招募的背景下。它批判性地审查了现有基准、方法和评估框架，以及在临床研究中采用LLM技术面临的挑战和未来方向。", "result": "本综述批判性地审查了现有基准、方法和评估框架，并指出了在临床研究中采用LLM技术所面临的挑战。", "conclusion": "本综述旨在为LLM在临床试验招募中的应用提供全面的理解，并指出未来的研究方向，以克服现有挑战并充分利用LLM的潜力。", "translation": "大型语言模型（LLMs）的最新进展极大地改善了通用领域的自然语言处理任务。然而，它们在临床试验招募等关键领域的应用仍然有限。由于试验以自然语言设计，患者数据以结构化和非结构化文本形式表示，因此匹配试验和患者的任务受益于LLMs的知识聚合和推理能力。经典方法是针对特定试验的，而LLMs凭借其整合分布式知识的能力，有望构建一个更通用的解决方案。然而，最近LLM辅助方法的应用依赖于专有模型和薄弱的评估基准。在本综述中，我们首次分析了试验-患者匹配任务，并将新兴的基于LLM的方法置于临床试验招募的背景下。我们批判性地审查了现有基准、方法和评估框架，在临床研究中采用LLM技术面临的挑战以及令人兴奋的未来方向。", "summary": "本综述探讨了大型语言模型（LLMs）在临床试验招募中进行试验-患者匹配的应用潜力。尽管LLMs在通用NLP任务中表现出色，但在临床领域的应用仍受限。文章分析了LLMs如何通过知识聚合和推理能力改进传统的试验特定匹配方法，并有望提供更通用的解决方案。同时，综述也批判性地审查了当前LLM辅助方法的评估基准、挑战，并提出了未来发展方向。", "keywords": "LLM, 临床试验招募, 队列发现, 试验-患者匹配, 综述", "comments": "这篇综述的重要性在于它首次系统性地分析了LLM在临床试验招募中的应用，填补了该领域研究的空白。它不仅肯定了LLM的潜力，也直指当前方法的不足（如依赖专有模型和弱评估基准），并提出了挑战与未来方向，对推动LLM在关键医疗领域的实际应用具有指导意义。"}}
{"id": "2506.15660", "title": "On the Upper Bounds for the Matrix Spectral Norm", "authors": ["Alexey Naumov", "Maxim Rakhuba", "Denis Ryapolov", "Sergey Samsonov"], "summary": "We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15660v1", "AI": {"title_translation": "矩阵谱范数的上界", "tldr": "提出了一种新的Counterbalance估计器，用于通过矩阵-向量乘积估计矩阵谱范数的上界，比现有方法更紧密，尤其适用于谱衰减快的矩阵。", "motivation": "现有方法在仅使用矩阵-向量乘积的情况下，估计矩阵谱范数的上界可能不够紧密，因此需要一种更精确的估计方法。", "method": "提出了一种新的“Counterbalance估计器”，该估计器通过矩阵-向量乘积提供谱范数的上界，并推导出其低估的概率保证。", "result": "所提出的Counterbalance估计器在合成和真实世界设置中，与标准方法（如幂法）相比，产生了显著更紧密（tighter）的谱范数上界。该方法对于谱衰减快的矩阵（如深度学习和逆问题中出现的矩阵）特别有效。", "conclusion": "Counterbalance估计器是一种有效且优越的矩阵谱范数上界估计方法，尤其适用于谱衰减快的矩阵，能够提供比传统方法更紧密的上界。", "translation": "我们考虑了仅使用矩阵-向量乘积来估计矩阵谱范数的问题。我们提出了一种新的Counterbalance估计器，它提供了范数的上界，并推导了其低估的概率保证。与幂法等标准方法相比，所提出的估计器在合成和真实世界设置中都产生了显著更紧密的上界。我们的方法对于谱衰减快的矩阵特别有效，例如深度学习和逆问题中出现的矩阵。", "summary": "本文提出了一种名为Counterbalance的新型估计器，旨在仅通过矩阵-向量乘积来估计矩阵的谱范数上界。该方法提供了范数的上界及其低估的概率保证。实验结果表明，与传统的幂法相比，Counterbalance估计器能够在合成数据和实际应用中提供更紧密的上界，尤其适用于深度学习和逆问题中常见的谱衰减快的矩阵。", "keywords": "矩阵谱范数, 上界, Counterbalance估计器, 矩阵-向量乘积, 幂法", "comments": "这项工作通过引入Counterbalance估计器，为仅依赖矩阵-向量乘积的矩阵谱范数估计提供了一个创新且更精确的解决方案。其主要贡献在于提高了上界的紧密性，特别是在处理特定类型的矩阵时，这对于计算资源受限或需要快速迭代的领域（如深度学习）具有重要意义。"}}
{"id": "2506.15402", "title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System", "authors": ["Miaoxin Pan", "Jinnan Li", "Yaowen Zhang", "Yi Yang", "Yufeng Yue"], "summary": "Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15402v1", "AI": {"title_translation": "MCOO-SLAM：一个多相机全向物体SLAM系统", "tldr": "MCOO-SLAM利用多相机全向视图实现复杂户外场景中鲁棒、一致的物体级SLAM。", "motivation": "大多数现有物体级SLAM方法依赖RGB-D或单目视图，存在视野窄、对遮挡敏感和深度感知有限的问题，尤其在大规模或户外环境，导致物体建模不准确和数据关联不可靠。", "method": "提出了MCOO-SLAM系统，充分利用环绕视图相机配置。整合了点特征和通过开放词汇语义增强的物体级地标。引入了语义-几何-时间融合策略用于鲁棒的物体关联。设计了全向闭环模块以实现视点不变的地点识别。构建的地图被抽象为分层3D场景图。", "result": "MCOO-SLAM在真实世界实验中实现了准确的定位和可扩展的物体级建图，提高了对遮挡、姿态变化和环境复杂性的鲁棒性。", "conclusion": "MCOO-SLAM通过利用多相机全向视图和创新的数据融合策略，显著提升了复杂户外环境中物体级SLAM的性能和鲁棒性。", "translation": "物体级SLAM提供结构化、语义丰富的环境表示，使其更具可解释性并适用于高级机器人任务。然而，大多数现有方法依赖于RGB-D传感器或单目视图，这些方法存在视野窄、对遮挡敏感以及深度感知有限的问题——尤其是在大规模或户外环境中。这些限制通常使系统只能从有限的视角观察物体部分视图，导致不准确的物体建模和不可靠的数据关联。在这项工作中，我们提出了MCOO-SLAM，一个新颖的多相机全向物体SLAM系统，它充分利用环绕视图相机配置，在复杂的户外场景中实现鲁棒、一致且语义丰富的建图。我们的方法整合了点特征和通过开放词汇语义增强的物体级地标。引入了一种语义-几何-时间融合策略，用于跨多个视图进行鲁棒的物体关联，从而提高一致性和准确的物体建模，并且设计了一个全向闭环模块，以使用场景级描述符实现视点不变的地点识别。此外，构建的地图被抽象为分层3D场景图，以支持下游推理任务。在真实世界中的大量实验表明，MCOO-SLAM实现了准确的定位和可扩展的物体级建图，提高了对遮挡、姿态变化和环境复杂性的鲁棒性。", "summary": "MCOO-SLAM是一个新颖的多相机全向物体SLAM系统，旨在克服现有方法在视野、遮挡和深度感知上的局限性。它利用环绕视图相机配置，结合点特征和语义增强的物体级地标，通过语义-几何-时间融合策略实现鲁棒的物体关联。系统还包括全向闭环模块和分层3D场景图构建。实验证明MCOO-SLAM在复杂户外场景中能实现准确的定位和鲁棒的物体级建图。", "keywords": "物体SLAM, 多相机, 全向, 语义建图, 场景图", "comments": "这篇论文的创新点在于充分利用多相机全向视图来解决传统SLAM在复杂户外环境中的局限性。其结合点特征和语义丰富物体级地标、引入语义-几何-时间融合策略以及全向闭环模块，显著提升了系统对遮挡和姿态变化的鲁棒性，并构建了支持高级推理的分层场景图，对于提升机器人环境感知能力具有重要意义。"}}
{"id": "2506.15078", "title": "Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study", "authors": ["Xianghong Fang", "Litao Guo", "Hengchao Chen", "Yuxuan Zhang", "XiaofanXia", "Dingjie Song", "Yexin Liu", "Hao Wang", "Harry Yang", "Yuan Yuan", "Qiang Sun"], "summary": "The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15078v1", "AI": {"title_translation": "增强带有分布匹配的向量量化：一项理论与实证研究", "tldr": "现有向量量化存在训练不稳定和码本崩溃问题，本文通过Wasserstein距离对齐特征与码本分布，解决了这些问题，提高了码本利用率并减少了量化误差。", "motivation": "现有向量量化方法存在训练不稳定（直通估计器导致的梯度差异）和码本崩溃（码本利用率低）问题。这些问题源于特征和码本分布不匹配，导致码本缺乏代表性并造成信息损失。", "method": "采用Wasserstein距离来对齐特征和码本的分布。", "result": "实现了近100%的码本利用率，并显著减少了量化误差。理论和实证分析都验证了所提出方法的有效性。", "conclusion": "通过使用Wasserstein距离进行分布匹配，可以有效解决向量量化中的训练不稳定和码本崩溃问题，提高码本利用率并降低量化误差。", "translation": "自回归模型的成功在很大程度上取决于向量量化的有效性，这是一种通过将连续特征映射到可学习码本中最近的码向量来离散化连续特征的技术。现有向量量化方法中的两个关键问题是训练不稳定性和码本崩溃。训练不稳定性源于直通估计器引入的梯度差异，尤其是在存在显著量化误差的情况下，而码本崩溃则发生在训练期间只使用了码本的一小部分码向量时。对这些问题的深入检查表明，它们主要是由特征和码向量分布之间的不匹配驱动的，这导致了不具代表性的码向量和压缩过程中大量数据信息损失。为了解决这个问题，我们采用Wasserstein距离来对齐这两个分布，实现了近100%的码本利用率并显著减少了量化误差。理论和实证分析都验证了所提出方法的有效性。", "summary": "本文针对向量量化中训练不稳定和码本崩溃问题，提出通过Wasserstein距离对齐特征与码本分布。研究发现，这些问题源于分布不匹配，导致码本代表性不足和信息损失。实验和理论分析表明，该方法有效提高了码本利用率至近100%，并显著降低了量化误差。", "keywords": "向量量化, 分布匹配, Wasserstein距离, 码本崩溃, 训练稳定性", "comments": "这篇论文通过引入分布匹配（使用Wasserstein距离）来解决向量量化中的核心问题，即训练不稳定性和码本崩溃，具有创新性。它从根本上解决了特征与码本分布不匹配的问题，提升了码本的利用效率和数据的压缩质量，对自回归模型的发展具有重要意义。"}}
{"id": "2506.15304", "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification", "authors": ["Negar Foroutan", "Jakhongir Saydaliev", "Ye Eun Kim", "Antoine Bosselut"], "summary": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.", "comment": "Submitted to EMNLP", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15304v1", "AI": {"title_translation": "ConLID：低资源语言识别的监督对比学习", "tldr": "ConLID提出了一种监督对比学习方法，用于解决低资源语言识别中的领域偏差和类别不平衡问题，显著提升了模型在域外数据上的性能。", "motivation": "在多语言大型语言模型预训练语料库的构建中，语言识别（LID）是关键一步。然而，许多LID模型训练研究侧重于收集多样化的训练数据以提高性能，但低资源语言（通常局限于单一领域数据，如圣经）的识别表现仍然不佳。为了解决这些类别不平衡和偏差问题，本研究提出了新的方法。", "method": "我们提出了一种新颖的监督对比学习（SCL）方法，旨在为低资源语言学习领域不变的表示。", "result": "通过广泛的分析，我们展示了我们的方法将低资源语言在域外数据上的LID性能提高了3.2%。", "conclusion": "本研究提出的监督对比学习方法有效提升了低资源语言的LID模型性能。", "translation": "语言识别（LID）是整理网络爬取的多语言大型语言模型预训练语料库的关键一步。尽管许多关于LID模型训练的研究侧重于收集多样化的训练数据以提高性能，但低资源语言——通常仅限于单一领域数据，如圣经——的识别表现仍然不佳。为了解决这些类别不平衡和偏差问题，我们提出了一种新颖的监督对比学习（SCL）方法，旨在为低资源语言学习领域不变的表示。通过广泛的分析，我们展示了我们的方法将低资源语言在域外数据上的LID性能提高了3.2%，证明了其在增强LID模型方面的有效性。", "summary": "本文提出了一种名为ConLID的监督对比学习（SCL）方法，旨在解决低资源语言识别（LID）中存在的类别不平衡和领域偏差问题。针对低资源语言数据稀缺且通常局限于单一领域导致LID性能不佳的现状，ConLID通过学习领域不变的表示来提升模型泛化能力。实验结果表明，该方法能够将低资源语言在域外数据上的LID性能提高3.2%，证明了其在增强LID模型方面的有效性，这对于构建高质量的多语言LLM预训练语料库至关重要。", "keywords": "语言识别, 低资源语言, 监督对比学习, 领域不变表示", "comments": "本文的创新点在于将监督对比学习应用于低资源语言识别任务，以解决传统方法在处理类别不平衡和领域偏差时遇到的挑战。通过学习领域不变的表示，该方法有望显著提升模型在实际应用中的泛化能力，尤其是在数据稀缺的语言场景下。其重要性体现在对多语言大型语言模型语料库构建的直接贡献。"}}
{"id": "2506.14822", "title": "Analysis and conditional optimization of projection estimates for the distribution of random variable using Legendre polynomials", "authors": ["Tatyana A. Averina", "Konstantin A. Rybakov"], "summary": "Algorithms for jointly obtaining projection estimates of the density and\ndistribution function of a random variable using the Legendre polynomials are\nproposed. For these algorithms, a problem of the conditional optimization is\nsolved. Such an optimization allows one increasing the approximation accuracy\nwith a minimum computational costs. The proposed algorithms are tested on\nexamples with different degree of smoothness of the density.", "comment": null, "cate": "stat.CO", "url": "http://arxiv.org/abs/2506.14822v1", "AI": {"title_translation": "使用勒让德多项式分析和条件优化随机变量分布的投影估计", "tldr": "提出并优化了使用勒让德多项式对随机变量密度和分布函数进行投影估计的算法，以提高精度并降低计算成本。", "motivation": "提高随机变量密度和分布函数投影估计的近似精度，同时保持最低的计算成本。", "method": "提出使用勒让德多项式联合获取随机变量密度和分布函数投影估计的算法，并通过解决条件优化问题来提高近似精度。", "result": "提出的算法在不同密度平滑度示例上进行了测试，结果表明通过条件优化可以提高近似精度并降低计算成本。", "conclusion": "通过条件优化，可以有效提高使用勒让德多项式进行随机变量密度和分布函数投影估计的精度，同时保持计算效率。", "translation": "提出了使用勒让德多项式联合获取随机变量密度和分布函数投影估计的算法。针对这些算法，解决了条件优化问题。这种优化可以在计算成本最低的情况下提高近似精度。所提出的算法在具有不同密度平滑度的示例上进行了测试。", "summary": "本文提出了一系列利用勒让德多项式对随机变量的密度和分布函数进行投影估计的算法。为了提高近似精度并最小化计算成本，研究解决了这些算法的条件优化问题。所提出的算法已在不同平滑度密度的示例上进行了验证。", "keywords": "勒让德多项式, 投影估计, 条件优化, 随机变量分布, 密度估计", "comments": "这篇论文的创新点在于将条件优化引入到基于勒让德多项式的投影估计中，旨在以最小的计算代价提高估计精度，这对于实际应用中的效率提升具有重要意义。"}}
{"id": "2506.15295", "title": "A theory of Lending Protocols in DeFi", "authors": ["Massimo Bartoletti", "Enrico Lipparini"], "summary": "Lending protocols are one of the main applications of Decentralized Finance\n(DeFi), enabling crypto-assets loan markets with a total value estimated in the\ntens of billions of dollars. Unlike traditional lending systems, these\nprotocols operate without relying on trusted authorities or off-chain\nenforcement mechanisms. To achieve key economic goals such as stability of the\nloan market, they devise instead trustless on-chain mechanisms, such as\nrewarding liquidators who repay the loans of under-collateralized borrowers by\nawarding them part of the borrower's collateral. The complexity of these\nincentive mechanisms, combined with their entanglement in low-level\nimplementation details, makes it challenging to precisely assess the structural\nand economic properties of lending protocols, as well as to analyze user\nstrategies and attacks. Crucially, since participation is open to anyone, any\nweaknesses in the incentive mechanism may give rise to unintended emergent\nbehaviours, or even enable adversarial strategies aimed at making profits to\nthe detriment of legit users, or at undermining the stability of the protocol.\nIn this work, we propose a formal model of lending protocols that captures the\nessential features of mainstream platforms, enabling us to identify and prove\nkey properties related to their economic and strategic dynamics.", "comment": null, "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.15295v1", "AI": {"title_translation": "DeFi中借贷协议的理论", "tldr": "DeFi借贷协议因其复杂的链上激励机制而难以分析，且开放参与可能导致漏洞。本文提出一个形式化模型来分析其经济和策略动态。", "motivation": "DeFi借贷协议的激励机制复杂且与底层实现细节交织，导致难以精确评估其结构和经济特性，也难以分析用户策略和攻击。此外，由于参与是开放的，激励机制的弱点可能导致意外的突发行为或恶意策略，损害合法用户或破坏协议稳定性。", "method": "本文提出了一个借贷协议的形式化模型，该模型捕捉了主流平台的关键特征，旨在识别并证明与其经济和策略动态相关的关键属性。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "借贷协议是去中心化金融（DeFi）的主要应用之一，它支持加密资产贷款市场，总价值估计达数百亿美元。与传统借贷系统不同，这些协议的运作不依赖于受信任的机构或链下执行机制。为了实现贷款市场稳定性等关键经济目标，它们设计了无需信任的链上机制，例如通过奖励清算人（清偿抵押不足借款人的贷款，并获得部分借款人抵押品）来激励他们。这些激励机制的复杂性，加上它们与底层实现细节的纠缠，使得精确评估借贷协议的结构和经济特性，以及分析用户策略和攻击变得具有挑战性。至关重要的是，由于参与是开放的，激励机制中的任何弱点都可能导致意想不到的突发行为，甚至促使旨在损害合法用户利益或破坏协议稳定性的对抗性策略。在这项工作中，我们提出了一个借贷协议的形式化模型，该模型捕捉了主流平台的关键特征，使我们能够识别并证明与其经济和策略动态相关的关键属性。", "summary": "借贷协议是DeFi的核心应用，但其复杂的链上激励机制使得精确评估其经济特性、用户策略和潜在攻击变得困难。开放的参与性可能导致激励机制的弱点被利用，产生非预期行为或恶意攻击。为解决此问题，本文提出了一个形式化模型，旨在捕捉主流DeFi借贷平台的关键特征，从而能够识别并证明其经济和策略动态相关的重要属性。", "keywords": "DeFi, 借贷协议, 形式化模型, 激励机制, 链上机制", "comments": "本文通过引入形式化模型来分析DeFi借贷协议，解决了当前协议复杂性高、难以评估其结构和经济属性的问题。这对于增强DeFi系统的安全性、稳定性和可预测性至关重要，尤其是在一个高价值且开放的市场环境中。其创新点在于将严谨的形式化方法应用于分析新兴且复杂的DeFi领域。"}}
{"id": "2506.15450", "title": "SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle", "authors": ["Kun Liu", "Junhao Xiao", "Hao Lin", "Yue Cao", "Hui Peng", "Kaihong Huang", "Huimin Lu"], "summary": "Despite significant advancements in the research of aquatic-aerial robots,\nexisting configurations struggle to efficiently perform underwater, surface,\nand aerial movement simultaneously. In this paper, we propose a novel\nmultimodal surfing aquatic-aerial vehicle, SurfAAV, which efficiently\nintegrates underwater navigation, surface gliding, and aerial flying\ncapabilities. Thanks to the design of the novel differential thrust vectoring\nhydrofoil, SurfAAV can achieve efficient surface gliding and underwater\nnavigation without the need for a buoyancy adjustment system. This design\nprovides flexible operational capabilities for both surface and underwater\ntasks, enabling the robot to quickly carry out underwater monitoring\nactivities. Additionally, when it is necessary to reach another water body,\nSurfAAV can switch to aerial mode through a gliding takeoff, flying to the\ntarget water area to perform corresponding tasks. The main contribution of this\nletter lies in proposing a new solution for underwater, surface, and aerial\nmovement, designing a novel hybrid prototype concept, developing the required\ncontrol laws, and validating the robot's ability to successfully perform\nsurface gliding and gliding takeoff. SurfAAV achieves a maximum surface gliding\nspeed of 7.96 m/s and a maximum underwater speed of 3.1 m/s. The prototype's\nsurface gliding maneuverability and underwater cruising maneuverability both\nexceed those of existing aquatic-aerial vehicles.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15450v1", "AI": {"title_translation": "SurfAAV：一种新型多模态水空两栖冲浪飞行器的设计与实现", "tldr": "SurfAAV是一种新型水空两栖机器人，通过创新的差动推力矢量水翼设计，实现了高效的水下、水面滑翔和空中飞行，且性能优于现有水空两栖飞行器。", "motivation": "现有的水空两栖机器人难以同时高效地进行水下、水面和空中移动。", "method": "本文提出了一种新型多模态水空两栖飞行器SurfAAV，它有效地集成了水下导航、水面滑翔和空中飞行能力。通过创新的差动推力矢量水翼设计，SurfAAV无需浮力调节系统即可实现高效的水面滑翔和水下导航，并能通过滑翔起飞切换到空中模式。", "result": "SurfAAV的最大水面滑翔速度达到7.96米/秒，最大水下速度达到3.1米/秒。该原型机的表面滑翔机动性和水下巡航机动性均超过了现有的水空两栖飞行器。", "conclusion": "本文提出了一种用于水下、水面和空中移动的新解决方案，设计了一种新颖的混合原型概念，开发了所需的控制律，并验证了机器人成功执行水面滑翔和滑翔起飞的能力。", "translation": "尽管水空两栖机器人的研究取得了显著进展，但现有配置难以同时有效地进行水下、水面和空中移动。本文提出了一种新型多模态水空两栖飞行器SurfAAV，它有效地集成了水下导航、水面滑翔和空中飞行能力。得益于新型差动推力矢量水翼的设计，SurfAAV无需浮力调节系统即可实现高效的水面滑翔和水下导航。这种设计为水面和水下任务提供了灵活的操作能力，使机器人能够快速开展水下监测活动。此外，当需要到达另一个水体时，SurfAAV可以通过滑翔起飞切换到空中模式，飞往目标水域执行相应任务。本文的主要贡献在于提出了一种水下、水面和空中移动的新解决方案，设计了一种新颖的混合原型概念，开发了所需的控制律，并验证了机器人成功执行水面滑翔和滑翔起飞的能力。SurfAAV的最大水面滑翔速度达到7.96米/秒，最大水下速度达到3.1米/秒。该原型机的表面滑翔机动性和水下巡航机动性均超过了现有的水空两栖飞行器。", "summary": "本文介绍了一种名为SurfAAV的新型多模态水空两栖飞行器，旨在解决现有水空两栖机器人无法高效同时进行水下、水面和空中移动的问题。SurfAAV通过创新的差动推力矢量水翼设计，实现了高效的水下导航、水面滑翔和空中飞行能力，且无需浮力调节系统。该原型机展示出卓越的性能，最大水面滑翔速度达7.96米/秒，最大水下速度达3.1米/秒，其机动性优于现有同类产品，并能通过滑翔起飞实现跨水域飞行，为水下监测等任务提供了灵活高效的解决方案。", "keywords": "水空两栖机器人, 多模态, 差动推力矢量水翼, 水面滑翔, 滑翔起飞", "comments": "这篇论文的创新点在于提出了新型的差动推力矢量水翼设计，使得水空两栖机器人无需复杂的浮力调节系统即可高效实现多模态转换和运动，显著提升了机器人的集成度和操作灵活性。其能够实现跨水域的滑翔起飞和高效的水面滑翔，为未来的水下监测、环境采样等任务提供了更高效、更灵活的平台。"}}
{"id": "2506.15153", "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts", "authors": ["Yufei Liu", "Haoke Xiao", "Jiaxing Chai", "Yongcun Zhang", "Rong Wang", "Zijie Meng", "Zhiming Luo"], "summary": "The advent of Large Vision Models (LVMs) offers new opportunities for\nfew-shot medical image segmentation. However, existing training-free methods\nbased on LVMs fail to effectively utilize negative prompts, leading to poor\nperformance on low-contrast medical images. To address this issue, we propose\nSynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core\ninsight: improving the quality of negative prompts. To select point prompts in\na more reliable confidence map, we design a novel Confidence Map Synergy Module\nby combining the strengths of DINOv2 and SAM. Based on the confidence map, we\nselect the top-k pixels as the positive points set and choose the negative\npoints set using a Gaussian distribution, followed by independent K-means\nclustering for both sets. Then, these selected points are leveraged as\nhigh-quality prompts for SAM to get the segmentation results. Extensive\nexperiments demonstrate that SynPo achieves performance comparable to\nstate-of-the-art training-based few-shot methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15153v1", "AI": {"title_translation": "SynPo：通过高质量负提示提升免训练小样本医学图像分割", "tldr": "SynPo 是一种免训练的小样本医学图像分割方法，通过生成高质量的负提示，解决了现有方法在低对比度图像上的性能问题，并取得了与最先进的训练方法相当的性能。", "motivation": "现有基于大型视觉模型 (LVMs) 的免训练小样本医学图像分割方法未能有效利用负提示，导致在低对比度医学图像上表现不佳。", "method": "提出 SynPo，一种基于 LVMs (如 SAM) 的免训练小样本方法。核心在于提升负提示质量。设计了一个新的置信度图协同模块 (Confidence Map Synergy Module)，结合 DINOv2 和 SAM 的优势来选择更可靠的置信度图中的点提示。基于置信度图，选择 top-k 像素作为正点集，并使用高斯分布选择负点集，随后对两组点进行独立的 K-means 聚类。最后，利用这些选定的点作为 SAM 的高质量提示以获得分割结果。", "result": "广泛的实验表明，SynPo 实现了与最先进的基于训练的小样本方法相当的性能。", "conclusion": "通过提升负提示的质量，SynPo 能够有效解决现有免训练方法在低对比度医学图像分割上的不足，并达到了与训练方法相当的SOTA性能。", "translation": "大型视觉模型 (LVMs) 的出现为小样本医学图像分割提供了新的机遇。然而，现有基于 LVMs 的免训练方法未能有效利用负提示，导致在低对比度医学图像上表现不佳。为了解决这个问题，我们提出了 SynPo，一种基于 LVMs (例如 SAM) 的免训练小样本方法，其核心思想是：提高负提示的质量。为了在更可靠的置信度图中选择点提示，我们设计了一个新颖的置信度图协同模块，结合了 DINOv2 和 SAM 的优势。基于置信度图，我们选择 top-k 像素作为正点集，并使用高斯分布选择负点集，随后对两组点进行独立的 K-means 聚类。然后，这些选定的点被用作 SAM 的高质量提示以获得分割结果。大量的实验表明，SynPo 取得了与最先进的基于训练的小样本方法相当的性能。", "summary": "SynPo 是一种创新的免训练小样本医学图像分割方法，旨在解决现有方法在低对比度医学图像上因负提示利用不足而导致的性能问题。该方法通过引入置信度图协同模块（结合 DINOv2 和 SAM 的优势）来生成高质量的正负点提示。这些高质量提示随后被用于 SAM 进行精确分割。实验结果表明，SynPo 的性能与当前最先进的基于训练的小样本方法相当，展现了其在医学图像分割领域的潜力。", "keywords": "小样本分割, 医学图像, 免训练, 负提示, 大型视觉模型", "comments": "SynPo 的创新点在于提出了通过提升负提示质量来改进免训练小样本医学图像分割。它巧妙地结合了 DINOv2 和 SAM 的优势来构建更可靠的置信度图，并在此基础上生成高质量的正负提示，从而提升了在低对比度医学图像上的分割性能。其达到与训练方法相当的性能，表明了其在实际应用中的巨大潜力，尤其是在数据稀缺的医学领域。"}}
{"id": "2506.14796", "title": "PFMBench: Protein Foundation Model Benchmark", "authors": ["Zhangyang Gao", "Hao Wang", "Cheng Tan", "Chenrui Xu", "Mengdi Liu", "Bozhen Hu", "Linlin Chao", "Xiaoming Zhang", "Stan Z. Li"], "summary": "This study investigates the current landscape and future directions of\nprotein foundation model research. While recent advancements have transformed\nprotein science and engineering, the field lacks a comprehensive benchmark for\nfair evaluation and in-depth understanding. Since ESM-1B, numerous protein\nfoundation models have emerged, each with unique datasets and methodologies.\nHowever, evaluations often focus on limited tasks tailored to specific models,\nhindering insights into broader generalization and limitations. Specifically,\nresearchers struggle to understand the relationships between tasks, assess how\nwell current models perform across them, and determine the criteria in\ndeveloping new foundation models. To fill this gap, we present PFMBench, a\ncomprehensive benchmark evaluating protein foundation models across 38 tasks\nspanning 8 key areas of protein science. Through hundreds of experiments on 17\nstate-of-the-art models across 38 tasks, PFMBench reveals the inherent\ncorrelations between tasks, identifies top-performing models, and provides a\nstreamlined evaluation protocol. Code is available at\n\\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}.", "comment": null, "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.14796v1", "AI": {"title_translation": "PFMBench：蛋白质基础模型基准", "tldr": "当前蛋白质基础模型缺乏全面基准进行公平评估。本文提出了PFMBench，一个包含8个关键领域38个任务的全面基准，评估了17个模型，揭示了任务相关性并识别了高性能模型。", "motivation": "尽管蛋白质科学和工程领域取得了显著进展，但该领域缺乏一个全面的基准来公平评估和深入理解蛋白质基础模型。目前的评估往往局限于特定任务，阻碍了对模型泛化能力和局限性的深入了解，也难以理解任务间的关系或开发新模型的标准。", "method": "本文提出了PFMBench，一个全面的基准，用于评估蛋白质基础模型在涵盖蛋白质科学8个关键领域的38个任务上的表现。通过对17个最先进的模型在38个任务上进行数百次实验。", "result": "PFMBench揭示了任务之间的内在关联，识别了表现最佳的模型，并提供了一个简化的评估协议。", "conclusion": "PFMBench填补了蛋白质基础模型评估的空白，提供了一个全面的框架，有助于深入理解模型并指导未来的研究和开发。", "translation": "本研究调查了蛋白质基础模型研究的当前状况和未来方向。尽管最近的进展已经改变了蛋白质科学和工程，但该领域缺乏一个全面的基准，以进行公平评估和深入理解。自ESM-1B以来，许多蛋白质基础模型已经出现，每个模型都有独特的数据集和方法。然而，评估通常侧重于为特定模型量身定制的有限任务，阻碍了对更广泛泛化和局限性的深入了解。具体来说，研究人员难以理解任务之间的关系，评估当前模型在这些任务上的表现如何，并确定开发新基础模型的标准。为了填补这一空白，我们提出了PFMBench，这是一个全面的基准，评估蛋白质基础模型在涵盖蛋白质科学8个关键领域的38个任务上的表现。通过对17个最先进模型在38个任务上进行数百次实验，PFMBench揭示了任务之间的内在关联，确定了表现最佳的模型，并提供了一个简化的评估协议。代码可在GitHub上获取。", "summary": "本文介绍了PFMBench，一个急需的蛋白质基础模型综合基准。它通过评估17个最先进的模型在蛋白质科学8个关键领域的38个不同任务上的表现，解决了当前缺乏标准化评估的问题。PFMBench有助于揭示任务相关性，识别顶级模型，并建立简化的评估协议，从而推动蛋白质基础模型的理解和发展。", "keywords": "蛋白质基础模型, 基准, 评估, 蛋白质科学, 机器学习", "comments": "PFMBench极具创新性，因为它通过提供首个全面和标准化的基准，解决了蛋白质基础模型研究中的关键空白。其重要性在于能够实现公平评估，揭示隐藏的任务相关性，并指导开发更具泛化能力和有效性的蛋白质模型。这项工作对于该领域的系统性进步至关重要。"}}
{"id": "2506.14843", "title": "CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration", "authors": ["Luca Gherardini", "Imre Lengyel", "Tunde Peto", "Caroline C. W. Klaverd", "Magda A. Meester-Smoord", "Johanna Maria Colijnd", "EYE-RISK Consortium", "E3 Consortium", "Jose Sousa"], "summary": "Machine Learning (ML) is used to tackle various tasks, such as disease\nclassification and prediction. The effectiveness of ML models relies heavily on\nhaving large amounts of complete data. However, healthcare data is often\nlimited or incomplete, which can hinder model performance. Additionally, issues\nlike the trustworthiness of solutions vary with the datasets used. The lack of\ntransparency in some ML models further complicates their understanding and use.\nIn healthcare, particularly in the case of Age-related Macular Degeneration\n(AMD), which affects millions of older adults, early diagnosis is crucial due\nto the absence of effective treatments for reversing progression. Diagnosing\nAMD involves assessing retinal images along with patients' symptom reports.\nThere is a need for classification approaches that consider genetic, dietary,\nclinical, and demographic factors. Recently, we introduced the -Comprehensive\nAbstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed\nat improving AMD stage classification. CACTUS offers explainability and\nflexibility, outperforming standard ML models. It enhances decision-making by\nidentifying key factors and providing confidence in its results. The important\nfeatures identified by CACTUS allow us to compare with existing medical\nknowledge. By eliminating less relevant or biased data, we created a clinical\nscenario for clinicians to offer feedback and address biases.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14843v1", "AI": {"title_translation": "CACTUS作为一种可靠的早期年龄相关性黄斑变性分类工具", "tldr": "CACTUS是一种新的机器学习工具，旨在克服医疗数据限制和模型透明度不足的问题，用于早期分类年龄相关性黄斑变性（AMD），并已被证明优于标准机器学习模型，提供可解释性和决策支持。", "motivation": "机器学习模型在医疗领域，特别是疾病分类和预测中面临数据量有限、不完整以及模型缺乏透明度的问题。对于影响数百万老年人的年龄相关性黄斑变性（AMD），早期诊断至关重要，但目前尚无有效的逆转治疗方法。因此，需要一种能够整合遗传、饮食、临床和人口统计学因素的分类方法来改进AMD的早期诊断。", "method": "本文引入了“综合抽象与分类工具以揭示结构”（CACTUS），旨在改进AMD阶段分类。CACTUS提供可解释性和灵活性，并通过识别关键因素和提供结果置信度来增强决策制定。它还通过消除不相关或有偏见的数据来创建临床场景，以便临床医生提供反馈。", "result": "CACTUS在AMD阶段分类方面表现优于标准机器学习模型，并增强了决策制定。它能够识别与现有医学知识相符的重要特征。通过CACTUS，可以消除不太相关或有偏见的数据，为临床医生提供反馈并解决偏见创造了临床场景。", "conclusion": "CACTUS被证明是早期分类年龄相关性黄斑变性的一种可靠工具，它解决了传统机器学习在医疗数据限制和模型透明度方面的挑战，并能有效提升临床决策的质量和可信度。", "translation": "机器学习（ML）被用于解决各种任务，例如疾病分类和预测。ML模型的有效性在很大程度上依赖于拥有大量完整数据。然而，医疗保健数据通常有限或不完整，这会阻碍模型性能。此外，解决方案的可信度等问题因所使用的数据集而异。一些ML模型缺乏透明度，进一步使其理解和使用复杂化。在医疗保健领域，特别是对于影响数百万老年人的年龄相关性黄斑变性（AMD），由于目前尚无有效的治疗方法来逆转其进展，早期诊断至关重要。诊断AMD涉及评估视网膜图像以及患者的症状报告。需要考虑遗传、饮食、临床和人口统计学因素的分类方法。最近，我们引入了“综合抽象与分类工具以揭示结构”（CACTUS），旨在改进AMD阶段分类。CACTUS提供可解释性和灵活性，优于标准ML模型。它通过识别关键因素并提供结果置信度来增强决策制定。CACTUS识别出的重要特征使我们能够与现有医学知识进行比较。通过消除不太相关或有偏见的数据，我们为临床医生创建了一个临床场景，以便他们提供反馈并解决偏见。", "summary": "本研究提出了一种名为CACTUS（综合抽象与分类工具以揭示结构）的机器学习工具，旨在解决医疗数据有限和模型透明度不足的问题，用于早期分类年龄相关性黄斑变性（AMD）。CACTUS在AMD阶段分类任务中表现优于标准机器学习模型，并提供高可解释性和灵活性。它通过识别关键特征、提供结果置信度以及帮助消除偏见数据来增强临床决策，从而有助于更好地诊断和管理AMD。", "keywords": "年龄相关性黄斑变性, 机器学习, CACTUS, 早期分类, 可解释性", "comments": "CACTUS的创新之处在于其对医疗领域特有挑战的应对：数据稀缺性和模型透明度不足。通过提供可解释性和灵活性的设计，它不仅提升了分类性能，更重要的是，增强了临床医生对模型结果的信任度，这在医疗诊断中至关重要。其能够识别与现有医学知识相符的重要特征，并支持临床反馈循环，显示了其在实际应用中的巨大潜力。"}}
{"id": "2506.15339", "title": "DeVisE: Behavioral Testing of Medical Large Language Models", "authors": ["Camila Zurdo Tagliabue", "Heloisa Oss Boll", "Aykut Erdem", "Erkut Erdem", "Iacer Calixto"], "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15339v1", "AI": {"title_translation": "DeVisE：医学大型语言模型的行为测试", "tldr": "该研究提出了DeVisE框架，用于行为测试医学LLM，发现零样本模型推理更连贯，微调模型更稳定但对临床变化不敏感，并强调了人口统计因素的影响。", "motivation": "当前LLM在临床决策支持中的评估方法未能区分真正的医学推理和表面模式，因此需要一种新的行为测试框架来探测细粒度的临床理解。", "method": "引入了DeVisE（Demographics and Vital signs Evaluation）行为测试框架。构建了MIMIC-IV中ICU出院记录的数据集，生成了原始（真实世界）和基于模板（合成）版本，并控制了针对人口统计（年龄、性别、种族）和生命体征属性的单变量反事实。评估了五种LLM（通用和医学微调），在零样本和微调设置下进行。通过（1）输入级敏感性（反事实如何改变笔记的可能性）和（2）下游推理（如何影响预测的住院时间）来评估模型行为。", "result": "零样本模型表现出更连贯的反事实推理模式，而微调模型趋于更稳定但对临床上有意义的变化响应较少。人口统计因素微妙但持续地影响输出。", "conclusion": "这项工作突出了行为测试在揭示临床LLM推理策略和为设计更安全、更透明的医疗AI系统提供信息方面的效用，并强调了公平性评估的重要性。", "translation": "大型语言模型（LLMs）在临床决策支持中的应用日益增多，然而当前的评估方法常常无法区分真正的医学推理和表面模式。我们引入了DeVisE（人口统计和生命体征评估），一个用于探测细粒度临床理解的行为测试框架。我们从MIMIC-IV构建了一个ICU出院记录数据集，生成了原始（真实世界）和基于模板（合成）版本，其中包含针对人口统计（年龄、性别、种族）和生命体征属性的受控单变量反事实。我们在零样本和微调设置下评估了五种LLM，涵盖通用和医学微调变体。我们通过（1）输入级敏感性——反事实如何改变笔记的可能性；以及（2）下游推理——它们如何影响预测的住院时间，来评估模型行为。我们的结果表明，零样本模型表现出更连贯的反事实推理模式，而微调模型趋于更稳定但对临床上有意义的变化响应较少。值得注意的是，人口统计因素微妙但持续地影响输出，强调了公平性感知评估的重要性。这项工作突出了行为测试在揭示临床LLM推理策略和为设计更安全、更透明的医疗AI系统提供信息方面的效用。", "summary": "本文提出了DeVisE框架，一个用于行为测试医学大型语言模型（LLM）的工具，旨在评估其细粒度的临床理解能力。通过构建包含反事实变化的ICU出院记录数据集，并评估不同LLM在零样本和微调设置下的表现，研究发现零样本模型展现出更连贯的推理，而微调模型虽稳定但对临床变化不敏感。研究还强调了人口统计因素对LLM输出的微妙影响，提示在医疗AI系统中进行公平性评估的重要性，并表明行为测试有助于设计更安全透明的医疗AI。", "keywords": "行为测试, 医学LLM, 反事实推理, 公平性评估, 临床决策支持", "comments": "该研究创新性地引入了行为测试框架DeVisE，以深入评估医学LLM的临床推理能力，而非仅仅依赖传统评估指标。其通过生成受控的反事实数据集来探测模型对细微临床变化的敏感性，并揭示了零样本与微调模型在推理模式上的差异。特别地，它强调了人口统计因素对模型输出的潜在影响，对医疗AI的公平性和透明度具有重要指导意义，为未来更安全、更可靠的医疗AI系统设计提供了宝贵见解。"}}
{"id": "2506.15518", "title": "Real-Time Initialization of Unknown Anchors for UWB-aided Navigation", "authors": ["Giulio Delama", "Igor Borowski", "Roland Jung", "Stephan Weiss"], "summary": "This paper presents a framework for the real-time initialization of unknown\nUltra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is\ndesigned for localization solutions where UWB modules act as supplementary\nsensors. Our approach enables the automatic detection and calibration of\npreviously unknown anchors during operation, removing the need for manual\nsetup. By combining an online Positional Dilution of Precision (PDOP)\nestimation, a lightweight outlier detection method, and an adaptive robust\nkernel for non-linear optimization, our approach significantly improves\nrobustness and suitability for real-world applications compared to\nstate-of-the-art. In particular, we show that our metric which triggers an\ninitialization decision is more conservative than current ones commonly based\non initial linear or non-linear initialization guesses. This allows for better\ninitialization geometry and subsequently lower initialization errors. We\ndemonstrate the proposed approach on two different mobile robots: an autonomous\nforklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry\n(VIO) framework. The results highlight the effectiveness of the proposed method\nwith robust initialization and low positioning error. We open-source our code\nin a C++ library including a ROS wrapper.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15518v1", "AI": {"title_translation": "UWB辅助导航中未知锚点的实时初始化", "tldr": "该论文提出了一种实时初始化UWB辅助导航系统中未知UWB锚点的方法，提高了鲁棒性和定位精度。", "motivation": "现有UWB辅助导航系统需要手动设置锚点，并且初始化过程可能不够鲁棒，导致定位误差。", "method": "该方法结合了在线位置精度稀释度（PDOP）估计、轻量级异常值检测方法和用于非线性优化的自适应鲁棒核。它还采用了比现有方法更保守的初始化触发机制，以获得更好的初始化几何形状。该方法在自主叉车和配备UWB辅助视觉惯性里程计（VIO）框架的四旋翼飞行器上进行了验证。", "result": "该方法实现了鲁棒的初始化和低定位误差。与现有技术相比，显著提高了鲁棒性和对实际应用的适用性。实验结果证明了其有效性。", "conclusion": "该论文提出的框架通过改进初始化几何和降低初始化误差，有效解决了UWB辅助导航中未知锚点的实时初始化问题，显著提升了系统在实际应用中的鲁棒性和定位精度。", "translation": "本文提出了一种用于UWB辅助导航系统中未知超宽带（UWB）锚点实时初始化的框架。该方法专为UWB模块作为辅助传感器的定位解决方案而设计。我们的方法能够在操作过程中自动检测和校准以前未知的锚点，从而无需手动设置。通过结合在线位置精度稀释度（PDOP）估计、轻量级异常值检测方法和用于非线性优化的自适应鲁棒核，我们的方法与现有技术相比，显著提高了鲁棒性和对实际应用的适用性。特别是，我们表明，我们触发初始化决策的度量比当前通常基于初始线性或非线性初始化猜测的度量更为保守。这允许更好的初始化几何形状，从而降低初始化误差。我们在两种不同的移动机器人上展示了所提出的方法：一辆自主叉车和一架配备UWB辅助视觉惯性里程计（VIO）框架的四旋翼飞行器。结果突出了所提出方法在鲁棒初始化和低定位误差方面的有效性。我们以C++库的形式开源了我们的代码，其中包括一个ROS封装。", "summary": "本文提出了一种用于UWB辅助导航系统中未知UWB锚点的实时初始化框架。该方法通过结合在线PDOP估计、轻量级异常值检测和自适应鲁棒核进行非线性优化，实现了未知锚点的自动检测和校准，无需手动设置。与现有技术相比，该方法显著提高了鲁棒性和实际应用适用性，并通过更保守的初始化触发机制实现了更好的初始化几何和更低的初始化误差。在自主叉车和四旋翼飞行器上的实验验证了其鲁棒的初始化和低定位误差性能。", "keywords": "UWB, 锚点初始化, 实时导航, 定位, 鲁棒性", "comments": "该论文的创新点在于提出了一个无需手动设置即可实时初始化未知UWB锚点的框架，通过结合多种技术（在线PDOP估计、轻量级异常值检测、自适应鲁棒核）显著提升了系统在实际应用中的鲁棒性和精度。其更保守的初始化触发机制是提升性能的关键。开源代码也为后续研究和应用提供了便利，具有较高的实用价值。"}}
{"id": "2506.15160", "title": "Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation", "authors": ["Jiaqi Shi", "Jin Xiao", "Xiaoguang Hu", "Boyang Song", "Hao Jiang", "Tianyou Chen", "Baochang Zhang"], "summary": "Point cloud analysis is the cornerstone of many downstream tasks, among which\naggregating local structures is the basis for understanding point cloud data.\nWhile numerous works aggregate neighbor using three-dimensional relative\ncoordinates, there are irrelevant point interference and feature hierarchy gap\nproblems due to the limitation of local coordinates. Although some works\naddress this limitation by refining spatial description though explicit\nmodeling of cross-stage structure, these enhancement methods based on direct\ngeometric structure encoding have problems of high computational overhead and\nnoise sensitivity. To overcome these problems, we propose the Point\nDistribution Set Abstraction module (PDSA) that utilizes the correlation in the\nhigh-dimensional space to correct the feature distribution during aggregation,\nwhich improves the computational efficiency and robustness. PDSA distinguishes\nthe point correlation based on a lightweight cross-stage structural descriptor,\nand enhances structural homogeneity by reducing the variance of the neighbor\nfeature matrix and increasing classes separability though long-distance\nmodeling. Additionally, we introducing a key point mechanism to optimize the\ncomputational overhead. The experimental result on semantic segmentation and\nclassification tasks based on different baselines verify the generalization of\nthe method we proposed, and achieve significant performance improvement with\nless parameter cost. The corresponding ablation and visualization results\ndemonstrate the effectiveness and rationality of our method. The code and\ntraining weight is available at: https://github.com/AGENT9717/PointDistribution", "comment": "17 papes, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15160v1", "AI": {"title_translation": "基于跨阶段结构相关性的邻居聚合校正增强点云分析", "tldr": "本文提出了点分布集合抽象（PDSA）模块，通过基于跨阶段结构相关性的邻居聚合校正来增强点云分析，提高了计算效率和鲁棒性，并在语义分割和分类任务中以更少的参数成本实现了显著的性能提升。", "motivation": "现有的点云分析方法在使用三维相对坐标聚合邻居时存在不相关点干扰和特征层次差距问题。而通过显式建模跨阶段结构来改进空间描述的方法，则面临计算开销大和噪声敏感的问题。", "method": "本文提出了点分布集合抽象（PDSA）模块。PDSA利用高维空间中的相关性来校正聚合过程中的特征分布，从而提高计算效率和鲁棒性。它基于轻量级跨阶段结构描述符区分点相关性，通过减少邻居特征矩阵的方差和通过长距离建模增加类别可分离性来增强结构同质性。此外，还引入了关键点机制来优化计算开销。", "result": "在语义分割和分类任务上的实验结果验证了所提出方法的泛化性，并在更少的参数成本下实现了显著的性能提升。相应的消融和可视化结果证明了该方法的有效性和合理性。", "conclusion": "本文提出的PDSA模块通过有效解决现有方法的局限性，成功增强了点云分析，在提高效率、鲁棒性和性能的同时，降低了计算成本。", "translation": "点云分析是许多下游任务的基石，其中聚合局部结构是理解点云数据的基础。虽然许多工作使用三维相对坐标聚合邻居，但由于局部坐标的限制，存在不相关点干扰和特征层次差距问题。尽管一些工作通过显式建模跨阶段结构来改进空间描述，但这些基于直接几何结构编码的增强方法存在计算开销大和噪声敏感的问题。为了克服这些问题，我们提出了点分布集合抽象（PDSA）模块，该模块利用高维空间中的相关性来校正聚合过程中的特征分布，从而提高了计算效率和鲁棒性。PDSA基于轻量级跨阶段结构描述符区分点相关性，并通过减少邻居特征矩阵的方差和通过长距离建模增加类别可分离性来增强结构同质性。此外，我们引入了关键点机制来优化计算开销。基于不同基线在语义分割和分类任务上的实验结果验证了我们所提出方法的泛化性，并以更少的参数成本实现了显著的性能提升。相应的消融和可视化结果证明了我们方法的有效性和合理性。代码和训练权重可在以下网址获取：https://github.com/AGENT9717/PointDistribution", "summary": "本文提出了一种名为点分布集合抽象（PDSA）的新模块，旨在解决现有方法在点云分析中面临的无关点干扰、特征层次差距、高计算开销和噪声敏感性等问题。PDSA通过利用高维空间中的相关性来校正聚合过程中的特征分布，并采用轻量级跨阶段结构描述符来区分点相关性，以提高结构同质性。此外，引入关键点机制以优化计算效率。实验结果表明，该方法在语义分割和分类任务上实现了显著的性能提升，同时降低了参数成本，并验证了其泛化能力、有效性和合理性。", "keywords": "点云分析, 邻居聚合, 跨阶段相关性, PDSA, 语义分割", "comments": "该论文提出了一种创新的方法（PDSA）来解决点云分析中的常见挑战，特别是计算效率和对噪声及无关点的鲁棒性。通过利用高维空间相关性和轻量级跨阶段描述符，它为直接几何编码提供了一种更高效、更有效的替代方案。对减少方差和增加类别可分离性的关注，以及关键点机制的引入，突出了对实际性能的考量。"}}
{"id": "2506.14895", "title": "Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification", "authors": ["Jenni Raitoharju"], "summary": "This paper focuses on small-scale one-class classification with some negative\nsamples available. We propose Generalized Reference Kernel with Negative\nSamples (GRKneg) for One-class Support Vector Machine (OC-SVM). We study\ndifferent ways to select/generate the reference vectors and recommend an\napproach for the problem at hand. It is worth noting that the proposed method\ndoes not use any labels in the model optimization but uses the original OC-SVM\nimplementation. Only the kernel used in the process is improved using the\nnegative data. We compare our method with the standard OC-SVM and with the\nbinary Support Vector Machine (SVM) using different amounts of negative\nsamples. Our approach consistently outperforms the standard OC-SVM using Radial\nBasis Function kernel. When there are plenty of negative samples, the binary\nSVM outperforms the one-class approaches as expected, but we show that for the\nlowest numbers of negative samples the proposed approach clearly outperforms\nthe binary SVM.", "comment": "Accepted to EUSIPCO2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14895v1", "AI": {"title_translation": "带负样本的广义参考核用于支持向量单类分类", "tldr": "本文提出了一种名为GRKneg的新方法，用于在存在少量负样本的情况下改进单类支持向量机（OC-SVM）的小规模分类性能。该方法通过改进核函数，在负样本数量极少时优于二分类SVM，并始终优于标准OC-SVM。", "motivation": "本文旨在解决在存在少量负样本的情况下，小规模单类分类的挑战。", "method": "本文提出了带负样本的广义参考核（GRKneg）用于单类支持向量机（OC-SVM）。该方法通过负数据改进了核函数，但在模型优化过程中不使用任何标签。研究了选择/生成参考向量的不同方法。", "result": "所提出的GRKneg方法在径向基函数核下始终优于标准OC-SVM。当负样本数量最少时，该方法明显优于二分类支持向量机（SVM）。当负样本充足时，二分类SVM表现最佳。", "conclusion": "本文提出的GRKneg方法在少量负样本的单类分类任务中表现出色，尤其是在负样本数量有限的情况下，其性能优于传统的单类SVM和二分类SVM。", "translation": "本文重点研究了存在少量负样本的小规模单类分类问题。我们提出了带负样本的广义参考核（GRKneg）用于单类支持向量机（OC-SVM）。我们研究了选择/生成参考向量的不同方式，并为当前问题推荐了一种方法。值得注意的是，所提出的方法在模型优化中不使用任何标签，但使用了原始的OC-SVM实现。在此过程中，仅使用负数据改进了所使用的核函数。我们将我们的方法与标准OC-SVM以及使用不同数量负样本的二分类支持向量机（SVM）进行了比较。我们的方法始终优于使用径向基函数核的标准OC-SVM。当负样本充足时，二分类SVM如预期地优于单类方法，但我们表明，在负样本数量最少的情况下，所提出的方法明显优于二分类SVM。", "summary": "本文提出了一种名为GRKneg的新型核方法，用于在存在少量负样本时改进单类支持向量机（OC-SVM）的小规模分类。GRKneg通过负数据优化核函数，而非直接利用标签进行模型优化。实验结果表明，GRKneg在性能上持续优于标准OC-SVM，并且在负样本数量最少的情况下，其表现显著优于二分类SVM。", "keywords": "单类分类, 支持向量机, 负样本, 核函数, 小规模分类", "comments": "该论文的创新点在于，它在单类分类的背景下，巧妙地利用了有限的负样本来改进核函数，而不是在优化过程中直接使用标签，这为处理半监督或弱监督的单类分类问题提供了一个有前景的方向。其价值在于，它在特定条件下（少量负样本）能够超越传统的单类和二分类方法，填补了这一领域的空白。"}}
{"id": "2506.15355", "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Sriparna Saha"], "summary": "Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15355v1", "AI": {"title_translation": "SANSKRITI：一个评估语言模型印度文化知识的综合基准", "tldr": "SANSKRITI是一个大型基准数据集，用于评估语言模型对印度文化的理解，揭示了现有模型在处理文化细微差别方面的不足。", "motivation": "语言模型（LMs）在全球范围内的有效性取决于它们对当地社会文化背景的理解。为了解决现有模型在印度文化知识方面的不足，并评估其对印度丰富文化多样性的理解能力，引入了SANSKRITI。", "method": "研究引入了SANSKRITI，一个包含21,853个精心策划的问答对的基准数据集，涵盖印度28个邦和8个联邦属地，以及印度文化的16个关键属性（如仪式、历史、美食、语言、宗教等）。该基准用于评估领先的大型语言模型（LLMs）、印度语语言模型（ILMs）和小型语言模型（SLMs）。", "result": "评估结果显示，现有模型在处理文化细微差别的查询时存在显著差异，许多模型在特定区域的语境中表现不佳。", "conclusion": "SANSKRITI通过提供一个广泛、文化丰富且多样化的数据集，为评估和提升语言模型的文化理解能力设定了新标准。", "translation": "语言模型（LMs）是塑造现代工作流程不可或缺的工具，但其全球有效性取决于对当地社会文化背景的理解。为了解决这个问题，我们引入了SANSKRITI，这是一个旨在评估语言模型对印度丰富文化多样性理解能力的基准。SANSKRITI由21,853个精心策划的问答对组成，涵盖28个邦和8个联邦属地，是测试印度文化知识的最大数据集。它涵盖了印度文化的十六个关键属性：仪式和典礼、历史、旅游、美食、舞蹈和音乐、服饰、语言、艺术、节日、宗教、医学、交通、体育、夜生活和人物，全面展现了印度的文化图景。我们使用SANSKRITI评估了领先的大型语言模型（LLMs）、印度语语言模型（ILMs）和小型语言模型（SLMs），揭示了它们在处理文化细微差别查询方面的显著差异，许多模型在特定区域的语境中表现挣扎。通过提供一个广泛、文化丰富和多样化的数据集，SANSKRITI为评估和提升语言模型的文化理解能力设定了新标准。", "summary": "该论文介绍了SANSKRITI，一个用于评估语言模型对印度文化理解能力的综合基准。SANSKRITI包含21,853个问答对，覆盖印度28个邦和8个联邦属地以及16个文化属性，是目前最大的印度文化知识数据集。对主流语言模型的评估显示，它们在处理文化细微差别和特定区域语境的查询时存在显著不足。SANSKRITI旨在为提升语言模型的文化理解能力提供新的评估标准。", "keywords": "SANSKRITI, 语言模型, 印度文化, 基准测试, 文化理解", "comments": "SANSKRITI的创新之处在于其作为目前最大的印度文化知识数据集，填补了现有语言模型在处理地域和文化细微差别方面的空白。它通过提供一个结构化、多维度的评估工具，揭示了当前LLMs、ILMs和SLMs在文化理解方面的局限性，对推动语言模型向更具文化意识的方向发展具有重要意义。"}}
{"id": "2506.15419", "title": "Density estimation via periodic scaled Korobov kernel method with exponential decay condition", "authors": ["Ziyang Ye", "Haoyuan Tan", "Xiaoqun Wang", "Zhijian He"], "summary": "We propose the periodic scaled Korobov kernel (PSKK) method for nonparametric\ndensity estimation on $\\mathbb{R}^d$. By first wrapping the target density into\na periodic version through modulo operation and subsequently applying kernel\nridge regression in scaled Korobov spaces, we extend the kernel approach\nproposed by Kazashi and Nobile (SIAM J. Numer. Anal., 2023) and eliminate its\nrequirement for inherent periodicity of the density function. This key\nmodification enables effective estimation of densities defined on unbounded\ndomains. We establish rigorous mean integrated squared error (MISE) bounds,\nproving that for densities with smoothness of order $\\alpha$ and exponential\ndecay, our method achieves the $\\mathcal{O}(M^{-1/(1+1/(2\\alpha)+\\epsilon)})$\nMISE convergence rate with an arbitrarily small $\\epsilon>0$. While matching\nthe convergence rate of the previous kernel approach, our approach applies to a\nbroader class of non-periodic distributions. Numerical experiments confirm the\ntheoretical results and demonstrate significant improvement over traditional\nkernel density estimation in large-sample regimes.", "comment": "26 pages, 6 figures", "cate": "math.ST", "url": "http://arxiv.org/abs/2506.15419v1", "AI": {"title_translation": "基于周期性尺度化Korobov核方法和指数衰减条件的密度估计", "tldr": "本文提出了一种周期性尺度化Korobov核（PSKK）方法，用于无界域上的非参数密度估计。该方法通过消除对密度函数固有周期性的要求，扩展了现有核方法，并实现了与现有方法相同的收敛速度，同时适用于更广泛的非周期性分布。数值实验证实了其有效性。", "motivation": "现有的核方法（如Kazashi和Nobile, 2023提出的方法）要求密度函数具有固有的周期性，这限制了其在无界域上非周期性密度估计的应用。本文旨在消除这一限制，以有效估计定义在无界域上的密度。", "method": "本文提出了周期性尺度化Korobov核（PSKK）方法。该方法首先通过模运算将目标密度转换为周期性版本，然后在外延的Korobov空间中应用核岭回归。这种关键修改扩展了Kazashi和Nobile（SIAM J. Numer. Anal., 2023）提出的核方法，并消除了其对密度函数固有周期性的要求。", "result": "1. 实现了对定义在无界域上的密度函数的有效估计。\n2. 建立了严格的均方积分误差（MISE）界限。\n3. 证明了对于具有$\\alpha$阶平滑度和指数衰减的密度函数，该方法能达到$\\mathcal{O}(M^{-1/(1+1/(2\\alpha)+\\epsilon)})$的MISE收敛速度，其中$\\epsilon>0$可任意小。\n4. 收敛速度与之前的核方法相匹配，但适用于更广泛的非周期性分布。\n5. 数值实验证实了理论结果，并表明在大样本情况下，该方法比传统核密度估计有显著改进。", "conclusion": "本文提出的周期性尺度化Korobov核（PSKK）方法成功地将核密度估计方法扩展到无界域上的非周期性分布，克服了现有方法的局限性。该方法在理论上保持了强大的收敛速度，并通过数值实验证明了其在实际应用中优于传统方法的性能。", "translation": "我们提出了一种周期性尺度化Korobov核（PSKK）方法，用于$\\mathbb{R}^d$上的非参数密度估计。通过首先通过模运算将目标密度转换为周期性版本，随后在尺度化的Korobov空间中应用核岭回归，我们扩展了Kazashi和Nobile（SIAM J. Numer. Anal., 2023）提出的核方法，并消除了其对密度函数固有周期性的要求。这一关键修改使得对定义在无界域上的密度进行有效估计成为可能。我们建立了严格的均方积分误差（MISE）界限，证明了对于具有$\\alpha$阶平滑度和指数衰减的密度函数，我们的方法能达到$\\mathcal{O}(M^{-1/(1+1/(2\\alpha)+\\epsilon)})$的MISE收敛速度，其中$\\epsilon>0$可任意小。尽管收敛速度与之前的核方法相匹配，但我们的方法适用于更广泛的非周期性分布。数值实验证实了理论结果，并表明在大样本情况下，该方法比传统核密度估计有显著改进。", "summary": "本文提出了一种周期性尺度化Korobov核（PSKK）方法，用于在无界域上进行非参数密度估计。该方法通过将目标密度转换为周期性版本并应用核岭回归，成功地扩展了现有核方法，消除了其对密度固有周期性的限制。研究证明，对于具有指数衰减的密度，该方法能达到与现有方法相同的MISE收敛速度，并适用于更广泛的非周期性分布。数值实验验证了理论结果，并显示出相对于传统核密度估计的显著性能提升。", "keywords": "密度估计, Korobov核, 非参数, 指数衰减, MISE", "comments": "该论文的关键创新在于通过巧妙地将无界域上的密度函数转换为周期性形式，成功地将一种先进的核方法（Korobov核）应用于更广泛的非周期性分布，从而克服了现有方法的局限性。这对于处理实际中常见的非周期性数据具有重要意义。该方法不仅在理论上保持了高阶收敛速度，而且在数值实验中也展现出优于传统方法的性能，显示了其在密度估计领域的潜力和实用价值。"}}
{"id": "2506.15539", "title": "Aerial Grasping via Maximizing Delta-Arm Workspace Utilization", "authors": ["Haoran Chen", "Weiliang Deng", "Biyu Ye", "Yifan Xiong", "Ximin Lyu"], "summary": "The workspace limits the operational capabilities and range of motion for the\nsystems with robotic arms. Maximizing workspace utilization has the potential\nto provide more optimal solutions for aerial manipulation tasks, increasing the\nsystem's flexibility and operational efficiency. In this paper, we introduce a\nnovel planning framework for aerial grasping that maximizes workspace\nutilization. We formulate an optimization problem to optimize the aerial\nmanipulator's trajectory, incorporating task constraints to achieve efficient\nmanipulation. To address the challenge of incorporating the delta arm's\nnon-convex workspace into optimization constraints, we leverage a Multilayer\nPerceptron (MLP) to map position points to feasibility\nprobabilities.Furthermore, we employ Reversible Residual Networks (RevNet) to\napproximate the complex forward kinematics of the delta arm, utilizing\nefficient model gradients to eliminate workspace constraints. We validate our\nmethods in simulations and real-world experiments to demonstrate their\neffectiveness.", "comment": "8 pages, 7 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15539v1", "AI": {"title_translation": "通过最大化Delta机械臂工作空间利用率实现空中抓取", "tldr": "本文提出了一种新的空中抓取规划框架，通过优化轨迹并利用MLP和RevNet来解决Delta机械臂非凸工作空间问题，以最大化工作空间利用率，提高空中操控效率。", "motivation": "机器人手臂系统的工作空间限制了其操作能力和运动范围。最大化工作空间利用率有望为空中操作任务提供更优解决方案，从而提高系统的灵活性和操作效率。", "method": "本文引入了一种新的空中抓取规划框架，旨在最大化工作空间利用率。该框架通过建立优化问题来优化空中机械臂的轨迹，并结合任务约束。为解决Delta机械臂非凸工作空间纳入优化约束的挑战，利用多层感知器（MLP）将位置点映射到可行性概率。此外，采用可逆残差网络（RevNet）近似Delta机械臂的复杂正向运动学，并利用高效模型梯度消除工作空间约束。", "result": "通过仿真和真实世界实验验证了所提出方法的有效性。", "conclusion": "本文提出的规划框架能够有效最大化空中机械臂的工作空间利用率，提高空中抓取任务的效率和灵活性。", "translation": "机器人手臂系统的工作空间限制了其操作能力和运动范围。最大化工作空间利用率有潜力为空中操作任务提供更优的解决方案，从而提高系统的灵活性和操作效率。本文介绍了一种新颖的空中抓取规划框架，该框架旨在最大化工作空间利用率。我们建立了一个优化问题来优化空中机械臂的轨迹，并结合任务约束以实现高效操作。为了解决将Delta机械臂的非凸工作空间纳入优化约束的挑战，我们利用多层感知器（MLP）将位置点映射到可行性概率。此外，我们采用可逆残差网络（RevNet）来近似Delta机械臂复杂的正向运动学，利用高效的模型梯度来消除工作空间约束。我们在仿真和真实世界实验中验证了我们的方法，以证明其有效性。", "summary": "本文提出了一种新颖的空中抓取规划框架，旨在最大化Delta机械臂的工作空间利用率。该框架通过优化空中机械臂轨迹来解决高效操作问题，并创新性地利用MLP处理Delta臂的非凸工作空间，以及RevNet近似其复杂正向运动学，从而消除工作空间约束。仿真和实际实验验证了该方法的有效性。", "keywords": "空中抓取, 工作空间利用, Delta机械臂, 多层感知器, 可逆残差网络", "comments": "本文的创新点在于结合机器学习方法（MLP和RevNet）来克服Delta机械臂非凸工作空间带来的优化难题，从而实现空中抓取任务中工作空间的最大化利用。这种方法有望显著提升空中机械臂的灵活性和操作效率，对于空中机器人领域具有重要意义。"}}
{"id": "2506.15166", "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography", "authors": ["Abdur Rahman", "Keerthiveena Balraj", "Manojkumar Ramteke", "Anurag Singh Rathore"], "summary": "Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND", "comment": "Version of record published in Discover Applied Sciences (Springer\n  Nature). The definitive article is available at\n  https://doi.org/10.1007/s42452-025-07055-5", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15166v1", "AI": {"title_translation": "Echo-DND：一种用于超声心动图左心室分割的鲁棒精确双噪声扩散模型", "tldr": "Echo-DND是一种新的双噪声扩散模型，通过结合高斯和伯努利噪声、多尺度融合和空间一致性校准，显著提高了超声心动图中左心室分割的精度和鲁棒性，超越了现有最先进模型。", "motivation": "超声心动图中的左心室（LV）准确分割对诊断和治疗至关重要，但超声图像固有的噪声大、对比度低和LV边界模糊等问题使得分割过程复杂化。", "method": "本文引入了Echo-DND，这是一种新颖的双噪声扩散模型，专为左心室分割任务设计。它结合了高斯和伯努利噪声，并整合了多尺度融合条件模块以提高分割精度。此外，它还利用空间一致性校准来保持分割掩膜的空间完整性。", "result": "Echo-DND模型在CAMUS和EchoNet-Dynamic数据集上进行了严格验证，并在这些数据集上分别取得了0.962和0.939的高Dice分数，性能优于现有最先进（SOTA）模型。", "conclusion": "Echo-DND模型在超声心动图分割领域建立了新标准，其架构有望在其他医学成像任务中得到更广泛的应用，从而可能提高各种医学领域的诊断准确性。", "translation": "扩散概率模型（DPMs）的最新进展彻底改变了图像处理，在医学应用中展现出巨大潜力。超声心动图中左心室（LV）的精确分割对于诊断程序和必要的治疗至关重要。然而，超声图像以其噪声大、对比度低和LV边界模糊而闻名，从而使分割过程复杂化。为了应对这些挑战，本文引入了Echo-DND，这是一种专门为此任务设计的新型双噪声扩散模型。Echo-DND利用高斯和伯努利噪声的独特组合。它还结合了一个多尺度融合条件模块，以提高分割精度。此外，它利用空间相干性校准来保持分割掩膜的空间完整性。该模型的性能在CAMUS和EchoNet-Dynamic数据集上得到了严格验证。广泛的评估表明，所提出的框架优于现有SOTA模型。它在这些数据集上分别实现了0.962和0.939的高Dice分数。所提出的Echo-DND模型在超声心动图分割方面建立了新标准，其架构有望在其他医学成像任务中具有更广泛的适用性，可能会提高各种医学领域的诊断准确性。项目页面：https://abdur75648.github.io/Echo-DND", "summary": "本文提出了Echo-DND，一种新颖的双噪声扩散模型，旨在解决超声心动图中左心室分割面临的噪声大、对比度低和边界模糊等挑战。该模型独特地结合了高斯和伯努利噪声，并融入了多尺度融合条件模块和空间一致性校准，以提高分割精度和空间完整性。在CAMUS和EchoNet-Dynamic数据集上的广泛评估表明，Echo-DND在Dice分数上优于现有最先进模型，分别为0.962和0.939，为超声心动图分割树立了新标准，并有望应用于其他医学成像领域。", "keywords": "扩散模型, 左心室分割, 超声心动图, 双噪声, 深度学习", "comments": "Echo-DND的创新之处在于其独特的双噪声扩散模型（结合高斯和伯努利噪声）以及多尺度融合和空间一致性校准模块，这些设计有效地解决了超声图像固有的挑战。其在两个标准数据集上超越SOTA模型的表现证明了其重要性，为医学图像分割领域提供了新的思路和强大的工具。该模型具有广泛的应用前景，可能显著提升医学诊断的准确性。"}}
{"id": "2506.14798", "title": "MODS: Multi-source Observations Conditional Diffusion Model for Meteorological State Downscaling", "authors": ["Siwei Tu", "Jingyi Xu", "Weidong Yang", "Lei Bai", "Ben Fei"], "summary": "Accurate acquisition of high-resolution surface meteorological conditions is\ncritical for forecasting and simulating meteorological variables. Directly\napplying spatial interpolation methods to derive meteorological values at\nspecific locations from low-resolution grid fields often yields results that\ndeviate significantly from the actual conditions. Existing downscaling methods\nprimarily rely on the coupling relationship between geostationary satellites\nand ERA5 variables as a condition. However, using brightness temperature data\nfrom geostationary satellites alone fails to comprehensively capture all the\nchanges in meteorological variables in ERA5 maps. To address this limitation,\nwe can use a wider range of satellite data to make more full use of its\ninversion effects on various meteorological variables, thus producing more\nrealistic results across different meteorological variables. To further improve\nthe accuracy of downscaling meteorological variables at any location, we\npropose the Multi-source Observation Down-Scaling Model (MODS). It is a\nconditional diffusion model that fuses data from multiple geostationary\nsatellites GridSat, polar-orbiting satellites (AMSU-A, HIRS, and MHS), and\ntopographic data (GEBCO), as conditions, and is pre-trained on the ERA5\nreanalysis dataset. During training, latent features from diverse conditional\ninputs are extracted separately and fused into ERA5 maps via a multi-source\ncross-attention module. By exploiting the inversion relationships between\nreanalysis data and multi-source atmospheric variables, MODS generates\natmospheric states that align more closely with real-world conditions. During\nsampling, MODS enhances downscaling consistency by incorporating low-resolution\nERA5 maps and station-level meteorological data as guidance. Experimental\nresults demonstrate that MODS achieves higher fidelity when downscaling ERA5\nmaps to a 6.25 km resolution.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.14798v1", "AI": {"title_translation": "MODS：基于多源观测条件扩散模型的气象状态降尺度", "tldr": "MODS是一个条件扩散模型，通过融合多源卫星和地形数据，将ERA5气象图降尺度到6.25公里，提高了高分辨率气象变量的准确性。", "motivation": "现有空间插值方法和仅依赖地球静止卫星的降尺度方法在获取高分辨率气象数据时存在偏差和局限性，未能全面捕捉气象变量的变化，因此需要更准确、更全面的降尺度方法。", "method": "本文提出了MODS（多源观测降尺度模型），一个条件扩散模型。它融合了来自多颗地球静止卫星（GridSat）、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）作为条件，并在ERA5再分析数据集上进行预训练。在训练过程中，模型通过多源交叉注意力模块将提取的潜在特征融合到ERA5图中。在采样阶段，通过结合低分辨率ERA5图和站点级气象数据作为指导，增强降尺度一致性。", "result": "实验结果表明，MODS在将ERA5图降尺度到6.25公里分辨率时，实现了更高的保真度。", "conclusion": "MODS通过融合多源观测数据和采用条件扩散模型，有效提升了高分辨率气象状态降尺度的准确性和保真度，解决了现有方法的局限性。", "translation": "获取高分辨率地表气象条件对于预报和模拟气象变量至关重要。直接将空间插值方法应用于从低分辨率网格场推导特定位置的气象值，往往会产生与实际情况显著偏离的结果。现有的降尺度方法主要依赖于地球静止卫星和ERA5变量之间的耦合关系作为条件。然而，单独使用地球静止卫星的亮度温度数据无法全面捕捉ERA5图中气象变量的所有变化。为了解决这一限制，我们可以利用更广泛的卫星数据，更充分地利用其对各种气象变量的反演效果，从而在不同的气象变量上产生更真实的结果。为了进一步提高任意位置气象变量降尺度的准确性，我们提出了多源观测降尺度模型（MODS）。它是一个条件扩散模型，融合了来自多颗地球静止卫星GridSat、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）作为条件，并在ERA5再分析数据集上进行了预训练。在训练过程中，来自不同条件输入的潜在特征被单独提取，并通过多源交叉注意力模块融合到ERA5图中。通过利用再分析数据与多源大气变量之间的反演关系，MODS生成的气象状态与真实世界条件更加吻合。在采样过程中，MODS通过结合低分辨率ERA5图和站点级气象数据作为指导，增强了降尺度的一致性。实验结果表明，MODS在将ERA5图降尺度到6.25公里分辨率时，实现了更高的保真度。", "summary": "本文提出了MODS（多源观测降尺度模型），一个条件扩散模型，旨在解决现有方法在获取高分辨率气象数据时的局限性。MODS通过融合来自多源地球静止卫星、极轨卫星和地形数据，并在ERA5数据集上进行预训练。模型在训练中利用多源交叉注意力融合特征，并在采样中结合低分辨率ERA5图和站点数据作为指导。实验证明，MODS能以更高的保真度将ERA5图降尺度至6.25公里分辨率。", "keywords": "多源观测, 条件扩散模型, 气象降尺度, ERA5, 卫星数据", "comments": "该论文的创新点在于提出了一个多源条件扩散模型MODS，它克服了传统降尺度方法仅依赖单一或有限数据源的局限性。通过融合多种卫星数据和地形数据，并结合条件扩散模型和多源交叉注意力机制，显著提升了气象变量降尺度的准确性和真实性。在采样阶段引入低分辨率ERA5图和站点数据作为指导，进一步增强了模型的一致性和实用性。这对于提高气象预报和模拟的精细化水平具有重要意义。"}}
{"id": "2506.15372", "title": "COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation", "authors": ["Raghvendra Kumar", "S. A. Mohammed Salman", "Aryan Sahu", "Tridib Nandi", "Pragathi Y. P.", "Sriparna Saha", "Jose G. Moreno"], "summary": "Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.", "comment": "ACL 2025 MAINs", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15372v1", "AI": {"title_translation": "COSMMIC：评论敏感的多模态多语言印度语语料库，用于摘要和标题生成", "tldr": "引入COSMMIC，一个包含9种印度语言的评论敏感多模态多语言语料库，用于摘要和标题生成任务。", "motivation": "尽管英汉语言的评论感知多模态多语言摘要研究有所进展，但印度语言在此领域的研究仍然有限。", "method": "本研究引入了COSMMIC数据集，一个开创性的评论敏感多模态多语言数据集，包含九种主要的印度语言，共4,959个文章-图片对和24,484条读者评论，并提供真实摘要。研究通过整合读者见解和反馈来增强摘要，并探索了四种配置下的摘要和标题生成：仅使用文章文本、结合用户评论、利用图片，以及结合文本、评论和图片。为评估数据集有效性，研究采用了LLama3和GPT-4等先进语言模型，并进行全面研究以评估不同的组件组合，包括识别支持性评论、使用IndicBERT分类器过滤噪音，以及使用多语言CLIP分类器从图片中提取有价值见解。", "result": "研究通过评估确定了自然语言生成（NLG）任务最有效的配置。COSMMIC独特地整合了文本、图像和用户反馈，弥补了印度语言资源在多模态、用户评论整合方面的空白。", "conclusion": "COSMMIC数据集通过整合文本、图像和用户反馈，弥补了印度语言资源在评论感知多模态摘要领域的空白，推动了自然语言处理研究并促进了包容性。", "translation": "尽管英汉语言的评论感知多模态多语言摘要研究取得了进展，但印度语言的研究仍然有限。本研究通过引入COSMMIC解决了这一空白，这是一个开创性的评论敏感多模态多语言数据集，包含九种主要的印度语言。COSMMIC包含4,959个文章-图片对和24,484条读者评论，并提供所有包含语言的真实摘要。我们的方法通过整合读者见解和反馈来增强摘要。我们探索了四种配置下的摘要和标题生成：(1) 仅使用文章文本，(2) 结合用户评论，(3) 利用图片，以及 (4) 结合文本、评论和图片。为了评估数据集的有效性，我们采用了LLama3和GPT-4等最先进的语言模型。我们进行了一项全面的研究，评估了不同的组件组合，包括识别支持性评论、使用专门的IndicBERT评论分类器过滤噪音，以及使用多语言CLIP分类器从图片中提取有价值的见解。这有助于确定自然语言生成（NLG）任务最有效的配置。与许多现有数据集不同，这些数据集要么仅限于文本，要么在多模态设置中缺乏用户评论，COSMMIC独特地整合了文本、图像和用户反馈。这种全面的方法弥合了印度语言资源的空白，推动了自然语言处理研究并促进了包容性。", "summary": "本文介绍了COSMMIC，一个针对印度语言的评论敏感多模态多语言语料库，旨在解决印度语言在评论感知摘要和标题生成方面的研究空白。该数据集包含文章、图片和读者评论，并提供多语言真实摘要。研究探索了不同输入配置（文本、评论、图片组合）对摘要和标题生成任务的影响，并使用先进语言模型进行评估，旨在确定最有效的NLG配置。COSMMIC通过整合多模态和用户反馈，弥补了现有印度语言资源的不足，推动了印度语言NLP研究。", "keywords": "评论敏感, 多模态, 多语言, 印度语, 摘要, 语料库", "comments": "COSMMIC数据集的创新之处在于其首次将评论、图像和文本结合起来应用于印度语言的摘要和标题生成任务，填补了该领域的研究空白。其多模态和多语言特性对于推动印度语言的NLP发展具有重要意义。通过整合用户反馈，该研究提供了一种更全面、更贴近实际的摘要生成方法，为未来的跨模态、多语言NLG研究提供了宝贵的资源。"}}
{"id": "2506.15349", "title": "Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference", "authors": ["Terrance Liu", "Matteo Boglioni", "Yiwei Fu", "Shengyuan Hu", "Pratiksha Thaker", "Zhiwei Steven Wu"], "summary": "Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15349v1", "AI": {"title_translation": "增强单次运行隐私审计与基于分位数回归的成员推断", "tldr": "本文通过引入基于分位数回归的成员推断攻击，显著改善了黑盒设置下差分隐私审计的单次运行效率和准确性，实现了更紧密的隐私界限。", "motivation": "现有的差分隐私（DP）审计方法，尤其是在更实用的黑盒设置下的单次运行审计，在经验下限和理论上限之间存在较大差距，导致审计结果不够精确。", "method": "本文提出了一种新方法，通过利用基于分位数回归的成员推断攻击（MIA）来增强黑盒设置下的单次运行隐私审计。", "result": "在CIFAR-10数据集上使用DP-SGD训练的图像分类模型上进行评估，该方法实现了更紧密的隐私界限，同时保持了单次运行方法的计算效率。", "conclusion": "通过将基于分位数回归的成员推断方法融入黑盒设置下的单次运行隐私审计，能够有效提升审计结果的准确性，获得更精确的隐私保证。", "translation": "差分隐私（DP）审计旨在为DP-SGD等DP机制的隐私保证提供经验下限。虽然一些现有技术需要多次训练运行，成本过高，但最近的工作引入了单次运行审计方法，这些方法在白盒设置下有效审计DP-SGD，同时计算效率高。然而，在更实用的黑盒设置中，训练期间无法操纵梯度，并且只观察到最后一个模型迭代，先前的研究表明，经验下限和理论上限之间仍然存在很大差距。因此，在这项工作中，我们研究了如何结合更强的成员推断攻击（MIA）方法来改进黑盒设置下的单次运行审计。在CIFAR-10上使用DP-SGD训练的图像分类模型上进行评估，我们证明了我们提出的利用分位数回归进行MIA的方法在实现更紧密界限的同时，关键地保持了单次运行方法的计算效率。", "summary": "本文旨在解决黑盒设置下单次运行差分隐私审计中经验下界与理论上界之间存在的显著差距。研究人员提出了一种新方法，该方法通过整合基于分位数回归的成员推断攻击（MIA）来增强审计过程。实验结果表明，在CIFAR-10数据集上训练的图像分类模型上，该方法能够实现更紧密的隐私界限，同时维持了单次运行审计的计算效率。", "keywords": "差分隐私审计, 成员推断攻击, 分位数回归, 单次运行审计, 黑盒设置", "comments": "本文的创新之处在于将分位数回归应用于成员推断攻击，并将其集成到黑盒设置的单次运行隐私审计中。这有效地弥补了现有方法在准确性上的不足，同时保持了计算效率，对于实际应用中评估差分隐私模型的隐私保证具有重要意义。"}}
{"id": "2506.15607", "title": "GRIM: Task-Oriented Grasping with Conditioning on Generative Examples", "authors": ["Shailesh", "Alok Raj", "Nayan Kumar", "Priya Shukla", "Andrew Melnik", "Micheal Beetz", "Gora Chand Nandi"], "summary": "Task-Oriented Grasping (TOG) presents a significant challenge, requiring a\nnuanced understanding of task semantics, object affordances, and the functional\nconstraints dictating how an object should be grasped for a specific task. To\naddress these challenges, we introduce GRIM (Grasp Re-alignment via Iterative\nMatching), a novel training-free framework for task-oriented grasping.\nInitially, a coarse alignment strategy is developed using a combination of\ngeometric cues and principal component analysis (PCA)-reduced DINO features for\nsimilarity scoring. Subsequently, the full grasp pose associated with the\nretrieved memory instance is transferred to the aligned scene object and\nfurther refined against a set of task-agnostic, geometrically stable grasps\ngenerated for the scene object, prioritizing task compatibility. In contrast to\nexisting learning-based methods, GRIM demonstrates strong generalization\ncapabilities, achieving robust performance with only a small number of\nconditioning examples.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15607v1", "AI": {"title_translation": "GRIM：基于生成示例条件化的任务导向抓取", "tldr": "GRIM是一种无需训练的任务导向抓取框架，通过迭代匹配实现，仅需少量示例即可展现强大的泛化能力。", "motivation": "任务导向抓取（TOG）面临巨大挑战，需要对任务语义、物体功能和抓取约束有细致的理解。", "method": "本文提出GRIM（通过迭代匹配进行抓取重新对齐），一个无需训练的任务导向抓取框架。该方法首先利用几何线索和PCA降维的DINO特征进行粗略对齐，然后将检索到的记忆实例的完整抓取姿态转移到对齐的场景物体上，并根据为场景物体生成的、与任务无关的几何稳定抓取进行精细化，优先考虑任务兼容性。", "result": "与现有基于学习的方法相比，GRIM展现出强大的泛化能力，仅需少量条件示例即可实现鲁棒性能。", "conclusion": "GRIM通过其无需训练的框架和对少量条件示例的依赖，有效解决了任务导向抓取中的挑战，并展现出强大的泛化能力。", "translation": "任务导向抓取（TOG）提出了一个重大挑战，需要对任务语义、物体功能和决定物体如何为特定任务抓取的函数约束有细致的理解。为了解决这些挑战，我们引入了GRIM（通过迭代匹配进行抓取重新对齐），一个新颖的无需训练的任务导向抓取框架。最初，利用几何线索和主成分分析（PCA）降维的DINO特征相结合，开发了一种粗略对齐策略，用于相似度评分。随后，与检索到的记忆实例相关的完整抓取姿态被转移到对齐的场景物体上，并根据为场景物体生成的一组与任务无关的几何稳定抓取进行进一步精细化，优先考虑任务兼容性。与现有基于学习的方法相比，GRIM展示了强大的泛化能力，仅需少量条件示例即可实现鲁棒性能。", "summary": "任务导向抓取（TOG）面临复杂挑战，需要理解任务语义和抓取约束。本文提出GRIM，一个新颖的无需训练的TOG框架。该方法通过结合几何线索和PCA降维的DINO特征进行粗略对齐，并精细化抓取姿态以优先考虑任务兼容性。GRIM与现有学习方法不同，展现出强大的泛化能力，仅需少量条件示例即可实现鲁棒性能。", "keywords": "任务导向抓取, GRIM, 无需训练, 泛化能力, 抓取姿态", "comments": "这篇论文的创新点在于提出了一个无需训练的任务导向抓取框架GRIM，并通过迭代匹配和精细化实现了强大的泛化能力。其仅需少量条件示例即可获得鲁棒性能的特点，与现有基于学习的方法形成对比，展现了其在效率和鲁棒性方面的优势和潜力。"}}
{"id": "2506.15180", "title": "ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections", "authors": ["Ziling Huang", "Yidan Zhang", "Shin'ichi Satoh"], "summary": "Large-scale visual search engines are expected to solve a dual problem at\nonce: (i) locate every image that truly contains the object described by a\nsentence and (ii) identify the object's bounding box or exact pixels within\neach hit. Existing techniques address only one side of this challenge. Visual\ngrounding yields tight boxes and masks but rests on the unrealistic assumption\nthat the object is present in every test image, producing a flood of false\nalarms when applied to web-scale collections. Text-to-image retrieval excels at\nsifting through massive databases to rank relevant images, yet it stops at\nwhole-image matches and offers no fine-grained localization. We introduce\nReferring Search and Discovery (ReSeDis), the first task that unifies\ncorpus-level retrieval with pixel-level grounding. Given a free-form\ndescription, a ReSeDis model must decide whether the queried object appears in\neach image and, if so, where it is, returning bounding boxes or segmentation\nmasks. To enable rigorous study, we curate a benchmark in which every\ndescription maps uniquely to object instances scattered across a large, diverse\ncorpus, eliminating unintended matches. We further design a task-specific\nmetric that jointly scores retrieval recall and localization precision.\nFinally, we provide a straightforward zero-shot baseline using a frozen\nvision-language model, revealing significant headroom for future study. ReSeDis\noffers a realistic, end-to-end testbed for building the next generation of\nrobust and scalable multimodal search systems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15180v1", "AI": {"title_translation": "ReSeDis：一个用于大规模图像集合中基于指代的物体搜索数据集", "tldr": "ReSeDis是一个新数据集和任务，它首次将大规模图像检索与像素级物体定位相结合，旨在解决现有视觉搜索方法的不足。", "motivation": "现有的大规模视觉搜索引擎在同时解决“定位包含描述物体的图像”和“识别图像中物体的精确位置”这两个问题上存在不足。视觉定位方法假设物体存在于每张测试图像中，导致大量误报；文本到图像检索虽能筛选大量数据库，但缺乏细粒度定位。", "method": "本文提出了Referring Search and Discovery (ReSeDis)任务，首次将语料库级检索与像素级定位相结合。给定自由形式的描述，ReSeDis模型需判断查询物体是否存在于每张图像中，并返回其边界框或分割掩膜。为实现严谨研究，作者构建了一个基准数据集，其中每个描述唯一对应于分散在大型多样语料库中的物体实例。此外，还设计了一个结合检索召回率和定位精度的任务特定度量标准。", "result": "论文提供了一个使用冻结视觉-语言模型的直接零样本基线，并指出未来研究仍有显著提升空间。", "conclusion": "ReSeDis为构建下一代鲁棒、可扩展的多模态搜索系统提供了一个现实的端到端测试平台。", "translation": "大规模视觉搜索引擎有望同时解决两个问题：(i) 定位所有真正包含句子描述的物体的图像，以及 (ii) 在每个命中结果中识别物体的边界框或精确像素。现有技术仅解决了这一挑战的其中一面。视觉定位能产生紧密的边界框和掩膜，但其基于物体存在于每张测试图像中的不切实际假设，当应用于网络规模集合时会产生大量误报。文本到图像检索擅长筛选海量数据库以对相关图像进行排序，但它止步于整图匹配，不提供细粒度的定位。我们引入了指代搜索与发现（ReSeDis），这是第一个将语料库级检索与像素级定位相结合的任务。给定一个自由形式的描述，ReSeDis模型必须决定查询物体是否出现在每张图像中，如果出现，则返回边界框或分割掩膜。为了实现严谨的研究，我们策划了一个基准，其中每个描述唯一映射到分散在大型、多样语料库中的物体实例，从而消除了意外匹配。我们进一步设计了一个任务特定的度量标准，共同衡量检索召回率和定位精度。最后，我们提供了一个使用冻结视觉-语言模型的直接零样本基线，揭示了未来研究的巨大空间。ReSeDis为构建下一代鲁棒和可扩展的多模态搜索系统提供了一个现实的、端到端的测试平台。", "summary": "本文引入了ReSeDis数据集和任务，旨在弥合大规模图像检索与细粒度物体定位之间的鸿沟。现有方法要么侧重于图像级检索，要么侧重于像素级定位，且都存在局限性。ReSeDis要求模型根据自由形式描述，不仅识别图像中是否存在目标物体，还要精确地定位它。为支持研究，作者构建了一个独特的基准数据集，并设计了结合检索召回率和定位精度的评估指标。初步的零样本基线展示了该领域巨大的研究潜力。", "keywords": "指代搜索, 物体定位, 图像检索, 大规模数据集, 视觉-语言模型", "comments": "ReSeDis的创新之处在于首次将大规模图像检索和像素级物体定位这两个长期分离的任务统一起来，解决了现有视觉搜索系统无法同时提供全面检索和精确识别的痛点。其构建的独特数据集和评估指标为该领域提供了急需的、更现实的基准，对未来多模态搜索系统的发展具有重要意义。"}}
{"id": "2506.14929", "title": "FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning", "authors": ["Ganyu Wang", "Jinjie Fang", "Maxwell J. Ying", "Bin Gu", "Xi Chen", "Boyu Wang", "Charles Ling"], "summary": "Black-Box Discrete Prompt Learning is a prompt-tuning method that optimizes\ndiscrete prompts without accessing model parameters or gradients, making the\nprompt tuning on a cloud-based Large Language Model (LLM) feasible. Adapting\nfederated learning to BDPL could further enhance prompt tuning performance by\nleveraging data from diverse sources. However, all previous research on\nfederated black-box prompt tuning had neglected the substantial query cost\nassociated with the cloud-based LLM service. To address this gap, we conducted\na theoretical analysis of query efficiency within the context of federated\nblack-box prompt tuning. Our findings revealed that degrading FedAvg to\nactivate only one client per round, a strategy we called \\textit{FedOne},\nenabled optimal query efficiency in federated black-box prompt learning.\nBuilding on this insight, we proposed the FedOne framework, a federated\nblack-box discrete prompt learning method designed to maximize query efficiency\nwhen interacting with cloud-based LLMs. We conducted numerical experiments on\nvarious aspects of our framework, demonstrating a significant improvement in\nquery efficiency, which aligns with our theoretical results.", "comment": "Published in Proceedings of the 42nd International Conference on\n  Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14929v1", "AI": {"title_translation": "FedOne：面向黑盒离散提示学习的查询高效联邦学习", "tldr": "FedOne是一种联邦学习框架，通过每轮只激活一个客户端来优化黑盒离散提示学习的查询效率，解决了云LLM服务的高查询成本问题，并取得了显著的效率提升。", "motivation": "现有的联邦黑盒提示调优研究忽略了与基于云的LLM服务相关的巨大查询成本。", "method": "对联邦黑盒提示调优中的查询效率进行了理论分析。提出FedOne框架，该框架将FedAvg降级为每轮仅激活一个客户端，以实现联邦黑盒离散提示学习中的最佳查询效率。通过数值实验验证了该框架。", "result": "理论分析表明，将FedAvg降级为每轮仅激活一个客户端（即FedOne策略）能够实现联邦黑盒提示学习中的最优查询效率。数值实验证明了FedOne框架在查询效率方面有显著提升，与理论结果一致。", "conclusion": "FedOne框架通过独特地将FedAvg降级为每轮仅激活一个客户端，有效地解决了联邦黑盒离散提示学习中云LLM服务的查询成本问题，并实现了最佳的查询效率。", "translation": "黑盒离散提示学习是一种提示调优方法，它无需访问模型参数或梯度即可优化离散提示，从而使基于云的大型语言模型（LLM）上的提示调优变得可行。将联邦学习应用于BDPL可以通过利用来自不同来源的数据进一步提高提示调优性能。然而，所有先前关于联邦黑盒提示调优的研究都忽略了与基于云的LLM服务相关的巨大查询成本。为了弥补这一空白，我们对联邦黑盒提示调优背景下的查询效率进行了理论分析。我们的发现表明，将FedAvg降级为每轮仅激活一个客户端（我们称之为FedOne策略）可以在联邦黑盒离散提示学习中实现最优查询效率。基于这一见解，我们提出了FedOne框架，这是一种联邦黑盒离散提示学习方法，旨在与基于云的LLM交互时最大化查询效率。我们对框架的各个方面进行了数值实验，结果表明查询效率显著提高，这与我们的理论结果一致。", "summary": "本文提出了FedOne框架，旨在解决联邦黑盒离散提示学习中与云端LLM服务相关的高昂查询成本问题。通过理论分析，研究发现将FedAvg降级为每轮仅激活一个客户端的FedOne策略能够实现最优查询效率。数值实验验证了该框架在查询效率上的显著提升，证实了其理论优势。", "keywords": "联邦学习, 黑盒提示学习, 查询效率, 大语言模型, FedOne", "comments": "FedOne的创新之处在于其通过限制每轮仅激活一个客户端来解决联邦黑盒提示学习中的查询效率瓶颈，这是一种简单而有效的策略。该方法对于在资源受限或成本敏感的云LLM环境中部署联邦学习具有重要意义，尤其是在需要保护模型参数的黑盒场景下。"}}
{"id": "2506.15415", "title": "Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning", "authors": ["Stanley Ngugi"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.", "comment": "11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15415v1", "AI": {"title_translation": "目标词汇注入：通过早期层LoRA微调解锁Lugha-Llama中潜在的跨语言对齐", "tldr": "本研究提出了一种名为目标词汇注入（TLI）的新型高效微调方法，通过利用Lugha-Llama在早期层中已有的跨语言词汇对齐能力，显著提高了低资源语言LLM的输出级词汇对齐，并且泛化能力强。", "motivation": "大型语言模型在低资源语言（如斯瓦希里语）中的表现因数据稀缺和预训练不足而滞后。一个关键挑战是实现稳健的跨语言词汇对齐，这对于翻译和跨语言信息检索等任务至关重要。", "method": "本研究首先发现斯瓦希里语为中心的LLM Lugha-Llama-8B-wura在其早期内部层（Layer 2）中对斯瓦希里语-英语词对表现出接近完美的词汇对齐。TLI利用低秩适应（LoRA）和对比学习目标来微调模型，专门针对这个经验识别出的最佳早期层的嵌入进行调整。", "result": "TLI显著改善了623个训练过的斯瓦希里语-英语词对的输出级词汇对齐，平均余弦相似度从0.3211增加到0.4113（+28.08%）。更重要的是，这些改进对63个未见的对照词对表现出极好的泛化能力，相似度从0.3143增加到0.4033（+28.32%）。", "conclusion": "TLI增强了模型保留和传播其固有的早期层跨语言知识的能力，为改善以低资源语言为重点的LLM中的词汇对齐提供了一种参数高效且有效的策略。", "translation": "大型语言模型（LLM）展示了卓越的能力，但由于数据稀缺和预训练中的代表性不足，它们在低资源语言（LRLs），如斯瓦希里语中的表现通常滞后。实现稳健的跨语言词汇对齐是关键挑战，这对于翻译和跨语言信息检索等任务至关重要。本文介绍了目标词汇注入（TLI），一种新颖高效的微调方法。我们首先证明，以斯瓦希里语为中心的LLM Lugha-Llama-8B-wura在其早期内部层（特别是第2层，根据一项初步研究，平均余弦相似度约为0.99998）中对斯瓦希里语-英语词对表现出强大、近乎完美的词汇对齐能力，而这种能力并未完全体现在其最终输出表示中（基线在我们的评估集上相似度约为0.32）。TLI利用这一洞察，通过使用低秩适应（LoRA）和对比学习目标来微调模型，专门针对从这个经验识别出的最佳早期层中提取的嵌入。我们的实验表明，TLI显著改善了623个训练过的斯瓦希里语-英语词对的输出级词汇对齐，将平均余弦相似度从0.3211提高到0.4113（+28.08%，p < 1.33 x 10^-240）。更重要的是，这些改进对63个未见的对照词对表现出极好的泛化能力，相似度从0.3143增加到0.4033（+28.32%，p < 7.17 x 10^-27）。这些发现表明，TLI增强了模型保留和传播其固有的早期层跨语言知识的能力，为改善以低资源语言为重点的LLM中的词汇对齐提供了一种参数高效且有效的策略。", "summary": "本论文提出了一种名为目标词汇注入（TLI）的新型微调方法，旨在解决大型语言模型在低资源语言中跨语言词汇对齐能力不足的问题。研究发现，Lugha-Llama模型在早期层已表现出强大的跨语言对齐能力，但这种能力未充分体现在最终输出中。TLI利用LoRA和对比学习，专门微调模型早期层的嵌入，从而显著提高了斯瓦希里语-英语词对的输出级词汇对齐，并且对未见词对也具有良好的泛化能力。这表明TLI是一种参数高效且有效提升低资源语言LLM词汇对齐的策略。", "keywords": "低资源语言, 跨语言对齐, LoRA, 微调, 大型语言模型", "comments": "该论文的创新点在于发现LLM在早期层就已存在潜在的跨语言对齐能力，并提出了一种有针对性的、参数高效的微调方法（TLI）来解锁和利用这种能力。这种方法通过局部微调关键层，避免了对整个模型进行昂贵的全面微调，对于提升低资源语言LLM的性能具有重要意义。其有效性和泛化能力也得到了实验验证。"}}
{"id": "2506.15674", "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers", "authors": ["Tommaso Green", "Martin Gubri", "Haritz Puerto", "Sangdoo Yun", "Seong Joon Oh"], "summary": "We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15674v1", "AI": {"title_translation": "泄露的思想：大型推理模型并非私密思考者", "tldr": "大型推理模型在推理过程中会泄露敏感用户数据，增加推理步骤会加剧这种泄露，暴露出隐私与效用之间的矛盾。", "motivation": "挑战“推理痕迹是内部且安全”的假设，研究大型推理模型作为个人代理时推理痕迹中的隐私泄露问题。", "method": "通过探针（probing）和代理评估（agentic evaluations）来证明推理痕迹中敏感用户数据的提取和泄露，并分析测试时计算方法（特别是增加推理步骤）对泄露的影响。", "result": "推理痕迹频繁包含敏感用户数据，可通过提示注入或意外泄露到输出中。增加推理步骤会放大隐私泄露，尽管最终答案更谨慎，但模型思考过程更冗长，泄露更多。", "conclusion": "推理提高了模型效用，但扩大了隐私攻击面。安全工作必须扩展到模型的内部思考，而不仅仅是其输出。", "translation": "我们研究了用作个人代理的大型推理模型在推理痕迹中的隐私泄露问题。与最终输出不同，推理痕迹通常被认为是内部和安全的。我们通过展示推理痕迹频繁包含敏感用户数据来挑战这一假设，这些数据可以通过提示注入提取或意外泄露到输出中。通过探针和代理评估，我们证明了测试时计算方法，特别是增加推理步骤，会放大此类泄露。虽然增加这些测试时计算方法的预算会使模型在最终答案中更加谨慎，但它也导致它们在思考过程中更加冗长，泄露更多。这揭示了一个核心矛盾：推理提高了效用，但扩大了隐私攻击面。我们认为安全工作必须扩展到模型的内部思考，而不仅仅是其输出。", "summary": "这项研究揭示了大型推理模型在作为个人代理时，其内部推理痕迹并非私密，而是频繁泄露敏感用户数据。研究通过实验证明，增加推理步骤虽然能提高模型最终输出的谨慎性，却也加剧了内部思考过程中的隐私泄露。这暴露了模型效用提升与隐私安全之间的核心矛盾，强调了安全防护应扩展至模型的内部推理过程。", "keywords": "大型推理模型, 隐私泄露, 推理痕迹, 提示注入, 安全性", "comments": "这篇论文揭示了大型语言模型隐私安全的一个重要且常被忽视的方面——其内部推理过程的隐私泄露问题。其创新之处在于挑战了推理痕迹“内部安全”的传统假设，并量化了推理步骤增加对隐私泄露的影响，指出了效用与隐私之间的内在张力。这对于未来LLM的安全设计和部署具有重要指导意义，提醒开发者不仅要关注模型输出的安全性，还要深入到模型“思考”的层面进行隐私保护。"}}
{"id": "2506.15666", "title": "Vision in Action: Learning Active Perception from Human Demonstrations", "authors": ["Haoyu Xiong", "Xiaomeng Xu", "Jimmy Wu", "Yifan Hou", "Jeannette Bohg", "Shuran Song"], "summary": "We present Vision in Action (ViA), an active perception system for bimanual\nrobot manipulation. ViA learns task-relevant active perceptual strategies\n(e.g., searching, tracking, and focusing) directly from human demonstrations.\nOn the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to\nenable flexible, human-like head movements. To capture human active perception\nstrategies, we design a VR-based teleoperation interface that creates a shared\nobservation space between the robot and the human operator. To mitigate VR\nmotion sickness caused by latency in the robot's physical movements, the\ninterface uses an intermediate 3D scene representation, enabling real-time view\nrendering on the operator side while asynchronously updating the scene with the\nrobot's latest observations. Together, these design elements enable the\nlearning of robust visuomotor policies for three complex, multi-stage bimanual\nmanipulation tasks involving visual occlusions, significantly outperforming\nbaseline systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15666v1", "AI": {"title_translation": "行动中的视觉：从人类演示中学习主动感知", "tldr": "ViA是一个双臂机器人操作系统，通过机器人颈部和VR遥操作界面从人类演示中学习主动感知策略，在复杂任务中表现优于基线系统。", "motivation": "旨在使双臂机器人能够从人类演示中学习任务相关的主动感知策略，以实现复杂的双臂操作。", "method": "该研究提出了Vision in Action (ViA)系统，用于双臂机器人操作中的主动感知。ViA通过一个简单的六自由度机器人颈部实现灵活的头部运动，并设计了一个基于VR的遥操作界面来捕捉人类的主动感知策略。为缓解VR运动眩晕，该界面采用中间3D场景表示，实现实时视图渲染并异步更新机器人观测，从而从人类演示中学习任务相关的主动感知策略。", "result": "这些设计元素使得系统能够学习用于三个涉及视觉遮挡的复杂、多阶段双臂操作任务的鲁棒视觉运动策略，并显著优于基线系统。", "conclusion": "该系统成功地从人类演示中学习了用于复杂双臂操作任务的鲁棒主动感知策略，并表现出优异的性能。", "translation": "我们提出了“行动中的视觉”（ViA），一个用于双臂机器人操作的主动感知系统。ViA直接从人类演示中学习任务相关的主动感知策略（例如，搜索、跟踪和聚焦）。在硬件方面，ViA采用了一个简单而有效的六自由度机器人颈部，以实现灵活的、类似人类的头部运动。为了捕捉人类的主动感知策略，我们设计了一个基于VR的遥操作界面，在机器人和人类操作员之间创建了一个共享的观察空间。为了减轻由机器人物理运动延迟引起的VR运动眩晕，该界面使用了一个中间3D场景表示，使得操作员侧能够实时渲染视图，同时异步更新机器人最新的观测。总而言之，这些设计元素使得系统能够学习用于三个涉及视觉遮挡的复杂、多阶段双臂操作任务的鲁棒视觉运动策略，并显著优于基线系统。", "summary": "本文介绍了“行动中的视觉”（ViA），一个用于双臂机器人操作的主动感知系统，它从人类演示中学习任务相关的感知策略。ViA集成了六自由度机器人颈部以实现类人头部运动，以及一个基于VR的遥操作界面，该界面旨在捕捉人类主动感知，并通过中间3D场景表示来减轻运动眩晕。这种方法使得系统能够学习用于涉及视觉遮挡的复杂双臂任务的鲁棒视觉运动策略，并显著优于基线系统。", "keywords": "主动感知, 双臂机器人, 人类演示, VR遥操作, 视觉运动策略", "comments": "该论文的创新点在于其ViA系统能够直接从人类演示中学习主动感知策略，特别是针对复杂的双臂机器人操作任务。其VR遥操作界面巧妙地解决了延迟引起的运动眩晕问题，并通过共享观察空间有效地捕捉人类感知数据。六自由度机器人颈部的应用也增强了系统的类人视觉能力。这项工作对于提升机器人在视觉遮挡环境下的操作鲁棒性具有重要意义。"}}
{"id": "2506.15200", "title": "Conquering the Retina: Bringing Visual in-Context Learning to OCT", "authors": ["Alessio Negrini", "Simon Reiß"], "summary": "Recent advancements in medical image analysis have led to the development of\nhighly specialized models tailored to specific clinical tasks. These models\nhave demonstrated exceptional performance and remain a crucial research\ndirection. Yet, their applicability is limited to predefined tasks, requiring\nexpertise and extensive resources for development and adaptation. In contrast,\ngeneralist models offer a different form of utility: allowing medical\npractitioners to define tasks on the fly without the need for task-specific\nmodel development. In this work, we explore how to train generalist models for\nthe domain of retinal optical coherence tomography using visual in-context\nlearning (VICL), i.e., training models to generalize across tasks based on a\nfew examples provided at inference time. To facilitate rigorous assessment, we\npropose a broad evaluation protocol tailored to VICL in OCT. We extensively\nevaluate a state-of-the-art medical VICL approach on multiple retinal OCT\ndatasets, establishing a first baseline to highlight the potential and current\nlimitations of in-context learning for OCT. To foster further research and\npractical adoption, we openly release our code.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15200v1", "AI": {"title_translation": "征服视网膜：将视觉上下文学习引入OCT", "tldr": "本文探索了如何将视觉上下文学习（VICL）应用于视网膜光学相干断层扫描（OCT），以开发能够处理各种任务的通用模型，并建立了一个初步的基线。", "motivation": "现有的医学图像分析模型高度专业化，但其适用性受限于预定义任务，开发和适应需要大量资源。研究旨在开发通用模型，使医生能够即时定义任务，无需开发特定任务的模型。", "method": "作者通过视觉上下文学习（VICL）训练用于视网膜光学相干断层扫描（OCT）领域的通用模型。他们提出了一个针对OCT中VICL的广泛评估协议，并在一系列视网膜OCT数据集上广泛评估了一种最先进的医学VICL方法。", "result": "他们的工作为OCT的上下文学习建立了一个初步的基线，并突出了其潜力与当前的局限性。", "conclusion": "视觉上下文学习在OCT领域具有潜力，但仍存在局限性，需要进一步研究和实际应用。", "translation": "近期医学图像分析的进展催生了针对特定临床任务的高度专业化模型。这些模型表现出卓越的性能，并仍然是一个重要的研究方向。然而，它们的适用性仅限于预定义任务，需要专业知识和大量资源进行开发和适应。相比之下，通用模型提供了另一种形式的效用：允许医疗从业者即时定义任务，而无需开发特定任务的模型。在这项工作中，我们探讨了如何利用视觉上下文学习（VICL），即训练模型在推理时根据提供的少量示例进行跨任务泛化，来训练视网膜光学相干断层扫描（OCT）领域的通用模型。为了促进严格评估，我们提出了一种针对OCT中VICL的广泛评估协议。我们在多个视网膜OCT数据集上广泛评估了一种最先进的医学VICL方法，建立了一个初步的基线，以突显上下文学习在OCT中的潜力和当前局限性。为了促进进一步的研究和实际应用，我们公开了我们的代码。", "summary": "本文旨在解决现有医学图像分析模型专业化程度高但泛化能力差的问题，提出将视觉上下文学习（VICL）应用于视网膜光学相干断层扫描（OCT）领域，以开发能够根据少量示例即时适应新任务的通用模型。研究提出了一个全面的VICL评估协议，并在一系列OCT数据集上评估了先进的VICL方法，为该领域建立了首个基线，展示了其潜力与局限性，并公开了代码以促进后续研究。", "keywords": "视觉上下文学习, 光学相干断层扫描, 通用模型, 视网膜, 医学图像分析", "comments": "这项工作创新性地将视觉上下文学习引入医学图像分析的OCT领域，旨在克服传统专业化模型的局限性，为开发更灵活、适应性更强的通用医疗AI模型奠定了基础。公开代码有助于推动该领域的进一步研究和实际应用。"}}
{"id": "2506.15425", "title": "Understanding GUI Agent Localization Biases through Logit Sharpness", "authors": ["Xingjian Tao", "Yiwei Wang", "Yujun Cai", "Zhicheng Yang", "Jing Tang"], "summary": "Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15425v1", "AI": {"title_translation": "通过Logit锐度理解GUI代理定位偏差", "tldr": "本研究提出了一套新的评估框架、峰值锐度分数（PSS）和上下文感知裁剪技术，以深入理解并改善多模态大语言模型在GUI代理中存在的定位偏差和幻觉问题。", "motivation": "尽管多模态大语言模型（MLLMs）在使GUI代理与操作系统交互方面表现出色，但它们经常出现幻觉——系统性的定位错误，这损害了可靠性。本文旨在解决这些定位偏差和幻觉问题。", "method": "本文提出了一个细粒度的评估框架，将模型预测分为四种不同类型，揭示了超越传统准确性指标的细微失败模式。为更好地量化模型不确定性，引入了峰值锐度分数（PSS），该指标评估语义连续性与坐标预测中logits分布之间的一致性。在此基础上，进一步提出了上下文感知裁剪（Context-Aware Cropping），这是一种无需训练的技术，通过自适应地优化输入上下文来提高模型性能。", "result": "广泛的实验表明，本文提出的框架和方法提供了可操作的见解，并增强了GUI代理行为的可解释性和鲁棒性。", "conclusion": "本研究通过引入新的评估框架、峰值锐度分数（PSS）和上下文感知裁剪技术，成功地深入理解并改善了多模态大语言模型在GUI代理中的定位偏差和幻觉问题，提升了模型的可靠性和可解释性。", "translation": "多模态大语言模型（MLLMs）通过将语言与空间动作关联，使得GUI代理能够与操作系统交互。尽管它们表现出良好的性能，但这些模型经常出现幻觉——系统性的定位错误，这损害了可靠性。我们提出了一个细粒度的评估框架，将模型预测分为四种不同类型，揭示了超越传统准确性指标的细微失败模式。为了更好地量化模型不确定性，我们引入了峰值锐度分数（PSS），该指标评估语义连续性与坐标预测中logits分布之间的一致性。在此基础上，我们进一步提出了上下文感知裁剪（Context-Aware Cropping），这是一种无需训练的技术，通过自适应地优化输入上下文来提高模型性能。广泛的实验表明，我们的框架和方法提供了可操作的见解，并增强了GUI代理行为的可解释性和鲁棒性。", "summary": "本研究关注多模态大语言模型（MLLMs）在GUI代理中出现的定位偏差和幻觉问题。为解决这些问题，论文提出了一套细粒度评估框架以识别模型失败模式，引入了峰值锐度分数（PSS）来量化模型不确定性，并开发了无需训练的上下文感知裁剪技术以优化输入上下文。实验证明，这些方法有效提升了GUI代理行为的可解释性和鲁棒性，为理解和改进MMLMs的可靠性提供了新的视角。", "keywords": "GUI代理, 定位偏差, Logit锐度, 多模态大语言模型, 幻觉", "comments": "这篇论文的创新点在于提出了一个多方面的解决方案来解决GUI代理中MLLMs的定位幻觉问题。细粒度的评估框架超越了传统准确率，提供了更深入的错误分析；峰值锐度分数（PSS）为量化模型不确定性提供了一个新颖的度量；而上下文感知裁剪则是一种实用且无需训练的性能提升技术。这些方法的结合有望显著提高GUI代理的可靠性和可解释性，对于推动实际应用具有重要意义。"}}
{"id": "2506.15680", "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos", "authors": ["Kaifeng Zhang", "Baoyu Li", "Kris Hauser", "Yunzhu Li"], "summary": "Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .", "comment": "Project page: https://kywind.github.io/pgnd", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15680v1", "AI": {"title_translation": "用于从RGB-D视频学习可变形物体模型的粒子-网格神经动力学", "tldr": "本文提出了一种结合粒子和空间网格的神经动力学框架，用于从RGB-D视频中学习可变形物体的动力学模型，并能实现数字孪生和模型基规划。", "motivation": "由于可变形物体物理特性多样以及从有限视觉信息中估计状态的困难，对可变形物体动力学建模具有挑战性。", "method": "本文提出了一种神经动力学框架，该框架结合了物体粒子和空间网格的混合表示。粒子表示物体形状，空间网格离散化3D空间以确保空间连续性和提高学习效率。该模型能够捕获全局形状和运动信息，并预测密集的粒子运动。结合高斯溅射进行视觉渲染，实现可变形物体的完全基于学习的数字孪生，并生成3D动作条件视频。", "result": "该模型能够从稀疏视角RGB-D机器人-物体交互记录中学习多种物体的动力学（如绳索、布料、毛绒玩具、纸袋），并在类别层面泛化到未见实例。它在有限摄像机视角场景下优于最先进的基于学习和基于物理的模拟器。此外，学习到的模型在模型基规划中也展现出实用性，能够实现目标条件物体操作。", "conclusion": "本文提出的粒子-网格神经动力学框架有效解决了可变形物体动力学建模的挑战，实现了高精度的数字孪生和实用的模型基规划能力，在视觉信息有限的情况下表现出色。", "translation": "由于可变形物体物理特性的多样性以及从有限视觉信息中估计状态的困难，对可变形物体动力学进行建模具有挑战性。我们通过一个神经动力学框架解决了这些挑战，该框架在混合表示中结合了物体粒子和空间网格。我们的粒子-网格模型捕获全局形状和运动信息，同时预测密集的粒子运动，从而能够对具有不同形状和材料的物体进行建模。粒子代表物体形状，而空间网格对3D空间进行离散化，以确保空间连续性并提高学习效率。结合高斯溅射进行视觉渲染，我们的框架实现了可变形物体的完全基于学习的数字孪生，并生成3D动作条件视频。通过实验，我们证明了我们的模型能够从机器人-物体交互的稀疏视角RGB-D记录中学习各种物体——例如绳索、布料、毛绒玩具和纸袋——的动力学，同时在类别级别上泛化到未见实例。我们的方法优于最先进的基于学习和基于物理的模拟器，特别是在摄像机视角有限的场景中。此外，我们展示了我们学习到的模型在基于模型的规划中的效用，从而能够在各种任务中实现目标条件物体操作。项目页面位于 https://kywind.github.io/pgnd。", "summary": "本文提出了一种名为“粒子-网格神经动力学”的新型框架，用于从RGB-D视频中学习可变形物体的动力学模型。该框架结合了粒子表示物体形状和空间网格确保空间连续性与学习效率，能够捕获全局信息并预测密集粒子运动。结合高斯溅射，实现了可变形物体的数字孪生和3D动作条件视频生成。实验证明，该模型能有效学习多种物体的动力学，在有限视觉条件下优于现有方法，并支持模型基规划进行目标操作。", "keywords": "可变形物体, 神经动力学, 粒子-网格模型, RGB-D, 机器人操作", "comments": "该论文的创新点在于其混合表示方法，结合了粒子和空间网格，有效解决了可变形物体动力学建模的复杂性。其能够从稀疏视觉数据中学习并泛化到新实例的能力非常重要，特别是在机器人操作等实际应用中具有巨大潜力。此外，与高斯溅射的结合实现了高质量的视觉渲染，进一步提升了数字孪生的实用性。该研究为可变形物体操作和模拟提供了一个强大的学习范式。"}}
{"id": "2506.15201", "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models", "authors": ["Xuelin Shen", "Jiayin Xu", "Kangsheng Yin", "Wenhan Yang"], "summary": "The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.", "comment": "11 pages, 6 figures, publised to ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15201v1", "AI": {"title_translation": "隐私保护图像压缩：防御视觉-语言预训练模型的利用", "tldr": "提出一种隐私保护图像压缩方法（PSIC），可在保持图像感知质量的同时，阻止VLP模型对图像的语义理解，并能按需恢复完整语义信息。", "motivation": "视觉-语言预训练（VLP）模型日益增强的语义理解能力使得公开图像难以防范搜索引擎等工具的利用，从而威胁用户隐私。", "method": "本文提出一种灵活的隐私保护图像压缩（PSIC）方法，可生成具有多重解码选项的比特流。默认解码在保持满意感知质量的同时，阻止VLP模型解释图像。通过可定制输入条件，该方案能重建保留完整语义信息的图像。引入条件潜在触发生成（CLTG）模块，根据可定制条件生成偏差信息以引导解码过程。设计不确定性感知加密导向（UAEO）优化函数，利用目标VLP模型在训练数据上的不确定性推断出的软标签。此外，采用自适应多目标优化策略，在统一训练过程中同时提高加密性能和感知质量。该方案可即插即用，无缝集成到大多数现有学习图像压缩（LIC）模型中。", "result": "在多个下游任务上的大量实验证明了所提出设计的有效性。", "conclusion": "本文提出了一种有效的隐私保护图像压缩方案，能够防御VLP模型对图像的利用，同时保持图像质量和原始压缩功能，并且易于集成。", "translation": "VLP模型语义理解能力的提升使得保护公开图像免受搜索引擎和其他类似工具的利用变得越来越困难。在此背景下，本文旨在通过在图像压缩阶段实施防御来防止图像被利用，从而保护用户隐私。具体而言，我们提出了一种灵活的编码方法，称为隐私保护图像压缩（PSIC），它可以生成具有多个解码选项的比特流。默认情况下，比特流解码后既能保持令人满意的感知质量，又能防止VLP模型进行解释。我们的方法还保留了原始的图像压缩功能。通过可定制的输入条件，所提出的方案可以重建保留其完整语义信息的图像。本文提出了一种条件潜在触发生成（CLTG）模块，根据可定制的条件生成偏差信息以引导解码过程生成不同的重建版本，并设计了一个不确定性感知加密导向（UAEO）优化函数，利用从目标VLP模型在训练数据上的不确定性推断出的软标签。本文进一步结合了自适应多目标优化策略，在统一的训练过程中同时获得改进的加密性能和感知质量。所提出的方案是即插即用的，可以无缝集成到大多数现有的学习图像压缩（LIC）模型中。在多个下游任务上的大量实验证明了我们设计的有效性。", "summary": "本文提出一种名为隐私保护图像压缩（PSIC）的新型图像编码方法，旨在防御视觉-语言预训练（VLP）模型对公开图像的利用，从而保护用户隐私。PSIC能生成多解码选项的比特流，默认解码在保持良好感知质量的同时阻止VLP模型理解图像语义。通过特定条件，也能恢复图像的完整语义信息。该方法引入了条件潜在触发生成（CLTG）模块和不确定性感知加密导向（UAEO）优化函数，并采用自适应多目标优化策略。PSIC可即插即用，易于集成到现有学习图像压缩模型中，并通过实验验证了其有效性。", "keywords": "隐私保护, 图像压缩, 视觉-语言预训练模型, PSIC, 语义理解", "comments": "这篇论文的创新点在于将隐私保护机制集成到图像压缩流程中，特别是通过生成多重解码选项的比特流来同时满足人类可读性和机器难理解性。其提出的CLTG模块和UAEO优化函数是实现这一目标的关键技术。该方法的重要性在于它提供了一种实用的防御策略，以应对VLP模型对图像语义的日益增强的利用，从而帮助用户保护其在线隐私。其即插即用的特性也增加了其实用性。"}}
{"id": "2506.15451", "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need", "authors": ["Zhouhong Gu", "Xiaoxuan Zhu", "Yin Cai", "Hao Shen", "Xingzhou Chen", "Qingyi Wang", "Jialin Li", "Xiaoran Shi", "Haoran Guo", "Wenxuan Huang", "Hongwei Feng", "Yanghua Xiao", "Zheyu Ye", "Yao Hu", "Shaosheng Cao"], "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15451v1", "AI": {"title_translation": "AgentGroupChat-V2: 分而治之是基于LLM的多智能体系统所需要的", "tldr": "AgentGroupChat-V2引入分而治之的并行架构，显著提升了LLM多智能体系统在复杂任务上的性能和通用性。", "motivation": "当前基于大型语言模型的多智能体系统在系统架构设计、跨领域泛化能力和性能保证方面面临严峻挑战，尤其是在任务复杂性和智能体数量增加时。", "method": "AgentGroupChat-V2框架通过三项核心创新解决了这些挑战：1) 分而治之的完全并行架构，将用户查询分解为分层任务森林结构，实现依赖管理和分布式并发处理；2) 自适应协作引擎，根据任务特性动态选择异构LLM组合和交互模式；3) 结合分而治之方法的智能体组织优化策略，实现高效的问题分解。", "result": "AgentGroupChat-V2在不同领域表现出卓越性能，在GSM8K上达到91.50%的准确率（超过最佳基线5.6个百分点），在竞赛级AIME上达到30.4%的准确率（几乎是其他方法的两倍），在HumanEval上达到79.20%的pass@1。性能优势随着任务难度增加而愈发显著，特别是在Level 5 MATH问题上，相比现有最佳基线提升超过11个百分点。", "conclusion": "这些结果证实AgentGroupChat-V2为构建高效、通用的大型语言模型多智能体系统提供了一个全面的解决方案，在复杂推理场景中具有显著优势。", "translation": "基于大型语言模型的多智能体系统在社会模拟和复杂任务解决领域展现出巨大潜力。然而，当前框架在系统架构设计、跨领域泛化能力和性能保证方面面临严峻挑战，尤其是在任务复杂性和智能体数量增加时。我们引入了AgentGroupChat-V2，一个通过三项核心创新解决这些挑战的新颖框架：(1) 分而治之的完全并行架构，将用户查询分解为分层任务森林结构，实现依赖管理和分布式并发处理。(2) 自适应协作引擎，根据任务特性动态选择异构LLM组合和交互模式。(3) 结合分而治之方法的智能体组织优化策略，实现高效的问题分解。广泛的实验表明，AgentGroupChat-V2在不同领域表现出卓越性能，在GSM8K上达到91.50%的准确率（超过最佳基线5.6个百分点），在竞赛级AIME上达到30.4%的准确率（几乎是其他方法的两倍），在HumanEval上达到79.20%的pass@1。性能优势随着任务难度增加而愈发显著，特别是在Level 5 MATH问题上，相比现有最佳基线提升超过11个百分点。这些结果证实AgentGroupChat-V2为构建高效、通用的大型语言模型多智能体系统提供了一个全面的解决方案，在复杂推理场景中具有显著优势。代码可在https://github.com/MikeGu721/AgentGroupChat-V2获取。", "summary": "AgentGroupChat-V2是一个新颖的LLM多智能体系统框架，通过引入分而治之的完全并行架构、自适应协作引擎和优化的智能体组织策略，解决了现有框架在复杂任务和多智能体场景下的架构、泛化和性能挑战。实验证明，AgentGroupChat-V2在GSM8K、AIME和HumanEval等基准测试中均展现出显著优于现有方法的性能，尤其在处理高难度复杂推理任务时优势更为明显，为构建高效通用的LLM多智能体系统提供了全面解决方案。", "keywords": "LLM多智能体系统, 分而治之, 并行架构, 协作, 复杂推理", "comments": "AgentGroupChat-V2的创新点在于其将“分而治之”策略深度融入到LLM多智能体系统的架构设计中，包括任务分解、并行处理和智能体协作。这种方法有效地解决了现有系统在扩展性和复杂任务处理上的瓶颈，通过实验数据展示了其在多个复杂推理任务上的显著性能提升，特别是对于难度更高的数学和编程问题，证明了其在实际应用中的巨大潜力。"}}
{"id": "2506.15218", "title": "DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder", "authors": ["Dan He", "Weisheng Li", "Guofen Wang", "Yuping Huang", "Shiqiang Liu"], "summary": "Multimodal medical image fusion (MMIF) extracts the most meaningful\ninformation from multiple source images, enabling a more comprehensive and\naccurate diagnosis. Achieving high-quality fusion results requires a careful\nbalance of brightness, color, contrast, and detail; this ensures that the fused\nimages effectively display relevant anatomical structures and reflect the\nfunctional status of the tissues. However, existing MMIF methods have limited\ncapacity to capture detailed features during conventional training and suffer\nfrom insufficient cross-modal feature interaction, leading to suboptimal fused\nimage quality. To address these issues, this study proposes a two-stage\ndiffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In\nStage I, a diffusion process trains UNet for image reconstruction. UNet\ncaptures detailed information through progressive denoising and represents\nmultilevel data, providing a rich set of feature representations for the\nsubsequent fusion network. In Stage II, noisy images at various steps are input\ninto the fusion network to enhance the model's feature recognition capability.\nThree key fusion modules are also integrated to process medical images from\ndifferent modalities adaptively. Ultimately, the robust network structure and a\nhybrid loss function are integrated to harmonize the fused image's brightness,\ncolor, contrast, and detail, enhancing its quality and information density. The\nexperimental results across various medical image types demonstrate that the\nproposed method performs exceptionally well regarding objective evaluation\nmetrics. The fused image preserves appropriate brightness, a comprehensive\ndistribution of radioactive tracers, rich textures, and clear edges. The code\nis available at https://github.com/HeDan-11/DM-FNet.", "comment": "This paper has been accepted by IEEE Transactions on Multimedia (TMM)\n  in March 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15218v1", "AI": {"title_translation": "DM-FNet：通过扩散过程训练的编码器-解码器实现统一多模态医学图像融合", "tldr": "DM-FNet是一种新的两阶段扩散模型医学图像融合网络，解决了现有方法细节捕获不足和跨模态交互差的问题，显著提升了融合图像的质量，保留了亮度、纹理和边缘等关键信息。", "motivation": "现有的多模态医学图像融合（MMIF）方法在常规训练中捕获细节特征的能力有限，并且存在跨模态特征交互不足的问题，导致融合图像质量不佳。", "method": "本研究提出了一种两阶段的基于扩散模型的融合网络（DM-FNet）来实现统一的多模态医学图像融合。第一阶段，通过扩散过程训练UNet进行图像重建，以捕获详细信息并提供丰富的特征表示。第二阶段，将不同步骤的噪声图像输入融合网络，以增强模型的特征识别能力。同时，集成了三个关键融合模块来自适应处理不同模态的医学图像，并通过鲁棒的网络结构和混合损失函数来协调融合图像的亮度、颜色、对比度和细节。", "result": "在各种医学图像类型上的实验结果表明，所提出的方法在客观评价指标方面表现出色。融合图像保留了适当的亮度、放射性示踪剂的全面分布、丰富的纹理和清晰的边缘。", "conclusion": "DM-FNet有效解决了现有MMIF方法的局限性，通过其两阶段扩散模型训练和融合模块，能够生成高质量、信息密度更高的多模态医学融合图像。", "translation": "多模态医学图像融合（MMIF）从多个源图像中提取最有意义的信息，从而实现更全面、更准确的诊断。实现高质量的融合结果需要亮度、颜色、对比度和细节之间的仔细平衡；这确保了融合图像有效显示相关的解剖结构并反映组织的功​​能状态。然而，现有的MMIF方法在常规训练期间捕获详细特征的能力有限，并且存在跨模态特征交互不足的问题，导致融合图像质量不佳。为了解决这些问题，本研究提出了一种两阶段基于扩散模型的融合网络（DM-FNet）来实现统一的MMIF。在第一阶段，扩散过程训练UNet进行图像重建。UNet通过渐进式去噪捕获详细信息并表示多级数据，为后续的融合网络提供丰富的特征表示。在第二阶段，将不同步骤的噪声图像输入融合网络，以增强模型的特征识别能力。还集成了三个关键融合模块，以自适应处理来自不同模态的医学图像。最终，鲁棒的网络结构和混合损失函数相结合，以协调融合图像的亮度、颜色、对比度和细节，从而提高其质量和信息密度。在各种医学图像类型上的实验结果表明，所提出的方法在客观评价指标方面表现出色。融合图像保留了适当的亮度、放射性示踪剂的全面分布、丰富的纹理和清晰的边缘。代码可在https://github.com/HeDan-11/DM-FNet获取。", "summary": "DM-FNet是一种新型的两阶段扩散模型融合网络，旨在解决现有多模态医学图像融合（MMIF）方法在细节捕获和跨模态特征交互方面的不足。该方法通过在第一阶段使用扩散过程训练UNet进行图像重建以获取丰富特征，并在第二阶段将噪声图像输入融合网络并整合三个关键融合模块来增强特征识别和自适应处理。结合鲁棒的网络结构和混合损失函数，DM-FNet能有效平衡图像的亮度、颜色、对比度和细节，从而生成高质量、信息密度高的融合图像。实验结果表明其在多种医学图像上表现优异，能有效保留图像的关键视觉特征。", "keywords": "多模态医学图像融合, 扩散模型, DM-FNet, UNet, 图像质量", "comments": "该论文提出了一种创新的两阶段扩散模型方法来解决多模态医学图像融合的挑战。其创新点在于将扩散模型引入特征提取和融合过程，特别是通过UNet在第一阶段进行细节捕获，以及在第二阶段利用不同噪声级别的图像增强特征识别，这有助于克服传统方法在细节保留和跨模态交互方面的不足。该方法的提出对于提升医学图像诊断的准确性和全面性具有重要意义。"}}
{"id": "2506.14965", "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "authors": ["Zhoujun Cheng", "Shibo Hao", "Tianyang Liu", "Fan Zhou", "Yutao Xie", "Feng Yao", "Yuexin Bian", "Yonghao Zhuang", "Nilabjo Dey", "Yuheng Zha", "Yi Gu", "Kun Zhou", "Yuqi Wang", "Yuan Li", "Richard Fan", "Jianshu She", "Chengqian Gao", "Abulhair Saparov", "Haonan Li", "Taylor W. Killian", "Mikhail Yurochkin", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu"], "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360", "comment": "38 pages, 9 figures. Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14965v1", "AI": {"title_translation": "从跨领域视角重新审视LLM推理中的强化学习", "tldr": "论文提出了一个包含六个推理领域的RL推理语料库Guru，并基于此系统地研究了RL对LLM推理的影响，发现RL不仅能激发现有知识，还能促进新技能习得，并发布了SOTA模型。", "motivation": "当前强化学习（RL）在提高大型语言模型（LLM）推理能力方面的研究主要集中在数学和代码领域，限制了其在通用推理方面的应用理解。一个关键挑战是缺乏跨多样推理领域可靠且可扩展的RL奖励信号。", "method": "引入了一个名为Guru的精选RL推理语料库，包含9.2万个可验证示例，涵盖数学、代码、科学、逻辑、模拟和表格六个推理领域，每个领域都通过领域特定奖励设计、去重和过滤构建。基于Guru，系统性地重新审视了LLM推理中RL的既定发现，并提出了Guru-7B和Guru-32B模型。", "result": "观察到RL对LLM推理的影响在不同领域存在显著差异：预训练中频繁出现的领域（数学、代码、科学）容易从跨领域RL训练中受益，而预训练曝光有限的领域（逻辑、模拟、表格）需要领域内训练才能获得有意义的性能提升。Guru-7B和Guru-32B模型在公开模型中RL训练的性能达到了最先进水平，在17项任务评估套件中，比最佳基线分别高出7.9%和6.7%。模型有效提升了其基础模型的Pass@k性能，尤其是在预训练数据中不太可能出现的复杂任务上。", "conclusion": "强化学习不仅能帮助LLM激发已有的知识，更重要的是，它可能促进LLM获得真正的技能。跨领域RL训练对常见领域有效，但稀有领域仍需领域内训练。", "translation": "强化学习（RL）已成为提高大型语言模型（LLM）推理能力的一种有前景的方法，但大多数开放性工作都狭隘地集中在数学和代码上，限制了我们对其在通用推理方面更广泛适用性的理解。一个关键挑战在于缺乏跨多样推理领域可靠、可扩展的RL奖励信号。我们引入了Guru，一个精选的RL推理语料库，包含9.2万个可验证示例，涵盖数学、代码、科学、逻辑、模拟和表格六个推理领域——每个领域都通过领域特定的奖励设计、去重和过滤构建，以确保RL训练的可靠性和有效性。基于Guru，我们系统性地重新审视了LLM推理中RL的既定发现，并观察到跨领域的显著差异。例如，虽然先前的工作表明RL主要从预训练模型中激发现有知识，但我们的结果揭示了一种更细致的模式：在预训练期间经常出现的领域（数学、代码、科学）很容易从跨领域RL训练中受益，而预训练曝光有限的领域（逻辑、模拟和表格）需要领域内训练才能获得有意义的性能提升，这表明RL可能有助于真正的技能获取。最后，我们提出了Guru-7B和Guru-32B，这两个模型在公开可用数据进行RL训练的开放模型中达到了最先进的性能，在我们的17项任务评估套件中，跨六个推理领域分别超越了最佳基线7.9%和6.7%。我们还表明，我们的模型有效提高了其基础模型的Pass@k性能，特别是在预训练数据中不太可能出现的复杂任务上。我们发布了数据、模型、训练和评估代码，以促进通用推理，网址为：https://github.com/LLM360/Reasoning360。", "summary": "本文针对当前强化学习（RL）在大型语言模型（LLM）推理应用中领域受限的问题，构建了一个名为Guru的综合性跨领域RL推理语料库，包含9.2万个涵盖六个不同推理领域的示例。基于此语料库，研究发现RL对LLM推理的影响存在显著的领域差异：常见领域可从跨领域RL中受益，而稀有领域则需领域内训练，这表明RL可能促进LLM获得新技能。论文还提出了Guru-7B和Guru-32B模型，在多领域推理任务上取得了SOTA性能，并开源了相关资源。", "keywords": "强化学习, 大型语言模型, 推理, 跨领域, 数据集, 技能习得", "comments": "本文的创新之处在于构建了一个大规模、多领域、高质量的RL推理语料库Guru，弥补了现有研究在奖励信号方面的不足。研究结果深入揭示了RL对LLM推理能力提升的机制，特别是区分了“知识激发”和“技能习得”两种模式，具有重要的理论意义。模型的开源也极大地促进了通用推理领域的研究发展。"}}
{"id": "2506.15455", "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation", "authors": ["Xinnuo Xu", "Rachel Lawrence", "Kshitij Dubey", "Atharva Pandey", "Risa Ueno", "Fabian Falck", "Aditya V. Nori", "Rahul Sharma", "Amit Sharma", "Javier Gonzalez"], "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15455v1", "AI": {"title_translation": "RE-IMAGINE：符号基准合成用于推理评估", "tldr": "大型语言模型（LLMs）在推理基准上表现高准确率，但RE-IMAGINE框架通过符号基准合成揭示，其表现部分依赖统计记忆而非真实推理。", "motivation": "大型语言模型（LLMs）在推理基准上表现出高准确率，但尚不清楚这是否源于真实推理能力，还是对训练集的统计回忆。", "method": "本文引入了RE-IMAGINE框架，该框架受因果关系阶梯（关联、干预、反事实）启发，旨在表征LLMs推理能力的层次结构。该框架通过一个自动化管道，在中间符号表示中改变问题，生成无法仅通过记忆解决的任意数量的问题变体。RE-IMAGINE通用，可应用于数学、代码和逻辑等多个推理领域。", "result": "在四个广泛使用的基准上评估了几系列LLMs，当模型被问题变体查询时，观察到性能显著下降。", "conclusion": "这些评估表明，LLMs在过去的性能中存在一定程度的统计回忆依赖，并为针对推理层次结构中技能的进一步研究打开了大门。", "translation": "最近的大型语言模型（LLMs）在推理基准上报告了高准确率。然而，目前尚不清楚观察到的结果是源于真正的推理能力，还是源于对训练集的统计回忆。受因果关系阶梯（Pearl, 2009）及其三个层次（关联、干预和反事实）的启发，本文引入了RE-IMAGINE，一个用于表征LLMs推理能力层次结构的框架，以及一个自动化管道，用于在层次结构的不同级别生成问题变体。通过改变中间符号表示中的问题，RE-IMAGINE可以生成任意数量无法仅通过记忆解决的问题。此外，该框架通用，可以跨推理领域工作，包括数学、代码和逻辑。我们在四个广泛使用的基准上演示了我们的框架，以评估几系列LLMs，并观察到当模型被问题变体查询时性能下降。这些评估表明，过去的性能在一定程度上依赖于统计回忆，并为针对推理层次结构中技能的进一步研究打开了大门。", "summary": "本文介绍了RE-IMAGINE框架，旨在评估大型语言模型（LLMs）的真实推理能力，而非其对训练数据的统计记忆。该框架受因果关系阶梯启发，通过自动生成不同难度层次的符号化问题变体，确保问题无法仅凭记忆解决。实验结果显示，LLMs在面对这些变体问题时性能下降，这表明其在现有基准上的高表现部分依赖于记忆，而非纯粹的推理。RE-IMAGINE为未来研究LLMs的深层推理能力提供了新途径。", "keywords": "大型语言模型, 推理评估, 符号基准, 因果关系阶梯, 统计回忆", "comments": "RE-IMAGINE的创新之处在于其通过符号化和层次化的方式生成无法通过记忆解决的推理问题，有效地揭示了LLMs真实推理能力与统计回忆之间的界限。这对于理解LLMs的局限性并指导未来模型发展具有重要意义。该框架的通用性也是其优势。"}}
{"id": "2506.14978", "title": "ODD: Overlap-aware Estimation of Model Performance under Distribution Shift", "authors": ["Aayush Mishra", "Anqi Liu"], "summary": "Reliable and accurate estimation of the error of an ML model in unseen test\ndomains is an important problem for safe intelligent systems. Prior work uses\ndisagreement discrepancy (DIS^2) to derive practical error bounds under\ndistribution shifts. It optimizes for a maximally disagreeing classifier on the\ntarget domain to bound the error of a given source classifier. Although this\napproach offers a reliable and competitively accurate estimate of the target\nerror, we identify a problem in this approach which causes the disagreement\ndiscrepancy objective to compete in the overlapping region between source and\ntarget domains. With an intuitive assumption that the target disagreement\nshould be no more than the source disagreement in the overlapping region due to\nhigh enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).\nMaximizing ODD only requires disagreement in the non-overlapping target domain,\nremoving the competition. Our ODD-based bound uses domain-classifiers to\nestimate domain-overlap and better predicts target performance than DIS^2. We\nconduct experiments on a wide array of benchmarks to show that our method\nimproves the overall performance-estimation error while remaining valid and\nreliable. Our code and results are available on GitHub.", "comment": "Accepted to the 41st Conference on Uncertainty in Artificial\n  Intelligence, 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14978v1", "AI": {"title_translation": "ODD：分布偏移下模型性能的重叠感知估计", "tldr": "ODD (Overlap-aware Disagreement Discrepancy) 改进了 DIS^2 方法，通过解决源域和目标域重叠区域的竞争问题，更准确地估计了分布偏移下机器学习模型的性能。", "motivation": "现有方法 DIS^2 在估计分布偏移下模型误差时，其不一致性差异目标在源域和目标域的重叠区域存在竞争问题，导致估计不够准确。", "method": "提出了重叠感知不一致性差异 (ODD) 方法。ODD 通过直观假设目标域不一致性在重叠区域不应超过源域不一致性，并利用域分类器估计域重叠，只在非重叠目标域最大化不一致性，从而消除了竞争。", "result": "ODD-based 边界比 DIS^2 更好地预测了目标性能。实验表明，该方法提高了整体性能估计误差，同时保持了有效性和可靠性。", "conclusion": "ODD 通过解决重叠区域的竞争问题，为分布偏移下的模型性能估计提供了一种更准确、有效和可靠的方法。", "translation": "可靠准确地估计机器学习模型在未见测试域中的误差对于安全的智能系统是一个重要问题。先前的研究使用不一致性差异 (DIS^2) 来推导分布偏移下的实际误差界限。它优化目标域上最大不一致的分类器来限制给定源分类器的误差。尽管这种方法提供了可靠且具有竞争力的目标误差估计，但我们发现这种方法存在一个问题，导致不一致性差异目标在源域和目标域之间的重叠区域产生竞争。通过一个直观的假设，即由于足够高的支持度，目标不一致性在重叠区域不应超过源不一致性，我们设计了重叠感知不一致性差异 (ODD)。最大化 ODD 只要求非重叠目标域中的不一致性，从而消除了竞争。我们基于 ODD 的界限使用域分类器来估计域重叠，并且比 DIS^2 更好地预测目标性能。我们在广泛的基准测试中进行了实验，表明我们的方法提高了整体性能估计误差，同时保持了有效性和可靠性。我们的代码和结果可在 GitHub 上获取。", "summary": "这篇论文提出了 ODD (Overlap-aware Disagreement Discrepancy) 方法，旨在改进在分布偏移下机器学习模型性能的误差估计。现有方法 DIS^2 在源域和目标域的重叠区域存在竞争问题。ODD 通过假设目标域不一致性在重叠区域不应超过源域不一致性，并利用域分类器估计域重叠，从而仅在非重叠目标域最大化不一致性，有效地消除了竞争。实验结果表明，ODD 能够比 DIS^2 更准确地预测目标性能，并提高了整体性能估计的准确性和可靠性。", "keywords": "分布偏移, 模型性能估计, 不一致性差异, 域适应, ODD", "comments": "这篇论文通过引入“重叠感知”的概念，巧妙地解决了现有 DIS^2 方法在处理域重叠区域时的局限性，提高了分布偏移下模型性能估计的准确性。其创新点在于对不一致性差异目标的重新定义，使得估计更加精确和可靠，对于构建更安全的智能系统具有重要意义。"}}
{"id": "2506.15480", "title": "Context-Informed Grounding Supervision", "authors": ["Hyunji Lee", "Seunghyun Yoon", "Yunjae Won", "Hanseok Oh", "Geewook Kim", "Trung Bui", "Franck Dernoncourt", "Elias Stengel-Eskin", "Mohit Bansal", "Minjoon Seo"], "summary": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15480v1", "AI": {"title_translation": "上下文感知的基础监督", "tldr": "提出CINGS方法，通过上下文感知监督训练LLMs，显著增强模型在文本和视觉领域的接地能力，减少幻觉，且不影响通用性能。", "motivation": "大型语言模型（LLMs）在补充外部知识时，简单地在推理时附加上下文并不能确保生成的内容与上下文“接地”，这可能导致模型产生幻觉或提供不准确的信息。", "method": "本文提出上下文感知接地监督（CINGS），这是一种后训练监督方法。在该方法中，模型在训练时将相关上下文预置到响应之前，但损失计算仅针对响应的令牌进行，并屏蔽掉上下文部分。", "result": "实验表明，使用CINGS训练的模型在文本和视觉领域都表现出更强的接地能力。在文本领域，CINGS在11个信息寻求数据集中优于其他训练方法，并与推理时接地技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干，在四个基准测试中减少了幻觉，并保持了生成响应的事实一致性。这种改进的接地能力并未导致通用下游性能下降。分析表明，CINGS诱导了模型先验知识和行为的转变，隐式地鼓励模型更多地依赖外部上下文。", "conclusion": "CINGS是一种有效的后训练监督方法，能够显著增强大型语言模型在多模态场景下的接地能力和事实一致性，同时不影响其通用性能，其作用机制在于促使模型更依赖外部上下文。", "translation": "大型语言模型（LLMs）通常会补充外部知识，以提供其参数中未编码的信息或减少幻觉。在这种情况下，我们期望模型通过将其响应基于所提供的外部上下文来生成响应。然而，先前的研究表明，仅仅在推理时附加上下文并不能确保接地生成。为了解决这个问题，我们提出了上下文感知接地监督（CINGS），这是一种后训练监督方法，其中模型在训练时将相关上下文预置到响应之前，同时仅对响应令牌计算损失并屏蔽上下文。我们的实验表明，与标准指令调整模型相比，使用CINGS训练的模型在文本和视觉领域都表现出更强的接地能力。在文本领域，CINGS在11个信息寻求数据集中优于其他训练方法，并与推理时接地技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干，在四个基准测试中减少了幻觉，并保持了生成响应的事实一致性。这种改进的接地能力并未导致通用下游性能下降。最后，我们分析了CINGS中增强接地能力的潜在机制，发现它诱导了模型先验知识和行为的转变，隐式地鼓励模型更多地依赖外部上下文。", "summary": "本文提出了上下文感知接地监督（CINGS），一种针对大型语言模型（LLMs）的后训练方法，旨在解决现有方法中LLMs难以有效利用外部上下文进行接地生成和减少幻觉的问题。CINGS通过在训练时将相关上下文预置到响应前，并仅对响应部分计算损失，从而使模型隐式地更依赖外部信息。实验结果表明，CINGS显著增强了模型在文本和视觉任务中的接地能力，减少了幻觉，并且没有损害模型的通用性能。", "keywords": "大型语言模型, 接地监督, 幻觉减少, 上下文感知, 后训练", "comments": "CINGS的创新之处在于其独特的损失计算机制，即在预置上下文的情况下仅对响应令牌计算损失，这有效地引导模型学习如何将生成内容“接地”到提供的上下文中，而非简单记忆或忽略。这项工作对于提升LLMs的事实性和可靠性具有重要意义，特别是在知识密集型和多模态应用中。其分析模型行为转变的部分也提供了有价值的见解。"}}
{"id": "2506.15065", "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Xiaopan Zhang", "Fahim Faisal Niloy", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "summary": "Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15065v1", "AI": {"title_translation": "HEAL：一项关于大型语言模型驱动的具身智能体幻觉的实证研究", "tldr": "本研究系统地调查了大型语言模型驱动的具身智能体在场景-任务不一致下的幻觉问题，发现现有模型在处理不可行任务时存在根本性局限，并提出了改进建议。", "motivation": "大型语言模型（LLMs）正被广泛用作具身智能体的认知核心，但其固有的幻觉问题（源于未能将用户指令与观察到的物理环境相结合）会导致导航错误，例如寻找不存在的冰箱。本研究旨在系统地理解幻觉发生的程度、触发幻觉的不一致类型以及当前模型的响应方式。", "method": "构建了一个幻觉探测集，该探测集基于现有基准，能够将幻觉率提高高达40倍。评估了两个模拟环境中的12个模型。", "result": "模型展现出推理能力，但未能解决场景-任务不一致问题，这突显了其在处理不可行任务方面的根本性局限。", "conclusion": "大型语言模型驱动的具身智能体在处理场景-任务不一致时存在幻觉问题，现有模型难以处理不可行任务。研究为开发更稳健可靠的规划策略提供了可操作的见解。", "translation": "大型语言模型（LLMs）正越来越多地被采纳为具身智能体的认知核心。然而，源于未能将用户指令与观察到的物理环境相结合的固有幻觉，可能导致导航错误，例如寻找一个不存在的冰箱。在本文中，我们首次对基于LLM的具身智能体在场景-任务不一致下执行长周期任务时的幻觉进行了系统研究。我们的目标是了解幻觉发生的程度，哪些类型的不一致会触发它们，以及当前模型如何响应。为了实现这些目标，我们通过在现有基准上构建一个幻觉探测集，该探测集能够将幻觉率提高到比基础提示高40倍。通过评估两个模拟环境中的12个模型，我们发现模型虽然表现出推理能力，但它们未能解决场景-任务不一致问题——这突显了在处理不可行任务方面的根本性局限。我们还针对每种场景提供了理想模型行为的可操作见解，为开发更稳健和可靠的规划策略提供了指导。", "summary": "本研究首次系统地调查了大型语言模型（LLMs）驱动的具身智能体在执行长周期任务时，在场景-任务不一致情况下出现的幻觉问题。通过构建一个能够显著提高幻觉率的探测集，并评估12个模型，研究发现尽管模型具有推理能力，但它们在解决场景与任务不一致方面存在根本性缺陷，无法有效处理不可行任务。论文还为开发更稳健和可靠的规划策略提供了实用指导。", "keywords": "具身智能体, 大型语言模型, 幻觉, 场景-任务不一致, 实证研究", "comments": "这项研究创新性地首次系统性地探讨了LLM驱动的具身智能体中的幻觉问题，特别是在场景与任务不一致的复杂情况下。其构建高幻觉率探测集的方法具有重要意义，揭示了当前模型在处理不可行任务时的深层局限性。研究结果对于未来开发更可靠、更少幻觉的具身智能体具有指导价值，特别是对于需要与物理世界深度交互的应用。"}}
{"id": "2506.15231", "title": "Convolutional Feature Enhancement and Attention Fusion BiFPN for Ship Detection in SAR Images", "authors": ["Liangjie Meng", "Danxia Li", "Jinrong He", "Lili Ma", "Zhixin Li"], "summary": "Synthetic Aperture Radar (SAR) enables submeter-resolution imaging and\nall-weather monitoring via active microwave and advanced signal processing.\nCurrently, SAR has found extensive applications in critical maritime domains\nsuch as ship detection. However, SAR ship detection faces several challenges,\nincluding significant scale variations among ships, the presence of small\noffshore vessels mixed with noise, and complex backgrounds for large nearshore\nships. To address these issues, this paper proposes a novel feature enhancement\nand fusion framework named C-AFBiFPN. C-AFBiFPN constructs a Convolutional\nFeature Enhancement (CFE) module following the backbone network, aiming to\nenrich feature representation and enhance the ability to capture and represent\nlocal details and contextual information. Furthermore, C-AFBiFPN innovatively\nintegrates BiFormer attention within the fusion strategy of BiFPN, creating the\nAFBiFPN network. AFBiFPN improves the global modeling capability of cross-scale\nfeature fusion and can adaptively focus on critical feature regions. The\nexperimental results on SAR Ship Detection Dataset (SSDD) indicate that the\nproposed approach substantially enhances detection accuracy for small targets,\nrobustness against occlusions, and adaptability to multi-scale features.", "comment": "5 pages, 4 figures, 2 tables. Code available at\n  https://github.com/mlj666219/C-AFBiFPN/tree/master", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15231v1", "AI": {"title_translation": "用于SAR图像舰船检测的卷积特征增强与注意力融合BiFPN", "tldr": "本文提出C-AFBiFPN框架，通过卷积特征增强模块和结合BiFormer注意力的AFBiFPN网络，显著提升SAR图像中舰船检测的准确性，尤其针对小目标、遮挡和多尺度特征。", "motivation": "SAR图像舰船检测面临挑战，包括舰船尺度变化大、近海小目标与噪声混淆、近岸大目标背景复杂等问题。", "method": "本文提出C-AFBiFPN框架，该框架在骨干网络后构建卷积特征增强（CFE）模块，以丰富特征表示并增强捕获和表示局部细节与上下文信息的能力。此外，C-AFBiFPN创新性地将BiFormer注意力集成到BiFPN的融合策略中，创建了AFBiFPN网络，以改善跨尺度特征融合的全局建模能力，并自适应地关注关键特征区域。", "result": "在SAR舰船检测数据集（SSDD）上的实验结果表明，所提出的方法显著提高了小目标的检测精度、对遮挡的鲁棒性以及对多尺度特征的适应性。", "conclusion": "本文提出的C-AFBiFPN框架通过结合卷积特征增强和注意力融合BiFPN，有效解决了SAR图像舰船检测中的多尺度、小目标和复杂背景等挑战，显著提升了检测性能。", "translation": "合成孔径雷达（SAR）通过有源微波和先进信号处理实现亚米级分辨率成像和全天候监测。目前，SAR已在舰船检测等关键海洋领域获得广泛应用。然而，SAR舰船检测面临多项挑战，包括舰船间显著的尺度变化、混杂噪声的近海小型船只以及大型近岸舰船的复杂背景。为解决这些问题，本文提出了一种新颖的特征增强与融合框架，命名为C-AFBiFPN。C-AFBiFPN在骨干网络之后构建卷积特征增强（CFE）模块，旨在丰富特征表示并增强捕获和表示局部细节和上下文信息的能力。此外，C-AFBiFPN创新性地将BiFormer注意力集成到BiFPN的融合策略中，创建了AFBiFPN网络。AFBiFPN改善了跨尺度特征融合的全局建模能力，并能自适应地关注关键特征区域。在SAR舰船检测数据集（SSDD）上的实验结果表明，所提出的方法显著提高了小目标的检测精度、对遮挡的鲁棒性以及对多尺度特征的适应性。", "summary": "本文针对SAR图像舰船检测中的多尺度、小目标和复杂背景挑战，提出了一种名为C-AFBiFPN的新型特征增强与融合框架。该框架结合了卷积特征增强（CFE）模块以丰富特征表示，并创新性地将BiFormer注意力融入BiFPN中形成AFBiFPN网络，以提升跨尺度特征融合的全局建模能力和关键区域关注能力。实验证明，该方法显著提升了小目标检测精度、抗遮挡鲁棒性及多尺度适应性。", "keywords": "SAR图像, 舰船检测, 卷积特征增强, 注意力机制, BiFPN", "comments": "本文的创新点在于结合了卷积特征增强和BiFormer注意力机制与BiFPN的融合策略，形成了一个新颖的特征增强和融合框架C-AFBiFPN。这种结合有效地解决了SAR图像中舰船检测面临的尺度变化大、小目标检测困难和复杂背景等挑战，显示出较强的实用性和有效性，特别是在提升小目标检测性能方面。"}}
{"id": "2506.14986", "title": "Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks", "authors": ["Maxime Usdin", "Lito Kriara", "Licinio Craveiro"], "summary": "Early multiple sclerosis (MS) disability progression prediction is\nchallenging due to disease heterogeneity. This work predicts 48- and 72-week\ndisability using sparse baseline clinical data and 12 weeks of daily digital\nFloodlight data from the CONSONANCE clinical trial. We employed\nstate-of-the-art tabular and time-series foundation models (FMs), a custom\nmultimodal attention-based transformer, and machine learning methods. Despite\nthe difficulty of early prediction (AUROC 0.63), integrating digital data via\nadvanced models improved performance over clinical data alone. A transformer\nmodel using unimodal embeddings from the Moment FM yielded the best result, but\nour multimodal transformer consistently outperformed its unimodal counterpart,\nconfirming the advantages of combining clinical with digital data. Our findings\ndemonstrate the promise of FMs and multimodal approaches to extract predictive\nsignals from complex and diverse clinical and digital life sciences data (e.g.,\nimaging, omics), enabling more accurate prognostics for MS and potentially\nother complex diseases.", "comment": "Accepted to IJCAI 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14986v1", "AI": {"title_translation": "通过多模态基础模型基准早期预测多发性硬化症残疾进展", "tldr": "本研究利用稀疏基线临床数据和数字Floodlight数据，通过先进的表格和时间序列基础模型以及多模态Transformer模型，成功预测了多发性硬化症的早期残疾进展，并证明了结合多模态数据和基础模型的优势。", "motivation": "由于疾病异质性，早期预测多发性硬化症（MS）残疾进展具有挑战性。", "method": "研究利用CONSONANCE临床试验中的稀疏基线临床数据和12周的日常数字Floodlight数据，预测48周和72周的残疾。采用了最先进的表格和时间序列基础模型（FMs）、自定义的多模态注意力Transformer模型以及机器学习方法。", "result": "尽管早期预测很困难（AUROC 0.63），但通过先进模型整合数字数据比单独使用临床数据提高了性能。使用Moment FM单模态嵌入的Transformer模型取得了最佳结果，但多模态Transformer模型始终优于其单模态对应模型，证实了结合临床数据和数字数据的优势。", "conclusion": "研究结果表明，基础模型和多模态方法在从复杂多样的临床和数字生命科学数据中提取预测信号方面具有前景，可以为MS以及潜在的其他复杂疾病实现更准确的预后。", "translation": "多发性硬化症（MS）的早期残疾进展预测由于疾病异质性而具有挑战性。这项工作利用CONSONANCE临床试验中稀疏的基线临床数据和12周的每日数字Floodlight数据，预测了48周和72周的残疾情况。我们采用了最先进的表格和时间序列基础模型（FMs）、定制的多模态注意力Transformer模型和机器学习方法。尽管早期预测存在困难（AUROC 0.63），但通过高级模型整合数字数据比单独使用临床数据提高了性能。使用Moment FM的单模态嵌入的Transformer模型取得了最佳结果，但我们的多模态Transformer模型始终优于其单模态对应模型，证实了结合临床数据与数字数据的优势。我们的发现证明了基础模型和多模态方法在从复杂多样的临床和数字生命科学数据（例如，影像、组学）中提取预测信号的潜力，从而为MS以及潜在的其他复杂疾病实现更准确的预后。", "summary": "本研究旨在解决多发性硬化症（MS）早期残疾进展预测的挑战，利用CONSONANCE临床试验的临床和数字Floodlight数据，评估了多种先进模型。研究发现，尽管预测难度大，但结合数字数据和多模态方法，特别是使用Transformer模型，能够显著提升预测性能，并强调了基础模型在处理复杂生命科学数据中的巨大潜力。", "keywords": "多发性硬化症, 残疾预测, 多模态, 基础模型, Transformer", "comments": "该论文的创新点在于将多模态基础模型应用于多发性硬化症的早期残疾预测，尤其强调了结合临床和数字数据的优势。它为未来在复杂疾病预后中利用先进AI模型和多源数据融合提供了有益的探索方向，具有重要的临床应用前景。"}}
{"id": "2506.15498", "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling", "authors": ["Md Imbesat Hassan Rizvi", "Xiaodan Zhu", "Iryna Gurevych"], "summary": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.", "comment": "8 pages main content, 4 figures, 4 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15498v1", "AI": {"title_translation": "SPARE：基于参考引导评估的单次标注，用于自动过程监督和奖励建模", "tldr": "SPARE是一种新的单次标注框架，通过将解决方案步骤与参考解决方案对齐来提高LLM的自动过程监督效率和质量，并在推理和奖励模型训练中显示出性能提升和显著效率优势。", "motivation": "现有的大语言模型（LLM）复杂多步推理能力提升依赖于过程或分步监督，但高效、高质量的自动化过程标注仍然是一个重大挑战。", "method": "本文提出了一种名为SPARE（Single-Pass Annotation with Reference-Guided Evaluation）的新型结构化框架。SPARE通过将每个解决方案步骤与参考解决方案中的一个或多个步骤对齐，并提供明确的评估理由，实现单次、每步标注。", "result": "参考引导的分步评估有效地促进了在数学推理、多跳组合问答和空间推理三个领域四个数据集上的过程监督。与基线相比，SPARE在离线RL设置中用于推理时贪婪解码的模型微调以及训练奖励模型以对多个LLM生成输出进行排名/聚合时，提高了推理性能。此外，SPARE在具有挑战性的数学数据集上实现了有竞争力的性能，同时效率提高了2.6倍，运行时仅为基于树搜索的自动标注的38%。", "conclusion": "SPARE通过其新颖的单次标注和参考引导评估方法，有效解决了LLM过程监督中高效高质量自动标注的挑战，显著提升了推理性能和标注效率。", "translation": "过程或分步监督在提升大型语言模型（LLM）的复杂多步推理能力方面发挥了关键作用。然而，高效、高质量的自动化过程标注仍然是一个重大挑战。为了解决这个问题，我们引入了单次标注与参考引导评估（SPARE），这是一个新颖的结构化框架，通过将每个解决方案步骤与参考解决方案中的一个或多个步骤对齐，并附带明确的评估理由，实现单次、每步标注。我们表明，参考引导的分步评估有效地促进了在数学推理、多跳组合问答和空间推理三个领域的四个数据集上的过程监督。我们证明，与基线相比，SPARE在以下情况下提高了推理性能：（1）在离线强化学习设置中用于推理时贪婪解码的模型微调，以及（2）训练奖励模型以对多个LLM生成输出进行排名/聚合。此外，SPARE在具有挑战性的数学数据集上实现了有竞争力的性能，同时效率提高了2.6倍，运行时仅为基于树搜索的自动标注的38%。代码库以及训练好的SPARE-PRM模型已公开发布，以促进进一步的研究和可重复性。", "summary": "本文提出了SPARE（Single-Pass Annotation with Reference-Guided Evaluation），一个用于解决LLM过程监督中高效高质量自动标注挑战的新型框架。SPARE通过单次、每步标注并将解决方案步骤与参考对齐，显著提升了在数学、问答和空间推理任务中的LLM推理性能，并提高了2.6倍的标注效率。它适用于模型微调和奖励模型训练。", "keywords": "过程监督, 奖励建模, 大型语言模型, 自动标注, 效率", "comments": "SPARE的创新之处在于其“单次标注与参考引导评估”机制，这显著提高了自动化过程监督的效率和质量。该方法在处理LLM复杂多步推理任务时具有重要意义，尤其是在需要大量高质量标注数据的情况下。其在效率上的显著提升（2.6倍）使其在实际应用中更具可行性，并且公开代码库有助于促进该领域的研究和复现。"}}
{"id": "2506.15242", "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories", "authors": ["Qingsong Yan", "Qiang Wang", "Kaiyong Zhao", "Jie Chen", "Bo Li", "Xiaowen Chu", "Fei Deng"], "summary": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged\nas powerful tools for 3D reconstruction and SLAM tasks. However, their\nperformance depends heavily on accurate camera pose priors. Existing approaches\nattempt to address this issue by introducing external constraints but fall\nshort of achieving satisfactory accuracy, particularly when camera trajectories\nare complex. In this paper, we propose a novel method, RA-NeRF, capable of\npredicting highly accurate camera poses even with complex camera trajectories.\nFollowing the incremental pipeline, RA-NeRF reconstructs the scene using NeRF\nwith photometric consistency and incorporates flow-driven pose regulation to\nenhance robustness during initialization and localization. Additionally,\nRA-NeRF employs an implicit pose filter to capture the camera movement pattern\nand eliminate the noise for pose estimation. To validate our method, we conduct\nextensive experiments on the Tanks\\&Temple dataset for standard evaluation, as\nwell as the NeRFBuster dataset, which presents challenging camera pose\ntrajectories. On both datasets, RA-NeRF achieves state-of-the-art results in\nboth camera pose estimation and visual quality, demonstrating its effectiveness\nand robustness in scene reconstruction under complex pose trajectories.", "comment": "IROS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15242v1", "AI": {"title_translation": "RA-NeRF：复杂轨迹下高精度相机姿态估计的鲁棒神经辐射场重建", "tldr": "RA-NeRF在复杂相机轨迹下，通过结合光度一致性、流驱动姿态调节和隐式姿态滤波器，实现了鲁棒的NeRF场景重建和高精度相机姿态估计，达到了最先进的性能。", "motivation": "神经辐射场（NeRF）和3D高斯泼溅（3DGS）在3D重建和SLAM任务中严重依赖准确的相机姿态先验，但现有方法在复杂相机轨迹下，通过外部约束提升姿态准确性仍不足。", "method": "RA-NeRF采用增量式流程，使用NeRF结合光度一致性重建场景，并引入流驱动姿态调节以增强初始化和定位的鲁棒性。此外，它还使用一个隐式姿态滤波器来捕捉相机运动模式并消除姿态估计中的噪声。", "result": "RA-NeRF在Tanks&Temple和NeRFBuster数据集上，在相机姿态估计和视觉质量方面均达到了最先进的结果。", "conclusion": "RA-NeRF在复杂姿态轨迹下进行场景重建时，展现出其有效性和鲁棒性。", "translation": "神经辐射场（NeRF）和3D高斯泼溅（3DGS）已成为3D重建和SLAM任务的强大工具。然而，它们的性能严重依赖于准确的相机姿态先验。现有方法试图通过引入外部约束来解决这个问题，但在相机轨迹复杂时，仍未能达到令人满意的精度。本文提出了一种新颖的方法RA-NeRF，即使在复杂的相机轨迹下也能预测高精度的相机姿态。RA-NeRF遵循增量式流程，利用NeRF结合光度一致性重建场景，并结合流驱动的姿态调节以增强初始化和定位过程中的鲁棒性。此外，RA-NeRF采用隐式姿态滤波器来捕捉相机运动模式并消除姿态估计中的噪声。为了验证我们的方法，我们在Tanks&Temple数据集上进行了标准评估的广泛实验，并在NeRFBuster数据集上进行了具有挑战性相机姿态轨迹的评估。在这两个数据集上，RA-NeRF在相机姿态估计和视觉质量方面均取得了最先进的结果，证明了其在复杂姿态轨迹下场景重建的有效性和鲁棒性。", "summary": "本文提出RA-NeRF，一种针对复杂相机轨迹下鲁棒神经辐射场重建和高精度相机姿态估计的新方法。该方法结合NeRF的光度一致性、流驱动姿态调节以增强鲁棒性，并利用隐式姿态滤波器去除噪声。实验证明，RA-NeRF在相机姿态估计和视觉质量上均达到最先进水平，尤其适用于复杂轨迹场景。", "keywords": "神经辐射场, 相机姿态估计, 3D重建, 鲁棒性, 复杂轨迹", "comments": "RA-NeRF的创新之处在于其结合了光度一致性、流驱动姿态调节以及隐式姿态滤波器，以解决NeRF和3DGS在复杂相机轨迹下对精确相机姿态高度依赖的问题。这种多策略结合显著提升了姿态估计的准确性和场景重建的鲁棒性，使其在具有挑战性的真实世界场景中更具实用价值。"}}
{"id": "2506.14988", "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits", "authors": ["Tianyi Xu", "Jiaxin Liu", "Zizhan Zheng"], "summary": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14988v1", "AI": {"title_translation": "多智能体多臂老虎机中的公平探测算法", "tldr": "该论文提出了一种新的探测框架，用于多智能体多臂老虎机中的公平算法，旨在平衡代理间的公平性与系统整体性能。", "motivation": "在多智能体多臂老虎机（MA-MAB）框架中，为了在最大化整体系统性能的同时，确保代理间的公平结果，并且解决在有限臂奖励信息下的决策挑战。", "method": "引入了一种新颖的探测框架，在分配前策略性地收集选定臂的信息。在奖励分布已知的离线设置中，利用次模性质设计了一种具有可证明性能界限的贪婪探测算法。对于更复杂的在线设置，开发了一种在保持公平性的同时实现次线性遗憾的算法。", "result": "在合成数据集和真实世界数据集上进行的广泛实验表明，所提出的方法优于基线方法，实现了更好的公平性和效率。", "conclusion": "所提出的带有探测机制的公平算法，在多智能体多臂老虎机设置中，能够有效平衡公平性和效率，并优于现有基线方法。", "translation": "我们提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在确保代理间的公平结果，同时最大化整体系统性能。在这种设置中，一个关键挑战是在臂奖励信息有限的情况下进行决策。为了解决这个问题，我们引入了一个新颖的探测框架，在分配前策略性地收集选定臂的信息。在奖励分布已知的离线设置中，我们利用次模性质设计了一个具有可证明性能界限的贪婪探测算法。对于更复杂的在线设置，我们开发了一个在保持公平性的同时实现次线性遗憾的算法。在合成数据集和真实世界数据集上进行的广泛实验表明，我们的方法优于基线方法，实现了更好的公平性和效率。", "summary": "该论文提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在解决代理间公平性与系统性能最大化之间的平衡问题，尤其是在信息有限的情况下。为克服这一挑战，作者引入了一种新颖的探测框架，用于在决策前收集臂信息。对于离线场景，设计了一个基于次模性质的贪婪探测算法，并提供了性能保证。对于在线场景，开发了一个能实现次线性遗憾并保持公平性的算法。实验结果表明，该方法在公平性和效率方面均优于现有基线方法。", "keywords": "多智能体多臂老虎机, 公平性, 探测, 次模, 在线学习", "comments": "该论文的创新点在于引入了一个新颖的探测框架，用于在多智能体多臂老虎机中策略性地收集信息，以解决公平性和性能之间的权衡问题。它同时考虑了离线和在线两种设置，并为离线设置提供了理论上的性能界限，为在线设置实现了次线性遗憾，这增加了其方法的严谨性和实用性。对公平性的关注是其重要贡献。"}}
{"id": "2506.15504", "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge", "authors": ["Li Zheng", "Sihang Wang", "Hao Fei", "Zuquan Peng", "Fei Li", "Jianming Fu", "Chong Teng", "Donghong Ji"], "summary": "Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.", "comment": "Accepted by ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15504v1", "AI": {"title_translation": "利用双向动态交互和情感知识增强夸张与隐喻检测", "tldr": "提出了一个名为EmoBi的情感引导框架，通过情感分析、领域映射和双向动态交互，显著提升了夸张和隐喻的检测性能，超越了现有方法。", "motivation": "由于语义模糊性和表达多样性，夸张和隐喻的文本检测具有挑战性。现有方法多关注表层文本特征，忽略了夸张与隐喻的关联以及隐性情感对其感知的 S影响。", "method": "提出了一个情感引导的夸张和隐喻检测框架（EmoBi）。该框架包含三个模块：情感分析模块（挖掘情感内涵）、情感域映射模块（识别目标和源域以理解隐性含义）和双向动态交互模块（促进夸张和隐喻的相互提升）。同时，设计了验证机制以确保检测的准确性和可靠性。", "result": "EmoBi在四个数据集上均优于所有基线方法。与当前最先进的方法相比，在TroFi数据集上的夸张检测F1分数提高了28.1%，在HYPO-L数据集上的隐喻检测F1分数提高了23.1%。", "conclusion": "本研究的结果及深入分析强调了EmoBi方法在推进夸张和隐喻检测方面的有效性和潜力。", "translation": "基于文本的夸张和隐喻检测对自然语言处理（NLP）任务具有重要意义。然而，由于其语义模糊性和表达多样性，识别它们相当具有挑战性。现有方法大多关注表层文本特征，忽略了夸张和隐喻的关联以及隐性情感对感知这些修辞手法的影响。为了实现这些假设，我们提出了一个基于双向动态交互的情感引导夸张和隐喻检测框架（EmoBi）。首先，情感分析模块深入挖掘夸张和隐喻背后的情感内涵。其次，基于情感的领域映射模块识别目标和源域，以更深入地理解夸张和隐喻的隐含意义。最后，双向动态交互模块实现了夸张和隐喻之间的相互促进。同时，设计了验证机制以确保检测的准确性和可靠性。实验表明，EmoBi在四个数据集上均优于所有基线方法。具体而言，与当前的SoTA相比，在TroFi数据集上的夸张检测F1分数提高了28.1%，在HYPO-L数据集上的隐喻检测F1分数提高了23.1%。这些结果，在深入分析的支持下，强调了我们方法在推进夸张和隐喻检测方面的有效性和潜力。", "summary": "该研究提出了一个名为EmoBi的情感引导框架，用于增强文本中的夸张和隐喻检测。针对现有方法忽视修辞手法间关联和情感影响的问题，EmoBi通过情感分析、情感域映射和双向动态交互模块，深入理解其隐含意义并实现相互促进。实验证明，EmoBi在多个数据集上显著优于现有基线方法，F1分数在夸张和隐喻检测上分别有大幅提升，验证了其有效性和潜力。", "keywords": "夸张检测, 隐喻检测, 情感分析, 双向动态交互, 自然语言处理", "comments": "该论文的创新点在于将情感知识和夸张与隐喻之间的双向动态交互引入到检测框架中，这与以往侧重表层特征的方法形成对比。通过深入挖掘情感内涵和域映射，有效解决了语义模糊性问题。实验结果的显著提升表明了其在NLP领域，特别是在理解复杂修辞手法方面的巨大潜力。"}}
{"id": "2506.15244", "title": "Retrospective Memory for Camouflaged Object Detection", "authors": ["Chenxi Zhang", "Jiayun Wu", "Qing Zhang", "Yazhe Zhai", "Youwei Pang"], "summary": "Camouflaged object detection (COD) primarily focuses on learning subtle yet\ndiscriminative representations from complex scenes. Existing methods\npredominantly follow the parametric feedforward architecture based on static\nvisual representation modeling. However, they lack explicit mechanisms for\nacquiring historical context, limiting their adaptation and effectiveness in\nhandling challenging camouflage scenes. In this paper, we propose a\nrecall-augmented COD architecture, namely RetroMem, which dynamically modulates\ncamouflage pattern perception and inference by integrating relevant historical\nknowledge into the process. Specifically, RetroMem employs a two-stage training\nparadigm consisting of a learning stage and a recall stage to construct,\nupdate, and utilize memory representations effectively. During the learning\nstage, we design a dense multi-scale adapter (DMA) to improve the pretrained\nencoder's capability to capture rich multi-scale visual information with very\nfew trainable parameters, thereby providing foundational inferences. In the\nrecall stage, we propose a dynamic memory mechanism (DMM) and an inference\npattern reconstruction (IPR). These components fully leverage the latent\nrelationships between learned knowledge and current sample context to\nreconstruct the inference of camouflage patterns, thereby significantly\nimproving the model's understanding of camouflage scenes. Extensive experiments\non several widely used datasets demonstrate that our RetroMem significantly\noutperforms existing state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15244v1", "AI": {"title_translation": "用于伪装目标检测的回溯记忆", "tldr": "RetroMem是一种新的伪装目标检测模型，通过引入历史记忆来动态调整感知和推理，显著优于现有方法。", "motivation": "现有伪装目标检测（COD）方法主要基于静态视觉表示建模的参数前馈架构，但缺乏获取历史上下文的显式机制，限制了它们在处理复杂伪装场景时的适应性和有效性。", "method": "本文提出了一种召回增强的COD架构，名为RetroMem。它通过整合相关历史知识来动态调制伪装模式感知和推理。RetroMem采用两阶段训练范式：学习阶段和召回阶段。在学习阶段，设计了密集多尺度适配器（DMA）以少量可训练参数提高预训练编码器捕获丰富多尺度视觉信息的能力。在召回阶段，提出了动态记忆机制（DMM）和推理模式重建（IPR），以利用学习知识和当前样本上下文之间的潜在关系来重建伪装模式的推理。", "result": "在多个广泛使用的数据集上进行的广泛实验表明，RetroMem显著优于现有的最先进方法。", "conclusion": "RetroMem通过引入回溯记忆和动态调制机制，有效解决了现有伪装目标检测方法缺乏历史上下文的问题，显著提升了模型在复杂伪装场景下的检测性能。", "translation": "伪装目标检测（COD）主要侧重于从复杂场景中学习微妙但具有区分性的表示。现有方法主要遵循基于静态视觉表示建模的参数前馈架构。然而，它们缺乏获取历史上下文的显式机制，限制了它们在处理具有挑战性的伪装场景时的适应性和有效性。在本文中，我们提出了一种召回增强的COD架构，名为RetroMem，它通过将相关的历史知识整合到过程中，动态地调制伪装模式感知和推理。具体而言，RetroMem采用两阶段训练范式，包括学习阶段和召回阶段，以有效地构建、更新和利用记忆表示。在学习阶段，我们设计了一个密集多尺度适配器（DMA），以用极少的可训练参数提高预训练编码器捕获丰富多尺度视觉信息的能力，从而提供基础推理。在召回阶段，我们提出了动态记忆机制（DMM）和推理模式重建（IPR）。这些组件充分利用学习知识和当前样本上下文之间的潜在关系来重建伪装模式的推理，从而显著提高模型对伪装场景的理解。在多个广泛使用的数据集上进行的广泛实验表明，我们的RetroMem显著优于现有的最先进方法。", "summary": "本文提出了一种名为RetroMem的召回增强型伪装目标检测（COD）架构，旨在解决现有方法缺乏历史上下文的问题。RetroMem通过两阶段训练范式（学习阶段和召回阶段）整合历史知识。学习阶段引入密集多尺度适配器（DMA）以增强多尺度特征捕获能力；召回阶段则利用动态记忆机制（DMM）和推理模式重建（IPR）来利用历史记忆重建伪装模式推理。实验证明，RetroMem在多个数据集上显著超越了现有最先进的COD方法。", "keywords": "伪装目标检测, 回溯记忆, 动态记忆机制, 多尺度适配器, 模式重建", "comments": "RetroMem的创新之处在于引入了“回溯记忆”的概念，将历史上下文动态地融入伪装目标检测过程，突破了传统静态前馈模型的局限性。其两阶段训练范式和具体的记忆机制（DMA、DMM、IPR）设计精巧，有效提升了模型对复杂伪装场景的理解和适应性，是该领域的重要进展。"}}
{"id": "2506.15019", "title": "Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment", "authors": ["Yue Gao"], "summary": "Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.", "comment": "Accepted to IJCAI2025 AI4TS", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15019v1", "AI": {"title_translation": "脓毒症治疗中具有敏锐度正则化的稳定CDE自编码器用于离线强化学习", "tldr": "针对脓毒症治疗中的离线强化学习，本研究提出通过确保训练稳定性和敏锐度正则化来学习稳定的CDE状态表示，从而显著提高策略性能。", "motivation": "现有的强化学习方法在脓毒症治疗中，学习稳定且有临床意义的状态表示面临挑战，特别是序列表示中训练不稳定性的关键挑战及其对策略性能的负面影响被忽视。", "method": "本文提出使用受控微分方程（CDE）状态表示，并通过两个关键因素实现：1) 通过早期停止或稳定方法确保训练稳定性；2) 通过与临床评分（SOFA、SAPS-II、OASIS）进行相关性正则化来强制执行敏锐度感知的表示。", "result": "稳定的CDE自编码器产生的表示与敏锐度评分强相关，并使强化学习策略具有卓越的性能（WIS回报 > 0.9）。相反，不稳定的CDE表示导致表示质量下降和策略失败（WIS回报约0）。潜在空间的可视化显示，稳定的CDE不仅能区分幸存者和非幸存者轨迹，还能揭示清晰的敏锐度评分梯度。", "conclusion": "这些发现为在临床强化学习中使用CDE编码不规则医疗时间序列提供了实用指导，强调了序列表示学习中训练稳定性的必要性。", "translation": "脓毒症治疗中有效的强化学习（RL）取决于从不规则ICU时间序列中学习稳定、具有临床意义的状态表示。虽然以前的工作已经探索了针对这项任务的表示学习，但序列表示中训练不稳定性的关键挑战及其对策略性能的有害影响却被忽视了。这项工作表明，当满足两个关键因素时，受控微分方程（CDE）状态表示可以实现强大的RL策略：(1) 通过早期停止或稳定方法确保训练稳定性，以及 (2) 通过与临床评分（SOFA、SAPS-II、OASIS）进行相关性正则化来强制执行敏锐度感知的表示。在MIMIC-III脓毒症队列上的实验表明，稳定的CDE自编码器产生的表示与敏锐度评分强相关，并使RL策略具有卓越的性能（WIS回报 > 0.9）。相比之下，不稳定的CDE表示导致表示质量下降和策略失败（WIS回报约0）。潜在空间的可视化显示，稳定的CDE不仅能区分幸存者和非幸存者轨迹，还能揭示清晰的敏锐度评分梯度，而不稳定的训练则未能捕捉到这两种模式。这些发现为在临床RL中使用CDE编码不规则医疗时间序列提供了实用指导，强调了序列表示学习中训练稳定性的必要性。", "summary": "本文提出一种用于脓毒症离线强化学习的稳定CDE自编码器，以解决不规则ICU时间序列中状态表示的训练不稳定性问题。通过确保训练稳定性（如早期停止）和引入敏锐度正则化（与临床评分关联），该方法能够学习到与患者敏锐度强相关的稳定表示。实验证明，这种稳定的CDE表示显著提升了强化学习策略的性能，并能有效区分患者轨迹及揭示敏锐度梯度，为临床强化学习提供了实用指导。", "keywords": "CDE自编码器, 强化学习, 脓毒症治疗, 状态表示, 训练稳定性", "comments": "本文的创新点在于明确指出了在序列表示学习中训练稳定性的重要性，并提出了结合CDE自编码器、稳定性措施和敏锐度正则化的解决方案，以应对临床不规则时间序列的挑战。其贡献在于提供了一种有效且具有临床意义的状态表示学习方法，对提高医疗领域强化学习的可靠性和性能具有重要意义。"}}
{"id": "2506.15522", "title": "Lessons from Training Grounded LLMs with Verifiable Rewards", "authors": ["Shang Hong Sim", "Tej Deep Pala", "Vernon Toh", "Hai Leong Chieu", "Amir Zadeh", "Chuan Li", "Navonil Majumder", "Soujanya Poria"], "summary": "Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15522v1", "AI": {"title_translation": "从使用可验证奖励训练接地LLM中吸取的教训", "tldr": "本文通过强化学习和内部推理，并使用GRPO方法及可验证的基于结果的奖励来提高LLM的接地能力，实验表明其显著优于仅指令调优模型，尤其在处理无法回答的查询和生成良好引用的响应方面。", "motivation": "大型语言模型（LLMs）在生成有根据且可信的响应方面面临关键挑战，尽管检索增强生成（RAG）有潜力，但指令调优模型常在简单场景中失败，如遗漏答案、错误引用或在有证据时拒绝回答。", "method": "本文探索了强化学习（RL）和内部推理如何增强LLM的接地能力。研究人员使用GRPO（Group Relative Policy Optimization）方法，通过可验证的基于结果的奖励来训练模型，奖励目标包括答案正确性、引用充分性和拒绝质量，无需黄金推理轨迹或昂贵的标注。此外，还采用了两阶段训练设置（先优化答案和引用行为，再优化拒绝），并结合了GPT-4蒸馏的指令调优。", "result": "实验表明，推理增强模型显著优于仅指令调优的变体，尤其在处理无法回答的查询和生成良好引用的响应方面。两阶段训练设置通过稳定学习信号进一步提高了接地能力。将GPT-4蒸馏的指令调优与GRPO结合，能增强长篇生成性问答任务的性能。", "conclusion": "本文的研究结果强调了推理、阶段性优化和结果驱动的强化学习对于构建更可验证和可靠的LLM的价值。", "translation": "生成有根据且可信的响应仍然是大型语言模型（LLMs）面临的关键挑战。尽管基于引用的检索增强生成（RAG）前景广阔，但指令调优模型即使在简单场景中也经常失败：遗漏明确陈述的答案、错误引用或在有证据时拒绝回答。在这项工作中，我们探索了强化学习（RL）和内部推理如何增强LLM的接地能力。我们使用GRPO（Group Relative Policy Optimization）方法，通过可验证的基于结果的奖励来训练模型，奖励目标包括答案正确性、引用充分性和拒绝质量，而无需黄金推理轨迹或昂贵标注。通过在ASQA、QAMPARI、ELI5和ExpertQA上的全面实验，我们表明推理增强模型显著优于仅指令调优的变体，尤其在处理无法回答的查询和生成良好引用的响应方面。两阶段训练设置，首先优化答案和引用行为，然后优化拒绝，通过稳定学习信号进一步提高了接地能力。此外，我们重新审视了通过GPT-4蒸馏进行的指令调优，发现将其与GRPO结合可以提高长篇生成性问答任务的性能。总的来说，我们的发现强调了推理、阶段性优化和结果驱动的强化学习对于构建更可验证和可靠的LLM的价值。", "summary": "本文旨在解决大型语言模型在生成有根据且可信响应方面的挑战，特别是指令调优模型在引用和拒绝行为上的不足。研究人员提出了一种结合强化学习（RL）和内部推理的方法，使用GRPO算法和基于可验证结果的奖励（针对答案正确性、引用充分性和拒绝质量）来训练模型。实验证明，这种推理增强模型在处理无法回答的查询和生成良好引用的响应方面显著优于仅指令调优的模型。此外，两阶段训练和结合GPT-4蒸馏的指令调优进一步提升了性能。研究强调了推理、阶段性优化和结果驱动的RL对于构建更可靠LLM的重要性。", "keywords": "大型语言模型, 接地能力, 强化学习, GRPO, 可验证奖励", "comments": "这篇论文通过引入可验证的基于结果的强化学习，为提高LLM的接地能力提供了一条有前景的路径。其创新点在于无需昂贵的黄金推理轨迹或标注，而是直接优化最终输出的正确性和可信度。两阶段训练和结合指令调优的策略也展示了实际应用中的鲁棒性。这对于提升LLM在事实性问答和信息检索领域的可靠性具有重要意义。"}}
{"id": "2506.15260", "title": "Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing", "authors": ["Adrian Poniatowski", "Natalie Gentner", "Manuel Barusco", "Davide Dalle Pezze", "Samuele Salti", "Gian Antonio Susto"], "summary": "In the semiconductor sector, due to high demand but also strong and\nincreasing competition, time to market and quality are key factors in securing\nsignificant market share in various application areas. Thanks to the success of\ndeep learning methods in recent years in the computer vision domain, Industry\n4.0 and 5.0 applications, such as defect classification, have achieved\nremarkable success. In particular, Domain Adaptation (DA) has proven highly\neffective since it focuses on using the knowledge learned on a (source) domain\nto adapt and perform effectively on a different but related (target) domain. By\nimproving robustness and scalability, DA minimizes the need for extensive\nmanual re-labeling or re-training of models. This not only reduces\ncomputational and resource costs but also allows human experts to focus on\nhigh-value tasks. Therefore, we tested the efficacy of DA techniques in\nsemi-supervised and unsupervised settings within the context of the\nsemiconductor field. Moreover, we propose the DBACS approach, a\nCycleGAN-inspired model enhanced with additional loss terms to improve\nperformance. All the approaches are studied and validated on real-world\nElectron Microscope images considering the unsupervised and semi-supervised\nsettings, proving the usefulness of our method in advancing DA techniques for\nthe semiconductor field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15260v1", "AI": {"title_translation": "半导体制造中缺陷图像分类的域适应", "tldr": "本文研究了域适应（DA）技术在半导体缺陷图像分类中的应用，并提出了一种名为DBACS的新方法，该方法在真实电子显微镜图像上验证了其在无监督和半监督设置下的有效性。", "motivation": "在半导体领域，由于高需求和日益激烈的竞争，上市时间和质量是获取市场份额的关键因素。深度学习在计算机视觉领域的成功，使得工业4.0和5.0应用（如缺陷分类）取得了显著进展。域适应（DA）能够将在一个领域（源）学习到的知识应用于另一个不同但相关的领域（目标），从而提高鲁棒性和可扩展性，减少手动重新标记或模型重新训练的需求，降低成本并使专家专注于高价值任务。因此，本文旨在测试DA技术在半导体领域半监督和无监督设置下的有效性。", "method": "本文在半导体领域测试了域适应（DA）技术在半监督和无监督设置下的有效性。此外，提出了一种名为DBACS的方法，该方法是一个受CycleGAN启发并增强了额外损失项的模型，以提高性能。所有方法都在真实世界的电子显微镜图像上，在无监督和半监督设置下进行了研究和验证。", "result": "研究结果表明，所提出的方法在推动半导体领域的域适应技术方面非常有用。所有方法都在真实世界的电子显微镜图像上进行了研究和验证，证明了其在无监督和半监督设置下的有效性。", "conclusion": "域适应技术在半导体制造的缺陷图像分类中非常有效，特别是本文提出的DBACS方法，能够提升性能并减少对大量手动标记数据的需求，从而在真实世界的应用中显示出其价值。", "translation": "在半导体行业，由于高需求以及激烈且日益增长的竞争，上市时间和质量是确保在各个应用领域获得重要市场份额的关键因素。近年来，得益于深度学习方法在计算机视觉领域的成功，工业4.0和5.0应用，例如缺陷分类，取得了显著成就。特别是，域适应（DA）已被证明非常有效，因为它专注于利用在一个（源）领域学习到的知识，以适应并在不同但相关的（目标）领域中有效执行。通过提高鲁棒性和可扩展性，DA最大限度地减少了对大量手动重新标记或模型重新训练的需求。这不仅降低了计算和资源成本，还使人类专家能够专注于高价值任务。因此，我们在半导体领域的背景下，测试了DA技术在半监督和无监督设置中的功效。此外，我们提出了一种名为DBACS的方法，这是一个受CycleGAN启发并增强了额外损失项以提高性能的模型。所有方法都在考虑无监督和半监督设置的真实世界电子显微镜图像上进行了研究和验证，证明了我们方法在推动半导体领域DA技术方面的实用性。", "summary": "本文研究了域适应（DA）技术在半导体制造缺陷图像分类中的应用，以应对行业对上市时间和质量的关键需求。作者测试了DA在半监督和无监督环境下的有效性，并提出了一种名为DBACS的新方法，该方法借鉴了CycleGAN并加入了额外的损失项。所有方法均在真实的电子显微镜图像上进行了验证，结果表明所提出的方法对于推进半导体领域的DA技术具有实用价值，能够提高鲁棒性和可扩展性，同时减少手动标记和训练成本。", "keywords": "域适应, 缺陷分类, 半导体制造, 图像分类, 深度学习", "comments": "本文的创新点在于将域适应技术应用于半导体制造中的缺陷图像分类这一特定且关键的工业场景。尤其值得关注的是，作者不仅验证了现有DA方法的有效性，还提出了一种受CycleGAN启发并增强了额外损失项的DBACS新方法。在真实世界的电子显微镜图像上进行验证，进一步增强了研究结果的实际应用价值和可信度。该研究对于提高半导体生产的自动化水平和效率具有重要意义。"}}
{"id": "2506.15021", "title": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models", "authors": ["Gyuhak Kim", "Sumiran Singh Thakur", "Su Min Park", "Wei Wei", "Yujia Bao"], "summary": "Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15021v1", "AI": {"title_translation": "SFT-GO：基于组优化的监督微调大型语言模型", "tldr": "SFT-GO通过根据重要性对令牌进行分组并结合最差组损失和标准交叉熵损失进行优化，改进了大型语言模型的监督微调，从而在各种LLM基准测试中持续优于基线方法。", "motivation": "现有的监督微调（SFT）方法将每个训练实例视为统一序列，对所有令牌一视同仁，忽略了只有部分令牌包含关键任务特定信息的事实。为了解决这一局限性，作者提出了SFT-GO。", "method": "SFT-GO是一种新颖的方法，它根据令牌的重要性对令牌进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化大型语言模型。该方法自适应地强调最具挑战性的令牌组，并引导模型更好地处理不同的组分布。论文还提供了SFT-GO收敛速度的理论分析。", "result": "通过三种不同的令牌分组策略应用SFT-GO后，与SFT-GO训练的模型在流行的LLM基准测试中持续优于基线方法。这些改进在各种数据集和基础模型上都成立，证明了该方法的鲁棒性和有效性。", "conclusion": "SFT-GO通过引入基于令牌重要性的组优化机制，显著提高了大型语言模型监督微调的性能、鲁棒性和学习动态。", "translation": "监督微调（SFT）已成为调整大型语言模型（LLM）以符合人类期望和特定下游任务的重要步骤。然而，现有的SFT方法通常将每个训练实例视为统一序列，对所有令牌一视同仁，无论其相关性如何。这忽略了通常只有部分令牌包含关键的、任务特定信息的事实。为了解决这一局限性，我们引入了基于组优化的监督微调（SFT-GO），这是一种根据重要性不同对待令牌组的新方法。SFT-GO根据其重要性值对每个样本中的令牌进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化LLM。这种机制自适应地强调最具挑战性的令牌组，并引导模型更好地处理不同的组分布，从而改善整体学习动态。我们提供了SFT-GO收敛速度的理论分析，证明了其效率。经验上，我们应用SFT-GO与三种不同的令牌分组策略，结果表明使用SFT-GO训练的模型在流行的LLM基准测试中持续优于基线方法。这些改进在各种数据集和基础模型上都成立，证明了我们方法的鲁棒性和有效性。", "summary": "SFT-GO提出了一种新颖的监督微调（SFT）方法，通过根据令牌的重要性进行分组优化，解决了现有SFT方法对所有令牌一视同仁的局限性。该方法通过结合最差组损失和标准交叉熵损失来优化大型语言模型（LLM），以强调最具挑战性的令牌组。SFT-GO经过理论分析并在一系列LLM基准测试中表现出色，其性能优于基线方法，并展现出在不同数据集和基础模型上的鲁棒性和有效性。", "keywords": "监督微调, 大型语言模型, 组优化, 令牌重要性, SFT-GO", "comments": "SFT-GO的创新点在于其引入了“组优化”的概念，通过区分不同重要性的令牌组来改进监督微调过程。这种方法能够更精细地处理训练数据，聚焦于关键信息，从而提升LLM的性能和泛化能力。其理论分析和在多个基准测试上的持续优异表现，证明了该方法的有效性和实用价值。"}}
{"id": "2506.15545", "title": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models", "authors": ["Bailin Wang", "Chang Lan", "Chong Wang", "Ruoming Pang"], "summary": "Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.", "comment": "9 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15545v1", "AI": {"title_translation": "RATTENTION：迈向局部-全局注意力模型中的最小滑动窗口大小", "tldr": "RATTENTION通过结合线性注意力机制，在局部-全局注意力模型中实现了更小的窗口大小（如512），同时保持甚至提升了性能和效率，解决了局部注意力的窗口外信息丢失问题。", "motivation": "局部-全局注意力模型在窗口大小选择上存在帕累托权衡：大窗口效率提升小，小窗口可能导致性能下降。现有模型为保性能采用保守的窗口大小。本研究的核心动机是解决局部注意力对定义窗口外令牌完全忽略的内在局限性，旨在使局部-全局模型即使在短上下文场景下也能实现效率提升。", "method": "本文探索了RATTENTION，这是一种局部注意力的变体，它集成了一种专门的线性注意力机制，旨在捕获来自这些窗口外令牌的信息。通过专门的核实现和减小的窗口大小，RATTENTION能够保持与现有最先进方法相当的训练速度。", "result": "在3B和12B规模的预训练实验中，RATTENTION在性能和效率之间实现了更优的帕累托权衡。作为最佳选择，窗口大小仅为512的RATTENTION在不同设置下能持续匹配全注意力模型的性能。此外，RATTENTION中线性注意力组件固有的循环特性有助于增强长上下文性能，这已在RULER基准测试中得到验证。这些改进并未牺牲训练效率。", "conclusion": "RATTENTION通过创新的线性注意力集成，成功克服了局部注意力在小窗口下的性能限制，实现了在保持或提升性能的同时显著减小窗口大小，从而在短上下文和长上下文场景下都提供了效率和性能优势，且不牺牲训练效率。", "translation": "局部-全局注意力模型最近已成为标准Transformer模型的引人注目的替代方案，有望在训练和推理效率方面实现改进。然而，窗口大小的关键选择呈现出帕累托权衡：较大的窗口保持与全注意力相似的性能，但在短上下文场景中提供的效率增益极小，而较小的窗口可能导致性能下降。当前的模型，如Gemma2和Mistral，采用保守的窗口大小（例如，8192预训练长度中的4096）以保持性能。这项工作研究了如何改变这种帕累托前沿的策略，使局部-全局模型即使在短上下文条件下也能实现效率增益。我们的核心动机是解决局部注意力的内在局限性——它完全忽略了定义窗口之外的令牌。我们探索了RATTENTION，这是一种局部注意力的变体，它集成了一种专门的线性注意力机制，旨在捕获来自这些窗口外令牌的信息。在3B和12B规模的预训练实验表明，RATTENTION在性能和效率之间实现了更优的帕累托权衡。作为最佳选择，窗口大小仅为512的RATTENTION在不同设置下能持续匹配全注意力模型的性能。此外，RATTENTION中线性注意力组件固有的循环特性有助于增强长上下文性能，这已在RULER基准测试中得到验证。至关重要的是，这些改进并未牺牲训练效率；得益于专门的核实现和减小的窗口大小，RATTENTION保持了与现有最先进方法相当的训练速度。", "summary": "RATTENTION是一种新型的局部-全局注意力模型，通过引入专门的线性注意力机制来捕获窗口外信息，解决了传统局部注意力模型在小窗口尺寸下的性能下降问题。实验证明，RATTENTION在仅512的窗口大小下即可匹配全注意力模型的性能，并在3B和12B规模上展现出优越的性能-效率权衡，同时提升了长上下文性能，且不影响训练效率。", "keywords": "局部-全局注意力, 线性注意力, RATTENTION, 窗口大小, Transformer", "comments": "RATTENTION的创新点在于其将线性注意力机制融入局部注意力，有效解决了局部注意力对窗口外信息忽略的固有缺陷。这使得模型能在更小的滑动窗口下保持甚至超越性能，显著提升了短上下文场景的效率。其在长上下文方面的表现也值得关注。这项工作对于推动高效Transformer模型的发展具有重要意义，特别是对于资源受限或需要处理极长序列的应用。"}}
{"id": "2506.15025", "title": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size", "authors": ["Soufiane Hayou", "Liyuan Liu"], "summary": "Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.", "comment": "TD,LR: How to set the learning rate for emebdding layer in LLMs?", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15025v1", "AI": {"title_translation": "LLM中最佳嵌入学习率：词汇量大小的影响", "tldr": "新理论表明，LLM中最佳嵌入学习率的缩放方式会随词汇量大小变化，在大词汇量场景下偏离μP理论预测，但与经验观察结果一致。", "motivation": "μP理论在应用于大型语言模型（LLM）时，其超参数可迁移性与实际经验存在冲突，部分原因是μP理论在宽度趋于无穷时将输入维度（即LLM的词汇量大小）视为固定，这与实际中词汇量远大于宽度的LLM不符。", "method": "本研究对词汇量大小对训练动态的影响进行了理论分析。此外，通过进行多项实验来验证理论，并从头开始预训练了一个1B模型以展示所提出嵌入学习率缩放规则的益处。", "result": "研究发现，随着词汇量大小的增加，训练动态在μP机制和一种新发现的“大词汇（LV）机制”之间进行插值。在LV机制中，最佳嵌入学习率与隐藏层学习率的比率应大致按$\\\\Theta(\\\\sqrt{width})$缩放，这与μP预测的$\\\\Theta(width)$不同，但与先前文献中的经验发现惊人地接近。实验验证了理论，并展示了新缩放规则的益处。", "conclusion": "本研究识别出一种新的“大词汇机制”，在此机制下，LLM的最佳学习率缩放规则不同于μP理论的预测，为先前观察到的经验发现提供了理论基础，并为实际LLM预训练中嵌入学习率的优化提供了有益的缩放规则。", "translation": "预训练大型语言模型是一个成本高昂的过程。为了提高这一过程的效率，已经提出了几种方法来优化模型架构/参数化和硬件使用。在参数化方面，$\\\\mu P$（最大更新参数化）以一种使超参数（HPs）能够随宽度（嵌入维度）转移的方式参数化模型权重和学习率（LR）：超参数可以针对小型模型进行调整，然后用于大型模型而无需额外调整。尽管$\\\\mu$P在实践中显示出令人印象深刻的结果，但最近的实证研究在应用于LLM时报告了相互矛盾的观察结果。$\\\\mu$P理论的一个局限性在于，当宽度趋于无穷大时，输入维度（LLM中的词汇量大小）被认为是固定的。这不切实际，因为在实践中词汇量大小通常远大于宽度。在这项工作中，我们对词汇量大小对训练动态的影响进行了理论分析，随后表明随着词汇量大小的增加，训练动态在$\\\\mu$P机制和我们称之为大词汇（LV）机制之间进行插值，其中最优缩放规则与$\\\\mu$P预测的不同。我们的分析表明，在LV机制中，最佳嵌入LR与隐藏层LR的比率应大致按$\\\\Theta(\\\\sqrt{width})$缩放，这与文献中先前报道的经验发现惊人地接近，并且与$\\\\mu$P预测的$\\\\Theta(width)$比率不同。我们进行了多项实验来验证我们的理论，并从头开始预训练了一个1B模型，以展示我们建议的嵌入LR缩放规则的益处。", "summary": "本研究解决了大型语言模型（LLM）预训练中$\\\\mu P$理论在处理大词汇量时的局限性。通过理论分析，论文发现LLM训练动态在$\\\\mu P$机制和新的“大词汇（LV）机制”之间插值。在LV机制中，最佳嵌入学习率与隐藏层学习率的比率应按$\\\\Theta(\\\\sqrt{width})$缩放，这与经验观察结果吻合，并不同于$\\\\mu P$的预测。实验验证了该理论，并展示了所提出缩放规则对1B模型预训练的益处。", "keywords": "LLMs, 学习率, 词汇量大小, 最大更新参数化, 缩放规则", "comments": "这篇论文通过解决$\\\\mu P$理论在LLM大词汇量场景下的局限性，对其进行了重要的理论完善。其发现的嵌入学习率$\\\\Theta(\\\\sqrt{width})$缩放规则具有重要意义，因为它弥合了理论与相互矛盾的经验观察之间的鸿沟，为更高效的LLM预训练提供了实用的指导。"}}
{"id": "2506.15553", "title": "Approximating Language Model Training Data from Weights", "authors": ["John X. Morris", "Junjie Oscar Yin", "Woojeong Kim", "Vitaly Shmatikov", "Alexander M. Rush"], "summary": "Modern language models often have open weights but closed training data. We\nformalize the problem of data approximation from model weights and propose\nseveral baselines and metrics. We develop a gradient-based approach that\nselects the highest-matching data from a large public text corpus and show its\neffectiveness at recovering useful data given only weights of the original and\nfinetuned models. Even when none of the true training data is known, our method\nis able to locate a small subset of public Web documents can be used to train a\nmodel to close to the original model performance given models trained for both\nclassification and supervised-finetuning. On the AG News classification task,\nour method improves performance from 65% (using randomly selected data) to 80%,\napproaching the expert benchmark of 88%. When applied to a model trained with\nSFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,\ncompared to an expert LLAMA model's perplexity of 2.0.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15553v1", "AI": {"title_translation": "从模型权重近似语言模型训练数据", "tldr": "本文提出了一种基于梯度的方法，可以从公开文本语料库中近似语言模型的训练数据，即使不知道原始数据也能有效恢复，并在分类和SFT任务上展现出接近专家模型的性能。", "motivation": "现代语言模型通常开放权重但封闭训练数据，这使得理解模型行为和复现结果变得困难。本文旨在解决从模型权重近似训练数据的问题。", "method": "本文提出了一种基于梯度的方法，通过从大型公共文本语料库中选择最高匹配的数据来近似语言模型的训练数据。该方法利用原始模型和微调模型的权重来恢复有用的数据。", "result": "在AG News分类任务上，该方法将性能从随机选择数据的65%提高到80%，接近专家基准的88%。应用于MSMARCO网络文档上通过SFT训练的模型时，该方法将困惑度从3.3降低到2.3，而专家LLAMA模型的困惑度为2.0。即使不知道真实的训练数据，该方法也能找到一小部分公共网络文档，用于训练模型以接近原始模型的性能。", "conclusion": "本文提出的基于梯度的方法能够有效地从语言模型权重中近似训练数据，即使在缺乏真实训练数据的情况下也能恢复有用的信息，并在多个任务上展现出接近专家模型的性能。", "translation": "现代语言模型通常具有开放权重但封闭训练数据。我们形式化了从模型权重近似数据的问​​题，并提出了几种基线和度量标准。我们开发了一种基于梯度的方法，该方法从大型公共文本语料库中选择最高匹配的数据，并展示了其在仅给定原始模型和微调模型权重的情况下恢复有用数据的有效性。即使不知道任何真实的训练数据，我们的方法也能够定位一小部分公共网络文档，这些文档可用于训练模型，使其性能接近原始模型，适用于分类和监督微调模型。在AG News分类任务上，我们的方法将性能从65%（使用随机选择的数据）提高到80%，接近专家基准的88%。当应用于使用SFT在MSMARCO网络文档上训练的模型时，我们的方法将困惑度从3.3降低到2.3，而专家LLAMA模型的困惑度为2.0。", "summary": "本文研究了从语言模型权重近似训练数据的问题，并提出了一种创新的基于梯度的方法。该方法能够从大型公共文本语料库中选择与模型权重高度匹配的数据，有效恢复原始训练数据或其近似，即使在不了解真实训练数据的情况下也能成功。实验结果表明，该方法在分类和监督微调任务上均能显著提升模型性能，使其接近专家模型的水平，为理解和复现大型语言模型提供了新途径。", "keywords": "语言模型, 训练数据近似, 模型权重, 梯度方法, 数据恢复", "comments": "本文提出了一种新颖且重要的方法，解决了当前大型语言模型领域中训练数据不透明的问题。其创新之处在于利用模型权重来反向推导或近似训练数据，这对于模型的公平性、可解释性和复现性具有重要意义。该方法在没有真实训练数据的情况下也能表现出色，证明了其强大的实用性。未来的工作可以探索这种方法在不同模型架构和更大规模数据集上的适用性。"}}
{"id": "2506.15279", "title": "BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via Bezier Curve Refinement", "authors": ["Qian Li", "Feng Liu", "Shuojue Yang", "Daiyun Shen", "Yueming Jin"], "summary": "Laparoscopic liver surgery, while minimally invasive, poses significant\nchallenges in accurately identifying critical anatomical structures. Augmented\nreality (AR) systems, integrating MRI/CT with laparoscopic images based on\n2D-3D registration, offer a promising solution for enhancing surgical\nnavigation. A vital aspect of the registration progress is the precise\ndetection of curvilinear anatomical landmarks in laparoscopic images. In this\npaper, we propose BCRNet (Bezier Curve Refinement Net), a novel framework that\nsignificantly enhances landmark detection in laparoscopic liver surgery\nprimarily via the Bezier curve refinement strategy. The framework starts with a\nMulti-modal Feature Extraction (MFE) module designed to robustly capture\nsemantic features. Then we propose Adaptive Curve Proposal Initialization\n(ACPI) to generate pixel-aligned Bezier curves and confidence scores for\nreliable initial proposals. Additionally, we design the Hierarchical Curve\nRefinement (HCR) mechanism to enhance these proposals iteratively through a\nmulti-stage process, capturing fine-grained contextual details from multi-scale\npixel-level features for precise Bezier curve adjustment. Extensive evaluations\non the L3D and P2ILF datasets demonstrate that BCRNet outperforms\nstate-of-the-art methods, achieving significant performance improvements. Code\nwill be available.", "comment": "Accepted at MICCAI 2025, 11 pages, 2 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15279v1", "AI": {"title_translation": "BCRNet：通过贝塞尔曲线细化增强腹腔镜肝脏手术中的地标检测", "tldr": "BCRNet 提出了一种通过贝塞尔曲线细化策略来显著增强腹腔镜肝脏手术中曲线地标检测的新框架，在多个数据集上表现优于现有SOTA方法。", "motivation": "腹腔镜肝脏手术中准确识别关键解剖结构具有挑战性，而增强现实（AR）系统中的2D-3D配准需要精确检测腹腔镜图像中的曲线解剖地标。", "method": "本文提出了BCRNet（Bezier Curve Refinement Net）框架。该框架首先使用多模态特征提取（MFE）模块捕获语义特征。接着，通过自适应曲线提议初始化（ACPI）生成像素对齐的贝塞尔曲线和置信分数作为初始提议。最后，设计了分层曲线细化（HCR）机制，通过多阶段过程迭代增强提议，并从多尺度像素级特征中捕获细粒度上下文细节以精确调整贝塞尔曲线。", "result": "在L3D和P2ILF数据集上的广泛评估表明，BCRNet优于最先进的方法，实现了显著的性能改进。", "conclusion": "BCRNet通过其新颖的贝塞尔曲线细化策略，能够显著提升腹腔镜肝脏手术中曲线解剖地标的检测精度，为增强手术导航提供了有效解决方案。", "translation": "腹腔镜肝脏手术虽然微创，但在准确识别关键解剖结构方面提出了重大挑战。增强现实（AR）系统，通过基于2D-3D配准将MRI/CT与腹腔镜图像整合，为增强手术导航提供了一个有前景的解决方案。配准过程的一个重要方面是精确检测腹腔镜图像中的曲线解剖地标。在本文中，我们提出了BCRNet（Bezier Curve Refinement Net），这是一个主要通过贝塞尔曲线细化策略显著增强腹腔镜肝脏手术中地标检测的新颖框架。该框架首先包含一个多模态特征提取（MFE）模块，旨在鲁棒地捕获语义特征。然后，我们提出了自适应曲线提议初始化（ACPI）来生成像素对齐的贝塞尔曲线和置信分数，以实现可靠的初始提议。此外，我们设计了分层曲线细化（HCR）机制，通过多阶段过程迭代增强这些提议，从多尺度像素级特征中捕获细粒度上下文细节，以精确调整贝塞尔曲线。在L3D和P2ILF数据集上的广泛评估表明，BCRNet优于最先进的方法，实现了显著的性能改进。代码将可用。", "summary": "本文提出了BCRNet，一个用于腹腔镜肝脏手术中精确检测曲线解剖地标的新框架。它通过多模态特征提取模块捕获特征，利用自适应曲线提议初始化生成初始贝塞尔曲线提议，并通过分层曲线细化机制迭代优化这些曲线。实验证明，BCRNet在L3D和P2ILF数据集上显著优于现有最先进方法。", "keywords": "腹腔镜肝脏手术, 地标检测, 贝塞尔曲线, 增强现实, 2D-3D配准", "comments": "本文通过引入贝塞尔曲线细化策略，为腹腔镜肝脏手术中的曲线地标检测提供了一个创新且有效的方法。其多阶段的特征提取和曲线优化机制有望显著提高手术导航的精度，对于提高手术安全性和效率具有重要意义。该方法的通用性也值得关注。"}}
{"id": "2506.15051", "title": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization", "authors": ["Zheng Li", "Jerry Cheng", "Huanying Helen Gu"], "summary": "Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.", "comment": "10 pages, 2 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15051v1", "AI": {"title_translation": "序列策略梯度用于自适应超参数优化", "tldr": "本文提出序列策略梯度建模（SPG），一种受DeepSeek-V3启发的轻量级在线超参数优化方法。SPG通过单次前向传播生成状态-动作轨迹，显著降低计算成本，并在多领域（计算机视觉、NLP、音频）的广泛模型上实现性能提升（+0.2%~7%），超越标准迁移微调。", "motivation": "传统的强化学习方法在神经架构搜索和超参数优化中因高昂的时间和计算成本而阻碍了其广泛应用。", "method": "本文提出序列策略梯度建模（SPG），一种用于轻量级在线超参数优化的新型轨迹生成范式。SPG通过临时模块扩展基础模型，使其能够在单次前向传播中生成状态-动作（填充）轨迹，与传统策略梯度方法不同。", "result": "实验表明，使用SPG重新训练的模型在其原始数据集上性能有所提升，并且优于标准迁移微调。在计算机视觉（ImageNet、COCO）、自然语言处理（GLUE、SQuAD）和音频（SUPERB）的五个数据集上评估，SPG在广泛采用的模型上表现出一致的改进，实现了+0.2%~7%的性能提升，同时计算成本显著降低。", "conclusion": "SPG方法在多种任务和模型上实现了性能提升，同时显著降低了计算成本，证明了其工业适用性。", "translation": "强化学习对于神经架构搜索和超参数优化至关重要，但传统方法因高昂的时间和计算成本而阻碍了其广泛应用。受DeepSeek-V3多令牌预测架构的启发，我们提出了序列策略梯度建模（SPG），这是一种用于轻量级在线超参数优化的新型轨迹生成范式。与传统策略梯度方法不同，SPG通过临时模块扩展基础模型，使其能够在单次前向传播中生成状态-动作（填充）轨迹。我们的实验表明，使用SPG在其原始数据集上重新训练的模型性能有所提升，并且也优于标准迁移微调。我们在涵盖计算机视觉（ImageNet、COCO）、自然语言处理（GLUE、SQuAD）和音频（SUPERB）的五个数据集上进行评估，以评估SPG的工业适用性。所提出的方法在广泛采用的模型上表现出一致的改进，实现了+0.2%~7%的性能提升，同时计算成本显著降低。完全可复现的代码和预训练模型：https://huggingface.co/UniversalAlgorithmic/SPG。", "summary": "本文提出一种名为序列策略梯度建模（SPG）的新型方法，用于轻量级在线超参数优化。SPG通过在单次前向传播中生成状态-动作轨迹，有效解决了传统强化学习在超参数优化中高昂计算成本的问题。实验证明，SPG在多个领域（计算机视觉、NLP、音频）的广泛模型上均能带来显著的性能提升（0.2%~7%），同时大幅降低计算开销，展现了良好的工业应用前景。", "keywords": "序列策略梯度, 超参数优化, 强化学习, 计算效率, 轨迹生成", "comments": "SPG的创新点在于其单次前向传播生成轨迹的机制，这显著降低了超参数优化的计算复杂度，使其更适用于实际应用。其在多领域和多模型上的验证也增加了其重要性。"}}
{"id": "2506.15560", "title": "RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation", "authors": ["Xingrui Qin", "Wentao Zhao", "Chuan Cao", "Yihe Niu", "Houcheng Jiang", "Jingchuan Wang"], "summary": "Dense metric depth estimation using millimeter-wave radar typically requires\ndense LiDAR supervision, generated via multi-frame projection and\ninterpolation, to guide the learning of accurate depth from sparse radar\nmeasurements and RGB images. However, this paradigm is both costly and\ndata-intensive. To address this, we propose RaCalNet, a novel framework that\neliminates the need for dense supervision by using sparse LiDAR to supervise\nthe learning of refined radar measurements, resulting in a supervision density\nof merely around 1% compared to dense-supervised methods. Unlike previous\napproaches that associate radar points with broad image regions and rely\nheavily on dense labels, RaCalNet first recalibrates and refines sparse radar\npoints to construct accurate depth priors. These priors then serve as reliable\nanchors to guide monocular depth prediction, enabling metric-scale estimation\nwithout resorting to dense supervision. This design improves structural\nconsistency and preserves fine details. Despite relying solely on sparse\nsupervision, RaCalNet surpasses state-of-the-art dense-supervised methods,\nproducing depth maps with clear object contours and fine-grained textures.\nExtensive experiments on the ZJU-4DRadarCam dataset and real-world deployment\nscenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%,\nrespectively.", "comment": "9 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15560v1", "AI": {"title_translation": "RaCalNet: 面向稀疏监督度量深度估计的雷达校准网络", "tldr": "RaCalNet是一种新颖的框架，通过使用稀疏的LiDAR监督来校准和优化雷达测量，从而实现度量深度估计，而无需密集的LiDAR监督，并且性能超越了现有密集监督方法。", "motivation": "现有的毫米波雷达密集度量深度估计方法通常需要昂贵且数据密集型的密集LiDAR监督，这限制了其应用。本研究的动机是解决这一问题，通过开发一种无需密集监督的替代方案。", "method": "RaCalNet首先重新校准和优化稀疏雷达点，以构建准确的深度先验。这些先验随后作为可靠的锚点，指导单目深度预测，从而在不依赖密集监督的情况下实现度量尺度的估计。其监督密度仅为密集监督方法的约1%。", "result": "RaCalNet在ZJU-4DRadarCam数据集和实际部署场景中均表现出色，与最先进的密集监督方法相比，RMSE分别降低了35.30%和34.89%。它能够生成具有清晰物体轮廓和精细纹理的深度图。", "conclusion": "RaCalNet成功地通过稀疏监督实现了高质量的度量深度估计，超越了传统的密集监督方法，证明了其在降低成本和数据需求方面的有效性。", "translation": "使用毫米波雷达进行密集度量深度估计通常需要密集的激光雷达监督，通过多帧投影和插值生成，以指导从稀疏雷达测量和RGB图像中学习准确的深度。然而，这种范式既昂贵又数据密集。为了解决这个问题，我们提出了RaCalNet，一个新颖的框架，通过使用稀疏激光雷达监督精炼雷达测量的学习，从而消除了对密集监督的需求，与密集监督方法相比，其监督密度仅为约1%。与以往将雷达点与宽泛图像区域关联并严重依赖密集标签的方法不同，RaCalNet首先重新校准和精炼稀疏雷达点，以构建准确的深度先验。这些先验随后作为可靠的锚点，指导单目深度预测，从而在不诉诸密集监督的情况下实现度量尺度的估计。这种设计提高了结构一致性并保留了精细细节。尽管仅依赖稀疏监督，RaCalNet超越了最先进的密集监督方法，生成了具有清晰物体轮廓和精细纹理的深度图。在ZJU-4DRadarCam数据集和实际部署场景中进行的广泛实验证明了其有效性，分别将RMSE降低了35.30%和34.89%。", "summary": "本文提出RaCalNet，一个用于稀疏监督度量深度估计的雷达校准网络。针对传统方法需要昂贵且数据密集型密集LiDAR监督的问题，RaCalNet创新性地利用稀疏LiDAR监督（密度仅为1%）来校准和精炼雷达测量，生成准确的深度先验，进而指导单目深度预测。实验证明，RaCalNet在仅使用稀疏监督的情况下，性能超越了现有密集监督方法，显著降低了RMSE，并生成了高质量的深度图。", "keywords": "雷达深度估计, 稀疏监督, 度量深度, RaCalNet, 毫米波雷达", "comments": "RaCalNet的创新之处在于其通过稀疏监督实现高精度深度估计的能力，这极大地降低了数据采集和标注的成本。其“先校准再锚定”的设计思路有效地利用了稀疏雷达数据，克服了传统方法对密集标签的依赖，对于自动驾驶和机器人领域具有重要意义。"}}
{"id": "2506.15285", "title": "AI-driven visual monitoring of industrial assembly tasks", "authors": ["Mattia Nardon", "Stefano Messelodi", "Antonio Granata", "Fabio Poiesi", "Alberto Danese", "Davide Boscaini"], "summary": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15285v1", "AI": {"title_translation": "AI驱动的工业装配任务视觉监控", "tldr": "ViMAT是一个新型的AI驱动系统，用于实时视觉监控工业装配任务，无需 rigid workspace setups 或 visual markers，并在挑战性真实场景中表现出有效性。", "motivation": "工业装配任务的视觉监控对于防止程序错误造成的设备损坏和确保工人安全至关重要。尽管存在商业解决方案，但它们通常需要严格的工作空间设置或应用视觉标记来简化问题。", "method": "ViMAT结合了一个感知模块，从多视角视频流中提取视觉观察结果，以及一个推理模块，根据观察到的装配状态和先验任务知识推断最可能执行的动作。", "result": "ViMAT在两项装配任务中进行了验证：乐高组件的更换和液压机模具的重新配置。通过定量和定性分析，在以部分和不确定视觉观察为特征的挑战性真实世界场景中，展示了其有效性。", "conclusion": "ViMAT在没有 rigid workspace setups 或 visual markers 的情况下，能够有效地进行实时工业装配任务的视觉监控，并在具有挑战性的真实世界场景中表现出其鲁棒性。", "translation": "工业装配任务的视觉监控对于防止因程序错误造成的设备损坏和确保工人安全至关重要。尽管存在商业解决方案，但它们通常需要僵硬的工作空间设置或应用视觉标记来简化问题。我们引入了ViMAT，一个新型的AI驱动系统，用于实时视觉监控装配任务，它无需这些限制即可操作。ViMAT结合了一个感知模块，该模块从多视角视频流中提取视觉观察结果，以及一个推理模块，该模块根据观察到的装配状态和先验任务知识推断最可能执行的动作。我们在两项装配任务中验证了ViMAT，包括乐高组件的更换和液压机模具的重新配置，通过定量和定性分析，在以部分和不确定视觉观察为特征的挑战性真实世界场景中展示了其有效性。项目页面：https://tev-fbk.github.io/ViMAT", "summary": "本论文介绍了ViMAT，一个创新的AI驱动系统，旨在实时视觉监控工业装配任务，无需传统商业解决方案所需的严格工作空间设置或视觉标记。ViMAT通过结合感知模块（从多视角视频流中提取视觉信息）和推理模块（根据观察到的状态和先验知识推断动作）来实现。该系统已在乐高组件更换和液压机模具重新配置等实际装配任务中得到验证，并在具有挑战性的真实世界条件下展现出其有效性。", "keywords": "工业装配, 视觉监控, AI驱动, 实时系统, 任务推理", "comments": "ViMAT的创新之处在于其无需 rigid workspace setups 或 visual markers 的能力，这大大提高了其在多样化工业环境中的适用性和灵活性。通过结合感知和推理模块，该系统能够处理复杂且不确定的视觉数据，这对于实际应用至关重要。其在真实世界场景中的验证进一步证明了其实用性和鲁棒性。"}}
{"id": "2506.15054", "title": "Muon Optimizes Under Spectral Norm Constraints", "authors": ["Lizhang Chen", "Jonathan Li", "Qiang Liu"], "summary": "The pursuit of faster optimization algorithms remains an active and important\nresearch direction in deep learning. Recently, the Muon optimizer [JJB+24] has\ndemonstrated promising empirical performance, but its theoretical foundation\nremains less understood. In this paper, we bridge this gap and provide a\ntheoretical analysis of Muon by placing it within the Lion-$\\mathcal{K}$ family\nof optimizers [CLLL24]. Specifically, we show that Muon corresponds to\nLion-$\\mathcal{K}$ when equipped with the nuclear norm, and we leverage the\ntheoretical results of Lion-$\\mathcal{K}$ to establish that Muon (with\ndecoupled weight decay) implicitly solves an optimization problem that enforces\na constraint on the spectral norm of weight matrices. This perspective not only\ndemystifies the implicit regularization effects of Muon but also leads to\nnatural generalizations through varying the choice of convex map $\\mathcal{K}$,\nallowing for the exploration of a broader class of implicitly regularized and\nconstrained optimization algorithms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15054v1", "AI": {"title_translation": "Muon在谱范数约束下进行优化", "tldr": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$家族中，对其进行了理论分析，揭示了Muon隐式地解决了对权重矩阵谱范数施加约束的优化问题。", "motivation": "深度学习中对更快优化算法的追求是一个活跃且重要的研究方向。Muon优化器虽然表现出色，但其理论基础尚不明确，本文旨在弥补这一理论空白。", "method": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$优化器家族中进行理论分析。具体来说，证明了Muon在配备核范数时与Lion-$\\mathcal{K}$相对应，并利用Lion-$\\mathcal{K}$的理论结果来确立Muon隐式地解决了一个对权重矩阵谱范数施加约束的优化问题。", "result": "研究表明，Muon优化器在配备核范数时与Lion-$\\mathcal{K}$优化器家族中的一员相对应。此外，Muon（带有解耦权重衰减）隐式地解决了一个对权重矩阵的谱范数施加约束的优化问题。", "conclusion": "这一新视角不仅揭示了Muon的隐式正则化效应，而且通过改变凸映射$\\mathcal{K}$的选择，为Muon的自然泛化提供了途径，从而能够探索更广泛的隐式正则化和受约束优化算法。", "translation": "追求更快的优化算法仍然是深度学习中一个活跃而重要的研究方向。最近，Muon优化器[JJB+24]展现了有前景的经验性能，但其理论基础仍有待深入理解。在本文中，我们弥补了这一空白，通过将Muon置于Lion-$\\mathcal{K}$优化器家族[CLLL24]中，对其进行了理论分析。具体来说，我们证明了当配备核范数时，Muon与Lion-$\\mathcal{K}$相对应，并且我们利用Lion-$\\mathcal{K}$的理论结果来确立Muon（带有解耦权重衰减）隐式地解决了一个对权重矩阵谱范数施加约束的优化问题。这一视角不仅揭示了Muon的隐式正则化效应，而且通过改变凸映射$\\mathcal{K}$的选择，为自然泛化提供了途径，从而能够探索更广泛的隐式正则化和受约束优化算法。", "summary": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$家族中，对其理论基础进行了深入分析。研究发现，Muon在结合核范数时等同于Lion-$\\mathcal{K}$，并且它隐式地解决了一个对权重矩阵谱范数施加约束的优化问题。这一发现不仅揭示了Muon的隐式正则化机制，也为开发更多样的隐式正则化优化算法提供了新的方向。", "keywords": "Muon优化器, 谱范数, 隐式正则化, Lion-$\\mathcal{K}$, 优化算法", "comments": "这篇论文通过将Muon优化器与已有的Lion-$\\mathcal{K}$框架联系起来，成功地为Muon的经验成功提供了理论解释，揭示了其隐式正则化效应。创新点在于这种联系揭示了Muon对谱范数的约束，这为理解深度学习优化器中的正则化机制提供了新的视角。此外，通过泛化$\\mathcal{K}$的选择，该工作为设计新型优化算法开辟了道路，具有重要的理论和实践意义。"}}
{"id": "2506.15568", "title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models", "authors": ["Zhengyang Shan", "Emily Ruth Diana", "Jiawei Zhou"], "summary": "We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.", "comment": "Accepted by ACL 2025 Main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15568v1", "AI": {"title_translation": "性别包容性公平指数 (GIFI)：一个评估大型语言模型性别多样性的多级框架", "tldr": "本文提出了性别包容性公平指数（GIFI），这是一个新的多级度量标准，用于全面评估大型语言模型（LLMs）在处理二元和非二元性别方面的公平性。通过对22个LLMs进行评估，研究发现它们在性别包容性方面存在显著差异，并强调了改进LLMs包容性的重要性。", "motivation": "之前的研究主要关注二元性别区分，但大型语言模型（LLMs）需要能够公平地处理二元和非二元性别。因此，需要一个全面且新颖的指标来量化LLMs在多元性别方面的包容性。", "method": "研究引入了性别包容性公平指数（GIFI），这是一个新颖且全面的多级度量标准，用于量化LLMs的多元性别包容性。GIFI的评估范围从探测模型对性别代词的反应，到测试模型在不同性别假设下的生成和认知行为，以揭示相关偏见。研究使用GIFI对22个著名的开源和专有LLMs进行了广泛评估。", "result": "研究发现不同的LLMs在性别包容性方面存在显著差异。", "conclusion": "本研究强调了提高LLMs包容性的重要性，并为生成模型中性别公平性的未来发展提供了一个关键基准。", "translation": "我们对大型语言模型（LLMs）中的性别公平性进行了全面评估，重点关注它们处理二元和非二元性别的能力。虽然之前的研究主要关注二元性别区分，但我们引入了性别包容性公平指数（GIFI），这是一个新颖且全面的指标，用于量化LLMs的多元性别包容性。GIFI包含不同层面的广泛评估，从简单地使用提供的性别代词探测模型，到测试模型在不同性别假设下的生成和认知行为的各个方面，揭示了与不同性别标识符相关的偏见。我们使用GIFI对22个不同规模和能力的著名开源和专有LLMs进行了广泛评估，发现了LLMs性别包容性方面的显著差异。我们的研究强调了提高LLMs包容性的重要性，为生成模型中性别公平性的未来发展提供了关键基准。", "summary": "本文介绍了性别包容性公平指数（GIFI），这是一个新颖的多级框架，旨在全面评估大型语言模型（LLMs）的性别公平性，特别关注其处理二元和非二元性别的能力。GIFI通过多层面评估，从代词探测到模型生成和认知行为测试，揭示了LLMs在不同性别假设下的偏见。研究使用GIFI对22个主流LLMs进行了广泛测试，发现它们在性别包容性方面存在显著差异。该研究强调了提升LLMs包容性的必要性，并为未来生成模型在性别公平性方面的进展提供了重要的基准。", "keywords": "性别公平性, 大型语言模型, 性别包容性, 多级评估, 偏见", "comments": "该研究的创新之处在于提出了GIFI，一个超越传统二元性别区分的多级、全面性别包容性评估框架。其重要性在于为LLMs的性别公平性评估提供了一个标准化基准，对于促进AI伦理发展和减少模型偏见具有关键意义。"}}
{"id": "2506.15635", "title": "FindingDory: A Benchmark to Evaluate Memory in Embodied Agents", "authors": ["Karmesh Yadav", "Yusuf Ali", "Gunshi Gupta", "Yarin Gal", "Zsolt Kira"], "summary": "Large vision-language models have recently demonstrated impressive\nperformance in planning and control tasks, driving interest in their\napplication to real-world robotics. However, deploying these models for\nreasoning in embodied contexts is limited by their ability to incorporate\nlong-term experience collected across multiple days and represented by vast\ncollections of images. Current VLMs typically struggle to process more than a\nfew hundred images concurrently, highlighting the need for more efficient\nmechanisms to handle long-term memory in embodied settings. To effectively\nevaluate these models for long-horizon control, a benchmark must specifically\ntarget scenarios where memory is crucial for success. Existing long-video QA\nbenchmarks overlook embodied challenges like object manipulation and\nnavigation, which demand low-level skills and fine-grained reasoning over past\ninteractions. Moreover, effective memory integration in embodied agents\ninvolves both recalling relevant historical information and executing actions\nbased on that information, making it essential to study these aspects together\nrather than in isolation. In this work, we introduce a new benchmark for\nlong-range embodied tasks in the Habitat simulator. This benchmark evaluates\nmemory-based capabilities across 60 tasks requiring sustained engagement and\ncontextual awareness in an environment. The tasks can also be procedurally\nextended to longer and more challenging versions, enabling scalable evaluation\nof memory and reasoning. We also present baselines that integrate\nstate-of-the-art VLMs with low level navigation policies, assessing their\nperformance on these memory-intensive tasks and highlight areas for\nimprovement.", "comment": "Our dataset and code will be made available at:\n  https://findingdory-benchmark.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15635v1", "AI": {"title_translation": "FindingDory: 一个评估具身智能体记忆能力的基准", "tldr": "引入FindingDory基准，用于评估具身智能体在需要长期记忆的复杂任务中的表现，并展示了现有VLMs的局限性。", "motivation": "现有大型视觉-语言模型（VLMs）在具身情境下处理大量长期经验的能力有限，难以整合跨多天的信息。同时，现有长视频问答基准忽视了具身挑战，如物体操作和导航中对记忆的低层次和精细化推理需求，而这些对成功至关重要。", "method": "本文引入了一个名为FindingDory的新基准，用于Habitat模拟器中的长距离具身任务。该基准包含60个需要持续参与和环境上下文感知的记忆密集型任务，并且这些任务可以程序化地扩展为更长、更具挑战性的版本。此外，还提出了将最先进的VLM与低级导航策略相结合的基线，并评估其在这些任务上的性能。", "result": "评估了基线模型在记忆密集型任务上的表现，并突出了需要改进的领域。具体性能数据未在摘要中提及。", "conclusion": "FindingDory基准能有效评估具身智能体在需要长期记忆和推理的复杂任务中的能力，并揭示了现有VLMs在处理此类任务时的局限性和改进空间。该基准为未来研究提供了可扩展的评估工具。", "translation": "大型视觉-语言模型最近在规划和控制任务中表现出色，推动了它们在现实世界机器人中的应用兴趣。然而，将这些模型部署到具身环境中进行推理受到其整合跨多天收集并由大量图像表示的长期经验的能力的限制。当前的VLM通常难以同时处理数百张以上的图像，这突显了在具身环境中处理长期记忆的更有效机制的需求。为了有效评估这些模型进行长周期控制，基准必须专门针对记忆对成功至关重要的场景。现有的长视频问答基准忽略了具身挑战，如物体操作和导航，这些挑战需要低级技能和对过去交互的精细推理。此外，具身智能体中有效的记忆整合涉及回忆相关的历史信息和根据这些信息执行动作，因此有必要将这些方面结合起来研究，而不是孤立地研究。在这项工作中，我们引入了一个在Habitat模拟器中用于长距离具身任务的新基准。该基准评估了在需要持续参与和环境上下文感知的60项任务中的基于记忆的能力。这些任务还可以程序化地扩展为更长、更具挑战性的版本，从而实现记忆和推理的可扩展评估。我们还提出了将最先进的VLM与低级导航策略相结合的基线，评估它们在这些记忆密集型任务上的性能，并突出需要改进的领域。", "summary": "本文介绍了FindingDory，一个用于评估具身智能体长期记忆能力的基准。鉴于现有视觉-语言模型在处理大量长期经验方面的局限性，以及现有基准未能充分涵盖具身任务中对记忆的精细推理需求，FindingDory在Habitat模拟器中设计了60项需要持续记忆和上下文感知的长距离具身任务，并支持任务的程序化扩展以实现可扩展评估。作者还提供了结合先进VLM和导航策略的基线，以评估其在记忆密集型任务上的表现，并指出未来改进方向。", "keywords": "具身智能体, 记忆, 基准, 视觉-语言模型, Habitat模拟器", "comments": "该工作创新性地提出了一个专门针对具身智能体长期记忆评估的基准，填补了现有基准在具身情境下对低级技能和精细推理关注不足的空白。其任务设计（可程序化扩展）为未来研究提供了可扩展的评估工具，对于推动具身智能体在复杂、长周期任务中的发展具有重要意义。"}}
{"id": "2506.15298", "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering", "authors": ["Xinqi Fan", "Jingting Li", "John See", "Moi Hoon Yap", "Wen-Huang Cheng", "Xiaobai Li", "Xiaopeng Hong", "Su-Jing Wang", "Adrian K. Davision"], "summary": "Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.", "comment": "Micro-Expression Grand Challenge (MEGC) at ACM MM 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15298v1", "AI": {"title_translation": "MEGC2025：微表情大挑战之发现即识别与视觉问答", "tldr": "MEGC2025提出了两个新的微表情分析任务：ME-STR（将微表情发现和识别整合）和ME-VQA（利用多模态大模型进行微表情理解的视觉问答），旨在解决传统方法的局限性并利用最新的人工智能进展。", "motivation": "传统的微表情发现和识别方法是独立的任务，这在分析长时间视频时效果不佳。同时，多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）的出现为微表情分析提供了新的强大推理能力，促使挑战引入新的研究方向。", "method": "MEGC 2025引入了两个任务：1) 微表情发现即识别（ME-STR），将微表情的发现和随后的识别整合到一个统一的顺序流程中。2) 微表情视觉问答（ME-VQA），通过视觉问答探索微表情理解，利用MLLMs或LVLMs处理与微表情相关的各种问题。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "面部微表情（MEs）是面部非自主的运动，当一个人经历某种情绪但试图抑制或压抑面部表情时自发产生，通常出现在高风险环境中。近年来，在微表情识别、发现和生成领域取得了实质性进展。然而，将发现和识别视为独立任务的传统方法并非最优，特别是对于分析现实场景中的长时间视频。同时，多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）的出现，通过其强大的多模态推理能力，为增强微表情分析提供了有前景的新途径。微表情大挑战（MEGC）2025引入了两项反映这些不断发展的研究方向的任务：(1) 微表情发现即识别（ME-STR），它将微表情发现和随后的识别整合到一个统一的顺序流程中；(2) 微表情视觉问答（ME-VQA），它通过视觉问答探索微表情理解，利用MLLMs或LVLMs解决与微表情相关的各种问题类型。所有参与算法都必须在此测试集上运行，并在排行榜上提交结果。更多详情请访问https://megc2025.github.io。", "summary": "MEGC2025微表情大挑战旨在推动微表情分析领域的发展，以应对传统方法在处理长时间视频时发现和识别任务分离的局限性。该挑战引入了两项创新任务：一是“微表情发现即识别”（ME-STR），它将微表情的发现与识别整合为统一流程；二是“微表情视觉问答”（ME-VQA），利用多模态大语言模型（MLLMs）或大型视觉语言模型（LVLMs）的强大推理能力，通过视觉问答形式实现对微表情的深入理解。这些任务旨在利用新兴的AI技术，促进微表情分析迈向更集成和智能的方向。", "keywords": "微表情, 大挑战, 发现即识别, 视觉问答, 多模态大模型", "comments": "这项挑战的创新之处在于它直接解决了传统微表情分析中发现和识别任务分离的局限性，并积极拥抱了多模态大模型在视觉-语言理解方面的最新进展。通过引入ME-STR和ME-VQA任务，它为研究人员提供了一个统一和更具挑战性的平台，以开发更贴近实际应用场景的微表情分析解决方案。利用MLLMs和LVLMs进行微表情理解，预示着该领域未来发展的一个重要方向。"}}
{"id": "2506.15569", "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification", "authors": ["Chengye Wang", "Yifei Shen", "Zexi Kuang", "Arman Cohan", "Yilun Zhao"], "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15569v1", "AI": {"title_translation": "SciVer：评估多模态科学声明验证的基础模型", "tldr": "本文介绍了SciVer，一个用于评估基础模型在多模态科学背景下验证声明能力的基准。研究发现当前模型与人类专家之间存在显著性能差距，并揭示了开源模型的关键局限性。", "motivation": "现有方法缺乏专门用于评估基础模型在多模态科学背景下验证声明能力的基准，因此需要一个新基准来填补这一空白并推动模型在此领域的进步。", "method": "研究引入了SciVer，一个包含3,000个专家标注示例的基准，覆盖1,113篇科学论文和四种推理类型。每个示例都包含专家标注的支持证据，以实现细粒度评估。研究评估了21个最先进的多模态基础模型（如GPT-4o-mini, Gemini-2.5-Flash, Llama-3.2-Vision, Qwen2.5-VL），并通过检索增强生成（RAG）和人工错误评估进行了深入分析。", "result": "实验结果显示，这些模型在SciVer上的性能与人类专家之间存在显著差距。通过深入分析，研究发现了当前开源模型在多模态科学文献理解和推理任务中的关键局限性。", "conclusion": "当前最先进的多模态基础模型在科学声明验证方面与人类专家相比仍有显著差距，需要进一步改进模型在多模态科学文献理解和推理方面的能力。", "translation": "我们引入了SciVer，这是第一个专门设计用于评估基础模型在多模态科学背景下验证声明能力的基准。SciVer包含3,000个专家标注的示例，涵盖1,113篇科学论文，分为四个子集，每个子集代表多模态科学声明验证中一种常见的推理类型。为了实现细粒度评估，每个示例都包含专家标注的支持证据。我们评估了21个最先进的多模态基础模型，包括GPT-4o-mini、Gemini-2.5-Flash、Llama-3.2-Vision和Qwen2.5-VL。我们的实验揭示了这些模型在SciVer上与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）和人工错误评估的深入分析，我们识别了当前开源模型的关键局限性，为推动模型在多模态科学文献任务中的理解和推理提供了重要见解。", "summary": "本文提出了SciVer，一个全新的多模态科学声明验证基准，旨在评估基础模型在此领域的性能。该基准包含3,000个专家标注的示例，覆盖多种推理类型和支持证据。研究评估了21个先进的多模态基础模型，发现它们在SciVer上的表现远低于人类专家水平。通过深入分析和错误评估，论文揭示了当前开源模型在科学文献理解和推理方面的核心不足，为未来模型改进提供了关键方向。", "keywords": "SciVer, 科学声明验证, 多模态, 基础模型, 基准评估", "comments": "SciVer的创新之处在于它是首个专门针对多模态科学声明验证设计的基准，填补了现有评估工具的空白。其重要性在于揭示了当前先进基础模型在复杂科学推理任务上的局限性，并为未来研究指明了方向，特别是在结合检索增强生成（RAG）和提升模型对科学文献的深层理解方面。"}}
{"id": "2506.15313", "title": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning", "authors": ["Leonid Ivanov", "Vasily Yuryev", "Dmitry Yudin"], "summary": "In autonomous driving, high-definition (HD) maps and semantic maps in\nbird's-eye view (BEV) are essential for accurate localization, planning, and\ndecision-making. This paper introduces an enhanced End-to-End model named MapFM\nfor online vectorized HD map generation. We show significantly boost feature\nrepresentation quality by incorporating powerful foundation model for encoding\ncamera images. To further enrich the model's understanding of the environment\nand improve prediction quality, we integrate auxiliary prediction heads for\nsemantic segmentation in the BEV representation. This multi-task learning\napproach provides richer contextual supervision, leading to a more\ncomprehensive scene representation and ultimately resulting in higher accuracy\nand improved quality of the predicted vectorized HD maps. The source code is\navailable at https://github.com/LIvanoff/MapFM.", "comment": "Preprint. Submitted. 12 pages, 4 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15313v1", "AI": {"title_translation": "MapFM：基础模型驱动的高清地图生成与多任务上下文学习", "tldr": "MapFM是一个用于在线生成矢量化高清地图的端到端模型，它结合了强大的基础模型来编码图像并利用多任务学习（包括BEV语义分割）来提高地图的准确性和质量。", "motivation": "在自动驾驶中，高精地图（HD map）和鸟瞰图（BEV）中的语义地图对于精确的定位、规划和决策至关重要。", "method": "本文提出了一个名为MapFM的增强型端到端模型，用于在线生成矢量化高清地图。该模型通过结合强大的基础模型来编码相机图像，显著提升特征表示质量。为了进一步丰富模型对环境的理解并提高预测质量，MapFM集成了用于BEV表示中语义分割的辅助预测头，采用多任务学习方法提供更丰富的上下文监督。", "result": "MapFM实现了更全面的场景表示，并最终获得了更高精度和更高质量的预测矢量化高精地图。", "conclusion": "通过结合强大的基础模型进行图像编码和集成多任务学习（特别是BEV语义分割），MapFM能够有效提升在线矢量化高清地图的生成质量和准确性，为自动驾驶提供更可靠的环境感知。", "translation": "在自动驾驶中，高精地图（HD map）和鸟瞰图（BEV）中的语义地图对于精确的定位、规划和决策至关重要。本文介绍了一种名为MapFM的增强型端到端模型，用于在线生成矢量化高精地图。我们展示了通过整合强大的基础模型来编码相机图像，可以显著提升特征表示质量。为了进一步丰富模型对环境的理解并提高预测质量，我们集成了用于BEV表示中语义分割的辅助预测头。这种多任务学习方法提供了更丰富的上下文监督，从而实现了更全面的场景表示，并最终带来了更高精度和更高质量的预测矢量化高精地图。源代码可在https://github.com/LIvanoff/MapFM获取。", "summary": "MapFM是一个为自动驾驶设计的端到端模型，专注于在线生成高质量的矢量化高清地图。它通过整合强大的基础模型来增强图像特征表示，并利用多任务学习（特别是BEV语义分割）来提供丰富的上下文信息，从而显著提高了地图生成的准确性和场景理解能力。", "keywords": "高清地图, 基础模型, 多任务学习, 自动驾驶, 矢量化地图", "comments": "MapFM的创新之处在于将强大的基础模型引入高清地图生成，并结合多任务学习以利用丰富的上下文信息，这对于提升自动驾驶中地图的质量和准确性具有重要意义。其端到端和在线的特性也符合实际应用需求，具有潜在的应用价值。"}}
{"id": "2506.15583", "title": "DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement", "authors": ["Shaoqing Lin", "Chong Teng", "Fei Li", "Donghong Ji", "Lizhen Qu", "Zhuang Li"], "summary": "Vision-Language Models (VLMs) now generate discourse-level, multi-sentence\nvisual descriptions, challenging text scene graph parsers originally designed\nfor single-sentence caption-to-graph mapping. Current approaches typically\nmerge sentence-level parsing outputs for discourse input, often missing\nphenomena like cross-sentence coreference, resulting in fragmented graphs and\ndegraded downstream VLM task performance. To address this, we introduce a new\ntask, Discourse-level text Scene Graph parsing (DiscoSG), supported by our\ndataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised\nmulti-sentence caption-graph pairs for images. Each caption averages 9\nsentences, and each graph contains at least 3 times more triples than those in\nexisting datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS\nimproves SPICE by approximately 48% over the best sentence-merging baseline,\nhigh inference cost and restrictive licensing hinder its open-source use, and\nsmaller fine-tuned PLMs struggle with complex graphs. We propose\nDiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a\nsecond PLM to iteratively propose graph edits, reducing full-graph generation\noverhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE\nby approximately 30% over the best baseline while achieving 86 times faster\ninference than GPT-4. It also consistently improves downstream VLM tasks like\ndiscourse-level caption evaluation and hallucination detection. Code and data\nare available at: https://github.com/ShaoqLin/DiscoSG", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15583v1", "AI": {"title_translation": "DiscoSG：通过迭代图细化实现篇章级文本场景图解析", "tldr": "该论文引入了DiscoSG，一个用于篇章级文本场景图解析的新任务和数据集，并提出了DiscoSG-Refiner模型，该模型通过迭代细化提高了解析性能和效率，并改善了下游视觉语言模型任务。", "motivation": "当前的文本场景图解析器主要为单句输入设计，当处理视觉语言模型生成的篇章级多句描述时，简单地合并句级解析结果会忽略跨句指代等现象，导致图碎片化并降低下游视觉语言模型任务的性能。", "method": "本文提出了一个新的任务——篇章级文本场景图解析（DiscoSG），并构建了数据集DiscoSG-DS，其中包含400对专家标注和8,430对合成的多句图像描述-图对。为了解决大型预训练语言模型（PLMs）的高推理成本和授权限制，以及小型PLMs难以处理复杂图的问题，作者提出了DiscoSG-Refiner模型。该模型首先使用一个小型PLM草拟一个基础图，然后利用第二个PLM迭代地提出图编辑，从而减少了生成完整图的开销。", "result": "在DiscoSG-DS数据集上微调大型PLMs（如GPT-4）相比最佳的句子合并基线，SPICE指标提升了约48%。DiscoSG-Refiner（使用两个Flan-T5-Base模型）相比最佳基线仍能将SPICE指标提升约30%，同时推理速度比GPT-4快86倍。此外，DiscoSG-Refiner持续改善了下游视觉语言模型任务，如篇章级描述评估和幻觉检测。", "conclusion": "DiscoSG-Refiner有效解决了篇章级文本场景图解析的挑战，通过迭代图细化机制，实现了性能的显著提升和推理效率的大幅优化，并改善了下游视觉语言模型任务的表现。", "translation": "视觉语言模型（VLMs）现在能够生成篇章级、多句的视觉描述，这对最初为单句描述到图映射设计的文本场景图解析器提出了挑战。当前的方法通常将句级解析输出合并用于篇章输入，但常常忽略跨句指代等现象，导致图碎片化并降低下游VLM任务的性能。为了解决这个问题，我们引入了一项新任务——篇章级文本场景图解析（DiscoSG），并由我们的数据集DiscoSG-DS提供支持，该数据集包含400对专家标注和8,430对合成的多句图像描述-图对。每个描述平均包含9个句子，每个图包含的谓词三元组数量至少是现有数据集的三倍。虽然在DiscoSG-DS上微调大型PLM（即GPT-4）比最佳的句子合并基线将SPICE指标提高了约48%，但高昂的推理成本和限制性许可阻碍了其开源使用，而较小的微调PLM难以处理复杂的图。我们提出了DiscoSG-Refiner，它使用一个小型PLM草拟一个基础图，然后利用第二个PLM迭代地提出图编辑，从而减少了生成完整图的开销。使用两个Flan-T5-Base模型，DiscoSG-Refiner相比最佳基线仍能将SPICE指标提高约30%，同时推理速度比GPT-4快86倍。它还持续改善了下游VLM任务，如篇章级描述评估和幻觉检测。代码和数据可在https://github.com/ShaoqLin/DiscoSG 获取。", "summary": "该论文提出了DiscoSG，一个针对视觉语言模型生成的多句描述进行篇章级文本场景图解析的新任务。为支持此任务，作者构建了大型数据集DiscoSG-DS。针对现有方法在处理多句输入时图碎片化的问题，论文引入了DiscoSG-Refiner模型。该模型采用两阶段迭代细化策略，利用小型预训练语言模型高效地生成和优化场景图。实验结果表明，DiscoSG-Refiner在保持高效率的同时，显著提升了场景图解析的性能，并对下游视觉语言模型任务产生了积极影响。", "keywords": "篇章级, 文本场景图, 迭代图细化, 视觉语言模型, 自然语言处理", "comments": "该论文的创新点在于提出了篇章级文本场景图解析这一新任务，并构建了大规模的DiscoSG-DS数据集，填补了现有数据集的空白。DiscoSG-Refiner模型通过引入迭代图细化机制，巧妙地解决了大型模型成本高昂和小型模型性能不足的矛盾，使得小型PLM也能高效地处理复杂的篇章级场景图。其显著的性能提升和推理效率优化，以及对下游VLM任务的积极影响，都凸显了这项工作的实用性和重要性。"}}
{"id": "2506.15318", "title": "OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models", "authors": ["Lanfeng Zhong", "Xin Liao", "Shichuan Zhang", "Shaoting Zhang", "Guotai Wang"], "summary": "Pathology image classification plays a crucial role in accurate medical\ndiagnosis and treatment planning. Training high-performance models for this\ntask typically requires large-scale annotated datasets, which are both\nexpensive and time-consuming to acquire. Active Learning (AL) offers a solution\nby iteratively selecting the most informative samples for annotation, thereby\nreducing the labeling effort. However, most AL methods are designed under the\nassumption of a closed-set scenario, where all the unannotated images belong to\ntarget classes. In real-world clinical environments, the unlabeled pool often\ncontains a substantial amount of Out-Of-Distribution (OOD) data, leading to low\nefficiency of annotation in traditional AL methods. Furthermore, most existing\nAL methods start with random selection in the first query round, leading to a\nsignificant waste of labeling costs in open-set scenarios. To address these\nchallenges, we propose OpenPath, a novel open-set active learning approach for\npathological image classification leveraging a pre-trained Vision-Language\nModel (VLM). In the first query, we propose task-specific prompts that combine\ntarget and relevant non-target class prompts to effectively select\nIn-Distribution (ID) and informative samples from the unlabeled pool. In\nsubsequent queries, Diverse Informative ID Sampling (DIS) that includes\nPrototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic\nSampling (EGSS) is proposed to ensure both purity and informativeness in a\nquery, avoiding the selection of OOD samples. Experiments on two public\npathology image datasets show that OpenPath significantly enhances the model's\nperformance due to its high purity of selected samples, and outperforms several\nstate-of-the-art open-set AL methods. The code is available at\n\\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..", "comment": "MICCAI 2025 early accept", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15318v1", "AI": {"title_translation": "OpenPath：基于预训练视觉-语言模型的病理图像分类开放集主动学习", "tldr": "OpenPath是一种新的开放集主动学习方法，利用预训练视觉-语言模型，通过改进的初始查询和后续的ID样本选择，显著提高了病理图像分类的标注效率和模型性能。", "motivation": "病理图像分类需要大量标注数据，获取成本高昂且耗时。传统主动学习方法在开放集场景下效率低下，因其假设所有未标注图像都属于目标类别，且初始查询是随机的，导致标注成本浪费。", "method": "提出OpenPath，一种用于病理图像分类的开放集主动学习方法，利用预训练视觉-语言模型（VLM）。在首次查询中，提出结合目标和相关非目标类别提示的任务特定提示，以有效选择分布内（ID）和信息丰富的样本。在后续查询中，提出多样化信息ID采样（DIS），包括基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保查询样本的纯度和信息量，避免选择分布外（OOD）样本。", "result": "在两个公共病理图像数据集上的实验表明，OpenPath显著提升了模型的性能，因为它选择的样本纯度高。OpenPath优于几种最先进的开放集主动学习方法。", "conclusion": "OpenPath通过创新性地利用预训练VLM并改进开放集主动学习流程，有效解决了病理图像分类中数据标注成本高和OOD样本干扰的问题，从而提高了模型性能和标注效率。", "translation": "病理图像分类在准确的医学诊断和治疗规划中起着关键作用。为这项任务训练高性能模型通常需要大规模的标注数据集，这既昂贵又耗时。主动学习（AL）通过迭代选择最具信息量的样本进行标注，从而减少了标注工作量，提供了一种解决方案。然而，大多数AL方法是在闭集场景下设计的，即所有未标注图像都属于目标类别。在真实的临床环境中，未标注池通常包含大量分布外（OOD）数据，导致传统AL方法的标注效率低下。此外，大多数现有的AL方法在首次查询轮中从随机选择开始，导致在开放集场景下标注成本的显著浪费。为了解决这些挑战，我们提出了OpenPath，一种新颖的开放集主动学习方法，用于病理图像分类，它利用了预训练的视觉-语言模型（VLM）。在首次查询中，我们提出了结合目标和相关非目标类别提示的任务特定提示，以有效地从未标注池中选择分布内（ID）和信息丰富的样本。在后续查询中，提出了多样化信息ID采样（DIS），包括基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保查询的纯度和信息量，避免选择OOD样本。在两个公共病理图像数据集上的实验表明，OpenPath由于其选择样本的高纯度而显著增强了模型的性能，并且优于几种最先进的开放集AL方法。代码可在https://github.com/HiLab-git/OpenPath获取。", "summary": "本论文提出OpenPath，一种用于病理图像分类的开放集主动学习新方法，旨在解决传统主动学习在实际临床环境中面临的标注成本高和存在大量分布外（OOD）数据的问题。OpenPath利用预训练的视觉-语言模型（VLM），在首次查询中采用任务特定提示以有效选择分布内（ID）和信息丰富的样本，并在后续查询中引入多样化信息ID采样（DIS），结合原型选择和熵引导采样，确保所选样本的纯度和信息量，避免OOD样本。实验结果表明，OpenPath显著提升了模型性能，并优于现有最先进的开放集主动学习方法。", "keywords": "主动学习, 开放集, 病理图像分类, 视觉-语言模型, OOD检测", "comments": "OpenPath的创新点在于将预训练视觉-语言模型引入开放集主动学习，并设计了针对性的初始查询和后续采样策略来处理OOD样本，这对于减少病理图像标注成本和提高模型在真实世界场景中的鲁棒性具有重要意义。"}}
{"id": "2506.15079", "title": "Neural Canonical Polyadic Factorization for Traffic Analysis", "authors": ["Yikai Hou", "Peng Tang"], "summary": "Modern intelligent transportation systems rely on accurate spatiotemporal\ntraffic analysis to optimize urban mobility and infrastructure resilience.\nHowever, pervasive missing data caused by sensor failures and heterogeneous\nsensing gaps fundamentally hinders reliable traffic modeling. This paper\nproposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes\nlow-rank tensor algebra with deep representation learning for robust traffic\ndata imputation. The model innovatively embeds CP decomposition into neural\narchitecture through learnable embedding projections, where sparse traffic\ntensors are encoded into dense latent factors across road segments, time\nintervals, and mobility metrics. A hierarchical feature fusion mechanism\nemploys Hadamard products to explicitly model multilinear interactions, while\nstacked multilayer perceptron layers nonlinearly refine these representations\nto capture complex spatiotemporal couplings. Extensive evaluations on six urban\ntraffic datasets demonstrate NCPF's superiority over six state-of-the-art\nbaselines. By unifying CP decomposition's interpretable factor analysis with\nneural network's nonlinear expressive power, NCPF provides a principled yet\nflexible approaches for high-dimensional traffic data imputation, offering\ncritical support for next-generation transportation digital twins and adaptive\ntraffic control systems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15079v1", "AI": {"title_translation": "用于交通分析的神经正则多项式分解", "tldr": "NCPF模型结合张量代数和深度学习，有效处理交通数据缺失问题，并在多个数据集上表现优于现有方法。", "motivation": "现代智能交通系统依赖准确的时空交通分析，但传感器故障和异构感知间隙导致的数据缺失严重阻碍了可靠的交通建模。", "method": "本文提出了神经正则多项式分解（NCPF）模型，该模型将CP分解嵌入到神经网络架构中，通过可学习的嵌入投影将稀疏交通张量编码为跨路段、时间间隔和移动性指标的密集潜在因子。模型采用层次特征融合机制（Hadamard积）显式建模多线性交互，并通过堆叠的多层感知器层非线性地细化这些表示，以捕获复杂的时空耦合。", "result": "在六个城市交通数据集上的广泛评估表明，NCPF模型优于六个最先进的基线方法。", "conclusion": "NCPF通过统一CP分解的可解释因子分析与神经网络的非线性表达能力，为高维交通数据插补提供了一种既有原则又灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了关键支持。", "translation": "现代智能交通系统依赖于准确的时空交通分析来优化城市交通和基础设施弹性。然而，传感器故障和异构感知间隙造成的普遍数据缺失从根本上阻碍了可靠的交通建模。本文提出了一种神经正则多项式分解（NCPF）模型，该模型将低秩张量代数与深度表示学习相结合，以实现鲁棒的交通数据插补。该模型创新性地将CP分解通过可学习的嵌入投影嵌入到神经网络架构中，将稀疏交通张量编码为跨路段、时间间隔和移动性指标的密集潜在因子。层次特征融合机制采用Hadamard积显式建模多线性交互，而堆叠的多层感知器层非线性地细化这些表示，以捕获复杂的时空耦合。对六个城市交通数据集的广泛评估表明，NCPF优于六个最先进的基线方法。通过统一CP分解的可解释因子分析与神经网络的非线性表达能力，NCPF为高维交通数据插补提供了一种既有原则又灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了关键支持。", "summary": "本文提出了一种名为神经正则多项式分解（NCPF）的新模型，用于解决智能交通系统中普遍存在的交通数据缺失问题。NCPF模型创新性地将低秩张量代数与深度学习相结合，通过将CP分解嵌入神经网络架构，将稀疏交通张量编码为密集潜在因子。它利用层次特征融合和多层感知器层来捕获复杂的时空耦合。实验结果表明，NCPF在六个城市交通数据集上的表现优于多种现有方法，为高维交通数据插补提供了一种有效且灵活的解决方案，支持未来的交通系统。", "keywords": "交通分析, 神经正则多项式分解, 数据插补, 张量分解, 深度学习", "comments": "该论文的创新点在于将可解释的CP分解与神经网络的非线性表达能力相结合，为交通数据插补提供了一种新颖且有效的范式。这种结合不仅提高了数据插补的准确性，也为理解交通数据的潜在结构提供了可能。其重要性在于解决了智能交通系统中的关键挑战——数据缺失，为未来交通数字孪生和自适应控制系统奠定了基础。"}}
{"id": "2506.15594", "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts", "authors": ["Negar Foroutan", "Angelika Romanou", "Matin Ansaripour", "Julian Martin Eisenschlos", "Karl Aberer", "Rémi Lebret"], "summary": "Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.", "comment": "ACL 2025 (Findings)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15594v1", "AI": {"title_translation": "WikiMixQA：一个用于表格和图表问答的多模态基准", "tldr": "WikiMixQA是一个新的多模态基准测试，用于评估大型视觉语言模型在处理包含表格和图表的复杂文档时的长文本理解和跨模态推理能力。研究发现，现有模型在需要从长文档中检索信息时表现不佳，凸显了该领域的挑战。", "motivation": "现有视觉语言大模型（VLLMs）在处理长上下文视觉输入时的有效性尚不明确，尤其是在包含复杂布局、表格和图表的文档理解方面面临显著挑战。", "method": "本文引入了WikiMixQA，这是一个包含1,000个多项选择题（MCQs）的基准测试，旨在评估模型对从4,000个维基百科页面中提取的表格和图表的跨模态推理能力。该基准强调复杂推理，要求模型综合来自多种模态的信息。研究评估了12个最先进的视觉语言模型。", "result": "评估结果显示，专有模型在提供直接上下文时能达到约70%的准确率，但在需要从长文档中检索信息时，性能显著下降。其中，GPT-4o是唯一在此设置下准确率超过50%的模型，而开源模型的表现则差得多，最高准确率仅为27%。", "conclusion": "这些发现强调了长上下文、多模态推理的挑战，并确立了WikiMixQA作为推动文档理解研究的关键基准。", "translation": "文档是保存和传播信息的基础，通常包含复杂的布局、表格和图表，这对自动文档理解（DU）构成了重大挑战。虽然视觉语言大模型（VLLMs）在各种任务中都表现出改进，但它们在处理长上下文视觉输入方面的有效性仍不清楚。本文介绍了WikiMixQA，这是一个包含1,000个多项选择题（MCQs）的基准测试，旨在评估从4,000个维基百科页面中提取的表格和图表的跨模态推理能力，这些页面涵盖七个不同的主题。与现有基准不同，WikiMixQA通过要求模型综合来自多种模态的信息来强调复杂推理。我们评估了12个最先进的视觉语言模型，结果显示，虽然专有模型在提供直接上下文时能达到约70%的准确率，但当需要从长文档中检索信息时，它们的性能会显著下降。其中，GPT-4o是唯一在此设置下准确率超过50%的模型，而开源模型的表现则差得多，最高准确率仅为27%。这些发现强调了长上下文、多模态推理的挑战，并确立了WikiMixQA作为推动文档理解研究的关键基准。", "summary": "本研究提出WikiMixQA，一个用于评估视觉语言模型在复杂文档理解中跨模态推理能力的新型多模态基准。该基准包含1000个多项选择题，要求模型综合来自维基百科表格和图表的信息。对12个最先进模型的评估显示，在长上下文检索场景下，即使是专有模型性能也大幅下降，凸显了当前模型在处理长篇幅、多模态信息方面的局限性，并强调了WikiMixQA在推动文档理解研究中的重要性。", "keywords": "多模态基准, 问答系统, 文档理解, 表格, 图表", "comments": "WikiMixQA的创新之处在于其专注于长上下文、跨模态的复杂推理，特别是结合了表格和图表信息，这在现有基准中是相对稀缺的。它揭示了当前VLLMs在处理真实世界复杂文档时的显著瓶颈，尤其是对长文档信息检索能力的不足。该基准的提出对于推动文档理解和多模态AI领域的发展具有重要意义。"}}
{"id": "2506.15368", "title": "Open-World Object Counting in Videos", "authors": ["Niki Amini-Naieni", "Andrew Zisserman"], "summary": "We introduce a new task of open-world object counting in videos: given a text\ndescription, or an image example, that specifies the target object, the\nobjective is to enumerate all the unique instances of the target objects in the\nvideo. This task is especially challenging in crowded scenes with occlusions\nand similar objects, where avoiding double counting and identifying\nreappearances is crucial. To this end, we make the following contributions: we\nintroduce a model, CountVid, for this task. It leverages an image-based\ncounting model, and a promptable video segmentation and tracking model to\nenable automated, open-world object counting across video frames. To evaluate\nits performance, we introduce VideoCount, a new dataset for our novel task\nbuilt from the TAO and MOT20 tracking datasets, as well as from videos of\npenguins and metal alloy crystallization captured by x-rays. Using this\ndataset, we demonstrate that CountVid provides accurate object counts, and\nsignificantly outperforms strong baselines. The VideoCount dataset, the\nCountVid model, and all the code are available at\nhttps://github.com/niki-amini-naieni/CountVid/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15368v1", "AI": {"title_translation": "视频开放世界目标计数", "tldr": "论文提出了一个新的视频开放世界目标计数任务和模型CountVid，以及数据集VideoCount，用于准确识别和计数视频中给定文本或图像描述的目标对象。", "motivation": "现有方法难以应对视频中拥挤场景、遮挡和相似对象导致的重复计数和识别再出现的问题，因此需要一种新的开放世界目标计数任务来准确枚举视频中所有唯一的目标实例。", "method": "提出模型CountVid，它结合了基于图像的计数模型和可提示的视频分割与跟踪模型，以实现在视频帧中自动、开放世界的目标计数。同时，引入了新数据集VideoCount，该数据集基于TAO和MOT20跟踪数据集以及企鹅和金属合金结晶的X射线视频构建。", "result": "CountVid在VideoCount数据集上提供了准确的目标计数，并且显著优于强大的基线模型。", "conclusion": "论文成功引入了视频开放世界目标计数的新任务，并提出了有效的模型CountVid和评估数据集VideoCount，证明了其在复杂视频场景中准确计数目标的能力。", "translation": "我们引入了一项新的视频开放世界目标计数任务：给定文本描述或图像示例来指定目标对象，目标是枚举视频中所有唯一的目标实例。这项任务在拥挤、有遮挡和相似对象的场景中尤其具有挑战性，其中避免重复计数和识别再次出现的目标至关重要。为此，我们做出了以下贡献：我们为这项任务引入了一个模型CountVid。它利用基于图像的计数模型和可提示的视频分割与跟踪模型，以实现在视频帧中自动、开放世界的目标计数。为了评估其性能，我们引入了VideoCount，这是一个为我们新任务构建的新数据集，它来源于TAO和MOT20跟踪数据集，以及通过X射线捕获的企鹅和金属合金结晶视频。使用这个数据集，我们证明了CountVid提供了准确的目标计数，并且显著优于强大的基线模型。VideoCount数据集、CountVid模型以及所有代码均可在https://github.com/niki-amini-naieni/CountVid/ 获取。", "summary": "本文提出了一项名为“视频开放世界目标计数”的新任务，旨在根据文本描述或图像示例在视频中枚举所有唯一的目标实例，解决了拥挤、遮挡和相似对象带来的挑战。为此，作者开发了CountVid模型，它结合了图像计数和视频分割跟踪技术，并构建了VideoCount数据集进行评估。实验结果表明，CountVid在目标计数方面表现准确，并显著优于现有基线方法。", "keywords": "视频目标计数, 开放世界, CountVid, VideoCount, 目标跟踪", "comments": "这篇论文通过引入“开放世界目标计数”这一新颖且具有挑战性的任务，填补了现有目标计数研究的空白。其创新之处在于结合了图像计数和可提示的视频分割与跟踪模型，使得模型能够灵活地根据文本或图像提示进行计数。同时，构建专门的VideoCount数据集对于推动该领域的研究具有重要意义。该工作有望在智能监控、行为分析等领域有广泛应用。"}}
{"id": "2506.15115", "title": "Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions", "authors": ["Lulu Xue", "Shengshan Hu", "Wei Lu", "Yan Shen", "Dongxu Li", "Peijin Guo", "Ziqi Zhou", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "summary": "With growing demands for privacy protection, security, and legal compliance\n(e.g., GDPR), machine unlearning has emerged as a critical technique for\nensuring the controllability and regulatory alignment of machine learning\nmodels. However, a fundamental challenge in this field lies in effectively\nverifying whether unlearning operations have been successfully and thoroughly\nexecuted. Despite a growing body of work on unlearning techniques, verification\nmethodologies remain comparatively underexplored and often fragmented. Existing\napproaches lack a unified taxonomy and a systematic framework for evaluation.\nTo bridge this gap, this paper presents the first structured survey of machine\nunlearning verification methods. We propose a taxonomy that organizes current\ntechniques into two principal categories -- behavioral verification and\nparametric verification -- based on the type of evidence used to assess\nunlearning fidelity. We examine representative methods within each category,\nanalyze their underlying assumptions, strengths, and limitations, and identify\npotential vulnerabilities in practical deployment. In closing, we articulate a\nset of open problems in current verification research, aiming to provide a\nfoundation for developing more robust, efficient, and theoretically grounded\nunlearning verification mechanisms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15115v1", "AI": {"title_translation": "迈向可靠遗忘：机器学习反学习验证、挑战与未来方向综述", "tldr": "本论文首次对机器学习反学习验证方法进行了结构化综述，提出了一个统一的分类法，并分析了现有方法的优缺点，指出了开放问题和未来研究方向。", "motivation": "随着隐私保护、安全和法律合规性（如GDPR）需求的增长，机器学习反学习已成为确保模型可控性和符合法规的关键技术。然而，该领域的一个根本挑战在于有效验证反学习操作是否成功且彻底执行。尽管反学习技术的研究越来越多，但验证方法相对未被充分探索且分散，缺乏统一的分类法和系统评估框架。", "method": "本文首次对机器学习反学习验证方法进行了结构化综述。作者提出了一个分类法，将现有技术分为行为验证和参数验证两大类。论文审查了每类中的代表性方法，分析了其基本假设、优点和局限性，并识别了实际部署中潜在的漏洞。", "result": "本文提出了第一个结构化的机器学习反学习验证方法综述，并提出了一个将现有技术分为行为验证和参数验证的分类法。论文分析了每类中代表性方法的假设、优缺点和潜在漏洞。此外，论文还阐明了当前验证研究中的一系列开放问题。", "conclusion": "本文旨在为开发更鲁棒、高效和理论基础更强的反学习验证机制提供基础，通过阐明当前验证研究中的一系列开放问题。", "translation": "随着隐私保护、安全和法律合规性（例如GDPR）需求的增长，机器学习反学习已成为确保机器学习模型可控性和符合法规的关键技术。然而，该领域的一个根本挑战在于有效验证反学习操作是否成功且彻底执行。尽管反学习技术的研究越来越多，但验证方法相对未被充分探索且常常是分散的。现有方法缺乏统一的分类法和系统评估框架。为了弥补这一空白，本文首次对机器学习反学习验证方法进行了结构化综述。我们提出了一个分类法，根据用于评估反学习保真度的证据类型，将现有技术分为两个主要类别——行为验证和参数验证。我们审查了每个类别中的代表性方法，分析了它们的基本假设、优点和局限性，并识别了实际部署中潜在的漏洞。最后，我们阐明了当前验证研究中的一系列开放问题，旨在为开发更鲁棒、高效和理论基础更强的反学习验证机制提供基础。", "summary": "本论文首次对机器学习反学习验证方法进行了结构化综述，旨在解决该领域验证方法碎片化和缺乏统一框架的挑战。论文提出了一个新颖的分类法，将验证技术分为行为验证和参数验证两大类，并对各类中的代表性方法进行了深入分析，探讨了其优缺点和潜在风险。此外，论文还指出了当前研究中的开放问题，为未来开发更可靠的反学习验证机制奠定了基础。", "keywords": "机器学习反学习, 验证, 综述, 隐私保护, 分类法", "comments": "这篇综述论文填补了机器学习反学习领域中验证方法研究的空白，提供了一个急需的统一分类法和系统分析。其贡献在于不仅梳理了现有技术，还深入剖析了其局限性，并指明了未来研究方向，对于推动反学习的可靠性和实际应用具有重要意义。"}}
{"id": "2506.15598", "title": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso", "Pedro Pinto", "Abel Ferreira", "Luís Abreu", "Isabel Rangel", "Sandra Monteiro"], "summary": "While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.", "comment": "This is a preprint version of the manuscript currently under review\n  at an international journal", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15598v1", "AI": {"title_translation": "从模型到课堂：评估葡萄牙语生成式多项选择题的叙事性和难度", "tldr": "本研究评估了生成模型为葡萄牙语阅读理解生成多项选择题（MCQs）的能力，发现其质量可与人工创作的媲美，但仍存在语义清晰度、可答性及干扰项设计方面的挑战。", "motivation": "多项选择题（MCQs）对于学习和评估很有价值，但手动创建不同难度和针对特定阅读技能的MCQs耗时且成本高昂。尽管生成式AI为MCQ自动化生成提供了机会，但对其质量和可靠性的评估，尤其是在生成失败的情况下，受到的关注有限，这在实际应用中尤为重要。此外，大多数MCQ生成研究集中于英语，其他语言（如葡萄牙语）尚未得到充分探索。", "method": "本研究调查了当前生成模型为形态丰富的葡萄牙语阅读理解生成MCQs的能力。研究重点是生成符合课程相关叙事元素并涵盖不同难度级别的MCQs。通过专家评审和分析学生回答中提取的心理测量特性来评估这些MCQs，以判断其对小学生是否适用。", "result": "结果显示，当前模型生成的MCQs质量可与人工创作的媲美。然而，研究也发现了一些与语义清晰度和可答性相关的问题。此外，在生成能够吸引学生并符合高质量MCQ选项设计既定标准的干扰项方面仍然存在挑战。", "conclusion": "尽管当前生成模型能够为葡萄牙语生成质量与人工创作相当的多项选择题，但在语义清晰度、可答性以及高质量干扰项的生成方面仍需进一步改进，以满足实际教学应用的需求。", "translation": "尽管多项选择题（MCQs）对于学习和评估很有价值，但手动创建不同难度级别和针对特定阅读技能的MCQs仍然是一项耗时且成本高昂的任务。生成式AI的最新进展为高效自动化MCQ生成提供了机会。然而，对生成MCQs的实际质量和可靠性的评估受到的关注有限——特别是在生成失败的情况下。当生成的MCQs旨在应用于真实世界环境时，这方面变得尤为重要。此外，大多数MCQ生成研究都集中于英语，而其他语言则未得到充分探索。本文研究了当前生成模型为形态丰富的葡萄牙语阅读理解生成MCQs的能力。我们的研究重点是生成符合课程相关叙事元素并涵盖不同难度级别的MCQs。我们通过专家评审和分析学生回答中提取的心理测量特性来评估这些MCQs，以评估它们对小学生的适用性。我们的结果表明，当前模型可以生成与人工创作的MCQs质量相当的MCQs。然而，我们发现了一些与语义清晰度和可答性相关的问题。此外，在生成能够吸引学生并符合高质量MCQ选项设计既定标准的干扰项方面仍然存在挑战。", "summary": "本研究旨在评估生成模型为葡萄牙语阅读理解生成多项选择题（MCQs）的性能，重点关注叙事相关性和难度分级。研究通过专家评审和学生心理测量数据评估了生成MCQs的质量。结果表明，AI生成的MCQs在质量上可与人类创作的媲美，但仍面临语义清晰度、可答性以及有效干扰项设计方面的挑战。", "keywords": "多项选择题生成, 葡萄牙语, 生成式AI, 阅读理解, 质量评估", "comments": "本论文创新性地将生成式AI应用于非英语语种（葡萄牙语）的MCQ生成，并深入探讨了实际应用中的质量评估，包括专家评审和心理测量分析。其重要性在于揭示了AI在教育内容生成方面的潜力，同时也明确指出了当前模型在语义精准度和干扰项设计上的局限性，为未来研究提供了明确的方向。"}}
{"id": "2506.15369", "title": "Unsupervised Pelage Pattern Unwrapping for Animal Re-identification", "authors": ["Aleksandr Algasov", "Ekaterina Nepovinnykh", "Fedor Zolotarev", "Tuomas Eerola", "Heikki Kälviäinen", "Pavel Zemčík", "Charles V. Stewart"], "summary": "Existing individual re-identification methods often struggle with the\ndeformable nature of animal fur or skin patterns which undergo geometric\ndistortions due to body movement and posture changes. In this paper, we propose\na geometry-aware texture mapping approach that unwarps pelage patterns, the\nunique markings found on an animal's skin or fur, into a canonical UV space,\nenabling more robust feature matching. Our method uses surface normal\nestimation to guide the unwrapping process while preserving the geometric\nconsistency between the 3D surface and the 2D texture space. We focus on two\nchallenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards\n(Panthera pardus). Both species have distinctive yet highly deformable fur\npatterns. By integrating our pattern-preserving UV mapping with existing\nre-identification techniques, we demonstrate improved accuracy across diverse\nposes and viewing angles. Our framework does not require ground truth UV\nannotations and can be trained in a self-supervised manner. Experiments on seal\nand leopard datasets show up to a 5.4% improvement in re-identification\naccuracy.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15369v1", "AI": {"title_translation": "动物再识别中的无监督皮毛图案展开", "tldr": "本文提出一种无监督的几何感知纹理映射方法，将动物皮毛图案展开到标准UV空间，以解决动物再识别中图案变形的问题，并在海豹和豹子的数据集上实现了再识别准确率的提升。", "motivation": "现有的个体再识别方法难以处理动物皮毛或皮肤图案的变形性质，这些图案会因身体运动和姿势变化而发生几何畸变，导致特征匹配不鲁棒。", "method": "本文提出一种几何感知的纹理映射方法，将动物特有的皮毛图案展开到规范的UV空间，从而实现更鲁棒的特征匹配。该方法利用表面法线估计来指导展开过程，同时保持3D表面和2D纹理空间之间的几何一致性。该框架不需要真值UV标注，可以以自监督方式训练。", "result": "通过将提出的图案保留UV映射与现有再识别技术相结合，该方法在不同姿势和视角下均表现出更高的准确性。在海豹和豹子数据集上的实验显示，再识别准确率提升高达5.4%。", "conclusion": "本文提出的无监督皮毛图案展开方法有效解决了动物再识别中图案变形的挑战，通过将图案映射到规范UV空间，显著提高了在多种姿势和视角下的再识别准确性，为动物再识别提供了一种鲁棒且无需标注的新范式。", "translation": "现有的大多数个体再识别方法在处理动物皮毛或皮肤图案的可变形性时往往会遇到困难，这些图案会因身体运动和姿势变化而发生几何畸变。在本文中，我们提出了一种几何感知的纹理映射方法，该方法将动物皮肤或皮毛上独特的皮毛图案展开到规范的UV空间中，从而实现更鲁棒的特征匹配。我们的方法利用表面法线估计来指导展开过程，同时保持3D表面和2D纹理空间之间的几何一致性。我们专注于两种具有挑战性的物种：波罗的海环斑海豹（Pusa hispida saimensis）和豹子（Panthera pardus）。这两种物种都具有独特但高度可变形的毛皮图案。通过将我们提出的图案保留UV映射与现有再识别技术相结合，我们证明了在不同姿势和视角下准确性有所提高。我们的框架不需要真值UV标注，并且可以以自监督方式进行训练。在海豹和豹子数据集上的实验显示，再识别准确率提升高达5.4%。", "summary": "本文提出一种无监督的几何感知纹理映射方法，用于动物再识别。该方法通过将动物皮毛图案展开到规范的UV空间，解决了现有方法在处理动物身体运动和姿势变化导致的图案变形问题。通过表面法线估计引导展开过程并保持几何一致性，该方法与现有再识别技术结合，在海豹和豹子数据集上将再识别准确率提高了高达5.4%，且无需真值UV标注，可自监督训练。", "keywords": "动物再识别, 皮毛图案展开, 无监督学习, 几何感知, UV映射", "comments": "这篇论文的创新点在于提出了一个无监督的几何感知纹理映射方法，巧妙地将动物可变形的皮毛图案“展开”到一个标准化的2D空间，从而极大地简化了后续的特征匹配任务。其重要性体现在解决了动物再识别领域中一个核心的挑战——图案变形问题。该方法的自监督训练特性降低了对大量标注数据的依赖，使其更具实用性。"}}
{"id": "2506.15181", "title": "ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed Machine Learning", "authors": ["Bing Liu", "Chengcheng Zhao", "Li Chai", "Peng Cheng", "Yaonan Wang"], "summary": "Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15181v1", "AI": {"title_translation": "ImprovDML：改进的私有拜占庭容错分布式机器学习中的权衡", "tldr": "ImprovDML提出了一种新的去中心化分布式机器学习框架，在保证隐私和拜占庭容错的同时，显著提高了模型精度，并证明了其收敛性和更好的隐私-精度权衡。", "motivation": "解决分布式机器学习中拜占庭攻击和隐私泄露的联合问题，以及现有方法在整合拜占庭容错和差分隐私机制时导致模型精度显著下降的问题。", "method": "提出ImprovDML框架，该框架利用一种弹性向量共识算法在每次迭代中计算正常（非拜占庭）代理的凸包内的点进行弹性聚合；然后引入多元高斯噪声到梯度中以实现隐私保护；采用集中式地理隐私概念进行隐私分析。", "result": "提供了非凸设置下的收敛性保证和渐近学习误差界限，这些界限比现有工作更紧密；在隐私分析方面，与差分隐私相比，实现了隐私保护和模型精度之间更好的权衡；数值模拟验证了理论结果。", "conclusion": "ImprovDML框架在分布式机器学习中有效解决了拜占庭攻击和隐私泄露问题，并在隐私保护和模型精度之间取得了显著改进的权衡。", "translation": "分布式机器学习 (DML) 中联合解决拜占庭攻击和隐私泄露已成为一个重要问题。一种常见的策略是整合拜占庭容错聚合规则与差分隐私机制。然而，这些技术的结合常常导致模型精度显著下降。为了解决这个问题，我们提出了一种去中心化的 DML 框架，名为 ImprovDML，它在确保隐私保护和拜占庭攻击弹性的同时，实现了高模型精度。该框架利用一种弹性向量共识算法，在每次迭代中计算正常（非拜占庭）代理的凸包内的点进行弹性聚合。然后，将多元高斯噪声引入梯度中以实现隐私保护。我们提供了非凸设置下的收敛性保证，并推导了渐近学习误差界限，这些界限比现有工作中报告的更紧密。对于隐私分析，我们采用了集中式地理隐私的概念，该概念根据输入之间的欧几里得距离量化隐私保护。我们证明，与差分隐私相比，它可以在隐私保护和模型精度之间实现更好的权衡。最后，数值模拟验证了我们的理论结果。", "summary": "ImprovDML是一个去中心化的分布式机器学习框架，旨在解决拜占庭攻击和隐私泄露问题，同时避免模型精度下降。它通过结合弹性向量共识算法进行聚合和引入多元高斯噪声进行隐私保护来实现。该框架在非凸设置下提供了更紧密的收敛性保证和误差界限，并在隐私保护和模型精度之间实现了比传统差分隐私更好的权衡。", "keywords": "分布式机器学习, 拜占庭容错, 隐私保护, 集中式地理隐私, 模型精度。", "comments": "ImprovDML的创新点在于提出了一个统一的框架来同时解决分布式机器学习中的拜占庭攻击和隐私泄露问题，并且显著改善了现有方法在隐私-精度权衡上的不足。通过引入弹性向量共识算法和采用集中式地理隐私概念，该工作在理论上提供了更严格的保证，并在实践中展示了更好的性能。"}}
{"id": "2506.15617", "title": "The Compositional Architecture of Regret in Large Language Models", "authors": ["Xiangxiang Cui", "Shu Yang", "Tianjin Huang", "Wanyu Lin", "Lijie Hu", "Di Wang"], "summary": "Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.", "comment": "23 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15617v1", "AI": {"title_translation": "大型语言模型中“后悔”的组合架构", "tldr": "本文提出新方法和指标来识别和分析大型语言模型中的“后悔”表达及其内部神经元机制。", "motivation": "研究大型语言模型中的“后悔”机制对于提升模型可靠性至关重要，同时有助于揭示认知是如何在神经网络中编码的。", "method": "本文提出了一套工作流程来构建综合的后悔数据集，并引入了监督压缩-解耦指数（S-CDI）指标以识别最佳后悔表示层，以及后悔主导分数（RDS）指标和群组影响系数（GIC）以识别和分析后悔神经元及激活模式。", "result": "实验成功使用S-CDI指标识别了最佳后悔表示层，显著提升了探针分类实验的性能。此外，研究发现了跨模型层的M形解耦模式，揭示了信息处理在耦合和解耦阶段之间的交替。通过RDS指标，神经元被分为后悔神经元、非后悔神经元和双重神经元三类。", "conclusion": "本文成功提出了识别和分析大型语言模型中后悔机制的方法和指标，揭示了其内部表示层和神经元的功能分类，为提升模型可靠性和理解认知编码提供了新视角。", "translation": "大型语言模型中的“后悔”是指当模型遇到与其先前生成的错误信息相矛盾的证据时，其明确表达的后悔。研究后悔机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。为了理解这一机制，我们首先需要识别模型输出中的后悔表达，然后分析其内部表示。这种分析需要检查模型隐藏状态，即信息在神经元层面进行处理的地方。然而，这面临三个关键挑战：（1）缺乏捕获后悔表达的专门数据集，（2）缺乏找到最佳后悔表示层的指标，以及（3）缺乏用于识别和分析后悔神经元的指标。为了解决这些局限性，我们提出：（1）通过策略性设计的提示场景构建综合后悔数据集的工作流程，（2）监督压缩-解耦指数（S-CDI）指标以识别最佳后悔表示层，以及（3）后悔主导分数（RDS）指标以识别后悔神经元和群组影响系数（GIC）以分析激活模式。我们的实验结果成功地使用S-CDI指标识别了最佳后悔表示层，这显著提高了探针分类实验的性能。此外，我们发现了跨模型层的M形解耦模式，揭示了信息处理如何在耦合和解耦阶段之间交替。通过RDS指标，我们将神经元分为三个不同的功能组：后悔神经元、非后悔神经元和双重神经元。", "summary": "本文深入研究大型语言模型（LLM）中“后悔”的组成架构，即模型在面对先前错误信息证据时表达后悔的机制。针对现有研究在数据集、最佳表示层及后悔神经元识别分析指标上的不足，作者提出了一套创新方法：包括构建综合后悔数据集的工作流程，引入S-CDI指标以识别最佳后悔表示层，以及利用RDS和GIC指标来识别和分析后悔神经元及其激活模式。实验结果验证了S-CDI在识别最佳后悔表示层上的有效性，并显著提升了探针分类性能；同时揭示了模型层间的M形解耦模式；并通过RDS将神经元功能性地划分为后悔、非后悔和双重三类。这项研究为理解LLM内部认知机制和提升模型可靠性提供了新的工具和见解。", "keywords": "大型语言模型, 后悔机制, 神经元分析, 模型可解释性, S-CDI", "comments": "这项研究通过提出一套系统的方法和创新的指标（S-CDI, RDS, GIC），有效地解决了大型语言模型中“后悔”机制研究的关键挑战。其创新之处在于不仅定义了“后悔”表达，更深入探究了其在模型内部的神经元层面的组成架构和信息处理模式。这对于提升LLM的可靠性、可解释性以及理解其内部认知过程具有重要意义。"}}
{"id": "2506.15381", "title": "When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class", "authors": ["Yujin Kim", "Hyunsoo Kim", "Hyunwoo J. Kim", "Suhyun Kim"], "summary": "Open-source pre-trained models hold great potential for diverse applications,\nbut their utility declines when their training data is unavailable. Data-Free\nImage Synthesis (DFIS) aims to generate images that approximate the learned\ndata distribution of a pre-trained model without accessing the original data.\nHowever, existing DFIS meth ods produce samples that deviate from the training\ndata distribution due to the lack of prior knowl edge about natural images. To\novercome this limitation, we propose DDIS, the first Diffusion-assisted\nData-free Image Synthesis method that leverages a text-to-image diffusion model\nas a powerful image prior, improving synthetic image quality. DDIS extracts\nknowledge about the learned distribution from the given model and uses it to\nguide the diffusion model, enabling the generation of images that accurately\nalign with the training data distribution. To achieve this, we introduce Domain\nAlignment Guidance (DAG) that aligns the synthetic data domain with the\ntraining data domain during the diffusion sampling process. Furthermore, we\noptimize a single Class Alignment Token (CAT) embedding to effectively capture\nclass-specific attributes in the training dataset. Experiments on PACS and Ima\ngeNet demonstrate that DDIS outperforms prior DFIS methods by generating\nsamples that better reflect the training data distribution, achieving SOTA\nperformance in data-free applications.", "comment": "Published at ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15381v1", "AI": {"title_translation": "当模型知识遇到扩散模型：扩散辅助的无数据图像合成，结合域和类对齐", "tldr": "DDIS是一种新的扩散辅助无数据图像合成方法，它利用文本到图像扩散模型作为强大的图像先验，通过域对齐指导（DAG）和类对齐令牌（CAT）来生成与训练数据分布高度对齐的图像，并在PACS和ImageNet上实现了最先进的性能。", "motivation": "现有无数据图像合成（DFIS）方法由于缺乏关于自然图像的先验知识，生成的样本偏离训练数据分布，导致合成图像质量不佳。", "method": "本文提出了DDIS，首个扩散辅助的无数据图像合成方法。DDIS利用文本到图像扩散模型作为强大的图像先验，并从给定模型中提取知识来指导扩散模型。具体地，DDIS引入了域对齐指导（DAG）以在扩散采样过程中对齐合成数据域与训练数据域，并优化了单一的类对齐令牌（CAT）嵌入以捕获类特定属性。", "result": "在PACS和ImageNet上的实验表明，DDIS生成了更能反映训练数据分布的样本，性能优于先前的DFIS方法，并在无数据应用中达到了最先进（SOTA）的性能。", "conclusion": "DDIS通过利用扩散模型作为强大的图像先验，并结合域对齐指导和类对齐令牌，成功克服了现有无数据图像合成方法的局限性，显著提高了合成图像的质量和与训练数据分布的对齐程度，实现了SOTA性能。", "translation": "开源预训练模型在多种应用中具有巨大潜力，但当其训练数据不可用时，其效用会下降。无数据图像合成（DFIS）旨在生成近似预训练模型学习到的数据分布的图像，而无需访问原始数据。然而，由于缺乏关于自然图像的先验知识，现有的DFIS方法生成的样本偏离训练数据分布。为了克服这一限制，我们提出了DDIS，这是第一个扩散辅助的无数据图像合成方法，它利用文本到图像扩散模型作为强大的图像先验，从而提高合成图像质量。DDIS从给定模型中提取关于学习分布的知识，并用它来指导扩散模型，从而能够生成与训练数据分布精确对齐的图像。为了实现这一点，我们引入了域对齐指导（DAG），它在扩散采样过程中将合成数据域与训练数据域对齐。此外，我们优化了一个单一的类对齐令牌（CAT）嵌入，以有效捕获训练数据集中的类特定属性。在PACS和ImageNet上的实验表明，DDIS通过生成更能反映训练数据分布的样本，性能优于先前的DFIS方法，在无数据应用中实现了SOTA性能。", "summary": "本文提出了DDIS，一种创新的扩散辅助无数据图像合成方法，旨在解决现有DFIS方法因缺乏先验知识导致生成图像偏离原始数据分布的问题。DDIS利用文本到图像扩散模型作为强大的图像先验，并结合模型知识来指导生成过程。为确保合成图像与训练数据分布精确对齐，DDIS引入了域对齐指导（DAG）和优化的类对齐令牌（CAT）。实验证明，DDIS在PACS和ImageNet数据集上均超越了现有方法，在无数据图像合成任务中达到了最先进的性能。", "keywords": "无数据图像合成, 扩散模型, 模型知识, 域对齐, 类对齐", "comments": "DDIS的创新之处在于首次将文本到图像扩散模型引入到无数据图像合成领域，并将其作为强大的图像先验。通过引入域对齐指导（DAG）和类对齐令牌（CAT）来解决合成数据与原始训练数据分布对齐的关键挑战，显著提升了合成图像的质量和真实性。这项工作对于隐私保护和数据效率受限的应用场景具有重要意义。"}}
{"id": "2506.15190", "title": "Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors", "authors": ["Jiyi Wang", "Jingyang Ke", "Bo Dai", "Anqi Wu"], "summary": "Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.", "comment": "9 pages and 4 figures for the main text", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15190v1", "AI": {"title_translation": "学习与任务无关的技能基础以揭示动物行为中的运动原语", "tldr": "提出SKIL框架，通过强化学习和表示学习，识别可重用技能并生成逼真行为轨迹，更好地理解动物行为的运动原语组合。", "motivation": "现有行为分割方法通过施加离散音节过度简化了动物灵活重组有限核心运动原语以满足多样任务需求的过程，未能反映真实的动物行为生成过程。", "method": "引入了基于强化学习的模仿框架——技能基模仿学习（SKIL），它通过利用转移概率上的表示学习来推断可解释的技能集（即行为的潜在基函数），并将策略参数化为这些技能的动态混合。", "result": "在简单网格世界、离散迷宫和自由移动动物的无约束视频中验证了该方法，结果表明它能够识别可重用的技能组件，学习持续演变的组合策略，并生成超出传统离散模型能力范围的逼真轨迹。", "conclusion": "通过利用具有组合表示的生成行为建模，该方法为复杂动物行为如何从基本运动原语的动态组合中出现提供了一种简洁、有原则的解释。", "translation": "动物灵活地重新组合有限的核心运动原语以满足多样化的任务需求，但现有的行为分割方法通过在限制性生成假设下施加离散音节来过度简化这一过程。为了反映动物行为的生成过程，我们引入了用于行为理解的基于技能的模仿学习（SKIL），这是一个基于强化学习的模仿框架，它（1）通过利用转移概率上的表示学习推断可解释的技能集，即行为的潜在基函数，以及（2）将策略参数化为这些技能的动态混合。我们在一个简单的网格世界、一个离散迷宫和自由移动动物的无约束视频上验证了我们的方法。在各项任务中，它识别出可重用的技能组件，学习持续演变的组合策略，并生成超出传统离散模型能力范围的逼真轨迹。通过利用具有组合表示的生成行为建模，我们的方法为复杂动物行为如何从基本运动原语的动态组合中出现提供了一种简洁、有原则的解释。", "summary": "本文提出了一种名为技能基模仿学习（SKIL）的强化学习框架，旨在克服现有行为分割方法在理解动物行为生成过程中的局限性。SKIL通过表示学习推断出可解释的潜在技能基函数，并将行为策略参数化为这些技能的动态混合。实验证明，该方法能识别可重用技能、学习组合策略并生成比传统模型更逼真的轨迹，从而为理解复杂动物行为如何从基本运动原语的动态组合中产生提供了新的视角。", "keywords": "运动原语, 技能基模仿学习, 强化学习, 行为理解, 组合表示", "comments": "这篇论文的创新点在于提出了SKIL框架，它通过引入基于表示学习的技能基础和动态混合策略，更真实地模拟了动物行为中运动原语的组合生成过程。这克服了传统方法对离散音节的过度简化，为理解复杂动物行为的底层机制提供了更精细和连续的解释，具有重要的理论和应用价值。"}}
{"id": "2506.15404", "title": "NERO: Explainable Out-of-Distribution Detection with Neuron-level Relevance", "authors": ["Anju Chhetri", "Jari Korhonen", "Prashnna Gyawali", "Binod Bhattarai"], "summary": "Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15404v1", "AI": {"title_translation": "NERO：基于神经元级别相关性的可解释性分布外检测", "tldr": "NERO提出了一种新的OOD检测机制，通过利用特征层的神经元级别相关性来提高OOD样本的可分离性，并在医学影像数据集上取得了优于SOTA的性能，并支持可解释性。", "motivation": "在深度学习中，尤其是在医疗影像领域，模型输出对诊断决策至关重要，因此确保模型的可靠性是首要任务。现有许多OOD检测方法依赖于特征或logit空间表示，但这些方法可能无法完全捕捉OOD的多样性。为了解决这个问题，需要一种新的机制来更有效地识别OOD样本并提供可解释性。", "method": "本文提出了一种名为NERO的新型OOD评分机制，该机制利用特征层的神经元级别相关性。具体来说，它通过聚类每个分布内(ID)类别的神经元级别相关性来形成代表性质心，并引入一个相关性距离度量来量化新样本与这些质心之间的偏差，从而增强OOD可分离性。此外，NERO通过在偏置项中结合缩放相关性和特征范数来优化性能，并支持可解释性OOD检测。", "result": "NERO在Kvasir和GastroVision胃肠道影像基准数据集上，对多种深度学习架构进行了验证，实现了优于现有最先进OOD检测方法的改进。", "conclusion": "NERO提出了一种新颖的OOD检测方法，通过利用神经元级别的相关性，有效提高了OOD样本的识别能力和可解释性，在医学影像领域显示出卓越的性能和潜在价值。", "translation": "确保深度学习的可靠性至关重要，尤其是在医疗影像领域，诊断决策往往取决于模型输出。在研究中，分离分布外 (OOD) 样本的能力已被证明是模型可靠性的宝贵指标。在医疗影像中，这一点尤为关键，因为识别 OOD 输入有助于标记可能未被发现的潜在异常。尽管许多 OOD 检测方法依赖于特征或逻辑空间表示，但最近的研究表明这些方法可能无法完全捕捉 OOD 的多样性。为了解决这个问题，我们提出了一种新颖的 OOD 评分机制，称为 NERO，它利用特征层的神经元级别相关性。具体来说，我们聚类每个分布内 (ID) 类别的神经元级别相关性以形成代表性质心，并引入相关性距离度量来量化新样本与这些质心之间的偏差，从而增强 OOD 可分离性。此外，我们通过在偏置项中结合缩放相关性和特征范数来优化性能。我们的框架还支持可解释的 OOD 检测。我们在胃肠道影像基准 Kvasir 和 GastroVision 上的多种深度学习架构中验证了其有效性，实现了优于最先进 OOD 检测方法的改进。", "summary": "本文提出了一种名为NERO的新型分布外(OOD)检测方法，旨在提高深度学习模型在医疗影像等关键领域的可靠性。与传统依赖特征或logit空间的方法不同，NERO利用特征层的神经元级别相关性来构建OOD评分机制。它通过聚类分布内类别的神经元相关性形成质心，并使用相关性距离度量来量化新样本的偏差，从而增强OOD的可分离性。NERO还通过结合缩放相关性和特征范数来优化性能，并支持可解释的OOD检测。实验结果表明，NERO在Kvasir和GastroVision等胃肠道影像数据集上，相较于现有最先进的OOD检测方法，取得了显著的性能提升。", "keywords": "分布外检测, 神经元相关性, 可解释性AI, 医疗影像, 深度学习可靠性", "comments": "NERO的创新点在于将神经元级别的相关性引入OOD检测，这可能捕捉到更细粒度的异常信息。其在医疗影像领域的应用尤其重要，因为该领域对模型可靠性和可解释性有极高要求。此外，提供可解释性是其重要优势，有助于医生理解模型判断。未来的工作可以探索其在其他高风险领域的应用或进一步提升解释的深度。"}}
{"id": "2506.15199", "title": "Interpretability and Generalization Bounds for Learning Spatial Physics", "authors": ["Alejandro Francisco Queiruga", "Theo Gutman-Solo", "Shuai Jiang"], "summary": "While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15199v1", "AI": {"title_translation": "学习空间物理的可解释性和泛化界限", "tldr": "本文量化了机器学习应用于科学问题的准确性，证明了泛化界限，并揭示了模型泛化行为的复杂性及可解释性方法。", "motivation": "尽管机器学习在科学问题中有许多应用看起来很有前景，但视觉效果可能具有欺骗性，实际定量精度至关重要。当前方法可能无法保证对真实物理方程的泛化。", "method": "将数值分析的严谨性应用于机器学习，量化不同ML技术应用于基本一维泊松微分方程的准确性。证明了在有限数据离散化和受限训练数据子空间下的泛化界限和收敛速度。分析了训练动态，并推导了白盒微分方程发现方法和黑盒线性模型的最佳参数。通过分析黑盒模型的权重，提取格林函数表示，以实现新的机械可解释性。提出了一种新的交叉验证技术来衡量物理系统中的泛化能力，并建议将其应用于泊松方程作为未来方法的评估基准。", "result": "识别出数据函数空间对模型泛化至关重要。证明了泛化界限和收敛速度，并得到了经验复制。经验证明了深度线性模型、浅层神经网络以及特定物理模型（DeepONets和Neural Operators）存在类似的泛化不足。理论和经验表明，在所探索的每种情况下，不能保证泛化到真实的物理方程。发现不同类别的模型可以表现出相反的泛化行为。展示了一种新的机械可解释性视角，即可以从黑盒模型的权重中提取格林函数表示。", "conclusion": "机器学习模型在科学应用中的泛化能力并非总能得到保证，数据函数空间和模型类型是关键因素。本文提出了一种新的机械可解释性方法和衡量物理系统泛化的交叉验证技术，为未来研究提供了评估基准。", "translation": "尽管机器学习在科学问题中有许多应用看起来很有前景，但视觉效果可能具有欺骗性。对于科学应用而言，实际的定量精度至关重要。这项工作将微分方程数值分析的严谨性应用于机器学习，通过具体量化不同机器学习技术应用于基本一维泊松微分方程的准确性。除了数据的数量和离散化，我们发现数据的函数空间对模型的泛化至关重要。我们通过分析训练动态并推导白盒微分方程发现方法和黑盒线性模型的最佳参数，证明了在有限数据离散化和受限训练数据子空间下的泛化界限和收敛速度。分析推导的泛化界限得到了经验复制。对于深度线性模型、浅层神经网络以及特定物理模型DeepONets和Neural Operators，也经验性地证明了类似的泛化不足。我们从理论和经验上证明，在所探索的每种情况下，不能保证泛化到真实的物理方程。令人惊讶的是，我们发现不同类别的模型可以表现出相反的泛化行为。基于我们的理论分析，我们还展示了一种关于科学模型的新机械可解释性视角，即可以从黑盒模型的权重中提取格林函数表示。我们的结果为衡量物理系统中泛化能力的新交叉验证技术提供了依据。我们建议将其应用于泊松方程作为未来方法的评估基准。", "summary": "本文研究了机器学习在科学应用中的泛化能力和可解释性，强调了定量准确性的重要性。通过将数值分析的严谨性应用于一维泊松方程，研究发现数据函数空间对模型泛化至关重要，并证明了泛化界限。研究还揭示了不同模型在泛化行为上的差异，以及机器学习模型在物理问题中泛化到真实方程的挑战。此外，本文提出了一种从黑盒模型中提取格林函数表示的新型可解释性方法，并建议了一种新的交叉验证技术作为物理系统泛化的评估基准。", "keywords": "机器学习, 泛化界限, 可解释性, 泊松方程, 物理系统", "comments": "本文创新性地将数值分析的严谨性引入机器学习在科学问题中的应用，特别关注了泛化能力和可解释性。它不仅提供了理论上的泛化界限证明，还通过实验验证了这些发现，并揭示了不同模型泛化行为的复杂性。更重要的是，它提出了一种从黑盒模型中提取物理可解释性（格林函数）的新方法，这对于提升科学领域机器学习模型的信任度和应用潜力具有重要意义。文章还提出了一种新的评估基准，有助于未来研究的标准化。"}}
{"id": "2506.15629", "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability", "authors": ["Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "summary": "In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.", "comment": "ACL 2025 Main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15629v1", "AI": {"title_translation": "重新审视大型语言模型在考虑指令遵循能力下的组合泛化能力", "tldr": "重新审视了大型语言模型在指令遵循下的组合泛化能力。提出了Ordered CommonGen基准，发现LLM理解指令但存在偏差，指令遵循能力仍需提高。", "motivation": "在生成式常识推理任务中，大型语言模型（LLM）能够组合包含给定概念的句子。然而，当提示指定概念顺序时，LLM难以生成遵循该顺序的句子。为了解决这一问题，需要一个新基准来同时评估LLM的组合泛化和指令遵循能力。", "method": "提出了Ordered CommonGen基准，旨在评估大型语言模型的组合泛化和指令遵循能力。该基准通过衡量“有序覆盖率”来评估概念是否按指定顺序生成，从而实现两种能力的同步评估。研究人员使用36个大型语言模型进行了全面分析。", "result": "通过对36个大型语言模型的分析发现，尽管LLM通常能理解指令意图，但对特定概念顺序模式存在偏见，这常导致输出多样性低，或即使概念顺序改变，结果也相同。此外，即使是最符合指令的LLM，其有序覆盖率也仅达到约75%。", "conclusion": "大型语言模型在指令遵循和组合泛化能力方面都需要显著改进。", "translation": "在CommonGen等生成式常识推理任务中，生成式大型语言模型（LLM）会组合包含所有给定概念的句子。然而，当关注指令遵循能力时，如果提示指定了概念顺序，LLM必须生成遵循指定顺序的句子。为了解决这个问题，我们提出了Ordered CommonGen，这是一个旨在评估LLM组合泛化和指令遵循能力的基准。该基准衡量有序覆盖率，以评估概念是否按指定顺序生成，从而实现两种能力的同步评估。我们对36个LLM进行了全面分析，发现虽然LLM通常理解指令意图，但对特定概念顺序模式的偏见常常导致低多样性输出，或者即使概念顺序改变，结果也相同。此外，即使是最符合指令的LLM，其有序覆盖率也仅达到约75%，这突出表明指令遵循和组合泛化能力都需要改进。", "summary": "本文重新审视了大型语言模型（LLM）在考虑指令遵循能力下的组合泛化能力。针对LLM在按指定概念顺序生成句子方面的挑战，提出了Ordered CommonGen基准，该基准通过衡量有序覆盖率来同时评估LLM的组合泛化和指令遵循能力。对36个LLM的分析显示，尽管LLM理解指令意图，但存在概念顺序模式偏差，导致输出多样性低，且即使表现最好的LLM，其有序覆盖率也仅为75%，表明LLM在这些方面仍需显著改进。", "keywords": "大型语言模型, 组合泛化, 指令遵循, Ordered CommonGen, 基准", "comments": "本文通过引入Ordered CommonGen基准，创新性地同时评估了大型语言模型的组合泛化能力和指令遵循能力，特别关注了概念顺序的影响。研究结果揭示了LLM在处理指令中特定顺序时的显著局限性，即使是最优秀的模型也仅达到75%的有序覆盖率，这对于提升LLM的可靠性和精确性具有重要指导意义。"}}
{"id": "2506.15442", "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material", "authors": ["Team Hunyuan3D", "Shuhui Yang", "Mingxin Yang", "Yifei Feng", "Xin Huang", "Sheng Zhang", "Zebin He", "Di Luo", "Haolin Liu", "Yunfei Zhao", "Qingxiang Lin", "Zeqiang Lai", "Xianghui Yang", "Huiwen Shi", "Zibo Zhao", "Bowen Zhang", "Hongyu Yan", "Lifu Wang", "Sicong Liu", "Jihong Zhang", "Meng Chen", "Liang Dong", "Yiwen Jia", "Yulin Cai", "Jiaao Yu", "Yixuan Tang", "Dongyuan Guo", "Junlin Yu", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Shida Wei", "Chao Zhang", "Yonghao Tan", "Yifu Sun", "Lin Niu", "Shirui Huang", "Bojian Zheng", "Shu Liu", "Shilin Chen", "Xiang Yuan", "Xiaofeng Yang", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Tian Liu", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Jingwei Huang", "Chunchao Guo"], "summary": "3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.", "comment": "Github link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15442v1", "AI": {"title_translation": "混元3D 2.1：从图像到高保真3D资产，具备生产就绪的PBR材质", "tldr": "Hunyuan3D 2.1是一个教程，旨在帮助研究人员、开发人员和设计师克服3D AI生成内容（AIGC）的复杂性，通过分步指导和系统介绍，使他们能够创建高分辨率、纹理化的3D资产。", "motivation": "尽管3D AI生成内容（AIGC）领域发展迅速，但由于3D模型收集、处理和训练的复杂性，该领域仍主要局限于研究人员、开发人员和设计师。本文旨在解决这些挑战，提高3D AIGC的可访问性。", "method": "本文以Hunyuan3D 2.1为例，提供了一个全面的、分步的教程，内容涵盖3D数据处理、3D生成模型训练和性能评估。Hunyuan3D 2.1系统包含两个核心组件：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint，并探讨了完整的工作流程。", "result": "Hunyuan3D 2.1系统能够生成高分辨率、纹理化的3D资产。通过本教程，用户将掌握微调或开发适用于游戏、虚拟现实和工业设计的强大3D生成模型的知识。", "conclusion": "本教程旨在赋予用户开发和微调鲁棒3D生成模型的知识，使其能够应用于游戏、虚拟现实和工业设计等领域，从而降低3D AIGC的门槛。", "translation": "3D AI生成内容（AIGC）是一个充满激情的领域，它极大地加速了游戏、电影和设计中3D模型的创建。尽管已经开发出一些革命性的突破性模型，彻底改变了3D生成，但由于3D模型收集、处理和训练的复杂性，该领域在很大程度上仍然只对研究人员、开发人员和设计师开放。为了应对这些挑战，我们在本教程中以Hunyuan3D 2.1为例进行介绍。本教程提供了一个全面的、分步的指南，内容涵盖了使用Hunyuan3D 2.1（一个用于生成高分辨率、纹理化3D资产的先进系统）处理3D数据、训练3D生成模型和评估其性能。该系统由两个核心组件组成：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint。我们将探讨整个工作流程，包括数据准备、模型架构、训练策略、评估指标和部署。通过本教程的结束，您将掌握微调或开发适用于游戏、虚拟现实和工业设计应用的强大3D生成模型的知识。", "summary": "Hunyuan3D 2.1是一个针对3D AI生成内容（AIGC）领域挑战的解决方案，旨在提高其可访问性。该教程详细介绍了Hunyuan3D 2.1系统，该系统包含Hunyuan3D-DiT（形状生成）和Hunyuan3D-Paint（纹理合成）两个核心组件，用于创建高分辨率、纹理化的3D资产。教程涵盖了从数据准备到部署的完整工作流程，旨在帮助用户掌握开发和微调3D生成模型的知识，使其适用于游戏、VR和工业设计等应用。", "keywords": "3D AIGC, Hunyuan3D 2.1, 3D生成模型, PBR材质, 纹理合成", "comments": "本文的创新之处在于通过提供一个详细的教程和一套生产就绪的工具（Hunyuan3D 2.1），显著降低了3D AIGC的门槛，使其不仅限于研究人员，也对更广泛的开发者和设计师开放。其重要性在于推动了3D内容创作的民主化，有望加速游戏、电影和设计行业的工作流程。该方法结合了形状生成和纹理合成的专门组件，提供了一个全面的解决方案。"}}
{"id": "2506.15251", "title": "Singular Value Decomposition on Kronecker Adaptation for Large Language Model", "authors": ["Yee Hin Chong", "Peng Qu"], "summary": "Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15251v1", "AI": {"title_translation": "大型语言模型中基于克罗内克适应的奇异值分解", "tldr": "提出SoKA，一种新的PEFT策略，结合克罗内克积张量分解和SVD驱动初始化，以及动态秩选择，减少LLM微调参数并保持性能。", "motivation": "大型预训练Transformer模型进行完全微调会产生巨大的存储、内存和计算成本。现有参数高效微调（PEFT）方法存在推理延迟、收敛不佳或固定秩选择不匹配任务复杂性等局限性。", "method": "本文提出SoKA（SVD on Kronecker Adaptation），一种新颖的参数高效微调（PEFT）策略。SoKA结合了克罗内克积张量分解与SVD驱动初始化和频谱感知动态秩选择。通过克罗内克积SVD（KPSVD）过程，将全权重更新的主成分提取为紧凑的克罗内克因子，并利用能量阈值和肘点准则的自适应秩选择算法来修剪可忽略的组件。", "result": "在LLaMA2-7B模型上，针对算术推理（GSM8K）、形式数学（MATH）和代码生成（MBPP）任务进行评估。SoKA仅需0.99M可训练参数，比LoRA/PiSSA少25%，同时达到或超过基线性能。此外，SoKA表现出更快的收敛速度和更稳定的梯度。", "conclusion": "SoKA是一种鲁棒且高效的大规模模型适应策略，它能在显著减少可训练参数的同时，保持或提升模型性能，并具有更快的收敛速度和更稳定的梯度，有效解决了现有PEFT方法的局限性。", "translation": "大型预训练Transformer模型在各种语言和推理任务中取得了最先进的结果，但完全微调会带来大量的存储、内存和计算开销。参数高效微调（PEFT）方法通过仅学习一小部分任务特定参数来减轻这些成本，但现有方法要么引入推理时延迟（适配器模块），要么收敛效果不佳（随机初始化的低秩更新），要么依赖于可能与任务复杂性不匹配的固定秩选择（基于克罗内克分解）。\n我们提出了SoKA（SVD on Kronecker Adaptation），一种新颖的PEFT策略，它将克罗内克积张量分解与SVD驱动初始化和频谱感知动态秩选择相结合。我们的克罗内克积SVD（KPSVD）过程将全权重更新的主成分提取为紧凑的克罗内克因子，而自适应秩选择算法使用能量阈值和肘点准则来修剪可忽略的组件。\n在LLaMA2-7B模型上对算术推理（GSM8K）、形式数学（MATH）和代码生成（MBPP）的实证评估表明，SoKA仅需要0.99M可训练参数，比LoRA/PiSSA少25%，同时达到或超过基线性能。此外，SoKA表现出更快的收敛速度和更稳定的梯度，突出了其在大规模模型适应方面的鲁棒性和效率。", "summary": "本文提出SoKA，一种新颖的参数高效微调（PEFT）策略，旨在解决大型语言模型微调中面临的存储、内存、计算开销以及现有PEFT方法的局限性，如推理延迟、收敛性差或固定秩选择不当。SoKA结合了克罗内克积张量分解、SVD驱动初始化和频谱感知动态秩选择，通过KPSVD提取紧凑的克罗内克因子并自适应修剪不重要的组件。实验证明，SoKA在LLaMA2-7B模型上能显著减少可训练参数（比LoRA/PiSSA少25%）的同时，保持或超越基线性能，并展现出更快的收敛速度和更稳定的梯度，验证了其鲁棒性和高效性。", "keywords": "参数高效微调, 克罗内克分解, 奇异值分解, 大型语言模型, 动态秩选择", "comments": "SoKA的创新点在于结合了克罗内克积分解与SVD驱动初始化和动态秩选择，有效解决了现有PEFT方法面临的挑战。其动态秩选择机制尤其重要，因为它允许模型根据任务复杂性自适应调整参数量，避免了固定秩的局限性。这种方法在减少计算和存储成本的同时，保持甚至提升了模型性能，对于大规模语言模型的实际应用具有重要意义。"}}
{"id": "2506.15650", "title": "Oldies but Goldies: The Potential of Character N-grams for Romanian Texts", "authors": ["Dana Lupsa", "Sanda-Maria Avram"], "summary": "This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15650v1", "AI": {"title_translation": "旧瓶装新酒：字符N-gram在罗马尼亚语文本中的潜力", "tldr": "本研究评估了字符N-gram特征结合六种机器学习技术在罗马尼亚语文本作者归属问题上的表现。结果显示，人工神经网络（ANN）模型表现最佳，证明了轻量级、可解释的字符N-gram方法在作者归属任务上可以达到最先进的准确率。", "motivation": "本研究旨在解决罗马尼亚语文本的作者归属问题，并评估字符N-gram特征在此任务中的潜力。", "method": "研究使用了ROST语料库，并系统评估了六种机器学习技术（SVM、LR、k-NN、DT、RF、ANN），采用字符N-gram特征进行分类。", "result": "ANN模型表现最佳，在使用5-gram特征时，在15次运行中有4次实现了完美分类。结果表明，轻量级、可解释的字符N-gram方法在罗马尼亚语作者归属任务上能提供先进的准确率，可与更复杂的方法媲美。", "conclusion": "字符N-gram等简单文体特征在资源受限或研究不足的语言环境中具有巨大潜力，能够实现最先进的作者归属准确率。", "translation": "本研究利用ROST语料库（该领域的标准基准）解决了罗马尼亚语文本的作者归属问题。我们系统地评估了六种机器学习技术：支持向量机（SVM）、逻辑回归（LR）、k-最近邻（k-NN）、决策树（DT）、随机森林（RF）和人工神经网络（ANN），采用字符N-gram特征进行分类。其中，ANN模型取得了最高性能，在使用5-gram特征时，在十五次运行中有四次实现了完美分类。这些结果表明，轻量级、可解释的字符N-gram方法可以为罗马尼亚语作者归属提供最先进的准确性，与更复杂的方法相媲美。我们的发现强调了简单文体特征在资源受限或研究不足的语言环境中的潜力。", "summary": "本研究探讨了字符N-gram特征在罗马尼亚语文本作者归属任务中的应用潜力。通过在ROST语料库上评估SVM、LR、k-NN、DT、RF和ANN六种机器学习模型，发现ANN模型表现最优，尤其在使用5-gram特征时能达到完美分类。研究结果强调了轻量级、可解释的字符N-gram方法在实现高精度作者归属方面的有效性，并指出其在资源受限语言环境中的广阔前景。", "keywords": "作者归属, 罗马尼亚语文本, 字符N-gram, 机器学习, 人工神经网络", "comments": "这项研究的创新之处在于，它证明了相对简单且可解释的字符N-gram特征，结合适当的机器学习模型（特别是ANN），在罗马尼亚语这类资源受限语言的作者归属任务中，能够达到甚至超越复杂方法的性能。这对于计算资源有限或语言学研究不足的领域具有重要意义，提供了一种高效且准确的解决方案。"}}
{"id": "2506.15477", "title": "Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning", "authors": ["Chunlei Li", "Jingyang Hou", "Yilei Shi", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "summary": "Medical report generation from imaging data remains a challenging task in\nclinical practice. While large language models (LLMs) show great promise in\naddressing this challenge, their effective integration with medical imaging\ndata still deserves in-depth exploration. In this paper, we present MRG-LLM, a\nnovel multimodal large language model (MLLM) that combines a frozen LLM with a\nlearnable visual encoder and introduces a dynamic prompt customization\nmechanism. Our key innovation lies in generating instance-specific prompts\ntailored to individual medical images through conditional affine\ntransformations derived from visual features. We propose two implementations:\nprompt-wise and promptbook-wise customization, enabling precise and targeted\nreport generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets\ndemonstrate that MRG-LLM achieves state-of-the-art performance in medical\nreport generation. Our code will be made publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15477v1", "AI": {"title_translation": "基于定制化提示词微调的多模态大语言模型用于医学报告生成", "tldr": "MRG-LLM是一个新颖的多模态大语言模型，通过结合冻结的LLM和可学习的视觉编码器，并引入动态提示词定制机制，在医学报告生成任务上实现了最先进的性能。", "motivation": "从影像数据生成医学报告在临床实践中仍然是一项具有挑战性的任务。尽管大型语言模型（LLMs）在解决这一挑战方面显示出巨大前景，但它们与医学影像数据的有效整合仍值得深入探索。", "method": "本文提出了MRG-LLM，这是一种新颖的多模态大型语言模型（MLLM），它将冻结的LLM与可学习的视觉编码器相结合，并引入了动态提示词定制机制。其关键创新在于通过源自视觉特征的条件仿射变换，生成针对个体医学图像量身定制的实例特定提示词。该方法提出了提示词级和提示词簿级两种定制实现方式。", "result": "在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医学报告生成方面取得了最先进的性能。", "conclusion": "MRG-LLM通过其结合冻结LLM、可学习视觉编码器以及动态提示词定制的创新方法，有效解决了医学报告生成的挑战，并在相关数据集上取得了最先进的性能。", "translation": "从影像数据生成医学报告在临床实践中仍然是一项具有挑战性的任务。尽管大型语言模型（LLMs）在解决这一挑战方面显示出巨大的前景，但它们与医学影像数据的有效整合仍值得深入探索。在本文中，我们提出了MRG-LLM，这是一种新颖的多模态大型语言模型（MLLM），它将冻结的LLM与可学习的视觉编码器相结合，并引入了动态提示词定制机制。我们的关键创新在于通过源自视觉特征的条件仿射变换，生成针对个体医学图像量身定制的实例特定提示词。我们提出了两种实现方式：提示词级和提示词簿级定制，从而实现精确和有针对性的报告生成。在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医学报告生成方面取得了最先进的性能。我们的代码将公开发布。", "summary": "本文介绍了一种名为MRG-LLM的新型多模态大语言模型，旨在解决从医学影像数据生成报告的挑战。MRG-LLM结合了冻结的LLM和可学习的视觉编码器，并引入了创新的动态提示词定制机制。通过视觉特征的条件仿射变换，该模型能够为每个医学图像生成实例特定的提示词，并提供提示词级和提示词簿级两种定制方式。实验结果表明，MRG-LLM在IU X-ray和MIMIC-CXR数据集上实现了医学报告生成的最新性能。", "keywords": "多模态大语言模型, 医学报告生成, 提示词微调, 动态提示词定制, 视觉编码器", "comments": "MRG-LLM的创新之处在于其动态提示词定制机制，特别是通过条件仿射变换生成实例特定提示词，这使得LLM能够更有效地与医学图像数据融合。这种方法为多模态大语言模型在特定领域，如医学报告生成，提供了一个有前景的解决方案，提升了报告的精确性和针对性。其实现最先进性能的成果也验证了该方法的有效性。"}}
{"id": "2506.15662", "title": "CC-LEARN: Cohort-based Consistency Learning", "authors": ["Xiao Ye", "Shaswat Shrivastava", "Zhaonan Li", "Jacob Dineen", "Shijie Lu", "Avneet Ahuja", "Ming Shen", "Zhikun Xu", "Ben Zhou"], "summary": "Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15662v1", "AI": {"title_translation": "CC-LEARN: 基于群组的一致性学习", "tldr": "CC-Learn是一个新的强化学习框架，通过在相似问题群组上训练，显著提高了大型语言模型推理的一致性和准确性。", "motivation": "大型语言模型在许多任务中表现出色，但在提供一致、鲁棒的推理方面仍存在困难。", "method": "引入了基于群组的一致性学习（CC-Learn），这是一个强化学习框架。它通过在源自共享程序抽象的相似问题群组上进行训练来提高LLM推理的可靠性。为强制实现群组级别的一致性，定义了一个复合目标，结合了群组准确性、有效问题分解的检索奖励和对琐碎或无效查找的拒绝惩罚，这些都可以通过强化学习直接优化。", "result": "在具有挑战性的推理基准测试（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn在准确性和推理稳定性方面均优于预训练和SFT基线。", "conclusion": "这些结果表明，群组级别的强化学习有效地增强了大型语言模型中的推理一致性。", "translation": "大型语言模型在许多任务中表现出色，但在提供一致、鲁棒的推理方面仍存在困难。我们引入了基于群组的一致性学习（CC-Learn），这是一个强化学习框架，通过在源自共享程序抽象的相似问题群组上进行训练，提高了大型语言模型推理的可靠性。为了强制实现群组级别的一致性，我们定义了一个复合目标，结合了群组准确性、有效问题分解的检索奖励以及对琐碎或无效查找的拒绝惩罚，与监督微调不同，强化学习可以直接优化这些目标。优化此奖励可以引导模型在所有群组成员中采用统一的推理模式。在具有挑战性的推理基准测试（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn在准确性和推理稳定性方面均优于预训练和SFT基线。这些结果表明，群组级别的强化学习有效地增强了大型语言模型中的推理一致性。", "summary": "CC-Learn是一个新颖的强化学习框架，旨在解决大型语言模型在推理一致性方面的不足。该框架通过在相似问题群组上进行训练，并结合一个复合奖励目标（包括群组准确性、检索奖励和拒绝惩罚），引导模型学习统一的推理模式。实验证明，CC-Learn在多个推理基准测试中显著提升了LLM的准确性和推理稳定性。", "keywords": "大型语言模型, 强化学习, 推理一致性, 群组学习, 准确性", "comments": "这项工作通过引入一个新颖的强化学习框架CC-Learn，解决了大型语言模型推理一致性不足的关键问题。其创新之处在于利用“群组”概念，并设计了一个可供RL直接优化的复合奖励函数，这与传统的监督微调方法不同。通过强制模型在相似问题群组中保持一致的推理模式，CC-Learn有效地提升了LLM的鲁棒性和准确性，为提高AI推理能力提供了一个有前景的方向。"}}
{"id": "2506.15483", "title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects", "authors": ["Shujia Li", "Haiyu Zhang", "Xinyuan Chen", "Yaohui Wang", "Yutong Ban"], "summary": "While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15483v1", "AI": {"title_translation": "GenHOI: 泛化文本驱动的4D人-物体交互合成，适用于未见物体", "tldr": "GenHOI是一个两阶段框架，通过学习稀疏3D HOI数据，实现了对未见物体的泛化4D人-物体交互合成。", "motivation": "文本驱动的人体运动合成取得了进展，但4D人-物体交互（HOI）合成仍具挑战，主要原因是缺乏大规模4D HOI数据集。", "method": "本文提出了GenHOI两阶段框架。第一阶段，使用Object-AnchorNet从3D HOI数据集重建未见物体的稀疏3D HOI关键帧，减少对大规模4D HOI数据集的依赖。第二阶段，引入Contact-Aware Diffusion Model (ContactDM) 将稀疏关键帧插值为密集时间连贯的4D HOI序列。ContactDM中包含Contact-Aware Encoder提取接触模式和Contact-Aware HOI Attention整合接触信号。", "result": "在OMOMO和3D-FUTURE数据集上取得了最先进的结果，展示了对未见物体的强大泛化能力，并实现了高保真4D HOI生成。", "conclusion": "GenHOI框架成功解决了4D HOI合成中对大规模数据集的依赖问题，并通过其两阶段方法实现了对未见物体的高质量、泛化性强的4D HOI序列生成。", "translation": "尽管扩散模型和大规模运动数据集推动了文本驱动的人体运动合成，但将这些进展扩展到4D人-物体交互（HOI）仍然具有挑战性，主要原因是缺乏大规模4D HOI数据集。在我们的研究中，我们引入了GenHOI，这是一个新颖的两阶段框架，旨在实现两个关键目标：1）泛化到未见物体，以及2）合成高保真4D HOI序列。在框架的初始阶段，我们采用Object-AnchorNet从仅有的3D HOI数据集中学习，重建未见物体的稀疏3D HOI关键帧，从而减轻了对大规模4D HOI数据集的依赖。随后，我们在第二阶段引入了一个接触感知扩散模型（ContactDM），以无缝地将稀疏3D HOI关键帧插值为密集且时间连贯的4D HOI序列。为了提高生成的4D HOI序列的质量，我们在ContactDM中提出了一个新颖的接触感知编码器来提取人-物体接触模式，以及一个新颖的接触感知HOI注意力机制来有效地将接触信号整合到扩散模型中。实验结果表明，我们在公开的OMOMO和3D-FUTURE数据集上取得了最先进的结果，展示了对未见物体的强大泛化能力，同时实现了高保真4D HOI生成。", "summary": "本文提出了GenHOI，一个解决4D人-物体交互（HOI）合成中数据稀缺和对未见物体泛化能力不足问题的两阶段框架。第一阶段利用Object-AnchorNet从稀疏3D HOI数据中重建关键帧；第二阶段使用Contact-Aware Diffusion Model（包含接触感知编码器和注意力机制）将关键帧插值为高保真4D HOI序列。实验证明GenHOI在多个数据集上实现了SOTA性能和强大的泛化能力。", "keywords": "4D人-物体交互, 扩散模型, 未见物体泛化, 稀疏数据合成, 接触感知", "comments": "GenHOI的创新之处在于其两阶段方法，特别是通过利用稀疏3D数据来解决大规模4D HOI数据集缺乏的瓶颈，并引入了接触感知机制来提高生成质量和泛化能力，这对于更真实的虚拟人机交互具有重要意义。"}}
{"id": "2506.15271", "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data", "authors": ["Bihe Zhao", "Pratyush Maini", "Franziska Boenisch", "Adam Dziedzic"], "summary": "The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.", "comment": "Accepted at ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15271v1", "AI": {"title_translation": "利用合成数据解锁事后数据集推断", "tldr": "现有数据集推断方法受限于缺乏分布匹配的私有数据，本文提出通过生成高质量的合成数据来解决此问题，从而实现对大型语言模型训练数据中未经授权使用的检测，赋能版权所有者进行合法维权。", "motivation": "大型语言模型（LLMs）的强大能力源于其庞大的训练数据集，这些数据集通常未经数据所有者许可从互联网抓取，引发知识产权问题。数据集推断（DI）旨在识别可疑数据集是否被用于训练，从而帮助数据所有者验证未经授权的使用。然而，现有DI方法要求一个私有的、已知未用于训练且与受损数据集分布紧密匹配的“保留集”，这种数据在实践中很少可用，严重限制了DI的适用性。", "method": "本文通过合成生成所需的保留集来解决上述挑战。方法包括两个关键步骤：1) 通过在精心设计的基于后缀的补全任务上训练数据生成器，创建高质量、多样化且能准确反映原始分布的合成数据；2) 通过事后校准来弥合真实数据和合成数据之间的似然差距。", "result": "在各种文本数据集上进行的广泛实验表明，使用本文生成的合成数据作为保留集，能使数据集推断以高置信度检测原始训练集，同时保持较低的误报率。", "conclusion": "这一结果赋能版权所有者对数据使用提出合法主张，并证明了本文方法在现实世界诉讼中的可靠性。", "translation": "大型语言模型（LLMs）的卓越能力主要归因于其庞大的训练数据集，这些数据集通常从互联网抓取，但未尊重数据所有者的知识产权。数据集推断（DI）提供了一种潜在的补救措施，通过识别可疑数据集是否被用于训练，从而使数据所有者能够验证未经授权的使用。然而，现有的DI方法需要一个私有的、已知未参与训练且与受损数据集分布紧密匹配的集合。这种分布内、保留的数据在实践中很少可用，严重限制了DI的适用性。在这项工作中，我们通过合成生成所需的保留集来解决这一挑战。我们的方法解决了两个关键障碍：（1）创建高质量、多样化且能准确反映原始分布的合成数据，我们通过在精心设计的基于后缀的补全任务上训练数据生成器来实现这一点；以及（2）弥合真实数据和合成数据之间的似然差距，这通过事后校准来实现。在各种文本数据集上进行的广泛实验表明，使用我们生成的数据作为保留集，能使DI以高置信度检测原始训练集，同时保持较低的误报率。这一结果赋能版权所有者对数据使用提出合法主张，并证明了我们方法在现实世界诉讼中的可靠性。我们的代码可在https://github.com/sprintml/PostHocDatasetInference 获取。", "summary": "本文提出了一种新颖的方法，通过合成生成高质量的“保留集”，克服了现有数据集推断（DI）方法在检测大型语言模型训练数据未经授权使用时的主要局限性。该方法利用一个基于后缀补全任务训练的数据生成器来创建与原始数据分布匹配的合成数据，并通过事后校准来缩小真实与合成数据之间的似然差距。实验证明，该方法能使DI以高置信度且低误报率地识别原始训练集，从而有效支持数据版权所有者的合法维权。", "keywords": "数据集推断, 合成数据, 大型语言模型, 知识产权, 版权", "comments": "这项工作在解决大型语言模型训练数据知识产权侵权问题上具有重要意义。其创新之处在于提出了一种有效生成合成“保留集”的方法，从而显著拓宽了数据集推断的实际应用范围。通过解决现有DI方法的关键限制，该研究为数据所有者提供了更实用的工具，以保护其知识产权，并可能对未来的数据使用规范和相关诉讼产生积极影响。方法的可靠性及其在现实世界诉讼中的潜在应用是其核心价值。"}}
{"id": "2506.15524", "title": "NTIRE 2025 Image Shadow Removal Challenge Report", "authors": ["Florin-Alexandru Vasluianu", "Tim Seizinger", "Zhuyun Zhou", "Cailian Chen", "Zongwei Wu", "Radu Timofte", "Mingjia Li", "Jin Hu", "Hainuo Wang", "Hengxing Liu", "Jiarui Wang", "Qiming Hu", "Xiaojie Guo", "Xin Lu", "Jiarong Yang", "Yuanfei Bao", "Anya Hu", "Zihao Fan", "Kunyu Wang", "Jie Xiao", "Xi Wang", "Xueyang Fu", "Zheng-Jun Zha", "Yu-Fan Lin", "Chia-Ming Lee", "Chih-Chung Hsu", "Xingbo Wang", "Dong Li", "Yuxu Chen", "Bin Chen", "Yuanbo Zhou", "Yuanbin Chen", "Hongwei Wang", "Jiannan Lin", "Qinquan Gao", "Tong Tong", "Zhao Zhang", "Yanyan Wei", "Wei Dong", "Han Zhou", "Seyed Amirreza Mousavi", "Jun Chen", "Haobo Liang", "Jiajie Jing", "Junyu Li", "Yan Yang", "Seoyeon Lee", "Chaewon Kim", "Ziyu Feng", "Shidi Chen", "Bowen Luan", "Zewen Chen", "Vijayalaxmi Ashok Aralikatti", "G Gyaneshwar Rao", "Nikhil Akalwadi", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Anas M. Ali", "Bilel Benjdira", "Wadii Boulila", "Alexandru Brateanu", "Cosmin Ancuti", "Tanmay Chaturvedi", "Manish Kumar", "Anmol Srivastav", "Daksh Trivedi", "Shashwat Thakur", "Kishor Upla", "Zeyu Xiao", "Zhuoyuan Li", "Boda Zhou", "Shashank Shekhar", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Suiyi Zhao", "Bo Wang", "Yan Luo", "Mingshen Wang", "Yilin Zhang"], "summary": "This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A\ntotal of 306 participants have registered, with 17 teams successfully\nsubmitting their solutions during the final evaluation phase. Following the\nlast two editions, this challenge had two evaluation tracks: one focusing on\nreconstruction fidelity and the other on visual perception through a user\nstudy. Both tracks were evaluated with images from the WSRD+ dataset,\nsimulating interactions between self- and cast-shadows with a large number of\ndiverse objects, textures, and materials.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15524v1", "AI": {"title_translation": "NTIRE 2025 图像去影挑战报告", "tldr": "报告了NTIRE 2025图像去影挑战的结果，该挑战有306名参与者和17支提交队伍，分为重建保真度和视觉感知两个评估赛道，使用了WSRD+数据集。", "motivation": "该论文旨在总结NTIRE 2025图像去影挑战的发现和结果。", "method": "挑战设置了两个评估赛道：一个侧重于重建保真度，另一个通过用户研究侧重于视觉感知。两个赛道都使用WSRD+数据集中的图像进行评估，该数据集模拟了自阴影和投射阴影与大量不同对象、纹理和材料之间的相互作用。", "result": "共有306名参与者注册，其中17支团队在最终评估阶段成功提交了解决方案。", "conclusion": "Not mentioned in abstract", "translation": "这项工作探讨了NTIRE 2025去影挑战的结果。共有306名参与者注册，其中17支团队在最终评估阶段成功提交了解决方案。继前两届之后，本次挑战赛有两个评估赛道：一个侧重于重建保真度，另一个通过用户研究侧重于视觉感知。这两个赛道都使用WSRD+数据集中的图像进行评估，该数据集模拟了自阴影和投射阴影与大量不同对象、纹理和材料之间的相互作用。", "summary": "这篇报告总结了NTIRE 2025图像去影挑战的发现。该挑战吸引了306名注册参与者，最终有17支团队提交了解决方案。挑战设有重建保真度和视觉感知（通过用户研究）两个评估赛道，所有评估均基于WSRD+数据集，该数据集包含模拟自阴影和投射阴影与多样化对象和材料交互的图像。", "keywords": "图像去影, NTIRE 2025, 挑战赛, WSRD+数据集, 视觉感知", "comments": "这篇报告对NTIRE 2025图像去影挑战进行了总结，展示了社区在解决图像去影问题上的活跃度。挑战设置了重建保真度和视觉感知两个评估维度，这对于全面评估去影算法的性能至关重要。使用WSRD+数据集确保了评估的多样性和复杂性。"}}
{"id": "2506.15289", "title": "DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for Optimizing Large-Scale EV Charging Infrastructure", "authors": ["Chuan Li", "Shunyu Zhao", "Vincent Gauthier", "Hassine Moungla"], "summary": "The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15289v1", "AI": {"title_translation": "DOVA-PATBM：一个用于优化大规模电动汽车充电基础设施的智能、自适应和可扩展框架", "tldr": "DOVA-PATBM是一个智能、可扩展的地理计算框架，用于优化大规模电动汽车充电基础设施，通过整合多源数据和先进模型，显著提高覆盖率、公平性并满足电网限制，同时保持计算效率。", "motivation": "现有电动汽车充电基础设施规划工具无法满足大规模电动汽车普及的需求，特别是跨城市、州和国家层面的规划，需要协调城市、郊区和农村地区的复杂需求。", "method": "DOVA-PATBM是一个地理计算框架，它将异构数据（道路、人口、夜间灯光、POI和馈线）栅格化到分层H3网格上。该方法使用区域归一化的图神经网络中心性模型推断交叉路口的重要性，并叠加Voronoi Tessellation以确保每30公里半径内至少有一个五端口直流快速充电器。 hourly arrival profiles, learned from loop-detector and floating-car traces, feed a finite M/M/c queue to size ports under feeder-capacity and outage-risk constraints. A greedy maximal-coverage heuristic with income-weighted penalties then selects the minimum number of sites that satisfy coverage and equity targets.", "result": "应用于美国佐治亚州，DOVA-PATBM将30公里瓦片覆盖率提高了12个百分点；将低收入居民到最近充电器的平均距离减半；在所有地方都满足了次输电净空；同时在计算上对于国家规模的部署仍具有可行性。", "conclusion": "紧密集成、GNN驱动的多分辨率方法可以弥合学术优化与可部署基础设施政策之间的鸿沟。", "translation": "电池电动汽车的加速普及需要既数据丰富又地理上可扩展的基础设施规划工具。尽管大多数先前的研究都针对单一城市优化充电位置，但州级和国家级网络必须协调密集大都市核心区、依赖汽车的远郊区以及电力受限的农村走廊之间相互冲突的需求。\n我们提出了DOVA-PATBM（Deployment Optimisation with Voronoi-oriented, Adaptive, POI-Aware Temporal Behaviour Model），这是一个将这些情境统一在一个单一管道中的地理计算框架。该方法将异构数据（道路、人口、夜间灯光、兴趣点和馈线）栅格化到分层H3网格上，使用区域归一化的图神经网络中心性模型推断交叉路口的重要性，并叠加Voronoi Tessellation，以确保在每30公里半径内至少有一个五端口直流快速充电器。从环路检测器和浮动汽车轨迹学习到的每小时到达配置文件，输入到一个有限的M/M/c排队模型中，以便在馈线容量和停电风险约束下确定端口规模。然后，一个带有收入加权惩罚的贪婪最大覆盖启发式算法选择满足覆盖率和公平性目标的最小站点数量。\n应用于美国佐治亚州，DOVA-PATBM (i) 将30公里瓦片覆盖率提高了12个百分点，(ii) 将低收入居民到最近充电器的平均距离减半，以及 (iii) 在所有地方都满足了次输电净空——所有这些都同时保持了国家规模部署的计算可行性。这些结果表明，一种紧密集成、GNN驱动的多分辨率方法可以弥合学术优化与可部署基础设施政策之间的鸿沟。", "summary": "本文提出了DOVA-PATBM，一个智能、自适应且可扩展的地理计算框架，用于优化大规模电动汽车充电基础设施。该框架通过将异构数据栅格化到分层H3网格上，并结合图神经网络、Voronoi Tessellation和M/M/c排队模型来确定充电站点和端口规模，同时考虑馈线容量、中断风险、覆盖率和公平性目标。在佐治亚州的应用表明，DOVA-PATBM显著提高了充电覆盖率，减少了低收入居民的出行距离，并满足了电网限制，同时保持了计算效率，证明了其在实际部署中的潜力。", "keywords": "电动汽车充电, 基础设施优化, 地理计算, 图神经网络, Voronoi Tessellation", "comments": "该论文的创新点在于整合了多源异构数据和先进的地理计算方法（如H3网格、GNN、Voronoi Tessellation和排队论），以解决大规模电动汽车充电基础设施规划的复杂性。其重要性在于其可扩展性以及对公平性（特别关注低收入居民）和电网限制的考虑，使研究结果更具实际应用价值。该框架为实现更高效、公平和电网友好的电动汽车充电基础设施部署提供了有力的工具。"}}
{"id": "2506.15676", "title": "Gender-Neutral Machine Translation Strategies in Practice", "authors": ["Hillary Dawkins", "Isar Nejadgholi", "Chi-kiu Lo"], "summary": "Gender-inclusive machine translation (MT) should preserve gender ambiguity in\nthe source to avoid misgendering and representational harms. While gender\nambiguity often occurs naturally in notional gender languages such as English,\nmaintaining that gender neutrality in grammatical gender languages is a\nchallenge. Here we assess the sensitivity of 21 MT systems to the need for\ngender neutrality in response to gender ambiguity in three translation\ndirections of varying difficulty. The specific gender-neutral strategies that\nare observed in practice are categorized and discussed. Additionally, we\nexamine the effect of binary gender stereotypes on the use of gender-neutral\ntranslation. In general, we report a disappointing absence of gender-neutral\ntranslations in response to gender ambiguity. However, we observe a small\nhandful of MT systems that switch to gender neutral translation using specific\nstrategies, depending on the target language.", "comment": "to appear at GITT 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15676v1", "AI": {"title_translation": "机器翻译中性化策略的实践", "tldr": "该研究评估了21个机器翻译系统在处理性别模糊时的中性化表现，发现普遍缺乏中性翻译，但少数系统会采用特定策略。", "motivation": "为避免性别误认和表征性伤害，性别包容的机器翻译应保留源语言中的性别模糊性。然而，在语法性别语言中保持性别中立性是一个挑战，因此需要评估现有机器翻译系统的表现。", "method": "评估了21个机器翻译系统在三种不同难度翻译方向上对性别模糊所需的中性化的敏感性。对实际观察到的性别中性策略进行了分类和讨论。此外，还研究了二元性别刻板印象对性别中性翻译使用的影响。", "result": "普遍存在令人失望的缺乏性别中性翻译以应对性别模糊的情况。然而，观察到少数机器翻译系统会根据目标语言使用特定策略切换到性别中性翻译。", "conclusion": "尽管大多数机器翻译系统在处理性别模糊时未能提供性别中性翻译，但少数系统展示了使用特定策略实现性别中性翻译的能力，这表明未来改进的可能性。", "translation": "性别包容的机器翻译（MT）应保留源语言中的性别模糊性，以避免性别误认和表征性伤害。虽然性别模糊性在英语等概念性别语言中自然发生，但在语法性别语言中保持性别中立性是一个挑战。本文评估了21个机器翻译系统在三种不同难度翻译方向上对性别模糊所需的中性化的敏感性。对实际观察到的特定性别中性策略进行了分类和讨论。此外，我们还研究了二元性别刻板印象对性别中性翻译使用的影响。总的来说，我们报告了在应对性别模糊时，性别中性翻译普遍缺失，这令人失望。然而，我们观察到少数机器翻译系统会根据目标语言，使用特定策略切换到性别中性翻译。", "summary": "本研究旨在评估机器翻译系统在处理性别模糊时的性别中立性表现。通过测试21个机器翻译系统在不同翻译方向上的敏感度，并分析其性别中性策略，研究发现多数系统未能有效实现性别中性翻译，表现出令人失望的缺失。但同时，也观察到少数系统能够根据目标语言，采用特定策略进行性别中性翻译。研究还探讨了二元性别刻板印象对性别中性翻译的影响。", "keywords": "机器翻译, 性别中立, 性别模糊, 翻译策略, 性别刻板印象", "comments": "该论文解决了机器翻译领域中一个重要的伦理和技术挑战——性别中立性。其创新之处在于系统性地评估了大量现有MT系统，并识别出实际应用的性别中立策略。尽管结果显示多数系统表现不佳，但发现少数系统的成功案例为未来研究和开发提供了宝贵的洞察和方向。这项工作对于推动更公平、更包容的机器翻译技术具有重要意义。"}}
{"id": "2506.15549", "title": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation", "authors": ["Farheen Ramzan", "Yusuf Kiberu", "Nikesh Jathanna", "Shahnaz Jamil-Copley", "Richard H. Clayton", "Chen", "Chen"], "summary": "Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.", "comment": "14 Pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15549v1", "AI": {"title_translation": "CLAIM：临床指导的LGE增强用于真实和多样化心肌瘢痕合成与分割", "tldr": "提出CLAIM框架，通过临床知识指导的扩散模型生成真实多样的心肌瘢痕图像，以解决LGE图像数据稀缺问题，并提高瘢痕分割模型的鲁棒性。", "motivation": "深度学习在LGE心脏MRI心肌瘢痕分割中潜力巨大，但高质量瘢痕标签的LGE图像可用性和变异性有限，限制了鲁棒分割模型的发展。", "method": "本文引入了CLAIM框架，其核心是SMILE模块（临床知识指导下的瘢痕掩膜生成），该模块以临床上采用的AHA 17段模型为条件，驱动基于扩散的生成器，合成具有解剖学一致性和空间多样性的瘢痕模式。CLAIM还采用联合训练策略，同时优化瘢痕分割网络和生成器，旨在提高合成瘢痕的真实性和心肌瘢痕分割性能的准确性。", "result": "实验结果表明，CLAIM生成了与解剖学一致的瘢痕模式，并且与基线模型相比，与真实瘢痕分布的Dice相似度更高。", "conclusion": "CLAIM方法能够实现可控、真实的心肌瘢痕合成，并已证明对下游医学图像任务有用。", "translation": "基于深度学习的晚期钆增强（LGE）心脏MRI心肌瘢痕分割在结构性心脏病的准确及时诊断和治疗规划方面显示出巨大潜力。然而，高质量瘢痕标签的LGE图像可用性和变异性有限，限制了鲁棒分割模型的发展。为了解决这个问题，我们引入了CLAIM框架：一个用于解剖学基础瘢痕生成和分割的临床指导LGE增强用于真实和多样化心肌瘢痕合成与分割框架。其核心是SMILE模块（临床知识指导下的瘢痕掩膜生成），该模块以临床上采用的AHA 17段模型为条件，驱动基于扩散的生成器，合成具有解剖学一致性和空间多样性的瘢痕模式。此外，CLAIM采用联合训练策略，其中瘢痕分割网络与生成器一起优化，旨在提高合成瘢痕的真实性和心肌瘢痕分割性能的准确性。实验结果表明，CLAIM生成了与解剖学一致的瘢痕模式，并且与基线模型相比，与真实瘢痕分布的Dice相似度更高。我们的方法实现了可控和真实的心肌瘢痕合成，并已证明对下游医学图像任务有用。", "summary": "该论文提出了CLAIM框架，旨在通过临床知识指导的LGE增强来合成真实多样的心肌瘢痕图像，以解决高质量LGE图像数据稀缺的问题，从而提升心肌瘢痕分割模型的鲁棒性。CLAIM的核心是SMILE模块，利用AHA 17段模型指导扩散生成器生成解剖学一致的瘢痕模式。此外，通过联合训练生成器和分割网络，CLAIM提高了合成瘢痕的真实性和分割精度。实验证明，该方法能生成解剖学一致的瘢痕，并获得更高的Dice相似度，对下游医学图像任务具有实用价值。", "keywords": "心肌瘢痕合成, LGE增强, 深度学习, 扩散模型, 图像分割", "comments": "该论文的创新点在于提出了一个结合临床知识指导的扩散模型来合成医学图像的框架，有效地解决了医学影像领域数据稀缺和多样性不足的难题。通过引入AHA 17段模型指导瘢痕生成，保证了合成图像的临床真实性和解剖学一致性，显著提升了下游分割模型的性能和鲁棒性。这种数据增强方法对于推动深度学习在医疗诊断中的应用具有重要意义。"}}
{"id": "2506.15305", "title": "Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance", "authors": ["Qingkai Zhang", "L. Jeff Hong", "Houmin Yan"], "summary": "The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15305v1", "AI": {"title_translation": "供应链金融中基于条件生成模型的信用风险管理增强", "tldr": "本研究提出了一种基于条件生成模型的先进框架，用于3PL主导的供应链金融中的信用风险评估和贷款规模确定，旨在改善中小企业融资。", "motivation": "跨境电商的快速发展为中小企业带来了巨大机遇，但由于信用历史有限，融资仍是中小企业面临的关键挑战。第三方物流（3PL）主导的供应链金融（SCF）作为一种利用在途库存作为抵押品的解决方案应运而生，但需要更先进的信用风险管理框架来解决信用风险评估和贷款规模确定的双重挑战。", "method": "本研究提出了一种先进的信用风险管理框架，专门针对3PL主导的供应链金融。该框架利用基于分位数回归的生成元模型（QRGMM）对销售分布进行条件生成建模，作为风险估计的基础。提出了一个统一的框架，能够灵活估计多种风险度量，并引入了一种函数风险度量公式，系统地捕捉这些风险度量与不同贷款水平之间的关系，并提供理论保证。为了捕捉电商销售数据中复杂的协变量交互，将QRGMM与深度因子分解机（DeepFM）相结合。", "result": "在合成数据和真实世界数据上进行了大量实验，验证了所提出模型在信用风险评估和贷款规模确定方面的有效性。", "conclusion": "本研究代表了生成式AI在跨境电商供应链金融风险管理中的开创性应用，为增强信贷实践和改善中小企业融资提供了坚实基础。", "translation": "跨境电商（CBEC）的快速扩张为中小企业（SMEs）创造了巨大的机遇，但由于中小企业信用历史有限，融资仍然是一个严峻的挑战。第三方物流（3PL）主导的供应链金融（SCF）作为一种有前景的解决方案应运而生，它利用在途库存作为抵押品。我们提出了一种专门针对3PL主导的SCF的先进信用风险管理框架，解决了信用风险评估和贷款规模确定的双重挑战。具体而言，我们利用基于分位数回归的生成元模型（QRGMM）对销售分布进行条件生成建模，作为风险估计的基础。我们提出了一个统一的框架，该框架能够灵活地估计多种风险度量，同时引入了一种函数风险度量公式，该公式在理论保证的支持下系统地捕捉这些风险度量与不同贷款水平之间的关系。为了捕捉电商销售数据中复杂的协变量交互，我们将QRGMM与深度因子分解机（DeepFM）相结合。在合成数据和真实世界数据上进行了大量实验，验证了我们模型在信用风险评估和贷款规模确定方面的有效性。这项研究代表了生成式AI在跨境电商SCF风险管理中的开创性应用，为增强信贷实践和改善中小企业获取资本提供了坚实的基础。", "summary": "本研究提出了一种针对第三方物流（3PL）主导的供应链金融（SCF）的先进信用风险管理框架，以应对中小企业（SMEs）融资挑战。该框架利用基于分位数回归的生成元模型（QRGMM）进行销售分布的条件生成建模，并结合深度因子分解机（DeepFM）来评估信用风险和确定贷款规模。该方法引入了一个统一的框架和函数风险度量公式，能够灵活估计多种风险，并经过合成和真实世界数据的验证，为跨境电商SCF风险管理中生成式AI的应用提供了开创性的解决方案，有助于提升信贷实践和中小企业融资可及性。", "keywords": "信用风险管理, 供应链金融, 条件生成模型, 跨境电商, 中小企业", "comments": "本文创新性地将条件生成模型（QRGMM与DeepFM结合）应用于供应链金融的信用风险管理，尤其是在跨境电商中小企业融资场景下。其提出的统一框架和函数风险度量公式具有理论保证，并能有效处理复杂的电商销售数据，为解决中小企业融资难题提供了新的视角和技术支持，是生成式AI在该领域的一个重要应用。"}}
{"id": "2506.15681", "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.", "comment": "Project page: https://byungkwanlee.github.io/GenRecal-page/", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15681v1", "AI": {"title_translation": "GenRecal：从大型到小型视觉语言模型的校准后生成", "tldr": "GenRecal是一个通用的VLM蒸馏框架，通过引入一个校准器来对齐异构VLM的特征表示，从而在资源受限设备上有效部署小型VLM，并在多个基准测试中超越大型VLM。", "motivation": "当前大型视觉语言模型（VLMs）计算需求高，难以部署在资源受限设备上，且不同VLM架构（基于不同LLM，使用不同令牌类型）之间的知识蒸馏存在挑战。", "method": "提出了一个名为GenRecal的新颖通用蒸馏框架。GenRecal包含一个校准器（Recalibrator），用于对齐和适应异构VLM之间的特征表示，从而实现不同类型VLM之间的有效知识迁移。", "result": "GenRecal显著提高了基线性能，并最终超越了大型开源和闭源VLM。", "conclusion": "GenRecal框架通过解决异构VLM之间的知识蒸馏挑战，成功实现了从大型到小型VLM的高效知识迁移，从而在资源受限设备上实现了高性能部署。", "translation": "视觉语言模型（VLMs）的最新进展利用大型语言模型（LLMs）实现了与GPT-4V等闭源系统相当的性能。然而，由于其巨大的计算需求，在实际场景中部署这些模型，特别是在资源受限设备上，仍然具有挑战性。这激发了人们将大型VLM的知识蒸馏到更小、更高效模型中的兴趣。这里出现的一个关键挑战是VLM架构的多样性，它们建立在不同的LLM之上，并采用不同的令牌类型——在词汇量大小、令牌分割和令牌索引顺序上有所不同。为了解决这种对特定VLM类型限制的挑战，我们提出了校准后生成（GenRecal），一个新颖的、通用的VLM蒸馏框架。GenRecal包含一个校准器，用于对齐和适应异构VLM之间的特征表示，从而实现不同类型VLM之间的有效知识迁移。通过在多个具有挑战性的基准测试中进行大量实验，我们证明GenRecal显著提高了基线性能，最终超越了大型开源和闭源VLM。", "summary": "GenRecal是一个新颖的通用视觉语言模型（VLM）蒸馏框架，旨在解决大型VLM在资源受限设备上的部署难题以及异构VLM之间知识迁移的挑战。该框架引入了一个校准器，能够对齐和适应不同VLM架构之间的特征表示，从而实现高效的知识转移。实验结果表明，GenRecal显著提升了基线性能，并超越了现有的大型开源和闭源VLM。", "keywords": "视觉语言模型, 知识蒸馏, 模型压缩, GenRecal, 特征对齐", "comments": "GenRecal的创新之处在于其通用的蒸馏框架和引入的校准器，有效解决了VLM架构多样性带来的知识迁移难题，使其能够跨越不同VLM类型进行有效蒸馏。这对于在边缘设备上部署高效VLM具有重要意义。"}}
{"id": "2506.14861", "title": "BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic Foundation Models", "authors": ["Bharath Dandala", "Michael M. Danziger", "Ella Barkan", "Tanwi Biswas", "Viatcheslav Gurev", "Jianying Hu", "Matthew Madgwick", "Akira Koseki", "Tal Kozlovski", "Michal Rosen-Zvi", "Yishai Shimoni", "Ching-Huei Tsou"], "summary": "Transcriptomic foundation models (TFMs) have recently emerged as powerful\ntools for analyzing gene expression in cells and tissues, supporting key tasks\nsuch as cell-type annotation, batch correction, and perturbation prediction.\nHowever, the diversity of model implementations and training strategies across\nrecent TFMs, though promising, makes it challenging to isolate the contribution\nof individual design choices or evaluate their potential synergies. This\nhinders the field's ability to converge on best practices and limits the\nreproducibility of insights across studies. We present BMFM-RNA, an\nopen-source, modular software package that unifies diverse TFM pretraining and\nfine-tuning objectives within a single framework. Leveraging this capability,\nwe introduce a novel training objective, whole cell expression decoder (WCED),\nwhich captures global expression patterns using an autoencoder-like CLS\nbottleneck representation. In this paper, we describe the framework, supported\ninput representations, and training objectives. We evaluated four model\ncheckpoints pretrained on CELLxGENE using combinations of masked language\nmodeling (MLM), WCED and multitask learning. Using the benchmarking\ncapabilities of BMFM-RNA, we show that WCED-based models achieve performance\nthat matches or exceeds state-of-the-art approaches like scGPT across more than\na dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available\nas part of the biomed-multi-omics project (\nhttps://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible\nfoundation for systematic benchmarking and community-driven exploration of\noptimal TFM training strategies, enabling the development of more effective\ntools to leverage the latest advances in AI for understanding cell biology.", "comment": null, "cate": "q-bio.GN", "url": "http://arxiv.org/abs/2506.14861v1", "AI": {"title_translation": "BMFM-RNA：一个用于构建和评估转录组基础模型的开放框架", "tldr": "BMFM-RNA是一个开放框架，统一了转录组基础模型的训练策略，并引入了新型WCED目标，其性能媲美或超越现有最先进方法。", "motivation": "现有转录组基础模型（TFMs）的实现和训练策略多样性，使得难以评估单一设计选择的贡献或协同效应，阻碍了最佳实践的形成和研究的可复现性。", "method": "本文提出了BMFM-RNA，一个开源、模块化的软件包，它在一个单一框架内统一了多样化的TFM预训练和微调目标。引入了新的训练目标——全细胞表达解码器（WCED），使用类似自编码器的CLS瓶颈表示来捕获全局表达模式。作者评估了在CELLxGENE上预训练的四个模型检查点，这些检查点结合了掩码语言建模（MLM）、WCED和多任务学习。", "result": "利用BMFM-RNA的基准测试能力，作者表明基于WCED的模型在零样本和微调任务中，在十多个数据集上实现了与scGPT等最先进方法媲美或超越的性能。", "conclusion": "BMFM-RNA为转录组基础模型系统基准测试和社区驱动的最佳训练策略探索提供了可复现的基础，从而能够开发更有效的工具，利用AI的最新进展来理解细胞生物学。", "translation": "转录组基础模型（TFMs）最近已成为分析细胞和组织中基因表达的强大工具，支持细胞类型注释、批次校正和扰动预测等关键任务。然而，近期TFMs模型实现和训练策略的多样性，尽管前景广阔，但使得难以分离单个设计选择的贡献或评估其潜在协同效应。这阻碍了该领域形成最佳实践的能力，并限制了跨研究结果的可复现性。我们提出了BMFM-RNA，一个开源的模块化软件包，它在一个单一框架内统一了多样化的TFM预训练和微调目标。利用这一能力，我们引入了一种新颖的训练目标——全细胞表达解码器（WCED），它使用类似自编码器的CLS瓶颈表示来捕获全局表达模式。在本文中，我们描述了该框架、支持的输入表示和训练目标。我们评估了在CELLxGENE上预训练的四个模型检查点，这些检查点结合了掩码语言建模（MLM）、WCED和多任务学习。利用BMFM-RNA的基准测试能力，我们表明基于WCED的模型在零样本和微调任务中，在十多个数据集上实现了与scGPT等最先进方法媲美或超越的性能。BMFM-RNA作为biomed-multi-omics项目的一部分提供（https://github.com/BiomedSciAI/biomed-multi-omic），为系统基准测试和社区驱动的最佳TFM训练策略探索提供了可复现的基础，从而能够开发更有效的工具，利用AI的最新进展来理解细胞生物学。", "summary": "本文提出了BMFM-RNA，一个开源、模块化的框架，旨在统一转录组基础模型（TFMs）的预训练和微调目标，以解决现有模型多样性导致的评估和复现性挑战。BMFM-RNA引入了新颖的WCED训练目标，并展示了其在多项基因表达分析任务中与现有最先进方法媲美或超越的性能。该框架为TFMs的系统基准测试和最佳策略探索提供了可复现的基础。", "keywords": "转录组基础模型, BMFM-RNA, WCED, 基因表达分析, 开源框架", "comments": "BMFM-RNA的创新之处在于其开放、模块化的框架设计，能够统一和评估不同的TFM训练策略，解决了领域内模型多样性导致的可复现性挑战。WCED的引入也展示了捕获全局表达模式的有效性。该工具对推动转录组基础模型的标准化和社区协作具有重要意义。"}}
{"id": "2506.15307", "title": "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language Models Using Forward-Only Passes", "authors": ["Jinglong Luo", "Zhuo Zhang", "Yehong Zhang", "Shiyu Liu", "Ye Dong", "Xun Zhou", "Hui Wang", "Yue Yu", "Zenglin Xu"], "summary": "Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15307v1", "AI": {"title_translation": "SecFwT：使用仅前向传播实现大型语言模型高效隐私保护微调", "tldr": "SecFwT提出了一种基于MPC的框架，通过仅前向传播和随机特征注意力，实现大型语言模型的高效隐私保护微调，解决了传统反向传播和优化器计算开销大的问题。", "motivation": "大型语言模型在隐私敏感领域（如医疗和金融）的应用受到训练数据稀缺的限制，因为严格的隐私要求导致数据难以获取。虽然安全多方计算（MPC）可以保护模型参数和用户数据，但其在LLM上的应用主要限于推理，微调面临巨大的计算挑战，尤其是在隐私保护的反向传播和优化器操作中。", "method": "本文提出了SecFwT框架，这是首个基于MPC的LLM高效隐私保护微调框架。SecFwT引入了“仅前向调优”范式，以消除反向传播和优化器计算；并采用MPC友好的随机特征注意力来近似softmax注意力，显著减少了昂贵的非线性操作和计算复杂度。", "result": "实验结果表明，SecFwT在效率和隐私保护方面实现了显著改进，能够为隐私关键型应用实现LLM的可扩展和安全微调。", "conclusion": "SecFwT通过创新的仅前向调优范式和MPC友好的随机特征注意力，成功解决了大型语言模型隐私保护微调中的计算挑战，实现了高效且可扩展的隐私保护微调。", "translation": "大型语言模型（LLMs）已经改变了众多领域，然而它们在隐私敏感领域（如医疗保健和金融）适应专业任务的能力受到严格隐私要求导致可访问训练数据稀缺的限制。基于安全多方计算（MPC）的隐私保护机器学习提供了一种强大的方法来保护模型参数和用户数据，但其在LLMs上的应用主要限于推理，因为微调带来了显著的计算挑战，特别是在隐私保护的反向传播和优化器操作中。本文识别了基于MPC的LLM隐私保护微调的两个主要障碍：（1）反向传播和优化器过程的巨大计算开销，以及（2）MPC设置中基于softmax的注意力机制效率低下。为了解决这些挑战，我们提出了SecFwT，这是首个为高效、隐私保护LLM微调设计的基于MPC的框架。SecFwT引入了仅前向调优范式，以消除反向和优化器计算，并采用MPC友好的随机特征注意力来近似softmax注意力，显著减少了昂贵的非线性操作和计算复杂度。实验结果表明，SecFwT在效率和隐私保护方面实现了显著改进，能够为隐私关键型应用实现LLM的可扩展和安全微调。", "summary": "本文提出了SecFwT，一个基于安全多方计算（MPC）的框架，旨在解决大型语言模型（LLMs）在隐私敏感领域微调时面临的计算挑战。针对传统反向传播和优化器操作的巨大开销以及softmax注意力机制的低效性，SecFwT引入了“仅前向调优”范式和MPC友好的随机特征注意力。实验证明，SecFwT显著提高了LLM隐私保护微调的效率和安全性，使其适用于隐私关键型应用。", "keywords": "隐私保护, 大型语言模型, 微调, 安全多方计算, 仅前向传播", "comments": "SecFwT的创新之处在于其“仅前向调优”范式，这彻底规避了传统反向传播和优化器在MPC环境中的高昂计算成本，从而使得LLM的隐私保护微调成为可能。此外，使用MPC友好的随机特征注意力替代softmax注意力也是一个关键的贡献，它进一步优化了计算效率。这项工作对于推动LLM在医疗、金融等强隐私保护领域落地具有重要意义。"}}
{"id": "2506.15563", "title": "Control and Realism: Best of Both Worlds in Layout-to-Image without Training", "authors": ["Bonan Li", "Yinhan Hu", "Songhua Liu", "Xinchao Wang"], "summary": "Layout-to-Image generation aims to create complex scenes with precise control\nover the placement and arrangement of subjects. Existing works have\ndemonstrated that pre-trained Text-to-Image diffusion models can achieve this\ngoal without training on any specific data; however, they often face challenges\nwith imprecise localization and unrealistic artifacts. Focusing on these\ndrawbacks, we propose a novel training-free method, WinWinLay. At its core,\nWinWinLay presents two key strategies, Non-local Attention Energy Function and\nAdaptive Update, that collaboratively enhance control precision and realism. On\none hand, we theoretically demonstrate that the commonly used attention energy\nfunction introduces inherent spatial distribution biases, hindering objects\nfrom being uniformly aligned with layout instructions. To overcome this issue,\nnon-local attention prior is explored to redistribute attention scores,\nfacilitating objects to better conform to the specified spatial conditions. On\nthe other hand, we identify that the vanilla backpropagation update rule can\ncause deviations from the pre-trained domain, leading to out-of-distribution\nartifacts. We accordingly introduce a Langevin dynamics-based adaptive update\nscheme as a remedy that promotes in-domain updating while respecting layout\nconstraints. Extensive experiments demonstrate that WinWinLay excels in\ncontrolling element placement and achieving photorealistic visual fidelity,\noutperforming the current state-of-the-art methods.", "comment": "Accepted by ICML2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15563v1", "AI": {"title_translation": "控制与真实感：无需训练的布局到图像生成中的两全其美", "tldr": "本文提出了一种名为WinWinLay的无需训练的布局到图像生成方法，通过引入非局部注意力能量函数和自适应更新策略，有效解决了现有方法在定位不精确和生成不真实伪影方面的挑战，显著提升了控制精度和图像真实感。", "motivation": "现有的无需训练的文本到图像扩散模型在布局到图像生成中存在定位不精确和产生不真实伪影的问题。", "method": "本文提出了一种新颖的无需训练的布局到图像生成方法WinWinLay。该方法包含两个核心策略：1. 非局部注意力能量函数：通过探索非局部注意力先验来重新分配注意力分数，以克服常用注意力能量函数引入的固有空间分布偏差，使对象更好地符合指定的空间条件。2. 自适应更新：引入基于Langevin动力学的自适应更新方案，以解决香草反向传播更新规则导致的偏离预训练域并产生域外伪影的问题，促进域内更新同时尊重布局约束。", "result": "广泛的实验表明，WinWinLay在控制元素放置和实现逼真视觉保真度方面表现出色，其性能优于当前最先进的方法。", "conclusion": "WinWinLay作为一种无需训练的布局到图像生成方法，成功地解决了现有方法的定位不精确和不真实伪影问题，显著提升了生成图像的控制精度和真实感。", "translation": "布局到图像生成旨在创建具有精确控制主体放置和排列的复杂场景。现有工作表明，预训练的文本到图像扩散模型无需在任何特定数据上训练即可实现此目标；然而，它们经常面临定位不精确和不真实伪影的挑战。针对这些缺点，我们提出了一种新颖的无需训练的方法，WinWinLay。WinWinLay的核心是提出两种关键策略：非局部注意力能量函数和自适应更新，它们协同增强了控制精度和真实感。一方面，我们从理论上证明，常用的注意力能量函数引入了固有的空间分布偏差，阻碍了对象与布局指令的均匀对齐。为了克服这个问题，我们探索了非局部注意力先验来重新分配注意力分数，从而促进对象更好地符合指定的空间条件。另一方面，我们发现香草反向传播更新规则可能导致偏离预训练领域，从而导致域外伪影。因此，我们引入了一种基于Langevin动力学的自适应更新方案作为补救措施，该方案在尊重布局约束的同时促进域内更新。广泛的实验表明，WinWinLay在控制元素放置和实现逼真视觉保真度方面表现出色，优于当前最先进的方法。", "summary": "本文提出了一种名为WinWinLay的无需训练的布局到图像生成方法，旨在解决现有方法在控制精度和图像真实感方面的不足。WinWinLay通过引入非局部注意力能量函数来纠正空间分布偏差，并采用基于Langevin动力学的自适应更新策略来避免域外伪影，从而显著提升了生成图像的定位准确性和视觉质量，并在实验中超越了现有SOTA方法。", "keywords": "布局到图像生成, 无需训练, 扩散模型, 注意力机制, 图像生成", "comments": "这篇论文的创新点在于提出了两种无需训练的策略来解决布局到图像生成中的核心问题：定位不精确和真实感不足。通过理论分析和实际验证，它有效地提升了预训练模型在特定任务上的表现，避免了昂贵的再训练成本，这对于实际应用具有重要意义。"}}
{"id": "2506.14862", "title": "Identifiability by common backdoor in summary causal graphs of time series", "authors": ["Clément Yvernes", "Charles K. Assaad", "Emilie Devijver", "Eric Gaussier"], "summary": "The identifiability problem for interventions aims at assessing whether the\ntotal effect of some given interventions can be written with a do-free formula,\nand thus be computed from observational data only. We study this problem,\nconsidering multiple interventions and multiple effects, in the context of time\nseries when only abstractions of the true causal graph in the form of summary\ncausal graphs are available. We focus in this study on identifiability by a\ncommon backdoor set, and establish, for time series with and without\nconsistency throughout time, conditions under which such a set exists. We also\nprovide algorithms of limited complexity to decide whether the problem is\nidentifiable or not.", "comment": null, "cate": "math.ST", "url": "http://arxiv.org/abs/2506.14862v1", "AI": {"title_translation": "时间序列摘要因果图中通过共同后门的可识别性", "tldr": "本文研究了在时间序列摘要因果图中，通过共同后门集合识别多重干预总效应的问题，并提供了判断可识别性的算法。", "motivation": "干预的可识别性问题旨在评估给定干预的总效应是否可以用无“do”公式表示，从而仅根据观测数据计算。本文在时间序列背景下，当只有真实因果图的抽象形式——摘要因果图可用时，研究了多重干预和多重效应的该问题。", "method": "本研究侧重于通过共同后门集的可识别性，并为具有或不具有时间一致性的时间序列建立了此类集合存在的条件。此外，还提供了复杂度有限的算法来判断问题是否可识别。", "result": "建立了在时间序列中通过共同后门集进行可识别的条件，并提供了复杂度有限的算法来判断问题是否可识别。", "conclusion": "本文成功地为时间序列中通过共同后门集的可识别性建立了条件，并开发了判断可识别性的算法。", "translation": "干预的可识别性问题旨在评估给定干预的总效应是否可以用无“do”公式表示，从而仅根据观测数据计算。我们研究了这个问题，考虑了时间序列背景下的多重干预和多重效应，此时只有真实因果图的抽象形式——摘要因果图可用。本研究侧重于通过共同后门集的可识别性，并为具有或不具有时间一致性的时间序列建立了此类集合存在的条件。我们还提供了复杂度有限的算法来判断问题是否可识别。", "summary": "本文研究了在时间序列背景下，当仅有摘要因果图可用时，多重干预和多重效应的可识别性问题。研究重点是通过共同后门集实现可识别性，并为具有或不具有时间一致性的时间序列建立了此类集合存在的条件。此外，论文还提供了复杂度有限的算法来判断该问题是否可识别。", "keywords": "可识别性, 共同后门, 摘要因果图, 时间序列, 因果推断", "comments": "这项研究通过关注时间序列中的摘要因果图和共同后门集，为因果推断领域的可识别性问题提供了新的见解。其创新点在于将传统的可识别性问题扩展到更复杂的时间序列和抽象图结构，并提供了实用的算法，对于从观测数据中进行因果效应估计具有重要意义。"}}
{"id": "2506.15309", "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation", "authors": ["Júlia Vilalta-Mor", "Alexis Molina", "Laura Ortega Varga", "Isaac Filella-Merce", "Victor Guallar"], "summary": "Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.", "comment": "16 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15309v1", "AI": {"title_translation": "主动学习引导的Seq2Seq变分自编码器用于多靶点抑制剂生成", "tldr": "本文提出了一种主动学习引导的Seq2Seq变分自编码器（VAE）方法，以有效生成多样化的多靶点抑制剂候选物，解决了药物发现中的挑战。", "motivation": "同时优化针对多个治疗靶点的分子仍然是药物发现中的一个重大挑战，特别是由于稀疏奖励和相互冲突的设计约束。", "method": "作者提出了一种结构化的主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）整合到迭代循环中。该方法通过在扩展潜在空间的化学可行区域和根据日益严格的多靶点对接阈值逐步约束分子之间交替进行，并策略性地放置化学过滤器。", "result": "在一项针对三种相关冠状病毒主要蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的概念验证研究中，他们的方法有效地生成了一组结构多样化的泛抑制剂候选物。化学过滤器的精心时机和策略性放置显著增强了对有益化学空间的探索。", "conclusion": "该框架将稀疏奖励、多目标药物设计问题转化为可行的计算任务，并为有效探索复杂的药物发现领域提供了一个可推广的路线图。", "translation": "同时优化针对多个治疗靶点的分子仍然是药物发现中的一个重大挑战，特别是由于稀疏奖励和相互冲突的设计约束。我们提出了一种结构化的主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）整合到迭代循环中，旨在平衡化学多样性、分子质量和多靶点亲和力。我们的方法在扩展潜在空间的化学可行区域和根据日益严格的多靶点对接阈值逐步约束分子之间交替进行。在一项针对三种相关冠状病毒主要蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的概念验证研究中，我们的方法有效地生成了一组结构多样化的泛抑制剂候选物。我们证明，在此主动学习流程中，化学过滤器的精心时机和策略性放置显著增强了对有益化学空间的探索，将稀疏奖励、多目标药物设计问题转化为可行的计算任务。因此，我们的框架为有效探索复杂的多药理学领域提供了一个可推广的路线图。", "summary": "本文介绍了一种主动学习引导的Seq2Seq变分自编码器（VAE）框架，旨在解决多靶点药物发现的挑战。该方法通过探索潜在空间和应用严格的对接约束，迭代地平衡化学多样性和多靶点亲和力。在冠状病毒蛋白酶上的概念验证研究表明，它能有效生成多样化的泛抑制剂候选物，证明其能够将复杂的、稀疏奖励的药物设计问题转化为可管理的计算任务。", "keywords": "主动学习, Seq2Seq VAE, 多靶点抑制剂, 药物发现, 多药理学", "comments": "本文创新性地将主动学习与Seq2Seq VAE相结合用于多靶点药物设计，这与当前多药理学面临的挑战高度相关。在AL流程中策略性地使用化学过滤器来指导化学空间探索，特别具有洞察力，为克服稀疏奖励和冲突设计约束提供了稳健的方法。其为探索复杂多药理学领域提供的可推广路线图是一项重要贡献。"}}
{"id": "2506.15564", "title": "Show-o2: Improved Native Unified Multimodal Models", "authors": ["Jinheng Xie", "Zhenheng Yang", "Mike Zheng Shou"], "summary": "This paper presents improved native unified multimodal models, \\emph{i.e.,}\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15564v1", "AI": {"title_translation": "Show-o2: 改进的原生统一多模态模型", "tldr": "Show-o2是一种新的统一多模态模型，结合自回归建模和流匹配，能在图像、视频和文本上进行理解和生成。", "motivation": "论文旨在提出并改进原生统一多模态模型Show-o2，以实现跨模态（文本、图像、视频）的有效理解和生成。", "method": "Show-o2模型利用自回归建模和流匹配，并基于3D因果变分自编码器空间构建统一的视觉表示，通过空间（-时间）融合的双路径实现。自回归建模和流匹配分别应用于语言头和流头，并设计了两阶段训练方案。", "result": "Show-o2模型在处理广泛的多模态理解和生成任务上表现出多功能性，能够有效处理文本、图像和视频等不同模态。", "conclusion": "Show-o2通过结合自回归建模和流匹配，并在3D因果变分自编码器空间上构建统一视觉表示，成功构建了改进的原生统一多模态模型，能够有效处理多种模态的理解和生成任务。", "translation": "这篇论文提出了改进的原生统一多模态模型，即Show-o2，它利用自回归建模和流匹配。基于3D因果变分自编码器空间，通过空间（-时间）融合的双路径构建统一的视觉表示，从而实现图像和视频模态的可扩展性，同时确保有效的多模态理解和生成。基于语言模型，自回归建模和流匹配分别原生应用于语言头和流头，以促进文本标记预测和图像/视频生成。设计了两阶段训练方案，以有效地学习并扩展到更大的模型。最终的Show-o2模型在处理跨文本、图像和视频等不同模态的广泛多模态理解和生成任务中表现出多功能性。代码和模型已在https://github.com/showlab/Show-o 发布。", "summary": "本文介绍了Show-o2，一种改进的原生统一多模态模型，它结合了自回归建模和流匹配。该模型在3D因果变分自编码器空间中构建统一视觉表示，通过双路径空间融合实现图像和视频的可扩展性。Show-o2能对文本、图像和视频进行理解和生成，并通过两阶段训练方案有效地扩展到大型模型，在多模态任务中表现出多功能性。", "keywords": "多模态模型, 自回归建模, 流匹配, 统一表示, 图像视频生成", "comments": "Show-o2通过结合自回归建模和流匹配，并在3D VAE空间中构建统一视觉表示，为统一多模态模型提供了一种创新方法。其双路径空间融合设计提升了图像和视频模态的可扩展性与理解生成能力。该模型展现了在多种模态任务上的通用性，为未来的多模态研究奠定了基础。"}}
{"id": "2506.15329", "title": "When and How Unlabeled Data Provably Improve In-Context Learning", "authors": ["Yingcong Li", "Xiangyu Chang", "Muti Kara", "Xiaofeng Liu", "Amit Roy-Chowdhury", "Samet Oymak"], "summary": "Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15329v1", "AI": {"title_translation": "无标签数据何时以及如何能证明性地改进上下文学习", "tldr": "本研究理论证明了多层或循环Transformer模型能有效利用无标签数据改进上下文学习，而单层模型不能，并通过实验验证了其在半监督表格学习中的有效性。", "motivation": "为了阐明上下文学习（ICL）在演示数据标签缺失或不正确时仍能有效工作的能力，特别是无标签数据如何提供帮助。", "method": "通过对二元高斯混合模型（GMM）设置下的演示数据进行理论研究，分析了单层线性注意力模型与多层或循环Transformer模型利用无标签数据的能力。研究将模型隐式构建的估计器与期望最大化（EM）算法联系起来，并提出通过循环现成的表格基础模型来增强其半监督能力，最后在真实世界数据集上进行评估。", "result": "1. 单层线性注意力模型的损失景观能恢复最优全监督估计器，但完全未能利用无标签数据。2. 多层或循环Transformer模型能通过隐式构建特定形式的估计器有效利用无标签数据。3. 这些估计器中的主导多项式幂与深度呈指数关系，表明适度的深度或循环就足够了。4. 将理论应用于表格基础模型后，在真实世界数据集上的广泛评估显示，该方法显著提升了半监督表格学习性能，优于标准的单次推理。", "conclusion": "多层或循环Transformer模型能够有效利用无标签数据来改进上下文学习，其能力与模型深度呈指数关系。将此理论应用于表格基础模型可以显著提高半监督学习性能。", "translation": "最近的研究表明，即使演示数据标签缺失或不正确，上下文学习（ICL）也能有效。为了阐明这种能力，我们研究了一个典型设置：演示数据根据二元高斯混合模型（GMM）抽取，并且其中一部分演示数据标签缺失。我们提供了一项全面的理论研究，表明：(1) 单层线性注意力模型的损失景观能够恢复最优的全监督估计器，但完全未能利用无标签数据；(2) 相比之下，多层或循环Transformer模型可以通过隐式构建形式为 $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ 的估计器来有效利用无标签数据，其中 $X$ 和 $y$ 分别表示特征和部分观测标签（缺失项设为零）。我们刻画了可以表示为深度函数的这类多项式，并将其与期望最大化（Expectation Maximization，一种半监督学习中常用的迭代伪标签算法）联系起来。重要的是，主导多项式幂与深度呈指数关系，因此适度的深度/循环就足够了。作为理论的一个应用，我们提出循环现成的表格基础模型以增强其半监督能力。在真实世界数据集上的广泛评估表明，我们的方法显著提高了半监督表格学习性能，优于标准的单次推理。", "summary": "本研究深入探讨了上下文学习（ICL）中无标签数据的作用。理论分析表明，单层线性注意力模型无法利用无标签数据，而多层或循环Transformer模型能够通过隐式构建特定形式的估计器来有效利用无标签数据，其能力与模型深度呈指数关系。研究将这一机制与期望最大化（EM）算法联系起来。作为理论应用，作者提出通过循环操作增强现有表格基础模型的半监督能力，并在真实数据集上验证了其显著优于标准单次推理的性能。", "keywords": "上下文学习, 无标签数据, 半监督学习, Transformer, 理论分析", "comments": "该论文通过严谨的理论分析，揭示了多层Transformer模型在上下文学习中利用无标签数据的内在机制，填补了该领域的一个理论空白。将模型深度与利用无标签数据的能力（与EM算法的联系）量化，并指出适度深度即可实现显著提升，具有重要理论意义。同时，将理论应用于实际的表格基础模型并取得显著性能提升，也展示了其重要的实践价值和创新性。"}}
{"id": "2506.15565", "title": "Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification", "authors": ["Junhao Wu", "Aboagye-Ntow Stephen", "Chuyuan Wang", "Gang Chen", "Xin Huang"], "summary": "Ultra-high Spatial Resolution Land Cover Classification is essential for\nfine-grained land cover analysis, yet it remains challenging due to the high\ncost of pixel-level annotations, significant scale variation, and the limited\nadaptability of large-scale vision models. Existing methods typically focus on\n1-meter spatial resolution imagery and rely heavily on annotated data, whereas\npractical applications often require processing higher-resolution imagery under\nweak supervision. To address this, we propose a parameter-efficient\nsemi-supervised segmentation framework for 0.3 m spatial resolution imagery,\nwhich leverages the knowledge of SAM2 and introduces a remote sensing-specific\nFreqWeaver Adapter to enhance fine-grained detail modeling while maintaining a\nlightweight design at only 5.96% of the total model parameters. By effectively\nleveraging unlabeled data and maintaining minimal parameter overhead, the\nproposed method delivers robust segmentation results with superior structural\nconsistency, achieving a 1.78% improvement over existing parameter-efficient\ntuning strategies and a 3.44% gain compared to state-of-the-art high-resolution\nremote sensing segmentation approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15565v1", "AI": {"title_translation": "巴尔的摩地图集：用于半监督超高空间分辨率土地覆盖分类的FreqWeaver适配器", "tldr": "提出了一种参数高效的半监督分割框架Baltimore Atlas，通过引入遥感专用FreqWeaver适配器，在0.3米空间分辨率图像上实现了优于现有方法的土地覆盖分类，解决了标注成本高、尺度变化大和模型适应性差的问题。", "motivation": "超高空间分辨率土地覆盖分类对于精细土地覆盖分析至关重要，但面临像素级标注成本高、尺度变化大以及大型视觉模型适应性有限的挑战。现有方法主要关注1米分辨率图像并高度依赖标注数据，而实际应用常需在弱监督下处理更高分辨率图像。", "method": "提出了一种针对0.3米空间分辨率图像的参数高效半监督分割框架，该框架利用SAM2的知识，并引入了一个遥感专用的FreqWeaver适配器，以增强细粒度细节建模，同时保持轻量化设计，仅占总模型参数的5.96%。", "result": "通过有效利用未标注数据和保持最小参数开销，所提出的方法提供了鲁棒的分割结果，具有卓越的结构一致性，比现有参数高效微调策略提高了1.78%，比最先进的高分辨率遥感分割方法提高了3.44%。", "conclusion": "所提出的Baltimore Atlas框架通过其参数高效的半监督方法和专门的FreqWeaver适配器，有效解决了超高空间分辨率土地覆盖分类的挑战，显著提升了在弱监督下的分类性能和结构一致性。", "translation": "超高空间分辨率土地覆盖分类对于精细的土地覆盖分析至关重要，但由于像素级标注成本高、显著的尺度变化以及大型视觉模型的有限适应性，它仍然具有挑战性。现有方法通常专注于1米空间分辨率图像，并严重依赖标注数据，而实际应用往往需要在弱监督下处理更高分辨率的图像。为了解决这个问题，我们提出了一种针对0.3米空间分辨率图像的参数高效半监督分割框架，该框架利用了SAM2的知识，并引入了一个遥感专用的FreqWeaver适配器，以增强细粒度细节建模，同时保持轻量化设计，仅占总模型参数的5.96%。通过有效利用未标注数据并保持最小的参数开销，所提出的方法提供了鲁棒的分割结果，具有卓越的结构一致性，比现有参数高效微调策略提高了1.78%，比最先进的高分辨率遥感分割方法提高了3.44%。", "summary": "本文提出了一种名为Baltimore Atlas的参数高效半监督分割框架，用于0.3米超高空间分辨率的土地覆盖分类。该方法通过整合SAM2的知识并引入一个轻量级的遥感专用FreqWeaver适配器，有效解决了高分辨率图像标注成本高、尺度变化大以及现有模型适应性差的问题。实验结果表明，该方法在利用未标注数据和保持低参数开销的同时，实现了优越的分割性能和结构一致性，显著超越了现有参数高效微调策略和最先进的高分辨率遥感分割方法。", "keywords": "超高空间分辨率, 土地覆盖分类, 半监督, FreqWeaver适配器, 遥感", "comments": "该论文的创新点在于提出了一个轻量级且参数高效的半监督框架，专门针对超高空间分辨率遥感图像的土地覆盖分类。通过引入遥感专用的FreqWeaver适配器和利用SAM2的知识，解决了高分辨率图像标注困难和模型适应性差的关键挑战，对于实际应用具有重要意义。"}}
{"id": "2506.15330", "title": "Universal Laboratory Model: prognosis of abnormal clinical outcomes based on routine tests", "authors": ["Pavel Karpov", "Ilya Petrenkov", "Ruslan Raiman"], "summary": "Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.", "comment": "7 pages, 2 figues", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15330v1", "AI": {"title_translation": "通用实验室模型：基于常规检测预测异常临床结果", "tldr": "开发了一种基于集合翻译和GPT-like嵌入的方法，用于从常规实验室数据中预测未检测到的异常临床结果，有效处理缺失值并提高了预测准确性。", "motivation": "旨在通过基于已完成的常规实验室检测结果预测未开具检测的异常值，实现早期诊断的普及化。同时，解决临床实验室数据中常见的缺失值问题。", "method": "将表格建模问题表述为集合翻译问题。源集包含GPT类标签列嵌入及其对应值的对，目标集仅包含相同类型的嵌入。该方法能有效处理缺失值，无需隐式估计，并连接了LLM与表格领域。", "result": "将此方法应用于临床实验室数据，在高尿酸、高血糖、高胆固醇和低铁蛋白水平的联合预测中实现了高达8%的AUC提升。", "conclusion": "该研究提出了一种新颖的表格建模方法，通过将表格数据视为集合翻译问题并利用GPT类嵌入，有效解决了临床实验室数据中的缺失值问题，并显著提高了异常临床结果的预测准确性，从而有助于早期诊断的普及。", "translation": "临床实验室结果在任何诊断中都无处不在。基于已完成检测的结果预测未开具检测的异常值看起来很有趣，因为这使得早期诊断可以普及到每个人。血常规（CBC）检测占据特殊地位，因为它是最广泛使用的临床程序。将常规生化指标与血常规结合会呈现一组因患者而异的检测值对，或者在常见设置中，一个带有缺失值的表格。在本文中，我们将表格建模问题表述为一个集合翻译问题，其中源集包含GPT类标签列嵌入及其对应值的对，而目标集仅包含相同类型的嵌入。所提出的方法可以有效地处理缺失值，而无需隐式估计它们，并连接了大型语言模型（LLM）的世界与表格领域。将此方法应用于临床实验室数据，我们在高尿酸、高血糖、高胆固醇和低铁蛋白水平的联合预测中实现了高达8%的AUC提升。", "summary": "这篇论文提出了一种通用实验室模型，旨在通过分析常规实验室检测结果来预测异常临床结果。研究将表格建模问题重新定义为集合翻译问题，利用GPT类标签列嵌入及其对应值来处理数据。这种方法能够有效处理临床数据中常见的缺失值，而无需进行隐式估计，并成功将大型语言模型（LLM）的概念引入到表格数据领域。实验结果表明，该方法在预测高尿酸、高血糖、高胆固醇和低铁蛋白等异常水平方面，其AUC值最高提升了8%。", "keywords": "临床预测, 缺失值处理, 集合翻译, GPT-like嵌入, 早期诊断", "comments": "这篇论文的创新之处在于将表格数据建模问题转化为集合翻译问题，并引入了GPT-like嵌入来处理临床实验室数据。这种方法不仅能够有效处理医疗数据中普遍存在的缺失值问题，而且无需传统的插补方法，显著提升了预测异常临床结果的准确性。其将LLM概念应用于表格数据的能力，也为未来的跨领域研究提供了新的思路，对于实现早期诊断的普及具有重要意义。"}}
{"id": "2506.15577", "title": "A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and Non-Destructive Biomass Estimation from Point Clouds", "authors": ["Di Wang", "Shi Li"], "summary": "Estimating forest above-ground biomass (AGB) is crucial for assessing carbon\nstorage and supporting sustainable forest management. Quantitative Structural\nModel (QSM) offers a non-destructive approach to AGB estimation through 3D tree\nstructural reconstruction. However, current QSM methods face significant\nlimitations, as they are primarily designed for individual trees,depend on\nhigh-quality point cloud data from terrestrial laser scanning (TLS), and also\nrequire multiple pre-processing steps that hinder scalability and practical\ndeployment. This study presents a novel unified framework that enables\nend-to-end processing of large-scale point clouds using an innovative\ngraph-based pipeline. The proposed approach seamlessly integrates tree\nsegmentation,leaf-wood separation and 3D skeletal reconstruction through\ndedicated graph operations including pathing and abstracting for tree topology\nreasoning. Comprehensive validation was conducted on datasets with varying leaf\nconditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and\ndata sources (TLS and UAV-based laser scanning, ULS). Experimental results\ndemonstrate strong performance under challenging conditions, particularly in\nleaf-on scenarios (~20% relative error) and low-density ULS datasets with\npartial coverage (~30% relative error). These findings indicate that the\nproposed framework provides a robust and scalable solution for large-scale,\nnon-destructive AGB estimation. It significantly reduces dependency on\nspecialized pre-processing tools and establishes ULS as a viable alternative to\nTLS. To our knowledge, this is the first method capable of enabling seamless,\nend-to-end 3D tree reconstruction at operational scales. This advancement\nsubstantially improves the feasibility of QSM-based AGB estimation, paving the\nway for broader applications in forest inventory and climate change research.", "comment": "17 pages,19 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15577v1", "AI": {"title_translation": "一种统一的基于图的框架，用于可扩展的三维树木重建和基于点云的无损生物量估算", "tldr": "提出了一种统一的基于图的框架，用于可扩展的三维树木重建和生物量估算，解决了现有QSM方法的可扩展性问题，并在挑战性条件下表现出色。", "motivation": "现有的定量结构模型（QSM）方法在可扩展性和实际部署方面存在显著局限性，它们主要针对单棵树，依赖高质量的地面激光扫描（TLS）数据，并需要多步预处理。", "method": "本研究提出了一种新颖的统一框架，采用创新的基于图的管道，实现大规模点云的端到端处理。该方法通过专用的图操作（包括路径规划和抽象化）进行树木拓扑推理，无缝集成了树木分割、叶木分离和三维骨架重建。", "result": "在不同叶片条件（有叶/无叶）、空间尺度（单棵树/样地级别）和数据源（TLS/ULS）的数据集上进行了全面验证，结果显示在挑战性条件下表现出色，特别是在有叶场景中相对误差约为20%，在低密度、部分覆盖的无人机载激光扫描（ULS）数据集上相对误差约为30%。", "conclusion": "所提出的框架为大规模、无损的地上生物量（AGB）估算提供了一个鲁棒且可扩展的解决方案。它显著降低了对专业预处理工具的依赖，并将无人机载激光扫描（ULS）确立为地面激光扫描（TLS）的可行替代方案，从而大大提高了基于QSM的AGB估算的可行性，为森林清查和气候变化研究的更广泛应用铺平了道路。", "translation": "估算森林地上生物量（AGB）对于评估碳储量和支持可持续森林管理至关重要。定量结构模型（QSM）通过三维树木结构重建提供了一种无损的AGB估算方法。然而，当前的QSM方法面临显著局限性，因为它们主要为单棵树设计，依赖于地面激光扫描（TLS）的高质量点云数据，并且还需要多个预处理步骤，这阻碍了其可扩展性和实际部署。本研究提出了一种新颖的统一框架，该框架利用创新的基于图的管道，能够对大规模点云进行端到端处理。所提出的方法通过专用的图操作（包括路径规划和抽象化）进行树木拓扑推理，无缝集成了树木分割、叶木分离和三维骨架重建。在不同叶片条件（有叶和无叶）、空间尺度（单棵树和样地级别）以及数据源（TLS和无人机载激光扫描，ULS）的数据集上进行了全面验证。实验结果表明，在挑战性条件下表现出色，特别是在有叶场景中（约20%的相对误差）以及低密度、部分覆盖的ULS数据集上（约30%的相对误差）。这些发现表明，所提出的框架为大规模、无损的AGB估算提供了一个鲁棒且可扩展的解决方案。它显著降低了对专业预处理工具的依赖，并将ULS确立为TLS的可行替代方案。据我们所知，这是第一个能够在操作尺度上实现无缝、端到端三维树木重建的方法。这一进展大大提高了基于QSM的AGB估算的可行性，为森林清查和气候变化研究的更广泛应用铺平了道路。", "summary": "当前的三维树木重建和生物量估算方法（QSM）因依赖高质量的地面激光扫描（TLS）数据和繁琐的预处理而缺乏可扩展性。本文提出了一种新颖的统一的基于图的框架，用于大规模点云的端到端处理。该框架利用图操作集成了树木分割、叶木分离和三维骨架重建。在不同条件和数据源（TLS、ULS）下进行验证，该框架展现出强大的性能，尤其是在具有挑战性的有叶场景和低密度无人机载激光扫描（ULS）数据集中。这项进展为地上生物量估算提供了一个可扩展的、无损的解决方案，减少了对预处理的依赖，并验证了ULS作为TLS的可行替代方案，从而扩展了QSM在林业和气候研究中的应用。", "keywords": "地上生物量, 三维树木重建, 点云, 图框架, 无人机载激光扫描", "comments": "该论文提出了一种创新的统一的基于图的框架，实现了大规模点云的端到端处理，显著解决了现有定量结构模型（QSM）方法在可扩展性方面的局限性。其核心创新在于无缝整合了树木分割、叶木分离和三维骨架重建，并通过图操作进行拓扑推理。此外，该方法通过验证无人机载激光扫描（ULS）作为地面激光扫描（TLS）的可行替代方案，大大拓宽了数据来源，降低了对专业预处理工具的依赖，对森林生物量估算和气候变化研究具有重要意义。"}}
{"id": "2506.15337", "title": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations", "authors": ["Naoki Matsumura", "Yuta Yoshimoto", "Yuto Iwasaki", "Meguru Yamazaki", "Yasufumi Sakai"], "summary": "Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15337v1", "AI": {"title_translation": "用于加速高精度神经网络分子动力学模拟的知识蒸馏框架", "tldr": "提出了一种新的知识蒸馏框架，利用未经微调的预训练神经网络作为教师模型，以更低的DFT计算成本生成高精度分子动力学模拟所需的训练数据，同时保持或提高准确性。", "motivation": "传统知识蒸馏方法在材料特定模型中，微调教师模型会增加能垒，导致难以生成包含高能结构的数据，而高能结构对于准确稳定的分子动力学模拟至关重要。", "method": "提出了一种新的知识蒸馏（KD）框架，利用未经微调的现成预训练神经网络作为教师模型。该框架采用两阶段训练过程：首先，学生神经网络使用由现成教师生成的数据集进行训练；然后，使用更小的高精度密度泛函理论（DFT）数据集进行微调。", "result": "该框架在有机（聚乙二醇）和无机（L$_{10}$GeP$_{2}$S$_{12}$）材料上均表现出有效性，在重现物理性质方面达到或优于现有方法的准确性。重要的是，与现有NNP生成方法相比，该方法将昂贵的DFT计算量减少了10倍，且没有牺牲准确性。", "conclusion": "该研究提出了一种高效的知识蒸馏框架，通过利用未经微调的预训练NNP作为教师模型，显著减少了高精度分子动力学模拟所需的昂贵DFT计算量，同时保持了高准确性，解决了传统KD方法在生成高能结构数据方面的挑战。", "translation": "神经网络势能（NNPs）为分子动力学（MD）模拟提供了传统力场的强大替代方案。准确稳定的MD模拟对于评估材料性质至关重要，需要包含低能稳定结构和高能结构的训练数据。传统的知识蒸馏（KD）方法将预训练的NNP微调为教师模型，以生成学生模型的训练数据。然而，在材料特定模型中，这种微调过程增加了能垒，使得难以创建包含高能结构的训练数据。为了解决这个问题，我们提出了一种新颖的KD框架，该框架利用未经微调的、现成的预训练NNP作为教师。其更平缓的能量景观有助于探索更广泛的结构，包括对稳定MD模拟至关重要的高能结构。我们的框架采用两阶段训练过程：首先，学生NNP使用由现成教师生成的数据集进行训练；然后，使用更小的高精度密度泛函理论（DFT）数据集进行微调。我们通过将其应用于有机（聚乙二醇）和无机（L$_{10}$GeP$_{2}$S$_{12}$）材料来证明我们框架的有效性，与现有方法相比，在重现物理性质方面达到了相当或更高的准确性。重要的是，与现有NNP生成方法相比，我们的方法将昂贵的DFT计算量减少了10倍，且没有牺牲准确性。", "summary": "该论文提出了一种新的知识蒸馏（KD）框架，旨在加速高精度神经网络分子动力学模拟。针对传统KD方法在生成高能结构数据方面的挑战，该框架利用未经微调的预训练神经网络作为教师模型，其更平缓的能量景观有助于探索更广泛的结构。通过两阶段训练过程，即学生模型先由现成教师生成的数据训练，再由少量DFT数据微调，该方法在有机和无机材料上均实现了与现有方法相当或更优的准确性，并将昂贵的DFT计算量减少了10倍。", "keywords": "知识蒸馏, 神经网络势能, 分子动力学, DFT计算, 高能结构", "comments": "该论文的创新点在于提出了一个利用未经微调的预训练NNP作为教师模型的知识蒸馏框架，有效解决了传统KD在生成高能结构数据方面的困难。这一方法显著降低了高精度分子动力学模拟所需的计算成本（减少10倍DFT计算），同时保持甚至提高了准确性，对于加速材料科学研究具有重要意义。"}}
{"id": "2506.15591", "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution", "authors": ["Yujing Sun", "Lingchen Sun", "Shuaizheng Liu", "Rongyuan Wu", "Zhengqiang Zhang", "Lei Zhang"], "summary": "It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15591v1", "AI": {"title_translation": "细节丰富且时间一致的视频超分辨率一站式扩散模型", "tldr": "本文提出DLoRAL，一个基于Dual LoRA学习的单步扩散模型，通过结合跨帧检索、一致性LoRA和细节LoRA，同时实现细节丰富和时间一致的视频超分辨率。", "motivation": "在真实世界视频超分辨率（Real-VSR）中，重现丰富的空间细节同时保持时间一致性是一个挑战，尤其是在利用预训练生成模型（如Stable Diffusion, SD）合成逼真细节时。现有的基于SD的Real-VSR方法通常会牺牲空间细节以换取时间连贯性，导致视觉质量不佳。本文认为关键在于如何有效地从低质量（LQ）输入视频中提取对降级鲁棒的时间一致性先验，并在保持这些提取的一致性先验的同时增强视频细节。", "method": "本文提出一种双LoRA学习（DLoRAL）范式来训练一个有效的基于SD的单步扩散模型，以同时实现逼真的帧细节和时间一致性。具体来说，引入了跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一个一致性LoRA（C-LoRA）来从降级输入中学习鲁棒的时间表示。在一致性学习之后，固定CFR和C-LoRA模块，然后训练一个细节LoRA（D-LoRA）来增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。这两个阶段交替迭代优化，协同提供一致且细节丰富的输出。在推理过程中，两个LoRA分支被合并到SD模型中，从而在单个扩散步骤中实现高效和高质量的视频恢复。", "result": "实验表明，DLoRAL在准确性和速度方面都取得了强大的性能。", "conclusion": "DLoRAL通过提出的双LoRA学习范式，结合跨帧检索、一致性LoRA和细节LoRA，成功地在一个扩散步骤中实现了细节丰富且时间一致的视频超分辨率。", "translation": "在真实世界视频超分辨率（Real-VSR）中，重现丰富的空间细节同时保持时间一致性是一个挑战，尤其是在我们利用预训练生成模型（如Stable Diffusion, SD）合成逼真细节时。现有的基于SD的Real-VSR方法通常会牺牲空间细节以换取时间连贯性，导致视觉质量不佳。我们认为关键在于如何有效地从低质量（LQ）输入视频中提取对降级鲁棒的时间一致性先验，并在保持这些提取的一致性先验的同时增强视频细节。为了实现这一点，我们提出了一种双LoRA学习（DLoRAL）范式来训练一个有效的基于SD的单步扩散模型，以同时实现逼真的帧细节和时间一致性。具体来说，我们引入了跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一个一致性LoRA（C-LoRA）来从降级输入中学习鲁棒的时间表示。在一致性学习之后，我们固定CFR和C-LoRA模块，并训练一个细节LoRA（D-LoRA）来增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。这两个阶段交替迭代优化，协同提供一致且细节丰富的输出。在推理过程中，两个LoRA分支被合并到SD模型中，从而在单个扩散步骤中实现高效和高质量的视频恢复。实验表明，DLoRAL在准确性和速度方面都取得了强大的性能。代码和模型可在https://github.com/yjsunnn/DLoRAL 获取。", "summary": "本文提出了DLoRAL，一种用于细节丰富且时间一致的视频超分辨率的单步扩散模型。针对现有基于Stable Diffusion的Real-VSR方法在细节与时间一致性之间权衡的挑战，DLoRAL引入了双LoRA学习范式。该模型利用跨帧检索模块聚合信息，并通过一致性LoRA（C-LoRA）学习时间一致性，随后通过细节LoRA（D-LoRA）增强空间细节，两者交替优化。这种方法在单个扩散步骤中高效地平衡了细节再现和时间连贯性，并在实验中展示了出色的性能。", "keywords": "视频超分辨率, 扩散模型, 时间一致性, LoRA, 生成模型", "comments": "该论文的创新点在于其提出的双LoRA学习（DLoRAL）范式，它巧妙地将视频超分辨率中空间细节增强和时间一致性保持这两个关键且相互制约的问题，通过两个LoRA分支（C-LoRA和D-LoRA）进行解耦和协同优化。通过交替训练和最终合并LoRA分支到SD模型中，实现了在单步扩散过程中的高效和高质量恢复，有效解决了现有SD-based VSR方法在细节和一致性之间权衡的痛点。这对于实时或近实时视频处理具有重要意义。"}}
{"id": "2506.14923", "title": "Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning", "authors": ["Jaehong Chung", "Michael Manga", "Timothy Kneafsey", "Tapan Mukerji", "Mengsu Hu"], "summary": "Microearthquakes (MEQs) generated by subsurface fluid injection record the\nevolving stress state and permeability of reservoirs. Forecasting their full\nspatiotemporal evolution is therefore critical for applications such as\nenhanced geothermal systems (EGS), CO$_2$ sequestration and other\ngeo-engineering applications. We present a transformer-based deep learning\nmodel that ingests hydraulic stimulation history and prior MEQ observations to\nforecast four key quantities: cumulative MEQ count, cumulative logarithmic\nseismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of\nthe MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model\nachieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the\n15-second forecast horizon across all targets, and supplies uncertainty\nestimates through a learned standard deviation term. These accurate,\nuncertainty-quantified forecasts enable real-time inference of fracture\npropagation and permeability evolution, demonstrating the strong potential of\ndeep-learning approaches to improve seismic-risk assessment and guide\nmitigation strategies in future fluid-injection operations.", "comment": null, "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.14923v1", "AI": {"title_translation": "利用深度学习预测流体诱发微地震的时空演化", "tldr": "本研究提出了一种基于Transformer的深度学习模型，用于预测流体诱发微地震的时空演化，并在实验数据上取得了高精度，有望改进地震风险评估。", "motivation": "地下流体注入产生的微地震（MEQs）记录了储层应力状态和渗透率的演变。预测其完整的时空演化对于增强型地热系统（EGS）、二氧化碳封存及其他地球工程应用至关重要。", "method": "提出了一种基于Transformer的深度学习模型。该模型输入水力压裂历史和先前的微地震观测数据，以预测四个关键量：累积微地震计数、累积对数地震矩以及微地震云的第50百分位和第95百分位范围（P50、P95）。模型还通过学习到的标准差项提供不确定性估计。", "result": "将该模型应用于EGS Collab实验1数据集，在所有目标上，1秒预测时程的R^2值大于0.98，15秒预测时程的R^2值大于0.88。", "conclusion": "这些准确且量化了不确定性的预测能够实时推断裂缝扩展和渗透率演化，证明了深度学习方法在改进地震风险评估和指导未来流体注入操作中的缓解策略方面的巨大潜力。", "translation": "地下流体注入产生的微地震（MEQs）记录了储层应力状态和渗透率的演变。因此，预测其完整的时空演化对于增强型地热系统（EGS）、二氧化碳封存及其他地球工程应用至关重要。我们提出了一种基于Transformer的深度学习模型，该模型输入水力压裂历史和先前的微地震观测数据，以预测四个关键量：累积微地震计数、累积对数地震矩以及微地震云的第50百分位和第95百分位范围（P50、P95）。将该模型应用于EGS Collab实验1数据集，在所有目标上，1秒预测时程的R^2值大于0.98，15秒预测时程的R^2值大于0.88，并通过学习到的标准差项提供不确定性估计。这些准确且量化了不确定性的预测能够实时推断裂缝扩展和渗透率演化，证明了深度学习方法在改进地震风险评估和指导未来流体注入操作中的缓解策略方面的巨大潜力。", "summary": "本研究提出了一种基于Transformer的深度学习模型，用于预测流体诱发微地震（MEQs）的时空演化。该模型整合了水力压裂历史和MEQ观测数据，能够预测累积MEQ计数、累积对数地震矩以及MEQ云的扩展范围（P50、P95），并提供不确定性估计。在EGS Collab实验1数据集上的应用显示，模型在1秒和15秒预测时程上均表现出高精度（R^2 > 0.98和R^2 > 0.88）。这些准确的预测有助于实时推断裂缝传播和渗透率演化，表明深度学习在改善地震风险评估和指导流体注入操作方面具有巨大潜力。", "keywords": "微地震, 深度学习, 预测, 流体注入, 地震风险", "comments": "本文的创新点在于首次将基于Transformer的深度学习模型应用于流体诱发微地震的时空演化预测，并成功地量化了预测的不确定性。这对于实时监测和风险评估具有重要意义，能够显著提升地热系统和碳封存等地球工程项目的安全性与效率。"}}
{"id": "2506.15346", "title": "Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges", "authors": ["A. S. Stankevich", "I. B. Petrov"], "summary": "Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.", "comment": "Submitted to \"Computational Mathematics And Mathematical Physics\",\n  ISSN 1555-6662, issue 8, August 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15346v1", "AI": {"title_translation": "声波波形反演与图像到图像薛定谔桥", "tldr": "提出一种基于条件图像到图像薛定谔桥（cI2SB）的声波全波形反演方法，克服了扩散模型在采样和输出控制上的局限性，实现了高分辨率和高效的反演。", "motivation": "现有深度学习应用于声波全波形反演（FWI）时，尽管扩散模型能生成高分辨率样本，但其迭代、随机采样以及启发式输出控制是限制因素，尤其是在整合近似速度模型方面存在不明确性。", "method": "论文通过采用一个薛定谔桥来解决问题，该桥在真实速度模型和光滑速度模型的分布之间进行插值。为了促进学习在分布之间传输样本的非线性漂移，将图像到图像薛定谔桥（I2SB）的概念扩展到条件采样，从而形成了条件图像到图像薛定谔桥（cI2SB）框架。", "result": "实验证明，所提出的cI2SB方案优于先前工作中建议的条件扩散模型的重新实现，并且仅需要少量神经函数评估（NFEs）即可实现优于基于监督学习方法的样本保真度。", "conclusion": "该研究成功地利用条件图像到图像薛定谔桥框架解决了声波全波形反演中扩散模型的局限性，提供了一种高效且高保真度的方法来重建参考速度模型。", "translation": "深度学习模型应用于声波全波形反演（FWI）的最新进展以使用扩散模型作为贝叶斯类推理过程的先验分布为标志。这些方法的优点是能够生成高分辨率样本，这是经典反演方法或其他基于深度学习的解决方案无法实现的。然而，扩散模型采样的迭代和随机性质以及输出控制的启发式性质仍然是其适用性的限制因素。例如，如何将近似速度模型以最佳方式纳入基于扩散的反演方案仍然不清楚，尽管它被认为是FWI管道的重要组成部分。我们通过采用薛定谔桥来解决这个问题，该桥在真实（ground truth）速度模型和光滑速度模型的分布之间进行插值。为了促进学习在分布之间传输样本的非线性漂移，我们将图像到图像薛定谔桥（I2SB）的概念扩展到条件采样，从而形成了条件图像到图像薛定谔桥（cI2SB）框架。为了验证我们的方法，我们评估了其在结合固定形状的观测地震信号的情况下，从其平滑近似中重建参考速度模型的有效性。我们的实验表明，所提出的解决方案优于我们对早期工作中建议的条件扩散模型的重新实现，同时仅需要少量神经函数评估（NFEs）即可实现优于监督学习方法的样本保真度。实现本文所述算法的补充代码可在存储库https://github.com/stankevich-mipt/seismic_inversion_via_I2SB中找到。", "summary": "本文提出了一种基于条件图像到图像薛定谔桥（cI2SB）框架的新型声波全波形反演（FWI）方法，旨在解决现有扩散模型在FWI中存在的采样迭代性、随机性及输出控制启发式等局限。通过构建一个在真实与平滑速度模型分布间插值的薛定谔桥，并扩展I2SB至条件采样，该方法能够有效地从平滑近似和地震信号中重建高分辨率速度模型。实验结果表明，cI2SB在样本保真度上优于条件扩散模型和监督学习方法，且计算效率更高。", "keywords": "声波全波形反演, 薛定谔桥, 扩散模型, 图像到图像, 地球物理反演", "comments": "这篇论文创新性地将薛定谔桥理论引入声波全波形反演，特别是通过开发条件图像到图像薛定谔桥（cI2SB）来克服传统扩散模型在FWI中面临的采样效率和输出控制难题。其重要性在于提供了一种更高效、更高分辨率且更鲁棒的反演方法，有望提升地球物理勘探中地下结构成像的精度和效率。该工作为将更复杂的生成模型应用于地球物理反演领域开辟了新的途径。"}}
{"id": "2506.15596", "title": "Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image Registration", "authors": ["Kyobin Choo", "Hyunkyung Han", "Jinyeong Kim", "Chanyong Yoon", "Seong Jae Hwang"], "summary": "In clinical practice, imaging modalities with functional characteristics,\nsuch as positron emission tomography (PET) and fractional anisotropy (FA), are\noften aligned with a structural reference (e.g., MRI, CT) for accurate\ninterpretation or group analysis, necessitating multi-modal deformable image\nregistration (DIR). However, due to the extreme heterogeneity of these\nmodalities compared to standard structural scans, conventional unsupervised DIR\nmethods struggle to learn reliable spatial mappings and often distort images.\nWe find that the similarity metrics guiding these models fail to capture\nalignment between highly disparate modalities. To address this, we propose\nM2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal\nDIR models using only mono-modal similarity while preserving the established\narchitectural paradigm for seamless integration into existing models. We also\nintroduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training\nscheme to promote diffeomorphism. Furthermore, our framework naturally extends\nto a semi-supervised setting, integrating pre-aligned and unaligned pairs only,\nwithout requiring ground-truth transformations or segmentation masks.\nExperiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\ndemonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for\nPET-MRI and FA-MRI registration, highlighting its effectiveness in handling\nhighly heterogeneous multi-modal DIR. Our code is available at\nhttps://github.com/MICV-yonsei/M2M-Reg.", "comment": "11 pages, 3 figures, 2 tables, Accepted at Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15596v1", "AI": {"title_translation": "单模态化极度异构的多模态医学图像配准", "tldr": "M2M-Reg是一种新颖的多模态医学图像配准框架，通过仅使用单模态相似性和循环训练方案来处理极度异构的模态，比现有方法实现了更高的配准精度。", "motivation": "在临床实践中，功能性成像模态（如PET、FA）与结构性参考（如MRI、CT）的配准是必要的，但由于模态间极高的异质性，传统的无监督可变形图像配准（DIR）方法难以学习可靠的空间映射，且相似性度量无法有效捕捉高度异构模态间的对齐，导致图像失真。", "method": "本文提出了M2M-Reg（Multi-to-Mono Registration）框架，通过仅使用单模态相似性来训练多模态DIR模型，同时保持现有架构范式以便无缝集成。引入了GradCyCon正则化器，利用M2M-Reg的循环训练方案来促进微分同胚。该框架还能自然地扩展到半监督设置，仅集成预对齐和未对齐的图像对，无需真实变换或分割掩模。", "result": "在阿尔茨海默病神经影像学倡议（ADNI）数据集上的实验表明，M2M-Reg在PET-MRI和FA-MRI配准方面比现有方法实现了高达2倍的DSC（Dice相似系数）。", "conclusion": "M2M-Reg通过采用单模态相似性和新颖的正则化器，有效解决了高度异构多模态可变形图像配准的挑战，显著提高了配准精度。", "translation": "在临床实践中，具有功能特性的成像模态，例如正电子发射断层扫描 (PET) 和各向异性分数 (FA)，通常与结构参考（例如 MRI、CT）对齐，以进行准确的解释或组分析，这需要多模态可变形图像配准 (DIR)。然而，由于这些模态与标准结构扫描相比具有极高的异质性，传统的无监督 DIR 方法难以学习可靠的空间映射，并且经常导致图像失真。我们发现指导这些模型的相似性度量未能捕捉高度不同模态之间的对齐。为了解决这个问题，我们提出了 M2M-Reg（多模态到单模态配准），这是一种新颖的框架，它仅使用单模态相似性来训练多模态 DIR 模型，同时保留了既定的架构范式，以便无缝集成到现有模型中。我们还引入了 GradCyCon，这是一种利用 M2M-Reg 循环训练方案来促进微分同胚的正则化器。此外，我们的框架自然地扩展到半监督设置，仅集成预对齐和未对齐的对，而无需真实变换或分割掩模。在阿尔茨海默病神经影像学倡议 (ADNI) 数据集上的实验表明，M2M-Reg 在 PET-MRI 和 FA-MRI 配准方面比现有方法实现了高达 2 倍的 DSC，突出了其在处理高度异构多模态 DIR 方面的有效性。我们的代码可在 https://github.com/MICV-yonsei/M2M-Reg 获取。", "summary": "M2M-Reg是一种创新的多模态医学图像配准框架，旨在解决传统方法在处理极度异构模态时（如PET/FA与MRI）遇到的挑战。该框架通过仅使用单模态相似性进行训练，并引入GradCyCon正则化器以确保微分同胚，同时保持现有架构兼容性。它还能扩展到半监督学习。实验证明，M2M-Reg在PET-MRI和FA-MRI配准上比现有方法实现了显著更高的精度。", "keywords": "多模态图像配准, 异构模态, 单模态相似性, 可变形图像配准, 微分同胚", "comments": "本文解决了医学图像配准领域的一个重要挑战：处理模态之间的极端异质性。其创新之处在于“单模态化”方法，该方法简化了相似性度量问题，并引入了GradCyCon以促进微分同胚。其与现有架构的兼容性以及在无需真实掩模的情况下扩展到半监督设置的能力，增加了其实用价值和灵活性。高达2倍的DSC改进有力地证明了其有效性。"}}
{"id": "2506.15610", "title": "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion", "authors": ["Yuqing Lan", "Chenyang Zhu", "Zhirui Gao", "Jiazhao Zhang", "Yihan Cao", "Renjiao Yi", "Yijie Wang", "Kai Xu"], "summary": "Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.", "comment": "11 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15610v1", "AI": {"title_translation": "BoxFusion：通过实时多视图包围盒融合实现无重建开放词汇3D目标检测", "tldr": "BoxFusion提出了一个无重建的在线框架，通过融合多视图3D包围盒实现实时、内存高效的开放词汇3D目标检测，并在ScanNetV2和CA-1M数据集上达到SOTA性能。", "motivation": "现有开放词汇3D目标检测方法依赖密集的点云重建，导致计算开销大和内存限制，阻碍了实时部署。", "method": "提出一个新颖的无重建在线框架。利用Cubify Anything作为预训练视觉基础模型（VFM）进行单视图3D目标检测，并结合CLIP捕获开放词汇语义。通过关联模块（3D NMS和包围盒对应匹配）和优化模块（IoU引导的粒子滤波随机优化）融合不同视图的检测包围盒，以实现多视图一致性并最小化计算复杂度。", "result": "在ScanNetV2和CA-1M数据集上，该方法在在线方法中实现了最先进的性能，并展示了强大的泛化能力，即使在1000平方米以上的环境中也能实现实时感知。", "conclusion": "该方法通过新颖的无重建范式实现了实时、内存高效的开放词汇3D目标检测，并在各种场景中表现出卓越的泛化能力和最先进的性能。", "translation": "开放词汇3D目标检测因其在自动驾驶和具身AI中的关键应用而受到广泛关注。现有的检测方法，无论是离线还是在线，通常依赖密集的点云重建，这带来了大量的计算开销和内存限制，阻碍了在下游任务中的实时部署。为了解决这个问题，我们提出了一个新颖的无重建在线框架，专为内存高效和实时3D检测而设计。具体来说，给定流式姿态RGB-D视频输入，我们利用Cubify Anything作为预训练的视觉基础模型（VFM）通过包围盒进行单视图3D目标检测，并结合CLIP捕获检测对象的开放词汇语义。为了将所有在不同视图中检测到的包围盒融合为一个统一的包围盒，我们采用了一个关联模块来处理多视图的对应关系，以及一个优化模块来融合在多视图中预测的同一实例的3D包围盒。关联模块利用3D非最大抑制（NMS）和包围盒对应匹配模块，而优化模块则使用基于粒子滤波的IoU引导高效随机优化技术，以强制执行3D包围盒的多视图一致性，同时最小化计算复杂性。在ScanNetV2和CA-1M数据集上进行的大量实验表明，我们的方法在在线方法中实现了最先进的性能。受益于这种新颖的无重建3D目标检测范式，我们的方法在各种场景中表现出强大的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。", "summary": "BoxFusion提出了一种无重建的在线框架，用于实时、内存高效的开放词汇3D目标检测。该方法利用预训练的视觉基础模型（Cubify Anything和CLIP）进行单视图检测，并通过创新的关联和优化模块（包括3D NMS、包围盒对应匹配和IoU引导的粒子滤波优化）将多视图包围盒融合，实现了卓越的性能和泛化能力，解决了传统方法中重建带来的计算和内存瓶颈。", "keywords": "开放词汇3D目标检测, 无重建, 多视图融合, 实时, 包围盒", "comments": "该论文提出了一种创新的无重建范式，有效解决了传统3D目标检测中点云重建带来的计算和内存瓶颈。其结合预训练视觉基础模型和多视图融合策略，实现了实时且内存高效的开放词汇3D检测，在实际应用中具有重要意义，特别是在大尺度环境下的实时感知。"}}
{"id": "2506.15378", "title": "Sampling 3D Molecular Conformers with Diffusion Transformers", "authors": ["J. Thorben Frank", "Winfried Ripken", "Gregor Lied", "Klaus-Robert Müller", "Oliver T. Unke", "Stefan Chmiela"], "summary": "Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15378v1", "AI": {"title_translation": "使用扩散Transformer采样3D分子构象", "tldr": "DiTMC是为分子构象生成设计的扩散Transformer框架，解决了整合离散图信息和3D几何、处理欧几里得对称性以及适应不同分子大小的挑战，实现了最先进的性能。", "motivation": "扩散Transformer（DiTs）在生成建模中表现出色，但将其应用于分子构象生成面临挑战：如何整合离散分子图信息与连续3D几何，处理欧几里得对称性，以及设计能泛化到不同大小和结构分子的条件机制。", "method": "提出DiTMC框架，通过模块化架构解决这些挑战，该架构将3D坐标处理与原子连接条件分离。引入两种互补的基于图的条件策略，与DiT架构无缝集成。结合不同的注意力机制，包括标准非等变和SO(3)等变公式，以灵活控制精度和计算效率之间的权衡。", "result": "在标准构象生成基准（GEOM-QM9, -DRUGS, -XL）上的实验表明，DiTMC达到了最先进的精度和物理有效性。", "conclusion": "结果强调了架构选择和对称先验如何影响样本质量和效率，为分子结构的大规模生成建模提供了有前景的方向。", "translation": "扩散Transformer (DiTs) 在生成建模，特别是在图像合成方面，表现出强大的性能，使其成为分子构象生成的引人注目的选择。然而，将DiTs应用于分子引入了新的挑战，例如将离散分子图信息与连续3D几何集成，处理欧几里得对称性，以及设计能够泛化到不同大小和结构分子的条件机制。我们提出了DiTMC，一个通过模块化架构调整DiTs来解决这些挑战的框架，该架构将3D坐标的处理与原子连接的条件分离。为此，我们引入了两种互补的基于图的条件策略，它们与DiT架构无缝集成。这些策略与不同的注意力机制结合，包括标准非等变和SO(3)等变公式，从而能够灵活控制精度和计算效率之间的权衡。在标准构象生成基准（GEOM-QM9, -DRUGS, -XL）上的实验表明，DiTMC实现了最先进的精度和物理有效性。我们的结果强调了架构选择和对称先验如何影响样本质量和效率，为分子结构的大规模生成建模提供了有前景的方向。代码可在 https://github.com/ML4MolSim/dit_mc 获取。", "summary": "本文提出了DiTMC框架，旨在将扩散Transformer（DiTs）应用于3D分子构象生成。该框架通过模块化设计解决了整合离散分子图信息与连续3D几何、处理欧几里得对称性以及泛化到不同分子大小等核心挑战。DiTMC引入了两种图基条件策略和灵活的注意力机制（包括SO(3)等变），以平衡精度和效率。实验证明，DiTMC在主流构象生成基准上实现了最先进的性能，并揭示了架构选择和对称性对样本质量和效率的关键影响。", "keywords": "分子构象生成, 扩散Transformer, DiTMC, 3D几何, SO(3)等变", "comments": "该论文创新性地将扩散Transformer应用于3D分子构象生成领域，通过DiTMC框架解决了该领域特有的挑战，如离散图与连续几何的融合以及对称性处理。其模块化架构和引入的图基条件策略以及等变注意力机制是重要创新点，提升了模型在分子构象生成中的精度和物理有效性。这为大规模分子结构生成建模开辟了新的方向。"}}
{"id": "2506.15625", "title": "HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization", "authors": ["Roey Ron", "Guy Tevet", "Haim Sawdayee", "Amit H. Bermano"], "summary": "We present HOIDiNi, a text-driven diffusion framework for synthesizing\nrealistic and plausible human-object interaction (HOI). HOI generation is\nextremely challenging since it induces strict contact accuracies alongside a\ndiverse motion manifold. While current literature trades off between realism\nand physical correctness, HOIDiNi optimizes directly in the noise space of a\npretrained diffusion model using Diffusion Noise Optimization (DNO), achieving\nboth. This is made feasible thanks to our observation that the problem can be\nseparated into two phases: an object-centric phase, primarily making discrete\nchoices of hand-object contact locations, and a human-centric phase that\nrefines the full-body motion to realize this blueprint. This structured\napproach allows for precise hand-object contact without compromising motion\nnaturalness. Quantitative, qualitative, and subjective evaluations on the GRAB\ndataset alone clearly indicate HOIDiNi outperforms prior works and baselines in\ncontact accuracy, physical validity, and overall quality. Our results\ndemonstrate the ability to generate complex, controllable interactions,\nincluding grasping, placing, and full-body coordination, driven solely by\ntextual prompts. https://hoidini.github.io.", "comment": "Project page: https://hoidini.github.io", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15625v1", "AI": {"title_translation": "HOIDiNi：通过扩散噪声优化实现人-物交互", "tldr": "HOIDiNi是一个文本驱动的扩散框架，通过在预训练扩散模型的噪声空间中优化，实现了逼真且合理的人-物交互生成，解决了现有方法在真实性和物理正确性之间的权衡问题。", "motivation": "人-物交互（HOI）生成极具挑战性，因为它不仅需要严格的接触精度，还需要处理多样化的运动流形。现有文献在真实性和物理正确性之间存在权衡，因此需要一种能够同时实现这两者的方法。", "method": "HOIDiNi是一个文本驱动的扩散框架，通过使用扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化。该方法将问题分为两个阶段：以物体为中心的阶段（主要选择手-物体接触位置）和以人为中心的阶段（精炼全身运动以实现此蓝图）。这种结构化方法允许精确的手-物体接触，同时不影响运动的自然性。", "result": "在GRAB数据集上的定量、定性和主观评估表明，HOIDiNi在接触精度、物理有效性和整体质量方面均优于现有工作和基线。结果表明，该框架能够仅通过文本提示生成复杂、可控的交互，包括抓取、放置和全身协调。", "conclusion": "HOIDiNi通过其独特的扩散噪声优化和两阶段结构化方法，成功解决了人-物交互生成的挑战，实现了高真实性和物理正确性，并在各项评估指标上超越了现有方法。", "translation": "我们提出了HOIDiNi，一个文本驱动的扩散框架，用于合成逼真且合理的人-物交互（HOI）。HOI生成极具挑战性，因为它需要严格的接触精度以及多样的运动流形。虽然当前的文献在真实性和物理正确性之间进行权衡，但HOIDiNi通过使用扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化，从而同时实现了这两者。这得益于我们的观察，即该问题可以分为两个阶段：一个以物体为中心的阶段，主要进行手-物体接触位置的离散选择；以及一个以人为中心的阶段，用于精炼全身运动以实现此蓝图。这种结构化方法允许精确的手-物体接触，同时不影响运动的自然性。仅在GRAB数据集上的定量、定性和主观评估清楚地表明，HOIDiNi在接触精度、物理有效性和整体质量方面均优于现有工作和基线。我们的结果展示了仅由文本提示驱动即可生成复杂、可控交互的能力，包括抓取、放置和全身协调。https://hoidini.github.io。", "summary": "HOIDiNi是一个创新的文本驱动扩散框架，专门用于生成逼真且物理正确的人-物交互（HOI）。该框架通过独特的扩散噪声优化（DNO）技术，在预训练扩散模型的噪声空间中直接进行优化，并采用两阶段方法：首先确定手-物体接触点，然后精炼全身运动。这种方法克服了传统HOI生成中真实性和物理正确性之间的权衡，并在GRAB数据集上表现出卓越的性能，能够生成由文本提示驱动的复杂和可控交互。", "keywords": "人-物交互, 扩散模型, 噪声优化, 文本驱动, 运动生成", "comments": "HOIDiNi的创新之处在于其将HOI生成问题分解为离散的手-物体接触选择和全身运动精炼的两个阶段，并结合了扩散噪声优化，这有效地解决了现有方法在真实性和物理正确性上的痛点。其文本驱动的特性也大大提升了交互生成的灵活性和控制性，为未来的虚拟现实、机器人和动画领域提供了新的可能性。"}}
{"id": "2506.15383", "title": "Global Ground Metric Learning with Applications to scRNA data", "authors": ["Damin Kühn", "Michael T. Schaub"], "summary": "Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.", "comment": "This method is provided as a Python package on PyPI, see\n  https://github.com/DaminK/ggml-ot", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15383v1", "AI": {"title_translation": "全局地度量学习及其在单细胞RNA数据中的应用", "tldr": "提出一种新颖的全局地度量学习方法，用于优化最优传输距离，仅需分布级类别标签，并在scRNA数据上表现出更好的嵌入、聚类和分类性能。", "motivation": "传统的地度量方法（预定义或监督学习）存在局限性：预定义度量无法捕获数据内在结构和特征重要性；现有监督方法泛化性差或受限于共享支持域。", "method": "提出一种新方法，学习在共享度量空间上任意分布的度量。该方法提供点之间的距离，但训练只需分布级别的类别标签。", "result": "学习到的全局地度量能够实现更准确的最优传输距离，从而改进嵌入、聚类和分类任务的性能。在跨多种疾病的患者级scRNA-seq数据上验证了其有效性和可解释性。", "conclusion": "该研究提出了一种有效且可解释的全局地度量学习方法，克服了传统方法的局限性，显著提升了最优传输在各种任务中的应用效果，特别是在复杂的生物医学数据如scRNA-seq分析中展现出巨大潜力。", "translation": "最优传输提供了一个比较概率分布的鲁棒框架。其有效性受到底层地度量选择的显著影响。传统上，地度量要么是(i)预定义的，例如欧几里得距离，要么是(ii)通过监督方式学习的，利用标记数据学习合适的地度量以增强特定任务的性能。然而，预定义的度量通常无法解释数据中固有的结构和不同特征的不同重要性，并且现有的监督地度量学习方法往往无法泛化到多个类别，或者局限于具有共享支持域的分布。为了解决这些限制，我们提出了一种新颖的方法，用于学习共享度量空间上任意分布的度量。我们的方法提供点之间的距离，如同一个全局度量，但训练时只需分布级别的类别标签。学习到的全局地度量能够实现更准确的最优传输距离，从而改进嵌入、聚类和分类任务的性能。我们使用跨多种疾病的患者级scRNA-seq数据证明了我们方法的有效性和可解释性。", "summary": "本文提出一种新颖的全局地度量学习方法，旨在克服传统最优传输地度量方法的局限性。该方法能够在共享度量空间上学习任意分布的度量，仅需分布级别的类别标签进行训练。通过学习到的全局地度量，能够获得更精确的最优传输距离，从而显著提升数据嵌入、聚类和分类任务的性能。研究通过患者级单细胞RNA测序数据验证了该方法的有效性和可解释性。", "keywords": "最优传输, 地度量学习, 单细胞RNA数据, 机器学习, 距离度量", "comments": "该论文的创新点在于提出了一种新型的全局地度量学习方法，它克服了传统方法在处理数据内在结构和泛化性上的不足。特别地，它仅需要分布级别的类别标签进行训练，降低了对详细点级标签的依赖，这在实际应用中具有重要意义，尤其是在标签稀缺的生物医学数据（如scRNA-seq）分析中。其在最优传输框架下的应用，有望提升生物数据分析的准确性和效率。"}}
{"id": "2506.15385", "title": "Provable Maximum Entropy Manifold Exploration via Diffusion Models", "authors": ["Riccardo De Santi", "Marin Vlastelica", "Ya-Ping Hsieh", "Zebang Shen", "Niao He", "Andreas Krause"], "summary": "Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15385v1", "AI": {"title_translation": "可证明的最大熵流形探索通过扩散模型", "tldr": "本文提出了一种利用扩散模型进行探索的新颖框架，通过最大化数据流形上的熵来生成新颖设计，并理论证明了其收敛性。", "motivation": "在科学发现等现实世界决策问题中，需要生成真正新颖的设计，而非仅仅模仿现有数据。传统的生成模型在不依赖明确不确定性量化的情况下，难以有效进行探索。", "method": "本文将探索定义为对预训练扩散模型隐式定义的近似数据流形上的熵最大化。通过利用扩散模型引起的密度熵与其分数函数之间的基本联系，克服了密度估计的挑战并实现了可扩展性。开发了一种基于镜像下降的算法，通过顺序微调预训练扩散模型来解决探索问题。", "result": "该方法在合成数据和高维文本到图像扩散上进行了经验性评估，并展示了有希望的结果。", "conclusion": "本文提出了一种新颖的、可证明收敛的探索方法，利用扩散模型通过最大化熵来生成新颖设计。该方法在现实假设下被证明收敛到最优探索性扩散模型。", "translation": "探索对于解决现实世界的决策问题至关重要，例如科学发现，其目标是生成真正新颖的设计，而不是模仿现有数据分布。在这项工作中，我们解决了利用生成模型表示能力进行探索的挑战，而无需依赖明确的不确定性量化。我们引入了一个新颖的框架，将探索视为对预训练扩散模型隐式定义的近似数据流形上的熵最大化。然后，我们提出了一种基于密度估计的探索新原则，这个问题在实践中众所周知是具有挑战性的。为了克服这个问题并使该方法真正可扩展，我们利用了扩散模型引起的密度熵与其分数函数之间的基本联系。在此基础上，我们开发了一种基于镜像下降的算法，将探索问题作为预训练扩散模型的顺序微调来解决。通过利用对镜像流的最新理解，我们证明了其在现实假设下收敛到最优探索性扩散模型。最后，我们通过在合成数据和高维文本到图像扩散上经验性评估了我们的方法，展示了有希望的结果。", "summary": "本文提出了一种新颖的探索框架，旨在通过最大化预训练扩散模型隐式定义的数据流形上的熵来生成真正新颖的设计。该方法通过利用扩散模型密度熵与分数函数之间的联系，有效解决了密度估计的挑战，并实现了可扩展性。论文开发了一种基于镜像下降的算法，将探索视为扩散模型的顺序微调，并从理论上证明了其收敛到最优探索性扩散模型。在合成数据和高维文本到图像扩散上的实验验证了该方法的有效性。", "keywords": "探索, 扩散模型, 熵最大化, 流形学习, 密度估计", "comments": "该论文通过将探索重新定义为扩散模型框架内的熵最大化问题，提供了一种创新性方法。其核心创新在于避免了显式的不确定性量化，而是利用扩散模型分数函数的固有特性来实现可扩展的密度估计。理论上的收敛性证明极大地增加了该方法的严谨性和价值。其在生成新颖设计，特别是高维文本到图像领域的应用，凸显了其实际重要性。"}}
{"id": "2506.15645", "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models", "authors": ["Shuo Xing", "Lanqing Guo", "Hongyuan Hua", "Seoyoung Lee", "Peiran Li", "Yufei Wang", "Zhangyang Wang", "Zhengzhong Tu"], "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.", "comment": "18 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15645v1", "AI": {"title_translation": "多模态大语言模型中视觉质量悖论的揭秘", "tldr": "研究发现多模态大语言模型（MLLMs）存在视觉质量悖论，即图像质量与人类感知不符时模型性能可能更好。为解决此问题，提出了VQ-TTT方法，在测试时动态调整图像，显著提升了MLLMs的准确性。", "motivation": "尽管多模态大语言模型（MLLMs）在视觉-语言任务上表现出色，但目前尚不清楚输入图像的视觉质量如何影响它们的响应，以及更高的感知质量是否能带来更好的模型理解。", "method": "本文进行了首次系统性研究，对主流MLLMs和一系列视觉-语言基准测试中的图像应用了受控的降级和风格转变。研究发现了一个视觉质量悖论。为了弥合这一差距，引入了视觉质量测试时调整（VQ-TTT）方法，这是一个轻量级适应模块，它在冻结的视觉编码器之前插入一个可学习的低秩核来调节频率内容，并通过LoRA仅微调浅层视觉编码器层。VQ-TTT通过一次前向传播动态调整每个输入图像，使其与任务特定的模型偏好对齐。", "result": "研究发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至个体实例的性能都可以提高。现成的图像修复管道无法解决这些特殊偏好。VQ-TTT在所有评估的MLLMs和所有数据集上都显著提高了平均准确率，且无需外部模型、缓存特征或额外训练数据。", "conclusion": "这些发现重新定义了MLLMs的“更好”视觉输入，并强调在新的人工智能作为主要数据客户的时代，需要自适应而非普遍“干净”的图像。", "translation": "近期的多模态大语言模型（MLLMs）在基准视觉-语言任务上表现出色，但关于输入视觉质量如何影响其响应的了解甚少。图像的更高感知质量是否已转化为更好的MLLM理解？我们进行了首次系统性研究，涵盖了主流MLLMs和一系列视觉-语言基准测试，对每张图像应用了受控的降级和风格转变。令人惊讶的是，我们发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至个体实例的性能都可以提高。现成的修复管道无法调和这些特有的偏好。为了弥合这一差距，我们引入了视觉质量测试时调整（VQ-TTT）——一个轻量级适应模块，它：（1）在冻结的视觉编码器之前插入一个可学习的低秩核来调节频率内容；（2）通过LoRA仅微调浅层视觉编码器层。VQ-TTT在单次前向传播中动态调整每个输入图像，使其与任务特定的模型偏好对齐。在所有评估的MLLMs和所有数据集上，VQ-TTT显著提高了平均准确率，且无需外部模型、缓存特征或额外训练数据。这些发现重新定义了MLLMs的“更好”视觉输入，并强调在新的人工智能作为主要数据客户的时代，需要自适应而非普遍“干净”的图像。", "summary": "本文系统性研究了输入视觉质量对多模态大语言模型（MLLMs）性能的影响，并首次揭示了“视觉质量悖论”，即图像质量与人类感知不符时MLLMs性能可能反而提升。为解决这一问题，作者提出了测试时调整方法VQ-TTT，通过在视觉编码器前插入可学习核并利用LoRA微调浅层，动态调整输入图像以匹配模型偏好。实验证明，VQ-TTT显著提升了MLLMs在多个基准测试上的准确性，强调了MLLMs需要自适应的而非普遍“干净”的视觉输入。", "keywords": "多模态大语言模型, 视觉质量悖论, 测试时调整, VQ-TTT, 图像适应", "comments": "这篇论文的创新点在于首次揭示了多模态大语言模型中存在的“视觉质量悖论”，挑战了传统认为高质量图像必然带来更好模型性能的观念。提出的VQ-TTT方法是一种轻量级且高效的测试时自适应方案，无需额外训练数据或外部模型即可显著提升性能，对于优化MLLMs的实际应用具有重要意义。它强调了AI作为数据消费者时，需要根据模型特性而非人类感知来定义“更好”的视觉输入，具有深刻的启示性。"}}
{"id": "2506.15649", "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "authors": ["Ankan Deria", "Adinath Madhavrao Dukre", "Feilong Tang", "Sara Atito", "Sudipta Roy", "Muhammad Awais", "Muhammad Haris Khan", "Imran Razzak"], "summary": "Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15649v1", "AI": {"title_translation": "基于边际奖励调整的双阶段价值引导推理，实现快速忠实的VLM图像字幕生成", "tldr": "ViMaR是一种双阶段推理框架，通过结合时序差分价值模型和边际感知奖励调整，提高了VLM图像字幕生成的效率和准确性，同时实现了显著的加速和跨模型泛化能力。", "motivation": "现有视觉-语言模型（VLM）的推理时搜索方法计算成本高昂，且容易产生未受惩罚的低置信度生成，导致持续的幻觉问题。", "method": "本研究引入了ViMaR（Value-guided Inference with Margin-based Reward），一个双阶段推理框架。第一阶段，通过单次遍历从多样化候选字幕中识别最高价值的字幕。第二阶段，选择性地只精炼那些被忽略或视觉接地较弱的片段，同时通过校准的基于边际的惩罚来抑制低置信度延续，但保留描述丰富性。", "result": "ViMaR生成的字幕显著更可靠、事实更准确、更详细、更具解释性，并且比现有价值引导方法提速超过4倍。ViMaR在LLaVA Mistral-7B上训练后，能有效泛化以指导更强的未见模型解码（如LLaVA-OneVision-Qwen2-7B），展现出鲁棒的跨模型指导能力。此外，当ViMaR生成的字幕用于自训练时，底层模型在视觉理解基准测试中取得了实质性提升。", "conclusion": "ViMaR提供了一种灵活、模块化、可扩展且可迁移的推理时解码策略，能够实现快速、准确且自我改进的VLM流水线。", "translation": "尽管视觉-语言模型（VLM）在推理时搜索方面取得了显著进展，但现有方法仍然计算成本高昂，并且容易产生未受惩罚的低置信度生成，这通常导致持续的幻觉。我们引入了**基于边际奖励的价值引导推理（ViMaR）**，一个双阶段推理框架，通过结合时序差分价值模型和边际感知奖励调整，提高了效率和输出忠实度。在第一阶段，我们执行单次遍历以在多样化候选字幕中识别最高价值的字幕。在第二阶段，我们选择性地只精炼那些被忽略或表现出弱视觉接地的片段，从而消除频繁奖励的评估。校准的基于边际的惩罚抑制了低置信度的延续，同时保留了描述丰富性。对多种VLM架构的广泛实验表明，ViMaR生成的字幕显著更可靠、事实更准确、更详细、更具解释性，同时比现有价值引导方法提速超过4倍。具体来说，我们展示了仅在LLaVA Mistral-7B上训练的ViMaR能够有效地泛化以指导更强的未见模型解码。为了进一步验证这一点，我们调整了ViMaR以引导LLaVA-OneVision-Qwen2-7B中的生成，从而持续提高了字幕质量并展示了强大的跨模型指导能力。这种跨模型泛化突出了ViMaR的灵活性和模块化，使其成为一种可扩展和可迁移的推理时解码策略。此外，当ViMaR生成的字幕用于自训练时，底层模型在广泛的视觉理解基准测试中取得了实质性提升，这强调了快速、准确和自我改进的VLM流水线的潜力。", "summary": "本研究提出了一种名为ViMaR（Value-guided Inference with Margin-based Reward）的双阶段推理框架，旨在解决现有VLM推理方法计算成本高且易产生幻觉的问题。ViMaR结合了时序差分价值模型和边际感知奖励调整，通过两阶段过程提高图像字幕生成的效率和忠实度。第一阶段快速识别高价值字幕，第二阶段选择性地精炼低置信度或视觉接地弱的片段。实验证明，ViMaR生成的字幕在可靠性、准确性和细节方面显著提升，并实现了超过4倍的速度提升。此外，ViMaR展现出强大的跨模型泛化能力，其生成的字幕还可用于自训练以进一步提升底层模型的性能，为构建高效、精确且自改进的VLM系统提供了新途径。", "keywords": "VLM字幕生成, 价值引导推理, 边际奖励, 双阶段推理, 跨模型泛化", "comments": "ViMaR的创新之处在于其双阶段推理框架结合了价值引导和边际奖励调整，有效解决了VLM字幕生成中的效率与忠实度问题。其在保持描述丰富性的同时，通过惩罚低置信度生成，显著减少了幻觉。更重要的是，ViMaR展现出的跨模型泛化能力和用于自训练的潜力，使其成为一个高度灵活和可扩展的推理策略，对未来的VLM发展具有重要意义。"}}
{"id": "2506.15408", "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI", "authors": ["David Dembinsky", "Adriano Lucieri", "Stanislav Frolov", "Hiba Najjar", "Ko Watanabe", "Andreas Dengel"], "summary": "Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.", "comment": "Submitted to TMLR, under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15408v1", "AI": {"title_translation": "统一VXAI：可解释人工智能评估的系统综述与框架", "tldr": "该研究对可解释人工智能（XAI）的评估进行了系统回顾，并提出了一个统一的VXAI框架，以解决缺乏标准化评估协议的问题。", "motivation": "现代AI系统，特别是深度神经网络，因其复杂性而缺乏透明度，导致信任度问题。可解释人工智能（XAI）旨在提供人类可理解的模型行为解释，但其有用性和可信度需要严格评估。当前XAI领域缺乏标准化评估协议和对适当指标的共识。", "method": "作者遵循PRISMA指南进行了一项系统文献综述，并引入了一个统一的XAI评估框架（VXAI）。", "result": "识别了362篇相关出版物，并将其贡献聚合为41个功能相似的指标组。提出了一个涵盖解释类型、评估语境性和解释质量要求的三维分类方案。", "conclusion": "所提出的VXAI框架提供了迄今为止最全面和结构化的VXAI概览，支持系统性的指标选择，促进方法间的可比性，并为未来的扩展提供了灵活的基础。", "translation": "现代AI系统经常依赖不透明的黑箱模型，最著名的是深度神经网络，其性能源于具有数百万个学习参数的复杂架构。虽然功能强大，但它们的复杂性对可信度构成了重大挑战，特别是由于缺乏透明度。可解释人工智能（XAI）通过提供人类可理解的模型行为解释来解决这个问题。然而，为了确保其有用性和可信度，此类解释必须经过严格评估。尽管XAI方法数量不断增长，但该领域缺乏标准化的评估协议和对适当指标的共识。为了弥补这一空白，我们遵循系统综述和荟萃分析首选报告项目（PRISMA）指南进行了一项系统文献综述，并引入了一个统一的XAI评估框架（VXAI）。我们识别了362篇相关出版物，并将其贡献聚合为41个功能相似的指标组。此外，我们提出了一个涵盖解释类型、评估语境性和解释质量要求的三个维度分类方案。我们的框架提供了迄今为止最全面和结构化的VXAI概览。它支持系统性的指标选择，促进方法间的可比性，并为未来的扩展提供了灵活的基础。", "summary": "本文针对可解释人工智能（XAI）领域缺乏标准化评估协议的问题，进行了一项系统的文献综述。研究识别了362篇相关文献，并将其贡献归纳为41个指标组。在此基础上，提出了一个统一的XAI评估框架（VXAI），该框架包含一个三维分类方案，涵盖解释类型、评估语境性和解释质量。该框架旨在提供一个全面的VXAI概览，以支持指标选择、提高方法间可比性并为未来研究奠定基础。", "keywords": "可解释人工智能, XAI, 评估框架, 系统综述, 透明度", "comments": "这篇论文的创新之处在于它首次系统地梳理了可解释人工智能（XAI）的评估方法，并提出了一个统一的框架（VXAI），这对于解决当前XAI领域评估缺乏标准化的问题至关重要。它通过大规模的文献综述，聚合了现有指标，并提出了新的分类方案，为未来的XAI评估研究提供了坚实的基础和指导。其重要性体现在它有助于提升XAI方法的可信度和实用性，促进该领域的健康发展。"}}
{"id": "2506.15041", "title": "Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models", "authors": ["Tobias Schmidt", "Kai-Robin Lange", "Matthias Reccius", "Henrik Müller", "Michael Roos", "Carsten Jentsch"], "summary": "As interest in economic narratives has grown in recent years, so has the\nnumber of pipelines dedicated to extracting such narratives from texts.\nPipelines often employ a mix of state-of-the-art natural language processing\ntechniques, such as BERT, to tackle this task. While effective on foundational\nlinguistic operations essential for narrative extraction, such models lack the\ndeeper semantic understanding required to distinguish extracting economic\nnarratives from merely conducting classic tasks like Semantic Role Labeling.\nInstead of relying on complex model pipelines, we evaluate the benefits of\nLarge Language Models (LLMs) by analyzing a corpus of Wall Street Journal and\nNew York Times newspaper articles about inflation. We apply a rigorous\nnarrative definition and compare GPT-4o outputs to gold-standard narratives\nproduced by expert annotators. Our results suggests that GPT-4o is capable of\nextracting valid economic narratives in a structured format, but still falls\nshort of expert-level performance when handling complex documents and\nnarratives. Given the novelty of LLMs in economic research, we also provide\nguidance for future work in economics and the social sciences that employs LLMs\nto pursue similar objectives.", "comment": "53 pages, 5 figures", "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.15041v1", "AI": {"title_translation": "在大规模文本语料库中识别经济叙事——一种使用大型语言模型的集成方法", "tldr": "本文评估了大型语言模型（LLMs），特别是GPT-4o，在从新闻文章中提取经济叙事方面的能力，发现它能够提取有效叙事但仍未达到专家水平，并为未来研究提供了指导。", "motivation": "现有用于经济叙事提取的自然语言处理（NLP）管道虽然在基础语言操作上有效，但缺乏区分经济叙事提取与经典语义任务所需的深层语义理解。因此，本文旨在评估大型语言模型（LLMs）在此任务中的益处。", "method": "研究评估了大型语言模型（LLMs）的益处，通过分析华尔街日报和纽约时报关于通胀的新闻文章语料库。研究应用了严格的叙事定义，并将GPT-4o的输出与专家标注者生成的黄金标准叙事进行了比较。", "result": "结果表明，GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时，其表现仍未能达到专家水平。", "conclusion": "尽管大型语言模型在经济研究中尚属新颖，但它们在经济叙事提取方面显示出潜力，不过在复杂任务上仍需改进。研究为未来在经济学和社会科学领域中使用LLMs实现类似目标提供了指导。", "translation": "近年来，随着人们对经济叙事兴趣的增长，专门用于从文本中提取此类叙事的管道数量也随之增加。这些管道通常采用最先进的自然语言处理技术，例如BERT，来解决这项任务。尽管这些模型在叙事提取所必需的基础语言操作上是有效的，但它们缺乏区分经济叙事提取与仅仅进行语义角色标注等经典任务所需的更深层次的语义理解。本研究没有依赖复杂的模型管道，而是通过分析《华尔街日报》和《纽约时报》关于通货膨胀的新闻文章语料库来评估大型语言模型（LLMs）的优势。我们应用了严格的叙事定义，并将GPT-4o的输出与专家标注者生成的黄金标准叙事进行了比较。我们的结果表明，GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时，其表现仍未能达到专家级别的水平。鉴于LLMs在经济研究中的新颖性，我们还为未来在经济学和社会科学领域中采用LLMs以追求类似目标的工作提供了指导。", "summary": "本研究评估了大型语言模型（LLMs），特别是GPT-4o，在从新闻文章中提取经济叙事方面的能力，旨在克服传统NLP模型在深层语义理解方面的不足。通过将GPT-4o的输出与专家标注的黄金标准进行比较，研究发现GPT-4o能够提取有效的结构化经济叙事，但其性能尚未达到专家水平，尤其是在处理复杂文本时。鉴于LLMs在经济研究中的新兴地位，本文还为未来相关研究提供了指导。", "keywords": "经济叙事, 大型语言模型, GPT-4o, 自然语言处理, 通胀", "comments": "该论文通过将大型语言模型（LLMs）应用于经济叙事提取这一复杂任务，并将其性能与人类专家进行严格比较，展现了其创新性。它明确指出了当前LLMs在此类高级语义理解任务中的潜力和局限性，即能够有效提取信息但尚未达到专家水平。这对于指导未来LLMs在经济学和社会科学领域的应用具有重要意义，尤其是在强调需要进一步提升模型处理复杂语境能力方面。"}}
{"id": "2506.15673", "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting", "authors": ["Kai He", "Ruofan Liang", "Jacob Munkberg", "Jon Hasselgren", "Nandita Vijaykumar", "Alexander Keller", "Sanja Fidler", "Igor Gilitschenski", "Zan Gojcic", "Zian Wang"], "summary": "We address the challenge of relighting a single image or video, a task that\ndemands precise scene intrinsic understanding and high-quality light transport\nsynthesis. Existing end-to-end relighting models are often limited by the\nscarcity of paired multi-illumination data, restricting their ability to\ngeneralize across diverse scenes. Conversely, two-stage pipelines that combine\ninverse and forward rendering can mitigate data requirements but are\nsusceptible to error accumulation and often fail to produce realistic outputs\nunder complex lighting conditions or with sophisticated materials. In this\nwork, we introduce a general-purpose approach that jointly estimates albedo and\nsynthesizes relit outputs in a single pass, harnessing the generative\ncapabilities of video diffusion models. This joint formulation enhances\nimplicit scene comprehension and facilitates the creation of realistic lighting\neffects and intricate material interactions, such as shadows, reflections, and\ntransparency. Trained on synthetic multi-illumination data and extensive\nautomatically labeled real-world videos, our model demonstrates strong\ngeneralization across diverse domains and surpasses previous methods in both\nvisual fidelity and temporal consistency.", "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/UniRelight/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15673v1", "AI": {"title_translation": "UniRelight：学习视频重新照明的联合分解与合成", "tldr": "UniRelight提出了一种新的视频重新照明方法，它通过视频扩散模型联合进行反照率估计和重新照明合成，解决了数据稀缺和现有方法效果不佳的问题，并在视觉保真度和时间一致性方面超越了现有方法。", "motivation": "现有的端到端重新照明模型受限于多光源数据稀缺，泛化能力差。两阶段管线虽然数据需求低，但容易累积误差，在复杂光照和材质下效果不佳。", "method": "本文提出一种通用方法，利用视频扩散模型的生成能力，在单次通过中联合估计反照率并合成重新照明后的输出。该模型在合成的多光源数据和大量自动标注的真实世界视频上进行训练。", "result": "我们的模型在不同领域表现出强大的泛化能力，并在视觉保真度和时间一致性方面超越了以前的方法。", "conclusion": "UniRelight通过联合分解与合成，显著提升了视频重新照明的质量和泛化能力，有效解决了现有方法的局限性。", "translation": "我们解决了重新照明单个图像或视频的挑战，这项任务需要精确的场景内在理解和高质量的光传输合成。现有的端到端重新照明模型通常受限于成对多光源数据的稀缺性，这限制了它们在不同场景中的泛化能力。相反，结合逆向和正向渲染的两阶段管线可以减轻数据需求，但容易受到误差累积的影响，并且在复杂照明条件或复杂材质下通常无法产生逼真的输出。在这项工作中，我们引入了一种通用方法，利用视频扩散模型的生成能力，在单次通过中联合估计反照率并合成重新照明后的输出。这种联合公式增强了隐式场景理解，并促进了逼真照明效果和复杂材质交互的创建，例如阴影、反射和透明度。我们的模型在合成的多光源数据和大量自动标注的真实世界视频上进行训练，在不同领域展现出强大的泛化能力，并在视觉保真度和时间一致性方面超越了以前的方法。", "summary": "UniRelight提出了一种针对图像和视频重新照明的通用方法，旨在解决现有端到端模型数据稀缺性导致的泛化问题以及两阶段管线易于误差累积的缺点。该方法利用视频扩散模型，在一个单一通道中联合进行场景的反照率估计和重新照明输出的合成，从而增强了对场景的理解并能生成逼真的光照效果和材质交互。模型通过合成和自动标注的真实世界视频训练，在泛化性、视觉保真度和时间一致性方面均超越了现有技术。", "keywords": "视频重新照明, 联合分解, 视频扩散模型, 光照合成, 场景理解", "comments": "该论文的创新点在于提出了一个联合分解与合成的统一框架，利用视频扩散模型解决了传统方法在数据稀缺和误差累积上的问题。其重要性在于提升了视频重新照明的真实感和泛化能力，尤其是在处理复杂光照和材质交互方面。通过结合合成数据和大量真实世界视频进行训练，有效地提升了模型的实用性。"}}
{"id": "2506.15421", "title": "Reward Models in Deep Reinforcement Learning: A Survey", "authors": ["Rui Yu", "Shenghua Wan", "Yucen Wang", "Chen-Xiao Gao", "Le Gan", "Zongzhang Zhang", "De-Chuan Zhan"], "summary": "In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.", "comment": "IJCAI 2025 Survey Track (To Appear)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15421v1", "AI": {"title_translation": "深度强化学习中的奖励模型：一项综述", "tldr": "本综述全面回顾了深度强化学习中的奖励建模技术，并展望了未来研究方向，填补了现有文献中系统性综述的空白。", "motivation": "强化学习中，奖励模型作为期望目标的代理，对于指导策略优化至关重要。鉴于学术界和工业界对开发与真实目标对齐并促进策略优化的奖励模型的日益关注，现有文献缺乏对深度强化学习中奖励模型的系统性综述。", "method": "本综述首先概述了奖励建模的背景和基础知识，然后根据来源、机制和学习范式对近期奖励建模方法进行分类概述。在此基础上，讨论了这些技术的各种应用和评估方法，并总结了有前景的研究方向。", "result": "本综述全面审查了深度强化学习文献中的奖励建模技术，涵盖了已建立和新兴的方法，填补了当前文献中对奖励模型进行系统性综述的空白。", "conclusion": "奖励建模是深度强化学习中的一个活跃研究领域，未来仍有许多有前景的研究方向值得探索。", "translation": "在强化学习（RL）中，智能体持续与环境交互并利用反馈来完善其行为。为了指导策略优化，引入了奖励模型作为期望目标的代理，这样当智能体最大化累积奖励时，也能实现任务设计者的意图。最近，学术界和工业界的研究人员都将大量注意力集中在开发不仅与真实目标紧密对齐，而且能促进策略优化的奖励模型上。在这项综述中，我们对深度强化学习文献中的奖励建模技术进行了全面回顾。我们首先概述了奖励建模的背景和基础知识。接着，我们对近期奖励建模方法进行了概述，根据来源、机制和学习范式对其进行分类。在此理解的基础上，我们讨论了这些奖励建模技术的各种应用，并回顾了评估奖励模型的方法。最后，我们总结并强调了奖励建模中有前景的研究方向。总而言之，这项综述包含了已建立和新兴的方法，填补了当前文献中对奖励模型进行系统性回顾的空白。", "summary": "本综述全面回顾了深度强化学习中奖励建模技术，强调其在指导策略优化中的重要性。文章首先介绍了奖励建模的背景，随后根据来源、机制和学习范式对现有方法进行分类，并讨论了其应用和评估方法。最后，指出了未来研究方向，旨在填补该领域系统性综述的空白。", "keywords": "奖励模型, 深度强化学习, 策略优化, 综述, 行为对齐", "comments": "这篇综述论文具有重要的价值，因为它系统性地整理和分类了深度强化学习中奖励模型这一关键领域的研究进展。它不仅为研究者提供了全面的背景知识和现有方法的概览，还指出了未来的研究方向，有助于推动该领域的发展。其创新之处在于填补了当前文献中缺乏系统性综述的空白。"}}
{"id": "2506.15675", "title": "Sekai: A Video Dataset towards World Exploration", "authors": ["Zhen Li", "Chuanhao Li", "Xiaofeng Mao", "Shaoheng Lin", "Ming Li", "Shitian Zhao", "Zhaopan Xu", "Xinyue Li", "Yukang Feng", "Jianwen Sun", "Zizhen Li", "Fanrui Zhang", "Jiaxin Ai", "Zhixiang Wang", "Yuwei Wu", "Tong He", "Jiangmiao Pang", "Yu Qiao", "Yunde Jia", "Kaipeng Zhang"], "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.", "comment": "12 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15675v1", "AI": {"title_translation": "Sekai：一个用于世界探索的视频数据集", "tldr": "Sekai是一个高质量的第一人称视角全球视频数据集，旨在克服现有数据集的局限性，并促进视频生成和世界探索。", "motivation": "现有的视频生成数据集在世界探索训练方面存在局限性，包括地点有限、持续时间短、场景静态以及缺乏探索和世界相关的注释，这阻碍了交互式世界探索的发展。", "method": "本文介绍了Sekai数据集，一个高质量的第一人称视角全球视频数据集，包含来自100多个国家和地区750个城市的5,000多小时的步行或无人机视角视频。开发了一个高效的工具箱来收集、预处理和标注视频，包括地点、场景、天气、人群密度、字幕和摄像机轨迹。", "result": "实验证明了Sekai数据集的质量。研究人员使用该数据集的一个子集训练了一个名为YUME的交互式视频世界探索模型。", "conclusion": "Sekai数据集将有益于视频生成和世界探索领域，并激发有价值的应用。", "translation": "视频生成技术取得了显著进展，有望成为交互式世界探索的基础。然而，现有视频生成数据集不适合世界探索训练，因为它们存在一些局限性：地点有限、持续时间短、场景静态以及缺乏关于探索和世界的注释。在本文中，我们介绍了Sekai（日语中意为“世界”），一个高质量的第一人称视角全球视频数据集，为世界探索提供了丰富的注释。它包含来自100多个国家和地区750个城市的5,000多个小时的步行或无人机视角（FPV和UVA）视频。我们开发了一个高效且有效的工具箱来收集、预处理和标注视频，包括地点、场景、天气、人群密度、字幕和摄像机轨迹。实验证明了数据集的质量。并且，我们使用一个子集来训练一个交互式视频世界探索模型，名为YUME（日语中意为“梦想”）。我们相信Sekai将有益于视频生成和世界探索领域，并激发有价值的应用。", "summary": "本文介绍了Sekai数据集，一个为解决现有视频生成数据集在世界探索方面局限性而设计的高质量第一人称视角全球视频数据集。Sekai包含来自全球100多个国家和地区的海量视频数据和丰富的探索相关注释。研究者开发了一个高效的工具箱用于数据收集、预处理和标注，并成功利用该数据集训练了一个交互式视频世界探索模型YUME，证明了数据集的实用性和潜力。", "keywords": "视频数据集, 世界探索, 第一人称视角, 视频生成, Sekai", "comments": "Sekai数据集的创新之处在于其大规模和全球覆盖性，以及为世界探索专门设计的丰富注释，这填补了现有数据集的空白。其重要性在于为视频生成和交互式世界探索领域提供了前所未有的资源，有望推动相关技术的发展和实际应用。通过提供详细的场景、天气、人群密度等信息，它为训练更真实、更具沉浸感的视频生成模型奠定了基础。"}}
{"id": "2506.15446", "title": "Zero-Shot Reinforcement Learning Under Partial Observability", "authors": ["Scott Jeen", "Tom Bewley", "Jonathan M. Cullen"], "summary": "Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.", "comment": "Reinforcement Learning Conference 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15446v1", "AI": {"title_translation": "部分可观测性下的零样本强化学习", "tldr": "本文研究了零样本强化学习在部分可观测环境中的性能下降问题，并提出基于记忆的架构作为有效的解决方案，实验证明其优于无记忆基线。", "motivation": "现有的零样本强化学习方法依赖于马尔可夫状态完全可观测的假设，然而在许多现实世界应用中，马尔可夫状态往往是部分可观测的，这可能导致其性能下降。", "method": "探索了标准零样本强化学习方法在部分可观测性下的性能下降情况，并提出并评估了基于记忆的架构作为解决方案。", "result": "实验证明，在状态、奖励和动态变化均部分可观测的领域中，所提出的基于记忆的零样本强化学习方法比无记忆基线表现出更好的性能。", "conclusion": "基于记忆的架构是解决零样本强化学习在部分可观测环境下性能下降问题的有效方法。", "translation": "近期工作表明，在特定假设下，零样本强化学习（RL）方法在无奖励预训练后可以泛化到环境中任何未见过的任务。访问马尔可夫状态是其中一个假设，然而，在许多现实世界应用中，马尔可夫状态仅是部分可观测的。本文探讨了标准零样本RL方法在部分可观测性下性能如何下降，并表明，与单任务RL一样，基于记忆的架构是一种有效的补救措施。我们在状态、奖励和动态变化均部分可观测的领域中评估了我们的基于记忆的零样本RL方法，并显示出优于无记忆基线的性能。我们的代码已通过以下链接开源：https://enjeeneer.io/projects/bfms-with-memory/。", "summary": "本文研究了零样本强化学习在部分可观测环境中的性能表现，发现其在缺乏完全马尔可夫状态信息时性能会下降。为解决此问题，作者提出并评估了基于记忆的架构。实验结果表明，在状态、奖励和动态均部分可观测的场景下，基于记忆的方法相较于无记忆基线表现出显著的性能提升。", "keywords": "零样本强化学习, 部分可观测性, 基于记忆的架构, 泛化, 马尔可夫状态", "comments": "本文解决了零样本强化学习在现实世界应用中面临的一个关键挑战——部分可观测性问题。通过引入基于记忆的架构，该研究为零样本RL在更复杂、不完美信息环境下的应用提供了有效的解决方案，具有重要的实践意义。"}}
{"id": "2506.15682", "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model", "authors": ["Anirud Aggarwal", "Abhinav Shrivastava", "Matthew Gwilliam"], "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.", "comment": "29 pages, 22 figures, 9 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15682v1", "AI": {"title_translation": "进化缓存以加速您的现成扩散模型", "tldr": "提出ECAD，一种基于遗传算法的进化缓存方法，能为现成扩散模型学习高效的缓存策略，显著加速推理并保持高质量，优于现有方法。", "motivation": "扩散模型生成高质量图像但推理缓慢且计算成本高昂。现有通过缓存加速的方法依赖僵硬的启发式，导致加速有限或泛化性差。", "method": "提出ECAD（Evolutionary Caching to Accelerate Diffusion models），一种遗传算法，通过少量校准提示学习每个模型的、形成帕累托前沿的有效缓存调度。ECAD不修改网络参数或参考图像。", "result": "ECAD提供了显著的推理加速，能够精细控制质量-延迟权衡，并无缝适应不同的扩散模型。学习到的调度可以有效地泛化到校准期间未见的S分辨率和模型变体。在PixArt-alpha上，ECAD的调度在COCO FID上比现有SOTA方法提升4.47，推理加速从2.35倍提高到2.58倍。", "conclusion": "ECAD是一种可扩展且通用的扩散推理加速方法。", "translation": "扩散图像生成模型在生成高质量合成内容方面表现出色，但推理速度慢且计算成本高昂。先前的工作试图通过在扩散Transformer的推理步骤中缓存和重用特征来缓解这一问题。然而，这些方法通常依赖于僵硬的启发式规则，导致加速有限或在不同架构上的泛化性差。我们提出了加速扩散模型的进化缓存（ECAD），这是一种遗传算法，仅使用一小组校准提示，就能学习高效的、针对每个模型的缓存调度，形成帕累托前沿。ECAD不需要修改网络参数或参考图像。它提供了显著的推理加速，能够精细控制质量-延迟权衡，并无缝适应不同的扩散模型。值得注意的是，ECAD学习到的调度可以有效地泛化到校准期间未见的S分辨率和模型变体。我们在PixArt-alpha、PixArt-Sigma和FLUX-1.dev上，使用多种指标（FID、CLIP、Image Reward）在不同基准（COCO、MJHQ-30k、PartiPrompts）上评估了ECAD，展示了比先前方法持续的改进。在PixArt-alpha上，ECAD识别出一个调度，其性能在COCO FID上比先前的最先进方法提高了4.47，同时推理加速从2.35倍增加到2.58倍。我们的结果确立了ECAD作为一种可扩展和可泛化的扩散推理加速方法。我们的项目网站位于https://aniaggarwal.github.io/ecad，代码位于https://github.com/aniaggarwal/ecad。", "summary": "本文提出ECAD，一种基于遗传算法的进化缓存方法，旨在解决扩散模型推理速度慢和计算成本高的问题。ECAD通过学习每个模型的优化缓存调度，无需修改网络，实现了显著的推理加速和质量-延迟的灵活控制。实验证明，ECAD在多个扩散模型和基准测试上优于现有方法，并能有效泛化到新场景，确立了其作为可扩展和通用加速方案的地位。", "keywords": "扩散模型, 缓存, 遗传算法, 推理加速, 图像生成", "comments": "这项工作通过引入遗传算法来优化扩散模型的缓存策略，克服了传统启发式方法的局限性，具有显著的创新性。其核心优势在于能够为不同模型学习定制化的缓存调度，实现性能与效率的平衡，并且无需修改模型本身，这大大降低了应用门槛。泛化能力强也是一个亮点。"}}
{"id": "2506.14995", "title": "Improved Image Reconstruction and Diffusion Parameter Estimation Using a Temporal Convolutional Network Model of Gradient Trajectory Errors", "authors": ["Jonathan B. Martin", "Hannah E. Alderson", "John C. Gore", "Mark D. Does", "Kevin D. Harkins"], "summary": "Summary: Errors in gradient trajectories introduce significant artifacts and\ndistortions in magnetic resonance images, particularly in non-Cartesian imaging\nsequences, where imperfect gradient waveforms can greatly reduce image quality.\nPurpose: Our objective is to develop a general, nonlinear gradient system model\nthat can accurately predict gradient distortions using convolutional networks.\nMethods: A set of training gradient waveforms were measured on a small animal\nimaging system, and used to train a temporal convolutional network to predict\nthe gradient waveforms produced by the imaging system. Results: The trained\nnetwork was able to accurately predict nonlinear distortions produced by the\ngradient system. Network prediction of gradient waveforms was incorporated into\nthe image reconstruction pipeline and provided improvements in image quality\nand diffusion parameter mapping compared to both the nominal gradient waveform\nand the gradient impulse response function. Conclusion: Temporal convolutional\nnetworks can more accurately model gradient system behavior than existing\nlinear methods and may be used to retrospectively correct gradient errors.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.14995v1", "AI": {"title_translation": "使用时间卷积网络模型校正梯度轨迹误差，改进图像重建和扩散参数估计", "tldr": "本文提出一种使用时间卷积网络（TCN）模型预测和校正梯度系统非线性失真的方法，显著改善了磁共振图像重建质量和扩散参数映射。", "motivation": "梯度轨迹误差在磁共振图像中引入显著伪影和失真，尤其在非笛卡尔成像序列中，不完美的梯度波形会大大降低图像质量。本文旨在开发一个通用的非线性梯度系统模型，利用卷积网络准确预测梯度失真。", "method": "在小动物成像系统上测量训练梯度波形，并用其训练一个时间卷积网络来预测成像系统产生的梯度波形。", "result": "训练好的网络能够准确预测梯度系统产生的非线性失真。将网络预测的梯度波形整合到图像重建流程中，与标称梯度波形和梯度脉冲响应函数相比，图像质量和扩散参数映射均得到改善。", "conclusion": "时间卷积网络比现有线性方法能更准确地建模梯度系统行为，并可用于回顾性校正梯度误差。", "translation": "总结：梯度轨迹误差在磁共振图像中引入显著的伪影和失真，特别是在非笛卡尔成像序列中，不完美的梯度波形会大大降低图像质量。\n目的：我们的目标是开发一个通用的非线性梯度系统模型，该模型可以使用卷积网络准确预测梯度失真。\n方法：在一台小动物成像系统上测量了一组训练梯度波形，并用于训练一个时间卷积网络，以预测成像系统产生的梯度波形。\n结果：训练好的网络能够准确预测梯度系统产生的非线性失真。将网络预测的梯度波形整合到图像重建流程中，与标称梯度波形和梯度脉冲响应函数相比，图像质量和扩散参数映射均得到改善。\n结论：时间卷积网络比现有线性方法能更准确地建模梯度系统行为，并可用于回顾性校正梯度误差。", "summary": "本文提出了一种利用时间卷积网络（TCN）对磁共振成像中的梯度轨迹误差进行建模和校正的方法。通过训练TCN预测梯度系统产生的非线性失真，并将预测结果整合到图像重建流程中，实验结果表明该方法显著提高了图像质量和扩散参数映射的准确性，优于传统的线性校正方法。", "keywords": "梯度轨迹误差, 时间卷积网络, 图像重建, 扩散参数估计, 磁共振成像", "comments": "本文的创新点在于将时间卷积网络应用于磁共振梯度系统非线性失真的建模和校正，突破了传统线性模型的局限性。这对于提高非笛卡尔成像序列的图像质量具有重要意义，并为回顾性梯度误差校正提供了新的有效途径。"}}
{"id": "2506.15448", "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning", "authors": ["Guoguo Ai", "Hezhe Qiao", "Hui Yan", "Guansong Pang"], "summary": "Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.", "comment": "18 pages, 11 figures, 3 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15448v1", "AI": {"title_translation": "半监督图异常检测通过鲁棒同质性学习", "tldr": "RHO提出了一种鲁棒同质性学习方法，通过自适应频率响应滤波器和图常态对齐模块，有效解决了半监督图异常检测中正常节点同质性多样性的问题，并显著优于现有方法。", "motivation": "现有的半监督图异常检测方法假设正常节点具有相似的同质性，并且标记的正常节点能够很好地代表正常类的同质性模式。然而，在真实的GAD数据集中，正常节点可能表现出不同的同质性，导致这一假设不成立。", "method": "论文提出了RHO（Robust Homophily Learning）方法。RHO包含两个模块：自适应频率响应滤波器（AdaFreq）和图常态对齐（GNA）。AdaFreq学习一组自适应频谱滤波器，从节点属性的通道和跨通道视图中捕获具有不同同质性的标记正常节点的不同频率分量。GNA用于强制通道和跨通道同质性表示之间的一致性，以增强滤波器在两个视图中学习到的常态的鲁棒性。", "result": "在八个真实世界GAD数据集上的实验表明，RHO能够有效地学习小型正常节点集中多变且通常未被充分表示的同质性，并且显著优于最先进的竞争方法。", "conclusion": "RHO通过自适应学习正常节点的多样同质性，解决了半监督图异常检测中的一个关键挑战，并显著提升了异常检测性能。", "translation": "半监督图异常检测（GAD）利用一小部分标记的正常节点，从图中大量未标记的节点中识别异常节点。当前该领域的方法假设：1）正常节点共享相似的同质性水平；2）标记的正常节点能够很好地代表正常类中的同质性模式。然而，由于图中的正常节点在真实世界的GAD数据集中可能表现出多样化的同质性，这个假设往往不成立。在本文中，我们提出了RHO（鲁棒同质性学习），以自适应地学习这种同质性模式。RHO包含两个新颖的模块：自适应频率响应滤波器（AdaFreq）和图常态对齐（GNA）。AdaFreq学习一组自适应频谱滤波器，这些滤波器从节点属性的通道内和跨通道视图中捕获具有不同同质性的标记正常节点的不同频率分量。引入GNA是为了强制通道内和跨通道同质性表示之间的一致性，以增强滤波器在两个视图中学习到的常态的鲁棒性。在八个真实世界GAD数据集上的实验表明，RHO能够有效地学习小型正常节点集中多变且通常未被充分表示的同质性，并显著优于最先进的竞争方法。代码可在https://github.com/mala-lab/RHO获取。", "summary": "本文针对半监督图异常检测（GAD）中现有方法无法有效处理正常节点同质性多样性的问题，提出了RHO（鲁棒同质性学习）框架。RHO包含自适应频率响应滤波器（AdaFreq）用于捕获不同同质性的频率分量，以及图常态对齐（GNA）用于增强学习到的常态表示的鲁棒性。实验证明，RHO在多个真实世界数据集上能够有效学习多样化的同质性，并显著优于现有SOTA方法。", "keywords": "图异常检测, 半监督学习, 同质性学习, 鲁棒性, 频谱滤波器", "comments": "这篇论文通过引入自适应同质性学习机制，解决了半监督图异常检测中一个重要的实际问题，即正常节点同质性的多样性。AdaFreq和GNA模块的设计具有创新性，能够更精细地建模正常模式。其鲁棒性和优越的性能使其在图异常检测领域具有重要价值。"}}
{"id": "2506.15452", "title": "Warping and Matching Subsequences Between Time Series", "authors": ["Simiao Lin", "Wannes Meert", "Pieter Robberechts", "Hendrik Blockeel"], "summary": "Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15452v1", "AI": {"title_translation": "时间序列间子序列的扭曲与匹配", "tldr": "本文提出了一种新颖的技术，通过简化扭曲路径来突出、量化和可视化时间序列比较中的关键变换（如移位、压缩、振幅差异），从而增强可解释性。", "motivation": "比较时间序列在聚类和分类等各种任务中至关重要。虽然允许扭曲的弹性距离度量提供了稳健的定量比较，但缺乏在此基础上的定性比较。传统的可视化方法侧重于点对点对齐，未能传达子序列层面上更广泛的结构关系。这种限制使得难以理解一个时间序列相对于另一个是如何以及在哪里发生移位、加速或减速的。", "method": "我们提出了一种新颖的技术，通过简化扭曲路径来突出、量化和可视化关键变换（移位、压缩、振幅差异）。", "result": "我们的方法通过提供更清晰的时间序列间子序列匹配表示，增强了时间序列比较的可解释性。", "conclusion": "通过简化扭曲路径并可视化关键变换，本方法显著提高了时间序列比较的定性理解和可解释性。", "translation": "比较时间序列在聚类和分类等各种任务中至关重要。虽然允许扭曲的弹性距离度量提供了稳健的定量比较，但在此基础上的定性比较是缺失的。传统的可视化方法侧重于点对点对齐，未能传达子序列层面上更广泛的结构关系。这种限制使得难以理解一个时间序列相对于另一个是如何以及在哪里发生移位、加速或减速的。为了解决这个问题，我们提出了一种新颖的技术，通过简化扭曲路径来突出、量化和可视化关键变换（移位、压缩、振幅差异）。通过提供更清晰的时间序列间子序列匹配表示，我们的方法增强了时间序列比较的可解释性。", "summary": "本研究提出了一种创新方法，旨在提升时间序列比较的定性分析能力。针对现有弹性距离度量虽能提供定量比较但缺乏定性洞察的问题，以及传统可视化在子序列层面结构关系表示上的不足，该方法通过简化扭曲路径，有效突出、量化并可视化时间序列间的关键变换，如移位、压缩和振幅差异，从而显著增强了时间序列比较的可解释性。", "keywords": "时间序列比较, 子序列匹配, 扭曲路径, 可解释性, 数据可视化", "comments": "该论文的创新点在于将时间序列的定量弹性距离比较提升到定性分析的层面，通过可视化关键变换来增强可解释性，这对于理解时间序列间复杂的结构关系具有重要意义。它解决了现有方法在传达子序列层面更广泛结构关系方面的局限性。"}}
{"id": "2506.15479", "title": "Creating User-steerable Projections with Interactive Semantic Mapping", "authors": ["Artur André Oliveira", "Mateus Espadoto", "Roberto Hirata Jr.", "Roberto M. Cesar Jr.", "Alex C. Telea"], "summary": "Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15479v1", "AI": {"title_translation": "创建用户可控的交互式语义映射投影", "tldr": "该研究提出了一种新的用户引导投影框架，利用多模态大语言模型和自然语言提示，实现可定制且可解释的数据可视化，弥补了自动化降维与人机交互数据探索之间的鸿沟。", "motivation": "当前的降维技术无法探索数据中未直接以变量或类别标签形式存在的语义结构，因此需要一种新的方法来允许用户引导和解释数据可视化。", "method": "本文引入了一种新颖的用户引导投影框架，用于图像和文本数据。该框架通过多模态大语言模型（MLLMs）的零样本分类，实现可定制、可解释的数据可视化。它允许用户通过自然语言引导提示动态地控制投影，以指定数据维度中未明确存在的高级语义关系。", "result": "该方法在多个数据集上进行了评估，结果表明它不仅增强了聚类分离，还将降维过程转变为一个交互式、用户驱动的过程。", "conclusion": "该方法弥合了全自动化降维技术与以人为中心的数据探索之间的差距，提供了一种灵活且自适应的方式，根据特定的分析需求调整投影。", "translation": "降维（DR）技术将高维数据映射到低维空间。然而，当前的降维技术并非旨在探索不直接以变量或类别标签形式存在的语义结构。我们引入了一种新颖的图像和文本数据用户引导投影框架，该框架通过多模态大语言模型（MLLMs）的零样本分类，实现可定制、可解释的数据可视化。我们使用户能够通过自然语言引导提示动态地控制投影，以指定用户感兴趣的、未明确存在于数据维度中的高级语义关系。我们在几个数据集上评估了我们的方法，并表明它不仅增强了聚类分离，而且将降维转化为一个交互式、用户驱动的过程。我们的方法弥合了全自动化降维技术与以人为中心的数据探索之间的差距，提供了一种灵活且自适应的方式，根据特定的分析需求调整投影。", "summary": "本文提出了一种创新的用户引导投影框架，旨在解决现有降维技术无法有效探索隐含语义结构的问题。该框架利用多模态大语言模型（MLLMs）和零样本分类，允许用户通过自然语言提示动态地控制和定制图像及文本数据的投影，从而实现可解释的数据可视化。实验结果表明，该方法不仅能提升聚类效果，还将降维过程转变为交互式用户驱动模式，有效连接了自动化降维与以人为中心的数据探索。", "keywords": "降维, 用户引导投影, 语义映射, 多模态大语言模型, 数据可视化", "comments": "该论文的创新点在于将多模态大语言模型（MLLMs）与降维技术相结合，实现了用户通过自然语言提示来引导和定制数据投影。这极大地增强了数据可视化的交互性和可解释性，突破了传统降维方法在语义探索方面的局限性。其重要性在于它将降维从一个被动的自动化过程转变为一个主动的用户驱动过程，使得非显性语义关系也能被有效地可视化和探索，有望在数据分析和人机交互领域带来新的应用。"}}
{"id": "2506.15538", "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework", "authors": ["Laura Kopf", "Nils Feldhus", "Kirill Bykov", "Philine Lou Bommer", "Anna Hedström", "Marina M. -C. Höhne", "Oliver Eberle"], "summary": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15538v1", "AI": {"title_translation": "使用PRISM捕捉多义性：一个多概念特征描述框架", "tldr": "PRISM是一个新的框架，旨在解决现有神经网络特征描述方法中普遍存在的单义性假设问题，通过提供更细致的描述来准确捕捉多义性特征。", "motivation": "当前的特征描述方法存在鲁棒性有限以及错误地假设每个神经元只编码一个单一概念（单义性）的问题。尽管越来越多的证据表明神经元通常是多义的，但这种假设限制了特征描述的表达能力，并限制了它们捕捉模型内部编码的全部行为的能力。", "method": "本文引入了多义性特征识别和评分方法（PRISM），这是一个新颖的框架，旨在捕捉神经网络特征固有的复杂性。与以往为每个特征分配单一描述的方法不同，PRISM为多义性和单义性特征都提供了更细致的描述。", "result": "将PRISM应用于语言模型，并通过与现有方法的广泛基准测试，结果表明PRISM能够生成更准确、更忠实的特征描述，提高了整体描述质量（通过描述得分）以及在存在多义性时捕获不同概念的能力（通过多义性得分）。", "conclusion": "PRISM有效解决了神经网络特征解释中的多义性问题，从而提高了对模型行为的理解。", "translation": "自动化可解释性研究旨在识别神经网络特征中编码的概念，以增强人类对模型行为的理解。当前的特征描述方法面临两个关键挑战：鲁棒性有限和错误地假设每个神经元只编码一个单一概念（单义性），尽管越来越多的证据表明神经元通常是多义的。这种假设限制了特征描述的表达能力，并限制了它们捕捉模型内部编码的全部行为。为了解决这个问题，我们引入了多义性特征识别和评分方法（PRISM），这是一个捕捉神经网络特征固有复杂性的新颖框架。与以往为每个特征分配单一描述的方法不同，PRISM为多义性和单义性特征都提供了更细致的描述。我们将PRISM应用于语言模型，并通过与现有方法的广泛基准测试，证明我们的方法能够生成更准确、更忠实的特征描述，提高了整体描述质量（通过描述得分）以及在存在多义性时捕获不同概念的能力（通过多义性得分）。", "summary": "该论文提出了PRISM（多义性特征识别和评分方法），一个旨在解决现有神经网络特征描述方法中“单义性”假设限制的新框架。PRISM能够为多义性和单义性特征提供更细致的描述，从而更准确地捕捉神经网络的复杂性。通过在语言模型上的应用和基准测试，PRISM被证明能生成更高质量、更忠实的特征描述，并有效提升了识别多义概念的能力。", "keywords": "多义性, 神经网络可解释性, 特征描述, PRISM, 语言模型", "comments": "本文的创新之处在于明确提出并解决了神经网络特征中存在的“多义性”问题，这突破了传统解释方法中普遍存在的“单义性”假设。PRISM框架能够提供更细致和准确的特征描述，对于提高神经网络的可解释性和理解模型行为具有重要意义。"}}
{"id": "2506.14803", "title": "Omnidirectional Video Super-Resolution using Deep Learning", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter W. Eklund", "Sunil Aryal"], "summary": "Omnidirectional Videos (or 360{\\deg} videos) are widely used in Virtual\nReality (VR) to facilitate immersive and interactive viewing experiences.\nHowever, the limited spatial resolution in 360{\\deg} videos does not allow for\neach degree of view to be represented with adequate pixels, limiting the visual\nquality offered in the immersive experience. Deep learning Video\nSuper-Resolution (VSR) techniques used for conventional videos could provide a\npromising software-based solution; however, these techniques do not tackle the\ndistortion present in equirectangular projections of 360{\\deg} video signals.\nAn additional obstacle is the limited availability of 360{\\deg} video datasets\nfor study. To address these issues, this paper creates a novel 360{\\deg} Video\nDataset (360VDS) with a study of the extensibility of conventional VSR models\nto 360{\\deg} videos. This paper further proposes a novel deep learning model\nfor 360{\\deg} Video Super-Resolution (360{\\deg} VSR), called Spherical Signal\nSuper-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent\nmodelling with an attention mechanism, unbound from conventional VSR techniques\nlike alignment. With a purpose-built feature extractor and a novel loss\nfunction addressing spherical distortion, S3PO outperforms most\nstate-of-the-art conventional VSR models and 360{\\deg}~specific\nsuper-resolution models on 360{\\deg} video datasets. A step-wise ablation study\nis presented to understand and demonstrate the impact of the chosen\narchitectural sub-components, targeted training and optimisation.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.14803v1", "AI": {"title_translation": "全景视频超分辨率深度学习", "tldr": "本文针对全景视频分辨率低和现有超分辨率技术不适用于全景视频失真问题，提出了一个新的全景视频数据集和名为S3PO的深度学习模型，该模型在全景视频超分任务上表现优异。", "motivation": "360度视频在VR中广泛使用，但其有限的空间分辨率导致视觉质量受限。传统VSR技术不适用于360度视频的等距柱状投影失真，且缺乏360度视频数据集。", "method": "1. 创建了一个新的360度视频数据集(360VDS)。 2. 研究了传统VSR模型对360度视频的扩展性。 3. 提出了一个新的深度学习模型S3PO (Spherical Signal Super-resolution with a Proportioned Optimisation)，该模型采用循环建模和注意力机制，并包含专门的特征提取器和解决球面失真的新损失函数。 4. 进行了逐步消融研究。", "result": "S3PO在360度视频数据集上优于大多数最先进的传统VSR模型和360度专用超分辨率模型。", "conclusion": "S3PO模型有效解决了360度视频的超分辨率问题，提升了沉浸式体验的视觉质量，并通过消融研究验证了其组件的有效性。", "translation": "全景视频（或360度视频）广泛应用于虚拟现实（VR）中，以提供沉浸式和交互式的观看体验。然而，360度视频有限的空间分辨率导致每个视角无法用足够的像素表示，从而限制了沉浸式体验中提供的视觉质量。用于传统视频的深度学习视频超分辨率（VSR）技术可以提供一个有前景的基于软件的解决方案；然而，这些技术未能解决360度视频信号等距柱状投影中存在的失真问题。另一个障碍是用于研究的360度视频数据集的可用性有限。为了解决这些问题，本文创建了一个新颖的360度视频数据集（360VDS），并研究了传统VSR模型对360度视频的扩展性。本文进一步提出了一种用于360度视频超分辨率（360度VSR）的新型深度学习模型，名为球形信号超分辨率与比例优化（S3PO）。S3PO采用带有注意力机制的循环建模，不受传统VSR技术（如对齐）的束缚。通过专门构建的特征提取器和解决球面失真的新型损失函数，S3PO在360度视频数据集上优于大多数最先进的传统VSR模型和360度专用超分辨率模型。本文还进行了逐步消融研究，以理解和展示所选架构子组件、目标训练和优化的影响。", "summary": "本文旨在解决360度视频分辨率低和现有超分辨率技术不适用于其特有失真及数据集缺乏的问题。为此，作者创建了一个新的360度视频数据集(360VDS)，并提出了一个名为S3PO的深度学习模型，该模型通过循环建模、注意力机制、专用特征提取器和新颖的损失函数来处理球面失真。实验结果表明，S3PO在360度视频超分辨率任务上优于现有主流模型，并通过消融研究验证了其组件的有效性。", "keywords": "全景视频超分辨率, 深度学习, S3PO, 360度视频数据集, 球面失真", "comments": "本文创新性地提出了一个专门针对360度视频超分辨率的深度学习模型S3PO，并构建了稀缺的360度视频数据集。其对球面失真的处理和不受传统VSR对齐约束的设计是其亮点，有望显著提升VR体验的视觉质量。"}}
{"id": "2506.15492", "title": "LIT-LVM: Structured Regularization for Interaction Terms in Linear Predictors using Latent Variable Models", "authors": ["Mohammadreza Nemati", "Zhipeng Huang", "Kevin S. Xu"], "summary": "Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15492v1", "AI": {"title_translation": "LIT-LVM：使用潜在变量模型对线性预测器中交互项的结构化正则化", "tldr": "LIT-LVM是一种使用潜在变量模型对线性预测器中交互项进行结构化正则化处理的方法，在交互项数量远超样本量时，其预测精度优于现有方法。", "motivation": "线性预测器通过添加交互项可以建模特征间的非线性关系，但准确估计高维设置下交互项的系数是一个挑战，尤其当交互项数量远高于样本量时，容易出现过拟合。", "method": "本文提出LIT-LVM方法，假设不同交互项的系数具有近似的低维结构，并用低维空间中的潜在向量表示每个特征。这种低维表示被视为一种结构化正则化方法，能进一步减轻高维设置下的过拟合，超越了Lasso和Elastic Net等标准正则化器。", "result": "LIT-LVM在各种模拟和真实数据集上，特别是当交互项数量远大于样本数量时，比Elastic Net和分解机取得了更优异的预测精度。LIT-LVM还提供了特征的低维潜在表示，有助于可视化和分析它们之间的关系。", "conclusion": "LIT-LVM通过对线性预测器中的交互项进行结构化正则化，有效提升了高维设置下的预测精度，并提供了有用的特征关系可视化工具。", "translation": "统计学和机器学习中一些最简单但最常用的预测器使用特征的加权线性组合。此类线性预测器可以通过添加对应于所有特征对乘积的交互项来建模特征之间的非线性关系。我们考虑了准确估计线性预测器中交互项系数的问题。我们假设不同交互项的系数具有近似的低维结构，并用低维空间中的潜在向量表示每个特征。这种低维表示可以被视为一种结构化正则化方法，它在高维设置下进一步减轻了过拟合，超越了Lasso和Elastic Net等标准正则化器。我们证明了我们的方法，称为LIT-LVM，在各种模拟和真实数据上，特别是当交互项数量与样本数量相比很高时，比Elastic Net和分解机取得了更优异的预测精度。LIT-LVM还提供了特征的低维潜在表示，有助于可视化和分析它们之间的关系。", "summary": "本文提出LIT-LVM，一种基于潜在变量模型对线性预测器中交互项进行结构化正则化的方法。该方法假设交互项系数具有低维结构，通过将每个特征表示为低维潜在向量来缓解高维设置下的过拟合。实验证明，LIT-LVM在预测精度方面优于Elastic Net和分解机，尤其在交互项数量远超样本量的情况下表现出色，并能提供有用的特征低维表示用于分析。", "keywords": "线性预测器, 交互项, 结构化正则化, 潜在变量模型, 过拟合", "comments": "LIT-LVM的创新点在于将潜在变量模型引入到线性预测器中交互项的正则化，通过捕捉低维结构来有效处理高维数据中的过拟合问题。其重要性在于在高维、小样本场景下，能够提供比传统正则化方法更优的预测性能，并为特征关系分析提供了新的视角。"}}
{"id": "2506.15606", "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning", "authors": ["Gabrel J. Perin", "Runjin Chen", "Xuxi Chen", "Nina S. T. Hirata", "Zhangyang Wang", "Junyuan Hong"], "summary": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15606v1", "AI": {"title_translation": "LoX：低秩外推增强大型语言模型对抗微调的安全性", "tldr": "LoX提出了一种无训练的低秩外推方法，通过外推安全子空间来增强大型语言模型对良性或恶意微调攻击的安全性，显著降低了攻击成功率。", "motivation": "大型语言模型（LLMs）在实际应用中日益普及，但其安全性，特别是在应对有害问题方面的安全性，引起了广泛关注。尽管已通过对齐进行大量努力来提高模型安全性，但即使是良性数据，后续的微调也可能破坏已对齐模型的安全保护。本研究旨在解决这一安全漏洞。", "method": "本文提出了一种名为低秩外推（LoX）的新型无训练方法，通过外推已对齐LLM的安全子空间来增强其安全鲁棒性。该方法基于一个观察，即安全关键的低秩子空间对微调很敏感。LoX通过将LLM参数移动到一个更平坦的区域，从而降低其对扰动的敏感性。", "result": "LoX在对抗良性或恶意微调攻击方面显示出显著的鲁棒性改进，同时保留了模型适应新任务的能力。例如，LoX使攻击成功率（ASR）绝对降低了11%到54%。研究发现，LoX的成功归因于外推将LLM参数移动到一个更平坦的区域，从而降低了对扰动的敏感性。", "conclusion": "LoX是一种有效的无训练方法，能够显著增强大型语言模型对抗微调攻击的安全性，通过调整参数使其处于一个对扰动不那么敏感的区域，从而提高模型的鲁棒性。", "translation": "大型语言模型（LLMs）已成为现实世界应用中不可或缺的一部分。然而，它们的广泛采用引发了重大的安全问题，尤其是在回应社会有害问题方面。尽管通过对齐在提高模型安全性方面做出了巨大努力，但已对齐模型的安全保护仍然可能被随后的微调所破坏——即使额外的训练数据看起来是良性的。在本文中，我们通过实证证明，这种脆弱性源于LLM参数中安全关键的低秩子空间对微调的敏感性。基于这一洞察，我们提出了一种新颖的无训练方法，称为低秩外推（LoX），通过外推已对齐LLM的安全子空间来增强安全鲁棒性。我们的实验结果证实了LoX的有效性，证明了在对抗良性微调和恶意微调攻击方面，模型鲁棒性都有显著改善，同时保留了模型适应新任务的能力。例如，LoX使面对良性或恶意微调攻击的攻击成功率（ASR）绝对降低了11%到54%。通过调查参数的ASR图景，我们将LoX的成功归因于外推将LLM参数移动到一个更平坦的区域，从而降低了对扰动的敏感性。代码可在github.com/VITA-Group/LoX获取。", "summary": "本文提出了一种名为LoX（低秩外推）的无训练方法，旨在解决大型语言模型（LLMs）在微调后安全性被削弱的问题。研究发现，LLM参数中安全关键的低秩子空间对微调敏感。LoX通过外推已对齐LLM的安全子空间来增强其鲁棒性，实验证明该方法能显著降低良性或恶意微调攻击的攻击成功率（ASR），例如降低11%至54%，同时保持模型适应新任务的能力。LoX的成功归因于它将LLM参数移动到更平坦、对扰动不敏感的区域。", "keywords": "大型语言模型, 安全性, 微调, 低秩外推, 鲁棒性", "comments": "LoX提出了一种创新的、无训练的方法来解决LLM安全性在微调后被破坏的关键问题。其核心洞察在于识别安全关键的低秩子空间对微调的敏感性，并通过外推将其参数移动到更鲁棒的区域。这种方法不仅有效，而且由于其无训练的特性，具有很高的实用价值和效率。它为提高LLM在实际部署中的安全性提供了一个有前景的方向。"}}
{"id": "2506.15499", "title": "Pixel-level Certified Explanations via Randomized Smoothing", "authors": ["Alaa Anani", "Tobias Lorenz", "Mario Fritz", "Bernt Schiele"], "summary": "Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15499v1", "AI": {"title_translation": "像素级认证解释通过随机平滑", "tldr": "本文提出了一个通过随机平滑来保证像素级归因方法鲁棒性的认证框架，解决了现有归因解释非鲁棒性问题，并证明了其方法的有效性。", "motivation": "现有的事后归因方法生成的解释（归因图）对微小的输入扰动非常敏感，导致其不可信赖。因此，需要严格的像素级归因分数鲁棒性保证。", "method": "本文引入了第一个认证框架，利用随机平滑技术为任何黑盒归因方法提供像素级鲁棒性保证。通过稀疏化和平滑归因图，将任务重新定义为分割问题，并针对L2范数有界扰动认证每个像素的重要性。同时提出了三个评估指标来评估认证鲁棒性、定位和忠实度。", "result": "对5个ImageNet模型上的12种归因方法进行广泛评估表明，本文的认证归因是鲁棒的、可解释的和忠实的。", "conclusion": "本文提出的认证归因方法能够实现下游任务的可靠使用。", "translation": "事后归因方法旨在通过突出有影响力的输入像素来解释深度学习预测。然而，这些解释是非鲁棒的：微小、难以察觉的输入扰动可以显著改变归因图，同时保持相同的预测。这种脆弱性削弱了它们的信任度，并要求对像素级归因分数进行严格的鲁棒性保证。我们引入了第一个认证框架，该框架利用随机平滑技术，为任何黑盒归因方法提供像素级鲁棒性保证。通过稀疏化和平滑归因图，我们将任务重新定义为分割问题，并针对L2范数有界扰动认证每个像素的重要性。我们进一步提出了三个评估指标来评估认证鲁棒性、定位和忠实度。对5个ImageNet模型上的12种归因方法进行广泛评估表明，我们的认证归因是鲁棒的、可解释的、忠实的，从而可以在下游任务中可靠使用。我们的代码位于 https://github.com/AlaaAnani/certified-attributions。", "summary": "本文针对现有深度学习归因方法解释非鲁棒性的问题，提出了一个基于随机平滑的像素级认证框架。该框架能够为任何黑盒归因方法提供像素级鲁棒性保证，通过将归因图稀疏化和平滑化，并将其重新定义为分割问题，从而认证每个像素的重要性。实验结果表明，该方法生成的归因具有鲁棒性、可解释性和忠实度，能够支持下游任务的可靠应用。", "keywords": "像素级解释, 认证鲁棒性, 随机平滑, 归因方法, 深度学习解释", "comments": "本文的创新点在于首次提出了一个通用的认证框架，通过随机平滑技术为像素级归因方法提供鲁棒性保证。这对于提升深度学习模型解释的信任度至关重要，因为非鲁棒的解释会严重影响其在实际应用中的可靠性。将归因问题转化为分割问题并进行认证，是一种新颖且有效的方法。"}}
{"id": "2506.15651", "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning", "authors": ["Tevin Wang", "Chenyan Xiong"], "summary": "Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15651v1", "AI": {"title_translation": "AutoRule：推理思维链提取的基于规则的奖励改进偏好学习", "tldr": "AutoRule是一种自动化方法，通过从偏好反馈中提取规则并将其转化为基于规则的奖励，显著提高了RLHF的性能，减少了奖励欺骗。", "motivation": "当前基于规则的奖励方法在从人类反馈中强化学习（RLHF）方面很有前景，但通常依赖于手动规则工程。本文旨在解决手动规则工程的限制，提供一种自动化方法。", "method": "AutoRule通过三个阶段自动化提取规则：1. 利用推理模型解释用户偏好；2. 从这些解释的推理链中识别候选规则；3. 将其合成为统一的规则集。然后，使用语言模型验证器计算每个输出满足规则的比例，并将其作为辅助奖励与学习到的奖励模型一起用于策略优化。", "result": "使用AutoRule训练Llama-3-8B模型，在AlpacaEval2.0上，长度控制的胜率相对提高了28.6%；在MT-Bench的保留子集上，第二轮性能相对提高了6.1%，优于没有基于规则辅助奖励的GRPO基线。分析证实提取的规则与数据集偏好高度一致，并且AutoRule在运行两个回合时相比学习到的奖励模型减少了奖励欺骗。案例研究表明提取的规则捕捉了不同数据集中独特的价值特质。", "conclusion": "AutoRule提供了一种有效且自动化的方法，通过从偏好反馈中提取规则并将其作为辅助奖励，显著提升了RLHF的性能，减少了奖励欺骗，并捕捉了数据集中独特的价值偏好。", "translation": "基于规则的奖励为从人类反馈中强化学习（RLHF）提供了一种有前景的策略，但目前的方法通常依赖于手动规则工程。我们提出了AutoRule，这是一种完全自动化的方法，用于从偏好反馈中提取规则并将其制定为基于规则的奖励。AutoRule的提取分三个阶段进行：它利用推理模型解释用户偏好，从这些解释的推理链中识别候选规则，并将其合成为统一的规则集。利用最终确定的规则集，我们采用语言模型验证器来计算每个输出满足规则的比例，将此指标作为辅助奖励与策略优化期间的学习奖励模型一起使用。与使用相同学习奖励模型但没有基于规则辅助奖励的GRPO基线相比，使用AutoRule训练Llama-3-8B模型，在AlpacaEval2.0上，长度控制的胜率相对提高了28.6%，在保留的MT-Bench子集上，第二轮性能相对提高了6.1%。我们的分析证实，提取的规则与数据集偏好表现出良好的一致性。我们发现，AutoRule在运行两个回合时，相比学习到的奖励模型减少了奖励欺骗。最后，我们的案例研究表明，提取的规则捕捉了不同数据集中独特的价值特质。提取的规则在附录中提供，代码已在https://github.com/cxcscmu/AutoRule开源。", "summary": "AutoRule提出了一种自动化的方法，通过从用户偏好反馈中提取规则并将其转化为基于规则的辅助奖励，以改进强化学习。该方法分三阶段提取规则：解释偏好、识别候选规则并合成统一规则集。实验结果显示，AutoRule显著提升了模型性能，减少了奖励欺骗，并能捕捉数据集特有的偏好，证明了其在RLHF中的有效性。", "keywords": "自动化规则提取, 偏好学习, 强化学习, 思维链, 奖励模型", "comments": "AutoRule的创新之处在于其全自动的规则提取流程，摆脱了传统RLHF中对人工规则工程的依赖。通过将推理思维链与规则提取相结合，它能够有效地从复杂的偏好数据中学习可解释的奖励信号。其在多个基准测试上的显著性能提升以及对奖励欺骗的缓解，凸显了该方法在提升大型语言模型对齐方面的潜力。该研究为未来RLHF的自动化和可解释性提供了新的方向。"}}
{"id": "2506.15506", "title": "Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review", "authors": ["Salijona Dyrmishi", "Mohamed Djilani", "Thibault Simonetto", "Salah Ghamizi", "Maxime Cordy"], "summary": "Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.", "comment": "This paper is currently under review at ACM Computing Surveys", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15506v1", "AI": {"title_translation": "对表格机器学习对抗性攻击的见解：一项系统文献综述", "tldr": "该论文首次系统综述了针对表格机器学习模型的对抗性攻击。", "motivation": "机器学习领域的对抗性攻击研究在计算机视觉和自然语言处理等领域已得到广泛回顾，但在表格数据方面的研究仍然分散，因此需要一个系统性的综述。", "method": "本文提供了一个首次专注于针对表格机器学习模型的对抗性攻击的系统文献综述。", "result": "综述突出关键趋势，分类攻击策略，分析它们如何解决实际应用考虑，并概述当前挑战和开放研究问题。", "conclusion": "通过提供清晰和结构化的概述，本综述旨在指导未来理解和解决表格机器学习中对抗性漏洞的工作。", "translation": "机器学习中的对抗性攻击已在计算机视觉和自然语言处理等领域得到广泛回顾，但针对表格数据的研究仍然分散。本文提供了第一个专注于针对表格机器学习模型的对抗性攻击的系统文献综述。我们强调了关键趋势，对攻击策略进行了分类，并分析了它们如何解决实际应用中的考虑因素。此外，我们还概述了当前的挑战和开放研究问题。通过提供清晰和结构化的概述，本综述旨在指导未来在理解和解决表格机器学习中对抗性漏洞方面的努力。", "summary": "本文首次对针对表格机器学习模型的对抗性攻击进行了系统文献综述。它分类了攻击策略，分析了实际应用考虑，并指出了当前挑战和开放研究问题，旨在为未来研究提供指导。", "keywords": "对抗性攻击, 表格机器学习, 系统文献综述, 漏洞, 攻击策略", "comments": "这篇论文的创新之处在于它是首次专门针对表格机器学习对抗性攻击的系统综述，填补了该领域研究分散的空白。其重要性在于为研究人员提供了结构化的概览和未来研究方向的指导，有助于推动表格数据对抗性攻击研究的进展。"}}
{"id": "2506.15507", "title": "Over-squashing in Spatiotemporal Graph Neural Networks", "authors": ["Ivan Marisca", "Jacob Bamberger", "Cesare Alippi", "Michael M. Bronstein"], "summary": "Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15507v1", "AI": {"title_translation": "时空图神经网络中的过压缩问题", "tldr": "本文首次形式化并分析了时空图神经网络（STGNNs）中的过压缩问题，发现其与静态GNNs不同，且卷积STGNNs反而倾向于传播时间上较远的信息，不同处理范式受影响程度相同。", "motivation": "静态图神经网络中的“过压缩”问题（即远距离节点信息交换困难）已得到广泛研究，但在处理序列信息的时空图神经网络（STGNNs）中，这个问题尚未被探索。由于时间维度的存在，STGNNs面临的信息传播挑战更为严峻。", "method": "本文形式化了时空过压缩问题，并分析了其与静态情况的不同特征。通过理论分析，证明了卷积STGNNs在信息传播上的反直觉偏好，并证明了不同时空处理范式（时间-空间或时间-然后-空间）受过压缩现象的影响程度相同。研究结果在合成和真实世界数据集上得到了验证。", "result": "研究发现时空过压缩问题与静态情况具有明显区别。反直觉地，卷积STGNNs更倾向于传播时间上遥远而非接近的信息。此外，理论证明了遵循“时间-空间”或“时间-然后-空间”处理范式的架构受过压缩现象的影响程度相同。", "conclusion": "本研究为时空图神经网络的操作动态提供了更深入的见解，并为设计更有效、更鲁棒的STGNNs提供了有原则的指导。", "translation": "图神经网络（GNNs）在各个领域取得了显著成功。然而，最近的理论进展已经确定了其信息传播能力方面的基本限制，例如过压缩问题，即远距离节点无法有效交换信息。虽然在静态环境中得到了广泛研究，但这个问题在处理与图节点相关联序列的时空图神经网络（STGNNs）中仍未被探索。尽管如此，时间维度通过增加必须传播的信息量而加剧了这一挑战。在这项工作中，我们形式化了时空过压缩问题，并展示了其与静态情况相比的独特特征。我们的分析表明，反直觉地，卷积STGNNs更倾向于传播时间上遥远而非接近的信息。此外，我们证明了遵循时间-空间或时间-然后-空间处理范式架构同样受这种现象的影响，为计算效率高的实现提供了理论依据。我们在合成和真实世界数据集上验证了我们的发现，为它们的操作动态提供了更深入的见解，并为更有效的设计提供了有原则的指导。", "summary": "本文首次形式化并探讨了时空图神经网络（STGNNs）中的“过压缩”问题，该问题在静态GNNs中已得到研究但在STGNNs中尚未被探索。研究发现时空过压缩具有独特的特性，反直觉地显示卷积STGNNs偏好传播时间上更远的信息。此外，理论证明了不同的时空处理范式（时间-空间或时间-然后-空间）对过压缩的影响程度相同，为高效实现提供了依据。通过实验验证，本研究为STGNNs的设计提供了深入见解和指导。", "keywords": "时空图神经网络, 过压缩, 信息传播, 卷积GNN, 理论分析", "comments": "这篇论文通过首次形式化时空图神经网络中的“过压缩”问题，填补了该领域的一个重要空白。其创新之处在于揭示了时空背景下“过压缩”的独特表现，特别是卷积STGNNs反直觉地偏好远距离时间信息传播的发现，以及对不同处理范式影响的理论证明。这不仅加深了对STGNNs信息传播机制的理解，也为未来设计更高效、更鲁棒的STGNNs提供了重要的理论指导和实践依据。"}}
{"id": "2506.15679", "title": "Dense SAE Latents Are Features, Not Bugs", "authors": ["Xiaoqing Sun", "Alessandro Stolfo", "Joshua Engels", "Ben Wu", "Senthooran Rajamanoharan", "Mrinmaya Sachan", "Max Tegmark"], "summary": "Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15679v1", "AI": {"title_translation": "密集型稀疏自编码器潜在变量是特征，而非缺陷", "tldr": "本文系统研究了稀疏自编码器（SAE）中密集激活的潜在变量，发现它们并非训练伪影，而是反映了语言模型中有意义的表示，并具有特定的功能角色。", "motivation": "稀疏自编码器（SAE）旨在通过强制稀疏性约束从语言模型中提取可解释的特征。然而，许多SAE潜在变量激活频繁（即是“密集”的），这引发了它们可能是训练过程产生的 undesirable artifacts 的担忧。", "method": "系统研究了密集潜在变量的几何、功能和起源。首先，证明了密集潜在变量倾向于形成对极对，并重建残差流中的特定方向；消融其子空间会抑制新密集特征的出现。其次，引入了密集潜在变量的分类法，识别出与位置跟踪、上下文绑定、熵调节、字母特异性输出信号、词性以及主成分重建相关的类别。最后，分析了这些特征在层间的演变。", "result": "密集潜在变量不仅持续存在，而且通常反映有意义的模型表示。它们倾向于形成对极对，重建残差流中的特定方向。消融其子空间能抑制新密集特征的出现，表明高密度特征是残差空间的内在属性。识别出多种功能性密集潜在变量，如位置跟踪、上下文绑定等。特征在层间演变：早期层是结构特征，中间层是语义特征，最后层是输出导向信号。", "conclusion": "密集潜在变量在语言模型计算中扮演功能性角色，不应被视为训练噪声。", "translation": "稀疏自编码器（SAE）旨在通过强制稀疏性约束从语言模型中提取可解释的特征。理想情况下，训练SAE会产生既稀疏又语义上有意义的潜在变量。然而，许多SAE潜在变量激活频繁（即是“密集”的），这引发了它们可能是训练过程产生的 undesirable artifacts 的担忧。在这项工作中，我们系统地研究了密集潜在变量的几何、功能和起源，并表明它们不仅持续存在，而且通常反映有意义的模型表示。我们首先证明，密集潜在变量倾向于形成对极对，重建残差流中的特定方向，并且消融其子空间会抑制重新训练的SAE中新密集特征的出现——这表明高密度特征是残差空间的内在属性。然后，我们引入了密集潜在变量的分类法，识别出与位置跟踪、上下文绑定、熵调节、字母特异性输出信号、词性以及主成分重建相关的类别。最后，我们分析了这些特征在层间的演变，揭示了从早期层的结构特征，到中间层的语义特征，再到模型最后层的面向输出信号的转变。我们的研究结果表明，密集潜在变量在语言模型计算中扮演功能性角色，不应被视为训练噪声。", "summary": "本文系统研究了稀疏自编码器（SAE）中被误解为训练伪影的密集潜在变量。研究发现，这些密集潜在变量并非缺陷，而是语言模型中有意义的表示，它们形成对极对并重建残差流方向。通过引入分类法，论文揭示了密集潜在变量在位置跟踪、上下文绑定、熵调节等方面的功能，并分析了它们在模型层间的演变。研究结论强调密集潜在变量在语言模型计算中具有重要功能性作用。", "keywords": "稀疏自编码器, 密集潜在变量, 语言模型, 可解释性, 特征分析", "comments": "这项工作挑战了稀疏自编码器领域中关于密集潜在变量的普遍看法，即它们是训练过程的 undesirable artifacts。通过深入分析其几何、功能和起源，论文揭示了这些密集特征的内在重要性，并为理解语言模型内部表示提供了新的视角。其创新之处在于将“缺陷”转化为有意义的“特征”，有助于提高SAE的可解释性和实用性。"}}
{"id": "2506.15513", "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Luong Van Nghia"], "summary": "Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.", "comment": "11 pages, 7 figures, 5 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15513v1", "AI": {"title_translation": "RePCS：诊断LLM驱动的检索增强生成中的数据记忆", "tldr": "RePCS是一种轻量级、黑盒诊断方法，用于检测RAG系统中LLM是否依赖记忆数据而非检索证据。", "motivation": "RAG系统中的LLM可能依赖记忆的训练数据，绕过检索到的证据，产生污染的输出。需要一种无需模型访问或再训练的方法来检测这种行为。", "method": "本文引入了检索路径污染评分（RePCS），这是一种诊断方法，通过计算仅使用查询的参数路径和使用查询及检索上下文的检索增强路径之间输出分布的Kullback-Leibler（KL）散度来检测模型是否依赖记忆数据。低散度表明检索上下文影响最小，指示潜在的记忆化。该方法模型无关，无需梯度或内部状态访问，且仅增加一次额外的正向传播。", "result": "在Prompt-WNQA基准测试中，RePCS的ROC-AUC达到0.918，比现有最强方法高出6.5个百分点。同时，在NVIDIA T4 GPU上的延迟开销低于4.7%。", "conclusion": "RePCS提供了一种轻量级、黑盒的安全保障，用于验证RAG系统是否有效地利用了检索，在安全关键应用中尤其有价值。", "translation": "检索增强生成（RAG）已成为使用当前外部信息更新大型语言模型（LLM）响应的常见策略。然而，模型可能仍然依赖记忆的训练数据，绕过检索到的证据，并产生被污染的输出。我们引入了检索路径污染评分（RePCS），这是一种诊断方法，无需模型访问或再训练即可检测此类行为。RePCS通过计算其输出分布之间的Kullback-Leibler（KL）散度，比较两条推理路径：(i)仅使用查询的参数路径，以及(ii)使用查询和检索上下文的检索增强路径。低散度表明检索上下文影响最小，表明可能存在记忆化。此过程与模型无关，无需梯度或内部状态访问，并且仅增加一次额外的正向传播。我们进一步推导了PAC风格的保证，将KL阈值与用户定义的假阳性率和假阴性率联系起来。在Prompt-WNQA基准测试中，RePCS的ROC-AUC达到0.918。该结果比现有最强方法高出6.5个百分点，同时在NVIDIA T4 GPU上的延迟开销低于4.7%。RePCS提供了一种轻量级、黑盒的安全保障，用于验证RAG系统是否有效地利用了检索，使其在安全关键应用中特别有价值。", "summary": "本文提出了RePCS，一种用于诊断LLM驱动的检索增强生成（RAG）系统中数据记忆化的方法。RePCS通过比较LLM在仅使用查询和同时使用查询与检索上下文两种推理路径下的输出分布的KL散度，来判断模型是否有效利用了检索到的证据。该方法无需模型访问或再训练，且计算开销低。实验结果表明，RePCS在检测数据记忆化方面表现出色，并显著优于现有方法，尤其适用于需要高可靠性的安全关键应用。", "keywords": "RAG, LLM, 数据记忆化, RePCS, KL散度", "comments": "RePCS的创新之处在于其无需模型内部访问或再训练即可诊断RAG系统中数据记忆化的能力，这使其成为一个高度实用且易于部署的工具。通过比较输出分布的KL散度来评估检索证据的影响，提供了一种新颖且有效的黑盒检测机制。其在性能上的显著提升以及低延迟开销，进一步增强了其在实际应用中的吸引力，特别是在对LLM响应可靠性有严格要求的安全关键领域。"}}
{"id": "2506.15535", "title": "A Simplified Analysis of SGD for Linear Regression with Weight Averaging", "authors": ["Alexandru Meterez", "Depen Morwani", "Costin-Andrei Oncescu", "Jingfeng Wu", "Cengiz Pehlevan", "Sham Kakade"], "summary": "Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15535v1", "AI": {"title_translation": "带有权重平均的线性回归SGD简化分析", "tldr": "本文通过简单的线性代数工具，简化了带有权重平均的线性回归SGD的理论分析，并恢复了与现有工作相同的偏差和方差界限。", "motivation": "现有工作对过参数化模型中随机梯度下降（SGD）的理论理解促进了优化算法的发展。作者旨在提供一个简化的分析方法，使线性回归中SGD的理解更易于访问，并恢复与现有复杂分析相同的理论界限。", "method": "作者采用简单的线性代数工具，对线性回归中SGD的偏差和方差界限进行了简化分析，成功地绕过了对半正定（PSD）矩阵算子进行复杂操作的要求。", "result": "本研究的简化分析成功地恢复了与先前研究（Zou et al., 2021）所提供的相同的偏差和方差界限。", "conclusion": "这项工作使线性回归中SGD的分析变得非常易于理解和访问，并有望在未来帮助进一步分析小批量处理和学习率调度，从而改进实际模型的训练。", "translation": "理论上理解过参数化模型中的随机梯度下降（SGD）促成了当今实践中广泛使用的几种优化算法的开发。Zou等人（2021）最近的工作基于风险的偏差-方差分解，为使用恒定学习率的线性回归中的SGD优化提供了精确的速率，包括有无尾部迭代平均的情况。在我们的工作中，我们提供了一种简化的分析，基于简单的线性代数工具，恢复了Zou等人（2021）提供的相同偏差和方差界限，绕过了操作半正定（PSD）矩阵算子的要求。我们相信我们的工作使线性回归中SGD的分析非常易于理解，并将有助于进一步分析小批量处理和学习率调度，从而改进实际模型的训练。", "summary": "本文提出了一种简化分析方法，用于理解过参数化线性回归模型中随机梯度下降（SGD）的性能，特别是涉及权重平均的情况。通过利用简单的线性代数工具，该方法成功地恢复了先前复杂分析（Zou et al., 2021）中得到的相同偏差和方差界限，避免了对半正定矩阵算子的复杂操作。这项工作旨在提高SGD理论分析的可访问性，并为未来在小批量训练和学习率调度方面的改进提供基础，从而有助于实际模型的有效训练。", "keywords": "随机梯度下降, 线性回归, 权重平均, 简化分析, 偏差-方差分解", "comments": "本文的主要创新在于其分析的简化性，通过使用更直接的线性代数工具，使得对SGD在线性回归中的理论理解变得更加平易近人。这对于推动该领域的研究具有重要意义，因为它降低了理论分析的门槛，并可能加速对更复杂训练策略（如小批量和学习率调度）的探索。"}}
{"id": "2506.15544", "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning", "authors": ["Roger Creus Castanyer", "Johan Obando-Ceron", "Lu Li", "Pierre-Luc Bacon", "Glen Berseth", "Aaron Courville", "Pablo Samuel Castro"], "summary": "Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15544v1", "AI": {"title_translation": "深度强化学习中大规模稳定学习的稳定梯度", "tldr": "深度强化学习网络扩展时性能下降，本文通过分析指出非平稳性和梯度病理是原因，并提出简单的干预措施来稳定梯度流，实现大规模下的强大性能。", "motivation": "扩展深度强化学习网络时性能下降，但失败的根本原因尚不清楚。现有方法复杂且未能揭示根本原因。", "method": "进行了一系列经验分析，指出非平稳性与梯度病理（由次优架构选择引起）是规模挑战的根本原因。提出了一系列直接干预措施来稳定梯度流。", "result": "所提出的干预措施简单易实现，兼容现有算法，能在不同网络深度和宽度下实现稳定的性能，即使在大规模下也能获得强大的性能。在多种智能体和环境套件上验证了发现。", "conclusion": "通过稳定梯度流的简单干预措施，可以有效解决深度强化学习网络大规模扩展时的性能下降问题，实现鲁棒的性能。", "translation": "扩展深度强化学习网络具有挑战性，并且通常会导致性能下降，但这种失败模式的根本原因仍然知之甚少。最近的一些工作提出了解决这一问题的机制，但它们通常很复杂，未能突出导致这种困难的原因。在这项工作中，我们进行了一系列经验分析，这些分析表明，非平稳性与梯度病理的结合（由于次优的架构选择）是规模挑战的根本原因。我们提出了一系列直接干预措施来稳定梯度流，从而在各种网络深度和宽度下实现鲁棒的性能。我们的干预措施易于实施，并且与成熟的算法兼容，从而形成一种有效的机制，即使在大规模下也能实现强大的性能。我们在各种智能体和环境套件上验证了我们的发现。", "summary": "本文探讨了深度强化学习网络大规模扩展时性能下降的问题，指出其根源在于非平稳性与次优架构选择导致的梯度病理。作者通过经验分析验证了这一观点，并提出了一系列简单有效的梯度流稳定干预措施。这些措施易于实现且兼容现有算法，能显著提升网络在不同深度和宽度下的鲁棒性能，即使在大型规模下也能保持强大表现。", "keywords": "深度强化学习, 梯度稳定, 规模化学习, 非平稳性, 梯度病理", "comments": "这篇论文的创新点在于它通过经验分析明确指出了深度强化学习在大规模扩展时遇到的性能下降的根本原因：非平稳性与梯度病理。与以往复杂的方法不同，本文提出的干预措施简单、易于实现且兼容性强，这对于实际应用具有重要价值。它提供了一种有效且实用的方法来解决深度强化学习中的扩展性挑战。"}}
{"id": "2506.15554", "title": "DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones", "authors": ["Akhil Singampalli", "Danish Gufran", "Sudeep Pasricha"], "summary": "Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15554v1", "AI": {"title_translation": "DAILOC：用于智能手机室内定位的域增量学习", "tldr": "DAILOC是一个新的域增量学习框架，通过解耦和记忆引导对齐，显著提高了基于Wi-Fi指纹的室内定位的准确性，解决了设备异构性和时间变化导致的域漂移和灾难性遗忘问题。", "motivation": "基于Wi-Fi指纹的室内定位在实际部署中面临由于设备异构性和室内环境时间变化引起的域漂移带来的显著挑战。现有方法通常独立解决这些问题，导致泛化能力差和随时间推移容易发生灾难性遗忘。", "method": "本文提出了DAILOC，一个新颖的域增量学习框架，它联合解决了时间和设备引起的域漂移。DAILOC引入了一种新颖的解耦策略，使用多级变分自编码器将域漂移从位置相关特征中分离出来。此外，还引入了一种新的记忆引导类潜在对齐机制，以解决随时间推移的灾难性遗忘效应。", "result": "在多部智能手机、建筑物和时间实例上的实验表明，DAILOC显著优于最先进的方法，平均误差降低高达2.74倍，最差情况误差降低高达4.6倍。", "conclusion": "DAILOC通过有效解决域漂移和灾难性遗忘问题，显著提升了基于Wi-Fi指纹的室内定位的鲁棒性和准确性。", "translation": "基于Wi-Fi指纹的室内定位在实际部署中面临由于设备异构性和室内环境时间变化引起的域漂移带来的显著挑战。现有方法通常独立解决这些问题，导致泛化能力差和随时间推移容易发生灾难性遗忘。在这项工作中，我们提出了DAILOC，一个新颖的域增量学习框架，它联合解决了时间和设备引起的域漂移。DAILOC引入了一种新颖的解耦策略，使用多级变分自编码器将域漂移从位置相关特征中分离出来。此外，我们引入了一种新的记忆引导类潜在对齐机制，以解决随时间推移的灾难性遗忘效应。在多部智能手机、建筑物和时间实例上的实验表明，DAILOC显著优于最先进的方法，平均误差降低高达2.74倍，最差情况误差降低高达4.6倍。", "summary": "本文提出了DAILOC，一个新颖的域增量学习框架，旨在解决基于Wi-Fi指纹的智能手机室内定位中因设备异构性和时间变化引起的域漂移及灾难性遗忘问题。DAILOC通过多级变分自编码器实现域漂移与位置特征的解耦，并引入记忆引导类潜在对齐机制来对抗遗忘。实验证明，DAILOC在定位精度上显著优于现有SOTA方法。", "keywords": "室内定位, Wi-Fi指纹, 域增量学习, 域漂移, 灾难性遗忘", "comments": "DAILOC的创新之处在于其联合处理时间性和设备引起的域漂移，并通过解耦策略和记忆引导机制有效解决了传统方法中泛化能力差和灾难性遗忘的问题。其在实际部署中的性能提升潜力巨大，对于提升室内定位的鲁棒性具有重要意义。"}}
{"id": "2506.15559", "title": "Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates", "authors": ["Danish Gufran", "Sudeep Pasricha"], "summary": "Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15559v1", "AI": {"title_translation": "迈向可解释的室内定位：使用逻辑门解释Wi-Fi指纹上的神经网络学习", "tldr": "LogNet是一个基于逻辑门的新框架，旨在解释和增强基于深度学习的室内定位，它通过提高可解释性来提高性能，减少误差，缩小模型大小并降低延迟。", "motivation": "现有的深度学习室内定位模型是黑箱模型，缺乏可解释性，难以理解预测机制、模型对噪声的响应以及时间变化的影响，从而影响长期可靠性。", "method": "引入了LogNet，一个新颖的基于逻辑门的框架，用于解释和增强基于深度学习的室内定位。LogNet通过识别哪些接入点（AP）对每个参考点（RP）最具影响力，并揭示环境噪声如何扰乱深度学习驱动的定位决策，从而实现透明推理。", "result": "LogNet不仅能够解释深度学习模型的内部行为，还显著提升了性能，包括定位误差降低1.1到2.8倍，模型大小缩小3.4到43.3倍，以及延迟降低1.5到3.6倍，优于先前的深度学习模型。", "conclusion": "LogNet通过提供可解释性，不仅帮助理解深度学习模型在室内定位中的行为和故障原因，还显著提高了其在长期部署中的性能和稳定性。", "translation": "使用深度学习（DL）的室内定位在将Wi-Fi RSS指纹映射到物理位置方面表现出强大的准确性；然而，大多数现有DL框架作为黑箱模型运行，对如何进行预测或模型如何随时间响应真实世界噪声提供有限的洞察。这种缺乏可解释性阻碍了我们理解由环境动态引起的时间变化影响的能力，以及调整模型以实现长期可靠性的能力。为了解决这个问题，我们引入了LogNet，一个新颖的基于逻辑门的框架，旨在解释和增强基于DL的室内定位。LogNet通过识别哪些接入点（AP）对每个参考点（RP）最具影响力，并揭示环境噪声如何扰乱DL驱动的定位决策，从而实现透明推理。这种可解释性使我们能够追踪和诊断模型故障，并调整DL系统以实现更稳定的长期部署。在多个真实世界建筑平面图和两年以上的时间变化中进行的评估表明，LogNet不仅能够解释DL模型的内部行为，而且还提高了性能——与先前的基于DL的模型相比，定位误差降低了1.1到2.8倍，模型大小缩小了3.4到43.3倍，延迟降低了1.5到3.6倍。", "summary": "这篇论文介绍了LogNet，一个基于逻辑门的新框架，旨在解决深度学习在室内定位中作为黑箱模型缺乏可解释性的问题。LogNet通过透明地揭示哪些Wi-Fi接入点对定位最关键以及环境噪声如何影响决策，从而增强了模型的理解和诊断能力。实验结果表明，LogNet不仅提供了对深度学习模型内部行为的解释，还在定位误差、模型大小和延迟方面显著优于现有深度学习方法，提升了长期部署的稳定性和性能。", "keywords": "室内定位, 深度学习, 可解释性, Wi-Fi指纹, 逻辑门", "comments": "这项研究通过引入LogNet框架，创新性地将逻辑门应用于解释深度学习在Wi-Fi指纹室内定位中的学习过程，有效解决了DL模型黑箱问题。其重要性在于，不仅提升了模型的可解释性，使得故障诊断和模型适应性增强，还在性能上取得了显著提升，为长期可靠的室内定位系统提供了新的方向。"}}
{"id": "2506.15566", "title": "Task-Agnostic Experts Composition for Continual Learning", "authors": ["Luigi Quarantiello", "Andrea Cossu", "Vincenzo Lomonaco"], "summary": "Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15566v1", "AI": {"title_translation": "面向持续学习的任务无关专家组合", "tldr": "本文提出了一种专家组合方法，通过零样本集成专家模型，以实现更高效和可持续的持续学习，并在挑战性基准测试中表现出更高的准确性和更低的计算资源需求。", "motivation": "人类推理过程中的组合性是将复杂问题分解为简单元素的能力，这对神经网络也至关重要，尤其是在追求更高效和可持续的AI框架时。", "method": "本文提出了一种组合方法，通过零样本集成一组专家模型，并使用一个旨在测试组合性能力的挑战性基准来评估该方法。", "result": "本文的专家组合方法在准确性上远超基线算法，同时所需的计算资源更少，因此效率更高。", "conclusion": "专家组合方法能够有效提升持续学习的效率和准确性。", "translation": "组合性是人类推理过程的基本能力之一，它允许将复杂问题分解为更简单的元素。这种特性对于神经网络也至关重要，尤其是在追求更高效和可持续的AI框架时。我们提出了一种组合方法，通过零样本集成一组专家模型，并使用一个旨在测试组合性能力的挑战性基准来评估我们的方法。我们展示了我们的专家组合方法能够比基线算法实现更高的准确性，同时所需的计算资源更少，因此效率更高。", "summary": "本文提出了一种名为“专家组合”的新型组合方法，用于持续学习。该方法通过零样本集成多组专家模型，旨在模仿人类的组合推理能力，以提高神经网络的效率和可持续性。在专门设计的挑战性基准测试中，该方法表现出比现有基线算法更高的准确性，同时显著降低了计算资源消耗。", "keywords": "持续学习, 专家组合, 零样本学习, 神经网络, 组合性", "comments": "这项工作提出了一种新颖的专家组合方法，用于解决持续学习中的效率和可持续性问题。其创新点在于采用零样本集成专家模型，并在专门设计的组合性基准上进行评估。该方法在提高准确性的同时降低了计算成本，对推动更高效的AI框架具有重要意义。"}}
{"id": "2506.15588", "title": "Memory-Efficient Differentially Private Training with Gradient Random Projection", "authors": ["Alex Mulrooney", "Devansh Gupta", "James Flemings", "Huanyu Zhang", "Murali Annavaram", "Meisam Razaviyayn", "Xinwei Zhang"], "summary": "Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15588v1", "AI": {"title_translation": "基于梯度随机投影的内存高效差分隐私训练", "tldr": "DP-GRAPE通过梯度随机投影显著降低了差分隐私神经网络训练的内存开销，同时保持了与现有方法相当的效用。", "motivation": "标准差分隐私（DP）训练方法（如DP-Adam）由于逐样本梯度裁剪而导致内存开销高，限制了可扩展性，尤其是在训练大型模型时。", "method": "本文提出了DP-GRAPE（Gradient RAndom ProjEction），一种差分隐私训练方法。其核心在于引入了三个关键修改：(1) 在梯度投影后进行隐私化处理；(2) 使用随机高斯矩阵取代基于SVD的子空间；(3) 在反向传播期间应用投影。这些改进消除了昂贵的SVD计算需求，实现了显著的内存节省，并提高了效用。", "result": "DP-GRAPE在理论上实现了与DP-SGD相当的隐私-效用权衡。在实证实验中，与DP-Adam相比，DP-GRAPE在预训练Vision Transformers时内存使用量减少了63%以上，在微调RoBERTa-Large时减少了70%以上，同时保持了相似的准确性和训练时间。此外，DP-GRAPE被证明可以扩展到微调大型模型，如具有多达67亿参数的OPT。", "conclusion": "DP-GRAPE提供了一种内存高效的差分隐私训练解决方案，能够在大幅降低内存占用的同时，保持与现有方法相当的效用，并可扩展到大型模型。", "translation": "差分隐私（DP）在神经网络训练期间保护敏感数据，但DP-Adam等标准方法由于逐样本梯度裁剪而导致内存开销高，限制了可扩展性。我们引入了DP-GRAPE（Gradient RAndom ProjEction），这是一种差分隐私训练方法，它显著降低了内存使用量，同时保持了与一阶DP方法相当的效用。DP-GRAPE并非直接将DP应用于GaLore，而是引入了三个关键修改：（1）梯度在投影后进行隐私化处理；（2）随机高斯矩阵取代了基于SVD的子空间；（3）投影在反向传播期间应用。这些贡献消除了对昂贵的SVD计算的需求，实现了显著的内存节省，并提高了效用。尽管在低维子空间中操作，我们的理论分析表明DP-GRAPE实现了与DP-SGD相当的隐私-效用权衡。我们广泛的实证实验表明，DP-GRAPE可以在不牺牲准确性或训练时间的情况下减少DP训练的内存占用。特别是，与DP-Adam相比，DP-GRAPE在预训练Vision Transformers时内存使用量减少了63%以上，在微调RoBERTa-Large时减少了70%以上，同时实现了相似的性能。我们进一步证明，DP-GRAPE可以扩展到微调大型模型，例如具有多达67亿参数的OPT。", "summary": "本文介绍了DP-GRAPE，一种用于差分隐私神经网络训练的新型内存高效方法。通过应用梯度随机投影和特定的修改（在投影后进行梯度隐私化、使用随机高斯矩阵以及在反向传播期间进行投影），DP-GRAPE与DP-Adam等方法相比显著降低了内存开销，同时保持了可比的效用和性能。经验证明，它实现了显著的内存节省（例如，Vision Transformers预训练超过63%，RoBERTa-Large微调超过70%），并且可以扩展到非常大的模型。", "keywords": "差分隐私, 内存高效, 梯度投影, 神经网络训练, 可扩展性", "comments": "DP-GRAPE通过解决内存瓶颈，在实践中推动了差分隐私深度学习的发展，使得DP能够应用于以前难以处理的更大规模模型。其使用随机投影而非SVD的策略是提高效率的巧妙方法。"}}
{"id": "2506.15620", "title": "GFLC: Graph-based Fairness-aware Label Correction for Fair Classification", "authors": ["Modar Sulaiman", "Kallol Roy"], "summary": "Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.", "comment": "25 pages, 6 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15620v1", "AI": {"title_translation": "GFLC：基于图的公平性感知标签校正，用于公平分类", "tldr": "GFLC是一种基于图的标签校正方法，旨在解决训练数据中的标签偏差，以实现更公平的机器学习分类。", "motivation": "机器学习系统日益影响社会各个方面，公平性至关重要。然而，用于训练去偏差技术的数据通常包含有偏差和噪声的标签，这会影响模型性能并错误地代表分类器的公平性。", "method": "本文提出了GFLC（基于图的公平性感知标签校正），这是一种在校正标签噪声的同时保持数据集中人口统计学平等的有效方法。它结合了三个关键组件：预测置信度测量、通过Ricci流优化图拉普拉斯的基于图的正则化，以及明确的人口统计学平等激励。", "result": "实验结果表明该方法有效，并且与基线相比，在性能和公平性指标之间的权衡方面有显著改善。", "conclusion": "GFLC能够有效地校正标签噪声，同时保持数据集中的人口统计学平等，从而提高机器学习模型的公平性和性能。", "translation": "机器学习（ML）中的公平性对于构建值得信赖的机器学习系统至关重要，因为人工智能（AI）系统日益影响社会的各个方面，包括医疗保健决策和法律判决。此外，大量研究表明ML中存在不公平结果的证据，并且需要更稳健的公平性感知方法。然而，我们用于训练和开发去偏差技术的数据通常包含有偏差和噪声的标签。因此，训练数据中的标签偏差会影响模型性能，并在测试期间错误地代表分类器的公平性。为了解决这个问题，我们的论文提出了基于图的公平性感知标签校正（GFLC），这是一种在校正标签噪声的同时保持数据集中人口统计学平等的有效方法。具体而言，我们的方法结合了三个关键组件：预测置信度测量、通过Ricci流优化图拉普拉斯的基于图的正则化，以及明确的人口统计学平等激励。我们的实验结果表明了我们提出方法的有效性，并显示与基线相比，在性能和公平性指标之间的权衡方面有显著改善。", "summary": "本文提出了GFLC（基于图的公平性感知标签校正）方法，旨在解决机器学习训练数据中普遍存在的标签偏差和噪声问题，这些问题会损害模型性能和公平性。GFLC通过结合预测置信度、基于Ricci流优化图拉普拉斯的图正则化以及明确的人口统计学平等激励来校正标签噪声，同时确保人口统计学平等。实验结果表明，GFLC在性能与公平性之间的权衡方面取得了显著改进。", "keywords": "公平性, 标签校正, 图学习, 机器学习, 偏差减轻", "comments": "GFLC的创新之处在于其结合了图正则化（特别是通过Ricci流优化的图拉普拉斯）和明确的公平性激励来解决标签噪声和偏差问题，这为构建更值得信赖的AI系统提供了新的视角。该方法的重要性在于它直接处理了训练数据质量对模型公平性的影响，是实现公平机器学习的关键一步。"}}
{"id": "2506.15654", "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization", "authors": ["Ranting Hu"], "summary": "Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.", "comment": "23 pages, 14 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15654v1", "AI": {"title_translation": "CAWR：抵抗腐败的优势加权回归用于鲁棒策略优化", "tldr": "本文提出了CAWR，通过鲁棒损失函数和经验回放来解决离线强化学习中优势加权回归（AWRs）因数据腐败导致策略过于保守的问题。", "motivation": "离线强化学习（offline RL）算法常因数据分布偏移（如次优离线数据中的不良探索）导致策略估计偏差。优势加权回归（AWRs）系列算法因此可能学习到过于保守的策略。", "method": "提出了抗腐败优势加权回归（CAWR），通过在策略优化中引入一组鲁棒损失函数，并采用基于优势的优先经验回放方法来过滤掉不良探索。", "result": "在D4RL基准测试上的数值实验表明，CAWR方法可以从次优离线数据中学习到更优的策略，显著提升了策略优化的性能。", "conclusion": "本文证明了AWRs的过度保守性源于损失函数对不良探索的敏感性，并提出了CAWR来有效解决这一问题，从而学习到更优的离线策略。", "translation": "离线强化学习（offline RL）算法通常需要额外的约束或惩罚项来解决分布偏移问题，例如在策略优化期间添加隐式或显式策略约束以减少函数估计偏差。本文关注优势加权回归（AWRs）系列的一个局限性，即由于数据腐败（特别是次优离线数据中的不良探索）可能导致学习到过于保守的策略。我们从两个角度研究了这个问题：（1）不良探索如何影响基于KL散度的理论最优策略，以及（2）此类不良探索如何影响理论最优策略的近似。我们证明，这种过度保守性主要由策略优化损失函数对不良探索的敏感性以及离线数据集中不良探索的比例引起。为了解决这个问题，我们提出了抗腐败优势加权回归（CAWR），它在策略优化中结合了一组鲁棒损失函数和一种基于优势的优先经验回放方法来过滤掉不良探索。D4RL基准测试的数值实验表明，我们的方法可以从次优离线数据中学习到更优的策略，显著提升了策略优化的性能。", "summary": "本文研究了离线强化学习中优势加权回归（AWRs）在次优离线数据下因不良探索导致策略过于保守的问题。作者从理论上分析了不良探索对最优策略及其近似的影响，并证明了损失函数敏感性和不良探索比例是主要原因。为解决此问题，提出了CAWR，通过鲁棒损失函数和基于优势的优先经验回放来过滤不良探索。实验结果显示CAWR能在次优数据上学习到更优策略，显著提升性能。", "keywords": "离线强化学习, 优势加权回归, 数据腐败, 策略优化, 鲁棒性", "comments": "本文创新性地提出CAWR框架，通过结合鲁棒损失函数和基于优势的优先经验回放，有效解决了离线强化学习中AWRs因数据腐败导致的策略保守性问题，对提升离线RL的实际应用价值具有重要意义。"}}
{"id": "2506.14772", "title": "SimBank: from Simulation to Solution in Prescriptive Process Monitoring", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "summary": "Prescriptive Process Monitoring (PresPM) is an emerging area within Process\nMining, focused on optimizing processes through real-time interventions for\neffective decision-making. PresPM holds significant promise for organizations\nseeking enhanced operational performance. However, the current literature faces\ntwo key limitations: a lack of extensive comparisons between techniques and\ninsufficient evaluation approaches. To address these gaps, we introduce\nSimBank: a simulator designed for accurate benchmarking of PresPM methods.\nModeled after a bank's loan application process, SimBank enables extensive\ncomparisons of both online and offline PresPM methods. It incorporates a\nvariety of intervention optimization problems with differing levels of\ncomplexity and supports experiments on key causal machine learning challenges,\nsuch as assessing a method's robustness to confounding in data. SimBank\nadditionally offers a comprehensive evaluation capability: for each test case,\nit can generate the true outcome under each intervention action, which is not\npossible using recorded datasets. The simulator incorporates parallel\nactivities and loops, drawing from common logs to generate cases that closely\nresemble real-life process instances. Our proof of concept demonstrates\nSimBank's benchmarking capabilities through experiments with various PresPM\nmethods across different interventions, highlighting its value as a publicly\navailable simulator for advancing research and practice in PresPM.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.14772v1", "AI": {"title_translation": "SimBank：从模拟到规范性流程监控中的解决方案", "tldr": "SimBank是一个用于规范性流程监控（PresPM）方法基准测试的模拟器，旨在解决现有研究中缺乏广泛比较和评估方法不足的问题，通过模拟银行贷款申请流程来支持在线和离线PresPM方法的广泛比较和评估。", "motivation": "现有规范性流程监控（PresPM）研究面临两个主要局限性：缺乏技术间的广泛比较和评估方法不足。为了解决这些差距，本研究引入SimBank。", "method": "本研究引入了SimBank，一个专为精确基准测试PresPM方法而设计的模拟器。它以银行贷款申请流程为模型，支持对在线和离线PresPM方法进行广泛比较。SimBank包含不同复杂度的干预优化问题，并支持因果机器学习挑战的实验。它能为每个测试用例生成每种干预行动下的真实结果，并包含并行活动和循环，生成接近真实流程实例的案例。", "result": "概念验证通过对不同干预措施下各种PresPM方法进行实验，展示了SimBank的基准测试能力，突出了其作为公共可用模拟器在推进PresPM研究和实践中的价值。", "conclusion": "SimBank是一个有价值的公开可用模拟器，能够促进规范性流程监控（PresPM）领域的研究和实践，通过提供一个精确的基准测试环境来解决现有文献中的评估和比较不足问题。", "translation": "规范性流程监控（PresPM）是流程挖掘中一个新兴领域，专注于通过实时干预来优化流程，以实现有效的决策制定。PresPM对寻求提高运营绩效的组织具有重要前景。然而，当前文献面临两个关键局限性：技术之间缺乏广泛比较和评估方法不足。为了解决这些差距，我们引入了SimBank：一个旨在精确基准测试PresPM方法的模拟器。SimBank以银行贷款申请流程为模型，能够对在线和离线PresPM方法进行广泛比较。它结合了各种复杂程度不同的干预优化问题，并支持关于关键因果机器学习挑战的实验，例如评估方法对数据中混杂因素的鲁棒性。SimBank还提供全面的评估能力：对于每个测试用例，它可以在每种干预行动下生成真实结果，这是使用记录数据集无法实现的。该模拟器结合了并行活动和循环，从常见日志中提取数据以生成与真实流程实例高度相似的案例。我们的概念验证通过对不同干预措施下的各种PresPM方法进行实验，展示了SimBank的基准测试能力，突出了其作为公共可用模拟器在推进PresPM研究和实践中的价值。", "summary": "本文介绍了SimBank，一个专门为规范性流程监控（PresPM）方法设计的模拟器，旨在解决当前PresPM研究中存在的评估和比较不足问题。SimBank以银行贷款申请流程为模型，支持对在线和离线PresPM方法进行广泛、精确的基准测试，并能生成干预下的真实结果，克服了现有数据集的局限性。它还支持因果机器学习挑战的实验，并通过概念验证展示了其作为推动PresPM研究和实践的有效工具的价值。", "keywords": "规范性流程监控, 流程挖掘, 模拟器, 基准测试, 因果机器学习", "comments": "SimBank的创新之处在于它提供了一个可控且可复现的环境来评估和比较不同的PresPM方法，尤其是在能够生成真实干预结果这一点上，弥补了真实数据集的不足。这对于推进PresPM领域的研究和实践具有重要意义，因为它为方法论的严谨测试和比较提供了基础。其价值在于解决了一个关键的限制——缺乏标准化和全面的评估平台。"}}
{"id": "2506.14850", "title": "Beyond Force Metrics: Pre-Training MLFFs for Stable MD Simulations", "authors": ["Shagun Maheshwari", "Janghoon Ock", "Adeesh Kolluru", "Amir Barati Farimani", "John R. Kitchin"], "summary": "Machine-learning force fields (MLFFs) have emerged as a promising solution\nfor speeding up ab initio molecular dynamics (MD) simulations, where accurate\nforce predictions are critical but often computationally expensive. In this\nwork, we employ GemNet-T, a graph neural network model, as an MLFF and\ninvestigate two training strategies: (1) direct training on MD17 (10K samples)\nwithout pre-training, and (2) pre-training on the large-scale OC20 dataset\nfollowed by fine-tuning on MD17 (10K). While both approaches achieve low force\nmean absolute errors (MAEs), reaching 5 meV/A per atom, we find that lower\nforce errors do not necessarily guarantee stable MD simulations. Notably, the\npre-trained GemNet-T model yields significantly improved simulation stability,\nsustaining trajectories up to three times longer than the model trained from\nscratch. These findings underscore the value of pre-training on large, diverse\ndatasets to capture complex molecular interactions and highlight that force MAE\nalone is not always a sufficient metric of MD simulation stability.", "comment": null, "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2506.14850v1", "AI": {"title_translation": "超越力学指标：预训练MLFFs实现稳定MD模拟", "tldr": "预训练机器学习力场（MLFFs）在大型数据集上能显著提高分子动力学（MD）模拟的稳定性，即使力误差相似。", "motivation": "加速从头算分子动力学（MD）模拟，因为准确的力预测虽然关键但计算成本高昂。", "method": "使用图神经网络模型GemNet-T作为MLFF，并比较两种训练策略：1) 直接在MD17数据集上训练；2) 在OC20大型数据集上预训练，然后在MD17上微调。", "result": "两种方法都达到了较低的力平均绝对误差（5 meV/A），但预训练的GemNet-T模型显著提高了模拟稳定性，轨迹持续时间可达从头训练模型的三倍。发现较低的力误差不一定保证稳定的MD模拟。", "conclusion": "在大型、多样化数据集上进行预训练对于捕获复杂分子相互作用非常重要，并且力平均绝对误差（MAE）本身并非衡量MD模拟稳定性的充分指标。", "translation": "机器学习力场（MLFFs）已成为加速从头算分子动力学（MD）模拟的一种有前景的解决方案，其中准确的力预测至关重要但通常计算成本高昂。在这项工作中，我们采用图神经网络模型GemNet-T作为MLFF，并研究了两种训练策略：(1) 在MD17（10K样本）上直接训练，不进行预训练；(2) 在大规模OC20数据集上预训练，然后在MD17（10K）上进行微调。虽然这两种方法都实现了较低的力平均绝对误差（MAE），达到每原子5 meV/A，但我们发现较低的力误差不一定能保证稳定的MD模拟。值得注意的是，预训练的GemNet-T模型显著提高了模拟稳定性，其轨迹持续时间比从头训练的模型长达三倍。这些发现强调了在大型、多样化数据集上进行预训练以捕获复杂分子相互作用的价值，并突出表明力MAE本身并非总是MD模拟稳定性的充分指标。", "summary": "本文研究了两种机器学习力场（MLFF）训练策略对分子动力学（MD）模拟稳定性的影响。作者使用GemNet-T模型，比较了直接在MD17数据集上训练和在大规模OC20数据集上预训练后在MD17上微调的效果。尽管两种方法在力预测上都达到了低误差，但预训练模型显著提高了MD模拟的稳定性，轨迹持续时间更长。研究强调了预训练在大规模数据集上的重要性，并指出力平均绝对误差并非衡量MD模拟稳定性的唯一标准。", "keywords": "机器学习力场, 分子动力学, 预训练, GemNet-T, 模拟稳定性", "comments": "这项工作创新性地指出，对于机器学习力场，仅仅追求低的力预测误差不足以保证分子动力学模拟的稳定性，预训练在大型多样化数据集上对于捕获复杂分子相互作用和提高模拟稳定性至关重要。这为MLFF的开发和评估提供了新的视角和更实际的指导。"}}
{"id": "2506.14853", "title": "DisProtEdit: Exploring Disentangled Representations for Multi-Attribute Protein Editing", "authors": ["Max Ku", "Sun Sun", "Hongyu Guo", "Wenhu Chen"], "summary": "We introduce DisProtEdit, a controllable protein editing framework that\nleverages dual-channel natural language supervision to learn disentangled\nrepresentations of structural and functional properties. Unlike prior\napproaches that rely on joint holistic embeddings, DisProtEdit explicitly\nseparates semantic factors, enabling modular and interpretable control. To\nsupport this, we construct SwissProtDis, a large-scale multimodal dataset where\neach protein sequence is paired with two textual descriptions, one for\nstructure and one for function, automatically decomposed using a large language\nmodel. DisProtEdit aligns protein and text embeddings using alignment and\nuniformity objectives, while a disentanglement loss promotes independence\nbetween structural and functional semantics. At inference time, protein editing\nis performed by modifying one or both text inputs and decoding from the updated\nlatent representation. Experiments on protein editing and representation\nlearning benchmarks demonstrate that DisProtEdit performs competitively with\nexisting methods while providing improved interpretability and controllability.\nOn a newly constructed multi-attribute editing benchmark, the model achieves a\nboth-hit success rate of up to 61.7%, highlighting its effectiveness in\ncoordinating simultaneous structural and functional edits.", "comment": "Accepted to ICMLW (GenBio) 2025 and ICMLW (FM4LS) 2025", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.14853v1", "AI": {"title_translation": "DisProtEdit：探索多属性蛋白质编辑的解耦表示", "tldr": "引入DisProtEdit，一个利用双通道自然语言监督学习解耦表示的蛋白质编辑框架，实现了模块化、可解释的蛋白质编辑，并构建了新的数据集。", "motivation": "现有方法依赖联合整体嵌入，导致蛋白质编辑缺乏模块化和可解释的控制。本研究旨在通过学习结构和功能属性的解耦表示来解决这一问题，从而提高蛋白质编辑的可解释性和可控性。", "method": "本文提出了DisProtEdit框架，该框架利用双通道自然语言监督来学习蛋白质结构和功能属性的解耦表示。为支持此框架，构建了大规模多模态数据集SwissProtDis，其中蛋白质序列与由大型语言模型自动分解的结构和功能文本描述配对。DisProtEdit通过对齐和一致性目标对齐蛋白质和文本嵌入，并引入解耦损失以促进结构和功能语义的独立性。在推理阶段，通过修改文本输入并从更新的潜在表示中解码来执行蛋白质编辑。", "result": "DisProtEdit在蛋白质编辑和表示学习基准测试中表现出与现有方法相当的竞争力，同时显著提高了可解释性和可控性。在一个新建的多属性编辑基准测试中，该模型在协调同时进行的结构和功能编辑方面，取得了高达61.7%的双命中成功率。", "conclusion": "DisProtEdit是一个有效的蛋白质编辑框架，通过学习解耦表示，实现了对蛋白质多属性编辑的更可解释和可控的控制，并在同时修改多个属性方面表现出色。", "translation": "我们引入了 DisProtEdit，一个可控的蛋白质编辑框架，它利用双通道自然语言监督来学习结构和功能属性的解耦表示。与依赖联合整体嵌入的现有方法不同，DisProtEdit 明确分离了语义因子，从而实现了模块化和可解释的控制。为了支持这一点，我们构建了 SwissProtDis，一个大规模多模态数据集，其中每个蛋白质序列都与两个文本描述配对，一个用于结构，一个用于功能，这些描述是使用大型语言模型自动分解的。DisProtEdit 使用对齐和一致性目标对齐蛋白质和文本嵌入，同时解耦损失促进了结构和功能语义之间的独立性。在推理时，通过修改一个或两个文本输入并从更新的潜在表示中解码来执行蛋白质编辑。在蛋白质编辑和表示学习基准上的实验表明，DisProtEdit 与现有方法相比具有竞争力，同时提供了改进的可解释性和可控性。在一个新构建的多属性编辑基准上，该模型实现了高达 61.7% 的双命中成功率，突出了其在协调同时结构和功能编辑方面的有效性。", "summary": "DisProtEdit是一个新颖的可控蛋白质编辑框架，它通过双通道自然语言监督学习蛋白质结构和功能的解耦表示，克服了传统方法的局限性。该框架通过引入解耦损失和构建大规模多模态数据集SwissProtDis，实现了对蛋白质属性的模块化和可解释控制。实验证明，DisProtEdit在蛋白质编辑和表示学习任务上表现出色，尤其在同时进行结构和功能编辑时，展现出高成功率和优越的控制能力。", "keywords": "蛋白质编辑, 解耦表示, 自然语言监督, 多属性控制, 深度学习", "comments": "创新点：1. 引入“双通道自然语言监督”来学习解耦表示，这与以往的整体嵌入方法不同，显著提升了蛋白质编辑的可控性和可解释性。2. 构建了大规模多模态数据集SwissProtDis，为蛋白质序列与结构/功能文本描述的配对提供了独特且重要的支持，这对于训练精确的解耦模型至关重要。3. 设计了独特的“解耦损失”来确保结构和功能语义的独立性，这是实现对蛋白质属性精确控制的关键技术。重要性：1. 极大地提高了蛋白质编辑的“可解释性”和“可控性”，这对于蛋白质工程、药物设计以及合成生物学等领域具有深远意义，允许研究人员以更精细和可预测的方式修改蛋白质特性。2. 在多属性编辑方面取得了显著的“双命中成功率”，证明了该框架在处理复杂、多目标蛋白质编辑任务中的实用性和高效性。"}}
{"id": "2506.14858", "title": "CutReg: A loss regularizer for enhancing the scalability of QML via adaptive circuit cutting", "authors": ["Maniraman Periyasamy", "Christian Ufrecht", "Daniel D. Scherer", "Wolfgang Mauerer"], "summary": "Whether QML can offer a transformative advantage remains an open question.\nThe severe constraints of NISQ hardware, particularly in circuit depth and\nconnectivity, hinder both the validation of quantum advantage and the empirical\ninvestigation of major obstacles like barren plateaus. Circuit cutting\ntechniques have emerged as a strategy to execute larger quantum circuits on\nsmaller, less connected hardware by dividing them into subcircuits. However,\nthis partitioning increases the number of samples needed to estimate the\nexpectation value accurately through classical post-processing compared to\nestimating it directly from the full circuit. This work introduces a novel\nregularization term into the QML optimization process, directly penalizing the\noverhead associated with sampling. We demonstrate that this approach enables\nthe optimizer to balance the advantages of gate cutting against the\noptimization of the typical ML cost function. Specifically, it navigates the\ntrade-off between minimizing the cutting overhead and maintaining the overall\naccuracy of the QML model, paving the way to study larger complex problems in\npursuit of quantum advantage.", "comment": "This work has been submitted to the QML@QCE workshop for possible\n  publication", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14858v1", "AI": {"title_translation": "CutReg：一种通过自适应电路切割增强QML可扩展性的损失正则化器", "tldr": "本文引入了CutReg，一种新的损失正则化项，用于量化机器学习（QML）优化过程，以平衡电路切割带来的采样开销与模型准确性，从而在受限的NISQ硬件上实现更大规模的量子电路执行。", "motivation": "NISQ硬件的严重限制（电路深度和连接性）阻碍了量子优势的验证和对诸如贫瘠高原等主要障碍的实证研究。尽管电路切割技术可以使更大的量子电路在较小的硬件上运行，但它会增加通过经典后处理准确估计期望值所需的样本数量，从而带来开销。", "method": "本文引入了一个新颖的正则化项，即CutReg，到QML优化过程中，直接惩罚与采样相关的开销。这使得优化器能够平衡门切割的优势与典型机器学习成本函数的优化。", "result": "该方法使优化器能够在最小化切割开销和保持QML模型整体准确性之间进行权衡。它证明了在追求量子优势的过程中研究更大、更复杂问题的可行性。", "conclusion": "通过引入CutReg正则化项，该工作为在NISQ硬件上研究更大、更复杂的QML问题铺平了道路，通过平衡电路切割的开销和模型准确性，克服了现有硬件限制。", "translation": "量子机器学习（QML）是否能提供变革性优势仍是一个悬而未决的问题。NISQ硬件的严重限制，特别是在电路深度和连接性方面，阻碍了量子优势的验证以及对贫瘠高原等主要障碍的实证研究。电路切割技术已成为一种策略，通过将较大的量子电路划分为子电路，使其能在较小、连接性较差的硬件上执行。然而，与直接从完整电路估计相比，这种划分增加了通过经典后处理准确估计期望值所需的样本数量。这项工作在QML优化过程中引入了一个新颖的正则化项，直接惩罚与采样相关的开销。我们证明了这种方法使优化器能够平衡门切割的优势与典型机器学习成本函数的优化。具体来说，它在最小化切割开销和保持QML模型整体准确性之间进行权衡，为研究更大、更复杂的问题以追求量子优势铺平了道路。", "summary": "本文提出了CutReg，一种用于量子机器学习（QML）优化过程的新型损失正则化项。它旨在解决在受限的NISQ硬件上执行大型量子电路时，电路切割技术引入的额外采样开销问题。通过直接惩罚采样开销，CutReg使优化器能够权衡电路切割的优势与QML模型的整体准确性，从而在保持性能的同时，实现更大规模的量子电路在小型硬件上的运行，为探索量子优势提供途径。", "keywords": "QML, 电路切割, 正则化, NISQ, 量子优势", "comments": "这项工作提出了一种新颖且实用的方法来解决NISQ设备中量子机器学习的可扩展性问题。通过将采样开销作为正则化项纳入优化过程，它为在现有硬件限制下探索更复杂的量子问题提供了一条有前途的途径。其创新之处在于将硬件限制直接编码到损失函数中，从而实现更有效的资源利用和模型训练。"}}
{"id": "2506.14899", "title": "Optimal Convergence Rates of Deep Neural Network Classifiers", "authors": ["Zihan Zhang", "Lei Shi", "Ding-Xuan Zhou"], "summary": "In this paper, we study the binary classification problem on $[0,1]^d$ under\nthe Tsybakov noise condition (with exponent $s \\in [0,\\infty]$) and the\ncompositional assumption. This assumption requires the conditional class\nprobability function of the data distribution to be the composition of $q+1$\nvector-valued multivariate functions, where each component function is either a\nmaximum value function or a H\\\"{o}lder-$\\beta$ smooth function that depends\nonly on $d_*$ of its input variables. Notably, $d_*$ can be significantly\nsmaller than the input dimension $d$. We prove that, under these conditions,\nthe optimal convergence rate for the excess 0-1 risk of classifiers is $$\n\\left( \\frac{1}{n}\n\\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}\\;\\;\\;,\n$$ which is independent of the input dimension $d$. Additionally, we\ndemonstrate that ReLU deep neural networks (DNNs) trained with hinge loss can\nachieve this optimal convergence rate up to a logarithmic factor. This result\nprovides theoretical justification for the excellent performance of ReLU DNNs\nin practical classification tasks, particularly in high-dimensional settings.\nThe technique used to establish these results extends the oracle inequality\npresented in our previous work. The generalized approach is of independent\ninterest.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14899v1", "AI": {"title_translation": "深度神经网络分类器的最优收敛率", "tldr": "本文研究了在Tsybakov噪声和组合假设下，深度神经网络分类器在0-1风险下的最优收敛率，并证明ReLU深度神经网络可以达到此最优率，且与输入维度无关。", "motivation": "现有研究缺乏对ReLU深度神经网络在实际分类任务中（尤其是在高维设置下）表现优异的理论解释。", "method": "在Tsybakov噪声条件和组合假设下，通过扩展先前的oracle不等式技术，证明了分类器0-1风险的最优收敛率，并展示ReLU深度神经网络可以达到此率。", "result": "在Tsybakov噪声条件和组合假设下，分类器0-1风险的最优收敛率为 $ \\left( \\frac{1}{n} \\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}} $，且与输入维度 $d$ 无关。ReLU深度神经网络在使用hinge损失训练时，可以达到这个最优收敛率（除了一个对数因子）。", "conclusion": "本研究为ReLU深度神经网络在实际分类任务中，特别是在高维设置下的优异性能提供了理论依据。", "translation": "在这篇论文中，我们研究了在Tsybakov噪声条件（指数 $s \\in [0,\\infty]$）和组合假设下，在$[0,1]^d$上的二分类问题。这个假设要求数据分布的条件类别概率函数是 $q+1$ 个向量值多元函数的组合，其中每个分量函数要么是最大值函数，要么是H\"{o}lder-$\\beta$光滑函数，并且仅依赖于其 $d_*$ 个输入变量。值得注意的是，$d_*$ 可以显著小于输入维度 $d$。我们证明，在这些条件下，分类器过量0-1风险的最优收敛率为 $$ \\left( \\frac{1}{n} \\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}\\;\\;\\;, $$ 这个收敛率与输入维度 $d$ 无关。此外，我们证明了用hinge损失训练的ReLU深度神经网络（DNNs）可以达到这个最优收敛率，误差在一个对数因子内。这个结果为ReLU深度神经网络在实际分类任务中，特别是在高维设置下的出色性能提供了理论依据。用于建立这些结果的技术扩展了我们之前工作中提出的oracle不等式。这种广义方法本身也具有独立的意义。", "summary": "本文研究了在Tsybakov噪声和组合假设下，二分类问题中深度神经网络分类器的最优收敛率。作者证明了在特定条件下，分类器过量0-1风险的最优收敛率与输入维度无关，并进一步展示了ReLU深度神经网络在使用hinge损失训练时可以达到此最优收敛率。这项工作为ReLU深度神经网络在高维分类任务中的优越表现提供了理论支持。", "keywords": "深度神经网络, 收敛率, 分类器, Tsybakov噪声, 0-1风险", "comments": "这篇论文通过严谨的数学证明，为ReLU深度神经网络在高维数据分类任务中的出色性能提供了坚实的理论基础。其创新点在于引入了组合假设和对oracle不等式的泛化，揭示了DNNs在特定条件下可以达到与输入维度无关的最优收敛率，这对于理解深度学习的有效性具有重要意义。"}}
{"id": "2506.14920", "title": "Q2SAR: A Quantum Multiple Kernel Learning Approach for Drug Discovery", "authors": ["Alejandro Giraldo", "Daniel Ruiz", "Mariano Caruso", "Javier Mancilla", "Guido Bellomo"], "summary": "Quantitative Structure-Activity Relationship (QSAR) modeling is a cornerstone\nof computational drug discovery. This research demonstrates the successful\napplication of a Quantum Multiple Kernel Learning (QMKL) framework to enhance\nQSAR classification, showing a notable performance improvement over classical\nmethods. We apply this methodology to a dataset for identifying DYRK1A kinase\ninhibitors. The workflow involves converting SMILES representations into\nnumerical molecular descriptors, reducing dimensionality via Principal\nComponent Analysis (PCA), and employing a Support Vector Machine (SVM) trained\non an optimized combination of multiple quantum and classical kernels. By\nbenchmarking the QMKL-SVM against a classical Gradient Boosting model, we show\nthat the quantum-enhanced approach achieves a superior AUC score, highlighting\nits potential to provide a quantum advantage in challenging cheminformatics\nclassification tasks.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14920v1", "AI": {"title_translation": "Q2SAR：一种用于药物发现的量子多核学习方法", "tldr": "量子多核学习（QMKL）框架成功应用于增强药物发现中的定量构效关系（QSAR）分类，与经典方法相比表现出显著的性能提升。", "motivation": "旨在通过引入量子多核学习（QMKL）框架来增强定量构效关系（QSAR）分类的性能，以克服传统方法的局限性，并为药物发现提供更有效的计算工具。", "method": "研究提出了一种量子多核学习（QMKL）框架，并将其应用于QSAR分类。具体工作流程包括将SMILES表示转换为数值分子描述符，通过主成分分析（PCA）进行降维，以及采用在多个量子和经典核函数优化组合上训练的支持向量机（SVM）。该方法（QMKL-SVM）随后与经典的梯度提升模型进行了基准测试。", "result": "量子增强的QMKL-SVM方法在识别DYRK1A激酶抑制剂的数据集上，与经典梯度提升模型相比，实现了更高的AUC分数，显示出显著的性能提升。", "conclusion": "量子多核学习（QMKL）方法在具有挑战性的化学信息学分类任务中具有提供量子优势的潜力，能够显著提高QSAR分类的性能。", "translation": "定量构效关系（QSAR）建模是计算药物发现的基石。本研究展示了量子多核学习（QMKL）框架在增强QSAR分类方面的成功应用，并显示出相对于经典方法的显著性能提升。我们将此方法应用于一个用于识别DYRK1A激酶抑制剂的数据集。工作流程包括将SMILES表示转换为数值分子描述符，通过主成分分析（PCA）进行降维，以及采用在多个量子和经典核函数优化组合上训练的支持向量机（SVM）。通过将QMKL-SVM与经典梯度提升模型进行基准测试，我们表明量子增强方法实现了更高的AUC分数，突显了其在具有挑战性的化学信息学分类任务中提供量子优势的潜力。", "summary": "本文提出并成功应用了一种量子多核学习（QMKL）框架来增强药物发现中的定量构效关系（QSAR）分类。该方法将SMILES表示转换为分子描述符，通过PCA降维，并利用优化组合的量子和经典核函数训练支持向量机（SVM）。在识别DYRK1A激酶抑制剂的数据集上，QMKL-SVM相较于经典梯度提升模型，取得了显著更高的AUC分数，展示了其在化学信息学分类任务中提供量子优势的潜力。", "keywords": "量子多核学习, QSAR, 药物发现, 化学信息学, 支持向量机", "comments": "这项研究通过引入量子多核学习（QMKL）为计算药物发现领域的QSAR建模带来了创新。其亮点在于结合了量子和经典核函数，并通过实验证明了其在性能上优于传统方法，为化学信息学中的复杂分类任务提供了潜在的“量子优势”，为未来的药物发现工作开辟了新途径。"}}
{"id": "2506.14925", "title": "Digital twin for virtual sensing of ferry quays via a Gaussian Process Latent Force Model", "authors": ["Luigi Sibille", "Torodd Skjerve Nord", "Alice Cicirello"], "summary": "Ferry quays experience rapid deterioration due to their exposure to harsh\nmaritime environments and ferry impacts. Vibration-based structural health\nmonitoring offers a valuable approach to assessing structural integrity and\nunderstanding the structural implications of these impacts. However, practical\nlimitations often restrict sensor placement at critical locations.\nConsequently, virtual sensing techniques become essential for establishing a\nDigital Twin and estimating the structural response. This study investigates\nthe application of the Gaussian Process Latent Force Model (GPLFM) for virtual\nsensing on the Magerholm ferry quay, combining in-operation experimental data\ncollected during a ferry impact with a detailed physics-based model. The\nproposed Physics-Encoded Machine Learning model integrates a reduced-order\nstructural model with a data-driven GPLFM representing the unknown impact\nforces via their modal contributions. Significant challenges are addressed for\nthe development of the Digital Twin of the ferry quay, including unknown impact\ncharacteristics (location, direction, intensity), time-varying boundary\nconditions, and sparse sensor configurations. Results show that the GPLFM\nprovides accurate acceleration response estimates at most locations, even under\nsimplifying modeling assumptions such as linear time-invariant behavior during\nthe impact phase. Lower accuracy was observed at locations in the impact zone.\nA numerical study was conducted to explore an optimal real-world sensor\nplacement strategy using a Backward Sequential Sensor Placement approach.\nSensitivity analyses were conducted to examine the influence of sensor types,\nsampling frequencies, and incorrectly assumed damping ratios. The results\nsuggest that the GP latent forces can help accommodate modeling and measurement\nuncertainties, maintaining acceptable estimation accuracy across scenarios.", "comment": "14 Figures, 1 Table", "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.14925v1", "AI": {"title_translation": "基于高斯过程潜在力模型的渡轮码头虚拟传感数字孪生", "tldr": "本文利用高斯过程潜在力模型（GPLFM）为渡轮码头构建数字孪生，以实现虚拟传感，克服了传感器限制和未知冲击特性，从而准确估计结构响应。", "motivation": "渡轮码头因恶劣海洋环境和渡轮冲击而迅速老化；虽然基于振动的结构健康监测很有价值，但传感器放置受限。因此，虚拟传感技术对于建立数字孪生和估计结构响应至关重要。", "method": "本研究探索了高斯过程潜在力模型（GPLFM）在Magerholm渡轮码头虚拟传感中的应用，结合了渡轮冲击期间收集的运行中实验数据和详细的基于物理的模型。所提出的物理编码机器学习模型将降阶结构模型与数据驱动的GPLFM相结合，通过模态贡献表示未知的冲击力。研究解决了数字孪生开发中的挑战，包括未知的冲击特性、时变边界条件和稀疏传感器配置。通过逆向顺序传感器放置方法进行了数值研究以探索最佳传感器放置策略，并进行了敏感性分析以检查传感器类型、采样频率和错误假设阻尼比的影响。", "result": "结果显示，即使在简化建模假设（如冲击阶段的线性时不变行为）下，GPLFM也能在大多数位置提供准确的加速度响应估计。在冲击区域的精度较低。研究表明，GP潜在力有助于适应建模和测量不确定性，并在各种情景下保持可接受的估计精度。", "conclusion": "高斯过程潜在力模型（GPLFM）是渡轮码头虚拟传感的一种有前景的方法，能够处理未知冲击力、稀疏传感器数据等复杂挑战，同时保持良好的精度并适应不确定性。", "translation": "渡轮码头由于暴露在恶劣的海洋环境和渡轮冲击下，会经历快速劣化。基于振动的结构健康监测为评估结构完整性和理解这些冲击的结构影响提供了一种有价值的方法。然而，实际限制通常会限制传感器在关键位置的放置。因此，虚拟传感技术对于建立数字孪生和估计结构响应变得至关重要。本研究调查了高斯过程潜在力模型（GPLFM）在Magerholm渡轮码头虚拟传感中的应用，结合了渡轮冲击期间收集的运行中实验数据和详细的基于物理的模型。所提出的物理编码机器学习模型将降阶结构模型与数据驱动的GPLFM相结合，通过其模态贡献表示未知的冲击力。渡轮码头数字孪生开发中的重大挑战得到了解决，包括未知的冲击特性（位置、方向、强度）、时变边界条件和稀疏传感器配置。结果表明，即使在简化建模假设（例如冲击阶段的线性时不变行为）下，GPLFM也能在大多数位置提供准确的加速度响应估计。在冲击区域的位置观察到较低的精度。进行了一项数值研究，使用逆向顺序传感器放置方法探索最佳的实际传感器放置策略。进行了敏感性分析，以检查传感器类型、采样频率和错误假设阻尼比的影响。结果表明，GP潜在力有助于适应建模和测量不确定性，在各种情景下保持可接受的估计精度。", "summary": "本文利用高斯过程潜在力模型（GPLFM）为渡轮码头开发了一个用于虚拟传感的数字孪生，以克服传感器放置限制和未知冲击力。通过将实验数据与基于物理的模型相结合，所提出的物理编码机器学习模型即使在传感器稀疏和存在不确定性的情况下也能准确估计结构响应。研究表明，GPLFM在提供准确的加速度估计和适应建模和测量不确定性方面是有效的，突出了其在结构健康监测方面的潜力。", "keywords": "数字孪生, 虚拟传感, 高斯过程潜在力模型, 渡轮码头, 结构健康监测", "comments": "本文的创新点在于提出了物理编码机器学习模型，将物理模型与数据驱动的高斯过程潜在力模型（GPLFM）相结合，以解决渡轮码头虚拟传感中的实际挑战，例如未知的冲击特性、时变边界条件和稀疏传感器配置。该方法对于恶劣海洋环境下关键基础设施的结构健康监测具有重要意义。尽管在冲击区域的精度略低，且在某些假设下，但GPLFM能够有效处理建模和测量不确定性，这展示了其强大的鲁棒性。"}}
{"id": "2506.14950", "title": "Double Machine Learning for Conditional Moment Restrictions: IV regression, Proximal Causal Learning and Beyond", "authors": ["Daqian Shao", "Ashkan Soleymani", "Francesco Quinzan", "Marta Kwiatkowska"], "summary": "Solving conditional moment restrictions (CMRs) is a key problem considered in\nstatistics, causal inference, and econometrics, where the aim is to solve for a\nfunction of interest that satisfies some conditional moment equalities.\nSpecifically, many techniques for causal inference, such as instrumental\nvariable (IV) regression and proximal causal learning (PCL), are CMR problems.\nMost CMR estimators use a two-stage approach, where the first-stage estimation\nis directly plugged into the second stage to estimate the function of interest.\nHowever, naively plugging in the first-stage estimator can cause heavy bias in\nthe second stage. This is particularly the case for recently proposed CMR\nestimators that use deep neural network (DNN) estimators for both stages, where\nregularisation and overfitting bias is present. We propose DML-CMR, a two-stage\nCMR estimator that provides an unbiased estimate with fast convergence rate\nguarantees. We derive a novel learning objective to reduce bias and develop the\nDML-CMR algorithm following the double/debiased machine learning (DML)\nframework. We show that our DML-CMR estimator can achieve the minimax optimal\nconvergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity\nconditions, where $N$ is the sample size. We apply DML-CMR to a range of\nproblems using DNN estimators, including IV regression and proximal causal\nlearning on real-world datasets, demonstrating state-of-the-art performance\nagainst existing CMR estimators and algorithms tailored to those problems.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14950v1", "AI": {"title_translation": "条件矩限制的双重机器学习：工具变量回归、近端因果学习及其他", "tldr": "提出DML-CMR，一个基于双重机器学习框架的两阶段条件矩限制估计器，解决了传统方法中的偏差问题，并实现了最优化收敛速度和最先进的性能。", "motivation": "解决条件矩限制（CMRs）是统计学、因果推断和计量经济学中的关键问题。现有CMR估计器通常采用两阶段方法，但简单地将第一阶段估计结果代入第二阶段会导致严重的偏差，尤其是在使用深度神经网络（DNN）估计器时，会存在正则化和过拟合偏差。", "method": "提出DML-CMR，这是一种两阶段的CMR估计器，旨在提供无偏估计并保证快速收敛。该方法通过推导一个新颖的学习目标来减少偏差，并遵循双重/去偏机器学习（DML）框架开发了DML-CMR算法。", "result": "DML-CMR估计器在参数化和温和正则条件下，能够达到$O(N^{-1/2})$的极小极大最优收敛速度（N为样本量）。在工具变量回归和近端因果学习等实际数据集问题上应用DNN估计器时，DML-CMR表现出优于现有CMR估计器和针对这些问题量身定制的算法的最先进性能。", "conclusion": "DML-CMR有效解决了条件矩限制估计中的偏差问题，并展示了在多种因果推断问题上的优越性能和理论保证。", "translation": "解决条件矩限制（CMRs）是统计学、因果推断和计量经济学中考虑的一个关键问题，其目的是求解满足某些条件矩等式的目标函数。具体来说，许多因果推断技术，如工具变量（IV）回归和近端因果学习（PCL），都是CMR问题。大多数CMR估计器采用两阶段方法，其中第一阶段的估计结果直接代入第二阶段以估计目标函数。然而，简单地代入第一阶段估计器会导致第二阶段的严重偏差。对于最近提出的使用深度神经网络（DNN）估计器进行两阶段估计的CMR估计器来说，这种情况尤为突出，因为存在正则化和过拟合偏差。我们提出了DML-CMR，这是一种两阶段的CMR估计器，它能提供无偏估计并保证快速收敛速度。我们推导了一个新颖的学习目标来减少偏差，并遵循双重/去偏机器学习（DML）框架开发了DML-CMR算法。我们证明了DML-CMR估计器在参数化和温和正则条件下，可以达到$O(N^{-1/2})$的极小极大最优收敛速度，其中N是样本量。我们将DML-CMR应用于一系列使用DNN估计器的问题，包括在真实世界数据集上的工具变量回归和近端因果学习，展示了其相对于现有CMR估计器和针对这些问题量身定制的算法的最先进性能。", "summary": "本文提出了DML-CMR，一种基于双重/去偏机器学习框架的两阶段条件矩限制（CMRs）估计器。该方法旨在解决传统CMR估计器中因简单代入第一阶段结果而导致的偏差问题，尤其是在使用深度神经网络时。DML-CMR通过引入新的学习目标，实现了无偏估计和快速收敛，并在理论上证明了其能达到最优收敛速度。在实际数据集上的工具变量回归和近端因果学习应用中，DML-CMR展示了优于现有方法的先进性能。", "keywords": "条件矩限制, 双重机器学习, 因果推断, 工具变量回归, 近端因果学习", "comments": "这篇论文通过将双重机器学习（DML）框架应用于条件矩限制（CMRs）问题，提供了一个重要的贡献。其创新之处在于解决了传统两阶段CMR估计中常见的偏差问题，特别是当结合深度神经网络时。理论上保证的$O(N^{-1/2})$最优收敛速度以及在实际应用中展现的最先进性能，凸显了该方法的有效性和重要性。它为因果推断和计量经济学中的复杂模型估计提供了更稳健的工具。"}}
{"id": "2506.14952", "title": "An Observation on Lloyd's k-Means Algorithm in High Dimensions", "authors": ["David Silva-Sánchez", "Roy R. Lederman"], "summary": "Clustering and estimating cluster means are core problems in statistics and\nmachine learning, with k-means and Expectation Maximization (EM) being two\nwidely used algorithms. In this work, we provide a theoretical explanation for\nthe failure of k-means in high-dimensional settings with high noise and limited\nsample sizes, using a simple Gaussian Mixture Model (GMM). We identify regimes\nwhere, with high probability, almost every partition of the data becomes a\nfixed point of the k-means algorithm. This study is motivated by challenges in\nthe analysis of more complex cases, such as masked GMMs, and those arising from\napplications in Cryo-Electron Microscopy.", "comment": "27 pages, 3 figures, 4 supplemental figures", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14952v1", "AI": {"title_translation": "对高维空间中Lloyd's k-Means算法的观察", "tldr": "本文理论解释了k-means算法在高维、高噪声和样本量有限的情况下失效的原因，发现几乎所有数据划分都会成为算法的固定点。", "motivation": "本研究的动机是分析更复杂情况（如掩膜高斯混合模型）以及低温电子显微镜应用中遇到的挑战。", "method": "通过使用一个简单的高斯混合模型（GMM）提供理论解释。", "result": "在高维、高噪声和有限样本量的情况下，识别出高概率下几乎所有数据划分都成为k-means算法固定点的区域。", "conclusion": "在高维、高噪声和有限样本量设置中，k-means算法会失效，因为几乎所有数据划分都可能成为其固定点。", "translation": "聚类和估计聚类均值是统计学和机器学习中的核心问题，其中k-means和期望最大化（EM）是两种广泛使用的算法。在这项工作中，我们使用简单的高斯混合模型（GMM），为k-means在高维、高噪声和有限样本量设置中失效提供了理论解释。我们确定了在这些情况下，几乎所有数据划分都以高概率成为k-means算法固定点的区域。这项研究的动机是分析更复杂情况（如掩膜高斯混合模型）以及低温电子显微镜应用中遇到的挑战。", "summary": "本文理论分析了在高维、高噪声和有限样本量条件下，Lloyd's k-Means算法失效的机制。研究通过一个简单的高斯混合模型发现，在这种设置下，几乎所有数据划分都可能成为k-means算法的固定点，从而解释了其聚类性能下降的原因。这项工作旨在解决复杂高斯混合模型和低温电子显微镜应用中的分析挑战。", "keywords": "k-means, 高维, 聚类, 高斯混合模型, 固定点", "comments": "本文为k-means算法在高维环境下的已知局限性提供了重要的理论解释，特别是在高噪声和样本量有限的情况下。其创新之处在于识别了导致算法失效的“固定点”机制，这对于理解和改进高维数据聚类算法具有指导意义。该研究的发现对于处理如低温电子显微镜等复杂应用中的高维数据具有实际价值。"}}
{"id": "2506.14957", "title": "POCO: Scalable Neural Forecasting through Population Conditioning", "authors": ["Yu Duan", "Hamza Tahir Chaudhry", "Misha B. Ahrens", "Christopher D Harvey", "Matthew G Perich", "Karl Deisseroth", "Kanaka Rajan"], "summary": "Predicting future neural activity is a core challenge in modeling brain\ndynamics, with applications ranging from scientific investigation to\nclosed-loop neurotechnology. While recent models of population activity\nemphasize interpretability and behavioral decoding, neural\nforecasting-particularly across multi-session, spontaneous recordings-remains\nunderexplored. We introduce POCO, a unified forecasting model that combines a\nlightweight univariate forecaster with a population-level encoder to capture\nboth neuron-specific and brain-wide dynamics. Trained across five calcium\nimaging datasets spanning zebrafish, mice, and C. elegans, POCO achieves\nstate-of-the-art accuracy at cellular resolution in spontaneous behaviors.\nAfter pre-training, POCO rapidly adapts to new recordings with minimal\nfine-tuning. Notably, POCO's learned unit embeddings recover biologically\nmeaningful structure-such as brain region clustering-without any anatomical\nlabels. Our comprehensive analysis reveals several key factors influencing\nperformance, including context length, session diversity, and preprocessing.\nTogether, these results position POCO as a scalable and adaptable approach for\ncross-session neural forecasting and offer actionable insights for future model\ndesign. By enabling accurate, generalizable forecasting models of neural\ndynamics across individuals and species, POCO lays the groundwork for adaptive\nneurotechnologies and large-scale efforts for neural foundation models.", "comment": null, "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.14957v1", "AI": {"title_translation": "POCO：通过群体条件化实现可扩展的神经预测", "tldr": "POCO是一种新型的神经预测模型，它结合了单元级预测器和群体编码器，实现了跨会话自发神经活动的精确预测，并在不同物种上表现出优异的性能和可扩展性，其学习到的嵌入还能揭示生物学意义上的脑区结构。", "motivation": "预测未来的神经活动是建模大脑动力学的核心挑战，在科学研究和闭环神经技术中都有应用。尽管近期对群体活动模型的关注在于可解释性和行为解码，但神经预测，尤其是在多会话、自发记录中，仍未得到充分探索。", "method": "本文引入了POCO模型，这是一种统一的预测模型，它将轻量级的单变量预测器与群体级编码器相结合，以捕获神经元特异性和全脑动力学。该模型在斑马鱼、小鼠和秀丽隐杆线虫的五个钙成像数据集上进行训练。", "result": "POCO在自发行为中实现了细胞分辨率下的最先进预测精度。经过预训练后，POCO能够通过最少的微调快速适应新的记录。值得注意的是，POCO学习到的单元嵌入在没有任何解剖标签的情况下，能够恢复具有生物学意义的结构，例如脑区聚类。全面的分析揭示了影响性能的几个关键因素，包括上下文长度、会话多样性和预处理。", "conclusion": "POCO作为一种可扩展且适应性强的跨会话神经预测方法，为未来的模型设计提供了可操作的见解。通过实现跨个体和物种的神经动力学准确、可泛化的预测模型，POCO为自适应神经技术和神经基础模型的大规模开发奠定了基础。", "translation": "预测未来的神经活动是建模大脑动力学的核心挑战，其应用范围从科学研究到闭环神经技术。虽然最近的群体活动模型强调可解释性和行为解码，但神经预测——尤其是在多会话、自发记录中——仍未得到充分探索。我们引入了POCO，一个统一的预测模型，它将轻量级的单变量预测器与群体级编码器相结合，以捕获神经元特异性和全脑动力学。POCO在斑马鱼、小鼠和秀丽隐杆线虫的五个钙成像数据集上进行训练，在自发行为中实现了细胞分辨率下的最先进精度。经过预训练后，POCO能够通过最少的微调快速适应新的记录。值得注意的是，POCO学习到的单元嵌入在没有任何解剖标签的情况下，能够恢复具有生物学意义的结构——例如脑区聚类。我们的综合分析揭示了影响性能的几个关键因素，包括上下文长度、会话多样性和预处理。总而言之，这些结果将POCO定位为一种可扩展且适应性强的跨会话神经预测方法，并为未来的模型设计提供了可操作的见解。通过实现跨个体和物种的神经动力学准确、可泛化的预测模型，POCO为自适应神经技术和大规模的神经基础模型工作奠定了基础。", "summary": "POCO是一种新颖的神经预测模型，旨在解决多会话、自发神经记录的预测挑战。它结合了轻量级单变量预测器和群体级编码器，能够同时捕捉神经元特异性和全脑动力学。该模型在多种物种的钙成像数据集上取得了最先进的预测精度，并能快速适应新数据。此外，POCO学习到的神经元嵌入能无监督地揭示生物学意义上的脑区结构。研究结果表明POCO具有高度的可扩展性和适应性，为未来的神经技术和基础模型奠定了基础。", "keywords": "神经预测, 群体条件化, 钙成像, 脑动力学, 机器学习", "comments": "POCO的创新之处在于其结合了单元级和群体级信息来预测神经活动，并实现了跨物种和跨会话的泛化能力。其能够在无解剖标签的情况下发现生物学意义上的脑区结构，显示了模型强大的表征学习能力。这对于理解大脑动力学和开发自适应神经技术具有重要意义。该模型的可扩展性和快速适应性是其关键优势。"}}
{"id": "2506.15020", "title": "Data analysis using discrete cubical homology", "authors": ["Chris Kapulkin", "Nathan Kershaw"], "summary": "We present a new tool for data analysis: persistence discrete homology, which\nis well-suited to analyze filtrations of graphs. In particular, we provide a\nnovel way of representing high-dimensional data as a filtration of graphs using\npairwise correlations. We discuss several applications of these tools, e.g., in\nweather and financial data, comparing them to the standard methods used in the\nrespective fields.", "comment": "17 pages; comments welcome", "cate": "math.AT", "url": "http://arxiv.org/abs/2506.15020v1", "AI": {"title_translation": "使用离散立方同调进行数据分析", "tldr": "提出了一种新的数据分析工具——持久离散同调，通过将高维数据表示为图过滤来分析，并应用于天气和金融数据。", "motivation": "提供一种新的数据分析工具，即持久离散同调，以有效分析图的过滤，并特别处理高维数据。", "method": "提出持久离散同调作为数据分析工具，并开发了一种新颖的方法，利用成对相关性将高维数据表示为图的过滤。", "result": "讨论了该工具在天气和金融数据等领域的多种应用，并将其与各自领域的标准方法进行了比较。", "conclusion": "本文介绍了一种新的数据分析工具——持久离散同调，它非常适合分析图的过滤，并能以新颖的方式表示高维数据，在天气和金融数据等实际应用中展现了潜力。", "translation": "我们提出了一种新的数据分析工具：持久离散同调，它非常适合分析图的过滤。特别是，我们提供了一种新颖的方式，利用成对相关性将高维数据表示为图的过滤。我们讨论了这些工具的几种应用，例如在天气和金融数据中，并将它们与各自领域中使用的标准方法进行比较。", "summary": "本文介绍了一种名为持久离散同调的新型数据分析工具，该工具特别适用于分析图的过滤。研究提出了一种创新方法，通过成对相关性将高维数据表示为图的过滤。论文还探讨了该工具在天气和金融数据等领域的实际应用，并将其性能与传统方法进行了对比。", "keywords": "持久同调, 离散同调, 数据分析, 图过滤, 高维数据", "comments": "这篇论文的创新点在于提出了持久离散同调这一新的数据分析工具，并将其应用于高维数据的图过滤表示。其重要性在于为处理复杂数据集提供了一种新的拓扑数据分析视角，尤其在处理金融和天气等领域的时间序列或复杂关联数据时可能展现出优势。摘要中未详细说明与标准方法的比较结果，这可能限制了对其具体性能的评估。"}}
{"id": "2506.15315", "title": "Proximal Operators of Sorted Nonconvex Penalties", "authors": ["Anne Gagneux", "Mathurin Massias", "Emmanuel Soubies"], "summary": "This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15315v1", "AI": {"title_translation": "排序非凸惩罚的近端算子", "tldr": "研究了用于稀疏信号恢复的排序非凸惩罚，并提供了高效计算其近端算子的方法，包括针对弱凸和非凸情况的PAV算法变体。", "motivation": "解决稀疏信号恢复中的变量自动分组问题，通过使用排序非凸惩罚作为正则化方法，旨在促进变量聚类并减少系数收缩，并提供计算其近端算子的有效方法以支持近端算法。", "method": "研究排序非光滑惩罚作为广义线性模型的正则化方法，特别关注推广SLOPE的排序非凸惩罚；区分弱凸和非凸两种情况下的近端算子计算，并针对弱凸情况（如排序MCP和SCAD）利用PAV算法精确计算，针对非凸情况（如q在]0,1[的排序Lq）展示了PAV算法的轻微修改版本能高效处理；提出了关于非凸近端问题最小化器的新理论见解；通过实验验证了这些惩罚的实用性。", "result": "开发了针对弱凸排序非凸惩罚（如排序MCP和SCAD）近端算子的PAV算法精确计算方法；展示了PAV算法的轻微修改版本能高效处理非凸排序非凸惩罚（如排序Lq，q在]0,1[）的近端算子计算；提供了关于非凸近端问题最小化器的新理论见解；实验证明了使用此类惩罚的实际价值。", "conclusion": "该工作为稀疏信号恢复中的排序非凸惩罚提供了高效的近端算子计算方法，包括针对弱凸和非凸情况的PAV算法及其变体，并提供了新的理论见解，证明了其在实际应用中的有效性。", "translation": "这项工作研究了稀疏信号恢复与变量自动分组的问题。为此，我们研究了排序非光滑惩罚作为广义线性模型的一种正则化方法。我们专注于一类排序非凸惩罚，它推广了排序L1范数（SLOPE）。这些惩罚由于其排序性质旨在促进变量的聚类，而非凸性则减少了系数的收缩。我们的目标是提供有效的方法来计算它们的近端算子，从而使得流行的近端算法能够解决使用这种排序惩罚的复合优化问题。我们区分了两类问题：弱凸情况，其中计算近端算子仍然是一个凸问题；以及非凸情况，其中计算近端算子成为一个具有挑战性的非凸组合问题。对于弱凸情况（例如排序MCP和SCAD），我们解释了邻池违规者（PAV）算法如何精确计算近端算子。对于非凸情况（例如q在]0,1[的排序Lq），我们表明该算法的轻微修改版本在处理近端算子计算方面非常高效。我们还提出了关于非凸近端问题最小化器的新理论见解。我们在几个实验中展示了使用此类惩罚的实际兴趣。", "summary": "本文探讨了在稀疏信号恢复中应用排序非凸惩罚以实现变量自动分组的问题。为了解决使用这类惩罚的复合优化问题，作者提供了高效计算其近端算子的方法。研究区分了弱凸和非凸两种情况：对于弱凸情况（如排序MCP、SCAD），利用PAV算法精确计算；对于非凸情况（如排序Lq），PAV算法的修改版本被证明高效。此外，本文还提供了关于非凸近端问题最小化器的新理论见解，并通过实验验证了这些惩罚的实用价值。", "keywords": "稀疏信号恢复, 排序非凸惩罚, 近端算子, PAV算法, 广义线性模型", "comments": "这篇论文通过提出高效计算排序非凸惩罚近端算子的方法，解决了稀疏信号恢复中一个关键的计算挑战。其创新之处在于将PAV算法及其变体应用于处理弱凸和非凸的近端问题，这对于推广排序惩罚在实际优化问题中的应用具有重要意义。对非凸情况的有效处理和理论见解也增加了其价值。"}}
{"id": "2506.15387", "title": "Multi-Timescale Gradient Sliding for Distributed Optimization", "authors": ["Junhui Zhang", "Patrick Jaillet"], "summary": "We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15387v1", "AI": {"title_translation": "分布式优化中的多时间尺度梯度滑动", "tldr": "本文提出了两种新的多时间尺度梯度滑动算法（MT-GS和AMT-GS），用于解决凸、非光滑、分布式优化问题。这些算法通过利用目标相似性来减少通信，具有灵活的通信速率和最优的收敛性，并解决了非光滑目标函数通信轮次依赖性的一个开放问题。", "motivation": "论文旨在解决凸、非光滑分布式优化问题中的通信效率和灵活性问题，并通过利用局部目标函数之间的相似性来减少通信轮次，并允许不同代理子集以不同速率通信，同时解决了非光滑目标函数通信轮次对相似性参数依赖性的一个开放问题。", "method": "提出了两种一阶方法：多时间尺度梯度滑动（MT-GS）及其加速变体（AMT-GS），用于凸、非光滑、分布式优化问题。这些方法通过块可分解的原始-对偶公式和一种多时间尺度滑动方法实现，其中不同的对偶块可以以不同的速率更新。它们能够利用局部目标函数的相似性来减少通信轮次，并允许不同代理子集以用户选择的速率进行通信，且是完全确定性的。", "result": "算法的复杂性对 $\\epsilon$ 具有最优依赖性。对于Lipschitz目标函数，MT-GS 需要 $O(\\overline{r}A/\\epsilon)$ 次通信轮次和 $O(\\overline{r}/\\epsilon^2)$ 次次梯度步骤。如果目标函数是 $\\mu$-强凸的，AMT-GS 需要 $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ 次通信轮次和 $O(\\overline{r}/(\\epsilon\\mu))$ 次次梯度步骤。其中 $\\overline{r}$ 衡量对偶块的“平均更新速率”，$A$ 衡量局部函数（次梯度）之间的相似性。通信轮次对 $A$ 的线性依赖性是最佳的。", "conclusion": "该论文提出的算法在非光滑目标函数的情况下，实现了通信轮次对相似性参数 $A$ 的最优线性依赖，从而对这一领域的开放问题给出了肯定的答案。", "translation": "我们提出了两种用于凸、非光滑、分布式优化问题的一阶方法，在此称为多时间尺度梯度滑动（MT-GS）及其加速变体（AMT-GS）。我们的MT-GS和AMT-GS可以利用（局部）目标之间的相似性来减少通信轮次，并且灵活，因此不同的（代理）子集可以以不同的、用户选择的速率进行通信，并且是完全确定性的。这三个理想的特性是通过块可分解的原始-对偶公式以及Lan等人（2020）、Lan（2016）引入的滑动方法的多时间尺度变体实现的，其中不同的对偶块可能以不同的速率更新。\n为了找到一个 $\\epsilon$-次优解，我们的算法的复杂性对 $\\epsilon$ 实现了最优依赖性：对于Lipschitz目标函数，MT-GS需要 $O(\\overline{r}A/\\epsilon)$ 次通信轮次和 $O(\\overline{r}/\\epsilon^2)$ 次次梯度步骤；如果目标函数也是 $\\mu$-强凸的，AMT-GS需要 $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ 次通信轮次和 $O(\\overline{r}/(\\epsilon\\mu))$ 次次梯度步骤。其中，$\\overline{r}$ 衡量对偶块的“平均更新速率”，$A$ 衡量局部函数（次梯度）之间的相似性。此外，通信轮次对 $A$ 的线性依赖性是最佳的（Arjevani和Shamir 2015），从而对非光滑目标函数是否可以实现这种依赖性的开放问题（Arjevani和Shamir 2015）提供了肯定的答案。", "summary": "本文提出了两种针对凸、非光滑分布式优化问题的新型一阶方法：多时间尺度梯度滑动（MT-GS）及其加速版本（AMT-GS）。这些算法通过块可分解的原始-对偶公式和多时间尺度滑动方法，能够利用局部目标函数的相似性来显著减少通信轮次，并支持不同代理子集的灵活通信速率。在收敛性方面，MT-GS和AMT-GS在达到 $\\epsilon$-次优解时，对 $\\epsilon$ 展现出最优的依赖性，并且实现了通信轮次对局部函数相似性参数 $A$ 的最优线性依赖，从而解决了非光滑目标函数领域的一个长期存在的开放问题。", "keywords": "分布式优化, 梯度滑动, 多时间尺度, 非光滑优化, 通信效率", "comments": "这篇论文的创新点在于提出了多时间尺度梯度滑动（MT-GS）及其加速变体（AMT-GS），通过引入多时间尺度更新机制和利用目标函数相似性，显著优化了分布式优化中的通信效率。其重要性体现在解决了非光滑目标函数通信轮次对相似性参数 $A$ 的最优线性依赖这一开放问题，为分布式优化领域提供了理论和实践上的新进展。"}}
{"id": "2506.15505", "title": "Time-dependent density estimation using binary classifiers", "authors": ["Agnimitra Dasgupta", "Javier Murgoitio-Esandi", "Ali Fardisi", "Assad A Oberai"], "summary": "We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15505v1", "AI": {"title_translation": "使用二元分类器进行时间依赖密度估计", "tldr": "本文提出了一种数据驱动的方法，利用时间依赖的二元分类器和对比估计来学习多元随机过程的时间依赖概率密度，并成功应用于随机激励系统、样本合成和异常检测。", "motivation": "需要一种数据驱动的方法来从样本路径中学习多元随机过程的时间依赖概率密度，尤其是在已知初始概率密度的情况下。", "method": "该方法提出了一种新颖的时间依赖二元分类器，通过基于对比估计的目标进行训练，以区分随机过程在两个相邻时间点上的实现。该方法显式地建模了时间依赖概率分布，并且分类器中最终激活前的输入是密度对数对时间偏导数的二阶近似。应用于随机激励系统、使用随机插值器生成训练所需的样本路径，并通过基于梯度的马尔可夫链蒙特卡罗方法（利用自动微分）生成新样本，以及应用于无监督异常检测。", "result": "该方法能够准确重构复杂的时间依赖、多模态和近简并密度，有效地扩展到中等高维问题，并可靠地检测真实世界数据中的罕见事件。", "conclusion": "所提出的方法通过数值实验证明能够准确重构复杂的时间依赖密度，有效处理高维问题，并可靠地检测异常事件，表明其在时间依赖概率密度估计方面的实用性。", "translation": "我们提出了一种数据驱动的方法，用于从样本路径中学习多元随机过程的时间依赖概率密度，假设初始概率密度已知且可评估。我们的方法使用一种新颖的时间依赖二元分类器，该分类器采用基于对比估计的目标进行训练，以区分随机过程在两个相邻时间点上的实现。重要的是，所提出的方法明确地建模了时间依赖概率分布，这意味着可以在感兴趣的时间范围内获得概率密度的值。此外，时间依赖分类器中最终激活之前的输入是密度对数对时间偏导数的二阶近似。我们将所提出的方法应用于近似由随机激励驱动的系统的时间依赖概率密度函数。我们还使用所提出的方法从给定的一组随机向量实现中合成新的样本。在此类应用中，我们使用随机插值器生成训练所需的样本路径。随后，由于自动微分可以有效地提供所需的梯度，因此使用基于梯度的马尔可夫链蒙特卡罗方法生成新样本。此外，我们通过在无监督异常检测中的应用，展示了时间依赖概率密度函数的显式近似的实用性。通过多项数值实验，我们表明所提出的方法能够准确地重构复杂的时间依赖、多模态和近简并密度，有效地扩展到中等高维问题，并可靠地检测真实世界数据中的罕见事件。", "summary": "本文提出了一种数据驱动的新方法，利用时间依赖的二元分类器和基于对比估计的目标，从样本路径中学习多元随机过程的时间依赖概率密度。该方法显式建模时间依赖分布，并利用分类器内部的二阶近似来估计密度对数的时间偏导数。实验表明，该方法能准确重构复杂的时间依赖密度，有效处理高维问题，并成功应用于随机激励系统、新样本合成和无监督异常检测，尤其在检测罕见事件方面表现出色。", "keywords": "时间依赖密度估计, 二元分类器, 对比估计, 随机过程, 异常检测", "comments": "这项研究的创新之处在于其将时间依赖密度估计问题转化为使用二元分类器和对比估计的框架。通过显式建模时间依赖分布并利用分类器内部的导数近似，该方法提供了一种有效且可扩展的解决方案。其在处理复杂、多模态密度以及在高维问题中的表现令人印象深刻，同时在异常检测等实际应用中也展现了潜力。"}}
{"id": "2506.15643", "title": "Revisiting Randomization in Greedy Model Search", "authors": ["Xin Chen", "Jason M. Klusowski", "Yan Shuo Tan", "Chang Yu"], "summary": "Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15643v1", "AI": {"title_translation": "重新审视贪婪模型搜索中的随机化", "tldr": "本文提出并分析了一种通过特征子采样随机化的贪婪前向选择估计器集成，显著提高了计算效率，并在稀疏线性回归中优于Lasso等方法。研究发现，随机集成可以同时降低训练误差和自由度，从而改变偏差-方差权衡曲线，并证明了其通过逻辑权重重新缩放系数来扩大模型搜索空间，加深了对随机森林和隐式正则化的理解。", "motivation": "组合随机估计器（如随机森林）是一种基本技术，但计算成本高昂，且其改善预测性能的机制尚不明确。本文旨在解决这些问题。", "method": "在稀疏线性回归背景下，提出并分析了一种通过特征子采样进行随机化的贪婪前向选择估计器集成。在每次迭代中，从随机子集中选择最佳特征。设计了一种基于动态规划的新颖实现，大大提高了计算效率。在正交特征设置下，严格证明了集成估计器通过两参数逻辑权重重新缩放普通最小二乘系数，从而扩大了模型搜索空间。", "result": "该方法大大提高了计算效率。通过数值实验表明，该方法在多种设置下优于Lasso和弹性网等流行方法。数值实验还表明，随机集成可以同时降低训练误差和自由度，从而改变基本估计器的整个偏差-方差权衡曲线。在正交特征设置下，证明了集成估计器通过两参数逻辑权重重新缩放普通最小二乘系数，从而扩大了模型搜索空间。", "conclusion": "本文提出的随机化贪婪模型搜索方法在计算效率和预测性能上表现出色，并深入揭示了随机集成如何通过同时降低训练误差和自由度来改变偏差-方差权衡曲线，以及通过扩大模型搜索空间来发挥作用。这些结果增强了对随机森林的理解，并表明隐式正则化可能比显式正则化具有更复杂的影响。", "translation": "将随机估计器组合成一个集成，例如通过随机森林，已成为现代数据科学中的一项基本技术，但计算成本可能很高。此外，这种方法改善预测性能的机制尚不清楚。我们通过提出和分析一种通过特征子采样随机化的贪婪前向选择估计器集成，解决了稀疏线性回归背景下的这些问题——在每次迭代中，从随机子集中选择最佳特征。我们设计了一种基于动态规划的新颖实现，极大地提高了其计算效率。此外，我们通过仔细的数值实验表明，我们的方法在广泛的设置下可以优于Lasso和弹性网等流行方法。接下来，与普遍认为随机集成类似于收缩的观点相反，我们通过数值实验表明，它可以在同时降低训练误差和自由度，从而改变基本估计器的整个偏差-方差权衡曲线。我们在正交特征的设置下严格证明了这一事实，在这种情况下，集成估计器通过两参数逻辑权重族重新缩放普通最小二乘系数，从而扩大了模型搜索空间。这些结果增强了我们对随机森林的理解，并表明一般的隐式正则化可能比显式正则化具有更复杂的影响。", "summary": "本文针对随机估计器集成（如随机森林）计算成本高和性能提升机制不明的问题，在稀疏线性回归中提出了一种基于特征子采样的随机化贪婪前向选择估计器集成方法。通过动态规划实现，显著提高了计算效率，并在实验中优于Lasso等流行方法。研究还发现，该方法能同时降低训练误差和自由度，从而改变偏差-方差权衡曲线，并通过逻辑权重扩大模型搜索空间。这些发现深化了对随机森林和隐式正则化的理解。", "keywords": "随机化, 贪婪模型搜索, 特征子采样, 稀疏线性回归, 偏差-方差权衡", "comments": "本文的创新点在于提出了一种高效的随机化贪婪模型搜索方法，并通过理论和实验揭示了随机集成在改善模型性能方面的独特机制，即并非简单地等同于收缩，而是通过同时降低训练误差和自由度来改变偏差-方差权衡，并扩大模型搜索空间。这为理解随机森林和隐式正则化提供了新的视角，具有重要的理论和实践意义。"}}
