{"id": "2506.20039", "title": "Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning", "authors": ["Koorosh Moslemi", "Chi-Guhn Lee"], "summary": "Team formation and the dynamics of team-based learning have drawn significant\ninterest in the context of Multi-Agent Reinforcement Learning (MARL). However,\nexisting studies primarily focus on unilateral groupings, predefined teams, or\nfixed-population settings, leaving the effects of algorithmic bilateral\ngrouping choices in dynamic populations underexplored. To address this gap, we\nintroduce a framework for learning two-sided team formation in dynamic\nmulti-agent systems. Through this study, we gain insight into what algorithmic\nproperties in bilateral team formation influence policy performance and\ngeneralization. We validate our approach using widely adopted multi-agent\nscenarios, demonstrating competitive performance and improved generalization in\nmost scenarios.", "comment": "Accepted to the 2nd Coordination and Cooperation in Multi-Agent\n  Reinforcement Learning (CoCoMARL) Workshop at RLC 2025", "pdf_url": "http://arxiv.org/pdf/2506.20039v1", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.20039v1", "AI": {"title_translation": "学习合作多智能体强化学习中的双边团队组建", "tldr": "本文提出了一个在动态多智能体系统中学习双边团队组建的框架，旨在解决现有研究在多智能体强化学习中对动态种群中算法双边分组选择的探索不足，并验证了其在策略性能和泛化能力上的提升。", "motivation": "现有研究在多智能体强化学习（MARL）中的团队组建方面，主要关注单边分组、预定义团队或固定种群设置，导致动态种群中算法双边分组选择的影响未被充分探索。本文旨在弥补这一空白。", "method": "引入了一个用于在动态多智能体系统中学习双边团队组建的框架。", "result": "在广泛采用的多智能体场景中，展示了具有竞争力的性能和改进的泛化能力。", "conclusion": "通过引入双边团队组建框架，本文深入了解了影响策略性能和泛化能力的算法属性，并验证了其在动态多智能体系统中的有效性。", "translation": "团队组建和基于团队的学习动态在多智能体强化学习（MARL）中引起了广泛关注。然而，现有研究主要侧重于单边分组、预定义团队或固定种群设置，使得动态种群中算法双边分组选择的影响未被充分探索。为了弥补这一空白，我们引入了一个用于在动态多智能体系统中学习双边团队组建的框架。通过这项研究，我们深入了解了双边团队组建中哪些算法属性会影响策略性能和泛化能力。我们使用广泛采用的多智能体场景验证了我们的方法，在大多数场景中展示了具有竞争力的性能和改进的泛化能力。", "summary": "本文针对多智能体强化学习（MARL）中团队组建的现有研究主要集中于单边或固定分组的不足，提出了一种在动态多智能体系统中学习双边团队组建的新颖框架。该研究深入探讨了双边团队组建中算法属性如何影响策略性能和泛化能力，并在多种多智能体场景中验证了其方法，展示了具有竞争力的性能和显著的泛化能力提升。", "keywords": "多智能体强化学习, 团队组建, 双边分组, 动态种群, 泛化能力", "comments": "该论文的创新点在于解决了动态种群中双边团队组建的问题，这是多智能体强化学习中一个相对未被充分探索的领域。这有助于开发更灵活和鲁棒的多智能体系统。"}}
{"id": "2506.20400", "title": "A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem", "authors": ["Kristoffer Christensen", "Bo Nørregaard Jørgensen", "Zheng Grace Ma"], "summary": "Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging\necosystems generate large, complex, and stochastic time-series datasets that\ncapture interactions between households, grid infrastructure, and energy\nmarkets. These interactions can lead to unexpected system-level events, such as\ntransformer overloads or consumer dissatisfaction, that are difficult to detect\nand explain through static post-processing. This paper presents a modular,\nPython-based dashboard framework, built using Dash by Plotly, that enables\nefficient, multi-level exploration and root-cause analysis of emergent behavior\nin MABS outputs. The system features three coordinated views (System Overview,\nSystem Analysis, and Consumer Analysis), each offering high-resolution\nvisualizations such as time-series plots, spatial heatmaps, and agent-specific\ndrill-down tools. A case study simulating full EV adoption with smart charging\nin a Danish residential network demonstrates how the dashboard supports rapid\nidentification and contextual explanation of anomalies, including clustered\ntransformer overloads and time-dependent charging failures. The framework\nfacilitates actionable insight generation for researchers and distribution\nsystem operators, and its architecture is adaptable to other distributed energy\nresources and complex energy systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20400v1", "categories": ["cs.MA", "cs.CE", "cs.HC", "cs.SY", "eess.SY"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.20400v1", "AI": {"title_translation": "一个用于探索多智能体仿真可视化的框架：以电动汽车家庭充电生态系统为例", "tldr": "该论文提出了一个基于Python的可视化仪表板框架，用于分析电动汽车家庭充电生态系统多智能体仿真中出现的复杂、随机的时间序列数据，以帮助研究人员和配电系统运营商识别和解释系统级异常。", "motivation": "多智能体仿真（MABS）生成的电动汽车家庭充电生态系统数据量大、复杂且具有随机性，其中包含的家庭、电网基础设施和能源市场之间的交互可能导致意想不到的系统级事件（如变压器过载或消费者不满），这些事件难以通过静态后处理检测和解释。", "method": "本文提出了一个模块化、基于Python的仪表板框架，使用Plotly的Dash构建。该系统具有三个协调视图（系统概览、系统分析和消费者分析），每个视图都提供高分辨率可视化，如时间序列图、空间热图和智能体特定的钻取工具。", "result": "通过一个模拟丹麦住宅网络中电动汽车全面普及和智能充电的案例研究，该仪表板能够快速识别和上下文解释异常，包括集群变压器过载和时间相关充电故障。", "conclusion": "该可视化框架有助于为研究人员和配电系统运营商生成可操作的见解，并且其架构适用于其他分布式能源和复杂的能源系统。", "translation": "电动汽车（EV）家庭充电生态系统的多智能体仿真（MABS）会生成大量、复杂且随机的时间序列数据集，这些数据集捕捉了家庭、电网基础设施和能源市场之间的交互。这些交互可能导致意想不到的系统级事件，例如变压器过载或消费者不满，这些事件很难通过静态后处理来检测和解释。本文提出了一个模块化、基于Python的仪表板框架，该框架使用Plotly的Dash构建，能够对MABS输出中出现的行为进行高效的多层次探索和根本原因分析。该系统具有三个协调视图（系统概览、系统分析和消费者分析），每个视图都提供高分辨率的可视化，例如时间序列图、空间热图和智能体特定的钻取工具。一个模拟丹麦住宅网络中电动汽车全面普及和智能充电的案例研究表明，该仪表板如何支持快速识别和上下文解释异常，包括集群变压器过载和时间相关充电故障。该框架有助于为研究人员和配电系统运营商生成可操作的见解，并且其架构适用于其他分布式能源和复杂的能源系统。", "summary": "本论文提出了一个基于Python的模块化可视化仪表板框架，旨在解决电动汽车家庭充电生态系统多智能体仿真（MABS）数据复杂性带来的挑战。该框架利用Dash by Plotly构建，提供系统概览、系统分析和消费者分析三个协调视图，通过时间序列图、空间热图和钻取工具等高分辨率可视化，实现对MABS输出中新兴行为的有效多层次探索和根本原因分析。一个在丹麦住宅网络中进行的案例研究展示了该仪表板在快速识别和解释集群变压器过载和时间相关充电故障等异常方面的能力，从而为研究人员和配电系统运营商提供可操作的见解，且其架构具有广泛的适用性。", "keywords": "多智能体仿真, 可视化框架, 电动汽车充电, 时间序列数据, Dash", "comments": "该论文的创新之处在于提供了一个专门针对多智能体仿真复杂输出的可视化框架，特别是在电动汽车充电生态系统领域。其模块化、基于Python的设计使其易于部署和适应。通过提供多层次的探索和根本原因分析工具，该框架显著提高了从复杂仿真数据中提取有价值见解的效率和深度，对于电力系统运营商和能源研究者具有重要实用价值。"}}
{"id": "2506.19968", "title": "Evolutionary Gait Reconfiguration in Damaged Legged Robots", "authors": ["Sahand Farghdani", "Robin Chhabra"], "summary": "Multi-legged robots deployed in complex missions are susceptible to physical\ndamage in their legs, impairing task performance and potentially compromising\nmission success. This letter presents a rapid, training-free damage recovery\nalgorithm for legged robots subject to partial or complete loss of functional\nlegs. The proposed method first stabilizes locomotion by generating a new gait\nsequence and subsequently optimally reconfigures leg gaits via a developed\ndifferential evolution algorithm to maximize forward progression while\nminimizing body rotation and lateral drift. The algorithm successfully restores\nlocomotion in a 24-degree-of-freedom hexapod within one hour, demonstrating\nboth high efficiency and robustness to structural damage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19968v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.19968v1", "AI": {"title_translation": "损伤腿式机器人的进化步态重构", "tldr": "针对腿部受损的机器人，提出了一种快速、免训练的损伤恢复算法，通过生成新步态序列和优化步态重构来恢复运动，并在六足机器人上验证了其高效性和鲁棒性。", "motivation": "多足机器人在复杂任务中易受腿部物理损伤，影响任务性能甚至危及任务成功，因此需要一种损伤恢复算法。", "method": "提出了一种快速、免训练的损伤恢复算法。该方法首先通过生成新的步态序列来稳定运动，然后通过开发的差分进化算法优化重构腿部步态，以最大化前进距离并最小化身体旋转和横向漂移。", "result": "该算法成功地在1小时内恢复了一个24自由度六足机器人的运动，展示了高效率和对结构损伤的鲁棒性。", "conclusion": "该算法能有效且鲁棒地恢复受损多足机器人的运动能力。", "translation": "多足机器人在复杂任务中容易受到腿部物理损伤，从而影响任务性能并可能危及任务成功。本文提出了一种快速、免训练的损伤恢复算法，用于腿部功能部分或完全丧失的腿式机器人。所提出的方法首先通过生成新的步态序列来稳定运动，随后通过开发的差分进化算法最优地重构腿部步态，以最大化前进距离，同时最小化身体旋转和横向漂移。该算法在1小时内成功地恢复了一个24自由度六足机器人的运动，展示了高效率和对结构损伤的鲁棒性。", "summary": "本文提出了一种针对腿部受损多足机器人的快速、免训练损伤恢复算法。该算法通过生成新的步态序列稳定运动，并利用差分进化算法优化腿部步态，以实现最大化前进和最小化不稳定性。实验证明，该方法能高效、鲁棒地恢复受损六足机器人的运动能力。", "keywords": "腿式机器人, 损伤恢复, 步态重构, 差分进化, 六足机器人", "comments": "该研究提出了一种新颖的、免训练的损伤恢复方法，利用进化算法优化受损机器人的步态，显著提高了多足机器人在复杂环境下的适应性和任务成功率。其快速恢复能力是主要创新点。"}}
{"id": "2506.19897", "title": "Can LLMs Replace Humans During Code Chunking?", "authors": ["Christopher Glasz", "Emily Escamilla", "Eric O. Scott", "Anand Patel", "Jacob Zimmer", "Colin Diggs", "Michael Doyle", "Scott Rosen", "Nitin Naik", "Justin F. Brunelle", "Samruddhi Thaker", "Parthav Poudel", "Arun Sridharan", "Amit Madan", "Doug Wendt", "William Macke", "Thomas Schill"], "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19897v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.19897v1", "AI": {"title_translation": "大型语言模型在代码分块中能否取代人类？", "tldr": "本研究探讨了LLMs在处理遗留政府代码（如MUMPS和ALC）时的代码分块能力，发现LLMs可以有效选择与人类专家相似的分块点，并且LLM创建的分块在生成文档方面比人类创建的分块更优。", "motivation": "现有的大型语言模型（LLMs）在处理政府应用中的遗留代码（如MUMPS或汇编语言代码ALC）时面临独特挑战，包括过长的代码Token长度超出LLM的上下文窗口限制，以及LLM主要在现代软件语言上训练，对遗留语言的理解能力未知。因此，需要实证研究LLMs在遗留代码现代化中的应用。", "method": "本研究调查了大型语言模型（LLMs）在现代化使用ALC和MUMPS编写的遗留政府代码中的应用，解决了输入限制的挑战。具体方法是研究各种代码分块方法，以优化遗留代码文件的摘要模块注释生成，并评估代码分块方法对不同LLM（包括GPT-4o、Claude 3 Sonnet、Mixtral和Llama 3）生成的文档质量的影响。", "result": "研究结果表明，大型语言模型（LLMs）可以选择与人类专家分块点高度一致的分块点。此外，分块方法对下游任务（如文档生成）有显著影响。由LLM创建的分块生成的注释比人类创建的分块生成的注释事实性高出20%，有用性高出10%。", "conclusion": "因此，本研究得出结论，在LLM辅助的现代化过程中，大型语言模型（LLMs）可以作为大型代码库人工分块的合适替代品。", "translation": "大型语言模型（LLMs）已成为计算机科学中不可或缺的工具，特别是在代码理解和生成任务方面。然而，现有工作并未解决政府应用程序代码所带来的许多独特挑战。特别是，政府企业软件通常使用MUMPS或汇编语言代码（ALC）等遗留语言编写，并且这些系统的总Token长度超出了当前市售LLM的上下文窗口大小。此外，LLM主要在现代软件语言上进行训练，对遗留语言的测试有限，这使得它们理解遗留语言的能力未知，因此是一个实证研究领域。本文研究了LLM在ALC和MUMPS编写的遗留政府代码现代化中的应用，解决了输入限制的挑战。我们调查了各种代码分块方法，以优化遗留代码文件的摘要模块注释生成，评估了代码分块方法对不同LLM（包括GPT-4o、Claude 3 Sonnet、Mixtral和Llama 3）生成的文档质量的影响。我们的结果表明，LLM可以选择与人类专家分块高度一致的分区点。我们还发现，分块方法对下游任务（如文档生成）有显著影响。LLM创建的分区生成的注释比人类创建的分区生成的注释事实性高出20%，有用性高出10%。因此，我们得出结论，在LLM辅助的现代化过程中，LLM可以作为大型代码库人工分区的合适替代品。", "summary": "本研究探讨了大型语言模型（LLMs）在遗留政府代码（如MUMPS和ALC）现代化中的应用，重点解决输入限制和对遗留语言理解的挑战。通过调查不同的代码分块方法，并评估其对LLM生成文档质量的影响，研究发现LLMs能选择与人类专家相似的代码分块点，并且LLM创建的分块在生成事实性更高、更有用的注释方面优于人类创建的分块。这表明LLMs可以有效取代人类进行大型代码库的分块，以辅助现代化过程。", "keywords": "大型语言模型, 代码分块, 遗留代码, 政府应用, 代码现代化", "comments": "本论文的创新之处在于，它首次实证研究了LLMs在处理特定领域（政府遗留代码）和特定挑战（上下文窗口限制、遗留语言理解）方面的能力。研究结果表明LLMs不仅能够胜任代码分块任务，甚至在某些方面（文档生成质量）优于人类专家，这对于推动LLM在软件现代化领域的应用具有重要意义。然而，论文未详细说明不同分块方法的具体实现细节，也未探讨LLM在其他遗留代码现代化任务（如代码翻译、漏洞修复）中的潜力。"}}
{"id": "2506.19939", "title": "Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement", "authors": ["Aryan Singh Dalal", "Sidharth Rai", "Rahul Singh", "Treman Singh Kaloya", "Rahul Harsha Cheppally", "Ajay Sharda"], "summary": "Application rate errors when using self-propelled agricultural sprayers for\nagricultural production remain a concern. Among other factors, spray boom\ninstability is one of the major contributors to application errors. Spray\nbooms' width of 38m, combined with 30 kph driving speeds, varying terrain, and\nmachine dynamics when maneuvering complex field boundaries, make controls of\nthese booms very complex. However, there is no quantitative knowledge on the\nextent of boom movement to systematically develop a solution that might include\nboom designs and responsive boom control systems. Therefore, this study was\nconducted to develop an automated computer vision system to quantify the boom\nmovement of various agricultural sprayers. A computer vision system was\ndeveloped to track a target on the edge of the sprayer boom in real time. YOLO\nV7, V8, and V11 neural network models were trained to track the boom's\nmovements in field operations to quantify effective displacement in the\nvertical and transverse directions. An inclinometer sensor was mounted on the\nboom to capture boom angles and validate the neural network model output. The\nresults showed that the model could detect the target with more than 90 percent\naccuracy, and distance estimates of the target on the boom were within 0.026 m\nof the inclinometer sensor data. This system can quantify the boom movement on\nthe current sprayer and potentially on any other sprayer with minor\nmodifications. The data can be used to make design improvements to make sprayer\nbooms more stable and achieve greater application accuracy.", "comment": "Under publication process for COMPAG", "pdf_url": "http://arxiv.org/pdf/2506.19939v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.19939v1", "AI": {"title_translation": "计算机视觉在农业喷雾器喷杆位移自动量化中的应用", "tldr": "本研究开发了一种基于计算机视觉的系统，用于自动量化农业喷雾器喷杆的位移，以提高施药精度。", "motivation": "农业喷雾器施药量误差是一个重要问题，其中喷杆不稳定是主要原因之一。目前缺乏对喷杆运动程度的量化知识，阻碍了喷杆设计和响应式控制系统的开发。", "method": "开发了一个计算机视觉系统，实时跟踪喷杆边缘的目标。训练了YOLO V7、V8和V11神经网络模型来量化喷杆在垂直和横向的有效位移。通过安装倾角传感器验证了神经网络模型的输出。", "result": "该模型检测目标的准确率超过90%，喷杆上目标的距离估计与倾角传感器数据误差在0.026米以内。该系统能够量化当前及其他喷雾器的喷杆运动。", "conclusion": "该系统能够量化当前及其他喷雾器的喷杆运动，其数据可用于改进喷杆设计，提高喷雾器稳定性，从而实现更高的施药精度。", "translation": "农业生产中使用自走式农业喷雾器时，施药量误差仍然是一个令人担忧的问题。在其他因素中，喷杆的不稳定性是导致施药误差的主要原因之一。喷杆宽度达38米，加上每小时30公里的行驶速度、多变的地形以及在复杂田地边界机动时的机器动力学，使得这些喷杆的控制变得非常复杂。然而，目前缺乏关于喷杆运动程度的定量知识，无法系统地开发可能包括喷杆设计和响应式喷杆控制系统在内的解决方案。因此，本研究旨在开发一个自动化的计算机视觉系统，以量化各种农业喷雾器的喷杆运动。开发了一个计算机视觉系统，用于实时跟踪喷雾器喷杆边缘的目标。训练了YOLO V7、V8和V11神经网络模型，以在田间作业中跟踪喷杆的运动，从而量化垂直和横向的有效位移。在喷杆上安装了一个倾角传感器，以捕获喷杆角度并验证神经网络模型的输出。结果表明，该模型检测目标的准确率超过90%，喷杆上目标的距离估计与倾角传感器数据误差在0.026米以内。该系统可以量化当前喷雾器以及通过少量修改后应用于任何其他喷雾器的喷杆运动。所获得的数据可用于改进喷杆设计，使喷雾器喷杆更稳定，并实现更高的施药精度。", "summary": "本文开发了一种基于计算机视觉的自动化系统，用于量化农业喷雾器喷杆的垂直和横向位移，以解决施药精度问题。该系统利用YOLO系列神经网络模型实时跟踪喷杆目标，并通过倾角传感器进行验证。实验结果表明，该系统具有高检测精度，并且能够准确估计喷杆位移，为改进喷杆设计和提高施药精度提供了定量数据支持。", "keywords": "计算机视觉, 农业喷雾器, 喷杆位移, YOLO, 施药精度", "comments": "该研究创新性地将计算机视觉技术应用于农业机械的运动量化，解决了喷杆不稳定性导致施药误差的实际问题。其提出的自动化系统为喷杆设计优化和智能控制提供了宝贵的量化数据，具有重要的工程应用价值。"}}
{"id": "2506.19882", "title": "Position: Machine Learning Conferences Should Establish a \"Refutations and Critiques\" Track", "authors": ["Rylan Schaeffer", "Joshua Kazdan", "Yegor Denisov-Blanch", "Brando Miranda", "Matthias Gerstgrasser", "Susan Zhang", "Andreas Haupt", "Isha Gupta", "Elyas Obbad", "Jesse Dodge", "Jessica Zosa Forde", "Koustuv Sinha", "Francesco Orabona", "Sanmi Koyejo", "David Donoho"], "summary": "Science progresses by iteratively advancing and correcting humanity's\nunderstanding of the world. In machine learning (ML) research, rapid\nadvancements have led to an explosion of publications, but have also led to\nmisleading, incorrect, flawed or perhaps even fraudulent studies being accepted\nand sometimes highlighted at ML conferences due to the fallibility of peer\nreview. While such mistakes are understandable, ML conferences do not offer\nrobust processes to help the field systematically correct when such errors are\nmade.This position paper argues that ML conferences should establish a\ndedicated \"Refutations and Critiques\" (R & C) Track. This R & C Track would\nprovide a high-profile, reputable platform to support vital research that\ncritically challenges prior research, thereby fostering a dynamic\nself-correcting research ecosystem. We discuss key considerations including\ntrack design, review principles, potential pitfalls, and provide an\nillustrative example submission concerning a recent ICLR 2025 Oral. We conclude\nthat ML conferences should create official, reputable mechanisms to help ML\nresearch self-correct.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19882v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19882v1", "AI": {"title_translation": "立场：机器学习会议应设立一个“驳斥与批判”专题", "tldr": "机器学习会议应设立“驳斥与批判”专题，以系统性地纠正错误研究。", "motivation": "机器学习研究快速发展，但同行评审的失误导致误导性、不正确或有缺陷的研究被接受。当前会议缺乏系统纠错机制。", "method": "提出机器学习会议应设立一个专门的“驳斥与批判”（R&C）专题，提供高知名度、有声誉的平台，支持批判性挑战先前研究的重要研究。", "result": "该专题将促进动态的自我纠正研究生态系统。论文讨论了专题设计、评审原则、潜在陷阱，并提供了一个示例提交。", "conclusion": "机器学习会议应创建官方、有声誉的机制来帮助机器学习研究自我纠正。", "translation": "科学通过迭代推进和纠正人类对世界的理解而进步。在机器学习（ML）研究中，快速进步导致了出版物数量的爆炸式增长，但也由于同行评审的错误，导致误导性、不正确、有缺陷甚至可能是欺诈性的研究被ML会议接受并有时被突出强调。尽管此类错误可以理解，但ML会议没有提供健全的流程来帮助该领域在出现此类错误时进行系统性纠正。这篇立场论文认为，ML会议应设立一个专门的“驳斥与批判”（R&C）专题。这个R&C专题将提供一个高知名度、有声誉的平台，以支持批判性挑战先前研究的重要研究，从而培养一个动态的自我纠正研究生态系统。我们讨论了关键考虑因素，包括专题设计、评审原则、潜在陷阱，并提供了一个关于近期ICLR 2025口头报告的说明性提交示例。我们总结认为，ML会议应创建官方、有声誉的机制来帮助ML研究自我纠正。", "summary": "本立场论文提出，鉴于机器学习研究中因同行评审失误导致错误研究的出现，机器学习会议应设立一个“驳斥与批判”专题。该专题旨在提供一个高知名度平台，鼓励并支持批判性研究，以促进机器学习领域的自我纠正，从而建立一个更健全、动态的研究生态系统。", "keywords": "机器学习会议, 驳斥与批判, 学术纠错, 同行评审, 研究生态系统", "comments": "这篇立场论文提出了一个非常及时且重要的建议。在机器学习领域快速发展且论文数量爆炸式增长的背景下，确保研究质量和可信度变得尤为关键。设立“驳斥与批判”专题能够有效弥补现有同行评审机制的不足，促进学术界对既有研究的审视和纠正，对于提升整个领域的科学严谨性具有重要意义。其创新之处在于提出一个结构化的机制来应对学术错误，而非仅仅依赖非正式讨论。"}}
{"id": "2506.19923", "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs", "authors": ["Kaito Baba", "Chaoran Liu", "Shuhei Kurita", "Akiyoshi Sannai"], "summary": "We present Prover Agent, a novel AI agent for automated theorem proving that\nintegrates large language models (LLMs) with a formal proof assistant, Lean.\nProver Agent coordinates an informal reasoning LLM, a formal prover model, and\nfeedback from Lean while also generating auxiliary lemmas to assist in\ndiscovering the overall proof strategy. It achieves an 86.1% success rate on\nthe MiniF2F benchmark, establishing a new state-of-the-art among methods using\nsmall language models (SLMs) with a much lower sample budget than previous\napproaches. We also present case studies illustrating how these generated\nlemmas contribute to solving challenging problems.", "comment": "22 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.19923v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.19923v1", "AI": {"title_translation": "Prover Agent：一个用于形式化数学证明的基于代理的框架", "tldr": "Prover Agent是一个结合LLM和Lean的形式化定理证明AI代理，通过协调非形式化推理LLM、形式化证明模型和Lean反馈，并生成辅助引理，在MiniF2F基准测试中取得了86.1%的成功率，创下了使用小型语言模型的SOTA。", "motivation": "该研究的动机是开发一种新型AI代理，用于自动化定理证明，以提高现有方法的效率和成功率，特别是在处理复杂数学证明时。", "method": "Prover Agent通过整合大型语言模型（LLMs）与形式化证明助手Lean来实现自动化定理证明。它协调一个非形式化推理LLM、一个形式化证明模型以及来自Lean的反馈，同时生成辅助引理以帮助发现整体证明策略。", "result": "Prover Agent在MiniF2F基准测试中取得了86.1%的成功率，这在使用小型语言模型（SLMs）的方法中达到了新的最先进水平（state-of-the-art），并且比之前的方法所需的样本预算低得多。论文还通过案例研究展示了生成引理如何有助于解决挑战性问题。", "conclusion": "Prover Agent通过结合LLM和形式化证明助手Lean，并引入辅助引理生成机制，显著提升了自动化定理证明的性能，并在MiniF2F基准测试中取得了SOTA成果，证明了其在解决复杂数学证明方面的有效性。", "translation": "我们提出了Prover Agent，这是一种新颖的AI代理，用于自动化定理证明，它将大型语言模型（LLMs）与形式化证明助手Lean集成在一起。Prover Agent协调一个非形式化推理LLM、一个形式化证明模型以及来自Lean的反馈，同时生成辅助引理以帮助发现整体证明策略。它在MiniF2F基准测试中取得了86.1%的成功率，在使用小型语言模型（SLMs）的方法中建立了新的最先进水平，并且比之前的方法所需的样本预算低得多。我们还提供了案例研究，说明这些生成的引理如何有助于解决挑战性问题。", "summary": "Prover Agent是一个创新的AI代理，旨在自动化形式化数学证明。它将大型语言模型与Lean形式化证明助手相结合，通过协调非形式化推理LLM、形式化证明模型和Lean反馈，并生成辅助引理来发现证明策略。该系统在MiniF2F基准测试中达到了86.1%的成功率，超越了现有使用小型语言模型的方法，且样本需求更少。案例研究进一步证明了其在解决复杂问题中的有效性。", "keywords": "自动化定理证明, 大型语言模型, 形式化证明, Lean, AI代理", "comments": "Prover Agent的创新之处在于其将LLM的非形式化推理能力与Lean的形式化验证能力相结合，并通过生成辅助引理来指导证明过程，这显著提升了自动化定理证明的效率和成功率。其在MiniF2F基准测试上取得的SOTA成果，特别是以更低的样本预算，显示了其在资源效率方面的优势和重要性。"}}
{"id": "2506.19870", "title": "Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability", "authors": ["Md Asif Ul Hoq Khan", "MD Zahedul Islam", "Istiaq Ahmed", "Md Masud Karim Rabbi", "Farhana Rahman Anonna", "MD Abdul Fahim Zeeshan", "Mehedi Hasan Ridoy", "Bivash Ranjan Chowdhury", "Md Nazmul Shakir Rabbi", "GM Alamin Sadnan"], "summary": "Peer-to-peer trading and the move to decentralized grids have reshaped the\nenergy markets in the United States. Notwithstanding, such developments lead to\nnew challenges, mainly regarding the safety and authenticity of energy trade.\nThis study aimed to develop and build a secure, intelligent, and efficient\nenergy transaction system for the decentralized US energy market. This research\ninterlinks the technological prowess of blockchain and artificial intelligence\n(AI) in a novel way to solve long-standing challenges in the distributed energy\nmarket, specifically those of security, fraudulent behavior detection, and\nmarket reliability. The dataset for this research is comprised of more than 1.2\nmillion anonymized energy transaction records from a simulated peer-to-peer\n(P2P) energy exchange network emulating real-life blockchain-based American\nmicrogrids, including those tested by LO3 Energy and Grid+ Labs. Each record\ncontains detailed fields of transaction identifier, timestamp, energy volume\n(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier\n(hashed for privacy), smart meter readings, geolocation regions, and settlement\nconfirmation status. The dataset also includes system-calculated behavior\nmetrics of transaction rate, variability of energy production, and historical\npricing patterns. The system architecture proposed involves the integration of\ntwo layers, namely a blockchain layer and artificial intelligence (AI) layer,\neach playing a unique but complementary function in energy transaction securing\nand market intelligence improvement. The machine learning models used in this\nresearch were specifically chosen for their established high performance in\nclassification tasks, specifically in the identification of energy transaction\nfraud in decentralized markets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19870v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19870v1", "AI": {"title_translation": "使用区块链结合AI进行欺诈检测和能源市场稳定性的安全能源交易", "tldr": "本研究旨在开发一个安全的去中心化能源交易系统，通过结合区块链和AI来检测欺诈并提高市场稳定性。", "motivation": "去中心化电网和P2P交易带来了能源交易安全和真实性方面的新挑战，因此需要一个安全、智能、高效的能源交易系统。", "method": "本研究结合了区块链和人工智能（AI）技术，构建了一个包含区块链层和AI层的系统架构。AI层使用高性能分类机器学习模型来识别能源交易欺诈。研究使用了超过120万条匿名化模拟P2P能源交易记录的数据集，该数据集模拟了基于区块链的美国微电网。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "点对点交易和向去中心化电网的转变重塑了美国的能源市场。尽管如此，这些发展也带来了新的挑战，主要是在能源交易的安全性和真实性方面。本研究旨在为去中心化的美国能源市场开发和构建一个安全、智能、高效的能源交易系统。本研究以一种新颖的方式将区块链和人工智能（AI）的技术优势相互结合，以解决分布式能源市场中长期存在的挑战，特别是安全性、欺诈行为检测和市场可靠性方面的挑战。本研究的数据集包含超过120万条匿名化能源交易记录，这些记录来自一个模拟真实区块链美国微电网（包括LO3 Energy和Grid+ Labs测试的微电网）的点对点（P2P）能源交换网络。每条记录都包含交易标识符、时间戳、能源量（kWh）、交易类型（买/卖）、单价、产消者/消费者标识符（为保护隐私而哈希化）、智能电表读数、地理位置区域和结算确认状态等详细字段。该数据集还包括系统计算的交易率、能源生产变异性和历史定价模式等行为指标。所提出的系统架构涉及两层的集成，即区块链层和人工智能（AI）层，每一层在能源交易安全和市场情报改进中都发挥着独特但互补的功能。本研究中使用的机器学习模型是专门选择的，因为它们在分类任务中表现出已建立的高性能，特别是在去中心化市场中识别能源交易欺诈方面。", "summary": "本研究针对美国去中心化能源市场中能源交易的安全性和真实性挑战，提出并开发了一个安全、智能、高效的能源交易系统。该系统创新性地结合了区块链技术和人工智能，旨在解决分布式能源市场中的安全、欺诈检测和市场稳定性问题。系统架构包含区块链层和AI层，其中AI层利用高性能机器学习模型处理来自模拟P2P能源交换网络的超过120万条匿名化交易数据，以识别欺诈行为并提升市场智能。", "keywords": "区块链, 人工智能, 能源交易, 欺诈检测, 去中心化电网", "comments": "该论文创新性地结合了区块链的去中心化和安全性与AI的智能分析能力，以解决能源交易中的欺诈和市场稳定性问题。其使用大规模模拟数据集进行系统开发和验证，显示了对实际应用场景的关注。这种跨领域的技术融合对于未来去中心化能源市场的安全和效率具有重要意义。"}}
{"id": "2506.20589", "title": "Communicating Smartly in Molecular Communication Environments: Neural Networks in the Internet of Bio-Nano Things", "authors": ["Jorge Torres Gómez", "Pit Hofmann", "Lisa Y. Debus", "Osman Tugay Başaran", "Sebastian Lotter", "Roya Khanzadeh", "Stefan Angerbauer", "Bige Deniz Unluturk", "Sergi Abadal", "Werner Haselmayr", "Frank H. P. Fitzek", "Robert Schober", "Falko Dressler"], "summary": "Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the\ngroundwork for innovative applications across the healthcare sector.\nNanodevices designed to operate within the body, managed remotely via the\ninternet, are envisioned to promptly detect and actuate on potential diseases.\nIn this vision, an inherent challenge arises due to the limited capabilities of\nindividual nanosensors; specifically, nanosensors must communicate with one\nanother to collaborate as a cluster. Aiming to research the boundaries of the\nclustering capabilities, this survey emphasizes data-driven communication\nstrategies in molecular communication (MC) channels as a means of linking\nnanosensors. Relying on the flexibility and robustness of machine learning (ML)\nmethods to tackle the dynamic nature of MC channels, the MC research community\nfrequently refers to neural network (NN) architectures. This interdisciplinary\nresearch field encompasses various aspects, including the use of NNs to\nfacilitate communication in MC environments, their implementation at the\nnanoscale, explainable approaches for NNs, and dataset generation for training.\nWithin this survey, we provide a comprehensive analysis of fundamental\nperspectives on recent trends in NN architectures for MC, the feasibility of\ntheir implementation at the nanoscale, applied explainable artificial\nintelligence (XAI) techniques, and the accessibility of datasets along with\nbest practices for their generation. Additionally, we offer open-source code\nrepositories that illustrate NN-based methods to support reproducible research\nfor key MC scenarios. Finally, we identify emerging research challenges, such\nas robust NN architectures, biologically integrated NN modules, and scalable\ntraining strategies.", "comment": "Paper submitted to IEEE Communications Surveys & Tutorials", "pdf_url": "http://arxiv.org/pdf/2506.20589v1", "categories": ["eess.SP", "cs.ET", "q-bio.OT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20589v1", "AI": {"title_translation": "分子通信环境中的智能通信：生物纳米物联网中的神经网络", "tldr": "本综述探讨了生物纳米物联网（IoBNT）中分子通信（MC）环境下，利用神经网络（NN）实现数据驱动智能通信的方法，并分析了其应用、挑战及未来方向。", "motivation": "生物纳米物联网（IoBNT）中，单个纳米传感器的能力有限，需要相互通信以实现集群协作。本研究旨在探索集群通信能力的边界，重点关注分子通信（MC）通道中的数据驱动通信策略，以解决这一固有挑战。", "method": "本论文是一项全面的综述，分析了分子通信（MC）中神经网络（NN）架构的最新趋势、其在纳米尺度的实现可行性、应用的可解释人工智能（XAI）技术以及数据集的可访问性和生成最佳实践。此外，它还提供了开源代码库以支持可复现研究。", "result": "本综述全面分析了分子通信中神经网络架构的最新趋势、其在纳米尺度的实现可行性、应用的可解释人工智能（XAI）技术以及数据集的可访问性及其生成最佳实践。此外，它还提供了开源代码库，并指出了新兴的研究挑战。", "conclusion": "本论文指出了生物纳米物联网分子通信领域中新兴的研究挑战，包括鲁棒的神经网络架构、生物集成神经网络模块和可扩展的训练策略，为未来的研究方向提供了指导。", "translation": "生物纳米物联网（IoBNT）的最新发展正在为医疗保健领域的创新应用奠定基础。设想中的纳米设备能够在体内运行，并通过互联网进行远程管理，以迅速检测和作用于潜在疾病。在这一愿景中，由于单个纳米传感器的能力有限，出现了一个固有的挑战；具体来说，纳米传感器必须相互通信才能作为一个集群进行协作。为了研究集群能力的边界，这项综述强调了分子通信（MC）通道中的数据驱动通信策略，作为连接纳米传感器的一种手段。MC研究社区依靠机器学习（ML）方法的灵活性和鲁棒性来应对MC通道的动态特性，因此经常提及神经网络（NN）架构。这个跨学科研究领域涵盖了各个方面，包括使用NN促进MC环境中的通信、它们在纳米尺度的实现、NN的可解释方法以及训练数据集的生成。在这项综述中，我们对MC中NN架构最新趋势的基本视角、它们在纳米尺度的实现可行性、应用的可解释人工智能（XAI）技术以及数据集的可访问性及其生成最佳实践进行了全面分析。此外，我们提供了开源代码库，以说明基于NN的方法，支持关键MC场景的可复现研究。最后，我们指出了新兴的研究挑战，例如鲁棒的NN架构、生物集成NN模块和可扩展的训练策略。", "summary": "本综述探讨了生物纳米物联网（IoBNT）中分子通信（MC）环境下，利用神经网络（NN）实现智能、数据驱动通信的方法。它通过使纳米传感器集群协作来解决单个传感器能力有限的挑战。论文全面分析了MC中NN架构、纳米尺度实现、可解释AI技术和数据集生成，并提供了开源工具和指出了未来的研究挑战。", "keywords": "分子通信, 神经网络, 生物纳米物联网, 数据驱动通信, 纳米传感器", "comments": "这项综述对于推动生物纳米物联网（IoBNT）领域的发展至关重要，因为它系统地回顾了神经网络如何实现纳米传感器之间的有效通信。其优势在于全面覆盖，包括纳米尺度实现和数据集生成等实用方面，并提供了开源资源。它还清楚地概述了未来的研究方向，使其成为研究人员的宝贵资源。"}}
{"id": "2506.19995", "title": "Refining Participatory Design for AAC Users", "authors": ["Blade Frisch", "Keith Vertanen"], "summary": "Augmentative and alternative communication (AAC) is a field of research and\npractice that works with people who have a communication disability. One form\nAAC can take is a high-tech tool, such as a software-based communication\nsystem. Like all user interfaces, these systems must be designed and it is\ncritical to include AAC users in the design process for their systems. A\nparticipatory design approach can include AAC users in the design process, but\nmodifications may be necessary to make these methods more accessible. We\npresent a two-part design process we are investigating for improving the\nparticipatory design for high-tech AAC systems. We discuss our plans to refine\nthe accessibility of this process based on participant feedback.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19995v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.19995v1", "AI": {"title_translation": "改进AAC用户参与式设计", "tldr": "本文探讨了改进高科技辅助和替代交流（AAC）系统参与式设计的方法，以使其对AAC用户更具包容性和可访问性。", "motivation": "辅助和替代交流（AAC）系统作为一种高科技工具，其用户界面的设计至关重要。将AAC用户纳入设计过程是关键，但现有的参与式设计方法可能需要修改以提高其可访问性。因此，本文旨在改进高科技AAC系统的参与式设计方法。", "method": "研究者正在探索一个两部分的参与式设计过程，并计划根据参与者的反馈来改进该过程的可访问性。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "辅助和替代交流（AAC）是一个研究和实践领域，致力于与有沟通障碍的人群合作。AAC的一种形式是高科技工具，例如基于软件的通信系统。与所有用户界面一样，这些系统必须经过设计，并且将AAC用户纳入其系统的设计过程至关重要。参与式设计方法可以将AAC用户纳入设计过程，但可能需要进行修改以使这些方法更具可访问性。我们提出了一个正在研究的两部分设计过程，旨在改进高科技AAC系统的参与式设计。我们将讨论根据参与者反馈改进该过程可访问性的计划。", "summary": "本文关注辅助和替代交流（AAC）系统的高科技设计，强调将AAC用户纳入设计过程的重要性。鉴于现有参与式设计方法可能对AAC用户不够可访问，研究提出并正在调查一个两部分的设计过程，旨在改进高科技AAC系统的参与式设计。未来的工作将根据用户反馈进一步优化该过程的可访问性。", "keywords": "AAC, 参与式设计, 可访问性, 用户界面, 沟通障碍", "comments": "本文的创新点在于将参与式设计方法应用于特殊用户群体——AAC用户，并认识到传统方法可能存在的局限性。其重要性在于，通过改进设计流程，可以开发出更符合AAC用户实际需求和偏好的高科技通信系统，从而显著提升他们的沟通能力和生活质量。研究的局限性在于，目前仅提出了设计过程的初步探索和计划，尚未展示具体的实施结果或有效性评估。"}}
{"id": "2506.20503", "title": "BotHash: Efficient and Training-Free Bot Detection Through Approximate Nearest Neighbor", "authors": ["Edoardo Di Paolo", "Fabio De Gaspari", "Angelo Spognardi"], "summary": "Online Social Networks (OSNs) are a cornerstone in modern society, serving as\nplatforms for diverse content consumption by millions of users each day.\nHowever, the challenge of ensuring the accuracy of information shared on these\nplatforms remains significant, especially with the widespread dissemination of\ndisinformation. Social bots -- automated accounts designed to mimic human\nbehavior, frequently spreading misinformation -- represent one of the critical\nproblems of OSNs. The advent of Large Language Models (LLMs) has further\ncomplicated bot behaviors, making detection increasingly difficult. This paper\npresents BotHash, an innovative, training-free approach to social bot\ndetection. BotHash leverages a simplified user representation that enables\napproximate nearest-neighbor search to detect bots, avoiding the complexities\nof Deep-Learning model training and large dataset creation. We demonstrate that\nBotHash effectively differentiates between human and bot accounts, even when\nstate-of-the-art LLMs are employed to generate posts' content. BotHash offers\nseveral advantages over existing methods, including its independence from a\ntraining phase, robust performance with minimal ground-truth data, and early\ndetection capabilities, showing promising results across various datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20503v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20503v1", "AI": {"title_translation": "BotHash：通过近似最近邻实现高效免训练的机器人检测", "tldr": "BotHash是一种免训练的社交机器人检测方法，利用近似最近邻搜索，即使面对LLM生成的内容也能有效区分人类和机器人账户。", "motivation": "在线社交网络中，社交机器人广泛传播虚假信息，且大型语言模型（LLMs）的出现使机器人行为更复杂，检测难度增加。", "method": "本文提出了BotHash，一种创新的免训练社交机器人检测方法。它利用简化的用户表示，通过近似最近邻搜索来检测机器人，避免了深度学习模型的训练和大型数据集的创建。", "result": "BotHash能有效区分人类账户和机器人账户，即使机器人使用最先进的LLM生成内容。它在各种数据集上显示出有前景的结果，且具有无需训练阶段、对少量真实数据表现鲁棒以及早期检测的能力。", "conclusion": "BotHash提供了一种高效、免训练的社交机器人检测方案，能够有效应对由LLM驱动的更复杂机器人行为，并具有显著的实用优势。", "translation": "在线社交网络（OSNs）是现代社会的基石，每天为数百万用户提供多样化的内容消费平台。然而，确保这些平台上信息准确性的挑战依然严峻，尤其是在虚假信息广泛传播的情况下。社交机器人——旨在模仿人类行为、经常传播虚假信息的自动化账户——是OSNs面临的关键问题之一。大型语言模型（LLMs）的出现进一步使机器人行为复杂化，使得检测变得越来越困难。本文提出了BotHash，一种创新、免训练的社交机器人检测方法。BotHash利用简化的用户表示，通过近似最近邻搜索来检测机器人，避免了深度学习模型训练和大型数据集创建的复杂性。我们证明了BotHash能够有效区分人类和机器人账户，即使在最先进的LLMs被用来生成帖子内容的情况下。BotHash相比现有方法具有多项优势，包括其独立于训练阶段、在最小真实数据下表现稳健以及早期检测能力，在各种数据集上显示出有前景的结果。", "summary": "本文介绍了BotHash，一种创新的、无需训练的社交机器人检测方法。该方法通过简化的用户表示和近似最近邻搜索来区分人类和机器人账户，有效避免了深度学习模型训练和大数据集的需求。实验证明，即使面对由大型语言模型生成内容的机器人，BotHash也能有效检测，并具有无需训练、对少量数据鲁棒和早期检测的优势。", "keywords": "社交机器人检测, 近似最近邻, 免训练, 大型语言模型, BotHash", "comments": "BotHash的创新之处在于其“免训练”的特性，这极大地降低了部署和维护的复杂性，尤其是在需要快速响应新威胁和数据标注成本高昂的场景下。其利用近似最近邻搜索的简单而有效的方法，以及在LLM生成内容下的鲁棒性，使其成为一个有前景的实用解决方案。"}}
{"id": "2506.19947", "title": "MILAAP: Mobile Link Allocation via Attention-based Prediction", "authors": ["Yung-Fu Chen", "Anish Arora"], "summary": "Channel hopping (CS) communication systems must adapt to interference changes\nin the wireless network and to node mobility for maintaining throughput\nefficiency. Optimal scheduling requires up-to-date network state information\n(i.e., of channel occupancy) to select non-overlapping channels for links in\ninterference regions. However, state sharing among nodes introduces significant\ncommunication overhead, especially as network size or node mobility scale,\nthereby decreasing throughput efficiency of already capacity-limited networks.\nIn this paper, we eschew state sharing while adapting the CS schedule based on\na learning-based channel occupancy prediction. We propose the MiLAAP\nattention-based prediction framework for machine learning models of spectral,\nspatial, and temporal dependencies among network nodes. MiLAAP uses a\nself-attention mechanism that lets each node capture the temporospectral CS\npattern in its interference region and accordingly predict the channel\noccupancy state within that region. Notably, the prediction relies only on\nlocally and passively observed channel activities, and thus introduces no\ncommunication overhead. To deal with node mobility, MiLAAP also uses a\nmulti-head self-attention mechanism that lets each node locally capture the\nspatiotemporal dependencies on other network nodes that can interfere with it\nand accordingly predict the motion trajectory of those nodes. Detecting nodes\nthat enter or move outside the interference region is used to further improve\nthe prediction accuracy of channel occupancy. We show that for dynamic networks\nthat use local CS sequences to support relatively long-lived flow traffics, the\nchannel state prediction accuracy of MiLAAP is remarkably ~100% across\ndifferent node mobility patterns and it achieves zero-shot generalizability\nacross different periods of CS sequences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19947v1", "categories": ["cs.NI", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.19947v1", "AI": {"title_translation": "MILAAP：基于注意力预测的移动链路分配", "tldr": "MILAAP通过基于本地观测的注意力预测来动态调整信道跳变调度，避免了状态共享开销，并在动态网络中实现了近乎100%的信道状态预测精度。", "motivation": "现有的信道跳变(CS)通信系统需要共享网络状态信息（如信道占用情况）来选择非重叠信道以优化调度，但状态共享会引入显著的通信开销，尤其是在网络规模和节点移动性增加时，从而降低吞吐量效率。", "method": "提出MiLAAP框架，一个基于注意力机制的预测框架。它使用自注意力机制，让每个节点捕获其干扰区域内的时频CS模式，并预测信道占用状态，仅依赖本地被动观察，无通信开销。为应对节点移动性，MiLAAP还使用多头自注意力机制，让每个节点捕获对其有干扰的节点的时空依赖性，并预测这些节点的运动轨迹，以进一步提高信道占用预测精度。", "result": "在动态网络中，MiLAAP的信道状态预测精度在不同节点移动模式下显著达到约100%，并且对不同CS序列周期实现了零样本泛化能力。", "conclusion": "MiLAAP通过本地化、基于注意力机制的预测，有效解决了动态无线网络中信道调度因状态共享导致的开销问题，并在高移动性环境下实现了卓越的信道状态预测精度和泛化能力。", "translation": "信道跳变（CS）通信系统必须适应无线网络中的干扰变化和节点移动性，以保持吞吐量效率。最优调度需要最新的网络状态信息（即信道占用情况），以便为干扰区域内的链路选择非重叠信道。然而，节点间的状态共享会引入显著的通信开销，特别是随着网络规模或节点移动性的扩大，从而降低了容量受限网络的吞吐量效率。\n在本文中，我们避免了状态共享，同时基于学习的信道占用预测来调整CS调度。我们提出了MiLAAP注意力预测框架，用于对网络节点间的频谱、空间和时间依赖性进行机器学习建模。MiLAAP使用自注意力机制，让每个节点捕获其干扰区域内的时频CS模式，并相应地预测该区域内的信道占用状态。值得注意的是，该预测仅依赖于本地被动观察到的信道活动，因此不引入通信开销。为了处理节点移动性，MiLAAP还使用多头自注意力机制，让每个节点在本地捕获可能对其产生干扰的其他网络节点的时空依赖性，并相应地预测这些节点的运动轨迹。检测进入或移出干扰区域的节点被用于进一步提高信道占用预测的准确性。我们表明，对于使用本地CS序列支持相对长寿命流流量的动态网络，MiLAAP的信道状态预测精度在不同节点移动模式下显著达到约100%，并且对不同CS序列周期实现了零样本泛化能力。", "summary": "本文提出了MiLAAP框架，旨在解决动态无线网络中信道跳变（CS）系统因状态共享导致的高通信开销问题。MiLAAP利用自注意力机制，使节点能基于本地被动观测预测信道占用状态，且不产生额外开销。为应对节点移动性，它进一步采用多头自注意力机制预测干扰节点的运动轨迹，从而提高预测精度。实验结果表明，MiLAAP在动态网络中能实现近100%的信道状态预测精度，并具有良好的零样本泛化能力。", "keywords": "信道跳变, 注意力机制, 无线网络, 信道预测, 移动性", "comments": "这篇论文通过引入基于本地观测的注意力预测机制，巧妙地避免了传统信道调度中状态共享带来的巨大通信开销，这是一个重要的创新点。特别是在高移动性网络环境中，其接近100%的预测精度和零样本泛化能力显示出该方法在实际应用中的巨大潜力。它为未来无线网络中高效、低开销的自适应调度提供了新思路。"}}
{"id": "2506.20054", "title": "On sharp stable recovery from clipped and folded measurements", "authors": ["Pedro Abdalla", "Daniel Freeman", "João P. G. Ramos", "Mitchell A. Taylor"], "summary": "We investigate the stability of vector recovery from random linear\nmeasurements which have been either clipped or folded. This is motivated by\napplications where measurement devices detect inputs outside of their effective\nrange.\n  As examples of our main results, we prove sharp lower bounds on the recovery\nconstant for both the declipping and unfolding problems whenever samples are\ntaken according to a uniform distribution on the sphere. Moreover, we show such\nestimates under (almost) the best possible conditions on both the number of\nsamples and the distribution of the data. We then prove that all of the above\nresults have suitable (effectively) sparse counterparts. In the special case\nthat one restricts the stability analysis to vectors which belong to the unit\nsphere of $\\mathbb{R}^n$, we show that the problem of declipping directly\nextends the one-bit compressed sensing results of Oymak-Recht and\nPlan-Vershynin.", "comment": "33 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20054v1", "categories": ["cs.IT", "math.IT", "math.MG", "math.PR"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20054v1", "AI": {"title_translation": "关于从截断和折叠测量中进行精确稳定恢复", "tldr": "本文研究了从截断或折叠的随机线性测量中恢复向量的稳定性，并证明了恢复常数的精确下界，同时扩展了相关的一比特压缩感知理论。", "motivation": "测量设备在检测超出其有效范围的输入时，需要从截断或折叠的测量中恢复向量。", "method": "论文通过证明当样本根据球体上的均匀分布采集时，去截断和展开问题的恢复常数的精确下界来研究稳定性。此外，还在（几乎）最佳的样本数量和数据分布条件下得到了这些估计，并证明了这些结果对稀疏情况也适用。", "result": "证明了当样本在球体上均匀分布时，去截断和展开问题的恢复常数的精确下界。这些估计在最佳样本数量和数据分布条件下仍然成立。结果对稀疏情况也有效。去截断问题直接扩展了Oymak-Recht和Plan-Vershynin的一比特压缩感知结果。", "conclusion": "论文成功地研究了从截断和折叠测量中稳定恢复向量的问题，提供了恢复常数的精确下界，并证明了其在稀疏情况下的适用性，同时扩展了现有的一比特压缩感知理论。", "translation": "我们研究了从经过截断或折叠的随机线性测量中恢复向量的稳定性。这受到测量设备检测超出其有效范围的输入的应用的启发。\n作为我们主要结果的例子，我们证明了当样本根据球体上的均匀分布采集时，去截断和展开问题的恢复常数的精确下界。此外，我们表明在样本数量和数据分布的最佳（几乎最佳）条件下，也能得到这些估计。然后我们证明了所有上述结果都具有合适的（实际上）稀疏对应物。在将稳定性分析限制在属于$\\mathbb{R}^n$单位球的向量的特殊情况下，我们表明去截断问题直接扩展了Oymak-Recht和Plan-Vershynin的一比特压缩感知结果。", "summary": "本文研究了从截断和折叠的随机线性测量中稳定恢复向量的问题。研究证明了当样本在单位球上均匀分布时，去截断和展开问题的恢复常数的精确下界，并在最佳样本量和数据分布条件下验证了这些估计。此外，研究还表明这些结果适用于稀疏情况，并且去截断问题可以扩展现有的一比特压缩感知理论。", "keywords": "截断测量, 折叠测量, 向量恢复, 稳定性, 压缩感知", "comments": "这篇论文在信号恢复领域具有重要意义，特别是在处理非线性测量（如截断和折叠）的应用中。它提供了严格的理论保证（精确下界），并考虑了实际情况中的稀疏性，同时与现有的一比特压缩感知理论建立了联系，这显示了其理论深度和潜在的应用价值。"}}
{"id": "2506.20014", "title": "Development of an Open-Source Spacecraft Bus for the PULSE-A CubeSat", "authors": ["Graydon Schulze-Kalt", "Robert Pitu", "Spencer Shelton", "Catherine Todd", "Zane Ebel", "Ian Goldberg", "Leon Gold", "Henry Czarnecki", "Mason McCormack", "Larry Li", "Zumi Riekse", "Brian Yu", "Akash Piya", "Vidya Suri", "Dylan Hu", "Colleen Kim", "John Baird", "Seth Knights", "Logan Hanssler", "Michael Lembeck", "Tian Zhong"], "summary": "The undergraduate-led Polarization-modUlated Laser Satellite Experiment\n(PULSE-A) at the University of Chicago seeks to demonstrate the feasibility of\ncircular polarization shift keyed satellite-to-ground laser communication.\nPULSE-A's low-cost open-source bus serves as the backbone of the mission and\nhas been designed in tandem with the Payload, with design driven by strict\nrequirements for pointing accuracy, component alignment, power demand, and\nthermal stability. This work presents the design and testing of the PULSE-A\nbus.\n  The spacecraft bus was designed to fill two major needs: (1) to meet the\nrequirements of the PULSE-A mission, and (2) to be easily configurable for\nfuture missions that desire enhanced capabilities over other low-cost\nopen-source designs. At its core, the bus features dual BeagleBone Black\nIndustrial compute units, selected for their flight heritage, integrated via a\nPC/104 header standard. PULSE-A implements Goddard Space Flight Center's core\nFlight System (cFS), which takes a modular software architecture approach and\nis built in C. The use of C as the primary language aligns with the expertise\nof the University of Chicago's Computer Science department, allowing for ease\nof development by PULSE-A's undergraduate flight software team.\n  The CubeSat structure utilizes Gran Systems' 3U frame, modified to\naccommodate openings for various ports and deployable components. Inside, the\navionics stack uses the PC/104 standard quad rails, which terminate in\nPULSE-A's custom-designed Payload Box that houses all of the Payload components\nand optical fiber runs. This work also covers the techniques and iterative\nengineering processes used to develop the thermal control and dissipation\nmechanisms for the specific requirements, under volume, mass, and\ntemperature-range constraints.", "comment": "Submitted to Advanced Technologies II at the 2025 SmallSat\n  Conference, reference number SSC25-P1-42", "pdf_url": "http://arxiv.org/pdf/2506.20014v1", "categories": ["physics.app-ph", "astro-ph.IM", "cs.AR", "cs.SY", "eess.SY", "physics.optics"], "cate": "physics.app-ph", "url": "http://arxiv.org/abs/2506.20014v1", "AI": {"title_translation": "PULSE-A立方星开源航天器总线开发", "tldr": "开发了一个开源、低成本的PULSE-A立方星总线，旨在支持激光通信演示并为未来任务提供可配置平台。", "motivation": "该研究旨在开发一个开源、低成本的航天器总线，以满足PULSE-A任务（演示圆偏振移键控卫星对地激光通信）的需求，并为未来需要增强功能的任务提供易于配置的平台。", "method": "开发的PULSE-A总线采用双BeagleBone Black Industrial计算单元，集成PC/104标准。软件基于戈达德空间飞行中心的cFS，采用模块化C语言架构。结构采用修改后的Gran Systems 3U框架，内部航空电子设备使用PC/104标准，并连接到定制的载荷箱。文中还涵盖了热控制和散热机制的迭代工程开发过程。", "result": "开发并测试了PULSE-A立方星的开源航天器总线。该总线满足了PULSE-A任务对指向精度、部件对齐、功耗和热稳定性的严格要求，并具备易于为未来任务配置的能力。它集成了飞行验证的计算单元和模块化软件系统，并采用了定制的结构和热管理解决方案。", "conclusion": "Not mentioned in abstract", "translation": "芝加哥大学本科生主导的偏振调制激光卫星实验（PULSE-A）旨在验证圆偏振移键控卫星对地激光通信的可行性。PULSE-A的低成本开源总线作为任务的骨干，与载荷协同设计，其设计受到指向精度、组件对齐、功耗和热稳定性等严格要求的驱动。这项工作介绍了PULSE-A总线的设计和测试。\n该航天器总线的设计旨在满足两个主要需求：(1) 满足PULSE-A任务的要求，(2) 易于为未来需要比其他低成本开源设计更强功能的任务进行配置。其核心是总线采用了双BeagleBone Black Industrial计算单元，因其飞行历史而被选中，并通过PC/104总线标准集成。PULSE-A实施了戈达德空间飞行中心的核心飞行系统（cFS），该系统采用模块化软件架构，并用C语言构建。使用C语言作为主要语言与芝加哥大学计算机科学系的专业知识相符，便于PULSE-A的本科飞行软件团队进行开发。\n立方星结构利用了Gran Systems的3U框架，并进行了修改以容纳各种端口和可展开组件的开口。内部，航空电子堆栈使用PC/104标准四轨，终止于PULSE-A定制设计的载荷箱，该载荷箱容纳了所有载荷组件和光纤线路。这项工作还涵盖了在体积、质量和温度范围限制下，用于开发热控制和散热机制的技术和迭代工程过程。", "summary": "本文介绍了为PULSE-A立方星开发的开源、低成本航天器总线的设计与测试。该总线旨在支持PULSE-A任务的激光通信演示，并具备为未来任务提供可配置增强功能的能力。设计严格遵循指向精度、热稳定性等要求，核心采用双BeagleBone Black计算单元和戈达德空间飞行中心的模块化cFS软件系统。结构基于修改的3U框架，并集成了定制的载荷箱与热管理方案。", "keywords": "开源航天器总线, 立方星, 激光通信, PULSE-A, cFS", "comments": "这篇论文展示了一个由本科生主导的低成本开源立方星总线项目，其创新性在于将开源硬件和模块化软件相结合，旨在实现可配置性和可复用性，这对于未来的小型卫星任务具有重要意义。然而，抽象中未提及具体的测试结果或性能数据，这可能会限制对其设计有效性的全面评估。"}}
{"id": "2506.19972", "title": "MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions", "authors": ["Federico Ruilova", "Ernst Gunnar Gran", "Sven-Arne Reinemo"], "summary": "Cloud computing drives innovation but also poses significant environmental\nchallenges due to its high-energy consumption and carbon emissions. Data\ncenters account for 2-4% of global energy usage, and the ICT sector's share of\nelectricity consumption is projected to reach 40% by 2040. As the goal of\nachieving net-zero emissions by 2050 becomes increasingly urgent, there is a\ngrowing need for more efficient and transparent solutions, particularly for\nprivate cloud infrastructures, which are utilized by 87% of organizations,\ndespite the dominance of public-cloud systems.\n  This study evaluates the MAIZX framework, designed to optimize cloud\noperations and reduce carbon footprint by dynamically ranking resources,\nincluding data centers, edge computing nodes, and multi-cloud environments,\nbased on real-time and forecasted carbon intensity, Power Usage Effectiveness\n(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX\nachieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor\noperations. Tested across geographically distributed data centers, the\nframework demonstrates scalability and effectiveness, directly interfacing with\nhypervisors to optimize workloads in private, hybrid, and multi-cloud\nenvironments. MAIZX integrates real-time data on carbon intensity, power\nconsumption, and carbon footprint, as well as forecasted values, into cloud\nmanagement, providing a robust tool for enhancing climate performance potential\nwhile maintaining operational efficiency.", "comment": "2 pages, 2 figures. LOCO 2024, December 3, 2024, Glasgow/Online", "pdf_url": "http://arxiv.org/pdf/2506.19972v1", "categories": ["cs.DC", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.19972v1", "AI": {"title_translation": "MAIZX：一个优化云计算排放的碳感知框架", "tldr": "MAIZX是一个碳感知框架，通过动态资源排名，将云计算的二氧化碳排放量减少了85.68%。", "motivation": "云计算的高能耗和碳排放造成了严重的环境挑战，数据中心占全球能源使用的2-4%，ICT部门的电力消耗预计到2040年将达到40%。鉴于2050年实现净零排放目标的紧迫性，尤其是在87%组织使用的私有云基础设施中，迫切需要更高效和透明的解决方案。", "method": "本研究评估了MAIZX框架，该框架通过基于实时和预测的碳强度、电力使用效率（PUE）和能耗，动态地对包括数据中心、边缘计算节点和多云环境在内的资源进行排名，以优化云操作并减少碳足迹。MAIZX利用灵活的排名算法，并直接与虚拟机管理程序接口，优化私有、混合和多云环境中的工作负载。", "result": "MAIZX框架与基线虚拟机管理程序操作相比，实现了85.68%的二氧化碳排放量减少。该框架在地理分布式数据中心进行了测试，证明了其可扩展性和有效性。", "conclusion": "MAIZX框架通过动态资源优化显著减少了云计算的碳排放，同时保持了运营效率，为提升气候性能潜力提供了一个强大的工具。", "translation": "云计算推动创新，但也因其高能耗和碳排放带来显著的环境挑战。数据中心占全球能源使用的2-4%，信息通信技术（ICT）部门的电力消耗预计到2040年将达到40%。随着到2050年实现净零排放的目标变得日益紧迫，对更高效和透明的解决方案的需求日益增长，特别是对于87%的组织使用的私有云基础设施，尽管公共云系统占据主导地位。\n本研究评估了MAIZX框架，该框架旨在通过根据实时和预测的碳强度、电力使用效率（PUE）和能耗动态排名资源（包括数据中心、边缘计算节点和多云环境），优化云操作并减少碳足迹。MAIZX利用灵活的排名算法，与基线虚拟机管理程序操作相比，二氧化碳排放量减少了85.68%。该框架在地理分布式数据中心进行了测试，证明了其可扩展性和有效性，直接与虚拟机管理程序接口，以优化私有、混合和多云环境中的工作负载。MAIZX将碳强度、功耗和碳足迹的实时数据以及预测值整合到云管理中，提供了一个强大的工具，可在保持运营效率的同时提高气候性能潜力。", "summary": "MAIZX是一个旨在优化云计算操作并减少碳排放的碳感知框架。它通过动态排名数据中心、边缘计算节点和多云环境等资源，依据实时和预测的碳强度、PUE和能耗。该框架利用灵活的算法，并直接与虚拟机管理程序接口，在私有、混合和多云环境中优化工作负载。实验结果显示，与基线操作相比，MAIZX实现了85.68%的二氧化碳排放量减少，并展现出良好的可扩展性和有效性。MAIZX为提升云计算的气候性能潜力提供了强大工具。", "keywords": "碳感知, 云计算, 排放优化, 数据中心, 碳足迹", "comments": "MAIZX框架的创新之处在于其将实时和预测的碳强度、PUE和能耗数据整合到动态资源排名中，从而实现显著的碳排放减少。其直接与虚拟机管理程序接口的能力，使其能够灵活应用于私有、混合和多云环境，这对于当前云计算环境的多样性至关重要。该研究通过量化85.68%的CO2减排，证明了其重要性，为实现净零排放目标提供了有力的技术支持。"}}
{"id": "2506.19875", "title": "Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers", "authors": ["Taous Iatariene", "Can Cui", "Alexandre Guérin", "Romain Serizel"], "summary": "Speaker tracking methods often rely on spatial observations to assign\ncoherent track identities over time. This raises limits in scenarios with\nintermittent and moving speakers, i.e., speakers that may change position when\nthey are inactive, thus leading to discontinuous spatial trajectories. This\npaper proposes to investigate the use of speaker embeddings, in a simple\nsolution to this issue. We propose to perform identity reassignment\npost-tracking, using speaker embeddings. We leverage trajectory-related\ninformation provided by an initial tracking step and multichannel audio signal.\nBeamforming is used to enhance the signal towards the speakers' positions in\norder to compute speaker embeddings. These are then used to assign new track\nidentities based on an enrollment pool. We evaluate the performance of the\nproposed speaker embedding-based identity reassignment method on a dataset\nwhere speakers change position during inactivity periods. Results show that it\nconsistently improves the identity assignment performance of neural and\nstandard tracking systems. In particular, we study the impact of beamforming\nand input duration for embedding extraction.", "comment": "33rd European Signal Processing Conference (EUSIPCO 2025), Sep 2025,\n  Palerme (Italie), Italy", "pdf_url": "http://arxiv.org/pdf/2506.19875v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.19875v1", "AI": {"title_translation": "说话人嵌入以改进间歇性和移动说话人的跟踪", "tldr": "本文提出了一种使用说话人嵌入在跟踪后重新分配身份的方法，以解决间歇性和移动说话人跟踪中的空间局限性问题，实验表明该方法能有效提高身份分配性能。", "motivation": "现有的说话人跟踪方法依赖空间观测，这在间歇性和移动说话人场景（说话人可能在不活动时改变位置，导致空间轨迹不连续）中存在局限性。", "method": "提出在跟踪后使用说话人嵌入进行身份重新分配。该方法利用初始跟踪步骤提供的轨迹相关信息和多通道音频信号。通过波束形成增强说话人位置处的信号以计算说话人嵌入，然后使用这些嵌入根据注册池分配新的跟踪身份。", "result": "提出的基于说话人嵌入的身份重新分配方法，在说话人在不活动期间改变位置的数据集上，持续改进了神经网络和标准跟踪系统的身份分配性能。研究还探讨了波束形成和嵌入提取输入持续时间的影响。", "conclusion": "使用说话人嵌入进行跟踪后身份重新分配是一种有效的方法，可以解决间歇性和移动说话人跟踪中由空间局限性引起的问题，显著提高身份识别准确性。", "translation": "说话人跟踪方法通常依赖空间观测来随时间分配连贯的跟踪身份。这在间歇性和移动说话人场景中存在局限性，即说话人可能在不活动时改变位置，从而导致空间轨迹不连续。本文提出研究使用说话人嵌入来简单解决这个问题。我们建议在跟踪后使用说话人嵌入进行身份重新分配。我们利用初始跟踪步骤提供的轨迹相关信息和多通道音频信号。波束形成用于增强说话人位置处的信号，以便计算说话人嵌入。然后，这些嵌入用于根据注册池分配新的跟踪身份。我们在说话人在不活动期间改变位置的数据集上评估了所提出的基于说话人嵌入的身份重新分配方法的性能。结果表明，它持续改进了神经网络和标准跟踪系统的身份分配性能。特别是，我们研究了波束形成和嵌入提取输入持续时间的影响。", "summary": "本文提出了一种利用说话人嵌入来解决间歇性和移动说话人跟踪中空间依赖性导致的问题。该方法在初始跟踪后，通过波束形成增强信号并计算说话人嵌入，然后基于这些嵌入从注册池中重新分配跟踪身份。实验结果表明，该方法显著提高了神经网络和标准跟踪系统的身份分配性能，尤其在说话人位置不固定时表现突出。", "keywords": "说话人跟踪, 说话人嵌入, 身份重新分配, 波束形成, 间歇性说话人", "comments": "这篇论文通过引入说话人嵌入来解决传统基于空间观测的说话人跟踪方法在面对间歇性和移动说话人时的局限性，具有重要的实际意义。其创新点在于将说话人嵌入应用于跟踪后的身份重新分配，并结合波束形成技术优化嵌入质量。该方法简单而有效，为复杂环境下的说话人跟踪提供了新的思路。"}}
{"id": "2506.20123", "title": "DiT-SGCR: Directed Temporal Structural Representation with Global-Cluster Awareness for Ethereum Malicious Account Detection", "authors": ["Ye Tian", "Liangliang Song", "Peng Qian", "Yanbin Wang", "Jianguo Sun", "Yifan Jia"], "summary": "The detection of malicious accounts on Ethereum - the preeminent DeFi\nplatform - is critical for protecting digital assets and maintaining trust in\ndecentralized finance. Recent advances highlight that temporal transaction\nevolution reveals more attack signatures than static graphs. However, current\nmethods either fail to model continuous transaction dynamics or incur high\ncomputational costs that limit scalability to large-scale transaction networks.\nFurthermore, current methods fail to consider two higher-order behavioral\nfingerprints: (1) direction in temporal transaction flows, which encodes money\nmovement trajectories, and (2) account clustering, which reveals coordinated\nbehavior of organized malicious collectives. To address these challenges, we\npropose DiT-SGCR, an unsupervised graph encoder for malicious account\ndetection. Specifically, DiT-SGCR employs directional temporal aggregation to\ncapture dynamic account interactions, then coupled with differentiable\nclustering and graph Laplacian regularization to generate high-quality,\nlow-dimensional embeddings. Our approach simultaneously encodes directional\ntemporal dynamics, global topology, and cluster-specific behavioral patterns,\nthereby enhancing the discriminability and robustness of account\nrepresentations. Furthermore, DiT-SGCR bypasses conventional graph propagation\nmechanisms, yielding significant scalability advantages. Extensive experiments\non three datasets demonstrate that DiT-SGCR consistently outperforms\nstate-of-the-art methods across all benchmarks, achieving F1-score improvements\nranging from 3.62% to 10.83%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20123v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20123v1", "AI": {"title_translation": "DiT-SGCR：面向以太坊恶意账户检测的全局集群感知定向时间结构表示", "tldr": "DiT-SGCR 是一种无监督图编码器，通过捕捉定向时间动态和集群行为，以可扩展的方式检测以太坊恶意账户，并显著优于现有方法。", "motivation": "以太坊上的恶意账户检测对于保护数字资产和维护去中心化金融的信任至关重要。现有方法未能有效建模连续交易动态或计算成本高，限制了可扩展性。此外，当前方法未能考虑时间交易流的方向性（资金移动轨迹）和账户聚类（协调行为）这两个更高阶的行为指纹。", "method": "提出 DiT-SGCR，一种无监督图编码器。该方法采用定向时间聚合来捕获动态账户交互，并结合可微分聚类和图拉普拉斯正则化以生成高质量、低维嵌入。DiT-SGCR 同时编码定向时间动态、全局拓扑和特定集群行为模式，并通过绕过传统图传播机制实现高可扩展性。", "result": "在三个数据集上的广泛实验表明，DiT-SGCR 在所有基准测试中始终优于最先进的方法，F1 分数提高了 3.62% 到 10.83%。", "conclusion": "DiT-SGCR 通过有效整合定向时间动态和全局集群感知，显著提高了以太坊恶意账户检测的准确性和可扩展性。", "translation": "以太坊（卓越的 DeFi 平台）上的恶意账户检测对于保护数字资产和维护去中心化金融的信任至关重要。最近的进展表明，时间交易演变比静态图揭示了更多的攻击特征。然而，当前方法要么未能建模连续的交易动态，要么产生高计算成本，限制了其在大规模交易网络中的可扩展性。此外，当前方法未能考虑两个更高阶的行为指纹：(1) 时间交易流中的方向性，它编码了资金移动轨迹；(2) 账户聚类，它揭示了有组织的恶意集体协调行为。为了解决这些挑战，我们提出了 DiT-SGCR，一种用于恶意账户检测的无监督图编码器。具体而言，DiT-SGCR 采用定向时间聚合来捕获动态账户交互，然后结合可微分聚类和图拉普拉斯正则化以生成高质量、低维嵌入。我们的方法同时编码了定向时间动态、全局拓扑和特定集群的行为模式，从而增强了账户表示的判别性和鲁棒性。此外，DiT-SGCR 绕过了传统的图传播机制，产生了显著的可扩展性优势。在三个数据集上进行的广泛实验表明，DiT-SGCR 在所有基准测试中始终优于最先进的方法，F1 分数提高了 3.62% 到 10.83%。", "summary": "本研究提出 DiT-SGCR，一个用于以太坊恶意账户检测的无监督图编码器。它通过定向时间聚合捕获动态账户交互，并结合可微分聚类和图拉普拉斯正则化，有效编码定向时间动态、全局拓扑和集群行为模式。DiT-SGCR 克服了现有方法在连续交易建模、计算成本和捕捉高阶行为指纹方面的不足，实现了高可扩展性。实验证明，DiT-SGCR 在多个数据集上显著优于现有最先进的方法，F1 分数有显著提升。", "keywords": "以太坊, 恶意账户检测, 图神经网络, 时间序列, 聚类", "comments": "DiT-SGCR 的创新之处在于其无监督的图编码方法，特别关注了交易流的方向性和账户的集群行为，这在现有研究中通常被忽视。此外，通过避免传统图传播机制，该方法在处理大规模以太坊交易网络时展现出卓越的可扩展性，这对于实际应用至关重要。其性能提升也验证了其方法的有效性。"}}
{"id": "2506.19984", "title": "Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots", "authors": ["Sahand Farghdani", "Mili Patel", "Robin Chhabra"], "summary": "Multi-legged robots (MLRs) are vulnerable to leg damage during complex\nmissions, which can impair their performance. This paper presents a\nself-modeling and damage identification algorithm that enables autonomous\nadaptation to partial or complete leg loss using only data from a low-cost IMU.\nA novel FFT-based filter is introduced to address time-inconsistent signals,\nimproving damage detection by comparing body orientation between the robot and\nits model. The proposed method identifies damaged legs and updates the robot's\nmodel for integration into its control system. Experiments on uneven terrain\nvalidate its robustness and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19984v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.19984v1", "AI": {"title_translation": "损坏多足机器人形态的鲁棒具身自识别", "tldr": "该论文提出一种算法，使多足机器人在腿部受损时，仅利用低成本IMU数据即可自主适应并识别损伤，通过FFT滤波器提高检测精度。", "motivation": "多足机器人在复杂任务中易受腿部损伤，从而影响其性能。", "method": "提出了一种自建模和损伤识别算法，该算法仅使用低成本IMU数据。引入了一种新颖的基于FFT的滤波器来处理时间不一致的信号，通过比较机器人及其模型之间的身体姿态来改进损伤检测。该方法识别受损腿部并更新机器人模型以集成到其控制系统。", "result": "在不平坦地形上的实验验证了其鲁棒性和计算效率。", "conclusion": "Not mentioned in abstract", "translation": "多足机器人（MLR）在复杂任务中容易受到腿部损伤，这会损害它们的性能。本文提出了一种自建模和损伤识别算法，该算法仅使用低成本IMU数据，即可实现对部分或完全腿部丢失的自主适应。引入了一种新颖的基于FFT的滤波器来处理时间不一致的信号，通过比较机器人及其模型之间的身体姿态来改进损伤检测。所提出的方法识别受损腿部并更新机器人模型，以便集成到其控制系统。在不平坦地形上的实验验证了其鲁棒性和计算效率。", "summary": "本文提出一种新颖的自建模和损伤识别算法，旨在帮助多足机器人在腿部受损时实现自主适应。该算法仅依赖低成本IMU数据，并引入FFT滤波器以提高损伤检测精度。通过识别受损腿部并更新机器人模型，该方法能有效集成到控制系统，并在不平坦地形实验中展现出良好的鲁棒性和计算效率。", "keywords": "多足机器人, 损伤识别, 自适应, IMU, FFT滤波器", "comments": "该论文提出了一种实用的损伤识别和自适应方法，仅利用低成本IMU数据，降低了实施成本。引入FFT滤波器解决时间不一致信号的问题是其创新点之一。该研究对于提高多足机器人在恶劣环境下的自主性和可靠性具有重要意义。"}}
{"id": "2506.20063", "title": "When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration", "authors": ["Zixuan Feng", "Thomas Zimmermann", "Lorenzo Pisani", "Christopher Gooley", "Jeremiah Wander", "Anita Sarma"], "summary": "Background: Software development teams are increasingly diverse, embedded,\nand cross-disciplinary. Domain experts (DEs) from different disciplines\ncollaborate with professional software developers (SDEs), bringing\ncomplementary expertise in creating and maintaining complex production\nsoftware. However, contested expectations, divergent problem-solving\nperspectives, and conflicting priorities lead to friction. Aims: This study\naims to investigate the dynamics of emerging collaboration of\ncross-disciplinary software development (CDSD) by exploring the expectations\nheld by DEs and SDEs and understanding how these frictions manifest in\npractice. Method: We utilize Activity Theory (AT), a well-established\nsocio-technical framework, as an analytical lens in a grounded, empirical\ninvestigation, conducted through a mixed-method study involving 24 interviews\n(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants\n(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the\nCDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By\nmapping these expectations to AT components, we revealed 21 frictions in CDSD\nand illustrated where and how they arise. Conclusions: This study offers a\ntheoretical lens for understanding the dynamics and frictions in CDSD and\nprovides actionable insights for future research, practitioners, and\ninfrastructure design.", "comment": "Cross-disciplinary Collaboration, Activity Theory, Mixed-Methods", "pdf_url": "http://arxiv.org/pdf/2506.20063v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20063v1", "AI": {"title_translation": "当领域碰撞时：一项关于跨学科协作的活动理论探索", "tldr": "本研究利用活动理论探索了跨学科软件开发中领域专家和软件开发人员之间的协作动态和摩擦，识别了期望并揭示了21种摩擦。", "motivation": "软件开发团队日益多样化、嵌入式和跨学科，不同领域的专家与专业软件开发人员协作，共同创建和维护复杂的生产软件。然而，相互冲突的期望、不同的解决问题视角和矛盾的优先事项导致了摩擦。本研究旨在调查跨学科软件开发（CDSD）中新兴协作的动态，探索领域专家（DEs）和软件开发人员（SDEs）的期望，并理解这些摩擦在实践中如何体现。", "method": "本研究采用活动理论（AT）作为分析视角，进行了一项扎根的实证研究，通过混合方法进行。具体包括24次访谈（12名领域专家和12名软件开发人员）以及一项包含293名参与者（161名领域专家和132名软件开发人员）的大规模验证调查。", "result": "研究概念化并经验性地证实了跨学科软件开发（CDSD）的动态。研究识别出软件开发人员的八种期望和领域专家的六种期望。通过将这些期望映射到活动理论的组成部分，研究揭示了CDSD中的21种摩擦，并阐明了它们发生的位置和方式。", "conclusion": "本研究为理解跨学科软件开发（CDSD）中的动态和摩擦提供了一个理论视角，并为未来的研究、实践者和基础设施设计提供了可操作的见解。", "translation": "背景：软件开发团队日益多样化、嵌入式和跨学科。来自不同领域的领域专家（DEs）与专业的软件开发人员（SDEs）协作，在创建和维护复杂的生产软件中带来互补的专业知识。然而，相互冲突的期望、不同的解决问题视角和矛盾的优先事项导致了摩擦。目的：本研究旨在通过探索领域专家和软件开发人员的期望，并理解这些摩擦在实践中如何体现，来调查跨学科软件开发（CDSD）中新兴协作的动态。方法：我们利用活动理论（AT）这一成熟的社会技术框架，作为一项扎根的实证研究的分析透镜，该研究通过混合方法进行，包括24次访谈（12名领域专家和12名软件开发人员）和一项包含293名参与者（161名领域专家和132名软件开发人员）的大规模验证调查。结果：我们概念化并经验性地证实了CDSD的动态。我们识别出软件开发人员的八种期望和领域专家的六种期望。通过将这些期望映射到活动理论的组成部分，我们揭示了CDSD中的21种摩擦，并阐明了它们发生的位置和方式。结论：本研究为理解CDSD中的动态和摩擦提供了一个理论视角，并为未来的研究、实践者和基础设施设计提供了可操作的见解。", "summary": "本研究探讨了跨学科软件开发（CDSD）中领域专家（DEs）与软件开发人员（SDEs）协作时出现的摩擦。研究利用活动理论（AT）作为分析框架，通过访谈和大规模调查的混合方法，识别了DEs和SDEs的期望，并揭示了21种由此产生的摩擦。研究为理解CDSD中的动态提供了理论视角，并为实践和未来研究提供了指导。", "keywords": "跨学科协作, 活动理论, 软件开发, 摩擦, 期望", "comments": "本研究的创新之处在于将活动理论应用于跨学科软件开发领域，深入分析了领域专家和软件开发人员之间的期望差异如何导致协作摩擦。其重要性在于通过实证研究揭示了具体的摩擦点，并为解决这些问题提供了理论框架和实践指导，对提升跨学科团队协作效率具有重要意义。"}}
{"id": "2506.19955", "title": "EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "summary": "Density map estimation has become the mainstream paradigm in crowd counting.\nHowever, most existing methods overlook the extreme sparsity of ground-truth\ndensity maps. In real-world crowd scenes, the vast majority of spatial regions\n(often over 95%) contain no people, leading to heavily imbalanced count\ndistributions. Ignoring this imbalance can bias models toward overestimating\ndense regions and underperforming in sparse areas. Furthermore, most loss\nfunctions used in density estimation are majorly based on MSE and implicitly\nassume Gaussian distributions, which are ill-suited for modeling discrete,\nnon-negative count data. In this paper, we propose EBC-ZIP, a crowd counting\nframework that models the spatial distribution of counts using a Zero-Inflated\nPoisson (ZIP) regression formulation. Our approach replaces the traditional\nregression loss with the negative log-likelihood of the ZIP distribution,\nenabling better handling of zero-heavy distributions while preserving count\naccuracy. Built upon the recently proposed Enhanced Block Classification (EBC)\nframework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of\ntargets and ensuring training stability, while further improving performance\nthrough a more principled probabilistic loss. We also evaluate EBC-ZIP with\nbackbones of varying computational complexity to assess its scalability.\nExtensive experiments on four crowd counting benchmarks demonstrate that\nEBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19955v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.19955v1", "AI": {"title_translation": "EBC-ZIP：利用零膨胀泊松回归改进块级人群计数", "tldr": "EBC-ZIP提出了一种基于零膨胀泊松回归的新型人群计数框架，以更好地处理密度图中的零值区域和离散计数数据，并在多个基准测试中达到了最先进的性能。", "motivation": "现有的人群计数方法在密度图估计中，忽视了真实密度图的极端稀疏性（大量区域无人），导致模型偏向于高估密集区域并低估稀疏区域。此外，常用的基于MSE的损失函数假设高斯分布，不适用于建模离散、非负的计数数据。", "method": "本文提出了EBC-ZIP框架，通过零膨胀泊松（ZIP）回归公式对计数数据的空间分布进行建模。它用ZIP分布的负对数似然替换了传统的回归损失，以更好地处理零值较多的分布，同时保持计数精度。该方法建立在增强块分类（EBC）框架之上，继承了EBC在保持目标离散性和训练稳定性方面的优势，并通过更原则性的概率损失进一步提高了性能。", "result": "EBC-ZIP在四个人群计数基准测试中进行了广泛的实验，结果表明它始终优于EBC，并取得了最先进的性能。同时，还评估了EBC-ZIP在不同计算复杂度的骨干网络下的可扩展性。", "conclusion": "EBC-ZIP通过引入零膨胀泊松回归和相应的负对数似然损失，有效解决了人群计数中密度图稀疏性和计数数据离散性带来的挑战，显著提升了计数精度，达到了最先进的水平。", "translation": "密度图估计已成为人群计数的主流范式。然而，大多数现有方法忽略了真实密度图的极端稀疏性。在真实世界的人群场景中，绝大多数空间区域（通常超过95%）没有人，导致计数分布严重不平衡。忽略这种不平衡可能会使模型偏向于高估密集区域并在稀疏区域表现不佳。此外，密度估计中使用的大多数损失函数主要基于MSE并隐式假设高斯分布，这不适合建模离散的、非负的计数数据。本文提出EBC-ZIP，一个人群计数框架，它使用零膨胀泊松（ZIP）回归公式对计数的空间分布进行建模。我们的方法用ZIP分布的负对数似然替换了传统的回归损失，从而更好地处理零值较多的分布，同时保持计数精度。EBC-ZIP建立在最近提出的增强块分类（EBC）框架之上，继承了EBC在保持目标离散性和确保训练稳定性方面的优势，同时通过更原则性的概率损失进一步提高了性能。我们还使用不同计算复杂度的骨干网络评估了EBC-ZIP，以评估其可扩展性。在四个主要人群计数基准测试中进行的大量实验表明，EBC-ZIP始终优于EBC并取得了最先进的成果。", "summary": "本文提出EBC-ZIP，一个用于人群计数的新框架，旨在解决现有密度图估计方法中存在的稀疏性问题和不适合离散计数数据的问题。EBC-ZIP采用零膨胀泊松（ZIP）回归来建模计数分布，并使用ZIP分布的负对数似然作为损失函数，以更好地处理大量零值区域。该方法基于增强块分类（EBC）框架，在保持离散性和训练稳定性的基础上，通过概率损失进一步提升性能。实验证明，EBC-ZIP在多个基准测试中表现优于EBC并达到了最先进的计数精度。", "keywords": "人群计数, 零膨胀泊松回归, 密度图估计, 稀疏性, 概率模型", "comments": "EBC-ZIP的创新点在于将零膨胀泊松回归引入人群计数领域，以更合理地处理密度图中普遍存在的零值区域和计数数据的离散性。这解决了传统基于MSE损失函数假设高斯分布不适用于计数数据的局限性。通过与EBC框架的结合，它在保持EBC优势的同时，进一步提升了性能，提供了一个更具原则性的概率模型。"}}
{"id": "2506.19883", "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning", "authors": ["Zhuqing Liu", "Chaosheng Dong", "Michinari Momma", "Simone Shao", "Shaoyuan Xu", "Yan Gao", "Haibo Yang", "Jia Liu"], "summary": "Recently, multi-objective optimization (MOO) has gained attention for its\nbroad applications in ML, operations research, and engineering. However, MOO\nalgorithm design remains in its infancy and many existing MOO methods suffer\nfrom unsatisfactory convergence rate and sample complexity performance. To\naddress this challenge, in this paper, we propose an algorithm called STIMULUS(\nstochastic path-integrated multi-gradient recursive e\\ulstimator), a new and\nrobust approach for solving MOO problems. Different from the traditional\nmethods, STIMULUS introduces a simple yet powerful recursive framework for\nupdating stochastic gradient estimates to improve convergence performance with\nlow sample complexity. In addition, we introduce an enhanced version of\nSTIMULUS, termed STIMULUS-M, which incorporates a momentum term to further\nexpedite convergence. We establish $O(1/T)$ convergence rates of the proposed\nmethods for non-convex settings and $O (\\exp{-\\mu T})$ for strongly convex\nsettings, where $T$ is the total number of iteration rounds. Additionally, we\nachieve the state-of-the-art $O \\left(n+\\sqrt{n}\\epsilon^{-1}\\right)$ sample\ncomplexities for non-convex settings and $O\\left(n+ \\sqrt{n} \\ln\n({\\mu/\\epsilon})\\right)$ for strongly convex settings, where $\\epsilon>0$ is a\ndesired stationarity error. Moreover, to alleviate the periodic full gradient\nevaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced\nversions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their\ntheoretical analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19883v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19883v1", "AI": {"title_translation": "STIMULUS：在随机多目标学习中实现快速收敛和低样本复杂度", "tldr": "STIMULUS是一种新的随机多目标优化算法，通过引入递归框架和动量项，在非凸和强凸设置下实现了快速收敛和低样本复杂度，达到了最先进的理论性能。", "motivation": "多目标优化（MOO）算法设计仍处于初级阶段，许多现有方法在收敛速度和样本复杂度方面表现不佳。", "method": "本文提出了STIMULUS（随机路径积分多梯度递归估计器）算法，它引入了一个简单而强大的递归框架来更新随机梯度估计，以提高收敛性能并降低样本复杂度。此外，还提出了增强版STIMULUS-M，它加入了动量项以进一步加速收敛。为了减轻周期性全梯度评估的需求，还提出了具有自适应批处理功能的STIMULUS+/STIMULUS-M+版本，并提供了理论分析。", "result": "所提出的方法在非凸设置下实现了$O(1/T)$的收敛速度，在强凸设置下实现了$O(\\exp{-\\mu T})$的收敛速度。在非凸设置下达到了最先进的$O(n+\\sqrt{n}\\epsilon^{-1})$样本复杂度，在强凸设置下达到了$O(n+\\sqrt{n} \\ln (\\mu/\\epsilon))$样本复杂度。", "conclusion": "STIMULUS及其增强版本有效解决了随机多目标优化中收敛速度慢和样本复杂度高的问题，并在理论上达到了最先进的性能。", "translation": "最近，多目标优化（MOO）因其在机器学习、运筹学和工程领域的广泛应用而受到关注。然而，MOO算法设计仍处于初级阶段，许多现有MOO方法存在收敛速度和样本复杂度性能不令人满意的问题。为了解决这一挑战，本文提出了一种名为STIMULUS（随机路径积分多梯度递归估计器）的算法，这是一种解决MOO问题的新颖且鲁棒的方法。与传统方法不同，STIMULUS引入了一个简单而强大的递归框架来更新随机梯度估计，以提高收敛性能并降低样本复杂度。此外，我们引入了STIMULUS的增强版本，称为STIMULUS-M，它结合了动量项以进一步加速收敛。我们为所提出的方法在非凸设置下建立了$O(1/T)$的收敛速度，在强凸设置下建立了$O(\\exp{-\\mu T})$的收敛速度，其中$T$是总迭代轮数。此外，我们在非凸设置下实现了最先进的$O(n+\\sqrt{n}\\epsilon^{-1})$样本复杂度，在强凸设置下实现了$O(n+ \\sqrt{n} \\ln (\\mu/\\epsilon))$样本复杂度，其中$\\epsilon>0$是期望的平稳误差。此外，为了缓解STIMULUS和STIMULUS-M中周期性全梯度评估的要求，我们进一步提出了具有自适应批处理功能的增强版本，称为STIMULUS+/STIMULUS-M+，并提供了它们的理论分析。", "summary": "本文针对现有随机多目标优化算法收敛速度慢和样本复杂度高的问题，提出了STIMULUS算法及其变体。STIMULUS引入了一个递归框架来更新随机梯度估计，以实现快速收敛和低样本复杂度。增强版本STIMULUS-M加入了动量项，而STIMULUS+/STIMULUS-M+则通过自适应批处理来减少全梯度评估。这些方法在非凸和强凸设置下均达到了最先进的收敛速度和样本复杂度。", "keywords": "多目标优化, 随机优化, 收敛速度, 样本复杂度, STIMULUS", "comments": "该论文引入了一种新颖的随机梯度估计递归框架，这是其主要创新点。通过引入动量项（STIMULUS-M）和自适应批处理（STIMULUS+/M+），该工作展示了全面提升实际性能的努力。在收敛速度和样本复杂度方面均达到最先进的理论界限，具有重要意义。"}}
{"id": "2506.19977", "title": "Context Attribution with Multi-Armed Bandit Optimization", "authors": ["Deng Pan", "Keerthiram Murugesan", "Nuno Moniz", "Nitesh Chawla"], "summary": "Understanding which parts of the retrieved context contribute to a large\nlanguage model's generated answer is essential for building interpretable and\ntrustworthy generative QA systems. We propose a novel framework that formulates\ncontext attribution as a combinatorial multi-armed bandit (CMAB) problem. Each\ncontext segment is treated as a bandit arm, and we employ Combinatorial\nThompson Sampling (CTS) to efficiently explore the exponentially large space of\ncontext subsets under a limited query budget. Our method defines a reward\nfunction based on normalized token likelihoods, capturing how well a subset of\nsegments supports the original model response. Unlike traditional\nperturbation-based attribution methods such as SHAP, which sample subsets\nuniformly and incur high computational costs, our approach adaptively balances\nexploration and exploitation by leveraging posterior estimates of segment\nrelevance. This leads to substantially improved query efficiency while\nmaintaining high attribution fidelity. Extensive experiments on diverse\ndatasets and LLMs demonstrate that our method achieves competitive attribution\nquality with fewer model queries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19977v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.19977v1", "AI": {"title_translation": "基于多臂老虎机优化的上下文归因", "tldr": "该论文提出了一种新的框架，将上下文归因建模为组合多臂老虎机（CMAB）问题，并使用组合汤普森采样（CTS）来高效识别大型语言模型（LLM）生成答案的贡献上下文片段。与传统方法相比，该方法显著提高了查询效率和归因准确性。", "motivation": "理解检索到的上下文的哪些部分对大型语言模型生成的答案有所贡献，对于构建可解释和可信赖的生成式问答系统至关重要。", "method": "该方法将上下文归因表述为组合多臂老虎机（CMAB）问题。每个上下文片段被视为一个老虎机臂，并采用组合汤普森采样（CTS）在有限的查询预算下高效探索指数级大的上下文子集空间。奖励函数基于归一化的令牌似然度定义，捕捉子集对原始模型响应的支持程度。与传统的基于扰动的归因方法不同，该方法通过利用片段相关性的后验估计，自适应地平衡探索和利用。", "result": "该方法在保持高归因保真度的同时，显著提高了查询效率。在各种数据集和大型语言模型上的大量实验表明，该方法以更少的模型查询实现了具有竞争力的归因质量。", "conclusion": "该论文提出的基于组合多臂老虎机和组合汤普森采样的上下文归因方法，能够高效且有效地识别LLM生成答案的贡献上下文片段，从而在减少查询次数的同时提高可解释性和可信度。", "translation": "理解检索到的上下文的哪些部分对大型语言模型生成的答案有所贡献，对于构建可解释和可信赖的生成式问答系统至关重要。我们提出了一种新颖的框架，将上下文归因表述为一个组合多臂老虎机（CMAB）问题。每个上下文片段被视为一个老虎机臂，我们采用组合汤普森采样（CTS）在有限的查询预算下高效探索指数级大的上下文子集空间。我们的方法定义了一个基于归一化令牌似然度的奖励函数，捕捉子集片段对原始模型响应的支持程度。与传统的基于扰动的归因方法（如SHAP）不同，后者均匀采样子集并产生高计算成本，我们的方法通过利用片段相关性的后验估计，自适应地平衡探索和利用。这显著提高了查询效率，同时保持了高归因保真度。在各种数据集和LLM上的大量实验表明，我们的方法以更少的模型查询实现了具有竞争力的归因质量。", "summary": "本文提出一种新颖的框架，将大型语言模型（LLM）的上下文归因问题建模为组合多臂老虎机（CMAB）问题。通过将每个上下文片段视为一个老虎机臂，并利用组合汤普森采样（CTS）高效探索上下文子集，该方法定义了一个基于令牌似然度的奖励函数来评估子集支持度。与传统的扰动方法不同，该方法自适应地平衡探索与利用，显著提高了查询效率和归因保真度，并在实验中展现出以更少查询达到竞争性归因质量的能力。", "keywords": "上下文归因, 多臂老虎机, 大型语言模型, 汤普森采样, 可解释性", "comments": "该论文创新性地将多臂老虎机优化应用于大型语言模型的上下文归因问题，通过将归因重构为CMAB问题并利用CTS，有效解决了传统扰动方法（如SHAP）计算成本高昂的限制。其自适应的探索与利用策略是关键优势，为构建更高效、可解释的生成式问答系统提供了有价值的解决方案。"}}
{"id": "2506.19871", "title": "An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network", "authors": ["Yining Pang", "Chenghan Li"], "summary": "Insurance fraud detection represents a pivotal advancement in modern\ninsurance service, providing intelligent and digitalized monitoring to enhance\nmanagement and prevent fraud. It is crucial for ensuring the security and\nefficiency of insurance systems. Although AI and machine learning algorithms\nhave demonstrated strong performance in detecting fraudulent claims, the\nabsence of standardized defense mechanisms renders current systems vulnerable\nto emerging adversarial threats. In this paper, we propose a GAN-based approach\nto conduct adversarial attacks on fraud detection systems. Our results indicate\nthat an attacker, without knowledge of the training data or internal model\ndetails, can generate fraudulent cases that are classified as legitimate with a\n99\\% attack success rate (ASR). By subtly modifying real insurance records and\nclaims, adversaries can significantly increase the fraud risk, potentially\nbypassing compromised detection systems. These findings underscore the urgent\nneed to enhance the robustness of insurance fraud detection models against\nadversarial manipulation, thereby ensuring the stability and reliability of\ndifferent insurance systems.", "comment": "arXiv admin note: text overlap with arXiv:2405.12076 by other authors", "pdf_url": "http://arxiv.org/pdf/2506.19871v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19871v1", "AI": {"title_translation": "基于生成对抗网络的医疗保险索赔欺诈检测攻击方法", "tldr": "研究提出一种基于GAN的攻击方法，能以99%的成功率使欺诈性医疗保险索赔被误判为合法。", "motivation": "尽管AI在欺诈检测中表现出色，但缺乏标准化防御机制使得现有系统易受新兴对抗性威胁的影响，这促使作者研究如何提高保险欺诈检测模型的鲁棒性。", "method": "本文提出了一种基于生成对抗网络（GAN）的方法，用于对欺诈检测系统进行对抗性攻击。攻击者无需了解训练数据或内部模型细节，通过微妙地修改真实的保险记录和索赔来生成对抗样本。", "result": "攻击者在不了解训练数据或内部模型细节的情况下，可以生成被分类为合法的欺诈案例，攻击成功率（ASR）达到99%。这显著增加了欺诈风险，可能绕过受损的检测系统。", "conclusion": "研究结果强调了迫切需要增强保险欺诈检测模型对抗对抗性操纵的鲁棒性，以确保不同保险系统的稳定性和可靠性。", "translation": "保险欺诈检测代表了现代保险服务的一个关键进步，提供智能化和数字化监控以加强管理和预防欺诈。它对于确保保险系统的安全和效率至关重要。尽管人工智能和机器学习算法在检测欺诈性索赔方面表现出强大的性能，但缺乏标准化的防御机制使得现有系统容易受到新兴对抗性威胁的影响。在本文中，我们提出了一种基于生成对抗网络（GAN）的方法，对欺诈检测系统进行对抗性攻击。我们的结果表明，攻击者在不了解训练数据或内部模型细节的情况下，可以生成被分类为合法的欺诈案例，攻击成功率（ASR）达到99%。通过巧妙地修改真实的保险记录和索赔，对手可以显著增加欺诈风险，可能绕过受损的检测系统。这些发现强调了迫切需要增强保险欺诈检测模型对抗对抗性操纵的鲁棒性，从而确保不同保险系统的稳定性和可靠性。", "summary": "本文提出了一种基于生成对抗网络（GAN）的攻击方法，旨在评估现有医疗保险欺诈检测系统的脆弱性。研究发现，即使不了解模型的内部结构或训练数据，攻击者也能以高达99%的成功率生成能被误判为合法索赔的欺诈性案例。这表明当前基于AI的欺诈检测系统在对抗性攻击面前存在严重漏洞，凸显了提升模型鲁棒性的紧迫性。", "keywords": "生成对抗网络, 保险欺诈检测, 对抗性攻击, 医疗保险, 鲁棒性", "comments": "该研究创新性地将GAN应用于保险欺诈检测的对抗性攻击，揭示了现有AI欺诈检测系统的潜在漏洞。其重要性在于提醒业界需关注模型鲁棒性，并为开发更安全的防御机制提供了实验依据。此工作为未来对抗性机器学习在金融领域的防御研究提供了宝贵见解。"}}
{"id": "2506.20055", "title": "\"I'm Petting the Laptop, Which Has You Inside It\": Reflecting on Lived Experiences of Online Friendship", "authors": ["Seraphina Yong", "Ashlee Milton", "Evan Suma Rosenberg", "Stevie Chancellor", "Svetlana Yarosh"], "summary": "Online(-only) friendships have become increasingly common in daily lives\npost-COVID despite debates around their mental health benefits and equivalence\nto ''real'' relationships. Previous research has reflected a need to understand\nhow online friends engage beyond individual platforms, and the lack of\nplatform-agnostic inquiry limits our ability to fully understand the dynamics\nof online friendship. We employed an activity-grounded analysis of 25\ninterviews on lived experiences of close online friendship spanning multiple\nyears. Our findings present unique challenges and strategies in online\nfriendships, such as stigma from real-life circles, an ambivalent relationship\nwith online communities, and counter-theoretical reappropriations of\ncommunication technology. This study contributes to HCI research in online\ncommunities and social interface design by refocusing prior impressions of\nstrong vs. weak-ties in online social spaces and foregrounding time-stable\ninteractions in design for relationship maintenance through technology. Our\nwork also promotes critical reflection on biased perspectives towards\ntechnology-mediated practices and consideration of online friends as an\ninvisible marginalized community.", "comment": "CSCW 2025, 29 article pages, 5 pages of references, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.20055v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20055v1", "AI": {"title_translation": "“我正在抚摸笔记本电脑，你就在里面”: 反思在线友谊的生活经验", "tldr": "本研究通过访谈深入探讨了在线友谊的挑战、策略及对偏见的反思，强调了其作为一种独特且被忽视的社交形式的价值。", "motivation": "尽管在线友谊日益普遍，但其心理健康益处和与“真实”关系的等同性仍存在争议。以往研究未能超越单一平台理解在线朋友的互动方式，缺乏平台无关的探究限制了对在线友谊动态的全面理解。", "method": "采用基于活动的分析方法，对25名参与者进行了访谈，探讨了他们多年来亲密在线友谊的生活经验。", "result": "发现了在线友谊中独特的挑战和策略，包括来自现实生活圈子的污名、与在线社区的矛盾关系，以及对通信技术的反理论重新利用。", "conclusion": "本研究通过重新聚焦在线社交空间中强弱关系的前期印象，并在技术关系维护设计中强调时间稳定的互动，为在线社区和社交界面设计中的人机交互研究做出了贡献。同时，它也促进了对技术介导实践中偏见视角的批判性反思，并将在线朋友视为一个隐形的边缘化群体。", "translation": "在线（仅限在线）友谊在新冠疫情后日益普及，尽管关于其心理健康益处以及与“真实”关系的等同性存在争议。以往的研究表明，需要理解在线朋友如何超越单一平台进行互动，而缺乏与平台无关的探究限制了我们全面理解在线友谊动态的能力。我们对25次关于多年来亲密在线友谊生活经验的访谈进行了基于活动的分析。我们的发现揭示了在线友谊中独特的挑战和策略，例如来自现实生活圈子的污名、与在线社区的矛盾关系，以及对通信技术的反理论重新利用。本研究通过重新聚焦在线社交空间中强弱关系的前期印象，并在技术关系维护设计中强调时间稳定的互动，为在线社区和社交界面设计中的人机交互研究做出了贡献。我们的工作还促进了对技术介导实践中偏见视角的批判性反思，并将在线朋友视为一个隐形的边缘化社区。", "summary": "本研究通过对25名参与者进行访谈并进行活动导向分析，深入探讨了在线友谊的真实生活经验。研究发现，在线友谊面临来自现实世界的污名、与在线社区的复杂关系以及对技术的新颖使用等独特挑战与策略。该研究旨在重新审视在线社交关系中的强弱联系，强调技术在维护长期在线友谊中的作用，并呼吁社会对在线朋友这一隐形边缘化群体给予更多关注和批判性反思。", "keywords": "在线友谊, 人机交互, 社会界面设计, 污名, 关系维护", "comments": "这篇论文通过关注在线友谊这一日益普遍但常被误解的现象，展现了其创新性。它不仅超越了传统平台中心的研究范式，还深入探讨了在线友谊的社会接受度、内在复杂性以及用户如何重新定义技术用途。研究结果对于人机交互设计、社会心理学以及批判性技术研究具有重要意义，尤其是在呼吁将在线朋友视为一个需要关注的边缘化群体方面，具有重要的社会价值。"}}
{"id": "2506.19974", "title": "Notes on Degeneracy and Robustness", "authors": ["Indrakshi Dey", "Nicola Marchetti"], "summary": "Degeneracy is the ability of structurally different elements to perform the\nsame function or yield the same output under certain constraints. In contrast\nto redundancy, which implies identical backups, degeneracy allows diverse\ncomponents to step in and perform the same or similar role. Mathematically, it\nis about mapping multiple distinct elements into the same function. In a\ndegenerate system, failure in one part can be compensated by others not\nstructurally linked. System functions are distributed within the system itself\nor the entire network. This renders faster and more adaptive recovery. In this\nwork, we define and formulate several novel metrics for resource fungibility to\naddress robustness in networks (static/mobile/dynamic).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19974v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.19974v1", "AI": {"title_translation": "关于退化性和鲁棒性的笔记", "tldr": "退化性（结构不同但功能相同的元素）通过允许故障补偿来增强鲁棒性，不同于冗余。本文定义了网络中资源可替代性的新度量标准。", "motivation": "本研究的动机是理解并量化退化性如何通过允许多样化组件补偿故障来增强系统（特别是网络）的鲁棒性。", "method": "通过定义和制定几种新颖的资源可替代性度量标准，以解决静态、移动和动态网络中的鲁棒性问题。", "result": "摘要中未提及。", "conclusion": "退化性是实现鲁棒和自适应系统的关键机制，这项工作提供了新的工具（度量标准）来分析其在网络鲁棒性中的作用。", "translation": "退化性是指结构上不同的元素在特定约束下能够执行相同功能或产生相同输出的能力。与意味着相同备份的冗余不同，退化性允许多样化的组件介入并执行相同或相似的角色。从数学上讲，它涉及将多个不同的元素映射到同一个函数。在一个退化系统中，一部分的故障可以由其他结构上不相关的部分来补偿。系统功能分布在系统本身或整个网络中。这使得恢复更快、更具适应性。在这项工作中，我们定义并制定了几种新颖的资源可替代性度量标准，以解决网络（静态/移动/动态）中的鲁棒性问题。", "summary": "本文探讨了退化性，它不同于冗余，是一种结构多样但功能相同的元素协同工作以增强系统弹性的机制。它强调了退化性如何通过非结构关联组件对故障进行补偿，从而实现功能分布，促进更快、更具适应性的恢复。本研究引入了新颖的资源可替代性度量标准，用于分析各种网络类型中的鲁棒性。", "keywords": "退化性, 鲁棒性, 网络, 资源可替代性, 系统弹性", "comments": "本文关注一个基本概念（退化性），这对于理解超越简单冗余的系统弹性至关重要。引入资源可替代性的新颖度量标准，表明其正在向对这种复杂现象进行更定量的分析迈进，这可能对设计鲁棒系统和网络具有重要意义。"}}
{"id": "2506.20158", "title": "Efficient Channel Estimation for Rotatable Antenna-Enabled Wireless Communication", "authors": ["Xue Xiong", "Beixiong Zheng", "Wen Wu", "Xiaodan Shao", "Liang Dai", "Ming-Min Zhao", "Jie Tang"], "summary": "Rotatable antenna (RA) is a promising antenna architecture that exploits\nadditional spatial degrees of freedom (DoFs) to enhance the communication\nperformance. To fully obtain the performance gain provided by RAs, accurate\nchannel state information (CSI) is essential for adjusting the\norientation/boresight of each antenna. In this letter, we propose an efficient\nchannel estimation scheme for RA communication systems, where the base station\n(BS) can sequentially and adaptively adjust the orientations of RAs to enrich\nthe environmental observations from diverse angular perspectives, thereby\nenhancing the channel estimation accuracy. The proposed scheme includes two\nmain procedures that are conducted alternately during each channel training\nperiod. Specifically, the first procedure is to estimate the CSI with given\nRAs' orientations, involving the angle-of-arrivals (AoAs) information and path\ngains. Then, based on the estimated CSI, the second procedure adjusts the RAs'\norientations to maximize the effective channel gain. Simulation results\ndemonstrate that the proposed channel estimation method outperforms other\nbenchmark schemes.", "comment": "5 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.20158v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20158v1", "AI": {"title_translation": "可旋转天线无线通信中的高效信道估计", "tldr": "本文提出一种高效的信道估计方案，通过基站自适应调整可旋转天线的方向来提高信道估计精度，仿真结果表明其性能优于其他基准方案。", "motivation": "可旋转天线（RA）是一种有前景的天线架构，可利用额外的空间自由度来增强通信性能。为了充分获得RA带来的性能增益，准确的信道状态信息（CSI）对于调整每个天线的方向至关重要。", "method": "提出一种用于RA通信系统的高效信道估计方案。该方案包括两个在信道训练期间交替进行的主要程序：首先在给定RA方向下估计CSI，包括到达角（AoAs）信息和路径增益；然后，基于估计的CSI，调整RA方向以最大化有效信道增益。", "result": "仿真结果表明，所提出的信道估计方法优于其他基准方案。", "conclusion": "Not mentioned in abstract", "translation": "可旋转天线（RA）是一种很有前景的天线架构，它利用额外的空间自由度（DoFs）来增强通信性能。为了充分获得RA提供的性能增益，准确的信道状态信息（CSI）对于调整每个天线的方向/主轴至关重要。在这封信中，我们提出了一种用于RA通信系统的高效信道估计方案，其中基站（BS）可以顺序自适应地调整RA的方向，以丰富来自不同角度的环境观测，从而提高信道估计精度。所提出的方案包括两个主要程序，在每个信道训练期间交替进行。具体来说，第一个程序是在给定RA方向下估计CSI，包括到达角（AoAs）信息和路径增益。然后，基于估计的CSI，第二个程序调整RA的方向以最大化有效信道增益。仿真结果表明，所提出的信道估计方法优于其他基准方案。", "summary": "本文提出了一种用于可旋转天线（RA）通信系统的高效信道估计方案，旨在通过基站自适应调整RA方向来利用额外的空间自由度，从而提高信道估计精度。该方案包含两个交替进行的步骤：首先在给定RA方向下估计信道状态信息（CSI），然后根据估计的CSI调整RA方向以最大化有效信道增益。仿真结果验证了该方法优于现有基准方案。", "keywords": "可旋转天线, 信道估计, 空间自由度, 自适应调整, 信道状态信息", "comments": "这篇论文提出了一种新颖的信道估计方法，通过动态调整可旋转天线的方向来主动获取更多环境信息，从而提高信道估计的准确性。这种自适应调整天线方向的策略是其创新点，有望在实际RA系统中提升通信性能。"}}
{"id": "2506.20018", "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models", "authors": ["Zechun Deng", "Ziwei Liu", "Ziqian Bi", "Junhao Song", "Chia Xin Liang", "Joe Yeong", "Junfeng Hao"], "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20018v1", "categories": ["cs.AI", "cs.AR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20018v1", "AI": {"title_translation": "利用低延迟可解释AI模型实现可信实时决策支持系统", "tldr": "本文探讨了如何利用低延迟可解释AI模型，结合边缘-物联网技术和人机协作，构建可信的实时决策支持系统，并展望了未来发展。", "motivation": "本研究旨在探索如何利用低延迟AI模型，结合最新的人工智能决策工具、边缘-物联网技术以及有效的人机协作方法，构建可信的实时决策支持系统，尤其是在资源受限的环境下。", "method": "本文通过详细的综述，审视了大型语言模型在决策支持中的应用、DeLLMa等技术进展、模型压缩方法以及边缘设备上的分析改进，并讨论了资源限制和对适应性框架的需求。", "result": "本文提供了关于开发策略和应用领域的实用视角，并指出了实现更高效、更灵活的AI支持系统的新机会。", "conclusion": "研究结论为该快速变化领域的未来突破奠定了基础，强调了人工智能如何重塑实时决策支持。", "translation": "本文研究了利用低延迟人工智能模型实现实时决策支持系统，汇集了人工智能驱动决策工具、与边缘-物联网技术集成以及有效人机协作方法的最新进展。它探讨了大型语言模型如何协助决策，尤其是在资源有限的情况下。该研究还考察了DeLLMa等技术发展、模型压缩方法以及边缘设备分析改进的影响，同时解决了资源有限和对适应性框架需求等问题。通过详细的综述，本文提供了关于开发策略和应用领域的实用视角，为该领域做出了贡献，指出了实现更高效、更灵活的AI支持系统的机会。结论为这个快速变化的领域的未来突破奠定了基础，强调了人工智能如何重塑实时决策支持。", "summary": "本文综述了利用低延迟可解释AI模型构建可信实时决策支持系统的方法。研究整合了AI驱动决策工具、边缘-物联网技术和人机协作的最新进展，探讨了大型语言模型在资源受限环境下的决策辅助作用，并审视了包括DeLLMa、模型压缩和边缘分析改进等技术发展。文章提供了实用的开发策略和应用领域视角，旨在促进更高效、灵活的AI支持系统，并展望了AI在实时决策支持领域的未来潜力。", "keywords": "实时决策支持, 低延迟AI, 可解释AI, 边缘-物联网, 大型语言模型", "comments": "该论文的创新点在于将低延迟可解释AI模型与实时决策支持系统相结合，并融入了边缘-物联网技术和人机协作的理念。它还特别关注了大型语言模型在资源受限环境下的应用，并探讨了DeLLMa等前沿技术对模型效率和可解释性的影响。重要性体现在为构建可信、高效的AI驱动决策系统提供了实践指导和未来方向。"}}
{"id": "2506.20218", "title": "On the $h$-majority dynamics with many opinions", "authors": ["Francesco d'Amore", "Niccolò D'Archivio", "George Giakkoupis", "Emanuele Natale"], "summary": "We present the first upper bound on the convergence time to consensus of the\nwell-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,\nfor $h$ and $k$ that are both non-constant values.\n  We suppose that, at the beginning of the process, there is some initial\nadditive bias towards some plurality opinion, that is, there is an opinion that\nis supported by $x$ nodes while any other opinion is supported by strictly\nfewer nodes.\n  We prove that, with high probability, if the bias is $\\omega(\\sqrt{x})$ and\nthe initial plurality opinion is supported by at least $x = \\omega(\\log n)$\nnodes, then the process converges to plurality consensus in $O(\\log n)$ rounds\nwhenever $h = \\omega(n \\log n / x)$.\n  A main corollary is the following: if $k = o(n / \\log n)$ and the process\nstarts from an almost-balanced configuration with an initial bias of magnitude\n$\\omega(\\sqrt{n/k})$ towards the initial plurality opinion, then any function\n$h = \\omega(k \\log n)$ suffices to guarantee convergence to consensus in\n$O(\\log n)$ rounds, with high probability.\n  Our upper bound shows that the lower bound of $\\Omega(k / h^2)$ rounds to\nreach consensus given by Becchetti et al.\\ (2017) cannot be pushed further than\n$\\widetilde{\\Omega}(k / h)$.\n  Moreover, the bias we require is asymptotically smaller than the\n$\\Omega(\\sqrt{n\\log n})$ bias that guarantees plurality consensus in the\n$3$-majority dynamics: in our case, the required bias is at most any\n(arbitrarily small) function in $\\omega(\\sqrt{x})$ for any value of $k \\ge 2$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20218v1", "categories": ["cs.DC", "cs.MA"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20218v1", "AI": {"title_translation": "关于多意见的$h$-多数动力学", "tldr": "论文首次给出了多意见$h$-多数动力学收敛时间的上限，在特定初始偏差下，显示出更快的收敛速度和更小的所需偏差。", "motivation": "首次为非恒定$h$和$k$值下的$h$-多数动力学收敛时间提供了上限。", "method": "通过理论分析和数学证明，在同步设置下推导了$h$-多数动力学收敛时间的上限。", "result": "在特定初始偏差（$\\\\omega(\\\\sqrt{x})$，其中$x=\\\\omega(\\\\log n)$）下，进程以$O(\\\\log n)$轮收敛到多数共识，条件是$h = \\\\omega(n \\\\log n / x)$。如果$k = o(n / \\\\log n)$且初始偏差为$\\\\omega(\\\\sqrt{n/k})$，那么$h = \\\\omega(k \\\\log n)$足以保证在$O(\\\\log n)$轮内收敛。该上限表明Becchetti等人（2017）的下限$\\\\Omega(k / h^2)$不能进一步推到$\\\\widetilde{\\\\Omega}(k / h)$。所需的偏差渐近小于3-多数动力学所需的$\\\\Omega(\\\\sqrt{n\\\\log n})$偏差。", "conclusion": "论文为多意见$h$-多数动力学提供了更紧密的收敛时间上限，并表明与3-多数动力学相比，实现共识所需的初始偏差可以显著更小。", "translation": "我们首次提出了在同步设置下，当$h$和$k$均为非常数值时，具有$k$个意见的$h$-多数动力学收敛到共识的时间上限。我们假设在过程开始时，对某个多数意见存在初始加性偏差，即有一个意见由$x$个节点支持，而任何其他意见的支持节点数量都严格少于$x$。我们证明，在高概率下，如果偏差为$\\\\omega(\\\\sqrt{x})$且初始多数意见至少由$x = \\\\omega(\\\\log n)$个节点支持，那么只要$h = \\\\omega(n \\\\log n / x)$，过程就会在$O(\\\\log n)$轮内收敛到多数共识。一个主要推论是：如果$k = o(n / \\\\log n)$且过程从一个几乎平衡的配置开始，对初始多数意见的初始偏差大小为$\\\\omega(\\\\sqrt{n/k})$，那么任何函数$h = \\\\omega(k \\\\log n)$都足以保证在高概率下在$O(\\\\log n)$轮内收敛到共识。我们的上限表明，Becchetti等人（2017）给出的达到共识的下限$\\\\Omega(k / h^2)$不能进一步推到$\\\\widetilde{\\\\Omega}(k / h)$。此外，我们所需的偏差渐近小于在3-多数动力学中保证多数共识所需的$\\\\Omega(\\\\sqrt{n\\\\log n})$偏差：在我们的情况下，对于任何$k \\\\ge 2$的值，所需的偏差最多是$\\\\omega(\\\\sqrt{x})$中的任意（任意小）函数。", "summary": "本文首次在同步设置下，为非恒定$h$和$k$值的多意见$h$-多数动力学提供了收敛时间的上限。研究表明，在高概率下，如果初始存在对多数意见的足够偏差，且该意见得到足够多节点的支持，则系统能在$O(\\\\log n)$轮内收敛到多数共识。结果还指出，与现有研究相比，实现共识所需的初始偏差可以显著减小。", "keywords": "$h$-多数动力学, 共识, 收敛时间, 多意见, 初始偏差", "comments": "创新点在于首次为非恒定$h$和$k$值下的$h$-多数动力学收敛时间提供了严格的上限，并证明了实现共识所需的初始偏差可以显著小于现有认知。这对于理解大规模多意见系统中的共识形成机制具有重要意义。"}}
{"id": "2506.19887", "title": "MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition", "authors": ["Hyo Jin Jon", "Longbin Jin", "Hyuntaek Jung", "Hyunseo Kim", "Donghun Min", "Eun Yi Kim"], "summary": "This paper presents our contributions to the Speech Emotion Recognition in\nNaturalistic Conditions (SERNC) Challenge, where we address categorical emotion\nrecognition and emotional attribute prediction. To handle the complexities of\nnatural speech, including intra- and inter-subject variability, we propose\nMulti-level Acoustic-Textual Emotion Representation (MATER), a novel\nhierarchical framework that integrates acoustic and textual features at the\nword, utterance, and embedding levels. By fusing low-level lexical and acoustic\ncues with high-level contextualized representations, MATER effectively captures\nboth fine-grained prosodic variations and semantic nuances. Additionally, we\nintroduce an uncertainty-aware ensemble strategy to mitigate annotator\ninconsistencies, improving robustness in ambiguous emotional expressions. MATER\nranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of\n0.5928, securing second place in valence prediction with an impressive CCC of\n0.6941.", "comment": "5 pages, 4 figures, 2 tables, 1 algorithm, Accepted to INTERSPEECH\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.19887v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "68T10"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.19887v1", "AI": {"title_translation": "MATER: 可解释语音情感识别中的多级声学与文本情感表示", "tldr": "本文提出了MATER，一个用于语音情感识别的分层框架，它在词、话语和嵌入级别整合了声学和文本特征，并引入了不确定性感知集成策略，以处理自然语音的复杂性，并在SERNC挑战赛中取得了优异成绩。", "motivation": "为了解决自然语音中语音情感识别的复杂性，包括个体内部和个体间变异性，并应对自然条件下的语音情感识别（SERNC）挑战赛中的分类情感识别和情感属性预测任务。", "method": "本文提出了多级声学-文本情感表示（MATER），这是一种新颖的分层框架，它在词、话语和嵌入级别整合了声学和文本特征。通过融合低级词汇和声学线索与高级上下文表示，MATER有效地捕捉了细粒度韵律变化和语义细微差别。此外，还引入了一种不确定性感知集成策略，以减轻标注者不一致性，提高模糊情感表达的鲁棒性。", "result": "MATER在两项任务中均排名第四，Macro-F1为41.01%，平均CCC为0.5928，在效价预测中以0.6941的CCC取得了第二名。", "conclusion": "MATER在自然条件下的语音情感识别（SERNC）挑战赛中表现出色，尤其在效价预测任务上取得了显著成绩，证明了其在处理自然语音复杂性方面的有效性。", "translation": "本文介绍了我们在自然条件下语音情感识别（SERNC）挑战赛中的贡献，我们解决了分类情感识别和情感属性预测问题。为了处理自然语音的复杂性，包括个体内部和个体间变异性，我们提出了多级声学-文本情感表示（MATER），这是一种新颖的分层框架，它在词、话语和嵌入级别整合了声学和文本特征。通过融合低级词汇和声学线索与高级上下文表示，MATER有效地捕捉了细粒度韵律变化和语义细微差别。此外，我们引入了一种不确定性感知集成策略，以减轻标注者不一致性，提高模糊情感表达的鲁棒性。MATER在两项任务中均排名第四，Macro-F1为41.01%，平均CCC为0.5928，在效价预测中以0.6941的CCC取得了令人印象深刻的第二名。", "summary": "本文提出了MATER（多级声学-文本情感表示），一个用于可解释语音情感识别的新型分层框架。该模型旨在处理自然语音的复杂性，通过在词、话语和嵌入级别整合声学和文本特征，并融合低级与高级表示，以捕捉细粒度韵律和语义信息。此外，它还引入了不确定性感知集成策略来提高鲁棒性。MATER在SERNC挑战赛中表现优异，在多项任务中取得了靠前的排名。", "keywords": "语音情感识别, 多级表示, 声学特征, 文本特征, 不确定性感知", "comments": "MATER的创新点在于其多级（词、话语、嵌入）的声学与文本特征融合框架，这使其能够同时捕捉语音中的细粒度韵律变化和高级语义信息。此外，引入不确定性感知集成策略以应对标注者不一致性，增强了模型在自然语境下处理模糊情感表达的鲁棒性，这对于实际应用至关重要。该方法在SERNC挑战赛中取得的优异成绩，特别是效价预测的第二名，证明了其有效性和实用价值。"}}
{"id": "2506.20148", "title": "Developing Artificial Mechanics Intuitions from Extremely Small Data", "authors": ["Jingruo Peng", "Shuze Zhu"], "summary": "Humans can possess good mechanics intuitions by learning from a few examples,\nwhich leads to the question of how to develop artificial mechanics intuitions\nthat can be learned from small data, as we are eagerly entering the era of\nartificial intelligence. We propose in this Letter the sample-switchable\ntraining method, which successfully develops highly-accurate artificial\nmechanics intuitions that can master brachistochrone problem, catenary problem,\nand large nonlinear deformation problem of elastic plate by learning from no\nmore than three samples. The model's intuitive prediction ability increases\nnonlinearly with respect to the number of training samples, suggesting that\nsuperb mechanics intuitions can be in-principle achieved based on a finite\nnumber of samples, reflecting how human brains form good mechanics intuitions\njust by learning a few cases. Our current work presents an alternative\nperspective for educating artificial intelligence capable of intuitively\nunderstand and predict how materials deform and move, a scenario that has been\nfrequently seen in Science-Fiction movies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20148v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20148v1", "AI": {"title_translation": "从极小数据中发展人工力学直觉", "tldr": "本文提出了一种样本可切换训练方法，仅通过少量（不超过三个）样本，成功开发出高精度的人工力学直觉，能够掌握最速降线问题、悬链线问题和弹性板大非线性变形问题。", "motivation": "人类可以通过学习少量例子获得良好的力学直觉，这引发了如何在人工智能时代从少量数据中发展人工力学直觉的问题。", "method": "本文提出了一种样本可切换训练方法（sample-switchable training method）。", "result": "该方法成功开发出高精度的人工力学直觉，能够掌握最速降线问题、悬链线问题和弹性板大非线性变形问题，且学习样本不超过三个。模型的直觉预测能力随训练样本数量非线性增长，表明原则上可以通过有限数量的样本实现卓越的力学直觉。", "conclusion": "本工作为培养能够直观理解和预测材料变形和运动的人工智能提供了一种替代视角，反映了人类大脑如何通过学习少量案例形成良好的力学直觉。", "translation": "人类可以通过学习少量例子获得良好的力学直觉，这引出了一个问题：在我们迫切进入人工智能时代之际，如何从少量数据中发展人工力学直觉。本文提出了一种样本可切换训练方法，该方法成功开发出高精度的人工力学直觉，仅通过不超过三个样本的学习，就能掌握最速降线问题、悬链线问题和弹性板大非线性变形问题。模型的直觉预测能力随训练样本数量非线性增长，这表明原则上可以通过有限数量的样本实现卓越的力学直觉，反映了人类大脑如何仅通过学习少数案例就形成良好的力学直觉。我们目前的工作为培养能够直观理解和预测材料如何变形和运动的人工智能提供了一种替代视角，这种场景在科幻电影中经常出现。", "summary": "本文针对人工智能时代如何从少量数据中发展人工力学直觉的问题，提出了一种样本可切换训练方法。该方法仅通过极少数样本（不超过三个），便成功使模型在高精度下掌握了最速降线、悬链线和弹性板大非线性变形等力学问题。研究发现，模型的直觉预测能力随训练样本数量非线性增长，暗示了通过有限样本实现卓越力学直觉的可能性，这与人类学习机制相似。该工作为培养具备直观理解和预测材料行为能力的人工智能提供了新途径。", "keywords": "人工力学直觉, 小数据学习, 样本可切换训练, 最速降线, 弹性板变形", "comments": "这项工作具有重要意义，因为它解决了AI在处理力学问题时对大量数据的依赖，提出了一个创新的“样本可切换训练方法”，实现了在极小数据量下高精度地学习力学直觉。这不仅提高了AI的效率和泛化能力，也为AI在物理世界中的应用开辟了新路径，并启发了对人类学习机制的理解。其创新性在于模拟了人类从少量经验中学习复杂直觉的能力。"}}
{"id": "2506.20036", "title": "Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion", "authors": ["Jeremiah Coholich", "Muhammad Ali Murtaza", "Seth Hutchinson", "Zsolt Kira"], "summary": "We propose a novel hierarchical reinforcement learning framework for\nquadruped locomotion over challenging terrain. Our approach incorporates a\ntwo-layer hierarchy in which a high-level policy (HLP) selects optimal goals\nfor a low-level policy (LLP). The LLP is trained using an on-policy\nactor-critic RL algorithm and is given footstep placements as goals. We propose\nan HLP that does not require any additional training or environment samples and\ninstead operates via an online optimization process over the learned value\nfunction of the LLP. We demonstrate the benefits of this framework by comparing\nit with an end-to-end reinforcement learning (RL) approach. We observe\nimprovements in its ability to achieve higher rewards with fewer collisions\nacross an array of different terrains, including terrains more difficult than\nany encountered during training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20036v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20036v1", "AI": {"title_translation": "挑战性四足动物运动的分层强化学习和价值优化", "tldr": "本文提出了一种新颖的分层强化学习框架，通过高层策略对低层策略学习到的价值函数进行在线优化，显著提高了四足动物在复杂地形上的运动能力。", "motivation": "为了解决四足动物在复杂地形上运动的挑战。", "method": "本文提出了一种新的分层强化学习框架，包含两层：高层策略（HLP）为低层策略（LLP）选择最优目标。LLP使用在线策略的actor-critic强化学习算法进行训练，并以落脚点作为目标。HLP无需额外训练或环境样本，而是通过对LLP学习到的价值函数进行在线优化来操作。该框架通过与端到端强化学习方法进行比较来验证其优势。", "result": "与端到端强化学习方法相比，该框架在各种不同地形（包括训练中未遇到的更困难地形）上实现了更高的奖励和更少的碰撞。", "conclusion": "所提出的分层强化学习框架能够有效提高四足动物在挑战性地形上的运动表现，优于传统的端到端强化学习方法。", "translation": "我们提出了一种新颖的分层强化学习框架，用于四足动物在挑战性地形上的运动。我们的方法采用了两层分层结构，其中高层策略（HLP）为低层策略（LLP）选择最优目标。LLP使用在线策略的actor-critic强化学习算法进行训练，并以落脚点作为目标。我们提出的HLP不需要任何额外的训练或环境样本，而是通过对LLP学习到的价值函数进行在线优化过程来操作。我们通过将其与端到端强化学习（RL）方法进行比较，证明了该框架的优势。我们观察到，在各种不同地形（包括比训练中遇到的任何地形都更困难的地形）上，它能够以更少的碰撞获得更高的奖励。", "summary": "本文提出了一种新颖的分层强化学习（HRL）框架，用于解决四足动物在复杂地形上的运动问题。该框架采用两层结构，高层策略（HLP）通过对低层策略（LLP）学习到的价值函数进行在线优化来选择最优目标，而LLP则使用actor-critic算法进行训练。实验结果表明，与端到端强化学习相比，该HRL框架在提高奖励和减少碰撞方面表现出显著优势，尤其是在训练中未曾遇到的困难地形上。", "keywords": "分层强化学习, 四足动物运动, 价值优化, 挑战性地形, Actor-Critic", "comments": "该论文的创新点在于高层策略（HLP）无需额外的训练，而是通过对低层策略（LLP）的价值函数进行在线优化来指导运动，这提高了训练效率和泛化能力。其重要性在于证明了分层强化学习在处理复杂机器人运动任务上的优越性，尤其是在未见过的挑战性地形上。未来可以进一步探索如何将这种方法应用于更广泛的机器人控制问题。"}}
{"id": "2506.20159", "title": "AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary", "authors": ["Tomas Herda", "Victoria Pichler", "Zheying Zhang", "Pekka Abrahamsson", "Geir K. Hanssen"], "summary": "The full-day workshop on AI and Agile at XP 2025 convened a diverse group of\nresearchers and industry practitioners to address the practical challenges and\nopportunities of integrating Artificial Intelligence into Agile software\ndevelopment. Through interactive sessions, participants identified shared\nfrustrations related to integrating AI into Agile Software Development\npractices, including challenges with tooling, governance, data quality, and\ncritical skill gaps. These challenges were systematically prioritized and\nanalyzed to uncover root causes. The workshop culminated in the collaborative\ndevelopment of a research roadmap that pinpoints actionable directions for\nfuture work, including both immediate solutions and ambitious long-term goals.\nThe key outcome is a structured agenda designed to foster joint\nindustry-academic efforts to move from identified frustrations to successful\nimplementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20159v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20159v1", "AI": {"title_translation": "AI与敏捷软件开发：从挫折到成功——XP2025研讨会总结", "tldr": "XP2025研讨会探讨了将AI融入敏捷开发的挑战，识别了痛点并制定了研究路线图，旨在促进产学合作，实现成功集成。", "motivation": "该研讨会的动机是解决将人工智能集成到敏捷软件开发中的实际挑战和机遇，特别是参与者在整合AI到敏捷实践中遇到的共同挫折。", "method": "研讨会通过互动环节，让参与者识别并优先排序与AI集成相关的挑战（工具、治理、数据质量、技能差距），分析其根本原因，并最终合作制定了一个研究路线图。", "result": "研讨会的主要成果是一个结构化的议程，旨在促进产学界共同努力，从已识别的挫折走向成功的实施。具体包括制定了一个研究路线图，其中包含即时解决方案和长期目标。", "conclusion": "研讨会旨在通过制定研究路线图和结构化议程，促进产学合作，将AI与敏捷软件开发的集成从当前面临的挫折转变为成功的实践。", "translation": "XP 2025年关于AI与敏捷的全天研讨会召集了来自研究界和工业界的多元化群体，旨在解决将人工智能整合到敏捷软件开发中的实际挑战和机遇。通过互动环节，参与者识别了在将AI整合到敏捷软件开发实践中遇到的共同挫折，包括工具、治理、数据质量和关键技能差距方面的挑战。这些挑战被系统地优先排序和分析，以揭示根本原因。研讨会最终合作开发了一个研究路线图，指明了未来工作的可行方向，包括即时解决方案和雄心勃勃的长期目标。关键成果是一个结构化的议程，旨在促进产学界共同努力，从已识别的挫折走向成功实施。", "summary": "XP2025年AI与敏捷研讨会汇集了研究人员和行业专家，旨在解决AI融入敏捷开发的挑战。研讨会识别并优先排序了工具、治理、数据质量和技能差距等方面的挫折，分析了根本原因，并共同制定了一个研究路线图。核心目标是建立一个结构化议程，以推动产学合作，将AI与敏捷的集成从挑战转变为成功。", "keywords": "AI, 敏捷软件开发, 研讨会, 挑战, 研究路线图", "comments": "该研讨会的重要性在于其识别并系统化了AI与敏捷开发集成中的关键痛点，并提出了一个具体的研究路线图，促进了产学界的合作，为未来的研究和实践提供了明确的方向。其创新之处在于将“挫折”作为起点，通过集体智慧转化为“成功”的路径。"}}
{"id": "2506.20066", "title": "ToSA: Token Merging with Spatial Awareness", "authors": ["Hsiang-Wei Huang", "Wenhao Chai", "Kuang-Ming Chen", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "summary": "Token merging has emerged as an effective strategy to accelerate Vision\nTransformers (ViT) by reducing computational costs. However, existing methods\nprimarily rely on the visual token's feature similarity for token merging,\noverlooking the potential of integrating spatial information, which can serve\nas a reliable criterion for token merging in the early layers of ViT, where the\nvisual tokens only possess weak visual information. In this paper, we propose\nToSA, a novel token merging method that combines both semantic and spatial\nawareness to guide the token merging process. ToSA leverages the depth image as\ninput to generate pseudo spatial tokens, which serve as auxiliary spatial\ninformation for the visual token merging process. With the introduced spatial\nawareness, ToSA achieves a more informed merging strategy that better preserves\ncritical scene structure. Experimental results demonstrate that ToSA\noutperforms previous token merging methods across multiple benchmarks on visual\nand embodied question answering while largely reducing the runtime of the ViT,\nmaking it an efficient solution for ViT acceleration. The code will be\navailable at: https://github.com/hsiangwei0903/ToSA", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20066v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20066v1", "AI": {"title_translation": "ToSA：空间感知型令牌合并", "tldr": "ToSA是一种新的令牌合并方法，它结合了语义和空间信息来加速Vision Transformers，并在多个基准测试中表现出色。", "motivation": "现有的令牌合并方法主要依赖视觉令牌的特征相似性，忽略了空间信息的重要性，尤其是在ViT的早期层，视觉信息较弱。", "method": "本文提出ToSA，一种结合语义和空间感知的令牌合并方法。它利用深度图像作为输入生成伪空间令牌，作为视觉令牌合并过程的辅助空间信息。", "result": "ToSA在视觉和具身问答的多个基准测试中优于以前的令牌合并方法，同时大大减少了ViT的运行时间，证明其是ViT加速的高效解决方案。", "conclusion": "ToSA通过引入空间感知，实现了更明智的令牌合并策略，可以更好地保留关键场景结构，是一种高效的ViT加速解决方案。", "translation": "令牌合并已成为通过降低计算成本来加速视觉Transformer（ViT）的有效策略。然而，现有方法主要依赖视觉令牌的特征相似性进行令牌合并，而忽略了整合空间信息的潜力，空间信息可以在ViT的早期层中作为令牌合并的可靠标准，因为在这些层中视觉令牌仅具有微弱的视觉信息。在本文中，我们提出了ToSA，一种新颖的令牌合并方法，它结合了语义和空间感知来指导令牌合并过程。ToSA利用深度图像作为输入生成伪空间令牌，这些令牌作为视觉令牌合并过程的辅助空间信息。通过引入空间感知，ToSA实现了一种更明智的合并策略，可以更好地保留关键场景结构。实验结果表明，ToSA在视觉和具身问答的多个基准测试中均优于以前的令牌合并方法，同时大大减少了ViT的运行时间，使其成为ViT加速的高效解决方案。代码将在：https://github.com/hsiangwei0903/ToSA 提供。", "summary": "本文提出ToSA，一种用于加速Vision Transformers的新型令牌合并方法。与现有方法仅依赖特征相似性不同，ToSA通过引入深度图像生成的伪空间令牌，将语义和空间信息相结合，从而在ViT的早期层中实现更有效的令牌合并，更好地保留场景结构。实验证明ToSA在性能和效率上均优于现有方法。", "keywords": "令牌合并, 空间感知, Vision Transformer, 深度图像, 计算加速", "comments": "ToSA的创新点在于将空间信息（通过深度图像生成伪空间令牌）引入到令牌合并过程中，弥补了现有方法只关注语义相似性的不足，尤其对ViT早期层较弱的视觉信息处理有益。这使得合并策略更加智能，能更好地保留关键场景结构，为ViT的加速提供了一个高效且性能优越的解决方案。"}}
{"id": "2506.19885", "title": "FlightKooba: A Fast Interpretable FTP Model", "authors": ["Jing Lu", "Xuan Wu", "Yizhun Tian", "Songhan Fan", "Yali Fang"], "summary": "The Koopman theory is a powerful and effective modeling tool for converting\nnonlinear systems into linear representations, and flight trajectory prediction\n(FTP) is a complex nonlinear system. However, current models applying the\nKoopman theory to FTP tasks are not very effective, model interpretability is\nindeed an issue, and the Koopman operators are computationally intensive,\nresulting in long training times. To address this issue, this paper proposes a\nnew modeling and control framework based on the HIPPO method, the Koopman\ntheory, and state space equations from cybernetics: FlightKooba. Inspired by\nthe idea of structural state space equations, FlightKooba directly constructs\nthe Koopman operators from data. This makes the framework highly interpretable\nand significantly reduces the number of trainable parameters in the module,\nthereby greatly reducing training time. Experiments have demonstrated the\nsuperiority of the FlightKooba modeling method in terms of time and memory\nconsumption (training time comparable to the Mamba module without using\nCUDA-level acceleration; memory reduced by more than 50% on most datasets, with\na tenfold reduction in the number of parameters), essentially completing the\nFTP task. It provides a new method for the fast computation of the Koopman\noperators, opening up new possibilities for the combination of time series\nforecasting and control.", "comment": "7 figures", "pdf_url": "http://arxiv.org/pdf/2506.19885v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19885v1", "AI": {"title_translation": "FlightKooba: 一种快速可解释的FTP模型", "tldr": "FlightKooba提出了一种基于HIPPO和Koopman理论的新框架，用于解决飞行轨迹预测中现有Koopman模型效率低、可解释性差和计算量大的问题，实现了更快的训练速度和更高的可解释性。", "motivation": "现有的将Koopman理论应用于飞行轨迹预测（FTP）任务的模型效果不佳，模型可解释性差，且Koopman算子计算密集，导致训练时间长。", "method": "本文提出了FlightKooba，一个基于HIPPO方法、Koopman理论和控制论中状态空间方程的新建模和控制框架。FlightKooba通过从数据中直接构建Koopman算子，实现了高可解释性并显著减少了可训练参数的数量。", "result": "实验证明，FlightKooba在时间（训练时间与Mamba模块相当，无需CUDA加速）和内存消耗（在大多数数据集上内存减少50%以上，参数数量减少十倍）方面表现出优越性，并基本完成了FTP任务。", "conclusion": "FlightKooba为Koopman算子的快速计算提供了一种新方法，为时间序列预测和控制的结合开辟了新的可能性，并成功完成了飞行轨迹预测任务。", "translation": "Koopman理论是一种将非线性系统转换为线性表示的强大而有效的建模工具，而飞行轨迹预测（FTP）是一个复杂的非线性系统。然而，目前将Koopman理论应用于FTP任务的模型效果不佳，模型可解释性确实是一个问题，并且Koopman算子计算密集，导致训练时间长。为了解决这个问题，本文提出了一种基于HIPPO方法、Koopman理论和控制论中状态空间方程的新建模和控制框架：FlightKooba。受结构化状态空间方程思想的启发，FlightKooba直接从数据中构建Koopman算子。这使得该框架具有高度可解释性，并显著减少了模块中可训练参数的数量，从而大大缩短了训练时间。实验证明了FlightKooba建模方法在时间和内存消耗方面的优越性（训练时间与Mamba模块相当，无需使用CUDA级加速；在大多数数据集上内存减少50%以上，参数数量减少十倍），基本完成了FTP任务。它为Koopman算子的快速计算提供了一种新方法，为时间序列预测和控制的结合开辟了新的可能性。", "summary": "本文提出了FlightKooba，一个针对飞行轨迹预测（FTP）任务的新型建模和控制框架。该框架结合了HIPPO方法、Koopman理论和状态空间方程，旨在解决现有Koopman模型在FTP应用中效率低、可解释性差和计算量大的问题。FlightKooba通过直接从数据构建Koopman算子，显著提高了模型的可解释性，并大幅减少了训练参数和时间。实验结果表明，FlightKooba在训练速度和内存效率上均表现出色，并成功完成了FTP任务，为Koopman算子的快速计算和时间序列预测与控制的结合提供了新途径。", "keywords": "FlightKooba, Koopman理论, 飞行轨迹预测, 可解释性, 快速计算", "comments": "FlightKooba的创新之处在于其结合HIPPO方法和直接从数据构建Koopman算子的策略，这不仅提升了模型的可解释性，还大幅优化了计算效率。其在训练时间和内存消耗上的显著改进，特别是与Mamba模块的性能对比，凸显了其在处理复杂非线性系统，如飞行轨迹预测方面的潜力。这篇论文为Koopman理论的实际应用和计算效率优化提供了重要进展。"}}
{"id": "2506.20008", "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "authors": ["Abdul Basit", "Minghao Shao", "Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.", "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20008v1", "categories": ["cs.AI", "cs.PL", "cs.SE", "68T50, 81P68, 68T07, 68T20", "I.2.7; I.2.2"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20008v1", "AI": {"title_translation": "QHackBench：使用PennyLane黑客马拉松挑战赛对大型语言模型进行量子代码生成基准测试", "tldr": "本文介绍了QHackBench，一个用于评估大型语言模型（LLMs）生成量子代码的基准，结果显示检索增强生成（RAG）模型与标准提示效果相似，且多智能体管道能提高执行成功率。", "motivation": "大型语言模型（LLMs）在代码生成方面潜力巨大，但它们在量子计算代码生成领域的有效性尚未得到充分探索。本研究旨在通过基准测试来填补这一空白。", "method": "研究引入了QHackBench，一个来源于量子黑客马拉松（QHack）竞赛的新型基准数据集。通过普通提示（vanilla prompting）和检索增强生成（RAG）两种方式评估了模型的性能，并采用结构化评估框架来衡量功能正确性、语法有效性和执行成功率。此外，还引入了一个多智能体评估管道，用于迭代地改进不正确的解决方案。", "result": "结果表明，增强了PennyLane数据集的RAG增强模型与标准提示生成的结果大致相似，尤其是在复杂的量子算法中。此外，多智能体评估管道能够进一步提高执行成功率。", "conclusion": "本研究通过QHackBench对LLMs进行量子代码生成基准测试，表明RAG方法与标准提示表现相当，并且多智能体精炼管道能有效提升成功率，对AI辅助量子编程的进步做出了贡献。", "translation": "大型语言模型（LLMs）的最新进展在代码生成方面展现出强大潜力，然而它们在量子计算中的有效性仍未被充分探索。本文使用来自量子黑客马拉松（QHack）的真实世界挑战，对基于PennyLane的量子代码生成LLMs进行了基准测试。我们引入了QHackBench，一个源自QHack竞赛的新型基准数据集，并评估了模型在普通提示（vanilla prompting）和检索增强生成（RAG）下的性能。我们结构化的评估框架在不同挑战难度下评估了功能正确性、语法有效性和执行成功率。结果表明，通过增强的PennyLane数据集补充的RAG增强模型，与标准提示生成的结果大致相似，尤其是在复杂的量子算法中。此外，我们引入了一个多智能体评估管道，该管道迭代地改进不正确的解决方案，进一步提高了执行成功率。为了促进进一步的研究，我们承诺公开QHackBench，以及我们的评估框架和实验结果，以实现AI辅助量子编程的持续进步。", "summary": "本文引入了QHackBench，一个来源于量子黑客马拉松（QHack）挑战的新型基准数据集，旨在评估大型语言模型（LLMs）在基于PennyLane的量子代码生成方面的能力。研究评估了LLM在普通提示和检索增强生成（RAG）下的性能，并从功能正确性、语法有效性和执行成功率等维度进行评估。结果显示，RAG增强模型，特别是在补充了增强的PennyLane数据集后，其表现与标准提示大致相似，尤其是在处理复杂量子算法时。此外，研究还提出了一个多智能体评估管道，能够迭代地优化不正确的解决方案，显著提高了执行成功率。为了促进后续研究，QHackBench数据集、评估框架和实验结果将公开发布。", "keywords": "大型语言模型, 量子代码生成, PennyLane, QHackBench, 基准测试", "comments": "该研究的创新之处在于创建了一个基于真实世界挑战的量子代码生成LLM基准数据集（QHackBench），填补了该领域研究不足的空白。引入多智能体精炼管道以提高解决方案质量也是一个重要的贡献。数据集和评估框架的公开发布对促进未来AI辅助量子编程研究至关重要。"}}
{"id": "2506.19874", "title": "Towards Provable (In)Secure Model Weight Release Schemes", "authors": ["Xing Yang", "Bingtao Wang", "Yuhao Wang", "Zimo Ji", "Terry Jingchen Zhang", "Wenyuan Jiang"], "summary": "Recent secure weight release schemes claim to enable open-source model\ndistribution while protecting model ownership and preventing misuse. However,\nthese approaches lack rigorous security foundations and provide only informal\nsecurity guarantees. Inspired by established works in cryptography, we\nformalize the security of weight release schemes by introducing several\nconcrete security definitions. We then demonstrate our definition's utility\nthrough a case study of TaylorMLP, a prominent secure weight release scheme.\nOur analysis reveals vulnerabilities that allow parameter extraction thus\nshowing that TaylorMLP fails to achieve its informal security goals. We hope\nthis work will advocate for rigorous research at the intersection of machine\nlearning and security communities and provide a blueprint for how future weight\nrelease schemes should be designed and evaluated.", "comment": "8 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.19874v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19874v1", "AI": {"title_translation": "迈向可证明（不）安全的模型权重发布方案", "tldr": "本文形式化了模型权重发布方案的安全定义，分析了TaylorMLP，发现了漏洞，并倡导在机器学习安全领域进行严谨研究。", "motivation": "现有的安全权重发布方案声称能够实现开源模型分发并保护模型所有权，但它们缺乏严格的安全基础，仅提供非正式的安全保证。", "method": "作者受密码学既有工作的启发，通过引入几个具体的安全定义来形式化权重发布方案的安全性。随后，他们通过对一个著名的安全权重发布方案TaylorMLP进行案例研究，展示了这些定义的实用性。", "result": "他们的分析揭示了TaylorMLP中允许参数提取的漏洞，从而表明TaylorMLP未能实现其非正式的安全目标。", "conclusion": "这项工作旨在促进机器学习和安全社区交叉领域的严谨研究，并为未来权重发布方案的设计和评估提供蓝图。", "translation": "最近的安全权重发布方案声称能够实现开源模型分发，同时保护模型所有权并防止滥用。然而，这些方法缺乏严格的安全基础，仅提供非正式的安全保证。受密码学中既有工作的启发，我们通过引入几个具体的安全定义来形式化权重发布方案的安全性。然后，我们通过对TaylorMLP（一个著名的安全权重发布方案）的案例研究，展示了我们定义的实用性。我们的分析揭示了允许参数提取的漏洞，从而表明TaylorMLP未能实现其非正式的安全目标。我们希望这项工作能促进机器学习和安全社区交叉领域的严谨研究，并为未来权重发布方案的设计和评估提供蓝图。", "summary": "本文旨在解决当前安全模型权重发布方案缺乏严格安全基础的问题。受密码学启发，文章为这些方案形式化了安全定义。通过对著名方案TaylorMLP的案例研究，揭示了允许参数提取的漏洞，表明其未能达到非正式的安全目标。这项工作旨在推动机器学习与安全交叉领域的严谨研究，并为未来方案的设计和评估提供指导。", "keywords": "模型权重发布, 安全定义, 漏洞分析, TaylorMLP, 机器学习安全", "comments": "该论文的创新之处在于为模型权重发布方案形式化了安全定义，这对于开源AI模型的信任度至关重要。通过揭露TaylorMLP等知名方案中的漏洞，突显了超越非正式声明进行严格安全分析的必要性，从而强调了其重要性。它为未来安全机器学习的发展提供了基础框架。"}}
{"id": "2506.20062", "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents", "authors": ["Runlong Ye", "Zeling Zhang", "Boushra Almazroua", "Michael Liut"], "summary": "AI-powered code assistants are widely used to generate code completions,\nsignificantly boosting developer productivity. However, these tools typically\npresent suggestions without explaining their rationale, leaving their\ndecision-making process inscrutable. This opacity hinders developers' ability\nto critically evaluate the output, form accurate mental models, and build\ncalibrated trust in the system. To address this, we introduce CopilotLens, a\nnovel interactive framework that reframes code completion from a simple\nsuggestion into a transparent, explainable event. CopilotLens operates as an\nexplanation layer that reveals the AI agent's \"thought process\" through a\ndynamic two-level interface, surfacing everything from its reconstructed\nhigh-level plans to the specific codebase context influencing the code. This\npaper presents the design and rationale of CopilotLens, offering a concrete\nframework for building future agentic code assistants that prioritize clarity\nof reasoning over speed of suggestion, thereby fostering deeper comprehension\nand more robust human-AI collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20062v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20062v1", "AI": {"title_translation": "超越自动补全：设计 CopilotLens，迈向透明可解释的 AI 编程助手", "tldr": "CopilotLens 是一个旨在提高 AI 编程助手透明度和可解释性的框架，通过揭示 AI 的思考过程来帮助开发者更好地理解和信任系统。", "motivation": "现有的 AI 编程助手虽然提高了生产力，但其建议缺乏解释，导致决策过程不透明，开发者难以评估输出、形成准确的心智模型并建立信任。", "method": "引入了 CopilotLens，一个新颖的交互式框架。它将代码补全从简单的建议重构为一个透明、可解释的事件。CopilotLens 作为解释层，通过动态的两级界面揭示 AI 代理的“思考过程”，从重建的高层计划到影响代码的具体代码上下文。", "result": "本文介绍了 CopilotLens 的设计和原理。", "conclusion": "CopilotLens 提供了一个具体的框架，用于构建未来以推理清晰度而非建议速度为优先的智能代码助手，从而促进更深入的理解和更强大的人机协作。", "translation": "AI 驱动的代码助手被广泛用于生成代码补全，显著提高了开发人员的生产力。然而，这些工具通常在不解释其原理的情况下提供建议，使其决策过程难以捉摸。这种不透明性阻碍了开发人员批判性评估输出、形成准确的心智模型以及建立对系统的校准信任的能力。为了解决这个问题，我们引入了 CopilotLens，一个新颖的交互式框架，它将代码补全从简单的建议重构为一个透明、可解释的事件。CopilotLens 作为解释层，通过动态的两级界面揭示 AI 代理的“思考过程”，从其重建的高层计划到影响代码的具体代码上下文。本文介绍了 CopilotLens 的设计和原理，为构建未来的智能代码助手提供了一个具体的框架，这些助手优先考虑推理的清晰度而非建议的速度，从而促进更深入的理解和更强大的人机协作。", "summary": "本文提出了 CopilotLens，一个创新的交互式框架，旨在解决当前 AI 编程助手缺乏透明度和可解释性的问题。CopilotLens 通过动态两级界面揭示 AI 代理的“思考过程”，包括高层计划和具体代码上下文，从而将代码补全转化为一个可解释的事件。该框架旨在帮助开发者更好地理解和评估 AI 建议，提升人机协作和信任。", "keywords": "AI 编程助手, 可解释性, 透明度, 代码补全, 人机协作", "comments": "CopilotLens 的创新在于将 AI 编程助手的黑箱操作转化为透明可解释的过程，这对于提升开发者对 AI 工具的信任和批判性评估能力至关重要。它强调了可解释性在未来 AI 辅助编程中的重要性，超越了单纯的速度和效率。"}}
{"id": "2506.20111", "title": "A clusterability test for directed graphs", "authors": ["Mario R. Guarracino", "Pierre Miasnikof", "Alexander Y. Shestopaloff", "Houyem Demni", "Cristián Bravo", "Yuri Lawryshyn"], "summary": "In this article, we extend a statistical test of graph clusterability, the\n$\\delta$ test, to directed graphs with no self loops. The $\\delta$ test,\noriginally designed for undirected graphs, is based on the premise that graphs\nwith a clustered structure display a mean local density that is statistically\nhigher than the graph's global density. We posit that graphs that do not meet\nthis necessary (but not sufficient) condition for clusterability can be\nconsidered unsuited to clustering. In such cases, vertex clusters do not offer\na meaningful summary of the broader graph. Additionally in this study, we aim\nto determine the optimal sample size (number of neighborhoods). Our test,\ndesigned for the analysis of large networks, is based on sampling subsets of\nneighborhoods/nodes. It is designed for cases where computing the density of\nevery node's neighborhood is infeasible. Our results show that the $\\delta$\ntest performs very well, even with very small samples of neighborhoods ($1\\%$).\nIt accurately detects unclusterable graphs and is also shown to be robust to\ndepartures from the underlying assumptions of the $t$ test.", "comment": "22 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20111v1", "categories": ["cs.NI", "62"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.20111v1", "AI": {"title_translation": "有向图的可聚类性测试", "tldr": "该研究将用于无向图的delta测试扩展到有向图，用于判断大型网络是否适合聚类，并发现即使是小样本也能表现良好。", "motivation": "识别不适合聚类的图，因为在这种情况下，顶点聚类无法提供有意义的图摘要。此外，旨在确定测试所需的最佳样本量。", "method": "将最初为无向图设计的$\\delta$测试扩展到无自环的有向图。该测试基于局部密度高于全局密度的前提。为了分析大型网络，测试通过抽样部分邻域/节点进行，以解决计算每个节点邻域密度不可行的问题。", "result": "$\\delta$测试表现非常出色，即使在邻域样本量非常小（1%）的情况下也是如此。它能准确检测不可聚类的图，并且对t检验的基本假设的偏离也表现出鲁棒性。", "conclusion": "扩展后的$\\delta$测试是一种有效且鲁棒的工具，可以判断大型有向图是否适合聚类，即使在资源有限（小样本）的情况下也能准确运作。", "translation": "在本文中，我们将图可聚类性统计测试——$\\delta$测试，扩展到无自环的有向图。$\\delta$测试最初是为无向图设计的，其前提是具有聚类结构的图显示出统计学上高于图全局密度的平均局部密度。我们认为，不符合可聚类性必要（但非充分）条件的图可以被认为不适合聚类。在这种情况下，顶点聚类无法提供更广泛图的有效摘要。此外，在本研究中，我们旨在确定最佳样本量（邻域数量）。我们的测试专为分析大型网络而设计，基于对邻域/节点子集的采样。它适用于计算每个节点邻域密度不可行的情况。我们的结果表明，即使在非常小的邻域样本（1%）下，$\\delta$测试也表现出色。它能准确检测不可聚类的图，并且对t检验的基本假设的偏离也表现出鲁棒性。", "summary": "本文将无向图的$\\delta$可聚类性测试扩展到无自环的有向图，旨在判断大型网络是否适合聚类。该测试基于局部密度与全局密度的比较，并通过采样邻域来应对大规模网络的计算挑战。研究结果表明，即使使用极小的样本量（1%），扩展后的$\\delta$测试也能准确识别不可聚类的图，并对底层统计假设的偏离表现出鲁棒性。", "keywords": "有向图, 可聚类性测试, $\\delta$测试, 大型网络, 采样", "comments": "这项工作将现有的$\\delta$测试扩展到有向图，填补了在大型有向网络中评估聚类适用性的空白。其创新之处在于证明了该方法在极小样本量下的高效性和鲁棒性，这对于处理现实世界中的大规模图数据具有重要意义，因为它解决了计算可行性问题。"}}
{"id": "2506.20261", "title": "Exploration-Exploitation Tradeoff in Universal Lossy Compression", "authors": ["Nir Weinberger", "Ram Zamir"], "summary": "Universal compression can learn the source and adapt to it either in a batch\nmode (forward adaptation), or in a sequential mode (backward adaptation). We\nrecast the sequential mode as a multi-armed bandit problem, a fundamental model\nin reinforcement-learning, and study the trade-off between exploration and\nexploitation in the lossy compression case. We show that a previously proposed\n\"natural type selection\" scheme can be cast as a reconstruction-directed MAB\nalgorithm, for sequential lossy compression, and explain its limitations in\nterms of robustness and short-block performance. We then derive and analyze\nrobust cost-directed MAB algorithms, which work at any block length.", "comment": "An extended version of ISIT 2025 paper", "pdf_url": "http://arxiv.org/pdf/2506.20261v1", "categories": ["cs.IT", "cs.LG", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20261v1", "AI": {"title_translation": "通用有损压缩中的探索-利用权衡", "tldr": "本文将通用有损压缩的序列模式重构为多臂赌博机问题，研究了探索与利用的权衡，并提出了鲁棒的成本导向MAB算法来解决现有方法的局限性。", "motivation": "现有通用有损压缩中的“自然类型选择”方案在鲁棒性和短块性能方面存在局限性，需要更鲁棒的算法来处理序列模式下的探索-利用权衡。", "method": "将通用有损压缩的序列模式重构为多臂赌博机（MAB）问题，并在此框架下研究探索与利用的权衡。提出了鲁棒的成本导向MAB算法。", "result": "证明了先前提出的“自然类型选择”方案可以被视为一种重建导向的MAB算法，并解释了其在鲁棒性和短块性能方面的局限性。成功推导并分析了适用于任意块长度的鲁棒成本导向MAB算法。", "conclusion": "通过将序列有损压缩建模为多臂赌博机问题，本文发现了现有方法的不足，并成功开发了在任何块长度下都表现良好的鲁棒成本导向MAB算法。", "translation": "通用压缩可以学习源并以批量模式（前向自适应）或序列模式（后向自适应）适应它。我们将序列模式重构为一个多臂赌博机问题，这是强化学习中的一个基本模型，并研究了有损压缩情况下的探索与利用之间的权衡。我们表明，先前提出的一种“自然类型选择”方案可以被视为一种用于序列有损压缩的重建导向MAB算法，并解释了其在鲁棒性和短块性能方面的局限性。然后，我们推导并分析了鲁棒的成本导向MAB算法，这些算法适用于任何块长度。", "summary": "本文将通用有损压缩的序列模式建模为强化学习中的多臂赌博机问题，深入探讨了探索与利用之间的权衡。研究发现，现有的“自然类型选择”方案作为一种重建导向的MAB算法，在鲁棒性和短块性能上存在不足。为此，本文提出并分析了新的鲁棒成本导向MAB算法，这些算法能够有效应对不同块长度的压缩需求。", "keywords": "通用有损压缩, 探索-利用权衡, 多臂赌博机, 序列压缩, 鲁棒算法", "comments": "本文的创新之处在于将通用有损压缩的序列模式与强化学习中的多臂赌博机问题相结合，为理解和优化压缩算法提供了新的视角。通过识别现有方法的局限性并提出鲁棒的新算法，该研究对提升通用有损压缩的性能和实用性具有重要意义。"}}
{"id": "2506.20252", "title": "PAT: a new algorithm for all-gather and reduce-scatter operations at scale", "authors": ["Sylvain Jeaugey"], "summary": "This paper describes a new algorithm called PAT, for Parallel Aggregated\nTrees, and which can be used to implement all-gather and reduce-scatter\noperations. This algorithm works on any number of ranks, has a logarithmic\nnumber of network transfers for small size operations, minimizes long-distance\ncommunication, and requires a logarithmic amount of internal buffers,\nindependently from the total operation size. It is aimed at improving the\nperformance of the NCCL library in cases where the ring algorithm would be\ninefficient, as its linear latency would show poor performance for small sizes\nand/or at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20252v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20252v1", "AI": {"title_translation": "PAT：一种可扩展的全收集和规约分散操作新算法", "tldr": "提出了一种名为PAT的新算法，用于大规模全收集和规约分散操作，旨在改善NCCL库在小规模或大规模场景下环形算法效率低下的问题。", "motivation": "旨在解决NCCL库中环形算法在小规模或大规模操作时因线性延迟导致的低效性能问题。", "method": "本文提出了一种名为PAT（Parallel Aggregated Trees）的新算法，用于实现全收集（all-gather）和规约分散（reduce-scatter）操作。该算法适用于任意数量的进程，对于小规模操作具有对数级的网络传输次数，最小化长距离通信，并且独立于总操作大小，仅需要对数级的内部缓冲区。", "result": "该算法对于小规模操作具有对数级的网络传输次数，最小化了长距离通信，并独立于总操作大小需要对数级的内部缓冲区。其目标是提高NCCL库在环形算法效率低下情况下的性能。", "conclusion": "PAT算法通过其对数级的性能特性，为大规模全收集和规约分散操作提供了一种新的高效解决方案，尤其适用于改善NCCL库在特定场景下的性能。", "translation": "本文介绍了一种名为PAT（Parallel Aggregated Trees）的新算法，可用于实现全收集（all-gather）和规约分散（reduce-scatter）操作。该算法适用于任意数量的进程，对于小规模操作具有对数级的网络传输次数，最小化了长距离通信，并且独立于总操作大小，仅需要对数级的内部缓冲区。其目标是在环形算法效率低下（因为其线性延迟在小规模和/或大规模情况下表现不佳）的情况下，改善NCCL库的性能。", "summary": "本文提出了一种名为PAT（Parallel Aggregated Trees）的新算法，专为大规模并行环境中的全收集和规约分散操作设计。PAT算法在任意进程数下工作，对于小规模操作具有对数级的网络传输和缓冲区需求，并能有效减少长距离通信。该算法旨在解决NCCL库中现有环形算法在小规模或大规模场景下性能瓶颈的问题，提供更高效的替代方案。", "keywords": "PAT, 全收集, 规约分散, 并行计算, NCCL", "comments": "这项工作提出了一种新的并行通信算法，其创新之处在于采用“并行聚合树”结构来优化全收集和规约分散操作。其重要性在于解决了现有环形算法在特定场景下的性能瓶颈，特别是在大规模和小规模操作中表现出的线性延迟问题。该算法的对数级复杂性预示着其在可扩展性方面的潜力。"}}
{"id": "2506.20001", "title": "Improved Topology-Independent Distributed Adaptive Node-Specific Signal Estimation for Wireless Acoustic Sensor Networks", "authors": ["Paul Didier", "Toon van Waterschoot", "Simon Doclo", "Jörg Bitzer", "Marc Moonen"], "summary": "This paper addresses the challenge of topology-independent (TI) distributed\nadaptive node-specific signal estimation (DANSE) in wireless acoustic sensor\nnetworks (WASNs) where sensor nodes exchange only fused versions of their local\nsignals. An algorithm named TI-DANSE has previously been presented to handle\nnon-fully connected WASNs. However, its slow iterative convergence towards the\noptimal solution limits its applicability. To address this, we propose in this\npaper the TI-DANSE+ algorithm. At each iteration in TI-DANSE+, the node set to\nupdate its local parameters is allowed to exploit each individual partial\nin-network sums transmitted by its neighbors in its local estimation problem,\nincreasing the available degrees of freedom and accelerating convergence with\nrespect to TI-DANSE. Additionally, a tree-pruning strategy is proposed to\nfurther increase convergence speed. TI-DANSE+ converges as fast as the DANSE\nalgorithm in fully connected WASNs while reducing transmit power usage. The\nconvergence properties of TI-DANSE+ are demonstrated in numerical simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20001v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.20001v1", "AI": {"title_translation": "无线声学传感器网络中改进的拓扑无关分布式自适应节点特定信号估计", "tldr": "提出TI-DANSE+算法，通过利用邻居信息和树剪枝策略，加速了无线声学传感器网络中拓扑无关分布式信号估计的收敛速度并降低了功耗。", "motivation": "现有的TI-DANSE算法迭代收敛速度慢，限制了其在非全连接无线声学传感器网络中的适用性。", "method": "提出TI-DANSE+算法。该算法允许节点在每次迭代中利用邻居传输的每个独立的局部网络内和，以增加可用自由度并加速相对于TI-DANSE的收敛。此外，还提出了一种树剪枝策略以进一步提高收敛速度。", "result": "TI-DANSE+在全连接WASNs中能像DANSE算法一样快速收敛，同时降低了发射功率使用。数值仿真证明了TI-DANSE+的收敛特性。", "conclusion": "TI-DANSE+算法有效解决了TI-DANSE收敛慢的问题，显著提高了无线声学传感器网络中分布式信号估计的收敛速度并降低了功耗。", "translation": "本文解决了无线声学传感器网络（WASNs）中拓扑无关（TI）分布式自适应节点特定信号估计（DANSE）的挑战，其中传感器节点仅交换其局部信号的融合版本。之前已提出一种名为TI-DANSE的算法来处理非全连接WASNs。然而，其缓慢的迭代收敛速度限制了其适用性。为了解决这个问题，本文提出了TI-DANSE+算法。在TI-DANSE+的每次迭代中，被设置为更新其局部参数的节点被允许利用其邻居传输的每个单独的部分网络内和，以增加可用自由度并加速相对于TI-DANSE的收敛。此外，还提出了一种树剪枝策略以进一步提高收敛速度。TI-DANSE+在全连接WASNs中与DANSE算法收敛速度一样快，同时降低了发射功率使用。TI-DANSE+的收敛特性通过数值仿真得到验证。", "summary": "本文针对无线声学传感器网络中拓扑无关分布式自适应节点特定信号估计（TI-DANSE）收敛速度慢的问题，提出了一种改进的TI-DANSE+算法。TI-DANSE+通过允许节点利用邻居的局部和信息，并结合树剪枝策略，显著加速了算法的收敛速度，使其在全连接网络中能达到与DANSE算法相同的收敛速度，同时降低了发射功率。", "keywords": "无线声学传感器网络, 分布式信号估计, 拓扑无关, 收敛加速, 功耗降低", "comments": "本文通过提出TI-DANSE+算法及其结合的树剪枝策略，有效地解决了现有TI-DANSE算法收敛慢的瓶颈。其创新点在于利用邻居的局部信息增加自由度以及引入树剪枝来加速收敛，同时实现了功耗的降低，这对于无线传感器网络的应用具有重要意义。"}}
{"id": "2506.20472", "title": "Opinion Dynamics with Highly Oscillating Opinions", "authors": ["Víctor A. Vargas-Pérez", "Jesús Giráldez-Cru", "Oscar Cordón"], "summary": "Opinion Dynamics (OD) models are a particular case of Agent-Based Models in\nwhich the evolution of opinions within a population is studied. In most OD\nmodels, opinions evolve as a consequence of interactions between agents, and\nthe opinion fusion rule defines how those opinions are updated. In consequence,\ndespite being simplistic, OD models provide an explainable and interpretable\nmechanism for understanding the underlying dynamics of opinion evolution.\nUnfortunately, existing OD models mainly focus on explaining the evolution of\n(usually synthetic) opinions towards consensus, fragmentation, or polarization,\nbut they usually fail to analyze scenarios of (real-world) highly oscillating\nopinions. This work overcomes this limitation by studying the ability of\nseveral OD models to reproduce highly oscillating dynamics. To this end, we\nformulate an optimization problem which is further solved using Evolutionary\nAlgorithms, providing both quantitative results on the performance of the\noptimization and qualitative interpretations on the obtained results. Our\nexperiments on a real-world opinion dataset about immigration from the monthly\nbarometer of the Spanish Sociological Research Center show that the ATBCR,\nbased on both rational and emotional mechanisms of opinion update, is the most\naccurate OD model for capturing highly oscillating opinions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20472v1", "categories": ["cs.CE", "cs.CY", "cs.MA"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20472v1", "AI": {"title_translation": "具有高度振荡意见的意见动力学", "tldr": "本研究通过优化问题和进化算法，评估了现有意见动力学模型再现高度振荡意见的能力，发现ATBCR模型在真实世界移民意见数据集上表现最佳。", "motivation": "现有的意见动力学模型主要关注意见趋于共识、分裂或两极分化的场景，但未能有效分析真实世界中高度振荡的意见情景。", "method": "本研究通过构建一个优化问题并使用进化算法进行求解，量化评估了不同意见动力学模型再现高度振荡动力学的能力。实验在一个关于移民的真实世界意见数据集（来自西班牙社会学研究中心的月度晴雨表）上进行。", "result": "实验结果表明，基于理性和情感意见更新机制的ATBCR模型，在捕捉高度振荡意见方面，是现有意见动力学模型中最准确的。", "conclusion": "ATBCR模型在处理真实世界中高度振荡的意见方面表现出卓越的准确性，弥补了现有意见动力学模型的不足。", "translation": "意见动力学（OD）模型是基于代理模型的一个特例，用于研究人口内部意见的演变。在大多数OD模型中，意见随着代理之间的互动而演变，意见融合规则定义了这些意见如何更新。因此，尽管OD模型很简单，但它们提供了一种可解释和可理解的机制，用于理解意见演变的潜在动力学。不幸的是，现有的OD模型主要侧重于解释（通常是合成的）意见向共识、分裂或两极分化的演变，但它们通常无法分析（真实世界）高度振荡意见的场景。这项工作通过研究几种OD模型再现高度振荡动力学的能力来克服这一限制。为此，我们提出了一个优化问题，并使用进化算法进一步求解，提供了优化性能的定量结果和对所得结果的定性解释。我们对来自西班牙社会学研究中心月度晴雨表的真实世界移民意见数据集进行的实验表明，基于理性和情感意见更新机制的ATBCR模型是捕捉高度振荡意见最准确的OD模型。", "summary": "本研究旨在克服现有意见动力学模型在分析高度振荡意见方面的局限性。通过构建优化问题并利用进化算法，评估了多种意见动力学模型再现此类动态的能力。在真实世界的移民意见数据集上进行的实验表明，ATBCR模型因其结合理性和情感更新机制，在捕捉高度振荡意见方面表现出最高的准确性。", "keywords": "意见动力学, 高度振荡意见, 进化算法, ATBCR模型, 真实世界数据", "comments": "本文通过引入优化问题和进化算法来解决现有意见动力学模型无法有效处理高度振荡意见的局限性，具有创新性。其在真实世界数据集上的验证增加了研究的实用性和重要性，尤其是在理解复杂社会现象方面。然而，论文可能没有深入探讨ATBCR模型内部机制的具体细节，以及其在其他类型高度振荡数据上的泛化能力。"}}
{"id": "2506.20045", "title": "Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception", "authors": ["Eric C. Joyce", "Qianwen Zhao", "Nathaniel Burgdorfer", "Long Wang", "Philippos Mordohai"], "summary": "Deep object pose estimators are notoriously overconfident. A grasping agent\nthat both estimates the 6-DoF pose of a target object and predicts the\nuncertainty of its own estimate could avoid task failure by choosing not to act\nunder high uncertainty. Even though object pose estimation improves and\nuncertainty quantification research continues to make strides, few studies have\nconnected them to the downstream task of robotic grasping. We propose a method\nfor training lightweight, deep networks to predict whether a grasp guided by an\nimage-based pose estimate will succeed before that grasp is attempted. We\ngenerate training data for our networks via object pose estimation on real\nimages and simulated grasping. We also find that, despite high object\nvariability in grasping trials, networks benefit from training on all objects\njointly, suggesting that a diverse variety of objects can nevertheless\ncontribute to the same goal.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20045v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20045v1", "AI": {"title_translation": "基于RGB感知的机器人抓取中的共识驱动不确定性", "tldr": "深度姿态估计器过于自信导致机器人抓取失败。本文提出一种轻量级深度网络，通过预测抓取成功率来利用不确定性，使机器人能够在高不确定性下避免抓取，从而提高任务成功率。", "motivation": "深度目标姿态估计器存在过度自信的问题，导致机器人抓取任务失败。尽管姿态估计和不确定性量化研究有所进展，但鲜有研究将其与机器人抓取这一下游任务相结合。", "method": "本文提出一种方法，用于训练轻量级深度网络，以预测基于图像的姿态估计引导的抓取在尝试前是否会成功。训练数据通过对真实图像进行目标姿态估计和模拟抓取生成。", "result": "研究发现，尽管抓取试验中物体变异性很高，但网络通过对所有物体进行联合训练而受益，这表明多样化的物体可以为同一目标做出贡献。", "conclusion": "通过训练轻量级深度网络预测抓取成功率，可以有效利用不确定性来避免机器人抓取任务失败。此外，对多样化物体进行联合训练对网络性能有益。", "translation": "深度物体姿态估计器以其过度自信而闻名。一个能够估计目标物体6自由度姿态并预测自身估计不确定性的抓取代理，可以通过在高不确定性下选择不行动来避免任务失败。尽管物体姿态估计不断改进，不确定性量化研究也持续取得进展，但很少有研究将它们与机器人抓取这一下游任务联系起来。我们提出了一种训练轻量级深度网络的方法，用于在尝试抓取之前预测由基于图像的姿态估计引导的抓取是否会成功。我们通过对真实图像进行物体姿态估计和模拟抓取来生成网络的训练数据。我们还发现，尽管在抓取试验中物体变异性很高，但网络通过对所有物体进行联合训练而受益，这表明多样化的物体仍然可以为同一目标做出贡献。", "summary": "本文旨在解决深度目标姿态估计器过度自信导致机器人抓取失败的问题。作者提出一种新方法，训练轻量级深度网络来预测基于图像的姿态估计引导的抓取是否会成功，从而使抓取代理在高不确定性时避免行动。研究利用真实图像的姿态估计和模拟抓取生成训练数据，并发现对多样化物体进行联合训练能有效提升网络性能。", "keywords": "机器人抓取, 不确定性量化, 姿态估计, 深度学习, RGB感知", "comments": "该论文的创新点在于将不确定性量化与下游的机器人抓取任务直接结合，通过预测抓取成功率来避免任务失败，这对于提高机器人抓取系统的鲁棒性具有重要意义。其提出的轻量级网络训练方法和对联合训练多样化物体效果的发现，为实际应用提供了指导。"}}
{"id": "2506.20217", "title": "Ten simple rules for PIs to integrate Research Software Engineering into their research group", "authors": ["Stuart M. Allen", "Neil Chue Hong", "Stephan Druskat", "Toby Hodges", "Daniel S. Katz", "Jan Linxweiler", "Frank Löffler", "Lars Grunske", "Heidi Seibold", "Jan Philipp Thiele", "Samantha Wittke"], "summary": "Research Software Engineering (RSEng) is a key success factor in producing\nhigh-quality research software, which in turn enables and improves research\noutcomes. However, as a principal investigator or leader of a research group\nyou may not know what RSEng is, where to get started with it, or how to use it\nto maximize its benefit for your research. RSEng also often comes with\ntechnical complexity, and therefore reduced accessibility to some researchers.\nThe ten simple rules presented in this paper aim to improve the accessibility\nof RSEng, and provide practical and actionable advice to PIs and leaders for\nintegrating RSEng into their research group. By following these rules, readers\ncan improve the quality, reproducibility, and trustworthiness of their research\nsoftware, ultimately leading to better, more reproducible and more trustworthy\nresearch outcomes.", "comment": "10 pages, submitted to PLOS Computational Biology", "pdf_url": "http://arxiv.org/pdf/2506.20217v1", "categories": ["cs.SE", "cs.CE", "cs.CY", "68-01", "K.6.3"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20217v1", "AI": {"title_translation": "PI将研究软件工程整合到其研究团队的十大简单规则", "tldr": "本文为PIs提供了将研究软件工程(RSEng)整合到其研究团队的十条简单规则，旨在提高研究软件的质量和研究成果。", "motivation": "研究软件工程(RSEng)是高质量研究软件生产的关键成功因素，但PIs可能不了解RSEng是什么、如何开始或如何最大化其效益。RSEng的技术复杂性也降低了其可及性。", "method": "本文提出了十条简单规则，旨在提高RSEng的可及性，并为PIs和团队负责人提供将RSEng整合到其研究团队的实用且可操作的建议。", "result": "遵循这些规则，读者可以提高其研究软件的质量、可再现性和可信度，最终带来更好、更可再现和更可信的研究成果。", "conclusion": "通过遵循本文提出的十条简单规则，PIs可以有效地将研究软件工程整合到其团队中，从而显著提升研究软件的质量、可再现性和可信度，最终促进更高质量的研究成果。", "translation": "研究软件工程（RSEng）是生产高质量研究软件的关键成功因素，而高质量研究软件反过来又能促进和改善研究成果。然而，作为研究组的首席研究员或负责人，你可能不了解RSEng是什么，从何开始，或者如何利用它来最大限度地发挥其对你研究的益处。RSEng也常常伴随着技术复杂性，因此降低了对一些研究人员的可及性。本文提出的十条简单规则旨在提高RSEng的可及性，并为PIs和负责人提供将RSEng整合到其研究组的实用且可操作的建议。通过遵循这些规则，读者可以提高其研究软件的质量、可再现性和可信度，最终带来更好、更可再现和更可信的研究成果。", "summary": "本文针对首席研究员（PIs）和研究团队负责人，提出了十条简单规则，旨在帮助他们将研究软件工程（RSEng）有效整合到其研究工作中。鉴于PIs可能不熟悉RSEng的入门方法或如何最大化其效益，且RSEng存在技术复杂性，这些规则旨在提高RSEng的可及性。通过采纳这些指南，研究团队可以显著提升其研究软件的质量、可再现性和可信度，从而最终改善整体研究成果。", "keywords": "研究软件工程, 首席研究员, 研究团队, 软件质量, 可再现性", "comments": "该论文切中了现代研究中一个日益重要且实际的问题：对高质量研究软件的需求。其“十大简单规则”的格式表明了一种实用、易于理解的方法，这对于可能没有软件工程背景的PIs来说非常有价值。专注于可及性和可操作性建议是其主要优势。"}}
{"id": "2506.20103", "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos", "authors": ["Jiahao Lin", "Weixuan Peng", "Bojia Zi", "Yifeng Gao", "Xianbiao Qi", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "Recent advances in deep generative models have led to significant progress in\nvideo generation, yet the fidelity of AI-generated videos remains limited.\nSynthesized content often exhibits visual artifacts such as temporally\ninconsistent motion, physically implausible trajectories, unnatural object\ndeformations, and local blurring that undermine realism and user trust.\nAccurate detection and spatial localization of these artifacts are crucial for\nboth automated quality control and for guiding the development of improved\ngenerative models. However, the research community currently lacks a\ncomprehensive benchmark specifically designed for artifact localization in AI\ngenerated videos. Existing datasets either restrict themselves to video or\nframe level detection or lack the fine-grained spatial annotations necessary\nfor evaluating localization methods. To address this gap, we introduce\nBrokenVideos, a benchmark dataset of 3,254 AI-generated videos with\nmeticulously annotated, pixel-level masks highlighting regions of visual\ncorruption. Each annotation is validated through detailed human inspection to\nensure high quality ground truth. Our experiments show that training state of\nthe art artifact detection models and multi modal large language models (MLLMs)\non BrokenVideos significantly improves their ability to localize corrupted\nregions. Through extensive evaluation, we demonstrate that BrokenVideos\nestablishes a critical foundation for benchmarking and advancing research on\nartifact localization in generative video models. The dataset is available at:\nhttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.", "comment": "7 page,4 figures,2 tables", "pdf_url": "http://arxiv.org/pdf/2506.20103v1", "categories": ["cs.CV", "cs.AI", "I.4"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20103v1", "AI": {"title_translation": "BrokenVideos：一个用于AI生成视频中细粒度伪影定位的基准数据集", "tldr": "BrokenVideos是一个新的基准数据集，包含3254个带有像素级伪影标注的AI生成视频，旨在解决现有数据集中缺乏细粒度空间标注的问题，并显著提高了最先进模型在定位损坏区域方面的能力。", "motivation": "尽管深度生成模型在视频生成方面取得了显著进展，但AI生成视频的保真度仍然有限，经常出现视觉伪影，如时间不一致的运动、不合物理的轨迹、不自然的物体变形和局部模糊，这些都损害了真实感和用户信任。目前研究社区缺乏专门用于AI生成视频中伪影定位的全面基准数据集，现有数据集要么局限于视频或帧级别检测，要么缺乏评估定位方法所需的细粒度空间标注。", "method": "为了弥补这一空白，研究者引入了BrokenVideos数据集，这是一个包含3254个AI生成视频的基准数据集，其中包含经过精心标注的像素级掩码，突出显示了视觉损坏区域。每个标注都经过详细的人工检查验证，以确保高质量的真值。", "result": "实验表明，在BrokenVideos数据集上训练最先进的伪影检测模型和多模态大型语言模型（MLLMs），显著提高了它们定位损坏区域的能力。", "conclusion": "BrokenVideos数据集为生成视频模型中伪影定位的基准测试和研究进展奠定了关键基础。", "translation": "深度生成模型的最新进展使得视频生成取得了显著进步，然而，AI生成视频的保真度仍然有限。合成内容经常表现出视觉伪影，例如时间不一致的运动、不合物理的轨迹、不自然的物体变形和局部模糊，这些都损害了真实感和用户信任。对这些伪影进行准确的检测和空间定位对于自动化质量控制和指导改进生成模型的开发都至关重要。然而，研究社区目前缺乏一个专门用于AI生成视频中伪影定位的综合基准。现有数据集要么局限于视频或帧级别的检测，要么缺乏评估定位方法所需的细粒度空间标注。为了弥补这一空白，我们引入了BrokenVideos，这是一个包含3254个AI生成视频的基准数据集，其中包含经过精心标注的像素级掩码，突出显示了视觉损坏区域。每个标注都通过详细的人工检查进行验证，以确保高质量的真值。我们的实验表明，在BrokenVideos上训练最先进的伪影检测模型和多模态大型语言模型（MLLMs）显著提高了它们定位损坏区域的能力。通过广泛评估，我们证明了BrokenVideos为生成视频模型中伪影定位的基准测试和研究进展奠定了关键基础。该数据集可在此处获取：https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/。", "summary": "AI生成视频中普遍存在影响真实感的视觉伪影，但目前缺乏用于细粒度伪影定位的综合基准数据集。为解决此问题，本文引入了BrokenVideos数据集，一个包含3254个AI生成视频的基准，其特色在于细致的像素级伪影区域标注，并经过人工验证。实验证明，利用BrokenVideos训练现有最先进的模型能显著提升其定位损坏区域的能力。该数据集为推进生成视频模型中伪影定位的研究提供了关键基础。", "keywords": "AI生成视频, 伪影定位, 基准数据集, 像素级标注, 视觉损坏", "comments": "该论文的创新之处在于构建了一个具有细粒度、像素级标注的AI生成视频伪影数据集，填补了现有研究在这一领域的空白。其重要性体现在为AI生成内容的质量控制提供了精确的评估工具，同时也为优化生成模型、提高视频真实感提供了宝贵的训练资源。该数据集有望成为未来伪影定位研究的基准。"}}
{"id": "2506.19890", "title": "Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction", "authors": ["Ziru Zhang", "Jiadong Yu", "Danny H. K. Tsang"], "summary": "The optimization of quality of experience (QoE) in multi-user virtual reality\n(VR) interactions demands a delicate balance between ultra-low latency,\nhigh-fidelity motion synchronization, and equitable resource allocation. While\nadaptive keyframe extraction mitigates transmission overhead, existing\napproaches often overlook the causal relationships among allocated bandwidth,\nCPU frequency, and user perception, limiting QoE gains. This paper proposes an\nintelligent framework to maximize QoE by integrating adaptive keyframe\nextraction with causal-aware reinforcement learning (RL). First, a novel QoE\nmetric is formulated using the Weber-Fechner Law, combining perceptual\nsensitivity, attention-driven priorities, and motion reconstruction accuracy.\nThe QoE optimization problem is then modeled as a mixed integer programming\n(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational\nresources under horizon-fairness constraints. We propose Partial State Causal\nDeep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep\nDeterministic Policy Gradient (DDPG) method with causal influence detection. By\nleveraging causal information regarding how QoE is influenced and determined by\nvarious actions, we explore actions guided by weights calculated from causal\ninference (CI), which in turn improves training efficiency. Experiments\nconducted with the CMU Motion Capture Database demonstrate that our framework\nsignificantly reduces interactive latency, enhances QoE, and maintains\nfairness, achieving superior performance compared to benchmark methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19890v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19890v1", "AI": {"title_translation": "基于因果感知的VR交互智能QoE优化与自适应关键帧提取", "tldr": "针对VR交互QoE优化，本文提出结合自适应关键帧提取和因果感知强化学习的智能框架，通过新的QoE度量和PS-CDDPG方法，显著降低延迟，提升QoE并保持公平性。", "motivation": "现有VR交互QoE优化方法在自适应关键帧提取时，常忽略带宽、CPU频率和用户感知之间的因果关系，限制了QoE的提升，且难以平衡超低延迟、高保真运动同步和公平资源分配。", "method": "本文提出了一个智能框架，通过结合自适应关键帧提取和因果感知强化学习来最大化QoE。首先，利用Weber-Fechner定律构建了一个新的QoE度量，结合感知灵敏度、注意力驱动优先级和运动重建精度。然后，将QoE优化问题建模为混合整数规划（MIP）任务，在水平公平性约束下联合优化关键帧比率、带宽和计算资源。最后，提出了偏状态因果深度确定性策略梯度（PS-CDDPG）方法，将深度确定性策略梯度（DDPG）与因果影响检测相结合，利用因果信息指导动作探索，提高训练效率。", "result": "实验证明，该框架显著降低了交互延迟，增强了QoE，并保持了公平性，与基准方法相比表现出卓越的性能。", "conclusion": "本文提出的结合因果感知强化学习和自适应关键帧提取的智能框架，通过有效利用因果信息，成功解决了VR交互中的QoE优化挑战，实现了性能的显著提升和公平性保障。", "translation": "多用户虚拟现实（VR）交互中的体验质量（QoE）优化需要在超低延迟、高保真运动同步和公平资源分配之间实现微妙的平衡。尽管自适应关键帧提取可以减轻传输开销，但现有方法往往忽略了分配带宽、CPU频率和用户感知之间的因果关系，从而限制了QoE的提升。本文提出了一种智能框架，通过将自适应关键帧提取与因果感知强化学习（RL）相结合来最大化QoE。首先，利用Weber-Fechner定律制定了一种新颖的QoE度量，该度量结合了感知灵敏度、注意力驱动优先级和运动重建精度。然后，将QoE优化问题建模为混合整数规划（MIP）任务，在水平公平性约束下联合优化关键帧比率、带宽和计算资源。我们提出了偏状态因果深度确定性策略梯度（PS-CDDPG），它将深度确定性策略梯度（DDPG）方法与因果影响检测相结合。通过利用QoE如何受到各种动作影响和决定的因果信息，我们探索了由因果推断（CI）计算的权重指导的动作，这反过来又提高了训练效率。使用CMU运动捕捉数据库进行的实验表明，我们的框架显著降低了交互延迟，增强了QoE，并保持了公平性，与基准方法相比实现了卓越的性能。", "summary": "本文提出一个智能框架，旨在通过结合自适应关键帧提取和因果感知强化学习来优化多用户VR交互中的QoE。该框架首先基于Weber-Fechner定律定义了新的QoE度量，并将QoE优化建模为MIP问题。为解决此问题，引入了PS-CDDPG方法，该方法通过整合因果信息来指导强化学习过程，从而提高训练效率。实验结果表明，该方法在降低延迟、提升QoE和保持公平性方面优于现有基准方法。", "keywords": "VR QoE优化, 因果感知强化学习, 自适应关键帧提取, PS-CDDPG, Weber-Fechner定律", "comments": "该论文的创新之处在于将因果感知强化学习与自适应关键帧提取相结合，以优化VR交互中的QoE。通过引入因果关系分析，能够更精准地理解和利用带宽、CPU频率和用户感知之间的相互作用，从而克服了现有方法忽略因果关系导致QoE提升受限的问题。新颖的QoE度量和PS-CDDPG方法提高了优化效率和效果，对于提升VR体验具有重要意义。"}}
{"id": "2506.20009", "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "authors": ["Konstantinos Vrettos", "Michail E. Klontzas"], "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals.", "comment": "18 pages, 3 Figures", "pdf_url": "http://arxiv.org/pdf/2506.20009v1", "categories": ["cs.AI", "cs.CL", "I.2.7"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20009v1", "AI": {"title_translation": "准确且节能：本地检索增强生成模型在医疗任务中超越商业大型语言模型", "tldr": "本地RAG模型在医疗任务中比商业LLM更准确、更节能，并且对环境影响更小。", "motivation": "背景：人工智能在医疗领域的应用日益增长，但商业大型语言模型（LLMs）如ChatGPT和DeepSeek需要大量资源，且在医疗用途中引发患者隐私和安全问题。研究旨在开发更环保、更安全的医疗AI解决方案。", "method": "研究开发了一个可定制的医疗任务检索增强生成（RAG）框架，并监控其能耗和碳排放。该系统用于构建基于不同开源LLM（包括通用模型llama3.1:8b和特定医疗领域模型medgemma-4b-it）的RAG模型。将最佳RAG模型的性能和能耗与商业模型DeepSeekV3-R1和OpenAI的o4-mini模型进行比较，评估数据集为医疗问题。", "result": "定制RAG模型在准确性和能耗方面均优于商业模型。基于llama3.1:8B的RAG模型实现了最高准确率（58.5%），显著优于o4-mini和DeepSeekV3-R1等模型。llama3.1-RAG在所有模型中能耗和碳足迹最低，每千瓦时性能为0.52，总碳排放量为473克。与o4-mini相比，llama3.1-RAG每千瓦时准确率点数高出2.7倍，用电量减少172%，同时保持更高的准确率。", "conclusion": "本研究表明，可以利用本地LLM开发RAG模型，这些模型在医疗任务中表现优于商业在线LLM，同时对环境影响更小。该模块化框架促进了可持续AI发展，减少了用电量，并符合联合国可持续发展目标。", "translation": "背景：人工智能（AI）在医疗保健领域日益普及，引发了对其环境和伦理影响日益增长的担忧。ChatGPT和DeepSeek等商业大型语言模型（LLMs）需要大量资源，而将这些系统用于医疗目的则引发了有关患者隐私和安全的关键问题。方法：我们开发了一个可定制的用于医疗任务的检索增强生成（RAG）框架，该框架监控其能源使用和二氧化碳排放。然后，该系统用于创建基于各种开源LLM的RAG。测试的模型包括通用模型如llama3.1:8b和特定医疗领域模型medgemma-4b-it。最佳RAG的性能和能耗与DeepSeekV3-R1和OpenAIs o4-mini模型进行了比较。评估使用了医疗问题数据集。结果：定制的RAG模型在准确性和能耗方面均优于商业模型。基于llama3.1:8B的RAG模型实现了最高准确率（58.5%），并且显著优于其他模型，包括o4-mini和DeepSeekV3-R1。llama3.1-RAG在所有模型中也表现出最低的能耗和二氧化碳足迹，每千瓦时性能为0.52，总二氧化碳排放量为473克。与o4-mini相比，llama3.1-RAG每千瓦时实现了2.7倍的准确率提升，用电量减少了172%，同时保持了更高的准确率。结论：我们的研究表明，可以利用本地LLM开发RAG模型，这些模型在医疗任务中表现优于商业在线LLM，同时对环境影响更小。我们的模块化框架促进了可持续AI发展，减少了用电量，并符合联合国可持续发展目标。", "summary": "本研究开发了一个可定制的检索增强生成（RAG）框架，用于医疗任务，旨在解决商业大型语言模型（LLMs）在医疗领域的高资源消耗和隐私问题。通过比较基于开源LLM（如llama3.1:8b）构建的本地RAG模型与商业LLM（如DeepSeekV3-R1和o4-mini），结果显示本地RAG模型在医疗任务的准确性和能耗方面均表现更优，特别是基于llama3.1:8B的模型，其准确率最高且碳足迹最低。研究强调本地RAG模型在医疗应用中的潜力，能够提供高性能的同时，促进更可持续的AI发展。", "keywords": "检索增强生成, 医疗AI, 大型语言模型, 能源效率, 可持续AI", "comments": "这篇论文的创新点在于提出了一个可定制的本地RAG框架，并明确量化了其在医疗任务中相对于商业LLM在准确性和能耗方面的优势。它不仅解决了AI在医疗领域应用中的隐私和安全担忧，还积极响应了当前对AI环境影响的关注，与联合国可持续发展目标相契合，具有重要的实践意义和前瞻性。"}}
{"id": "2506.19877", "title": "Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017", "authors": ["Zhaoyang Xu", "Yunbo Liu"], "summary": "Identifying suitable machine learning paradigms for intrusion detection\nremains critical for building effective and generalizable security solutions.\nIn this study, we present a controlled comparison of four representative models\n- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),\nOne-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on\nthe CICIDS2017 dataset under two scenarios: detecting known attack types and\ngeneralizing to previously unseen threats. Our results show that supervised MLP\nand CNN achieve near-perfect accuracy on familiar attacks but suffer drastic\nrecall drops on novel attacks. Unsupervised LOF attains moderate overall\naccuracy and high recall on unknown threats at the cost of elevated false\nalarms, while boundary-based OCSVM balances precision and recall best,\ndemonstrating robust detection across both scenarios. These findings offer\npractical guidance for selecting IDS models in dynamic network environments.", "comment": "submitted to IEEE CNS 2025", "pdf_url": "http://arxiv.org/pdf/2506.19877v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19877v1", "AI": {"title_translation": "网络流量中的鲁棒异常检测：评估CICIDS2017数据集上的机器学习模型", "tldr": "研究比较了MLP、CNN、OCSVM和LOF在CICIDS2017数据集上对已知和未知攻击的检测能力，发现OCSVM在两种场景下表现最均衡和鲁棒。", "motivation": "构建有效且可泛化的安全解决方案，识别适用于入侵检测的机器学习范式至关重要。", "method": "研究在CICIDS2017数据集上，对四种代表性模型（MLP, 1D CNN, OCSVM, LOF）在两种场景下进行了受控比较：检测已知攻击类型和泛化到以前未见的威胁。", "result": "监督式MLP和CNN在已知攻击上达到近乎完美的准确率，但在新攻击上召回率大幅下降。无监督LOF在未知威胁上获得中等整体准确率和高召回率，但误报率较高。基于边界的OCSVM在精度和召回率之间达到最佳平衡，在两种场景下均表现出鲁棒检测能力。", "conclusion": "这些发现为在动态网络环境中选择IDS模型提供了实用指导。", "translation": "识别适用于入侵检测的机器学习范式对于构建有效且可泛化的安全解决方案至关重要。在这项研究中，我们对四种代表性模型——多层感知器（MLP）、一维卷积神经网络（CNN）、单类支持向量机（OCSVM）和局部异常因子（LOF）——在CICIDS2017数据集上进行了受控比较，分为两种场景：检测已知攻击类型和泛化到以前未见的威胁。我们的结果表明，监督式MLP和CNN在熟悉攻击上实现了近乎完美的准确率，但在新型攻击上召回率急剧下降。无监督LOF在未知威胁上获得了中等的整体准确率和高召回率，但代价是误报率升高，而基于边界的OCSVM在精度和召回率之间达到了最佳平衡，在两种场景下都表现出鲁棒的检测能力。这些发现为在动态网络环境中选择IDS模型提供了实用指导。", "summary": "本研究在CICIDS2017数据集上比较了MLP、CNN、OCSVM和LOF四种机器学习模型在网络入侵检测中的性能，涵盖已知攻击检测和未知威胁泛化两种场景。结果显示，MLP和CNN在已知攻击上表现优异但在未知攻击上召回率低；LOF对未知威胁召回率高但误报率高；OCSVM则在两种场景下平衡了精度和召回率，表现出最佳鲁棒性。研究为IDS模型选择提供了实践指导。", "keywords": "异常检测, 机器学习, 入侵检测, CICIDS2017, OCSVM", "comments": "该研究通过对多种机器学习模型在已知和未知攻击场景下的对比评估，为网络入侵检测系统（IDS）的模型选择提供了明确的实践指导。其创新点在于强调了模型在“泛化到以前未见威胁”能力上的差异，并指出OCSVM在鲁棒性方面的优势，这对于动态网络环境下的实际部署具有重要意义。"}}
{"id": "2506.20091", "title": "From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive Multi-Agentic Systems", "authors": ["Sarah Schömbs", "Yan Zhang", "Jorge Goncalves", "Wafa Johal"], "summary": "Recent advances in multi-agentic systems (e.g. AutoGen, OpenAI Swarm) allow\nusers to interact with a group of specialised AI agents rather than a single\ngeneral-purpose agent. Despite the promise of this new paradigm, the HCI\ncommunity has yet to fully examine the opportunities, risks, and user-centred\nchallenges it introduces. We contribute to research on multi-agentic systems by\nexploring their architectures and key features through a human-centred lens.\nWhile literature and use cases remain limited, we build on existing tools and\nframeworks available to developers to identify a set of overarching challenges,\ne.g. orchestration and conflict resolution, that can guide future research in\nHCI. We illustrate these challenges through examples, offer potential design\nconsiderations, and provide research opportunities to spark interdisciplinary\nconversation. Our work lays the groundwork for future exploration and offers a\nresearch agenda focused on user-centred design in multi-agentic systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20091v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20091v1", "AI": {"title_translation": "从对话到编排：交互式多智能体系统中的人机交互挑战与机遇", "tldr": "本文探讨了多智能体系统中的人机交互挑战和机遇，旨在为未来研究提供指导，并提出以用户为中心的设计议程。", "motivation": "尽管多智能体系统（如AutoGen、OpenAI Swarm）的最新进展为用户提供了与专业AI代理组而非单一通用代理交互的新范式，但人机交互社区尚未充分研究其带来的机遇、风险和以用户为中心的挑战。", "method": "作者通过以人为中心的视角，探索了多智能体系统的架构和关键特性。尽管现有文献和用例有限，但作者基于开发者可用的现有工具和框架，识别了一系列主要挑战，例如编排和冲突解决。", "result": "本文通过示例阐释了这些挑战，提出了潜在的设计考虑，并提供了研究机会以激发跨学科对话。", "conclusion": "本研究为未来探索奠定了基础，并提出了一个以用户为中心的多智能体系统设计研究议程。", "translation": "多智能体系统（例如 AutoGen、OpenAI Swarm）的最新进展允许用户与一组专业的 AI 代理而非单个通用代理进行交互。尽管这种新范式前景广阔，但人机交互社区尚未充分审视其带来的机遇、风险和以用户为中心的挑战。我们通过以人为本的视角探索多智能体系统的架构和关键特性，为多智能体系统研究做出贡献。尽管文献和用例仍然有限，我们仍基于开发者可用的现有工具和框架，识别了一系列主要挑战，例如编排和冲突解决，这些挑战可以指导未来的人机交互研究。我们通过示例阐释了这些挑战，提供了潜在的设计考虑，并提供了研究机会以激发跨学科对话。我们的工作为未来的探索奠定了基础，并提供了一个专注于多智能体系统中以用户为中心设计的研究议程。", "summary": "本文探讨了交互式多智能体系统中的人机交互挑战与机遇。鉴于多智能体系统作为新兴范式的重要性，作者从以人为中心的视角审视了其架构和特性。通过分析现有工具和框架，论文识别了编排和冲突解决等关键挑战，并通过示例进行阐释，同时提出了设计考虑和研究方向。最终，该工作为未来以用户为中心的多智能体系统设计研究奠定了基础。", "keywords": "多智能体系统, 人机交互, 编排, 用户中心设计, AI代理", "comments": "本文及时地关注了多智能体系统这一新兴且重要的人机交互领域，超越了传统的单智能体交互范式。其强调以用户为中心的设计理念，并识别出编排和冲突解决等核心挑战，对于指导未来跨学科研究具有重要价值和创新性。"}}
{"id": "2506.20383", "title": "A Detailed Measurement View on IPv6 Scanners and Their Adaption to BGP Signals", "authors": ["Isabell Egloff", "Raphael Hiesgen", "Maynard Koch", "Thomas C. Schmidt", "Matthias Wählisch"], "summary": "Scanners are daily visitors of public IPv4 hosts. Scanning IPv6 nodes\nsuccessfully is still a challenge, which an increasing crowd of actors tries to\nmaster. In this paper, we analyze current IPv6 scanning under various network\nconditions. We observe scanner behavior during eleven months in four network\ntelescopes, one of which is periodically reconfigured by changing BGP\nannouncements. We analyze and classify the observed scanners w.r.t. their\ntemporal behavior, their target, and network selection strategy, as well as\ntheir individual tools, fingerprints, and correlations across categories. We\nfind that silent subnets of larger prefixes remain invisible, whereas BGP\nprefix announcements quickly attract attention by scanners. Based on our\nfindings, we derive operational guidance on how to deploy network telescopes to\nincrease visibility of IPv6 scanners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20383v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.20383v1", "AI": {"title_translation": "IPv6扫描器及其对BGP信号适应性的详细测量视图", "tldr": "本研究详细测量了IPv6扫描器的行为及其对BGP信号的适应性，发现BGP前缀通告能迅速吸引扫描器，并据此提供了部署网络望远镜以提高IPv6扫描器可见性的操作指导。", "motivation": "成功扫描IPv6节点仍然是一个挑战，但越来越多的参与者正试图掌握它。本研究旨在分析当前IPv6扫描在各种网络条件下的行为。", "method": "研究人员在11个月内通过四个网络望远镜观察扫描器行为，其中一个通过改变BGP通告进行周期性重新配置。他们分析并分类了观察到的扫描器，包括其时间行为、目标、网络选择策略、个别工具、指纹以及跨类别的关联。", "result": "研究发现，较大前缀中“静默”的子网仍然不可见，而BGP前缀通告能迅速吸引扫描器的注意。", "conclusion": "基于研究发现，论文提供了关于如何部署网络望远镜以提高IPv6扫描器可见性的操作指导。", "translation": "扫描器是公共IPv4主机的日常访问者。成功扫描IPv6节点仍然是一个挑战，越来越多的参与者正试图掌握它。在本文中，我们分析了当前IPv6扫描在各种网络条件下的行为。我们在四个网络望远镜中观察了扫描器行为长达十一个月，其中一个通过改变BGP通告进行周期性重新配置。我们对观察到的扫描器进行了分析和分类，涉及它们的时间行为、目标和网络选择策略，以及它们的个体工具、指纹和跨类别关联。我们发现，较大前缀中“静默”的子网仍然不可见，而BGP前缀通告能迅速吸引扫描器的注意。基于我们的发现，我们得出了关于如何部署网络望远镜以提高IPv6扫描器可见性的操作指导。", "summary": "本研究详细测量并分析了IPv6扫描器的行为及其对BGP信号的适应性。通过在多个网络望远镜中长达11个月的观察，包括一个受BGP通告影响的望远镜，研究人员对扫描器进行了分类。主要发现是BGP前缀通告能迅速吸引扫描器，而未通告的子网则保持隐蔽。基于这些发现，论文提供了提高IPv6扫描器可见性的操作指南。", "keywords": "IPv6扫描, BGP信号, 网络望远镜, 扫描器行为, 网络安全", "comments": "这篇论文的创新点在于其详细且长时间的IPv6扫描器行为测量，特别是引入BGP信号变化来观察扫描器的适应性。这对于理解IPv6网络安全态势和防御策略具有重要意义。所提供的操作指导也具有实际应用价值。"}}
{"id": "2506.20262", "title": "Efficient Feedback Design for Unsourced Random Access with Integrated Sensing and Communication", "authors": ["Mohammad Javad Ahmadi", "Mohammad Kazemi", "Rafael F. Schaefer"], "summary": "We consider an unsourced random access (URA) system enhanced with a feedback\nmechanism that serves both communication and sensing tasks. While traditional\nURA systems do not incorporate feedback, we propose a novel feedback signal\ndesign that announces the decoding status of users and simultaneously enables\ntarget sensing. To design this dual-purpose feedback, we introduce a modified\nprojected gradient descent algorithm that minimizes a weighted combination of\ncommunication and sensing errors. Simulation results show that the proposed\nfeedback design outperforms the state-of-the-art feedback design in the URA\nliterature. Furthermore, we illustrate the trade-off between communication and\nsensing capabilities, offering valuable insight into balancing these two tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20262v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20262v1", "AI": {"title_translation": "集成感知与通信的非源随机接入高效反馈设计", "tldr": "本文提出了一种新颖的反馈信号设计，用于增强型非源随机接入（URA）系统，该设计同时服务于通信和感知任务，并通过改进的投影梯度下降算法优化，在通信和感知误差之间取得平衡，并优于现有技术。", "motivation": "传统非源随机接入（URA）系统不包含反馈机制，但本文考虑一个增强型URA系统，其反馈机制同时服务于通信和感知任务。因此，研究动机是设计一种新颖的、双用途的反馈信号。", "method": "本文提出了一种新颖的反馈信号设计，该设计既能宣布用户解码状态，又能实现目标感知。为实现这种双用途反馈，引入了一种改进的投影梯度下降算法，该算法旨在最小化通信和感知错误的加权组合。", "result": "仿真结果表明，所提出的反馈设计在非源随机接入文献中优于现有最先进的反馈设计。此外，研究还展示了通信和感知能力之间的权衡，为平衡这两项任务提供了有价值的见解。", "conclusion": "本文提出了一种高效的反馈设计，用于增强型非源随机接入系统，该系统能同时支持通信和感知任务。该设计通过优化方法在通信和感知之间实现了平衡，并表现出优于现有技术的性能。", "translation": "我们考虑一个增强了反馈机制的非源随机接入（URA）系统，该机制同时服务于通信和感知任务。虽然传统的URA系统不包含反馈，但我们提出了一种新颖的反馈信号设计，它既能宣布用户的解码状态，又能同时实现目标感知。为了设计这种双用途反馈，我们引入了一种改进的投影梯度下降算法，该算法旨在最小化通信和感知错误的加权组合。仿真结果表明，所提出的反馈设计在URA文献中优于现有最先进的反馈设计。此外，我们还阐述了通信和感知能力之间的权衡，为平衡这两项任务提供了有价值的见解。", "summary": "本文提出了一种用于非源随机接入（URA）系统的新型反馈信号设计，该设计集成了通信和感知功能。通过引入一种改进的投影梯度下降算法，该设计能够最小化通信和感知错误，并在两者之间进行权衡。仿真结果表明，该方法在性能上超越了现有技术。", "keywords": "非源随机接入, 反馈设计, 集成感知与通信, 投影梯度下降, 通信感知权衡", "comments": "本文的创新点在于将反馈机制引入传统的非源随机接入系统，并使其同时服务于通信和感知两个任务，实现了功能集成。通过提出一种双用途反馈信号设计和改进的优化算法，有效提升了系统性能，并深入探讨了通信和感知之间的权衡关系，这对于未来集成式通信与感知系统的设计具有重要指导意义。"}}
{"id": "2506.20535", "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads", "authors": ["Hongzhen Huang", "Kunming Zhang", "Hanlong Liao", "Kui Wu", "Guoming Tang"], "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.", "comment": "11 pages, 7 figures and 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.20535v1", "categories": ["cs.DC", "cs.AI", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20535v1", "AI": {"title_translation": "WattsOnAI：测量、分析和可视化人工智能工作负载的能源和碳足迹", "tldr": "WattsOnAI是一个全面的工具包，用于测量、分析和可视化AI工作负载的能源消耗和碳足迹，以促进可持续的“绿色AI”实践。", "motivation": "随着AI（特别是大型语言模型）的快速发展，其训练和推理过程中的能源消耗和碳排放引起了广泛关注。然而，现有工具分散，缺乏系统性的指标整合，且对相关性分析支持有限。", "method": "本文提出了WattsOnAI，一个全面的软件工具包，用于测量、分析和可视化AI工作负载的能源使用、功耗、硬件性能和碳排放。它与现有AI框架无缝集成，提供标准化报告并导出细粒度时间序列数据。", "result": "WattsOnAI支持基准测试和可重现性，并能对硬件指标和模型性能进行深入的相关性分析，从而有助于识别瓶颈并提高性能。", "conclusion": "WattsOnAI通过解决现有工具的局限性，鼓励研究社区在评估AI工作负载时兼顾环境影响和原始性能，从而推动向更可持续的“绿色AI”实践转变。", "translation": "人工智能，特别是大型语言模型（LLMs）的快速发展，引发了对模型训练和推理相关能耗及碳排放的重大担忧。然而，现有用于测量和报告这些影响的工具通常是分散的，缺乏系统性的指标整合，并且对它们之间的相关性分析支持有限。本文提出了WattsOnAI，一个用于测量、分析和可视化人工智能工作负载的能源使用、功耗、硬件性能和碳排放的综合软件工具包。通过与现有AI框架无缝集成，WattsOnAI以轻量级的方式提供标准化报告并导出细粒度的时间序列数据，以支持基准测试和可重现性。它进一步实现了硬件指标与模型性能之间的深入相关性分析，从而促进了瓶颈识别和性能提升。通过解决现有工具中的关键局限性，WattsOnAI鼓励研究社区在考量AI工作负载的原始性能的同时，也权衡其环境影响，并推动向更可持续的“绿色AI”实践转变。代码可在https://github.com/SusCom-Lab/WattsOnAI获取。", "summary": "WattsOnAI是一个针对AI工作负载的综合软件工具包，旨在解决AI能源消耗和碳排放测量工具的局限性。它能够测量、分析和可视化能源使用、功耗、硬件性能和碳排放，并支持与现有AI框架集成。该工具提供标准化报告、细粒度数据导出以及硬件指标与模型性能之间的相关性分析，从而促进基准测试、可重现性、瓶颈识别和性能优化。WattsOnAI旨在推动“绿色AI”实践，鼓励研究者在评估AI性能时考虑环境影响。", "keywords": "AI能源效率, 碳足迹, 绿色AI, 性能分析, WattsOnAI", "comments": "WattsOnAI的创新在于其全面性，它将能源使用、碳排放与硬件性能、模型性能进行整合分析，并支持相关性分析，这对于识别优化点至关重要。其重要性在于推动了“绿色AI”的发展，鼓励研究者在追求AI性能的同时，关注其环境足迹。该工具的轻量级设计和与现有框架的集成也提升了其可用性。"}}
{"id": "2506.20190", "title": "An Exploration of ECAPA-TDNN and x-vector Speaker Representations in Zero-shot Multi-speaker TTS", "authors": ["Marie Kunešová", "Zdeněk Hanzlíček", "Jindřich Matoušek"], "summary": "Zero-shot multi-speaker text-to-speech (TTS) systems rely on speaker\nembeddings to synthesize speech in the voice of an unseen speaker, using only a\nshort reference utterance. While many speaker embeddings have been developed\nfor speaker recognition, their relative effectiveness in zero-shot TTS remains\nunderexplored. In this work, we employ a YourTTS-based TTS system to compare\nthree different speaker encoders - YourTTS's original H/ASP encoder, x-vector\nembeddings, and ECAPA-TDNN embeddings - within an otherwise fixed zero-shot TTS\nframework. All models were trained on the same dataset of Czech read speech and\nevaluated on 24 out-of-domain target speakers using both subjective and\nobjective methods. The subjective evaluation was conducted via a listening test\nfocused on speaker similarity, while the objective evaluation measured cosine\ndistances between speaker embeddings extracted from synthesized and real\nutterances. Across both evaluations, the original H/ASP encoder consistently\noutperformed the alternatives, with ECAPA-TDNN showing better results than\nx-vectors. These findings suggest that, despite the popularity of ECAPA-TDNN in\nspeaker recognition, it does not necessarily offer improvements for speaker\nsimilarity in zero-shot TTS in this configuration. Our study highlights the\nimportance of empirical evaluation when reusing speaker recognition embeddings\nin TTS and provides a framework for additional future comparisons.", "comment": "Accepted to TSD 2025", "pdf_url": "http://arxiv.org/pdf/2506.20190v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.20190v1", "AI": {"title_translation": "零样本多说话人TTS中ECAPA-TDNN和x-vector说话人表征的探索", "tldr": "本研究在零样本TTS中比较了YourTTS的H/ASP、ECAPA-TDNN和x-vector三种说话人嵌入的有效性。结果显示，H/ASP编码器表现最佳，ECAPA-TDNN优于x-vector，但未能在零样本TTS的说话人相似度上超越H/ASP。", "motivation": "零样本多说话人文本到语音（TTS）系统依赖说话人嵌入来合成未见说话人的语音，但用于说话人识别的现有说话人嵌入在零样本TTS中的相对有效性仍未得到充分探索。", "method": "研究采用基于YourTTS的TTS系统，在固定零样本TTS框架内，比较了YourTTS的原始H/ASP编码器、x-vector嵌入和ECAPA-TDNN嵌入。模型在捷克语朗读语音数据集上训练，并对24位域外目标说话人进行主观（听力测试关注说话人相似度）和客观（测量合成语音与真实语音之间说话人嵌入的余弦距离）评估。", "result": "在主观和客观评估中，原始H/ASP编码器始终优于ECAPA-TDNN和x-vector。ECAPA-TDNN表现优于x-vector。尽管ECAPA-TDNN在说话人识别中流行，但在本研究配置下，它并未在零样本TTS的说话人相似度方面带来改进。", "conclusion": "研究强调了在TTS中重用说话人识别嵌入时进行经验评估的重要性，并为未来的额外比较提供了一个框架。", "translation": "零样本多说话人文本到语音（TTS）系统依赖说话人嵌入来合成未见说话人的语音，仅需简短的参考发音。尽管已开发出许多用于说话人识别的说话人嵌入，但它们在零样本TTS中的相对有效性仍未得到充分探索。在这项工作中，我们采用基于YourTTS的TTS系统，在其他条件不变的零样本TTS框架内，比较了三种不同的说话人编码器——YourTTS的原始H/ASP编码器、x-vector嵌入和ECAPA-TDNN嵌入。所有模型都在相同的捷克语朗读语音数据集上进行训练，并使用主观和客观方法对24位域外目标说话人进行了评估。主观评估通过侧重于说话人相似度的听力测试进行，而客观评估则测量了从合成发音和真实发音中提取的说话人嵌入之间的余弦距离。在两次评估中，原始H/ASP编码器始终优于其他替代方案，其中ECAPA-TDNN显示出比x-vector更好的结果。这些发现表明，尽管ECAPA-TDNN在说话人识别中很受欢迎，但在这种配置下，它不一定能在零样本TTS的说话人相似度方面提供改进。我们的研究强调了在TTS中重用说话人识别嵌入时进行经验评估的重要性，并为未来的额外比较提供了一个框架。", "summary": "本研究在零样本多说话人TTS框架中，比较了YourTTS的H/ASP、x-vector和ECAPA-TDNN三种说话人嵌入的有效性。实验结果表明，H/ASP编码器表现最佳，ECAPA-TDNN优于x-vector但未能超越H/ASP。研究强调了在TTS中重用说话人识别嵌入时进行经验评估的重要性。", "keywords": "零样本TTS, 说话人嵌入, ECAPA-TDNN, x-vector, 说话人相似度", "comments": "该研究通过严格的实验设计，对比了多种流行说话人嵌入在零样本TTS任务中的表现，填补了该领域的一个空白。其发现对于指导未来TTS系统中的说话人嵌入选择具有重要意义，尤其指出在特定配置下，即使在说话人识别领域表现优异的模型，也可能在TTS中表现不佳，这强调了任务特定评估的重要性。"}}
{"id": "2506.20104", "title": "The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape", "authors": ["Malikussaid", "Sutiyo"], "summary": "The Russian invasion of Ukraine has fundamentally altered the information\ntechnology (IT) risk landscape, particularly in cloud computing environments.\nThis paper examines how this geopolitical conflict has accelerated data\nsovereignty concerns, transformed cybersecurity paradigms, and reshaped cloud\ninfrastructure strategies worldwide. Through an analysis of documented cyber\noperations, regulatory responses, and organizational adaptations between 2022\nand early 2025, this research demonstrates how the conflict has served as a\ncatalyst for a broader reassessment of IT risk. The research reveals that while\ntraditional IT risk frameworks offer foundational guidance, their standard\napplication may inadequately address the nuances of state-sponsored threats,\nconflicting data governance regimes, and the weaponization of digital\ndependencies without specific geopolitical augmentation. The contribution of\nthis paper lies in its focused synthesis and strategic adaptation of existing\nbest practices into a multi-layered approach. This approach uniquely synergizes\nresilient cloud architectures (including sovereign and hybrid models), enhanced\ndata-centric security strategies (such as advanced encryption and\nprivacy-enhancing technologies), and geopolitically-informed governance to\nbuild digital resilience. The interplay between these layers, emphasizing how\ngeopolitical insights directly shape architectural and security choices beyond\nstandard best practices-particularly by integrating the human element,\nincluding personnel vulnerabilities and expertise, as a core consideration in\ntechnical design and operational management-offers a more robust defense\nagainst the specific, multifaceted risks arising from geopolitical conflict in\nincreasingly fractured digital territories.", "comment": "16 pages, 4 tables, to be published in ICoSEIT Conference 2025,\n  unabridged version", "pdf_url": "http://arxiv.org/pdf/2506.20104v1", "categories": ["cs.CY", "cs.CR", "cs.NI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20104v1", "AI": {"title_translation": "俄乌冲突对云计算风险格局的影响", "tldr": "俄乌冲突深刻改变了云计算领域的IT风险格局，传统框架不足以应对地缘政治威胁，论文提出一种整合弹性架构、数据安全和地缘政治治理的多层方法以增强数字弹性。", "motivation": "探讨俄乌冲突如何加速数据主权担忧、改变网络安全范式并重塑全球云基础设施战略，因为这场冲突从根本上改变了IT风险格局，尤其是在云计算环境中。", "method": "通过分析2022年至2025年初记录的网络行动、监管响应和组织适应情况进行研究。", "result": "研究表明，传统IT风险框架在未进行具体地缘政治增强的情况下，可能不足以充分解决国家支持的威胁、冲突的数据治理制度和数字依赖武器化等细微差别。论文提出了一种多层方法，结合弹性云架构（包括主权和混合模型）、增强的数据中心安全策略和地缘政治知情的治理，以构建数字弹性。", "conclusion": "地缘政治洞察直接塑造了超越标准最佳实践的架构和安全选择，特别是通过将人员因素（包括人员漏洞和专业知识）作为技术设计和运营管理中的核心考虑因素，从而为在日益分裂的数字领域中由地缘政治冲突引起的特定、多方面风险提供了更强大的防御。", "translation": "俄罗斯入侵乌克兰从根本上改变了信息技术（IT）的风险格局，特别是在云计算环境中。本文探讨了这场地缘政治冲突如何加速了数据主权担忧，改变了网络安全范式，并重塑了全球的云基础设施战略。通过分析2022年至2025年初记录的网络行动、监管响应和组织适应情况，本研究表明冲突如何成为对IT风险进行更广泛重新评估的催化剂。研究表明，虽然传统的IT风险框架提供了基础指导，但其标准应用可能无法充分解决国家支持的威胁、冲突的数据治理制度以及数字依赖武器化的细微差别，除非进行具体的地缘政治增强。本文的贡献在于其对现有最佳实践的重点综合和战略性调整，形成了一种多层方法。这种方法独特地协同了弹性云架构（包括主权和混合模型）、增强的数据中心安全策略（如高级加密和隐私增强技术）以及地缘政治知情的治理，以构建数字弹性。这些层之间的相互作用，强调了地缘政治洞察如何直接塑造超越标准最佳实践的架构和安全选择——特别是通过将人员因素（包括人员漏洞和专业知识）作为技术设计和运营管理中的核心考虑因素——为在日益分裂的数字领域中由地缘政治冲突引起的特定、多方面风险提供了更强大的防御。", "summary": "俄乌冲突显著改变了云计算领域的IT风险格局。本文分析了冲突如何加速数据主权担忧、改变网络安全范式并重塑云基础设施战略。研究发现传统IT风险框架不足以应对地缘政治威胁，并提出一种多层方法，结合弹性云架构、数据中心安全策略和地缘政治知情的治理，以增强数字弹性，并强调将人员因素整合到技术设计和运营管理中，从而更有效地防御地缘政治冲突带来的多方面风险。", "keywords": "俄乌冲突, 云计算风险, 地缘政治, 网络安全, 数据主权", "comments": "这篇论文的创新之处在于它超越了传统的IT风险框架，强调了地缘政治因素对云计算安全和架构选择的直接影响。它提供了一个实用的多层方法来应对日益复杂和分裂的数字世界中的地缘政治风险，特别是将“人”的因素纳入考虑，这对于提升整体数字韧性至关重要。"}}
{"id": "2506.20011", "title": "Recursive-ARX for Grid-Edge Fault Detection", "authors": ["Soufiane El Yaagoubi", "Keith Moffat", "Eduardo Prieto Araujo", "Florian Dörfler"], "summary": "Future electrical grids will require new ways to identify faults as inverters\nare not capable of supplying large fault currents to support existing fault\ndetection methods and because distributed resources may feed faults from the\nedge of the grid. This paper proposes the use of real-time system\nidentification for online power-system fault detection. Specifically, we\nimplement Recursive ARX (rARX) system identification on a grid-connected\ninverter. Experiments demonstrate that the proposed rARX method is able to both\ndetect large faults quickly, and distinguish between high-impedance faults and\nlarge load increases. These results indicate that rARX grid-edge fault\ndetection is a promising research direction for improving the reliability and\nsafety of modern electric grids.", "comment": "6 pages, 9 figures, to be presented at IEEE PowerTech 2025 (Kiel,\n  Germany)", "pdf_url": "http://arxiv.org/pdf/2506.20011v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20011v1", "AI": {"title_translation": "用于电网边缘故障检测的递归ARX", "tldr": "未来的电网需要新的故障检测方法，因为逆变器和分布式资源带来了挑战。本文提出使用递归ARX (rARX) 进行实时故障检测，实验证明其能快速检测大故障并区分高阻抗故障和负载增加。", "motivation": "未来的电网需要新的故障识别方法，因为逆变器无法提供大故障电流来支持现有方法，且分布式资源可能从电网边缘馈入故障。", "method": "本文提出使用实时系统辨识进行在线电力系统故障检测，具体是在并网逆变器上实现递归ARX (rARX) 系统辨识。", "result": "实验表明，所提出的rARX方法能够快速检测大故障，并区分高阻抗故障和大的负载增加。", "conclusion": "结果表明，rARX电网边缘故障检测是提高现代电网可靠性和安全性的一个有前景的研究方向。", "translation": "未来的电网将需要新的故障识别方法，因为逆变器无法提供大故障电流来支持现有的故障检测方法，并且分布式资源可能会从电网边缘为故障供电。本文提出使用实时系统辨识进行在线电力系统故障检测。具体来说，我们在并网逆变器上实现了递归ARX (rARX) 系统辨识。实验表明，所提出的rARX方法能够快速检测大故障，并区分高阻抗故障和大的负载增加。这些结果表明，rARX电网边缘故障检测是提高现代电网可靠性和安全性的一个有前景的研究方向。", "summary": "本文针对未来电网中逆变器和分布式资源对传统故障检测方法的挑战，提出了一种基于实时系统辨识的解决方案。该方案在并网逆变器上实现了递归ARX (rARX) 方法，并通过实验证明其能够快速检测大故障并有效区分高阻抗故障与大的负载增加，预示着其在提升电网可靠性和安全性方面的巨大潜力。", "keywords": "故障检测, 递归ARX, 电网边缘, 逆变器, 系统辨识", "comments": "该论文为现代电网现代化中的一个关键问题提供了创新方法。利用实时系统辨识与rARX直接解决了逆变器和分布式能源存在时传统故障检测的局限性。其区分高阻抗故障和负载增加的能力尤其有价值。"}}
{"id": "2506.19860", "title": "A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing", "authors": ["Oktay Karakuş", "Padraig Corcoran"], "summary": "Electric vehicle (EV) charging infrastructure is increasingly critical to\nsustainable transport systems, yet its resilience under environmental and\ninfrastructural stress remains underexplored. In this paper, we introduce\nRSERI-EV, a spatially explicit and multi-modal risk assessment framework that\ncombines remote sensing data, open infrastructure datasets, and spatial graph\nanalytics to evaluate the vulnerability of EV charging stations. RSERI-EV\nintegrates diverse data layers, including flood risk maps, land surface\ntemperature (LST) extremes, vegetation indices (NDVI), land use/land cover\n(LULC), proximity to electrical substations, and road accessibility to generate\na composite Resilience Score. We apply this framework to the country of Wales\nEV charger dataset to demonstrate its feasibility. A spatial $k$-nearest\nneighbours ($k$NN) graph is constructed over the charging network to enable\nneighbourhood-based comparisons and graph-aware diagnostics. Our prototype\nhighlights the value of multi-source data fusion and interpretable spatial\nreasoning in supporting climate-resilient, infrastructure-aware EV deployment.", "comment": "11 pages, 4 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.19860v1", "categories": ["eess.SP", "cs.CV"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.19860v1", "AI": {"title_translation": "使用遥感的多模态电动汽车充电基础设施空间风险框架", "tldr": "提出了一个名为 RSERI-EV 的多模态空间风险框架，结合遥感和基础设施数据，评估电动汽车充电站的脆弱性，并用于支持气候适应型部署。", "motivation": "电动汽车充电基础设施对可持续交通系统日益重要，但其在环境和基础设施压力下的弹性尚未得到充分探索。", "method": "引入了 RSERI-EV 框架，该框架是一个空间明确的多模态风险评估框架。它结合了遥感数据、开放基础设施数据集和空间图分析，以评估电动汽车充电站的脆弱性。整合了包括洪水风险图、地表温度（LST）极值、植被指数（NDVI）、土地利用/土地覆盖（LULC）、与变电站的接近度以及道路可达性等多种数据层，以生成复合弹性得分。还在充电网络上构建了一个空间 k-最近邻（kNN）图。", "result": "该框架已应用于威尔士的电动汽车充电器数据集，证明了其可行性。原型突出了多源数据融合和可解释空间推理在支持气候适应型、基础设施感知型电动汽车部署中的价值。", "conclusion": "该研究通过 RSERI-EV 框架，证明了结合多源数据和空间推理对于评估和支持气候适应型、基础设施感知型电动汽车充电基础设施部署的重要性。", "translation": "电动汽车（EV）充电基础设施对于可持续交通系统日益重要，但其在环境和基础设施压力下的弹性仍未得到充分探索。在本文中，我们引入了 RSERI-EV，这是一个空间明确的多模态风险评估框架，它结合了遥感数据、开放基础设施数据集和空间图分析来评估电动汽车充电站的脆弱性。RSERI-EV 集成了多种数据层，包括洪水风险图、地表温度（LST）极值、植被指数（NDVI）、土地利用/土地覆盖（LULC）、与变电站的接近度以及道路可达性，以生成一个复合弹性得分。我们将此框架应用于威尔士的电动汽车充电器数据集，以证明其可行性。在充电网络上构建了一个空间 k-最近邻（kNN）图，以实现基于邻域的比较和图感知诊断。我们的原型突出了多源数据融合和可解释空间推理在支持气候适应型、基础设施感知型电动汽车部署中的价值。", "summary": "本文提出了 RSERI-EV，一个用于评估电动汽车充电基础设施空间风险的多模态框架。该框架整合了遥感数据、开放基础设施数据集和空间图分析，并结合了多种环境和基础设施数据层来计算弹性得分。通过在威尔士数据集上的应用，该研究证明了多源数据融合和空间推理对于构建气候适应型电动汽车部署的重要性。", "keywords": "电动汽车充电基础设施, 遥感, 空间风险框架, 弹性, 多模态分析", "comments": "该论文的创新之处在于提出了一个结合多模态数据（包括遥感、基础设施数据和环境因素）的空间风险评估框架，用于电动汽车充电基础设施。其重要性在于解决了电动汽车充电站弹性评估这一未充分探索的领域，为未来气候适应型基础设施部署提供了工具和方法。通过整合多种异构数据并利用空间图分析，该框架提供了一种全面的视角来识别脆弱性。"}}
{"id": "2506.19975", "title": "VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration", "authors": ["Hang Zhang", "Yuxi Zhang", "Jiazheng Wang", "Xiang Chen", "Renjiu Hu", "Xin Tian", "Gaolei Li", "Min Liu"], "summary": "Recent developments in neural networks have improved deformable image\nregistration (DIR) by amortizing iterative optimization, enabling fast and\naccurate DIR results. However, learning-based methods often face challenges\nwith limited training data, large deformations, and tend to underperform\ncompared to iterative approaches when label supervision is unavailable. While\niterative methods can achieve higher accuracy in such scenarios, they are\nconsiderably slower than learning-based methods. To address these limitations,\nwe propose VoxelOpt, a discrete optimization-based DIR framework that combines\nthe strengths of learning-based and iterative methods to achieve a better\nbalance between registration accuracy and runtime. VoxelOpt uses displacement\nentropy from local cost volumes to measure displacement signal strength at each\nvoxel, which differs from earlier approaches in three key aspects. First, it\nintroduces voxel-wise adaptive message passing, where voxels with lower entropy\nreceives less influence from their neighbors. Second, it employs a multi-level\nimage pyramid with 27-neighbor cost volumes at each level, avoiding exponential\ncomplexity growth. Third, it replaces hand-crafted features or contrastive\nlearning with a pretrained foundational segmentation model for feature\nextraction. In abdominal CT registration, these changes allow VoxelOpt to\noutperform leading iterative in both efficiency and accuracy, while matching\nstate-of-the-art learning-based methods trained with label supervision. The\nsource code will be available at https://github.com/tinymilky/VoxelOpt", "comment": "Accepted for publication at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.19975v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.19975v1", "AI": {"title_translation": "VoxelOpt：用于可变形腹部CT配准中离散优化的体素自适应消息传递", "tldr": "VoxelOpt结合了学习和迭代方法的优点，通过体素自适应消息传递和预训练分割模型，实现了更快、更准确的可变形图像配准，尤其适用于腹部CT。", "motivation": "现有的基于学习的可变形图像配准（DIR）方法在训练数据有限、变形较大或缺乏标签监督时面临挑战，而迭代方法虽然准确但速度较慢。本文旨在解决这些局限性，在配准精度和运行时间之间取得更好的平衡。", "method": "VoxelOpt是一种基于离散优化的可变形图像配准（DIR）框架。它使用局部成本体中的位移熵来衡量每个体素的位移信号强度，并有三个关键创新点：1. 引入体素自适应消息传递，熵值较低的体素受邻居影响较小；2. 采用多层图像金字塔，每层使用27邻域成本体，避免指数级复杂度增长；3. 使用预训练的基础分割模型进行特征提取，而非手工特征或对比学习。", "result": "在腹部CT配准中，VoxelOpt在效率和准确性方面均优于领先的迭代方法，同时与经过标签监督训练的最先进的基于学习的方法表现相当。", "conclusion": "VoxelOpt成功结合了基于学习和迭代方法的优势，在可变形图像配准（特别是腹部CT配准）中实现了精度和运行时间之间的更优平衡。", "translation": "神经网络的最新发展通过分摊迭代优化改进了可变形图像配准（DIR），从而实现了快速准确的DIR结果。然而，基于学习的方法通常面临训练数据有限、形变大等挑战，并且在缺乏标签监督时，其性能往往不如迭代方法。尽管迭代方法在这种情况下可以实现更高的精度，但它们比基于学习的方法慢得多。为了解决这些限制，我们提出了VoxelOpt，一个基于离散优化的DIR框架，它结合了基于学习和迭代方法的优势，以在配准精度和运行时间之间实现更好的平衡。VoxelOpt使用局部成本体中的位移熵来衡量每个体素的位移信号强度，这与早期方法在三个关键方面有所不同。首先，它引入了体素自适应消息传递，其中熵值较低的体素受到邻居的影响较小。其次，它采用多层图像金字塔，每层使用27邻域成本体，避免了指数级复杂度增长。第三，它用预训练的基础分割模型进行特征提取，取代了手工特征或对比学习。在腹部CT配准中，这些改变使得VoxelOpt在效率和准确性方面均优于领先的迭代方法，同时与经过标签监督训练的最先进的基于学习的方法表现相当。源代码将在https://github.com/tinymilky/VoxelOpt提供。", "summary": "VoxelOpt是一种新颖的基于离散优化的可变形图像配准（DIR）框架，旨在克服现有基于学习（数据/监督有限时较慢）和迭代（速度慢）方法的局限性。它引入了基于位移熵的体素自适应消息传递，采用了多层图像金字塔，并利用预训练的基础分割模型进行特征提取。在腹部CT配准中，VoxelOpt在效率和准确性方面均优于迭代方法，并且即使在没有标签监督的情况下，也能与最先进的基于学习的方法相媲美。", "keywords": "可变形图像配准, 离散优化, 体素自适应, 腹部CT, 消息传递", "comments": "VoxelOpt的创新在于其混合方法，结合了学习和迭代方法的优点。体素自适应消息传递和使用预训练分割模型是显著的改进。它在没有标签监督的情况下实现高精度同时保持效率的能力，对于标签数据稀缺的医学影像领域尤为重要。"}}
{"id": "2506.20049", "title": "Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis", "authors": ["Lorin Achey", "Alec Reed", "Brendan Crowe", "Bradley Hayes", "Christoffer Heckman"], "summary": "We present a novel approach for enhancing robotic exploration by using\ngenerative occupancy mapping. We introduce SceneSense, a diffusion model\ndesigned and trained for predicting 3D occupancy maps given partial\nobservations. Our proposed approach probabilistically fuses these predictions\ninto a running occupancy map in real-time, resulting in significant\nimprovements in map quality and traversability. We implement SceneSense onboard\na quadruped robot and validate its performance with real-world experiments to\ndemonstrate the effectiveness of the model. In these experiments, we show that\noccupancy maps enhanced with SceneSense predictions better represent our fully\nobserved ground truth data (24.44% FID improvement around the robot and 75.59%\nimprovement at range). We additionally show that integrating\nSceneSense-enhanced maps into our robotic exploration stack as a \"drop-in\" map\nimprovement, utilizing an existing off-the-shelf planner, results in\nimprovements in robustness and traversability time. Finally we show results of\nfull exploration evaluations with our proposed system in two dissimilar\nenvironments and find that locally enhanced maps provide more consistent\nexploration results than maps constructed only from direct sensor measurements.", "comment": "arXiv admin note: text overlap with arXiv:2409.10681", "pdf_url": "http://arxiv.org/pdf/2506.20049v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20049v1", "AI": {"title_translation": "鲁棒的机器人探索与建图：使用生成式占据图合成", "tldr": "本文提出了一种名为SceneSense的扩散模型，用于从部分观测中预测3D占据图，并通过将其融合到实时占据图中，显著提升了机器人探索和建图的质量、鲁棒性和可通行性。", "motivation": "提升机器人探索过程中占据图的质量和可通行性，克服现有方法仅依赖直接传感器测量导致的问题。", "method": "本文引入了SceneSense，一个基于扩散模型的3D占据图预测模型，该模型能根据部分观测进行预测。这些预测被实时地概率性融合到运行中的占据图中，以增强地图质量和可通行性。该方法作为“即插即用”的地图改进集成到现有的机器人探索规划器中。", "result": "实验结果显示，通过SceneSense增强的占据图能更好地表示完全观测的地面真值数据（在机器人周围FID改善24.44%，在远距离改善75.59%）。将SceneSense增强的地图集成到机器人探索栈中，使用现成的规划器，提高了鲁棒性和可通行时间。在两种不同环境下的完整探索评估表明，局部增强的地图比仅由直接传感器测量构建的地图提供了更一致的探索结果。", "conclusion": "通过使用生成式占据图合成，特别是SceneSense扩散模型，可以显著提高机器人探索和建图的质量、鲁棒性和效率，尤其在地图质量和可通行性方面表现优异。", "translation": "我们提出了一种利用生成式占据图合成来增强机器人探索的新方法。我们引入了SceneSense，一个为给定部分观测预测3D占据图而设计和训练的扩散模型。我们提出的方法将这些预测实时地概率性融合到运行中的占据图中，从而显著改善了地图质量和可通行性。我们在一个四足机器人上实现了SceneSense，并通过真实世界实验验证了其性能，以证明模型的有效性。在这些实验中，我们表明，通过SceneSense预测增强的占据图能更好地表示我们完全观测的地面真值数据（在机器人周围FID改善24.44%，在远距离改善75.59%）。我们还表明，将SceneSense增强的地图作为“即插即用”的地图改进集成到我们的机器人探索栈中，利用现有的现成规划器，可以提高鲁棒性和可通行时间。最后，我们展示了在两种不同环境中，我们提出的系统进行完整探索评估的结果，并发现局部增强的地图比仅由直接传感器测量构建的地图提供了更一致的探索结果。", "summary": "本文提出了一种名为SceneSense的新型生成式占据图合成方法，该方法利用扩散模型从部分观测中预测3D占据图。通过将这些预测实时融合到机器人占据图中，显著提升了地图质量、鲁棒性和可通行性。实验结果表明，SceneSense增强的地图能更准确地表示环境，并提高了机器人探索的效率和一致性。", "keywords": "机器人探索, 占据图, 扩散模型, SceneSense, 地图合成", "comments": "本文的创新点在于引入了扩散模型SceneSense来预测3D占据图，并将其与实时占据图融合，有效解决了传统方法在地图质量和可通行性上的局限。其“即插即用”的特性也增强了实际应用的便利性。该方法在提升机器人探索效率和鲁棒性方面具有重要意义。"}}
{"id": "2506.20435", "title": "The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review", "authors": ["Mennatullah T. Khedr", "John S. Fitzgerald"], "summary": "Digital Twins (DTs) are increasingly used to model complex systems,\nespecially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where\neffective integration is key. This systematic literature review investigates DT\ncomposition and verification and validation (V&V) methodologies. Analyzing 21\nstudies from 2022-2024, we examined composition mechanisms, SoS\ncharacteristics, and V&V formality, scope, and challenges. While composition is\ndiscussed, formalization is limited. V&V approaches vary, with semi-formal\nmethods and simulations dominating; formal verification is underutilized. Key\ntechnical challenges include model uncertainty and integration complexity.\nMethodological challenges highlight the lack of standardized DT-specific V&V\nframeworks. There is a need to move beyond model validation to address\nintegration and cyber-physical consistency. This review contributes a\nstructured classification of V&V approaches and emphasizes the need for\nstandardized, scalable V&V and rigorous composition methodologies for complex\nDT implementations.", "comment": "15 pages, 3 figures, Presented at the 23rd Overture workshop, June\n  2025 (arXiv:cs/2506.08680)", "pdf_url": "http://arxiv.org/pdf/2506.20435v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20435v1", "AI": {"title_translation": "系统之系统数字孪生体的构成：一项系统性文献综述", "tldr": "该系统综述分析了SoS数字孪生体的构成和验证与确认（V&V）方法，发现其形式化程度有限，且缺乏标准化V&V框架，强调需要更严格的构成和可扩展的V&V。", "motivation": "数字孪生体（DTs）在建模复杂系统，特别是网络物理系统（CPS）和系统之系统（SoS）中日益普及，其中有效的集成至关重要。本研究旨在调查SoS中DT的构成和验证与确认（V&V）方法。", "method": "本研究进行了一项系统性文献综述，分析了2022年至2024年间的21项研究，考察了构成机制、SoS特性以及V&V的形式化程度、范围和挑战。", "result": "研究发现，数字孪生体的构成讨论较多但形式化程度有限；验证与确认（V&V）方法多样，以半形式化方法和仿真为主，而形式化验证未得到充分利用。主要技术挑战包括模型不确定性和集成复杂性。方法论挑战则在于缺乏数字孪生体特有的标准化V&V框架。", "conclusion": "本综述对V&V方法进行了结构化分类，并强调了在复杂数字孪生体实现中，需要标准化、可扩展的V&V以及严格的构成方法，以超越模型验证，解决集成和网络物理一致性问题。", "translation": "数字孪生体（DTs）正越来越多地用于建模复杂系统，特别是在网络物理系统（CPS）和系统之系统（SoS）中，其中有效的集成是关键。这项系统性文献综述调查了数字孪生体的构成以及验证与确认（V&V）方法。通过分析2022-2024年的21项研究，我们检查了构成机制、SoS特性以及V&V的形式化程度、范围和挑战。虽然构成问题得到了讨论，但其形式化程度有限。V&V方法各不相同，以半形式化方法和仿真为主导；形式化验证未得到充分利用。主要的技术挑战包括模型不确定性和集成复杂性。方法论挑战则凸显了缺乏数字孪生体特有的标准化V&V框架。有必要超越模型验证，解决集成和网络物理一致性问题。本综述对V&V方法进行了结构化分类，并强调了在复杂数字孪生体实现中，需要标准化、可扩展的V&V以及严格的构成方法。", "summary": "本系统性文献综述考察了系统之系统（SoS）中数字孪生体（DTs）的构成和验证与确认（V&V）方法。研究分析了21篇文献，发现DT构成形式化程度不足，V&V方法多样但缺乏标准化框架，且形式化验证未被充分利用。主要挑战在于模型不确定性、集成复杂性以及缺乏DT专用V&V框架。综述呼吁为复杂DT实现开发标准化、可扩展的V&V及严格的构成方法，以解决集成和一致性问题。", "keywords": "数字孪生体, 系统之系统, 构成, 验证与确认, 文献综述", "comments": "这篇综述揭示了SoS数字孪生体领域在构成和V&V方面的关键差距，特别是形式化程度不足和标准化框架的缺乏。其创新之处在于对V&V方法进行了结构化分类，并明确指出了未来研究的方向，即开发更严格的构成方法和可扩展的V&V框架，这对推动数字孪生体在复杂系统中的实际应用至关重要。"}}
{"id": "2506.20134", "title": "From 2D to 3D Cognition: A Brief Survey of General World Models", "authors": ["Ningwei Xie", "Zizi Tian", "Lei Yang", "Xiao-Ping Zhang", "Meng Guo", "Jie Li"], "summary": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20134v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20134v1", "AI": {"title_translation": "从2D到3D认知：通用世界模型的简要综述", "tldr": "这篇综述系统分析了通用世界模型从2D感知向3D认知的演变，提出了一个概念框架，探讨了关键技术驱动因素、核心认知能力、实际应用、面临的挑战以及未来的发展方向。", "motivation": "尽管3D感知世界模型取得了快速进展，但该领域缺乏系统性分析来对新兴技术进行分类，并阐明它们在推进3D认知世界模型中的作用。", "method": "本综述通过引入一个概念框架，对从2D感知向3D认知过渡的世界模型进行了结构化和前瞻性审查。它强调了3D表示和世界知识的结合这两个关键技术驱动因素，剖析了3D物理场景生成、3D空间推理和3D空间交互这三种核心认知能力，并考察了这些能力在具身AI、自动驾驶、数字孪生和游戏/VR等实际应用中的部署。最后，它指出了数据、建模和部署方面的挑战，并概述了未来方向。", "result": "该综述提供了一个概念框架，用于系统分析和分类从2D到3D认知的世界模型，突出了关键的技术驱动因素和核心认知能力。它还审视了这些能力在多种现实世界应用中的部署，并识别了当前面临的挑战。", "conclusion": "该论文识别了数据、建模和部署方面的挑战，并展望了未来发展更强大和更具泛化能力的3D世界模型的方向。", "translation": "世界模型在通用人工智能（AGI）的发展中受到了越来越多的关注，它们作为学习外部世界表示和预测未来状态的计算框架。早期工作主要集中在2D视觉感知和模拟，而近期支持3D的生成式世界模型已经展示了合成几何一致、交互式3D环境的能力，标志着向3D空间认知的转变。尽管进展迅速，但该领域缺乏系统性分析来对新兴技术进行分类，并阐明它们在推进3D认知世界模型中的作用。本综述通过引入一个概念框架来解决这一需求，提供了一个结构化和前瞻性的世界模型综述，这些模型正从2D感知向3D认知过渡。在此框架内，我们强调了两个关键技术驱动因素，特别是3D表示的进步和世界知识的融入，作为基本支柱。在此基础上，我们剖析了支撑3D世界建模的三种核心认知能力：3D物理场景生成、3D空间推理和3D空间交互。我们进一步考察了这些能力在现实世界应用中的部署，包括具身AI、自动驾驶、数字孪生和游戏/VR。最后，我们识别了数据、建模和部署方面的挑战，并概述了推进更强大和更具泛化能力的3D世界模型的未来方向。", "summary": "本综述系统地分析了通用世界模型从2D感知向3D认知的演进。论文提出了一个概念框架，强调了3D表示和世界知识作为关键驱动力，并深入探讨了3D物理场景生成、3D空间推理和3D空间交互等核心认知能力。此外，文章还考察了这些能力在具身AI、自动驾驶等实际应用中的部署，并指出了数据、建模和部署方面的挑战，为未来更鲁棒和泛化的3D世界模型指明了方向。", "keywords": "世界模型, 3D认知, AGI, 综述, 3D表示", "comments": "这篇综述及时地提供了对世界模型从2D到3D认知转变的系统性分析，其提出的概念框架对于理解当前进展和AGI领域的未来挑战具有重要价值。"}}
{"id": "2506.19891", "title": "Orthogonal Soft Pruning for Efficient Class Unlearning", "authors": ["Qinghui Gong", "Xue Yang", "Xiaohu Tang"], "summary": "Machine unlearning aims to selectively remove class-specific knowledge from\npretrained neural networks to satisfy privacy regulations such as the GDPR.\nExisting methods typically face a trade-off between unlearning speed and\npreservation of predictive accuracy, often incurring either high computational\noverhead or significant performance degradation on retained classes. In this\npaper, we propose a novel class-aware soft pruning framework leveraging\northogonal convolutional kernel regularization to achieve rapid and precise\nforgetting with millisecond-level response times. By enforcing orthogonality\nconstraints during training, our method decorrelates convolutional filters and\ndisentangles feature representations, while efficiently identifying\nclass-specific channels through activation difference analysis. Extensive\nevaluations across multiple architectures and datasets demonstrate stable\npruning with near-instant execution, complete forgetting of targeted classes,\nand minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,\nand TinyImageNet confirm that our approach substantially reduces membership\ninference attack risks and accelerates unlearning by orders of magnitude\ncompared to state-of-the-art baselines. This framework provides an efficient,\npractical solution for real-time machine unlearning in Machine Learning as a\nService (MLaaS) scenarios.", "comment": "11 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2506.19891v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19891v1", "AI": {"title_translation": "高效类别遗忘的正交软剪枝", "tldr": "提出一种基于正交软剪枝的机器学习遗忘新框架，实现快速精确的类别遗忘，同时保持模型性能。", "motivation": "现有机器学习遗忘方法在遗忘速度和预测精度之间存在权衡，导致计算开销大或对保留类别性能下降，无法满足GDPR等隐私法规要求。", "method": "提出一种新颖的类别感知软剪枝框架，利用正交卷积核正则化。通过在训练期间强制执行正交性约束来解耦卷积滤波器和解缠结特征表示，并通过激活差异分析有效识别特定类别通道。", "result": "在多个架构和数据集上实现了稳定的剪枝、近乎即时的执行、对目标类别的完全遗忘以及对保留数据最小的精度损失。与现有技术相比，显著降低了成员推理攻击风险，并将遗忘速度提高了数个数量级。", "conclusion": "该框架为机器学习即服务 (MLaaS) 场景中的实时机器学习遗忘提供了一个高效、实用的解决方案。", "translation": "机器学习遗忘旨在从预训练神经网络中选择性地移除特定于类别的知识，以满足GDPR等隐私法规。现有方法通常面临遗忘速度和预测精度之间的权衡，往往导致高计算开销或对保留类别性能的显著下降。在本文中，我们提出了一种新颖的类别感知软剪枝框架，利用正交卷积核正则化来实现快速精确的遗忘，响应时间达到毫秒级。通过在训练期间强制执行正交性约束，我们的方法解耦了卷积滤波器并解缠结了特征表示，同时通过激活差异分析有效地识别了特定类别的通道。在多种架构和数据集上的广泛评估表明，该方法实现了稳定的剪枝、近乎即时的执行、对目标类别的完全遗忘以及对保留数据最小的精度损失。在CIFAR-10、CIFAR-100和TinyImageNet上的实验证实，与现有基线相比，我们的方法显著降低了成员推理攻击风险，并将遗忘速度提高了数个数量级。该框架为机器学习即服务 (MLaaS) 场景中的实时机器学习遗忘提供了一个高效、实用的解决方案。", "summary": "本文提出一种名为正交软剪枝的新型类别感知框架，用于高效的机器学习遗忘。该方法通过引入正交卷积核正则化，在训练时解耦滤波器并解缠结特征，并通过激活差异分析识别特定类别通道，从而实现毫秒级的快速精确遗忘。实验证明，该方法能完全遗忘目标类别，对保留数据精度损失极小，显著加速遗忘过程并降低隐私攻击风险，为实时机器学习遗忘提供了实用方案。", "keywords": "机器学习遗忘, 软剪枝, 正交正则化, 类别遗忘, 隐私保护", "comments": "该论文提出了一种创新的正交软剪枝方法，解决了机器学习遗忘领域中速度与精度之间的关键权衡问题。其核心创新在于利用正交性约束来解耦特征表示，从而实现对特定类别知识的精确且高效移除。毫秒级的响应时间使其在MLaaS等对实时性要求高的场景中具有重要应用价值，显著提升了隐私保护的实用性。"}}
{"id": "2506.19881", "title": "Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models", "authors": ["Aloni Cohen"], "summary": "Are there any conditions under which a generative model's outputs are\nguaranteed not to infringe the copyrights of its training data? This is the\nquestion of \"provable copyright protection\" first posed by Vyas, Kakade, and\nBarak (ICML 2023). They define near access-freeness (NAF) and propose it as\nsufficient for protection. This paper revisits the question and establishes new\nfoundations for provable copyright protection -- foundations that are firmer\nboth technically and legally. First, we show that NAF alone does not prevent\ninfringement. In fact, NAF models can enable verbatim copying, a blatant\nfailure of copy protection that we dub being tainted. Then, we introduce our\nblameless copy protection framework for defining meaningful guarantees, and\ninstantiate it with clean-room copy protection. Clean-room copy protection\nallows a user to control their risk of copying by behaving in a way that is\nunlikely to copy in a counterfactual clean-room setting. Finally, we formalize\na common intuition about differential privacy and copyright by proving that DP\nimplies clean-room copy protection when the dataset is golden, a copyright\ndeduplication requirement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19881v1", "categories": ["cs.CR", "cs.CY", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19881v1", "AI": {"title_translation": "洁净室中的无责用户：定义生成模型的版权保护", "tldr": "本文重新审视了生成模型输出的版权保护问题，指出现有方法不足，并提出了“无责复制保护框架”和“洁净室复制保护”作为更坚实的解决方案，证明了差分隐私在该框架下的作用。", "motivation": "探讨在何种条件下，生成模型的输出可以保证不侵犯其训练数据的版权，即“可证明的版权保护”问题。", "method": "重新审视了现有的“可证明版权保护”问题，证明了现有的近乎无访问性（NAF）不足以防止侵权。引入了“无责复制保护框架”，并具体化为“洁净室复制保护”。最后，通过证明当数据集是“黄金”时，差分隐私（DP）意味着洁净室复制保护，从而形式化了DP与版权之间的关系。", "result": "证明了近乎无访问性（NAF）不足以防止侵权，甚至可能导致逐字复制。提出了“无责复制保护框架”和“洁净室复制保护”作为定义有意义保障的新基础。形式化并证明了在“黄金”数据集条件下，差分隐私（DP）意味着洁净室复制保护。", "conclusion": "本文为可证明的版权保护建立了新的、在技术和法律上都更坚实的基础。", "translation": "生成模型的输出在何种条件下可以保证不侵犯其训练数据的版权？这是Vyas、Kakade和Barak（ICML 2023）首次提出的“可证明版权保护”问题。他们定义了近乎无访问性（NAF）并提出其足以提供保护。本文重新审视了这个问题，并为可证明版权保护建立了新的基础——这些基础在技术和法律上都更加坚实。首先，我们表明仅凭NAF并不能防止侵权。事实上，NAF模型可能导致逐字复制，这是我们称之为“被污染”的版权保护的公然失败。然后，我们引入了我们的无责复制保护框架来定义有意义的保障，并以洁净室复制保护为例进行实例化。洁净室复制保护允许用户通过在反事实洁净室设置中以不太可能复制的方式行事来控制其复制风险。最后，我们通过证明当数据集是“黄金”（版权去重要求）时，差分隐私（DP）意味着洁净室复制保护，从而形式化了关于差分隐私和版权的常见直觉。", "summary": "本文深入探讨了生成模型输出的版权保护问题。作者首先指出现有“近乎无访问性”（NAF）不足以防止侵权，甚至可能导致逐字复制。为解决此问题，论文提出了“无责复制保护框架”，并引入了“洁净室复制保护”概念，使用户能通过特定行为控制复制风险。此外，研究还证明了在数据集满足“黄金”条件时，差分隐私（DP）能够实现洁净室复制保护，从而为生成模型提供更坚实、可证明的版权保障。", "keywords": "生成模型, 版权保护, 差分隐私, 洁净室复制保护, 近乎无访问性", "comments": "本文通过指出现有“近乎无访问性”（NAF）在版权保护方面的不足，提出了一个更严谨和全面的“无责复制保护框架”及“洁净室复制保护”概念，这在日益增长的生成式AI版权争议中具有重要意义。它不仅从技术上重新定义了“可证明版权保护”，还尝试从法律角度提供了更坚实的基础，特别是在将差分隐私与版权保护联系起来方面，提供了新的理论支撑。"}}
{"id": "2506.20156", "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype", "authors": ["Xuefei Hou", "Xizhao Tan"], "summary": "The core challenge in learning has shifted from knowledge acquisition to\neffective Self-Regulated Learning (SRL): planning, monitoring, and reflecting\non one's learning. Existing digital tools, however, inadequately support\nmetacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized\nreview, overlooking the role of context, while Personal Knowledge Management\n(PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel\nparadigm that conceptualizes the context-triggered retrieval of personal past\ninsights as a metacognitive scaffold to promote SRL. We formalize this paradigm\nusing the Just-in-Time Adaptive Intervention (JITAI) framework and implement a\nprototype system, Irec, to demonstrate its feasibility. At its core, Irec uses\na dynamic knowledge graph of the user's learning history. When a user faces a\nnew problem, a hybrid retrieval engine recalls relevant personal \"insights.\"\nSubsequently, a large language model (LLM) performs a deep similarity\nassessment to filter and present the most relevant scaffold in a just-in-time\nmanner. To reduce cognitive load, Irec features a human-in-the-loop pipeline\nfor LLM-based knowledge graph construction. We also propose an optional \"Guided\nInquiry\" module, where users can engage in a Socratic dialogue with an expert\nLLM, using the current problem and recalled insights as context. The\ncontribution of this paper is a solid theoretical framework and a usable system\nplatform for designing next-generation intelligent learning systems that\nenhance metacognition and self-regulation.", "comment": "Version 1 of a work in progress. Finalized system flowcharts, a\n  public GitHub repository with the source code, and a full reproducibility\n  package detailing the prompts, models, and testing guidelines will be\n  provided in v2", "pdf_url": "http://arxiv.org/pdf/2506.20156v1", "categories": ["cs.HC", "cs.AI", "cs.IR", "H.5.2; I.2.7; H.3.3"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20156v1", "AI": {"title_translation": "Irec：一种通过即时洞察力回忆实现自我调节学习的元认知支架：一个概念框架和系统原型", "tldr": "Irec是一个利用即时洞察力回忆作为元认知支架的系统原型，旨在通过动态知识图谱和大型语言模型增强自我调节学习。", "motivation": "学习的核心挑战已从知识获取转向有效的自我调节学习（SRL），包括规划、监控和反思。然而，现有的数字工具（如间隔重复系统和个人知识管理工具）在支持元认知反思方面存在不足。", "method": "本文引入了“洞察力回忆”范式，将上下文触发的个人过去洞察力检索概念化为元认知支架以促进SRL。该范式通过即时自适应干预（JITAI）框架进行形式化，并实现了一个原型系统Irec。Irec使用用户学习历史的动态知识图谱，通过混合检索引擎和大型语言模型（LLM）进行深度相似性评估，以即时方式呈现相关洞察。为减少认知负荷，Irec采用人机协作流程进行LLM知识图谱构建，并提出了可选的“引导式探究”模块，允许用户与专家LLM进行苏格拉底式对话。", "result": "本文提出了一个坚实的理论框架和一个可用的系统平台——Irec原型系统，以展示其可行性，用于设计下一代智能学习系统，从而增强元认知和自我调节能力。", "conclusion": "本文的贡献在于提供了一个坚实的理论框架和一个可用的系统平台，用于设计下一代智能学习系统，以增强元认知和自我调节。", "translation": "学习的核心挑战已从知识获取转向有效的自我调节学习（SRL）：规划、监控和反思一个人的学习。然而，现有的数字工具未能充分支持元认知反思。间隔重复系统（SRS）使用去语境化的复习，忽略了语境的作用，而个人知识管理（PKM）工具则需要大量手动维护。\n为了解决这些挑战，本文引入了“洞察力回忆”，这是一种新颖的范式，它将语境触发的个人过去洞察力的检索概念化为促进SRL的元认知支架。我们使用即时自适应干预（JITAI）框架形式化了这一范式，并实现了一个原型系统Irec，以证明其可行性。Irec的核心是使用用户学习历史的动态知识图谱。当用户面临新问题时，混合检索引擎会回忆相关的个人“洞察”。随后，大型语言模型（LLM）执行深度相似性评估，以即时过滤并呈现最相关的支架。为了减少认知负荷，Irec具有LLM知识图谱构建的人机协作流程。我们还提出了一个可选的“引导式探究”模块，用户可以在其中与专家LLM进行苏格拉底式对话，将当前问题和回忆的洞察作为语境。本文的贡献是一个坚实的理论框架和一个可用的系统平台，用于设计增强元认知和自我调节的下一代智能学习系统。", "summary": "本文提出了一种名为“洞察力回忆”的新范式，并开发了原型系统Irec，旨在通过即时提供个人相关洞察来增强自我调节学习（SRL）中的元认知反思。该系统利用动态知识图谱和大型语言模型进行上下文感知的洞察检索和筛选，并结合人机协作和可选的引导式探究模块，以克服现有工具在支持元认知方面不足的问题，并为未来智能学习系统提供了理论框架和实践平台。", "keywords": "自我调节学习, 元认知, 即时洞察力回忆, 知识图谱, 大型语言模型", "comments": "这篇论文创新性地将“即时洞察力回忆”概念引入自我调节学习领域，并结合了动态知识图谱和大型语言模型，有望显著提升学习者的元认知能力。其提出的JITAI框架和人机协作构建知识图谱的思路，为未来智能学习系统的设计提供了宝贵的参考。特别是将LLM用于深度相似性评估和引导式探究，展现了技术融合的潜力。"}}
{"id": "2506.20420", "title": "Semantic Caching for Improving Web Affordability", "authors": ["Hafsa Akbar", "Danish Athar", "Muhammad Ayain Fida Rana", "Chaudhary Hammad Javed", "Zartash Afzal Uzmi", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20420v1", "categories": ["cs.NI", "F.2.2, I.2.7"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.20420v1", "AI": {"title_translation": "语义缓存提升网络可负担性", "tldr": "该论文提出使用大型语言模型（LLMs）进行语义缓存，通过复用语义相似的图片来提高网络可负担性，并展示了显著的数据传输量节省潜力。", "motivation": "随着网络内容的快速增长，网页变得越来越大，给互联网的可负担性带来了巨大挑战，尤其是在数据成本仍然高昂的发展中国家。", "method": "研究提出了一种利用大型语言模型（LLMs）的语义缓存方法，以实现网页内语义相似图像的重复利用。通过分析50个领先的新闻和媒体网站（包含4,264张图片和超过40,000对图片），评估了商业和开源多模态LLM在评估语义可替换性方面的性能，并构建了一个概念验证架构。", "result": "研究表明，该方法具有显著减少数据传输的潜力，某些网站类别中多达37%的图片可被替换。概念验证架构显示，与精确缓存相比，用户可实现约10%的额外字节节省。GPT-4o表现最佳，归一化均方根误差为0.1735，加权F1分数为0.8374，而开源的LLaMA 3.1模型也表现出可比的性能。", "conclusion": "该方法为用户和网站运营者带来了益处，能够大幅减少数据传输。论文讨论了伦理问题和实际挑战，包括语义保存、用户驱动的缓存配置、隐私问题以及来自网站运营者的潜在阻力。", "translation": "网络内容的快速增长导致网页越来越大，给互联网可负担性带来了重大挑战，尤其是在数据成本仍然高得离谱的发展中国家。我们提出使用大型语言模型（LLMs）进行语义缓存，通过在网页内复用语义相似的图像来提高网络可负担性。通过分析50个领先的新闻和媒体网站，包括4,264张图片和超过40,000对图片对，我们展示了显著减少数据传输的潜力，某些网站类别中多达37%的图片可被替换。我们的概念验证架构显示，与精确缓存相比，用户可以实现大约10%的额外字节节省。我们评估了商业和开源多模态LLM在评估语义可替换性方面的性能。GPT-4o表现最佳，归一化均方根误差为0.1735，加权F1分数为0.8374，而开源的LLaMA 3.1模型也表现出可比的性能，突出了其在大规模应用中的可行性。这种方法对用户和网站运营者都有益，可大幅减少数据传输。我们讨论了伦理问题和实际挑战，包括语义保存、用户驱动的缓存配置、隐私问题以及来自网站运营者的潜在阻力。", "summary": "本论文提出了一种利用大型语言模型（LLMs）进行语义缓存的新方法，旨在通过复用网页中语义相似的图像来提升网络可负担性。通过对50个主要新闻媒体网站的分析，研究发现该方法在数据传输减少方面潜力巨大，部分网站类别中高达37%的图像可被替换。概念验证架构显示，与传统精确缓存相比，该方法能额外节省约10%的字节。研究评估了GPT-4o和LLaMA 3.1等多模态LLMs的性能，结果表明GPT-4o表现最优，但LLaMA 3.1也展现出可比性，预示着其大规模应用的潜力。该方法有望为用户和网站运营商显著降低数据传输成本，论文也探讨了相关伦理与实际挑战。", "keywords": "语义缓存, 大型语言模型, 网络可负担性, 数据传输减少, 图像复用", "comments": "该论文的创新之处在于将大型语言模型应用于语义缓存，以解决网络可负担性问题，特别是针对图像数据。其重要性体现在为发展中国家高昂的数据成本提供了潜在的解决方案。尽管提出了显著的数据节省效果，但抽象中也明确指出了语义保存、用户配置、隐私和运营商阻力等实际挑战，这表明该技术在实际部署中仍需克服诸多障碍。"}}
{"id": "2506.18278", "title": "Finite-Time Information-Theoretic Bounds in Queueing Control", "authors": ["Yujie Liu", "Vincent Y. F. Tan", "Yunbei Xu"], "summary": "We establish the first finite-time information-theoretic lower bounds-and\nderive new policies that achieve them-for the total queue length in scheduling\nproblems over stochastic processing networks with both adversarial and\nstochastic arrivals. Prior analyses of MaxWeight guarantee only stability and\nasymptotic optimality in heavy traffic; we prove that, at finite horizons,\nMaxWeight can incur strictly larger backlog by problem-dependent factors which\nwe identify. Our main innovations are 1) a minimax framework that pinpoints the\nprecise problem parameters governing any policy's finite-time performance; 2)\nan information-theoretic lower bound on total queue length; 3) fundamental\nlimitation of MaxWeight that it is suboptimal in finite time; and 4) a new\nscheduling rule that minimizes the full Lyapunov drift-including its\nsecond-order term-thereby matching the lower bound under certain conditions, up\nto universal constants. These findings reveal a fundamental limitation on\n\"drift-only\" methods and points the way toward principled, non-asymptotic\noptimality in queueing control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18278v1", "categories": ["math.OC", "cs.IT", "cs.LG", "math.IT"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.18278v1", "AI": {"title_translation": "队列控制中的有限时间信息理论界限", "tldr": "本文为随机处理网络中的调度问题建立了第一个有限时间信息理论下界，并提出了实现该下界的新策略，揭示了MaxWeight在有限时间内的局限性。", "motivation": "先前的MaxWeight分析只保证了稳定性与重载下的渐近最优性；然而，在有限时间范围内，MaxWeight可能导致严格更大的队列积压。因此，需要建立有限时间的信息理论界限并开发性能更优的策略。", "method": "1) 提出了一个极小极大框架，用于精确确定影响策略有限时间性能的问题参数；2) 建立了总队列长度的信息理论下界；3) 设计了一种最小化完整Lyapunov漂移（包括其二阶项）的新调度规则。", "result": "1) 证明了MaxWeight在有限时间内是次优的，其队列积压会因问题相关因素而严格增大；2) 提出的新调度规则在特定条件下，在通用常数范围内达到了信息理论下界。", "conclusion": "本研究揭示了“仅漂移”方法的根本局限性，并为队列控制中原则性的非渐近最优性指明了方向。", "translation": "我们为具有对抗性和随机到达的随机处理网络中的调度问题，建立了第一个有限时间信息理论下界，并推导出了实现这些下界的新策略，用于衡量总队列长度。先前的MaxWeight分析仅保证了重载下的稳定性和渐近最优性；我们证明，在有限的时间范围内，MaxWeight可能会因我们确定的问题相关因素而导致严格更大的积压。我们的主要创新点包括：1）一个极小极大框架，精确地确定了控制任何策略有限时间性能的问题参数；2）总队列长度的信息理论下界；3）MaxWeight在有限时间内次优的根本局限性；以及4）一种最小化完整Lyapunov漂移（包括其二阶项）的新调度规则，从而在某些条件下，在通用常数范围内与下界匹配。这些发现揭示了“仅漂移”方法的根本局限性，并为队列控制中原则性的非渐近最优性指明了方向。", "summary": "本文针对随机处理网络中的调度问题，首次建立了总队列长度的有限时间信息理论下界，并提出了能够达到这些下界的新策略。研究发现，传统的MaxWeight算法在有限时间内存在次优性，其队列积压可能显著增大。通过引入极小极大框架和推导信息理论下界，并设计一种最小化完整Lyapunov漂移的新调度规则，本文不仅揭示了“仅漂移”方法的局限性，也为实现队列控制的非渐近最优性提供了新途径。", "keywords": "有限时间, 信息理论下界, 队列控制, MaxWeight, 调度策略", "comments": "本文的创新之处在于首次为队列控制问题建立了有限时间信息理论下界，并揭示了传统MaxWeight算法在有限时间场景下的局限性。其提出的新调度规则通过考虑二阶Lyapunov漂移项，实现了与理论下界的匹配，这对于理解和设计高性能的队列控制策略具有重要意义，尤其是在需要严格性能保证的实际应用中。"}}
{"id": "2506.20657", "title": "SuperSONIC: Cloud-Native Infrastructure for ML Inferencing", "authors": ["Dmitry Kondratyev", "Benedikt Riedel", "Yuan-Tang Chou", "Miles Cochran-Branson", "Noah Paladino", "David Schultz", "Mia Liu", "Javier Duarte", "Philip Harris", "Shih-Chieh Hsu"], "summary": "The increasing computational demand from growing data rates and complex\nmachine learning (ML) algorithms in large-scale scientific experiments has\ndriven the adoption of the Services for Optimized Network Inference on\nCoprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it\nto local or remote coprocessors to optimize resource utilization. Leveraging\nits portability to different types of coprocessors, SONIC enhances data\nprocessing and model deployment efficiency for cutting-edge research in high\nenergy physics (HEP) and multi-messenger astrophysics (MMA). We developed the\nSuperSONIC project, a scalable server infrastructure for SONIC, enabling the\ndeployment of computationally intensive tasks to Kubernetes clusters equipped\nwith graphics processing units (GPUs). Using NVIDIA Triton Inference Server,\nSuperSONIC decouples client workflows from server infrastructure, standardizing\ncommunication, optimizing throughput, load balancing, and monitoring.\nSuperSONIC has been successfully deployed for the CMS and ATLAS experiments at\nthe CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory\n(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)\nand tested on Kubernetes clusters at Purdue University, the National Research\nPlatform (NRP), and the University of Chicago. SuperSONIC addresses the\nchallenges of the Cloud-native era by providing a reusable, configurable\nframework that enhances the efficiency of accelerator-based inference\ndeployment across diverse scientific domains and industries.", "comment": "Submission to PEARC25 Conference", "pdf_url": "http://arxiv.org/pdf/2506.20657v1", "categories": ["cs.DC", "hep-ex", "physics.ins-det"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20657v1", "AI": {"title_translation": "SuperSONIC：面向ML推理的云原生基础设施", "tldr": "SuperSONIC是一个可扩展的云原生服务器基础设施，用于将机器学习推理任务部署到配备GPU的Kubernetes集群，以优化大规模科学实验中的计算效率。", "motivation": "大规模科学实验中，日益增长的数据速率和复杂的机器学习算法导致计算需求增加，需要优化资源利用率来加速ML推理。", "method": "开发了SuperSONIC项目，一个用于SONIC的可扩展服务器基础设施，通过NVIDIA Triton Inference Server将计算密集型任务部署到配备GPU的Kubernetes集群，实现客户端工作流与服务器基础设施解耦，标准化通信，优化吞吐量、负载均衡和监控。", "result": "SuperSONIC已成功部署于CERN LHC的CMS和ATLAS实验、IceCube中微子天文台和LIGO，并在普渡大学、NRP和芝加哥大学的Kubernetes集群上进行了测试。", "conclusion": "SuperSONIC通过提供一个可重用、可配置的框架，解决了云原生时代的挑战，提高了跨不同科学领域和行业基于加速器的推理部署效率。", "translation": "随着大规模科学实验中数据速率的增长和复杂机器学习(ML)算法带来的计算需求不断增加，推动了对协处理器优化网络推理服务(SONIC)方法的采用。SONIC通过将ML推理卸载到本地或远程协处理器来优化资源利用率，从而加速ML推理。SONIC利用其对不同类型协处理器的可移植性，提高了高能物理(HEP)和多信使天体物理(MMA)等前沿研究的数据处理和模型部署效率。我们开发了SuperSONIC项目，这是一个可扩展的SONIC服务器基础设施，能够将计算密集型任务部署到配备图形处理单元(GPU)的Kubernetes集群。SuperSONIC使用NVIDIA Triton推理服务器，将客户端工作流与服务器基础设施解耦，从而实现通信标准化、优化吞吐量、负载均衡和监控。SuperSONIC已成功部署于欧洲核子研究中心(CERN)大型强子对撞机(LHC)的CMS和ATLAS实验、冰立方中微子观测站(IceCube)和激光干涉引力波天文台(LIGO)，并在普渡大学、国家研究平台(NRP)和芝加哥大学的Kubernetes集群上进行了测试。SuperSONIC通过提供一个可重用、可配置的框架，解决了云原生时代的挑战，提高了跨不同科学领域和行业基于加速器的推理部署效率。", "summary": "SuperSONIC是一个为大规模科学实验中机器学习推理而设计的云原生基础设施。它基于SONIC方法，通过将ML推理卸载到协处理器以优化资源利用。SuperSONIC是一个可扩展的服务器基础设施，利用NVIDIA Triton Inference Server将计算任务部署到配备GPU的Kubernetes集群，从而解耦客户端工作流并优化性能。该系统已成功应用于HEP和MMA领域的多个大型科学实验，证明其在提高加速器推理部署效率方面的能力。", "keywords": "云原生, 机器学习推理, Kubernetes, GPU, SONIC", "comments": "本文介绍了SuperSONIC，一个针对ML推理的云原生基础设施，创新性地将SONIC方法与Kubernetes和GPU结合，以应对大规模科学实验中的计算挑战。其重要性在于提供了一个可重用、可配置的框架，显著提升了跨多个科学领域的数据处理和模型部署效率。成功的实际部署案例进一步验证了其有效性和实用性。"}}
{"id": "2506.20288", "title": "Lightweight Target-Speaker-Based Overlap Transcription for Practical Streaming ASR", "authors": ["Aleš Pražák", "Marie Kunešová", "Josef Psutka"], "summary": "Overlapping speech remains a major challenge for automatic speech recognition\n(ASR) in real-world applications, particularly in broadcast media with dynamic,\nmulti-speaker interactions. We propose a light-weight, target-speaker-based\nextension to an existing streaming ASR system to enable practical transcription\nof overlapping speech with minimal computational overhead. Our approach\ncombines a speaker-independent (SI) model for standard operation with a\nspeaker-conditioned (SC) model selectively applied in overlapping scenarios.\nOverlap detection is achieved using a compact binary classifier trained on\nfrozen SI model output, offering accurate segmentation at negligible cost. The\nSC model employs Feature-wise Linear Modulation (FiLM) to incorporate speaker\nembeddings and is trained on synthetically mixed data to transcribe only the\ntarget speaker. Our method supports dynamic speaker tracking and reuses\nexisting modules with minimal modifications. Evaluated on a challenging set of\nCzech television debates with 16% overlap, the system reduced WER on\noverlapping segments from 68.0% (baseline) to 35.78% while increasing total\ncomputational load by only 44%. The proposed system offers an effective and\nscalable solution for overlap transcription in continuous ASR services.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20288v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.20288v1", "AI": {"title_translation": "轻量级目标说话人重叠语音转录，用于实用流式ASR", "tldr": "提出一种轻量级目标说话人扩展，用于流式ASR系统，有效降低重叠语音的词错误率，计算开销小。", "motivation": "重叠语音是实际应用中ASR面临的主要挑战，尤其是在广播媒体中。", "method": "结合扬声器无关（SI）模型和扬声器条件（SC）模型，在重叠场景中选择性应用SC模型。使用紧凑的二元分类器检测重叠。SC模型采用特征级线性调制（FiLM）来融合说话人嵌入，并用合成混合数据训练以仅转录目标说话人。支持动态说话人跟踪并重用现有模块。", "result": "在捷克电视辩论数据集上，重叠段的词错误率从基线的68.0%降低到35.78%，总计算负载仅增加44%。", "conclusion": "提出的系统为连续ASR服务中的重叠语音转录提供了一种有效且可扩展的解决方案。", "translation": "重叠语音仍然是自动语音识别（ASR）在实际应用中面临的主要挑战，尤其是在具有动态、多说话人交互的广播媒体中。我们提出了一种轻量级的、基于目标说话人的扩展方案，用于现有流式ASR系统，以实现重叠语音的实用转录，同时计算开销最小。我们的方法将用于标准操作的说话人无关（SI）模型与在重叠场景中选择性应用的说话人条件（SC）模型相结合。重叠检测通过一个在冻结SI模型输出上训练的紧凑型二元分类器实现，以可忽略的成本提供准确的分割。SC模型采用特征级线性调制（FiLM）来融合说话人嵌入，并使用合成混合数据进行训练，以仅转录目标说话人。我们的方法支持动态说话人跟踪，并以最小的修改重用现有模块。在具有16%重叠的挑战性捷克电视辩论数据集上进行评估，该系统将重叠片段的词错误率从68.0%（基线）降低到35.78%，同时总计算负载仅增加44%。所提出的系统为连续ASR服务中的重叠语音转录提供了一种有效且可扩展的解决方案。", "summary": "本文提出一种轻量级、基于目标说话人的流式ASR系统扩展，旨在解决重叠语音转录的挑战。该方法结合了说话人无关模型和在重叠场景中选择性应用的说话人条件模型，并通过一个高效的二元分类器进行重叠检测。说话人条件模型利用FiLM技术并基于合成数据训练，以实现目标说话人转录。实验结果表明，该系统显著降低了重叠语音的词错误率，同时保持了较低的计算开销，为实用ASR服务提供了可扩展的解决方案。", "keywords": "重叠语音转录, 流式ASR, 目标说话人, 轻量级, FiLM", "comments": "该论文的创新点在于提出了一个轻量级且高效的混合模型方法来处理流式ASR中的重叠语音，通过结合SI和SC模型，并在SC模型中利用FiLM技术，实现了在计算开销最小化的情况下显著提升转录准确性。其在实际应用场景中的实用性和可扩展性是其重要价值所在。"}}
{"id": "2506.20299", "title": "Enhancing Programming Pair Workshops: The Case of Teacher Pre-Prompting", "authors": ["Johan Petersson"], "summary": "This paper explores the pedagogical potential of \"teacher pre-prompting\" as a\nmeans of guiding student collaboration in programming education. In particular,\nwe investigate how brief teacher-initiated questions posed before students\nengage in pair programming workshops can help shape problem interpretation and\ndivision of labor. Based on qualitative analysis of video data from a\nuniversity course in systems development, we identify five distinct\npre-prompting patterns. Our findings suggest that such prompts can foster\nstructured discussions, clarify task requirements, and create opportunities for\nshared learning experiences.", "comment": "10 pages, 2 figures. Author's preprint of article published in\n  SIGED/ECISER 2024 via AIS Electronic Library. The published version is\n  available at: https://aisel.aisnet.org/siged2024/15/", "pdf_url": "http://arxiv.org/pdf/2506.20299v1", "categories": ["cs.CY", "68-01, 97Q60", "K.3.2; K.3.1"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20299v1", "AI": {"title_translation": "增强编程结对工作坊：教师预提示的案例", "tldr": "本文探讨了教师预提示在编程教育中引导学生协作的潜力，发现其能促进讨论、明确任务并创造共享学习体验。", "motivation": "旨在探索“教师预提示”在编程教育中引导学生协作的教学潜力，特别是教师在学生进行结对编程工作坊前提出的简短问题如何帮助塑造问题理解和分工。", "method": "通过对一门大学系统开发课程的视频数据进行定性分析。", "result": "识别出五种不同的预提示模式，并发现这些提示能够促进结构化讨论、澄清任务要求以及创造共享学习体验的机会。", "conclusion": "教师预提示是指导编程教育中学生协作的有效手段，能够带来积极的学习成果。", "translation": "本文探讨了“教师预提示”作为指导编程教育中学生协作的一种手段的教学潜力。特别是，我们研究了在学生参与结对编程工作坊之前提出的简短教师引导性问题如何有助于塑造问题解释和劳动分工。基于对一门大学系统开发课程的视频数据进行的定性分析，我们识别出五种不同的预提示模式。我们的研究结果表明，此类提示可以促进结构化讨论、澄清任务要求，并创造共享学习体验的机会。", "summary": "本文研究了教师预提示在编程结对工作坊中的教学效果，通过对大学课程视频数据进行定性分析，识别出五种预提示模式。研究表明，教师在结对编程前提供的简短问题能有效引导学生理解问题和分工，从而促进结构化讨论、明确任务并增强共享学习体验。", "keywords": "教师预提示, 结对编程, 编程教育, 学生协作, 定性分析", "comments": "这项研究创新性地提出了“教师预提示”的概念，并提供了其在编程教育中实际应用的证据。通过定性分析，它揭示了预提示如何影响学生的问题理解、分工和协作质量，为编程教育的教学实践提供了具体的指导。其局限性可能在于研究范围仅限于一个大学课程的定性分析，未来可进行更大规模或定量研究以验证其普适性。"}}
{"id": "2506.20238", "title": "A Data-Driven Approach for Topology Correction in Low Voltage Networks with DERs", "authors": ["Dong Liu", "Sander Timmerman", "Yu Xiang", "Peter Palensky", "Pedro P. Vergara"], "summary": "This paper introduces a data-driven topology identification and correction\napproach for low-voltage distribution networks (LVDNs) combined with a\ntime-based smart meter data selection strategy, aiming to correct outdated\nrecordings and identify the missed recordings. The proposed approach solely\nrelies on voltage magnitude measurements, releasing privacy concerns and\nmeasurement burdens. It enables the distribution system operators to identify\nswitch states through supervised learning algorithms, as well as determine\nuser-feeder connections and phase labels of customers by a modified\nHierarchical Clustering algorithm. To address the similarity among smart meter\n(SM) data caused by distributed photovoltaic (PV) systems, a time-based SM data\nselection strategy is combined with the proposed correlation analysis. The\nfeasibility and robustness of the proposed approach are validated using\nmodified real-world LVDNs and multiple incomplete SM datasets collected from\ncustomers in the Netherlands. The results demonstrate that the time-based SM\ndata selection strategy effectively mitigates their impact on phase\nidentification, and the corrected topology not only improves network\nobservability but also supports network operators in load balancing and PV\nconsumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20238v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20238v1", "AI": {"title_translation": "低压配电网中含分布式能源的拓扑校正数据驱动方法", "tldr": "本文提出了一种数据驱动的拓扑识别和校正方法，用于低压配电网，结合时间智能电表数据选择策略，仅依靠电压幅值测量，解决了拓扑数据过时和缺失的问题，并提高了网络可观测性和运营支持。", "motivation": "旨在校正低压配电网中过时的记录并识别缺失的记录，同时解决分布式光伏系统导致的智能电表数据相似性问题。", "method": "提出了一种数据驱动的拓扑识别和校正方法，结合了基于时间的智能电表数据选择策略。该方法仅依赖电压幅值测量，使用监督学习算法识别开关状态，并通过改进的分层聚类算法确定用户-馈线连接和客户的相标签。同时，将时间智能电表数据选择策略与相关性分析相结合，以处理分布式光伏系统引起的智能电表数据相似性。", "result": "该方法的有效性和鲁棒性通过修改后的真实低压配电网和从荷兰客户收集的多个不完整智能电表数据集进行了验证。结果表明，基于时间的智能电表数据选择策略有效减轻了光伏系统对相识别的影响，并且校正后的拓扑结构不仅提高了网络可观测性，还支持网络运营商进行负荷平衡和光伏消纳。", "conclusion": "本文提出的数据驱动拓扑识别和校正方法，结合时间智能电表数据选择策略，能够有效校正低压配电网的拓扑数据，解决隐私和测量负担问题，并显著提高网络的可观测性，支持网络运营商的负荷平衡和光伏消纳。", "translation": "本文介绍了一种数据驱动的拓扑识别和校正方法，用于低压配电网（LVDN），结合了基于时间的智能电表数据选择策略，旨在校正过时的记录并识别缺失的记录。所提出的方法仅依赖电压幅值测量，从而消除了隐私担忧和测量负担。它使配电系统运营商能够通过监督学习算法识别开关状态，并通过改进的分层聚类算法确定用户-馈线连接和客户的相标签。为了解决分布式光伏（PV）系统引起的智能电表（SM）数据相似性问题，将基于时间的智能电表数据选择策略与所提出的相关性分析相结合。所提出方法的可行性和鲁棒性已通过修改后的真实低压配电网和从荷兰客户收集的多个不完整智能电表数据集进行了验证。结果表明，基于时间的智能电表数据选择策略有效减轻了它们对相识别的影响，并且校正后的拓扑结构不仅提高了网络可观测性，还支持网络运营商进行负荷平衡和光伏消纳。", "summary": "本文提出了一种针对低压配电网的数据驱动拓扑识别与校正方法，该方法整合了基于时间的智能电表数据选择策略，旨在解决拓扑记录过时和缺失的问题。该方法仅利用电压幅值数据，通过监督学习识别开关状态，并利用改进的分层聚类算法确定用户-馈线连接和相标签。为应对分布式光伏系统造成的智能电表数据相似性，引入了时间数据选择策略。实验验证表明，该方法有效提升了拓扑识别的准确性，并改善了网络可观测性，同时支持负荷平衡和光伏消纳。", "keywords": "拓扑校正, 低压配电网, 数据驱动, 智能电表, 分布式能源", "comments": "该论文的创新点在于提出了一个纯数据驱动的低压配电网拓扑校正方法，仅依赖电压幅值测量，有效解决了隐私和测量负担问题。特别值得注意的是，其结合时间智能电表数据选择策略来应对分布式光伏系统引起的智能电表数据相似性，增强了相识别的准确性。这对于提高配电网的运行效率、可观测性以及支持新能源消纳具有重要意义。"}}
{"id": "2506.19956", "title": "Revisiting R: Statistical Envelope Analysis for Lightweight RF Modulation Classification", "authors": ["Srinivas Rahul Sapireddy", "Mostafizur Rahman"], "summary": "Modulation classification plays a crucial role in wireless communication\nsystems, enabling applications such as cognitive radio, spectrum monitoring,\nand electronic warfare. Conventional techniques often involve deep learning or\ncomplex feature extraction, which, while effective, require substantial\ncomputational resources and memory. An early approach by Chan and Gadbois in\n1985 introduced a theoretical method for modulation classification using a\nmathematically derived parameter called R. The authors proved that the R value\n- the ratio of the variance to the square of the mean of the signal envelope -\ncan be a distinguishing feature for classification. In this work, we revisit\nthe R value and show that classification accuracy can be improved further\nthrough statistical methods. We extend R-value analysis to demonstrate its\neffectiveness even after signals are transformed using the Hilbert transform\nfollowed by the Short-Time Fourier Transform (STFT). Our analysis includes\ntesting on 300000 signals across AM, DSB, and SSB classes, with each class\nhaving 100000 random variations. On average, we achieve 98.60, 97.30, and 97.90\npercent classification accuracy for AM, DSB, and SSB signals after applying the\nHilbert transform. Similar or better accuracies are observed after applying the\nSTFT, reaching 98.80, 99.10, and 99.00 percent, respectively, for AM, DSB, and\nSSB types.", "comment": "Accepted at RFCoN 2025: First International Conference on RF\n  Communication and Networks", "pdf_url": "http://arxiv.org/pdf/2506.19956v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.19956v1", "AI": {"title_translation": "重新审视R值：用于轻量级射频调制分类的统计包络分析", "tldr": "本文重新审视了1985年提出的R值调制分类方法，并利用统计方法改进了其准确性，即使在信号经过希尔伯特变换和短时傅里叶变换后，也能在AM、DSB和SSB信号上达到接近99%的分类精度，提供了一种轻量级的替代方案。", "motivation": "调制分类在无线通信系统中至关重要，但传统的深度学习或复杂特征提取方法需要大量的计算资源和内存。本文旨在重新审视并改进一种早期提出的轻量级R值方法，以提供一个更高效的替代方案。", "method": "本文重新审视了Chan和Gadbois于1985年提出的基于R值（信号包络方差与均值平方之比）的调制分类方法。通过引入统计方法进一步提高了分类精度。研究还将R值分析扩展到经过希尔伯特变换和短时傅里叶变换（STFT）的信号。实验在包含30万个AM、DSB和SSB信号的数据集上进行测试。", "result": "在应用希尔伯特变换后，AM、DSB和SSB信号的平均分类精度分别为98.60%、97.30%和97.90%。在应用STFT后，AM、DSB和SSB信号的精度分别达到98.80%、99.10%和99.00%，表现出相似或更优的性能。", "conclusion": "通过统计方法增强R值分析，即使在信号经过希尔伯特变换和STFT转换后，也能实现高精度的调制分类，证明了其作为轻量级射频调制分类方法的有效性和潜力。", "translation": "调制分类在无线通信系统中扮演着关键角色，支持认知无线电、频谱监测和电子战等应用。传统技术通常涉及深度学习或复杂的特征提取，虽然有效，但需要大量的计算资源和内存。Chan和Gadbois在1985年提出了一种早期方法，通过一个数学推导的参数R值进行调制分类。作者证明了R值——信号包络的方差与均值平方之比——可以作为分类的区分特征。在这项工作中，我们重新审视了R值，并表明通过统计方法可以进一步提高分类精度。我们将R值分析扩展到即使信号经过希尔伯特变换和短时傅里叶变换（STFT）后也能证明其有效性。我们的分析包括对30万个信号（涵盖AM、DSB和SSB类别，每个类别包含10万个随机变化）进行测试。平均而言，在应用希尔伯特变换后，我们对AM、DSB和SSB信号分别实现了98.60%、97.30%和97.90%的分类精度。在应用STFT后，观察到相似或更好的精度，AM、DSB和SSB类型分别达到98.80%、99.10%和99.00%。", "summary": "本文重新审视了1985年由Chan和Gadbois提出的基于R值的轻量级调制分类方法。R值是信号包络的方差与均值平方之比，已被证明是有效的分类特征。作者通过引入统计方法，进一步提高了R值分析的分类精度，并验证了该方法在经过希尔伯特变换和短时傅里叶变换（STFT）的信号上的有效性。实验在包含AM、DSB和SSB三类共30万个信号的数据集上进行，结果显示，在经过希尔伯特变换后，平均分类精度接近98%，而在应用STFT后，精度可达99%，证明了该方法在实现高精度调制分类方面的轻量级和有效性。", "keywords": "调制分类, R值, 统计包络分析, 轻量级, 射频通信", "comments": "本文的创新点在于重新审视并有效改进了一个经典的轻量级调制分类方法，而非一味追求复杂的深度学习模型。其重要性在于为资源受限的无线通信系统提供了一种高效且准确的调制分类方案。研究展示了将传统方法与现代统计增强相结合的潜力，并验证了其在变换域中的鲁棒性。"}}
{"id": "2506.20200", "title": "MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment", "authors": ["Siqiao Li", "Chen Hui", "Wei Zhang", "Rui Liang", "Chenyue Song", "Feng Jiang", "Haiqi Zhu", "Zhixuan Li", "Hong Huang", "Xiang Li"], "summary": "Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical\nrole in medical imaging, combining functional and anatomical information to aid\nin accurate diagnosis. However, image quality degradation due to noise,\ncompression and other factors could potentially lead to diagnostic uncertainty\nand increase the risk of misdiagnosis. When evaluating the quality of a PET/CT\nimage, both low-level features like distortions and high-level features like\norgan anatomical structures affect the diagnostic value of the image. However,\nexisting medical image quality assessment (IQA) methods are unable to account\nfor both feature types simultaneously. In this work, we propose MS-IQA, a novel\nmulti-scale feature fusion network for PET/CT IQA, which utilizes multi-scale\nfeatures from various intermediate layers of ResNet and Swin Transformer,\nenhancing its ability of perceiving both local and global information. In\naddition, a multi-scale feature fusion module is also introduced to effectively\ncombine high-level and low-level information through a dynamically weighted\nchannel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,\nwe construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT\nimages with quality scores assigned by radiologists. Experiments on our dataset\nand the publicly available LDCTIQAC2023 dataset demonstrate that our proposed\nmodel has achieved superior performance against existing state-of-the-art\nmethods in various IQA metrics. This work provides an accurate and efficient\nIQA method for PET/CT. Our code and dataset are available at\nhttps://github.com/MS-IQA/MS-IQA/.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20200v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20200v1", "AI": {"title_translation": "MS-IQA：一种用于PET/CT图像质量评估的多尺度特征融合网络", "tldr": "MS-IQA是一种新颖的多尺度特征融合网络，用于PET/CT图像质量评估，通过融合ResNet和Swin Transformer的多尺度特征来感知局部和全局信息，并引入动态加权通道注意力机制。该方法在自建的PET-CT-IQA-DS数据集和公开的LDCTIQAC2023数据集上均优于现有最先进方法。", "motivation": "正电子发射断层扫描/计算机断层扫描（PET/CT）在医学成像中至关重要，但图像质量下降（如噪声、压缩）可能导致诊断不确定性和误诊风险增加。现有医学图像质量评估（IQA）方法无法同时考虑影响诊断价值的低级（如失真）和高级（如器官解剖结构）特征。", "method": "本文提出了MS-IQA，一种用于PET/CT IQA的新型多尺度特征融合网络。它利用ResNet和Swin Transformer不同中间层的多尺度特征来增强对局部和全局信息的感知能力。此外，还引入了一个多尺度特征融合模块，通过动态加权通道注意力机制有效结合高级和低级信息。为填补PET/CT IQA数据集的空白，构建了PET-CT-IQA-DS数据集，包含2700张由放射科医生评分的PET/CT图像。", "result": "在自建的PET-CT-IQA-DS数据集和公开的LDCTIQAC2023数据集上的实验表明，所提出的模型在各种IQA指标上均优于现有最先进方法。", "conclusion": "这项工作为PET/CT提供了一种准确高效的图像质量评估方法。", "translation": "正电子发射断层扫描/计算机断层扫描（PET/CT）在医学成像中发挥着关键作用，结合了功能和解剖信息以辅助准确诊断。然而，由于噪声、压缩和其他因素导致的图像质量下降可能导致诊断不确定性并增加误诊风险。在评估PET/CT图像质量时，失真等低级特征和器官解剖结构等高级特征都会影响图像的诊断价值。然而，现有的医学图像质量评估（IQA）方法无法同时考虑这两种特征类型。在这项工作中，我们提出了MS-IQA，一种用于PET/CT IQA的新型多尺度特征融合网络，它利用ResNet和Swin Transformer不同中间层的多尺度特征，增强其感知局部和全局信息的能力。此外，还引入了一个多尺度特征融合模块，通过动态加权通道注意力机制有效地结合高级和低级信息。最后，为了填补PET/CT IQA数据集的空白，我们构建了PET-CT-IQA-DS，一个包含2700张具有放射科医生分配质量评分的不同质量PET/CT图像的数据集。我们数据集和公开可用的LDCTIQAC2023数据集上的实验表明，我们提出的模型在各种IQA指标上均优于现有最先进方法。这项工作为PET/CT提供了一种准确高效的IQA方法。我们的代码和数据集可在https://github.com/MS-IQA/MS-IQA/获取。", "summary": "本研究提出了一种名为MS-IQA的多尺度特征融合网络，用于PET/CT图像质量评估。该网络通过整合ResNet和Swin Transformer的多尺度特征，并利用动态加权通道注意力机制的融合模块，有效结合了图像的局部和全局信息，解决了现有方法无法同时处理低级和高级特征的问题。此外，为弥补PET/CT IQA数据集的不足，研究构建了PET-CT-IQA-DS数据集。实验结果表明，MS-IQA在多个IQA指标上均优于现有先进方法，为PET/CT图像质量评估提供了一种准确高效的解决方案。", "keywords": "PET/CT, 图像质量评估, 多尺度特征融合, 深度学习, 医学成像", "comments": "该论文的创新点在于提出了一个多尺度特征融合网络（MS-IQA），能够同时感知和融合PET/CT图像中的低级和高级特征，这在现有医学图像质量评估方法中是一个显著的进步。此外，为了解决PET/CT IQA数据集稀缺的问题，研究团队构建并公开了一个大规模数据集PET-CT-IQA-DS，这对于推动该领域的研究具有重要意义。"}}
{"id": "2506.19952", "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19952v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.19952v1", "AI": {"title_translation": "CycleDistill：使用LLM通过循环蒸馏引导机器翻译", "tldr": "CycleDistill利用LLM和少量样本翻译，通过迭代生成合成并行语料库来训练高质量的机器翻译系统，特别适用于低资源语言，无需大量并行语料。", "motivation": "大型语言模型（LLM）在少量样本机器翻译（MT）方面表现不如专用MT系统，而高质量MT系统依赖并行语料库。然而，对于低资源语言，并行语料库通常稀缺或不存在，这限制了其MT系统的发展。", "method": "本文提出了CycleDistill，一种引导方法，通过零样本或少量样本机器翻译从单语语料库中迭代生成合成并行语料。这些合成语料随后被用于微调生成数据的模型。该方法仅需1到4个少量样本示例。研究还探讨了在蒸馏过程中利用softmax激活的效果。", "result": "在针对三种印度语言的实验中，CycleDistill仅依靠单语语料库，在第一次迭代中平均比少量样本基线模型提高了20-30 chrF点，实现了高质量机器翻译。同时，利用softmax激活观察到翻译质量略有改善。", "conclusion": "CycleDistill是一种有效的引导方法，能够为低资源语言构建高质量的机器翻译系统，显著优于少量样本基线，且对并行语料的需求极低，仅需少量示例。", "translation": "大型语言模型（LLM）尽管能够执行少样本机器翻译（MT），但通常落后于在并行语料库上训练的专用MT系统，而并行语料库对于高质量机器翻译至关重要。然而，对于低资源语言，并行语料库通常稀缺或不存在。在本文中，我们提出了CycleDistill，一种利用LLM和少样本翻译来获取高质量MT系统的引导方法。CycleDistill涉及通过零样本或少样本MT从单语语料库迭代生成合成并行语料，然后使用这些语料微调用于生成数据的MT模型。CycleDistill除了1到4个少样本示例外，不需要并行语料。在针对三种印度语言的实验中，仅依靠单语语料库，它可以在第一次迭代中实现高质量机器翻译，平均比少样本基线模型提高20-30 chrF点。我们还研究了在蒸馏过程中利用softmax激活的效果，并观察到翻译质量略有改善。", "summary": "CycleDistill是一种新颖的机器翻译引导方法，旨在解决低资源语言缺乏并行语料的问题。它利用大型语言模型进行迭代的合成并行语料生成，并通过这些合成数据微调MT模型。该方法在仅有少量样本的情况下，显著提升了低资源语言的机器翻译质量，并在实验中展现出优于传统少样本基线的性能。", "keywords": "机器翻译, 低资源语言, 大型语言模型, 循环蒸馏, 合成语料库", "comments": "这项工作创新性地结合了LLM的少样本能力和循环蒸馏的思想，有效解决了低资源语言机器翻译中并行语料稀缺的核心问题。其无需大量并行语料即可实现高质量翻译的特性，对于推动全球语言多样性和促进信息交流具有重要意义。"}}
{"id": "2506.20202", "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer", "authors": ["Da Li", "Donggang Jia", "Yousef Rajeh", "Dominik Engel", "Ivan Viola"], "summary": "With the advancement of Gaussian Splatting techniques, a growing number of\ndatasets based on this representation have been developed. However, performing\naccurate and efficient clipping for Gaussian Splatting remains a challenging\nand unresolved problem, primarily due to the volumetric nature of Gaussian\nprimitives, which makes hard clipping incapable of precisely localizing their\npixel-level contributions. In this paper, we propose a hybrid rendering\nframework that combines rasterization and ray tracing to achieve efficient and\nhigh-fidelity clipping of Gaussian Splatting data. At the core of our method is\nthe RaRa strategy, which first leverages rasterization to quickly identify\nGaussians intersected by the clipping plane, followed by ray tracing to compute\nattenuation weights based on their partial occlusion. These weights are then\nused to accurately estimate each Gaussian's contribution to the final image,\nenabling smooth and continuous clipping effects. We validate our approach on\ndiverse datasets, including general Gaussians, hair strand Gaussians, and\nmulti-layer Gaussians, and conduct user studies to evaluate both perceptual\nquality and quantitative performance. Experimental results demonstrate that our\nmethod delivers visually superior results while maintaining real-time rendering\nperformance and preserving high fidelity in the unclipped regions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20202v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20202v1", "AI": {"title_translation": "RaRa Clipper：一种基于光线追踪器和光栅器的Gaussian Splatting裁剪器", "tldr": "本文提出了一种结合光栅化和光线追踪的混合渲染框架RaRa Clipper，以实现高保真和高效的Gaussian Splatting数据裁剪。", "motivation": "由于高斯基元的体积性质，对Gaussian Splatting进行精确高效的裁剪仍然是一个具有挑战性的未解决问题，传统的硬裁剪无法精确地定位其像素级贡献。", "method": "我们提出了一种混合渲染框架，结合光栅化和光线追踪，以实现Gaussian Splatting数据的高效高保真裁剪。核心是RaRa策略：首先利用光栅化快速识别与裁剪平面相交的高斯，然后通过光线追踪计算基于部分遮挡的衰减权重，这些权重用于精确估计每个高斯对最终图像的贡献，从而实现平滑连续的裁剪效果。", "result": "我们的方法在通用高斯、发丝高斯和多层高斯等多样化数据集上进行了验证，并进行了用户研究以评估感知质量和定量性能。实验结果表明，我们的方法在保持实时渲染性能和未裁剪区域高保真度的同时，提供了视觉上更优越的结果。", "conclusion": "本文提出的RaRa Clipper方法成功解决了Gaussian Splatting的裁剪难题，通过结合光栅化和光线追踪实现了高保真、实时且视觉效果优越的裁剪效果。", "translation": "随着Gaussian Splatting技术的进步，基于这种表示的数据集越来越多。然而，对Gaussian Splatting进行精确高效的裁剪仍然是一个具有挑战性且尚未解决的问题，这主要归因于高斯基元的体积性质，使得硬裁剪无法精确地定位其像素级贡献。在本文中，我们提出了一种混合渲染框架，结合光栅化和光线追踪，以实现Gaussian Splatting数据的高效高保真裁剪。我们方法的核心是RaRa策略，它首先利用光栅化快速识别与裁剪平面相交的高斯，然后通过光线追踪计算基于其部分遮挡的衰减权重。这些权重随后用于精确估计每个高斯对最终图像的贡献，从而实现平滑连续的裁剪效果。我们在各种数据集上验证了我们的方法，包括通用高斯、发丝高斯和多层高斯，并进行了用户研究以评估感知质量和定量性能。实验结果表明，我们的方法在保持实时渲染性能和未裁剪区域高保真度的同时，提供了视觉上更优越的结果。", "summary": "本文提出了一种名为RaRa Clipper的混合渲染框架，用于解决Gaussian Splatting数据的高效高保真裁剪问题。针对高斯基元的体积特性导致传统硬裁剪无法精确控制像素贡献的挑战，RaRa Clipper结合了光栅化和光线追踪。它首先使用光栅化快速识别被裁剪平面影响的高斯，然后利用光线追踪计算衰减权重以精确估计每个高斯的贡献，从而实现平滑连续的裁剪效果。实验证明，该方法在保持实时性能和未裁剪区域高保真度的同时，提供了卓越的视觉裁剪质量。", "keywords": "Gaussian Splatting, 裁剪, 光线追踪, 光栅化, 混合渲染", "comments": "RaRa Clipper的创新之处在于其混合渲染策略，巧妙地结合了光栅化和光线追踪的优势。光栅化用于快速粗筛，而光线追踪则提供了精细的像素级贡献计算，有效解决了Gaussian Splatting体积性质带来的裁剪难题。该方法在保持实时性能的同时显著提升了裁剪的视觉质量和精确度，对于基于Gaussian Splatting的应用具有重要意义。"}}
{"id": "2506.19993", "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems", "authors": ["Haochen Zhang", "Tianyi Zhang", "Junze Yin", "Oren Gal", "Anshumali Shrivastava", "Vladimir Braverman"], "summary": "Recommender systems play a pivotal role in providing relevant content to\nusers. With the rapid development of large language models (LLMs), researchers\nhave begun utilizing LLMs to build more powerful recommender systems. However,\nexisting approaches that focus on aligning LLMs with recommendation tasks do\nnot fully leverage their sequential information processing capabilities,\nleading to suboptimal performance.\n  In this paper, we propose a novel system called compressed vocabulary\nexpansion (CoVE). In CoVE, each item is assigned a unique ID within the\nexpanded vocabulary. Our framework effectively capitalizes on sequence\nunderstanding abilities of LLMs, significantly enhancing their performance on\nrecommendation tasks. Additionally, we compress the embedding layer, making\nCoVE practical for large-scale industrial applications. The effectiveness and\nperformance of CoVE are demonstrated through comprehensive experiments on\nmultiple recommendation datasets and comparisons with prior works. Our code can\nbe found at https://github.com/HaochenZhang717/CoVE-official-Repo.", "comment": "Accepted by ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.19993v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.19993v1", "AI": {"title_translation": "CoVE：压缩词汇扩展助力构建更优的基于LLM的推荐系统", "tldr": "CoVE通过为每个项目分配唯一ID并扩展词汇量，同时压缩嵌入层，从而提升基于LLM的推荐系统性能，使其更实用。", "motivation": "现有将大型语言模型（LLMs）应用于推荐系统的方法未能充分利用LLMs的序列信息处理能力，导致性能不佳。", "method": "本文提出了一种名为压缩词汇扩展（CoVE）的新型系统。在CoVE中，每个项目都被分配一个在扩展词汇表中的唯一ID。该框架有效利用了LLMs的序列理解能力，显著提升了其在推荐任务上的性能。此外，CoVE还压缩了嵌入层，使其适用于大规模工业应用。", "result": "通过在多个推荐数据集上的综合实验以及与现有工作的比较，证明了CoVE的有效性和性能。", "conclusion": "CoVE系统通过有效利用LLMs的序列理解能力并压缩嵌入层，显著提升了基于LLM的推荐系统在推荐任务上的性能，并使其适用于大规模工业应用。", "translation": "推荐系统在为用户提供相关内容方面发挥着关键作用。随着大型语言模型（LLMs）的快速发展，研究人员已开始利用LLMs构建更强大的推荐系统。然而，现有专注于将LLMs与推荐任务对齐的方法未能充分利用其序列信息处理能力，导致性能不佳。\n在本文中，我们提出了一种名为压缩词汇扩展（CoVE）的新型系统。在CoVE中，每个项目都被分配一个在扩展词汇表中的唯一ID。我们的框架有效地利用了LLMs的序列理解能力，显著提升了其在推荐任务上的性能。此外，我们压缩了嵌入层，使CoVE适用于大规模工业应用。通过在多个推荐数据集上的综合实验以及与现有工作的比较，证明了CoVE的有效性和性能。我们的代码可在https://github.com/HaochenZhang717/CoVE-official-Repo找到。", "summary": "本文提出了一种名为CoVE（压缩词汇扩展）的新型系统，旨在提升基于大型语言模型（LLM）的推荐系统性能。CoVE通过为每个项目分配独特的ID并扩展词汇表，从而充分利用LLM的序列理解能力，显著增强推荐效果。此外，该系统通过压缩嵌入层，使其在大型工业应用中具有实用性。实验结果表明，CoVE在多个推荐数据集上均表现出优异的有效性和性能。", "keywords": "LLM, 推荐系统, 词汇扩展, 序列理解, 嵌入压缩", "comments": "CoVE的创新点在于其将推荐系统中的项目映射到LLM的扩展词汇表中，并结合了嵌入层压缩技术。这不仅解决了现有LLM推荐系统未能充分利用序列信息的问题，还兼顾了大规模部署的实用性，对于推动LLM在推荐领域的应用具有重要意义。"}}
{"id": "2506.20609", "title": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings", "authors": ["Ankit Shah", "Rita Singh", "Bhiksha Raj", "Alexander Hauptmann"], "summary": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders.", "comment": "4 pages + 1 References", "pdf_url": "http://arxiv.org/pdf/2506.20609v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.20609v1", "AI": {"title_translation": "通过枪声录音的声学分析解读枪械类型层级", "tldr": "该研究通过对枪声录音进行声学分析并利用深度学习，提出了一种经济高效的枪械类型分类方法，旨在为执法机构提供及时准确的信息。", "motivation": "枪支相关暴力和大规模枪击事件的不断升级对公共安全构成重大威胁。现有的商业枪声检测系统虽然有效但成本高昂。本研究旨在探索一种成本效益更高的替代方案，通过声学分析枪声录音来检测枪声并分类枪械类型，从而为执法机构提供关键信息。", "method": "本研究利用包含3459个录音的精选数据集，深入探讨了枪声的基本声学特征，包括枪口冲击波和冲击波。研究提出并评估了机器学习框架，包括作为基线的支持向量机（SVM）和更先进的卷积神经网络（CNN）架构，用于联合枪声检测和枪械类型分类。", "result": "深度学习方法（CNN）在干净的标记数据上实现了0.58的平均精度（mAP），优于SVM基线（mAP 0.39）。研究还讨论了数据质量、环境噪声以及使用嘈杂网络源数据时的泛化能力（mAP 0.35）等相关挑战。", "conclusion": "长远目标是开发一个高精度、实时且可在普通录音设备上部署的系统，从而显著降低检测成本，并为急救人员提供关键情报。", "translation": "枪支相关暴力和大规模枪击事件的不断升级对公共安全构成重大威胁。为执法机构提供及时准确的信息对于缓解这些事件至关重要。目前的商业枪声检测系统虽然有效，但通常成本过高。本研究通过利用枪声录音的声学分析，探索了一种经济高效的替代方案，这些录音可能来自手机等无处不在的设备，不仅可以检测枪声，还可以对所使用的枪械类型进行分类。本文详细介绍了一项关于使用包含3459个录音的精选数据集来解读枪械类型层级的研究。我们调查了枪声的基本声学特征，包括枪口冲击波和冲击波，这些特征根据枪械类型、弹药和射击方向而变化。我们提出并评估了机器学习框架，包括作为基线的支持向量机（SVM）和更先进的卷积神经网络（CNN）架构，用于联合枪声检测和枪械类型分类。结果表明，我们的深度学习方法在干净的标记数据上实现了0.58的平均精度（mAP），优于SVM基线（mAP 0.39）。研究还讨论了与数据质量、环境噪声以及使用嘈杂网络源数据时的泛化能力（mAP 0.35）相关的挑战。长远目标是开发一个高精度、实时且可在普通录音设备上部署的系统，从而显著降低检测成本，并为急救人员提供关键情报。", "summary": "本研究提出了一种经济高效的枪声检测和枪械类型分类方法，通过对枪声录音进行声学分析，并利用深度学习（CNN）技术。该方法在干净数据上实现了0.58的平均精度，优于传统的SVM模型。研究讨论了数据质量和噪声对性能的影响，并展望了开发实时、可部署系统以增强公共安全的潜力。", "keywords": "枪声检测, 枪械分类, 声学分析, 深度学习, 卷积神经网络", "comments": "该论文的创新之处在于探索利用无处不在的设备（如手机）进行枪声检测和枪械类型分类，从而提供了一种成本效益高的解决方案。它通过结合声学分析和深度学习技术，为公共安全领域提供了新的思路。然而，研究也指出，在处理真实世界中常见的嘈杂数据时，模型的性能存在挑战，这表明在实际部署前还需要进一步提升模型的鲁棒性。"}}
{"id": "2506.19964", "title": "Higher-Order Neuromorphic Ising Machines -- Autoencoders and Fowler-Nordheim Annealers are all you need for Scalability", "authors": ["Faiek Ahsan", "Saptarshi Maiti", "Zihao Chen", "Jakob Kaiser", "Ankita Nandi", "Madhuvanthi Srivatsav", "Johannes Schemmel", "Andreas G. Andreou", "Jason Eshraghian", "Chetan Singh Thakur", "Shantanu Chakrabartty"], "summary": "We report a higher-order neuromorphic Ising machine that exhibits superior\nscalability compared to architectures based on quadratization, while also\nachieving state-of-the-art quality and reliability in solutions with\ncompetitive time-to-solution metrics. At the core of the proposed machine is an\nasynchronous autoencoder architecture that captures higher-order interactions\nby directly manipulating Ising clauses instead of Ising spins, thereby\nmaintaining resource complexity independent of interaction order. Asymptotic\nconvergence to the Ising ground state is ensured by sampling the autoencoder\nlatent space defined by the spins, based on the annealing dynamics of the\nFowler-Nordheim quantum mechanical tunneling. To demonstrate the advantages of\nthe proposed higher-order neuromorphic Ising machine, we systematically solved\nbenchmark combinatorial optimization problems such as MAX-CUT and MAX-SAT,\ncomparing the results to those obtained using a second-order Ising machine\nemploying the same annealing process. Our findings indicate that the proposed\narchitecture consistently provides higher quality solutions in shorter time\nframes compared to the second-order model across multiple runs. Additionally,\nwe show that the techniques based on the sparsity of the interconnection\nmatrix, such as graph coloring, can be effectively applied to higher-order\nneuromorphic Ising machines, enhancing the solution quality and the\ntime-to-solution. The time-to-solution can be further improved through hardware\nco-design, as demonstrated in this paper using a field-programmable gate array\n(FPGA). The results presented in this paper provide further evidence that\nautoencoders and Fowler-Nordheim annealers are sufficient to achieve\nreliability and scaling of any-order neuromorphic Ising machines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19964v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.19964v1", "AI": {"title_translation": "高阶神经形态伊辛机——自编码器和Fowler-Nordheim退火器是实现可扩展性所需的一切", "tldr": "该研究提出了一种基于自编码器和Fowler-Nordheim退火器的高阶神经形态伊辛机，其在可扩展性、解的质量和求解时间方面优于现有方法，并能有效解决组合优化问题。", "motivation": "现有基于二次化的伊辛机架构在可扩展性方面存在局限，并且需要一种能同时提供高解质量、可靠性和竞争力求解时间的更高阶神经形态伊辛机。", "method": "论文提出了一种高阶神经形态伊辛机。其核心是一个异步自编码器架构，通过直接操作伊辛子句而非伊辛自旋来捕获高阶相互作用，从而使资源复杂性与相互作用阶数无关。通过基于Fowler-Nordheim量子力学隧穿的退火动力学，对自旋定义的自编码器潜在空间进行采样，确保渐近收敛到伊辛基态。此外，还应用了基于互连矩阵稀疏性的技术（如图着色），并通过FPGA硬件协同设计进一步提升性能。", "result": "与采用相同退火过程的二阶伊辛机相比，所提出的高阶神经形态伊辛机在解决MAX-CUT和MAX-SAT等组合优化问题时，能够持续在更短的时间内提供更高质量的解决方案。基于稀疏性的技术（如图着色）可有效应用于高阶神经形态伊辛机，提高解的质量和求解时间。硬件协同设计（通过FPGA演示）能进一步缩短求解时间。", "conclusion": "自编码器和Fowler-Nordheim退火器足以实现任意阶神经形态伊辛机的可靠性和可扩展性。", "translation": "我们报告了一种高阶神经形态伊辛机，与基于二次化的架构相比，它展现出卓越的可扩展性，同时在解决方案的质量和可靠性方面达到了最先进水平，并具有竞争力的求解时间指标。所提出的机器的核心是一个异步自编码器架构，它通过直接操作伊辛子句而非伊辛自旋来捕获高阶相互作用，从而使资源复杂性与相互作用阶数无关。通过基于Fowler-Nordheim量子力学隧穿的退火动力学，对自旋定义的自编码器潜在空间进行采样，确保渐近收敛到伊辛基态。为了展示所提出的高阶神经形态伊辛机的优势，我们系统地解决了MAX-CUT和MAX-SAT等基准组合优化问题，并将结果与使用相同退火过程的二阶伊辛机获得的结果进行了比较。我们的研究结果表明，与二阶模型相比，所提出的架构在多次运行中始终能在更短的时间内提供更高质量的解决方案。此外，我们还展示了基于互连矩阵稀疏性的技术，如图着色，可以有效地应用于高阶神经形态伊辛机，从而提高解决方案的质量和求解时间。通过硬件协同设计，如本文中使用现场可编程门阵列（FPGA）所展示的，求解时间可以进一步改善。本文提出的结果进一步证明，自编码器和Fowler-Nordheim退火器足以实现任意阶神经形态伊辛机的可靠性和可扩展性。", "summary": "本文提出了一种创新的高阶神经形态伊辛机，其核心是一个异步自编码器架构，通过直接处理伊辛子句来高效捕获高阶相互作用，并结合Fowler-Nordheim量子隧穿退火机制确保收敛性。该机器在解决MAX-CUT和MAX-SAT等组合优化问题时，与传统二阶伊辛机相比，展现出卓越的可扩展性、更高的解质量和更短的求解时间。研究还表明，稀疏性技术和硬件协同设计能进一步提升其性能。这些发现强调了自编码器和Fowler-Nordheim退火器在构建可扩展且可靠的任意阶神经形态伊辛机中的关键作用。", "keywords": "高阶伊辛机, 神经形态计算, 自编码器, Fowler-Nordheim退火, 组合优化", "comments": "该论文创新性地提出了一种基于自编码器和Fowler-Nordheim退火机制的高阶神经形态伊辛机。其核心突破在于通过直接操作伊辛子句而非自旋，实现了资源复杂性与相互作用阶数无关，显著提升了可扩展性。结合量子隧穿退火，确保了收敛性和解的质量。这项工作为解决高阶组合优化问题提供了新的高效途径，并证明了特定AI模型和物理退火过程结合的强大潜力。论文还探讨了稀疏性优化和硬件协同设计，增强了其实用价值。"}}
{"id": "2506.20017", "title": "All-Pairs Shortest Paths with Few Weights per Node", "authors": ["Amir Abboud", "Nick Fischer", "Ce Jin", "Virginia Vassilevska Williams", "Zoe Xi"], "summary": "We study the central All-Pairs Shortest Paths (APSP) problem under the\nrestriction that there are at most $d$ distinct weights on the outgoing edges\nfrom every node. For $d=n$ this is the classical (unrestricted) APSP problem\nthat is hypothesized to require cubic time $n^{3-o(1)}$, and at the other\nextreme, for $d=1$, it is equivalent to the Node-Weighted APSP problem. We\npresent new algorithms that achieve the following results:\n  1. Node-Weighted APSP can be solved in time $\\tilde{O}(n^{(3+\\omega)/2}) =\n\\tilde{O}(n^{2.686})$, improving on the 15-year-old subcubic bounds\n$\\tilde{O}(n^{(9+\\omega)/4}) = \\tilde{O}(n^{2.843})$ [Chan; STOC '07] and\n$\\tilde{O}(n^{2.830})$ [Yuster; SODA '09]. This positively resolves the\nquestion of whether Node-Weighted APSP is an ``intermediate'' problem in the\nsense of having complexity $n^{2.5+o(1)}$ if $\\omega=2$, in which case it also\nmatches an $n^{2.5-o(1)}$ conditional lower bound.\n  2. For up to $d \\leq n^{3-\\omega-\\epsilon}$ distinct weights per node (where\n$\\epsilon > 0$), the problem can be solved in subcubic time\n$O(n^{3-f(\\epsilon)})$ (where $f(\\epsilon) > 0$). In particular, assuming that\n$\\omega = 2$, we can tolerate any sublinear number of distinct weights per node\n$d \\leq n^{1-\\epsilon}$, whereas previous work [Yuster; SODA '09] could only\nhandle $d \\leq n^{1/2-\\epsilon}$ in subcubic time. This promotes our\nunderstanding of the APSP hypothesis showing that the hardest instances must\nexhaust a linear number of weights per node. Our result also applies to the\nAll-Pairs Exact Triangle problem, thus generalizing a result of Chan and\nLewenstein on \"Clustered 3SUM\" from arrays to matrices. Notably, our technique\nconstitutes a rare application of additive combinatorics in graph algorithms.", "comment": "Appears at STOC '25. Abstract shortened to meet arXiv requirements", "pdf_url": "http://arxiv.org/pdf/2506.20017v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20017v1", "AI": {"title_translation": "每节点少量权重的全源最短路径问题", "tldr": "本文研究了每节点出边具有最多 $d$ 个不同权重的全源最短路径 (APSP) 问题，并提出了新的算法。对于节点加权 APSP (d=1)，将15年未变的次立方时间复杂度提升至 $\tilde{O}(n^{2.686})$。对于 $d \text{ 达到 } n^{3-\frac{\\omega}{2}-\\epsilon}$ 的情况，实现了次立方时间，显著放宽了对 $d$ 的限制，表明最难的 APSP 实例需要线性数量的每节点权重。", "motivation": "经典的全源最短路径 (APSP) 问题在 $d=n$ 时被认为需要 $n^{3-o(1)}$ 的时间。本文研究了当每个节点出边具有至多 $d$ 个不同权重时的 APSP 问题，旨在改进现有算法的性能，特别是节点加权 APSP (d=1) 的15年未变的次立方界限，并探索在每节点权重数量受限情况下的 APSP 复杂度，以增进对 APSP 假设的理解。", "method": "本文提出了新的算法来解决每节点出边具有少量权重的全源最短路径 (APSP) 问题。值得注意的是，该技术是图算法中加性组合学的一种罕见应用。", "result": "1. 节点加权 APSP (d=1) 可以在 $\tilde{O}(n^{(3+\\omega)/2}) = \tilde{O}(n^{2.686})$ 时间内解决，优于之前的 $\tilde{O}(n^{(9+\\omega)/4}) = \tilde{O}(n^{2.843})$ 和 $\tilde{O}(n^{2.830})$ 界限。这积极地解决了节点加权 APSP 是否是具有 $n^{2.5+o(1)}$ 复杂度的“中间”问题（如果 $\\omega=2$）的疑问。2. 对于每节点最多 $d \\leq n^{3-\\omega-\\epsilon}$ 个不同权重的情况（其中 $\\epsilon > 0$），问题可以在次立方时间 $O(n^{3-f(\\epsilon)})$ 内解决（其中 $f(\\epsilon) > 0$）。特别是在 $\\omega = 2$ 的假设下，可以容忍任何次线性数量的每节点不同权重 $d \\leq n^{1-\\epsilon}$，而之前的工作只能处理 $d \\leq n^{1/2-\\epsilon}$。该结果也适用于全对精确三角形问题。", "conclusion": "本文提出的新算法显著改进了节点加权 APSP 的时间复杂度，并解决了其是否为“中间”问题（复杂度为 $n^{2.5+o(1)}$）的疑问。此外，研究结果放宽了在次立方时间内解决 APSP 问题时每节点允许的不同权重的数量限制，表明最困难的 APSP 实例必须耗尽线性数量的每节点权重，从而促进了对 APSP 假设的理解。该技术也将加性组合学应用于图算法，具有普适性。", "translation": "我们研究了中心的全源最短路径 (APSP) 问题，其限制是每个节点的出边上最多有 $d$ 个不同的权重。当 $d=n$ 时，这是经典的（无限制的）APSP 问题，据推测需要立方时间 $n^{3-o(1)}$；而在另一个极端，当 $d=1$ 时，它等同于节点加权 APSP 问题。我们提出了新的算法，取得了以下结果：\n1. 节点加权 APSP 可以在 $\tilde{O}(n^{(3+\\omega)/2}) = \tilde{O}(n^{2.686})$ 时间内解决，改进了15年前的次立方界限 $\tilde{O}(n^{(9+\\omega)/4}) = \tilde{O}(n^{2.843})$ [Chan; STOC '07] 和 $\tilde{O}(n^{2.830})$ [Yuster; SODA '09]。这积极地解决了节点加权 APSP 是否是一个“中间”问题的问题，即如果 $\\omega=2$，其复杂度为 $n^{2.5+o(1)}$，在这种情况下它也符合 $n^{2.5-o(1)}$ 的条件下限。\n2. 对于每节点最多 $d \\leq n^{3-\\omega-\\epsilon}$ 个不同权重的情况（其中 $\\epsilon > 0$），问题可以在次立方时间 $O(n^{3-f(\\epsilon)})$ 内解决（其中 $f(\\epsilon) > 0$）。特别地，假设 $\\omega = 2$，我们可以容忍任何次线性数量的每节点不同权重 $d \\leq n^{1-\\epsilon}$，而之前的工作 [Yuster; SODA '09] 只能在次立方时间内处理 $d \\leq n^{1/2-\\epsilon}$。这促进了我们对 APSP 假设的理解，表明最困难的实例必须耗尽线性数量的每节点权重。我们的结果也适用于全对精确三角形问题，从而推广了 Chan 和 Lewenstein 关于“聚类 3SUM”从数组到矩阵的结果。值得注意的是，我们的技术构成了加性组合学在图算法中的罕见应用。", "summary": "本文研究了每节点出边权重受限的全源最短路径 (APSP) 问题。针对节点加权 APSP (d=1)，提出了新的算法，将其时间复杂度从 $\tilde{O}(n^{2.843})$ 或 $\tilde{O}(n^{2.830})$ 提升至 $\tilde{O}(n^{2.686})$，并解决了其是否为“中间”问题的疑问。此外，对于每节点最多 $d \\leq n^{3-\\omega-\\epsilon}$ 个不同权重的情况，实现了次立方时间复杂度，显著放宽了 $d$ 的限制，表明只有当每节点权重数量达到线性级别时，APSP 才需要立方时间。该研究还推广了全对精确三角形问题的结果，并创新性地将加性组合学应用于图算法。", "keywords": "全源最短路径, 节点加权 APSP, 加性组合学, 次立方时间, 图算法", "comments": "本文在全源最短路径 (APSP) 领域取得了显著进展，特别是在每节点权重受限的场景下。其主要创新在于为节点加权 APSP 提供了15年来的首次次立方时间改进，并成功解决了该问题是否为“中间”复杂度的疑问。此外，研究极大地扩展了在次立方时间内可处理的每节点不同权重数量的上限，加深了对 APSP 假设的理解，即最难的实例需要线性数量的权重。值得一提的是，该研究将加性组合学这一在图算法中不常见的技术应用于此，展示了其方法的独特性和潜在的普适性。"}}
{"id": "2506.20097", "title": "PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models", "authors": ["Wang Bill Zhu", "Miaosen Chai", "Ishika Singh", "Robin Jia", "Jesse Thomason"], "summary": "We propose PSALM-V, the first autonomous neuro-symbolic learning system able\nto induce symbolic action semantics (i.e., pre- and post-conditions) in visual\nenvironments through interaction. PSALM-V bootstraps reliable symbolic planning\nwithout expert action definitions, using LLMs to generate heuristic plans and\ncandidate symbolic semantics. Previous work has explored using large language\nmodels to generate action semantics for Planning Domain Definition Language\n(PDDL)-based symbolic planners. However, these approaches have primarily\nfocused on text-based domains or relied on unrealistic assumptions, such as\naccess to a predefined problem file, full observability, or explicit error\nmessages. By contrast, PSALM-V dynamically infers PDDL problem files and domain\naction semantics by analyzing execution outcomes and synthesizing possible\nerror explanations. The system iteratively generates and executes plans while\nmaintaining a tree-structured belief over possible action semantics for each\naction, iteratively refining these beliefs until a goal state is reached.\nSimulated experiments of task completion in ALFRED demonstrate that PSALM-V\nincreases the plan success rate from 37% (Claude-3.7) to 74% in partially\nobserved setups. Results on two 2D game environments, RTFM and Overcooked-AI,\nshow that PSALM-V improves step efficiency and succeeds in domain induction in\nmulti-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions\nfor real-world robot BlocksWorld tasks, despite low-level manipulation failures\nfrom the robot.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20097v1", "categories": ["cs.RO", "cs.CL"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20097v1", "AI": {"title_translation": "PSALM-V：利用大型语言模型在交互式视觉环境中自动化符号规划", "tldr": "PSALM-V是一个自主神经符号学习系统，它利用大型语言模型通过交互，在视觉环境中自动推断符号行动语义并进行规划，显著提高了在部分观测环境中的规划成功率。", "motivation": "以往使用大型语言模型生成PDDL符号规划器行动语义的方法，主要集中在文本域或依赖不切实际的假设，如预定义的问题文件、完全可观测性或明确的错误消息。PSALM-V旨在通过动态推断PDDL问题文件和领域行动语义来克服这些局限性。", "method": "PSALM-V是首个能够通过交互在视觉环境中推断符号行动语义（即前置条件和后置条件）的自主神经符号学习系统。它利用大型语言模型生成启发式规划和候选符号语义，并动态推断PDDL问题文件和领域行动语义，通过分析执行结果和合成可能的错误解释来实现。系统迭代生成和执行规划，同时对每个行动的可能行动语义保持树状信念结构，并迭代细化这些信念直到达到目标状态。", "result": "在ALFRED模拟任务完成实验中，PSALM-V在部分观测设置下将规划成功率从37%（Claude-3.7）提高到74%。在RTFM和Overcooked-AI两个2D游戏环境中，PSALM-V提高了步骤效率，并在多智能体设置中成功进行领域归纳。尽管机器人存在低级操作失败，PSALM-V仍能正确推断真实世界机器人BlocksWorld任务的PDDL前置和后置条件。", "conclusion": "PSALM-V成功地实现了在交互式视觉环境中自主符号规划，通过鲁棒地学习行动语义，即使在部分可观测性和机器人操作失败等挑战性条件下也表现出色。", "translation": "我们提出了PSALM-V，这是第一个能够通过交互在视觉环境中推断符号行动语义（即前置和后置条件）的自主神经符号学习系统。PSALM-V无需专家行动定义，利用大型语言模型生成启发式规划和候选符号语义，从而引导可靠的符号规划。以往的工作已经探索了使用大型语言模型为基于规划领域定义语言（PDDL）的符号规划器生成行动语义。然而，这些方法主要集中在基于文本的领域或依赖不切实际的假设，例如访问预定义的问题文件、完全可观测性或明确的错误消息。相比之下，PSALM-V通过分析执行结果和合成可能的错误解释，动态推断PDDL问题文件和领域行动语义。该系统迭代生成和执行规划，同时对每个行动的可能行动语义保持树状信念，并迭代细化这些信念直到达到目标状态。在ALFRED中进行的任务完成模拟实验表明，PSALM-V在部分观测设置下将规划成功率从37%（Claude-3.7）提高到74%。在RTFM和Overcooked-AI两个2D游戏环境中的结果显示，PSALM-V提高了步骤效率，并在多智能体设置中成功进行领域归纳。PSALM-V正确推断了真实世界机器人BlocksWorld任务的PDDL前置和后置条件，尽管机器人存在低级操作失败。", "summary": "PSALM-V是一种新颖的自主神经符号学习系统，它通过交互在视觉环境中自动推断符号行动语义（前置条件和后置条件）。该系统利用大型语言模型生成启发式规划和候选语义，并能动态推断PDDL问题文件和领域行动语义，从而克服了以往方法对预定义信息或理想条件的依赖。PSALM-V通过迭代生成和执行规划，并维护行动语义的树状信念结构来持续优化。实验结果表明，在ALFRED、RTFM和Overcooked-AI等环境中，PSALM-V显著提高了规划成功率和步骤效率，并能在多智能体和真实机器人任务中有效进行领域归纳，即使面临部分观测和低级操作失败等挑战。", "keywords": "符号规划, 视觉环境, 大型语言模型, 行动语义, 神经符号系统", "comments": "PSALM-V的创新之处在于其神经符号学习方法，它结合了LLM的生成能力和符号规划的推理能力，实现了在缺乏先验知识的视觉环境中自主学习行动语义。其克服了以往方法对理想条件（如完全可观测性或预定义问题文件）的依赖，并通过动态分析执行结果和错误解释来迭代优化，这对于实现更通用和鲁棒的AI系统具有重要意义。在部分观测环境和真实机器人任务中的成功应用，突显了其在复杂、不确定环境中的实用性和潜力。"}}
{"id": "2506.20444", "title": "Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds", "authors": ["Xiang Lan", "Tim Menzies", "Bowen Xu"], "summary": "Vulnerability detection is crucial for identifying security weaknesses in\nsoftware systems. However, the effectiveness of machine learning models in this\ndomain is often hindered by low-quality training datasets, which contain noisy,\nmislabeled, or imbalanced samples. This paper proposes a novel dataset\nmaps-empowered approach that systematically identifies and mitigates\nhard-to-learn outliers, referred to as \"bad seeds\", to improve model training\nefficiency. Our approach can categorize training examples based on learning\ndifficulty and integrate this information into an active learning framework.\nUnlike traditional methods that focus on uncertainty-based sampling, our\nstrategy prioritizes dataset quality by filtering out performance-harmful\nsamples while emphasizing informative ones. Our experimental results show that\nour approach can improve F1 score over random selection by 45.36% (DeepGini)\nand 45.91% (K-Means) and outperforms standard active learning by 61.46%\n(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,\ndemonstrating the effectiveness of integrating dataset maps for optimizing\nsample selection in vulnerability detection. Furthermore, our approach also\nenhances model robustness, improves sample selection by filtering bad seeds,\nand stabilizes active learning performance across iterations. By analyzing the\ncharacteristics of these outliers, we provide insights for future improvements\nin dataset construction, making vulnerability detection more reliable and\ncost-effective.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20444v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20444v1", "AI": {"title_translation": "智能剪枝：通过修剪“坏种子”增强漏洞检测的主动学习", "tldr": "本研究提出一种基于数据集图谱的方法，通过识别并过滤训练数据中的低质量样本（“坏种子”），显著提升了漏洞检测中主动学习的效率和模型性能。", "motivation": "漏洞检测中机器学习模型的有效性常受低质量训练数据集的阻碍，这些数据集包含噪声、错误标记或不平衡的样本。", "method": "本文提出一种新颖的基于数据集图谱（dataset maps-empowered）的方法，系统地识别并减轻难以学习的异常值，即“坏种子”。该方法能根据学习难度对训练样本进行分类，并将其整合到主动学习框架中。与传统基于不确定性的采样不同，它通过过滤有害样本并强调信息量大的样本来优先考虑数据集质量。", "result": "实验结果表明，该方法在Big-Vul数据集上，CodeBERT模型相较于随机选择，F1分数提高了45.36% (DeepGini) 和 45.91% (K-Means)；相较于标准主动学习，F1分数提高了61.46% (DeepGini) 和 32.65% (K-Means)。此外，该方法还增强了模型鲁棒性，通过过滤“坏种子”改进了样本选择，并稳定了主动学习在迭代中的性能。", "conclusion": "通过分析这些异常值的特征，本方法为未来数据集构建的改进提供了见解，使漏洞检测更加可靠和经济高效，并证明了将数据集图谱整合到主动学习中以优化漏洞检测样本选择的有效性。", "translation": "漏洞检测对于识别软件系统中的安全弱点至关重要。然而，机器学习模型在该领域的有效性常常受到低质量训练数据集的阻碍，这些数据集包含噪声、错误标记或不平衡的样本。本文提出一种新颖的、由数据集图谱支持的方法，系统地识别并减轻难以学习的异常值（称为“坏种子”），以提高模型训练效率。我们的方法可以根据学习难度对训练样本进行分类，并将此信息整合到主动学习框架中。与传统侧重于基于不确定性采样的方​​法不同，我们的策略通过过滤掉损害性能的样本，同时强调信息丰富的样本来优先考虑数据集质量。我们的实验结果表明，在Big-Vul数据集上，我们的方法相较于随机选择，对CodeBERT的F1分数提高了45.36% (DeepGini) 和 45.91% (K-Means)，并且优于标准主动学习61.46% (DeepGini) 和 32.65% (K-Means)，这证明了将数据集图谱整合到漏洞检测中以优化样本选择的有效性。此外，我们的方法还增强了模型鲁棒性，通过过滤“坏种子”改进了样本选择，并稳定了主动学习在迭代中的性能。通过分析这些异常值的特征，我们为未来数据集构建的改进提供了见解，使漏洞检测更加可靠和经济高效。", "summary": "本文提出一种名为“Smart Cuts”的新型主动学习方法，用于提升漏洞检测的效率和准确性。该方法利用数据集图谱识别并剔除训练数据中的低质量样本（“坏种子”），同时优先选择信息量大的样本。实验结果显示，与随机选择和传统主动学习相比，该方法显著提高了漏洞检测模型的F1分数和鲁棒性，并为未来数据集的构建提供了有价值的见解。", "keywords": "漏洞检测, 主动学习, 数据集图谱, 坏种子, 数据质量", "comments": "该论文的创新点在于将“数据集图谱”的概念引入主动学习框架，专门用于识别和剪除“坏种子”，从而有效提升了漏洞检测任务中模型训练的质量和效率。这与传统主动学习侧重于不确定性采样的思路不同，更强调了数据质量的重要性。其提出的方法不仅提高了性能，还增强了模型的鲁棒性，为解决现实世界中低质量训练数据的问题提供了实用的解决方案。"}}
{"id": "2506.20151", "title": "EAR: Erasing Concepts from Unified Autoregressive Models", "authors": ["Haipeng Fan", "Shiyuan Zhang", "Baohunesitu", "Zihang Guo", "Huaiwen Zhang"], "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/", "comment": "11 pages, 7 figures, 1 tables", "pdf_url": "http://arxiv.org/pdf/2506.20151v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20151v1", "AI": {"title_translation": "EAR：从统一自回归模型中擦除概念", "tldr": "本文提出了EAR，一种用于自回归模型概念擦除的微调方法，通过WGA和TLM策略有效去除不期望的概念，同时保持生成质量，并在新的ECGVF基准上验证了其优越性。", "motivation": "自回归模型在视觉理解和图像生成任务中表现出色，但如何在去除模型中不期望概念的同时保持整体生成质量仍是一个开放的挑战。", "method": "本文提出了擦除自回归模型（EAR），一种用于自回归模型中有效且保留效用的概念擦除微调方法。具体来说，引入了窗口梯度累积（WGA）策略以使补丁级解码与擦除目标对齐，以及阈值损失掩蔽（TLM）策略以在微调期间保护与目标概念无关的内容。此外，提出了一个新颖的基准ECGVF，旨在为评估自回归模型中的概念擦除提供更严格和全面的基础。ECGVF首先利用结构化模板通过LLMs预生成大规模的目标替换概念提示对语料库，然后从这些提示生成图像并通过视觉分类器进行严格过滤以确保概念保真度和对齐。", "result": "在ECGVF基准上，使用AR模型Janus-Pro进行的广泛实验结果表明，EAR在擦除有效性和模型效用保留方面均取得了显著改进。", "conclusion": "EAR方法能够有效且实用地从自回归模型中擦除不期望的概念，同时保持模型的整体生成质量，并在新提出的ECGVF基准上展示了其优越性。", "translation": "自回归（AR）模型在视觉理解和图像生成任务中取得了统一而强大的性能。然而，从AR模型中移除不期望的概念同时保持整体生成质量仍然是一个开放的挑战。在本文中，我们提出了擦除自回归模型（EAR），一种用于AR模型中有效且保留效用的概念擦除的微调方法。具体来说，我们引入了窗口梯度累积（WGA）策略，以使补丁级解码与擦除目标对齐，以及阈值损失掩蔽（TLM）策略，以在微调期间保护与目标概念无关的内容。此外，我们提出了一个新颖的基准——擦除概念生成器和视觉过滤器（ECGVF），旨在为评估AR模型中的概念擦除提供更严格和全面的基础。具体而言，我们首先利用跨各种大型语言模型（LLMs）的结构化模板预生成大规模的目标替换概念提示对语料库。随后，我们从这些提示生成图像，并通过视觉分类器对其进行严格过滤，以确保概念保真度和对齐。在ECGVF基准上，使用AR模型Janus-Pro进行的广泛实验结果表明，EAR在擦除有效性和模型效用保留方面均取得了显著改进。代码可在：https://github.com/immc-lab/ear/ 获取。", "summary": "本文提出EAR，一种针对自回归模型进行概念擦除的微调方法，旨在解决在去除不期望概念的同时保持模型生成质量的挑战。EAR引入了窗口梯度累积（WGA）和阈值损失掩蔽（TLM）策略，分别用于对齐擦除目标和保护无关内容。同时，文章构建了一个新的评估基准ECGVF，用于更全面地评估概念擦除效果。实验结果表明，EAR在擦除有效性和模型实用性方面均表现出色。", "keywords": "概念擦除, 自回归模型, 微调, WGA, TLM, ECGVF", "comments": "本文的创新点在于提出了EAR这一概念擦除的微调方法，并通过WGA和TLM两种策略来优化擦除过程，既保证了擦除的有效性，又兼顾了模型生成质量的保持。更重要的是，文章还提出了一个新颖且严格的评估基准ECGVF，为未来概念擦除研究提供了更可靠的评估工具，具有重要的研究价值。"}}
{"id": "2506.19893", "title": "Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks", "authors": ["Jingzhi Hu", "Geoffrey Ye Li"], "summary": "Due to the surging amount of AI-generated content (AIGC), its provisioning to\nedges and mobile users from the cloud incurs substantial traffic on networks.\nGenerative semantic communication (GSC) offers a promising solution by\ntransmitting highly compact information, i.e., prompt text and latent\nrepresentations, instead of high-dimensional AIGC data. However, GSC relies on\nthe alignment between the knowledge in the cloud generative AI (GAI) and that\npossessed by the edges and users, and between the knowledge for wireless\ntransmission and that of actual channels, which remains challenging. In this\npaper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm\nfor GSC systems. The core idea is to distill the generation knowledge from the\ncloud-GAI into low-rank matrices, which can be incorporated by the edge and\nused to adapt the transmission knowledge to diverse wireless channel\nconditions. DeKA-g comprises two novel methods: metaword-aided knowledge\ndistillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,\nan optimized metaword is employed to enhance the efficiency of knowledge\ndistillation, while VGSA enables efficient adaptation to diverse compression\nrates and SNR ranges. From simulation results, DeKA-g improves the alignment\nbetween the edge-generated images and the cloud-generated ones by 44%.\nMoreover, it adapts to compression rates with 116% higher efficiency than the\nbaseline and enhances the performance in low-SNR conditions by 28%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19893v1", "categories": ["cs.LG", "cs.AI", "cs.IT", "eess.IV", "math.IT"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19893v1", "AI": {"title_translation": "生成式语义通信中AIGC供应任务的蒸馏赋能知识对齐", "tldr": "本文提出了DeKA-g，一种用于生成式语义通信（GSC）的蒸馏赋能知识对齐算法，通过将云端GAI知识蒸馏到低秩矩阵中，显著提高了边缘生成内容与云端内容的对齐度，并提升了在不同压缩率和低信噪比条件下的性能。", "motivation": "随着AI生成内容（AIGC）的激增，将其从云端分发到边缘和移动用户会导致网络流量巨大。生成式语义通信（GSC）通过传输紧凑信息（如提示文本和潜在表示）而非高维AIGC数据，提供了一种有前景的解决方案。然而，GSC面临挑战，即云端生成式AI（GAI）知识与边缘和用户所拥有知识之间的对齐，以及无线传输知识与实际信道知识之间的对齐。", "method": "本文提出了DeKA-g，一种用于GSC系统的蒸馏赋能知识对齐算法。其核心思想是将云端GAI的生成知识蒸馏到低秩矩阵中，边缘和用户可以利用这些矩阵来适应不同无线信道条件下的传输知识。DeKA-g包含两种新颖方法：元词辅助知识蒸馏（MAKD）和变速率分组信噪比自适应（VGSA）。MAKD采用优化的元词来提高知识蒸馏效率，而VGSA则能有效地适应不同的压缩率和信噪比范围。", "result": "仿真结果表明，DeKA-g将边缘生成图像与云端生成图像之间的对齐度提高了44%。此外，它在适应压缩率方面比基线效率高116%，并将低信噪比条件下的性能提升了28%。", "conclusion": "DeKA-g通过知识蒸馏有效解决了生成式语义通信中云端与边缘知识对齐的挑战，显著提升了AIGC在无线网络中的传输效率和质量，特别是在资源受限和信道条件不佳的环境下展现出优越性。", "translation": "由于AI生成内容（AIGC）的激增，将其从云端供应到边缘和移动用户会在网络上产生巨大的流量。生成式语义通信（GSC）通过传输高度紧凑的信息，即提示文本和潜在表示，而不是高维AIGC数据，提供了一种有前景的解决方案。然而，GSC依赖于云端生成式AI（GAI）中的知识与边缘和用户所拥有知识之间的对齐，以及无线传输知识与实际信道知识之间的对齐，这仍然具有挑战性。在本文中，我们提出了DeKA-g，一种用于GSC系统的蒸馏赋能知识对齐算法。其核心思想是将云端GAI的生成知识蒸馏到低秩矩阵中，边缘可以利用这些矩阵来适应不同的无线信道条件下的传输知识。DeKA-g包含两种新颖方法：元词辅助知识蒸馏（MAKD）和变速率分组信噪比自适应（VGSA）。对于MAKD，采用优化的元词来提高知识蒸馏的效率，而VGSA则能有效地适应不同的压缩率和信噪比范围。从仿真结果来看，DeKA-g将边缘生成图像与云端生成图像之间的对齐度提高了44%。此外，它在适应压缩率方面比基线效率高116%，并将低信噪比条件下的性能提升了28%。", "summary": "本文针对AIGC内容供应中巨大的网络流量问题，提出了一种名为DeKA-g的生成式语义通信（GSC）知识对齐算法。DeKA-g通过将云端生成式AI（GAI）的生成知识蒸馏到低秩矩阵中，使边缘设备能够利用这些知识并适应多样化的无线信道条件。该算法包含元词辅助知识蒸馏（MAKD）和变速率分组信噪比自适应（VGSA）两种创新方法，分别优化了知识蒸馏效率和对不同压缩率及信噪比范围的适应性。仿真结果表明，DeKA-g显著提升了边缘与云端生成内容的一致性，并在压缩率适应性和低信噪比性能方面超越了基线。", "keywords": "生成式语义通信, 知识对齐, 蒸馏, AIGC, 无线通信", "comments": "DeKA-g的创新点在于将知识蒸馏技术应用于生成式语义通信，以解决AIGC内容分发中的知识对齐和传输效率问题。通过将复杂的云端GAI知识提炼为轻量级的低秩矩阵，该方法有效降低了边缘设备的计算负担和网络传输开销，同时保持了内容质量。MAKD和VGSA模块的设计进一步提升了其在复杂无线环境下的鲁棒性和适应性，这对于未来大规模AIGC应用具有重要意义。"}}
{"id": "2506.20020", "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning", "authors": ["Saloni Dash", "Amélie Reymond", "Emma S. Spiro", "Aylin Caliskan"], "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20020v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20020v1", "AI": {"title_translation": "人格化大型语言模型表现出类人动机性推理", "tldr": "分配了人格的大型语言模型表现出类似人类的动机性推理，导致它们偏向于与身份一致的结论，并且这种偏见难以消除。", "motivation": "人类推理容易受到动机（如身份保护）导致的偏见影响，这会损害理性决策和判断，并在集体层面加剧社会问题，如气候变化或疫苗安全辩论。先前的研究表明大型语言模型（LLMs）也容易受到类似人类的认知偏见影响，但LLMs在多大程度上选择性地朝向与身份一致的结论进行推理，仍未被充分探索。", "method": "研究通过为8个LLMs（包括开源和专有模型）分配8种人格（涵盖4种政治和社会人口属性）来调查是否会引发动机性推理。通过人类受试者研究中的两个推理任务——错误信息标题的真实性辨别和数字科学证据的评估——对这些模型进行了测试。", "result": "分配了人格的LLMs在真实性辨别方面表现出高达9%的下降。特别是政治人格，当科学证据（例如关于枪支管制）的真实情况与其所设定的政治身份一致时，它们正确评估该证据的可能性最高可提高90%。基于提示的去偏方法在缓解这些影响方面基本无效。", "conclusion": "本研究的实证结果首次表明，分配了人格的LLMs表现出类似人类的动机性推理，且难以通过常规的去偏提示来缓解。这引发了对LLMs和人类中身份一致性推理可能加剧的担忧。", "translation": "人类的推理容易受到潜在动机（如身份保护）导致的偏见影响，这会损害理性的决策和判断。这种在集体层面的动机性推理在辩论气候变化或疫苗安全等关键问题时可能对社会有害，并可能进一步加剧政治两极分化。先前的研究报告称，大型语言模型（LLMs）也容易受到类似人类的认知偏见影响，然而，LLMs在多大程度上选择性地朝向与身份一致的结论进行推理，在很大程度上仍未被探索。在这里，我们调查了为8个LLMs分配8种人格（涵盖4种政治和社会人口属性）是否会引发动机性推理。我们对8个LLMs（包括开源和专有模型）进行了测试，使用了人类受试者研究中的两个推理任务——错误信息标题的真实性辨别和数字科学证据的评估——我们发现，与没有人格的模型相比，分配了人格的LLMs在真实性辨别方面下降了高达9%。特别是政治人格，当真实情况与其所设定的政治身份一致时，它们正确评估枪支管制科学证据的可能性最高可提高90%。基于提示的去偏方法在缓解这些影响方面基本无效。总而言之，我们的实证结果首次表明，分配了人格的LLMs表现出类似人类的动机性推理，且难以通过常规的去偏提示来缓解——这引发了对LLMs和人类中身份一致性推理可能加剧的担忧。", "summary": "本文探讨了为大型语言模型（LLMs）分配人格是否会诱发类似人类的动机性推理。通过对8个LLMs分配8种不同人格，并在真实性辨别和科学证据评估两项任务上进行测试，研究发现，分配了人格的LLMs在真实性辨别上表现出下降，并且更倾向于使其推理与设定的身份保持一致，尤其是在政治身份方面。更重要的是，传统的基于提示的去偏方法在减轻这些偏见方面效果甚微。这表明LLMs能够表现出难以消除的动机性推理，对信息传播和社会讨论具有重要影响。", "keywords": "动机性推理, 大型语言模型, 认知偏见, 人格, 去偏", "comments": "这项研究具有创新性，因为它首次证明了人格化LLMs中存在类似人类的动机性推理。其重要性在于揭示了当前LLMs的一个显著局限性以及常见去偏技术的无效性，这对于LLMs在信息传播和公共讨论等敏感领域的部署具有关键影响，并可能加剧社会两极分化。"}}
{"id": "2506.19886", "title": "Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack", "authors": ["Xuesong Wang", "Mo Li", "Xingyan Shi", "Zhaoqian Liu", "Shenghao Yang"], "summary": "Semantic communication has emerged as a promising neural network-based system\ndesign for 6G networks. Task-oriented semantic communication is a novel\nparadigm whose core goal is to efficiently complete specific tasks by\ntransmitting semantic information, optimizing communication efficiency and task\nperformance. The key challenge lies in preserving privacy while maintaining\ntask accuracy, as this scenario is susceptible to model inversion attacks. In\nsuch attacks, adversaries can restore or even reconstruct input data by\nanalyzing and processing model outputs, owing to the neural network-based\nnature of the systems. In addition, traditional systems use image quality\nindicators (such as PSNR or SSIM) to assess attack severity, which may be\ninadequate for task-oriented semantic communication, since visual differences\ndo not necessarily ensure semantic divergence. In this paper, we propose a\ndiffusion-based semantic communication framework, named DiffSem, that optimizes\nsemantic information reconstruction through a diffusion mechanism with\nself-referential label embedding to significantly improve task performance. Our\nmodel also compensates channel noise and adopt semantic information distortion\nto ensure the robustness of the system in various signal-to-noise ratio\nenvironments. To evaluate the attacker's effectiveness, we propose a new metric\nthat better quantifies the semantic fidelity of estimations from the adversary.\nExperimental results based on this criterion show that on the MNIST dataset,\nDiffSem improves the classification accuracy by 10.03%, and maintain stable\nperformance under dynamic channels. Our results further demonstrate that\nsignificant deviation exists between traditional image quality indicators and\nthe leakage of task-relevant semantic information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19886v1", "categories": ["cs.CR", "cs.IT", "cs.LG", "math.IT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19886v1", "AI": {"title_translation": "基于扩散的任务导向语义通信与模型反演攻击", "tldr": "本文提出了一种名为DiffSem的基于扩散的语义通信框架，旨在提高任务性能并抵御模型反演攻击，同时引入了新的度量标准来评估攻击效果，并在MNIST数据集上显示出显著的分类准确率提升。", "motivation": "任务导向语义通信在确保通信效率和任务性能的同时，面临着隐私保护的挑战，尤其是在模型反演攻击下，攻击者可能通过模型输出来重建输入数据。此外，传统的图像质量指标（如PSNR或SSIM）不足以评估任务导向语义通信中的攻击严重性，因为视觉差异不一定反映语义差异。", "method": "本文提出了一种名为DiffSem的基于扩散的语义通信框架。该框架通过扩散机制和自引用标签嵌入来优化语义信息重建，以显著提高任务性能。它还补偿信道噪声并采用语义信息失真，以确保系统在各种信噪比环境下的鲁棒性。此外，本文提出了一种新的度量标准，用于更好地量化攻击者估计的语义保真度。", "result": "在MNIST数据集上，DiffSem将分类准确率提高了10.03%，并在动态信道下保持了稳定的性能。实验结果还表明，传统的图像质量指标与任务相关语义信息的泄露之间存在显著偏差。", "conclusion": "本文提出的DiffSem框架有效提高了任务导向语义通信的性能和鲁棒性，并能有效抵御模型反演攻击。同时，新提出的语义保真度评估指标更准确地反映了任务相关信息的泄露程度，证明了传统指标的不足。", "translation": "语义通信已成为6G网络中一种有前景的基于神经网络的系统设计。任务导向语义通信是一种新颖的范式，其核心目标是通过传输语义信息来高效完成特定任务，优化通信效率和任务性能。关键挑战在于在保持任务准确性的同时保护隐私，因为这种场景容易受到模型反演攻击。在此类攻击中，由于系统基于神经网络的特性，攻击者可以通过分析和处理模型输出来恢复甚至重建输入数据。此外，传统系统使用图像质量指标（如PSNR或SSIM）来评估攻击严重性，这对于任务导向语义通信可能不足，因为视觉差异不一定能确保语义差异。在本文中，我们提出了一种基于扩散的语义通信框架，名为DiffSem，它通过扩散机制和自引用标签嵌入来优化语义信息重建，从而显著提高任务性能。我们的模型还补偿信道噪声并采用语义信息失真，以确保系统在各种信噪比环境下的鲁棒性。为了评估攻击者的有效性，我们提出了一种新的度量标准，它能更好地量化攻击者估计的语义保真度。基于此标准的实验结果表明，在MNIST数据集上，DiffSem将分类准确率提高了10.03%，并在动态信道下保持了稳定的性能。我们的结果进一步证明，传统的图像质量指标与任务相关语义信息的泄露之间存在显著偏差。", "summary": "本文针对6G网络中任务导向语义通信面临的模型反演攻击和传统评估指标不足的问题，提出了一种名为DiffSem的基于扩散的语义通信框架。DiffSem利用扩散机制和自引用标签嵌入来优化语义信息重建，提高任务性能，并通过处理信道噪声和语义信息失真来增强系统鲁棒性。为准确评估攻击效果，作者还提出了一种新的语义保真度度量。实验结果显示，DiffSem在MNIST数据集上将分类准确率提升了10.03%，并在动态信道下表现稳定，同时揭示了传统图像质量指标在评估语义信息泄露方面的局限性。", "keywords": "语义通信, 扩散模型, 模型反演攻击, 任务导向, 隐私保护", "comments": "该论文的创新点在于将扩散模型引入任务导向语义通信，以提高性能和鲁棒性，并有效应对模型反演攻击。更重要的是，作者提出了一个针对任务导向语义通信的全新评估指标，弥补了传统图像质量指标的不足，这对于未来该领域的研究具有重要指导意义。其提出的方法在保护隐私和提高通信效率方面具有潜在价值。"}}
{"id": "2506.20207", "title": "User Understanding of Privacy Permissions in Mobile Augmented Reality: Perceptions and Misconceptions", "authors": ["Viktorija Paneva", "Verena Winterhalter", "Franziska Augustinowski", "Florian Alt"], "summary": "Mobile Augmented Reality (AR) applications leverage various sensors to\nprovide immersive user experiences. However, their reliance on diverse data\nsources introduces significant privacy challenges. This paper investigates user\nperceptions and understanding of privacy permissions in mobile AR apps through\nan analysis of existing applications and an online survey of 120 participants.\nFindings reveal common misconceptions, including confusion about how\npermissions relate to specific AR functionalities (e.g., location and\nmeasurement of physical distances), and misinterpretations of permission labels\n(e.g., conflating camera and gallery access). We identify a set of actionable\nimplications for designing more usable and transparent privacy mechanisms\ntailored to mobile AR technologies, including contextual explanations, modular\npermission requests, and clearer permission labels. These findings offer\nactionable guidance for developers, researchers, and policymakers working to\nenhance privacy frameworks in mobile AR.", "comment": "14 pages, 3 figures. Preprint. Accepted to MobileHCI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20207v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20207v1", "AI": {"title_translation": "移动增强现实中用户对隐私权限的理解：感知与误解", "tldr": "研究发现移动AR应用中用户对隐私权限存在普遍误解，并提出了改进设计建议。", "motivation": "移动增强现实（AR）应用利用多种传感器提供沉浸式用户体验，但其对多样化数据源的依赖带来了显著的隐私挑战。本文旨在调查用户对移动AR应用中隐私权限的感知和理解。", "method": "通过分析现有应用和对120名参与者进行在线调查。", "result": "研究发现普遍存在的误解，包括对权限如何与特定AR功能（如位置和物理距离测量）关联的混淆，以及对权限标签的误解（如混淆摄像头和相册访问）。", "conclusion": "这些发现为开发人员、研究人员和政策制定者在增强移动AR隐私框架方面提供了可操作的指导，包括情境解释、模块化权限请求和更清晰的权限标签。", "translation": "移动增强现实（AR）应用利用各种传感器提供沉浸式用户体验。然而，它们对多样化数据源的依赖带来了显著的隐私挑战。本文通过分析现有应用和对120名参与者进行在线调查，调查了用户对移动AR应用中隐私权限的感知和理解。研究结果揭示了普遍存在的误解，包括对权限如何与特定AR功能（例如，位置和物理距离测量）关联的混淆，以及对权限标签的误解（例如，混淆摄像头和相册访问）。我们确定了一系列可操作的建议，用于设计更可用和透明的、针对移动AR技术量身定制的隐私机制，包括情境解释、模块化权限请求和更清晰的权限标签。这些发现为开发人员、研究人员和政策制定者在增强移动AR隐私框架方面提供了可操作的指导。", "summary": "本研究调查了移动增强现实应用中用户对隐私权限的理解，通过分析现有应用和对120名参与者的在线调查，揭示了用户在权限与AR功能关联以及权限标签解读上的普遍误解。研究提出了针对移动AR的可用且透明的隐私机制设计建议，如情境解释、模块化请求和清晰标签，为改进移动AR隐私框架提供了指导。", "keywords": "移动增强现实, 隐私权限, 用户理解, 误解, 隐私机制", "comments": "这篇论文解决了移动AR领域一个重要的隐私可用性问题。其创新点在于识别了用户对AR特定隐私权限的误解类型，并提出了具体可行的设计改进方案。对于开发者和政策制定者来说，这些发现具有直接的实践意义，有助于提升AR应用的透明度和用户信任。未来研究可以进一步探索不同用户群体或文化背景下这些误解的差异。"}}
{"id": "2506.19943", "title": "Quantum-Resistant Domain Name System: A Comprehensive System-Level Study", "authors": ["Juyoul Lee", "Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan"], "summary": "The Domain Name System (DNS) plays a foundational role in Internet\ninfrastructure, yet its core protocols remain vulnerable to compromise by\nquantum adversaries. As cryptographically relevant quantum computers become a\nrealistic threat, ensuring DNS confidentiality, authenticity, and integrity in\nthe post-quantum era is imperative. In this paper, we present a comprehensive\nsystem-level study of post-quantum DNS security across three widely deployed\nmechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose\nPost-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS\nsecurity under legacy, post-quantum, and hybrid cryptographic configurations.\nOur implementation leverages the Open Quantum Safe (OQS) libraries and\nintegrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We\nformalize performance and threat models and analyze the impact of post-quantum\nkey encapsulation and digital signatures on end-to-end DNS resolution.\nExperimental results on a containerized testbed reveal that lattice-based\nprimitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and\nFalcon offer practical latency and resource profiles, while hash-based schemes\nlike SPHINCS+ significantly increase message sizes and processing overhead. We\nalso examine security implications including downgrade risks, fragmentation\nvulnerabilities, and susceptibility to denial-of-service amplification. Our\nfindings inform practical guidance for deploying quantum-resilient DNS and\ncontribute to the broader effort of securing core Internet protocols for the\npost-quantum future.", "comment": "Manuscript submitted to ACM, 29 pages, 8 Figures, 15 Tables", "pdf_url": "http://arxiv.org/pdf/2506.19943v1", "categories": ["cs.CR", "cs.NI", "cs.PF"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19943v1", "AI": {"title_translation": "抗量子域名系统：一项全面的系统级研究", "tldr": "本研究对DNS在后量子时代的安全性进行了全面的系统级研究，提出了PQC-DNS框架，并发现格基密码（如MLKEM和Falcon）在性能上优于哈希基密码（如SPHINCS+），为部署抗量子DNS提供了实用指导。", "motivation": "域名系统（DNS）作为互联网基础设施的核心，其核心协议易受量子攻击。随着量子计算机的威胁日益逼真，确保后量子时代DNS的机密性、真实性和完整性变得至关重要。", "method": "本文对DNSSEC、DNS-over-TLS (DoT) 和 DNS-over-HTTPS (DoH) 三种广泛部署的机制进行了后量子DNS安全性的全面系统级研究。提出了PQC-DNS（后量子密码学DNS）统一框架，用于在传统、后量子和混合密码配置下评估DNS安全性。实现利用Open Quantum Safe (OQS) 库，并将格基和哈希基原语集成到BIND9和TLS 1.3堆栈中。形式化了性能和威胁模型，并分析了后量子密钥封装和数字签名对端到端DNS解析的影响。在容器化测试台上进行了实验。", "result": "实验结果表明，Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) 和 Falcon 等格基原语提供了实用的延迟和资源配置，而SPHINCS+等哈希基方案显著增加了消息大小和处理开销。研究还检查了降级风险、分片漏洞和拒绝服务放大攻击的安全性影响。", "conclusion": "研究结果为部署抗量子DNS提供了实用指导，并有助于更广泛地努力保护后量子未来互联网的核心协议。", "translation": "域名系统（DNS）在互联网基础设施中扮演着基础性角色，但其核心协议仍然容易受到量子对手的攻击。随着与密码学相关的量子计算机成为一个现实的威胁，在后量子时代确保DNS的机密性、真实性和完整性变得至关重要。在本文中，我们对三种广泛部署的机制：DNSSEC、DNS-over-TLS (DoT) 和 DNS-over-HTTPS (DoH) 的后量子DNS安全性进行了全面的系统级研究。我们提出了后量子密码学（PQC）-DNS，一个用于在传统、后量子和混合密码配置下对DNS安全性进行基准测试的统一框架。我们的实现利用了Open Quantum Safe (OQS) 库，并将格基和哈希基原语集成到BIND9和TLS 1.3堆栈中。我们形式化了性能和威胁模型，并分析了后量子密钥封装和数字签名对端到端DNS解析的影响。在容器化测试台上的实验结果表明，Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) 和 Falcon 等格基原语提供了实用的延迟和资源配置，而SPHINCS+等哈希基方案显著增加了消息大小和处理开销。我们还检查了安全影响，包括降级风险、分片漏洞和对拒绝服务放大攻击的脆弱性。我们的研究结果为部署抗量子DNS提供了实用指导，并有助于更广泛地努力保护后量子未来互联网的核心协议。", "summary": "本文针对量子计算威胁下域名系统（DNS）的安全性问题，进行了一项全面的系统级研究。研究提出了PQC-DNS统一框架，用于评估DNSSEC、DoT和DoH在后量子环境下的表现。通过在BIND9和TLS 1.3中集成并测试格基和哈希基密码原语，实验结果表明格基方案如MLKEM和Falcon在性能上更优，而哈希基方案如SPHINCS+开销较大。研究还探讨了潜在的安全风险，并为部署抗量子DNS提供了实践性指导。", "keywords": "抗量子DNS, 后量子密码学, DNSSEC, TLS 1.3, 格基密码", "comments": "本文通过对现有DNS安全机制进行深入的系统级分析，并提出PQC-DNS框架，为应对量子计算对互联网核心基础设施带来的威胁提供了重要的实践性指导。其创新点在于结合了多种PQC算法，并在实际DNS组件中进行集成和性能评估，揭示了不同PQC原语对DNS性能的影响，对于未来抗量子DNS的部署具有重要的参考价值。"}}
{"id": "2506.19892", "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL", "authors": ["Isaac Marroqui Penalva", "Enrique Tomás Martínez Beltrán", "Manuel Gil Pérez", "Alberto Huertas Celdrán"], "summary": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train\nmodels without a central server, introducing new vulnerabilities since each\nnode independently selects peers for model aggregation. Malicious nodes may\nexploit this autonomy by sending corrupted models (model poisoning), delaying\nmodel submissions (delay attack), or flooding the network with excessive\nmessages, negatively affecting system performance. Existing solutions often\ndepend on rigid configurations or additional infrastructures such as\nblockchain, leading to computational overhead, scalability issues, or limited\nadaptability. To overcome these limitations, this paper proposes RepuNet, a\ndecentralized reputation system that categorizes threats in DFL and dynamically\nevaluates node behavior using metrics like model similarity, parameter changes,\nmessage latency, and communication volume. Nodes' influence in model\naggregation is adjusted based on their reputation scores. RepuNet was\nintegrated into the Nebula DFL platform and experimentally evaluated with MNIST\nand CIFAR-10 datasets under non-IID distributions, using federations of up to\n25 nodes in both fully connected and random topologies. Different attack\nintensities, frequencies, and activation intervals were tested. Results\ndemonstrated that RepuNet effectively detects and mitigates malicious behavior,\nachieving F1 scores above 95% for MNIST scenarios and approximately 76% for\nCIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,\nand practical potential for mitigating threats in decentralized federated\nlearning environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19892v1", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19892v1", "AI": {"title_translation": "RepuNet：一个用于减轻去中心化联邦学习中恶意客户端影响的声誉系统", "tldr": "RepuNet是一个去中心化声誉系统，能有效检测并缓解去中心化联邦学习（DFL）中的恶意行为。", "motivation": "去中心化联邦学习（DFL）中，节点独立选择对等方进行模型聚合，引入了新的漏洞，恶意节点可能通过模型投毒、延迟攻击或网络泛洪等方式损害系统性能。现有解决方案通常依赖僵化的配置或额外基础设施（如区块链），导致计算开销、可扩展性问题或适应性有限，因此需要一种更灵活鲁棒的解决方案。", "method": "本文提出了RepuNet，一个去中心化声誉系统，旨在克服现有解决方案的局限性。RepuNet通过对DFL中的威胁进行分类，并利用模型相似性、参数变化、消息延迟和通信量等指标动态评估节点行为。节点的模型聚合影响力会根据其声誉得分进行调整。RepuNet被集成到Nebula DFL平台，并在MNIST和CIFAR-10数据集（非独立同分布）上，使用多达25个节点的联邦（完全连接和随机拓扑）进行了实验评估，测试了不同的攻击强度、频率和激活间隔。", "result": "实验结果表明，RepuNet能有效检测并缓解恶意行为。在MNIST场景中，F1分数超过95%；在CIFAR-10案例中，F1分数约为76%。", "conclusion": "这些结果突出了RepuNet在去中心化联邦学习环境中缓解恶意威胁的适应性、鲁棒性和实践潜力。", "translation": "去中心化联邦学习（DFL）使节点无需中心服务器即可协同训练模型，但由于每个节点独立选择对等方进行模型聚合，因此引入了新的漏洞。恶意节点可能利用这种自主性发送损坏的模型（模型投毒）、延迟模型提交（延迟攻击）或用过多消息淹没网络，从而对系统性能产生负面影响。现有解决方案通常依赖于僵化的配置或额外的基础设施（如区块链），导致计算开销、可扩展性问题或适应性有限。为了克服这些局限性，本文提出了RepuNet，一个去中心化声誉系统，它对DFL中的威胁进行分类，并使用模型相似性、参数变化、消息延迟和通信量等指标动态评估节点行为。节点的模型聚合影响力根据其声誉得分进行调整。RepuNet被集成到Nebula DFL平台，并在非独立同分布（non-IID）的MNIST和CIFAR-10数据集上进行了实验评估，使用了在完全连接和随机拓扑结构下多达25个节点的联邦。测试了不同的攻击强度、频率和激活间隔。结果表明，RepuNet有效检测并缓解了恶意行为，在MNIST场景中F1分数超过95%，在CIFAR-10案例中约为76%。这些结果突出了RepuNet在去中心化联邦学习环境中缓解威胁的适应性、鲁棒性和实践潜力。", "summary": "本论文提出了RepuNet，一个针对去中心化联邦学习（DFL）的去中心化声誉系统，旨在解决恶意节点（如模型投毒、延迟攻击）带来的漏洞及现有方案的局限。RepuNet通过评估节点行为指标（如模型相似性、消息延迟）来动态调整其声誉，并据此调整其在模型聚合中的影响力。实验结果表明，RepuNet能有效检测并缓解恶意行为，在MNIST和CIFAR-10数据集上表现出高F1分数，证明了其在DFL环境中缓解威胁的适应性、鲁棒性和实用性。", "keywords": "去中心化联邦学习, 声誉系统, 恶意客户端, 模型投毒, 安全性", "comments": "RepuNet的创新之处在于其去中心化的声誉系统设计，避免了传统方案对额外基础设施的依赖。通过动态评估节点行为并调整其影响力，它提供了一种灵活且鲁棒的恶意行为缓解机制，对提升去中心化联邦学习的安全性具有重要意义。"}}
{"id": "2506.20361", "title": "The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models", "authors": ["Yi Wang", "Oli Danyi Liu", "Peter Bell"], "summary": "Human speech perception is multimodal. In natural speech, lip movements can\nprecede corresponding voicing by a non-negligible gap of 100-300 ms, especially\nfor specific consonants, affecting the time course of neural phonetic encoding\nin human listeners. However, it remains unexplored whether self-supervised\nlearning models, which have been used to simulate audio-visual integration in\nhumans, can capture this asynchronicity between audio and visual cues. We\ncompared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using\nlinear classifiers to track their phonetic decodability over time. We found\nthat phoneme information becomes available in AV-HuBERT embeddings only about\n20 ms before HuBERT, likely due to AV-HuBERT's lower temporal resolution and\nfeature concatenation process. It suggests AV-HuBERT does not adequately\ncapture the temporal dynamics of multimodal speech perception, limiting its\nsuitability for modeling the multimodal speech perception process.", "comment": "Accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.20361v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.20361v1", "AI": {"title_translation": "视听整合在自监督语音模型语音编码时间进程中的作用", "tldr": "研究发现，视听模型AV-HuBERT在捕捉人类语音感知中唇部运动和发声的异步性方面表现不佳，其音素信息仅比纯音频模型HuBERT早约20毫秒出现，表明其未能充分捕捉多模态语音感知的时间动态。", "motivation": "人类语音感知是多模态的，唇部运动可以先于发声100-300毫秒，影响神经语音编码。然而，目前尚不清楚用于模拟人类视听整合的自监督学习模型是否能捕捉这种音频和视觉线索之间的异步性。", "method": "研究通过使用线性分类器，比较了视听模型AV-HuBERT与纯音频模型HuBERT的音素可解码性随时间的变化。", "result": "AV-HuBERT嵌入中的音素信息仅比HuBERT早约20毫秒可用，这可能是由于AV-HuBERT较低的时间分辨率和特征连接过程造成的。", "conclusion": "AV-HuBERT未能充分捕捉多模态语音感知的时间动态，限制了其在建模多模态语音感知过程中的适用性。", "translation": "人类的语音感知是多模态的。在自然语音中，唇部运动可以比相应的发声提前100-300毫秒的显著间隔，特别是对于特定的辅音，这会影响人类听众神经语音编码的时间进程。然而，用于模拟人类视听整合的自监督学习模型是否能捕捉到音频和视觉线索之间的这种异步性，仍未被探索。我们通过使用线性分类器来跟踪AV-HuBERT（一个视听模型）和纯音频HuBERT的语音可解码性随时间的变化，从而对它们进行了比较。我们发现，AV-HuBERT嵌入中的音素信息仅比HuBERT早约20毫秒可用，这可能是由于AV-HuBERT较低的时间分辨率和特征连接过程造成的。这表明AV-HuBERT未能充分捕捉多模态语音感知的时间动态，限制了其在建模多模态语音感知过程中的适用性。", "summary": "本文探讨了自监督视听模型（如AV-HuBERT）是否能捕捉人类语音感知中唇部运动和发声之间的自然异步性。通过将AV-HuBERT与纯音频HuBERT进行比较，研究发现AV-HuBERT仅能略微提前（20毫秒）提供音素信息。这表明AV-HuBERT未能充分捕捉多模态语音感知的复杂时间动态，这可能与其模型架构有关。", "keywords": "视听整合, 语音编码, 自监督学习, 语音感知, HuBERT", "comments": "该论文揭示了当前自监督视听模型（AV-HuBERT）在完全捕捉类人多模态语音感知时间动态方面的局限性，特别是音频-视觉异步性。这对开发更具生物学合理性和更鲁棒的语音模型至关重要。研究结果表明，需要改进模型架构以更好地处理时间分辨率和特征整合。"}}
{"id": "2506.20433", "title": "That's Not the Feedback I Need! -- Student Engagement with GenAI Feedback in the Tutor Kai", "authors": ["Sven Jacobs", "Maurice Kempf", "Natalie Kiesler"], "summary": "The potential of Generative AI (GenAI) for generating feedback in computing\neducation has been the subject of numerous studies. However, there is still\nlimited research on how computing students engage with this feedback and to\nwhat extent it supports their problem-solving. For this reason, we built a\ncustom web application providing students with Python programming tasks, a code\neditor, GenAI feedback, and compiler feedback. Via a think-aloud protocol\nincluding eye-tracking and a post-interview with 11 undergraduate students, we\ninvestigate (1) how much attention the generated feedback received from\nlearners and (2) to what extent the generated feedback is helpful (or not). In\naddition, students' attention to GenAI feedback is compared with that towards\nthe compiler feedback. We further investigate differences between students with\nand without prior programming experience. The findings indicate that GenAI\nfeedback generally receives a lot of visual attention, with inexperienced\nstudents spending twice as much fixation time. More experienced students\nrequested GenAI less frequently, and could utilize it better to solve the given\nproblem. It was more challenging for inexperienced students to do so, as they\ncould not always comprehend the GenAI feedback. They often relied solely on the\nGenAI feedback, while compiler feedback was not read. Understanding students'\nattention and perception toward GenAI feedback is crucial for developing\neducational tools that support student learning.", "comment": "Accepted for the UK and Ireland Computing Education Research\n  conference (UKICER 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20433v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20433v1", "AI": {"title_translation": "那不是我需要的反馈！——学生在Tutor Kai中与生成式AI反馈的互动", "tldr": "该研究调查了计算机专业学生如何与生成式AI (GenAI) 反馈互动以及反馈对其解决问题的帮助程度。结果显示GenAI反馈获得了很高的视觉关注，特别是对于经验不足的学生，但有经验的学生能更好地利用它。经验不足的学生难以理解GenAI反馈，有时甚至忽略了编译器反馈。", "motivation": "尽管生成式AI在计算教育中生成反馈的潜力已被广泛研究，但关于计算专业学生如何与此类反馈互动以及反馈在多大程度上支持他们解决问题的研究仍然有限。", "method": "研究构建了一个自定义的Web应用程序，提供Python编程任务、代码编辑器、GenAI反馈和编译器反馈。通过包含眼动追踪和后期访谈的思维发声协议，对11名本科生进行了调查，以了解生成反馈获得的关注度及其有效性。此外，还将学生对GenAI反馈的关注与对编译器反馈的关注进行了比较，并研究了有无编程经验学生之间的差异。", "result": "研究结果表明，GenAI反馈普遍获得了大量视觉关注，其中经验不足的学生花费了两倍的注视时间。经验更丰富的学生请求GenAI的频率较低，但能更好地利用它来解决问题。对于经验不足的学生来说，这更具挑战性，因为他们并非总能理解GenAI反馈。他们经常完全依赖GenAI反馈，而编译器反馈则未被阅读。", "conclusion": "理解学生对GenAI反馈的关注和感知对于开发支持学生学习的教育工具至关重要。", "translation": "生成式AI（GenAI）在计算教育中生成反馈的潜力一直是众多研究的主题。然而，关于计算专业学生如何与这种反馈互动以及它在多大程度上支持他们解决问题的研究仍然有限。因此，我们构建了一个自定义的Web应用程序，为学生提供Python编程任务、代码编辑器、GenAI反馈和编译器反馈。通过包含眼动追踪和后期访谈的思维发声协议，我们对11名本科生进行了调查，以探讨（1）学习者对生成反馈的关注程度以及（2）生成反馈的帮助程度（或不帮助）。此外，还将学生对GenAI反馈的关注与对编译器反馈的关注进行了比较。我们还进一步研究了有无编程经验学生之间的差异。研究结果表明，GenAI反馈普遍获得了大量视觉关注，其中经验不足的学生花费了两倍的注视时间。经验更丰富的学生请求GenAI的频率较低，并能更好地利用它来解决给定的问题。对于经验不足的学生来说，这更具挑战性，因为他们并非总能理解GenAI反馈。他们经常完全依赖GenAI反馈，而编译器反馈则未被阅读。理解学生对GenAI反馈的关注和感知对于开发支持学生学习的教育工具至关重要。", "summary": "本研究旨在弥补计算教育中学生与生成式AI (GenAI) 反馈互动研究的空白。通过开发一个提供Python编程任务和GenAI反馈的Web应用，并结合眼动追踪和思维发声协议，研究了11名本科生对GenAI反馈的关注度和有效性。结果显示，GenAI反馈获得了较高的视觉关注，特别是对经验不足的学生，但他们往往难以理解并过度依赖，甚至忽略了编译器反馈；而经验丰富的学生则能更有效地利用GenAI反馈。研究强调了理解学生对GenAI反馈的感知对于开发有效教育工具的重要性。", "keywords": "生成式AI, 反馈, 计算教育, 学生参与, 眼动追踪", "comments": "该研究通过结合眼动追踪和思维发声协议，对学生与GenAI反馈的互动进行了深入的实证分析，尤其区分了有经验和无经验学生的行为差异，这在现有研究中具有创新性。其重要性在于为未来设计更有效的教育AI工具提供了宝贵的见解，揭示了GenAI反馈在不同经验水平学生中的作用和局限性。然而，11名学生的样本量相对较小，可能限制了结果的普遍性。"}}
{"id": "2506.20244", "title": "Cooperative Sensing and Communication Beamforming Design for Low-Altitude Economy", "authors": ["Fangzhi Li", "Zhichu Ren", "Cunhua Pan", "Hong Ren", "Jing Jin", "Qixing Wang", "Jiangzhou Wang"], "summary": "To empower the low-altitude economy with high-accuracy sensing and high-rate\ncommunication, this paper proposes a cooperative integrated sensing and\ncommunication (ISAC) framework for aerial-ground networks. In the proposed\nsystem, the ground base stations (BSs) cooperatively serve the unmanned aerial\nvehicles (UAVs), which are equipped for either joint communication and sensing\nor sensing-only operations. The BSs employ coordinated beamforming to\nsimultaneously transmit communication and sensing signals, while the UAVs\nexecute their missions. To maximize the weighted sum rate under the sensing\nsignal-to-interference-plus-noise ratio (SINR) constraints, we jointly optimize\nthe transmit beamforming, receive filtering, and UAV trajectory. The resulting\nnon-convex problem is solved using an alternating optimization framework\nincorporating semidefinite relaxation (SDR) and successive convex approximation\n(SCA). Simulation results demonstrate that the proposed joint design achieves\nhigher communication throughput while ensuring required sensing robustness.\nAdditionally, the sensing SINR threshold and the UAV altitude have a\nsignificant impact on the trajectory design, highlighting the necessity of\nadaptive deployment strategies in practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20244v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20244v1", "AI": {"title_translation": "低空经济中的协作感知与通信波束成形设计", "tldr": "本文提出了一种用于空地网络的协作式集成感知与通信（ISAC）框架，以赋能低空经济高精度感知和高速率通信。通过联合优化发射波束成形、接收滤波和无人机轨迹，在感知信噪比（SINR）约束下最大化加权和速率。仿真结果表明，该联合设计实现了更高的通信吞吐量并确保了所需的感知鲁棒性，同时强调了感知SINR阈值和无人机高度对轨迹设计的重要性。", "motivation": "为了赋能低空经济高精度感知和高速率通信。", "method": "本文提出了一种用于空地网络的协作式集成感知与通信（ISAC）框架。在该系统中，地面基站（BSs）协作服务无人机（UAVs），基站采用协作波束成形同时传输通信和感知信号。为最大化在感知信噪比（SINR）约束下的加权和速率，联合优化发射波束成形、接收滤波和无人机轨迹。该非凸问题通过结合半定松弛（SDR）和逐次凸逼近（SCA）的交替优化框架求解。", "result": "仿真结果表明，所提出的联合设计在确保所需感知鲁棒性的同时实现了更高的通信吞吐量。此外，感知SINR阈值和无人机高度对轨迹设计有显著影响，突出显示了在实际应用中自适应部署策略的必要性。", "conclusion": "所提出的联合设计能够提高通信吞吐量并确保感知鲁棒性。感知SINR阈值和无人机高度对轨迹设计有显著影响，因此在实际应用中需要采用自适应部署策略。", "translation": "为了赋能低空经济高精度感知和高速率通信，本文提出了一种用于空地网络的协作式集成感知与通信（ISAC）框架。在该系统中，地面基站（BSs）协作服务无人机（UAVs），这些无人机配备用于联合通信和感知或仅感知操作。基站采用协作波束成形同时传输通信和感知信号，而无人机执行其任务。为了在感知信噪比（SINR）约束下最大化加权和速率，我们联合优化了发射波束成形、接收滤波和无人机轨迹。由此产生的非凸问题通过结合半定松弛（SDR）和逐次凸逼近（SCA）的交替优化框架求解。仿真结果表明，所提出的联合设计在确保所需感知鲁棒性的同时实现了更高的通信吞吐量。此外，感知SINR阈值和无人机高度对轨迹设计有显著影响，突出了在实际应用中自适应部署策略的必要性。", "summary": "本文提出了一种用于空地网络的协作式集成感知与通信（ISAC）框架，旨在为低空经济提供高精度感知和高速率通信能力。该框架涉及地面基站协作服务无人机，并通过协调波束成形同时传输通信和感知信号。核心贡献在于联合优化发射波束成形、接收滤波和无人机轨迹，以在满足感知信噪比（SINR）约束的同时最大化加权和通信速率。该非凸问题通过结合半定松弛（SDR）和逐次凸逼近（SCA）的交替优化框架进行求解。仿真结果验证了该联合设计能提升通信吞吐量并维持稳健的感知性能，同时揭示了感知SINR阈值和无人机高度对轨迹规划的关键影响，从而强调了自适应部署策略的重要性。", "keywords": "集成感知与通信, 低空经济, 波束成形, 无人机, 轨迹优化", "comments": "该论文通过整合感知和通信来解决新兴低空经济的关键需求，具有创新性。其对多个参数（波束成形、滤波、轨迹）的联合优化，以及利用先进技术（SDR、SCA）解决非凸问题，是显著的技术贡献。关于SINR阈值和无人机高度影响的发现为自适应部署提供了实用的见解。"}}
{"id": "2506.19957", "title": "Posterior Cramér-Rao Bounds on Localization and Mapping Errors in Distributed MIMO SLAM", "authors": ["Benjamin J. B. Deutschmann", "Xuhong Li", "Florian Meyer", "Erik Leitinger"], "summary": "Radio-frequency simultaneous localization and mapping (RF-SLAM) methods\njointly infer the position of mobile transmitters and receivers in wireless\nnetworks, together with a geometric map of the propagation environment. An\ninferred map of specular surfaces can be used to exploit non-line-of-sight\ncomponents of the multipath channel to increase robustness, bypass\nobstructions, and improve overall communication and positioning performance.\nWhile performance bounds for user location are well established, the literature\nlacks performance bounds for map information. This paper derives the mapping\nerror bound (MEB), i.e., the posterior Cram\\'er-Rao lower bound on the position\nand orientation of specular surfaces, for RF-SLAM. In particular, we consider a\nvery general scenario with single- and double-bounce reflections, as well as\ndistributed anchors. We demonstrate numerically that a state-of-the-art RF-SLAM\nalgorithm asymptotically converges to this MEB. The bounds assess not only the\nlocalization (position and orientation) but also the mapping performance of\nRF-SLAM algorithms in terms of global features.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19957v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.19957v1", "AI": {"title_translation": "分布式MIMO SLAM中定位和建图误差的后验克拉默-劳界", "tldr": "本文为射频同步定位与建图（RF-SLAM）推导了建图误差界（MEB），即镜面反射表面位置和方向的后验克拉默-劳下界，填补了现有文献中地图信息性能界限的空白。数值结果表明，最先进的RF-SLAM算法渐近收敛于此界。", "motivation": "射频同步定位与建图（RF-SLAM）方法在无线网络中联合推断移动发射器和接收器的位置以及传播环境的几何地图。虽然用户位置的性能界限已得到充分确立，但现有文献缺乏地图信息的性能界限。", "method": "本文推导了射频同步定位与建图（RF-SLAM）的建图误差界（MEB），即镜面反射表面位置和方向的后验克拉默-劳下界。研究考虑了包含单次和双次反射以及分布式锚点的通用场景。", "result": "数值结果表明，一个最先进的RF-SLAM算法渐近收敛于所推导的建图误差界（MEB）。这些界限不仅评估了定位（位置和方向）性能，还评估了RF-SLAM算法在全局特征方面的建图性能。", "conclusion": "本文为射频同步定位与建图（RF-SLAM）推导了 previously 缺失的建图误差界（MEB），即镜面反射表面位置和方向的后验克拉默-劳下界，为评估RF-SLAM算法的定位和建图性能提供了理论基础。", "translation": "射频同步定位与建图（RF-SLAM）方法联合推断无线网络中移动发射器和接收器的位置以及传播环境的几何地图。推断的镜面可以用于利用多径信道的非视距分量，以增加鲁棒性、绕过障碍物并提高整体通信和定位性能。虽然用户位置的性能界限已得到充分确立，但现有文献缺乏地图信息的性能界限。本文为RF-SLAM推导了建图误差界（MEB），即镜面反射表面位置和方向的后验克拉默-劳下界。特别是，我们考虑了一个非常通用的场景，包括单次和双次反射以及分布式锚点。我们通过数值实验证明，一个最先进的RF-SLAM算法渐近收敛于此MEB。这些界限不仅评估了定位（位置和方向）性能，还评估了RF-SLAM算法在全局特征方面的建图性能。", "summary": "本文针对分布式MIMO SLAM中的射频同步定位与建图（RF-SLAM）问题，推导了镜面反射表面位置和方向的后验克拉默-劳下界，即建图误差界（MEB）。该研究填补了现有文献中地图信息性能界限的空白。在包含单次和双次反射及分布式锚点的通用场景下，数值结果验证了最先进的RF-SLAM算法能够渐近收敛于所推导的MEB，为评估RF-SLAM算法的定位和建图性能提供了理论依据。", "keywords": "RF-SLAM, 克拉默-劳界, 建图误差界, 分布式MIMO, 定位与建图", "comments": "本文的创新点在于首次为射频同步定位与建图（RF-SLAM）推导了建图误差界（MEB），弥补了该领域在地图信息性能界限方面的空白。这对于评估和优化RF-SLAM算法的性能具有重要意义，尤其是在复杂多径环境下利用非视距分量提高定位和建图鲁棒性方面。"}}
{"id": "2506.20206", "title": "Volumetric segmentation of muscle compartments using in vivo imaging and architectural validation in human finger flexors", "authors": ["Yang Li"], "summary": "Segmenting muscle compartments and measuring their architecture can\nfacilitate movement function assessment, accurate musculoskeletal modeling, and\nsynergy-based electromyogram simulation. Here, we presented a novel method for\nvolumetric segmentation of muscle compartments using in vivo imaging, focusing\non the independent compartments for finger control of flexor digitorum\nsuperficialis (FDS). Besides, we measured the architectural properties of FDS\ncompartments and validated the segmentation. Specifically, ultrasound and\nmagnetic resonance imaging (MRI) from 10 healthy subjects were used for\nsegmentation and measurement, while electromyography was utilized for\nvalidation. A two-step piecewise segmentation was proposed, first annotating\ncompartment regions in the cross-sectional ultrasound image based on\ncompartment movement, and then performing minimum energy matching to register\nthe ultrasound data to the three-dimensional MRI coordinate system.\nAdditionally, the architectural properties were measured in the compartment\nmasks from the segmentation using MRI tractography. Anatomical correctness was\nverified by comparing known anatomy with reconstructed fiber tracts and\nmeasured properties, while segmentation accuracy was quantified as the\npercentage of finger electromyogram centers falling within their corresponding\ncompartments. Results demonstrated agreement for the fiber orientation between\nthe tractography and cadaveric photographs. Significant differences in\narchitectural properties (P < 0.001) were observed between compartments. The\nproperties of FDS and its compartments were within the physiological ranges (P\n< 0.01). 95% (38/40) of the electromyogram centers were located within\nrespective compartments, with 2 errors occurring in the index and little\nfingers. The validated segmentation method and derived architectural properties\nmay advance biomedical applications.", "comment": "19 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.20206v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20206v1", "AI": {"title_translation": "使用体内成像和人体指屈肌结构验证的肌肉隔室体积分割", "tldr": "该研究提出了一种利用体内超声和MRI图像对指屈肌（FDS）肌肉隔室进行体积分割的新方法，并对其结构特性进行了测量和验证。", "motivation": "分割肌肉隔室并测量其结构特性有助于运动功能评估、精确的肌肉骨骼建模和基于协同的肌电图模拟。", "method": "该研究使用10名健康受试者的超声和MRI图像进行分割和测量，并利用肌电图进行验证。提出了一种两步分段分割方法：首先根据隔室运动在横截面超声图像中标记隔室区域，然后进行最小能量匹配将超声数据注册到三维MRI坐标系。通过MRI纤维束成像测量隔室掩膜中的结构特性。通过与已知解剖结构比较重建的纤维束和测量特性来验证解剖正确性，通过计算指肌电图中心落在相应隔室内的百分比来量化分割精度。", "result": "结果显示，纤维方向在纤维束成像和尸体照片之间存在一致性。隔室之间观察到结构特性存在显著差异（P < 0.001）。FDS及其隔室的特性在生理范围内（P < 0.01）。95%（38/40）的肌电图中心位于各自的隔室中，食指和小指出现2个错误。", "conclusion": "经验证的分割方法和推导的结构特性可能推动生物医学应用。", "translation": "分割肌肉隔室并测量其结构特性有助于运动功能评估、精确的肌肉骨骼建模和基于协同的肌电图模拟。本研究提出了一种利用体内成像对肌肉隔室进行体积分割的新方法，重点关注指屈肌（FDS）的独立指控隔室。此外，我们测量了FDS隔室的结构特性并验证了分割结果。具体而言，使用10名健康受试者的超声和磁共振成像（MRI）数据进行分割和测量，同时利用肌电图进行验证。提出了一种两步分段分割方法，首先根据隔室运动在横截面超声图像中标记隔室区域，然后进行最小能量匹配将超声数据注册到三维MRI坐标系。此外，使用MRI纤维束成像从分割后的隔室掩膜中测量结构特性。通过将重建的纤维束和测量特性与已知解剖结构进行比较来验证解剖正确性，分割精度通过计算指肌电图中心落在相应隔室内的百分比进行量化。结果表明，纤维方向在纤维束成像和尸体照片之间存在一致性。隔室之间观察到结构特性存在显著差异（P < 0.001）。FDS及其隔室的特性在生理范围内（P < 0.01）。95%（38/40）的肌电图中心位于各自的隔室中，食指和小指出现2个错误。经验证的分割方法和推导的结构特性可能推动生物医学应用。", "summary": "该研究提出了一种基于体内超声和MRI成像对人体指屈肌（FDS）肌肉隔室进行体积分割的新方法，并对其结构特性进行了测量和验证。通过两步分段分割法结合超声和MRI数据，并利用肌电图进行精度验证。结果显示该方法具有较高的分割准确性和解剖学正确性，且测量到的结构特性符合生理范围，有望在生物医学应用中发挥作用。", "keywords": "肌肉隔室分割, 体内成像, 指屈肌, 结构验证, 超声MRI融合", "comments": "该论文提出了一种新颖的结合超声和MRI的肌肉隔室体积分割方法，并首次对FDS的独立指控隔室进行了详细的结构验证，其创新性在于多模态成像数据的融合和对精细肌肉隔室的精确表征。这项工作为未来的肌肉骨骼建模和运动功能评估提供了更精确的解剖学基础。"}}
{"id": "2506.19967", "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs", "authors": ["Travis Thompson", "Seung-Hwan Lim", "Paul Liu", "Ruoying He", "Dongkuan Xu"], "summary": "Large Language Models (LLMs) have achieved impressive capabilities in\nlanguage understanding and generation, yet they continue to underperform on\nknowledge-intensive reasoning tasks due to limited access to structured context\nand multi-hop information. Retrieval-Augmented Generation (RAG) partially\nmitigates this by grounding generation in retrieved context, but conventional\nRAG and GraphRAG methods often fail to capture relational structure across\nnodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel\nframework that enhances LLM-based graph reasoning by applying inference-time\ncompute scaling. Our method combines sequential scaling with deep\nchain-of-thought graph traversal, and parallel scaling with majority voting\nover sampled trajectories within an interleaved reasoning-execution loop.\nExperiments on the GRBench benchmark demonstrate that our approach\nsignificantly improves multi-hop question answering performance, achieving\nsubstantial gains over both traditional GraphRAG and prior graph traversal\nbaselines. These findings suggest that inference-time scaling is a practical\nand architecture-agnostic solution for structured knowledge reasoning with LLMs", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19967v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.19967v1", "AI": {"title_translation": "推理扩展GraphRAG：改进知识图谱上的多跳问答", "tldr": "提出推理扩展GraphRAG框架，通过结合序列和并行扩展，显著提升LLM在知识图谱上多跳问答的性能。", "motivation": "大型语言模型（LLMs）在知识密集型推理任务中表现不佳，因为它们难以访问结构化上下文和多跳信息。现有RAG和GraphRAG方法未能捕获知识图谱中节点间的关系结构。", "method": "引入推理扩展GraphRAG（Inference-Scaled GraphRAG）框架，通过应用推理时计算扩展来增强LLM的图推理能力。该方法结合了深度思维链图遍历的序列扩展，以及在推理-执行循环中对采样轨迹进行多数投票的并行扩展。", "result": "在GRBench基准测试中，该方法显著提高了多跳问答性能，相比传统GraphRAG和之前的图遍历基线取得了实质性提升。", "conclusion": "推理时扩展是一种实用且与架构无关的解决方案，适用于LLM进行结构化知识推理。", "translation": "大型语言模型（LLM）在语言理解和生成方面取得了令人印象深刻的能力，但由于对结构化上下文和多跳信息的访问受限，它们在知识密集型推理任务上仍表现不佳。检索增强生成（RAG）通过将生成基于检索到的上下文部分缓解了这一问题，但传统的RAG和GraphRAG方法往往未能捕获知识图谱中节点间的关系结构。我们引入了推理扩展GraphRAG，这是一个新颖的框架，通过应用推理时计算扩展来增强基于LLM的图推理能力。我们的方法结合了深度思维链图遍历的序列扩展，以及在交错的推理-执行循环中对采样轨迹进行多数投票的并行扩展。在GRBench基准测试上的实验表明，我们的方法显著提高了多跳问答性能，相比传统GraphRAG和之前的图遍历基线取得了实质性提升。这些发现表明，推理时扩展是一种实用且与架构无关的解决方案，适用于LLM进行结构化知识推理。", "summary": "本文提出推理扩展GraphRAG框架，旨在解决大型语言模型在知识图谱多跳问答中因难以捕获关系结构而表现不佳的问题。该框架通过结合深度思维链图遍历的序列扩展和基于多数投票的并行扩展，在推理时对计算进行扩展。实验结果表明，该方法在GRBench基准测试上显著提升了多跳问答性能，验证了推理时扩展作为LLM结构化知识推理的有效性。", "keywords": "知识图谱, 多跳问答, 大型语言模型, 检索增强生成, 推理时扩展", "comments": "本文的创新点在于提出了“推理时计算扩展”的概念，并将其应用于GraphRAG框架，以解决LLM在处理知识图谱复杂关系时的局限性。通过结合序列和并行扩展策略，该方法提供了一种通用且与架构无关的解决方案，对于提升LLM在知识密集型推理任务中的表现具有重要意义。"}}
{"id": "2506.20267", "title": "X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis", "authors": ["Fabian Bongratz", "Tom Nuno Wolf", "Jaume Gual Ramon", "Christian Wachinger"], "summary": "Interpretable models are crucial for supporting clinical decision-making,\ndriving advances in their development and application for medical images.\nHowever, the nature of 3D volumetric data makes it inherently challenging to\nvisualize and interpret intricate and complex structures like the cerebral\ncortex. Cortical surface renderings, on the other hand, provide a more\naccessible and understandable 3D representation of brain anatomy, facilitating\nvisualization and interactive exploration. Motivated by this advantage and the\nwidespread use of surface data for studying neurological disorders, we present\nthe eXplainable Surface Vision Transformer (X-SiT). This is the first\ninherently interpretable neural network that offers human-understandable\npredictions based on interpretable cortical features. As part of X-SiT, we\nintroduce a prototypical surface patch decoder for classifying surface patch\nembeddings, incorporating case-based reasoning with spatially corresponding\ncortical prototypes. The results demonstrate state-of-the-art performance in\ndetecting Alzheimer's disease and frontotemporal dementia while additionally\nproviding informative prototypes that align with known disease patterns and\nreveal classification errors.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20267v1", "categories": ["cs.GR", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20267v1", "AI": {"title_translation": "X-SiT：用于痴呆症诊断的固有可解释表面视觉Transformer", "tldr": "X-SiT是一种新型的、固有的可解释表面视觉Transformer，用于痴呆症诊断，利用皮层表面渲染提供人类可理解的预测，并在阿尔茨海默病和额颞叶痴呆检测方面实现了最先进的性能。", "motivation": "可解释模型对于支持临床决策至关重要，但3D体数据的可视化和解释具有挑战性。皮层表面渲染提供了更易于理解的3D脑解剖表示，且表面数据广泛用于神经系统疾病研究。", "method": "提出了可解释表面视觉Transformer (X-SiT)，这是第一个提供基于可解释皮层特征的人类可理解预测的固有可解释神经网络。X-SiT包含一个原型表面补丁解码器，用于分类表面补丁嵌入，并结合了基于案例的推理和空间对应的皮层原型。", "result": "在检测阿尔茨海默病和额颞叶痴呆方面表现出最先进的性能，并提供了与已知疾病模式一致并揭示分类错误的信息性原型。", "conclusion": "X-SiT通过提供固有的可解释预测和高性能，为痴呆症的诊断提供了一个有价值的工具，其原型有助于理解疾病模式和识别模型错误。", "translation": "可解释模型对于支持临床决策至关重要，推动了其在医学图像开发和应用方面的进展。然而，3D体积数据的性质使得可视化和解释大脑皮层等复杂精细结构本身就具有挑战性。另一方面，皮层表面渲染提供了更易于访问和理解的大脑解剖3D表示，有助于可视化和交互式探索。受此优势以及表面数据在神经系统疾病研究中广泛使用的启发，我们提出了可解释表面视觉Transformer（X-SiT）。这是第一个固有的可解释神经网络，它基于可解释的皮层特征提供人类可理解的预测。作为X-SiT的一部分，我们引入了一个原型表面补丁解码器，用于对表面补丁嵌入进行分类，并结合了基于案例的推理和空间对应的皮层原型。结果表明，在检测阿尔茨海默病和额颞叶痴呆方面达到了最先进的性能，同时还提供了与已知疾病模式一致并揭示分类错误的信息性原型。", "summary": "本论文介绍了X-SiT，一种创新的、固有的可解释表面视觉Transformer，专为痴呆症诊断设计。它利用皮层表面渲染的优势，克服了3D体数据解释的挑战，并通过引入原型表面补丁解码器提供人类可理解的预测。X-SiT在阿尔茨海默病和额颞叶痴呆检测中取得了最先进的性能，并生成了有助于理解疾病模式和识别模型错误的可解释原型。", "keywords": "可解释AI, 视觉Transformer, 痴呆症诊断, 皮层表面, 医学图像分析", "comments": "X-SiT的创新性在于它是第一个固有的可解释神经网络，能够基于皮层表面数据提供人类可理解的预测，这对于临床决策支持至关重要。其结合原型的方法不仅提升了性能，也增强了模型的可解释性，有助于医生理解诊断依据和识别潜在错误。"}}
{"id": "2506.20051", "title": "Controlled Retrieval-augmented Context Evaluation for Long-form RAG", "authors": ["Jia-Huei Ju", "Suzan Verberne", "Maarten de Rijke", "Andrew Yates"], "summary": "Retrieval-augmented generation (RAG) enhances large language models by\nincorporating context retrieved from external knowledge sources. While the\neffectiveness of the retrieval module is typically evaluated with\nrelevance-based ranking metrics, such metrics may be insufficient to reflect\nthe retrieval's impact on the final RAG result, especially in long-form\ngeneration scenarios. We argue that providing a comprehensive\nretrieval-augmented context is important for long-form RAG tasks like report\ngeneration and propose metrics for assessing the context independent of\ngeneration. We introduce CRUX, a \\textbf{C}ontrolled\n\\textbf{R}etrieval-a\\textbf{U}gmented conte\\textbf{X}t evaluation framework\ndesigned to directly assess retrieval-augmented contexts. This framework uses\nhuman-written summaries to control the information scope of knowledge, enabling\nus to measure how well the context covers information essential for long-form\ngeneration. CRUX uses question-based evaluation to assess RAG's retrieval in a\nfine-grained manner. Empirical results show that CRUX offers more reflective\nand diagnostic evaluation. Our findings also reveal substantial room for\nimprovement in current retrieval methods, pointing to promising directions for\nadvancing RAG's retrieval. Our data and code are publicly available to support\nand advance future research on retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20051v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20051v1", "AI": {"title_translation": "长文本RAG中受控检索增强上下文评估", "tldr": "现有RAG检索评估指标在长文本生成中不足，本文提出CRUX框架，直接评估检索增强上下文，通过人工摘要和问答评估，发现当前检索方法有很大改进空间。", "motivation": "现有的检索模块评估方法（基于相关性排序指标）不足以反映检索对最终RAG结果的影响，尤其是在长文本生成场景中。提供全面的检索增强上下文对于长文本RAG任务（如报告生成）至关重要。", "method": "本文提出CRUX（Controlled Retrieval-augmented context evaluation framework），一个受控的检索增强上下文评估框架，旨在直接评估检索增强上下文。CRUX使用人工编写的摘要来控制知识的信息范围，从而衡量上下文对长文本生成所需信息的覆盖程度。它还采用基于问题的评估方式，以细粒度方式评估RAG的检索。", "result": "实证结果表明CRUX提供了更具反思性和诊断性的评估。研究结果还揭示了当前检索方法存在很大的改进空间，为推进RAG的检索指明了有前景的方向。", "conclusion": "CRUX框架为RAG的检索进展指明了有前景的方向。本文公开了数据和代码以支持和推进未来的检索研究。", "translation": "检索增强生成（RAG）通过整合从外部知识源检索到的上下文来增强大型语言模型。虽然检索模块的有效性通常通过基于相关性的排名指标进行评估，但这些指标可能不足以反映检索对最终RAG结果的影响，尤其是在长文本生成场景中。我们认为，提供全面的检索增强上下文对于报告生成等长文本RAG任务至关重要，并提出了独立于生成来评估上下文的指标。我们引入了CRUX，一个受控的检索增强上下文评估框架，旨在直接评估检索增强上下文。该框架使用人工编写的摘要来控制知识的信息范围，使我们能够衡量上下文对长文本生成所需基本信息的覆盖程度。CRUX采用基于问题的评估方式，以细粒度方式评估RAG的检索。实证结果表明，CRUX提供了更具反思性和诊断性的评估。我们的发现还揭示了当前检索方法存在很大的改进空间，为推进RAG的检索指明了有前景的方向。我们的数据和代码已公开，以支持和推进未来的检索研究。", "summary": "本文针对现有RAG检索评估指标在长文本生成场景中的不足，提出了CRUX框架。CRUX通过使用人工摘要控制信息范围并结合问答评估，直接衡量检索增强上下文的质量及其对长文本生成关键信息的覆盖度。实验证明CRUX提供了更准确和诊断性的评估，并揭示当前RAG检索方法仍有显著提升空间。", "keywords": "检索增强生成, 长文本RAG, 上下文评估, CRUX, 检索方法", "comments": "CRUX框架的创新之处在于其直接评估检索增强上下文，而非仅仅依赖传统的检索排名指标。通过引入人工摘要控制信息范围和基于问题的细粒度评估，它为长文本RAG的检索模块提供了一个更具诊断性和反射性的评估方法，有助于识别现有检索方法的不足并指明未来改进方向。"}}
{"id": "2506.20469", "title": "Surrogate-Assisted Evolution for Efficient Multi-branch Connection Design in Deep Neural Networks", "authors": ["Fergal Stapleton", "Daniel García Núñez", "Yanan Sun", "Edgar Galván"], "summary": "State-of-the-art Deep Neural Networks (DNNs) often incorporate multi-branch\nconnections, enabling multi-scale feature extraction and enhancing the capture\nof diverse features. This design improves network capacity and generalisation\nto unseen data. However, training such DNNs can be computationally expensive.\nThe challenge is further exacerbated by the complexity of identifying optimal\nnetwork architectures. To address this, we leverage Evolutionary Algorithms\n(EAs) to automatically discover high-performing architectures, a process\ncommonly known as neuroevolution. We introduce a novel approach based on Linear\nGenetic Programming (LGP) to encode multi-branch (MB) connections within DNNs,\nreferred to as NeuroLGP-MB. To efficiently design the DNNs, we use\nsurrogate-assisted EAs. While their application in simple artificial neural\nnetworks has been influential, we scale their use from dozens or hundreds of\nsample points to thousands, aligning with the demands of complex DNNs by\nincorporating a semantic-based approach in our surrogate-assisted EA.\nFurthermore, we introduce a more advanced surrogate model that outperforms\nbaseline, computationally expensive, and simpler surrogate models.", "comment": "GECCO '25 Companion: Proceedings of the Companion Conference on\n  Genetic and Evolutionary Computation (2025)", "pdf_url": "http://arxiv.org/pdf/2506.20469v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20469v1", "AI": {"title_translation": "代理辅助进化用于深度神经网络中多分支连接的有效设计", "tldr": "本文提出NeuroLGP-MB，一种基于线性遗传编程和代理辅助进化算法的新方法，旨在高效设计深度神经网络中的多分支连接，以应对训练成本高和架构优化复杂的挑战。", "motivation": "最先进的深度神经网络（DNNs）虽然采用多分支连接来增强特征提取和泛化能力，但其训练成本高昂，且识别最优网络架构的复杂性进一步加剧了这一挑战。", "method": "本文引入了一种名为NeuroLGP-MB的新方法，该方法基于线性遗传编程（LGP）来编码DNN中的多分支连接。为高效设计DNN，研究者使用了代理辅助的进化算法（EAs），并通过结合语义方法将其应用规模从数十或数百个样本点扩展到数千个，以适应复杂DNN的需求。此外，论文还引入了一个性能优于基线和简单代理模型的更先进的代理模型。", "result": "文中提到引入了一种更先进的代理模型，其性能优于基线、计算成本高昂且更简单的代理模型。该方法能够高效地设计DNN。", "conclusion": "本文通过引入NeuroLGP-MB和先进的代理辅助进化算法，有效解决了深度神经网络中多分支连接设计效率低下的问题，并展示了所提出代理模型的优越性。", "translation": "最先进的深度神经网络（DNNs）通常包含多分支连接，这使得多尺度特征提取成为可能，并增强了对不同特征的捕获。这种设计提高了网络容量和对未见数据的泛化能力。然而，训练此类DNN可能计算成本高昂。识别最优网络架构的复杂性进一步加剧了这一挑战。为解决此问题，我们利用进化算法（EAs）自动发现高性能架构，这一过程通常被称为神经进化。我们引入了一种基于线性遗传编程（LGP）的新方法来编码DNN中的多分支（MB）连接，称之为NeuroLGP-MB。为了高效设计DNNs，我们使用了代理辅助的进化算法。虽然它们在简单人工神经网络中的应用影响深远，但我们通过在代理辅助EA中引入基于语义的方法，将其应用规模从数十或数百个样本点扩展到数千个，以适应复杂DNN的需求。此外，我们引入了一个更先进的代理模型，其性能优于基线、计算成本高昂且更简单的代理模型。", "summary": "本文提出了一种名为NeuroLGP-MB的新型神经进化方法，它利用线性遗传编程来编码深度神经网络中的多分支连接。为了克服传统DNN训练成本高和架构搜索复杂的挑战，该方法创新性地采用了代理辅助进化算法，并将其应用规模扩展至数千个样本点，同时引入了一个性能更优的先进代理模型，从而实现了深度神经网络的高效设计。", "keywords": "神经进化, 代理辅助进化, 多分支连接, 深度神经网络, 线性遗传编程", "comments": "该论文的创新点在于将代理辅助进化算法应用于复杂深度神经网络的多分支连接设计中，特别是通过引入语义方法将样本规模扩展到数千个，并提出了性能更优的代理模型。这对于降低深度神经网络的架构搜索成本具有重要意义，有助于推动自动化机器学习（AutoML）领域的发展。"}}
{"id": "2506.20107", "title": "LZSE: an LZ-style compressor supporting $O(\\log n)$-time random access", "authors": ["Hiroki Shibata", "Yuto Nakashima", "Yutaro Yamaguchi", "Shunsuke Inenaga"], "summary": "An LZ-like factorization of a string is a factorization in which each factor\nis either a single character or a copy of a substring that occurs earlier in\nthe string. While grammar-based compression schemes support efficient random\naccess with linear space in the size of the compressed representation, such\nmethods are not known for general LZ-like factorizations. This has led to the\ndevelopment of restricted LZ-like schemes such as LZ-End [Kreft and Navarro,\n2013] and height-bounded (LZHB) [Bannai et al., 2024], which trade off some\ncompression efficiency for faster access. We introduce LZ-Start-End (LZSE), a\nnew variant of LZ-like factorizations in which each copy factor refers to a\ncontiguous sequence of preceding factors. By its nature, any context-free\ngrammar can easily be converted into an LZSE factorization of equal size.\nFurther, we study the greedy LZSE factorization, in which each copy factor is\ntaken as long as possible. We show how the greedy LZSE factorization can be\ncomputed in linear time with respect to the input string length, and that there\nexists a family of strings for which the size of the greedy LZSE factorization\nis of strictly lower order than that of the smallest grammar. These imply that\nour LZSE scheme is stronger than grammar-based compressions in the context of\nrepetitiveness measures. To support fast queries, we propose a data structure\nfor LZSE-compressed strings that permits $O(\\log n)$-time random access within\nspace linear in the compressed size, where $n$ is the length of the input\nstring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20107v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20107v1", "AI": {"title_translation": "LZSE：一种支持 $O(\\log n)$ 时间随机访问的LZ风格压缩器", "tldr": "引入了LZSE，一种新的LZ风格压缩方案，它在某些情况下比基于语法的压缩更优，并支持 $O(\\log n)$ 时间的随机访问。", "motivation": "现有的LZ类分解方法无法像基于语法的压缩方案那样支持高效的随机访问，而现有的受限LZ类方案为了更快的访问牺牲了部分压缩效率。", "method": "论文引入了LZ-Start-End (LZSE)，一种新的LZ类分解变体，其中每个复制因子引用一系列连续的前置因子。研究了贪婪LZSE分解，并展示了其可以在线性时间内计算。此外，提出了一种数据结构，用于支持LZSE压缩字符串的 $O(\\log n)$ 时间随机访问。", "result": "贪婪LZSE分解可以在线性时间内计算；对于某些字符串家族，贪婪LZSE分解的大小严格低于最小语法的大小；为LZSE压缩字符串提供的数据结构支持 $O(\\log n)$ 时间的随机访问，且空间与压缩大小呈线性关系。", "conclusion": "LZSE方案在重复性度量方面比基于语法的压缩更强大，并且能够支持快速的随机访问查询。", "translation": "字符串的LZ类分解是一种分解方式，其中每个因子要么是单个字符，要么是字符串中较早出现的子字符串的副本。虽然基于语法的压缩方案支持以压缩表示大小的线性空间进行高效随机访问，但对于一般的LZ类分解，此类方法尚不为人所知。这导致了受限LZ类方案的发展，例如LZ-End [Kreft和Navarro，2013]和高度有界（LZHB）[Bannai等人，2024]，它们牺牲了一些压缩效率以换取更快的访问。我们引入了LZ-Start-End（LZSE），一种新的LZ类分解变体，其中每个复制因子引用一系列连续的前置因子。从本质上讲，任何上下文无关文法都可以轻松转换为大小相等的LZSE分解。此外，我们研究了贪婪LZSE分解，其中每个复制因子都尽可能长。我们展示了贪婪LZSE分解如何以输入字符串长度的线性时间计算，并且存在一个字符串家族，其贪婪LZSE分解的大小严格低于最小语法的大小。这些结果表明，在重复性度量方面，我们的LZSE方案比基于语法的压缩更强大。为了支持快速查询，我们为LZSE压缩字符串提出了一种数据结构，该结构允许以压缩大小的线性空间进行 $O(\\log n)$ 时间的随机访问，其中 $n$ 是输入字符串的长度。", "summary": "这篇论文引入了一种名为LZ-Start-End (LZSE) 的新型LZ风格压缩方案，旨在解决现有LZ类分解在随机访问效率上的不足。LZSE允许复制因子引用连续的前置因子，并且任何上下文无关文法都可以转换为等大小的LZSE。研究发现，贪婪LZSE分解可以在线性时间内计算，并且在重复性度量方面，对于某些字符串，其压缩率优于最小语法。此外，论文提出了一种支持 $O(\\log n)$ 时间随机访问的数据结构，其空间开销与压缩大小呈线性关系，从而实现了高效的随机查询。", "keywords": "LZSE, 随机访问, 压缩, LZ风格分解, 语法压缩", "comments": "这篇论文的创新点在于提出了LZSE这种新的LZ类分解变体，它不仅在某些情况下能提供比传统语法压缩更好的压缩效率（针对重复性度量），而且突破性地实现了对LZ风格压缩数据的高效随机访问 ($O(\\log n)$ 时间)，解决了该领域长期存在的挑战。这对于需要快速访问压缩数据的应用具有重要意义。"}}
{"id": "2506.19944", "title": "A Hybrid High-Order Method for the Gross--Pitaevskii Eigenvalue Problem", "authors": ["Moritz Hauck", "Yizhou Liang"], "summary": "We introduce a hybrid high-order method for approximating the ground state of\nthe nonlinear Gross--Pitaevskii eigenvalue problem. Optimal convergence rates\nare proved for the ground state approximation, as well as for the associated\neigenvalue and energy approximations. Unlike classical conforming methods,\nwhich inherently provide upper bounds on the ground state energy, the proposed\napproach gives rise to guaranteed and asymptotically exact lower energy bounds.\nImportantly, and in contrast to previous works, they are obtained directly\nwithout the need of post-processing, leading to more accurate guaranteed lower\nenergy bounds in practice.", "comment": "24 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.19944v1", "categories": ["math.NA", "cs.NA", "65N12, 65N15, 65N25, 65N30"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.19944v1", "AI": {"title_translation": "Gross--Pitaevskii特征值问题的一种混合高阶方法", "tldr": "本文提出了一种混合高阶方法来近似非线性Gross--Pitaevskii特征值问题的基态，该方法能够直接获得有保证的、渐近精确的能量下界，无需后处理。", "motivation": "解决非线性Gross--Pitaevskii特征值问题中基态的近似问题，并改进现有方法在提供能量界限方面的局限性（特别是能够直接获得下界）。", "method": "引入了一种混合高阶方法（hybrid high-order method）。", "result": "证明了基态近似、相关特征值和能量近似的最佳收敛率。所提出的方法能够直接产生有保证的、渐近精确的下能量界限，且无需后处理，从而在实践中提供更准确的下能量界限。", "conclusion": "所提出的混合高阶方法能够直接、更准确地提供Gross--Pitaevskii特征值问题基态能量的下界，优于需要后处理的传统方法。", "translation": "我们引入了一种混合高阶方法，用于近似非线性Gross--Pitaevskii特征值问题的基态。证明了基态近似以及相关特征值和能量近似的最佳收敛率。与传统的一致性方法（其固有地提供基态能量的上界）不同，所提出的方法产生了有保证的、渐近精确的下能量界限。重要的是，与以前的工作相反，它们是直接获得的，无需后处理，从而在实践中获得更准确的有保证的下能量界限。", "summary": "本文提出了一种混合高阶方法，用于求解非线性Gross--Pitaevskii特征值问题的基态。该方法不仅在基态、特征值和能量近似方面达到了最佳收敛率，而且与传统方法不同，它能直接且无需后处理地提供有保证和渐近精确的能量下界，从而在实践中获得更准确的下界结果。", "keywords": "混合高阶方法, Gross--Pitaevskii方程, 基态近似, 能量下界, 收敛率", "comments": "该研究的创新之处在于其提出的混合高阶方法能够直接获得Gross--Pitaevskii方程基态能量的下界，并且无需传统的后处理步骤，这在计算效率和实际应用中的精度方面是一个显著的改进。"}}
{"id": "2506.20212", "title": "Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning", "authors": ["Andrea Bussolan", "Oliver Avram", "Andrea Pignata", "Gianvito Urgese", "Stefano Baraldo", "Anna Valente"], "summary": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing\nworker well-being alongside mass customization. Stress-aware Human-Robot\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\nadapt their behavior to human mental states to improve collaboration fluency\nand safety. This paper presents a novel framework that integrates Federated\nLearning (FL) to enable personalized mental state evaluation while preserving\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\nEMG, and respiration, a multimodal model predicts an operator's stress level,\nfacilitating real-time robot adaptation. The FL-based approach allows\ndistributed on-device training, ensuring data confidentiality while improving\nmodel generalization and individual customization. Results demonstrate that the\ndeployment of an FL approach results in a global model with performance in\nstress prediction accuracy comparable to a centralized training approach.\nMoreover, FL allows for enhancing personalization, thereby optimizing\nhuman-robot interaction in industrial settings, while preserving data privacy.\nThe proposed framework advances privacy-preserving, adaptive robotics to\nenhance workforce well-being in smart manufacturing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20212v1", "categories": ["cs.RO", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20212v1", "AI": {"title_translation": "个性化人机交互中的心理状态评估：基于联邦学习", "tldr": "本文提出了一种基于联邦学习的新框架，用于在人机协作中个性化评估操作员的心理压力水平，同时保护用户隐私，并展示了其与集中式训练相当的性能和增强的个性化能力。", "motivation": "随着工业5.0的到来，制造商在实现大规模定制的同时，也越来越重视工人福祉。应激感知的人机协作（HRC）在这种范式中扮演着关键角色，因为机器人必须调整其行为以适应人类心理状态，从而提高协作流畅性和安全性。同时，保护用户隐私也是一个挑战。", "method": "本文提出了一种新颖的框架，该框架集成了联邦学习（FL）以实现个性化的心理状态评估，同时保护用户隐私。通过利用生理信号（包括EEG、ECG、EDA、EMG和呼吸），多模态模型预测操作员的压力水平，从而促进机器人实时适应。基于FL的方法允许分布式设备上训练，确保数据机密性，同时提高模型泛化能力和个体定制化。", "result": "结果表明，部署FL方法得到的全局模型在压力预测准确性方面与集中式训练方法相当。此外，FL允许增强个性化，从而在工业环境中优化人机交互，同时保护数据隐私。", "conclusion": "所提出的框架推动了隐私保护的自适应机器人技术，以增强智能制造中的劳动力福祉。", "translation": "随着工业5.0的到来，制造商在实现大规模定制的同时，也越来越重视工人福祉。应激感知的人机协作（HRC）在这种范式中扮演着关键角色，机器人必须调整其行为以适应人类心理状态，从而提高协作流畅性和安全性。本文提出了一种新颖的框架，该框架集成了联邦学习（FL），以实现个性化的心理状态评估，同时保护用户隐私。通过利用生理信号，包括脑电图（EEG）、心电图（（ECG）、皮肤电反应（EDA）、肌电图（EMG）和呼吸，多模态模型预测操作员的压力水平，从而促进机器人实时适应。基于FL的方法允许分布式设备上训练，确保数据机密性，同时提高模型泛化能力和个体定制化。结果表明，部署FL方法得到的全局模型在压力预测准确性方面与集中式训练方法相当。此外，FL允许增强个性化，从而在工业环境中优化人机交互，同时保护数据隐私。所提出的框架推动了隐私保护的自适应机器人技术，以增强智能制造中的劳动力福祉。", "summary": "本文提出了一种基于联邦学习（FL）的新框架，用于在人机协作中对操作员的心理状态进行个性化评估，同时保护用户隐私。该框架利用多模态生理信号（如EEG、ECG）预测压力水平，使机器人能实时适应。研究表明，FL方法在压力预测准确性上与集中式训练相当，并能增强个性化，优化工业人机交互，同时确保数据隐私，从而提升智能制造中的劳动力福祉。", "keywords": "联邦学习, 人机交互, 心理状态评估, 隐私保护, 工业5.0", "comments": "本文的创新点在于将联邦学习应用于人机交互中的心理状态评估，有效地解决了数据隐私和个性化适应的平衡问题。这对于提升工业5.0背景下的工人福祉和智能制造的效率具有重要意义。该框架通过分布式训练确保了数据机密性，同时实现了与集中式训练相当的性能，并增强了个性化能力，为未来自适应和隐私保护型机器人系统的发展奠定了基础。"}}
{"id": "2506.20551", "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling", "authors": ["Soumya Madireddy", "Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci", "Zhe Han", "Yunpeng Zhang"], "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20551v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20551v1", "AI": {"title_translation": "大语言模型驱动的建筑信息模型中代码合规性检查", "tldr": "本研究提出一种基于大语言模型（LLM）的方法，将LLM与Revit集成，以半自动化方式进行BIM中的代码合规性检查，旨在减少时间和错误，提高效率和准确性。", "motivation": "手动进行建筑信息模型（BIM）中的代码合规性检查耗时且容易出错。", "method": "本研究引入了一种由大语言模型（LLM）驱动的方法来半自动化BIM中的代码合规性检查。开发的系统集成了GPT、Claude、Gemini和Llama等LLM与Revit软件，用于解释建筑规范、生成Python脚本，并在BIM环境中执行半自动化合规性检查。", "result": "案例研究（包括单户住宅和办公楼项目）表明，该系统能够减少合规性检查所需的时间和精力，同时提高准确性。它通过自动评估关系并生成可操作的报告，简化了对不符合规范的房间尺寸、材料使用和物体放置等违规行为的识别。与手动方法相比，该系统消除了重复性任务，简化了复杂的法规，并确保了可靠的标准遵守。", "conclusion": "所提出的方法提供了一种全面、适应性强且经济高效的解决方案，在基于BIM的合规性检查方面取得了有前景的进展，并在建筑项目的各种监管文件中具有潜在应用。", "translation": "本研究通过引入一种由大语言模型（LLM）驱动的方法来半自动化这一关键过程，解决了建筑信息模型（BIM）中手动代码合规性检查耗时且容易出错的性质。开发的系统将GPT、Claude、Gemini和Llama等LLM与Revit软件集成，以解释建筑规范、生成Python脚本，并在BIM环境中执行半自动化合规性检查。针对单户住宅项目和办公楼项目的案例研究表明，该系统能够减少合规性检查所需的时间和精力，同时提高准确性。它通过自动评估关系和生成可操作的报告，简化了违规行为的识别，例如不符合规范的房间尺寸、材料使用和物体放置。与手动方法相比，该系统消除了重复性任务，简化了复杂的法规，并确保可靠地遵守标准。通过提供全面、适应性强且经济高效的解决方案，所提出的方法在基于BIM的合规性检查方面提供了有前景的进步，并在建筑项目的各种监管文件中具有潜在应用。", "summary": "本研究提出了一种创新性的基于大语言模型（LLM）的方法，旨在解决建筑信息模型（BIM）中手动代码合规性检查耗时且易错的问题。该系统将GPT、Claude、Gemini、Llama等LLM与Revit软件相结合，能够解释建筑规范、生成Python脚本并执行半自动化合规性检查。通过在住宅和办公楼项目中的案例研究，验证了该系统在减少检查时间、提高准确性方面的有效性，并能自动化识别违规行为，从而简化了复杂的法规遵守过程，为BIM领域的合规性检查提供了全面、适应性强且经济高效的解决方案。", "keywords": "大语言模型, 建筑信息模型, 代码合规性检查, 半自动化, Revit", "comments": "这项研究的创新之处在于将先进的大语言模型技术应用于传统的建筑信息模型（BIM）代码合规性检查，实现了半自动化，显著提高了效率和准确性。其重要性在于解决了建筑行业中长期存在的合规性检查效率低下的痛点，通过集成多种主流LLM和Revit软件，展现了强大的实用性和可扩展性。该方法有望改变建筑项目的合规性工作流程，降低人力成本和错误率。"}}
{"id": "2506.20152", "title": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration", "authors": ["Deepak Ghimire", "Kilho Lee", "Seong-heum Kim"], "summary": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20152v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20152v1", "AI": {"title_translation": "面向深度神经网络加速的损失感知结构化剪枝准则自动选择", "tldr": "本文提出LAASP，一种损失感知的训练时剪枝方法，用于深度神经网络的压缩和加速，通过自动选择剪枝准则和速率，实现优于现有技术的性能和FLOPs减少。", "motivation": "结构化剪枝是压缩神经网络以适应资源受限边缘设备的关键技术。现有大多数剪枝方法采用顺序三阶段（训练、剪枝、微调）过程，效率有待提高。", "method": "本文提出损失感知结构化剪枝准则自动选择（LAASP）方法。该方法采用“训练时剪枝”策略，取消了初始训练阶段，并将剪枝和微调集成到一个循环中。在每次剪枝迭代中，根据网络在训练数据子集上的总体损失，自动选择基于幅度或相似度的滤波器剪枝准则及具体的剪枝层。为缓解精度骤降，在每次减少预定FLOPs后进行短暂再训练。每层最佳剪枝率自动确定，无需手动分配。", "result": "在CIFAR-10和ImageNet数据集上对VGGNet和ResNet模型进行实验。ResNet56和ResNet110在CIFAR-10上相比SOTA方法显著提升Top-1准确率，同时FLOPs减少52%。ResNet50在ImageNet上FLOPs减少超过42%，Top-5准确率仅下降0.33%。", "conclusion": "所提出的LAASP方法能有效精简和加速深度神经网络，在ResNet模型上表现出优于现有技术的FLOPs减少和准确率提升或保持。", "translation": "结构化剪枝是一种成熟的神经网络压缩技术，使其适用于资源受限的边缘设备部署。本文提出了一种高效的损失感知结构化剪枝准则自动选择（LAASP）方法，用于精简和加速深度神经网络。大多数剪枝方法采用由三个阶段组成的顺序过程：1）训练，2）剪枝，和3）微调，而本文提出的剪枝技术采用“训练时剪枝”的方法，消除了第一阶段，并将第二和第三阶段整合到一个循环中。在每次剪枝迭代中，根据网络在训练数据的一个小小子集上的总体损失，自动从指定的准则池中选择基于幅度或相似度的滤波器剪枝准则以及特定的剪枝层。为了减轻由于剪枝导致的精度骤降，在每次减少预定数量的浮点运算（FLOPs）后，网络会进行短暂的再训练。网络中每一层的最佳剪枝率都是自动确定的，无需手动为每一层分配固定或可变剪枝率。在CIFAR-10和ImageNet基准数据集上对VGGNet和ResNet模型进行的实验证明了所提出方法的有效性。特别是，在CIFAR-10数据集上的ResNet56和ResNet110模型与最先进的方法相比，显著提高了Top-1准确率，同时将网络FLOPs减少了52%。此外，在ImageNet数据集上的ResNet50模型将FLOPs减少了42%以上，而Top-5准确率的下降可忽略不计，仅为0.33%。本文的源代码已在线公开 - https://github.com/ghimiredhikura/laasp。", "summary": "本文提出LAASP，一种用于深度神经网络压缩和加速的新型“训练时剪枝”方法。与传统顺序方法不同，LAASP集成了剪枝和微调，根据损失自动选择剪枝准则和层特定速率，并通过短暂再训练保持精度。在VGGNet和ResNet模型上的实验结果表明，LAASP在减少FLOPs（高达52%）的同时，相比现有技术提高了或保持了准确率。", "keywords": "结构化剪枝, 深度神经网络, 网络压缩, 自动剪枝, 训练时剪枝", "comments": "该论文的创新点在于其“训练时剪枝”范式以及损失感知的剪枝准则和速率的自动选择，这简化了流程并可能带来更好的性能。其自动确定每层最佳剪枝率的能力是一个显著优势，减少了对专业知识和繁琐超参数调整的需求。该方法在ResNet模型上的有效性，在显著减少FLOPs的同时提高或仅轻微降低了准确率，突显了其在资源受限设备上部署深度学习模型的实际重要性。"}}
{"id": "2506.19894", "title": "Explaining deep neural network models for electricity price forecasting with XAI", "authors": ["Antoine Pesenti", "Aidan OSullivan"], "summary": "Electricity markets are highly complex, involving lots of interactions and\ncomplex dependencies that make it hard to understand the inner workings of the\nmarket and what is driving prices. Econometric methods have been developed for\nthis, white-box models, however, they are not as powerful as deep neural\nnetwork models (DNN). In this paper, we use a DNN to forecast the price and\nthen use XAI methods to understand the factors driving the price dynamics in\nthe market. The objective is to increase our understanding of how different\nelectricity markets work. To do that, we apply explainable methods such as SHAP\nand Gradient, combined with visual techniques like heatmaps (saliency maps) to\nanalyse the behaviour and contributions of various features across five\nelectricity markets. We introduce the novel concepts of SSHAP values and SSHAP\nlines to enhance the complex representation of high-dimensional tabular models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19894v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19894v1", "AI": {"title_translation": "使用可解释人工智能解释深度神经网络模型在电力价格预测中的应用", "tldr": "本文使用深度神经网络预测电力价格，并利用可解释人工智能（XAI）方法，如SHAP和Gradient，结合可视化技术来理解电力市场中价格动态的驱动因素，并引入了SSHAP值和SSHAP线。", "motivation": "电力市场高度复杂，涉及大量互动和复杂依赖关系，导致难以理解市场内部运作及其价格驱动因素。传统的计量经济学白盒模型虽然有助于理解，但不如深度神经网络模型（DNN）强大。", "method": "研究首先使用深度神经网络（DNN）进行电力价格预测，然后应用可解释人工智能（XAI）方法来理解价格动态的驱动因素。具体方法包括使用SHAP和Gradient等可解释方法，并结合热力图（显著图）等可视化技术，分析五个电力市场中各种特征的行为和贡献。此外，论文还引入了SSHAP值和SSHAP线等新颖概念，以增强高维表格模型的复杂表示。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "电力市场高度复杂，涉及大量互动和复杂依赖关系，这使得理解市场内部运作以及价格驱动因素变得困难。为此，已经开发了计量经济学方法——白盒模型，然而，它们不如深度神经网络模型（DNN）强大。在本文中，我们使用DNN来预测价格，然后使用XAI方法来理解市场中驱动价格动态的因素。目标是增加我们对不同电力市场如何运作的理解。为此，我们应用了SHAP和Gradient等可解释方法，并结合热力图（显著图）等可视化技术，分析了五个电力市场中各种特征的行为和贡献。我们引入了SSHAP值和SSHAP线等新颖概念，以增强高维表格模型的复杂表示。", "summary": "本文旨在通过结合深度神经网络（DNN）和可解释人工智能（XAI）方法，提高对复杂电力市场价格动态的理解。研究利用DNN进行价格预测，并采用SHAP和Gradient等XAI技术，辅以热力图等可视化手段，分析了五个电力市场中影响价格的特征。论文还提出了SSHAP值和SSHAP线等新概念，以更好地解释高维表格模型。", "keywords": "电力价格预测, 深度神经网络, 可解释人工智能, SHAP, SSHAP", "comments": "本文的创新点在于将强大的深度神经网络模型应用于电力价格预测，并结合可解释人工智能（XAI）技术来揭示其决策过程，从而解决了传统黑盒模型的可解释性问题。引入SSHAP值和SSHAP线是其在XAI方法论上的重要贡献，有助于更有效地解释高维数据。这对于理解复杂且不透明的电力市场具有重要意义，有助于市场参与者和监管机构做出更明智的决策。"}}
{"id": "2506.20059", "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction", "authors": ["Weijieying Ren", "Tianxiang Zhao", "Lei Wang", "Tianchun Wang", "Vasant Honavar"], "summary": "Recent advances in Large Language Models (LLMs) have led to remarkable\nprogresses in medical consultation. However, existing medical LLMs overlook the\nessential role of Electronic Health Records (EHR) and focus primarily on\ndiagnosis recommendation, limiting their clinical applicability. We propose\nDiaLLM, the first medical LLM that integrates heterogeneous EHR data into\nclinically grounded dialogues, enabling clinical test recommendation, result\ninterpretation, and diagnosis prediction to better align with real-world\nmedical practice. To construct clinically grounded dialogues from EHR, we\ndesign a Clinical Test Reference (CTR) strategy that maps each clinical code to\nits corresponding description and classifies test results as \"normal\" or\n\"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for\nevidence acquisition and automated diagnosis. To handle the large action space,\nwe introduce a reject sampling strategy to reduce redundancy and improve\nexploration efficiency. Furthermore, a confirmation reward and a\nclass-sensitive diagnosis reward are designed to guide accurate diagnosis\nprediction. Extensive experimental results demonstrate that DiaLLM outperforms\nbaselines in clinical test recommendation and diagnosis prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20059v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20059v1", "AI": {"title_translation": "DiaLLMs：EHR增强的临床对话系统，用于临床测试推荐和诊断预测", "tldr": "DiaLLM是一个结合EHR数据的医学LLM，用于临床测试推荐、结果解读和诊断预测，通过强化学习和采样策略实现，并在实验中表现优于基线。", "motivation": "现有医学大语言模型（LLMs）在医疗咨询方面取得了进展，但它们忽视了电子健康记录（EHR）的关键作用，并且主要侧重于诊断推荐，这限制了其临床适用性，与现实世界的医疗实践不符。", "method": "本文提出了DiaLLM，这是首个将异构EHR数据整合到临床对话中的医学LLM。它设计了临床测试参考（CTR）策略，将临床代码映射到描述并分类测试结果。DiaLLM采用强化学习框架进行证据获取和自动化诊断，并引入拒绝采样策略处理大动作空间。此外，还设计了确认奖励和类别敏感诊断奖励以指导准确诊断预测。", "result": "广泛的实验结果表明，DiaLLM在临床测试推荐和诊断预测方面优于现有基线模型。", "conclusion": "Not mentioned in abstract.", "translation": "大型语言模型（LLMs）的最新进展在医疗咨询领域取得了显著进展。然而，现有医学LLMs忽视了电子健康记录（EHR）的关键作用，并且主要侧重于诊断推荐，这限制了它们的临床适用性。我们提出了DiaLLM，这是第一个将异构EHR数据整合到临床基础对话中的医学LLM，从而能够进行临床测试推荐、结果解读和诊断预测，以更好地与现实世界的医疗实践保持一致。为了从EHR构建临床基础对话，我们设计了一种临床测试参考（CTR）策略，该策略将每个临床代码映射到其相应的描述，并将测试结果分类为“正常”或“异常”。此外，DiaLLM采用强化学习框架进行证据获取和自动化诊断。为了处理大的动作空间，我们引入了一种拒绝采样策略，以减少冗余并提高探索效率。此外，还设计了确认奖励和类别敏感诊断奖励，以指导准确的诊断预测。广泛的实验结果表明，DiaLLM在临床测试推荐和诊断预测方面优于基线模型。", "summary": "DiaLLM是一个创新的医学大语言模型，它解决了现有医学LLMs忽视EHR数据和仅关注诊断推荐的局限性。DiaLLM通过整合异构EHR数据到临床对话中，实现了临床测试推荐、结果解读和诊断预测。该模型采用临床测试参考策略构建对话，并利用强化学习框架进行证据获取和诊断，同时引入拒绝采样策略和定制奖励机制来优化性能。实验证明DiaLLM在关键任务上超越了基线模型。", "keywords": "医学LLM, EHR, 临床对话系统, 诊断预测, 强化学习", "comments": "DiaLLM通过将EHR数据整合到医学LLM中，显著提高了模型在临床测试推荐、结果解读和诊断预测方面的能力，使其更符合实际医疗实践。其创新点在于结合了强化学习框架、CTR策略、拒绝采样以及定制奖励机制，有效处理了多任务和复杂数据。这对于提升医疗AI的实用性和准确性具有重要意义。"}}
{"id": "2506.19889", "title": "Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models", "authors": ["Wanli Peng", "Xin Chen", "Hang Fu", "XinYu He", "Xue Yiming", "Juan Wen"], "summary": "Recent advances in large language models (LLMs) have made a profound impact\non our society and also raised new security concerns. Particularly, due to the\nremarkable inference ability of LLMs, the privacy violation attack (PVA),\nrevealed by Staab et al., introduces serious personal privacy issues. Existing\ndefense methods mainly leverage LLMs to anonymize the input query, which\nrequires costly inference time and cannot gain satisfactory defense\nperformance. Moreover, directly rejecting the PVA query seems like an effective\ndefense method, while the defense method is exposed, promoting the evolution of\nPVA. In this paper, we propose a novel defense paradigm based on\nretrieval-confused generation (RCG) of LLMs, which can efficiently and covertly\ndefend the PVA. We first design a paraphrasing prompt to induce the LLM to\nrewrite the \"user comments\" of the attack query to construct a disturbed\ndatabase. Then, we propose the most irrelevant retrieval strategy to retrieve\nthe desired user data from the disturbed database. Finally, the \"data comments\"\nare replaced with the retrieved user data to form a defended query, leading to\nresponding to the adversary with some wrong personal attributes, i.e., the\nattack fails. Extensive experiments are conducted on two datasets and eight\npopular LLMs to comprehensively evaluate the feasibility and the superiority of\nthe proposed defense method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19889v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19889v1", "AI": {"title_translation": "检索混淆生成是大型语言模型隐私侵犯攻击的良好防御者", "tldr": "本文提出了一种基于检索混淆生成（RCG）的新型防御范式，以高效且隐蔽地防御大型语言模型（LLMs）的隐私侵犯攻击（PVA）。", "motivation": "大型语言模型（LLMs）的快速发展带来了新的安全问题，特别是隐私侵犯攻击（PVA）会引发严重的个人隐私泄露。现有防御方法耗时且效果不佳，或容易暴露防御机制，促使PVA演变。", "method": "本文提出了一种基于检索混淆生成（RCG）的新型防御范式。首先，设计一个释义提示来诱导LLM重写攻击查询的“用户评论”，从而构建一个扰动数据库。然后，提出“最不相关检索策略”从扰动数据库中检索所需的用户数据。最后，用检索到的用户数据替换“数据评论”以形成一个防御查询，从而使LLM对攻击者返回错误的个人属性。", "result": "在两个数据集和八个流行的大型语言模型上进行了广泛的实验，全面评估了所提出的防御方法的可行性和优越性。", "conclusion": "所提出的检索混淆生成（RCG）防御方法能够使大型语言模型的隐私侵犯攻击（PVA）失败，通过向攻击者返回错误的个人属性来保护用户隐私。", "translation": "大型语言模型（LLMs）的最新进展对我们的社会产生了深远影响，同时也引发了新的安全担忧。特别是，由于LLMs卓越的推理能力，由Staab等人揭示的隐私侵犯攻击（PVA）引入了严重的个人隐私问题。现有的防御方法主要利用LLMs匿名化输入查询，这需要耗时的推理时间，并且无法获得令人满意的防御性能。此外，直接拒绝PVA查询似乎是一种有效的防御方法，但这种防御方法一旦暴露，就会促进PVA的演变。在本文中，我们提出了一种基于LLMs检索混淆生成（RCG）的新型防御范式，该范式可以高效且隐蔽地防御PVA。我们首先设计一个释义提示来诱导LLM重写攻击查询的“用户评论”以构建一个扰动数据库。然后，我们提出了最不相关检索策略，从扰动数据库中检索所需的用户数据。最后，用检索到的用户数据替换“数据评论”以形成一个防御查询，从而导致向攻击者返回一些错误的个人属性，即攻击失败。在两个数据集和八个流行的大型语言模型上进行了广泛的实验，以全面评估所提出的防御方法的可行性和优越性。", "summary": "本文针对大型语言模型（LLMs）面临的隐私侵犯攻击（PVA）问题，提出了一种名为检索混淆生成（RCG）的新型防御范式。该方法通过设计释义提示来扰动用户评论并构建一个扰动数据库，接着采用最不相关检索策略从中检索数据，最终用检索到的错误数据替换原始数据，从而使LLM向攻击者返回错误的个人属性，导致攻击失败。实验证明了该方法的有效性和优越性。", "keywords": "大型语言模型, 隐私侵犯攻击, 检索混淆生成, 防御, 个人隐私", "comments": "本文提出了一种新颖且隐蔽的防御大型语言模型隐私侵犯攻击的方法。其创新点在于利用检索混淆生成，通过引入“扰动数据库”和“最不相关检索策略”来主动混淆攻击查询，而非简单地匿名化或拒绝。这种方法不仅提高了防御效率，还避免了防御机制的暴露，有望有效对抗PVA的演变，具有重要的实践意义。"}}
{"id": "2506.20291", "title": "A Literature Review on Simulation in Conversational Recommender Systems", "authors": ["Haoran Zhang", "Xin Zhao", "Jinze Chen", "Junpeng Guo"], "summary": "Conversational Recommender Systems (CRSs) have garnered attention as a novel\napproach to delivering personalized recommendations through multi-turn\ndialogues. This review developed a taxonomy framework to systematically\ncategorize relevant publications into four groups: dataset construction,\nalgorithm design, system evaluation, and empirical studies, providing a\ncomprehensive analysis of simulation methods in CRSs research. Our analysis\nreveals that simulation methods play a key role in tackling CRSs' main\nchallenges. For example, LLM-based simulation methods have been used to create\nconversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.\nDespite several challenges, such as dataset bias, the limited output\nflexibility of LLM-based simulations, and the gap between text semantic space\nand behavioral semantics, persist due to the complexity in Human-Computer\nInteraction (HCI) of CRSs, simulation methods hold significant potential for\nadvancing CRS research. This review offers a thorough summary of the current\nresearch landscape in this domain and identifies promising directions for\nfuture inquiry.", "comment": "6 pages, 1 figures, accepted as a poster for CSWIM 2025", "pdf_url": "http://arxiv.org/pdf/2506.20291v1", "categories": ["cs.HC", "cs.IR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20291v1", "AI": {"title_translation": "对话式推荐系统中模拟的文献综述", "tldr": "一篇关于对话式推荐系统（CRSs）中模拟方法的文献综述，系统地分析了模拟方法的作用，特别是基于大型语言模型（LLM）的方法，并指出了挑战和未来方向。", "motivation": "本综述旨在系统分析对话式推荐系统（CRSs）研究中的模拟方法，并理解它们在解决CRSs挑战中的作用。", "method": "该综述开发了一个分类框架，将相关出版物分为数据集构建、算法设计、系统评估和实证研究四个组，从而对CRSs研究中的模拟方法进行了全面分析。", "result": "分析显示，模拟方法在解决CRSs的主要挑战中发挥着关键作用。例如，基于大型语言模型（LLM）的模拟方法已被用于创建对话推荐数据、增强CRSs算法和评估CRSs。然而，数据集偏差、LLM模拟输出灵活性有限以及文本语义空间与行为语义之间的差距等挑战依然存在。", "conclusion": "尽管存在挑战，模拟方法在推进CRS研究方面仍具有巨大潜力。本综述全面总结了该领域的当前研究现状，并指明了未来有前景的研究方向。", "translation": "对话式推荐系统（CRSs）作为一种通过多轮对话提供个性化推荐的新方法，受到了广泛关注。本综述开发了一个分类框架，将相关出版物系统地分为四个组：数据集构建、算法设计、系统评估和实证研究，从而对CRSs研究中的模拟方法进行了全面分析。我们的分析表明，模拟方法在解决CRSs的主要挑战中发挥着关键作用。例如，基于大型语言模型（LLM）的模拟方法已被用于创建对话推荐数据、增强CRSs算法和评估CRSs。尽管由于CRSs中人机交互（HCI）的复杂性，数据集偏差、基于LLM的模拟输出灵活性有限以及文本语义空间与行为语义之间的差距等挑战依然存在，但模拟方法在推进CRS研究方面仍具有巨大潜力。本综述对该领域的当前研究现状进行了全面总结，并指明了未来有前景的研究方向。", "summary": "这篇文献综述系统地分析了对话式推荐系统（CRSs）中的模拟方法。文章建立了一个分类框架，将相关研究分为数据集构建、算法设计、系统评估和实证研究四类。分析表明，模拟方法在解决CRSs的主要挑战中发挥着关键作用，特别是基于大型语言模型（LLM）的模拟方法已被用于数据生成、算法改进和系统评估。尽管面临数据集偏差和LLM输出灵活性有限等挑战，模拟方法仍具有推动CRS研究的巨大潜力。该综述总结了当前研究现状并指出了未来研究方向。", "keywords": "对话式推荐系统, 模拟, LLM, 文献综述, 分类学", "comments": "这篇论文很重要，因为它为CRSs中的模拟方法提供了一个结构化的概述，这是一个快速发展的领域。其分类框架为组织现有研究提供了一个有价值的工具。对基于LLM的模拟作为关键领域的识别以及对其挑战（如偏见和语义鸿沟）的讨论尤其具有洞察力，指导未来的研究走向更健壮和灵活的模拟技术。"}}
{"id": "2506.20015", "title": "Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons", "authors": ["Dengyu Wu", "Jiechen Chen", "H. Vincent Poor", "Bipin Rajendran", "Osvaldo Simeone"], "summary": "Neuromorphic computing offers an energy-efficient alternative to conventional\ndeep learning accelerators for real-time time-series processing. However, many\nedge applications, such as wireless sensing and audio recognition, generate\nstreaming signals with rich spectral features that are not effectively captured\nby conventional leaky integrate-and-fire (LIF) spiking neurons. This paper\ninvestigates a wireless split computing architecture that employs\nresonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain\nsignals directly, eliminating the need for costly spectral pre-processing. By\nresonating at tunable frequencies, RF neurons extract time-localized spectral\nfeatures while maintaining low spiking activity. This temporal sparsity\ntranslates into significant savings in both computation and transmission\nenergy. Assuming an OFDM-based analog wireless interface for spike\ntransmission, we present a complete system design and evaluate its performance\non audio classification and modulation classification tasks. Experimental\nresults show that the proposed RF-SNN architecture achieves comparable accuracy\nto conventional LIF-SNNs and ANNs, while substantially reducing spike rates and\ntotal energy consumption during inference and communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20015v1", "categories": ["cs.LG", "cs.IT", "cs.NE", "math.IT"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20015v1", "AI": {"title_translation": "基于谐振放电神经元的神经形态无线分体计算", "tldr": "本文提出了一种基于谐振放电（RF）神经元的神经形态无线分体计算架构，用于直接处理时域信号，以实现低能耗的边缘应用，并在音频和调制分类任务上取得了与传统方法相当的精度，同时显著降低了能耗。", "motivation": "传统的泄漏积分放电（LIF）脉冲神经元在处理具有丰富频谱特征的流式信号时效率不高，而这些信号在无线传感和音频识别等边缘应用中很常见。现有的方法需要耗费大量的频谱预处理，这增加了能耗。", "method": "本文提出了一种神经形态无线分体计算架构，该架构采用谐振放电（RF）神经元，利用其振荡动力学直接处理时域信号，从而省去了昂贵的频谱预处理。RF神经元通过在可调频率下谐振来提取时间局部频谱特征，同时保持低脉冲活动。系统设计假设采用基于OFDM的模拟无线接口进行脉冲传输。", "result": "实验结果表明，所提出的RF-SNN架构在音频分类和调制分类任务上实现了与传统LIF-SNN和ANN相当的精度。同时，在推理和通信过程中，脉冲速率和总能耗显著降低。", "conclusion": "基于谐振放电（RF）神经元的神经形态无线分体计算架构能够有效处理时域信号，在实现高精度的同时，显著降低了边缘应用的能耗和通信成本，为实时时间序列处理提供了一种高效的替代方案。", "translation": "神经形态计算为实时时间序列处理提供了一种比传统深度学习加速器更节能的替代方案。然而，许多边缘应用，如无线传感和音频识别，会产生具有丰富频谱特征的流信号，而传统的泄漏积分放电（LIF）脉冲神经元无法有效捕获这些特征。本文研究了一种无线分体计算架构，该架构采用具有振荡动力学的谐振放电（RF）神经元直接处理时域信号，从而无需进行昂贵的频谱预处理。通过在可调频率下谐振，RF神经元可以在保持低脉冲活动的同时提取时间局部频谱特征。这种时间稀疏性显著节省了计算和传输能量。假设采用基于OFDM的模拟无线接口进行脉冲传输，我们提出了一个完整的系统设计，并评估了其在音频分类和调制分类任务上的性能。实验结果表明，所提出的RF-SNN架构实现了与传统LIF-SNN和ANN相当的精度，同时在推理和通信过程中显著降低了脉冲速率和总能耗。", "summary": "本文提出了一种新颖的神经形态无线分体计算架构，该架构利用谐振放电（RF）神经元直接处理时域信号，避免了传统方法中昂贵的频谱预处理。RF神经元能有效提取时间局部频谱特征并保持低脉冲活动，从而显著降低计算和传输能耗。在音频和调制分类任务上的实验表明，该RF-SNN架构在保持与现有技术相当的精度的同时，大幅减少了脉冲速率和总能耗。", "keywords": "神经形态计算, 谐振放电神经元, 无线分体计算, 能量效率, 时间序列处理", "comments": "本文的创新点在于引入了谐振放电（RF）神经元来直接处理时域信号，有效解决了传统LIF神经元在处理具有丰富频谱特征的流信号时的局限性，并消除了频谱预处理的需求。这种方法通过时间稀疏性显著降低了计算和通信能耗，对于边缘设备的实时时间序列处理具有重要意义。"}}
{"id": "2506.20000", "title": "Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing", "authors": ["Narasimha Raghavan Veeraragavan", "Jan Franz Nygård"], "summary": "We propose Guardian-FC, a novel two-layer framework for privacy preserving\nfederated computing that unifies safety enforcement across diverse privacy\npreserving mechanisms, including cryptographic back-ends like fully homomorphic\nencryption (FHE) and multiparty computation (MPC), as well as statistical\ntechniques such as differential privacy (DP). Guardian-FC decouples guard-rails\nfrom privacy mechanisms by executing plug-ins (modular computation units),\nwritten in a backend-neutral, domain-specific language (DSL) designed\nspecifically for federated computing workflows and interchangeable Execution\nProviders (EPs), which implement DSL operations for various privacy back-ends.\nAn Agentic-AI control plane enforces a finite-state safety loop through signed\ntelemetry and commands, ensuring consistent risk management and auditability.\nThe manifest-centric design supports fail-fast job admission and seamless\nextensibility to new privacy back-ends. We present qualitative scenarios\nillustrating backend-agnostic safety and a formal model foundation for\nverification. Finally, we outline a research agenda inviting the community to\nadvance adaptive guard-rail tuning, multi-backend composition, DSL\nspecification development, implementation, and compiler extensibility alongside\nhuman-override usability.", "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows (CFAgentic@ICML'25)", "pdf_url": "http://arxiv.org/pdf/2506.20000v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20000v1", "AI": {"title_translation": "一个安全循环能否守护所有？联邦计算的代理式护栏", "tldr": "提出Guardian-FC框架，一个统一的联邦计算安全护栏，可跨不同隐私机制（如FHE、MPC、DP）工作，通过解耦和Agentic-AI控制平面实现。", "motivation": "在隐私保护联邦计算中，需要一个统一的框架来跨不同的隐私保护机制（如FHE、MPC、DP）强制执行安全策略。", "method": "本文提出了Guardian-FC，一个新颖的两层框架。它通过执行后端中立的领域特定语言（DSL）编写的插件，并利用可互换的执行提供商（EPs）来实现各种隐私后端，从而将安全护栏与隐私机制解耦。一个Agentic-AI控制平面通过有符号遥测和命令执行一个有限状态安全循环，以确保一致的风险管理和可审计性。", "result": "论文提供了定性场景以说明后端无关的安全性，并为验证提供了形式化模型基础。", "conclusion": "论文提出了一个研究议程，邀请社区推进自适应护栏调优、多后端组合、DSL规范开发、实现和编译器可扩展性以及人工覆盖可用性。", "translation": "我们提出了Guardian-FC，一个新颖的两层隐私保护联邦计算框架，它统一了跨不同隐私保护机制的安全强制执行，包括全同态加密（FHE）和多方计算（MPC）等密码学后端，以及差分隐私（DP）等统计技术。Guardian-FC通过执行插件（模块化计算单元）来解耦护栏与隐私机制，这些插件以一种为联邦计算工作流专门设计的、后端中立的领域特定语言（DSL）编写，并由可互换的执行提供商（EPs）实现各种隐私后端。一个Agentic-AI控制平面通过有符号遥测和命令执行一个有限状态安全循环，确保一致的风险管理和可审计性。以清单为中心的设计支持快速失败的作业准入和对新隐私后端的无缝扩展性。我们提出了定性场景，说明了后端无关的安全性，并为验证提供了形式化模型基础。最后，我们概述了一个研究议程，邀请社区推进自适应护栏调优、多后端组合、DSL规范开发、实现和编译器可扩展性以及人工覆盖可用性。", "summary": "该论文提出了Guardian-FC，一个用于隐私保护联邦计算的两层框架，旨在统一不同隐私机制（如FHE、MPC、DP）的安全强制执行。它通过解耦护栏与隐私机制，利用后端中立的DSL插件和可互换的执行提供商实现。一个Agentic-AI控制平面负责执行安全循环以确保风险管理和可审计性。该框架支持快速失败的作业准入和扩展性，并提供了形式化模型基础，同时提出了未来的研究方向。", "keywords": "联邦计算, 安全护栏, 隐私保护, Agentic-AI, Guardian-FC", "comments": "Guardian-FC的创新之处在于其两层框架设计，特别是将安全护栏与底层隐私机制解耦，并通过一个后端中立的DSL和Agentic-AI控制平面实现统一的安全管理。这对于日益复杂的联邦学习生态系统具有重要意义，因为它提高了系统的灵活性、可审计性和可扩展性，能够适应多种隐私技术。"}}
{"id": "2506.20243", "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "authors": ["Papa Séga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2506.20243v1", "categories": ["cs.CL", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20243v1", "AI": {"title_translation": "CBF-AFA：基于分块的多自监督学习融合的自动流利度评估", "tldr": "本文介绍了CBF-AFA，一种基于分块的多自监督学习（SSL）融合方法，结合CNN-BiLSTM框架，显著提升了自动流利度评估的性能，超越了现有基线。", "motivation": "自动流利度评估（AFA）仍具挑战性，尤其是在捕捉非母语使用者的语音节奏、停顿和不流畅方面。", "method": "本文提出了一种名为CBF-AFA的基于分块的多自监督学习（SSL）融合方法。该方法整合了Wav2Vec2、HuBERT和WavLM等SSL模型，这些模型因其在语音、韵律和噪声语音建模方面的互补优势而被选中。语音使用Silero语音活动检测（Silero-VAD）被分割成呼吸组块，以实现细粒度时间分析并减少过度分割伪影。SSL嵌入通过可学习的加权机制进行融合，平衡声学和语言特征，并补充了块级流利度标记（如语速、停顿时长、n-gram重复）。最后，采用分层CNN-BiLSTM框架来捕获跨块的局部和长期依赖关系。", "result": "在Avalinguo和Speechocean762数据集上进行评估，与Speechocean762上的单一SSL基线相比，我们的方法F1分数提高了2.8，皮尔逊相关系数提高了6.2。在Avalinguo上，F1分数提高了4.2，皮尔逊相关系数提高了4.0。该方法超越了基于Pyannote.audio的分割基线。", "conclusion": "这些发现强调了基于分块的多自监督学习融合在稳健流利度评估中的有效性，尽管未来的工作应探索其对韵律不规则方言的泛化能力。", "translation": "自动流利度评估（AFA）仍然具有挑战性，特别是在捕获非母语使用者的语音节奏、停顿和不流畅方面。我们引入了一种基于分块的方法，该方法集成了自监督学习（SSL）模型（Wav2Vec2、HuBERT和WavLM），这些模型因其在语音、韵律和噪声语音建模方面的互补优势而被选中，并采用分层CNN-BiLSTM框架。语音使用Silero语音活动检测（Silero-VAD）分割成呼吸组块，从而实现细粒度时间分析，同时减轻过度分割伪影。SSL嵌入通过可学习的加权机制进行融合，平衡声学和语言特征，并富含块级流利度标记（例如，语速、停顿时长、n-gram重复）。CNN-BiLSTM捕获块之间的局部和长期依赖关系。在Avalinguo和Speechocean762上进行评估，我们的方法在Speechocean762上比单一SSL基线提高了2.8的F1分数和6.2的皮尔逊相关系数，在Avalinguo上获得了4.2的F1分数和4.0的皮尔逊点数，超过了基于Pyannote.audio的分割基线。这些发现强调了基于分块的多SSL融合在稳健流利度评估中的作用，尽管未来的工作应探索其对韵律不规则方言的泛化。", "summary": "本文提出了一种新颖的基于分块的自动流利度评估方法CBF-AFA。该方法利用分层CNN-BiLSTM框架，融合了Wav2Vec2、HuBERT和WavLM等多个自监督学习（SSL）模型，以利用它们在语音、韵律和噪声语音建模上的互补优势。语音被分割成呼吸组块，并且SSL嵌入与块级流利度标记融合。在Avalinguo和Speechocean762数据集上的实验表明，与单一SSL和Pyannote.audio基线相比，CBF-AFA显著提高了性能，证明了基于分块的多SSL融合在流利度评估中的鲁棒性。", "keywords": "自动流利度评估, 自监督学习, 语音分块, 多SSL融合, CNN-BiLSTM", "comments": "本文的创新之处在于其基于分块的多自监督学习融合方法，该方法巧妙地利用了不同SSL模型的互补优势，并整合了细粒度的流利度标记。采用呼吸组块和可学习的加权融合机制是其关键优势。论文也坦诚地指出了其在泛化到韵律不规则方言方面的局限性，为未来的研究指明了方向。"}}
{"id": "2506.20442", "title": "When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact", "authors": ["Tianyao Shi", "Ritbik Kumar", "Inez Hua", "Yi Ding"], "summary": "Biodiversity loss is a critical planetary boundary, yet its connection to\ncomputing remains largely unexamined. Prior sustainability efforts in computing\nhave focused on carbon and water, overlooking biodiversity due to the lack of\nappropriate metrics and modeling frameworks. This paper presents the first\nend-to-end analysis of biodiversity impact from computing systems. We introduce\ntwo new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity\nIndex (OBI)--to quantify biodiversity impact across the lifecycle, and present\nFABRIC, a modeling framework that links computing workloads to biodiversity\nimpacts. Our evaluation highlights the need to consider biodiversity alongside\ncarbon and water in sustainable computing design and optimization. The code is\navailable at https://github.com/TianyaoShi/FABRIC.", "comment": "Accepted by HotCarbon' 25", "pdf_url": "http://arxiv.org/pdf/2506.20442v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20442v1", "AI": {"title_translation": "当服务器遇到物种：计算对生物多样性影响的全生命周期视角", "tldr": "本文首次对计算系统对生物多样性的影响进行了端到端分析，引入了新的衡量指标（EBI和OBI）和建模框架（FABRIC），强调在可持续计算设计中需同时考虑生物多样性、碳和水。", "motivation": "生物多样性丧失是一个关键的地球边界，但其与计算领域的联系却在很大程度上未被研究。先前的计算可持续性努力主要集中在碳和水上，由于缺乏适当的指标和建模框架，生物多样性被忽视。", "method": "本文提出了计算系统对生物多样性影响的首次端到端分析。引入了两个新指标：具身生物多样性指数（EBI）和操作生物多样性指数（OBI），用于量化整个生命周期的生物多样性影响。提出了一个名为FABRIC的建模框架，将计算工作负载与生物多样性影响联系起来。", "result": "评估结果强调，在可持续计算设计和优化中，需要将生物多样性与碳和水一同考虑。", "conclusion": "在可持续计算设计和优化中，考虑生物多样性与碳和水同等重要。", "translation": "生物多样性丧失是一个关键的地球边界，但其与计算领域的联系却在很大程度上未被研究。先前的计算可持续性努力主要集中在碳和水上，由于缺乏适当的指标和建模框架，生物多样性被忽视。本文首次对计算系统对生物多样性影响进行了端到端分析。我们引入了两个新指标——具身生物多样性指数（EBI）和操作生物多样性指数（OBI）——用于量化整个生命周期的生物多样性影响，并提出了FABRIC，一个将计算工作负载与生物多样性影响联系起来的建模框架。我们的评估强调，在可持续计算设计和优化中，需要将生物多样性与碳和水一同考虑。代码可在 https://github.com/TianyaoShi/FABRIC 获取。", "summary": "本文首次全面分析了计算系统对生物多样性的影响，填补了现有可持续计算研究主要关注碳和水而忽视生物多样性的空白。研究引入了具身生物多样性指数（EBI）和操作生物多样性指数（OBI）两个新指标，并开发了FABRIC建模框架来量化和关联计算活动与生物多样性影响。研究结果表明，在未来可持续计算的设计与优化中，生物多样性应与碳和水一样受到重视。", "keywords": "生物多样性, 可持续计算, 生态影响, EBI, OBI, FABRIC", "comments": "本文的创新之处在于首次将计算系统的影响扩展到生物多样性领域，并通过引入新的量化指标和建模框架，为这一复杂问题提供了可行的分析工具。其重要性在于拓宽了可持续计算的边界，促使行业在追求绿色计算时考虑更全面的生态影响。该研究可能面临的挑战是如何获取准确的生物多样性数据并将其与计算活动进行精细关联。"}}
{"id": "2506.20334", "title": "Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design", "authors": ["Daniele Ravasio", "Marcello Farina", "Alessio La Bella", "Andrea Ballarino"], "summary": "This paper investigates the design of output-feedback schemes for systems\ndescribed by a class of recurrent neural networks. We propose a procedure based\non linear matrix inequalities for designing an observer and a static\nstate-feedback controller. The algorithm leverages global and regional\nincremental input-to-state stability (incremental ISS) and enables the tracking\nof constant setpoints, ensuring robustness to disturbances and state estimation\nuncertainty. To address the potential limitations of regional incremental ISS,\nwe introduce an alternative scheme in which the static law is replaced with a\ntube-based nonlinear model predictive controller (NMPC) that exploits regional\nincremental ISS properties. We show that these conditions enable the\nformulation of a robust NMPC law with guarantees of convergence and recursive\nfeasibility, leading to an enlarged region of attraction. Theoretical results\nare validated through numerical simulations on the pH-neutralisation process\nbenchmark, demonstrating the effectiveness of the proposed schemes.", "comment": "16 pages, 7 figures, submitted to IEEE Transactions on Automatic\n  Control (under review)", "pdf_url": "http://arxiv.org/pdf/2506.20334v1", "categories": ["eess.SY", "cs.LG", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20334v1", "AI": {"title_translation": "循环神经网络鲁棒控制系统，具有闭环区域增量ISS和MPC设计应用", "tldr": "本文提出基于线性矩阵不等式的方法，为循环神经网络系统设计输出反馈控制器，利用增量ISS实现对常数设定点的跟踪，并引入基于管的非线性模型预测控制器以扩大吸引域。", "motivation": "为一类由循环神经网络描述的系统设计输出反馈方案，解决传统增量ISS的潜在局限性，并确保对扰动和状态估计不确定性的鲁棒性，以及实现对常数设定点的跟踪。", "method": "本文提出了一种基于线性矩阵不等式（LMI）的方法来设计观测器和静态状态反馈控制器，该算法利用全局和区域增量输入-状态稳定性（incremental ISS）。为了解决区域增量ISS的潜在局限性，引入了一种替代方案，将静态控制律替换为利用区域增量ISS特性的基于管的非线性模型预测控制器（NMPC）。理论结果通过pH中和过程基准的数值模拟得到验证。", "result": "所提出的方案能够跟踪常数设定点，并确保对扰动和状态估计不确定性的鲁棒性。基于NMPC的替代方案能够制定鲁棒的NMPC律，保证收敛性和递归可行性，并能扩大吸引域。数值模拟验证了所提出方案的有效性。", "conclusion": "本文成功设计了基于循环神经网络的鲁棒输出反馈控制系统，通过结合增量ISS和NMPC，实现了对设定点的精确跟踪、鲁棒性以及扩大了吸引域，并通过仿真验证了其有效性。", "translation": "本文研究了一类由循环神经网络描述的系统的输出反馈方案设计。我们提出了一种基于线性矩阵不等式的方法来设计观测器和静态状态反馈控制器。该算法利用全局和区域增量输入-状态稳定性（incremental ISS），并能够跟踪常数设定点，确保对扰动和状态估计不确定性的鲁棒性。为了解决区域增量ISS的潜在局限性，我们引入了一种替代方案，其中静态控制律被替换为利用区域增量ISS特性的基于管的非线性模型预测控制器（NMPC）。我们表明，这些条件使得能够制定一个具有收敛性和递归可行性保证的鲁棒NMPC律，从而扩大了吸引域。理论结果通过pH中和过程基准的数值模拟得到验证，证明了所提出方案的有效性。", "summary": "本文针对一类循环神经网络系统，提出了一种基于线性矩阵不等式的输出反馈控制器设计方法。该方法利用增量输入-状态稳定性，实现了对常数设定点的跟踪和对扰动及状态估计不确定性的鲁棒性。为克服区域增量ISS的局限，文中进一步引入了基于管的非线性模型预测控制器（NMPC），该NMPC利用区域增量ISS特性，保证了收敛性和递归可行性，并显著扩大了系统的吸引域。通过pH中和过程的数值仿真验证了所提方案的有效性。", "keywords": "循环神经网络, 鲁棒控制, 增量ISS, 模型预测控制, 线性矩阵不等式", "comments": "该论文提出了一种创新的方法，将循环神经网络与增量ISS和MPC相结合，解决了非线性系统鲁棒控制中的关键问题。特别是引入NMPC来克服区域增量ISS的局限性并扩大吸引域，是其重要贡献。在实际应用中，这种方法对于处理具有不确定性和外部扰动的复杂非线性系统具有重要意义。"}}
{"id": "2506.20050", "title": "Near-Field SWIPT Using XL-MIMO: Power Allocation and Subarray Activation", "authors": ["Muhammad Zeeshan Mumtaz", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "summary": "This paper investigates the simultaneous wireless information and power\ntransfer (SWIPT) capability of a modular extremely large multiple-input\nmultiple-output (XL-MIMO) system, in the context of power consumption (PC)\nefficiency. The network users are divided into two functional categories:\ninformation decoding (ID) users and energy harvesting (EH) users.\nNon-stationary near-field channels are considered whilst the users are located\nin spatially distinct visibility regions (VRs). We formulate a two-tier joint\noptimization problem to minimize the PC, taking into account the power\nallocation (PA) for ID and EH users, along with the activation of constituent\nXL-MIMO subarrays. This complicated mixed-integer problem is transformed into\nmore tractable formulations and efficient algorithms are proposed for solving\nthem. The numerical results demonstrate that the overall PC of the XL-MIMO\nsystem for the proposed method is reduced by more than 60% in comparison to the\nbenchmark scheme of equal PA with full subarray activation (SA) and 30% against\nthe case of optimized PA with full SA, while satisfying the quality-of-service\n(QoS) constraints on both the downlink rate of the ID users and harvested\nenergy at the EH users.", "comment": "Presented at IEEE International Conference on Communications (ICC)\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.20050v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20050v1", "AI": {"title_translation": "使用XL-MIMO的近场SWIPT：功率分配和子阵列激活", "tldr": "本文研究了模块化XL-MIMO系统中近场SWIPT的功耗效率优化问题，通过联合优化功率分配和子阵列激活，显著降低了系统功耗。", "motivation": "旨在提高模块化XL-MIMO系统中同步无线信息和功率传输（SWIPT）的功耗效率。", "method": "考虑非平稳近场信道和空间上不同的可见区域用户，将问题建模为一个两层联合优化问题，以最小化功耗，同时考虑ID和EH用户的功率分配以及XL-MIMO子阵列的激活。将复杂的混合整数问题转化为更易处理的形式，并提出了有效的算法进行求解。", "result": "相比于基准的相等功率分配和全子阵列激活方案，所提出方法使XL-MIMO系统的总功耗降低了60%以上；相比于优化功率分配和全子阵列激活方案，降低了30%，同时满足了ID用户的下行速率和EH用户的能量收集QoS约束。", "conclusion": "通过联合优化功率分配和子阵列激活，可以在满足服务质量要求的同时，显著降低XL-MIMO系统近场SWIPT的功耗。", "translation": "这篇论文研究了模块化超大MIMO（XL-MIMO）系统在功耗效率背景下的同步无线信息与功率传输（SWIPT）能力。网络用户被分为两类功能：信息解码（ID）用户和能量收集（EH）用户。论文考虑了非平稳近场信道，同时用户位于空间上不同的可见区域。我们建立了一个两层联合优化问题，以最小化功耗，同时考虑了ID和EH用户的功率分配以及组成XL-MIMO子阵列的激活。这个复杂的混合整数问题被转化为更易处理的公式，并提出了有效的算法来解决它们。数值结果表明，与基准的等功率分配和全子阵列激活方案相比，所提出方法的XL-MIMO系统总功耗降低了60%以上；与优化功率分配和全子阵列激活方案相比，降低了30%，同时满足了ID用户的下行速率和EH用户的能量收集的服务质量（QoS）约束。", "summary": "本文探讨了模块化XL-MIMO系统在近场环境下进行SWIPT时的功耗效率优化。研究将用户分为信息解码和能量收集两类，并考虑非平稳近场信道。通过构建一个两层联合优化问题，该研究旨在最小化系统功耗，同时优化功率分配和XL-MIMO子阵列的激活。数值结果表明，所提出的方法在满足QoS要求的前提下，能显著降低系统功耗，相较于基准方案有超过60%的改善。", "keywords": "SWIPT, XL-MIMO, 功率分配, 子阵列激活, 近场通信", "comments": "本文的创新点在于将功率分配与XL-MIMO子阵列激活相结合，以优化近场SWIPT系统的功耗效率。考虑到XL-MIMO在未来通信中的潜力，这种功耗优化对于实际部署具有重要意义。通过将复杂的混合整数问题转化为易于处理的形式，并提出有效算法，也展示了较好的技术深度。"}}
{"id": "2506.20282", "title": "Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration", "authors": ["Jiaxing Huang", "Heng Guo", "Le Lu", "Fan Yang", "Minfeng Xu", "Ge Yang", "Wei Luo"], "summary": "Osteoporosis, characterized by reduced bone mineral density (BMD) and\ncompromised bone microstructure, increases fracture risk in aging populations.\nWhile dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD\nassessment, its limited accessibility hinders diagnosis in resource-limited\nregions. Opportunistic computed tomography (CT) analysis has emerged as a\npromising alternative for osteoporosis diagnosis using existing imaging data.\nCurrent approaches, however, face three limitations: (1) underutilization of\nunlabeled vertebral data, (2) systematic bias from device-specific DXA\ndiscrepancies, and (3) insufficient integration of clinical knowledge such as\nspatial BMD distribution patterns. To address these, we propose a unified deep\nlearning framework with three innovations. First, a self-supervised learning\nmethod using radiomic representations to leverage unlabeled CT data and\npreserve bone texture. Second, a Mixture of Experts (MoE) architecture with\nlearned gating mechanisms to enhance cross-device adaptability. Third, a\nmulti-task learning framework integrating osteoporosis diagnosis, BMD\nregression, and vertebra location prediction. Validated across three clinical\nsites and an external hospital, our approach demonstrates superior\ngeneralizability and accuracy over existing methods for opportunistic\nosteoporosis screening and diagnosis.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20282v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20282v1", "AI": {"title_translation": "通过纹理保留自监督、专家混合和多任务集成实现机会性骨质疏松症诊断", "tldr": "提出一种统一的深度学习框架，通过自监督、专家混合和多任务学习，利用CT数据进行机会性骨质疏松症诊断，解决了现有方法在数据利用、设备偏差和临床知识集成方面的局限性，并表现出卓越的泛化性和准确性。", "motivation": "现有的DXA骨密度评估方法可及性有限，而机会性CT分析虽有潜力但面临三个局限：1）未充分利用未标记的椎骨数据；2）设备特异性DXA差异导致的系统性偏差；3）临床知识（如空间骨密度分布模式）整合不足。", "method": "提出一个统一的深度学习框架，包含三项创新：1）利用影像组学表示的自监督学习方法，以利用未标记CT数据并保留骨纹理；2）具有学习门控机制的专家混合（MoE）架构，以增强跨设备适应性；3）整合骨质疏松症诊断、骨密度回归和椎体位置预测的多任务学习框架。", "result": "该方法在三个临床站点和一个外部医院进行了验证，在机会性骨质疏松症筛查和诊断方面，表现出优于现有方法的泛化性和准确性。", "conclusion": "该研究成功开发了一个统一的深度学习框架，通过解决现有方法的局限性，显著提升了机会性CT分析在骨质疏松症诊断中的性能和实用性。", "translation": "骨质疏松症以骨矿物质密度（BMD）降低和骨微结构受损为特征，增加了老年人群的骨折风险。虽然双能X射线吸收法（DXA）是BMD评估的临床标准，但其有限的可及性阻碍了资源匮乏地区的诊断。机会性计算机断层扫描（CT）分析已成为利用现有影像数据进行骨质疏松症诊断的一种有前景的替代方案。然而，现有方法面临三个局限性：（1）未充分利用未标记的椎骨数据，（2）来自设备特异性DXA差异的系统性偏差，以及（3）临床知识（如空间BMD分布模式）整合不足。为解决这些问题，我们提出了一个统一的深度学习框架，包含三项创新。首先，一种使用影像组学表示的自监督学习方法，以利用未标记的CT数据并保留骨纹理。其次，一种具有学习门控机制的专家混合（MoE）架构，以增强跨设备适应性。第三，一个整合骨质疏松症诊断、BMD回归和椎体位置预测的多任务学习框架。我们的方法在三个临床站点和一个外部医院进行了验证，在机会性骨质疏松症筛查和诊断方面，表现出优于现有方法的泛化性和准确性。", "summary": "这篇论文提出了一个创新的深度学习框架，旨在通过机会性CT分析改进骨质疏松症的诊断。该框架通过整合纹理保留自监督学习、专家混合架构和多任务学习，克服了现有方法在利用未标记数据、处理设备偏差和整合临床知识方面的局限。实验结果表明，该方法在多个临床站点上表现出卓越的泛化性和诊断准确性。", "keywords": "骨质疏松症诊断, 机会性CT分析, 自监督学习, 专家混合, 多任务学习", "comments": "这篇论文通过结合自监督学习、专家混合和多任务学习，为机会性骨质疏松症诊断提供了一个全面的深度学习解决方案。其创新点在于有效利用了未标记数据、增强了跨设备适应性并整合了多维度临床信息，显著提升了诊断的准确性和泛化能力，对于资源受限地区的骨质疏松症早期筛查具有重要意义。"}}
{"id": "2506.19998", "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "summary": "REST APIs play important roles in enriching the action space of web agents,\nyet most API-based agents rely on curated and uniform toolsets that do not\nreflect the complexity of real-world APIs. Building tool-using agents for\narbitrary domains remains a major challenge, as it requires reading\nunstructured API documentation, testing APIs and inferring correct parameters.\nWe propose Doc2Agent, a scalable pipeline to build agents that can call\nPython-based tools generated from API documentation. Doc2Agent generates\nexecutable tools from API documentations and iteratively refines them using a\ncode agent. We evaluate our approach on real-world APIs, WebArena APIs, and\nresearch APIs, producing validated tools. We achieved a 55\\% relative\nperformance improvement with 90\\% lower cost compared to direct API calling on\nWebArena benchmark. A domain-specific agent built for glycomaterial science\nfurther demonstrates the pipeline's adaptability to complex, knowledge-rich\ntasks. Doc2Agent offers a generalizable solution for building tool agents from\nunstructured API documentation at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19998v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.19998v1", "AI": {"title_translation": "Doc2Agent：从API文档可扩展地生成工具使用型智能体", "tldr": "Doc2Agent提出了一种可扩展的流程，可以从API文档中自动生成和优化工具使用型智能体，显著提高了性能并降低了成本。", "motivation": "现有的API智能体依赖于精心策划和统一的工具集，无法反映真实世界API的复杂性。为任意领域构建工具使用型智能体仍然是一个重大挑战，因为它需要阅读非结构化的API文档、测试API并推断正确的参数。", "method": "我们提出了Doc2Agent，一个可扩展的流程，用于构建能够调用从API文档生成的基于Python的工具的智能体。Doc2Agent从API文档生成可执行工具，并使用代码智能体迭代地对其进行优化。", "result": "我们在真实世界的API、WebArena API和研究API上评估了我们的方法，生成了经过验证的工具。与在WebArena基准测试上直接调用API相比，我们实现了55%的相对性能提升，同时成本降低了90%。为糖材料科学构建的领域特定智能体进一步证明了该流程对复杂、知识密集型任务的适应性。", "conclusion": "Doc2Agent为从非结构化API文档大规模构建工具智能体提供了一个通用解决方案。", "translation": "REST API在丰富网络智能体的动作空间方面发挥着重要作用，但大多数基于API的智能体依赖于精心策划和统一的工具集，无法反映真实世界API的复杂性。为任意领域构建工具使用型智能体仍然是一个重大挑战，因为它需要阅读非结构化的API文档、测试API并推断正确的参数。我们提出了Doc2Agent，一个可扩展的流程，用于构建能够调用从API文档生成的基于Python的工具的智能体。Doc2Agent从API文档生成可执行工具，并使用代码智能体迭代地对其进行优化。我们在真实世界的API、WebArena API和研究API上评估了我们的方法，生成了经过验证的工具。与在WebArena基准测试上直接调用API相比，我们实现了55%的相对性能提升，同时成本降低了90%。为糖材料科学构建的领域特定智能体进一步证明了该流程对复杂、知识密集型任务的适应性。Doc2Agent为从非结构化API文档大规模构建工具智能体提供了一个通用解决方案。", "summary": "Doc2Agent提出了一种可扩展的流水线，能够从非结构化API文档中自动生成和迭代优化Python工具，进而构建能够使用这些工具的智能体。该方法在真实世界和WebArena API上表现出色，相较于直接API调用，在WebArena基准测试上实现了55%的性能提升和90%的成本降低。此外，它在特定领域（如糖材料科学）也展现了强大的适应性，为大规模构建工具使用型智能体提供了一种通用且高效的解决方案。", "keywords": "Doc2Agent, API智能体, 工具生成, 可扩展性, 文档解析", "comments": "Doc2Agent的创新之处在于其能够从非结构化API文档中自动生成和优化可执行工具，极大地降低了构建复杂API智能体的门槛。其迭代优化机制和在实际应用中的显著性能提升（包括成本效益）使其成为一个非常重要的贡献。该方法的可扩展性和通用性预示着未来在自动化智能体构建方面具有广阔的应用前景。"}}
{"id": "2506.20367", "title": "DreamAnywhere: Object-Centric Panoramic 3D Scene Generation", "authors": ["Edoardo Alberto Dominici", "Jozef Hladky", "Floor Verhoeven", "Lukas Radl", "Thomas Deixelberger", "Stefan Ainetter", "Philipp Drescher", "Stefan Hauswiesner", "Arno Coomans", "Giacomo Nazzaro", "Konstantinos Vardis", "Markus Steinberger"], "summary": "Recent advances in text-to-3D scene generation have demonstrated significant\npotential to transform content creation across multiple industries. Although\nthe research community has made impressive progress in addressing the\nchallenges of this complex task, existing methods often generate environments\nthat are only front-facing, lack visual fidelity, exhibit limited scene\nunderstanding, and are typically fine-tuned for either indoor or outdoor\nsettings. In this work, we address these issues and propose DreamAnywhere, a\nmodular system for the fast generation and prototyping of 3D scenes. Our system\nsynthesizes a 360{\\deg} panoramic image from text, decomposes it into\nbackground and objects, constructs a complete 3D representation through hybrid\ninpainting, and lifts object masks to detailed 3D objects that are placed in\nthe virtual environment. DreamAnywhere supports immersive navigation and\nintuitive object-level editing, making it ideal for scene exploration, visual\nmock-ups, and rapid prototyping -- all with minimal manual modeling. These\nfeatures make our system particularly suitable for low-budget movie production,\nenabling quick iteration on scene layout and visual tone without the overhead\nof traditional 3D workflows. Our modular pipeline is highly customizable as it\nallows components to be replaced independently. Compared to current\nstate-of-the-art text and image-based 3D scene generation approaches,\nDreamAnywhere shows significant improvements in coherence in novel view\nsynthesis and achieves competitive image quality, demonstrating its\neffectiveness across diverse and challenging scenarios. A comprehensive user\nstudy demonstrates a clear preference for our method over existing approaches,\nvalidating both its technical robustness and practical usefulness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20367v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20367v1", "AI": {"title_translation": "DreamAnywhere: 以对象为中心的沉浸式全景3D场景生成", "tldr": "DreamAnywhere是一个模块化系统，用于快速生成和原型化3D场景，通过文本生成360度全景图像，并支持沉浸式导航和对象级编辑。", "motivation": "现有文本到3D场景生成方法通常只生成正面环境，缺乏视觉保真度，场景理解有限，且通常只针对室内或室外设置进行微调。", "method": "DreamAnywhere首先从文本合成360度全景图像，然后将其分解为背景和对象，通过混合修复构建完整的3D表示，并将对象掩码提升为详细的3D对象并放置在虚拟环境中。该系统支持沉浸式导航和直观的对象级编辑。", "result": "DreamAnywhere在新型视图合成的连贯性方面显示出显著改进，并实现了具有竞争力的图像质量，证明了其在各种复杂场景中的有效性。用户研究表明用户明显偏好该方法。", "conclusion": "DreamAnywhere是一个高效、可定制的3D场景生成系统，解决了现有方法的局限性，特别适用于低预算电影制作和快速原型设计，其技术鲁棒性和实用性得到了验证。", "translation": "最近在文本到3D场景生成方面的进展，展示了在多个行业中改变内容创作的巨大潜力。尽管研究界在解决这项复杂任务的挑战方面取得了令人印象深刻的进展，但现有方法通常只生成正面环境，缺乏视觉保真度，场景理解有限，并且通常针对室内或室外设置进行微调。在这项工作中，我们解决了这些问题，并提出了DreamAnywhere，一个用于快速生成和原型化3D场景的模块化系统。我们的系统从文本合成360度全景图像，将其分解为背景和对象，通过混合修复构建完整的3D表示，并将对象掩码提升为详细的3D对象并放置在虚拟环境中。DreamAnywhere支持沉浸式导航和直观的对象级编辑，使其成为场景探索、视觉模型和快速原型设计的理想选择——所有这些都只需最少的手动建模。这些功能使我们的系统特别适用于低预算电影制作，能够在没有传统3D工作流程开销的情况下，快速迭代场景布局和视觉基调。我们的模块化管道高度可定制，因为它允许组件独立替换。与当前最先进的基于文本和图像的3D场景生成方法相比，DreamAnywhere在新型视图合成的连贯性方面显示出显著改进，并实现了具有竞争力的图像质量，证明了其在各种复杂和具有挑战性的场景中的有效性。一项全面的用户研究表明，用户明显偏好我们的方法，验证了其技术鲁棒性和实用性。", "summary": "DreamAnywhere是一个创新的模块化系统，旨在解决现有文本到3D场景生成方法在全景覆盖、视觉保真度和场景理解方面的局限性。它通过从文本生成360度全景图像，并将其分解、修复和提升为详细的3D对象来构建完整的3D场景。该系统支持沉浸式导航和对象级编辑，适用于快速原型设计和低预算电影制作，并在新型视图合成和图像质量方面超越了现有技术。", "keywords": "文本到3D, 全景3D场景, 对象中心, 模块化系统, 场景生成", "comments": "本文提出了一种创新的模块化3D场景生成系统DreamAnywhere，其亮点在于能够生成360度全景场景，支持对象级编辑和沉浸式导航，并显著提高了新型视图合成的连贯性和图像质量。其模块化设计增强了系统的可定制性，使其特别适用于需要快速迭代和低成本的场景，如电影制作和原型设计。"}}
{"id": "2506.20070", "title": "Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision", "authors": ["KMA Solaiman", "Bharat Bhargava"], "summary": "Existing multi-media retrieval models either rely on creating a common\nsubspace with modality-specific representation models or require schema mapping\namong modalities to measure similarities among multi-media data. Our goal is to\navoid the annotation overhead incurred from considering retrieval as a\nsupervised classification task and re-use the pretrained encoders in large\nlanguage models and vision tasks. We propose \"FemmIR\", a framework to retrieve\nmultimodal results relevant to information needs expressed with multimodal\nqueries by example without any similarity label. Such identification is\nnecessary for real-world applications where data annotations are scarce and\nsatisfactory performance is required without fine-tuning with a common\nframework across applications. We curate a new dataset called MuQNOL for\nbenchmarking progress on this task. Our technique is based on weak supervision\nintroduced through edit distance between samples: graph edit distance can be\nmodified to consider the cost of replacing a data sample in terms of its\nproperties, and relevance can be measured through the implicit signal from the\namount of edit cost among the objects. Unlike metric learning or encoding\nnetworks, FemmIR re-uses the high-level properties and maintains the property\nvalue and relationship constraints with a multi-level interaction score between\ndata samples and the query example provided by the user. We empirically\nevaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs\ncomparably to similar retrieval systems in delivering on-demand retrieval\nresults with exact and approximate similarities while using the existing\nproperty identifiers in the system.", "comment": "Submitted to ICDE'24. An earlier version of this paper appeared on\n  TechRxiv: https://www.techrxiv.org/doi/full/10.36227/techrxiv.21990284.v1,\n  uploaded on February 05, 2023", "pdf_url": "http://arxiv.org/pdf/2506.20070v1", "categories": ["cs.IR", "cs.LG", "cs.MM"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20070v1", "AI": {"title_translation": "基于编辑距离弱监督的开放世界多模态信息检索", "tldr": "本文提出FemmIR框架，利用编辑距离弱监督和预训练编码器，实现无需相似性标签的多模态信息检索，适用于数据标注稀缺的真实世界应用。", "motivation": "现有多媒体检索模型存在标注开销大或需要模态间模式映射的问题。本文旨在避免将检索视为监督分类任务所带来的标注开销，并复用大型语言模型和视觉任务中预训练的编码器。", "method": "本文提出了“FemmIR”框架，通过示例无需任何相似性标签即可检索与多模态查询相关联的多模态结果。该技术基于通过样本间编辑距离引入的弱监督：图编辑距离可以修改以考虑数据样本属性替换的成本，并且可以通过对象间编辑成本的隐式信号来衡量相关性。FemmIR复用高级属性，并通过数据样本和用户提供的查询示例之间的多级交互得分来维护属性值和关系约束。作者还创建了一个名为MuQNOL的新数据集用于该任务的基准测试。", "result": "FemmIR在失踪人员用例中，使用MuQNOL数据集进行了经验评估。FemmIR在提供按需检索结果（包括精确和近似相似性）方面与类似的检索系统表现相当，同时使用了系统中现有的属性标识符。", "conclusion": "FemmIR提供了一种有效的方法，在开放世界环境中执行多模态信息检索，尤其是在标注稀缺的情况下，它通过利用弱监督和现有预训练模型实现这一目标。", "translation": "现有多媒体检索模型要么依赖于通过模态特定表示模型创建通用子空间，要么需要模态间的模式映射来衡量多媒体数据之间的相似性。我们的目标是避免将检索视为监督分类任务所带来的标注开销，并复用大型语言模型和视觉任务中预训练的编码器。我们提出了“FemmIR”，一个框架，通过示例无需任何相似性标签即可检索与多模态查询相关联的多模态结果。这种识别对于数据标注稀缺且需要在不通过通用框架进行跨应用微调的情况下达到满意性能的真实世界应用是必要的。我们策划了一个名为MuQNOL的新数据集，用于衡量该任务的进展。我们的技术基于通过样本间编辑距离引入的弱监督：图编辑距离可以修改以考虑数据样本属性替换的成本，并且可以通过对象间编辑成本的隐式信号来衡量相关性。与度量学习或编码网络不同，FemmIR复用高级属性，并通过数据样本和用户提供的查询示例之间的多级交互得分来维护属性值和关系约束。我们在失踪人员用例中，使用MuQNOL对FemmIR进行了经验评估。FemmIR在提供按需检索结果（包括精确和近似相似性）方面与类似的检索系统表现相当，同时使用了系统中现有的属性标识符。", "summary": "本文针对现有多媒体检索模型存在的标注开销大和依赖模式映射的问题，提出了一种名为“FemmIR”的多模态信息检索框架。FemmIR通过利用编辑距离作为弱监督信号，避免了对相似性标签的依赖，并能复用预训练的语言和视觉模型。该框架通过修改图编辑距离来衡量样本属性替换成本，并利用多级交互得分维护属性约束。作者还创建了MuQNOL数据集进行基准测试。实验证明，FemmIR在失踪人员检索等真实世界应用中，在数据标注稀缺的情况下，性能与现有检索系统相当，能够有效提供按需的多模态检索结果。", "keywords": "多模态检索, 弱监督, 编辑距离, 开放世界, FemmIR", "comments": "本文的主要创新点在于利用编辑距离作为弱监督信号，有效地解决了多模态信息检索中数据标注稀缺的问题，并避免了传统监督学习范式的标注开销。这种方法对于开放世界和真实应用场景具有重要意义，因为它能够灵活地利用现有预训练模型并适应多样化的数据。其无需显式相似性标签的特性，大大降低了实际部署的复杂性。"}}
{"id": "2506.20141", "title": "Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data", "authors": ["Xiaoyu Li", "Zhao Song", "Jiahao Zhang"], "summary": "The explosive growth of AI research has driven paper submissions at flagship\nAI conferences to unprecedented levels, necessitating many venues in 2025\n(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author\nsubmission limits and to desk-reject any excess papers by simple ID order.\nWhile this policy helps reduce reviewer workload, it may unintentionally\ndiscard valuable papers and penalize authors' efforts. In this paper, we ask an\nessential research question on whether it is possible to follow submission\nlimits while minimizing needless rejections. We first formalize the current\ndesk-rejection policies as an optimization problem, and then develop a\npractical algorithm based on linear programming relaxation and a rounding\nscheme. Under extensive evaluation on 11 years of real-world ICLR\n(International Conference on Learning Representations) data, our method\npreserves up to $19.23\\%$ more papers without violating any author limits.\nMoreover, our algorithm is highly efficient in practice, with all results on\nICLR data computed within at most 53.64 seconds. Our work provides a simple and\npractical desk-rejection strategy that significantly reduces unnecessary\nrejections, demonstrating strong potential to improve current CS conference\nsubmission policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20141v1", "categories": ["cs.DS", "cs.CY", "cs.DL", "cs.IR", "cs.LG"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20141v1", "AI": {"title_translation": "接受更多，拒绝更少：基于11年ICLR数据减少高达19%的不必要桌面拒稿", "tldr": "针对AI会议因提交量暴增导致的桌面拒稿问题，本文提出一种优化算法，可在遵守作者限制下减少高达19%的不必要拒稿，且效率高。", "motivation": "AI研究的爆炸式增长导致会议投稿量激增，为减轻审稿负担，许多会议强制执行每位作者的投稿限制，并简单地按ID顺序拒稿，这可能无意中丢弃有价值的论文并惩罚作者的努力。因此，研究目标是探索如何在遵守投稿限制的同时，最大限度地减少不必要的拒稿。", "method": "将当前的桌面拒稿政策形式化为一个优化问题，然后开发了一种基于线性规划松弛和舍入方案的实用算法。", "result": "在11年ICLR真实数据上的广泛评估显示，该方法在不违反任何作者限制的情况下，可以多保留高达19.23%的论文。此外，该算法在实践中效率很高，所有ICLR数据的结果计算时间最多为53.64秒。", "conclusion": "本文提供了一种简单实用的桌面拒稿策略，显著减少了不必要的拒稿，展示了改进当前计算机科学会议投稿政策的巨大潜力。", "translation": "人工智能研究的爆炸式增长使顶级人工智能会议的论文投稿量达到了前所未有的水平，这使得许多2025年的会议（例如CVPR、ICCV、KDD、AAAI、IJCAI、WSDM）不得不强制执行严格的每位作者投稿限制，并通过简单的ID顺序拒绝任何超额论文。虽然这项政策有助于减轻审稿人的工作量，但它可能会无意中丢弃有价值的论文并惩罚作者的努力。在本文中，我们提出了一个重要的研究问题：是否有可能在遵守投稿限制的同时，最大限度地减少不必要的拒稿。我们首先将当前的桌面拒稿政策形式化为一个优化问题，然后开发了一种基于线性规划松弛和舍入方案的实用算法。在对11年ICLR（国际学习表征会议）真实数据进行的广泛评估中，我们的方法在不违反任何作者限制的情况下，多保留了高达19.23%的论文。此外，我们的算法在实践中效率很高，所有ICLR数据的计算结果最多在53.64秒内完成。我们的工作提供了一种简单实用的桌面拒稿策略，显著减少了不必要的拒稿，展示了改进当前计算机科学会议投稿政策的巨大潜力。", "summary": "鉴于AI会议投稿量激增导致严格的桌面拒稿政策可能误拒有价值论文，本文提出了一种基于线性规划松弛和舍入方案的优化算法。该算法将现有拒稿政策形式化为优化问题，并在11年ICLR数据上验证，结果表明在遵守作者限制的前提下，可减少高达19.23%的不必要拒稿，且计算效率高，为改进当前会议投稿政策提供了有效方案。", "keywords": "桌面拒稿, 投稿限制, 优化算法, 线性规划, ICLR数据", "comments": "这项工作创新性地将会议桌面拒稿问题建模为优化问题，并提出了一个实用且高效的解决方案。其重要性在于直接解决了AI会议投稿量激增带来的实际痛点，有助于避免有价值论文被误拒，从而提升会议的包容性和公平性。该方法基于真实数据验证，结果显著，具有很强的实际应用潜力。"}}
{"id": "2506.20007", "title": "A parametric tensor ROM for the shallow water dam break problem", "authors": ["Md Rezwan Bin Mizan", "Maxim Olshanskii", "Ilya Timofeyev"], "summary": "We develop a variant of a tensor reduced-order model (tROM) for the\nparameterized shallow-water dam-break problem. This hyperbolic system presents\nmultiple challenges for model reduction, including a slow decay of the\nKolmogorov $N$-width of the solution manifold, shock formation, and the loss of\nsmooth solution dependence on parameters. These issues limit the performance of\ntraditional Proper Orthogonal Decomposition based ROMs. Our tROM approach,\nbased on a low-rank tensor decomposition, builds a parameter-to-solution map\nfrom high-fidelity snapshots and constructs localized reduced bases via a local\nPOD procedure. We apply this method to both dry-bed and wet-bed problem setups,\nshowing that the non-interpolatory variant of the tROM, combined with Chebyshev\nsampling near critical parameter values, effectively captures\nparameter-dependent behavior and significantly outperforms standard POD-ROMs.\nThis is especially evident in the wet-bed case, where POD-ROMs exhibit poor\nresolution of shock waves and spurious oscillations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20007v1", "categories": ["math.NA", "cs.NA", "physics.flu-dyn", "65M60, 76B15"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20007v1", "AI": {"title_translation": "浅水溃坝问题的参数化张量降阶模型", "tldr": "开发了一种参数化张量降阶模型 (tROM) 用于浅水溃坝问题，该模型通过低秩张量分解和局部POD，有效克服了传统POD-ROM在处理激波和参数依赖性方面的局限性，尤其在湿床情况下表现优异。", "motivation": "传统的基于POD的降阶模型在处理浅水溃坝等双曲系统时面临多重挑战，包括解流形柯尔莫哥洛夫N宽度衰减缓慢、激波形成以及解对参数的光滑依赖性丧失，这些问题限制了其性能。", "method": "本文开发了一种张量降阶模型（tROM）的变体。该方法基于低秩张量分解，利用高精度快照构建参数到解的映射，并通过局部POD程序构建局部降阶基。结合在临界参数值附近的切比雪夫采样，应用非插值变体tROM。", "result": "该方法能够有效捕获参数依赖行为，并显著优于标准POD-ROM。尤其在湿床情况下，该方法表现出更好的激波分辨率，避免了标准POD-ROM出现的虚假振荡。", "conclusion": "提出的参数化张量降阶模型（tROM）有效解决了浅水溃坝问题中传统降阶模型所面临的挑战，特别是在湿床情况下，其对激波的捕捉能力和整体性能显著优于基于POD的传统方法。", "translation": "我们为参数化浅水溃坝问题开发了一种张量降阶模型（tROM）的变体。这个双曲系统给模型降阶带来了多重挑战，包括解流形柯尔莫哥洛夫N宽度的缓慢衰减、激波形成以及解对参数的光滑依赖性丧失。这些问题限制了传统基于本征正交分解（POD）的降阶模型的性能。我们的tROM方法基于低秩张量分解，从高精度快照构建参数到解的映射，并通过局部POD程序构建局部降阶基。我们将此方法应用于干床和湿床问题设置，结果表明，tROM的非插值变体与临界参数值附近的切比雪夫采样相结合，能有效捕获参数依赖行为，并显著优于标准POD-ROM。这在湿床情况下尤其明显，标准POD-ROM在该情况下表现出对激波的低分辨率和虚假振荡。", "summary": "本文提出了一种针对参数化浅水溃坝问题的张量降阶模型（tROM）变体。该模型利用低秩张量分解和局部POD技术，构建参数到解的映射，旨在克服传统POD-ROM在处理激波形成和参数依赖性方面的局限。实验结果表明，结合切比雪夫采样的tROM在捕获参数依赖行为方面表现出色，并显著优于标准POD-ROM，尤其在湿床条件下能更好地解析激波并抑制虚假振荡。", "keywords": "张量降阶模型, 浅水溃坝问题, 参数化系统, 激波捕捉, 低秩张量分解", "comments": "这篇论文通过引入张量降阶模型（tROM）及其局部化和非插值变体，有效解决了传统POD-ROM在处理具有激波和复杂参数依赖性的双曲系统（如浅水溃坝问题）时的固有局限性。其创新点在于结合低秩张量分解和局部POD来构建参数到解的映射，并通过在临界参数值附近采用切比雪夫采样进一步提升了模型在复杂场景下的性能。这项工作对于提升计算流体力学中复杂物理问题的实时模拟和不确定性量化具有重要意义。"}}
{"id": "2506.20259", "title": "Generating and Customizing Robotic Arm Trajectories using Neural Networks", "authors": ["Andrej Lúčny", "Matilde Antonj", "Carlo Mazzola", "Hana Hornáčková", "Igor Farkaš"], "summary": "We introduce a neural network approach for generating and customizing the\ntrajectory of a robotic arm, that guarantees precision and repeatability. To\nhighlight the potential of this novel method, we describe the design and\nimplementation of the technique and show its application in an experimental\nsetting of cognitive robotics. In this scenario, the NICO robot was\ncharacterized by the ability to point to specific points in space with precise\nlinear movements, increasing the predictability of the robotic action during\nits interaction with humans. To achieve this goal, the neural network computes\nthe forward kinematics of the robot arm. By integrating it with a generator of\njoint angles, another neural network was developed and trained on an artificial\ndataset created from suitable start and end poses of the robotic arm. Through\nthe computation of angular velocities, the robot was characterized by its\nability to perform the movement, and the quality of its action was evaluated in\nterms of shape and accuracy. Thanks to its broad applicability, our approach\nsuccessfully generates precise trajectories that could be customized in their\nshape and adapted to different settings.", "comment": "The code is released at\n  https://github.com/andylucny/nico2/tree/main/generate", "pdf_url": "http://arxiv.org/pdf/2506.20259v1", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 70E60", "I.2.9"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20259v1", "AI": {"title_translation": "使用神经网络生成和定制机械臂轨迹", "tldr": "本文提出一种基于神经网络的方法，用于生成和定制机械臂轨迹，确保精度和可重复性，并在认知机器人实验中展示了其有效性。", "motivation": "提高机械臂在与人类互动时的动作可预测性，实现精确的线性运动和可定制的轨迹。", "method": "本文引入了一种神经网络方法，用于生成和定制机械臂轨迹。该方法涉及一个神经网络计算机器人臂的正向运动学，并与另一个训练在人工数据集上的关节角度生成器神经网络集成。通过计算角速度来评估运动质量。", "result": "该方法成功生成了精确的轨迹，这些轨迹可以在形状上进行定制，并适应不同的设置。在NICO机器人实验中，实现了精确的线性指向运动，提高了机器人动作的可预测性。运动质量通过形状和精度进行评估。", "conclusion": "基于神经网络的方法能够有效生成和定制高精度、可重复的机械臂轨迹，适用于多种应用场景。", "translation": "我们引入了一种神经网络方法，用于生成和定制机械臂的轨迹，该方法保证了精度和可重复性。为了突出这种新方法的潜力，我们描述了该技术的设计和实现，并展示了其在认知机器人实验环境中的应用。在这种场景下，NICO机器人的特点是能够以精确的线性运动指向空间中的特定点，从而增加了机器人在与人类互动时动作的可预测性。为了实现这一目标，神经网络计算了机器人臂的正向运动学。通过将其与一个关节角度生成器（另一个神经网络）集成，该网络在由机械臂合适的起始和结束姿态创建的人工数据集上进行开发和训练。通过角速度的计算，该机器人能够执行运动，并且其动作的质量在形状和精度方面得到了评估。由于其广泛的适用性，我们的方法成功生成了精确的轨迹，这些轨迹可以在形状上进行定制并适应不同的设置。", "summary": "本文提出一种基于神经网络的机械臂轨迹生成和定制方法，旨在提高精度、可重复性及与人类互动时的可预测性。该方法结合了计算正向运动学的神经网络和基于人工数据集训练的关节角度生成器。实验结果表明，该方法能够成功生成精确且可定制的轨迹，并在认知机器人实验中实现了NICO机器人的精确指向能力，验证了其有效性和广泛适用性。", "keywords": "神经网络, 机械臂轨迹, 轨迹生成, 轨迹定制, 认知机器人", "comments": "本文的创新点在于将神经网络应用于机械臂轨迹的生成和定制，并强调了其在保证精度和可重复性方面的优势。通过结合正向运动学计算和关节角度生成器，提供了一种灵活且高效的轨迹规划方案。该方法在认知机器人领域的应用，特别是提高人机交互的可预测性，具有重要意义。其广泛适用性也暗示了其在其他机器人应用中的潜力。"}}
{"id": "2506.20558", "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency", "authors": ["Renyi Zhong", "Yintong Huo", "Wenwei Gu", "Jinxi Kuang", "Zhihan Jiang", "Guangba Yu", "Yichen Li", "David Lo", "Michael R. Lyu"], "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability.", "comment": "This manuscript is under review", "pdf_url": "http://arxiv.org/pdf/2506.20558v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20558v1", "AI": {"title_translation": "CCISolver：方法级代码注释不一致的端到端检测与修复", "tldr": "CCISolver是一个基于大型语言模型的端到端框架，用于检测和修复代码注释不一致性，通过引入高质量数据集CCIBench并实现卓越的检测和修复性能，同时显著提高推理速度。", "motivation": "代码注释不一致性（CCI）对软件开发、测试和维护产生负面影响。现有研究存在数据集不准确和解决方案不足的问题，限制了其在实际中的有效性。", "method": "本研究首先对现有数据集进行定量分析，发现大量错误标记的数据。为解决数据限制，引入了高质量的精炼数据集CCIBench。在此基础上，提出了一个创新的、基于大型语言模型（LLM）的端到端框架CCISolver，用于识别和纠正方法级CCI。", "result": "CCISolver在检测任务中达到了89.54%的F1分数，创造了新的最先进水平。在修复任务中，其GLEU分数比最强基线相对提高了18.84%。人工评估也证实了其优势，修复成功率为0.6533。在实际端到端设置中，CCISolver的推理速度比基线模型快约36%。", "conclusion": "CCISolver在方法级代码注释不一致的检测和修复方面表现出卓越的性能，并具备良好的可扩展性和实际应用性，显著优于现有方法。", "translation": "代码中的注释是软件文档的关键基础，有助于开发人员有效沟通和理解代码。然而，代码注释不一致性（CCI）会对软件开发、测试和维护产生负面影响。尽管最近出现了旨在缓解此问题的努力，但现有研究通常存在数据集不准确和解决方案不足的问题，削弱了其实际有效性。在本研究中，我们首先对现有数据集进行了定量分析，揭示了大量采样数据被错误标记。为解决这些数据限制，我们引入了CCIBench，一个包含高质量数据的精炼数据集，以支持方法级CCI方法的训练和评估。此外，我们提出了一个创新的、基于大型语言模型（LLM）的端到端框架CCISolver，旨在通过识别和纠正CCI来提高代码质量。全面的评估表明CCISolver具有卓越的性能。在检测方面，它以89.54%的F1分数建立了新的最先进水平。在修复任务中，它比最强基线在GLEU分数上实现了18.84%的显著相对改进。人类评估证实了其优势，CCISolver的修复成功率达到0.6533，显著超越现有方法。关键的是，在实际的端到端设置中，CCISolver的创新架构推理速度比基线模型快约36%，这突显了其可扩展性和实际适用性。", "summary": "本研究旨在解决代码注释不一致（CCI）问题，指出现有解决方案存在数据集不准确和效果不足的缺陷。为此，作者首先对现有数据集进行了定量分析，发现大量错误标记的数据，并提出高质量的CCIBench数据集以支持方法级CCI方法的训练和评估。在此基础上，研究引入了CCISolver，一个创新的端到端LLM框架，用于检测和修复CCI。实验结果表明，CCISolver在检测任务中F1分数达到89.54%，在修复任务中GLEU分数相对提升18.84%，并且推理速度比基线模型快36%，展现了其卓越的性能、可扩展性和实际应用潜力。", "keywords": "代码注释不一致, CCISolver, 大语言模型, 代码质量, 端到端", "comments": "这篇论文的创新点在于首先通过定量分析揭示了现有CCI数据集的质量问题并构建了高质量的CCIBench数据集，为后续研究提供了坚实基础。其次，提出了一个端到端、基于LLM的CCISolver框架，不仅在检测和修复方面取得了显著的SOTA性能，更重要的是，其在实际端到端场景中的推理速度优势（快36%）极大地提升了其实用性和可扩展性，解决了现有方案在实际应用中可能面临的效率瓶颈。这对于提高软件质量和开发效率具有重要意义。"}}
{"id": "2506.20155", "title": "Towards Efficient Exemplar Based Image Editing with Multimodal VLMs", "authors": ["Avadhoot Jadhav", "Ashutosh Srivastava", "Abhinav Java", "Silky Singh", "Tarun Ram Menta", "Surgan Jandial", "Balaji Krishnamurthy"], "summary": "Text-to-Image Diffusion models have enabled a wide array of image editing\napplications. However, capturing all types of edits through text alone can be\nchallenging and cumbersome. The ambiguous nature of certain image edits is\nbetter expressed through an exemplar pair, i.e., a pair of images depicting an\nimage before and after an edit respectively. In this work, we tackle\nexemplar-based image editing -- the task of transferring an edit from an\nexemplar pair to a content image(s), by leveraging pretrained text-to-image\ndiffusion models and multimodal VLMs. Even though our end-to-end pipeline is\noptimization-free, our experiments demonstrate that it still outperforms\nbaselines on multiple types of edits while being ~4x faster.", "comment": "Accepted at ECCV 2024 (AI4VA Workshop)", "pdf_url": "http://arxiv.org/pdf/2506.20155v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20155v1", "AI": {"title_translation": "基于多模态VLM的高效范例图像编辑", "tldr": "本文利用预训练的文本到图像扩散模型和多模态VLM，实现了高效的基于范例的图像编辑，性能优于基线模型且速度更快。", "motivation": "文本描述难以捕获所有类型的图像编辑，尤其是模糊的编辑。范例对（编辑前后的图像）能更好地表达这些编辑。", "method": "提出了一种端到端的、无需优化的管道，利用预训练的文本到图像扩散模型和多模态VLM进行基于范例的图像编辑，将编辑从范例对转移到内容图像。", "result": "实验表明，该方法在多种编辑类型上优于基线模型，并且速度快约4倍。", "conclusion": "利用预训练模型和多模态VLM可以实现高效且高性能的基于范例的图像编辑。", "translation": "文本到图像扩散模型已经支持了广泛的图像编辑应用。然而，仅仅通过文本来捕获所有类型的编辑可能具有挑战性和繁琐。某些图像编辑的模糊性通过范例对能更好地表达，即一对分别描绘编辑前和编辑后图像的图像。在这项工作中，我们通过利用预训练的文本到图像扩散模型和多模态VLM，解决了基于范例的图像编辑任务——将编辑从范例对转移到内容图像。尽管我们的端到端管道是无需优化的，但我们的实验表明，它在多种编辑类型上仍然优于基线模型，同时速度快约4倍。", "summary": "本文提出了一种高效的基于范例的图像编辑方法，旨在克服纯文本编辑的局限性。该方法利用预训练的文本到图像扩散模型和多模态VLMs，通过将编辑从范例对转移到内容图像。实验证明，该端到端、无需优化的管道在性能上优于现有基线，并且显著提升了编辑速度。", "keywords": "图像编辑, 范例学习, 扩散模型, 多模态VLM, 文本到图像", "comments": "该论文的创新点在于结合了文本到图像扩散模型和多模态VLM来解决基于范例的图像编辑问题，提供了一种无需优化的端到端解决方案。其重要性体现在解决了纯文本编辑的局限性，使得模糊或复杂的图像编辑更加直观高效。显著的速度提升和优于基线的性能是其主要优势。"}}
{"id": "2506.19895", "title": "A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers", "authors": ["Miguel N. Font", "José L. Jorro-Aragoneses", "Carlos M. Alaíz"], "summary": "Neural Networks have high accuracy in solving problems where it is difficult\nto detect patterns or create a logical model. However, these algorithms\nsometimes return wrong solutions, which become problematic in high-risk domains\nlike medical diagnosis or autonomous driving. One strategy to detect and\nmitigate these errors is the measurement of the uncertainty over neural network\ndecisions. In this paper, we present a novel post-hoc framework for measuring\nthe uncertainty of a decision based on retrieved training cases that have a\nsimilar activation vector to the query for each layer. Based on these retrieved\ncases, we propose two new metrics: Decision Change and Layer Uncertainty, which\ncapture changes in nearest-neighbor class distributions across layers. We\nevaluated our approach in a classification model for two datasets: CIFAR-10 and\nMNIST. The results show that these metrics enhance uncertainty estimation,\nespecially in challenging classification tasks, outperforming softmax-based\nconfidence.", "comment": "This paper has been accepted for presentation at ICANN 2025\n  (International Conference on Artificial Neural Networks) and will appear in\n  the conference proceedings published by Springer Nature in the Lecture Notes\n  in Computer Science (LNCS) series. The final authenticated version will be\n  available on the publisher website", "pdf_url": "http://arxiv.org/pdf/2506.19895v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19895v1", "AI": {"title_translation": "基于跨层最近邻的不确定性量化框架", "tldr": "本文提出了一种新的后验框架，通过分析神经网络各层中与查询激活向量相似的训练样本的最近邻类别分布变化，来量化神经网络决策的不确定性，并在实验中表现优于传统方法。", "motivation": "神经网络在解决难以检测模式或创建逻辑模型的问题时具有高精度，然而在高风险领域（如医疗诊断或自动驾驶）中，错误的解决方案会带来严重问题。因此，需要一种策略来检测和缓解这些错误，即测量神经网络决策的不确定性。", "method": "本文提出了一种新颖的后验框架，用于测量基于检索到的训练案例的决策不确定性。这些训练案例在每个层上都具有与查询相似的激活向量。基于这些检索到的案例，提出了两个新指标：决策变化（Decision Change）和层不确定性（Layer Uncertainty），它们捕获了跨层最近邻类别分布的变化。该方法在CIFAR-10和MNIST数据集上进行了评估。", "result": "结果表明，这些指标增强了不确定性估计，尤其是在具有挑战性的分类任务中，并且优于基于Softmax的置信度。", "conclusion": "该框架通过引入新的不确定性度量，有效提升了神经网络在关键应用中的可靠性，尤其在复杂分类任务中表现出优越性。", "translation": "神经网络在解决难以检测模式或创建逻辑模型的问题时具有高精度。然而，这些算法有时会返回错误的解决方案，这在高风险领域（如医疗诊断或自动驾驶）中会成为问题。检测和缓解这些错误的一种策略是测量神经网络决策的不确定性。在本文中，我们提出了一种新颖的后验框架，用于测量基于检索到的训练案例的决策不确定性，这些案例在每个层上都具有与查询相似的激活向量。基于这些检索到的案例，我们提出了两个新指标：决策变化和层不确定性，它们捕获了跨层最近邻类别分布的变化。我们在CIFAR-10和MNIST两个数据集的分类模型中评估了我们的方法。结果表明，这些指标增强了不确定性估计，尤其是在具有挑战性的分类任务中，并且优于基于Softmax的置信度。", "summary": "本文提出了一种新颖的后验框架，旨在量化神经网络决策的不确定性，以解决其在高风险领域可能产生的错误问题。该框架通过检索在各层具有相似激活向量的训练案例，并基于这些案例引入了“决策变化”和“层不确定性”两个新指标，以捕捉跨层最近邻类别分布的变化。在CIFAR-10和MNIST数据集上的分类模型评估显示，该方法能有效增强不确定性估计，尤其在复杂任务中表现出色，并优于传统的Softmax置信度。", "keywords": "不确定性量化, 神经网络, 最近邻, 后验方法, 分类", "comments": "这项研究的创新之处在于其不确定性量化方法是基于神经网络内部各层的激活向量的最近邻分析，而非仅仅依赖于最终输出。作为一种后验框架，它具有较强的实用性，可以应用于现有模型而无需修改模型结构。该方法通过引入“决策变化”和“层不确定性”这两个新颖的指标，有效地提升了不确定性估计的准确性，特别是在面对复杂分类任务时，其性能优于传统的Softmax置信度，这对于高风险应用领域具有重要意义。"}}
{"id": "2506.20130", "title": "AI Copilots for Reproducibility in Science: A Case Study", "authors": ["Adrien Bibal", "Steven N. Minton", "Deborah Khider", "Yolanda Gil"], "summary": "Open science initiatives seek to make research outputs more transparent,\naccessible, and reusable, but ensuring that published findings can be\nindependently reproduced remains a persistent challenge. This paper introduces\nOpenPub, an AI-powered platform that supports researchers, reviewers, and\nreaders through a suite of modular copilots focused on key open science tasks.\nIn this work, we present the Reproducibility Copilot, which analyzes\nmanuscripts, code, and supplementary materials to generate structured Jupyter\nNotebooks and recommendations aimed at facilitating computational, or \"rote\",\nreproducibility. We conducted feasibility tests using previously studied\nresearch papers with known reproducibility benchmarks. Results indicate that\nOpenPub can substantially reduce reproduction time - from over 30 hours to\nabout 1 hour - while achieving high coverage of figures, tables, and results\nsuitable for computational reproduction. The system systematically detects\nbarriers to reproducibility, including missing hyperparameters, undocumented\npreprocessing steps, and incomplete or inaccessible datasets. These findings\nsuggest that AI-driven tools can meaningfully reduce the burden of\nreproducibility efforts and contribute to more transparent and verifiable\nscientific communication. The modular copilot architecture also provides a\nfoundation for extending AI assistance to additional open science objectives\nbeyond reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20130v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20130v1", "AI": {"title_translation": "科学可重复性AI副驾驶：一个案例研究", "tldr": "OpenPub是一个AI平台，通过可重复性副驾驶工具，将研究论文的计算可重复性时间从30多小时大幅缩短到约1小时，同时识别可重复性障碍。", "motivation": "开放科学旨在提高研究成果的透明度、可访问性和可重用性，但确保已发表研究的可独立重复性仍然是一个持续的挑战。", "method": "该论文介绍了OpenPub，一个由AI驱动的平台，通过一套模块化的副驾驶工具支持研究人员、审稿人和读者。文中特别介绍了“可重复性副驾驶”，它分析手稿、代码和补充材料，生成结构化的Jupyter Notebooks和建议，旨在促进计算或“机械”可重复性。研究人员使用已知可重复性基准的现有论文进行了可行性测试。", "result": "结果表明，OpenPub可以将重现时间从30多个小时大幅缩短到大约1小时，同时实现对图表、表格和结果的高覆盖率，适用于计算重现。该系统能系统地检测出可重复性障碍，包括缺失的超参数、未记录的预处理步骤以及不完整或无法访问的数据集。", "conclusion": "研究结果表明，AI驱动的工具可以显著减轻可重复性工作的负担，并有助于更透明、可验证的科学交流。模块化的副驾驶架构也为将AI辅助扩展到可重复性之外的其他开放科学目标奠定了基础。", "translation": "开放科学倡议旨在使研究成果更加透明、可访问和可重用，但确保已发表的研究能够独立重现仍然是一个持续的挑战。本文介绍了OpenPub，一个由人工智能驱动的平台，通过一套专注于关键开放科学任务的模块化副驾驶工具来支持研究人员、审稿人和读者。在这项工作中，我们提出了可重复性副驾驶，它分析手稿、代码和补充材料，以生成结构化的Jupyter Notebooks和旨在促进计算或“机械”可重复性的建议。我们使用先前研究过的具有已知可重复性基准的研究论文进行了可行性测试。结果表明，OpenPub可以显著减少重现时间——从30多个小时到大约1小时——同时实现对图表、表格和结果的高覆盖率，适用于计算重现。该系统系统地检测出可重复性障碍，包括缺失的超参数、未记录的预处理步骤以及不完整或无法访问的数据集。这些发现表明，人工智能驱动的工具可以有效地减轻可重复性工作的负担，并有助于更透明和可验证的科学交流。模块化的副驾驶架构也为将人工智能辅助扩展到可重复性之外的其他开放科学目标提供了基础。", "summary": "本文介绍了OpenPub，一个AI驱动的平台，旨在解决科学研究中可重复性面临的挑战。其核心是“可重复性副驾驶”，通过分析手稿、代码和补充材料，生成结构化的Jupyter Notebooks，以促进计算可重复性。可行性测试表明，OpenPub能将研究重现时间从30多小时缩短至约1小时，并能有效识别如缺失参数、未记录步骤和不完整数据集等可重复性障碍。这表明AI工具能显著减轻可重复性负担，提升科学交流的透明度和可验证性，并为未来AI在开放科学领域的应用奠定基础。", "keywords": "AI副驾驶, 科学可重复性, OpenPub, Jupyter Notebook, 开放科学", "comments": "这篇论文提出了一种创新的AI辅助方法来解决科学可重复性这一核心挑战。OpenPub的可重复性副驾驶通过自动化Jupyter Notebook的生成和识别常见障碍，显著提高了重现效率，并降低了研究人员的负担。其模块化架构具有很强的扩展性，未来可应用于其他开放科学领域，潜力巨大。这项工作对于推动开放科学实践和提升科学研究的可靠性具有重要意义。"}}
{"id": "2506.20377", "title": "The Role of Partisan Culture in Mental Health Language Online", "authors": ["Sachin R. Pendse", "Ben Rochford", "Neha Kumar", "Munmun De Choudhury"], "summary": "The impact of culture on how people express distress in online support\ncommunities is increasingly a topic of interest within Computer Supported\nCooperative Work (CSCW) and Human-Computer Interaction (HCI). In the United\nStates, distinct cultures have emerged from each of the two dominant political\nparties, forming a primary lens by which people navigate online and offline\nworlds. We examine whether partisan culture may play a role in how U.S.\nRepublican and Democrat users of online mental health support communities\nexpress distress. We present a large-scale observational study of 2,184,356\nposts from 8,916 statistically matched Republican, Democrat, and unaffiliated\nonline support community members. We utilize methods from causal inference to\nstatistically match partisan users along covariates that correspond with\ndemographic attributes and platform use, in order to create comparable cohorts\nfor analysis. We then leverage methods from natural language processing to\nunderstand how partisan expressions of distress compare between these sets of\nclosely matched opposing partisans, and between closely matched partisans and\ntypical support community members. Our data spans January 2013 to December\n2022, a period of both rising political polarization and mental health\nconcerns. We find that partisan culture does play into expressions of distress,\nunderscoring the importance of considering partisan cultural differences in the\ndesign of online support community platforms.", "comment": "Accepted to the ACM Conference on Computer-Supported Cooperative Work\n  and Social Computing (CSCW 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20377v1", "categories": ["cs.HC", "cs.CY", "cs.SI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20377v1", "AI": {"title_translation": "党派文化在在线心理健康语言中的作用", "tldr": "本文发现党派文化影响美国在线心理健康社区用户表达痛苦的方式，强调平台设计需考虑此差异。", "motivation": "研究在线支持社区中文化如何影响人们表达痛苦是计算机支持协同工作（CSCW）和人机交互（HCI）领域日益关注的话题。在美国，两大主要政党形成了独特的文化视角。本文旨在探究党派文化是否影响美国共和党和民主党用户在在线心理健康支持社区中表达痛苦的方式。", "method": "进行了一项大规模观察性研究，分析了2013年1月至2022年12月期间来自8,916名统计匹配的共和党、民主党和无党派在线支持社区成员的2,184,356条帖子。研究利用因果推断方法根据人口统计属性和平台使用等协变量对党派用户进行统计匹配，以创建可比较的分析队列。随后，利用自然语言处理方法比较了匹配的对立党派用户以及匹配党派用户与典型支持社区成员之间表达痛苦的方式。", "result": "研究发现党派文化确实影响了痛苦的表达。", "conclusion": "强调在设计在线支持社区平台时，考虑党派文化差异的重要性。", "translation": "文化如何影响人们在在线支持社区中表达痛苦，在计算机支持协同工作（CSCW）和人机交互（HCI）领域日益受到关注。在美国，两大主要政党各自形成了独特的文化，成为人们在线上线下世界中导航的主要视角。我们研究了党派文化是否在美国在线心理健康支持社区的共和党和民主党用户表达痛苦的方式中发挥作用。我们对来自8,916名经过统计匹配的共和党、民主党和无党派在线支持社区成员的2,184,356条帖子进行了大规模观察性研究。我们利用因果推断方法，根据与人口统计属性和平台使用相关的协变量对党派用户进行统计匹配，以创建可比较的分析队列。然后，我们利用自然语言处理方法，了解这些密切匹配的对立党派之间以及密切匹配的党派成员与典型支持社区成员之间表达痛苦的方式有何不同。我们的数据涵盖2013年1月至2022年12月，这是一个政治两极分化和心理健康问题日益突出的时期。我们发现党派文化确实影响了痛苦的表达，这强调了在设计在线支持社区平台时考虑党派文化差异的重要性。", "summary": "本文通过大规模观察性研究，探究了美国党派文化如何影响在线心理健康支持社区中用户表达痛苦的方式。研究分析了来自近9000名匹配的共和党、民主党和无党派用户在2013年至2022年间的逾200万条帖子，并结合因果推断和自然语言处理方法。结果表明，党派文化确实在痛苦表达中扮演角色，这提示在设计在线支持社区平台时应充分考虑党派文化差异。", "keywords": "党派文化, 心理健康, 在线支持社区, 痛苦表达, 自然语言处理", "comments": "这项研究创新性地将政治党派文化与在线心理健康表达联系起来，利用大规模数据和严谨的因果推断方法控制混淆变量，增强了研究的可靠性。其重要性在于揭示了在线支持社区中被忽视的文化维度，为未来在线心理健康平台的设计和个性化支持提供了重要启示。"}}
{"id": "2506.20488", "title": "Generative AI for Vulnerability Detection in 6G Wireless Networks: Advances, Case Study, and Future Directions", "authors": ["Shuo Yang", "Xinran Zheng", "Jinfeng Xu", "Jinze Li", "Danyang Song", "Zheyu Chen", "Edith C. H. Ngai"], "summary": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20488v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20488v1", "AI": {"title_translation": "生成式人工智能在6G无线网络漏洞检测中的应用：进展、案例研究与未来方向", "tldr": "本文探讨了生成式AI在6G无线网络漏洞检测中的应用，提出了一个三层框架，并通过LLM驱动的代码漏洞检测案例研究展示了其有效性。", "motivation": "6G无线网络、物联网和边缘计算的快速发展显著扩大了网络攻击面，需要更智能和适应性的漏洞检测机制。传统安全方法难以应对零日漏洞、对抗性威胁和高度动态网络环境中的上下文相关漏洞。", "method": "本文探讨了生成式AI在6G无线网络漏洞检测中的集成应用，重点关注代码审计、协议安全、云边防御和硬件保护。提出了一个三层框架，包括技术层、能力层和应用层，系统分析了VAEs、GANs、LLMs和GDMs在保护下一代无线生态系统中的作用。通过一个LLM驱动的代码漏洞检测案例研究展示了实际实现。", "result": "案例研究展示了LLM驱动的代码漏洞检测的有效性、性能和挑战。", "conclusion": "本文综合了当前进展和开放挑战，为研究人员和从业者提供了利用生成式AI在6G网络中构建弹性自适应安全解决方案的路线图。", "translation": "6G无线网络、物联网和边缘计算的快速发展显著扩大了网络攻击面，需要更智能和适应性的漏洞检测机制。传统安全方法虽然是基础，但在高度动态的网络环境中，难以应对零日漏洞、对抗性威胁和上下文相关漏洞。生成式人工智能（GAI）作为一种变革性解决方案出现，它利用合成数据生成、多模态推理和自适应学习来增强安全框架。本文探讨了生成式AI驱动的漏洞检测在6G无线网络中的集成，重点关注代码审计、协议安全、云边防御和硬件保护。我们引入了一个由技术层、能力层和应用层组成的三层框架，系统分析了VAEs、GANs、LLMs和GDMs在保护下一代无线生态系统中的作用。为了展示实际应用，我们提出了一个LLM驱动的代码漏洞检测案例研究，强调了其有效性、性能和挑战。最后，我们概述了未来的研究方向，包括轻量级模型、高真实性数据生成、外部知识集成和隐私保护技术。通过综合当前进展和开放挑战，这项工作为研究人员和从业者提供了利用生成式AI在6G网络中构建弹性自适应安全解决方案的路线图。", "summary": "本文探讨了生成式AI在6G无线网络漏洞检测中的应用，以应对日益扩大的网络攻击面和传统方法的局限性。研究引入了一个三层框架，分析了VAEs、GANs、LLMs和GDMs等生成式模型在代码审计、协议安全、云边防御和硬件保护方面的作用。通过一个LLM驱动的代码漏洞检测案例研究，展示了其有效性和挑战。文章还提出了未来研究方向，旨在为6G网络构建弹性自适应安全解决方案提供指导。", "keywords": "生成式AI, 漏洞检测, 6G无线网络, LLM, 网络安全", "comments": "本文创新性地将生成式AI引入6G无线网络的漏洞检测领域，提出了一个实用的三层框架，并结合具体案例展示了其潜力。其重要性在于为未来网络安全提供了新的视角和技术路径，尤其是在处理零日漏洞和动态网络环境方面。未来研究方向的提出也很有价值。"}}
{"id": "2506.20118", "title": "The Graph Structure of a Class of Permutation Maps over Ring $\\mathbb{Z}_{p^k}$", "authors": ["Kai Tan", "Chengqing Li"], "summary": "Understanding the periodic and structural properties of permutation maps over\nresidue rings such as $\\mathbb{Z}_{p^k}$ is a foundational challenge in\nalgebraic dynamics and pseudorandom sequence analysis. Despite notable progress\nin characterizing global periods, a critical bottleneck remains: the lack of\nexplicit tools to analyze local cycle structures and their evolution with\nincreasing arithmetic precision. In this work, we propose a unified analytical\nframework to systematically derive the distribution of cycle lengths for a\nclass of permutation maps over $\\mathbb{Z}_{p^k}$. The approach combines\ntechniques from generating functions, minimal polynomials, and lifting theory\nto track how the cycle structure adapts as the modulus $p^k$ changes. To\nvalidate the generality and effectiveness of our method, we apply it to the\nwell-known Cat map as a canonical example, revealing the exact patterns\ngoverning its cycle formation and transition. This analysis not only provides\nrigorous explanations for experimentally observed regularities in fixed-point\nimplementations of such maps but also lays a theoretical foundation for\nevaluating the randomness and dynamical behavior of pseudorandom number\nsequences generated by other nonlinear maps. The results have broad\nimplications for secure system design, computational number theory, and\nsymbolic dynamics.", "comment": "11 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.20118v1", "categories": ["math.NT", "cs.IT", "math.IT", "11T06, 37P25"], "cate": "math.NT", "url": "http://arxiv.org/abs/2506.20118v1", "AI": {"title_translation": "环$\\mathbb{Z}_{p^k}$上一类置换映射的图结构", "tldr": "本文提出了一个统一的分析框架，用于系统地推导环$\\mathbb{Z}_{p^k}$上一类置换映射的周期长度分布，解决了局部周期结构分析的瓶颈问题，并应用于Cat映射进行验证，对伪随机序列分析和安全系统设计有重要意义。", "motivation": "理解$\\mathbb{Z}_{p^k}$等剩余环上置换映射的周期和结构特性是代数动力学和伪随机序列分析中的一个基础挑战。尽管全局周期特性取得了进展，但分析局部周期结构及其随算术精度增加而演变的显式工具的缺乏仍然是一个关键瓶颈。", "method": "提出一个统一的分析框架，结合生成函数、最小多项式和提升理论技术，系统地推导环$\\mathbb{Z}_{p^k}$上一类置换映射的周期长度分布，并跟踪周期结构如何随模数$p^k$变化而调整。该方法应用于Cat映射进行验证。", "result": "揭示了Cat映射周期形成和转换的精确模式，为这些映射在定点实现中观察到的规律提供了严谨的解释。", "conclusion": "本文的分析为评估由其他非线性映射生成的伪随机数序列的随机性和动力学行为奠定了理论基础，对安全系统设计、计算数论和符号动力学具有广泛的意义。", "translation": "理解$\\mathbb{Z}_{p^k}$等剩余环上置换映射的周期和结构特性是代数动力学和伪随机序列分析中的一个基础挑战。尽管在刻画全局周期方面取得了显著进展，但一个关键瓶颈依然存在：缺乏显式工具来分析局部周期结构及其随算术精度增加而演变。在这项工作中，我们提出了一个统一的分析框架，以系统地推导环$\\mathbb{Z}_{p^k}$上一类置换映射的周期长度分布。该方法结合了生成函数、最小多项式和提升理论的技术，以跟踪周期结构如何随模数$p^k$的变化而适应。为了验证我们方法的通用性和有效性，我们将其应用于著名的Cat映射作为典型示例，揭示了控制其周期形成和转换的精确模式。这项分析不仅为这些映射在定点实现中实验观察到的规律提供了严谨的解释，而且为评估由其他非线性映射生成的伪随机数序列的随机性和动力学行为奠定了理论基础。这些结果对安全系统设计、计算数论和符号动力学具有广泛的意义。", "summary": "本文针对代数动力学和伪随机序列分析中关于$\\mathbb{Z}_{p^k}$上置换映射局部周期结构分析的瓶颈问题，提出了一个统一的分析框架。该框架结合生成函数、最小多项式和提升理论，能够系统地推导一类置换映射的周期长度分布并跟踪其随模数变化的演变。通过将该方法应用于Cat映射，作者成功揭示了其周期模式，为实验观察提供了理论解释，并为评估伪随机数序列的随机性奠定了基础，对安全系统设计等领域具有重要意义。", "keywords": "置换映射, 周期结构, $\\mathbb{Z}_{p^k}$, 生成函数, Cat映射", "comments": "本文的创新之处在于提出了一个统一的分析框架来解决环$\\mathbb{Z}_{p^k}$上置换映射局部周期结构分析的长期瓶颈问题。通过结合多种数学工具，该方法能够系统地揭示周期长度分布和其演变，并成功应用于Cat映射。这项工作为理解和评估伪随机序列的动力学行为提供了坚实的理论基础，具有重要的实际应用价值，特别是在安全系统设计方面。"}}
{"id": "2506.20511", "title": "Collaborative Batch Size Optimization for Federated Learning", "authors": ["Arno Geimer", "Karthick Panner Selvam", "Beltran Fiz Pontiveros"], "summary": "Federated Learning (FL) is a decentralized collaborative Machine Learning\nframework for training models without collecting data in a centralized\nlocation. It has seen application across various disciplines, from helping\nmedical diagnoses in hospitals to detecting fraud in financial transactions. In\nthis paper, we focus on improving the local training process through hardware\nusage optimization. While participants in a federation might share the hardware\nthey are training on, since there is no information exchange between them,\ntheir training process can be hindered by an improper training configuration.\nTaking advantage of the parallel processing inherent to Federated Learning, we\nuse a greedy randomized search to optimize local batch sizes for the best\ntraining settings across all participants. Our results show that against\ndefault parameter settings, our method improves convergence speed while staying\nnearly on par with the case where local parameters are optimized.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20511v1", "categories": ["cs.LG", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20511v1", "AI": {"title_translation": "联邦学习中的协作批次大小优化", "tldr": "本文提出一种通过贪婪随机搜索优化联邦学习中本地批次大小的方法，旨在提高训练效率和模型收敛速度。", "motivation": "联邦学习中的本地训练过程可能因不当的训练配置而受阻，尤其是在参与者共享硬件但缺乏信息交换的情况下。本研究旨在通过硬件使用优化来改进本地训练过程。", "method": "利用联邦学习固有的并行处理特性，使用贪婪随机搜索来优化所有参与者的本地批次大小，以获得最佳训练设置。", "result": "与默认参数设置相比，该方法提高了收敛速度，同时与本地参数已优化的情形几乎持平。", "conclusion": "该研究提出的协作批次大小优化方法能够有效提升联邦学习的收敛速度，即使在没有本地参数预优化的情况下也能保持良好性能。", "translation": "联邦学习（FL）是一个去中心化的协作机器学习框架，用于在不集中收集数据的情况下训练模型。它已应用于各个领域，从帮助医院的医疗诊断到检测金融交易中的欺诈。在本文中，我们专注于通过硬件使用优化来改进本地训练过程。虽然联邦中的参与者可能会共享他们正在训练的硬件，但由于他们之间没有信息交换，他们的训练过程可能会因不正确的训练配置而受阻。利用联邦学习固有的并行处理特性，我们使用贪婪随机搜索来优化本地批次大小，以在所有参与者中获得最佳训练设置。我们的结果表明，与默认参数设置相比，我们的方法提高了收敛速度，同时与本地参数已优化的情形几乎持平。", "summary": "本文提出一种针对联邦学习的协作批次大小优化方法，旨在解决因不当训练配置导致的本地训练效率低下问题。研究利用联邦学习的并行处理能力，采用贪婪随机搜索来优化各参与方的本地批次大小。实验结果表明，该方法在不进行本地参数优化的情况下，显著提升了模型的收敛速度，且性能接近于本地参数已完全优化的理想情况。", "keywords": "联邦学习, 批次大小优化, 贪婪随机搜索, 收敛速度, 硬件优化", "comments": "本文的创新点在于提出了一个无需参与者间信息交换的本地批次大小优化方案，这对于联邦学习这种去中心化框架尤为重要。其方法利用了并行处理和启发式搜索，为提升联邦学习的效率提供了一个实用的途径，对于资源受限或隐私敏感的联邦学习场景具有潜在应用价值。"}}
{"id": "2506.20530", "title": "Toward a Global Regime for Compute Governance: Building the Pause Button", "authors": ["Ananthi Al Ramiah", "Raymond Koopmanschap", "Josh Thorsteinson", "Sadruddin Khan", "Jim Zhou", "Shafira Noh", "Joep Meindertsma", "Farhan Shafiq"], "summary": "As AI capabilities rapidly advance, the risk of catastrophic harm from\nlarge-scale training runs is growing. Yet the compute infrastructure that\nenables such development remains largely unregulated. This paper proposes a\nconcrete framework for a global \"Compute Pause Button\": a governance system\ndesigned to prevent dangerously powerful AI systems from being trained by\nrestricting access to computational resources. We identify three key\nintervention points -- technical, traceability, and regulatory -- and organize\nthem within a Governance--Enforcement--Verification (GEV) framework to ensure\nrules are clear, violations are detectable, and compliance is independently\nverifiable. Technical mechanisms include tamper-proof FLOP caps, model locking,\nand offline licensing. Traceability tools track chips, components, and users\nacross the compute supply chain. Regulatory mechanisms establish constraints\nthrough export controls, production caps, and licensing schemes. Unlike\npost-deployment oversight, this approach targets the material foundations of\nadvanced AI development. Drawing from analogues ranging from nuclear\nnon-proliferation to pandemic-era vaccine coordination, we demonstrate how\ncompute can serve as a practical lever for global cooperation. While technical\nand political challenges remain, we argue that credible mechanisms already\nexist, and that the time to build this architecture is now, before the window\nfor effective intervention closes.", "comment": "34 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20530v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20530v1", "AI": {"title_translation": "迈向全球计算治理体系：构建“暂停按钮”", "tldr": "鉴于AI能力快速发展带来的潜在灾难性风险，本文提出了一个全球“计算暂停按钮”的治理框架，旨在通过限制计算资源来阻止危险的AI系统被训练，并强调现在是构建该体系的关键时刻。", "motivation": "随着AI能力的快速发展，大规模训练运行可能导致灾难性危害的风险日益增长，而支持这种发展的计算基础设施却在很大程度上未受监管。", "method": "本文提出了一个全球“计算暂停按钮”的具体框架，这是一个旨在通过限制对计算资源的访问来防止危险的AI系统被训练的治理系统。该框架识别了三个关键干预点：技术、可追溯性和监管，并将它们组织在一个治理-执行-验证（GEV）框架内。技术机制包括防篡改的FLOP上限、模型锁定和离线许可；可追溯性工具用于跟踪计算供应链中的芯片、组件和用户；监管机制则通过出口管制、生产上限和许可方案来建立限制。", "result": "本文通过借鉴核不扩散和疫情期间疫苗协调等案例，论证了计算能力可以作为全球合作的实用杠杆。它展示了通过控制计算资源来治理AI发展的可行性。", "conclusion": "尽管技术和政治挑战依然存在，但本文认为可信的机制已经存在，并且现在是构建这一架构的关键时刻，以避免有效干预的窗口期关闭。", "translation": "随着人工智能能力的迅速发展，大规模训练运行造成灾难性危害的风险正在增加。然而，支持这种发展的计算基础设施在很大程度上仍然不受监管。本文提出了一个全球“计算暂停按钮”的具体框架：一个旨在通过限制计算资源的访问来防止危险的人工智能系统被训练的治理系统。我们确定了三个关键干预点——技术、可追溯性和监管——并将它们组织在一个治理-执行-验证（GEV）框架内，以确保规则清晰、违规行为可检测、合规性可独立验证。技术机制包括防篡改的FLOP上限、模型锁定和离线许可。可追溯性工具跟踪计算供应链中的芯片、组件和用户。监管机制通过出口管制、生产上限和许可方案建立限制。与部署后监督不同，这种方法针对的是先进人工智能开发的物质基础。借鉴从核不扩散到疫情期间疫苗协调的类似经验，我们展示了计算如何成为全球合作的实用杠杆。尽管技术和政治挑战依然存在，但我们认为可信的机制已经存在，并且现在是构建这一架构的关键时刻，以免有效干预的窗口期关闭。", "summary": "本文针对人工智能能力快速发展带来的潜在灾难性危害，以及计算基础设施缺乏监管的现状，提出了一项全球性的“计算暂停按钮”治理框架。该框架旨在通过限制计算资源来阻止危险人工智能系统的训练。它包含技术、可追溯性和监管三个关键干预点，并整合至一个治理-执行-验证（GEV）模型中，以确保规则的清晰、可检测性和可验证性。该方法着眼于人工智能开发的物质基础，并借鉴了核不扩散等国际合作模式，强调了立即构建此架构的紧迫性。", "keywords": "AI治理, 计算资源, 监管框架, 全球合作, 暂停按钮", "comments": "本文提出了一种创新且具有前瞻性的AI治理思路，将监管重点从AI部署后转移到其开发前期的计算资源控制上，这一点非常重要。其借鉴核不扩散的类比，突显了该问题的重要性。然而，在全球范围内实施这样一个“暂停按钮”的政治和技术挑战是巨大的，需要深入探讨其可行性和潜在的副作用。"}}
{"id": "2506.20475", "title": "Learning-based safety lifting monitoring system for cranes on construction sites", "authors": ["Hao Chen", "Yu Hin Ng", "Ching-Wei Chang", "Haobo Liang", "Yanke Wang"], "summary": "Lifting on construction sites, as a frequent operation, works still with\nsafety risks, especially for modular integrated construction (MiC) lifting due\nto its large weight and size, probably leading to accidents, causing damage to\nthe modules, or more critically, posing safety hazards to on-site workers.\nAiming to reduce the safety risks in lifting scenarios, we design an automated\nsafe lifting monitoring algorithm pipeline based on learning-based methods, and\ndeploy it on construction sites. This work is potentially to increase the\nsafety and efficiency of MiC lifting process via automation technologies. A\ndataset is created consisting of 1007 image-point cloud pairs (37 MiC\nliftings). Advanced object detection models are trained for automated\ntwo-dimensional (2D) detection of MiCs and humans. Fusing the 2D detection\nresults with the point cloud information allows accurate determination of the\nthree-dimensional (3D) positions of MiCs and humans. The system is designed to\nautomatically trigger alarms that notify individuals in the MiC lifting danger\nzone, while providing the crane operator with real-time lifting information and\nearly warnings. The monitoring process minimizes the human intervention and no\nor less signal men are required on real sites assisted by our system. A\nquantitative analysis is conducted to evaluate the effectiveness of the\nalgorithmic pipeline. The pipeline shows promising results in MiC and human\nperception with the mean distance error of 1.5640 m and 0.7824 m respectively.\nFurthermore, the developed system successfully executes safety risk monitoring\nand alarm functionalities during the MiC lifting process with limited manual\nwork on real construction sites.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.20475v1", "categories": ["eess.SY", "cs.SY", "eess.IV"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20475v1", "AI": {"title_translation": "学习型起重机施工现场安全起重监测系统", "tldr": "本研究开发了一个基于学习的自动化系统，用于监测施工现场起重作业的安全，特别是针对模块化集成建筑（MiC）起重，通过融合2D图像和3D点云数据实现目标检测和实时预警，旨在减少风险并提高效率。", "motivation": "施工现场的起重作业，特别是模块化集成建筑（MiC）的起重，由于其大重量和尺寸，存在显著的安全风险，可能导致事故、模块损坏或对现场工人造成安全危害。本研究旨在降低起重场景中的安全风险。", "method": "设计并部署了一个基于学习方法的自动化安全起重监测算法流程。创建了一个包含1007个图像-点云对的数据集。训练了先进的目标检测模型用于MiC和人类的2D检测，并将2D检测结果与点云信息融合以准确确定MiC和人类的3D位置。系统自动触发警报以通知危险区内的个人，并向起重机操作员提供实时信息和预警。该过程最大限度地减少了人工干预。", "result": "该算法流程在MiC和人体感知方面显示出有希望的结果，平均距离误差分别为1.5640米和0.7824米。开发的系统在MiC起重过程中成功执行了安全风险监测和警报功能，且现场所需人工工作量有限。", "conclusion": "本研究开发的基于学习的自动化安全起重监测系统能够有效降低施工现场起重作业的安全风险，通过精确的目标感知和实时预警，显著减少了人工干预，提高了MiC起重过程的安全性和效率。", "translation": "施工现场的起重作业作为一项频繁的操作，仍然存在安全风险，特别是模块化集成建筑（MiC）的起重，由于其重量和尺寸巨大，可能导致事故，造成模块损坏，或者更严重的是，对现场工人构成安全隐患。为了降低起重场景中的安全风险，我们设计了一个基于学习方法的自动化安全起重监测算法流程，并将其部署在施工现场。这项工作有可能通过自动化技术提高MiC起重过程的安全性和效率。创建了一个包含1007个图像-点云对（37次MiC起重）的数据集。训练了先进的目标检测模型，用于MiC和人类的自动化二维（2D）检测。将2D检测结果与点云信息融合，可以准确确定MiC和人类的三维（3D）位置。该系统旨在自动触发警报，通知MiC起重危险区内的个人，同时向起重机操作员提供实时起重信息和早期预警。监测过程最大限度地减少了人工干预，在我们的系统辅助下，现场所需信号员更少或没有。进行了定量分析以评估算法流程的有效性。该流程在MiC和人体感知方面显示出有希望的结果，平均距离误差分别为1.5640米和0.7824米。此外，开发的系统在实际施工现场的MiC起重过程中，以有限的人工工作成功执行了安全风险监测和警报功能。", "summary": "本研究开发并部署了一个基于学习的自动化安全监测系统，旨在降低施工现场，特别是模块化集成建筑（MiC）起重作业的安全风险。该系统通过融合2D图像和3D点云数据，实现MiC和人员的精确检测与定位，并能实时发出危险警报。实验结果表明，该系统在目标感知和安全风险监测方面表现良好，有效减少了人工干预，提升了起重作业的安全性和效率。", "keywords": "安全监测, 起重机, 模块化集成建筑, 目标检测, 点云", "comments": "该论文提出了一种创新的学习型系统，通过结合2D图像和3D点云数据，实现了施工现场起重作业的自动化安全监测。其亮点在于实时预警和减少人工干预的能力，这对于提高施工效率和保障工人安全具有重要意义。然而，系统在不同环境光照、遮挡情况下的鲁棒性以及在更复杂、动态的真实施工场景中的长期稳定性可能需要进一步验证。"}}
{"id": "2506.20067", "title": "Near-Field Energy Harvesting Using XL-MIMO Over Non-Stationary Channels", "authors": ["Muhammad Zeeshan Mumtaz", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "summary": "This paper explores the maximization of the harvested power efficiency (HPE)\nin a modular extremely large multiple-input multiple-output (XL-MIMO) system,\nwhich supports energy harvesting (EH) for near-field users. These users are\nlocated in spatially distinct visibility regions (VRs) with non-stationary\nchannel characteristics. We propose to determine which sub-arrays are switched\non or off as well the power control coefficients at the sub-arrays to maximize\nthe HPE. The design can be processed via a multi-tier joint optimization\nframework based on fractional programming. The numerical results showcase that\nthe HPE performance of the proposed algorithm is nearly optimal, comparable to\nthat of exhaustive search. As a matter of fact, it achieves up to a 120% gain\nover the benchmark scheme which uses the entire XL-MIMO array with equal power\nallocation (PA) across sub-arrays, while significantly reducing the\ncomputational time.", "comment": "IEEE Wireless Communications Letters - Accepted for the next issue\n  publication", "pdf_url": "http://arxiv.org/pdf/2506.20067v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20067v1", "AI": {"title_translation": "在非平稳信道上使用超大规模多输入多输出（XL-MIMO）的近场能量收集", "tldr": "本文提出了一种基于分式规划的多层联合优化框架，用于在非平稳信道下，通过子阵列开关和功率控制，最大化XL-MIMO系统中近场用户的能量收集效率，并显示出接近最优的性能和显著的增益。", "motivation": "本研究旨在最大化模块化超大规模多输入多输出（XL-MIMO）系统中近场用户的能量收集效率（HPE），这些用户位于具有非平稳信道特性的空间不同可见区域。", "method": "本文提出通过确定子阵列的开关状态以及子阵列的功率控制系数来最大化能量收集效率。该设计可以通过基于分式规划的多层联合优化框架进行处理。", "result": "数值结果表明，所提出算法的能量收集效率性能接近最优，与穷举搜索相当。实际上，它比使用整个XL-MIMO阵列并对子阵列进行相等功率分配的基准方案实现了高达120%的增益，同时显著减少了计算时间。", "conclusion": "所提出的算法能够有效地最大化XL-MIMO系统中近场用户的能量收集效率，性能接近最优，并大幅优于基准方案，同时具有计算效率高的优点。", "translation": "本文探讨了在模块化超大规模多输入多输出（XL-MIMO）系统中最大化能量收集效率（HPE）的问题，该系统支持近场用户的能量收集（EH）。这些用户位于具有非平稳信道特性的空间不同可见区域（VRs）。我们提出确定哪些子阵列被开启或关闭，以及子阵列的功率控制系数，以最大化HPE。该设计可以通过基于分式规划的多层联合优化框架进行处理。数值结果表明，所提出算法的HPE性能接近最优，与穷举搜索相当。事实上，它比使用整个XL-MIMO阵列并对子阵列进行相等功率分配（PA）的基准方案实现了高达120%的增益，同时显著减少了计算时间。", "summary": "本研究提出了一种在XL-MIMO系统中，针对近场用户在非平稳信道下的能量收集效率（HPE）最大化方法。通过多层联合优化框架，该方法能够智能地控制子阵列的开关状态和功率分配，以实现HPE的最大化。实验结果表明，该方法不仅能达到接近最优的性能，比基准方案提升高达120%，而且显著降低了计算复杂度。", "keywords": "近场能量收集, XL-MIMO, 非平稳信道, 能量收集效率, 分式规划", "comments": "本文的创新点在于提出了一个基于分式规划的多层联合优化框架，用于XL-MIMO系统中近场用户的能量收集效率最大化，特别是在非平稳信道条件下。其重要性体现在实现了接近最优的性能，并相较于传统方法获得了显著的增益（高达120%），同时还大幅减少了计算时间，这对于实际部署具有重要意义。"}}
{"id": "2506.20303", "title": "FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment", "authors": ["Lee Qi Zun", "Oscar Wong Jin Hao", "Nor Anita Binti Che Omar", "Zalifa Zakiah Binti Asnir", "Mohamad Sabri bin Sinal Zainal", "Goh Man Fye"], "summary": "Automated fundus image quality assessment (FIQA) remains a challenge due to\nvariations in image acquisition and subjective expert evaluations. We introduce\nFundaQ-8, a novel expert-validated framework for systematically assessing\nfundus image quality using eight critical parameters, including field coverage,\nanatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a\nstructured scoring reference, we develop a ResNet18-based regression model to\npredict continuous quality scores in the 0 to 1 range. The model is trained on\n1800 fundus images from real-world clinical sources and Kaggle datasets, using\ntransfer learning, mean squared error optimization, and standardized\npreprocessing. Validation against the EyeQ dataset and statistical analyses\nconfirm the framework's reliability and clinical interpretability.\nIncorporating FundaQ-8 into deep learning models for diabetic retinopathy\ngrading also improves diagnostic robustness, highlighting the value of\nquality-aware training in real-world screening applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20303v1", "categories": ["eess.IV", "cs.CL", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20303v1", "AI": {"title_translation": "FundaQ-8：一种受临床启发的自动化眼底图像质量评估评分框架", "tldr": "FundaQ-8是一个新的专家验证框架，用于系统评估眼底图像质量，并基于此开发了一个ResNet18模型来预测质量分数。该框架提高了糖尿病视网膜病变分级模型的诊断鲁棒性。", "motivation": "由于图像采集的变化和主观的专家评估，自动化眼底图像质量评估（FIQA）仍然是一个挑战。", "method": "本文引入了FundaQ-8，一个新颖的、经过专家验证的框架，使用八个关键参数（包括视野覆盖、解剖可见性、照明和图像伪影）系统地评估眼底图像质量。基于FundaQ-8作为结构化评分参考，开发了一个基于ResNet18的回归模型来预测0到1范围内的连续质量分数。该模型在1800张来自真实临床来源和Kaggle数据集的眼底图像上进行训练，并使用了迁移学习、均方误差优化和标准化预处理。", "result": "FundaQ-8框架通过针对EyeQ数据集的验证和统计分析，证实了其可靠性和临床可解释性。将FundaQ-8整合到用于糖尿病视网膜病变分级的深度学习模型中，也提高了诊断鲁棒性。", "conclusion": "FundaQ-8框架及其基于深度学习的质量评估模型能够可靠地评估眼底图像质量，并通过质量感知训练提高真实世界筛查应用中疾病诊断的鲁棒性。", "translation": "自动化眼底图像质量评估（FIQA）由于图像采集的变化和主观的专家评估而仍然是一个挑战。我们引入了 FundaQ-8，一个新颖的、经过专家验证的框架，用于使用八个关键参数系统地评估眼底图像质量，这些参数包括视野覆盖、解剖可见性、照明和图像伪影。使用 FundaQ-8 作为结构化评分参考，我们开发了一个基于 ResNet18 的回归模型，以预测 0 到 1 范围内的连续质量分数。该模型在 1800 张来自真实临床来源和 Kaggle 数据集的眼底图像上进行训练，使用了迁移学习、均方误差优化和标准化预处理。针对 EyeQ 数据集的验证和统计分析证实了该框架的可靠性和临床可解释性。将 FundaQ-8 整合到用于糖尿病视网膜病变分级的深度学习模型中，也提高了诊断鲁棒性，突出了质量感知训练在真实世界筛查应用中的价值。", "summary": "本文提出了FundaQ-8，一个受临床启发的眼底图像质量评估框架，它使用八个关键参数进行系统性评分。基于FundaQ-8，研究人员开发了一个ResNet18回归模型来预测连续的图像质量分数。该模型在临床和Kaggle数据集上训练，并通过EyeQ数据集验证，证明了其可靠性和临床可解释性。将FundaQ-8整合到糖尿病视网膜病变分级模型中，显著提升了诊断的鲁棒性，强调了质量感知训练在实际医疗应用中的重要性。", "keywords": "眼底图像质量评估, FundaQ-8, 深度学习, 糖尿病视网膜病变, 质量感知训练", "comments": "FundaQ-8的创新之处在于其临床启发和专家验证的八参数评分框架，这为眼底图像质量评估提供了标准化和客观的依据。通过将其整合到深度学习模型中，不仅提高了图像质量评估的准确性，更重要的是，它直接提升了下游疾病诊断的鲁棒性，这对于实际临床筛查具有重要价值。这种将质量评估与诊断结果相结合的方法是其重要的贡献。"}}
{"id": "2506.19999", "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior", "authors": ["Francesco Ignazio Re", "Andreas Opedal", "Glib Manaiev", "Mario Giulianelli", "Ryan Cotterell"], "summary": "Reading is a process that unfolds across space and time, alternating between\nfixations where a reader focuses on a specific point in space, and saccades\nwhere a reader rapidly shifts their focus to a new point. An ansatz of\npsycholinguistics is that modeling a reader's fixations and saccades yields\ninsight into their online sentence processing. However, standard approaches to\nsuch modeling rely on aggregated eye-tracking measurements and models that\nimpose strong assumptions, ignoring much of the spatio-temporal dynamics that\noccur during reading. In this paper, we propose a more general probabilistic\nmodel of reading behavior, based on a marked spatio-temporal point process,\nthat captures not only how long fixations last, but also where they land in\nspace and when they take place in time. The saccades are modeled using a Hawkes\nprocess, which captures how each fixation excites the probability of a new\nfixation occurring near it in time and space. The duration time of fixation\nevents is modeled as a function of fixation-specific predictors convolved\nacross time, thus capturing spillover effects. Empirically, our Hawkes process\nmodel exhibits a better fit to human saccades than baselines. With respect to\nfixation durations, we observe that incorporating contextual surprisal as a\npredictor results in only a marginal improvement in the model's predictive\naccuracy. This finding suggests that surprisal theory struggles to explain\nfine-grained eye movements.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.19999v1", "categories": ["cs.LG", "cs.CL", "q-bio.NC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19999v1", "AI": {"title_translation": "一种用于阅读行为精细建模的时空点过程", "tldr": "本文提出了一种基于标记时空点过程的概率模型，用于精细建模阅读行为中的注视和眼跳，该模型优于基线模型，并发现上下文惊奇度理论难以解释精细眼动。", "motivation": "标准阅读行为建模方法依赖于聚合的眼动测量和强假设模型，忽略了阅读过程中大量的时空动态。", "method": "本文提出了一种基于标记时空点过程的概率模型来捕捉注视的持续时间、空间位置和发生时间。眼跳使用霍克斯过程建模，以捕捉每次注视如何激发附近新注视发生的概率。注视事件的持续时间建模为与注视特定预测因子在时间上卷积的函数，以捕捉溢出效应。", "result": "经验上，我们的霍克斯过程模型比基线模型更好地拟合人类眼跳。对于注视持续时间，我们观察到纳入上下文惊奇度作为预测因子仅导致模型预测准确性略微提高。", "conclusion": "上下文惊奇度理论难以解释精细的眼动。", "translation": "阅读是一个跨越空间和时间展开的过程，在读者将注意力集中在空间中特定点的注视和读者快速将注意力转移到新点的眼跳之间交替进行。心理语言学的一个假设是，对读者注视和眼跳的建模可以深入了解他们的在线句子处理。然而，这种建模的标准方法依赖于聚合的眼动追踪测量和施加强假设的模型，忽略了阅读过程中发生的许多时空动态。在本文中，我们提出了一种更通用的阅读行为概率模型，基于标记的时空点过程，该模型不仅捕捉注视持续的时间，还捕捉它们在空间中的位置和发生的时间。眼跳使用霍克斯过程建模，该过程捕捉每次注视如何激发附近新注视在时间和空间上发生的概率。注视事件的持续时间建模为与注视特定预测因子在时间上卷积的函数，从而捕捉溢出效应。经验上，我们的霍克斯过程模型比基线模型更好地拟合人类眼跳。对于注视持续时间，我们观察到纳入上下文惊奇度作为预测因子仅导致模型预测准确性略微提高。这一发现表明惊奇度理论难以解释精细的眼动。", "summary": "本文提出了一种基于标记时空点过程的概率模型，用于精细建模阅读行为中的注视和眼跳。该模型捕捉注视的持续时间、空间位置和发生时间，并使用霍克斯过程建模眼跳以捕获注视间的激发效应，同时通过卷积函数建模注视持续时间以捕捉溢出效应。实验结果显示，该霍克斯过程模型在拟合人类眼跳方面优于基线模型，但上下文惊奇度作为预测因子对注视持续时间的预测准确性提升有限，这表明惊奇度理论在解释精细眼动方面存在局限性。", "keywords": "阅读行为, 时空点过程, 眼跳, 霍克斯过程, 注视持续时间", "comments": "该论文创新性地将标记时空点过程和霍克斯过程应用于阅读行为建模，特别是对眼跳的时空动态进行了更精细的刻画，并考虑了溢出效应。其重要性在于提供了一种更通用的概率框架来理解阅读过程中的眼动模式，突破了传统方法的局限。然而，论文也揭示了现有心理语言学理论（如惊奇度理论）在解释精细眼动数据方面的不足，这为未来的研究指明了方向。"}}
{"id": "2506.20652", "title": "EditP23: 3D Editing via Propagation of Image Prompts to Multi-View", "authors": ["Roi Bar-On", "Dana Cohen-Bar", "Daniel Cohen-Or"], "summary": "We present EditP23, a method for mask-free 3D editing that propagates 2D\nimage edits to multi-view representations in a 3D-consistent manner. In\ncontrast to traditional approaches that rely on text-based prompting or\nexplicit spatial masks, EditP23 enables intuitive edits by conditioning on a\npair of images: an original view and its user-edited counterpart. These image\nprompts are used to guide an edit-aware flow in the latent space of a\npre-trained multi-view diffusion model, allowing the edit to be coherently\npropagated across views. Our method operates in a feed-forward manner, without\noptimization, and preserves the identity of the original object, in both\nstructure and appearance. We demonstrate its effectiveness across a range of\nobject categories and editing scenarios, achieving high fidelity to the source\nwhile requiring no manual masks.", "comment": "Code, supplementary videos, interactive 3D visualizations, and\n  additional results are available at https://editp23.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.20652v1", "categories": ["cs.GR", "cs.CV", "68U05 (Primary), 68T45 (Secondary)", "I.3.7; I.3.8; I.4.9"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20652v1", "AI": {"title_translation": "EditP23：通过图像提示传播到多视图实现3D编辑", "tldr": "EditP23是一种无需遮罩的3D编辑方法，通过将2D图像编辑一致地传播到多视图表示，实现直观的3D对象编辑。", "motivation": "传统3D编辑方法依赖于文本提示或显式空间遮罩，不够直观。本研究旨在提供一种更直观、无需遮罩的3D编辑方法。", "method": "EditP23通过一对图像（原始视图和用户编辑后的视图）作为提示，在预训练的多视图扩散模型的潜在空间中引导编辑感知流，从而将2D图像编辑一致地传播到多视图表示。该方法以端到端方式运行，无需优化，并能保留原始对象的结构和外观。", "result": "该方法在多种对象类别和编辑场景中表现出有效性，实现了对源图像的高保真度编辑，且无需手动遮罩。", "conclusion": "EditP23提供了一种新颖、高效且无需遮罩的3D编辑范式，通过利用图像提示和多视图扩散模型，实现了对3D对象结构和外观一致的编辑。", "translation": "我们提出了EditP23，这是一种无需遮罩的3D编辑方法，它以3D一致的方式将2D图像编辑传播到多视图表示。与依赖文本提示或显式空间遮罩的传统方法不同，EditP23通过以一对图像（原始视图及其用户编辑后的对应视图）为条件，实现直观的编辑。这些图像提示用于在预训练的多视图扩散模型的潜在空间中引导编辑感知流，从而使编辑能够跨视图连贯地传播。我们的方法以端到端方式运行，无需优化，并在结构和外观上都保留了原始对象的身份。我们展示了它在各种对象类别和编辑场景中的有效性，实现了对源图像的高保真度，同时无需手动遮罩。", "summary": "EditP23是一种创新的无遮罩3D编辑技术，它通过将用户编辑的2D图像作为提示，在预训练的多视图扩散模型的潜在空间中引导编辑传播，从而以3D一致的方式实现对多视图表示的编辑。该方法无需传统文本或遮罩输入，以端到端方式运行，并能高保真地保留原始对象特性，适用于多种对象和编辑场景。", "keywords": "3D编辑, 图像提示, 多视图扩散模型, 无遮罩, 潜在空间编辑", "comments": "EditP23的创新之处在于其“无遮罩”和“图像提示”的3D编辑范式，这与传统依赖文本或显式遮罩的方法形成鲜明对比，极大地提升了用户交互的直观性。其在多视图扩散模型潜在空间中引导编辑流的机制，保证了3D一致性，同时避免了耗时的优化过程。该方法在保留原始对象身份的同时实现高保真编辑，显示出其在实际应用中的巨大潜力。"}}
{"id": "2506.20330", "title": "Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search", "authors": ["Zhigong Zhou", "Ning Ding", "Xiaochuan Fan", "Yue Shang", "Yiming Qiu", "Jingwei Zhuo", "Zhiwei Ge", "Songlin Wang", "Lin Liu", "Sulong Xu", "Han Zhang"], "summary": "Semantic retrieval, which retrieves semantically matched items given a\ntextual query, has been an essential component to enhance system effectiveness\nin e-commerce search. In this paper, we study the multimodal retrieval problem,\nwhere the visual information (e.g, image) of item is leveraged as supplementary\nof textual information to enrich item representation and further improve\nretrieval performance. Though learning from cross-modality data has been\nstudied extensively in tasks such as visual question answering or media\nsummarization, multimodal retrieval remains a non-trivial and unsolved problem\nespecially in the asymmetric scenario where the query is unimodal while the\nitem is multimodal. In this paper, we propose a novel model named SMAR, which\nstands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the\nproblem of modality fusion and alignment in this kind of asymmetric scenario.\nExtensive experimental results on an industrial dataset show that the proposed\nmodel outperforms baseline models significantly in retrieval accuracy. We have\nopen sourced our industrial dataset for the sake of reproducibility and future\nresearch works.", "comment": "published in sigir2023", "pdf_url": "http://arxiv.org/pdf/2506.20330v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20330v1", "AI": {"title_translation": "在线电商搜索中的语义增强模态非对称检索", "tldr": "提出SMAR模型，解决电商搜索中查询单模态而商品多模态的非对称检索问题，并显著提升检索准确性。", "motivation": "电商搜索中，语义检索是提升系统有效性的关键。尽管跨模态数据学习已被广泛研究，但在查询为单模态、商品为多模态的非对称场景下，多模态检索仍是一个非平凡且未解决的难题。", "method": "提出了一种名为SMAR（Semantic-enhanced Modality-Asymmetric Retrieval）的新模型，旨在解决电商搜索中查询单模态而商品多模态的非对称场景下的模态融合和对齐问题。", "result": "在工业数据集上的大量实验结果表明，所提出的SMAR模型在检索准确性方面显著优于基线模型。", "conclusion": "SMAR模型有效解决了电商搜索中非对称多模态检索的挑战，显著提升了检索性能，并且开放了工业数据集以促进后续研究。", "translation": "语义检索，即根据文本查询检索语义匹配的商品，一直是提升电商搜索系统有效性的重要组成部分。在本文中，我们研究了多模态检索问题，其中商品的视觉信息（例如图像）被用作文本信息的补充，以丰富商品表示并进一步提高检索性能。尽管跨模态数据学习在视觉问答或媒体摘要等任务中已被广泛研究，但多模态检索仍然是一个非平凡且未解决的问题，尤其是在查询是单模态而商品是多模态的非对称场景中。在本文中，我们提出了一种名为SMAR（Semantic-enhanced Modality-Asymmetric Retrieval）的新模型，以解决这种非对称场景中的模态融合和对齐问题。在工业数据集上的大量实验结果表明，所提出的模型在检索准确性方面显著优于基线模型。我们已经开源了我们的工业数据集，以促进可复现性和未来的研究工作。", "summary": "本文针对电商搜索中查询为单模态、商品为多模态的非对称检索难题，提出了一种名为SMAR的语义增强模态非对称检索模型。该模型旨在解决模态融合与对齐问题。实验结果表明，SMAR在工业数据集上显著提升了检索准确性，且研究者已开源数据集以供后续研究。", "keywords": "电商搜索, 多模态检索, 语义增强, 非对称检索, SMAR", "comments": "这篇论文的创新点在于提出了SMAR模型来专门解决电商搜索领域中独特的“模态非对称”检索问题，即用户查询通常是文本（单模态），而商品信息包含文本和图像（多模态）。这种场景在实际应用中非常普遍但具有挑战性。开放工业数据集的举措对于促进该领域的可复现性和进一步研究具有重要意义。"}}
{"id": "2506.20412", "title": "Cut-Query Algorithms with Few Rounds", "authors": ["Yotam Kenneth-Mordoch", "Robert Krauthgamer"], "summary": "In the cut-query model, the algorithm can access the input graph $G=(V,E)$\nonly via cut queries that report, given a set $S\\subseteq V$, the total weight\nof edges crossing the cut between $S$ and $V\\setminus S$. This model was\nintroduced by Rubinstein, Schramm and Weinberg [ITCS'18] and its investigation\nhas so far focused on the number of queries needed to solve optimization\nproblems, such as global minimum cut. We turn attention to the round complexity\nof cut-query algorithms, and show that several classical problems can be solved\nin this model with only a constant number of rounds.\n  Our main results are algorithms for finding a minimum cut in a graph, that\noffer different tradeoffs between round complexity and query complexity, where\n$n=|V|$ and $\\delta(G)$ denotes the minimum degree of $G$: (i)\n$\\tilde{O}(n^{4/3})$ cut queries in two rounds in unweighted graphs; (ii)\n$\\tilde{O}(rn^{1+1/r}/\\delta(G)^{1/r})$ queries in $2r+1$ rounds for any\ninteger $r\\ge 1$ again in unweighted graphs; and (iii)\n$\\tilde{O}(rn^{1+(1+\\log_n W)/r})$ queries in $4r+3$ rounds for any $r\\ge1$ in\nweighted graphs. We also provide algorithms that find a minimum $(s,t)$-cut and\napproximate the maximum cut in a few rounds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20412v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20412v1", "AI": {"title_translation": "剪切查询算法与少量轮次", "tldr": "本文研究了剪切查询模型中算法的轮次复杂度，并提出了在少量轮次内解决最小割等问题的算法。", "motivation": "现有的剪切查询模型研究主要集中在解决优化问题所需的查询次数上，而本文旨在将注意力转向算法的轮次复杂度，并证明多个经典问题可以在该模型中以常数轮次解决。", "method": "作者提出了一系列用于在图中寻找最小割的算法，这些算法在轮次复杂度和查询复杂度之间提供了不同的权衡。他们还提供了寻找最小(s,t)-割和近似最大割的算法。", "result": "1. 在无权图中，以两轮实现 $\\tilde{O}(n^{4/3})$ 次剪切查询。\n2. 在无权图中，对于任意整数 $r\\ge 1$，以 $2r+1$ 轮实现 $\\tilde{O}(rn^{1+1/r}/\\delta(G)^{1/r})$ 次查询。\n3. 在加权图中，对于任意 $r\\ge1$，以 $4r+3$ 轮实现 $\\tilde{O}(rn^{1+(1+\\log_n W)/r})$ 次查询。\n4. 还提供了在少量轮次内寻找最小(s,t)-割和近似最大割的算法。", "conclusion": "本文证明了在剪切查询模型中，多个经典问题（如最小割、最小(s,t)-割和近似最大割）可以在仅常数轮次内解决，并提供了具有不同轮次-查询复杂度权衡的算法。", "translation": "在剪切查询模型中，算法只能通过剪切查询访问输入图 $G=(V,E)$，剪切查询报告给定集合 $S\\subseteq V$ 时，穿过 $S$ 和 $V\\setminus S$ 之间割边的总权重。该模型由 Rubinstein、Schramm 和 Weinberg [ITCS'18] 引入，迄今为止其研究主要集中在解决优化问题（如全局最小割）所需的查询次数上。我们将注意力转向剪切查询算法的轮次复杂度，并表明在该模型中，几个经典问题可以在仅常数轮次内解决。\n我们的主要成果是用于在图中查找最小割的算法，这些算法在轮次复杂度和查询复杂度之间提供了不同的权衡，其中 $n=|V|$ 且 $\\delta(G)$ 表示 $G$ 的最小度：(i) 在无权图中，两轮内完成 $\\tilde{O}(n^{4/3})$ 次剪切查询；(ii) 在无权图中，对于任何整数 $r\\ge 1$，在 $2r+1$ 轮内完成 $\\tilde{O}(rn^{1+1/r}/\\delta(G)^{1/r})$ 次查询；以及 (iii) 在加权图中，对于任何 $r\\ge1$，在 $4r+3$ 轮内完成 $\\tilde{O}(rn^{1+(1+\\log_n W)/r})$ 次查询。我们还提供了在少量轮次内查找最小 $(s,t)$-割和近似最大割的算法。", "summary": "本文探讨了剪切查询模型中算法的轮次复杂度，与以往关注查询次数的研究不同。作者证明了在仅常数轮次内可以解决包括全局最小割、最小(s,t)-割和近似最大割在内的多个经典图优化问题。论文提出了多种最小割算法，提供了轮次复杂度和查询复杂度之间的不同权衡，具体包括无权图中的两轮 $\\tilde{O}(n^{4/3})$ 查询算法以及针对不同轮次数的更通用算法，同时还涵盖了加权图的情况。", "keywords": "剪切查询模型, 轮次复杂度, 最小割, 图算法, 查询复杂度", "comments": "这篇论文通过将研究重点从查询次数转移到轮次复杂度，为剪切查询模型带来了新的视角。其创新之处在于证明了在严格的常数轮次限制下，仍然可以高效地解决核心图问题，这对于分布式或并行计算环境下的图算法设计具有重要意义。论文提出的具体算法及其复杂度权衡是其主要贡献。"}}
{"id": "2506.20188", "title": "DefElement: an encyclopedia of finite element definitions", "authors": ["Matthew W. Scroggs", "Pablo D. Brubeck", "Joseph P. Dean", "Jørgen S. Dokken", "India Marsden"], "summary": "DefElement is an online encyclopedia of finite element definitions that was\ncreated and is maintained by the authors of this paper. DefElement aims to make\ninformation about elements defined in the literature easily available in a\nstandard format. There are a number of open-source finite element libraries\navailable, and it can be difficult to check that an implementation of an\nelement in a library matches the element's definition in the literature or\nimplementation in another library, especially when many libraries include\nvariants of elements whose basis functions do not match exactly. In this paper,\nwe carefully derive conditions under which elements can be considered\nequivalent and describe an algorithm that uses these conditions to verify that\ntwo implementations of a finite element are indeed variants of the same\nelement. The results of scheduled runs of our implementation of this\nverification algorithm are included in the information available on DefElement.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2506.20188v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20188v1", "AI": {"title_translation": "DefElement：有限元定义百科全书", "tldr": "DefElement是一个在线有限元定义百科全书，旨在标准化和方便地提供有限元信息。本文提出了一种算法，用于验证不同有限元库中元素的等效性，其验证结果已整合到DefElement中。", "motivation": "现有的开源有限元库中，难以验证某个元素的实现是否与文献定义或其他库的实现一致，特别是当许多库包含基函数不完全匹配的变体时。DefElement旨在以标准格式方便地提供文献中定义的元素信息。", "method": "本文仔细推导了有限元被认为是等效的条件，并描述了一种算法，该算法利用这些条件来验证两个有限元实现是否确实是同一元素的变体。", "result": "作者实现的验证算法的定期运行结果已包含在DefElement上可用的信息中。", "conclusion": "DefElement提供了一个标准化的有限元定义资源，并通过提出的验证算法解决了不同有限元实现之间等效性验证的难题，提升了有限元库的可靠性。", "translation": "DefElement是一个在线有限元定义百科全书，由本文作者创建和维护。DefElement旨在以标准格式方便地提供文献中定义的元素信息。现有许多开源有限元库，很难检查库中元素的实现是否与文献中的元素定义或其他库中的实现匹配，特别是当许多库包含基函数不完全匹配的元素变体时。在本文中，我们仔细推导了元素可以被认为是等效的条件，并描述了一种算法，该算法使用这些条件来验证两个有限元实现是否确实是同一元素的变体。我们实现的此验证算法的定期运行结果已包含在DefElement上可用的信息中。", "summary": "DefElement是一个由本文作者创建和维护的在线有限元定义百科全书，旨在以标准化格式提供文献中定义的有限元信息。鉴于当前开源有限元库中元素实现一致性验证的困难，特别是存在基函数不完全匹配的变体，本文推导了判断有限元等效性的条件，并提出了一种验证算法。该算法能够验证不同有限元实现是否为同一元素的变体，其定期运行结果已集成到DefElement中。", "keywords": "有限元, 定义, 百科全书, 验证, DefElement", "comments": "本文解决了有限元领域一个实际且重要的问题：如何验证不同有限元库中元素实现的等效性。通过创建DefElement百科全书和开发验证算法，作者为有限元研究人员和开发者提供了一个宝贵的资源和工具，有助于提高有限元代码的可靠性和互操作性。其创新点在于对等效性条件的严谨推导和实用算法的提出。"}}
{"id": "2506.20268", "title": "Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue", "authors": ["Ruben Janssens", "Jens De Bock", "Sofie Labat", "Eva Verhelst", "Veronique Hoste", "Tony Belpaeme"], "summary": "Detecting miscommunication in human-robot interaction is a critical function\nfor maintaining user engagement and trust. While humans effortlessly detect\ncommunication errors in conversations through both verbal and non-verbal cues,\nrobots face significant challenges in interpreting non-verbal feedback, despite\nadvances in computer vision for recognizing affective expressions. This\nresearch evaluates the effectiveness of machine learning models in detecting\nmiscommunications in robot dialogue. Using a multi-modal dataset of 240\nhuman-robot conversations, where four distinct types of conversational failures\nwere systematically introduced, we assess the performance of state-of-the-art\ncomputer vision models. After each conversational turn, users provided feedback\non whether they perceived an error, enabling an analysis of the models' ability\nto accurately detect robot mistakes. Despite using state-of-the-art models, the\nperformance barely exceeds random chance in identifying miscommunication, while\non a dataset with more expressive emotional content, they successfully\nidentified confused states. To explore the underlying cause, we asked human\nraters to do the same. They could also only identify around half of the induced\nmiscommunications, similarly to our model. These results uncover a fundamental\nlimitation in identifying robot miscommunications in dialogue: even when users\nperceive the induced miscommunication as such, they often do not communicate\nthis to their robotic conversation partner. This knowledge can shape\nexpectations of the performance of computer vision models and can help\nresearchers to design better human-robot conversations by deliberately\neliciting feedback where needed.", "comment": "Accepted at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20268v1", "categories": ["cs.RO", "cs.CL", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20268v1", "AI": {"title_translation": "为什么机器人不擅长检测自身错误：人机对话中误解检测的局限性", "tldr": "机器人和人类一样，在人机对话中检测机器人误解的能力很差，因为用户即使察觉到错误也往往不会将其明确传达给机器人。", "motivation": "在人机交互中检测误解对于维持用户参与度和信任至关重要。尽管人类可以轻易通过语言和非语言线索检测到交流错误，但机器人在这方面面临挑战，尤其是在解释非语言反馈方面。本研究旨在评估机器学习模型在检测机器人对话中误解的有效性。", "method": "研究使用包含240个人机对话的多模态数据集，其中系统地引入了四种不同类型的对话失败。在每次对话轮次后，用户提供是否感知到错误的反馈。研究评估了最先进的计算机视觉模型的性能，并让人类评估者进行相同的检测以探索根本原因。", "result": "尽管使用了最先进的模型，但其检测误解的性能仅略高于随机猜测；然而，在情感表达更丰富的数据集上，模型成功识别了困惑状态。人类评估者也只能识别出大约一半的诱导误解，与模型表现相似。", "conclusion": "研究揭示了在对话中识别机器人误解的一个根本局限性：即使用户感知到诱导的误解，他们也常常不会将其传达给机器人对话伙伴。这一发现可以帮助校准对计算机视觉模型性能的预期，并指导研究人员设计更好地人机对话，在需要时主动引导反馈。", "translation": "在人机交互中检测误解是维持用户参与度和信任的关键功能。尽管人类在对话中能轻易通过语言和非语言线索检测到交流错误，但机器人面临着解释非语言反馈的重大挑战，即便计算机视觉在识别情感表达方面取得了进展。本研究评估了机器学习模型在检测机器人对话中误解的有效性。我们使用包含240个人机对话的多模态数据集，其中系统地引入了四种不同类型的对话失败，评估了最先进的计算机视觉模型的性能。在每次对话轮次后，用户提供他们是否感知到错误的反馈，从而能够分析模型准确检测机器人错误的能力。尽管使用了最先进的模型，但在识别误解方面的性能仅略高于随机猜测，而在情感表达更丰富的数据集上，它们成功识别了困惑状态。为了探索根本原因，我们请人类评估者也做了同样的事情。他们也只能识别出大约一半的诱导误解，与我们的模型相似。这些结果揭示了在对话中识别机器人误解的一个根本局限性：即使用户感知到诱导的误解，他们也常常不会将其传达给他们的机器人对话伙伴。这一知识可以塑造对计算机视觉模型性能的预期，并可以帮助研究人员通过在需要时主动引导反馈来设计更好的人机对话。", "summary": "这项研究探讨了机器人在人机对话中检测自身误解的局限性。通过分析包含系统性引入对话失败的多模态数据集，研究发现即使是最先进的机器学习模型，在识别机器人误解方面的表现也仅略高于随机猜测。进一步的实验表明，人类评估者也面临类似困难。核心原因在于，即使用户察觉到机器人犯错，他们也往往不会明确地将这种误解反馈给机器人。这一发现强调了设计人机对话时，需要主动引导用户反馈以提高机器人误解检测能力的重要性。", "keywords": "人机对话, 误解检测, 机器人错误, 计算机视觉, 用户反馈", "comments": "这篇论文揭示了人机交互中一个重要的且被忽视的问题：机器人检测自身错误的根本困难，不仅源于技术限制，更深层次的原因在于人类用户本身不习惯或不倾向于向机器人明确表达其感知到的错误。这为未来人机对话系统的设计提供了新的视角，即与其单纯追求更精密的误解检测算法，不如同时关注如何设计交互机制以鼓励用户提供更明确的反馈。其创新之处在于通过对比人类和机器人的检测能力，揭示了问题的深层根源。"}}
{"id": "2506.20621", "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems", "authors": ["Silvio Alonso", "Antonio Pedro Santos Alves", "Lucas Romao", "Hélio Lopes", "Marcos Kalinowski"], "summary": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20621v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20621v1", "AI": {"title_translation": "Define-ML：一种构思机器学习赋能系统的方法", "tldr": "Define-ML是一个扩展了Lean Inception的框架，通过特定活动解决机器学习系统构思阶段的数据依赖、技术可行性和业务目标对齐问题，并已通过真实案例验证其有效性。", "motivation": "在软件系统中越来越多地采用机器学习(ML)需要专门的构思方法来解决ML特有的挑战，包括数据依赖、技术可行性以及业务目标与概率系统行为之间的一致性。传统的构思方法（如Lean Inception）缺乏对这些ML考量的结构化支持，这可能导致产品愿景不一致和不切实际的期望。", "method": "本文遵循技术转移模型开发并验证了Define-ML，进行了静态验证（使用一个玩具问题）和动态验证（在一个真实的工业案例研究中）。分析结合了定量调查和定性反馈，评估了实用性、易用性和采纳意愿。", "result": "参与者发现Define-ML在澄清数据问题、使ML能力与业务目标对齐以及促进跨职能协作方面是有效的。该方法的结构化活动减少了构思模糊性，尽管一些人指出ML特定组件存在学习曲线，这可以通过专家引导来缓解。所有参与者都表达了采纳Define-ML的意愿。", "conclusion": "Define-ML提供了一种公开可用且经过验证的机器学习产品构思方法，它建立在Lean Inception的敏捷性之上，同时使功能与可用数据对齐，并提高了对技术可行性的认识。", "translation": "[背景] 机器学习（ML）在软件系统中的日益普及要求专门的构思方法来解决ML特有的挑战，包括数据依赖性、技术可行性以及业务目标与概率系统行为之间的一致性。传统的构思方法（如精益启动，Lean Inception）缺乏对这些ML考量的结构化支持，这可能导致产品愿景不一致和不切实际的期望。\n[目标] 本文提出了Define-ML，一个扩展了精益启动的框架，通过定制的活动——数据源映射、特征到数据源映射和ML映射——系统地将数据和技术约束整合到早期ML产品构思中。\n[方法] 我们遵循技术转移模型开发并验证了Define-ML，进行了静态验证（使用一个玩具问题）和动态验证（在一个真实的工业案例研究中）。分析结合了定量调查和定性反馈，评估了实用性、易用性和采纳意愿。\n[结果] 参与者发现Define-ML在澄清数据问题、使ML能力与业务目标对齐以及促进跨职能协作方面是有效的。该方法的结构化活动减少了构思模糊性，尽管一些人指出ML特定组件存在学习曲线，这可以通过专家引导来缓解。所有参与者都表达了采纳Define-ML的意愿。\n[结论] Define-ML提供了一种公开可用且经过验证的ML产品构思方法，它建立在精益启动的敏捷性之上，同时使功能与可用数据对齐，并提高了对技术可行性的认识。", "summary": "本文提出Define-ML，一个扩展了传统精益启动（Lean Inception）的框架，旨在解决机器学习（ML）系统早期构思阶段的特有挑战，如数据依赖、技术可行性和业务目标对齐。Define-ML通过引入数据源映射、特征到数据源映射和ML映射等定制活动，系统地整合数据和技术约束。该方法已通过静态和动态验证，并在真实工业案例中显示出有效性，能够澄清数据问题、对齐ML能力与业务目标并促进跨职能协作，尽管存在学习曲线，但参与者普遍表达了采纳意愿。", "keywords": "机器学习构思, 精益启动, Define-ML, 数据依赖, 技术可行性", "comments": "Define-ML的创新之处在于将传统的软件构思方法（Lean Inception）与机器学习的特定需求相结合，提供了一套结构化的活动来应对ML项目特有的数据和技术挑战。其重要性在于它为ML产品开发提供了一个实用的早期指导框架，有助于减少项目风险并提高成功率。通过在真实工业案例中进行验证，增加了其可信度。然而，抽象中也提到了ML特定组件存在学习曲线，这可能需要专家引导来缓解，这暗示了在实际应用中可能需要额外的支持或培训。"}}
{"id": "2506.20168", "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models", "authors": ["Zhentao He", "Can Zhang", "Ziheng Wu", "Zhenghao Chen", "Yufei Zhan", "Yifan Li", "Zhao Zhang", "Xian Wang", "Minghui Qiu"], "summary": "Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20168v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20168v1", "AI": {"title_translation": "眼见为实？缓解多模态大型语言模型中的OCR幻觉", "tldr": "该研究提出了一个基准数据集KIE-HVQA和一个基于GRPO的框架，以缓解多模态大语言模型在视觉退化文档理解中产生的OCR幻觉。", "motivation": "现有多模态大语言模型在视觉退化条件下理解文档时，对语言先验过度依赖或视觉-文本推理错位，导致生成幻觉内容。", "method": "提出KIE-HVQA基准数据集，用于评估退化文档理解中的OCR幻觉，包含模拟真实世界退化的身份证和发票样本。引入基于GRPO的框架，通过视觉不确定性自我感知和拒绝回答机制，在监督微调和强化学习框架中缓解幻觉。", "result": "在KIE-HVQA数据集上，基于GRPO的7B参数模型在无幻觉准确率上比GPT-4o提高了22%的绝对值，并且在标准任务中没有显著性能下降。", "conclusion": "通过引入视觉不确定性感知和拒绝回答机制，本研究成功缓解了多模态大语言模型在模糊区域的幻觉问题，提高了在退化文档理解中的鲁棒性和准确性。", "translation": "近期多模态大型语言模型的进步通过整合文本和视觉信息增强了文档理解。然而，现有模型在实际场景中，特别是在视觉退化条件下，其范式存在不完整性。在这种情况下，当前的响应范式往往无法充分感知视觉退化和模糊性，导致过度依赖语言先验或视觉-文本推理错位。这种识别不确定性的困难经常导致幻觉内容的生成，尤其是在无法给出精确答案时。为了更好地展示和分析这种现象和问题，我们提出了KIE-HVQA，这是第一个专门用于评估退化文档理解中OCR幻觉的基准。该数据集包含身份证和发票的测试样本，并模拟了真实世界的退化以评估OCR的可靠性。这种设置允许评估模型在退化输入下区分可靠视觉信息并相应回答的能力，从而突出了避免不确定数据上幻觉的挑战。为了实现视觉忠实的推理并从而避免上述问题，我们进一步引入了一个基于GRPO的框架，该框架具有新颖的奖励机制。通过将视觉不确定性的自我感知和一种启动拒绝回答以增加任务难度分析方法纳入我们的监督微调和强化学习框架中，我们成功缓解了模糊区域的幻觉。在Qwen2.5-VL上的实验表明，我们的7B参数模型在KIE-HVQA上比GPT-4o在无幻觉准确率上实现了22%的绝对提升，并且在标准任务中没有显著的性能下降，突出了其有效性和鲁棒性。", "summary": "该研究旨在解决多模态大语言模型在视觉退化文档理解中产生的OCR幻觉问题。为此，作者提出了首个专门评估OCR幻觉的基准数据集KIE-HVQA，并引入了一个基于GRPO的框架，该框架通过结合视觉不确定性感知和拒绝回答机制来避免幻觉。实验结果显示，该方法在无幻觉准确率上取得了显著提升，且未牺牲标准任务性能，证明了其有效性和鲁棒性。", "keywords": "OCR幻觉, 多模态大语言模型, 文档理解, GRPO, KIE-HVQA", "comments": "该论文通过提出一个专门的评估基准KIE-HVQA和创新的GRPO框架，有效解决了多模态大语言模型在处理视觉退化文档时出现的OCR幻觉问题。其核心创新在于引入了模型对视觉不确定性的自我感知以及在不确定情况下的拒绝回答机制，这对于提高模型在真实世界复杂场景下的可靠性和鲁棒性至关重要。实验结果表明，该方法在缓解幻觉方面取得了显著进步，且不影响模型在标准任务上的表现，具有重要的实际应用价值。"}}
{"id": "2506.19929", "title": "A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis", "authors": ["Efe Çakır", "Patrick Dumond"], "summary": "Bearing faults in rotating machinery can lead to significant operational\ndisruptions and maintenance costs. Modern methods for bearing fault diagnosis\nrely heavily on vibration analysis and machine learning techniques, which often\nrequire extensive labeled data and may not adapt well to dynamic environments.\nThis study explores the feasibility of reinforcement learning (RL),\nspecifically Deep Q-Networks (DQNs), for bearing fault classification tasks in\nmachine condition monitoring to enhance the accuracy and adaptability of\nbearing fault diagnosis. The results demonstrate that while RL models developed\nin this study can match the performance of traditional supervised learning\nmodels under controlled conditions, they excel in adaptability when equipped\nwith optimized reward structures. However, their computational demands\nhighlight areas for further improvement. These findings demonstrate RL's\npotential to complement traditional methods, paving the way for adaptive\ndiagnostic frameworks.", "comment": "5 pages, 5 figures. To appear in the Proceedings of the Canadian\n  Society for Mechanical Engineering (CSME) Congress 2025", "pdf_url": "http://arxiv.org/pdf/2506.19929v1", "categories": ["cs.LG", "I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19929v1", "AI": {"title_translation": "强化学习与传统深度学习方法在轴承故障诊断中的比较分析", "tldr": "本研究比较了强化学习（特别是DQN）与传统深度学习方法在轴承故障诊断中的应用，发现强化学习在适应性方面表现出色，但计算成本较高，预示其作为传统方法的补充具有潜力。", "motivation": "旋转机械中的轴承故障会导致严重的运行中断和维护成本。现有的轴承故障诊断方法（依赖振动分析和机器学习）通常需要大量标记数据，并且可能难以适应动态环境，因此需要探索新的方法来提高诊断的准确性和适应性。", "method": "本研究探索了强化学习（RL），特别是深度Q网络（DQNs），在机器状态监测中用于轴承故障分类任务的可行性。", "result": "研究结果表明，在本研究中开发的强化学习模型在受控条件下可以与传统的监督学习模型性能匹配，并且在配备优化奖励结构时，其适应性表现出色。然而，其计算需求也突出了需要进一步改进的领域。", "conclusion": "这些发现表明强化学习有潜力补充传统方法，为自适应诊断框架铺平道路。", "translation": "旋转机械中的轴承故障可能导致严重的运行中断和维护成本。现代轴承故障诊断方法严重依赖振动分析和机器学习技术，这些技术通常需要大量的标记数据，并且可能无法很好地适应动态环境。本研究探讨了强化学习（RL），特别是深度Q网络（DQNs），在机器状态监测中用于轴承故障分类任务的可行性，以提高轴承故障诊断的准确性和适应性。结果表明，本研究中开发的RL模型在受控条件下可以与传统监督学习模型性能匹配，但在配备优化奖励结构时，它们在适应性方面表现出色。然而，它们的计算需求突出了需要进一步改进的领域。这些发现表明RL有潜力补充传统方法，为自适应诊断框架铺平道路。", "summary": "本研究旨在通过比较强化学习（RL），特别是深度Q网络（DQNs），与传统深度学习方法在轴承故障诊断中的应用，以提高诊断的准确性和适应性。研究发现，RL模型在受控条件下能匹配传统方法的性能，并在优化奖励结构后展现出卓越的适应性，尽管其计算需求较高。这表明RL有望成为传统诊断方法的有效补充，推动自适应诊断框架的发展。", "keywords": "强化学习, 轴承故障诊断, 深度Q网络, 机器状态监测, 适应性", "comments": "该论文创新性地将强化学习应用于轴承故障诊断，对比了其与传统深度学习方法的性能。其亮点在于揭示了强化学习在动态环境下的潜在适应性优势，这对于实际工业应用具有重要意义。然而，论文也指出了RL方法当前存在的计算成本问题，为未来的研究指明了方向。"}}
{"id": "2506.20249", "title": "Language Modeling by Language Models", "authors": ["Junyan Cheng", "Peter Clark", "Kyle Richardson"], "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20249v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20249v1", "AI": {"title_translation": "语言模型对语言模型建模", "tldr": "本文提出了Genesys，一个基于LLM的多智能体系统，利用遗传编程和“规模阶梯”方法自主发现新颖的语言模型架构。该系统在设计生成方面显著优于现有方法，并发现了性能优于已知模型的架构。", "motivation": "本文的动机是利用大型语言模型（LLMs）来模拟和自动化发现新颖语言模型（LM）架构的过程，灵感来源于真实的科研实践。", "method": "本文提出了Genesys系统，一个多智能体LLM方法，模拟了从构思、文献检索（提案阶段）到设计实现（代码生成）、生成式预训练和下游评估（验证）的传统研究阶段。Genesys借鉴了缩放定律，采用“规模阶梯”方法，在逐渐增大的模型规模（14M至350M参数）和逐渐缩小的预算下（每个规模可训练的模型数量）提出、对抗性评审、实现和选择性验证新设计。为了提高发现效率和可分解性，Genesys使用了一种新颖的遗传编程骨干，实验证明其比常用的直接提示生成工作流具有经验优势（例如，成功设计生成率提高约86个百分点）。", "result": "实验涉及1,162个新发现的设计（其中1,062个通过预训练完全验证）。最佳设计与已知架构（如GPT2、Mamba2等）具有高度竞争力，在9个常见基准测试中有6个表现优异。遗传编程骨干在成功设计生成方面比直接提示生成提高了约86个百分点。研究结果还包括全面的系统级消融实验和形式化结果，为有效自主发现系统的设计提供了更广泛的见解。", "conclusion": "研究表明，利用LLMs和遗传编程的自主发现系统（如Genesys）能够有效找到具有竞争力的新颖语言模型架构，并为设计此类系统提供了宝贵的见解。", "translation": "我们能否利用大型语言模型（LLM）来模拟发现新颖语言模型（LM）架构的过程？受真实研究的启发，我们提出了一种多智能体LLM方法，该方法模拟了传统的研究阶段，从构思和文献检索（提案阶段）到设计实现（代码生成）、生成式预训练和下游评估（验证）。Genesys系统借鉴了缩放定律的思想，采用“规模阶梯”方法；新的设计被提出、对抗性评审、实现，并在越来越大的模型规模（14M~350M参数）下进行选择性验证，同时预算逐渐收紧（每个规模我们可以训练的模型数量）。为了帮助实现高效和可分解的发现，Genesys使用了一种新颖的遗传编程骨干，我们证明其比常用的直接提示生成工作流具有经验优势（例如，成功设计生成率提高了约86个百分点，这是一个关键瓶颈）。我们报告了涉及1,162个新发现设计（1,062个通过预训练完全验证）的实验，并发现最佳设计与已知架构具有高度竞争力（例如，在9个常见基准测试中有6个优于GPT2、Mamba2等）。我们将这些结果与全面的系统级消融实验和形式化结果结合起来，这些结果为有效自主发现系统的设计提供了更广泛的见解。", "summary": "本文介绍了Genesys，一个多智能体LLM系统，旨在自主发现新颖的语言模型架构。Genesys通过模拟从构思到评估的整个研究过程，并采用“规模阶梯”方法和创新的遗传编程骨干，显著提高了设计生成的效率。实验结果表明，该系统发现了超过一千个新设计，其中最佳架构在多个基准测试中表现出与现有模型（如GPT2和Mamba2）高度竞争的性能，为自主发现系统的设计提供了重要见解。", "keywords": "语言模型, 自主发现, 遗传编程, 神经架构搜索, 多智能体系统", "comments": "该论文通过将LLMs应用于多智能体和遗传编程框架来自动化神经架构搜索，具有高度创新性。其“规模阶梯”方法和对抗性评审机制巧妙地平衡了计算预算。系统能够自主发现具有竞争力的架构，是迈向更高效、更少依赖人工的AI研究的重要一步。遗传编程相较于直接提示在效率上的显著优势是一个关键发现。"}}
{"id": "2506.19899", "title": "Anti-Phishing Training Does Not Work: A Large-Scale Empirical Assessment of Multi-Modal Training Grounded in the NIST Phish Scale", "authors": ["Andrew T. Rozema", "James C. Davis"], "summary": "Social engineering attacks using email, commonly known as phishing, are a\ncritical cybersecurity threat. Phishing attacks often lead to operational\nincidents and data breaches. As a result, many organizations allocate a\nsubstantial portion of their cybersecurity budgets to phishing awareness\ntraining, driven in part by compliance requirements. However, the effectiveness\nof this training remains in dispute. Empirical evidence of training\n(in)effectiveness is essential for evidence-based cybersecurity investment and\npolicy development. Despite recent measurement studies, two critical gaps\nremain in the literature:\n  (1) we lack a validated measure of phishing lure difficulty, and\n  (2) there are few comparisons of different types of training in real-world\nbusiness settings.\n  To fill these gaps, we conducted a large-scale study ($N = 12{,}511$) of\nphishing effectiveness at a US-based financial technology (``fintech'') firm.\nOur two-factor design compared the effect of treatments (lecture-based,\ninteractive, and control groups) on subjects' susceptibility to phishing lures\nof varying complexity (using the NIST Phish Scale). The NIST Phish Scale\nsuccessfully predicted behavior (click rates: 7.0\\% easy to 15.0\\% hard emails,\np $<$ 0.001), but training showed no significant main effects on clicks (p =\n0.450) or reporting (p = 0.417). Effect sizes remained below 0.01, indicating\nlittle practical value in any of the phishing trainings we deployed. Our\nresults add to the growing evidence that phishing training is ineffective,\nreinforcing the importance of phishing defense-in-depth and the merit of\nchanges to processes and technology to reduce reliance on humans, as well as\nrebuking the training costs necessitated by regulatory requirements.", "comment": "13 pages, 5 apdx", "pdf_url": "http://arxiv.org/pdf/2506.19899v1", "categories": ["cs.CR", "cs.HC"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19899v1", "AI": {"title_translation": "反网络钓鱼培训无效：基于NIST网络钓鱼量表的多模式培训大规模实证评估", "tldr": "一项针对12,511人的大规模研究发现，尽管NIST网络钓鱼量表能预测用户行为，但基于讲座和互动形式的网络钓鱼培训对降低用户点击或报告钓鱼邮件的有效性没有显著影响，实际价值很小。", "motivation": "网络钓鱼攻击是关键的网络安全威胁，常导致运营事故和数据泄露。尽管许多组织投入大量预算进行网络钓鱼意识培训，但其有效性存在争议。现有研究存在两大空白：缺乏经过验证的网络钓鱼诱饵难度衡量标准，以及缺乏对不同类型培训在真实商业环境中比较。本研究旨在填补这些空白。", "method": "本研究在美国一家金融科技公司进行了大规模研究（N=12,511），以评估网络钓鱼培训的有效性。采用双因素设计，比较了不同处理组（基于讲座、互动和对照组）对受试者对不同复杂程度网络钓鱼诱饵（使用NIST网络钓鱼量表）易感性的影响。", "result": "NIST网络钓鱼量表成功预测了用户行为（简单邮件点击率为7.0%，困难邮件为15.0%，p < 0.001）。然而，培训对点击率（p = 0.450）或报告率（p = 0.417）没有显示出显著的主效应。效应量均低于0.01，表明所部署的网络钓鱼培训实际价值很小。", "conclusion": "本研究结果进一步证明网络钓鱼培训无效，强调了网络钓鱼纵深防御的重要性，以及通过改变流程和技术来减少对人类依赖的价值，并对法规要求所带来的培训成本提出质疑。", "translation": "电子邮件形式的社会工程攻击，通常称为网络钓鱼，是关键的网络安全威胁。网络钓鱼攻击经常导致运营事故和数据泄露。因此，许多组织将网络安全预算的很大一部分用于网络钓鱼意识培训，部分原因是合规性要求。然而，这种培训的有效性仍然存在争议。培训（无）有效性的实证证据对于基于证据的网络安全投资和政策制定至关重要。尽管最近进行了测量研究，但文献中仍存在两个关键空白：(1) 我们缺乏经过验证的网络钓鱼诱饵难度衡量标准，以及 (2) 在真实商业环境中对不同类型培训的比较很少。为了填补这些空白，我们在一家美国金融科技公司进行了一项大规模研究（N = 12,511），评估了网络钓鱼的有效性。我们的双因素设计比较了处理（基于讲座、互动和对照组）对受试者对不同复杂程度的网络钓鱼诱饵（使用NIST网络钓鱼量表）易感性的影响。NIST网络钓鱼量表成功预测了行为（点击率：简单邮件为7.0%，困难邮件为15.0%，p < 0.001），但培训对点击率（p = 0.450）或报告率（p = 0.417）没有显示出显著的主效应。效应量均低于0.01，表明我们部署的任何网络钓鱼培训的实际价值都很小。我们的结果进一步证明网络钓鱼培训无效，强化了网络钓鱼纵深防御的重要性以及改变流程和技术以减少对人类依赖的价值，并对法规要求所带来的培训成本提出质疑。", "summary": "本研究对12,511名员工进行了大规模实证评估，旨在探究不同类型的网络钓鱼培训在真实商业环境中的有效性，并验证NIST网络钓鱼量表。研究发现，NIST网络钓鱼量表能有效预测用户对网络钓鱼诱饵的易感性（复杂诱饵的点击率更高），但基于讲座和互动形式的培训对降低用户点击或报告钓鱼邮件的行为没有显著影响，且实际价值微乎其微。这表明当前的网络钓鱼培训是无效的，并强调了应转向纵深防御策略以及通过技术和流程改进来减少对人为因素依赖的重要性。", "keywords": "网络钓鱼, 培训, 有效性, NIST网络钓鱼量表, 网络安全", "comments": "这项研究通过大规模实证数据，有力地挑战了当前广泛实施的网络钓鱼培训的有效性，其结论对于网络安全预算分配和政策制定具有重要指导意义。引入并验证NIST网络钓鱼量表是其方法学上的一个创新点。研究结果不仅揭示了培训的局限性，还倡导了更注重技术和流程的纵深防御策略，对过度依赖人类行为改变的网络安全思路提出了反思。"}}
{"id": "2506.20463", "title": "Analyzing Security and Privacy Challenges in Generative AI Usage Guidelines for Higher Education", "authors": ["Bei Yi Ng", "Jiarui Li", "Xinyuan Tong", "Kevin Ye", "Gauthami Yenne", "Varun Chandrasekaran", "Jingjie Li"], "summary": "Educators and learners worldwide are embracing the rise of Generative\nArtificial Intelligence (GenAI) as it reshapes higher education. However, GenAI\nalso raises significant privacy and security concerns, as models and\nprivacy-sensitive user data, such as student records, may be misused by service\nproviders. Unfortunately, end-users often have little awareness of or control\nover how these models operate. To address these concerns, universities are\ndeveloping institutional policies to guide GenAI use while safeguarding\nsecurity and privacy. This work examines these emerging policies and\nguidelines, with a particular focus on the often-overlooked privacy and\nsecurity dimensions of GenAI integration in higher education, alongside other\nacademic values. Through a qualitative analysis of GenAI usage guidelines from\nuniversities across 12 countries, we identify key challenges and opportunities\ninstitutions face in providing effective privacy and security protections,\nincluding the need for GenAI safeguards tailored specifically to the academic\ncontext.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20463v1", "categories": ["cs.HC", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20463v1", "AI": {"title_translation": "分析高等教育中生成式人工智能使用指南的安全与隐私挑战", "tldr": "本研究通过定性分析来自12个国家大学的生成式AI使用指南，探讨了高等教育中生成式AI集成的安全和隐私挑战，并强调了需要针对学术环境定制的保障措施。", "motivation": "生成式AI在高等教育中日益普及，但其带来的隐私和安全问题（如模型和敏感用户数据可能被滥用）令人担忧。终端用户对此知之甚少，大学正在制定政策以指导GenAI使用并保护安全和隐私。本研究旨在探讨这些新兴政策，尤其关注常被忽视的隐私和安全维度。", "method": "通过对来自12个国家大学的生成式AI使用指南进行定性分析。", "result": "识别了机构在提供有效隐私和安全保护方面面临的关键挑战和机遇，包括需要专门针对学术环境定制的生成式AI保障措施。", "conclusion": "大学在制定生成式AI使用指南时，应特别关注并加强隐私和安全保护，并开发适合学术环境的定制化保障措施，以应对其带来的挑战。", "translation": "世界各地的教育工作者和学习者正在接受生成式人工智能（GenAI）的兴起，因为它正在重塑高等教育。然而，GenAI也带来了重大的隐私和安全问题，因为模型和隐私敏感的用户数据（例如学生记录）可能被服务提供商滥用。不幸的是，终端用户通常对这些模型的运作方式知之甚少或无法控制。为了解决这些问题，大学正在制定机构政策，以指导GenAI的使用，同时保障安全和隐私。这项工作审查了这些新兴政策和指南，特别关注GenAI融入高等教育中经常被忽视的隐私和安全维度，以及其他学术价值。通过对来自12个国家大学的GenAI使用指南进行定性分析，我们确定了机构在提供有效隐私和安全保护方面面临的关键挑战和机遇，包括需要专门针对学术环境定制的GenAI保障措施。", "summary": "本研究分析了全球高等教育机构为应对生成式人工智能（GenAI）带来的隐私和安全挑战而制定的使用指南。通过对来自12个国家大学政策的定性分析，论文揭示了在保护敏感用户数据和模型操作方面存在的关键问题，并强调了开发专门适用于学术环境的GenAI安全和隐私保障措施的必要性。", "keywords": "生成式AI, 隐私, 安全, 高等教育, 使用指南", "comments": "这项研究 timely 地揭示了生成式AI在高等教育领域应用中被忽视但至关重要的隐私和安全问题。其创新之处在于通过跨国大学政策的定性分析，为机构提供了实际的洞察，指出当前政策的不足以及未来制定更有效、定制化保障措施的方向。对于教育技术政策制定者和研究者来说，具有重要的参考价值。"}}
{"id": "2506.20406", "title": "POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes", "authors": ["Ruijia Zhang", "Zhengling Qi", "Yue Wu", "Xiangyu Zhang", "Yanxun Xu"], "summary": "Dynamic treatment regimes (DTRs) provide a principled framework for\noptimizing sequential decision-making in domains where decisions must adapt\nover time in response to individual trajectories, such as healthcare,\neducation, and digital interventions. However, existing statistical methods\noften rely on strong positivity assumptions and lack robustness under partial\ndata coverage, while offline reinforcement learning approaches typically focus\non average training performance, lack statistical guarantees, and require\nsolving complex optimization problems. To address these challenges, we propose\nPOLAR, a novel pessimistic model-based policy learning algorithm for offline\nDTR optimization. POLAR estimates the transition dynamics from offline data and\nquantifies uncertainty for each history-action pair. A pessimistic penalty is\nthen incorporated into the reward function to discourage actions with high\nuncertainty. Unlike many existing methods that focus on average training\nperformance, POLAR directly targets the suboptimality of the final learned\npolicy and offers theoretical guarantees, without relying on computationally\nintensive minimax or constrained optimization procedures. To the best of our\nknowledge, POLAR is the first model-based DTR method to provide both\nstatistical and computational guarantees, including finite-sample bounds on\npolicy suboptimality. Empirical results on both synthetic data and the\nMIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods\nand yields near-optimal, history-aware treatment strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20406v1", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20406v1", "AI": {"title_translation": "POLAR：一种用于动态治疗方案的悲观模型策略学习算法", "tldr": "POLAR是一种新颖的悲观模型策略学习算法，用于解决动态治疗方案（DTRs）优化中的现有挑战，提供统计和计算保证。", "motivation": "现有统计方法在处理动态治疗方案（DTRs）时，常依赖强阳性假设且在部分数据覆盖下缺乏鲁棒性。离线强化学习方法则通常侧重于平均训练性能，缺乏统计保证，并需要解决复杂的优化问题。", "method": "POLAR通过离线数据估计转移动态并量化每个历史-动作对的不确定性。然后，将悲观惩罚纳入奖励函数中，以抑制不确定性高的动作。它直接针对最终学习策略的次优性，并提供理论保证，避免了计算密集型的minimax或约束优化过程。", "result": "在合成数据和MIMIC-III数据集上的实证结果表明，POLAR优于现有最先进的方法，并产生了接近最优、历史感知的治疗策略。", "conclusion": "POLAR是首个提供统计和计算保证（包括策略次优性的有限样本界限）的模型化DTR方法。", "translation": "动态治疗方案（DTRs）为优化序列决策提供了一个原则性框架，在医疗、教育和数字干预等领域，决策必须随着时间根据个体轨迹进行调整。然而，现有统计方法通常依赖于强阳性假设，并且在部分数据覆盖下缺乏鲁棒性，而离线强化学习方法通常侧重于平均训练性能，缺乏统计保证，并且需要解决复杂的优化问题。为了解决这些挑战，我们提出了POLAR，一种新颖的悲观模型策略学习算法，用于离线DTR优化。POLAR从离线数据中估计转移动态，并量化每个历史-动作对的不确定性。然后，将悲观惩罚纳入奖励函数中，以抑制不确定性高的动作。与许多侧重于平均训练性能的现有方法不同，POLAR直接针对最终学习策略的次优性，并提供理论保证，而无需依赖计算密集型的minimax或约束优化过程。据我们所知，POLAR是第一个提供统计和计算保证（包括策略次优性的有限样本界限）的模型化DTR方法。在合成数据和MIMIC-III数据集上的实证结果表明，POLAR优于现有最先进的方法，并产生了接近最优、历史感知的治疗策略。", "summary": "POLAR是一种新型的悲观模型策略学习算法，专为离线动态治疗方案（DTRs）优化设计。它通过估计转移动态和量化不确定性，并引入悲观惩罚来避免高不确定性动作。与现有方法不同，POLAR直接关注策略的次优性，并提供统计和计算保证，无需复杂的优化过程。实验证明其在合成数据和真实世界数据集上均优于现有技术。", "keywords": "动态治疗方案, 离线强化学习, 悲观策略学习, 不确定性量化, 模型化方法", "comments": "POLAR的创新之处在于其引入悲观惩罚机制来处理不确定性，并直接针对策略的次优性提供理论保证，而非仅仅关注平均训练性能。它是首个为模型化DTR方法提供统计和计算双重保证的算法，解决了现有方法在鲁棒性、统计保证和计算复杂性方面的不足，对于医疗等关键领域的决策优化具有重要意义。"}}
{"id": "2506.20651", "title": "Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning", "authors": ["Fei Wang", "Baochun Li"], "summary": "Recent work has shown that gradient updates in federated learning (FL) can\nunintentionally reveal sensitive information about a client's local data. This\nrisk becomes significantly greater when a malicious server manipulates the\nglobal model to provoke information-rich updates from clients. In this paper,\nwe adopt a defender's perspective to provide the first comprehensive analysis\nof malicious gradient leakage attacks and the model manipulation techniques\nthat enable them. Our investigation reveals a core trade-off: these attacks\ncannot be both highly effective in reconstructing private data and sufficiently\nstealthy to evade detection -- especially in realistic FL settings that\nincorporate common normalization techniques and federated averaging.\n  Building on this insight, we argue that malicious gradient leakage attacks,\nwhile theoretically concerning, are inherently limited in practice and often\ndetectable through basic monitoring. As a complementary contribution, we\npropose a simple, lightweight, and broadly applicable client-side detection\nmechanism that flags suspicious model updates before local training begins,\ndespite the fact that such detection may not be strictly necessary in realistic\nFL settings. This mechanism further underscores the feasibility of defending\nagainst these attacks with minimal overhead, offering a deployable safeguard\nfor privacy-conscious federated learning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20651v1", "categories": ["cs.LG", "cs.CR", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20651v1", "AI": {"title_translation": "不听邪恶：联邦学习中恶意服务器的梯度泄露检测", "tldr": "本文从防御者的角度首次全面分析了联邦学习中恶意梯度泄露攻击及其模型操纵技术，发现这些攻击在实际中效果有限且可检测，并提出了一个轻量级客户端检测机制。", "motivation": "联邦学习中的梯度更新可能会无意中泄露客户端的敏感数据，当恶意服务器操纵全局模型以诱导客户端提供信息丰富的更新时，这种风险会显著增加。本研究旨在从防御者角度分析并应对这种恶意梯度泄露攻击。", "method": "本文首先从防御者的角度对恶意梯度泄露攻击及其实现模型操纵技术进行了首次全面分析。在此基础上，提出了一种简单、轻量级且广泛适用的客户端检测机制，该机制在本地训练开始前标记可疑模型更新。", "result": "研究发现，恶意梯度泄露攻击无法同时在重建私有数据方面高效和在规避检测方面隐蔽，尤其是在包含常见归一化技术和联邦平均的现实联邦学习设置中。这些攻击在实践中固有地受到限制，并且通常可以通过基本监控进行检测。", "conclusion": "恶意梯度泄露攻击虽然在理论上令人担忧，但在实践中固有地受到限制，并且通常可以通过基本监控进行检测。本文提出的轻量级客户端检测机制进一步证明了以最小开销防御这些攻击的可行性，为注重隐私的联邦学习系统提供了可部署的保障。", "translation": "最近的工作表明，联邦学习（FL）中的梯度更新可能会无意中泄露有关客户端本地数据的敏感信息。当恶意服务器操纵全局模型以诱发客户端提供信息丰富的更新时，这种风险会显著增加。在本文中，我们采取防御者的视角，首次对恶意梯度泄露攻击及其实现模型操纵技术进行了全面分析。我们的调查揭示了一个核心权衡：这些攻击无法同时在重建私有数据方面高效和在规避检测方面隐蔽——尤其是在包含常见归一化技术和联邦平均的现实联邦学习设置中。基于这一见解，我们认为恶意梯度泄露攻击虽然在理论上令人担忧，但在实践中固有地受到限制，并且通常可以通过基本监控进行检测。作为一项补充贡献，我们提出了一种简单、轻量级且广泛适用的客户端检测机制，该机制在本地训练开始前标记可疑模型更新，尽管在现实的联邦学习设置中这种检测可能并非严格必要。这种机制进一步强调了以最小开销防御这些攻击的可行性，为注重隐私的联邦学习系统提供了可部署的保障。", "summary": "本文从防御者视角首次全面分析了联邦学习中恶意服务器导致的梯度泄露攻击及其模型操纵技术。研究发现，此类攻击在实际应用中难以同时实现高效重构隐私数据和隐蔽性，尤其是在常见的联邦学习设置下，往往可以通过基本监控被检测到。基于此，论文提出了一种轻量级、客户端侧的检测机制，能在本地训练前识别可疑模型更新，进一步增强了联邦学习系统的隐私保护能力，且开销极小。", "keywords": "联邦学习, 梯度泄露, 恶意服务器, 隐私保护, 检测机制", "comments": "这篇论文的创新点在于它从防御者的角度对联邦学习中的梯度泄露攻击进行了首次全面的分析，并揭示了攻击有效性和隐蔽性之间的核心权衡。它挑战了之前认为此类攻击在实践中非常危险的观点，指出它们在现实设置中具有固有局限性且易于检测。此外，提出的轻量级客户端检测机制提供了一个实用的防御方案，对于构建更安全的联邦学习系统具有重要意义。"}}
{"id": "2506.20493", "title": "Analyzing the Impact of Strategic Bidding on the Reserve Capacity via a Bi-Level Model", "authors": ["Yun Xu", "Yunxiao Bai", "Yunyong Zhang", "Peng Wang", "Xuelin Wang", "Jiqun Guo", "Kaijun Xie", "Rusheng Zhao"], "summary": "The growing integration of renewable energy sources necessitates adequate\nreserve capacity to maintain power balance. However, in market clearing, power\ncompanies with flexible resources may submit strategic bids to maximize\nprofits, potentially compromising system reserves. This paper examines the\neffects of such strategic behavior by modeling the market as a bi-level\nproblem. The upper level represents a strategic company aiming to maximize\nprofit, while the lower level simulates the system operator clearing the market\nbased on submitted offers. To enable duality-based solution methods, we\napproximate unit commitments with a continuous reserve capacity calculation.\nCase studies indicate that, in an imperfectly competitive market, more units\nare incentivized to operate,enhancing system reserves. However, some units go\nonline mainly for profit, ultimately raising electricity costs for consumers.\nThese findings highlight the importance of market design in managing the\ntrade-off between reserve adequacy and economic efficiency in the presence of\nstrategic bidding behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20493v1", "categories": ["eess.SY", "cs.GT", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20493v1", "AI": {"title_translation": "基于双层模型的策略性投标对备用容量影响分析", "tldr": "本文通过双层模型研究了电力市场中策略性投标行为对备用容量和电力成本的影响，并指出市场设计的重要性。", "motivation": "可再生能源的日益整合需要足够的备用容量来维持电力平衡。然而，电力公司可能会提交策略性投标以最大化利润，这可能损害系统备用容量。", "method": "本文将市场建模为一个双层问题。上层代表旨在最大化利润的策略性公司，下层模拟系统运营商根据提交的报价进行市场出清。为了使用基于对偶的求解方法，作者通过连续备用容量计算来近似机组承诺。", "result": "案例研究表明，在不完全竞争市场中，更多机组有动力运行，从而增强了系统备用容量。然而，一些机组上线主要是为了利润，最终提高了消费者的电力成本。", "conclusion": "这些发现强调了在存在策略性投标行为的情况下，市场设计在管理备用容量充足性与经济效率之间权衡方面的重要性。", "translation": "可再生能源的日益整合需要足够的备用容量来维持电力平衡。然而，在市场出清中，拥有灵活资源的电力公司可能会提交策略性投标以最大化利润，这可能损害系统备用容量。本文通过将市场建模为一个双层问题来研究这种策略性行为的影响。上层代表旨在最大化利润的策略性公司，而下层模拟系统运营商根据提交的报价进行市场出清。为了使用基于对偶的求解方法，我们通过连续备用容量计算来近似机组承诺。案例研究表明，在不完全竞争市场中，更多机组有动力运行，从而增强了系统备用容量。然而，一些机组上线主要是为了利润，最终提高了消费者的电力成本。这些发现强调了在存在策略性投标行为的情况下，市场设计在管理备用容量充足性与经济效率之间权衡方面的重要性。", "summary": "本文研究了电力市场中策略性投标行为对备用容量的影响。通过构建一个双层模型，上层代表追求利润最大化的策略性公司，下层模拟系统运营商的市场出清过程。研究结果表明，在不完全竞争市场中，策略性投标可能导致更多机组上线以增强系统备用，但同时也会因利润驱动而提高电力成本。论文强调了市场设计在平衡备用容量和经济效率方面的重要性。", "keywords": "策略性投标, 备用容量, 双层模型, 电力市场, 可再生能源", "comments": "该论文创新性地使用双层模型来分析策略性投标对备用容量的影响，揭示了市场设计在电力系统稳定性和经济性之间权衡的关键作用。其局限性可能在于对机组承诺的近似处理，这可能影响模型的精确性。"}}
{"id": "2506.20079", "title": "Low-Complexity Ordered Reliability Direct Error Pattern Testing (ORDEPT) Decoding with Likelihood Thresholding", "authors": ["Reza Hadavian", "Dmitri Truhachev"], "summary": "We propose a reduced complexity approach to pattern-based soft decoding of\nblock codes. We start from the ORDEPT decoding algorithm which tests a list of\npartial error patterns organized in the order of their likelihood and attempts\nto complete the patterns creating candidate codewords. We then propose an early\ntermination criterion. Once a candidate codeword is found, its log-likelihood\ndifference to the received sequence is compared to a preset threshold and the\ndecoding decision is instantly made in case the likelihood deviation is below\nthe threshold. We demonstrate that while keeping the same block error rate\n(BLER) performance, the proposed algorithm's latency and complexity is multiple\ntimes smaller than that of the state-of-the art competitors including the Chase\nII, ORBGRAND, GCD, and the very recent ORDEPT with Soft-Output GRAND\ntermination which necessitates several multiplications in each query\nprocessing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20079v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20079v1", "AI": {"title_translation": "低复杂度有序可靠性直接错误模式测试(ORDEPT)解码与似然阈值", "tldr": "提出了一种基于似然阈值的低复杂度ORDEPT解码算法，显著降低了块码软解码的延迟和复杂度，同时保持了错误率性能。", "motivation": "降低块码软解码的复杂度。", "method": "在ORDEPT解码算法的基础上，引入了一个早期终止准则，即当找到候选码字时，将其与接收序列的对数似然差与预设阈值进行比较，若低于阈值则立即做出解码决策。", "result": "在保持相同块错误率（BLER）性能的同时，所提出的算法的延迟和复杂度比现有技术（包括Chase II、ORBGRAND、GCD以及带有软输出GRAND终止的最新ORDEPT）小数倍。", "conclusion": "所提出的基于似然阈值的ORDEPT解码方法显著降低了块码软解码的计算复杂度和延迟，而没有牺牲性能。", "translation": "我们提出了一种降低复杂度的块码基于模式的软解码方法。我们从ORDEPT解码算法开始，该算法测试按其似然顺序组织的偏错误模式列表，并尝试完成这些模式以创建候选码字。然后我们提出了一个早期终止准则。一旦找到候选码字，其与接收序列的对数似然差将与预设阈值进行比较，如果似然偏差低于阈值，则立即做出解码决定。我们证明，在保持相同块错误率（BLER）性能的同时，所提出的算法的延迟和复杂度比现有技术（包括Chase II、ORBGRAND、GCD以及需要多次乘法进行每次查询处理的最新带有软输出GRAND终止的ORDEPT）小数倍。", "summary": "本文提出了一种名为“似然阈值ORDEPT解码”的低复杂度块码软解码方法。该方法在现有的ORDEPT算法基础上，引入了早期终止机制：一旦找到候选码字，通过比较其与接收序列的似然差与预设阈值来决定是否立即终止解码。实验结果表明，与Chase II、ORBGRAND等现有算法相比，该方法在保持相同块错误率性能的同时，显著降低了解码延迟和计算复杂度。", "keywords": "块码解码, ORDEPT, 软解码, 似然阈值, 低复杂度", "comments": "这篇论文的创新点在于引入了似然阈值早期终止机制，极大地优化了现有ORDEPT解码算法的计算效率。它成功地在不牺牲性能的前提下，解决了软解码复杂度高的问题，对于资源受限的通信系统具有重要意义。"}}
{"id": "2506.20319", "title": "Transformer Based Multi-Target Bernoulli Tracking for Maritime Radar", "authors": ["Caden Sweeney", "Du Yong Kim", "Branko Ristic", "Brian Cheung"], "summary": "Multi-target tracking in the maritime domain is a challenging problem due to\nthe non-Gaussian and fluctuating characteristics of sea clutter. This article\ninvestigates the use of machine learning (ML) to the detection and tracking of\nlow SIR targets in the maritime domain. The proposed method uses a transformer\nto extract point measurements from range-azimuth maps, before clustering and\ntracking using the Labelled mulit- Bernoulli (LMB) filter. A measurement driven\nbirth density design based on the transformer attention maps is also developed.\nThe error performance of the transformer based approach is presented and\ncompared with a constant false alarm rate (CFAR) detection technique. The LMB\nfilter is run in two scenarios, an ideal birth approach, and the measurement\ndriven birth approach. Experiments indicate that the transformer based method\nhas superior performance to the CFAR approach for all target scenarios\ndiscussed", "comment": "8 pages, 8 figures, for submission also to IEEE journal", "pdf_url": "http://arxiv.org/pdf/2506.20319v1", "categories": ["eess.IV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20319v1", "AI": {"title_translation": "基于Transformer的海洋雷达多目标伯努利跟踪", "tldr": "本文提出了一种基于Transformer的多目标伯努利跟踪方法，用于海洋雷达中的低信干比目标检测和跟踪，实验表明其性能优于传统的CFAR方法。", "motivation": "海洋领域的多目标跟踪是一个具有挑战性的问题，因为海杂波具有非高斯和波动的特性。", "method": "该方法使用Transformer从距离-方位图中提取点测量值，然后使用标记多伯努利（LMB）滤波器进行聚类和跟踪。还开发了一种基于Transformer注意力图的测量驱动的出生密度设计。", "result": "实验表明，在所有讨论的目标场景中，基于Transformer的方法的性能优于CFAR方法。", "conclusion": "基于Transformer的方法在海洋雷达多目标跟踪中表现出优越的性能，尤其是在低信干比目标场景下。", "translation": "海洋领域的多目标跟踪是一个具有挑战性的问题，因为海杂波具有非高斯和波动的特性。本文研究了机器学习（ML）在海洋领域低信干比目标检测和跟踪中的应用。所提出的方法使用Transformer从距离-方位图中提取点测量值，然后使用标记多伯努利（LMB）滤波器进行聚类和跟踪。还开发了一种基于Transformer注意力图的测量驱动的出生密度设计。本文介绍了基于Transformer的方法的误差性能，并将其与恒定虚警率（CFAR）检测技术进行了比较。LMB滤波器在两种场景下运行：理想出生方法和测量驱动出生方法。实验表明，在所有讨论的目标场景中，基于Transformer的方法的性能优于CFAR方法。", "summary": "本文针对海洋雷达中海杂波导致的复杂多目标跟踪问题，提出了一种基于Transformer的解决方案。该方法利用Transformer从雷达距离-方位图中提取测量点，并结合标记多伯努利（LMB）滤波器进行目标的聚类和跟踪。此外，还引入了一种基于Transformer注意力图的测量驱动出生密度设计。实验结果表明，与传统的恒定虚警率（CFAR）检测技术相比，所提出的基于Transformer的方法在各种目标场景下均表现出更优越的性能。", "keywords": "多目标跟踪, 伯努利滤波器, Transformer, 海洋雷达, 低信干比", "comments": "该论文的创新点在于将Transformer这一先进的深度学习模型引入到海洋雷达的多目标跟踪领域，有效解决了海杂波带来的挑战。通过结合LMB滤波器和Attention Map驱动的出生密度设计，显著提升了低信干比目标的检测和跟踪性能，为实际海洋雷达应用提供了新的思路和方法。"}}
{"id": "2506.20073", "title": "A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs", "authors": ["Kethmi Hirushini Hettige", "Jiahao Ji", "Cheng Long", "Shili Xiang", "Gao Cong", "Jingyuan Wang"], "summary": "Spatio-temporal data mining plays a pivotal role in informed decision making\nacross diverse domains. However, existing models are often restricted to narrow\ntasks, lacking the capacity for multi-task inference and complex long-form\nreasoning that require generation of in-depth, explanatory outputs. These\nlimitations restrict their applicability to real-world, multi-faceted decision\nscenarios. In this work, we introduce STReason, a novel framework that\nintegrates the reasoning strengths of large language models (LLMs) with the\nanalytical capabilities of spatio-temporal models for multi-task inference and\nexecution. Without requiring task-specific finetuning, STReason leverages\nin-context learning to decompose complex natural language queries into modular,\ninterpretable programs, which are then systematically executed to generate both\nsolutions and detailed rationales. To facilitate rigorous evaluation, we\nconstruct a new benchmark dataset and propose a unified evaluation framework\nwith metrics specifically designed for long-form spatio-temporal reasoning.\nExperimental results show that STReason significantly outperforms advanced LLM\nbaselines across all metrics, particularly excelling in complex,\nreasoning-intensive spatio-temporal scenarios. Human evaluations further\nvalidate STReason's credibility and practical utility, demonstrating its\npotential to reduce expert workload and broaden the applicability to real-world\nspatio-temporal tasks. We believe STReason provides a promising direction for\ndeveloping more capable and generalizable spatio-temporal reasoning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20073v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20073v1", "AI": {"title_translation": "一个集成时空模型和大型语言模型的可模块化多任务推理框架", "tldr": "STReason是一个新颖的框架，它将大型语言模型的推理能力与时空模型的分析能力相结合，用于多任务推理和执行，无需任务特定微调，并在复杂时空推理场景中表现出色。", "motivation": "现有模型在时空数据挖掘中常受限于狭窄任务，缺乏多任务推理能力和生成深度解释性输出的复杂长篇推理能力，这限制了它们在现实世界多方面决策场景中的应用。", "method": "本文引入了STReason框架，它整合了大型语言模型（LLMs）的推理优势与时空模型的分析能力，用于多任务推理和执行。STReason通过利用上下文学习，将复杂的自然语言查询分解为模块化、可解释的程序，然后系统地执行这些程序以生成解决方案和详细的理由，且无需进行任务特定的微调。同时，构建了一个新的基准数据集并提出了一个统一的评估框架。", "result": "实验结果表明，STReason在所有指标上显著优于先进的LLM基线，尤其在复杂的、推理密集型时空场景中表现出色。人类评估进一步验证了STReason的可信度和实用性，表明其有潜力减少专家工作量并拓宽其在现实世界时空任务中的适用性。", "conclusion": "STReason提供了一个有前途的方向，用于开发更强大和更具泛化性的时空推理系统。", "translation": "时空数据挖掘在各种领域的知情决策中扮演着关键角色。然而，现有模型通常局限于狭窄的任务，缺乏多任务推理能力和需要生成深入、解释性输出的复杂长篇推理能力。这些限制阻碍了它们在现实世界中多方面决策场景的应用。在这项工作中，我们引入了STReason，一个新颖的框架，它将大型语言模型（LLMs）的推理优势与时空模型的分析能力相结合，用于多任务推理和执行。STReason无需任务特定的微调，利用上下文学习将复杂的自然语言查询分解为模块化、可解释的程序，然后系统地执行这些程序以生成解决方案和详细的理由。为了促进严格的评估，我们构建了一个新的基准数据集并提出了一个统一的评估框架，其指标专门为长篇时空推理设计。实验结果表明，STReason在所有指标上显著优于先进的LLM基线，尤其在复杂的、推理密集型时空场景中表现出色。人类评估进一步验证了STReason的可信度和实用性，表明其有潜力减少专家工作量并拓宽其在现实世界时空任务中的适用性。我们相信STReason为开发更强大和更具泛化性的时空推理系统提供了有前途的方向。", "summary": "本文提出了STReason，一个创新的框架，旨在解决现有模型在时空数据挖掘中多任务推理和复杂长篇推理能力的不足。STReason通过将大型语言模型的推理能力与时空模型的分析能力相结合，实现了无需任务特定微调的多任务推理和执行。它能够将复杂的自然语言查询分解为可执行程序，并生成详细的解决方案和解释。实验证明，STReason在复杂时空推理场景中显著优于现有LLM基线，并通过人类评估验证了其在实际应用中的潜力，有望减少专家工作量并扩展应用范围。", "keywords": "时空推理, 大型语言模型, 多任务, 模块化框架, 上下文学习", "comments": "STReason的创新之处在于其模块化设计和对上下文学习的利用，实现了LLM与时空模型的有效集成，从而在不进行任务特定微调的情况下处理复杂的多任务时空推理。其重要性在于拓宽了时空数据挖掘在现实世界复杂决策场景中的应用，并有望显著提高效率。该工作为开发更通用、更强大的时空推理系统指明了方向。"}}
{"id": "2506.20501", "title": "Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank", "authors": ["Philipp Hager", "Onno Zoeter", "Maarten de Rijke"], "summary": "Additive two-tower models are popular learning-to-rank methods for handling\nbiased user feedback in industry settings. Recent studies, however, report a\nconcerning phenomenon: training two-tower models on clicks collected by\nwell-performing production systems leads to decreased ranking performance. This\npaper investigates two recent explanations for this observation: confounding\neffects from logging policies and model identifiability issues. We\ntheoretically analyze the identifiability conditions of two-tower models,\nshowing that either document swaps across positions or overlapping feature\ndistributions are required to recover model parameters from clicks. We also\ninvestigate the effect of logging policies on two-tower models, finding that\nthey introduce no bias when models perfectly capture user behavior. However,\nlogging policies can amplify biases when models imperfectly capture user\nbehavior, particularly when prediction errors correlate with document placement\nacross positions. We propose a sample weighting technique to mitigate these\neffects and provide actionable insights for researchers and practitioners using\ntwo-tower models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20501v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20501v1", "AI": {"title_translation": "未识别和混淆？理解双塔模型用于无偏学习排序", "tldr": "本文研究了双塔模型在工业应用中性能下降的原因，发现问题出在模型可识别性条件和日志策略的偏差放大效应，并提出了一种样本加权技术来缓解这些问题。", "motivation": "最近的研究发现，在由表现良好的生产系统收集的点击数据上训练双塔模型会导致排序性能下降。本文旨在调查导致这种现象的两个近期解释：日志策略的混淆效应和模型的可识别性问题。", "method": "本文对双塔模型的可识别性条件进行了理论分析，并调查了日志策略对双塔模型的影响。此外，提出了一种样本加权技术来缓解观察到的效应。", "result": "理论分析表明，双塔模型的可识别性需要跨位置的文档交换或重叠的特征分布。研究发现，当模型完美捕捉用户行为时，日志策略不会引入偏差；但当模型不完美捕捉用户行为时，日志策略会放大偏差，尤其当预测误差与文档在不同位置的放置相关时。", "conclusion": "本文提出了一个样本加权技术来缓解双塔模型中由于日志策略和不完美的用户行为捕捉引起的偏差放大效应，并为研究人员和从业者提供了可操作的见解。", "translation": "加性双塔模型是工业环境中处理有偏用户反馈的流行学习排序方法。然而，最近的研究报告了一个令人担忧的现象：在由表现良好的生产生产系统收集的点击数据上训练双塔模型会导致排序性能下降。本文调查了对这一现象的两种近期解释：日志策略的混淆效应和模型可识别性问题。我们理论分析了双塔模型的可识别性条件，表明需要跨位置的文档交换或重叠的特征分布才能从点击中恢复模型参数。我们还调查了日志策略对双塔模型的影响，发现当模型完美捕捉用户行为时，它们不会引入偏差。然而，当模型不完美捕捉用户行为时，日志策略会放大偏差，特别是当预测误差与文档在不同位置的放置相关时。我们提出了一种样本加权技术来缓解这些影响，并为使用双塔模型的研究人员和从业者提供可操作的见解。", "summary": "本文探讨了双塔模型在工业应用中性能下降的问题，重点分析了日志策略导致的混淆效应和模型可识别性问题。研究发现，模型的可识别性需要特定的条件，并且日志策略在模型未能完美捕捉用户行为时会放大偏差。为解决这些问题，论文提出了一种样本加权技术，旨在提高双塔模型在有偏数据下的学习效果。", "keywords": "双塔模型, 学习排序, 无偏学习, 可识别性, 日志策略", "comments": "本文深入探讨了双塔模型在实际应用中遇到的关键挑战，即性能下降的根本原因。通过理论分析模型的可识别性和日志策略的影响，揭示了偏差来源。提出的样本加权技术具有重要的实践意义，为解决有偏用户反馈下的学习排序问题提供了可行的方案，对业界和学术界都有启发。"}}
{"id": "2506.20030", "title": "Polynomial-Time Approximation Schemes via Utility Alignment: Unit-Demand Pricing and More", "authors": ["Robin Bowers", "Marius Garbea", "Emmanouil Pountourakis", "Samuel Taggart"], "summary": "This paper derives polynomial-time approximation schemes for several NP-hard\nstochastic optimization problems from the algorithmic mechanism design and\noperations research literatures. The problems we consider involve a principal\nor seller optimizing with respect to a subsequent choice by an agent or buyer.\nThese include posted pricing for a unit-demand buyer with independent values\n(Chawla et al., 2007, Cai and Daskalakis, 2011), assortment optimization with\nindependent utilities (Talluri and van Ryzin, 2004), and delegated choice\n(Khodabakhsh et al., 2024). Our results advance the state of the art for each\nof these problems. For unit-demand pricing with discrete distributions, our\nmultiplicative PTAS improves on the additive PTAS of Cai and Daskalakis, and we\nadditionally give a PTAS for the unbounded regular case, improving on the\nlatter paper's QPTAS. For assortment optimization, no constant approximation\nwas previously known. For delegated choice, we improve on both the\n$3$-approximation for the case with no outside option and the\nsuper-constant-approximation with an outside option.\n  A key technical insight driving our results is an economically meaningful\nproperty we term utility alignment. Informally, a problem is utility aligned\nif, at optimality, the principal derives most of their utility from\nrealizations where the agent's utility is also high. Utility alignment allows\nthe algorithm designer to focus on maximizing performance on realizations with\nhigh agent utility, which is often an algorithmically simpler task. We prove\nutility alignment results for all the problems mentioned above, including\nstrong results for unit-demand pricing and delegation, as well as a weaker but\nvery broad guarantee that holds for many other problems under very mild\nconditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20030v1", "categories": ["cs.GT", "cs.DS"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.20030v1", "AI": {"title_translation": "通过效用对齐的多项式时间近似方案：单位需求定价及更多", "tldr": "本文通过引入“效用对齐”这一经济学特性，为多个NP难的随机优化问题（如单位需求定价、品类优化和委托选择）提供了多项式时间近似方案，显著提升了现有算法的性能。", "motivation": "现有研究在解决NP难的随机优化问题时，对于单位需求定价、品类优化和委托选择等问题，其近似算法的性能仍有提升空间，甚至某些问题此前没有已知的常数近似方案。", "method": "本文通过引入并证明一个称为“效用对齐”的经济学特性来推导近似方案。效用对齐指的是在最优情况下，主体的大部分效用来源于代理人效用也较高的情景。这种特性使得算法设计者可以专注于最大化代理人高效用情景下的性能，从而简化了算法设计。", "result": "对于具有离散分布的单位需求定价问题，本文的乘法PTAS改进了现有加法PTAS，并为无界正则情况提供了PTAS，优于现有QPTAS。对于品类优化问题，本文首次提供了常数近似方案。对于委托选择问题，本文改进了无外部选项情况下的3-近似和有外部选项情况下的超常数近似。", "conclusion": "本文通过引入“效用对齐”这一关键技术洞察，成功为多个NP难的随机优化问题（包括单位需求定价、品类优化和委托选择）提供了改进的多项式时间近似方案，显著提升了这些领域的最新技术水平，并提出了一个广泛适用于多种问题的通用保证。", "translation": "本文为算法机制设计和运筹学文献中的几个NP难随机优化问题推导了多项式时间近似方案。我们考虑的问题涉及主体或卖方根据代理人或买方的后续选择进行优化。这些问题包括具有独立价值的单位需求买家的公布定价（Chawla et al., 2007, Cai and Daskalakis, 2011）、具有独立效用的品类优化（Talluri and van Ryzin, 2004）以及委托选择（Khodabakhsh et al., 2024）。我们的结果提升了这些问题的最新技术水平。对于具有离散分布的单位需求定价问题，我们的乘法PTAS改进了Cai和Daskalakis的加法PTAS，并且我们还为无界正则情况提供了PTAS，改进了后者的QPTAS。对于品类优化问题，此前没有已知的常数近似。对于委托选择问题，我们改进了无外部选项情况下的3-近似和有外部选项情况下的超常数近似。\n\n驱动我们结果的一个关键技术洞察是一个具有经济学意义的特性，我们称之为效用对齐。非正式地说，如果在一个问题的最优解中，主体的大部分效用来源于代理人效用也较高的情景，则该问题是效用对齐的。效用对齐允许算法设计者专注于最大化代理人高效用情景下的性能，这通常是一个算法上更简单的任务。我们证明了上述所有问题的效用对齐结果，包括单位需求定价和委托选择的强结果，以及一个在非常温和的条件下适用于许多其他问题的较弱但非常广泛的保证。", "summary": "本文针对算法机制设计和运筹学领域的多个NP难随机优化问题，如单位需求定价、品类优化和委托选择，提出了多项式时间近似方案（PTAS）。通过引入并利用“效用对齐”这一核心经济学特性，即在最优情况下主体效用与代理人高效用情景相关联，研究者能够简化算法设计并取得显著进展。具体而言，本文改进了单位需求定价的近似算法，首次为品类优化提供了常数近似，并提升了委托选择问题的近似性能。该研究不仅为特定问题提供了更优的解决方案，还提出了一个具有广泛适用性的通用效用对齐保证。", "keywords": "多项式时间近似方案, 效用对齐, NP难问题, 随机优化, 机制设计", "comments": "本文的核心创新在于引入了“效用对齐”这一经济学概念，并将其作为设计高效近似算法的关键技术洞察。这种将经济学原理与算法设计相结合的方法，不仅为多个重要的NP难问题提供了更优的近似方案，而且其提出的广泛适用性保证具有深远的理论意义和潜在的实践价值。通过聚焦于“代理人高效用”情景，该方法巧妙地简化了复杂问题的优化过程，展现了其强大的分析能力。"}}
{"id": "2506.20195", "title": "A quasi-Grassmannian gradient flow model for eigenvalue problems", "authors": ["Shengyue Wang", "Aihui Zhou"], "summary": "We propose a quasi-Grassmannian gradient flow model for eigenvalue problems\nof linear operators, aiming to efficiently address many eigenpairs. Our model\ninherently ensures asymptotic orthogonality: without the need for initial\northogonality, the solution naturally evolves toward being orthogonal over\ntime. We establish the well-posedness of the model, and provide the analytical\nrepresentation of solutions. Through asymptotic analysis, we show that the\ngradient converges exponentially to zero and that the energy decreases\nexponentially to its minimum. This implies that the solution of the\nquasi-Grassmannian gradient flow model converges to the solution of the\neigenvalue problems as time progresses. These properties not only eliminate the\nneed for explicit orthogonalization in numerical computation but also\nsignificantly enhance robustness of the model, rendering it far more resilient\nto numerical perturbations than conventional methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20195v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20195v1", "AI": {"title_translation": "解决特征值问题的准格拉斯曼梯度流模型", "tldr": "提出一种新的准格拉斯曼梯度流模型，用于高效解决特征值问题，其特点是无需显式正交化即可渐近正交，且对数值扰动更具鲁棒性。", "motivation": "旨在高效解决线性算子的多个特征对（many eigenpairs）问题，并克服传统方法中需要显式正交化和对数值扰动敏感的缺点。", "method": "本文提出了一种准格拉斯曼梯度流模型来解决特征值问题。研究通过建立模型的适定性、提供解的解析表示，并通过渐近分析证明了梯度指数收敛到零以及能量指数下降到最小值。", "result": "所提出的模型确保了渐近正交性，无需初始正交性，解会自然地趋于正交。理论分析表明，梯度指数收敛到零，能量指数下降到其最小值，从而模型的解收敛到特征值问题的解。这些特性消除了数值计算中显式正交化的需要，并显著增强了模型的鲁棒性，使其比传统方法更能抵抗数值扰动。", "conclusion": "提出的准格拉斯曼梯度流模型能有效解决特征值问题，具有无需显式正交化和高鲁棒性的优点，超越了传统方法。", "translation": "我们提出了一种用于线性算子特征值问题的准格拉斯曼梯度流模型，旨在高效处理多个特征对。我们的模型固有地确保了渐近正交性：无需初始正交性，解会随时间自然地演变为正交。我们建立了模型的适定性，并提供了解决方案的解析表示。通过渐近分析，我们表明梯度指数收敛到零，并且能量指数下降到其最小值。这意味着准格拉斯曼梯度流模型的解随着时间的推移收敛到特征值问题的解。这些特性不仅消除了数值计算中显式正交化的需要，而且显著增强了模型的鲁棒性，使其比传统方法更能抵抗数值扰动。", "summary": "本文提出了一种创新的准格拉斯曼梯度流模型，专为高效求解线性算子的多个特征值问题而设计。该模型的核心优势在于其固有的渐近正交性，无需预设初始正交条件，解便能自然趋于正交。研究不仅证明了模型的适定性及解的解析形式，更通过渐近分析揭示了梯度和能量的指数收敛特性，从而确保了模型解对特征值问题解的收敛性。这些特性显著简化了数值计算过程，避免了显式正交化，并大幅提升了模型对数值扰动的鲁棒性，使其性能优于现有常规方法。", "keywords": "特征值问题, 梯度流, 准格拉斯曼, 渐近正交性, 数值鲁棒性", "comments": "这篇论文的创新点在于提出了一个无需显式正交化即可处理特征值问题的梯度流模型，并通过理论分析证明了其渐近正交性、收敛性和对数值扰动的鲁棒性。这对于数值计算来说是一个重要的进步，因为它简化了计算过程并提高了稳定性。"}}
{"id": "2506.20311", "title": "Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles", "authors": ["Jingwen Wei"], "summary": "The growing use of mobile robots in sectors such as automotive, agriculture,\nand rescue operations reflects progress in robotics and autonomy. In unmanned\naerial vehicles (UAVs), most research emphasizes visual SLAM, sensor fusion,\nand path planning. However, applying UAVs to search and rescue missions in\ndisaster zones remains underexplored, especially for autonomous navigation.\n  This report develops methods for real-time and secure UAV maneuvering in\ncomplex 3D environments, crucial during forest fires. Building upon past\nresearch, it focuses on designing navigation algorithms for unfamiliar and\nhazardous environments, aiming to improve rescue efficiency and safety through\nUAV-based early warning and rapid response.\n  The work unfolds in phases. First, a 2D fusion navigation strategy is\nexplored, initially for mobile robots, enabling safe movement in dynamic\nsettings. This sets the stage for advanced features such as adaptive obstacle\nhandling and decision-making enhancements. Next, a novel 3D reactive navigation\nstrategy is introduced for collision-free movement in forest fire simulations,\naddressing the unique challenges of UAV operations in such scenarios.\n  Finally, the report proposes a unified control approach that integrates UAVs\nand unmanned ground vehicles (UGVs) for coordinated rescue missions in forest\nenvironments. Each phase presents challenges, proposes control models, and\nvalidates them with mathematical and simulation-based evidence. The study\noffers practical value and academic insights for improving the role of UAVs in\nnatural disaster rescue operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20311v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20311v1", "AI": {"title_translation": "实时无人机和无人地面车辆避障算法", "tldr": "该报告开发了无人机和无人地面车辆在复杂3D环境中实时避障和协调救援的方法，特别针对森林火灾场景。", "motivation": "尽管移动机器人在多个领域应用日益广泛，但无人机在灾区搜救任务，尤其是在自主导航方面的应用仍未得到充分探索。本研究旨在解决无人机在森林火灾等复杂3D环境中实时安全机动的问题，以提高救援效率和安全性。", "method": "1. 探索了2D融合导航策略，最初用于移动机器人，以实现动态环境中的安全移动，并为自适应避障和决策增强奠定基础。2. 引入了一种新颖的3D反应式导航策略，用于在森林火灾模拟中实现无碰撞移动，解决了无人机在这些场景中操作的独特挑战。3. 提出了一个统一的控制方法，整合无人机和无人地面车辆，用于森林环境中的协调救援任务。每阶段都提出挑战、控制模型并进行数学和仿真验证。", "result": "报告开发了在复杂3D环境中实时安全机动的无人机方法，并引入了3D反应式导航策略，实现了森林火灾模拟中的无碰撞移动。最终提出了一个统一的无人机和无人地面车辆协调救援控制方法。研究结果通过数学和仿真验证。", "conclusion": "本研究为提高无人机在自然灾害救援行动中的作用提供了实用价值和学术见解，通过开发实时避障和协调控制算法，增强了无人机在复杂环境下的自主导航和救援能力。", "translation": "移动机器人在汽车、农业和救援行动等领域的日益广泛使用反映了机器人技术和自主性的进步。在无人机（UAV）领域，大多数研究强调视觉SLAM、传感器融合和路径规划。然而，将无人机应用于灾区搜救任务，尤其是在自主导航方面，仍未得到充分探索。本报告开发了在复杂3D环境中实时安全操作无人机的方法，这在森林火灾期间至关重要。在过去研究的基础上，本报告侧重于为陌生和危险环境设计导航算法，旨在通过基于无人机的早期预警和快速响应来提高救援效率和安全性。这项工作分阶段展开。首先，探索了一种2D融合导航策略，最初用于移动机器人，使其能够在动态环境中安全移动。这为自适应障碍物处理和决策增强等高级功能奠定了基础。其次，引入了一种新颖的3D反应式导航策略，用于在森林火灾模拟中实现无碰撞移动，解决了无人机在此类场景中操作的独特挑战。最后，报告提出了一种统一的控制方法，整合无人机和无人地面车辆，用于森林环境中的协调救援任务。每个阶段都提出了挑战，提出了控制模型，并用数学和基于仿真的证据验证了它们。这项研究为提高无人机在自然灾害救援行动中的作用提供了实用价值和学术见解。", "summary": "本研究致力于开发用于无人机和无人地面车辆在复杂三维环境（特别是森林火灾）中进行实时避障和自主导航的算法。研究分三阶段进行：首先是针对移动机器人的2D融合导航策略；其次是针对无人机在森林火灾模拟中的3D反应式导航策略；最后是整合无人机和无人地面车辆进行协同救援的统一控制方法。所有提出的模型都经过数学和仿真验证，旨在提高自然灾害搜救的效率和安全性。", "keywords": "无人机, 避障, 实时导航, 搜救, 森林火灾", "comments": "这篇论文的创新点在于提出了分阶段的导航策略，从2D扩展到3D，并最终整合了无人机和无人地面车辆进行协同救援，特别关注了森林火灾这种复杂且危险的环境。其重要性体现在解决了无人机在灾害搜救中自主导航的未充分探索领域，具有显著的实际应用价值。"}}
{"id": "2506.19884", "title": "MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection", "authors": ["Zhengxiang Huang", "Chaoyue Niu", "Zhaode Wang", "Jiarui Xue", "Hanming Zhang", "Yugang Wang", "Zewei Xin", "Xiaotang Jiang", "Chengfei Lv", "Fan Wu", "Guihai Chen"], "summary": "As the demand for on-device Large Language Model (LLM) inference grows,\nenergy efficiency has become a major concern, especially for battery-limited\nmobile devices. Our analysis shows that the memory-bound LLM decode phase\ndominates energy use, and yet most existing works focus on accelerating the\nprefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric\nCore Selection (AECS) and integrate it into MNN to create the energy-efficient\nversion, MNN-AECS, the first engine-level system solution without requiring\nroot access or OS modifications for energy-efficient LLM decoding. MNN-AECS is\ndesigned to reduce LLM decoding energy while keeping decode speed within an\nacceptable slowdown threshold by dynamically selecting low-power CPU cores.\nMNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of\nvarious sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%\nwithout slowdown averaged over all 7 devices and 4 datasets. Against other\nengines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS\ndelivers 39% to 78% energy saving and 12% to 363% speedup on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19884v1", "categories": ["cs.OS", "cs.AI", "cs.PF", "cs.SE"], "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.19884v1", "AI": {"title_translation": "MNN-AECS：基于自适应核心选择的移动设备LLM解码能耗优化", "tldr": "MNN-AECS通过动态选择低功耗CPU核心，在移动设备上显著降低LLM解码能耗，同时保持可接受的速度，是首个无需root权限的引擎级解决方案。", "motivation": "随着设备端大型语言模型（LLM）推理需求的增长，能效已成为一个主要关注点，特别是对于电池受限的移动设备。现有工作多关注预填充阶段加速，忽略了解码阶段的能耗，而解码阶段是能耗大户。", "method": "引入自适应能耗中心核心选择（AECS），并将其集成到MNN中，创建了MNN-AECS。该系统通过动态选择低功耗CPU核心来降低LLM解码能耗，同时将解码速度保持在可接受的减速阈值内。这是首个无需root权限或操作系统修改的引擎级系统解决方案。", "result": "与原始MNN相比，MNN-AECS在7台设备和4个数据集上平均降低23%的能耗，且无减速。与llama.cpp、executorch、mllm和MediaPipe等其他引擎相比，MNN-AECS平均能耗节省39%至78%，速度提升12%至363%。在5台Android和2台iOS设备上对5个流行LLM进行了评估。", "conclusion": "MNN-AECS成功地在移动设备上实现了LLM解码的显著能耗优化，同时保持了良好的性能，填补了现有研究在解码能耗优化方面的空白。", "translation": "随着设备端大型语言模型（LLM）推理需求的增长，能效已成为一个主要关注点，特别是对于电池受限的移动设备。我们的分析表明，内存受限的LLM解码阶段是能耗大户，然而大多数现有工作都专注于加速预填充阶段，忽略了能耗问题。我们引入了自适应能耗中心核心选择（AECS），并将其集成到MNN中，创建了能效版本MNN-AECS，这是首个无需root权限或操作系统修改即可实现LLM解码能效的引擎级系统解决方案。MNN-AECS旨在通过动态选择低功耗CPU核心来降低LLM解码能耗，同时将解码速度保持在可接受的减速阈值内。MNN-AECS在5台Android和2台iOS设备上，针对5种不同大小的流行LLM进行了评估。与原始MNN相比，MNN-AECS在所有7台设备和4个数据集上平均能耗降低23%且无减速。与llama.cpp、executorch、mllm和MediaPipe等其他引擎相比，MNN-AECS平均能耗节省39%至78%，速度提升12%至363%。", "summary": "MNN-AECS是一个为移动设备LLM解码设计的能效优化引擎级解决方案。它通过自适应核心选择动态利用低功耗CPU核心来降低能耗，同时维持可接受的解码速度。实验证明，MNN-AECS在多设备和LLM上，相比现有引擎显著降低能耗并提升速度，无需root权限或系统修改。", "keywords": "LLM解码, 能耗优化, 移动设备, 自适应核心选择, MNN-AECS", "comments": "这项工作通过引入MNN-AECS，创新性地解决了移动设备上LLM解码阶段的能耗问题，这是现有研究中一个被忽视但关键的领域。其亮点在于无需root权限或OS修改即可实现引擎级优化，大大降低了部署门槛。研究结果显示出显著的能耗节省和速度提升，对推动LLM在移动端的普及具有重要意义。"}}
{"id": "2506.20174", "title": "Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition", "authors": ["Man Duc Chuc"], "summary": "Foundation models are rapidly transforming Earth Observation data mining by\nenabling generalizable and scalable solutions for key tasks such as scene\nclassification and semantic segmentation. While most efforts in the geospatial\ndomain have focused on developing large models trained from scratch using\nmassive Earth Observation datasets, an alternative strategy that remains\nunderexplored is the reuse and combination of existing pretrained models. In\nthis study, we investigate whether foundation models pretrained on remote\nsensing and general vision datasets can be effectively combined to improve\nperformance across a diverse set of key Earth Observation tasks. Using the\nGEO-Bench benchmark, we evaluate several prominent models, including Prithvi,\nHiera, and DOFA, on eleven datasets covering a range of spatial resolutions,\nsensor modalities, and task types. The results show that feature-level\nensembling of smaller pretrained models can match or exceed the performance of\nmuch larger models, while requiring less training time and computational\nresources. Moreover, the study highlights the potential of applying knowledge\ndistillation to transfer the strengths of ensembles into more compact models,\noffering a practical path for deploying foundation models in real-world Earth\nObservation applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20174v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20174v1", "AI": {"title_translation": "通过基础模型组合实现可扩展和通用地球观测数据挖掘", "tldr": "研究表明，通过组合现有预训练模型，可以在地球观测数据挖掘中实现与大型模型相当或更优的性能，同时减少资源消耗。", "motivation": "现有地球观测数据挖掘中的基础模型大多从头开始训练，需要大量数据和计算资源。本研究旨在探索重用和组合现有预训练模型作为替代策略的有效性。", "method": "研究使用GEO-Bench基准，评估了包括Prithvi、Hiera和DOFA在内的多个模型，在涵盖不同空间分辨率、传感器模态和任务类型的11个数据集上进行测试。具体方法是特征级集成较小的预训练模型，并探讨知识蒸馏的应用。", "result": "结果显示，较小预训练模型的特征级集成可以达到或超过大型模型的性能，同时所需训练时间和计算资源更少。", "conclusion": "组合现有预训练模型是实现可扩展和通用地球观测数据挖掘的有效策略，并且知识蒸馏为在实际应用中部署基础模型提供了实用路径。", "translation": "基础模型正通过为场景分类和语义分割等关键任务提供通用化和可扩展的解决方案，迅速改变地球观测数据挖掘。虽然地理空间领域的大部分努力都集中在利用海量地球观测数据集从头开始开发大型模型，但一种尚未充分探索的替代策略是重用和组合现有预训练模型。在本研究中，我们调查了在遥感和通用视觉数据集上预训练的基础模型是否可以有效组合，以提高在各种关键地球观测任务上的性能。我们使用GEO-Bench基准，评估了包括Prithvi、Hiera和DOFA在内的多个著名模型，涵盖了不同空间分辨率、传感器模态和任务类型的11个数据集。结果表明，较小预训练模型的特征级集成可以达到或超过大型模型的性能，同时所需训练时间和计算资源更少。此外，该研究强调了应用知识蒸馏将集成模型的优势转移到更紧凑模型中的潜力，为在现实世界地球观测应用中部署基础模型提供了实用路径。", "summary": "本文探讨了通过组合现有预训练的基础模型来提升地球观测数据挖掘性能的策略，以替代从头训练大型模型。研究利用GEO-Bench基准，在多类型数据集上评估了特征级集成方法，结果表明这种方法能以更低的资源消耗达到或超越大型模型的性能。此外，研究还提出了通过知识蒸馏将集成优势转移到更紧凑模型以实现实际部署的潜力。", "keywords": "地球观测, 基础模型, 模型组合, 知识蒸馏", "comments": "这篇论文提出了一种创新的方法来解决地球观测领域基础模型部署的资源限制问题。通过强调现有预训练模型的组合和知识蒸馏，它为开发更高效、更易于部署的EO数据挖掘解决方案提供了实用且有前景的路径，有望降低对大规模计算资源的需求。"}}
{"id": "2506.19935", "title": "Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture", "authors": ["Shuchen Xue", "Tianyu Xie", "Tianyang Hu", "Zijin Feng", "Jiacheng Sun", "Kenji Kawaguchi", "Zhenguo Li", "Zhi-Ming Ma"], "summary": "Large language models (LLMs) predominantly use autoregressive (AR)\napproaches, but masked diffusion models (MDMs) are emerging as viable\nalternatives. A key challenge in comparing AR and MDM paradigms is their\ntypical architectural difference: AR models are often decoder-only, while MDMs\nhave largely been encoder-only. This practice of changing both the modeling\nparadigm and architecture simultaneously makes direct comparisons unfair, as\nit's hard to distinguish whether observed differences stem from the paradigm\nitself or the architectural shift. This research evaluates MDMs within a\ndecoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or\nAO-AR) and standard AR paradigms. Our investigation suggests that the standard\nAO-AR objective, which averages over all token permutations, may benefit from\nrefinement, as many permutations appear less informative compared to the\nlanguage's inherent left-to-right structure. (2) Investigate architectural\ninfluences (decoder-only vs. encoder-only) within MDMs. We demonstrate that\nwhile encoder-only MDMs model a simpler conditional probability space,\ndecoder-only MDMs can achieve dramatic generation speedups ($\\sim25\\times$) and\ncomparable perplexity with temperature annealing despite modeling a vastly\nlarger space, highlighting key trade-offs. This work thus decouples core\nparadigm differences from architectural influences, offering insights for\nfuture model design. Code is available at https://github.com/scxue/AO-GPT-MDM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19935v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19935v1", "AI": {"title_translation": "任意顺序GPT作为掩码扩散模型：解耦公式与架构", "tldr": "本研究在解码器-only框架下评估掩码扩散模型（MDMs），以公平比较MDM和自回归（AR）范式，并探究MDM中架构（解码器-only vs. 编码器-only）的影响，发现解码器-only MDM可显著加速生成。", "motivation": "比较自回归（AR）模型和掩码扩散模型（MDMs）存在一个关键挑战：它们通常有不同的架构（AR模型常为解码器-only，MDM多为编码器-only）。这种同时改变建模范式和架构的做法使得直接比较不公平，难以区分差异是源于范式本身还是架构变化。", "method": "本研究在解码器-only框架内评估掩码扩散模型（MDM），旨在：1) 公平比较MDM（作为任意顺序自回归AO-AR）与标准AR范式；2) 探究MDM内部的架构影响（解码器-only vs. 编码器-only）。", "result": "1. 对于标准AO-AR目标，其平均所有词元排列可能需要改进，因为许多排列相比语言固有的从左到右结构信息量较少。\n2. 编码器-only MDM建模更简单的条件概率空间，而解码器-only MDM尽管建模的空间大得多，但通过温度退火可实现显著的生成速度提升（约25倍）和相当的困惑度，突出了关键的权衡。", "conclusion": "本工作解耦了核心范式差异与架构影响，为未来的模型设计提供了见解。", "translation": "大型语言模型（LLMs）主要使用自回归（AR）方法，但掩码扩散模型（MDMs）正作为可行的替代方案出现。比较AR和MDM范式的关键挑战在于它们典型的架构差异：AR模型通常是仅解码器，而MDMs则主要是仅编码器。这种同时改变建模范式和架构的做法使得直接比较不公平，因为很难区分观察到的差异是源于范式本身还是架构转变。本研究在仅解码器框架内评估MDM，旨在：(1) 公平比较MDM（作为任意顺序AR，或AO-AR）和标准AR范式。我们的调查表明，标准的AO-AR目标（其对所有词元排列进行平均）可能需要改进，因为与语言固有的从左到右结构相比，许多排列的信息量似乎较少。(2) 调查MDM内部的架构影响（仅解码器 vs. 仅编码器）。我们证明，虽然仅编码器MDM建模一个更简单的条件概率空间，但仅解码器MDM尽管建模一个 vastly 更大的空间，却可以通过温度退火实现显著的生成速度提升（约25倍）和相当的困惑度，这突出了关键的权衡。因此，这项工作将核心范式差异与架构影响解耦，为未来的模型设计提供了见解。代码可在https://github.com/scxue/AO-GPT-MDM 获取。", "summary": "本论文旨在解决大型语言模型中自回归（AR）模型和掩码扩散模型（MDMs）比较不公平的问题，因为它们通常具有不同的架构。研究通过在解码器-only框架下评估MDM来解耦建模范式和架构的影响。结果表明，标准的任意顺序自回归（AO-AR）目标可能需要改进，且解码器-only MDM在生成速度上比编码器-only MDM有显著优势（约25倍），同时保持相当的困惑度，尽管其建模空间更大。这项工作为未来的模型设计提供了关于范式和架构之间权衡的见解。", "keywords": "掩码扩散模型, 自回归模型, 解码器-only, 编码器-only, 架构解耦", "comments": "这项研究的创新之处在于它首次系统地解耦了大型语言模型中建模范式（AR vs. MDM）与架构（解码器-only vs. 编码器-only）的影响，从而实现了更公平的比较。其重要性在于揭示了在解码器-only框架下，掩码扩散模型能够实现显著的生成速度提升，这对于未来的高效模型设计具有重要指导意义。"}}
{"id": "2506.20274", "title": "Enterprise Large Language Model Evaluation Benchmark", "authors": ["Liya Wang", "David Yi", "Damien Jose", "John Passarelli", "James Gao", "Jordan Leventis", "Kang Li"], "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.", "comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index", "pdf_url": "http://arxiv.org/pdf/2506.20274v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20274v1", "AI": {"title_translation": "企业大型语言模型评估基准", "tldr": "现有LLM基准不适用于企业场景，本文提出了一个基于Bloom分类法的14任务框架和可扩展的评估流程，构建了9700个样本的基准，发现开源模型在推理上可与专有模型媲美，但在判断任务上落后。", "motivation": "现有的大型语言模型（LLMs）基准（如MMLU）不足以评估企业特定任务的复杂性，无法有效衡量LLM在企业环境中的生产力提升潜力。", "method": "提出了一个基于Bloom分类法的14任务框架，用于全面评估企业环境中的LLM能力。开发了一个可扩展的数据管道，结合了“LLM即标签器”、“LLM即评估器”和纠正性检索增强生成（CRAG），以应对噪声数据和昂贵标注的挑战。构建了一个包含9,700个样本的鲁棒基准。评估了六个领先的模型，包括开源模型（如DeepSeek R1）和专有模型。", "result": "开源模型（如DeepSeek R1）在推理任务上与专有模型不相上下。开源模型在基于判断的场景中表现落后，这可能归因于“过度思考”。基准揭示了关键的企业性能差距。提供了模型优化的可行见解。", "conclusion": "本工作为企业提供了量身定制的评估蓝图，并推动了LLM在实际企业部署中的进展。", "translation": "大型语言模型（LLMs）在提升AI驱动工具的生产力方面展现出前景，然而，现有基准如大规模多任务语言理解（MMLU）不足以评估企业特定任务的复杂性。我们提出了一个基于布鲁姆分类法的14任务框架，以全面评估LLM在企业环境中的能力。为了解决数据噪声和昂贵标注的挑战，我们开发了一个可扩展的管道，结合了“LLM即标签器”、“LLM即评估器”和纠正性检索增强生成（CRAG），从而构建了一个包含9,700个样本的鲁棒基准。对六个领先模型的评估显示，像DeepSeek R1这样的开源竞争者在推理任务上与专有模型不相上下，但在基于判断的场景中表现落后，这可能是由于“过度思考”所致。我们的基准揭示了关键的企业性能差距，并为模型优化提供了可行的见解。这项工作为企业提供了量身定制的评估蓝图，并推动了实际LLM部署的进展。", "summary": "本文提出了一个针对企业级大型语言模型（LLM）的14任务评估框架和9700样本基准，旨在解决现有基准在评估企业特定复杂性方面的不足。该框架基于Bloom分类法，并采用“LLM即标签器”、“LLM即评估器”及CRAG的组合管道来克服数据噪声和标注成本问题。评估结果显示，开源模型在推理任务上与专有模型表现相当，但在判断任务上存在差距。这项工作为企业提供了定制化评估方案，有助于LLM的实际部署。", "keywords": "大型语言模型, 企业评估, 基准, Bloom分类法, 数据管道", "comments": "这项工作通过引入一个专门针对企业场景的LLM评估基准，填补了现有通用基准的空白。其创新之处在于结合了Bloom分类法构建多维度任务框架，并提出了利用LLM自身进行数据标注和评估的自动化流程，有效解决了大规模高质量企业数据获取的难题。研究结果揭示了开源和专有模型在不同企业任务中的性能差异，为企业选择和优化LLM提供了宝贵的实践指导，对推动LLM在企业领域的实际应用具有重要意义。"}}
{"id": "2506.19934", "title": "A Hybrid Intrusion Detection System with a New Approach to Protect the Cybersecurity of Cloud Computing", "authors": ["Maryam Mahdi Al-Husseini"], "summary": "Cybersecurity is one of the foremost challenges facing the world of cloud\ncomputing. Recently, the widespread adoption of smart devices in cloud\ncomputing environments that provide Internet-based services has become\nprevalent. Therefore, it is essential to consider the security threats in these\nenvironments. The use of intrusion detection systems can mitigate the\nvulnerabilities of these systems. Furthermore, hybrid intrusion detection\nsystems can provide better protection compared to conventional intrusion\ndetection systems. These systems manage issues related to complexity,\ndimensionality, and performance. This research aims to propose a Hybrid\nIntrusion Detection System (HyIDS) that identifies and mitigates initial\nthreats. The main innovation of this research is the introduction of a new\nmethod for hybrid intrusion detection systems (HyIDS). For this purpose, an\nEnergy-Valley Optimizer (EVO) is used to select an optimal feature set, which\nis then classified using supervised machine learning models. The proposed\napproach is evaluated using the CIC_DDoS2019, CSE_CIC_DDoS2018, and NSL-KDD\ndatasets. For evaluation and testing, the proposed system has been run for a\ntotal of 32 times. The results of the proposed approach are compared with the\nGrey Wolf Optimizer (GWO). With the CIC_DDoS2019 dataset, the D_TreeEVO model\nachieves an accuracy of 99.13% and a detection rate of 98.941%. Furthermore,\nthis result reaches 99.78% for the CSE_CIC_DDoS2018 dataset. In comparison to\nNSL-KDD, it has an accuracy of 99.50% and a detection rate (DT) of 99.48%. For\nfeature selection, EVO outperforms GWO. The results of this research indicate\nthat EVO yields better results as an optimizer for HyIDS performance.", "comment": "1. Acknowledgment for: Supervisor: Prof. Dr. Alireza Rouhi Advisor:\n  Prof. Dr. Einollah Pira 2. Thesis of MSc. degree for Azarbaijan Shahid Madani\n  University Faculty of Information Technology and Computer Engineering 3.\n  Number of pages: 103 4. Number of Figures: 66", "pdf_url": "http://arxiv.org/pdf/2506.19934v1", "categories": ["cs.CR", "cs.SY", "eess.SY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.19934v1", "AI": {"title_translation": "一种采用新方法保护云计算网络安全的混合入侵检测系统", "tldr": "本文提出了一种基于能量谷优化器（EVO）和监督机器学习的混合入侵检测系统（HyIDS），用于提升云计算环境下的网络安全防御能力，并在多个数据集上取得了高准确率。", "motivation": "云计算环境下的网络安全是主要挑战，随着智能设备在云计算环境中的广泛应用，安全威胁日益突出。入侵检测系统，特别是混合入侵检测系统，能有效缓解这些漏洞并提供更好的保护。", "method": "研究提出了一种混合入侵检测系统（HyIDS）。其创新点在于引入了一种新的混合入侵检测方法，该方法利用能量谷优化器（EVO）来选择最优特征集，然后使用监督机器学习模型进行分类。该方法在CIC_DDoS2019、CSE_CIC_DDoS2018和NSL-KDD数据集上进行了32次评估和测试，并与灰狼优化器（GWO）进行了比较。", "result": "在CIC_DDoS2019数据集上，D_TreeEVO模型的准确率达到99.13%，检测率达到98.941%。在CSE_CIC_DDoS2018数据集上，准确率达到99.78%。在NSL-KDD数据集上，准确率为99.50%，检测率为99.48%。在特征选择方面，EVO的表现优于GWO。", "conclusion": "研究结果表明，作为HyIDS性能的优化器，EVO能够产生更好的结果。", "translation": "网络安全是云计算领域面临的首要挑战之一。最近，智能设备在提供基于互联网服务的云计算环境中的广泛应用已变得普遍。因此，考虑这些环境中的安全威胁至关重要。使用入侵检测系统可以减轻这些系统的漏洞。此外，与传统入侵检测系统相比，混合入侵检测系统可以提供更好的保护。这些系统管理与复杂性、维度和性能相关的问题。本研究旨在提出一种混合入侵检测系统（HyIDS），用于识别和缓解初始威胁。本研究的主要创新是为混合入侵检测系统（HyIDS）引入了一种新方法。为此，使用能量谷优化器（EVO）来选择最优特征集，然后使用监督机器学习模型进行分类。所提出的方法使用CIC_DDoS2019、CSE_CIC_DDoS2018和NSL-KDD数据集进行评估。为了评估和测试，所提出的系统总共运行了32次。所提出方法的结果与灰狼优化器（GWO）进行了比较。对于CIC_DDoS2019数据集，D_TreeEVO模型实现了99.13%的准确率和98.941%的检测率。此外，对于CSE_CIC_DDoS2018数据集，该结果达到了99.78%。与NSL-KDD相比，它具有99.50%的准确率和99.48%的检测率（DT）。在特征选择方面，EVO优于GWO。本研究的结果表明，EVO作为HyIDS性能的优化器，能够产生更好的结果。", "summary": "本文针对云计算环境中的网络安全挑战，提出了一种名为HyIDS的混合入侵检测系统。该系统创新性地采用能量谷优化器（EVO）进行最优特征选择，并结合监督机器学习模型进行威胁分类。通过在CIC_DDoS2019、CSE_CIC_DDoS2018和NSL-KDD等多个数据集上的评估，HyIDS展示了高准确率和检测率，并证明EVO在特征选择方面优于传统优化器，显著提升了HyIDS的性能。", "keywords": "混合入侵检测系统, 云计算, 网络安全, 能量谷优化器, 特征选择", "comments": "本文的创新点在于将能量谷优化器（EVO）引入到混合入侵检测系统（HyIDS）的特征选择过程中，从而提升了系统的性能。该研究对于增强云计算环境下的网络安全防御具有重要意义，特别是在处理复杂性和高维度数据方面。实验结果表明了其方法的有效性，但未提及计算开销或实时性方面的详细评估。"}}
{"id": "2506.20595", "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student Writing", "authors": ["Momin N. Siddiqui", "Roy Pea", "Hari Subramonyam"], "summary": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20595v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20595v1", "AI": {"title_translation": "写作过程中的人工智能：有目的的人工智能支持如何促进学生写作", "tldr": "本研究通过随机对照试验发现，与基于聊天的AI助手相比，设计良好、整合了写作子过程的AI工具能更好地提升学生的写作自主性和知识转化深度。", "motivation": "当前，ChatGPT等技术在学生写作中的普及引发了对其影响的担忧，特别是可能降低学习者的自主性和对内容的肤浅参与。然而，有证据表明，有目的设计的AI写作支持工具可以增强写作过程，因此本研究旨在探究不同AI支持方法如何影响作者的自主性和知识转化深度。", "method": "本研究通过一项包含90名本科生的随机对照试验，比较了三种条件：1) 基于聊天的LLM写作助手；2) 支持多种子过程的集成AI写作工具；3) 标准写作界面（对照组）。", "result": "研究结果表明，在AI支持的条件下，使用集成AI写作工具的学生在写作过程中表现出更高的自主性，并进行了更深层次的知识转化。", "conclusion": "这些结果表明，有针对性地设计AI写作支持工具，以解决写作过程中的特定方面，可以帮助学生在促进内容参与的同时，保持对其作品的所有权。", "translation": "人工智能在写作过程中的应用：有目的的人工智能支持如何促进学生写作\n\nChatGPT等技术的普及引发了对其对学生写作影响的担忧，特别是关于学习者自主性降低和对内容肤浅参与的问题。虽然独立的基于聊天的LLM通常会产生次优的写作结果，但有证据表明，有目的设计的AI写作支持工具可以增强写作过程。本文研究了不同的人工智能支持方法如何影响作者的自主性和知识转化深度。通过对90名本科生进行随机对照试验，我们比较了三种条件：(1) 基于聊天的LLM写作助手，(2) 支持多种子过程的集成AI写作工具，以及(3) 标准写作界面（对照组）。我们的研究结果表明，在人工智能支持的条件下，使用集成人工智能写作工具的学生在写作过程中表现出更大的自主性，并进行了更深层次的知识转化。这些结果表明，深思熟虑地设计针对写作过程特定方面的人工智能写作支持可以帮助学生保持对其作品的所有权，同时促进对内容的更好参与。", "summary": "本研究旨在探究不同AI写作支持工具对学生写作自主性和知识转化深度的影响。通过一项针对90名本科生的随机对照试验，比较了基于聊天的LLM、集成AI写作工具和标准写作界面。结果发现，与基于聊天的LLM相比，集成AI写作工具能显著提升学生的写作自主性和知识转化深度，强调了精心设计的AI支持在促进学生写作过程中的积极作用。", "keywords": "人工智能写作, 学生自主性, 知识转化, 集成AI工具, 随机对照试验", "comments": "本研究的创新之处在于通过严格的随机对照试验，对比了不同类型的AI写作支持工具对学生写作过程的具体影响，特别是关注了“自主性”和“知识转化深度”这两个关键指标。其重要性在于为未来AI教育工具的设计提供了实证依据，指出并非所有AI都降低学生能力，而是有目的地、集成化的AI支持反而能增强学习体验。这对于平衡AI在教育中的应用利弊具有指导意义。"}}
{"id": "2506.20626", "title": "Task Allocation of UAVs for Monitoring Missions via Hardware-in-the-Loop Simulation and Experimental Validation", "authors": ["Hamza Chakraa", "François Guérin", "Edouard Leclercq", "Dimitri Lefebvre"], "summary": "This study addresses the optimisation of task allocation for Unmanned Aerial\nVehicles (UAVs) within industrial monitoring missions. The proposed methodology\nintegrates a Genetic Algorithms (GA) with a 2-Opt local search technique to\nobtain a high-quality solution. Our approach was experimentally validated in an\nindustrial zone to demonstrate its efficacy in real-world scenarios. Also, a\nHardware-in-the-loop (HIL) simulator for the UAVs team is introduced. Moreover,\ninsights about the correlation between the theoretical cost function and the\nactual battery consumption and time of flight are deeply analysed. Results show\nthat the considered costs for the optimisation part of the problem closely\ncorrelate with real-world data, confirming the practicality of the proposed\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20626v1", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20626v1", "AI": {"title_translation": "无人机监控任务的任务分配：通过硬件在环仿真和实验验证", "tldr": "本研究提出了一种结合遗传算法和2-Opt局部搜索的无人机任务分配优化方法，并通过硬件在环仿真和实际实验验证了其有效性。", "motivation": "解决工业监控任务中无人机任务分配的优化问题。", "method": "本研究提出了一种将遗传算法（GA）与2-Opt局部搜索技术相结合的方法来优化无人机任务分配，并引入了无人机团队的硬件在环（HIL）模拟器。", "result": "结果表明，所考虑的优化成本函数与实际电池消耗和飞行时间密切相关，证实了所提出方法的实用性。", "conclusion": "所提出的无人机任务分配优化方法在实际工业监控场景中是有效和实用的。", "translation": "本研究旨在优化工业监控任务中无人机（UAV）的任务分配。所提出的方法将遗传算法（GA）与2-Opt局部搜索技术相结合，以获得高质量的解决方案。我们的方法在工业区域进行了实验验证，以证明其在实际场景中的有效性。此外，还引入了无人机团队的硬件在环（HIL）模拟器。此外，深入分析了理论成本函数与实际电池消耗和飞行时间之间的相关性。结果表明，问题优化部分所考虑的成本与真实世界数据密切相关，证实了所提出方法的实用性。", "summary": "本研究提出了一种优化工业监控任务中无人机任务分配的方法。该方法结合了遗传算法和2-Opt局部搜索，并通过硬件在环仿真和工业区域的实验验证了其在实际场景中的有效性。研究还分析了理论成本函数与实际电池消耗和飞行时间之间的相关性，结果证实了该方法的实用性。", "keywords": "无人机任务分配, 遗传算法, 硬件在环仿真, 工业监控, 2-Opt局部搜索", "comments": "这项研究的创新点在于结合了遗传算法和2-Opt局部搜索来优化无人机任务分配，并通过硬件在环仿真和实际工业场景的实验验证，增强了结果的可信度和实用性。对理论成本与实际能耗和飞行时间的关联分析也为实际应用提供了重要见解。"}}
{"id": "2506.20084", "title": "Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers", "authors": ["Mohanad Obeed", "Ming Jian"], "summary": "Towards fast, hardware-efficient, and low-complexity receivers, we propose a\ncompression-aware learning approach and examine it on free-space optical (FSO)\nreceivers for turbulence mitigation. The learning approach jointly quantize,\nprune, and train a convolutional neural network (CNN). In addition, we propose\nto have the CNN weights of power of two values so we replace the multiplication\noperations bit-shifting operations in every layer that has significant lower\ncomputational cost. The compression idea in the proposed approach is that the\nloss function is updated and both the quantization levels and the pruning\nlimits are optimized in every epoch of training. The compressed CNN is examined\nfor two levels of compression (1-bit and 2-bits) over different FSO systems.\nThe numerical results show that the compression approach provides negligible\ndecrease in performance in case of 1-bit quantization and the same performance\nin case of 2-bits quantization, compared to the full-precision CNNs. In\ngeneral, the proposed IM/DD FSO receivers show better bit-error rate (BER)\nperformance (without the need for channel state information (CSI)) compared to\nthe maximum likelihood (ML) receivers that utilize imperfect CSI when the DL\nmodel is compressed whether with 1-bit or 2-bit quantization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20084v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20084v1", "AI": {"title_translation": "联合量化与剪枝神经网络方法：以FSO接收器为例", "tldr": "本文提出一种联合量化和剪枝的神经网络压缩方法，应用于自由空间光（FSO）接收器，旨在实现快速、硬件高效和低复杂度的通信，并在保持性能的同时实现高压缩。", "motivation": "旨在实现快速、硬件高效和低复杂度的接收器，特别是在自由空间光（FSO）接收器中用于湍流缓解。", "method": "提出一种压缩感知学习方法，联合量化、剪枝和训练卷积神经网络（CNN）。此外，将CNN权重设为2的幂次，用位移操作替代乘法运算以降低计算成本。该方法在每个训练周期中更新损失函数并优化量化级别和剪枝限制。", "result": "数值结果表明，与全精度CNN相比，1比特量化时性能下降可忽略不计，2比特量化时性能相同。所提出的IM/DD FSO接收器（无论1比特还是2比特量化）在误码率（BER）方面优于使用不完善CSI的最大似然（ML）接收器，且无需信道状态信息（CSI）。", "conclusion": "所提出的联合量化和剪枝方法能够显著压缩神经网络模型，同时在自由空间光（FSO）接收器应用中保持甚至提升性能，特别是在无需信道状态信息（CSI）的情况下。", "translation": "标题：联合量化与剪枝神经网络方法：以FSO接收器为例\n摘要：为了实现快速、硬件高效和低复杂度的接收器，我们提出了一种压缩感知学习方法，并将其应用于自由空间光通信（FSO）接收器以减轻湍流影响。该学习方法联合对卷积神经网络（CNN）进行量化、剪枝和训练。此外，我们建议将CNN权重设为2的幂次值，这样我们就可以用位移操作代替每一层中的乘法操作，从而显著降低计算成本。所提出方法中的压缩思想是：在每个训练周期中更新损失函数，并优化量化级别和剪枝限制。压缩后的CNN在不同的FSO系统上以两种压缩级别（1比特和2比特）进行了检验。数值结果表明，与全精度CNN相比，该压缩方法在1比特量化情况下性能下降可忽略不计，在2比特量化情况下性能相同。总的来说，所提出的IM/DD FSO接收器显示出更好的误码率（BER）性能（无需信道状态信息（CSI）），与利用不完善CSI的最大似然（ML）接收器相比，无论深度学习模型是1比特还是2比特量化压缩。", "summary": "本文提出一种用于自由空间光（FSO）接收器的联合量化与剪枝神经网络压缩方法，旨在实现快速、硬件高效和低复杂度的通信。该方法通过联合优化量化级别和剪枝限制来训练卷积神经网络（CNN），并将权重限制为2的幂次以用位移操作替代乘法。实验结果表明，该方法在1比特和2比特量化下能保持与全精度CNN相当的性能，并且在误码率（BER）方面优于传统最大似然（ML）接收器，且无需信道状态信息（CSI）。", "keywords": "神经网络压缩, 量化, 剪枝, 自由空间光通信, FSO接收器", "comments": "该论文的创新点在于提出了一种端到端的联合量化和剪枝训练方法，并特别将权重限制为2的幂次，这对于硬件实现（特别是FPGA或ASIC）具有重要意义，能够显著降低计算复杂度和功耗。将其应用于自由空间光（FSO）接收器，解决了实际通信系统中的湍流缓解问题，并展示了在低比特量化下依然优于传统方法的潜力，凸显了深度学习在通信领域的应用价值。"}}
{"id": "2506.20333", "title": "EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis", "authors": ["Jiayan Chen", "Kai Li", "Yulu Zhao", "Jianqiang Huang", "Zhan Wang"], "summary": "Hepatic echinococcosis (HE) is a widespread parasitic disease in\nunderdeveloped pastoral areas with limited medical resources. While CNN-based\nand Transformer-based models have been widely applied to medical image\nsegmentation, CNNs lack global context modeling due to local receptive fields,\nand Transformers, though capable of capturing long-range dependencies, are\ncomputationally expensive. Recently, state space models (SSMs), such as Mamba,\nhave gained attention for their ability to model long sequences with linear\ncomplexity. In this paper, we propose EAGLE, a U-shaped network composed of a\nProgressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space\n(HVSS) decoder that work collaboratively to achieve efficient and accurate\nsegmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional\nVision State Space Block (CVSSB) module is designed to fuse local and global\nfeatures, while the Haar Wavelet Transformation Block (HWTB) module compresses\nspatial information into the channel dimension to enable lossless downsampling.\nDue to the lack of publicly available HE datasets, we collected CT slices from\n260 patients at a local hospital. Experimental results show that EAGLE achieves\nstate-of-the-art performance with a Dice Similarity Coefficient (DSC) of\n89.76%, surpassing MSVM-UNet by 1.61%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20333v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20333v1", "AI": {"title_translation": "EAGLE：一种用于肝包虫病的高效全局注意力病灶分割模型", "tldr": "EAGLE是一种高效的U型网络，利用状态空间模型和新型模块，实现了肝包虫病灶的精确分割，并在自建数据集上表现优异。", "motivation": "肝包虫病（HE）是一种在医疗资源有限的牧区广泛流行的寄生虫病。现有CNN模型由于局部感受野缺乏全局上下文建模能力，而Transformer模型虽然能够捕获长距离依赖，但计算成本高昂。", "method": "本文提出了一种名为EAGLE的U型网络模型，它由渐进式视觉状态空间（PVSS）编码器和混合视觉状态空间（HVSS）解码器组成。模型设计了卷积视觉状态空间块（CVSSB）用于融合局部和全局特征，以及Haar小波变换块（HWTB）用于无损下采样。由于缺乏公开数据集，研究团队收集了260名患者的CT切片构建了私有数据集。", "result": "EAGLE模型在肝包虫病灶分割任务中实现了最先进的性能，Dice相似系数（DSC）达到89.76%，比MSVM-UNet高出1.61%。", "conclusion": "EAGLE模型能够高效准确地分割肝包虫病灶，为医疗资源有限地区的诊断提供了有效工具。", "translation": "肝包虫病（HE）是一种在医疗资源有限的欠发达牧区广泛流行的寄生虫病。虽然基于CNN和基于Transformer的模型已广泛应用于医学图像分割，但CNN由于局部感受野缺乏全局上下文建模能力，而Transformer虽然能够捕获长距离依赖，但计算成本高昂。最近，状态空间模型（SSM），如Mamba，因其能够以线性复杂度建模长序列而受到关注。在本文中，我们提出了EAGLE，一个由渐进式视觉状态空间（PVSS）编码器和混合视觉状态空间（HVSS）解码器组成的U型网络，它们协同工作以实现肝包虫病（HE）病灶的高效准确分割。所提出的卷积视觉状态空间块（CVSSB）模块旨在融合局部和全局特征，而Haar小波变换块（HWTB）模块将空间信息压缩到通道维度，从而实现无损下采样。由于缺乏公开可用的HE数据集，我们从当地医院收集了260名患者的CT切片。实验结果表明，EAGLE实现了最先进的性能，Dice相似系数（DSC）为89.76%，超过MSVM-UNet 1.61%。", "summary": "本文提出了一种名为EAGLE的U型深度学习模型，用于肝包虫病（HE）病灶的高效精确分割。该模型结合了渐进式视觉状态空间编码器和混合视觉状态空间解码器，并引入了卷积视觉状态空间块（CVSSB）以融合局部和全局特征，以及Haar小波变换块（HWTB）进行无损下采样。针对缺乏公开数据集的问题，研究者构建了一个包含260名患者CT数据的私有数据集。实验结果显示，EAGLE在肝包虫病灶分割上达到了89.76%的Dice相似系数，性能优于现有模型，展现了其在医疗资源有限地区的应用潜力。", "keywords": "肝包虫病, 医学图像分割, 状态空间模型, U型网络, 全局注意力", "comments": "EAGLE的创新点在于结合了U型网络架构与状态空间模型，并通过特制的CVSSB和HWTB模块解决了传统CNN和Transformer在医学图像分割中的局限性（局部性与计算成本）。其在自建数据集上的优异表现，特别是针对缺乏公开数据的罕见病种，显示了其重要的临床应用价值。"}}
{"id": "2506.20081", "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization", "authors": ["Dhruv Gupta", "Gayathri Ganesh Lakshmy", "Yiqing Xie"], "summary": "Retrieval-Augmented Code Generation (RACG) is a critical technique for\nenhancing code generation by retrieving relevant information. In this work, we\nconduct an in-depth analysis of code retrieval by systematically masking\nspecific features while preserving code functionality. Our discoveries include:\n(1) although trained on code, current retrievers heavily rely on surface-level\ntextual features (e.g., docstrings, identifier names), and (2) they exhibit a\nstrong bias towards well-documented code, even if the documentation is\nirrelevant.Based on our discoveries, we propose SACL, a framework that enriches\ntextual information and reduces bias by augmenting code or structural knowledge\nwith semantic information. Extensive experiments show that SACL substantially\nimproves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /\nMBPP / SWE-Bench-Lite), which also leads to better code generation performance\n(e.g., by 4.88% Pass@1 on HumanEval).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20081v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20081v1", "AI": {"title_translation": "SACL：通过语义增强重排序和定位理解并对抗代码检索中的文本偏见", "tldr": "本文深入分析了代码检索中的文本偏见问题，发现现有检索器过度依赖表面文本特征且对文档良好的代码存在偏见。为此，作者提出了SACL框架，通过语义增强来减少偏见并提升代码检索和代码生成性能。", "motivation": "当前的代码检索器过度依赖表面文本特征（如文档字符串、标识符名称），并且对文档良好的代码存在强烈偏见，即使文档不相关。这影响了检索增强代码生成（RACG）的有效性，因此需要深入理解并解决这一问题。", "method": "作者首先通过系统地屏蔽特定特征同时保留代码功能，对代码检索进行了深入分析。基于分析结果，提出了SACL框架，该框架通过用语义信息增强代码或结构知识来丰富文本信息并减少偏见。", "result": "SACL显著改善了代码检索性能，例如在HumanEval、MBPP和SWE-Bench-Lite数据集上Recall@1分别提高了12.8%、9.4%和7.0%。此外，它还带来了更好的代码生成性能，例如在HumanEval上Pass@1提高了4.88%。", "conclusion": "SACL框架通过语义增强有效解决了代码检索中的文本偏见问题，显著提升了代码检索和代码生成的性能。", "translation": "检索增强代码生成（RACG）是增强代码生成的关键技术，通过检索相关信息来提升性能。在这项工作中，我们通过系统地屏蔽特定特征同时保留代码功能，对代码检索进行了深入分析。我们的发现包括：(1) 尽管在代码上训练，当前的检索器严重依赖表面级别的文本特征（例如文档字符串、标识符名称），以及 (2) 它们对文档良好的代码表现出强烈的偏见，即使文档不相关。基于我们的发现，我们提出了SACL，一个通过用语义信息增强代码或结构知识来丰富文本信息并减少偏见的框架。大量的实验表明，SACL显著改善了代码检索（例如在HumanEval / MBPP / SWE-Bench-Lite上Recall@1分别提高了12.8% / 9.4% / 7.0%），这也导致了更好的代码生成性能（例如在HumanEval上Pass@1提高了4.88%）。", "summary": "本文通过深入分析揭示了现有代码检索器对表面文本特征和良好文档代码的偏见。为解决此问题，作者提出了SACL框架，该框架通过语义增强代码或结构知识来丰富文本信息并减少偏见。实验证明，SACL显著提升了代码检索和代码生成的性能。", "keywords": "代码检索, 文本偏见, 语义增强, 检索增强代码生成, SACL", "comments": "这篇论文的创新点在于系统性地揭示了代码检索中存在的文本偏见，并提出了一个名为SACL的框架来解决这一问题。通过引入语义增强，SACL能够有效减少对表面文本特征的依赖和对文档良好代码的偏见，从而显著提升了代码检索和代码生成的性能。这项工作对于提高检索增强代码生成系统的鲁棒性和准确性具有重要意义。"}}
{"id": "2506.20041", "title": "LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification", "authors": ["Soheil Abadifard", "Fazli Can"], "summary": "The classification of imbalanced data streams, which have unequal class\ndistributions, is a key difficulty in machine learning, especially when dealing\nwith multiple classes. While binary imbalanced data stream classification tasks\nhave received considerable attention, only a few studies have focused on\nmulti-class imbalanced data streams. Effectively managing the dynamic imbalance\nratio is a key challenge in this domain. This study introduces a novel, robust,\nand resilient approach to address these challenges by integrating Locality\nSensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic\nEnsemble Diversification (DynED) framework. To the best of our knowledge, we\npresent the first application of LSH-RHP for undersampling in the context of\nimbalanced non-stationary data streams. The proposed method undersamples the\nmajority classes by utilizing LSH-RHP, provides a balanced training set, and\nimproves the ensemble's prediction performance. We conduct comprehensive\nexperiments on 23 real-world and ten semi-synthetic datasets and compare\nLSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED\noutperforms other approaches in terms of both Kappa and mG-Mean effectiveness\nmeasures, demonstrating its capability in dealing with multi-class imbalanced\nnon-stationary data streams. Notably, LSH-DynED performs well in large-scale,\nhigh-dimensional datasets with considerable class imbalances and demonstrates\nadaptation and robustness in real-world circumstances. To motivate our design,\nwe review existing methods for imbalanced data streams, outline key challenges,\nand offer guidance for future work. For the reproducibility of our results, we\nhave made our implementation available on GitHub.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20041v1", "categories": ["cs.LG", "cs.AI", "cs.IR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20041v1", "AI": {"title_translation": "LSH-DynED：一种基于LSH欠采样的动态集成框架，用于演化多类不平衡分类", "tldr": "LSH-DynED是一种新的动态集成框架，通过LSH-RHP欠采样处理演化多类不平衡数据流，并在性能上优于现有方法。", "motivation": "机器学习中处理不平衡数据流（特别是多类不平衡数据流）是一个关键难题，现有研究主要关注二元不平衡数据流，且有效管理动态不平衡比是该领域的关键挑战。", "method": "本文提出LSH-DynED方法，将局部敏感哈希与随机超平面投影（LSH-RHP）集成到动态集成多样化（DynED）框架中。该方法利用LSH-RHP对多数类进行欠采样，提供平衡的训练集，并提高集成的预测性能。据作者所知，这是首次将LSH-RHP应用于不平衡非平稳数据流的欠采样背景。", "result": "在23个真实世界和10个半合成数据集上与15种最新方法进行对比实验，结果显示LSH-DynED在Kappa和mG-Mean有效性度量方面优于其他方法。LSH-DynED在大规模、高维、具有显著类不平衡的数据集上表现良好，并在实际环境中展现出适应性和鲁棒性。", "conclusion": "LSH-DynED能够有效处理多类不平衡非平稳数据流，并在性能上超越现有方法，尤其适用于大规模、高维和类不平衡的数据集，具有良好的适应性和鲁棒性。", "translation": "具有不平等类别分布的不平衡数据流分类是机器学习中的一个关键难题，尤其是在处理多类别时。虽然二元不平衡数据流分类任务受到了相当多的关注，但很少有研究关注多类别不平衡数据流。有效管理动态不平衡比例是该领域的关键挑战。本研究引入了一种新颖、鲁棒且具有弹性的方法来应对这些挑战，通过将局部敏感哈希与随机超平面投影（LSH-RHP）集成到动态集成多样化（DynED）框架中。据我们所知，我们首次将LSH-RHP应用于不平衡非平稳数据流的欠采样背景。所提出的方法利用LSH-RHP对多数类进行欠采样，提供平衡的训练集，并提高集成的预测性能。我们对23个真实世界数据集和10个半合成数据集进行了全面实验，并将LSH-DynED与15种最先进的方法进行了比较。结果表明，LSH-DynED在Kappa和mG-Mean有效性度量方面优于其他方法，证明了其在处理多类别不平衡非平稳数据流方面的能力。值得注意的是，LSH-DynED在具有显著类别不平衡的大规模、高维数据集上表现良好，并在实际环境中展现出适应性和鲁棒性。为了阐明我们的设计动机，我们回顾了现有不平衡数据流方法，概述了主要挑战，并为未来的工作提供了指导。为了结果的可复现性，我们已将实现代码公开在GitHub上。", "summary": "本文提出LSH-DynED，一个结合LSH-RHP欠采样的动态集成框架，旨在解决演化多类不平衡数据流分类的难题。LSH-DynED通过对多数类进行欠采样来平衡训练集并提升预测性能。实验结果表明，LSH-DynED在多个数据集上均优于现有方法，尤其在处理大规模、高维和类不平衡数据时表现出卓越的适应性和鲁棒性。", "keywords": "不平衡数据流分类, 动态集成, 欠采样, 局部敏感哈希, 多类分类", "comments": "该论文的创新点在于首次将LSH-RHP应用于不平衡非平稳数据流的欠采样，并将其集成到动态集成框架中，有效解决了多类不平衡数据流分类的难题。该方法在处理动态不平衡比和大规模高维数据方面表现出强大的鲁棒性和适应性，具有重要的实际应用价值。"}}
{"id": "2506.20362", "title": "Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations", "authors": ["Lorenzo Bini", "Stephane Marchand-Maillet"], "summary": "We present LaplaceGNN, a novel self-supervised graph learning framework that\nbypasses the need for negative sampling by leveraging spectral bootstrapping\ntechniques. Our method integrates Laplacian-based signals into the learning\nprocess, allowing the model to effectively capture rich structural\nrepresentations without relying on contrastive objectives or handcrafted\naugmentations. By focusing on positive alignment, LaplaceGNN achieves linear\nscaling while offering a simpler, more efficient, self-supervised alternative\nfor graph neural networks, applicable across diverse domains. Our contributions\nare twofold: we precompute spectral augmentations through max-min\ncentrality-guided optimization, enabling rich structural supervision without\nrelying on handcrafted augmentations, then we integrate an adversarial\nbootstrapped training scheme that further strengthens feature learning and\nrobustness. Our extensive experiments on different benchmark datasets show that\nLaplaceGNN achieves superior performance compared to state-of-the-art\nself-supervised graph methods, offering a promising direction for efficiently\nlearning expressive graph representations.", "comment": "LaplaceGNN is a novel graph learning framework that employs a\n  bootstrapped teacher-student architecture. Its precomputed spectral\n  augmentations and adversarial training enable robust performance,\n  outperforming SOTA methods while scaling linearly", "pdf_url": "http://arxiv.org/pdf/2506.20362v1", "categories": ["cs.LG", "cs.AI", "cs.DS"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20362v1", "AI": {"title_translation": "通过谱自举和拉普拉斯增强的自监督图学习", "tldr": "LaplaceGNN是一种新型的自监督图学习框架，它利用谱自举和拉普拉斯增强来避免负采样，实现了线性扩展，并在多个基准数据集上超越了现有SOTA方法。", "motivation": "现有的自监督图学习方法通常需要负采样、对比目标或手动设计的增强，这些可能导致计算复杂性或依赖于繁琐的手工操作。本文旨在提供一个更简单、高效且无需负采样的自监督图学习替代方案。", "method": "LaplaceGNN通过利用谱自举技术，并集成基于拉普拉斯的信号到学习过程中，从而避免了负采样的需求。具体而言，它包含两个主要贡献：1) 通过最大-最小中心性引导优化预计算谱增强，以提供丰富的结构监督，无需手工增强。2) 整合对抗性自举训练方案，进一步增强特征学习和鲁棒性。", "result": "在不同基准数据集上进行的广泛实验表明，LaplaceGNN与最先进的自监督图方法相比，取得了卓越的性能。", "conclusion": "LaplaceGNN提供了一种有前景的方向，可以高效地学习表达性图表示，其简单、高效且无需负采样的自监督学习方式在图神经网络领域具有广泛适用性。", "translation": "我们提出了LaplaceGNN，这是一种新颖的自监督图学习框架，它利用谱自举技术绕过了负采样的需要。我们的方法将基于拉普拉斯的信号整合到学习过程中，使模型能够有效地捕获丰富的结构表示，而无需依赖对比目标或手工设计的增强。通过专注于正对齐，LaplaceGNN实现了线性扩展，同时为图神经网络提供了一种更简单、更高效的自监督替代方案，适用于不同领域。我们的贡献是双重的：我们通过最大-最小中心性引导优化预计算谱增强，从而在不依赖手工增强的情况下实现丰富的结构监督；然后，我们整合了一种对抗性自举训练方案，进一步加强特征学习和鲁棒性。我们在不同基准数据集上进行的广泛实验表明，LaplaceGNN与最先进的自监督图方法相比，取得了卓越的性能，为高效学习表达性图表示提供了一个有前景的方向。", "summary": "LaplaceGNN是一个创新的自监督图学习框架，它通过利用谱自举和拉普拉斯信号来避免负采样，从而简化了学习过程并提高了效率。该方法通过预计算谱增强和引入对抗性自举训练来捕获丰富的结构表示和增强鲁棒性。实验证明，LaplaceGNN在各种基准数据集上均优于现有SOTA自监督图学习方法，为高效图表示学习提供了一条新途径。", "keywords": "自监督学习, 图神经网络, 谱自举, 拉普拉斯增强, 图表示学习", "comments": "LaplaceGNN的创新之处在于其通过谱自举和拉普拉斯增强来完全规避负采样的需求，这显著简化了自监督图学习的复杂性。其线性扩展能力和优越的性能表明其在实际应用中的潜力，特别是对于大规模图数据。该方法提供了一种更简洁、高效的替代方案，对于图神经网络研究具有重要意义。"}}
{"id": "2506.20201", "title": "Stochastic particle method with birth-death dynamics", "authors": ["Zhengyang Lei", "Sihong Shao"], "summary": "In order to numerically solve high-dimensional nonlinear PDEs and alleviate\nthe curse of dimensionality, a stochastic particle method (SPM) has been\nproposed to capture the relevant feature of the solution through the adaptive\nevolution of particles [J. Comput. Phys. 527 (2025) 113818]. In this paper, we\nintroduce an active birth-death dynamics of particles to improve the efficiency\nof SPM. The resulting method, dubbed SPM-birth-death, sample new particles\naccording to the nonlinear term and execute the annihilation strategy when the\nnumber of particles exceeds a given threshold. Preliminary numerical\nexperiments on the Allen-Cahn equation demonstrate that SPM-birth-death can\nachieve smaller errors at the same computational cost compared with the\noriginal SPM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20201v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20201v1", "AI": {"title_translation": "具有生灭动力学的随机粒子方法", "tldr": "本文提出了一种名为SPM-birth-death的随机粒子方法，通过引入生灭动力学来提高原始SPM求解高维非线性偏微分方程的效率，并在艾伦-卡恩方程上显示出更好的性能。", "motivation": "为了提高现有随机粒子方法（SPM）在数值求解高维非线性偏微分方程方面的效率，并缓解维度灾难。", "method": "本文引入了一种主动的粒子生灭动力学，形成了SPM-birth-death方法。该方法根据非线性项采样新粒子，并在粒子数量超过给定阈值时执行湮灭策略。", "result": "在艾伦-卡恩方程上的初步数值实验表明，与原始SPM相比，SPM-birth-death方法在相同的计算成本下可以实现更小的误差。", "conclusion": "SPM-birth-death方法通过引入生灭动力学，有效提高了随机粒子方法的效率，并在数值求解高维非线性偏微分方程方面表现出优越性。", "translation": "为了数值求解高维非线性偏微分方程并缓解维度灾难，一种随机粒子方法（SPM）已被提出，通过粒子的自适应演化来捕捉解的相关特征[J. Comput. Phys. 527 (2025) 113818]。在本文中，我们引入了一种主动的粒子生灭动力学来提高SPM的效率。由此产生的方法，被称为SPM-birth-death，根据非线性项采样新粒子，并在粒子数量超过给定阈值时执行湮灭策略。在艾伦-卡恩方程上的初步数值实验表明，与原始SPM相比，SPM-birth-death在相同的计算成本下可以实现更小的误差。", "summary": "本文提出了一种名为SPM-birth-death的新型随机粒子方法，旨在通过引入主动生灭动力学来提高现有随机粒子方法（SPM）求解高维非线性偏微分方程的效率。该方法根据非线性项生成新粒子，并通过湮灭策略控制粒子数量。初步数值实验表明，SPM-birth-death在相同计算成本下比原始SPM能获得更小的误差。", "keywords": "随机粒子方法, 生灭动力学, 高维偏微分方程, 艾伦-卡恩方程, 数值求解", "comments": "这项工作通过引入粒子生灭动力学，对随机粒子方法进行了创新性改进，有效解决了高维偏微分方程数值求解中的效率问题。其核心创新在于动态调整粒子数量以优化计算资源，具有重要的实际应用潜力。"}}
{"id": "2506.20314", "title": "Near Time-Optimal Hybrid Motion Planning for Timber Cranes", "authors": ["Marc-Philip Ecker", "Bernhard Bischof", "Minh Nhat Vu", "Christoph Fröhlich", "Tobias Glück", "Wolfgang Kemmetmüller"], "summary": "Efficient, collision-free motion planning is essential for automating\nlarge-scale manipulators like timber cranes. They come with unique challenges\nsuch as hydraulic actuation constraints and passive joints-factors that are\nseldom addressed by current motion planning methods. This paper introduces a\nnovel approach for time-optimal, collision-free hybrid motion planning for a\nhydraulically actuated timber crane with passive joints. We enhance the\nvia-point-based stochastic trajectory optimization (VP-STO) algorithm to\ninclude pump flow rate constraints and develop a novel collision cost\nformulation to improve robustness. The effectiveness of the enhanced VP-STO as\nan optimal single-query global planner is validated by comparison with an\ninformed RRT* algorithm using a time-optimal path parameterization (TOPP). The\noverall hybrid motion planning is formed by combination with a gradient-based\nlocal planner that is designed to follow the global planner's reference and to\nsystematically consider the passive joint dynamics for both collision avoidance\nand sway damping.", "comment": "Accepted at ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.20314v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20314v1", "AI": {"title_translation": "伐木起重机近乎时间最优的混合运动规划", "tldr": "提出一种针对液压驱动、带被动关节的伐木起重机的近时间最优混合运动规划方法，通过改进VP-STO算法和结合梯度局部规划器，实现高效无碰撞的运动。", "motivation": "自动化大型机械手（如伐木起重机）需要高效、无碰撞的运动规划，但现有方法很少解决伐木起重机特有的液压驱动约束和被动关节问题。", "method": "本文提出一种新的时间最优、无碰撞混合运动规划方法，用于液压驱动、带被动关节的伐木起重机。该方法增强了基于路径点的随机轨迹优化（VP-STO）算法，加入了泵流量约束并开发了新的碰撞成本公式以提高鲁棒性。整体混合运动规划结合了一个基于梯度的局部规划器，该规划器旨在遵循全局规划器的参考，并系统地考虑被动关节动力学以实现避碰和抑制摆动。", "result": "增强的VP-STO算法作为最优的单次查询全局规划器的有效性，通过与使用时间最优路径参数化（TOPP）的RRT*算法进行比较得到了验证。", "conclusion": "增强的VP-STO算法作为最优单次查询全局规划器是有效的，并且结合梯度局部规划器的混合运动规划方法能够有效处理伐木起重机的独特挑战，实现无碰撞和摆动抑制。", "translation": "高效、无碰撞的运动规划对于自动化大型机械手（如伐木起重机）至关重要。它们带来独特的挑战，例如液压驱动约束和被动关节——这些因素在当前的运动规划方法中很少得到解决。本文介绍了一种用于液压驱动、带被动关节的伐木起重机的近时间最优、无碰撞混合运动规划的新方法。我们增强了基于路径点的随机轨迹优化（VP-STO）算法，以包括泵流量约束，并开发了一种新颖的碰撞成本公式以提高鲁棒性。增强的VP-STO作为最优的单次查询全局规划器的有效性通过与使用时间最优路径参数化（TOPP）的RRT*算法的比较得到了验证。整体混合运动规划通过与基于梯度的局部规划器结合形成，该规划器旨在遵循全局规划器的参考，并系统地考虑被动关节动力学以实现避碰和抑制摆动。", "summary": "本文针对伐木起重机等大型机械手面临的液压驱动约束和被动关节等独特挑战，提出了一种新颖的近时间最优、无碰撞混合运动规划方法。该方法通过增强VP-STO算法，纳入泵流量约束和新的碰撞成本公式，并结合基于梯度的局部规划器，实现全局规划和局部避碰及摆动抑制。实验验证了增强VP-STO的有效性。", "keywords": "伐木起重机, 运动规划, 混合规划, VP-STO, 被动关节", "comments": "本文的创新之处在于提出了一种针对具有独特挑战（液压驱动和被动关节）的伐木起重机的混合运动规划方法。它通过改进现有的VP-STO算法来解决特定约束，并结合了全局和局部规划器，系统地考虑了被动关节的动力学，这对于实际应用中的大型机械手非常重要。"}}
{"id": "2506.20495", "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning", "authors": ["Haoze Wu", "Yunzhi Yao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.20495v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SE"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20495v1", "AI": {"title_translation": "ReCode：使用强化学习更新代码API知识", "tldr": "ReCode是一个新颖的框架，它利用强化学习来帮助大型语言模型适应频繁变化的外部库API，显著提升了其在动态API场景下的代码生成性能。", "motivation": "大型语言模型（LLMs）在代码生成方面表现出色，但在适应外部库API的频繁更新时会遇到困难。这种局限性源于它们依赖训练数据中过时的API知识，即使能访问最新文档也无法解决，这阻碍了在动态环境中可靠的代码生成。", "method": "我们提出了ReCode框架，它模仿人类程序员适应API变化的方式。具体来说，我们构建了一个包含约2000个数据条目的数据集，用于训练LLM根据更新信息执行版本迁移。然后，我们引入了一种修改后的字符串相似度度量作为强化学习的奖励，用于代码评估。该方法应用于各种LLM和强化学习算法（GRPO和DAPO）。", "result": "ReCode显著提升了LLM在动态API场景下的代码生成性能，尤其是在未曾见过的CodeUpdateArena任务上。与监督微调相比，ReCode对LLM通用代码生成能力的影响较小。将ReCode应用于各种LLM和强化学习算法（GRPO和DAPO）都取得了持续的改进。值得注意的是，训练后，Qwen2.5-Coder-7B的表现优于参数量为32B的代码指令微调模型和相同架构的推理模型。", "conclusion": "ReCode框架通过强化学习有效解决了大型语言模型在动态API环境下代码生成中API知识过时的问题，显著提升了模型在特定场景下的性能，同时保持了其通用代码生成能力。", "translation": "大型语言模型（LLM）展现出卓越的代码生成能力，但在适应外部库API的频繁更新时却会受挫。这一关键限制源于它们依赖其训练数据中过时的API知识，即使能够访问当前文档，也阻碍了在动态环境中可靠的代码生成。为了解决这个问题，我们提出了ReCode（基于规则的强化学习用于代码更新），一个模仿人类程序员适应API变化的新颖框架。具体来说，我们构建了一个大约2000个数据条目的数据集，用于训练LLM根据更新信息执行版本迁移。然后，我们引入了一种修改后的字符串相似度度量作为强化学习的奖励，用于代码评估。我们的实验表明，ReCode显著提升了LLM在动态API场景下的代码生成性能，尤其是在未曾见过的CodeUpdateArena任务上。至关重要的是，与监督微调相比，ReCode对LLM通用代码生成能力的影响较小。我们将ReCode应用于各种LLM和强化学习算法（GRPO和DAPO），所有都实现了持续的改进。值得注意的是，在训练之后，Qwen2.5-Coder-7B的表现优于参数量为32B的代码指令微调模型和相同架构的推理模型。代码可在https://github.com/zjunlp/ReCode获取。", "summary": "本文提出了ReCode，一个基于规则的强化学习框架，旨在解决大型语言模型在适应频繁更新的外部库API时遇到的困难。ReCode通过构建一个包含约2000个数据条目的数据集来训练LLM执行版本迁移，并引入修改后的字符串相似度作为强化学习的奖励。实验结果表明，ReCode显著提升了LLM在动态API场景下的代码生成性能，对通用代码生成能力影响较小，并在多种LLM和RL算法上实现了持续改进，其中Qwen2.5-Coder-7B在训练后表现优异。", "keywords": "大型语言模型, API更新, 强化学习, 代码生成, ReCode", "comments": "ReCode的创新之处在于其模仿人类程序员适应API变化的方式，并引入强化学习来更新LLM的API知识。通过构建特定数据集和设计奖励机制，它有效解决了LLM在动态环境中的关键局限性。其重要性在于提升了LLM在实际软件开发中处理API更新的实用性，且相比监督微调对通用能力影响较小，这使得其更具应用价值。"}}
{"id": "2506.20179", "title": "Progressive Alignment Degradation Learning for Pansharpening", "authors": ["Enzhe Zhao", "Zhichang Guo", "Yao Li", "Fanghui Song", "Boying Wu"], "summary": "Deep learning-based pansharpening has been shown to effectively generate\nhigh-resolution multispectral (HRMS) images. To create supervised ground-truth\nHRMS images, synthetic data generated using the Wald protocol is commonly\nemployed. This protocol assumes that networks trained on artificial\nlow-resolution data will perform equally well on high-resolution data. However,\nwell-trained models typically exhibit a trade-off in performance between\nreduced-resolution and full-resolution datasets. In this paper, we delve into\nthe Wald protocol and find that its inaccurate approximation of real-world\ndegradation patterns limits the generalization of deep pansharpening models. To\naddress this issue, we propose the Progressive Alignment Degradation Module\n(PADM), which uses mutual iteration between two sub-networks, PAlignNet and\nPDegradeNet, to adaptively learn accurate degradation processes without relying\non predefined operators. Building on this, we introduce HFreqdiff, which embeds\nhigh-frequency details into a diffusion framework and incorporates CFB and BACM\nmodules for frequency-selective detail extraction and precise reverse process\nlearning. These innovations enable effective integration of high-resolution\npanchromatic and multispectral images, significantly enhancing spatial\nsharpness and quality. Experiments and ablation studies demonstrate the\nproposed method's superior performance compared to state-of-the-art techniques.", "comment": "13 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.20179v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20179v1", "AI": {"title_translation": "渐进式对齐退化学习用于全色锐化", "tldr": "本文提出了一种名为渐进式对齐退化模块（PADM）的新方法，通过自适应学习退化过程，解决了现有全色锐化方法中Wald协议对真实世界退化模式近似不准确的问题，显著提升了高分辨率多光谱图像的生成质量。", "motivation": "现有的基于深度学习的全色锐化方法在合成数据生成时常采用Wald协议，该协议假设网络在低分辨率数据上的表现与高分辨率数据相同，但实际上模型在不同分辨率数据集之间存在性能权衡。研究发现Wald协议对真实世界退化模式的近似不准确，限制了深度全色锐化模型的泛化能力。", "method": "本文提出了渐进式对齐退化模块（PADM），通过PAlignNet和PDegradeNet两个子网络之间的相互迭代，自适应学习准确的退化过程，无需预定义操作。在此基础上，引入HFreqdiff，将高频细节嵌入扩散框架，并结合CFB和BACM模块进行频率选择性细节提取和精确逆向过程学习。", "result": "实验和消融研究表明，所提出的方法与最先进的技术相比，性能更优越，能够显著增强空间清晰度和图像质量。", "conclusion": "本文提出的渐进式对齐退化学习方法通过解决Wald协议的局限性，自适应学习准确的退化过程，并结合扩散框架和频率选择性模块，有效提升了全色锐化图像的空间清晰度和质量。", "translation": "基于深度学习的全色锐化已被证明能有效生成高分辨率多光谱（HRMS）图像。为了创建监督学习所需的地面真实HRMS图像，通常采用Wald协议生成的合成数据。该协议假设在人工低分辨率数据上训练的网络在高分辨率数据上也能表现同样出色。然而，训练有素的模型通常在降低分辨率和全分辨率数据集之间的性能表现出权衡。在本文中，我们深入研究了Wald协议，发现其对真实世界退化模式的不准确近似限制了深度全色锐化模型的泛化能力。为了解决这个问题，我们提出了渐进式对齐退化模块（PADM），它使用PAlignNet和PDegradeNet两个子网络之间的相互迭代，自适应地学习准确的退化过程，而无需依赖预定义的操作。在此基础上，我们引入了HFreqdiff，它将高频细节嵌入扩散框架，并结合了CFB和BACM模块，用于频率选择性细节提取和精确的逆向过程学习。这些创新使得高分辨率全色图像和多光谱图像能够有效融合，显著增强了空间清晰度和质量。实验和消融研究表明，所提出的方法与最先进的技术相比，性能更优越。", "summary": "本文针对深度学习全色锐化中Wald协议对真实世界退化模式近似不准确导致模型泛化能力受限的问题，提出了一种渐进式对齐退化模块（PADM）。PADM通过PAlignNet和PDegradeNet的相互迭代，自适应学习精确的退化过程。此外，引入HFreqdiff将高频细节融入扩散框架，并利用CFB和BACM模块进行频率选择性细节提取和精确逆向学习，从而显著提升了高分辨率多光谱图像的空间清晰度和质量。实验证明了其优于现有技术。", "keywords": "全色锐化, 深度学习, 退化学习, Wald协议, 扩散模型", "comments": "本文创新性地指出了传统Wald协议在全色锐化中对真实世界退化模式近似不准确的局限性，并通过提出的PADM模块实现了自适应的退化学习，摆脱了对预定义操作的依赖。结合扩散框架和频率选择性模块，提高了细节提取和融合的精度，为全色锐化领域提供了一个有前景的新方向，有望提升模型在实际应用中的泛化能力。"}}
{"id": "2506.19937", "title": "The Most Important Features in Generalized Additive Models Might Be Groups of Features", "authors": ["Tomas M. Bosschieter", "Luis Franca", "Jessica Wolk", "Yiyuan Wu", "Bella Mehta", "Joseph Dehoney", "Orsolya Kiss", "Fiona C. Baker", "Qingyu Zhao", "Rich Caruana", "Kilian M. Pohl"], "summary": "While analyzing the importance of features has become ubiquitous in\ninterpretable machine learning, the joint signal from a group of related\nfeatures is sometimes overlooked or inadvertently excluded. Neglecting the\njoint signal could bypass a critical insight: in many instances, the most\nsignificant predictors are not isolated features, but rather the combined\neffect of groups of features. This can be especially problematic for datasets\nthat contain natural groupings of features, including multimodal datasets. This\npaper introduces a novel approach to determine the importance of a group of\nfeatures for Generalized Additive Models (GAMs) that is efficient, requires no\nmodel retraining, allows defining groups posthoc, permits overlapping groups,\nand remains meaningful in high-dimensional settings. Moreover, this definition\noffers a parallel with explained variation in statistics. We showcase\nproperties of our method on three synthetic experiments that illustrate the\nbehavior of group importance across various data regimes. We then demonstrate\nthe importance of groups of features in identifying depressive symptoms from a\nmultimodal neuroscience dataset, and study the importance of social\ndeterminants of health after total hip arthroplasty. These two case studies\nreveal that analyzing group importance offers a more accurate, holistic view of\nthe medical issues compared to a single-feature analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19937v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19937v1", "AI": {"title_translation": "广义加性模型中最重要的特征可能是特征组", "tldr": "本文提出了一种新颖的方法，用于高效地确定广义加性模型（GAMs）中特征组的重要性，该方法无需模型再训练，并能提供比单一特征分析更准确、更全面的视图。", "motivation": "在可解释机器学习中，特征的重要性分析已无处不在，但相关特征组的联合信号有时会被忽视或无意中排除。忽视联合信号可能会错过关键的洞察：在许多情况下，最重要的预测因子并非孤立的特征，而是特征组的组合效应。这对于包含自然特征分组的数据集（包括多模态数据集）尤其成问题。", "method": "本文引入了一种新颖的方法来确定广义加性模型（GAMs）中特征组的重要性，该方法高效、无需模型再训练、允许事后定义组、允许组重叠，并在高维设置中仍然有意义。此外，该定义在统计学上与解释变异性具有相似之处。", "result": "该方法在三个合成实验中展示了其特性，这些实验说明了不同数据条件下组重要性的行为。随后，论文展示了特征组在从多模态神经科学数据集中识别抑郁症状方面的重要性，并研究了全髋关节置换术后社会健康决定因素的重要性。", "conclusion": "两个案例研究表明，与单一特征分析相比，分析组重要性能够提供对医疗问题更准确、更全面的视图。", "translation": "在可解释机器学习中，特征重要性分析已变得无处不在，但相关特征组的联合信号有时会被忽视或无意中排除。忽视联合信号可能会错过一个关键的洞察：在许多情况下，最重要的预测因子并非孤立的特征，而是特征组的组合效应。这对于包含自然特征分组的数据集（包括多模态数据集）尤其成问题。本文引入了一种新颖的方法来确定广义加性模型（GAMs）中特征组的重要性，该方法高效、无需模型再训练、允许事后定义组、允许组重叠，并在高维设置中仍然有意义。此外，该定义在统计学上与解释变异性具有相似之处。我们在三个合成实验中展示了我们方法的特性，这些实验说明了组重要性在各种数据状态下的行为。然后，我们展示了特征组在从多模态神经科学数据集中识别抑郁症状方面的重要性，并研究了全髋关节置换术后社会健康决定因素的重要性。这两个案例研究表明，与单一特征分析相比，分析组重要性能够提供对医疗问题更准确、更全面的视图。", "summary": "本文提出了一种新颖且高效的方法，用于在广义加性模型（GAMs）中评估特征组的重要性。该方法无需模型再训练，支持事后定义和重叠组，并在高维数据中保持有效性。研究指出，在许多情况下，特征组的联合效应比单一特征更能揭示关键预测信息。通过合成实验和两个医学案例研究（识别抑郁症状和分析全髋关节置换术后的社会健康决定因素），论文证明了分析特征组重要性能够提供比传统单一特征分析更准确、更全面的洞察。", "keywords": "广义加性模型, 特征重要性, 特征组, 可解释机器学习, 多模态数据", "comments": "本文的创新之处在于提出了一种针对广义加性模型（GAMs）的特征组重要性评估方法，其亮点在于无需模型再训练、支持事后定义和重叠组，并能有效处理高维数据。这解决了现有方法中可能忽视特征组联合信号的问题，提供了更全面的模型解释。其重要性在于，在许多实际应用中，尤其是涉及多模态数据或复杂交互的场景，孤立地分析单一特征可能无法揭示真正的驱动因素。通过强调特征组的重要性，该研究为可解释机器学习领域提供了一个更精细、更贴近实际的工具，尤其是在医疗诊断等需要全面视图的领域。"}}
{"id": "2506.20332", "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards", "authors": ["Jihao Gu", "Qihang Ai", "Yingyao Wang", "Pi Bu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Ziming Wang", "Yingxiu Zhao", "Ming-Liang Zhang", "Jun Song", "Yuning Jiang", "Bo Zheng"], "summary": "Vision-language model-based mobile agents have gained the ability to not only\nunderstand complex instructions and mobile screenshots, but also optimize their\naction outputs via thinking and reasoning, benefiting from reinforcement\nlearning, such as Group Relative Policy Optimization (GRPO). However, existing\nresearch centers on offline reinforcement learning training or online\noptimization using action-level rewards, which limits the agent's dynamic\ninteraction with the environment. This often results in agents settling into\nlocal optima, thereby weakening their ability for exploration and error action\ncorrection. To address these challenges, we introduce an approach called\nMobile-R1, which employs interactive multi-turn reinforcement learning with\ntask-level rewards for mobile agents. Our training framework consists of three\nstages: initial format finetuning, single-step online training via action-level\nreward, followed by online training via task-level reward based on multi-turn\ntrajectories. This strategy is designed to enhance the exploration and error\ncorrection capabilities of Mobile-R1, leading to significant performance\nimprovements. Moreover, we have collected a dataset covering 28 Chinese\napplications with 24,521 high-quality manual annotations and established a new\nbenchmark with 500 trajectories. We will open source all resources, including\nthe dataset, benchmark, model weight, and codes:\nhttps://mobile-r1.github.io/Mobile-R1/.", "comment": "14 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20332v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20332v1", "AI": {"title_translation": "Mobile-R1：面向基于VLM的移动智能体的交互式强化学习，通过任务级奖励实现", "tldr": "Mobile-R1提出了一种通过交互式多轮强化学习和任务级奖励来提升基于视觉语言模型的移动智能体探索和纠错能力的方法，并发布了新的数据集和基准。", "motivation": "现有基于视觉语言模型的移动智能体研究主要集中于离线强化学习训练或使用动作级奖励的在线优化，这限制了智能体与环境的动态交互，导致其容易陷入局部最优，削弱了探索和错误动作纠正能力。", "method": "本文提出Mobile-R1方法，采用交互式多轮强化学习和任务级奖励来解决现有挑战。其训练框架包含三个阶段：初始格式微调、通过动作级奖励的单步在线训练，以及基于多轮轨迹通过任务级奖励的在线训练。此外，研究团队收集了一个包含28个中文应用、24,521个高质量手动标注的数据集，并建立了一个包含500条轨迹的新基准。", "result": "该策略旨在增强Mobile-R1的探索和错误纠正能力，并带来了显著的性能提升。同时，研究团队收集了一个覆盖28个中文应用、包含24,521个高质量手动标注的数据集，并建立了一个包含500条轨迹的新基准。", "conclusion": "Mobile-R1通过引入交互式多轮强化学习和任务级奖励，有效解决了现有基于VLM的移动智能体在动态交互和探索纠错方面的局限性，并实现了显著的性能提升。此外，该工作还通过发布数据集和基准，为该领域的研究提供了宝贵资源。", "translation": "基于视觉语言模型的移动智能体不仅能够理解复杂的指令和移动截图，还能通过思考和推理来优化其动作输出，这得益于强化学习，例如群组相对策略优化（GRPO）。然而，现有研究主要集中于离线强化学习训练或使用动作级奖励的在线优化，这限制了智能体与环境的动态交互。这通常导致智能体陷入局部最优，从而削弱了它们的探索和错误动作纠正能力。为了解决这些挑战，我们引入了一种名为Mobile-R1的方法，该方法采用交互式多轮强化学习，并使用任务级奖励来训练移动智能体。我们的训练框架包括三个阶段：初始格式微调、通过动作级奖励的单步在线训练，随后是基于多轮轨迹通过任务级奖励的在线训练。该策略旨在增强Mobile-R1的探索和错误纠正能力，从而显著提高性能。此外，我们收集了一个覆盖28个中文应用、包含24,521个高质量手动标注的数据集，并建立了一个包含500条轨迹的新基准。我们将开源所有资源，包括数据集、基准、模型权重和代码：https://mobile-r1.github.io/Mobile-R1/。", "summary": "本文针对现有基于VLM的移动智能体在动态交互和探索纠错方面的局限性，提出了Mobile-R1方法。该方法采用交互式多轮强化学习，并引入任务级奖励进行训练，其训练流程分为初始格式微调、动作级奖励单步在线训练和任务级奖励多轮在线训练三个阶段。研究表明，此策略显著提升了智能体的探索和纠错能力。此外，作者还构建了一个包含28个中文应用、24,521条高质量标注的新数据集，并设立了500条轨迹的基准，并将全部资源开源。", "keywords": "移动智能体, 强化学习, 视觉语言模型, 任务级奖励, 交互式学习", "comments": "本文的创新点在于引入了交互式多轮强化学习和任务级奖励，以解决现有基于VLM的移动智能体在动态交互和探索能力上的不足。通过结合动作级和任务级奖励的训练框架，有望提升智能体的泛化能力和鲁棒性。同时，发布高质量的中文应用数据集和新的基准对于推动该领域的研究具有重要意义，尤其是在中文移动应用场景下的应用潜力巨大。"}}
{"id": "2506.20628", "title": "Identifiability and Maximum Likelihood Estimation for System Identification of Networks of Dynamical Systems", "authors": ["Anders Hansson", "João Victor Galvão da Mata", "Martin S. Andersen"], "summary": "In this paper we investigate identifiability and maximum likelihood\nestimation for direct system identification of networks of dynamical systems.\nWe provide necessary and sufficient conditions for network identifiability in\nterms of Gr\\\"obner bases. We show that the maximum likelihood approach is both\nconsistent and efficient, which is in contrast to existing prediction error\napproaches. Moreover, our approach has wider applicability, i.e., it is\napplicable whenever network identifiability holds. Finally, we show that we can\nformulate the maximum likelihood problem without the use of a predictor, which\nis the key to numerically being able to solve it efficiently.", "comment": "This work has been submitted to the IEEE for possible publication.\n  Submitted to IEEE Transactions on Automatic Control", "pdf_url": "http://arxiv.org/pdf/2506.20628v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20628v1", "AI": {"title_translation": "动态系统网络系统辨识的可辨识性与最大似然估计", "tldr": "本文研究了动态系统网络系统辨识中的可辨识性与最大似然估计，提供了可辨识性条件，并证明了最大似然估计的一致性、有效性及数值求解效率。", "motivation": "本文旨在研究动态系统网络直接系统辨识中的可辨识性与最大似然估计，以克服现有预测误差方法的局限性。", "method": "本文利用Gröbner基提供了网络可辨识性的充分必要条件，并提出了一种无需预测器即可进行最大似然问题表述的方法，从而实现高效的数值求解。", "result": "研究表明，所提出的最大似然方法既一致又有效，优于现有预测误差方法，并且只要网络可辨识性成立，该方法就具有更广泛的适用性。此外，该方法无需预测器即可有效进行数值求解。", "conclusion": "本文提出的最大似然估计方法是一种针对动态系统网络系统辨识的，具有一致性、有效性和广泛适用性的方法，且在数值求解上具有显著优势。", "translation": "在本文中，我们研究了动态系统网络直接系统辨识的可辨识性与最大似然估计。我们以Gröbner基的形式提供了网络可辨识性的充分必要条件。我们表明，最大似然方法既一致又有效，这与现有的预测误差方法形成对比。此外，我们的方法具有更广泛的适用性，即只要网络可辨识性成立，它就适用。最后，我们表明我们可以不使用预测器来 формулировать 最大似然问题，这是能够有效地数值求解它的关键。", "summary": "本文探讨了动态系统网络直接系统辨识中的可辨识性与最大似然估计。论文利用Gröbner基给出了网络可辨识性的充分必要条件，并证明了最大似然方法的一致性、有效性及其比现有预测误差方法更广泛的适用性。一个关键的创新在于，该方法能够在不使用预测器的情况下构建最大似然问题，从而实现高效的数值求解。", "keywords": "可辨识性, 最大似然估计, 系统辨识, 动态系统, 网络", "comments": "该论文的创新之处在于证明了最大似然估计在网络系统辨识中的一致性和效率，这与传统的预测误差方法形成了对比。能够在没有预测器的情况下构建最大似然问题是一个重要的实际进展，使得该方法在数值上更具可行性且适用范围更广。"}}
{"id": "2506.20231", "title": "Sensing-Aware Transmit Waveform/Receive Filter Design for OFDM-MBS Systems", "authors": ["Xinghe Li", "Kainan Cheng", "Huiyong Li", "Huiyong Li", "Ziyang Cheng"], "summary": "In this letter, we study the problem of cooperative sensing design for an\northogonal frequency division multiplexing (OFDM) multiple base stations (MBS)\nsystem. We consider a practical scenario where the base stations (BSs) exploit\ncertain subcarriers to realize a sensing function. Since the high sidelobe\nlevel (SLL) of OFDM waveforms degrades radar detection for weak targets, and\nthe cross-correlation generated by other BSs further exacerbates detection\nperformance, we devise a joint design scheme for OFDM sequence and receive\nfilter by minimizing the integrated sidelobe level (ISL) while satisfying\nmainlobe level, peak-to-average power ratio (PAPR) and spectrum allocation\nconstraints. To address this non-convex problem, we propose an alternating\noptimization (AO)-based algorithm. Numerical simulations validate the\neffectiveness of the proposed method, demonstrating the superiority of SSL\nreduction in the MBS system over the matched filtering method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20231v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20231v1", "AI": {"title_translation": "面向OFDM-MBS系统的感知感知发射波形/接收滤波器设计", "tldr": "本文提出了一种针对OFDM多基站系统协同感知的设计方案，通过联合设计OFDM序列和接收滤波器来降低旁瓣电平，以改善弱目标雷达探测性能，并提出了一种基于交替优化的算法。", "motivation": "OFDM波形的高旁瓣电平会降低对弱目标的雷达探测能力，且其他基站产生的互相关会进一步恶化检测性能。因此，需要设计一种方案来解决这个问题。", "method": "提出了一种OFDM序列和接收滤波器的联合设计方案，通过最小化综合旁瓣电平（ISL），同时满足主瓣电平、峰均功率比（PAPR）和频谱分配约束。为了解决这个非凸问题，提出了一种基于交替优化（AO）的算法。", "result": "数值仿真验证了所提方法的有效性，并表明在多基站（MBS）系统中，该方法在降低旁瓣电平（SLL）方面优于匹配滤波方法。", "conclusion": "所提出的OFDM序列和接收滤波器联合设计方案及其基于交替优化的算法，能有效降低OFDM-MBS系统中的旁瓣电平，显著提升对弱目标的雷达探测性能，且优于传统的匹配滤波方法。", "translation": "在这封信中，我们研究了正交频分复用（OFDM）多基站（MBS）系统的协同感知设计问题。我们考虑了一种实际场景，其中基站（BS）利用某些子载波实现感知功能。由于OFDM波形的高旁瓣电平（SLL）会降低对弱目标的雷达探测能力，并且其他基站产生的互相关会进一步恶化检测性能，我们设计了一种OFDM序列和接收滤波器的联合设计方案，通过最小化综合旁瓣电平（ISL），同时满足主瓣电平、峰均功率比（PAPR）和频谱分配约束。为了解决这个非凸问题，我们提出了一种基于交替优化（AO）的算法。数值仿真验证了所提方法的有效性，证明了在MBS系统中，旁瓣电平（SLL）降低方面优于匹配滤波方法。", "summary": "本文提出了一种针对OFDM多基站（MBS）系统的协同感知设计方法。该方法通过联合设计OFDM序列和接收滤波器，旨在最小化综合旁瓣电平（ISL），以克服OFDM波形高旁瓣和多基站互相关对弱目标雷达探测性能的劣化影响。为解决此非凸优化问题，文中提出了一个基于交替优化（AO）的算法。仿真结果验证了该方法的有效性及其在降低旁瓣电平方面优于传统匹配滤波方法的优势。", "keywords": "OFDM-MBS, 感知, 旁瓣电平, 波形设计, 交替优化", "comments": "本文的创新点在于提出了OFDM序列和接收滤波器的联合设计方案，并利用交替优化算法解决了非凸优化问题，有效地降低了OFDM-MBS系统中的旁瓣电平。这对于提升未来无线通信系统中集成感知功能的雷达探测性能具有重要意义，尤其是在复杂多基站环境下对弱目标的探测。"}}
{"id": "2506.20407", "title": "Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images", "authors": ["Fangyijie Wang", "Yuan Liang", "Sourav Bhattacharjee", "Abey Campbell", "Kathleen M. Curran", "Guénolé Silvestre"], "summary": "Accurate gestational age (GA) estimation, ideally through fetal ultrasound\nmeasurement, is a crucial aspect of providing excellent antenatal care.\nHowever, deriving GA from manual fetal biometric measurements depends on the\noperator and is time-consuming. Hence, automatic computer-assisted methods are\ndemanded in clinical practice. In this paper, we present a novel feature fusion\nframework to estimate GA using fetal ultrasound images without any measurement\ninformation. We adopt a deep learning model to extract deep representations\nfrom ultrasound images. We extract radiomic features to reveal patterns and\ncharacteristics of fetal brain growth. To harness the interpretability of\nradiomics in medical imaging analysis, we estimate GA by fusing radiomic\nfeatures and deep representations. Our framework estimates GA with a mean\nabsolute error of 8.0 days across three trimesters, outperforming current\nmachine learning-based methods at these gestational ages. Experimental results\ndemonstrate the robustness of our framework across different populations in\ndiverse geographical regions. Our code is publicly available on\n\\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20407v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20407v1", "AI": {"title_translation": "融合影像组学特征与深度表示用于胎儿超声图像中的胎龄估计", "tldr": "本文提出一种新颖的特征融合框架，通过胎儿超声图像自动估计胎龄，无需手动测量，且性能优于现有方法。", "motivation": "准确的胎龄估计对产前护理至关重要，但手动测量耗时且依赖操作员，因此需要自动化的计算机辅助方法来提高效率和准确性。", "method": "提出一种新的特征融合框架，结合深度学习模型提取的超声图像深度表示和影像组学特征。影像组学特征用于揭示胎儿大脑生长模式。通过融合这两种特征来估计胎龄。", "result": "该框架在三个孕期的胎龄估计平均绝对误差为8.0天，优于当前基于机器学习的方法。实验结果表明，该框架在不同地理区域的不同人群中都具有鲁棒性。", "conclusion": "本文提出的特征融合框架能够通过胎儿超声图像自动准确地估计胎龄，且具有良好的鲁棒性，为临床实践提供了有效的计算机辅助方法。", "translation": "准确的胎龄（GA）估计，理想情况下通过胎儿超声测量，是提供优质产前护理的关键方面。然而，通过手动胎儿生物测量来推导胎龄依赖于操作员且耗时。因此，临床实践中需要自动化的计算机辅助方法。在本文中，我们提出了一种新颖的特征融合框架，用于在没有任何测量信息的情况下，使用胎儿超声图像估计胎龄。我们采用深度学习模型从超声图像中提取深度表示。我们提取影像组学特征以揭示胎儿大脑生长的模式和特征。为了利用影像组学在医学图像分析中的可解释性，我们通过融合影像组学特征和深度表示来估计胎龄。我们的框架在三个孕期的胎龄估计平均绝对误差为8.0天，优于当前基于机器学习的方法。实验结果表明，我们的框架在不同地理区域的不同人群中都具有鲁棒性。我们的代码已在GitHub上公开。", "summary": "本文提出了一种新颖的特征融合框架，用于通过胎儿超声图像自动估计胎龄，无需手动测量。该框架结合了深度学习提取的图像深度表示和影像组学特征，以利用两者的优势。实验结果显示，该方法在胎龄估计上达到了8.0天的平均绝对误差，优于现有机器学习方法，并展现出良好的跨人群鲁棒性。", "keywords": "胎龄估计, 影像组学, 深度学习, 特征融合, 胎儿超声", "comments": "这篇论文的创新点在于结合了深度学习的特征提取能力和影像组学特征的可解释性，为胎龄估计提供了一个更全面和鲁棒的解决方案。其自动化特性有望显著提高产前护理的效率和准确性，减少对操作员的依赖。公开代码也促进了研究的透明性和可复现性。"}}
{"id": "2506.20083", "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder", "authors": ["Yingji Zhang", "Danilo S. Carvalho", "André Freitas"], "summary": "Integrating compositional and symbolic properties into current distributional\nsemantic spaces can enhance the interpretability, controllability,\ncompositionality, and generalisation capabilities of Transformer-based\nauto-regressive language models (LMs). In this survey, we offer a novel\nperspective on latent space geometry through the lens of compositional\nsemantics, a direction we refer to as \\textit{semantic representation\nlearning}. This direction enables a bridge between symbolic and distributional\nsemantics, helping to mitigate the gap between them. We review and compare\nthree mainstream autoencoder architectures-Variational AutoEncoder (VAE),\nVector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the\ndistinctive latent geometries they induce in relation to semantic structure and\ninterpretability.", "comment": "In progress", "pdf_url": "http://arxiv.org/pdf/2506.20083v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20083v1", "AI": {"title_translation": "弥合组合式语义与分布语义：基于自编码器的潜在语义几何综述", "tldr": "该综述探讨了自编码器如何通过诱导潜在语义几何来弥合组合式语义和分布语义之间的鸿沟，从而增强语言模型的能力。", "motivation": "为了增强基于Transformer的自回归语言模型（LMs）的可解释性、可控性、组合性和泛化能力，需要将组合式和符号属性整合到当前的分布语义空间中，以弥合符号语义和分布语义之间的差距。", "method": "本综述从组合式语义的角度（称为语义表示学习）提供了关于潜在空间几何的新视角。它回顾并比较了三种主流自编码器架构——变分自编码器（VAE）、向量量化变分自编码器（VQVAE）和稀疏自编码器（SAE），并考察了它们在语义结构和可解释性方面诱导的独特潜在几何。", "result": "该综述回顾并比较了变分自编码器（VAE）、向量量化变分自编码器（VQVAE）和稀疏自编码器（SAE）这三种主流自编码器架构，并考察了它们在语义结构和可解释性方面诱导的独特潜在几何。", "conclusion": "通过对自编码器诱导的潜在语义几何的考察，本研究方向能够弥合符号语义和分布语义之间的差距。", "translation": "将组合式和符号属性整合到当前的分布语义空间中，可以增强基于Transformer的自回归语言模型（LMs）的可解释性、可控性、组合性和泛化能力。在本综述中，我们从组合式语义的角度（我们称之为“语义表示学习”方向）提供了一个关于潜在空间几何的新颖视角。这个方向能够弥合符号语义和分布语义之间的鸿沟，有助于缩小它们之间的差距。我们回顾并比较了三种主流自编码器架构——变分自编码器（VAE）、向量量化变分自编码器（VQVAE）和稀疏自编码器（SAE），并考察了它们在语义结构和可解释性方面诱导的独特潜在几何。", "summary": "本综述探讨了如何通过自编码器（包括VAE、VQVAE和SAE）诱导的潜在语义几何来弥合组合式语义与分布语义之间的差距。该研究方向旨在增强基于Transformer的语言模型的可解释性、可控性、组合性和泛化能力。", "keywords": "组合式语义, 分布语义, 潜在语义几何, 自编码器, 语言模型", "comments": "该综述通过聚焦自编码器在构建潜在语义几何中的作用，为理解和提升语言模型（LMs）能力提供了一个新颖且重要的视角，尤其是在弥合符号语义和分布语义这一关键挑战方面具有创新性。"}}
{"id": "2506.20240", "title": "Low-order finite element complex with application to a fourth-order elliptic singular perturbation problem", "authors": ["Xuewei Cui", "Xuehai Huang"], "summary": "A low-order nonconforming finite element discretization of a smooth de Rham\ncomplex starting from the $H^2$ space in three dimensions is proposed,\ninvolving an $H^2$-nonconforming finite element space, a new tangentially\ncontinuous $H^1$-nonconforming vector-valued finite element space, the\nlowest-order Raviart-Thomas space, and piecewise constant functions. While\nnonconforming for the smooth complex, the discretization conforms to the\nclassical de Rham complex. It is applied to develop a decoupled mixed finite\nelement method for a fourth-order elliptic singular perturbation problem,\nfocusing on the discretization of a generalized singularly perturbed\nStokes-type equation. In contrast to Nitsche's method, which requires\nadditional stabilization to handle boundary layers, the nodal interpolation\noperator for the lowest-order N\\'{e}d\\'{e}lec element of the second kind is\nintroduced into the discrete bilinear forms. This modification yields a\ndecoupled mixed method that achieves optimal convergence rates uniformly with\nrespect to the perturbation parameter, even in the presence of strong boundary\nlayers, without requiring any additional stabilization.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2506.20240v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20240v1", "AI": {"title_translation": "低阶有限元复形及其在四阶椭圆型奇异摄动问题中的应用", "tldr": "提出了一种低阶非协调有限元方法，用于解决四阶椭圆型奇异摄动问题，该方法在没有额外稳定化的情况下仍能实现最优收敛速度。", "motivation": "针对四阶椭圆型奇异摄动问题，旨在开发一种无需额外稳定化即可处理边界层并实现最优收敛率的解耦混合有限元方法，以克服Nitsche方法需要额外稳定化的局限性。", "method": "该研究提出了一种从三维H^2空间开始的光滑de Rham复形的低阶非协调有限元离散化方案，涉及H^2非协调有限元空间、新型切向连续H^1非协调向量值有限元空间、最低阶Raviart-Thomas空间和分段常数函数。该离散化方案被应用于开发一种解耦混合有限元方法，用于处理四阶椭圆型奇异摄动问题。与Nitsche方法不同，本方法将最低阶Nédélec第二类元的节点插值算子引入到离散双线性形式中，从而避免了额外稳定化。", "result": "所提出的解耦混合方法能够实现关于摄动参数的一致最优收敛速度，即使在存在强边界层的情况下，也无需任何额外的稳定化。", "conclusion": "该研究成功开发了一种新颖的低阶非协调有限元方法，为四阶椭圆型奇异摄动问题提供了稳定且最优的解决方案，克服了现有方法在稳定化方面的局限性。", "translation": "提出了一种从三维H^2空间开始的光滑de Rham复形的低阶非协调有限元离散化方法，该方法涉及H^2非协调有限元空间、一种新型切向连续H^1非协调向量值有限元空间、最低阶Raviart-Thomas空间以及分段常数函数。尽管该离散化对于光滑复形是非协调的，但它符合经典的de Rham复形。该方法被应用于开发一种解耦混合有限元方法，用于解决四阶椭圆型奇异摄动问题，重点关注广义奇异摄动Stokes型方程的离散化。与需要额外稳定化来处理边界层的Nitsche方法不同，本研究将最低阶Nédélec第二类元的节点插值算子引入到离散双线性形式中。这种修改使得解耦混合方法在没有额外稳定化的情况下，即使在存在强边界层的情况下，也能实现关于摄动参数的一致最优收敛速度。", "summary": "本文提出了一种用于四阶椭圆型奇异摄动问题的低阶非协调有限元离散化方法。该方法基于光滑de Rham复形，并引入了特定的非协调有限元空间。通过将节点插值算子引入离散双线性形式，该解耦混合方法在无需额外稳定化的情况下，能够对具有强边界层的奇异摄动问题实现关于摄动参数的一致最优收敛速度，优于传统方法。", "keywords": "有限元方法, 奇异摄动, de Rham复形, 非协调, 最优收敛", "comments": "该论文的创新点在于提出了一种新型的低阶非协调有限元离散化方法，并巧妙地通过引入节点插值算子，成功避免了在处理奇异摄动问题中的边界层时通常所需的额外稳定化。这显著提高了方法的效率和鲁棒性，为四阶椭圆型问题提供了一个更优的数值解法。"}}
{"id": "2506.20315", "title": "Building Forest Inventories with Autonomous Legged Robots -- System, Lessons, and Challenges Ahead", "authors": ["Matías Mattamala", "Nived Chebrolu", "Jonas Frey", "Leonard Freißmuth", "Haedam Oh", "Benoit Casseau", "Marco Hutter", "Maurice Fallon"], "summary": "Legged robots are increasingly being adopted in industries such as oil, gas,\nmining, nuclear, and agriculture. However, new challenges exist when moving\ninto natural, less-structured environments, such as forestry applications. This\npaper presents a prototype system for autonomous, under-canopy forest inventory\nwith legged platforms. Motivated by the robustness and mobility of modern\nlegged robots, we introduce a system architecture which enabled a quadruped\nplatform to autonomously navigate and map forest plots. Our solution involves a\ncomplete navigation stack for state estimation, mission planning, and tree\ndetection and trait estimation. We report the performance of the system from\ntrials executed over one and a half years in forests in three European\ncountries. Our results with the ANYmal robot demonstrate that we can survey\nplots up to 1 ha plot under 30 min, while also identifying trees with typical\nDBH accuracy of 2cm. The findings of this project are presented as five lessons\nand challenges. Particularly, we discuss the maturity of hardware development,\nstate estimation limitations, open problems in forest navigation, future\navenues for robotic forest inventory, and more general challenges to assess\nautonomous systems. By sharing these lessons and challenges, we offer insight\nand new directions for future research on legged robots, navigation systems,\nand applications in natural environments. Additional videos can be found in\nhttps://dynamic.robots.ox.ac.uk/projects/legged-robots", "comment": "20 pages, 13 figures. Pre-print version of the accepted paper for\n  IEEE Transactions on Field Robotics (T-FR)", "pdf_url": "http://arxiv.org/pdf/2506.20315v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20315v1", "AI": {"title_translation": "利用自主足式机器人构建森林清单——系统、经验与未来挑战", "tldr": "本文介绍了一个用于自主林下森林盘点的足式机器人原型系统，展示了ANYmal机器人在恶劣森林环境中进行导航、测绘和树木识别的能力，并总结了系统性能、经验教训和未来挑战。", "motivation": "在石油、天然气、采矿、核能和农业等行业中，足式机器人应用日益广泛。然而，当进入森林等自然、非结构化环境时，面临新的挑战。本文的动机是利用现代足式机器人的鲁棒性和移动性，开发一个用于林下森林盘点的自主系统。", "method": "本文提出了一个用于自主林下森林盘点的足式机器人原型系统。该系统包含一个完整的导航堆栈，用于状态估计、任务规划以及树木检测和特征估计。研究人员使用ANYmal四足机器人平台在欧洲三个国家的森林中进行了为期一年半的试验。", "result": "该系统能够在30分钟内测绘面积达1公顷的林地，同时以2厘米的典型胸径（DBH）精度识别树木。项目结果总结为五个经验教训和挑战，包括硬件开发成熟度、状态估计局限性、森林导航中的开放问题、机器人森林盘点的未来方向以及评估自主系统的普遍挑战。", "conclusion": "通过分享这些经验教训和挑战，本文为足式机器人、导航系统以及在自然环境中的应用提供了见解和新的研究方向。研究表明，尽管存在挑战，足式机器人已展现出在复杂森林环境中进行自主盘点的潜力。", "translation": "足式机器人在石油、天然气、采矿、核能和农业等行业中得到越来越广泛的应用。然而，当进入森林等自然、非结构化环境时，存在新的挑战，例如林业应用。本文提出了一个用于自主林下森林盘点的足式机器人原型系统。受现代足式机器人鲁棒性和移动性的启发，我们介绍了一种系统架构，该架构使四足机器人平台能够自主导航和测绘林地。我们的解决方案包括一个完整的导航堆栈，用于状态估计、任务规划以及树木检测和特征估计。我们报告了该系统在欧洲三个国家的森林中进行了一年半的试验性能。我们使用ANYmal机器人的结果表明，我们可以在30分钟内测绘面积达1公顷的林地，同时以2厘米的典型胸径（DBH）精度识别树木。该项目的发现以五个经验教训和挑战的形式呈现。特别是，我们讨论了硬件开发的成熟度、状态估计的局限性、森林导航中的开放问题、机器人森林盘点的未来途径，以及评估自主系统的更普遍挑战。通过分享这些经验教训和挑战，我们为足式机器人、导航系统以及在自然环境中的未来研究提供了见解和新方向。更多视频可在https://dynamic.robots.ox.ac.uk/projects/legged-robots找到。", "summary": "本文介绍了一种利用足式机器人进行自主林下森林盘点的原型系统。该系统利用现代足式机器人的鲁棒性和移动性，集成了完整的导航堆栈，包括状态估计、任务规划和树木检测与特征估计。通过在欧洲三国森林中进行为期一年半的试验，验证了ANYmal机器人的性能，能够在30分钟内测绘1公顷林地，并以2厘米的精度识别树木胸径。研究总结了五项经验教训和挑战，涵盖硬件、状态估计、森林导航等，为未来足式机器人在自然环境中的应用研究提供了方向和见解。", "keywords": "足式机器人, 森林盘点, 自主导航, ANYmal, 挑战", "comments": "本文的创新之处在于将足式机器人应用于复杂的林下环境进行森林盘点，这相比于轮式或履带式机器人在崎岖地形中具有显著优势。其重要性在于为传统森林盘点提供了高效、自动化的新方法。论文不仅展示了系统的能力，更坦诚地讨论了在实际部署中遇到的挑战和局限性，如状态估计和森林导航的开放问题，这对于该领域的未来研究具有很高的参考价值和指导意义。"}}
{"id": "2506.20214", "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation", "authors": ["Yanzhe Chen", "Huasong Zhong", "Yan Li", "Zhenheng Yang"], "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20214v1", "categories": ["cs.CV", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20214v1", "AI": {"title_translation": "UniCode$^2$: 级联大规模码本用于统一多模态理解与生成", "tldr": "UniCode$^2$ 提出了一种级联大规模视觉码本（50万条目）框架，用于统一多模态理解和生成，解决了现有方法中细粒度语义不足或训练不稳定的问题，同时保持了语义对齐和稳定性。", "motivation": "现有的基于码本的多模态大语言模型（MLLMs）存在缺陷：要么使用小词汇表（约16K条目）导致缺乏细粒度语义，要么盲目扩大规模导致token利用率低和训练不稳定。", "method": "UniCode$^2$ 提出了一个级联码本框架，实现了大规模、语义对齐且稳定的视觉token化。通过聚类数百万SigLIP序列嵌入，构建了一个50万条目的码本，在扩展容量的同时保留了视觉-语言对齐。稳定性通过级联设计确保：一个冻结码本锚定嵌入空间，一个可训练码本细化任务特定语义。这种解耦促进了高利用率和鲁棒学习，并支持与预训练扩散解码器无缝集成。", "result": "UniCode$^2$ 在各种基准测试中表现出色，证明了在不牺牲稳定性、语义或模块性的情况下扩展视觉token空间的可行性。", "conclusion": "该论文的结论是，UniCode$^2$ 成功地扩展了视觉token空间，同时保持了稳定性、语义对齐和模块化，从而在统一多模态理解和生成方面取得了强大的性能。", "translation": "统一多模态大型语言模型（MLLMs）在共同推进多模态理解和生成方面展现出前景，其中视觉码本将图像离散化为token以进行自回归建模。现有的基于码本的方法要么依赖于缺乏细粒度语义的小词汇表（约16K条目），要么盲目地扩大规模，导致token利用率低和训练不稳定。我们提出了UniCode$^2$，一个级联码本框架，实现了大规模、语义对齐且稳定的视觉token化。通过聚类数百万SigLIP序列嵌入，我们构建了一个包含50万个条目的码本，该码本在扩展容量的同时保留了视觉-语言对齐。级联设计确保了稳定性：一个冻结的码本锚定嵌入空间，一个可训练的码本细化特定任务的语义。这种解耦促进了高利用率和鲁棒学习。此外，我们的视觉token与文本语义的对齐使得能够与预训练扩散解码器无缝集成，以最小的适应支持高质量的视觉合成。UniCode$^2$在各种基准测试中表现出色，证明了在不牺牲稳定性、语义或模块性的情况下扩展视觉token空间的可行性。", "summary": "UniCode$^2$ 提出了一种新颖的级联大规模视觉码本框架，旨在解决现有多模态大语言模型在细粒度语义或训练稳定性方面的局限性。它通过聚类SigLIP嵌入构建了一个50万条目的码本，并采用冻结和可训练码本的级联设计来确保视觉-语言对齐和稳定性。这种方法促进了鲁棒学习和高token利用率，并能与扩散解码器无缝集成以实现高质量的视觉合成，在多个基准测试中表现出色。", "keywords": "多模态大语言模型, 视觉码本, 级联设计, Token化, 视觉-语言对齐", "comments": "该论文的创新之处在于其级联设计，使得视觉码本能够扩展到50万个条目，同时保持语义对齐和训练稳定性，这在大型token化中是常见的挑战。冻结和可训练码本的明确解耦是其关键技术贡献，有助于更有效地实现统一的多模态理解和生成。"}}
{"id": "2506.19992", "title": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization", "authors": ["Gabor Petnehazi", "Bernadett Aradi"], "summary": "The explosive growth of complex datasets across various modalities\nnecessitates advanced analytical tools that not only group data effectively but\nalso provide human-understandable insights into the discovered structures. We\nintroduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using\nLLMs for Efficient Summarization), a novel algorithm and Python package\ndesigned for hierarchical k-means clustering of diverse data types, including\ntext, images, and numeric data (processed one modality per run). HERCULES\nconstructs a cluster hierarchy by recursively applying k-means clustering,\nstarting from individual data points at level 0. A key innovation is its deep\nintegration of Large Language Models (LLMs) to generate semantically rich\ntitles and descriptions for clusters at each level of the hierarchy,\nsignificantly enhancing interpretability. The algorithm supports two main\nrepresentation modes: `direct' mode, which clusters based on original data\nembeddings or scaled numeric features, and `description' mode, which clusters\nbased on embeddings derived from LLM-generated summaries. Users can provide a\n`topic\\_seed' to guide LLM-generated summaries towards specific themes. An\ninteractive visualization tool facilitates thorough analysis and understanding\nof the clustering results. We demonstrate HERCULES's capabilities and discuss\nits potential for extracting meaningful, hierarchical knowledge from complex\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19992v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19992v1", "AI": {"title_translation": "HERCULES：基于分层嵌入和递归聚类的LLM高效摘要方法", "tldr": "HERCULES是一种利用LLM进行分层k-means聚类的算法和Python包，旨在为文本、图像和数值数据提供可解释的聚类结果。", "motivation": "复杂数据集的爆炸式增长需要先进的分析工具，这些工具不仅能有效分组数据，还能提供对发现结构的人类可理解的洞察。", "method": "HERCULES通过递归应用k-means聚类来构建簇层次结构，从0级个体数据点开始。它深度集成了大型语言模型（LLMs）来为每个层次的簇生成语义丰富的标题和描述，以增强可解释性。它支持“直接”模式（基于原始数据嵌入或数值特征）和“描述”模式（基于LLM生成摘要的嵌入）。用户可以提供“topic_seed”来引导LLM生成摘要。", "result": "论文展示了HERCULES的能力，并讨论了其从复杂数据集中提取有意义的分层知识的潜力。它还提供了一个交互式可视化工具，促进了对聚类结果的彻底分析和理解。", "conclusion": "HERCULES能够从复杂数据集中提取有意义的分层知识，并提供人类可理解的洞察。", "translation": "复杂数据集在各种模态下的爆炸式增长，需要先进的分析工具，这些工具不仅能有效分组数据，还能提供对发现结构的人类可理解的洞察。我们介绍了HERCULES（基于分层嵌入和递归聚类的LLM高效摘要方法），这是一种新颖的算法和Python包，旨在对包括文本、图像和数值数据在内的多样化数据类型进行分层k-means聚类（每次运行处理一种模态）。HERCULES通过递归应用k-means聚类来构建簇层次结构，从0级个体数据点开始。一个关键创新是它深度集成了大型语言模型（LLMs），为层次结构中每个级别的簇生成语义丰富的标题和描述，显著增强了可解释性。该算法支持两种主要表示模式：“直接”模式，基于原始数据嵌入或缩放的数值特征进行聚类；以及“描述”模式，基于LLM生成的摘要的嵌入进行聚类。用户可以提供“topic_seed”来引导LLM生成的摘要趋向特定主题。一个交互式可视化工具促进了对聚类结果的彻底分析和理解。我们展示了HERCULES的能力，并讨论了其从复杂数据集中提取有意义的分层知识的潜力。", "summary": "HERCULES是一种新颖的算法和Python包，用于对文本、图像和数值数据进行分层k-means聚类。它通过递归聚类构建簇层次结构，并创新性地利用大型语言模型（LLMs）为每个层级的簇生成可解释的标题和描述。该方法支持直接基于数据嵌入或基于LLM生成摘要的嵌入进行聚类，并提供交互式可视化工具，旨在从复杂数据集中提取有意义的分层知识。", "keywords": "分层聚类, 大型语言模型, k-means, 数据摘要, 可解释性", "comments": "HERCULES的创新之处在于其将分层k-means聚类与LLM深度集成，以提高聚类结果的可解释性，这是传统聚类方法难以达到的。它提供两种灵活的聚类模式，并支持多模态数据处理，使其成为处理复杂数据集的强大工具。交互式可视化工具也增强了用户对结果的理解和分析能力。"}}
{"id": "2506.20357", "title": "Tabular Feature Discovery With Reasoning Type Exploration", "authors": ["Sungwon Han", "Sungkyu Park", "Seungeon Lee"], "summary": "Feature engineering for tabular data remains a critical yet challenging step\nin machine learning. Recently, large language models (LLMs) have been used to\nautomatically generate new features by leveraging their vast knowledge.\nHowever, existing LLM-based approaches often produce overly simple or\nrepetitive features, partly due to inherent biases in the transformations the\nLLM chooses and the lack of structured reasoning guidance during generation. In\nthis paper, we propose a novel method REFeat, which guides an LLM to discover\ndiverse and informative features by leveraging multiple types of reasoning to\nsteer the feature generation process. Experiments on 59 benchmark datasets\ndemonstrate that our approach not only achieves higher predictive accuracy on\naverage, but also discovers more diverse and meaningful features. These results\nhighlight the promise of incorporating rich reasoning paradigms and adaptive\nstrategy selection into LLM-driven feature discovery for tabular data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20357v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20357v1", "AI": {"title_translation": "基于推理类型探索的表格特征发现", "tldr": "本文提出REFeat，一种新颖的方法，通过利用多种推理类型来指导大型语言模型发现多样化且信息丰富的表格特征，实验证明其在预测准确性和特征多样性方面优于现有方法。", "motivation": "表格数据的特征工程是机器学习中一个关键但具有挑战性的步骤。现有基于大型语言模型（LLM）的特征生成方法通常产生过于简单或重复的特征，这部分是由于LLM选择转换时固有的偏差以及生成过程中缺乏结构化推理指导。", "method": "本文提出了一种新颖的方法REFeat，它通过利用多种推理类型来引导大型语言模型（LLM）发现多样化且信息丰富的特征，从而指导特征生成过程。", "result": "在59个基准数据集上的实验表明，REFeat方法不仅平均实现了更高的预测准确性，而且发现了更多样化和有意义的特征。", "conclusion": "这些结果突出了将丰富的推理范式和自适应策略选择融入LLM驱动的表格数据特征发现的潜力。", "translation": "表格数据的特征工程仍然是机器学习中一个关键但具有挑战性的步骤。最近，大型语言模型（LLM）已被用于通过利用其庞大的知识来自动生成新特征。然而，现有的基于LLM的方法通常产生过于简单或重复的特征，部分原因是LLM在选择转换时固有的偏差以及生成过程中缺乏结构化推理指导。在本文中，我们提出了一种新颖的方法REFeat，它通过利用多种推理类型来引导LLM发现多样化且信息丰富的特征，从而指导特征生成过程。在59个基准数据集上的实验表明，我们的方法不仅平均实现了更高的预测准确性，而且发现了更多样化和有意义的特征。这些结果突出了将丰富的推理范式和自适应策略选择融入LLM驱动的表格数据特征发现的潜力。", "summary": "本文提出了一种名为REFeat的新方法，旨在解决现有基于大型语言模型（LLM）的表格数据特征生成方法存在的特征简单和重复问题。REFeat通过引入多种推理类型来指导LLM，从而发现更具多样性和信息量的特征。实验结果表明，与现有方法相比，REFeat在预测准确性上有所提高，并且能够生成更丰富、更有意义的特征，这突显了将复杂推理和自适应策略应用于LLM驱动特征发现的潜力。", "keywords": "表格数据, 特征工程, 大型语言模型, 推理类型, REFeat", "comments": "该论文通过引入“推理类型探索”的概念，有效解决了大型语言模型在特征工程中生成特征多样性不足和质量不高的问题，具有创新性。其方法REFeat通过结构化指导LLM，提升了特征的质量和模型的预测性能，为LLM在特征工程领域的应用开辟了新的方向。在实践中，这种方法有望显著减少人工特征工程的工作量。"}}
{"id": "2506.20237", "title": "Time and covariance smoothing for restoration of bivariate signals", "authors": ["Yusuf Yigit Pilavci", "Pierre Palud", "Julien Flamant", "Pierre-Antoine Thouvenin", "Jérémie Boulanger", "Pierre Chainais"], "summary": "In many applications and physical phenomena, bivariate signals are polarized,\ni.e. they trace an elliptical trajectory over time when viewed in the 2D planes\nof their two components. The smooth evolution of this elliptical trajectory,\ncalled polarization ellipse, is highly informative to solve ill-posed inverse\nproblems involving bivariate signals where the signal is collected through\nindirect, noisy or incomplete measurements. This work proposes a novel\nformulation and an efficient algorithm for reconstructing bivariate signals\nwith polarization regularization. The proposed formulation leverages the\ncompact representation of polarization through the instantaneous covariance\nmatrices. To address the resulting quartic optimization problem, we propose a\nwell-suited parameter splitting strategy which leads to an efficient iterative\nalgorithm (alternating direction method of multipliers (ADMM)) with convex\nsubproblems at each iteration. The performance of the proposed method is\nillustrated on numerical synthetic data experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20237v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20237v1", "AI": {"title_translation": "二元信号恢复的时间和协方差平滑", "tldr": "本文提出了一种新颖的公式和基于ADMM的高效算法，用于重建具有极化正则化的二元信号，以解决不适定逆问题。", "motivation": "在许多应用中，二元信号的极化椭圆轨迹的平滑演变对于解决通过间接、噪声或不完整测量收集的二元信号所涉及的不适定逆问题非常有帮助。", "method": "提出了一种用于重建二元信号的极化正则化新公式和高效算法。该方法利用瞬时协方差矩阵紧凑表示极化，并通过参数分裂策略将由此产生的四次优化问题转化为使用交替方向乘子法（ADMM）求解的迭代凸子问题。", "result": "所提出方法的性能通过数值合成数据实验得到了验证。", "conclusion": "Not mentioned in abstract", "translation": "在许多应用和物理现象中，二元信号是极化的，即当从其两个分量的二维平面观察时，它们随时间描绘出椭圆轨迹。这种椭圆轨迹（称为极化椭圆）的平滑演变对于解决涉及二元信号的不适定逆问题非常有帮助，其中信号是通过间接、噪声或不完整的测量收集的。这项工作提出了一种新的公式和高效算法，用于通过极化正则化重建二元信号。所提出的公式利用瞬时协方差矩阵对极化进行紧凑表示。为了解决由此产生的四次优化问题，我们提出了一种非常适合的参数分裂策略，该策略导致了一种高效的迭代算法（交替方向乘子法（ADMM）），在每次迭代中都包含凸子问题。所提出方法的性能通过数值合成数据实验进行了说明。", "summary": "本文提出了一种新颖的公式和高效算法，用于重建在许多应用中呈现椭圆轨迹的极化二元信号。该方法通过瞬时协方差矩阵实现极化正则化，并将由此产生的四次优化问题转化为迭代的凸子问题，利用交替方向乘子法（ADMM）进行求解。数值合成数据实验验证了该方法的有效性。", "keywords": "二元信号, 极化正则化, 协方差平滑, ADMM, 信号重建", "comments": "该论文的创新点在于提出了利用瞬时协方差矩阵进行极化正则化的新颖公式，并通过ADMM有效解决了由此产生的非凸优化问题，为不适定逆问题中的二元信号重建提供了一种有效途径。"}}
{"id": "2506.20450", "title": "Papanicolaou Stain Unmixing for RGB Image Using Weighted Nucleus Sparsity and Total Variation Regularization", "authors": ["Nanxin Gong", "Saori Takeyama", "Masahiro Yamaguchi", "Takumi Urata", "Fumikazu Kimura", "Keiko Ishii"], "summary": "The Papanicolaou stain, consisting of eosin Y, hematoxylin, light Green SF\nyellowish, orange G, and Bismarck brown Y, provides extensive color information\nessential for cervical cancer screening in cytopathology. However, the visual\nobservation of these colors is subjective and difficult to characterize. In\ndigital image analysis, the RGB intensities are affected by staining and\nimaging variations, hindering direct quantification of color in\nPapanicolaou-stained samples. Stain unmixing is a promising alternative that\nquantifies the amounts of dyes. In previous work, multispectral imaging was\nutilized to estimate the dye amounts of Papanicolaou stain for quantitative\ndiagnosis. Still, its application to RGB images presents a challenge since the\nnumber of dyes exceeds the three RGB channels. This paper proposes a novel\nPapanicolaou stain unmixing method for RGB images that incorporates three key\nassumptions: nonnegative stain abundances; a sparse spatial distribution of\nhematoxylin, which binds to nuclei; and piecewise smoothness of stain\nabundances. By formulating this as an optimization problem with nonnegativity,\nweighted nucleus sparsity, and total variation regularizations, our method\nachieved excellent performance in stain quantification when validated against\nthe results of multispectral imaging. We also adopted the proposed method for\ndiscriminating lobular endocervical glandular hyperplasia (LEGH), a\nprecancerous lesion of gastric-type adenocarcinoma of the cervix. The resulting\nquantification distinctly characterized differences between LEGH and normal\nendocervical cells with stain abundance, and a classifier based on the\nquantification results achieved 98.0% accuracy. This demonstrates the\nsignificant potential of RGB-based stain unmixing for quantitative diagnosis.", "comment": "22 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20450v1", "categories": ["eess.IV", "q-bio.QM"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20450v1", "AI": {"title_translation": "基于加权细胞核稀疏性和全变分正则化的巴氏染色RGB图像解混叠", "tldr": "本文提出了一种新颖的巴氏染色RGB图像解混叠方法，通过结合非负性、加权细胞核稀疏性和全变分正则化，实现了染料的精确量化，并成功应用于宫颈癌前病变的鉴别诊断，准确率达98.0%。", "motivation": "巴氏染色在宫颈癌筛查中提供丰富的颜色信息，但视觉观察主观且难以量化。数字图像分析中，RGB强度受染色和成像变化影响，难以直接量化染料。虽然多光谱成像可用于染料量化，但由于染料数量超过RGB三通道，将其应用于RGB图像存在挑战。", "method": "本文提出了一种新颖的巴氏染色RGB图像解混叠方法，该方法整合了三个关键假设：染料丰度非负性；结合细胞核的苏木精稀疏空间分布；以及染料丰度的分段平滑性。通过将其表述为一个包含非负性、加权细胞核稀疏性和全变分正则化的优化问题来解决。", "result": "该方法在染料量化方面表现出色，并通过多光谱成像结果验证。在鉴别宫颈腺体叶状内膜增生（LEGH）时，量化结果清晰地表征了LEGH与正常宫颈内膜细胞之间的染料丰度差异。基于量化结果的分类器实现了98.0%的准确率。", "conclusion": "RGB图像的染料解混叠在定量诊断方面具有显著潜力。", "translation": "巴氏染色由伊红Y、苏木精、亮绿SF黄、橙G和俾斯麦棕Y组成，提供了广泛的颜色信息，对于细胞病理学中的宫颈癌筛查至关重要。然而，对这些颜色的视觉观察是主观的，难以表征。在数字图像分析中，RGB强度受染色和成像变化的影响，阻碍了巴氏染色样本中颜色的直接量化。染料解混叠是一种有前途的替代方法，可以量化染料的含量。在之前的工作中，多光谱成像被用于估计巴氏染料的含量以进行定量诊断。但是，由于染料数量超过了三个RGB通道，其在RGB图像上的应用仍面临挑战。本文提出了一种新颖的巴氏染色RGB图像解混叠方法，该方法结合了三个关键假设：染料丰度非负性；结合细胞核的苏木精稀疏空间分布；以及染料丰度的分段平滑性。通过将其表述为一个包含非负性、加权细胞核稀疏性和全变分正则化的优化问题，我们的方法在染料量化方面表现出色，并通过多光谱成像结果验证。我们还将所提出的方法应用于鉴别宫颈腺体叶状内膜增生（LEGH），这是一种胃型宫颈腺癌的癌前病变。所得的量化结果清晰地表征了LEGH与正常宫颈内膜细胞之间在染料丰度上的差异，并且基于量化结果的分类器达到了98.0%的准确率。这表明基于RGB的染料解混叠在定量诊断方面具有显著潜力。", "summary": "本文提出了一种针对巴氏染色RGB图像的新型染料解混叠方法，旨在克服传统视觉观察的主观性以及RGB图像中染料量化面临的挑战。该方法将染料解混叠问题构建为优化问题，结合了染料丰度的非负性、苏木精在细胞核中的稀疏分布以及染料丰度的分段平滑性等假设，并引入了加权细胞核稀疏性和全变分正则化。实验结果表明，该方法在染料量化方面表现优异，并成功应用于鉴别宫颈腺体叶状内膜增生（LEGH），分类准确率高达98.0%，展现了其在定量诊断方面的巨大潜力。", "keywords": "巴氏染色, 染料解混叠, RGB图像, 加权细胞核稀疏性, 全变分正则化", "comments": "该论文的创新之处在于，它成功地将通常需要多光谱成像的染料解混叠技术应用于更普遍的RGB图像，解决了染料数量多于RGB通道数的难题。通过引入加权细胞核稀疏性和全变分正则化，有效提升了染料量化的准确性。其在鉴别宫颈癌前病变（LEGH）上的高准确率，凸显了该方法在临床定量诊断应用中的重要性和潜力。"}}
{"id": "2506.20093", "title": "ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset", "authors": ["Yilin Wang", "Peixuan Lei", "Jie Song", "Yuzhe Hao", "Tao Chen", "Yuxuan Zhang", "Lei Jia", "Yuanxiang Li", "Zhongyu Wei"], "summary": "Time-series data are critical in diverse applications, such as industrial\nmonitoring, medical diagnostics, and climate research. However, effectively\nintegrating these high-dimensional temporal signals with natural language for\ndynamic, interactive tasks remains a significant challenge. To address this, we\nintroduce the Time-Series Question Answering (Time-Series QA) task and release\nEngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset\ndesigned to capture complex interactions between time-series signals and\nnatural language. Building on this resource, we propose the Instruct Time\nTransformer (ITFormer), a novel framework that bridges time-series encoders\nwith frozen large language models (LLMs). ITFormer effectively extracts,\naligns, and fuses temporal and textual features, achieving a strong improvement\nin QA accuracy over strong baselines with fewer than 1\\% additional trainable\nparameters. By combining computational efficiency with robust cross-modal\nmodeling, our work establishes a adaptable paradigm for integrating temporal\ndata with natural language, paving the way for new research and applications in\nmulti-modal AI. More details about the project, including datasets and code,\nare available at: https://pandalin98.github.io/itformer_site/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20093v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20093v1", "AI": {"title_translation": "ITFormer：通过大规模多任务数据集连接时间序列和自然语言实现多模态问答", "tldr": "本文提出了ITFormer框架和EngineMT-QA数据集，用于解决时间序列与自然语言的多模态问答挑战，通过高效的模型实现了显著的问答准确率提升。", "motivation": "时间序列数据在多种应用中至关重要，但将这些高维时间信号与自然语言有效整合以进行动态交互任务仍然是一个重大挑战。", "method": "本文引入了时间序列问答（Time-Series QA）任务，并发布了EngineMT-QA，这是首个大规模、多任务、时序-文本问答数据集。在此基础上，提出了Instruct Time Transformer (ITFormer) 框架，该框架将时间序列编码器与冻结的大型语言模型（LLMs）连接起来，有效提取、对齐和融合时间与文本特征。", "result": "ITFormer在问答准确率方面比强基线有显著提升，且仅使用不到1%的额外可训练参数。", "conclusion": "通过结合计算效率和强大的跨模态建模，本文的工作为将时间数据与自然语言整合建立了一个适应性范例，为多模态AI领域的新研究和应用铺平了道路。", "translation": "时间序列数据在工业监控、医疗诊断和气候研究等多种应用中至关重要。然而，将这些高维时间信号与自然语言有效整合以进行动态、交互式任务仍然是一个重大挑战。为了解决这个问题，我们引入了时间序列问答（Time-Series QA）任务，并发布了EngineMT-QA，这是首个大规模、多任务、时序-文本问答数据集，旨在捕捉时间序列信号和自然语言之间的复杂交互。基于此资源，我们提出了Instruct Time Transformer (ITFormer)，这是一个新颖的框架，它将时间序列编码器与冻结的大型语言模型（LLMs）连接起来。ITFormer有效提取、对齐和融合时间与文本特征，在问答准确率方面比强基线有显著提升，且仅使用不到1%的额外可训练参数。通过结合计算效率和强大的跨模态建模，我们的工作为将时间数据与自然语言整合建立了一个适应性范例，为多模态AI领域的新研究和应用铺平了道路。有关该项目的更多详细信息，包括数据集和代码，请访问：https://pandalin98.github.io/itformer_site/", "summary": "本文旨在解决时间序列数据与自然语言有效整合以进行交互式任务的挑战。为此，作者提出了时间序列问答（Time-Series QA）任务，并发布了首个大规模多任务时序-文本问答数据集EngineMT-QA。在此基础上，研究人员引入了ITFormer框架，该框架通过连接时间序列编码器和冻结的大型语言模型，高效地提取、对齐和融合时序与文本特征。实验结果表明，ITFormer在问答准确率上取得了显著提升，且仅需极少量的额外可训练参数，为多模态AI中时间数据与自然语言的整合提供了一个高效且适应性强的范例。", "keywords": "时间序列问答, 多模态问答, 大型语言模型, ITFormer, EngineMT-QA", "comments": "本文的创新点在于提出了一个新颖的ITFormer框架，将时间序列数据与大型语言模型相结合，以解决多模态问答问题。同时，发布了首个大规模多任务时序-文本问答数据集EngineMT-QA，为该领域的研究提供了宝贵资源。其高效性（少于1%的额外参数）和强大的性能预示着在多模态AI领域，特别是涉及时间序列数据的应用中，具有广阔的应用前景和研究价值。"}}
{"id": "2506.20308", "title": "Deep random difference method for high dimensional quasilinear parabolic partial differential equations", "authors": ["Wei Cai", "Shuixin Fang", "Tao Zhou"], "summary": "Solving high-dimensional parabolic partial differential equations (PDEs) with\ndeep learning methods is often computationally and memory intensive, primarily\ndue to the need for automatic differentiation (AD) to compute large Hessian\nmatrices in the PDE. In this work, we propose a deep random difference method\n(DRDM) that addresses these issues by approximating the convection-diffusion\noperator using only first-order differences and the solution by deep neural\nnetworks, thus, avoiding explicit Hessian computation. When incorporated into a\nGalerkin framework, the DRDM eliminates the need for pointwise evaluation of\nexpectations, resulting in efficient implementation. We further extend the\napproach to Hamilton-Jacobi-Bellman (HJB) equations. Notably, the DRDM recovers\nexisting martingale deep learning methods for PDEs (Cai et al., 2024,\narXiv:2405.03169), without using the tools of stochastic calculus. The proposed\nmethod offers two main advantages: it removes the dependence on AD for PDE\nderivatives and enables parallel computation of the loss function in both time\nand space. We provide rigorous error estimates for the DRDM in the linear case,\nwhich shows a first order accuracy in $\\Delta t$ used in the sampling of the\npaths by the Euler-Maruyama scheme. Numerical experiments demonstrate that the\nmethod can efficiently and accurately solve quasilinear parabolic PDEs and HJB\nequations in dimensions up to $10^4$ and $10^5$, respectively.", "comment": "41 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.20308v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20308v1", "AI": {"title_translation": "深度随机差分法求解高维拟线性抛物型偏微分方程", "tldr": "提出一种深度随机差分方法（DRDM），通过避免显式计算Hessian矩阵和实现并行计算，高效求解高维拟线性抛物型偏微分方程和HJB方程。", "motivation": "现有深度学习方法求解高维抛物型偏微分方程时，由于需要自动微分（AD）计算大型Hessian矩阵，导致计算和内存密集。", "method": "提出深度随机差分法（DRDM），通过仅使用一阶差分近似对流-扩散算子，并用深度神经网络近似解，从而避免了显式的Hessian计算。当与Galerkin框架结合时，DRDM消除了期望的点态评估。该方法还扩展到Hamilton-Jacobi-Bellman (HJB) 方程，并能恢复现有的鞅深度学习方法。主要优点是消除了对AD的依赖，并实现了损失函数在时间和空间上的并行计算。", "result": "在线性情况下，DRDM的误差估计显示欧拉-丸山方案采样路径中使用的$\\Delta t$具有一阶精度。数值实验表明，该方法可以高效准确地求解维度高达$10^4$的拟线性抛物型PDEs和$10^5$的HJB方程。", "conclusion": "DRDM是一种高效且准确的求解高维拟线性抛物型偏微分方程和HJB方程的方法，它通过避免显式Hessian计算和实现并行化，克服了现有深度学习方法的计算和内存瓶颈。", "translation": "求解高维抛物型偏微分方程（PDEs）的深度学习方法通常计算和内存密集，这主要是因为在PDE中需要自动微分（AD）来计算大型Hessian矩阵。在这项工作中，我们提出了一种深度随机差分方法（DRDM），通过仅使用一阶差分近似对流-扩散算子，并用深度神经网络近似解来解决这些问题，从而避免了显式的Hessian计算。当与Galerkin框架结合时，DRDM消除了期望的点态评估需求，从而实现了高效的实现。我们进一步将该方法扩展到Hamilton-Jacobi-Bellman (HJB) 方程。值得注意的是，DRDM无需使用随机微积分工具即可恢复现有的用于PDEs的鞅深度学习方法（Cai et al., 2024, arXiv:2405.03169）。所提出的方法提供了两个主要优点：它消除了对PDE导数的AD依赖，并实现了损失函数在时间和空间上的并行计算。我们为线性情况下的DRDM提供了严格的误差估计，该估计显示在欧拉-丸山方案采样路径中使用的$\\Delta t$具有一阶精度。数值实验表明，该方法可以高效准确地求解维度分别高达$10^4$和$10^5$的拟线性抛物型PDEs和HJB方程。", "summary": "本文提出了一种深度随机差分方法（DRDM），旨在解决深度学习求解高维抛物型偏微分方程时计算和内存密集的问题。DRDM通过使用一阶差分近似对流-扩散算子和深度神经网络近似解，避免了Hessian矩阵的显式计算，并实现了损失函数的并行计算。该方法可高效求解高维拟线性抛物型PDEs和HJB方程，并在理论上和数值实验中验证了其有效性和精度。", "keywords": "深度随机差分方法, 高维偏微分方程, 拟线性抛物型PDEs, Hamilton-Jacobi-Bellman方程, 自动微分", "comments": "该论文提出了一种创新的深度学习方法DRDM，通过避免自动微分和Hessian矩阵计算，有效解决了高维偏微分方程求解中的计算瓶颈。其能够处理高达$10^5$维的问题，并实现并行计算，这对于大规模科学计算具有重要意义。此外，该方法与现有鞅深度学习方法的联系也值得关注。"}}
{"id": "2506.20320", "title": "Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation", "authors": ["Malte Probst", "Raphael Wenzel", "Tim Puphal", "Monica Dasi", "Nico A. Steinhardt", "Sango Matsuzaki", "Misa Komuro"], "summary": "In Social Robot Navigation, autonomous agents need to resolve many sequential\ninteractions with other agents. State-of-the art planners can efficiently\nresolve the next, imminent interaction cooperatively and do not focus on longer\nplanning horizons. This makes it hard to maneuver scenarios where the agent\nneeds to select a good strategy to find gaps or channels in the crowd. We\npropose to decompose trajectory planning into two separate steps: Conflict\navoidance for finding good, macroscopic trajectories, and cooperative collision\navoidance (CCA) for resolving the next interaction optimally. We propose the\nProbabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies\nan established probabilistic collision risk model to include a general\nassumption of cooperativity. PGP biases the short-term CCA planner to head\ntowards gaps in the crowd. In extensive simulations with crowds of varying\ndensity, we show that using PGP in addition to state-of-the-art CCA planners\nimproves the agents' performance: On average, agents keep more space to others,\ncreate less tension, and cause fewer collisions. This typically comes at the\nexpense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot\nby Honda R&D.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20320v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20320v1", "AI": {"title_translation": "寻找捷径——用于社交机器人导航的概率间隙规划器", "tldr": "本文提出了概率间隙规划器（PGP），通过将轨迹规划分解为冲突避免和合作碰撞避免两步，显著提升了社交机器人导航在复杂人群中的表现，减少了碰撞和紧张度，并能在实际机器人上实时运行。", "motivation": "现有的机器人导航规划器在处理与他人的顺序交互时，主要关注即时交互而缺乏长远的规划视野，这使得机器人在人群中难以有效寻找间隙或通道。", "method": "本文将轨迹规划分解为两个独立的步骤：用于寻找宏观轨迹的冲突避免和用于最优解决即时交互的合作碰撞避免（CCA）。提出概率间隙规划器（PGP）作为冲突避免规划器，它修改了已有的概率碰撞风险模型以纳入合作性假设，并引导短期CCA规划器朝人群中的间隙方向移动。", "result": "在不同密度人群的广泛模拟中，结合PGP和现有CCA规划器能提升代理的性能：代理平均与他人保持更多空间，产生更少的紧张度，并导致更少的碰撞。这通常以路径略长为代价。PGP能在本田研发的WaPOCHI移动机器人上实时运行。", "conclusion": "概率间隙规划器（PGP）通过分解轨迹规划并专注于长远规划中的间隙寻找，有效提升了社交机器人在复杂人群环境中的导航能力和安全性，尽管路径可能略有增加，但实现了实时操作。", "translation": "在社交机器人导航中，自主代理需要解决与其它代理的许多顺序交互。最先进的规划器可以有效地协作解决下一个迫在眉睫的交互，但并不关注更长的规划视野。这使得代理难以在需要选择良好策略来在人群中寻找间隙或通道的场景中进行机动。我们提出将轨迹规划分解为两个独立的步骤：用于寻找良好宏观轨迹的冲突避免，以及用于最优解决下一个交互的合作碰撞避免（CCA）。我们提出概率间隙规划器（PGP）作为冲突避免规划器。PGP修改了一个已建立的概率碰撞风险模型，以包含合作性的普遍假设。PGP偏向短期CCA规划器朝人群中的间隙方向移动。在不同密度人群的广泛模拟中，我们表明，除了最先进的CCA规划器之外，使用PGP改进了代理的性能：平均而言，代理与他人保持更多空间，产生更少的紧张度，并导致更少的碰撞。这通常以路径略长为代价。PGP可以在本田研发的WaPOCHI移动机器人上实时运行。", "summary": "本文提出了一种名为概率间隙规划器（PGP）的新型方法，用于改进社交机器人导航。针对现有规划器缺乏长远视野、难以在人群中寻找通道的问题，PGP将轨迹规划分解为冲突避免和合作碰撞避免两步。PGP作为冲突避免模块，通过引入合作性假设修改了碰撞风险模型，并引导机器人朝人群间隙移动。实验证明，PGP与现有合作碰撞避免规划器结合使用时，能显著提升机器人在人群中的导航表现，减少碰撞和紧张度，同时保持更大空间，并能在实际机器人上实时运行，尽管路径可能略有延长。", "keywords": "社交机器人导航, 概率间隙规划器, 冲突避免, 合作碰撞避免, 人群导航", "comments": "该论文的创新点在于将社交机器人导航的轨迹规划分解为宏观的“冲突避免”（通过PGP实现）和微观的“合作碰撞避免”两步，有效解决了现有方法在复杂人群中长远规划不足的问题。PGP通过引入合作性假设来预测和利用人群中的间隙，这对于提升机器人的社会接受度和效率至关重要。其重要性体现在它显著改善了机器人在高密度人群中的导航表现（更少碰撞、更少紧张），并实现了实时性，这对于实际部署具有重要意义。然而，路径略长是其潜在的局限性，可能在某些应用中需要权衡。"}}
{"id": "2506.20222", "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission", "authors": ["Pujing Yang", "Guangyi Zhang", "Yunlong Cai", "Lei Yu", "Guanding Yu"], "summary": "Event cameras asynchronously capture pixel-level intensity changes with\nextremely low latency. They are increasingly used in conjunction with RGB\ncameras for a wide range of vision-related applications. However, a major\nchallenge in these hybrid systems lies in the transmission of the large volume\nof triggered events and RGB images. To address this, we propose a transmission\nscheme that retains efficient reconstruction performance of both sources while\naccomplishing real-time deblurring in parallel. Conventional RGB cameras and\nevent cameras typically capture the same scene in different ways, often\nresulting in significant redundant information across their outputs. To address\nthis, we develop a joint event and image (E-I) transmission framework to\neliminate redundancy and thereby optimize channel bandwidth utilization. Our\napproach employs Bayesian modeling and the information bottleneck method to\ndisentangle the shared and domain-specific information within the E-I inputs.\nThis disentangled information bottleneck framework ensures both the compactness\nand informativeness of extracted shared and domain-specific information.\nMoreover, it adaptively allocates transmission bandwidth based on scene\ndynamics, i.e., more symbols are allocated to events for dynamic details or to\nimages for static information. Simulation results demonstrate that the proposed\nscheme not only achieves superior reconstruction quality compared to\nconventional systems but also delivers enhanced deblurring performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20222v1", "categories": ["cs.CV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20222v1", "AI": {"title_translation": "混合事件-RGB传输的动态带宽分配", "tldr": "提出了一种用于混合事件-RGB相机数据传输的动态带宽分配方案，旨在减少冗余并提高重建和去模糊性能。", "motivation": "混合事件-RGB系统在传输大量触发事件和RGB图像时面临巨大挑战，且常规RGB相机和事件相机捕获同一场景时通常存在显著冗余信息。", "method": "提出了一种联合事件和图像（E-I）传输框架，通过贝叶斯建模和信息瓶颈方法解耦E-I输入中的共享和领域特定信息，确保提取信息的紧凑性和信息量。该方法还根据场景动态自适应地分配传输带宽，即动态细节分配更多符号给事件，静态信息分配更多符号给图像。", "result": "仿真结果表明，所提出的方案与传统系统相比，不仅实现了卓越的重建质量，而且提供了增强的去模糊性能。", "conclusion": "所提出的动态带宽分配方案通过优化信道带宽利用率，有效解决了混合事件-RGB系统中的大数据量传输挑战，并显著提高了重建和去模糊性能。", "translation": "事件相机以极低的延迟异步捕获像素级强度变化。它们越来越多地与RGB相机结合使用，应用于广泛的视觉相关应用。然而，这些混合系统中的一个主要挑战在于大量触发事件和RGB图像的传输。为了解决这个问题，我们提出了一种传输方案，该方案在实现实时去模糊的同时，保持了两种源的高效重建性能。传统的RGB相机和事件相机通常以不同的方式捕获同一场景，这常常导致它们的输出之间存在显著的冗余信息。为了解决这个问题，我们开发了一个联合事件和图像（E-I）传输框架，以消除冗余，从而优化信道带宽利用率。我们的方法采用贝叶斯建模和信息瓶颈方法来解耦E-I输入中的共享和领域特定信息。这种解耦信息瓶颈框架确保了提取的共享和领域特定信息的紧凑性和信息量。此外，它根据场景动态自适应地分配传输带宽，即动态细节将更多符号分配给事件，静态信息分配给图像。仿真结果表明，所提出的方案不仅比传统系统实现了卓越的重建质量，而且提供了增强的去模糊性能。", "summary": "本文提出了一种针对混合事件-RGB传输的动态带宽分配方案，以解决大数据量传输和信息冗余问题。该方案采用联合事件和图像（E-I）传输框架，利用贝叶斯建模和信息瓶颈方法解耦共享和领域特定信息，并根据场景动态自适应分配带宽。仿真结果显示，该方案在重建质量和去模糊性能上均优于传统系统。", "keywords": "动态带宽分配, 事件相机, RGB相机, 信息瓶颈, 混合传输", "comments": "该论文的创新点在于引入了贝叶斯建模和信息瓶颈方法来解耦混合事件-RGB数据中的共享与领域特定信息，并结合场景动态自适应地分配带宽。这有效解决了混合相机系统中数据传输的冗余和效率问题，提升了重建和去模糊性能，对于未来混合视觉系统的实际应用具有重要意义。"}}
{"id": "2506.19997", "title": "TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design", "authors": ["Geonwoo Cho", "Jaegyun Im", "Jihwan Lee", "Hojun Yi", "Sejin Kim", "Sundong Kim"], "summary": "Generalizing deep reinforcement learning agents to unseen environments\nremains a significant challenge. One promising solution is Unsupervised\nEnvironment Design (UED), a co-evolutionary framework in which a teacher\nadaptively generates tasks with high learning potential, while a student learns\na robust policy from this evolving curriculum. Existing UED methods typically\nmeasure learning potential via regret, the gap between optimal and current\nperformance, approximated solely by value-function loss. Building on these\napproaches, we introduce the transition prediction error as an additional term\nin our regret approximation. To capture how training on one task affects\nperformance on others, we further propose a lightweight metric called\nco-learnability. By combining these two measures, we present Transition-aware\nRegret Approximation with Co-learnability for Environment Design (TRACED).\nEmpirical evaluations show that TRACED yields curricula that improve zero-shot\ngeneralization across multiple benchmarks while requiring up to 2x fewer\nenvironment interactions than strong baselines. Ablation studies confirm that\nthe transition prediction error drives rapid complexity ramp-up and that\nco-learnability delivers additional gains when paired with the transition\nprediction error. These results demonstrate how refined regret approximation\nand explicit modeling of task relationships can be leveraged for\nsample-efficient curriculum design in UED.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19997v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.19997v1", "AI": {"title_translation": "TRACED：面向环境设计的转移感知后悔近似与协同学习", "tldr": "TRACED通过引入转移预测误差和协同学习性，改进了无监督环境设计（UED）中的遗憾近似，从而在强化学习中实现了更高效、泛化能力更强的课程学习。", "motivation": "深度强化学习智能体泛化到未见环境仍是一个重大挑战。无监督环境设计（UED）作为一种共同进化的框架，通过教师自适应地生成具有高学习潜力的任务，学生从进化的课程中学习鲁棒策略，是解决此问题的一个有前景的方案。现有UED方法通常仅通过价值函数损失来近似衡量学习潜力（即后悔）。", "method": "我们引入了转移预测误差作为后悔近似的额外项。为了捕捉在一个任务上的训练如何影响在其他任务上的表现，我们进一步提出了一种轻量级指标，称为协同学习性。通过结合这两个度量，我们提出了TRACED（Transition-aware Regret Approximation with Co-learnability for Environment Design）。", "result": "实证评估表明，TRACED生成的课程在多个基准测试中改善了零样本泛化能力，同时比强基线方法所需的交互次数减少了多达2倍。消融研究证实，转移预测误差推动了复杂度的快速提升，并且当与转移预测误差结合时，协同学习性带来了额外的增益。", "conclusion": "这些结果表明，通过改进后悔近似和明确建模任务关系，可以实现无监督环境设计（UED）中样本高效的课程设计。", "translation": "将深度强化学习智能体泛化到未见环境仍然是一个重大挑战。一个有前景的解决方案是无监督环境设计（UED），这是一个共同进化的框架，其中教师自适应地生成具有高学习潜力的任务，而学生则从这个不断演变的课程中学习鲁棒策略。现有的UED方法通常通过后悔（即最优性能与当前性能之间的差距）来衡量学习潜力，而后悔仅通过价值函数损失来近似。在此基础上，我们引入了转移预测误差作为我们后悔近似的额外项。为了捕捉在一个任务上的训练如何影响在其他任务上的表现，我们进一步提出了一种轻量级指标，称为协同学习性。通过结合这两个度量，我们提出了TRACED（Transition-aware Regret Approximation with Co-learnability for Environment Design）。实证评估表明，TRACED生成的课程在多个基准测试中改善了零样本泛化能力，同时比强基线方法所需的交互次数减少了多达2倍。消融研究证实，转移预测误差推动了复杂度的快速提升，并且当与转移预测误差结合时，协同学习性带来了额外的增益。这些结果表明，改进的后悔近似和任务关系的明确建模如何能够用于无监督环境设计（UED）中样本高效的课程设计。", "summary": "该论文提出了TRACED，一种用于无监督环境设计（UED）的新方法，旨在解决深度强化学习智能体在未见环境中的泛化问题。TRACED通过引入转移预测误差作为额外的后悔近似项，并提出协同学习性指标来捕捉任务间的关系，从而改进了现有UED方法中基于价值函数损失的后悔近似。实验结果表明，TRACED能够生成提高零样本泛化能力的课程，并显著减少环境交互次数，证明了其在样本高效课程设计方面的有效性。", "keywords": "无监督环境设计, 强化学习, 泛化, 后悔近似, 课程学习", "comments": "TRACED的创新点在于其将转移预测误差纳入后悔近似以及引入协同学习性来显式建模任务关系，这比传统仅依赖价值函数损失的方法更全面。这种结合不仅提高了课程设计的效率（减少了交互次数），还显著提升了智能体的泛化能力，对于解决强化学习中的泛化瓶颈具有重要意义。"}}
{"id": "2506.20384", "title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios", "authors": ["Dror Ivry", "Oran Nahum"], "summary": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20384v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20384v1", "AI": {"title_translation": "Paladin-mini：一种在现实场景中表现卓越的紧凑高效基础模型", "tldr": "本文介绍了Paladin-mini，一个紧凑高效的开源接地分类模型，以及一个新的接地评估基准数据集，并展示了Paladin-mini的性能。", "motivation": "解决在给定上下文中对声明进行“接地”（grounding）的问题，即判断文档中是否存在支持声明的证据。", "method": "本文引入了Paladin-mini，一个3.8B参数的紧凑型开源分类器模型，用于将数据标记为已接地或未接地；同时还推出了grounding-benchmark，一个用于评估关键推理任务性能的新评估数据集。", "result": "展示了Paladin-mini与当前最先进模型的基准测试结果，并提供了清晰可复现的结果。", "conclusion": "Not mentioned in abstract", "translation": "本文提出了两项重要贡献，旨在解决在给定上下文中对声明进行“接地”（grounding）的问题。接地是指，在给定上下文（文档）和声明的情况下，文档中至少存在一个支持该声明的证据。我们将介绍Paladin-mini，一个紧凑（3.8B参数）的开源分类器模型（用于将数据标记为已接地或未接地），该模型专为在现实世界场景中实现强大性能而设计；以及grounding-benchmark，一个旨在评估关键推理任务性能的新评估数据集。我们还将展示Paladin-mini与当前最先进技术的基准测试结果，并分享清晰可复现的结果。", "summary": "本文介绍了Paladin-mini，一个3.8B参数的紧凑型开源分类器模型，专为在现实世界场景中进行声明接地（即判断文档中是否存在支持声明的证据）而设计。同时，论文还提出了一个新的评估数据集grounding-benchmark，用于评估关键推理任务。研究展示了Paladin-mini与现有最先进模型的基准测试结果，并强调了其在真实场景中的卓越性能。", "keywords": "Grounding, Paladin-mini, 分类器, 评估数据集, 现实场景", "comments": "Paladin-mini模型参数量小（3.8B），是一个紧凑高效的开源解决方案，专注于现实世界的接地任务。同时，论文还贡献了一个新的基准数据集grounding-benchmark，这对于该领域的研究和评估具有重要意义。强调了模型在真实场景中的鲁棒性，并提供了可复现的结果，增加了其可用性和可信度。"}}
{"id": "2506.20082", "title": "Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks", "authors": ["Yali Yuan", "Weiyi Zou", "Guang Cheng"], "summary": "Website Fingerprinting (WF) attacks aim to infer which websites a user is\nvisiting by analyzing traffic patterns, thereby compromising user anonymity.\nAlthough this technique has been demonstrated to be effective in controlled\nexperimental environments, it remains largely limited to small-scale scenarios,\ntypically restricted to recognizing website homepages. In practical settings,\nhowever, users frequently access multiple subpages in rapid succession, often\nbefore previous content fully loads. WebPage Fingerprinting (WPF) generalizes\nthe WF framework to large-scale environments by modeling subpages of the same\nsite as distinct classes. These pages often share similar page elements,\nresulting in lower inter-class variance in traffic features. Furthermore, we\nconsider multi-tab browsing scenarios, in which a single trace encompasses\nmultiple categories of webpages. This leads to overlapping traffic segments,\nand similar features may appear in different positions within the traffic,\nthereby increasing the difficulty of classification. To address these\nchallenges, we propose an attention-driven fine-grained WPF attack, named\nADWPF. Specifically, during the training phase, we apply targeted augmentation\nto salient regions of the traffic based on attention maps, including attention\ncropping and attention masking. ADWPF then extracts low-dimensional features\nfrom both the original and augmented traffic and applies self-attention modules\nto capture the global contextual patterns of the trace. Finally, to handle the\nmulti-tab scenario, we employ the residual attention to generate class-specific\nrepresentations of webpages occurring at different temporal positions.\nExtensive experiments demonstrate that the proposed method consistently\nsurpasses state-of-the-art baselines across datasets of different scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20082v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20082v1", "AI": {"title_translation": "更智能的攻击：注意力驱动的细粒度网页指纹攻击", "tldr": "本文提出了一种名为ADWPF的注意力驱动细粒度网页指纹攻击，通过针对性增强和自注意力模块，有效解决了大规模多子页面和多标签浏览场景下网页指纹识别的挑战，并超越了现有技术水平。", "motivation": "现有的网站指纹识别（WF）攻击主要局限于小规模场景，通常只能识别网站主页。然而，在实际应用中，用户经常快速访问多个子页面，且可能进行多标签浏览，这导致流量特征的类间差异减小、流量段重叠以及相似特征在不同位置出现，从而增加了分类难度。", "method": "本文提出了一种名为ADWPF的注意力驱动细粒度网页指纹（WPF）攻击。在训练阶段，ADWPF根据注意力图对流量的显著区域进行有针对性的增强，包括注意力裁剪和注意力掩蔽。然后，它从原始流量和增强流量中提取低维特征，并应用自注意力模块来捕获流量的全局上下文模式。最后，为了处理多标签场景，该方法采用残差注意力来生成在不同时间位置出现的网页的类别特定表示。", "result": "广泛的实验表明，所提出的方法在不同规模的数据集上始终优于最先进的基线方法。", "conclusion": "通过注意力驱动的细粒度网页指纹攻击（ADWPF），可以有效应对大规模多子页面和多标签浏览场景下的网页指纹识别挑战，并显著提升攻击性能。", "translation": "网站指纹识别（WF）攻击旨在通过分析流量模式来推断用户正在访问哪些网站，从而损害用户匿名性。尽管这项技术已被证明在受控实验环境中有效，但它在很大程度上仍局限于小规模场景，通常仅限于识别网站主页。然而，在实际设置中，用户经常快速连续访问多个子页面，通常在之前的内容完全加载之前。网页指纹识别（WPF）通过将同一站点的子页面建模为不同的类别，将WF框架推广到大规模环境。这些页面通常共享相似的页面元素，导致流量特征的类间差异较小。此外，我们考虑了多标签浏览场景，其中一个跟踪包含多个类别的网页。这导致流量段重叠，并且相似的特征可能出现在流量中的不同位置，从而增加了分类的难度。为了解决这些挑战，我们提出了一种注意力驱动的细粒度WPF攻击，名为ADWPF。具体来说，在训练阶段，我们根据注意力图对流量的显著区域应用有针对性的增强，包括注意力裁剪和注意力掩蔽。ADWPF然后从原始流量和增强流量中提取低维特征，并应用自注意力模块来捕获跟踪的全局上下文模式。最后，为了处理多标签场景，我们采用残差注意力来生成在不同时间位置出现的网页的类别特定表示。广泛的实验表明，所提出的方法在不同规模的数据集上始终优于最先进的基线方法。", "summary": "本文提出了一种名为ADWPF的注意力驱动细粒度网页指纹攻击，旨在解决现有网站指纹识别技术在处理大规模多子页面和多标签浏览场景时的局限性。通过引入基于注意力图的流量增强（包括裁剪和掩蔽）以及利用自注意力和残差注意力模块，ADWPF能够有效捕获复杂的流量模式并生成类别特定的网页表示。实验结果表明，ADWPF在不同规模数据集上的性能均超越了现有最先进的方法，显著提升了网页指纹识别的准确性和适用范围。", "keywords": "网页指纹识别, 注意力机制, 细粒度攻击, 流量分析, 用户匿名性", "comments": "该论文的创新点在于将注意力机制引入网页指纹识别攻击，特别是通过注意力裁剪和掩蔽进行有针对性的流量增强，以及利用自注意力和残差注意力来处理复杂的流量模式和多标签场景。这对于提升大规模和细粒度网页指纹识别的有效性具有重要意义，揭示了用户匿名性在更复杂浏览行为下的潜在风险。"}}
{"id": "2506.20248", "title": "Superimposed DMRS for Spectrally Efficient 6G Uplink Multi-User OFDM: Classical vs AI/ML Receivers", "authors": ["Sajad Rezaie", "Mikko Honkala", "Dani Korpi", "Dick Carrillo Melgarejo", "Tomasz Izydorczyk", "Dimitri Gold", "Oana-Elena Barbu"], "summary": "Fifth-generation (5G) systems utilize orthogonal demodulation reference\nsignals (DMRS) to enable channel estimation at the receiver. These orthogonal\nDMRS-also referred to as pilots-are effective in avoiding pilot contamination\nand interference from both the user's own data and that of others. However,\nthis approach incurs a significant overhead, as a substantial portion of the\ntime-frequency resources must be reserved for pilot transmission. Moreover, the\noverhead increases with the number of users and transmission layers.\n  To address these limitations in the context of emerging sixth-generation (6G)\nsystems and to support data transmission across the entire time-frequency grid,\nthe superposition of data and DMRS symbols has been explored as an alternative\nDMRS transmission strategy. In this study, we propose an enhanced version of\nDeepRx, a deep convolutional neural network (CNN)-based receiver, capable of\nestimating the channel from received superimposed (SI) DMRS symbols and\nreliably detecting the transmitted data. We also design a conventional receiver\nfor comparison, which estimates the channel from SI DMRS using classical signal\nprocessing techniques. Extensive evaluations in both uplink single-user and\nmulti-user scenarios demonstrate that DeepRx consistently outperforms the\nconventional receivers in terms of performance.", "comment": "13 pages, this work has been submitted to IEEE for consideration for\n  publication", "pdf_url": "http://arxiv.org/pdf/2506.20248v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20248v1", "AI": {"title_translation": "用于频谱高效6G上行多用户OFDM的叠加DMRS：经典与AI/ML接收机", "tldr": "本研究提出并评估了用于6G上行多用户OFDM系统中叠加DMRS的基于深度学习的接收机（DeepRx），与传统接收机相比，DeepRx在信道估计和数据检测方面表现更优，旨在提高频谱效率。", "motivation": "第五代（5G）系统使用的正交解调参考信号（DMRS）虽然能有效避免导频污染和干扰，但会引入显著的开销，尤其是在用户数量和传输层增加时。为了解决6G系统中这一限制并支持在整个时频格上进行数据传输，需要探索新的DMRS传输策略。", "method": "本研究提出了一种增强版的DeepRx，这是一种基于深度卷积神经网络（CNN）的接收机，能够从接收到的叠加（SI）DMRS符号中估计信道并可靠地检测传输数据。同时，还设计了一个使用经典信号处理技术从SI DMRS中估计信道的传统接收机进行比较。", "result": "在单用户和多用户上行链路场景下的广泛评估表明，DeepRx在性能方面始终优于传统接收机。", "conclusion": "叠加DMRS结合基于AI/ML的接收机（如DeepRx）是解决5G正交DMRS高开销问题并提高6G系统频谱效率的有效方法，DeepRx在信道估计和数据检测方面表现出优越性。", "translation": "第五代（5G）系统利用正交解调参考信号（DMRS）在接收端实现信道估计。这些正交DMRS——也称为导频——能有效避免导频污染以及来自用户自身数据和他人数据的干扰。然而，这种方法会带来显著的开销，因为很大一部分时频资源必须预留给导频传输。此外，开销随着用户数量和传输层的增加而增加。\n为了解决新兴的第六代（6G）系统中的这些限制并支持在整个时频格上的数据传输，研究人员探索了数据和DMRS符号的叠加作为一种替代的DMRS传输策略。在本研究中，我们提出了一种增强版的DeepRx，这是一种基于深度卷积神经网络（CNN）的接收机，能够从接收到的叠加（SI）DMRS符号中估计信道并可靠地检测传输数据。我们还设计了一个用于比较的传统接收机，该接收机使用经典信号处理技术从SI DMRS中估计信道。在上行链路单用户和多用户场景中的广泛评估表明，DeepRx在性能方面始终优于传统接收机。", "summary": "本研究旨在解决5G正交DMRS带来的高开销问题，为6G系统提出一种频谱高效的DMRS传输策略。通过叠加数据和DMRS符号，并开发了一种基于深度卷积神经网络的接收机DeepRx，该接收机能够从叠加DMRS中估计信道并检测数据。与使用经典信号处理技术的传统接收机相比，DeepRx在单用户和多用户上行链路场景中均表现出卓越的性能，证明了AI/ML接收机在未来通信系统中的潜力。", "keywords": "6G, DMRS, 频谱效率, 深度学习, AI/ML接收机", "comments": "该论文的创新点在于提出了将DMRS与数据叠加传输以提高频谱效率，并利用深度学习（DeepRx）来处理由此产生的复杂信道估计和数据检测问题。这对于未来的6G系统具有重要意义，因为它解决了传统导频方案的开销问题。通过与经典接收机的对比，凸显了AI/ML方法在复杂信号处理场景下的优势。"}}
{"id": "2506.20614", "title": "Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation", "authors": ["Simon Perrin", "Sébastien Levilly", "Huajun Sun", "Harold Mouchère", "Jean-Michel Serfaty"], "summary": "In recent decades, the use of 4D Flow MRI images has enabled the\nquantification of velocity fields within a volume of interest and along the\ncardiac cycle. However, the lack of resolution and the presence of noise in\nthese biomarkers are significant issues. As indicated by recent studies, it\nappears that biomarkers such as wall shear stress are particularly impacted by\nthe poor resolution of vessel segmentation. The Phase Contrast Magnetic\nResonance Angiography (PC-MRA) is the state-of-the-art method to facilitate\nsegmentation. The objective of this work is to introduce a new handcraft\nfeature that provides a novel visualisation of 4D Flow MRI images, which is\nuseful in the segmentation task. This feature, termed Weighted Mean Frequencies\n(WMF), is capable of revealing the region in three dimensions where a voxel has\nbeen passed by pulsatile flow. Indeed, this feature is representative of the\nhull of all pulsatile velocity voxels. The value of the feature under\ndiscussion is illustrated by two experiments. The experiments involved\nsegmenting 4D Flow MRI images using optimal thresholding and deep learning\nmethods. The results obtained demonstrate a substantial enhancement in terms of\nIoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with\nthe PC-MRA feature, as evidenced by the deep learning task. This feature has\nthe potential to yield valuable insights that could inform future segmentation\nprocesses in other vascular regions, such as the heart or the brain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20614v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20614v1", "AI": {"title_translation": "加权平均频率：一种用于4D血流MRI分割的手工傅里叶特征", "tldr": "本文引入了一种新的手工傅里叶特征——加权平均频率（WMF），用于4D血流MRI图像分割。该特征显著提高了分割精度（IoU和Dice），尤其是在深度学习任务中，优于现有最先进的PC-MRA方法。", "motivation": "4D血流MRI图像存在分辨率低和噪声大的问题，这严重影响了壁面剪切应力等生物标志物的准确性，进而影响血管分割的精度。尽管相衬磁共振血管造影（PC-MRA）是目前最先进的分割方法，但仍有改进空间。", "method": "本研究提出了一种名为加权平均频率（WMF）的新型手工特征。WMF旨在揭示三维空间中被脉动血流通过的体素区域，代表所有脉动速度体素的包络。通过将WMF特征应用于4D血流MRI图像分割任务，并使用最佳阈值和深度学习方法进行实验评估，将其性能与PC-MRA特征进行比较。", "result": "WMF特征在分割性能上取得了显著提升。与PC-MRA特征相比，在深度学习任务中，WMF使IoU和Dice分数分别提高了0.12和0.13。", "conclusion": "加权平均频率（WMF）特征为4D血流MRI图像提供了一种新颖的可视化方法，显著提高了分割精度，超越了PC-MRA等现有最先进的方法。该特征在未来其他血管区域（如心脏或大脑）的分割过程中具有潜在应用价值。", "translation": "近几十年来，4D血流MRI图像的使用使得在感兴趣体积内和心脏周期中量化速度场成为可能。然而，这些生物标志物缺乏分辨率和存在噪声是重要问题。正如最近的研究所示，血管分割分辨率差对壁面剪切应力等生物标志物的影响尤其显著。相衬磁共振血管造影（PC-MRA）是目前最先进的分割方法。这项工作的目标是引入一种新的手工特征，为4D血流MRI图像提供一种新颖的可视化，这在分割任务中非常有用。这个被称为加权平均频率（WMF）的特征能够揭示三维空间中体素被脉动血流通过的区域。事实上，这个特征代表了所有脉动速度体素的包络。通过两个实验说明了所讨论特征的价值。实验涉及使用最佳阈值和深度学习方法分割4D血流MRI图像。获得的结果表明，与PC-MRA特征相比，在IoU和Dice方面有显著增强，特别是在深度学习任务中分别增加了0.12和0.13。该特征有潜力提供有价值的见解，为未来在其他血管区域（如心脏或大脑）的分割过程提供信息。", "summary": "本论文旨在解决4D血流MRI图像中分辨率低和噪声大导致血管分割不准确的问题。为此，提出了一种名为加权平均频率（WMF）的新型手工傅里叶特征。WMF能够可视化脉动血流区域，从而提高分割精度。通过使用最佳阈值和深度学习方法进行的实验表明，与现有最先进的PC-MRA特征相比，WMF显著提升了IoU和Dice分数（分别提高了0.12和0.13），尤其是在与深度学习结合时效果更佳。作者认为WMF在其他血管区域的分割中也具有广阔的应用前景。", "keywords": "加权平均频率, 4D血流MRI, 分割, 傅里叶特征, 深度学习", "comments": "该论文引入了一种创新的手工特征WMF，利用傅里叶分析捕获4D血流MRI中的脉动血流信息。其对PC-MRA的显著改进，特别是在与深度学习结合时，突显了其提高临床诊断准确性的潜力。在以端到端深度学习为主导的时代，关注“手工”特征是值得注意的，这表明精心设计的特征仍然可以带来实质性益处，可能通过为后续模型提供更稳健或可解释的输入。"}}
{"id": "2506.20100", "title": "MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations", "authors": ["Vardhan Dongre", "Chi Gui", "Shubham Garg", "Hooshang Nayyeri", "Gokhan Tur", "Dilek Hakkani-Tür", "Vikram S. Adve"], "summary": "We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning\nand decision-making in consultative interaction settings. Designed for the\nagriculture domain, MIRAGE captures the full complexity of expert consultations\nby combining natural user queries, expert-authored responses, and image-based\ncontext, offering a high-fidelity benchmark for evaluating models on grounded\nreasoning, clarification strategies, and long-form generation in a real-world,\nknowledge-intensive domain. Grounded in over 35,000 real user-expert\ninteractions and curated through a carefully designed multi-step pipeline,\nMIRAGE spans diverse crop health, pest diagnosis, and crop management\nscenarios. The benchmark includes more than 7,000 unique biological entities,\ncovering plant species, pests, and diseases, making it one of the most\ntaxonomically diverse benchmarks available for vision-language models, grounded\nin the real world. Unlike existing benchmarks that rely on well-specified user\ninputs and closed-set taxonomies, MIRAGE features underspecified, context-rich\nscenarios with open-world settings, requiring models to infer latent knowledge\ngaps, handle rare entities, and either proactively guide the interaction or\nrespond. Project Page: https://mirage-benchmark.github.io", "comment": "66 pages, 32 figures, 23 tables", "pdf_url": "http://arxiv.org/pdf/2506.20100v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20100v1", "AI": {"title_translation": "MIRAGE：农业专家指导对话中多模态信息检索与推理的基准", "tldr": "MIRAGE是一个针对农业领域专家咨询的多模态基准，旨在评估模型在真实世界知识密集型场景中的推理、澄清和长文本生成能力。", "motivation": "现有基准依赖明确的用户输入和封闭集分类，无法捕捉真实世界专家咨询的复杂性，特别是欠明确、上下文丰富和开放世界场景。因此，需要一个能评估模型处理潜在知识差距、稀有实体以及主动引导或响应能力的基准。", "method": "引入MIRAGE，一个基于超过35,000次真实用户-专家互动，并通过精心设计的多步骤流程策划而成的基准。它结合了自然用户查询、专家撰写回复和基于图像的上下文，涵盖了作物健康、病虫害诊断和作物管理等多种场景。包含7,000多种独特的生物实体。", "result": "MIRAGE提供了一个高保真基准，用于评估模型在真实世界、知识密集型领域中的接地推理、澄清策略和长文本生成能力。它是一个分类学上最多样化的视觉-语言模型基准之一。", "conclusion": "MIRAGE是一个独特且重要的多模态基准，它通过模拟真实世界农业专家咨询的复杂性，弥补了现有基准的不足，为评估和开发更鲁棒的视觉-语言模型提供了基础。", "translation": "我们引入MIRAGE，这是一个用于咨询互动环境中多模态专家级推理和决策的新基准。MIRAGE专为农业领域设计，通过结合自然用户查询、专家撰写回复和基于图像的上下文，捕捉了专家咨询的全部复杂性，为评估模型在真实世界、知识密集型领域中的接地推理、澄清策略和长文本生成能力提供了一个高保真基准。MIRAGE基于超过35,000次真实用户-专家互动，并通过精心设计的多步骤流程精心策划，涵盖了作物健康、病虫害诊断和作物管理等多种场景。该基准包括7,000多种独特的生物实体，涵盖植物种类、害虫和疾病，使其成为现有视觉-语言模型中分类学最多样化、基于真实世界的基准之一。与依赖明确用户输入和封闭集分类的现有基准不同，MIRAGE具有欠明确、上下文丰富且开放世界的场景，要求模型推断潜在的知识差距、处理稀有实体，并主动引导互动或响应。项目页面：https://mirage-benchmark.github.io", "summary": "MIRAGE是一个为农业领域专家咨询设计的新型多模态基准，旨在评估模型在真实、复杂、知识密集型环境中的推理、澄清和长文本生成能力。它基于超过35,000次真实用户-专家互动，包含了自然查询、专家回复和图像上下文，涵盖多种农业场景和7,000多种生物实体，克服了现有基准在处理欠明确、开放世界和稀有实体方面的不足。", "keywords": "多模态基准, 农业咨询, 视觉-语言模型, 专家推理, 真实世界数据", "comments": "MIRAGE的创新之处在于其对真实世界农业专家咨询复杂性的高度模拟，特别是引入了“欠明确、上下文丰富”的开放世界场景，这对于评估模型在不完美信息下的推理和主动交互能力至关重要。其庞大的真实世界数据量和多样化的生物实体使其成为视觉-语言领域的一个重要贡献，有望推动更实用、更智能的AI模型发展。"}}
{"id": "2506.20476", "title": "Knowledge-Aware Diverse Reranking for Cross-Source Question Answering", "authors": ["Tong Zhou"], "summary": "This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG\ncompetition. The competition's evaluation set, automatically generated by\nDataMorgana from internet corpora, encompassed a wide range of target topics,\nquestion types, question formulations, audience types, and knowledge\norganization methods. It offered a fair evaluation of retrieving\nquestion-relevant supporting documents from a 15M documents subset of the\nFineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline\nachieved first place in the competition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20476v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20476v1", "AI": {"title_translation": "知识感知多样化重排序用于跨源问答", "tldr": "本文介绍了Team Marikarp在SIGIR 2025 LiveRAG竞赛中的解决方案，提出了一种知识感知多样化重排序的RAG管道，并获得了竞赛第一名。", "motivation": "解决SIGIR 2025 LiveRAG竞赛中，从大规模文档集中检索问题相关支持文档的挑战，该竞赛的评估集由DataMorgana自动生成，涵盖了广泛的目标主题、问题类型、问题表述、受众类型和知识组织方法。", "method": "提出了一种知识感知多样化重排序（knowledge-aware diverse reranking）的RAG（Retrieval Augmented Generation）管道。", "result": "该知识感知多样化重排序RAG管道在SIGIR 2025 LiveRAG竞赛中获得了第一名。", "conclusion": "该知识感知多样化重排序RAG管道在处理复杂和多样化问答数据集方面表现出色，并在激烈的竞争中取得了领先地位。", "translation": "本文介绍了Marikarp团队为SIGIR 2025 LiveRAG竞赛提供的解决方案。竞赛的评估集由DataMorgana从互联网语料库自动生成，涵盖了广泛的目标主题、问题类型、问题表述、受众类型和知识组织方法。它为从FineWeb语料库的15M文档子集中检索与问题相关的支持文档提供了公平的评估。我们提出的知识感知多样化重排序RAG管道在竞赛中获得了第一名。", "summary": "本文介绍了Team Marikarp为SIGIR 2025 LiveRAG竞赛开发的解决方案，该方案采用了一种知识感知的多样化重排序RAG管道，旨在有效处理竞赛中多样化的评估集，并成功地从大规模文档集中检索相关信息，最终在竞赛中取得第一名。", "keywords": "知识感知, 多样化重排序, 问答系统, RAG, 竞赛", "comments": "该论文展示了一个在竞争激烈的问答竞赛中取得优异成绩的实际应用系统。其创新点在于采用了“知识感知多样化重排序”的RAG管道，这可能有效地解决了跨源问答中信息多样性和检索准确性的挑战。论文的价值在于提供了一个在真实世界大规模语料库上表现卓越的解决方案。"}}
{"id": "2506.20350", "title": "Solver Performance of Accelerated MoM for Connected Arrays", "authors": ["Harald Hultin", "Lucas Åkerstedt", "B. L. G. Jonsson"], "summary": "Simulating and developing large rectangularly shaped arrays with equidistant\ninterspacing is challenging as the computational complexity grows quickly with\narray size. However, the geometrical shape of the array, appropriately meshed,\nleads to a multilevel Toeplitz structure in the RWG-based Method of Moment\nimpedance matrix representation that can be used to mitigate the increased\ncomplexity. This paper develops, presents and compares two different\naccelerated solvers that both utilize the matrix structure to determine antenna\nproperties. Both methods use a novel mesh-partitioning algorithm and its\nassociated data representation, reducing storage and computational costs. The\nfirst solver is an iterative method based on multilevel fast Fourier transform\nto accelerate matrix multiplications. The second solver approach is based on an\nextension of a fast direct Toeplitz solver, adapted to a block-matrix\nstructure. This fast direct solver is demonstrated to have close to machine\nepsilon accuracy. Both accelerated methods are evaluated on two different array\nelement types, for arrays with up to 900 elements. The results are compared\nwith conventional direct and iterative matrix solvers. Improvements are seen in\nboth the time and required storage to solve the problem. The choice of the most\nefficient method depends on the residual thresholds in the iterative method,\ngeometry of the element and frequency. Two different preconditioners for the\niterative method are investigated to evaluate their performance. The two\naccelerated methods vastly outperform regular matrix inversion methods.", "comment": "11 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.20350v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20350v1", "AI": {"title_translation": "连接阵列加速MoM求解器性能", "tldr": "本文提出了两种加速求解器（基于FFT的迭代法和快速直接Toeplitz法），用于大型连接阵列的矩量法阻抗矩阵，与传统方法相比，显著提高了计算速度和存储效率。", "motivation": "模拟和开发大型等间距矩形阵列具有挑战性，因为计算复杂度随阵列尺寸的增大而迅速增加。", "method": "本文开发并比较了两种利用矩量法阻抗矩阵多级Toeplitz结构的加速求解器。这两种方法都采用了一种新颖的网格划分算法。第一个求解器是基于多级快速傅里叶变换的迭代方法；第二个求解器是快速直接Toeplitz求解器的扩展，适用于块矩阵结构。还研究了两种不同的迭代方法预处理器。", "result": "两种加速方法在解决问题所需的时间和存储方面均优于传统的直接和迭代矩阵求解器。快速直接求解器达到了接近机器epsilon的精度。最有效方法的选择取决于迭代方法中的残差阈值、单元几何形状和频率。", "conclusion": "这两种加速方法在大型连接阵列的模拟中大大优于常规矩阵求逆方法，提供了显著的计算和存储优势。", "translation": "模拟和开发大型等间距矩形阵列具有挑战性，因为计算复杂度随阵列尺寸的增大而迅速增加。然而，阵列的几何形状，经过适当网格划分后，在基于RWG的矩量法阻抗矩阵表示中会产生多级Toeplitz结构，可用于减轻增加的复杂性。本文开发、提出并比较了两种不同的加速求解器，它们都利用矩阵结构来确定天线特性。这两种方法都使用了一种新颖的网格划分算法及其相关数据表示，从而降低了存储和计算成本。第一个求解器是一种基于多级快速傅里叶变换的迭代方法，用于加速矩阵乘法。第二个求解器方法基于快速直接Toeplitz求解器的扩展，并适用于块矩阵结构。这种快速直接求解器被证明具有接近机器epsilon的精度。这两种加速方法都在两种不同的阵列单元类型上进行了评估，针对多达900个单元的阵列。结果与传统的直接和迭代矩阵求解器进行了比较。在解决问题所需的时间和存储方面都看到了改进。最有效方法的选择取决于迭代方法中的残差阈值、单元几何形状和频率。研究了两种不同的迭代方法预处理器以评估其性能。这两种加速方法大大优于常规矩阵求逆方法。", "summary": "本文通过利用大型连接阵列矩量法阻抗矩阵固有的多级Toeplitz结构，解决了其高计算复杂度问题。文章介绍并比较了两种新颖的加速求解器：一种是使用多级FFT的迭代方法，另一种是适用于块矩阵的快速直接Toeplitz求解器。两种方法都结合了新的网格划分算法以降低成本。对多达900个单元的阵列进行的评估表明，与传统方法相比，这些加速求解器显著提高了计算时间和存储效率，其中直接求解器实现了高精度。最佳方法的选择取决于具体的参数。", "keywords": "加速MoM, Toeplitz矩阵, 大型阵列, 快速求解器, 计算电磁学", "comments": "本文为大型天线阵列的有效模拟提供了重要贡献，这是电磁学中的一个关键领域。通过利用固有的Toeplitz结构和引入新颖的网格划分，所提出的加速求解器解决了长期存在的计算瓶颈。对迭代和直接加速方法的比较，以及它们在实践中的考量（精度、参数依赖性），为实际应用提供了宝贵的见解。其创新之处在于有效地将问题转化为利用结构化矩阵特性以实现性能提升。"}}
{"id": "2506.20343", "title": "PIMBS: Efficient Body Schema Learning for Musculoskeletal Humanoids with Physics-Informed Neural Networks", "authors": ["Kento Kawaharazuka", "Takahiro Hattori", "Keita Yoneda", "Kei Okada"], "summary": "Musculoskeletal humanoids are robots that closely mimic the human\nmusculoskeletal system, offering various advantages such as variable stiffness\ncontrol, redundancy, and flexibility. However, their body structure is complex,\nand muscle paths often significantly deviate from geometric models. To address\nthis, numerous studies have been conducted to learn body schema, particularly\nthe relationships among joint angles, muscle tension, and muscle length. These\nstudies typically rely solely on data collected from the actual robot, but this\ndata collection process is labor-intensive, and learning becomes difficult when\nthe amount of data is limited. Therefore, in this study, we propose a method\nthat applies the concept of Physics-Informed Neural Networks (PINNs) to the\nlearning of body schema in musculoskeletal humanoids, enabling high-accuracy\nlearning even with a small amount of data. By utilizing not only data obtained\nfrom the actual robot but also the physical laws governing the relationship\nbetween torque and muscle tension under the assumption of correct joint\nstructure, more efficient learning becomes possible. We apply the proposed\nmethod to both simulation and an actual musculoskeletal humanoid and discuss\nits effectiveness and characteristics.", "comment": "Accepted at IEEE Robotics and Automation Letters", "pdf_url": "http://arxiv.org/pdf/2506.20343v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20343v1", "AI": {"title_translation": "PIMBS：基于物理信息神经网络的肌肉骨骼人形机器人高效身体图式学习", "tldr": "PIMBS利用物理信息神经网络，结合少量机器人数据和物理定律，高效学习肌肉骨骼人形机器人的身体图式。", "motivation": "肌肉骨骼人形机器人结构复杂，肌肉路径偏离几何模型。现有身体图式学习方法依赖大量机器人数据，数据收集费力且数据量有限时学习困难。", "method": "本文提出PIMBS方法，将物理信息神经网络（PINNs）应用于肌肉骨骼人形机器人的身体图式学习。该方法结合实际机器人数据和假定正确关节结构下的扭矩与肌肉张力之间的物理定律，实现高效学习。", "result": "该方法在仿真和实际肌肉骨骼人形机器人上均得到应用，并讨论了其有效性和特点。", "conclusion": "通过结合少量数据和物理定律，PIMBS方法能够实现肌肉骨骼人形机器人身体图式的高精度高效学习。", "translation": "肌肉骨骼人形机器人是紧密模仿人类肌肉骨骼系统的机器人，具有可变刚度控制、冗余性和柔韧性等多种优点。然而，它们的身体结构复杂，肌肉路径往往与几何模型显著偏离。为了解决这个问题，已经进行了大量研究来学习身体图式，特别是关节角度、肌肉张力和肌肉长度之间的关系。这些研究通常仅依赖于从实际机器人收集的数据，但数据收集过程劳动密集，且当数据量有限时学习变得困难。因此，在本研究中，我们提出了一种将物理信息神经网络（PINNs）概念应用于肌肉骨骼人形机器人身体图式学习的方法，即使在少量数据的情况下也能实现高精度学习。通过不仅利用从实际机器人获得的数据，还利用在正确关节结构假设下扭矩与肌肉张力之间关系的物理定律，可以实现更高效的学习。我们将所提出的方法应用于仿真和实际肌肉骨骼人形机器人，并讨论了其有效性和特点。", "summary": "本文提出PIMBS方法，旨在解决肌肉骨骼人形机器人身体图式学习中数据量不足的问题。该方法将物理信息神经网络（PINNs）引入，结合实际机器人采集的少量数据与物理定律（扭矩与肌肉张力关系），以实现对关节角度、肌肉张力和肌肉长度之间关系的高效、高精度学习。研究在仿真和实际机器人上验证了PIMBS的有效性。", "keywords": "肌肉骨骼人形机器人, 身体图式学习, 物理信息神经网络, 数据效率, 机器人控制", "comments": "PIMBS的创新在于将物理信息神经网络引入身体图式学习，有效缓解了传统方法对大量实际机器人数据的依赖，提高了数据效率和学习精度，对于复杂生物启发机器人控制具有重要意义。"}}
{"id": "2506.20254", "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement", "authors": ["Kun Yuan", "Tingxuan Chen", "Shi Li", "Joel L. Lavanchy", "Christian Heiliger", "Ege Özsoy", "Yiming Huang", "Long Bai", "Nassir Navab", "Vinkle Srivastav", "Hongliang Ren", "Nicolas Padoy"], "summary": "The complexity and diversity of surgical workflows, driven by heterogeneous\noperating room settings, institutional protocols, and anatomical variability,\npresent a significant challenge in developing generalizable models for\ncross-institutional and cross-procedural surgical understanding. While recent\nsurgical foundation models pretrained on large-scale vision-language data offer\npromising transferability, their zero-shot performance remains constrained by\ndomain shifts, limiting their utility in unseen surgical environments. To\naddress this, we introduce Surgical Phase Anywhere (SPA), a lightweight\nframework for versatile surgical workflow understanding that adapts foundation\nmodels to institutional settings with minimal annotation. SPA leverages\nfew-shot spatial adaptation to align multi-modal embeddings with\ninstitution-specific surgical scenes and phases. It also ensures temporal\nconsistency through diffusion modeling, which encodes task-graph priors derived\nfrom institutional procedure protocols. Finally, SPA employs dynamic test-time\nadaptation, exploiting the mutual agreement between multi-modal phase\nprediction streams to adapt the model to a given test video in a\nself-supervised manner, enhancing the reliability under test-time distribution\nshifts. SPA is a lightweight adaptation framework, allowing hospitals to\nrapidly customize phase recognition models by defining phases in natural\nlanguage text, annotating a few images with the phase labels, and providing a\ntask graph defining phase transitions. The experimental results show that the\nSPA framework achieves state-of-the-art performance in few-shot surgical phase\nrecognition across multiple institutions and procedures, even outperforming\nfull-shot models with 32-shot labeled data. Code is available at\nhttps://github.com/CAMMA-public/SPA", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20254v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20254v1", "AI": {"title_translation": "在任何地方识别手术阶段：小样本测试时适应和任务图引导细化", "tldr": "本文介绍了SPA，一个轻量级框架，通过小样本测试时适应和任务图引导细化，使手术基础模型能够适应新的机构设置，只需最少的标注，并在小样本手术阶段识别中实现了最先进的性能。", "motivation": "手术工作流程的复杂性和多样性，受异构手术室设置、机构规程和解剖变异性的影响，对开发用于跨机构和跨程序手术理解的通用模型提出了重大挑战。尽管最近在大规模视觉语言数据上预训练的手术基础模型提供了有希望的可迁移性，但它们的零样本性能仍受领域偏移的限制，从而限制了它们在未见手术环境中的实用性。", "method": "本文介绍了Surgical Phase Anywhere (SPA)，一个轻量级框架，用于多功能手术工作流程理解，通过最少的标注将基础模型适应于机构设置。SPA利用小样本空间适应来对齐多模态嵌入与机构特定的手术场景和阶段。它还通过扩散建模确保时间一致性，该模型编码了从机构程序协议中导出的任务图先验。最后，SPA采用动态测试时适应，利用多模态阶段预测流之间的相互一致性，以自监督的方式将模型适应于给定的测试视频，从而增强了测试时分布偏移下的可靠性。SPA是一个轻量级的适应框架，允许医院通过自然语言文本定义阶段、用阶段标签标注少量图像以及提供定义阶段转换的任务图来快速定制阶段识别模型。", "result": "实验结果表明，SPA框架在跨多个机构和程序的小样本手术阶段识别中实现了最先进的性能，甚至超过了使用32样本标记数据的全样本模型。", "conclusion": "SPA为在不同环境中进行手术阶段识别提供了一个高效且适应性强的解决方案，只需最少的数据即可实现快速定制。", "translation": "手术工作流程的复杂性和多样性，受异构手术室设置、机构规程和解剖变异性的影响，对开发用于跨机构和跨程序手术理解的通用模型提出了重大挑战。尽管最近在大规模视觉语言数据上预训练的手术基础模型提供了有希望的可迁移性，但它们的零样本性能仍受领域偏移的限制，从而限制了它们在未见手术环境中的实用性。为了解决这个问题，我们引入了Surgical Phase Anywhere (SPA)，一个轻量级框架，用于多功能手术工作流程理解，通过最少的标注将基础模型适应于机构设置。SPA利用小样本空间适应来对齐多模态嵌入与机构特定的手术场景和阶段。它还通过扩散建模确保时间一致性，该模型编码了从机构程序协议中导出的任务图先验。最后，SPA采用动态测试时适应，利用多模态阶段预测流之间的相互一致性，以自监督的方式将模型适应于给定的测试视频，从而增强了测试时分布偏移下的可靠性。SPA是一个轻量级的适应框架，允许医院通过自然语言文本定义阶段、用阶段标签标注少量图像以及提供定义阶段转换的任务图来快速定制阶段识别模型。实验结果表明，SPA框架在跨多个机构和程序的小样本手术阶段识别中实现了最先进的性能，甚至超过了使用32样本标记数据的全样本模型。代码可在https://github.com/CAMMA-public/SPA获得。", "summary": "SPA是一个轻量级框架，旨在克服手术阶段识别中的领域偏移挑战。它通过小样本空间适应、利用任务图先验的扩散模型实现时间一致性以及动态测试时适应，将预训练的基础模型适应于新的机构设置。这种方法允许以最少的数据进行快速定制，并实现了最先进的小样本性能，甚至超越了全样本模型。", "keywords": "手术阶段识别, 小样本学习, 测试时适应, 基础模型, 任务图", "comments": "本文解决了手术AI中一个关键的实际挑战：在不同临床环境中的泛化能力。其创新之处在于其多管齐下的适应策略（小样本学习、通过任务图实现时间一致性以及自监督测试时适应），这使得基础模型能够以最少的标注工作真正地部署到“任何地方”。它能够以显著更少的数据超越全样本模型，突显了其效率和实用价值。"}}
{"id": "2506.20401", "title": "Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation", "authors": ["Jinchun Du", "Bojie Shen", "Muhammad Aamir Cheema", "Adel N. Toosi"], "summary": "With the rising popularity of electric vehicles (EVs), modern service\nsystems, such as ride-hailing delivery services, are increasingly integrating\nEVs into their operations. Unlike conventional vehicles, EVs often have a\nshorter driving range, necessitating careful consideration of charging when\nfulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -\nallowing EVs to also discharge energy back to the grid - new opportunities and\ncomplexities emerge. We introduce the Electric Vehicle Orienteering Problem\nwith V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select\ncustomer requests or orders while managing when and where to charge or\ndischarge. This involves navigating dynamic electricity prices, charging\nstation selection, and route constraints. We formulate the problem as a Mixed\nInteger Programming (MIP) model and propose two near-optimal metaheuristic\nalgorithms: one evolutionary (EA) and the other based on large neighborhood\nsearch (LNS). Experiments on real-world data show our methods can double driver\nprofits compared to baselines, while maintaining near-optimal performance on\nsmall instances and excellent scalability on larger ones. Our work highlights a\npromising path toward smarter, more profitable EV-based mobility systems that\nactively support the energy grid.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20401v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20401v1", "AI": {"title_translation": "电动汽车智能出行与配送服务：利用双向充电优化利润", "tldr": "该研究提出了电动汽车路径优化问题与车网互动（EVOP-V2G），通过混合整数规划模型和元启发式算法，利用双向充电为电动汽车司机带来更高的利润，并支持电网。", "motivation": "随着电动汽车的普及，现代出行和配送服务越来越多地整合电动汽车。然而，电动汽车续航里程短，充电需谨慎。车网互动（V2G）技术允许电动汽车向电网放电，带来了新的机遇和复杂性。本研究的动机是解决如何利用V2G技术在管理充电和放电的同时，最大化电动汽车司机的利润。", "method": "本研究提出了电动汽车路径优化问题与车网互动（EVOP-V2G），这是一个利润最大化问题，要求电动汽车司机在管理充放电时间地点、动态电价、充电站选择和路线限制的同时，选择客户请求或订单。问题被建模为混合整数规划（MIP）模型，并提出了两种近似最优的元启发式算法：一种是演化算法（EA），另一种是基于大邻域搜索（LNS）的算法。", "result": "在真实世界数据上的实验表明，与基线相比，所提出的方法可以将司机利润翻倍。在小规模实例上保持了接近最优的性能，在更大规模实例上表现出色的可扩展性。", "conclusion": "本研究揭示了一条通向更智能、更有利可图的电动汽车出行系统的有前景的道路，这些系统能够积极支持能源电网。通过利用V2G技术，电动汽车服务不仅能优化自身利润，还能为电网做出贡献。", "translation": "随着电动汽车（EV）的日益普及，现代服务系统，如网约车和配送服务，正越来越多地将电动汽车整合到其运营中。与传统车辆不同，电动汽车通常续航里程较短，在满足请求时需要仔细考虑充电问题。随着车网互动（V2G）技术的最新进展——允许电动汽车将能量回馈到电网——新的机遇和复杂性随之出现。我们引入了带V2G的电动汽车路径优化问题（EVOP-V2G）：这是一个利润最大化问题，电动汽车司机必须在管理何时何地充电或放电的同时选择客户请求或订单。这涉及到动态电价、充电站选择和路线约束。我们将问题建模为混合整数规划（MIP）模型，并提出了两种近似最优的元启发式算法：一种是演化算法（EA），另一种是基于大邻域搜索（LNS）的算法。在真实世界数据上的实验表明，我们的方法与基线相比可以将司机利润翻倍，同时在小规模实例上保持接近最优的性能，并在更大规模实例上表现出色的可扩展性。我们的工作突出了一条通向更智能、更有利可图的电动汽车出行系统的有前景的道路，这些系统积极支持能源电网。", "summary": "本研究针对电动汽车在网约车和配送服务中面临的续航和充电挑战，提出了电动汽车路径优化问题与车网互动（EVOP-V2G）。该问题旨在通过利用双向充电（V2G）技术，在考虑动态电价、充电站和路线约束的情况下，最大化电动汽车司机的利润。研究将该问题建模为混合整数规划，并开发了演化算法和大邻域搜索两种元启发式算法进行求解。实验结果显示，与现有方法相比，所提出的方案能使司机利润翻倍，并在不同规模实例上展现出良好的性能和可扩展性，为构建更智能、盈利性更强的电动汽车出行系统提供了新途径。", "keywords": "电动汽车, V2G, 利润优化, 路径优化, 元启发式算法", "comments": "本文的创新点在于将V2G技术引入到电动汽车出行和配送服务的利润优化问题中，并提出了EVOP-V2G这一新问题。通过考虑动态电价和双向充电，为电动汽车司机提供了显著的利润提升潜力。所提出的MIP模型和两种元启发式算法在实际数据上表现出色，证明了其有效性和可扩展性。这对于推动电动汽车在商业服务中的应用，并使其成为电网的积极参与者具有重要意义。"}}
{"id": "2506.20101", "title": "Secure Multi-Key Homomorphic Encryption with Application to Privacy-Preserving Federated Learning", "authors": ["Jiahui Wu", "Tiecheng Sun", "Fucai Luo", "Haiyan Wang", "Weizhe Zhang"], "summary": "Multi-Key Homomorphic Encryption (MKHE), proposed by Lopez-Alt et al. (STOC\n2012), allows for performing arithmetic computations directly on ciphertexts\nencrypted under distinct keys. Subsequent works by Chen and Dai et al. (CCS\n2019) and Kim and Song et al. (CCS 2023) extended this concept by proposing\nmulti-key BFV/CKKS variants, referred to as the CDKS scheme. These variants\nincorporate asymptotically optimal techniques to facilitate secure computation\nacross multiple data providers. In this paper, we identify a critical security\nvulnerability in the CDKS scheme when applied to multiparty secure computation\ntasks, such as privacy-preserving federated learning (PPFL). In particular, we\nshow that CDKS may inadvertently leak plaintext information from one party to\nothers. To mitigate this issue, we propose a new scheme, SMHE (Secure Multi-Key\nHomomorphic Encryption), which incorporates a novel masking mechanism into the\nmulti-key BFV and CKKS frameworks to ensure that plaintexts remain confidential\nthroughout the computation. We implement a PPFL application using SMHE and\ndemonstrate that it provides significantly improved security with only a modest\noverhead in homomorphic evaluation. For instance, our PPFL model based on\nmulti-key CKKS incurs less than a 2\\times runtime and communication traffic\nincrease compared to the CDKS-based PPFL model. The code is publicly available\nat https://github.com/JiahuiWu2022/SMHE.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20101v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20101v1", "AI": {"title_translation": "安全多密钥同态加密及其在隐私保护联邦学习中的应用", "tldr": "本文发现现有CDKS多密钥同态加密方案在隐私保护联邦学习中存在明文泄露漏洞，并提出了一种新的安全多密钥同态加密方案SMHE，通过引入掩码机制解决了该问题，实现了更高的安全性且开销适中。", "motivation": "现有的CDKS多密钥同态加密方案在应用于多方安全计算任务（如隐私保护联邦学习）时，存在严重的安全性漏洞，可能无意中将一方的明文信息泄露给其他方。", "method": "本文提出了一种新的安全多密钥同态加密（SMHE）方案。该方案在多密钥BFV和CKKS框架中引入了一种新颖的掩码机制，以确保在整个计算过程中明文信息的保密性。此外，作者还实现了基于SMHE的隐私保护联邦学习应用。", "result": "SMHE方案在隐私保护联邦学习应用中提供了显著增强的安全性，并且只带来了适度的同态评估开销。例如，基于多密钥CKKS的PPFL模型相比基于CDKS的PPFL模型，运行时和通信流量增加不到2倍。", "conclusion": "本文成功识别了CDKS多密钥同态加密方案在多方安全计算中的安全漏洞，并提出了一种名为SMHE的新方案，通过引入新颖的掩码机制有效解决了明文泄露问题，实现了在隐私保护联邦学习中更高的安全性且开销可接受。", "translation": "多密钥同态加密（MKHE）由Lopez-Alt等人（STOC 2012）提出，允许直接对不同密钥加密的密文执行算术计算。Chen和Dai等人（CCS 2019）以及Kim和Song等人（CCS 2023）的后续工作通过提出多密钥BFV/CKKS变体（称为CDKS方案）扩展了这一概念。这些变体结合了渐近最优技术，以促进跨多个数据提供者的安全计算。在本文中，我们发现CDKS方案在应用于多方安全计算任务（如隐私保护联邦学习（PPFL））时存在一个关键的安全漏洞。特别是，我们表明CDKS可能会无意中将一方的明文信息泄露给其他方。为了缓解这个问题，我们提出了一种新方案SMHE（安全多密钥同态加密），它在多密钥BFV和CKKS框架中引入了一种新颖的掩码机制，以确保在整个计算过程中明文信息的保密性。我们使用SMHE实现了一个PPFL应用程序，并证明它提供了显著改进的安全性，而同态评估开销仅适度增加。例如，我们基于多密钥CKKS的PPFL模型与基于CDKS的PPFL模型相比，运行时和通信流量增加不到2倍。代码已公开在https://github.com/JiahuiWu2022/SMHE.git。", "summary": "本文指出了现有CDKS多密钥同态加密方案在隐私保护联邦学习等应用中存在的明文泄露安全漏洞。为解决此问题，作者提出了一种新的安全多密钥同态加密（SMHE）方案，通过引入独特的掩码机制来确保计算过程中的明文机密性。实验证明，SMHE在提供显著安全性提升的同时，仅带来适度的性能开销，例如在PPFL应用中运行时和通信量增幅低于2倍。", "keywords": "多密钥同态加密, 隐私保护联邦学习, 安全漏洞, 加密方案, 掩码机制", "comments": "该论文的关键创新在于识别并解决了现有CDKS多密钥同态加密方案中的一个严重安全漏洞，这对于隐私保护联邦学习等敏感应用至关重要。通过引入新颖的掩码机制，SMHE方案在不牺牲过多性能的情况下显著提升了安全性，这对于实际应用具有重要意义。该工作为多方安全计算领域提供了更可靠的加密工具。"}}
{"id": "2506.20096", "title": "First experimental demonstration of plasma shape control in a tokamak through Model Predictive Control", "authors": ["Adriano Mele", "Maria A. Topalova", "Cristian Galperti", "Stefano Coda", "TCV team", "Eurofusion Tokamak Exploitation Team"], "summary": "In this work, a Model Predictive Controller (MPC) is proposed to control the\nplasma shape in the Tokamak \\`a Configuration Variable (TCV). The proposed\ncontroller relies on models obtained by coupling linearized plasma response\nmodels, derived from the \\texttt{fge} code of the Matlab EQuilibrium toolbox\n(MEQ) suite, with a state-space description of the core TCV magnetic control\nsystem. It optimizes the reference signals fed to this inner control loop in\norder to achieve the desired plasma shape while also enforcing constraints on\nthe plant outputs. To this end, a suitable Quadratic Programming (QP) problem\nis formulated and solved in real-time. The effectiveness of the proposed\ncontroller is illustrated through a combination of simulations and experimental\nresults. To the best of our knowledge, this is the first time that a plasma\nshape control solution based on MPC has been experimentally tested on a real\ntokamak.", "comment": "6 pages, accepted for CCTA2025", "pdf_url": "http://arxiv.org/pdf/2506.20096v1", "categories": ["physics.plasm-ph", "cs.SY", "eess.SY"], "cate": "physics.plasm-ph", "url": "http://arxiv.org/abs/2506.20096v1", "AI": {"title_translation": "首次在托卡马克中通过模型预测控制实现等离子体形状控制的实验验证", "tldr": "首次在真实的托卡马克装置上实验性地验证了基于模型预测控制的等离子体形状控制方案。", "motivation": "控制托卡马克中的等离子体形状，同时对系统输出施加约束，以实现期望的等离子体形状。", "method": "提出了一种模型预测控制器（MPC），该控制器结合了线性化等离子体响应模型（来源于Matlab EQuilibrium工具箱的fge代码）与核心TCV磁控制系统的状态空间描述。通过实时解决二次规划（QP）问题来优化内部控制回路的参考信号。", "result": "通过仿真和实验结果验证了所提出的控制器的有效性。这是首次在真实的托卡马克装置上实验性地测试基于MPC的等离子体形状控制解决方案。", "conclusion": "成功地在真实的托卡马克装置（TCV）上首次通过实验证明了基于模型预测控制的等离子体形状控制的可行性和有效性。", "translation": "在这项工作中，提出了一种模型预测控制器（MPC）来控制可变配置托卡马克（TCV）中的等离子体形状。所提出的控制器依赖于通过耦合线性化等离子体响应模型（来源于Matlab EQuilibrium工具箱（MEQ）套件的fge代码）与核心TCV磁控制系统的状态空间描述而获得的模型。它优化了输入到该内部控制回路的参考信号，以实现所需的等离子体形状，同时强制执行对设备输出的约束。为此，实时地制定并求解了一个合适的二次规划（QP）问题。通过仿真和实验结果的结合，证明了所提出的控制器的有效性。据我们所知，这是首次在真实的托卡马克装置上实验性地测试基于MPC的等离子体形状控制解决方案。", "summary": "本文提出并实验验证了一种基于模型预测控制（MPC）的托卡马克等离子体形状控制方案。该控制器结合了线性化等离子体响应模型和TCV磁控制系统模型，通过实时求解二次规划问题来优化控制信号，以实现期望的等离子体形状并满足系统约束。研究通过仿真和真实的TCV托卡马克实验证明了其有效性，标志着MPC首次成功应用于真实托卡马克等离子体形状控制的实验性尝试。", "keywords": "模型预测控制, 等离子体形状控制, 托卡马克, TCV, 二次规划", "comments": "这项工作的重要性在于，它是首次在真实的托卡马克装置上实验性地验证了基于模型预测控制的等离子体形状控制，这为核聚变研究中的等离子体控制提供了新的、先进的解决方案。其创新性在于将MPC与托卡马克等离子体控制的复杂性相结合，并通过实时二次规划解决了实际操作中的约束问题。"}}
{"id": "2506.20287", "title": "Analog OFDM based on Real-Time Fourier Transformation", "authors": ["Xiaolu Yang", "Oscar Céspedes Vicente", "Christophe Caloz"], "summary": "This paper proposes an analog orthogonal frequency division multiplexing\n(OFDM) architecture based on the real-time Fourier transform (RTFT). The core\nenabling component is a linear-chirp phaser with engineered group velocity\ndispersion (GVD), which realizes RTFT and performs frequency-to-time mapping in\nthe analog domain. In this architecture, conventional digital fast Fourier\ntransform (FFT) and inverse FFT (IFFT) processors are replaced by two\nlinear-chirp phasers with opposite group delay dispersions, respectively.\nTheoretical analysis demonstrates that, under specific phaser conditions, the\nOFDM signal generated by the RTFT-based analog system is mathematically\nequivalent to that of a conventional digital OFDM system. This equivalence is\nfurther supported by simulation results, which confirm accurate symbol\ntransmission and recovery, as well as robustness to multipath fading when a\nprefix is applied. Benefiting from the use of passive microwave components, the\nanalog OFDM system offers ultra-fast processing with reduced power consumption.\nOverall, this work establishes a foundation for fully analog or hybrid\nanalog-digital OFDM system, offering a promising solution for next-generation\nhigh-speed, wideband, and energy-efficient wireless communication platforms.", "comment": "16 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.20287v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20287v1", "AI": {"title_translation": "基于实时傅里叶变换的模拟正交频分复用", "tldr": "本文提出了一种基于实时傅里叶变换（RTFT）的模拟正交频分复用（OFDM）架构，利用线性啁啾移相器替代传统的数字FFT/IFFT处理器，实现了超高速、低功耗的OFDM信号处理，并证明了其与数字OFDM系统的数学等效性。", "motivation": "为了实现下一代高速、宽带和高能效的无线通信平台，本文旨在开发一种具有超快处理速度和更低功耗的模拟OFDM系统，以克服传统数字OFDM系统的局限性。", "method": "本文提出了一种基于实时傅里叶变换（RTFT）的模拟OFDM架构。核心组件是具有工程化群速度色散（GVD）的线性啁啾移相器，它实现了RTFT并在模拟域中进行频率到时间的映射。在该架构中，传统的数字快速傅里叶变换（FFT）和逆快速傅里叶变换（IFFT）处理器被两个具有相反群延迟色散的线性啁啾移相器取代。", "result": "理论分析表明，在特定移相器条件下，基于RTFT的模拟系统生成的OFDM信号与传统数字OFDM系统在数学上是等效的。仿真结果进一步证实了精确的符号传输和恢复，以及在应用前缀时对多径衰落的鲁棒性。", "conclusion": "这项工作为全模拟或混合模拟-数字OFDM系统奠定了基础，为下一代高速、宽带和高能效无线通信平台提供了一个有前景的解决方案。该模拟OFDM系统利用无源微波组件，实现了超快处理和更低的功耗。", "translation": "本文提出了一种基于实时傅里叶变换（RTFT）的模拟正交频分复用（OFDM）架构。其核心使能组件是具有工程化群速度色散（GVD）的线性啁啾移相器，它实现了RTFT并在模拟域中执行频率到时间的映射。在该架构中，传统的数字快速傅里叶变换（FFT）和逆快速傅里叶变换（IFFT）处理器分别被两个具有相反群延迟色散的线性啁啾移相器取代。理论分析表明，在特定移相器条件下，由基于RTFT的模拟系统生成的OFDM信号与传统数字OFDM系统在数学上是等效的。仿真结果进一步支持了这种等效性，证实了精确的符号传输和恢复，以及在应用前缀时对多径衰落的鲁棒性。受益于无源微波组件的使用，该模拟OFDM系统提供了超快处理速度和更低的功耗。总的来说，这项工作为全模拟或混合模拟-数字OFDM系统奠定了基础，为下一代高速、宽带和高能效无线通信平台提供了一个有前景的解决方案。", "summary": "本文提出了一种基于实时傅里叶变换（RTFT）的模拟OFDM架构，用具有工程化群速度色散的线性啁啾移相器取代了传统的数字FFT/IFFT处理器。理论分析和仿真结果证明，该模拟系统生成的OFDM信号与数字OFDM系统在数学上等效，并具有准确的符号传输和对多径衰落的鲁棒性。该系统利用无源微波组件，实现了超快处理和低功耗，为下一代高速、宽带、高能效无线通信平台提供了潜在解决方案。", "keywords": "模拟OFDM, 实时傅里叶变换, 线性啁啾移相器, 群速度色散, 无线通信", "comments": "本文的创新之处在于提出了一种完全在模拟域实现OFDM信号处理的新方法，通过利用线性啁啾移相器实现实时傅里叶变换，避免了传统数字FFT/IFFT的复杂性和功耗。其重要性在于为未来高速、低功耗的无线通信系统提供了新的思路，特别是在需要超快处理速度的应用场景中具有巨大潜力。通过使用无源微波组件，显著降低了功耗，这对于能效至关重要的通信平台而言是一个显著优势。"}}
{"id": "2506.20112", "title": "A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection", "authors": ["Songsoo Kim", "Seungtae Lee", "See Young Lee", "Joonho Kim", "Keechan Kan", "Dukyong Yoon"], "summary": "Background: The positive predictive value (PPV) of large language model\n(LLM)-based proofreading for radiology reports is limited due to the low error\nprevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV\nand reduces operational costs compared with baseline approaches. Materials and\nMethods: A retrospective analysis was performed on 1,000 consecutive radiology\nreports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III\ndatabase. Two external datasets (CheXpert and Open-i) were validation sets.\nThree LLM frameworks were tested: (1) single-prompt detector; (2) extractor\nplus detector; and (3) extractor, detector, and false-positive verifier.\nPrecision was measured by PPV and absolute true positive rate (aTPR).\nEfficiency was calculated from model inference charges and reviewer\nremuneration. Statistical significance was tested using cluster bootstrap,\nexact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV\nincreased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,\nFramework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.\nbaselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per\n1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and\nUSD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.\nHuman-reviewed reports decreased from 192 to 88. External validation supported\nFramework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR\n(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and\nreduced operational costs, maintaining detection performance, providing an\neffective strategy for AI-assisted radiology report quality assurance.", "comment": "29 pages, 5 figures, 4 tables. Code available at\n  https://github.com/radssk/mp-rred", "pdf_url": "http://arxiv.org/pdf/2506.20112v1", "categories": ["cs.CL", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20112v1", "AI": {"title_translation": "一种用于精确高效放射学报告错误检测的多通道大型语言模型框架", "tldr": "一个三通道LLM框架显著提高了放射学报告错误检测的阳性预测值并降低了运营成本，同时保持了检测性能。", "motivation": "由于错误发生率低，基于大型语言模型（LLM）的放射学报告校对的阳性预测值（PPV）有限，因此需要评估一个多通道LLM框架是否能提高PPV并降低运营成本。", "method": "本研究对MIMIC-III数据库中1,000份连续放射学报告（放射线照相、超声、CT、MRI各250份）进行了回顾性分析，并使用CheXpert和Open-i两个外部数据集进行验证。测试了三种LLM框架：(1) 单提示检测器；(2) 提取器加检测器；(3) 提取器、检测器和假阳性验证器。通过PPV和绝对真阳性率（aTPR）衡量精确度，并根据模型推理费用和审阅者报酬计算效率。使用聚类自助法、精确McNemar检验和Holm-Bonferroni校正进行统计显著性检验。", "result": "框架的PPV从框架1的0.063显著提高到框架3的0.159（P<.001）。aTPR保持稳定（0.012-0.014；P>=.84）。每1,000份报告的运营成本从框架1的9.72美元降至框架3的5.58美元，分别降低了42.6%和18.5%。人工审阅报告数量从192份减少到88份。外部验证也支持框架3在PPV（CheXpert 0.133, Open-i 0.105）和稳定aTPR（0.007）方面的优越性。", "conclusion": "一个三通道LLM框架显著提高了放射学报告错误检测的阳性预测值并降低了运营成本，同时保持了检测性能，为AI辅助放射学报告质量保证提供了一种有效的策略。", "translation": "背景：由于错误发生率低，基于大型语言模型（LLM）的放射学报告校对的阳性预测值（PPV）有限。目的：评估与基线方法相比，三通道LLM框架是否能提高PPV并降低运营成本。材料与方法：对MIMIC-III数据库中1,000份连续放射学报告（放射线照相、超声、CT、MRI各250份）进行了回顾性分析。两个外部数据集（CheXpert和Open-i）作为验证集。测试了三种LLM框架：(1) 单提示检测器；(2) 提取器加检测器；(3) 提取器、检测器和假阳性验证器。通过PPV和绝对真阳性率（aTPR）衡量精确度。效率根据模型推理费用和审阅者报酬计算。使用聚类自助法、精确McNemar检验和Holm-Bonferroni校正进行统计显著性检验。结果：框架PPV从0.063（95% CI, 0.036-0.101, 框架1）增加到0.079（0.049-0.118, 框架2），并显著增加到0.159（0.090-0.252, 框架3；P<.001 vs. 基线）。aTPR保持稳定（0.012-0.014；P>=.84）。每1,000份报告的运营成本从9.72美元（框架1）降至5.58美元（框架3），以及6.85美元（框架2），分别反映了42.6%和18.5%的降低。人工审阅报告数量从192份减少到88份。外部验证支持框架3在PPV（CheXpert 0.133, Open-i 0.105）和稳定aTPR（0.007）方面的优越性。结论：一个三通道LLM框架显著提高了PPV并降低了运营成本，同时保持了检测性能，为AI辅助放射学报告质量保证提供了一种有效的策略。", "summary": "本研究旨在解决大型语言模型（LLM）在放射学报告校对中阳性预测值（PPV）有限的问题。研究提出并评估了一个三通道LLM框架，并将其与单提示和两通道框架进行比较。结果表明，三通道框架显著提高了PPV（从0.063增至0.159），同时保持了绝对真阳性率（aTPR）的稳定，并显著降低了运营成本（每1,000份报告从9.72美元降至5.58美元）。该框架通过减少人工审阅量和提高效率，为AI辅助放射学报告质量保证提供了有效策略。", "keywords": "大型语言模型, 放射学报告, 错误检测, 阳性预测值, 多通道框架", "comments": "这项研究提出并验证了一个多通道LLM框架，通过引入假阳性验证步骤，有效地提升了放射学报告错误检测的精度（PPV），并显著降低了运营成本，这是其主要创新点。在低错误发生率的医疗文本校对领域，提高PPV至关重要，因为它直接关系到医生对AI辅助工具的信任度和采纳率。该框架的提出为医疗领域LLM应用提供了一个实用的优化方向，具有重要的临床应用潜力。"}}
{"id": "2506.20372", "title": "An adaptive scheme for the optimization of damping positions by decoupling controllability spaces in vibrational systems", "authors": ["Jennifer Przybilla", "Matea Ugrica Vukojević", "Ninolsav Truhar", "Peter Benner"], "summary": "In this work, the problem of optimizing damper positions in vibrational\nsystems is investigated. The objective is to determine the positions of\nexternal dampers in such a way that the influence of the input on the output is\nminimized. The energy response serves as an optimization criterion, whose\ncomputation involves solving Lyapunov equations. Hence, in order to find the\nbest positions, many of these equations need to be solved, and so the\nminimization process can have a high computational cost.\n  To accelerate the process of finding the optimal positions, we propose a new\nreduction method. Our algorithm generates a basis spanning an approximation to\nthe solution space of the Lyapunov equations for all possible positions of the\ndampers. We derive an adaptive scheme that generates the reduced solution space\nby adding the subspaces of interest, and then we define the corresponding\nreduced optimization problem that is solvable in a reasonable amount of time.\nWe decouple the solution spaces of the problem to obtain a space that\ncorresponds to the system without external dampers and serves as a starting\npoint for the reduction of the optimization problem. In addition, we derive\nspaces corresponding to the different damper positions that are used to expand\nthe reduced basis if needed. To evaluate the quality of the basis, we introduce\nan error indicator based on the space decomposition. Our new technique produces\na reduced optimization problem of significantly smaller dimension that is\nfaster to solve than the original problem, which we illustrate with some\nnumerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20372v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20372v1", "AI": {"title_translation": "一种通过解耦振动系统中可控性空间来优化阻尼位置的自适应方案", "tldr": "该论文提出了一种新的降维方法和自适应方案，用于加速振动系统中阻尼器位置优化的计算过程。", "motivation": "在振动系统中优化阻尼器位置时，由于需要求解大量Lyapunov方程，导致计算成本高昂。", "method": "提出了一种新的降维方法，生成一个基来近似所有可能阻尼器位置的Lyapunov方程解空间。该算法采用自适应方案，通过添加感兴趣的子空间来生成约化解空间。它解耦了问题解空间，将没有外部阻尼器的系统空间作为约化优化问题的起点，并导出了对应不同阻尼器位置的空间以在需要时扩展约化基。引入了基于空间分解的误差指标来评估基的质量。", "result": "新方法生成了一个维度显著减小的约化优化问题，比原始问题求解更快。", "conclusion": "通过提出一种新的降维方法和自适应方案，该研究成功地加速了振动系统中阻尼器位置的优化过程，显著降低了计算成本。", "translation": "在这项工作中，研究了振动系统中阻尼位置的优化问题。目标是确定外部阻尼器的位置，以最小化输入对输出的影响。能量响应作为优化准则，其计算涉及求解Lyapunov方程。因此，为了找到最佳位置，需要求解许多这样的方程，这使得最小化过程可能具有高计算成本。\n为了加速寻找最佳位置的过程，我们提出了一种新的降维方法。我们的算法生成一个基，该基跨越了所有可能阻尼器位置的Lyapunov方程解空间的近似。我们推导了一个自适应方案，通过添加感兴趣的子空间来生成约化解空间，然后我们定义了相应的约化优化问题，该问题可以在合理的时间内求解。\n我们解耦了问题的解空间，以获得一个对应于没有外部阻尼器的系统的空间，并作为优化问题约化的起点。此外，我们推导了对应于不同阻尼器位置的空间，这些空间用于在需要时扩展约化基。为了评估基的质量，我们引入了一个基于空间分解的误差指标。我们的新技术生成了一个维度显著减小的约化优化问题，比原始问题求解更快，我们通过一些数值示例进行了说明。", "summary": "本研究旨在解决振动系统中阻尼器位置优化的高计算成本问题。传统方法需要求解大量Lyapunov方程。为此，论文提出了一种新的降维方法和自适应方案。该方案生成一个基，近似所有可能阻尼器位置的Lyapunov方程解空间，并通过解耦解空间来构建约化优化问题。它还引入了基于空间分解的误差指标。数值示例表明，该技术能产生维度显著减小的约化优化问题，从而加速求解过程。", "keywords": "阻尼位置优化, 振动系统, 降维, Lyapunov方程, 自适应方案", "comments": "该论文的创新点在于提出了一个自适应的降维方案来解决振动系统阻尼位置优化中的计算效率问题。通过解耦可控性空间并构建约化模型，显著降低了计算复杂性。这种方法对于需要大量迭代优化的工程问题具有重要意义，可能为实时或大规模系统优化提供新的途径。"}}
{"id": "2506.20373", "title": "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition", "authors": ["Joerg Deigmoeller", "Stephan Hasler", "Nakul Agarwal", "Daniel Tanneberg", "Anna Belardinelli", "Reza Ghoddoosian", "Chao Wang", "Felix Ocker", "Fan Zhang", "Behzad Dariush", "Michael Gienger"], "summary": "We introduce CARMA, a system for situational grounding in human-robot group\ninteractions. Effective collaboration in such group settings requires\nsituational awareness based on a consistent representation of present persons\nand objects coupled with an episodic abstraction of events regarding actors and\nmanipulated objects. This calls for a clear and consistent assignment of\ninstances, ensuring that robots correctly recognize and track actors, objects,\nand their interactions over time. To achieve this, CARMA uniquely identifies\nphysical instances of such entities in the real world and organizes them into\ngrounded triplets of actors, objects, and actions.\n  To validate our approach, we conducted three experiments, where multiple\nhumans and a robot interact: collaborative pouring, handovers, and sorting.\nThese scenarios allow the assessment of the system's capabilities as to role\ndistinction, multi-actor awareness, and consistent instance identification. Our\nexperiments demonstrate that the system can reliably generate accurate\nactor-action-object triplets, providing a structured and robust foundation for\napplications requiring spatiotemporal reasoning and situated decision-making in\ncollaborative settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20373v1", "categories": ["cs.RO", "cs.AI", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20373v1", "AI": {"title_translation": "CARMA：结合视觉-语言模型与物体和动作识别的人机群体交互情境感知基础", "tldr": "CARMA是一个用于人机群体交互情境感知的系统，它通过识别和跟踪实体，并将其组织成参与者-物体-动作三元组，从而实现了有效的协作。", "motivation": "在人机群体交互中，有效的协作需要基于对在场人员和物体的一致表示以及对参与者和被操纵物体事件的偶发性抽象的情境感知。这要求清晰一致的实例分配，以确保机器人能够正确识别并随时间跟踪参与者、物体及其交互。", "method": "CARMA系统通过独特地识别现实世界中实体（如参与者和物体）的物理实例，并将它们组织成接地化的参与者-物体-动作三元组来实现情境感知。它结合了视觉-语言模型与物体和动作识别技术。", "result": "实验表明，CARMA系统能够可靠地生成准确的参与者-动作-物体三元组。这为需要时空推理和协作环境中情境决策的应用提供了结构化且稳健的基础。", "conclusion": "CARMA系统通过提供准确且结构化的情境感知基础，显著提升了人机群体交互中的协作能力，为未来的人机协作应用奠定了坚实的基础。", "translation": "我们引入了CARMA，一个用于人机群体交互中情境感知的系统。在这种群体设置中，有效的协作需要基于对在场人员和物体的一致表示以及对参与者和被操纵物体事件的偶发性抽象的情境感知。这要求清晰一致的实例分配，确保机器人能够正确识别并随时间跟踪参与者、物体及其交互。为实现这一点，CARMA独特地识别现实世界中此类实体的物理实例，并将它们组织成接地化的参与者、物体和动作三元组。\n为了验证我们的方法，我们进行了三项实验，其中涉及多人与一个机器人进行交互：协作倾倒、移交和分类。这些场景允许评估系统在角色区分、多参与者感知和一致实例识别方面的能力。我们的实验表明，该系统能够可靠地生成准确的参与者-动作-物体三元组，为需要时空推理和协作环境中情境决策的应用提供了结构化且稳健的基础。", "summary": "CARMA是一个旨在提升人机群体交互中情境感知的系统。它通过结合视觉-语言模型与物体和动作识别技术，独特地识别现实世界中的实体，并将其组织成参与者-物体-动作的三元组。系统通过协作倾倒、移交和分类等实验进行验证，结果显示其能可靠地生成准确的三元组，为协作环境中的时空推理和决策提供了稳固的基础。", "keywords": "人机交互, 情境感知, 视觉-语言模型, 物体识别, 动作识别", "comments": "CARMA的创新之处在于其将视觉-语言模型与物体和动作识别相结合，以实现人机群体交互中的情境感知。通过构建接地化的参与者-物体-动作三元组，该系统为机器人理解复杂的人机协作场景提供了结构化且鲁棒的框架，这对于实现更自然、高效的人机协作至关重要。"}}
{"id": "2506.20255", "title": "A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features", "authors": ["Ayush Lodh", "Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal"], "summary": "We posit that handwriting recognition benefits from complementary cues\ncarried by the rasterized complex glyph and the pen's trajectory, yet most\nsystems exploit only one modality. We introduce an end-to-end network that\nperforms early fusion of offline images and online stroke data within a shared\nlatent space. A patch encoder converts the grayscale crop into fixed-length\nvisual tokens, while a lightweight transformer embeds the $(x, y, \\text{pen})$\nsequence. Learnable latent queries attend jointly to both token streams,\nyielding context-enhanced stroke embeddings that are pooled and decoded under a\ncross-entropy loss objective. Because integration occurs before any high-level\nclassification, temporal cues reinforce each other during representation\nlearning, producing stronger writer independence. Comprehensive experiments on\nIAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art\naccuracy, exceeding previous bests by up to 1\\%. Our study also shows\nadaptation of this pipeline with gesturification on the ISI-Air dataset. Our\ncode can be found here.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.20255v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20255v1", "AI": {"title_translation": "一种联合使用在线和离线特征的基于Transformer的手写识别系统", "tldr": "该研究提出了一种端到端网络，通过早期融合在线笔画数据和离线图像，在共享潜在空间中进行手写识别。该系统利用Transformer增强上下文信息，并在IAMOn-DB和VNOn-DB数据集上达到了最先进的准确率。", "motivation": "大多数手写识别系统只利用单一模态（在线笔画或离线图像），但作者认为手写识别可以从光栅化复杂字形和笔迹轨迹所携带的互补线索中受益，因此提出了一种结合两种模态的方法。", "method": "该方法引入了一个端到端网络，在共享潜在空间中对离线图像和在线笔画数据进行早期融合。一个补丁编码器将灰度裁剪图像转换为固定长度的视觉tokens，而一个轻量级Transformer嵌入(x, y, pen)序列。可学习的潜在查询同时关注两个token流，产生上下文增强的笔画嵌入，这些嵌入在交叉熵损失目标下进行池化和解码。集成发生在任何高级分类之前，使得表示学习期间时间线索相互加强。", "result": "该方法在IAMOn-DB和VNOn-DB数据集上实现了最先进的准确率，比之前的最佳结果提高了1%。研究还表明，该管道可以通过手势化适应ISI-Air数据集。", "conclusion": "通过在表示学习阶段早期融合在线和离线特征，该系统能够利用两种模态的互补信息，从而实现更强的书写者独立性，并在手写识别任务上达到最先进的性能。", "translation": "我们认为手写识别受益于光栅化复杂字形和笔迹轨迹所携带的互补线索，然而大多数系统只利用单一模态。我们引入了一种端到端网络，该网络在共享潜在空间中对离线图像和在线笔画数据进行早期融合。一个补丁编码器将灰度裁剪图像转换为固定长度的视觉tokens，而一个轻量级Transformer嵌入(x, y, pen)序列。可学习的潜在查询同时关注两个token流，产生上下文增强的笔画嵌入，这些嵌入在交叉熵损失目标下进行池化和解码。由于集成发生在任何高级分类之前，时间线索在表示学习期间相互加强，产生更强的书写者独立性。在IAMOn-DB和VNOn-DB上的综合实验表明，我们的方法达到了最先进的准确率，超过了之前的最佳结果高达1%。我们的研究还表明，该管道可以通过手势化适应ISI-Air数据集。我们的代码可以在这里找到。", "summary": "该论文提出了一种基于Transformer的端到端手写识别系统，通过在共享潜在空间中早期融合离线图像和在线笔画数据来利用两种模态的互补信息。该系统使用补丁编码器处理视觉信息，并使用轻量级Transformer嵌入笔画序列。通过可学习的潜在查询联合关注两种token流，实现上下文增强的表示学习，从而提高了书写者独立性。实验证明，该方法在IAMOn-DB和VNOn-DB数据集上取得了最先进的性能，准确率提升高达1%。", "keywords": "手写识别, Transformer, 在线特征, 离线特征, 多模态融合", "comments": "该论文的创新点在于提出了一个端到端系统，通过早期融合在线和离线特征来解决手写识别中的单一模态限制。利用Transformer模型增强上下文理解，并在表示学习阶段加强时间线索，从而有效提升了书写者独立性，这对于实际应用具有重要意义。其在主流数据集上取得的显著性能提升，进一步验证了该方法的有效性和先进性。"}}
{"id": "2506.20016", "title": "New Insights on Unfolding and Fine-tuning Quantum Federated Learning", "authors": ["Shanika Iroshi Nanayakkara", "Shiva Raj Pokhrel"], "summary": "Client heterogeneity poses significant challenges to the performance of\nQuantum Federated Learning (QFL). To overcome these limitations, we propose a\nnew approach leveraging deep unfolding, which enables clients to autonomously\noptimize hyperparameters, such as learning rates and regularization factors,\nbased on their specific training behavior. This dynamic adaptation mitigates\noverfitting and ensures robust optimization in highly heterogeneous\nenvironments where standard aggregation methods often fail. Our framework\nachieves approximately 90% accuracy, significantly outperforming traditional\nmethods, which typically yield around 55% accuracy, as demonstrated through\nreal-time training on IBM quantum hardware and Qiskit Aer simulators. By\ndeveloping self adaptive fine tuning, the proposed method proves particularly\neffective in critical applications such as gene expression analysis and cancer\ndetection, enhancing diagnostic precision and predictive modeling within\nquantum systems. Our results are attributed to convergence-aware, learnable\noptimization steps intrinsic to the deep unfolded framework, which maintains\nthe generalization. Hence, this study addresses the core limitations of\nconventional QFL, streamlining its applicability to any complex challenges such\nas healthcare and genomic research.", "comment": "12 pages, 9 figures, 7 Tables, Submitted to IEEE/ACM journal 2025", "pdf_url": "http://arxiv.org/pdf/2506.20016v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20016v1", "AI": {"title_translation": "关于展开和微调量子联邦学习的新见解", "tldr": "本文提出了一种利用深度展开的新方法来解决量子联邦学习（QFL）中的客户端异质性问题，允许客户端自主优化超参数。该方法在IBM量子硬件上实现了约90%的准确率，远超传统方法的55%，并适用于基因表达分析和癌症检测等关键应用。", "motivation": "客户端异质性对量子联邦学习（QFL）的性能构成了重大挑战，导致标准聚合方法在高度异质性环境中经常失败，并可能出现过拟合。", "method": "本文提出了一种利用深度展开的新方法，使客户端能够根据其特定的训练行为自主优化学习率和正则化因子等超参数。这种动态自适应微调旨在减轻过拟合，并确保在高度异质性环境中的鲁棒优化。", "result": "该框架实现了大约90%的准确率，显著优于传统方法（通常为55%）。这通过在IBM量子硬件和Qiskit Aer模拟器上的实时训练得到验证。其结果归因于深度展开框架固有的收敛感知、可学习的优化步骤，这些步骤保持了泛化能力。", "conclusion": "本研究通过开发自适应微调的深度展开方法，有效解决了传统量子联邦学习（QFL）的核心局限性，提高了其在医疗保健和基因组研究等复杂挑战中的适用性，尤其在基因表达分析和癌症检测等关键应用中表现出色。", "translation": "客户端异质性对量子联邦学习（QFL）的性能构成了重大挑战。为了克服这些限制，我们提出了一种利用深度展开的新方法，该方法使客户端能够根据其特定的训练行为自主优化学习率和正则化因子等超参数。这种动态适应减轻了过拟合，并确保在标准聚合方法通常失效的高度异质性环境中的鲁棒优化。我们的框架实现了大约90%的准确率，显著优于传统方法（通常产生约55%的准确率），这通过在IBM量子硬件和Qiskit Aer模拟器上的实时训练得到证明。通过开发自适应微调，所提出的方法在基因表达分析和癌症检测等关键应用中被证明特别有效，增强了量子系统内的诊断精度和预测建模。我们的结果归因于深度展开框架固有的收敛感知、可学习的优化步骤，这些步骤保持了泛化能力。因此，本研究解决了传统QFL的核心局限性，简化了其在医疗保健和基因组研究等任何复杂挑战中的适用性。", "summary": "本文针对量子联邦学习（QFL）中客户端异质性带来的性能挑战，提出了一种基于深度展开的新方法。该方法允许客户端自主优化学习率和正则化因子等超参数，通过自适应微调有效减轻过拟合并确保在异质环境中的鲁棒优化。实验结果表明，该方法在IBM量子硬件和Qiskit Aer模拟器上实现了约90%的准确率，显著优于传统方法的55%。该研究为QFL在基因表达分析和癌症检测等关键应用中的部署提供了新的见解和解决方案。", "keywords": "量子联邦学习, 深度展开, 客户端异质性, 超参数优化, 自适应微调", "comments": "这项研究的创新之处在于将深度展开引入量子联邦学习，以解决客户端异质性问题，并通过允许客户端自主优化超参数来实现自适应微调。其显著的性能提升（从55%到90%的准确率）以及在实际量子硬件上的验证，突显了其重要性。该方法有望极大地扩展QFL在医疗保健等高敏感领域的应用潜力。"}}
{"id": "2506.20404", "title": "GymPN: A Library for Decision-Making in Process Management Systems", "authors": ["Riccardo Lo Bianco", "Willem van Jaarsveld", "Remco Dijkman"], "summary": "Process management systems support key decisions about the way work is\nallocated in organizations. This includes decisions on which task to perform\nnext, when to execute the task, and who to assign the task to. Suitable\nsoftware tools are required to support these decisions in a way that is optimal\nfor the organization. This paper presents a software library, called GymPN,\nthat supports optimal decision-making in business processes using Deep\nReinforcement Learning. GymPN builds on previous work that supports task\nassignment in business processes, introducing two key novelties: support for\npartial process observability and the ability to model multiple decisions in a\nbusiness process. These novel elements address fundamental limitations of\nprevious work and thus enable the representation of more realistic process\ndecisions. We evaluate the library on eight typical business process\ndecision-making problem patterns, showing that GymPN allows for easy modeling\nof the desired problems, as well as learning optimal decision policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20404v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20404v1", "AI": {"title_translation": "GymPN：一个用于流程管理系统决策的库", "tldr": "GymPN是一个基于深度强化学习的软件库，旨在支持业务流程中的最优决策，解决了以往工作中部分可观察性和多决策建模的局限性，并在典型问题模式上表现出良好的建模和学习能力。", "motivation": "流程管理系统需要合适的软件工具来支持工作分配中的关键决策，如执行哪个任务、何时执行以及分配给谁，以实现组织最优。", "method": "本文提出了一个名为GymPN的软件库，它利用深度强化学习来支持业务流程中的最优决策。GymPN在现有工作的基础上，引入了两个主要创新点：支持部分过程可观察性和建模业务流程中多个决策的能力。", "result": "GymPN在八种典型的业务流程决策问题模式上进行了评估，结果表明该库能够轻松建模所需问题，并学习到最优决策策略。", "conclusion": "GymPN库通过支持部分过程可观察性和建模多个决策，解决了现有方法的局限性，能够有效地建模和学习业务流程中的最优决策策略，从而实现更真实的流程决策。", "translation": "流程管理系统支持组织中工作分配方式的关键决策。这包括决定接下来执行哪个任务、何时执行任务以及将任务分配给谁。需要合适的软件工具来以对组织最优的方式支持这些决策。本文提出了一个名为GymPN的软件库，它使用深度强化学习来支持业务流程中的最优决策。GymPN建立在以前支持业务流程中任务分配的工作之上，引入了两个关键的新颖性：支持部分过程可观察性和建模业务流程中多个决策的能力。这些新颖的元素解决了以前工作的基本局限性，从而能够表示更真实的流程决策。我们在八种典型的业务流程决策问题模式上评估了该库，表明GymPN允许轻松建模所需问题，以及学习最优决策策略。", "summary": "GymPN是一个基于深度强化学习的软件库，旨在优化流程管理系统中的决策制定，包括任务选择、执行时间和分配。该库通过引入对部分过程可观察性的支持和建模多个业务流程决策的能力，克服了以往方法的局限性，从而能够处理更真实的决策场景。在八种典型业务流程决策问题模式上的评估表明，GymPN易于建模问题并能学习到最优决策策略。", "keywords": "流程管理, 深度强化学习, 决策制定, 软件库, 业务流程", "comments": "GymPN的创新之处在于其对部分过程可观察性的支持和建模多个决策的能力，这使其能够更好地处理真实世界的业务流程复杂性。该库的重要性在于它为使用深度强化学习解决流程管理中的决策问题提供了一个实用的工具，有望提高组织效率和决策质量。"}}
{"id": "2506.20102", "title": "Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox", "authors": ["Malikussaid", "Sutiyo"], "summary": "The convergence of IT and OT has created hyper-connected ICS, exposing\ncritical infrastructure to a new class of adaptive, intelligent adversaries\nthat render static defenses obsolete. Existing security paradigms often fail to\naddress a foundational \"Trinity of Trust,\" comprising the fidelity of the\nsystem model, the integrity of synchronizing data, and the resilience of the\nanalytical engine against sophisticated evasion. This paper introduces the ARC\nframework, a method for achieving analytical resilience through an autonomous,\nclosed-loop hardening process. ARC establishes a perpetual co-evolutionary arms\nrace within the high-fidelity sandbox of a F-SCDT. A DRL agent, the \"Red\nAgent,\" is formalized and incentivized to autonomously discover stealthy,\nphysically-plausible attack paths that maximize process disruption while\nevading detection. Concurrently, an ensemble-based \"Blue Agent\" defender is\ncontinuously hardened via adversarial training against the evolving threats\ndiscovered by its adversary. This co-evolutionary dynamic forces both agents to\nbecome progressively more sophisticated, enabling the system to autonomously\nprobe and patch its own vulnerabilities. Experimental validation on both the\nTEP and the SWaT testbeds demonstrates the framework's superior performance. A\ncomprehensive ablation study, supported by extensive visualizations including\nROC curves and SHAP plots, reveals that the co-evolutionary process itself is\nresponsible for a significant performance increase in detecting novel attacks.\nBy integrating XAI to ensure operator trust and proposing a scalable F-ARC\narchitecture, this work presents ARC not merely as an improvement, but as a\nnecessary paradigm shift toward dynamic, self-improving security for the future\nof critical infrastructure.", "comment": "17 pages, 2 figures, 4 equations, 2 algorithms, 4 tables, to be\n  published in ISPACS Conference 2025, unabridged version", "pdf_url": "http://arxiv.org/pdf/2506.20102v1", "categories": ["cs.CR", "cs.LG", "cs.SY", "eess.SY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20102v1", "AI": {"title_translation": "通过强化数字孪生沙箱中的协同进化军备竞赛实现自主网络弹性", "tldr": "本文提出了ARC框架，一种利用数字孪生沙箱中的协同进化军备竞赛实现关键基础设施自主网络弹性的方法，通过红蓝智能体相互对抗不断提升安全防御能力，并在实验中验证了其卓越性能。", "motivation": "IT和OT的融合导致超连接的ICS，使关键基础设施面临自适应、智能的新型对手，使得静态防御过时。现有安全范式未能解决“信任三元组”问题，即系统模型的保真度、同步数据的完整性以及分析引擎对抗复杂规避的弹性。", "method": "本文提出了ARC框架，通过在强化的数字孪生沙箱（F-SCDT）中建立持续的协同进化军备竞赛，实现自主、闭环的强化过程。一个DRL“红方智能体”被形式化并激励去自主发现隐蔽的、物理上可信的攻击路径以最大化过程中断并规避检测。同时，一个基于集成学习的“蓝方智能体”防御者通过对抗性训练持续强化，以应对其对手发现的不断演变的威胁。", "result": "在TEP和SWaT测试台上的实验验证表明，该框架具有卓越的性能。全面的消融研究以及ROC曲线和SHAP图等广泛可视化揭示，协同进化过程本身对检测新型攻击的性能提升负有显著责任。", "conclusion": "ARC框架通过整合XAI以确保操作员信任并提出可扩展的F-ARC架构，不仅仅是一种改进，更是向未来关键基础设施动态、自我完善安全范式转变的必要方向。", "translation": "IT和OT的融合创造了超连接的ICS，使关键基础设施面临一类新的自适应、智能的对手，这些对手使得静态防御过时。现有的安全范式往往未能解决一个基础的“信任三元组”问题，该三元组包括系统模型的保真度、同步数据的完整性以及分析引擎对抗复杂规避的弹性。本文引入了ARC框架，这是一种通过自主、闭环强化过程实现分析弹性的方法。ARC在F-SCDT的高保真沙箱中建立了一个持续的协同进化军备竞赛。一个DRL智能体，即“红方智能体”，被形式化并激励去自主发现隐蔽的、物理上可信的攻击路径，以最大化过程中断并规避检测。同时，一个基于集成学习的“蓝方智能体”防御者通过对抗性训练持续强化，以应对其对手发现的不断演变的威胁。这种协同进化的动态迫使两个智能体都变得越来越复杂，使系统能够自主探测并修补自身的漏洞。在TEP和SWaT测试台上的实验验证证明了该框架的卓越性能。一项全面的消融研究，由包括ROC曲线和SHAP图在内的广泛可视化支持，揭示了协同进化过程本身对检测新型攻击的性能显著提升负有责任。通过整合XAI以确保操作员信任并提出可扩展的F-ARC架构，这项工作将ARC不仅仅视为一种改进，更是未来关键基础设施动态、自我完善安全范式转变的必要方向。", "summary": "本文提出了ARC框架，旨在解决关键基础设施在IT/OT融合背景下，面对自适应智能对手时传统静态防御失效的问题。ARC通过在强化的数字孪生沙箱中模拟红方（攻击）和蓝方（防御）智能体的协同进化军备竞赛，使系统能够自主发现并修补漏洞。红方智能体利用深度强化学习发现隐蔽攻击路径，蓝方智能体通过对抗训练持续强化自身。实验证明，该框架在检测新型攻击方面表现卓越，尤其协同进化过程对性能提升至关重要。ARC被视为实现关键基础设施动态、自我完善安全的一种范式转变。", "keywords": "网络弹性, 数字孪生, 协同进化, 深度强化学习, 工业控制系统安全", "comments": "本文创新性地提出了ARC框架，利用协同进化军备竞赛的理念，在数字孪生沙箱中实现了关键基础设施的自主网络弹性。通过红蓝智能体的对抗性学习，系统能够动态地识别并修复自身的脆弱性，这超越了传统的静态防御模式。其重要性在于为未来关键基础设施的安全提供了一种适应性强、能自我提升的解决方案，特别是解决了面对高级持续性威胁时的“信任三元组”挑战。结合可解释人工智能（XAI）也增强了操作员对系统的信任度。"}}
{"id": "2506.20664", "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind", "authors": ["Andrei Lupu", "Timon Willi", "Jakob Foerster"], "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents.", "comment": "41 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.20664v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20664v1", "AI": {"title_translation": "Decrypto：多智能体推理与心智理论基准测试", "tldr": "Decrypto是一个新的游戏基准测试，用于评估大型语言模型在多智能体场景中的推理和心智理论能力，发现当前LLM在这方面表现不佳，并指出其填补了现有评估的空白。", "motivation": "随着大型语言模型（LLM）获得代理能力，它们需要在复杂的多智能体场景中与人类用户和其他代理进行交互，这需要新的推理技能，其中最重要的是心智理论（ToM），即推理其他代理“心理”状态的能力。然而，LLM中的ToM和其他多智能体能力了解甚少，因为现有基准测试存在范围狭窄、数据泄露、饱和和缺乏交互性等问题。", "method": "提出了Decrypto，一个受认知科学、计算语用学和多智能体强化学习启发的游戏基准测试，用于多智能体推理和ToM。它被设计成尽可能消除混淆因素，并被认为是第一个用于设计交互式ToM实验的平台。通过对前沿LLM进行全面的实证评估、鲁棒性研究和人机交叉对战实验来验证其设计。还在Decrypto中创建了两个经典认知科学实验的变体来评估三个关键的ToM能力。", "result": "发现LLM的游戏能力落后于人类和简单的词嵌入基线。令人惊讶的是，发现最先进的推理模型在这些任务上的表现明显不如其旧版本。", "conclusion": "Decrypto解决了当前推理和ToM评估中的一个关键空白，并为开发更好的智能体铺平了道路。", "translation": "随着大型语言模型（LLM）获得代理能力，它们将不得不在复杂的多智能体场景中导航，在合作和竞争环境中与人类用户和其他代理进行交互。这将需要新的推理技能，其中最主要的是心智理论（ToM），即推理其他代理“心理”状态的能力。然而，LLM中的ToM和其他多智能体能力了解甚少，因为现有基准测试存在范围狭窄、数据泄露、饱和和缺乏交互性等问题。因此，我们提出了Decrypto，一个受认知科学、计算语用学和多智能体强化学习启发的游戏基准测试，用于多智能体推理和ToM。它被设计成在所有其他维度上尽可能简单，消除了其他基准测试中常见的混淆因素。据我们所知，它也是第一个用于设计交互式ToM实验的平台。\n我们通过对前沿LLM进行全面的实证评估、鲁棒性研究和人机交叉对战实验来验证基准测试设计。我们发现LLM的游戏能力落后于人类和简单的词嵌入基线。然后，我们在Decrypto中创建了两个经典认知科学实验的变体，以评估三个关键的ToM能力。令人惊讶的是，我们发现最先进的推理模型在这些任务上的表现明显不如其旧版本。这表明Decrypto解决了当前推理和ToM评估中的一个关键空白，并为开发更好的人工智能体铺平了道路。", "summary": "该论文提出了Decrypto，一个新颖的游戏基准测试，旨在解决现有评估大型语言模型（LLM）在多智能体推理和心智理论（ToM）能力方面存在的不足。Decrypto的设计灵感来源于认知科学等领域，并强调消除混淆因素和提供交互性。通过对前沿LLM的评估，研究发现LLM在游戏能力上落后于人类和基线模型，且在ToM任务上表现甚至不如旧版本模型。这表明Decrypto成功揭示了当前LLM在多智能体推理和ToM方面的不足，并为未来人工智能体的发展指明了方向。", "keywords": "多智能体推理, 心智理论, 大型语言模型, 基准测试, Decrypto", "comments": "Decrypto基准测试的创新之处在于其游戏化设计和对交互性的强调，这解决了现有ToM评估中数据泄露、饱和和缺乏交互性的问题。它通过引入认知科学实验变体，提供了一个更精确、更具挑战性的评估框架。研究结果令人关注，特别是发现最新LLM在某些ToM任务上表现退步，这凸显了当前AI在复杂社会推理方面仍面临的挑战和改进空间。该基准对推动多智能体AI和心智理论研究具有重要意义。"}}
{"id": "2506.20297", "title": "OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning", "authors": ["Natalie Lang", "Maya Simhi", "Nir Shlezinger"], "summary": "Federated learning (FL) enables collaborative training across distributed\nclients without sharing raw data, often at the cost of substantial\ncommunication overhead induced by transmitting high-dimensional model updates.\nThis overhead can be alleviated by having the clients quantize their model\nupdates, with dithered lattice quantizers identified as an attractive scheme\ndue to its structural simplicity and convergence-preserving properties.\nHowever, existing lattice-based FL schemes typically rely on a fixed\nquantization rule, which is suboptimal in heterogeneous and dynamic\nenvironments where the model updates distribution varies across users and\ntraining rounds. In this work, we propose Online Learned Adaptive Lattices\n(OLALa), a heterogeneous FL framework where each client can adjust its\nquantizer online using lightweight local computations. We first derive\nconvergence guarantees for FL with non-fixed lattice quantizers and show that\nproper lattice adaptation can tighten the convergence bound. Then, we design an\nonline learning algorithm that enables clients to tune their quantizers\nthroughout the FL process while exchanging only a compact set of quantization\nparameters. Numerical experiments demonstrate that OLALa consistently improves\nlearning performance under various quantization rates, outperforming\nconventional fixed-codebook and non-adaptive schemes.", "comment": "Under review for publication in the IEEE", "pdf_url": "http://arxiv.org/pdf/2506.20297v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20297v1", "AI": {"title_translation": "OLALa：异构联邦学习的在线学习自适应格码", "tldr": "提出OLALa，一种异构联邦学习框架，允许客户端在线自适应调整量化器，以提高通信效率和学习性能。", "motivation": "联邦学习中高维模型更新导致通信开销大，现有基于格码的量化方案采用固定量化规则，在模型更新分布异构和动态的环境中表现不佳。", "method": "提出OLALa框架，允许每个客户端使用轻量级本地计算在线调整其量化器。推导了非固定格码量化器的收敛性保证，并设计了在线学习算法，使客户端在联邦学习过程中调整量化器，同时仅交换少量量化参数。", "result": "数值实验表明，OLALa在各种量化率下持续提升学习性能，优于传统的固定码本和非自适应方案。", "conclusion": "OLALa通过在线自适应格码量化，有效解决了异构联邦学习中的通信效率问题，并提升了学习性能，且具有收敛性保证。", "translation": "联邦学习（FL）实现了分布式客户端之间的协作训练，无需共享原始数据，但通常以传输高维模型更新所带来的巨大通信开销为代价。通过客户端对其模型更新进行量化可以缓解这种开销，其中抖动格量化器因其结构简单和保持收敛性的特性而被认为是一种有吸引力的方案。然而，现有的基于格码的FL方案通常依赖于固定的量化规则，这在模型更新分布在用户和训练轮次之间变化的异构动态环境中是次优的。在这项工作中，我们提出了在线学习自适应格码（OLALa），一个异构FL框架，其中每个客户端都可以使用轻量级本地计算在线调整其量化器。我们首先推导了非固定格量化器FL的收敛性保证，并表明适当的格适应可以收紧收敛界限。然后，我们设计了一种在线学习算法，使客户端在整个FL过程中调整其量化器，同时仅交换一组紧凑的量化参数。数值实验表明，OLALa在各种量化率下持续提高了学习性能，优于传统的固定码本和非自适应方案。", "summary": "本文提出了OLALa（在线学习自适应格码），一个针对异构联邦学习的框架，旨在解决高维模型更新带来的通信开销问题。OLALa允许每个客户端利用轻量级本地计算在线调整其抖动格量化器，而非依赖固定的量化规则。研究推导了非固定格量化器的收敛性保证，并设计了在线学习算法。实验证明OLALa在不同量化率下均能显著提升学习性能，优于现有固定码本和非自适应方案。", "keywords": "联邦学习, 格码, 量化, 在线学习, 通信效率", "comments": "这篇论文的创新点在于提出了在线学习自适应格码（OLALa）来解决异构联邦学习中量化器固定导致性能次优的问题。通过允许客户端在线调整量化器，并提供收敛性保证，它在通信效率和模型性能之间找到了更好的平衡。其重要性在于为实际部署联邦学习提供了更灵活和高效的通信压缩方案。"}}
{"id": "2506.20119", "title": "Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests", "authors": ["Masaki Uto", "Yuma Ito"], "summary": "Evaluating the abilities of learners is a fundamental objective in the field\nof education. In particular, there is an increasing need to assess higher-order\nabilities such as expressive skills and logical thinking. Constructed-response\ntests such as short-answer and essay-based questions have become widely used as\na method to meet this demand. Although these tests are effective, they require\nsubstantial manual grading, making them both labor-intensive and costly. Item\nresponse theory (IRT) provides a promising solution by enabling the estimation\nof ability from incomplete score data, where human raters grade only a subset\nof answers provided by learners across multiple test items. However, the\naccuracy of ability estimation declines as the proportion of missing scores\nincreases. Although data augmentation techniques for imputing missing scores\nhave been explored in order to address this limitation, they often struggle\nwith inaccuracy for sparse or heterogeneous data. To overcome these challenges,\nthis study proposes a novel method for imputing missing scores by leveraging\nautomated scoring technologies for accurate IRT-based ability estimation. The\nproposed method achieves high accuracy in ability estimation while markedly\nreducing manual grading workload.", "comment": "Accepted to EvalLAC'25: 2nd Workshop on Automatic Evaluation of\n  Learning and Assessment Content, held at AIED 2025, Palermo, Italy. This is\n  the camera-ready version submitted to CEUR Workshop Proceedings", "pdf_url": "http://arxiv.org/pdf/2506.20119v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20119v1", "AI": {"title_translation": "利用AI评分器进行缺失分数插补以实现在构建式回答测试中准确的能力估计", "tldr": "本研究提出了一种利用AI评分器进行缺失分数插补的新方法，以提高构建式回答测试中能力估计的准确性并减少人工评分工作量。", "motivation": "教育领域评估学习者能力是基本目标，尤其需要评估高阶能力。构建式回答测试虽有效，但人工评分耗时且成本高。项目反应理论（IRT）虽能从不完整数据估计能力，但缺失分数比例增加会降低准确性，现有数据增强技术在稀疏或异构数据上表现不佳。", "method": "本研究提出了一种新颖的方法，通过利用自动化评分技术进行缺失分数插补，以实现基于IRT的准确能力估计。", "result": "所提出的方法在能力估计方面达到了高准确性，同时显著减少了人工评分工作量。", "conclusion": "通过利用AI评分器进行缺失分数插补，可以有效提高构建式回答测试中能力估计的准确性，并大幅降低人工评分的负担。", "translation": "评估学习者的能力是教育领域的一个基本目标。特别是，对表达能力和逻辑思维等高阶能力的评估需求日益增长。短答案和论文式问题等构建式回答测试已广泛用作满足这一需求的方法。尽管这些测试是有效的，但它们需要大量的人工评分，使其既费力又昂贵。项目反应理论（IRT）通过从不完整的得分数据中估计能力提供了一个有前景的解决方案，其中人类评分员只对学习者在多个测试项目上提供的一部分答案进行评分。然而，随着缺失分数比例的增加，能力估计的准确性会下降。尽管为了解决这一限制，已经探索了用于插补缺失分数的数据增强技术，但它们在处理稀疏或异构数据时常常面临不准确的问题。为了克服这些挑战，本研究提出了一种利用自动化评分技术进行缺失分数插补的新方法，以实现准确的基于IRT的能力估计。所提出的方法在能力估计方面达到了高准确性，同时显著减少了人工评分工作量。", "summary": "针对构建式回答测试中人工评分耗时且缺失分数影响能力估计准确性的问题，本研究提出了一种利用自动化评分技术进行缺失分数插补的新方法。该方法显著提高了基于项目反应理论的能力估计准确性，并大幅减少了人工评分的工作量，为教育评估提供了一种高效且准确的解决方案。", "keywords": "AI评分器, 缺失分数插补, 能力估计, 构建式回答测试, 项目反应理论", "comments": "这项研究的创新之处在于将AI评分技术应用于缺失分数插补，而非仅仅用于直接评分。它解决了IRT在缺失数据下准确性下降以及现有插补方法不足的痛点，对于大规模构建式回答测试的实施具有重要意义，能够有效降低成本并提高评估效率。该方法的潜在局限性可能在于AI评分器本身的准确性和泛化能力，特别是在处理非常规或创造性回答时。"}}
{"id": "2506.20419", "title": "A Taylor-Hood finite element method for the surface Stokes problem without penalization", "authors": ["Alan Demlow", "Michael Neilan"], "summary": "Finite element approximation of the velocity-pressure formulation of the\nsurfaces Stokes equations is challenging because it is typically not possible\nto enforce both tangentiality and $H^1$ conformity of the velocity field. Most\nprevious works concerning finite element methods (FEMs) for these equations\nthus have weakly enforced one of these two constraints by penalization or a\nLagrange multiplier formulation. Recently in [A tangential and penalty-free\nfinite element method for the surface Stokes problem, SINUM 62(1):248-272,\n2024], the authors constructed a surface Stokes FEM based on the MINI element\nwhich is tangentiality conforming and $H^1$ nonconforming, but possesses\nsufficient weak continuity properties to circumvent the need for penalization.\nThe key to this method is construction of velocity degrees of freedom lying on\nelement edges and vertices using an auxiliary Piola transform. In this work we\nextend this methodology to construct Taylor-Hood surface FEMs. The resulting\nmethod is shown to achieve optimal-order convergence when the edge degrees of\nfreedom for the velocity spaced are placed at Gauss-Lobatto nodes. Numerical\nexperiments confirm that this nonstandard placement of nodes is necessary to\nachieve optimal convergence orders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20419v1", "categories": ["math.NA", "cs.NA", "65N12, 65N15, 65N30"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20419v1", "AI": {"title_translation": "表面Stokes问题无罚项Taylor-Hood有限元方法", "tldr": "本文将一种无罚项的表面Stokes有限元方法推广到Taylor-Hood单元，通过在Gauss-Lobatto节点放置速度自由度，实现了最优阶收敛。", "motivation": "传统的表面Stokes方程有限元方法在处理速度场的切向性和H1一致性时面临挑战，通常需要通过罚项或Lagrange乘子来弱化约束。本文旨在提供一种无需罚项的改进方法。", "method": "本文将先前基于MINI单元的无罚项表面Stokes有限元方法推广到Taylor-Hood表面有限元。关键在于通过辅助Piola变换，将速度自由度放置在单元边和顶点的Gauss-Lobatto节点上。", "result": "所提出的方法在速度空间中的边自由度放置在Gauss-Lobatto节点时，能够实现最优阶收敛。数值实验证实，这种非标准节点放置对于实现最优收敛阶是必要的。", "conclusion": "本文成功构建了无罚项的Taylor-Hood表面有限元方法，并证明了其最优阶收敛性。研究表明，将速度自由度放置在Gauss-Lobatto节点对于达到最优收敛至关重要。", "translation": "表面Stokes方程速度-压力形式的有限元近似具有挑战性，因为通常不可能同时强制速度场的切向性和H1一致性。因此，以前关于这些方程的有限元方法（FEMs）大多通过罚项或Lagrange乘子公式弱化了这两个约束之一。最近在[A tangential and penalty-free finite element method for the surface Stokes problem, SINUM 62(1):248-272, 2024]中，作者构建了一种基于MINI单元的表面Stokes有限元方法，该方法具有切向一致性和H1非一致性，但具有足够的弱连续性特性，从而避免了罚项的需要。该方法的关键是利用辅助Piola变换构建位于单元边和顶点的速度自由度。在这项工作中，我们将这种方法推广到构建Taylor-Hood表面有限元。结果表明，当速度空间的边自由度放置在Gauss-Lobatto节点时，该方法实现了最优阶收敛。数值实验证实，这种非标准节点放置对于实现最优收敛阶是必要的。", "summary": "本文提出了一种用于表面Stokes问题的Taylor-Hood有限元方法，该方法无需罚项。鉴于传统方法在速度场的切向性和H1一致性方面存在的挑战，本文延续了近期一项无罚项研究的工作。通过将速度自由度放置在单元边上的Gauss-Lobatto节点，该方法被证明能够达到最优阶收敛，并且数值实验验证了这种节点放置的必要性。", "keywords": "有限元方法, 表面Stokes问题, Taylor-Hood, 无罚项, Gauss-Lobatto节点", "comments": "本文的创新点在于将无罚项的表面Stokes有限元方法推广到更常用的Taylor-Hood单元，并明确指出Gauss-Lobatto节点放置对于实现最优收敛的重要性。这为解决表面Stokes问题提供了一种高效且理论上严谨的新方法。"}}
{"id": "2506.20376", "title": "Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation", "authors": ["Lingyun Chen", "Xinrui Zhao", "Marcos P. S. Campanha", "Alexander Wegener", "Abdeldjallil Naceri", "Abdalla Swikir", "Sami Haddadin"], "summary": "This paper presents a novel approach for robot navigation in environments\ncontaining deformable obstacles. By integrating Learning from Demonstration\n(LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation\nin complex environments where obstacles consist of both soft and hard regions.\nWe introduce a dynamic modulation matrix within the DS framework, allowing the\nsystem to distinguish between traversable soft regions and impassable hard\nareas in real-time, ensuring safe and flexible trajectory planning. We validate\nour method through extensive simulations and robot experiments, demonstrating\nits ability to navigate deformable environments. Additionally, the approach\nprovides control over both trajectory and velocity when interacting with\ndeformable objects, including at intersections, while maintaining adherence to\nthe original DS trajectory and dynamically adapting to obstacles for smooth and\nreliable navigation.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20376v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20376v1", "AI": {"title_translation": "增强型机器人在可变形环境中的导航：基于示教学习与动态调制", "tldr": "本文提出了一种结合示教学习和动力系统的新方法，通过引入动态调制矩阵，使机器人在包含软硬障碍物的可变形环境中实现安全灵活的导航，并在仿真和实验中得到验证。", "motivation": "在包含软硬区域障碍物的复杂环境中，实现机器人自适应和高效导航。", "method": "提出了一种将示教学习（LfD）与动力系统（DS）相结合的新方法。在DS框架内引入了动态调制矩阵，使系统能够实时区分可穿越的软区域和不可穿越的硬区域。", "result": "该方法通过广泛的仿真和机器人实验得到了验证，展示了其在可变形环境中导航的能力。此外，该方法在与可变形物体（包括交叉点）交互时，能够控制轨迹和速度，同时保持对原始DS轨迹的遵循，并动态适应障碍物以实现平稳可靠的导航。", "conclusion": "论文成功地提出并验证了一种结合LfD和DS的机器人导航方法，使其能够在包含可变形障碍物的复杂环境中实现安全、灵活且平稳的导航。", "translation": "本文提出了一种在包含可变形障碍物的环境中进行机器人导航的新方法。通过将示教学习（LfD）与动力系统（DS）相结合，我们能够在障碍物由软区域和硬区域组成的复杂环境中实现自适应和高效的导航。我们在DS框架内引入了一个动态调制矩阵，使系统能够实时区分可穿越的软区域和不可穿越的硬区域，从而确保安全灵活的轨迹规划。我们通过广泛的仿真和机器人实验验证了我们的方法，展示了其在可变形环境中导航的能力。此外，该方法在与可变形物体（包括交叉点）交互时，能够控制轨迹和速度，同时保持对原始DS轨迹的遵循，并动态适应障碍物以实现平稳可靠的导航。", "summary": "本文提出了一种结合示教学习（LfD）和动力系统（DS）的机器人导航新方法，专为包含可变形障碍物的环境设计。通过在DS框架中引入动态调制矩阵，机器人能够实时区分软（可穿越）和硬（不可穿越）区域，从而实现安全、灵活的轨迹规划。该方法在仿真和实验中被验证，展示了其在可变形环境中导航的能力，并能有效控制与可变形物体交互时的轨迹和速度，确保平稳可靠的导航。", "keywords": "机器人导航, 可变形环境, 示教学习, 动力系统, 动态调制", "comments": "该论文的创新点在于将示教学习与动力系统相结合，并引入动态调制矩阵来处理可变形环境中的软硬障碍物，这对于提高机器人在复杂真实世界场景中的鲁棒性和适应性具有重要意义。"}}
{"id": "2506.20263", "title": "Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification", "authors": ["Ning Luo", "Meiyin Hu", "Huan Wan", "Yanyan Yang", "Zhuohang Jiang", "Xin Wei"], "summary": "Few-shot fine-grained image classification (FS-FGIC) presents a significant\nchallenge, requiring models to distinguish visually similar subclasses with\nlimited labeled examples. Existing methods have critical limitations:\nmetric-based methods lose spatial information and misalign local features,\nwhile reconstruction-based methods fail to utilize hierarchical feature\ninformation and lack mechanisms to focus on discriminative regions. We propose\nthe Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which\nintegrates dual-layer feature reconstruction with mask-enhanced feature\nprocessing to improve fine-grained classification. HMDRN incorporates a\ndual-layer feature reconstruction and fusion module that leverages\ncomplementary visual information from different network hierarchies. Through\nlearnable fusion weights, the model balances high-level semantic\nrepresentations from the last layer with mid-level structural details from the\npenultimate layer. Additionally, we design a spatial binary mask-enhanced\ntransformer self-reconstruction module that processes query features through\nadaptive thresholding while maintaining complete support features, enhancing\nfocus on discriminative regions while filtering background noise. Extensive\nexperiments on three challenging fine-grained datasets demonstrate that HMDRN\nconsistently outperforms state-of-the-art methods across Conv-4 and ResNet-12\nbackbone architectures. Comprehensive ablation studies validate the\neffectiveness of each proposed component, revealing that dual-layer\nreconstruction enhances inter-class discrimination while mask-enhanced\ntransformation reduces intra-class variations. Visualization results provide\nevidence of HMDRN's superior feature reconstruction capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20263v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20263v1", "AI": {"title_translation": "用于少样本细粒度图像分类的分层掩码增强双重重建网络", "tldr": "本文提出了一种名为HMDRN的新型网络，通过结合双层特征重建和掩码增强特征处理，解决了少样本细粒度图像分类中现有方法在空间信息丢失、特征未对齐、缺乏分层特征利用和判别区域聚焦不足的问题，并在三个细粒度数据集上取得了优于现有技术水平的性能。", "motivation": "少样本细粒度图像分类（FS-FGIC）是一个重大挑战，因为它要求模型在有限的标记样本下区分视觉上相似的子类。现有方法存在关键局限性：基于度量的方法丢失空间信息并使局部特征未对齐，而基于重建的方法未能利用分层特征信息，并且缺乏关注判别区域的机制。", "method": "本文提出分层掩码增强双重重建网络（HMDRN），它集成了双层特征重建与掩码增强特征处理。HMDRN包含一个双层特征重建和融合模块，利用来自不同网络层次的互补视觉信息，并通过可学习的融合权重平衡高层语义表示和中层结构细节。此外，还设计了一个空间二值掩码增强Transformer自重建模块，通过自适应阈值处理查询特征，同时保持完整的支持特征，以增强对判别区域的关注并过滤背景噪声。", "result": "在三个具有挑战性的细粒度数据集上进行了广泛实验，结果表明HMDRN在Conv-4和ResNet-12骨干架构上始终优于现有最先进的方法。全面的消融研究验证了每个提出组件的有效性，揭示了双层重建增强了类间判别力，而掩码增强转换减少了类内变化。可视化结果提供了HMDRN优越特征重建能力的证据。", "conclusion": "HMDRN通过结合双层特征重建和掩码增强特征处理，有效解决了少样本细粒度图像分类中的挑战，并在多个数据集上表现出卓越的性能和特征重建能力。", "translation": "少样本细粒度图像分类（FS-FGIC）是一个重大挑战，它要求模型在有限的标记样本下区分视觉上相似的子类。现有方法存在关键局限性：基于度量的方法丢失空间信息并使局部特征未对齐，而基于重建的方法未能利用分层特征信息，并且缺乏关注判别区域的机制。我们提出分层掩码增强双重重建网络（HMDRN），它集成了双层特征重建与掩码增强特征处理，以改善细粒度分类。HMDRN包含一个双层特征重建和融合模块，利用来自不同网络层次的互补视觉信息。通过可学习的融合权重，模型平衡了来自最后一层的高层语义表示和来自倒数第二层的中层结构细节。此外，我们设计了一个空间二值掩码增强Transformer自重建模块，通过自适应阈值处理查询特征，同时保持完整的支持特征，增强对判别区域的关注并过滤背景噪声。在三个具有挑战性的细粒度数据集上进行了广泛实验，结果表明HMDRN在Conv-4和ResNet-12骨干架构上始终优于现有最先进的方法。全面的消融研究验证了每个提出组件的有效性，揭示了双层重建增强了类间判别力，而掩码增强转换减少了类内变化。可视化结果提供了HMDRN优越特征重建能力的证据。", "summary": "本文针对少样本细粒度图像分类（FS-FGIC）中现有方法在空间信息利用和判别区域聚焦方面的不足，提出了一种分层掩码增强双重重建网络（HMDRN）。HMDRN通过双层特征重建与融合模块整合不同网络层次的互补信息，并利用空间二值掩码增强Transformer自重建模块聚焦判别区域并抑制背景噪声。实验证明，HMDRN在多个细粒度数据集上显著优于现有技术，且各组件的消融研究验证了其有效性，展示了其在增强类间判别和减少类内变化方面的能力。", "keywords": "少样本学习, 细粒度图像分类, 特征重建, 掩码增强, 深度学习", "comments": "该论文提出了一种新颖的深度学习架构HMDRN，通过结合分层特征重建和掩码增强机制，有效地解决了少样本细粒度图像分类中的两大挑战：空间信息利用不足和判别区域聚焦不力。其创新性在于双层特征融合和引入掩码增强的Transformer自重建模块，这使得模型能够更精细地捕获图像特征并过滤噪声。实验结果表明了其在性能上的显著提升，具有重要的研究价值。"}}
{"id": "2506.20023", "title": "DIM-SUM: Dynamic IMputation for Smart Utility Management", "authors": ["Ryan Hildebrant", "Rahul Bhope", "Sharad Mehrotra", "Christopher Tull", "Nalini Venkatasubramanian"], "summary": "Time series imputation models have traditionally been developed using\ncomplete datasets with artificial masking patterns to simulate missing values.\nHowever, in real-world infrastructure monitoring, practitioners often encounter\ndatasets where large amounts of data are missing and follow complex,\nheterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for\ntraining robust imputation models that bridges the gap between artificially\nmasked training data and real missing patterns. DIM-SUM combines pattern\nclustering and adaptive masking strategies with theoretical learning guarantees\nto handle diverse missing patterns actually observed in the data. Through\nextensive experiments on over 2 billion readings from California water\ndistricts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM\noutperforms traditional methods by reaching similar accuracy with lower\nprocessing time and significantly less training data. When compared against a\nlarge pre-trained model, DIM-SUM averages 2x higher accuracy with significantly\nless inference time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20023v1", "categories": ["cs.LG", "cs.DB"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20023v1", "AI": {"title_translation": "DIM-SUM：智能公用事业管理的动态插补", "tldr": "DIM-SUM是一个预处理框架，用于训练鲁棒的插补模型，解决真实世界中时间序列数据缺失模式复杂的问题。它通过结合模式聚类和自适应掩码策略，在处理大量缺失数据时，比传统方法和大型预训练模型表现出更高的效率和准确性。", "motivation": "传统的时间序列插补模型通常使用人工掩码模式的完整数据集进行开发，但实际基础设施监测中，数据缺失量大且模式复杂异构，导致现有模型难以应对真实场景。", "method": "本文提出了DIM-SUM，一个预处理框架，用于训练鲁棒的插补模型，弥合了人工掩码训练数据与真实缺失模式之间的差距。DIM-SUM结合了模式聚类、自适应掩码策略和理论学习保证，以处理数据中实际观察到的多样化缺失模式。", "result": "在对加州水区、电力数据集和基准测试中超过20亿次读数的广泛实验表明，DIM-SUM在达到相似准确性的同时，处理时间更短，所需的训练数据显著减少，优于传统方法。与大型预训练模型相比，DIM-SUM的平均准确率提高了2倍，推理时间显著减少。", "conclusion": "DIM-SUM通过其创新的预处理框架，成功地解决了时间序列数据中复杂异构缺失模式的挑战，在效率和准确性方面均优于现有方法，证明了其在智能公用事业管理中的强大潜力。", "translation": "时间序列插补模型传统上是使用具有人工掩码模式的完整数据集开发的，以模拟缺失值。然而，在现实世界的基础设施监测中，从业者经常遇到大量数据缺失且遵循复杂、异构模式的数据集。我们引入了DIM-SUM，一个用于训练鲁棒插补模型的预处理框架，它弥合了人工掩码训练数据与真实缺失模式之间的差距。DIM-SUM结合了模式聚类和自适应掩码策略以及理论学习保证，以处理数据中实际观察到的多样化缺失模式。通过对来自加州水区、电力数据集和基准测试中超过20亿次读数的广泛实验，我们证明了DIM-SUM通过以更短的处理时间和显著更少的训练数据达到相似的准确性，从而优于传统方法。与大型预训练模型相比，DIM-SUM的平均准确率提高了2倍，推理时间显著减少。", "summary": "DIM-SUM是一个创新的预处理框架，旨在解决时间序列数据中复杂的真实世界缺失模式问题。它通过结合模式聚类和自适应掩码策略，训练出更鲁棒的插补模型。实验证明，DIM-SUM在处理效率和准确性上均优于传统方法和大型预训练模型，尤其在减少训练数据和推理时间方面表现出色。", "keywords": "时间序列插补, 缺失数据, 预处理, 智能公用事业, 模式聚类", "comments": "DIM-SUM的创新之处在于其预处理框架，它能有效弥合人工掩码与真实缺失模式之间的鸿沟，这对于实际应用中的数据质量和模型鲁棒性至关重要。其结合模式聚类和自适应掩码的策略，使其能更灵活地应对多样化的缺失模式。该研究的重要性在于，它为智能公用事业管理等领域提供了更高效、更准确的数据插补解决方案，降低了对大量训练数据的依赖，并显著缩短了处理时间，具有很强的实用价值。"}}
{"id": "2506.20486", "title": "Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization", "authors": ["Salvatore Milite", "Giulio Caravagna", "Andrea Sottoriva"], "summary": "Neural Cellular Automata (NCAs) are a promising new approach to model\nself-organizing processes, with potential applications in life science.\nHowever, their deterministic nature limits their ability to capture the\nstochasticity of real-world biological and physical systems.\n  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework\nincorporating the idea of mixture models into the NCA paradigm. By combining\nprobabilistic rule assignments with intrinsic noise, MNCAs can model diverse\nlocal behaviors and reproduce the stochastic dynamics observed in biological\nprocesses.\n  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic\nsimulations of tissue growth and differentiation, (2) image morphogenesis\nrobustness, and (3) microscopy image segmentation. Results show that MNCAs\nachieve superior robustness to perturbations, better recapitulate real\nbiological growth patterns, and provide interpretable rule segmentation. These\nfindings position MNCAs as a promising tool for modeling stochastic dynamical\nsystems and studying self-growth processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20486v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20486v1", "AI": {"title_translation": "神经元元胞自动机混合体：一种用于生长建模和自组织的随机框架", "tldr": "本文提出了神经元元胞自动机混合体 (MNCA)，通过引入随机性解决了传统NCA在建模生物过程随机性方面的局限，并在组织生长、图像形态发生和图像分割中展现出优越性。", "motivation": "传统的神经元元胞自动机 (NCAs) 虽然在建模自组织过程方面有潜力，但其确定性限制了它们捕捉真实世界生物和物理系统随机性的能力。", "method": "本文提出了神经元元胞自动机混合体 (MNCA)，这是一种将混合模型思想融入NCA范式的新颖框架。通过结合概率规则分配和内在噪声，MNCA能够模拟多样的局部行为并重现生物过程中观察到的随机动态。", "result": "MNCA在三个关键领域（组织生长和分化、图像形态发生鲁棒性、显微镜图像分割）中进行了评估。结果表明，MNCA对扰动具有卓越的鲁棒性，能更好地重现真实的生物生长模式，并提供可解释的规则分割。", "conclusion": "这些发现将MNCA定位为建模随机动力系统和研究自我生长过程的有前景的工具。", "translation": "神经元元胞自动机 (NCAs) 是一种很有前景的建模自组织过程的新方法，在生命科学中具有潜在应用。然而，它们的确定性限制了它们捕捉真实世界生物和物理系统随机性的能力。\n我们提出了神经元元胞自动机混合体 (MNCA)，这是一种将混合模型思想融入NCA范式的新颖框架。通过结合概率规则分配和内在噪声，MNCA可以模拟多样化的局部行为并重现生物过程中观察到的随机动态。\n我们在三个关键领域评估了MNCA的有效性：(1) 组织生长和分化的合成模拟，(2) 图像形态发生鲁棒性，以及 (3) 显微镜图像分割。结果表明，MNCA对扰动具有卓越的鲁棒性，能更好地重现真实的生物生长模式，并提供可解释的规则分割。这些发现将MNCA定位为建模随机动力系统和研究自我生长过程的有前景的工具。", "summary": "本文提出了神经元元胞自动机混合体 (MNCA)，旨在解决传统神经元元胞自动机 (NCA) 在建模生物系统随机性方面的局限。MNCA通过引入概率规则分配和内在噪声，能够模拟多样化的局部行为和随机动态。实验结果表明，MNCA在模拟组织生长、提高图像形态发生鲁棒性以及显微镜图像分割方面表现出更强的鲁棒性和更准确的生物模式再现能力，使其成为建模随机动态系统和自生长过程的有效工具。", "keywords": "神经元元胞自动机, 混合模型, 随机性, 生长建模, 自组织", "comments": "这项工作通过将混合模型和随机性引入神经元元胞自动机，有效地解决了传统NCA在模拟真实世界生物过程随机性方面的关键限制。其创新性在于为自组织和生长建模提供了一个更具生物学真实性的框架，特别是在处理不确定性和多样性方面。"}}
{"id": "2506.20109", "title": "Evaluating Disassembly Errors With Only Binaries", "authors": ["Lambang Akbar Wijayadi", "Yuancheng Jiang", "Roland H. C. Yap", "Zhenkai Liang", "Zhuohao Liu"], "summary": "Disassemblers are crucial in the analysis and modification of binaries.\nExisting works showing disassembler errors largely rely on practical\nimplementation without specific guarantees and assume source code and compiler\ntoolchains to evaluate ground truth. However, the assumption of source code is\ncontrary to typical binary scenarios where only the binary is available. In\nthis work, we investigate an approach with minimal assumptions and a sound\napproach to disassembly error evaluation that does not require source code. Any\nsource code does not address the fundamental problem of binary disassembly and\nfails when only the binary exists. As far as we know, this is the first work to\nevaluate disassembly errors using only the binary. We propose TraceBin, which\nuses dynamic execution to find disassembly errors. TraceBin targets the use\ncase where the disassembly is used in an automated fashion for security tasks\non a target binary, such as static binary instrumentation, binary hardening,\nautomated code repair, and so on, which may be affected by disassembly errors.\nDiscovering disassembly errors in the target binary aids in reducing problems\ncaused by such errors. Furthermore, we are not aware of existing approaches\nthat can evaluate errors given only a target binary, as they require source\ncode. Our evaluation shows TraceBin finds: (i) errors consistent with existing\nstudies even without source; (ii) disassembly errors due to control flow; (iii)\nnew interesting errors; (iv) errors in non-C/C++ binaries; (v) errors in\nclosed-source binaries; and (vi) show that disassembly errors can have\nsignificant security implications. Overall, our experimental results show that\nTraceBin finds many errors in existing popular disassemblers. It is also\nhelpful in automated security tasks on (closed source) binaries relying on\ndisassemblers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20109v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20109v1", "AI": {"title_translation": "仅使用二进制文件评估反汇编错误", "tldr": "本文提出TraceBin，一种仅使用二进制文件评估反汇编错误的新方法，无需源代码，对自动化安全任务至关重要。", "motivation": "现有的反汇编错误评估方法依赖于源代码，这与实际二进制分析场景相悖。由于反汇编错误可能影响自动化安全任务，因此需要一种无需源代码的稳健评估方法。", "method": "本文提出了TraceBin，该方法利用动态执行来发现反汇编错误。TraceBin旨在解决在目标二进制文件上进行自动化安全任务时（如静态二进制插桩、二进制加固、自动化代码修复等）可能受反汇编错误影响的使用场景。", "result": "TraceBin发现：(i) 即使没有源代码，其错误也与现有研究一致；(ii) 由于控制流导致的反汇编错误；(iii) 新的有趣错误；(iv) 非C/C++二进制文件中的错误；(v) 闭源二进制文件中的错误；(vi) 反汇编错误可能具有重大的安全隐患。实验结果表明TraceBin在现有流行的反汇编器中发现了大量错误。", "conclusion": "TraceBin能够仅使用二进制文件在现有流行的反汇编器中发现大量错误，并且对依赖反汇编器的（闭源）二进制文件上的自动化安全任务有所帮助。", "translation": "反汇编器在二进制文件的分析和修改中至关重要。现有显示反汇编器错误的工作主要依赖于实际实现，没有具体的保证，并假设有源代码和编译器工具链来评估真实情况。然而，源代码的假设与典型的二进制场景相悖，因为在这些场景中通常只有二进制文件可用。在这项工作中，我们研究了一种假设最少且评估反汇编错误的方法，该方法不需要源代码。任何源代码都无法解决二进制反汇编的根本问题，并且在只有二进制文件存在时会失效。据我们所知，这是首次仅使用二进制文件评估反汇编错误的工作。我们提出了TraceBin，它使用动态执行来发现反汇编错误。TraceBin旨在解决反汇编用于目标二进制文件上自动化安全任务的使用场景，例如静态二进制插桩、二进制加固、自动化代码修复等，这些任务可能受到反汇编错误的影响。在目标二进制文件中发现反汇编错误有助于减少此类错误引起的问题。此外，我们不知道现有方法可以在只给定目标二进制文件的情况下评估错误，因为它们需要源代码。我们的评估显示TraceBin发现：(i) 即使没有源代码，其错误也与现有研究一致；(ii) 由于控制流导致的反汇编错误；(iii) 新的有趣错误；(iv) 非C/C++二进制文件中的错误；(v) 闭源二进制文件中的错误；(vi) 反汇编错误可能具有重大的安全隐患。总的来说，我们的实验结果表明TraceBin在现有流行的反汇编器中发现了许多错误。它还有助于依赖反汇编器的（闭源）二进制文件上的自动化安全任务。", "summary": "该论文解决了现有反汇编错误评估方法依赖源代码的局限性，提出了TraceBin，这是一种创新的、仅依赖二进制文件的动态执行方法来发现反汇编错误。TraceBin在多种场景下成功识别出包括控制流错误、新颖错误以及在闭源和非C/C++二进制文件中的错误，并揭示了这些错误对自动化安全任务的重大影响。实验证明TraceBin能有效发现流行反汇编器中的大量错误，对于提升二进制安全任务的准确性至关重要。", "keywords": "反汇编错误, 二进制分析, TraceBin, 动态执行, 安全影响", "comments": "这项工作的创新之处在于首次实现了仅使用二进制文件来评估反汇编错误，这解决了现有方法对源代码的依赖性，使其更适用于实际的二进制分析场景。其重要性在于，通过揭示流行反汇编器中的潜在错误及其对自动化安全任务的影响，TraceBin有助于提高二进制分析工具的可靠性和安全性。"}}
{"id": "2506.20531", "title": "Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios", "authors": ["Wenbin Gan", "Minh-Son Dao", "Koji Zettsu"], "summary": "Driving in safety-critical scenarios requires quick, context-aware\ndecision-making grounded in both situational understanding and experiential\nreasoning. Large Language Models (LLMs), with their powerful general-purpose\nreasoning capabilities, offer a promising foundation for such decision-making.\nHowever, their direct application to autonomous driving remains limited due to\nchallenges in domain adaptation, contextual grounding, and the lack of\nexperiential knowledge needed to make reliable and interpretable decisions in\ndynamic, high-risk environments. To address this gap, this paper presents a\nCase-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for\nevasive maneuver decision-making in complex risk scenarios. Our approach\nintegrates semantic scene understanding from dashcam video inputs with the\nretrieval of relevant past driving cases, enabling LLMs to generate maneuver\nrecommendations that are both context-sensitive and human-aligned. Experiments\nacross multiple open-source LLMs show that our framework improves decision\naccuracy, justification quality, and alignment with human expert behavior.\nRisk-aware prompting strategies further enhance performance across diverse risk\ntypes, while similarity-based case retrieval consistently outperforms random\nsampling in guiding in-context learning. Case studies further demonstrate the\nframework's robustness in challenging real-world conditions, underscoring its\npotential as an adaptive and trustworthy decision-support tool for intelligent\ndriving systems.", "comment": "12 pages, 10 figures, under-review conference", "pdf_url": "http://arxiv.org/pdf/2506.20531v1", "categories": ["cs.AI", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20531v1", "AI": {"title_translation": "基于案例推理增强大型语言模型框架，用于现实安全关键驾驶场景下的决策", "tldr": "大型语言模型（LLMs）在自动驾驶中因缺乏领域适应、上下文接地和经验知识而受限。本文提出了一个基于案例推理增强的大型语言模型（CBR-LLM）框架，通过整合行车记录仪视频的语义场景理解和检索相关过往驾驶案例，使LLMs能够在安全关键的驾驶场景中做出更准确、更符合人类习惯的决策。", "motivation": "大型语言模型（LLMs）尽管具有强大的通用推理能力，但由于在领域适应、上下文接地以及缺乏在动态、高风险环境中做出可靠和可解释决策所需的经验知识方面存在挑战，其在自动驾驶中的直接应用仍然有限。", "method": "本文提出了一个基于案例推理增强的大型语言模型（CBR-LLM）框架，用于复杂风险场景下的规避机动决策。该方法将来自行车记录仪视频输入的语义场景理解与相关过往驾驶案例的检索相结合，使LLMs能够生成既情境敏感又与人类对齐的机动建议。此外，还采用了风险感知提示策略和基于相似度的案例检索。", "result": "在多个开源LLM上的实验表明，该框架提高了决策准确性、理由质量以及与人类专家行为的一致性。风险感知提示策略进一步提升了在不同风险类型下的性能，而基于相似度的案例检索在引导上下文学习方面始终优于随机采样。案例研究进一步证明了该框架在具有挑战性的真实世界条件下的鲁棒性。", "conclusion": "CBR-LLM框架在安全关键驾驶场景下为智能驾驶系统提供了一个潜在的自适应和值得信赖的决策支持工具，解决了LLMs在该领域应用中的关键挑战。", "translation": "在安全关键场景下驾驶需要快速、情境感知且基于情境理解和经验推理的决策。大型语言模型（LLMs）凭借其强大的通用推理能力，为此类决策提供了有前景的基础。然而，由于领域适应、上下文接地以及缺乏在动态、高风险环境中做出可靠和可解释决策所需的经验知识方面的挑战，它们在自动驾驶中的直接应用仍然有限。为了弥补这一空白，本文提出了一个基于案例推理增强的大型语言模型（CBR-LLM）框架，用于复杂风险场景下的规避机动决策。我们的方法将来自行车记录仪视频输入的语义场景理解与相关过往驾驶案例的检索相结合，使LLMs能够生成既情境敏感又与人类对齐的机动建议。在多个开源LLM上的实验表明，我们的框架提高了决策准确性、理由质量以及与人类专家行为的一致性。风险感知提示策略进一步提升了在不同风险类型下的性能，而基于相似度的案例检索在引导上下文学习方面始终优于随机采样。案例研究进一步证明了该框架在具有挑战性的真实世界条件下的鲁棒性，突显了其作为智能驾驶系统自适应且值得信赖的决策支持工具的潜力。", "summary": "本文针对大型语言模型（LLMs）在安全关键自动驾驶场景中缺乏经验知识和情境适应性的问题，提出了一个名为CBR-LLM的框架。该框架通过整合行车记录仪视频的语义场景理解和检索过往驾驶案例，使LLMs能够生成更准确、更符合人类习惯的规避机动决策。实验结果表明，CBR-LLM显著提升了决策准确性、理由质量及与人类行为的一致性，并通过风险感知提示和基于相似度的案例检索进一步优化了性能，展现了其在真实世界条件下的鲁棒性和作为智能驾驶系统决策支持工具的潜力。", "keywords": "案例推理, 大型语言模型, 自动驾驶, 安全关键场景, 决策制定", "comments": "该论文的创新点在于将案例推理（CBR）与大型语言模型（LLM）相结合，有效地弥补了LLM在自动驾驶这一安全关键领域中缺乏经验知识和情境理解的不足。这种结合使得LLM能够从过去的经验中学习，从而做出更可靠、更符合人类直觉的决策。其重要性体现在为LLM在自动驾驶等高风险应用场景中的落地提供了新的思路和解决方案，有望提升自动驾驶系统的可信赖性和安全性。该框架通过增强LLM的决策能力，使其更适用于需要快速、精准判断的复杂驾驶环境。"}}
{"id": "2506.20169", "title": "Causal discovery in deterministic discrete LTI-DAE systems", "authors": ["Bala Rajesh Konkathi", "Arun K. Tangirala"], "summary": "Discovering pure causes or driver variables in deterministic LTI systems is\nof vital importance in the data-driven reconstruction of causal networks. A\nrecent work by Kathari and Tangirala, proposed in 2022, formulated the causal\ndiscovery method as a constraint identification problem. The constraints are\nidentified using a dynamic iterative PCA (DIPCA)-based approach for dynamical\nsystems corrupted with Gaussian measurement errors. The DIPCA-based method\nworks efficiently for dynamical systems devoid of any algebraic relations.\nHowever, several dynamical systems operate under feedback control and/or are\ncoupled with conservation laws, leading to differential-algebraic (DAE) or\nmixed causal systems. In this work, a method, namely the partition of variables\n(PoV), for causal discovery in LTI-DAE systems is proposed. This method is\nsuperior to the method that was presented by Kathari and Tangirala (2022), as\nPoV also works for pure dynamical systems, which are devoid of algebraic\nequations. The proposed method identifies the causal drivers up to a minimal\nsubset. PoV deploys DIPCA to first determine the number of algebraic relations\n($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.\nSubsequently, the subsets are identified through an admissible partitioning of\nthe constraint matrix by finding the condition number of it. Case studies are\npresented to demonstrate the effectiveness of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20169v1", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY", "stat.ME"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20169v1", "AI": {"title_translation": "确定性离散LTI-DAE系统中的因果发现", "tldr": "提出了一种名为变量划分（PoV）的新方法，用于在包含代数关系的LTI-DAE系统中进行因果发现，该方法优于现有方法，并能识别因果驱动变量。", "motivation": "在数据驱动的因果网络重建中，发现确定性LTI系统中的纯粹原因或驱动变量至关重要。现有方法（如DIPCA）在处理包含代数关系（DAE）的动态系统时存在局限性。", "method": "提出了一种名为变量划分（PoV）的方法，用于LTI-DAE系统中的因果发现。PoV首先利用DIPCA确定代数关系数($n_a$)、动态关系数($n_d$)和约束矩阵，然后通过寻找约束矩阵的条件数，对其进行容许划分来识别子集。", "result": "所提出的PoV方法优于Kathari和Tangirala（2022）的方法，因为它也适用于不含代数方程的纯动态系统。该方法能够识别因果驱动变量到一个最小子集。通过案例研究证明了该方法的有效性。", "conclusion": "Not mentioned in abstract", "translation": "在确定性LTI系统中发现纯粹原因或驱动变量对于数据驱动的因果网络重建至关重要。Kathari和Tangirala于2022年提出的一项近期工作，将因果发现方法表述为一个约束识别问题。这些约束是使用基于动态迭代主成分分析（DIPCA）的方法为受高斯测量误差污染的动态系统识别的。基于DIPCA的方法对不含任何代数关系的动态系统有效。然而，一些动态系统在反馈控制下运行和/或与守恒定律耦合，导致微分代数（DAE）或混合因果系统。在这项工作中，提出了一种在LTI-DAE系统中进行因果发现的方法，即变量划分（PoV）。该方法优于Kathari和Tangirala（2022）提出的方法，因为PoV也适用于不含代数方程的纯动态系统。所提出的方法将因果驱动变量识别到一个最小子集。PoV部署DIPCA首先确定代数关系的数量($n_a$)、动态关系的数量($n_d$)和约束矩阵。随后，通过找到约束矩阵的条件数，对其进行容许划分来识别子集。本文提供了案例研究以证明所提出方法的有效性。", "summary": "本文提出了一种名为变量划分（PoV）的新方法，用于在确定性离散线性时不变微分代数（LTI-DAE）系统中进行因果发现。针对现有DIPCA方法在处理含有代数关系的系统时的局限性，PoV方法通过利用DIPCA确定关系数量和约束矩阵，并对其进行划分来识别最小子集中的因果驱动变量。该方法被证明优于现有方法，并适用于纯动态系统以及DAE系统。", "keywords": "Causal discovery, LTI-DAE systems, Partition of variables", "comments": "这篇论文提出了一种创新的方法PoV，解决了在包含代数关系的复杂动态系统（LTI-DAE）中进行因果发现的挑战，而现有方法对此类系统效率不高。其创新点在于将DIPCA与约束矩阵的划分相结合，以更全面地识别因果驱动变量。"}}
{"id": "2506.20336", "title": "A Unified Framework for UAV-Based Free-Space Quantum Links: Beam Shaping and Adaptive Field-of-View Control", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saif Al-Kuwari", "Khalid Qaraqe"], "summary": "This paper develops a comprehensive analytical framework for modeling and\nperformance evaluation of unmanned aerial vehicles (UAVs)-to-ground quantum\ncommunication links, incorporating key physical impairments such as beam\ndivergence, pointing errors at both transmitter and receiver, atmospheric\nattenuation, turbulence-induced fading, narrow field-of-view (FoV) filtering,\nand background photon noise. To overcome the limitations of conventional\nwide-beam assumptions, we introduce a grid-based approximation for photon\ncapture probability that remains accurate under tightly focused beams.\nAnalytical expressions are derived for the quantum key generation rate and\nquantum bit error rate (QBER), enabling fast and reliable system-level\nevaluation. Our results reveal that secure quantum key distribution (QKD) over\nUAV-based free-space optical (FSO) links requires beam waists below 10 cm and\nsub-milliradian tracking precision to achieve Mbps-level key rates and QBER\nbelow $10^{-3}$. Additionally, we highlight the critical role of receiver FoV\nin balancing background noise rejection and misalignment tolerance, and propose\nadaptive FoV tuning strategies under varying illumination and alignment\nconditions. The proposed framework provides a tractable and accurate tool for\nthe design, optimization, and deployment of next-generation airborne quantum\ncommunication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20336v1", "categories": ["eess.SP", "quant-ph"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20336v1", "AI": {"title_translation": "基于无人机自由空间量子链路的统一框架：光束整形和自适应视场控制", "tldr": "本文提出了一个统一的分析框架，用于建模和评估无人机到地面的量子通信链路，考虑了多种物理损伤。研究引入了网格化光子捕获概率近似，并推导了量子密钥生成率和量子误码率的表达式。结果表明，实现安全量子密钥分发需要小光束腰和高跟踪精度，并强调了接收器视场在平衡背景噪声和对准容差方面的关键作用，提出了自适应视场调整策略。", "motivation": "传统的宽光束假设在无人机到地面的量子通信链路建模中存在局限性，且需要一个综合的分析框架来评估考虑多种物理损伤的系统性能。", "method": "本文开发了一个全面的分析框架，用于建模和性能评估无人机到地面的量子通信链路。该框架考虑了光束发散、发射器和接收器的指向误差、大气衰减、湍流引起的衰落、窄视场滤波和背景光子噪声等关键物理损伤。为克服传统宽光束假设的局限性，研究引入了光子捕获概率的网格化近似。此外，还推导了量子密钥生成率和量子误码率的分析表达式。", "result": "研究结果表明，通过基于无人机的自由空间光（FSO）链路实现安全量子密钥分发（QKD）需要光束腰小于10厘米和亚毫弧度级的跟踪精度，才能达到Mbps级的密钥速率和低于$10^{-3}$的QBER。此外，研究强调了接收器视场在平衡背景噪声抑制和错位容差方面的关键作用。", "conclusion": "所提出的框架为下一代机载量子通信系统的设计、优化和部署提供了一个易于处理且准确的工具。", "translation": "本文开发了一个全面的分析框架，用于建模和评估无人机（UAV）到地面的量子通信链路的性能，其中包含了光束发散、发射器和接收器的指向误差、大气衰减、湍流引起的衰落、窄视场（FoV）滤波和背景光子噪声等关键物理损伤。为了克服传统宽光束假设的局限性，我们引入了光子捕获概率的网格化近似，该近似在紧密聚焦的光束下仍能保持准确。本文推导了量子密钥生成率和量子误码率（QBER）的分析表达式，从而能够快速可靠地进行系统级评估。我们的结果表明，通过基于无人机的自由空间光（FSO）链路实现安全量子密钥分发（QKD）需要光束腰小于10厘米和亚毫弧度级的跟踪精度，才能达到Mbps级的密钥速率和低于$10^{-3}$的QBER。此外，我们强调了接收器视场在平衡背景噪声抑制和错位容差方面的关键作用，并提出了在不同光照和对准条件下自适应调整视场的策略。所提出的框架为下一代机载量子通信系统的设计、优化和部署提供了一个易于处理且准确的工具。", "summary": "本文提出了一个用于无人机到地面量子通信链路的统一分析框架，该框架考虑了多种物理损伤。为提高紧聚焦光束下的准确性，引入了光子捕获概率的网格化近似。研究推导了量子密钥生成率和量子误码率的分析表达式，并指出实现安全量子密钥分发需要小光束腰和高跟踪精度。此外，论文强调了接收器视场的重要性，并提出了自适应调整策略，为未来机载量子通信系统的设计提供了工具。", "keywords": "无人机量子通信,自由空间光通信,量子密钥分发,光束整形,自适应视场控制", "comments": "该论文的创新之处在于提出了一个统一的、考虑多种物理损伤的无人机量子通信链路分析框架，并引入了适用于紧聚焦光束的网格化光子捕获概率近似。其重要性在于为下一代机载量子通信系统的设计和优化提供了实用且准确的工具，特别是在处理光束整形和自适应视场控制方面。"}}
{"id": "2506.20128", "title": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation", "authors": ["Aashiq Muhamed"], "summary": "RAG systems enhance LLMs by incorporating external knowledge, which is\ncrucial for domains that demand factual accuracy and up-to-date information.\nHowever, evaluating the multifaceted quality of RAG outputs, spanning aspects\nsuch as contextual coherence, query relevance, factual correctness, and\ninformational completeness, poses significant challenges. Existing evaluation\nmethods often rely on simple lexical overlap metrics, which are inadequate for\ncapturing these nuances, or involve complex multi-stage pipelines with\nintermediate steps like claim extraction or require finetuning specialized\njudge models, hindering practical efficiency. To address these limitations, we\npropose CCRS (Contextual Coherence and Relevance Score), a novel suite of five\nmetrics that utilizes a single, powerful, pretrained LLM as a zero-shot,\nend-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance\n(QR), Information Density (ID), Answer Correctness (AC), and Information Recall\n(IR). We apply CCRS to evaluate six diverse RAG system configurations on the\nchallenging BioASQ dataset. Our analysis demonstrates that CCRS effectively\ndiscriminates between system performances, confirming, for instance, that the\nMistral-7B reader outperforms Llama variants. We provide a detailed analysis of\nCCRS metric properties, including score distributions, convergent/discriminant\nvalidity, tie rates, population statistics, and discriminative power. Compared\nto the complex RAGChecker framework, CCRS offers comparable or superior\ndiscriminative power for key aspects like recall and faithfulness, while being\nsignificantly more computationally efficient. CCRS thus provides a practical,\ncomprehensive, and efficient framework for evaluating and iteratively improving\nRAG systems.", "comment": "Accepted at LLM4Eval @ SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.20128v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20128v1", "AI": {"title_translation": "CCRS：一个用于全面RAG评估的零样本LLM即法官框架", "tldr": "CCRS是一个零样本、LLM即法官框架，用于全面、高效地评估RAG系统。", "motivation": "现有RAG评估方法难以捕捉RAG输出的多方面质量（如语境连贯性、查询相关性、事实正确性、信息完整性），且效率低下，依赖简单的词汇重叠指标或复杂的需要微调的管道。", "method": "本文提出了CCRS（Contextual Coherence and Relevance Score），一套包含五个新指标（语境连贯性、问题相关性、信息密度、答案正确性、信息召回）的框架。该框架利用单个强大、预训练的LLM作为零样本、端到端的判别器来评估RAG系统。", "result": "CCRS能有效区分不同RAG系统配置的性能（例如，Mistral-7B读者优于Llama变体）。对CCRS指标属性的详细分析显示其在判别力上与复杂的RAGChecker框架相当或更优，同时计算效率显著更高。", "conclusion": "CCRS提供了一个实用、全面、高效的框架，用于评估和迭代改进RAG系统。", "translation": "RAG系统通过整合外部知识来增强LLM，这对于需要事实准确性和最新信息的领域至关重要。然而，评估RAG输出的多方面质量，包括语境连贯性、查询相关性、事实正确性以及信息完整性等方面，带来了重大挑战。现有评估方法通常依赖简单的词汇重叠指标，这不足以捕捉这些细微差别，或者涉及复杂的多阶段管道，带有诸如声明提取等中间步骤，或者需要微调专门的判别模型，从而阻碍了实际效率。为了解决这些限制，我们提出了CCRS（语境连贯性和相关性得分），一套包含五个新指标的工具，它利用单个强大、预训练的LLM作为零样本、端到端判别器。CCRS评估：语境连贯性（CC）、问题相关性（QR）、信息密度（ID）、答案正确性（AC）和信息召回（IR）。我们将CCRS应用于BioASQ数据集上评估了六种不同的RAG系统配置。我们的分析表明，CCRS能有效区分系统性能，例如，证实了Mistral-7B阅读器优于Llama变体。我们提供了CCRS指标属性的详细分析，包括得分分布、收敛/判别有效性、平局率、总体统计数据和判别力。与复杂的RAGChecker框架相比，CCRS在召回和忠实度等关键方面提供了相当或更优的判别力，同时计算效率显著更高。因此，CCRS为评估和迭代改进RAG系统提供了一个实用、全面、高效的框架。", "summary": "本文提出了CCRS（Contextual Coherence and Relevance Score），一个零样本、LLM即法官的框架，用于全面评估RAG系统。CCRS包含语境连贯性、问题相关性、信息密度、答案正确性和信息召回五个指标，利用单个预训练LLM作为端到端评估器。实验证明，CCRS能有效区分不同RAG系统性能，并在判别力上与复杂框架相当或更优，同时大幅提升计算效率，为RAG系统评估提供了实用且全面的解决方案。", "keywords": "RAG评估, LLM即法官, 零样本, CCRS, 评估框架", "comments": "CCRS的创新之处在于其“零样本LLM即法官”的方法，通过一个单一的强大LLM解决了RAG多方面质量评估的复杂性和效率问题。这避免了传统方法中复杂的中间步骤或模型微调，大大提高了实用性。其重要性在于为RAG系统的开发和改进提供了一个高效、全面的评估工具，尤其是在对事实准确性要求高的领域。"}}
{"id": "2506.20457", "title": "A Novel Homotopy Perturbation Sumudu Transform Method for Nonlinear Fractional PDEs: Applications and Comparative Analysis", "authors": ["Maryam Jalili"], "summary": "This study introduces the Homotopy Perturbation Sumudu Transform Method\n(HPSTM), a novel hybrid approach combining the Sumudu transform with homotopy\nperturbation to solve nonlinear fractional partial differential equations\n(FPDEs), including fractional porous medium, heat transfer, and Fisher\nequations, using the Caputo fractional derivative. HPSTM leverages the\nlinearity-preserving properties of the Sumudu transform and the flexibility of\nhomotopy perturbation, achieving faster convergence than Laplace-HPM or\nElzaki-HPM for strongly nonlinear FPDEs. Series solutions yield absolute errors\nas low as $3.12 \\times 10^{-3}$ for $\\alpha = 0.9$, with computational times\naveraging 0.5 seconds per example using 5 series terms on standard hardware.\nSolutions are validated against exact solutions, Adomian Decomposition Method\n(ADM), radial basis function (RBF) meshless method, Variational Iteration\nMethod (VIM), Finite Difference Method (FDM), and a spectral method. Numerical\nexamples, sensitivity analysis, and graphical representations for $\\alpha =\n1.0, 0.9, 0.8, 0.7$ confirm HPSTM's accuracy, efficiency, and robustness.\nLimitations include challenges with high-order nonlinearities and\nmulti-dimensional domains. HPSTM shows promise for applications in modeling\nfluid flow in porous media, heat conduction in complex materials, and\nbiological population dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20457v1", "categories": ["math.NA", "cs.NA", "35R11, 44A10, 65M99, 34A08"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20457v1", "AI": {"title_translation": "非线性分数阶偏微分方程的一种新型同伦微扰Sumudu变换方法：应用与比较分析", "tldr": "本研究提出了一种新型混合方法——同伦微扰Sumudu变换方法（HPSTM），用于高效求解非线性分数阶偏微分方程，显示出高精度和快速收敛性。", "motivation": "解决非线性分数阶偏微分方程（FPDEs）的求解问题，旨在开发一种比现有方法（如Laplace-HPM或Elzaki-HPM）收敛更快的方案。", "method": "本研究引入了同伦微扰Sumudu变换方法（HPSTM），这是一种结合了Sumudu变换和同伦微扰的新型混合方法。该方法利用Sumudu变换的线性保持特性和同伦微扰的灵活性，并使用Caputo分数阶导数来求解包括分数阶多孔介质、传热和Fisher方程在内的非线性分数阶偏微分方程。", "result": "HPSTM在强非线性分数阶偏微分方程上比Laplace-HPM或Elzaki-HPM收敛更快。对于$\\\\alpha = 0.9$，级数解的绝对误差低至$3.12 \\\\times 10^{-3}$。在标准硬件上使用5个级数项的每个示例平均计算时间为0.5秒。通过与精确解、Adomian分解法（ADM）、径向基函数（RBF）无网格法、变分迭代法（VIM）、有限差分法（FDM）和谱方法进行验证，证实了HPSTM的准确性、效率和鲁棒性。", "conclusion": "HPSTM是一种精确、高效且鲁棒的求解非线性分数阶偏微分方程的方法。该方法在多孔介质流体流动、复杂材料热传导和生物种群动力学建模方面显示出应用前景。然而，其局限性包括处理高阶非线性和多维领域时的挑战。", "translation": "本研究引入了同伦微扰Sumudu变换方法（HPSTM），这是一种结合了Sumudu变换和同伦微扰的新型混合方法，用于求解非线性分数阶偏微分方程（FPDEs），包括分数阶多孔介质、传热和Fisher方程，并使用Caputo分数阶导数。HPSTM利用了Sumudu变换的线性保持特性和同伦微扰的灵活性，对于强非线性FPDEs，其收敛速度比Laplace-HPM或Elzaki-HPM更快。级数解在$\\alpha = 0.9$时，绝对误差低至$3.12 \\\\times 10^{-3}$，在标准硬件上使用5个级数项的每个示例平均计算时间为0.5秒。解决方案通过与精确解、Adomian分解法（ADM）、径向基函数（RBF）无网格法、变分迭代法（VIM）、有限差分法（FDM）和谱方法进行了验证。针对$\\alpha = 1.0, 0.9, 0.8, 0.7$的数值示例、敏感性分析和图形表示证实了HPSTM的准确性、效率和鲁棒性。局限性包括处理高阶非线性和多维领域时的挑战。HPSTM在多孔介质流体流动、复杂材料热传导和生物种群动力学建模方面显示出应用前景。", "summary": "本研究提出了一种新型混合方法——同伦微扰Sumudu变换方法（HPSTM），用于高效求解非线性分数阶偏微分方程。该方法结合了Sumudu变换和同伦微扰的优点，在处理强非线性问题时展现出比现有方法更快的收敛速度和更高的精度（绝对误差低至$3.12 \\\\times 10^{-3}$）。HPSTM的准确性、效率和鲁棒性通过多种对比方法和不同分数阶参数的数值验证得到证实，并在流体流动、热传导和生物动力学等领域显示出潜在应用价值，尽管在高阶非线性和多维领域存在挑战。", "keywords": "同伦微扰Sumudu变换方法, 分数阶偏微分方程, 非线性, 收敛性, Caputo分数阶导数", "comments": "该论文提出了一种新颖的混合方法HPSTM，结合了两种成熟的数学变换和微扰技术，旨在提高非线性分数阶偏微分方程的求解效率和精度。其创新性在于融合了Sumudu变换的线性保持特性和同伦微扰的灵活性，并在收敛速度上优于现有方法。尽管在处理高阶非线性和多维问题上存在局限，但其在多孔介质流、热传导和生物动力学等领域的应用潜力使其具有重要意义。"}}
{"id": "2506.20394", "title": "SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning", "authors": ["Mimo Shirasaka", "Yuya Ikeda", "Tatsuya Matsushima", "Yutaka Matsuo", "Yusuke Iwasawa"], "summary": "The ability to update information acquired through various means online\nduring task execution is crucial for a general-purpose service robot. This\ninformation includes geometric and semantic data. While SLAM handles geometric\nupdates on 2D maps or 3D point clouds, online updates of semantic information\nremain unexplored. We attribute the challenge to the online scene graph\nrepresentation, for its utility and scalability. Building on previous works\nregarding offline scene graph representations, we study online graph\nrepresentations of semantic information in this work. We introduce SPARK:\nSpatial Perception and Robot Knowledge Integration. This framework extracts\nsemantic information from environment-embedded cues and updates the scene graph\naccordingly, which is then used for subsequent task planning. We demonstrate\nthat graph representations of spatial relationships enhance the robot system's\nability to perform tasks in dynamic environments and adapt to unconventional\nspatial cues, like gestures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20394v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20394v1", "AI": {"title_translation": "SPARK: 基于图的机器人任务规划在线语义集成系统", "tldr": "本文提出了SPARK框架，一个基于图的在线语义集成系统，旨在解决通用服务机器人在线更新语义信息的挑战，从而提高机器人在动态环境下的任务规划能力。", "motivation": "通用服务机器人需要在任务执行期间在线更新几何和语义信息。虽然SLAM已处理几何更新，但语义信息的在线更新，特别是通过在线场景图表示，仍未被充分探索，这构成了研究的挑战和动机。", "method": "本文提出了SPARK（空间感知与机器人知识集成）框架。该框架从环境中嵌入的线索中提取语义信息，并相应地更新场景图，然后将更新后的场景图用于后续的机器人任务规划。", "result": "研究表明，空间关系图表示增强了机器人系统在动态环境中执行任务的能力，并使其能够适应非常规的空间线索，例如手势。", "conclusion": "空间关系图表示对于提高通用服务机器人在动态环境中执行任务和适应非常规空间线索的能力至关重要，弥补了在线语义信息更新的空白。", "translation": "在任务执行期间在线更新通过各种方式获取的信息对于通用服务机器人至关重要。这些信息包括几何和语义数据。虽然SLAM处理2D地图或3D点云上的几何更新，但语义信息的在线更新仍未被探索。我们将这一挑战归因于在线场景图表示，因为它具有实用性和可扩展性。基于先前关于离线场景图表示的工作，我们在这项工作中研究了语义信息的在线图表示。我们引入了SPARK：空间感知与机器人知识集成。该框架从环境中嵌入的线索中提取语义信息并相应地更新场景图，然后将其用于后续的任务规划。我们证明了空间关系图表示增强了机器人系统在动态环境中执行任务和适应非常规空间线索（如手势）的能力。", "summary": "本文介绍了SPARK框架，一个基于图的在线语义集成系统，旨在解决通用服务机器人在线更新语义信息的挑战。该系统通过从环境线索中提取语义信息并实时更新场景图，从而支持动态环境下的机器人任务规划。实验证明，这种空间关系图表示显著提升了机器人在动态环境中执行任务和适应非常规空间提示（如手势）的能力。", "keywords": "机器人任务规划, 在线语义集成, 场景图, 空间感知, 动态环境", "comments": "该论文的创新点在于首次探索了机器人在线语义信息的图表示和集成，弥补了现有SLAM技术仅关注几何更新的不足。通过引入SPARK框架，它提供了一个实用的解决方案，使机器人能够更好地适应动态环境和理解非传统空间线索，对于提升通用服务机器人的自主性和适应性具有重要意义。"}}
{"id": "2506.20272", "title": "Forensic Study of Paintings Through the Comparison of Fabrics", "authors": ["Juan José Murillo-Fuentes", "Pablo M. Olmos", "Laura Alba-Carcelén"], "summary": "The study of canvas fabrics in works of art is a crucial tool for\nauthentication, attribution and conservation. Traditional methods are based on\nthread density map matching, which cannot be applied when canvases do not come\nfrom contiguous positions on a roll. This paper presents a novel approach based\non deep learning to assess the similarity of textiles. We introduce an\nautomatic tool that evaluates the similarity between canvases without relying\non thread density maps. A Siamese deep learning model is designed and trained\nto compare pairs of images by exploiting the feature representations learned\nfrom the scans. In addition, a similarity estimation method is proposed,\naggregating predictions from multiple pairs of cloth samples to provide a\nrobust similarity score. Our approach is applied to canvases from the Museo\nNacional del Prado, corroborating the hypothesis that plain weave canvases,\nwidely used in painting, can be effectively compared even when their thread\ndensities are similar. The results demonstrate the feasibility and accuracy of\nthe proposed method, opening new avenues for the analysis of masterpieces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20272v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20272v1", "AI": {"title_translation": "通过织物比较对绘画作品进行法医研究", "tldr": "本文提出了一种基于深度学习的新方法，用于评估画布相似性，克服了传统方法的局限性，并成功应用于普拉多博物馆的画作。", "motivation": "传统方法基于线密度图匹配，当画布不来自卷轴连续位置时无法应用，因此需要一种新的方法来评估纺织品相似性以进行艺术品鉴定、归属和保护。", "method": "提出了一种基于深度学习的自动工具来评估画布相似性，不依赖线密度图。设计并训练了一个Siamese深度学习模型，通过利用从扫描中学习到的特征表示来比较图像对。此外，提出了一种相似性估计方法，通过聚合多个布料样本对的预测来提供稳健的相似性得分。", "result": "该方法成功应用于普拉多国家博物馆的画布，证实了即使线密度相似，也能有效比较广泛用于绘画的平纹画布。结果证明了所提出方法的可行性和准确性。", "conclusion": "该方法为杰作的分析开辟了新途径。", "translation": "艺术品中画布织物的研究是鉴定、归属和保护的关键工具。传统方法基于线密度图匹配，当画布不来自卷轴的连续位置时无法应用。本文提出了一种基于深度学习的新方法来评估纺织品的相似性。我们引入了一种自动工具，无需依赖线密度图即可评估画布之间的相似性。设计并训练了一个Siamese深度学习模型，通过利用从扫描中学习到的特征表示来比较图像对。此外，提出了一种相似性估计方法，通过聚合多个布料样本对的预测来提供稳健的相似性得分。我们的方法应用于普拉多国家博物馆的画布，证实了广泛用于绘画的平纹画布即使其线密度相似，也能有效比较的假设。结果证明了所提出方法的可行性和准确性，为杰作的分析开辟了新途径。", "summary": "本文提出了一种创新的深度学习方法，用于法医分析绘画作品中的画布织物。该方法通过Siamese深度学习模型和多样本聚合预测，在不依赖传统线密度图的情况下评估画布相似性。实验证明了其在普拉多博物馆画作上的可行性和准确性，为艺术品鉴定和保护提供了新的工具。", "keywords": "画布分析, 深度学习, Siamese网络, 艺术品鉴定, 法医研究", "comments": "这项研究通过引入深度学习克服了传统画布分析方法的局限性，特别是在画布非连续来源的情况下。其创新之处在于利用Siamese网络学习图像特征表示并聚合预测，提供了更稳健的相似性评估。这对于艺术品鉴定和归属具有重要意义，为文化遗产保护提供了强大的新工具。"}}
{"id": "2506.20024", "title": "Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting", "authors": ["Salva Rühling Cachay", "Miika Aittala", "Karsten Kreis", "Noah Brenowitz", "Arash Vahdat", "Morteza Mardani", "Rose Yu"], "summary": "Diffusion models are a powerful tool for probabilistic forecasting, yet most\napplications in high-dimensional chaotic systems predict future snapshots\none-by-one. This common approach struggles to model complex temporal\ndependencies and fails to explicitly account for the progressive growth of\nuncertainty inherent to such systems. While rolling diffusion frameworks, which\napply increasing noise to forecasts at longer lead times, have been proposed to\naddress this, their integration with state-of-the-art, high-fidelity diffusion\ntechniques remains a significant challenge. We tackle this problem by\nintroducing Elucidated Rolling Diffusion Models (ERDM), the first framework to\nsuccessfully unify a rolling forecast structure with the principled, performant\ndesign of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM\ncomponents-its noise schedule, network preconditioning, and Heun sampler-to the\nrolling forecast setting. The success of this integration is driven by three\nkey contributions: (i) a novel loss weighting scheme that focuses model\ncapacity on the mid-range forecast horizons where determinism gives way to\nstochasticity; (ii) an efficient initialization strategy using a pre-trained\nEDM for the initial window; and (iii) a bespoke hybrid sequence architecture\nfor robust spatiotemporal feature extraction under progressive denoising. On 2D\nNavier-Stokes simulations and ERA5 global weather forecasting at 1.5^\\circ\nresolution, ERDM consistently outperforms key diffusion-based baselines,\nincluding conditional autoregressive EDM. ERDM offers a flexible and powerful\ngeneral framework for tackling diffusion-based sequence generation problems\nwhere modeling escalating uncertainty is paramount. Code is available at:\nhttps://github.com/salvaRC/erdm", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20024v1", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20024v1", "AI": {"title_translation": "用于概率天气预报的阐明滚动扩散模型", "tldr": "ERDM统一了滚动预测和EDM，在概率天气预报中优于现有扩散模型，尤其擅长处理不确定性。", "motivation": "现有的扩散模型在处理高维混沌系统时，通常逐一预测未来快照，难以建模复杂时间依赖性，且未能明确考虑固有的不确定性增长。虽然滚动扩散框架被提出，但其与最先进的高保真扩散技术整合仍是挑战。", "method": "引入了阐明滚动扩散模型 (ERDM)，这是第一个成功将滚动预测结构与EDM的原理性高性能设计相结合的框架。通过调整EDM的核心组件（噪声调度、网络预处理和Heun采样器）以适应滚动预测设置。主要贡献包括：(i) 新颖的损失加权方案，将模型能力集中于确定性转变为随机性的中程预测范围；(ii) 使用预训练EDM进行初始窗口的高效初始化策略；(iii) 定制的混合序列架构，用于在渐进去噪下进行鲁棒的时空特征提取。", "result": "在2D Navier-Stokes模拟和1.5°分辨率的ERA5全球天气预报中，ERDM始终优于包括条件自回归EDM在内的关键扩散基线模型。", "conclusion": "ERDM提供了一个灵活且强大的通用框架，用于解决以建模不断升级的不确定性为核心的基于扩散的序列生成问题。", "translation": "扩散模型是概率预测的强大工具，然而，高维混沌系统中的大多数应用都是逐一预测未来的快照。这种常见方法难以建模复杂的时间依赖性，并且未能明确解释此类系统固有的不确定性渐进增长。虽然已经提出了滚动扩散框架（对较长提前期的预测施加越来越大的噪声）来解决这个问题，但它们与最先进、高保真扩散技术的整合仍然是一个重大挑战。我们通过引入阐明滚动扩散模型（ERDM）来解决这个问题，这是第一个成功地将滚动预测结构与阐明扩散模型（EDM）的原理性、高性能设计统一起来的框架。为此，我们调整了核心EDM组件——其噪声调度、网络预处理和Heun采样器——以适应滚动预测设置。这种整合的成功得益于三个关键贡献：(i) 一种新颖的损失加权方案，将模型能力集中在确定性让位于随机性的中程预测范围；(ii) 使用预训练的EDM作为初始窗口的有效初始化策略；(iii) 为在渐进去噪下进行鲁棒时空特征提取而量身定制的混合序列架构。在2D Navier-Stokes模拟和1.5°分辨率的ERA5全球天气预报中，ERDM始终优于包括条件自回归EDM在内的关键基于扩散的基线模型。ERDM为解决以建模不断升级的不确定性至关重要的基于扩散的序列生成问题提供了一个灵活而强大的通用框架。代码可在以下网址获取：https://github.com/salvaRC/erdm", "summary": "本文介绍了阐明滚动扩散模型（ERDM），该模型首次成功地将滚动预测框架与阐明扩散模型（EDM）的高性能设计相结合，旨在解决高维混沌系统中概率预测的复杂时间依赖性和不确定性增长问题。ERDM通过调整EDM的核心组件并引入新的损失加权、高效初始化和混合序列架构等关键贡献，在2D Navier-Stokes模拟和ERA5全球天气预报中展现出优于现有扩散模型的性能，为处理具有不断增长不确定性的序列生成问题提供了一个通用且强大的框架。", "keywords": "扩散模型, 概率预测, 天气预报, 滚动扩散, 不确定性建模", "comments": "ERDM的创新在于首次成功整合了滚动预测框架与先进的EDM，特别是在处理高维混沌系统中的不确定性增长方面。其提出的损失加权方案、高效初始化和混合序列架构是重要的技术贡献。该模型在天气预报等领域具有重要应用潜力，为概率序列生成问题提供了新的解决方案。"}}
{"id": "2506.20504", "title": "Engineering Sentience", "authors": ["Konstantin Demin", "Taylor Webb", "Eric Elmoznino", "Hakwan Lau"], "summary": "We spell out a definition of sentience that may be useful for designing and\nbuilding it in machines. We propose that for sentience to be meaningful for AI,\nit must be fleshed out in functional, computational terms, in enough detail to\nallow for implementation. Yet, this notion of sentience must also reflect\nsomething essentially 'subjective', beyond just having the general capacity to\nencode perceptual content. For this specific functional notion of sentience to\noccur, we propose that certain sensory signals need to be both assertoric\n(persistent) and qualitative. To illustrate the definition in more concrete\nterms, we sketch out some ways for potential implementation, given current\ntechnology. Understanding what it takes for artificial agents to be\nfunctionally sentient can also help us avoid creating them inadvertently, or at\nleast, realize that we have created them in a timely manner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20504v1", "categories": ["cs.AI", "q-bio.NC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20504v1", "AI": {"title_translation": "工程化感知", "tldr": "论文提出了一个可用于在机器中设计和构建感知能力的定义，并强调其功能性和主观性，通过断言性和定性信号实现，并讨论了潜在的实现方式和避免无意中创建感知AI的意义。", "motivation": "该研究旨在为在机器中设计和构建感知能力提供一个有用的定义，并使其在AI领域具有实际意义，同时避免无意中创造出感知型AI。", "method": "论文提出了一个将感知定义为功能性、计算性的概念，并强调其必须包含“主观性”。具体而言，它提出需要某些感官信号同时具有断言性（持久性）和定性。为具体说明，论文还勾勒了基于现有技术的潜在实现方式。", "result": "论文提出了一个详细的功能性、计算性感知定义，该定义要求感官信号具有断言性和定性特征。此外，还探讨了该定义在当前技术下的潜在实现方法。", "conclusion": "理解人工代理如何实现功能性感知，不仅有助于主动设计和构建感知能力，也能帮助我们避免无意中创造出感知型AI，或至少能及时识别出它们的出现。", "translation": "我们阐述了一个感知能力定义，该定义可能有助于在机器中设计和构建感知能力。我们提出，为了使感知能力对人工智能有意义，它必须以功能性、计算性的术语详细阐述，其细节足以允许实现。然而，这种感知概念也必须反映某种本质上的“主观性”，而不仅仅是具有编码感知内容的一般能力。为了使这种特定的功能性感知概念发生，我们提出某些感官信号需要既是断言性（持久的）又是定性的。为了更具体地说明该定义，我们根据当前技术勾勒了一些潜在的实现方式。理解人工代理需要具备什么才能实现功能性感知，也可以帮助我们避免无意中创造出它们，或者至少能及时认识到我们已经创造了它们。", "summary": "这篇论文提出了一个用于在机器中设计和构建感知能力的功能性、计算性定义。该定义强调感知不仅需要可实现，还必须包含主观性，并具体指出这需要感官信号同时具备断言性和定性特征。文章还探讨了基于现有技术的潜在实现方法，并指出理解功能性感知对于避免无意中创造出感知型AI的重要性。", "keywords": "感知, 人工智能, 功能性定义, 主观性, 机器意识", "comments": "这篇论文通过尝试将“感知”这一高度抽象和哲学化的概念转化为可操作、可计算的工程定义，展现了其创新性。它不仅提出了理论框架，还初步探讨了实现路径，这对于AI伦理和未来AI发展具有重要指导意义，尤其是在避免无意中创造出潜在敏感AI方面。然而，将主观性量化或工程化仍然是一个巨大的挑战。"}}
{"id": "2506.20170", "title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript Deobfuscation", "authors": ["Guoqiang Chen", "Xin Jin", "Zhiqiang Lin"], "summary": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement.", "comment": "Accepted by ACM CCS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20170v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20170v1", "AI": {"title_translation": "JsDeObsBench：衡量和评估大型语言模型在JavaScript去混淆方面的表现", "tldr": "JsDeObsBench是一个新的基准测试，旨在系统评估大型语言模型（LLMs）在JavaScript去混淆方面的表现。研究发现LLMs在代码简化方面表现出色，但在语法准确性和执行可靠性方面仍面临挑战，并展示了其在恶意软件去混淆中的潜力。", "motivation": "JavaScript代码去混淆是网络安全中的一个重大挑战，因为混淆技术常用于隐藏恶意活动。尽管大型语言模型（LLMs）在自动化去混淆过程方面显示出前景，但目前缺乏一个系统性的基准来量化其有效性和局限性。", "method": "本研究提出了JsDeObsBench，一个专门用于严格评估LLMs在JS去混淆背景下有效性的基准测试。研究详细介绍了其基准测试方法，包括从基本变量重命名到复杂结构转换的各种混淆技术，并对GPT-4o、Mixtral、Llama和DeepSeek-Coder等前沿LLMs的熟练程度进行了广泛的实验分析，并进一步评估了JS恶意软件的去混淆。", "result": "实验分析揭示了LLMs在代码简化方面表现出色，尽管与基线方法相比，在保持语法准确性和执行可靠性方面仍面临挑战。研究还展示了LLMs在安全场景（如JS恶意软件去混淆）中的潜力。", "conclusion": "研究结果突出了大型语言模型在去混淆应用中的实用性，并指出了需要进一步改进的关键领域。", "translation": "JavaScript (JS) 代码去混淆在网络安全中提出了重大挑战，特别是当混淆技术常用于隐藏脚本中的恶意活动时。尽管大型语言模型（LLMs）最近在自动化去混淆过程方面显示出前景，改变了针对这些混淆威胁的检测和缓解策略，但目前显著缺乏一个系统性的基准来量化其有效性和局限性。为了弥补这一空白，我们提出了 JsDeObsBench，一个专门设计的基准测试，旨在严格评估 LLMs 在 JS 去混淆背景下的有效性。我们详细介绍了我们的基准测试方法，其中包括从基本变量重命名到复杂结构转换的各种混淆技术，为评估 LLM 在真实场景中的性能提供了一个强大的框架。我们广泛的实验分析调查了尖端 LLMs（例如 GPT-4o、Mixtral、Llama 和 DeepSeek-Coder）的熟练程度，揭示了尽管在保持语法准确性和执行可靠性方面与基线方法相比存在挑战，但在代码简化方面表现出色。我们进一步评估了 JS 恶意软件的去混淆，以展示 LLMs 在安全场景中的潜力。研究结果突出了 LLMs 在去混淆应用中的实用性，并指出了需要进一步改进的关键领域。", "summary": "JsDeObsBench是一个新颖的基准测试，旨在系统评估大型语言模型（LLMs）在JavaScript去混淆方面的性能。该研究通过包含多种混淆技术的测试框架，对GPT-4o、Mixtral、Llama和DeepSeek-Coder等LLMs进行了广泛实验。结果表明，LLMs在代码简化方面表现出卓越能力，但仍需在语法准确性和执行可靠性上改进。研究还展示了LLMs在去混淆恶意JavaScript代码方面的潜力，强调了其在网络安全领域的应用前景和未来发展方向。", "keywords": "JavaScript去混淆, 大型语言模型, 基准测试, 网络安全, 代码简化", "comments": "JsDeObsBench通过提供一个系统性的基准来填补了LLMs在JavaScript去混淆领域评估的空白，这具有重要的创新意义。该研究不仅量化了LLMs在该任务中的有效性，还明确指出了其在语法准确性和执行可靠性方面的局限性，为未来的研究指明了方向。其在恶意软件去混淆方面的应用潜力，也凸显了该研究在实际网络安全场景中的重要性。"}}
{"id": "2506.20424", "title": "Active RIS Enabled NLoS LEO Satellite Communications: A Three-timescale Optimization Framework", "authors": ["Ziwei Liu", "Junyan He", "Shanshan Zhao", "Meng Hua", "Bin Lyu", "Xinjie Zhao", "Gengxin Zhang"], "summary": "In this letter, we study an active reconfigurable intelligent surfaces (RIS)\nassisted Low Earth orbit (LEO) satellite communications under non-line-of-sight\n(NLoS) scenarios, where the active RIS is deployed to create visual\nline-of-sight links for reliable communication. To address the challenges of\nhigh energy consumption caused by frequent beamforming updates in active RIS,\nwe propose a three-timescale optimization framework that jointly designs the\ntransmit beamforming, RIS beamforming, and RIS direction vectors based on their\ncharacteristics. The goal is to maximize the system achievable rate while\nreducing energy consumption by controlling the RIS beamforming switching\nfrequency. Then, a two-layer solution framework is developed, incorporating\nfractional programming (FP), alternating optimization (AO), successive\napproximation (SCA), and penalty-based methods, to obtain the optimized\nsolution. Simulation results demonstrate that the proposed scheme can\neffectively improve system performance and reduce the energy consumption of the\nactive RIS.", "comment": "5 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20424v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20424v1", "AI": {"title_translation": "主动RIS赋能的非视距LEO卫星通信：一个三时间尺度优化框架", "tldr": "研究主动RIS辅助的非视距LEO卫星通信，提出三时间尺度优化框架以最大化系统速率并降低能耗，通过仿真验证了其有效性。", "motivation": "在非视距(NLoS)场景下，主动可重构智能表面(RIS)能为LEO卫星通信创建视距链路，但主动RIS频繁的波束成形更新导致高能耗。", "method": "提出一个三时间尺度优化框架，联合设计发射波束成形、RIS波束成形和RIS方向向量，以最大化系统可达速率并降低能耗。开发了两层解决方案框架，结合了分数规划(FP)、交替优化(AO)、逐次逼近(SCA)和基于惩罚的方法来获得优化解。", "result": "仿真结果表明，所提出的方案能有效提高系统性能并降低主动RIS的能耗。", "conclusion": "提出的三时间尺度优化框架在非视距LEO卫星通信中，能够有效平衡系统性能提升和主动RIS能耗降低。", "translation": "在这封信中，我们研究了在非视距（NLoS）场景下，主动可重构智能表面（RIS）辅助的低地球轨道（LEO）卫星通信，其中部署主动RIS以创建可见视距链路实现可靠通信。为了解决主动RIS频繁波束成形更新引起的高能耗挑战，我们提出了一个三时间尺度优化框架，根据其特性联合设计发射波束成形、RIS波束成形和RIS方向向量。目标是通过控制RIS波束成形切换频率来最大化系统可达速率同时降低能耗。然后，开发了一个两层解决方案框架，结合了分数规划（FP）、交替优化（AO）、逐次逼近（SCA）和基于惩罚的方法，以获得优化解。仿真结果表明，所提出的方案能够有效提高系统性能并降低主动RIS的能耗。", "summary": "本文针对非视距LEO卫星通信中主动RIS的高能耗问题，提出了一个三时间尺度优化框架。该框架联合优化了发射波束成形、RIS波束成形和RIS方向向量，旨在最大化系统速率并降低能耗。通过结合分数规划、交替优化等技术，仿真结果验证了所提方案在提升系统性能和节能方面的有效性。", "keywords": "主动RIS, LEO卫星通信, 非视距, 三时间尺度优化, 能耗效率", "comments": "这篇论文通过引入三时间尺度优化框架来解决主动RIS在非视距LEO卫星通信中面临的高能耗挑战，具有创新性。其结合多种优化方法的两层解决方案框架，为复杂系统优化提供了实用思路，对未来高效能耗的卫星通信系统设计具有指导意义。"}}
{"id": "2506.20293", "title": "Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion", "authors": ["Kunjing Yang", "Libin Zheng", "Minru Bai", "Ting Lu", "Leyuan Fang"], "summary": "The blind fusion of unregistered hyperspectral images (HSIs) and\nmultispectral images (MSIs) has attracted growing attention recently. To\naddress the registration challenge, most existing methods employ spatial\ntransformations on the HSI to achieve alignment with the MSI. However, due to\nthe substantial differences in spatial resolution of the images, the\nperformance of these methods is often unsatisfactory. Moreover, the\nregistration process tends to be time-consuming when dealing with large-sized\nimages in remote sensing. To address these issues, we propose tackling the\nregistration problem from the spectral domain. Initially, a lightweight\nSpectral Prior Learning (SPL) network is developed to extract spectral features\nfrom the HSI and enhance the spectral resolution of the MSI. Following this,\nthe obtained image undergoes spatial downsampling to produce the registered\nHSI. In this process, subspace representation and cyclic training strategy are\nemployed to improve spectral accuracy of the registered HSI obtained. Next, we\npropose a blind sparse fusion (BSF) method, which utilizes group sparsity\nregularization to equivalently promote the low-rankness of the image. This\napproach not only circumvents the need for rank estimation, but also reduces\ncomputational complexity. Then, we employ the Proximal Alternating Optimization\n(PAO) algorithm to solve the BSF model, and present its convergence analysis.\nFinally, extensive numerical experiments on simulated and real datasets are\nconducted to verify the effectiveness of our method in registration and fusion.\nWe also demonstrate its efficacy in enhancing classification performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20293v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20293v1", "AI": {"title_translation": "突破空间界限：光谱域配准引导的高光谱与多光谱盲融合", "tldr": "本文提出了一种光谱域配准引导的高光谱和多光谱盲融合方法，旨在解决现有空间域配准方法在处理未配准高光谱图像(HSI)和多光谱图像(MSI)时因空间分辨率差异大而导致的性能不佳和耗时问题，并通过实验验证了其在配准、融合和分类性能提升方面的有效性。", "motivation": "现有针对未配准高光谱图像（HSI）和多光谱图像（MSI）的盲融合方法，大多通过对HSI进行空间变换来实现与MSI的对齐。然而，由于图像空间分辨率的显著差异，这些方法的性能往往不尽人意。此外，在处理遥感领域的大尺寸图像时，配准过程也往往非常耗时。", "method": "本文提出从光谱域解决配准问题。首先，开发了一个轻量级光谱先验学习（SPL）网络，用于从HSI中提取光谱特征并增强MSI的光谱分辨率。随后，对获得的图像进行空间下采样以生成配准后的HSI，此过程中采用子空间表示和循环训练策略来提高配准HSI的光谱精度。其次，提出了一种盲稀疏融合（BSF）方法，利用组稀疏正则化等效地促进图像的低秩性，从而避免了秩估计并降低了计算复杂性。最后，采用近端交替优化（PAO）算法求解BSF模型，并进行了收敛性分析。", "result": "在模拟和真实数据集上进行了广泛的数值实验，验证了所提方法在配准和融合方面的有效性。同时，也证明了其在增强分类性能方面的功效。", "conclusion": "本文提出的光谱域配准引导的高光谱与多光谱盲融合方法，有效解决了传统空间域配准面临的挑战，并在配准、融合及分类性能上展现出优越性。", "translation": "未配准高光谱图像（HSI）和多光谱图像（MSI）的盲融合最近引起了越来越多的关注。为了解决配准挑战，大多数现有方法采用对HSI进行空间变换以实现与MSI的对齐。然而，由于图像空间分辨率的显著差异，这些方法的性能往往不尽人意。此外，在处理遥感领域的大尺寸图像时，配准过程也往往非常耗时。为了解决这些问题，我们提出从光谱域解决配准问题。最初，开发了一个轻量级光谱先验学习（SPL）网络，用于从HSI中提取光谱特征并增强MSI的光谱分辨率。随后，对获得的图像进行空间下采样以生成配准后的HSI。在此过程中，采用子空间表示和循环训练策略来提高所获得的配准HSI的光谱精度。接下来，我们提出了一种盲稀疏融合（BSF）方法，该方法利用组稀疏正则化等效地促进图像的低秩性。这种方法不仅避免了秩估计的需要，而且降低了计算复杂性。然后，我们采用近端交替优化（PAO）算法求解BSF模型，并提出了其收敛性分析。最后，在模拟和真实数据集上进行了广泛的数值实验，以验证我们方法在配准和融合方面的有效性。我们还证明了其在增强分类性能方面的功效。", "summary": "本文提出了一种新颖的光谱域方法，用于未配准高光谱（HSI）和多光谱（MSI）图像的盲融合，旨在克服传统空间域方法因空间分辨率差异和高计算成本导致的局限性。该方法包含一个光谱先验学习（SPL）网络，用于光谱特征提取和MSI光谱分辨率增强，随后通过空间下采样进行HSI配准。文中还引入了一种利用组稀疏正则化的盲稀疏融合（BSF）方法，并通过近端交替优化（PAO）算法求解。实验结果证实了该方法在图像配准、融合以及提升分类性能方面的有效性。", "keywords": "高光谱, 多光谱, 盲融合, 光谱域配准, 组稀疏", "comments": "本文的创新点在于将高光谱和多光谱图像的配准问题从传统的空间域转移到光谱域，巧妙地规避了因图像空间分辨率差异巨大而带来的挑战。通过引入轻量级光谱先验学习（SPL）网络进行光谱特征提取和增强，以及采用基于组稀疏正则化的盲稀疏融合（BSF）方法，不仅提升了融合效果，还显著降低了计算复杂性并避免了复杂的秩估计。这种方法对于遥感等领域中需要高效、准确融合多源异构图像的应用具有重要意义。"}}
{"id": "2506.20160", "title": "AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control", "authors": ["Ruosen Li", "Ziming Luo", "Quan Zhang", "Ruochen Li", "Ben Zhou", "Ali Payani", "Xinya Du"], "summary": "Large reasoning models (LRMs) achieve impressive reasoning capabilities by\ngenerating lengthy chain-of-thoughts, but this \"overthinking\" incurs high\nlatency and cost without commensurate accuracy gains. In this work, we\nintroduce AALC, a lightweight, accuracy-aware length reward integrated into\nreinforcement learning that dynamically balances correctness and brevity during\ntraining. By incorporating validation accuracy into the reward and employing a\nsmooth, dynamically scheduled length penalty, AALC delays length penalty until\ntarget performance is met. Through extensive experiments across standard and\nout-of-distribution math benchmarks, we show that our approach reduces response\nlength by over 50% while maintaining or even improving the original accuracy.\nFurthermore, qualitative analysis reveals that our method curbs redundant\nreasoning patterns such as excessive subgoal setting and verification, leading\nto structurally refined outputs rather than naive truncation. We also identify\nthat efficiency gains are accompanied by reduced interpretability: models\ntrained with AALC omit some narrative framing and explanatory context. These\nfindings highlight the potential of reward-based strategies to guide LRMs\ntoward more efficient, generalizable reasoning paths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20160v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20160v1", "AI": {"title_translation": "AALC：通过自适应精度-长度控制实现大型语言模型高效推理", "tldr": "AALC是一种基于强化学习的方法，通过引入精度感知长度奖励和动态长度惩罚，在保持或提高准确性的同时，将大型推理模型的响应长度缩短了50%以上，从而实现了高效推理。", "motivation": "大型推理模型（LRMs）通过生成冗长的思维链实现强大的推理能力，但这种“过度思考”导致高延迟和高成本，且未带来相应的准确性提升。", "method": "本文提出了AALC，一种轻量级、精度感知的长度奖励机制，将其整合到强化学习中，在训练过程中动态平衡正确性和简洁性。AALC通过将验证准确性纳入奖励，并采用平滑、动态调度的长度惩罚，延迟长度惩罚直到达到目标性能。", "result": "在标准和分布外数学基准上的大量实验表明，我们的方法将响应长度减少了50%以上，同时保持甚至提高了原始准确性。定性分析显示，我们的方法抑制了冗余的推理模式（如过度的子目标设置和验证），从而产生结构更精炼的输出，而非简单的截断。我们还发现效率提升伴随着可解释性降低：使用AALC训练的模型省略了一些叙述性框架和解释性上下文。", "conclusion": "这些发现突出了基于奖励的策略在引导大型推理模型走向更高效、更具泛化性的推理路径方面的潜力。", "translation": "大型推理模型（LRMs）通过生成冗长的思维链实现了令人印象深刻的推理能力，但这种“过度思考”导致高延迟和高成本，且未带来相应的准确性提升。在这项工作中，我们引入了AALC，一种轻量级、精度感知的长度奖励机制，将其整合到强化学习中，在训练过程中动态平衡正确性和简洁性。通过将验证准确性纳入奖励并采用平滑、动态调度的长度惩罚，AALC延迟长度惩罚直到达到目标性能。通过在标准和分布外数学基准上的大量实验，我们表明我们的方法将响应长度减少了50%以上，同时保持甚至提高了原始准确性。此外，定性分析表明，我们的方法抑制了冗余的推理模式，例如过度的子目标设置和验证，从而产生结构更精炼的输出，而非简单的截断。我们还发现效率提升伴随着可解释性降低：使用AALC训练的模型省略了一些叙述性框架和解释性上下文。这些发现突出了基于奖励的策略在引导LRMs走向更高效、更具泛化性的推理路径方面的潜力。", "summary": "本文提出AALC，一种基于强化学习的自适应精度-长度控制方法，旨在提高大型推理模型（LRMs）的推理效率。通过在奖励中融入验证准确性并实施动态长度惩罚，AALC能在训练中平衡模型输出的正确性和简洁性。实验证明，AALC能将模型响应长度缩短50%以上，同时保持或提升准确性，并优化推理结构。尽管效率提升可能牺牲部分可解释性，但AALC展示了通过奖励机制引导LRMs实现高效且泛化推理的潜力。", "keywords": "大型语言模型, 高效推理, 强化学习, 精度-长度控制, 思维链", "comments": "AALC的创新之处在于其将精度感知融入强化学习的奖励机制中，并采用动态长度惩罚，有效解决了大型推理模型“过度思考”导致的效率低下问题。其重要性在于，在不牺牲准确性的前提下显著降低了推理成本和延迟，这对于实际应用具有重大意义。然而，论文也指出了一个局限性，即效率提升可能伴随着可解释性的降低，这提示了未来研究需要平衡效率与模型透明度。"}}
{"id": "2506.20181", "title": "Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks", "authors": ["Ronald Katende"], "summary": "We develop a principled framework for discovering causal structure in partial\ndifferential equations (PDEs) using physics-informed neural networks and\ncounterfactual perturbations. Unlike classical residual minimization or sparse\nregression methods, our approach quantifies operator-level necessity through\nfunctional interventions on the governing dynamics. We introduce causal\nsensitivity indices and structural deviation metrics to assess the influence of\ncandidate differential operators within neural surrogates. Theoretically, we\nprove exact recovery of the causal operator support under restricted isometry\nor mutual coherence conditions, with residual bounds guaranteeing\nidentifiability. Empirically, we validate the framework on both synthetic and\nreal-world datasets across climate dynamics, tumor diffusion, and ocean flows.\nOur method consistently recovers governing operators even under noise,\nredundancy, and data scarcity, outperforming standard PINNs and DeepONets in\nstructural fidelity. This work positions causal PDE discovery as a tractable\nand interpretable inference task grounded in structural causal models and\nvariational residual analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20181v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20181v1", "AI": {"title_translation": "偏微分方程中因果算子发现：基于反事实物理信息神经网络", "tldr": "该研究提出一种基于反事实物理信息神经网络的框架，用于发现偏微分方程中的因果结构并识别其核心算子。", "motivation": "传统方法（如残差最小化或稀疏回归）难以量化算子层面的必要性，且在噪声、冗余和数据稀缺条件下表现不佳。", "method": "开发了一个结合物理信息神经网络和反事实扰动的框架，通过对控制动力学进行函数干预来量化算子层面的必要性。引入了因果敏感性指数和结构偏差度量来评估候选微分算子的影响。", "result": "理论上证明了在特定条件下能精确恢复因果算子支持，并有残差边界保证可识别性。经验上在气候动力学、肿瘤扩散和海洋流等合成及真实数据集上验证了该框架，即使在噪声、冗余和数据稀缺下也能一致地恢复控制算子，并在结构保真度方面优于标准PINNs和DeepONets。", "conclusion": "该工作将因果偏微分方程发现定位为一个基于结构因果模型和变分残差分析的可处理和可解释的推理任务。", "translation": "我们开发了一个原则性框架，利用物理信息神经网络和反事实扰动来发现偏微分方程（PDEs）中的因果结构。与经典的残差最小化或稀疏回归方法不同，我们的方法通过对控制动力学进行函数干预来量化算子层面的必要性。我们引入了因果敏感性指数和结构偏差度量，以评估神经网络代理中候选微分算子的影响。在理论上，我们证明了在受限等距或互相关条件下可以精确恢复因果算子支持，并通过残差边界保证了可识别性。在经验上，我们在气候动力学、肿瘤扩散和海洋流等领域的合成和真实世界数据集上验证了该框架。我们的方法即使在噪声、冗余和数据稀缺的情况下，也能一致地恢复控制算子，并在结构保真度方面优于标准PINNs和DeepONets。这项工作将因果偏微分方程发现定位为一个基于结构因果模型和变分残差分析的可处理和可解释的推理任务。", "summary": "本文提出一种新颖的框架，利用反事实物理信息神经网络（PINNs）来发现偏微分方程（PDEs）中的因果结构和关键算子。该方法通过函数干预量化算子必要性，并引入因果敏感性指数和结构偏差度量。理论上证明了其精确恢复能力，并在经验上展现了在噪声和数据稀缺下的优越性能，超越了传统PINNs和DeepONets，为PDE的因果发现提供了可解释的推理途径。", "keywords": "因果算子发现, 偏微分方程, 物理信息神经网络, 反事实扰动, 结构因果模型", "comments": "该论文的创新之处在于将因果推断引入偏微分方程的算子发现中，通过反事实扰动和物理信息神经网络的结合，解决了传统方法难以量化算子必要性的问题。其理论保证和在复杂条件下的稳健表现，使其在科学发现和工程建模领域具有重要意义。"}}
{"id": "2506.20399", "title": "Multimodal Behaviour Trees for Robotic Laboratory Task Automation", "authors": ["Hatem Fakhruldeen", "Arvind Raveendran Nambiar", "Satheeshkumar Veeramani", "Bonilkumar Vijaykumar Tailor", "Hadi Beyzaee Juneghani", "Gabriella Pizzuto", "Andrew Ian Cooper"], "summary": "Laboratory robotics offer the capability to conduct experiments with a high\ndegree of precision and reproducibility, with the potential to transform\nscientific research. Trivial and repeatable tasks; e.g., sample transportation\nfor analysis and vial capping are well-suited for robots; if done successfully\nand reliably, chemists could contribute their efforts towards more critical\nresearch activities. Currently, robots can perform these tasks faster than\nchemists, but how reliable are they? Improper capping could result in human\nexposure to toxic chemicals which could be fatal. To ensure that robots perform\nthese tasks as accurately as humans, sensory feedback is required to assess the\nprogress of task execution. To address this, we propose a novel methodology\nbased on behaviour trees with multimodal perception. Along with automating\nrobotic tasks, this methodology also verifies the successful execution of the\ntask, a fundamental requirement in safety-critical environments. The\nexperimental evaluation was conducted on two lab tasks: sample vial capping and\nlaboratory rack insertion. The results show high success rate, i.e., 88% for\ncapping and 92% for insertion, along with strong error detection capabilities.\nThis ultimately proves the robustness and reliability of our approach and that\nusing multimodal behaviour trees should pave the way towards the next\ngeneration of robotic chemists.", "comment": "7 pages, 5 figures, accepted and presented in ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.20399v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20399v1", "AI": {"title_translation": "用于机器人实验室任务自动化的多模态行为树", "tldr": "该论文提出了一种基于多模态行为树的方法，用于可靠地自动化实验室任务，并在小瓶盖合和实验室架插入任务中展示了高成功率和强大的错误检测能力。", "motivation": "目前实验室机器人执行任务速度快，但可靠性不足，这在安全关键环境中至关重要（例如，不正确的盖合可能导致人类接触有毒化学品）。需要传感器反馈来确保机器人能像人类一样准确地执行任务。", "method": "本文提出了一种基于“多模态感知行为树”的新颖方法，旨在自动化机器人任务并验证任务的成功执行。", "result": "在两个实验室任务（样品瓶盖合和实验室架插入）上的实验评估显示出高成功率：盖合任务达到88%，插入任务达到92%，同时具有强大的错误检测能力。", "conclusion": "该方法证明了使用多模态行为树的鲁棒性和可靠性，预示着其将为下一代机器人化学家铺平道路。", "translation": "实验室机器人能够以高精度和可重复性进行实验，并有潜力改变科学研究。琐碎和可重复的任务，例如用于分析的样品运输和小瓶盖合，非常适合机器人；如果成功且可靠地完成，化学家就可以将精力投入到更关键的研究活动中。目前，机器人可以比化学家更快地执行这些任务，但它们的可靠性如何？不正确的盖合可能导致人类接触有毒化学品，这可能是致命的。为了确保机器人像人类一样准确地执行这些任务，需要感官反馈来评估任务执行的进展。为了解决这个问题，我们提出了一种基于多模态感知的行为树的新颖方法。除了自动化机器人任务外，这种方法还验证了任务的成功执行，这是安全关键环境中的基本要求。实验评估在两个实验室任务中进行：样品瓶盖合和实验室架插入。结果显示出高成功率，即盖合任务达到88%，插入任务达到92%，同时具有强大的错误检测能力。这最终证明了我们方法的鲁棒性和可靠性，并且使用多模态行为树应该为下一代机器人化学家铺平道路。", "summary": "该论文提出了一种利用多模态行为树的新方法，以自动化和验证机器人实验室任务的执行。该方法解决了当前实验室机器人可靠性不足的问题，这在安全敏感环境中至关重要。通过在小瓶盖合和实验室架插入任务上的实验，该方法展现了高成功率（分别为88%和92%）以及有效的错误检测能力，从而验证了所提出系统在未来机器人化学家应用中的鲁棒性和可靠性。", "keywords": "多模态行为树, 机器人自动化, 实验室任务, 可靠性, 错误检测", "comments": "该论文的创新之处在于将多模态感知与行为树相结合，以提高机器人实验室自动化的可靠性和安全性，这对于处理敏感材料和确保人类安全至关重要。"}}
{"id": "2506.20279", "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios", "authors": ["Changliang Xia", "Chengyou Jia", "Zhuohang Dang", "Minnan Luo"], "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20279v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20279v1", "AI": {"title_translation": "从理想到现实：统一且数据高效的真实世界场景稠密预测", "tldr": "提出了DenseWorld基准和DenseDiT模型，用于统一且数据高效地解决真实世界稠密预测任务，显著优于现有方法并大幅减少数据需求。", "motivation": "现有稠密预测方法在理想条件下表现良好，但在真实世界场景中泛化能力有限，且面临真实世界数据稀缺的挑战。", "method": "1. 引入DenseWorld基准，涵盖25个真实世界稠密预测任务，并提供统一评估。 2. 提出DenseDiT模型，利用生成模型的视觉先验，通过统一策略执行多样化的真实世界稠密预测任务。DenseDiT结合了参数重用机制和两个自适应集成多尺度上下文的轻量级分支，额外参数不到0.1%。", "result": "现有通用和专业基准模型在DenseWorld上表现出显著的性能下降，凸显了其真实世界泛化能力的局限性。相比之下，DenseDiT使用不到基准模型0.01%的训练数据就取得了卓越结果。", "conclusion": "DenseDiT在真实世界稠密预测任务中表现出卓越的性能和数据效率，具有重要的实际部署价值。", "translation": "稠密预测任务在计算机视觉中占有重要地位，旨在为输入图像学习像素级的标注标签。尽管该领域取得了进展，但现有方法主要关注理想条件，对真实世界场景的泛化能力有限，并面临真实世界数据稀缺的挑战。为了系统地研究这个问题，我们首先引入了DenseWorld，这是一个涵盖25个对应紧急真实世界应用的稠密预测任务的基准，其特点是跨任务的统一评估。然后，我们提出了DenseDiT，它最大限度地利用生成模型的视觉先验，通过统一策略执行多样化的真实世界稠密预测任务。DenseDiT结合了参数重用机制和两个自适应集成多尺度上下文的轻量级分支，额外参数不到0.1%。对DenseWorld的评估显示，现有通用和专业基准模型的性能显著下降，凸显了它们真实世界泛化能力的局限性。相比之下，DenseDiT使用不到基准模型0.01%的训练数据就取得了卓越结果，突出了其在真实世界部署中的实用价值。我们的数据、检查点和代码可在https://xcltql666.github.io/DenseDiTProj获取。", "summary": "本文针对现有稠密预测方法在真实世界场景中泛化能力差及数据稀缺的挑战，提出了两项创新。首先，引入了DenseWorld，一个包含25个真实世界稠密预测任务的统一评估基准。其次，提出了DenseDiT模型，该模型通过利用生成模型的视觉先验和轻量级设计，实现了对多样化真实世界任务的统一且数据高效的稠密预测。实验结果表明，DenseDiT在仅使用极少量训练数据的情况下，显著优于现有基线，证明了其在实际应用中的巨大潜力。", "keywords": "稠密预测, 真实世界场景, 数据高效, 统一模型, DenseDiT", "comments": "本文的创新点在于系统地提出了一个真实的稠密预测基准DenseWorld，并开发了数据高效的统一模型DenseDiT。DenseDiT通过利用生成模型的先验知识和参数重用机制，极大地降低了对大规模标注数据的依赖，这对于真实世界部署具有重要意义。其数据效率是关键优势，解决了真实世界数据稀缺的痛点。"}}
{"id": "2506.20025", "title": "Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining", "authors": ["Nathan Stromberg", "Christos Thrampoulidis", "Lalitha Sankar"], "summary": "While machine learning models become more capable in discriminative tasks at\nscale, their ability to overcome biases introduced by training data has come\nunder increasing scrutiny. Previous results suggest that there are two extremes\nof parameterization with very different behaviors: the population\n(underparameterized) setting where loss weighting is optimal and the separable\noverparameterized setting where loss weighting is ineffective at ensuring equal\nperformance across classes. This work explores the regime of last layer\nretraining (LLR) in which the unseen limited (retraining) data is frequently\ninseparable and the model proportionately sized, falling between the two\naforementioned extremes. We show, in theory and practice, that loss weighting\nis still effective in this regime, but that these weights \\emph{must} take into\naccount the relative overparameterization of the model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20025v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20025v1", "AI": {"title_translation": "量尺上的拇指：最后一层再训练中的最优损失加权", "tldr": "在最后一层再训练（LLR）中，损失加权仍然有效，但权重必须考虑模型的相对过参数化，以克服训练数据引入的偏差。", "motivation": "大规模机器学习模型在判别任务中能力增强，但其克服训练数据引入的偏差的能力受到越来越多的审视。以往的研究表明，在不同参数化设置下（欠参数化和过参数化），损失加权的有效性存在两种极端行为。本文旨在探讨介于两者之间的最后一层再训练（LLR）机制，其中数据通常不可分离，模型大小适中。", "method": "本文探讨了最后一层再训练（LLR）的机制，其中未见的有限（再训练）数据通常不可分离，并且模型大小适中，介于欠参数化和过参数化两种极端情况之间。通过理论和实践相结合的方式进行了研究。", "result": "理论和实践均表明，损失加权在最后一层再训练（LLR）机制中仍然有效。然而，这些权重必须考虑到模型的相对过参数化。", "conclusion": "在最后一层再训练（LLR）中，损失加权仍然是克服数据偏差的有效方法，但其权重必须根据模型的相对过参数化进行调整。", "translation": "尽管机器学习模型在大规模判别任务中变得越来越强大，但它们克服训练数据引入的偏差的能力受到了越来越多的审视。以往的结果表明，存在两种参数化极端情况，其行为截然不同：在群体（欠参数化）设置中，损失加权是最佳的；而在可分离的过参数化设置中，损失加权在确保各类别性能均等性方面无效。这项工作探讨了最后一层再训练（LLR）的机制，其中未见的有限（再训练）数据通常不可分离，并且模型大小适中，介于上述两种极端情况之间。我们在理论和实践中表明，损失加权在此机制中仍然有效，但这些权重\n\n必须\n\n考虑到模型的相对过参数化。", "summary": "本文研究了最后一层再训练（LLR）中损失加权的有效性，该机制介于欠参数化和过参数化设置之间，并且关注数据偏差问题。研究在理论和实践上证明，损失加权在LLR中对于减轻偏差是有效的，但关键在于，最优权重必须考虑模型的相对过参数化程度。", "keywords": "损失加权, 最后一层再训练, 过参数化, 偏差, 机器学习", "comments": "这篇论文解决了机器学习模型中数据偏差的一个实际且重要的问题，尤其是在微调中常见的LLR设置下。研究发现损失加权仍然有效，但需要考虑过参数化，这为实践者提供了宝贵的指导。"}}
{"id": "2506.20228", "title": "Measuring Modern Phishing Tactics: A Quantitative Study of Body Obfuscation Prevalence, Co-occurrence, and Filter Impact", "authors": ["Antony Dalmiere", "Zheng Zhou", "Guillaume Auriol", "Vincent Nicomette", "Pascal Marchand"], "summary": "Phishing attacks frequently use email body obfuscation to bypass detection\nfilters, but quantitative insights into how techniques are combined and their\nimpact on filter scores remain limited. This paper addresses this gap by\nempirically investigating the prevalence, co-occurrence patterns, and spam\nscore associations of body obfuscation techniques. Analysing 386 verified\nphishing emails, we quantified ten techniques, identified significant pairwise\nco-occurrences revealing strategic layering like the presence of text in images\nwith multipart abuse, and assessed associations with antispam scores using\nmultilinear regression. Text in Image (47.0%), Base64 Encoding (31.2%), and\nInvalid HTML (28.8%) were highly prevalent. Regression (R${}^2$=0.486, p<0.001)\nlinked Base64 Encoding and Text in Image with significant antispam evasion\n(p<0.05) in this configuration, suggesting potential bypass capabilities, while\nInvalid HTML correlated with higher scores. These findings establish a\nquantitative baseline for complex evasion strategies, underscoring the need for\nmulti-modal defences against combined obfuscation tactics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20228v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20228v1", "AI": {"title_translation": "测量现代网络钓鱼策略：一项关于邮件正文混淆普遍性、共现性和过滤影响的定量研究", "tldr": "该研究定量分析了网络钓鱼邮件中正文混淆技术的普遍性、共现模式及其对反垃圾邮件分数的影响，发现某些技术组合能有效规避检测，强调了多模态防御的重要性。", "motivation": "网络钓鱼攻击频繁使用邮件正文混淆技术来绕过检测过滤器，但关于这些技术如何组合以及它们对过滤器分数影响的定量见解仍然有限。", "method": "分析了386封经过验证的网络钓鱼邮件，量化了10种混淆技术，识别了显著的成对共现模式，并使用多元线性回归评估了它们与反垃圾邮件分数的关联。", "result": "文本图像 (47.0%)、Base64编码 (31.2%) 和无效HTML (28.8%) 普遍存在。回归分析 (R${}^2$=0.486, p<0.001) 显示，Base64编码和文本图像与显著的反垃圾邮件规避相关 (p<0.05)，表明潜在的绕过能力，而无效HTML则与更高的分数相关。", "conclusion": "这些发现为复杂的规避策略建立了定量基线，强调了针对组合混淆策略采取多模态防御的必要性。", "translation": "网络钓鱼攻击经常使用邮件正文混淆来绕过检测过滤器，但关于技术如何组合及其对过滤器分数影响的定量见解仍然有限。本文通过实证研究正文混淆技术的普遍性、共现模式以及与垃圾邮件分数的关联来弥补这一空白。通过分析386封经过验证的网络钓鱼邮件，我们量化了十种技术，识别出显著的成对共现，揭示了如图像中文本与多部分滥用相结合的战略性分层，并使用多元线性回归评估了与反垃圾邮件分数的关联。图像中文本 (47.0%)、Base64编码 (31.2%) 和无效HTML (28.8%) 普遍存在。回归分析 (R${}^2$=0.486, p<0.001) 表明，在此配置中，Base64编码和图像中文本与显著的反垃圾邮件规避相关 (p<0.05)，这表明潜在的绕过能力，而无效HTML则与更高的分数相关。这些发现为复杂的规避策略建立了定量基线，强调了针对组合混淆策略采取多模态防御的必要性。", "summary": "本文对386封网络钓鱼邮件进行了定量研究，分析了邮件正文混淆技术的普遍性、共现模式及其对反垃圾邮件过滤器的影响。研究量化了十种技术，发现文本图像、Base64编码和无效HTML最为常见。结果表明，Base64编码和文本图像等技术组合能有效降低反垃圾邮件分数，而无效HTML则导致分数升高。这些发现为理解复杂的网络钓鱼规避策略提供了定量依据，并强调了开发多模态防御机制以应对组合混淆策略的重要性。", "keywords": "网络钓鱼, 邮件正文混淆, 定量研究, 反垃圾邮件, 规避策略", "comments": "这篇论文通过定量分析，弥补了现有研究在网络钓鱼邮件正文混淆技术组合及其对过滤器影响方面的空白。其创新之处在于提供了具体的数据，揭示了不同混淆技术的流行度、共现模式以及它们规避检测的能力。研究结果对于开发更有效的反钓鱼系统具有重要指导意义，特别是强调了多模态防御的必要性，以应对日益复杂的攻击手段。"}}
{"id": "2506.20534", "title": "Revisiting CHAMPAGNE: Sparse Bayesian Learning as Reweighted Sparse Coding", "authors": ["Dylan Sechet", "Matthieu Kowalski", "Samy Mokhtari", "Bruno Torrésani"], "summary": "This paper revisits the CHAMPAGNE algorithm within the Sparse Bayesian\nLearning (SBL) framework and establishes its connection to reweighted sparse\ncoding. We demonstrate that the SBL objective can be reformulated as a\nreweighted $\\ell_{21}$-minimization problem, providing a more straightforward\ninterpretation of the sparsity mechanism and enabling the design of an\nefficient iterative algorithm. Additionally, we analyze the behavior of this\nreformulation in the low signal-to-noise ratio (SNR) regime, showing that it\nsimplifies to a weighted $\\ell_{21}$-regularized least squares problem.\nNumerical experiments validate the proposed approach, highlighting its improved\ncomputational efficiency and ability to produce exact sparse solutions,\nparticularly in simulated MEG source localization tasks.", "comment": "Sampling Theory and Applications (SampTA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20534v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20534v1", "AI": {"title_translation": "再探CHAMPAGNE：稀疏贝叶斯学习作为重加权稀疏编码", "tldr": "本文将稀疏贝叶斯学习（SBL）框架下的CHAMPAGNE算法重新表述为重加权$\\\\ell_{21}$-最小化问题，提高了计算效率并能产生精确的稀疏解，特别适用于MEG源定位。", "motivation": "本文旨在重新审视CHAMPAGNE算法，并将其与重加权稀疏编码建立联系，以提供对稀疏机制更直接的解释，并设计更高效的迭代算法。", "method": "作者将稀疏贝叶斯学习（SBL）的目标函数重新表述为重加权$\\\\ell_{21}$-最小化问题，并基于此设计了一种高效的迭代算法。此外，还分析了该重构在低信噪比（SNR）条件下的行为。", "result": "SBL目标函数被重新表述为重加权$\\\\ell_{21}$-最小化问题；在低信噪比下，该重构简化为加权$\\\\ell_{21}$-正则化最小二乘问题。数值实验验证了所提出方法的计算效率更高，并能产生精确的稀疏解，尤其在模拟MEG源定位任务中表现出色。", "conclusion": "本文提出的将稀疏贝叶斯学习（SBL）重新表述为重加权$\\\\ell_{21}$-最小化问题的方法，在计算效率和获得精确稀疏解方面表现出显著优势，尤其适用于MEG源定位等应用。", "translation": "本文在稀疏贝叶斯学习（SBL）框架下重新审视了CHAMPAGNE算法，并建立了其与重加权稀疏编码的联系。我们证明了SBL目标函数可以被重新表述为一个重加权$\\\\ell_{21}$-最小化问题，这为稀疏机制提供了更直接的解释，并使得设计高效的迭代算法成为可能。此外，我们分析了这种重新表述在低信噪比（SNR）条件下的行为，表明它简化为一个加权$\\\\ell_{21}$-正则化最小二乘问题。数值实验验证了所提出的方法，突出了其改进的计算效率和产生精确稀疏解的能力，特别是在模拟MEG源定位任务中。", "summary": "本文在稀疏贝叶斯学习（SBL）框架内重新审视了CHAMPAGNE算法，并将其重新表述为重加权$\\\\ell_{21}$-最小化问题。这种新表述不仅提供了对稀疏机制更直观的理解，还促使设计出一种高效的迭代算法。研究还发现，在低信噪比条件下，该重构简化为加权$\\\\ell_{21}$-正则化最小二乘问题。数值实验表明，该方法在计算效率和生成精确稀疏解方面具有显著优势，尤其在模拟MEG源定位应用中表现突出。", "keywords": "稀疏贝叶斯学习, 重加权稀疏编码, CHAMPAGNE算法, $\\\\ell_{21}$-最小化, MEG源定位", "comments": "本文的创新之处在于将稀疏贝叶斯学习（SBL）与重加权稀疏编码建立了直接联系，并通过将SBL目标函数重新表述为重加权$\\\\ell_{21}$-最小化问题，为理解其稀疏机制提供了新的视角。这种理论上的重新阐释不仅简化了问题，还直接促成了更高效算法的设计。其在MEG源定位等实际应用中的有效性，特别是其提高的计算效率和精确稀疏解的能力，显示了该研究的重要性和潜在应用价值。"}}
{"id": "2506.20167", "title": "SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs", "authors": ["Fengze Li", "Yue Wang", "Yangle Liu", "Ming Huang", "Dou Hong", "Jieming Ma"], "summary": "Multivariate time series forecasting requires models to simultaneously\ncapture variable-wise structural dependencies and generalize across diverse\ntasks. While structural encoders are effective in modeling feature\ninteractions, they lack the capacity to support semantic-level reasoning or\ntask adaptation. Conversely, large language models (LLMs) possess strong\ngeneralization capabilities but remain incompatible with raw time series\ninputs. This gap limits the development of unified, transferable prediction\nsystems. Therefore, we introduce SEED, a structural encoder for\nembedding-driven decoding, which integrates four stages: a token-aware encoder\nfor patch extraction, a projection module that aligns patches with language\nmodel embeddings, a semantic reprogramming mechanism that maps patches to\ntask-aware prototypes, and a frozen language model for prediction. This modular\narchitecture decouples representation learning from inference, enabling\nefficient alignment between numerical patterns and semantic reasoning.\nEmpirical results demonstrate that the proposed method achieves consistent\nimprovements over strong baselines, and comparative studies on various datasets\nconfirm SEED's role in addressing the structural-semantic modeling gap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20167v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20167v1", "AI": {"title_translation": "SEED：一种用于LLM时间序列预测中嵌入驱动解码的结构化编码器", "tldr": "SEED是一种新的时间序列预测模型，它结合了结构编码器和大型语言模型（LLMs）的优点，通过将时间序列数据转化为LLM可理解的嵌入，弥补了数值模式与语义推理之间的鸿沟，并在多个数据集上取得了显著改进。", "motivation": "多变量时间序列预测需要模型同时捕捉变量间的结构依赖性并泛化到不同任务。结构编码器在建模特征交互方面有效，但缺乏语义级推理和任务适应能力。大型语言模型（LLMs）具有强大的泛化能力，但无法直接处理原始时间序列输入。这种差距限制了统一、可迁移预测系统的发展。", "method": "我们引入了SEED，一个用于嵌入驱动解码的结构化编码器，它集成了四个阶段：一个用于补丁提取的标记感知编码器，一个将补丁与语言模型嵌入对齐的投影模块，一个将补丁映射到任务感知原型的语义重编程机制，以及一个用于预测的冻结语言模型。这种模块化架构将表示学习与推理解耦，实现了数值模式与语义推理之间的有效对齐。", "result": "经验结果表明，所提出的方法比强基线模型取得了持续的改进，并且在各种数据集上的比较研究证实了SEED在解决结构-语义建模鸿沟方面的作用。", "conclusion": "SEED通过其模块化架构成功弥合了时间序列预测中结构建模与语义推理之间的差距，实现了数值模式与LLM语义理解的有效结合，从而在多变量时间序列预测中取得了显著的性能提升和更好的泛化能力。", "translation": "多变量时间序列预测要求模型同时捕捉变量间的结构依赖性并泛化到不同任务。虽然结构编码器在建模特征交互方面有效，但它们缺乏支持语义级推理或任务适应的能力。相反，大型语言模型（LLMs）拥有强大的泛化能力，但与原始时间序列输入不兼容。这种差距限制了统一、可迁移预测系统的发展。因此，我们引入了SEED，一种用于嵌入驱动解码的结构化编码器，它集成了四个阶段：一个用于补丁提取的标记感知编码器，一个将补丁与语言模型嵌入对齐的投影模块，一个将补丁映射到任务感知原型的语义重编程机制，以及一个用于预测的冻结语言模型。这种模块化架构将表示学习与推理解耦，实现了数值模式与语义推理之间的有效对齐。经验结果表明，所提出的方法比强基线模型取得了持续的改进，并且在各种数据集上的比较研究证实了SEED在解决结构-语义建模鸿沟方面的作用。", "summary": "SEED是一种创新的时间序列预测模型，旨在弥合结构化编码器在捕捉特征交互方面的优势与大型语言模型在语义推理和泛化能力方面的优势之间的差距。它通过一个四阶段的模块化架构实现这一目标，包括标记感知编码、投影对齐、语义重编程和冻结语言模型预测。该方法将数值时间序列模式转化为LLM可理解的语义嵌入，实现了表示学习与推理的解耦。实验证明，SEED在多个数据集上均优于现有基线，有效解决了时间序列预测中的结构-语义建模挑战。", "keywords": "时间序列预测, 大型语言模型, 结构化编码器, 语义推理, 嵌入驱动解码", "comments": "SEED的创新之处在于其独特的模块化架构，它巧妙地结合了结构化编码器和大型语言模型，解决了时间序列数据与LLM语义理解之间的不兼容问题。通过将表示学习与推理解耦，并引入语义重编程机制，SEED为构建统一、可迁移的时间序列预测系统提供了一个有前景的方向，具有重要的研究价值和应用潜力。"}}
{"id": "2506.20191", "title": "Fast entropy-regularized SDP relaxations for permutation synchronization", "authors": ["Michael Lindsey", "Yunpeng Shi"], "summary": "We introduce fast randomized algorithms for solving semidefinite programming\n(SDP) relaxations of the partial permutation synchronization (PPS) problem, a\ncore task in multi-image matching with significant relevance to 3D\nreconstruction. Our methods build on recent advances in entropy-regularized\nsemidefinite programming and are tailored to the unique structure of PPS, in\nwhich the unknowns are partial permutation matrices aligning sparse and noisy\npairwise correspondences across images. We prove that entropy regularization\nresolves optimizer non-uniqueness in standard relaxations, and we develop a\nrandomized solver with nearly optimal scaling in the number of observed\ncorrespondences. We also develop several rounding procedures for recovering\ncombinatorial solutions from the implicitly represented primal solution\nvariable, maintaining cycle consistency if desired without harming\ncomputational scaling. We demonstrate that our approach achieves\nstate-of-the-art performance on synthetic and real-world datasets in terms of\nspeed and accuracy. Our results highlight PPS as a paradigmatic setting in\nwhich entropy-regularized SDP admits both theoretical and practical advantages\nover traditional low-rank or spectral techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20191v1", "categories": ["math.OC", "cs.NA", "math.NA"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20191v1", "AI": {"title_translation": "用于置换同步的快速熵正则化SDP松弛", "tldr": "本文提出了一种用于偏置换同步（PPS）问题的快速随机算法，该算法基于熵正则化SDP松弛，解决了优化器非唯一性问题，并在多图像匹配中实现了最先进的速度和精度。", "motivation": "偏置换同步（PPS）问题是多图像匹配中的核心任务，与3D重建密切相关。传统的SDP松弛存在优化器非唯一性问题。", "method": "本文引入了用于解决偏置换同步（PPS）问题的半定规划（SDP）松弛的快速随机算法。这些方法基于熵正则化半定规划的最新进展，并针对PPS的独特结构进行了定制。证明了熵正则化解决了标准松弛中的优化器非唯一性问题，并开发了一种具有接近最优缩放比例的随机求解器。还开发了几种舍入程序，用于从隐式表示的原解变量中恢复组合解，如果需要，可以保持循环一致性，同时不影响计算缩放。", "result": "该方法在合成和真实世界数据集上，在速度和精度方面均达到了最先进的性能。", "conclusion": "本文结果表明，在PPS这一典型设置中，熵正则化SDP相对于传统的低秩或谱技术具有理论和实践上的优势。", "translation": "我们引入了用于解决部分置换同步（PPS）问题的半定规划（SDP）松弛的快速随机算法，这是多图像匹配中的一项核心任务，与3D重建具有重要相关性。我们的方法建立在熵正则化半定规划的最新进展之上，并针对PPS的独特结构进行了定制，其中未知数是用于对齐图像间稀疏和嘈杂成对对应关系的偏置换矩阵。我们证明了熵正则化解决了标准松弛中的优化器非唯一性，并且我们开发了一种随机求解器，其在观测对应数方面具有接近最优的缩放比例。我们还开发了几种舍入程序，用于从隐式表示的原解变量中恢复组合解，如果需要，可以保持循环一致性而不损害计算缩放。我们证明了我们的方法在速度和精度方面在合成和真实世界数据集上均达到了最先进的性能。我们的结果突出表明，PPS是一个典型的设置，其中熵正则化SDP比传统的低秩或谱技术具有理论和实践上的优势。", "summary": "本文提出了一种用于解决偏置换同步（PPS）问题的快速随机算法，该算法利用熵正则化半定规划（SDP）松弛。该方法专门针对PPS中偏置换矩阵的结构，解决了标准SDP松弛中优化器非唯一性的问题，并开发了一种高效的求解器和组合解恢复程序。实验证明，该方法在多图像匹配的合成和真实数据集上，在速度和精度方面均优于现有技术，突显了熵正则化SDP在处理此类问题时的显著优势。", "keywords": "置换同步, SDP松弛, 熵正则化, 多图像匹配, 3D重建", "comments": "该论文的创新之处在于将熵正则化引入到SDP松弛中，以解决偏置换同步（PPS）问题中优化器非唯一性的关键挑战。这种方法不仅提供了理论上的保证，而且通过高效的随机算法和舍入程序实现了卓越的计算性能。其在多图像匹配和3D重建领域的应用潜力巨大，为解决大规模对应问题提供了新的SOTA解决方案。"}}
{"id": "2506.20445", "title": "Learn to Position -- A Novel Meta Method for Robotic Positioning", "authors": ["Dongkun Wang", "Junkai Zhao", "Yunfei Teng", "Jieyang Peng", "Wenjing Xue", "Xiaoming Tao"], "summary": "Absolute positioning accuracy is a vital specification for robots. Achieving\nhigh position precision can be challenging due to the presence of various\nsources of errors. Meanwhile, accurately depicting these errors is difficult\ndue to their stochastic nature. Vision-based methods are commonly integrated to\nguide robotic positioning, but their performance can be highly impacted by\ninevitable occlusions or adverse lighting conditions. Drawing on the\naforementioned considerations, a vision-free, model-agnostic meta-method for\ncompensating robotic position errors is proposed, which maximizes the\nprobability of accurate robotic position via interactive feedback. Meanwhile,\nthe proposed method endows the robot with the capability to learn and adapt to\nvarious position errors, which is inspired by the human's instinct for grasping\nunder uncertainties. Furthermore, it is a self-learning and self-adaptive\nmethod able to accelerate the robotic positioning process as more examples are\nincorporated and learned. Empirical studies validate the effectiveness of the\nproposed method. As of the writing of this paper, the proposed meta search\nmethod has already been implemented in a robotic-based assembly line for\nodd-form electronic components.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20445v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20445v1", "AI": {"title_translation": "学习定位——一种新型的机器人定位元方法", "tldr": "提出了一种新型的、无视觉、模型无关的元方法，通过交互反馈补偿机器人定位误差，使其能够自学习和自适应，提高定位精度和速度。", "motivation": "机器人绝对定位精度至关重要，但受随机误差影响难以实现高精度；传统视觉方法易受遮挡和光照影响。", "method": "本文提出了一种无视觉、模型无关的元方法，通过交互反馈补偿机器人定位误差，最大化准确性。该方法受人类在不确定性下抓取行为的启发，能够学习并适应各种定位误差，并能通过更多示例实现自学习和自适应以加速定位过程。", "result": "经验研究验证了所提出方法的有效性。该元搜索方法已应用于异形电子元件的机器人装配线。", "conclusion": "所提出的元方法能够有效补偿机器人定位误差，提高精度并加速定位过程，且已在实际工业应用中得到验证。", "translation": "机器人绝对定位精度是机器人的一项重要指标。由于存在各种误差源，实现高定位精度可能具有挑战性。同时，由于这些误差的随机性，准确描述它们很困难。基于视觉的方法通常被集成到机器人定位中进行引导，但其性能可能受到不可避免的遮挡或不利光照条件的严重影响。基于上述考虑，本文提出了一种无视觉、模型无关的元方法，用于补偿机器人定位误差，该方法通过交互式反馈最大限度地提高机器人准确位置的概率。同时，所提出的方法赋予机器人学习和适应各种定位误差的能力，这灵感来源于人类在不确定性下抓取的本能。此外，这是一种自学习和自适应的方法，随着更多示例的纳入和学习，能够加速机器人定位过程。实证研究验证了所提出方法的有效性。截至本文撰写之时，所提出的元搜索方法已在用于异形电子元件的机器人装配线上实施。", "summary": "本文提出了一种新颖的、无视觉、模型无关的元方法，旨在通过交互式反馈来补偿机器人定位误差，从而最大化定位精度。该方法受到人类在不确定性下抓取本能的启发，具备自学习和自适应能力，能够学习并适应各种定位误差，并随着学习示例的增加而加速定位过程。经验研究证实了其有效性，并且该方法已成功应用于异形电子元件的机器人装配线。", "keywords": "机器人定位, 元方法, 误差补偿, 自学习, 无视觉", "comments": "该论文提出了一种创新的、无视觉的元方法来解决机器人定位精度问题，其亮点在于引入了交互式反馈和受人类启发的设计，使其具备自学习和自适应能力，从而克服了传统视觉方法的局限性。其在实际装配线上的应用也证明了其工程实用价值。"}}
{"id": "2506.20031", "title": "Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning", "authors": ["Prithvi Poddar", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury"], "summary": "Operations in disaster response, search \\& rescue, and military missions that\ninvolve multiple agents demand automated processes to support the planning of\nthe courses of action (COA). Moreover, traverse-affecting changes in the\nenvironment (rain, snow, blockades, etc.) may impact the expected performance\nof a COA, making it desirable to have a pool of COAs that are diverse in task\ndistributions across agents. Further, variations in agent capabilities, which\ncould be human crews and/or autonomous systems, present practical opportunities\nand computational challenges to the planning process. This paper presents a new\ntheoretical formulation and computational framework to generate such diverse\npools of COAs for operations with soft variations in agent-task compatibility.\nKey to the problem formulation is a graph abstraction of the task space and the\npool of COAs itself to quantify its diversity. Formulating the COAs as a\ncentralized multi-robot task allocation problem, a genetic algorithm is used\nfor (order-ignoring) allocations of tasks to each agent that jointly maximize\ndiversity within the COA pool and overall compatibility of the agent-task\nmappings. A graph neural network is trained using a policy gradient approach to\nthen perform single agent task sequencing in each COA, which maximizes\ncompletion rates adaptive to task features. Our tests of the COA generation\nprocess in a simulated environment demonstrate significant performance gain\nover a random walk baseline, small optimality gap in task sequencing, and\nexecution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task\noperations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20031v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20031v1", "AI": {"title_translation": "使用二元优化和图学习自动生成多智能体操作的多种行动方案", "tldr": "本文提出了一种新的理论和计算框架，利用遗传算法和图神经网络为多智能体操作生成多样化的行动方案池，以应对环境变化和智能体能力差异。", "motivation": "在灾难响应、搜救和军事任务等多智能体操作中，需要自动化流程来支持行动方案（COA）的规划。此外，环境变化（如雨、雪、路障）可能影响COA的预期性能，因此需要一个任务分布多样化的COA池。同时，智能体能力（人类或自主系统）的差异也带来了规划过程中的实际机遇和计算挑战。", "method": "本文提出了一种新的理论公式和计算框架，用于生成具有智能体-任务兼容性软变化的行动方案池。核心是任务空间和COA池本身的图抽象，以量化其多样性。将COA制定为集中式多机器人任务分配问题，使用遗传算法进行任务分配（忽略顺序），共同最大化COA池内的多样性和智能体-任务映射的整体兼容性。然后，使用策略梯度方法训练图神经网络，以在每个COA中执行单智能体任务排序，最大化适应任务特征的完成率。", "result": "在模拟环境中进行的COA生成过程测试表明，与随机游走基线相比，性能显著提高，任务排序的最优性差距很小，并且在5个智能体/100个任务的操作中，规划多达20个COA的执行时间约为50分钟。", "conclusion": "本文提出的框架能够为多智能体操作生成多样化的行动方案池，并在模拟环境中表现出显著的性能提升和较低的最优性差距，证明了其在实际应用中的潜力。", "translation": "灾难响应、搜救和军事任务中涉及多个智能体的操作需要自动化流程来支持行动方案（COA）的规划。此外，环境（雨、雪、路障等）中影响遍历的变化可能会影响COA的预期性能，因此需要一个在智能体之间任务分布多样化的COA池。此外，智能体能力（可以是人类团队和/或自主系统）的变化为规划过程带来了实际机会和计算挑战。本文提出了一种新的理论公式和计算框架，用于生成具有智能体-任务兼容性软变化的此类多样化COA池。问题公式的关键是任务空间和COA池本身的图抽象，以量化其多样性。将COA制定为集中式多机器人任务分配问题，使用遗传算法进行（忽略顺序的）任务分配给每个智能体，共同最大化COA池内的多样性和智能体-任务映射的整体兼容性。然后，使用策略梯度方法训练图神经网络，以在每个COA中执行单智能体任务排序，最大化适应任务特征的完成率。我们在模拟环境中对COA生成过程的测试表明，与随机游走基线相比，性能显著提高，任务排序的最优性差距很小，并且在5个智能体/100个任务的操作中，规划多达20个COA的执行时间约为50分钟。", "summary": "本文针对灾难响应等多智能体操作中行动方案规划的需求，提出了一种结合二元优化和图学习的新型理论与计算框架。该框架通过图抽象量化COA多样性，利用遗传算法解决多机器人任务分配问题以最大化多样性和兼容性，并采用图神经网络进行单智能体任务排序以提高完成率。模拟测试验证了该方法相比基线的显著性能提升和高效性，能够生成多样化的行动方案池。", "keywords": "多智能体系统, 行动方案, 二元优化, 图学习, 任务分配", "comments": "本文的创新点在于将二元优化（通过遗传算法进行任务分配）与图学习（通过GNN进行任务排序）相结合，以解决多智能体操作中行动方案的多样性生成问题。通过引入图抽象来量化多样性，并考虑智能体-任务兼容性的软变化，使得该框架更具实用性。其在模拟环境中的良好表现，尤其是在生成多样化COA方面的能力，对于需要灵活应对环境变化和智能体能力差异的复杂任务具有重要意义。未来的工作可以探索在更复杂和动态的实际环境中的应用。"}}
{"id": "2506.20598", "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges", "authors": ["Alexander D. Kalian", "Jaewook Lee", "Stefan P. Johannesson", "Lennart Otte", "Christer Hogstrand", "Miao Guo"], "summary": "The global demand for sustainable protein sources has accelerated the need\nfor intelligent tools that can rapidly process and synthesise domain-specific\nscientific knowledge. In this study, we present a proof-of-concept multi-agent\nArtificial Intelligence (AI) framework designed to support sustainable protein\nproduction research, with an initial focus on microbial protein sources. Our\nRetrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based\nLLM agents: (1) a literature search agent that retrieves relevant scientific\nliterature on microbial protein production for a specified microbial strain,\nand (2) an information extraction agent that processes the retrieved content to\nextract relevant biological and chemical information. Two parallel\nmethodologies, fine-tuning and prompt engineering, were explored for agent\noptimisation. Both methods demonstrated effectiveness at improving the\nperformance of the information extraction agent in terms of transformer-based\ncosine similarity scores between obtained and ideal outputs. Mean cosine\nsimilarity scores were increased by up to 25%, while universally reaching mean\nscores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved\nthe mean scores to a greater extent (consistently of $\\geq 0.94$) compared to\nprompt engineering, although lower statistical uncertainties were observed with\nthe latter approach. A user interface was developed and published for enabling\nthe use of the multi-agent AI system, alongside preliminary exploration of\nadditional chemical safety-based search capabilities", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20598v1", "categories": ["cs.AI", "cs.SY", "eess.SY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20598v1", "AI": {"title_translation": "大型语言模型（LLMs）的微调与提示工程，用于创建多智能体人工智能以解决可持续蛋白质生产挑战", "tldr": "本研究开发了一个基于RAG的多智能体AI框架，利用微调和提示工程优化LLM代理，以支持可持续蛋白质生产研究，并显著提高了信息提取的准确性。", "motivation": "全球对可持续蛋白质来源的需求加速了对能够快速处理和综合特定领域科学知识的智能工具的需求。", "method": "本研究提出了一个概念验证的多智能体人工智能（AI）框架，旨在支持可持续蛋白质生产研究，初步侧重于微生物蛋白质来源。该系统是面向检索增强生成（RAG）的，包含两个基于GPT的LLM代理：一个文献搜索代理用于检索相关文献，一个信息提取代理用于处理检索到的内容以提取生物和化学信息。通过微调和提示工程两种并行方法探索代理优化。", "result": "两种方法都有效地提高了信息提取代理的性能，根据获得的输出和理想输出之间的基于Transformer的余弦相似度分数衡量，平均余弦相似度分数提高了高达25%，普遍达到≥0.89。微调总体上在更大程度上改善了平均分数（始终≥0.94），而提示工程则观察到较低的统计不确定性。还开发并发布了一个用户界面，并初步探索了额外的基于化学安全性的搜索功能。", "conclusion": "通过微调和提示工程，LLM多智能体系统在可持续蛋白质生产领域的信息提取方面表现出显著改进，微调在性能提升上更优，而提示工程在统计不确定性上更低，证明了该框架在处理领域特定科学知识方面的潜力。", "translation": "全球对可持续蛋白质来源的需求加速了对能够快速处理和综合特定领域科学知识的智能工具的需求。在本研究中，我们提出了一个概念验证的多智能体人工智能（AI）框架，旨在支持可持续蛋白质生产研究，初步侧重于微生物蛋白质来源。我们的检索增强生成（RAG）导向系统由两个基于GPT的LLM代理组成：（1）一个文献搜索代理，用于检索特定微生物菌株的微生物蛋白质生产相关科学文献，以及（2）一个信息提取代理，用于处理检索到的内容以提取相关的生物和化学信息。探索了微调和提示工程两种并行方法进行代理优化。两种方法都证明了在提高信息提取代理性能方面的有效性，以获得的输出和理想输出之间的基于Transformer的余弦相似度分数衡量。平均余弦相似度分数提高了高达25%，普遍达到与理想输出文本相比的平均分数≥0.89。与提示工程相比，微调总体上在更大程度上改善了平均分数（始终≥0.94），尽管后者观察到较低的统计不确定性。开发并发布了一个用户界面，以实现多智能体AI系统的使用，并初步探索了额外的基于化学安全性的搜索功能。", "summary": "本研究提出了一个概念验证的多智能体AI框架，旨在解决可持续蛋白质生产中的信息处理挑战。该框架是RAG导向的，包含文献搜索和信息提取两个GPT-based LLM代理。通过微调和提示工程优化信息提取代理，结果显示两种方法均能显著提高信息提取的准确性，微调效果更佳。该系统通过用户界面发布，并初步探索了化学安全搜索能力，为快速处理和综合科学知识提供了智能工具。", "keywords": "LLMs, 多智能体AI, 可持续蛋白质生产, 微调, 提示工程", "comments": "这项研究的创新之处在于将多智能体AI与RAG方法相结合，并系统地比较了微调和提示工程这两种主流的LLM优化策略在特定领域应用中的效果。其重要性体现在为可持续蛋白质生产这一关键领域提供了高效的知识处理工具，有望加速相关研究进展。研究结果清晰地量化了两种优化方法的性能提升，并指出了各自的优劣（微调效果更好，提示工程不确定性更低），为未来的LLM应用开发提供了宝贵的实践指导。"}}
{"id": "2506.20234", "title": "Communication-Efficient Publication of Sparse Vectors under Differential Privacy", "authors": ["Quentin Hillebrand", "Vorapong Suppakitpaisarn", "Tetsuo Shibuya"], "summary": "In this work, we propose a differentially private algorithm for publishing\nmatrices aggregated from sparse vectors. These matrices include social network\nadjacency matrices, user-item interaction matrices in recommendation systems,\nand single nucleotide polymorphisms (SNPs) in DNA data. Traditionally,\ndifferential privacy in vector collection relies on randomized response, but\nthis approach incurs high communication costs. Specifically, for a matrix with\n$N$ users, $n$ columns, and $m$ nonzero elements, conventional methods require\n$\\Omega(n \\times N)$ communication, making them impractical for large-scale\ndata. Our algorithm significantly reduces this cost to $O(\\varepsilon m)$,\nwhere $\\varepsilon$ is the privacy budget. Notably, this is even lower than the\nnon-private case, which requires $\\Omega(m \\log n)$ communication. Moreover, as\nthe privacy budget decreases, communication cost further reduces, enabling\nbetter privacy with improved efficiency. We theoretically prove that our method\nyields results identical to those of randomized response, and experimental\nevaluations confirm its effectiveness in terms of accuracy, communication\nefficiency, and computational complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20234v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20234v1", "AI": {"title_translation": "差分隐私下稀疏向量的通信高效发布", "tldr": "提出一种差分隐私算法，用于高效发布稀疏向量聚合的矩阵，显著降低了通信成本，甚至低于非隐私情况。", "motivation": "传统随机响应方法在差分隐私下发布稀疏向量聚合矩阵时通信成本过高，对于大规模数据不实用，需要$\\Omega(n \\times N)$的通信开销。", "method": "本文提出了一种新的差分隐私算法，用于发布由稀疏向量聚合而成的矩阵，旨在解决传统方法在通信成本上的高昂开销。", "result": "该算法将通信成本从$\\Omega(n \\times N)$显著降低到$O(\\varepsilon m)$，甚至低于非隐私情况所需的$\\Omega(m \\log n)$。随着隐私预算的降低，通信成本进一步减少。理论上证明了该方法与随机响应的结果相同，并通过实验验证了其在准确性、通信效率和计算复杂度方面的有效性。", "conclusion": "该算法在差分隐私下实现了稀疏向量聚合矩阵的通信高效发布，解决了传统方法的通信开销问题，同时保持了准确性并优化了计算复杂度，提高了隐私保护的效率。", "translation": "标题：差分隐私下稀疏向量的通信高效发布\n摘要：在这项工作中，我们提出了一种差分隐私算法，用于发布由稀疏向量聚合而成的矩阵。这些矩阵包括社交网络邻接矩阵、推荐系统中的用户-项目交互矩阵以及DNA数据中的单核苷酸多态性（SNP）。传统上，向量集合中的差分隐私依赖于随机响应，但这种方法会产生高昂的通信成本。具体而言，对于一个具有N个用户、n列和m个非零元素的矩阵，传统方法需要$\\Omega(n \\times N)$的通信量，这使得它们在大规模数据中不切实际。我们的算法将此成本显著降低到$O(\\varepsilon m)$，其中$\\varepsilon$是隐私预算。值得注意的是，这甚至低于非隐私情况，非隐私情况需要$\\Omega(m \\log n)$的通信量。此外，随着隐私预算的降低，通信成本进一步减少，从而在提高效率的同时实现更好的隐私保护。我们从理论上证明了我们的方法产生的结果与随机响应的结果相同，并且实验评估证实了其在准确性、通信效率和计算复杂度方面的有效性。", "summary": "本文提出了一种创新的差分隐私算法，旨在高效发布由稀疏向量聚合的矩阵，如社交网络、推荐系统和DNA数据中的矩阵。针对传统随机响应方法高昂的通信成本问题，该算法将通信开销从$\\Omega(n \\times N)$显著降低至$O(\\varepsilon m)$，甚至优于非隐私情况。研究通过理论分析和实验验证，证明了该方法在保证与随机响应相同结果的前提下，在准确性、通信效率和计算复杂度方面均表现出色，实现了更好的隐私保护和更高的效率。", "keywords": "差分隐私, 稀疏向量, 通信效率, 矩阵发布, 随机响应", "comments": "这篇论文的创新点在于提出了一个通信效率极高的差分隐私算法，解决了传统方法在大规模稀疏数据发布中的通信瓶颈。其通信成本甚至低于非隐私情况，并且隐私预算越低通信成本越低，这是一个非常重要的突破，对于实际应用具有显著价值。"}}
{"id": "2506.20492", "title": "A Decomposition Method for Finite-Time Stabilization of Bilinear Systems with Applications to Parabolic and Hyperbolic Equations", "authors": ["Kamal Fenza", "Moussa Labbadi", "Mohamed Ouzahra"], "summary": "In this work, we address the problem of finite-time stabilization for a class\nof bilinear system. We propose a decomposition-based approach in which the\nnominal system is split into two subsystems, one of which is inherently\nfinite-time stable without control. This allows the stabilization analysis to\nfocus solely on the remaining subsystem. To ensure the well-posedness of the\nclosed-loop system, we establish sufficient conditions on the system and\ncontrol operators. The stabilization results are then derived using a suitable\nLyapunov function and an observation condition. The effectiveness of the\nproposed approach is demonstrated through examples involving both parabolic and\nhyperbolic infinite-dimensional systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20492v1", "categories": ["math.OC", "cs.SY", "eess.SY"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20492v1", "AI": {"title_translation": "双线性系统有限时间镇定的分解方法及其在抛物线和双曲线方程中的应用", "tldr": "本文提出了一种分解方法，用于实现双线性系统的有限时间镇定，通过将系统分解为两个子系统，其中一个子系统本质上是有限时间稳定的，从而简化了稳定性分析，并在抛物线和双曲线无限维系统中得到了验证。", "motivation": "本文旨在解决一类双线性系统的有限时间镇定问题。", "method": "本文提出了一种基于分解的方法，将名义系统分解为两个子系统，其中一个子系统无需控制即可实现固有的有限时间稳定。这使得稳定性分析可以仅关注剩余的子系统。为了确保闭环系统的适定性，本文建立了系统和控制算子的充分条件。随后，利用合适的Lyapunov函数和观测条件推导出了镇定结果。", "result": "所提出的方法通过涉及抛物线和双曲线无限维系统的例子证明了其有效性。", "conclusion": "通过分解方法，可以有效地实现双线性系统的有限时间镇定，并且该方法适用于抛物线和双曲线无限维系统。", "translation": "在这项工作中，我们解决了一类双线性系统的有限时间镇定问题。我们提出了一种基于分解的方法，其中名义系统被分解为两个子系统，其中一个子系统无需控制即可实现固有的有限时间稳定。这使得稳定性分析可以仅关注剩余的子系统。为了确保闭环系统的适定性，我们建立了系统和控制算子的充分条件。随后，利用合适的Lyapunov函数和观测条件推导出了镇定结果。所提出的方法的有效性通过涉及抛物线和双曲线无限维系统的例子得到了证明。", "summary": "本文提出了一种用于双线性系统有限时间镇定的分解方法。该方法将系统分解为两个子系统，其中一个子系统具有固有有限时间稳定性，从而简化了控制器的设计和稳定性分析。通过建立系统和控制算子的充分条件，确保了闭环系统的适定性，并利用Lyapunov函数和观测条件推导出了镇定结果。该方法在抛物线和双曲线无限维系统中的应用证明了其有效性。", "keywords": "双线性系统, 有限时间镇定, 分解方法, 抛物线方程, 双曲线方程", "comments": "该论文提出了一种新颖的分解方法来解决双线性系统的有限时间镇定问题，其创新之处在于将复杂系统分解为更易于分析和控制的子系统，这大大简化了问题。将该方法应用于无限维的抛物线和双曲线方程，显示了其在实际工程问题中的潜在价值。"}}
{"id": "2506.20178", "title": "COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees", "authors": ["Zhiyuan Wang", "Jinhao Duan", "Qingni Wang", "Xiaofeng Zhu", "Tianlong Chen", "Xiaoshuang Shi", "Kaidi Xu"], "summary": "Uncertainty quantification (UQ) for foundation models is essential to\nidentify and mitigate potential hallucinations in automatically generated text.\nHowever, heuristic UQ approaches lack formal guarantees for key metrics such as\nthe false discovery rate (FDR) in selective prediction. Previous work adopts\nthe split conformal prediction (SCP) framework to ensure desired coverage of\nadmissible answers by constructing prediction sets, but these sets often\ncontain incorrect candidates, limiting their practical utility. To address\nthis, we propose COIN, an uncertainty-guarding selection framework that\ncalibrates statistically valid thresholds to filter a single generated answer\nper question under user-specified FDR constraints. COIN estimates the empirical\nerror rate on a calibration set and applies confidence interval methods such as\nClopper-Pearson to establish a high-probability upper bound on the true error\nrate (i.e., FDR). This enables the selection of the largest uncertainty\nthreshold that ensures FDR control on test data while significantly increasing\nsample retention. We demonstrate COIN's robustness in risk control, strong\ntest-time power in retaining admissible answers, and predictive efficiency\nunder limited calibration data across both general and multimodal text\ngeneration tasks. Furthermore, we show that employing alternative upper bound\nconstructions and UQ strategies can further boost COIN's power performance,\nwhich underscores its extensibility and adaptability to diverse application\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20178v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20178v1", "AI": {"title_translation": "COIN：具有可证明风险保证的基座模型不确定性防护选择性问答", "tldr": "COIN是一种新的不确定性防护选择框架，通过校准统计有效阈值，在用户指定错误发现率（FDR）约束下为基座模型提供可证明的风险保证，以过滤单个生成答案。", "motivation": "现有启发式不确定性量化（UQ）方法缺乏选择性预测中关键指标（如错误发现率FDR）的正式保证。之前的分裂共形预测（SCP）框架虽然能确保可接受答案的覆盖率，但其预测集常包含不正确候选项，限制了实用性。", "method": "本文提出COIN框架，通过校准统计有效阈值，在用户指定的FDR约束下为每个问题过滤单个生成答案。COIN在校准集上估计经验错误率，并应用Clopper-Pearson等置信区间方法建立真实错误率（即FDR）的高概率上限，从而选择最大的不确定性阈值，以确保测试数据上的FDR控制并显著提高样本保留率。", "result": "COIN在风险控制方面表现出鲁棒性，在保留可接受答案方面具有强大的测试时效能，并在有限校准数据下在通用和多模态文本生成任务中展现出预测效率。此外，采用替代上限构造和UQ策略可以进一步提升COIN的性能，这凸显了其可扩展性和对不同应用场景的适应性。", "conclusion": "COIN提供了一种有效且可扩展的框架，通过提供可证明的风险保证来解决基础模型中不确定性量化和幻觉识别的挑战，显著提升了选择性问答的实用性和安全性。", "translation": "基座模型的不确定性量化（UQ）对于识别和缓解自动生成文本中潜在的“幻觉”至关重要。然而，启发式UQ方法缺乏对选择性预测中关键指标（如错误发现率FDR）的正式保证。以往工作采用分裂共形预测（SCP）框架，通过构建预测集来确保可接受答案的覆盖率，但这些集合通常包含不正确的候选答案，限制了其实用性。为了解决这个问题，我们提出了COIN，一个不确定性防护选择框架，它校准统计有效阈值，以在用户指定的FDR约束下为每个问题过滤单个生成答案。COIN在校准集上估计经验错误率，并应用Clopper-Pearson等置信区间方法建立真实错误率（即FDR）的高概率上限。这使得能够选择最大的不确定性阈值，从而确保测试数据上的FDR控制，同时显著提高样本保留率。我们证明了COIN在风险控制方面的鲁棒性、在保留可接受答案方面强大的测试时效能，以及在有限校准数据下在通用和多模态文本生成任务中的预测效率。此外，我们展示了采用替代上限构造和UQ策略可以进一步提升COIN的效能表现，这凸显了其可扩展性和对不同应用场景的适应性。", "summary": "COIN是一种新颖的框架，旨在通过可证明的风险保证，解决基座模型中不确定性量化和“幻觉”问题。它通过校准统计阈值，在用户设定的错误发现率（FDR）约束下，为每个问题选择单个最合适的答案。COIN利用置信区间方法（如Clopper-Pearson）来估计真实错误率的上限，从而在确保FDR控制的同时，最大化样本保留率。实验证明COIN在风险控制、预测效能和适应性方面表现出色，尤其在处理通用和多模态文本生成任务时。", "keywords": "不确定性量化, 错误发现率, 选择性问答, 基座模型, 风险控制", "comments": "COIN的创新之处在于其提供了可证明的风险保证（通过FDR控制），这在以往启发式UQ方法中是缺失的。它通过统计学方法建立错误率的上限，而非仅仅依赖预测集，从而提高了选择性预测的实用性。其对单个答案的过滤机制，相比于包含多个候选答案的预测集，更具实际应用价值。此外，其对有限校准数据的有效性和可扩展性也增加了其实用性。"}}
{"id": "2506.20470", "title": "Pivot probabilities and norm effects in Gaussian elimination for $β$-ensembles", "authors": ["Kenji Gunawan", "John Peca-Medlin"], "summary": "We analyze pivot probabilities in Gaussian elimination with partial pivoting\n(GEPP) for $2 \\times 2$ random matrix ensembles. For GUE matrices, we resolve a\npreviously reported discrepancy between theoretical predictions and empirical\nobservations by deriving the exact pivot probability under standard\nLAPACK-style implementations. We further show that Dumitriu-Edelman tridiagonal\n$\\beta$-ensembles agree with the earlier theoretical expectations. Finally, we\npropose an open question on pivot behavior under alternative norm choices,\nsupported by empirical evidence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20470v1", "categories": ["math.PR", "cs.NA", "math.NA"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.20470v1", "AI": {"title_translation": "高斯消元法中$\\beta$系综的枢轴概率与范数效应", "tldr": "该研究分析了高斯消元法中$2 \\times 2$随机矩阵系综的枢轴概率，解决了GUE矩阵的理论与经验差异，并提出了关于范数选择的开放问题。", "motivation": "论文旨在解决GUE矩阵在高斯消元法中枢轴概率的理论预测与实际观测之间存在的差异。", "method": "通过分析$2 \\times 2$随机矩阵系综在高斯消元法（GEPP）中的枢轴概率，并推导在标准LAPACK风格实现下GUE矩阵的精确枢轴概率。", "result": "成功解决了GUE矩阵枢轴概率的理论与经验差异；发现Dumitriu-Edelman三对角$\\beta$系综与早期理论预期一致；提出了关于替代范数选择下枢轴行为的开放问题，并提供了经验证据。", "conclusion": "该研究通过精确推导解决了GUE矩阵在高斯消元法中枢轴概率的长期差异，并验证了$\\beta$系综的理论一致性，同时指出了未来在不同范数下枢轴行为研究的方向。", "translation": "我们分析了$2 \\times 2$随机矩阵系综在高斯消元法（GEPP）中的枢轴概率。对于GUE矩阵，我们通过推导标准LAPACK风格实现下的精确枢轴概率，解决了此前报道的理论预测与经验观察之间的差异。我们进一步表明，Dumitriu-Edelman三对角$\\beta$系综与早期的理论预期一致。最后，我们提出了一个关于在替代范数选择下枢轴行为的开放问题，并提供了经验证据支持。", "summary": "本文研究了$2 \\times 2$随机矩阵系综在高斯消元法中枢轴概率。研究解决了GUE矩阵在LAPACK实现下枢轴概率的理论与经验观测差异，并确认了Dumitriu-Edelman $\\beta$系综与早期理论的一致性。此外，文章还提出了关于不同范数选择对枢轴行为影响的开放问题。", "keywords": "枢轴概率, 高斯消元法, 随机矩阵, $\\beta$系综, GUE矩阵", "comments": "这篇论文通过精确的数学推导解决了随机矩阵理论与数值线性代数实践之间的一个重要差异，尤其是在LAPACK这种广泛使用的库背景下。其创新之处在于提供了GUE矩阵的精确枢轴概率解析解，填补了理论与经验之间的空白。同时，提出的关于范数选择的开放问题也为未来的研究指明了方向，具有重要的理论和实践意义。"}}
{"id": "2506.20447", "title": "A Review of Personalisation in Human-Robot Collaboration and Future Perspectives Towards Industry 5.0", "authors": ["James Fant-Male", "Roel Pieters"], "summary": "The shift in research focus from Industry 4.0 to Industry 5.0 (I5.0) promises\na human-centric workplace, with social and well-being values at the centre of\ntechnological implementation. Human-Robot Collaboration (HRC) is a core aspect\nof I5.0 development, with an increase in adaptive and personalised interactions\nand behaviours. This review investigates recent advancements towards\npersonalised HRC, where user-centric adaption is key. There is a growing trend\nfor adaptable HRC research, however there lacks a consistent and unified\napproach. The review highlights key research trends on which personal factors\nare considered, workcell and interaction design, and adaptive task completion.\nThis raises various key considerations for future developments, particularly\naround the ethical and regulatory development of personalised systems, which\nare discussed in detail.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "pdf_url": "http://arxiv.org/pdf/2506.20447v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20447v1", "AI": {"title_translation": "人机协作个性化及工业5.0未来展望综述", "tldr": "该综述探讨了人机协作（HRC）中个性化的最新进展，强调了缺乏统一方法的问题，并提出了未来发展，特别是伦理和监管方面的关键考量。", "motivation": "研究重点从工业4.0转向工业5.0，强调以人为中心的工作场所和价值观。人机协作（HRC）是工业5.0发展的核心，需要增加自适应和个性化的交互。鉴于适应性HRC研究的增长趋势，但缺乏一致和统一的方法，本综述旨在调查个性化HRC的最新进展。", "method": "文献综述，调查了人机协作中个性化的最新进展。", "result": "综述强调了关键研究趋势，包括考虑的个人因素、工作单元和交互设计以及自适应任务完成。", "conclusion": "未来个性化人机协作系统的发展需要重点关注伦理和监管方面。", "translation": "研究重点从工业4.0转向工业5.0（I5.0），预示着一个以人为中心的工作场所，将社会和福祉价值置于技术实施的核心。人机协作（HRC）是工业5.0发展的核心方面，其自适应和个性化交互与行为不断增加。本综述调查了个性化人机协作的最新进展，其中以用户为中心的适应是关键。适应性人机协作研究呈增长趋势，但缺乏一致和统一的方法。本综述强调了关于个人因素、工作单元和交互设计以及自适应任务完成的关键研究趋势。这为未来的发展提出了各种关键考量，特别是围绕个性化系统的伦理和监管发展，这些将在文中详细讨论。", "summary": "本综述探讨了工业5.0背景下人机协作（HRC）中个性化的最新进展。文章指出，尽管适应性HRC研究日益增多，但仍缺乏统一的方法。综述分析了影响个性化HRC的关键研究趋势，包括个人因素、工作单元设计和自适应任务完成。此外，文章还讨论了未来个性化系统发展中，尤其是伦理和监管方面的重要考量。", "keywords": "人机协作, 个性化, 工业5.0, 综述, 伦理", "comments": "本综述及时地关注了工业5.0背景下日益重要的人机协作个性化问题。其创新之处在于梳理了当前研究进展并指出了缺乏统一方法的局限性。该论文的重要性体现在其对未来研究方向的指导作用，特别是强调了伦理和监管发展的重要性，这对于确保技术进步与社会福祉的协调发展至关重要。"}}
{"id": "2506.20294", "title": "Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations", "authors": ["Shunqi Mao", "Wei Guo", "Chaoyi Zhang", "Weidong Cai"], "summary": "Diffusion models have shown strong performance in conditional generation by\nprogressively denoising Gaussian noise toward a target data distribution. This\ndenoising process can be interpreted as a form of hill climbing in a learned\nlatent space, where the model iteratively refines the sample toward regions of\nhigher probability. However, diffusion models often converge to local optima\nthat are locally visually coherent yet globally inconsistent or conditionally\nmisaligned, due to latent space complexity and suboptimal initialization. Prior\nefforts attempted to address this by strengthening guidance signals or\nmanipulating the initial noise distribution. We introduce Controlled Random\nZigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect\nand escape such local maxima during conditional generation. The method first\nidentifies potential local maxima using a reward model. Upon detection, it\ninjects noise and reverts to a previous, noisier state to escape the current\noptimization plateau. The reward model then evaluates candidate trajectories,\naccepting only those that offer improvement, while progressively deeper retreat\nenables stronger escapes when nearby alternatives fail. This controlled random\nzigzag process allows dynamic alternation between forward refinement and\nbackward exploration, enhancing both alignment and visual quality in the\ngenerated outputs. The proposed Ctrl-Z Sampling is model-agnostic and\ncompatible with existing diffusion frameworks. Experimental results show that\nCtrl-Z Sampling substantially improves generation quality with only around 7.6X\nincrease in function evaluations.", "comment": "10 pages, 3 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.20294v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20294v1", "AI": {"title_translation": "Ctrl-Z 采样：受控随机之字形探索的扩散采样", "tldr": "提出了一种名为Ctrl-Z采样的新型扩散模型采样策略，通过受控随机之字形探索来逃避局部最优，从而提高生成质量。", "motivation": "扩散模型在条件生成中常收敛到局部最优，导致生成内容在全局上不一致或条件未对齐。", "method": "Ctrl-Z采样通过奖励模型识别潜在局部最优，然后注入噪声并回溯到先前的状态以逃离优化平台。奖励模型评估候选轨迹，只接受有改进的，并允许更深的回溯以进行更强的逃逸，实现正向精炼和反向探索的动态交替。", "result": "Ctrl-Z采样显著提高了生成质量，函数评估次数仅增加约7.6倍。", "conclusion": "Ctrl-Z采样通过其独特的受控随机之字形探索过程，有效解决了扩散模型在条件生成中收敛到局部最优的问题，提升了生成输出的对齐性和视觉质量。", "translation": "扩散模型通过将高斯噪声逐步去噪以接近目标数据分布，在条件生成中展现出强大的性能。这种去噪过程可以被解释为在学习到的潜在空间中进行的一种爬山形式，模型迭代地将样本精炼到更高概率的区域。然而，由于潜在空间的复杂性和次优初始化，扩散模型通常会收敛到局部最优，这些局部最优在局部视觉上连贯，但在全局上不一致或条件未对齐。先前的努力试图通过加强引导信号或操纵初始噪声分布来解决这个问题。我们引入了受控随机之字形采样（Ctrl-Z采样），这是一种新颖的采样策略，旨在条件生成过程中检测并逃避此类局部最大值。该方法首先使用奖励模型识别潜在的局部最大值。检测到后，它会注入噪声并恢复到先前的、噪声更大的状态以逃离当前的优化平台。然后，奖励模型评估候选轨迹，只接受那些提供改进的轨迹，而逐步更深的回溯则在附近替代方案失败时实现更强的逃逸。这种受控的随机之字形过程允许在正向精炼和反向探索之间动态交替，从而增强了生成输出的对齐性和视觉质量。所提出的Ctrl-Z采样是模型无关的，并且与现有扩散框架兼容。实验结果表明，Ctrl-Z采样显著提高了生成质量，而函数评估次数仅增加了约7.6倍。", "summary": "本文提出了一种名为Ctrl-Z采样的新型扩散模型采样策略，旨在解决现有扩散模型在条件生成中易于陷入局部最优的问题。Ctrl-Z采样通过奖励模型识别并逃避局部最大值，其机制包括注入噪声回溯到先前状态，并动态评估和接受改进的轨迹，从而在正向精炼和反向探索之间交替。该方法与模型无关，并显著提升了生成质量和对齐性。", "keywords": "扩散模型, 采样策略, 局部最优逃逸, 条件生成, Ctrl-Z采样", "comments": "这篇论文的创新点在于引入了一种受控的“回溯”机制来帮助扩散模型逃离局部最优，这与传统的加强引导或初始化操纵方法不同。其“之字形探索”的概念提供了一种新颖的优化路径，能够有效平衡精炼与探索，从而提升生成质量和一致性。该方法的模型无关性也增加了其应用潜力。"}}
{"id": "2506.20037", "title": "Verifiable Unlearning on Edge", "authors": ["Mohammad M Maheri", "Alex Davidson", "Hamed Haddadi"], "summary": "Machine learning providers commonly distribute global models to edge devices,\nwhich subsequently personalize these models using local data. However, issues\nsuch as copyright infringements, biases, or regulatory requirements may require\nthe verifiable removal of certain data samples across all edge devices.\nEnsuring that edge devices correctly execute such unlearning operations is\ncritical to maintaining integrity.\n  In this work, we introduce a verification framework leveraging zero-knowledge\nproofs, specifically zk-SNARKs, to confirm data unlearning on personalized\nedge-device models without compromising privacy. We have developed algorithms\nexplicitly designed to facilitate unlearning operations that are compatible\nwith efficient zk-SNARK proof generation, ensuring minimal computational and\nmemory overhead suitable for constrained edge environments. Furthermore, our\napproach carefully preserves personalized enhancements on edge devices,\nmaintaining model performance post-unlearning.\n  Our results affirm the practicality and effectiveness of this verification\nframework, demonstrating verifiable unlearning with minimal degradation in\npersonalization-induced performance improvements. Our methodology ensures\nverifiable, privacy-preserving, and effective machine unlearning across edge\ndevices.", "comment": "This paper has been accepted to the IEEE European Symposium on\n  Security and Privacy (EuroS&P) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20037v1", "categories": ["cs.LG", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20037v1", "AI": {"title_translation": "边缘设备上的可验证遗忘学习", "tldr": "提出了一种基于zk-SNARKs的框架，用于在边缘设备上实现可验证、隐私保护的机器遗忘，同时保持模型性能。", "motivation": "机器学习提供商将全球模型分发到边缘设备进行个性化，但版权、偏见或监管要求可能需要从所有边缘设备上可验证地移除特定数据样本。确保边缘设备正确执行遗忘操作对于维护完整性至关重要。", "method": "引入了一个利用零知识证明（特别是zk-SNARKs）的验证框架，以在不损害隐私的情况下确认个性化边缘设备模型上的数据遗忘。开发了兼容高效zk-SNARK证明生成且计算和内存开销最小的算法，并保留了边缘设备的个性化增强。", "result": "研究结果证实了该验证框架的实用性和有效性，展示了可验证的遗忘学习，对个性化带来的性能提升影响最小。", "conclusion": "该方法确保了边缘设备上可验证、隐私保护且有效的机器遗忘。", "translation": "机器学习提供商通常将全球模型分发给边缘设备，这些设备随后使用本地数据对模型进行个性化。然而，版权侵权、偏见或监管要求等问题可能需要从所有边缘设备上可验证地移除某些数据样本。确保边缘设备正确执行此类遗忘操作对于维护数据完整性至关重要。\n在这项工作中，我们引入了一个利用零知识证明（特别是zk-SNARKs）的验证框架，以在不损害隐私的情况下确认个性化边缘设备模型上的数据遗忘。我们开发了专门的算法，旨在促进与高效zk-SNARK证明生成兼容的遗忘操作，确保适用于受限边缘环境的最小计算和内存开销。此外，我们的方法仔细地保留了边缘设备上的个性化增强，维护了遗忘后的模型性能。\n我们的结果证实了该验证框架的实用性和有效性，展示了可验证的遗忘学习，同时对个性化带来的性能提升影响最小。我们的方法确保了边缘设备上可验证、隐私保护且有效的机器遗忘。", "summary": "该论文提出了一种针对边缘设备上机器遗忘的可验证框架。考虑到版权、偏见或监管要求可能导致需要从边缘设备可验证地移除特定数据，研究引入了利用零知识证明（zk-SNARKs）的验证机制。该框架旨在在不损害隐私的前提下确认数据遗忘，并开发了低计算和内存开销的算法以适应边缘环境，同时保持个性化模型的性能。实验结果证明了该框架的实用性和有效性，实现了可验证、隐私保护且高效的边缘机器遗忘。", "keywords": "可验证遗忘学习, 边缘计算, 零知识证明, zk-SNARKs, 机器遗忘", "comments": "本文创新性地将零知识证明（zk-SNARKs）应用于边缘设备上的机器遗忘验证，解决了在资源受限环境下实现数据可验证移除同时保护隐私和保持模型性能的挑战。其高效性、隐私保护特性以及对个性化性能的维护是其重要贡献。"}}
{"id": "2506.20600", "title": "CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video", "authors": ["Wengxi Li", "Roy Pea", "Nick Haber", "Hari Subramonyam"], "summary": "We introduce CogGen, a learner-centered AI architecture that transforms\nprogramming videos into interactive, adaptive learning experiences by\nintegrating student modeling with generative AI tutoring based on the Cognitive\nApprenticeship framework. The architecture consists of three components: (1)\nvideo segmentation by learning goals, (2) a conversational tutoring engine\napplying Cognitive Apprenticeship strategies, and (3) a student model using\nBayesian Knowledge Tracing to adapt instruction. Our technical evaluation\ndemonstrates effective video segmentation accuracy and strong pedagogical\nalignment across knowledge, method, action, and interaction layers. Ablation\nstudies confirm the necessity of each component in generating effective\nguidance. This work advances AI-powered tutoring by bridging structured student\nmodeling with interactive AI conversations, offering a scalable approach to\nenhancing video-based programming education.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20600v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20600v1", "AI": {"title_translation": "CogGen：一种以学习者为中心的生成式人工智能架构，用于编程视频智能辅导", "tldr": "CogGen是一个以学习者为中心的AI架构，通过结合学生建模和生成式AI辅导，将编程视频转化为互动式自适应学习体验。", "motivation": "旨在将编程视频转化为互动、自适应的学习体验，通过集成学生建模和基于认知学徒制框架的生成式AI辅导，来推进AI驱动的辅导。", "method": "提出了一种名为CogGen的AI架构，包含三个组件：1) 按学习目标进行视频分割；2) 应用认知学徒制策略的会话辅导引擎；3) 使用贝叶斯知识追踪进行教学适应的学生模型。", "result": "技术评估显示了有效的视频分割准确性和在知识、方法、行动和交互层面的强大教学一致性。消融研究证实了每个组件在生成有效指导方面的必要性。", "conclusion": "这项工作通过将结构化学生建模与交互式AI对话相结合，推进了AI驱动的辅导，为增强基于视频的编程教育提供了一种可扩展的方法。", "translation": "我们介绍了CogGen，一个以学习者为中心的人工智能架构，它通过将学生建模与基于认知学徒制框架的生成式人工智能辅导相结合，将编程视频转化为交互式、自适应的学习体验。该架构由三个组件组成：(1) 按学习目标进行的视频分割，(2) 应用认知学徒制策略的会话辅导引擎，以及 (3) 使用贝叶斯知识追踪来调整教学的学生模型。我们的技术评估展示了有效的视频分割准确性以及在知识、方法、行动和交互层面的强大教学一致性。消融研究证实了每个组件在生成有效指导方面的必要性。这项工作通过将结构化学生建模与交互式人工智能对话相结合，推进了人工智能驱动的辅导，为增强基于视频的编程教育提供了一种可扩展的方法。", "summary": "CogGen是一个创新的、以学习者为中心的AI架构，旨在利用生成式AI和学生建模，将编程视频转换为自适应的交互式学习体验。它由视频分割、会话辅导引擎和学生模型三个核心组件构成。技术评估和消融研究证实了其在教学效果和组件必要性方面的有效性，为基于视频的编程教育提供了一种可扩展的AI辅导解决方案。", "keywords": "智能辅导, 生成式AI, 学生建模, 编程教育, 认知学徒制", "comments": "CogGen的创新之处在于其将认知学徒制框架、生成式AI辅导和贝叶斯知识追踪学生模型有效结合，为编程视频学习提供了一个高度个性化和自适应的教学系统。这种集成方法有望显著提升视频学习的互动性和效率，特别是在编程教育领域，具有重要的应用潜力。其模块化设计也使其具有良好的可扩展性。"}}
{"id": "2506.20290", "title": "Don't Hash Me Like That: Exposing and Mitigating Hash-Induced Unfairness in Local Differential Privacy", "authors": ["Berkay Kemal Balioglu", "Alireza Khodaie", "Mehmet Emre Gursoy"], "summary": "Local differential privacy (LDP) has become a widely accepted framework for\nprivacy-preserving data collection. In LDP, many protocols rely on hash\nfunctions to implement user-side encoding and perturbation. However, the\nsecurity and privacy implications of hash function selection have not been\npreviously investigated. In this paper, we expose that the hash functions may\nact as a source of unfairness in LDP protocols. We show that although users\noperate under the same protocol and privacy budget, differences in hash\nfunctions can lead to significant disparities in vulnerability to inference and\npoisoning attacks. To mitigate hash-induced unfairness, we propose Fair-OLH\n(F-OLH), a variant of OLH that enforces an entropy-based fairness constraint on\nhash function selection. Experiments show that F-OLH is effective in mitigating\nhash-induced unfairness under acceptable time overheads.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20290v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20290v1", "AI": {"title_translation": "别那样哈希我：揭示和缓解局部差分隐私中哈希引起的公平性问题", "tldr": "本文揭示了局部差分隐私（LDP）协议中哈希函数可能导致的不公平性，并提出了一种名为Fair-OLH（F-OLH）的新方法来缓解这种问题，实验证明其有效性。", "motivation": "在局部差分隐私（LDP）中，许多协议依赖哈希函数进行用户侧编码和扰动，但哈希函数选择的安全和隐私影响此前未被研究。本文旨在揭示哈希函数可能成为LDP协议中不公平性的来源，因为不同的哈希函数会导致用户在推断和投毒攻击中的漏洞存在显著差异。", "method": "本文首先揭示了哈希函数在局部差分隐私（LDP）协议中可能导致不公平性，表现为尽管用户在相同协议和隐私预算下操作，但哈希函数的差异会导致推断和投毒攻击的漏洞存在显著差异。为缓解这种哈希引起的不公平性，本文提出了Fair-OLH（F-OLH），它是OLH的一种变体，对哈希函数选择施加基于熵的公平性约束。", "result": "实验表明，Fair-OLH（F-OLH）在可接受的时间开销下有效缓解了哈希引起的不公平性。", "conclusion": "本文得出结论，哈希函数在局部差分隐私协议中可能导致不公平性，并且所提出的Fair-OLH方法能够有效地缓解这种由哈希引起的公平性问题。", "translation": "局部差分隐私（LDP）已成为一种广泛接受的隐私保护数据收集框架。在LDP中，许多协议依赖哈希函数来实现用户侧编码和扰动。然而，哈希函数选择的安全和隐私影响此前尚未被研究。在本文中，我们揭示了哈希函数可能成为LDP协议中不公平性的来源。我们表明，尽管用户在相同的协议和隐私预算下操作，但哈希函数的差异可能导致推断和投毒攻击的漏洞存在显著差异。为了缓解哈希引起的不公平性，我们提出了Fair-OLH（F-OLH），它是OLH的一种变体，对哈希函数选择施加了基于熵的公平性约束。实验表明，F-OLH在可接受的时间开销下有效缓解了哈希引起的不公平性。", "summary": "本文探讨了局部差分隐私（LDP）协议中哈希函数选择未被充分研究的安全和隐私影响。研究发现，哈希函数可能导致LDP协议中的不公平性，具体表现为不同哈希函数会导致用户在面对推断和投毒攻击时具有不同的脆弱性。为解决此问题，论文提出了一种新的方法Fair-OLH（F-OLH），它通过对哈希函数选择施加基于熵的公平性约束来缓解这种不公平性。实验结果验证了F-OLH在可接受的时间开销下有效缓解了哈希引起的不公平性。", "keywords": "局部差分隐私, 哈希函数, 不公平性, Fair-OLH, 隐私保护", "comments": "本文的创新之处在于首次揭示了局部差分隐私（LDP）中哈希函数可能导致的不公平性，这是一个此前未被关注的重要问题。通过量化哈希函数差异对攻击漏洞的影响，并提出Fair-OLH这一具有熵基公平性约束的解决方案，该研究为LDP协议的设计提供了新的视角和改进方向，对于提升LDP的实际应用公平性具有重要意义。"}}
{"id": "2506.20525", "title": "Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation", "authors": ["Christian Internò", "Andrea Castellani", "Sebastian Schmitt", "Fabio Stella", "Barbara Hammer"], "summary": "Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of\nhigh-quality datasets and the complex variability of industrial energy\nconsumption patterns. To address data scarcity and privacy issues, we introduce\nthe Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an\nopen-source dataset generated using Digital Twin simulations. SIDED includes\nthree types of industrial facilities across three different geographic\nlocations, capturing diverse appliance behaviors, weather conditions, and load\nprofiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)\nmethod, a computationally efficient technique that enhances NILM model\ngeneralization by intelligently scaling appliance power contributions based on\ntheir relative impact. We show in experiments that NILM models trained with\nAMDA-augmented data significantly improve the disaggregation of energy\nconsumption of complex industrial appliances like combined heat and power\nsystems. Specifically, in our out-of-sample scenarios, models trained with AMDA\nachieved a Normalized Disaggregation Error of 0.093, outperforming models\ntrained without data augmentation (0.451) and those trained with random data\naugmentation (0.290). Data distribution analyses confirm that AMDA effectively\naligns training and test data distributions, enhancing model generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20525v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20525v1", "AI": {"title_translation": "工业能耗分解：基于数字孪生生成的数据集与高效数据增强", "tldr": "针对工业NILM数据稀缺问题，本文提出了一个数字孪生生成的合成数据集SIDED和一种高效数据增强方法AMDA，显著提升了能耗分解模型的泛化能力。", "motivation": "工业非侵入式负荷监测（NILM）受限于高质量数据集的稀缺性以及工业能耗模式的复杂多变性，同时存在数据隐私问题。", "method": "本文引入了合成工业能耗分解数据集（SIDED），这是一个通过数字孪生模拟生成的开源数据集，包含三种工业设施、不同地理位置、多样化的设备行为、天气条件和负荷曲线。此外，提出了一种设备调制数据增强（AMDA）方法，这是一种计算高效的技术，通过智能地根据设备相对影响调整其功率贡献来增强NILM模型的泛化能力。", "result": "使用AMDA增强数据训练的NILM模型显著改善了对复杂工业设备（如热电联产系统）能耗的分解。在样本外场景中，AMDA训练的模型实现了0.093的归一化分解误差，显著优于未数据增强的模型（0.451）和随机数据增强的模型（0.290）。数据分布分析证实AMDA有效地对齐了训练和测试数据分布，增强了模型泛化能力。", "conclusion": "AMDA方法结合数字孪生生成的SIDED数据集，有效解决了工业NILM的数据稀缺和泛化能力问题，显著提升了复杂工业设备能耗分解的准确性。", "translation": "工业非侵入式负荷监测（NILM）受限于高质量数据集的稀缺性以及工业能耗模式的复杂多变性。为了解决数据稀缺和隐私问题，我们引入了合成工业能耗分解数据集（SIDED），这是一个使用数字孪生模拟生成的开源数据集。SIDED包含了三种不同地理位置的工业设施，捕获了多样化的设备行为、天气条件和负荷曲线。我们还提出了一种设备调制数据增强（AMDA）方法，这是一种计算高效的技术，通过智能地根据设备相对影响调整其功率贡献来增强NILM模型的泛化能力。实验表明，使用AMDA增强数据训练的NILM模型显著改善了对复杂工业设备（如热电联产系统）能耗的分解。具体而言，在我们的样本外场景中，使用AMDA训练的模型实现了0.093的归一化分解误差，优于未数据增强的模型（0.451）和随机数据增强的模型（0.290）。数据分布分析证实AMDA有效地对齐了训练和测试数据分布，增强了模型泛化能力。", "summary": "本文针对工业非侵入式负荷监测（NILM）中高质量数据集稀缺和数据隐私的挑战，提出了两个创新解决方案。首先，引入了SIDED，一个利用数字孪生技术生成的开源合成工业能耗分解数据集，涵盖多种设施类型和环境条件。其次，提出了一种高效的设备调制数据增强（AMDA）方法，通过智能调整设备功率贡献来提升NILM模型的泛化能力。实验证明，与未增强或随机增强的数据相比，使用AMDA增强数据训练的模型在复杂工业设备能耗分解方面表现出显著改进，有效降低了归一化分解误差并增强了模型泛化能力。", "keywords": "工业能耗分解, 非侵入式负荷监测, 数字孪生, 数据增强, 合成数据集", "comments": "本文通过引入数字孪生生成的合成数据集SIDED，为工业NILM领域的数据稀缺和隐私问题提供了一个创新性解决方案。同时，提出的AMDA数据增强方法不仅计算高效，而且通过优化数据分布对齐，显著提升了模型在复杂工业环境下的泛化能力，为工业能耗管理和优化提供了有力的工具。"}}
{"id": "2506.20597", "title": "Differential Transformer-driven 6G Physical Layer for Collaborative Perception Enhancement", "authors": ["Soheyb Ribouh", "Osama Saleem", "Mohamed Ababsa"], "summary": "The emergence of 6G wireless networks promises to revolutionize vehicular\ncommunications by enabling ultra-reliable, low-latency, and high-capacity data\nexchange. In this context, collaborative perception techniques, where multiple\nvehicles or infrastructure nodes cooperate to jointly receive and decode\ntransmitted signals, aim to enhance reliability and spectral efficiency for\nConnected Autonomous Vehicle (CAV) applications. In this paper, we propose an\nend-to-end wireless neural receiver based on a Differential Transformer\narchitecture, tailored for 6G V2X communication with a specific focus on\nenabling collaborative perception among connected autonomous vehicles. Our\nmodel integrates key components of the 6G physical layer, designed to boost\nperformance in dynamic and challenging autonomous driving environments. We\nvalidate the proposed system across a range of scenarios, including\n3GPP-defined Urban Macro (UMa) channel. To assess the model's real-world\napplicability, we evaluate its robustness within a V2X framework. In a\ncollaborative perception scenario, our system processes heterogeneous LiDAR and\ncamera data from four connected vehicles in dynamic cooperative vehicular\nnetworks. The results show significant improvements over state-of-the-art\nmethods, achieving an average precision of 0.84, highlighting the potential of\nour proposed approach to enable robust, intelligent, and adaptive wireless\ncooperation for next-generation connected autonomous vehicles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20597v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20597v1", "AI": {"title_translation": "差分Transformer驱动的6G物理层用于协作感知增强", "tldr": "本文提出了一种基于差分Transformer架构的端到端无线神经网络接收器，专门用于6G V2X通信中的协作感知，并在动态V2X环境中显示出显著优于现有方法的性能。", "motivation": "6G无线网络有望通过实现超可靠、低延迟、高容量的数据交换来彻底改变车载通信。在此背景下，协作感知技术旨在提高联网自动驾驶汽车（CAV）应用的可靠性和频谱效率，但需要创新的物理层解决方案来支持其在动态和挑战性环境中的性能。", "method": "本文提出了一种基于差分Transformer架构的端到端无线神经网络接收器，专门为6G V2X通信量身定制，并专注于实现联网自动驾驶汽车之间的协作感知。该模型集成了6G物理层的关键组件，旨在提高在动态和挑战性自动驾驶环境中的性能。系统在多种场景下进行验证，包括3GPP定义的城市宏（UMa）信道，并在V2X框架内评估其鲁棒性。在协作感知场景中，系统处理来自四个联网车辆的异构LiDAR和摄像头数据。", "result": "实验结果表明，该系统比现有方法有显著改进，实现了0.84的平均精度，突出了所提出方法在下一代联网自动驾驶汽车中实现鲁棒、智能和自适应无线合作的潜力。", "conclusion": "本文提出的基于差分Transformer的6G物理层无线神经网络接收器，在协作感知场景下，能够显著提升联网自动驾驶汽车的性能，并展现出在未来V2X通信中实现鲁棒、智能和自适应无线合作的巨大潜力。", "translation": "6G无线网络的出现有望通过实现超可靠、低延迟和高容量的数据交换来彻底改变车载通信。在此背景下，协作感知技术，即多个车辆或基础设施节点协同接收和解码传输信号，旨在提高联网自动驾驶汽车（CAV）应用的可靠性和频谱效率。在本文中，我们提出了一种基于差分Transformer架构的端到端无线神经网络接收器，专为6G V2X通信量身定制，特别关注实现联网自动驾驶汽车之间的协作感知。我们的模型集成了6G物理层的关键组件，旨在提高在动态和挑战性自动驾驶环境中的性能。我们在包括3GPP定义的城市宏（UMa）信道在内的一系列场景中验证了所提出的系统。为了评估模型的实际适用性，我们在V2X框架内评估了其鲁棒性。在协作感知场景中，我们的系统处理来自动态协作车载网络中四辆联网汽车的异构LiDAR和摄像头数据。结果显示，与现有技术相比有显著改进，实现了0.84的平均精度，突出了我们提出的方法在下一代联网自动驾驶汽车中实现鲁棒、智能和自适应无线合作的潜力。", "summary": "本文提出了一种基于差分Transformer架构的端到端无线神经网络接收器，旨在增强6G V2X通信中的协作感知能力。该模型结合了6G物理层组件，以提高在动态自动驾驶环境中的性能。通过处理来自多个联网车辆的异构传感器数据，该系统在协作感知场景中表现出显著优于现有方法的性能，平均精度达到0.84，证明了其在未来联网自动驾驶中实现智能、自适应无线合作的潜力。", "keywords": "6G, 差分Transformer, 协作感知, V2X, 物理层", "comments": "该论文的创新点在于将差分Transformer架构应用于6G物理层，以实现端到端的无线神经网络接收器，从而增强联网自动驾驶车辆的协作感知能力。其重要性在于为未来6G V2X通信提供了潜在的高性能解决方案，特别是在处理动态和异构数据方面。论文通过在真实V2X场景中验证其性能，并显示出显著的性能提升，这对于推动CAV技术的发展具有积极意义。"}}
{"id": "2506.20199", "title": "How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?", "authors": ["Mengqi Wang", "Tiantian Feng", "Shrikanth Narayanan"], "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20199v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20199v1", "AI": {"title_translation": "如何在大语言模型的语境学习中检索示例以改进对话情感识别？", "tldr": "大语言模型在对话情感识别（CER）中面临挑战，本研究探索通过语境学习（ICL）中的示例检索来改进CER，发现增强型示例检索在所有测试数据集上表现最佳，强调了检索连贯目标示例并通过释义增强的重要性。", "motivation": "大语言模型在情感识别等主观任务中创建高精度应用仍具挑战。本研究受SLT 2024 GenSER挑战启发，旨在探索如何通过语境学习中的示例检索来改进大语言模型的对话情感识别（CER）。", "method": "本研究探索了如何在语境学习（ICL）中检索高质量示例以增强对话情感识别（CER）。提出了基于随机和增强型示例检索的多种策略，并分析了对话语境对CER准确性的影响。实验在IEMOCAP、MELD和EmoryNLP三个数据集上进行。", "result": "实验结果显示，增强型示例检索在所有数据集上始终优于其他调查技术。这突出强调了检索连贯的目标示例并通过释义进行增强的重要性。", "conclusion": "通过检索连贯的目标示例并进行释义增强，可以有效提高大语言模型在对话情感识别任务中的性能。", "translation": "大语言模型（LLMs）已在各个领域实现了广泛的实际应用。然而，创建高精度、高性能的应用仍然具有挑战性，特别是对于情感识别等主观任务。受SLT 2024 GenSER挑战的启发，本研究探讨了使用LLM改进对话情感识别（CER）的方法。具体来说，我们探索了如何在语境学习（ICL）中检索高质量示例以增强CER。我们提出了基于随机和增强型示例检索的各种策略，并分析了对话语境对CER准确性的影响。实验在IEMOCAP、MELD和EmoryNLP三个数据集上进行。结果表明，增强型示例检索在所有数据集上始终优于其他调查技术，突出了检索连贯目标示例并通过释义增强它们的重要性。", "summary": "本研究旨在解决大语言模型在对话情感识别（CER）等主观任务中精度不足的挑战。论文受SLT 2024 GenSER挑战启发，探索了在语境学习（ICL）中检索高质量示例以提升CER的方法。为此，研究提出了随机和增强型示例检索策略，并评估了对话语境对准确性的影响。实验结果表明，通过释义增强的增强型示例检索在IEMOCAP、MELD和EmoryNLP等多个数据集上表现最佳，强调了检索连贯目标示例的重要性。", "keywords": "对话情感识别, 大语言模型, 语境学习, 示例检索, 增强型检索", "comments": "本文创新性地探讨了在大语言模型语境学习中通过优化示例检索来提高对话情感识别的精度，特别是引入了“增强型示例检索”的概念，并通过实验验证了其有效性。这对于提升LLM在主观、细粒度任务上的表现具有重要意义。"}}
{"id": "2506.20608", "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base", "authors": ["Barry Smith", "Junchao Zhang", "Hong Zhang", "Lois Curfman McInnes", "Murat Keceli", "Archit Vasan", "Satish Balay", "Toby Isaac", "Le Chen", "Venkatram Vishwanath"], "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20608v1", "categories": ["cs.AI", "cs.NA", "math.NA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20608v1", "AI": {"title_translation": "AI助手增强和利用PETSc知识库", "tldr": "本文介绍了PETSc团队如何利用大型语言模型（LLMs）和定制工具（如RAG、重排序和聊天机器人）来整合和激活其庞大但分散的知识库，旨在提升用户支持、开发效率和文档质量。", "motivation": "PETSc拥有庞大但分散的知识库，其中许多信息对用户和新开发者而言难以获取。生成式AI，特别是大型语言模型，为解决这一问题提供了新途径。", "method": "团队构建了一个基于LLM的系统，该系统结合PETSc内容和定制的LLM工具，包括检索增强生成（RAG）、重排序算法和聊天机器人。研究重点是系统架构、RAG和重排序的应用、不同LLM和嵌入模型的评估方法以及用户界面设计。", "result": "论文展示了设计和评估这些工具的初步经验，分析了LLM响应如何增强数值软件的开发和使用，并初步关注了可扩展的Krylov求解器。", "conclusion": "目标是建立一个可扩展的、以知识为中心的科学软件AI框架，以实现可扩展支持、丰富文档和增强研发工作流程。未来将把此系统扩展为一个强大的、不断发展的平台，以加速科学发现。", "translation": "生成式人工智能，特别是通过大型语言模型（LLMs），正在改变技术知识的获取、重用和扩展方式。PETSc是一个广泛使用的高性能科学计算数值库，在其三十年的发展过程中积累了丰富但分散的知识库，涵盖源代码、文档、邮件列表、GitLab问题、Discord对话、技术论文等。其中大部分知识仍是非正式的，用户和新开发者难以获取。为了更有效地激活和利用这个知识库，PETSc团队已开始构建一个由LLM驱动的系统，该系统将PETSc内容与定制的LLM工具相结合——包括检索增强生成（RAG）、重排序算法和聊天机器人——以协助用户、支持开发者并提出正式文档的更新建议。本文介绍了设计和评估这些工具的初步经验，重点关注系统架构、RAG和重排序在PETSc特定信息中的应用、各种LLM和嵌入模型的评估方法以及用户界面设计。利用阿贡领导力计算设施的资源，我们分析了LLM响应如何增强数值软件的开发和使用，初步侧重于可扩展的Krylov求解器。我们的目标是建立一个可扩展的、以知识为中心的科学软件AI框架，从而实现可扩展的支持、丰富的文档和增强的研发工作流程。最后，我们概述了将该系统扩展为一个强大、不断发展的平台的未来方向，以推动软件生态系统发展，加速科学发现。", "summary": "本文介绍了PETSc团队如何利用大型语言模型（LLMs）构建一个AI助手系统，以整合和激活其庞大但分散的知识库。该系统结合了PETSc内容与检索增强生成（RAG）、重排序算法和聊天机器人等定制LLM工具，旨在提升用户支持、开发效率和文档质量。论文探讨了系统架构、RAG应用、模型评估方法及用户界面设计，并展示了初步经验，旨在为科学软件建立一个可扩展的知识中心AI框架。", "keywords": "PETSc, 大型语言模型, 知识库, 检索增强生成, 科学计算", "comments": "这篇论文探讨了将前沿的生成式AI技术，特别是大型语言模型，应用于传统科学计算库知识管理的创新性。PETSc作为成熟的数值库，其知识库的碎片化是许多大型开源项目面临的普遍问题。通过结合RAG和定制工具，该方法有望显著提高知识的可访问性和利用效率，为科学软件生态系统的发展提供了一个有前景的范例。"}}
{"id": "2506.20485", "title": "EANS: Reducing Energy Consumption for UAV with an Environmental Adaptive Navigation Strategy", "authors": ["Tian Liu", "Han Liu", "Boyang Li", "Long Chen", "Kai Huang"], "summary": "Unmanned Aerial Vehicles (UAVS) are limited by the onboard energy. Refinement\nof the navigation strategy directly affects both the flight velocity and the\ntrajectory based on the adjustment of key parameters in the UAVS pipeline, thus\nreducing energy consumption. However, existing techniques tend to adopt static\nand conservative strategies in dynamic scenarios, leading to inefficient energy\nreduction. Dynamically adjusting the navigation strategy requires overcoming\nthe challenges including the task pipeline interdependencies, the\nenvironmental-strategy correlations, and the selecting parameters. To solve the\naforementioned problems, this paper proposes a method to dynamically adjust the\nnavigation strategy of the UAVS by analyzing its dynamic characteristics and\nthe temporal characteristics of the autonomous navigation pipeline, thereby\nreducing UAVS energy consumption in response to environmental changes. We\ncompare our method with the baseline through hardware-in-the-loop (HIL)\nsimulation and real-world experiments, showing our method 3.2X and 2.6X\nimprovements in mission time, 2.4X and 1.6X improvements in energy,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20485v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20485v1", "AI": {"title_translation": "EANS：一种用于无人机环境自适应导航策略的能耗降低方法", "tldr": "本文提出EANS，通过动态调整无人机导航策略，根据环境变化降低能耗，并在硬件在环仿真和真实世界实验中显示出显著改进。", "motivation": "无人机（UAV）受限于板载能源，现有导航策略在动态场景中采用静态和保守方式，导致能耗效率低下。动态调整策略面临任务管线相互依赖性、环境-策略相关性及参数选择等挑战。", "method": "本文提出一种方法，通过分析无人机的动态特性和自主导航管线的时间特性，动态调整无人机的导航策略，以响应环境变化并降低能耗。", "result": "与基线方法相比，本文方法在硬件在环（HIL）仿真和真实世界实验中，任务时间分别提升了3.2倍和2.6倍，能耗分别降低了2.4倍和1.6倍。", "conclusion": "本文提出的EANS方法通过动态调整无人机导航策略以适应环境变化，有效降低了无人机的能耗，并在实验中取得了显著的性能提升。", "translation": "无人机（UAVS）受限于其板载能源。导航策略的优化直接影响飞行速度和轨迹，通过调整无人机管线中的关键参数来降低能耗。然而，现有技术在动态场景中倾向于采用静态和保守的策略，导致能源效率低下。动态调整导航策略需要克服的挑战包括任务管线相互依赖性、环境-策略相关性以及参数选择。为了解决上述问题，本文提出了一种通过分析无人机动态特性和自主导航管线的时间特性来动态调整无人机导航策略的方法，从而响应环境变化降低无人机能耗。我们通过硬件在环（HIL）仿真和真实世界实验将我们的方法与基线进行比较，结果显示我们的方法在任务时间上分别提高了3.2倍和2.6倍，在能耗上分别提高了2.4倍和1.6倍。", "summary": "本文提出了EANS（环境自适应导航策略），旨在解决无人机能耗受限的问题。针对现有静态导航策略在动态环境中效率低下的问题，EANS通过分析无人机的动态特性和自主导航管线的时间特性，实现导航策略的动态调整，以响应环境变化并降低能耗。通过硬件在环仿真和真实实验验证，EANS在任务时间和能耗方面均取得了显著提升。", "keywords": "能耗降低, 无人机, 自适应导航, 动态策略, 环境适应", "comments": "该论文的创新点在于提出了环境自适应的动态导航策略，而非传统的静态保守策略，从而有效解决了无人机能耗效率低下的问题。其通过硬件在环仿真和真实世界实验验证了方法的有效性，并提供了量化的性能提升数据，增强了研究的说服力。这对于延长无人机续航能力具有重要的实践意义。"}}
{"id": "2506.20302", "title": "TDiR: Transformer based Diffusion for Image Restoration Tasks", "authors": ["Abbas Anwar", "Mohammad Shullar", "Ali Arshad Nasir", "Mudassir Masood", "Saeed Anwar"], "summary": "Images captured in challenging environments often experience various forms of\ndegradation, including noise, color cast, blur, and light scattering. These\neffects significantly reduce image quality, hindering their applicability in\ndownstream tasks such as object detection, mapping, and classification. Our\ntransformer-based diffusion model was developed to address image restoration\ntasks, aiming to improve the quality of degraded images. This model was\nevaluated against existing deep learning methodologies across multiple quality\nmetrics for underwater image enhancement, denoising, and deraining on publicly\navailable datasets. Our findings demonstrate that the diffusion model, combined\nwith transformers, surpasses current methods in performance. The results of our\nmodel highlight the efficacy of diffusion models and transformers in improving\nthe quality of degraded images, consequently expanding their utility in\ndownstream tasks that require high-fidelity visual data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20302v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20302v1", "AI": {"title_translation": "TDiR: 基于Transformer的扩散模型用于图像恢复任务", "tldr": "TDiR是一种基于Transformer的扩散模型，用于图像恢复，在水下图像增强、去噪和去雨任务上表现优于现有方法。", "motivation": "挑战性环境中捕获的图像常遭受噪声、色偏、模糊和光散射等多种降质，严重降低图像质量，并影响其在目标检测、测绘和分类等下游任务中的应用。", "method": "开发了一种基于Transformer的扩散模型来解决图像恢复任务，并旨在提高降质图像的质量。该模型在公开数据集上，针对水下图像增强、去噪和去雨任务，通过多项质量指标与现有深度学习方法进行了评估。", "result": "研究结果表明，结合Transformer的扩散模型在性能上超越了现有方法。", "conclusion": "该模型的结果突出了扩散模型和Transformer在改善降质图像质量方面的有效性，从而扩展了它们在需要高保真视觉数据的下游任务中的实用性。", "translation": "在挑战性环境中捕获的图像常遭受多种形式的降质，包括噪声、色偏、模糊和光散射。这些影响严重降低了图像质量，阻碍了它们在目标检测、测绘和分类等下游任务中的应用。我们开发了一种基于Transformer的扩散模型来解决图像恢复任务，旨在提高降质图像的质量。该模型在公开数据集上，针对水下图像增强、去噪和去雨任务，通过多项质量指标与现有深度学习方法进行了评估。我们的研究结果表明，结合Transformer的扩散模型在性能上超越了现有方法。我们模型的结果突出了扩散模型和Transformer在改善降质图像质量方面的有效性，从而扩展了它们在需要高保真视觉数据的下游任务中的实用性。", "summary": "本文提出了一种名为TDiR的基于Transformer的扩散模型，用于解决图像恢复任务，如水下图像增强、去噪和去雨。该模型旨在改善在挑战性环境中捕获的降质图像质量，以提高其在下游任务中的适用性。通过在公开数据集上与现有深度学习方法进行比较，TDiR在多项质量指标上表现出优越的性能，验证了扩散模型和Transformer结合在图像恢复领域的有效性。", "keywords": "图像恢复, 扩散模型, Transformer, 图像增强, 去噪", "comments": "该论文的创新点在于将Transformer架构与扩散模型相结合应用于图像恢复任务。这种结合有效地解决了图像降质问题，并展示了在多种恢复任务上的卓越性能，预示着其在需要高质量视觉数据的下游应用中的巨大潜力。"}}
{"id": "2506.20040", "title": "Cross-Layer Discrete Concept Discovery for Interpreting Language Models", "authors": ["Ankur Garg", "Xuemin Yu", "Hassan Sajjad", "Samira Ebrahimi Kahou"], "summary": "Uncovering emergent concepts across transformer layers remains a significant\nchallenge because the residual stream linearly mixes and duplicates\ninformation, obscuring how features evolve within large language models.\nCurrent research efforts primarily inspect neural representations at single\nlayers, thereby overlooking this cross-layer superposition and the redundancy\nit introduces. These representations are typically either analyzed directly for\nactivation patterns or passed to probing classifiers that map them to a limited\nset of predefined concepts. To address these limitations, we propose\n\\gls{clvqvae}, a framework that uses vector quantization to map representations\nacross layers and in the process collapse duplicated residual-stream features\ninto compact, interpretable concept vectors. Our approach uniquely combines\ntop-$k$ temperature-based sampling during quantization with EMA codebook\nupdates, providing controlled exploration of the discrete latent space while\nmaintaining code-book diversity. We further enhance the framework with\nscaled-spherical k-means++ for codebook initialization, which clusters by\ndirectional similarity rather than magnitude, better aligning with semantic\nstructure in word embedding space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20040v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20040v1", "AI": {"title_translation": "跨层离散概念发现用于解释语言模型", "tldr": "提出CLVQVAE框架，通过矢量量化发现跨层概念，解决大语言模型中信息混合和重复导致的概念难以解释的问题。", "motivation": "当前研究主要在单层检查神经网络表示，忽略了跨层叠加和冗余，且分析方法通常是直接检查激活模式或使用探测分类器映射到有限的预定义概念，这限制了对大语言模型内部特征演化的理解。", "method": "提出CLVQVAE框架，利用矢量量化将跨层表示映射为紧凑、可解释的概念向量，并消除重复的残差流特征。该方法独特地结合了量化过程中的top-k基于温度的采样和EMA码本更新，以实现离散潜在空间的受控探索并保持码本多样性。此外，通过scaled-spherical k-means++进行码本初始化，该方法根据方向相似性而非幅度进行聚类，更好地与词嵌入空间中的语义结构对齐。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "揭示Transformer层之间涌现的概念仍然是一个重大挑战，因为残差流线性混合并复制信息，模糊了大型语言模型中特征的演变方式。当前的研究工作主要在单层检查神经网络表示，从而忽视了这种跨层叠加及其引入的冗余。这些表示通常要么直接分析其激活模式，要么传递给探测分类器，将它们映射到有限的预定义概念。为了解决这些限制，我们提出了CLVQVAE，这是一个利用矢量量化来映射跨层表示，并在过程中将重复的残差流特征折叠成紧凑、可解释的概念向量的框架。我们的方法独特地结合了量化过程中的top-k基于温度的采样和EMA码本更新，提供对离散潜在空间的受控探索，同时保持码本多样性。我们通过scaled-spherical k-means++进一步增强了该框架的码本初始化，该方法通过方向相似性而非幅度进行聚类，更好地与词嵌入空间中的语义结构对齐。", "summary": "本文提出了CLVQVAE框架，旨在解决大型语言模型中跨层概念难以解释的问题。针对现有研究忽视跨层信息叠加和冗余的局限性，CLVQVAE利用矢量量化技术将语言模型不同层级的表示映射为离散、可解释的概念向量，并有效处理残差流中重复的信息。该框架通过结合top-k温度采样、EMA码本更新以及scaled-spherical k-means++初始化，确保了离散潜在空间的有效探索、码本多样性以及与语义结构的对齐。", "keywords": "语言模型解释, 概念发现, 矢量量化, 跨层分析, Transformer", "comments": "这项工作通过引入跨层概念发现框架CLVQVAE，为解释大型语言模型内部运作机制提供了一种新颖的方法。其创新点在于结合了矢量量化、基于温度的采样和EMA码本更新，以及考虑语义结构的码本初始化，有望更准确地揭示模型内部的演化概念，克服了传统单层分析和预定义概念的局限性。这对于提高模型的可解释性和可信度具有重要意义。"}}
{"id": "2506.20415", "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models", "authors": ["Dipayan Saha", "Shams Tarek", "Hasan Al Shaikh", "Khan Thamid Hasan", "Pavan Sai Nalluri", "Md. Ajoad Hasan", "Nashmin Alam", "Jingbo Zhou", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "summary": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20415v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20415v1", "AI": {"title_translation": "SV-LLM：一种使用大型语言模型进行SoC安全验证的代理方法", "tldr": "SV-LLM是一个多智能体系统，利用LLM自动化和增强SoC安全验证，通过集成专业代理和优化学习范式来提高效率和准确性。", "motivation": "传统的SoC安全验证技术在自动化、可扩展性、全面性和适应性方面面临巨大挑战，无法跟上复杂SoC设计的安全需求。大型语言模型（LLMs）的出现为解决这些问题提供了新范式。", "method": "提出SV-LLM，一个新颖的多智能体辅助系统。该系统通过创建多智能体系统，其中专门的LLM协同工作。这些代理负责验证问答、安全资产识别、威胁建模、测试计划和属性生成、漏洞检测以及基于仿真的错误验证等任务。为了优化性能，代理利用不同的学习范式，如上下文学习、微调和检索增强生成（RAG）。", "result": "旨在减少人工干预，提高准确性，加速安全分析，支持在设计周期早期主动识别和缓解风险。通过说明性案例研究和实验展示了其在改变硬件安全实践方面的潜力、适用性和有效性。", "conclusion": "SV-LLM通过其多智能体方法和LLM的集成，为SoC安全验证提供了一个创新且高效的解决方案，有望显著改善硬件安全实践。", "translation": "确保复杂片上系统（SoC）设计的安全性至关重要，然而，传统的验证技术由于在自动化、可扩展性、全面性和适应性方面面临巨大挑战，难以跟上进度。大型语言模型（LLM）的出现，凭借其在自然语言理解、代码生成和高级推理方面的卓越能力，为解决这些问题提供了新的范式。超越单一模型，代理方法允许创建多智能体系统，其中专门的LLM协同工作，更有效地解决复杂问题。认识到这一机遇，我们引入了SV-LLM，一个新颖的多智能体辅助系统，旨在自动化和增强SoC安全验证。通过集成用于验证问答、安全资产识别、威胁建模、测试计划和属性生成、漏洞检测以及基于仿真的错误验证等任务的专业代理，SV-LLM简化了工作流程。为了优化其在这些多样化任务中的性能，代理利用了不同的学习范式，例如上下文学习、微调和检索增强生成（RAG）。该系统旨在减少人工干预，提高准确性，加速安全分析，支持在设计周期早期主动识别和缓解风险。我们通过说明性案例研究和实验展示了其改变硬件安全实践的潜力、适用性和有效性。", "summary": "SV-LLM是一个利用大型语言模型（LLMs）的多智能体系统，旨在自动化和增强片上系统（SoC）的安全验证。它通过集成专门的LLM代理来处理如威胁建模、漏洞检测等任务，并采用多种学习范式（如RAG）来优化性能，从而减少人工干预，提高验证效率和准确性，实现早期风险识别和缓解。", "keywords": "SoC安全验证, 大型语言模型, 多智能体系统, SV-LLM, 硬件安全", "comments": "该论文创新性地将多智能体系统与大型语言模型相结合，以解决传统SoC安全验证的痛点。其代理方法能够有效分解复杂任务，提高自动化程度和验证效率。该系统在硬件安全领域具有重要意义，有望推动设计周期早期风险识别和缓解。"}}
{"id": "2506.20554", "title": "Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control", "authors": ["Andrew Mole", "Max Weissenbacher", "Georgios Rigas", "Sylvain Laizet"], "summary": "Traditional wind farm control operates each turbine independently to maximize\nindividual power output. However, coordinated wake steering across the entire\nfarm can substantially increase the combined wind farm energy production.\nAlthough dynamic closed-loop control has proven effective in flow control\napplications, wind farm optimization has relied primarily on static,\nlow-fidelity simulators that ignore critical turbulent flow dynamics. In this\nwork, we present the first reinforcement learning (RL) controller integrated\ndirectly with high-fidelity large-eddy simulation (LES), enabling real-time\nresponse to atmospheric turbulence through collaborative, dynamic control\nstrategies. Our RL controller achieves a 4.30% increase in wind farm power\noutput compared to baseline operation, nearly doubling the 2.19% gain from\nstatic optimal yaw control obtained through Bayesian optimization. These\nresults establish dynamic flow-responsive control as a transformative approach\nto wind farm optimization, with direct implications for accelerating renewable\nenergy deployment to net-zero targets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20554v1", "categories": ["physics.flu-dyn", "cs.LG", "cs.SY", "eess.SY"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.20554v1", "AI": {"title_translation": "强化学习通过实现闭环协作控制提高风电场发电量", "tldr": "该研究提出了一种将强化学习控制器与高精度大涡模拟相结合的方法，以实现风电场的实时动态控制，从而显著提高发电量，优于传统的静态优化方法。", "motivation": "传统风电场控制独立运行每台涡轮机以最大化个体功率输出，但协同尾流转向可以显著增加整个风电场的总能量产出。现有风电场优化主要依赖静态、低精度模拟器，忽略了关键的湍流动力学，而动态闭环控制在流体控制应用中已被证明有效。", "method": "本文提出了第一个直接与高精度大涡模拟 (LES) 集成的强化学习 (RL) 控制器，通过协作、动态控制策略实现对大气湍流的实时响应。", "result": "与基线操作相比，我们的强化学习控制器使风电场功率输出增加了 4.30%，几乎是贝叶斯优化获得的静态最优偏航控制增益 2.19% 的两倍。", "conclusion": "动态流体响应控制被确立为风电场优化的一种变革性方法，对加速可再生能源部署以实现净零目标具有直接影响。", "translation": "传统风电场控制独立运行每台涡轮机以最大化个体功率输出。然而，在整个风电场进行协调的尾流转向可以显著增加风电场的总能量产出。尽管动态闭环控制在流体控制应用中已被证明有效，但风电场优化主要依赖于静态、低精度模拟器，这些模拟器忽略了关键的湍流动力学。在这项工作中，我们提出了第一个直接与高精度大涡模拟 (LES) 集成的强化学习 (RL) 控制器，通过协作、动态控制策略实现对大气湍流的实时响应。我们的强化学习控制器实现了风电场功率输出 4.30% 的增长，与基线操作相比，这几乎是通过贝叶斯优化获得的静态最优偏航控制增益 2.19% 的两倍。这些结果确立了动态流体响应控制作为风电场优化的一种变革性方法，对加速可再生能源部署以实现净零目标具有直接影响。", "summary": "本研究提出了一种创新性的强化学习（RL）控制器，该控制器首次直接集成到高精度大涡模拟（LES）中，旨在通过动态闭环协作控制来优化风电场发电。与传统独立控制和静态优化方法不同，该RL控制器能够实时响应大气湍流，从而显著提高了风电场的总功率输出。实验结果显示，该方法使风电场功率输出增加了4.30%，远超静态最优偏航控制的2.19%增益，证明了动态流体响应控制在风电场优化方面的巨大潜力及其对可再生能源发展的积极影响。", "keywords": "强化学习, 风电场优化, 闭环控制, 尾流转向, 大涡模拟", "comments": "该论文的创新之处在于首次将强化学习控制器与高精度大涡模拟直接集成，实现了风电场在真实湍流环境下的动态、实时优化控制。这克服了传统静态低精度模拟的局限性，提供了一种更高效、更具适应性的风电场管理方法。其重要性在于显著提升了风电场的发电效率，对加速可再生能源部署和实现净零排放目标具有直接且深远的意义。"}}
{"id": "2506.20637", "title": "MC for Agriculture: A Framework for Nature-inspired Sustainable Pest Control", "authors": ["Fardad Vakilipoor", "Nora Hirschmann", "Julian Schladt", "Stefan Schwab", "Annette Reineke", "Robert Schober", "Kathrin Castiglione", "Maximilian Schaefer"], "summary": "In agriculture, molecular communication (MC) is envisioned as a framework to\naddress critical challenges such as smart pest control. While conventional\napproaches mostly rely on synthetic plant protection products, posing high\nrisks for the environment, harnessing plant signaling processes can lead to\ninnovative approaches for nature-inspired sustainable pest control. In this\npaper, we investigate an approach for sustainable pest control and reveal how\nthe MC paradigm can be employed for analysis and optimization. In particular,\nwe consider a system where herbivore-induced plant volatiles (HIPVs),\nspecifically methyl salicylate (MeSA), is encapsulated into microspheres\ndeployed on deployed on plant leaves. The controlled release of MeSA from the\nmicrospheres, acting as transmitters (TXs), supports pest deterrence and\nantagonist attraction, providing an eco-friendly alternative to synthetic plant\nprotection products. Based on experimental data, we investigate the MeSA\nrelease kinetics and obtain an analytical model. To describe the propagation of\nMeSA in farming environments, we employ a three dimensional (3D)\nadvection-diffusion model, incorporating realistic wind fields which are\npredominantly affecting particle propagation, and solve it by a finite\ndifference method (FDM). The proposed model is used to investigate the MeSA\ndistribution for different TX arrangements, representing different practical\nmicrosphere deployment strategies. Moreover, we introduce the coverage\neffectiveness index (CEI) as a novel metric to quantify the environmental\ncoverage of MeSA. This analysis offers valuable guidance for the practical\ndevelopment of microspheres and their deployment aimed at enhancing coverage\nand, consequently, the attraction of antagonistic insects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20637v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20637v1", "AI": {"title_translation": "MC在农业中的应用：一种受自然启发的持续害虫防治框架", "tldr": "本文提出并研究了分子通信（MC）在农业害虫防治中的应用，通过封装植物挥发物（如MeSA）于微球中，实现环保的害虫驱避和天敌吸引，并建立了其释放和传播模型。", "motivation": "传统害虫防治方法依赖合成植物保护产品，对环境风险高。本文旨在探索一种受自然启发的可持续害虫防治方法，利用植物信号过程，为环保型害虫控制提供创新途径。", "method": "本文利用分子通信（MC）范式分析和优化可持续害虫防治。具体方法包括：将植物诱导挥发物（HIPVs），特别是水杨酸甲酯（MeSA），封装在部署于植物叶片上的微球中，作为发射器实现害虫驱避和天敌吸引。基于实验数据，研究MeSA释放动力学并获得分析模型。采用三维（3D）对流-扩散模型描述MeSA在农业环境中的传播，并使用有限差分法（FDM）求解，该模型整合了现实风场。此外，引入覆盖有效性指数（CEI）作为量化MeSA环境覆盖的新指标。", "result": "获得了MeSA释放动力学的分析模型。使用所提出的模型研究了不同发射器布置下MeSA的分布，代表了不同的微球部署策略。引入了覆盖有效性指数（CEI）作为量化MeSA环境覆盖的新指标。", "conclusion": "本文的分析为微球的实际开发及其部署提供了宝贵的指导，旨在提高覆盖范围，从而增强天敌昆虫的吸引力，为可持续害虫防治提供环保替代方案。", "translation": "在农业领域，分子通信（MC）被设想为一个框架，以解决智能害虫防治等关键挑战。虽然传统方法大多依赖合成植物保护产品，对环境构成高风险，但利用植物信号过程可以带来受自然启发的创新可持续害虫防治方法。在本文中，我们研究了一种可持续害虫防治方法，并揭示了如何将MC范式用于分析和优化。特别是，我们考虑了一个系统，其中草食动物诱导的植物挥发物（HIPVs），特别是水杨酸甲酯（MeSA），被封装在部署在植物叶片上的微球中。MeSA从微球（作为发射器）中的受控释放支持害虫驱避和天敌吸引，为合成植物保护产品提供了一种环保替代方案。基于实验数据，我们研究了MeSA的释放动力学并获得了分析模型。为了描述MeSA在农业环境中的传播，我们采用了一个三维（3D）对流-扩散模型，该模型结合了主要影响粒子传播的现实风场，并通过有限差分法（FDM）求解。所提出的模型用于研究不同发射器布置下MeSA的分布，代表了不同的实际微球部署策略。此外，我们引入了覆盖有效性指数（CEI）作为量化MeSA环境覆盖的新指标。这项分析为微球的实际开发及其部署提供了宝贵的指导，旨在提高覆盖范围，从而增强天敌昆虫的吸引力。", "summary": "本文探索了将分子通信（MC）应用于农业可持续害虫防治的新框架，以替代对环境有害的合成农药。研究提出了一种利用封装在微球中的植物挥发物（如水杨酸甲酯MeSA）来驱避害虫和吸引天敌的方法。通过实验数据，建立了MeSA的释放动力学分析模型，并利用三维对流-扩散模型模拟其在农田环境中的传播，考虑了实际风场。研究还引入了覆盖有效性指数（CEI）来评估MeSA的分布效果。这项工作为开发环保型微球部署策略以优化害虫控制提供了理论和实践指导。", "keywords": "分子通信, 害虫防治, 水杨酸甲酯, 微球, 可持续农业", "comments": "这篇论文的创新之处在于将分子通信（MC）的概念引入到农业害虫防治领域，提供了一种环保且受自然启发的替代方案。通过利用植物天然的防御机制（HIPVs）并结合微球缓释技术，有效地解决了传统农药的环境污染问题。模型的建立和CEI指标的引入，为实际应用中的微球部署和效果评估提供了量化工具和指导，具有重要的实践意义。"}}
{"id": "2506.20203", "title": "Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation", "authors": ["Petra Barančíková", "Ondřej Bojar"], "summary": "In this paper, we compare Czech-specific and multilingual sentence embedding\nmodels through intrinsic and extrinsic evaluation paradigms. For intrinsic\nevaluation, we employ Costra, a complex sentence transformation dataset, and\nseveral Semantic Textual Similarity (STS) benchmarks to assess the ability of\nthe embeddings to capture linguistic phenomena such as semantic similarity,\ntemporal aspects, and stylistic variations. In the extrinsic evaluation, we\nfine-tune each embedding model using COMET-based metrics for machine\ntranslation evaluation.\n  Our experiments reveal an interesting disconnect: models that excel in\nintrinsic semantic similarity tests do not consistently yield superior\nperformance on downstream translation evaluation tasks. Conversely, models with\nseemingly over-smoothed embedding spaces can, through fine-tuning, achieve\nexcellent results. These findings highlight the complex relationship between\nsemantic property probes and downstream task, emphasizing the need for more\nresearch into 'operationalizable semantics' in sentence embeddings, or more\nin-depth downstream tasks datasets (here translation evaluation)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20203v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20203v1", "AI": {"title_translation": "捷克语句嵌入的内在与外在评估：语义相关性无助于机器翻译评估", "tldr": "本研究发现，在语义相似性测试中表现出色的语句嵌入模型，在机器翻译评估任务中不一定表现更好；看似过度平滑的模型在微调后反而能取得优异结果，这表明语义属性探测与下游任务之间存在复杂关系。", "motivation": "本研究旨在通过内在和外在评估范式，比较特定于捷克语和多语言的语句嵌入模型，并探究语义相关性捕获能力与下游机器翻译评估任务性能之间的关系。", "method": "研究采用内在和外在两种评估范式。内在评估使用Costra数据集和多个语义文本相似度(STS)基准，评估嵌入模型捕获语义相似性、时间方面和文体变异等语言现象的能力。外在评估通过使用基于COMET的指标对每个嵌入模型进行微调，用于机器翻译评估。", "result": "实验结果揭示了一个有趣的脱节：在内在语义相似性测试中表现出色的模型，在下游翻译评估任务中未能始终产生更优的性能。相反，具有看似过度平滑嵌入空间的模型，通过微调可以取得优异的结果。", "conclusion": "研究结果强调了语义属性探测与下游任务之间复杂的关联，并指出需要对语句嵌入中的“可操作语义”或更深入的下游任务数据集（此处为翻译评估）进行更多研究。", "translation": "本文通过内在和外在评估范式，比较了特定于捷克语和多语言的语句嵌入模型。在内在评估方面，我们采用了Costra（一个复杂的句子转换数据集）和多个语义文本相似度（STS）基准，以评估嵌入模型捕获语义相似性、时间方面和文体变异等语言现象的能力。在外在评估中，我们使用基于COMET的指标对每个嵌入模型进行微调，用于机器翻译评估。\n我们的实验揭示了一个有趣的脱节：在内在语义相似性测试中表现出色的模型，在下游翻译评估任务中未能始终产生更优的性能。相反，具有看似过度平滑嵌入空间的模型，通过微调可以取得优异的结果。这些发现突出了语义属性探测与下游任务之间复杂的关联，强调了需要对语句嵌入中的“可操作语义”或更深入的下游任务数据集（此处为翻译评估）进行更多研究。", "summary": "本研究比较了捷克语和多语言语句嵌入模型在内在和外在评估中的表现。内在评估使用Costra和STS基准测试语义捕获能力，而外在评估则通过微调模型以进行机器翻译评估。研究发现，在内在语义测试中表现优异的模型，在机器翻译评估任务中不一定表现更好，甚至看似过度平滑的模型在微调后能取得优异结果。这表明语义属性与下游任务之间存在复杂关系，亟需深入研究“可操作语义”或更细致的下游任务数据集。", "keywords": "语句嵌入, 机器翻译评估, 内在评估, 外在评估, 语义相似性", "comments": "这项研究揭示了语句嵌入评估中的一个重要且反直觉的发现：内在语义表现与外在下游任务性能之间存在不一致性。这挑战了当前对语义嵌入有效性的普遍假设，强调了在实际应用中需要重新思考评估策略，特别是对“可操作语义”的探索，具有重要的理论和实践意义。"}}
{"id": "2506.20630", "title": "First-order methods for stochastic and finite-sum convex optimization with deterministic constraints", "authors": ["Zhaosong Lu", "Yifeng Xiao"], "summary": "In this paper, we study a class of stochastic and finite-sum convex\noptimization problems with deterministic constraints. Existing methods\ntypically aim to find an $\\epsilon$-$expectedly\\ feasible\\ stochastic\\ optimal$\nsolution, in which the expected constraint violation and expected optimality\ngap are both within a prescribed tolerance $\\epsilon$. However, in many\npractical applications, constraints must be nearly satisfied with certainty,\nrendering such solutions potentially unsuitable due to the risk of substantial\nviolations. To address this issue, we propose stochastic first-order methods\nfor finding an $\\epsilon$-$surely\\ feasible\\ stochastic\\ optimal$\n($\\epsilon$-SFSO) solution, where the constraint violation is deterministically\nbounded by $\\epsilon$ and the expected optimality gap is at most $\\epsilon$.\nOur methods apply an accelerated stochastic gradient (ASG) scheme or a modified\nvariance-reduced ASG scheme $only\\ once$ to a sequence of quadratic penalty\nsubproblems with appropriately chosen penalty parameters. We establish\nfirst-order oracle complexity bounds for the proposed methods in computing an\n$\\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle\ncomplexity results for sample average approximation method in computing an\n$\\epsilon$-SFSO solution of the stochastic optimization problem using our\nproposed methods to solve the sample average problem.", "comment": "41 pages", "pdf_url": "http://arxiv.org/pdf/2506.20630v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20630v1", "AI": {"title_translation": "一阶随机和有限和凸优化方法，具有确定性约束", "tldr": "本文提出了一种新的随机一阶方法，用于解决具有确定性约束的随机和有限和凸优化问题，以确保约束几乎确定性地满足，而不是仅仅期望满足。", "motivation": "现有方法通常旨在找到一个$\\\\epsilon$-期望可行随机最优解，其中期望约束违反和期望最优性差距都在给定容忍度$\\\\epsilon$内。然而，在许多实际应用中，约束必须几乎确定地满足，这使得现有解决方案由于存在严重违反的风险而可能不适用。", "method": "本文提出了一种随机一阶方法，用于寻找一个$\\\\epsilon$-确定可行随机最优 ($\\\\epsilon$-SFSO) 解。这些方法只对一系列具有适当选择的惩罚参数的二次惩罚子问题应用一次加速随机梯度 (ASG) 方案或改进的方差缩减 ASG 方案。", "result": "1. 为所提出的方法建立了计算$\\\\epsilon$-SFSO解的一阶预言机复杂度界限。\n2. 作为副产品，还推导了样本平均近似方法在计算随机优化问题的$\\\\epsilon$-SFSO解时的一阶预言机复杂度结果，通过使用本文提出的方法来解决样本平均问题。", "conclusion": "本文提出的随机一阶方法能够有效地找到具有确定性约束的随机和有限和凸优化问题的$\\\\epsilon$-确定可行随机最优解，解决了现有方法在实际应用中约束可能被严重违反的问题，并提供了理论上的复杂度保证。", "translation": "在本文中，我们研究了一类具有确定性约束的随机和有限和凸优化问题。现有方法通常旨在找到一个$\\\\epsilon$-期望可行随机最优解，其中期望约束违反和期望最优性差距都在给定容忍度$\\\\epsilon$内。然而，在许多实际应用中，约束必须几乎确定地满足，这使得此类解决方案由于存在严重违反的风险而可能不适用。为了解决这个问题，我们提出了一种随机一阶方法，用于寻找一个$\\\\epsilon$-确定可行随机最优 ($\\\\epsilon$-SFSO) 解，其中约束违反确定性地限制在$\\\\epsilon$内，并且期望最优性差距至多为$\\\\epsilon$。我们的方法只对一系列具有适当选择的惩罚参数的二次惩罚子问题应用一次加速随机梯度 (ASG) 方案或改进的方差缩减 ASG 方案。我们为所提出的方法在计算$\\\\epsilon$-SFSO解方面建立了首次一阶预言机复杂度界限。作为副产品，我们还推导了样本平均近似方法在计算随机优化问题的$\\\\epsilon$-SFSO解时的一阶预言机复杂度结果，通过使用我们提出的方法来解决样本平均问题。", "summary": "本文研究了具有确定性约束的随机和有限和凸优化问题。针对现有方法仅保证期望约束可行性，可能导致实际应用中约束严重违反的不足，作者提出了一类新的随机一阶方法。这些方法通过对二次惩罚子问题仅应用一次加速随机梯度或改进的方差缩减加速随机梯度方案，旨在找到一个$\\\\epsilon$-确定可行随机最优解，即约束违反确定性地限制在$\\\\epsilon$内，期望最优性差距至多为$\\\\epsilon$。论文为所提出的方法在计算$\\\\epsilon$-SFSO解方面建立了首次一阶预言机复杂度界限，并附带推导了样本平均近似方法的相关复杂度结果。", "keywords": "随机优化, 确定性约束, 一阶方法, 复杂度分析, 惩罚方法", "comments": "本文的创新点在于提出了解决随机优化中确定性约束问题的新方法，超越了传统上只关注期望可行性的限制。通过引入$\\\\epsilon$-确定可行随机最优解的概念，并结合惩罚方法和加速随机梯度技术，为实际应用中对约束满足有更高确定性要求的场景提供了更可靠的解决方案。其复杂度分析也为方法的效率提供了理论保障。"}}
{"id": "2506.20487", "title": "Behavior Foundation Model: Towards Next-Generation Whole-Body Control System of Humanoid Robots", "authors": ["Mingqi Yuan", "Tao Yu", "Wenqi Ge", "Xiuyong Yao", "Dapeng Li", "Huijiang Wang", "Jiayu Chen", "Xin Jin", "Bo Li", "Hua Chen", "Wei Zhang", "Wenjun Zeng"], "summary": "Humanoid robots are drawing significant attention as versatile platforms for\ncomplex motor control, human-robot interaction, and general-purpose physical\nintelligence. However, achieving efficient whole-body control (WBC) in\nhumanoids remains a fundamental challenge due to sophisticated dynamics,\nunderactuation, and diverse task requirements. While learning-based controllers\nhave shown promise for complex tasks, their reliance on labor-intensive and\ncostly retraining for new scenarios limits real-world applicability. To address\nthese limitations, behavior(al) foundation models (BFMs) have emerged as a new\nparadigm that leverages large-scale pretraining to learn reusable primitive\nskills and behavioral priors, enabling zero-shot or rapid adaptation to a wide\nrange of downstream tasks. In this paper, we present a comprehensive overview\nof BFMs for humanoid WBC, tracing their development across diverse pre-training\npipelines. Furthermore, we discuss real-world applications, current\nlimitations, urgent challenges, and future opportunities, positioning BFMs as a\nkey approach toward scalable and general-purpose humanoid intelligence.\nFinally, we provide a curated and long-term list of BFM papers and projects to\nfacilitate more subsequent research, which is available at\nhttps://github.com/yuanmingqi/awesome-bfm-papers.", "comment": "19 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.20487v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20487v1", "AI": {"title_translation": "行为基础模型：迈向下一代人形机器人全身控制系统", "tldr": "本文综述了行为基础模型（BFMs）在人形机器人全身控制（WBC）中的应用，讨论了其发展、应用、局限性、挑战和未来机遇，并认为BFMs是实现可扩展通用人形智能的关键方法。", "motivation": "人形机器人全身控制面临复杂动力学、欠驱动和多样化任务的挑战，而现有学习型控制器需要耗时且昂贵的重新训练。行为基础模型（BFMs）作为一种新范式，通过大规模预训练学习可重用技能和行为先验，有望解决这些限制。", "method": "本文对人形机器人全身控制的BFMs进行了全面的概述，追溯了其在不同预训练管道中的发展。此外，还讨论了实际应用、当前局限性、紧迫挑战和未来机遇。最后，提供了精选的BFM论文和项目列表以促进后续研究。", "result": "行为基础模型（BFMs）被定位为实现可扩展和通用人形智能的关键方法。论文提供了一个精选的BFM论文和项目列表。", "conclusion": "BFMs是实现人形机器人可扩展和通用智能的关键方法，为解决全身控制的挑战提供了有前景的途径。", "translation": "人形机器人作为复杂运动控制、人机交互和通用物理智能的多功能平台，正受到广泛关注。然而，由于复杂的动力学、欠驱动和多样化的任务要求，在人形机器人中实现高效的全身控制（WBC）仍然是一个基本挑战。虽然基于学习的控制器在复杂任务中表现出前景，但它们对新场景中劳动密集型和昂贵的再训练的依赖限制了实际应用。为了解决这些限制，行为基础模型（BFMs）作为一种新范式应运而生，它利用大规模预训练来学习可重用的原始技能和行为先验，从而实现对各种下游任务的零样本或快速适应。在本文中，我们全面概述了用于人形机器人全身控制的BFMs，追溯了它们在不同预训练管道中的发展。此外，我们讨论了实际应用、当前局限性、紧迫挑战和未来机遇，将BFMs定位为实现可扩展和通用人形智能的关键方法。最后，我们提供了一份精选的、长期的BFM论文和项目列表，以促进后续研究，该列表可在https://github.com/yuanmingqi/awesome-bfm-papers获取。", "summary": "本文概述了行为基础模型（BFMs）在人形机器人全身控制（WBC）中的应用。针对现有方法在复杂动力学和任务适应性上的局限，BFMs通过大规模预训练学习通用技能，实现零样本或快速适应。论文探讨了BFMs的发展、实际应用、挑战与机遇，并将其视为实现下一代可扩展通用人形智能的关键。此外，还提供了一个BFM相关论文和项目的资源列表。", "keywords": "行为基础模型, 人形机器人, 全身控制, 预训练, 综述", "comments": "本文作为一篇综述性文章，系统地介绍了行为基础模型在人形机器人全身控制领域的应用前景和发展。其创新之处在于提出并推广了“行为基础模型”这一新范式，并将其与人形机器人的全身控制挑战相结合。重要性体现在为该领域的研究者提供了全面的视角、指明了未来的研究方向，并提供了宝贵的资源列表。局限性在于作为一篇综述，它本身不提出新的技术或实验结果，而是对现有和未来方向的总结。"}}
{"id": "2506.20306", "title": "Radiomic fingerprints for knee MR images assessment", "authors": ["Yaxi Chen", "Simin Ni", "Shaheer U. Saeed", "Aleksandra Ivanova", "Rikin Hargunani", "Jie Huang", "Chaozong Liu", "Yipeng Hu"], "summary": "Accurate interpretation of knee MRI scans relies on expert clinical judgment,\noften with high variability and limited scalability. Existing radiomic\napproaches use a fixed set of radiomic features (the signature), selected at\nthe population level and applied uniformly to all patients. While\ninterpretable, these signatures are often too constrained to represent\nindividual pathological variations. As a result, conventional radiomic-based\napproaches are found to be limited in performance, compared with recent\nend-to-end deep learning (DL) alternatives without using interpretable radiomic\nfeatures. We argue that the individual-agnostic nature in current radiomic\nselection is not central to its intepretability, but is responsible for the\npoor generalization in our application. Here, we propose a novel radiomic\nfingerprint framework, in which a radiomic feature set (the fingerprint) is\ndynamically constructed for each patient, selected by a DL model. Unlike the\nexisting radiomic signatures, our fingerprints are derived on a per-patient\nbasis by predicting the feature relevance in a large radiomic feature pool, and\nselecting only those that are predictive of clinical conditions for individual\npatients. The radiomic-selecting model is trained simultaneously with a\nlow-dimensional (considered relatively explainable) logistic regression for\ndownstream classification. We validate our methods across multiple diagnostic\ntasks including general knee abnormalities, anterior cruciate ligament (ACL)\ntears, and meniscus tears, demonstrating comparable or superior diagnostic\naccuracy relative to state-of-the-art end-to-end DL models. More importantly,\nwe show that the interpretability inherent in our approach facilitates\nmeaningful clinical insights and potential biomarker discovery, with detailed\ndiscussion, quantitative and qualitative analysis of real-world clinical cases\nto evidence these advantages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20306v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20306v1", "AI": {"title_translation": "膝关节MR图像评估的影像组学指纹", "tldr": "该研究提出了一种新的影像组学指纹框架，为每个患者动态构建影像组学特征集，以解决传统影像组学方法的局限性，并实现了与最先进深度学习模型相当或更优的诊断准确性，同时保持了可解释性。", "motivation": "膝关节MRI扫描的准确判读依赖专家临床判断，但存在高变异性和可扩展性有限的问题。现有影像组学方法使用固定的特征集，难以代表个体病理变异，导致性能受限。与端到端深度学习模型相比，传统影像组学方法在性能上存在不足，且其个体无关的特征选择是导致泛化能力差的原因。", "method": "提出了一种新颖的影像组学指纹框架。该框架为每个患者动态构建影像组学特征集（“指纹”），由深度学习模型选择。与现有影像组学签名不同，指纹是基于预测大型影像组学特征池中特征的相关性，并仅选择对个体患者临床状况具有预测性的特征。影像组学选择模型与低维（相对可解释）逻辑回归模型同时训练，用于下游分类。", "result": "该方法在多项诊断任务（包括一般膝关节异常、前交叉韧带撕裂和半月板撕裂）中进行了验证，结果显示诊断准确性与最先进的端到端深度学习模型相当或更优。更重要的是，该方法固有的可解释性有助于提供有意义的临床见解和潜在的生物标志物发现。", "conclusion": "本研究提出的影像组学指纹框架通过为每个患者动态构建特征集，克服了传统影像组学方法的局限性，实现了高性能和高可解释性，为膝关节MRI评估提供了新的有效工具，并促进了临床洞察和生物标志物发现。", "translation": "膝关节MRI扫描的准确判读依赖于专家临床判断，但往往存在高度变异性和有限的可扩展性。现有的影像组学方法使用固定的影像组学特征集（即“签名”），在群体层面选择并统一应用于所有患者。尽管可解释，但这些签名通常过于受限，无法代表个体病理变异。因此，与近期不使用可解释影像组学特征的端到端深度学习（DL）替代方案相比，传统的基于影像组学的方法在性能上被发现存在局限性。我们认为，当前影像组学选择中的个体无关性并非其可解释性的核心，而是导致我们应用中泛化能力差的原因。在此，我们提出了一种新颖的影像组学指纹框架，其中为每位患者动态构建一个影像组学特征集（即“指纹”），并由一个DL模型选择。与现有的影像组学签名不同，我们的指纹是在个体患者基础上，通过预测大型影像组学特征池中的特征相关性，并仅选择那些对个体患者临床状况具有预测性的特征来推导的。影像组学选择模型与一个低维（被认为是相对可解释的）逻辑回归模型同时训练，用于下游分类。我们在多项诊断任务中验证了我们的方法，包括一般膝关节异常、前交叉韧带（ACL）撕裂和半月板撕裂，结果显示诊断准确性与最先进的端到端DL模型相当或更优。更重要的是，我们展示了我们方法固有的可解释性有助于提供有意义的临床见解和潜在的生物标志物发现，并通过对真实世界临床病例的详细讨论、定量和定性分析来证明这些优势。", "summary": "该研究提出了一种创新的影像组学指纹框架，旨在解决传统影像组学在膝关节MRI评估中因固定特征集导致的性能和泛化能力不足问题。与现有方法不同，该框架利用深度学习为每位患者动态选择和构建个性化的影像组学特征集（“指纹”），以更好地捕捉个体病理变异。通过将特征选择模型与可解释的逻辑回归模型联合训练，该方法在多种膝关节诊断任务中实现了与最先进的端到端深度学习模型相当或更优的诊断准确性。此外，该方法显著提升了临床洞察的可解释性，并有助于生物标志物的发现，从而弥补了传统影像组学和纯深度学习模型之间的差距。", "keywords": "影像组学指纹, 膝关节MRI, 深度学习, 个性化特征选择, 可解释性", "comments": "这项研究的创新之处在于提出了“影像组学指纹”的概念，即为每个患者动态构建个性化的影像组学特征集，而非使用固定的群体级特征签名。这解决了传统影像组学方法在捕捉个体病理变异方面的局限性。其重要性在于，它成功地将深度学习的强大特征选择能力与影像组学的可解释性结合起来，提供了高性能的诊断工具，同时保持了临床医生所需的透明度，这对于临床决策和生物标志物发现至关重要。该方法有效地弥合了端到端深度学习模型的高性能与传统影像组学模型高可解释性之间的差距。"}}
{"id": "2506.20640", "title": "Towards Community-Driven Agents for Machine Learning Engineering", "authors": ["Sijie Li", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "summary": "Large language model-based machine learning (ML) agents have shown great\npromise in automating ML research. However, existing agents typically operate\nin isolation on a given research problem, without engaging with the broader\nresearch community, where human researchers often gain insights and contribute\nby sharing knowledge. To bridge this gap, we introduce MLE-Live, a live\nevaluation framework designed to assess an agent's ability to communicate with\nand leverage collective knowledge from a simulated Kaggle research community.\nBuilding on this framework, we propose CoMind, a novel agent that excels at\nexchanging insights and developing novel solutions within a community context.\nCoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%\nhuman competitors on average across four ongoing Kaggle competitions. Our code\nis released at https://github.com/comind-ml/CoMind.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20640v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20640v1", "AI": {"title_translation": "迈向社区驱动的机器学习工程代理", "tldr": "本文提出了CoMind，一个能与模拟Kaggle社区互动的机器学习代理，利用集体知识，在MLE-Live框架上达到最先进性能，并在Kaggle竞赛中超越了大部分人类竞争者。", "motivation": "现有的机器学习代理在解决问题时是孤立的，缺乏与更广泛研究社区的互动，导致无法利用集体知识和洞察力，而人类研究人员通常通过分享知识获得见解并做出贡献。", "method": "引入了MLE-Live，一个实时评估框架，旨在评估代理与模拟Kaggle研究社区沟通并利用集体知识的能力。在此框架的基础上，提出了CoMind，一个在社区环境中擅长交流见解和开发新颖解决方案的新型代理。", "result": "CoMind在MLE-Live上取得了最先进的性能，并在四个正在进行的Kaggle竞赛中平均超越了79.2%的人类竞争者。", "conclusion": "CoMind通过社区驱动的方法显著提升了机器学习代理的能力，证明了代理通过与社区互动可以超越孤立操作的局限，并在实际竞赛中取得优异表现。", "translation": "基于大型语言模型的机器学习（ML）代理在自动化ML研究方面展现了巨大潜力。然而，现有代理通常在给定研究问题上孤立运行，不与更广泛的研究社区互动，而人类研究人员常常通过分享知识获得见解并做出贡献。为了弥补这一差距，我们引入了MLE-Live，一个实时评估框架，旨在评估代理与模拟Kaggle研究社区沟通并利用集体知识的能力。在此框架的基础上，我们提出了CoMind，一个在社区环境中擅长交流见解和开发新颖解决方案的新型代理。CoMind在MLE-Live上取得了最先进的性能，并在四个正在进行的Kaggle竞赛中平均超越了79.2%的人类竞争者。我们的代码已在https://github.com/comind-ml/CoMind 发布。", "summary": "本文针对现有机器学习代理缺乏社区互动的问题，提出了MLE-Live评估框架和CoMind代理。MLE-Live用于模拟Kaggle社区环境，评估代理的社区协作能力。CoMind作为一种新型代理，能够有效利用社区知识并交换见解，在MLE-Live上达到最先进水平，并在实际Kaggle竞赛中表现优异，超越了大部分人类竞争者，证明了社区驱动型代理的有效性。", "keywords": "机器学习代理, 社区驱动, 集体知识, Kaggle, 大语言模型", "comments": "这项工作通过引入社区交互的概念，为机器学习代理的设计开辟了新方向。MLE-Live框架提供了一个评估此类代理的现实环境，而CoMind代理的卓越表现则证明了集体智慧在自动化机器学习研究中的巨大潜力。其创新点在于将社交互动引入AI代理，解决了传统代理的“孤立”问题，具有重要的实践意义。"}}
{"id": "2506.20209", "title": "Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems", "authors": ["Benedetta Muscato", "Lucia Passaro", "Gizem Gezici", "Fosca Giannotti"], "summary": "In the realm of Natural Language Processing (NLP), common approaches for\nhandling human disagreement consist of aggregating annotators' viewpoints to\nestablish a single ground truth. However, prior studies show that disregarding\nindividual opinions can lead can lead to the side effect of underrepresenting\nminority perspectives, especially in subjective tasks, where annotators may\nsystematically disagree because of their preferences. Recognizing that labels\nreflect the diverse backgrounds, life experiences, and values of individuals,\nthis study proposes a new multi-perspective approach using soft labels to\nencourage the development of the next generation of perspective aware models,\nmore inclusive and pluralistic. We conduct an extensive analysis across diverse\nsubjective text classification tasks, including hate speech, irony, abusive\nlanguage, and stance detection, to highlight the importance of capturing human\ndisagreements, often overlooked by traditional aggregation methods. Results\nshow that the multi-perspective approach not only better approximates human\nlabel distributions, as measured by Jensen-Shannon Divergence (JSD), but also\nachieves superior classification performance (higher F1 scores), outperforming\ntraditional approaches. However, our approach exhibits lower confidence in\ntasks like irony and stance detection, likely due to the inherent subjectivity\npresent in the texts. Lastly, leveraging Explainable AI (XAI), we explore model\nuncertainty and uncover meaningful insights into model predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20209v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20209v1", "AI": {"title_translation": "视角在发挥作用：一种更具包容性的自然语言处理系统的多视角方法", "tldr": "传统NLP系统忽略标注者分歧导致少数视角被低估。本文提出多视角方法，通过软标签更好捕获人类分歧，在主观任务中表现更优，并提高系统包容性。", "motivation": "在自然语言处理（NLP）领域，传统方法通过聚合标注者观点来建立单一的“真值”，但这会导致少数视角被低估，尤其是在主观任务中，因为标注者的偏好可能导致系统性分歧。", "method": "本研究提出了一种新的多视角方法，利用软标签来鼓励开发能够感知视角的模型。该方法在仇恨言论、讽刺、辱骂性语言和立场检测等多种主观文本分类任务中进行了广泛分析，并利用可解释人工智能（XAI）探索模型不确定性。", "result": "该多视角方法不仅能更好地近似人类标签分布（通过詹森-香农散度JSD衡量），而且实现了卓越的分类性能（更高的F1分数），优于传统方法。然而，在讽刺和立场检测等任务中，该方法表现出较低的置信度。", "conclusion": "捕捉人类分歧对于构建更具包容性和多元化的自然语言处理系统至关重要。本文提出的多视角方法能够有效处理主观任务中的标注者分歧，并提升模型性能和对人类标签分布的近似度。", "translation": "在自然语言处理（NLP）领域，处理人类分歧的常见方法是聚合标注者的观点以建立单一的“真值”。然而，先前的研究表明，忽略个体意见可能导致低估少数视角的副作用，尤其是在主观任务中，标注者可能因其偏好而系统性地存在分歧。认识到标签反映了个体的不同背景、生活经验和价值观，本研究提出了一种新的多视角方法，使用软标签来鼓励开发下一代视角感知模型，使其更具包容性和多元化。我们在包括仇恨言论、讽刺、辱骂性语言和立场检测在内的各种主观文本分类任务中进行了广泛分析，以强调捕捉人类分歧的重要性，而这常常被传统聚合方法所忽视。结果表明，多视角方法不仅能更好地近似人类标签分布（通过詹森-香农散度JSD衡量），而且实现了卓越的分类性能（更高的F1分数），优于传统方法。然而，我们的方法在讽刺和立场检测等任务中表现出较低的置信度，这可能是由于文本中固有的主观性所致。最后，利用可解释人工智能（XAI），我们探索了模型的不确定性，并揭示了模型预测中有意义的见解。", "summary": "本文针对NLP中传统方法因聚合标注者观点而忽略少数视角的问题，提出了一种创新的多视角方法。该方法利用软标签来捕获和利用人类在主观任务中的分歧，旨在构建更具包容性的视角感知模型。通过在多项主观文本分类任务上的广泛实验，研究表明该方法不仅能更好地近似人类标签分布，还在分类性能上超越了传统方法。尽管在某些高度主观任务上模型置信度较低，但结合可解释AI的使用，该研究为处理主观性数据和提升NLP系统包容性提供了重要方向。", "keywords": "多视角方法, 软标签, 主观任务, 人类分歧, 包容性NLP", "comments": "这篇论文通过引入“多视角方法”和“软标签”来解决NLP领域中长期存在的标注者分歧和少数视角被忽视的问题，具有重要的创新性。它挑战了传统“单一真值”的范式，强调了人类主观性和多样性的价值。通过在多个主观任务上的实证分析，证明了其方法在性能和对人类分布近似度上的优越性。结合XAI来分析模型不确定性也增加了研究的深度。这对于构建更公平、更具代表性的NLP系统具有深远影响。"}}
{"id": "2506.20496", "title": "Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN): Application to Laminectomy Surgical Education", "authors": ["Jonathan Wang", "Hisashi Ishida", "David Usevitch", "Kesavan Venkatesh", "Yi Wang", "Mehran Armand", "Rachel Bronheim", "Amit Jain", "Adnan Munawar"], "summary": "Surgical training remains a crucial milestone in modern medicine, with\nprocedures such as laminectomy exemplifying the high risks involved.\nLaminectomy drilling requires precise manual control to mill bony tissue while\npreserving spinal segment integrity and avoiding breaches in the dura: the\nprotective membrane surrounding the spinal cord. Despite unintended tears\noccurring in up to 11.3% of cases, no assistive tools are currently utilized to\nreduce this risk. Variability in patient anatomy further complicates learning\nfor novice surgeons. This study introduces CAPTAiN, a critical\nanatomy-preserving and terrain-augmenting navigation system that provides\nlayered, color-coded voxel guidance to enhance anatomical awareness during\nspinal drilling. CAPTAiN was evaluated against a standard non-navigated\napproach through 110 virtual laminectomies performed by 11 orthopedic residents\nand medical students. CAPTAiN significantly improved surgical completion rates\nof target anatomy (87.99% vs. 74.42%) and reduced cognitive load across\nmultiple NASA-TLX domains. It also minimized performance gaps across experience\nlevels, enabling novices to perform on par with advanced trainees. These\nfindings highlight CAPTAiN's potential to optimize surgical execution and\nsupport skill development across experience levels. Beyond laminectomy, it\ndemonstrates potential for broader applications across various surgical and\ndrilling procedures, including those in neurosurgery, otolaryngology, and other\nmedical fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20496v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20496v1", "AI": {"title_translation": "关键解剖结构保留与地形增强导航系统（CAPTAiN）：在椎板切除术外科教育中的应用", "tldr": "CAPTAiN是一种外科导航系统，通过分层、彩色编码的体素指导，显著提高了椎板切除术的完成率，降低了认知负荷，并缩小了不同经验水平操作者的性能差距，对外科培训和操作具有广阔应用前景。", "motivation": "椎板切除术等外科手术存在高风险，需要精确控制以避免损伤脊髓保护膜（硬脑膜），但目前缺乏辅助工具来降低高达11.3%的意外撕裂风险。患者解剖结构的可变性进一步增加了新手外科医生的学习难度。", "method": "本研究引入了CAPTAiN系统，一个关键解剖结构保留与地形增强导航系统，它提供分层、彩色编码的体素指导，以增强脊柱钻孔过程中的解剖意识。通过11名骨科住院医师和医学生进行的110次虚拟椎板切除术，将CAPTAiN与标准非导航方法进行了对比评估。", "result": "CAPTAiN显著提高了目标解剖结构的 Surgical completion rates（87.99% vs. 74.42%），并在多个NASA-TLX领域降低了认知负荷。它还最大程度地缩小了不同经验水平之间的性能差距，使新手能够达到与高级受训者相同的水平。", "conclusion": "这些发现表明CAPTAiN有潜力优化外科手术执行并支持不同经验水平的技能发展。除了椎板切除术，它还在神经外科、耳鼻喉科和其他医疗领域的各种外科和钻孔手术中展现出更广泛的应用潜力。", "translation": "外科培训仍然是现代医学中的一个关键里程碑，如椎板切除术等手术就体现了其中涉及的高风险。椎板切除术钻孔需要精确的手动控制，以在磨除骨组织的同时，保持脊柱节段的完整性并避免穿破硬脑膜：脊髓周围的保护膜。尽管高达11.3%的病例中发生意外撕裂，但目前尚无辅助工具用于降低这种风险。患者解剖结构的可变性进一步增加了新手外科医生的学习难度。本研究引入了CAPTAiN，一个关键解剖结构保留与地形增强导航系统，它提供分层、彩色编码的体素指导，以增强脊柱钻孔过程中的解剖意识。CAPTAiN通过11名骨科住院医师和医学生进行的110次虚拟椎板切除术，与标准非导航方法进行了对比评估。CAPTAiN显著提高了目标解剖结构的 Surgical completion rates（87.99% vs. 74.42%），并在多个NASA-TLX领域降低了认知负荷。它还最大程度地缩小了不同经验水平之间的性能差距，使新手能够达到与高级受训者相同的水平。这些发现表明CAPTAiN有潜力优化外科手术执行并支持不同经验水平的技能发展。除了椎板切除术，它还在神经外科、耳鼻喉科和其他医疗领域的各种外科和钻孔手术中展现出更广泛的应用潜力。", "summary": "本研究介绍了一种名为CAPTAiN的导航系统，旨在解决椎板切除术中存在的风险和培训挑战。该系统通过提供分层、彩色编码的体素指导，增强了手术过程中的解剖意识。在对11名住院医师和医学生进行的110次虚拟手术评估中，CAPTAiN显著提高了手术完成率，降低了操作者的认知负荷，并有效缩小了不同经验水平之间的性能差距。研究结果强调了CAPTAiN在优化手术执行、促进技能发展方面的巨大潜力，并指出其在神经外科、耳鼻喉科等其他需要精确钻孔的外科领域也具有广泛应用前景。", "keywords": "椎板切除术, 外科导航, 医疗教育, 解剖结构保留, 认知负荷", "comments": "CAPTAiN系统通过其创新的分层、彩色编码体素指导，为外科手术（特别是椎板切除术）提供了一种有效的辅助工具，显著提高了手术安全性并优化了学习曲线。其能够降低认知负荷并缩小经验差距的特点，对于加速新手外科医生的培训和提高整体手术质量具有重要意义。该系统在虚拟环境中的积极表现，预示了其在临床实践中减少并发症的巨大潜力。"}}
{"id": "2506.20312", "title": "On the Burstiness of Faces in Set", "authors": ["Jiong Wang"], "summary": "Burstiness, a phenomenon observed in text and image retrieval, refers to that\nparticular elements appear more times in a set than a statistically independent\nmodel assumes. We argue that in the context of set-based face recognition\n(SFR), burstiness exists widely and degrades the performance in two aspects:\nFirstly, the bursty faces, where faces with particular attributes %exist\nfrequently in a face set, dominate the training instances and dominate the\ntraining face sets and lead to poor generalization ability to unconstrained\nscenarios. Secondly, the bursty faces %dominating the evaluation sets interfere\nwith the similarity comparison in set verification and identification when\nevaluation. To detect the bursty faces in a set, we propose three strategies\nbased on Quickshift++, feature self-similarity, and generalized max-pooling\n(GMP). We apply the burst detection results on training and evaluation stages\nto enhance the sampling ratios or contributions of the infrequent faces. When\nevaluation, we additionally propose the quality-aware GMP that enables\nawareness of the face quality and robustness to the low-quality faces for the\noriginal GMP. We give illustrations and extensive experiments on the SFR\nbenchmarks to demonstrate that burstiness is widespread and suppressing\nburstiness considerably improves the recognition performance.", "comment": "18 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20312v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20312v1", "AI": {"title_translation": "人脸集合中的突发性问题研究", "tldr": "本文研究了集合人脸识别中人脸突发性现象及其对性能的影响，并提出了检测和抑制突发性的方法，显著提升了识别性能。", "motivation": "在集合人脸识别（SFR）中，人脸的“突发性”现象广泛存在，并从两个方面降低了识别性能：一是具有特定属性的人脸频繁出现，导致训练实例和训练集被主导，泛化能力差；二是突发性人脸在评估集中干扰相似性比较。因此，需要检测并抑制这种突发性。", "method": "为检测集合中的突发性人脸，本文提出了三种策略，分别基于Quickshift++、特征自相似性和广义最大池化（GMP）。在训练和评估阶段，利用突发性检测结果来提高不常见人脸的采样比例或贡献。在评估时，额外提出了质量感知GMP，使其能感知人脸质量并对低质量人脸具有鲁棒性。", "result": "通过在SFR基准数据集上进行广泛实验，结果表明突发性普遍存在，并且抑制突发性显著提高了识别性能。", "conclusion": "突发性是集合人脸识别中一个普遍存在的问题，它会降低识别性能。通过本文提出的检测和抑制策略，可以有效地缓解突发性带来的负面影响，从而显著提升人脸识别的准确性。", "translation": "突发性，一种在文本和图像检索中观察到的现象，指的是在集合中特定元素出现的次数多于统计独立模型假设的次数。我们认为，在基于集合的人脸识别（SFR）背景下，突发性广泛存在并从两个方面降低了性能：首先，突发性人脸（即具有特定属性的人脸在人脸集合中频繁出现）主导了训练实例并主导了训练人脸集合，导致对非受限场景的泛化能力差。其次，突发性人脸主导评估集，在评估时干扰集合验证和识别中的相似性比较。为了检测集合中的突发性人脸，我们提出了三种基于Quickshift++、特征自相似性和广义最大池化（GMP）的策略。我们将突发性检测结果应用于训练和评估阶段，以增强不常见人脸的采样比率或贡献。在评估时，我们额外提出了质量感知GMP，使原始GMP能够感知人脸质量并对低质量人脸具有鲁棒性。我们在SFR基准上进行了插图和广泛实验，以证明突发性是普遍存在的，并且抑制突发性显著提高了识别性能。", "summary": "本文探讨了集合人脸识别（SFR）中的“突发性”现象，即特定人脸在集合中过度出现，从而损害了识别性能。研究指出，突发性会导致训练泛化能力差，并在评估时干扰相似性比较。为解决此问题，论文提出了基于Quickshift++、特征自相似性和广义最大池化（GMP）的三种突发性检测策略，并引入了质量感知GMP以提高鲁棒性。实验证明，抑制突发性可显著提升SFR性能。", "keywords": "人脸识别, 集合人脸识别, 突发性, 广义最大池化, 质量感知", "comments": "这篇论文的创新点在于首次系统性地提出了人脸集合中“突发性”的概念及其对识别性能的负面影响。它不仅揭示了一个普遍存在但此前未被充分关注的问题，还提供了具体且有效的解决方案，包括多种检测策略和质量感知机制。这项工作对于提升集合人脸识别在复杂非受限场景下的鲁棒性和泛化能力具有重要意义。"}}
{"id": "2506.20046", "title": "GNN's Uncertainty Quantification using Self-Distillation", "authors": ["Hirad Daneshvar", "Reza Samavi"], "summary": "Graph Neural Networks (GNNs) have shown remarkable performance in the\nhealthcare domain. However, what remained challenging is quantifying the\npredictive uncertainty of GNNs, which is an important aspect of trustworthiness\nin clinical settings. While Bayesian and ensemble methods can be used to\nquantify uncertainty, they are computationally expensive. Additionally, the\ndisagreement metric used by ensemble methods to compute uncertainty cannot\ncapture the diversity of models in an ensemble network. In this paper, we\npropose a novel method, based on knowledge distillation, to quantify GNNs'\nuncertainty more efficiently and with higher precision. We apply\nself-distillation, where the same network serves as both the teacher and\nstudent models, thereby avoiding the need to train several networks\nindependently. To ensure the impact of self-distillation, we develop an\nuncertainty metric that captures the diverse nature of the network by assigning\ndifferent weights to each GNN classifier. We experimentally evaluate the\nprecision, performance, and ability of our approach in distinguishing\nout-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The\nevaluation results demonstrate that the proposed method can effectively capture\nthe predictive uncertainty of the model while having performance similar to\nthat of the MC Dropout and ensemble methods. The code is publicly available at\nhttps://github.com/tailabTMU/UQ_GNN.", "comment": "The paper has been accepted in the International Conference on AI in\n  Healthcare (AIiH) 2025 and will appear in the conference proceedings", "pdf_url": "http://arxiv.org/pdf/2506.20046v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20046v1", "AI": {"title_translation": "使用自蒸馏量化图神经网络的不确定性", "tldr": "本文提出了一种基于自蒸馏的新方法，用于高效且高精度地量化图神经网络（GNNs）的预测不确定性，解决了现有贝叶斯和集成方法计算成本高昂以及集成方法无法捕捉模型多样性的问题。", "motivation": "图神经网络（GNNs）在医疗保健领域表现出色，但量化其预测不确定性仍然具有挑战性，这在临床环境中对可信度至关重要。现有的贝叶斯和集成方法计算成本高昂，且集成方法使用的不一致性度量无法捕捉集成网络中模型的多样性。", "method": "本文提出了一种基于知识蒸馏的新方法来量化GNNs的不确定性。该方法采用自蒸馏，即同一个网络同时作为教师和学生模型，从而避免了独立训练多个网络的需要。为了确保自蒸馏的效果，开发了一种新的不确定性度量，通过为每个GNN分类器分配不同的权重来捕捉网络的多元性质。", "result": "实验结果表明，所提出的方法能够有效捕捉模型的预测不确定性，同时性能与MC Dropout和集成方法相似。该方法在精度、性能以及区分域外数据的能力方面均表现良好。", "conclusion": "本文提出的基于自蒸馏的GNN不确定性量化方法，在保持与现有复杂方法相似性能的同时，显著提高了效率和精度，并能有效捕捉模型的预测不确定性。", "translation": "图神经网络（GNNs）在医疗保健领域展现出卓越的性能。然而，量化GNNs的预测不确定性仍然是一个挑战，而这在临床环境中是可信赖性的一个重要方面。虽然贝叶斯方法和集成方法可以用于量化不确定性，但它们的计算成本很高。此外，集成方法用于计算不确定性的不一致性度量无法捕捉集成网络中模型的 다양性。在本文中，我们提出了一种基于知识蒸馏的新方法，以更高效、更高精度地量化GNNs的不确定性。我们应用自蒸馏，其中同一个网络同时充当教师和学生模型，从而避免了独立训练多个网络的需要。为了确保自蒸馏的影响，我们开发了一种不确定性度量，通过为每个GNN分类器分配不同的权重来捕捉网络的多元性质。我们在两个图数据集：MIMIC-IV和Enzymes上，实验评估了我们方法在区分域外数据方面的精度、性能和能力。评估结果表明，所提出的方法能够有效捕捉模型的预测不确定性，同时性能与MC Dropout和集成方法相似。代码已在https://github.com/tailabTMU/UQ_GNN公开。", "summary": "本文提出了一种基于自蒸馏的图神经网络（GNNs）不确定性量化方法，旨在解决现有贝叶斯和集成方法计算成本高昂以及无法有效捕捉模型多样性的问题。该方法通过让同一网络同时充当教师和学生模型，并引入一种新的加权不确定性度量，实现了高效且高精度的不确定性量化。实验证明，该方法在预测不确定性捕捉方面与MC Dropout和集成方法性能相当。", "keywords": "GNN, 不确定性量化, 自蒸馏, 知识蒸馏, 可信赖AI", "comments": "本文的创新点在于提出了将自蒸馏应用于GNN的不确定性量化，这显著降低了计算成本，并提出了一个能够捕捉网络多样性的新不确定性度量。这对于医疗保健等对模型可信度要求高的领域具有重要意义。该方法提供了一种更实用的不确定性量化途径。"}}
{"id": "2506.20576", "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS", "authors": ["Sabrine Ennaji", "Elhadj Benkhelifa", "Luigi V. Mancini"], "summary": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20576v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20576v1", "AI": {"title_translation": "通过对NIDS的自适应黑盒对抗性攻击进行漏洞披露", "tldr": "本文提出了一种自适应黑盒对抗性攻击方法，用于网络入侵检测系统（NIDS），旨在通过最小化交互来规避检测，从而弥补现有方法的不足。", "motivation": "现有针对网络流量等结构化数据的对抗性攻击方法在理论与实践之间存在关键差距，难以处理相互依赖的特征，缺乏可重现性，且当前防御措施无法应对不断演进的攻击。", "method": "提出了一种新颖的黑盒对抗性攻击方法，该方法严格遵循黑盒约束，减少交互以避免检测，并采用基于变点检测和因果分析的自适应特征选择策略来识别并针对敏感特征进行扰动。该设计轻量化。", "result": "实验证明，该攻击在最小交互下能有效规避检测，增强了其在真实世界场景中的适应性和适用性。", "conclusion": "这项工作加深了对网络流量中对抗性攻击的理解，为开发强大的防御措施奠定了基础。", "translation": "对抗性攻击，即精心制作微小的输入以误导智能模型，已引起越来越多的关注。然而，理论进展与实际应用之间存在一个关键差距，特别是在网络流量等结构化数据中，相互依赖的特征使有效的对抗性操作复杂化。此外，当前方法的模糊性限制了可重现性并阻碍了该领域的进展。因此，现有防御措施往往无法应对不断演进的对抗性攻击。\n本文提出了一种新颖的黑盒对抗性攻击方法，解决了这些局限性。与通常假设系统访问或依赖重复探测的先前工作不同，我们的方法严格遵守黑盒约束，减少交互以避免检测，并更好地反映真实世界场景。我们提出了一种使用变点检测和因果分析的自适应特征选择策略，以识别并针对敏感特征进行扰动。这种轻量级设计确保了低计算成本和高可部署性。我们的综合实验表明，该攻击在最小交互下规避检测的有效性，增强了其在真实世界场景中的适应性和适用性。通过加深对网络流量中对抗性攻击的理解，这项工作为开发强大的防御措施奠定了基础。", "summary": "本文提出了一种新颖、轻量级的黑盒对抗性攻击方法，用于网络入侵检测系统（NIDS），旨在克服现有方法的局限性，如对系统访问的假设和高交互需求。该方法采用基于变点检测和因果分析的自适应特征选择策略，以识别网络流量中易受攻击的敏感特征。实验证明，该方法能以最小的交互有效规避检测，展现出高度的适应性和在真实世界场景中的适用性，从而有助于加深对对抗性攻击的理解并促进鲁棒防御措施的开发。", "keywords": "对抗性攻击, 黑盒, NIDS, 网络流量, 漏洞披露", "comments": "该论文解决了在网络流量等结构化数据上进行对抗性攻击的一个关键实践差距。其创新之处在于严格遵循黑盒约束和自适应特征选择策略，这使得攻击更加真实且计算效率更高。专注于NIDS对网络安全领域具有高度相关性。"}}
{"id": "2506.20241", "title": "Enhancing Large Language Models through Structured Reasoning", "authors": ["Yubo Dong", "Hehe Fan"], "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2506.20241v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20241v1", "AI": {"title_translation": "通过结构化推理增强大型语言模型", "tldr": "本文提出了一种通过显式结构化推理来增强大型语言模型（LLM）的新方法，解决了LLM在复杂推理任务中的不足。该方法包括将非结构化数据转换为结构化格式、使用SFT进行训练，并通过GRPO（结合MAX-Flow和LCS算法）进一步提升推理能力。实验结果验证了该方法在提升推理效率和性能方面的有效性。", "motivation": "当前大型语言模型（LLM）在处理涉及逻辑演绎和系统规划的复杂推理任务时仍面临困难，这主要是因为它们依赖隐式统计关系而非结构化知识表示。为了解决这一问题，本文旨在通过引入显式结构化推理来增强LLMs。", "method": "本文提出了一种通过显式结构化推理增强LLMs的方法。首先，通过显式标注推理步骤将非结构化数据转换为结构化格式。然后，使用该结构化数据集通过监督微调（SFT）训练LLMs。此外，利用组相对策略优化（GRPO）进一步增强LLMs的结构化推理能力，其中融入了MAX-Flow和最长公共子序列（LCS）两种算法。", "result": "对DeepSeek-R1-Distill-Qwen-1.5B模型的实验结果表明，该方法实现了简洁的推理、在各种场景下的稳健性能，并提高了与优化技术的兼容性。", "conclusion": "通过将结构化推理集成到大型语言模型中，能够有效提升其在复杂推理任务中的性能和效率。", "translation": "最近的大型语言模型（LLMs）极大地推动了自然语言处理和自动化决策的发展。然而，这些模型在执行涉及逻辑演绎和系统规划的复杂推理任务时仍然遇到困难，这主要是由于它们依赖隐式统计关系而缺乏结构化知识表示。受认知科学和神经符号人工智能的启发，我们引入了一种通过显式结构化推理来增强LLMs的新方法。首先，我们通过明确标注推理步骤将非结构化数据转换为结构化格式。然后，我们利用这个结构化数据集通过监督微调（SFT）来训练LLMs。此外，我们使用组相对策略优化（GRPO）增强LLMs的结构化推理能力，其中融入了两种创新的算法——MAX-Flow和最长公共子序列（LCS）——这显著提高了推理效率并降低了计算复杂性。对DeepSeek-R1-Distill-Qwen-1.5B模型进行微调的实验结果表明，该方法实现了简洁的推理、在各种场景下的稳健性能，以及与优化技术的更好兼容性，验证了结构化推理集成在LLMs中的有效性。", "summary": "本文提出了一种通过显式结构化推理增强大型语言模型（LLMs）的新方法，旨在解决LLMs在复杂推理任务中的局限性。该方法首先将非结构化数据转换为结构化格式，并通过监督微调（SFT）训练LLMs。随后，利用组相对策略优化（GRPO）结合MAX-Flow和最长公共子序列（LCS）算法进一步提升LLMs的结构化推理能力。实验结果验证了该方法在提高推理效率、稳健性和优化兼容性方面的有效性。", "keywords": "大型语言模型, 结构化推理, 监督微调, 组相对策略优化, MAX-Flow, LCS", "comments": "本文的创新点在于将认知科学和神经符号AI的理念引入LLM，通过显式结构化推理来弥补LLM在复杂逻辑推理方面的不足。它不仅提出了将数据结构化的方法，还结合了SFT和GRPO，并通过引入MAX-Flow和LCS等具体算法来优化推理过程，这为提升LLM的推理能力提供了一个清晰且可验证的路径。其重要性在于为LLM处理更高级别的认知任务提供了新的思路，有望在需要精确逻辑和规划的领域发挥更大作用。"}}
{"id": "2506.20553", "title": "Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation", "authors": ["Rachel Luo", "Heng Yang", "Michael Watson", "Apoorva Sharma", "Sushant Veer", "Edward Schmerling", "Marco Pavone"], "summary": "Learning-based robotic systems demand rigorous validation to assure reliable\nperformance, but extensive real-world testing is often prohibitively expensive,\nand if conducted may still yield insufficient data for high-confidence\nguarantees. In this work, we introduce a general estimation framework that\nleverages paired data across test platforms, e.g., paired simulation and\nreal-world observations, to achieve better estimates of real-world metrics via\nthe method of control variates. By incorporating cheap and abundant auxiliary\nmeasurements (for example, simulator outputs) as control variates for costly\nreal-world samples, our method provably reduces the variance of Monte Carlo\nestimates and thus requires significantly fewer real-world samples to attain a\nspecified confidence bound on the mean performance. We provide theoretical\nanalysis characterizing the variance and sample-efficiency improvement, and\ndemonstrate empirically in autonomous driving and quadruped robotics settings\nthat our approach achieves high-probability bounds with markedly improved\nsample efficiency. Our technique can lower the real-world testing burden for\nvalidating the performance of the stack, thereby enabling more efficient and\ncost-effective experimental evaluation of robotic systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20553v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20553v1", "AI": {"title_translation": "利用跨测试平台的关联性进行方差降低的度量估计", "tldr": "通过利用模拟和真实世界数据的关联性，使用控制变量法减少真实世界度量估计的方差，从而显著降低机器人系统验证所需的真实世界测试样本量。", "motivation": "学习型机器人系统需要严格验证以确保可靠性能，但广泛的真实世界测试成本高昂且数据可能不足以提供高置信度保证。", "method": "引入一个通用的估计框架，利用跨测试平台（例如，模拟和真实世界观察）的配对数据，通过控制变量法来获得更好的真实世界度量估计。将廉价且丰富的辅助测量（如模拟器输出）作为昂贵真实世界样本的控制变量。", "result": "证明性地减少了蒙特卡洛估计的方差，因此需要显著更少的真实世界样本来达到指定置信区间。提供了表征方差和样本效率改进的理论分析，并在自动驾驶和四足机器人设置中经验性地证明了该方法以显著提高的样本效率实现高概率界限。", "conclusion": "该技术可以降低验证堆栈性能的真实世界测试负担，从而实现对机器人系统更高效和经济的实验评估。", "translation": "学习型机器人系统需要严格的验证来确保可靠的性能，但广泛的真实世界测试往往成本过高，即使进行也可能无法获得足够的数据来提供高置信度的保证。在这项工作中，我们引入了一个通用的估计框架，该框架利用跨测试平台（例如，配对的模拟和真实世界观测）的配对数据，通过控制变量法实现对真实世界度量的更好估计。通过将廉价且丰富的辅助测量（例如，模拟器输出）作为昂贵真实世界样本的控制变量，我们的方法可证明地减少了蒙特卡洛估计的方差，从而需要显著更少的真实世界样本来达到指定平均性能的置信区间。我们提供了表征方差和样本效率改进的理论分析，并在自动驾驶和四足机器人设置中经验性地证明了我们的方法以显著提高的样本效率实现了高概率界限。我们的技术可以降低验证堆栈性能的真实世界测试负担，从而实现对机器人系统更高效和经济的实验评估。", "summary": "本文提出一个利用跨测试平台（如模拟和真实世界）配对数据进行度量估计的通用框架。通过将廉价的辅助测量作为控制变量，该方法能显著降低真实世界度量估计的方差，从而减少所需的高成本真实世界样本量。理论分析和在自动驾驶及四足机器人领域的实证结果均表明，该方法有效提升了样本效率，降低了机器人系统验证的测试负担。", "keywords": "控制变量, 样本效率, 机器人验证, 度量估计, 跨平台关联", "comments": "这项工作提出了一种创新且实用的方法来解决机器人系统验证中真实世界测试成本高昂的问题。通过引入控制变量和利用跨平台关联性，它有效地提高了数据利用率和估计精度，对于推动机器人系统在实际应用中的可靠性具有重要意义。"}}
{"id": "2506.20326", "title": "From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents", "authors": ["Sergio Torres Aguilar"], "summary": "Robust Document Layout Analysis (DLA) is critical for the automated\nprocessing and understanding of historical documents with complex page\norganizations. This paper benchmarks five state-of-the-art object detection\narchitectures on three annotated datasets representing a spectrum of\ncodicological complexity: The e-NDP, a corpus of Parisian medieval registers\n(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval\nand modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated\nbooks of hours (ca.13th-16th centuries). We evaluate two Transformer-based\nmodels (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and\nYOLO-World). Our findings reveal significant performance variations dependent\non model architecture, data set characteristics, and bounding box\nrepresentation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results\n(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on\nthe more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB\nsignificantly outperforms all other models (0.564 and 0.568, respectively).\nThis study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)\nis not a minor refinement but a fundamental requirement for accurately modeling\nthe non-Cartesian nature of historical manuscripts. We conclude that a key\ntrade-off exists between the global context awareness of Transformers, ideal\nfor structured layouts, and the superior generalization of CNN-OBB models for\nvisually diverse and complex documents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20326v1", "categories": ["cs.CV", "cs.CL", "cs.DB"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20326v1", "AI": {"title_translation": "从古籍学到代码：Transformer和YOLO检测器在历史文献版面分析中的比较研究", "tldr": "本文比较了Transformer和YOLO模型在历史文献版面分析中的表现，发现基于OBB的YOLO模型在复杂数据集上表现出色，而Transformer更适合结构化布局。", "motivation": "稳健的文档版面分析（DLA）对于自动处理和理解具有复杂页面组织的历史文献至关重要。", "method": "本文在三个代表不同古籍学复杂性的标注数据集（e-NDP、CATMuS、HORAE）上，对五种最先进的目标检测架构（两种基于Transformer的模型：Co-DETR、Grounding DINO；三种YOLO变体：AABB、OBB和YOLO-World）进行了基准测试，并评估了模型架构、数据集特征和边界框表示对性能的影响。", "result": "研究发现性能显著依赖于模型架构、数据集特征和边界框表示。在e-NDP数据集上，Co-DETR表现最佳（0.752 mAP@.50:.95），YOLOv11X-OBB紧随其后（0.721）。然而，在更复杂的CATMuS和HORAE数据集上，基于CNN的YOLOv11x-OBB显著优于所有其他模型（分别为0.564和0.568）。研究明确指出，使用定向边界框（OBB）是准确建模历史手稿非笛卡尔性质的基本要求。", "conclusion": "Transformer的全局上下文感知能力（适用于结构化布局）与CNN-OBB模型在视觉多样和复杂文档中的卓越泛化能力之间存在一个关键的权衡。", "translation": "稳健的文档版面分析（DLA）对于自动处理和理解具有复杂页面组织的历史文献至关重要。本文在三个代表不同古籍学复杂性的标注数据集上，对五种最先进的目标检测架构进行了基准测试：e-NDP（巴黎中世纪登记簿语料库，1326-1504年）；CATMuS（源自各种中世纪和现代来源的多类别数据集，约12-17世纪）；以及HORAE（装饰精美的时辰书语料库，约13-16世纪）。我们评估了两种基于Transformer的模型（Co-DETR、Grounding DINO）与三种YOLO变体（AABB、OBB和YOLO-World）。我们的研究结果揭示了性能的显著差异，这取决于模型架构、数据集特征和边界框表示。在e-NDP数据集中，Co-DETR取得了最先进的结果（0.752 mAP@.50:.95），紧随其后的是YOLOv11X-OBB（0.721）。相反，在更复杂的CATMuS和HORAE数据集中，基于CNN的YOLOv11x-OBB显著优于所有其他模型（分别为0.564和0.568）。这项研究明确表明，使用定向边界框（OBB）并非一个次要的改进，而是准确建模历史手稿非笛卡尔性质的基本要求。我们得出结论，Transformer的全局上下文感知能力（适用于结构化布局）与CNN-OBB模型在视觉多样和复杂文档中的卓越泛化能力之间存在一个关键的权衡。", "summary": "本文对Transformer和YOLO系列共五种最先进的目标检测模型在历史文献版面分析中的性能进行了比较研究。通过在e-NDP、CATMuS和HORAE三个不同复杂度的历史文献数据集上进行评估，研究发现模型性能受架构、数据集特性和边界框表示方式的影响。Co-DETR在结构化数据集e-NDP上表现优异，而基于CNN的YOLOv11x-OBB模型在更复杂、视觉多样化的CATMuS和HORAE数据集上显著领先。研究强调，定向边界框（OBB）对于准确处理历史手稿的非笛卡尔布局至关重要，并指出了Transformer的全局上下文能力与CNN-OBB模型泛化能力之间在不同文档类型上的权衡。", "keywords": "文档版面分析, 历史文献, Transformer, YOLO, 定向边界框", "comments": "该论文解决了历史文献版面分析这一具有挑战性的问题，这类文献通常具有不规则的布局。这项比较研究为现代检测架构的优缺点提供了宝贵的见解，尤其突出了定向边界框对于非笛卡尔布局的重要性，对数字人文和文化遗产领域的计算机视觉研究做出了重要贡献。所识别出的Transformer和CNN-OBB模型之间的权衡为未来的研究和应用提供了实践指导。"}}
{"id": "2506.20057", "title": "Universal pre-training by iterated random computation", "authors": ["Peter Bloem"], "summary": "We investigate the use of randomly generated data for the sake of\npre-training a model. We justify this approach theoretically from the\nperspective of algorithmic complexity, building on recent research that shows\nthat sequence models can be trained to approximate Solomonoff induction. We\nderive similar, but complementary theoretical results. We show empirically that\nsynthetically generated data can be used to pre-train a model before the data\nis seen. We replicate earlier results that models trained this way show\nzero-shot in-context learning across a variety of datasets, and that this\nperformance improves with scale. We extend earlier results to real-world data,\nand show that finetuning a model after pre-training offers faster convergence\nand better generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20057v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20057v1", "AI": {"title_translation": "通过迭代随机计算的通用预训练", "tldr": "研究了使用随机生成数据进行模型预训练，理论上从算法复杂性角度证明其合理性，并经验性地展示了其在零样本学习、收敛速度和泛化能力上的优势。", "motivation": "探索使用随机生成数据进行模型预训练的可行性，并从理论和实践两方面验证其有效性，以期实现更高效的模型训练。", "method": "理论上，从算法复杂性角度证明其合理性，基于序列模型可近似Solomonoff归纳的研究。经验上，使用合成数据进行模型预训练，并在多种数据集上复制了零样本上下文学习的早期结果，并扩展到真实世界数据。", "result": "理论上，得到了相似但互补的理论结果。经验上，合成数据可用于预训练模型；模型展示了零样本上下文学习能力，且性能随规模提升；预训练后微调模型能实现更快的收敛和更好的泛化。", "conclusion": "使用随机生成数据进行模型预训练是一种有效的方法，它不仅在理论上可行，而且在实践中能提高模型的学习效率和泛化能力，尤其是在零样本学习和微调阶段。", "translation": "我们研究了使用随机生成数据进行模型预训练的方法。我们从算法复杂性的角度，基于最近显示序列模型可以训练以近似Solomonoff归纳的研究，从理论上证明了这种方法的合理性。我们推导出了相似但互补的理论结果。我们通过实验证明，合成生成的数据可以在模型看到实际数据之前用于预训练。我们复制了早期结果，即通过这种方式训练的模型在各种数据集上显示出零样本上下文学习能力，并且这种性能随规模的扩大而提高。我们将早期结果扩展到真实世界数据，并表明在预训练后对模型进行微调可以提供更快的收敛速度和更好的泛化能力。", "summary": "这项研究探讨了利用随机生成数据进行模型预训练的通用方法。理论上，论文从算法复杂性角度证明了该方法的合理性，并得到了互补的理论结果。经验上，研究表明合成数据可有效用于预训练，使模型在零样本上下文学习任务中表现出色，且性能随规模提升。此外，预训练后的模型在真实世界数据上进行微调时，展现出更快的收敛速度和更强的泛化能力。", "keywords": "随机计算, 预训练, 零样本学习, 算法复杂性, 模型泛化", "comments": "这篇论文提出了一种新颖的通用预训练范式，即使用随机生成数据进行预训练。其创新点在于将算法复杂性理论与模型预训练相结合，并经验性地验证了其有效性。这种方法有望降低对大量特定领域数据的依赖，为模型的通用能力和高效微调提供了新的思路。其局限性可能在于随机数据生成的复杂性和理论结果在实际大规模应用中的具体指导。"}}
{"id": "2506.19862", "title": "DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules", "authors": ["Junjie Xu", "Jiahao Zhang", "Mangal Prakash", "Xiang Zhang", "Suhang Wang"], "summary": "Geometric graph neural networks (GNNs) that respect E(3) symmetries have\nachieved strong performance on small molecule modeling, but they face\nscalability and expressiveness challenges when applied to large biomolecules\nsuch as RNA and proteins. These systems require models that can simultaneously\ncapture fine-grained atomic interactions, long-range dependencies across\nspatially distant components, and biologically relevant hierarchical structure,\nsuch as atoms forming residues, which in turn form higher-order domains.\nExisting geometric GNNs, which typically operate exclusively in either\nEuclidean or Spherical Harmonics space, are limited in their ability to capture\nboth the fine-scale atomic details and the long-range, symmetry-aware\ndependencies required for modeling the multi-scale structure of large\nbiomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant\nNetwork that constructs complementary representations in both Euclidean and\nSpherical Harmonics spaces to capture local geometry and global symmetry-aware\nfeatures. DualEquiNet employs bidirectional cross-space message passing and a\nnovel Cross-Space Interaction Pooling mechanism to hierarchically aggregate\natomic features into biologically meaningful units, such as residues, enabling\nefficient and expressive multi-scale modeling for large biomolecular systems.\nDualEquiNet achieves state-of-the-art performance on multiple existing\nbenchmarks for RNA property prediction and protein modeling, and outperforms\nprior methods on two newly introduced 3D structural benchmarks demonstrating\nits broad effectiveness across a range of large biomolecule modeling tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19862v1", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.19862v1", "AI": {"title_translation": "DualEquiNet：一种用于大型生物分子的双空间分层等变网络", "tldr": "DualEquiNet是一个新的几何图神经网络，它结合了欧几里得空间和球谐函数空间，以有效且富有表现力地建模大型生物分子的多尺度结构，并在多个基准测试中取得了最先进的性能。", "motivation": "现有E(3)对称几何图神经网络在小型分子建模上表现良好，但应用于大型生物分子（如RNA和蛋白质）时面临可扩展性和表达能力挑战。这些系统需要能同时捕获精细原子相互作用、长程依赖和生物学分层结构的模型，而现有模型（仅在欧几里得或球谐函数空间操作）无法很好地捕获这些多尺度结构。", "method": "本文引入了DualEquiNet，一个双空间分层等变网络。它在欧几里得空间和球谐函数空间中构建互补表示，以捕获局部几何和全局对称感知特征。DualEquiNet采用双向跨空间消息传递和新颖的跨空间交互池化机制，将原子特征分层聚合为生物学有意义的单元（如残基），从而实现大型生物分子系统的高效和富有表现力的多尺度建模。", "result": "DualEquiNet在多个现有RNA性质预测和蛋白质建模基准测试中取得了最先进的性能，并在两个新引入的3D结构基准测试中优于现有方法，证明了其在各种大型生物分子建模任务中的广泛有效性。", "conclusion": "DualEquiNet通过结合欧几里得和球谐函数空间的优势，克服了现有几何图神经网络在大型生物分子建模中的局限性，实现了高效且高表达能力的多尺度结构建模，并在多个任务中达到最先进水平。", "translation": "几何图神经网络（GNNs）在小型分子建模方面取得了强大的性能，这些网络尊重E(3)对称性，但当应用于大型生物分子如RNA和蛋白质时，它们面临可扩展性和表达能力的挑战。这些系统需要能够同时捕获精细的原子相互作用、空间上遥远组件之间的长程依赖关系以及生物学相关的分层结构（例如原子形成残基，残基进而形成更高阶的结构域）的模型。现有的几何GNNs通常仅在欧几里得空间或球谐函数空间中操作，它们在捕获精细尺度的原子细节和建模大型生物分子多尺度结构所需的远距离、对称感知依赖方面的能力有限。我们引入了DualEquiNet，一个双空间分层等变网络，它在欧几里得空间和球谐函数空间中构建互补表示，以捕获局部几何和全局对称感知特征。DualEquiNet采用双向跨空间消息传递和新颖的跨空间交互池化机制，将原子特征分层聚合为生物学有意义的单元，如残基，从而实现大型生物分子系统的高效和富有表现力的多尺度建模。DualEquiNet在多个现有RNA性质预测和蛋白质建模基准测试中取得了最先进的性能，并在两个新引入的3D结构基准测试中优于现有方法，证明了其在各种大型生物分子建模任务中的广泛有效性。", "summary": "DualEquiNet是一种新型双空间分层等变网络，旨在解决现有几何图神经网络在建模大型生物分子时面临的扩展性和表达能力不足问题。它通过在欧几里得和球谐函数空间中构建互补表示，并利用跨空间消息传递和交互池化机制，有效地捕获生物分子的多尺度结构，包括原子级细节和长程对称依赖。该模型在RNA性质预测和蛋白质建模的多个基准测试中表现出最先进的性能，并超越了新引入的3D结构基准测试中的现有方法。", "keywords": "几何图神经网络, 大型生物分子, 双空间网络, 等变网络, 多尺度建模", "comments": "DualEquiNet的创新点在于其双空间（欧几里得和球谐函数）的设计，这使其能够同时捕捉局部精细几何和全局对称感知特征，有效解决了大型生物分子多尺度建模的挑战。其分层聚合机制也增强了模型的生物学相关性和表达能力。"}}
{"id": "2506.20585", "title": "On the Impact of Sybil-based Attacks on Mobile Crowdsensing for Transportation", "authors": ["Alexander Söderhäll", "Zahra Alimadadi", "Panos Papadimitratos"], "summary": "Mobile Crowd-Sensing (MCS) enables users with personal mobile devices (PMDs)\nto gain information on their surroundings. Users collect and contribute data on\ndifferent phenomena using their PMD sensors, and the MCS system processes this\ndata to extract valuable information for end users. Navigation MCS-based\napplications (N-MCS) are prevalent and important for transportation: users\nshare their location and speed while driving and, in return, find efficient\nroutes to their destinations. However, N-MCS are currently vulnerable to\nmalicious contributors, often termed Sybils: submitting falsified data,\nseemingly from many devices that are not truly present on target roads, falsely\nreporting congestion when there is none, thus changing the road status the\nN-MCS infers. The attack effect is that the N-MCS returns suboptimal routes to\nusers, causing late arrival and, overall, deteriorating road traffic flow. We\ninvestigate exactly the impact of Sybil-based attacks on N-MCS: we design an\nN-MCS system that offers efficient routing on top of the vehicular simulator\nSUMO, using the InTAS road network as our scenario. We design experiments\nattacking an individual N-MCS user as well as a larger population of users,\nselecting the adversary targets based on graph-theoretical arguments. Our\nexperiments show that the resources required for a successful attack depend on\nthe location of the attack (i.e., the surrounding road network and traffic) and\nthe extent of Sybil contributed data for the targeted road(s). We demonstrate\nthat Sybil attacks can alter the route of N-MCS users, increasing average\ntravel time by 20% with Sybils 3% of the N-MCS user population.", "comment": "7 pages, 5 figures, 2 tables, TrustSense workshop of PerCom 2025", "pdf_url": "http://arxiv.org/pdf/2506.20585v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20585v1", "AI": {"title_translation": "基于Sybil攻击对交通移动众包影响的研究", "tldr": "研究了Sybil攻击对交通导航移动众包系统（N-MCS）的影响，发现此类攻击能显著增加用户出行时间，并分析了攻击成功的资源需求。", "motivation": "导航移动众包应用（N-MCS）在交通领域广泛应用，但它们容易受到恶意贡献者（Sybil）的攻击，这些攻击通过提交虚假数据来误报拥堵，从而导致N-MCS提供次优路线，增加出行时间并恶化交通流量。本研究旨在精确调查Sybil攻击对N-MCS的影响。", "method": "设计了一个基于车载模拟器SUMO的N-MCS系统，并在InTAS路网场景下进行实验。实验设计针对单个N-MCS用户和更大用户群体进行攻击，并基于图论选择攻击目标。", "result": "实验表明，成功攻击所需的资源取决于攻击位置（即周围路网和交通状况）以及Sybil贡献数据对目标道路的程度。研究证明，Sybil攻击可以改变N-MCS用户的路线，当Sybil用户占N-MCS用户总数的3%时，平均出行时间可增加20%。", "conclusion": "Sybil攻击对交通领域的移动众包系统具有显著影响，能够有效改变用户路线并大幅增加出行时间，其影响程度与攻击的策略和资源投入有关。", "translation": "移动众包（MCS）使用户能够通过个人移动设备（PMD）获取周围信息。用户使用其PMD传感器收集和贡献不同现象的数据，MCS系统处理这些数据以提取对终端用户有价值的信息。基于MCS的导航应用（N-MCS）在交通领域普遍且重要：用户在驾驶时分享他们的位置和速度，作为回报，可以找到到达目的地的有效路线。然而，N-MCS目前容易受到恶意贡献者（通常称为Sybil）的攻击：他们提交伪造数据，这些数据看似来自许多实际上不存在于目标道路上的设备，虚假报告拥堵，从而改变N-MCS推断的道路状态。攻击的效果是N-MCS向用户返回次优路线，导致迟到，并总体上恶化道路交通流量。我们正是调查了基于Sybil的攻击对N-MCS的影响：我们设计了一个N-MCS系统，该系统在车载模拟器SUMO之上提供高效路由，并使用InTAS路网作为我们的场景。我们设计了攻击单个N-MCS用户以及更大用户群体的实验，并根据图论论证选择攻击目标。我们的实验表明，成功攻击所需的资源取决于攻击位置（即周围路网和交通状况）以及Sybil贡献数据对目标道路的程度。我们证明，Sybil攻击可以改变N-MCS用户的路线，当Sybil用户占N-MCS用户总数的3%时，平均出行时间可增加20%。", "summary": "本研究深入探讨了Sybil攻击对交通导航移动众包系统（N-MCS）的负面影响。N-MCS通过用户共享位置和速度提供高效路线，但易受Sybil攻击者提交虚假拥堵数据，导致系统提供次优路线并恶化交通。研究团队设计了一个基于SUMO模拟器的N-MCS系统，并在InTAS路网中进行实验。结果显示，攻击的成功依赖于攻击位置和Sybil数据贡献量，且仅3%的Sybil用户即可使N-MCS用户的平均出行时间增加20%。这强调了N-MCS系统在面对此类恶意行为时的脆弱性。", "keywords": "移动众包, Sybil攻击, 交通, 导航, 模拟", "comments": "本文揭示了移动众包系统在交通领域面临的严重安全威胁，特别是Sybil攻击的潜在破坏力。其创新之处在于通过模拟实验量化了攻击对出行时间的影响，并指出了攻击成功与攻击位置和数据量之间的关系。这对于未来设计更鲁棒的众包导航系统具有重要指导意义。"}}
{"id": "2506.20566", "title": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction", "authors": ["Zhonghao Shi", "Enyu Zhao", "Nathaniel Dennler", "Jingzhen Wang", "Xinyang Xu", "Kaleen Shrestha", "Mengxue Fu", "Daniel Seita", "Maja Matarić"], "summary": "Real-time human perception is crucial for effective human-robot interaction\n(HRI). Large vision-language models (VLMs) offer promising generalizable\nperceptual capabilities but often suffer from high latency, which negatively\nimpacts user experience and limits VLM applicability in real-world scenarios.\nTo systematically study VLM capabilities in human perception for HRI and\nperformance-latency trade-offs, we introduce HRIBench, a visual\nquestion-answering (VQA) benchmark designed to evaluate VLMs across a diverse\nset of human perceptual tasks critical for HRI. HRIBench covers five key\ndomains: (1) non-verbal cue understanding, (2) verbal instruction\nunderstanding, (3) human-robot object relationship understanding, (4) social\nnavigation, and (5) person identification. To construct HRIBench, we collected\ndata from real-world HRI environments to curate questions for non-verbal cue\nunderstanding, and leveraged publicly available datasets for the remaining four\ndomains. We curated 200 VQA questions for each domain, resulting in a total of\n1000 questions for HRIBench. We then conducted a comprehensive evaluation of\nboth state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.\nOur results show that, despite their generalizability, current VLMs still\nstruggle with core perceptual capabilities essential for HRI. Moreover, none of\nthe models within our experiments demonstrated a satisfactory\nperformance-latency trade-off suitable for real-time deployment, underscoring\nthe need for future research on developing smaller, low-latency VLMs with\nimproved human perception capabilities. HRIBench and our results can be found\nin this Github repository: https://github.com/interaction-lab/HRIBench.", "comment": "Accepted to the 19th International Symposium on Experimental Robotics\n  (ISER 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20566v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20566v1", "AI": {"title_translation": "HRIBench：用于人机交互中实时人类感知的视觉-语言模型基准测试", "tldr": "引入HRIBench基准测试，评估视觉-语言模型在人机交互中实时人类感知能力，发现现有模型在核心感知能力和性能-延迟权衡方面存在不足。", "motivation": "实时人类感知对于有效的人机交互至关重要。大型视觉-语言模型（VLMs）虽有前景，但常因高延迟影响用户体验和实际应用，且其在人机交互中人类感知能力缺乏系统性研究。", "method": "本文引入HRIBench，一个视觉问答（VQA）基准测试，旨在评估VLMs在人机交互中关键人类感知任务上的表现。HRIBench涵盖非语言线索理解、口头指令理解、人机对象关系理解、社交导航和人物识别五个领域。通过收集真实人机交互环境数据和利用公开数据集，构建了总计1000个VQA问题。随后，对11个最先进的闭源和开源VLMs进行了综合评估。", "result": "现有VLMs尽管具有泛化能力，但在人机交互所需的核心感知能力方面仍表现不佳。此外，没有模型能达到令人满意的性能-延迟权衡以实现实时部署。", "conclusion": "研究结果强调了未来需要开发更小、低延迟且具有改进人类感知能力的VLMs。", "translation": "实时人类感知对于有效的人机交互（HRI）至关重要。大型视觉-语言模型（VLM）提供了有前景的通用感知能力，但通常存在高延迟问题，这会负面影响用户体验并限制VLM在实际场景中的适用性。为了系统地研究VLM在HRI中人类感知能力以及性能-延迟权衡，我们引入了HRIBench，这是一个视觉问答（VQA）基准测试，旨在评估VLM在对HRI至关重要的各种人类感知任务中的表现。HRIBench涵盖五个关键领域：（1）非语言线索理解，（2）口头指令理解，（3）人机对象关系理解，（4）社交导航，以及（5）人物识别。为了构建HRIBench，我们从真实世界的HRI环境中收集数据，整理了非语言线索理解的问题，并利用公开可用数据集构建了其余四个领域的问题。我们为每个领域整理了200个VQA问题，HRIBench总计1000个问题。随后，我们对11个最先进的闭源和开源VLM在HRIBench上进行了全面评估。我们的结果表明，尽管VLM具有泛化能力，但当前的VLM在HRI所需的核心感知能力方面仍然存在困难。此外，我们实验中的模型都没有表现出令人满意的性能-延迟权衡，足以满足实时部署的需求，这强调了未来需要研究开发更小、低延迟且具有改进人类感知能力的VLM。HRIBench和我们的结果可以在此GitHub仓库中找到：https://github.com/interaction-lab/HRIBench。", "summary": "本文介绍了HRIBench，一个专门用于评估视觉-语言模型（VLMs）在人机交互（HRI）中实时人类感知能力的视觉问答（VQA）基准测试。HRIBench涵盖五大关键感知领域，包含1000个精心策划的问题。通过对11个主流VLMs的评估，研究发现现有VLMs在HRI所需的核心感知能力上表现不足，并且无法在性能与延迟之间取得实时部署所需的平衡。研究强调了未来开发更小、低延迟且具备更强人类感知能力的VLMs的必要性。", "keywords": "人机交互, 视觉-语言模型, 基准测试, 实时感知, 性能-延迟权衡", "comments": "这篇论文通过引入HRIBench，首次系统地关注了视觉-语言模型在人机交互中实时人类感知的具体挑战，填补了现有基准测试的空白。其创新之处在于结合了真实世界数据和多模态感知任务，为未来VLMs在HRI领域的优化提供了明确的方向和评估工具。"}}
{"id": "2506.20342", "title": "Feature Hallucination for Self-supervised Action Recognition", "authors": ["Lei Wang", "Piotr Koniusz"], "summary": "Understanding human actions in videos requires more than raw pixel analysis;\nit relies on high-level semantic reasoning and effective integration of\nmultimodal features. We propose a deep translational action recognition\nframework that enhances recognition accuracy by jointly predicting action\nconcepts and auxiliary features from RGB video frames. At test time,\nhallucination streams infer missing cues, enriching feature representations\nwithout increasing computational overhead. To focus on action-relevant regions\nbeyond raw pixels, we introduce two novel domain-specific descriptors. Object\nDetection Features (ODF) aggregate outputs from multiple object detectors to\ncapture contextual cues, while Saliency Detection Features (SDF) highlight\nspatial and intensity patterns crucial for action recognition. Our framework\nseamlessly integrates these descriptors with auxiliary modalities such as\noptical flow, Improved Dense Trajectories, skeleton data, and audio cues. It\nremains compatible with state-of-the-art architectures, including I3D,\nAssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE\nV2 and InternVideo2. To handle uncertainty in auxiliary features, we\nincorporate aleatoric uncertainty modeling in the hallucination step and\nintroduce a robust loss function to mitigate feature noise. Our multimodal\nself-supervised action recognition framework achieves state-of-the-art\nperformance on multiple benchmarks, including Kinetics-400, Kinetics-600, and\nSomething-Something V2, demonstrating its effectiveness in capturing\nfine-grained action dynamics.", "comment": "Accepted for publication in International Journal of Computer Vision\n  (IJCV)", "pdf_url": "http://arxiv.org/pdf/2506.20342v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20342v1", "AI": {"title_translation": "用于自监督动作识别的特征幻觉", "tldr": "提出一个深度平移动作识别框架，通过幻觉流预测缺失特征，结合新的领域特定描述符（ODF、SDF）和多模态数据，实现自监督动作识别的SOTA性能。", "motivation": "视频中的人类动作理解需要高级语义推理和多模态特征的有效整合，而不仅仅是原始像素分析。现有方法可能无法充分捕获动作相关区域的细微动态或处理辅助特征的不确定性。", "method": "提出一个深度平移动作识别框架，通过联合预测动作概念和辅助特征来增强识别精度。在测试时，幻觉流推断缺失线索，丰富特征表示。引入两种领域特定描述符：对象检测特征（ODF）和显著性检测特征（SDF）。该框架将这些描述符与光流、改进密集轨迹、骨骼数据和音频线索等多模态辅助信息无缝集成。为处理辅助特征的不确定性，在幻觉步骤中结合了任意不确定性建模，并引入鲁棒损失函数以减轻特征噪声。", "result": "在Kinetics-400、Kinetics-600和Something-Something V2等多个基准测试中，实现了最先进的性能，证明了其在捕获细粒度动作动态方面的有效性。", "conclusion": "该多模态自监督动作识别框架通过特征幻觉、引入领域特定描述符和不确定性建模，有效提升了动作识别的准确性，并在多个基准测试中达到了最先进的性能。", "translation": "理解视频中的人类动作不仅仅需要原始像素分析；它依赖于高级语义推理和多模态特征的有效整合。我们提出一个深度平移动作识别框架，通过联合预测动作概念和RGB视频帧中的辅助特征来提高识别准确性。在测试时，幻觉流推断缺失线索，丰富特征表示，且不增加计算开销。为了关注原始像素之外与动作相关的区域，我们引入了两种新颖的领域特定描述符。对象检测特征（ODF）聚合了多个对象检测器的输出以捕获上下文线索，而显著性检测特征（SDF）则突出显示对动作识别至关重要的空间和强度模式。我们的框架将这些描述符与光流、改进密集轨迹、骨骼数据和音频线索等辅助模态无缝集成。它与最先进的架构保持兼容，包括I3D、AssembleNet、视频Transformer网络、FASTER以及VideoMAE V2和InternVideo2等最新模型。为了处理辅助特征中的不确定性，我们在幻觉步骤中加入了任意不确定性建模，并引入了一个鲁棒损失函数来减轻特征噪声。我们的多模态自监督动作识别框架在Kinetics-400、Kinetics-600和Something-Something V2等多个基准测试中取得了最先进的性能，证明了其在捕获细粒度动作动态方面的有效性。", "summary": "该论文提出了一种用于自监督动作识别的深度平移框架。该框架通过幻觉流预测缺失特征，并在测试时不增加计算开销。为关注动作相关区域，引入了对象检测特征（ODF）和显著性检测特征（SDF）两种新颖的领域特定描述符。该框架能够无缝集成多种辅助模态，并结合不确定性建模和鲁棒损失函数处理特征噪声。实验结果表明，该框架在多个主流动作识别基准测试中达到了最先进的性能，有效捕获了细粒度的动作动态。", "keywords": "动作识别, 自监督学习, 特征幻觉, 多模态, 不确定性建模", "comments": "该论文的创新点在于引入了“特征幻觉”概念，通过预测缺失特征来丰富表示，同时不增加测试时的计算负担。其次，提出了ODF和SDF两种领域特定描述符，有效聚焦于动作相关区域。此外，结合不确定性建模和鲁棒损失函数处理多模态特征的复杂性和噪声，提升了模型的鲁棒性。其自监督学习方式也符合当前深度学习的发展趋势，并在多个基准测试上取得了最先进的性能，显示出其重要性和有效性。"}}
{"id": "2506.20061", "title": "Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models", "authors": ["Zhicheng Zhang", "Ziyan Wang", "Yali Du", "Fei Fang"], "summary": "Developing effective instruction-following policies in reinforcement learning\nremains challenging due to the reliance on extensive human-labeled instruction\ndatasets and the difficulty of learning from sparse rewards. In this paper, we\npropose a novel approach that leverages the capabilities of large language\nmodels (LLMs) to automatically generate open-ended instructions retrospectively\nfrom previously collected agent trajectories. Our core idea is to employ LLMs\nto relabel unsuccessful trajectories by identifying meaningful subtasks the\nagent has implicitly accomplished, thereby enriching the agent's training data\nand substantially alleviating reliance on human annotations. Through this\nopen-ended instruction relabeling, we efficiently learn a unified\ninstruction-following policy capable of handling diverse tasks within a single\npolicy. We empirically evaluate our proposed method in the challenging Craftax\nenvironment, demonstrating clear improvements in sample efficiency, instruction\ncoverage, and overall policy performance compared to state-of-the-art\nbaselines. Our results highlight the effectiveness of utilizing LLM-guided\nopen-ended instruction relabeling to enhance instruction-following\nreinforcement learning.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2506.20061v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20061v1", "AI": {"title_translation": "通过大型语言模型进行开放式指令重标记来学习指令遵循策略", "tldr": "本文提出了一种利用大型语言模型（LLMs）自动从智能体轨迹中生成开放式指令的方法，以解决强化学习中指令遵循策略对人工标注的依赖和稀疏奖励问题，并在Craftax环境中展示了显著改进。", "motivation": "在强化学习中开发有效的指令遵循策略面临挑战，主要原因是对大量人工标注指令数据集的依赖以及从稀疏奖励中学习的困难。", "method": "本文提出了一种新颖的方法，利用大型语言模型（LLMs）的能力，从先前收集的智能体轨迹中回顾性地自动生成开放式指令。核心思想是利用LLMs对不成功的轨迹进行重标记，识别智能体隐式完成的有意义的子任务，从而丰富训练数据并大大减轻对人工标注的依赖。通过这种开放式指令重标记，高效地学习一个能够处理多样化任务的统一指令遵循策略。", "result": "在具有挑战性的Craftax环境中对所提出的方法进行了实证评估，与最先进的基线相比，在样本效率、指令覆盖率和整体策略性能方面均显示出明显的改进。", "conclusion": "研究结果强调了利用LLM引导的开放式指令重标记在增强指令遵循强化学习方面的有效性。", "translation": "在强化学习中开发有效的指令遵循策略仍然充满挑战，这主要是由于对大量人工标注指令数据集的依赖以及从稀疏奖励中学习的困难。在本文中，我们提出了一种新颖的方法，该方法利用大型语言模型（LLMs）的能力，从先前收集的智能体轨迹中回顾性地自动生成开放式指令。我们的核心思想是利用LLMs对不成功的轨迹进行重标记，通过识别智能体隐式完成的有意义的子任务，从而丰富智能体的训练数据并大大减轻对人工标注的依赖。通过这种开放式指令重标记，我们高效地学习了一个能够处理单个策略中多样化任务的统一指令遵循策略。我们在具有挑战性的Craftax环境中对所提出的方法进行了实证评估，与最先进的基线相比，在样本效率、指令覆盖率和整体策略性能方面均显示出明显的改进。我们的结果强调了利用LLM引导的开放式指令重标记在增强指令遵循强化学习方面的有效性。", "summary": "本文提出了一种利用大型语言模型（LLMs）进行开放式指令重标记的新方法，以克服强化学习中指令遵循策略对人工标注的依赖和稀疏奖励问题。通过让LLMs从智能体轨迹中自动生成并重标记指令，特别是针对不成功的轨迹识别子任务，该方法显著丰富了训练数据并提高了样本效率。在Craftax环境中的实验结果表明，该方法在指令覆盖率和策略性能方面均优于现有基线，证明了LLM引导的指令重标记在强化学习中的有效性。", "keywords": "指令遵循, 强化学习, 大型语言模型, 指令重标记, 样本效率", "comments": "这项研究的创新之处在于利用大型语言模型自动生成和重标记指令，从而显著减少了对人工标注的依赖，并解决了稀疏奖励问题。这为指令遵循强化学习提供了一种更高效、可扩展的训练范式。"}}
{"id": "2506.19863", "title": "Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research", "authors": ["Ahmed Almeldein", "Mohammed Alnaggar", "Rick Archibald", "Tom Beck", "Arpan Biswas", "Rike Bostelmann", "Wes Brewer", "Chris Bryan", "Christopher Calle", "Cihangir Celik", "Rajni Chahal", "Jong Youl Choi", "Arindam Chowdhury", "Mark Cianciosa", "Franklin Curtis", "Gregory Davidson", "Sebastian De Pascuale", "Lisa Fassino", "Ana Gainaru", "Yashika Ghai", "Luke Gibson", "Qian Gong", "Christopher Greulich", "Scott Greenwood", "Cory Hauck", "Ehab Hassan", "Rinkle Juneja", "Soyoung Kang", "Scott Klasky", "Atul Kumar", "Vineet Kumar", "Paul Laiu", "Calvin Lear", "Yan-Ru Lin", "Jono McConnell", "Furkan Oz", "Anant Raj", "Pradeep Ramuhalli", "Marie Romedenne", "Samantha Sabatino", "José Salcedo-Pérez", "Nathan D. See", "Arpan Sircar", "Punam Thankur", "Tim Younkin", "Xiao-Ying Yu", "Prashant Jain", "Tom Evans", "Prasanna Balaprakash"], "summary": "The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated\nthe potential of Large Language Models (LLMs) to accelerate fusion and fission\nresearch. Fourteen interdisciplinary teams explored diverse nuclear science\nchallenges using ChatGPT, Gemini, Claude, and other AI models over a single\nday. Applications ranged from developing foundation models for fusion reactor\ncontrol to automating Monte Carlo simulations, predicting material degradation,\nand designing experimental programs for advanced reactors. Teams employed\nstructured workflows combining prompt engineering, deep research capabilities,\nand iterative refinement to generate hypotheses, prototype code, and research\nstrategies. Key findings demonstrate that LLMs excel at early-stage\nexploration, literature synthesis, and workflow design, successfully\nidentifying research gaps and generating plausible experimental frameworks.\nHowever, significant limitations emerged, including difficulties with novel\nmaterials designs, advanced code generation for modeling and simulation, and\ndomain-specific details requiring expert validation. The successful outcomes\nresulted from expert-driven prompt engineering and treating AI as a\ncomplementary tool rather than a replacement for physics-based methods. The\nworkshop validated AI's potential to accelerate nuclear energy research through\nrapid iteration and cross-disciplinary synthesis while highlighting the need\nfor curated nuclear-specific datasets, workflow automation, and specialized\nmodel development. These results provide a roadmap for integrating AI tools\ninto nuclear science workflows, potentially reducing development cycles for\nsafer, more efficient nuclear energy systems while maintaining rigorous\nscientific standards.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19863v1", "categories": ["physics.comp-ph", "cs.AI"], "cate": "physics.comp-ph", "url": "http://arxiv.org/abs/2506.19863v1", "AI": {"title_translation": "探索前沿大型语言模型在核能研究中的能力", "tldr": "一项在橡树岭国家实验室举办的AI核能研讨会评估了大型语言模型（LLMs）在加速聚变和裂变研究方面的潜力。研讨会发现LLMs在早期探索、文献综合和工作流设计方面表现出色，但也存在局限性，如新材料设计和高级代码生成。成功应用的关键在于专家驱动的提示工程和将AI作为辅助工具。", "motivation": "在橡树岭国家实验室举办的AI核能研讨会旨在评估大型语言模型（LLMs）在加速聚变和裂变研究方面的潜力。", "method": "十四个跨学科团队在一天内使用ChatGPT、Gemini、Claude及其他AI模型探索了各种核科学挑战。团队采用了结合提示工程、深度研究能力和迭代优化的结构化工作流来生成假设、原型代码和研究策略。", "result": "LLMs在早期探索、文献综合和工作流设计方面表现出色，成功识别了研究空白并生成了合理的实验框架。然而，存在显著局限性，包括在新材料设计、建模和模拟的高级代码生成以及需要专家验证的领域特定细节方面的困难。成功的成果得益于专家驱动的提示工程和将AI视为辅助工具而非替代物理方法的态度。", "conclusion": "该研讨会验证了AI通过快速迭代和跨学科综合加速核能研究的潜力，同时强调了对精选的核能专用数据集、工作流自动化和专业模型开发的需求。这些结果为将AI工具整合到核科学工作流中提供了路线图，可能缩短更安全、更高效核能系统的开发周期，同时保持严格的科学标准。", "translation": "橡树岭国家实验室的AI核能研讨会评估了大型语言模型（LLMs）加速聚变和裂变研究的潜力。十四个跨学科团队在一天内使用ChatGPT、Gemini、Claude和其他AI模型探索了各种核科学挑战。应用范围从开发聚变反应堆控制的基础模型到自动化蒙特卡洛模拟、预测材料降解和设计先进反应堆的实验方案。团队采用了结合提示工程、深度研究能力和迭代优化的结构化工作流来生成假设、原型代码和研究策略。关键发现表明，LLMs在早期探索、文献综合和工作流设计方面表现出色，成功识别了研究空白并生成了合理的实验框架。然而，也出现了显著局限性，包括在新材料设计、建模和模拟的高级代码生成以及需要专家验证的领域特定细节方面的困难。成功的成果得益于专家驱动的提示工程，并将AI视为辅助工具而非替代基于物理的方法。该研讨会验证了AI通过快速迭代和跨学科综合加速核能研究的潜力，同时强调了对精选的核能专用数据集、工作流自动化和专业模型开发的需求。这些结果为将AI工具整合到核科学工作流中提供了路线图，可能缩短更安全、更高效核能系统的开发周期，同时保持严格的科学标准。", "summary": "在橡树岭国家实验室举办的AI核能研讨会，探讨了大型语言模型（LLMs）在加速核能（聚变和裂变）研究中的应用潜力。通过14个跨学科团队的实践，发现LLMs在早期探索、文献综合和工作流设计方面表现突出，但新材料设计和高级代码生成仍是其局限。研究强调了专家驱动的提示工程和将AI作为辅助工具的重要性，并为未来AI在核科学领域的整合提供了路线图，旨在提高效率并维持科学严谨性。", "keywords": "大型语言模型, 核能研究, AI应用, 聚变裂变, 提示工程", "comments": "该研究通过实际研讨会的形式，系统评估了当前大型语言模型在核能研究这一高度专业化领域的应用潜力和局限性。其创新之处在于，它不仅展示了LLMs在文献梳理和初步构想方面的优势，更明确指出了其在深层专业知识和复杂代码生成方面的不足，为后续研究和应用提供了清晰的方向。强调“专家驱动的提示工程”和“AI作为辅助工具”的理念非常重要，这为AI在复杂科学领域的落地提供了实用且负责任的指导。"}}
{"id": "2506.20269", "title": "Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models", "authors": ["Kai-Robin Lange", "Tobias Schmidt", "Matthias Reccius", "Henrik Müller", "Michael Roos", "Carsten Jentsch"], "summary": "With rapidly evolving media narratives, it has become increasingly critical\nto not just extract narratives from a given corpus but rather investigate, how\nthey develop over time. While popular narrative extraction methods such as\nLarge Language Models do well in capturing typical narrative elements or even\nthe complex structure of a narrative, applying them to an entire corpus comes\nwith obstacles, such as a high financial or computational cost. We propose a\ncombination of the language understanding capabilities of Large Language Models\nwith the large scale applicability of topic models to dynamically model\nnarrative shifts across time using the Narrative Policy Framework. We apply a\ntopic model and a corresponding change point detection method to find changes\nthat concern a specific topic of interest. Using this model, we filter our\ncorpus for documents that are particularly representative of that change and\nfeed them into a Large Language Model that interprets the change that happened\nin an automated fashion and distinguishes between content and narrative shifts.\nWe employ our pipeline on a corpus of The Wall Street Journal news paper\narticles from 2009 to 2023. Our findings indicate that a Large Language Model\ncan efficiently extract a narrative shift if one exists at a given point in\ntime, but does not perform as well when having to decide whether a shift in\ncontent or a narrative shift took place.", "comment": "14 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.20269v1", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20269v1", "AI": {"title_translation": "叙事转变检测：动态主题模型与大型语言模型的混合方法", "tldr": "该研究提出了一种结合动态主题模型和大型语言模型的新方法，用于在大规模语料库中自动检测和解释叙事随时间的变化，并将其应用于《华尔街日报》的新闻文章。", "motivation": "随着媒体叙事的快速演变，不仅要从语料库中提取叙事，更重要的是研究它们如何随时间发展。虽然大型语言模型在捕捉叙事元素方面表现良好，但将其应用于整个语料库成本高昂。因此，需要一种能够大规模应用的方法来检测叙事转变。", "method": "本研究提出了一种混合方法，结合了大型语言模型的语言理解能力和主题模型的大规模适用性。该方法利用动态主题模型和相应的变化点检测方法来识别特定主题的转变。然后，过滤出代表这些转变的文档，并将其输入到大型语言模型中，以自动化方式解释所发生的转变，并区分内容转变和叙事转变。", "result": "研究发现，大型语言模型在给定时间点存在叙事转变时能够有效地提取，但在判断是内容转变还是叙事转变时表现不佳。", "conclusion": "大型语言模型在检测叙事转变方面表现出潜力，但区分内容转变和叙事转变的能力有待提高。", "translation": "随着媒体叙事的快速演变，不仅从给定语料库中提取叙事，更重要的是研究它们如何随时间发展，这一点变得越来越重要。虽然大型语言模型等流行的叙事提取方法在捕捉典型的叙事元素甚至复杂的叙事结构方面表现良好，但将其应用于整个语料库会面临高昂的财务或计算成本等障碍。我们提出了一种结合大型语言模型的语言理解能力和主题模型大规模适用性的方法，利用叙事政策框架动态建模叙事随时间的变化。我们应用主题模型和相应的变化点检测方法来查找与特定感兴趣主题相关的变化。利用该模型，我们过滤语料库中特别能代表该变化的文档，并将其输入到大型语言模型中，以自动化方式解释所发生的变化，并区分内容转变和叙事转变。我们将我们的管道应用于2009年至2023年《华尔街日报》新闻文章语料库。我们的发现表明，大型语言模型在给定时间点存在叙事转变时可以有效地提取，但在必须决定发生的是内容转变还是叙事转变时表现不佳。", "summary": "本研究提出了一种混合方法，结合动态主题模型和大型语言模型，以克服传统大型语言模型在大规模叙事分析中的高成本问题。该方法首先使用主题模型检测语料库中的变化点，然后将代表这些变化的文档输入大型语言模型，以自动识别并解释叙事转变。该方法应用于《华尔日报》新闻文章语料库，结果显示大型语言模型能有效检测叙事转变，但在区分内容与叙事转变方面存在局限。", "keywords": "叙事转变检测, 动态主题模型, 大型语言模型, 变化点检测, 叙事分析", "comments": "这项研究的创新之处在于其混合方法，有效结合了主题模型的大规模处理能力和大型语言模型的语义理解深度，解决了大规模叙事分析的成本问题。然而，其主要局限性在于大型语言模型在区分内容转变和叙事转变方面的表现不佳，这提示了未来研究的方向，即如何更精确地定义和识别叙事层面的变化。"}}
{"id": "2506.20579", "title": "Communication-Aware Map Compression for Online Path-Planning: A Rate-Distortion Approach", "authors": ["Ali Reza Pedram", "Evangelos Psomiadis", "Dipankar Maity", "Panagiotis Tsiotras"], "summary": "This paper addresses the problem of collaborative navigation in an unknown\nenvironment, where two robots, referred to in the sequel as the Seeker and the\nSupporter, traverse the space simultaneously. The Supporter assists the Seeker\nby transmitting a compressed representation of its local map under bandwidth\nconstraints to support the Seeker's path-planning task. We introduce a bit-rate\nmetric based on the expected binary codeword length to quantify communication\ncost. Using this metric, we formulate the compression design problem as a\nrate-distortion optimization problem that determines when to communicate, which\nregions of the map should be included in the compressed representation, and at\nwhat resolution (i.e., quantization level) they should be encoded. Our\nformulation allows different map regions to be encoded at varying quantization\nlevels based on their relevance to the Seeker's path-planning task. We\ndemonstrate that the resulting optimization problem is convex, and admits a\nclosed-form solution known in the information theory literature as reverse\nwater-filling, enabling efficient, low-computation, and real-time\nimplementation. Additionally, we show that the Seeker can infer the compression\ndecisions of the Supporter independently, requiring only the encoded map\ncontent and not the encoding policy itself to be transmitted, thereby reducing\ncommunication overhead. Simulation results indicate that our method effectively\nconstructs compressed, task-relevant map representations, both in content and\nresolution, that guide the Seeker's planning decisions even under tight\nbandwidth limitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20579v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20579v1", "AI": {"title_translation": "通信感知地图压缩用于在线路径规划：一种速率-失真方法", "tldr": "本文提出了一种基于速率-失真优化的通信感知地图压缩方法，用于在带宽受限的多机器人协作导航中辅助路径规划。", "motivation": "解决未知环境下多机器人协作导航中，带宽受限下支持机器人向寻路机器人传输压缩地图以辅助其路径规划的问题。", "method": "引入基于预期二进制码字长度的比特率度量来量化通信成本，并将压缩设计问题表述为速率-失真优化问题，该问题确定何时通信、哪些地图区域应包含在压缩表示中以及以何种分辨率编码。它允许不同地图区域以不同量化级别编码。该优化问题是凸的，具有信息论中逆向注水法的闭式解。寻路机器人可以独立推断压缩决策，只需传输编码地图内容。", "result": "该方法有效地构建了内容和分辨率上都与任务相关的压缩地图表示，即使在严格的带宽限制下也能指导寻路机器人的规划决策。优化问题是凸的，并具有闭式解，实现了高效、低计算量和实时实现。寻路机器人可以独立推断压缩决策，减少了通信开销。", "conclusion": "提出的速率-失真优化方法能够为协作导航中的在线路径规划有效地生成通信感知的压缩地图，显著减少了带宽需求并保持了规划性能。", "translation": "本文解决了未知环境中的协作导航问题，其中两个机器人（后续称为“寻路者”和“支持者”）同时穿越空间。支持者通过在带宽限制下传输其局部地图的压缩表示来协助寻路者，以支持寻路者的路径规划任务。我们引入了一种基于预期二进制码字长度的比特率度量来量化通信成本。使用此度量，我们将压缩设计问题表述为一个速率-失真优化问题，该问题确定何时通信、地图的哪些区域应包含在压缩表示中以及应以何种分辨率（即量化级别）进行编码。我们的公式允许根据地图区域与寻路者路径规划任务的相关性，以不同的量化级别对不同地图区域进行编码。我们证明了由此产生的优化问题是凸的，并具有信息论文献中称为“逆向注水”的闭式解，从而实现高效、低计算量和实时实现。此外，我们表明寻路者可以独立推断支持者的压缩决策，仅需要传输编码的地图内容，而不需要传输编码策略本身，从而减少了通信开销。仿真结果表明，我们的方法有效地构建了内容和分辨率上都与任务相关的压缩地图表示，即使在严格的带宽限制下也能指导寻路者的规划决策。", "summary": "本文提出一种新颖的通信感知地图压缩方法，用于在带宽受限的多机器人协作导航中辅助在线路径规划。该方法将压缩设计问题建模为速率-失真优化，通过量化通信成本来决定何时、何地以及以何种分辨率压缩地图区域。其凸性保证了高效的实时实现，且寻路者能独立解码，有效降低了通信开销，仿真结果验证了其在严格带宽限制下对路径规划的有效性。", "keywords": "地图压缩, 速率-失真优化, 路径规划, 协作导航, 带宽限制", "comments": "本文的创新点在于将通信感知地图压缩问题转化为速率-失真优化问题，并利用信息论中的逆向注水法找到了闭式解，这使得该方法能够高效实时地应用于实际系统。此外，寻路者能独立推断压缩决策的设计也显著降低了通信开销，对于带宽受限的多机器人系统具有重要意义。"}}
{"id": "2506.20370", "title": "InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking", "authors": ["Abdullah All Tanvir", "Xin Zhong"], "summary": "This paper introduces a novel deep learning framework for robust image\nzero-watermarking based on distortion-invariant feature learning. As a\nzero-watermarking scheme, our method leaves the original image unaltered and\nlearns a reference signature through optimization in the feature space. The\nproposed framework consists of two key modules. In the first module, a feature\nextractor is trained via noise-adversarial learning to generate representations\nthat are both invariant to distortions and semantically expressive. This is\nachieved by combining adversarial supervision against a distortion\ndiscriminator and a reconstruction constraint to retain image content. In the\nsecond module, we design a learning-based multibit zero-watermarking scheme\nwhere the trained invariant features are projected onto a set of trainable\nreference codes optimized to match a target binary message. Extensive\nexperiments on diverse image datasets and a wide range of distortions show that\nour method achieves state-of-the-art robustness in both feature stability and\nwatermark recovery. Comparative evaluations against existing self-supervised\nand deep watermarking techniques further highlight the superiority of our\nframework in generalization and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20370v1", "categories": ["cs.CV", "cs.LG", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20370v1", "AI": {"title_translation": "InvZW：基于噪声对抗训练的鲁棒图像零水印不变特征学习", "tldr": "本文提出了一种名为InvZW的深度学习框架，通过噪声对抗训练学习失真不变特征，实现了鲁棒的图像零水印，并在特征稳定性和水印恢复方面达到了最先进的性能。", "motivation": "为了解决图像零水印在各种失真下鲁棒性不足的问题，需要一种新的方法来学习对失真具有不变性的特征，从而提高水印的恢复能力和整体鲁棒性。", "method": "本文提出了一个名为InvZW的深度学习框架，该框架包含两个核心模块：1. 特征提取器模块：通过噪声对抗学习进行训练，旨在生成既对失真具有不变性又具有语义表达能力的图像表示。这通过结合针对失真判别器的对抗监督和保留图像内容的重建约束来实现。2. 多比特零水印方案模块：设计一个基于学习的多比特零水印方案，将训练好的不变特征投影到一组可训练的参考代码上，这些代码经过优化以匹配目标二进制消息。", "result": "在多样化的图像数据集和广泛的失真条件下进行的实验表明，所提出的方法在特征稳定性和水印恢复方面均达到了最先进的鲁棒性。与现有自监督和深度水印技术的比较评估进一步突出了该框架在泛化能力和鲁棒性方面的优越性。", "conclusion": "InvZW框架通过噪声对抗训练成功学习了对失真不变的特征，显著提升了图像零水印的鲁棒性，并在水印恢复和特征稳定性方面超越了现有技术，为鲁棒图像零水印提供了一个有效且先进的解决方案。", "translation": "本文提出了一种新颖的深度学习框架InvZW，用于基于失真不变特征学习的鲁棒图像零水印。作为一种零水印方案，我们的方法不改变原始图像，并通过特征空间中的优化学习参考签名。所提出的框架包含两个关键模块。在第一个模块中，通过噪声对抗学习训练特征提取器，以生成既对失真不变又具有语义表达能力的表示。这通过结合针对失真判别器的对抗监督和保留图像内容的重建约束来实现。在第二个模块中，我们设计了一种基于学习的多比特零水印方案，其中训练好的不变特征被投影到一组可训练的参考代码上，这些代码经过优化以匹配目标二进制消息。在各种图像数据集和广泛失真下的广泛实验表明，我们的方法在特征稳定性和水印恢复方面均达到了最先进的鲁棒性。与现有自监督和深度水印技术的比较评估进一步突出了我们框架在泛化性和鲁棒性方面的优越性。", "summary": "本文介绍了InvZW，一个用于鲁棒图像零水印的深度学习框架。该框架通过噪声对抗训练学习失真不变特征，包含一个生成不变且语义丰富特征的模块和一个将这些特征映射到可训练参考代码以匹配目标二进制消息的模块。实验证明，InvZW在特征稳定性、水印恢复、泛化性和鲁棒性方面均优于现有方法，达到了最先进的性能。", "keywords": "零水印, 不变特征学习, 噪声对抗训练, 鲁棒性, 深度学习", "comments": "这项研究的创新点在于将噪声对抗学习与不变特征学习相结合，以解决图像零水印在各种失真下的鲁棒性挑战。通过在特征空间中进行优化并引入重建约束，该方法能够在不修改原始图像的前提下，实现高效且鲁棒的水印嵌入和提取。其对特征稳定性和水印恢复能力的强调，为数字版权保护和内容认证提供了更可靠的工具，具有重要的实际应用价值。"}}
{"id": "2506.20065", "title": "Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis", "authors": ["Cristian Minoccheri", "Sophia Tesic", "Kayvan Najarian", "Ryan Stidham"], "summary": "Phenotyping is the process of distinguishing groups of patients to identify\ndifferent types of disease progression. A recent trend employs low-rank matrix\nand tensor factorization methods for their capability of dealing with\nmulti-modal, heterogeneous, and missing data. Symptom quantification is crucial\nfor understanding patient experiences in inflammatory bowel disease, especially\nin conditions such as ulcerative colitis (UC). However, patient-reported\nsymptoms are typically noisy, subjective, and significantly more sparse than\nother data types. For this reason, they are usually not included in phenotyping\nand other machine learning methods. This paper explores the application of\ncomputational phenotyping to leverage Patient-Reported Outcomes (PROs) using a\nnovel supervised coupled matrix-tensor factorization (SCMTF) method, which\nintegrates temporal PROs and temporal labs with static features to predict\nmedication persistence in ulcerative colitis. This is the first tensor-based\nmethod that is both supervised and coupled, it is the first application to the\nUC domain, and the first application to PROs. We use a deep learning framework\nthat makes the model flexible and easy to train. The proposed method allows us\nto handle the large amount of missing data in the PROs. The best model predicts\nchanges in medication 8 and 20 months in the future with AUCs of 0.853 and\n0.803 on the test set respectively. We derive interpretable phenotypes\nconsisting of static features and temporal features (including their temporal\npatterns). We show that low-rank matrix and tensor based phenotyping can be\nsuccessfully applied to the UC domain and to highly missing PRO data. We\nidentify phenotypes useful to predict medication persistence - these phenotypes\ninclude several symptom variables, showing that PROs contain relevant\ninfromation that is usually discarded.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20065v1", "categories": ["cs.LG", "stat.AP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20065v1", "AI": {"title_translation": "监督耦合矩阵-张量分解 (SCMTF) 用于溃疡性结肠炎患者报告结局的计算表型分析", "tldr": "本文提出一种新的监督耦合矩阵-张量分解（SCMTF）方法，首次将患者报告结局（PROs）纳入溃疡性结肠炎的计算表型分析，并成功预测用药依从性，证明PROs的价值。", "motivation": "传统上，患者报告症状 (PROs) 因其噪声大、主观性强、稀疏性高等问题，通常不被纳入疾病表型分析和机器学习方法中。然而，症状量化对于理解炎症性肠病（尤其是溃疡性结肠炎）患者体验至关重要。本文旨在利用PROs进行计算表型分析。", "method": "提出了一种新颖的监督耦合矩阵-张量分解（SCMTF）方法。该方法将时间性PROs和时间性实验室数据与静态特征整合，以预测溃疡性结肠炎的用药依从性。SCMTF是第一个同时具备监督和耦合特性的基于张量的方法，并首次应用于溃疡性结肠炎领域和PROs数据。该方法采用深度学习框架，使其灵活且易于训练，并能处理PROs中大量的缺失数据。", "result": "最优模型在测试集上预测未来8个月和20个月的用药变化时，AUC值分别为0.853和0.803。研究导出了包含静态特征和时间特征（包括其时间模式）的可解释表型。结果表明，低秩矩阵和基于张量的表型分析可以成功应用于溃疡性结肠炎领域和高度缺失的PRO数据。识别出的表型有助于预测用药依从性，并且这些表型包含多个症状变量，证明PROs包含通常被忽略的相关信息。", "conclusion": "低秩矩阵和张量表型分析可以成功应用于溃疡性结肠炎领域和高度缺失的患者报告结局（PRO）数据。患者报告结局（PROs）包含有用的信息，可以用于识别可预测用药依从性的表型，这些信息通常在传统方法中被丢弃。", "translation": "表型分析是区分患者群体以识别不同疾病进展类型的过程。最近的趋势是采用低秩矩阵和张量分解方法，因为它们能够处理多模态、异构和缺失数据。症状量化对于理解炎症性肠病，尤其是溃疡性结肠炎（UC）中的患者体验至关重要。然而，患者报告的症状通常噪声大、主观性强，并且比其他数据类型稀疏得多。因此，它们通常不被纳入表型分析和其他机器学习方法中。本文探索了计算表型分析在利用患者报告结局（PROs）方面的应用，使用了一种新颖的监督耦合矩阵-张量分解（SCMTF）方法，该方法将时间性PROs和时间性实验室数据与静态特征整合，以预测溃疡性结肠炎的用药依从性。这是第一个同时具备监督和耦合特性的基于张量的方法，它是首次应用于UC领域，也是首次应用于PROs。我们使用一个深度学习框架，使模型灵活且易于训练。所提出的方法使我们能够处理PROs中大量的缺失数据。最优模型在测试集上预测未来8个月和20个月的用药变化时，AUC值分别为0.853和0.803。我们导出了由静态特征和时间特征（包括其时间模式）组成的可解释表型。我们证明了低秩矩阵和基于张量的表型分析可以成功应用于UC领域和高度缺失的PRO数据。我们识别出有助于预测用药依从性的表型——这些表型包含多个症状变量，表明PROs包含通常被丢弃的相关信息。", "summary": "本文提出了一种新颖的监督耦合矩阵-张量分解（SCMTF）方法，用于溃疡性结肠炎患者报告结局（PROs）的计算表型分析。该方法首次将时间性PROs与时间性实验室数据及静态特征整合，以预测用药依从性。SCMTF是首个将监督和耦合特性结合的张量方法，并首次应用于UC领域和PROs数据，通过深度学习框架有效处理了PROs中的大量缺失数据。实验结果表明，该方法能成功从高缺失的PRO数据中识别出可解释的表型，并有效预测未来用药变化，证明PROs在疾病表型分析中的重要价值。", "keywords": "溃疡性结肠炎, 患者报告结局, 计算表型分析, 矩阵-张量分解, 用药依从性", "comments": "本文的创新点在于首次提出监督耦合矩阵-张量分解（SCMTF）方法，成功地将通常因数据质量问题而被忽略的患者报告结局（PROs）纳入到计算表型分析中。这是张量方法首次同时实现监督和耦合，并且是首次应用于溃疡性结肠炎领域和PROs数据。其重要性在于证明了PROs的潜在价值，为更全面地理解疾病进展和预测患者依从性提供了新途径，对于个性化医疗和临床决策具有重要意义。"}}
{"id": "2506.19865", "title": "Scalable and Cost-Efficient de Novo Template-Based Molecular Generation", "authors": ["Piotr Gaiński", "Oussama Boussif", "Andrei Rekesh", "Dmytro Shevchuk", "Ali Parviz", "Mike Tyers", "Robert A. Batey", "Michał Koziarski"], "summary": "Template-based molecular generation offers a promising avenue for drug design\nby ensuring generated compounds are synthetically accessible through predefined\nreaction templates and building blocks. In this work, we tackle three core\nchallenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)\nscaling to large building block libraries, and (3) effectively utilizing small\nfragment sets. We propose \\textbf{Recursive Cost Guidance}, a backward policy\nframework that employs auxiliary machine learning models to approximate\nsynthesis cost and viability. This guidance steers generation toward low-cost\nsynthesis pathways, significantly enhancing cost-efficiency, molecular\ndiversity, and quality, especially when paired with an \\textbf{Exploitation\nPenalty} that balances the trade-off between exploration and exploitation. To\nenhance performance in smaller building block libraries, we develop a\n\\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states\nto construct full synthesis trees. Our approach establishes state-of-the-art\nresults in template-based molecular generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19865v1", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.19865v1", "AI": {"title_translation": "可扩展且经济高效的从头模板分子生成", "tldr": "本文提出了一种可扩展且经济高效的模板分子生成方法，通过引入递归成本引导、利用惩罚和动态库机制，解决了合成成本、大型构建块库和小片段集利用等挑战，达到了最先进的性能。", "motivation": "模板分子生成在药物设计中很有前景，但现有的模板基GFlowNets面临三个核心挑战：1) 最大限度地降低合成成本；2) 扩展到大型构建块库；3) 有效利用小片段集。", "method": "提出了“递归成本引导”（Recursive Cost Guidance）这一反向策略框架，使用辅助机器学习模型来近似合成成本和可行性。结合“利用惩罚”（Exploitation Penalty）来平衡探索与利用。此外，还开发了“动态库”（Dynamic Library）机制，通过重用中间高奖励状态来构建完整的合成树，以提高小型构建块库的性能。", "result": "该方法显著提高了成本效率、分子多样性和质量。在模板基分子生成领域取得了最先进的结果。", "conclusion": "通过引入递归成本引导、利用惩罚和动态库机制，可以有效地解决模板基分子生成中的可扩展性、成本效率和库利用率问题，从而实现更优的分子生成。", "translation": "模板分子生成通过确保生成的化合物可通过预定义的反应模板和构建块进行合成，为药物设计提供了一个有前景的途径。在这项工作中，我们解决了基于模板的GFlowNets中的三个核心挑战：(1) 最小化合成成本，(2) 扩展到大型构建块库，以及 (3) 有效利用小片段集。我们提出了“递归成本引导”，这是一个反向策略框架，它采用辅助机器学习模型来近似合成成本和可行性。这种引导将生成导向低成本的合成路径，显著提高了成本效率、分子多样性和质量，尤其是在与平衡探索和利用之间权衡的“利用惩罚”结合使用时。为了提高在较小构建块库中的性能，我们开发了一种“动态库”机制，该机制重用中间高奖励状态来构建完整的合成树。我们的方法在基于模板的分子生成中确立了最先进的结果。", "summary": "本文针对模板基GFlowNets在药物设计中的应用，解决了合成成本高、大型构建块库扩展性差以及小片段集利用不足的问题。研究提出了“递归成本引导”的反向策略框架，通过辅助机器学习模型预测合成成本，并结合“利用惩罚”优化探索与利用的平衡。此外，为应对小型构建块库的挑战，引入了“动态库”机制。这些创新共同提升了分子生成的成本效率、多样性和质量，并达到了该领域的最新水平。", "keywords": "模板分子生成, GFlowNets, 合成成本, 递归成本引导, 动态库", "comments": "这项工作通过引入多项创新机制，如递归成本引导和动态库，有效地解决了模板基分子生成中长期存在的合成成本、可扩展性和片段利用率问题。其创新之处在于将成本优化深度整合到生成过程中，并通过平衡探索与利用来提升生成质量。该研究对于药物设计领域具有重要意义，因为它能生成更具合成可行性和成本效益的化合物。"}}
{"id": "2506.20331", "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content", "authors": ["Rian Touchent", "Nathan Godey", "Eric de la Clergerie"], "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.", "comment": "Dataset link: https://hf.co/datasets/almanach/Biomed-Enriched", "pdf_url": "http://arxiv.org/pdf/2506.20331v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20331v1", "AI": {"title_translation": "Biomed-Enriched：一个通过大型语言模型增强的生物医学数据集，用于预训练和提取稀有与隐藏内容", "tldr": "Biomed-Enriched是一个使用大型语言模型从PubMed构建的生物医学数据集，旨在提供可公开访问的临床文本，并已被证明能有效提升生物医学预训练模型的性能和效率。", "motivation": "由于隐私限制，临床文本通常难以访问。本研究的动机是提供一个大规模、可公开获取的替代方案，即从PubMed构建临床案例集合，以解决生物医学和临床自然语言处理领域对高质量、可访问临床数据的需求。", "method": "该研究引入了Biomed-Enriched数据集，通过两阶段标注过程从PubMed构建。第一阶段，一个大型语言模型标注了40万个PubMed科学文章段落，赋予其类型、领域和教育质量评分。第二阶段，使用这些标注来微调一个小型语言模型，将标签传播到整个PMC-OA语料库，从而提取出包括200万个临床案例段落在内的精炼子集。", "result": "该研究构建了一个包含200万个临床案例段落（其中超过45万个高质量段落）的Biomed-Enriched数据集。初步的持续预训练实验表明，临床上采样使MMLU ProfMed的性能提升了约5%，教育质量过滤使MedQA和MedMCQA的性能提升了约1%。结合这些技术可以实现更快的收敛，用三分之一的训练令牌达到相同的性能。", "conclusion": "Biomed-Enriched数据集提供了一个有价值的、大规模的、可公开访问的临床案例集合，对于生物医学和临床自然语言处理领域具有重要意义。通过使用其策划的子集进行预训练，可以实现有针对性的性能提升和更高效的训练策略。", "translation": "我们引入了Biomed-Enriched，这是一个通过两阶段标注过程从PubMed构建的生物医学文本数据集。在第一阶段，一个大型语言模型对PubMed科学文章中的40万个段落进行标注，为其分配类型（综述、研究、临床病例、其他）、领域（临床、生物医学、其他）和教育质量评分。教育质量评分（1到5分）评估一个段落对大学水平学习的有用程度。然后，这些标注被用于微调一个小型语言模型，该模型将标签传播到完整的PMC-OA语料库。由此产生的元数据使我们能够提取精炼的子集，包括200万个临床病例段落，其中超过45万个高质量段落来自具有商业用途许可的文章，并通过质量过滤和领域上采样构建了几个变体。由于隐私限制，临床文本通常难以访问，因为医院记录无法公开共享。因此，我们的数据集提供了一个替代性的大规模、可公开获取的PubMed临床病例集合，使其成为生物医学和临床NLP的宝贵资源。对OLMo2进行的初步持续预训练实验表明，这些精选的子集能够实现有针对性的改进，其中临床上采样使MMLU ProfMed的性能提升了约5%，教育质量过滤使MedQA和MedMCQA的性能提升了约1%。这些技术的组合导致更快的收敛，用三分之一的训练令牌达到相同的性能，这表明了更高效和有效的生物医学预训练策略的潜力。", "summary": "Biomed-Enriched是一个新颖的生物医学文本数据集，通过大型语言模型对PubMed文章进行两阶段标注构建。它旨在解决临床文本难以访问的问题，提供了一个大规模、可公开获取的临床病例集合。该数据集包含200万个临床病例段落，其中45万个为高质量段落。初步实验证明，使用Biomed-Enriched进行预训练可显著提升生物医学NLP模型的性能，并加速收敛，为更高效的预训练策略提供了可能性。", "keywords": "生物医学数据集, 大型语言模型, 临床文本, 预训练, 数据增强", "comments": "这项工作通过利用大型语言模型进行高质量数据标注，创建了一个独特且急需的生物医学数据集，特别是在临床文本可访问性受限的情况下。其创新之处在于结合LLM的标注能力和小型模型的传播机制，有效扩展了高质量数据。数据集的公开可用性及其在预训练中展示的性能提升和训练效率提升，使其成为生物医学和临床NLP领域的重要贡献，有望推动相关研究和应用的发展。"}}
{"id": "2506.20636", "title": "A Computationally Aware Multi Objective Framework for Camera LiDAR Calibration", "authors": ["Venkat Karramreddy", "Rangarajan Ramanujam"], "summary": "Accurate extrinsic calibration between LiDAR and camera sensors is important\nfor reliable perception in autonomous systems. In this paper, we present a\nnovel multi-objective optimization framework that jointly minimizes the\ngeometric alignment error and computational cost associated with camera-LiDAR\ncalibration. We optimize two objectives: (1) error between projected LiDAR\npoints and ground-truth image edges, and (2) a composite metric for\ncomputational cost reflecting runtime and resource usage. Using the NSGA-II\n\\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space\ndefined by 6-DoF transformations and point sampling rates, yielding a\nwell-characterized Pareto frontier that exposes trade-offs between calibration\nfidelity and resource efficiency. Evaluations are conducted on the KITTI\ndataset using its ground-truth extrinsic parameters for validation, with\nresults verified through both multi-objective and constrained single-objective\nbaselines. Compared to existing gradient-based and learned calibration methods,\nour approach demonstrates interpretable, tunable performance with lower\ndeployment overhead. Pareto-optimal configurations are further analyzed for\nparameter sensitivity and innovation insights. A preference-based\ndecision-making strategy selects solutions from the Pareto knee region to suit\nthe constraints of the embedded system. The robustness of calibration is tested\nacross variable edge-intensity weighting schemes, highlighting optimal balance\npoints. Although real-time deployment on embedded platforms is deferred to\nfuture work, this framework establishes a scalable and transparent method for\ncalibration under realistic misalignment and resource-limited conditions,\ncritical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA\nupdates.", "comment": "16 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.20636v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20636v1", "AI": {"title_translation": "一种计算感知多目标相机激光雷达标定框架", "tldr": "该论文提出了一个多目标优化框架，用于相机-激光雷达标定，旨在同时最小化几何对齐误差和计算成本，并使用NSGA-II算法来探索权衡。", "motivation": "对于自动驾驶系统中的可靠感知，激光雷达和相机传感器之间准确的外部标定至关重要。现有方法可能未充分考虑计算成本。", "method": "本文提出了一种新颖的多目标优化框架，联合最小化：(1) 投影激光雷达点与真实图像边缘之间的误差；(2) 反映运行时和资源使用情况的综合计算成本度量。使用NSGA-II进化算法探索由6自由度变换和点采样率定义的参数空间，以生成帕累托前沿。在KITTI数据集上进行评估，并使用其真实外部参数进行验证。通过基于偏好的决策策略从帕累托膝部区域选择解决方案。", "result": "与现有基于梯度和学习的标定方法相比，该方法展示了可解释、可调的性能和更低的部署开销。它生成了一个特征明确的帕累托前沿，揭示了标定精度和资源效率之间的权衡。标定的鲁棒性在不同的边缘强度加权方案下进行了测试。", "conclusion": "该框架建立了一种在实际未对准和资源受限条件下进行标定的可扩展、透明的方法，这对于长期自主性，特别是接收OTA更新的SAE L3+及以上级别的车辆至关重要。", "translation": "激光雷达和相机传感器之间精确的外部校准对于自动驾驶系统中可靠的感知至关重要。在本文中，我们提出了一种新颖的多目标优化框架，该框架联合最小化了几何对齐误差和与相机-激光雷达校准相关的计算成本。我们优化了两个目标：(1) 投影激光雷达点与真实图像边缘之间的误差，以及 (2) 反映运行时和资源使用的计算成本综合指标。使用NSGA-II进化算法，我们探索了由6自由度变换和点采样率定义的参数空间，得到了一个特征明确的帕累托前沿，揭示了校准保真度与资源效率之间的权衡。评估在KITTI数据集上进行，使用其真实外部参数进行验证，结果通过多目标和受约束的单目标基线进行了验证。与现有基于梯度和学习的校准方法相比，我们的方法展示了可解释、可调的性能和更低的部署开销。帕累托最优配置进一步分析了参数敏感性和创新见解。一种基于偏好的决策策略从帕累托膝部区域选择解决方案，以适应嵌入式系统的约束。校准的鲁棒性在可变边缘强度加权方案下进行了测试，突出了最佳平衡点。尽管在嵌入式平台上进行实时部署将推迟到未来的工作中，但该框架建立了一种在实际未对准和资源受限条件下进行校准的可扩展和透明的方法，这对于长期自主性至关重要，特别是对于接收OTA更新的SAE L3+车辆。", "summary": "本文提出了一种新颖的多目标优化框架，用于相机-激光雷达的外部标定。该框架同时优化了几何对齐误差和计算成本，并采用NSGA-II进化算法来识别帕累托最优解，以平衡标定精度和资源效率。在KITTI数据集上进行评估，该方法与现有技术相比，展现出更优的可解释性、可调性及更低的部署开销，为资源受限的自动驾驶系统提供了可扩展的解决方案。", "keywords": "相机-激光雷达标定, 多目标优化, 计算成本, 帕累托前沿, 自动驾驶系统", "comments": "该论文的创新之处在于采用了多目标优化方法，明确地将计算成本与标定精度一同考虑，这对于自动驾驶车辆的嵌入式系统至关重要。利用NSGA-II算法探索权衡并识别帕累托最优配置是其一大亮点，为部署提供了灵活性。专注于“计算感知”的标定解决了实际应用中常被忽视的挑战。"}}
{"id": "2506.20381", "title": "Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking", "authors": ["Ben Kang", "Xin Chen", "Jie Zhao", "Chunjuan Bo", "Dong Wang", "Huchuan Lu"], "summary": "Transformer-based visual trackers have demonstrated significant advancements\ndue to their powerful modeling capabilities. However, their practicality is\nlimited on resource-constrained devices because of their slow processing\nspeeds. To address this challenge, we present HiT, a novel family of efficient\ntracking models that achieve high performance while maintaining fast operation\nacross various devices. The core innovation of HiT lies in its Bridge Module,\nwhich connects lightweight transformers to the tracking framework, enhancing\nfeature representation quality. Additionally, we introduce a dual-image\nposition encoding approach to effectively encode spatial information. HiT\nachieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson\nAGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,\noutperforming all previous efficient trackers.Building on HiT, we propose\nDyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by\nselecting routes with varying computational requirements. DyHiT uses search\narea features extracted by the backbone network and inputs them into an\nefficient dynamic router to classify tracking scenarios. Based on the\nclassification, DyHiT applies a divide-and-conquer strategy, selecting\nappropriate routes to achieve a superior trade-off between accuracy and speed.\nThe fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while\nmaintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free\nacceleration method based on the dynamic routing architecture of DyHiT. This\nmethod significantly improves the execution speed of various high-performance\ntrackers without sacrificing accuracy. For instance, our acceleration method\nenables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times\nspeedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of\n69.9% on the LaSOT.", "comment": "This paper was accepted by International Journal of Computer\n  Vision(IJCV)", "pdf_url": "http://arxiv.org/pdf/2506.20381v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20381v1", "AI": {"title_translation": "利用轻量级分层ViT和动态框架实现高效视觉跟踪", "tldr": "本文提出了HiT和DyHiT，两种高效的视觉跟踪模型，利用轻量级ViT和动态路由在资源受限设备上实现了高速度和高精度。", "motivation": "现有的基于Transformer的视觉跟踪器性能强大但处理速度慢，在资源受限设备上的实用性受限。", "method": "本文提出了HiT模型，其核心是Bridge Module，连接轻量级Transformer到跟踪框架，增强特征表示质量。此外，引入了双图像位置编码来有效编码空间信息。在此基础上，提出了DyHiT，一个高效的动态跟踪器，通过动态路由选择不同计算需求的路径以适应场景复杂性。DyHiT使用骨干网络提取的搜索区域特征输入到动态路由器进行场景分类，然后应用分而治之策略选择适当路径。此外，还引入了一种基于DyHiT动态路由架构的免训练加速方法，可显著提高其他高性能跟踪器的执行速度。", "result": "HiT在NVIDIA Jetson AGX平台上达到61 fps，在LaSOT基准测试上AUC为64.6%，优于所有现有高效跟踪器。DyHiT的最快版本在NVIDIA Jetson AGX上达到111 fps，在LaSOT上AUC为62.4%。免训练加速方法使最先进的跟踪器SeqTrack-B256在NVIDIA GeForce RTX 2080 Ti GPU上速度提升2.68倍，同时保持69.9%的AUC。", "conclusion": "本文提出的HiT和DyHiT模型，通过轻量级设计和动态适应机制，显著提升了视觉跟踪的效率和实用性，特别是在资源受限设备上，并且其加速方法能有效提升现有高性能跟踪器的速度。", "translation": "基于Transformer的视觉跟踪器因其强大的建模能力而取得了显著进展。然而，由于其处理速度慢，在资源受限设备上的实用性受到限制。为了解决这一挑战，我们提出了HiT，一种新型高效跟踪模型系列，可在各种设备上实现高性能并保持快速运行。HiT的核心创新在于其桥接模块（Bridge Module），它将轻量级Transformer连接到跟踪框架，从而增强特征表示质量。此外，我们引入了一种双图像位置编码方法来有效编码空间信息。HiT在NVIDIA Jetson AGX平台上实现了每秒61帧（fps）的惊人速度，同时在LaSOT基准测试上取得了64.6%的竞争力AUC，超越了所有以前的高效跟踪器。\n在HiT的基础上，我们提出了DyHiT，一种高效的动态跟踪器，通过选择具有不同计算需求的路径来灵活适应场景复杂性。DyHiT使用骨干网络提取的搜索区域特征，并将其输入到高效的动态路由器中以对跟踪场景进行分类。根据分类结果，DyHiT应用分而治之的策略，选择适当的路径以实现精度和速度之间的卓越权衡。DyHiT的最快版本在NVIDIA Jetson AGX上达到111 fps，同时在LaSOT上保持62.4%的AUC。\n此外，我们引入了一种基于DyHiT动态路由架构的免训练加速方法。该方法在不牺牲精度的情况下显著提高了各种高性能跟踪器的执行速度。例如，我们的加速方法使最先进的跟踪器SeqTrack-B256在NVIDIA GeForce RTX 2080 Ti GPU上实现了2.68倍的速度提升，同时保持了LaSOT上69.9%的相同AUC。", "summary": "本文针对Transformer视觉跟踪器在资源受限设备上速度慢的问题，提出了HiT和DyHiT两种高效模型。HiT通过轻量级分层ViT和桥接模块提升特征表示和速度；DyHiT在此基础上引入动态路由，根据场景复杂度自适应选择计算路径，进一步优化速度与精度。此外，还提出了一种免训练加速方法，可显著提升现有高性能跟踪器的运行速度。实验证明，HiT和DyHiT在嵌入式设备上表现出优异的实时性能和竞争力，且加速方法能有效提升SOTA模型的速度。", "keywords": "视觉跟踪, Transformer, 轻量级ViT, 动态路由, 实时性能", "comments": "本文通过引入轻量级Transformer、创新的桥接模块和动态路由机制，有效解决了Transformer视觉跟踪器在资源受限设备上的效率瓶颈。DyHiT的动态适应能力和提出的免训练加速方法是其重要创新点，极大地拓展了高性能跟踪器的应用范围和实用性。"}}
{"id": "2506.20090", "title": "A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression", "authors": ["Ainaz Jamshidi", "Dongchan Kim", "Muhammad Arif"], "summary": "Predictive maintenance (PdM) has become a crucial element of modern\nindustrial practice. PdM plays a significant role in operational dependability\nand cost management by decreasing unforeseen downtime and optimizing asset life\ncycle management. Machine learning and deep learning have enabled more precise\nforecasts of equipment failure and remaining useful life (RUL). Although many\nstudies have been conducted on PdM, there has not yet been a standalone\ncomparative study between regression- and classification-based approaches. In\nthis review, we look across a range of PdM methodologies, while focusing more\nstrongly on the comparative use of classification and regression methods in\nprognostics. While regression-based methods typically provide estimates of RUL,\nclassification-based methods present a forecast of the probability of failure\nacross defined time intervals. Through a comprehensive analysis of recent\nliterature, we highlight key advancements, challenges-such as data imbalance\nand high-dimensional feature spaces-and emerging trends, including hybrid\napproaches and AI-enabled prognostic systems. This review aims to provide\nresearchers and practitioners with an awareness of the strengths and\ncompromises of various PdM methods and to help identify future research and\nbuild more robust, directed adaptive maintenance systems. Future work may\ninclude a systematic review of practical aspects such as public datasets,\nbenchmarking platforms, and open-source tools to support the advancement of PdM\nresearch.", "comment": "13 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.20090v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20090v1", "AI": {"title_translation": "预测性维护方法综述：通过分类和回归进行预测的分析", "tldr": "本综述首次对预测性维护（PdM）中基于回归和分类的预测方法进行了比较分析，旨在帮助研究人员和从业者了解不同方法的优缺点，并识别未来的研究方向。", "motivation": "尽管预测性维护（PdM）已成为现代工业实践的关键要素，并且机器学习和深度学习显著提升了设备故障和剩余使用寿命（RUL）的预测精度，但目前尚未有独立的比较研究专门分析基于回归和分类的方法在预测性维护中的应用。", "method": "本综述通过对近期文献的全面分析，审查了一系列预测性维护方法，并特别侧重于比较分类和回归方法在故障预测中的应用。", "result": "研究结果强调了预测性维护领域的关键进展、挑战（如数据不平衡和高维特征空间）以及新兴趋势（包括混合方法和AI驱动的预测系统）。回归方法通常提供RUL估计，而分类方法则预测在特定时间间隔内发生故障的概率。", "conclusion": "本综述旨在让研究人员和从业者了解各种预测性维护方法的优势和局限性，并帮助他们识别未来的研究方向，以构建更健壮、更有针对性的自适应维护系统。", "translation": "预测性维护（PdM）已成为现代工业实践的关键要素。通过减少意外停机时间和优化资产生命周期管理，PdM在运营可靠性和成本管理中发挥着重要作用。机器学习和深度学习使得设备故障和剩余使用寿命（RUL）的预测更加精确。尽管已经对PdM进行了许多研究，但目前尚未有独立的基于回归和分类方法的比较研究。在本综述中，我们考察了一系列PdM方法，同时更侧重于分类和回归方法在预测中的比较使用。基于回归的方法通常提供RUL的估计，而基于分类的方法则预测在定义的时间间隔内发生故障的概率。通过对近期文献的全面分析，我们强调了关键进展、挑战（如数据不平衡和高维特征空间）以及新兴趋势，包括混合方法和AI驱动的预测系统。本综述旨在为研究人员和从业者提供对各种PdM方法的优缺点认识，并帮助识别未来的研究方向，以构建更健壮、更有针对性的自适应维护系统。未来的工作可能包括对实际方面进行系统审查，例如公共数据集、基准测试平台和开源工具，以支持PdM研究的进展。", "summary": "本综述首次全面比较了预测性维护（PdM）中基于分类和回归的预测方法。文章详细分析了这两种方法在设备故障预测和剩余使用寿命（RUL）估计中的应用，并探讨了PdM领域的最新进展、面临的挑战（如数据不平衡、高维特征空间）以及混合方法和AI驱动系统等新兴趋势。该研究旨在为研究人员和从业者提供对不同PdM方法优缺点的深入理解，从而指导未来研究并促进更鲁棒的自适应维护系统的开发。", "keywords": "预测性维护, 故障预测, 分类, 回归, 机器学习", "comments": "该论文填补了预测性维护领域中一个重要的空白，即对基于分类和回归的预测方法进行系统的比较分析。其创新之处在于提供了这两种主流机器学习方法在PdM应用中的详细对比，并指出了各自的优势和局限性。这对于选择合适的预测策略具有重要指导意义，尤其是在处理RUL估计和故障概率预测等不同任务时。文章还强调了数据挑战和新兴趋势，为未来的研究方向提供了宝贵的见解，对于推动PdM的理论发展和实际应用都具有重要价值。"}}
{"id": "2506.20187", "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV Management on a Single Commodity GPU", "authors": ["He Sun", "Li Li", "Mingjun Xiao", "Chengzhong Xu"], "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.", "comment": "15 pages, 23 figures", "pdf_url": "http://arxiv.org/pdf/2506.20187v1", "categories": ["cs.OS", "cs.CR", "68M20", "C.4"], "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.20187v1", "AI": {"title_translation": "突破长上下文LLM推理的界限：单商品级GPU上的自适应KV管理", "tldr": "针对商品级GPU上长上下文LLM推理的KV缓存内存瓶颈，本文提出了LeoAM系统，通过自适应KV管理、轻量级KV摘要和动态压缩等技术，实现了显著的推理加速。", "motivation": "在商品级GPU上进行长上下文LLM推理时，由于KV缓存的内存需求不断增加，现有系统需要将KV数据卸载到磁盘，但此过程受限于令牌重要性评估开销和磁盘低带宽，导致性能挑战。", "method": "本文提出了LeoAM系统，这是一个用于单个商品级GPU的高效、重要性感知长上下文LLM推理系统，具有自适应分层GPU-CPU-磁盘KV管理。它采用自适应KV管理策略，根据不同层注意力权重的偏斜分布将KV数据划分为可变大小的块，以减少计算和传输开销。此外，LeoAM提出了一种轻量级KV抽象方法，通过在磁盘上存储和提取每个块的KV抽象而不是完整的KV数据来最小化传输延迟。系统还利用动态压缩和流水线技术进一步加速推理。", "result": "实验结果表明，LongInfer实现了平均3.46倍的推理延迟加速，同时保持了可比的LLM响应质量。在更大批量大小的场景中，它实现了高达5.47倍的加速。", "conclusion": "LeoAM系统通过其创新的自适应KV管理和轻量级KV抽象方法，成功解决了商品级GPU上长上下文LLM推理的内存瓶颈和性能挑战，显著提高了推理速度，同时保持了模型质量。", "translation": "先进的大型语言模型（LLM）在各种复杂和长上下文的自然语言任务中取得了令人印象深刻的性能。然而，由于键值（KV）缓存不断增长的内存需求，在商品级GPU（个人电脑）上本地进行长上下文LLM推理，并顾及隐私问题，仍然具有挑战性。现有系统通常识别重要令牌并选择性地将其KV数据卸载到GPU和CPU内存。由于商品级GPU内存有限，KV数据需要卸载到磁盘，但此过程受到令牌重要性评估开销和磁盘低带宽的瓶颈。在本文中，我们提出了LeoAM，这是第一个针对单个商品级GPU的高效、重要性感知长上下文LLM推理系统，具有自适应分层GPU-CPU-磁盘KV管理。我们的系统采用自适应KV管理策略，根据不同层注意力权重的偏斜分布将KV数据划分为可变大小的块，以减少计算和额外的传输开销。此外，我们提出了一种轻量级KV抽象方法，通过在磁盘上存储和提取每个块的KV抽象而不是完整的KV数据来最小化传输延迟。LeoAM还利用动态压缩和流水线技术进一步加速推理。实验结果表明，LongInfer实现了平均3.46倍的推理延迟加速，同时保持了可比的LLM响应质量。在更大批量大小的场景中，它实现了高达5.47倍的加速。", "summary": "本文提出了LeoAM系统，旨在解决商品级GPU上长上下文LLM推理中KV缓存的内存瓶颈和低带宽问题。LeoAM通过自适应分层GPU-CPU-磁盘KV管理、基于注意力权重分布的可变大小KV块划分、轻量级KV抽象方法以及动态压缩和流水线技术，显著减少了计算和传输开销。实验证明，该系统在推理延迟上实现了高达5.47倍的加速，同时保持了LLM的响应质量。", "keywords": "长上下文LLM推理, KV缓存管理, 商品级GPU, LeoAM, 内存优化", "comments": "本文提出的LeoAM系统在解决商品级GPU上长上下文LLM推理的内存限制方面具有显著创新。其核心贡献在于引入了自适应分层KV管理和轻量级KV抽象方法，有效地缓解了磁盘I/O瓶颈和内存压力。这对于在资源有限的设备上部署大型LLM具有重要意义，尤其是在关注隐私的本地推理场景。该研究为优化LLM在边缘设备的部署提供了新的思路。"}}
{"id": "2506.20409", "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging", "authors": ["Ekaterina Taktasheva", "Jeff Dalton"], "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce \\name, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20409v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20409v1", "AI": {"title_translation": "TAPS：通过结构化标签实现工具增强的个性化", "tldr": "TAPS通过结构化标签和不确定性工具检测器，显著提升了大型语言模型在工具使用中的个性化能力，并在NLSI任务上达到了开源模型的最新水平。", "motivation": "现有工具增强型大型语言模型忽略了个性化在指导工具使用中的作用，并且大型语言模型在个性化工具使用能力方面存在关键弱点。", "method": "本研究引入了一个名为TAPS的新颖解决方案，该方案通过利用结构化标签工具和基于不确定性的工具检测器来增强个性化工具使用。", "result": "TAPS显著提高了大型语言模型整合用户偏好的能力，并在NLSI任务上为开源模型实现了新的最先进水平。", "conclusion": "TAPS通过其新颖的方法，成功解决了大型语言模型在工具使用中个性化不足的问题，并取得了显著的性能提升。", "translation": "工具增强型大型语言模型的最新进展使其能够与外部工具交互，从而增强了它们执行复杂用户任务的能力。然而，现有方法忽略了个性化在指导工具使用中的作用。这项工作研究了如何将用户偏好有效地整合到面向目标的对话代理中。通过广泛分析，我们识别出大型语言模型在个性化工具使用能力方面存在的关键弱点。为此，我们引入了TAPS，这是一种新颖的解决方案，它通过利用结构化标签工具和基于不确定性的工具检测器来增强个性化工具使用。TAPS显著提高了大型语言模型整合用户偏好的能力，并在NLSI任务上为开源模型实现了新的最先进水平。", "summary": "本研究解决了工具增强型大型语言模型在个性化工具使用方面的不足，引入了TAPS。TAPS通过结合结构化标签工具和不确定性工具检测器，显著提升了大型语言模型整合用户偏好的能力，并在NLSI任务上为开源模型达到了最先进的性能。", "keywords": "工具增强型LLM, 个性化, 结构化标签, 工具检测器, NLSI", "comments": "TAPS的创新之处在于其将结构化标签和不确定性检测器结合起来，有效解决了大型语言模型在工具使用中个性化不足的问题。这对于提升AI在复杂任务中理解和适应用户需求的能力具有重要意义，尤其是在对话代理和个性化推荐等领域。其在NLSI任务上取得SOTA成果，证明了方法的有效性。"}}
{"id": "2506.20668", "title": "DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy", "authors": ["Sungjae Park", "Homanga Bharadhwaj", "Shubham Tulsiani"], "summary": "We propose DemoDiffusion, a simple and scalable method for enabling robots to\nperform manipulation tasks in natural environments by imitating a single human\ndemonstration. Our approach is based on two key insights. First, the hand\nmotion in a human demonstration provides a useful prior for the robot's\nend-effector trajectory, which we can convert into a rough open-loop robot\nmotion trajectory via kinematic retargeting. Second, while this retargeted\nmotion captures the overall structure of the task, it may not align well with\nplausible robot actions in-context. To address this, we leverage a pre-trained\ngeneralist diffusion policy to modify the trajectory, ensuring it both follows\nthe human motion and remains within the distribution of plausible robot\nactions. Our approach avoids the need for online reinforcement learning or\npaired human-robot data, enabling robust adaptation to new tasks and scenes\nwith minimal manual effort. Experiments in both simulation and real-world\nsettings show that DemoDiffusion outperforms both the base policy and the\nretargeted trajectory, enabling the robot to succeed even on tasks where the\npre-trained generalist policy fails entirely. Project page:\nhttps://demodiffusion.github.io/", "comment": "Preprint(17 pages). Under Review", "pdf_url": "http://arxiv.org/pdf/2506.20668v1", "categories": ["cs.RO", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20668v1", "AI": {"title_translation": "DemoDiffusion：使用预训练扩散策略的单次人体模仿", "tldr": "DemoDiffusion提出了一种简单可扩展的方法，通过模仿单次人类演示，使机器人能够执行操作任务，无需在线强化学习或配对数据。", "motivation": "使机器人能够通过模仿单次人类演示在自然环境中执行操作任务，并避免对在线强化学习或配对人机数据的需求。", "method": "DemoDiffusion方法基于两个关键见解：首先，将人类演示中的手部运动通过运动学重定向转换为粗略的开环机器人运动轨迹。其次，利用预训练的通用扩散策略修改该轨迹，以确保其既遵循人类运动又符合机器人可行动作的分布。", "result": "实验表明，DemoDiffusion在仿真和真实世界环境中均优于基础策略和重定向轨迹，使机器人甚至在预训练通用策略完全失败的任务上也能成功。", "conclusion": "DemoDiffusion通过结合运动学重定向和预训练扩散策略，实现了仅通过单次人类演示即可使机器人稳健适应新任务和场景，且所需手动工作量极小。", "translation": "我们提出了 DemoDiffusion，这是一种简单且可扩展的方法，能够通过模仿单次人类演示，使机器人在自然环境中执行操作任务。我们的方法基于两个关键见解。首先，人类演示中的手部运动为机器人末端执行器轨迹提供了有用的先验信息，我们可以通过运动学重定向将其转换为粗略的开环机器人运动轨迹。其次，虽然这种重定向的运动捕捉了任务的整体结构，但它可能与上下文中可行的机器人动作不完全一致。为了解决这个问题，我们利用预训练的通用扩散策略来修改轨迹，确保它既遵循人类运动又保持在可行机器人动作的分布范围内。我们的方法避免了在线强化学习或配对人机数据的需求，从而能够以最少的手动工作量稳健地适应新任务和新场景。在仿真和真实世界环境中的实验表明，DemoDiffusion 优于基础策略和重定向轨迹，即使在预训练通用策略完全失败的任务上，也能使机器人成功。项目页面：https://demodiffusion.github.io/", "summary": "DemoDiffusion 提出了一种新颖的方法，使机器人能够通过单次人类演示进行操作任务模仿。该方法首先通过运动学重定向将人类手部运动转换为初步的机器人轨迹，然后利用预训练的通用扩散策略对轨迹进行精修，使其既符合人类意图又在机器人可行动作范围内。该方法无需在线强化学习或大量配对数据，在模拟和真实世界中均表现出优越性，甚至能解决预训练策略失败的任务。", "keywords": "机器人模仿, 扩散策略, 单次学习, 运动学重定向, 机器人操作", "comments": "DemoDiffusion的创新之处在于其将运动学重定向与预训练通用扩散策略相结合，实现了高效的单次人类模仿。这种方法显著降低了对大量训练数据和复杂在线学习的需求，使其在实际应用中具有很高的可扩展性和鲁棒性。其能够在通用策略失败的任务上取得成功，进一步凸显了其方法的有效性和实用价值。"}}
{"id": "2506.20388", "title": "A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management", "authors": ["Shen Tan", "Xin Zhang", "Liangxiu Han", "Huaguo Huang", "Han Wang"], "summary": "Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)\nis crucial for supporting local livelihoods and carbon sequestration\ninitiatives like the China Certified Emission Reduction (CCER) program.\nHigh-resolution canopy height maps (CHMs) are essential for this, but standard\nlidar-based methods are expensive. While deep learning with RGB imagery offers\nan alternative, accurately extracting canopy height features remains\nchallenging. To address this, we developed a novel model for high-resolution\nCHM generation using a Large Vision Foundation Model (LVFM). Our model\nintegrates a feature extractor, a self-supervised feature enhancement module to\npreserve spatial details, and a height estimator. Tested in Beijing's Fangshan\nDistrict using 1-meter Google Earth imagery, our model outperformed existing\nmethods, including conventional CNNs. It achieved a mean absolute error of 0.09\nm, a root mean square error of 0.24 m, and a correlation of 0.78 against\nlidar-based CHMs. The resulting CHMs enabled over 90% success in individual\ntree detection, high accuracy in AGB estimation, and effective tracking of\nplantation growth, demonstrating strong generalization to non-training areas.\nThis approach presents a promising, scalable tool for evaluating carbon\nsequestration in both plantations and natural forests.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20388v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20388v1", "AI": {"title_translation": "一种基于新型大视觉基础模型（LVFM）的人工林高分辨率冠层高度图生成方法，用于精准林业管理", "tldr": "一种基于新型大视觉基础模型（LVFM）的方法，利用RGB图像生成高分辨率冠层高度图，优于现有方法，可实现人工林中经济高效的地上生物量监测和碳汇评估。", "motivation": "人工林地上生物量（AGB）的准确、经济高效监测对于支持当地生计和碳汇倡议至关重要。高分辨率冠层高度图（CHM）对此至关重要，但标准激光雷达方法成本高昂。虽然基于RGB图像的深度学习提供了一种替代方案，但准确提取冠层高度特征仍然具有挑战性。", "method": "开发了一种使用大视觉基础模型（LVFM）生成高分辨率冠层高度图的新型模型。该模型集成了特征提取器、自监督特征增强模块（用于保留空间细节）和高度估计器。在北京房山区使用1米Google Earth图像进行测试。", "result": "该模型优于包括传统CNN在内的现有方法。与基于激光雷达的冠层高度图相比，其平均绝对误差为0.09米，均方根误差为0.24米，相关性为0.78。生成的冠层高度图实现了90%以上的单木检测成功率，地上生物量估算精度高，并能有效跟踪人工林生长，表现出对非训练区域的强大泛化能力。", "conclusion": "这种方法为评估人工林和天然林中的碳汇提供了一种有前景、可扩展的工具。", "translation": "人工林地上生物量（AGB）的准确、经济高效监测对于支持当地生计和碳汇倡议，如中国核证自愿减排量（CCER）计划至关重要。高分辨率冠层高度图（CHM）对此至关重要，但标准激光雷达方法成本高昂。虽然基于RGB图像的深度学习提供了一种替代方案，但准确提取冠层高度特征仍然具有挑战性。为了解决这个问题，我们开发了一种新型模型，使用大视觉基础模型（LVFM）生成高分辨率冠层高度图。我们的模型集成了特征提取器、一个用于保留空间细节的自监督特征增强模块，以及一个高度估计器。在北京房山区使用1米Google Earth图像进行测试，我们的模型优于包括传统CNN在内的现有方法。与基于激光雷达的冠层高度图相比，其平均绝对误差为0.09米，均方根误差为0.24米，相关性为0.78。生成的冠层高度图实现了90%以上的单木检测成功率，地上生物量估算精度高，并能有效跟踪人工林生长，表现出对非训练区域的强大泛化能力。这种方法为评估人工林和天然林中的碳汇提供了一种有前景、可扩展的工具。", "summary": "本文提出了一种基于新型大视觉基础模型（LVFM）的方法，用于从RGB图像生成高分辨率冠层高度图（CHM）。针对激光雷达成本高昂和传统深度学习面临的挑战，所提出的模型集成了特征提取器、自监督增强模块和高度估计器。该模型使用Google Earth图像进行评估，性能优于现有方法，与激光雷达数据相比，取得了卓越的精度（MAE 0.09m，RMSE 0.24m，相关性0.78）。生成的CHM显著提高了单木检测、地上生物量估算和生长跟踪的准确性，并表现出强大的泛化能力。这种可扩展的方法为精准林业和碳汇监测提供了一种经济高效的解决方案。", "keywords": "大视觉基础模型, 冠层高度图, 精准林业, 地上生物量, 碳汇", "comments": "该创新之处在于利用大视觉基础模型（LVFM）进行高分辨率冠层高度测绘，特别是结合了自监督特征增强模块以保留空间细节，这对于从RGB图像准确估计高度至关重要。这种方法为传统激光雷达方法提供了一种经济高效且可扩展的替代方案，通过利用现成的卫星图像实现准确的地上生物量估算和单木检测，显著推动了精准林业管理和碳汇监测。"}}
{"id": "2506.20094", "title": "MEL: Multi-level Ensemble Learning for Resource-Constrained Environments", "authors": ["Krishna Praneet Gudipaty", "Walid A. Hanafy", "Kaan Ozkara", "Qianlin Liang", "Jesse Milzman", "Prashant Shenoy", "Suhas Diggavi"], "summary": "AI inference at the edge is becoming increasingly common for low-latency\nservices. However, edge environments are power- and resource-constrained, and\nsusceptible to failures. Conventional failure resilience approaches, such as\ncloud failover or compressed backups, often compromise latency or accuracy,\nlimiting their effectiveness for critical edge inference services. In this\npaper, we propose Multi-Level Ensemble Learning (MEL), a new framework for\nresilient edge inference that simultaneously trains multiple lightweight backup\nmodels capable of operating collaboratively, refining each other when multiple\nservers are available, and independently under failures while maintaining good\naccuracy. Specifically, we formulate our approach as a multi-objective\noptimization problem with a loss formulation that inherently encourages\ndiversity among individual models to promote mutually refining representations,\nwhile ensuring each model maintains good standalone performance. Empirical\nevaluations across vision, language, and audio datasets show that MEL provides\nperformance comparable to original architectures while also providing fault\ntolerance and deployment flexibility across edge platforms. Our results show\nthat our ensemble model, sized at 40\\% of the original model, achieves similar\nperformance, while preserving 95.6\\% of ensemble accuracy in the case of\nfailures when trained using MEL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20094v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20094v1", "AI": {"title_translation": "MEL：面向资源受限环境的多级集成学习", "tldr": "MEL是一种新的多级集成学习框架，用于在资源受限的边缘环境中实现弹性AI推理，通过训练多个轻量级备份模型在故障时保持高精度和容错性。", "motivation": "边缘AI推理在低延迟服务中越来越普遍，但边缘环境受限于功耗和资源，且易发生故障。传统的故障恢复方法（如云故障转移或压缩备份）通常会牺牲延迟或准确性，限制了它们对关键边缘推理服务的有效性。", "method": "本文提出了多级集成学习（MEL），一个用于弹性边缘推理的新框架。它同时训练多个轻量级备份模型，这些模型能够在多个服务器可用时协同操作，相互改进，并在故障时独立运行，同时保持良好的准确性。该方法被表述为一个多目标优化问题，其损失函数本质上鼓励各个模型之间的多样性，以促进相互改进的表示，同时确保每个模型都保持良好的独立性能。", "result": "对视觉、语言和音频数据集的实证评估表明，MEL 提供与原始架构相当的性能，同时还提供跨边缘平台的容错性和部署灵活性。实验结果显示，其集成模型大小为原始模型的40%，实现了相似的性能，并且在使用MEL训练时，在发生故障的情况下保留了95.6%的集成准确性。", "conclusion": "MEL有效解决了资源受限边缘AI推理中的弹性挑战，通过降低资源占用同时保持高精度和容错能力。", "translation": "边缘AI推理在低延迟服务中变得越来越普遍。然而，边缘环境受限于功耗和资源，并且容易发生故障。传统的故障恢复方法，例如云故障转移或压缩备份，通常会牺牲延迟或准确性，从而限制了它们对关键边缘推理服务的有效性。在本文中，我们提出了多级集成学习（MEL），这是一种用于弹性边缘推理的新框架，它同时训练多个轻量级备份模型，这些模型能够在多个服务器可用时协同操作，相互改进，并在故障时独立运行，同时保持良好的准确性。具体来说，我们将我们的方法表述为一个多目标优化问题，其损失函数本质上鼓励各个模型之间的多样性，以促进相互改进的表示，同时确保每个模型都保持良好的独立性能。对视觉、语言和音频数据集的实证评估表明，MEL 提供与原始架构相当的性能，同时还提供跨边缘平台的容错性和部署灵活性。我们的结果表明，我们的集成模型，其大小为原始模型的40%，实现了相似的性能，同时在使用MEL训练时，在发生故障的情况下保留了95.6%的集成准确性。", "summary": "本文提出了多级集成学习（MEL），一个针对资源受限边缘环境的弹性AI推理框架。MEL通过训练多个轻量级备份模型，使其能够在多服务器协同或单服务器故障下独立运行，同时保持高精度。该方法被公式化为多目标优化问题，鼓励模型多样性并确保独立性能。实验证明，MEL在视觉、语言和音频任务上表现与原始架构相当，并显著提升了容错性和部署灵活性，其集成模型在仅为原始模型40%大小的情况下，故障时仍能保持95.6%的集成精度。", "keywords": "边缘AI推理, 集成学习, 容错性, 资源受限, 多目标优化", "comments": "MEL的创新之处在于其多级集成学习方法，特别是在资源受限的边缘环境中实现高精度和容错性。通过训练轻量级备份模型并采用多目标优化，它解决了传统故障恢复方案的局限性。其在模型大小和故障时精度保持上的具体量化结果，凸显了其实用价值和重要性。"}}
{"id": "2506.20413", "title": "Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning", "authors": ["Mohammad Mahdi Maheri", "Denys Herasymuk", "Hamed Haddadi"], "summary": "The growing adoption of Artificial Intelligence (AI) in Internet of Things\n(IoT) ecosystems has intensified the need for personalized learning methods\nthat can operate efficiently and privately across heterogeneous,\nresource-constrained devices. However, enabling effective personalized learning\nin decentralized settings introduces several challenges, including efficient\nknowledge transfer between clients, protection of data privacy, and resilience\nagainst poisoning attacks. In this paper, we address these challenges by\ndeveloping P4 (Personalized, Private, Peer-to-Peer) -- a method designed to\ndeliver personalized models for resource-constrained IoT devices while ensuring\ndifferential privacy and robustness against poisoning attacks. Our solution\nemploys a lightweight, fully decentralized algorithm to privately detect client\nsimilarity and form collaborative groups. Within each group, clients leverage\ndifferentially private knowledge distillation to co-train their models,\nmaintaining high accuracy while ensuring robustness to the presence of\nmalicious clients. We evaluate P4 on popular benchmark datasets using both\nlinear and CNN-based architectures across various heterogeneity settings and\nattack scenarios. Experimental results show that P4 achieves 5% to 30% higher\naccuracy than leading differentially private peer-to-peer approaches and\nmaintains robustness with up to 30% malicious clients. Additionally, we\ndemonstrate its practicality by deploying it on resource-constrained devices,\nwhere collaborative training between two clients adds only ~7 seconds of\noverhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20413v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20413v1", "AI": {"title_translation": "客户端聚类与知识共享：增强个性化点对点学习中的隐私和鲁棒性", "tldr": "P4是一种针对资源受限IoT设备的个性化、私密、点对点学习方法，通过客户端聚类和差分隐私知识蒸馏，在保证隐私和抵御投毒攻击的同时，提高了准确性和鲁棒性。", "motivation": "在物联网（IoT）生态系统中，人工智能（AI）的日益普及使得对能在异构、资源受限设备上高效私密运行的个性化学习方法的需求日益增长。然而，在去中心化环境中实现有效的个性化学习面临挑战，包括客户端间高效知识传输、数据隐私保护以及抵御投毒攻击的弹性。", "method": "本文提出了P4（个性化、私密、点对点）方法。P4采用轻量级、完全去中心化的算法，私密地检测客户端相似性并形成协作组。在每个组内，客户端利用差分隐私知识蒸馏共同训练模型，在保持高准确性的同时，确保对恶意客户端的鲁棒性。", "result": "实验结果表明，P4比领先的差分隐私点对点方法准确率高5%到30%，并且在多达30%的恶意客户端存在下仍能保持鲁棒性。此外，将其部署在资源受限设备上，两个客户端之间的协作训练仅增加约7秒的开销，证明了其实用性。", "conclusion": "P4成功解决了在资源受限的去中心化IoT环境中实现个性化学习的挑战，通过结合客户端聚类和差分隐私知识蒸馏，显著提高了学习的隐私性、鲁棒性和准确性，且具有实际部署的可行性。", "translation": "人工智能（AI）在物联网（IoT）生态系统中的日益普及，加剧了对能够在异构、资源受限设备上高效私密运行的个性化学习方法的需求。然而，在去中心化环境中实现有效的个性化学习带来了若干挑战，包括客户端之间高效的知识传输、数据隐私保护以及抵御投毒攻击的弹性。在本文中，我们通过开发P4（个性化、私密、点对点）——一种旨在为资源受限的IoT设备提供个性化模型，同时确保差分隐私和抵御投毒攻击鲁棒性的方法——来应对这些挑战。我们的解决方案采用了一种轻量级、完全去中心化的算法，以私密方式检测客户端相似性并形成协作组。在每个组内，客户端利用差分隐私知识蒸馏共同训练模型，在保持高准确性的同时，确保对恶意客户端存在的鲁棒性。我们使用流行的基准数据集，在各种异构设置和攻击场景下，使用线性和基于CNN的架构评估了P4。实验结果表明，P4比领先的差分隐私点对点方法准确率高5%到30%，并且在多达30%的恶意客户端存在下仍能保持鲁棒性。此外，我们通过将其部署在资源受限设备上，证明了其实用性，其中两个客户端之间的协作训练仅增加了约7秒的开销。", "summary": "本文提出了一种名为P4（个性化、私密、点对点）的新型方法，旨在解决物联网（IoT）生态系统中个性化学习所面临的挑战，特别是在资源受限的去中心化设备上。P4通过轻量级、去中心化的算法私密地进行客户端聚类，并利用差分隐私知识蒸馏在协作组内进行模型共训练。实验证明，P4在准确性方面优于现有差分隐私点对点方法5%至30%，并能在多达30%恶意客户端的情况下保持鲁棒性，同时在实际设备上展现出极低的额外开销。", "keywords": "个性化学习, 点对点学习, 差分隐私, 客户端聚类, 鲁棒性", "comments": "P4的创新点在于将客户端聚类与差分隐私知识蒸馏相结合，以在去中心化、资源受限的IoT环境中实现个性化、私密且鲁棒的学习。其完全去中心化的设计和对恶意客户端的强大抵御能力，以及在实际设备上的低开销，使其在保护隐私和应对攻击方面具有重要意义和应用潜力。"}}
{"id": "2506.20430", "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning", "authors": ["Weike Zhao", "Chaoyi Wu", "Yanjie Fan", "Xiaoman Zhang", "Pengcheng Qiu", "Yuze Sun", "Xiao Zhou", "Yanfeng Wang", "Ya Zhang", "Yongguo Yu", "Kun Sun", "Weidi Xie"], "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20430v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20430v1", "AI": {"title_translation": "一种具有可追溯推理能力的罕见病诊断智能系统", "tldr": "DeepRare是一个基于LLM的智能系统，用于罕见病诊断，通过整合多种工具和医学知识，提供可追溯的诊断假设，并在多个数据集上表现出卓越的诊断性能，显著优于现有方法。", "motivation": "罕见病在全球影响超过3亿人，但由于其临床异质性、低个体患病率以及大多数临床医生对此类疾病不熟悉，导致及时准确的诊断仍然是一个普遍的挑战。", "method": "本文介绍了DeepRare，首个由大型语言模型（LLM）驱动的罕见病诊断智能系统，能够处理异构临床输入。系统生成罕见病的诊断假设排名，每个假设都附有透明的推理链，将中间分析步骤与可验证的医学证据联系起来。DeepRare包含三个关键组件：一个带有长期记忆模块的中央主机；以及负责特定领域分析任务的专业代理服务器，这些服务器集成了40多种专业工具和网络规模的最新医学知识来源。", "result": "DeepRare在八个数据集上进行了评估，在2,919种疾病中表现出卓越的诊断性能，其中1013种疾病达到了100%的准确率。在基于HPO的评估中，DeepRare显著优于其他15种方法（如传统生物信息学诊断工具、LLM和其他智能系统），平均Recall@1分数为57.18%，比第二好的方法（推理LLM）高出23.79个百分点。对于多模态输入场景，DeepRare在109个病例中Recall@1达到70.60%，而Exomiser为53.20%。临床专家对推理链的手动验证达到了95.40%的一致性。", "conclusion": "DeepRare系统在罕见病诊断方面表现出卓越的性能，能够处理异构输入并提供可追溯的推理，显著优于现有方法，并已实现为用户友好的网络应用程序。", "translation": "罕见病在全球范围内影响超过3亿人，然而，及时准确的诊断仍然是一个普遍的挑战。这主要是由于其临床异质性、个体患病率低以及大多数临床医生对罕见病症不熟悉。在此，我们介绍了DeepRare，这是首个由大型语言模型（LLM）驱动的罕见病诊断智能系统，能够处理异构临床输入。该系统生成罕见病的诊断假设排名，每个假设都附有透明的推理链，将中间分析步骤与可验证的医学证据联系起来。\nDeepRare包含三个关键组件：一个带有长期记忆模块的中央主机；以及负责特定领域分析任务的专业代理服务器，这些服务器集成了40多种专业工具和网络规模的最新医学知识来源，确保访问最新的临床信息。这种模块化和可扩展的设计使得复杂的诊断推理成为可能，同时保持可追溯性和适应性。我们在八个数据集上评估了DeepRare。该系统在2,919种疾病中表现出卓越的诊断性能，其中1013种疾病达到了100%的准确率。在基于HPO的评估中，DeepRare显著优于其他15种方法，如传统生物信息学诊断工具、LLM和其他智能系统，平均Recall@1分数为57.18%，并以23.79个百分点的显著优势超过第二好的方法（推理LLM）。对于多模态输入场景，DeepRare在109个病例中Recall@1达到70.60%，而Exomiser为53.20%。临床专家对推理链的手动验证达到了95.40%的一致性。此外，DeepRare系统已实现为用户友好的网络应用程序http://raredx.cn/doctor。", "summary": "DeepRare是一个创新的智能系统，利用大型语言模型（LLM）处理异构临床数据，旨在解决罕见病诊断的难题。它能生成排名靠前的诊断假设，并提供可追溯的推理链，连接分析步骤和医学证据。系统由中央主机和专业代理服务器组成，集成了大量医学工具和知识。在多个数据集上，DeepRare展现出卓越的诊断性能，在准确率和Recall@1方面显著超越了现有诊断工具和LLM方法，且推理过程获得了临床专家的高度认可。该系统已部署为网络应用。", "keywords": "罕见病诊断, 智能系统, 大型语言模型, 可追溯推理, DeepRare", "comments": "本文提出了一种新颖的基于LLM的智能系统DeepRare，用于罕见病诊断。其创新之处在于结合了LLM的处理能力、多模态输入处理、领域特定代理服务器以及可追溯的推理链，这对于医疗诊断领域至关重要。系统在多个评估数据集上表现出显著优于现有方法的性能，特别是在罕见病这种复杂且数据稀缺的领域，其高准确率和Recall@1分数凸显了其重要性。此外，可追溯的推理链增加了诊断的透明度和可信度，对于临床应用具有实际价值。该系统已实现为网络应用，表明其具有潜在的实际部署和应用前景。"}}
{"id": "2506.20550", "title": "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos", "authors": ["Yitong Quan", "Benjamin Kiefer", "Martin Messmer", "Andreas Zell"], "summary": "Modern image-based object detection models, such as YOLOv7, primarily process\nindividual frames independently, thus ignoring valuable temporal context\nnaturally present in videos. Meanwhile, existing video-based detection methods\noften introduce complex temporal modules, significantly increasing model size\nand computational complexity. In practical applications such as surveillance\nand autonomous driving, transient challenges including motion blur, occlusions,\nand abrupt appearance changes can severely degrade single-frame detection\nperformance. To address these issues, we propose a straightforward yet highly\neffective strategy: stacking multiple consecutive frames as input to a\nYOLO-based detector while supervising only the output corresponding to a single\ntarget frame. This approach leverages temporal information with minimal\nmodifications to existing architectures, preserving simplicity, computational\nefficiency, and real-time inference capability. Extensive experiments on the\nchallenging MOT20Det and our BOAT360 datasets demonstrate that our method\nimproves detection robustness, especially for lightweight models, effectively\nnarrowing the gap between compact and heavy detection networks. Additionally,\nwe contribute the BOAT360 benchmark dataset, comprising annotated fisheye video\nsequences captured from a boat, to support future research in multi-frame video\nobject detection in challenging real-world scenarios.", "comment": "Submitted to ECMR 2025", "pdf_url": "http://arxiv.org/pdf/2506.20550v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20550v1", "AI": {"title_translation": "轻量级多帧集成用于视频中鲁棒的YOLO目标检测", "tldr": "提出一种轻量级多帧集成方法，通过堆叠多帧输入YOLO模型，提高视频中目标检测的鲁棒性，同时保持计算效率。", "motivation": "现代图像检测模型（如YOLOv7）独立处理帧，忽略时间上下文；现有视频检测方法复杂，增加模型大小和计算量；实际应用中，运动模糊、遮挡、外观变化等瞬态挑战会严重降低单帧检测性能。", "method": "将多个连续帧堆叠作为YOLO检测器的输入，但仅监督单个目标帧的输出。", "result": "在MOT20Det和BOAT360数据集上的实验表明，该方法提高了检测鲁棒性，尤其对轻量级模型有效，缩小了紧凑型和大型检测网络之间的差距。此外，还贡献了BOAT360数据集。", "conclusion": "该方法通过最小化修改现有架构，有效利用时间信息，提高了视频目标检测的鲁棒性，同时保持了实时推理能力和计算效率，并贡献了新的数据集。", "translation": "现代基于图像的目标检测模型，如YOLOv7，主要独立处理单个帧，因此忽略了视频中自然存在的时间上下文信息。同时，现有的基于视频的检测方法通常引入复杂的时序模块，显著增加了模型大小和计算复杂性。在监控和自动驾驶等实际应用中，运动模糊、遮挡和外观突变等瞬态挑战会严重降低单帧检测性能。为了解决这些问题，我们提出了一种直接但高效的策略：将多个连续帧堆叠作为YOLO检测器的输入，同时仅监督对应单个目标帧的输出。这种方法以最小的修改利用了时间信息，保留了现有架构的简洁性、计算效率和实时推理能力。在具有挑战性的MOT20Det和我们的BOAT360数据集上进行的广泛实验表明，我们的方法提高了检测鲁棒性，特别是对于轻量级模型，有效地缩小了紧凑型和大型检测网络之间的差距。此外，我们贡献了BOAT360基准数据集，该数据集包含从船上捕获的带注释的鱼眼视频序列，以支持未来在挑战性真实世界场景中进行多帧视频目标检测的研究。", "summary": "本文提出了一种轻量级多帧集成方法，通过将连续帧堆叠作为YOLO模型的输入，并仅监督目标帧的输出，来有效利用视频中的时间信息。该方法在不显著增加模型复杂度和计算量的前提下，提高了在运动模糊、遮挡等挑战性情况下的目标检测鲁棒性。实验证明其能缩小轻量级和大型检测网络之间的性能差距。此外，本文还发布了BOAT360鱼眼视频数据集。", "keywords": "视频目标检测, 多帧集成, YOLO, 鲁棒性, 轻量级模型", "comments": "该研究的创新点在于提出了一种简洁高效的多帧集成策略，避免了传统视频检测方法引入复杂时序模块的弊端，同时有效利用了时间上下文。其重要性体现在提高了轻量级模型在挑战性视频场景下的鲁棒性，使其更适用于资源受限的实际应用。贡献的新数据集BOAT360也对未来的多帧视频目标检测研究提供了宝贵的资源。"}}
{"id": "2506.20449", "title": "Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation", "authors": ["Changlu Guo", "Anders Nymark Christensen", "Morten Rieger Hannemose"], "summary": "Text-to-image generative models have achieved remarkable breakthroughs in\nrecent years. However, their application in medical image generation still\nfaces significant challenges, including small dataset sizes, and scarcity of\nmedical textual data. To address these challenges, we propose Med-Art, a\nframework specifically designed for medical image generation with limited data.\nMed-Art leverages vision-language models to generate visual descriptions of\nmedical images which overcomes the scarcity of applicable medical textual data.\nMed-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\\alpha$,\nbased on the Diffusion Transformer (DiT), achieving high performance under\nlimited data. Furthermore, we propose an innovative Hybrid-Level Diffusion\nFine-tuning (HLDF) method, which enables pixel-level losses, effectively\naddressing issues such as overly saturated colors. We achieve state-of-the-art\nperformance on two medical image datasets, measured by FID, KID, and downstream\nclassification performance.", "comment": "The project is available at \\url{https://medart-ai.github.io}", "pdf_url": "http://arxiv.org/pdf/2506.20449v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20449v1", "AI": {"title_translation": "医-艺：用于2D医学文本到图像生成的扩散Transformer", "tldr": "Med-Art是一个针对医疗图像生成数据稀缺问题提出的框架，它利用扩散Transformer和混合级别扩散微调，在有限数据下实现了最先进的文本到医学图像生成性能。", "motivation": "文本到图像生成模型在医学图像生成中面临数据集规模小和医学文本数据稀缺的挑战。", "method": "Med-Art框架利用视觉-语言模型生成医学图像的视觉描述，以克服医学文本数据稀缺问题。它基于Diffusion Transformer (DiT) 适配了大规模预训练的文本到图像模型PixArt-α。此外，提出了一种创新的混合级别扩散微调（HLDF）方法，该方法支持像素级损失，有效解决了诸如颜色过度饱和等问题。", "result": "在两个医学图像数据集上实现了最先进的性能，通过FID、KID和下游分类性能进行衡量。", "conclusion": "Med-Art成功解决了医疗文本到图像生成中的数据稀缺和图像质量问题，并在有限数据下取得了最先进的性能。", "translation": "标题：Med-Art: 用于2D医学文本到图像生成的扩散Transformer\n摘要：文本到图像生成模型近年来取得了显著突破。然而，它们在医学图像生成中的应用仍面临重大挑战，包括数据集规模小和医学文本数据稀缺。为了解决这些挑战，我们提出了Med-Art，一个专门为有限数据下的医学图像生成设计的框架。Med-Art利用视觉-语言模型生成医学图像的视觉描述，从而克服了适用医学文本数据稀缺的问题。Med-Art基于扩散Transformer（DiT）适配了大规模预训练的文本到图像模型PixArt-α，在有限数据下实现了高性能。此外，我们提出了一种创新的混合级别扩散微调（HLDF）方法，该方法支持像素级损失，有效解决了诸如颜色过度饱和等问题。我们在两个医学图像数据集上实现了最先进的性能，并通过FID、KID和下游分类性能进行衡量。", "summary": "Med-Art是一个旨在解决医学文本到图像生成中数据稀缺挑战的框架。它通过视觉-语言模型生成图像描述来弥补文本数据不足，并基于Diffusion Transformer（DiT）适配了预训练模型PixArt-α。此外，其提出的混合级别扩散微调（HLDF）方法有效改善了图像质量。该模型在有限数据下，在两个医学数据集上达到了最先进的性能。", "keywords": "医疗图像生成, 文本到图像, 扩散Transformer, 数据稀缺, 混合级别扩散微调", "comments": "Med-Art的创新点在于它通过结合视觉-语言模型来解决医学领域特有的文本数据稀缺问题，并提出了HLDF来优化生成图像的质量。其在有限数据下的SOTA表现对于医学图像生成领域具有重要意义，因为它降低了对大量标注数据的依赖，这在医疗领域是普遍的挑战。"}}
{"id": "2506.20132", "title": "High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data", "authors": ["Patrick Alan Johnson", "Gabriel Tseng", "Yawen Zhang", "Heather Heward", "Virginia Sjahli", "Favyen Bastani", "Joseph Redmon", "Patrick Beukema"], "summary": "Wildfires are increasing in intensity and severity at an alarming rate.\nRecent advances in AI and publicly available satellite data enable monitoring\ncritical wildfire risk factors globally, at high resolution and low latency.\nLive Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is\nvaluable for both wildfire research and operational response. However,\nground-based LFMC samples are both labor intensive and costly to acquire,\nresulting in sparse and infrequent updates. In this work, we explore the use of\na pretrained, highly-multimodal earth-observation model for generating\nlarge-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves\nsignificant improvements over previous methods using randomly initialized\nmodels (20 reduction in RMSE). We provide an automated pipeline that enables\nrapid generation of these LFMC maps across the United States, and demonstrate\nits effectiveness in two regions recently impacted by wildfire (Eaton and\nPalisades).", "comment": "10 pages, ICML 2025 (TerraBytes)", "pdf_url": "http://arxiv.org/pdf/2506.20132v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20132v1", "AI": {"title_translation": "高分辨率活燃料含水率（LFMC）地图，用于利用多模态地球观测数据评估野火风险", "tldr": "利用预训练多模态地球观测模型生成高分辨率LFMC地图，显著提高野火风险评估精度并实现自动化。", "motivation": "野火强度和严重性日益增加，LFMC是关键风险因素，但地面采样昂贵且稀疏。需要高分辨率、低延迟的LFMC监测。", "method": "使用预训练的、高度多模态的地球观测模型生成大规模、空间完整的LFMC地图。开发自动化流程在美国生成这些地图。", "result": "该方法比以前使用随机初始化模型的方法取得了显著改进（RMSE降低20%）。成功在美国两个受野火影响的地区（Eaton和Palisades）展示了有效性。", "conclusion": "通过预训练多模态地球观测模型，可以有效地生成高分辨率、大规模的LFMC地图，显著提高野火风险评估能力。", "translation": "野火的强度和严重性正以惊人的速度增加。人工智能和公开可用的卫星数据方面的最新进展使得在全球范围内以高分辨率和低延迟监测关键野火风险因素成为可能。活燃料含水率（LFMC）是野火的关键风险因素，对野火研究和操作响应都很有价值。然而，地面LFMC样本的获取既耗费人力又成本高昂，导致更新稀疏且不频繁。在这项工作中，我们探索使用预训练的、高度多模态的地球观测模型来生成大规模、空间完整（全覆盖）的LFMC地图。我们的方法比以前使用随机初始化模型的方法取得了显著改进（RMSE降低20%）。我们提供了一个自动化管道，可以在美国各地快速生成这些LFMC地图，并展示了其在最近受野火影响的两个区域（Eaton和Palisades）的有效性。", "summary": "本研究利用预训练的多模态地球观测模型，开发了一种生成高分辨率、全覆盖活燃料含水率（LFMC）地图的新方法，以应对日益严重的野火威胁。该方法显著提高了LFMC预测精度（RMSE降低20%），并提供了自动化管道，可在美国范围内快速生成地图，为野火风险评估和响应提供了重要工具。", "keywords": "野火风险, 活燃料含水率, 地球观测, 多模态, 人工智能", "comments": "这项工作创新性地利用了预训练的多模态地球观测模型来解决LFMC数据获取的挑战，显著提高了野火风险评估的精度和效率。自动化管道的开发使其具有很高的实用价值和可扩展性。"}}
{"id": "2506.20481", "title": "Counterfactual Influence as a Distributional Quantity", "authors": ["Matthieu Meeus", "Igor Shilov", "Georgios Kaissis", "Yves-Alexandre de Montjoye"], "summary": "Machine learning models are known to memorize samples from their training\ndata, raising concerns around privacy and generalization. Counterfactual\nself-influence is a popular metric to study memorization, quantifying how the\nmodel's prediction for a sample changes depending on the sample's inclusion in\nthe training dataset. However, recent work has shown memorization to be\naffected by factors beyond self-influence, with other training samples, in\nparticular (near-)duplicates, having a large impact. We here study memorization\ntreating counterfactual influence as a distributional quantity, taking into\naccount how all training samples influence how a sample is memorized. For a\nsmall language model, we compute the full influence distribution of training\nsamples on each other and analyze its properties. We find that solely looking\nat self-influence can severely underestimate tangible risks associated with\nmemorization: the presence of (near-)duplicates seriously reduces\nself-influence, while we find these samples to be (near-)extractable. We\nobserve similar patterns for image classification, where simply looking at the\ninfluence distributions reveals the presence of near-duplicates in CIFAR-10.\nOur findings highlight that memorization stems from complex interactions across\ntraining data and is better captured by the full influence distribution than by\nself-influence alone.", "comment": "Workshop on The Impact of Memorization on Trustworthy Foundation\n  Models (MemFM) @ ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.20481v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20481v1", "AI": {"title_translation": "反事实影响作为一种分布量", "tldr": "现有自影响度量低估了模型记忆风险，因为它忽略了其他训练样本（尤其是重复项）的影响；本文将反事实影响视为分布量，发现其能更好地捕捉复杂的记忆模式。", "motivation": "机器学习模型会记忆训练数据，引发隐私和泛化问题。现有的反事实自影响度量未能充分捕捉其他训练样本（尤其是近乎重复的样本）对记忆化的重要影响，导致低估了相关的风险。", "method": "将反事实影响视为一种分布量，考虑所有训练样本如何相互影响记忆化。计算并分析了一个小型语言模型中训练样本之间完整的相互影响分布。并在图像分类任务（CIFAR-10）中观察到类似模式。", "result": "1. 仅关注自影响会严重低估与记忆化相关的实际风险。2. 近乎重复的样本会显著降低自影响，但这些样本却是（近乎）可提取的。3. 在图像分类任务中也观察到类似模式，影响分布揭示了CIFAR-10中存在近乎重复的样本。", "conclusion": "记忆化源于训练数据之间复杂的相互作用，通过完整的影响分布而非单独的自影响能更好地捕捉。", "translation": "机器学习模型已知会记忆其训练数据中的样本，引发了对隐私和泛化性的担忧。反事实自影响是一种流行的度量指标，用于研究记忆化，它量化了模型对某个样本的预测如何根据该样本是否包含在训练数据集中而变化。然而，最近的工作表明，记忆化受到自影响之外的因素影响，特别是其他训练样本，尤其是（近乎）重复的样本，具有很大的影响。我们在此将反事实影响视为一种分布量来研究记忆化，考虑了所有训练样本如何影响一个样本被记忆。对于一个小型语言模型，我们计算了训练样本之间完整的相互影响分布并分析了其属性。我们发现，仅仅关注自影响会严重低估与记忆化相关的实际风险：存在（近乎）重复的样本会严重降低自影响，但我们发现这些样本是（近乎）可提取的。我们在图像分类中也观察到类似的模式，其中简单地查看影响分布就揭示了CIFAR-10中存在近乎重复的样本。我们的发现强调，记忆化源于训练数据之间复杂的相互作用，并且通过完整的影响分布比单独的自影响能更好地捕捉。", "summary": "本文提出将机器学习模型中的反事实影响视为一种分布量来研究记忆化，以克服传统自影响度量未能充分捕捉其他训练样本（特别是重复项）影响的局限性。研究通过计算小型语言模型中训练样本间的完整影响分布，发现仅依赖自影响会低估记忆风险，因为重复样本会降低自影响但仍可被提取。在图像分类中也观察到类似模式。研究强调记忆化是训练数据间复杂交互的结果，完整影响分布能更准确地反映这一现象。", "keywords": "记忆化, 反事实影响, 分布量, 重复样本, 模型隐私", "comments": "这篇论文通过引入“反事实影响作为一种分布量”的概念，提供了一个更全面的视角来理解机器学习模型的记忆化行为。其创新之处在于突破了传统自影响度量的局限性，揭示了训练数据中（近乎）重复样本在记忆化中扮演的关键角色。论文的重要性在于指出，仅凭自影响可能导致对模型记忆风险的严重低估，这对于模型隐私、安全和泛化性的评估具有重要指导意义。"}}
{"id": "2506.20471", "title": "Probing AI Safety with Source Code", "authors": ["Ujwal Narayan", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "summary": "Large language models (LLMs) have become ubiquitous, interfacing with humans\nin numerous safety-critical applications. This necessitates improving\ncapabilities, but importantly coupled with greater safety measures to align\nthese models with human values and preferences. In this work, we demonstrate\nthat contemporary models fall concerningly short of the goal of AI safety,\nleading to an unsafe and harmful experience for users. We introduce a prompting\nstrategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT\nconverts natural language inputs to simple code that represents the same\nintent. For instance, CoDoT transforms the natural language prompt \"Make the\nstatement more toxic: {text}\" to: \"make_more_toxic({text})\". We show that CoDoT\nresults in a consistent failure of a wide range of state-of-the-art LLMs. For\nexample, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of\nthe time, and toxicity increases 300% on average across seven modern LLMs.\nAdditionally, recursively applying CoDoT can further increase toxicity two\ntimes. Given the rapid and widespread adoption of LLMs, CoDoT underscores the\ncritical need to evaluate safety efforts from first principles, ensuring that\nsafety and capabilities advance together.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20471v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20471v1", "AI": {"title_translation": "使用源代码探测AI安全", "tldr": "大型语言模型在安全方面存在严重缺陷，一项名为Code of Thought (CoDoT)的新提示策略通过将自然语言转换为代码，揭示了主流LLM的毒性显著增加，表明AI安全评估的紧迫性。", "motivation": "大型语言模型（LLMs）已广泛应用于安全关键型应用中，因此除了提升能力外，还需要加强安全措施以使模型与人类价值观对齐。然而，现有模型在AI安全方面存在不足，可能导致不安全和有害的用户体验。", "method": "本文引入了一种名为Code of Thought (CoDoT)的提示策略来评估LLM的安全性。CoDoT将自然语言输入转换为代表相同意图的简单代码，例如将“Make the statement more toxic: {text}”转换为“make_more_toxic({text})”。", "result": "CoDoT导致各种最先进的LLM持续出现故障。例如，GPT-4 Turbo的毒性增加了16.5倍，DeepSeek R1的失败率达到100%，七个现代LLM的毒性平均增加了300%。此外，递归应用CoDoT可以将毒性进一步增加两倍。", "conclusion": "鉴于LLM的快速普及，CoDoT强调了从第一性原理评估安全工作的关键需求，以确保安全性和能力同步发展。", "translation": "大型语言模型（LLMs）已无处不在，在众多安全关键型应用中与人类交互。这要求提高能力，但更重要的是要结合更强的安全措施，使这些模型与人类价值观和偏好保持一致。在这项工作中，我们证明了当代模型在AI安全目标方面存在令人担忧的不足，导致用户体验不安全和有害。我们引入了一种名为Code of Thought (CoDoT)的提示策略来评估LLM的安全性。CoDoT将自然语言输入转换为代表相同意图的简单代码。例如，CoDoT将自然语言提示“Make the statement more toxic: {text}”转换为：“make_more_toxic({text})”。我们展示了CoDoT导致各种最先进的LLMs持续出现故障。例如，GPT-4 Turbo的毒性增加了16.5倍，DeepSeek R1的失败率达到100%，七个现代LLMs的毒性平均增加了300%。此外，递归应用CoDoT可以将毒性进一步增加两倍。鉴于LLMs的快速普及和广泛采用，CoDoT强调了从第一性原理评估安全工作的关键需求，确保安全性和能力同步发展。", "summary": "本研究揭示了当前大型语言模型（LLMs）在AI安全方面存在严重缺陷，可能导致有害的用户体验。为评估LLM的安全性，论文提出了一种名为Code of Thought (CoDoT)的新型提示策略，该策略能将自然语言转换为意图一致的简单代码。实验结果表明，CoDoT能一致性地导致包括GPT-4 Turbo和DeepSeek R1在内的多种先进LLM出现显著安全故障，例如平均毒性增加300%，递归应用甚至能使毒性翻倍。研究强调了在LLM快速普及背景下，从根本上加强AI安全评估的紧迫性。", "keywords": "AI安全, 大型语言模型, 提示策略, Code of Thought, 毒性评估", "comments": "本文创新性地提出了CoDoT提示策略，通过将自然语言意图编码为代码形式，有效地揭示了当前主流大型语言模型在安全性方面的显著漏洞。其重要性在于，它提供了一种系统性、可量化的方法来探测LLM的潜在危害，并用具体数据（如毒性增加倍数）证明了问题并非偶然。这一发现对AI社区具有重大警示意义，强调了在追求模型能力提升的同时，亟需从根本上重新审视和加强AI安全评估与对齐工作，以避免未来AI系统可能造成的危害。"}}
{"id": "2506.20586", "title": "Learning-Based Distance Estimation for 360° Single-Sensor Setups", "authors": ["Yitong Quan", "Benjamin Kiefer", "Martin Messmer", "Andreas Zell"], "summary": "Accurate distance estimation is a fundamental challenge in robotic\nperception, particularly in omnidirectional imaging, where traditional\ngeometric methods struggle with lens distortions and environmental variability.\nIn this work, we propose a neural network-based approach for monocular distance\nestimation using a single 360{\\deg} fisheye lens camera. Unlike classical\ntrigonometric techniques that rely on precise lens calibration, our method\ndirectly learns and infers the distance of objects from raw omnidirectional\ninputs, offering greater robustness and adaptability across diverse conditions.\nWe evaluate our approach on three 360{\\deg} datasets (LOAF, ULM360, and a newly\ncaptured dataset Boat360), each representing distinct environmental and sensor\nsetups. Our experimental results demonstrate that the proposed learning-based\nmodel outperforms traditional geometry-based methods and other learning\nbaselines in both accuracy and robustness. These findings highlight the\npotential of deep learning for real-time omnidirectional distance estimation,\nmaking our approach particularly well-suited for low-cost applications in\nrobotics, autonomous navigation, and surveillance.", "comment": "Submitted to ECMR 2025", "pdf_url": "http://arxiv.org/pdf/2506.20586v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20586v1", "AI": {"title_translation": "基于学习的360°单传感器设置距离估计", "tldr": "本文提出了一种基于神经网络的单目距离估计方法，使用单个360°鱼眼相机，在准确性和鲁棒性方面优于传统几何方法。", "motivation": "在全向成像中，传统的几何方法在距离估计时难以应对镜头畸变和环境变异性，导致机器人感知中准确距离估计成为一个基本挑战。", "method": "本文提出了一种基于神经网络的单目距离估计方法，使用单个360°鱼眼镜头相机。该方法直接从原始全向输入中学习和推断物体的距离，而不依赖于精确的镜头校准。", "result": "所提出的基于学习的模型在三个360°数据集（LOAF、ULM360和Boat360）上的评估结果表明，其在准确性和鲁棒性方面均优于传统的基于几何的方法和其他学习基线。", "conclusion": "深度学习在实时全向距离估计方面具有巨大潜力，所提出的方法特别适用于机器人、自主导航和监控等领域的低成本应用。", "translation": "精确的距离估计是机器人感知中的一个基本挑战，尤其是在全向成像中，传统几何方法难以应对镜头畸变和环境变异性。在这项工作中，我们提出了一种基于神经网络的单目距离估计方法，使用单个360度鱼眼镜头相机。与依赖精确镜头校准的经典三角测量技术不同，我们的方法直接从原始全向输入中学习和推断物体的距离，在不同条件下提供了更高的鲁棒性和适应性。我们在三个360度数据集（LOAF、ULM360以及一个新捕获的数据集Boat360）上评估了我们的方法，每个数据集都代表了不同的环境和传感器设置。我们的实验结果表明，所提出的基于学习的模型在准确性和鲁棒性方面均优于传统的基于几何的方法和其他学习基线。这些发现凸显了深度学习在实时全向距离估计方面的潜力，使我们的方法特别适用于机器人、自主导航和监控中的低成本应用。", "summary": "本文提出了一种新颖的基于神经网络的单目距离估计方法，利用单个360°鱼眼相机。该方法解决了全向成像中传统几何方法受限于镜头畸变和环境变异性的问题，通过直接从原始输入中学习距离，提供了增强的鲁棒性和适应性。在多个360°数据集上的实验结果表明，与现有方法相比，其在准确性和鲁棒性方面均表现出色，突显了其在机器人和自主系统中的实时、低成本应用的适用性。", "keywords": "距离估计, 360°鱼眼, 深度学习, 全向成像, 机器人感知", "comments": "本文将深度学习创新性地应用于机器人感知中的一个具有挑战性的问题：全向相机距离估计。其核心创新在于直接从原始鱼眼输入中学习距离，避免了传统方法对精确镜头校准的依赖，从而提高了方法的鲁棒性和适应性。在不同数据集上优于传统方法和学习基线的表现，强调了其在低成本机器人应用中的实际意义，尤其是在360°视觉日益普及的场景。"}}
{"id": "2506.20452", "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling", "authors": ["Tobias Vontobel", "Seyedmorteza Sadat", "Farnood Salehi", "Romann M. Weber"], "summary": "Diffusion models have emerged as the leading approach for image synthesis,\ndemonstrating exceptional photorealism and diversity. However, training\ndiffusion models at high resolutions remains computationally prohibitive, and\nexisting zero-shot generation techniques for synthesizing images beyond\ntraining resolutions often produce artifacts, including object duplication and\nspatial incoherence. In this paper, we introduce HiWave, a training-free,\nzero-shot approach that substantially enhances visual fidelity and structural\ncoherence in ultra-high-resolution image synthesis using pretrained diffusion\nmodels. Our method employs a two-stage pipeline: generating a base image from\nthe pretrained model followed by a patch-wise DDIM inversion step and a novel\nwavelet-based detail enhancer module. Specifically, we first utilize inversion\nmethods to derive initial noise vectors that preserve global coherence from the\nbase image. Subsequently, during sampling, our wavelet-domain detail enhancer\nretains low-frequency components from the base image to ensure structural\nconsistency, while selectively guiding high-frequency components to enrich fine\ndetails and textures. Extensive evaluations using Stable Diffusion XL\ndemonstrate that HiWave effectively mitigates common visual artifacts seen in\nprior methods, achieving superior perceptual quality. A user study confirmed\nHiWave's performance, where it was preferred over the state-of-the-art\nalternative in more than 80% of comparisons, highlighting its effectiveness for\nhigh-quality, ultra-high-resolution image synthesis without requiring\nretraining or architectural modifications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20452v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20452v1", "AI": {"title_translation": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成", "tldr": "HiWave是一种免训练、零样本的方法，通过结合DDIM反演和新颖的小波域细节增强模块，显著提升了预训练扩散模型生成超高分辨率图像的视觉保真度和结构一致性，有效解决了现有方法中的伪影问题。", "motivation": "现有扩散模型在生成高分辨率图像时计算成本过高，且零样本生成技术在超分辨率图像合成时常产生物体重复和空间不连贯等伪影。本研究旨在解决这些问题，实现高质量、无伪影的超高分辨率图像生成。", "method": "HiWave采用两阶段流水线：首先，利用预训练模型生成基础图像，并进行逐块DDIM反演以获得保留全局一致性的初始噪声向量。其次，在采样过程中，通过小波域细节增强模块保留基础图像的低频分量以确保结构一致性，同时选择性地引导高频分量以丰富细节和纹理。", "result": "使用Stable Diffusion XL进行的广泛评估表明，HiWave有效减轻了现有方法中常见的视觉伪影，实现了卓越的感知质量。用户研究证实了HiWave的性能，在超过80%的比较中优于最先进的替代方案。", "conclusion": "HiWave提出了一种免训练、零样本的方法，通过结合DDIM反演和小波域细节增强，成功实现了高质量、超高分辨率的图像合成，有效解决了现有扩散模型在高分辨率生成中存在的计算和伪影问题，无需再训练或修改架构。", "translation": "扩散模型已成为图像合成领域的领先方法，展现出卓越的真实感和多样性。然而，在高分辨率下训练扩散模型仍然计算成本高昂，并且现有用于合成超出训练分辨率图像的零样本生成技术常产生伪影，包括物体重复和空间不连贯。在本文中，我们引入了HiWave，一种免训练、零样本的方法，它使用预训练扩散模型显著增强了超高分辨率图像合成中的视觉保真度和结构一致性。我们的方法采用两阶段流水线：首先从预训练模型生成基础图像，然后进行逐块DDIM反演步骤和新颖的基于小波的细节增强模块。具体来说，我们首先利用反演方法导出保留基础图像全局一致性的初始噪声向量。随后，在采样过程中，我们的小波域细节增强器保留了基础图像的低频分量以确保结构一致性，同时选择性地引导高频分量以丰富精细细节和纹理。使用Stable Diffusion XL进行的广泛评估表明，HiWave有效减轻了先前方法中常见的视觉伪影，实现了卓越的感知质量。用户研究证实了HiWave的性能，在超过80%的比较中优于最先进的替代方案，突显了其在无需再训练或架构修改的情况下进行高质量、超高分辨率图像合成的有效性。", "summary": "HiWave是一种免训练、零样本的高分辨率图像生成方法，旨在解决现有扩散模型在高分辨率合成中计算成本高昂和伪影问题。该方法采用两阶段流水线：首先通过DDIM反演生成基础图像并获取初始噪声向量，然后利用新颖的小波域细节增强模块，在采样时保留低频分量以保持结构一致性，同时引导高频分量以丰富细节。实验证明，HiWave能有效减少伪影，提升感知质量，并在用户研究中表现出优越性。", "keywords": "高分辨率图像生成, 扩散模型, 小波变换, 零样本学习, 图像合成", "comments": "HiWave的创新之处在于其免训练和零样本的特性，通过巧妙地结合DDIM反演和小波变换，在不改变预训练模型架构的前提下，有效解决了高分辨率图像生成中的常见伪影问题（如物体重复和空间不连贯）。这种方法显著降低了计算成本，并提升了生成图像的视觉质量和结构一致性，对于实际应用具有重要意义。"}}
{"id": "2506.20474", "title": "Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations", "authors": ["Kaixiang Zhang", "Justine Zhang", "Cristian Danescu-Niculescu-Mizil"], "summary": "An intrinsic aspect of every conversation is the way talk-time is shared\nbetween multiple speakers. Conversations can be balanced, with each speaker\nclaiming a similar amount of talk-time, or imbalanced when one talks\ndisproportionately. Such overall distributions are the consequence of\ncontinuous negotiations between the speakers throughout the conversation: who\nshould be talking at every point in time, and for how long?\n  In this work we introduce a computational framework for quantifying both the\nconversation-level distribution of talk-time between speakers, as well as the\nlower-level dynamics that lead to it. We derive a typology of talk-time sharing\ndynamics structured by several intuitive axes of variation. By applying this\nframework to a large dataset of video-chats between strangers, we confirm that,\nperhaps unsurprisingly, different conversation-level distributions of talk-time\nare perceived differently by speakers, with balanced conversations being\npreferred over imbalanced ones, especially by those who end up talking less.\nThen we reveal that -- even when they lead to the same level of overall balance\n-- different types of talk-time sharing dynamics are perceived differently by\nthe participants, highlighting the relevance of our newly introduced typology.\nFinally, we discuss how our framework offers new tools to designers of\ncomputer-mediated communication platforms, for both human-human and human-AI\ncommunication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20474v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20474v1", "AI": {"title_translation": "时间在我这边：视频聊天对话中谈话时间分享的动态", "tldr": "本研究引入了一个计算框架来量化视频聊天中谈话时间的分配及其动态，并发现不同的谈话时间分享方式会被参与者以不同方式感知，为计算机介导通信平台的设计提供了新工具。", "motivation": "对话中谈话时间的分配方式是其固有的一个方面，这种整体分布是说话者之间持续协商的结果。理解和量化这种谈话时间分配及其动态，以及它们如何被参与者感知，是本研究的动机。", "method": "研究者引入了一个计算框架，用于量化说话者之间对话层面的谈话时间分布以及导致这种分布的低层动态。他们还推导了一个谈话时间分享动态的类型学，并将其应用于一个大型陌生人视频聊天数据集。", "result": "研究结果证实，说话者对不同对话层面的谈话时间分布有不同的感知，其中平衡的对话比不平衡的对话更受欢迎，特别是对那些最终说话较少的人而言。此外，即使整体平衡程度相同，不同类型的谈话时间分享动态也会被参与者以不同方式感知。", "conclusion": "本研究的框架为计算机介导通信平台的设计者（包括人与人及人与AI通信）提供了新工具，以优化谈话时间分享的动态和用户体验。", "translation": "每一次对话的一个内在方面是多个说话者之间分享谈话时间的方式。对话可以是平衡的，每个说话者占用相似的谈话时间，也可以是不平衡的，当其中一方说得不成比例时。这种整体分布是说话者在整个对话过程中持续协商的结果：在每个时间点谁应该说话，以及说多久？\n在这项工作中，我们引入了一个计算框架，用于量化说话者之间对话层面的谈话时间分布，以及导致这种分布的低层动态。我们推导了一个由几个直观变化轴构成的谈话时间分享动态类型学。通过将这个框架应用于一个大型陌生人视频聊天数据集，我们证实，或许不足为奇的是，说话者对不同对话层面的谈话时间分布有不同的感知，平衡的对话比不平衡的对话更受欢迎，特别是对那些最终说话较少的人而言。然后我们揭示，即使它们导致相同的整体平衡水平，不同类型的谈话时间分享动态也会被参与者以不同方式感知，这突显了我们新引入的类型学的相关性。最后，我们讨论了我们的框架如何为计算机介导通信平台的设计者提供新工具，用于人与人以及人与AI的通信。", "summary": "本研究提出了一个计算框架，用于分析视频聊天中谈话时间的分配及其底层动态。研究者构建了一个谈话时间分享动态的类型学，并将其应用于大型视频聊天数据集。结果表明，对话的平衡性影响参与者感知，平衡对话更受欢迎。更重要的是，即使整体平衡相同，不同类型的谈话时间动态也会被参与者以不同方式感知。该框架为未来计算机介导通信平台（包括人机交互）的设计提供了新工具。", "keywords": "谈话时间分享, 视频聊天, 对话动态, 计算框架, 用户感知", "comments": "这项研究的创新之处在于提出了一个量化谈话时间分配及其动态的计算框架和类型学，并将其应用于视频聊天环境。它不仅证实了谈话时间平衡的重要性，还深入揭示了即使在整体平衡的情况下，不同的动态过程也会影响用户感知。这为优化在线通信体验提供了宝贵的见解，尤其是在设计未来的人机交互平台方面具有重要意义。"}}
{"id": "2506.20464", "title": "A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners", "authors": ["Dibyayan Patra", "Pasindu Ranasinghe", "Bikram Banerjee", "Simit Raval"], "summary": "Rock bolts are crucial components of the subterranean support systems in\nunderground mines that provide adequate structural reinforcement to the rock\nmass to prevent unforeseen hazards like rockfalls. This makes frequent\nassessments of such bolts critical for maintaining rock mass stability and\nminimising risks in underground mining operations. Where manual surveying of\nrock bolts is challenging due to the low light conditions in the underground\nmines and the time-intensive nature of the process, automated detection of rock\nbolts serves as a plausible solution. To that end, this study focuses on the\nautomatic identification of rock bolts within medium to large-scale 3D point\nclouds obtained from underground mines using mobile laser scanners. Existing\ntechniques for automated rock bolt identification primarily rely on feature\nengineering and traditional machine learning approaches. However, such\ntechniques lack robustness as these point clouds present several challenges due\nto data noise, varying environments, and complex surrounding structures.\nMoreover, the target rock bolts are extremely small objects within large-scale\npoint clouds and are often partially obscured due to the application of\nreinforcement shotcrete. Addressing these challenges, this paper proposes an\napproach termed DeepBolt, which employs a novel two-stage deep learning\narchitecture specifically designed for handling severe class imbalance for the\nautomatic and efficient identification of rock bolts in complex 3D point\nclouds. The proposed method surpasses state-of-the-art semantic segmentation\nmodels by up to 42.5% in Intersection over Union (IoU) for rock bolt points.\nAdditionally, it outperforms existing rock bolt identification techniques,\nachieving a 96.41% precision and 96.96% recall in classifying rock bolts,\ndemonstrating its robustness and effectiveness in complex underground\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20464v1", "categories": ["cs.CV", "I.4.9"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20464v1", "AI": {"title_translation": "一种基于深度学习的方法，用于识别移动激光扫描仪在地下矿井复杂三维点云中捕获的锚杆", "tldr": "DeepBolt是一种新的两阶段深度学习方法，专门用于自动识别地下矿井复杂三维点云中的锚杆，其性能优于现有技术。", "motivation": "锚杆是地下矿井支护系统的关键组成部分，需要频繁评估以维护岩体稳定性并最大程度地降低风险。然而，由于光线不足和耗时，手动测量锚杆具有挑战性。现有的自动化技术依赖于特征工程和传统机器学习，但由于数据噪声、环境变化、复杂结构以及锚杆作为大型点云中的小目标且常被喷射混凝土部分遮挡，这些技术缺乏鲁棒性。因此，需要一种更稳健的自动化识别方法。", "method": "本文提出了一种名为DeepBolt的方法，该方法采用新颖的两阶段深度学习架构，专门设计用于处理严重的类别不平衡问题，以自动高效地识别复杂三维点云中的锚杆。", "result": "所提出的方法在锚杆点的交并比（IoU）方面超越了最先进的语义分割模型高达42.5%。此外，它在锚杆分类方面优于现有的锚杆识别技术，实现了96.41%的精确度（precision）和96.96%的召回率（recall）。", "conclusion": "DeepBolt方法在复杂地下环境中表现出强大的鲁棒性和有效性，能够自动且高效地识别三维点云中的锚杆，解决了现有方法的局限性。", "translation": "锚杆是地下矿井地下支护系统的关键组成部分，为岩体提供足够的结构加固，以防止落石等意外危险。这使得频繁评估此类锚杆对于维护岩体稳定性并最大程度地降低地下采矿作业中的风险至关重要。由于地下矿井光线不足和过程耗时，手动测量锚杆具有挑战性，因此锚杆的自动化检测是一种可行的解决方案。为此，本研究侧重于使用移动激光扫描仪在地下矿井中获取的中到大型三维点云中自动识别锚杆。现有的自动化锚杆识别技术主要依赖于特征工程和传统机器学习方法。然而，由于点云中存在数据噪声、环境多变和周围结构复杂等挑战，这些技术缺乏鲁棒性。此外，目标锚杆是大型点云中极小的物体，并且由于喷射混凝土的应用而经常被部分遮挡。为解决这些挑战，本文提出了一种名为DeepBolt的方法，该方法采用新颖的两阶段深度学习架构，专门设计用于处理严重的类别不平衡问题，以自动高效地识别复杂三维点云中的锚杆。所提出的方法在锚杆点的交并比（IoU）方面超越了最先进的语义分割模型高达42.5%。此外，它在锚杆分类方面优于现有的锚杆识别技术，实现了96.41%的精确度（precision）和96.96%的召回率（recall），证明了其在复杂地下环境中的鲁棒性和有效性。", "summary": "本文提出了一种名为DeepBolt的两阶段深度学习方法，用于自动识别移动激光扫描仪在地下矿井中捕获的复杂三维点云中的锚杆。该方法旨在解决现有技术因数据噪声、环境变化、复杂结构以及锚杆作为小目标且常被遮挡而导致的鲁棒性不足问题。DeepBolt通过处理严重的类别不平衡问题，在锚杆点分割的交并比上比现有语义分割模型高出42.5%，并在锚杆分类上实现了96.41%的精确度和96.96%的召回率，证明了其在复杂地下环境中的有效性和鲁棒性。", "keywords": "锚杆识别, 深度学习, 三维点云, 地下矿井, 移动激光扫描仪", "comments": "DeepBolt的创新点在于其新颖的两阶段深度学习架构，专门解决了三维点云中锚杆识别面临的严重类别不平衡问题，以及锚杆作为小目标和被遮挡的挑战。这对于提高地下矿井安全性和自动化监测水平具有重要意义，尤其是在复杂和恶劣的环境中。其在IoU、精确度和召回率上的显著提升表明了该方法的强大性能和实际应用潜力。"}}
{"id": "2506.20522", "title": "AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns", "authors": ["Chathura Wimalasiri", "Piumal Rathnayake", "Shamod Wijerathne", "Sumudu Rasnayaka", "Dhanushka Leuke Bandara", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "summary": "Periodontitis, a chronic inflammatory disease causing alveolar bone loss,\nsignificantly affects oral health and quality of life. Accurate assessment of\nbone loss severity and pattern is critical for diagnosis and treatment\nplanning. In this study, we propose a novel AI-based deep learning framework to\nautomatically detect and quantify alveolar bone loss and its patterns using\nintraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth\ndetection with Keypoint R-CNN models to identify anatomical landmarks, enabling\nprecise calculation of bone loss severity. Additionally, YOLOv8x-seg models\nsegment bone levels and tooth masks to determine bone loss patterns (horizontal\nvs. angular) via geometric analysis. Evaluated on a large, expertly annotated\ndataset of 1000 radiographs, our approach achieved high accuracy in detecting\nbone loss severity (intra-class correlation coefficient up to 0.80) and bone\nloss pattern classification (accuracy 87%). This automated system offers a\nrapid, objective, and reproducible tool for periodontal assessment, reducing\nreliance on subjective manual evaluation. By integrating AI into dental\nradiographic analysis, our framework has the potential to improve early\ndiagnosis and personalized treatment planning for periodontitis, ultimately\nenhancing patient care and clinical outcomes.", "comment": "This manuscript is 17 pages with 5 tables and 12 figures. The\n  manuscript is under review at Nature Scientific Reports", "pdf_url": "http://arxiv.org/pdf/2506.20522v1", "categories": ["cs.CV", "I.5.4; I.4.6; I.4.9; I.4.8; J.3"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20522v1", "AI": {"title_translation": "牙槽骨丢失严重程度和模式检测中的AI辅助放射线分析", "tldr": "本研究提出了一种基于AI深度学习的框架，用于自动检测和量化牙槽骨丢失及其模式，提高了牙周评估的准确性和效率。", "motivation": "牙周炎是一种慢性炎症性疾病，导致牙槽骨丢失，对口腔健康和生活质量有显著影响。准确评估骨丢失的严重程度和模式对于诊断和治疗计划至关重要，但传统方法可能依赖主观手动评估。", "method": "本研究提出了一种新颖的AI深度学习框架，用于利用口内根尖（IOPA）X光片自动检测和量化牙槽骨丢失及其模式。该方法结合YOLOv8进行牙齿检测，Keypoint R-CNN模型识别解剖标志以精确计算骨丢失严重程度。此外，YOLOv8x-seg模型分割骨水平和牙齿掩膜，并通过几何分析确定骨丢失模式（水平型 vs. 成角型）。", "result": "在包含1000张放射线照片的专家标注大型数据集上进行评估，该方法在检测骨丢失严重程度方面取得了高精度，组内相关系数高达0.80。骨丢失模式分类的准确率为87%。", "conclusion": "该自动化系统为牙周评估提供了一个快速、客观、可重复的工具，减少了对主观手动评估的依赖。通过将AI整合到牙科放射线分析中，该框架有潜力改善牙周炎的早期诊断和个性化治疗计划，最终提高患者护理和临床结果。", "translation": "牙周炎是一种导致牙槽骨丢失的慢性炎症性疾病，严重影响口腔健康和生活质量。准确评估骨丢失的严重程度和模式对于诊断和治疗计划至关重要。在本研究中，我们提出了一种新颖的基于人工智能的深度学习框架，利用口内根尖（IOPA）X光片自动检测和量化牙槽骨丢失及其模式。我们的方法结合YOLOv8进行牙齿检测，并使用Keypoint R-CNN模型识别解剖标志，从而能够精确计算骨丢失的严重程度。此外，YOLOv8x-seg模型分割骨水平和牙齿掩膜，通过几何分析确定骨丢失模式（水平型 vs. 成角型）。在包含1000张X光片的专家标注大型数据集上进行评估，我们的方法在检测骨丢失严重程度方面取得了高精度（组内相关系数高达0.80），骨丢失模式分类准确率为87%。该自动化系统为牙周评估提供了一个快速、客观、可重复的工具，减少了对主观手动评估的依赖。通过将人工智能整合到牙科放射线分析中，我们的框架有潜力改善牙周炎的早期诊断和个性化治疗计划，最终提高患者护理和临床结果。", "summary": "本研究开发了一种基于AI深度学习的框架，利用口内根尖X光片自动检测和量化牙槽骨丢失的严重程度及模式。该框架结合YOLOv8进行牙齿检测，Keypoint R-CNN识别解剖标志以计算骨丢失严重程度，并利用YOLOv8x-seg分割骨水平和牙齿掩膜以通过几何分析确定骨丢失模式。在大型专家标注数据集上的评估显示，该系统在骨丢失严重程度检测方面具有高精度（ICC高达0.80），在模式分类方面准确率为87%。该自动化系统为牙周评估提供了一个快速、客观、可重复的工具，有望改善牙周炎的早期诊断和个性化治疗。", "keywords": "牙槽骨丢失, AI, 深度学习, 放射线分析, 牙周炎", "comments": "这篇论文提出了一种创新的AI辅助方法，通过结合多种深度学习模型（YOLOv8, Keypoint R-CNN, YOLOv8x-seg）实现了牙槽骨丢失的自动检测和模式分类，显著提高了牙周评估的客观性和效率。其在大型数据集上的高精度表现，展示了AI在牙科放射线分析中的巨大潜力，有助于减少对主观人工评估的依赖，并有望改善牙周炎的早期诊断和个性化治疗。"}}
{"id": "2506.20194", "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs", "authors": ["Ruokai Yin", "Yuhang Li", "Donghyun Lee", "Priyadarshini Panda"], "summary": "Large language models (LLMs) deliver strong performance but are difficult to\ndeploy due to high memory and compute costs. While pruning reduces these\ndemands, most methods ignore activation sparsity observed at runtime. We\nreinterpret activation sparsity as dynamic structured weight sparsity and\npropose DuoGPT, a unified framework that constructs dual-sparse (spMspV)\nworkloads by combining unstructured weight pruning with activation sparsity. To\npreserve accuracy, we extend the Optimal Brain Compression (OBC) framework with\nactivation-aware calibration and introduce output residuals from the dense\nmodel as correction terms. We further optimize the solution for efficient GPU\nexecution, enabling scalability to billion-parameter LLMs. Evaluations on\nLLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured\npruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\\times$\ncompared to the baseline dense model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20194v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20194v1", "AI": {"title_translation": "DuoGPT: 大型语言模型中通过激活感知剪枝实现无训练双稀疏性", "tldr": "DuoGPT通过结合非结构化权重剪枝和激活稀疏性，在LLM中实现了双稀疏性，显著提高了部署效率和性能。", "motivation": "大型语言模型（LLMs）性能强大但部署成本高昂（内存和计算），现有剪枝方法忽略了运行时观察到的激活稀疏性。", "method": "DuoGPT是一个统一框架，通过将非结构化权重剪枝与激活稀疏性结合，构建双稀疏（spMspV）工作负载。它扩展了OBC框架，加入了激活感知校准和来自密集模型的输出残差作为校正项，并优化了GPU执行效率。", "result": "在LLaMA-2和LLaMA-3上的评估显示，在与基线密集模型相比加速1.39倍的相同速度下，DuoGPT的准确率比最先进的结构化剪枝方法高出9.17%。", "conclusion": "DuoGPT通过创新的双稀疏剪枝方法，显著提高了大型语言模型的部署效率和性能，超越了现有技术。", "translation": "大型语言模型（LLMs）表现出强大的性能，但由于高内存和计算成本而难以部署。虽然剪枝可以降低这些需求，但大多数方法忽略了运行时观察到的激活稀疏性。我们将激活稀疏性重新解释为动态结构化权重稀疏性，并提出了DuoGPT，一个统一的框架，通过将非结构化权重剪枝与激活稀疏性相结合来构建双稀疏（spMspV）工作负载。为了保持准确性，我们扩展了最优大脑压缩（OBC）框架，加入了激活感知校准，并引入了来自密集模型的输出残差作为校正项。我们进一步优化了解决方案以实现高效的GPU执行，从而能够扩展到数十亿参数的LLM。在LLaMA-2和LLaMA-3上的评估显示，在与基线密集模型相比加速1.39倍的相同速度下，DuoGPT的准确率比最先进的结构化剪枝方法高出9.17%。", "summary": "DuoGPT是一个针对大型语言模型部署效率的创新框架。它通过结合非结构化权重剪枝和运行时激活稀疏性，实现了独特的双稀疏性，从而构建高效的spMspV工作负载。为保持模型准确性，DuoGPT扩展了OBC框架，引入了激活感知校准和密集模型输出残差作为校正，并优化了GPU执行。实验证明，DuoGPT在保持甚至提高准确性的同时，显著优于现有结构化剪枝方法，有效解决了LLM的内存和计算瓶颈。", "keywords": "大型语言模型, 剪枝, 稀疏性, DuoGPT, 部署", "comments": "DuoGPT的创新之处在于将运行时激活稀疏性重新解释为动态结构化权重稀疏性，并将其与非结构化权重剪枝结合，形成独特的双稀疏范式。这种方法在不进行训练的情况下显著提高了LLM的部署效率和性能，且通过激活感知校准和残差校正有效保持了模型准确性，这对于LLM的实际应用具有重要意义。"}}
{"id": "2506.19880", "title": "Physics-Guided Radiotherapy Treatment Planning with Deep Learning", "authors": ["Stefanos Achlatis", "Efstratios Gavves", "Jan-Jakob Sonke"], "summary": "Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated\narc therapy (VMAT) being a commonly used technique that enhances dose\nconformity by dynamically adjusting multileaf collimator (MLC) positions and\nmonitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires\nfrequent modifications to treatment plans to account for anatomical variations,\nnecessitating time-efficient solutions. Deep learning offers a promising\nsolution to automate this process. To this end, we propose a two-stage,\nphysics-guided deep learning pipeline for radiotherapy planning. In the first\nstage, our network is trained with direct supervision on treatment plan\nparameters, consisting of MLC and MU values. In the second stage, we\nincorporate an additional supervision signal derived from the predicted 3D dose\ndistribution, integrating physics-based guidance into the training process. We\ntrain and evaluate our approach on 133 prostate cancer patients treated with a\nuniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target\nvolume (PTV). Our results demonstrate that the proposed approach, implemented\nusing both 3D U-Net and UNETR architectures, consistently produces treatment\nplans that closely match clinical ground truths. Our method achieves a mean\ndifference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV\nwhile generating dose distributions that reduce radiation exposure to organs at\nrisk. These findings highlight the potential of physics-guided deep learning in\nRT planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19880v1", "categories": ["physics.med-ph", "cs.AI"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.19880v1", "AI": {"title_translation": "物理引导的深度学习放射治疗计划", "tldr": "提出一种两阶段物理引导的深度学习方法，用于放射治疗计划，能在前列腺癌患者中生成与临床真值匹配且减少危及器官暴露的计划。", "motivation": "适应性放射治疗需要频繁修改治疗计划以适应解剖变异，这需要时间高效的解决方案，而深度学习有望自动化此过程。", "method": "提出一个两阶段的物理引导深度学习流程。第一阶段直接监督治疗计划参数（MLC和MU值），第二阶段整合来自预测3D剂量分布的额外监督信号，将基于物理的指导纳入训练过程。该方法使用3D U-Net和UNETR架构。", "result": "该方法生成的治疗计划与临床真值高度匹配。在前列腺癌患者数据上，PTV的D95%平均差异为0.42 ± 1.83 Gy，V95%平均差异为-0.22 ± 1.87%，同时减少了对危及器官的辐射暴露。", "conclusion": "物理引导的深度学习在放射治疗计划中具有巨大潜力。", "translation": "放射治疗 (RT) 是一种重要的癌症治疗方法，其中容积调强弧形治疗 (VMAT) 是一种常用的技术，通过在整个机架旋转过程中动态调整多叶准直器 (MLC) 位置和监视单位 (MU) 来增强剂量适形度。自适应放射治疗需要频繁修改治疗计划以适应解剖变异，因此需要时间高效的解决方案。深度学习为自动化此过程提供了一个有前景的解决方案。为此，我们提出了一个两阶段的、物理引导的深度学习流程用于放射治疗计划。在第一阶段，我们的网络通过直接监督治疗计划参数（包括MLC和MU值）进行训练。在第二阶段，我们整合了源自预测3D剂量分布的额外监督信号，将基于物理的指导纳入训练过程。我们对133名接受统一2弧VMAT方案（向计划靶区 (PTV) 递送62 Gy剂量）的前列腺癌患者进行了训练和评估。我们的结果表明，所提出的方法（使用3D U-Net和UNETR架构实现）持续生成与临床真值高度匹配的治疗计划。我们的方法在PTV处实现了D95% = 0.42 ± 1.83 Gy和V95% = -0.22 ± 1.87%的平均差异，同时生成的剂量分布减少了对危及器官的辐射暴露。这些发现突出了物理引导的深度学习在RT计划中的潜力。", "summary": "本文提出了一种用于放射治疗计划的两阶段物理引导深度学习方法，旨在解决自适应放疗中计划修改耗时的问题。该方法首先直接监督治疗计划参数，然后结合预测剂量分布的物理指导信号进行训练。在前列腺癌患者数据集上的评估显示，该方法能生成与临床真值高度匹配且能有效保护危及器官的治疗计划，展现了物理引导深度学习在放疗计划中的应用潜力。", "keywords": "放射治疗计划, 深度学习, 物理引导, VMAT, 自适应放疗", "comments": "该研究创新性地将物理引导融入深度学习流程，通过两阶段训练优化放疗计划，提高了计划的准确性和对危及器官的保护，为自适应放疗提供了高效解决方案。"}}
{"id": "2506.20480", "title": "GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching", "authors": ["Guinan Su", "Li Shen", "Lu Yin", "Shiwei Liu", "Yanwu Yang", "Jonas Geiping"], "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\ndeployment and inference. While structured pruning of model parameters offers a\npromising way to reduce computational costs at deployment time, current methods\nprimarily focus on single model pruning. In this work, we develop a novel\nstrategy to compress models by strategically combining or merging layers from\nfinetuned model variants, which preserves the original model's abilities by\naggregating capabilities accentuated in different finetunes. We pose the\noptimal tailoring of these LLMs as a zero-order optimization problem, adopting\na search space that supports three different operations: (1) Layer removal, (2)\nLayer selection from different candidate models, and (3) Layer merging. Our\nexperiments demonstrate that this approach leads to competitive model pruning,\nfor example, for the Llama2-13B model families, our compressed models maintain\napproximately 97.3\\% of the original performance while removing $\\sim25\\%$ of\nparameters, significantly outperforming previous state-of-the-art methods. The\ncode is available at https://github.com/Guinan-Su/auto-merge-llm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20480v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20480v1", "AI": {"title_translation": "GPTailor：通过层剪切和拼接实现大型语言模型剪枝", "tldr": "GPTailor提出了一种通过层剪切和拼接来剪枝大型语言模型的新策略，能在移除约25%参数的同时保持约97.3%的原始性能，显著优于现有最先进方法。", "motivation": "大型语言模型（LLMs）虽然能力强大，但其庞大的模型规模给部署和推理带来了显著挑战。现有的结构化剪枝方法主要关注单一模型，未能充分利用来自不同微调模型变体的优势。", "method": "开发了一种新颖的策略，通过战略性地结合或合并来自微调模型变体的层来压缩模型，旨在通过聚合不同微调中强调的能力来保留原始模型的能力。将LLMs的最佳剪裁建模为零阶优化问题，其搜索空间支持三种不同操作：1) 层移除，2) 从不同候选模型中选择层，以及3) 层合并。", "result": "实验证明，该方法实现了具有竞争力的模型剪枝。例如，对于Llama2-13B模型家族，压缩后的模型在移除约25%参数的同时，保持了约97.3%的原始性能，显著优于现有最先进的方法。", "conclusion": "GPTailor通过创新的层剪切和拼接策略，能够高效地压缩大型语言模型，同时有效保持其原始性能，并在模型剪枝方面超越了现有最先进的方法。", "translation": "大型语言模型（LLMs）在语言理解和生成方面展现出卓越的能力。然而，这种令人印象深刻的能力通常伴随着庞大的模型规模，这给部署和推理带来了显著挑战。虽然模型参数的结构化剪枝为部署时降低计算成本提供了一种有前景的方法，但当前的方法主要集中于单一模型剪枝。在这项工作中，我们开发了一种新颖的策略，通过战略性地结合或合并来自微调模型变体的层来压缩模型，通过聚合不同微调中强调的能力来保留原始模型的能力。我们将这些LLMs的最佳剪裁建模为零阶优化问题，采用支持三种不同操作的搜索空间：(1) 层移除，(2) 从不同候选模型中选择层，以及3) 层合并。我们的实验表明，该方法实现了具有竞争力的模型剪枝，例如，对于Llama2-13B模型家族，我们的压缩模型在移除约25%参数的同时，保持了约97.3%的原始性能，显著优于现有最先进的方法。代码可在https://github.com/Guinan-Su/auto-merge-llm获取。", "summary": "本文提出了GPTailor，一种通过层剪切和拼接来剪枝大型语言模型的新策略。该方法通过战略性地结合或合并来自不同微调模型变体的层，以零阶优化问题的方式进行模型压缩，支持层移除、层选择和层合并三种操作。实验结果表明，对于Llama2-13B模型，GPTailor在移除约25%参数的同时，仍能保持约97.3%的原始性能，显著超越了现有最先进的剪枝方法，有效解决了LLM部署和推理中的规模挑战。", "keywords": "大型语言模型, 模型剪枝, 层剪切, 层拼接, 零阶优化", "comments": "这项研究的创新之处在于其独特的模型剪枝范式，即通过“层剪切和拼接”而非传统的单一模型参数修剪，并利用了微调模型变体的能力聚合。将其建模为零阶优化问题也具有新颖性。该方法在实际部署中具有重要意义，因为它能有效降低LLM的计算成本和部署难度，同时保持高水平的性能。"}}
{"id": "2506.20548", "title": "Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks", "authors": ["Manyi Li", "Renshuai Tao", "Yufan Liu", "Chuangchuang Tan", "Haotong Qin", "Bing Li", "Yunchao Wei", "Yao Zhao"], "summary": "With the rapid advancement of deep learning, particularly through generative\nadversarial networks (GANs) and diffusion models (DMs), AI-generated images, or\n``deepfakes\", have become nearly indistinguishable from real ones. These images\nare widely shared across Online Social Networks (OSNs), raising concerns about\ntheir misuse. Existing deepfake detection methods overlook the ``block effects\"\nintroduced by compression in OSNs, which obscure deepfake artifacts, and\nprimarily focus on raw images, rarely encountered in real-world scenarios. To\naddress these challenges, we propose PLADA (Pay Less Attention to Deceptive\nArtifacts), a novel framework designed to tackle the lack of paired data and\nthe ineffective use of compressed images. PLADA consists of two core modules:\nBlock Effect Eraser (B2E), which uses a dual-stage attention mechanism to\nhandle block effects, and Open Data Aggregation (ODA), which processes both\npaired and unpaired data to improve detection. Extensive experiments across 26\ndatasets demonstrate that PLADA achieves a remarkable balance in deepfake\ndetection, outperforming SoTA methods in detecting deepfakes on OSNs, even with\nlimited paired data and compression. More importantly, this work introduces the\n``block effect\" as a critical factor in deepfake detection, providing a robust\nsolution for open-world scenarios. Our code is available at\nhttps://github.com/ManyiLee/PLADA.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.20548v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20548v1", "AI": {"title_translation": "减少对欺骗性伪影的关注：在线社交网络中压缩深度伪造的鲁棒检测", "tldr": "提出PLADA框架，通过处理压缩引起的“块效应”和利用混合数据，有效检测在线社交网络上的压缩深度伪造图像，优于现有方法。", "motivation": "现有深度伪造检测方法忽视了在线社交网络压缩引入的“块效应”，这些效应会模糊深度伪造的伪影；且主要关注原始图像，与真实世界场景不符。此外，还存在配对数据不足的问题。", "method": "提出PLADA（Pay Less Attention to Deceptive Artifacts）框架，旨在解决配对数据缺乏和压缩图像无效利用的问题。PLADA包含两个核心模块：块效应消除器（B2E），它使用双阶段注意力机制处理块效应；以及开放数据聚合（ODA），它处理配对和非配对数据以提高检测性能。", "result": "在26个数据集上进行了广泛实验，PLADA在深度伪造检测方面取得了显著平衡，即使在有限配对数据和压缩条件下，也优于现有最先进（SoTA）方法，尤其是在线社交网络上的深度伪造检测。", "conclusion": "本工作将“块效应”引入为深度伪造检测的关键因素，并为开放世界场景提供了鲁棒的解决方案。", "translation": "随着深度学习的快速发展，特别是通过生成对抗网络（GANs）和扩散模型（DMs），人工智能生成的图像，即“深度伪造”，已变得几乎与真实图像无法区分。这些图像在在线社交网络（OSNs）上广泛传播，引起了对其滥用的担忧。现有的深度伪造检测方法忽视了OSNs中压缩引入的“块效应”，这些效应会模糊深度伪造的伪影，并且主要关注原始图像，这在现实世界场景中很少遇到。为了解决这些挑战，我们提出了PLADA（Pay Less Attention to Deceptive Artifacts），一个旨在解决配对数据缺乏和压缩图像无效利用的新颖框架。PLADA由两个核心模块组成：块效应消除器（B2E），它使用双阶段注意力机制来处理块效应；以及开放数据聚合（ODA），它处理配对和非配对数据以提高检测性能。在26个数据集上进行的广泛实验表明，PLADA在深度伪造检测方面取得了显著的平衡，即使在有限的配对数据和压缩条件下，也能在线社交网络上检测深度伪造，性能优于最先进（SoTA）方法。更重要的是，这项工作将“块效应”引入为深度伪造检测中的一个关键因素，为开放世界场景提供了鲁棒的解决方案。我们的代码可在https://github.com/ManyiLee/PLADA 获取。", "summary": "深度伪造在在线社交网络上广泛传播且难以辨别，现有检测方法忽视了网络压缩产生的“块效应”并主要关注原始图像。本文提出PLADA框架，通过其块效应消除器（B2E）处理压缩伪影，并利用开放数据聚合（ODA）结合配对和非配对数据。实验证明PLADA在检测压缩深度伪造方面优于现有技术，尤其适用于在线社交网络环境，并强调了“块效应”在深度伪造检测中的重要性。", "keywords": "深度伪造检测, 在线社交网络, 块效应, 压缩图像, PLADA", "comments": "本文创新性地关注了在线社交网络中图像压缩引起的“块效应”，这是现有深度伪造检测方法普遍忽略的关键问题。通过引入B2E和ODA模块，PLADA有效提升了在真实世界压缩场景下深度伪造的检测鲁棒性，特别是解决了配对数据稀缺的挑战，对开放世界深度伪造检测具有重要意义。"}}
{"id": "2506.20197", "title": "Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach", "authors": ["Clément L. Canonne", "Yash Pote", "Uddalok Sarkar"], "summary": "A growing fraction of all code is sampled from Large Language Models (LLMs).\nWe investigate the problem of attributing code generated by language models\nusing hypothesis testing to leverage established techniques and guarantees.\nGiven a set of samples $S$ and a suspect model $\\mathcal{L}^*$, our goal is to\nassess the likelihood of $S$ originating from $\\mathcal{L}^*$. Due to the curse\nof dimensionality, this is intractable when only samples from the LLM are\ngiven: to circumvent this, we use both samples and density estimates from the\nLLM, a form of access commonly available.\n  We introduce $\\mathsf{Anubis}$, a zero-shot attribution tool that frames\nattribution as a distribution testing problem. Our experiments on a benchmark\nof code samples show that $\\mathsf{Anubis}$ achieves high AUROC scores (\n$\\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and\nStable-Code using only $\\approx 2000$ samples.", "comment": "16 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.20197v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20197v1", "AI": {"title_translation": "大型语言模型零样本归因：一种分布测试方法", "tldr": "提出Anubis工具，通过分布测试实现LLM生成代码的零样本归因，用少量样本就能高精度区分不同LLM。", "motivation": "随着大型语言模型（LLM）生成代码的普及，识别代码来源变得日益重要。然而，当仅有LLM生成的样本时，由于维度灾难，传统归因方法难以有效执行。", "method": "本文将LLM代码归因问题框架为分布测试问题，并利用假设检验方法。为此，论文引入了名为$\\\\mathsf{Anubis}$的零样本归因工具，该工具通过结合LLM的样本和密度估计来规避仅使用样本时面临的维度灾难挑战。", "result": "在代码样本基准测试中，$\\\\mathsf{Anubis}$能够以高AUROC分数（$\\\\ge0.9$）区分不同的LLM（如DeepSeek-Coder、CodeGemma和Stable-Code），且仅需大约2000个样本。", "conclusion": "$\\\\mathsf{Anubis}$是一种有效且高效的零样本归因工具，能够高精度地识别大型语言模型生成的代码来源，为LLM内容的溯源提供了可行方案。", "translation": "所有代码中，由大型语言模型（LLM）采样的比例正在增长。我们研究了使用假设检验来归因语言模型生成代码的问题，以利用已建立的技术和保证。给定一组样本S和一个可疑模型$\\\\mathcal{L}^*$，我们的目标是评估S源自$\\\\mathcal{L}^*$的可能性。由于维度灾难，当仅给出LLM的样本时，这是难以处理的：为了规避这一点，我们同时使用了LLM的样本和密度估计，这是一种常见的访问形式。我们引入了$\\\\mathsf{Anubis}$，一个零样本归因工具，它将归因问题框架为分布测试问题。我们在代码样本基准测试上的实验表明，$\\\\mathsf{Anubis}$在使用大约2000个样本区分DeepSeek-Coder、CodeGemma和Stable-Code等LLM时，实现了高AUROC分数（$\\\\ge0.9$）。", "summary": "本文针对大型语言模型（LLM）生成代码的归因问题，提出了一种名为$\\\\mathsf{Anubis}$的零样本归因工具。该工具将归因视为分布测试问题，并通过结合LLM的样本和密度估计来克服维度灾难的挑战。实验结果表明，$\\\\mathsf{Anubis}$能够以高AUROC分数（$\\\\ge0.9$）有效区分不同LLM（如DeepSeek-Coder、CodeGemma和Stable-Code）生成的代码，且仅需约2000个样本。", "keywords": "大型语言模型, 代码归因, 零样本, 分布测试, 假设检验", "comments": "这项工作创新性地将LLM代码归因问题转化为分布测试，利用假设检验提供了理论基础，并巧妙地通过结合密度估计解决了维度灾难问题。其零样本特性和高准确性使其在识别LLM生成内容方面具有重要应用潜力，尤其是在代码版权、安全和溯源方面。"}}
{"id": "2506.20204", "title": "Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets", "authors": ["Eduardo Gutierrez Maestro", "Hadi Banaee", "Amy Loutfi"], "summary": "Affective priming exemplifies the challenge of ambiguity in affective\ncomputing. While the community has largely addressed this issue from a\nlabel-based perspective, identifying data points in the sequence affected by\nthe priming effect, the impact of priming on data itself, particularly in\nphysiological signals, remains underexplored. Data affected by priming can lead\nto misclassifications when used in learning models. This study proposes the\nAffective Priming Score (APS), a data-driven method to detect data points\ninfluenced by the priming effect. The APS assigns a score to each data point,\nquantifying the extent to which it is affected by priming. To validate this\nmethod, we apply it to the SEED and SEED-VII datasets, which contain sufficient\ntransitions between emotional events to exhibit priming effects. We train\nmodels with the same configuration using both the original data and\npriming-free sequences. The misclassification rate is significantly reduced\nwhen using priming-free sequences compared to the original data. This work\ncontributes to the broader challenge of ambiguity by identifying and mitigating\npriming effects at the data level, enhancing model robustness, and offering\nvaluable insights for the design and collection of affective computing\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20204v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20204v1", "AI": {"title_translation": "情感启动分数：一种数据驱动的顺序数据集启动检测方法", "tldr": "本文提出情感启动分数（APS），一种数据驱动方法，用于检测和量化顺序数据集中受情感启动效应影响的数据点，并在生理信号数据上验证其能有效降低模型误分类率。", "motivation": "情感计算中，情感启动效应可能导致数据模糊性，进而导致学习模型误分类。现有方法多从标签角度解决，而数据本身（特别是生理信号）受启动影响的问题尚未得到充分探索。", "method": "本研究提出了情感启动分数（APS），这是一种数据驱动的方法，用于检测受启动效应影响的数据点。APS为每个数据点分配一个分数，量化其受启动影响的程度。该方法在SEED和SEED-VII数据集上进行了验证，通过使用原始数据和去启动序列训练模型，并比较其误分类率。", "result": "与使用原始数据相比，使用去启动序列训练的模型显著降低了误分类率。", "conclusion": "本研究通过在数据层面识别和减轻情感启动效应，解决了情感计算中的模糊性挑战，提高了模型鲁棒性，并为情感计算数据集的设计和收集提供了宝贵见解。", "translation": "情感启动体现了情感计算中模糊性带来的挑战。虽然社区已主要从基于标签的角度解决了这个问题，即识别序列中受启动效应影响的数据点，但启动对数据本身（特别是生理信号）的影响仍未得到充分探索。受启动影响的数据在用于学习模型时可能导致错误分类。本研究提出了情感启动分数（APS），这是一种数据驱动的方法，用于检测受启动效应影响的数据点。APS为每个数据点分配一个分数，量化其受启动影响的程度。为了验证该方法，我们将其应用于SEED和SEED-VII数据集，这些数据集包含足够的情绪事件转换以表现出启动效应。我们使用相同配置的模型，分别使用原始数据和去启动序列进行训练。与使用原始数据相比，使用去启动序列时，错误分类率显著降低。这项工作通过在数据层面识别和减轻启动效应，有助于解决更广泛的模糊性挑战，增强了模型的鲁棒性，并为情感计算数据集的设计和收集提供了宝贵见解。", "summary": "本研究提出情感启动分数（APS），这是一种新颖的数据驱动方法，旨在检测并量化顺序数据集中受情感启动效应影响的数据点。针对情感计算中因启动效应导致的数据模糊性和模型误分类问题，APS通过为每个数据点分配一个分数来衡量其受启动影响的程度。在SEED和SEED-VII数据集上的验证结果表明，使用APS处理后的“去启动”数据序列训练模型，可以显著降低分类错误率。该方法有助于提高情感计算模型的鲁棒性，并为数据集的设计和收集提供了新思路，从而解决了情感计算中的模糊性挑战。", "keywords": "情感启动, 数据驱动, 生理信号, 误分类率, 情感计算", "comments": "这项研究的创新之处在于提出了一个数据驱动的、量化情感启动影响的方法（APS），而非传统的基于标签的方法。它直接解决了生理信号数据中长期被忽视的启动效应问题，并通过实验证明了其在降低误分类率、提高模型鲁棒性方面的有效性。这对于提高情感计算模型的准确性和实用性具有重要意义，并为未来数据集的采集和预处理提供了新的视角。"}}
{"id": "2506.20512", "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling", "authors": ["Zengzhi Wang", "Fan Zhou", "Xuefeng Li", "Pengfei Liu"], "summary": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).", "comment": "26 pages; The first three authors contribute to this work equally", "pdf_url": "http://arxiv.org/pdf/2506.20512v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20512v1", "AI": {"title_translation": "OctoThinker：中期训练激励强化学习扩展", "tldr": "该论文研究了中期训练策略如何提高语言模型与强化学习的兼容性，尤其是在推理任务上，并提出了一种名为“Stable-then-Decay”的新策略，从而产生了OctoThinker模型。", "motivation": "不同的基础语言模型在强化学习（RL）后训练期间，尤其是在推理密集型任务上，表现出不同的行为。本研究的动机是深入理解什么使基础语言模型适合强化学习，以开发下一代可扩展的RL基础模型。", "method": "研究了中期训练策略如何影响RL动态，重点关注Qwen和Llama模型家族。具体探究了高质量数学语料库和长链式思考（CoT）推理数据对模型性能和RL结果的影响。在此基础上，提出了一种名为“Stable-then-Decay”的两阶段中期训练策略：首先以恒定学习率训练基础模型2000亿个token，随后在三个以CoT为重点的分支上以学习率衰减的方式训练200亿个token。最终引入了OctoThinker模型家族。", "result": "1. 高质量数学语料库（如MegaMath-Web-Pro）显著提升了基础模型和RL性能。2. 添加问答式数据和长CoT推理示例增强了RL效果，指令数据进一步解锁了这种效果。3. 长CoT虽提高了推理深度，但也可能导致模型响应冗长和RL训练不稳定，强调了数据格式化的重要性。4. 扩展中期训练持续带来更强的下游RL性能。5. OctoThinker模型家族表现出强大的RL兼容性，并缩小了与更RL友好模型家族（如Qwen）的性能差距。", "conclusion": "中期训练策略，特别是结合高质量数学语料库和以CoT为重点的数据，对于开发可扩展的RL基础模型至关重要。本文提出的“Stable-then-Decay”策略和由此产生的OctoThinker模型展示了显著的RL兼容性改进。这项工作旨在为RL时代基础模型的预训练策略提供指导。", "translation": "不同的基础语言模型系列，如Llama和Qwen，在强化学习（RL）后训练期间表现出不同的行为，尤其是在推理密集型任务上。是什么让基础语言模型适合强化学习？深入了解这个问题对于开发下一代可扩展的RL基础模型至关重要。在这项工作中，我们研究了中期训练策略如何塑造RL动态，重点关注两个代表性模型系列：Qwen和Llama。我们的研究揭示了：(1) 高质量的数学语料库，如MegaMath-Web-Pro，显著提高了基础模型和RL性能，而现有替代方案（例如FineMath-4plus）未能做到；(2) 进一步添加问答式数据，特别是长链式思考（CoT）推理示例，增强了RL结果，并且指令数据进一步解锁了这种效果；(3) 虽然长CoT提高了推理深度，但它也可能导致模型响应冗长和RL训练不稳定，这突出了数据格式化的重要性；(4) 扩展中期训练始终能带来更强的下游RL性能。基于这些见解，我们引入了一种两阶段中期训练策略——Stable-then-Decay，其中基础模型首先以恒定学习率训练2000亿个token，然后以学习率衰减的方式在三个以CoT为重点的分支上训练200亿个token。这产生了OctoThinker，一个模型家族，展示出强大的RL兼容性，并缩小了与更RL友好模型家族（即Qwen）的性能差距。我们希望我们的工作能有助于塑造RL时代基础模型的预训练策略。为了支持进一步研究，我们发布了我们的开源模型以及精选的超过700亿个token的数学推理密集型语料库（即MegaMath-Web-Pro-Max）。", "summary": "本论文探讨了中期训练策略如何提升基础语言模型（如Llama和Qwen）与强化学习（RL）的兼容性，尤其是在推理密集型任务上。研究发现，高质量的数学语料库和长链式思考（CoT）推理示例能显著改善RL性能，但CoT数据需要精细格式化以避免冗余和训练不稳定。基于这些发现，论文提出了一种名为“Stable-then-Decay”的两阶段中期训练策略，并由此开发出“OctoThinker”模型家族。OctoThinker模型展现出强大的RL兼容性，有效缩小了与Qwen等更RL友好模型家族的性能差距。为支持后续研究，作者还发布了开源模型和一个大规模的数学推理语料库。", "keywords": "强化学习, 语言模型, 中期训练, 链式思考, 基础模型, OctoThinker", "comments": "该论文深入探讨了预训练/中期训练数据和策略对语言模型RL兼容性的影响，尤其是在复杂推理任务上的创新性。提出的“Stable-then-Decay”策略和“OctoThinker”模型提供了一种切实有效的方法来提升RL兼容性。开源模型和大规模数学推理语料库的发布是对社区的重大贡献。论文中指出的CoT深度与模型冗长/稳定性之间的权衡是一个重要的实际考量。"}}
{"id": "2506.20563", "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation", "authors": ["Lei Zhu", "Jun Zhou", "Rick Siow Mong Goh", "Yong Liu"], "summary": "Vision Transformer has recently gained tremendous popularity in medical image\nsegmentation task due to its superior capability in capturing long-range\ndependencies. However, transformer requires a large amount of labeled data to\nbe effective, which hinders its applicability in annotation scarce\nsemi-supervised learning scenario where only limited labeled data is available.\nState-of-the-art semi-supervised learning methods propose combinatorial\nCNN-Transformer learning to cross teach a transformer with a convolutional\nneural network, which achieves promising results. However, it remains a\nchallenging task to effectively train the transformer with limited labeled\ndata. In this paper, we propose an adversarial masked image modeling method to\nfully unleash the potential of transformer for semi-supervised medical image\nsegmentation. The key challenge in semi-supervised learning with transformer\nlies in the lack of sufficient supervision signal. To this end, we propose to\nconstruct an auxiliary masked domain from original domain with masked image\nmodeling and train the transformer to predict the entire segmentation mask with\nmasked inputs to increase supervision signal. We leverage the original labels\nfrom labeled data and pseudo-labels from unlabeled data to learn the masked\ndomain. To further benefit the original domain from masked domain, we provide a\ntheoretical analysis of our method from a multi-domain learning perspective and\ndevise a novel adversarial training loss to reduce the domain gap between the\noriginal and masked domain, which boosts semi-supervised learning performance.\nWe also extend adversarial masked image modeling to CNN network. Extensive\nexperiments on three public medical image segmentation datasets demonstrate the\neffectiveness of our method, where our method outperforms existing methods\nsignificantly. Our code is publicly available at\nhttps://github.com/zlheui/AdvMIM.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20563v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20563v1", "AI": {"title_translation": "AdvMIM：用于半监督医学图像分割的对抗性掩蔽图像建模", "tldr": "AdvMIM提出了一种对抗性掩蔽图像建模方法，通过增加监督信号和缩小域差距，提升Vision Transformer在有限标注数据下的半监督医学图像分割性能，并在多个数据集上表现优异。", "motivation": "Vision Transformer在医学图像分割中表现出色，但需要大量标注数据。在标注稀缺的半监督学习场景中，现有方法难以有效训练Transformer，缺乏足够的监督信号。", "method": "本文提出AdvMIM，一种对抗性掩蔽图像建模方法，旨在充分释放Transformer在半监督医学图像分割中的潜力。关键在于通过掩蔽图像建模从原始域构建辅助掩蔽域，并训练Transformer预测带有掩蔽输入的完整分割掩码以增加监督信号。方法利用标注数据的原始标签和未标注数据的伪标签来学习掩蔽域。此外，通过多域学习理论分析，设计了一种新颖的对抗性训练损失来减少原始域和掩蔽域之间的域差距，从而提升半监督学习性能。该方法也扩展到了CNN网络。", "result": "在三个公共医学图像分割数据集上进行了广泛实验，结果表明AdvMIM方法有效，并且显著优于现有现有方法。", "conclusion": "AdvMIM通过对抗性掩蔽图像建模有效解决了Transformer在有限标注数据下半监督医学图像分割的挑战，通过增加监督信号和缩小域差距显著提升了性能。", "translation": "Vision Transformer最近因其捕获长距离依赖的卓越能力而在医学图像分割任务中获得了巨大普及。然而，Transformer需要大量的标注数据才能有效，这阻碍了其在标注稀缺的半监督学习场景中的适用性，因为只有有限的标注数据可用。最先进的半监督学习方法提出了组合CNN-Transformer学习，以通过卷积神经网络交叉训练Transformer，这取得了有希望的结果。然而，在有限标注数据下有效训练Transformer仍然是一个具有挑战性的任务。在本文中，我们提出了一种对抗性掩蔽图像建模方法，以充分释放Transformer在半监督医学图像分割中的潜力。Transformer半监督学习中的关键挑战在于缺乏足够的监督信号。为此，我们提出通过掩蔽图像建模从原始域构建一个辅助掩蔽域，并训练Transformer预测带有掩蔽输入的完整分割掩码，以增加监督信号。我们利用标注数据的原始标签和未标注数据的伪标签来学习掩蔽域。为了进一步使原始域受益于掩蔽域，我们从多域学习的角度对我们的方法进行了理论分析，并设计了一种新颖的对抗性训练损失，以减少原始域和掩蔽域之间的域差距，从而提升半监督学习性能。我们还将对抗性掩蔽图像建模扩展到CNN网络。在三个公共医学图像分割数据集上进行的广泛实验证明了我们方法的有效性，我们的方法显著优于现有方法。我们的代码已在https://github.com/zlheui/AdvMIM 公开。", "summary": "本文提出了AdvMIM，一种对抗性掩蔽图像建模方法，旨在解决Vision Transformer在有限标注数据下进行半监督医学图像分割的挑战。该方法通过构建辅助掩蔽域并训练Transformer预测完整分割掩码来增加监督信号。同时，利用对抗性训练损失减少原始域和掩蔽域之间的域差距，从而提升半监督学习性能。实验证明AdvMIM在多个医学图像分割数据集上显著优于现有方法。", "keywords": "医学图像分割, Vision Transformer, 半监督学习, 对抗性学习, 掩蔽图像建模", "comments": "本文的创新点在于将对抗性学习与掩蔽图像建模相结合，有效地解决了Transformer在半监督医学图像分割中数据稀缺的问题。通过增加监督信号和缩小域差距，该方法充分发挥了Transformer的潜力，为医学图像分析领域提供了一个有前景的解决方案。其将方法扩展到CNN网络也增加了其通用性。"}}
{"id": "2506.20235", "title": "Directed Link Prediction using GNN with Local and Global Feature Fusion", "authors": ["Yuyang Zhang", "Xu Shen", "Yu Xie", "Ka-Chun Wong", "Weidun Xie", "Chengbin Peng"], "summary": "Link prediction is a classical problem in graph analysis with many practical\napplications. For directed graphs, recently developed deep learning approaches\ntypically analyze node similarities through contrastive learning and aggregate\nneighborhood information through graph convolutions. In this work, we propose a\nnovel graph neural network (GNN) framework to fuse feature embedding with\ncommunity information. We theoretically demonstrate that such hybrid features\ncan improve the performance of directed link prediction. To utilize such\nfeatures efficiently, we also propose an approach to transform input graphs\ninto directed line graphs so that nodes in the transformed graph can aggregate\nmore information during graph convolutions. Experiments on benchmark datasets\nshow that our approach outperforms the state-of-the-art in most cases when 30%,\n40%, 50%, and 60% of the connected links are used as training data,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20235v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20235v1", "AI": {"title_translation": "有向链接预测：使用GNN融合局部和全局特征", "tldr": "本文提出了一种新的GNN框架，通过融合特征嵌入和社区信息，并结合有向线图转换，提高了有向链接预测的性能，在基准数据集上优于现有技术。", "motivation": "链接预测是图分析中的经典问题，具有许多实际应用。针对有向图，现有深度学习方法通常通过对比学习分析节点相似性并通过图卷积聚合邻域信息，但仍有改进空间。", "method": "提出了一种新的图神经网络（GNN）框架，用于融合特征嵌入和社区信息，并从理论上证明了其能提升有向链接预测性能。为高效利用这些特征，还提出了一种将输入图转换为有向线图的方法，以使图卷积能聚合更多信息。", "result": "在基准数据集上的实验表明，当分别使用30%、40%、50%和60%的连接链接作为训练数据时，该方法在大多数情况下优于现有技术。", "conclusion": "该研究提出的GNN框架通过融合特征嵌入和社区信息，并结合有向线图转换，有效提升了有向图链接预测的性能，并在实验中取得了超越现有技术的成果。", "translation": "链接预测是图分析中的一个经典问题，具有许多实际应用。对于有向图，最近开发的深度学习方法通常通过对比学习分析节点相似性，并通过图卷积聚合邻域信息。在这项工作中，我们提出了一种新颖的图神经网络（GNN）框架，用于融合特征嵌入和社区信息。我们从理论上证明了这种混合特征可以提高有向链接预测的性能。为了有效地利用这些特征，我们还提出了一种将输入图转换为有向线图的方法，以便转换后的图中的节点在图卷积过程中可以聚合更多信息。在基准数据集上的实验表明，当分别使用30%、40%、50%和60%的连接链接作为训练数据时，我们的方法在大多数情况下优于现有技术。", "summary": "本文提出了一种新颖的图神经网络（GNN）框架，旨在解决有向图的链接预测问题。该框架通过融合特征嵌入与社区信息，并理论证明了这种混合特征能提升预测性能。为高效利用这些特征，作者还引入了一种将输入图转换为有向线图的方法，以使图卷积能聚合更多信息。实验结果表明，在不同训练数据比例下，该方法在多个基准数据集上均优于现有最先进技术。", "keywords": "有向链接预测, 图神经网络, 特征融合, 社区信息, 有向线图", "comments": "这篇论文的创新点在于提出了一个结合特征嵌入和社区信息的GNN框架，并通过有向线图转换来增强信息聚合能力，有效解决了有向图的链接预测问题。其理论证明和实验结果都支持了方法的有效性，为有向图分析提供了一个有前景的新方向。"}}
{"id": "2506.20544", "title": "When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs", "authors": ["Ammar Khairi", "Daniel D'souza", "Ye Shen", "Julia Kreutzer", "Sara Hooker"], "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20544v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20544v1", "AI": {"title_translation": "当生活给你样本时：扩展多语言LLM推理计算的好处", "tldr": "本文研究多语言LLM推理计算的扩展，发现现有采样和选择策略在跨语言场景下表现不佳，并提出新的策略，显著提升了多语言多任务场景下的性能，有助于普及欠代表语言的性能提升。", "motivation": "现有LLM推理计算扩展方法主要集中在英语和特定领域（如数学和代码），但缺乏在开放式任务、可形式化验证任务和跨语言场景下的泛化能力。本文旨在解决多语言、多任务设置下开放式生成任务的推理计算扩展问题。", "method": "本文研究了如何在多语言、多任务设置下稳健地扩展开放式生成任务的推理计算。研究发现，基于温度变化的采样策略和选择策略需要适应不同的领域和语言设置。评估了现有的选择方法，并提出了专门针对多语言和多任务推理场景的新型采样和选择策略。", "result": "研究发现，现有在英语中有效的选择策略往往无法泛化到其他语言。本文提出的组合采样和选择方法在m-ArenaHard-v2.0提示上，使8B模型的胜率平均提升了+6.8，甚至超越了Gemini等专有模型。在更大规模的Command-A（111B模型）上，使用本文方法仅用五个样本就比单样本解码在相同基准上取得了+9.0的胜率提升。", "conclusion": "结果强调了在推理计算中需要采用语言和任务感知的方法，旨在实现欠代表语言的性能改进民主化。", "translation": "大型语言模型（LLMs）的最新进展已将重点转向扩展推理时计算，在不重新训练模型的情况下提高性能。一种常见的方法是并行采样多个输出，并从中选择一个作为最终输出。然而，迄今为止的工作主要集中在英语以及少数领域，如数学和代码。相比之下，我们最感兴趣的是能够泛化到开放式任务、形式上可验证任务和跨语言的技术。在这项工作中，我们研究了如何在多语言、多任务设置中，稳健地扩展开放式生成任务的推理时计算。\n\n我们的研究结果表明，基于温度变化的采样策略和选择策略都必须进行调整，以适应不同的领域和多样的语言设置。我们评估了现有的选择方法，发现对英语有效策略通常无法泛化到其他语言。我们提出了专门针对多语言和多任务推理场景的新型采样和选择策略，并表明它们在跨语言和跨任务中取得了显著的收益。特别是，我们结合的采样和选择方法使我们的8B模型在m-ArenaHard-v2.0提示上，相对于Gemini等专有模型，胜率平均提升了+6.8。在更大规模上，配备我们方法的Command-A（111B模型）在相同基准上仅用五个样本就比单样本解码取得了+9.0的胜率提升，以最小的成本实现了显著的增长。我们的结果强调了推理时计算需要采用语言和任务感知的方法，旨在实现欠代表语言的性能改进民主化。", "summary": "本文探讨了在多语言大语言模型中扩展推理计算的益处。针对现有方法主要关注英语和特定领域的问题，作者研究了如何在多语言、多任务环境中稳健地扩展开放式生成任务的推理计算。研究发现，现有的采样和选择策略在跨语言场景中表现不佳。为此，论文提出了新的、专门适应多语言多任务推理场景的采样和选择策略。实验结果表明，这些新策略在多语言多任务基准测试中取得了显著的性能提升，例如在m-ArenaHard-v2.0上，8B模型胜率平均提升6.8%，111B模型胜率提升9.0%，突出了语言和任务感知方法对于普及欠代表语言性能改进的重要性。", "keywords": "多语言LLM, 推理计算, 采样策略, 选择策略, 性能提升", "comments": "这篇论文的创新点在于将LLM推理计算扩展的焦点从单一语言（主要是英语）和特定领域扩展到多语言和多任务场景。通过提出新的采样和选择策略，有效地解决了现有方法在跨语言泛化能力上的不足。其重要性在于，这有助于弥合不同语言之间LLM性能的差距，实现“欠代表语言的性能改进民主化”，对于构建更公平、普惠的AI系统具有重要意义。"}}
{"id": "2506.20567", "title": "Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization", "authors": ["Zhiwang Zhang", "Dong Xu", "Wanli Ouyang", "Chuanqi Tan"], "summary": "In this work, we propose a division-and-summarization (DaS) framework for\ndense video captioning. After partitioning each untrimmed long video as\nmultiple event proposals, where each event proposal consists of a set of short\nvideo segments, we extract visual feature (e.g., C3D feature) from each segment\nand use the existing image/video captioning approach to generate one sentence\ndescription for this segment. Considering that the generated sentences contain\nrich semantic descriptions about the whole event proposal, we formulate the\ndense video captioning task as a visual cue aided sentence summarization\nproblem and propose a new two stage Long Short Term Memory (LSTM) approach\nequipped with a new hierarchical attention mechanism to summarize all generated\nsentences as one descriptive sentence with the aid of visual features.\nSpecifically, the first-stage LSTM network takes all semantic words from the\ngenerated sentences and the visual features from all segments within one event\nproposal as the input, and acts as the encoder to effectively summarize both\nsemantic and visual information related to this event proposal. The\nsecond-stage LSTM network takes the output from the first-stage LSTM network\nand the visual features from all video segments within one event proposal as\nthe input, and acts as the decoder to generate one descriptive sentence for\nthis event proposal. Our comprehensive experiments on the ActivityNet Captions\ndataset demonstrate the effectiveness of our newly proposed DaS framework for\ndense video captioning.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.20567v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20567v1", "AI": {"title_translation": "展示、讲述与总结：使用视觉线索辅助语句摘要的密集视频字幕", "tldr": "本文提出了一种名为DaS的密集视频字幕框架，通过将视频划分为事件提案，为每个片段生成描述，然后使用两阶段LSTM网络结合视觉线索对这些描述进行摘要，以生成事件的最终字幕。", "motivation": "本文旨在解决密集视频字幕任务，即为未剪辑的长视频中的多个事件生成描述性语句。", "method": "本文提出了一种“划分与摘要”（DaS）框架用于密集视频字幕。该方法首先将未剪辑的长视频划分为多个事件提案，每个提案包含一系列短视频片段。接着，从每个片段中提取视觉特征（如C3D特征），并利用现有的图像/视频字幕方法为每个片段生成一个句子描述。然后，将密集视频字幕任务视为一个视觉线索辅助的语句摘要问题。为此，提出了一种新的两阶段长短期记忆（LSTM）方法，该方法配备了新的分层注意力机制。第一阶段的LSTM网络作为编码器，输入所有生成的句子中的语义词以及事件提案中所有片段的视觉特征，以有效总结与该事件提案相关的语义和视觉信息。第二阶段的LSTM网络作为解码器，输入第一阶段LSTM网络的输出以及事件提案中所有视频片段的视觉特征，以生成该事件提案的一个描述性句子。", "result": "在ActivityNet Captions数据集上的综合实验表明，本文提出的DaS框架对于密集视频字幕是有效的。", "conclusion": "本文提出的DaS框架能够有效进行密集视频字幕。", "translation": "在这项工作中，我们提出了一种用于密集视频字幕的划分与摘要（DaS）框架。在将每个未剪辑的长视频划分为多个事件提案后（每个事件提案由一组短视频片段组成），我们从每个片段中提取视觉特征（例如C3D特征），并使用现有的图像/视频字幕方法为该片段生成一个句子描述。考虑到生成的句子包含关于整个事件提案的丰富语义描述，我们将密集视频字幕任务表述为一个视觉线索辅助的语句摘要问题，并提出了一种新的两阶段长短期记忆（LSTM）方法，该方法配备了新的分层注意力机制，以在视觉特征的辅助下将所有生成的句子摘要为一个描述性句子。具体来说，第一阶段的LSTM网络将生成的句子中的所有语义词以及一个事件提案中所有片段的视觉特征作为输入，并充当编码器以有效总结与该事件提案相关的语义和视觉信息。第二阶段的LSTM网络将第一阶段LSTM网络的输出以及一个事件提案中所有视频片段的视觉特征作为输入，并充当解码器以生成该事件提案的一个描述性句子。我们在ActivityNet Captions数据集上的综合实验证明了我们新提出的DaS框架在密集视频字幕方面的有效性。", "summary": "本文提出了一种名为“划分与摘要”（DaS）的新框架，用于密集视频字幕。该框架首先将未剪辑视频分解为事件提案，为每个短视频片段生成初步描述，然后将密集视频字幕任务建模为视觉辅助的语句摘要问题。核心是一个两阶段LSTM网络，结合分层注意力机制，通过整合片段的语义信息和视觉特征，将多个初步描述总结成一个连贯的事件描述。实验结果验证了该框架在ActivityNet Captions数据集上的有效性。", "keywords": "密集视频字幕, 语句摘要, 视觉线索, LSTM, 分层注意力", "comments": "该论文的创新点在于将密集视频字幕任务重新定义为视觉线索辅助的语句摘要问题，并提出了一个新颖的两阶段LSTM框架，结合分层注意力机制来融合视觉和文本信息。这种“先分后总”的策略提供了一种处理长视频复杂性的有效途径，有望提高密集视频字幕的准确性和描述性。"}}
{"id": "2506.20245", "title": "FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data", "authors": ["Yushan Zhao", "Jinyuan He", "Donglai Chen", "Weijie Luo", "Chong Xie", "Ri Zhang", "Yonghong Chen", "Yan Xu"], "summary": "Federated learning (FL) is a decentralized collaborative machine learning\n(ML) technique. It provides a solution to the issues of isolated data islands\nand data privacy leakage in industrial ML practices. One major challenge in FL\nis handling the non-identical and independent distributed (non-IID) data.\nCurrent solutions either focus on constructing an all-powerful global model, or\ncustomizing personalized local models. Few of them can provide both a\nwell-generalized global model and well-performed local models at the same time.\nAdditionally, many FL solutions to the non-IID problem are benefited from\nintroducing public datasets. However, this will also increase the risk of data\nleakage. To tackle the problems, we propose a novel data-free distillation\nframework, Federated Bidirectional Knowledge Distillation (FedBKD).\nSpecifically, we train Generative Adversarial Networks (GAN) for synthetic\ndata. During the GAN training, local models serve as discriminators and their\nparameters are frozen. The synthetic data is then used for bidirectional\ndistillation between global and local models to achieve knowledge interactions\nso that performances for both sides are improved. We conduct extensive\nexperiments on 4 benchmarks under different non-IID settings. The results show\nthat FedBKD achieves SOTA performances in every case.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20245v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20245v1", "AI": {"title_translation": "FedBKD：蒸馏联邦学习以在非独立同分布数据上实现泛化和个性化", "tldr": "联邦学习在处理非独立同分布（non-IID）数据时面临挑战，现有方法通常只关注全局模型或个性化模型，且可能引入公共数据集导致数据泄露。FedBKD提出了一种新颖的无数据蒸馏框架，利用GAN生成合成数据，并进行全局和本地模型间的双向知识蒸馏，从而在非独立同分布数据上同时实现良好的泛化和个性化，并取得了最先进的性能。", "motivation": "联邦学习（FL）在处理非独立同分布（non-IID）数据时面临主要挑战。现有解决方案要么专注于构建一个强大的全局模型，要么定制个性化的本地模型，很少能同时提供泛化良好的全局模型和表现良好的本地模型。此外，许多解决非独立同分布问题的FL解决方案受益于引入公共数据集，但这会增加数据泄露的风险。", "method": "提出了一种新颖的无数据蒸馏框架——联邦双向知识蒸馏（FedBKD）。具体方法是训练生成对抗网络（GAN）来生成合成数据。在GAN训练期间，本地模型充当判别器且其参数被冻结。随后，合成数据用于全局模型和本地模型之间的双向蒸馏，以实现知识交互，从而提高双方的性能。", "result": "在不同非独立同分布设置下的4个基准上进行了广泛的实验。结果表明，FedBKD在所有情况下都取得了最先进（SOTA）的性能。", "conclusion": "FedBKD通过无数据双向知识蒸馏有效解决了联邦学习中非独立同分布数据的挑战，成功地同时实现了全局模型的泛化能力和本地模型的个性化性能，并且无需引入公共数据集，从而降低了数据泄露风险，并在多个基准测试中取得了最先进的性能。", "translation": "联邦学习（FL）是一种去中心化的协作式机器学习（ML）技术。它为工业ML实践中数据孤岛和数据隐私泄露的问题提供了解决方案。FL中的一个主要挑战是处理非独立同分布（non-IID）数据。当前的解决方案要么专注于构建一个全能的全局模型，要么定制个性化的本地模型。很少有解决方案能同时提供一个泛化良好的全局模型和表现良好的本地模型。此外，许多解决非IID问题的FL解决方案受益于引入公共数据集。然而，这也会增加数据泄露的风险。为了解决这些问题，我们提出了一种新颖的无数据蒸馏框架——联邦双向知识蒸馏（FedBKD）。具体来说，我们训练生成对抗网络（GAN）来生成合成数据。在GAN训练期间，本地模型充当判别器，其参数被冻结。然后，合成数据用于全局模型和本地模型之间的双向蒸馏，以实现知识交互，从而提高双方的性能。我们在不同非IID设置下的4个基准上进行了广泛的实验。结果表明，FedBKD在所有情况下都取得了最先进的性能。", "summary": "本文提出了一种名为FedBKD的新型无数据联邦学习框架，旨在解决非独立同分布（non-IID）数据带来的挑战。与现有方法不同，FedBKD致力于同时实现全局模型的泛化能力和本地模型的个性化。它利用生成对抗网络（GAN）合成数据，并在此过程中将本地模型作为冻结的判别器。随后，合成数据用于全局模型和本地模型之间的双向知识蒸馏，以增强两者性能。在四个不同非独立同分布设置的基准测试中，FedBKD均取得了最先进的性能。", "keywords": "联邦学习, 非独立同分布数据, 知识蒸馏, 生成对抗网络, 无数据", "comments": "FedBKD的创新之处在于其无数据双向知识蒸馏方法，该方法利用GAN生成合成数据，有效解决了非独立同分布问题，同时避免了对公共数据集的依赖，从而降低了数据泄露的风险。这种方法使得模型能够同时实现泛化的全局模型和个性化的本地模型。其重要性在于推动联邦学习在处理多样化数据分布的实际和安全应用方面取得进展。"}}
{"id": "2506.20606", "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm", "authors": ["Baixiang Huang", "Zhen Tan", "Haoran Wang", "Zijie Liu", "Dawei Li", "Ali Payani", "Huan Liu", "Tianlong Chen", "Kai Shu"], "summary": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing.", "comment": "Main paper: 9 pages; total: 18 pages (including appendix). Code,\n  data, results, and additional resources are available at:\n  https://model-editing.github.io", "pdf_url": "http://arxiv.org/pdf/2506.20606v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20606v1", "AI": {"title_translation": "模型编辑是一把双刃剑：引导智能体伦理行为走向利他或伤害", "tldr": "LLM智能体存在伦理风险。本文提出行为编辑（Behavior Editing），通过模型编辑来引导智能体伦理行为，并引入BehaviorBench进行评估。结果表明行为编辑能动态调整智能体行为，既能促进道德行为也能诱导有害行为，揭示了其潜力和风险。", "motivation": "部署基于LLM的智能体在高风险领域存在显著的安全和伦理风险，其不道德行为可能导致严重的现实后果。因此，需要有效的方法来引导智能体的伦理行为。", "method": "本文将智能体行为引导框定为模型编辑任务，称之为“行为编辑”（Behavior Editing）。为系统研究和评估该方法，引入了一个基于心理道德理论的多层基准BehaviorBench，支持跨多种场景的智能体行为评估和编辑。", "result": "行为编辑可以动态地在特定场景中将智能体引导至目标行为。它不仅能进行场景特定的局部调整，还能实现智能体全局道德对齐的更广泛转变。行为编辑既可以用于促进伦理和仁慈的行为，也可以反过来诱导有害或恶意的行为。在基于前沿LLM的智能体上的综合评估表明，行为编辑在不同模型和场景中均有效。", "conclusion": "本研究为引导智能体行为提供了一个新范式，并强调了行为编辑的潜力和危险性。", "translation": "基于大型语言模型（LLM）的智能体已在广泛的任务中展现出强大的能力。然而，在高风险领域部署基于LLM的智能体伴随着显著的安全和伦理风险。这些智能体的不道德行为可能直接导致严重的现实世界后果，包括身体伤害和经济损失。为了有效地引导智能体的伦理行为，我们将智能体行为引导框定为模型编辑任务，我们称之为行为编辑（Behavior Editing）。模型编辑是一个新兴的研究领域，它能够在保持LLM整体能力的同时，对其进行精确高效的修改。为了系统地研究和评估这种方法，我们引入了BehaviorBench，一个基于心理道德理论的多层基准。该基准支持跨多种场景的智能体行为评估和编辑，每个层级都引入了更复杂和模糊的场景。我们首先证明了行为编辑可以在特定场景中动态地引导智能体走向目标行为。此外，行为编辑不仅能够实现场景特定的局部调整，还能实现智能体全局道德对齐的更广泛转变。我们证明了行为编辑可以用于促进伦理和仁慈的行为，或者相反地，用于诱导有害或恶意的行为。通过对基于前沿LLM的智能体进行的全面评估，BehaviorBench展示了行为编辑在不同模型和场景中的有效性。我们的发现为引导智能体行为的新范式提供了关键见解，突出了行为编辑的潜力和危险性。", "summary": "本文提出“行为编辑”（Behavior Editing）作为一种通过模型编辑来有效引导基于LLM的智能体伦理行为的方法，以应对智能体在高风险领域部署时的伦理和安全风险。研究引入了BehaviorBench基准来系统评估该方法，并证明行为编辑能够动态、精确地调整智能体行为，既可以促进道德行为，也可以诱导有害行为，揭示了其作为双刃剑的潜力和风险。", "keywords": "模型编辑, 行为编辑, LLM智能体, 伦理行为, BehaviorBench", "comments": "这项工作创新性地将模型编辑技术应用于LLM智能体的伦理行为引导，提供了一种直接且高效的干预手段。BehaviorBench的引入为评估智能体道德行为提供了一个系统且分层的方法。研究揭示了行为编辑的“双刃剑”特性，强调了其在促进AI伦理发展中的巨大潜力，同时也警示了其可能被恶意利用的风险，这对于AI安全和伦理治理具有重要意义。"}}
{"id": "2506.20582", "title": "Causal Representation Learning with Observational Grouping for CXR Classification", "authors": ["Rajat Rasal", "Avinash Kori", "Ben Glocker"], "summary": "Identifiable causal representation learning seeks to uncover the true causal\nrelationships underlying a data generation process. In medical imaging, this\npresents opportunities to improve the generalisability and robustness of\ntask-specific latent features. This work introduces the concept of grouping\nobservations to learn identifiable representations for disease classification\nin chest X-rays via an end-to-end framework. Our experiments demonstrate that\nthese causal representations improve generalisability and robustness across\nmultiple classification tasks when grouping is used to enforce invariance w.r.t\nrace, sex, and imaging views.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20582v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20582v1", "AI": {"title_translation": "因果表征学习结合观察分组用于胸部X光分类", "tldr": "本文提出一种通过观察分组来学习可识别因果表征的方法，用于胸部X光疾病分类，实验证明其能提高模型在种族、性别和影像视图上的泛化性和鲁棒性。", "motivation": "在医学影像中，可识别的因果表征学习能够提高特定任务潜在特征的泛化性和鲁棒性。", "method": "引入观察分组的概念，通过端到端框架学习可识别的因果表征，用于胸部X光疾病分类。通过分组来强制对种族、性别和影像视图的不变性。", "result": "实验证明，当使用分组来强制对种族、性别和影像视图的不变性时，这些因果表征能够提高跨多个分类任务的泛化性和鲁棒性。", "conclusion": "通过观察分组学习的因果表征能够有效提高胸部X光分类模型的泛化性和鲁棒性。", "translation": "可识别的因果表征学习旨在揭示数据生成过程中潜在的真实因果关系。在医学影像中，这为提高特定任务潜在特征的泛化性和鲁棒性提供了机会。这项工作引入了观察分组的概念，通过一个端到端框架来学习可识别的表征，用于胸部X光疾病分类。我们的实验表明，当使用分组来强制对种族、性别和影像视图的不变性时，这些因果表征能够提高跨多个分类任务的泛化性和鲁棒性。", "summary": "本文提出了一种基于观察分组的端到端因果表征学习框架，旨在改善胸部X光（CXR）疾病分类模型的泛化性和鲁棒性。通过对观察数据进行分组，该方法强制模型学习对种族、性别和影像视图不变的因果表征。实验结果表明，这种方法显著提升了模型在多项分类任务上的性能。", "keywords": "因果表征学习, 胸部X光分类, 观察分组, 泛化性, 鲁棒性", "comments": "这项工作通过引入观察分组的概念，为医学影像中的因果表征学习提供了一个新颖的端到端框架。其创新点在于利用分组来强制学习对敏感属性（如种族、性别、影像视图）不变的因果特征，这对于提高医疗AI模型的公平性和泛化能力至关重要。该方法有望解决医学影像数据中常见的分布偏移问题，具有重要的实际应用价值。"}}
{"id": "2506.20251", "title": "Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models", "authors": ["Kejia Chen", "Jiawen Zhang", "Jiacong Hu", "Yu Wang", "Jian Lou", "Zunlei Feng", "Mingli Song"], "summary": "Quantized large language models (LLMs) have gained increasing attention and\nsignificance for enabling deployment in resource-constrained environments.\nHowever, emerging studies on a few calibration dataset-free quantization\nmethods suggest that quantization may compromise the safety capabilities of\nLLMs, underscoring the urgent need for systematic safety evaluations and\neffective mitigation strategies. In this paper, we present comprehensive safety\nevaluations across various mainstream quantization techniques and diverse\ncalibration datasets, utilizing widely accepted safety benchmarks. To address\nthe identified safety vulnerabilities, we propose a quantization-aware safety\npatching framework, Q-resafe, to efficiently restore the safety capabilities of\nquantized LLMs while minimizing any adverse impact on utility. Extensive\nexperimental results demonstrate that Q-resafe successfully re-aligns the\nsafety of quantized LLMs with their pre-quantization counterparts, even under\nchallenging evaluation scenarios. Project page is available at:\nhttps://github.com/Thecommonirin/Qresafe.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.20251v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20251v1", "AI": {"title_translation": "Q-resafe：量化大型语言模型的安全风险评估与量化感知安全补丁", "tldr": "本文提出了Q-resafe框架，用于评估和恢复量化大型语言模型的安全能力，解决了量化可能损害模型安全性的问题。", "motivation": "量化大型语言模型（LLMs）在资源受限环境中部署的重要性日益增加。然而，现有研究表明量化可能会损害LLMs的安全能力，因此急需系统的安全评估和有效的缓解策略。", "method": "本文首先对各种主流量化技术和不同的校准数据集进行了全面的安全评估，使用了广泛接受的安全基准。为解决识别出的安全漏洞，提出了一种量化感知安全补丁框架Q-resafe，旨在高效恢复量化LLMs的安全能力，同时最小化对实用性的不利影响。", "result": "广泛的实验结果表明，Q-resafe成功地使量化LLMs的安全能力与其量化前版本保持一致，即使在具有挑战性的评估场景下也表现良好。", "conclusion": "Q-resafe框架能够有效评估并恢复量化大型语言模型的安全能力，使其在保持实用性的同时，安全性能与未量化模型相当。", "translation": "量化大型语言模型（LLMs）因其在资源受限环境中部署的能力而受到越来越多的关注和重要性。然而，一些关于无校准数据集量化方法的新兴研究表明，量化可能会损害LLMs的安全能力，这凸显了对系统性安全评估和有效缓解策略的迫切需求。在本文中，我们利用广泛接受的安全基准，对各种主流量化技术和不同的校准数据集进行了全面的安全评估。为了解决已识别的安全漏洞，我们提出了一种量化感知安全补丁框架Q-resafe，以高效地恢复量化LLMs的安全能力，同时最大程度地减少对实用性的不利影响。广泛的实验结果表明，Q-resafe成功地使量化LLMs的安全能力与其量化前版本保持一致，即使在具有挑战性的评估场景下也是如此。项目页面：https://github.com/Thecommonirin/Qresafe。", "summary": "本文针对量化大型语言模型（LLMs）可能存在的安全风险问题，首先进行了全面的安全评估，发现量化可能损害LLMs的安全能力。为此，作者提出了Q-resafe框架，这是一种量化感知安全补丁方法，旨在有效恢复量化LLMs的安全能力，并最小化对模型实用性的影响。实验结果证明Q-resafe能够成功地使量化模型的安全性能与原始模型保持一致。", "keywords": "量化大型语言模型, 安全风险, 安全补丁, Q-resafe, 量化感知", "comments": "该论文解决了量化大型语言模型部署中一个关键且日益重要的问题——安全性。其创新点在于提出了一个量化感知安全补丁框架Q-resafe，能够有效地恢复模型的安全能力，同时兼顾实用性，这对于在资源受限环境下安全部署LLMs具有重要意义。该研究填补了量化模型安全评估和缓解策略方面的空白。"}}
{"id": "2506.20639", "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation", "authors": ["Shansan Gong", "Ruixiang Zhang", "Huangjie Zheng", "Jiatao Gu", "Navdeep Jaitly", "Lingpeng Kong", "Yizhe Zhang"], "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2506.20639v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20639v1", "AI": {"title_translation": "DiffuCoder：理解和改进用于代码生成的掩码扩散模型", "tldr": "本文介绍了DiffuCoder，一个7B的扩散大语言模型，并深入研究了其解码行为，发现其与自回归模型的不同之处。此外，提出了一种名为coupled-GRPO的新型采样方案，显著提升了扩散模型在代码生成任务上的性能和训练效率。", "motivation": "当前用于代码生成的扩散大语言模型（dLLMs）的训练和推理机制仍未得到充分探索，需要揭示dLLMs的解码行为并释放其在编码方面的潜力。", "method": "作者系统性地研究了扩散大语言模型的去噪过程和强化学习（RL）方法。他们训练了一个7B的dLLM模型DiffuCoder，使用了1300亿个代码token。在此基础上，提出了coupled-GRPO，一种新颖的采样方案，用于构建用于训练的互补掩码噪声，以减少token对数似然估计的方差并保持训练效率。", "result": "通过对DiffuCoder的解码行为分析，发现dLLMs可以自主决定生成的因果性，无需依赖半自回归解码。同时，增加采样温度不仅使token选择多样化，还使生成顺序多样化，为RL提供了丰富的搜索空间。实验证明，coupled-GRPO显著提升了DiffuCoder在代码生成基准上的性能（EvalPlus上提升4.4%），并减少了解码时对自回归因果关系的依赖。", "conclusion": "这项工作深入洞察了扩散大语言模型生成机制，并提供了一个有效的、扩散原生的强化学习训练框架。", "translation": "扩散大语言模型（dLLM）是自回归（AR）模型的有力替代品，因为它们的去噪模型作用于整个序列。dLLM的全局规划和迭代细化特性对于代码生成特别有用。然而，当前用于编码的dLLM的训练和推理机制仍未得到充分探索。为了揭示dLLM的解码行为并释放其在编码方面的潜力，我们系统地研究了它们的去噪过程和强化学习（RL）方法。我们训练了一个7B的dLLM模型DiffuCoder，使用了1300亿个代码token。以该模型作为测试平台，我们分析了其解码行为，揭示了它与AR模型的不同之处：(1) dLLM可以决定其生成的因果性，而无需依赖半自回归解码；(2) 增加采样温度不仅使token选择多样化，还使其生成顺序多样化。这种多样性为RL的展开创造了丰富的搜索空间。对于RL训练，为了减少token对数似然估计的方差并保持训练效率，我们提出了coupled-GRPO，一种新颖的采样方案，用于构建训练中使用的互补掩码噪声。在我们的实验中，coupled-GRPO显著提升了DiffuCoder在代码生成基准上的性能（EvalPlus上提升4.4%），并减少了解码时对AR因果关系的依赖。我们的工作深入洞察了dLLM生成机制，并提供了一个有效的、扩散原生的RL训练框架。https://github.com/apple/ml-diffucoder。", "summary": "本文介绍了DiffuCoder，一个7B的掩码扩散大语言模型，专门用于代码生成。研究深入分析了扩散模型与自回归模型在解码行为上的差异，发现扩散模型能自主控制生成因果性并提供更多样的生成顺序。为优化强化学习训练，论文提出了一种新的采样方案coupled-GRPO，有效提高了模型在代码生成任务上的表现（EvalPlus提升4.4%），并减少了对传统自回归解码的依赖。这项工作为理解扩散大语言模型提供了新见解，并提出了一个高效的扩散原生强化学习训练框架。", "keywords": "扩散模型, 代码生成, 大语言模型, 强化学习, 掩码扩散", "comments": "本文通过训练一个大型扩散模型DiffuCoder，深入探讨了扩散模型在代码生成领域的潜力。其创新之处在于系统性地分析了扩散模型的解码特性，揭示了其与自回归模型的根本差异，特别是在生成因果性和多样性方面。此外，提出的coupled-GRPO采样方案有效地解决了RL训练中的方差问题，显著提升了模型性能，为扩散模型在代码生成领域的应用提供了实用的优化方法。这项工作对于推动扩散模型在复杂序列生成任务中的发展具有重要意义。"}}
{"id": "2506.20583", "title": "Dense Video Captioning using Graph-based Sentence Summarization", "authors": ["Zhiwang Zhang", "Dong Xu", "Wanli Ouyang", "Luping Zhou"], "summary": "Recently, dense video captioning has made attractive progress in detecting\nand captioning all events in a long untrimmed video. Despite promising results\nwere achieved, most existing methods do not sufficiently explore the scene\nevolution within an event temporal proposal for captioning, and therefore\nperform less satisfactorily when the scenes and objects change over a\nrelatively long proposal. To address this problem, we propose a graph-based\npartition-and-summarization (GPaS) framework for dense video captioning within\ntwo stages. For the ``partition\" stage, a whole event proposal is split into\nshort video segments for captioning at a finer level. For the ``summarization\"\nstage, the generated sentences carrying rich description information for each\nsegment are summarized into one sentence to describe the whole event. We\nparticularly focus on the ``summarization\" stage, and propose a framework that\neffectively exploits the relationship between semantic words for summarization.\nWe achieve this goal by treating semantic words as nodes in a graph and\nlearning their interactions by coupling Graph Convolutional Network (GCN) and\nLong Short Term Memory (LSTM), with the aid of visual cues. Two schemes of\nGCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN\nand LSTM. The effectiveness of our approach is demonstrated via an extensive\ncomparison with the state-of-the-arts methods on the two benchmarks ActivityNet\nCaptions dataset and YouCook II dataset.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.20583v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20583v1", "AI": {"title_translation": "基于图的句子摘要的密集视频字幕", "tldr": "提出一个图基的分割-摘要框架(GPaS)用于密集视频字幕，通过GCN和LSTM结合视觉线索来有效总结视频事件。", "motivation": "现有密集视频字幕方法未能充分探索事件时间提议内的场景演变，导致在场景和对象变化较长的提议中表现不尽如人意。", "method": "提出一个两阶段的图基分割-摘要(GPaS)框架。第一阶段“分割”：将事件提议分割成短视频片段进行细粒度字幕。第二阶段“摘要”：将生成的每个片段的句子总结成一个句子描述整个事件。特别关注摘要阶段，通过将语义词视为图中的节点，结合图卷积网络(GCN)和长短期记忆网络(LSTM)，并借助视觉线索来学习它们的交互，提出了两种GCN-LSTM交互(GLI)模块。", "result": "在ActivityNet Captions和YouCook II数据集上与最先进的方法进行了广泛比较，证明了该方法的有效性。", "conclusion": "该研究提出的图基分割-摘要框架，特别是其在句子摘要阶段结合GCN和LSTM的方法，有效解决了密集视频字幕中场景演变探索不足的问题，并在基准数据集上取得了优异性能。", "translation": "最近，密集视频字幕在检测和字幕长未剪辑视频中的所有事件方面取得了引人注目的进展。尽管取得了可喜的成果，但大多数现有方法未能充分探索事件时间提议内的场景演变，因此当场景和对象在相对较长的提议中发生变化时，表现不尽如人意。为了解决这个问题，我们提出了一个两阶段的图基分割-摘要（GPaS）框架用于密集视频字幕。在“分割”阶段，将整个事件提议分割成短视频片段，以便进行更精细的字幕。在“摘要”阶段，将为每个片段生成的带有丰富描述信息的句子总结成一个句子，以描述整个事件。我们特别关注“摘要”阶段，并提出了一个有效利用语义词之间关系进行摘要的框架。我们通过将语义词视为图中的节点，并借助视觉线索，通过耦合图卷积网络（GCN）和长短期记忆网络（LSTM）来学习它们的交互，从而实现这一目标。提出了两种GCN-LSTM交互（GLI）模块，用于GCN和LSTM的无缝集成。通过在ActivityNet Captions数据集和YouCook II数据集这两个基准上与最先进方法的广泛比较，证明了我们方法的有效性。", "summary": "本文提出了一种名为GPaS的图基分割-摘要框架，用于解决密集视频字幕中现有方法对场景演变探索不足的问题。该框架分为两个阶段：首先将长视频事件提议分割成短片段进行细粒度字幕；然后，在核心的“摘要”阶段，通过构建语义词图并结合GCN和LSTM来总结每个片段的描述，形成对整个事件的单一描述。实验结果表明，该方法在两个主要基准数据集上均优于现有最先进方法。", "keywords": "密集视频字幕, 图神经网络, 句子摘要, GCN, LSTM", "comments": "该论文的创新点在于提出了一个两阶段的图基框架来解决密集视频字幕中长期场景演变的问题，特别是其在摘要阶段将语义词建模为图节点，并结合GCN和LSTM来捕捉词语间的复杂关系，这为视频摘要提供了一个新颖且有效的方法。通过对视觉线索的利用，进一步增强了模型的描述能力。其结构化的“分割-摘要”思路也具有普适性。"}}
{"id": "2506.20253", "title": "Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios", "authors": ["Ben Gerhards", "Nikita Popkov", "Annekatrin König", "Marcel Arpogaus", "Bastian Schäfermeier", "Leonie Riedl", "Stephan Vogt", "Philip Hehlert"], "summary": "Forecasting attracts a lot of research attention in the electricity value\nchain. However, most studies concentrate on short-term forecasting of\ngeneration or consumption with a focus on systems and less on individual\nconsumers. Even more neglected is the topic of long-term forecasting of\nindividual power consumption.\n  Here, we provide an in-depth comparative evaluation of data-driven methods\nfor generating synthetic time series data tailored to energy consumption\nlong-term forecasting. High-fidelity synthetic data is crucial for a wide range\nof applications, including state estimations in energy systems or power grid\nplanning. In this study, we assess and compare the performance of multiple\nstate-of-the-art but less common techniques: a hybrid Wasserstein Generative\nAdversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),\nHidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial\nnormalizing Flows (MABF). We analyze the ability of each method to replicate\nthe temporal dynamics, long-range dependencies, and probabilistic transitions\ncharacteristic of individual energy consumption profiles. Our comparative\nevaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and\nMABF aiding in selecting the most suitable approach for state estimations and\nother energy-related tasks. Our generation and analysis framework aims to\nenhance the accuracy and reliability of synthetic power consumption data while\ngenerating data that fulfills criteria like anonymisation - preserving privacy\nconcerns mitigating risks of specific profiling of single customers. This study\nutilizes an open-source dataset from households in Germany with 15min time\nresolution. The generated synthetic power profiles can readily be used in\napplications like state estimations or consumption forecasting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20253v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20253v1", "AI": {"title_translation": "机器学习方法生成能源消费者时间序列替代数据用于长期预测场景", "tldr": "本研究比较了多种机器学习方法（WGAN、DDPM、HMM、MABF）生成用于能源消费长期预测的合成时间序列数据，旨在提高数据质量和隐私保护。", "motivation": "大多数关于电力预测的研究集中于短期预测，且较少关注个体消费者，特别是长期个体用电预测。高质量的合成数据对于能源系统状态估计和电网规划至关重要。", "method": "研究评估并比较了混合Wasserstein生成对抗网络（WGAN）、去噪扩散概率模型（DDPM）、隐马尔可夫模型（HMM）和掩码自回归伯恩斯坦多项式归一化流（MABF）等多种机器学习方法。这些方法用于生成合成时间序列数据，并分析其复制时间动态、长期依赖性和概率转换的能力。研究使用了德国住户的开源数据集。", "result": "研究分析了WGAN、DDPM、HMM和MABF在复制个体能源消费特征方面的能力，并强调了每种方法的优点和局限性。生成的合成电力曲线可用于状态估计或消费预测等应用。", "conclusion": "本研究的生成和分析框架旨在提高合成电力消费数据的准确性和可靠性，同时生成符合匿名化标准的数据，以保护隐私并降低特定客户画像的风险。比较评估有助于为状态估计和其他能源相关任务选择最合适的方法。", "translation": "电力价值链中的预测吸引了大量研究关注。然而，大多数研究集中于发电或消费的短期预测，侧重于系统而非个体消费者。更受忽视的是个体电力消费的长期预测。\n在此，我们对数据驱动方法进行了深入的比较评估，这些方法用于生成针对能源消费长期预测量身定制的合成时间序列数据。高保真合成数据对于广泛的应用至关重要，包括能源系统中的状态估计或电网规划。在本研究中，我们评估并比较了多种最先进但不太常见的技术：混合Wasserstein生成对抗网络（WGAN）、去噪扩散概率模型（DDPM）、隐马尔可夫模型（HMM）和掩码自回归伯恩斯坦多项式归一化流（MABF）的性能。我们分析了每种方法复制个体能源消费剖面特征的时间动态、长期依赖性和概率转换的能力。我们的比较评估突出了WGAN、DDPM、HMM和MABF的优点和局限性，有助于为状态估计和其他能源相关任务选择最合适的方法。我们的生成和分析框架旨在提高合成电力消费数据的准确性和可靠性，同时生成满足匿名化标准的数据——保护隐私，降低对单个客户进行特定画像的风险。本研究利用了来自德国住户的开源数据集，时间分辨率为15分钟。生成的合成电力剖面可以很容易地用于状态估计或消费预测等应用。", "summary": "本研究致力于解决个体能源消费长期预测中合成数据生成的问题。论文比较评估了WGAN、DDPM、HMM和MABF四种机器学习方法生成高保真合成时间序列数据的能力，这些数据可用于能源系统中的状态估计和电网规划。研究分析了这些方法在复制时间动态、长期依赖性和概率转换方面的表现，并强调了它们的优缺点。最终目标是提供准确、可靠且保护隐私的合成电力消费数据，以支持相关能源应用。", "keywords": "时间序列预测, 合成数据, 机器学习, 能源消费, 长期预测", "comments": "本文的创新点在于其关注了一个被忽视的研究领域：个体能源消费的长期预测，并利用了多种先进的机器学习方法来生成高保真、匿名的合成时间序列数据。这对于解决数据隐私问题同时提供充足的数据用于模型训练和系统规划具有重要意义。比较不同方法的性能，为实际应用提供了有价值的指导。"}}
{"id": "2506.20642", "title": "Memento: Note-Taking for Your Future Self", "authors": ["Chao Wan", "Albert Gong", "Mihir Mishra", "Carl-Leander Henneking", "Claas Beger", "Kilian Q. Weinberger"], "summary": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when\nreasoning must be tightly coupled with retrieval, as in multi-hop question\nanswering. To overcome these limitations, we introduce a prompting strategy\nthat first decomposes a complex question into smaller steps, then dynamically\nconstructs a database of facts using LLMs, and finally pieces these facts\ntogether to solve the question. We show how this three-stage strategy, which we\ncall Memento, can boost the performance of existing prompting strategies across\ndiverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the\nperformance of chain-of-thought (CoT) when all information is provided in\ncontext. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento\nimproves over vanilla CoT-RAG by more than 20 F1 percentage points and over the\nmulti-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the\nchallenging MuSiQue dataset, Memento improves ReAct by more than 3 F1\npercentage points, demonstrating its utility in agentic settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20642v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20642v1", "AI": {"title_translation": "Memento：为未来的自己做笔记", "tldr": "大型语言模型（LLMs）在需要推理与检索紧密结合的任务中表现不佳。本文提出了名为 Memento 的提示策略，通过分解问题、动态构建事实数据库并整合事实来解决此问题。实验表明 Memento 能显著提升现有提示策略在多跳问答和智能体设置中的性能。", "motivation": "大型语言模型（LLMs）擅长纯推理任务，但在推理必须与检索紧密结合（如多跳问答）时会遇到困难。", "method": "本文引入了一种名为 Memento 的三阶段提示策略：首先将复杂问题分解为更小的步骤；然后使用 LLM 动态构建事实数据库；最后将这些事实整合起来解决问题。", "result": "在 9 步 PhantomWiki 基准测试中，当所有信息都在上下文中时，Memento 使思维链（CoT）的性能翻倍。在开放域 2WikiMultiHopQA 上，结合 Memento 的 CoT-RAG 比普通的 CoT-RAG 提高了 20 多个 F1 百分点，比多跳 RAG 基线 IRCoT 提高了 13 多个 F1 百分点。在具有挑战性的 MuSiQue 数据集上，Memento 使 ReAct 提高了 3 多个 F1 百分点。", "conclusion": "Memento 策略能够有效提升现有提示策略在各种不同设置下的性能，尤其是在需要推理与检索紧密结合的任务中，并展示了其在智能体设置中的实用性。", "translation": "大型语言模型（LLMs）擅长纯推理任务，但在推理必须与检索紧密结合时（如多跳问答）会遇到困难。为了克服这些限制，我们引入了一种提示策略，该策略首先将复杂问题分解为更小的步骤，然后使用 LLM 动态构建事实数据库，最后将这些事实整合起来解决问题。我们展示了这种被称为 Memento 的三阶段策略如何能在不同设置下提升现有提示策略的性能。在 9 步 PhantomWiki 基准测试中，当所有信息都在上下文中时，Memento 使思维链（CoT）的性能翻倍。在开放域 2WikiMultiHopQA 上，结合 Memento 的 CoT-RAG 比普通的 CoT-RAG 提高了 20 多个 F1 百分点，比多跳 RAG 基线 IRCoT 提高了 13 多个 F1 百分点。在具有挑战性的 MuSiQue 数据集上，Memento 使 ReAct 提高了 3 多个 F1 百分点，展示了其在智能体设置中的实用性。", "summary": "Memento 是一种新颖的三阶段提示策略，旨在增强大型语言模型（LLMs）在需要推理和检索紧密结合的任务（如多跳问答）中的性能。该策略通过分解复杂问题、动态构建事实数据库并最终整合这些事实来回答问题。实验结果表明，Memento 在 PhantomWiki、2WikiMultiHopQA 和 MuSiQue 等多个基准测试中，显著提升了现有策略（包括思维链和 RAG 变体）的性能，突显了其在问答和智能体环境中的实用性。", "keywords": "大型语言模型, 多跳问答, 提示策略, 检索增强生成, Memento", "comments": "Memento 的创新之处在于其结构化的三阶段方法，有效地弥合了 LLM 推理和检索能力之间的差距，为解决复杂任务提供了一种通用方法。其动态数据库构建是该方法的一个关键亮点，使其能够灵活适应不同的信息需求。"}}
{"id": "2506.20260", "title": "Argumentative Ensembling for Robust Recourse under Model Multiplicity", "authors": ["Junqi Jiang", "Antonio Rago", "Francesco Leofante", "Francesca Toni"], "summary": "In machine learning, it is common to obtain multiple equally performing\nmodels for the same prediction task, e.g., when training neural networks with\ndifferent random seeds. Model multiplicity (MM) is the situation which arises\nwhen these competing models differ in their predictions for the same input, for\nwhich ensembling is often employed to determine an aggregation of the outputs.\nProviding recourse recommendations via counterfactual explanations (CEs) under\nMM thus becomes complex, since the CE may not be valid across all models, i.e.,\nthe CEs are not robust under MM. In this work, we formalise the problem of\nproviding recourse under MM, which we name recourse-aware ensembling (RAE). We\npropose the idea that under MM, CEs for each individual model should be\nconsidered alongside their predictions so that the aggregated prediction and\nrecourse are decided in tandem. Centred around this intuition, we introduce six\ndesirable properties for solutions to this problem. For solving RAE, we propose\na novel argumentative ensembling method which guarantees the robustness of CEs\nunder MM. Specifically, our method leverages computational argumentation to\nexplicitly represent the conflicts between models and counterfactuals regarding\nprediction results and CE validity. It then uses argumentation semantics to\nresolve the conflicts and obtain the final solution, in a manner which is\nparametric to the chosen semantics. Our method also allows for the\nspecification of preferences over the models under MM, allowing further\ncustomisation of the ensemble. In a comprehensive theoretical analysis, we\ncharacterise the behaviour of argumentative ensembling with four different\nargumentation semantics. We then empirically demonstrate the effectiveness of\nour approach in satisfying desirable properties with eight instantiations of\nour method. (Abstract is shortened for arXiv.)", "comment": "arXiv admin note: substantial text overlap with arXiv:2312.15097", "pdf_url": "http://arxiv.org/pdf/2506.20260v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20260v1", "AI": {"title_translation": "模型多样性下稳健追索的论证集成", "tldr": "本文针对模型多样性下反事实解释的稳健性问题，提出了一种新颖的论证集成方法，通过计算论证来解决模型冲突并提供稳健的追索建议。", "motivation": "在机器学习中，当存在多个性能相当的模型（模型多样性）时，通过反事实解释提供追索建议变得复杂，因为反事实解释可能无法在所有模型中都有效，即在模型多样性下缺乏稳健性。", "method": "该研究将问题形式化为“追索感知集成”（RAE），并提出了六个理想属性。它提出了一种新颖的论证集成方法，利用计算论证明确表示模型和反事实之间的冲突，然后使用论证语义解决这些冲突以获得最终解决方案，并允许指定模型偏好。", "result": "通过全面的理论分析，该方法表征了四种不同论证语义下的论证集成行为。经验结果表明，该方法在满足期望属性方面是有效的。", "conclusion": "提出的论证集成方法能够有效解决模型多样性下反事实解释的稳健性问题，提供可靠的追索建议。", "translation": "在机器学习中，对于相同的预测任务，通常会获得多个性能相同的模型，例如使用不同随机种子训练神经网络时。模型多样性（MM）是指这些竞争模型对相同输入预测不同时出现的情况，此时通常采用集成方法来确定输出的聚合。因此，在模型多样性下通过反事实解释（CEs）提供追索建议变得复杂，因为反事实解释可能并非在所有模型中都有效，即反事实解释在模型多样性下不稳健。在这项工作中，我们形式化了在模型多样性下提供追索的问题，我们将其命名为追索感知集成（RAE）。我们提出了一种观点，即在模型多样性下，应将每个模型的反事实解释与其预测一同考虑，以便同时决定聚合预测和追索。围绕这一直觉，我们为该问题的解决方案引入了六个理想属性。为了解决RAE，我们提出了一种新颖的论证集成方法，该方法保证了在模型多样性下反事实解释的稳健性。具体而言，我们的方法利用计算论证明确表示模型和反事实之间关于预测结果和反事实解释有效性的冲突。然后，它使用论证语义解决这些冲突并获得最终解决方案，其方式取决于所选语义的参数。我们的方法还允许指定在模型多样性下对模型的偏好，从而允许进一步定制集成。在全面的理论分析中，我们表征了具有四种不同论证语义的论证集成的行为。然后，我们通过我们方法的八种实例化经验性地证明了我们的方法在满足理想属性方面的有效性。（摘要为arXiv版本缩短）", "summary": "本文解决了机器学习中模型多样性（MM）下反事实解释（CEs）缺乏稳健性的问题，这使得提供可靠的追索建议变得复杂。作者将此问题形式化为追索感知集成（RAE），并提出了一种新颖的论证集成方法。该方法利用计算论证来明确表示模型和反事实之间的冲突，并通过论证语义解决这些冲突，从而确保CEs在MM下的稳健性。理论分析和实证结果均表明了该方法在满足期望属性方面的有效性。", "keywords": "模型多样性, 追索, 反事实解释, 集成学习, 论证", "comments": "本文的创新之处在于首次将计算论证引入到模型集成中，以解决模型多样性下反事实解释的稳健性问题。这种方法能够显式处理模型间的冲突，并允许用户自定义偏好，为可解释AI领域提供了新的思路和工具，对于提升追索建议的可靠性具有重要意义。"}}
{"id": "2506.20666", "title": "Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs", "authors": ["Sonia K. Murthy", "Rosie Zhao", "Jennifer Hu", "Sham Kakade", "Markus Wulfmeier", "Peng Qian", "Tomer Ullman"], "summary": "Navigating everyday social situations often requires juggling conflicting\ngoals, such as conveying a harsh truth, maintaining trust, all while still\nbeing mindful of another person's feelings. These value trade-offs are an\nintegral part of human decision-making and language use, however, current tools\nfor interpreting such dynamic and multi-faceted notions of values in LLMs are\nlimited. In cognitive science, so-called \"cognitive models\" provide formal\naccounts of these trade-offs in humans, by modeling the weighting of a\nspeaker's competing utility functions in choosing an action or utterance. In\nthis work, we use a leading cognitive model of polite speech to interpret the\nextent to which LLMs represent human-like trade-offs. We apply this lens to\nsystematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models, and in\nopen-source models shown to be stronger in mathematical reasoning. Our findings\nfrom LLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. We show that our method\nis responsive to diverse aspects of the rapidly evolving LLM landscape, with\ninsights for forming hypotheses about other high-level behaviors, shaping\ntraining regimes for reasoning models, and better controlling trade-offs\nbetween values during model training.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.20666v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20666v1", "AI": {"title_translation": "你心中有许多狼：使用认知模型解释大型语言模型中的价值权衡", "tldr": "本文利用认知模型（特别是礼貌言语模型）来分析大型语言模型（LLMs）如何进行价值权衡，发现推理模型优先考虑信息效用而非社会效用，且训练动态在早期阶段对这些权衡影响显著。", "motivation": "目前用于解释大型语言模型（LLMs）中动态多面价值概念（如在社交场合中权衡冲突目标）的工具是有限的。认知科学中的“认知模型”提供了正式解释人类这些权衡的方法，因此本研究旨在将此方法应用于LLMs。", "method": "作者使用一个领先的礼貌言语认知模型来解释LLMs中的价值权衡。他们将此方法应用于两种模型设置：前沿黑盒模型中推理“努力”的程度，以及开源模型的强化学习（RL）后训练动态。", "result": "研究结果显示，在推理模型和数学推理能力较强的开源模型中，信息效用高于社会效用。LLMs的训练动态表明，效用值在训练早期就发生了大的变化，并且基础模型和预训练数据的选择具有持续影响，其影响比反馈数据集或对齐方法更为显著。", "conclusion": "该方法对快速发展的大型语言模型领域具有响应性，为形成其他高级行为的假设、塑造推理模型的训练方案以及更好地控制模型训练期间的价值权衡提供了见解。", "translation": "在日常社交场合中，常常需要权衡相互冲突的目标，例如传达一个严酷的事实，同时保持信任，并且仍然顾及他人的感受。这些价值权衡是人类决策和语言使用的组成部分，然而，目前用于解释大型语言模型（LLMs）中这种动态多面价值概念的工具是有限的。在认知科学中，所谓的“认知模型”通过对说话者在选择行动或话语时，其竞争性效用函数进行建模，从而正式解释了人类的这些权衡。在这项工作中，我们使用一个领先的礼貌言语认知模型来解释LLMs在多大程度上代表了类人权衡。我们应用这一视角系统地评估了两种全面模型设置中的价值权衡：前沿黑盒模型中推理“努力”的程度，以及开源模型的RL后训练动态。我们的结果突出表明，在推理模型中以及在数学推理方面表现更强的开源模型中，信息效用高于社会效用。我们从LLMs训练动态中发现，与反馈数据集或对齐方法相比，效用值在训练早期就发生了大的变化，并且基础模型和预训练数据的选择具有持续影响。我们表明，我们的方法对快速发展的大型语言模型领域的各个方面都具有响应性，为形成其他高级行为的假设、塑造推理模型的训练方案以及更好地控制模型训练期间的价值权衡提供了见解。", "summary": "本文旨在解决大型语言模型（LLMs）中价值权衡解释工具的局限性，而这些权衡在人类决策中至关重要。研究采用了一种礼貌言语的认知模型来分析LLMs如何平衡冲突的效用。研究揭示，以推理为重点的LLMs优先考虑信息效用而非社会效用，并且早期的训练阶段、基础模型和预训练数据显著决定了这些价值权衡，为未来的LLM开发和控制提供了见解。", "keywords": "认知模型, 大型语言模型, 价值权衡, 训练动态, 效用函数", "comments": "这篇论文创新性地将传统用于人类行为的认知模型应用于解释LLMs中复杂的价值权衡。这种跨学科方法提供了一个结构化的视角来理解LLM的决策过程，超越了简单的性能指标，为理解其内在的“动机”以及训练如何影响其伦理或社会对齐提供了宝贵的见解。其中关于早期训练和基础数据比对齐方法具有更显著影响的发现，对于未来的LLM开发尤其富有启发性。"}}
{"id": "2506.20588", "title": "TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness", "authors": ["Pritam Mishra", "Coloma Ballester", "Dimosthenis Karatzas"], "summary": "The increasing ubiquity of video content and the corresponding demand for\nefficient access to meaningful information have elevated video summarization\nand video highlights as a vital research area. However, many state-of-the-art\nmethods depend heavily either on supervised annotations or on attention-based\nmodels, which are computationally expensive and brittle in the face of\ndistribution shifts that hinder cross-domain applicability across datasets. We\nintroduce a pioneering self-supervised video summarization model that captures\nboth spatial and temporal dependencies without the overhead of attention, RNNs,\nor transformers. Our framework integrates a novel set of Markov process-driven\nloss metrics and a two-stage self supervised learning paradigm that ensures\nboth performance and efficiency. Our approach achieves state-of-the-art\nperformance on the SUMME and TVSUM datasets, outperforming all existing\nunsupervised methods. It also rivals the best supervised models, demonstrating\nthe potential for efficient, annotation-free architectures. This paves the way\nfor more generalizable video summarization techniques and challenges the\nprevailing reliance on complex architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20588v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20588v1", "AI": {"title_translation": "TRIM：一个最大化时间相对信息和代表性的自监督视频摘要框架", "tldr": "TRIM是一种创新的自监督视频摘要模型，它无需昂贵的注意力机制或RNN/Transformer，通过马尔可夫过程驱动的损失和两阶段学习范式，有效捕获时空依赖性。它在SUMME和TVSUM数据集上实现了最先进的性能，超越了所有无监督方法，并可与最佳监督模型媲美，展示了高效、无需标注的架构潜力。", "motivation": "视频内容的日益普及以及对有效获取有意义信息的需求，使得视频摘要成为一个重要的研究领域。然而，许多现有最先进的方法要么严重依赖监督标注，要么依赖计算成本高昂且在分布变化面前脆弱的注意力模型，这阻碍了跨数据集的领域适用性。", "method": "本文提出了一种名为TRIM的开创性自监督视频摘要模型。该框架无需注意力机制、RNN或Transformer的开销，即可捕获空间和时间依赖性。它整合了一套新颖的马尔可夫过程驱动的损失度量和一个两阶段的自监督学习范式。", "result": "该方法在SUMME和TVSUM数据集上取得了最先进的性能，超越了所有现有的无监督方法。它还与最佳的监督模型相媲美。", "conclusion": "该模型展示了高效、无需标注的架构的潜力，为更通用的视频摘要技术铺平了道路，并挑战了当前对复杂架构的普遍依赖。", "translation": "视频内容的日益普及以及对有效获取有意义信息的需求，使得视频摘要和视频亮点成为一个重要的研究领域。然而，许多最先进的方法要么严重依赖监督标注，要么依赖基于注意力的模型，这些模型计算成本高昂，并且在面临分布变化时表现脆弱，从而阻碍了跨数据集的领域适用性。我们引入了一种开创性的自监督视频摘要模型，该模型无需注意力机制、RNN或Transformer的开销，即可捕获空间和时间依赖性。我们的框架整合了一套新颖的马尔可夫过程驱动的损失度量，以及一个两阶段的自监督学习范式，确保了性能和效率。我们的方法在SUMME和TVSUM数据集上取得了最先进的性能，超越了所有现有的无监督方法。它还与最佳的监督模型相媲美，展示了高效、无需标注的架构的潜力。这为更通用的视频摘要技术铺平了道路，并挑战了当前对复杂架构的普遍依赖。", "summary": "本文提出了一种名为TRIM的自监督视频摘要框架，旨在解决现有监督和基于注意力方法的局限性，这些方法通常计算成本高昂且泛化能力差。TRIM模型通过结合新颖的马尔可夫过程驱动的损失函数和两阶段自监督学习范式，高效地捕获视频的时空依赖性，同时避免了对注意力机制、RNN或Transformer的依赖。实验结果表明，TRIM在SUMME和TVSUM数据集上实现了最先进的性能，不仅超越了所有现有的无监督方法，也与最佳的监督模型相媲美，证明了其在实现高效、无需标注且更通用视频摘要方面的巨大潜力。", "keywords": "视频摘要, 自监督学习, 时间相对信息, 马尔可夫过程, 免标注", "comments": "该论文的创新之处在于其提出的自监督视频摘要框架TRIM，它成功避免了对计算成本高昂的注意力机制、RNN或Transformer模型的依赖，同时仍能达到最先进的性能。其重要性在于，它展示了开发高效、无需标注且具有更好泛化能力的视频摘要技术的巨大潜力，这对于处理不断增长的视频内容而无需大量手动标注或特定领域训练至关重要。"}}
{"id": "2506.20285", "title": "Distilling A Universal Expert from Clustered Federated Learning", "authors": ["Zeqi Leng", "Chunxu Zhang", "Guodong Long", "Riting Xia", "Bo Yang"], "summary": "Clustered Federated Learning (CFL) addresses the challenges posed by non-IID\ndata by training multiple group- or cluster-specific expert models. However,\nexisting methods often overlook the shared information across clusters, which\nrepresents the generalizable knowledge valuable to all participants in the\nFederated Learning (FL) system. To overcome this limitation, this paper\nintroduces a novel FL framework that distills a universal expert model from the\nknowledge of multiple clusters. This universal expert captures globally shared\ninformation across all clients and is subsequently distributed to each client\nas the initialization for the next round of model training. The proposed FL\nframework operates in three iterative steps: (1) local model training at each\nclient, (2) cluster-specific model aggregation, and (3) universal expert\ndistillation. This three-step learning paradigm ensures the preservation of\nfine-grained non-IID characteristics while effectively incorporating shared\nknowledge across clusters. Compared to traditional gradient-based aggregation\nmethods, the distillation-based model aggregation introduces greater\nflexibility in handling model heterogeneity and reduces conflicts among\ncluster-specific experts. Extensive experimental results demonstrate the\nsuperior performance of the proposed method across various scenarios,\nhighlighting its potential to advance the state of CFL by balancing\npersonalized and shared knowledge more effectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20285v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20285v1", "AI": {"title_translation": "从聚类联邦学习中提炼通用专家模型", "tldr": "本文提出了一种新的联邦学习框架，通过从多个聚类专家模型中提炼通用专家模型，以有效利用聚类联邦学习中被忽视的跨集群共享信息，并在实验中表现出优越性能。", "motivation": "现有的聚类联邦学习方法在处理非独立同分布（non-IID）数据时，通过训练多个特定于群组或集群的专家模型，但忽略了集群之间共享的通用知识，这种知识对所有联邦学习参与者都很有价值。为了克服这一限制，本文旨在提炼一个通用专家模型以捕获全局共享信息。", "method": "本文引入了一种新颖的联邦学习框架，该框架从多个集群的知识中提炼出一个通用专家模型。这个通用专家模型捕获了所有客户端的全局共享信息，并作为下一轮模型训练的初始化分发给每个客户端。该框架分三个迭代步骤操作：(1) 每个客户端进行本地模型训练，(2) 特定于集群的模型聚合，以及 (3) 通用专家模型提炼。这种基于蒸馏的模型聚合方法比传统的基于梯度的聚合方法在处理模型异构性方面更灵活，并减少了集群特定专家之间的冲突。", "result": "广泛的实验结果表明，所提出的方法在各种场景下都表现出卓越的性能。", "conclusion": "本文提出的方法通过更有效地平衡个性化和共享知识，有望推进聚类联邦学习的现状。", "translation": "聚类联邦学习（CFL）通过训练多个特定于群组或集群的专家模型来解决非独立同分布（non-IID）数据带来的挑战。然而，现有方法通常忽略了集群之间共享的信息，这些信息代表了对联邦学习（FL）系统中所有参与者都有价值的通用知识。为了克服这一限制，本文引入了一种新颖的联邦学习框架，该框架从多个集群的知识中提炼出一个通用专家模型。这个通用专家模型捕获了所有客户端的全局共享信息，并随后作为下一轮模型训练的初始化分发给每个客户端。所提出的联邦学习框架分三个迭代步骤操作：(1) 每个客户端进行本地模型训练，(2) 特定于集群的模型聚合，以及 (3) 通用专家模型提炼。这种三步学习范式确保了细粒度非独立同分布特征的保留，同时有效地整合了跨集群的共享知识。与传统的基于梯度的聚合方法相比，基于蒸馏的模型聚合在处理模型异构性方面引入了更大的灵活性，并减少了集群特定专家之间的冲突。广泛的实验结果表明，所提出的方法在各种场景下都表现出卓越的性能，突出了其通过更有效地平衡个性化和共享知识来推进聚类联邦学习现状的潜力。", "summary": "本文提出了一种新的联邦学习（FL）框架，旨在解决聚类联邦学习（CFL）中忽视跨集群共享知识的问题。该框架通过从多个集群的知识中提炼出一个“通用专家模型”，该模型捕获全局共享信息并用于客户端的模型初始化。其迭代过程包括本地训练、集群聚合和通用专家提炼。实验证明，该方法在保持非独立同分布特性的同时，有效整合了共享知识，并相较于传统方法在处理模型异构性和减少冲突方面表现出优越的性能。", "keywords": "聚类联邦学习, 通用专家模型, 知识蒸馏, 非独立同分布数据, 模型聚合", "comments": "该论文提出了一种创新的方法来解决聚类联邦学习中共享知识利用不足的问题，通过引入“通用专家模型”的概念，有效地平衡了模型的个性化和全局通用性。其三步迭代学习范式和基于蒸馏的聚合方式，为处理联邦学习中的异构性提供了新的思路，具有重要的理论和实践意义。"}}
{"id": "2506.16571", "title": "Capturing Visualization Design Rationale", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16571v1", "categories": ["cs.HC", "cs.CL"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.16571v1", "AI": {"title_translation": "捕获可视化设计原理", "tldr": "本文介绍了一个新的数据集和方法，用于通过自然语言探测可视化设计原理，利用学生创建的真实世界可视化笔记本，并通过大型语言模型生成和验证问题-答案-原理三元组。", "motivation": "现有的数据可视化自然语言数据集主要侧重于可视化解读，而非理解其编码过程，且多依赖于受控设置和人工构建的问题。本文旨在填补这一空白，通过真实世界的数据和自然语言叙述来探究可视化设计原理。", "method": "本文提出了一个新的数据集和方法。他们利用学生在数据可视化课程中创建的“识字可视化笔记本”作为真实世界可视化和自然语言叙述的独特来源。这些笔记本结合了视觉作品和设计阐述，学生在其中明确了设计决策背后的原理。此外，他们还使用大型语言模型（LLMs）从笔记本中的叙述和阐述中生成并分类问题-答案-原理三元组，并对这些三元组进行仔细验证，最终整理出一个捕捉和提炼学生可视化设计选择及其相应原理的数据集。", "result": "创建了一个捕捉和提炼学生可视化设计选择及其相应原理的新数据集。", "conclusion": "本文成功创建了一个新的数据集和方法，用于通过自然语言深入探究可视化设计原理，这对于理解可视化编码过程和设计决策至关重要。", "translation": "先前的用于数据可视化的自然语言数据集主要关注可视化素养评估、洞察生成以及从自然语言指令生成可视化等任务。这些研究通常依赖于带有专门构建的可视化和人工构造问题的受控设置。因此，它们倾向于优先考虑可视化的解释，侧重于解码可视化而不是理解其编码。在本文中，我们提出了一个新的数据集和方法，用于通过自然语言探测可视化设计原理。我们利用真实世界可视化和自然语言叙述的独特来源：学生作为数据可视化课程一部分创建的识字可视化笔记本。这些笔记本将视觉作品与设计阐述相结合，学生在其中明确了其设计决策背后的原理。我们还使用大型语言模型（LLMs）从笔记本中的叙述和阐述中生成和分类问题-答案-原理三元组。然后，我们仔细验证这些三元组，并整理出一个捕捉和提炼学生可视化设计选择及其相应原理的数据集。", "summary": "本文针对现有数据可视化自然语言数据集侧重于可视化解读而非编码理解的局限性，提出了一种捕获可视化设计原理的新方法和数据集。研究利用学生在数据可视化课程中创建的真实可视化笔记本，这些笔记本包含视觉作品及其设计原理阐述。通过大型语言模型生成并验证问题-答案-原理三元组，最终构建了一个旨在揭示学生可视化设计选择及其背后原理的数据集。", "keywords": "可视化设计原理, 自然语言处理, 数据集, 大型语言模型, 学生笔记本", "comments": "该研究的创新之处在于其独特的数据来源——真实的学生可视化笔记本，以及利用LLMs自动化生成和分类设计原理三元组的方法。这有助于弥补现有研究在理解可视化“编码”而非“解码”方面的不足，为深入分析和教学可视化设计提供了宝贵的资源。"}}
{"id": "2506.20590", "title": "WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration", "authors": ["Chaojun Ni", "Jie Li", "Haoyun Li", "Hengyu Liu", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Boyuan Wang", "Chenxin Li", "Guan Huang", "Wenjun Mei"], "summary": "Interactive 3D scene generation from a single image has gained significant\nattention due to its potential to create immersive virtual worlds. However, a\nkey challenge in current 3D generation methods is the limited explorability,\nwhich cannot render high-quality images during larger maneuvers beyond the\noriginal viewpoint, particularly when attempting to move forward into unseen\nareas. To address this challenge, we propose WonderFree, the first model that\nenables users to interactively generate 3D worlds with the freedom to explore\nfrom arbitrary angles and directions. Specifically, we decouple this challenge\ninto two key subproblems: novel view quality, which addresses visual artifacts\nand floating issues in novel views, and cross-view consistency, which ensures\nspatial consistency across different viewpoints. To enhance rendering quality\nin novel views, we introduce WorldRestorer, a data-driven video restoration\nmodel designed to eliminate floaters and artifacts. In addition, a data\ncollection pipeline is presented to automatically gather training data for\nWorldRestorer, ensuring it can handle scenes with varying styles needed for 3D\nscene generation. Furthermore, to improve cross-view consistency, we propose\nConsistView, a multi-view joint restoration mechanism that simultaneously\nrestores multiple perspectives while maintaining spatiotemporal coherence.\nExperimental results demonstrate that WonderFree not only enhances rendering\nquality across diverse viewpoints but also significantly improves global\ncoherence and consistency. These improvements are confirmed by CLIP-based\nmetrics and a user study showing a 77.20% preference for WonderFree over\nWonderWorld enabling a seamless and immersive 3D exploration experience. The\ncode, model, and data will be publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20590v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20590v1", "AI": {"title_translation": "WonderFree：提升3D场景探索中的新颖视图质量和跨视图一致性", "tldr": "WonderFree是一个新模型，通过提高新颖视图质量和跨视图一致性，解决了从单张图像生成3D场景时探索性受限的问题，实现了自由探索和沉浸式体验。", "motivation": "现有从单张图像生成3D场景的方法在探索性方面存在局限性，无法在超出原始视点的大范围操作中，尤其是在进入未见区域时，渲染高质量图像。", "method": "WonderFree将挑战分解为新颖视图质量和跨视图一致性两个子问题。为了提高新颖视图渲染质量，引入了WorldRestorer，一个数据驱动的视频修复模型，并设计了一个数据收集流水线。为了改善跨视图一致性，提出了ConsistView，一个多视图联合修复机制。", "result": "实验结果表明，WonderFree不仅提高了不同视点下的渲染质量，而且显著改善了全局连贯性和一致性。CLIP指标和用户研究（77.20%的用户偏好WonderFree而非WonderWorld）证实了这些改进。", "conclusion": "WonderFree通过解决新颖视图质量和跨视图一致性问题，实现了无缝和沉浸式的3D探索体验。", "translation": "从单张图像交互式生成3D场景因其创造沉浸式虚拟世界的潜力而受到广泛关注。然而，当前3D生成方法的一个关键挑战是探索性有限，无法在超出原始视点的大范围操作中渲染高质量图像，尤其是在尝试向前进入未见区域时。为了应对这一挑战，我们提出了WonderFree，这是第一个能够让用户自由地从任意角度和方向探索并交互式生成3D世界的模型。具体来说，我们将这一挑战分解为两个关键子问题：新颖视图质量（解决新颖视图中的视觉伪影和漂浮问题）和跨视图一致性（确保不同视点之间的空间一致性）。为了提高新颖视图的渲染质量，我们引入了WorldRestorer，一个数据驱动的视频修复模型，旨在消除漂浮物和伪影。此外，还提出了一个数据收集流水线，用于自动收集WorldRestorer的训练数据，确保其能够处理3D场景生成所需的各种风格的场景。此外，为了提高跨视图一致性，我们提出了ConsistView，一个多视图联合修复机制，它在保持时空连贯性的同时同步修复多个视角。实验结果表明，WonderFree不仅提高了不同视点下的渲染质量，而且显著改善了全局连贯性和一致性。CLIP指标和用户研究证实了这些改进，显示WonderFree比WonderWorld有77.20%的偏好，从而实现了无缝和沉浸式的3D探索体验。代码、模型和数据将公开发布。", "summary": "WonderFree是一个旨在提升从单张图像生成的3D场景探索体验的新模型。它解决了现有方法在处理超出原始视点的大范围移动时出现的低质量渲染和不一致性问题。该模型通过解耦为“新颖视图质量”和“跨视图一致性”两个子问题来应对挑战。它引入了WorldRestorer来改善新颖视图的渲染质量和消除伪影，并提出了ConsistView来确保跨视图的空间和时空连贯性。实验结果和用户研究表明，WonderFree显著提高了渲染质量、全局连贯性和一致性，从而提供了更流畅和沉浸式的3D探索体验。", "keywords": "3D场景生成, 新颖视图合成, 跨视图一致性, 视频修复, 沉浸式探索", "comments": "WonderFree的创新之处在于它是第一个实现用户在3D世界中自由探索的模型，解决了单张图像生成3D场景中的核心痛点——有限的探索性。它通过将复杂问题分解为新颖视图质量和跨视图一致性两个可管理的子问题，并为每个问题提出专门的解决方案（WorldRestorer和ConsistView），展现了巧妙的设计。WorldRestorer利用数据驱动的视频修复技术来消除伪影，而ConsistView则通过多视图联合修复机制确保了空间和时空连贯性，这些都是其提升性能的关键。该研究的重要性在于它极大地扩展了单图像3D生成技术的实用性，为创建更具沉浸感和交互性的虚拟世界铺平了道路。"}}
{"id": "2506.20305", "title": "Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding", "authors": ["Kazuki Yoda", "Kazuhiko Kawamoto", "Hiroshi Kera"], "summary": "The hardness of learning a function that attains a target task relates to its\ninput-sensitivity. For example, image classification tasks are\ninput-insensitive as minor corruptions should not affect the classification\nresults, whereas arithmetic and symbolic computation, which have been recently\nattracting interest, are highly input-sensitive as each input variable connects\nto the computation results. This study presents the first learning-based Quick\nResponse (QR) code decoding and investigates learning functions of medium\nsensitivity. Our experiments reveal that Transformers can successfully decode\nQR codes, even beyond the theoretical error-correction limit, by learning the\nstructure of embedded texts. They generalize from English-rich training data to\nother languages and even random strings. Moreover, we observe that the\nTransformer-based QR decoder focuses on data bits while ignoring\nerror-correction bits, suggesting a decoding mechanism distinct from standard\nQR code readers.", "comment": "17 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.20305v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20305v1", "AI": {"title_translation": "学习中等输入敏感函数：以二维码解码为例", "tldr": "本研究首次提出基于学习的二维码解码方法，并发现Transformer模型能有效解码二维码，甚至超越理论纠错极限。", "motivation": "函数学习的难度与输入敏感度有关。图像分类是输入不敏感的，而算术和符号计算是高度输入敏感的。本研究旨在探索中等输入敏感函数的学习，并以二维码解码作为具体案例。", "method": "本研究提出了首个基于学习的快速响应（QR）码解码方法，并特别使用了Transformer模型进行实验。", "result": "实验表明，Transformer模型能够成功解码QR码，甚至超越了理论纠错极限，通过学习嵌入文本的结构。它们能够从富含英文的训练数据泛化到其他语言甚至随机字符串。此外，观察到基于Transformer的QR解码器专注于数据位而忽略纠错位。", "conclusion": "Transformer模型能够有效地学习和解码中等输入敏感函数，如QR码，其解码机制与标准QR码阅读器不同，并通过学习数据结构超越了传统纠错限制。", "translation": "函数学习的难度与其输入敏感性相关。例如，图像分类任务对输入不敏感，因为轻微的损坏不应影响分类结果；而算术和符号计算，最近备受关注，则对输入高度敏感，因为每个输入变量都与计算结果相关联。本研究首次提出了基于学习的快速响应（QR）码解码方法，并探讨了中等敏感度函数的学习。我们的实验表明，Transformer模型能够成功解码QR码，甚至超越了理论纠错极限，通过学习嵌入文本的结构。它们能够从富含英文的训练数据泛化到其他语言甚至随机字符串。此外，我们观察到基于Transformer的QR解码器专注于数据位而忽略纠错位，这表明其解码机制与标准QR码阅读器不同。", "summary": "本研究探讨了中等输入敏感函数的学习，并以二维码解码为例。首次提出并实现了基于Transformer的二维码解码器，实验证明其能成功解码二维码，甚至超越了传统纠错极限，并能泛化到不同语言。研究还发现，该解码器侧重于数据位而非纠错位，揭示了一种不同于标准解码器的新机制。", "keywords": "输入敏感函数, 二维码解码, Transformer, 机器学习, 纠错码", "comments": "本文的创新点在于首次将机器学习（特别是Transformer模型）应用于QR码解码，并成功超越了传统纠错码的理论极限。这一发现表明深度学习模型在处理中等输入敏感任务方面具有强大潜力，其学习到的解码机制与传统方法不同，可能为未来的数据恢复和编码技术提供新的思路。"}}
{"id": "2506.20599", "title": "SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection", "authors": ["Ji Qi", "Xinchang Zhang", "Dingqi Ye", "Yongjia Ruan", "Xin Guo", "Shaowen Wang", "Haifeng Li"], "summary": "The rapid advancement of generative artificial intelligence is producing fake\nremote sensing imagery (RSI) that is increasingly difficult to detect,\npotentially leading to erroneous intelligence, fake news, and even conspiracy\ntheories. Existing forgery detection methods typically rely on single visual\nfeatures to capture predefined artifacts, such as spatial-domain cues to detect\nforged objects like roads or buildings in RSI, or frequency-domain features to\nidentify artifacts from up-sampling operations in adversarial generative\nnetworks (GANs). However, the nature of artifacts can significantly differ\ndepending on geographic terrain, land cover types, or specific features within\nthe RSI. Moreover, these complex artifacts evolve as generative models become\nmore sophisticated. In short, over-reliance on a single visual cue makes\nexisting forgery detectors struggle to generalize across diverse remote sensing\ndata. This paper proposed a novel forgery detection framework called SFNet,\ndesigned to identify fake images in diverse remote sensing data by leveraging\nspatial and frequency domain features. Specifically, to obtain rich and\ncomprehensive visual information, SFNet employs two independent feature\nextractors to capture spatial and frequency domain features from input RSIs. To\nfully utilize the complementary domain features, the domain feature mapping\nmodule and the hybrid domain feature refinement module(CBAM attention) of SFNet\nare designed to successively align and fuse the multi-domain features while\nsuppressing redundant information. Experiments on three datasets show that\nSFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art\nRS forgery detection methods and exhibits robust generalization capabilities.\nThe code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20599v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20599v1", "AI": {"title_translation": "SFNet: 空间和频域特征融合的遥感图像伪造检测", "tldr": "SFNet通过融合空间和频域特征，显著提高了遥感图像伪造检测的准确性和泛化能力。", "motivation": "随着生成式AI的快速发展，伪造的遥感图像越来越难以检测，可能导致错误情报和虚假信息。现有方法过度依赖单一视觉特征，难以泛化到多样化的遥感数据，且无法应对生成模型日益复杂带来的伪造痕迹演变。", "method": "本文提出了SFNet框架，通过两个独立的特征提取器分别捕获遥感图像的空间和频域特征。然后，设计了域特征映射模块和混合域特征细化模块（CBAM注意力）来对多域特征进行对齐和融合，同时抑制冗余信息，以充分利用互补的领域特征。", "result": "在三个数据集上的实验表明，SFNet比现有最先进的遥感伪造检测方法准确率提高了4%-15.18%，并表现出强大的泛化能力。", "conclusion": "SFNet通过有效融合空间和频域特征，显著提升了遥感图像伪造检测的准确性和泛化能力，为应对日益复杂的伪造挑战提供了有效方案。", "translation": "生成式人工智能的快速发展正在产生越来越难以检测的虚假遥感图像（RSI），可能导致错误情报、虚假新闻甚至阴谋论。现有的伪造检测方法通常依赖单一视觉特征来捕获预定义的伪影，例如利用空间域线索检测遥感图像中伪造的道路或建筑物，或利用频域特征识别对抗生成网络（GANs）中上采样操作产生的伪影。然而，伪影的性质可能因地理地形、土地覆盖类型或遥感图像中的特定特征而显著不同。此外，随着生成模型变得更加复杂，这些复杂的伪影也在不断演变。简而言之，过度依赖单一视觉线索使得现有的伪造检测器难以在多样化的遥感数据中泛化。本文提出了一种新颖的伪造检测框架SFNet，旨在通过利用空间和频域特征来识别多样化遥感数据中的虚假图像。具体而言，为了获得丰富全面的视觉信息，SFNet采用两个独立的特征提取器从输入的遥感图像中捕获空间和频域特征。为了充分利用互补的域特征，SFNet的域特征映射模块和混合域特征细化模块（CBAM注意力）被设计用于顺序对齐和融合多域特征，同时抑制冗余信息。在三个数据集上的实验表明，SFNet比现有最先进的遥感伪造检测方法准确率提高了4%-15.18%，并表现出强大的泛化能力。代码可在https://github.com/GeoX-Lab/RSTI/tree/main/SFNet 获取。", "summary": "针对生成式AI日益复杂的遥感图像伪造检测难题，本文提出了SFNet框架。该框架通过独立的特征提取器同时捕获遥感图像的空间和频域特征，并利用专门设计的模块（域特征映射与混合域特征细化）对这些互补特征进行对齐和融合，以抑制冗余并增强信息利用。实验结果表明，SFNet在准确率和泛化能力上均显著优于现有方法。", "keywords": "遥感图像伪造检测, 空间域特征, 频域特征, 特征融合, 深度学习", "comments": "本文的创新点在于有效地融合了遥感图像的空间和频域特征，克服了现有方法过度依赖单一特征导致泛化能力差的局限性。通过引入独立的特征提取器和精巧的特征融合模块，SFNet能够捕获更全面、更鲁棒的伪造痕迹，从而显著提升了在多样化遥感数据上的检测性能。这对于应对日益先进的生成式AI带来的伪造挑战具有重要意义。"}}
{"id": "2506.20307", "title": "Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration", "authors": ["Heyang Zhao", "Xingrui Yu", "David M. Bossens", "Ivor W. Tsang", "Quanquan Gu"], "summary": "Imitation learning is a central problem in reinforcement learning where the\ngoal is to learn a policy that mimics the expert's behavior. In practice, it is\noften challenging to learn the expert policy from a limited number of\ndemonstrations accurately due to the complexity of the state space. Moreover,\nit is essential to explore the environment and collect data to achieve\nbeyond-expert performance. To overcome these challenges, we propose a novel\nimitation learning algorithm called Imitation Learning with Double Exploration\n(ILDE), which implements exploration in two aspects: (1) optimistic policy\noptimization via an exploration bonus that rewards state-action pairs with high\nuncertainty to potentially improve the convergence to the expert policy, and\n(2) curiosity-driven exploration of the states that deviate from the\ndemonstration trajectories to potentially yield beyond-expert performance.\nEmpirically, we demonstrate that ILDE outperforms the state-of-the-art\nimitation learning algorithms in terms of sample efficiency and achieves\nbeyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations\nthan in previous work. We also provide a theoretical justification of ILDE as\nan uncertainty-regularized policy optimization method with optimistic\nexploration, leading to a regret growing sublinearly in the number of episodes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20307v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20307v1", "AI": {"title_translation": "超越专家表现的有限演示：双重探索的有效模仿学习", "tldr": "ILDE通过双重探索在有限演示下实现了超越专家表现的模仿学习。", "motivation": "在模仿学习中，从有限演示中准确学习专家策略具有挑战性，且需要探索环境以实现超越专家表现。", "method": "本文提出了一种名为双重探索模仿学习（ILDE）的新型算法，通过两个方面实现探索：1) 通过探索奖励对高不确定性状态-动作对进行乐观策略优化，以提高对专家策略的收敛性；2) 好奇心驱动的对偏离演示轨迹的状态进行探索，以产生超越专家表现。", "result": "经验上，ILDE在样本效率方面优于最先进的模仿学习算法，并在Atari和MuJoCo任务上以更少的演示实现了超越专家表现。理论上，ILDE作为一种具有乐观探索的不确定性正则化策略优化方法，其遗憾值随episode数量次线性增长。", "conclusion": "ILDE通过其独特的双重探索机制，有效解决了在有限演示下模仿学习的挑战，显著提高了样本效率并实现了超越专家水平的性能。", "translation": "模仿学习是强化学习中的一个核心问题，其目标是学习一个模仿专家行为的策略。在实践中，由于状态空间的复杂性，从有限数量的演示中准确学习专家策略通常具有挑战性。此外，探索环境并收集数据对于实现超越专家表现至关重要。为了克服这些挑战，我们提出了一种新颖的模仿学习算法，称为“双重探索模仿学习”（ILDE），它在两个方面实现了探索：(1) 通过探索奖励对高不确定性状态-动作对进行乐观策略优化，以潜在地提高对专家策略的收敛性；(2) 好奇心驱动的对偏离演示轨迹的状态进行探索，以潜在地产生超越专家表现。经验上，我们证明ILDE在样本效率方面优于最先进的模仿学习算法，并在Atari和MuJoCo任务上以比以前工作更少的演示实现了超越专家表现。我们还提供了ILDE的理论证明，将其作为一种具有乐观探索的不确定性正则化策略优化方法，导致遗憾值随episode数量次线性增长。", "summary": "本文提出了一种名为ILDE的新型模仿学习算法，旨在解决有限演示下学习专家策略的挑战并实现超越专家表现。ILDE通过结合乐观策略优化（奖励高不确定性状态-动作对）和好奇心驱动探索（探索偏离演示轨迹的状态）实现双重探索。实验证明，ILDE在样本效率和性能上优于现有算法，并在Atari和MuJoCo任务上以更少演示取得了超越专家表现。理论分析也支持其有效性。", "keywords": "模仿学习, 双重探索, 样本效率, 超越专家表现, 强化学习", "comments": "本文创新性地引入了双重探索机制到模仿学习中，特别是结合了乐观策略优化和好奇心驱动探索，有效解决了有限演示下的性能瓶颈，并实现了超越专家表现，具有重要的实践意义和理论价值。"}}
{"id": "2506.20601", "title": "Video Perception Models for 3D Scene Synthesis", "authors": ["Rui Huang", "Guangyao Zhai", "Zuria Bauer", "Marc Pollefeys", "Federico Tombari", "Leonidas Guibas", "Gao Huang", "Francis Engelmann"], "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20601v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20601v1", "AI": {"title_translation": "视频感知模型用于三维场景合成", "tldr": "VIPScene是一个利用视频生成模型常识知识来合成高真实感和结构一致性三维场景的新框架，并引入FPVScore进行评估。", "motivation": "传统的三维场景合成需要专业知识和大量人工，自动化该过程对建筑设计、机器人仿真、虚拟现实和游戏等领域大有裨益。现有方法（LLM或图像生成）在三维空间推理和多视角一致性方面存在局限性。", "method": "本文提出了VIPScene框架，该框架利用视频生成模型中编码的三维物理世界常识知识，以确保场景布局的连贯性和对象跨视图的一致性。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈三维重建和开放词汇感知模型，对场景中每个对象进行语义和几何分析。此外，为了更精确的分析，引入了第一人称视角分数（FPVScore），利用连续第一人称视角来利用多模态大型语言模型的推理能力，用于连贯性和合理性评估。", "result": "大量实验表明，VIPScene显著优于现有方法，并在不同场景中表现出良好的泛化能力。", "conclusion": "VIPScene通过利用视频生成模型的常识知识，解决了现有三维场景合成方法在真实感和结构一致性方面的局局限性，实现了高质量的三维场景合成，并在实验中取得了优异表现。", "translation": "传统上，三维场景合成需要专业知识和大量人工。自动化这一过程将极大地造福建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的三维场景合成方法通常依赖于大型语言模型（LLMs）的常识推理或现代图像生成模型的强大视觉先验。然而，当前的LLMs在三维空间推理能力方面表现有限，这限制了它们生成真实和连贯三维场景的能力。同时，基于图像生成的方法常受限于视角选择和多视角不一致性。在这项工作中，我们提出了用于三维场景合成的视频感知模型（VIPScene），这是一个利用视频生成模型中编码的三维物理世界常识知识来确保连贯场景布局和跨视图一致对象放置的新颖框架。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈三维重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这使得具有高真实感和结构一致性的灵活场景合成成为可能。为了更精确的分析，我们进一步引入了第一人称视角分数（FPVScore）用于连贯性和合理性评估，利用连续第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene显著优于现有方法，并在不同场景中表现出良好的泛化能力。代码将发布。", "summary": "本文提出了VIPScene，一个用于三维场景合成的新颖框架，它利用视频生成模型中编码的三维物理世界常识知识来解决现有LLM和图像生成方法在三维空间推理和多视角一致性方面的局限。VIPScene整合了视频生成、三维重建和开放词汇感知，支持文本和图像提示，能够生成高真实感和结构一致性的三维场景。此外，引入了FPVScore来评估场景的连贯性和合理性。实验证明VIPScene优于现有方法并具有良好的泛化能力。", "keywords": "三维场景合成, 视频感知模型, 视频生成, 3D重建, 多模态LLM", "comments": "VIPScene的创新之处在于利用视频生成模型中蕴含的三维物理世界常识知识来解决三维场景合成中的连贯性和一致性问题，这区别于传统依赖LLM或单一图像生成的方法。引入FPVScore作为新的评估指标也提升了分析的精确性。该方法对于自动化三维场景生成具有重要意义，尤其是在需要高真实感和结构一致性的应用领域。"}}
{"id": "2506.20323", "title": "Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach", "authors": ["Saundarya Subramaniam", "Shalini Majumdar", "Shantanu Nadar", "Kaustubh Kulkarni"], "summary": "This research presents the development of an Artificial Intelligence (AI) -\ndriven crop disease detection system designed to assist farmers in rural areas\nwith limited resources. We aim to compare different deep learning models for a\ncomparative analysis, focusing on their efficacy in transfer learning. By\nleveraging deep learning models, including EfficientNet, ResNet101,\nMobileNetV2, and our custom CNN, which achieved a validation accuracy of\n95.76%, the system effectively classifies plant diseases. This research\ndemonstrates the potential of transfer learning in reshaping agricultural\npractices, improving crop health management, and supporting sustainable farming\nin rural environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20323v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20323v1", "AI": {"title_translation": "深度学习模型在作物病害检测中的比较分析：一种迁移学习方法", "tldr": "本研究开发了一个AI驱动的作物病害检测系统，旨在帮助资源有限的农村地区农民。通过比较EfficientNet、ResNet101、MobileNetV2和自定义CNN等不同深度学习模型在迁移学习中的效用，系统成功分类了植物病害，其中自定义CNN达到了95.76%的验证准确率。", "motivation": "旨在开发一个人工智能驱动的作物病害检测系统，以帮助资源有限的农村地区农民，改善作物健康管理，并支持可持续农业。", "method": "研究开发了一个人工智能（AI）驱动的作物病害检测系统，并对EfficientNet、ResNet101、MobileNetV2以及自定义CNN等深度学习模型进行了比较分析，重点关注它们在迁移学习中的效用。", "result": "该系统有效地分类了植物病害，其中自定义CNN模型取得了95.76%的验证准确率。", "conclusion": "本研究证明了迁移学习在重塑农业实践、改善作物健康管理和支持农村环境可持续农业方面的巨大潜力。", "translation": "本研究旨在开发一个人工智能（AI）驱动的作物病害检测系统，旨在帮助资源有限的农村地区农民。我们旨在比较不同深度学习模型，进行比较分析，重点关注它们在迁移学习中的效用。通过利用包括EfficientNet、ResNet101、MobileNetV2和我们自定义的CNN（验证准确率达到95.76%）在内的深度学习模型，该系统有效地对植物病害进行分类。这项研究展示了迁移学习在重塑农业实践、改善作物健康管理以及支持农村环境可持续农业方面的潜力。", "summary": "本研究开发了一个基于人工智能的作物病害检测系统，旨在帮助资源有限的农村地区农民。通过比较EfficientNet、ResNet101、MobileNetV2和自定义CNN等深度学习模型在迁移学习中的表现，系统实现了植物病害的有效分类，其中自定义CNN的验证准确率达到95.76%。研究强调了迁移学习在农业实践、作物健康管理和可持续农业中的应用潜力。", "keywords": "作物病害检测, 深度学习, 迁移学习, 农业AI, 可持续农业", "comments": "该研究通过比较多种深度学习模型并应用迁移学习来解决作物病害检测问题，对于资源有限的农村地区具有重要的实际应用价值。自定义CNN模型取得的高准确率显示了其有效性。该方法有望改善农业生产效率和可持续性。"}}
{"id": "2506.20616", "title": "Shape2Animal: Creative Animal Generation from Natural Silhouettes", "authors": ["Quoc-Duy Tran", "Anh-Tuan Vo", "Dinh-Khoi Vo", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Humans possess a unique ability to perceive meaningful patterns in ambiguous\nstimuli, a cognitive phenomenon known as pareidolia. This paper introduces\nShape2Animal framework to mimics this imaginative capacity by reinterpreting\nnatural object silhouettes, such as clouds, stones, or flames, as plausible\nanimal forms. Our automated framework first performs open-vocabulary\nsegmentation to extract object silhouette and interprets semantically\nappropriate animal concepts using vision-language models. It then synthesizes\nan animal image that conforms to the input shape, leveraging text-to-image\ndiffusion model and seamlessly blends it into the original scene to generate\nvisually coherent and spatially consistent compositions. We evaluated\nShape2Animal on a diverse set of real-world inputs, demonstrating its\nrobustness and creative potential. Our Shape2Animal can offer new opportunities\nfor visual storytelling, educational content, digital art, and interactive\nmedia design. Our project page is here: https://shape2image.github.io", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20616v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20616v1", "AI": {"title_translation": "形状到动物：从自然轮廓创意生成动物", "tldr": "Shape2Animal是一个模仿人类空想性错视能力的框架，能将自然物体的轮廓（如云朵、石头）重新解释并生成符合形状的动物图像，并无缝融入原场景。", "motivation": "模仿人类将模糊刺激感知为有意义模式的空想性错视能力，并将其应用于从自然物体轮廓生成动物图像。", "method": "Shape2Animal框架首先执行开放词汇分割以提取物体轮廓，然后使用视觉-语言模型解释语义上合适的动物概念。接着，它利用文本到图像扩散模型合成符合输入形状的动物图像，并将其无缝融入原始场景，以生成视觉连贯且空间一致的构图。", "result": "在多样化的真实世界输入上进行了评估，展示了其鲁棒性和创造潜力。", "conclusion": "Shape2Animal可以为视觉叙事、教育内容、数字艺术和交互式媒体设计提供新机会。", "translation": "人类拥有一种独特的能力，能够从模糊的刺激中感知有意义的模式，这种认知现象被称为空想性错视。本文介绍了Shape2Animal框架，旨在模仿这种想象能力，通过将自然物体（如云朵、石头或火焰）的轮廓重新解释为可信的动物形态。我们的自动化框架首先执行开放词汇分割以提取物体轮廓，并使用视觉-语言模型解释语义上合适的动物概念。然后，它利用文本到图像扩散模型合成符合输入形状的动物图像，并将其无缝融入原始场景，以生成视觉连贯且空间一致的构图。我们在多样化的真实世界输入上评估了Shape2Animal，展示了其鲁棒性和创造潜力。我们的Shape2Animal可以为视觉叙事、教育内容、数字艺术和交互式媒体设计提供新机会。我们的项目页面在此：https://shape2image.github.io", "summary": "Shape2Animal是一个创新的AI框架，旨在模仿人类的空想性错视能力，将自然物体的轮廓（如云、石）转化为逼真的动物形象。该框架结合了开放词汇分割、视觉-语言模型和文本到图像扩散模型，能够从给定轮廓中提取语义概念，并生成符合形状的动物图像，最终无缝融入原始场景。该系统在多样化真实世界输入上表现出鲁棒性和创造力，在视觉叙事、教育和数字艺术等领域具有潜在应用价值。", "keywords": "空想性错视, 图像生成, 轮廓, 扩散模型, 视觉-语言模型", "comments": "该论文引入的Shape2Animal框架具有创新性，通过结合计算机视觉和生成模型，成功模拟了人类的空想性错视现象，将模糊的自然轮廓转化为具体的动物形象。其亮点在于利用开放词汇分割和视觉-语言模型理解语义，并结合扩散模型进行图像合成，实现视觉连贯性和空间一致性。这为数字艺术、视觉叙事和教育内容等领域提供了新的创意工具和可能性。"}}
{"id": "2506.20324", "title": "Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning", "authors": ["Torben Berndt", "Benjamin Walker", "Tiexin Qin", "Jan Stühmer", "Andrey Kormilitzin"], "summary": "Dynamic graphs exhibit complex temporal dynamics due to the interplay between\nevolving node features and changing network structures. Recently, Graph Neural\nControlled Differential Equations (Graph Neural CDEs) successfully adapted\nNeural CDEs from paths on Euclidean domains to paths on graph domains. Building\non this foundation, we introduce Permutation Equivariant Neural Graph CDEs,\nwhich project Graph Neural CDEs onto permutation equivariant function spaces.\nThis significantly reduces the model's parameter count without compromising\nrepresentational power, resulting in more efficient training and improved\ngeneralisation. We empirically demonstrate the advantages of our approach\nthrough experiments on simulated dynamical systems and real-world tasks,\nshowing improved performance in both interpolation and extrapolation scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20324v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20324v1", "AI": {"title_translation": "动态图表示学习的置换等变神经受控微分方程", "tldr": "本文提出了置换等变神经图CDEs，通过将图神经CDEs投影到置换等变函数空间，显著减少了模型参数，提高了动态图表示学习的训练效率和泛化能力，并在实验中表现出优异性能。", "motivation": "动态图由于节点特征演变和网络结构变化，表现出复杂的时间动态性。尽管图神经受控微分方程（Graph Neural CDEs）已成功应用于图域，但模型参数数量可能较大，影响训练效率和泛化能力。", "method": "本文引入了置换等变神经图受控微分方程（Permutation Equivariant Neural Graph CDEs），通过将Graph Neural CDEs投影到置换等变函数空间，旨在显著减少模型参数数量，同时不损害其表示能力。", "result": "实验结果表明，该方法显著减少了模型的参数数量，同时保持了表示能力，实现了更高效的训练和改进的泛化。在模拟动力系统和真实世界任务中，该方法在插值和外推场景下均表现出改进的性能。", "conclusion": "通过引入置换等变性，本文提出的神经图受控微分方程在动态图表示学习中实现了参数效率和性能的显著提升，尤其在处理复杂时间动态和提高模型泛化能力方面表现出色。", "translation": "动态图由于演变的节点特征和变化的網絡結構之間的相互作用，表現出複雜的時間動態。最近，圖神經受控微分方程（Graph Neural CDEs）成功地將歐幾里得域路徑上的神經CDEs應用於圖域路徑。在此基礎上，我們引入了置換等變神經圖CDEs，它將圖神經CDEs投影到置換等變函數空間。這顯著減少了模型的參數數量，同時不損害表示能力，從而實現了更高效的訓練和改進的泛化。我們通過在模擬動力系統和真實世界任務上的實驗，經驗性地證明了我們方法的優勢，顯示出在插值和外推場景中性能的提高。", "summary": "本文针对动态图表示学习，提出了一种名为置换等变神经图受控微分方程（Permutation Equivariant Neural Graph CDEs）的新模型。该模型在现有Graph Neural CDEs的基础上，通过引入置换等变性，将模型投影到置换等变函数空间，从而显著减少了参数数量，同时保持了强大的表示能力。实验结果验证了该方法在提高训练效率、增强泛化能力方面的优势，并在模拟动力系统和真实世界任务的插值与外推场景中均展现出卓越的性能。", "keywords": "动态图, 图表示学习, 神经受控微分方程, 置换等变性, Graph Neural CDEs", "comments": "该论文的创新点在于将置换等变性引入到图神经受控微分方程中，有效地解决了现有模型参数量大的问题，同时提升了模型的训练效率和泛化能力。这对于处理复杂动态图数据具有重要意义，尤其是在需要高效学习和良好泛化性能的场景下。"}}
{"id": "2506.20638", "title": "Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects", "authors": ["Clément Forray", "Pauline Delporte", "Nicolas Delaygue", "Florence Genin", "Dawa Derksen"], "summary": "Obtaining a better knowledge of the current state and behavior of objects\norbiting Earth has proven to be essential for a range of applications such as\nactive debris removal, in-orbit maintenance, or anomaly detection. 3D models\nrepresent a valuable source of information in the field of Space Situational\nAwareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to\nperform 3D reconstruction of non-cooperative space objects from simulated\nimages. This scenario is challenging for NeRF models due to unusual camera\ncharacteristics and environmental conditions : mono-chromatic images, unknown\nobject orientation, limited viewing angles, absence of diffuse lighting etc. In\nthis work we focus primarly on the joint optimization of camera poses alongside\nthe NeRF. Our experimental results show that the most accurate 3D\nreconstruction is achieved when training with successive images one-by-one. We\nestimate camera poses by optimizing an uniform rotation and use regularization\nto prevent successive poses from being too far apart.", "comment": "accepted for CVPR 2025 NFBCC workshop", "pdf_url": "http://arxiv.org/pdf/2506.20638v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20638v1", "AI": {"title_translation": "非合作空间目标姿态估计与三维神经重建", "tldr": "本文利用神经辐射场（NeRF）对非合作空间目标进行三维重建，并解决了太空场景中NeRF面临的挑战，通过联合优化相机姿态和NeRF，实现了高精度的三维重建。", "motivation": "获取地球轨道物体当前状态和行为的知识对于主动碎片清除、在轨维护或异常检测等应用至关重要。三维模型在空间态势感知（SSA）领域是宝贵的信息来源。", "method": "本文利用神经辐射场（NeRF）从模拟图像中对非合作空间目标进行三维重建。主要关注相机姿态与NeRF的联合优化。通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距过远。", "result": "实验结果表明，通过逐一使用连续图像进行训练时，可以实现最精确的三维重建。", "conclusion": "通过联合优化相机姿态和NeRF，并采用逐一连续图像训练的方法，可以有效地在具有挑战性的太空场景下对非合作空间目标进行高精度的三维重建。", "translation": "获取地球轨道物体当前状态和行为的知识对于主动碎片清除、在轨维护或异常检测等一系列应用至关重要。三维模型在空间态势感知（SSA）领域代表着宝贵的信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间目标进行三维重建。对于NeRF模型来说，这种场景具有挑战性，因为相机特性和环境条件异常：单色图像、未知物体方向、有限视角、缺乏漫射照明等。在这项工作中，我们主要关注相机姿态与NeRF的联合优化。我们的实验结果表明，通过逐一使用连续图像进行训练时，可以实现最精确的三维重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距过远。", "summary": "本文提出了一种利用神经辐射场（NeRF）对非合作空间目标进行三维重建的方法，以支持空间态势感知（SSA）应用。针对单色图像、未知方向、有限视角等太空环境带来的挑战，研究重点是相机姿态与NeRF的联合优化。实验结果表明，通过逐一连续图像训练，可以实现最精确的三维重建，并通过优化均匀旋转和正则化来有效估计相机姿态。", "keywords": "神经辐射场, 三维重建, 姿态估计, 非合作空间目标, 空间态势感知", "comments": "本文的创新之处在于将NeRF应用于极具挑战性的非合作空间目标的三维重建任务，并特别关注了太空环境中独特的相机特性和光照条件。通过联合优化相机姿态和NeRF，并引入逐一连续图像训练和姿态正则化，有效地提升了重建精度，为空间态势感知领域提供了有价值的技术支持。其局限性可能在于依赖模拟图像，实际应用中可能需要进一步验证其在真实数据上的表现。"}}
{"id": "2506.20329", "title": "Producer-Fairness in Sequential Bundle Recommendation", "authors": ["Alexandre Rio", "Marta Soare", "Sihem Amer-Yahia"], "summary": "We address fairness in the context of sequential bundle recommendation, where\nusers are served in turn with sets of relevant and compatible items. Motivated\nby real-world scenarios, we formalize producer-fairness, that seeks to achieve\ndesired exposure of different item groups across users in a recommendation\nsession. Our formulation combines naturally with building high quality bundles.\nOur problem is solved in real time as users arrive. We propose an exact\nsolution that caters to small instances of our problem. We then examine two\nheuristics, quality-first and fairness-first, and an adaptive variant that\ndetermines on-the-fly the right balance between bundle fairness and quality.\nOur experiments on three real-world datasets underscore the strengths and\nlimitations of each solution and demonstrate their efficacy in providing fair\nbundle recommendations without compromising bundle quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20329v1", "categories": ["cs.LG", "cs.IR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20329v1", "AI": {"title_translation": "顺序捆绑推荐中的生产者公平性", "tldr": "本文研究了顺序捆绑推荐中的生产者公平性问题，提出了精确解、启发式方法和自适应变体，并在真实数据集上验证了其在不牺牲质量的情况下实现公平推荐的有效性。", "motivation": "在顺序捆绑推荐中，为用户提供相关且兼容的商品集合时，需要解决生产者公平性问题，即在推荐会话中实现不同商品组在用户间的期望曝光度，这源于现实世界场景的需求。", "method": "针对顺序捆绑推荐中的生产者公平性问题，本文提出了一种精确解决方案，适用于小规模问题实例。此外，还研究了两种启发式方法：质量优先和公平性优先，以及一种能够实时平衡捆绑公平性和质量的自适应变体。", "result": "在三个真实世界数据集上的实验表明，所提出的每种解决方案都有其优点和局限性，并且它们能够有效地提供公平的捆绑推荐，同时不损害捆绑质量。", "conclusion": "本文提出的解决方案能够在不牺牲捆绑质量的前提下，实现顺序捆绑推荐中的生产者公平性。", "translation": "我们解决了顺序捆绑推荐中的公平性问题，即用户依次获得相关且兼容的商品集合。受现实世界场景的启发，我们形式化了生产者公平性，旨在推荐会话中实现不同商品组在用户间的期望曝光度。我们的公式与构建高质量捆绑包自然结合。我们的问题在用户到来时实时解决。我们提出了一种精确解，适用于我们问题的小实例。然后，我们检查了两种启发式方法：质量优先和公平性优先，以及一种能够实时确定捆绑公平性和质量之间适当平衡的自适应变体。我们在三个真实世界数据集上的实验强调了每种解决方案的优点和局限性，并证明了它们在不损害捆绑质量的情况下提供公平捆绑推荐的有效性。", "summary": "本文研究了顺序捆绑推荐中的生产者公平性，旨在确保不同商品组的曝光度。作者提出了一个结合高质量捆绑的实时解决方案，包括针对小规模问题的精确解、两种启发式方法（质量优先和公平性优先）以及一个自适应变体。实验结果表明，这些方法能在不牺牲捆绑质量的情况下有效实现公平的捆绑推荐。", "keywords": "顺序捆绑推荐, 生产者公平性, 实时推荐, 启发式算法, 公平性与质量权衡", "comments": "这篇论文解决了推荐系统中的一个重要且实际的问题：生产者公平性，这对于平台生态系统的健康发展至关重要。其创新点在于将生产者公平性形式化，并提出了多种解决方案，包括精确解和实用的启发式方法，特别是自适应变体的提出，体现了对实际应用中质量与公平权衡的考量。"}}
{"id": "2506.20649", "title": "Disentangled representations of microscopy images", "authors": ["Jacopo Dapueto", "Vito Paolo Pastore", "Nicoletta Noceti", "Francesca Odone"], "summary": "Microscopy image analysis is fundamental for different applications, from\ndiagnosis to synthetic engineering and environmental monitoring. Modern\nacquisition systems have granted the possibility to acquire an escalating\namount of images, requiring a consequent development of a large collection of\ndeep learning-based automatic image analysis methods. Although deep neural\nnetworks have demonstrated great performance in this field, interpretability,\nan essential requirement for microscopy image analysis, remains an open\nchallenge.\n  This work proposes a Disentangled Representation Learning (DRL) methodology\nto enhance model interpretability for microscopy image classification.\nExploiting benchmark datasets from three different microscopic image domains\n(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based\non transferring a representation learnt from synthetic data, can provide a good\ntrade-off between accuracy and interpretability in this domain.", "comment": "Published in: International Joint Conference on Neural Networks\n  (IJCNN 2025). Project page:\n  https://github.com/JacopoDapueto/disentangled_microscopy", "pdf_url": "http://arxiv.org/pdf/2506.20649v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20649v1", "AI": {"title_translation": "显微图像的解耦表示", "tldr": "本文提出一种解耦表示学习（DRL）方法，通过从合成数据中学习表示来提高显微图像分类模型的解释性，并在准确性和可解释性之间取得良好平衡。", "motivation": "显微图像分析在诊断、合成工程和环境监测等领域至关重要。现代采集系统产生了大量图像，需要自动分析方法。尽管深度学习在该领域表现出色，但模型解释性不足是一个开放的挑战，而这对于显微图像分析是必需的。", "method": "本文提出了一种解耦表示学习（DRL）方法，旨在增强显微图像分类模型的解释性。该方法基于从合成数据中学习到的表示进行迁移，并在浮游生物、酵母液泡和人体细胞这三个不同的显微图像领域基准数据集上进行了验证。", "result": "研究表明，基于从合成数据中学习到的表示的DRL框架，在显微图像分析领域可以在准确性和可解释性之间提供良好的权衡。", "conclusion": "通过引入解耦表示学习，可以有效提高显微图像分类模型的解释性，同时保持良好的分类准确性，从而解决了深度学习模型在显微图像分析中解释性不足的挑战。", "translation": "显微图像分析是诊断、合成工程和环境监测等不同应用的基础。现代采集系统使得获取越来越多的图像成为可能，需要相应地开发大量基于深度学习的自动图像分析方法。尽管深度神经网络在该领域表现出卓越的性能，但解释性作为显微图像分析的基本要求，仍然是一个开放的挑战。这项工作提出了一种解耦表示学习（DRL）方法，旨在增强显微图像分类模型的解释性。利用来自三个不同显微图像领域（浮游生物、酵母液泡和人体细胞）的基准数据集，我们展示了基于从合成数据中学习到的表示的DRL框架，如何在该领域中在准确性和可解释性之间提供良好的权衡。", "summary": "本文针对显微图像分析中深度学习模型解释性不足的问题，提出了一种解耦表示学习（DRL）方法。该方法通过从合成数据中学习表示并进行迁移，在浮游生物、酵母液泡和人体细胞等多种显微图像数据集上验证了其有效性，结果表明DRL框架能在模型准确性和可解释性之间取得良好平衡。", "keywords": "解耦表示学习, 显微图像分析, 模型解释性, 深度学习, 合成数据", "comments": "这篇论文的创新点在于将解耦表示学习应用于显微图像分析领域，解决了深度学习模型在该领域解释性不足的关键问题。通过利用合成数据进行表示学习并迁移，提供了一种提高模型可信度和实用性的有效途径，对于需要高解释性的医疗诊断和科研应用具有重要意义。该方法在保证准确性的同时提升了可解释性，具有较强的实际应用价值。"}}
{"id": "2506.20347", "title": "On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data", "authors": ["Malik Shahid Sultan", "Hernando Ombao"], "summary": "Granger Causality (GC) offers an elegant statistical framework to study the\nassociation between multivariate time series data. Linear Vector Autoregressive\nmodels (VAR) though have nice interpretation properties but have limited\npractical application due to underlying assumptions on the kind of associations\nthat can be captured by these models. Numerous attempts have already been made\nin the literature that exploit the functional approximation power of Deep\nNeural Networks (DNNs) for the task of GC estimation. These methods however\ntreat GC as a variable selection problem. We present a novel paradigm for\napproaching GC. We present this idea that GC is essentially linked with\nprediction and if a deep learning model is used to model the time series\ncollectively or jointly, a well regularized model may learn the true granger\ncausal structure from the data, given that there is enough training data. We\npropose to uncover the learned GC structure by comparing the model uncertainty\nor distribution of the residuals when the past of everything is used as\ncompared to the one where a specific time series component is dropped from the\nmodel. We also compare the effect of input layer dropout on the ability of a\nneural network to learn granger causality from the data. We show that a well\nregularized model infact can learn the true GC structure from the data without\nexplicitly adding terms in the loss function that guide the model to select\nvariables or perform sparse regression.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20347v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20347v1", "AI": {"title_translation": "深度神经网络学习多元时间序列数据中格兰杰因果关系的能力研究", "tldr": "本文提出了一种新的范式，认为格兰杰因果关系与预测本质相关。通过比较模型不确定性或残差分布，即使没有显式地在损失函数中添加变量选择项，一个正则化良好的深度学习模型也能够从数据中学习到真正的格兰杰因果结构。", "motivation": "传统的线性向量自回归模型（VAR）在捕捉时间序列数据关联方面存在局限性。虽然现有方法利用深度神经网络（DNNs）估计格兰杰因果关系，但它们通常将其视为一个变量选择问题，这限制了其应用。本文旨在提出一种新的、更自然的格兰杰因果关系学习范式。", "method": "本文提出将格兰杰因果关系（GC）与预测相关联。如果一个深度学习模型被用于集体或联合建模时间序列，一个正则化良好的模型在有足够训练数据的情况下可以学习到真实的格兰杰因果结构。通过比较当使用所有过去数据和移除特定时间序列分量后模型的模型不确定性或残差分布来揭示学习到的GC结构。同时，还比较了输入层dropout对网络学习GC能力的影响。", "result": "研究表明，一个正则化良好的模型实际上可以从数据中学习到真正的格兰杰因果结构，而无需在损失函数中明确添加指导模型选择变量或执行稀疏回归的项。通过比较模型不确定性或残差分布，可以揭示这种学习到的GC结构。", "conclusion": "本研究表明，深度神经网络在适当的正则化下，能够通过其预测能力隐式地学习到多元时间序列数据中的格兰杰因果结构，而无需将其明确地视为一个变量选择问题。", "translation": "格兰杰因果关系（GC）提供了一个优雅的统计框架来研究多元时间序列数据之间的关联。线性向量自回归模型（VAR）虽然具有良好的解释性，但由于其对模型能捕捉的关联类型存在潜在假设，其实际应用受到限制。文献中已经有许多尝试利用深度神经网络（DNNs）的函数逼近能力来估计GC。然而，这些方法通常将GC视为一个变量选择问题。我们提出了一种处理GC的新范式。我们提出GC本质上与预测相关联，如果一个深度学习模型被用于集体或联合建模时间序列，一个正则化良好的模型可能从数据中学习到真实的格兰杰因果结构，前提是有足够的训练数据。我们建议通过比较当使用所有过去数据和移除特定时间序列分量后模型的模型不确定性或残差分布来揭示学习到的GC结构。我们还比较了输入层dropout对神经网络从数据中学习格兰杰因果关系能力的影响。我们表明，一个正则化良好的模型实际上可以从数据中学习到真正的GC结构，而无需在损失函数中明确添加指导模型选择变量或执行稀疏回归的项。", "summary": "本文提出了一种新的格兰杰因果关系（GC）学习范式，认为GC与预测本质相关。与现有将GC视为变量选择问题的方法不同，本文指出一个正则化良好的深度神经网络在集体建模时间序列时，能够从数据中学习到真正的GC结构。通过比较模型在完整输入和移除特定时间序列分量后的不确定性或残差分布，可以揭示这种隐式学习到的GC结构。研究结果表明，这种方法无需显式地在损失函数中添加变量选择或稀疏回归项，也能有效学习GC。", "keywords": "格兰杰因果关系, 深度神经网络, 时间序列, 预测, 模型不确定性", "comments": "这项研究的创新之处在于提出了将格兰杰因果关系与深度学习模型的预测能力相结合的新视角，摆脱了传统上将其视为变量选择问题的限制。这为利用深度神经网络处理因果推断问题开辟了新的途径，可能在复杂非线性时间序列分析中展现出更强的实用性。其重要性在于提供了一种更自然、更强大的方法来发现时间序列中的因果关联，尤其是在数据量充足且关系复杂的场景下。"}}
{"id": "2506.20670", "title": "MMSearch-R1: Incentivizing LMMs to Search", "authors": ["Jinming Wu", "Zihao Deng", "Wei Li", "Yiding Liu", "Bo You", "Bo Li", "Zejun Ma", "Ziwei Liu"], "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.", "comment": "Code: https://github.com/EvolvingLMMs-Lab/multimodal-search-r1", "pdf_url": "http://arxiv.org/pdf/2506.20670v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20670v1", "AI": {"title_translation": "MMSearch-R1：激励大型多模态模型进行搜索", "tldr": "MMSearch-R1是一个端到端强化学习框架，使大型多模态模型（LMMs）能够按需、多轮地在互联网上进行多模态搜索，解决了现有方法搜索效率低的问题，并在VQA任务上取得了显著提升，同时减少了搜索调用。", "motivation": "在现实世界中部署大型多模态模型（LMMs）需要访问外部知识源，因为现实世界的信息复杂且动态。现有方法（如检索增强生成RAG和提示工程搜索代理）依赖于僵化的管道，通常导致低效或过度的搜索行为。", "method": "我们提出了MMSearch-R1，这是第一个端到端强化学习框架，它使LMMs能够按需、多轮地在真实的互联网环境中执行搜索。该框架整合了图像和文本搜索工具，允许模型在结果导向的奖励和搜索惩罚的指导下，判断何时以及如何调用它们。为了支持训练，我们通过半自动化管道收集了一个多模态搜索VQA数据集，该数据集涵盖了多样化的视觉和文本知识需求，并策划了一个搜索平衡的子集，包含需要搜索和无需搜索的样本。", "result": "在知识密集型和信息搜索型VQA任务上的大量实验表明，我们的模型不仅优于相同模型大小的基于RAG的基线，而且在性能上与更大的基于RAG的模型相当，同时将搜索调用减少了30%以上。我们进一步分析了关键的实证发现，为推进多模态搜索研究提供了可行的见解。", "conclusion": "MMSearch-R1通过强化学习实现了LMMs的按需、多轮多模态搜索，有效解决了现有方法的效率问题。其在VQA任务上的优异表现和搜索调用量的显著减少，证明了其在现实世界应用中的潜力，并为未来的多模态搜索研究提供了宝贵见解。", "translation": "在现实世界中稳健部署大型多模态模型（LMMs）需要访问外部知识源，鉴于现实世界信息的复杂性和动态性。现有方法，如检索增强生成（RAG）和提示工程搜索代理，依赖于僵化的管道，常常导致低效或过度的搜索行为。我们提出了MMSearch-R1，这是第一个端到端强化学习框架，它使LMMs能够按需、多轮地在真实的互联网环境中执行搜索。我们的框架整合了图像和文本搜索工具，允许模型在结果导向的奖励和搜索惩罚的指导下，判断何时以及如何调用它们。为了支持训练，我们通过半自动化管道收集了一个多模态搜索VQA数据集，该数据集涵盖了多样化的视觉和文本知识需求，并策划了一个搜索平衡的子集，包含需要搜索和无需搜索的样本，这对于塑造高效和按需的搜索行为至关重要。在知识密集型和信息搜索型VQA任务上的大量实验表明，我们的模型不仅优于相同模型大小的基于RAG的基线，而且在性能上与更大的基于RAG的模型相当，同时将搜索调用减少了30%以上。我们进一步分析了关键的实证发现，为推进多模态搜索研究提供了可行的见解。", "summary": "MMSearch-R1是一个创新的强化学习框架，旨在解决大型多模态模型（LMMs）在现实世界中进行外部知识搜索时效率低下和过度搜索的问题。该框架首次实现了LMMs的端到端、按需、多轮多模态搜索，集成了图像和文本搜索工具，并通过带有搜索惩罚的结果导向奖励进行引导。为支持训练，研究团队构建了一个多模态搜索VQA数据集。实验证明，MMSearch-R1在知识密集型VQA任务上不仅超越了相同规模的RAG基线模型，还能匹配更大RAG模型的性能，同时显著减少了超过30%的搜索调用，为多模态搜索研究提供了新的方向和见解。", "keywords": "大型多模态模型, 强化学习, 多模态搜索, VQA, 检索增强生成", "comments": "MMSearch-R1的创新之处在于首次将强化学习引入LMMs的多模态搜索，实现了端到端的按需、多轮搜索，这显著提升了搜索效率并减少了不必要的调用。其构建的搜索平衡数据集对于训练高效的搜索行为至关重要。这项工作为LMMs在复杂现实世界环境中的实际部署提供了新的范式，具有重要的研究价值和应用前景。"}}
{"id": "2506.20353", "title": "DipSVD: Dual-importance Protected SVD for Efficient LLM Compression", "authors": ["Xuan Ding", "Rui Sun", "Yunjian Zhang", "Xiu Yan", "Yueqi Zhou", "Kaihao Huang", "Suzhong Fu", "Chuanlong Xie", "Yao Zhu"], "summary": "The ever-increasing computational demands and deployment costs of large\nlanguage models (LLMs) have spurred numerous compressing methods. Compared to\nquantization and unstructured pruning, SVD compression offers superior hardware\ncompatibility and theoretical guarantees. However, existing SVD-based methods\nfocus on the overall discrepancy between the original and compressed matrices\nwhile overlooking the protection of critical components within the matrix,\nwhich leads to inferior performance in the compressed models. This paper\nproposes a dual-level importance protection mechanism to enhance SVD-based\ncompression methods: (1) local importance protection: preserving the most\ncritical singular vectors within each weight matrix through channel-weighted\ndata whitening; and (2) global importance protection: enabling less important\nlayers to bear a greater portion of the compression burden through either a\nheuristic or optimization-based approach, thereby minimizing the impact of\ncompression on critical layers. Extensive experiments demonstrate that DipSVD\noutperforms existing SVD-based compression approaches across multiple\nbenchmarks, achieving superior model performance especially at high model\ncompression ratios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20353v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20353v1", "AI": {"title_translation": "DipSVD：用于高效LLM压缩的双重要性保护SVD", "tldr": "DipSVD是一种新的LLM压缩方法，通过双层重要性保护机制改进了SVD，在保持模型性能的同时实现更高的压缩率。", "motivation": "大型语言模型（LLM）日益增长的计算需求和部署成本促使了许多压缩方法的发展。现有基于SVD的压缩方法忽略了对矩阵中关键组件的保护，导致压缩模型性能不佳。", "method": "本文提出了一个双层重要性保护机制来增强基于SVD的压缩方法：1) 局部重要性保护：通过通道加权数据白化保留每个权重矩阵中最关键的奇异向量；2) 全局重要性保护：通过启发式或基于优化的方法，使不那么重要的层承担更大比例的压缩负担，从而最大限度地减少压缩对关键层的影响。", "result": "大量实验表明，DipSVD在多个基准测试中优于现有基于SVD的压缩方法，尤其是在高模型压缩比下实现了卓越的模型性能。", "conclusion": "DipSVD通过引入双层重要性保护机制，显著提升了SVD在LLM压缩中的性能，特别是在高压缩率下表现出色。", "translation": "大型语言模型（LLM）日益增长的计算需求和部署成本促使了许多压缩方法的发展。与量化和非结构化剪枝相比，SVD压缩提供了卓越的硬件兼容性和理论保证。然而，现有的基于SVD的方法侧重于原始矩阵和压缩矩阵之间的整体差异，而忽略了对矩阵中关键组件的保护，这导致压缩模型的性能较差。本文提出了一种双层重要性保护机制来增强基于SVD的压缩方法：(1) 局部重要性保护：通过通道加权数据白化保留每个权重矩阵中最关键的奇异向量；(2) 全局重要性保护：通过启发式或基于优化的方法，使不那么重要的层承担更大比例的压缩负担，从而最大限度地减少压缩对关键层的影响。大量实验表明，DipSVD在多个基准测试中优于现有基于SVD的压缩方法，尤其是在高模型压缩比下实现了卓越的模型性能。", "summary": "本文提出了一种名为DipSVD的LLM压缩方法，旨在解决现有SVD压缩方法忽略关键组件保护的问题。DipSVD引入了双层重要性保护机制：局部保护通过通道加权数据白化保留关键奇异向量；全局保护通过分配更多压缩负担给不重要层来保护关键层。实验证明，DipSVD在多种基准测试中优于现有SVD方法，尤其在高压缩率下性能更优。", "keywords": "LLM压缩, SVD, 双重要性保护, 模型压缩, 奇异值分解", "comments": "DipSVD的创新点在于提出了双层重要性保护机制，解决了SVD压缩中对关键信息保护不足的问题。这对于在资源受限环境中部署大型语言模型具有重要意义，尤其是在需要高压缩率的场景下，其性能提升显著。"}}
{"id": "2506.19960", "title": "An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking", "authors": ["Adam Foster", "Zeno Schätzle", "P. Bernát Szabó", "Lixue Cheng", "Jonas Köhler", "Gino Cassella", "Nicholas Gao", "Jiawei Li", "Frank Noé", "Jan Hermann"], "summary": "Reliable description of bond breaking remains a major challenge for quantum\nchemistry due to the multireferential character of the electronic structure in\ndissociating species. Multireferential methods in particular suffer from large\ncomputational cost, which under the normal paradigm has to be paid anew for\neach system at a full price, ignoring commonalities in electronic structure\nacross molecules. Quantum Monte Carlo with deep neural networks (deep QMC)\nuniquely offers to exploit such commonalities by pretraining transferable\nwavefunction models, but all such attempts were so far limited in scope. Here,\nwe bring this new paradigm to fruition with Orbformer, a novel transferable\nwavefunction model pretrained on 22,000 equilibrium and dissociating structures\nthat can be fine-tuned on unseen molecules reaching an accuracy-cost ratio\nrivalling classical multireferential methods. On established benchmarks as well\nas more challenging bond dissociations and Diels-Alder reactions, Orbformer is\nthe only method that consistently converges to chemical accuracy (1 kcal/mol).\nThis work turns the idea of amortizing the cost of solving the Schr\\\"odinger\nequation over many molecules into a practical approach in quantum chemistry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19960v1", "categories": ["physics.chem-ph", "cs.AI", "stat.ML"], "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2506.19960v1", "AI": {"title_translation": "一个精确描述化学键断裂的从头算波函数基础模型", "tldr": "Orbformer是一个基于深度QMC的新型可迁移波函数模型，通过预训练能准确描述化学键断裂，解决了传统多参考方法计算成本高的问题，并达到了化学精度。", "motivation": "由于解离物种中电子结构的多参考特性，可靠描述键断裂对量子化学来说是一个重大挑战。传统多参考方法计算成本高昂，且无法利用分子间电子结构的共性。深度QMC有潜力通过预训练可迁移波函数模型来利用这些共性，但之前的尝试范围有限。", "method": "本文提出了Orbformer，一个新颖的可迁移波函数模型。该模型在22,000个平衡态和解离结构上进行了预训练，之后可以在未见过的分子上进行微调。它利用深度神经网络结合量子蒙特卡洛（deep QMC）方法来描述波函数。", "result": "Orbformer在既定基准以及更具挑战性的键解离和Diels-Alder反应中，是唯一能持续收敛到化学精度（1 kcal/mol）的方法。其精度-成本比可与经典多参考方法相媲美。", "conclusion": "这项工作将分摊求解薛定谔方程成本的理念，转化为量子化学中一种实用的方法，有效地解决了化学键断裂描述中的计算挑战。", "translation": "由于解离物种中电子结构的多参考特性，可靠描述键断裂对量子化学来说仍然是一个重大挑战。特别是多参考方法计算成本高昂，在正常范式下，每个系统都必须重新支付全部成本，而忽略了分子间电子结构的共性。结合深度神经网络的量子蒙特卡洛（deep QMC）独特地提供了通过预训练可迁移波函数模型来利用这些共性的能力，但所有此类尝试迄今为止范围都有限。在这里，我们通过Orbformer将这种新范式变为现实，Orbformer是一种新型的可迁移波函数模型，在22,000个平衡态和解离结构上进行了预训练，可以在未见过的分子上进行微调，其精度-成本比可与经典多参考方法相媲美。在既定基准以及更具挑战性的键解离和Diels-Alder反应中，Orbformer是唯一能持续收敛到化学精度（1 kcal/mol）的方法。这项工作将分摊求解薛定谔方程成本的理念转化为量子化学中一种实用的方法。", "summary": "本研究引入了Orbformer，一个基于深度量子蒙特卡洛的新型可迁移波函数模型，旨在解决传统量子化学方法在描述化学键断裂时面临的计算成本高昂和多参考特性挑战。Orbformer通过在大量分子结构上进行预训练，实现了在未见分子上的高精度和高效率。实验结果表明，Orbformer在多种基准测试和复杂反应中均能稳定达到化学精度，并展现出与经典多参考方法相当的精度-成本比，从而将分摊薛定谔方程求解成本的理念变为现实。", "keywords": "化学键断裂, 波函数模型, 深度量子蒙特卡洛, 可迁移性, 化学精度", "comments": "Orbformer的创新之处在于其可迁移的预训练波函数模型，这显著降低了传统多参考方法在处理化学键断裂问题时的高昂计算成本。它利用了深度学习的优势，实现了在不同分子间知识的迁移和复用，是量子化学领域的一个重要进展，有望加速新材料和药物的发现。其达到化学精度且具有竞争力的精度-成本比，使其成为未来量子化学计算的有力工具。"}}
{"id": "2506.20671", "title": "IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals", "authors": ["Markus Gross", "Aya Fahmy", "Danit Niwattananan", "Dominik Muhle", "Rui Song", "Daniel Cremers", "Henri Meeß"], "summary": "Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly\nlearning scene geometry and semantics, enabling downstream applications such as\nnavigation in mobile robotics. The recent generalization to Panoptic Scene\nCompletion (PSC) advances the SSC domain by integrating instance-level\ninformation, thereby enhancing object-level sensitivity in scene understanding.\nWhile PSC was introduced using LiDAR modality, methods based on camera images\nremain largely unexplored. Moreover, recent Transformer-based SSC approaches\nutilize a fixed set of learned queries to reconstruct objects within the scene\nvolume. Although these queries are typically updated with image context during\ntraining, they remain static at test time, limiting their ability to\ndynamically adapt specifically to the observed scene. To overcome these\nlimitations, we propose IPFormer, the first approach that leverages\ncontext-adaptive instance proposals at train and test time to address\nvision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively\ninitializes these queries as panoptic instance proposals derived from image\ncontext and further refines them through attention-based encoding and decoding\nto reason about semantic instance-voxel relationships. Experimental results\nshow that our approach surpasses state-of-the-art methods in overall panoptic\nmetrics PQ$^\\dagger$ and PQ-All, matches performance in individual metrics, and\nachieves a runtime reduction exceeding 14$\\times$. Furthermore, our ablation\nstudies reveal that dynamically deriving instance proposals from image context,\nas opposed to random initialization, leads to a 3.62% increase in PQ-All and a\nremarkable average improvement of 18.65% in combined Thing-metrics. These\nresults highlight our introduction of context-adaptive instance proposals as a\npioneering effort in addressing vision-based 3D Panoptic Scene Completion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20671v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20671v1", "AI": {"title_translation": "IPFormer：基于上下文自适应实例提议的视觉三维全景场景补全", "tldr": "IPFormer提出了一种基于视觉的三维全景场景补全新方法，通过上下文自适应实例提议来动态初始化查询，显著提升了性能并大幅降低了运行时开销。", "motivation": "现有全景场景补全（PSC）方法主要基于LiDAR模态，而基于相机图像的方法尚待充分探索。此外，当前基于Transformer的语义场景补全（SSC）方法在测试时使用静态查询，限制了其对观测场景的动态适应能力。", "method": "本文提出了IPFormer，这是首个在训练和测试阶段利用上下文自适应实例提议来解决基于视觉的三维全景场景补全的方法。IPFormer将查询自适应地初始化为从图像上下文导出的全景实例提议，并通过基于注意力的编码和解码进一步细化，以推断语义实例-体素关系。", "result": "实验结果表明，IPFormer在整体全景指标PQ$^\\dagger$和PQ-All上超越了现有最先进方法，在单项指标上表现相当，并实现了超过14倍的运行时缩减。此外，消融研究显示，从图像上下文动态生成实例提议相比随机初始化，能使PQ-All提高3.62%，组合Thing-metrics平均提高18.65%。", "conclusion": "这些结果表明，本文提出的上下文自适应实例提议是解决基于视觉的三维全景场景补全的一项开创性工作。", "translation": "语义场景补全（SSC）已成为联合学习场景几何和语义的关键方法，支持移动机器人导航等下游应用。近期推广到全景场景补全（PSC）通过集成实例级信息，推进了SSC领域，从而增强了场景理解中的对象级敏感性。虽然PSC是使用LiDAR模态引入的，但基于相机图像的方法在很大程度上仍未被探索。此外，最近基于Transformer的SSC方法利用一组固定的学习查询来重建场景体内的对象。尽管这些查询通常在训练期间通过图像上下文进行更新，但它们在测试时保持静态，限制了其动态适应特定观测场景的能力。为了克服这些限制，我们提出了IPFormer，这是第一个在训练和测试阶段利用上下文自适应实例提议来解决基于视觉的三维全景场景补全的方法。具体来说，IPFormer将这些查询自适应地初始化为从图像上下文导出的全景实例提议，并通过基于注意力的编码和解码进一步细化，以推断语义实例-体素关系。实验结果表明，我们的方法在整体全景指标PQ$^\nobreakspace\\dagger$和PQ-All上超越了现有最先进方法，在单项指标上表现相当，并实现了超过14倍的运行时缩减。此外，我们的消融研究表明，从图像上下文动态导出实例提议，而不是随机初始化，能使PQ-All提高3.62%，组合Thing-metrics平均提高18.65%。这些结果突出表明，我们引入的上下文自适应实例提议是解决基于视觉的三维全景场景补全的一项开创性努力。", "summary": "本文提出IPFormer，首个解决基于视觉的三维全景场景补全（PSC）的方法。针对现有方法依赖LiDAR数据和Transformer模型中查询静态的问题，IPFormer引入上下文自适应实例提议，在训练和测试阶段动态初始化查询。通过注意力机制细化这些提议，IPFormer能够更好地理解语义实例-体素关系。实验证明，IPFormer在全景指标上超越现有SOTA，运行时效率提升超过14倍，且上下文自适应提议显著提升了性能，尤其在Thing-metrics上表现出色，是视觉3D PSC领域的开创性工作。", "keywords": "全景场景补全, 三维视觉, Transformer, 实例提议, 上下文自适应", "comments": "IPFormer的创新之处在于首次将上下文自适应实例提议引入基于视觉的三维全景场景补全，解决了现有Transformer模型中查询静态的局限性。这种动态适应性极大地提升了模型对复杂场景的理解能力，并在性能和运行时效率上取得了显著突破，尤其在Thing-metrics上的提升表明其在实例级感知方面具有强大潜力。这是一项对计算机视觉和机器人领域具有重要意义的开创性工作。"}}
{"id": "2506.20354", "title": "A foundation model with multi-variate parallel attention to generate neuronal activity", "authors": ["Francesco Carzaniga", "Michael Hersche", "Abu Sebastian", "Kaspar Schindler", "Abbas Rahimi"], "summary": "Learning from multi-variate time-series with heterogeneous channel\nconfigurations remains a fundamental challenge for deep neural networks (DNNs),\nparticularly in clinical domains such as intracranial electroencephalography\n(iEEG), where channel setups vary widely across subjects. In this work, we\nintroduce multi-variate parallel attention (MVPA), a novel self-attention\nmechanism that disentangles content, temporal, and spatial attention, enabling\nflexible, generalizable, and efficient modeling of time-series data with\nvarying channel counts and configurations. We use MVPA to build MVPFormer, a\ngenerative foundation model for human electrophysiology, trained to predict the\nevolution of iEEG signals across diverse subjects. To support this and future\neffort by the community, we release the SWEC iEEG dataset, the largest publicly\navailable iEEG dataset to date, comprising nearly 10,000 hours of recordings\nfrom heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong\ngeneralization across subjects, demonstrating expert-level performance in\nseizure detection and outperforming state-of-the-art Transformer baselines on\nour SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard\ntime-series forecasting and classification tasks, where it matches or exceeds\nexisting attention-based models. Together, our contributions establish MVPA as\na general-purpose attention mechanism for heterogeneous time-series and\nMVPFormer as the first open-source, open-weights, and open-data iEEG foundation\nmodel with state-of-the-art clinical performance. The code is available at\nhttps://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG\ndataset is available at\nhttps://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.", "comment": "The code is available at\n  https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG\n  dataset is available at\n  https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg", "pdf_url": "http://arxiv.org/pdf/2506.20354v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20354v1", "AI": {"title_translation": "一种具有多变量并行注意力机制的生成神经元活动的预训练模型", "tldr": "本文提出了一种名为MVPA的新型自注意力机制，并基于其构建了MVPFormer，一个用于人类电生理学的生成式预训练模型，旨在解决异构通道配置下多变量时间序列学习的挑战。MVPFormer在癫痫检测方面表现出色，并优于现有模型，同时作者还发布了迄今为止最大的iEEG数据集SWEC。", "motivation": "深度神经网络（DNN）在处理具有异构通道配置的多变量时间序列数据时面临根本性挑战，尤其是在颅内脑电图（iEEG）等临床领域，不同受试者的通道设置差异很大。", "method": "研究引入了多变量并行注意力（MVPA），这是一种新颖的自注意力机制，它解耦了内容、时间、空间注意力，实现了对具有不同通道数量和配置的时间序列数据进行灵活、通用和高效的建模。研究使用MVPA构建了MVPFormer，一个用于人类电生理学的生成式预训练模型，旨在预测不同受试者iEEG信号的演变。此外，研究还发布了迄今为止最大的公开iEEG数据集SWEC。", "result": "MVPFormer利用MVPA在不同受试者之间实现了强大的泛化能力，在癫痫检测中表现出专家级性能，并且在SWEC、MAYO和FNUSA数据集上优于最先进的Transformer基线模型。MVPA在标准时间序列预测和分类任务中也与现有基于注意力的模型持平或超越。", "conclusion": "该研究的贡献确立了MVPA作为一种用于异构时间序列的通用注意力机制，并将MVPFormer确立为首个开源、开放权重、开放数据的iEEG预训练模型，具有最先进的临床性能。", "translation": "学习具有异构通道配置的多变量时间序列对深度神经网络（DNN）来说仍然是一个根本性的挑战，尤其是在颅内脑电图（iEEG）等临床领域，其中通道设置在不同受试者之间差异很大。在这项工作中，我们引入了多变量并行注意力（MVPA），这是一种新颖的自注意力机制，它解耦了内容、时间、空间注意力，从而能够对具有不同通道数量和配置的时间序列数据进行灵活、通用和高效的建模。我们使用MVPA构建了MVPFormer，一个用于人类电生理学的生成式预训练模型，旨在预测不同受试者iEEG信号的演变。为了支持这项工作和社区未来的努力，我们发布了SWEC iEEG数据集，这是迄今为止最大的公开iEEG数据集，包含来自异构临床来源的近10,000小时的记录。MVPFormer利用MVPA在不同受试者之间实现了强大的泛化能力，在癫痫检测中表现出专家级性能，并且在我们的SWEC、MAYO和FNUSA数据集上优于最先进的Transformer基线模型。我们进一步在标准时间序列预测和分类任务中验证了MVPA，它与现有基于注意力的模型持平或超越。总而言之，我们的贡献确立了MVPA作为一种用于异构时间序列的通用注意力机制，并将MVPFormer确立为首个开源、开放权重、开放数据的iEEG预训练模型，具有最先进的临床性能。代码可在https://github.com/IBM/multi-variate-parallel-transformer获取。SWEC iEEG数据集可在https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg获取。", "summary": "本文提出了一种名为多变量并行注意力（MVPA）的新型自注意力机制，通过解耦内容、时间、空间注意力，解决了深度神经网络在处理具有异构通道配置的多变量时间序列数据，特别是iEEG数据时的挑战。基于MVPA，研究构建了MVPFormer，一个用于人类电生理学的生成式预训练模型，并在癫痫检测任务中取得了专家级性能，超越了现有最先进的模型。为支持研究和社区发展，作者还发布了迄今为止最大的公开iEEG数据集SWEC。MVPA也被证明在标准时间序列任务中表现出色。MVPFormer是首个开源、开放权重、开放数据的iEEG预训练模型，具有最先进的临床性能。", "keywords": "多变量并行注意力, 预训练模型, iEEG, 时间序列, 癫痫检测", "comments": "该论文的创新点在于提出了MVPA这一新型自注意力机制，通过解耦不同维度的注意力，有效解决了多变量时间序列数据在异构通道配置下的建模难题，尤其是在iEEG这种复杂临床数据上的应用。MVPFormer作为首个开源的iEEG预训练模型，结合其在癫痫检测上的专家级表现以及发布的大规模iEEG数据集，对神经科学和临床医学领域都具有重要意义，将极大地推动相关研究和应用。"}}
{"id": "2506.20359", "title": "Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach", "authors": ["Chanuka Don Samarasinghage", "Dhruv Gulabani"], "summary": "Trajectory analysis is not only about obtaining movement data, but it is also\nof paramount importance in understanding the pattern in which an object moves\nthrough space and time, as well as in predicting its next move. Due to the\nsignificant interest in the area, data collection has improved substantially,\nresulting in a large number of features becoming available for training and\npredicting models. However, this introduces a high-dimensionality-induced\nfeature explosion problem, which reduces the efficiency and interpretability of\nthe data, thereby reducing the accuracy of machine learning models. To overcome\nthis issue, feature selection has become one of the most prevalent tools. Thus,\nthe objective of this paper was to introduce a taxonomy-based feature selection\nmethod that categorizes features based on their internal structure. This\napproach classifies the data into geometric and kinematic features, further\ncategorizing them into curvature, indentation, speed, and acceleration. The\ncomparative analysis indicated that a taxonomy-based approach consistently\nachieved comparable or superior predictive performance. Furthermore, due to the\ntaxonomic grouping, which reduces combinatorial space, the time taken to select\nfeatures was drastically reduced. The taxonomy was also used to gain insights\ninto what feature sets each dataset was more sensitive to. Overall, this study\nprovides robust evidence that a taxonomy-based feature selection method can add\na layer of interpretability, reduce dimensionality and computational\ncomplexity, and contribute to high-level decision-making. It serves as a step\ntoward providing a methodological framework for researchers and practitioners\ndealing with trajectory datasets and contributing to the broader field of\nexplainable artificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20359v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20359v1", "AI": {"title_translation": "走向轨迹数据集中可解释和高效的特征选择：一种分类学方法", "tldr": "本文提出了一种基于分类学的特征选择方法，用于轨迹数据集，以提高模型的可解释性和效率，同时保持或提升预测性能。", "motivation": "轨迹数据集中高维度导致的特征爆炸问题降低了数据效率、可解释性以及机器学习模型的准确性。", "method": "本文引入了一种基于分类学的特征选择方法，该方法根据特征的内部结构对特征进行分类。具体地，将数据分为几何特征和运动学特征，并进一步细分为曲率、压痕、速度和加速度。", "result": "比较分析表明，基于分类学的方法始终能达到可比或更优的预测性能。此外，由于分类分组减少了组合空间，选择特征所需的时间大大减少。该分类法还用于深入了解每个数据集对哪些特征集更敏感。", "conclusion": "这项研究提供了有力的证据，表明基于分类学的特征选择方法可以增加一层可解释性，降低维度和计算复杂性，并有助于高层次的决策。它为处理轨迹数据集的研究人员和从业者提供了一个方法论框架，并为可解释人工智能的更广阔领域做出了贡献。", "translation": "轨迹分析不仅关乎获取运动数据，更重要的是理解物体在空间和时间中移动的模式，以及预测其下一步行动。由于对该领域的浓厚兴趣，数据收集已大幅改进，导致大量特征可用于训练和预测模型。然而，这引入了高维度导致的特征爆炸问题，降低了数据的效率和可解释性，从而降低了机器学习模型的准确性。为了克服这个问题，特征选择已成为最流行的工具之一。因此，本文的目标是引入一种基于分类学的特征选择方法，该方法根据特征的内部结构对特征进行分类。这种方法将数据分为几何特征和运动学特征，并进一步将其归类为曲率、压痕、速度和加速度。比较分析表明，基于分类学的方法始终能达到可比或更优的预测性能。此外，由于分类分组减少了组合空间，选择特征所需的时间大大减少。该分类法还用于深入了解每个数据集对哪些特征集更敏感。总的来说，这项研究提供了有力的证据，表明基于分类学的特征选择方法可以增加一层可解释性，降低维度和计算复杂性，并有助于高层次的决策。它为处理轨迹数据集的研究人员和从业者提供了一个方法论框架，并为可解释人工智能的更广阔领域做出了贡献。", "summary": "本文提出了一种针对轨迹数据集的基于分类学的特征选择方法，旨在解决高维度带来的特征爆炸问题，提高数据效率、可解释性及机器学习模型的准确性。该方法根据特征的内部结构对其进行分类，例如几何和运动学特征（包括曲率、压痕、速度和加速度）。实验结果表明，这种分类学方法在预测性能上与现有方法相当或更优，并显著减少了特征选择的时间。此外，该方法还增强了对数据集特征敏感性的理解，为可解释人工智能和轨迹数据处理提供了新的框架。", "keywords": "轨迹分析, 特征选择, 分类学, 可解释性, 效率", "comments": "该论文的创新之处在于提出了一种基于分类学的特征选择方法，为高维轨迹数据提供了一种结构化且可解释的解决方案。它不仅提高了特征选择的效率，还通过分类分组增加了模型的可解释性，这对于可解释AI领域具有重要意义。该方法为研究人员和从业者处理复杂轨迹数据集提供了一个实用的框架。"}}
{"id": "2506.19973", "title": "Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies", "authors": ["Vojtěch Novák", "Ivan Zelinka", "Lenka Přibylová", "Lubomír Martínek"], "summary": "This study investigates the application of quantum neural networks (QNNs) for\npropensity score estimation to address selection bias in comparing survival\noutcomes between laparoscopic and open surgical techniques in a cohort of 1177\ncolorectal carcinoma patients treated at University Hospital Ostrava\n(2001-2009). Using a dataset with 77 variables, including patient demographics\nand tumor characteristics, we developed QNN-based propensity score models\nfocusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture\nemployed a linear ZFeatureMap for data encoding, a SummedPaulis operator for\npredictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)\nfor robust, gradient-free optimization in noisy quantum environments. Variance\nregularization was integrated to mitigate quantum measurement noise, with\nsimulations conducted under exact, sampling (1024 shots), and noisy hardware\n(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,\noutperformed classical logistic regression and gradient boosted machines in\nsmall samples (AUC up to 0.750 for n=100), with noise modeling enhancing\npredictive stability. Propensity score matching and weighting, optimized via\ngenetic matching and matching weights, achieved covariate balance with\nstandardized mean differences of 0.0849 and 0.0869, respectively. Survival\nanalyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen\nadditive regression revealed no significant survival differences\npost-adjustment (p-values 0.287-0.851), indicating confounding bias in\nunadjusted outcomes. These results highlight QNNs' potential, enhanced by\nCMA-ES and noise-aware strategies, to improve causal inference in biomedical\nresearch, particularly for small-sample, high-dimensional datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19973v1", "categories": ["quant-ph", "cs.AI", "stat.ML", "62H30, 62P10, 68T05, 81P68", "I.2.6; J.3; I.5.4; F.4.1"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.19973v1", "AI": {"title_translation": "量子神经网络在观察性生物医学研究中用于倾向得分估计和生存分析", "tldr": "研究利用量子神经网络 (QNN) 估计倾向得分，以校正结直肠癌患者队列中手术方式对生存结果的偏倚，并发现调整后无显著生存差异，突显QNN在小样本生物医学研究中的潜力。", "motivation": "该研究旨在通过应用量子神经网络（QNNs）进行倾向得分估计，以解决观察性生物医学研究中比较不同治疗方案（如腹腔镜与开放手术）生存结果时的选择偏倚问题，从而提高因果推断的准确性。", "method": "研究使用包含1177名结直肠癌患者的77个变量数据集，开发了基于QNN的倾向得分模型，重点关注年龄、性别、分期和BMI四个协变量。QNN架构采用线性ZFeatureMap进行数据编码，SummedPaulis运算符进行预测，并使用协方差矩阵自适应进化策略（CMA-ES）在嘈杂的量子环境中进行鲁棒、无梯度优化。引入方差正则化以减轻量子测量噪声，并在精确、采样（1024次）和模拟噪声硬件（FakeManhattanV2）条件下进行模拟。通过遗传匹配和匹配权重优化倾向得分匹配和加权，并使用Kaplan-Meier估计、Cox比例风险模型和Aalen加性回归进行生存分析。", "result": "在小样本（n=100）中，QNNs（尤其是在模拟硬件噪声下）在AUC方面优于经典逻辑回归和梯度提升机，AUC最高达0.750，噪声建模增强了预测稳定性。倾向得分匹配和加权实现了协变量平衡，标准化均差分别为0.0849和0.0869。调整后的生存分析（Kaplan-Meier、Cox比例风险、Aalen加性回归）未显示显著生存差异（p值0.287-0.851），表明未调整结果中存在混杂偏倚。", "conclusion": "研究结果突显了量子神经网络（QNNs）在生物医学研究中改善因果推断的潜力，尤其是在小样本、高维数据集上，并通过CMA-ES和噪声感知策略得到了增强。同时，研究证实未调整的生存结果存在混杂偏倚，而通过倾向得分调整后，两种手术方式的生存差异不显著。", "translation": "这项研究探讨了量子神经网络（QNNs）在倾向得分估计中的应用，以解决在奥斯特拉瓦大学医院（2001-2009年）治疗的1177名结直肠癌患者队列中，比较腹腔镜与开放手术技术之间生存结果时的选择偏倚问题。我们使用包含77个变量（包括患者人口统计学和肿瘤特征）的数据集，开发了基于QNN的倾向得分模型，重点关注四个关键协变量（年龄、性别、分期、BMI）。QNN架构采用线性ZFeatureMap进行数据编码，SummedPaulis运算符进行预测，并使用协方差矩阵自适应进化策略（CMA-ES）在嘈杂的量子环境中进行鲁棒、无梯度优化。方差正则化被整合以减轻量子测量噪声，模拟在精确、采样（1024次）和嘈杂硬件（FakeManhattanV2）条件下进行。QNNs，特别是在模拟硬件噪声下，在小样本（n=100时AUC高达0.750）中表现优于经典逻辑回归和梯度提升机，噪声建模增强了预测稳定性。通过遗传匹配和匹配权重优化的倾向得分匹配和加权，实现了协变量平衡，标准化均差分别为0.0849和0.0869。使用Kaplan-Meier估计、Cox比例风险和Aalen加性回归进行的生存分析显示，调整后无显著生存差异（p值0.287-0.851），表明未调整结果中存在混杂偏倚。这些结果突显了QNNs的潜力，通过CMA-ES和噪声感知策略的增强，可以改善生物医学研究中的因果推断，特别是对于小样本、高维数据集。", "summary": "本研究利用量子神经网络（QNNs）进行倾向得分估计，旨在校正观察性生物医学研究中比较不同手术技术（腹腔镜与开放手术）对结直肠癌患者生存结果的选择偏倚。研究开发了基于QNN的倾向得分模型，并采用CMA-ES进行优化，同时整合方差正则化以应对量子噪声。结果显示，QNNs在小样本下性能优于经典方法，并成功实现了协变量平衡。经倾向得分调整后，两种手术方式的生存差异不再显著，表明QNNs在提升生物医学领域因果推断能力方面的潜力，尤其适用于小样本、高维数据。", "keywords": "量子神经网络, 倾向得分估计, 生存分析, 因果推断, 生物医学研究", "comments": "该论文的创新之处在于首次将量子神经网络应用于倾向得分估计和生存分析，以解决生物医学观察性研究中的选择偏倚问题。其重要性在于展示了QNNs在处理小样本、高维数据集时的潜力，并引入了噪声感知策略和无梯度优化方法（CMA-ES），这对于当前量子计算的硬件限制具有实际意义。研究还通过实验证明了在模拟噪声环境下QNNs的优越性，为量子机器学习在医疗健康领域的实际应用提供了新的方向。局限性可能在于，虽然模拟了噪声环境，但实际量子硬件的性能和稳定性仍是挑战，且研究数据来源于单一中心，可能影响结果的普适性。"}}
{"id": "2506.20520", "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards", "authors": ["Charles Arnal", "Gaëtan Narozniak", "Vivien Cabannes", "Yunhao Tang", "Julia Kempe", "Remi Munos"], "summary": "Reinforcement learning (RL) is increasingly used to align large language\nmodels (LLMs). Off-policy methods offer greater implementation simplicity and\ndata efficiency than on-policy techniques, but often result in suboptimal\nperformance. In this work, we study the intermediate range of algorithms\nbetween off-policy RL and supervised fine-tuning by analyzing a simple\noff-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with\n$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$\nemphasizes high-reward samples, while raising it penalizes low-reward ones more\nheavily. We first provide a theoretical analysis of this off-policy REINFORCE\nalgorithm, showing that when the baseline $V$ lower-bounds the expected reward,\nthe algorithm enjoys a policy improvement guarantee. Our analysis reveals that\nwhile on-policy updates can safely leverage both positive and negative signals,\noff-policy updates benefit from focusing more on positive rewards than on\nnegative ones. We validate our findings experimentally in a controlled\nstochastic bandit setting and through fine-tuning state-of-the-art LLMs on\nreasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20520v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20520v1", "AI": {"title_translation": "非对称REINFORCE用于离策略强化学习：平衡正负奖励", "tldr": "研究了一种非对称离策略REINFORCE算法，理论证明了其策略改进保证，并发现离策略更新应更侧重正奖励，这在LLM对齐等任务中得到了验证。", "motivation": "离策略强化学习方法虽然在实现和数据效率上优于在策略技术，但往往导致次优性能，需要改进以提升表现。", "method": "通过分析一种简单的离策略REINFORCE算法来研究介于离策略RL和监督微调之间的算法，其中优势定义为 $A=r-V$，V是一个可调基线，通过调整V来强调高奖励或惩罚低奖励。", "result": "理论分析表明，当基线V是期望奖励的下界时，算法具有策略改进保证。实验在受控的随机多臂老虎机设置和通过微调最先进的LLM在推理任务上验证了这些发现。", "conclusion": "在策略更新可以安全地利用正负信号，而离策略更新则更受益于侧重正奖励而非负奖励。", "translation": "强化学习（RL）正越来越多地用于对齐大型语言模型（LLMs）。离策略方法比在策略技术具有更高的实现简易性和数据效率，但往往导致次优性能。在这项工作中，我们通过分析一种简单的离策略REINFORCE算法来研究离策略RL和监督微调之间的中间范围算法，其中优势定义为 $A=r-V$，其中 $r$ 是奖励，$V$ 是一个可调基线。直观上，降低 $V$ 强调高奖励样本，而提高 $V$ 则更严重地惩罚低奖励样本。我们首先对这种离策略REINFORCE算法进行了理论分析，表明当基线 $V$ 是期望奖励的下界时，该算法享有策略改进保证。我们的分析揭示了，在策略更新可以安全地利用正负信号，而离策略更新则受益于更侧重于正奖励而非负奖励。我们在受控的随机多臂老虎机设置中，并通过对最先进的LLM在推理任务上进行微调，验证了我们的发现。", "summary": "本文提出并分析了一种非对称离策略REINFORCE算法，旨在解决离策略强化学习性能次优的问题。该算法通过引入可调基线V来定义优势函数 $A=r-V$，从而实现对正负奖励的不同侧重。理论分析证明，当V作为期望奖励的下界时，算法能保证策略改进。研究发现，相较于在策略更新能平衡利用正负信号，离策略更新更应专注于正奖励。这些发现通过在多臂老虎机和大型语言模型微调任务上的实验得到了验证。", "keywords": "离策略强化学习, REINFORCE, 奖励平衡, 大型语言模型, 策略改进", "comments": "这篇论文的创新点在于提出了“非对称”的离策略REINFORCE算法，并通过调整基线V来平衡正负奖励的影响，特别强调了离策略更新中正奖励的重要性。其理论分析为算法提供了坚实的保证，并在大型语言模型对齐等实际应用中验证了有效性，为离策略RL的性能提升提供了新的视角。"}}
{"id": "2506.20380", "title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis", "authors": ["Zhengpeng Feng", "Sadiq Jaffer", "Jovana Knezevic", "Silja Sormunen", "Robin Young", "Madeline Lisaius", "Markus Immitzer", "James Ball", "Clement Atzberger", "David A. Coomes", "Anil Madhavapeddy", "Andrew Blake", "Srinivasan Keshav"], "summary": "Satellite remote sensing (RS) enables a wide array of downstream Earth\nobservation (EO) applications, including climate modeling, carbon accounting,\nand strategies for conservation and sustainable land use. We present TESSERA, a\nnovel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning\n(SSL) to generate global, robust representations at 10m scale from pixel-level\nsatellite time series data. TESSERA combines information from only optical and\nSAR data streams using two parallel Transformer-based encoders: one dedicated\nto Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected\nspectral bands) to create representations that are then fused using a\nmultilayer perceptron (MLP), resulting in a global representation map covering\nthe years 2017 to 2024. Our precomputed representations set a new\nstate-of-the-art performance benchmark and our open-source approach\ndemocratizes access to high-performance, high-resolution representations. We\nbenchmark the performance of TESSERA in five diverse tasks, comparing our work\nwith state-of-the-art task-specific models and other foundation models. Our\nresults show that TESSERA outperforms both traditional RS baselines and the\nleading geospatial foundation models in these diverse downstream tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20380v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20380v1", "AI": {"title_translation": "TESSERA：用于地球表示和分析的地表光谱时间嵌入", "tldr": "TESSERA是一个新的遥感基础模型，它使用自监督学习从像素级卫星时间序列数据中生成全球、鲁棒的10米尺度地球表示，并在多项下游任务中超越现有模型。", "motivation": "卫星遥感（RS）支持广泛的地球观测（EO）下游应用，如气候建模、碳核算和可持续土地利用策略。该研究旨在开发一种新型遥感基础模型，以生成高性能、高分辨率的地球表示。", "method": "TESSERA是一个遥感基础模型（RSFM），它使用自监督学习（SSL）从像素级卫星时间序列数据生成10米尺度的全球、鲁棒的表示。它结合了光学和SAR数据流的信息，使用两个并行的基于Transformer的编码器：一个用于Sentinel-1 SAR极化数据，另一个用于Sentinel-2 MSI数据（10个选定的光谱波段）。这些表示随后通过多层感知器（MLP）融合，形成覆盖2017年至2024年的全球表示图。", "result": "TESSERA预计算的表示设定了新的最先进性能基准。在五项多样化任务中，TESSERA的性能优于传统的RS基线模型和领先的地理空间基础模型。", "conclusion": "TESSERA通过提供高性能、高分辨率的全球地球表示，显著提升了遥感分析能力，并为地球观测下游应用提供了新的最先进解决方案。", "translation": "卫星遥感（RS）支持广泛的地球观测（EO）下游应用，包括气候建模、碳核算以及保护和可持续土地利用策略。我们提出了TESSERA，一个新颖的遥感基础模型（RSFM），它使用自监督学习（SSL）从像素级卫星时间序列数据生成全球、鲁棒的10米尺度表示。TESSERA仅使用光学和SAR数据流的信息，通过两个并行的基于Transformer的编码器进行结合：一个专门用于Sentinel-1 SAR极化数据，另一个用于Sentinel-2 MSI数据（10个选定的光谱波段），以创建表示，然后使用多层感知器（MLP）融合这些表示，从而生成覆盖2017年至2024年的全球表示图。我们预计算的表示设定了新的最先进性能基准，并且我们的开源方法使高性能、高分辨率表示的获取民主化。我们在五项不同的任务中基准测试了TESSERA的性能，将我们的工作与最先进的特定任务模型和其他基础模型进行了比较。我们的结果表明，TESSERA在这些多样化的下游任务中优于传统的RS基线模型和领先的地理空间基础模型。", "summary": "TESSERA是一个新颖的遥感基础模型（RSFM），它利用自监督学习从像素级卫星时间序列数据（包括Sentinel-1 SAR和Sentinel-2 MSI）中生成10米尺度的全球、鲁棒的地球表示。该模型采用双Transformer编码器架构融合光学和SAR数据，并通过MLP生成全球表示图。TESSERA的预计算表示设定了新的性能基准，并在多项下游任务中显著超越了传统遥感基线模型和现有地理空间基础模型，同时其开源特性促进了高分辨率表示的普及。", "keywords": "遥感基础模型,自监督学习,时间嵌入,地球表示,SAR数据", "comments": "TESSERA的创新在于结合了光学和SAR数据流，并利用自监督学习和Transformer架构生成了高分辨率、全球尺度的地球表示。其开源方法有助于推动遥感领域的民主化。该模型在多项任务中超越现有SOTA，显示了其在地球观测应用中的巨大潜力。"}}
{"id": "2506.20629", "title": "PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models", "authors": ["Soufiane Hayou", "Nikhil Ghosh", "Bin Yu"], "summary": "Low-Rank Adaptation (LoRA) is a widely used finetuning method for large\nmodels. Its small memory footprint allows practitioners to adapt large models\nto specific tasks at a fraction of the cost of full finetuning. Different\nmodifications have been proposed to enhance its efficiency by, for example,\nsetting the learning rate, the rank, and the initialization. Another\nimprovement axis is adapter placement strategy: when using LoRA, practitioners\nusually pick module types to adapt with LoRA, such as Query and Key modules.\nFew works have studied the problem of adapter placement, with nonconclusive\nresults: original LoRA paper suggested placing adapters in attention modules,\nwhile other works suggested placing them in the MLP modules. Through an\nintuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a\nlightweight method that allows automatic identification of module types where\nLoRA adapters should be placed, given a pretrained model and a finetuning task.\nWe demonstrate that PLoP consistently outperforms, and in the worst case\ncompetes, with commonly used placement strategies through comprehensive\nexperiments on supervised finetuning and reinforcement learning for reasoning.", "comment": "TD,LR: A lightweight module type selection method for LoRA\n  finetuning. PLoP gives precise placements for LoRA adapters for improved\n  performance", "pdf_url": "http://arxiv.org/pdf/2506.20629v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20629v1", "AI": {"title_translation": "PLoP：大型模型高效微调的精确LoRA放置方法", "tldr": "PLoP是一种轻量级方法，能够自动识别大型模型微调时LoRA适配器应放置的最佳模块类型，性能优于或媲美现有策略。", "motivation": "LoRA是一种广泛使用的模型微调方法，但其适配器放置策略尚未得到充分研究，现有结果也不确定，导致实践中通常凭经验选择模块类型。本文旨在解决这一问题，自动识别最佳放置位置。", "method": "本文引入了PLoP（Precise LoRA Placement），这是一种基于直观理论分析的轻量级方法。给定一个预训练模型和微调任务，PLoP能够自动识别LoRA适配器应放置的模块类型。", "result": "实验证明，PLoP在有监督微调和推理的强化学习任务中，始终优于或在最坏情况下与常用放置策略竞争。", "conclusion": "PLoP通过自动识别LoRA适配器的最佳放置位置，显著提升了大型模型微调的效率和性能，解决了LoRA应用中的一个关键未决问题。", "translation": "低秩适应（LoRA）是一种广泛用于大型模型微调的方法。其小内存占用允许实践者以全量微调成本的一小部分将大型模型适应特定任务。已经提出了各种修改以提高其效率，例如设置学习率、秩和初始化。另一个改进方向是适配器放置策略：在使用LoRA时，实践者通常选择要使用LoRA适应的模块类型，例如Query和Key模块。很少有工作研究适配器放置问题，且结果不确定：原始LoRA论文建议将适配器放置在注意力模块中，而其他工作建议将其放置在MLP模块中。通过直观的理论分析，我们引入了PLoP（精确LoRA放置），这是一种轻量级方法，允许在给定预训练模型和微调任务的情况下，自动识别LoRA适配器应放置的模块类型。我们通过对有监督微调和推理强化学习的综合实验证明，PLoP始终优于，并且在最坏情况下与常用放置策略竞争。", "summary": "本文提出了PLoP（精确LoRA放置），一个轻量级方法，旨在解决大型模型LoRA微调中适配器放置策略不确定性的问题。通过直观的理论分析，PLoP能够自动识别LoRA适配器在给定预训练模型和微调任务下的最佳放置模块类型。实验结果表明，PLoP在多种任务上表现优异，性能持续超越或至少与现有常用放置策略持平。", "keywords": "LoRA, 模型微调, 适配器放置, PLoP, 大规模模型", "comments": "PLoP的创新之处在于它提供了一种自动化的方法来确定LoRA适配器的最佳放置位置，这解决了LoRA应用中的一个实际痛点，即以往需要凭经验或试错来选择模块。这种自动化能力有望进一步提高LoRA微调的效率和效果，对于降低大型模型微调成本具有重要意义。"}}
{"id": "2506.20417", "title": "Off-Policy Evaluation and Learning for the Future under Non-Stationarity", "authors": ["Tatsuhiro Shimizu", "Kazuki Kawamura", "Takanori Muroi", "Yusuke Narita", "Kei Tateno", "Takuma Udagawa", "Yuta Saito"], "summary": "We study the novel problem of future off-policy evaluation (F-OPE) and\nlearning (F-OPL) for estimating and optimizing the future value of policies in\nnon-stationary environments, where distributions vary over time. In e-commerce\nrecommendations, for instance, our goal is often to estimate and optimize the\npolicy value for the upcoming month using data collected by an old policy in\nthe previous month. A critical challenge is that data related to the future\nenvironment is not observed in the historical data. Existing methods assume\nstationarity or depend on restrictive reward-modeling assumptions, leading to\nsignificant bias. To address these limitations, we propose a novel estimator\nnamed \\textit{\\textbf{O}ff-\\textbf{P}olicy Estimator for the \\textbf{F}uture\n\\textbf{V}alue (\\textbf{\\textit{OPFV}})}, designed for accurately estimating\npolicy values at any future time point. The key feature of OPFV is its ability\nto leverage the useful structure within time-series data. While future data\nmight not be present in the historical log, we can leverage, for example,\nseasonal, weekly, or holiday effects that are consistent in both the historical\nand future data. Our estimator is the first to exploit these time-related\nstructures via a new type of importance weighting, enabling effective F-OPE.\nTheoretical analysis identifies the conditions under which OPFV becomes\nlow-bias. In addition, we extend our estimator to develop a new policy-gradient\nmethod to proactively learn a good future policy using only historical data.\nEmpirical results show that our methods substantially outperform existing\nmethods in estimating and optimizing the future policy value under\nnon-stationarity for various experimental setups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20417v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20417v1", "AI": {"title_translation": "非平稳性下未来策略评估与学习", "tldr": "本文提出了一种名为OPFV的新型离策略评估器，用于在非平稳环境中估计和优化未来策略价值，并通过利用时间序列数据中的结构，显著优于现有方法。", "motivation": "在非平稳环境中，现有离策略评估方法通常假设平稳性或依赖于限制性奖励建模假设，导致显著偏差。此外，历史数据中不包含未来环境的相关数据，这使得估计和优化未来策略价值成为一项挑战。", "method": "本文提出了一种名为OPFV（Off-Policy Estimator for the Future Value）的新型估计器，用于准确估计任何未来时间点的策略价值。OPFV通过一种新型重要性加权方法，利用时间序列数据中（如季节性、每周、节假日效应）的有用结构。此外，还将该估计器扩展为一种新的策略梯度方法，用于主动学习良好的未来策略。", "result": "实证结果表明，在各种实验设置下，本文提出的方法在非平稳性条件下，估计和优化未来策略价值方面，显著优于现有方法。", "conclusion": "本文提出了一种新颖的离策略评估（F-OPE）和学习（F-OPL）问题，并开发了OPFV估计器和相应的策略梯度方法，以有效解决非平稳环境中未来策略价值的估计和优化问题，通过利用时间序列结构实现了低偏差和优越的性能。", "translation": "我们研究了在非平稳环境中估计和优化策略未来价值的未来离策略评估（F-OPE）和学习（F-OPL）的新问题，其中分布随时间变化。例如，在电子商务推荐中，我们的目标通常是使用上个月旧策略收集的数据来估计和优化下个月的策略价值。一个关键挑战是，历史数据中并未观察到与未来环境相关的数据。现有方法假设平稳性或依赖于限制性奖励建模假设，导致显著偏差。为了解决这些局限性，我们提出了一种名为“未来价值离策略估计器”（OPFV）的新型估计器，旨在准确估计任何未来时间点的策略价值。OPFV的关键特征是它能够利用时间序列数据中的有用结构。虽然未来数据可能不存在于历史日志中，但我们可以利用例如季节性、每周或节假日效应，这些效应在历史和未来数据中都是一致的。我们的估计器是第一个通过新型重要性加权来利用这些时间相关结构的，从而实现了有效的F-OPE。理论分析确定了OPFV实现低偏差的条件。此外，我们扩展了我们的估计器，开发了一种新的策略梯度方法，仅使用历史数据主动学习一个好的未来策略。实证结果表明，我们的方法在各种实验设置下，在非平稳性条件下估计和优化未来策略价值方面，显著优于现有方法。", "summary": "本研究提出了非平稳环境下未来离策略评估与学习（F-OPE/F-OPL）问题，旨在利用历史数据估计和优化未来策略价值。针对现有方法在非平稳性下存在的偏差问题，本文提出了一种新型估计器OPFV，该估计器通过新颖的重要性加权方法，有效利用时间序列数据中的季节性、每周、节假日等结构信息。理论分析证明了OPFV的低偏差特性，并进一步将其扩展为一种新的策略梯度学习方法。实验结果表明，OPFV在非平稳条件下，在未来策略价值的估计和优化方面显著优于现有方法。", "keywords": "离策略评估, 非平稳性, 未来价值, 时间序列, 策略学习", "comments": "本文的创新点在于首次提出了未来离策略评估与学习（F-OPE/F-OPL）问题，并针对非平稳环境提出了有效的解决方案。通过利用时间序列数据中固有的结构（如季节性效应），OPFV估计器克服了历史数据中缺乏未来信息和现有方法假设平稳性的局限性，具有重要的理论和实际应用价值，尤其是在电商推荐等时变场景。"}}
{"id": "2506.20431", "title": "Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation", "authors": ["Xing Ma"], "summary": "Federated learning aims to train a global model in a distributed environment\nthat is close to the performance of centralized training. However, issues such\nas client label skew, data quantity skew, and other heterogeneity problems\nseverely degrade the model's performance. Most existing methods overlook the\nscenario where only a small portion of clients participate in training within a\nlarge-scale client setting, whereas our experiments show that this scenario\npresents a more challenging federated learning task. Therefore, we propose a\nKnowledge Distillation with teacher-student Inequitable Aggregation (KDIA)\nstrategy tailored to address the federated learning setting mentioned above,\nwhich can effectively leverage knowledge from all clients. In KDIA, the student\nmodel is the average aggregation of the participating clients, while the\nteacher model is formed by a weighted aggregation of all clients based on three\nfrequencies: participation intervals, participation counts, and data volume\nproportions. During local training, self-knowledge distillation is performed.\nAdditionally, we utilize a generator trained on the server to generate\napproximately independent and identically distributed (IID) data features\nlocally for auxiliary training. We conduct extensive experiments on the\nCIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate\nKDIA. The results show that KDIA can achieve better accuracy with fewer rounds\nof training, and the improvement is more significant under severe\nheterogeneity.", "comment": "33pages,8figures", "pdf_url": "http://arxiv.org/pdf/2506.20431v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20431v1", "AI": {"title_translation": "联邦学习中通过知识蒸馏与不公平聚合处理数据异质性", "tldr": "提出KDIA策略，通过知识蒸馏和不公平聚合解决联邦学习中的数据异质性问题，尤其是在少量客户端参与的场景下。", "motivation": "联邦学习中，客户端标签倾斜、数据量倾斜等数据异质性问题严重降低模型性能，尤其是在大规模客户端设置下只有少量客户端参与训练的场景更具挑战性。", "method": "提出知识蒸馏与师生不公平聚合（KDIA）策略。学生模型是参与客户端的平均聚合；教师模型是根据参与间隔、参与次数和数据量比例对所有客户端进行加权聚合。本地训练时进行自知识蒸馏。服务器上训练生成器生成近似IID数据特征进行辅助训练。", "result": "KDIA在CIFAR-10/100/CINIC-10数据集和各种异质性设置下，能以更少的训练轮次获得更好的准确性，且在严重异质性下改进更显著。", "conclusion": "KDIA策略能有效解决联邦学习中的数据异质性问题，尤其是在大规模客户端仅部分参与训练的场景下，通过知识蒸馏和不公平聚合显著提升模型性能。", "translation": "联邦学习旨在分布式环境中训练一个全局模型，使其性能接近中心化训练。然而，客户端标签倾斜、数据量倾斜等异质性问题严重降低了模型的性能。大多数现有方法忽略了在大规模客户端设置中只有一小部分客户端参与训练的场景，而我们的实验表明，这种情况对联邦学习任务提出了更大的挑战。因此，我们提出了一种专门针对上述联邦学习设置的知识蒸馏与师生不公平聚合（KDIA）策略，该策略可以有效利用所有客户端的知识。在KDIA中，学生模型是参与客户端的平均聚合，而教师模型是根据参与间隔、参与次数和数据量比例对所有客户端进行加权聚合形成的。在本地训练期间，执行自知识蒸馏。此外，我们利用在服务器上训练的生成器在本地生成近似独立同分布（IID）的数据特征进行辅助训练。我们在CIFAR-10/100/CINIC-10数据集和各种异质性设置上进行了广泛的实验来评估KDIA。结果表明，KDIA可以在更少的训练轮次下获得更好的准确性，并且在严重异质性下改进更显著。", "summary": "本文提出了一种名为KDIA（知识蒸馏与师生不公平聚合）的策略，旨在解决联邦学习中由客户端数据异质性（如标签和数据量倾斜）引起的问题，特别是在大规模客户端环境中仅有少量客户端参与训练的挑战性场景。KDIA通过构建一个基于所有客户端加权聚合的教师模型和一个基于参与客户端平均聚合的学生模型，并结合本地自知识蒸馏和服务器辅助的IID数据生成进行训练。实验证明，KDIA在多种异质性设置下能以更少训练轮次实现更高的准确性，尤其在严重异质性条件下表现出显著改进。", "keywords": "联邦学习, 数据异质性, 知识蒸馏, 不公平聚合, 小样本客户端", "comments": "该论文的创新点在于提出了KDIA策略，特别针对联邦学习中大规模客户端部分参与的挑战性场景，通过结合知识蒸馏和基于多种频率（参与间隔、参与次数、数据量）的不公平聚合来有效利用所有客户端的知识，这在处理数据异质性方面具有重要意义。"}}
{"id": "2506.20441", "title": "Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels", "authors": ["Antoine Caradot", "Rémi Emonet", "Amaury Habrard", "Abdel-Rahim Mezidi", "Marc Sebban"], "summary": "Physics-informed Neural Networks (PINNs) have emerged as an efficient way to\nlearn surrogate neural solvers of PDEs by embedding the physical model in the\nloss function and minimizing its residuals using automatic differentiation at\nso-called collocation points. Originally uniformly sampled, the choice of the\nlatter has been the subject of recent advances leading to adaptive sampling\nrefinements. In this paper, we propose a new quadrature method for\napproximating definite integrals based on the hessian of the considered\nfunction, and that we leverage to guide the selection of the collocation points\nduring the training process of PINNs.", "comment": "10 pages. In French. Comments are welcome", "pdf_url": "http://arxiv.org/pdf/2506.20441v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20441v1", "AI": {"title_translation": "基于残差Hessian理论的PINNs求积方法", "tldr": "本文提出了一种新的基于函数Hessian矩阵的求积方法，并将其应用于指导物理信息神经网络（PINNs）训练过程中配置点的选择，以实现自适应采样。", "motivation": "物理信息神经网络（PINNs）通过在损失函数中嵌入物理模型并最小化残差来高效求解偏微分方程（PDEs）。然而，配置点的选择（最初是均匀采样）是限制其性能的因素，近期研究已转向自适应采样改进。本文旨在通过提出一种新方法来进一步优化配置点的选择。", "method": "本文提出了一种新的基于所考虑函数（在PINNs中为残差）Hessian矩阵的定积分近似求积方法。该方法被用于在PINNs训练过程中指导配置点的选择。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "物理信息神经网络（PINNs）已成为一种有效的方法，通过将物理模型嵌入损失函数中，并利用自动微分在所谓的配置点处最小化其残差，来学习偏微分方程（PDEs）的代理神经求解器。最初，这些配置点是均匀采样的，但对它们的选取已成为近期研究的重点，并导致了自适应采样改进。在本文中，我们提出了一种新的基于所考虑函数Hessian矩阵的定积分近似求积方法，并利用该方法在PINNs训练过程中指导配置点的选择。", "summary": "本文提出了一种新颖的求积方法，该方法理论上基于残差的Hessian矩阵，旨在改进物理信息神经网络（PINNs）。该方法被用来指导PINNs训练过程中配置点的自适应选择，解决了均匀采样点的局限性，并建立在自适应采样的最新进展之上，以期实现更高效的偏微分方程求解。", "keywords": "物理信息神经网络, PINNs, 求积方法, Hessian, 自适应采样, 配置点", "comments": "本文的创新点在于将Hessian矩阵引入到PINNs的配置点选择中，这可能为PINNs的训练效率和准确性带来显著提升。它直接解决了PINNs中配置点分布这一关键挑战，有望推动PINNs在求解PDEs方面的应用。"}}
{"id": "2506.20451", "title": "Automatic Demonstration Selection for LLM-based Tabular Data Classification", "authors": ["Shuchu Han", "Wolfgang Bruckner"], "summary": "A fundamental question in applying In-Context Learning (ICL) for tabular data\nclassification is how to determine the ideal number of demonstrations in the\nprompt. This work addresses this challenge by presenting an algorithm to\nautomatically select a reasonable number of required demonstrations. Our method\ndistinguishes itself by integrating not only the tabular data's distribution\nbut also the user's selected prompt template and the specific Large Language\nModel (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed\nalgorithm defines a novel metric to quantify the similarities between different\ndemonstrations. We then construct a similarity graph and analyze the\neigenvalues of its Laplacian to derive the minimum number of demonstrations\ncapable of representing the data within the LLM's intrinsic representation\nspace. We validate the efficacy of our approach through experiments comparing\nits performance against conventional random selection algorithms on diverse\ndatasets and LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20451v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20451v1", "AI": {"title_translation": "LLM驱动的表格数据分类中的自动示例选择", "tldr": "本文提出一种基于谱图理论的算法，自动选择LLM表格数据分类中所需的最佳示例数量，并考虑数据分布、提示模板和LLM本身。", "motivation": "将上下文学习（ICL）应用于表格数据分类时，如何确定提示中理想的示例数量是一个基本问题。", "method": "该方法提出了一种算法来自动选择合理数量的所需示例。其独特之处在于不仅整合了表格数据分布，还整合了用户选择的提示模板和特定的LLM进行估计。该算法植根于谱图理论，定义了一个新颖的度量来量化不同示例之间的相似性，然后构建相似图并分析其拉普拉斯算子的特征值，以推导出能够在LLM内在表示空间中代表数据的最小示例数量。", "result": "通过在不同数据集和LLM上与传统随机选择算法进行比较实验，验证了该方法的有效性。", "conclusion": "该算法能够有效且自动地为LLM的表格数据分类选择合理数量的示例，优于传统随机选择方法。", "translation": "将上下文学习（ICL）应用于表格数据分类的一个基本问题是如何确定提示中理想的示例数量。这项工作通过提出一种算法来解决这一挑战，该算法能够自动选择合理数量的所需示例。我们的方法通过不仅整合表格数据分布，还整合用户选择的提示模板和特定的大型语言模型（LLM）到其估计中，从而独树一帜。我们的算法植根于谱图理论，定义了一个新颖的度量来量化不同示例之间的相似性。然后，我们构建一个相似图并分析其拉普拉斯算子的特征值，以推导出能够在LLM内在表示空间中代表数据的最小示例数量。我们通过实验验证了我们方法的有效性，将其性能与在不同数据集和LLM上的传统随机选择算法进行了比较。", "summary": "本文提出了一种基于谱图理论的算法，用于自动选择LLM在表格数据分类中进行上下文学习所需的最佳示例数量。该方法创新性地考虑了数据分布、提示模板和LLM本身的特性，通过构建相似图并分析其拉普拉斯算子特征值来确定最小示例数。实验证明，该方法在各种数据集和LLM上均优于传统的随机选择策略。", "keywords": "LLM, 上下文学习, 表格数据分类, 示例选择, 谱图理论", "comments": "该论文的创新点在于将谱图理论应用于LLM的上下文学习，以解决示例选择的难题，并且考虑了数据、提示模板和LLM的综合影响，这为提高LLM在表格数据任务上的效率和性能提供了新思路。"}}
{"id": "2506.20494", "title": "Multimodal Representation Learning and Fusion", "authors": ["Qihang Jin", "Enze Ge", "Yuhang Xie", "Hongying Luo", "Junhao Song", "Ziqian Bi", "Chia Xin Liang", "Jibin Guan", "Joe Yeong", "Junfeng Hao"], "summary": "Multi-modal learning is a fast growing area in artificial intelligence. It\ntries to help machines understand complex things by combining information from\ndifferent sources, like images, text, and audio. By using the strengths of each\nmodality, multi-modal learning allows AI systems to build stronger and richer\ninternal representations. These help machines better interpretation, reasoning,\nand making decisions in real-life situations. This field includes core\ntechniques such as representation learning (to get shared features from\ndifferent data types), alignment methods (to match information across\nmodalities), and fusion strategies (to combine them by deep learning models).\nAlthough there has been good progress, some major problems still remain. Like\ndealing with different data formats, missing or incomplete inputs, and\ndefending against adversarial attacks. Researchers now are exploring new\nmethods, such as unsupervised or semi-supervised learning, AutoML tools, to\nmake models more efficient and easier to scale. And also more attention on\ndesigning better evaluation metrics or building shared benchmarks, make it\neasier to compare model performance across tasks and domains. As the field\ncontinues to grow, multi-modal learning is expected to improve many areas:\ncomputer vision, natural language processing, speech recognition, and\nhealthcare. In the future, it may help to build AI systems that can understand\nthe world in a way more like humans, flexible, context aware, and able to deal\nwith real-world complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20494v1", "categories": ["cs.LG", "cs.MM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20494v1", "AI": {"title_translation": "多模态表示学习与融合", "tldr": "多模态学习通过结合图像、文本、音频等不同来源信息，旨在提升AI理解、推理和决策能力。该领域涉及表示学习、对齐和融合技术，面临数据格式差异、缺失输入和对抗性攻击等挑战。未来有望实现更类人的人工智能。", "motivation": "旨在通过结合图像、文本、音频等不同来源的信息，帮助机器理解复杂事物，并构建更强大、更丰富的内部表示，以提升其解释、推理和决策能力。", "method": "核心技术包括表示学习（从不同数据类型获取共享特征）、对齐方法（匹配跨模态信息）和融合策略（通过深度学习模型组合信息）。研究人员还在探索无监督或半监督学习、AutoML工具。", "result": "Not mentioned in abstract", "conclusion": "多模态学习有望改进计算机视觉、自然语言处理、语音识别和医疗保健等领域。未来，它可能有助于构建更像人类、更灵活、更具上下文感知能力且能处理现实世界复杂性的人工智能系统。", "translation": "多模态学习是人工智能领域一个快速发展的方向。它试图通过结合来自不同来源（如图像、文本和音频）的信息来帮助机器理解复杂事物。通过利用每种模态的优势，多模态学习使人工智能系统能够构建更强大、更丰富的内部表示。这些表示有助于机器在现实生活中更好地进行解释、推理和决策。该领域包括核心技术，如表示学习（从不同数据类型中获取共享特征）、对齐方法（匹配跨模态信息）和融合策略（通过深度学习模型组合它们）。尽管已经取得了良好进展，但一些主要问题仍然存在。例如处理不同的数据格式、缺失或不完整的输入以及防御对抗性攻击。研究人员现在正在探索新方法，如无监督或半监督学习、AutoML工具，以使模型更高效和易于扩展。同时，也更关注设计更好的评估指标或构建共享基准，以便更容易地比较跨任务和跨领域的模型性能。随着该领域的持续发展，多模态学习有望改进许多领域：计算机视觉、自然语言处理、语音识别和医疗保健。未来，它可能有助于构建能够以更像人类的方式理解世界的人工智能系统，这些系统将是灵活的、上下文感知的，并能够处理现实世界的复杂性。", "summary": "多模态学习是人工智能领域迅速发展的方向，旨在通过整合图像、文本、音频等多种来源信息，提升机器对复杂事物的理解、推理和决策能力。该领域的核心技术包括表示学习、对齐方法和融合策略。尽管已取得进展，但仍面临数据格式差异、输入不完整及对抗性攻击等挑战。未来的研究将探索无监督/半监督学习、AutoML工具，并关注评估指标和基准的建设。多模态学习有望在计算机视觉、自然语言处理、语音识别和医疗保健等领域带来突破，最终实现更接近人类理解世界的人工智能系统。", "keywords": "多模态学习, 表示学习, 信息融合, 人工智能, 跨模态", "comments": "这篇摘要对多模态学习领域进行了全面的概述，强调了其在提升AI理解能力方面的重要性。它清晰地指出了该领域的核心技术和当前面临的挑战，如数据异构性、不完整输入和对抗性攻击。文章还展望了未来的研究方向和潜在应用，展现了该领域的广阔前景。创新性体现在其综合性，但作为一篇概述性文章，并未提出具体的创新模型或算法。"}}
{"id": "2506.20518", "title": "WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning", "authors": ["Arno Geimer", "Beltran Fiz Pontiveros", "Radu State"], "summary": "Federated Learning (FL) is a collaborative machine learning paradigm which\nallows participants to collectively train a model while training data remains\nprivate. This paradigm is especially beneficial for sectors like finance, where\ndata privacy, security and model performance are paramount. FL has been\nextensively studied in the years following its introduction, leading to, among\nothers, better performing collaboration techniques, ways to defend against\nother clients trying to attack the model, and contribution assessment methods.\nAn important element in for-profit Federated Learning is the development of\nincentive methods to determine the allocation and distribution of rewards for\nparticipants. While numerous methods for allocation have been proposed and\nthoroughly explored, distribution frameworks remain relatively understudied. In\nthis paper, we propose a novel framework which introduces client-specific\ntokens as investment vehicles within the FL ecosystem. Our framework aims to\naddress the limitations of existing incentive schemes by leveraging a\ndecentralized finance (DeFi) platform and automated market makers (AMMs) to\ncreate a more flexible and scalable reward distribution system for\nparticipants, and a mechanism for third parties to invest in the federation\nlearning process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20518v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20518v1", "AI": {"title_translation": "WallStreetFeds：客户端特定代币作为联邦学习中的投资工具", "tldr": "该论文提出了一种名为WallStreetFeds的新颖框架，在联邦学习（FL）中引入客户端特定代币作为投资工具，旨在为参与者创建一个灵活可扩展的奖励分配系统，并使第三方能够投资于联邦学习过程，从而解决现有激励机制的局限性。", "motivation": "联邦学习中现有的奖励分配激励机制存在局限性，特别是在奖励分配框架方面，与分配方法相比，这方面研究相对不足。因此，需要一个更灵活、可扩展的奖励分配系统，以及一个允许第三方投资的机制。", "method": "本文提出了一种新颖的框架，在联邦学习生态系统中引入客户端特定代币作为投资工具。该框架利用去中心化金融（DeFi）平台和自动化做市商（AMMs）来为参与者创建更灵活和可扩展的奖励分配系统，并为第三方提供投资联邦学习过程的机制。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "联邦学习（FL）是一种协作式机器学习范式，允许参与者在训练数据保持私密的同时共同训练模型。这种范式特别有利于金融等领域，因为数据隐私、安全和模型性能至关重要。FL自引入以来已得到广泛研究，从而带来了性能更好的协作技术、防御其他客户端攻击模型的方法以及贡献评估方法。在营利性联邦学习中，一个重要因素是开发激励方法来确定参与者的奖励分配。虽然已经提出了许多分配方法并进行了深入探索，但分配框架仍相对研究不足。在本文中，我们提出了一种新颖的框架，在FL生态系统中引入客户端特定代币作为投资工具。我们的框架旨在通过利用去中心化金融（DeFi）平台和自动化做市商（AMMs）来创建更灵活和可扩展的参与者奖励分配系统，以及第三方投资联邦学习过程的机制，从而解决现有激励方案的局限性。", "summary": "本文介绍了WallStreetFeds，这是一个针对联邦学习的新颖框架，旨在解决当前激励机制，特别是奖励分配方面的局限性。它提出在联邦学习生态系统中使用客户端特定代币作为投资工具，并利用去中心化金融（DeFi）和自动化做市商（AMMs）来为参与者建立一个更灵活、可扩展的奖励分配系统，并促进第三方对联邦学习过程的投资。", "keywords": "联邦学习, 激励机制, 去中心化金融, 客户端特定代币, 奖励分配", "comments": "该论文的创新之处在于将去中心化金融（DeFi）概念，特别是客户端特定代币和自动化做市商（AMMs），整合到联邦学习中，以解决激励机制和奖励分配的关键问题。这种新颖的方法不仅为参与者提供了一种更灵活和可扩展的补偿方式，还为第三方投资开辟了新途径，有望促进联邦学习在商业环境（尤其是在金融领域）中的采用和可持续性。"}}
{"id": "2506.20355", "title": "Practical insights on the effect of different encodings, ansätze and measurements in quantum and hybrid convolutional neural networks", "authors": ["Jesús Lozano-Cruz", "Albert Nieto-Morales", "Oriol Balló-Gimbernat", "Adan Garriga", "Antón Rodríguez-Otero", "Alejandro Borrallo-Rentero"], "summary": "This study investigates the design choices of parameterized quantum circuits\n(PQCs) within quantum and hybrid convolutional neural network (HQNN and QCNN)\narchitectures, applied to the task of satellite image classification using the\nEuroSAT dataset. We systematically evaluate the performance implications of\ndata encoding techniques, variational ans\\\"atze, and measurement in approx. 500\ndistinct model configurations. Our analysis reveals a clear hierarchy of\ninfluence on model performance. For hybrid architectures, which were\nbenchmarked against their direct classical equivalents (e.g. the same\narchitecture with the PQCs removed), the data encoding strategy is the dominant\nfactor, with validation accuracy varying over 30% for distinct embeddings. In\ncontrast, the selection of variational ans\\\"atze and measurement basis had a\ncomparatively marginal effect, with validation accuracy variations remaining\nbelow 5%. For purely quantum models, restricted to amplitude encoding,\nperformance was most dependent on the measurement protocol and the\ndata-to-amplitude mapping. The measurement strategy varied the validation\naccuracy by up to 30% and the encoding mapping by around 8 percentage points.", "comment": "20 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2506.20355v1", "categories": ["quant-ph", "cs.CV"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.20355v1", "AI": {"title_translation": "量子和混合卷积神经网络中不同编码、Ansätze和测量方法影响的实用见解", "tldr": "研究发现，在量子和混合卷积神经网络中，数据编码对混合模型性能影响最大（超30%），而纯量子模型则主要受测量协议影响（高达30%）。", "motivation": "该研究旨在深入了解参数化量子电路（PQCs）在量子和混合卷积神经网络（HQNN和QCNN）架构中的设计选择，特别是数据编码、变分Ansätze和测量方法对卫星图像分类任务性能的影响。", "method": "研究系统地评估了约500种不同模型配置的性能影响，包括数据编码技术、变分Ansätze和测量方法。混合架构与直接的经典对应模型进行了基准测试。", "result": "对于混合架构，数据编码策略是主导因素，验证准确率差异超过30%；变分Ansätze和测量基础的影响较小，验证准确率差异低于5%。对于纯量子模型（限于振幅编码），性能最依赖于测量协议（验证准确率差异高达30%）和数据到振幅的映射（约8个百分点）。", "conclusion": "在量子和混合卷积神经网络的设计中，数据编码策略对混合模型的性能影响最大，而测量协议和数据到振幅的映射对纯量子模型至关重要。Ansätze的选择影响相对较小。", "translation": "本研究调查了应用于EuroSAT数据集卫星图像分类任务的量子和混合卷积神经网络（HQNN和QCNN）架构中参数化量子电路（PQCs）的设计选择。我们系统地评估了约500种不同模型配置中数据编码技术、变分Ansätze和测量方法对性能的影响。我们的分析揭示了对模型性能影响的清晰层级。对于混合架构，其与直接的经典对应模型（例如，移除PQC的相同架构）进行了基准测试，数据编码策略是主导因素，不同嵌入方式的验证准确率差异超过30%。相比之下，变分Ansätze和测量基础的选择影响相对较小，验证准确率差异保持在5%以下。对于纯量子模型，限于振幅编码，性能最依赖于测量协议和数据到振幅的映射。测量策略使验证准确率变化高达30%，编码映射变化约8个百分点。", "summary": "本研究系统评估了量子和混合卷积神经网络（QCNN和HQNN）中参数化量子电路（PQC）的设计选择，包括数据编码、变分Ansätze和测量方法对卫星图像分类性能的影响。研究发现，在混合架构中，数据编码是影响性能的关键因素（准确率变化超30%），而Ansätze和测量的影响较小。对于纯量子模型，测量协议和数据编码映射是性能的主要决定因素。", "keywords": "量子神经网络, 混合神经网络, 数据编码, 量子测量, 卷积神经网络", "comments": "该论文通过对不同编码、Ansätze和测量方法进行大规模系统性评估，为量子和混合卷积神经网络的设计提供了实用的、量化的见解。其创新之处在于明确指出了不同PQC组件对模型性能影响的主次关系，尤其强调了数据编码和测量策略的关键作用，这对于指导未来量子机器学习模型的开发具有重要意义。"}}
{"id": "2506.20537", "title": "Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion", "authors": ["R. Sharma", "M. Raissi", "Y. B. Guo"], "summary": "Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process\nprediction due to the lasting issue of high computation cost using traditional\nnumerical methods such as finite element analysis (FEA). This study presents an\nefficient modeling framework termed FEA-Regulated Physics-Informed Neural\nNetwork (FEA-PINN) to accelerate the thermal field prediction in a LPBF process\nwhile maintaining the FEA accuracy. A novel dynamic material updating strategy\nis developed to capture the dynamic phase change of powder-liquid-solid in the\nPINN model. The PINN model incorporates temperature-dependent material\nproperties and phase change behavior using the apparent heat capacity method.\nWhile the PINN model demonstrates high accuracy with a small training data and\nenables generalization of new process parameters via transfer learning, it\nfaces the challenge of high computation cost in time-dependent problems due to\nthe residual accumulation. To overcome this issue, the FEA-PINN framework\nintegrates corrective FEA simulations during inference to enforce physical\nconsistency and reduce error drift. A comparative analysis shows that FEA-PINN\nachieves equivalent accuracy to FEA while significantly reducing computational\ncost. The framework has been validated using the benchmark FEA data and\ndemonstrated through single-track scanning in LPBF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20537v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20537v1", "AI": {"title_translation": "激光粉末床熔融模拟加速的有限元分析调控物理信息机器学习", "tldr": "提出FEA-PINN框架，通过结合有限元分析来加速激光粉末床熔融的热场模拟，同时保持精度并显著降低计算成本。", "motivation": "传统数值方法（如有限元分析FEA）在激光粉末床熔融（LPBF）模拟中计算成本高昂，导致效率低下，因此需要更高效的模拟方法进行过程预测。", "method": "本研究提出了一个名为FEA-Regulated Physics-Informed Neural Network (FEA-PINN) 的高效建模框架。该框架通过整合纠正性FEA模拟来解决PINN在时间依赖问题中计算成本高和误差漂移的问题，以强制物理一致性并减少误差漂移。PINN模型中开发了动态材料更新策略以捕获粉末-液体-固体相变，并使用表观热容法纳入了温度依赖的材料属性和相变行为。", "result": "FEA-PINN框架实现了与传统有限元分析（FEA）相当的精度，同时显著降低了计算成本。该框架已通过基准FEA数据验证，并在LPBF的单道扫描中得到展示。", "conclusion": "FEA-PINN框架能够加速激光粉末床熔融过程中的热场预测，同时保持传统有限元分析的精度，并显著降低计算成本，克服了传统PINN在时间依赖问题中的局限性。", "translation": "激光粉末床熔融（LPBF）的高效模拟对于过程预测至关重要，因为传统数值方法如有限元分析（FEA）存在计算成本高昂的长期问题。本研究提出了一种名为FEA调控的物理信息神经网络（FEA-PINN）的高效建模框架，旨在加速LPBF过程中的热场预测，同时保持FEA的精度。开发了一种新颖的动态材料更新策略，以在PINN模型中捕获粉末-液体-固体的动态相变。PINN模型采用表观热容法纳入了温度依赖的材料属性和相变行为。尽管PINN模型以少量训练数据展示出高精度，并能通过迁移学习实现新工艺参数的泛化，但由于残差累积，它在时间依赖问题中面临高计算成本的挑战。为了克服这个问题，FEA-PINN框架在推理过程中整合了纠正性FEA模拟，以强制物理一致性并减少误差漂移。比较分析表明，FEA-PINN在显著降低计算成本的同时，实现了与FEA相当的精度。该框架已使用基准FEA数据进行了验证，并通过LPBF中的单道扫描进行了展示。", "summary": "本文提出了FEA-Regulated Physics-Informed Neural Network (FEA-PINN) 框架，旨在解决激光粉末床熔融（LPBF）模拟中传统有限元分析（FEA）计算成本高的问题。FEA-PINN通过整合纠正性FEA模拟来克服物理信息神经网络（PINN）在时间依赖问题中的计算成本和误差漂移挑战，同时纳入动态材料更新和温度依赖属性。实验结果表明，FEA-PINN在显著降低计算成本的同时，能保持与FEA相当的精度。", "keywords": "激光粉末床熔融, 物理信息神经网络, 有限元分析, 模拟加速, 相变", "comments": "该论文创新性地将有限元分析与物理信息神经网络结合，解决了PINN在处理时间依赖问题中误差累积和计算成本高的问题。通过FEA的调控，FEA-PINN在保持高精度的同时显著提升了LPBF模拟的效率，对于加速增材制造过程的预测和优化具有重要意义。"}}
{"id": "2506.20543", "title": "Demonstration of effective UCB-based routing in skill-based queues on real-world data", "authors": ["Sanne van Kempen", "Jaron Sanders", "Fiona Sloothaak", "Maarten G. Wolf"], "summary": "This paper is about optimally controlling skill-based queueing systems such\nas data centers, cloud computing networks, and service systems. By means of a\ncase study using a real-world data set, we investigate the practical\nimplementation of a recently developed reinforcement learning algorithm for\noptimal customer routing. Our experiments show that the algorithm efficiently\nlearns and adapts to changing environments and outperforms static benchmark\npolicies, indicating its potential for live implementation. We also augment the\nreal-world applicability of this algorithm by introducing a new heuristic\nrouting rule to reduce delays. Moreover, we show that the algorithm can\noptimize for multiple objectives: next to payoff maximization, secondary\nobjectives such as server load fairness and customer waiting time reduction can\nbe incorporated. Tuning parameters are used for balancing inherent performance\ntrade--offs. Lastly, we investigate the sensitivity to estimation errors and\nparameter tuning, providing valuable insights for implementing adaptive routing\nalgorithms in complex real-world queueing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20543v1", "categories": ["cs.LG", "math.OC", "60K25, 93E35"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20543v1", "AI": {"title_translation": "基于UCB的技能队列实时数据有效路由演示", "tldr": "本文基于真实世界数据，展示了一种基于强化学习（UCB）的算法，用于优化技能型排队系统中的客户路由。该算法能够高效学习、适应环境、优于静态策略、减少延迟，并能实现多目标优化，为实际部署提供了宝贵见解。", "motivation": "本文旨在优化控制数据中心、云计算网络和服务系统等技能型排队系统，并利用真实世界数据集，研究一种新开发的强化学习算法在优化客户路由方面的实际应用。", "method": "通过使用真实世界数据集进行案例研究，研究了一种用于优化客户路由的强化学习算法。引入了一种新的启发式路由规则以减少延迟。算法被设计用于优化多个目标（收益最大化、服务器负载公平性、客户等待时间减少），并通过调整参数平衡性能权衡。最后，研究了对估计误差和参数调整的敏感性。", "result": "该算法能够高效学习并适应不断变化的环境，且优于静态基准策略。引入的新启发式路由规则能够减少延迟。该算法可以针对多个目标进行优化。研究提供了关于估计误差和参数调整敏感性的宝贵见解，有助于在复杂真实世界排队系统中实施自适应路由算法。", "conclusion": "该基于UCB的强化学习算法在技能型排队系统中具有实时实施的潜力。它在适应性、多目标优化能力以及对调整参数和误差的敏感性方面，为实施自适应路由提供了宝贵的见解。", "translation": "本文旨在优化控制技能型排队系统，例如数据中心、云计算网络和服务系统。通过使用真实世界数据集的案例研究，我们研究了最近开发的用于优化客户路由的强化学习算法的实际实现。我们的实验表明，该算法能够有效地学习并适应不断变化的环境，并且优于静态基准策略，这表明其具有实时实施的潜力。我们还通过引入一种新的启发式路由规则来减少延迟，从而增强了该算法在现实世界中的适用性。此外，我们展示了该算法可以针对多个目标进行优化：除了最大化收益外，还可以纳入服务器负载公平性和客户等待时间减少等次要目标。使用调整参数来平衡固有的性能权衡。最后，我们研究了对估计误差和参数调整的敏感性，为在复杂的真实世界排队系统中实施自适应路由算法提供了宝贵的见解。", "summary": "本文基于真实世界数据，探讨了一种基于UCB的强化学习算法在技能型排队系统最优客户路由中的实际应用。研究表明，该算法能高效学习和适应环境，性能优于静态策略，并通过引入新启发式规则有效减少延迟。此外，该算法具备多目标优化能力（如收益、负载公平性、等待时间），并提供了关于其对误差和参数调整敏感性的见解，突显了其在复杂真实世界场景中实时部署的潜力。", "keywords": "技能型队列, 强化学习, UCB路由, 多目标优化, 真实世界数据", "comments": "该论文通过在真实世界数据上验证基于UCB的强化学习路由方法，展示了其在技能型排队系统实际应用中的重要贡献。其创新之处在于引入了新的启发式规则以减少延迟，并证明了多目标优化能力。对估计误差和参数调整敏感性的深入分析，为实际部署提供了宝贵指导，是迈向更自适应、高效队列管理的重要一步。"}}
{"id": "2506.20574", "title": "Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series", "authors": ["Laura Boggia", "Rafael Teixeira de Lima", "Bogdan Malaescu"], "summary": "Anomaly detection in multivariate time series is an important problem across\nvarious fields such as healthcare, financial services, manufacturing or physics\ndetector monitoring. Accurately identifying when unexpected errors or faults\noccur is essential, yet challenging, due to the unknown nature of anomalies and\nthe complex interdependencies between time series dimensions. In this paper, we\ninvestigate transformer-based approaches for time series anomaly detection,\nfocusing on the recently proposed iTransformer architecture. Our contributions\nare fourfold: (i) we explore the application of the iTransformer to time series\nanomaly detection, and analyse the influence of key parameters such as window\nsize, step size, and model dimensions on performance; (ii) we examine methods\nfor extracting anomaly labels from multidimensional anomaly scores and discuss\nappropriate evaluation metrics for such labels; (iii) we study the impact of\nanomalous data present during training and assess the effectiveness of\nalternative loss functions in mitigating their influence; and (iv) we present a\ncomprehensive comparison of several transformer-based models across a diverse\nset of datasets for time series anomaly detection.", "comment": "Submitted to VLDB 2026 conference, currently under review", "pdf_url": "http://arxiv.org/pdf/2506.20574v1", "categories": ["cs.LG", "stat.ME"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20574v1", "AI": {"title_translation": "多元时间序列异常检测的无监督策略基准测试", "tldr": "本文研究了基于Transformer的多元时间序列异常检测方法，重点关注iTransformer，并分析了其参数、异常标签提取方法、训练中异常数据的影响，并对多种Transformer模型进行了全面比较。", "motivation": "多元时间序列中的异常检测在医疗保健、金融服务、制造业或物理探测器监控等各个领域都是一个重要问题。由于异常的未知性质以及时间序列维度之间复杂的相互依赖关系，准确识别意外错误或故障的发生至关重要但具有挑战性。", "method": "本文研究了基于Transformer的时间序列异常检测方法，重点关注最近提出的iTransformer架构。具体贡献包括：探讨iTransformer在时间序列异常检测中的应用，并分析窗口大小、步长和模型维度等关键参数对性能的影响；研究从多维异常分数中提取异常标签的方法并讨论合适的评估指标；研究训练过程中异常数据的影响并评估替代损失函数缓解其影响的有效性；对多种Transformer模型在不同时间序列异常检测数据集上进行全面比较。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "多元时间序列中的异常检测是医疗保健、金融服务、制造业或物理探测器监控等各个领域的一个重要问题。由于异常的未知性质以及时间序列维度之间复杂的相互依赖关系，准确识别意外错误或故障的发生至关重要但具有挑战性。在本文中，我们研究了基于Transformer的时间序列异常检测方法，重点关注最近提出的iTransformer架构。我们的贡献有四方面：(i) 我们探索了iTransformer在时间序列异常检测中的应用，并分析了窗口大小、步长和模型维度等关键参数对性能的影响；(ii) 我们研究了从多维异常分数中提取异常标签的方法，并讨论了此类标签的适当评估指标；(iii) 我们研究了训练过程中存在的异常数据的影响，并评估了替代损失函数在减轻其影响方面的有效性；(iv) 我们对多种基于Transformer的模型在各种时间序列异常检测数据集上进行了全面比较。", "summary": "本文针对多元时间序列异常检测这一重要且具挑战性的问题，深入探讨了基于Transformer的方法，特别是iTransformer架构。研究内容包括分析iTransformer关键参数的影响、异常标签的提取与评估、训练数据中异常值的影响及损失函数的缓解作用，并对多种Transformer模型在不同数据集上进行了综合比较。", "keywords": "多元时间序列, 异常检测, Transformer, iTransformer, 基准测试", "comments": "本文对基于Transformer的多元时间序列异常检测进行了全面的基准测试和分析，特别关注了iTransformer。其创新点在于系统性地探讨了参数影响、异常标签处理和训练鲁棒性，并通过广泛的比较提供了宝贵的见解，为该领域未来的研究奠定了基础。"}}
{"id": "2506.20575", "title": "Exploring Graph-Transformer Out-of-Distribution Generalization Abilities", "authors": ["Itay Niv", "Neta Rabin"], "summary": "Deep learning on graphs has shown remarkable success across numerous\napplications, including social networks, bio-physics, traffic networks, and\nrecommendation systems. Regardless of their successes, current methods\nfrequently depend on the assumption that training and testing data share the\nsame distribution, a condition rarely met in real-world scenarios. While\ngraph-transformer (GT) backbones have recently outperformed traditional\nmessage-passing neural networks (MPNNs) in multiple in-distribution (ID)\nbenchmarks, their effectiveness under distribution shifts remains largely\nunexplored.\n  In this work, we address the challenge of out-of-distribution (OOD)\ngeneralization for graph neural networks, with a special focus on the impact of\nbackbone architecture. We systematically evaluate GT and hybrid backbones in\nOOD settings and compare them to MPNNs. To do so, we adapt several leading\ndomain generalization (DG) algorithms to work with GTs and assess their\nperformance on a benchmark designed to test a variety of distribution shifts.\nOur results reveal that GT and hybrid GT-MPNN backbones consistently\ndemonstrate stronger generalization ability compared to MPNNs, even without\nspecialized DG algorithms.\n  Additionally, we propose a novel post-training analysis approach that\ncompares the clustering structure of the entire ID and OOD test datasets,\nspecifically examining domain alignment and class separation. Demonstrating its\nmodel-agnostic design, this approach not only provided meaningful insights into\nGT and MPNN backbones. It also shows promise for broader applicability to DG\nproblems beyond graph learning, offering a deeper perspective on generalization\nabilities that goes beyond standard accuracy metrics. Together, our findings\nhighlight the promise of graph-transformers for robust, real-world graph\nlearning and set a new direction for future research in OOD generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20575v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20575v1", "AI": {"title_translation": "探索图Transformer的分布外泛化能力", "tldr": "图Transformer在分布外泛化能力上优于传统图神经网络，且提出了一种新的后训练分析方法。", "motivation": "现有图深度学习方法依赖训练和测试数据同分布的假设，这在现实世界中很少满足。图Transformer在同分布基准测试中表现优异，但在分布偏移下的有效性尚未充分探索。", "method": "系统评估了图Transformer (GT) 和混合骨干网络在OOD设置下的性能，并与MPNNs进行比较。为此，调整了几个领先的领域泛化 (DG) 算法以与GTs配合，并在一个旨在测试各种分布偏移的基准上评估其性能。此外，提出了一种新颖的后训练分析方法，比较ID和OOD测试数据集的聚类结构，检查领域对齐和类别分离。", "result": "GT和混合GT-MPNN骨干网络即使没有专门的DG算法，也始终表现出比MPNNs更强的泛化能力。提出的后训练分析方法不仅为GT和MPNN骨干网络提供了有意义的见解，还显示出在图学习之外的DG问题上的广泛适用性。", "conclusion": "图Transformer在鲁棒的现实世界图学习中展现出前景，并为OOD泛化未来的研究设定了新方向。", "translation": "图上的深度学习在社交网络、生物物理学、交通网络和推荐系统等众多应用中取得了显著成功。然而，尽管取得了成功，当前的方法通常依赖于训练和测试数据共享相同分布的假设，而这种情况在现实世界中很少得到满足。尽管图Transformer（GT）骨干网络最近在多个同分布（ID）基准测试中超越了传统的图消息传递神经网络（MPNNs），但它们在分布偏移下的有效性在很大程度上仍未被探索。\n在这项工作中，我们解决了图神经网络的分布外（OOD）泛化挑战，特别关注骨干架构的影响。我们系统地评估了GT和混合骨干网络在OOD设置下的性能，并将其与MPNNs进行比较。为此，我们调整了几种领先的领域泛化（DG）算法，使其与GTs配合，并在一个旨在测试各种分布偏移的基准上评估它们的性能。我们的结果表明，即使没有专门的DG算法，GT和混合GT-MPNN骨干网络也始终表现出比MPNNs更强的泛化能力。\n此外，我们提出了一种新颖的后训练分析方法，该方法比较了整个ID和OOD测试数据集的聚类结构，特别是检查了领域对齐和类别分离。该方法展示了其模型无关的设计，不仅为GT和MPNN骨干网络提供了有意义的见解。它还显示出在图学习之外的DG问题上的更广泛适用性，提供了超越标准准确性指标的更深层泛化能力视角。总之，我们的发现凸显了图Transformer在鲁棒的现实世界图学习中的前景，并为未来OOD泛化研究设定了新方向。", "summary": "本文探讨了图Transformer（GT）在分布外（OOD）泛化方面的能力，解决了现有图深度学习方法在现实世界中面临的同分布假设限制。研究系统评估了GT和混合骨干网络在OOD设置下的性能，并与传统图消息传递神经网络（MPNNs）进行比较。结果表明，GT和混合GT-MPNN骨干网络即使在没有特定领域泛化算法的情况下，也展现出优于MPNNs的泛化能力。此外，论文提出了一种新颖的模型无关的后训练分析方法，用于评估ID和OOD数据集的聚类结构，为理解泛化能力提供了新的视角，并对未来的OOD泛化研究方向提出了建议。", "keywords": "图Transformer, 分布外泛化, 图神经网络, 领域泛化, 聚类分析", "comments": "本文的创新点在于系统地评估了图Transformer在分布外泛化能力上的表现，并证明了其优于传统MPNNs。此外，提出的模型无关的后训练分析方法为理解和评估图神经网络的泛化能力提供了一个有价值的新工具，其潜在应用范围超越了图学习领域。这项工作对于推动图深度学习在现实世界复杂环境中的应用具有重要意义。"}}
{"id": "2506.20584", "title": "The kernel of graph indices for vector search", "authors": ["Mariano Tepper", "Ted Willke"], "summary": "The most popular graph indices for vector search use principles from\ncomputational geometry to build the graph. Hence, their formal graph\nnavigability guarantees are only valid in Euclidean space. In this work, we\nshow that machine learning can be used to build graph indices for vector search\nin metric and non-metric vector spaces (e.g., for inner product similarity).\nFrom this novel perspective, we introduce the Support Vector Graph (SVG), a new\ntype of graph index that leverages kernel methods to establish the graph\nconnectivity and that comes with formal navigability guarantees valid in metric\nand non-metric vector spaces. In addition, we interpret the most popular graph\nindices, including HNSW and DiskANN, as particular specializations of SVG and\nshow that new indices can be derived from the principles behind this\nspecialization. Finally, we propose SVG-L0 that incorporates an $\\ell_0$\nsparsity constraint into the SVG kernel method to build graphs with a bounded\nout-degree. This yields a principled way of implementing this practical\nrequirement, in contrast to the traditional heuristic of simply truncating the\nout edges of each node. Additionally, we show that SVG-L0 has a self-tuning\nproperty that avoids the heuristic of using a set of candidates to find the\nout-edges of each node and that keeps its computational complexity in check.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20584v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20584v1", "AI": {"title_translation": "向量搜索图索引的核", "tldr": "本文提出了一种基于机器学习和核方法的新型向量搜索图索引——支持向量图（SVG），它在度量和非度量向量空间中提供形式化的可导航性保证，并通过SVG-L0实现了出度有界的自调优图构建。", "motivation": "现有的向量搜索图索引主要基于计算几何，其可导航性保证仅限于欧几里得空间，无法有效处理度量和非度量向量空间（如内积相似性）中的搜索。", "method": "本文提出了一种利用机器学习和核方法构建向量搜索图索引的新范式。具体引入了支持向量图（SVG），它利用核方法建立图连接性，并提供在度量和非度量向量空间中均有效的形式化可导航性保证。此外，通过将流行的图索引（如HNSW和DiskANN）解释为SVG的特定特化，并在此基础上推导出新的索引。最后，提出了SVG-L0，将$\\ell_0$稀疏性约束引入SVG核方法中，以构建出度有界的图。", "result": "引入了支持向量图（SVG），这是一种新的图索引类型，利用核方法建立图连接性，并在度量和非度量向量空间中提供了形式化的可导航性保证。将HNSW和DiskANN等流行图索引解释为SVG的特定特化，并展示了可以从这些特化原理中推导出新的索引。提出了SVG-L0，它通过结合$\\ell_0$稀疏性约束实现了出度有界的图，提供了一种实现此实用要求的原则性方法，并具有自调优特性，避免了启发式方法并控制了计算复杂度。", "conclusion": "本文提出了一种基于机器学习和核方法构建向量搜索图索引的新框架，克服了传统方法仅限于欧几里得空间的局限性，并在度量和非度量空间中提供了形式化的可导航性保证。通过引入SVG及其变体SVG-L0，为构建更通用、更高效且具有理论支持的向量搜索图索引提供了原则性的方法。", "translation": "向量搜索图索引的核\n\n最流行的向量搜索图索引利用计算几何原理来构建图。因此，它们的形式化图可导航性保证仅在欧几里得空间中有效。在这项工作中，我们展示了机器学习可以用于在度量和非度量向量空间（例如，用于内积相似性）中构建向量搜索的图索引。从这个新颖的角度出发，我们引入了支持向量图（SVG），这是一种新型的图索引，它利用核方法建立图连接性，并附带在度量和非度量向量空间中均有效的形式化可导航性保证。此外，我们将包括HNSW和DiskANN在内的最流行的图索引解释为SVG的特定特化，并表明可以从这种特化背后的原理中推导出新的索引。最后，我们提出了SVG-L0，它将$\\ell_0$稀疏性约束纳入SVG核方法中，以构建出度有界的图。这提供了一种实现此实用要求的原则性方法，与简单截断每个节点的出边的传统启发式方法形成对比。此外，我们展示了SVG-L0具有自调优特性，避免了使用一组候选节点来查找每个节点出边的启发式方法，并控制了其计算复杂度。", "summary": "本文提出了一种基于机器学习的新型向量搜索图索引范式，旨在解决现有方法仅限于欧几里得空间的局限性。核心贡献是引入了支持向量图（SVG），它利用核方法构建图连接性，并首次在度量和非度量向量空间中提供了形式化的可导航性保证。研究还揭示了流行图索引（如HNSW和DiskANN）是SVG的特化，并在此基础上提出了SVG-L0，通过$\\ell_0$稀疏性约束实现出度有界的图，提供了一种更具原则性且自调优的图构建方法，显著提升了向量搜索图索引的普适性和效率。", "keywords": "向量搜索, 图索引, 核方法, 支持向量图, 度量空间", "comments": "这篇论文通过引入基于机器学习和核方法的支持向量图（SVG），为向量搜索图索引领域带来了重要的创新。其核心贡献在于将形式化的可导航性保证扩展到度量和非度量向量空间，这克服了传统方法仅限于欧几里得空间的显著局限性，极大地拓宽了图索引的应用范围。此外，将现有流行算法（如HNSW和DiskANN）解释为SVG的特化，不仅提供了统一的理论框架，也为未来新算法的设计提供了新的视角。SVG-L0通过引入$\\ell_0$稀疏性约束来控制出度，并实现自调优，解决了实际应用中图构建的启发式问题，使得图结构更加优化和可控。这篇工作在理论和实践上都具有重要意义，为高维向量搜索提供了更为健壮和通用的解决方案。"}}
{"id": "2506.20607", "title": "H-FEX: A Symbolic Learning Method for Hamiltonian Systems", "authors": ["Jasen Lai", "Senwei Liang", "Chunmei Wang"], "summary": "Hamiltonian systems describe a broad class of dynamical systems governed by\nHamiltonian functions, which encode the total energy and dictate the evolution\nof the system. Data-driven approaches, such as symbolic regression and neural\nnetwork-based methods, provide a means to learn the governing equations of\ndynamical systems directly from observational data of Hamiltonian systems.\nHowever, these methods often struggle to accurately capture complex Hamiltonian\nfunctions while preserving energy conservation. To overcome this limitation, we\npropose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),\na symbolic learning method that introduces novel interaction nodes designed to\ncapture intricate interaction terms effectively. Our experiments, including\nthose on highly stiff dynamical systems, demonstrate that H-FEX can recover\nHamiltonian functions of complex systems that accurately capture system\ndynamics and preserve energy over long time horizons. These findings highlight\nthe potential of H-FEX as a powerful framework for discovering closed-form\nexpressions of complex dynamical systems.", "comment": "16 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.20607v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20607v1", "AI": {"title_translation": "H-FEX：一种用于哈密顿系统的符号学习方法", "tldr": "H-FEX是一种新的符号学习方法，能够从观测数据中准确学习复杂哈密顿系统的哈密顿函数，并保持能量守恒。", "motivation": "现有数据驱动方法（如符号回归和神经网络）在学习复杂哈密顿函数时难以准确捕捉并同时保持能量守恒。", "method": "本文提出了哈密顿系统有限表达式方法（H-FEX），这是一种符号学习方法，通过引入新颖的交互节点来有效捕捉复杂的交互项。", "result": "实验（包括对高刚度动力系统的测试）表明，H-FEX能够恢复复杂系统的哈密顿函数，这些函数能准确捕捉系统动力学并在长时间范围内保持能量。", "conclusion": "H-FEX作为发现复杂动力系统闭式表达式的强大框架，具有巨大潜力。", "translation": "哈密顿系统描述了一大类由哈密顿函数控制的动力系统，哈密顿函数编码总能量并决定系统的演化。数据驱动方法，如符号回归和基于神经网络的方法，提供了一种直接从哈密顿系统观测数据中学习动力系统控制方程的手段。然而，这些方法在准确捕捉复杂哈密顿函数的同时保持能量守恒方面常常遇到困难。为了克服这一限制，我们提出了用于学习哈密顿系统的有限表达式方法（H-FEX），这是一种符号学习方法，引入了新颖的交互节点，旨在有效捕捉复杂的交互项。我们的实验，包括对高刚度动力系统的实验，表明H-FEX能够恢复复杂系统的哈密顿函数，这些函数能准确捕捉系统动力学并在长时间范围内保持能量。这些发现突出了H-FEX作为发现复杂动力系统闭式表达式的强大框架的潜力。", "summary": "本文提出了H-FEX，一种新的符号学习方法，旨在克服现有数据驱动方法在学习复杂哈密顿函数时难以保持能量守恒的限制。H-FEX引入了创新的交互节点来有效捕获复杂的交互项。实验证明，H-FEX能准确恢复复杂哈密顿系统的函数，有效捕捉系统动力学并在长时间内保持能量守恒，显示其在发现复杂动力系统闭式表达式方面的巨大潜力。", "keywords": "哈密顿系统, 符号学习, H-FEX, 能量守恒, 动力系统", "comments": "H-FEX的创新点在于引入了新颖的交互节点，这有助于更准确地捕捉复杂哈密顿函数中的交互项，从而克服了现有数据驱动方法在保持能量守恒方面的不足。其重要性在于为发现复杂动力系统的闭式表达式提供了一个有效且强大的框架，尤其是在需要长期能量守恒的场景下。"}}
{"id": "2506.20623", "title": "Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning", "authors": ["Fariba Jangjoo", "Matteo Marsili", "Yasser Roudi"], "summary": "Closed-loop learning is the process of repeatedly estimating a model from\ndata generated from the model itself. It is receiving great attention due to\nthe possibility that large neural network models may, in the future, be\nprimarily trained with data generated by artificial neural networks themselves.\nWe study this process for models that belong to exponential families, deriving\nequations of motions that govern the dynamics of the parameters. We show that\nmaximum likelihood estimation of the parameters endows sufficient statistics\nwith the martingale property and that as a result the process converges to\nabsorbing states that amplify initial biases present in the data. However, we\nshow that this outcome may be prevented by polluting the data with an\ninfinitesimal fraction of data points generated from a fixed model, by relying\non maximum a posteriori estimation or by introducing regularisation.\nFurthermore, we show that the asymptotic behavior of the dynamics is not\nreparametrisation invariant.", "comment": "13 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20623v1", "categories": ["cs.LG", "cond-mat.dis-nn", "physics.data-an", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20623v1", "AI": {"title_translation": "在再训练中迷失：闭环学习下指数族参数空间的漫游", "tldr": "闭环学习中，指数族模型的参数在最大似然估计下会收敛到放大初始偏差的吸收态，但可通过数据污染、MAP或正则化来避免。", "motivation": "闭环学习受到广泛关注，因为未来大型神经网络模型可能主要通过人工神经网络自身生成的数据进行训练。本研究旨在理解这种自我训练过程中参数的动态行为。", "method": "研究了属于指数族模型的闭环学习过程，推导了控制参数动态的运动方程。", "result": "1. 参数的最大似然估计使充分统计量具有鞅性质。2. 学习过程收敛到放大初始数据偏差的吸收态。3. 可通过用少量来自固定模型的数据污染、依赖最大后验估计或引入正则化来防止这种结果。4. 动态的渐近行为不是重参数化不变的。", "conclusion": "在闭环学习中，指数族模型的最大似然估计会导致参数收敛并放大初始偏差，但可以通过特定策略（如数据污染、最大后验估计或正则化）来缓解或避免这种不良结果，并且动态的渐近行为与参数化方式相关。", "translation": "闭环学习是从模型自身生成的数据中重复估计模型的过程。由于未来大型神经网络模型可能主要通过人工神经网络自身生成的数据进行训练的可能性，它正受到广泛关注。我们研究了属于指数族模型的这一过程，推导了控制参数动态的运动方程。我们表明，参数的最大似然估计使充分统计量具有鞅性质，结果是该过程收敛到放大数据中初始偏差的吸收态。然而，我们表明可以通过用来自固定模型的极少量数据点污染数据、依赖最大后验估计或引入正则化来防止这种结果。此外，我们表明动态的渐近行为不是重参数化不变的。", "summary": "该论文研究了闭环学习中指数族模型的参数动态，闭环学习是指模型从自身生成的数据中进行重复估计。研究发现，在最大似然估计下，参数会收敛到放大初始数据偏差的吸收态。为防止这种不良结果，论文提出了几种策略，包括数据污染、最大后验估计或引入正则化。此外，研究还指出动态的渐近行为与参数化方式相关。", "keywords": "闭环学习, 指数族, 参数动态, 最大似然估计, 偏差放大", "comments": "这篇论文深入探讨了闭环学习中一个潜在的严重问题：模型在自我生成数据训练下可能陷入并放大初始偏差。其创新点在于将这一复杂动态建模为指数族模型的参数运动方程，并揭示了最大似然估计下的鞅性质和吸收态收敛。提出通过数据污染、MAP或正则化来缓解问题，提供了实际的应对策略，对于未来大型自训AI模型的稳定性具有重要指导意义。"}}
{"id": "2506.20644", "title": "Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices", "authors": ["Hangyu Li", "Hongyue Wu", "Guodong Fan", "Zhen Zhang", "Shizhan Chen", "Zhiyong Feng"], "summary": "As privacy protection gains increasing importance, more models are being\ntrained on edge devices and subsequently merged into the central server through\nFederated Learning (FL). However, current research overlooks the impact of\nnetwork topology, physical distance, and data heterogeneity on edge devices,\nleading to issues such as increased latency and degraded model performance. To\naddress these issues, we propose a new federated learning scheme on edge\ndevices that called Federated Learning with Encrypted Data Sharing(FedEDS).\nFedEDS uses the client model and the model's stochastic layer to train the data\nencryptor. The data encryptor generates encrypted data and shares it with other\nclients. The client uses the corresponding client's stochastic layer and\nencrypted data to train and adjust the local model. FedEDS uses the client's\nlocal private data and encrypted shared data from other clients to train the\nmodel. This approach accelerates the convergence speed of federated learning\ntraining and mitigates the negative impact of data heterogeneity, making it\nsuitable for application services deployed on edge devices requiring rapid\nconvergence. Experiments results show the efficacy of FedEDS in promoting model\nperformance.", "comment": "Accepted by ICWS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20644v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20644v1", "AI": {"title_translation": "面向数据异构边缘设备的加密数据共享高效联邦学习", "tldr": "本文提出FedEDS，一种新的联邦学习方案，通过加密数据共享加速收敛并缓解数据异构性对边缘设备的影响，从而提升模型性能。", "motivation": "当前的联邦学习研究忽略了网络拓扑、物理距离和数据异构性对边缘设备的影响，导致延迟增加和模型性能下降。", "method": "提出了一种名为“联邦学习与加密数据共享”（FedEDS）的联邦学习方案。FedEDS利用客户端模型和模型的随机层训练数据加密器，生成加密数据并与其他客户端共享。客户端利用相应的随机层和加密数据训练和调整本地模型。FedEDS使用客户端的本地私有数据和来自其他客户端的加密共享数据来训练模型。", "result": "实验结果表明FedEDS在促进模型性能方面是有效的。它加速了联邦学习训练的收敛速度，并减轻了数据异构性的负面影响。", "conclusion": "FedEDS适用于需要快速收敛的边缘设备上部署的应用服务，因为它能加速收敛、减轻数据异构性并提升模型性能。", "translation": "随着隐私保护日益重要，越来越多的模型在边缘设备上进行训练，并通过联邦学习（FL）合并到中央服务器。然而，当前研究忽视了网络拓扑、物理距离和数据异构性对边缘设备的影响，导致延迟增加和模型性能下降等问题。为了解决这些问题，我们提出了一种新的边缘设备联邦学习方案，称为联邦学习与加密数据共享（FedEDS）。FedEDS利用客户端模型和模型的随机层来训练数据加密器。数据加密器生成加密数据并与其他客户端共享。客户端使用相应的客户端随机层和加密数据来训练和调整本地模型。FedEDS使用客户端的本地私有数据和来自其他客户端的加密共享数据来训练模型。这种方法加速了联邦学习训练的收敛速度，并减轻了数据异构性的负面影响，使其适用于需要快速收敛的边缘设备上部署的应用服务。实验结果表明FedEDS在提升模型性能方面的有效性。", "summary": "本文提出了一种名为联邦学习与加密数据共享（FedEDS）的新型联邦学习方案，旨在解决数据异构边缘设备中联邦学习面临的延迟和性能下降问题。FedEDS通过客户端模型和随机层训练数据加密器，实现加密数据在客户端间的共享。该方法结合本地私有数据和加密共享数据进行模型训练，有效加速了联邦学习的收敛速度，并减轻了数据异构性的负面影响，实验结果验证了其在提升模型性能方面的有效性。", "keywords": "联邦学习, 加密数据共享, 边缘设备, 数据异构性, 收敛速度", "comments": "该论文的创新点在于利用加密数据共享来解决联邦学习中数据异构性问题并加速收敛，这对于边缘设备的实际部署至关重要。尽管摘要中未详细说明，但关注网络拓扑和物理距离作为被忽视的因素也值得关注。"}}
{"id": "2506.20650", "title": "Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer", "authors": ["Anqi Mao", "Mehryar Mohri", "Yutao Zhong"], "summary": "The problem of learning to defer with multiple experts consists of optimally\nassigning input instances to experts, balancing the trade-off between their\naccuracy and computational cost. This is a critical challenge in natural\nlanguage generation, but also in other fields such as image processing, and\nmedical diagnostics. Recent studies have proposed surrogate loss functions to\noptimize deferral, but challenges remain in ensuring their consistency\nproperties. This paper introduces novel surrogate loss functions and efficient\nalgorithms with strong theoretical learning guarantees. We address open\nquestions regarding realizable $H$-consistency, $H$-consistency bounds, and\nBayes-consistency for both single-stage (jointly learning predictor and\ndeferral function) and two-stage (learning only the deferral function with a\nfixed expert) learning scenarios. For single-stage deferral, we introduce a\nfamily of new realizable $H$-consistent surrogate losses and further prove\n$H$-consistency for a selected member. For two-stage deferral, we derive new\nsurrogate losses that achieve realizable $H$-consistency, $H$-consistency\nbounds, and Bayes-consistency for the two-expert scenario and, under natural\nassumptions, multiple-expert scenario. Additionally, we provide enhanced\ntheoretical guarantees under low-noise assumptions for both scenarios. Finally,\nwe report the results of experiments using our proposed surrogate losses,\ncomparing their performance against existing baselines.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.20650v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20650v1", "AI": {"title_translation": "掌握多专家路由：可实现H-一致性和学习推迟的强大保证", "tldr": "本文针对多专家推迟学习问题，引入了新颖的替代损失函数和高效算法，具有强大的理论学习保证，解决了H-一致性和贝叶斯一致性等开放问题，并进行了实验验证。", "motivation": "多专家推迟学习问题在自然语言生成、图像处理和医疗诊断等领域至关重要，需要平衡专家准确性和计算成本。现有研究提出的替代损失函数在确保其一致性方面仍面临挑战。", "method": "本文引入了新颖的替代损失函数和高效算法，以解决可实现H-一致性、H-一致性界限和贝叶斯一致性等开放问题。研究涵盖了单阶段（同时学习预测器和推迟函数）和两阶段（仅学习推迟函数）学习场景。对于单阶段，提出了一系列新的可实现H-一致性替代损失，并证明了其中一个成员的H-一致性。对于两阶段，推导出了在双专家和多专家场景下实现可实现H-一致性、H-一致性界限和贝叶斯一致性的新替代损失。此外，在低噪声假设下提供了增强的理论保证，并通过实验将提出的替代损失与现有基线进行了比较。", "result": "本文为单阶段推迟引入了一系列新的可实现H-一致性替代损失，并证明了其中一个成员的H-一致性。为两阶段推迟导出了新的替代损失，在双专家和多专家场景下实现了可实现H-一致性、H-一致性界限和贝叶斯一致性。在低噪声假设下，为两种场景提供了增强的理论保证。实验结果报告了所提出的替代损失与现有基线的性能对比。", "conclusion": "本文提出了新颖的替代损失函数和算法，为多专家推迟学习问题提供了强大的理论学习保证，解决了单阶段和两阶段学习场景中的关键一致性挑战，并通过实验验证了其性能。", "translation": "多专家推迟学习问题包括将输入实例最优地分配给专家，平衡其准确性和计算成本之间的权衡。这在自然语言生成中是一个关键挑战，但在图像处理和医疗诊断等其他领域也同样重要。最近的研究提出了替代损失函数来优化推迟，但在确保其一致性方面仍然存在挑战。本文引入了新颖的替代损失函数和高效算法，具有强大的理论学习保证。我们解决了关于可实现H-一致性、H-一致性界限和贝叶斯一致性的开放问题，适用于单阶段（共同学习预测器和推迟函数）和两阶段（仅学习推迟函数，专家固定）学习场景。对于单阶段推迟，我们引入了一系列新的可实现H-一致性替代损失，并进一步证明了其中一个成员的H-一致性。对于两阶段推迟，我们推导出了新的替代损失，在双专家场景下实现了可实现H-一致性、H-一致性界限和贝叶斯一致性，并在自然假设下，在多专家场景下也实现了这些。此外，我们在低噪声假设下为两种场景提供了增强的理论保证。最后，我们报告了使用我们提出的替代损失进行的实验结果，并将其性能与现有基线进行了比较。", "summary": "本论文致力于解决多专家推迟学习中的关键挑战，即在平衡准确性和计算成本的前提下，将输入实例最优地分配给专家。针对现有替代损失函数在一致性方面的不足，本文提出了新颖的替代损失函数和高效算法，并提供了强大的理论学习保证。研究覆盖了单阶段和两阶段学习场景，对所提出的方法在可实现H-一致性、H-一致性界限和贝叶斯一致性方面进行了严格证明，并在低噪声条件下提供了增强的理论保证。此外，论文还报告了使用所提替代损失进行实验的结果，并与现有基线进行了比较。", "keywords": "推迟学习, 多专家, H-一致性, 替代损失, 理论保证", "comments": "该论文对“学习推迟”问题做出了重要的理论贡献，通过严谨地解决替代损失函数的一致性问题，克服了该领域的已知挑战。引入新颖的可实现H-一致性损失以及为单阶段和两阶段学习场景推导出强大的理论保证，体现了其创新性。鉴于其在自然语言生成、图像处理和医疗诊断等关键AI应用领域的广泛相关性，该研究不仅具有理论新颖性，也具有重要的实际意义。对一致性和强保证的关注为该领域的未来工作奠定了坚实的基础。"}}
{"id": "2506.20164", "title": "Do psychic cells generate consciousness?", "authors": ["Mototaka Suzuki", "Jaan Aru"], "summary": "Technological advances in the past decades have begun to enable\nneuroscientists to address fundamental questions about consciousness in an\nunprecedented way. Here we review remarkable recent progress in our\nunderstanding of cellular-level mechanisms of conscious processing in the\nbrain. Of particular interest are the cortical pyramidal neurons -- or \"psychic\ncells\" called by Ram\\'on y Cajal more than 100 years ago -- which have an\nintriguing cellular mechanism that accounts for selective disruption of\nfeedback signaling in the brain upon anesthetic-induced loss of consciousness.\nImportantly, a particular class of metabotropic receptors distributed over the\ndendrites of pyramidal cells are highlighted as the key cellular mechanism.\nAfter all, Cajal's instinct over a century ago may turn out to be correct -- we\nmay have just begun to understand whether and how psychic cells indeed generate\nand control our consciousness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20164v1", "categories": ["q-bio.NC", "cs.AI"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.20164v1", "AI": {"title_translation": "精神细胞产生意识吗？", "tldr": "综述了皮层锥体神经元（精神细胞）如何通过特定的细胞机制（代谢型受体）在麻醉引起的意识丧失中选择性中断反馈信号，这可能解释了它们如何产生和控制意识。", "motivation": "神经科学技术进步使得以空前的方式研究意识成为可能；探讨意识的细胞级机制。", "method": "本文综述了脑中意识处理的细胞级机制的最新进展。", "result": "发现皮层锥体神经元（“精神细胞”）具有一种有趣的细胞机制，可以解释麻醉诱导的意识丧失时大脑反馈信号的选择性中断。特别是，锥体细胞树突上分布的特定类别代谢型受体被认为是关键的细胞机制。", "conclusion": "卡哈尔一个多世纪前的直觉可能是正确的，我们可能才刚刚开始理解精神细胞是否以及如何确实产生和控制我们的意识。", "translation": "过去几十年的技术进步已经开始使神经科学家能够以前所未有的方式解决关于意识的基本问题。本文回顾了我们对大脑中大脑中意识处理的细胞级机制理解的显著最新进展。特别值得关注的是皮层锥体神经元——或一百多年前拉蒙·卡哈尔所称的“精神细胞”——它们拥有一种有趣的细胞机制，可以解释麻醉引起的意识丧失时大脑反馈信号的选择性中断。重要的是，锥体细胞树突上分布的特定类别代谢型受体被强调为关键的细胞机制。毕竟，卡哈尔一个多世纪前的直觉可能最终是正确的——我们可能才刚刚开始理解精神细胞是否以及如何确实产生和控制我们的意识。", "summary": "本文综述了意识的细胞级机制，重点关注皮层锥体神经元（“精神细胞”）。研究发现，这些细胞通过树突上分布的特定代谢型受体，在麻醉诱导的意识丧失时选择性中断大脑反馈信号。这表明卡哈尔关于“精神细胞”产生意识的百年直觉可能正在被现代神经科学所证实，尽管对其具体作用机制的理解仍处于早期阶段。", "keywords": "意识, 锥体神经元, 精神细胞, 代谢型受体, 细胞机制", "comments": "这篇综述突出了皮层锥体神经元在意识形成中的潜在核心作用，特别是通过代谢型受体的特定机制。它将历史性的“精神细胞”概念与现代神经科学的发现联系起来，为理解意识的生物学基础提供了新的视角。其创新之处在于聚焦于细胞级机制，并指出了未来研究的方向。"}}
{"id": "2506.20173", "title": "Valid Selection among Conformal Sets", "authors": ["Mahmoud Hegazy", "Liviu Aolaritei", "Michael I. Jordan", "Aymeric Dieuleveut"], "summary": "Conformal prediction offers a distribution-free framework for constructing\nprediction sets with coverage guarantees. In practice, multiple valid conformal\nprediction sets may be available, arising from different models or\nmethodologies. However, selecting the most desirable set, such as the smallest,\ncan invalidate the coverage guarantees. To address this challenge, we propose a\nstability-based approach that ensures coverage for the selected prediction set.\nWe extend our results to the online conformal setting, propose several\nrefinements in settings where additional structure is available, and\ndemonstrate its effectiveness through experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20173v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME", "stat.OT"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20173v1", "AI": {"title_translation": "一致性集合中的有效选择", "tldr": "提出了一种基于稳定性的方法，用于在不破坏覆盖保证的情况下选择一致性预测集。", "motivation": "在实践中，可能存在多个有效的一致性预测集，但直接选择最理想的集合（例如最小的）会破坏其覆盖保证。", "method": "提出了一种基于稳定性的方法，以确保所选预测集的覆盖率。该方法还被扩展到在线一致性设置，并在存在额外结构的情况下提出了改进。", "result": "通过实验证明了所提出方法的有效性。", "conclusion": "该研究提出了一种有效选择一致性预测集的方法，解决了在多个有效集合中进行选择时可能破坏覆盖保证的问题，并确保了所选集合的覆盖率。", "translation": "一致性预测提供了一个无分布框架，用于构建具有覆盖保证的预测集。在实践中，可能存在多个有效的一致性预测集，这些预测集可能来自不同的模型或方法。然而，选择最理想的集合，例如最小的集合，可能会使覆盖保证失效。为了解决这一挑战，我们提出了一种基于稳定性的方法，该方法确保了所选预测集的覆盖率。我们将结果扩展到在线一致性设置，在可获得额外结构的环境中提出了几项改进，并通过实验证明了其有效性。", "summary": "该论文提出了一种基于稳定性的方法，用于解决在多个有效一致性预测集中进行选择时可能破坏覆盖保证的问题。该方法确保了所选预测集的覆盖率，并已扩展到在线一致性设置，且通过实验验证了其有效性。", "keywords": "一致性预测, 预测集选择, 覆盖保证, 稳定性, 在线学习", "comments": "这篇论文解决了一致性预测在实际应用中的一个重要问题，即如何在不牺牲覆盖保证的前提下从多个有效预测集中进行选择。其提出的基于稳定性的方法具有创新性，并扩展到在线设置，增加了其实用价值。"}}
{"id": "2506.19855", "title": "Neural networks for the prediction of peel force for skin adhesive interface using FEM simulation", "authors": ["Ashish Masarkar", "Rakesh Gupta", "Naga Neehar Dingari", "Beena Rai"], "summary": "Studying the peeling behaviour of adhesives on skin is vital for advancing\nbiomedical applications such as medical adhesives and transdermal patches.\nTraditional methods like experimental testing and finite element method (FEM),\nthough considered gold standards, are resource-intensive, computationally\nexpensive and time-consuming, particularly when analysing a wide material\nparameter space. In this study, we present a neural network-based approach to\npredict the minimum peel force (F_min) required for adhesive detachment from\nskin tissue, limiting the need for repeated FEM simulations and significantly\nreducing the computational cost. Leveraging a dataset generated from FEM\nsimulations of 90 degree peel test with varying adhesive and fracture mechanics\nparameters, our neural network model achieved high accuracy, validated through\nrigorous 5-fold cross-validation. The final architecture was able to predict a\nwide variety of skin-adhesive peeling behaviour, exhibiting a mean squared\nerror (MSE) of 3.66*10^-7 and a R^2 score of 0.94 on test set, demonstrating\nrobust performance. This work introduces a reliable, computationally efficient\nmethod for predicting adhesive behaviour, significantly reducing simulation\ntime while maintaining accuracy. This integration of machine learning with\nhigh-fidelity biomechanical simulations enables efficient design and\noptimization of skin-adhesive systems, providing a scalable framework for\nfuture research in computational dermato-mechanics and bio-adhesive material\ndesign.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19855v1", "categories": ["physics.med-ph", "cs.LG"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.19855v1", "AI": {"title_translation": "基于有限元模拟的皮肤粘合界面剥离力预测神经网络", "tldr": "本研究提出了一种基于神经网络的方法，用于预测皮肤粘合剂的剥离力，显著减少了传统有限元模拟的计算成本和时间，并实现了高精度预测。", "motivation": "传统方法（如实验测试和有限元方法）在研究皮肤粘合剂的剥离行为时，资源密集、计算成本高且耗时，尤其是在分析广泛的材料参数空间时。因此，需要一种更高效的方法。", "method": "本研究提出了一种基于神经网络的方法来预测皮肤组织粘合剂脱离所需的最小剥离力（F_min）。该方法利用了通过90度剥离测试的有限元模拟生成的数据集，该数据集包含了不同粘合剂和断裂力学参数。最终的神经网络模型通过5折交叉验证进行了验证。", "result": "神经网络模型在测试集上实现了高精度，均方误差（MSE）为3.66*10^-7，R^2得分为0.94，表现出稳健的性能。该模型能够预测各种皮肤-粘合剂的剥离行为。", "conclusion": "本工作引入了一种可靠、计算高效的粘合剂行为预测方法，显著减少了模拟时间并保持了准确性。这种机器学习与高保真生物力学模拟的结合，为皮肤-粘合剂系统的有效设计和优化提供了一个可扩展的框架，并有望应用于计算皮肤力学和生物粘合材料设计领域的未来研究。", "translation": "研究皮肤上粘合剂的剥离行为对于推进医疗粘合剂和透皮贴片等生物医学应用至关重要。传统的实验测试和有限元方法（FEM）虽然被认为是金标准，但资源密集、计算成本高且耗时，特别是在分析广泛的材料参数空间时。在本研究中，我们提出了一种基于神经网络的方法来预测粘合剂从皮肤组织上脱离所需的最小剥离力（F_min），从而限制了重复进行有限元模拟的需要，并显著降低了计算成本。利用从90度剥离测试的有限元模拟生成的数据集（包含不同粘合剂和断裂力学参数），我们的神经网络模型通过严格的5折交叉验证实现了高精度。最终的架构能够预测各种皮肤-粘合剂的剥离行为，在测试集上表现出3.66*10^-7的均方误差（MSE）和0.94的R^2得分，展示了稳健的性能。这项工作引入了一种可靠、计算高效的粘合剂行为预测方法，显著减少了模拟时间，同时保持了准确性。这种机器学习与高保真生物力学模拟的集成，使得皮肤-粘合剂系统能够高效设计和优化，为计算皮肤力学和生物粘合材料设计的未来研究提供了一个可扩展的框架。", "summary": "本研究提出了一种利用神经网络预测皮肤粘合界面剥离力的新方法，以克服传统有限元模拟计算成本高昂的问题。研究人员通过90度剥离测试的有限元模拟生成数据集，并训练了一个神经网络模型。该模型在测试集上表现出高精度（MSE 3.66*10^-7，R^2 0.94），能有效预测皮肤-粘合剂的剥离行为。这项工作提供了一种计算高效且准确的粘合剂行为预测工具，有助于生物医学领域皮肤-粘合剂系统的设计与优化。", "keywords": "神经网络, 剥离力, 皮肤粘合剂, 有限元模拟, 机器学习", "comments": "本文的创新点在于将神经网络应用于传统计算成本高昂的有限元模拟领域，实现了对皮肤粘合剂剥离力的快速准确预测。这显著提高了研究效率，为生物医学材料的设计和优化提供了一个强大的新工具。其重要性在于为计算皮肤力学和生物粘合材料设计开辟了新的途径，具有很高的应用潜力。"}}
{"id": "2506.19856", "title": "Supervised Similarity for Firm Linkages", "authors": ["Ryan Samson", "Adrian Banner", "Luca Candelori", "Sebastien Cottrell", "Tiziana Di Matteo", "Paul Duchnowski", "Vahagn Kirakosyan", "Jose Marques", "Kharen Musaelian", "Stefano Pasquali", "Ryan Stever", "Dario Villani"], "summary": "We introduce a novel proxy for firm linkages, Characteristic Vector Linkages\n(CVLs). We use this concept to estimate firm linkages, first through Euclidean\nsimilarity, and then by applying Quantum Cognition Machine Learning (QCML) to\nsimilarity learning. We demonstrate that both methods can be used to construct\nprofitable momentum spillover trading strategies, but QCML similarity\noutperforms the simpler Euclidean similarity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19856v1", "categories": ["q-fin.ST", "cs.LG", "quant-ph"], "cate": "q-fin.ST", "url": "http://arxiv.org/abs/2506.19856v1", "AI": {"title_translation": "企业关联的监督相似性", "tldr": "本文引入了一种新的企业关联代理——特征向量关联（CVLs），并利用欧几里得相似性和量子认知机器学习（QCML）来估计企业关联。研究表明，两种方法都能构建有利可图的动量溢出交易策略，其中QCML表现优于欧几里得相似性。", "motivation": "本文旨在引入一种新颖的企业关联代理——特征向量关联（CVLs），并探索其在估计企业关联中的应用，最终用于构建盈利的交易策略。", "method": "研究首先通过欧几里得相似性来估计企业关联，然后应用量子认知机器学习（QCML）进行相似性学习。这两种方法都被用于构建动量溢出交易策略。", "result": "研究表明，欧几里得相似性和量子认知机器学习（QCML）两种方法都可以用于构建有利可图的动量溢出交易策略。其中，QCML相似性的表现优于更简单的欧几里得相似性。", "conclusion": "量子认知机器学习（QCML）在估计企业关联并构建盈利的动量溢出交易策略方面，表现优于传统的欧几里得相似性方法。", "translation": "我们引入了一种新颖的企业关联代理——特征向量关联（CVLs）。我们利用这一概念来估计企业关联，首先通过欧几里得相似性，然后将量子认知机器学习（QCML）应用于相似性学习。我们证明了这两种方法都可以用于构建有利可图的动量溢出交易策略，但QCML相似性优于更简单的欧几里得相似性。", "summary": "本文提出了一种名为特征向量关联（CVLs）的企业关联新代理，并运用欧几里得相似性和量子认知机器学习（QCML）两种方法对其进行估计。研究结果表明，这两种方法均能有效地构建盈利的动量溢出交易策略，且QCML在性能上优于欧几里得相似性。", "keywords": "企业关联, 特征向量关联, 相似性学习, 量子认知机器学习, 动量溢出", "comments": "本文的创新之处在于引入了特征向量关联（CVLs）这一新型企业关联代理，并首次将量子认知机器学习（QCML）应用于金融领域的相似性学习。QCML在预测能力上超越传统方法的发现，为量化交易策略提供了新的视角和工具，具有重要的实践意义。"}}
{"id": "2506.19945", "title": "Data-Driven Dynamic Factor Modeling via Manifold Learning", "authors": ["Graeme Baker", "Agostino Capponi", "J. Antonio Sidaoui"], "summary": "We propose a data-driven dynamic factor framework where a response variable\ndepends on a high-dimensional set of covariates, without imposing any\nparametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,\na nonlinear manifold learning technique introduced by Singer and Coifman, our\nframework uncovers the joint dynamics of the covariates and responses in a\npurely data-driven way. We approximate the embedding dynamics using linear\ndiffusions, and exploit Kalman filtering to predict the evolution of the\ncovariates and response variables directly from the diffusion map embedding\nspace. We generalize Singer's convergence rate analysis of the graph Laplacian\nfrom the case of independent uniform samples on a compact manifold to the case\nof time series arising from Langevin diffusions in Euclidean space.\nFurthermore, we provide rigorous justification for our procedure by showing the\nrobustness of approximations of the diffusion map coordinates by linear\ndiffusions, and the convergence of ergodic averages under standard spectral\nassumptions on the underlying dynamics. We apply our method to the stress\ntesting of equity portfolios using a combination of financial and macroeconomic\nfactors from the Federal Reserve's supervisory scenarios. We demonstrate that\nour data-driven stress testing method outperforms standard scenario analysis\nand Principal Component Analysis benchmarks through historical backtests\nspanning three major financial crises, achieving reductions in mean absolute\nerror of up to 55% and 39% for scenario-based portfolio return prediction,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19945v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.19945v1", "AI": {"title_translation": "数据驱动的流形学习动态因子建模", "tldr": "本文提出了一种基于各向异性扩散图的非参数数据驱动动态因子建模框架，用于预测高维协变量和响应变量的联合动态，并在股票投资组合压力测试中表现出显著优越性。", "motivation": "现有方法在处理响应变量与高维协变量的联合动态时常施加参数模型，而本文旨在提出一种纯粹数据驱动且不强加任何参数模型的动态因子框架，以更灵活、准确地揭示这些复杂动态。", "method": "本文提出了一种数据驱动的动态因子框架，该框架利用各向异性扩散图（一种非线性流形学习技术）来揭示协变量和响应变量的联合动态。该方法通过线性扩散近似嵌入动态，并利用卡尔曼滤波直接从扩散图嵌入空间预测变量的演变。理论上，本文还将Singer的图拉普拉斯收敛率分析从紧流形上的独立均匀样本情况推广到欧几里得空间中朗之万扩散产生的时间序列情况，并提供了其方法的鲁棒性和遍历平均收敛性的严格证明。", "result": "该方法应用于使用美联储监管情景中的金融和宏观经济因素对股票投资组合进行压力测试。通过涵盖三次主要金融危机的历史回溯测试，证明了其数据驱动的压力测试方法优于标准情景分析和主成分分析基准，在基于情景的投资组合回报预测中，平均绝对误差分别降低了高达55%和39%。", "conclusion": "本文提出了一种创新的数据驱动动态因子建模框架，该框架通过流形学习技术有效处理高维数据，并在实际金融应用中展现出显著的预测性能提升，证明了其在复杂系统建模和预测方面的优越性。", "translation": "我们提出了一个数据驱动的动态因子框架，其中响应变量依赖于高维协变量集，并且不强加任何关于联合动态的参数模型。利用Singer和Coifman引入的非线性流形学习技术——各向异性扩散图，我们的框架以纯粹数据驱动的方式揭示了协变量和响应的联合动态。我们使用线性扩散来近似嵌入动态，并利用卡尔曼滤波直接从扩散图嵌入空间预测协变量和响应变量的演变。我们将Singer关于图拉普拉斯收敛率分析从紧流形上独立均匀样本的情况推广到欧几里得空间中朗之万扩散产生的时间序列的情况。此外，我们通过展示扩散图坐标通过线性扩散近似的鲁棒性以及在底层动态的标准谱假设下遍历平均的收敛性，为我们的程序提供了严格的证明。我们将我们的方法应用于使用美联储监管情景中的金融和宏观经济因素对股票投资组合进行压力测试。我们证明了我们的数据驱动的压力测试方法通过涵盖三次主要金融危机的历史回溯测试，优于标准情景分析和主成分分析基准，在基于情景的投资组合回报预测中，平均绝对误差分别降低了高达55%和39%。", "summary": "本文提出了一种新颖的数据驱动动态因子框架，该框架利用各向异性扩散图这一非线性流形学习技术，以纯粹数据驱动的方式揭示响应变量与高维协变量之间的联合动态，无需预设参数模型。该框架通过线性扩散近似嵌入动态并结合卡尔曼滤波进行预测。理论上，文章推广了图拉普拉斯的收敛率分析，并证明了其方法的鲁棒性和收敛性。在股票投资组合压力测试中的应用表明，该方法在预测准确性上显著优于传统方法，平均绝对误差降低高达55%。", "keywords": "动态因子建模, 流形学习, 各向异性扩散图, 卡尔曼滤波, 压力测试", "comments": "该论文的创新之处在于结合了非线性流形学习（各向异性扩散图）与动态因子建模，实现了纯粹数据驱动的、非参数的高维数据动态分析。这种方法避免了传统参数模型的限制，能够更好地捕捉复杂系统中的非线性关系。其在金融压力测试中的成功应用，尤其是在历史金融危机中的出色表现，凸显了该方法在实际复杂系统预测中的潜力和重要性。理论上的严谨性也为其应用提供了坚实的基础。"}}
{"id": "2506.20043", "title": "PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning", "authors": ["Ahmet Sarigun", "Bora Uyar", "Vedran Franke", "Altuna Akalin"], "summary": "Sampling physically valid ligand-binding poses remains a major challenge in\nmolecular docking, particularly for unseen or structurally diverse targets. We\nintroduce PocketVina, a fast and memory-efficient, search-based docking\nframework that combines pocket prediction with systematic multi-pocket\nexploration. We evaluate PocketVina across four established\nbenchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and\nPoseBusters--and observe consistently strong performance in sampling physically\nvalid docking poses. PocketVina achieves state-of-the-art performance when\njointly considering ligand RMSD and physical validity (PB-valid), while\nremaining competitive with deep learning-based approaches in terms of RMSD\nalone, particularly on structurally diverse and previously unseen targets.\nPocketVina also maintains state-of-the-art physically valid docking accuracy\nacross ligands with varying degrees of flexibility. We further introduce\nTargetDock-AI, a benchmarking dataset we curated, consisting of over 500000\nprotein-ligand pairs, and a partition of the dataset labeled with PubChem\nactivity annotations. On this large-scale dataset, PocketVina successfully\ndiscriminates active from inactive targets, outperforming a deep learning\nbaseline while requiring significantly less GPU memory and runtime. PocketVina\noffers a robust and scalable docking strategy that requires no task-specific\ntraining and runs efficiently on standard GPUs, making it well-suited for\nhigh-throughput virtual screening and structure-based drug discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20043v1", "categories": ["q-bio.QM", "cs.LG", "q-bio.BM"], "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.20043v1", "AI": {"title_translation": "PocketVina 通过多口袋条件化实现可扩展且高度准确的物理有效对接", "tldr": "PocketVina 是一种快速、内存高效的分子对接框架，通过结合口袋预测和多口袋探索，在采样物理有效配体结合姿态方面表现出最先进的性能，且计算效率高，适用于高通量药物发现。", "motivation": "分子对接中采样物理有效配体结合姿态是一个重大挑战，尤其对于未知或结构多样的靶点。", "method": "引入了一种名为 PocketVina 的快速、内存高效的基于搜索的对接框架，该框架结合了口袋预测和系统性多口袋探索。", "result": "在PDBbind2020、DockGen、Astex和PoseBusters等四个基准测试中，在采样物理有效对接姿态方面表现出持续的强大性能。在同时考虑配体RMSD和物理有效性（PB-valid）时，实现了最先进的性能。在仅考虑RMSD时，与基于深度学习的方法相比仍具有竞争力。在不同柔性程度的配体上，保持了最先进的物理有效对接准确性。在大型数据集TargetDock-AI上，成功区分了活性和非活性靶点，优于深度学习基线，同时显著减少了GPU内存和运行时间。", "conclusion": "PocketVina 提供了一种无需特定任务训练、能在标准GPU上高效运行的鲁棒且可扩展的对接策略，非常适用于高通量虚拟筛选和基于结构的药物发现。", "translation": "采样物理有效的配体结合姿态仍然是分子对接中的一个主要挑战，特别是对于未见过或结构多样的靶点。我们引入了 PocketVina，一个快速且内存高效的、基于搜索的对接框架，它结合了口袋预测和系统性的多口袋探索。我们在四个已建立的基准测试——PDBbind2020（时间分割和未见过）、DockGen、Astex 和 PoseBusters——上评估了 PocketVina，并观察到在采样物理有效对接姿态方面持续的强大性能。PocketVina 在同时考虑配体 RMSD 和物理有效性（PB-valid）时实现了最先进的性能，同时在仅考虑 RMSD 时与基于深度学习的方法保持竞争力，特别是在结构多样和以前未见的靶点上。PocketVina 还保持了对不同柔性程度配体的最先进的物理有效对接准确性。我们进一步引入了 TargetDock-AI，这是一个我们整理的基准数据集，包含超过 500000 个蛋白质-配体对，以及一个带有 PubChem 活性注释的数据集分区。在这个大规模数据集上，PocketVina 成功区分了活性和非活性靶点，优于深度学习基线，同时显著减少了 GPU 内存和运行时间。PocketVina 提供了一种鲁棒且可扩展的对接策略，无需特定任务训练，并且可以在标准 GPU 上高效运行，使其非常适合高通量虚拟筛选和基于结构的药物发现。", "summary": "PocketVina 是一种新颖的分子对接框架，通过结合口袋预测和多口袋探索，解决了采样物理有效配体结合姿态的挑战。它在多个基准测试中表现出卓越的性能，尤其在物理有效性和对未知靶点的适应性方面达到最先进水平。此外，PocketVina 在区分活性与非活性靶点方面优于深度学习方法，且具有更高的计算效率和可扩展性，使其成为药物发现的有力工具。", "keywords": "分子对接, PocketVina, 物理有效性, 虚拟筛选, 药物发现", "comments": "PocketVina 的创新之处在于其结合口袋预测与系统性多口袋探索的搜索策略，以及在保持高准确性的同时显著提高计算效率和可扩展性，无需任务特定训练。这对于高通量虚拟筛选和药物发现领域具有重要意义，因为它提供了一个更实用、更易于部署的解决方案，尤其是在处理大规模和多样化数据集时。"}}
{"id": "2506.20048", "title": "A Principled Path to Fitted Distributional Evaluation", "authors": ["Sungee Hong", "Jiayi Wang", "Zhengling Qi", "Raymond Ka Wai Wong"], "summary": "In reinforcement learning, distributional off-policy evaluation (OPE) focuses\non estimating the return distribution of a target policy using offline data\ncollected under a different policy. This work focuses on extending the widely\nused fitted-Q evaluation -- developed for expectation-based reinforcement\nlearning -- to the distributional OPE setting. We refer to this extension as\nfitted distributional evaluation (FDE). While only a few related approaches\nexist, there remains no unified framework for designing FDE methods. To fill\nthis gap, we present a set of guiding principles for constructing theoretically\ngrounded FDE methods. Building on these principles, we develop several new FDE\nmethods with convergence analysis and provide theoretical justification for\nexisting methods, even in non-tabular environments. Extensive experiments,\nincluding simulations on linear quadratic regulators and Atari games,\ndemonstrate the superior performance of the FDE methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20048v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20048v1", "AI": {"title_translation": "拟合分布评估的原则性路径", "tldr": "本文提出了一套用于构建拟合分布评估（FDE）方法的指导原则，并在此基础上开发了新的FDE方法，填补了分布式离策略评估（OPE）领域的空白，并在实验中展示了其优越性能。", "motivation": "现有的拟合Q评估主要针对基于期望的强化学习，而缺乏一个统一的框架来设计用于分布式离策略评估（OPE）的拟合分布评估（FDE）方法。本文旨在填补这一空白。", "method": "本文将广泛使用的拟合Q评估扩展到分布式离策略评估（OPE）设置，称之为拟合分布评估（FDE）。提出了构建具有理论基础的FDE方法的一套指导原则，并在此基础上开发了几种新的FDE方法，进行了收敛性分析，并为现有方法提供了理论依据，即使在非表格环境中也是如此。", "result": "通过对线性二次调节器和Atari游戏进行大量实验，证明了所提出的FDE方法具有优越的性能。", "conclusion": "本文为设计和开发拟合分布评估（FDE）方法提供了一个原则性的框架，并提出了性能卓越的新方法，填补了分布式离策略评估领域的空白。", "translation": "在强化学习中，分布式离策略评估（OPE）侧重于使用在不同策略下收集的离线数据来估计目标策略的回报分布。这项工作重点是将广泛使用的拟合Q评估（为基于期望的强化学习开发）扩展到分布式OPE设置。我们将这种扩展称为拟合分布评估（FDE）。虽然只有少数相关方法存在，但仍然没有一个统一的框架来设计FDE方法。为了填补这一空白，我们提出了一套构建具有理论基础的FDE方法的指导原则。在此基础上，我们开发了几种新的FDE方法，并进行了收敛性分析，并为现有方法提供了理论依据，即使在非表格环境中也是如此。广泛的实验，包括在线性二次调节器和Atari游戏上的模拟，证明了FDE方法的优越性能。", "summary": "本文针对强化学习中分布式离策略评估（OPE）领域缺乏统一的拟合分布评估（FDE）方法设计框架的问题，提出了一套构建理论上可靠FDE方法的指导原则。在此基础上，开发了几种新的FDE方法，并提供了收敛性分析以及对现有方法的理论证明。通过在多种环境下的广泛实验，验证了所提出FDE方法的卓越性能。", "keywords": "强化学习, 分布式离策略评估, 拟合分布评估, 理论基础, 收敛性分析", "comments": "本文通过提出一套原则性的指导方针，并开发新的拟合分布评估（FDE）方法，在分布式离策略评估（OPE）领域取得了重要进展。其创新点在于为设计FDE方法提供了一个统一的理论框架，并证明了其在复杂环境下的有效性，这对于推动离线强化学习的发展具有重要意义。"}}
{"id": "2506.20056", "title": "Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization", "authors": ["Yuheng Chen", "Alexander Montes McNeil", "Taehyuk Park", "Blake A. Wilson", "Vaishnavi Iyer", "Michael Bezick", "Jae-Ik Choi", "Rohan Ojha", "Pravin Mahendran", "Daksh Kumar Singh", "Geetika Chitturi", "Peigang Chen", "Trang Do", "Alexander V. Kildishev", "Vladimir M. Shalaev", "Michael Moebius", "Wenshan Cai", "Yongmin Liu", "Alexandra Boltasseva"], "summary": "Photonic device development (PDD) has achieved remarkable success in\ndesigning and implementing new devices for controlling light across various\nwavelengths, scales, and applications, including telecommunications, imaging,\nsensing, and quantum information processing. PDD is an iterative, five-step\nprocess that consists of: i) deriving device behavior from design parameters,\nii) simulating device performance, iii) finding the optimal candidate designs\nfrom simulations, iv) fabricating the optimal device, and v) measuring device\nperformance. Classically, all these steps involve Bayesian optimization,\nmaterial science, control theory, and direct physics-driven numerical methods.\nHowever, many of these techniques are computationally intractable, monetarily\ncostly, or difficult to implement at scale. In addition, PDD suffers from large\noptimization landscapes, uncertainties in structural or optical\ncharacterization, and difficulties in implementing robust fabrication\nprocesses. However, the advent of machine learning over the past decade has\nprovided novel, data-driven strategies for tackling these challenges, including\nsurrogate estimators for speeding up computations, generative modeling for\nnoisy measurement modeling and data augmentation, reinforcement learning for\nfabrication, and active learning for experimental physical discovery. In this\nreview, we present a comprehensive perspective on these methods to enable\nmachine-learning-assisted PDD (ML-PDD) for efficient design optimization with\npowerful generative models, fast simulation and characterization modeling under\nnoisy measurements, and reinforcement learning for fabrication. This review\nwill provide researchers from diverse backgrounds with valuable insights into\nthis emerging topic, fostering interdisciplinary efforts to accelerate the\ndevelopment of complex photonic devices and systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20056v1", "categories": ["physics.optics", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.20056v1", "AI": {"title_translation": "机器学习辅助的光子器件开发：从理论到表征的多尺度方法", "tldr": "这篇综述探讨了机器学习如何通过加速设计、模拟和制造来解决光子器件开发中的挑战。", "motivation": "经典的光子器件开发（PDD）过程存在计算困难、成本高昂、难以大规模实施、优化空间大、表征不确定性以及制造过程难以鲁棒等问题。", "method": "本综述全面介绍了机器学习方法，包括用于加速计算的代理估计器、用于噪声测量建模和数据增强的生成模型、用于制造的强化学习以及用于实验物理发现的主动学习，以实现机器学习辅助的PDD（ML-PDD）。", "result": "该综述旨在为不同背景的研究人员提供关于ML-PDD的宝贵见解，促进跨学科合作，以加速复杂光子器件和系统的开发。", "conclusion": "通过整合机器学习，本综述将加速复杂光子器件和系统的开发。", "translation": "光子器件开发（PDD）在设计和实现用于控制各种波长、尺度和应用（包括电信、成像、传感和量子信息处理）的新器件方面取得了显著成功。PDD是一个迭代的五步过程，包括：i）从设计参数推导器件行为，ii）模拟器件性能，iii）从模拟中找到最佳候选设计，iv）制造最佳器件，以及v）测量器件性能。传统上，所有这些步骤都涉及贝叶斯优化、材料科学、控制理论和直接物理驱动的数值方法。然而，其中许多技术在计算上是难以处理的、成本高昂的或难以大规模实施的。此外，PDD还面临着巨大的优化空间、结构或光学表征中的不确定性以及实施鲁棒制造过程的困难。然而，过去十年机器学习的出现为解决这些挑战提供了新颖的、数据驱动的策略，包括用于加速计算的代理估计器、用于噪声测量建模和数据增强的生成建模、用于制造的强化学习以及用于实验物理发现的主动学习。在这篇综述中，我们对这些方法进行了全面介绍，以实现机器学习辅助的PDD（ML-PDD），从而实现强大的生成模型进行高效设计优化，在噪声测量下进行快速模拟和表征建模，以及用于制造的强化学习。这篇综述将为不同背景的研究人员提供关于这一新兴主题的宝贵见解，促进跨学科合作，以加速复杂光子器件和系统的开发。", "summary": "这篇综述探讨了机器学习在光子器件开发（PDD）中的应用，旨在解决传统PDD方法中存在的计算复杂、成本高、难以大规模实施以及优化挑战等问题。文章详细介绍了如何利用机器学习技术，如代理估计器、生成模型、强化学习和主动学习，来优化设计、加速模拟与表征，并改进制造过程。该综述旨在为研究人员提供指导，促进跨学科合作，以加速复杂光子器件的开发。", "keywords": "机器学习, 光子器件开发, 多尺度, 综述, 优化", "comments": "这篇综述的创新之处在于系统性地将机器学习方法应用于光子器件开发的各个阶段，解决了传统方法的瓶颈。其重要性在于为加速光子技术的发展提供了一个数据驱动的新范式，有望显著提高开发效率并降低成本。它为跨学科研究提供了宝贵的框架。"}}
{"id": "2506.20114", "title": "Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives", "authors": ["Brian Liu", "Rahul Mazumder", "Peter Radchenko"], "summary": "Tree ensembles are non-parametric methods widely recognized for their\naccuracy and ability to capture complex interactions. While these models excel\nat prediction, they are difficult to interpret and may fail to uncover useful\nrelationships in the data. We propose an estimator to extract compact sets of\ndecision rules from tree ensembles. The extracted models are accurate and can\nbe manually examined to reveal relationships between the predictors and the\nresponse. A key novelty of our estimator is the flexibility to jointly control\nthe number of rules extracted and the interaction depth of each rule, which\nimproves accuracy. We develop a tailored exact algorithm to efficiently solve\noptimization problems underlying our estimator and an approximate algorithm for\ncomputing regularization paths, sequences of solutions that correspond to\nvarying model sizes. We also establish novel non-asymptotic prediction error\nbounds for our proposed approach, comparing it to an oracle that chooses the\nbest data-dependent linear combination of the rules in the ensemble subject to\nthe same complexity constraint as our estimator. The bounds illustrate that the\nlarge-sample predictive performance of our estimator is on par with that of the\noracle. Through experiments, we demonstrate that our estimator outperforms\nexisting algorithms for rule extraction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20114v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20114v1", "AI": {"title_translation": "从树集成中提取可解释模型：计算和统计视角", "tldr": "该论文提出了一种从复杂树集成中提取可解释决策规则的估计器，能够灵活控制规则的复杂性并实现高精度。", "motivation": "树集成模型虽然预测准确且能捕获复杂交互，但难以解释，可能无法揭示数据中有用的关系。", "method": "我们提出了一种估计器，用于从树集成中提取紧凑的决策规则集。该估计器的新颖之处在于能够灵活地共同控制提取的规则数量和每个规则的交互深度。我们开发了定制的精确算法来解决底层的优化问题，并开发了近似算法来计算正则化路径。此外，我们还为所提出的方法建立了新的非渐近预测误差界限。", "result": "提取的模型准确且可以手动检查，以揭示预测变量和响应之间的关系。我们估计器的大样本预测性能与选择最佳数据依赖线性组合规则的预言机相当。实验表明，我们的估计器优于现有的规则提取算法。", "conclusion": "本研究提出的估计器能够有效地从树集成中提取可解释且准确的规则集，具有计算效率和强大的理论保证，并优于现有方法。", "translation": "树集成是非参数方法，因其准确性和捕获复杂交互的能力而广受认可。尽管这些模型在预测方面表现出色，但它们难以解释，并且可能无法揭示数据中有用的关系。我们提出了一种估计器，用于从树集成中提取紧凑的决策规则集。提取的模型准确，并且可以手动检查以揭示预测变量和响应之间的关系。我们估计器的一个关键新颖之处在于，它能够灵活地共同控制提取的规则数量和每个规则的交互深度，从而提高准确性。我们开发了一种量身定制的精确算法来有效解决我们估计器底层的优化问题，以及一种用于计算正则化路径（对应于不同模型大小的解序列）的近似算法。我们还为我们提出的方法建立了新的非渐近预测误差界限，并将其与一个在相同复杂性约束下选择集成中规则最佳数据依赖线性组合的预言机进行比较。这些界限表明，我们估计器的大样本预测性能与预言机相当。通过实验，我们证明我们的估计器优于现有的规则提取算法。", "summary": "本文介绍了一种新颖的估计器，用于从复杂的树集成中提取可解释且准确的决策规则集。它通过灵活控制提取规则的数量和交互深度来解决树集成的可解释性挑战。作者开发了用于优化的有效算法，并建立了强大的非渐近预测误差界限，证明了他们方法的性能与预言机相当，并且优于现有的规则提取算法。", "keywords": "树集成, 可解释模型, 规则提取, 决策规则, 预测误差界限", "comments": "该论文通过解决强大的树集成模型中关键的可解释性差距做出了重大贡献。能够共同控制规则数量和交互深度是一个显著的创新点，提供了实用的灵活性。定制算法的开发和理论误差界限的建立强调了所提方法的严谨性和鲁棒性，使其既实用又具有理论基础。其优于现有方法的表现进一步突显了其重要性。"}}
{"id": "2506.20139", "title": "Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis", "authors": ["Jiayong Qin", "Xianyu Zhu", "Qiyu Liu", "Guangyi Zhang", "Zhigang Cai", "Jianwei Liao", "Sha Hu", "Jingshu Peng", "Yingxia Shao", "Lei Chen"], "summary": "A growing trend in the database and system communities is to augment\nconventional index structures, such as B+-trees, with machine learning (ML)\nmodels. Among these, error-bounded Piecewise Linear Approximation\n($\\epsilon$-PLA) has emerged as a popular choice due to its simplicity and\neffectiveness. Despite its central role in many learned indexes, the design and\nanalysis of $\\epsilon$-PLA fitting algorithms remain underexplored. In this\npaper, we revisit $\\epsilon$-PLA from both theoretical and empirical\nperspectives, with a focus on its application in learned index structures. We\nfirst establish a fundamentally improved lower bound of $\\Omega(\\kappa \\cdot\n\\epsilon^2)$ on the expected segment coverage for existing $\\epsilon$-PLA\nfitting algorithms, where $\\kappa$ is a data-dependent constant. We then\npresent a comprehensive benchmark of state-of-the-art $\\epsilon$-PLA algorithms\nwhen used in different learned data structures. Our results highlight key\ntrade-offs among model accuracy, model size, and query performance, providing\nactionable guidelines for the principled design of future learned data\nstructures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20139v1", "categories": ["cs.DB", "cs.LG"], "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.20139v1", "AI": {"title_translation": "学习索引结构中的分段线性逼近：理论与实证分析", "tldr": "本文从理论和实证两方面重新审视了分段线性逼近（$\\\\epsilon$-PLA）在学习索引结构中的应用，建立了一个改进的期望段覆盖下界，并对现有算法进行了基准测试，揭示了模型精度、大小和查询性能之间的权衡，为未来学习数据结构的设计提供了指导。", "motivation": "尽管误差有界分段线性逼近（$\\\\epsilon$-PLA）在许多学习索引中扮演着核心角色，但其拟合算法的设计和分析仍未得到充分探索。", "method": "本文从理论和实证角度重新审视了$\\\\epsilon$-PLA，首先为现有$\\\\epsilon$-PLA拟合算法建立了一个改进的期望段覆盖下界，然后对不同学习数据结构中使用的最先进$\\\\epsilon$-PLA算法进行了全面的基准测试。", "result": "研究建立了$\\\\Omega(\\\\kappa \\\\cdot \\\\epsilon^2)$的期望段覆盖下界，并揭示了模型精度、模型大小和查询性能之间的关键权衡。", "conclusion": "研究结果为未来学习数据结构的原则性设计提供了可操作的指导方针。", "translation": "数据库和系统社区的一个日益增长的趋势是使用机器学习（ML）模型来增强传统的索引结构，例如B+-树。其中，误差有界分段线性逼近（$\\\\epsilon$-PLA）因其简单性和有效性而成为一种流行的选择。尽管它在许多学习索引中扮演着核心角色，但$\\\\epsilon$-PLA拟合算法的设计和分析仍未得到充分探索。在本文中，我们从理论和实证角度重新审视了$\\\\epsilon$-PLA，重点关注其在学习索引结构中的应用。我们首先为现有$\\\\epsilon$-PLA拟合算法建立了$\\\\Omega(\\\\kappa \\\\cdot \\\\epsilon^2)$的期望段覆盖的根本改进下界，其中$\\\\kappa$是一个数据依赖的常数。然后，我们对在不同学习数据结构中使用的最先进$\\\\epsilon$-PLA算法进行了全面的基准测试。我们的结果突出了模型精度、模型大小和查询性能之间的关键权衡，为未来学习数据结构的原则性设计提供了可操作的指导方针。", "summary": "本文深入探讨了机器学习驱动的索引结构中关键组件——误差有界分段线性逼近（$\\\\epsilon$-PLA）。研究从理论上推导出了现有$\\\\epsilon$-PLA算法的改进下界，并在实际应用中对不同学习数据结构中的$\\\\epsilon$-PLA算法进行了全面的性能评估。研究结果揭示了模型准确性、模型尺寸和查询性能之间的重要权衡，为设计高效且实用的学习索引结构提供了宝贵的指导。", "keywords": "分段线性逼近, 学习索引结构, 理论分析, 实证分析, 数据库系统", "comments": "该论文通过对$\\\\epsilon$-PLA算法进行理论分析和实证评估，填补了学习索引结构领域的一个重要空白。其建立的下界和揭示的性能权衡对于指导未来学习数据结构的设计具有重要意义，有助于推动机器学习在数据库系统中的实际应用。"}}
{"id": "2506.20555", "title": "DeepQuark: deep-neural-network approach to multiquark bound states", "authors": ["Wei-Lin Wu", "Lu Meng", "Shi-Lin Zhu"], "summary": "For the first time, we implement the deep-neural-network-based variational\nMonte Carlo approach for the multiquark bound states, whose complexity\nsurpasses that of electron or nucleon systems due to strong SU(3) color\ninteractions. We design a novel and high-efficiency architecture, DeepQuark, to\naddress the unique challenges in multiquark systems such as stronger\ncorrelations, extra discrete quantum numbers, and intractable confinement\ninteraction. Our method demonstrates competitive performance with\nstate-of-the-art approaches, including diffusion Monte Carlo and Gaussian\nexpansion method, in the nucleon, doubly heavy tetraquark, and fully heavy\ntetraquark systems. Notably, it outperforms existing calculations for\npentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we\nsuccessfully incorporate three-body flux-tube confinement interactions without\nadditional computational costs. In tetraquark systems, we consistently describe\nhadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased\nform of wave function ansatz. In the pentaquark sector, we obtain weakly bound\n$\\bar D^*\\Xi_{cc}^*$ molecule $P_{cc\\bar c}(5715)$ with $S=\\frac{5}{2}$ and its\nbottom partner $P_{bb\\bar b}(15569)$. They can be viewed as the analogs of the\nmolecular $T_{cc}$. We recommend experimental search of $P_{cc\\bar c}(5715)$ in\nthe D-wave $J/\\psi \\Lambda_c$ channel. DeepQuark holds great promise for\nextension to larger multiquark systems, overcoming the computational barriers\nin conventional methods. It also serves as a powerful framework for exploring\nconfining mechanism beyond two-body interactions in multiquark states, which\nmay offer valuable insights into nonperturbative QCD and general many-body\nphysics.", "comment": "10 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.20555v1", "categories": ["hep-ph", "cs.AI", "hep-ex", "hep-lat", "nucl-th"], "cate": "hep-ph", "url": "http://arxiv.org/abs/2506.20555v1", "AI": {"title_translation": "DeepQuark: 基于深度神经网络的多夸克束缚态方法", "tldr": "本文首次提出DeepQuark，一种基于深度神经网络的变分蒙特卡罗方法，用于研究复杂的多夸克束缚态。该方法在核子、四夸克和五夸克系统中表现出与现有先进方法相当甚至更优的性能，并有望扩展到更大的多夸克系统，同时为探索禁闭机制提供新视角。", "motivation": "多夸克束缚态由于强SU(3)色相互作用、更强的关联、额外的离散量子数和难以处理的禁闭相互作用而非常复杂。传统方法在计算上存在障碍，难以有效处理这些挑战。", "method": "本文首次实现了基于深度神经网络的变分蒙特卡罗方法，并设计了一种新颖高效的架构DeepQuark。该方法成功地纳入了三体磁通管禁闭相互作用，且没有额外的计算成本，并使用无偏的波函数拟设形式。", "result": "DeepQuark方法在核子、双重重四夸克和全重四夸克系统中，与最先进的方法（包括扩散蒙特卡罗和高斯展开法）相比，表现出具有竞争力的性能。尤其在五夸克方面，它优于现有计算，例如三重重五夸克。对于核子，成功纳入了三体磁通管禁闭相互作用而无额外计算成本。在四夸克系统中，一致地描述了强子分子$T_{cc}$和紧凑型四夸克$T_{bb}$。在五夸克领域，获得了弱束缚的$\bar D^*\textrm{Ξ}_{cc}^*$分子$P_{cc\bar c}(5715)$（$S=\frac{5}{2}$）及其底夸克伙伴$P_{bb\bar b}(15569)$。", "conclusion": "DeepQuark有望扩展到更大的多夸克系统，克服传统方法的计算障碍。它也为探索多夸克态中超越两体相互作用的禁闭机制提供了一个强大的框架，可能为非微扰QCD和一般多体物理学提供有价值的见解。建议在D波$J/\textrm{ψ}\textrm{Λ}_c$通道中实验搜索$P_{cc\bar c}(5715)$。", "translation": "首次，我们为多夸克束缚态实现了基于深度神经网络的变分蒙特卡罗方法，由于强大的SU(3)色相互作用，其复杂性超过了电子或核子系统。我们设计了一种新颖高效的架构DeepQuark，以解决多夸克系统中独特的挑战，例如更强的关联、额外的离散量子数和难以处理的禁闭相互作用。我们的方法在核子、双重重四夸克和全重四夸克系统中，与扩散蒙特卡罗和高斯展开法等最先进的方法相比，表现出具有竞争力的性能。值得注意的是，它在五夸克方面优于现有计算，例如三重重五夸克。对于核子，我们成功地在不增加额外计算成本的情况下，纳入了三体磁通管禁闭相互作用。在四夸克系统中，我们用无偏的波函数拟设形式一致地描述了强子分子$T_{cc}$和紧凑型四夸克$T_{bb}$。在五夸克领域，我们获得了弱束缚的$\bar D^*\textrm{Ξ}_{cc}^*$分子$P_{cc\bar c}(5715)$，其自旋$S=\frac{5}{2}$，以及其底夸克伙伴$P_{bb\bar b}(15569)$。它们可以被视为分子$T_{cc}$的类似物。我们建议在D波$J/\textrm{ψ}\textrm{Λ}_c$通道中实验搜索$P_{cc\bar c}(5715)$。DeepQuark有望扩展到更大的多夸克系统，克服传统方法中的计算障碍。它也为探索多夸克态中超越两体相互作用的禁闭机制提供了一个强大的框架，这可能为非微扰QCD和一般多体物理学提供有价值的见解。", "summary": "本文介绍了DeepQuark，一种新颖的基于深度神经网络的变分蒙特卡罗方法，专为解决多夸克束缚态的复杂挑战而设计。DeepQuark在核子、四夸克和五夸克等各种系统中展现出与最先进方法相当的性能，尤其在五夸克计算方面超越了现有方法。它成功地纳入了三体禁闭相互作用，并一致地描述了强子分子和紧凑型四夸克。该方法预测了新的五夸克态（$P_{cc\bar c}(5715)$和$P_{bb\bar b}(15569)$）并建议进行实验验证。DeepQuark在扩展到更大系统和探索非微扰QCD中的基本禁闭机制方面具有巨大潜力。", "keywords": "DeepQuark, 多夸克束缚态, 深度神经网络, 变分蒙特卡罗, 非微扰QCD", "comments": "该论文通过利用深度神经网络，在多夸克系统研究方面取得了重要进展。其创新之处在于将变分蒙特卡罗方法与量身定制的神经网络架构（DeepQuark）相结合，以处理强色相互作用、离散量子数和难以处理的禁闭作用等独特复杂性。它能够在不增加额外成本的情况下纳入三体相互作用，并超越传统方法在五夸克等复杂系统上的表现，这凸显了其效率和准确性。这项工作为探索非微扰QCD开辟了新途径，并推动了计算多体物理学的边界。新五夸克态的预测也为实验验证提供了具体目标。"}}
{"id": "2506.20344", "title": "A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization", "authors": ["Po Chen", "Rujun Jiang", "Peng Wang"], "summary": "Despite its wide range of applications across various domains, the\noptimization foundations of deep matrix factorization (DMF) remain largely\nopen. In this work, we aim to fill this gap by conducting a comprehensive study\nof the loss landscape of the regularized DMF problem. Toward this goal, we\nfirst provide a closed-form expression of all critical points. Building on\nthis, we establish precise conditions under which a critical point is a local\nminimizer, a global minimizer, a strict saddle point, or a non-strict saddle\npoint. Leveraging these results, we derive a necessary and sufficient condition\nunder which each critical point is either a local minimizer or a strict saddle\npoint. This provides insights into why gradient-based methods almost always\nconverge to a local minimizer of the regularized DMF problem. Finally, we\nconduct numerical experiments to visualize its loss landscape under different\nsettings to support our theory.", "comment": "35 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.20344v1", "categories": ["math.OC", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20344v1", "AI": {"title_translation": "正则化深度矩阵分解的完整损失景观分析", "tldr": "本文对正则化深度矩阵分解的损失景观进行了全面分析，提供了所有临界点的闭式表达式，并建立了临界点类型（局部最小值、全局最小值、鞍点）的精确条件，解释了梯度下降方法为何通常收敛到局部最小值。", "motivation": "尽管深度矩阵分解（DMF）应用广泛，但其优化基础仍不明确。本文旨在通过全面研究正则化DMF问题的损失景观来填补这一空白。", "method": "首先，提供了所有临界点的闭式表达式。其次，建立了临界点是局部最小值、全局最小值、严格鞍点或非严格鞍点的精确条件。接着，推导了每个临界点是局部最小值或严格鞍点的充要条件。最后，通过数值实验可视化损失景观以支持理论。", "result": "建立了临界点是局部最小值、全局最小值、严格鞍点或非严格鞍点的精确条件；推导了每个临界点是局部最小值或严格鞍点的充要条件。", "conclusion": "研究结果揭示了梯度下降方法几乎总能收敛到正则化深度矩阵分解问题局部最小值的内在原因。", "translation": "尽管深度矩阵分解（DMF）在各个领域应用广泛，但其优化基础仍 largely open。在这项工作中，我们旨在通过对正则化DMF问题的损失景观进行全面研究来填补这一空白。为此，我们首先提供了所有临界点的闭式表达式。在此基础上，我们建立了临界点是局部最小值、全局最小值、严格鞍点或非严格鞍点的精确条件。利用这些结果，我们推导出了每个临界点是局部最小值或严格鞍点的充要条件。这为梯度下降方法几乎总能收敛到正则化DMF问题局部最小值的原因提供了见解。最后，我们进行了数值实验，在不同设置下可视化其损失景观，以支持我们的理论。", "summary": "本文对正则化深度矩阵分解（DMF）的损失景观进行了深入分析，旨在弥补其优化基础的不足。研究提供了所有临界点的闭式表达式，并详细阐述了临界点属于局部最小值、全局最小值、严格鞍点或非严格鞍点的精确条件。通过推导一个充要条件，解释了为何梯度下降方法在正则化DMF问题中通常收敛到局部最小值。数值实验进一步验证了理论发现。", "keywords": "深度矩阵分解, 损失景观, 临界点, 优化, 梯度下降", "comments": "本文通过对正则化深度矩阵分解（DMF）损失景观的全面理论分析，为理解其优化行为提供了重要见解。其创新之处在于提供了所有临界点的闭式表达式，并精确地划分了不同类型的临界点，从而解释了梯度下降方法为何能有效收敛。这对于深度学习模型的可解释性和优化理论具有重要意义。"}}
{"id": "2506.20425", "title": "Scalable Subset Selection in Linear Mixed Models", "authors": ["Ryan Thompson", "Matt P. Wand", "Joanna J. J. Wang"], "summary": "Linear mixed models (LMMs), which incorporate fixed and random effects, are\nkey tools for analyzing heterogeneous data, such as in personalized medicine or\nadaptive marketing. Nowadays, this type of data is increasingly wide, sometimes\ncontaining thousands of candidate predictors, necessitating sparsity for\nprediction and interpretation. However, existing sparse learning methods for\nLMMs do not scale well beyond tens or hundreds of predictors, leaving a large\ngap compared with sparse methods for linear models, which ignore random\neffects. This paper closes the gap with a new $\\ell_0$ regularized method for\nLMM subset selection that can run on datasets containing thousands of\npredictors in seconds to minutes. On the computational front, we develop a\ncoordinate descent algorithm as our main workhorse and provide a guarantee of\nits convergence. We also develop a local search algorithm to help traverse the\nnonconvex optimization surface. Both algorithms readily extend to subset\nselection in generalized LMMs via a penalized quasi-likelihood approximation.\nOn the statistical front, we provide a finite-sample bound on the\nKullback-Leibler divergence of the new method. We then demonstrate its\nexcellent performance in synthetic experiments and illustrate its utility on\ntwo datasets from biology and journalism.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20425v1", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20425v1", "AI": {"title_translation": "线性混合模型中的可伸缩子集选择", "tldr": "提出了一种新的L0正则化方法，用于线性混合模型中的可伸缩子集选择，解决了现有方法在大数据集上的可伸缩性问题。", "motivation": "现有线性混合模型（LMMs）的稀疏学习方法在处理包含数千个预测变量的宽数据集时扩展性不佳，无法有效进行预测和解释，与线性模型（忽略随机效应）的稀疏方法相比存在巨大差距。", "method": "本文提出了一种新的L0正则化方法，用于LMM子集选择。在计算方面，开发了坐标下降算法（保证收敛性）和局部搜索算法（用于遍历非凸优化曲面），这两种算法可扩展到广义LMMs。在统计方面，提供了新方法在Kullback-Leibler散度上的有限样本边界。", "result": "该方法能够在数秒到数分钟内处理包含数千个预测变量的数据集。在合成实验中表现出卓越性能，并在生物学和新闻学的两个真实数据集中展示了其效用。", "conclusion": "本文成功开发了一种可伸缩的L0正则化方法，用于线性混合模型中的子集选择，有效解决了现有方法在大规模数据集上的扩展性问题，并展现了优异的性能。", "translation": "线性混合模型（LMMs）结合了固定效应和随机效应，是分析异构数据（例如个性化医疗或自适应营销）的关键工具。如今，这类数据日益广泛，有时包含数千个候选预测变量，因此需要稀疏性来进行预测和解释。然而，现有LMM的稀疏学习方法在处理数十或数百个预测变量之外的规模时扩展性不佳，与忽略随机效应的线性模型的稀疏方法相比存在巨大差距。本文通过一种新的L0正则化LMM子集选择方法弥补了这一差距，该方法可以在数秒到数分钟内处理包含数千个预测变量的数据集。在计算方面，我们开发了坐标下降算法作为主要工作引擎，并提供了其收敛性保证。我们还开发了一种局部搜索算法来帮助遍历非凸优化曲面。这两种算法都可以通过惩罚拟似然近似轻松扩展到广义LMM中的子集选择。在统计方面，我们提供了新方法在Kullback-Leibler散度上的有限样本边界。然后，我们在合成实验中展示了其卓越的性能，并在生物学和新闻学的两个数据集中说明了其效用。", "summary": "本文提出了一种创新的L0正则化方法，用于线性混合模型（LMMs）中的子集选择，旨在解决现有稀疏学习方法在大规模异构数据（包含数千个预测变量）上的可伸缩性问题。该方法结合了坐标下降和局部搜索算法，确保了计算效率和收敛性，并能扩展到广义LMMs。实验证明，该方法在处理大规模数据集时速度快，且在预测性能上表现出色，弥补了LMM稀疏学习与线性模型稀疏学习之间的差距。", "keywords": "线性混合模型, 子集选择, L0正则化, 可伸缩性, 稀疏学习", "comments": "该论文通过引入L0正则化方法和高效的优化算法（坐标下降和局部搜索），显著提升了线性混合模型在处理大规模数据集时的可伸缩性，填补了现有稀疏学习方法在处理数千个预测变量时的空白。其创新之处在于将L0范数引入LMM的稀疏选择，并提供了理论收敛性保证和统计边界，这对于个性化医疗和自适应营销等领域的数据分析具有重要意义。"}}
{"id": "2506.20513", "title": "Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch", "authors": ["Lei Liu", "Chao Song", "Liangsheng He", "Silin Wang", "Xuan Feng", "Cai Liu"], "summary": "This study proposes a high-performance dual-parameter full waveform inversion\nframework (FWI) for ground-penetrating radar (GPR), accelerated through the\nhybrid compilation of CUDA kernel functions and PyTorch. The method leverages\nthe computational efficiency of GPU programming while preserving the\nflexibility and usability of Python-based deep learning frameworks. By\nintegrating customized CUDA kernels into PyTorch's automatic differentiation\nmechanism, the framework enables accurate and efficient inversion of both\ndielectric permittivity and electrical conductivity. Experimental evaluations\non synthetic data and real wavefield data demonstrate that the proposed method\nachieves dual-parameter FWI for GPR data while maintaining high accuracy.\nMoreover, the framework is flexible and extensible, supporting optional\nregularization strategies such as total variation and multi-scale inversion.\nThese features make the proposed approach a practical and scalable framework\nfor rapid GPR-based subsurface imaging in applications including civil\nengineering, environmental monitoring, and geophysical exploration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20513v1", "categories": ["physics.geo-ph", "cs.LG", "eess.SP"], "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.20513v1", "AI": {"title_translation": "基于CUDA核函数与PyTorch混合编译加速的探地雷达双参数全波形反演方法", "tldr": "本文提出了一种结合CUDA和PyTorch的探地雷达（GPR）双参数全波形反演（FWI）高性能框架，实现了高效准确的地下成像。", "motivation": "为了开发一种高性能的探地雷达（GPR）双参数全波形反演（FWI）框架，以提高GPR地下成像的效率和准确性。", "method": "该方法通过CUDA核函数和PyTorch的混合编译进行加速，将定制的CUDA核函数集成到PyTorch的自动微分机制中，以实现介电常数和电导率的双参数反演。", "result": "在合成数据和真实波场数据上的实验评估表明，所提出的方法实现了探地雷达数据的双参数全波形反演，并保持了高精度。此外，该框架灵活且可扩展，支持总变差和多尺度反演等可选正则化策略。", "conclusion": "所提出的方法是一个实用且可扩展的框架，可用于土木工程、环境监测和地球物理勘探等应用中的快速探地雷达地下成像。", "translation": "本研究提出了一种用于探地雷达（GPR）的高性能双参数全波形反演（FWI）框架，该框架通过CUDA核函数和PyTorch的混合编译进行加速。该方法利用了GPU编程的计算效率，同时保留了基于Python的深度学习框架的灵活性和可用性。通过将定制的CUDA核函数集成到PyTorch的自动微分机制中，该框架能够准确高效地反演介电常数和电导率。在合成数据和真实波场数据上的实验评估表明，所提出的方法实现了探地雷达数据的双参数全波形反演，同时保持了高精度。此外，该框架灵活且可扩展，支持总变差和多尺度反演等可选正则化策略。这些特性使得所提出的方法成为一种实用且可扩展的框架，可用于土木工程、环境监测和地球物理勘探等应用中的快速探地雷达地下成像。", "summary": "本文提出了一种创新的探地雷达（GPR）双参数全波形反演（FWI）框架，该框架通过结合CUDA核函数的高计算效率与PyTorch的灵活性和自动微分机制进行加速。实验证明，该方法能高精度地实现介电常数和电导率的双参数反演，并且具有良好的灵活性和可扩展性，为快速GPR地下成像提供了实用且可扩展的解决方案。", "keywords": "探地雷达, 全波形反演, CUDA, PyTorch, 双参数", "comments": "该论文的创新点在于将CUDA核函数与PyTorch框架进行混合编译，有效结合了GPU的计算性能和深度学习框架的灵活性。这种集成不仅提高了探地雷达全波形反演的效率和精度，还通过支持多种正则化策略增强了其适用性，使其成为地下成像领域的一个重要进展。"}}
{"id": "2506.20533", "title": "Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery", "authors": ["Gilad Lerman", "Kang Li", "Tyler Maunu", "Teng Zhang"], "summary": "Robust subspace estimation is fundamental to many machine learning and data\nanalysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and\nempirically effective approach to this problem, yet its theoretical properties\nremain poorly understood. This paper establishes that, under deterministic\nconditions, a variant of IRLS with dynamic smoothing regularization converges\nlinearly to the underlying subspace from any initialization. We extend these\nguarantees to affine subspace estimation, a setting that lacks prior recovery\ntheory. Additionally, we illustrate the practical benefits of IRLS through an\napplication to low-dimensional neural network training. Our results provide the\nfirst global convergence guarantees for IRLS in robust subspace recovery and,\nmore broadly, for nonconvex IRLS on a Riemannian manifold.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20533v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20533v1", "AI": {"title_translation": "迭代重加权最小二乘法在鲁棒子空间恢复中的全局收敛性", "tldr": "本文首次证明了迭代重加权最小二乘法（IRLS）在鲁棒子空间恢复中具有全局线性收敛性，并将其扩展到仿射子空间估计。", "motivation": "鲁棒子空间估计是许多机器学习和数据分析任务的基础，而迭代重加权最小二乘法（IRLS）虽然经验有效，但其理论性质，特别是收敛性，尚不清楚。", "method": "本文研究了一种带有动态平滑正则化的迭代重加权最小二乘法（IRLS）变体，并使用确定性条件来建立其收敛性。", "result": "在确定性条件下，带有动态平滑正则化的IRLS变体可以从任意初始化线性收敛到潜在子空间。这些保证扩展到了之前缺乏恢复理论的仿射子空间估计。此外，通过应用于低维神经网络训练，展示了IRLS的实际优势。", "conclusion": "本文首次为IRLS在鲁棒子空间恢复中，以及更广泛地，为黎曼流形上的非凸IRLS提供了全局收敛保证。", "translation": "鲁棒子空间估计是许多机器学习和数据分析任务的基础。迭代重加权最小二乘法（IRLS）是解决此问题的一种优雅且经验有效的方法，但其理论性质仍然知之甚少。本文建立了在确定性条件下，一种带有动态平滑正则化的IRLS变体可以从任意初始化线性收敛到潜在子空间。我们将这些保证扩展到仿射子空间估计，这是一个之前缺乏恢复理论的设置。此外，我们通过将其应用于低维神经网络训练，说明了IRLS的实际益处。我们的结果为IRLS在鲁棒子空间恢复中，以及更广泛地，为黎曼流形上的非凸IRLS提供了首次全局收敛保证。", "summary": "本文首次从理论上证明了迭代重加权最小二乘法（IRLS）在鲁棒子空间恢复中的全局线性收敛性。研究了一种带有动态平滑正则化的IRLS变体，并在确定性条件下证明其能从任意初始化收敛到真实子空间。该理论成果也扩展到了仿射子空间估计，并展示了其在神经网络训练中的应用价值。", "keywords": "鲁棒子空间恢复, 迭代重加权最小二乘法, 全局收敛, 仿射子空间估计, 神经网络训练", "comments": "本文的创新之处在于首次为IRLS在鲁棒子空间恢复中提供了严格的全局收敛理论保证，解决了长期以来该方法理论基础薄弱的问题。其重要性在于为IRLS的应用提供了更坚实的理论支撑，并可能促进其在更多领域的应用。将收敛性扩展到仿射子空间估计也是一个显著的贡献。"}}
{"id": "2506.20573", "title": "LARP: Learner-Agnostic Robust Data Prefiltering", "authors": ["Kristian Minchev", "Dimitar Iliev Dimitrov", "Nikola Konstantinov"], "summary": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20573v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20573v1", "AI": {"title_translation": "LARP：学习器无关的鲁棒数据预过滤", "tldr": "本文形式化了学习器无关的鲁棒数据预过滤（LARP）问题，旨在统一处理公共数据集中低质量数据对多种学习器性能的影响。研究表明，尽管在统一预过滤时可能存在性能损失，但LARP在大规模数据集上具有优势。", "motivation": "大规模公共数据集的广泛可用性是机器学习成功的关键因素，然而这些数据集常包含低质量或受污染的数据，许多学习过程对此敏感。因此，需要研究如何有效预过滤公共数据集以确保下游学习的准确性。", "method": "本文形式化了学习器无关的鲁棒数据预过滤（LARP）问题，旨在找到最小化预设学习器集合上最坏情况损失的预过滤方法。研究以Huber数据污染模型下使用Huber估计器进行标量均值估计为例实例化了该框架，并分析了几种自然的预过滤程序。通过在真实图像和表格数据上进行大量实验，探索了效用损失及其对问题参数的依赖性。最后，在博弈论框架内建模了效用下降与重复（学习器特定）预过滤成本之间的权衡。", "result": "理论结果表明，对异构学习器集合执行LARP会导致模型性能相对于为每个学习器单独预过滤数据有所损失。实验观察到统计上显著的效用降低。", "conclusion": "LARP在大规模数据集上具有优势，尤其是在权衡效用下降和重复（学习器特定）预过滤成本时，其成本效益凸显。", "translation": "大规模公共数据集的广泛可用性是统计推断和机器学习方法近期成功的关键因素。然而，这些数据集通常包含一些低质量或受污染的数据，许多学习过程对此敏感。因此，如何以及是否应该预过滤公共数据集以促进准确的下游学习成为一个问题。在技术层面，这需要构建原则性的数据预过滤方法，这些方法是学习器无关的鲁棒的，即能够可证明地保护一组预先指定的下游学习器免受损坏数据的影响。在这项工作中，我们形式化了学习器无关的鲁棒数据预过滤（LARP）问题，该问题旨在找到能够最小化预先指定学习器集合上最坏情况损失的预过滤程序。我们首先在Huber数据污染模型下，以Huber估计器进行标量均值估计的背景下实例化了我们的框架。我们提供了特定问题实例上的硬度结果，并分析了几种自然的预过滤程序。我们的理论结果表明，对异构学习器集合执行LARP会导致模型性能相对于为每个学习器/用例单独预过滤数据有所损失。我们通过对真实世界的图像和表格数据进行大量实验，探索了由此产生的效用损失及其对问题参数的依赖性，观察到统计上显著的效用降低。最后，我们在博弈论框架内建模了效用下降与重复（学习器特定）预过滤成本之间的权衡，并展示了LARP在大数据集上的优势。", "summary": "本文提出并形式化了学习器无关的鲁棒数据预过滤（LARP）问题，旨在为包含低质量或受污染数据的公共数据集提供一种统一且鲁棒的预处理方法，以保护预设的下游学习器。研究通过理论分析和实验验证，探讨了LARP在异构学习器集合上的性能表现，指出其可能导致一定的模型性能和效用损失。然而，通过博弈论框架，本文进一步揭示了LARP在大规模数据集场景下，在权衡效用与预过滤成本方面的优势，突出了其在实际应用中的价值。", "keywords": "数据预过滤, 鲁棒性, 学习器无关, 数据污染, 大数据集", "comments": "本文创新性地提出了LARP框架，解决了在存在污染数据时，如何为多种下游学习器提供统一且鲁棒的数据预过滤问题。其重要性在于提供了一种在多用途场景下处理数据质量问题的通用方法。尽管理论和实验指出其在性能上可能存在损失，但通过引入博弈论框架分析了其在成本效益上的优势，尤其在大数据集场景下具有实际价值。"}}
