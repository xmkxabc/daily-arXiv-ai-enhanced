{"id": "2506.20770", "title": "Perry: A High-level Framework for Accelerating Cyber Deception Experimentation", "authors": ["Brian Singer", "Yusuf Saquib", "Lujo Bauer", "Vyas Sekar"], "summary": "Cyber deception aims to distract, delay, and detect network attackers with\nfake assets such as honeypots, decoy credentials, or decoy files. However,\ntoday, it is difficult for operators to experiment, explore, and evaluate\ndeception approaches. Existing tools and platforms have non-portable and\ncomplex implementations that are difficult to modify and extend. We address\nthis pain point by introducing Perry, a high-level framework that accelerates\nthe design and exploration of deception what-if scenarios. Perry has two\ncomponents: a high-level abstraction layer for security operators to specify\nattackers and deception strategies, and an experimentation module to run these\nattackers and defenders in realistic emulated networks. To translate these\nhigh-level specifications we design four key modules for Perry: 1) an action\nplanner that translates high-level actions into low-level implementations, 2)\nan observability module to translate low-level telemetry into high-level\nobservations, 3) an environment state service that enables environment agnostic\nstrategies, and 4) an attack graph service to reason about how attackers could\nexplore an environment. We illustrate that Perry's abstractions reduce the\nimplementation effort of exploring a wide variety of deception defenses,\nattackers, and environments. We demonstrate the value of Perry by emulating 55\nunique deception what-if scenarios and illustrate how these experiments enable\noperators to shed light on subtle tradeoffs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20770v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20770v1", "AI": {"title_translation": "Perry：一个加速网络欺骗实验的高级框架", "tldr": "Perry是一个高级框架，旨在通过提供抽象层和实验模块来加速网络欺骗实验的设计和探索，解决了现有工具难以修改和扩展的问题。", "motivation": "网络欺骗旨在通过虚假资产分散、延迟和检测网络攻击者。然而，目前操作员难以实验、探索和评估欺骗方法，因为现有工具和平台实现复杂且不可移植，难以修改和扩展。", "method": "Perry框架通过引入两个主要组件来解决问题：一个用于安全操作员指定攻击者和欺骗策略的高级抽象层，以及一个在真实模拟网络中运行这些攻击者和防御者的实验模块。为了将高级规范转换为低级实现，Perry设计了四个关键模块：1) 一个将高级操作转换为低级实现的行动规划器，2) 一个将低级遥测数据转换为高级观察的可观察性模块，3) 一个实现环境无关策略的环境状态服务，以及4) 一个推断攻击者如何探索环境的攻击图服务。", "result": "Perry的抽象减少了探索各种欺骗防御、攻击者和环境的实现工作。通过模拟55个独特的欺骗假设情景，展示了Perry的价值，并说明了这些实验如何帮助操作员揭示细微的权衡。", "conclusion": "Perry框架通过其高级抽象和模块化设计，显著加速了网络欺骗实验的设计和探索，并帮助操作员理解欺骗策略中的复杂权衡。", "translation": "网络欺骗旨在通过虚假资产（如蜜罐、诱饵凭证或诱饵文件）分散、延迟和检测网络攻击者。然而，目前操作员难以实验、探索和评估欺骗方法。现有工具和平台实现复杂且不可移植，难以修改和扩展。我们通过引入Perry来解决这一痛点，Perry是一个高级框架，可加速欺骗“假设”场景的设计和探索。Perry有两个组件：一个用于安全操作员指定攻击者和欺骗策略的高级抽象层，以及一个在真实模拟网络中运行这些攻击者和防御者的实验模块。为了转换这些高级规范，我们为Perry设计了四个关键模块：1) 一个将高级操作转换为低级实现的行动规划器，2) 一个将低级遥测数据转换为高级观察的可观察性模块，3) 一个实现环境无关策略的环境状态服务，以及4) 一个推断攻击者如何探索环境的攻击图服务。我们证明了Perry的抽象减少了探索各种欺骗防御、攻击者和环境的实现工作。我们通过模拟55个独特的欺骗假设情景来展示Perry的价值，并说明这些实验如何使操作员能够揭示细微的权衡。", "summary": "Perry是一个为加速网络欺骗实验而设计的高级框架。它通过提供一个高级抽象层，使安全操作员能够轻松定义攻击者和欺骗策略，并结合一个实验模块在模拟网络中运行这些场景。Perry包含行动规划器、可观察性模块、环境状态服务和攻击图服务，以实现高级规范到低级实现的转换。该框架显著降低了实验实现难度，并通过55个模拟情景验证了其在揭示欺骗策略权衡方面的有效性。", "keywords": "网络欺骗, 实验框架, 高级抽象, 安全操作, 模拟网络", "comments": "Perry框架的创新之处在于其高层次的抽象和模块化设计，极大地简化了网络欺骗实验的复杂性。它提供了一个统一的平台来设计、探索和评估不同的欺骗策略，这对于网络安全研究和实践具有重要意义。通过减少实现工作量并支持大规模实验，Perry有望加速网络欺骗领域的发展。"}}
{"id": "2506.20800", "title": "SIMulator: SIM Tracing on a (Pico-)Budget", "authors": ["Gabriel K. Gegenhuber", "Philipp É. Frenzel", "Adrian Dabrowski"], "summary": "SIM tracing -- the ability to inspect, modify, and relay communication\nbetween a SIM card and modem -- has become a significant technique in cellular\nnetwork research. It enables essential security- and development-related\napplications such as fuzzing communication interfaces, extracting session keys,\nmonitoring hidden SIM activity (e.g., proactive SIM commands or over-the-air\nupdates), and facilitating scalable, distributed measurement platforms through\nSIM reuse. Traditionally, achieving these capabilities has relied on\nspecialized hardware, which can pose financial and logistical burdens for\nresearchers, particularly those new to the field. In this work, we show that\nfull SIM tracing functionality can be achieved using only simple, widely\navailable components, such as UART interfaces and GPIO ports. We port these\ncapabilities to low-cost microcontrollers, exemplified by the Raspberry Pi Pico\n(4~USD). Unlike other approaches, it dramatically reduces hardware complexity\nby electrically decoupling the SIM and the modem and only transferring on APDU\nlevel. By significantly reducing hardware requirements and associated costs, we\naim to make SIM tracing techniques accessible to a broader community of\nresearchers and hobbyists, fostering wider exploration and experimentation in\ncellular network research.", "comment": "Accepted Poster at WiSec 2025", "pdf_url": "http://arxiv.org/pdf/2506.20800v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20800v1", "AI": {"title_translation": "SIMulator：低成本SIM卡追踪", "tldr": "本文介绍了一种使用低成本、易得组件（如树莓派Pico）实现SIM卡追踪功能的方法，大大降低了硬件复杂性和成本，使SIM卡追踪技术更易于获取。", "motivation": "传统的SIM卡追踪依赖于专用硬件，这给研究人员，尤其是新入门者，带来了经济和物流负担。", "method": "通过使用UART接口和GPIO端口等简单、广泛可用的组件，在低成本微控制器（例如树莓派Pico）上实现了完整的SIM卡追踪功能。该方法通过电气隔离SIM卡和调制解调器，并仅在APDU级别传输数据，从而显著降低了硬件复杂性。", "result": "实现了使用简单、广泛可用的组件（如树莓派Pico，4美元）进行完整的SIM卡追踪功能，显著降低了硬件要求和相关成本。", "conclusion": "通过显著降低硬件要求和相关成本，本工作旨在使SIM卡追踪技术更广泛地应用于研究人员和爱好者社区，从而促进蜂窝网络研究中更广泛的探索和实验。", "translation": "SIM卡追踪——检查、修改和转发SIM卡与调制解调器之间通信的能力——已成为蜂窝网络研究中的一项重要技术。它支持重要的安全和开发相关应用，例如模糊测试通信接口、提取会话密钥、监控隐藏的SIM活动（例如主动SIM命令或空中更新），并通过SIM卡复用促进可扩展的分布式测量平台。传统上，实现这些功能依赖于专用硬件，这可能给研究人员，特别是新入门者带来经济和物流负担。在这项工作中，我们展示了仅使用简单、广泛可用的组件（如UART接口和GPIO端口）即可实现完整的SIM卡追踪功能。我们将这些功能移植到低成本微控制器上，例如树莓派Pico（4美元）。与其他方法不同，它通过电气隔离SIM卡和调制解调器并仅在APDU级别传输数据，从而显著降低了硬件复杂性。通过显著降低硬件要求和相关成本，我们旨在使SIM卡追踪技术更广泛地应用于研究人员和爱好者社区，从而促进蜂窝网络研究中更广泛的探索和实验。", "summary": "本文提出了一种名为“SIMulator”的低成本SIM卡追踪解决方案，旨在解决传统SIM卡追踪依赖昂贵专用硬件的问题。研究人员展示了如何利用UART接口和GPIO端口等通用组件，在如树莓派Pico（4美元）之类的低成本微控制器上实现完整的SIM卡追踪功能。该方法通过电气隔离SIM卡和调制解调器并在APDU级别传输数据，显著降低了硬件复杂性。这项工作旨在使SIM卡追踪技术更易于获取，从而促进蜂窝网络研究的广泛探索和实验。", "keywords": "SIM卡追踪, 蜂窝网络, 低成本, 树莓派Pico, 硬件简化", "comments": "这项工作的创新之处在于，它通过利用低成本、现成的组件（如树莓派Pico）实现了通常需要昂贵专用硬件才能完成的SIM卡追踪功能，极大地降低了研究门槛和成本。其重要性在于，它使更广泛的研究人员和爱好者能够进行蜂窝网络安全和开发相关的实验，从而可能加速该领域的创新。通过电气解耦SIM卡和调制解调器并在APDU级别传输数据，该方法有效地简化了硬件设计。"}}
{"id": "2506.20806", "title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis", "authors": ["Zhonghao Zhan", "Huichi Zhou", "Hamed Haddadi"], "summary": "Graph Neural Networks (GNNs) show great promise for Network Intrusion\nDetection Systems (NIDS), particularly in IoT environments, but suffer\nperformance degradation due to distribution drift and lack robustness against\nrealistic adversarial attacks. Current robustness evaluations often rely on\nunrealistic synthetic perturbations and lack demonstrations on systematic\nanalysis of different kinds of adversarial attack, which encompass both\nblack-box and white-box scenarios. This work proposes a novel approach to\nenhance GNN robustness and generalization by employing Large Language Models\n(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These\nagents scrutinize graph structures derived from network flow data, identifying\nand potentially mitigating suspicious or adversarially perturbed elements\nbefore GNN processing. Our experiments, using a framework designed for\nrealistic evaluation and testing with a variety of adversarial attacks\nincluding a dataset collected from physical testbed experiments, demonstrate\nthat integrating LLM analysis can significantly improve the resilience of\nGNN-based NIDS against challenges, showcasing the potential of LLM agent as a\ncomplementary layer in intrusion detection architectures.", "comment": "Poster accepted at the 10th IEEE European Symposium on Security and\n  Privacy (Euro S&P 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20806v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20806v1", "AI": {"title_translation": "海报：通过基于代理的分析增强GNN在网络入侵检测中的鲁棒性", "tldr": "本研究提出一种新方法，通过使用大型语言模型（LLMs）作为模拟网络安全专家代理，提高图神经网络（GNNs）在网络入侵检测系统（NIDS）中的鲁棒性和泛化能力，实验证明该方法能显著提升GNN-NIDS的韧性。", "motivation": "图神经网络（GNNs）在网络入侵检测系统（NIDS）中显示出巨大潜力，尤其是在物联网环境中，但由于分布漂移和缺乏对真实对抗性攻击的鲁棒性而导致性能下降。目前的鲁棒性评估通常依赖不切实际的合成扰动，并且缺乏对不同类型对抗性攻击（包括黑盒和白盒场景）进行系统分析的演示。", "method": "本研究提出一种新颖的方法，通过在代理管道中利用大型语言模型（LLMs）作为模拟网络安全专家代理，以增强GNN的鲁棒性和泛化能力。这些代理在GNN处理之前，审查从网络流数据中导出的图结构，识别并可能缓解可疑或受对抗性扰动的元素。", "result": "我们的实验使用为真实评估设计的框架，并结合多种对抗性攻击（包括从物理测试台实验中收集的数据集），证明集成LLM分析可以显著提高基于GNN的NIDS对抗挑战的韧性。", "conclusion": "将LLM代理作为入侵检测架构中的补充层，具有增强GNN-NIDS鲁棒性和泛化能力的潜力。", "translation": "图神经网络（GNNs）在网络入侵检测系统（NIDS）中显示出巨大潜力，尤其是在物联网环境中，但由于分布漂移和缺乏对真实对抗性攻击的鲁棒性而导致性能下降。目前的鲁棒性评估通常依赖不切实际的合成扰动，并且缺乏对不同类型对抗性攻击（包括黑盒和白盒场景）进行系统分析的演示。本研究提出一种新颖的方法，通过在代理管道中利用大型语言模型（LLMs）作为模拟网络安全专家代理，以增强GNN的鲁棒性和泛化能力。这些代理在GNN处理之前，审查从网络流数据中导出的图结构，识别并可能缓解可疑或受对抗性扰动的元素。我们的实验使用为真实评估设计的框架，并结合多种对抗性攻击（包括从物理测试台实验中收集的数据集），证明集成LLM分析可以显著提高基于GNN的NIDS对抗挑战的韧性，展示了LLM代理作为入侵检测架构中补充层的潜力。", "summary": "本研究旨在解决图神经网络（GNNs）在网络入侵检测系统（NIDS）中面临的鲁棒性和泛化能力不足的问题。作者提出一种创新方法，利用大型语言模型（LLMs）作为网络安全专家代理，在GNN处理网络流数据之前，对其图结构进行审查，识别并减轻对抗性扰动。实验结果表明，这种LLM与GNN的集成显著提升了NIDS在面对真实对抗性攻击时的韧性，突显了LLM代理在入侵检测架构中的潜在价值。", "keywords": "图神经网络, 网络入侵检测, 鲁棒性, 大型语言模型, 对抗性攻击", "comments": "该论文的创新点在于将大型语言模型（LLMs）引入到图神经网络（GNNs）的鲁棒性增强中，特别是通过模拟网络安全专家代理的方式，在GNN处理前进行预分析和缓解。这种代理式的方法为提升GNN在对抗性环境下的性能提供了一个新颖的视角，且其在真实数据集上的验证增加了研究的可信度。该方法有望为未来的入侵检测系统提供更强的防御能力。"}}
{"id": "2506.20872", "title": "Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research", "authors": ["Osama Zafar", "Rosemarie Santa González", "Mina Namazi", "Alfonso Morales", "Erman Ayday"], "summary": "Data-driven agriculture, which integrates technology and data into\nagricultural practices, has the potential to improve crop yield, disease\nresilience, and long-term soil health. However, privacy concerns, such as\nadverse pricing, discrimination, and resource manipulation, deter farmers from\nsharing data, as it can be used against them. To address this barrier, we\npropose a privacy-preserving framework that enables secure data sharing and\ncollaboration for research and development while mitigating privacy risks. The\nframework combines dimensionality reduction techniques (like Principal\nComponent Analysis (PCA)) and differential privacy by introducing Laplacian\nnoise to protect sensitive information. The proposed framework allows\nresearchers to identify potential collaborators for a target farmer and train\npersonalized machine learning models either on the data of identified\ncollaborators via federated learning or directly on the aggregated\nprivacy-protected data. It also allows farmers to identify potential\ncollaborators based on similarities. We have validated this on real-life\ndatasets, demonstrating robust privacy protection against adversarial attacks\nand utility performance comparable to a centralized system. We demonstrate how\nthis framework can facilitate collaboration among farmers and help researchers\npursue broader research objectives. The adoption of the framework can empower\nresearchers and policymakers to leverage agricultural data responsibly, paving\nthe way for transformative advances in data-driven agriculture. By addressing\ncritical privacy challenges, this work supports secure data integration,\nfostering innovation and sustainability in agricultural systems.", "comment": "arXiv admin note: text overlap with arXiv:2409.06069", "pdf_url": "http://arxiv.org/pdf/2506.20872v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20872v1", "AI": {"title_translation": "赋能数字农业：一个用于数据共享和协作研究的隐私保护框架", "tldr": "该研究提出了一个隐私保护框架，通过结合降维技术和差分隐私，解决农业数据共享中的隐私问题，并支持安全的协作研究和个性化模型训练。", "motivation": "数据驱动的农业有潜力提高作物产量、疾病抵抗力和土壤健康，但隐私问题（如不利定价、歧视和资源操纵）阻碍了农民共享数据，因为数据可能被用来对付他们。为解决这一障碍，论文提出了一个隐私保护框架。", "method": "该框架结合了降维技术（如主成分分析PCA）和差分隐私，通过引入拉普拉斯噪声来保护敏感信息。它允许研究人员为目标农民识别潜在合作者，并通过联邦学习或直接在聚合的隐私保护数据上训练个性化机器学习模型。它还允许农民根据相似性识别潜在合作者。", "result": "该框架已在真实数据集上得到验证，证明了对对抗性攻击的强大隐私保护能力，并且实用性表现与中心化系统相当。它能够促进农民之间的协作，并帮助研究人员实现更广泛的研究目标。", "conclusion": "通过解决关键的隐私挑战，该框架支持安全的数据集成，从而促进农业系统的创新和可持续性，并赋能研究人员和政策制定者负责任地利用农业数据，为数据驱动农业的变革性进步铺平道路。", "translation": "数据驱动的农业将技术和数据融入农业实践，有潜力提高作物产量、疾病抵抗力和长期土壤健康。然而，隐私担忧，如不利定价、歧视和资源操纵，阻碍了农民分享数据，因为这些数据可能被用来对付他们。为了解决这一障碍，我们提出了一个隐私保护框架，该框架能够实现安全的数据共享和研究开发协作，同时减轻隐私风险。该框架结合了降维技术（如主成分分析（PCA））和差分隐私，通过引入拉普拉斯噪声来保护敏感信息。所提出的框架允许研究人员为目标农民识别潜在合作者，并通过联邦学习或直接在聚合的隐私保护数据上，利用已识别合作者的数据训练个性化机器学习模型。它还允许农民根据相似性识别潜在合作者。我们已在真实数据集上验证了这一点，证明了其对对抗性攻击的强大隐私保护能力，并且实用性表现与中心化系统相当。我们展示了该框架如何促进农民之间的协作，并帮助研究人员追求更广泛的研究目标。该框架的采用可以赋能研究人员和政策制定者负责任地利用农业数据，为数据驱动农业的变革性进步铺平道路。通过解决关键的隐私挑战，这项工作支持安全的数据集成，促进农业系统的创新和可持续性。", "summary": "该论文提出了一个隐私保护框架，旨在解决数字农业中数据共享的隐私障碍。该框架结合了降维技术（如PCA）和差分隐私（通过拉普拉斯噪声），以保护敏感农业数据。它支持研究人员识别合作者、训练个性化机器学习模型（通过联邦学习或聚合数据），并允许农民基于相似性进行协作。在真实数据集上的验证表明，该框架提供了强大的隐私保护，且性能与中心化系统相当，从而促进了安全的数据集成和数字农业的创新。", "keywords": "数字农业, 隐私保护, 数据共享, 差分隐私, 联邦学习", "comments": "该论文的创新点在于将降维技术与差分隐私相结合，构建了一个实用的隐私保护框架，专门应对数字农业数据共享的挑战。它不仅关注数据安全，还考虑了实际应用中的协作和个性化模型训练需求。其重要性在于能够促进农业数据的负责任利用，克服阻碍数据驱动农业发展的关键隐私瓶颈，对推动农业智能化和可持续发展具有积极意义。"}}
{"id": "2506.20754", "title": "Domain Knowledge in Requirements Engineering: A Systematic Mapping Study", "authors": ["Marina Araújo", "Júlia Araújo", "Romeu Oliveira", "Lucas Romao", "Marcos Kalinowski"], "summary": "[Context] Domain knowledge is recognized as a key component for the success\nof Requirements Engineering (RE), as it provides the conceptual support needed\nto understand the system context, ensure alignment with stakeholder needs, and\nreduce ambiguity in requirements specification. Despite its relevance, the\nscientific literature still lacks a systematic consolidation of how domain\nknowledge can be effectively used and operationalized in RE. [Goal] This paper\naddresses this gap by offering a comprehensive overview of existing\ncontributions, including methods, techniques, and tools to incorporate domain\nknowledge into RE practices. [Method] We conducted a systematic mapping study\nusing a hybrid search strategy that combines database searches with iterative\nbackward and forward snowballing. [Results] In total, we found 75 papers that\nmet our inclusion criteria. The analysis highlights the main types of\nrequirements addressed, the most frequently considered quality attributes, and\nrecurring challenges in the formalization, acquisition, and long-term\nmaintenance of domain knowledge. The results provide support for researchers\nand practitioners in identifying established approaches and unresolved issues.\nThe study also outlines promising directions for future research, emphasizing\nthe development of scalable, automated, and sustainable solutions to integrate\ndomain knowledge into RE processes. [Conclusion] The study contributes by\nproviding a comprehensive overview that helps to build a conceptual and\nmethodological foundation for knowledge-driven requirements engineering.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20754v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20754v1", "AI": {"title_translation": "需求工程中的领域知识：一项系统映射研究", "tldr": "本研究通过系统映射研究，全面概述了在需求工程中有效利用领域知识的现有方法、技术和工具，并指出了未来的研究方向。", "motivation": "尽管领域知识在需求工程（RE）中至关重要，但现有科学文献缺乏对其在RE中如何有效使用和操作化的系统性整合。", "method": "我们采用混合搜索策略进行了系统映射研究，结合了数据库搜索以及迭代的向前和向后滚雪球法。", "result": "共发现了75篇符合纳入标准的论文。分析揭示了所处理的需求类型、最常考虑的质量属性，以及领域知识形式化、获取和长期维护中的常见挑战。研究结果为研究人员和实践者识别既定方法和未解决问题提供了支持，并为未来研究指明了方向，强调开发可扩展、自动化和可持续的解决方案。", "conclusion": "本研究通过提供全面的概述，为知识驱动的需求工程构建概念和方法论基础做出了贡献。", "translation": "[背景] 领域知识被认为是需求工程（RE）成功的关键组成部分，因为它提供了理解系统上下文、确保与利益相关者需求对齐以及减少需求规范模糊性所需的概念支持。尽管其具有相关性，但科学文献仍然缺乏对领域知识如何在RE中有效使用和操作化的系统性整合。[目标] 本文通过全面概述现有贡献，包括将领域知识纳入RE实践的方法、技术和工具，来弥补这一空白。[方法] 我们采用混合搜索策略进行了系统映射研究，该策略结合了数据库搜索和迭代的向前和向后滚雪球法。[结果] 我们总共找到了75篇符合我们纳入标准的论文。分析强调了所处理的需求主要类型、最常考虑的质量属性，以及领域知识形式化、获取和长期维护中的常见挑战。研究结果为研究人员和实践者识别既定方法和未解决问题提供了支持。该研究还概述了未来研究的有前景方向，强调开发可扩展、自动化和可持续的解决方案，以将领域知识整合到RE过程中。[结论] 本研究通过提供全面的概述，有助于为知识驱动的需求工程构建概念和方法论基础。", "summary": "本系统映射研究旨在弥补在需求工程（RE）中有效利用和操作化领域知识的文献空白。研究通过混合搜索策略识别了75篇相关论文，并分析了领域知识在RE中的应用、挑战和质量属性。研究结果为研究人员和实践者提供了现有方法的洞察，并指明了未来在开发可扩展、自动化和可持续解决方案方面的研究方向，最终为知识驱动的RE奠定了概念和方法论基础。", "keywords": "需求工程, 领域知识, 系统映射研究, 知识管理, 需求规范", "comments": "这项研究通过系统映射的方式，对需求工程中领域知识的应用进行了全面的梳理，填补了该领域系统性整合的空白。其价值在于为研究人员和实践者提供了一个宝贵的资源，帮助他们理解现有方法、识别未解决的问题，并指明了未来研究的重点，特别是强调了自动化和可持续解决方案的重要性。其创新性体现在通过严格的系统映射方法，对分散的文献进行了有效的整合和分析。"}}
{"id": "2506.20748", "title": "Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots", "authors": ["Jingshu Li", "Zicheng Zhu", "Renwen Zhang", "Yi-Chieh Lee"], "summary": "Chatbots are increasingly integrated into people's lives and are widely used\nto help people. Recently, there has also been growing interest in the reverse\ndirection-humans help chatbots-due to a wide range of benefits including better\nchatbot performance, human well-being, and collaborative outcomes. However,\nlittle research has explored the factors that motivate people to help chatbots.\nTo address this gap, we draw on the Computers Are Social Actors (CASA)\nframework to examine how chatbot anthropomorphism-including human-like\nidentity, emotional expression, and non-verbal expression-influences human\nempathy toward chatbots and their subsequent prosocial behaviors and\nintentions. We also explore people's own interpretations of their prosocial\nbehaviors toward chatbots. We conducted an online experiment (N = 244) in which\nchatbots made mistakes in a collaborative image labeling task and explained the\nreasons to participants. We then measured participants' prosocial behaviors and\nintentions toward the chatbots. Our findings revealed that human identity and\nemotional expression of chatbots increased participants' prosocial behavior and\nintention toward chatbots, with empathy mediating these effects. Qualitative\nanalysis further identified two motivations for participants' prosocial\nbehaviors: empathy for the chatbot and perceiving the chatbot as human-like. We\ndiscuss the implications of these results for understanding and promoting human\nprosocial behaviors toward chatbots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20748v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20748v1", "AI": {"title_translation": "探索聊天机器人拟人化和人类同理心对人类对聊天机器人亲社会行为的影响", "tldr": "聊天机器人的拟人化和情感表达能增强人类对其的同理心和亲社会行为。", "motivation": "尽管人类帮助聊天机器人能带来多方面益处（如提升聊天机器人性能、人类福祉和协作成果），但目前鲜有研究探讨促使人类帮助聊天机器人的因素。本研究旨在填补这一空白。", "method": "本研究基于“计算机是社会角色”（CASA）框架，通过一项在线实验（N=244）进行。实验中，聊天机器人在协作图像标注任务中犯错并向参与者解释原因，随后测量了参与者对聊天机器人的亲社会行为和意图。研究探讨了聊天机器人拟人化（包括类人身份、情感表达和非语言表达）如何影响人类对聊天机器人的同理心及其随后的亲社会行为和意图。", "result": "研究发现，聊天机器人的类人身份和情感表达增加了参与者对聊天机器人的亲社会行为和意图，且同理心在这些影响中起到了中介作用。定性分析进一步识别出参与者亲社会行为的两种动机：对聊天机器人的同理心和将聊天机器人视为类人。", "conclusion": "本研究的结果对于理解和促进人类对聊天机器人的亲社会行为具有重要意义，并可指导未来人机协作系统的设计。", "translation": "聊天机器人日益融入人们的生活，并被广泛用于帮助人们。最近，人们对相反的方向——人类帮助聊天机器人——的兴趣也日益增长，这带来了包括更好的聊天机器人性能、人类福祉和协作成果在内的诸多益处。然而，很少有研究探讨促使人们帮助聊天机器人的因素。为了弥补这一空白，我们借鉴了“计算机是社会角色”（CASA）框架，研究聊天机器人拟人化——包括类人身份、情感表达和非语言表达——如何影响人类对聊天机器人的同理心以及随后的亲社会行为和意图。我们还探讨了人们对他们帮助聊天机器人亲社会行为的自身解释。我们进行了一项在线实验（N = 244），其中聊天机器人在协作图像标注任务中犯错并向参与者解释了原因。然后，我们测量了参与者对聊天机器人的亲社会行为和意图。我们的研究结果表明，聊天机器人的类人身份和情感表达增加了参与者对聊天机器人的亲社会行为和意图，同理心在这些影响中起中介作用。定性分析进一步确定了参与者亲社会行为的两种动机：对聊天机器人的同理心和将聊天机器人视为类人。我们讨论了这些结果对于理解和促进人类对聊天机器人的亲社会行为的启示。", "summary": "本研究探讨了聊天机器人的拟人化（包括类人身份和情感表达）如何通过影响人类的同理心进而促进人类对聊天机器人的亲社会行为。一项在线实验（N=244）显示，聊天机器人的类人身份和情感表达显著增加了用户对其的亲社会行为和意图，且同理心在此过程中发挥了中介作用。定性分析揭示，对聊天机器人的同理心和将其视为类人是促使人类提供帮助的主要动机。研究结果为理解和促进人机协作中的亲社会行为提供了理论依据和实践指导。", "keywords": "聊天机器人拟人化, 人类同理心, 亲社会行为, 人机交互, CASA框架", "comments": "该研究创新性地探讨了人类对聊天机器人亲社会行为的影响因素，填补了该领域的空白。通过结合CASA框架、量化实验和定性分析，研究提供了有力证据，证明了聊天机器人拟人化和情感表达在诱发人类同理心和亲社会行为中的关键作用。这对于设计更具吸引力、协作性和用户福祉的AI系统具有重要指导意义。"}}
{"id": "2506.21490", "title": "Ad-Hoc Human-AI Coordination Challenge", "authors": ["Tin Dizdarević", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Jakob Nicolaus Foerster"], "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21490v1", "categories": ["cs.AI", "cs.HC", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21490v1", "AI": {"title_translation": "即时人机协作挑战", "tldr": "本文提出了即时人机协作挑战（AH2AC2），通过开发人类代理智能体和开放数据集来克服人类评估的挑战，旨在促进人机协作领域的发展。", "motivation": "实现AI智能体与人类之间的无缝协作对于实际应用至关重要，但目前仍是一个重大的开放挑战。现有人机交互中对Hanabi游戏的使用受限于昂贵且难以重现的人类评估。", "method": "本文引入了即时人机协作挑战（AH2AC2），以克服昂贵且难以重现的人类评估限制。研究人员在一个大规模人类数据集上开发了“人类代理智能体”，这些智能体可以作为AH2AC2中稳健、廉价且可复现的类人评估伙伴。为了鼓励开发数据高效的方法，他们开源了一个包含3,079场游戏的数据集，并故意限制了可用的人类游戏数据量。为了确保公平评估，代理智能体通过受控评估系统托管，而不是公开发布。", "result": "本文为两人和三人Hanabi场景提供了基线结果。开源了一个包含3,079场游戏的数据集。", "conclusion": "本文通过引入AH2AC2挑战、开发人类代理智能体和提供有限但有价值的数据集，为解决人机协作评估难题提供了一个创新且可扩展的解决方案，从而促进了该领域数据高效方法的发展。", "translation": "在实际应用中，实现AI智能体与人类之间的无缝协作至关重要，但这仍然是一个重大的开放挑战。Hanabi是一款具有不完美信息、受限通信、心智理论要求和协调行动的合作纸牌游戏——这使其成为人机协作的理想试验平台。然而，其在人机交互中的使用一直受到人类评估挑战的限制。在这项工作中，我们引入了即时人机协作挑战（AH2AC2），以克服昂贵且难以重现的人类评估限制。我们在一个大规模人类数据集上开发了“人类代理智能体”，它们在AH2AC2中可以作为稳健、廉价且可复现的类人评估伙伴。为了鼓励开发数据高效的方法，我们开源了一个包含3,079场游戏的数据集，并故意限制了可用的人类游戏数据量。我们展示了两人和三人Hanabi场景的基线结果。为了确保公平评估，我们通过受控评估系统托管代理智能体，而不是公开发布它们。代码可在https://github.com/FLAIROx/ah2ac2获取。", "summary": "本文提出了即时人机协作挑战（AH2AC2），旨在通过克服传统人类评估的局限性来促进人机协作领域的发展。研究人员在合作纸牌游戏Hanabi中，利用大规模人类数据集开发了“人类代理智能体”，这些智能体可作为廉价、可复现的类人评估伙伴。此外，他们开源了一个有限规模的数据集，以鼓励开发数据高效的AI方法，并提供了基线结果，通过受控系统确保公平评估。", "keywords": "人机协作, AI代理, Hanabi, 评估, 数据集", "comments": "这项工作具有重要的创新性，它通过引入“人类代理智能体”解决了人机协作研究中昂贵且难以重现的人类评估这一核心挑战。这种方法为开发和测试人机协作AI提供了一个高效且可扩展的平台。通过开源有限数据量的数据集，该论文还巧妙地鼓励了数据高效型AI算法的开发，这在实际应用中具有重要意义。其贡献在于为未来的人机协作研究奠定了坚实的基础。"}}
{"id": "2506.20782", "title": "Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing", "authors": ["Marc Bara"], "summary": "We present the first theoretical framework for applying spiking neural\nnetworks (SNNs) to synthetic aperture radar (SAR) interferometric phase\nunwrapping. Despite extensive research in both domains, our comprehensive\nliterature review confirms that SNNs have never been applied to phase\nunwrapping, representing a significant gap in current methodologies. As Earth\nobservation data volumes continue to grow exponentially (with missions like\nNISAR expected to generate 100PB in two years) energy-efficient processing\nbecomes critical for sustainable data center operations. SNNs, with their\nevent-driven computation model, offer potential energy savings of 30-100x\ncompared to conventional approaches while maintaining comparable accuracy. We\ndevelop spike encoding schemes specifically designed for wrapped phase data,\npropose SNN architectures that leverage the spatial propagation nature of phase\nunwrapping, and provide theoretical analysis of computational complexity and\nconvergence properties. Our framework demonstrates how the temporal dynamics\ninherent in SNNs can naturally model the spatial continuity constraints\nfundamental to phase unwrapping. This work opens a new research direction at\nthe intersection of neuromorphic computing and SAR interferometry, offering a\ncomplementary approach to existing algorithms that could enable more\nsustainable large-scale InSAR processing.", "comment": "8 pages, 2 figures, patent pending", "pdf_url": "http://arxiv.org/pdf/2506.20782v1", "categories": ["cs.NE", "cs.ET", "cs.LG", "eess.SP", "68T07, 94A08", "I.2.6; G.1.6; B.7.1"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20782v1", "AI": {"title_translation": "脉冲神经网络用于SAR干涉相位解缠：一种节能处理的理论框架", "tldr": "首次提出将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，旨在实现节能高效的处理。", "motivation": "地球观测数据量呈指数级增长，需要节能处理以实现可持续的数据中心运营。传统方法能耗高，而脉冲神经网络（SNNs）具有显著节能潜力（30-100倍）。", "method": "开发了专门用于缠绕相位数据的脉冲编码方案；提出了利用相位解缠空间传播特性的SNN架构；提供了计算复杂度和收敛特性的理论分析。", "result": "该框架展示了SNNs固有的时间动态如何自然地模拟相位解缠中基本的空间连续性约束。", "conclusion": "这项工作开辟了神经形态计算和SAR干涉测量交叉领域的新研究方向，为现有算法提供了一种补充方法，可能实现更可持续的大规模InSAR处理。", "translation": "我们首次提出了将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架。尽管这两个领域都有广泛的研究，但我们全面的文献综述证实，SNNs从未应用于相位解缠，这代表了当前方法论中的一个显著空白。随着地球观测数据量继续呈指数级增长（像NISAR这样的任务预计两年内将产生100PB数据），节能处理对于可持续的数据中心运营变得至关重要。SNNs以其事件驱动的计算模型，与传统方法相比，在保持相当准确性的同时，可提供30-100倍的潜在能源节省。我们开发了专门为缠绕相位数据设计的脉冲编码方案，提出了利用相位解缠空间传播性质的SNN架构，并提供了计算复杂度和收敛特性的理论分析。我们的框架展示了SNNs固有的时间动态如何自然地模拟相位解缠中基本的空间连续性约束。这项工作开辟了神经形态计算和SAR干涉测量交叉领域的新研究方向，为现有算法提供了一种补充方法，可能实现更可持续的大规模InSAR处理。", "summary": "本文首次提出了将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架。针对地球观测数据量激增导致的能耗问题，SNNs因其事件驱动特性可实现显著节能。研究开发了专门的脉冲编码方案和SNN架构，并分析了其计算复杂度和收敛性，证明SNN的时间动态能有效模拟相位解缠的空间连续性约束，为大规模InSAR处理提供了节能新途径。", "keywords": "脉冲神经网络, SAR干涉相位解缠, 节能处理, 神经形态计算, 理论框架", "comments": "这项工作具有创新性，因为它首次将SNNs应用于SAR相位解缠，填补了现有研究空白。其重要性在于为未来大规模地球观测数据的节能处理提供了新的范式，尤其是在神经形态计算和SAR干涉测量交叉领域开辟了新方向。"}}
{"id": "2506.20801", "title": "IMA-Catcher: An IMpact-Aware Nonprehensile Catching Framework based on Combined Optimization and Learning", "authors": ["Francesco Tassi", "Jianzhuang Zhao", "Gustavo J. G. Lahr", "Luna Gava", "Marco Monforte", "Arren Glover", "Chiara Bartolozzi", "Arash Ajoudani"], "summary": "Robotic catching of flying objects typically generates high impact forces\nthat might lead to task failure and potential hardware damages. This is\naccentuated when the object mass to robot payload ratio increases, given the\nstrong inertial components characterizing this task. This paper aims to address\nthis problem by proposing an implicitly impact-aware framework that\naccomplishes the catching task in both pre- and post-catching phases. In the\nfirst phase, a motion planner generates optimal trajectories that minimize\ncatching forces, while in the second, the object's energy is dissipated\nsmoothly, minimizing bouncing. In particular, in the pre-catching phase, a\nreal-time optimal planner is responsible for generating trajectories of the\nend-effector that minimize the velocity difference between the robot and the\nobject to reduce impact forces during catching. In the post-catching phase, the\nrobot's position, velocity, and stiffness trajectories are generated based on\nhuman demonstrations when catching a series of free-falling objects with\nunknown masses. A hierarchical quadratic programming-based controller is used\nto enforce the robot's constraints (i.e., joint and torque limits) and create a\nstack of tasks that minimizes the reflected mass at the end-effector as a\nsecondary objective. The initial experiments isolate the problem along one\ndimension to accurately study the effects of each contribution on the metrics\nproposed. We show how the same task, without velocity matching, would be\ninfeasible due to excessive joint torques resulting from the impact. The\naddition of reflected mass minimization is then investigated, and the catching\nheight is increased to evaluate the method's robustness. Finally, the setup is\nextended to catching along multiple Cartesian axes, to prove its generalization\nin space.", "comment": "25 pages, 17 figures, accepted by International Journal of Robotics\n  Research (IJRR)", "pdf_url": "http://arxiv.org/pdf/2506.20801v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20801v1", "AI": {"title_translation": "IMA-Catcher：一个基于组合优化和学习的冲击感知非抓取捕获框架", "tldr": "提出IMA-Catcher框架，通过在捕获前优化轨迹以减小冲击力，并在捕获后通过学习和控制来消散能量、减少弹跳，解决机器人捕获高速物体时的高冲击力问题。", "motivation": "机器人捕获飞行物体时会产生高冲击力，这可能导致任务失败和潜在的硬件损坏，尤其当物体质量与机器人有效载荷比增加时，问题会更加突出。", "method": "本文提出了一个隐式冲击感知框架，分为两个阶段：在捕获前阶段，实时优化规划器生成末端执行器轨迹以最小化机器人与物体间的速度差，从而减少冲击力；在捕获后阶段，基于人类演示生成机器人的位置、速度和刚度轨迹，以平滑耗散能量并最小化弹跳。此外，使用分层二次规划控制器来强制执行机器人约束（如关节和扭矩限制），并将末端执行器反射质量最小化作为次要目标。", "result": "初步实验证明，如果没有速度匹配，相同任务会因冲击导致的关节扭矩过大而不可行。研究了反射质量最小化的添加效果，并增加了捕获高度以评估方法的鲁棒性。最后，将设置扩展到沿多个笛卡尔轴的捕获，证明了其在空间上的泛化能力。", "conclusion": "该框架通过结合优化和学习，有效解决了机器人捕获高速物体时的高冲击力问题，提高了捕获任务的成功率和鲁棒性，并展现了其在多轴捕获中的泛化能力。", "translation": "机器人捕获飞行物体通常会产生高冲击力，这可能导致任务失败和潜在的硬件损坏。当物体质量与机器人有效载荷之比增加时，考虑到表征此任务的强惯性分量，这种情况会更加突出。本文旨在通过提出一个隐式冲击感知框架来解决这个问题，该框架在捕获前和捕获后阶段都能完成捕获任务。在第一阶段，运动规划器生成最小化捕获力的最佳轨迹，而在第二阶段，物体的能量被平滑地耗散，从而最大程度地减少弹跳。特别是，在捕获前阶段，实时优化规划器负责生成末端执行器的轨迹，以最小化机器人与物体之间的速度差，从而减少捕获时的冲击力。在捕获后阶段，机器人的位置、速度和刚度轨迹是根据人类演示生成的，这些演示涉及捕获一系列未知质量的自由落体物体。使用分层二次规划控制器来强制执行机器人的约束（即关节和扭矩限制），并创建一堆任务，将末端执行器的反射质量最小化作为次要目标。初步实验在一维上隔离了问题，以准确研究每个贡献对所提出指标的影响。我们展示了如果没有速度匹配，由于冲击导致关节扭矩过大，相同的任务将不可行。然后研究了反射质量最小化的添加，并增加了捕获高度以评估该方法的鲁棒性。最后，将设置扩展到沿多个笛卡尔轴的捕获，以证明其在空间上的泛化能力。", "summary": "本文提出了IMA-Catcher，一个隐式冲击感知的非抓取捕获框架，旨在解决机器人捕获高速飞行物体时产生的高冲击力问题。该框架在捕获前通过优化机器人末端执行器与物体间的速度匹配来减小冲击力，在捕获后则通过基于人类演示学习的轨迹规划和分层二次规划控制器来平滑耗散能量、最小化弹跳，并满足机器人约束。实验证明该方法能有效降低冲击、提高捕获成功率和鲁棒性，并具有多轴泛化能力。", "keywords": "机器人捕获, 冲击感知, 优化, 学习, 非抓取", "comments": "这篇论文通过结合优化和学习，为机器人非抓取捕获任务中高冲击力问题提供了一个创新性解决方案。其亮点在于将捕获过程分解为冲击前的速度匹配优化和冲击后的能量耗散与约束满足，并通过人类演示引入学习，增强了系统的适应性。将反射质量最小化作为次要目标，进一步提升了控制的精细度。实验结果也清晰地展示了其有效性和鲁棒性，特别是对多轴泛化的验证，预示了其在复杂实际应用中的潜力。"}}
{"id": "2506.20679", "title": "Establishing validated standards for Home and Work location Detection", "authors": ["Silvia de Sojo", "Lorenzo Lucchini", "Ollin D. Langle-Chimal", "Samuel P. Fraiberger", "Laura Alessandretti"], "summary": "Smartphone location data have transformed urban mobility research, providing\nunprecedented insights into how people navigate and interact in cities.\nHowever, leveraging location data at scale presents methodological challenges.\nAccurately identifying individuals' home and work locations is critical for a\nrange of applications, including commuting analysis, unemployment estimation,\nand urban accessibility studies. Despite their widespread use, home-work\ndetection methods lack a standardized framework that accounts for differing\ndata quality and that is validated against ground-truth observations. This\nlimits the comparability and reproducibility of results across studies and\ndatasets. In this paper, we present HoWDe, a robust algorithm for identifying\nhome and work locations from mobility data, explicitly designed to handle\nmissing data and varying data quality across individuals. Using two unique\nground-truth datasets comprising over 5100 individuals from more than 80\ncountries, HoWDe achieves home and work detection accuracies of up to 97% and\n88%, respectively, with consistent performance across countries and demographic\ngroups. We examine how parameter choices shape the trade-off between accuracy\nand user retention, and demonstrate how these methodological decisions\ninfluence downstream applications such as employment estimation and commuting\npattern analysis. By supporting in-house pre-processing through a transparent\nand validated pipeline, HoWDe also facilitates the sharing of\nprivacy-preserving mobility data. Together, our tools and findings establish\nmethodological standards that support more robust, scalable, and reproducible\nmobility research at both individual and urban scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20679v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20679v1", "AI": {"title_translation": "建立家庭和工作地点检测的验证标准", "tldr": "提出了HoWDe算法，用于从智能手机数据中准确识别家庭和工作地点，并经验证具有高精度和可重复性，解决了现有方法的标准化问题。", "motivation": "智能手机位置数据对城市交通研究至关重要，但大规模利用存在方法学挑战。准确识别家庭和工作地点对通勤分析、失业估计和城市可达性研究至关重要。现有家庭-工作地点检测方法缺乏考虑不同数据质量并经过地面真实验证的标准化框架，限制了研究结果的可比性和可重复性。", "method": "本文提出了HoWDe算法，一种用于从移动数据中识别家庭和工作地点的鲁棒算法，专门设计用于处理缺失数据和个体之间不同的数据质量。该算法使用来自80多个国家的5100多名个体的两个独特的地面真实数据集进行验证。", "result": "HoWDe算法在家庭和工作地点检测方面分别达到高达97%和88%的准确率，并且在不同国家和人口群体中表现一致。研究还检查了参数选择如何影响准确性和用户保留之间的权衡，并展示了这些方法学决策如何影响失业估计和通勤模式分析等下游应用。", "conclusion": "HoWDe工具和发现建立了方法学标准，支持在个体和城市尺度上进行更稳健、可扩展和可重复的移动性研究。通过透明和经过验证的管道支持内部预处理，HoWDe还促进了隐私保护移动数据的共享。", "translation": "智能手机位置数据已经改变了城市交通研究，提供了前所未有的洞察力，了解人们如何在城市中导航和互动。然而，大规模利用位置数据带来了方法学挑战。准确识别个人家庭和工作地点对于一系列应用至关重要，包括通勤分析、失业估计和城市可达性研究。尽管广泛使用，家庭-工作地点检测方法缺乏一个标准化框架，该框架能够考虑不同的数据质量并根据地面真实观测进行验证。这限制了研究结果在不同研究和数据集之间的可比性和可重复性。在本文中，我们提出了HoWDe，一种从移动数据中识别家庭和工作地点的鲁棒算法，明确设计用于处理缺失数据和个体之间不同的数据质量。使用包含来自80多个国家的5100多名个体的两个独特的地面真实数据集，HoWDe在家庭和工作地点检测方面分别达到高达97%和88%的准确率，并且在不同国家和人口群体中表现一致。我们检查了参数选择如何影响准确性和用户保留之间的权衡，并展示了这些方法学决策如何影响失业估计和通勤模式分析等下游应用。通过透明和经过验证的管道支持内部预处理，HoWDe还促进了隐私保护移动数据的共享。总而言之，我们的工具和发现建立了方法学标准，支持在个体和城市尺度上进行更稳健、可扩展和可重复的移动性研究。", "summary": "本论文介绍了HoWDe，一种创新的算法，旨在通过处理数据缺失和质量差异，从智能手机移动数据中准确识别家庭和工作地点。该算法利用超过5100个个体的大规模地面真实数据集进行验证，实现了高达97%的家庭地点检测准确率和88%的工作地点检测准确率，并在不同国家和人口群体中表现出一致性。HoWDe通过提供一个标准化且经验证的框架，解决了现有方法在可比性和可重复性方面的限制，从而支持更稳健、可扩展和可重复的城市移动性研究，并促进隐私保护数据的共享。", "keywords": "家庭和工作地点检测, 移动数据, HoWDe, 准确性, 标准化", "comments": "HoWDe的创新在于其鲁棒性，能够处理不同质量和缺失的移动数据，并结合大规模地面真实数据进行验证，这在现有方法中是缺失的。其重要性在于建立了家庭和工作地点检测的标准化方法，显著提高了研究结果的可比性和可重复性，对城市规划、通勤分析和失业估计等下游应用具有重要意义。此外，它还促进了隐私保护数据的共享，这在当前数据隐私日益受关注的背景下尤为关键。"}}
{"id": "2506.20762", "title": "Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks", "authors": ["Shisheng Hu", "Jie Gao", "Xue Qin", "Conghao Zhou", "Xinyu Huang", "Mushu Li", "Mingcheng He", "Xuemin Shen"], "summary": "In this paper, we propose a novel drift-adaptive slicing-based resource\nmanagement scheme for cooperative integrated sensing and communication (ISAC)\nnetworks. Particularly, we establish two network slices to provide sensing and\ncommunication services, respectively. In the large-timescale planning for the\nslices, we partition the sensing region of interest (RoI) of each mobile device\nand reserve network resources accordingly, facilitating low-complexity\ndistance-based sensing target assignment in small timescales. To cope with the\nnon-stationary spatial distributions of mobile devices and sensing targets,\nwhich can result in the drift in modeling the distributions and ineffective\nplanning decisions, we construct digital twins (DTs) of the slices. In each DT,\na drift-adaptive statistical model and an emulation function are developed for\nthe spatial distributions in the corresponding slice, which facilitates\nclosed-form decision-making and efficient validation of a planning decision,\nrespectively. Numerical results show that the proposed drift-adaptive\nslicing-based resource management scheme can increase the service satisfaction\nratio by up to 18% and reduce resource consumption by up to 13.1% when compared\nwith benchmark schemes.", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking", "pdf_url": "http://arxiv.org/pdf/2506.20762v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.20762v1", "AI": {"title_translation": "漂移自适应切片式合作ISAC网络资源管理", "tldr": "本文提出了一种新颖的漂移自适应切片式资源管理方案，通过构建数字孪生来应对移动设备和感知目标的非平稳空间分布，从而提高ISAC网络的资源管理效率和用户满意度。", "motivation": "现有的资源管理方案难以应对移动设备和感知目标非平稳空间分布导致的建模漂移和无效规划决策问题。", "method": "提出了一种漂移自适应切片式资源管理方案。该方案为感知和通信服务建立两个网络切片，并在大时间尺度规划中划分感知感兴趣区域并预留资源。为应对非平稳空间分布，构建了切片的数字孪生（DTs），每个DT中开发了漂移自适应统计模型和仿真功能，以实现闭式决策和高效验证。", "result": "与基准方案相比，所提出的漂移自适应切片式资源管理方案可以将服务满意度提高多达18%，并减少资源消耗多达13.1%。", "conclusion": "提出的漂移自适应切片式资源管理方案能有效应对ISAC网络中移动设备和感知目标的非平稳分布，显著提升服务满意度和资源利用效率。", "translation": "在本文中，我们提出了一种新颖的漂移自适应切片式合作集成感知与通信（ISAC）网络资源管理方案。具体而言，我们建立了两个网络切片，分别提供感知和通信服务。在切片的大时间尺度规划中，我们划分了每个移动设备的感知感兴趣区域（RoI），并相应地预留了网络资源，从而促进了小时间尺度下基于距离的低复杂度感知目标分配。为了应对移动设备和感知目标的非平稳空间分布，这些分布可能导致建模漂移和无效的规划决策，我们构建了切片的数字孪生（DTs）。在每个DT中，分别为相应切片中的空间分布开发了漂移自适应统计模型和仿真功能，分别有助于闭式决策和规划决策的有效验证。数值结果表明，与基准方案相比，所提出的漂移自适应切片式资源管理方案可以将服务满意度提高多达18%，并减少资源消耗多达13.1%。", "summary": "本文提出了一种针对合作ISAC网络的新型漂移自适应切片式资源管理方案。该方案通过建立感知和通信网络切片，并引入数字孪生来处理移动设备和感知目标的非平稳空间分布问题。数字孪生内部集成了漂移自适应统计模型和仿真功能，以优化资源规划和决策验证。数值结果显示，该方案显著提升了服务满意度并降低了资源消耗。", "keywords": "漂移自适应, 切片式资源管理, ISAC网络, 数字孪生, 非平稳分布", "comments": "这篇论文的创新点在于引入了数字孪生技术来应对ISAC网络中移动设备和感知目标的非平稳动态特性，通过漂移自适应模型实现了更鲁棒和高效的资源管理。这种将DTs应用于网络切片资源管理以处理动态不确定性的方法具有重要的实际意义。"}}
{"id": "2506.21073", "title": "Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G Networks", "authors": ["Ilias Papalamprou", "Nikolaos Fotos", "Nikolaos Chatzivasileiadis", "Anna Angelogianni", "Dimosthenis Masouros", "Dimitrios Soudris"], "summary": "The advent of 5G and beyond has brought increased performance networks,\nfacilitating the deployment of services closer to the user. To meet performance\nrequirements such services require specialized hardware, such as Field\nProgrammable Gate Arrays (FPGAs). However, FPGAs are often deployed in\nunprotected environments, leaving the user's applications vulnerable to\nmultiple attacks. With the rise of quantum computing, which threatens the\nintegrity of widely-used cryptographic algorithms, the need for a robust\nsecurity infrastructure is even more crucial. In this paper we introduce a\nhybrid hardware-software solution utilizing remote attestation to securely\nconfigure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms\nfor enhanced security. Additionally, to enable trustworthiness across the whole\nedge computing continuum, our solution integrates a blockchain infrastructure,\nensuring the secure storage of any security evidence. We evaluate the proposed\nsecure configuration process under different PQC algorithms in two FPGA\nfamilies, showcasing only 2% overheard compared to the non PQC approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21073v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21073v1", "AI": {"title_translation": "B5G网络中基于后量子和区块链的可信FPGA认证", "tldr": "本文提出了一种结合后量子密码学和区块链的混合硬件-软件解决方案，用于安全配置FPGA并确保其在B5G网络中的可信性，相较于非PQC方法仅增加2%的开销。", "motivation": "5G及未来网络的发展需要高性能硬件如FPGA来部署服务。然而，FPGA常部署在非保护环境中，易受攻击。量子计算的兴起对现有密码算法构成威胁，因此急需健壮的安全基础设施来保护用户应用。", "method": "本文提出了一种混合硬件-软件解决方案，利用远程认证来安全配置FPGA，并集成了后量子密码（PQC）算法以增强安全性。此外，为确保整个边缘计算连续体的可信度，解决方案还集成了区块链基础设施，以安全存储任何安全证据。", "result": "在两种FPGA系列中，使用不同的PQC算法评估了所提出的安全配置过程，结果显示与非PQC方法相比，开销仅为2%。", "conclusion": "本文提出的基于后量子和区块链的混合硬件-软件方案能够为B5G网络中的可信FPGA提供安全配置，且性能开销极低，有效应对了未来网络的安全挑战。", "translation": "5G及未来网络的出现带来了性能更高的网络，促进了服务更靠近用户的部署。为了满足性能要求，这些服务需要专用硬件，例如现场可编程门阵列（FPGA）。然而，FPGA通常部署在不受保护的环境中，导致用户的应用程序容易受到多种攻击。随着量子计算的兴起，它威胁到广泛使用的密码算法的完整性，对健壮安全基础设施的需求变得更加关键。在本文中，我们引入了一种混合硬件-软件解决方案，利用远程认证来安全配置FPGA，同时集成了后量子密码（PQC）算法以增强安全性。此外，为了在整个边缘计算连续体中实现可信度，我们的解决方案集成了区块链基础设施，确保安全存储任何安全证据。我们在两种FPGA系列中，使用不同的PQC算法评估了所提出的安全配置过程，结果显示与非PQC方法相比，开销仅为2%。", "summary": "本文提出了一种针对B5G网络中FPGA安全配置的混合硬件-软件解决方案。该方案通过远程认证技术，结合后量子密码算法增强安全性，并利用区块链基础设施确保安全证据的存储和整个边缘计算的可信度。实验结果表明，与非PQC方法相比，该方案的开销仅为2%。", "keywords": "后量子密码, 区块链, FPGA, 远程认证, B5G网络", "comments": "该论文的创新点在于将后量子密码学与区块链技术结合起来，为B5G网络中的FPGA提供了一个全面的安全配置和认证机制，有效应对了量子计算带来的潜在威胁。其低开销的性能表现也增强了方案的实用性。"}}
{"id": "2506.20763", "title": "A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion", "authors": ["Y. Navidtehrani", "C. Betegón", "E. Martínez-Pañeda"], "summary": "We present a novel, generalised formulation to treat coupled structural\nintegrity problems by combining phase field and multi-physics modelling. The\napproach exploits the versatility of the heat transfer equation and is\ntherefore well suited to be adopted in commercial finite element packages,\nrequiring only integration point-level implementation. This aspect is\ndemonstrated here by implementing coupled, multi-variable phenomena through\nsimple \\texttt{UMAT} and \\texttt{UMATHT} subroutines in the finite element\npackage \\texttt{Abaqus}. The generalised theoretical and computational\nframework presented is particularised to four problems of engineering and\nscientific relevance: thermo-mechanical fracture, hydraulic fracture,\nhydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are\nconsidered. The results reveal a very good agreement with experimental data,\nand existing numerical and analytical solutions.The user subroutines developed\nare made freely available at https://mechmat.web.ox.ac.uk/codes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20763v1", "categories": ["cs.CE", "cs.NA", "math.NA", "physics.app-ph"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20763v1", "AI": {"title_translation": "基于相场法的耦合问题广义建模框架：应用于热机械断裂、水力压裂、氢脆和腐蚀", "tldr": "本文提出了一种通用的相场多物理场耦合建模框架，用于处理结构完整性问题。该框架利用传热方程的通用性，易于在商业有限元软件中实现，并通过应用于热机械断裂、水力压裂、氢致开裂和金属腐蚀等问题，验证了其与实验数据和现有解决方案的良好一致性。", "motivation": "该研究旨在提出一种新颖的、广义的公式，通过结合相场和多物理场建模来处理耦合结构完整性问题，并使其易于在商业有限元软件包中实现。", "method": "研究提出了一种结合相场和多物理场建模的广义理论和计算框架，该方法利用传热方程的通用性，并通过在有限元软件包Abaqus中实现简单的UMAT和UMATHT子程序来处理耦合多变量现象。该框架被应用于热机械断裂、水力压裂、氢致开裂和金属腐蚀等问题。", "result": "结果显示，该框架与实验数据以及现有数值和解析解具有非常好的一致性。", "conclusion": "所提出的广义理论和计算框架能够有效且准确地模拟各种耦合结构完整性问题。", "translation": "我们提出了一种新颖的、广义的公式，通过结合相场和多物理场建模来处理耦合结构完整性问题。该方法利用了传热方程的通用性，因此非常适合在商业有限元软件包中采用，仅需要积分点级别的实现。本文通过在有限元软件包Abaqus中通过简单的UMAT和UMATHT子程序实现耦合的多变量现象来证明了这一点。所提出的广义理论和计算框架被专门应用于四个具有工程和科学相关性的问题：热机械断裂、水力压裂、氢致开裂和金属腐蚀。考虑了二维和三维问题。结果显示与实验数据以及现有数值和解析解具有非常好的一致性。所开发的用户子程序可在https://mechmat.web.ox.ac.uk/codes免费获取。", "summary": "本文提出了一种新颖的、基于相场的广义框架，用于模拟耦合结构完整性问题。该框架结合了相场和多物理场建模，并利用传热方程的通用性，使得在Abaqus等商业有限元软件中通过UMAT/UMATHT子程序实现变得容易。该框架被成功应用于热机械断裂、水力压裂、氢致开裂和金属腐蚀等问题，并显示出与实验数据和现有解决方案的高度一致性。", "keywords": "相场, 耦合问题, 断裂, 氢脆, 腐蚀", "comments": "该论文的创新之处在于提供了一个广义且易于实现的框架，利用现有有限元工具（如Abaqus）处理复杂的耦合问题。其多功能性和开放源代码的用户子程序具有重要意义，有助于推动相关领域的研究和应用。"}}
{"id": "2506.20702", "title": "The Singapore Consensus on Global AI Safety Research Priorities", "authors": ["Yoshua Bengio", "Tegan Maharaj", "Luke Ong", "Stuart Russell", "Dawn Song", "Max Tegmark", "Lan Xue", "Ya-Qin Zhang", "Stephen Casper", "Wan Sie Lee", "Sören Mindermann", "Vanessa Wilfred", "Vidhisha Balachandran", "Fazl Barez", "Michael Belinsky", "Imane Bello", "Malo Bourgon", "Mark Brakel", "Siméon Campos", "Duncan Cass-Beggs", "Jiahao Chen", "Rumman Chowdhury", "Kuan Chua Seah", "Jeff Clune", "Juntao Dai", "Agnes Delaborde", "Nouha Dziri", "Francisco Eiras", "Joshua Engels", "Jinyu Fan", "Adam Gleave", "Noah Goodman", "Fynn Heide", "Dan Hendrycks", "Cyrus Hodes", "Bryan Low Kian Hsiang", "Minlie Huang", "Sami Jawhar", "Wang Jingyu", "Adam Tauman Kalai", "Meindert Kamphuis", "Mohan Kankanhalli", "Subhash Kantamneni", "Mathias Bonde Kirk", "Thomas Kwa", "Jeffrey Ladish", "Kwok-Yan Lam", "Wan Lee Sie", "Taewhi Lee", "Xiaojian Li", "Jiajun Liu", "Chaochao Lu", "Yifan Mai", "Richard Mallah", "Julian Michael", "Nick Moës", "Simon Möller", "Kihyuk Nam", "Kwan Yee Ng", "Mark Nitzberg", "Besmira Nushi", "Seán O hÉigeartaigh", "Alejandro Ortega", "Pierre Peigné", "James Petrie", "Benjamin Prud'Homme", "Reihaneh Rabbany", "Nayat Sanchez-Pi", "Sarah Schwettmann", "Buck Shlegeris", "Saad Siddiqui", "Aradhana Sinha", "Martín Soto", "Cheston Tan", "Dong Ting", "Robert Trager", "Brian Tse", "Anthony Tung K. H.", "Vanessa Wilfred", "John Willes", "Denise Wong", "Wei Xu", "Rongwu Xu", "Yi Zeng", "HongJiang Zhang", "Djordje Žikelić"], "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).", "comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report", "pdf_url": "http://arxiv.org/pdf/2506.20702v1", "categories": ["cs.AI", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20702v1", "AI": {"title_translation": "全球AI安全研究重点的新加坡共识", "tldr": "AI能力快速提升，AI安全至关重要。2025年新加坡AI会议旨在汇集全球AI科学家，识别并综合AI安全研究重点，将挑战分为开发、评估和控制三类。", "motivation": "快速发展的AI能力带来巨大潜力，但也引发了关于如何确保AI安全（可信、可靠、安全）的激烈辩论。建立一个可信的AI生态系统至关重要，以促进AI的自信采纳和创新，同时避免负面影响。因此，需要支持AI安全领域的研究。", "method": "2025年新加坡AI会议（SCAI）通过汇集全球AI科学家，旨在识别和综合AI安全研究重点。该报告基于Yoshua Bengio主持并由33个政府支持的国际AI安全报告，并采用了深度防御模型，将AI安全研究领域组织成三类：创建可信AI系统面临的挑战（开发）、评估其风险面临的挑战（评估）、以及部署后监控和干预面临的挑战（控制）。", "result": "该报告组织了AI安全研究领域，将其挑战分为开发、评估和控制三类。", "conclusion": "该报告的结论是将AI安全研究领域划分为开发、评估和控制三大类挑战，以构建可信赖的AI生态系统。", "translation": "快速提升的AI能力和自主性带来了巨大的变革前景，但也引发了关于如何确保AI安全，即值得信赖、可靠和安全的激烈辩论。因此，建立一个值得信赖的生态系统至关重要——它有助于人们自信地拥抱AI，为创新提供最大的空间，同时避免反弹。\n“2025年新加坡AI会议（SCAI）：AI安全国际科学交流”旨在通过汇集全球AI科学家，识别和综合AI安全研究重点，从而支持该领域的研究。这份报告建立在由Yoshua Bengio主持并由33个政府支持的国际AI安全报告的基础上。通过采用深度防御模型，本报告将AI安全研究领域组织为三类：创建可信AI系统面临的挑战（开发）、评估其风险面临的挑战（评估）、以及部署后监控和干预面临的挑战（控制）。", "summary": "本文介绍了“2025年新加坡AI会议”的成果，该会议旨在汇集全球AI科学家，识别并综合AI安全研究重点。报告在国际AI安全报告的基础上，采用深度防御模型，将AI安全研究挑战分为三大类：开发可信AI系统、评估AI系统风险以及部署后监控和干预。", "keywords": "AI安全, 研究重点, 新加坡共识, 深度防御, 可信AI", "comments": "这篇报告的重要性在于它汇集了全球AI科学家的共识，并为AI安全研究提供了一个结构化的框架（开发、评估、控制）。它强调了在AI快速发展背景下，确保AI可信、可靠和安全的重要性，并为未来的研究指明了方向。其创新之处在于提出了一个“深度防御模型”来组织AI安全研究。"}}
{"id": "2506.20982", "title": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers", "authors": ["Martin Ruskov"], "summary": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings.", "comment": "accepted at D-SAIL Workshop - Transformative Curriculum Design:\n  Digitalization, Sustainability, and AI Literacy for 21st Century Learning", "pdf_url": "http://arxiv.org/pdf/2506.20982v1", "categories": ["cs.CY", "cs.RO", "K.3.1"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20982v1", "AI": {"title_translation": "我们的编程冒险：使用大型语言模型为学龄前儿童的实体编程机器人个性化叙事", "tldr": "该研究探索使用大型语言模型（LLMs）为学龄前儿童的实体编程机器人Cubetto生成个性化故事，作为教师辅助工具，避免儿童直接接触LLMs，并解决了内容一致性和幻觉问题。", "motivation": "在教育领域平衡使用大型语言模型（LLMs）面临挑战，尤其对于幼儿而言，存在对技术理解不足和屏幕时间过长的风险。本研究旨在探索一种利用LLMs能力的方法，为学龄前儿童提供个性化叙事，以帮助他们适应编程机器人操作，同时避免儿童直接接触LLMs。", "method": "研究团队与实体编程机器人Cubetto合作，提出了一种利用LLMs生成个性化故事的方法，以帮助学龄前儿童熟悉机器人指令。通过行动研究，开发了一个早期形式化的流程，用于快速原型化Cubetto的游戏故事。该方法使用开放权重模型，具有可复现性，并对模型具有普适性，因为它在5种不同的LLMs上进行了测试。研究记录了过程、所用材料、提示词、学习经验和成果。儿童不直接接触LLMs，而是教师使用该技术开发个性化叙事。", "result": "该方法生成的故事被认为成功地达到了作为教师辅助工具的目的。该方法具有可复现性，且对模型具有普适性（在5种不同LLMs上测试）。在4种不同的任务场景中测试模型时，研究人员遇到了内容一致性和“幻觉”问题，并记录了相应的评估过程以及克服这些问题的尝试（部分成功）。", "conclusion": "研究人员认为他们的方法适用于学龄前课堂，并计划在真实的教育环境中进行进一步的实验。", "translation": "在教育中平衡使用大型语言模型（LLMs）是一个挑战，因为存在对技术理解不足和受众易受影响的固有风险。对于年幼的儿童来说尤其如此，他们已知在屏幕时间过长方面存在困难。我们与一个名为Cubetto的实体编程机器人合作，提出了一种方法，通过在个性化故事的准备中使用LLMs来利用其能力，这对于学龄前儿童适应指挥机器人的实践是必要的。我们进行行动研究，开发了一个早期版本的规范化流程，用于快速原型化Cubetto的游戏故事。我们的方法既有可复现的结果，因为它使用了开放权重模型，又与模型无关，因为我们在5种不同的LLMs上对其进行了测试。我们一方面记录了过程、使用的材料和提示词，另一方面记录了学习体验和成果。我们认为生成对于将结果用作教师辅助的预期目的而言是成功的。在4种不同的任务场景中测试模型时，我们遇到了内容一致性和幻觉问题，并记录了相应的评估过程以及克服这些问题的尝试（有些成功，有些不成功）。重要的是，这个过程不会让儿童直接接触LLMs。相反，这项技术被用来帮助教师轻松地开发关于儿童喜欢主题的个性化叙事。我们相信我们的方法适用于学龄前课堂，我们计划在真实的教育环境中进一步进行实验。", "summary": "本研究提出了一种利用大型语言模型（LLMs）为学龄前儿童的实体编程机器人Cubetto生成个性化叙事的方法。旨在克服LLMs在儿童教育中应用的挑战，避免儿童直接接触LLMs，而是作为教师辅助工具。该方法通过行动研究开发，测试了多种LLMs，并记录了流程、材料和结果。尽管在一致性和“幻觉”方面存在挑战，但生成的叙事被认为是成功的教师辅助工具。研究认为该方法适用于学龄前教育，并计划进行实地实验。", "keywords": "大型语言模型, 实体编程机器人, 学龄前儿童, 个性化叙事, 教育技术", "comments": "该论文提出了一种新颖且负责任地将大型语言模型应用于学龄前教育的方法，通过为实体编程机器人提供个性化叙事，既利用了LLMs的能力，又避免了儿童直接暴露于潜在风险。其强调作为教师辅助工具而非直接面向儿童的策略，以及对模型普适性和可复现性的关注，都体现了严谨性。同时，论文也坦诚地指出了LLMs在内容一致性和“幻觉”方面存在的问题，并记录了克服这些问题的尝试，这对于LLMs在教育领域的实际应用具有重要的参考价值。"}}
{"id": "2506.20673", "title": "ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data", "authors": ["Yongqian Sun", "Xijie Pan", "Xiao Xiong", "Lei Tao", "Jiaju Wang", "Shenglin Zhang", "Yuan Yuan", "Yuqi Li", "Kunlin Jian"], "summary": "Network failure diagnosis is challenging yet critical for high-performance\ncomputing (HPC) systems. Existing methods cannot be directly applied to HPC\nscenarios due to data heterogeneity and lack of accuracy. This paper proposes a\nnovel framework, called ClusterRCA, to localize culprit nodes and determine\nfailure types by leveraging multimodal data. ClusterRCA extracts features from\ntopologically connected network interface controller (NIC) pairs to analyze the\ndiverse, multimodal data in HPC systems. To accurately localize culprit nodes\nand determine failure types, ClusterRCA combines classifier-based and\ngraph-based approaches. A failure graph is constructed based on the output of\nthe state classifier, and then it performs a customized random walk on the\ngraph to localize the root cause. Experiments on datasets collected by a\ntop-tier global HPC device vendor show ClusterRCA achieves high accuracy in\ndiagnosing network failure for HPC systems. ClusterRCA also maintains robust\nperformance across different application scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20673v1", "categories": ["cs.DC", "cs.AI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20673v1", "AI": {"title_translation": "ClusterRCA：使用多模态数据进行HPC系统网络故障诊断", "tldr": "ClusterRCA是一个用于HPC系统网络故障诊断的新框架，它利用多模态数据，结合分类器和图方法，以高精度定位故障节点并确定故障类型。", "motivation": "网络故障诊断对于高性能计算（HPC）系统来说具有挑战性但至关重要。现有方法由于数据异构性和缺乏准确性，无法直接应用于HPC场景。", "method": "本文提出了一个名为ClusterRCA的新颖框架。它通过从拓扑连接的网络接口控制器（NIC）对中提取特征来分析HPC系统中的多样化多模态数据。为了准确地定位罪魁祸首节点并确定故障类型，ClusterRCA结合了基于分类器和基于图的方法。它根据状态分类器的输出构建一个故障图，然后在该图上执行定制的随机游走以定位根本原因。", "result": "在由顶级全球HPC设备供应商收集的数据集上进行的实验表明，ClusterRCA在诊断HPC系统网络故障方面实现了高精度。ClusterRCA在不同的应用场景中也保持了稳健的性能。", "conclusion": "ClusterRCA是一种有效且准确的网络故障诊断框架，专门为HPC系统设计，能够处理多模态数据并保持在不同场景下的鲁棒性。", "translation": "网络故障诊断对于高性能计算（HPC）系统来说具有挑战性但至关重要。现有方法由于数据异构性和缺乏准确性，无法直接应用于HPC场景。本文提出了一个名为ClusterRCA的新颖框架，通过利用多模态数据来定位罪魁祸首节点并确定故障类型。ClusterRCA从拓扑连接的网络接口控制器（NIC）对中提取特征，以分析HPC系统中多样化的多模态数据。为了准确地定位罪魁祸首节点并确定故障类型，ClusterRCA结合了基于分类器和基于图的方法。它根据状态分类器的输出构建一个故障图，然后在该图上执行定制的随机游走以定位根本原因。在由顶级全球HPC设备供应商收集的数据集上进行的实验表明，ClusterRCA在诊断HPC系统网络故障方面实现了高精度。ClusterRCA在不同的应用场景中也保持了稳健的性能。", "summary": "ClusterRCA是一个专门为高性能计算（HPC）系统设计的网络故障诊断框架。针对现有方法在HPC场景中因数据异构性和准确性不足而无法直接应用的问题，ClusterRCA利用多模态数据，通过从网络接口控制器（NIC）对中提取特征，并结合基于分类器和基于图的方法来定位故障节点并确定故障类型。实验证明，该框架在诊断HPC系统网络故障方面具有高精度，并在不同应用场景下表现出稳健的性能。", "keywords": "HPC系统, 网络故障诊断, 多模态数据, 根本原因分析, 图算法", "comments": "ClusterRCA的创新之处在于其专门针对HPC系统中的多模态数据异构性问题，并结合了分类器和图方法进行故障诊断。这种结合使得它能够更准确地定位根本原因。其重要性体现在解决了HPC系统网络故障诊断的痛点，提高了诊断精度和鲁棒性。"}}
{"id": "2506.20741", "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20741v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20741v1", "AI": {"title_translation": "OTSurv：一种用于生存预测的异质性感知最优传输多示例学习框架", "tldr": "OTSurv是一种新的MIL框架，它通过异质性感知最优传输来提高生存预测的准确性，解决了现有方法未能捕捉病理异质性的问题，并在六个基准测试中取得了SOTA结果。", "motivation": "现有的多示例学习（MIL）方法在利用全玻片图像（WSI）进行生存预测时，未能明确捕捉WSI内的病理异质性，包括全局的长尾形态分布和局部的切片级预测不确定性。", "method": "本文提出OTSurv，一个基于最优传输（OT）的新型多示例学习（MIL）框架。它将生存预测公式化为一个异质性感知的OT问题，并带有两个约束：1）全局长尾约束，通过调节传输质量分配来建模先验形态分布，从而避免模式崩溃和过度均匀性；2）局部不确定性感知约束，通过逐步增加总传输质量来优先处理高置信度补丁并抑制噪声。该问题被重铸为非平衡OT公式，并通过高效、硬件友好的矩阵缩放算法求解。", "result": "OTSurv在六个流行的基准测试中取得了新的最先进结果，平均C-index绝对提高了3.6%。此外，OTSurv在对数秩检验中达到了统计显著性，并提供了高可解释性。", "conclusion": "OTSurv是一个强大的数字病理学生存预测工具，它通过有效捕捉病理异质性，提高了预测准确性和可解释性。", "translation": "使用全玻片图像（WSI）进行生存预测可以被视为一个多示例学习（MIL）问题。然而，现有的MIL方法往往未能明确捕捉WSI内部的病理异质性，无论是全局的——通过长尾形态分布，还是局部的——通过切片级预测不确定性。最优传输（OT）通过整合边际分布约束，提供了一种建模这种异质性的原则性方法。基于这一见解，我们提出了OTSurv，一个从最优传输角度出发的新型MIL框架。具体来说，OTSurv将生存预测公式化为一个异质性感知的OT问题，并带有两个约束：（1）全局长尾约束，通过调节传输质量分配来建模先验形态分布，从而避免模式崩溃和过度均匀性，以及（2）局部不确定性感知约束，通过逐步增加总传输质量来优先处理高置信度补丁并抑制噪声。然后，我们将最初的OT问题（通过这些约束增强）重铸为非平衡OT公式，该公式可以通过高效、硬件友好的矩阵缩放算法求解。经验上，OTSurv在六个流行的基准测试中取得了新的最先进结果，平均C-index绝对提高了3.6%。此外，OTSurv在对数秩检验中达到了统计显著性，并提供了高可解释性，使其成为数字病理学中生存预测的强大工具。我们的代码可在 https://github.com/Y-Research-SBU/OTSurv 获取。", "summary": "本文提出OTSurv，一个基于最优传输（OT）的新型多示例学习（MIL）框架，用于通过全玻片图像（WSI）进行生存预测。针对现有MIL方法未能有效捕捉WSI内病理异质性的问题，OTSurv引入了全局长尾约束和局部不确定性感知约束，将生存预测建模为异质性感知的OT问题。该方法通过高效算法求解，并在六个基准测试中实现了SOTA性能，平均C-index提升3.6%，同时具有高可解释性。", "keywords": "多示例学习, 生存预测, 最优传输, 病理异质性, 全玻片图像", "comments": "OTSurv的创新之处在于将最优传输理论引入到多示例学习框架中，以显式地处理全玻片图像中的病理异质性，这对于提高生存预测的准确性和模型鲁棒性至关重要。其通过全局长尾和局部不确定性感知约束来优化传输过程，有效解决了模式崩溃和噪声问题，并实现了显著的性能提升。该方法为数字病理学中的WSI分析提供了一个强大且可解释的工具。"}}
{"id": "2506.20685", "title": "Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan", "Naima Iltaf", "Ihtesham ul Islam"], "summary": "Federated Learning (FL) has emerged as a transformative paradigm for\ndistributed machine learning while preserving data privacy. However, existing\napproaches predominantly focus on model heterogeneity and aggregation\ntechniques, largely overlooking the fundamental impact of dataset size\ncharacteristics on federated training dynamics. This paper introduces\nSize-Based Adaptive Federated Learning (SAFL), a novel progressive training\nframework that systematically organizes federated learning based on dataset\nsize characteristics across heterogeneous multi-modal data. Our comprehensive\nexperimental evaluation across 13 diverse datasets spanning 7 modalities\n(vision, text, time series, audio, sensor, medical vision, and multimodal)\nreveals critical insights: 1) an optimal dataset size range of 1000-1500\nsamples for federated learning effectiveness; 2) a clear modality performance\nhierarchy with structured data (time series, sensor) significantly\noutperforming unstructured data (text, multimodal); and 3) systematic\nperformance degradation for large datasets exceeding 2000 samples. SAFL\nachieves an average accuracy of 87.68% across all datasets, with structured\ndata modalities reaching 99%+ accuracy. The framework demonstrates superior\ncommunication efficiency, reducing total data transfer to 7.38 GB across 558\ncommunications while maintaining high performance. Our real-time monitoring\nframework provides unprecedented insights into system resource utilization,\nnetwork efficiency, and training dynamics. This work fills critical gaps in\nunderstanding how data characteristics should drive federated learning\nstrategies, providing both theoretical insights and practical guidance for\nreal-world FL deployments in neural network and learning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20685v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20685v1", "AI": {"title_translation": "渐进式尺寸自适应联邦学习：异构多模态数据系统的综合框架", "tldr": "本文提出了SAFL，一个渐进式训练框架，根据数据集大小组织联邦学习，并在13个数据集上展示了数据大小和模态对联邦学习性能的关键影响，SAFL在准确性、通信效率和资源利用方面表现出色。", "motivation": "现有联邦学习方法主要关注模型异构性和聚合技术，但忽视了数据集大小特性对联邦训练动态的根本影响，这导致了理解数据特性如何驱动联邦学习策略方面的关键空白。", "method": "本文提出了尺寸自适应联邦学习（SAFL），一个新颖的渐进式训练框架，系统地根据异构多模态数据的数据集大小特性来组织联邦学习。该框架通过对13个不同数据集（涵盖7种模态）进行全面实验评估。", "result": "1. 联邦学习有效性的最佳数据集大小范围为1000-1500个样本。\n2. 存在明显的模态性能等级：结构化数据（时间序列、传感器）显著优于非结构化数据（文本、多模态）。\n3. 超过2000个样本的大数据集性能系统性下降。\n4. SAFL在所有数据集上平均准确率达到87.68%，结构化数据模态达到99%+准确率。\n5. 框架展现出卓越的通信效率，在558次通信中将总数据传输量减少到7.38 GB。\n6. 实时监控框架提供了对系统资源利用、网络效率和训练动态的深入洞察。", "conclusion": "本研究填补了理解数据特性如何驱动联邦学习策略方面的关键空白，为真实世界的联邦学习部署提供了理论见解和实践指导。", "translation": "联邦学习（FL）已成为一种在保留数据隐私的同时实现分布式机器学习的变革性范式。然而，现有方法主要侧重于模型异构性和聚合技术，在很大程度上忽视了数据集大小特性对联邦训练动态的根本影响。本文引入了基于大小的自适应联邦学习（SAFL），这是一种新颖的渐进式训练框架，它根据异构多模态数据的数据集大小特性系统地组织联邦学习。我们对涵盖7种模态（视觉、文本、时间序列、音频、传感器、医学视觉和多模态）的13个不同数据集进行的全面实验评估揭示了关键见解：1）联邦学习有效性的最佳数据集大小范围为1000-1500个样本；2）存在明显的模态性能等级，结构化数据（时间序列、传感器）显著优于非结构化数据（文本、多模态）；3）超过2000个样本的大数据集性能系统性下降。SAFL在所有数据集上的平均准确率达到87.68%，其中结构化数据模态达到99%+的准确率。该框架展示了卓越的通信效率，在558次通信中将总数据传输量减少到7.38 GB，同时保持了高性能。我们的实时监控框架提供了对系统资源利用、网络效率和训练动态前所未有的洞察。这项工作填补了理解数据特性如何驱动联邦学习策略方面的关键空白，为神经网络和学习系统中的真实世界联邦学习部署提供了理论见解和实践指导。", "summary": "本文提出了尺寸自适应联邦学习（SAFL），一个新颖的渐进式训练框架，旨在解决现有联邦学习方法忽视数据集大小影响的问题。SAFL根据数据集大小特性组织联邦学习，并在13个多模态数据集上进行了全面评估。研究发现，联邦学习的最佳数据集大小范围为1000-1500样本，结构化数据性能优于非结构化数据，且大数据集性能会下降。SAFL在准确性、通信效率和资源监控方面表现出色，为理解数据特性如何影响联邦学习策略提供了理论和实践指导。", "keywords": "联邦学习, 数据集大小, 多模态数据, 尺寸自适应, 异构系统", "comments": "该论文的创新点在于首次系统性地探究了数据集大小和模态对联邦学习性能的影响，并提出了尺寸自适应联邦学习（SAFL）框架来优化训练过程。其重要性体现在填补了联邦学习领域对数据特性驱动策略的理解空白，并为实际部署提供了具体的数据大小和模态选择指导。SAFL在多模态数据上的广泛验证及其在通信效率和性能上的提升，使其成为联邦学习发展中的一个重要贡献。"}}
{"id": "2506.20813", "title": "Entropic additive energy and entropy inequalities for sums and products", "authors": ["Rupert Li", "Lampros Gavalakis", "Ioannis Kontoyiannis"], "summary": "Following a growing number of studies that, over the past 15 years, have\nestablished entropy inequalities via ideas and tools from additive\ncombinatorics, in this work we obtain a number of new bounds for the\ndifferential entropy of sums, products, and sum-product combinations of\ncontinuous random variables. Partly motivated by recent work by Goh on the\ndiscrete entropic version of the notion of \"additive energy\", we introduce the\nadditive energy of pairs of continuous random variables and prove various\nversions of the statement that \"the additive energy is large if and only if the\nentropy of the sum is small\", along with a version of the\nBalog-Szemer\\'edi-Gowers theorem for differential entropy. Then, motivated in\npart by recent work by M\\'ath\\'e and O'Regan, we establish a series of new\ndifferential entropy inequalities for products and sum-product combinations of\ncontinuous random variables. In particular, we prove a new, general, ring\nPl\\\"unnecke-Ruzsa entropy inequality. We briefly return to the case of discrete\nentropy and provide a characterization of discrete random variables with \"large\ndoubling\", analogous to Tao's Freiman-type inverse sumset theory for the case\nof small doubling. Finally, we consider the natural entropic analog of the\nErd\\\"os-Szemer\\'edi sum-product phenomenon for integer-valued random variables.\nWe show that, if it does hold, then the range of parameters for which it does\nwould necessarily be significantly more restricted than its anticipated\ncombinatorial counterpart.", "comment": "26 pages, no figures", "pdf_url": "http://arxiv.org/pdf/2506.20813v1", "categories": ["cs.IT", "math.CO", "math.IT", "94A17 (Primary) 11B13 (Secondary)"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20813v1", "AI": {"title_translation": "熵加性能量与和积的熵不等式", "tldr": "本文在加性组合学和熵理论交叉领域取得了进展，为连续随机变量的和、积、和积组合的微分熵建立了新的界限，引入了连续随机变量对的加性能量概念并证明其与和熵的关系，推广了Balog-Szemerédi-Gowers定理，提出了新的环Plünnecke-Ruzsa熵不等式，并探讨了离散熵及Erdős-Szemerédi和积现象的熵类比。", "motivation": "本研究受到过去15年通过加性组合学思想和工具建立熵不等式的研究日益增多的趋势启发。具体而言，部分受到Goh关于离散熵版本“加性能量”近期工作的启发，引入了连续随机变量对的加性能量概念。此外，部分受到Máthé和O'Regan近期工作的启发，旨在建立一系列新的连续随机变量的积与和积组合的微分熵不等式。", "method": "本研究引入了连续随机变量对的加性能量概念，并证明了“加性能量大当且仅当和的熵小”的各种版本。同时，证明了微分熵的Balog-Szemerédi-Gowers定理的一个版本。此外，建立了一系列新的连续随机变量的积与和积组合的微分熵不等式，特别是证明了一个新的、通用的环Plünnecke-Ruzsa熵不等式。文章还探讨了离散熵下具有“大倍增”离散随机变量的刻画，并考虑了整数值随机变量的Erdős-Szemerédi和积现象的自然熵类比。", "result": "本工作获得了连续随机变量的和、积以及和积组合的微分熵的许多新界。证明了“加性能量大当且仅当和的熵小”的各种版本，以及微分熵的Balog-Szemerédi-Gowers定理的一个版本。证明了一个新的、通用的环Plünnecke-Ruzsa熵不等式。提供了离散熵下具有“大倍增”的离散随机变量的刻画。此外，研究表明，如果Erdős-Szemerédi和积现象的熵类比成立，则其参数范围将必然比其预期的组合对应物受到显著更多的限制。", "conclusion": "本文在连续随机变量的熵不等式方面取得了多项新进展，通过引入加性能量概念并建立其与熵的关系，成功将加性组合学中的关键定理推广到微分熵领域。研究还对离散熵和和积现象的熵类比进行了深入探讨，为理解熵与结构之间的关系提供了新的视角和工具。", "translation": "遵循过去15年通过加性组合学思想和工具建立熵不等式的研究日益增多的趋势，本工作获得了连续随机变量的和、积以及和积组合的微分熵的许多新界。部分受Goh关于“加性能量”离散熵版本的近期工作的启发，我们引入了连续随机变量对的加性能量，并证明了“加性能量大当且仅当和的熵小”的各种版本，以及微分熵的Balog-Szemerédi-Gowers定理的一个版本。然后，部分受Máthé和O'Regan近期工作的启发，我们建立了一系列新的连续随机变量的积与和积组合的微分熵不等式。特别是，我们证明了一个新的、通用的环Plünnecke-Ruzsa熵不等式。我们简要地回到离散熵的情况，并提供了具有“大倍增”的离散随机变量的刻画，类似于Tao在小倍增情况下的Freiman型逆和集理论。最后，我们考虑了整数值随机变量的Erdős-Szemerédi和积现象的自然熵类比。我们表明，如果它确实成立，那么它成立的参数范围将必然比其预期的组合对应物受到显著更多的限制。", "summary": "本文在加性组合学和熵理论的交叉领域取得了进展，为连续随机变量的和、积及和积组合的微分熵建立了新的界限。研究引入了连续随机变量对的加性能量概念，并证明了其与和熵之间的关系，同时将Balog-Szemerédi-Gowers定理推广至微分熵。此外，文章还提出了新的环Plünnecke-Ruzsa熵不等式，并探讨了离散熵中“大倍增”随机变量的刻画，以及Erdős-Szemerédi和积现象的熵类比。", "keywords": "熵不等式, 加性能量, 微分熵, 和积现象, 加性组合学", "comments": "这篇论文在熵理论和加性组合学之间架起了桥梁，将组合学中的重要概念和定理（如加性能量、Balog-Szemerédi-Gowers定理、Plünnecke-Ruzsa不等式、Erdős-Szemerédi和积现象）推广到微分熵和离散熵的语境中。其创新性在于为连续随机变量提供了新的熵不等式，并对离散熵的“大倍增”情况进行了刻画，深化了对熵与结构之间关系的理解。"}}
{"id": "2506.20677", "title": "Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions", "authors": ["Shrinivass Arunachalam Balasubramanian"], "summary": "Sorting is an essential operation in computer science with direct\nconsequences on the performance of large scale data systems, real-time systems,\nand embedded computation. However, no sorting algorithm is optimal under all\ndistributions of data. The new adaptive hybrid sorting paradigm proposed in\nthis paper is the paradigm that automatically selects the most effective\nsorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time\nmonitoring of patterns in input data. The architecture begins by having a\nfeature extraction module to compute significant parameters such as data\nvolume, value range and entropy. These parameters are sent to a decision engine\ninvolving Finite State Machine and XGBoost classifier to aid smart and\neffective in choosing the optimal sorting strategy. It implements Counting Sort\non small key ranges, Radix Sort on large range structured input with\nlow-entropy keys and QuickSort on general purpose sorting. The experimental\nfindings of both synthetic and real life dataset confirm that the proposed\nsolution is actually inclined to excel significantly by comparison in execution\ntime, flexibility and the efficiency of conventional static sorting algorithms.\nThe proposed framework provides a scalable, high perhaps and applicable to a\nwide range of data processing operations like big data analytics, edge\ncomputing, and systems with hardware limitations.", "comment": "11 Pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20677v1", "categories": ["cs.DS", "cs.DB", "cs.PF"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20677v1", "AI": {"title_translation": "自适应混合排序：针对不同数据分布的动态策略选择以实现最优排序", "tldr": "本文提出了一种自适应混合排序范式，通过实时监测输入数据模式，自动选择最有效的排序算法（计数排序、基数排序或快速排序），以在执行时间、灵活性和效率方面显著优于传统静态排序算法。", "motivation": "传统的排序算法无法在所有数据分布下都达到最优性能，这影响了大规模数据系统、实时系统和嵌入式计算的性能。", "method": "该方法首先通过特征提取模块计算数据量、值范围和熵等关键参数。然后，这些参数被发送到一个决策引擎，该引擎结合有限状态机和XGBoost分类器来智能选择最优排序策略。具体实现为：在小键范围上使用计数排序，在大范围、低熵键结构化输入上使用基数排序，在通用排序上使用快速排序。", "result": "在合成数据集和真实数据集上的实验结果表明，所提出的解决方案在执行时间、灵活性和效率方面显著优于传统的静态排序算法。", "conclusion": "所提出的自适应混合排序框架提供了一个可扩展、高性能且适用于大数据分析、边缘计算和硬件受限系统等广泛数据处理操作的解决方案。", "translation": "排序是计算机科学中一项基本操作，直接影响大规模数据系统、实时系统和嵌入式计算的性能。然而，没有一种排序算法能在所有数据分布下都达到最优。本文提出了一种新的自适应混合排序范式，该范式通过实时监测输入数据模式，自动选择最有效的排序算法，包括计数排序、基数排序或快速排序。该架构首先通过一个特征提取模块计算数据量、值范围和熵等重要参数。这些参数被发送到一个决策引擎，该引擎结合有限状态机和XGBoost分类器，以智能有效地选择最优排序策略。它在小键范围上实现计数排序，在大范围、低熵键结构化输入上实现基数排序，并在通用排序上实现快速排序。在合成数据集和真实数据集上的实验结果证实，所提出的解决方案在执行时间、灵活性和效率方面显著优于传统的静态排序算法。所提出的框架是可扩展、高性能的，并适用于广泛的数据处理操作，如大数据分析、边缘计算和具有硬件限制的系统。", "summary": "本文提出了一种自适应混合排序范式，旨在解决传统排序算法在不同数据分布下性能不一的问题。该范式通过特征提取模块（计算数据量、值范围、熵）和决策引擎（结合有限状态机和XGBoost分类器），实时监测输入数据模式，并动态选择最适合的排序算法（计数排序、基数排序或快速排序）。实验结果表明，该方法在执行时间、灵活性和效率上显著优于传统静态排序算法，为大数据分析、边缘计算等领域提供了可扩展、高性能的解决方案。", "keywords": "自适应排序, 混合排序, 动态策略选择, 数据分布, XGBoost", "comments": "该论文的创新点在于提出了一个动态自适应的混合排序框架，通过机器学习（XGBoost）和状态机来智能选择最佳排序策略，有效解决了单一排序算法无法适应多样化数据分布的挑战。其重要性在于提升了排序操作在实际应用中的效率和灵活性，特别是在大数据和资源受限环境中具有潜在价值。"}}
{"id": "2506.20915", "title": "ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models", "authors": ["Mina Namazi", "Alexander Nemecek", "Erman Ayday"], "summary": "As the deployment of large language models (LLMs) grows in sensitive domains,\nensuring the integrity of their computational provenance becomes a critical\nchallenge, particularly in regulated sectors such as healthcare, where strict\nrequirements are applied in dataset usage. We introduce ZKPROV, a novel\ncryptographic framework that enables zero-knowledge proofs of LLM provenance.\nIt allows users to verify that a model is trained on a reliable dataset without\nrevealing sensitive information about it or its parameters. Unlike prior\napproaches that focus on complete verification of the training process\n(incurring significant computational cost) or depend on trusted execution\nenvironments, ZKPROV offers a distinct balance. Our method cryptographically\nbinds a trained model to its authorized training dataset(s) through\nzero-knowledge proofs while avoiding proof of every training step. By\nleveraging dataset-signed metadata and compact model parameter commitments,\nZKPROV provides sound and privacy-preserving assurances that the result of the\nLLM is derived from a model trained on the claimed authorized and relevant\ndataset. Experimental results demonstrate the efficiency and scalability of the\nZKPROV in generating this proof and verifying it, achieving a practical\nsolution for real-world deployments. We also provide formal security\nguarantees, proving that our approach preserves dataset confidentiality while\nensuring trustworthy dataset provenance.", "comment": "12 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.20915v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20915v1", "AI": {"title_translation": "ZKPROV: 大语言模型数据集溯源的零知识方法", "tldr": "ZKPROV使用零知识证明验证LLM的数据集来源，无需泄露敏感信息，比现有方法更高效且实用。", "motivation": "随着大型语言模型（LLM）在敏感领域（如医疗保健）的部署日益增长，确保其计算溯源的完整性成为一项关键挑战，特别是在对数据集使用有严格要求的受监管行业。", "method": "ZKPROV是一个新颖的密码学框架，它通过零知识证明实现LLM溯源。该方法通过数据集签名的元数据和紧凑的模型参数承诺，将训练好的模型与其授权训练数据集进行密码学绑定，同时避免证明每个训练步骤，从而允许用户在不泄露敏感信息的情况下验证模型是否在可靠数据集上训练。", "result": "实验结果表明ZKPROV在生成和验证证明方面具有效率和可扩展性，为实际部署提供了实用解决方案。论文还提供了形式化的安全保障，证明该方法在确保可信数据集溯源的同时保留了数据集机密性。", "conclusion": "ZKPROV提供了一种实用、高效且隐私保护的零知识方法来验证LLM的数据集溯源，解决了在敏感领域部署LLM时的关键完整性挑战。", "translation": "随着大型语言模型（LLM）在敏感领域的部署日益增长，确保其计算溯源的完整性成为一项关键挑战，特别是在医疗保健等受监管行业，这些行业对数据集的使用有严格要求。我们引入了ZKPROV，一个新颖的密码学框架，能够实现LLM溯源的零知识证明。它允许用户验证模型是否在可靠数据集上训练，而无需泄露有关数据集或其参数的敏感信息。与之前侧重于训练过程完整验证（导致显著的计算成本）或依赖可信执行环境的方法不同，ZKPROV提供了一种独特的平衡。我们的方法通过零知识证明，将训练好的模型与其授权训练数据集进行密码学绑定，同时避免证明每个训练步骤。通过利用数据集签名的元数据和紧凑的模型参数承诺，ZKPROV提供了可靠且隐私保护的保证，即LLM的结果源自于在声明的授权和相关数据集上训练的模型。实验结果表明ZKPROV在生成和验证此证明方面的效率和可扩展性，为实际部署提供了实用解决方案。我们还提供了形式化的安全保障，证明我们的方法在确保可信数据集溯源的同时保留了数据集机密性。", "summary": "ZKPROV是一种新颖的零知识密码学框架，旨在解决大型语言模型在敏感领域部署时的数据集溯源完整性问题。它允许用户在不泄露敏感数据集或模型参数信息的情况下，验证LLM是否在授权且可靠的数据集上训练。通过密码学绑定模型与数据集，并避免对每个训练步骤进行证明，ZKPROV在效率和隐私保护之间取得了平衡。实验证明其在实际应用中的高效性和可扩展性，并提供了形式化的安全保障，确保了数据集的机密性和溯源的可靠性。", "keywords": "零知识证明, 数据集溯源, 大型语言模型, 隐私保护, 计算完整性", "comments": "ZKPROV的创新之处在于其在LLM溯源验证中引入了零知识证明，有效解决了隐私保护和计算成本之间的矛盾。它避免了传统方法中对整个训练过程的昂贵验证，也摆脱了对可信执行环境的依赖，提供了一个更实用且可扩展的解决方案。这对于LLM在医疗、金融等高度敏感且受监管领域的应用具有重要意义，因为它能在不牺牲数据隐私的前提下，增强模型的透明度和可信度。"}}
{"id": "2506.20759", "title": "Agile Management for Machine Learning: A Systematic Mapping Study", "authors": ["Lucas Romao", "Hugo Villamizar", "Romeu Oliveira", "Silvio Alonso", "Marcos Kalinowski"], "summary": "[Context] Machine learning (ML)-enabled systems are present in our society,\ndriving significant digital transformations. The dynamic nature of ML\ndevelopment, characterized by experimental cycles and rapid changes in data,\nposes challenges to traditional project management. Agile methods, with their\nflexibility and incremental delivery, seem well-suited to address this\ndynamism. However, it is unclear how to effectively apply these methods in the\ncontext of ML-enabled systems, where challenges require tailored approaches.\n[Goal] Our goal is to outline the state of the art in agile management for\nML-enabled systems. [Method] We conducted a systematic mapping study using a\nhybrid search strategy that combines database searches with backward and\nforward snowballing iterations. [Results] Our study identified 27 papers\npublished between 2008 and 2024. From these, we identified eight frameworks and\ncategorized recommendations and practices into eight key themes, such as\nIteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable\nModel. The main challenge identified across studies was accurate effort\nestimation for ML-related tasks. [Conclusion] This study contributes by mapping\nthe state of the art and identifying open gaps in the field. While relevant\nwork exists, more robust empirical evaluation is still needed to validate these\ncontributions.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20759v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20759v1", "AI": {"title_translation": "机器学习的敏捷管理：一项系统性映射研究", "tldr": "本研究对机器学习系统敏捷管理的现状进行了系统性映射，识别了现有框架、实践和主要挑战，并指出仍需更多实证评估。", "motivation": "机器学习（ML）驱动的系统日益普及，但其开发过程的动态性（实验周期、数据快速变化）对传统项目管理构成挑战。敏捷方法看似适用，但如何在ML系统中有效应用尚不明确，需要量身定制的方法。本文旨在概述机器学习系统敏捷管理的现状。", "method": "我们采用混合搜索策略进行了一项系统性映射研究，结合了数据库搜索以及向前和向后滚雪球迭代。", "result": "研究识别了2008年至2024年间发表的27篇论文，从中归纳出8个框架，并将推荐和实践分为8个关键主题，例如迭代灵活性、创新的ML特定工件和最小可行模型。研究中发现的主要挑战是ML相关任务的准确工作量估算。", "conclusion": "本研究通过绘制该领域的现状图并识别开放性差距做出了贡献。尽管存在相关工作，但仍需要更稳健的实证评估来验证这些贡献。", "translation": "[背景] 机器学习（ML）驱动的系统已遍布我们的社会，推动着重大的数字化转型。ML开发的动态性，以实验周期和数据快速变化为特征，对传统项目管理提出了挑战。敏捷方法凭借其灵活性和增量交付，似乎非常适合应对这种动态性。然而，目前尚不清楚如何在ML驱动系统的背景下有效应用这些方法，因为这些挑战需要量身定制的方法。\n[目标] 我们的目标是概述ML驱动系统敏捷管理的现状。\n[方法] 我们采用混合搜索策略进行了一项系统性映射研究，该策略结合了数据库搜索以及向前和向后滚雪球迭代。\n[结果] 我们的研究识别了2008年至2024年间发表的27篇论文。从中，我们识别了八个框架，并将推荐和实践分为八个关键主题，例如迭代灵活性、创新的ML特定工件和最小可行模型。研究中发现的主要挑战是ML相关任务的准确工作量估算。\n[结论] 本研究通过绘制该领域的现状图并识别开放性差距做出了贡献。尽管存在相关工作，但仍需要更稳健的实证评估来验证这些贡献。", "summary": "本系统性映射研究旨在概述机器学习（ML）系统敏捷管理的现状。通过对2008-2024年间27篇论文的分析，研究识别了8个现有框架，并将相关实践和推荐归纳为8个关键主题，如迭代灵活性和最小可行模型。研究强调了ML任务估算准确性是主要挑战，并指出尽管已有相关工作，但仍需更强的实证评估来验证现有贡献。", "keywords": "敏捷管理, 机器学习, 系统性映射研究, 项目管理, 软件开发", "comments": "这项研究通过系统性映射为机器学习项目管理领域提供了宝贵的概览，特别是在敏捷方法应用方面。其创新之处在于识别并分类了ML特有的敏捷实践和工件。重要性体现在揭示了该领域的主要挑战（如工作量估算）和未来研究方向（如实证验证）。"}}
{"id": "2506.20884", "title": "\"TikTok, Do Your Thing\": User Reactions to Social Surveillance in the Public Sphere", "authors": ["Meira Gilbert", "Miranda Wei", "Lindah Kotut"], "summary": "''TikTok, Do Your Thing'' is a viral trend where users attempt to identify\nstrangers they see in public via information crowd-sourcing. The trend started\nas early as 2021 and users typically engage with it for romantic purposes\n(similar to a ''Missed Connections'' personal advertisement). This practice\nincludes acts of surveillance and identification in the public sphere, although\nby peers rather than governments or corporations. To understand users'\nreactions to this trend we conducted a qualitative analysis of 60 TikTok videos\nand 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were\nsuccessfully identified. We also find that while there were comments expressing\ndisapproval (n=310), more than double the number expressed support (n=883).\nSupportive comments demonstrated genuine interest and empathy, reflecting\nevolving conceptions of community and algorithmic engagement. On the other\nhand, disapproving comments highlighted concerns about inappropriate\nrelationships, stalking, consent, and gendered double standards. We discuss\nthese insights in relation to the normalization of interpersonal surveillance,\nonline stalking, and as an evolution of social surveillance to offer a new\nperspective on user perceptions surrounding interpersonal surveillance and\nidentification in the public sphere.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20884v1", "categories": ["cs.HC", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20884v1", "AI": {"title_translation": "“TikTok，做你的事”：用户对公共领域社会监控的反应", "tldr": "该研究通过对TikTok视频和评论的定性分析，探讨了用户对“TikTok，做你的事”这一通过众包识别陌生人的趋势的反应，发现支持者多于反对者，并讨论了人际监控的常态化。", "motivation": "为了理解用户对“TikTok，做你的事”（一种通过众包识别公共领域陌生人的病毒式趋势）的反应。", "method": "对60个TikTok视频和1,901条用户评论进行了定性分析。", "result": "在审查的60个视频中，成功识别了19个人。发现表达反对的评论有310条，而表达支持的评论数量是其两倍多（883条）。支持性评论表现出真正的兴趣和同情，反映了社区和算法参与观念的演变。反对性评论则强调了对不当关系、跟踪、同意和性别双重标准的担忧。", "conclusion": "研究结果讨论了人际监控、在线跟踪的常态化，以及社会监控的演变，为用户对公共领域人际监控和识别的看法提供了新视角。", "translation": "“TikTok，做你的事”是一个病毒式趋势，用户试图通过信息众包来识别他们在公共场合看到的陌生人。这个趋势早在2021年就开始了，用户通常出于浪漫目的参与其中（类似于“寻人启事”个人广告）。这种做法包括在公共领域的监控和识别行为，尽管是由同伴而非政府或企业进行的。为了理解用户对这个趋势的反应，我们对60个TikTok视频和1,901条用户评论进行了定性分析。在审查的60个视频中，我们发现有19个人被成功识别。我们还发现，虽然有表达反对的评论（n=310），但表达支持的评论数量是其两倍多（n=883）。支持性评论表现出真正的兴趣和同情，反映了社区和算法参与观念的演变。另一方面，反对性评论则强调了对不当关系、跟踪、同意和性别双重标准的担忧。我们结合人际监控的常态化、在线跟踪以及社会监控的演变来讨论这些见解，从而为用户对公共领域人际监控和识别的看法提供新视角。", "summary": "本文对TikTok上“TikTok，做你的事”这一通过众包识别陌生人的病毒式趋势进行了研究。通过对60个视频和1901条评论的定性分析，发现该趋势成功识别了一些人，且用户支持度远高于反对度。研究探讨了用户对这种由同伴而非政府或企业主导的公共领域监控行为的复杂反应，揭示了人际监控常态化、在线跟踪以及社会监控演变的新视角。", "keywords": "TikTok, 社会监控, 用户反应, 众包, 公共领域", "comments": "本研究通过关注TikTok上独特的“TikTok，做你的事”趋势，提供了一个关于社交媒体时代人际监控和隐私观念演变的重要视角。其创新之处在于聚焦于由用户自身发起的众包监控行为，而非传统的政府或企业监控。研究结果揭示了用户对这种新型监控行为的复杂态度，特别是支持者多于反对者这一发现，对于理解数字时代社群、隐私和同意的边界具有重要意义。"}}
{"id": "2506.20834", "title": "Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher", "authors": ["Tomas Gallo Aquino", "Victoria Liu", "Habiba Azab", "Raissa Mathura", "Andrew J Watrous", "Eleonora Bartoli", "Benjamin Y Hayden", "Paul Sajda", "Sameer A Sheth", "Nuttida Rungratsameetaweemana"], "summary": "Transfer learning enhances the training of novel sensory and decision models\nby employing rich feature representations from large, pre-trained teacher\nmodels. Cognitive neuroscience shows that the human brain creates\nlow-dimensional, abstract representations for efficient sensorimotor coding.\nImportantly, the brain can learn these representations with significantly fewer\ndata points and less computational power than artificial models require. We\nintroduce Brain2Model Transfer Learning (B2M), a framework where neural\nactivity from human sensory and decision-making tasks acts as the teacher model\nfor training artificial neural networks. We propose two B2M strategies: (1)\nBrain Contrastive Transfer, which aligns brain activity and network activations\nthrough a contrastive objective; and (2) Brain Latent Transfer, which projects\nlatent dynamics from similar cognitive tasks onto student networks via\nsupervised regression of brain-derived features. We validate B2M in\nmemory-based decision-making with a recurrent neural network and scene\nreconstruction for autonomous driving with a variational autoencoder. The\nresults show that student networks benefiting from brain-based transfer\nconverge faster and achieve higher predictive accuracy than networks trained in\nisolation. Our findings indicate that the brain's representations are valuable\nfor artificial learners, paving the way for more efficient learning of complex\ndecision-making representations, which would be costly or slow through purely\nartificial training.", "comment": "15 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.20834v1", "categories": ["cs.NE", "cs.ET", "q-bio.NC"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20834v1", "AI": {"title_translation": "脑到模型迁移：以人类神经活动为教师训练感觉和决策模型", "tldr": "引入Brain2Model Transfer (B2M) 框架，利用人类神经活动作为教师信号，更高效地训练人工神经网络，使其收敛更快、准确率更高。", "motivation": "现有的迁移学习依赖于大型预训练人工模型，而人类大脑能以更少的数据和计算资源学习高效的低维抽象表示。该研究的动机是利用人类大脑的这种高效学习能力来改进人工模型的训练。", "method": "提出Brain2Model Transfer Learning (B2M) 框架，将人类感觉和决策任务中的神经活动作为教师模型来训练人工神经网络。具体包括两种策略：1) 脑对比迁移（Brain Contrastive Transfer），通过对比目标对齐大脑活动和网络激活；2) 脑潜在迁移（Brain Latent Transfer），通过对脑源性特征进行监督回归，将相似认知任务的潜在动态投射到学生网络。在基于记忆的决策任务（使用循环神经网络）和自动驾驶的场景重建（使用变分自编码器）中验证了B2M。", "result": "接受脑基迁移的学生网络比单独训练的网络收敛更快，并获得更高的预测准确性。", "conclusion": "研究结果表明，大脑的表征对于人工学习器很有价值，为更有效地学习复杂的决策表征铺平了道路，而这些表征通过纯粹的人工训练可能成本高昂或速度缓慢。", "translation": "迁移学习通过利用大型预训练教师模型丰富的特征表示，增强了新型感觉和决策模型的训练。认知神经科学表明，人脑为高效的感觉运动编码创建低维、抽象的表示。重要的是，与人工模型所需的数据点和计算能力相比，大脑能够以显著更少的数据点和更少的计算能力学习这些表示。我们引入了脑到模型迁移学习（B2M），这是一个框架，其中人类感觉和决策任务中的神经活动充当训练人工神经网络的教师模型。我们提出了两种B2M策略：（1）脑对比迁移，它通过对比目标对齐大脑活动和网络激活；（2）脑潜在迁移，它通过对脑源性特征进行监督回归，将相似认知任务的潜在动态投射到学生网络。我们在基于记忆的决策任务（使用循环神经网络）和自动驾驶的场景重建（使用变分自编码器）中验证了B2M。结果表明，受益于脑基迁移的学生网络比单独训练的网络收敛更快，并获得更高的预测准确性。我们的发现表明，大脑的表示对于人工学习器很有价值，为更有效地学习复杂的决策表示铺平了道路，而这些表示通过纯粹的人工训练可能成本高昂或速度缓慢。", "summary": "本文提出了Brain2Model Transfer (B2M) 框架，旨在利用人类神经活动作为教师信号来训练人工神经网络。针对人类大脑在学习高效低维表示时所需数据和计算资源远少于人工模型的特点，B2M提供了两种策略：脑对比迁移和脑潜在迁移。实验结果表明，通过B2M训练的学生网络能够更快收敛并达到更高的预测精度，证明了大脑表征对于提升人工学习效率的价值，尤其是在复杂决策表征学习方面。", "keywords": "脑到模型迁移, 神经活动, 迁移学习, 人工神经网络, 决策模型", "comments": "这项研究通过将人类神经活动引入人工神经网络的训练过程，开辟了神经科学与人工智能结合的新途径。其创新之处在于将大脑视为一种高效的“教师模型”，从而克服了传统迁移学习中对大型预训练人工模型的依赖。该方法有望显著提高人工模型在学习复杂、高效表征方面的效率和准确性，尤其是在数据或计算资源有限的场景下，具有重要的理论和实际意义。"}}
{"id": "2506.20804", "title": "Online Planning for Cooperative Air-Ground Robot Systems with Unknown Fuel Requirements", "authors": ["Ritvik Agarwal", "Behnoushsadat Hatami", "Alvika Gautam", "Parikshit Maini"], "summary": "We consider an online variant of the fuel-constrained UAV routing problem\nwith a ground-based mobile refueling station (FCURP-MRS), where targets incur\nunknown fuel costs. We develop a two-phase solution: an offline heuristic-based\nplanner computes initial UAV and UGV paths, and a novel online planning\nalgorithm that dynamically adjusts rendezvous points based on real-time fuel\nconsumption during target processing. Preliminary Gazebo simulations\ndemonstrate the feasibility of our approach in maintaining UAV-UGV path\nvalidity, ensuring mission completion. Link to video:\nhttps://youtu.be/EmpVj-fjqNY", "comment": "Submitted to RSS (MRS Workshop)", "pdf_url": "http://arxiv.org/pdf/2506.20804v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20804v1", "AI": {"title_translation": "未知燃料需求下空地机器人协同系统的在线规划", "tldr": "针对燃料受限无人机与移动加油站的在线路径规划问题，当目标燃料成本未知时，论文提出一种两阶段解决方案：离线启发式规划初始路径，在线算法动态调整会合点。Gazebo仿真验证了其可行性。", "motivation": "解决带有地面移动加油站（FCURP-MRS）的燃料受限无人机路径规划问题的在线变体，特别是当目标会产生未知燃料成本时。", "method": "开发了一个两阶段解决方案：一个离线启发式规划器计算初始无人机和无人车路径，以及一个新颖的在线规划算法，根据目标处理期间的实时燃料消耗动态调整会合点。", "result": "初步的Gazebo仿真证明了该方法在保持无人机-无人车路径有效性、确保任务完成方面的可行性。", "conclusion": "该方法对于解决带有未知燃料需求的空地机器人协同系统的在线规划问题是可行的，能够有效保持路径有效性并确保任务完成。", "translation": "我们考虑了带有地面移动加油站（FCURP-MRS）的燃料受限无人机路径规划问题的在线变体，其中目标会产生未知的燃料成本。我们开发了一个两阶段解决方案：一个离线启发式规划器计算初始无人机和无人车路径，以及一个新颖的在线规划算法，根据目标处理期间的实时燃料消耗动态调整会合点。初步的Gazebo仿真证明了我们方法在保持无人机-无人车路径有效性、确保任务完成方面的可行性。视频链接：https://youtu.be/EmpVj-fjqNY", "summary": "该研究提出了一种针对燃料受限无人机与地面移动加油站协同系统（FCURP-MRS）的在线规划解决方案，旨在解决目标燃料成本未知的问题。该方案包含一个离线启发式路径规划阶段和一个在线动态调整会合点的算法。初步的Gazebo仿真验证了该方法在维持路径有效性和确保任务完成方面的可行性。", "keywords": "空地机器人系统, 在线规划, 燃料受限路径规划, 无人机, 无人车", "comments": "该论文提出了一种创新的两阶段在线规划方法，有效应对了空地机器人协同系统中未知燃料需求这一复杂挑战。其在线动态调整机制是关键创新点，提高了系统在不确定环境下的适应性。初步仿真结果令人鼓舞，但未来可能需要更复杂的场景和实际部署来进一步验证其鲁棒性。"}}
{"id": "2506.20695", "title": "Malicious earworms and useful memes, how the far-right surfs on TikTok audio trends", "authors": ["Marloes Geboers", "Marcus Bösch"], "summary": "With its features of remix, TikTok is the designated platform for meme-making\nand dissemination. Creative combinations of video, emoji, and filters allow for\nan endless stream of memes and trends animated by sound. The platform has\nfocused its moderation on upholding physical safety, hence investing in the\ndetection of harmful challenges. In response to the DSA, TikTok implemented\nopt-outs for personalized feeds and features allowing users to report illegal\ncontent. At the same time, the platform remains subject to scrutiny. Centering\non the role of sound and its intersections with ambiguous memes, the presented\nresearch probed right-wing extremist formations relating to the 2024 German\nstate elections. The analysis evidences how the TikTok sound infrastructure\naffords a sustained presence of xenophobic content, often cloaked through\nvernacular modes of communication. These cloaking practices benefit from a\nsound infrastructure that affords the ongoing posting of user-generated sounds\nthat instantly spread through the use-this-sound button. Importantly, these\nsounds are often not clearly recognizable as networkers of extremist content.\nSongs that do contain hateful lyrics are not eligible for personalized feeds,\nhowever, they remain online where they profit from intersecting with benign\nmeme trends, rendering them visible in search results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20695v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20695v1", "AI": {"title_translation": "恶意耳虫和有用模因：极右翼如何利用TikTok音频趋势", "tldr": "研究发现TikTok的声音基础设施被极右翼利用，通过模糊的模因和用户生成的声音传播仇外内容，即使有害内容不被个性化推荐，仍通过与良性趋势结合而传播。", "motivation": "探讨TikTok作为模因传播平台，其声音基础设施如何被右翼极端主义利用来传播仇外内容，尤其是在2024年德国州选举背景下，以及平台现有审核机制的不足。", "method": "本研究以声音的作用及其与模糊模因的交叉点为中心，探测了与2024年德国州选举相关的右翼极端主义形成。分析了TikTok声音基础设施如何支持仇外内容的持续存在。", "result": "分析表明，TikTok的声音基础设施使得仇外内容能够持续存在，这些内容常常通过本土化的交流方式进行伪装。这些伪装行为受益于允许用户生成声音持续发布并迅速传播的声学基础设施。这些声音通常不易被识别为极端主义内容的传播者。含有仇恨歌词的歌曲虽然不符合个性化推送的条件，但仍在线上，并与良性模因趋势结合，使其在搜索结果中可见。", "conclusion": "TikTok的声音基础设施存在漏洞，使得极右翼能够以不易察觉的方式传播仇外内容，即使平台有审核机制，这些内容仍能通过与良性趋势的结合而获得可见性。平台需要进一步加强对声音内容的审核和识别。", "translation": "凭借其混音功能，TikTok是模因制作和传播的指定平台。视频、表情符号和滤镜的创意组合带来了源源不断的由声音激活的模因和趋势。该平台已将其审核重点放在维护人身安全上，因此投资于有害挑战的检测。为了响应《数字服务法案》(DSA)，TikTok实施了个性化推送的退出选项，并提供了允许用户举报非法内容的功能。与此同时，该平台仍受到审查。本研究以声音的作用及其与模糊模因的交叉点为中心，探讨了与2024年德国州选举相关的右翼极端主义形成。分析证据表明，TikTok的声音基础设施如何支持仇外内容的持续存在，这些内容通常通过本土化的交流方式进行伪装。这些伪装行为受益于一种声音基础设施，该基础设施允许用户生成的声音持续发布，并通过“使用此声音”按钮迅速传播。重要的是，这些声音通常不易被明确识别为极端主义内容的传播者。确实含有仇恨歌词的歌曲不符合个性化推送的条件，但它们仍然在线上，并受益于与良性模因趋势的交叉，使其在搜索结果中可见。", "summary": "这项研究探讨了极右翼如何利用TikTok的声音基础设施和模因趋势传播仇外内容。研究发现，尽管TikTok有内容审核和DSA响应措施，其平台上的用户生成声音和模糊模因仍被用来伪装和传播仇外信息，尤其是在德国选举背景下。这些内容即使不被个性化推荐，也能通过与良性趋势结合而获得可见性，揭示了平台在应对恶意内容传播方面的挑战。", "keywords": "TikTok, 极右翼, 音频趋势, 模因, 仇外内容", "comments": "这篇论文揭示了社交媒体平台在内容审核方面面临的复杂挑战，特别是针对那些通过声音和模糊模因进行隐蔽传播的恶意内容。其创新之处在于关注了“声音基础设施”这一常被忽视的方面，并指出其如何被滥用。研究强调了平台需要更精细的策略来识别和打击那些不明显但具有潜在危害性的内容，而不仅仅是关注显而易见的有害挑战。论文的重要性在于其揭示了数字平台在政治极化和虚假信息传播中的潜在作用，尤其是在选举背景下。"}}
{"id": "2506.21406", "title": "Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees", "authors": ["Tommaso Bonato", "Daniele De Sensi", "Salvatore Di Girolamo", "Abdulla Bataineh", "David Hewson", "Duncan Roweth", "Torsten Hoefler"], "summary": "Network latency severely impacts the performance of applications running on\nsupercomputers. Adaptive routing algorithms route packets over different\navailable paths to reduce latency and improve network utilization. However, if\na switch routes packets belonging to the same network flow on different paths,\nthey might arrive at the destination out-of-order due to differences in the\nlatency of these paths. For some transport protocols like TCP, QUIC, and RoCE,\nout-of-order (OOO) packets might cause large performance drops or significantly\nincrease CPU utilization. In this work, we propose flowcut switching, a new\nadaptive routing algorithm that provides high-performance in-order packet\ndelivery. Differently from existing solutions like flowlet switching, which are\nbased on the assumption of bursty traffic and that might still reorder packets,\nflowcut switching guarantees in-order delivery under any network conditions,\nand is effective also for non-bursty traffic, as it is often the case for RDMA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21406v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.21406v1", "AI": {"title_translation": "Flowcut 交换：高性能自适应路由与按序交付保证", "tldr": "Flowcut 交换是一种新的自适应路由算法，它解决了现有方法在超算网络中可能导致乱序数据包的问题，确保在任何网络条件下都能按序交付，同时保持高性能。", "motivation": "网络延迟严重影响超级计算机应用的性能。自适应路由算法旨在减少延迟和提高网络利用率，但可能导致同一流的数据包乱序到达，这对于TCP、QUIC和RoCE等传输协议可能导致性能大幅下降或CPU利用率显著增加。", "method": "提出了一种名为“Flowcut 交换”的新型自适应路由算法。与现有基于突发流量假设且可能导致乱序的解决方案（如flowlet 交换）不同，Flowcut 交换在任何网络条件下都能保证按序交付，并且对非突发流量（如RDMA）也有效。", "result": "Flowcut 交换提供了高性能的按序数据包交付，解决了现有自适应路由算法可能导致的乱序问题，并且适用于非突发流量。", "conclusion": "Flowcut 交换是一种能够保证数据包按序交付的高性能自适应路由算法，克服了现有解决方案的局限性，尤其适用于需要严格按序传输的场景（如RDMA）。", "translation": "网络延迟严重影响超级计算机上运行应用程序的性能。自适应路由算法通过不同的可用路径路由数据包，以减少延迟并提高网络利用率。然而，如果交换机将属于同一网络流的数据包路由到不同的路径，由于这些路径的延迟差异，它们可能会乱序到达目的地。对于某些传输协议，如TCP、QUIC和RoCE，乱序（OOO）数据包可能会导致性能大幅下降或显著增加CPU利用率。在这项工作中，我们提出了流剪切（flowcut）交换，这是一种新的自适应路由算法，可提供高性能的按序数据包交付。与现有解决方案（如flowlet 交换）不同，后者基于突发流量的假设并可能仍然重新排序数据包，流剪切交换在任何网络条件下都能保证按序交付，并且对于非突发流量（RDMA常见情况）也有效。", "summary": "这篇论文介绍了一种名为Flowcut交换的新型自适应路由算法，旨在解决现有方法在超级计算机网络中可能导致数据包乱序的问题。现有自适应路由虽能降低延迟，但可能因多路径传输导致乱序，进而影响TCP、QUIC等协议的性能。Flowcut交换保证在任何网络条件下都能实现高性能的按序数据包交付，且对非突发流量（如RDMA）同样有效，克服了传统Flowlet交换等方法的局限性。", "keywords": "自适应路由, 按序交付, 超级计算机, 网络性能, Flowcut 交换", "comments": "这项工作针对超算网络中自适应路由导致的乱序问题，提出了Flowcut交换。其创新点在于能够在任何网络条件下保证数据包的按序交付，并且对非突发流量也有效，这对于需要严格按序传输的协议（如RDMA）尤其重要，有助于提升应用性能并降低CPU开销。"}}
{"id": "2506.21414", "title": "Accelerating GNN Training through Locality-aware Dropout and Merge", "authors": ["Gongjian Sun", "Mingyu Yan", "Dengke Han", "Runzhen Xue", "Duo Wang", "Xiaochun Ye", "Dongrui Fan"], "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in graph\nlearning and are widely adopted across various critical domains. However, the\nirregular connectivity between vertices leads to inefficient neighbor\naggregation, resulting in substantial irregular and coarse-grained DRAM\naccesses. This lack of data locality presents significant challenges for\nexecution platforms, ultimately degrading performance. While previous\naccelerator designs have leveraged on-chip memory and data access scheduling\nstrategies to address this issue, they still inevitably access features at\nirregular addresses from DRAM. In this work, we propose LiGNN, a hardware-based\nsolution that improves data locality by applying dropout and merge techniques\nduring neighbor aggregation to accelerate GNN training. Unlike conventional\nalgorithm-level dropout methods that primarily aim to improve accuracy while\noverlooking hardware costs, LiGNN introduces a locality-aware feature dropout\nmechanism. This approach selectively drops node features with data locality\nawareness, effectively reducing irregular DRAM accesses without compromising\nmodel accuracy. Moreover, by leveraging detailed knowledge of memory layout and\norganization-including critical alignment constraints-LiGNN strategically\nmerges memory accesses during neighbor aggregation at the DRAM row level,\nguided by GNN-level semantics. This optimization significantly improves data\nlocality with minimal additional cost. Under the commonly adopted 0.5 dropout\nrate, LiGNN outperforms state-of-the-art methods, delivering a 1.48~3.02x\nspeedup, reducing DRAM accesses by 34%~55%, and lowering DRAM row activations\nby 59%~82%, all while maintaining model accuracy.", "comment": "under review in TPDS. extend version of DATE 2025", "pdf_url": "http://arxiv.org/pdf/2506.21414v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21414v1", "AI": {"title_translation": "通过局部性感知Dropout和合并加速GNN训练", "tldr": "LiGNN通过局部性感知的特征丢弃和内存访问合并来加速GNN训练，显著减少DRAM访问并提高性能。", "motivation": "GNN训练中不规则的邻居聚合导致低效的DRAM访问和数据局部性差，从而降低性能。现有加速器未能彻底解决DRAM不规则访问问题。", "method": "本文提出了LiGNN，一个基于硬件的解决方案，旨在通过在邻居聚合期间应用丢弃和合并技术来提高数据局部性，从而加速GNN训练。LiGNN引入了一种局部性感知特征丢弃机制，该机制在数据局部性感知下选择性地丢弃节点特征，以有效减少不规则DRAM访问，同时不影响模型精度。此外，LiGNN利用对内存布局和组织（包括关键对齐约束）的详细了解，在GNN级语义的指导下，在DRAM行级别战略性地合并邻居聚合期间的内存访问，以最小的额外成本显著改善数据局部性。", "result": "在普遍采用的0.5丢弃率下，LiGNN比现有最佳方法提速1.48~3.02倍，DRAM访问减少34%~55%，DRAM行激活降低59%~82%，同时保持了模型精度。", "conclusion": "LiGNN通过其局部性感知丢弃和内存合并策略，显著提高了GNN训练效率和性能，有效解决了数据局部性差的问题，同时保持了模型准确性。", "translation": "图神经网络 (GNN) 在图学习中取得了显著成功，并被广泛应用于各种关键领域。然而，顶点之间不规则的连接导致低效的邻居聚合，从而产生大量不规则和粗粒度的DRAM访问。这种数据局部性的缺失给执行平台带来了巨大挑战，最终降低了性能。虽然之前的加速器设计利用了片上内存和数据访问调度策略来解决这个问题，但它们仍然不可避免地从DRAM访问不规则地址的特征。在这项工作中，我们提出了LiGNN，一种基于硬件的解决方案，通过在邻居聚合期间应用丢弃和合并技术来提高数据局部性，从而加速GNN训练。与主要旨在提高精度而忽视硬件成本的传统算法级丢弃方法不同，LiGNN引入了一种局部性感知特征丢弃机制。这种方法在数据局部性感知下选择性地丢弃节点特征，在不影响模型精度的情况下有效减少不规则DRAM访问。此外，通过利用对内存布局和组织（包括关键对齐约束）的详细了解，LiGNN在GNN级语义的指导下，在DRAM行级别战略性地合并邻居聚合期间的内存访问。这种优化以最小的额外成本显著提高了数据局部性。在普遍采用的0.5丢弃率下，LiGNN优于现有最佳方法，实现了1.48~3.02倍的加速，DRAM访问减少34%~55%，DRAM行激活降低59%~82%，同时保持了模型精度。", "summary": "本文提出LiGNN，一种基于硬件的GNN训练加速方案，旨在解决不规则邻居聚合导致的数据局部性差和DRAM访问效率低的问题。LiGNN通过引入局部性感知特征丢弃机制，选择性地减少不规则DRAM访问，并在DRAM行级别战略性合并内存访问，从而显著提高数据局部性。实验结果表明，LiGNN在保持模型精度的同时，显著加速GNN训练并减少DRAM访问和行激活。", "keywords": "GNN训练, 数据局部性, 硬件加速, Dropout, 内存合并", "comments": "LiGNN的创新之处在于将数据局部性优化与GNN的Dropout机制结合，提出了硬件感知的特征丢弃，并结合DRAM行级合并策略，从硬件层面解决了GNN训练中的数据局部性瓶颈。这对于提高GNN在实际应用中的部署效率具有重要意义。"}}
{"id": "2506.20773", "title": "A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers", "authors": ["Stephen T. Castonguay", "Joshua B. Fernandes", "Michael A. Puso", "Sylvie Aubry"], "summary": "An efficient numerical framework is presented for modeling viscoelasticity\nand permanent set of polymers. It is based on the hereditary integral form of\ntransient network theory, in which polymer chains belong to distinct networks\neach with different natural equilibrium states. Chains continually detach from\npreviously formed networks and reattach to new networks in a state of zero\nstress. The free energy of these networks is given in terms of the deformation\ngradient relative to the configuration at which the network was born. A\ndecomposition of the kernel for various free energies allows for a recurrence\nrelationship to be established, bypassing the need to integrate over all time\nhistory. The technique is established for both highly compressible and nearly\nincompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and\nOgden-Hill material models. Multiple examples are presented showing the ability\nto handle rate-dependent response and residual strains under complex loading\nhistories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20773v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20773v1", "AI": {"title_translation": "聚合物永久形变和粘弹性响应建模的遗传积分瞬态网络方法", "tldr": "提出了一种基于遗传积分瞬态网络理论的高效数值框架，用于模拟聚合物的粘弹性和永久形变，通过引入递归关系避免了对整个时间历史的积分，并能处理复杂加载历史下的速率依赖响应和残余应变。", "motivation": "为了建立一个高效的数值框架来模拟聚合物的粘弹性和永久形变。", "method": "该方法基于瞬态网络理论的遗传积分形式，其中聚合物链属于不同的网络，每个网络具有不同的自然平衡状态。链不断从先前形成的旧网络中脱离，并在零应力状态下重新连接到新网络。网络的自由能是相对于网络生成时的构型变形梯度给出的。通过对各种自由能的核进行分解，建立了一个递归关系，从而避免了对所有时间历史的积分。该技术通过使用新胡克、布拉茨-科、叶奥和奥格登-希尔材料模型，适用于高度可压缩和近乎不可压缩的材料。", "result": "该框架能够处理复杂加载历史下的速率依赖响应和残余应变，并通过多个例子进行了展示。", "conclusion": "Not mentioned in abstract", "translation": "本文提出了一种用于模拟聚合物粘弹性和永久形变的高效数值框架。该框架基于瞬态网络理论的遗传积分形式，其中聚合物链属于不同的网络，每个网络具有不同的自然平衡状态。链不断从先前形成的旧网络中脱离，并在零应力状态下重新连接到新网络。这些网络的自由能是根据相对于网络诞生时构型的变形梯度给出的。通过对各种自由能的核进行分解，可以建立一个递归关系，从而避免了对所有时间历史的积分。该技术通过使用新胡克、布拉茨-科、叶奥和奥格登-希尔材料模型，适用于高度可压缩和近乎不可压缩的材料。文章给出了多个例子，展示了其在复杂加载历史下处理速率依赖响应和残余应变的能力。", "summary": "本文提出了一种基于遗传积分瞬态网络理论的高效数值框架，用于模拟聚合物的粘弹性和永久形变。该模型通过链的动态脱离和重新连接来描述网络行为，并引入了递归关系以避免对整个时间历史的积分。该方法适用于多种材料模型，并能有效处理复杂加载历史下的速率依赖响应和残余应变。", "keywords": "聚合物, 粘弹性, 永久形变, 瞬态网络理论, 遗传积分", "comments": "该论文的关键创新在于将遗传积分瞬态网络理论与递归关系相结合，从而极大地提高了模拟聚合物粘弹性和永久形变的效率。这种方法避免了传统方法中对整个时间历史的积分需求，使其在处理复杂加载历史时更具计算优势。其能够处理不同材料模型和复杂加载条件下的材料响应，显示了其广泛的应用潜力。"}}
{"id": "2506.20737", "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation", "authors": ["Gurusha Juneja", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "summary": "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20737v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20737v1", "AI": {"title_translation": "MAGPIE：一个用于多智能体情境隐私评估的数据集", "tldr": "本研究提出了MAGPIE数据集，用于评估基于LLM的多智能体在情境隐私方面的表现。结果表明，当前最先进的模型在理解和保护情境隐私方面表现不佳，且在多轮对话中仍会泄露私有信息。", "motivation": "随着基于大型语言模型（LLM）的智能体在日程安排、谈判、资源分配等任务中进行跨智能体协作的部署日益增多，隐私问题变得至关重要，因为智能体经常访问需要严格保密的专有工具和领域特定数据库。现有评估LLM智能体情境隐私的基准主要评估单轮、低复杂度的任务，其中私有信息可以轻易排除。因此，本文旨在探究LLM智能体是否理解情境隐私，以及在收到指示后，这些系统在非对抗性多轮对话中是否能保护推断时期的用户隐私。", "method": "本文首先提出了一个名为MAGPIE的基准数据集，该数据集包含158个跨越15个领域的真实世界高风险场景。这些场景的设计使得完全排除私有数据会阻碍任务完成，但无限制的信息共享又可能导致重大损失。然后，研究人员评估了当前最先进的LLM模型（包括GPT-4o和Claude-2.7-Sonnet）在(a)其对情境私有数据的理解和(b)其在不违反用户隐私的情况下进行协作的能力。", "result": "实证实验表明，当前模型（包括GPT-4o和Claude-2.7-Sonnet）缺乏对情境隐私的鲁棒理解，将私有数据错误分类为可共享数据的比例分别为25.2%和43.6%。在多轮对话中，即使有明确的隐私指令，这些模型仍有59.9%和50.5%的案例会泄露私有信息。此外，多智能体系统在71%的场景中未能完成任务。", "conclusion": "这些结果强调，当前模型尚未实现情境隐私保护与协作任务解决的同步对齐。", "translation": "基于LLM的智能体日益普及，导致跨智能体协作在日程安排、谈判、资源分配等任务中的部署不断增加。在此类系统中，隐私至关重要，因为智能体通常访问需要严格保密的专有工具和领域特定数据库。本文研究LLM智能体是否能理解情境隐私。并且，如果受到指示，这些系统是否能在非对抗性多轮对话中保护推理时的用户隐私。现有评估LLM智能体情境隐私的基准主要评估单轮、低复杂度的任务，其中私有信息可以轻易排除。我们首先提出了一个基准——MAGPIE，它包含15个领域中158个真实世界的高风险场景。这些场景的设计使得完全排除私有数据会阻碍任务完成，而无限制的信息共享可能导致重大损失。然后，我们评估了当前最先进的LLM模型在(a)它们对情境私有数据的理解和(b)它们在不违反用户隐私的情况下进行协作的能力。实证实验表明，当前模型，包括GPT-4o和Claude-2.7-Sonnet，缺乏对情境隐私的鲁棒理解，将私有数据错误分类为可共享的比例分别为25.2%和43.6%。在多轮对话中，即使在明确的隐私指令下，这些模型在59.9%和50.5%的案例中披露了私有信息。此外，多智能体系统在71%的场景中未能完成任务。这些结果强调，当前模型尚未实现情境隐私保护与协作任务解决的同步对齐。", "summary": "本文针对基于LLM的多智能体协作中情境隐私的挑战，提出了一个名为MAGPIE的新基准数据集。该数据集包含158个高风险真实场景，旨在评估智能体对情境隐私的理解及其在不泄露私有信息前提下完成协作任务的能力。通过对当前最先进LLM模型（如GPT-4o和Claude-2.7-Sonnet）的评估，研究发现，现有模型在识别和保护情境隐私方面表现不佳，即使在明确指示下，仍频繁泄露私有信息，并导致多智能体系统任务完成率低下。这表明当前模型在隐私保护和协作任务解决之间存在对齐问题。", "keywords": "LLM智能体, 情境隐私, 多智能体系统, 数据集, 隐私评估", "comments": "本文通过引入MAGPIE数据集，填补了LLM多智能体情境隐私评估领域的空白，尤其关注了高风险和复杂场景，这具有重要的创新性。研究结果清晰地揭示了当前LLM在处理情境隐私方面的显著局限性，即使是顶级模型也无法有效应对，这对于未来LLM模型的设计和部署具有重要的指导意义。该研究强调了在追求模型能力的同时，隐私保护对齐的紧迫性。"}}
{"id": "2506.20674", "title": "Scalable GPU Performance Variability Analysis framework", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Seth Ockerman", "Amal Gueroudji", "Line Pouchard", "Tanzima Z. Islam"], "summary": "Analyzing large-scale performance logs from GPU profilers often requires\nterabytes of memory and hours of runtime, even for basic summaries. These\nconstraints prevent timely insight and hinder the integration of performance\nanalytics into automated workflows. Existing analysis tools typically process\ndata sequentially, making them ill-suited for HPC workflows with growing trace\ncomplexity and volume. We introduce a distributed data analysis framework that\nscales with dataset size and compute availability. Rather than treating the\ndataset as a single entity, our system partitions it into independently\nanalyzable shards and processes them concurrently across MPI ranks. This design\nreduces per-node memory pressure, avoids central bottlenecks, and enables\nlow-latency exploration of high-dimensional trace data. We apply the framework\nto end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate\nits ability to diagnose performance variability, and uncover the impact of\nmemory transfer latency on GPU kernel behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20674v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20674v1", "AI": {"title_translation": "可扩展GPU性能变异性分析框架", "tldr": "现有GPU性能分析慢且耗内存。本文提出一个分布式框架，通过数据分片并发处理，减少内存并避免瓶颈，实现对大规模HPC/AI跟踪数据的快速分析。", "motivation": "分析大规模GPU性能日志需要数TB内存和数小时运行时间，阻碍及时洞察和自动化集成。现有分析工具顺序处理数据，不适用于日益增长的HPC跟踪复杂性和数据量。", "method": "引入一个分布式数据分析框架，该框架将数据集划分为可独立分析的分片，并跨MPI等级并发处理。此设计旨在减少每节点内存压力并避免中心瓶颈。", "result": "将该框架应用于实际HPC和AI工作负载的Nsight Compute跟踪，展示了其诊断性能变异性的能力，并揭示了内存传输延迟对GPU内核行为的影响。", "conclusion": "该分布式框架有效解决了大规模GPU性能分析的挑战，能够更快地诊断性能变异性并深入理解GPU行为。", "translation": "分析来自GPU性能分析器的大规模性能日志通常需要数TB的内存和数小时的运行时间，即使是进行基本的汇总。这些限制阻碍了及时洞察，并妨碍了性能分析集成到自动化工作流程中。现有的分析工具通常按顺序处理数据，这使得它们不适合处理日益增长的跟踪复杂性和数据量的HPC工作流程。我们引入了一个分布式数据分析框架，该框架可以随数据集大小和计算可用性进行扩展。我们的系统不是将数据集视为一个单一实体，而是将其划分为可独立分析的分片，并跨MPI等级并发处理它们。这种设计减少了每节点内存压力，避免了中心瓶颈，并实现了高维跟踪数据的低延迟探索。我们将该框架应用于来自实际HPC和AI工作负载的端到端Nsight Compute跟踪，展示了其诊断性能变异性的能力，并揭示了内存传输延迟对GPU内核行为的影响。", "summary": "本文提出一个可扩展的分布式数据分析框架，旨在解决现有GPU性能日志分析工具在处理大规模数据时面临的内存和时间限制。该框架通过将数据集划分为独立的分片并进行并发处理，有效降低了内存压力并消除了中心瓶颈，从而实现了对高维跟踪数据的低延迟探索。实验证明，该框架能够诊断实际HPC和AI工作负载中的性能变异性，并揭示内存传输延迟对GPU内核行为的影响。", "keywords": "GPU性能分析, 分布式框架, 性能变异性, 高性能计算, Nsight Compute", "comments": "该论文的创新点在于提出了一个分布式、可扩展的GPU性能分析框架，通过数据分片和并发处理解决了传统工具在处理大规模高性能计算（HPC）和AI工作负载时面临的内存和时间瓶颈。这对于加速性能诊断和集成分析到自动化流程中具有重要意义。"}}
{"id": "2506.20756", "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.20756v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20756v1", "AI": {"title_translation": "StereoDiff：视频深度估计中的立体-扩散协同", "tldr": "StereoDiff通过结合立体匹配处理静态区域和视频深度扩散处理动态区域，显著提升了视频深度估计的性能和一致性。", "motivation": "现有的视频深度估计方法将视频深度估计视为图像深度估计的简单扩展，但作者认为视频中动态和静态区域的时间一致性要求不同，静态区域更适合立体匹配以获取全局3D线索，而动态区域需要从大规模数据中学习以确保平滑过渡。", "method": "本文提出了StereoDiff，一个两阶段视频深度估计器。它将立体匹配（主要用于静态区域以实现一致深度）与视频深度扩散（用于保持动态区域的深度平滑过渡）相结合。通过频域分析，数学地证明了立体匹配和视频深度扩散如何提供互补优势。", "result": "在零样本、真实世界、动态视频深度基准（包括室内和室外）上的实验结果表明，StereoDiff达到了最先进的性能，展示了其在视频深度估计中卓越的一致性和准确性。", "conclusion": "StereoDiff通过结合立体匹配和视频深度扩散，有效地解决了视频深度估计中动态和静态区域的不同一致性要求，实现了最先进的性能，并提供了卓越的一致性和准确性。", "translation": "最近的视频深度估计方法通过遵循图像深度估计的范式，即通常使用海量数据微调预训练的视频扩散模型，取得了优异的性能。然而，我们认为视频深度估计并非图像深度估计的简单扩展。视频中动态和静态区域的时间一致性要求根本不同。静态区域（通常是背景）中一致的视频深度可以通过跨所有帧的立体匹配更有效地实现，这提供了更强的全局3D线索。而动态区域的一致性，由于三角测量约束的违反，仍应从大规模视频深度数据中学习以确保平滑过渡。基于这些见解，我们引入了StereoDiff，一个两阶段视频深度估计器，它将主要用于静态区域的立体匹配与用于保持动态区域深度平滑过渡的视频深度扩散相结合。我们通过频域分析数学地证明了立体匹配和视频深度扩散如何提供互补优势，突出了它们协同作用在捕捉两者优点方面的有效性。在零样本、真实世界、动态视频深度基准（包括室内和室外）上的实验结果表明，StereoDiff达到了最先进的性能，展示了其在视频深度估计中卓越的一致性和准确性。", "summary": "StereoDiff是一种新颖的两阶段视频深度估计方法，它克服了现有方法将视频深度视为图像深度简单扩展的局限性。该方法创新性地结合了立体匹配（用于静态区域的全局3D一致性）和视频深度扩散（用于动态区域的平滑过渡），并通过频域分析证明了其互补性。实验结果表明，StereoDiff在零样本、真实世界动态视频深度估计任务上取得了最先进的性能，显著提升了视频深度估计的一致性和准确性。", "keywords": "视频深度估计, 立体匹配, 扩散模型, 时间一致性, 频域分析", "comments": "这篇论文的创新点在于认识到视频中动态和静态区域对深度一致性的不同需求，并提出了一种巧妙的两阶段协同方法。通过结合立体匹配的全局3D信息和视频深度扩散的动态平滑过渡能力，StereoDiff有效解决了视频深度估计的挑战。数学上的频域分析进一步增强了其理论基础。其在零样本真实世界场景下的SoTA表现，证明了该方法的实用性和泛化能力。"}}
{"id": "2506.20693", "title": "E-ABIN: an Explainable module for Anomaly detection in BIological Networks", "authors": ["Ugo Lomoio", "Tommaso Mazza", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "summary": "The increasing availability of large-scale omics data calls for robust\nanalytical frameworks capable of handling complex gene expression datasets\nwhile offering interpretable results. Recent advances in artificial\nintelligence have enabled the identification of aberrant molecular patterns\ndistinguishing disease states from healthy controls. Coupled with improvements\nin model interpretability, these tools now support the identification of genes\npotentially driving disease phenotypes. However, current approaches to gene\nanomaly detection often remain limited to single datasets and lack accessible\ngraphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable\nframework for Anomaly detection in Biological Networks. E-ABIN combines\nclassical machine learning and graph-based deep learning techniques within a\nunified, user-friendly platform, enabling the detection and interpretation of\nanomalies from gene expression or methylation-derived networks. By integrating\nalgorithms such as Support Vector Machines, Random Forests, Graph Autoencoders\n(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a\nhigh predictive accuracy while maintaining interpretability. We demonstrate the\nutility of E-ABIN through case studies of bladder cancer and coeliac disease,\nwhere it effectively uncovers biologically relevant anomalies and offers\ninsights into disease mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20693v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20693v1", "AI": {"title_translation": "E-ABIN：一种用于生物网络异常检测的可解释模块", "tldr": "E-ABIN是一个可解释的框架，结合了传统机器学习和图深度学习技术，用于在基因表达或甲基化网络中检测和解释异常，并通过膀胱癌和乳糜泻案例研究验证了其有效性。", "motivation": "现有的组学数据分析框架需要更强的可解释性。当前基因异常检测方法通常仅限于单一数据集，且缺乏易于访问的图形界面，限制了其应用范围和用户友好性。", "method": "本文介绍了E-ABIN，一个通用、可解释的生物网络异常检测框架。E-ABIN在一个统一、用户友好的平台中结合了经典机器学习（如支持向量机、随机森林）和基于图的深度学习技术（如图自编码器、图对抗属性网络），能够从基因表达或甲基化衍生的网络中检测和解释异常。", "result": "E-ABIN在膀胱癌和乳糜泻的案例研究中展示了其效用，有效地揭示了生物学相关的异常，并为疾病机制提供了见解。该框架在保持可解释性的同时，确保了高预测准确性。", "conclusion": "E-ABIN提供了一个通用、可解释的框架，能够有效地从生物网络中检测并解释异常，为疾病机制研究提供了新的工具和见解，解决了当前方法在可解释性和用户界面方面的局限性。", "translation": "随着大规模组学数据可用性的增加，需要强大的分析框架来处理复杂的基因表达数据集，同时提供可解释的结果。人工智能的最新进展使得识别区分疾病状态与健康对照的异常分子模式成为可能。结合模型可解释性的改进，这些工具现在支持识别可能驱动疾病表型的基因。然而，当前的基因异常检测方法往往仅限于单一数据集，并且缺乏易于访问的图形界面。在此，我们引入了E-ABIN，一个用于生物网络异常检测的通用、可解释框架。E-ABIN在一个统一、用户友好的平台中结合了经典机器学习和基于图的深度学习技术，能够检测和解释来自基因表达或甲基化衍生网络的异常。通过整合支持向量机、随机森林、图自编码器（GAEs）和图对抗属性网络（GAANs）等算法，E-ABIN在保持可解释性的同时，确保了高预测准确性。我们通过膀胱癌和乳糜泻的案例研究展示了E-ABIN的实用性，它有效地揭示了生物学相关的异常，并为疾病机制提供了见解。", "summary": "E-ABIN是一个新颖的、可解释的生物网络异常检测框架，旨在解决现有方法在处理大规模组学数据时可解释性和用户界面的不足。它整合了传统机器学习和图深度学习算法（如SVM、RF、GAEs、GAANs），在一个用户友好的平台中实现对基因表达或甲基化网络中异常的检测与解释。该框架在膀胱癌和乳糜泻的案例研究中被证明能够准确识别生物学相关异常，并提供疾病机制的深入见解。", "keywords": "异常检测, 生物网络, 可解释性, 机器学习, 深度学习", "comments": "E-ABIN的创新之处在于其结合了传统机器学习和图深度学习的混合方法，并特别强调了结果的可解释性，这在复杂的生物网络分析中至关重要。其用户友好的平台设计也降低了使用门槛。该框架通过整合多种算法，旨在实现高预测准确性和可解释性的平衡，这对于生物医学研究具有重要意义，有助于推动对疾病机制的理解。"}}
{"id": "2506.21078", "title": "Constant Modulus Waveforms for IoT-Centric Integrated Sensing and Communications", "authors": ["Tian Han", "Shalanika Dayarathna", "Rajitha Senanayake", "Peter Smith", "Aryan Kaushik", "Alain Mourad", "Richard A. Stirling-Gallacher", "Jamie Evans"], "summary": "Integrated sensing and communications (ISAC) is considered a key enabler to\nsupport application scenarios such as the Internet-of-Things (IoT) in which\nboth communications and sensing play significant roles. Multi-carrier\nwaveforms, such as orthogonal frequency division multiplexing (OFDM), have been\nconsidered as good candidates for ISAC due to their high communications data\nrate and good time bandwidth property for sensing. Nevertheless, their high\npeak-to-average-power-ratio (PAPR) values lead to either performance\ndegradation or an increase in system complexity. This can make OFDM unsuitable\nfor IoT applications with insufficient resources in terms of power, system\ncomplexity, hardware size or cost. This article provides IoT-centric constant\nmodulus waveform designs that leverage the advantage of unit PAPR and thus are\nmore suitable in resource-limited scenarios. More specifically, several\nsingle-carrier frequency and/or phase-modulated waveforms are considered. A\ncomprehensive discussion on their radar sensing and communications performance\nis conducted based on performance metrics, including the radar ambiguity\nfunction, the bandwidth property, the data rate, and the communications\nreceiver complexity.", "comment": "Submitted for publication to IEEE Communications Standards Magazine", "pdf_url": "http://arxiv.org/pdf/2506.21078v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21078v1", "AI": {"title_translation": "用于以物联网为中心的集成感知与通信的恒模波形", "tldr": "本文提出了适用于资源受限物联网场景的恒模波形设计，以解决多载波波形（如OFDM）在高峰均功率比（PAPR）方面的问题，并对其感知和通信性能进行了全面讨论。", "motivation": "集成感知与通信（ISAC）是支持物联网（IoT）应用场景的关键技术。然而，传统的多载波波形（如OFDM）由于其高峰均功率比（PAPR），在资源受限的物联网应用中会导致性能下降或系统复杂性增加，使其不适用。", "method": "本文提出了以物联网为中心的恒模波形设计，利用其单位高峰均功率比（PAPR）的优势。具体而言，研究了多种单载波频率和/或相位调制波形。", "result": "基于雷达模糊函数、带宽特性、数据速率和通信接收机复杂性等性能指标，对所提出的恒模波形的雷达感知和通信性能进行了全面讨论。", "conclusion": "恒模波形利用其单位高峰均功率比（PAPR）的优势，更适用于资源受限的物联网集成感知与通信场景。", "translation": "集成感知与通信（ISAC）被认为是支持物联网（IoT）等应用场景的关键使能技术，在这些场景中，通信和感知都扮演着重要角色。多载波波形，如正交频分复用（OFDM），因其高通信数据速率和良好的感知时间带宽特性，被认为是ISAC的良好候选。然而，其高峰均功率比（PAPR）值会导致性能下降或系统复杂性增加。这可能使得OFDM不适用于在功率、系统复杂性、硬件尺寸或成本方面资源不足的物联网应用。本文提供了以物联网为中心的恒模波形设计，利用了单位高峰均功率比的优势，因此更适用于资源受限的场景。更具体地说，本文考虑了几种单载波频率和/或相位调制波形。基于雷达模糊函数、带宽特性、数据速率和通信接收机复杂性等性能指标，对其雷达感知和通信性能进行了全面讨论。", "summary": "本文针对物联网（IoT）中集成感知与通信（ISAC）场景的需求，提出了一种新的恒模波形设计。鉴于传统多载波波形（如OFDM）在高PAPR下不适用于资源受限的IoT设备，该研究利用恒模波形单位PAPR的优势，设计了多种单载波频率和/或相位调制波形。论文全面分析了这些波形在雷达感知和通信方面的性能，证明了其在资源受限环境下的适用性。", "keywords": "集成感知与通信, 物联网, 恒模波形, 高峰均功率比, 单载波", "comments": "该论文创新性地将恒模波形引入物联网集成感知与通信领域，有效解决了OFDM等传统波形在资源受限IoT设备中高PAPR带来的挑战。其提出的单载波频率/相位调制波形设计，对于推动低功耗、低复杂度的ISAC系统发展具有重要意义。对各项性能指标的全面讨论也增加了研究的严谨性。"}}
{"id": "2506.20687", "title": "Review of Three Variants of the k-d Tree", "authors": ["Russell A. Brown"], "summary": "The original description of the k-d tree recognized that rebalancing\ntechniques, such as used to build an AVL tree or a red-black tree, are not\napplicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is\nnecessary to find the median of a set of data for each recursive subdivision of\nthat set. The sort or selection used to find the median, and the technique used\nto partition the set about that median, strongly influence the computational\ncomplexity of building a k-d tree. This article describes and contrasts three\nvariants of the k-d tree that differ in their technique used to partition the\nset, and compares the performance of those variants. In addition, dual-threaded\nexecution is proposed and analyzed for one of the three variants.", "comment": "29 pages, 11 figures, one listing, one table", "pdf_url": "http://arxiv.org/pdf/2506.20687v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20687v1", "AI": {"title_translation": "k-d树三种变体的综述", "tldr": "本文综述并对比了k-d树的三种变体，重点关注其分区技术对构建复杂度的影响，并提出了一种双线程执行方案。", "motivation": "k-d树的重平衡技术不适用，导致构建平衡k-d树需要通过中位数查找和数据分区实现，而这些技术显著影响构建的计算复杂度。", "method": "本文描述并对比了三种k-d树变体，这些变体在数据分区技术上有所不同，并比较了它们的性能。此外，还针对其中一种变体提出了双线程执行方案并进行了分析。", "result": "本文描述并对比了三种k-d树变体，并比较了它们的性能。同时，针对其中一种变体提出了双线程执行方案并进行了分析。", "conclusion": "Not mentioned in abstract", "translation": "k-d树的原始描述认识到，重新平衡技术，例如用于构建AVL树或红黑树的技术，不适用于k-d树。因此，为了构建一个平衡的k-d树，有必要在每次递归细分数据集时找到该数据集的中位数。用于查找中位数以及围绕该中位数划分数据集的排序或选择技术，强烈影响构建k-d树的计算复杂度。本文描述并对比了k-d树的三种变体，它们在用于划分数据集的技术上有所不同，并比较了这些变体的性能。此外，还针对其中一种变体提出了双线程执行方案并进行了分析。", "summary": "本文综述了k-d树的三种变体，着重分析了它们不同的数据分区技术如何影响构建时的计算复杂度。文章指出，传统的重平衡方法不适用于k-d树，因此构建平衡树依赖于中位数查找。论文对比了这些变体的性能，并为其中一种引入了双线程执行方案。", "keywords": "k-d树, 数据分区, 计算复杂度, 树变体, 双线程执行", "comments": "本文解决了k-d树构建中的一个核心挑战，即在不使用传统重平衡技术的情况下实现平衡。通过回顾和比较不同的分区方法，并提出新颖的双线程执行方案，该研究为优化k-d树的性能和构建效率提供了见解。对分区技术的关注对于k-d树的实际应用至关重要。"}}
{"id": "2506.20728", "title": "Distributed Lyapunov Functions for Nonlinear Networks", "authors": ["Yiming Wang", "Arthur N. Montanari", "Adilson E. Motter"], "summary": "Nonlinear networks are often multistable, exhibiting coexisting stable states\nwith competing regions of attraction (ROAs). As a result, ROAs can have complex\n\"tentacle-like\" morphologies that are challenging to characterize analytically\nor computationally. In addition, the high dimensionality of the state space\nprohibits the automated construction of Lyapunov functions using\nstate-of-the-art optimization methods, such as sum-of-squares (SOS)\nprogramming. In this letter, we propose a distributed approach for the\nconstruction of Lyapunov functions based solely on local information. To this\nend, we establish an augmented comparison lemma that characterizes the\nexistence conditions of partial Lyapunov functions, while also accounting for\nresidual effects caused by the associated dimensionality reduction. These\ntheoretical results allow us to formulate an SOS optimization that iteratively\nconstructs such partial functions, whose aggregation forms a composite Lyapunov\nfunction. The resulting composite function provides accurate convex\napproximations of both the volumes and shapes of the ROAs. We validate our\nmethod on networks of van der Pol and Ising oscillators, demonstrating its\neffectiveness in characterizing high-dimensional systems with non-convex ROAs.", "comment": "Codes are available at our GitHub repository\n  https://github.com/YimingSci/Distributed-Lya-Func", "pdf_url": "http://arxiv.org/pdf/2506.20728v1", "categories": ["eess.SY", "cond-mat.dis-nn", "cs.SY", "math.DS"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20728v1", "AI": {"title_translation": "非线性网络的分布式Lyapunov函数", "tldr": "本文提出一种基于局部信息的分布式方法来构建高维非线性网络中的Lyapunov函数，以准确近似吸引域。", "motivation": "非线性网络通常具有复杂的吸引域（ROAs）形态，难以分析和计算。此外，高维状态空间使得传统优化方法（如SOS编程）难以自动构建Lyapunov函数。", "method": "提出一种基于局部信息的分布式Lyapunov函数构建方法。该方法建立了一个增广比较引理来表征部分Lyapunov函数的存在条件，并考虑维度降低的残余效应。通过迭代的SOS优化构建部分函数，最终聚合形成复合Lyapunov函数。", "result": "所构建的复合Lyapunov函数能够提供吸引域（ROAs）体积和形状的精确凸近似。该方法在van der Pol和Ising振荡器网络上得到验证，证明其在表征具有非凸吸引域的高维系统方面的有效性。", "conclusion": "本文提出了一种分布式方法来构建非线性网络的Lyapunov函数，克服了高维下传统方法的挑战，并成功地表征了具有复杂吸引域的高维系统。", "translation": "非线性网络通常是多稳态的，表现出共存的稳定状态和竞争的吸引域（ROAs）。因此，吸引域可能具有复杂的“触手状”形态，这在分析或计算上都具有挑战性。此外，状态空间的高维性使得使用最先进的优化方法（如平方和（SOS）编程）自动构建Lyapunov函数变得困难。在这封信中，我们提出了一种基于局部信息的分布式方法来构建Lyapunov函数。为此，我们建立了一个增广比较引理，它表征了部分Lyapunov函数的存在条件，同时考虑了由相关维度降低引起的残余效应。这些理论结果使我们能够制定一个SOS优化问题，迭代地构建这些部分函数，它们的聚合形成一个复合Lyapunov函数。由此产生的复合函数提供了吸引域体积和形状的精确凸近似。我们在van der Pol和Ising振荡器网络上验证了我们的方法，证明了其在表征具有非凸吸引域的高维系统方面的有效性。", "summary": "本文针对非线性网络中吸引域（ROAs）表征的挑战以及高维状态空间下Lyapunov函数构建的困难，提出了一种创新的分布式方法。该方法仅依赖局部信息，通过建立增广比较引理和迭代的SOS优化，构建部分Lyapunov函数并聚合为复合Lyapunov函数。实验结果表明，该方法能够准确地凸近似ROAs的体积和形状，并有效应用于高维非凸ROAs系统。", "keywords": "分布式Lyapunov函数, 非线性网络, 吸引域, SOS优化, 高维系统", "comments": "本文提出了一种新颖的分布式方法来解决高维非线性系统中Lyapunov函数构建和吸引域表征的难题。其创新点在于利用局部信息和增广比较引理，将复杂的高维问题分解为可迭代优化的子问题，最终聚合得到全局有效的Lyapunov函数。这对于理解和控制复杂非线性动态系统具有重要意义，尤其是在大规模网络分析中展现出巨大潜力。"}}
{"id": "2506.20783", "title": "Precise Near-Field Beam Training with DFT Codebook based on Amplitude-only Measurement", "authors": ["Zijun Wang", "Shawn Tsai", "Rama Kiran", "Rui Zhang"], "summary": "Extremely large antenna arrays (ELAAs) operating in high-frequency bands have\nspurred the development of near-field communication, driving advancements in\nbeam training and signal processing design. In this work, we present a\nlow-complexity near-field beam training scheme that fully utilizes the\nconventional discrete Fourier transform (DFT) codebook designed for far-field\nusers. We begin by analyzing the received beam pattern in the near field and\nderive closed-form expressions for the beam width and central gain. These\nanalytical results enable the definition of an angle-dependent, modified\nRayleigh distance, which effectively distinguishes near-field and far-field\nuser regimes. Building on the analysis, we develop a direct and computationally\nefficient method to estimate user distance, with a complexity of O(1), and\nfurther improve its accuracy through a simple refinement. Simulation results\ndemonstrate significant gains in both single- and multi-user settings, with up\nto 2.38 dB SNR improvement over exhaustive search. To further enhance\nestimation accuracy, we additionally propose a maximum likelihood estimation\n(MLE) based refinement method, leveraging the Rician distribution of signal\namplitudes and achieving accuracy close to the Cramer--Rao bound (CRB).\nSimulation shows the single-user and multi-user achievable rates can both\napproach those obtained with ideal channel state information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20783v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20783v1", "AI": {"title_translation": "基于幅度测量的DFT码本的精确近场波束训练", "tldr": "本文提出了一种低复杂度的近场波束训练方案，利用传统DFT码本，通过幅度测量实现高精度距离估计和波束训练，性能接近理想信道信息。", "motivation": "极端大天线阵列 (ELAAs) 在高频段运行，推动了近场通信的发展，进而对波束训练和信号处理设计提出了更高的要求。", "method": "该方案利用为远场设计的传统DFT码本，通过分析近场接收波束图，推导了波束宽度和中心增益的闭式表达式，并定义了修正瑞利距离。在此基础上，开发了一种复杂度为O(1)的用户距离直接估计方法，并通过简单细化提高精度。此外，还提出了一种基于最大似然估计 (MLE) 的细化方法，利用信号幅度的莱斯分布来进一步增强估计精度。", "result": "仿真结果表明，在单用户和多用户设置中，信噪比相对于穷举搜索提高了高达2.38 dB。MLE细化方法实现了接近克拉默-拉奥界 (CRB) 的精度。单用户和多用户可实现速率均能接近通过理想信道状态信息获得的速率。", "conclusion": "本文提出的基于DFT码本的低复杂度近场波束训练方案，通过深入的理论分析和高效的距离估计方法，实现了高精度的波束训练，其性能在信噪比和可实现速率方面均表现出色，接近理想信道信息下的水平。", "translation": "极端大天线阵列 (ELAAs) 在高频段运行，推动了近场通信的发展，进而促进了波束训练和信号处理设计方面的进步。在这项工作中，我们提出了一种低复杂度的近场波束训练方案，该方案充分利用了为远场用户设计的传统离散傅里叶变换 (DFT) 码本。我们首先分析了近场中的接收波束图，并推导了波束宽度和中心增益的闭式表达式。这些分析结果使得能够定义一个与角度相关的修正瑞利距离，该距离有效地区分了近场和远场用户区域。在此分析的基础上，我们开发了一种直接且计算高效的用户距离估计方法，其复杂度为 O(1)，并通过简单的细化进一步提高了其精度。仿真结果表明，在单用户和多用户设置中均实现了显著增益，相对于穷举搜索，信噪比提高了高达 2.38 dB。为了进一步提高估计精度，我们额外提出了一种基于最大似然估计 (MLE) 的细化方法，该方法利用信号幅度的莱斯分布，实现了接近克拉默-拉奥界 (CRB) 的精度。仿真表明，单用户和多用户可实现速率均能接近通过理想信道状态信息获得的速率。", "summary": "本文提出了一种低复杂度的近场波束训练方案，该方案创新性地利用了为远场设计的传统DFT码本，并仅基于幅度测量。通过对近场波束图的深入分析，推导了关键参数并定义了修正瑞利距离。在此基础上，开发了O(1)复杂度的高效用户距离估计方法，并通过基于最大似然估计的细化进一步提升了精度，实现了接近克拉默-拉奥界的高性能。仿真结果验证了该方案在信噪比增益和可实现速率方面均表现出色，接近理想信道信息下的性能。", "keywords": "近场通信, 波束训练, DFT码本, 幅度测量, 距离估计", "comments": "该论文创新性地将传统为远场设计的DFT码本应用于近场波束训练，并实现了低复杂度。通过深入的理论分析和高效的估计方法，解决了近场距离估计的挑战，并且仅依赖幅度测量，具有较高的实际应用价值和工程可行性。"}}
{"id": "2506.21074", "title": "CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via Dynamic Frame Rate", "authors": ["Hankun Wang", "Yiwei Guo", "Chongtian Shao", "Bohan Li", "Xie Chen", "Kai Yu"], "summary": "Neural speech codecs have been widely used in audio compression and various\ndownstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which\nallocate the same number of tokens to every equal-duration slice. However,\nspeech is inherently non-uniform in temporal information density. As a result,\nmany tokens are wasted on steady-state segments like long vowels and silences.\nTo address this mismatch, we present CodecSlime, a plugin-style method for\ncompressing temporal redundancy through supporting dynamic frame rate (DFR) on\nneural speech codecs for the first time. Our method is unsupervised and\narchitecture-agnostic, combining two key innovations, ScheDFR and\nMelt-and-Cool, for adapting inference and training, respectively. When\nintegrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR\n($\\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to\n46% relative to conventional FFR baselines with the same model architecture and\nsimilar bitrates, while other metrics are also competitive. CodecSlime also\nenables flexible trade-offs between reconstruction quality and bitrate: a\nsingle model supports inference at multiple frame rates and consistently\noutperforms FFR models at the corresponding frame rates. Audio samples are\navailable at https://acadarmeria.github.io/codecslime/.", "comment": "16 pages, 5 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2506.21074v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21074v1", "AI": {"title_translation": "CodecSlime：基于动态帧率的神经语音编解码器时间冗余压缩", "tldr": "CodecSlime首次为神经语音编解码器引入动态帧率（DFR），通过压缩时间冗余，显著降低了语音识别错误率，并支持灵活的质量-比特率权衡，优于传统的固定帧率方法。", "motivation": "当前主流的神经语音编解码器采用固定帧率（FFR），为每个等时长片段分配相同数量的token。然而，语音在时间信息密度上是非均匀的，导致在长元音和静音等稳态片段上浪费了大量token。", "method": "CodecSlime是一种插件式方法，通过支持神经语音编解码器上的动态帧率（DFR）来压缩时间冗余。它是一种无监督且与架构无关的方法，结合了ScheDFR（用于推理）和Melt-and-Cool（用于训练）两个关键创新。", "result": "当集成到典型的VQ-GAN编解码器骨干并以40 Hz DFR（约600 bps）运行时，CodecSlime的重建WER相对于具有相同模型架构和相似比特率的传统FFR基线降低了高达46%，同时其他指标也具有竞争力。CodecSlime还实现了重建质量和比特率之间的灵活权衡：单个模型支持多帧率推理，并始终优于相应帧率下的FFR模型。", "conclusion": "CodecSlime通过引入动态帧率有效解决了神经语音编解码器中的时间冗余问题，显著提高了压缩效率和性能，并提供了灵活的质量-比特率控制，使其成为传统固定帧率方法的优越替代方案。", "translation": "神经语音编解码器已广泛应用于音频压缩和各种下游任务。当前主流的编解码器是固定帧率（FFR）的，它们为每个等时长切片分配相同数量的token。然而，语音在时间信息密度上是固有非均匀的。因此，许多token浪费在长元音和静音等稳态片段上。为了解决这种不匹配，我们提出了CodecSlime，这是一种插件式方法，首次通过支持神经语音编解码器上的动态帧率（DFR）来压缩时间冗余。我们的方法是无监督且与架构无关的，结合了ScheDFR和Melt-and-Cool两项关键创新，分别用于适应推理和训练。当集成到典型的VQ-GAN编解码器骨干并以40 Hz DFR（约600 bps）运行时，CodecSlime的重建WER相对于具有相同模型架构和相似比特率的传统FFR基线降低了高达46%，同时其他指标也具有竞争力。CodecSlime还实现了重建质量和比特率之间的灵活权衡：单个模型支持多帧率推理，并始终优于相应帧率下的FFR模型。音频样本可在https://acadarmeria.github.io/codecslime/获取。", "summary": "本文提出了CodecSlime，一种首次应用于神经语音编解码器的插件式动态帧率（DFR）方法，旨在解决固定帧率（FFR）编解码器在语音稳态段中浪费token的问题。该方法是无监督且与架构无关的，结合了ScheDFR和Melt-and-Cool进行推理和训练。实验结果表明，CodecSlime在相似比特率下可将语音识别错误率（WER）降低高达46%，并支持灵活的质量-比特率权衡，在多帧率推理方面持续优于FFR模型。", "keywords": "神经语音编解码器, 动态帧率, 时间冗余, 语音压缩, CodecSlime", "comments": "CodecSlime的创新点在于首次将动态帧率引入神经语音编解码器，以解决时间冗余问题。其插件式、无监督和架构无关的特性使其具有很强的通用性和实用性。通过显著降低WER并提供灵活的质量-比特率权衡，该方法对语音压缩领域具有重要意义。"}}
{"id": "2506.20683", "title": "Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG", "authors": ["Alexander Selivanov", "Philip Müller", "Özgün Turgut", "Nil Stolt-Ansó", "Daniel Rückert"], "summary": "An electrocardiogram (ECG) is a widely used, cost-effective tool for\ndetecting electrical abnormalities in the heart. However, it cannot directly\nmeasure functional parameters, such as ventricular volumes and ejection\nfraction, which are crucial for assessing cardiac function. Cardiac magnetic\nresonance (CMR) is the gold standard for these measurements, providing detailed\nstructural and functional insights, but is expensive and less accessible. To\nbridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive\nLearning), a multimodal contrastive learning framework that enhances ECG\nrepresentations by integrating spatio-temporal information from CMR. PTACL uses\nglobal patient-level contrastive loss and local temporal-level contrastive\nloss. The global loss aligns patient-level representations by pulling ECG and\nCMR embeddings from the same patient closer together, while pushing apart\nembeddings from different patients. Local loss enforces fine-grained temporal\nalignment within each patient by contrasting encoded ECG segments with\ncorresponding encoded CMR frames. This approach enriches ECG representations\nwith diagnostic information beyond electrical activity and transfers more\ninsights between modalities than global alignment alone, all without\nintroducing new learnable weights. We evaluate PTACL on paired ECG-CMR data\nfrom 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL\nachieves better performance in two clinically relevant tasks: (1) retrieving\npatients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac\nfunction parameters, such as ventricular volumes and ejection fraction. Our\nresults highlight the potential of PTACL to enhance non-invasive cardiac\ndiagnostics using ECG. The code is available at:\nhttps://github.com/alsalivan/ecgcmr", "comment": "accepted to MICCAI 2025 (Springer LNCS)", "pdf_url": "http://arxiv.org/pdf/2506.20683v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20683v1", "AI": {"title_translation": "用于心脏MRI和ECG联合表示的全局和局部对比学习", "tldr": "本研究提出PTACL，一个多模态对比学习框架，通过全局和局部对比损失，将心脏MRI（CMR）的时空信息整合到心电图（ECG）表示中，以提高ECG在心脏表型检索和功能参数预测方面的诊断能力。", "motivation": "心电图（ECG）虽成本效益高，但无法直接测量心室容积和射血分数等功能参数，而心脏磁共振（CMR）虽是金标准但昂贵且不易获得。为了弥合这一差距，本研究旨在通过整合CMR信息来增强ECG的表示能力。", "method": "本文提出了PTACL（Patient and Temporal Alignment Contrastive Learning），一个多模态对比学习框架。PTACL利用全局患者级对比损失来对齐来自同一患者的ECG和CMR嵌入，同时推开不同患者的嵌入。此外，它还使用局部时间级对比损失，通过对比编码的ECG片段与对应的编码CMR帧，强制在每个患者内部进行细粒度的时间对齐。此方法在不引入新可学习权重的情况下，丰富了ECG表示并促进了模态间信息传递。", "result": "PTACL在英国生物样本库的27,951名受试者的配对ECG-CMR数据上进行了评估。与基线方法相比，PTACL在两个临床相关任务中取得了更好的性能：1）检索具有相似心脏表型的患者；2）预测CMR衍生的心脏功能参数（如心室容积和射血分数）。", "conclusion": "PTACL的实验结果突出表明，该框架有潜力通过增强ECG来改进非侵入性心脏诊断。", "translation": "心电图（ECG）是一种广泛使用、成本效益高的工具，用于检测心脏的电生理异常。然而，它不能直接测量功能参数，如心室容积和射血分数，这些参数对评估心脏功能至关重要。心脏磁共振（CMR）是测量这些参数的金标准，能提供详细的结构和功能洞察，但其成本高昂且可及性较低。为了弥合这一差距，我们提出了PTACL（Patient and Temporal Alignment Contrastive Learning），一个多模态对比学习框架，通过整合来自CMR的时空信息来增强ECG表示。PTACL使用全局患者级对比损失和局部时间级对比损失。全局损失通过将来自同一患者的ECG和CMR嵌入拉近，同时将不同患者的嵌入推开，从而对齐患者级表示。局部损失通过对比编码的ECG片段与对应的编码CMR帧，在每个患者内部强制执行细粒度的时间对齐。这种方法在不引入新的可学习权重的情况下，用超越电活动的诊断信息丰富了ECG表示，并比单独的全局对齐在模态之间传递了更多的洞察。我们在英国生物样本库中来自27,951名受试者的配对ECG-CMR数据上评估了PTACL。与基线方法相比，PTACL在两个临床相关任务中取得了更好的性能：（1）检索具有相似心脏表型的患者；（2）预测CMR衍生的心脏功能参数，如心室容积和射血分数。我们的结果突出了PTACL通过使用ECG增强非侵入性心脏诊断的潜力。代码可在：https://github.com/alsalivan/ecgcmr 获得。", "summary": "本研究提出PTACL，一个创新的多模态对比学习框架，旨在通过整合心脏磁共振（CMR）的时空信息来增强心电图（ECG）的诊断能力。该框架结合了全局患者级和局部时间级对比损失，以实现ECG与CMR的联合表示。实验结果表明，PTACL在检索具有相似心脏表型的患者和预测CMR衍生的心脏功能参数方面，优于基线方法，展示了其在非侵入性心脏诊断中的巨大潜力。", "keywords": "对比学习, 心电图, 心脏磁共振, 多模态学习, 心脏功能预测", "comments": "PTACL的创新之处在于其结合了全局和局部对比学习策略，有效整合了多模态数据（ECG和CMR）的优势，从而在不增加模型复杂度的前提下，提升了单一模态（ECG）的诊断信息量。这对于成本效益高但信息有限的ECG而言，是一个重要的突破，有望推动非侵入性心脏诊断的发展。该方法在大量真实世界数据上的验证也增加了其可信度。"}}
{"id": "2506.20703", "title": "Generative Blocks World: Moving Things Around in Pictures", "authors": ["Vaibhav Vavilala", "Seemandhar Jain", "Rahul Vasanth", "D. A. Forsyth", "Anand Bhattad"], "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.", "comment": "23 pages, 16 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.20703v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20703v1", "AI": {"title_translation": "生成式积木世界：在图像中移动物体", "tldr": "本文提出了一种名为“生成式积木世界”的方法，通过操作简单的几何抽象体来编辑生成图像中的场景。该方法将场景表示为凸3D图元组合，并利用基于深度和纹理提示的流式方法生成图像，显著提升了视觉保真度、可编辑性和组合泛化能力。", "motivation": "旨在通过操纵简单的几何抽象体来与生成图像的场景进行交互和编辑。", "method": "该方法将场景表示为凸3D图元的组合，同一个场景可以用不同数量的图元表示，允许编辑者移动整体结构或小细节。一旦场景几何体被编辑，图像将通过一种基于深度和纹理提示的流式方法生成。纹理提示考虑了修改后的3D图元，超越了现有键值缓存技术提供的纹理一致性。", "result": "纹理提示能够实现准确的物体和相机移动，并基本保留了所描绘物体的身份。定量和定性实验表明，该方法在视觉保真度、可编辑性和组合泛化方面优于现有工作。", "conclusion": "该方法通过创新的场景表示和图像生成机制，显著提升了生成图像的编辑能力和视觉质量，超越了现有技术。", "translation": "我们描述了“生成式积木世界”，通过操纵简单的几何抽象体来与生成图像的场景进行交互。我们的方法将场景表示为凸3D图元的组合，同一个场景可以用不同数量的图元表示，允许编辑者移动整个结构或小细节。一旦场景几何体被编辑，图像将通过一种基于深度和纹理提示的流式方法生成。我们的纹理提示考虑了修改后的3D图元，超越了现有键值缓存技术提供的纹理一致性。这些纹理提示（a）允许准确的物体和相机移动，并且（b）在很大程度上保留了所描绘物体的身份。定量和定性实验表明，我们的方法在视觉保真度、可编辑性和组合泛化方面优于现有工作。", "summary": "本文提出了一种名为“生成式积木世界”的新方法，用于编辑生成图像中的场景。该方法通过将场景建模为可操作的凸3D图元集合，实现了对场景细节或整体结构的灵活编辑。图像生成采用了一种流式方法，结合了深度信息和创新的纹理提示，该纹理提示能有效处理修改后的3D图元，显著提升了纹理一致性。实验证明，该方法在视觉质量、编辑灵活性和泛化能力上均超越了现有技术。", "keywords": "生成式模型, 场景编辑, 3D图元, 纹理一致性, 图像生成", "comments": "这篇论文的创新点在于其将场景分解为可编辑的3D图元，并结合了考虑3D修改的纹理提示，从而在图像编辑中实现了高水平的纹理一致性和对象身份保留。这种方法对于需要精细控制生成图像内容的应用程序具有重要意义，尤其是在虚拟现实、游戏开发和创意设计领域。"}}
{"id": "2506.20817", "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation", "authors": ["Ali Tourani", "Fatemeh Nazary", "Yashar Deldjoo"], "summary": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec", "comment": "20 pages, 6 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.20817v1", "categories": ["cs.IR", "cs.MM"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20817v1", "AI": {"title_translation": "RAG-VisualRec：一个用于推荐系统中视觉和文本增强的检索增强生成开放资源", "tldr": "RAG-VisualRec是一个开放资源，它结合了LLM生成的剧情描述和预告片视觉嵌入，用于多模态电影推荐，并展示了召回率和NDCG的提升。", "motivation": "在电影领域，有限的元数据（例如，标题、类型）通常会阻碍鲁棒推荐的生成，因此需要开发多模态推荐系统来解决这一挑战。", "method": "该方法引入了一个资源，将LLM生成的剧情描述与预告片提取的视觉嵌入在一个统一的管道中结合起来，支持检索增强生成（RAG）和协同过滤。其核心是数据增强步骤，将稀疏元数据转换为更丰富的文本信号，并采用融合策略（如PCA、CCA）整合视觉线索。此外，还包含一个LLM驱动的重排序步骤。", "result": "实验评估表明，与单模态基线相比，基于CCA的融合显著提高了召回率，而LLM驱动的重排序步骤进一步改善了NDCG，尤其是在文本数据有限的情况下。", "conclusion": "该论文发布的框架鼓励进一步探索针对冷启动、注重新颖性和特定领域设置的多模态推荐技术。", "translation": "本文旨在解决电影领域开发多模态推荐系统所面临的挑战，即有限的元数据（例如，标题、类型）常常阻碍生成鲁棒的推荐。我们引入了一个资源，它将LLM生成的剧情描述与预告片提取的视觉嵌入在一个统一的管道中结合起来，支持检索增强生成（RAG）和协同过滤。我们方法的核心是数据增强步骤，将稀疏元数据转换为更丰富的文本信号，以及整合视觉线索的融合策略（例如，PCA、CCA）。实验评估表明，与单模态基线相比，基于CCA的融合显著提高了召回率，而LLM驱动的重排序步骤进一步改善了NDCG，尤其是在文本数据有限的情况下。通过发布此框架，我们邀请对针对冷启动、注重新颖性和特定领域设置的多模态推荐技术进行进一步探索。所有代码、数据和详细文档均可在以下网址公开获取：https://github.com/RecSys-lab/RAG-VisualRec", "summary": "RAG-VisualRec是一个开放资源，旨在解决电影推荐中元数据有限的问题，通过整合LLM生成的剧情描述和预告片视觉嵌入来增强推荐。它采用数据增强、融合策略（如CCA、PCA）以及LLM驱动的重排序，以实现多模态推荐，并展示了召回率和NDCG的显著提升。", "keywords": "多模态推荐, 检索增强生成, 视觉嵌入, 冷启动, 电影推荐", "comments": "该论文提供了一个有价值的开放资源，解决了多模态推荐中常见的稀疏元数据挑战。它创新性地利用LLM生成的描述和预告片视觉嵌入，结合融合技术和重排序，为更鲁棒和细致的推荐系统（特别是冷启动场景）提供了有前景的方向。代码和数据的公开发布对未来的研究非常有益。"}}
{"id": "2506.20747", "title": "Towards Probabilistic Question Answering Over Tabular Data", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20747v1", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20747v1", "AI": {"title_translation": "面向表格数据的概率问答", "tldr": "本文提出了LUCARIO基准和框架，用于处理大型表格数据上的概率问答，通过从表格中推断贝叶斯网络并将自然语言查询转换为概率查询，结合大型语言模型生成答案，显著优于基线方法。", "motivation": "现有的表格数据问答方法（如NL2SQL系统）在事实性问题上表现良好，但对于需要不确定性推理的概率性问题表现不佳。", "method": "本文引入了一个新的基准LUCARIO和一个用于大型表格数据上概率问答的框架。该方法从表格中推断贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。", "result": "实证结果表明，与基线方法相比有显著改进，突出了混合符号-神经推理的优势。", "conclusion": "混合符号-神经推理的方法在处理表格数据的概率问答方面表现出显著优势，能够有效解决现有方法在不确定性推理上的不足。", "translation": "当前针对表格数据的问答（QA）方法，例如NL2SQL系统，在答案可以直接从表格中检索的事实性问题上表现良好。然而，它们在需要不确定性推理的概率性问题上表现不足。在本文中，我们引入了一个新的基准LUCARIO和一个用于大型表格数据上概率问答的框架。我们的方法从表格中推断贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。实证结果表明，与基线方法相比有显著改进，突出了混合符号-神经推理的优势。", "summary": "本文针对现有表格数据问答系统在处理不确定性概率问题上的不足，提出了一个名为LUCARIO的新基准和框架。该框架通过从表格数据中构建贝叶斯网络，将自然语言问题转化为概率查询，并结合大型语言模型生成答案。实验结果表明，该混合符号-神经推理方法在概率问答任务上显著优于现有基线。", "keywords": "概率问答, 表格数据, 贝叶斯网络, 大型语言模型, LUCARIO", "comments": "该论文的创新点在于提出了一个专门针对表格数据上概率问答的新基准和框架，填补了现有NL2SQL系统在不确定性推理方面的空白。其结合贝叶斯网络和大型语言模型的混合方法，为处理复杂概率查询提供了有效途径，展示了符号推理与神经网络结合的潜力。"}}
{"id": "2506.20945", "title": "A Multi-Stage Framework for Multimodal Controllable Speech Synthesis", "authors": ["Rui Niu", "Weihao Wu", "Jie Chen", "Long Ma", "Zhiyong Wu"], "summary": "Controllable speech synthesis aims to control the style of generated speech\nusing reference input, which can be of various modalities. Existing face-based\nmethods struggle with robustness and generalization due to data quality\nconstraints, while text prompt methods offer limited diversity and fine-grained\ncontrol. Although multimodal approaches aim to integrate various modalities,\ntheir reliance on fully matched training data significantly constrains their\nperformance and applicability. This paper proposes a 3-stage multimodal\ncontrollable speech synthesis framework to address these challenges. For face\nencoder, we use supervised learning and knowledge distillation to tackle\ngeneralization issues. Furthermore, the text encoder is trained on both\ntext-face and text-speech data to enhance the diversity of the generated\nspeech. Experimental results demonstrate that this method outperforms\nsingle-modal baseline methods in both face based and text prompt based speech\nsynthesis, highlighting its effectiveness in generating high-quality speech.", "comment": "Accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2506.20945v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.20945v1", "AI": {"title_translation": "多阶段多模态可控语音合成框架", "tldr": "本文提出一个三阶段多模态可控语音合成框架，通过解决现有方法在鲁棒性、泛化性和多样性方面的挑战，生成高质量语音。", "motivation": "现有基于面部的可控语音合成方法因数据质量受限于鲁棒性和泛化性，而文本提示方法在多样性和精细控制方面有限。多模态方法则受限于对完全匹配训练数据的依赖，影响其性能和适用性。", "method": "本文提出了一个三阶段多模态可控语音合成框架。针对人脸编码器，采用监督学习和知识蒸馏来解决泛化问题。此外，文本编码器在文本-人脸和文本-语音数据上进行训练，以增强生成语音的多样性。", "result": "实验结果表明，该方法在基于人脸和基于文本提示的语音合成方面均优于单模态基线方法，突出了其在生成高质量语音方面的有效性。", "conclusion": "所提出的多阶段多模态框架有效解决了现有可控语音合成方法的局限性，显著提升了生成语音的质量、鲁棒性和多样性。", "translation": "可控语音合成旨在利用各种模态的参考输入来控制生成语音的风格。现有基于人部的方法由于数据质量限制，在鲁棒性和泛化性方面存在困难，而文本提示方法则提供有限的多样性和精细控制。尽管多模态方法旨在整合各种模态，但它们对完全匹配训练数据的依赖显著限制了其性能和适用性。本文提出了一个三阶段多模态可控语音合成框架来解决这些挑战。对于人脸编码器，我们使用监督学习和知识蒸馏来解决泛化问题。此外，文本编码器在文本-人脸和文本-语音数据上进行训练，以增强生成语音的多样性。实验结果表明，该方法在基于人脸和基于文本提示的语音合成方面均优于单模态基线方法，突出了其在生成高质量语音方面的有效性。", "summary": "本文提出了一种三阶段多模态可控语音合成框架，旨在克服现有面部和文本提示方法的局限性。该框架通过对人脸编码器采用监督学习和知识蒸馏提升泛化能力，并对文本编码器在多源数据上训练以增强语音多样性。实验证明，该方法在高质量语音生成方面优于单模态基线。", "keywords": "多模态, 可控语音合成, 多阶段框架, 知识蒸馏, 语音多样性", "comments": "这篇论文的创新点在于提出了一个多阶段框架来整合多模态输入，并针对性地解决了现有方法的痛点，如数据质量导致的泛化性问题（通过知识蒸馏）以及文本提示的多样性不足（通过多源数据训练）。其多阶段设计和对不同模态编码器的优化，提升了可控语音合成的鲁棒性、泛化性和多样性，具有重要的实践意义。"}}
{"id": "2506.20777", "title": "Inverse initial data reconstruction for Maxwell's equations via time-dimensional reduction method", "authors": ["Thuy T. Le", "Cong B. Van", "Trong D. Dang", "Loc H. Nguyen"], "summary": "We study an inverse problem for the time-dependent Maxwell system in an\ninhomogeneous and anisotropic medium. The objective is to recover the initial\nelectric field $\\mathbf{E}_0$ in a bounded domain $\\Omega \\subset\n\\mathbb{R}^3$, using boundary measurements of the electric field and its normal\nderivative over a finite time interval. Informed by practical constraints, we\nadopt an under-determined formulation of Maxwell's equations that avoids the\nneed for initial magnetic field data and charge density information. To address\nthis inverse problem, we develop a time-dimension reduction approach by\nprojecting the electric field onto a finite-dimensional Legendre\npolynomial-exponential basis in time. This reformulates the original space-time\nproblem into a sequence of spatial systems for the projection coefficients. The\nreconstruction is carried out using the quasi-reversibility method within a\nminimum-norm framework, which accommodates the inherent non-uniqueness of the\nunder-determined setting. We prove a convergence theorem that ensures the\nquasi-reversibility solution approximates the true solution as the noise and\nregularization parameters vanish. Numerical experiments in a fully\nthree-dimensional setting validate the method's performance. The reconstructed\ninitial electric field remains accurate even with $10\\%$ noise in the data,\ndemonstrating the robustness and applicability of the proposed approach to\nrealistic inverse electromagnetic problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20777v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20777v1", "AI": {"title_translation": "麦克斯韦方程组初始数据逆向重构的时间维度约简方法", "tldr": "通过时间维度约简和准可逆性方法，从边界测量中重构非均匀各向异性介质中麦克斯韦方程组的初始电场，即使有噪声数据也表现出鲁棒性。", "motivation": "本研究旨在解决非均匀和各向异性介质中时变麦克斯韦系统的一个逆问题，即利用有限时间间隔内电场及其法向导数的边界测量来恢复有界域内的初始电场。此方法还通过欠定公式避免了对初始磁场数据和电荷密度信息的需要，以适应实际约束。", "method": "1. 采用麦克斯韦方程组的欠定公式。2. 开发了一种时间维度约简方法，将电场投影到有限维勒让德多项式-指数基上，将原始时空问题转化为一系列空间系统。3. 使用准可逆性方法在最小范数框架内进行重构。4. 证明了收敛定理，确保准可逆性解的近似性。", "result": "1. 数值实验验证了该方法在完全三维设置下的性能。2. 即使数据中存在10%的噪声，重构的初始电场仍然准确。3. 证明了当噪声和正则化参数消失时，准可逆性解逼近真实解。", "conclusion": "提出的时间维度约简方法结合准可逆性方法，能够鲁棒且准确地从边界测量中重构非均匀各向异性介质中麦克斯韦方程组的初始电场，即使在有噪声数据的情况下也适用，对实际逆电磁问题具有重要意义。", "translation": "我们研究了非均匀各向异性介质中时变麦克斯韦系统的一个逆问题。目标是利用有限时间间隔内电场及其法向导数的边界测量，恢复有界域 $\\Omega \\subset \\mathbb{R}^3$ 中的初始电场 $\\mathbf{E}_0$。根据实际约束，我们采用了麦克斯韦方程组的欠定公式，避免了对初始磁场数据和电荷密度信息的需要。为了解决这个逆问题，我们开发了一种时间维度约简方法，通过将电场投影到时间上的有限维勒让德多项式-指数基上。这使得原始时空问题被重构为一系列关于投影系数的空间系统。重构是使用准可逆性方法在最小范数框架内进行的，这适应了欠定设置固有的非唯一性。我们证明了一个收敛定理，确保当噪声和正则化参数消失时，准可逆性解逼近真实解。在完全三维环境中的数值实验验证了该方法的性能。即使数据中存在10%的噪声，重构的初始电场仍然准确，这表明所提出方法对实际逆电磁问题的鲁棒性和适用性。", "summary": "本文研究了非均匀各向异性介质中麦克斯韦方程组的初始电场逆问题。通过采用欠定公式并引入时间维度约简方法，将原始时空问题转化为一系列空间系统。利用准可逆性方法在最小范数框架下进行重构，并证明了其收敛性。数值实验表明，该方法在存在噪声的情况下也能准确重构初始电场，具有良好的鲁棒性和实际应用潜力。", "keywords": "麦克斯韦方程组, 逆问题, 时间维度约简, 准可逆性, 初始电场重构", "comments": "该论文的创新点在于引入了时间维度约简方法（基于Legendre多项式-指数基）将时空逆问题转化为一系列空间系统，并结合准可逆性方法有效处理了欠定设置下的非唯一性问题。其重要性体现在解决了在实际约束下（无需初始磁场和电荷密度）重构麦克斯韦方程组初始电场的难题，且在有噪声数据下仍表现出高准确性和鲁棒性，具有重要的实际应用价值。抽象中未提及具体限制，但可能需要进一步探讨基函数选择、计算成本或特定介质条件对方法性能的影响。"}}
{"id": "2506.20926", "title": "CodeGuard: A Generalized and Stealthy Backdoor Watermarking for Generative Code Models", "authors": ["Haoxuan Li", "Jiale Zhang", "Xiaobing Sun", "Xiapu Luo"], "summary": "Generative code models (GCMs) significantly enhance development efficiency\nthrough automated code generation and code summarization. However, building and\ntraining these models require computational resources and time, necessitating\neffective digital copyright protection to prevent unauthorized leaks and\nmisuse. Backdoor watermarking, by embedding hidden identifiers, simplifies\ncopyright verification by breaking the model's black-box nature. Current\nbackdoor watermarking techniques face two main challenges: first, limited\ngeneralization across different tasks and datasets, causing fluctuating\nverification rates; second, insufficient stealthiness, as watermarks are easily\ndetected and removed by automated methods. To address these issues, we propose\nCodeGuard, a novel watermarking method combining attention mechanisms with\ndistributed trigger embedding strategies. Specifically, CodeGuard employs\nattention mechanisms to identify watermark embedding positions, ensuring\nverifiability. Moreover, by using homomorphic character replacement, it avoids\nmanual detection, while distributed trigger embedding reduces the likelihood of\nautomated detection. Experimental results demonstrate that CodeGuard achieves\nup to 100% watermark verification rates in both code summarization and code\ngeneration tasks, with no impact on the primary task performance. In terms of\nstealthiness, CodeGuard performs exceptionally, with a maximum detection rate\nof only 0.078 against ONION detection methods, significantly lower than\nbaseline methods.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.20926v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20926v1", "AI": {"title_translation": "CodeGuard：一种用于生成式代码模型的通用隐蔽后门水印技术", "tldr": "CodeGuard是一种新型后门水印技术，通过结合注意力机制和分布式触发器嵌入策略，解决了现有技术在生成式代码模型版权保护中面临的通用性和隐蔽性不足问题，实现了高验证率和极低的检测率。", "motivation": "生成式代码模型（GCMs）的开发和训练成本高昂，需要有效的数字版权保护来防止未经授权的泄露和滥用。现有后门水印技术面临两大挑战：一是泛化能力有限，导致验证率波动；二是隐蔽性不足，容易被自动检测和移除。", "method": "CodeGuard提出了一种结合注意力机制和分布式触发器嵌入策略的新型水印方法。它利用注意力机制识别水印嵌入位置以确保可验证性；通过同形字符替换避免人工检测；通过分布式触发器嵌入降低自动检测的可能性。", "result": "CodeGuard在代码摘要和代码生成任务中均实现了高达100%的水印验证率，且不影响主任务性能。在隐蔽性方面表现出色，对抗ONION检测方法的最大检测率仅为0.078，远低于基线方法。", "conclusion": "CodeGuard成功解决了生成式代码模型后门水印技术在泛化性和隐蔽性方面的挑战，提供了一种高效且难以检测的版权保护方案。", "translation": "生成式代码模型（GCMs）通过自动化代码生成和代码摘要显著提高了开发效率。然而，构建和训练这些模型需要大量的计算资源和时间，因此需要有效的数字版权保护来防止未经授权的泄露和滥用。后门水印技术通过嵌入隐藏标识符，打破了模型的黑盒性质，从而简化了版权验证。当前后门水印技术面临两大主要挑战：首先，在不同任务和数据集上的泛化能力有限，导致验证率波动；其次，隐蔽性不足，水印容易被自动化方法检测和移除。为了解决这些问题，我们提出了CodeGuard，一种结合注意力机制和分布式触发器嵌入策略的新型水印方法。具体来说，CodeGuard采用注意力机制来识别水印嵌入位置，确保可验证性。此外，通过使用同形字符替换，它避免了人工检测，而分布式触发器嵌入则降低了自动检测的可能性。实验结果表明，CodeGuard在代码摘要和代码生成任务中均实现了高达100%的水印验证率，且对主要任务性能没有影响。在隐蔽性方面，CodeGuard表现异常出色，对抗ONION检测方法的最大检测率仅为0.078，显著低于基线方法。", "summary": "本论文提出了CodeGuard，一种针对生成式代码模型的新型后门水印技术，旨在解决现有方法在版权保护方面存在的泛化能力和隐蔽性不足的问题。CodeGuard通过结合注意力机制识别水印嵌入位置，并利用同形字符替换和分布式触发器嵌入策略来增强隐蔽性。实验证明，CodeGuard在代码摘要和代码生成任务中均能达到100%的水印验证率，且不影响模型主任务性能，同时其对抗自动检测的隐蔽性远超现有基线方法。", "keywords": "CodeGuard, 后门水印, 生成式代码模型, 版权保护, 隐蔽性", "comments": "CodeGuard的创新之处在于其结合了注意力机制和分布式触发器嵌入策略，有效解决了现有后门水印技术在通用性和隐蔽性上的痛点。特别是同形字符替换的应用，提升了水印对人工检测的抵抗力，而分布式触发器则增强了对自动化检测的鲁棒性。该研究对于生成式代码模型的知识产权保护具有重要意义，为AI模型版权保护提供了新的思路和高效的解决方案。"}}
{"id": "2506.20851", "title": "Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach", "authors": ["Srikar Reddy Gadusu", "Larry Callahan", "Samir Lababidi", "Arunasri Nishtala", "Sophia Healey", "Hande McGinty"], "summary": "As data and knowledge expand rapidly, adopting systematic methodologies for\nontology generation has become crucial. With the daily increases in data\nvolumes and frequent content changes, the demand for databases to store and\nretrieve information for the creation of knowledge graphs has become\nincreasingly urgent. The previously established Knowledge Acquisition and\nRepresentation Methodology (KNARM) outlines a systematic approach to address\nthese challenges and create knowledge graphs. However, following this\nmethodology highlights the existing challenge of seamlessly integrating Neo4j\ndatabases with the Web Ontology Language (OWL). Previous attempts to integrate\ndata from Neo4j into an ontology have been discussed, but these approaches\noften require an understanding of description logics (DL) syntax, which may not\nbe familiar to many users. Thus, a more accessible method is necessary to\nbridge this gap. This paper presents a user-friendly approach that utilizes\nPython and its rdflib library to support ontology development. We showcase our\nnovel approach through a Neo4j database we created by integrating data from the\nFood and Drug Administration (FDA) Adverse Event Reporting System (FAERS)\ndatabase. Using this dataset, we developed a Python script that automatically\ngenerates the required classes and their axioms, facilitating a smoother\nintegration process. This approach offers a practical solution to the\nchallenges of ontology generation in the context of rapidly growing adverse\ndrug event datasets, supporting improved drug safety monitoring and public\nhealth decision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20851v1", "categories": ["cs.SE", "cs.AI", "cs.DB"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20851v1", "AI": {"title_translation": "通过自动化集成数据生成可靠不良事件档案（GRAPH-AID）：一种半自动化本体构建方法", "tldr": "本论文提出了一种用户友好的半自动化方法，利用Python和rdflib库将Neo4j数据库中的不良事件数据集成到OWL本体中，以解决现有方法对描述逻辑语法要求高的问题，从而支持药物安全监测。", "motivation": "随着数据和知识的快速增长，本体生成系统化方法变得至关重要。数据量和内容变化日益频繁，对存储和检索信息以创建知识图谱的数据库需求日益紧迫。虽然KNARM方法论为解决这些挑战提供了系统方法，但它突显了Neo4j数据库与Web本体语言（OWL）无缝集成存在的挑战。现有将Neo4j数据集成到本体中的尝试通常需要用户理解描述逻辑（DL）语法，这对于许多用户来说可能不熟悉，因此需要一种更易于访问的方法来弥补这一差距。", "method": "本研究提出了一种用户友好的方法，利用Python及其rdflib库支持本体开发。作者通过集成美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）数据库的数据创建了一个Neo4j数据库，并以此展示了他们的新方法。他们开发了一个Python脚本，自动生成所需的类及其公理，从而促进了更顺畅的集成过程。", "result": "该方法成功地将FDA不良事件报告系统（FAERS）的数据从Neo4j数据库集成到OWL本体中，并通过自动生成类和公理的Python脚本，实现了平滑的集成过程。这提供了一个实用的解决方案，应对快速增长的不良药物事件数据集背景下的本体生成挑战。", "conclusion": "该论文提供了一种实用的解决方案，解决了在快速增长的不良药物事件数据集背景下本体生成所面临的挑战，支持改进药物安全监测和公共卫生决策。", "translation": "随着数据和知识的快速扩展，采用系统方法进行本体生成已变得至关重要。随着数据量的日常增加和内容的频繁变化，对存储和检索信息以创建知识图谱的数据库的需求变得日益紧迫。先前建立的知识获取和表示方法论（KNARM）概述了一种系统方法来解决这些挑战并创建知识图谱。然而，遵循这种方法论突出了将Neo4j数据库与Web本体语言（OWL）无缝集成存在的现有挑战。之前将Neo4j数据集成到本体中的尝试已被讨论，但这些方法通常需要理解描述逻辑（DL）语法，这可能对许多用户不熟悉。因此，需要一种更易于访问的方法来弥补这一差距。本文提出了一种用户友好的方法，利用Python及其rdflib库支持本体开发。我们通过我们创建的Neo4j数据库展示了我们的新方法，该数据库通过集成美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）数据库的数据而创建。使用该数据集，我们开发了一个Python脚本，自动生成所需的类及其公理，从而促进了更顺畅的集成过程。这种方法为在快速增长的不良药物事件数据集背景下本体生成所面临的挑战提供了一个实用解决方案，支持改进药物安全监测和公共卫生决策。", "summary": "本论文提出了一种名为GRAPH-AID的半自动化本体构建方法，旨在解决将Neo4j数据库与OWL本体集成时面临的挑战，特别是现有方法对描述逻辑语法要求高的问题。研究人员利用Python及其rdflib库开发了一个用户友好的方法，并通过集成FDA不良事件报告系统（FAERS）数据构建的Neo4j数据库进行了展示。他们开发了一个Python脚本，能够自动生成本体所需的类和公理，从而实现了数据向本体的平滑集成。该方法为在快速增长的不良药物事件数据背景下进行本体生成提供了一个实用且可访问的解决方案，有助于提升药物安全监测和公共卫生决策的效率。", "keywords": "本体构建, Neo4j, OWL, Python, 不良事件报告", "comments": "该论文的创新之处在于提供了一种半自动化且用户友好的方法来桥接Neo4j数据库和OWL本体之间的集成鸿沟，特别是在处理不良事件数据方面。它通过利用Python和rdflib库，并开发自动化脚本来生成本体结构，降低了对用户描述逻辑语法知识的要求，提高了本体构建的可访问性。这对于快速变化和增长的数据集（如药物不良事件数据）尤其重要，有助于改进药物安全监测和公共卫生决策，具有重要的实践意义。"}}
{"id": "2506.20952", "title": "Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment", "authors": ["Kyosuke Ishibashi", "Atsushi Saito", "Zin Y. Tun", "Lucas Ray", "Megan C. Coram", "Akihiro Sakurai", "Allison M. Okamura", "Ko Yamamoto"], "summary": "Human crowd simulation in virtual reality (VR) is a powerful tool with\npotential applications including emergency evacuation training and assessment\nof building layout. While haptic feedback in VR enhances immersive experience,\nits effect on walking behavior in dense and dynamic pedestrian flows is\nunknown. Through a user study, we investigated how haptic feedback changes user\nwalking motion in crowded pedestrian flows in VR. The results indicate that\nhaptic feedback changed users' collision avoidance movements, as measured by\nincreased walking trajectory length and change in pelvis angle. The\ndisplacements of users' lateral position and pelvis angle were also increased\nin the instantaneous response to a collision with a non-player character (NPC),\neven when the NPC was inside the field of view. Haptic feedback also enhanced\nusers' awareness and visual exploration when an NPC approached from the side\nand back. Furthermore, variation in walking speed was increased by the haptic\nfeedback. These results suggested that the haptic feedback enhanced users'\nsensitivity to a collision in VR environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20952v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20952v1", "AI": {"title_translation": "触觉反馈对动态VR行人环境中避让行为和视觉探索的影响", "tldr": "研究发现VR中的触觉反馈会改变用户在拥挤行人流中的避碰行为和视觉探索，提高对碰撞的敏感性。", "motivation": "虚拟现实中的人群模拟在紧急疏散训练和建筑布局评估中有潜在应用，但触觉反馈对密集动态行人流中行走行为的影响尚不明确。", "method": "通过一项用户研究，调查触觉反馈如何改变用户在VR拥挤行人流中的行走运动。测量指标包括行走轨迹长度、骨盆角度变化、侧向位置位移、骨盆角度瞬时响应以及行走速度变化。", "result": "触觉反馈改变了用户的碰撞避让运动，表现为行走轨迹长度增加和骨盆角度变化。即使NPC在视野内，用户对与NPC碰撞的瞬时响应中，侧向位置和骨盆角度的位移也增加了。当NPC从侧面和后面接近时，触觉反馈增强了用户的意识和视觉探索。触觉反馈增加了行走速度的变化。", "conclusion": "这些结果表明触觉反馈增强了用户在VR环境中对碰撞的敏感性。", "translation": "虚拟现实（VR）中的人群模拟是一个强大的工具，在紧急疏散训练和建筑布局评估等领域具有潜在应用。尽管VR中的触觉反馈能增强沉浸式体验，但其对密集动态行人流中行走行为的影响尚不明确。通过一项用户研究，我们调查了触觉反馈如何改变用户在VR拥挤行人流中的行走运动。结果表明，触觉反馈改变了用户的碰撞避让运动，表现为行走轨迹长度增加和骨盆角度变化。即使当非玩家角色（NPC）在视野内时，用户对与NPC碰撞的瞬时响应中，侧向位置和骨盆角度的位移也增加了。触觉反馈还增强了当NPC从侧面和后面接近时用户的意识和视觉探索。此外，触觉反馈增加了行走速度的变化。这些结果表明触觉反馈增强了用户在VR环境中对碰撞的敏感性。", "summary": "这项研究探讨了虚拟现实中触觉反馈对用户在拥挤行人流中行为的影响。通过用户研究发现，触觉反馈显著改变了用户的避碰动作，增加了行走轨迹长度、骨盆角度变化以及对碰撞的瞬时反应。它还提升了用户对侧面和后面接近的NPC的感知和视觉探索，并增加了行走速度的变异性。研究结论是触觉反馈能增强用户在VR环境中对碰撞的敏感度。", "keywords": "触觉反馈, 虚拟现实, 行人行为, 碰撞避让, 视觉探索", "comments": "这项研究创新性地探讨了触觉反馈在VR行人模拟中的具体作用，揭示了其对避让行为和空间感知的重要影响。研究结果对于提升VR沉浸感、优化虚拟训练（如紧急疏散）以及改进虚拟环境中的人机交互具有重要意义。局限性可能在于用户研究的规模和特定VR设备的使用。"}}
{"id": "2506.20918", "title": "Metadata Enrichment of Long Text Documents using Large Language Models", "authors": ["Manika Lamba", "You Peng", "Sophie Nikolov", "Glen Layne-Worthey", "J. Stephen Downie"], "summary": "In this project, we semantically enriched and enhanced the metadata of long\ntext documents, theses and dissertations, retrieved from the HathiTrust Digital\nLibrary in English published from 1920 to 2020 through a combination of manual\nefforts and large language models. This dataset provides a valuable resource\nfor advancing research in areas such as computational social science, digital\nhumanities, and information science. Our paper shows that enriching metadata\nusing LLMs is particularly beneficial for digital repositories by introducing\nadditional metadata access points that may not have originally been foreseen to\naccommodate various content types. This approach is particularly effective for\nrepositories that have significant missing data in their existing metadata\nfields, enhancing search results and improving the accessibility of the digital\nrepository.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20918v1", "categories": ["cs.DL", "cs.ET", "cs.IR"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.20918v1", "AI": {"title_translation": "使用大型语言模型丰富长文本元数据", "tldr": "本项目利用人工和大型语言模型对HathiTrust数字图书馆的长文本（论文和学位论文）元数据进行了语义丰富和增强，该方法对数字存储库尤其有益，能改善搜索结果和可访问性。", "motivation": "现有数字存储库的元数据可能存在大量缺失数据，且未预见到各种内容类型的访问点，导致搜索结果不佳和可访问性受限。", "method": "通过结合人工努力和大型语言模型，对从HathiTrust数字图书馆检索到的1920年至2020年间出版的英文长文本文档、论文和学位论文的元数据进行语义丰富和增强。", "result": "该方法为数字存储库引入了额外的元数据访问点，有效解决了现有元数据字段中数据大量缺失的问题。", "conclusion": "使用大型语言模型丰富元数据对数字存储库特别有益，能够增强搜索结果并提高数字存储库的可访问性。", "translation": "本项目结合人工努力和大型语言模型，对从HathiTrust数字图书馆检索到的1920年至2020年间出版的英文长文本文档、论文和学位论文的元数据进行了语义丰富和增强。该数据集为计算社会科学、数字人文和信息科学等领域的研究提供了宝贵的资源。我们的论文表明，使用大型语言模型丰富元数据对于数字存储库特别有益，因为它引入了可能最初未预见到的额外元数据访问点，以适应各种内容类型。这种方法对于现有元数据字段中存在大量缺失数据的存储库尤其有效，能增强搜索结果并提高数字存储库的可访问性。", "summary": "本文介绍了一个项目，该项目通过结合人工和大型语言模型，对HathiTrust数字图书馆中1920年至2020年间的英文长文本（包括论文和学位论文）的元数据进行了语义丰富和增强。研究表明，利用大型语言模型进行元数据丰富，能为数字存储库提供更多访问点，尤其对元数据缺失严重的存储库而言，显著提升了搜索效率和可访问性，为计算社会科学、数字人文和信息科学等领域提供了宝贵资源。", "keywords": "元数据丰富, 大型语言模型, 数字图书馆, 长文本, HathiTrust", "comments": "该论文提出了一种利用大型语言模型和人工协同工作来丰富长文本元数据的方法，这在提升数字图书馆和存储库的数据可发现性与可访问性方面具有创新性。其重要性在于解决了传统元数据缺失和不足的问题，尤其对于大规模历史文献数据集，能显著改善用户体验和研究效率。该方法的普适性使其可能应用于其他类似的数据管理场景。"}}
{"id": "2506.20812", "title": "Model-Based Real-Time Pose and Sag Estimation of Overhead Power Lines Using LiDAR for Drone Inspection", "authors": ["Alexandre Girard", "Steven A. Parkison", "Philippe Hamelin"], "summary": "Drones can inspect overhead power lines while they remain energized,\nsignificantly simplifying the inspection process. However, localizing a drone\nrelative to all conductors using an onboard LiDAR sensor presents several\nchallenges: (1) conductors provide minimal surface for LiDAR beams limiting the\nnumber of conductor points in a scan, (2) not all conductors are consistently\ndetected, and (3) distinguishing LiDAR points corresponding to conductors from\nother objects, such as trees and pylons, is difficult. This paper proposes an\nestimation approach that minimizes the error between LiDAR measurements and a\nsingle geometric model representing the entire conductor array, rather than\ntracking individual conductors separately. Experimental results, using data\nfrom a power line drone inspection, demonstrate that this method achieves\naccurate tracking, with a solver converging under 50 ms per frame, even in the\npresence of partial observations, noise, and outliers. A sensitivity analysis\nshows that the estimation approach can tolerate up to twice as many outlier\npoints as valid conductors measurements.", "comment": "Submitted to IEEE case 2025", "pdf_url": "http://arxiv.org/pdf/2506.20812v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20812v1", "AI": {"title_translation": "架空电力线基于模型的实时姿态和垂度LiDAR无人机检查估计", "tldr": "提出了一种基于LiDAR的电力线模型化估计方法，用于无人机检查中的姿态和垂度估计，解决了现有技术在数据稀疏、检测不一致和点云区分上的挑战，实现了快速准确的跟踪。", "motivation": "现有无人机LiDAR检查架空电力线面临挑战：导体表面小导致LiDAR点少；并非所有导体都能持续检测；难以区分导体点与其他物体（如树木、电线杆）。", "method": "提出一种估计方法，通过最小化LiDAR测量值与代表整个导体阵列的单一几何模型之间的误差，而不是单独跟踪每个导体。", "result": "实验结果表明该方法实现了精确跟踪，求解器每帧收敛时间少于50毫秒，即使在部分观测、噪声和异常值存在的情况下也能表现良好。敏感性分析显示，该估计方法能容忍高达两倍于有效导体测量值的异常点。", "conclusion": "该模型化估计方法为无人机LiDAR检查架空电力线提供了一种鲁棒且高效的解决方案，克服了传统方法在数据稀疏性和噪声下的局限性。", "translation": "无人机可以在带电状态下检查架空电力线，这大大简化了检查过程。然而，使用机载LiDAR传感器定位无人机相对于所有导体存在几个挑战：(1) 导体为LiDAR光束提供的表面极小，限制了每次扫描中导体点的数量，(2) 并非所有导体都能持续检测到，以及 (3) 区分对应于导体的LiDAR点与其他物体（如树木和电线杆）的点很困难。本文提出了一种估计方法，该方法通过最小化LiDAR测量值与代表整个导体阵列的单一几何模型之间的误差，而不是单独跟踪每个导体。使用电力线无人机检查数据进行的实验结果表明，该方法实现了精确跟踪，即使在部分观测、噪声和异常值存在的情况下，求解器也能在每帧50毫秒内收敛。敏感性分析表明，该估计方法可以容忍多达两倍于有效导体测量值的异常点。", "summary": "本文提出一种基于LiDAR的架空电力线模型化估计方法，旨在解决无人机检查中导体LiDAR数据稀疏、检测不一致及点云区分困难等挑战。该方法通过最小化LiDAR测量与整体导体几何模型间的误差进行估计，而非独立跟踪每根导体。实验证明，该方法能实现精确跟踪，求解器每帧收敛时间小于50毫秒，并对部分观测、噪声和异常值具有高鲁棒性。", "keywords": "无人机检查, LiDAR, 电力线, 姿态估计, 垂度估计, 模型化估计", "comments": "该研究通过引入一种整体几何模型来解决LiDAR数据稀疏和噪声干扰下的电力线跟踪问题，具有创新性。其高效的实时性能和对异常点的鲁棒性，对于提升无人机电力线检查的自动化和可靠性具有重要意义。"}}
{"id": "2506.20971", "title": "Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)", "authors": ["Shihui Feng", "Huilin Zhang", "Dragan Gašević"], "summary": "In this study, we analyze 2,398 research articles published between 2020 and\n2024 across eight core venues related to the field of Artificial Intelligence\nin Education (AIED). Using a three-step knowledge co-occurrence network\nanalysis, we analyze the knowledge structure of the field, the evolving\nknowledge clusters, and the emerging frontiers. Our findings reveal that AIED\nresearch remains strongly technically focused, with sustained themes such as\nintelligent tutoring systems, learning analytics, and natural language\nprocessing, alongside rising interest in large language models (LLMs) and\ngenerative artificial intelligence (GenAI). By tracking the bridging keywords\nover the past five years, we identify four emerging frontiers in AIED--LLMs,\nGenAI, multimodal learning analytics, and human-AI collaboration. The current\nresearch interests in GenAI are centered around GAI-driven personalization,\nself-regulated learning, feedback, assessment, motivation, and ethics.The key\nresearch interests and emerging frontiers in AIED reflect a growing emphasis on\nco-adaptive, human-centered AI for education. This study provides the first\nlarge-scale field-level mapping of AIED's transformation in the GenAI era and\nsheds light on the future research development and educational practices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20971v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20971v1", "AI": {"title_translation": "AIED 何去何从？关键主题与新兴前沿 (2020-2024)", "tldr": "本研究分析了2020-2024年间2398篇AIED研究文章，利用知识共现网络分析揭示了该领域的知识结构、演变趋势和新兴前沿。研究发现AIED仍以技术为中心，并识别出大型语言模型（LLMs）、生成式AI（GenAI）、多模态学习分析和人机协作作为新兴前沿，强调以人为中心的AI教育发展趋势。", "motivation": "该研究旨在分析2020年至2024年期间AIED领域的研究文章，以揭示该领域的知识结构、演变中的知识集群以及新兴前沿，并提供AIED在生成式AI时代转型的首次大规模领域级映射，从而为未来的研究发展和教育实践提供启示。", "method": "研究分析了2020年至2024年间在八个核心会议上发表的2,398篇AIED研究文章。采用三步知识共现网络分析方法，分析了知识结构、演变知识集群和新兴前沿，并通过追踪过去五年的桥接关键词来识别新兴前沿。", "result": "研究发现AIED研究仍强烈关注技术，持续的主题包括智能辅导系统、学习分析和自然语言处理，同时对大型语言模型（LLMs）和生成式人工智能（GenAI）的兴趣日益增长。识别出AIED的四个新兴前沿：LLMs、GenAI、多模态学习分析和人机协作。目前GenAI的研究兴趣集中在GAI驱动的个性化、自主学习、反馈、评估、动机和伦理。", "conclusion": "AIED的关键研究兴趣和新兴前沿反映出对协同自适应、以人为中心的教育AI的日益重视。该研究首次大规模地映射了AIED在生成式AI时代的转型，并为未来的研究发展和教育实践提供了启示。", "translation": "在这项研究中，我们分析了2020年至2024年期间在八个与教育人工智能（AIED）领域相关的核心场所发表的2,398篇研究文章。通过采用三步知识共现网络分析，我们分析了该领域的知识结构、不断演变的知识集群以及新兴前沿。我们的研究结果显示，AIED研究仍然强烈地以技术为中心，持续的主题包括智能辅导系统、学习分析和自然语言处理，同时对大型语言模型（LLMs）和生成式人工智能（GenAI）的兴趣日益增长。通过追踪过去五年中的桥接关键词，我们确定了AIED的四个新兴前沿——LLMs、GenAI、多模态学习分析和人机协作。目前GenAI的研究兴趣集中在GAI驱动的个性化、自主学习、反馈、评估、动机和伦理。AIED中的关键研究兴趣和新兴前沿反映出对协同自适应、以人为中心的教育AI的日益重视。这项研究首次提供了AIED在GenAI时代转型的大规模领域级映射，并为未来的研究发展和教育实践提供了启示。", "summary": "本研究通过对2020-2024年间2398篇AIED论文进行三步知识共现网络分析，系统地描绘了该领域的知识结构、演变趋势及新兴前沿。研究发现AIED仍以技术为重心，并识别出LLMs、GenAI、多模态学习分析和人机协作等四个新兴前沿。特别指出GenAI在个性化、自主学习、反馈、评估、动机和伦理方面的应用兴趣。该研究强调了AIED向协同自适应、以人为中心的AI教育发展的趋势，并首次提供了AIED在GenAI时代转型的宏观图景，对未来研究和实践具有指导意义。", "keywords": "教育人工智能, 大型语言模型, 生成式AI, 知识共现网络分析, 新兴前沿", "comments": "这项研究的创新之处在于它首次对AIED领域在生成式AI时代进行大规模、领域级的转型映射。通过结合大规模文献分析和知识共现网络分析，它提供了对该领域当前状态和未来方向的宝贵见解，特别是在LLMs和GenAI兴起背景下。研究结果对于研究人员、政策制定者和教育实践者理解AIED的发展趋势及其对教育的潜在影响具有重要意义。"}}
{"id": "2506.20965", "title": "Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment", "authors": ["Craig Steven Wright"], "summary": "This paper integrates Austrian capital theory with repeated game theory to\nexamine strategic miner behaviour under different institutional conditions in\nblockchain systems. It shows that when protocol rules are mutable, effective\ntime preference rises, undermining rational long-term planning and cooperative\nequilibria. Using formal game-theoretic analysis and Austrian economic\nprinciples, the paper demonstrates how mutable protocols shift miner incentives\nfrom productive investment to political rent-seeking and influence games. The\noriginal Bitcoin protocol is interpreted as an institutional anchor: a fixed\nrule-set enabling calculability and low time preference. Drawing on the work of\nBohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability\nis essential for restoring strategic coherence, entrepreneurial confidence, and\nsustainable network equilibrium.", "comment": "Approximately 10,770 words, 0 figure, 0 table. Submitted to The\n  Quarterly Journal of Austrian Economics", "pdf_url": "http://arxiv.org/pdf/2506.20965v1", "categories": ["econ.GN", "cs.CR", "cs.GT", "cs.NI", "q-fin.EC", "q-fin.GN", "91B42, 91A25, 91B50", "K.4.4; J.4; C.2.4"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.20965v1", "AI": {"title_translation": "理性矿工行为、协议稳定性与时间偏好：比特币激励环境的奥地利学派与博弈论分析", "tldr": "论文结合奥地利资本理论和重复博弈论，分析了可变协议如何提高矿工的时间偏好，导致寻租行为，并指出比特币协议的不可变性对网络稳定的重要性。", "motivation": "探讨区块链系统中不同制度条件下战略矿工的行为，特别是协议可变性对矿工激励和长期规划的影响。", "method": "结合奥地利资本理论和重复博弈论，进行正式的博弈论分析。", "result": "当协议规则可变时，有效时间偏好会上升，从而破坏理性的长期规划和合作均衡，并将矿工的激励从生产性投资转向政治寻租和影响力博弈。", "conclusion": "协议的不可变性对于恢复战略连贯性、创业信心和可持续网络均衡至关重要。原始比特币协议被视为一个制度锚点，能够实现可计算性和低时间偏好。", "translation": "本文将奥地利资本理论与重复博弈论相结合，考察了区块链系统在不同制度条件下的战略矿工行为。研究表明，当协议规则可变时，有效时间偏好会上升，从而破坏理性的长期规划和合作均衡。本文运用正式的博弈论分析和奥地利经济学原理，论证了可变协议如何将矿工的激励从生产性投资转向政治寻租和影响力博弈。原始比特币协议被解释为一个制度锚点：一个固定的规则集，能够实现可计算性和低时间偏好。借鉴庞巴维克、米塞斯和哈耶克的工作，本文提出协议的不可变性对于恢复战略连贯性、创业信心和可持续网络均衡至关重要。", "summary": "本文结合奥地利资本理论和重复博弈论，分析了区块链系统中矿工的策略行为。研究发现，可变的协议规则会提高矿工的时间偏好，从而损害长期规划并促使矿工从生产性投资转向寻租活动。论文强调了原始比特币协议作为固定规则集的重要性，认为协议的不可变性对于维持网络稳定、促进创业信心至关重要。", "keywords": "矿工行为, 协议稳定性, 时间偏好, 奥地利经济学, 博弈论, 比特币", "comments": "这篇论文创新性地将奥地利经济学派的时间偏好理论与博弈论相结合，为理解区块链协议设计中协议可变性对矿工激励和网络稳定性的影响提供了新的视角。其强调比特币协议不可变性的重要性，对区块链治理和长期发展具有重要的理论和实践意义。"}}
{"id": "2506.21487", "title": "OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits", "authors": ["Maryam Ghane", "Amir M. Hajisadeghi", "Hamid R. Zarandi"], "summary": "This paper presents OptGM, an optimized gate merging method designed to\nmitigate negative bias temperature instability (NBTI) in digital circuits.\nFirst, the proposed approach effectively identifies NBTI-critical internal\nnodes, defined as those with a signal probability exceeding a predefined\nthreshold. Next, based on the proposed optimized algorithm, the sensitizer gate\n(which drives the critical node) and the sensitive gate (which is fed by it)\nare merged into a new complex gate. This complex gate preserves the original\nlogic while eliminating NBTI-critical nodes. Finally, to evaluate the\neffectiveness of OptGM, we assess it on several combinational and sequential\nbenchmark circuits. Simulation results demonstrate that, on average, the number\nof NBTI-critical transistors (i.e., PMOS transistors connected to critical\nnodes), NBTI-induced delay degradation, and the total transistor count are\nreduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances\nperformance per cost (PPC) by 12.8% on average, with minimal area overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21487v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21487v1", "AI": {"title_translation": "OptGM：一种优化门合并方法以减轻数字电路中的NBTI效应", "tldr": "OptGM是一种优化门合并方法，通过识别并消除NBTI关键节点，有效减轻数字电路中的NBTI效应，同时显著减少关键晶体管数量和延迟退化，并提升性能功耗比。", "motivation": "数字电路中的负偏压温度不稳定性（NBTI）是一个重要的可靠性问题，会导致电路性能退化。本文旨在提出一种有效的方法来减轻NBTI效应。", "method": "本文提出了OptGM方法。首先，识别信号概率超过预设阈值的NBTI关键内部节点。其次，基于优化的算法，将驱动关键节点的敏感门和被其馈入的敏感门合并成一个新的复杂门，该复杂门在保留原始逻辑的同时消除了NBTI关键节点。最后，在多种组合和时序基准电路上评估OptGM的有效性。", "result": "仿真结果表明，OptGM平均减少了89.29%的NBTI关键晶体管（即连接到关键节点的PMOS晶体管），23.87%的NBTI引起的延迟退化，以及6.47%的总晶体管数量。此外，OptGM平均提升了12.8%的性能功耗比（PPC），且面积开销极小。", "conclusion": "OptGM是一种有效的门合并方法，能够显著减轻数字电路中的NBTI效应，同时带来晶体管数量减少、延迟退化降低以及性能功耗比提升等多方面益处，且面积开销小。", "translation": "本文提出了一种名为OptGM的优化门合并方法，旨在减轻数字电路中的负偏压温度不稳定性（NBTI）。首先，该方法有效地识别NBTI关键内部节点，这些节点被定义为信号概率超过预设阈值的节点。接下来，基于所提出的优化算法，驱动关键节点的敏感门和被其馈入的敏感门被合并成一个新的复杂门。这个复杂门在保留原始逻辑的同时消除了NBTI关键节点。最后，为了评估OptGM的有效性，我们在几个组合和时序基准电路上进行了评估。仿真结果表明，NBTI关键晶体管（即连接到关键节点的PMOS晶体管）、NBTI引起的延迟退化以及总晶体管数量平均分别减少了89.29%、23.87%和6.47%。此外，OptGM平均将性能功耗比（PPC）提高了12.8%，且面积开销极小。", "summary": "OptGM是一种优化的门合并方法，旨在减轻数字电路中的NBTI效应。该方法通过识别信号概率高的NBTI关键内部节点，并将驱动和被驱动这些关键节点的门合并为新的复杂门，从而在保持逻辑功能的同时消除关键节点。在基准电路上的评估显示，OptGM显著减少了NBTI关键晶体管数量（89.29%）、NBTI引起的延迟退化（23.87%）和总晶体管数量（6.47%），同时以最小的面积开销平均提升了12.8%的性能功耗比。", "keywords": "NBTI, 门合并, 数字电路, 延迟退化, 性能功耗比", "comments": "本文提出了一种新颖的门合并方法OptGM，通过针对性地消除NBTI关键节点来缓解数字电路的NBTI效应。其创新点在于结合了关键节点识别和优化的门合并算法，并在减少NBTI影响的同时实现了显著的性能提升和面积优化，具有重要的实际应用价值。"}}
{"id": "2506.21362", "title": "Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation", "authors": ["Chang Liu", "Yixin Wang", "Moontae Lee"], "summary": "Efficient access to high-quality information is vital for online platforms.\nTo promote more useful information, users not only create new content but also\nevaluate existing content, often through helpfulness voting. Although\naggregated votes help service providers rank their user content, these votes\nare often biased by disparate accessibility per position and the cascaded\ninfluence of prior votes. For a fairer assessment of information quality, we\npropose the Counterfactual Voting Adjustment (CVA), a causal framework that\naccounts for the context in which individual votes are cast. Through\npreliminary and semi-synthetic experiments, we show that CVA effectively models\nthe position and herding biases, accurately recovering the predefined content\nquality. In a real experiment, we demonstrate that reranking content based on\nthe learned quality by CVA exhibits stronger alignment with both user sentiment\nand quality evaluation assessed by GPT-4o, outperforming system rankings based\non aggregated votes and model-based rerankings without causal inference. Beyond\nthe individual quality inference, our embeddings offer comparative insights\ninto the behavioral dynamics of expert user groups across 120 major\nStackExchange communities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21362v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21362v1", "AI": {"title_translation": "基于有用性评估的在线平台质量评估和更公平投票的反事实投票调整", "tldr": "在线平台上的有用性投票常因位置和先前投票而存在偏差，导致内容质量评估不公。本文提出了反事实投票调整（CVA），一个因果框架，用于校正这些偏差，从而实现更公平、准确的内容质量评估和排名。", "motivation": "在线平台上的内容有用性投票常受到位置差异和先前投票级联效应的偏见，这阻碍了高效获取高质量信息并导致了不公平的内容评估。因此，需要一种方法来更公平地评估信息质量。", "method": "本文提出了反事实投票调整（CVA），这是一个因果框架，用于解释个人投票所处的上下文。CVA旨在有效地建模和消除位置偏见和羊群效应偏见，以准确恢复内容的真实质量。", "result": "通过初步和半合成实验，CVA被证明能有效建模位置和羊群偏见，并准确恢复预定义的内容质量。在真实实验中，基于CVA学习到的质量进行的内容重新排名，与用户情绪和GPT-4o评估的质量表现出更强的一致性，优于基于汇总投票的系统排名以及没有因果推断的模型重新排名。此外，CVA的嵌入还提供了对120个主要StackExchange社区中专家用户群体行为动态的比较性见解。", "conclusion": "CVA通过解决在线平台投票中的固有偏差，提供了一种更公平、更准确的内容质量评估方法，从而显著改善了内容排名并提供了对用户行为的深入洞察。", "translation": "在线平台高效获取高质量信息至关重要。为了推广更有用的信息，用户不仅创建新内容，还会通过有用性投票来评估现有内容。尽管汇总投票有助于服务提供商对用户内容进行排名，但这些投票常常受到不同位置可访问性以及先前投票级联影响的偏见。为了更公平地评估信息质量，我们提出了反事实投票调整（CVA），这是一个因果框架，它考虑了个人投票所处的上下文。通过初步和半合成实验，我们表明CVA有效地模拟了位置和羊群偏见，准确地恢复了预定义的内容质量。在真实实验中，我们证明基于CVA学习到的质量对内容进行重新排名，与用户情绪和GPT-4o评估的质量都表现出更强的一致性，优于基于汇总投票的系统排名以及没有因果推断的模型重新排名。除了个体质量推断之外，我们的嵌入还在120个主要StackExchange社区中提供了对专家用户群体行为动态的比较性见解。", "summary": "本文提出了反事实投票调整（CVA），一个因果框架，旨在纠正在线平台上有用性投票中存在的偏见（如位置偏见和羊群效应），从而实现更公平、准确的内容质量评估。实验结果表明，CVA能够有效地建模这些偏见，准确恢复真实内容质量，并且基于CVA学习到的质量对内容进行重新排名，与用户情绪和GPT-4o评估的质量表现出更强的一致性，优于传统方法。此外，CVA的嵌入还为分析专家用户行为动态提供了有价值的见解。", "keywords": "反事实投票调整, 因果推断, 在线平台, 质量评估, 投票偏见", "comments": "本文的创新之处在于引入了一个因果框架（CVA）来纠正在线平台有用性投票中的固有偏差，这超越了传统的投票聚合或非因果模型。其重要性在于，通过提供更公平、更准确的内容质量评估，它对于在线平台有效获取高质量信息至关重要。在真实实验中利用GPT-4o进行质量评估是一个值得注意的现代化验证。此外，从嵌入中获得对专家用户群体行为动态的额外洞察也增加了研究的价值。"}}
{"id": "2506.20815", "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications", "authors": ["Xinye Tang", "Haijun Zhai", "Chaitanya Belwal", "Vineeth Thayanithi", "Philip Baumann", "Yogesh K Roy"], "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20815v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20815v1", "AI": {"title_translation": "面向领域特定AI应用的动态上下文感知提示推荐", "tldr": "本文提出了一种动态上下文感知的提示推荐系统，用于解决领域特定AI应用中高质量提示的生成难题，并通过实验证明了其有效性和实用性。", "motivation": "LLM（大型语言模型）驱动的应用极易受到用户提示质量的影响，尤其对于领域特定的应用，创建高质量的提示往往具有挑战性。", "method": "本系统结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排序来生成相关且可操作的提示建议。它利用行为遥测和两阶段分层推理过程来动态选择和排序相关技能，并使用预定义和通过少量学习增强的自适应模板来合成提示。", "result": "在真实世界数据集上的实验表明，该方法实现了高实用性和相关性，并通过自动化和专家评估得到了验证。", "conclusion": "该动态上下文感知提示推荐系统有效解决了领域特定AI应用中高质量提示的生成挑战，显著提升了LLM应用的表现。", "translation": "LLM驱动的应用极易受到用户提示质量的影响，尤其对于领域特定的应用，创建高质量的提示往往具有挑战性。本文提出了一种新颖的面向领域特定AI应用的动态上下文感知提示推荐系统。我们的解决方案结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排序，以生成相关且可操作的提示建议。该系统利用行为遥测和两阶段分层推理过程来动态选择和排序相关技能，并使用预定义和通过少量学习增强的自适应模板来合成提示。在真实世界数据集上的实验表明，我们的方法实现了高实用性和相关性，并通过自动化和专家评估得到了验证。", "summary": "本文提出了一种新颖的动态上下文感知提示推荐系统，专为领域特定AI应用设计。该系统通过整合上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排序，旨在生成高质量、相关且可操作的提示建议。它利用行为遥测和两阶段分层推理过程动态选择和排序技能，并通过结合预定义与少量学习增强的自适应模板来合成提示。实验结果表明，该方法在实用性和相关性方面表现出色，有效提升了LLM驱动应用的性能。", "keywords": "提示推荐, 领域特定AI, LLM应用, 上下文感知, 检索增强", "comments": "该论文的创新点在于其提出的动态上下文感知提示推荐系统，它通过多方面技术结合（如检索增强、分层技能组织和自适应排序）来解决领域特定AI应用中提示质量的痛点。这对于提升LLM在专业领域应用的可用性和效率具有重要意义。"}}
{"id": "2506.20675", "title": "Utility-Driven Speculative Decoding for Mixture-of-Experts", "authors": ["Anish Saxena", "Po-An Tsai", "Hritvik Taneja", "Aamer Jaleel", "Moinuddin Qureshi"], "summary": "GPU memory bandwidth is the main bottleneck for low-latency Large Language\nModel (LLM) inference. Speculative decoding leverages idle GPU compute by using\na lightweight drafter to propose K tokens, which the LLM verifies in parallel,\nboosting token throughput. In conventional dense LLMs, all model weights are\nfetched each iteration, so speculation adds no latency overhead. Emerging\nMixture of Experts (MoE) models activate only a subset of weights per token,\ngreatly reducing data movement. However, we show that speculation is\nineffective for MoEs: draft tokens collectively activate more weights,\nincreasing data movement and verification time by 2-3x. When token throughput\ngains fail to offset this overhead, speculation causes slowdowns up to 1.5x,\nmaking it infeasible. Even when useful, the optimal K varies by task, model,\nand even between requests and iterations. Thus, despite widespread use in dense\nLLMs, speculation remains impractical in leading MoEs.\n  We present Cascade, a utility-driven framework that selectively enables\nspeculation to avoid slowdowns and dynamically tunes K to accelerate MoE\nserving. Cascade uses a lightweight metric, speculation utility, the ratio of\ntoken gains to verification cost, which shows iteration-level locality,\nenabling periodic decisions via short test and longer set phases. For each\nrequest, Cascade disables speculation if utility drops below one during\ntesting, and when utility exceeds one, tests multiple K-values to choose the\nutility-maximizing K for the set phase. We implement Cascade in vLLM and\nevaluate it on five popular MoEs with workloads spanning code, math,\nextraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and\nimproves throughput by 7-14% over static K, making speculative decoding\npractical for MoEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20675v1", "categories": ["cs.DC", "cs.AI", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20675v1", "AI": {"title_translation": "效用驱动的专家混合模型推测解码", "tldr": "推测解码在MoE模型中效率低下并可能导致减速。本文提出了Cascade框架，通过动态调整和选择性启用推测解码来使其在MoE模型中实用化，显著提高了吞吐量并避免了减速。", "motivation": "GPU内存带宽是LLM推理的瓶颈。推测解码通过使用轻量级草稿器提出K个token并由LLM并行验证来提高token吞吐量。然而，研究发现推测解码对新兴的专家混合（MoE）模型无效，因为它导致数据移动和验证时间增加2-3倍，甚至可能导致高达1.5倍的减速。这使得推测解码在MoE模型中不实用，即使有用，最佳K值也因任务、模型和请求而异。因此，需要一种方法来解决推测解码在MoE模型中的局限性。", "method": "本文提出了Cascade，一个效用驱动的框架，用于选择性地启用推测解码以避免减速，并动态调整K值以加速MoE服务。Cascade使用一个轻量级指标——推测效用（token收益与验证成本之比），该指标显示出迭代级别的局部性，从而可以通过短测试阶段和长设置阶段进行周期性决策。对于每个请求，如果效用在测试期间低于1，Cascade会禁用推测解码；如果效用超过1，则测试多个K值以选择在设置阶段最大化效用的K值。该框架在vLLM中实现，并在五种流行的MoE模型上进行了评估。", "result": "Cascade框架将推测解码导致的减速限制在5%以内（相比于无Cascade时的1.5倍减速），并且比静态K值方法提高了7-14%的吞吐量。", "conclusion": "Cascade框架使得推测解码在专家混合（MoE）模型中变得实用，显著解决了其在MoE模型中因增加数据移动和验证时间而导致的效率低下和减速问题。", "translation": "GPU内存带宽是低延迟大型语言模型（LLM）推理的主要瓶颈。推测解码利用空闲的GPU计算能力，通过使用轻量级草稿器提出K个token，然后由LLM并行验证，从而提高token吞吐量。在传统的密集型LLM中，每次迭代都会获取所有模型权重，因此推测解码不会增加延迟开销。新兴的专家混合（MoE）模型每个token只激活一部分权重，大大减少了数据移动。然而，我们发现推测解码对MoE模型无效：草稿token会共同激活更多的权重，使数据移动和验证时间增加2-3倍。当token吞吐量增益无法抵消这一开销时，推测解码会导致高达1.5倍的减速，使其不可行。即使有用，最佳K值也因任务、模型，甚至请求和迭代而异。因此，尽管在密集型LLM中广泛使用，推测解码在领先的MoE模型中仍然不实用。\n我们提出了Cascade，一个效用驱动的框架，它选择性地启用推测解码以避免减速，并动态调整K值以加速MoE服务。Cascade使用一个轻量级指标——推测效用，即token收益与验证成本之比，该指标显示出迭代级别的局部性，从而可以通过短测试阶段和长设置阶段进行周期性决策。对于每个请求，如果效用在测试期间低于1，Cascade会禁用推测解码；当效用超过1时，它会测试多个K值以选择在设置阶段最大化效用的K值。我们在vLLM中实现了Cascade，并在五种流行的MoE模型上进行了评估，工作负载涵盖代码、数学、提取和混合任务。Cascade将减速限制在5%以内（相比于1.5倍），并比静态K值方法提高了7-14%的吞吐量，使得推测解码在MoE模型中变得实用。", "summary": "该论文解决了推测解码在专家混合（MoE）模型中效率低下且可能导致性能下降的问题。尽管推测解码在密集型LLM中能有效提高吞吐量，但对于MoE模型，它会显著增加数据移动和验证时间，导致高达1.5倍的减速。为解决此问题，作者提出了Cascade框架，该框架通过引入“推测效用”指标，能够选择性地启用推测解码并在运行时动态调整K值。Cascade在测试阶段评估效用，若效用低则禁用推测，若效用高则选择最佳K值。实验结果表明，Cascade成功将推测解码导致的减速限制在5%以内，并相对静态K值方法提高了7-14%的吞吐量，从而使推测解码在MoE模型中变得实用。", "keywords": "推测解码, 专家混合模型, LLM推理, 效用驱动, 动态调整", "comments": "这篇论文的创新点在于提出了一个实用的框架Cascade，解决了推测解码在专家混合（MoE）模型中遇到的核心挑战。以往，推测解码在MoE模型中因其特有的稀疏激活模式而导致性能下降，使其难以应用。Cascade通过引入“推测效用”这一动态指标，并结合测试/设置阶段的自适应策略，有效地规避了减速风险并优化了K值选择，从而显著提高了MoE模型的推理效率。这一工作的重要性在于，它使得一种在密集型LLM中行之有效的优化技术得以推广到新兴且高效的MoE架构中，为未来LLM的低延迟推理提供了新的方向。"}}
{"id": "2506.20757", "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20757v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20757v1", "AI": {"title_translation": "ConViTac: 对齐视觉-触觉融合与对比表示", "tldr": "ConViTac通过引入对比表示和对比嵌入条件（CEC）机制，有效解决了视觉-触觉融合中特征对齐不佳的问题，显著提升了机器人感知任务的性能。", "motivation": "机器人视觉和触觉是互补的感知模态，但现有联合学习视觉-触觉表示的方法常依赖直接组合（如特征相加和拼接），导致模态融合时特征整合效果不佳。", "method": "本文提出了ConViTac，一个视觉-触觉表示学习网络。其核心是对比嵌入条件（CEC）机制，该机制利用通过自监督对比学习预训练的对比编码器，将视觉和触觉输入投射到统一的潜在嵌入中。这些嵌入通过跨模态注意力来耦合视觉-触觉特征融合，以实现统一表示的对齐并提升下游任务性能。", "result": "ConViTac在真实世界实验中表现出优于当前最先进方法的性能。所提出的CEC机制有效性得到证实，在材料分类和抓取预测任务中，准确率提高了高达12.0%。", "conclusion": "ConViTac通过创新的对比表示和对比嵌入条件（CEC）机制，成功解决了视觉-触觉融合中的特征对齐挑战，从而显著提升了机器人感知和操作任务的准确性和鲁棒性。", "translation": "视觉和触觉是机器人两种基本的感觉模态，提供互补信息，增强感知和操作任务。先前的研究试图联合学习视觉-触觉表示以提取更有意义的信息。然而，这些方法通常依赖于直接组合，例如特征相加和拼接，进行模态融合，这往往导致特征整合不佳。在本文中，我们提出了ConViTac，一个视觉-触觉表示学习网络，旨在通过对比表示增强融合过程中的特征对齐。我们的主要贡献是对比嵌入条件（CEC）机制，该机制利用通过自监督对比学习预训练的对比编码器将视觉和触觉输入投射到统一的潜在嵌入中。这些嵌入用于通过跨模态注意力耦合视觉-触觉特征融合，旨在对齐统一表示并增强下游任务的性能。我们进行了广泛的实验，证明了ConViTac在真实世界中优于当前最先进的方法，以及我们提出的CEC机制的有效性，该机制在材料分类和抓取预测任务中将准确率提高了高达12.0%。", "summary": "本文提出了ConViTac，一个旨在通过对比表示改进视觉-触觉融合的表示学习网络。为了解决现有融合方法中特征整合不佳的问题，ConViTac引入了对比嵌入条件（CEC）机制，利用自监督对比学习预训练的编码器将多模态输入映射到统一的潜在嵌入，并通过跨模态注意力实现特征的有效对齐。实验结果表明，ConViTac在真实世界的材料分类和抓取预测任务中显著优于现有技术，准确率提升高达12.0%。", "keywords": "视觉-触觉融合, 对比学习, 机器人感知, 特征对齐, 多模态学习", "comments": "ConViTac的创新点在于将对比学习引入视觉-触觉融合领域，通过其独特的对比嵌入条件（CEC）机制，有效地解决了多模态特征对齐这一关键挑战。这种方法为机器人感知和操作任务提供了更鲁棒和高性能的解决方案，对于未来多模态机器人学习具有重要意义。"}}
{"id": "2506.20699", "title": "On Context-Content Uncertainty Principle", "authors": ["Xin Li"], "summary": "The Context-Content Uncertainty Principle (CCUP) proposes that inference\nunder uncertainty is governed by an entropy asymmetry between context and\ncontent: high-entropy contexts must be interpreted through alignment with\nlow-entropy, structured content. In this paper, we develop a layered\ncomputational framework that derives operational principles from this\nfoundational asymmetry. At the base level, CCUP formalizes inference as\ndirectional entropy minimization, establishing a variational gradient that\nfavors content-first structuring. Building upon this, we identify four\nhierarchical layers of operational principles: (\\textbf{L1}) \\emph{Core\nInference Constraints}, including structure-before-specificity, asymmetric\ninference flow, cycle-consistent bootstrapping, and conditional compression,\nall shown to be mutually reducible; (\\textbf{L2}) \\emph{Resource Allocation\nPrinciples}, such as precision-weighted attention, asymmetric learning rates,\nand attractor-based memory encoding; (\\textbf{L3}) \\emph{Temporal Bootstrapping\nDynamics}, which organize learning over time via structure-guided curricula;\nand (\\textbf{L4}) \\emph{Spatial Hierarchical Composition}, which integrates\nthese mechanisms into self-organizing cycles of memory, inference, and\nplanning. We present formal equivalence theorems, a dependency lattice among\nprinciples, and computational simulations demonstrating the efficiency gains of\nCCUP-aligned inference. This work provides a unified theoretical foundation for\nunderstanding how brains and machines minimize uncertainty through recursive\nstructure-specificity alignment. The brain is not just an inference machine. It\nis a cycle-consistent entropy gradient resolver, aligning structure and\nspecificity via path-dependent, content-seeded simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20699v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20699v1", "AI": {"title_translation": "关于上下文-内容不确定性原理", "tldr": "上下文-内容不确定性原理（CCUP）提出，不确定性下的推断受上下文与内容之间的熵不对称性支配。本文开发了一个分层计算框架，从这一不对称性中推导出操作原则，为大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了统一的理论基础。", "motivation": "本文的动机是基于上下文-内容不确定性原理（CCUP），该原理认为不确定性下的推断受上下文和内容之间熵不对称性的支配，即高熵上下文必须通过与低熵、结构化内容的对齐来解释。研究旨在开发一个分层计算框架，从这一基本不对称性中推导出操作原则，并为大脑和机器如何最小化不确定性提供统一的理论基础。", "method": "本文开发了一个分层计算框架，该框架源自上下文-内容不确定性原理（CCUP）。它将推断形式化为方向性熵最小化，建立了一个有利于内容优先结构化的变分梯度。在此基础上，研究确定了四个操作原则的层次结构层：核心推断约束（L1）、资源分配原则（L2）、时间自举动力学（L3）和空间分层组合（L4）。", "result": "本文提出了形式等价定理，展示了原则之间的依赖关系格，并通过计算模拟证明了与CCUP对齐的推断具有效率增益。这项工作为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。", "conclusion": "这项工作为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。它将大脑重新定义为一个循环一致的熵梯度解析器，通过路径依赖的、内容引导的模拟来对齐结构和特异性。", "translation": "上下文-内容不确定性原理（CCUP）提出，不确定性下的推断受制于上下文与内容之间的熵不对称性：高熵的上下文必须通过与低熵、结构化内容的对齐来解释。在本文中，我们开发了一个分层计算框架，从这一基本不对称性中推导出操作原则。在基础层面，CCUP 将推断形式化为方向性熵最小化，建立了一个有利于内容优先结构化的变分梯度。在此基础上，我们确定了四个操作原则的层次结构层：（L1）核心推断约束，包括结构优先于特异性、非对称推断流、循环一致性自举和条件压缩，所有这些都被证明可以相互归约；（L2）资源分配原则，例如精度加权注意力、非对称学习率和基于吸引子的记忆编码；（L3）时间自举动力学，通过结构引导的课程组织随时间的学习；以及（L4）空间分层组合，它将这些机制整合到记忆、推断和规划的自组织循环中。我们提出了形式等价定理、原则之间的依赖关系格以及计算模拟，展示了与CCUP对齐的推断的效率增益。这项工作为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了统一的理论基础。大脑不仅仅是一个推断机器。它是一个循环一致的熵梯度解析器，通过路径依赖的、内容引导的模拟来对齐结构和特异性。", "summary": "本文引入了上下文-内容不确定性原理（CCUP），该原理认为不确定性下的推断依赖于熵不对称性，即高熵上下文需与低熵、结构化内容对齐。作者开发了一个分层计算框架，将推断形式化为方向性熵最小化。他们从CCUP中推导并确定了四个层次的操作原则：核心推断约束、资源分配原则、时间自举动力学和空间分层组合。该工作提出了形式定理和模拟结果，证明了效率增益，为大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了统一的理论基础。", "keywords": "上下文-内容不确定性原理, 熵不对称性, 推断, 计算框架, 不确定性最小化", "comments": "本文提出了一个新颖的理论框架——CCUP，为生物和人工智能中的不确定性最小化提供了一个统一的视角。其创新之处在于从一个基本的熵不对称性中推导出具体的、分层的操作原则，为理解复杂的认知过程提供了一种结构化的方法。其中“方向性熵最小化”的概念以及所识别的层级原则尤为创新，可能为未来设计更高效、更具类脑特性的AI系统提供指导。"}}
{"id": "2506.21126", "title": "Semantic-aware Digital Twin for AI-based CSI Acquisition", "authors": ["Jiajia Guo", "Yiming Cui", "Shi Jin"], "summary": "Artificial intelligence (AI) substantially enhances channel state information\n(CSI) acquisition performance but is limited by its reliance on single-modality\ninformation and deployment challenges, particularly in dataset collection. This\npaper investigates the use of semantic-aware digital twin (DT) to enhance\nAI-based CSI acquisition. We first briefly introduce the motivation and recent\nadvancements in AI-driven CSI acquisition and semantic-aware DT employment for\nair interfaces. Then, we thoroughly explore how semantic-aware DT can bolster\nAI-based CSI acquisition. We categorizes the semantic-aware DT for AI-based CSI\nacquisition into two classes: enhancing AI-based CSI acquisition through\nintegration with DT and using DT to aid AI-based CSI deployment. Potential\nintegration frameworks are introduced in detail. Finally, we conclude by\noutlining potential research directions within the semantic-aware DT-assisted\nAI-based CSI acquisition.", "comment": "This article has been accepted by IEEE Communications Standards\n  Magazine", "pdf_url": "http://arxiv.org/pdf/2506.21126v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21126v1", "AI": {"title_translation": "基于AI的CSI采集的语义感知数字孪生", "tldr": "AI增强CSI采集受限于单模态信息和部署挑战，本文探讨语义感知数字孪生如何提升AI-based CSI采集，包括集成和辅助部署两类。", "motivation": "人工智能（AI）显著提升了信道状态信息（CSI）采集性能，但受限于对单模态信息的依赖以及部署挑战，特别是在数据集收集方面。", "method": "本文首先介绍了AI驱动CSI采集和语义感知数字孪生在空口应用中的动机和进展。然后，详细探讨了语义感知数字孪生如何增强AI-based CSI采集，将其分为通过与DT集成来增强和使用DT辅助AI-based CSI部署两类。文中还介绍了潜在的集成框架。", "result": "Not mentioned in abstract", "conclusion": "总结并概述了语义感知数字孪生辅助AI-based CSI采集中的潜在研究方向。", "translation": "标题：基于AI的CSI采集的语义感知数字孪生\n摘要：人工智能（AI）显著提升了信道状态信息（CSI）采集性能，但受限于对单模态信息的依赖和部署挑战，特别是在数据集收集方面。本文研究了使用语义感知数字孪生（DT）来增强基于AI的CSI采集。我们首先简要介绍了AI驱动的CSI采集的动机和最新进展，以及语义感知DT在空口应用中的部署。然后，我们深入探讨了语义感知DT如何增强基于AI的CSI采集。我们将用于基于AI的CSI采集的语义感知DT分为两类：通过与DT集成增强基于AI的CSI采集，以及使用DT辅助基于AI的CSI部署。详细介绍了潜在的集成框架。最后，我们总结并概述了语义感知DT辅助基于AI的CSI采集中的潜在研究方向。", "summary": "本文探讨了语义感知数字孪生（DT）如何解决AI驱动信道状态信息（CSI）采集面临的单模态信息依赖和部署挑战。研究将语义感知DT应用于AI-based CSI采集分为与DT集成和DT辅助部署两类，并介绍了潜在的集成框架，最后指出了未来的研究方向。", "keywords": "语义感知数字孪生, AI, CSI采集, 信道状态信息, 数字孪生", "comments": "本文提出了将语义感知数字孪生应用于AI-based CSI采集的新颖视角，旨在解决AI在CSI采集中面临的数据集收集和部署难题。其创新点在于将DT与AI结合，为未来无线通信中的CSI获取提供了潜在的解决方案，对提升AI在实际通信系统中的应用具有重要意义。"}}
{"id": "2506.20761", "title": "A Framework for Building Data Structures from Communication Protocols", "authors": ["Alexandr Andoni", "Shunhua Jiang", "Omri Weinstein"], "summary": "We present a general framework for designing efficient data structures for\nhigh-dimensional pattern-matching problems ($\\exists \\;? i\\in[n], f(x_i,y)=1$)\nthrough communication models in which $f(x,y)$ admits sublinear communication\nprotocols with exponentially-small error. Specifically, we reduce the data\nstructure problem to the Unambiguous Arthur-Merlin (UAM) communication\ncomplexity of $f(x,y)$ under product distributions.\n  We apply our framework to the Partial Match problem (a.k.a, matching with\nwildcards), whose underlying communication problem is sparse set-disjointness.\nWhen the database consists of $n$ points in dimension $d$, and the number of\n$\\star$'s in the query is at most $w = c\\log n \\;(\\ll d)$, the fastest known\nlinear-space data structure (Cole, Gottlieb and Lewenstein, STOC'04) had query\ntime $t \\approx 2^w = n^c$, which is nontrivial only when $c<1$. By contrast,\nour framework produces a data structure with query time $n^{1-1/(c \\log^2 c)}$\nand space close to linear.\n  To achieve this, we develop a one-sided $\\epsilon$-error communication\nprotocol for Set-Disjointness under product distributions with\n$\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$ complexity, improving on the\nclassical result of Babai, Frankl and Simon (FOCS'86). Building on this\nprotocol, we show that the Unambiguous AM communication complexity of\n$w$-Sparse Set-Disjointness with $\\epsilon$-error under product distributions\nis $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$, independent of the ambient dimension\n$d$, which is crucial for the Partial Match result. Our framework sheds further\nlight on the power of data-dependent data structures, which is instrumental for\nreducing to the (much easier) case of product distributions.", "comment": "53 pages, STOC 2025", "pdf_url": "http://arxiv.org/pdf/2506.20761v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20761v1", "AI": {"title_translation": "一种从通信协议构建数据结构的框架", "tldr": "本文提出了一种通用框架，通过通信模型设计高效的高维模式匹配数据结构，并将其应用于部分匹配问题，显著提升了查询时间。", "motivation": "本文旨在通过通信模型为高维模式匹配问题设计高效的数据结构，特别是针对现有线性空间数据结构在部分匹配问题上的查询时间限制（例如，Cole, Gottlieb 和 Lewenstein (STOC'04) 的查询时间为 $2^w = n^c$，仅在 $c<1$ 时非平凡）。", "method": "该框架将数据结构问题归结为乘积分布下函数 $f(x,y)$ 的无歧义阿瑟-梅林（UAM）通信复杂度。具体地，研究人员开发了一种针对乘积分布下 Set-Disjointness 的单边 $\\epsilon$ 误差通信协议，其复杂度为 $\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$，并在此基础上证明了乘积分布下 $w$-稀疏 Set-Disjointness 的无歧义 AM 通信复杂度为 $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$，与环境维度 $d$ 无关。", "result": "将该框架应用于部分匹配问题，当数据库包含 $n$ 个 $d$ 维点且查询中 $\\star$ 的数量至多为 $w = c\\log n$ 时，该框架生成的数据结构查询时间为 $n^{1-1/(c \\log^2 c)}$，空间接近线性，显著优于先前最快的线性空间数据结构（Cole, Gottlieb 和 Lewenstein, STOC'04）的查询时间 $t \\approx 2^w = n^c$。此外，开发了一种改进的 Set-Disjointness 通信协议。", "conclusion": "本文提出的框架为从通信协议构建数据结构提供了一种通用方法，特别是在高维模式匹配问题上显示出其强大能力，通过利用数据依赖的数据结构并将其简化为乘积分布下更易处理的情况，取得了显著的性能提升。", "translation": "我们提出了一个通用框架，用于通过通信模型设计高效的高维模式匹配问题数据结构（存在？$i\\in[n], f(x_i,y)=1$），其中 $f(x,y)$ 允许亚线性通信协议，且误差呈指数级小。具体而言，我们将数据结构问题归结为乘积分布下 $f(x,y)$ 的无歧义阿瑟-梅林（UAM）通信复杂度。我们将我们的框架应用于部分匹配问题（又称带通配符匹配），其底层通信问题是稀疏集合不交性。当数据库包含 $n$ 个 $d$ 维点，且查询中 $\\star$ 的数量最多为 $w = c\\log n \\;(\\ll d)$ 时，已知最快的线性空间数据结构（Cole, Gottlieb 和 Lewenstein, STOC'04）的查询时间 $t \\approx 2^w = n^c$，仅当 $c<1$ 时才非平凡。相比之下，我们的框架生成的数据结构查询时间为 $n^{1-1/(c \\log^2 c)}$，空间接近线性。为了实现这一点，我们开发了一种在乘积分布下具有 $\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$ 复杂度的单边 $\\epsilon$ 误差通信协议，改进了 Babai, Frankl 和 Simon (FOCS'86) 的经典结果。在此协议的基础上，我们表明在乘积分布下，具有 $\\epsilon$ 误差的 $w$-稀疏集合不交性的无歧义 AM 通信复杂度为 $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$，与环境维度 $d$ 无关，这对部分匹配结果至关重要。我们的框架进一步阐明了数据依赖数据结构的力量，这对于简化到（更容易的）乘积分布情况至关重要。", "summary": "本文提出了一个通用框架，通过将数据结构问题归结为通信复杂度，来构建高维模式匹配的高效数据结构。该框架成功应用于部分匹配问题，在查询时间上显著超越了现有最佳线性空间方法，实现了 $n^{1-1/(c \\log^2 c)}$ 的查询时间。其核心在于开发了一种新的、改进的乘积分布下 Set-Disjointness 通信协议，并证明了其无歧义 AM 通信复杂度与维度无关，这对于实际应用至关重要。该工作突出了数据依赖数据结构在处理复杂问题时的强大能力。", "keywords": "数据结构, 通信协议, 模式匹配, 部分匹配, 集合不交性", "comments": "本文的创新之处在于将数据结构设计问题与通信复杂度理论相结合，提供了一个通用的、基于理论基础的框架。通过将问题归约为UAM通信复杂度，并开发了针对特定通信问题的改进协议，作者们成功地在部分匹配等高维模式匹配问题上取得了突破性的性能提升。这种方法论上的创新，即利用通信模型的洞察力来构建数据结构，为该领域开辟了新的研究方向。其重要性在于，它不仅提供了更高效的算法，还加深了我们对数据结构与信息论之间关系的理解。"}}
{"id": "2506.20780", "title": "Noise-Tolerant Hybrid Approach for Data-Driven Predictive Control", "authors": ["Mahmood Mazare", "Hossein Ramezani"], "summary": "This paper focuses on a key challenge in hybrid data-driven predictive\ncontrol: the effect of measurement noise on Hankel matrices. While noise is\nhandled in direct and indirect methods, hybrid approaches often overlook its\nimpact during trajectory estimation. We propose a Noise-Tolerant Data-Driven\nPredictive Control (NTDPC) framework that integrates singular value\ndecomposition to separate system dynamics from noise within reduced-order\nHankel matrices. This enables accurate prediction with shorter data horizons\nand lower computational effort. A sensitivity index is introduced to support\nhorizon selection under different noise levels. Simulation results indicate\nimproved robustness and efficiency compared to existing hybrid methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20780v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20780v1", "AI": {"title_translation": "容噪混合数据驱动预测控制方法", "tldr": "提出一种容噪数据驱动预测控制（NTDPC）框架，通过SVD处理测量噪声对Hankel矩阵的影响，提高混合数据驱动预测控制的鲁棒性和效率。", "motivation": "混合数据驱动预测控制中，测量噪声对Hankel矩阵的影响是一个关键挑战，现有混合方法在轨迹估计时常忽略其影响。", "method": "提出容噪数据驱动预测控制（NTDPC）框架，该框架将奇异值分解（SVD）集成到降阶Hankel矩阵中，以分离系统动态和噪声。文中还引入了一个灵敏度指标，以支持在不同噪声水平下的预测范围选择。", "result": "仿真结果表明，与现有混合方法相比，NTDPC提高了鲁棒性和效率，能够以更短的数据范围和更低的计算量进行准确预测。", "conclusion": "NTDPC框架有效解决了混合数据驱动预测控制中测量噪声的影响问题，提升了系统的性能。", "translation": "这篇论文关注混合数据驱动预测控制中的一个关键挑战：测量噪声对Hankel矩阵的影响。虽然直接和间接方法能够处理噪声，但混合方法在轨迹估计时通常会忽略其影响。我们提出了一种容噪数据驱动预测控制（NTDPC）框架，该框架集成了奇异值分解，用于在降阶Hankel矩阵中分离系统动态和噪声。这使得在更短的数据范围和更低的计算量下实现精确预测成为可能。文中引入了一个灵敏度指标，以支持在不同噪声水平下的预测范围选择。仿真结果表明，与现有混合方法相比，该方法提高了鲁棒性和效率。", "summary": "本文针对混合数据驱动预测控制中测量噪声对Hankel矩阵的关键影响，提出了容噪数据驱动预测控制（NTDPC）框架。该框架利用奇异值分解在降阶Hankel矩阵中分离系统动态和噪声，从而实现更短数据范围和更低计算量的精确预测。此外，引入了灵敏度指标以辅助不同噪声水平下的预测范围选择。仿真结果验证了NTDPC在鲁棒性和效率上优于现有混合方法。", "keywords": "数据驱动预测控制, 噪声容忍, Hankel矩阵, 奇异值分解, 混合方法", "comments": "这篇论文的创新点在于提出了一个专门处理测量噪声的混合数据驱动预测控制框架，通过引入SVD有效分离噪声，并提供了敏感度指标来优化预测范围。这对于提高数据驱动控制在实际应用中的可靠性和性能具有重要意义。"}}
{"id": "2506.20798", "title": "Physical Limits of Entanglement-Based Quantum Key Distribution over Long-Distance Satellite Links", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saif Al-Kuwari", "Khalid Qaraqe"], "summary": "Entanglement-based quantum key distribution (QKD) protocols, such as E91 and\nBBM92, offer strong information-theoretic security and are naturally suited for\nsatellite-to-satellite QKD (SatQKD) links. However, implementing these\nprotocols over long-distance inter-satellite free-space optical (FSO) channels\nposes critical physical-layer challenges that are not addressed in the existing\nliterature. In particular, photon losses due to beam divergence, pointing\nerrors, and background noise can severely degrade the key generation rate and\nquantum bit error rate (QBER), especially under narrow receiver field-of-view\n(FoV) constraints. This paper presents a comprehensive performance analysis of\nentanglement-based inter-satellite QKD, focusing on photon-level modeling and\nthe impact of practical impairments. We develop analytical expressions for\nsignal detection probabilities, background photon influence, multi-pair\nemissions, and QBER, incorporating key parameters such as link distance,\ntransmitter tracking jitter, receiver misalignment, and photon pair generation\nrate. Simulation results reveal the nonlinear sensitivity of system performance\nto tracking error and FoV limitations, and highlight optimal parameter regimes\nthat jointly maximize secret key rate while maintaining QBER below acceptable\nthresholds. The proposed model provides actionable design insights for reliable\nand efficient deployment of entanglement-based SatQKD systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20798v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20798v1", "AI": {"title_translation": "基于纠缠的远距离卫星链路量子密钥分发的物理极限", "tldr": "本文对基于纠缠的卫星间量子密钥分发(SatQKD)进行了全面的性能分析，特别关注光子级建模和实际损伤的影响，并提出了可操作的设计见解。", "motivation": "现有的文献未能解决在长距离卫星间自由空间光(FSO)信道上实现基于纠缠的量子密钥分发(QKD)协议所面临的关键物理层挑战，特别是由于光束发散、指向误差和背景噪声引起的光子损耗会严重降低密钥生成速率和量子误码率(QBER)。", "method": "本文对基于纠缠的卫星间量子密钥分发进行了全面的性能分析，专注于光子级建模和实际损伤的影响。研究人员开发了信号检测概率、背景光子影响、多对发射和QBER的分析表达式，并纳入了链路距离、发射机跟踪抖动、接收机失准和光子对生成速率等关键参数。", "result": "仿真结果揭示了系统性能对跟踪误差和视场(FoV)限制的非线性敏感性，并强调了在保持QBER低于可接受阈值的同时，共同最大化秘密密钥速率的最佳参数范围。", "conclusion": "所提出的模型为可靠高效地部署基于纠缠的卫星量子密钥分发系统提供了可操作的设计见解。", "translation": "基于纠缠的量子密钥分发（QKD）协议，如E91和BBM92，提供了强大的信息理论安全性，并且天然适用于卫星到卫星的QKD（SatQKD）链路。然而，在长距离卫星间自由空间光（FSO）信道上实现这些协议带来了现有文献中尚未解决的关键物理层挑战。特别是，由于光束发散、指向误差和背景噪声引起的光子损耗会严重降低密钥生成速率和量子误码率（QBER），尤其是在窄接收机视场（FoV）限制下。本文对基于纠缠的卫星间QKD进行了全面的性能分析，重点关注光子级建模和实际损伤的影响。我们开发了信号检测概率、背景光子影响、多对发射和QBER的分析表达式，并纳入了链路距离、发射机跟踪抖动、接收机失准和光子对生成速率等关键参数。仿真结果揭示了系统性能对跟踪误差和FoV限制的非线性敏感性，并强调了在保持QBER低于可接受阈值的同时，共同最大化秘密密钥速率的最佳参数范围。所提出的模型为可靠高效地部署基于纠缠的SatQKD系统提供了可操作的设计见解。", "summary": "本文针对基于纠缠的卫星量子密钥分发(SatQKD)在长距离卫星间自由空间光链路中面临的物理层挑战进行了深入分析。研究建立了光子级模型，并推导了信号检测概率、背景噪声、多对发射和量子误码率(QBER)的分析表达式，考虑了链路距离、跟踪抖动等关键参数。仿真结果揭示了系统性能对跟踪误差和视场限制的非线性敏感性，并确定了能同时优化密钥速率和QBER的最佳参数范围。该模型为设计和部署可靠高效的基于纠缠的SatQKD系统提供了实用指导。", "keywords": "量子密钥分发, 卫星链路, 纠缠, 物理极限, 性能分析", "comments": "本文解决了基于纠缠的卫星量子密钥分发在实际长距离链路中面临的关键物理层挑战，填补了现有文献的空白。其创新之处在于提出了全面的光子级建模和分析表达式，并揭示了系统性能对关键参数的非线性敏感性。通过提供可操作的设计见解，该研究对未来卫星QKD系统的可靠和高效部署具有重要指导意义。"}}
{"id": "2506.21090", "title": "Post-training for Deepfake Speech Detection", "authors": ["Wanying Ge", "Xin Wang", "Xuechen Liu", "Junichi Yamagishi"], "summary": "We introduce a post-training approach that adapts self-supervised learning\n(SSL) models for deepfake speech detection by bridging the gap between general\npre-training and domain-specific fine-tuning. We present AntiDeepfake models, a\nseries of post-trained models developed using a large-scale multilingual speech\ndataset containing over 56,000 hours of genuine speech and 18,000 hours of\nspeech with various artifacts in over one hundred languages. Experimental\nresults show that the post-trained models already exhibit strong robustness and\ngeneralization to unseen deepfake speech. When they are further fine-tuned on\nthe Deepfake-Eval-2024 dataset, these models consistently surpass existing\nstate-of-the-art detectors that do not leverage post-training. Model\ncheckpoints and source code are available online.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21090v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21090v1", "AI": {"title_translation": "用于深度伪造语音检测的后训练", "tldr": "本文提出了一种后训练方法，用于深度伪造语音检测，通过弥合通用预训练和领域特定微调之间的差距，使自监督学习模型适应此任务，并在实验中表现出强大的鲁棒性和泛化能力，超越现有最先进的检测器。", "motivation": "本文旨在通过引入一种后训练方法来弥合通用预训练和领域特定微调之间的差距，从而使自监督学习（SSL）模型适用于深度伪造语音检测。", "method": "研究人员提出了一种名为 AntiDeepfake 的后训练模型系列，这些模型是使用一个大规模多语言语音数据集开发的，该数据集包含超过 56,000 小时真实语音和 18,000 小时带有各种伪影的语音，涵盖一百多种语言。", "result": "实验结果表明，后训练模型对未见过的深度伪造语音已经表现出强大的鲁棒性和泛化能力。当在 Deepfake-Eval-2024 数据集上进一步微调时，这些模型持续超越了不利用后训练的现有最先进检测器。", "conclusion": "后训练方法能够显著提高自监督学习模型在深度伪造语音检测任务上的性能、鲁棒性和泛化能力，使其超越现有的最先进方法。", "translation": "我们引入了一种后训练方法，通过弥合通用预训练和领域特定微调之间的差距，使自监督学习（SSL）模型适应深度伪造语音检测。我们提出了 AntiDeepfake 模型，这是一系列使用大规模多语言语音数据集开发的后训练模型，该数据集包含超过 56,000 小时真实语音和 18,000 小时带有各种伪影的语音，涵盖一百多种语言。实验结果表明，后训练模型对未见过的深度伪造语音已经表现出强大的鲁棒性和泛化能力。当在 Deepfake-Eval-2024 数据集上进一步微调时，这些模型持续超越了不利用后训练的现有最先进检测器。模型检查点和源代码可在网上获取。", "summary": "本文提出了一种针对深度伪造语音检测的后训练方法，旨在弥合自监督学习模型预训练和领域特定微调之间的差距。研究人员开发了 AntiDeepfake 模型系列，并使用包含大量多语言真实和伪造语音的数据集进行训练。实验证明，这些后训练模型对未知深度伪造语音具有强大的鲁棒性和泛化能力，并且在进一步微调后，其性能优于现有最先进的检测器。", "keywords": "深度伪造语音检测, 后训练, 自监督学习, 鲁棒性, 泛化能力", "comments": "本文的创新点在于提出了“后训练”这一概念，有效地将通用预训练模型与特定领域任务相结合，显著提升了深度伪造语音检测的性能和泛化能力。其使用的大规模多语言数据集也增强了模型的实用性和鲁棒性。该方法为未来深度伪造检测领域的研究提供了新的方向。"}}
{"id": "2506.20688", "title": "Building Lightweight Semantic Segmentation Models for Aerial Images Using Dual Relation Distillation", "authors": ["Minglong Li", "Lianlei Shan", "Weiqiang Wang", "Ke Lv", "Bin Luo", "Si-Bao Chen"], "summary": "Recently, there have been significant improvements in the accuracy of CNN\nmodels for semantic segmentation. However, these models are often heavy and\nsuffer from low inference speed, which limits their practical application. To\naddress this issue, knowledge distillation has emerged as a promising approach\nto achieve a good trade-off between segmentation accuracy and efficiency. In\nthis paper, we propose a novel dual relation distillation (DRD) technique that\ntransfers both spatial and channel relations in feature maps from a cumbersome\nmodel (teacher) to a compact model (student). Specifically, we compute spatial\nand channel relation maps separately for the teacher and student models, and\nthen align corresponding relation maps by minimizing their distance. Since the\nteacher model usually learns more information and collects richer spatial and\nchannel correlations than the student model, transferring these correlations\nfrom the teacher to the student can help the student mimic the teacher better\nin terms of feature distribution, thus improving the segmentation accuracy of\nthe student model. We conduct comprehensive experiments on three segmentation\ndatasets, including two widely adopted benchmarks in the remote sensing field\n(Vaihingen and Potsdam datasets) and one popular benchmark in general scene\n(Cityscapes dataset). The experimental results demonstrate that our novel\ndistillation framework can significantly boost the performance of the student\nnetwork without incurring extra computational overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20688v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20688v1", "AI": {"title_translation": "使用双重关系蒸馏构建航空图像的轻量级语义分割模型", "tldr": "本文提出了一种新颖的双重关系蒸馏（DRD）技术，通过传递空间和通道关系，显著提升了轻量级语义分割模型的性能，同时不增加计算开销。", "motivation": "现有的语义分割CNN模型虽然精度高，但通常模型庞大且推理速度慢，限制了其实际应用。", "method": "我们提出了一种新颖的双重关系蒸馏（DRD）技术。该技术将笨重模型（教师）的特征图中的空间和通道关系同时传递给紧凑模型（学生）。具体而言，我们分别为教师和学生模型计算空间和通道关系图，然后通过最小化它们之间的距离来对齐相应的关系图。", "result": "在三个分割数据集（包括Vaihingen、Potsdam和Cityscapes）上进行了综合实验。实验结果表明，我们新颖的蒸馏框架可以在不产生额外计算开销的情况下显著提升学生网络的性能。", "conclusion": "双重关系蒸馏（DRD）方法能够有效地将教师模型的丰富空间和通道关联信息传递给学生模型，从而显著提高轻量级语义分割模型的准确性，同时保持其高效性。", "translation": "近年来，用于语义分割的CNN模型在准确性方面取得了显著进步。然而，这些模型通常很笨重，推理速度慢，这限制了它们的实际应用。为了解决这个问题，知识蒸馏已成为一种有前途的方法，可以在分割精度和效率之间实现良好的权衡。在本文中，我们提出了一种新颖的双重关系蒸馏（DRD）技术，该技术将特征图中的空间和通道关系从笨重模型（教师）转移到紧凑模型（学生）。具体而言，我们分别为教师和学生模型计算空间和通道关系图，然后通过最小化它们之间的距离来对齐相应的关系图。由于教师模型通常比学生模型学习到更多的信息并收集更丰富的空间和通道关联，因此将这些关联从教师转移到学生可以帮助学生在特征分布方面更好地模仿教师，从而提高学生模型的分割准确性。我们在三个分割数据集上进行了综合实验，包括遥感领域中两个广泛采用的基准（Vaihingen和Potsdam数据集）和一个通用场景中的流行基准（Cityscapes数据集）。实验结果表明，我们新颖的蒸馏框架可以在不产生额外计算开销的情况下显著提升学生网络的性能。", "summary": "本文提出了一种名为双重关系蒸馏（DRD）的新型知识蒸馏技术，旨在解决语义分割CNN模型在精度和效率之间的权衡问题。DRD通过将教师模型的特征图中的空间和通道关系同时传递给学生模型，使学生模型更好地模仿教师的特征分布，从而提高其分割准确性。实验证明，该方法在不增加计算开销的情况下，显著提升了轻量级学生模型在多个遥感和通用场景数据集上的性能。", "keywords": "双重关系蒸馏, 语义分割, 知识蒸馏, 航空图像, 轻量级模型", "comments": "这项工作在知识蒸馏领域具有创新性，特别是在语义分割任务中引入了同时蒸馏空间和通道关系的双重机制。它有效解决了轻量级模型精度不足的问题，对于资源受限的实际应用具有重要意义。该方法的普适性通过在遥感和通用场景数据集上的验证得到了体现。"}}
{"id": "2506.20875", "title": "3DGH: 3D Head Generation with Composable Hair and Face", "authors": ["Chengan He", "Junxuan Li", "Tobias Kirschstein", "Artem Sevastopolsky", "Shunsuke Saito", "Qingyang Tan", "Javier Romero", "Chen Cao", "Holly Rushmeier", "Giljoo Nam"], "summary": "We present 3DGH, an unconditional generative model for 3D human heads with\ncomposable hair and face components. Unlike previous work that entangles the\nmodeling of hair and face, we propose to separate them using a novel data\nrepresentation with template-based 3D Gaussian Splatting, in which deformable\nhair geometry is introduced to capture the geometric variations across\ndifferent hairstyles. Based on this data representation, we design a 3D\nGAN-based architecture with dual generators and employ a cross-attention\nmechanism to model the inherent correlation between hair and face. The model is\ntrained on synthetic renderings using carefully designed objectives to\nstabilize training and facilitate hair-face separation. We conduct extensive\nexperiments to validate the design choice of 3DGH, and evaluate it both\nqualitatively and quantitatively by comparing with several state-of-the-art 3D\nGAN methods, demonstrating its effectiveness in unconditional full-head image\nsynthesis and composable 3D hairstyle editing. More details will be available\non our project page: https://c-he.github.io/projects/3dgh/.", "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://c-he.github.io/projects/3dgh/", "pdf_url": "http://arxiv.org/pdf/2506.20875v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20875v1", "AI": {"title_translation": "3DGH：可组合头发和面部的3D头部生成", "tldr": "3DGH是一种新的3D头部生成模型，通过分离头发和面部，实现了可组合的头部图像合成和发型编辑。", "motivation": "以前的工作将头发和面部的建模纠缠在一起，限制了可组合性，而3DGH旨在解决这一问题。", "method": "提出了一种基于模板的3D高斯泼溅的新型数据表示，其中引入了可变形头发几何体以捕捉发型变化。在此数据表示基础上，设计了一个带有双生成器的3D GAN架构，并采用交叉注意力机制来建模头发和面部之间的内在关联。模型在合成渲染上训练，采用精心设计的优化目标以稳定训练并促进头发与面部的分离。", "result": "3DGH在无条件全头图像合成和可组合3D发型编辑方面表现出有效性。通过与现有最先进的3D GAN方法进行定性和定量比较，验证了其设计选择和性能。", "conclusion": "3DGH通过分离头发和面部，提出了一种有效且可组合的3D头部生成方法，成功实现了无条件全头图像合成和3D发型编辑。", "translation": "我们提出了3DGH，一个用于生成具有可组合头发和面部组件的3D人头的无条件生成模型。与以往将头发和面部建模纠缠在一起的工作不同，我们提出使用一种基于模板的3D高斯泼溅的新型数据表示来分离它们，其中引入了可变形的头发几何体以捕捉不同发型之间的几何变化。基于这种数据表示，我们设计了一个带有双生成器的3D GAN架构，并采用交叉注意力机制来建模头发和面部之间的内在关联。该模型在合成渲染上训练，采用精心设计的优化目标以稳定训练并促进头发与面部的分离。我们进行了广泛的实验来验证3DGH的设计选择，并通过与几种最先进的3D GAN方法进行定性和定量比较来评估它，证明了其在无条件全头图像合成和可组合3D发型编辑方面的有效性。更多详细信息将在我们的项目页面上提供：https://c-he.github.io/projects/3dgh/。", "summary": "3DGH是一种创新的无条件3D人头生成模型，其核心在于通过引入基于模板的3D高斯泼溅和可变形头发几何体，实现了头发和面部的独立建模。该模型采用双生成器3D GAN架构和交叉注意力机制来处理头发与面部的相关性，并通过精心设计的训练目标在合成数据上进行优化。实验证明，3DGH在无条件全头图像合成和可组合3D发型编辑方面均表现出色，超越了现有技术。", "keywords": "3D头部生成, 高斯泼溅, GAN, 可组合性, 发型编辑", "comments": "3DGH的创新之处在于其独特的数据表示和双生成器架构，成功解决了头发和面部建模的纠缠问题，从而实现了更精细的可组合性。这对于虚拟形象、游戏和动画等领域具有重要意义，因为它允许用户更灵活地编辑和生成3D头部资产。"}}
{"id": "2506.20844", "title": "The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "summary": "Scientific fact-checking aims to determine the veracity of scientific claims\nby retrieving and analysing evidence from research literature. The problem is\ninherently more complex than general fact-checking since it must accommodate\nthe evolving nature of scientific knowledge, the structural complexity of\nacademic literature and the challenges posed by long-form, multimodal\nscientific expression. However, existing approaches focus on simplified\nversions of the problem based on small-scale datasets consisting of abstracts\nrather than full papers, thereby avoiding the distinct challenges associated\nwith processing complete documents. This paper examines the limitations of\ncurrent scientific fact-checking systems and reveals the many potential\nfeatures and resources that could be exploited to advance their performance. It\nidentifies key research challenges within evidence retrieval, including (1)\nevidence-driven retrieval that addresses semantic limitations and topic\nimbalance (2) time-aware evidence retrieval with citation tracking to mitigate\noutdated information, (3) structured document parsing to leverage long-range\ncontext, (4) handling complex scientific expressions, including tables,\nfigures, and domain-specific terminology and (5) assessing the credibility of\nscientific literature. Preliminary experiments were conducted to substantiate\nthese challenges and identify potential solutions. This perspective paper aims\nto advance scientific fact-checking with a specialised IR system tailored for\nreal-world applications.", "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR'25)", "pdf_url": "http://arxiv.org/pdf/2506.20844v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20844v1", "AI": {"title_translation": "科学事实核查的下一阶段：从复杂结构化学术论文中进行高级证据检索", "tldr": "现有科学事实核查系统无法处理完整的学术论文，本研究旨在通过识别挑战和潜在解决方案来改进专门的信息检索系统，以应对复杂科学文献的证据检索。", "motivation": "现有的科学事实核查方法主要基于小型数据集（如摘要），未能解决处理完整学术论文所带来的独特挑战，包括科学知识的演变性、学术文献的结构复杂性以及长篇、多模态科学表达的挑战。", "method": "本文审视了当前科学事实核查系统的局限性，揭示了可用于提升其性能的潜在特征和资源。它识别了证据检索中的关键研究挑战，并进行了初步实验来证实这些挑战并确定潜在解决方案。", "result": "论文识别了证据检索中的关键研究挑战，包括：解决语义限制和主题不平衡的证据驱动检索；利用引文跟踪缓解过时信息的时间感知证据检索；利用长程上下文的结构化文档解析；处理复杂科学表达（表格、图、领域特定术语）；以及评估科学文献的可信度。初步实验证实了这些挑战并确定了潜在解决方案。", "conclusion": "本视角论文旨在通过一个专门为实际应用定制的信息检索系统来推进科学事实核查。", "translation": "科学事实核查旨在通过从研究文献中检索和分析证据来确定科学主张的真实性。这个问题本质上比一般事实核查更复杂，因为它必须适应科学知识的演变性质、学术文献的结构复杂性以及长篇、多模态科学表达所带来的挑战。然而，现有方法侧重于基于由摘要而非完整论文组成的小规模数据集的简化版本问题，从而避免了处理完整文档所带来的独特挑战。本文审视了当前科学事实核查系统的局限性，并揭示了可用于提升其性能的许多潜在特征和资源。它识别了证据检索中的关键研究挑战，包括：(1) 解决语义限制和主题不平衡的证据驱动检索；(2) 利用引文跟踪缓解过时信息的时间感知证据检索；(3) 利用长程上下文的结构化文档解析；(4) 处理复杂科学表达，包括表格、图和领域特定术语；以及(5) 评估科学文献的可信度。进行了初步实验以证实这些挑战并确定潜在解决方案。本视角论文旨在通过一个专门为实际应用定制的专业信息检索系统来推进科学事实核查。", "summary": "本文探讨了科学事实核查的下一阶段，指出现有系统在处理完整、复杂结构化学术论文方面的局限性。研究识别了在证据检索中面临的关键挑战，包括语义、时间、结构化文档解析、复杂表达处理和可信度评估。通过初步实验证实这些挑战并提出潜在解决方案，旨在开发一个专门的信息检索系统以改进科学事实核查。", "keywords": "科学事实核查, 证据检索, 学术论文, 信息检索, 挑战", "comments": "这篇论文的创新点在于它明确指出了现有科学事实核查系统在处理完整学术论文时的不足，并系统地列出了未来需要解决的关键挑战。它强调了将信息检索技术应用于复杂科学文本的重要性，特别是考虑到多模态信息和时间维度。这对于提高科学信息的可信度和自动化核查效率具有重要意义。"}}
{"id": "2506.20793", "title": "Multi-lingual Functional Evaluation for Large Language Models", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20793v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20793v1", "AI": {"title_translation": "大型语言模型的多语言功能评估", "tldr": "本文创建了新的多语言功能基准测试，以更好地评估大型语言模型在多语言环境下的实际性能和鲁棒性，发现现有静态基准的评估效果差异显著。", "motivation": "大型语言模型的多语言能力评估通常依赖静态数据基准，但这些评估往往无法充分理解模型在多语言环境下的实际性能和鲁棒性。", "method": "研究人员通过将现有的英语功能基准模板翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种额外语言，创建了多语言功能基准测试：跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）。", "result": "结果显示，一些静态多语言基准比其他基准更能反映功能性能（例如，M-GSM与CL-GSM Symbolic之间在英语、法语和西班牙语中的性能分别下降了24%、17%和18%；Belebele与CL-IFEval之间在各语言中性能下降了15-24%；而M-MMLU与CL-IFEval之间仅下降了0.5%-3%）。同时，模型在不同语言间的鲁棒性差异显著，其中阿拉伯语和英语在评估迭代中表现最为稳定。", "conclusion": "该研究通过引入多语言功能基准测试，揭示了现有静态多语言评估的局限性，并提供了对大型语言模型在不同语言环境下实际性能和鲁棒性更深入的理解。结果表明，不同静态基准捕获功能性能的程度不同，且模型的跨语言鲁棒性存在显著差异。", "translation": "大型语言模型的多语言能力通常通过Belebele、M-MMLU和M-GSM等静态数据基准进行评估。然而，这些评估往往无法充分理解模型在多语言环境下的实际性能和鲁棒性。为此，我们通过将现有的英语功能基准模板翻译成五种额外的语言，包括法语、西班牙语、印地语、阿拉伯语和约鲁巴语，创建了多语言功能基准测试——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）。我们的结果显示，一些静态多语言基准比其他基准更能密切地反映功能性能（即，在不同模型中，M-GSM与CL-GSM Symbolic之间在英语、法语和西班牙语中的性能分别下降了24%、17%和18%；同样，Belebele与CL-IFEval之间在各语言中性能下降了15-24%，而M-MMLU与CL-IFEval之间仅下降了0.5%-3%）。类似地，我们发现模型在不同语言间的鲁棒性差异显著，某些语言（例如阿拉伯语、英语）在评估迭代中表现最为稳定。", "summary": "本研究旨在解决现有静态基准测试在评估大型语言模型多语言功能和鲁棒性方面的不足。为此，作者创建了两个新的多语言功能基准测试：CL-GSM Symbolic和CL-IFEval，通过将英语模板翻译成五种不同语言。实验结果表明，不同静态基准对功能性能的捕获能力差异显著，且模型的跨语言鲁棒性表现不一，其中阿拉伯语和英语表现最为稳定。", "keywords": "多语言评估, 大型语言模型, 功能基准, 鲁棒性, 跨语言", "comments": "这项研究通过引入“功能性”评估的概念，为大型语言模型的多语言能力评估提供了一个更实用和深入的视角，弥补了传统静态基准的不足。其创新之处在于构建了跨语言的功能性测试集，并量化了不同静态基准与实际功能性能之间的差距。这对于理解和改进多语言LLM的实际应用性能具有重要意义。"}}
{"id": "2506.21086", "title": "PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching", "authors": ["Guillem Cortès-Sebastià", "Benjamin Martin", "Emilio Molina", "Xavier Serra", "Romain Hennequin"], "summary": "This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)\nsystem designed specifically around spectral peaks. This novel system is\ndesigned to leverage the sparse spectral coordinates typically computed by\ntraditional peak-based AFP methods. PeakNetFP performs hierarchical point\nfeature extraction techniques similar to the computer vision model PointNet++,\nand is trained using contrastive learning like in the state-of-the-art deep\nlearning AFP, NeuralFP. This combination allows PeakNetFP to outperform\nconventional AFP systems and achieves comparable performance to NeuralFP when\nhandling challenging time-stretched audio data. In extensive evaluation,\nPeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging\nfrom 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:\ncompared to NeuralFP, it has 100 times fewer parameters and uses 11 times\nsmaller input data. These features make PeakNetFP a lightweight and efficient\nsolution for AFP tasks where time stretching is involved. Overall, this system\nrepresents a promising direction for future AFP technologies, as it\nsuccessfully merges the lightweight nature of peak-based AFP with the\nadaptability and pattern recognition capabilities of neural network-based\napproaches, paving the way for more scalable and efficient solutions in the\nfield.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21086v1", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.3.1; H.3.3; H.3.4"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21086v1", "AI": {"title_translation": "PeakNetFP：基于峰值的神经网络音频指纹识别，对极端时间拉伸具有鲁棒性", "tldr": "PeakNetFP是一种新的基于峰值的神经网络音频指纹识别系统，它结合了传统峰值方法的轻量级和神经网络的适应性，在处理时间拉伸音频方面表现出色，同时比现有最先进的方法更高效。", "motivation": "现有的音频指纹识别系统在处理极端时间拉伸的音频数据时可能性能不佳，并且一些先进的深度学习方法（如NeuralFP）可能在效率上存在不足（参数多、输入数据大）。本文旨在开发一个既鲁棒又高效的解决方案。", "method": "本文提出了PeakNetFP，这是一个围绕频谱峰值设计的神经网络音频指纹识别系统。它利用稀疏频谱坐标，采用类似于计算机视觉模型PointNet++的分层点特征提取技术，并使用对比学习进行训练（类似于NeuralFP）。", "result": "PeakNetFP在处理时间拉伸音频数据时，性能优于传统音频指纹识别系统，并与最先进的NeuralFP表现相当。它在50%到200%的时间拉伸因子下保持超过90%的Top-1命中率。此外，PeakNetFP具有显著的效率优势：与NeuralFP相比，其参数减少100倍，输入数据小11倍。", "conclusion": "PeakNetFP为涉及时间拉伸的音频指纹识别任务提供了一个轻量且高效的解决方案。该系统成功地将基于峰值的音频指纹识别的轻量级特性与基于神经网络方法的适应性和模式识别能力相结合，为未来的音频指纹识别技术开辟了更具可扩展性和效率的解决方案。", "translation": "这项工作介绍了PeakNetFP，这是第一个专门围绕频谱峰值设计的神经网络音频指纹识别（AFP）系统。这个新颖的系统旨在利用传统基于峰值的AFP方法通常计算的稀疏频谱坐标。PeakNetFP执行类似于计算机视觉模型PointNet++的分层点特征提取技术，并像最先进的深度学习AFP系统NeuralFP一样使用对比学习进行训练。这种组合使得PeakNetFP在处理具有挑战性的时间拉伸音频数据时，能够超越传统的AFP系统，并实现与NeuralFP相当的性能。在广泛的评估中，PeakNetFP在50%到200%的时间拉伸因子范围内保持超过90%的Top-1命中率。此外，PeakNetFP提供了显著的效率优势：与NeuralFP相比，它的参数减少100倍，输入数据小11倍。这些特性使PeakNetFP成为涉及时间拉伸的AFP任务的轻量级高效解决方案。总的来说，该系统代表了未来AFP技术的一个有前景的方向，因为它成功地将基于峰值的AFP的轻量级特性与基于神经网络方法的适应性和模式识别能力相结合，为该领域更具可扩展性和效率的解决方案铺平了道路。", "summary": "PeakNetFP是一种新型的神经网络音频指纹识别（AFP）系统，它创新性地结合了传统峰值方法的稀疏性与神经网络的强大模式识别能力。该系统采用分层点特征提取和对比学习，在处理极端时间拉伸音频数据时，表现出与最先进的NeuralFP相当的鲁棒性，并显著提高了效率，参数量减少100倍，输入数据量减少11倍，为音频指纹识别领域提供了轻量、高效且可扩展的解决方案。", "keywords": "音频指纹识别, 神经网络, 时间拉伸, 频谱峰值, 效率", "comments": "PeakNetFP的创新之处在于它成功地将传统基于峰值的音频指纹识别的轻量级特性与深度学习（神经网络）的强大适应性和模式识别能力融合。这种结合不仅解决了音频时间拉伸带来的挑战，还在效率上取得了显著突破，大幅减少了模型参数和输入数据量，使其在实际应用中更具吸引力。这为未来开发更高效、可扩展的音频处理技术指明了方向。"}}
{"id": "2506.20809", "title": "Boundary integral equation analysis for spheroidal suspensions", "authors": ["Leo Crowder", "Tianyue Li", "Eduardo Corona", "Shravan Veerapaneni"], "summary": "In this work, we provide a fast, spectrally accurate method for the\nevaluation of boundary integral operators (BIOs) on a suspension of prolate and\noblate spheroids. We first derive formulas for the standard layer potential\noperators for the Laplace equation applied to an expansion of the integral\ndensities in the appropriate spheroidal harmonic basis. These then lead to\nanalytical expressions in solid harmonics that allow spectrally accurate\nevaluation of near-field particle interactions. Finally, a standard quadrature\nscheme is used to evaluate smooth, far-field interactions; these are then\naccelerated using the fast multipole method.\n  Through a number of numerical test cases, we verify the accuracy and\nefficiency of our BIO evaluation framework for dense, polydisperse suspensions\nof spheroids. Through the use of standard formulas linking Stokes and Laplace\npotentials, we show our scheme can be readily applied to problems involving\nparticulate suspension flows. For both Laplace and Stokes, our method allows us\nto evaluate BIOs for suspensions up to hundreds of particles on a single\nprocessor.", "comment": "Submitted to Journal of Computational Physics, June 2025", "pdf_url": "http://arxiv.org/pdf/2506.20809v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20809v1", "AI": {"title_translation": "扁球体悬浮液的边界积分方程分析", "tldr": "该论文提出了一种快速、谱精确的边界积分方法，用于分析扁球体悬浮液，适用于拉普拉斯和斯托克斯问题，并能处理数百个粒子。", "motivation": "开发一种快速、谱精确的方法，用于评估长椭球体和扁椭球体悬浮液上的边界积分算子（BIOs），并将其应用于颗粒悬浮流问题。", "method": "该方法首先推导了拉普拉斯方程的标准层势算子公式，该算子应用于适当的扁球谐函数基底中的积分密度展开。然后，这些公式引出实体谐函数中的解析表达式，用于谱精确评估近场粒子相互作用。远场相互作用则通过标准求积方案评估，并使用快速多极方法加速。此外，通过连接斯托克斯和拉普拉斯势的标准公式，该方案可应用于颗粒悬浮流问题。", "result": "通过数值测试，验证了该BIO评估框架在密集、多分散扁球体悬浮液中的准确性和效率。该方案可以轻松应用于涉及颗粒悬浮流的问题。对于拉普拉斯和斯托克斯问题，该方法允许在单个处理器上评估多达数百个粒子的悬浮液的BIOs。", "conclusion": "该论文开发的快速、谱精确的边界积分方法能有效分析拉普拉斯和斯托克斯问题中的扁球体悬浮液，并能高效处理数百个粒子。", "translation": "在这项工作中，我们提供了一种快速、谱精确的方法，用于评估长椭球体和扁椭球体悬浮液上的边界积分算子（BIOs）。我们首先推导了适用于拉普拉斯方程的标准层势算子的公式，这些算子应用于适当的扁球谐函数基底中的积分密度展开。然后，这些公式引出了实体谐函数中的解析表达式，从而能够对近场粒子相互作用进行谱精确评估。最后，使用标准求积方案评估平滑的远场相互作用；这些相互作用随后通过快速多极方法进行加速。\n通过大量的数值测试案例，我们验证了我们BIO评估框架在密集、多分散扁球体悬浮液中的准确性和效率。通过使用连接斯托克斯和拉普拉斯势的标准公式，我们表明我们的方案可以很容易地应用于涉及颗粒悬浮流的问题。对于拉普拉斯和斯托克斯问题，我们的方法允许我们在单个处理器上评估多达数百个粒子的悬浮液的BIOs。", "summary": "该论文介绍了一种快速、谱精确的边界积分方法，用于分析长椭球体和扁椭球体悬浮液。它基于扁球谐函数和实体谐函数推导了近场相互作用的解析表达式，并结合快速多极方法处理远场相互作用。该方法在密集、多分散悬浮液中表现出高精度和效率，适用于拉普拉斯和斯托克斯问题，并能在单个处理器上处理数百个粒子。", "keywords": "边界积分方程, 扁球体, 快速多极方法, 拉普拉斯方程, 斯托克斯流", "comments": "该论文的创新之处在于结合了扁球谐函数展开和实体谐函数来精确处理近场相互作用，并整合了快速多极方法来加速远场计算。其重要性在于为涉及非球形粒子的复杂流体动力学问题提供了一个高效且精确的工具。论文中提到在单个处理器上能处理“数百个粒子”，这可能暗示在处理更大规模系统时仍需考虑可扩展性。"}}
{"id": "2506.20931", "title": "SPA: Towards More Stealth and Persistent Backdoor Attacks in Federated Learning", "authors": ["Chengcheng Zhu", "Ye Li", "Bosen Rao", "Jiale Zhang", "Yunlong Mao", "Sheng Zhong"], "summary": "Federated Learning (FL) has emerged as a leading paradigm for\nprivacy-preserving distributed machine learning, yet the distributed nature of\nFL introduces unique security challenges, notably the threat of backdoor\nattacks. Existing backdoor strategies predominantly rely on end-to-end label\nsupervision, which, despite their efficacy, often results in detectable feature\ndisentanglement and limited persistence. In this work, we propose a novel and\nstealthy backdoor attack framework, named SPA, which fundamentally departs from\ntraditional approaches by leveraging feature-space alignment rather than direct\ntrigger-label association. Specifically, SPA reduces representational distances\nbetween backdoor trigger features and target class features, enabling the\nglobal model to misclassify trigger-embedded inputs with high stealth and\npersistence. We further introduce an adaptive, adversarial trigger optimization\nmechanism, utilizing boundary-search in the feature space to enhance attack\nlongevity and effectiveness, even against defensive FL scenarios and non-IID\ndata distributions. Extensive experiments on various FL benchmarks demonstrate\nthat SPA consistently achieves high attack success rates with minimal impact on\nmodel utility, maintains robustness under challenging participation and data\nheterogeneity conditions, and exhibits persistent backdoor effects far\nexceeding those of conventional techniques. Our results call urgent attention\nto the evolving sophistication of backdoor threats in FL and emphasize the\npressing need for advanced, feature-level defense techniques.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2506.20931v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20931v1", "AI": {"title_translation": "SPA：迈向联邦学习中更隐蔽和持久的后门攻击", "tldr": "SPA是一种新的联邦学习后门攻击框架，通过特征空间对齐实现更隐蔽和持久的攻击，优于传统方法。", "motivation": "联邦学习（FL）作为隐私保护分布式机器学习的领先范式，面临独特的安全挑战，尤其是后门攻击的威胁。现有后门策略主要依赖于端到端标签监督，但这通常导致可检测的特征解缠和有限的攻击持久性。", "method": "本文提出了一种名为SPA的新型隐蔽后门攻击框架，它通过利用特征空间对齐而非直接触发-标签关联，从根本上区别于传统方法。具体而言，SPA通过减少后门触发特征与目标类特征之间的表征距离，使全局模型能够以高隐蔽性和持久性错误分类嵌入触发器的输入。此外，引入了一种自适应的对抗性触发优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性。", "result": "在各种FL基准测试上的广泛实验表明，SPA始终实现高攻击成功率，对模型效用影响最小。它在挑战性参与和数据异质性条件下保持鲁棒性，并展现出远超传统技术的持久后门效应。", "conclusion": "研究结果呼吁紧急关注联邦学习中后门威胁的演变复杂性，并强调迫切需要先进的特征级防御技术。", "translation": "联邦学习（FL）已成为保护隐私的分布式机器学习的领先范式，但FL的分布式特性带来了独特的安全挑战，特别是后门攻击的威胁。现有的后门策略主要依赖于端到端标签监督，尽管它们有效，但通常会导致可检测的特征解缠和有限的持久性。在这项工作中，我们提出了一种新颖且隐蔽的后门攻击框架，名为SPA，它通过利用特征空间对齐而非直接的触发-标签关联，从根本上脱离了传统方法。具体而言，SPA减少了后门触发特征和目标类特征之间的表征距离，使全局模型能够以高度隐蔽性和持久性错误分类嵌入触发器的输入。我们进一步引入了一种自适应的对抗性触发优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性，即使在防御性FL场景和非IID数据分布下也是如此。在各种FL基准测试上的广泛实验表明，SPA始终实现高攻击成功率，对模型效用影响最小，在挑战性参与和数据异质性条件下保持鲁棒性，并展现出远超传统技术的持久后门效应。我们的结果呼吁紧急关注FL中后门威胁的演变复杂性，并强调迫切需要先进的特征级防御技术。", "summary": "本论文提出了一种名为SPA的新型联邦学习后门攻击框架，旨在解决现有攻击中特征可检测性和持久性不足的问题。SPA通过在特征空间中对齐后门触发特征与目标类特征，而非传统的标签关联，实现了更隐蔽和持久的攻击。此外，该框架引入了自适应对抗性触发优化机制以增强攻击寿命和有效性。实验证明，SPA在多种FL场景下表现出高攻击成功率、低模型影响、强鲁棒性以及优越的持久性，揭示了联邦学习中后门威胁的日益复杂性，并强调了对特征级防御的迫切需求。", "keywords": "联邦学习, 后门攻击, 特征空间对齐, 隐蔽性, 持久性", "comments": "这项工作创新性地将后门攻击从传统的标签监督转向特征空间对齐，显著提高了攻击的隐蔽性和持久性。其提出的自适应对抗性触发优化机制也增强了攻击的鲁棒性。这项研究揭示了联邦学习中新的安全漏洞，对未来防御策略的开发具有重要指导意义，特别是需要关注特征层面的防御。"}}
{"id": "2506.20869", "title": "Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation", "authors": ["Md Toufique Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Ayman Asad Khan", "Mika Saari", "Pekka Abrahamsson"], "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.", "comment": "Accepted as a full paper to the 51st Euromicro Conference on Software\n  Engineering and Advanced Applications (SEAA 2025). 9 pages, 4 figures. This\n  is the preprint version and not the final camera ready version", "pdf_url": "http://arxiv.org/pdf/2506.20869v1", "categories": ["cs.SE", "cs.AI", "cs.IR", "D.2.11; I.2.6; H.3.3"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20869v1", "AI": {"title_translation": "为真实世界应用构建RAG系统：设计、开发与评估", "tldr": "本文介绍了为政府、网络安全、农业、工业研究和医学诊断等领域开发的五个真实世界RAG应用，通过100名用户的评估，并总结了12条关于RAG系统可靠性和可用性的实践经验。", "motivation": "现有研究缺乏关于基于真实世界用例、通过用户参与评估并系统记录经验教训的RAG系统开发的实证研究。", "method": "本文开发了五个针对真实世界场景的领域特定RAG应用，涵盖治理、网络安全、农业、工业研究和医学诊断。每个系统都集成了多语言OCR、通过向量嵌入进行的语义检索以及领域适应的LLM。通过一项涉及100名参与者的基于网络的评估，从易用性、相关性、透明度、响应性、准确性和推荐可能性六个维度对系统进行了评估。", "result": "基于用户反馈和开发经验，本文记录了十二条关键经验教训，强调了影响RAG系统在实践中可靠性和可用性的技术、操作和伦理挑战。", "conclusion": "本文通过开发和评估多个真实世界的RAG系统，填补了该领域实证研究的空白，并提供了关于RAG系统在实际应用中设计、开发和部署的关键经验和挑战的宝贵见解。", "translation": "检索增强生成（RAG）系统正成为将大型语言模型（LLM）基于外部知识的关键方法，以解决事实准确性和上下文相关性方面的局限性。然而，目前缺乏关于基于真实世界用例、通过普通用户参与评估并伴随系统性经验教训文档的RAG实现开发的实证研究。本文介绍了为治理、网络安全、农业、工业研究和医学诊断等真实世界场景开发的五个领域特定RAG应用。每个系统都集成了多语言OCR、通过向量嵌入进行的语义检索以及领域适应的LLM，通过本地服务器或云API部署以满足不同的用户需求。一项涉及总共100名参与者的基于网络的评估从六个维度对系统进行了评估：(i)易用性、(ii)相关性、(iii)透明度、(iv)响应性、(v)准确性，以及(vi)推荐可能性。基于用户反馈和我们的开发经验，我们记录了十二条关键经验教训，强调了影响RAG系统在实践中可靠性和可用性的技术、操作和伦理挑战。", "summary": "本文旨在弥补真实世界RAG系统开发和评估实证研究的不足。研究设计并实现了五个领域特定的RAG应用，涵盖政府、网络安全、农业、工业研究和医学诊断等多个实际场景。这些系统整合了多语言OCR、语义检索和领域适应的LLM。通过一项由100名用户参与的在线评估，从易用性、相关性、透明度、响应性、准确性和推荐可能性等六个维度对系统进行了综合评估。基于开发经验和用户反馈，论文总结了十二条关键经验教训，揭示了在实际部署RAG系统时面临的技术、操作和伦理挑战，为提升系统可靠性和可用性提供了实践指导。", "keywords": "RAG系统, 大型语言模型, 真实世界应用, 实证研究, 经验教训", "comments": "本文的创新之处在于提供了RAG系统在多个真实世界应用场景下的实证开发、部署和用户评估数据。这对于推动RAG技术从理论走向实际应用具有重要意义。所总结的十二条经验教训，特别是对技术、操作和伦理挑战的深入分析，为未来RAG系统的工程化提供了宝贵的实践指导和警示，有助于研究人员和开发者构建更可靠、更实用的RAG解决方案。"}}
{"id": "2506.21195", "title": "Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact", "authors": ["Neha Raghuvanshi"], "summary": "Have you wondered how cross-functional teams balance between maximizing value\nthat users derive and business growth leading to win-win situations? This case\nstudy shows how User Experience Research (UXR) and Data Science teams used\nmixed methods research to strategically influence Product Led Growth (PLG) for\na Password Manager used by million+ users, thus allowing our users, internal\nteams, and business to win. The audience will take away practical\nlessons/techniques related to leveraging mixed methods to: a. Maximize user\nvalue while meeting business growth goals b. Influence cross-functional teams\nc. Measure user and business impact This case study can be easily tied to the\nUXR Point of view pyramid (POV) [2] that represents a methodological approach\nto construct a POV and further dives into actioning POV to create measurable\nuser and business impact.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21195v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21195v1", "AI": {"title_translation": "追随用户需求，产品增长自然到来：一项将用户体验视角与增长联系起来并带来可衡量影响的混合方法案例研究", "tldr": "本案例研究展示了用户体验研究（UXR）和数据科学团队如何利用混合方法战略性地影响产品主导增长，从而实现用户价值最大化和业务增长的双赢。", "motivation": "探讨跨职能团队如何在最大化用户价值和实现业务增长之间取得平衡，以达到双赢局面。", "method": "用户体验研究（UXR）和数据科学团队使用混合方法研究，战略性地影响了拥有百万以上用户的密码管理器的产品主导增长（PLG）。该研究与用户体验研究的“观点金字塔”（POV）方法论相结合。", "result": "通过混合方法，成功帮助用户、内部团队和业务实现共赢，并带来了可衡量的用户和业务影响。", "conclusion": "本案例研究提供了利用混合方法来最大化用户价值、实现业务增长目标、影响跨职能团队以及衡量用户和业务影响的实用经验和技术。", "translation": "你想过跨职能团队如何平衡用户价值最大化和业务增长以实现双赢局面吗？本案例研究展示了用户体验研究（UXR）和数据科学团队如何利用混合方法研究，战略性地影响一款拥有百万以上用户的密码管理器的产品主导增长（PLG），从而使我们的用户、内部团队和业务都取得了成功。读者将从中汲取利用混合方法的实用经验/技术，以：a. 在满足业务增长目标的同时最大化用户价值；b. 影响跨职能团队；c. 衡量用户和业务影响。本案例研究可以轻松地与用户体验研究的“观点金字塔”（POV）[2]联系起来，该金字塔代表了一种构建观点的方法论，并进一步深入探讨如何将观点付诸行动以创造可衡量的用户和业务影响。", "summary": "本案例研究探讨了跨职能团队如何通过平衡用户价值和业务增长实现双赢。具体地，它展示了用户体验研究（UXR）和数据科学团队如何利用混合方法研究，成功地战略性影响了拥有百万以上用户的密码管理器的产品主导增长（PLG），从而实现了用户、团队和业务的共同成功。该研究提供了一系列实用经验，旨在帮助读者利用混合方法最大化用户价值、达成业务增长目标、影响跨职能团队并衡量用户和业务影响，并与用户体验研究的“观点金字塔”方法论紧密结合。", "keywords": "混合方法, 用户体验研究, 产品主导增长, 案例研究, 用户价值, 业务增长", "comments": "这篇论文通过一个具体的案例研究，展示了用户体验研究和数据科学的结合如何在实际业务中发挥战略性作用，实现用户价值与商业增长的双赢。其创新之处在于强调了“混合方法”和“用户体验视角（POV）”在产品增长中的实际应用，并提供了可衡量的影响。对于面临用户价值和业务增长平衡挑战的团队来说，具有重要的实践指导意义。"}}
{"id": "2506.21246", "title": "From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting", "authors": ["Giorgos Demosthenous", "Chryssis Georgiou", "Eliada Polydorou"], "summary": "This study investigates the impact of data source diversity on the\nperformance of cryptocurrency forecasting models by integrating various data\ncategories, including technical indicators, on-chain metrics, sentiment and\ninterest metrics, traditional market indices, and macroeconomic indicators. We\nintroduce the Crypto100 index, representing the top 100 cryptocurrencies by\nmarket capitalization, and propose a novel feature reduction algorithm to\nidentify the most impactful and resilient features from diverse data sources.\nOur comprehensive experiments demonstrate that data source diversity\nsignificantly enhances the predictive performance of forecasting models across\ndifferent time horizons. Key findings include the paramount importance of\non-chain metrics for both short-term and long-term predictions, the growing\nrelevance of traditional market indices and macroeconomic indicators for\nlonger-term forecasts, and substantial improvements in model accuracy when\ndiverse data sources are utilized. These insights help demystify the short-term\nand long-term driving factors of the cryptocurrency market and lay the\ngroundwork for developing more accurate and resilient forecasting models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21246v1", "categories": ["q-fin.PM", "cs.AI", "cs.ET", "cs.LG", "q-fin.ST"], "cate": "q-fin.PM", "url": "http://arxiv.org/abs/2506.21246v1", "AI": {"title_translation": "从链上到宏观：评估数据源多样性在加密货币市场预测中的重要性", "tldr": "本研究发现，整合链上、情感、传统市场和宏观经济等多源数据能显著提升加密货币市场预测模型的性能，尤其强调链上指标的重要性以及传统和宏观指标对长期预测的关联性。", "motivation": "本研究旨在通过整合多种数据类别，探讨数据源多样性对加密货币预测模型性能的影响。", "method": "研究整合了技术指标、链上指标、情绪和兴趣指标、传统市场指数以及宏观经济指标等多种数据类别。引入了代表市值前100位加密货币的Crypto100指数，并提出了一种新颖的特征降维算法，以识别最具影响力且稳健的特征。通过全面的实验来评估模型性能。", "result": "数据源多样性显著提升了预测模型在不同时间范围内的预测性能。链上指标对短期和长期预测都至关重要。传统市场指数和宏观经济指标对长期预测的关联性日益增强。利用多样化数据源能显著提高模型准确性。", "conclusion": "这些见解有助于揭示加密货币市场的短期和长期驱动因素，并为开发更准确、更具韧性的预测模型奠定基础。", "translation": "本研究通过整合包括技术指标、链上指标、情绪和兴趣指标、传统市场指数以及宏观经济指标在内的各种数据类别，探讨了数据源多样性对加密货币预测模型性能的影响。我们引入了代表市值前100位加密货币的Crypto100指数，并提出了一种新颖的特征降维算法，以从多样化的数据源中识别最具影响力且稳健的特征。我们全面的实验表明，数据源多样性显著提升了预测模型在不同时间范围内的预测性能。主要发现包括：链上指标对短期和长期预测都至关重要；传统市场指数和宏观经济指标对长期预测的关联性日益增强；以及利用多样化数据源能显著提高模型准确性。这些见解有助于揭示加密货币市场的短期和长期驱动因素，并为开发更准确、更具韧性的预测模型奠定基础。", "summary": "本研究探讨了数据源多样性（包括链上指标、情感、传统市场指数和宏观经济指标等）如何提升加密货币市场预测性能。研究引入了Crypto100指数和一种新的特征降维算法。实验结果表明，数据源多样性显著提高了模型的准确性，其中链上指标对所有时间范围的预测都至关重要，而宏观指标对长期预测的重要性日益增加，这有助于构建更稳健的预测模型。", "keywords": "加密货币预测, 数据源多样性, 链上指标, 宏观经济指标, 特征降维", "comments": "该论文的创新之处在于系统性地评估了数据源多样性对加密货币预测的影响，特别是将宏观经济和传统市场数据与加密货币特有的链上指标相结合。其重要性在于提供了多方面数据方法价值的实证证据，这有助于在波动市场中开发更准确、更具韧性的模型。Crypto100指数的引入和新颖的特征降维算法也增加了方法论的价值。"}}
{"id": "2506.20954", "title": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard Sensing", "authors": ["Xueming Liu", "Lin Li", "Xiang Zhou", "Qingrui Zhang", "Tianjiang Hu"], "summary": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations.", "comment": "8 Pages, 7 figures. Accepted by RA-L", "pdf_url": "http://arxiv.org/pdf/2506.20954v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20954v1", "AI": {"title_translation": "多旋翼系统基于机载传感的协同环绕飞行", "tldr": "提出了一种多旋翼无人机协同环绕框架，利用机载传感和改进卡尔曼滤波实现无外部定位系统下的目标跟踪，并在遮挡和故障条件下验证了其鲁棒性。", "motivation": "旨在为多旋翼系统提供一个无需外部定位系统即可包围和跟踪移动目标的协同环绕框架。", "method": "采用异构感知策略和状态估计算法评估旋翼-旋翼及旋翼-目标交互关系；开发了改进的卡尔曼滤波器融合视觉-惯性里程计与测距信息以提高旋翼间相对定位精度；设计了事件触发分布式卡尔曼滤波器，通过结合邻居测量和估计的旋翼间相对位置，在视觉遮挡下实现鲁棒的目标状态估计；并利用振荡器基的自主编队飞行策略构建了协同环绕控制器。", "result": "通过广泛的室内外实验验证了所提出环绕框架在遮挡环境下的效率。此外，四旋翼故障实验突出了该框架固有的容错特性。", "conclusion": "所提出的协同环绕框架无需外部定位系统即可有效跟踪移动目标，并在遮挡环境下表现出高效性与鲁棒性，具备固有的容错能力，有望应用于搜救行动。", "translation": "针对多旋翼系统，提出了一种协同环绕框架，用于在不依赖外部定位系统的情况下包围和跟踪移动目标。使用异构感知策略和相应的状态估计算法，评估了旋翼-旋翼和旋翼-目标交互之间的独特关系。开发了一种改进的卡尔曼滤波器，将视觉-惯性里程计与测距数据融合，以提高旋翼间相对定位的精度。设计了一种事件触发分布式卡尔曼滤波器，通过结合邻居测量和估计的旋翼间相对位置，在视觉遮挡下实现鲁棒的目标状态估计。利用估计结果，构建了一个协同环绕控制器，利用基于振荡器的自主编队飞行策略。我们进行了广泛的室内外实验，以验证所提出的环绕框架在遮挡环境中的效率。此外，一项旋翼故障实验突出了所提出框架固有的容错特性，强调了其在搜救行动中部署的潜力。", "summary": "本文提出了一种创新的多旋翼协同环绕框架，使其能够在没有外部定位系统的情况下，仅依靠机载传感来包围和跟踪移动目标。该框架融合了异构感知、改进卡尔曼滤波进行旋翼间高精度相对定位，以及事件触发分布式卡尔曼滤波以在视觉遮挡下鲁棒估计目标状态。通过基于振荡器的编队飞行策略构建控制器。实验证明了该框架在遮挡环境下的高效性及在故障条件下的容错能力，展现了其在搜救等实际应用中的巨大潜力。", "keywords": "多旋翼系统, 协同环绕, 机载传感, 卡尔曼滤波, 目标跟踪, 容错性", "comments": "这篇论文的创新点在于提出了一个完全依赖机载传感器的多旋翼协同环绕框架，克服了对外部定位系统的依赖，这对于野外或GPS受限环境下的应用至关重要。其结合多种卡尔曼滤波变体处理不同层面的估计问题（旋翼间相对定位和目标状态估计）显得精巧。尤其是在遮挡和故障情况下的验证，极大地增强了其在实际复杂场景中部署的可信度，特别是其在搜救任务中的潜在应用前景。"}}
{"id": "2506.20980", "title": "Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs", "authors": ["Ziyu Zheng", "Yaming Yang", "Ziyu Guan", "Wei Zhao", "Weigang Lu"], "summary": "Real-world networks usually have a property of node heterophily, that is, the\nconnected nodes usually have different features or different labels. This\nheterophily issue has been extensively studied in homogeneous graphs but\nremains under-explored in heterogeneous graphs, where there are multiple types\nof nodes and edges. Capturing node heterophily in heterogeneous graphs is very\nchallenging since both node/edge heterogeneity and node heterophily should be\ncarefully taken into consideration. Existing methods typically convert\nheterogeneous graphs into homogeneous ones to learn node heterophily, which\nwill inevitably lose the potential heterophily conveyed by heterogeneous\nrelations. To bridge this gap, we propose Relation-Aware Separation of\nHomophily and Heterophily (RASH), a novel contrastive learning framework that\nexplicitly models high-order semantics of heterogeneous interactions and\nadaptively separates homophilic and heterophilic patterns. Particularly, RASH\nintroduces dual heterogeneous hypergraphs to encode multi-relational bipartite\nsubgraphs and dynamically constructs homophilic graphs and heterophilic graphs\nbased on relation importance. A multi-relation contrastive loss is designed to\nalign heterogeneous and homophilic/heterophilic views by maximizing mutual\ninformation. In this way, RASH simultaneously resolves the challenges of\nheterogeneity and heterophily in heterogeneous graphs. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of RASH across various\ndownstream tasks. The code is available at:\nhttps://github.com/zhengziyu77/RASH.", "comment": "accepted by KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.20980v1", "categories": ["cs.SI", "cs.AI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20980v1", "AI": {"title_translation": "增强同质性-异质性分离：异构图中的关系感知学习", "tldr": "本文提出RASH，一个新颖的对比学习框架，用于在异构图中有效处理节点异质性问题，通过关系感知学习来分离同质性和异质性模式。", "motivation": "现实世界网络中普遍存在节点异质性，即连接的节点特征或标签不同。同质图中的异质性问题已得到广泛研究，但在异构图中仍未充分探索，因为异构图包含多种节点和边类型，同时考虑节点/边异构性和节点异质性极具挑战。现有方法通常将异构图转换为同质图来学习节点异质性，但这会不可避免地丢失异构关系所传达的潜在异质性。", "method": "本文提出了关系感知同质异质分离（RASH），一个新颖的对比学习框架。RASH通过以下方式显式建模异构交互的高阶语义并自适应地分离同质和异质模式：1. 引入双重异构超图来编码多关系二分图；2. 基于关系重要性动态构建同质图和异质图；3. 设计多关系对比损失，通过最大化互信息来对齐异构视图以及同质/异质视图。", "result": "在基准数据集上的大量实验表明，RASH在各种下游任务中都表现出有效性。", "conclusion": "RASH框架能够同时解决异构图中异构性和异质性的挑战，并在多个下游任务中表现出优异的性能，有效增强了异构图中的同质性-异质性分离。", "translation": "现实世界网络通常具有节点异质性，即连接的节点通常具有不同的特征或不同的标签。这种异质性问题已在同质图中得到广泛研究，但在异构图中仍未充分探索，异构图包含多种类型的节点和边。在异构图中捕获节点异质性非常具有挑战性，因为节点/边异构性和节点异质性都应仔细考虑。现有方法通常将异构图转换为同质图来学习节点异质性，这将不可避免地丢失异构关系所传达的潜在异质性。为了弥补这一差距，我们提出了关系感知同质异质分离（RASH），一个新颖的对比学习框架，它显式建模异构交互的高阶语义并自适应地分离同质和异质模式。特别是，RASH引入了双重异构超图来编码多关系二分图，并基于关系重要性动态构建同质图和异质图。设计了一种多关系对比损失，通过最大化互信息来对齐异构视图以及同质/异质视图。通过这种方式，RASH同时解决了异构图中异构性和异质性的挑战。在基准数据集上的大量实验证明了RASH在各种下游任务中的有效性。代码可在：https://github.com/zhengziyu77/RASH 获取。", "summary": "本文提出了一种名为RASH（关系感知同质异质分离）的新型对比学习框架，旨在解决异构图中节点异质性学习的挑战。针对现有方法在转换异构图时丢失异质性信息的缺点，RASH通过引入双重异构超图、动态构建同质/异质图以及设计多关系对比损失，显式地建模异构交互的高阶语义，并自适应地分离同质和异质模式。实验结果表明RASH在处理异构图的异构性和异质性问题上表现出有效性。", "keywords": "异构图, 节点异质性, 对比学习, 关系感知学习, 同质性-异质性分离", "comments": "本文提出的RASH框架通过引入关系感知机制和对比学习，有效解决了异构图中节点异质性学习的痛点。其创新点在于显式建模多关系高阶语义，并能自适应分离同质和异质模式，避免了传统方法信息丢失的问题。该方法对于理解和处理复杂异构网络中的节点关系具有重要意义。"}}
{"id": "2506.21134", "title": "Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations", "authors": ["Jacopo Bufalino", "Jose Luis Martin-Navarro", "Mario Di Francesco", "Tuomas Aura"], "summary": "Kubernetes has emerged as the de facto standard for container orchestration.\nUnfortunately, its increasing popularity has also made it an attractive target\nfor malicious actors. Despite extensive research on securing Kubernetes, little\nattention has been paid to the impact of network configuration on the security\nof application deployments. This paper addresses this gap by conducting a\ncomprehensive analysis of network misconfigurations in a Kubernetes cluster\nwith specific reference to lateral movement. Accordingly, we carried out an\nextensive evaluation of 287 open-source applications belonging to six different\norganizations, ranging from IT companies and public entities to non-profits. As\na result, we identified 634 misconfigurations, well beyond what could be found\nby solutions in the state of the art. We responsibly disclosed our findings to\nthe concerned organizations and engaged in a discussion to assess their\nseverity. As of now, misconfigurations affecting more than thirty applications\nhave been fixed with the mitigations we proposed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21134v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21134v1", "AI": {"title_translation": "内部工作：防御Kubernetes集群的网络错误配置", "tldr": "本文分析了Kubernetes集群中的网络错误配置，发现大量现有工具未能识别的漏洞，并协助相关组织进行了修复。", "motivation": "尽管对Kubernetes的安全研究广泛，但很少有研究关注网络配置对应用程序部署安全的影响，特别是与横向移动相关的部分，本文旨在填补这一空白。", "method": "研究人员对来自六个不同组织（包括IT公司、公共实体和非营利组织）的287个开源应用程序进行了广泛评估，以分析Kubernetes集群中的网络错误配置。", "result": "识别出634个网络错误配置，远超现有解决方案的发现能力。研究人员已负责任地向相关组织披露了发现，并已促使30多个应用程序的错误配置得到修复。", "conclusion": "通过对Kubernetes集群中网络错误配置的全面分析和发现披露，本文强调了网络配置对应用安全的重要性，并证明了其方法在识别和解决这些漏洞方面的有效性。", "translation": "Kubernetes已成为容器编排的事实标准。不幸的是，其日益增长的普及也使其成为恶意攻击者的诱人目标。尽管对Kubernetes的安全研究广泛，但很少有研究关注网络配置对应用程序部署安全的影响。本文通过对Kubernetes集群中的网络错误配置进行全面分析，特别提及横向移动，来弥补这一空白。因此，我们对来自六个不同组织（包括IT公司、公共实体和非营利组织）的287个开源应用程序进行了广泛评估。结果，我们识别出634个错误配置，远超现有解决方案所能发现的数量。我们负责任地向相关组织披露了我们的发现，并进行了讨论以评估其严重性。截至目前，受我们提出的缓解措施影响的30多个应用程序的错误配置已得到修复。", "summary": "本文针对Kubernetes集群中网络配置对应用安全的影响这一研究空白，进行了全面的网络错误配置分析，尤其关注横向移动。通过对287个开源应用的评估，发现了634个错误配置，远超现有工具的发现能力。研究团队已将发现告知相关组织，并成功推动了30多个应用程序的错误配置修复，突显了网络配置安全的重要性及现有工具的不足。", "keywords": "Kubernetes, 网络配置, 安全, 错误配置, 横向移动", "comments": "这篇论文通过对大量实际开源应用的分析，揭示了Kubernetes网络配置中普遍存在的安全漏洞。其发现的数量远超现有工具，具有重要的实践意义。研究不仅停留在发现问题，还负责任地披露并协助修复，体现了很强的实用性和影响力，对提升Kubernetes集群的实际安全性具有积极作用。"}}
{"id": "2506.20743", "title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "summary": "Foundation models (FMs) are catalyzing a transformative shift in materials\nscience (MatSci) by enabling scalable, general-purpose, and multimodal AI\nsystems for scientific discovery. Unlike traditional machine learning models,\nwhich are typically narrow in scope and require task-specific engineering, FMs\noffer cross-domain generalization and exhibit emergent capabilities. Their\nversatility is especially well-suited to materials science, where research\nchallenges span diverse data types and scales. This survey provides a\ncomprehensive overview of foundation models, agentic systems, datasets, and\ncomputational tools supporting this growing field. We introduce a task-driven\ntaxonomy encompassing six broad application areas: data extraction,\ninterpretation and Q\\&A; atomistic simulation; property prediction; materials\nstructure, design and discovery; process planning, discovery, and optimization;\nand multiscale modeling. We discuss recent advances in both unimodal and\nmultimodal FMs, as well as emerging large language model (LLM) agents.\nFurthermore, we review standardized datasets, open-source tools, and autonomous\nexperimental platforms that collectively fuel the development and integration\nof FMs into research workflows. We assess the early successes of foundation\nmodels and identify persistent limitations, including challenges in\ngeneralizability, interpretability, data imbalance, safety concerns, and\nlimited multimodal fusion. Finally, we articulate future research directions\ncentered on scalable pretraining, continual learning, data governance, and\ntrustworthiness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20743v1", "categories": ["cs.LG", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20743v1", "AI": {"title_translation": "材料科学中的人工智能综述：基础模型、LLM智能体、数据集和工具", "tldr": "这篇综述全面概述了基础模型（FMs）如何通过其通用性和新兴能力，在材料科学（MatSci）中实现可扩展、通用和多模态的人工智能系统，并讨论了其应用、挑战和未来方向。", "motivation": "基础模型（FMs）正在推动材料科学的变革，实现可扩展、通用和多模态的AI系统，以加速科学发现。与传统机器学习模型不同，FMs具有跨领域泛化和新兴能力，特别适用于材料科学中多样化的数据类型和尺度的挑战。因此，需要一份全面的综述来概述这一领域的发展。", "method": "这篇综述提供了一个全面的概述，涵盖基础模型、智能体系统、数据集和支持该领域的计算工具。它引入了一个任务驱动的分类法，包括六个广泛的应用领域：数据提取、解释和问答；原子模拟；属性预测；材料结构、设计和发现；过程规划、发现和优化；以及多尺度建模。它讨论了单模态和多模态FMs以及新兴LLM智能体的最新进展，并审查了标准化数据集、开源工具和自主实验平台。", "result": "综述评估了基础模型的早期成功，并指出了持续存在的局限性，包括泛化性、可解释性、数据不平衡、安全问题和有限的多模态融合方面的挑战。", "conclusion": "综述最后阐述了未来的研究方向，重点是可扩展的预训练、持续学习、数据治理和可信度。", "translation": "基础模型（FMs）正在通过实现可扩展、通用和多模态的人工智能系统，为科学发现催化材料科学（MatSci）的变革性转变。与通常范围狭窄且需要针对特定任务进行工程设计的传统机器学习模型不同，FMs提供跨领域泛化并展现出涌现能力。它们的通用性特别适合材料科学，因为材料科学的研究挑战涵盖多样的数据类型和尺度。本综述全面概述了支持这一不断发展领域的基础模型、智能体系统、数据集和计算工具。我们引入了一个任务驱动的分类法，涵盖六个广泛的应用领域：数据提取、解释和问答；原子模拟；属性预测；材料结构、设计和发现；过程规划、发现和优化；以及多尺度建模。我们讨论了单模态和多模态FMs的最新进展，以及新兴的大型语言模型（LLM）智能体。此外，我们审查了标准化数据集、开源工具和自主实验平台，它们共同推动了FMs在研究工作流程中的开发和整合。我们评估了基础模型的早期成功，并指出了持续存在的局限性，包括泛化性、可解释性、数据不平衡、安全问题和有限的多模态融合方面的挑战。最后，我们阐明了以可扩展预训练、持续学习、数据治理和可信度为中心的未来研究方向。", "summary": "这篇综述探讨了基础模型（FMs）如何通过其通用性和新兴能力变革材料科学。它全面概述了FMs、LLM智能体、数据集和工具在材料科学中的应用，并提出了一个任务驱动的分类法，涵盖六个主要应用领域。文章讨论了FMs的最新进展，审查了相关资源，评估了其早期成功，并指出了泛化性、可解释性、数据不平衡等局限性。最后，它提出了可扩展预训练、持续学习、数据治理和可信度等未来研究方向。", "keywords": "基础模型, 材料科学, 人工智能, LLM智能体, 综述", "comments": "这篇综述非常及时且全面，它系统地梳理了基础模型在材料科学领域的应用现状、挑战和未来趋势。其提出的任务驱动分类法有助于理解FMs的广泛适用性。同时，文章不仅关注了技术进展，还强调了数据治理和可信度等重要非技术因素，具有重要的指导意义。"}}
{"id": "2506.20949", "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation", "authors": ["Chenkai Sun", "Denghui Zhang", "ChengXiang Zhai", "Heng Ji"], "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20949v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20949v1", "AI": {"title_translation": "超越被动安全：通过长周期模拟实现风险感知LLM对齐", "tldr": "本文提出了一个概念验证框架和新的数据集，旨在通过长周期模拟来提升语言模型的风险感知和长期安全对齐，并在新旧安全基准上取得了显著改进。", "motivation": "鉴于语言模型代理在公共政策和医疗保健等高风险社会决策中日益增长的影响力，确保其有益影响需要理解其建议的深远影响。现有安全措施可能过于被动，未能充分考虑长期、间接的风险。", "method": "提出一个概念验证框架，用于宏观尺度上模拟模型生成建议在社会系统中随时间传播的方式，以实现更稳健的对齐。此外，引入了一个包含100个间接危害场景的数据集，用于评估模型预测看似无害的用户提示可能导致的负面、不明显结果的能力。", "result": "该方法在新数据集上实现了超过20%的改进，并且在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）上，相对于强基线取得了超过70%的平均胜率。", "conclusion": "这项工作为开发更安全的代理提供了一个有前景的方向，通过超越被动安全，转向通过长周期模拟实现风险感知的LLM对齐。", "translation": "鉴于基于语言模型的代理在公共政策到医疗保健等高风险社会决策中日益增长的影响力，确保其有益影响需要理解其建议的深远影响。我们提出了一个概念验证框架，该框架预测模型生成的建议如何随时间在宏观层面上传播到社会系统中，从而实现更稳健的对齐。为了评估语言模型的长期安全意识，我们还引入了一个包含100个间接危害场景的数据集，测试模型预测看似无害的用户提示可能导致的负面、不明显结果的能力。我们的方法不仅在新数据集上取得了超过20%的改进，而且在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）上，相对于强基线，平均胜率超过70%，这表明了开发更安全代理的一个有前景的方向。", "summary": "本文提出了一个超越传统被动安全的风险感知LLM对齐框架，通过长周期模拟来预测语言模型建议在社会系统中的长期影响。为评估模型的长期安全意识，还引入了一个包含100个间接危害场景的新数据集。实验结果表明，该方法在新数据集上表现显著，并在现有安全基准上超越了强基线，为构建更安全的AI代理提供了有价值的方向。", "keywords": "语言模型对齐, 风险感知, 长周期模拟, AI安全, 间接危害", "comments": "这项工作创新性地将LLM安全对齐的视角从即时、被动的响应扩展到对长期、间接社会影响的风险感知和预测。通过引入长周期模拟和间接危害数据集，解决了现有安全评估可能忽视的复杂链式反应问题，对于开发真正负责任和安全的AI具有重要意义。"}}
{"id": "2506.20938", "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks", "authors": ["Joshua H. Davis", "Daniel Nichols", "Ishan Khillan", "Abhinav Bhatele"], "summary": "GPGPU architectures have become significantly diverse in recent years, which\nhas led to an emergence of a variety of specialized programming models and\nsoftware stacks to support them. While portable execution models exist, they\nstill require significant developer effort to port to and optimize for\ndifferent hardware architectures. Recent advances in large language models\n(LLMs) can help us reduce some of this programmer burden. In this paper, we\npresent a novel benchmark and testing framework, ParEval-Repo, which can be\nused to evaluate the efficacy of LLM-based approaches in automatically\ntranslating entire codebases across GPGPU execution models. ParEval-Repo\nincludes several scientific computing and AI mini-applications in a range of\nprogramming models, and levels of repository complexity. We use ParEval-Repo to\nevaluate a range of state-of-the-art open-source and commercial LLMs, with both\na non-agentic and a top-down agentic approach. We assess code generated by the\nLLMs and approaches in terms of compilability, functional correctness,\ncategories of build errors, and the cost of translation in terms of the number\nof inference tokens. Our results demonstrate that LLM translation of scientific\napplications is feasible for small programs but difficulty with generating\nfunctional build systems and cross-file dependencies pose challenges in scaling\nto larger codebases.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20938v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20938v1", "AI": {"title_translation": "ParEval-Repo：一个用于评估LLM在仓库级HPC翻译任务中表现的基准测试套件", "tldr": "提出了ParEval-Repo，一个基准测试套件，用于评估LLM在GPGPU代码库翻译任务中的能力，发现LLM对小程序可行，但大型代码库面临构建系统和跨文件依赖挑战。", "motivation": "近年来，GPGPU架构日益多样化，导致了多种专用编程模型和软件栈的出现。尽管存在可移植的执行模型，但将代码移植并优化到不同硬件架构仍需大量的开发人员工作。大型语言模型（LLMs）的最新进展有望减轻这种编程负担。", "method": "本文提出了一个名为ParEval-Repo的新型基准测试和测试框架，用于评估基于LLM的方法在自动翻译GPGPU执行模型间整个代码库的效率。ParEval-Repo包含多个科学计算和AI小型应用程序，涵盖多种编程模型和仓库复杂性。作者使用ParEval-Repo评估了一系列最先进的开源和商业LLM，采用了非代理和自上而下的代理方法。评估指标包括代码的可编译性、功能正确性、构建错误的类别以及翻译成本（推理token数量）。", "result": "结果表明，LLM翻译科学应用程序对于小型程序是可行的，但生成功能性构建系统和处理跨文件依赖的困难对扩展到更大的代码库构成了挑战。", "conclusion": "LLM在GPGPU代码库翻译方面对于小型程序具有潜力，但在处理复杂构建系统和跨文件依赖时，其有效性在大规模应用中受到限制。", "translation": "近年来，GPGPU架构变得异常多样化，这导致了各种专用编程模型和软件栈的出现以支持它们。尽管存在可移植的执行模型，但它们仍然需要开发人员付出大量努力才能移植到不同的硬件架构并进行优化。大型语言模型（LLMs）的最新进展可以帮助我们减轻部分编程负担。在本文中，我们提出了一个新颖的基准测试和测试框架ParEval-Repo，它可以用于评估基于LLM的方法在GPGPU执行模型之间自动翻译整个代码库的效率。ParEval-Repo包含多个科学计算和AI小型应用程序，涵盖了多种编程模型和仓库复杂性级别。我们使用ParEval-Repo评估了一系列最先进的开源和商业LLM，采用了非代理和自上而下的代理方法。我们根据可编译性、功能正确性、构建错误的类别以及翻译成本（推理token数量）来评估LLM和方法生成的代码。我们的结果表明，LLM翻译科学应用程序对于小型程序是可行的，但生成功能性构建系统和处理跨文件依赖的困难对扩展到更大的代码库构成了挑战。", "summary": "本文介绍了ParEval-Repo，一个新颖的基准测试套件和框架，旨在评估大型语言模型（LLMs）在自动将整个代码库从一种GPGPU执行模型翻译到另一种执行模型方面的能力。ParEval-Repo包含多种科学计算和AI小型应用程序，涵盖不同编程模型和仓库复杂性。研究评估了多种LLM，并根据可编译性、功能正确性、构建错误和推理成本进行衡量。结果显示LLM对小型程序的代码翻译可行，但在处理复杂构建系统和跨文件依赖时，扩展到大型代码库面临挑战。", "keywords": "LLM, 代码翻译, GPGPU, 基准测试, HPC", "comments": "本文提出了一个及时且重要的基准测试套件ParEval-Repo，解决了GPGPU架构多样化带来的代码移植挑战，并探索了LLM在此领域的应用潜力。其创新点在于专注于仓库级别的代码翻译，并评估了LLM在处理跨文件依赖和构建系统方面的能力。研究结果指出了LLM在代码翻译领域的局限性，特别是在处理大型复杂项目时的挑战，这为未来研究提供了明确的方向。"}}
{"id": "2506.20786", "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20786v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20786v1", "AI": {"title_translation": "AI驱动的MRI脑肿瘤分割基准测试", "tldr": "本研究评估了多种基于SAM的模型（SAM, SAM 2, MedSAM, SAM-Med-3D）和nnU-Net在BraTS 2023数据集上的零样本脑肿瘤分割性能，发现虽然SAM系列模型在精确边界框提示下表现出色，但nnU-Net在实际应用中仍占主导地位，微调后点提示性能有所提升但仍不及边界框或nnU-Net。", "motivation": "当前缺乏对各种通用提示模型及其医学变体在通用医学数据集上，针对不同提示质量进行评估和比较。", "method": "使用Segment Anything Model (SAM)、Segment Anything Model 2 (SAM 2)、MedSAM、SAM-Med-3D和nnU-Net在BraTS 2023成人胶质瘤和儿科数据集上进行零样本推理，评估点和边界框两种提示质量。此外，还在儿科数据集上对SAM、SAM 2、MedSAM和SAM-Med-3D进行了微调，并评估了性能改进。", "result": "在极度精确的边界框提示下，SAM和SAM 2的Dice分数分别高达0.894和0.893，超过了nnU-Net的分割性能。然而，由于提供高精度提示的不切实际性，nnU-Net仍然是主要的医学图像分割网络。微调后，点提示性能有显著提升，但仍未能达到边界框或nnU-Net的分割效果。", "conclusion": "尽管基于SAM的模型在特定理想条件下表现优异，但考虑到实际应用中获取高精度提示的挑战，nnU-Net在医学图像分割领域仍保持其主导地位。微调可以显著提升点提示的性能，但仍有待进一步研究以超越现有最佳方案。", "translation": "医学图像分割极大地辅助了医学诊断，其中基于U-Net的架构和nnU-Net提供了最先进的性能。近年来，已经引入了许多通用可提示模型和医学变体，但目前缺乏在通用医学数据集上，针对各种提示质量对这些模型进行评估和比较。本研究使用Segment Anything Model (SAM)、Segment Anything Model 2 (SAM 2)、MedSAM、SAM-Med-3D和nnU-Net在BraTS 2023成人胶质瘤和儿科数据集上进行零样本推理，针对点和边界框两种提示质量进行评估。其中几个模型表现出有希望的Dice分数，特别是SAM和SAM 2在给定极其精确的边界框提示时，分别达到了0.894和0.893的分数，这超过了nnU-Net的分割性能。然而，由于向模型提供高度精确提示的不切实际性，nnU-Net仍然是主要的医学图像分割网络。模型和提示评估以及比较通过在儿科数据集上微调SAM、SAM 2、MedSAM和SAM-Med-3D而得到扩展。微调后点提示性能的提升是显著的，并显示出未来研究的潜力，但仍未能实现比边界框或nnU-Net更好的分割效果。", "summary": "本文评估了SAM系列模型（SAM、SAM 2、MedSAM、SAM-Med-3D）和nnU-Net在BraTS 2023数据集上进行脑肿瘤分割的性能。研究发现，在提供极精确边界框提示时，SAM和SAM 2的表现优于nnU-Net。然而，考虑到实际应用中高精度提示的获取难度，nnU-Net仍是主流选择。文章还探讨了模型微调对点提示性能的改善，尽管有显著提升，但仍未能超越边界框提示或nnU-Net。", "keywords": "脑肿瘤分割, SAM, nnU-Net, 零样本推理, 医学图像分割", "comments": "这项研究通过系统地评估多种基于SAM的通用可提示模型在医学图像分割领域的零样本和微调性能，填补了当前医学图像分割领域对这类模型缺乏统一评估的空白。其创新之处在于对比了不同提示质量（点和边界框）对模型性能的影响，并指出了通用大模型在医学应用中面临的实际挑战（即难以获得高精度提示）。尽管SAM模型在理想条件下表现出色，但研究强调了nnU-Net在实际场景中的鲁棒性和实用性，这为医学图像分割领域未来的研究方向提供了重要参考。"}}
{"id": "2506.20701", "title": "Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models", "authors": ["Vineet Jain", "Kusha Sareen", "Mohammad Pedramfar", "Siamak Ravanbakhsh"], "summary": "Adapting a pretrained diffusion model to new objectives at inference time\nremains an open problem in generative modeling. Existing steering methods\nsuffer from inaccurate value estimation, especially at high noise levels, which\nbiases guidance. Moreover, information from past runs is not reused to improve\nsample quality, resulting in inefficient use of compute. Inspired by the\nsuccess of Monte Carlo Tree Search, we address these limitations by casting\ninference-time alignment as a search problem that reuses past computations. We\nintroduce a tree-based approach that samples from the reward-aligned target\ndensity by propagating terminal rewards back through the diffusion chain and\niteratively refining value estimates with each additional generation. Our\nproposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact\nsamples from the target distribution in the limit of infinite rollouts, and its\ngreedy variant, Diffusion Tree Search (DTS$^\\star$), performs a global search\nfor high reward samples. On MNIST and CIFAR-10 class-conditional generation,\nDTS matches the FID of the best-performing baseline with up to $10\\times$ less\ncompute. In text-to-image generation and language completion tasks, DTS$^\\star$\neffectively searches for high reward samples that match best-of-N with up to\n$5\\times$ less compute. By reusing information from previous generations, we\nget an anytime algorithm that turns additional compute into steadily better\nsamples, providing a scalable approach for inference-time alignment of\ndiffusion models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20701v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20701v1", "AI": {"title_translation": "扩散树采样：扩散模型可扩展的推理时对齐", "tldr": "本文提出了一种名为扩散树采样（DTS）的新方法，通过将推理时对齐视为一个搜索问题并重用过去的计算，解决了现有扩散模型推理时自适应方法的局限性。DTS在计算效率显著提高的情况下，实现了与现有最佳方法相当或更好的性能。", "motivation": "预训练扩散模型在推理时适应新目标仍是一个开放问题。现有引导方法在噪声水平较高时存在不准确的价值估计，导致偏差。此外，它们不重用过去运行的信息来提高样本质量，导致计算效率低下。", "method": "受蒙特卡洛树搜索的启发，本文将推理时对齐问题转化为一个搜索问题，并重用过去的计算。提出了一种基于树的方法，通过将最终奖励反向传播回扩散链，并随着每次额外生成迭代地细化价值估计，从奖励对齐的目标密度中采样。所提出的方法包括扩散树采样（DTS）和其贪婪变体扩散树搜索（DTS*）。", "result": "在MNIST和CIFAR-10类条件生成任务中，DTS以高达10倍的计算量减少匹配了最佳基线的FID。在文本到图像生成和语言补全任务中，DTS*有效地搜索高奖励样本，以高达5倍的计算量减少匹配了best-of-N的结果。通过重用前几代的信息，该算法可以随时将额外的计算转化为持续改进的样本。", "conclusion": "扩散树采样（DTS）提供了一种可扩展的扩散模型推理时对齐方法。通过将推理时对齐视为一个搜索问题并重用过去计算，DTS解决了现有方法的局限性，实现了计算效率的显著提升，同时保持了样本质量，甚至在某些任务上表现更优。", "translation": "将预训练的扩散模型在推理时适应新的目标仍然是生成建模中的一个开放问题。现有的引导方法存在价值估计不准确的问题，尤其是在高噪声水平下，这会导致引导偏差。此外，过去运行的信息没有被重用来提高样本质量，导致计算使用效率低下。受蒙特卡洛树搜索成功的启发，我们通过将推理时对齐视为一个重用过去计算的搜索问题来解决这些限制。我们引入了一种基于树的方法，通过将最终奖励反向传播回扩散链，并随着每次额外生成迭代地细化价值估计，从奖励对齐的目标密度中采样。我们提出的方法，扩散树采样（DTS），在无限次展开的极限下，可以从目标分布中产生渐近精确的样本，其贪婪变体，扩散树搜索（DTS*），则执行全局搜索以寻找高奖励样本。在MNIST和CIFAR-10类条件生成任务中，DTS以高达10倍的计算量减少匹配了表现最佳基线的FID。在文本到图像生成和语言补全任务中，DTS*有效地搜索高奖励样本，以高达5倍的计算量减少匹配了best-of-N的结果。通过重用前几代的信息，我们获得了一个随时可用的算法，可以将额外的计算转化为持续更好的样本，为扩散模型的推理时对齐提供了一种可扩展的方法。", "summary": "本文提出了一种新颖的扩散树采样（DTS）方法，用于解决预训练扩散模型在推理时自适应新目标的挑战。现有方法面临价值估计不准确和计算效率低下的问题。受蒙特卡洛树搜索的启发，DTS将推理时对齐问题重新定义为一种搜索，通过树状结构重用过去的计算，并迭代优化价值估计。DTS在理论上能产生渐近精确的样本，其贪婪变体DTS*则专注于寻找高奖励样本。实验证明，DTS在图像生成任务中能以显著更少的计算量（高达10倍）达到与最佳基线相当的性能，而在文本到图像和语言补全任务中，DTS*也能以高达5倍的计算量减少匹配最佳结果。该方法通过重用信息，实现了可伸缩且计算高效的扩散模型推理时对齐。", "keywords": "扩散模型, 推理时对齐, 树采样, 计算效率, 生成模型", "comments": "本文创新性地将蒙特卡洛树搜索的思想引入到扩散模型的推理时对齐问题中，通过构建决策树和重用计算，有效解决了现有方法在价值估计不准确和计算效率低下的问题。其“随时可用”的特性和显著的计算效率提升（高达10倍）是其重要贡献，为扩散模型在实际应用中的部署提供了更具扩展性的解决方案。"}}
{"id": "2506.21370", "title": "Cluster-Aware Two-Stage Method for Fast Iterative MIMO Detection in LEO Satellite Communications", "authors": ["Jiuyu Liu", "Yi Ma", "Qihao Peng", "Rahim Tafazolli"], "summary": "In this paper, a cluster-aware two-stage multiple-input multiple-output\n(MIMO) detection method is proposed for direct-to-cell satellite\ncommunications. The method achieves computational efficiency by exploiting a\ndistinctive property of satellite MIMO channels: users within the same\ngeographical cluster exhibit highly correlated channel characteristics due to\ntheir physical proximity, which typically impedes convergence in conventional\niterative MIMO detectors. The proposed method implements a two-stage strategy\nthat first eliminates intra-cluster interference using computationally\nefficient small matrix inversions, then utilizes these pre-computed matrices to\naccelerate standard iterative MIMO detectors such as Gauss-Seidel (GS) and\nsymmetric successive over-relaxation (SSOR) for effective inter-cluster\ninterference cancellation. Computer simulations demonstrate that the proposed\nmethod achieves more than 12 times faster convergence under perfect channel\nstate information. Even when accounting for channel estimation errors, the\nmethod maintains 9 times faster convergence, demonstrating its robustness and\neffectiveness for next-generation satellite MIMO communications.", "comment": "This work has been accepted by IEEE/CIC ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2506.21370v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21370v1", "AI": {"title_translation": "低轨卫星通信中用于快速迭代MIMO检测的聚类感知两阶段方法", "tldr": "本文提出了一种用于低轨卫星通信的聚类感知两阶段MIMO检测方法，通过利用卫星MIMO信道的特性，显著提高了迭代检测器的收敛速度和效率，即使存在信道估计误差也能保持鲁棒性。", "motivation": "在卫星MIMO通信中，同一地理簇内的用户由于物理接近，其信道特性高度相关，这通常会阻碍传统迭代MIMO检测器的收敛。因此，需要一种计算效率更高的方法来解决这一问题。", "method": "提出了一种聚类感知两阶段MIMO检测方法。第一阶段利用计算高效的小矩阵求逆消除簇内干扰；第二阶段利用这些预计算的矩阵来加速标准迭代MIMO检测器（如高斯-赛德尔和对称逐次超松弛），以有效消除簇间干扰。", "result": "在理想信道状态信息下，所提出的方法实现了超过12倍的收敛速度提升。即使考虑到信道估计误差，该方法仍能保持9倍的收敛速度提升。", "conclusion": "所提出的聚类感知两阶段MIMO检测方法在低轨卫星通信中表现出鲁棒性和有效性，适用于下一代卫星MIMO通信。", "translation": "本文提出了一种用于直通蜂窝卫星通信的聚类感知两阶段多输入多输出（MIMO）检测方法。该方法通过利用卫星MIMO信道的独特特性实现了计算效率，即同一地理簇内的用户由于物理接近而表现出高度相关的信道特性，这通常会阻碍传统迭代MIMO检测器的收敛。所提出的方法采用两阶段策略，首先使用计算高效的小矩阵求逆消除簇内干扰，然后利用这些预计算的矩阵来加速标准迭代MIMO检测器（如高斯-赛德尔（GS）和对称逐次超松弛（SSOR）），以有效消除簇间干扰。计算机仿真结果表明，在理想信道状态信息下，所提出的方法实现了超过12倍的收敛速度提升。即使考虑到信道估计误差，该方法仍能保持9倍的收敛速度提升，这表明其对下一代卫星MIMO通信的鲁棒性和有效性。", "summary": "本文针对直通蜂窝卫星通信提出了一种聚类感知两阶段MIMO检测方法。该方法利用卫星MIMO信道中同簇用户信道相关性高的特点，通过先消除簇内干扰，再加速传统迭代检测器消除簇间干扰，显著提高了计算效率和收敛速度。仿真结果表明，该方法在理想和非理想信道条件下均能实现显著的收敛速度提升，展现了其在未来卫星MIMO通信中的潜力和鲁棒性。", "keywords": "MIMO检测, 卫星通信, 聚类感知, 两阶段方法, 快速迭代", "comments": "该论文的创新点在于提出了一个两阶段的聚类感知方法，有效地利用了卫星MIMO信道的特定物理特性（同簇信道相关性），从而解决了传统迭代检测器收敛慢的问题。这种方法通过分解问题，将复杂的全局干扰消除转化为更易处理的簇内和簇间干扰消除，显著提升了计算效率和收敛速度，对下一代卫星通信系统具有重要意义。"}}
{"id": "2506.20828", "title": "Practical and Accurate Local Edge Differentially Private Graph Algorithms", "authors": ["Pranay Mundra", "Charalampos Papamanthou", "Julian Shun", "Quanquan C. Liu"], "summary": "The rise of massive networks across diverse domains necessitates\nsophisticated graph analytics, often involving sensitive data and raising\nprivacy concerns. This paper addresses these challenges using local\ndifferential privacy (LDP), which enforces privacy at the individual level,\nwhere no third-party entity is trusted, unlike centralized models that assume a\ntrusted curator. We introduce novel LDP algorithms for two fundamental graph\nstatistics: k-core decomposition and triangle counting. Our approach leverages\ninput-dependent private graph properties, specifically the degeneracy and\nmaximum degree of the graph, to improve theoretical utility. Unlike prior\nmethods, our error bounds are determined by the maximum degree rather than the\ntotal number of edges, resulting in significantly tighter guarantees. For\ntriangle counting, we improve upon the work of Imola, Murakami, and\nChaudhury~\\cite{IMC21locally, IMC21communication}, which bounds error in terms\nof edge count. Instead, our algorithm achieves bounds based on graph degeneracy\nby leveraging a private out-degree orientation, a refined variant of Eden et\nal.'s randomized response technique~\\cite{ELRS23, and a novel analysis,\nyielding stronger guarantees than prior work. Beyond theoretical gains, we are\nthe first to evaluate local DP algorithms in a distributed simulation, unlike\nprior work tested on a single processor. Experiments on real-world graphs show\nsubstantial accuracy gains: our k-core decomposition achieves errors within 3x\nof exact values, far outperforming the 131x error in the baseline of Dhulipala\net al.~\\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative\napproximation errors by up to six orders of magnitude, while maintaining\ncompetitive runtime.", "comment": "To appear in VLDB 2025", "pdf_url": "http://arxiv.org/pdf/2506.20828v1", "categories": ["cs.DS", "cs.CR", "cs.DB"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20828v1", "AI": {"title_translation": "实用且准确的局部边缘差分隐私图算法", "tldr": "本论文提出了新的局部差分隐私(LDP)算法，用于k-核分解和三角形计数，通过利用图的退化度和最大度来显著提高准确性，并在分布式模拟中首次进行了评估。", "motivation": "大规模网络数据分析中存在敏感数据隐私泄露的担忧，需要一种在个体层面保障隐私的机制，即局部差分隐私（LDP），以应对现有中心化模型对可信第三方的假设。", "method": "本文提出了两种新的LDP算法，分别用于k-核分解和三角形计数。这些算法利用图的输入依赖私有属性，特别是退化度和最大度，以提高理论效用。与现有方法不同，其误差界限由最大度而非总边数决定。对于三角形计数，算法通过私有出度定向、改进的随机响应技术和新颖的分析，实现了基于图退化度的误差界限。此外，本研究首次在分布式模拟中评估了局部DP算法。", "result": "实验结果显示显著的准确性提升：k-核分解算法的误差在精确值的三倍以内，远优于基线方法的131倍误差。三角形计数算法将乘法近似误差降低了多达六个数量级，同时保持了有竞争力的运行时间。", "conclusion": "本文提出了实用且准确的局部差分隐私图算法，用于k-核分解和三角形计数，通过引入新的理论方法和在分布式环境中的首次评估，显著提高了准确性并解决了现有LDP图算法的局限性。", "translation": "大规模网络在不同领域的兴起，使得复杂的图分析变得必要，这往往涉及敏感数据并引发隐私担忧。本文使用局部差分隐私（LDP）来解决这些挑战，LDP在个体层面强制执行隐私，不信任任何第三方实体，这与假设存在可信策展者的中心化模型不同。我们引入了用于两种基本图统计量的新型LDP算法：k-核分解和三角形计数。我们的方法利用输入依赖的私有图属性，特别是图的退化度和最大度，以提高理论效用。与现有方法不同，我们的误差界限由最大度而非总边数决定，从而产生了显著更紧密的保证。对于三角形计数，我们改进了Imola、Murakami和Chaudhury~\\[IMC21locally, IMC21communication\\]的工作，他们的误差界限以边数表示。相反，我们的算法通过利用私有出度定向（Eden等人~\\[ELRS23\\]随机响应技术的改进变体）和新颖的分析，实现了基于图退化度的界限，从而获得了比现有工作更强的保证。除了理论收益，我们是第一个在分布式模拟中评估局部DP算法的研究，而以往的工作仅在单个处理器上进行测试。在真实世界图上的实验显示出显著的准确性提升：我们的k-核分解误差在精确值的3倍以内，远优于Dhulipala等人~\\[DLRSSY22\\]基线中131倍的误差。我们的三角形计数算法将乘法近似误差减少了多达六个数量级，同时保持了有竞争力的运行时间。", "summary": "本论文针对大规模网络数据分析中的隐私问题，提出了两种实用且准确的局部差分隐私（LDP）图算法，分别用于k-核分解和三角形计数。通过利用图的退化度和最大度等输入依赖私有属性，新算法的误差界限由最大度而非总边数决定，从而显著提高了理论效用。在三角形计数方面，算法通过私有出度定向和改进的随机响应技术，实现了基于图退化度的更强保证。此外，本文首次在分布式模拟中评估了LDP算法。实验结果表明，与现有基线相比，k-核分解的误差大幅降低，三角形计数算法的近似误差减少了多达六个数量级，同时保持了高效的运行时间。", "keywords": "局部差分隐私, 图算法, k-核分解, 三角形计数, 隐私保护", "comments": "本文的创新点在于其将图的退化度和最大度等输入依赖私有属性引入LDP算法设计，从而显著收紧了误差界限，并首次在分布式模拟中验证了LDP图算法的实用性。这对于在敏感数据上进行大规模图分析具有重要意义，克服了现有LDP方法在准确性和可扩展性方面的局限。"}}
{"id": "2506.20819", "title": "DPLib: A Standard Benchmark Library for Distributed Power System Analysis and Optimization", "authors": ["Milad Hasanzadeh", "Amin Kargarian"], "summary": "\\textit{DPLib} is an open-source MATLAB-based benchmark library created to\nsupport research and development in distributed and decentralized power system\nanalysis and optimization. Distributed and decentralized methods offer\nscalability, privacy preservation, and resilience to single points of failure,\nmaking them increasingly important for modern power systems. However, unlike\ncentralized tools such as MATPOWER, no general-purpose, reproducible data\nlibrary package currently exists for distributed power system studies. DPLib\nfills this gap by providing a standard power system library featuring over 20\nmulti-region benchmark test cases of varying sizes, along with a graph-based\npartitioning toolkit that decomposes any MATPOWER test system into multiple\nelectrically coherent regions. The partitioning toolkit, an easy-to-use MATLAB\ncode, generates standardized \\texttt{.mat} and \\texttt{.m} files, along with\nregion visualizations for intuitive understanding. We also provide modular,\neasy-to-use distributed optimal power flow (OPF) solvers: an alternating\ndirection method of multipliers(ADMM)-based DC-OPF solver implemented in\nYALMIP, and an ADMM-based AC-OPF solver leveraging IPOPT. These solvers\nvalidate the generated test systems for distributed optimization applications.\nNumerical results validate the generated test cases, establishing DPLib as a\nfoundation for reproducible distributed power system research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20819v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20819v1", "AI": {"title_translation": "DPLib：一个用于分布式电力系统分析与优化的标准基准库", "tldr": "DPLib是一个开源的MATLAB基准库，为分布式电力系统分析和优化提供标准测试用例和分区工具，填补了现有集中式工具的空白。", "motivation": "分布式和去中心化方法对于现代电力系统日益重要，因其提供了可扩展性、隐私保护和对单点故障的弹性。然而，与MATPOWER等集中式工具不同，目前尚无适用于分布式电力系统研究的通用、可复现的数据库包。", "method": "DPLib通过提供一个标准的电力系统库来填补这一空白，该库包含20多个不同规模的多区域基准测试用例，以及一个基于图的分区工具包，可将任何MATPOWER测试系统分解为多个电学上连贯的区域。此外，DPLib还提供了模块化、易于使用的分布式最优潮流(OPF)求解器：一个基于ADMM的DC-OPF求解器（在YALMIP中实现）和一个基于ADMM的AC-OPF求解器（利用IPOPT）。", "result": "数值结果验证了所生成的测试用例，并且所提供的求解器验证了生成的测试系统适用于分布式优化应用。", "conclusion": "DPLib为可复现的分布式电力系统研究奠定了基础。", "translation": "DPLib是一个开源的MATLAB基准库，旨在支持分布式和去中心化电力系统分析与优化的研究与开发。分布式和去中心化方法具有可扩展性、隐私保护和对单点故障的弹性，这使得它们对现代电力系统越来越重要。然而，与MATPOWER等集中式工具不同，目前尚无适用于分布式电力系统研究的通用、可复现的数据库包。DPLib通过提供一个标准的电力系统库来填补这一空白，该库包含20多个不同规模的多区域基准测试用例，以及一个基于图的分区工具包，可将任何MATPOWER测试系统分解为多个电学上连贯的区域。该分区工具包是一个易于使用的MATLAB代码，可生成标准化的.mat和.m文件，以及用于直观理解的区域可视化。我们还提供了模块化、易于使用的分布式最优潮流（OPF）求解器：一个基于交替方向乘子法（ADMM）的DC-OPF求解器（在YALMIP中实现），以及一个利用IPOPT的基于ADMM的AC-OPF求解器。这些求解器验证了生成的测试系统适用于分布式优化应用。数值结果验证了生成的测试用例，从而将DPLib确立为可复现分布式电力系统研究的基础。", "summary": "DPLib是一个开源的MATLAB基准库，旨在解决分布式电力系统研究中缺乏标准化、可复现数据包的问题。它提供了20多个多区域测试用例和一个将MATPOWER系统分解为连贯区域的分区工具包。此外，DPLib还集成了基于ADMM的DC-OPF和AC-OPF求解器，用于验证测试系统。DPLib的推出为可复现的分布式电力系统研究提供了坚实的基础。", "keywords": "DPLib, 分布式电力系统, 基准库, 最优潮流, ADMM", "comments": "DPLib通过提供一个标准化的、可复现的基准库和测试用例，填补了分布式电力系统研究领域的一个重要空白。其创新之处在于提供了易于使用的分区工具包和集成的分布式OPF求解器，极大地促进了分布式电力系统算法的开发和验证。这对于推动现代电力系统的可扩展性、隐私性和弹性研究具有重要意义。"}}
{"id": "2506.20823", "title": "Compact Analytical Model for Real-Time Evaluation of OAM-Based Inter-Satellite Links", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna"], "summary": "This paper presents an efficient analytical framework for evaluating the\nperformance of inter-satellite communication systems utilizing orbital angular\nmomentum (OAM) beams under pointing errors. An accurate analytical model is\nfirst developed to characterize intermodal crosstalk caused by beam\nmisalignment in OAM-based inter-satellite links. Building upon this model, we\nderive efficient expressions to analyze and optimize system performance in\nterms of bit error rate (BER). Unlike traditional Monte Carlo-based methods\nthat are computationally intensive, the proposed approach offers accurate\nperformance predictions. This enables a substantial decrease in computation\ntime while maintaining high accuracy, thanks to the use of analytical\nexpressions for both crosstalk and BER. This fast and accurate evaluation\ncapability is particularly critical for dynamic low Earth orbit (LEO) satellite\nconstellations, where network topology and channel conditions change rapidly,\nrequiring real-time link adaptation. Furthermore, we systematically design and\nevaluate asymmetric OAM mode sets, which significantly outperform symmetric\nconfigurations in the presence of pointing errors. Our results also reveal key\ninsights into the interaction between beam divergence, tracking accuracy, and\nlink distance, demonstrating that the proposed framework enables real-time\noptimization of system parameters with high fidelity. The analytical findings\nare rigorously validated against extensive Monte Carlo simulations, confirming\ntheir practical applicability for high-mobility optical wireless systems such\nas LEO satellite networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20823v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20823v1", "AI": {"title_translation": "用于实时评估基于OAM的星间链路的紧凑分析模型", "tldr": "本文提出了一个紧凑的分析模型，用于在指向误差下实时评估基于OAM的星间链路性能，比传统蒙特卡洛方法更快、更准确，并设计了性能更优的非对称OAM模式集。", "motivation": "传统蒙特卡洛方法在评估基于轨道角动量（OAM）的星间链路性能时计算量大，难以满足动态低地球轨道（LEO）卫星星座网络拓扑和信道条件快速变化对实时链路适应的需求。", "method": "本文首先开发了一个精确的分析模型来表征OAM星间链路中由光束未对准引起的模间串扰。在此模型基础上，推导了用于分析和优化误码率（BER）系统性能的有效表达式。此外，系统地设计和评估了非对称OAM模式集。", "result": "所提出的方法能够提供准确的性能预测，显著减少计算时间并保持高精度。非对称OAM模式集在存在指向误差的情况下显著优于对称配置。研究结果还揭示了光束发散、跟踪精度和链路距离之间的关键相互作用，表明该框架能够高保真地实时优化系统参数。", "conclusion": "分析结果通过广泛的蒙特卡洛模拟得到了严格验证，证实了其在LEO卫星网络等高移动性光无线系统中的实际适用性。", "translation": "本文提出了一种高效的分析框架，用于评估在指向误差下利用轨道角动量（OAM）光束的星间通信系统的性能。首先，开发了一个精确的分析模型来表征基于OAM的星间链路中由光束未对准引起的模间串扰。在此模型基础上，我们推导了分析和优化误码率（BER）系统性能的有效表达式。与计算密集型的传统蒙特卡洛方法不同，所提出的方法提供了准确的性能预测。这使得计算时间大幅减少，同时通过使用串扰和BER的分析表达式保持了高精度。这种快速准确的评估能力对于动态低地球轨道（LEO）卫星星座尤其关键，因为其网络拓扑和信道条件变化迅速，需要实时链路适应。此外，我们系统地设计和评估了非对称OAM模式集，它们在存在指向误差的情况下显著优于对称配置。我们的结果还揭示了光束发散、跟踪精度和链路距离之间相互作用的关键见解，表明所提出的框架能够高保真地实时优化系统参数。分析结果通过广泛的蒙特卡洛模拟得到了严格验证，证实了其在LEO卫星网络等高移动性光无线系统中的实际适用性。", "summary": "本文提出了一个紧凑的分析模型，用于在指向误差下实时评估基于OAM的星间链路性能。该模型通过表征模间串扰并推导误码率分析表达式，实现了比传统蒙特卡洛方法更高的计算效率和准确性。研究还系统地设计并验证了在指向误差下性能更优的非对称OAM模式集，并揭示了光束发散、跟踪精度和链路距离之间的相互作用。该框架对于需要实时链路适应的动态LEO卫星星座尤为重要，其分析结果已通过蒙特卡洛模拟验证。", "keywords": "OAM, 星间链路, 指向误差, 分析模型, 实时评估", "comments": "该论文的创新之处在于提出了一个紧凑且高效的分析模型，用于实时评估OAM星间链路在指向误差下的性能，显著优于计算量大的传统蒙特卡洛方法。其对非对称OAM模式集的设计和验证，为提高系统鲁棒性提供了新途径。这对于高动态的LEO卫星网络具有重要的实际应用价值。"}}
{"id": "2506.21174", "title": "Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4", "authors": ["Jongyeon Park", "Joonhee Lee", "Do-Hyeon Lim", "Hong Kook Kim", "Hyeongcheol Geum", "Jeong Eun Lim"], "summary": "This technical report presents submission systems for Task 4 of the DCASE\n2025 Challenge. This model incorporates additional audio features (spectral\nroll-off and chroma features) into the embedding feature extracted from the\nmel-spectral feature to im-prove the classification capabilities of an\naudio-tagging model in the spatial semantic segmentation of sound scenes (S5)\nsystem. This approach is motivated by the fact that mixed audio often contains\nsubtle cues that are difficult to capture with mel-spectrograms alone. Thus,\nthese additional features offer alterna-tive perspectives for the model.\nSecond, an agent-based label correction system is applied to the outputs\nprocessed by the S5 system. This system reduces false positives, improving the\nfinal class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.\nFinally, we refine the training dataset to enhance the classi-fication accuracy\nof low-performing classes by removing irrele-vant samples and incorporating\nexternal data. That is, audio mix-tures are generated from a limited number of\ndata points; thus, even a small number of out-of-class data points could\ndegrade model performance. The experiments demonstrate that the submit-ted\nsystems employing these approaches relatively improve CA-SDRi by up to 14.7%\ncompared to the baseline of DCASE 2025 Challenge Task 4.", "comment": "DCASE 2025 challenge Task4, 5 pages", "pdf_url": "http://arxiv.org/pdf/2506.21174v1", "categories": ["eess.AS", "cs.LG"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21174v1", "AI": {"title_translation": "DCASE 2025 挑战任务 4 中，通过丰富音频特征和基于代理的错误校正来改进空间语义分割的性能", "tldr": "本文提出DCASE 2025挑战任务4的提交系统，通过融合额外音频特征、应用基于代理的标签校正系统和优化训练数据集，显著提升了空间语义分割的性能。", "motivation": "混音音频中包含仅靠梅尔谱图难以捕捉的细微线索，因此需要额外的特征来提供替代视角。此外，需要减少假阳性并提升低性能类别的分类准确性。", "method": "1. 将额外的音频特征（谱滚降和色度特征）融入到从梅尔谱特征中提取的嵌入特征中，以改进空间语义分割 (S5) 系统中音频标注模型的分类能力。2. 应用基于代理的标签校正系统对S5系统处理的输出进行校正，以减少假阳性。3. 通过移除不相关样本和整合外部数据来优化训练数据集，以增强低性能类别的分类准确性。", "result": "实验表明，所提交的系统与DCASE 2025挑战任务4的基线相比，CA-SDRi相对提升高达14.7%。", "conclusion": "通过融合丰富的音频特征、应用代理纠错和优化数据集，可以显著提升空间语义分割的性能，并在DCASE 2025挑战中取得了优异表现。", "translation": "这份技术报告介绍了 DCASE 2025 挑战任务 4 的提交系统。该模型将额外的音频特征（谱滚降和色度特征）融入到从梅尔谱特征中提取的嵌入特征中，以提高声场景空间语义分割 (S5) 系统中音频标注模型的分类能力。这种方法的原因是，混音音频通常包含仅凭梅尔谱图难以捕捉的细微线索。因此，这些额外的特征为模型提供了替代视角。其次，将一个基于代理的标签校正系统应用于 S5 系统处理后的输出。该系统减少了假阳性，提高了最终的类别感知信噪比改善 (CA-SDRi) 指标。最后，我们通过移除不相关样本和整合外部数据来优化训练数据集，以增强低性能类别的分类准确性。也就是说，音频混合物是由有限的数据点生成的；因此，即使少量类外数据点也可能降低模型性能。实验表明，与 DCASE 2025 挑战任务 4 的基线相比，采用这些方法的提交系统使 CA-SDRi 相对提高了高达 14.7%。", "summary": "本文针对 DCASE 2025 挑战任务 4，提出了一种改进空间语义分割性能的系统。该系统通过整合谱滚降和色度等额外音频特征，增强了音频标注模型的分类能力，以捕捉梅尔谱图难以捕捉的微妙线索。此外，引入了基于代理的标签校正系统来减少假阳性，并优化了训练数据集以提升低性能类别的准确性。实验结果显示，该方法使CA-SDRi指标相对于基线提高了高达14.7%。", "keywords": "空间语义分割, 音频特征, 代理校正, DCASE 2025, CA-SDRi", "comments": "这篇论文通过多方面创新提升了空间语义分割的性能。它不仅通过引入补充音频特征（如谱滚降和色度）来丰富音频表示，还通过基于代理的错误校正系统有效降低了假阳性，同时通过数据精炼解决了低性能类别的问题。这种多策略结合的方法，特别是对音频特征的深度挖掘和后处理校正，为声场景分析领域提供了有益的参考，展现了其在复杂声学环境下的鲁棒性和有效性。"}}
{"id": "2506.20689", "title": "U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "summary": "Artificial intelligence, including deep learning models, will play a\ntransformative role in automated medical image analysis for the diagnosis of\ncardiac disorders and their management. Automated accurate delineation of\ncardiac images is the first necessary initial step for the quantification and\nautomated diagnosis of cardiac disorders. In this paper, we propose a deep\nlearning based enhanced UNet model, U-R-Veda, which integrates convolution\ntransformations, vision transformer, residual links, channel-attention, and\nspatial attention, together with edge-detection based skip-connections for an\naccurate fully-automated semantic segmentation of cardiac magnetic resonance\n(CMR) images. The model extracts local-features and their interrelationships\nusing a stack of combination convolution blocks, with embedded channel and\nspatial attention in the convolution block, and vision transformers. Deep\nembedding of channel and spatial attention in the convolution block identifies\nimportant features and their spatial localization. The combined edge\ninformation with channel and spatial attention as skip connection reduces\ninformation-loss during convolution transformations. The overall model\nsignificantly improves the semantic segmentation of CMR images necessary for\nimproved medical image analysis. An algorithm for the dual attention module\n(channel and spatial attention) has been presented. Performance results show\nthat U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The\nmodel outperforms the accuracy attained by other models, based on DSC and HD\nmetrics, especially for the delineation of right-ventricle and\nleft-ventricle-myocardium.", "comment": "15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.20689v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20689v1", "AI": {"title_translation": "U-R-VEDA：集成UNET、残差连接、边缘和双重注意力、以及视觉Transformer用于CMR的精确语义分割", "tldr": "U-R-VEDA是一个新的深度学习模型，结合了UNet、残差连接、边缘检测、双重注意力和视觉Transformer，显著提高了CMR图像的语义分割精度。", "motivation": "自动化医学图像分析对于心脏疾病的诊断和管理至关重要。精确的心脏图像描绘是量化和自动化诊断心脏疾病的必要第一步。", "method": "提出了一种名为U-R-VEDA的增强型UNet深度学习模型。该模型整合了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力以及基于边缘检测的跳跃连接。它通过组合卷积块（嵌入通道和空间注意力）和视觉Transformer提取局部特征及其相互关系。结合边缘信息与通道和空间注意力作为跳跃连接，减少了卷积变换过程中的信息损失。", "result": "U-R-VEDA在DSC指标上达到了平均95.2%的准确率。该模型在DSC和HD指标上优于其他模型，尤其在描绘右心室和左心室心肌方面表现突出。", "conclusion": "U-R-VEDA模型显著改善了CMR图像的语义分割，这对于改进医学图像分析至关重要。", "translation": "人工智能，包括深度学习模型，将在心脏疾病诊断和管理中的自动化医学图像分析中发挥变革性作用。心脏图像的自动化精确描绘是心脏疾病量化和自动化诊断的第一个必要初始步骤。在本文中，我们提出了一种基于深度学习的增强型UNet模型U-R-Veda，该模型集成了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力以及基于边缘检测的跳跃连接，用于心脏磁共振（CMR）图像的精确全自动语义分割。该模型通过一系列组合卷积块（其中嵌入了通道和空间注意力）和视觉Transformer提取局部特征及其相互关系。卷积块中通道和空间注意力的深度嵌入识别重要的特征及其空间定位。结合边缘信息与通道和空间注意力作为跳跃连接减少了卷积变换过程中的信息损失。整个模型显著改善了CMR图像的语义分割，这对于改进医学图像分析是必要的。本文还介绍了一种双重注意力模块（通道和空间注意力）的算法。性能结果显示，U-R-Veda在DSC指标上达到了平均95.2%的准确率。该模型在DSC和HD指标上优于其他模型所达到的准确率，尤其是在描绘右心室和左心室心肌方面。", "summary": "本文提出了一种名为U-R-VEDA的深度学习模型，旨在实现心脏磁共振（CMR）图像的精确全自动语义分割。U-R-VEDA是一个增强型UNet模型，创新性地结合了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力以及基于边缘检测的跳跃连接。通过嵌入双重注意力和利用边缘信息，模型有效提取和保留了特征，减少了信息损失。实验结果表明，U-R-VEDA在DSC指标上达到了95.2%的平均准确率，并优于其他模型，尤其在右心室和左心室心肌的描绘上表现出色，对自动化医学图像分析具有重要意义。", "keywords": "语义分割, CMR, UNet, 视觉Transformer, 注意力机制", "comments": "该论文提出了一种创新的UNet变体，通过集成视觉Transformer、多种注意力机制和边缘信息跳跃连接，有效提升了医学图像分割的性能。其亮点在于多组件的融合，解决了传统UNet在特征提取和信息损失方面的局限性。尤其在心脏MR图像分割上的高精度表现，预示着其在临床诊断辅助方面的巨大潜力。"}}
{"id": "2506.20901", "title": "Data Visualization for Improving Financial Literacy: A Systematic Review", "authors": ["Meng Du", "Robert Amor", "Kwan-Liu Ma", "Burkhard C. Wünsche"], "summary": "Financial literacy empowers individuals to make informed and effective\nfinancial decisions, improving their overall financial well-being and security.\nHowever, for many people understanding financial concepts can be daunting and\nonly half of US adults are considered financially literate. Data visualization\nsimplifies these concepts, making them accessible and engaging for learners of\nall ages. This systematic review analyzes 37 research papers exploring the use\nof data visualization and visual analytics in financial education and literacy\nenhancement. We classify these studies into five key areas: (1) the evolution\nof visualization use across time and space, (2) motivations for using\nvisualization tools, (3) the financial topics addressed and instructional\napproaches used, (4) the types of tools and technologies applied, and (5) how\nthe effectiveness of teaching interventions was evaluated. Furthermore, we\nidentify research gaps and highlight opportunities for advancing financial\nliteracy. Our findings offer practical insights for educators and professionals\nto effectively utilize or design visual tools for financial literacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20901v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20901v1", "AI": {"title_translation": "数据可视化对提高金融素养的作用：一项系统综述", "tldr": "金融素养对个人至关重要但理解困难，数据可视化能简化概念。本系统综述分析了37篇论文，分类了数据可视化在金融教育中的应用，并提出了研究空白和实践建议。", "motivation": "金融素养对个人财务福祉和安全至关重要，但许多人觉得金融概念难以理解，且只有一半的美国成年人具备金融素养。数据可视化可以简化这些概念，使其更易于理解和吸引人。", "method": "本文进行了一项系统综述，分析了37篇探讨数据可视化和视觉分析在金融教育和素养提升中应用的论文。研究将这些论文分为五个关键领域进行分类。", "result": "研究将文献分为五个关键领域：(1) 可视化使用在时间和空间上的演变，(2) 使用可视化工具的动机，(3) 所涉及的金融主题和教学方法，(4) 所应用的工具和技术类型，以及 (5) 如何评估教学干预的有效性。此外，研究还识别了研究空白并强调了推进金融素养的机会。", "conclusion": "本研究的发现为教育工作者和专业人士有效利用或设计可视化工具以提高金融素养提供了实用的见解。", "translation": "金融素养使个人能够做出明智有效的财务决策，从而提高他们的整体财务福祉和安全。然而，对许多人来说，理解金融概念可能令人生畏，而且只有一半的美国成年人被认为是具备金融素养的。数据可视化简化了这些概念，使其对所有年龄段的学习者来说都易于理解和引人入胜。本系统综述分析了37篇研究论文，探讨了数据可视化和视觉分析在金融教育和素养提升中的应用。我们将这些研究分为五个关键领域：(1) 可视化使用在时间和空间上的演变，(2) 使用可视化工具的动机，(3) 所涉及的金融主题和教学方法，(4) 所应用的工具和技术类型，以及 (5) 如何评估教学干预的有效性。此外，我们还识别了研究空白，并强调了推进金融素养的机会。我们的研究结果为教育工作者和专业人士有效利用或设计可视化工具以提高金融素养提供了实用的见解。", "summary": "本文通过对37篇研究论文的系统综述，探讨了数据可视化在金融教育和金融素养提升中的应用。研究将现有文献分类为可视化演变、使用动机、主题与教学方法、工具与技术类型以及效果评估五个方面。同时，该综述指出了研究空白，并为教育者和专业人士提供了利用可视化工具提高金融素养的实践指导。", "keywords": "数据可视化, 金融素养, 系统综述, 金融教育, 视觉分析", "comments": "这篇系统综述通过对现有文献的全面梳理和分类，为数据可视化在金融素养教育领域的应用提供了宝贵的结构化知识和实践指导，并指明了未来的研究方向。"}}
{"id": "2506.20854", "title": "Towards Two-Stage Counterfactual Learning to Rank", "authors": ["Shashank Gupta", "Yiming Liao", "Maarten de Rijke"], "summary": "Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.", "comment": "Accepted at ICTIR 2025 (co-located with SIGIR 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20854v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20854v1", "AI": {"title_translation": "迈向两阶段反事实学习排序", "tldr": "本文提出了一种两阶段反事实学习排序（CLTR）方法，旨在联合训练候选生成器和排序器，以解决现有单阶段CLTR在大规模数据集上的不实用性以及现有两阶段CLTR方法的局限性。", "motivation": "现有反事实学习排序（CLTR）方法多为单阶段，不适用于包含数百万文档的实际大规模排序系统。现有的针对两阶段系统的CLTR方法仅限于Top-1设置且只训练候选生成器而固定排序器，缺乏一种能联合训练排序器和候选生成器的CLTR方法。", "method": "本文提出了一种两阶段CLTR估计器，该估计器考虑了两个阶段之间的交互，并离线估计了两种策略的联合价值。此外，还提出了一种新颖的联合优化方法，分别训练候选生成器和排序器策略。", "result": "在半合成基准上的实验结果表明，所提出的联合CLTR方法优于基线方法。", "conclusion": "本文首次提出了针对两阶段排序系统的CLTR估计器和学习方法，实现了候选生成器和排序器策略的联合优化，并验证了其有效性。", "translation": "反事实学习排序（CLTR）旨在从用户交互中学习排序策略，同时纠正交互数据中固有的偏差，例如位置偏差。现有的CLTR方法假设单一排序策略从整个文档候选集中选择Top-K排序。在实际应用中，候选文档集数量级达到数百万，使得单一阶段的排序策略不切实际。为了扩展到数百万文档，实际的排序系统设计为两阶段模式，即候选生成器后接一个排序器。现有针对两阶段离线排序系统的CLTR方法只考虑了Top-1排序设置，并且只专注于训练候选生成器，而排序器是固定的。现有文献中缺少一种用于联合训练排序器和候选生成器的CLTR方法。在本文中，我们提出了一种两阶段CLTR估计器，它考虑了两个阶段之间的交互，并离线估计了两种策略的联合价值。此外，我们提出了一种新颖的联合优化方法，分别训练候选生成器和排序器策略。据我们所知，我们是第一个提出两阶段排序的CLTR估计器和学习方法。在半合成基准上的实验结果证明了所提出的联合CLTR方法优于基线方法。", "summary": "本文针对现有反事实学习排序（CLTR）方法在处理大规模文档集时的局限性，提出了一种新颖的两阶段CLTR框架。该框架包含一个考虑两阶段交互并离线估计联合价值的CLTR估计器，以及一种用于联合训练候选生成器和排序器的新型优化方法。实验结果在一个半合成基准上验证了所提联合CLTR方法的有效性，填补了现有文献中缺乏两阶段CLTR联合训练方法的空白。", "keywords": "反事实学习排序, 两阶段排序, 联合优化, 候选生成, 排序器", "comments": "该论文解决了CLTR研究中的一个关键空白，将其扩展到更符合实际应用的两阶段排序系统。其创新之处在于首次提出了针对两阶段排序的CLTR估计器和联合优化学习方法，这对于处理大规模文档集的真实世界系统具有重要意义。"}}
{"id": "2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "comment": "main paper is 14 pages", "pdf_url": "http://arxiv.org/pdf/2506.20803v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20803v1", "AI": {"title_translation": "构思-执行差距：LLM生成与人类研究想法的执行结果", "tldr": "本研究发现，尽管大型语言模型（LLM）生成的科研想法在构思阶段可能被认为更具新颖性，但在经过专家执行后，其效果显著低于人类生成的想法，揭示了LLM在生成真正有效研究想法方面的局限性。", "motivation": "大型语言模型（LLM）在加速科学研究流程方面展现出潜力，尤其是在生成新颖研究想法方面。然而，一个好的想法不仅要新颖，还应在执行后产生更好的研究成果。本研究旨在检验AI生成的想法是否能带来更好的研究成果，以弥补构思与执行之间的差距。", "method": "本研究招募了43位专家研究员，随机分配执行由专家撰写或由LLM生成的想法。每位专家投入超过100小时实施想法，并撰写一份4页的短论文记录实验。所有执行后的项目均由专家NLP研究员进行盲审。通过比较想法在执行前后的评估分数来分析结果。", "result": "与执行前相比，LLM生成的想法在执行后，其分数在所有评估指标（新颖性、兴奋度、有效性和总体；p < 0.05）上均显著下降，且降幅远大于专家撰写的想法，从而弥合了在构思阶段观察到的LLM与人类想法之间的差距。在比较执行研究的汇总评审分数时，甚至发现在许多指标上排名发生了逆转，人类想法得分高于LLM想法。", "conclusion": "本研究揭示的构思-执行差距突出了当前大型语言模型在生成真正有效的研究想法方面的局限性，以及在缺乏执行结果的情况下评估研究想法的挑战。", "translation": "大型语言模型（LLM）在加速科学研究流程方面展现出潜力。这一过程的一个关键能力是生成新颖的研究想法，并且先前的研究发现，在某些情况下，LLM生成的想法被判断为比人类专家想法更具新颖性。然而，一个好的想法不应仅仅看起来新颖，它还应在执行后产生更好的研究成果。为了测试AI生成的想法是否能带来更好的研究成果，我们进行了一项执行研究，招募了43位专家研究员来执行随机分配的想法，这些想法要么由专家撰写，要么由LLM生成。每位专家投入超过100小时实施想法，并撰写了一份4页的短论文来记录实验。所有执行后的项目都由专家NLP研究员进行盲审。比较想法在执行前后的评审分数，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性和总体；p < 0.05）上均显著下降，降幅远大于专家撰写的想法，从而弥合了在构思阶段观察到的LLM与人类想法之间的差距。在比较执行研究的汇总评审分数时，我们甚至观察到，在许多指标上排名发生了逆转，人类想法得分高于LLM想法。这种构思-执行差距突出了当前LLM在生成真正有效的研究想法方面的局限性，以及在缺乏执行结果的情况下评估研究想法的挑战。", "summary": "本研究旨在探究大型语言模型（LLM）生成的科研想法在实际执行后的效果。通过一项实验，43位专家研究员分别执行LLM或人类生成的想法，并由独立专家进行盲审。结果显示，尽管LLM想法在构思阶段可能显得新颖，但在执行后，其表现显著劣于人类想法，且分数下降幅度更大。这表明当前LLM在生成真正有效的研究想法方面存在局限性，并强调了在没有实际执行结果的情况下评估研究想法的挑战。", "keywords": "大型语言模型, 科研想法生成, 构思-执行差距, 研究评估, 人工智能局限性", "comments": "这项研究非常重要，因为它揭示了当前LLM在科研辅助方面的深层局限性。虽然LLM能够生成看似新颖的构思，但其有效性在实际执行中却大打折扣，这对于依赖LLM进行科研创新的领域是一个重要的警示。研究方法严谨，通过专家执行和盲审确保了结果的可靠性。其创新点在于从“构思”到“执行”的完整链条进行验证，而非仅仅停留在构思阶段的评价。"}}
{"id": "2506.21324", "title": "Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning", "authors": ["Jiechen Chen", "Bipin Rajendran", "Osvaldo Simeone"], "summary": "Neuromorphic and quantum computing have recently emerged as promising\nparadigms for advancing artificial intelligence, each offering complementary\nstrengths. Neuromorphic systems built on spiking neurons excel at processing\ntime-series data efficiently through sparse, event-driven computation,\nconsuming energy only upon input events. Quantum computing, on the other hand,\nleverages superposition and entanglement to explore feature spaces that are\nexponentially large in the number of qubits. Hybrid approaches combining these\nparadigms have begun to show potential, but existing quantum spiking models\nhave important limitations. Notably, prior quantum spiking neuron\nimplementations rely on classical memory mechanisms on single qubits, requiring\nrepeated measurements to estimate firing probabilities, and they use\nconventional backpropagation on classical simulators for training. Here we\npropose a stochastic quantum spiking (SQS) neuron model that addresses these\nchallenges. The SQS neuron uses multi-qubit quantum circuits to realize a\nspiking unit with internal quantum memory, enabling event-driven probabilistic\nspike generation in a single shot. Furthermore, we outline how networks of SQS\nneurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a\nhardware-friendly local learning rule, eliminating the need for global\nclassical backpropagation. The proposed SQSNN model fuses the time-series\nefficiency of neuromorphic computing with the exponentially large inner state\nspace of quantum computing, paving the way for quantum spiking neural networks\nthat are modular, scalable, and trainable on quantum hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21324v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21324v1", "AI": {"title_translation": "具有量子记忆和局部学习的随机量子脉冲神经网络", "tldr": "本文提出了一种随机量子脉冲（SQS）神经元模型及其网络（SQSNNs），通过利用多量子比特量子电路实现内部量子记忆和一次性概率脉冲生成，并采用硬件友好的局部学习规则进行训练，从而结合了神经形态计算的效率和量子计算的巨大状态空间，旨在实现模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络。", "motivation": "神经形态计算和量子计算是推动人工智能发展的有前景的范式，但现有量子脉冲模型存在局限性，例如依赖于单量子比特上的经典记忆机制（需要重复测量来估计放电概率）和使用经典模拟器上的传统反向传播进行训练。本文旨在解决这些挑战。", "method": "本文提出了一种随机量子脉冲（SQS）神经元模型。SQS神经元使用多量子比特量子电路实现具有内部量子记忆的脉冲单元，从而能够一次性实现事件驱动的概率脉冲生成。此外，SQS神经元网络（SQSNNs）可以通过硬件友好的局部学习规则进行训练，从而消除了对全局经典反向传播的需求。", "result": "所提出的SQSNN模型融合了神经形态计算的时间序列效率与量子计算的指数级大的内部状态空间，为模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络铺平了道路。", "conclusion": "本文提出了随机量子脉冲神经网络（SQSNN）模型，该模型结合了神经形态计算的效率和量子计算的强大能力，实现了模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络。", "translation": "神经形态计算和量子计算最近已成为推动人工智能发展的有前途的范式，各自提供互补的优势。基于脉冲神经元的神经形态系统通过稀疏、事件驱动的计算，在处理时间序列数据方面表现出色，仅在输入事件发生时才消耗能量。另一方面，量子计算利用叠加和纠缠来探索量子比特数量呈指数级增长的特征空间。结合这些范式的混合方法已开始显示出潜力，但现有的量子脉冲模型存在重要局限性。值得注意的是，先前的量子脉冲神经元实现依赖于单量子比特上的经典记忆机制，需要重复测量来估计放电概率，并且它们使用经典模拟器上的传统反向传播进行训练。本文提出了一种随机量子脉冲（SQS）神经元模型来解决这些挑战。SQS神经元使用多量子比特量子电路实现具有内部量子记忆的脉冲单元，从而能够一次性实现事件驱动的概率脉冲生成。此外，我们概述了如何通过硬件友好的局部学习规则训练SQS神经元网络——称为SQS神经网络（SQSNNs），从而消除了对全局经典反向传播的需求。所提出的SQSNN模型融合了神经形态计算的时间序列效率与量子计算的指数级大的内部状态空间，为模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络铺平了道路。", "summary": "本文提出了一种随机量子脉冲（SQS）神经元模型及其网络（SQSNNs），旨在克服现有量子脉冲模型在经典记忆依赖和传统反向传播方面的局限性。SQS神经元利用多量子比特量子电路实现内部量子记忆，从而能够一次性进行事件驱动的概率脉冲生成。SQSNNs采用硬件友好的局部学习规则进行训练，无需全局经典反向传播。这种方法结合了神经形态计算的效率和量子计算的巨大状态空间，为开发模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络奠定了基础。", "keywords": "随机量子脉冲神经网络, 量子记忆, 局部学习, 神经形态计算, 量子计算", "comments": "该论文提出了一种创新的混合方法，将量子计算原理与神经形态系统相结合。其主要创新包括使用多量子比特量子记忆实现脉冲神经元以及硬件友好的局部学习规则，这解决了现有量子脉冲模型的显著局限性。这可能是迈向实用、可扩展的量子神经网络的关键一步。"}}
{"id": "2506.21167", "title": "A Hierarchical Deep Learning Approach for Minority Instrument Detection", "authors": ["Dylan Sechet", "Francesca Bugiotti", "Matthieu Kowalski", "Edouard d'Hérouville", "Filip Langiewicz"], "summary": "Identifying instrument activities within audio excerpts is vital in music\ninformation retrieval, with significant implications for music cataloging and\ndiscovery. Prior deep learning endeavors in musical instrument recognition have\npredominantly emphasized instrument classes with ample data availability.\nRecent studies have demonstrated the applicability of hierarchical\nclassification in detecting instrument activities in orchestral music, even\nwith limited fine-grained annotations at the instrument level. Based on the\nHornbostel-Sachs classification, such a hierarchical classification system is\nevaluated using the MedleyDB dataset, renowned for its diversity and richness\nconcerning various instruments and music genres. This work presents various\nstrategies to integrate hierarchical structures into models and tests a new\nclass of models for hierarchical music prediction. This study showcases more\nreliable coarse-level instrument detection by bridging the gap between detailed\ninstrument identification and group-level recognition, paving the way for\nfurther advancements in this domain.", "comment": "International Conference on Digital Audio Effects (DAFx)", "pdf_url": "http://arxiv.org/pdf/2506.21167v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21167v1", "AI": {"title_translation": "一种用于少数乐器检测的层次深度学习方法", "tldr": "本文提出了一种层次深度学习方法，利用霍恩博斯特尔-萨克斯分类法，在MedleyDB数据集上评估并实现了更可靠的粗粒度乐器检测，尤其解决了少数乐器识别中的数据稀缺问题。", "motivation": "在音乐信息检索中，识别音频片段中的乐器活动至乐器编目和发现至关重要。之前的深度学习方法主要侧重于数据量充足的乐器类别，而忽略了少数乐器。最近的研究表明，即使在乐器层面缺乏细粒度标注的情况下，分层分类在管弦乐器活动检测中也适用。", "method": "本研究基于霍恩博斯特尔-萨克斯分类法，使用MedleyDB数据集评估了分层分类系统。文中提出了多种策略将分层结构整合到模型中，并测试了一类新的用于分层音乐预测的模型。", "result": "研究结果表明，通过弥合详细乐器识别和组级识别之间的差距，实现了更可靠的粗粒度乐器检测。", "conclusion": "这项工作为该领域未来的进一步发展铺平了道路，尤其是在处理数据稀缺的少数乐器检测方面。", "translation": "在音乐信息检索中，识别音频片段中的乐器活动至关重要，对音乐编目和发现具有重要意义。先前的音乐乐器识别深度学习研究主要强调数据可用性充足的乐器类别。最近的研究表明，即使乐器层面缺乏细粒度标注，分层分类在管弦乐器活动检测中也适用。基于霍恩博斯特尔-萨克斯分类法，使用以其多样性和丰富性闻名的MedleyDB数据集，对这种分层分类系统进行了评估。这项工作提出了将分层结构整合到模型中的各种策略，并测试了一类新的用于分层音乐预测的模型。本研究通过弥合详细乐器识别和组级识别之间的差距，展示了更可靠的粗粒度乐器检测，为该领域未来的进一步发展铺平了道路。", "summary": "本文提出了一种层次深度学习方法，旨在解决音乐信息检索中少数乐器检测的数据稀缺问题。研究基于霍恩博斯特尔-萨克斯分类法，利用MedleyDB数据集评估了分层分类系统，并开发了新的模型来整合层次结构。实验结果表明，该方法能够实现更可靠的粗粒度乐器检测，有效连接了详细乐器识别与组级识别，为未来该领域的发展奠定了基础。", "keywords": "层次深度学习, 乐器检测, 音乐信息检索, 霍恩博斯特尔-萨克斯分类, MedleyDB", "comments": "本文的创新之处在于提出了一种层次深度学习方法来解决音乐信息检索中少数乐器检测的数据稀缺问题。通过利用霍恩博斯特尔-萨克斯分类法，并将分层结构整合到模型中，有效地提高了粗粒度乐器检测的可靠性。这对于乐器编目和发现具有重要意义，尤其是在处理数据有限的乐器类别时。"}}
{"id": "2506.20890", "title": "Multicontinuum Homogenization for Poroelasticity Model", "authors": ["Dmitry Ammosov", "Mohammed Al-Kobaisi", "Yalchin Efendiev"], "summary": "In this paper, we derive multicontinuum poroelasticity models using the\nmulticontinuum homogenization method. Poroelasticity models are widely used in\nmany areas of science and engineering to describe coupled flow and mechanics\nprocesses in porous media. However, in many applications, the properties of\nporoelastic media possess high contrast, presenting serious computational\nchallenges. It is well known that standard homogenization approaches often fail\nto give an accurate solution due to the lack of macroscopic parameters.\nMulticontinuum approaches allow us to consider such cases by defining several\naverage states known as continua. In the field of poroelasticity,\nmultiple-network models arising from the multiple porous media theory are\nrepresentatives of these approaches. In this work, we extend previous findings\nby deriving the generalized multicontinuum poroelasticity model. We apply the\nrecently developed multicontinuum homogenization method and provide a rigorous\nderivation of multicontinuum equations. For this purpose, we formulate coupled\nconstraint cell problems in oversampled regions to consider different\nhomogenized effects. Then, we obtain a multicontinuum expansion of the\nfine-scale fields and derive the multicontinuum model supposing the smoothness\nof macroscopic variables. We present the most general version of equations and\nthe simplified ones based on our numerical experiments. Numerical results are\npresented for different heterogeneous media cases and demonstrate the high\naccuracy of our proposed multicontinuum models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20890v1", "categories": ["math.NA", "cs.CE", "cs.NA", "physics.comp-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20890v1", "AI": {"title_translation": "多连续介质均匀化孔隙弹性模型", "tldr": "本文利用多连续介质均匀化方法，推导了多连续介质孔隙弹性模型，旨在解决高对比度多孔介质的计算挑战，并展示了其高精度。", "motivation": "孔隙弹性模型在描述多孔介质中的耦合流动和力学过程方面广泛应用，但在许多应用中，多孔介质的性质具有高对比度，导致严重的计算挑战。标准均匀化方法因缺乏宏观参数而无法提供准确解决方案。多连续介质方法通过定义多个平均状态（即连续介质）来解决这一问题。", "method": "本研究通过应用最近开发的多连续介质均匀化方法，推导了广义多连续介质孔隙弹性模型。方法包括在过采样区域中建立耦合约束单元问题以考虑不同的均匀化效应，获得细尺度场的多连续介质展开，并假设宏观变量平滑性来推导多连续介质模型。文中还提供了方程的最通用版本和简化版本。", "result": "数值结果表明，针对不同的非均匀介质情况，所提出的多连续介质模型具有高精度。", "conclusion": "通过严格的均匀化方法推导的多连续介质孔隙弹性模型，能够准确描述高对比度多孔介质中的耦合流动和力学过程。", "translation": "在本文中，我们使用多连续介质均匀化方法推导了多连续介质孔隙弹性模型。孔隙弹性模型广泛应用于科学和工程的许多领域，以描述多孔介质中的耦合流动和力学过程。然而，在许多应用中，孔隙弹性介质的性质具有高对比度，这带来了严重的计算挑战。众所周知，由于缺乏宏观参数，标准均匀化方法通常无法给出准确的解决方案。多连续介质方法通过定义几个称为连续介质的平均状态，允许我们考虑这种情况。在孔隙弹性领域，源自多孔介质理论的多网络模型是这些方法的代表。在这项工作中，我们通过推导广义多连续介质孔隙弹性模型来扩展以前的发现。我们应用最近开发的多连续介质均匀化方法，并提供了多连续介质方程的严格推导。为此，我们在过采样区域中制定了耦合约束单元问题，以考虑不同的均匀化效应。然后，我们获得了细尺度场的多连续介质展开，并假设宏观变量的平滑性推导了多连续介质模型。我们根据数值实验给出了方程的最通用版本和简化版本。对不同的非均匀介质情况进行了数值结果，并证明了我们提出的多连续介质模型具有高精度。", "summary": "本文利用严格的多连续介质均匀化方法，开发了广义多连续介质孔隙弹性模型。该模型旨在解决高对比度多孔介质所带来的计算难题，弥补了标准均匀化方法的不足。研究方法包括构建耦合约束单元问题并基于细尺度场展开推导多连续介质方程。数值实验验证了所提出模型在各种非均匀介质中的高精度表现。", "keywords": "多连续介质, 孔隙弹性, 均匀化, 多孔介质, 高对比度", "comments": "该论文解决了高对比度多孔介质建模中的一个重要挑战，这对于许多工程和科学应用至关重要。利用多连续介质均匀化方法是克服标准方法局限性的一种创新途径，为复杂系统提供了更准确的描述。严格的推导和数值验证增强了所提出模型的可靠性。"}}
{"id": "2506.20981", "title": "PrivacyGo: Privacy-Preserving Ad Measurement with Multidimensional Intersection", "authors": ["Jian Du", "Haohao Qian", "Shikun Zhang", "Wen-jie Lu", "Donghang Lu", "Yongchuan Niu", "Bo Jiang", "Yongjun Zhao", "Qiang Yan"], "summary": "This paper tackles the challenging and practical problem of multi-identifier\nprivate user profile matching for privacy-preserving ad measurement, a\ncornerstone of modern advertising analytics. We introduce a comprehensive\ncryptographic framework leveraging reversed Oblivious Pseudorandom Functions\n(OPRF) and novel blind key rotation techniques to support secure matching\nacross multiple identifiers. Our design prevents cross-identifier linkages and\nincludes a differentially private mechanism to obfuscate intersection sizes,\nmitigating risks such as membership inference attacks.\n  We present a concrete construction of our protocol that achieves both strong\nprivacy guarantees and high efficiency. It scales to large datasets, offering a\npractical and scalable solution for privacy-centric applications like secure ad\nconversion tracking. By combining rigorous cryptographic principles with\ndifferential privacy, our work addresses a critical need in the advertising\nindustry, setting a new standard for privacy-preserving ad measurement\nframeworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20981v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20981v1", "AI": {"title_translation": "PrivacyGo：基于多维交集分析的隐私保护广告衡量", "tldr": "本文提出PrivacyGo，一个使用反向不经意伪随机函数和盲密钥轮换技术，实现多标识符隐私保护广告衡量的加密框架。", "motivation": "解决现代广告分析中多标识符私人用户画像匹配的挑战性问题，即隐私保护广告衡量。", "method": "引入一个综合的加密框架，利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术，支持跨多个标识符的安全匹配。设计上防止跨标识符链接，并包含一个差分隐私机制来混淆交集大小，以减轻成员推断攻击等风险。", "result": "实现了强大的隐私保证和高效率，能够扩展到大型数据集，为安全广告转化跟踪等以隐私为中心的应用提供实用且可扩展的解决方案。", "conclusion": "通过结合严格的密码学原理和差分隐私，解决了广告行业的一个关键需求，为隐私保护广告衡量框架设定了新标准。", "translation": "本文解决了多标识符隐私用户画像匹配这一具有挑战性且实用的问题，该问题是现代广告分析的基石。我们引入了一个综合的加密框架，利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术，以支持跨多个标识符的安全匹配。我们的设计阻止了跨标识符链接，并包含一个差分隐私机制来混淆交集大小，从而减轻了诸如成员推断攻击等风险。\n我们提出了一个具体的协议构造，该构造实现了强大的隐私保证和高效率。它能够扩展到大型数据集，为诸如安全广告转化跟踪等以隐私为中心的应用提供了一个实用且可扩展的解决方案。通过将严格的密码学原理与差分隐私相结合，我们的工作解决了广告行业的一个关键需求，为隐私保护广告衡量框架设定了新标准。", "summary": "PrivacyGo提出一个创新的加密框架，利用反向不经意伪随机函数和盲密钥轮换技术，实现隐私保护的多标识符用户画像匹配和广告衡量。该框架通过差分隐私机制防止跨标识符链接和成员推断攻击，同时保证了高效率和可扩展性，为广告行业提供了一个实用的隐私保护解决方案。", "keywords": "隐私保护, 广告衡量, 多标识符匹配, 不经意伪随机函数, 差分隐私", "comments": "本文的创新之处在于结合了反向不经意伪随机函数、盲密钥轮换技术和差分隐私，以解决多标识符广告衡量中的隐私挑战。其重要性在于为广告行业提供了可扩展且高效率的隐私保护方案，为该领域设定了新标准，有效缓解了用户隐私泄露的风险。"}}
{"id": "2506.20883", "title": "Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance", "authors": ["Kyanna Dagenais", "Istvan David"], "summary": "Model-driven engineering problems often require complex model transformations\n(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of\nsuch problems include model synchronization, automated model repair, and design\nspace exploration. Manually developing complex MTs is an error-prone and often\ninfeasible process. Reinforcement learning (RL) is an apt way to alleviate\nthese issues. In RL, an autonomous agent explores the state space through trial\nand error to identify beneficial sequences of actions, such as MTs. However, RL\nmethods exhibit performance issues in complex problems. In these situations,\nhuman guidance can be of high utility. In this paper, we present an approach\nand technical framework for developing complex MT sequences through RL, guided\nby potentially uncertain human advice. Our framework allows user-defined MTs to\nbe mapped onto RL primitives, and executes them as RL programs to find optimal\nMT sequences. Our evaluation shows that human guidance, even if uncertain,\nsubstantially improves RL performance, and results in more efficient\ndevelopment of complex MTs. Through a trade-off between the certainty and\ntimeliness of human advice, our method takes a step towards RL-driven\nhuman-in-the-loop engineering methods.", "comment": "Accepted for ACM/IEEE MODELS'25", "pdf_url": "http://arxiv.org/pdf/2506.20883v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20883v1", "AI": {"title_translation": "不确定人类指导下通过强化学习进行的复杂模型转换", "tldr": "本文提出了一种结合强化学习和不确定人类指导的方法，用于开发复杂的模型转换序列，并证明了人类指导能显著提高RL性能。", "motivation": "模型驱动工程中的复杂模型转换（MTs）手动开发容易出错且不可行。强化学习（RL）可以缓解这些问题，但在复杂问题中存在性能问题。因此，需要一种结合人类指导的方法来提高RL在复杂MTs开发中的表现。", "method": "本文提出了一种方法和技术框架，通过强化学习并结合可能不确定的人类建议来开发复杂的模型转换序列。该框架允许将用户定义的MTs映射到RL原语，并将其作为RL程序执行以找到最佳MT序列。", "result": "评估结果表明，即使是不确定的人类指导也能显著提高强化学习的性能，并使得复杂模型转换的开发更加高效。", "conclusion": "通过权衡人类建议的确定性和及时性，该方法向着强化学习驱动的人机协作工程方法迈进了一步。", "translation": "模型驱动工程问题通常需要复杂的模型转换（MTs），即以广泛序列链接的MTs。此类问题的相关示例包括模型同步、自动化模型修复和设计空间探索。手动开发复杂的MTs是一个容易出错且通常不可行的过程。强化学习（RL）是缓解这些问题的合适方法。在RL中，自主代理通过试错探索状态空间，以识别有益的动作序列，例如MTs。然而，RL方法在复杂问题中表现出性能问题。在这些情况下，人类指导具有很高的效用。在本文中，我们提出了一种通过强化学习开发复杂MT序列的方法和技术框架，该框架由可能不确定的人类建议指导。我们的框架允许将用户定义的MTs映射到RL原语，并将其作为RL程序执行以找到最佳MT序列。我们的评估表明，人类指导，即使是不确定的，也能显著提高RL性能，并导致更高效的复杂MT开发。通过权衡人类建议的确定性和及时性，我们的方法向着RL驱动的人机协作工程方法迈进了一步。", "summary": "本文针对模型驱动工程中复杂模型转换（MTs）手动开发困难的问题，提出了一种结合强化学习（RL）与不确定人类指导的方法。该方法通过将用户定义的MTs映射到RL原语并执行，旨在找到最优的MT序列。实验结果表明，即使是不确定的人类指导也能显著提升RL的性能，从而更高效地开发复杂的MTs，为RL驱动的人机协作工程提供了新的方向。", "keywords": "模型转换, 强化学习, 人类指导, 模型驱动工程, 人机协作", "comments": "该论文的创新点在于将强化学习应用于复杂的模型转换问题，并引入了“不确定人类指导”的概念来克服RL在复杂问题上的性能瓶颈。这一方法通过有效整合人类领域知识和RL的探索能力，提高了模型转换的效率和准确性，对于推进模型驱动工程的自动化和人机协作具有重要意义。未来研究可以进一步探索如何量化和优化不确定性指导的效用。"}}
{"id": "2506.21201", "title": "Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices", "authors": ["Zihao You", "Michael Crabb"], "summary": "The consumption of subtitles via TVs, laptops and smartphones has the\npotential to marginalize people based on their complex accessibility needs. The\ncurrent one-size-fits-all approach to this accessibility aid is no longer fit\nfor purpose and work is required to look at how it can be adapted to be\npersonalised for individual users based on individual context, content, and\nconsumption habits. People with Aphasia, for example, encounter significant\nchallenges in understanding subtitle texts.\n  We see our work as a call to action for more inclusive practices, focusing on\nhow the thoughts and opinions of people with aphasia can be included in media\nresearch. Our work investigates how to develop future media solutions for\npeople with aphasia to create a more inclusive media viewing environment. We\nbelieve the key to this is appropriate prototyping tools and methods to allow\nequitable inclusion in the system design process.", "comment": "3 pages, 1 figure, Access InContext Workshop at CHI 2025 on 26th of\n  April", "pdf_url": "http://arxiv.org/pdf/2506.21201v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21201v1", "AI": {"title_translation": "失语症患者的字幕媒体改编：持续存在的无障碍障碍和新兴设计实践", "tldr": "针对失语症患者，当前字幕的“一刀切”方法存在可访问性障碍；本研究呼吁通过原型工具和方法，开发个性化且更具包容性的媒体解决方案，以纳入失语症患者的意见。", "motivation": "当前字幕的“一刀切”方法无法满足具有复杂无障碍需求的人群（尤其是失语症患者）的需求，导致他们在理解字幕文本时面临显著挑战，进而可能被边缘化。因此，需要开发个性化和更具包容性的媒体解决方案。", "method": "本文旨在调查如何为失语症患者开发未来的媒体解决方案，以创建更具包容性的媒体观看环境。作者认为关键在于使用适当的原型工具和方法，以确保在系统设计过程中公平地纳入失语症患者的意见。", "result": "Not mentioned in abstract", "conclusion": "本文呼吁采取更具包容性的实践，强调在媒体研究中纳入失语症患者的观点和意见，并通过原型工具和方法开发个性化媒体解决方案，以实现更具包容性的媒体观看环境。", "translation": "电视、笔记本电脑和智能手机上的字幕消费有可能因其复杂的无障碍需求而使人们被边缘化。当前这种“一刀切”的无障碍辅助方法已不再适用，需要研究如何根据个人语境、内容和消费习惯进行个性化调整。例如，失语症患者在理解字幕文本方面遇到了重大挑战。我们将我们的工作视为一项行动呼吁，旨在推行更具包容性的实践，重点关注如何将失语症患者的想法和意见纳入媒体研究。我们的工作旨在调查如何为失语症患者开发未来的媒体解决方案，以创建更具包容性的媒体观看环境。我们相信，实现这一目标的关键在于适当的原型工具和方法，以实现在系统设计过程中的公平包容。", "summary": "本文指出当前字幕的“一刀切”方法无法满足失语症患者等具有复杂无障碍需求人群的需求，导致他们难以理解字幕。作者呼吁采取更具包容性的实践，通过研究和开发个性化的媒体解决方案，并利用适当的原型工具和方法，将失语症患者的观点和意见纳入系统设计过程，从而为他们创造一个更具包容性的媒体观看环境。", "keywords": "失语症, 字幕, 无障碍, 个性化, 包容性设计", "comments": "这篇论文强调了一个重要的社会问题：数字媒体辅助工具（如字幕）在设计上可能无意中排除特定用户群体，特别是失语症患者。其创新之处在于提出“个性化”和“包容性设计”作为解决当前“一刀切”方法的关键，并特别强调在设计过程中纳入受影响人群（失语症患者）的意见。这种以用户为中心、强调公平包容的设计理念具有重要的实践意义，有助于推动更公平的数字无障碍环境。"}}
{"id": "2506.21537", "title": "ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers", "authors": ["Nicholas S. DiBrita", "Jason Han", "Tirthak Patel"], "summary": "Research in quantum machine learning has recently proliferated due to the\npotential of quantum computing to accelerate machine learning. An area of\nmachine learning that has not yet been explored is neural ordinary differential\nequation (neural ODE) based residual neural networks (ResNets), which aim to\nimprove the effectiveness of neural networks using the principles of ordinary\ndifferential equations. In this work, we present our insights about why analog\nRydberg atom quantum computers are especially well-suited for ResNets. We also\nintroduce ResQ, a novel framework to optimize the dynamics of Rydberg atom\nquantum computers to solve classification problems in machine learning using\nanalog quantum neural ODEs.", "comment": "ResQ will appear in the Proceedings of the IEEE International\n  Conference on Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2506.21537v1", "categories": ["quant-ph", "cs.CV", "cs.ET"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21537v1", "AI": {"title_translation": "ResQ：一种在模拟里德堡原子量子计算机上实现残差神经网络的新颖框架", "tldr": "提出ResQ框架，在模拟里德堡原子量子计算机上利用量子神经ODE实现残差神经网络，解决机器学习分类问题。", "motivation": "量子计算在加速机器学习方面的潜力巨大，但基于神经常微分方程（neural ODE）的残差神经网络（ResNets）尚未在量子计算机上探索。本研究旨在填补这一空白，并利用模拟里德堡原子量子计算机的优势实现ResNets。", "method": "作者提出了ResQ，一个新颖的框架，用于优化里德堡原子量子计算机的动力学，以利用模拟量子神经ODE解决机器学习中的分类问题。他们还阐述了模拟里德堡原子量子计算机特别适合ResNets的原因。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "量子机器学习研究最近因量子计算加速机器学习的潜力而迅速发展。机器学习中一个尚未探索的领域是基于神经常微分方程（neural ODE）的残差神经网络（ResNets），其旨在利用常微分方程的原理提高神经网络的有效性。在这项工作中，我们提出了关于模拟里德堡原子量子计算机为何特别适合ResNets的见解。我们还介绍了ResQ，一个新颖的框架，用于优化里德堡原子量子计算机的动力学，以利用模拟量子神经ODE解决机器学习中的分类问题。", "summary": "本文介绍了ResQ，一个新颖的框架，旨在将基于神经常微分方程的残差神经网络（ResNets）应用于模拟里德堡原子量子计算机。研究指出量子计算在机器学习中的潜力，并阐述了模拟里德堡原子量子计算机为何特别适合ResNets。ResQ框架通过优化里德堡原子量子计算机的动力学，利用模拟量子神经ODE来解决机器学习中的分类问题。", "keywords": "量子机器学习, 残差神经网络, 里德堡原子量子计算机, 神经常微分方程, 量子神经ODE", "comments": "这篇论文的创新点在于将残差神经网络（ResNets）这一机器学习模型与模拟里德堡原子量子计算机相结合，特别是引入了量子神经ODE的概念。它探索了量子计算在加速机器学习，特别是复杂网络结构方面的潜力，为量子机器学习领域开辟了新的研究方向。"}}
{"id": "2506.20966", "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends", "authors": ["Tian-Yu Xiang", "Ao-Qun Jin", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Sheng-Bin Duan", "Fu-Chao Xie", "Wen-Kai Wang", "Si-Cheng Wang", "Ling-Yun Li", "Tian Tu", "Zeng-Guang Hou"], "summary": "Vision-language-action (VLA) models extend vision-language models (VLM) by\nintegrating action generation modules for robotic manipulation. Leveraging\nstrengths of VLM in vision perception and instruction understanding, VLA models\nexhibit promising generalization across diverse manipulation tasks. However,\napplications demanding high precision and accuracy reveal performance gaps\nwithout further adaptation. Evidence from multiple domains highlights the\ncritical role of post-training to align foundational models with downstream\napplications, spurring extensive research on post-training VLA models. VLA\nmodel post-training aims to address the challenge of improving an embodiment's\nability to interact with the environment for the given tasks, analogous to the\nprocess of humans motor skills acquisition. Accordingly, this paper reviews\npost-training strategies for VLA models through the lens of human motor\nlearning, focusing on three dimensions: environments, embodiments, and tasks. A\nstructured taxonomy is introduced aligned with human learning mechanisms: (1)\nenhancing environmental perception, (2) improving embodiment awareness, (3)\ndeepening task comprehension, and (4) multi-component integration. Finally, key\nchallenges and trends in post-training VLA models are identified, establishing\na conceptual framework to guide future research. This work delivers both a\ncomprehensive overview of current VLA model post-training methods from a human\nmotor learning perspective and practical insights for VLA model development.\n(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20966v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20966v1", "AI": {"title_translation": "VLA模型后训练与人类运动学习的并行性：进展、挑战与趋势", "tldr": "本文从人类运动学习的角度回顾了VLA模型的后训练策略，并提出了一个结构化分类法，以应对VLA模型在机器人操作中精度和准确性方面的挑战。", "motivation": "VLA模型在机器人操作中展现出良好的泛化能力，但在需要高精度和准确性的应用中存在性能差距。后训练对于弥合基础模型与下游应用之间的差距至关重要，因此需要深入研究VLA模型的后训练方法。", "method": "本文从人类运动学习的角度，通过环境、具身和任务三个维度，回顾了VLA模型的后训练策略。引入了一个与人类学习机制对齐的结构化分类法，包括：(1) 增强环境感知，(2) 提高具身意识，(3) 加深任务理解，以及 (4) 多组件集成。", "result": "本文识别了VLA模型后训练中的关键挑战和趋势，并建立了一个概念框架来指导未来的研究。", "conclusion": "本文从人类运动学习的角度全面概述了当前的VLA模型后训练方法，并为VLA模型开发提供了实用的见解。", "translation": "视觉-语言-动作（VLA）模型通过集成动作生成模块，将视觉-语言模型（VLM）扩展到机器人操作领域。VLA模型利用VLM在视觉感知和指令理解方面的优势，在各种操作任务中展现出良好的泛化能力。然而，需要高精度和准确性的应用揭示了在不进行进一步适应的情况下存在的性能差距。来自多个领域的证据强调了后训练在使基础模型与下游应用对齐方面的关键作用，从而激发了对VLA模型后训练的广泛研究。VLA模型后训练旨在解决提高具身与环境交互能力以完成给定任务的挑战，这类似于人类运动技能习得的过程。因此，本文从人类运动学习的角度回顾了VLA模型的后训练策略，重点关注三个维度：环境、具身和任务。引入了一个与人类学习机制对齐的结构化分类法：(1) 增强环境感知，(2) 提高具身意识，(3) 加深任务理解，以及 (4) 多组件集成。最后，识别了VLA模型后训练中的关键挑战和趋势，建立了一个概念框架来指导未来的研究。这项工作提供了从人类运动学习角度对当前VLA模型后训练方法的全面概述，以及对VLA模型开发的实用见解。", "summary": "本文从人类运动学习的角度，对视觉-语言-动作（VLA）模型的后训练策略进行了全面综述。针对VLA模型在机器人操作中高精度和准确性应用中存在的性能差距，文章强调了后训练的重要性。作者提出了一个结构化的分类法，将VLA模型的后训练方法分为增强环境感知、提高具身意识、加深任务理解和多组件集成四个方面，并探讨了该领域的挑战和未来趋势，旨在为VLA模型的发展提供指导和实用见解。", "keywords": "VLA模型, 后训练, 人类运动学习, 机器人操作, 综述", "comments": "该论文的创新之处在于其独特的视角，将VLA模型的后训练与人类运动学习过程进行类比，这为理解和改进机器人学习提供了一个新颖且直观的框架。通过借鉴人类学习机制，论文不仅对现有方法进行了系统分类，还为未来的研究指明了方向，具有重要的理论和实践意义。"}}
{"id": "2506.20992", "title": "Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty", "authors": ["Craig Steven Wright"], "summary": "This paper develops a formal game-theoretic model to examine how protocol\nmutability disrupts cooperative mining behaviour in blockchain systems. Using a\nrepeated game framework with stochastic rule shocks, we show that even minor\nuncertainty in institutional rules increases time preference and induces\nstrategic deviation. Fixed-rule environments support long-term investment and\nstable equilibrium strategies; in contrast, mutable protocols lead to\nshort-termism, higher discounting, and collapse of coordinated engagement.\nSimulation results identify instability zones in the parameter space where\nrational mining gives way to extractive or arbitrage conduct. These findings\nsupport an Austrian economic interpretation: calculability requires rule\nstability. Institutional noise undermines the informational basis for\nproductive action. We conclude that protocol design must be treated as a\nconstitutional economic constraint, not a discretionary variable, if\nsustainable cooperation is to emerge in decentralised systems.", "comment": "40 pages, submitted to QJAE", "pdf_url": "http://arxiv.org/pdf/2506.20992v1", "categories": ["econ.GN", "cs.CE", "cs.CY", "cs.GT", "cs.SI", "q-fin.EC", "91A05, 91B42, 68M14, 91B62", "J.4; C.2.4; K.4.1; F.1.1"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.20992v1", "AI": {"title_translation": "制度噪音、策略偏差与跨期崩溃：协议不确定性下矿工行为的形式模型", "tldr": "论文通过博弈论模型分析区块链协议可变性如何破坏合作挖矿行为，指出规则不确定性导致短期主义和合作崩溃，强调协议设计需稳定。", "motivation": "探讨协议可变性如何扰乱区块链系统中的合作挖矿行为。", "method": "采用形式化的博弈论模型，使用带有随机规则冲击的重复博弈框架。", "result": "即使是微小的制度规则不确定性也会增加时间偏好并导致策略偏差。固定规则环境支持长期投资和稳定均衡策略，而可变协议导致短期主义、高贴现和协调参与的崩溃。模拟结果识别出参数空间中的不稳定区域，其中理性挖矿让位于掠夺性或套利行为。", "conclusion": "协议设计必须被视为一种宪法经济约束，而不是一个可自由裁量的变量，这样才能在去中心化系统中实现可持续合作。", "translation": "本文开发了一个形式化的博弈论模型，以检验协议可变性如何扰乱区块链系统中的合作挖矿行为。通过使用一个带有随机规则冲击的重复博弈框架，我们表明即使制度规则中存在微小的不确定性也会增加时间偏好并诱导策略偏差。固定规则环境支持长期投资和稳定的均衡策略；相比之下，可变协议导致短期主义、更高的贴现率和协调参与的崩溃。模拟结果确定了参数空间中的不稳定区域，其中理性的挖矿行为让位于掠夺性或套利行为。这些发现支持奥地利经济学的解释：可计算性需要规则稳定性。制度噪音破坏了生产性行动的信息基础。我们得出结论，如果要在去中心化系统中实现可持续合作，协议设计必须被视为一种宪法经济约束，而不是一个可自由裁量的变量。", "summary": "本文构建了一个形式化的博弈论模型，旨在分析区块链协议的可变性对合作挖矿行为的影响。研究发现，即使是轻微的规则不确定性也会导致矿工的时间偏好增加和策略偏离，进而引发短期主义和协作瓦解。相反，稳定的规则环境则有利于长期投资和均衡策略。模拟结果揭示了理性挖矿行为转向掠夺或套利的“不稳定区域”。论文强调，协议设计应被视为一种宪法经济学约束，而非可变因素，以确保去中心化系统的可持续合作。", "keywords": "区块链, 合作挖矿, 协议不确定性, 博弈论, 制度噪音", "comments": "这篇论文通过引入博弈论模型，为理解区块链协议设计中的“制度噪音”如何影响矿工行为提供了严谨的理论框架。其创新之处在于将随机规则冲击引入重复博弈，揭示了协议不确定性对长期合作和投资的负面影响，并从奥地利经济学角度强调了规则稳定性的重要性。这对于去中心化系统（如区块链）的协议设计具有重要的实践指导意义，提示设计者需重视规则的不可变性以促进可持续发展。"}}
{"id": "2506.20745", "title": "Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates", "authors": ["C. Betegón", "C. Rodríguez", "E. Martínez-Pañeda", "R. M. McMeeking"], "summary": "The exceptional adhesion properties of biological fibrillar structures --\nsuch as those found in geckos -- have inspired the development of synthetic\nadhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated\nsuperior pull-off strength compared to other geometries. In this study, we\nemploy a computational approach based on a Dugdale cohesive zone model to\nanalyze the detachment behavior of these fibrils when adhered to a rigid\nsubstrate. The results provide complete pull-off curves, revealing that the\nseparation process is inherently unstable under load control, regardless of\nwhether detachment initiates at the fibril edge or center. Our findings show\nthat fibrils with a wide, thin mushroom cap effectively reduce stress\nconcentrations and promote central detachment, leading to enhanced adhesion.\nHowever, detachment from the center is not observed in all geometries, whereas\nedge detachment can occur under certain conditions in all cases. Additionally,\nwe investigate the impact of adhesion defects at the fibril center, showing\nthat they can significantly reduce pull-off strength, particularly at high\nvalues of the dimensionless parameter \\c{hi}. These insights contribute to the\noptimization of bio-inspired adhesives and microstructured surfaces for various\nengineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20745v1", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cs.CE", "physics.app-ph", "physics.bio-ph"], "cate": "cond-mat.soft", "url": "http://arxiv.org/abs/2506.20745v1", "AI": {"title_translation": "蘑菇状纤维粘附于刚性基底的剥离强度", "tldr": "研究利用计算方法分析蘑菇状纤维的剥离行为，发现分离过程在载荷控制下不稳定，宽薄的蘑菇帽能增强粘附，中心缺陷会显著降低剥离强度。", "motivation": "生物纤维结构（如壁虎）卓越的粘附性能启发了合成粘附表面的开发，其中蘑菇状纤维显示出优越的剥离强度，因此需要深入理解其脱离行为以优化设计。", "method": "本研究采用基于Dugdale内聚区模型的计算方法，分析了蘑菇状纤维粘附于刚性基底时的脱离行为。", "result": "1. 提供了完整的剥离曲线，显示分离过程在载荷控制下本质上是不稳定的，无论脱离是从纤维边缘还是中心开始。\n2. 宽而薄的蘑菇帽能有效减少应力集中并促进中心脱离，从而增强粘附。\n3. 并非所有几何形状都观察到中心脱离，但在所有情况下，边缘脱离在特定条件下都可能发生。\n4. 纤维中心的粘附缺陷会显著降低剥离强度，特别是在无量纲参数\\c{hi}较高时。", "conclusion": "这些研究结果有助于优化仿生粘合剂和微结构表面，应用于各种工程领域。", "translation": "生物纤维结构（如壁虎身上的）卓越的粘附性能启发了合成粘附表面的开发。其中，蘑菇状纤维与其他几何形状相比，表现出优越的剥离强度。在这项研究中，我们采用基于Dugdale内聚区模型的计算方法，分析了这些纤维粘附到刚性基底时的脱离行为。结果提供了完整的剥离曲线，揭示了分离过程在载荷控制下本质上是不稳定的，无论脱离是从纤维边缘还是中心开始。我们的发现表明，具有宽而薄的蘑菇帽的纤维能有效减少应力集中并促进中心脱离，从而增强粘附。然而，并非所有几何形状都观察到中心脱离，而在所有情况下，边缘脱离在特定条件下都可能发生。此外，我们研究了纤维中心粘附缺陷的影响，表明它们可以显著降低剥离强度，特别是在无量纲参数\\c{hi}值较高时。这些见解有助于优化仿生粘合剂和微结构表面，应用于各种工程领域。", "summary": "本文通过基于Dugdale内聚区模型的计算方法，深入分析了蘑菇状纤维与刚性基底的剥离行为。研究发现，脱离过程在载荷控制下不稳定，且宽薄的蘑菇帽能有效降低应力集中，促进中心脱离以增强粘附力。同时，揭示了中心粘附缺陷对剥离强度的显著负面影响。这些发现为优化仿生粘合剂和微结构表面提供了重要指导。", "keywords": "蘑菇状纤维, 剥离强度, 粘附, Dugdale内聚区模型, 仿生粘合剂", "comments": "这项研究通过计算模拟深入分析了蘑菇状纤维的粘附机制，揭示了其剥离过程的不稳定性以及结构参数（如蘑菇帽形状）和缺陷对粘附性能的关键影响。这些发现对于设计和优化高性能仿生粘合剂和微结构表面具有重要的理论和实践指导意义，有助于推动相关工程应用的发展。"}}
{"id": "2506.21215", "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?", "authors": ["Haoang Chi", "He Li", "Wenjing Yang", "Feng Liu", "Long Lan", "Xiaoguang Ren", "Tongliang Liu", "Bo Han"], "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.", "comment": "24 pages, accepted at NeurIPS 2024", "pdf_url": "http://arxiv.org/pdf/2506.21215v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21215v1", "AI": {"title_translation": "揭示大型语言模型中的因果推理：现实还是海市蜃楼？", "tldr": "大型语言模型目前仅能进行浅层（一级）因果推理，缺乏类人（二级）能力。新基准CausalProbe-2024证实了这一点。提出的G^2-Reasoner方法通过引入通用知识和目标导向提示，显著提升了LLM的因果推理能力，使其向二级推理迈进。", "motivation": "探讨大型语言模型是否具备真正的类人因果推理能力，现有证据表明其仅限于浅层（一级）因果推理，缺乏深层（二级）能力。", "method": "方法论上，分析了基于Transformer的LLM的自回归机制，指出其并非内在地具有因果性。经验上，引入了新的因果问答基准CausalProbe-2024，其语料对LLM是新鲜的。为弥补一级到二级因果推理的差距，提出了G^2-Reasoner方法，将通用知识和目标导向提示融入LLM的因果推理过程。", "result": "LLM在CausalProbe-2024上的表现显著下降，表明其主要进行一级因果推理。G^2-Reasoner显著提升了LLM的因果推理能力，尤其是在新鲜和反事实语境中。", "conclusion": "本工作为LLM迈向真正的因果推理、超越一级并向二级发展指明了新路径。", "translation": "因果推理能力对于推动大型语言模型（LLM）向强人工智能发展至关重要。尽管多功能LLM似乎已展现出理解语境因果关系并提供符合因果律的响应的能力，但它们是否进行着类似人类的真正因果推理仍不清楚。然而，现有证据表明情况恰恰相反。具体而言，LLM仅能进行浅层（一级）因果推理，这主要归因于其参数中嵌入的因果知识，但它们缺乏真正类人（二级）因果推理的能力。为了支持这一假设，在方法论上，我们深入研究了基于Transformer的LLM的自回归机制，揭示其并非内在地具有因果性。在经验上，我们引入了一个新的因果问答基准CausalProbe-2024，其语料对于所研究的LLM是新鲜且几乎未曾见过的。与早期基准相比，LLM在CausalProbe-2024上的表现显著下降，这表明它们主要进行一级因果推理。为了弥合向二级因果推理的差距，我们从人类推理通常由通用知识和预期目标促进这一事实中获得灵感。我们提出了G^2-Reasoner，这是一种将通用知识和目标导向提示融入LLM因果推理过程的方法。实验表明，G^2-Reasoner显著增强了LLM的因果推理能力，特别是在新鲜和反事实的语境中。这项工作为LLM迈向真正的因果推理、超越一级并向二级发展指明了新路径。", "summary": "本研究探讨了大型语言模型（LLM）的因果推理能力，指出当前LLM主要进行浅层（一级）因果推理，缺乏真正的类人（二级）能力。通过分析Transformer的自回归机制和引入新的因果问答基准CausalProbe-2024，验证了LLM在未见过数据上的表现下降，证实其一级推理的局限性。为弥补这一差距，论文提出了G^2-Reasoner方法，该方法将通用知识和目标导向提示融入LLM的因果推理过程，实验证明其能显著提升LLM在新鲜和反事实语境下的因果推理能力，为LLM向更高级的因果推理发展提供了新途径。", "keywords": "大型语言模型, 因果推理, CausalProbe-2024, G^2-Reasoner, 人工智能", "comments": "本文创新性地提出了LLM因果推理的“一级”和“二级”区分，并通过新颖的基准CausalProbe-2024揭示了LLM在未见数据上因果推理能力的局限性。G^2-Reasoner方法为提升LLM的深层因果推理能力提供了有益的探索方向，强调了通用知识和目标导向提示的重要性。这项工作对于推动LLM向更强人工智能发展具有重要意义，同时也指出了当前LLM在真正理解因果关系方面的不足。"}}
{"id": "2506.20994", "title": "Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe", "authors": ["Måns I. Andersson", "Martin Karp", "Niclas Jansson", "Stefano Markidis"], "summary": "With the emergence of new high-performance computing (HPC) accelerators, such\nas Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures\nhas become a major challenge for HPC application developers. The increasing\nhardware diversity in HPC systems often necessitates the development of\narchitecture-specific code, hindering the sustainability of large-scale\nscientific applications. In this work, we leverage DaCe, a data-centric\nparallel programming framework, to automate the generation of high-performance\nkernels. DaCe enables automatic code generation for multicore processors and\nvarious accelerators, reducing the burden on developers who would otherwise\nneed to rewrite code for each new architecture. Our study demonstrates DaCe's\ncapabilities by applying its automatic code generation to a critical\ncomputational kernel used in Computational Fluid Dynamics (CFD). Specifically,\nwe focus on Neko, a Fortran-based solver that employs the spectral-element\nmethod, which relies on small tensor operations. We detail the formulation of\nthis computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)\nrepresentation and discuss how this approach facilitates high-performance code\ngeneration. Additionally, we outline the workflow for seamlessly integrating\nDaCe's generated code into the Neko solver. Our results highlight the\nportability and performance of the generated code across multiple platforms,\nincluding Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive\nperformance results. By demonstrating the potential of automatic code\ngeneration, we emphasise the feasibility of using portable solutions to ensure\nthe long-term sustainability of large-scale scientific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20994v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20994v1", "AI": {"title_translation": "使用 DaCe 为计算流体动力学代码生成可移植高性能核", "tldr": "为了应对高性能计算（HPC）硬件多样性带来的挑战，本研究利用数据中心并行编程框架 DaCe 自动生成高性能内核。通过将 DaCe 应用于计算流体动力学（CFD）代码 Neko，证明了生成代码在不同 GPU 平台上的可移植性和竞争力，从而提高了大规模科学应用的长期可持续性。", "motivation": "随着新的高性能计算（HPC）加速器（如 Nvidia 和 AMD GPU）的出现，有效针对多样化硬件架构已成为 HPC 应用程序开发者的主要挑战。HPC 系统中不断增长的硬件多样性通常需要开发特定于架构的代码，这阻碍了大规模科学应用的可持续性。", "method": "本研究利用 DaCe 这一数据中心并行编程框架来自动化高性能内核的生成。DaCe 能够为多核处理器和各种加速器自动生成代码，从而减轻了开发者为每个新架构重写代码的负担。具体而言，研究将 DaCe 的自动代码生成应用于计算流体动力学（CFD）中使用的关键计算内核（Neko，一个基于 Fortran 的谱元法求解器，依赖于小型张量操作）。研究详细阐述了使用 DaCe 的有状态数据流多图（SDFG）表示来制定此计算内核，并讨论了这种方法如何促进高性能代码生成。此外，还概述了将 DaCe 生成的代码无缝集成到 Neko 求解器中的工作流程。", "result": "结果突出显示了所生成代码在多个平台（包括 Nvidia GH200、Nvidia A100 和 AMD MI250X GPU）上的可移植性和性能，并取得了有竞争力的性能结果。", "conclusion": "通过展示自动代码生成的潜力，本研究强调了使用可移植解决方案确保大规模科学应用长期可持续性的可行性。", "translation": "随着新的高性能计算（HPC）加速器（如 Nvidia 和 AMD GPU）的出现，有效针对多样化硬件架构已成为 HPC 应用程序开发者的主要挑战。HPC 系统中不断增长的硬件多样性通常需要开发特定于架构的代码，这阻碍了大规模科学应用的可持续性。在这项工作中，我们利用数据中心并行编程框架 DaCe 来自动化高性能内核的生成。DaCe 能够为多核处理器和各种加速器自动生成代码，从而减轻了开发者为每个新架构重写代码的负担。我们的研究通过将 DaCe 的自动代码生成应用于计算流体动力学（CFD）中使用的关键计算内核，展示了 DaCe 的能力。具体而言，我们专注于 Neko，一个基于 Fortran 的求解器，它采用谱元法，该方法依赖于小型张量操作。我们详细阐述了使用 DaCe 的有状态数据流多图（SDFG）表示来制定此计算内核，并讨论了这种方法如何促进高性能代码生成。此外，我们概述了将 DaCe 生成的代码无缝集成到 Neko 求解器中的工作流程。我们的结果突出显示了所生成代码在多个平台（包括 Nvidia GH200、Nvidia A100 和 AMD MI250X GPU）上的可移植性和性能，并取得了有竞争力的性能结果。通过展示自动代码生成的潜力，我们强调了使用可移植解决方案确保大规模科学应用长期可持续性的可行性。", "summary": "本研究旨在解决高性能计算（HPC）领域中由于硬件多样性导致的应用程序可持续性挑战。为此，论文引入了 DaCe，一个数据中心并行编程框架，用于自动化高性能内核的生成。研究将 DaCe 应用于一个关键的计算流体动力学（CFD）内核（Neko 求解器），详细说明了使用 DaCe 的有状态数据流多图（SDFG）表示进行代码生成和集成的工作流程。实验结果表明，DaCe 生成的代码在 Nvidia GH200、Nvidia A100 和 AMD MI250X 等多种 GPU 平台上均表现出良好的可移植性和竞争力，证明了自动代码生成对于确保大规模科学应用长期可持续性的潜力。", "keywords": "DaCe, HPC, 自动代码生成, CFD, 可移植性", "comments": "该论文通过利用 DaCe 框架解决高性能计算领域中日益增长的硬件多样性问题，具有重要的创新性。它提供了一种自动生成可移植高性能内核的解决方案，从而显著减轻了开发者为不同架构重写代码的负担。这种方法对于提高大规模科学应用的可持续性至关重要。"}}
{"id": "2506.20795", "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20795v1", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20795v1", "AI": {"title_translation": "基础模型与基于骨架的方法在人机交互手势识别中的比较如何？", "tldr": "本研究比较了基础模型（V-JEPA, Gemini Flash 2.0）与基于骨架的方法（HD-GCN）在人机交互手势识别中的表现，并引入了新数据集NUGGET。结果显示HD-GCN性能最佳，但V-JEPA也表现出色，表明其在简化系统方面的潜力。Gemini在零样本设置下表现不佳。", "motivation": "传统手势识别方法依赖于复杂的任务特定架构，而视觉基础模型（VFMs）和视觉语言模型（VLMs）具有强大的泛化能力，有望通过取代专用模块来降低系统复杂性。本研究旨在探索这些模型在动态全身手势识别中的应用潜力。", "method": "本研究比较了V-JEPA（一种最先进的视觉基础模型）、Gemini Flash 2.0（一种多模态视觉语言模型）以及HD-GCN（一种性能卓越的基于骨架的方法）在动态全身手势识别方面的性能。研究引入了一个名为NUGGET的新数据集，该数据集专为内部物流环境中的人机交流而设计，用于评估不同的手势识别方法。", "result": "在实验中，HD-GCN取得了最佳性能。V-JEPA仅使用一个简单的任务特定分类头就取得了接近HD-GCN的性能。相比之下，Gemini Flash 2.0在零样本设置下仅凭文本描述难以区分手势。", "conclusion": "尽管HD-GCN表现最佳，但V-JEPA接近的性能，以及其作为共享多任务模型的潜力，为降低系统复杂性提供了一条可能的途径。对于Gemini这类模型，需要进一步研究适合手势的输入表示，尤其是在零样本设置下。", "translation": "手势实现了非语言的人机交流，尤其是在敏捷生产等嘈杂环境中。传统的基于深度学习的手势识别依赖于使用图像、视频或骨骼姿态估计作为输入的任务特定架构。与此同时，视觉基础模型（VFMs）和视觉语言模型（VLMs）凭借其强大的泛化能力，通过取代专用任务特定模块，有望降低系统复杂性。本研究调查了如何调整这些模型进行动态全身手势识别，并比较了V-JEPA（最先进的VFM）、Gemini Flash 2.0（多模态VLM）和HD-GCN（表现最佳的基于骨架的方法）。我们引入了NUGGET，一个专为内部物流环境中人机交流量身定制的数据集，以评估不同的手势识别方法。在我们的实验中，HD-GCN取得了最佳性能，但V-JEPA在仅使用一个简单的任务特定分类头的情况下也表现接近——这为通过将其用作共享多任务模型来降低系统复杂性铺平了可能的道路。相比之下，Gemini在零样本设置下仅凭文本描述难以区分手势，这突出表明需要进一步研究适合手势的输入表示。", "summary": "本论文评估了基础模型（V-JEPA、Gemini Flash 2.0）与基于骨架的方法（HD-GCN）在人机交互中动态全身手势识别方面的表现，并为此目的引入了新的NUGGET数据集。结果表明，HD-GCN性能最优，但V-JEPA在仅使用简单分类头的情况下也表现接近，这表明其作为多任务模型在简化人机交互系统方面的潜力。Gemini在零样本设置下表现不佳，这强调了对更优手势输入表示的需求。", "keywords": "手势识别, 基础模型, 人机交互, 基于骨架的方法, NUGGET数据集", "comments": "该论文解决了人机交互中的一个重要问题。其创新之处在于评估了基础模型与传统方法在手势识别中的表现，并引入了一个新的数据集。V-JEPA在仅使用简单头部的情况下能够与专门的HD-GCN相媲美的发现，对于降低系统复杂性具有重要意义。同时，指出了Gemini在零样本设置下的局限性，为未来的研究指明了方向。"}}
{"id": "2506.20705", "title": "On Convolutions, Intrinsic Dimension, and Diffusion Models", "authors": ["Kin Kwan Leung", "Rasa Hosseinzadeh", "Gabriel Loaiza-Ganem"], "summary": "The manifold hypothesis asserts that data of interest in high-dimensional\nambient spaces, such as image data, lies on unknown low-dimensional\nsubmanifolds. Diffusion models (DMs) -- which operate by convolving data with\nprogressively larger amounts of Gaussian noise and then learning to revert this\nprocess -- have risen to prominence as the most performant generative models,\nand are known to be able to learn distributions with low-dimensional support.\nFor a given datum in one of these submanifolds, we should thus intuitively\nexpect DMs to have implicitly learned its corresponding local intrinsic\ndimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari\net al. (2024b) recently showed that this is indeed the case by linking this LID\nto the rate of change of the log marginal densities of the DM with respect to\nthe amount of added noise, resulting in an LID estimator known as FLIPD. LID\nestimators such as FLIPD have a plethora of uses, among others they quantify\nthe complexity of a given datum, and can be used to detect outliers,\nadversarial examples and AI-generated text. FLIPD achieves state-of-the-art\nperformance at LID estimation, yet its theoretical underpinnings are incomplete\nsince Kamkari et al. (2024b) only proved its correctness under the highly\nunrealistic assumption of affine submanifolds. In this work we bridge this gap\nby formally proving the correctness of FLIPD under realistic assumptions.\nAdditionally, we show that an analogous result holds when Gaussian convolutions\nare replaced with uniform ones, and discuss the relevance of this result.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20705v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20705v1", "AI": {"title_translation": "关于卷积、内蕴维度和扩散模型", "tldr": "本文在现实假设下正式证明了扩散模型中局部内蕴维度（LID）估计器FLIPD的正确性，并探讨了均匀卷积的类似结果。", "motivation": "扩散模型中的局部内蕴维度（LID）估计器FLIPD虽然性能卓越，但其理论基础不完整，此前仅在高度不现实的仿射子流形假设下证明了其正确性，这限制了其理论严谨性和实际应用范围。", "method": "本文通过形式化证明，在现实假设下验证了FLIPD的正确性。此外，研究还表明当高斯卷积被均匀卷积替换时，一个类似的结论仍然成立。", "result": "1. 在现实假设下正式证明了LID估计器FLIPD的正确性。2. 证明了当高斯卷积被均匀卷积替换时，一个类似的LID估计结论也成立。", "conclusion": "本文弥补了局部内蕴维度（LID）估计器FLIPD在理论基础上的不足，为其在更广泛的应用中提供了坚实的理论支撑，并扩展了卷积类型对LID估计影响的理解。", "translation": "高维环境空间（如图像数据）中的数据位于未知的低维子流形上，这是流形假设的核心。扩散模型（DM）——通过将数据与逐渐增大的高斯噪声进行卷积，然后学习逆转这个过程——已成为性能最优的生成模型，并已知能够学习具有低维支持的分布。因此，对于这些子流形中的给定数据，我们应该直观地期望DM能够隐式地学习到其对应的局部内蕴维度（LID），即其所属子流形的维度。Kamkari 等人（2024b）最近通过将LID与DM的对数边际密度随添加噪声量的变化率联系起来，证明了确实如此，从而产生了一个名为FLIPD的LID估计器。FLIPD 等LID估计器用途广泛，它们可以量化给定数据的复杂性，并可用于检测异常值、对抗性样本和AI生成文本。FLIPD 在LID估计方面取得了最先进的性能，但其理论基础不完整，因为Kamkari 等人（2024b）仅在高度不现实的仿射子流形假设下证明了其正确性。在这项工作中，我们通过在现实假设下正式证明FLIPD的正确性来弥补这一空白。此外，我们还表明，当高斯卷积被均匀卷积替换时，一个类似的结论仍然成立，并讨论了这一结果的相关性。", "summary": "本文弥补了扩散模型中局部内蕴维度（LID）估计器FLIPD的理论空白。此前的研究仅在非现实的仿射子流形假设下证明了FLIPD的正确性。本研究在现实假设下正式证明了FLIPD的正确性，并进一步展示了当高斯卷积被均匀卷积替换时，一个类似的LID估计结果依然成立。这些发现为FLIPD的广泛应用提供了更坚实的理论基础。", "keywords": "扩散模型, 局部内蕴维度, FLIPD, 卷积, 流形假设", "comments": "这项工作的重要创新在于它解决了LID估计器FLIPD在理论证明上的一个关键限制，即将其正确性扩展到更现实的非仿射子流形情况。这极大地增强了FLIPD的适用性和可信度，对于利用LID进行数据复杂性量化、异常检测和AI生成内容识别等应用具有重要意义。同时，对均匀卷积的探讨也拓宽了对扩散模型中卷积操作与内蕴维度关系的理解。"}}
{"id": "2506.20904", "title": "Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL", "authors": ["Matthew Zurek", "Guy Zamir", "Yudong Chen"], "summary": "We study offline reinforcement learning in average-reward MDPs, which\npresents increased challenges from the perspectives of distribution shift and\nnon-uniform coverage, and has been relatively underexamined from a theoretical\nperspective. While previous work obtains performance guarantees under\nsingle-policy data coverage assumptions, such guarantees utilize additional\ncomplexity measures which are uniform over all policies, such as the uniform\nmixing time. We develop sharp guarantees depending only on the target policy,\nspecifically the bias span and a novel policy hitting radius, yielding the\nfirst fully single-policy sample complexity bound for average-reward offline\nRL. We are also the first to handle general weakly communicating MDPs,\ncontrasting restrictive structural assumptions made in prior work. To achieve\nthis, we introduce an algorithm based on pessimistic discounted value iteration\nenhanced by a novel quantile clipping technique, which enables the use of a\nsharper empirical-span-based penalty function. Our algorithm also does not\nrequire any prior parameter knowledge for its implementation. Remarkably, we\nshow via hard examples that learning under our conditions requires coverage\nassumptions beyond the stationary distribution of the target policy,\ndistinguishing single-policy complexity measures from previously examined\ncases. We also develop lower bounds nearly matching our main result.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20904v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20904v1", "AI": {"title_translation": "平均奖励离线强化学习中的最优单策略样本复杂度和瞬态覆盖", "tldr": "该论文首次为平均奖励离线强化学习提供了完全单策略样本复杂度边界，并处理了一般弱通信MDPs，提出了一种新的具有尖锐保证的算法。", "motivation": "平均奖励MDPs中的离线强化学习在分布偏移和非均匀覆盖方面具有挑战性，且理论研究不足。现有工作依赖于统一的复杂性度量（如统一混合时间），而非针对特定策略。本文旨在开发仅依赖于目标策略的更尖锐的保证，并处理一般弱通信MDPs。", "method": "作者提出了一种基于悲观折扣值迭代的算法，该算法通过新颖的分位数裁剪技术得到增强，从而能够使用更尖锐的基于经验跨度的惩罚函数。该算法无需任何先验参数知识。", "result": "本文首次获得了平均奖励离线RL的完全单策略样本复杂度边界。它也是第一个处理一般弱通信MDPs的研究。通过困难示例，作者表明在他们的条件下学习需要超出目标策略的平稳分布的覆盖假设。他们还开发了几乎与主要结果匹配的下限。", "conclusion": "本文首次建立了平均奖励离线强化学习的完全单策略样本复杂度边界，证明了单策略复杂性度量需要超出平稳分布的特定覆盖假设，并提供了一种有效的算法。", "translation": "我们研究平均奖励MDPs中的离线强化学习，这在分布偏移和非均匀覆盖方面带来了更大的挑战，并且从理论角度来看相对未被充分研究。虽然之前的工作在单策略数据覆盖假设下获得了性能保证，但这些保证利用了对所有策略统一的额外复杂性度量，例如统一混合时间。我们开发了仅依赖于目标策略的尖锐保证，特别是偏差跨度和一种新颖的策略命中半径，从而首次获得了平均奖励离线RL的完全单策略样本复杂度边界。我们也是第一个处理一般弱通信MDPs的研究，这与先前工作中做出的限制性结构假设形成对比。为了实现这一点，我们引入了一种基于悲观折扣值迭代的算法，该算法通过一种新颖的分位数裁剪技术得到增强，从而能够使用更尖锐的基于经验跨度的惩罚函数。我们的算法也不需要任何先验参数知识即可实现。值得注意的是，我们通过困难示例表明，在我们的条件下学习需要超出目标策略的平稳分布的覆盖假设，从而将单策略复杂性度量与先前研究的案例区分开来。我们还开发了几乎与我们主要结果匹配的下限。", "summary": "本文旨在解决平均奖励离线强化学习中理论研究不足的问题，该领域存在分布偏移和非均匀覆盖的挑战。文章首次提出了针对平均奖励离线RL的完全单策略样本复杂度边界，其保证仅依赖于目标策略特定的度量（如偏差跨度和新颖的策略命中半径），而非统一复杂性度量。该工作还首次处理了一般弱通信MDPs。为此，作者提出了一种基于悲观折扣值迭代并结合分位数裁剪技术的新算法，该算法无需先验参数知识且使用了更尖锐的惩罚函数。研究表明，单策略学习需要超出目标策略平稳分布的覆盖假设，并提供了匹配的下限。", "keywords": "离线强化学习, 平均奖励MDPs, 样本复杂度, 单策略, 弱通信MDPs", "comments": "这篇论文在离线强化学习领域做出了重要的理论贡献，首次为平均奖励MDPs提供了单策略样本复杂度边界，这是一个较少探索的领域。引入目标策略特定的复杂性度量（偏差跨度、策略命中半径）是创新之处，超越了限制性的统一假设。该算法能够处理一般弱通信MDPs且无需参数知识，具有实际优势。研究发现单策略学习需要超出平稳分布的覆盖假设，这挑战了现有的一些假设。"}}
{"id": "2506.20906", "title": "Almost Tight Additive Guarantees for \\boldmath $k$-Edge-Connectivity", "authors": ["Nikhil Kumar", "Chaitanya Swamy"], "summary": "We consider the \\emph{$k$-edge connected spanning subgraph} (kECSS) problem,\nwhere we are given an undirected graph $G = (V, E)$ with nonnegative edge costs\n$\\{c_e\\}_{e\\in E}$, and we seek a minimum-cost \\emph{$k$-edge connected}\nsubgraph $H$ of $G$. For even $k$, we present a polytime algorithm that\ncomputes a $(k-2)$-edge connected subgraph of cost at most the optimal value\n$LP^*$ of the natural LP-relaxation for kECSS; for odd $k$, we obtain a\n$(k-3)$-edge connected subgraph of cost at most $LP^*$. Since kECSS is APX-hard\nfor all $k\\geq 2$, our results are nearly optimal. They also significantly\nimprove upon the recent work of Hershkowitz et al., both in terms of solution\nquality and the simplicity of algorithm and its analysis. Our techniques also\nyield an alternate guarantee, where we obtain a $(k-1)$-edge connected subgraph\nof cost at most $1.5\\cdot LP^*$; with unit edge costs, the cost guarantee\nimproves to $(1+\\frac{4}{3k})\\cdot LP^*$, which improves upon the\nstate-of-the-art approximation for unit edge costs, but with a unit loss in\nedge connectivity.\n  Our kECSS-result also yields results for the \\emph{$k$-edge connected\nspanning multigraph} (kECSM) problem, where multiple copies of an edge can be\nselected: we obtain a $(1+2/k)$-approximation algorithm for even $k$, and a\n$(1+3/k)$-approximation algorithm for odd $k$.\n  Our techniques extend to the degree-bounded versions of kECSS and kECSM,\nwherein we also impose degree lower- and upper- bounds on the nodes. We obtain\nthe same cost and connectivity guarantees for these degree-bounded versions\nwith an additive violation of (roughly) $2$ for the degree bounds. These are\nthe first results for degree-bounded \\{kECSS,kECSM\\} of the form where the cost\nof the solution obtained is at most the optimum, and the connectivity\nconstraints are violated by an additive constant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20906v1", "categories": ["cs.DS", "F.2.2; G.2"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20906v1", "AI": {"title_translation": "关于$k$-边连通性的几乎紧致加性保证", "tldr": "该论文为$k$-边连通生成子图 (kECSS) 问题提出了几乎最优的算法，改进了现有结果的解质量和简洁性，并扩展到$k$-边连通生成多图 (kECSM) 和度数受限版本。", "motivation": "该论文研究了$k$-边连通生成子图 (kECSS) 问题，该问题是APX-hard的。其动机是为该问题及相关变体寻找更好的近似解，并旨在改进现有工作。", "method": "论文提出了一种多项式时间算法。其技术还扩展到kECSS和kECSM的度数受限版本。这些算法实现了边连通性和度数限制的加性保证。", "result": "对于kECSS中的偶数$k$，计算出成本至多为$LP^*$的$(k-2)$-边连通子图；对于奇数$k$，获得成本至多为$LP^*$的$(k-3)$-边连通子图。对于kECSS的替代保证，获得成本至多为$1.5\\cdot LP^*$的$(k-1)$-边连通子图；对于单位边成本，成本保证提高到$(1+\\frac{4}{3k})\\cdot LP^*$。对于kECSM，偶数$k$获得$(1+2/k)$-近似算法，奇数$k$获得$(1+3/k)$-近似算法。对于度数受限版本，获得了与原始问题相同的成本和连通性保证，但在度数限制方面有大约2的加性违反。", "conclusion": "由于kECSS是APX-hard的，因此该结果几乎是最优的。它们在解决方案质量和算法及其分析的简单性方面显著改进了Hershkowitz等人的近期工作。对于度数受限的{kECSS,kECSM}问题，这些是首次获得解成本至多为最优值且连通性约束有加性常数违反的结果。", "translation": "我们考虑$k$-边连通生成子图 (kECSS) 问题，其中给定一个具有非负边成本的无向图$G = (V, E)$，我们寻求一个最小成本的$k$-边连通子图$H$。对于偶数$k$，我们提出了一个多项式时间算法，该算法计算一个$(k-2)$-边连通子图，其成本至多为kECSS的自然LP-松弛的最优值$LP^*$；对于奇数$k$，我们获得一个$(k-3)$-边连通子图，其成本至多为$LP^*$。由于kECSS对于所有$k\\geq 2$都是APX-hard的，我们的结果几乎是最优的。它们还在解决方案质量以及算法及其分析的简单性方面显著改进了Hershkowitz等人最近的工作。我们的技术还提供了一个替代保证，即我们获得一个$(k-1)$-边连通子图，其成本至多为$1.5\\cdot LP^*$；对于单位边成本，成本保证提高到$(1+\\frac{4}{3k})\\cdot LP^*$，这改进了单位边成本的最新近似算法，但边连通性损失了一个单位。我们的kECSS结果也为$k$-边连通生成多图 (kECSM) 问题带来了结果，其中可以选择边的多个副本：对于偶数$k$，我们获得一个$(1+2/k)$-近似算法；对于奇数$k$，我们获得一个$(1+3/k)$-近似算法。我们的技术扩展到kECSS和kECSM的度数受限版本，其中我们还对节点施加度数下限和上限。对于这些度数受限版本，我们获得了相同的成本和连通性保证，但在度数限制方面有（大约）2的加性违反。这些是度数受限{kECSS,kECSM}问题的首次结果，其解决方案成本至多为最优值，并且连通性约束被一个加性常数违反。", "summary": "该论文解决了APX-hard的$k$-边连通生成子图 (kECSS) 问题，提供了多项式时间算法，实现了几乎紧致的加性保证。对于偶数$k$，它找到了一个$(k-2)$-边连通子图；对于奇数$k$，找到了一个$(k-3)$-边连通子图，两者成本均达到最优LP松弛值。它还提供了一个$(k-1)$-边连通子图，成本至多为$1.5\\cdot LP^*$（或对于单位成本为$(1+4/(3k))\\cdot LP^*$）。这些方法扩展到$k$-边连通生成多图 (kECSM) 问题，产生了改进的近似比率，并扩展到度数受限版本，对度数约束有加性违反。这些结果在解质量和算法简洁性方面显著提升了现有技术水平。", "keywords": "k-边连通性, 生成子图, 近似算法, LP松弛, 度数受限问题", "comments": "该论文通过为已知的APX-hard问题提供几乎最优的加性保证，做出了重要贡献。在解质量和算法简洁性方面对现有工作的改进值得关注。扩展到多图和度数受限版本，特别是后者首次实现了加性违反的结果，突出了其技术的通用性和影响力。使用LP松弛作为基准是标准做法，并证明了其界限相对于理论最优的强度。"}}
{"id": "2506.20882", "title": "Resilience Through Escalation: A Graph-Based PACE Architecture for Satellite Threat Response", "authors": ["Anouar Boumeftah", "Sarah McKenzie-Picot", "Peter Klimas", "Gunes Karabulut Kurt"], "summary": "Satellite systems increasingly face operational risks from jamming,\ncyberattacks, and electromagnetic disruptions. Traditional redundancy\nstrategies often fail against dynamic, multi-vector threats. This paper\nintroduces a resilience-by-design framework grounded in the PACE (Primary,\nAlternate, Contingency, Emergency) methodology, originally developed for\ntactical communications in military operations, adapting it to satellite\nsystems through a layered state-transition model informed by threat scoring\nframeworks such as CVSS, DREAD, and NASA's risk matrix. We define a dynamic\nresilience index to quantify system adaptability and implement three PACE\nvariants: static, adaptive, and softmax-based decision models, to evaluate\nresilience under diverse disruption scenarios. The proposed approach highlights\nthe effectiveness of lightweight, decision-aware fallback mechanisms in\nimproving survivability and operational continuity for next-generation space\nassets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20882v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20882v1", "AI": {"title_translation": "通过升级实现韧性：一种基于图的卫星威胁响应PACE架构", "tldr": "本文提出了一种基于PACE（主用、备用、应急、紧急）方法的分层状态转换模型，用于提高卫星系统对动态多向量威胁的韧性，并通过三种PACE变体评估了其有效性。", "motivation": "卫星系统日益面临来自干扰、网络攻击和电磁中断的运行风险，而传统的冗余策略在面对动态、多向量威胁时往往失效。", "method": "本文引入了一种基于PACE方法（源自军事战术通信）的韧性设计框架，并通过分层状态转换模型将其应用于卫星系统，该模型结合了CVSS、DREAD和NASA风险矩阵等威胁评分框架。研究定义了动态韧性指数来量化系统适应性，并实现了三种PACE变体：静态、自适应和基于softmax的决策模型，以评估在不同中断场景下的韧性。", "result": "所提出的方法突出了轻量级、决策感知的故障恢复机制在提高下一代空间资产生存能力和运行连续性方面的有效性。", "conclusion": "轻量级、决策感知的故障恢复机制可以有效提高下一代空间资产的生存能力和运行连续性。", "translation": "卫星系统日益面临来自干扰、网络攻击和电磁中断的运行风险。传统的冗余策略在应对动态、多向量威胁时往往失效。本文引入了一种基于PACE（主用、备用、应急、紧急）方法论的韧性设计框架，该方法论最初为军事行动中的战术通信而开发，通过结合CVSS、DREAD和NASA风险矩阵等威胁评分框架的分层状态转换模型，将其应用于卫星系统。我们定义了一个动态韧性指数来量化系统适应性，并实现了三种PACE变体：静态、自适应和基于softmax的决策模型，以评估在不同中断场景下的韧性。所提出的方法突出了轻量级、决策感知的故障恢复机制在提高下一代空间资产生存能力和运行连续性方面的有效性。", "summary": "本文针对卫星系统面临的动态多向量威胁，提出了一种基于PACE方法的分层状态转换韧性设计框架。该框架借鉴军事领域的PACE理念，并结合威胁评分模型，定义了动态韧性指数。通过实现静态、自适应和softmax三种PACE决策模型，验证了轻量级、决策感知的故障恢复机制能有效提升下一代空间资产的生存能力和运行连续性。", "keywords": "卫星韧性, PACE架构, 威胁响应, 故障恢复, 空间资产", "comments": "本文创新性地将军事领域的PACE方法引入卫星系统韧性设计，并通过结合威胁评分框架和分层状态转换模型，提供了一种新颖的威胁响应策略。其强调轻量级、决策感知的故障恢复机制，对于提升未来空间资产的生存能力和运行连续性具有重要意义。"}}
{"id": "2506.20858", "title": "Doppler Estimation and Compensation Techniques in LoRa Direct-to-Satellite Communications", "authors": ["Jamil Farhat", "Gianni Pasolini", "Enrico Paolini", "Muhammad Asad Ullah", "Richard Demo Souza"], "summary": "Within the LPWAN framework, the LoRa modulation adopted by LoRaWAN technology\nhas garnered significant interest as a connectivity solution for IoT\napplications due to its ability to offer low-cost, low-power, and long-range\ncommunications. One emerging use case of LoRa is DtS connectivity, which\nextends coverage to remote areas for supporting IoT operations. The satellite\nIoT industry mainly prefers LEO because it has lower launch costs and less path\nloss compared to Geostationary orbit. However, a major drawback of LEO\nsatellites is the impact of the Doppler effect caused by their mobility.\nEarlier studies have confirmed that the Doppler effect significantly degrades\nthe LoRa DtS performance. In this paper, we propose four frameworks for Doppler\nestimation and compensation in LoRa DtS connectivity and numerically compare\nthe performance against the ideal scenario without the Doppler effect.\nFurthermore, we investigate the trade-offs among these frameworks by analyzing\nthe interplay between spreading factor, and other key parameters related to the\nDoppler effect. The results provide insights into how to achieve robust LoRa\nconfigurations for DtS connectivity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20858v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20858v1", "AI": {"title_translation": "LoRa直连卫星通信中的多普勒估计与补偿技术", "tldr": "本文提出了四种多普勒效应估计与补偿框架，以提高LoRa直连卫星通信性能。", "motivation": "LoRaWAN技术因其低成本、低功耗和长距离通信能力在物联网应用中受到关注，尤其是在直连卫星（DtS）通信中扩展覆盖范围。然而，低地球轨道（LEO）卫星的移动性导致的多普勒效应严重降低了LoRa DtS的性能，因此需要有效的多普勒估计和补偿技术。", "method": "本文提出了四种用于LoRa直连卫星连接中的多普勒效应估计和补偿框架。通过数值比较这些框架的性能，并与没有多普勒效应的理想场景进行对比。此外，还分析了扩频因子与多普勒效应相关关键参数之间的相互作用，以研究这些框架的权衡。", "result": "结果提供了关于如何为直连卫星连接实现鲁棒LoRa配置的见解。", "conclusion": "通过提出的多普勒估计和补偿框架以及对关键参数权衡的分析，可以为LoRa直连卫星通信找到鲁棒的配置，从而有效解决多普勒效应带来的性能下降问题。", "translation": "在LPWAN框架内，LoRaWAN技术采用的LoRa调制因其提供低成本、低功耗和长距离通信的能力，作为物联网应用的连接解决方案受到了广泛关注。LoRa的一个新兴用例是DtS连接，它将覆盖范围扩展到偏远地区以支持物联网操作。卫星物联网行业主要偏好LEO，因为它与地球同步轨道相比具有更低的发射成本和更小的路径损耗。然而，LEO卫星的一个主要缺点是其移动性引起的多普勒效应的影响。早期的研究已经证实，多普勒效应显著降低了LoRa DtS的性能。在本文中，我们提出了四种用于LoRa DtS连接中的多普勒估计和补偿框架，并将其性能与没有多普勒效应的理想场景进行数值比较。此外，我们通过分析扩频因子与多普勒效应相关的其他关键参数之间的相互作用，研究了这些框架之间的权衡。结果提供了关于如何为DtS连接实现鲁棒LoRa配置的见解。", "summary": "本文针对LoRa直连卫星（DtS）通信中低地球轨道（LEO）卫星移动性导致的多普勒效应严重影响性能的问题，提出了四种多普勒估计与补偿框架。研究通过数值比较这些框架的性能，并分析扩频因子等关键参数与多普勒效应之间的权衡，旨在为LoRa DtS连接提供鲁棒的配置指导。", "keywords": "LoRa, 直连卫星通信, 多普勒效应, 补偿技术, 物联网", "comments": "本文解决了LoRa直连卫星通信中一个关键且实际的问题——多普勒效应，这对于扩展物联网覆盖范围至关重要。通过提出并比较多种补偿框架，并深入分析关键参数的权衡，为实际系统部署提供了有价值的指导，具有较高的工程应用价值。"}}
{"id": "2506.21386", "title": "Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings", "authors": ["Ghazal Al-Shwayyat", "Omer Nezih Gerek"], "summary": "Arabic dialect recognition presents a significant challenge in speech\ntechnology due to the linguistic diversity of Arabic and the scarcity of large\nannotated datasets, particularly for underrepresented dialects. This research\ninvestigates hybrid modeling strategies that integrate classical signal\nprocessing techniques with deep learning architectures to address this problem\nin low-resource scenarios. Two hybrid models were developed and evaluated: (1)\nMel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural\nNetwork (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with\na Recurrent Neural Network (RNN). The models were trained on a dialect-filtered\nsubset of the Common Voice Arabic dataset, with dialect labels assigned based\non speaker metadata. Experimental results demonstrate that the MFCC + CNN\narchitecture achieved superior performance, with an accuracy of 91.2% and\nstrong precision, recall, and F1-scores, significantly outperforming the\nWavelet + RNN configuration, which achieved an accuracy of 66.5%. These\nfindings highlight the effectiveness of leveraging spectral features with\nconvolutional models for Arabic dialect recognition, especially when working\nwith limited labeled data. The study also identifies limitations related to\ndataset size, potential regional overlaps in labeling, and model optimization,\nproviding a roadmap for future research. Recommendations for further\nimprovement include the adoption of larger annotated corpora, integration of\nself-supervised learning techniques, and exploration of advanced neural\narchitectures such as Transformers. Overall, this research establishes a strong\nbaseline for future developments in Arabic dialect recognition within\nresource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21386v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21386v1", "AI": {"title_translation": "混合深度学习与信号处理在低资源环境下阿拉伯语方言识别中的应用", "tldr": "本研究探讨了在低资源环境下，结合信号处理和深度学习的混合模型在阿拉伯语方言识别中的应用，其中MFCC+CNN模型表现最佳。", "motivation": "阿拉伯语方言识别面临重大挑战，原因在于阿拉伯语的语言多样性以及大型标注数据集的稀缺性，特别是对于代表性不足的方言。", "method": "开发并评估了两种混合模型：(1) 梅尔频率倒谱系数 (MFCC) 结合卷积神经网络 (CNN)，以及 (2) 离散小波变换 (DWT) 特征结合循环神经网络 (RNN)。模型在Common Voice阿拉伯语数据集的方言过滤子集上进行训练，方言标签根据说话人元数据分配。", "result": "MFCC + CNN架构取得了卓越的性能，准确率达到91.2%，并具有强大的精确度、召回率和F1分数，显著优于小波 + RNN 配置，后者准确率为66.5%。这些发现突出了利用谱特征与卷积模型进行阿拉伯语方言识别的有效性，尤其是在处理有限标注数据时。", "conclusion": "该研究为资源受限环境下阿拉伯语方言识别的未来发展奠定了坚实的基础。研究还指出了与数据集大小、标签中潜在的区域重叠以及模型优化相关的局限性，并为未来的研究提供了路线图，包括采用更大的标注语料库、整合自监督学习技术以及探索更先进的神经网络架构。", "translation": "阿拉伯语方言识别在语音技术中面临重大挑战，原因在于阿拉伯语的语言多样性以及大型标注数据集的稀缺性，特别是对于代表性不足的方言。本研究调查了结合经典信号处理技术与深度学习架构的混合建模策略，以解决低资源场景下的这一问题。开发并评估了两种混合模型：(1) 梅尔频率倒谱系数 (MFCC) 结合卷积神经网络 (CNN)，以及 (2) 离散小波变换 (DWT) 特征结合循环神经网络 (RNN)。这些模型在Common Voice阿拉伯语数据集的方言过滤子集上进行训练，方言标签根据说话人元数据分配。实验结果表明，MFCC + CNN架构取得了卓越的性能，准确率达到91.2%，并具有强大的精确度、召回率和F1分数，显著优于小波 + RNN 配置，后者准确率为66.5%。这些发现突出了利用谱特征与卷积模型进行阿拉伯语方言识别的有效性，尤其是在处理有限标注数据时。该研究还指出了与数据集大小、标签中潜在的区域重叠以及模型优化相关的局限性，为未来的研究提供了路线图。进一步改进的建议包括采用更大的标注语料库、整合自监督学习技术以及探索更先进的神经网络架构，例如Transformer。总的来说，这项研究为资源受限环境下阿拉伯语方言识别的未来发展奠定了坚实的基础。", "summary": "本文针对低资源环境下阿拉伯语方言识别的挑战，提出并评估了两种结合信号处理与深度学习的混合模型：MFCC+CNN和DWT+RNN。实验结果表明，MFCC+CNN模型表现最优，准确率达91.2%，突显了谱特征与卷积模型在有限数据下的有效性。研究还讨论了数据集局限性及未来改进方向，为该领域研究奠定了基础。", "keywords": "阿拉伯语方言识别, 混合深度学习, 信号处理, 低资源, MFCC-CNN", "comments": "该研究的创新点在于结合了传统的信号处理技术（如MFCC）与现代深度学习架构（CNN），有效解决了低资源环境下阿拉伯语方言识别的难题。MFCC+CNN模型的优异表现证明了这种混合策略的有效性。其重要性在于为资源匮乏的语言技术领域提供了可行的解决方案，并为未来的研究指明了方向，如采用更大的数据集和更先进的模型。局限性在于数据集大小和标签潜在的区域重叠，这可能影响模型的泛化能力。"}}
{"id": "2506.20897", "title": "Development of MR spectral analysis method robust against static magnetic field inhomogeneity", "authors": ["Shuki Maruyama", "Hidenori Takeshima"], "summary": "Purpose:To develop a method that enhances the accuracy of spectral analysis\nin the presence of static magnetic field B0 inhomogeneity. Methods:The authors\nproposed a new spectral analysis method utilizing a deep learning model trained\non modeled spectra that consistently represent the spectral variations induced\nby B0 inhomogeneity. These modeled spectra were generated from the B0 map and\nmetabolite ratios of the healthy human brain. The B0 map was divided into a\npatch size of subregions, and the separately estimated metabolites and baseline\ncomponents were averaged and then integrated. The quality of the modeled\nspectra was visually and quantitatively evaluated against the measured spectra.\nThe analysis models were trained using measured, simulated, and modeled\nspectra. The performance of the proposed method was assessed using mean squared\nerrors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)\nof the metabolite ratios were also compared to LCModel when analyzing the\nphantom spectra acquired under two types of B0 inhomogeneity. Results:The\nmodeled spectra exhibited broadened and narrowed spectral peaks depending on\nthe B0 inhomogeneity and were quantitatively close to the measured spectra. The\nanalysis model trained using measured spectra with modeled spectra improved\nMSEs by 49.89% compared to that trained using measured spectra alone, and by\n26.66% compared to that trained using measured spectra with simulated spectra.\nThe performance improved as the number of modeled spectra increased from 0 to\n1,000. This model showed significantly lower MAPEs than LCModel under both\ntypes of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep\nlearning model using the modeled spectra was developed. The results suggest\nthat the proposed method has the potential to improve the accuracy of spectral\nanalysis by increasing the training samples of spectra.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20897v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20897v1", "AI": {"title_translation": "开发对静磁场不均匀性具有鲁棒性的MR光谱分析方法", "tldr": "本文开发了一种新的深度学习MR光谱分析方法，利用建模光谱来提高在静磁场不均匀性下的分析准确性，并显示出比传统方法更低的误差。", "motivation": "为了开发一种在静磁场B0不均匀性存在下提高光谱分析准确性的方法。", "method": "作者提出了一种新的光谱分析方法，利用在模拟光谱上训练的深度学习模型。这些模拟光谱通过B0图和健康人脑的代谢物比例生成，能够一致地表示由B0不均匀性引起的光谱变化。B0图被划分为子区域，分别估计的代谢物和基线成分被平均并整合。通过均方误差（MSEs）和平均绝对百分比误差（MAPEs）评估了所提出方法的性能，并与LCModel进行了比较。", "result": "模拟光谱根据B0不均匀性表现出加宽和变窄的光谱峰，并与测量光谱定量接近。使用测量光谱和模拟光谱训练的分析模型，与仅使用测量光谱训练的模型相比，MSEs提高了49.89%；与使用测量光谱和模拟光谱训练的模型相比，MSEs提高了26.66%。性能随着模拟光谱数量从0增加到1,000而提高。该模型在两种B0不均匀性下均显示出比LCModel显著更低的MAPE。", "conclusion": "开发了一种使用建模光谱训练的新的光谱分析深度学习模型。结果表明，所提出的方法通过增加光谱训练样本，有潜力提高光谱分析的准确性。", "translation": "目的：开发一种在静磁场B0不均匀性存在下提高光谱分析准确性的方法。方法：作者提出了一种新的光谱分析方法，利用在模拟光谱上训练的深度学习模型，这些模型能够一致地表示由B0不均匀性引起的光谱变化。这些模拟光谱由健康人脑的B0图和代谢物比例生成。B0图被划分为补丁大小的子区域，分别估计的代谢物和基线成分被平均然后整合。通过视觉和定量评估，将模拟光谱的质量与测量光谱进行了比较。分析模型使用测量光谱、模拟光谱和建模光谱进行训练。所提出方法的性能通过代谢物比例的均方误差（MSEs）进行评估。在两种B0不均匀性下采集的体模光谱分析中，还将代谢物比例的平均绝对百分比误差（MAPEs）与LCModel进行了比较。结果：模拟光谱根据B0不均匀性表现出加宽和变窄的光谱峰，并与测量光谱定量接近。使用测量光谱和建模光谱训练的分析模型，与仅使用测量光谱训练的模型相比，MSEs提高了49.89%；与使用测量光谱和模拟光谱训练的模型相比，MSEs提高了26.66%。性能随着建模光谱数量从0增加到1,000而提高。该模型在两种B0不均匀性下均显示出比LCModel显著更低的MAPE。结论：开发了一种使用建模光谱训练的新的光谱分析深度学习模型。结果表明，所提出的方法通过增加光谱训练样本，有潜力提高光谱分析的准确性。", "summary": "本文提出了一种基于深度学习的MR光谱分析新方法，旨在解决静磁场B0不均匀性导致的光谱分析准确性问题。该方法通过生成并利用大量建模光谱来训练深度学习模型，这些建模光谱能有效模拟B0不均匀性下的光谱变化。实验结果表明，与传统方法LCModel及仅使用测量或模拟光谱训练的模型相比，所提出的方法显著提高了代谢物比例的分析准确性，证明了其在实际应用中提升MR光谱分析质量的潜力。", "keywords": "MR光谱分析, 深度学习, B0不均匀性, 建模光谱, 代谢物分析", "comments": "该论文的创新点在于利用大量“建模光谱”来训练深度学习模型，以克服MR光谱分析中静磁场不均匀性带来的挑战。这种方法有效地增加了训练数据量，并提升了模型对复杂B0不均匀性的鲁棒性，从而显著提高了分析准确性。其重要性在于为临床MR光谱应用提供了一种更可靠、更精确的分析工具。"}}
{"id": "2506.20946", "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models", "authors": ["Donggoo Kang", "Jangyeong Kim", "Dasol Jeong", "Junyoung Choi", "Jeonga Wi", "Hyunmin Lee", "Joonho Gwon", "Joonki Paik"], "summary": "Current texture synthesis methods, which generate textures from fixed\nviewpoints, suffer from inconsistencies due to the lack of global context and\ngeometric understanding. Meanwhile, recent advancements in video generation\nmodels have demonstrated remarkable success in achieving temporally consistent\nvideos. In this paper, we introduce VideoTex, a novel framework for seamless\ntexture synthesis that leverages video generation models to address both\nspatial and temporal inconsistencies in 3D textures. Our approach incorporates\ngeometry-aware conditions, enabling precise utilization of 3D mesh structures.\nAdditionally, we propose a structure-wise UV diffusion strategy, which enhances\nthe generation of occluded areas by preserving semantic information, resulting\nin smoother and more coherent textures. VideoTex not only achieves smoother\ntransitions across UV boundaries but also ensures high-quality, temporally\nstable textures across video frames. Extensive experiments demonstrate that\nVideoTex outperforms existing methods in texture fidelity, seam blending, and\nstability, paving the way for dynamic real-time applications that demand both\nvisual quality and temporal coherence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20946v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.7; I.4.10; I.2.10"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20946v1", "AI": {"title_translation": "使用几何感知扩散和时间视频模型进行一致的零样本3D纹理合成", "tldr": "VideoTex利用视频生成模型实现一致的3D纹理合成，解决了现有方法的空间和时间不一致性问题，并通过几何感知和结构化UV扩散策略提高了纹理质量和稳定性。", "motivation": "当前的纹理合成方法由于缺乏全局上下文和几何理解，从固定视角生成纹理时存在不一致性。尽管视频生成模型在实现时间一致性视频方面取得了显著成功，但3D纹理合成仍面临挑战。", "method": "本文引入了VideoTex框架，该框架利用视频生成模型解决3D纹理中的空间和时间不一致性。它整合了几何感知条件以精确利用3D网格结构，并提出了一种结构化UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而产生更平滑、更连贯的纹理。", "result": "VideoTex不仅实现了UV边界之间更平滑的过渡，而且确保了视频帧之间高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法。", "conclusion": "VideoTex为需要视觉质量和时间连贯性的动态实时应用铺平了道路，通过其创新的几何感知扩散和时间视频模型方法，解决了3D纹理合成中的关键一致性问题。", "translation": "当前的纹理合成方法从固定视角生成纹理时，由于缺乏全局上下文和几何理解，存在不一致性。与此同时，视频生成模型的最新进展在实现时间一致性视频方面取得了显著成功。在本文中，我们引入了VideoTex，这是一个用于无缝纹理合成的新颖框架，它利用视频生成模型来解决3D纹理中的空间和时间不一致性。我们的方法结合了几何感知条件，能够精确利用3D网格结构。此外，我们提出了一种结构化UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而产生更平滑、更连贯的纹理。VideoTex不仅实现了UV边界之间更平滑的过渡，而且确保了视频帧之间高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法，为需要视觉质量和时间连贯性的动态实时应用铺平了道路。", "summary": "VideoTex是一个新颖的框架，利用视频生成模型解决3D纹理合成中的空间和时间不一致性问题。它通过引入几何感知条件和结构化UV扩散策略，精确利用3D网格结构并增强遮挡区域的生成，从而实现高质量、时间稳定的纹理。实验证明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法，为动态实时应用提供了可能。", "keywords": "3D纹理合成, 视频生成模型, 几何感知, 扩散模型, 时间一致性", "comments": "该论文的创新点在于将视频生成模型的优势引入到3D纹理合成中，有效地解决了传统方法在全局上下文和几何理解方面的不足，尤其是在处理空间和时间一致性问题上。几何感知和结构化UV扩散策略是其核心贡献，显著提升了纹理质量和跨帧稳定性，这对于实时3D应用具有重要意义。"}}
{"id": "2506.20963", "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora", "authors": ["Fangyuan Zhang", "Zhengjun Huang", "Yingli Zhou", "Qintian Guo", "Zhixun Li", "Wensheng Luo", "Di Jiang", "Yixiang Fang", "Xiaofang Zhou"], "summary": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20963v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20963v1", "AI": {"title_translation": "EraRAG：面向增长型语料库的高效增量检索增强生成", "tldr": "EraRAG 是一种新的图增强检索生成 (Graph-RAG) 框架，通过引入高效的增量更新机制，解决了现有系统在动态语料库中重建图的昂贵问题，显著提升了更新效率和准确性。", "motivation": "现有的图增强检索生成 (Graph-RAG) 方法通常假设语料库是静态的，当有新文档到来时，需要昂贵的完整图重建，这限制了它们在动态、不断演变环境中的可扩展性。", "method": "引入了一种名为 EraRAG 的新型多层图增强检索生成 (Graph-RAG) 框架。该方法利用基于超平面的局部敏感哈希 (LSH) 将原始语料库分区并组织成层次图结构，从而实现新数据的高效局部插入，而不会破坏现有拓扑结构，消除了再训练或昂贵重新计算的需求。", "result": "在大规模基准测试中，EraRAG 与现有 Graph-RAG 系统相比，更新时间与令牌消耗减少了一个数量级，同时提供了卓越的准确性性能。", "conclusion": "这项工作为必须在持续增长的语料库上运行的 RAG 系统提供了一条实用的前进道路，弥合了检索效率和适应性之间的差距。", "translation": "图增强检索生成 (Graph-RAG) 通过对外部语料库进行结构化检索来增强大型语言模型 (LLM)。然而，现有方法通常假设语料库是静态的，当有新文档到来时，需要昂贵的完整图重建，这限制了它们在动态、不断演变环境中的可扩展性。为了解决这些限制，我们引入了 EraRAG，一种新型多层 Graph-RAG 框架，它支持高效和可扩展的动态更新。我们的方法利用基于超平面的局部敏感哈希 (LSH) 将原始语料库分区并组织成层次图结构，从而实现新数据的高效局部插入，而不会破坏现有拓扑结构。这种设计消除了再训练或昂贵重新计算的需求，同时保持了高检索准确性和低延迟。大规模基准测试的实验表明，与现有 Graph-RAG 系统相比，EraRAG 在更新时间与令牌消耗方面实现了高达一个数量级的减少，同时提供了卓越的准确性性能。这项工作为必须在持续增长的语料库上运行的 RAG 系统提供了一条实用的前进道路，弥合了检索效率和适应性之间的差距。我们的代码和数据可在 https://github.com/EverM0re/EraRAG-Official 获取。", "summary": "EraRAG 是一种创新的多层图增强检索生成 (Graph-RAG) 框架，旨在解决现有 Graph-RAG 系统在动态增长语料库中效率低下的问题。它通过利用基于超平面的局部敏感哈希 (LSH) 构建层次图结构，实现了新数据的局部高效插入，无需昂贵的图重建或再训练。实验证明，EraRAG 在更新效率、令牌消耗和检索准确性方面均显著优于现有系统，为在不断扩展的语料库上运行的 RAG 系统提供了实用的解决方案。", "keywords": "Retrieval Augmented Generation, Graph-RAG, Incremental Updates, Locality-Sensitive Hashing, Dynamic Corpora", "comments": "这篇论文的创新点在于提出了一个支持高效增量更新的 Graph-RAG 框架，通过引入 LSH 和分层图结构，解决了传统 Graph-RAG 在动态语料库中重建成本高昂的痛点。这对于需要处理持续增长数据的实际应用场景具有重要意义，极大地提升了 RAG 系统的实用性和可扩展性。"}}
{"id": "2506.20821", "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "comment": "Preprint Copy", "pdf_url": "http://arxiv.org/pdf/2506.20821v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20821v1", "AI": {"title_translation": "MultiFinRAG：一个用于金融问答的优化多模态检索增强生成（RAG）框架", "tldr": "MultiFinRAG是一个为金融问答设计的多模态RAG框架，通过优化多模态信息提取和检索策略，在商品硬件上比ChatGPT-4o在复杂金融问答任务上提高了19%的准确率。", "motivation": "金融文档（如10-K、10-Q和投资者演示）篇幅巨大且包含文本、表格和图像等多种模态，回答这些内容的问题需要跨模态联合推理。传统的大型语言模型（LLMs）和检索增强生成（RAG）管道因令牌限制、布局丢失和碎片化的跨模态上下文而难以应对。", "method": "MultiFinRAG首先通过将表格和图像分组并发送到轻量级、量化的开源多模态LLM进行多模态提取，生成结构化JSON输出和简洁文本摘要。然后，这些输出与叙述性文本一起通过模态感知相似性阈值进行嵌入和索引，以实现精确检索。最后，采用分层回退策略，根据需要动态地从仅文本上下文升级到文本+表格+图像上下文，从而实现跨模态推理并减少不相关上下文。", "result": "MultiFinRAG在包含文本、表格、图像和组合多模态推理的复杂金融问答任务上，即使在商品硬件上运行，也比ChatGPT-4o（免费版）的准确率高出19个百分点。", "conclusion": "MultiFinRAG通过其优化的多模态信息处理和检索策略，显著提升了金融领域复杂问答的准确性，超越了现有先进模型，证明了其在实际应用中的有效性。", "translation": "金融文档——例如10-K、10-Q和投资者演示——长达数百页，并结合了多种模态，包括密集的叙述性文本、结构化表格和复杂的图形。对这些内容进行问题回答通常需要跨模态的联合推理，这由于令牌限制、布局丢失和碎片化的跨模态上下文而使传统的大型语言模型（LLMs）和检索增强生成（RAG）管道面临压力。我们引入了MultiFinRAG，一个专为金融问答而构建的检索增强生成框架。MultiFinRAG首先通过将表格和图形图像分批并发送到轻量级、量化的开源多模态LLM进行多模态提取，生成结构化的JSON输出和简洁的文本摘要。这些输出与叙述性文本一起，通过模态感知相似性阈值进行嵌入和索引，以实现精确检索。然后，当需要时，分层回退策略会动态地从仅文本上下文升级到文本+表格+图像上下文，从而实现跨模态推理，同时减少不相关的上下文。尽管在商品硬件上运行，MultiFinRAG在涉及文本、表格、图像和组合多模态推理的复杂金融问答任务上，比ChatGPT-4o（免费版）的准确率高出19个百分点。", "summary": "MultiFinRAG是一个针对金融问答优化的多模态检索增强生成（RAG）框架。它旨在解决传统LLMs和RAG在处理包含文本、表格和图像的复杂金融文档时面临的挑战，如令牌限制和跨模态上下文碎片化。MultiFinRAG通过轻量级多模态LLM进行多模态信息提取，生成结构化JSON和文本摘要，并结合叙述文本进行模态感知索引。其分层回退检索策略允许根据需要动态整合跨模态上下文。实验结果表明，MultiFinRAG在商品硬件上运行，在复杂金融问答任务上比ChatGPT-4o的准确率高出19个百分点。", "keywords": "MultiFinRAG, 多模态RAG, 金融问答, 检索增强生成, 跨模态推理", "comments": "MultiFinRAG的创新之处在于其专门为金融领域设计的多模态处理和检索策略，有效解决了金融文档的复杂性问题。通过结合轻量级多模态LLM进行信息提取和分层检索，它显著提高了跨模态推理能力。该框架在商品硬件上实现了优于ChatGPT-4o的性能，凸显了其在实际部署中的潜力和成本效益。"}}
{"id": "2506.21512", "title": "Assessing an evolutionary search engine for small language models, prompts, and evaluation metrics", "authors": ["Cláudio Lúcio do Val Lopes", "Lucca Machado"], "summary": "The concurrent optimization of language models and instructional prompts\npresents a significant challenge for deploying efficient and effective AI\nsystems, particularly when balancing performance against computational costs\nlike token usage. This paper introduces and assesses a bi-objective\nevolutionary search engine designed to navigate this complex space, focusing\nspecifically on Small Language Models (SLMs). We employ the NSGA-II algorithm\nand prompt grammar to simultaneously optimize for task accuracy and token\nefficiency across some reasoning tasks. Our results successfully identify\ndiverse, high-performing model-prompt combinations, quantitatively revealing\nthe critical trade-off between the two objectives. This research highlights\ntask-specific affinities between particular SLMs and prompt structures (e.g.,\ninstructions, context, chain of thought). The generated practical Pareto fronts\noffer decision-makers a portfolio of optimized solutions adaptable to their\nspecific constraints. This automated approach moves beyond traditional manual\ntuning, providing a foundational framework for discovering effective human-AI\ninteraction patterns.", "comment": "14 pages, 1 figure, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.21512v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21512v1", "AI": {"title_translation": "评估一种用于小型语言模型、提示和评估指标的进化搜索引擎", "tldr": "本文介绍并评估了一个双目标进化搜索引擎，用于同时优化小型语言模型（SLMs）和提示，以提高任务准确性和令牌效率，并成功识别出高性能的模型-提示组合。", "motivation": "语言模型和指令提示的同步优化在部署高效和有效的AI系统时面临重大挑战，尤其是在平衡性能与计算成本（如令牌使用）时。", "method": "本文引入并评估了一个双目标进化搜索引擎，专门针对小型语言模型（SLMs）。该研究采用NSGA-II算法和提示语法，在推理任务中同时优化任务准确性和令牌效率。", "result": "结果成功识别出多样化的高性能模型-提示组合，并定量揭示了两个目标之间的关键权衡。研究还强调了特定SLM与提示结构（例如指令、上下文、思维链）之间的任务特异性亲和力。生成的实用帕累托前沿为决策者提供了可根据其特定约束调整的优化解决方案组合。", "conclusion": "这种自动化方法超越了传统的手动调优，为发现有效的人机交互模式提供了一个基础框架。", "translation": "语言模型和指令提示的同步优化在部署高效和有效的AI系统时面临重大挑战，尤其是在平衡性能与计算成本（如令牌使用）时。本文介绍并评估了一个双目标进化搜索引擎，旨在驾驭这一复杂空间，特别关注小型语言模型（SLMs）。我们采用NSGA-II算法和提示语法，在一些推理任务中同时优化任务准确性和令牌效率。我们的结果成功识别出多样化的高性能模型-提示组合，定量揭示了两个目标之间的关键权衡。这项研究强调了特定SLM与提示结构（例如指令、上下文、思维链）之间的任务特异性亲和力。生成的实用帕累度前沿为决策者提供了可根据其特定约束调整的优化解决方案组合。这种自动化方法超越了传统的手动调优，为发现有效的人机交互模式提供了一个基础框架。", "summary": "本研究提出并评估了一种双目标进化搜索引擎，旨在解决小型语言模型（SLMs）和提示的同步优化难题，以平衡任务准确性和令牌效率。通过NSGA-II算法和提示语法，该引擎成功发现了多样化的高性能模型-提示组合，并揭示了性能与计算成本之间的权衡。该方法为决策者提供了优化的解决方案组合，并为自动化发现有效的人机交互模式奠定了基础，超越了传统的手动调优。", "keywords": "进化搜索, 小型语言模型, 提示优化, NSGA-II, 令牌效率", "comments": "该论文的创新之处在于引入了双目标进化搜索引擎（使用NSGA-II算法和提示语法）来自动化优化小型语言模型和提示，以同时兼顾性能和效率。这为AI系统部署提供了一个更系统和高效的方法，避免了耗时的人工调优。其重要性在于为发现有效的人机交互模式提供了一个基础框架，并为决策者提供了实用的帕累托前沿，以适应不同的约束。"}}
{"id": "2506.21269", "title": "Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou", "authors": ["Pengfei Fan", "Yuli Zhang", "Xinheng Wang", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Shangbo Wang"], "summary": "This study presents and publicly releases the Suzhou Urban Road Acoustic\nDataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive\ndata-acquisition protocols and annotation guidelines to ensure transparency and\nreproducibility of the experimental workflow. To model the coupling between\nvehicular noise and driving speed, we propose a bimodal-feature-fusion deep\nconvolutional neural network (BMCNN). During preprocessing, an adaptive\ndenoising and normalization strategy is applied to suppress environmental\nbackground interference; in the network architecture, parallel branches extract\nMel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,\nwhich are subsequently fused via a cross-modal attention mechanism in the\nintermediate feature space to fully exploit time-frequency information.\nExperimental results demonstrate that BMCNN achieves a classification accuracy\nof 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic\ndataset. Ablation studies and robustness tests on the Suzhou dataset further\nvalidate the contributions of each module to performance improvement and\noverfitting mitigation. The proposed acoustics-based speed classification\nmethod can be integrated into smart-city traffic management systems for\nreal-time noise monitoring and speed estimation, thereby optimizing traffic\nflow control, reducing roadside noise pollution, and supporting sustainable\nurban planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21269v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21269v1", "AI": {"title_translation": "整合车辆声学数据以增强城市交通管理：以苏州为例的速度分类研究", "tldr": "本研究发布了苏州城市道路声学数据集，并提出了一种双模态特征融合深度卷积神经网络（BMCNN）用于车辆速度分类，旨在通过声学数据实现智能交通管理和噪声监测。", "motivation": "为了更好地理解车辆噪声与驾驶速度之间的关系，并利用声学数据优化交通流控制、减少路边噪声污染以及支持可持续城市规划，本研究旨在开发一种基于声学的车辆速度分类方法。", "method": "本研究公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一个双模态特征融合深度卷积神经网络（BMCNN）。在预处理阶段，应用了自适应去噪和归一化策略。在网络架构中，并行分支提取梅尔频率倒谱系数（MFCCs）和小波包能量特征，并通过跨模态注意力机制在中间特征空间进行融合，以充分利用时频信息。", "result": "BMCNN在SZUR-Acoustic数据集上实现了87.56%的分类准确率，在公共IDMT-Traffic数据集上达到了96.28%的准确率。消融研究和鲁棒性测试进一步验证了各模块对性能提升和过拟合缓解的贡献。", "conclusion": "所提出的基于声学的速度分类方法可集成到智慧城市交通管理系统中，用于实时噪声监测和速度估计，从而优化交通流控制，减少路边噪声污染，并支持可持续的城市规划。", "translation": "本研究介绍并公开发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset），该数据集附有全面的数据采集协议和注释指南，以确保实验工作流程的透明度和可复现性。为了模拟车辆噪声与驾驶速度之间的耦合关系，我们提出了一种双模态特征融合深度卷积神经网络（BMCNN）。在预处理阶段，应用了自适应去噪和归一化策略来抑制环境背景干扰；在网络架构中，并行分支提取梅尔频率倒谱系数（MFCCs）和小波包能量特征，随后通过中间特征空间中的跨模态注意力机制进行融合，以充分利用时频信息。实验结果表明，BMCNN在SZUR-Acoustic数据集上实现了87.56%的分类准确率，在公共IDMT-Traffic数据集上达到了96.28%。对苏州数据集进行的消融研究和鲁棒性测试进一步验证了每个模块对性能改进和过拟合缓解的贡献。所提出的基于声学的速度分类方法可以集成到智慧城市交通管理系统中，用于实时噪声监测和速度估计，从而优化交通流控制，减少路边噪声污染，并支持可持续的城市规划。", "summary": "本研究发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种名为BMCNN的双模态特征融合深度卷积神经网络，用于基于车辆噪声的驾驶速度分类。BMCNN通过自适应去噪预处理，并结合MFCCs和小波包能量特征，利用跨模态注意力机制进行信息融合。实验证明，BMCNN在自建和公共数据集上均表现出高准确性，并通过消融研究验证了其模块的有效性。该方法有望应用于智慧城市交通管理，实现实时噪声监测和速度估计，从而优化交通流、减少噪声污染并支持城市可持续发展。", "keywords": "车辆声学数据, 速度分类, 深度学习, 交通管理, 苏州", "comments": "本文通过构建并公开新的数据集，为车辆声学研究提供了宝贵资源。所提出的BMCNN模型结合了多模态特征和注意力机制，有效提升了速度分类的准确性和鲁棒性，并在实际应用中展现了优化城市交通和环境的潜力。"}}
{"id": "2506.20940", "title": "Two-dimensional greedy randomized Kaczmarz methods for solving large-scale linear systems", "authors": ["Tao Li", "Meng-Long Xiao", "Xin-Fang Zhang"], "summary": "In this paper, we consider a novel two-dimensional randomized Kaczmarz method\nand its improved version with simple random sampling, which chooses two active\nrows with probability proportional to the square of their cross-product-like\nconstant, for solving large-scale linear systems. From the greedy selection\nstrategy with grasping two larger entries of the residual vector at each\niteration, we then devise a two-dimensional greedy randomized Kaczmarz method.\nTo improve the above methods further, motivated by the semi-randomized Kaczmarz\nmethod and Chebyshev's law of large numbers, we propose a two-dimensional\nsemi-randomized Kaczmarz method and its modified version with simple random\nsampling, which is particularly advantageous for big data problems.\nTheoretically, we prove that the proposed methods converge to the unique\nleast-norm solution of the consistent linear systems. Numerical results on some\npractical applications illustrate the superiority of the proposed methods\ncompared with some existing ones in terms of computing time.", "comment": "arXiv admin note: text overlap with arXiv:2506.16106", "pdf_url": "http://arxiv.org/pdf/2506.20940v1", "categories": ["math.NA", "cs.NA", "65F10, 65F20, 94A08"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20940v1", "AI": {"title_translation": "求解大规模线性系统的二维贪婪随机Kaczmarz方法", "tldr": "本文提出并分析了多种二维Kaczmarz方法，包括贪婪随机和半随机版本，用于高效求解大规模线性系统，并证明了其收敛性及数值优越性。", "motivation": "为了更有效地求解大规模线性系统，并改进现有Kaczmarz方法的性能，特别是针对大数据问题，本文提出了新的二维随机Kaczmarz方法及其改进版本。", "method": "本文提出了多种二维Kaczmarz方法：1. 二维随机Kaczmarz方法及其基于简单随机采样的改进版本，通过选择与交叉积常数平方成比例的概率选择两行。2. 二维贪婪随机Kaczmarz方法，通过在每次迭代中抓住残差向量的两个较大条目进行贪婪选择。3. 二维半随机Kaczmarz方法及其基于简单随机采样的修改版本，灵感来源于半随机Kaczmarz方法和切比雪夫大数定律。", "result": "理论上，所提出的方法被证明收敛到一致线性系统的唯一最小范数解。数值结果表明，在计算时间方面，所提出的方法优于一些现有方法，特别适用于实际应用。", "conclusion": "本文提出的二维贪婪随机Kaczmarz方法及其变体，通过理论分析和数值实验，证明了其在求解大规模线性系统方面的收敛性和计算效率优势，尤其适用于大数据问题。", "translation": "在本文中，我们考虑了一种新颖的二维随机Kaczmarz方法及其使用简单随机采样的改进版本，该方法以与其交叉积类常数平方成比例的概率选择两个活跃行，用于求解大规模线性系统。从每次迭代中抓住残差向量的两个较大条目的贪婪选择策略出发，我们随后设计了一种二维贪婪随机Kaczmarz方法。为了进一步改进上述方法，受半随机Kaczmarz方法和切比雪夫大数定律的启发，我们提出了一种二维半随机Kaczmarz方法及其使用简单随机采样的修改版本，这对于大数据问题尤其有利。理论上，我们证明了所提出的方法收敛到一致线性系统的唯一最小范数解。在一些实际应用上的数值结果表明，所提出的方法在计算时间方面优于一些现有方法。", "summary": "本文提出并分析了一系列用于求解大规模线性系统的新型二维Kaczmarz方法。这些方法包括二维随机Kaczmarz、二维贪婪随机Kaczmarz和二维半随机Kaczmarz及其变体，它们通过选择两行或利用贪婪策略来加速收敛。理论上，论文证明了这些方法收敛到最小范数解。数值实验结果表明，与现有方法相比，这些新方法在计算时间上具有显著优势，特别适用于大数据问题。", "keywords": "二维Kaczmarz方法, 贪婪随机, 半随机, 大规模线性系统, 收敛性", "comments": "本文的创新点在于将Kaczmarz方法扩展到二维选择策略，并结合了贪婪和半随机的思想，这对于加速大规模线性系统的求解具有重要意义。特别地，其对大数据问题的适用性提升了方法的实用价值。理论收敛性证明和数值优越性验证使得该研究成果具有坚实的理论基础和实际应用潜力。"}}
{"id": "2506.21069", "title": "TEMPEST-LoRa: Cross-Technology Covert Communication", "authors": ["Xieyang Sun", "Yuanqing Zheng", "Wei Xi", "Zuhao Chen", "Zhizhen Chen", "Han Hao", "Zhiping Jiang", "Sheng Zhong"], "summary": "Electromagnetic (EM) covert channels pose significant threats to computer and\ncommunications security in air-gapped networks. Previous works exploit EM\nradiation from various components (e.g., video cables, memory buses, CPUs) to\nsecretly send sensitive information. These approaches typically require the\nattacker to deploy highly specialized receivers near the victim, which limits\ntheir real-world impact. This paper reports a new EM covert channel,\nTEMPEST-LoRa, that builds on Cross-Technology Covert Communication (CTCC),\nwhich could allow attackers to covertly transmit EM-modulated secret data from\nair-gapped networks to widely deployed operational LoRa receivers from afar. We\nreveal the potential risk and demonstrate the feasibility of CTCC by tackling\npractical challenges involved in manipulating video cables to precisely\ngenerate the EM leakage that could readily be received by third-party\ncommercial LoRa nodes/gateways. Experiment results show that attackers can\nreliably decode secret data modulated by the EM leakage from a video cable at a\nmaximum distance of 87.5m or a rate of 21.6 kbps. We note that the secret data\ntransmission can be performed with monitors turned off (therefore covertly).", "comment": "15 pages, 19 figures, and this paper has been accepted to ACM CCS\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21069v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21069v1", "AI": {"title_translation": "TEMPEST-LoRa：跨技术隐蔽通信", "tldr": "本文提出了一种名为TEMPEST-LoRa的跨技术隐蔽通信方法，通过操纵视频线缆产生的电磁泄漏，将气隙网络中的秘密数据远距离传输到商用LoRa接收器，揭示了潜在的安全风险。", "motivation": "电磁（EM）隐蔽信道对气隙网络中的计算机和通信安全构成重大威胁。以往的工作利用各种组件（如视频线缆、内存总线、CPU）的电磁辐射秘密发送敏感信息，但这些方法通常要求攻击者在受害者附近部署高度专业化的接收器，这限制了它们的实际影响。本文旨在克服这一限制，展示一种更具实际影响力的电磁隐蔽信道。", "method": "本文提出了一种新的电磁隐蔽信道TEMPEST-LoRa，其基于跨技术隐蔽通信（CTCC）。该方法通过操纵视频线缆精确生成电磁泄漏，这些泄漏可以被广泛部署的商用LoRa接收器从远处接收，从而实现从气隙网络中隐蔽传输电磁调制秘密数据。", "result": "实验结果表明，攻击者可以可靠地解码来自视频线缆电磁泄漏调制的秘密数据，最大距离可达87.5米，或传输速率达到21.6 kbps。此外，秘密数据传输可以在显示器关闭的情况下进行，从而实现隐蔽性。", "conclusion": "本文成功揭示了利用跨技术隐蔽通信（CTCC）将气隙网络中的数据通过电磁泄漏传输到商用LoRa接收器的潜在风险和可行性，证明了这种新型隐蔽信道的实用性和有效性。", "translation": "电磁（EM）隐蔽信道对气隙网络中的计算机和通信安全构成重大威胁。以往的工作利用各种组件（如视频线缆、内存总线、CPU）的电磁辐射秘密发送敏感信息。这些方法通常要求攻击者在受害者附近部署高度专业化的接收器，这限制了它们的实际影响。本文报告了一种新的电磁隐蔽信道TEMPEST-LoRa，它建立在跨技术隐蔽通信（CTCC）的基础上，可以使攻击者将电磁调制的秘密数据从气隙网络隐蔽地传输到远处广泛部署的LoRa接收器。我们通过解决操纵视频线缆精确生成电磁泄漏所涉及的实际挑战，揭示了CTCC的潜在风险并证明了其可行性，这些泄漏可以很容易地被第三方商用LoRa节点/网关接收。实验结果表明，攻击者可以可靠地解码来自视频线缆电磁泄漏调制的秘密数据，最大距离可达87.5米，或传输速率达到21.6 kbps。我们注意到秘密数据传输可以在显示器关闭的情况下进行（因此是隐蔽的）。", "summary": "本文介绍了一种名为TEMPEST-LoRa的新型电磁隐蔽信道，该信道利用跨技术隐蔽通信（CTCC）原理。与传统方法需要专业接收器不同，TEMPEST-LoRa通过操纵视频线缆产生电磁泄漏，使商用LoRa接收器能够远距离接收来自气隙网络的秘密数据。实验证明，该方法在最大87.5米距离或21.6 kbps速率下可可靠传输数据，且可在显示器关闭时进行，揭示了气隙网络通信中潜在的严重安全威胁。", "keywords": "TEMPEST, LoRa, 隐蔽通信, 跨技术, 气隙网络", "comments": "该研究的创新之处在于利用了普遍部署的LoRa技术作为接收端，并通过操纵常见的视频线缆来产生电磁泄漏，克服了以往电磁隐蔽信道对特殊接收设备和近距离传输的限制。这使得隐蔽通信更具实际威胁性，对气隙网络的安全防护提出了新的挑战。其能够远距离、高速率传输数据且能在显示器关闭时进行，进一步凸显了其潜在的危害性。"}}
{"id": "2506.21014", "title": "Boosting Vulnerability Detection with Inter-function Multilateral Association Insights", "authors": ["Shaojian Qiu", "Mengyang Huang", "Jiahao Cheng"], "summary": "Vulnerability detection is a crucial yet challenging technique for ensuring\nthe security of software systems. Currently, most deep learning-based\nvulnerability detection methods focus on stand-alone functions, neglecting the\ncomplex inter-function interrelations, particularly the multilateral\nassociations. This oversight can fail to detect vulnerabilities in these\ninterrelations. To address this gap, we present an Inter-Function Multilateral\nAssociation analysis framework for Vulnerability Detection (IFMA-VD). The\ncornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and\nutilizing hyperedge convolution to extract multilateral association features.\nSpecifically, we first parse functions into a code property graph to generate\nintra-function features. Following this, we construct a code behavior\nhypergraph by segmenting the program dependency graph to isolate and encode\nbehavioral features into hyperedges. Finally, we utilize a hypergraph network\nto capture the multilateral association knowledge for augmenting vulnerability\ndetection. We evaluate IFMA-VD on three widely used vulnerability datasets and\ndemonstrate improvements in F-measure and Recall compared to baseline methods.\nAdditionally, we illustrate that multilateral association features can boost\ncode feature representation and validate the effectiveness of IFMA-VD on\nreal-world datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21014v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21014v1", "AI": {"title_translation": "通过函数间多边关联洞察提升漏洞检测", "tldr": "提出IFMA-VD框架，通过构建代码行为超图并利用超边卷积来提取多边关联特征，从而提升漏洞检测能力。", "motivation": "当前的深度学习漏洞检测方法主要关注独立函数，忽略了复杂的函数间相互关系，特别是多边关联，这可能导致无法检测到这些关系中的漏洞。", "method": "提出IFMA-VD框架，其核心是构建代码行为超图并利用超边卷积来提取多边关联特征。具体步骤包括：首先将函数解析为代码属性图以生成函数内特征；然后通过分割程序依赖图构建代码行为超图，将行为特征编码到超边中；最后利用超图网络捕获多边关联知识以增强漏洞检测。", "result": "在三个广泛使用的漏洞数据集上评估了IFMA-VD，与基线方法相比，F-measure和Recall有所提高。此外，研究表明多边关联特征可以增强代码特征表示，并在真实世界数据集上验证了IFMA-VD的有效性。", "conclusion": "通过考虑函数间多边关联，IFMA-VD框架能够有效提升漏洞检测的性能，并增强代码特征表示。", "translation": "漏洞检测是确保软件系统安全的关键但具有挑战性的技术。目前，大多数基于深度学习的漏洞检测方法侧重于独立函数，忽略了复杂的函数间相互关系，特别是多边关联。这种疏忽可能导致无法检测到这些相互关系中的漏洞。为了解决这一差距，我们提出了一个用于漏洞检测的函数间多边关联分析框架（IFMA-VD）。IFMA-VD的基石在于构建代码行为超图并利用超边卷积来提取多边关联特征。具体来说，我们首先将函数解析为代码属性图以生成函数内特征。在此之后，我们通过分割程序依赖图来构建代码行为超图，以隔离并将行为特征编码到超边中。最后，我们利用超图网络捕获多边关联知识以增强漏洞检测。我们在三个广泛使用的漏洞数据集上评估了IFMA-VD，并展示了与基线方法相比，F-measure和Recall的改进。此外，我们说明了多边关联特征可以提升代码特征表示，并验证了IFMA-VD在真实世界数据集上的有效性。", "summary": "该论文提出了一个名为IFMA-VD的框架，旨在解决现有深度学习漏洞检测方法忽略函数间复杂多边关联的问题。IFMA-VD通过构建代码行为超图并利用超边卷积来提取这些多边关联特征，从而增强漏洞检测能力。实验结果表明，与基线方法相比，IFMA-VD在多个漏洞数据集上提高了F-measure和Recall，并能有效提升代码特征表示，验证了其在真实世界数据集上的有效性。", "keywords": "漏洞检测, 函数间关联, 超图, 超边卷积, 深度学习", "comments": "该论文的创新点在于引入了“函数间多边关联”的概念，并首次提出使用超图（特别是超边卷积）来建模和提取这种复杂关系，以增强漏洞检测。这克服了传统方法仅关注独立函数或简单二元关系的局限性，为漏洞检测领域提供了一个新颖且有效的视角。其方法论严谨，通过超图捕捉更丰富的上下文信息，具有重要的理论和实践意义。"}}
{"id": "2506.21319", "title": "Multimodal LLMs for Visualization Reconstruction and Understanding", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21319v1", "categories": ["cs.HC", "cs.CV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21319v1", "AI": {"title_translation": "用于可视化重建和理解的多模态大型语言模型", "tldr": "本文提出了一个新数据集并训练了专门的多模态LLM，以解决现有模型在理解可视化方面（特别是数据到视觉映射和结构化信息提取）的不足，并在数据提取和图表重建方面取得了显著改进。", "motivation": "可视化对于数据交流至关重要，但理解它们需要同时理解视觉元素和底层数据关系。当前的多模态大型模型在自然图像理解方面表现出色，但由于无法解码数据到视觉的映射规则和提取结构化信息，因此在理解可视化方面存在困难。", "method": "为了解决这些挑战，本文提出了一个新颖的数据集，并训练了专门用于理解可视化的多模态可视化LLM。该方法将图表图像与其对应的矢量化表示、编码方案和数据特征相结合。提出的矢量格式能够紧凑而准确地重建可视化内容。", "result": "实验结果表明，在数据提取准确性和图表重建质量方面都有显著改进。", "conclusion": "通过引入专门的数据集和训练方法，本文成功地提升了多模态LLMs在可视化重建和理解方面的能力，解决了现有模型在处理复杂数据可视化时的局限性。", "translation": "可视化对于数据交流至关重要，但理解它们需要同时理解视觉元素和底层数据关系。当前的多模态大型模型在自然图像理解方面表现出色，但由于无法解码数据到视觉的映射规则和提取结构化信息，因此在理解可视化方面存在困难。为了解决这些挑战，我们提出了一个新颖的数据集并训练了专门用于理解可视化的多模态可视化LLMs。我们的方法将图表图像与其对应的矢量化表示、编码方案和数据特征相结合。所提出的矢量格式能够紧凑而准确地重建可视化内容。实验结果表明，在数据提取准确性和图表重建质量方面都有显著改进。", "summary": "本文提出了一种新颖的方法，通过构建专门的数据集并训练多模态可视化大型语言模型（LLMs），以解决现有通用多模态模型在理解和重建数据可视化方面的不足。该方法结合了图表图像、矢量化表示、编码方案和数据特征，并利用一种紧凑的矢量格式进行可视化内容重建。实验证明，该方法显著提高了数据提取的准确性和图表重建的质量。", "keywords": "多模态LLMs, 可视化理解, 数据提取, 图表重建, 矢量化表示", "comments": "这项研究的创新之处在于其专注于解决多模态LLMs在理解复杂数据可视化方面的特定挑战，特别是通过引入专门的数据集和利用矢量化表示来提高数据到视觉映射的解码能力和结构化信息提取的准确性。这对于提升AI在数据分析和解释领域的应用具有重要意义。"}}
{"id": "2506.20969", "title": "ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation", "authors": ["Shruti Bansal", "Wenshan Wang", "Yifei Liu", "Parv Maheshwari"], "summary": "Autonomous systems rely on sensors to estimate the environment around them.\nHowever, cameras, LiDARs, and RADARs have their own limitations. In nighttime\nor degraded environments such as fog, mist, or dust, thermal cameras can\nprovide valuable information regarding the presence of objects of interest due\nto their heat signature. They make it easy to identify humans and vehicles that\nare usually at higher temperatures compared to their surroundings. In this\npaper, we focus on the adaptation of thermal cameras for robotics and\nautomation, where the biggest hurdle is the lack of data. Several multi-modal\ndatasets are available for driving robotics research in tasks such as scene\nsegmentation, object detection, and depth estimation, which are the cornerstone\nof autonomous systems. However, they are found to be lacking in thermal\nimagery. Our paper proposes a solution to augment these datasets with synthetic\nthermal data to enable widespread and rapid adaptation of thermal cameras. We\nexplore the use of conditional diffusion models to convert existing RGB images\nto thermal images using self-attention to learn the thermal properties of\nreal-world objects.", "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.20969v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20969v1", "AI": {"title_translation": "ThermalDiffusion：用于自主导航的视觉到热像图像到图像转换", "tldr": "本文提出了一种使用条件扩散模型将现有RGB图像转换为热像图像的方法，以解决自主导航中热像数据缺乏的问题。", "motivation": "自主系统依赖传感器感知环境，但相机、激光雷达和雷达在夜间或恶劣环境下有局限性。热像仪能提供有价值的信息，但机器人和自动化领域面临热像数据缺乏的巨大障碍，现有多模态数据集缺乏热像数据。", "method": "提出了一种解决方案，通过合成热像数据来扩充数据集。具体利用条件扩散模型，通过自注意力学习真实物体的热特性，将现有RGB图像转换为热像图像。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "自主系统依赖传感器来估计周围环境。然而，相机、激光雷达和雷达都有其自身的局限性。在夜间或雾、霾或灰尘等恶劣环境中，热像仪由于物体的热特征，可以提供有关感兴趣物体存在的重要信息。它们使得识别通常比周围环境温度更高的人和车辆变得容易。在本文中，我们专注于热像仪在机器人和自动化领域的应用，其中最大的障碍是数据的缺乏。有几个多模态数据集可用于驾驶机器人研究，涉及场景分割、目标检测和深度估计等任务，这些是自主系统的基石。然而，这些数据集被发现缺乏热像数据。我们的论文提出了一种解决方案，通过合成热像数据来扩充这些数据集，以实现热像仪的广泛和快速适应。我们探索使用条件扩散模型，通过自注意力学习真实物体的热特性，将现有RGB图像转换为热像图像。", "summary": "该论文提出ThermalDiffusion，一种利用条件扩散模型将RGB图像转换为热像图像的方法，旨在解决自主导航领域热像数据稀缺的问题。通过合成热像数据来扩充现有数据集，从而促进热像仪在机器人和自动化系统中的广泛应用。", "keywords": "热像仪, 图像转换, 自主导航, 条件扩散模型, 数据增强", "comments": "该论文的关键创新在于提出了一个通过条件扩散模型生成合成热像数据的方法，有效解决了热像数据稀缺的痛点。这对于在恶劣环境下提升自主系统的感知能力具有重要意义，有助于推动热像仪在机器人和自动化领域的普及和应用。"}}
{"id": "2506.21426", "title": "Evolution and determinants of firm-level systemic risk in local production networks", "authors": ["Anna Mancini", "Balázs Lengyel", "Riccardo Di Clemente", "Giulio Cimini"], "summary": "Recent crises like the COVID-19 pandemic and geopolitical tensions have\nexposed vulnerabilities and caused disruptions of supply chains, leading to\nproduct shortages, increased costs, and economic instability. This has prompted\nincreasing efforts to assess systemic risk, namely the effects of firm\ndisruptions on entire economies. However, the ability of firms to react to\ncrises by rewiring their supply links has been largely overlooked, limiting our\nunderstanding of production networks resilience. Here we study dynamics and\ndeterminants of firm-level systemic risk in the Hungarian production network\nfrom 2015 to 2022. We use as benchmark a heuristic maximum entropy null model\nthat generates an ensemble of production networks at equilibrium, by preserving\nthe total input (demand) and output (supply) of each firm at the sector level.\nWe show that the fairly stable set of firms with highest systemic risk\nundergoes a structural change during COVID-19, as those enabling economic\nexchanges become key players in the economy -- a result which is not reproduced\nby the null model. Although the empirical systemic risk aligns well with the\nnull value until the onset of the pandemic, it becomes significantly smaller\nafterwards as the adaptive behavior of firms leads to a more resilient economy.\nFurthermore, firms' international trade volume (being a subject of disruption)\nbecomes a significant predictor of their systemic risk. However, international\nlinks cannot provide an unequivocal explanation for the observed trends, as\nimports and exports have opposing effects on local systemic risk through the\nsupply and demand channels.", "comment": "15 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21426v1", "categories": ["physics.soc-ph", "cs.SI", "econ.GN", "physics.data-an", "q-fin.EC", "q-fin.RM"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.21426v1", "AI": {"title_translation": "地方生产网络中企业层面系统性风险的演变与决定因素", "tldr": "本研究分析了匈牙利生产网络中企业层面的系统性风险及其决定因素，发现企业在危机中（如COVID-19）的适应性行为显著降低了系统性风险，提高了经济韧性，并指出国际贸易量是系统性风险的重要预测因子。", "motivation": "近期危机（如COVID-19大流行和地缘政治紧张）暴露了供应链的脆弱性，导致产品短缺、成本增加和经济不稳定。这促使人们越来越努力评估系统性风险。然而，企业通过重新连接供应环节来应对危机的能力在很大程度上被忽视，限制了我们对生产网络韧性的理解。", "method": "本研究调查了2015年至2022年匈牙利生产网络中企业层面系统性风险的动态和决定因素。研究使用启发式最大熵零模型作为基准，该模型通过保留每个企业在部门层面的总投入（需求）和产出（供应）来生成一组处于均衡状态的生产网络。", "result": "研究发现，系统性风险最高的企业集合在COVID-19期间发生了结构性变化，促成经济交易的企业成为经济中的关键参与者，这一结果无法通过零模型重现。尽管经验系统性风险在疫情爆发前与零模型值吻合良好，但此后显著减小，因为企业的适应性行为导致了更具韧性的经济。此外，企业的国际贸易量（作为中断的对象）成为其系统性风险的重要预测因子。然而，国际联系不能为观察到的趋势提供明确的解释，因为进口和出口通过供需渠道对当地系统性风险产生相反的影响。", "conclusion": "企业在危机中的适应性行为可以显著降低系统性风险，提高经济韧性。国际贸易量是企业系统性风险的重要预测因子，但其对系统性风险的影响复杂，进口和出口的作用可能相反。", "translation": "最近的危机，如COVID-19大流行和地缘政治紧张，暴露了供应链的脆弱性，并导致中断，从而引发产品短缺、成本增加和经济不稳定。这促使人们越来越努力评估系统性风险，即企业中断对整个经济体的影响。然而，企业通过重新连接供应环节来应对危机的能力在很大程度上被忽视，限制了我们对生产网络韧性的理解。本研究调查了2015年至2022年匈牙利生产网络中企业层面系统性风险的动态和决定因素。我们使用启发式最大熵零模型作为基准，该模型通过保留每个企业在部门层面的总投入（需求）和产出（供应）来生成一组处于均衡状态的生产网络。我们发现，系统性风险最高的、相对稳定的企业集合在COVID-19期间发生了结构性变化，因为那些促成经济交易的企业成为经济中的关键参与者——这一结果无法通过零模型重现。尽管经验系统性风险在疫情爆发前与零模型值吻合良好，但此后显著减小，因为企业的适应性行为导致了更具韧性的经济。此外，企业的国际贸易量（作为中断的对象）成为其系统性风险的重要预测因子。然而，国际联系不能为观察到的趋势提供明确的解释，因为进口和出口通过供需渠道对当地系统性风险产生相反的影响。", "summary": "本研究探讨了在危机背景下（如COVID-19）地方生产网络中企业层面系统性风险的演变和决定因素。通过分析2015年至2022年匈牙利生产网络数据，并使用最大熵零模型进行对比，研究发现企业在疫情期间展现出结构性适应行为，使得具有高系统性风险的企业集合发生变化，并且这种适应性行为显著降低了整体系统性风险，增强了经济韧性。此外，研究揭示了企业的国际贸易量是其系统性风险的重要预测因子，尽管进出口通过不同渠道对局部系统性风险产生复杂且相反的影响。", "keywords": "系统性风险, 生产网络, 供应链韧性, 企业适应性, COVID-19", "comments": "本研究的创新之处在于强调了企业在危机中通过重构供应链来适应环境的能力，并量化了这种适应性对系统性风险的影响。通过使用零模型作为基准，该研究能够突出实际生产网络中适应性行为的重要性，这对于理解和构建更具韧性的经济体系具有重要意义。同时，对国际贸易量作为预测因子的发现也为政策制定提供了新的视角。"}}
{"id": "2506.21230", "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "authors": ["Junhao Shi", "Zhaoye Fei", "Siyin Wang", "Qipeng Guo", "Jingjing Gong", "Xipeng QIu"], "summary": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks\nbut struggle with complex scenarios involving unfamiliar environments and\nmulti-step goals. Current approaches rely on environment-agnostic imitation\nlearning that disconnects instructions from environmental contexts, causing\nmodels to struggle with context-sensitive instructions and rely on\nsupplementary cues rather than visual reasoning during long-horizon\ninteractions. In this work, we propose World-Aware Planning Narrative\nEnhancement (WAP), a framework that infuses LVLMs with comprehensive\nenvironmental understanding through four cognitive capabilities (visual\nappearance modeling, spatial reasoning, functional abstraction, and syntactic\ngrounding) while developing and evaluating models using only raw visual\nobservations through curriculum learning. Evaluations on the EB-ALFRED\nbenchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a\n60.7 absolute improvement in task success rates, particularly in commonsense\nreasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced\nopen-source models outperform proprietary systems like GPT-4o and\nClaude-3.5-Sonnet by a large margin.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21230v1", "categories": ["cs.AI", "cs.RO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21230v1", "AI": {"title_translation": "世界感知规划叙事增强大型视觉-语言模型规划器", "tldr": "大型视觉-语言模型（LVLMs）在复杂具身规划任务中表现不佳；本文提出世界感知规划叙事增强（WAP）框架，通过注入全面的环境理解来显著提升LVLMs的性能，并在基准测试中超越了专有模型。", "motivation": "当前的大型视觉-语言模型（LVLMs）在处理涉及不熟悉环境和多步骤目标的复杂具身规划任务时面临困难。现有方法依赖于与环境无关的模仿学习，导致指令与环境背景脱节，模型难以处理上下文敏感的指令，并在长时间交互中过度依赖补充线索而非视觉推理。", "method": "本文提出了世界感知规划叙事增强（WAP）框架，通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法接地）为大型视觉-语言模型（LVLMs）注入全面的环境理解。模型仅使用原始视觉观测数据，并通过课程学习进行开发和评估。", "result": "在EB-ALFRED基准测试中取得了显著改进，Qwen2.5-VL在任务成功率上实现了60.7的绝对提升，尤其在常识推理（+60.0）和长时程规划（+70.0）方面。值得注意的是，本文增强的开源模型大幅优于GPT-4o和Claude-3.5-Sonnet等专有系统。", "conclusion": "世界感知规划叙事增强（WAP）框架通过整合全面的环境理解，显著提升了大型视觉-语言模型（LVLMs）在复杂具身规划任务中的性能，使其能够超越现有的先进专有模型。", "translation": "大型视觉-语言模型（LVLMs）在具身规划任务中展现出潜力，但在涉及不熟悉环境和多步骤目标的复杂场景中表现不佳。当前的方法依赖于与环境无关的模仿学习，这使得指令与环境上下文脱节，导致模型难以处理上下文敏感的指令，并在长时间交互中过度依赖补充线索而非视觉推理。在这项工作中，我们提出了世界感知规划叙事增强（WAP），这是一个通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法接地）为LVLMs注入全面环境理解的框架，同时仅通过课程学习使用原始视觉观测数据开发和评估模型。在EB-ALFRED基准测试上的评估显示出显著的改进，Qwen2.5-VL在任务成功率上实现了60.7的绝对提升，特别是在常识推理（+60.0）和长时程规划（+70.0）方面。值得注意的是，我们增强的开源模型大幅优于GPT-4o和Claude-3.5-Sonnet等专有系统。", "summary": "本文提出了一种名为世界感知规划叙事增强（WAP）的框架，旨在提升大型视觉-语言模型（LVLMs）在复杂具身规划任务中的表现。该框架通过注入视觉外观建模、空间推理、功能抽象和句法接地这四种认知能力，解决了现有LVLMs因环境无关学习而导致的上下文理解和视觉推理不足的问题。WAP利用课程学习，仅通过原始视觉观测数据进行模型开发和评估。在EB-ALFRED基准测试中，WAP展示了显著的性能提升，其中Qwen2.5-VL的任务成功率绝对提高了60.7%，特别是在常识推理和长时程规划方面表现突出。此外，增强后的开源模型甚至超越了GPT-4o和Claude-3.5-Sonnet等专有系统。", "keywords": "大型视觉-语言模型, 具身规划, 世界感知规划, 认知能力, 课程学习", "comments": "本文的创新之处在于，它通过明确注入“世界感知”的认知能力，超越了以往与环境无关的模仿学习范式，显著提升了大型视觉-语言模型在具身规划中的表现。其重要性体现在，不仅大幅提高了任务成功率，更令人瞩目的是，其增强的开源模型能够超越顶级的专有系统，这对于推动具身AI领域的发展和实际应用具有深远意义。"}}
{"id": "2506.21033", "title": "BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services", "authors": ["Zhaojiacheng Zhou", "Hongze Liu", "Shijing Yuan", "Hanning Zhang", "Jiong Lou", "Chentao Wu", "Jie Li"], "summary": "The hallucination problem of Large Language Models (LLMs) has increasingly\ndrawn attention. Augmenting LLMs with external knowledge is a promising\nsolution to address this issue. However, due to privacy and security concerns,\na vast amount of downstream task-related knowledge remains dispersed and\nisolated across various \"silos,\" making it difficult to access. To bridge this\nknowledge gap, we propose a blockchain-based external knowledge framework that\ncoordinates multiple knowledge silos to provide reliable foundational knowledge\nfor large model retrieval while ensuring data security. Technically, we distill\nknowledge from local data into prompts and execute transactions and records on\nthe blockchain. Additionally, we introduce a reputation mechanism and\ncross-validation to ensure knowledge quality and provide incentives for\nparticipation. Furthermore, we design a query generation framework that\nprovides a direct API interface for large model retrieval. To evaluate the\nperformance of our proposed framework, we conducted extensive experiments on\nvarious knowledge sources. The results demonstrate that the proposed framework\nachieves efficient LLM service knowledge sharing in blockchain environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21033v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21033v1", "AI": {"title_translation": "BLOCKS：区块链支持的跨孤岛知识共享，实现高效LLM服务", "tldr": "提出一个基于区块链的框架BLOCKS，解决LLM幻觉问题，通过协调分散的知识孤岛，安全高效地为LLM提供外部知识。", "motivation": "大型语言模型（LLM）存在幻觉问题，外部知识增强是解决方案。然而，由于隐私和安全问题，大量下游任务相关知识分散在各种“孤岛”中，难以访问。", "method": "提出一个区块链外部知识框架，BLOCKS。该框架将本地数据中的知识提炼成提示，并在区块链上执行交易和记录。此外，引入信誉机制和交叉验证以确保知识质量并激励参与，并设计了一个查询生成框架提供直接API接口用于大型模型检索。", "result": "实验结果表明，所提出的框架在区块链环境中实现了高效的LLM服务知识共享。", "conclusion": "该论文成功提出了一个基于区块链的框架BLOCKS，有效地解决了LLM的幻觉问题，通过安全、高效地协调分散的知识孤岛，为LLM提供了可靠的外部知识。", "translation": "大型语言模型（LLM）的幻觉问题日益受到关注。用外部知识增强LLM是解决此问题的一个有前景的方案。然而，由于隐私和安全问题，大量与下游任务相关的知识分散并孤立在各种“孤岛”中，难以访问。为了弥合这一知识鸿沟，我们提出了一个基于区块链的外部知识框架，该框架协调多个知识孤岛，为大型模型检索提供可靠的基础知识，同时确保数据安全。技术上，我们将本地数据中的知识提炼成提示，并在区块链上执行交易和记录。此外，我们引入了信誉机制和交叉验证，以确保知识质量并激励参与。再者，我们设计了一个查询生成框架，为大型模型检索提供直接的API接口。为了评估我们所提出框架的性能，我们对各种知识源进行了广泛的实验。结果表明，所提出的框架在区块链环境中实现了高效的LLM服务知识共享。", "summary": "本文提出了BLOCKS，一个基于区块链的外部知识框架，旨在解决大型语言模型（LLM）的幻觉问题。针对现有知识因隐私和安全顾虑而分散在不同“孤岛”的挑战，BLOCKS通过在区块链上记录提炼的知识提示、引入信誉机制和交叉验证以保证知识质量，并提供API接口实现LLM的知识检索。实验证明，该框架能有效促进区块链环境下LLM服务的高效知识共享。", "keywords": "区块链, 知识共享, 大型语言模型, 幻觉问题, 知识孤岛", "comments": "该论文创新性地将区块链技术应用于解决LLM的幻觉问题，通过构建去中心化的知识共享机制，有效克服了传统中心化方案在隐私和安全方面的障碍。其提出的信誉机制和交叉验证确保了知识的可靠性，而API接口则提升了LLM获取外部知识的便捷性。该框架为未来LLM的知识增强提供了一个有前景的方向，特别是在涉及敏感数据和多方协作的场景中具有重要意义。"}}
{"id": "2506.20832", "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "pdf_url": "http://arxiv.org/pdf/2506.20832v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20832v1", "AI": {"title_translation": "利用视觉-语言模型选择扩散模型生成的可靠超分辨率样本", "tldr": "本文提出一个利用视觉-语言模型（VLM）从扩散模型生成的超分辨率图像中选择最可靠样本的自动化框架，并通过结合语义、结构和伪影敏感性的新型可信度分数（TWS）进行验证。", "motivation": "回归型超分辨率（SR）模型在保真度和感知质量之间存在权衡，可能引入伪影，在信息关键应用中造成歧义。扩散模型虽然能生成多样化的SR图像，但从其生成的集合中选择最可靠的解决方案仍然是一个挑战。", "method": "本文引入一个鲁棒的自动化框架，通过利用视觉-语言模型（VLM，如BLIP-2、GPT-4o及其变体）的语义推理能力，评估扩散模型生成的SR样本的语义正确性、视觉质量和伪影存在。排名靠前的SR候选样本随后被集成以产生单一的可靠输出。为评估VLM选择的有效性，提出了一种新颖的可信度分数（TWS），该分数结合了CLIP嵌入的语义相似性、边缘图SSIM的结构完整性以及多级小波分解的伪影敏感性。", "result": "经验证明，TWS在模糊图像和自然图像中都与人类偏好高度相关。VLM引导的选择始终产生高TWS值。与未能反映信息保真度的传统指标（如PSNR、LPIPS）相比，该方法提供了一种原则性、可扩展且可推广的解决方案，用于解决扩散SR空间的不确定性。", "conclusion": "本研究通过使生成式超分辨率的输出与人类期望和语义正确性对齐，为其可信度设定了新的基准。", "translation": "超分辨率（SR）是一个不适定逆问题，对于给定的低分辨率图像，存在许多可行的解决方案。一方面，回归型SR模型旨在平衡保真度和感知质量以产生单一解决方案，但这种权衡常常引入伪影，在识别数字或字母等信息关键应用中造成歧义。另一方面，扩散模型生成多样化的SR图像，但从该集合中选择最可靠的解决方案仍然是一个挑战。本文引入了一个鲁棒的自动化框架，通过利用视觉-语言模型（VLM）的语义推理能力，从扩散模型生成的集合中识别最可靠的SR样本。具体来说，BLIP-2、GPT-4o及其变体等VLM被用于结构化查询，以评估语义正确性、视觉质量和伪影存在。然后，将排名靠前的SR候选样本进行集成，以经济高效的方式产生单一的可靠输出。为了严格评估VLM选择样本的有效性，我们提出了一种新颖的可信度分数（TWS）——一种混合度量，通过三个互补的组成部分量化SR可靠性：通过CLIP嵌入的语义相似性、使用边缘图SSIM的结构完整性以及通过多级小波分解的伪影敏感性。我们通过经验证明，TWS在模糊图像和自然图像中都与人类偏好高度相关，并且VLM引导的选择始终产生高TWS值。与PSNR、LPIPS等未能反映信息保真度的传统指标相比，我们的方法为解决扩散SR空间的不确定性提供了一种原则性、可扩展且可推广的解决方案。通过使输出与人类期望和语义正确性对齐，这项工作为生成式SR的可信度设定了新的基准。", "summary": "本文提出一个利用视觉-语言模型（VLM）从扩散模型生成的超分辨率（SR）图像中选择最可靠样本的自动化框架。针对回归型SR模型在保真度和感知质量间的权衡以及扩散模型选择可靠样本的挑战，研究者利用BLIP-2、GPT-4o等VLM评估SR样本的语义正确性、视觉质量和伪影，并集成优秀候选。为验证VLM选择的有效性，引入了结合语义相似性、结构完整性和伪影敏感性的新型可信度分数（TWS）。实验证明TWS与人类偏好高度相关，且VLM选择的样本具有高TWS值，优于传统指标，为生成式SR的可信度提供了可扩展的解决方案。", "keywords": "超分辨率, 扩散模型, 视觉-语言模型, 可信度分数, 图像质量评估", "comments": "这项工作创新性地将视觉-语言模型引入超分辨率领域，解决了扩散模型生成多样性样本后难以选择“最可靠”输出的关键问题。通过引入可信度分数TWS，为评估生成式SR的质量提供了一个更全面、更贴近人类感知的量化标准，克服了传统指标的局限性。其通用性和可扩展性使其在信息敏感型应用中具有重要意义。"}}
{"id": "2506.20729", "title": "Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset", "authors": ["Zhiqi Gao", "Tianyi Li", "Yurii Kvasiuk", "Sai Chaitanya Tadepalli", "Maja Rudolph", "Daniel J. H. Chung", "Frederic Sala", "Moritz Münchmeyer"], "summary": "Large language models (LLMs) have shown strong capabilities in complex\nreasoning, and test-time scaling techniques can enhance their performance with\ncomparably low cost. Many of these methods have been developed and evaluated on\nmathematical reasoning benchmarks such as AIME. This paper investigates whether\nthe lessons learned from these benchmarks generalize to the domain of advanced\ntheoretical physics. We evaluate a range of common test-time scaling methods on\nthe TPBench physics dataset and compare their effectiveness with results on\nAIME. To better leverage the structure of physics problems, we develop a novel,\nsymbolic weak-verifier framework to improve parallel scaling results. Our\nempirical results demonstrate that this method significantly outperforms\nexisting test-time scaling approaches on TPBench. We also evaluate our method\non AIME, confirming its effectiveness in solving advanced mathematical\nproblems. Our findings highlight the power of step-wise symbolic verification\nfor tackling complex scientific problems.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20729v1", "categories": ["cs.LG", "astro-ph.CO", "cs.AI", "hep-ph", "hep-th"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20729v1", "AI": {"title_translation": "理论物理中的测试时缩放技术——TPBench数据集上的方法比较", "tldr": "本文研究了大型语言模型（LLMs）在理论物理问题上的测试时缩放技术，并在TPBench数据集上比较了现有方法。论文引入了一种新颖的符号弱验证器框架，该框架在TPBench上显著优于现有方法，并在数学问题（AIME）上也表现出有效性。", "motivation": "大型语言模型（LLMs）在复杂推理方面表现出色，而测试时缩放技术能以较低成本提升其性能。许多此类方法已在数学推理基准（如AIME）上开发和评估。本文旨在探究这些经验是否能推广到高级理论物理领域，并提升LLMs在该领域的表现。", "method": "论文在TPBench物理数据集上评估了一系列常见的测试时缩放方法，并将其有效性与AIME上的结果进行比较。为更好地利用物理问题的结构，本文开发了一种新颖的、符号化的弱验证器框架，以改善并行缩放结果。该方法在TPBench和AIME上进行了实证评估。", "result": "所提出的新型符号弱验证器方法在TPBench数据集上显著优于现有的测试时缩放方法。该方法在AIME数据集上也得到了验证，证实了其在解决高级数学问题方面的有效性。", "conclusion": "研究结果强调了分步符号验证在解决复杂科学问题方面的强大能力。", "translation": "大型语言模型（LLMs）在复杂推理方面展现出强大的能力，而测试时缩放技术能以相对较低的成本提升其性能。许多此类方法已在AIME等数学推理基准上开发和评估。本文研究了从这些基准中学到的经验是否能推广到高级理论物理领域。我们评估了TPBench物理数据集上的一系列常见测试时缩放方法，并将其有效性与AIME上的结果进行比较。为了更好地利用物理问题的结构，我们开发了一种新颖的符号弱验证器框架，以改善并行缩放结果。我们的实证结果表明，该方法在TPBench上显著优于现有测试时缩放方法。我们还在AIME上评估了我们的方法，证实了其在解决高级数学问题方面的有效性。我们的发现强调了分步符号验证在解决复杂科学问题方面的强大能力。", "summary": "本文探讨了大型语言模型（LLMs）的测试时缩放技术从数学推理向高级理论物理领域的泛化能力，并使用TPBench数据集进行了评估。论文比较了现有方法，并提出了一种新颖的符号弱验证器框架。该框架利用物理问题的结构，经验证明在TPBench上显著优于现有方法，并在AIME数学基准上也表现出有效性。这项研究强调了分步符号验证对于解决复杂科学问题的有效性。", "keywords": "测试时缩放, 大型语言模型, 理论物理, 符号验证, TPBench", "comments": "本文的创新之处在于将通常用于数学推理的测试时缩放技术应用于理论物理领域，并引入了一种专门针对物理问题结构的新型符号弱验证器框架。该框架在TPBench上显著优于现有方法，并在AIME上同样表现出色，这表明其具有广泛的适用性和提升LLM在科学领域推理能力的重要性。对“分步符号验证”的关注是其关键洞察。"}}
{"id": "2506.21093", "title": "Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection", "authors": ["Li Fan", "Peng Wang", "Jing Yang", "Cong Shen"], "summary": "Transformers have shown potential in solving wireless communication problems,\nparticularly via in-context learning (ICL), where models adapt to new tasks\nthrough prompts without requiring model updates. However, prior ICL-based\nTransformer models rely on deep architectures with many layers to achieve\nsatisfactory performance, resulting in substantial storage and computational\ncosts. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a\nCoT-enhanced shallow Transformer framework for wireless symbol detection. By\nintroducing autoregressive latent reasoning steps within the hidden space,\nCHOOSE significantly improves the reasoning capacity of shallow models (1-2\nlayers) without increasing model depth. This design enables lightweight\nTransformers to achieve detection performance comparable to much deeper models,\nmaking them well-suited for deployment on resource-constrained mobile devices.\nExperimental results demonstrate that our approach outperforms conventional\nshallow Transformers and achieves performance comparable to that of deep\nTransformers, while maintaining storage and computational efficiency. This\nrepresents a promising direction for implementing Transformer-based algorithms\nin wireless receivers with limited computational resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21093v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21093v1", "AI": {"title_translation": "思维链增强型浅层Transformer用于无线符号检测", "tldr": "本文提出了CHOOSE，一种思维链增强型浅层（1-2层）Transformer，通过引入自回归潜在推理步骤，在无线符号检测中实现了与深层Transformer相当的性能，同时保持高效率，适用于资源受限设备。", "motivation": "此前的基于上下文学习（ICL）的Transformer模型依赖于深层架构才能达到满意性能，导致高昂的存储和计算成本，不适用于资源受限设备。", "method": "本文提出了CHOOSE（思维链符号检测），一个思维链增强型浅层Transformer框架，用于无线符号检测。通过在隐藏空间中引入自回归潜在推理步骤，CHOOSE显著提高了浅层模型（1-2层）的推理能力，而无需增加模型深度。", "result": "实验结果表明，CHOOSE优于传统的浅层Transformer，并实现了与深层Transformer相当的检测性能，同时保持了存储和计算效率。", "conclusion": "该研究为在计算资源有限的无线接收器中实现基于Transformer的算法提供了一个有前景的方向。", "translation": "Transformer在解决无线通信问题，特别是通过上下文学习（ICL）方面，展现了巨大潜力。ICL允许模型通过提示适应新任务，而无需更新模型。然而，此前基于ICL的Transformer模型依赖于多层深层架构才能达到满意性能，导致高昂的存储和计算成本。本文提出了CHOOSE（思维链符号检测），一个用于无线符号检测的思维链增强型浅层Transformer框架。通过在隐藏空间中引入自回归潜在推理步骤，CHOOSE显著提高了浅层模型（1-2层）的推理能力，而无需增加模型深度。这种设计使得轻量级Transformer能够实现与更深层模型相当的检测性能，非常适合部署在资源受限的移动设备上。实验结果表明，我们的方法优于传统的浅层Transformer，并实现了与深层Transformer相当的性能，同时保持了存储和计算效率。这为在计算资源有限的无线接收器中实现基于Transformer的算法提供了一个有前景的方向。", "summary": "本文介绍了CHOOSE，一种思维链增强型浅层Transformer，专为高效无线符号检测而设计。为解决深层Transformer的高计算成本问题，CHOOSE将自回归潜在推理步骤整合到浅层架构（1-2层）中，显著提升了其推理能力而未增加深度。这使得轻量级模型能够匹敌更深层模型的性能，适用于资源受限的移动设备。实验证实，CHOOSE的性能优于传统浅层Transformer，并与深层Transformer相当，同时保持了高效率。", "keywords": "无线符号检测, 浅层Transformer, 思维链, 上下文学习, 资源受限设备", "comments": "该论文的创新之处在于通过思维链增强，特别是引入自回归潜在推理步骤，使浅层Transformer能够达到深层Transformer的性能。这对于在资源受限的无线设备上部署先进模型至关重要，解决了重要的实际挑战。论文成功展示了一种平衡性能与计算效率的方法，为无线通信中的边缘AI开辟了新的可能性。"}}
{"id": "2506.21118", "title": "Courcelle's Theorem for Lipschitz Continuity", "authors": ["Tatsuya Gima", "Soh Kumabe", "Yuichi Yoshida"], "summary": "Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida\n(FOCS'23), measures the stability of an algorithm against small input\nperturbations. Algorithms with small Lipschitz continuity are desirable, as\nthey ensure reliable decision-making and reproducible scientific research.\nSeveral studies have proposed Lipschitz continuous algorithms for various\ncombinatorial optimization problems, but these algorithms are problem-specific,\nrequiring a separate design for each problem.\n  To address this issue, we provide the first algorithmic meta-theorem in the\nfield of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz\ncontinuous analogue of Courcelle's theorem, which offers Lipschitz continuous\nalgorithms for problems on bounded-treewidth graphs. Specifically, we consider\nthe problem of finding a vertex set in a graph that maximizes or minimizes the\ntotal weight, subject to constraints expressed in monadic second-order logic\n(MSO_2). We show that for any $\\varepsilon>0$, there exists a $(1\\pm\n\\varepsilon)$-approximation algorithm for the problem with a polylogarithmic\nLipschitz constant on bounded treewidth graphs. On such graphs, our result\noutperforms most existing Lipschitz continuous algorithms in terms of\napproximability and/or Lipschitz continuity. Further, we provide similar\nresults for problems on bounded-clique-width graphs subject to constraints\nexpressed in MSO_1. Additionally, we construct a Lipschitz continuous version\nof Baker's decomposition using our meta-theorem as a subroutine.", "comment": "ESA 2025, 27 pages", "pdf_url": "http://arxiv.org/pdf/2506.21118v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21118v1", "AI": {"title_translation": "Lipschitz连续性的Courcelle定理", "tldr": "提出了第一个针对Lipschitz连续算法的元定理，为有界树宽图上的MSO_2约束问题提供了近似算法，并构建了Baker分解的Lipschitz连续版本。", "motivation": "现有Lipschitz连续算法是针对特定问题的，需要为每个问题单独设计，缺乏通用性。", "method": "提出了Lipschitz连续算法领域的第一个算法元定理，可以看作是Courcelle定理的Lipschitz连续模拟。该方法用于解决有界树宽图上受MSO_2逻辑约束的顶点集问题，以及有界团宽图上受MSO_1约束的问题。", "result": "对于有界树宽图，对于任意ε>0，存在一个具有多对数Lipschitz常数的(1±ε)-近似算法，该算法在近似性和/或Lipschitz连续性方面优于大多数现有算法。此外，对于有界团宽图上受MSO_1约束的问题，也提供了类似的结果。另外，利用该元定理作为子程序，构建了Baker分解的Lipschitz连续版本。", "conclusion": "该研究通过引入一个通用的算法元定理，显著推进了Lipschitz连续算法领域的发展，解决了现有算法特异性的问题，并为特定图类上的优化问题提供了高性能的稳定算法。", "translation": "Kumabe和Yoshida（FOCS'23）引入的算法的Lipschitz连续性，衡量了算法对小输入扰动的稳定性。具有小Lipschitz连续性的算法是理想的，因为它们能确保可靠的决策和可重复的科学研究。一些研究已经为各种组合优化问题提出了Lipschitz连续算法，但这些算法是针对特定问题的，需要为每个问题单独设计。为了解决这个问题，我们提供了Lipschitz连续算法领域的第一个算法元定理。我们的结果可以看作是Courcelle定理的Lipschitz连续模拟，它为有界树宽图上的问题提供了Lipschitz连续算法。具体来说，我们考虑了在图中找到一个顶点集，该顶点集在满足单子二阶逻辑（MSO_2）表达的约束下，使总权重最大化或最小化的问题。我们表明，对于任意$\\varepsilon>0$，对于有界树宽图上的该问题，存在一个具有多对数Lipschitz常数的$(1\\pm \\varepsilon)$-近似算法。在这些图上，我们的结果在近似性和/或Lipschitz连续性方面优于大多数现有Lipschitz连续算法。此外，我们还为有界团宽图上受MSO_1约束的问题提供了类似的结果。另外，我们利用我们的元定理作为子程序，构建了Baker分解的Lipschitz连续版本。", "summary": "本文提出了Lipschitz连续算法领域的首个算法元定理，作为Courcelle定理的Lipschitz连续性类比。该元定理解决了现有Lipschitz连续算法特异性的问题，为有界树宽图上受MSO_2逻辑约束的顶点集优化问题提供了具有多对数Lipschitz常数的(1±ε)-近似算法，并在近似性和稳定性方面超越了现有方法。研究还为有界团宽图上受MSO_1约束的问题提供了类似结果，并利用该元定理构建了Baker分解的Lipschitz连续版本。", "keywords": "Lipschitz连续性, Courcelle定理, 元定理, 树宽有界图, 近似算法", "comments": "这项工作的重要性在于首次提出了Lipschitz连续算法的元定理，极大地提升了该领域算法设计的通用性。它将经典的Courcelle定理思想推广到稳定性分析，为在特定图结构（如树宽有界图）上设计鲁棒性好的近似算法提供了通用框架，对可重复科学和可靠决策具有重要意义。"}}
{"id": "2506.20987", "title": "Optimal Parameter Design for Power Electronic Converters Using a Probabilistic Learning-Based Stochastic Surrogate Model", "authors": ["Akash Mahajan", "Shivam Chaturvedi", "Srijita Das", "Wencong Su", "Van-Hai Bui"], "summary": "The selection of optimal design for power electronic converter parameters\ninvolves balancing efficiency and thermal constraints to ensure high\nperformance without compromising safety. This paper introduces a\nprobabilistic-learning-based stochastic surrogate modeling framework to address\nthis challenge and significantly reduce the time required during the design\nphase. The approach begins with a neural network classifier that evaluates the\nfeasibility of parameter configurations, effectively filtering out unsafe\nand/or impractical inputs. Subsequently, a probabilistic prediction model\nestimates the converter's efficiency and temperature while quantifying\nprediction uncertainty, providing both performance insights and reliability\nmetrics. Finally, a heuristic optimization-based model is employed to optimize\na multi-objective function that maximizes efficiency while adhering to thermal\nconstraints. The optimization process incorporates penalty terms to discourage\nsolutions that violate practical thresholds, ensuring actionable and realistic\nrecommendations. An advanced heuristic optimization method is used to find the\noptimal solution and is compared with several well-known search algorithms,\nincluding Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Simulated\nAnnealing (SA), Tabu-Search (TS), and Stochastic Hill Climbing (SHC). The\nresults demonstrate significant improvements in predictive accuracy and\noptimization outcomes, offering a robust solution for advancing power\nelectronics design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20987v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20987v1", "AI": {"title_translation": "基于概率学习随机代理模型的电力电子变换器最优参数设计", "tldr": "提出一种基于概率学习的随机代理建模框架，用于电力电子变换器参数优化，显著减少设计时间，提高预测精度和优化结果。", "motivation": "电力电子变换器参数选择需要在效率和热约束之间取得平衡，以确保高性能和安全性，同时设计阶段耗时较长。", "method": "该方法包含三个阶段：首先，使用神经网络分类器评估参数配置的可行性，过滤不安全/不切实际的输入；其次，使用概率预测模型估计变换器效率和温度，并量化预测不确定性；最后，采用启发式优化模型优化多目标函数，以最大化效率并遵守热约束，并加入惩罚项。此外，使用先进的启发式优化方法寻找最优解，并与遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）、禁忌搜索（TS）和随机爬山（SHC）等算法进行比较。", "result": "结果表明，该方法显著提高了预测精度和优化结果。", "conclusion": "该方法为推进电力电子设计提供了一个稳健的解决方案。", "translation": "电力电子变换器参数的最优设计选择涉及平衡效率和热约束，以确保高性能而不损害安全性。本文介绍了一种基于概率学习的随机代理建模框架，以解决这一挑战并显著减少设计阶段所需的时间。该方法首先使用神经网络分类器评估参数配置的可行性，有效过滤掉不安全和/或不切实际的输入。随后，概率预测模型估计变换器的效率和温度，同时量化预测不确定性，提供性能洞察和可靠性指标。最后，采用基于启发式优化的模型来优化多目标函数，该函数在遵守热约束的同时最大化效率。优化过程结合了惩罚项，以阻止违反实际阈值的解决方案，确保可操作和实际的建议。使用先进的启发式优化方法来寻找最优解，并与几种知名的搜索算法进行比较，包括遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）、禁忌搜索（TS）和随机爬山（SHC）。结果表明，预测精度和优化结果显著改善，为推进电力电子设计提供了稳健的解决方案。", "summary": "本文提出一种基于概率学习的随机代理建模框架，旨在优化电力电子变换器参数设计，以平衡效率和热约束并缩短设计时间。该框架通过神经网络分类器筛选可行配置，利用概率预测模型评估性能并量化不确定性，最后采用启发式优化方法解决多目标优化问题，以最大化效率并满足热约束。实验结果显示，该方法在预测精度和优化效果上均有显著提升，为电力电子设计提供了有效方案。", "keywords": "电力电子变换器, 参数优化, 概率学习, 随机代理模型, 多目标优化", "comments": "该论文的创新点在于结合了概率学习和随机代理模型来解决电力电子变换器参数设计的复杂多目标优化问题，特别是在处理不确定性和确保实际可行性方面。通过引入神经网络分类器过滤不切实际的输入和使用惩罚项，提高了优化结果的实用性。与多种现有优化算法的比较也增强了其方法的说服力。"}}
{"id": "2506.20863", "title": "Quantum-Accelerated Wireless Communications: Concepts, Connections, and Implications", "authors": ["Naoki Ishikawa", "Giuseppe Thadeu Freitas de Abreu", "Petar Popovski", "Robert W. Heath Jr"], "summary": "Quantum computing is poised to redefine the algorithmic foundations of\ncommunication systems. While quantum superposition and entanglement enable\nquadratic or exponential speedups for specific problems, identifying use cases\nwhere these advantages yield engineering benefits is, however, still\nnontrivial. This article presents the fundamentals of quantum computing in a\nstyle familiar to the communications society, outlining the current limits of\nfault-tolerant quantum computing and uncovering a mathematical harmony between\nquantum and wireless systems, which makes the topic more enticing to wireless\nresearchers. Based on a systematic review of pioneering and state-of-the-art\nstudies, we distill common design trends for the research and development of\nquantum-accelerated communication systems and highlight lessons learned. The\nkey insight is that classical heuristics can sharpen certain quantum\nparameters, underscoring the complementary strengths of classical and quantum\ncomputing. This article aims to catalyze interdisciplinary research at the\nfrontier of quantum information processing and future communication systems.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20863v1", "categories": ["eess.SP", "quant-ph"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20863v1", "AI": {"title_translation": "量子加速无线通信：概念、连接与启示", "tldr": "本文探讨了量子计算如何应用于无线通信系统，揭示了量子与经典计算的互补优势，旨在促进跨学科研究。", "motivation": "尽管量子计算在特定问题上能提供加速，但在通信系统中找到能产生工程效益的用例仍非易事。本文旨在向通信界介绍量子计算基础，揭示量子与无线系统间的数学和谐，并促进量子信息处理与未来通信系统间的跨学科研究。", "method": "本文以通信界熟悉的方式介绍了量子计算的基础知识，概述了容错量子计算的当前限制，并揭示了量子与无线系统之间的数学和谐。此外，通过系统回顾开创性和最先进的研究，提炼了量子加速通信系统研发的常见设计趋势，并强调了经验教训。", "result": "关键的见解是经典启发式方法可以优化某些量子参数，这强调了经典计算和量子计算的互补优势。", "conclusion": "本文旨在促进量子信息处理与未来通信系统前沿的跨学科研究。", "translation": "量子计算有望重新定义通信系统的算法基础。尽管量子叠加和纠缠能为特定问题带来二次或指数级的加速，但识别这些优势能够产生工程效益的用例仍然并非易事。本文以通信界熟悉的方式介绍了量子计算的基础知识，概述了容错量子计算的当前限制，并揭示了量子与无线系统之间的数学和谐，这使得该主题对无线研究人员更具吸引力。基于对开创性和最先进研究的系统回顾，我们提炼了量子加速通信系统研发的常见设计趋势，并强调了经验教训。关键的见解是经典启发式方法可以优化某些量子参数，这强调了经典计算和量子计算的互补优势。本文旨在促进量子信息处理与未来通信系统前沿的跨学科研究。", "summary": "本文探讨了量子计算在无线通信系统中的应用前景。文章向通信领域研究者介绍了量子计算的基础，并揭示了量子与无线系统间的数学关联。通过对现有研究的系统性回顾，论文提炼了量子加速通信系统的设计趋势，并指出经典与量子计算具有互补优势，即经典启发式方法可优化量子参数。该研究旨在推动量子信息处理与未来通信系统领域的跨学科合作。", "keywords": "量子计算, 无线通信, 量子加速, 跨学科研究, 互补优势", "comments": "这篇论文通过系统梳理量子计算与无线通信的结合点，为该新兴领域的跨学科研究提供了清晰的路线图和概念框架。其创新之处在于强调了经典与量子计算的互补性，而非单纯的替代关系，这对于实际工程应用具有重要指导意义。论文内容全面，对于希望进入量子通信领域的无线研究人员而言，具有很高的参考价值。"}}
{"id": "2506.21448", "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing", "authors": ["Huadai Liu", "Jialei Wang", "Kaicheng Luo", "Wen Wang", "Qian Chen", "Zhou Zhao", "Wei Xue"], "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\n\\textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT)\nreasoning to enable stepwise, interactive audio generation and editing for\nvideos. Our approach decomposes the process into three complementary stages:\nfoundational foley generation that creates semantically coherent soundscapes,\ninteractive object-centric refinement through precise user interactions, and\ntargeted editing guided by natural language instructions. At each stage, a\nmultimodal large language model generates contextually aligned CoT reasoning\nthat guides a unified audio foundation model. Furthermore, we introduce\n\\textbf{AudioCoT}, a comprehensive dataset with structured reasoning\nannotations that establishes connections between visual content, textual\ndescriptions, and sound synthesis. Experiments demonstrate that ThinkSound\nachieves state-of-the-art performance in video-to-audio generation across both\naudio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio\nbenchmark. The demo page is available at https://ThinkSound-Demo.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21448v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21448v1", "AI": {"title_translation": "ThinkSound：多模态大型语言模型中用于音频生成与编辑的思维链推理", "tldr": "ThinkSound是一个利用思维链推理的多模态大语言模型框架，用于视频的逐步、交互式音频生成和编辑，并在视频到音频生成上达到了SOTA性能。", "motivation": "虽然端到端视频到音频生成已大大改进，但生成能真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。这需要像创意产业专业人士一样，对视觉动态、声学环境和时间关系等进行复杂的推理。", "method": "提出ThinkSound框架，利用思维链（CoT）推理实现视频的逐步、交互式音频生成和编辑。该方法将过程分解为三个互补阶段：创建语义连贯声景的基础拟音生成、通过精确用户交互实现交互式以对象为中心的细化、以及自然语言指令引导的定向编辑。在每个阶段，多模态大语言模型生成与上下文对齐的CoT推理，指导统一的音频基础模型。此外，引入AudioCoT数据集，包含结构化推理标注，建立了视觉内容、文本描述和声音合成之间的联系。", "result": "ThinkSound在视频到音频生成方面，无论是在音频指标还是CoT指标上都达到了最先进的性能，并在分布外Movie Gen Audio基准测试中表现出色。", "conclusion": "ThinkSound通过引入思维链推理和分阶段处理，显著提升了视频到音频生成和编辑的质量和交互性，达到了最先进的性能，解决了现有方法在生成高保真音频方面的挑战。", "translation": "虽然端到端视频到音频生成已大大改进，但生成能真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。就像创意产业的专业人士一样，这种生成需要对视觉动态、声学环境和时间关系等项目进行复杂的推理。我们提出了ThinkSound，这是一个新颖的框架，它利用思维链（CoT）推理来实现视频的逐步、交互式音频生成和编辑。我们的方法将过程分解为三个互补阶段：创建语义连贯声景的基础拟音生成、通过精确用户交互实现交互式以对象为中心的细化、以及自然语言指令引导的定向编辑。在每个阶段，多模态大型语言模型生成与上下文对齐的CoT推理，指导统一的音频基础模型。此外，我们引入了AudioCoT，一个包含结构化推理标注的综合数据集，建立了视觉内容、文本描述和声音合成之间的联系。实验表明，ThinkSound在视频到音频生成方面，无论是在音频指标还是CoT指标上都达到了最先进的性能，并在分布外Movie Gen Audio基准测试中表现出色。演示页面可在https://ThinkSound-Demo.github.io获取。", "summary": "ThinkSound是一个创新的框架，它利用多模态大型语言模型的思维链（CoT）推理，实现了视频的逐步、交互式音频生成和编辑。该方法将复杂的音频生成过程分解为基础拟音生成、交互式对象细化和自然语言引导编辑三个阶段，每个阶段都由上下文对齐的CoT推理指导统一的音频基础模型。为支持此研究，还引入了AudioCoT数据集。实验证明，ThinkSound在视频到音频生成方面达到了最先进的性能，尤其在处理复杂和分布外数据时表现优异，有效解决了生成高保真视频音频的挑战。", "keywords": "思维链, 多模态大语言模型, 音频生成, 视频到音频, 音频编辑", "comments": "ThinkSound的创新之处在于将思维链推理引入多模态大语言模型，用于视频音频生成和编辑，模仿专业人士的推理过程，实现了分阶段、交互式的控制，显著提升了音频的保真度和与视觉内容的匹配度。这种分阶段、可交互的框架是其主要亮点。引入AudioCoT数据集也对未来研究有重要贡献，为该领域提供了新的基准和研究资源。"}}
{"id": "2506.21162", "title": "A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation", "authors": ["Shuwei Xing", "Derek W. Cool", "David Tessier", "Elvis C. S. Chen", "Terry M. Peters", "Aaron Fenster"], "summary": "3D ultrasound (US) imaging has shown significant benefits in enhancing the\noutcomes of percutaneous liver tumour ablation. Its clinical integration is\ncrucial for transitioning 3D US into the therapeutic domain. However,\nchallenges of tumour identification in US images continue to hinder its broader\nadoption. In this work, we propose a novel framework for integrating 3D US into\nthe standard ablation workflow. We present a key component, a clinically viable\n2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to\nreduce registration complexity. To facilitate efficient verification of the\nregistration workflow, we also propose an intuitive multimodal image\nvisualization technique. In our study, 2D US-CT/MRI registration achieved a\nlandmark distance error of approximately 2-4 mm with a runtime of 0.22s per\nimage pair. Additionally, non-rigid registration reduced the mean alignment\nerror by approximately 40% compared to rigid registration. Results demonstrated\nthe efficacy of the proposed 2D US-CT/MRI registration workflow. Our\nintegration framework advanced the capabilities of 3D US imaging in improving\npercutaneous tumour ablation, demonstrating the potential to expand the\ntherapeutic role of 3D US in clinical interventions.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21162v1", "categories": ["eess.IV", "cs.AI"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21162v1", "AI": {"title_translation": "将3D超声整合到经皮肝肿瘤消融中的新颖框架", "tldr": "该论文提出了一种将3D超声整合到肝肿瘤消融中的新颖框架，其核心是一个2D超声-CT/MRI配准方法，旨在提高准确性和效率。", "motivation": "3D超声成像在增强经皮肝肿瘤消融效果方面显示出显著益处，但肿瘤识别的挑战阻碍了其更广泛的临床应用和整合。", "method": "提出了一种将3D超声整合到标准消融工作流程中的新颖框架。关键组成部分是利用3D超声作为中介来降低配准复杂度的临床可行的2D超声-CT/MRI配准方法。同时提出了一种直观的多模态图像可视化技术，以促进配准工作流程的有效验证。", "result": "2D超声-CT/MRI配准的标志点距离误差约为2-4毫米，每对图像的运行时间为0.22秒。此外，与刚性配准相比，非刚性配准将平均对齐误差减少了约40%。", "conclusion": "所提出的2D超声-CT/MRI配准工作流程被证明是有效的。该整合框架提升了3D超声成像在改善经皮肿瘤消融方面的能力，展示了扩大3D超声在临床干预中治疗作用的潜力。", "translation": "3D超声(US)成像在增强经皮肝肿瘤消融效果方面显示出显著益处。其临床整合对于将3D超声转化为治疗领域至关重要。然而，超声图像中肿瘤识别的挑战持续阻碍其更广泛的应用。在这项工作中，我们提出了一种将3D超声整合到标准消融工作流程中的新颖框架。我们提出了一个关键组成部分，一种临床可行的2D超声-CT/MRI配准方法，利用3D超声作为中介来降低配准复杂性。为了促进配准工作流程的有效验证，我们还提出了一种直观的多模态图像可视化技术。在我们的研究中，2D超声-CT/MRI配准实现了约2-4毫米的标志点距离误差，每对图像的运行时间为0.22秒。此外，与刚性配准相比，非刚性配准将平均对齐误差减少了约40%。结果证明了所提出的2D超声-CT/MRI配准工作流程的有效性。我们的整合框架提升了3D超声成像在改善经皮肿瘤消融方面的能力，展示了扩大3D超声在临床干预中治疗作用的潜力。", "summary": "该论文介绍了一种新颖的框架，旨在将3D超声（US）整合到经皮肝肿瘤消融中，以克服肿瘤识别和临床应用方面的挑战。其核心组件是利用3D超声作为中间体来简化过程的2D超声-CT/MRI配准方法。该框架还包含一种多模态图像可视化技术，用于验证。实验结果表明，2D超声-CT/MRI配准实现了2-4毫米的标志点距离误差，运行速度快，并且非刚性配准显著提高了对齐精度。该框架增强了3D超声在经皮肿瘤消融中的能力，预示着其治疗作用的扩大。", "keywords": "3D超声, 肝肿瘤消融, 图像配准, 多模态成像, 经皮介入", "comments": "该研究的创新之处在于利用3D超声作为2D超声-CT/MRI配准的中间体，有效简化了复杂的配准问题。其报告的准确性（2-4毫米误差）和速度（0.22秒）在临床实时应用中具有显著潜力。这项工作对于弥合先进成像技术与肝肿瘤消融临床干预之间的差距具有重要意义。"}}
{"id": "2506.21272", "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character", "authors": ["Jiayi Zheng", "Xiaodong Cun"], "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon\nvideos from a single child's drawing, while faithfully preserving its unique\nartistic style. Unlike previous storytelling methods that primarily focus on\ncharacter consistency and basic motion, FairyGen explicitly disentangles\ncharacter modeling from stylized background generation and incorporates\ncinematic shot design to support expressive and coherent storytelling. Given a\nsingle character sketch, we first employ an MLLM to generate a structured\nstoryboard with shot-level descriptions that specify environment settings,\ncharacter actions, and camera perspectives. To ensure visual consistency, we\nintroduce a style propagation adapter that captures the character's visual\nstyle and applies it to the background, faithfully retaining the character's\nfull visual identity while synthesizing style-consistent scenes. A shot design\nmodule further enhances visual diversity and cinematic quality through frame\ncropping and multi-view synthesis based on the storyboard. To animate the\nstory, we reconstruct a 3D proxy of the character to derive physically\nplausible motion sequences, which are then used to fine-tune an MMDiT-based\nimage-to-video diffusion model. We further propose a two-stage motion\ncustomization adapter: the first stage learns appearance features from\ntemporally unordered frames, disentangling identity from motion; the second\nstage models temporal dynamics using a timestep-shift strategy with frozen\nidentity weights. Once trained, FairyGen directly renders diverse and coherent\nvideo scenes aligned with the storyboard. Extensive experiments demonstrate\nthat our system produces animations that are stylistically faithful,\nnarratively structured natural motion, highlighting its potential for\npersonalized and engaging story animation. The code will be available at\nhttps://github.com/GVCLab/FairyGen", "comment": "Project Page: https://jayleejia.github.io/FairyGen/ ; Code:\n  https://github.com/GVCLab/FairyGen", "pdf_url": "http://arxiv.org/pdf/2506.21272v1", "categories": ["cs.GR", "cs.CV", "cs.MM"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21272v1", "AI": {"title_translation": "FairyGen: 从儿童手绘角色生成故事驱动的卡通视频", "tldr": "FairyGen是一个从单个儿童绘画自动生成故事驱动卡通视频的系统，它忠实保留了绘画的独特艺术风格，并通过解耦角色与背景、融入电影镜头设计等方式，实现富有表现力和连贯的动画。", "motivation": "现有讲故事方法主要关注角色一致性和基本运动，而FairyGen旨在解决这些局限性，通过明确解耦角色建模与风格化背景生成，并融入电影镜头设计，以支持更具表现力和连贯性的故事讲述，从而实现个性化和引人入胜的故事动画。", "method": "FairyGen系统首先使用MLLM从单个角色草图生成带有镜头级别描述的结构化故事板。然后，引入风格传播适配器将角色风格应用于背景以确保视觉一致性。接着，通过镜头设计模块（帧裁剪和多视角合成）增强视觉多样性和电影质量。为了动画化，系统重建角色的3D代理以获得物理上合理的运动序列，并用其微调基于MMDiT的图像到视频扩散模型。此外，提出了一个两阶段运动定制适配器：第一阶段从时间无序帧中学习外观特征以解耦身份与运动；第二阶段使用带有冻结身份权重的时步偏移策略建模时间动态。", "result": "FairyGen系统能够生成与故事板对齐的多样且连贯的视频场景。实验证明，其生成的动画在风格上忠实于原始绘画，叙事结构清晰，且运动自然。", "conclusion": "FairyGen系统通过从单个儿童绘画生成风格忠实、叙事结构化且运动自然的卡通视频，展示了其在个性化和引人入胜的故事动画方面的巨大潜力。", "translation": "我们提出了FairyGen，一个从单个儿童绘画中自动生成故事驱动卡通视频的系统，同时忠实地保留其独特的艺术风格。与之前主要关注角色一致性和基本运动的讲故事方法不同，FairyGen明确地将角色建模与风格化背景生成解耦，并融入电影镜头设计以支持富有表现力和连贯的讲故事。给定一个单一的角色草图，我们首先使用MLLM生成一个带有镜头级别描述的结构化故事板，指定环境设置、角色动作和摄像机视角。为了确保视觉一致性，我们引入了一个风格传播适配器，它捕获角色的视觉风格并将其应用于背景，忠实地保留角色的完整视觉特征，同时合成风格一致的场景。一个镜头设计模块通过基于故事板的帧裁剪和多视角合成进一步增强视觉多样性和电影质量。为了动画化故事，我们重建了角色的3D代理以导出物理上合理的运动序列，然后用于微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一个两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动解耦；第二阶段使用带有冻结身份权重的时步偏移策略对时间动态进行建模。一旦训练完成，FairyGen直接渲染与故事板对齐的多样且连贯的视频场景。广泛的实验表明，我们的系统生成了风格忠实、叙事结构化、运动自然的动画，突出了其在个性化和引人入胜的故事动画方面的潜力。代码将在https://github.com/GVCLab/FairyGen提供。", "summary": "FairyGen是一个创新的自动化系统，能将儿童手绘角色转化为故事驱动的卡通视频，并忠实保留其独特艺术风格。该系统通过使用MLLM生成详细故事板，引入风格传播适配器确保视觉一致性，利用镜头设计模块提升电影感，并通过3D代理重建和两阶段运动定制适配器（基于MMDiT扩散模型）实现逼真动画。它有效解决了传统方法的局限，通过解耦角色与背景、融入电影镜头设计，最终生成风格忠实、叙事连贯且动作自然的动画，为个性化故事动画开辟了新途径。", "keywords": "儿童手绘, 卡通视频生成, 故事驱动动画, 风格保持, 电影镜头设计", "comments": "该论文提出了一种创新方法，利用多模态大语言模型和扩散模型，实现了从儿童手绘角色生成故事驱动的卡通视频。其核心创新在于明确地将角色建模与风格化背景生成解耦，并巧妙地融入电影镜头设计，显著提升了叙事表现力和视觉质量。此外，两阶段运动定制适配器的设计也有效解决了动画中身份与运动的解耦问题，确保了动作的自然与合理。该系统在忠实保留原画艺术风格的同时，生成复杂、故事驱动的动画的能力，展现了其在个性化内容创作领域的巨大潜力，具有重要的应用价值。"}}
{"id": "2506.20978", "title": "Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality", "authors": ["Naihe Feng", "Yi Sui", "Shiyi Hou", "Jesse C. Cresswell", "Ga Wu"], "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.", "comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment", "pdf_url": "http://arxiv.org/pdf/2506.20978v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20978v1", "AI": {"title_translation": "通过条件共形事实性评估检索增强生成响应质量", "tldr": "Conformal-RAG利用共形预测为RAG响应子声明的质量提供统计保证，无需真实标签，并比直接应用CP效果更好。", "motivation": "现有RAG研究主要关注整体问答准确性，忽视了生成响应中子声明的质量；此外，现有的RAG自动评估方法缺乏概率保证或需要真实答案。", "method": "本文提出了Conformal-RAG框架，该框架受共形预测（CP）在大型语言模型（LLM）上应用的启发。Conformal-RAG利用CP和RAG机制的内部信息，为响应质量提供统计保证，并确保跨多个子域的组条件覆盖，无需手动标记共形集。", "result": "与现有RAG自动评估方法相比，Conformal-RAG对精炼子声明的质量提供统计保证，确保响应可靠性而无需真实答案。实验表明，在保持相同可靠性保证的情况下，Conformal-RAG比直接将CP应用于LLM保留多达60%的高质量子声明。", "conclusion": "Conformal-RAG通过利用RAG系统信息，为RAG响应的子声明质量提供可靠的统计保证，解决了现有评估方法缺乏概率保证和对真实标签依赖的局限性。", "translation": "现有关于检索增强生成（RAG）的研究主要侧重于提高整体问答准确性，往往忽视了生成响应中子声明的质量。最近旨在通过自动评估指标等方式提高RAG可信度的方法，缺乏概率保证或需要真实答案。为了解决这些局限性，我们提出了Conformal-RAG，一个受最近共形预测（CP）在大型语言模型（LLM）上应用启发的全新框架。Conformal-RAG利用CP和RAG机制的内部信息，为响应质量提供统计保证。它确保跨多个子域的组条件覆盖，而无需手动标记共形集，使其适用于复杂的RAG应用。与现有RAG自动评估方法相比，Conformal-RAG对精炼子声明的质量提供统计保证，确保响应可靠性而无需真实答案。此外，我们的实验表明，通过利用RAG系统的信息，Conformal-RAG在保持相同可靠性保证的情况下，比直接将CP应用于LLM保留多达60%的高质量子声明。", "summary": "本文提出了Conformal-RAG框架，旨在解决检索增强生成（RAG）中子声明质量评估的不足。该框架利用共形预测（CP）和RAG机制的内部信息，为RAG生成响应的子声明质量提供统计保证，克服了现有评估方法缺乏概率保证和需要真实标签的限制。实验证明，Conformal-RAG在提供可靠性保证的同时，能比直接应用CP到LLM保留更多高质量子声明。", "keywords": "检索增强生成, 响应质量评估, 共形预测, 事实性, 统计保证", "comments": "Conformal-RAG的创新之处在于将共形预测与RAG系统内部信息结合，为RAG响应的子声明质量提供统计层面的可靠性保证。这解决了RAG在实际应用中可信度评估的关键挑战，尤其是在无需真实标签的情况下提供概率保证，具有重要的实际意义和潜在影响。"}}
{"id": "2506.20822", "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "authors": ["Quintin Myers", "Yanjun Gao"], "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20822v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20822v1", "AI": {"title_translation": "揭示大型语言模型中隐藏的暴力倾向：通过行为短篇故事进行人口统计学分析", "tldr": "本研究评估了大型语言模型（LLMs）处理道德模糊情景的能力，发现其表面输出与内部暴力倾向不符，且暴力倾向因人口统计学特征而异，与人类行为研究结果相悖。", "motivation": "大型语言模型（LLMs）被广泛应用于在线暴力内容的检测和响应，但它们对道德模糊的现实世界情景进行推理的能力尚未得到充分检验。本研究旨在评估LLMs的这种能力，并分析其潜在的偏见。", "method": "研究首次使用经社会科学验证的暴力行为短篇故事问卷（VBVQ）来评估LLMs。通过基于角色的提示，在美国境内改变种族、年龄和地理身份，以评估潜在偏见。在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLM。", "result": "1. LLMs的表面文本生成常常与其内部对暴力反应的偏好存在差异。2. 它们的暴力倾向因人口统计学特征而异，这经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "conclusion": "大型语言模型在处理道德模糊情景时，其内部可能存在与表面输出不符的暴力倾向，并且这些倾向表现出与人类社会科学研究相悖的人口统计学偏见，这表明在将其应用于敏感领域时需要谨慎。", "translation": "大型语言模型（LLM）越来越多地被提议用于在线暴力内容的检测和响应，但它们对道德模糊的现实世界情景进行推理的能力仍未得到充分检验。我们提出了第一个使用经过验证的社会科学工具来评估LLM的研究，该工具旨在衡量人类对日常冲突的反应，即暴力行为短篇故事问卷（VBVQ）。为了评估潜在偏见，我们引入了基于角色的提示，其中在美国境内改变了种族、年龄和地理身份。在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLM。我们的研究揭示了两个关键发现：（1）LLM的表面文本生成通常与其内部对暴力反应的偏好存在差异；（2）它们的暴力倾向因人口统计学特征而异，这经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "summary": "本研究探讨了大型语言模型（LLMs）在理解和应对道德模糊的现实世界暴力情景方面的能力，指出其相关能力尚未得到充分检验。研究首次采用经社会科学验证的暴力行为短篇故事问卷（VBVQ）来评估LLMs，并通过基于角色的提示在美国境内引入了种族、年龄和地理身份等人口统计学变量，以评估潜在偏见。研究在统一的零样本设置下评估了六个LLMs，结果发现LLMs的表面文本生成与其内部对暴力反应的偏好存在差异，且其暴力倾向因人口统计学特征而异，这经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "keywords": "大型语言模型, 暴力倾向, 偏见, 人口统计学, 行为短篇故事", "comments": "该论文通过将经过验证的社会科学工具（VBVQ）应用于LLM评估，展现了创新性，超越了传统的内容审核方法，深入探讨了模型的道德推理和潜在偏藏。其重要性在于揭示了关键且反直觉的发现：LLMs可能在内部倾向于暴力，尽管其输出表面上看似无害，且其偏见与已知的人类人口统计学模式相悖。这揭示了LLMs在应用于内容检测等敏感领域时存在的显著局限性，敦促对LLMs的伦理对齐和内部机制进行更深入的研究。"}}
{"id": "2506.21298", "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation", "authors": ["Atharva Mehta", "Shivam Chauhan", "Monojit Choudhury"], "summary": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21298v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21298v1", "AI": {"title_translation": "探索低资源音乐生成中的适配器设计权衡", "tldr": "本研究探讨了针对低资源音乐生成的大型模型中不同适配器设计的权衡，发现卷积适配器擅长捕捉局部细节，而Transformer适配器更擅长处理长程依赖，并分析了计算效率和模型性能。", "motivation": "微调大型音乐生成模型（如MusicGen和Mustango）计算成本高昂，需要大量硬件资源。参数高效微调（PEFT）技术，特别是基于适配器的方法，提供了一种有前景的替代方案。然而，适配器设计选择众多，在低资源音乐流派中，哪种组合能产生最佳适配器以及原因尚不清楚。", "method": "本文通过研究两种AI音乐模型（MusicGen和Mustango）在两种流派（印度斯坦古典音乐和土耳其马卡姆音乐）上的各种适配器配置，试图回答这个问题。", "result": "研究发现，卷积适配器擅长捕捉精细的局部音乐细节（如装饰音和短旋律），而Transformer适配器能更好地保持对结构化即兴创作至关重要的长程依赖。中等大小的适配器（40M参数）在表达能力和质量之间取得了最佳平衡。Mustango模型生成更多样化、更符合描述的输出，但缺乏音符稳定性、节奏对齐和美学，且计算密集型；MusicGen等自回归模型训练更快、效率更高，能产生更高质量的输出，但生成内容冗余度略高。", "conclusion": "本研究揭示了在低资源音乐生成背景下，不同适配器设计选择的独特权衡，并比较了不同模型架构的性能和计算效率，为优化PEFT策略提供了见解。", "translation": "微调大型音乐生成模型，如MusicGen和Mustango，是一个计算成本高昂的过程，通常需要更新数十亿参数，因此需要大量硬件资源。参数高效微调（PEFT）技术，特别是基于适配器的方法，已成为一种有前景的替代方案，可以在最小化可训练参数的同时保持模型性能。然而，适配器的设计选择，包括其架构、放置和大小，数量众多，并且对于给定低资源音乐流派的案例，尚不清楚这些组合中的哪一种会产生最佳适配器以及原因。在本文中，我们试图通过研究两种AI音乐模型（MusicGen和Mustango）在两种流派：印度斯坦古典音乐和土耳其马卡姆音乐上的各种适配器配置来回答这个问题。\n我们的研究结果揭示了明显的权衡：基于卷积的适配器擅长捕捉精细的局部音乐细节，如装饰音和短旋律，而基于Transformer的适配器能更好地保持对结构化即兴创作至关重要的长程依赖。此外，我们分析了不同适配器规模下的计算资源需求，展示了中等大小的适配器（40M参数）如何在表达能力和质量之间实现最佳平衡。此外，我们发现基于扩散模型的Mustango在生成更多样化、更符合输入提示描述的输出方面表现更好，但在音符稳定性、节奏对齐和美学方面有所欠缺。而且，它的计算量大，训练时间显著增加。相比之下，像MusicGen这样的自回归模型训练更快，效率更高，并且可以产生更高质量的输出，但其生成内容略有冗余。", "summary": "本研究探讨了在低资源音乐生成场景下，大型模型（MusicGen和Mustango）中适配器设计选择的权衡。论文评估了不同适配器配置（卷积和Transformer）对两种音乐流派（印度斯坦古典和土耳其马卡姆）的影响，并比较了其捕捉音乐细节、保持长程依赖的能力以及计算资源需求。研究发现，卷积适配器擅长局部细节，Transformer适配器擅长长程依赖，中等大小的适配器（40M参数）达到性能与效率的平衡。同时，论文对比了扩散模型Mustango和自回归模型MusicGen的优缺点。", "keywords": "音乐生成, 适配器, 参数高效微调, MusicGen, Mustango", "comments": "该论文通过对不同适配器设计在低资源音乐生成中的系统性研究，为参数高效微调（PEFT）在音乐领域的应用提供了宝贵的实践指导。其创新点在于细致地分析了不同适配器架构（卷积与Transformer）对音乐特征捕捉的差异，并量化了计算资源与模型性能的权衡。这对于资源受限的研究者和开发者而言具有重要意义，有助于他们在实际应用中做出更明智的设计选择。同时，对两种不同音乐生成模型（扩散与自回归）的比较也丰富了对各自优缺点的理解。"}}
{"id": "2506.21025", "title": "An energy-stable parametric finite element method for the Willmore flow in three dimensions", "authors": ["Weizhu Bao", "Yifei Li", "Dongmin Wang"], "summary": "This work develops novel energy-stable parametric finite element methods\n(ES-PFEM) for the Willmore flow and curvature-dependent geometric gradient\nflows of surfaces in three dimensions. The key to achieving the energy\nstability lies in the use of two novel geometric identities: (i) a reformulated\nvariational form of the normal velocity field, and (ii) incorporation of the\ntemporal evolution of the mean curvature into the governing equations. These\nidentities enable the derivation of a new variational formulation. By using the\nparametric finite element method, an implicit fully discrete scheme is\nsubsequently developed, which maintains the energy dissipative property at the\nfully discrete level. Based on the ES-PFEM, comprehensive insights into the\ndesign of ES-PFEM for general curvature-dependent geometric gradient flows and\na new understanding of mesh quality improvement in PFEM are provided. In\nparticular, we develop the first PFEM for the Gauss curvature flow of surfaces.\nFurthermore, a tangential velocity control methodology is applied to improve\nthe mesh quality and enhance the robustness of the proposed numerical method.\nExtensive numerical experiments confirm that the proposed method preserves\nenergy dissipation properties and maintain good mesh quality in the surface\nevolution under the Willmore flow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21025v1", "categories": ["math.NA", "cs.NA", "65M60, 65M12, 35K55, 53C44"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21025v1", "AI": {"title_translation": "三维Willmore流的一种能量稳定参数有限元方法", "tldr": "本文开发了一种新的能量稳定的参数有限元方法（ES-PFEM），用于三维Willmore流和曲率相关几何梯度流，通过引入新的几何恒等式和切向速度控制，实现了能量耗散并保持了良好的网格质量。", "motivation": "针对三维Willmore流和曲率相关几何梯度流，开发一种能量稳定的参数有限元方法，以保持能量耗散特性并提高网格质量，同时提供对ES-PFEM设计和网格质量改进的新理解。", "method": "该方法通过使用两个新的几何恒等式（法向速度场的重构变分形式和将平均曲率时间演化纳入控制方程）来推导新的变分公式。基于此，开发了一个隐式全离散参数有限元方案，确保能量耗散。此外，还首次开发了用于高斯曲率流的PFEM，并应用了切向速度控制以改善网格质量和增强鲁棒性。", "result": "所提出的ES-PFEM方法在全离散层面保持了能量耗散特性。在Willmore流下的曲面演化过程中，该方法能够维持良好的网格质量。数值实验广泛证实了该方法在能量耗散和网格质量方面的有效性，并且首次实现了高斯曲率流的PFEM。", "conclusion": "本文提出的能量稳定参数有限元方法（ES-PFEM）能够有效地模拟三维Willmore流和曲率相关几何梯度流，通过新颖的几何恒等式和切向速度控制，确保了能量耗散特性并保持了良好的网格质量，为相关领域提供了新的设计思路和理解。", "translation": "这项工作开发了一种新颖的能量稳定参数有限元方法（ES-PFEM），用于三维曲面上的Willmore流和曲率相关几何梯度流。实现能量稳定性的关键在于使用了两个新颖的几何恒等式：(i) 法向速度场的重新表述变分形式，以及 (ii) 将平均曲率的时间演化纳入控制方程。这些恒等式使得能够推导出一个新的变分公式。通过使用参数有限元方法，随后开发了一个隐式全离散方案，该方案在全离散层面上保持了能量耗散特性。基于ES-PFEM，本文提供了关于通用曲率相关几何梯度流的ES-PFEM设计以及PFEM中网格质量改进的新理解的全面见解。特别是，我们首次开发了用于曲面高斯曲率流的PFEM。此外，应用了切向速度控制方法来改善网格质量并增强所提出数值方法的鲁棒性。大量的数值实验证实，所提出的方法在Willmore流下的曲面演化过程中保持了能量耗散特性并维持了良好的网格质量。", "summary": "本文提出了一种用于三维Willmore流和曲率相关几何梯度流的新型能量稳定参数有限元方法（ES-PFEM）。该方法通过引入两个新的几何恒等式，推导了新的变分公式，并开发了保持能量耗散特性的隐式全离散方案。研究还提供了ES-PFEM设计和PFEM网格质量改进的见解，首次实现了高斯曲率流的PFEM，并利用切向速度控制提高了网格质量和方法鲁棒性。数值实验验证了其在能量耗散和网格质量保持方面的有效性。", "keywords": "Willmore流, 参数有限元方法, 能量稳定, 几何梯度流, 网格质量", "comments": "这项工作通过引入新颖的几何恒等式和切向速度控制机制，显著提升了参数有限元方法在处理Willmore流和曲率相关几何梯度流时的能量稳定性和网格质量，特别是在三维空间中。首次将PFEM应用于高斯曲率流，显示了其创新性。该方法对于模拟复杂曲面演化具有重要意义。"}}
{"id": "2506.21106", "title": "PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction", "authors": ["Felipe Castaño", "Eduardo Fidalgo", "Enrique Alegre", "Rocio Alaiz-Rodríguez", "Raul Orduna", "Francesco Zola"], "summary": "Phishing attacks pose a significant cybersecurity threat, evolving rapidly to\nbypass detection mechanisms and exploit human vulnerabilities. This paper\nintroduces PhishKey to address the challenges of adaptability, robustness, and\nefficiency. PhishKey is a novel phishing detection method using automatic\nfeature extraction from hybrid sources. PhishKey combines character-level\nprocessing with Convolutional Neural Networks (CNN) for URL classification, and\na Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at\nthe word level. CAPE reduces noise and ensures complete sample processing\navoiding crop operations on the input data. The predictions from both modules\nare integrated using a soft-voting ensemble to achieve more accurate and\nreliable classifications. Experimental evaluations on four state-of-the-art\ndatasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1\nScore and shows strong resistance to adversarial manipulations such as\ninjection attacks with minimal performance degradation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21106v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21106v1", "AI": {"title_translation": "PhishKey：一种基于新型质心的方法，用于通过自适应HTML组件提取增强网络钓鱼检测", "tldr": "PhishKey是一种新颖的网络钓鱼检测方法，结合了基于CNN的URL分类和基于质心的HTML内容提取（CAPE），通过软投票集成实现高精度，并在对抗性攻击下表现出强大的鲁棒性。", "motivation": "网络钓鱼攻击对网络安全构成重大威胁，且快速演变以规避检测机制并利用人类弱点。本文引入PhishKey旨在解决网络钓鱼检测中适应性、鲁棒性和效率的挑战。", "method": "PhishKey是一种新颖的网络钓鱼检测方法，采用混合源的自动特征提取。它结合了字符级处理与卷积神经网络（CNN）进行URL分类，并使用基于质心的关键组件网络钓鱼提取器（CAPE）处理词级的HTML内容。CAPE减少了噪音并确保完整样本处理，避免对输入数据进行裁剪操作。两个模块的预测通过软投票集成进行整合，以实现更准确和可靠的分类。", "result": "在四个最先进的数据集上进行的实验评估证明了PhishKey的有效性。它实现了高达98.70%的F1分数，并显示出对注入攻击等对抗性操纵的强大抵抗力，性能下降最小。", "conclusion": "PhishKey通过结合URL和HTML内容分析的混合方法，有效提高了网络钓鱼检测的准确性和鲁棒性，即使在面对对抗性攻击时也能保持高性能。", "translation": "网络钓鱼攻击构成了重大的网络安全威胁，其快速演变以规避检测机制并利用人类的弱点。本文引入PhishKey，旨在解决适应性、鲁棒性和效率方面的挑战。PhishKey是一种新颖的网络钓鱼检测方法，利用混合源的自动特征提取。PhishKey结合了字符级处理与卷积神经网络（CNN）进行URL分类，以及用于词级HTML内容的基于质心的关键组件网络钓鱼提取器（CAPE）。CAPE减少了噪音并确保完整样本处理，避免对输入数据进行裁剪操作。来自两个模块的预测通过软投票集成进行整合，以实现更准确和可靠的分类。在四个最先进的数据集上进行的实验评估证明了PhishKey的有效性。它实现了高达98.70%的F1分数，并显示出对注入攻击等对抗性操纵的强大抵抗力，性能下降最小。", "summary": "PhishKey是一种针对网络钓鱼攻击的新型检测方法，旨在提高适应性、鲁棒性和效率。它通过结合CNN进行URL字符级分类和基于质心的CAPE进行HTML词级内容提取来实现自动特征提取。PhishKey利用软投票集成整合两者的预测以提高准确性。实验结果表明，PhishKey在多个数据集上表现出色，F1分数高达98.70%，并对对抗性攻击具有很强的抵抗力。", "keywords": "网络钓鱼检测, 质心方法, HTML组件提取, 卷积神经网络, 对抗性鲁棒性", "comments": "PhishKey的创新之处在于其混合方法，结合了URL的字符级CNN分析和HTML内容的词级质心提取，这种双管齐下的策略增强了检测的全面性。特别值得注意的是CAPE模块，它通过避免裁剪操作来减少噪声并确保完整样本处理，这对于保持数据完整性至关重要。此外，软投票集成策略提高了分类的可靠性。该方法在对抗性操纵下的鲁棒性是其重要优势，表明其在实际部署中具有潜力。"}}
{"id": "2506.21138", "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE", "authors": ["Abdelkarim El-Hajjami", "Camille Salinesi"], "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21138v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21138v1", "AI": {"title_translation": "合成需求有多好？评估用于AI4RE的LLM生成数据集", "tldr": "鉴于AI4RE领域标注需求数据集的短缺，本文提出了Synthline v1，一种增强的产品线方法，用于生成合成需求数据。研究表明，通过多样本提示等策略，LLM生成的合成数据在特定任务上可以与甚至超越人工数据，为缓解数据集稀缺性提供了可行路径。", "motivation": "人工智能需求工程（AI4RE）领域面临公开可用、带标签的需求数据集严重短缺的重大障碍。尽管大型语言模型（LLMs）在合成数据生成方面展现出前景，但系统性地控制和优化生成需求质量的方法仍未得到充分探索。", "method": "本文提出了Synthline v1，一种增强的产品线方法，用于生成合成需求数据，该版本在v0基础上增加了高级生成策略和策展技术。研究通过评估提示策略（多样本与单样本）、自动化提示优化（PACE）和生成后策展对数据质量的影响，探讨了四个研究问题。评估涵盖四种分类任务：缺陷检测、功能性与非功能性、质量与非质量、以及安全性与非安全性。", "result": "多样本提示显著提升了实用性和多样性，F1分数提高了6到44分。PACE自动化提示优化结果因任务而异，显著改善了功能分类（+32.5分），但在其他任务上性能下降。基于相似度的策展提高了多样性，但往往损害分类性能，表明某些冗余可能有助于机器学习模型。最重要的是，合成需求在特定任务上可以与或优于人工撰写的需求，例如在安全性（+7.8分）和缺陷分类（+15.4分）上超越了人工数据。", "conclusion": "这些发现为AI4RE提供了实用见解，并为通过系统性合成生成来缓解数据集稀缺性描绘了一条可行路径，表明合成数据在特定任务上可以匹配甚至超越人工数据。", "translation": "公开可用、带标签的需求数据集的短缺仍然是推进人工智能需求工程（AI4RE）的主要障碍。尽管大型语言模型在合成数据生成方面提供了有前景的能力，但系统地控制和优化生成需求质量的方法仍未得到充分探索。本文提出了Synthline v1，一种增强的产品线方法，用于生成合成需求数据，该方法通过高级生成策略和策展技术扩展了我们早期的v0版本。我们调查了四个研究问题，评估了提示策略、自动化提示优化和生成后策展如何影响四种分类任务（缺陷检测、功能性与非功能性、质量与非质量、安全与非安全）的数据质量。我们的评估表明，多样本提示显著提升了单样本生成的实用性和多样性，F1分数提高了6到44分。使用PACE（提示行动者-评论家编辑）进行自动化提示优化产生了依赖于任务的结果，极大地改善了功能分类（+32.5分），但降低了其他任务的性能。有趣的是，基于相似度的策展提高了多样性，但通常损害分类性能，这表明某些冗余可能有助于机器学习模型。最重要的是，我们的结果表明，合成需求在特定任务上可以与或优于人工撰写的需求，其中合成数据在安全性（+7.8分）和缺陷分类（+15.4分）方面超越了人工数据。这些发现为AI4RE提供了实用见解，并为通过系统性合成生成来缓解数据集稀缺性描绘了一条可行路径。", "summary": "为解决AI4RE领域标注需求数据集稀缺的问题，本文提出了Synthline v1，一种利用大型语言模型生成合成需求的增强产品线方法。研究系统评估了提示策略、自动化提示优化和生成后策展对数据质量的影响，涵盖多种分类任务。结果显示，多样本提示能显著提升数据效用与多样性，并且在安全性与缺陷检测等特定任务上，合成数据表现甚至优于人工数据，证明了其作为缓解数据集稀缺性有效手段的潜力。", "keywords": "合成需求, LLM, AI4RE, 数据集生成, 数据质量", "comments": "本文通过系统性地探索LLM生成合成需求数据的质量控制与优化方法，展现了创新性。其重要性在于，它不仅证明了合成数据在AI4RE领域的可行性，更指出在特定任务上合成数据甚至能超越人工数据，为解决数据稀缺性提供了强有力的替代方案。对于提示优化结果的任务依赖性以及策展对性能的复杂影响的深入分析，也提供了宝贵的实践指导。"}}
{"id": "2506.21322", "title": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory", "authors": ["Hong Wang", "Natalia Calvo-Barajas", "Katie Winkle", "Ginevra Castellano"], "summary": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2506.21322v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21322v1", "AI": {"title_translation": "\"我该相信谁？\"：当家庭医疗机器人与人类记忆矛盾时用户的解释与决策", "tldr": "研究家庭医疗机器人信息与用户记忆冲突时，机器人透明度和社交性对用户信任和决策的影响。", "motivation": "随着智能医疗机器人在家庭环境中部署的可能性增加，当机器人提供的信息与用户记忆矛盾时，会引发用户信任和决策问题。", "method": "采用2x2被试间在线研究，176名参与者观看Furhat机器人作为家庭医疗助手，建议虚构用户在不同于其记忆的时间服药的视频。研究考察机器人透明度和社交性对用户解释、决策和信任的影响。", "result": "机器人透明度影响用户对信息差异的解释：低透明度机器人下，用户倾向认为自己记错了时间；高透明度机器人下，用户更倾向归因于外部因素（如伴侣修改了信息）。参与者表现出过度信任倾向，即使怀疑系统故障或第三方干扰，也常优先采纳机器人建议。", "conclusion": "研究强调了机器人系统中透明度机制的影响、多用户家庭机器人系统访问控制的复杂性和重要性，以及用户在医疗等敏感领域过度依赖机器人的潜在风险。", "translation": "\"我该相信谁？\"：当家庭医疗机器人与人类记忆矛盾时用户的解释与决策\n\n摘要：\n随着机器人提供身体协助、心理支持和日常健康管理能力的进步，在不久的将来，在家庭环境中部署智能医疗机器人变得越来越可行。然而，当这些机器人提供的信息与用户的记忆相矛盾时，挑战随之出现，引发了用户信任和决策方面的担忧。本文提出了一项研究，探讨了当机器人提供冲突信息时，改变机器人的透明度和社交性水平如何影响用户的解释、决策和感知信任。在一项2x2被试间在线研究中，176名参与者观看了Furhat机器人扮演家庭医疗助手，建议虚构用户在不同于其记忆的时间服药的视频。结果表明，机器人透明度影响了用户对信息差异的解释：对于低透明度机器人，最常见的假设是用户没有正确记住时间，而对于高透明度机器人，参与者更有可能将差异归因于外部因素，例如伴侣或另一个家庭成员修改了机器人的信息。此外，参与者表现出过度信任的倾向，即使怀疑系统故障或第三方干扰，也经常优先采纳机器人的建议而非用户的记忆。这些发现强调了透明度机制在机器人系统中的影响、与家庭环境中部署的多用户机器人系统访问控制相关的复杂性和重要性，以及用户在医疗等敏感领域过度依赖机器人的潜在风险。", "summary": "本文研究了当家庭医疗机器人提供的信息与用户记忆冲突时，机器人透明度和社交性对用户解释、决策和信任的影响。一项2x2在线研究发现，机器人透明度影响用户对信息差异的归因，高透明度下用户更倾向归因于外部因素。研究还揭示用户存在对机器人的过度信任倾向，即使怀疑系统问题也优先采纳机器人建议。这些结果强调了透明度机制、多用户系统访问控制的重要性以及过度依赖机器人的潜在风险。", "keywords": "家庭医疗机器人, 用户信任, 信息冲突, 透明度, 决策制定", "comments": "这项研究通过探讨家庭医疗机器人与用户记忆冲突这一具体场景，揭示了机器人设计中透明度机制的关键作用。其创新之处在于量化了透明度对用户归因的影响，并提出了用户可能存在的过度信任风险，这对于未来家庭医疗机器人的设计和部署具有重要的实践指导意义。特别是在多用户家庭环境中，系统访问控制的重要性被凸显，为后续研究提供了方向。"}}
{"id": "2506.21016", "title": "Fault-Tolerant Spacecraft Attitude Determination using State Estimation Techniques", "authors": ["B. Chidambaram", "A. Hilbert", "M. Silva"], "summary": "The extended and unscented Kalman filter, and the particle filter provide a\nrobust framework for fault-tolerant attitude estimation on spacecraft. This\npaper explores how each filter performs for a large satellite in a low earth\norbit. Additionally, various techniques, built on these filters, for fault\ndetection, isolation and recovery from erroneous sensor measurements, are\nanalyzed. Key results from this analysis include filter performance for various\nfault modes.", "comment": "8 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.21016v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21016v1", "AI": {"title_translation": "基于状态估计算技术的容错航天器姿态确定", "tldr": "本文评估了扩展卡尔曼滤波器、无迹卡尔曼滤波器和粒子滤波器在航天器容错姿态估计中的性能，并分析了基于这些滤波器的故障检测、隔离和恢复技术。", "motivation": "本文旨在探讨扩展卡尔曼滤波器、无迹卡尔曼滤波器和粒子滤波器在航天器容错姿态估计中的性能，特别是针对故障检测、隔离和从错误传感器测量中恢复的能力。", "method": "本文采用扩展卡尔曼滤波器（EKF）、无迹卡尔曼滤波器（UKF）和粒子滤波器（PF）作为容错姿态估计的鲁棒框架。研究分析了基于这些滤波器构建的各种故障检测、隔离和恢复（FDIR）技术，以应对错误的传感器测量。分析对象为低地球轨道上的一颗大型卫星。", "result": "分析的主要结果包括各种故障模式下滤波器的性能表现。", "conclusion": "扩展卡尔曼滤波器、无迹卡尔曼滤波器和粒子滤波器为航天器容错姿态估计提供了鲁棒框架，它们的性能在不同故障模式下有所差异，并且在故障检测、隔离和恢复方面得到了应用分析。", "translation": "扩展卡尔曼滤波器、无迹卡尔曼滤波器和粒子滤波器为航天器容错姿态估计提供了一个鲁棒的框架。本文探讨了每种滤波器在低地球轨道大型卫星上的性能。此外，还分析了基于这些滤波器构建的各种技术，用于故障检测、隔离和从错误传感器测量中恢复。这项分析的主要结果包括各种故障模式下的滤波器性能。", "summary": "本文研究了扩展卡尔曼滤波器、无迹卡尔曼滤波器和粒子滤波器在航天器容错姿态确定中的应用和性能。它专门分析了这些状态估计算技术，以及集成的故障检测、隔离和恢复方法，在处理低地球轨道大型卫星上错误的传感器测量时的表现，并强调了它们在不同故障模式下的性能。", "keywords": "容错, 姿态确定, 卡尔曼滤波器, 粒子滤波器, 状态估计", "comments": "该论文关注航天器自主性和可靠性的一个关键方面：容错姿态确定。使用已建立的状态估计技术（卡尔曼滤波器变体、粒子滤波器）结合故障检测、隔离和恢复（FDIR）策略是一种实用的方法。其创新之处在于对这些滤波器在各种故障模式下的比较分析，这对于实际应用至关重要。"}}
{"id": "2506.21310", "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems", "authors": ["Pauline Speckmann", "Mario Nadj", "Christian Janiesch"], "summary": "Although several post-hoc methods for explainable AI have been developed,\nmost are static and neglect the user perspective, limiting their effectiveness\nfor the target audience. In response, we developed the interactive explainable\nintelligent system called IXAII that offers explanations from four explainable\nAI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored\nviews for five user groups and gives users agency over the explanations'\ncontent and their format. We evaluated IXAII through interviews with experts\nand lay users. Our results indicate that IXAII, which provides different\nexplanations with multiple visualization options, is perceived as helpful to\nincrease transparency. By bridging the gaps between explainable AI methods,\ninteractivity, and practical implementation, we provide a novel perspective on\nAI explanation practices and human-AI interaction.", "comment": "9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track", "pdf_url": "http://arxiv.org/pdf/2506.21310v1", "categories": ["cs.AI", "cs.SE", "K.6.3 Software Management"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21310v1", "AI": {"title_translation": "IXAII：一个用于决策支持系统的交互式可解释人工智能界面", "tldr": "IXAII是一个交互式可解释人工智能系统，它整合了多种解释方法，为不同用户群体提供定制化的解释视图和内容控制，旨在提高AI决策支持系统的透明度。", "motivation": "现有的事后可解释AI方法大多是静态的，并且忽视了用户视角，限制了它们对目标受众的有效性。", "method": "开发了一个名为IXAII的交互式可解释智能系统，该系统集成了LIME、SHAP、Anchors和DiCE四种可解释AI方法，为五种用户群体提供定制化视图，并允许用户控制解释内容和格式。通过专家和普通用户的访谈对IXAII进行了评估。", "result": "评估结果表明，IXAII通过提供具有多种可视化选项的不同解释，被认为有助于提高透明度。", "conclusion": "该研究通过弥合可解释AI方法、交互性和实际实现之间的差距，为AI解释实践和人机交互提供了新颖的视角。", "translation": "尽管已经开发了几种事后可解释AI方法，但大多数都是静态的，并且忽视了用户视角，限制了它们对目标受众的有效性。为此，我们开发了一个名为IXAII的交互式可解释智能系统，它提供了来自LIME、SHAP、Anchors和DiCE四种可解释AI方法的解释。我们的原型为五种用户群体提供了定制化的视图，并赋予用户对解释内容和格式的自主权。我们通过对专家和普通用户的访谈评估了IXAII。我们的结果表明，IXAII通过提供具有多种可视化选项的不同解释，被认为有助于提高透明度。通过弥合可解释AI方法、交互性和实际实现之间的差距，我们为AI解释实践和人机交互提供了新颖的视角。", "summary": "本文介绍了一个名为IXAII的交互式可解释人工智能（XAI）系统，旨在解决现有XAI方法静态且忽视用户视角的问题。IXAII整合了LIME、SHAP、Anchors和DiCE等四种XAI方法，为五类用户提供定制化的解释视图，并赋予用户控制解释内容和格式的权力。通过对专家和普通用户的访谈评估，结果显示IXAII通过提供多样化的解释和可视化选项，有效提高了AI决策支持系统的透明度，为AI解释实践和人机交互带来了新的视角。", "keywords": "可解释AI, 交互式界面, 决策支持系统, 透明度, 用户中心设计", "comments": "IXAII的创新之处在于其强调交互性和用户中心设计，通过整合多种解释方法并提供定制化视图，显著提升了可解释AI的实用性和用户接受度。它解决了传统XAI方法缺乏灵活性和用户参与度的问题，为AI决策支持系统的透明化提供了有力的工具。这项工作在人机交互和可解释AI的实际应用方面具有重要意义。"}}
{"id": "2506.21072", "title": "Bridding OT and PaaS in Edge-to-Cloud Continuum", "authors": ["Carlos J Barrios", "Yves Denneulin"], "summary": "The Operational Technology Platform as a Service (OTPaaS) initiative provides\na structured framework for the efficient management and storage of data. It\nensures excellent response times while improving security, reliability, data\nand technology sovereignty, robustness, and energy efficiency, which are\ncrucial for industrial transformation and data sovereignty. This paper\nillustrates successful deployment, adaptable application management, and\nvarious integration components catering to Edge and Cloud environments. It\nleverages the advantages of the Platform as a Service model and highlights key\nchallenges that have been addressed for specific use cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21072v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21072v1", "AI": {"title_translation": "在边缘到云连续体中连接OT和PaaS", "tldr": "运营技术平台即服务（OTPaaS）提供了一个用于高效数据管理和存储的框架，旨在改善工业环境中的安全性、可靠性、数据主权和能源效率，适用于边缘到云的连续体。", "motivation": "该论文的动机是提供一个结构化的框架（OTPaaS），用于高效的数据管理和存储，以确保卓越的响应时间，并提高安全性、可靠性、数据和技术主权、鲁棒性以及能源效率，这些对于工业转型和数据主权至关重要。", "method": "本文阐述了OTPaaS的成功部署、适应性强的应用程序管理以及满足边缘和云环境的各种集成组件。它利用了平台即服务模型的优势，并强调了针对特定用例已解决的关键挑战。", "result": "文中阐述了成功的部署、适应性强的应用程序管理以及满足边缘和云环境的各种集成组件。针对特定用例的关键挑战已得到解决。", "conclusion": "OTPaaS有效地连接了OT和PaaS，提供了一个结构化框架，可增强边缘到云连续体中的数据管理、安全性与效率，从而推动工业转型。", "translation": "运营技术平台即服务（OTPaaS）倡议提供了一个用于高效管理和存储数据的结构化框架。它确保了卓越的响应时间，同时提高了安全性、可靠性、数据和技术主权、鲁棒性和能源效率，这些对于工业转型和数据主权至关重要。本文阐述了成功的部署、适应性强的应用程序管理以及满足边缘和云环境的各种集成组件。它利用了平台即服务模型的优势，并强调了针对特定用例已解决的关键挑战。", "summary": "本文介绍了运营技术平台即服务（OTPaaS）倡议，这是一个旨在工业环境中高效管理和存储数据的结构化框架。OTPaaS旨在提高安全性、可靠性、数据主权、鲁棒性和能源效率，这些对于工业转型至关重要。该论文展示了其在边缘和云环境中的成功部署、适应性强的应用程序管理和集成组件，并强调了其如何利用PaaS模型来解决特定挑战。", "keywords": "运营技术, 平台即服务, 边缘计算, 云计算, 工业转型", "comments": "该论文创新性地将OT（运营技术）与PaaS（平台即服务）结合，创建了OTPaaS，解决了工业转型中，特别是在边缘到云连续体的数据主权和效率方面，关键的需求。其重要性在于为管理复杂的工业数据环境提供了一种结构化的方法。"}}
{"id": "2506.20841", "title": "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization", "authors": ["Ha Min Son", "Shahbaz Rezaei", "Xin Liu"], "summary": "Semi-supervised domain generalization (SSDG) aims to solve the problem of\ngeneralizing to out-of-distribution data when only a few labels are available.\nDue to label scarcity, applying domain generalization methods often\nunderperform. Consequently, existing SSDG methods combine semi-supervised\nlearning methods with various regularization terms. However, these methods do\nnot explicitly regularize to learn domains invariant representations across all\ndomains, which is a key goal for domain generalization. To address this, we\nintroduce FixCLR. Inspired by success in self-supervised learning, we change\ntwo crucial components to adapt contrastive learning for explicit domain\ninvariance regularization: utilization of class information from pseudo-labels\nand using only a repelling term. FixCLR can also be added on top of most\nexisting SSDG and semi-supervised methods for complementary performance\nimprovements. Our research includes extensive experiments that have not been\npreviously explored in SSDG studies. These experiments include benchmarking\ndifferent improvements to semi-supervised methods, evaluating the performance\nof pretrained versus non-pretrained models, and testing on datasets with many\ndomains. Overall, FixCLR proves to be an effective SSDG method, especially when\ncombined with other semi-supervised methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20841v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20841v1", "AI": {"title_translation": "FixCLR：半监督域泛化的负类对比学习", "tldr": "FixCLR是一种新的半监督域泛化方法，通过修改对比学习来显式学习域不变表示，尤其与现有方法结合时效果显著。", "motivation": "半监督域泛化（SSDG）在标签稀缺时，现有方法未能显式地正则化以学习跨所有域的域不变表示，而这正是域泛化的关键目标。", "method": "本文引入了FixCLR，受自监督学习启发，通过改变对比学习的两个关键组件来实现显式域不变性正则化：利用伪标签的类信息和仅使用排斥项。FixCLR还可以作为插件与大多数现有SSDG和半监督方法结合使用。", "result": "进行了SSDG研究中未曾探索的广泛实验，包括对半监督方法不同改进的基准测试、评估预训练与非预训练模型的性能，以及在多域数据集上的测试。", "conclusion": "FixCLR被证明是一种有效的SSDG方法，特别是与其它半监督方法结合时。", "translation": "半监督域泛化（SSDG）旨在解决当只有少量标签可用时，泛化到分布外数据的问题。由于标签稀缺，应用域泛化方法通常表现不佳。因此，现有的SSDG方法将半监督学习方法与各种正则化项结合起来。然而，这些方法没有明确地正则化以学习跨所有域的域不变表示，这是域泛化的一个关键目标。为了解决这个问题，我们引入了FixCLR。受自监督学习成功的启发，我们改变了两个关键组件，以使对比学习适应显式域不变性正则化：利用伪标签的类信息和仅使用排斥项。FixCLR还可以添加到大多数现有SSDG和半监督方法之上，以实现互补的性能改进。我们的研究包括以前在SSDG研究中未曾探索的广泛实验。这些实验包括对半监督方法不同改进的基准测试、评估预训练与非预训练模型的性能，以及在多域数据集上的测试。总的来说，FixCLR被证明是一种有效的SSDG方法，特别是与其它半监督方法结合时。", "summary": "本文针对半监督域泛化（SSDG）中标签稀缺导致现有方法难以学习域不变表示的问题，提出了FixCLR。该方法受自监督学习启发，通过利用伪标签的类信息和仅使用排斥项来改进对比学习，从而实现显式的域不变性正则化。广泛的实验表明，FixCLR是一种有效的SSDG方法，尤其是在与现有半监督方法结合使用时，能带来互补的性能提升。", "keywords": "半监督域泛化, 对比学习, 域不变性, 伪标签, FixCLR", "comments": "FixCLR的创新之处在于其通过修改对比学习来显式地解决SSDG中域不变性学习的关键挑战，特别是利用伪标签和仅使用排斥项的设计。其作为插件可与现有方法结合的特性也增加了其实用性和影响力。该研究还进行了更全面的实验评估，进一步验证了其有效性。"}}
{"id": "2506.21308", "title": "Balancing Privacy and Utility in Correlated Data: A Study of Bayesian Differential Privacy", "authors": ["Martin Lange", "Patricia Guerra-Balboa", "Javier Parra-Arnau", "Thorsten Strufe"], "summary": "Privacy risks in differentially private (DP) systems increase significantly\nwhen data is correlated, as standard DP metrics often underestimate the\nresulting privacy leakage, leaving sensitive information vulnerable. Given the\nubiquity of dependencies in real-world databases, this oversight poses a\ncritical challenge for privacy protections. Bayesian differential privacy (BDP)\nextends DP to account for these correlations, yet current BDP mechanisms\nindicate notable utility loss, limiting its adoption.\n  In this work, we address whether BDP can be realistically implemented in\ncommon data structures without sacrificing utility -- a key factor for its\napplicability. By analyzing arbitrary and structured correlation models,\nincluding Gaussian multivariate distributions and Markov chains, we derive\npractical utility guarantees for BDP. Our contributions include theoretical\nlinks between DP and BDP and a novel methodology for adapting DP mechanisms to\nmeet the BDP requirements. Through evaluations on real-world databases, we\ndemonstrate that our novel theorems enable the design of BDP mechanisms that\nmaintain competitive utility, paving the way for practical privacy-preserving\ndata practices in correlated settings.", "comment": "This is the extended version of the paper accepted in the Proceedings\n  of the VLDB Endowment (PVLDB), 2025. The code used for our experiments is\n  accessible in https://github.com/lange-martin/privacy-utility-bdp", "pdf_url": "http://arxiv.org/pdf/2506.21308v1", "categories": ["cs.CR", "cs.IT", "math.IT", "68P27"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21308v1", "AI": {"title_translation": "相关数据中隐私与效用的平衡：一项关于贝叶斯差分隐私的研究", "tldr": "本研究探讨了如何在相关数据中实现贝叶斯差分隐私（BDP），同时保持竞争力的数据效用，解决了现有BDP机制中效用损失的问题，为实际应用铺平了道路。", "motivation": "当数据存在相关性时，差分隐私（DP）系统中的隐私风险显著增加，因为标准的DP度量通常会低估由此产生的隐私泄露，使敏感信息面临风险。鉴于现实世界数据库中依赖关系的普遍性，这种疏忽对隐私保护构成了严峻挑战。贝叶斯差分隐私（BDP）扩展了DP以解释这些相关性，但当前的BDP机制表现出显著的效用损失，限制了其采用。本研究旨在解决BDP是否能在不牺牲效用的情况下实际应用于常见数据结构的问题。", "method": "通过分析任意和结构化的相关模型，包括高斯多元分布和马尔可夫链，推导出BDP的实用效用保证。贡献包括建立DP和BDP之间的理论联系，以及提出一种新颖的方法，用于调整DP机制以满足BDP要求。通过在真实世界数据库上进行评估。", "result": "研究证明，他们的新颖定理能够设计出保持竞争性效用的BDP机制。", "conclusion": "这为在相关设置中实现实用的隐私保护数据实践铺平了道路。", "translation": "当数据存在相关性时，差分隐私（DP）系统中的隐私风险显著增加，因为标准的DP度量通常会低估由此产生的隐私泄露，使敏感信息面临风险。鉴于现实世界数据库中依赖关系的普遍性，这种疏忽对隐私保护构成了严峻挑战。贝叶斯差分隐私（BDP）扩展了DP以解释这些相关性，但当前的BDP机制表现出显著的效用损失，限制了其采用。\n在这项工作中，我们探讨了BDP是否能在不牺牲效用的情况下在常见数据结构中实际实现——这是其适用性的关键因素。通过分析任意和结构化的相关模型，包括高斯多元分布和马尔可夫链，我们推导了BDP的实用效用保证。我们的贡献包括建立DP和BDP之间的理论联系，以及提出一种新颖的方法，用于调整DP机制以满足BDP要求。通过在真实世界数据库上的评估，我们证明了我们的新颖定理能够设计出保持竞争性效用的BDP机制，为在相关设置中实现实用的隐私保护数据实践铺平了道路。", "summary": "本研究旨在解决贝叶斯差分隐私（BDP）在相关数据中应用时面临的效用损失挑战。鉴于标准差分隐私在处理相关数据时存在隐私泄露低估的问题，BDP被提出以更好地考虑数据相关性。通过分析高斯多元分布和马尔可夫链等相关模型，本研究推导了BDP的实用效用保证，并建立了DP与BDP之间的理论联系。此外，还提出了一种新颖的方法来调整DP机制以满足BDP要求。在真实世界数据库上的评估表明，所提出的新定理能够设计出具有竞争性效用的BDP机制，从而促进了在相关数据环境中实际的隐私保护实践。", "keywords": "贝叶斯差分隐私, 相关数据, 隐私保护, 效用平衡, 差分隐私", "comments": "该论文解决了差分隐私在处理相关数据时的核心挑战，即如何平衡隐私保护和数据效用。通过提出新的理论和方法，使得贝叶斯差分隐私在实际应用中更具可行性，解决了现有BDP机制效用损失过大的问题，具有重要的理论和实践意义。"}}
{"id": "2506.21175", "title": "On Minimizing Wiggle in Stacked Area Charts", "authors": ["Alexander Dobler", "Martin Nöllenburg"], "summary": "Stacked area charts are a widely used visualization technique for numerical\ntime series. The x-axis represents time, and the time series are displayed as\nhorizontal, variable-height layers stacked on top of each other. The height of\neach layer corresponds to the time series values at each time point. The main\naesthetic criterion for optimizing the readability of stacked area charts is\nthe amount of vertical change of the borders between the time series in the\nvisualization, called wiggle. While many heuristic algorithms have been\ndeveloped to minimize wiggle, the computational complexity of minimizing wiggle\nhas not been formally analyzed. In this paper, we show that different variants\nof wiggle minimization are NP-hard and even hard to approximate. We also\npresent an exact mixed-integer linear programming formulation and compare its\nperformance with a state-of-the-art heuristic in an experimental evaluation.\nLastly, we consider a special case of wiggle minimization that corresponds to\nthe fundamentally interesting and natural problem of ordering a set of numbers\nas to minimize their sum of absolute prefix sums. We show several complexity\nresults for this problem that imply some of the mentioned hardness results for\nwiggle minimization.", "comment": "19 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21175v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21175v1", "AI": {"title_translation": "最小化堆叠面积图中的“摆动”", "tldr": "本文正式分析了堆叠面积图中最小化“摆动”的计算复杂性，证明其是NP-难甚至难以近似的，并提出了一个精确的混合整数线性规划公式。", "motivation": "堆叠面积图中的“摆动”（时间序列边界的垂直变化量）是影响可读性的主要美学标准。尽管已有许多启发式算法来最小化“摆动”，但其计算复杂性尚未得到正式分析。", "method": "本文证明了“摆动”最小化的不同变体是NP-难的，甚至难以近似。同时，提出了一个精确的混合整数线性规划（MILP）公式，并将其性能与最先进的启发式算法进行了实验比较。此外，还考虑了“摆动”最小化的一种特殊情况，即排序一组数字以最小化其绝对前缀和之和，并给出了该问题的复杂性结果。", "result": "研究表明，“摆动”最小化的不同变体是NP-难的，并且难以近似。论文提出了一个精确的混合整数线性规划公式，并在实验评估中与现有启发式算法进行了性能比较。对于最小化绝对前缀和之和的特殊情况，也得出了多个复杂性结果，这些结果也支持了“摆动”最小化的某些难度结论。", "conclusion": "本文首次对堆叠面积图中最小化“摆动”的计算复杂性进行了正式分析，揭示了其NP-难甚至难以近似的特性，并提供了一个精确的混合整数线性规划解决方案，同时通过分析一个相关的特殊问题进一步证实了其计算难度。", "translation": "堆叠面积图是一种广泛用于数值时间序列的可视化技术。x轴代表时间，时间序列显示为水平、可变高度的层，彼此堆叠。每层的高度对应于每个时间点的时间序列值。优化堆叠面积图可读性的主要美学标准是可视化中时间序列之间边界的垂直变化量，称为“摆动”。尽管已经开发了许多启发式算法来最小化“摆动”，但最小化“摆动”的计算复杂性尚未得到正式分析。在本文中，我们证明了“摆动”最小化的不同变体是NP-难的，甚至难以近似。我们还提出了一个精确的混合整数线性规划公式，并在实验评估中将其性能与最先进的启发式算法进行了比较。最后，我们考虑了“摆动”最小化的一种特殊情况，它对应于对一组数字进行排序以最小化其绝对前缀和之和这一根本上有趣且自然的问题。我们展示了该问题的几个复杂性结果，这些结果也暗示了“摆动”最小化的某些难度结果。", "summary": "本文首次对堆叠面积图中最小化“摆动”的计算复杂性进行了正式分析。研究发现，最小化“摆动”的各种变体是NP-难的，甚至难以近似。为了解决这一问题，论文提出了一个精确的混合整数线性规划公式，并与现有启发式算法进行了性能对比。此外，通过分析一个特殊的数学排序问题（最小化绝对前缀和之和），进一步揭示了“摆动”最小化的内在计算难度。", "keywords": "堆叠面积图, 摆动最小化, NP-难, 混合整数线性规划, 计算复杂性", "comments": "本文的创新之处在于首次从理论计算复杂性角度深入分析了堆叠面积图“摆动”最小化问题，填补了现有研究主要集中于启发式算法的空白。证明NP-难性对于理解问题本质和指导未来算法设计具有重要意义。提出的混合整数线性规划公式为寻找精确解提供了一条途径，尽管计算成本可能较高。对特殊情况的分析进一步增强了结论的普适性。"}}
{"id": "2506.21302", "title": "Coordinated Control of Autonomous Vehicles for Traffic Density Reduction at a Signalized Junction: An MPC Approach", "authors": ["Rudra Sen", "Subashish Datta"], "summary": "The effective and safe management of traffic is a key issue due to the rapid\nadvancement of the urban transportation system. Connected autonomous vehicles\n(CAVs) possess the capability to connect with each other and adjacent\ninfrastructure, presenting novel opportunities for enhancing traffic flow and\ncoordination. This work proposes a dual-mode model predictive control (MPC)\narchitecture that tackles two interrelated issues: mitigating traffic density\nat signalized junctions and facilitating seamless, cooperative lane changes in\nhigh-density traffic conditions. The objective of this work is to facilitate\nresponsive decision-making for CAVs, thereby enhancing the efficiency and\nsafety of urban mobility. Moreover, we ensure recursive feasibility and\nconvergence of the proposed MPC scheme by the integration of an\nonline-calculated maximal control invariant terminal set. Finally, the efficacy\nof the proposed approach is validated through numerical simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21302v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21302v1", "AI": {"title_translation": "信号交叉口交通密度降低的自动驾驶车辆协同控制：一种MPC方法", "tldr": "本文提出了一种双模态模型预测控制（MPC）架构，用于在信号交叉口降低交通密度并促进车联网自动驾驶车辆（CAVs）的协同变道，并通过数值仿真验证了其有效性。", "motivation": "由于城市交通系统的快速发展，有效和安全地管理交通是一个关键问题。车联网自动驾驶车辆（CAVs）能够相互连接并与相邻基础设施连接，为增强交通流和协调性提供了新机遇。本研究旨在通过为CAVs提供响应式决策，从而提高城市出行的效率和安全性。", "method": "本文提出了一种双模态模型预测控制（MPC）架构，旨在解决信号交叉口的交通密度缓解和高密度交通条件下的无缝协同变道这两个相互关联的问题。此外，通过集成在线计算的最大控制不变终端集，确保了所提出的MPC方案的递归可行性和收敛性。", "result": "所提出的方法通过数值仿真验证了其有效性。", "conclusion": "本研究旨在通过为车联网自动驾驶车辆（CAVs）提供响应式决策，从而提高城市出行的效率和安全性。", "translation": "城市交通系统的快速发展使得交通的有效和安全管理成为一个关键问题。车联网自动驾驶车辆（CAVs）具备相互连接和与相邻基础设施连接的能力，为增强交通流和协调性带来了新的机遇。这项工作提出了一种双模态模型预测控制（MPC）架构，解决了两个相互关联的问题：缓解信号交叉口的交通密度，以及在高密度交通条件下促进无缝、协作的变道。这项工作的目标是促进CAVs的响应式决策，从而提高城市出行的效率和安全性。此外，我们通过集成在线计算的最大控制不变终端集，确保了所提出的MPC方案的递归可行性和收敛性。最后，通过数值仿真验证了所提出方法的有效性。", "summary": "本文提出了一种双模态模型预测控制（MPC）架构，用于车联网自动驾驶车辆（CAVs）在信号交叉口进行交通管理。该架构旨在降低交通密度并促进高密度条件下的协同变道，最终提升城市出行的效率和安全性。该MPC方案通过集成在线计算的最大控制不变终端集来确保递归可行性和收敛性，并通过数值仿真验证了其有效性。", "keywords": "车联网自动驾驶车辆, 模型预测控制, 交通密度降低, 信号交叉口, 协同变道", "comments": "该论文利用车联网自动驾驶车辆（CAVs）的能力，解决了城市交通中的一个重要问题。其创新点在于提出了一种双模态MPC架构，并通过集成在线计算的最大控制不变终端集来确保其递归可行性和收敛性，这增强了方法的理论严谨性。然而，方法的有效性目前仅通过数值仿真验证，实际部署可能面临更多挑战。"}}
{"id": "2506.20970", "title": "Co-Design of Sensing, Communications, and Control for Low-Altitude Wireless Networks", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Fan Liu", "Yuanhao Cui"], "summary": "The rapid advancement of Internet of Things (IoT) services and the evolution\ntoward the sixth generation (6G) have positioned unmanned aerial vehicles\n(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This\nwork investigates the co-design of integrated sensing, communication, and\ncontrol ($\\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite\nblocklength (FBL) transmission. In particular, the UAVs continuously monitor\nthe state of the field robots and transmit their observations to the robot\ncontroller to ensure stable control while cooperating to localize an unknown\nsensing target (ST). To this end, a weighted optimization problem is first\nformulated by jointly considering the control and localization performance in\nterms of the linear quadratic regulator (LQR) cost and the determinant of the\nFisher information matrix (FIM), respectively. The resultant problem,\noptimizing resource allocations, the UAVs' deployment positions, and multi-user\nscheduling, is non-convex. To circumvent this challenge, we first derive a\nclosed-form expression of the LQR cost with respect to other variables.\nSubsequently, the non-convex optimization problem is decomposed into a series\nof sub-problems by leveraging the alternating optimization (AO) approach, in\nwhich the difference of convex functions (DC) programming and projected\ngradient descent (PGD) method are employed to obtain an efficient near-optimal\nsolution. Furthermore, the convergence and computational complexity of the\nproposed algorithm are thoroughly analyzed. Extensive simulation results are\npresented to validate the effectiveness of our proposed approach compared to\nthe benchmark schemes and reveal the trade-off between control and sensing\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20970v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20970v1", "AI": {"title_translation": "低空无线网络中传感、通信与控制的协同设计", "tldr": "本文研究了低空无线网络中多无人机协作系统的传感、通信与控制（SC²）协同设计。通过构建一个加权优化问题，平衡控制稳定性和定位精度，并提出了一种基于交替优化（AO）、凸函数差分（DC）规划和投影梯度下降（PGD）的非凸问题求解方法，以实现资源分配、无人机部署和调度优化。", "motivation": "物联网（IoT）服务的快速发展和向第六代（6G）的演进，使得无人机（UAVs）成为低空无线网络（LAWNs）的关键使能者。为了确保稳定控制和定位未知传感目标，需要对多无人机协作系统中的集成传感、通信和控制（SC²）进行协同设计。", "method": "本文首先将控制性能（通过线性二次调节器LQR成本）和定位性能（通过费舍尔信息矩阵FIM的行列式）联合考虑，构建了一个加权优化问题。为了解决这个非凸问题，首先推导了LQR成本的闭式表达式，然后利用交替优化（AO）方法将其分解为一系列子问题，并采用凸函数差分（DC）规划和投影梯度下降（PGD）方法来获得高效的近最优解。此外，还分析了所提出算法的收敛性和计算复杂性。", "result": "大量的仿真结果验证了所提出方法相对于基准方案的有效性，并揭示了控制和传感性能之间的权衡。", "conclusion": "本文提出的协同设计方法有效地集成了多无人机系统中的传感、通信和控制，并通过仿真结果证明了其有效性，同时揭示了控制稳定性与传感精度之间的性能权衡。", "translation": "物联网（IoT）服务的快速发展和向第六代（6G）的演进，已将无人机（UAV）定位为低空无线网络（LAWNs）的关键使能者。这项工作研究了多无人机协作系统中集成传感、通信和控制（SC²）的协同设计，并采用有限块长（FBL）传输。具体而言，无人机持续监测地面机器人的状态并将其观测结果传输给机器人控制器，以确保稳定控制，同时协作定位未知传感目标（ST）。为此，首先通过联合考虑线性二次调节器（LQR）成本和费舍尔信息矩阵（FIM）行列式来衡量控制和定位性能，从而构建了一个加权优化问题。由此产生的问题，即优化资源分配、无人机部署位置和多用户调度，是非凸的。为了规避这一挑战，我们首先推导了LQR成本相对于其他变量的闭式表达式。随后，利用交替优化（AO）方法将非凸优化问题分解为一系列子问题，其中采用凸函数差分（DC）规划和投影梯度下降（PGD）方法来获得高效的近最优解。此外，本文还彻底分析了所提出算法的收敛性和计算复杂性。大量的仿真结果验证了我们提出的方法与基准方案相比的有效性，并揭示了控制和传感性能之间的权衡。", "summary": "本文针对物联网和6G背景下低空无线网络中多无人机协作系统，提出了集成传感、通信与控制（SC²）的协同设计。研究构建了一个非凸加权优化问题，以平衡控制稳定性（LQR成本）和定位精度（FIM行列式）。为解决该问题，论文提出了一种基于交替优化（AO）的方法，并结合凸函数差分（DC）规划和投影梯度下降（PGD）技术来获得近最优解。仿真结果验证了所提方法的有效性，并揭示了控制与传感性能之间的权衡关系。", "keywords": "无人机, 协同设计, SC², 低空无线网络, 优化", "comments": "本文提出了一种新颖的SC²协同设计框架，用于多无人机系统，这对于未来的物联网和6G应用具有高度相关性。通过结合LQR和FIM的加权优化问题以及利用AO结合DC/PGD来解决复杂的非凸问题，是解决该领域的创新方法。对控制和传感性能之间权衡的分析也提供了一个有价值的贡献。"}}
{"id": "2506.21171", "title": "Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations", "authors": ["Jing Yang", "Qunliang Xing", "Mai Xu", "Minglang Qiao"], "summary": "Joint Photographic Experts Group (JPEG) achieves data compression by\nquantizing Discrete Cosine Transform (DCT) coefficients, which inevitably\nintroduces compression artifacts. Most existing JPEG quality enhancement\nmethods operate in the pixel domain, suffering from the high computational\ncosts of decoding. Consequently, direct enhancement of JPEG images in the DCT\ndomain has gained increasing attention. However, current DCT-domain methods\noften exhibit limited performance. To address this challenge, we identify two\ncritical types of correlations within the DCT coefficients of JPEG images.\nBuilding on this insight, we propose an Advanced DCT-domain JPEG Quality\nEnhancement (AJQE) method that fully exploits these correlations. The AJQE\nmethod enables the adaptation of numerous well-established pixel-domain models\nto the DCT domain, achieving superior performance with reduced computational\ncomplexity. Compared to the pixel-domain counterparts, the DCT-domain models\nderived by our method demonstrate a 0.35 dB improvement in PSNR and a 60.5%\nincrease in enhancement throughput on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21171v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21171v1", "AI": {"title_translation": "在DCT中发掘宝藏：通过利用潜在相关性推进JPEG图像质量增强", "tldr": "本文提出了一种在DCT域直接增强JPEG图像质量的新方法AJQE，通过利用DCT系数中的潜在相关性，实现了比像素域方法更好的性能和更低的计算成本。", "motivation": "JPEG压缩会引入伪影；现有像素域增强方法计算成本高；现有DCT域方法性能有限。", "method": "识别JPEG图像DCT系数中的两种关键相关性，并提出高级DCT域JPEG质量增强（AJQE）方法，充分利用这些相关性，使像素域模型能适应DCT域。", "result": "相比像素域方法，AJQE在PSNR上平均提升0.35 dB，增强吞吐量平均增加60.5%。", "conclusion": "通过利用DCT系数的内在相关性，可以在DCT域高效且高性能地进行JPEG图像质量增强，且能将现有像素域模型迁移到DCT域。", "translation": "联合图像专家组（JPEG）通过量化离散余弦变换（DCT）系数实现数据压缩，这不可避免地引入了压缩伪影。大多数现有的JPEG质量增强方法在像素域操作，受限于解码带来的高计算成本。因此，直接在DCT域对JPEG图像进行增强受到了越来越多的关注。然而，当前的DCT域方法通常表现出有限的性能。为了解决这一挑战，我们识别了JPEG图像DCT系数中的两种关键相关性。基于这一洞察，我们提出了一种高级DCT域JPEG质量增强（AJQE）方法，该方法充分利用了这些相关性。AJQE方法使得许多成熟的像素域模型能够适应到DCT域，以更低的计算复杂度实现卓越的性能。与像素域的对应方法相比，我们方法导出的DCT域模型在PSNR上平均提高了0.35 dB，增强吞吐量平均增加了60.5%。", "summary": "本文针对JPEG图像压缩伪影问题，提出了一种名为AJQE的先进DCT域质量增强方法。该方法通过识别并利用DCT系数中的关键相关性，克服了现有像素域方法计算成本高和DCT域方法性能有限的缺点。AJQE能够将成熟的像素域模型有效地迁移到DCT域，从而在提升图像质量（PSNR提高0.35 dB）的同时，显著提高增强效率（吞吐量增加60.5%）。", "keywords": "JPEG质量增强, DCT域, 相关性, 压缩伪影, AJQE", "comments": "这项工作创新性在于深入挖掘DCT域的内在相关性，并成功将其应用于JPEG质量增强，解决了长久以来像素域方法计算开销大和DCT域方法性能不足的痛点。其能够使现有像素域模型在DCT域复用，具有很强的实用价值和潜在影响力。"}}
{"id": "2506.21425", "title": "IDGraphs: Intrusion Detection and Analysis Using Stream Compositing", "authors": ["Pin Ren", "Yan Gao", "Zhichun Li", "Yan Chen", "Benjamin Watson"], "summary": "Traffic anomalies and attacks are commonplace in today's networks and\nidentifying them rapidly and accurately is critical for large network\noperators. For a statistical intrusion detection system (IDS), it is crucial to\ndetect at the flow-level for accurate detection and mitigation. However,\nexisting IDS systems offer only limited support for 1) interactively examining\ndetected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)\nand discovering correlated attacks. These problems are becoming even more acute\nas the traffic on today's high-speed routers continues to grow.\n  IDGraphs is an interactive visualization system for intrusion detection that\naddresses these challenges. The central visualization in the system is a\nflow-level trace plotted with time on the horizontal axis and aggregated number\nof unsuccessful connections on the vertical axis. We then summarize a stack of\ntens or hundreds of thousands of these traces using the Histographs [RW05]\ntechnique, which maps data frequency at each pixel to brightness. Users may\nthen interactively query the summary view, performing analysis by highlighting\nsubsets of the traces. For example, brushing a linked correlation matrix view\nhighlights traces with similar patterns, revealing distributed attacks that are\ndifficult to detect using standard statistical analysis.\n  We apply IDGraphs system to a real network router data-set with 179M\nflow-level records representing a total traffic of 1.16TB. The system\nsuccessfully detects and analyzes a variety of attacks and anomalies, including\nport scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21425v1", "categories": ["cs.GR", "cs.CR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21425v1", "AI": {"title_translation": "IDGraphs：使用流合成的入侵检测与分析", "tldr": "IDGraphs是一个交互式可视化系统，用于通过流合成技术检测和分析网络入侵，解决了现有IDS在交互性、蠕虫传播分析和关联攻击发现方面的不足，并在真实数据集上成功检测多种攻击。", "motivation": "现有入侵检测系统在交互式检查、分析蠕虫传播模式和发现关联攻击方面支持有限，且在高速网络流量增长下问题日益严重，快速准确地识别流量异常和攻击对大型网络运营商至关重要。", "method": "IDGraphs是一个交互式可视化系统。它通过在水平轴上绘制时间、垂直轴上绘制聚合的未成功连接数来生成流级别的跟踪。系统使用Histographs技术将数万个此类跟踪堆栈汇总，将每个像素的数据频率映射到亮度。用户可以交互式查询汇总视图，通过突出显示跟踪子集进行分析，例如，刷选关联矩阵视图可突出显示具有相似模式的跟踪，从而揭示难以通过标准统计分析检测到的分布式攻击。", "result": "将IDGraphs系统应用于包含1.79亿流级别记录（总流量1.16TB）的真实网络路由器数据集。系统成功检测并分析了多种攻击和异常，包括端口扫描、蠕虫爆发、隐蔽的TCP SYN洪泛以及一些分布式攻击。", "conclusion": "IDGraphs系统通过其创新的交互式可视化方法，有效解决了现有入侵检测系统在处理高速网络流量和复杂攻击模式方面的局限性，实现了对多种网络攻击和异常的成功检测与分析。", "translation": "今天的网络中，流量异常和攻击司空见惯，对大型网络运营商而言，快速准确地识别它们至关重要。对于统计入侵检测系统（IDS）来说，在流级别进行检测对于准确的检测和缓解至关重要。然而，现有IDS系统仅提供有限的支持，用于：1) 交互式检查已检测到的入侵和异常，2) 分析蠕虫传播模式，3) 发现关联攻击。随着当今高速路由器上的流量持续增长，这些问题变得更加严峻。\nIDGraphs是一个交互式可视化系统，用于入侵检测，旨在解决这些挑战。该系统的核心可视化是一个流级别的跟踪图，其中时间在水平轴上，聚合的未成功连接数在垂直轴上。然后，我们使用Histographs [RW05]技术总结了数万个此类跟踪的堆栈，该技术将每个像素的数据频率映射到亮度。用户可以交互式查询汇总视图，通过突出显示跟踪子集进行分析。例如，刷选链接的关联矩阵视图会突出显示具有相似模式的跟踪，从而揭示难以使用标准统计分析检测到的分布式攻击。\n我们将IDGraphs系统应用于一个真实的网络路由器数据集，该数据集包含1.79亿条流级别记录，代表总流量1.16TB。该系统成功检测并分析了各种攻击和异常，包括端口扫描、蠕虫爆发、隐蔽的TCP SYN洪泛以及一些分布式攻击。", "summary": "本文提出IDGraphs，一个交互式入侵检测可视化系统，旨在解决现有IDS在高速网络中对复杂攻击（如蠕虫传播和关联攻击）交互式分析的局限性。IDGraphs通过流级别跟踪和Histographs技术对大量数据进行可视化汇总，并允许用户交互式查询和分析。在真实网络数据集上的应用表明，IDGraphs能有效检测并分析包括端口扫描、蠕虫爆发和分布式攻击在内的多种流量异常。", "keywords": "入侵检测, 可视化, 流合成, 网络安全, 异常检测", "comments": "IDGraphs的创新之处在于其将流合成和Histographs技术应用于入侵检测的可视化分析，提供了一种交互式的方法来处理大规模网络流量数据。其重要性体现在解决了现有IDS在处理复杂、分布式攻击和提供深入交互式分析方面的不足。它通过可视化手段，使得操作员能够更直观地发现和理解网络中的异常模式，尤其是在识别关联攻击和蠕虫传播方面。抽象中未明确提及限制，但可以推断其可能需要一定的可视化专业知识来有效利用，且对实时性要求极高的场景可能需要进一步优化。"}}
{"id": "2506.21032", "title": "RecCoT: Enhancing Recommendation via Chain-of-Thought", "authors": ["Shuo Yang", "Jiangxia Cao", "Haipeng Li", "Yuqi Mao", "Shuchao Pang"], "summary": "In real-world applications, users always interact with items in multiple\naspects, such as through implicit binary feedback (e.g., clicks, dislikes, long\nviews) and explicit feedback (e.g., comments, reviews). Modern recommendation\nsystems (RecSys) learn user-item collaborative signals from these implicit\nfeedback signals as a large-scale binary data-streaming, subsequently\nrecommending other highly similar items based on users' personalized historical\ninteractions. However, from this collaborative-connection perspective, the\nRecSys does not focus on the actual content of the items themselves but instead\nprioritizes higher-probability signals of behavioral co-occurrence among items.\nConsequently, under this binary learning paradigm, the RecSys struggles to\nunderstand why a user likes or dislikes certain items. To alleviate it, some\nworks attempt to utilize the content-based reviews to capture the semantic\nknowledge to enhance recommender models. However, most of these methods focus\non predicting the ratings of reviews, but do not provide a human-understandable\nexplanation.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.21032v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21032v1", "AI": {"title_translation": "RecCoT：通过思维链增强推荐", "tldr": "现有推荐系统难以理解用户偏好背后的原因，因为它们侧重于行为共现而非内容语义。一些方法尝试利用评论内容，但未能提供人类可理解的解释。", "motivation": "现有推荐系统主要通过隐式反馈学习用户-物品协作信号，但它们侧重于行为共现而非物品内容本身，导致难以理解用户喜欢或不喜欢某些物品的原因。尽管一些工作尝试利用基于内容的评论来捕获语义知识以增强推荐模型，但这些方法大多关注预测评论评分，未能提供人类可理解的解释。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在现实应用中，用户总是通过多个方面与物品互动，例如通过隐式二元反馈（如点击、不喜欢、长时间观看）和显式反馈（如评论、评价）。现代推荐系统（RecSys）从这些隐式反馈信号中学习用户-物品协作信号，作为大规模二元数据流，随后根据用户的个性化历史互动推荐其他高度相似的物品。然而，从这种协作连接的角度来看，推荐系统并不关注物品本身的实际内容，而是优先考虑物品之间行为共现的更高概率信号。因此，在这种二元学习范式下，推荐系统难以理解用户喜欢或不喜欢某些物品的原因。为了缓解这个问题，一些工作尝试利用基于内容的评论来捕获语义知识以增强推荐模型。然而，这些方法大多侧重于预测评论评分，但未能提供人类可理解的解释。", "summary": "该论文指出，当前推荐系统在理解用户偏好原因方面存在局限性，因为它们主要依赖于行为共现信号，而非物品内容。尽管有研究尝试利用评论内容来增强模型，但这些方法通常只预测评分，缺乏可解释性。", "keywords": "推荐系统, 思维链, 可解释性, 用户偏好, 评论分析", "comments": "该论文旨在解决现有推荐系统在理解用户偏好原因方面的不足，并指出现有基于评论的方法未能提供人类可理解的解释。其创新点在于提出“思维链”来增强推荐，这暗示了对推荐系统可解释性或推理能力的关注，是一个重要的研究方向。"}}
{"id": "2506.20876", "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20876v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20876v1", "AI": {"title_translation": "少做决策，多做沟通：论医学领域端到端事实核查的构念效度", "tldr": "医学领域的端到端事实核查系统因其在连接声明与证据、处理歧义和主观性方面的挑战而未被充分利用。研究表明，事实核查应被视为一个交互式沟通问题，而非简单的端到端过程。", "motivation": "尽管自动化事实核查技术取得了进步，并且在公共卫生和医学领域的需求日益增长（由于医疗决策的高风险性质和评估大量医学文献的挑战），但现有的端到端事实核查系统仍未被充分利用。研究旨在理解为何这些系统在医学领域中未能被广泛采用。", "method": "本研究首次通过调查临床专家如何综合医学证据来核实社交媒体上的真实声明，以探究端到端事实核查的上限。", "result": "研究揭示了在医学领域应用端到端事实核查的根本挑战：难以将网络上的声明与临床试验形式的科学证据联系起来；不明确的声明与不匹配的意图混合导致的歧义；以及固有的主观真实性标签。", "conclusion": "研究认为，事实核查应被视为一个交互式沟通问题来处理和评估，而非一个简单的端到端过程。", "translation": "技术进步使得曾被认为是挑战性的任务取得了具体进展，例如自动化事实核查。由于医疗决策的高风险性质以及批判性评估大量多样化医学文献的挑战，在公共卫生和医学领域采用这些系统的兴趣日益增长。循证医学与每个人息息相关，但其本质是高度技术性的，导致大多数用户的医学素养不足以充分驾驭该领域。医学沟通中的此类问题为端到端事实核查代理奠定了基础：针对当前的医学文献核查声明并返回一个有证据支持的结论。然而，此类系统仍未被广泛使用。为了理解这一点，我们提出了第一个研究，通过综合医学证据来检查临床专家如何核实社交媒体上的真实声明。在寻找这个上限的过程中，我们揭示了在医学领域应用端到端事实核查的根本挑战：难以将网络上的声明与临床试验形式的科学证据联系起来；不明确的声明与不匹配的意图混合导致的歧义；以及固有的主观真实性标签。我们认为，事实核查应被视为一个交互式沟通问题来处理和评估，而非一个端到端过程。", "summary": "本研究探讨了医学领域端到端事实核查系统未被广泛采用的原因。通过分析临床专家如何核实社交媒体上的医疗声明，研究揭示了现有系统在连接声明与证据、处理歧义和主观性方面的根本挑战。作者提出，事实核查应被视为一个交互式沟通问题，而非一个端到端过程。", "keywords": "事实核查, 医学, 端到端系统, 沟通, 构念效度", "comments": "该论文对医学领域的事实核查提出了重要的批判性视角，指出当前端到端方法的局限性。其创新之处在于将事实核查重新定义为一个交互式沟通问题，这为未来的研究和系统开发提供了新的方向。该研究的发现对于理解自动化系统在复杂、高风险领域（如医疗保健）中的应用瓶颈具有重要意义。"}}
{"id": "2506.21440", "title": "Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform", "authors": ["Maxime Leiber", "Yosra Marnissi", "Axel Barrau", "Sylvain Meignen", "Laurent Massoulié"], "summary": "The short-time Fourier transform (STFT) is widely used for analyzing\nnon-stationary signals. However, its performance is highly sensitive to its\nparameters, and manual or heuristic tuning often yields suboptimal results. To\novercome this limitation, we propose a unified differentiable formulation of\nthe STFT that enables gradient-based optimization of its parameters. This\napproach addresses the limitations of traditional STFT parameter tuning\nmethods, which often rely on computationally intensive discrete searches. It\nenables fine-tuning of the time-frequency representation (TFR) based on any\ndesired criterion. Moreover, our approach integrates seamlessly with neural\nnetworks, allowing joint optimization of the STFT parameters and network\nweights. The efficacy of the proposed differentiable STFT in enhancing TFRs and\nimproving performance in downstream tasks is demonstrated through experiments\non both simulated and real-world data.", "comment": "DSTFT, STFT, spectrogram, time-frequency, IEEE Transactions on Signal\n  Processing, 10 pages", "pdf_url": "http://arxiv.org/pdf/2506.21440v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21440v1", "AI": {"title_translation": "可学习自适应时频表示通过可微分短时傅里叶变换", "tldr": "提出了一种可微分的短时傅里叶变换(STFT)，解决了传统STFT参数调优困难的问题，实现了基于梯度的优化，并能与神经网络联合优化，有效提升了时频表示和下游任务性能。", "motivation": "传统的短时傅里叶变换（STFT）在分析非平稳信号时广泛使用，但其性能对参数高度敏感，且手动或启发式调优常导致次优结果，现有方法依赖计算密集型离散搜索。", "method": "提出了一种统一的可微分STFT公式，通过梯度优化其参数，克服了传统离散搜索的局限性。该方法支持基于任意准则对时频表示（TFR）进行微调，并能与神经网络无缝集成，实现STFT参数和网络权重的联合优化。", "result": "实验证明，所提出的可微分STFT在增强时频表示（TFR）和改进下游任务性能方面是有效的，并在仿真数据和真实世界数据上都得到了验证。", "conclusion": "通过引入可微分的短时傅里叶变换，可以实现STFT参数的梯度优化，有效克服传统调优的局限性，提升时频表示的质量以及下游任务的性能，并能与深度学习模型结合。", "translation": "短时傅里叶变换（STFT）广泛应用于非平稳信号分析。然而，其性能对其参数高度敏感，手动或启发式调优常导致次优结果。为了克服这一局限性，我们提出了一种统一的可微分STFT公式，该公式支持基于梯度的参数优化。这种方法解决了传统STFT参数调优方法的局限性，这些方法通常依赖于计算密集型的离散搜索。它能够根据任何期望的准则对时频表示（TFR）进行微调。此外，我们的方法与神经网络无缝集成，允许STFT参数和网络权重的联合优化。通过在模拟数据和真实世界数据上的实验，证明了所提出的可微分STFT在增强TFR和改进下游任务性能方面的有效性。", "summary": "本文提出了一种创新的可微分短时傅里叶变换（STFT）方法，旨在解决传统STFT参数调优困难且效率低下的问题。通过引入统一的可微分公式，该方法实现了STFT参数的梯度优化，避免了计算密集型离散搜索。它允许根据特定准则对时频表示（TFR）进行精细调整，并能与神经网络无缝结合，实现参数和网络权重的联合优化。实验结果表明，该可微分STFT能有效提升TFR质量，并改善下游任务的表现。", "keywords": "短时傅里叶变换, 可微分, 时频表示, 参数优化, 神经网络", "comments": "这篇论文的创新点在于将短时傅里叶变换（STFT）参数调优问题转化为可微分的形式，从而能够利用梯度下降进行优化，极大地提高了效率和性能。它解决了传统STFT参数选择的痛点，并为信号处理与深度学习的结合提供了新的范式，特别是其与神经网络的无缝集成能力，有望在各种非平稳信号处理任务中发挥重要作用。"}}
{"id": "2506.21065", "title": "Entropy-stable in- and outflow boundary conditions for the compressible Navier-Stokes equations", "authors": ["Magnus Svärd", "Anita Gjesteland"], "summary": "We propose inflow and outflow boundary conditions for the compressible\nNavier-Stokes equations and prove that they allow a priori estimates of the\nentropy, mass and total energy. Furthermore, we demonstrate how to approximate\nthese boundary conditions in conjunction with an entropy-stable finite-volume\nscheme. The method is also applicable to other types of entropy-stable schemes.\nFinally, we carry out some numerical computations with the finite-volume scheme\nand demonstrate their robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21065v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21065v1", "AI": {"title_translation": "可压缩Navier-Stokes方程的熵稳定进出口边界条件", "tldr": "本文提出并证明了可压缩Navier-Stokes方程的熵稳定进出口边界条件，并展示了其与熵稳定有限体积方案的结合应用及稳健性。", "motivation": "开发并证明可压缩Navier-Stokes方程的进出口边界条件，使其能够实现熵、质量和总能量的先验估计，并确保其稳定性。", "method": "提出进出口边界条件，并证明其允许熵、质量和总能量的先验估计。展示了如何将这些边界条件与熵稳定有限体积方案结合使用，并指出该方法也适用于其他类型的熵稳定方案。最后，通过有限体积方案进行数值计算以验证其稳健性。", "result": "所提出的边界条件允许对熵、质量和总能量进行先验估计。数值计算结果证明了这些边界条件的稳健性。", "conclusion": "所提出的进出口边界条件对于可压缩Navier-Stokes方程是熵稳定的，并且能够保证熵、质量和总能量的先验估计，同时在数值计算中表现出良好的稳健性。", "translation": "我们提出了可压缩Navier-Stokes方程的进出口边界条件，并证明它们允许对熵、质量和总能量进行先验估计。此外，我们展示了如何将这些边界条件与熵稳定有限体积方案结合起来近似处理。该方法也适用于其他类型的熵稳定方案。最后，我们使用有限体积方案进行了一些数值计算，并证明了它们的稳健性。", "summary": "本文针对可压缩Navier-Stokes方程，提出了一套熵稳定的进出口边界条件。研究证明这些条件能够实现熵、质量和总能量的先验估计。此外，文中详细阐述了如何将这些边界条件与熵稳定有限体积方案相结合，并指出其同样适用于其他熵稳定数值方法。通过数值模拟，验证了所提边界条件的稳健性。", "keywords": "熵稳定, 边界条件, Navier-Stokes方程, 有限体积, 先验估计", "comments": "本文的核心创新在于提出了可压缩Navier-Stokes方程的熵稳定进出口边界条件，并从理论上证明了其能够保证物理量的先验估计。其重要性在于为计算流体力学中的边界处理提供了一种理论基础更强、稳定性更好的方法，尤其对于需要精确能量守恒和熵耗散控制的复杂流动问题具有重要意义。数值验证进一步证实了其在实际应用中的可行性和稳健性。"}}
{"id": "2506.21211", "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models", "authors": ["Quanming Liu", "Xupeng Bu", "Zhichao Yan", "Ru Li"], "summary": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21211v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21211v1", "AI": {"title_translation": "$T^3$: 基于多级树的大语言模型自动程序修复", "tldr": "提出 $T^3$ 框架，结合大语言模型和树搜索，提升自动程序修复的精度和效率。", "motivation": "尽管大语言模型（LLMs）和思维链（CoT）技术取得了显著进展，但由于自动程序修复（APR）任务需要复杂的逻辑和多步推理能力，CoT技术在APR领域的应用仍显不足。", "method": "本研究系统评估了几种常见思维链（CoT）技术在自动程序修复（APR）任务中的表现，并提出了一个创新的 $T^3$ 框架，该框架将大语言模型（LLMs）的强大推理能力与树搜索相结合。", "result": "$T^3$ 有效提高了生成候选修复方案的精度，并为自动程序修复（APR）任务中的样本选择和修复策略优化提供了有价值的指导。", "conclusion": "$T^3$ 为实现高效的自动化调试建立了一个稳健的框架。", "translation": "自动程序修复（APR）是软件开发和维护中的一项核心技术，旨在以最少的人工干预实现自动化缺陷修复。近年来，大型语言模型（LLMs）和思维链（CoT）技术的巨大进步显著增强了这些模型的推理能力。然而，由于APR领域所需的复杂逻辑和多步推理能力，CoT技术在该领域的应用仍然不足。本研究系统评估了几种常见CoT技术在APR任务中的性能，并提出了一个创新的框架$T^3$，该框架将LLMs强大的推理能力与树搜索相结合，有效提高了生成候选修复方案的精度。此外，$T^3$ 为优化APR任务中的样本选择和修复策略提供了宝贵的指导，为实现高效的自动化调试建立了一个稳健的框架。", "summary": "本文针对自动程序修复（APR）中思维链（CoT）技术应用不足的问题，提出了一个名为 $T^3$ 的创新框架。该框架将大型语言模型（LLM）的推理能力与树搜索相结合，旨在提高生成候选修复方案的精度。研究表明，$T^3$ 不仅提升了修复精度，还为APR任务的样本选择和修复策略优化提供了指导，最终为高效自动化调试构建了一个稳健的框架。", "keywords": "自动程序修复, 大语言模型, 思维链, 树搜索, 自动化调试", "comments": "该论文的创新之处在于将大语言模型的强大推理能力与树搜索机制相结合，以解决自动程序修复中复杂逻辑和多步推理的挑战。这种结合有望显著提升修复方案的生成精度，为软件开发中的自动化调试提供新的有效途径，具有重要的实践意义。"}}
{"id": "2506.21333", "title": "A Systematic Review of Human-AI Co-Creativity", "authors": ["Saloni Singh", "Koen Hndriks", "Drik Heylen", "Kim Baraka"], "summary": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21333v1", "categories": ["cs.HC", "cs.AI", "I.2.11"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21333v1", "AI": {"title_translation": "人机协同创造的系统综述", "tldr": "本文对62篇关于协同创造系统的论文进行了系统综述，识别了关键设计维度和24个设计考量，发现高用户控制和适应性主动系统能提升用户满意度和协作，但也存在早期创意阶段支持有限等挑战。", "motivation": "协同创造社区在开发更复杂和定制化的系统以支持和增强人类创造力方面取得了显著进展。先前的设计考量可以为未来的系统提供有价值且高效的基础。为支持这项工作，本文旨在对协同创造系统进行系统文献综述。", "method": "本文对62篇关于协同创造系统的论文进行了系统文献综述。这些论文涵盖了视觉艺术、设计和写作等多种应用，其中AI不仅作为工具，而且作为创意过程中的积极协作者。", "result": "从综述中，本文识别了几个与系统设计相关的关键维度：创意过程的阶段、创意任务、系统的主动行为、用户控制、系统体现和AI模型类型。研究结果表明，提供高用户控制的系统能带来更高的满意度、信任和对创意成果更强的归属感。此外，主动系统在适应性和上下文敏感时可以增强协作。本文还提取了24个设计考量，强调了鼓励用户外化思想以及增加系统的社会存在感和透明度以培养信任的价值。", "conclusion": "尽管最近取得了进展，但仍存在重要空白，例如对问题澄清等早期创意阶段的支持有限，以及用户适应AI系统相关的挑战。高用户控制和适应性主动系统对提升用户满意度和协作至关重要，未来的系统应关注填补现有空白。", "translation": "协同创造社区在开发更复杂和定制化的系统以支持和增强人类创造力方面取得了显著进展。先前的设计考量可以为未来的系统提供有价值且高效的基础。为支持这项工作，我们对62篇关于协同创造系统的论文进行了系统文献综述。这些论文涵盖了视觉艺术、设计和写作等多种应用，其中AI不仅作为工具，而且作为创意过程中的积极协作者。从本次综述中，我们识别了几个与系统设计相关的关键维度：创意过程的阶段、创意任务、系统的主动行为、用户控制、系统体现和AI模型类型。我们的发现表明，提供高用户控制的系统能带来更高的满意度、信任和对创意成果更强的归属感。此外，主动系统在适应性和上下文敏感时可以增强协作。我们还提取了24个设计考量，强调了鼓励用户外化思想以及增加系统的社会存在感和透明度以培养信任的价值。尽管最近取得了进展，但仍存在重要空白，例如对问题澄清等早期创意阶段的支持有限，以及用户适应AI系统相关的挑战。", "summary": "本文对62篇关于人机协同创造系统的文献进行了系统综述，旨在为未来系统设计提供基础。研究识别了创意过程阶段、任务、系统主动性、用户控制、系统体现和AI模型类型等关键设计维度，并提出了24个设计考量。结果表明，高用户控制能提升用户满意度和归属感，适应性主动系统能增强协作。同时，也指出了早期创意阶段支持不足和用户适应挑战等现有差距。", "keywords": "人机协同创造, 系统综述, 设计考量, 用户控制, 人工智能协作", "comments": "这是一篇有价值的系统综述，它系统地梳理了人机协同创造领域的设计考量和现有挑战。其创新之处在于提炼出关键的设计维度和具体的设计考量，为未来的协同创造系统开发提供了清晰的指导。论文强调了用户控制和系统主动性在协作中的重要性，并指出了该领域仍需关注的空白，如早期创意阶段的支持。这对于推动人机交互和人工智能在创意领域的应用具有重要意义。"}}
{"id": "2506.21030", "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner", "authors": ["Zhou Tianxing", "Wang Zhirui", "Ao Haojia", "Chen Guangyan", "Xing Boyang", "Cheng Jingwen", "Yang Yi", "Yue Yufeng"], "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21030v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21030v1", "AI": {"title_translation": "STEP规划器：构建跨层级子目标树作为具身长周期任务规划器", "tldr": "STEP规划器通过构建跨层级子目标树和使用闭环模型（子目标分解与叶节点终止），显著提高了机器人执行长周期具身任务的成功率，优于现有方法。", "motivation": "部署机器人在现实环境中需要可靠的长周期任务规划能力。然而，直接使用大型语言模型（LLMs）生成动作序列在长周期具身任务中成功率低，因为LLMs的推理能力有限。", "method": "STEP框架通过一对闭环模型构建一个子目标树：一个子目标分解模型和一个叶节点终止模型。该框架开发了一个从粗到细的分层树结构。子目标分解模型利用基础LLM将复杂目标分解为可管理的子目标，从而扩展子目标树。叶节点终止模型根据环境状态提供实时反馈，确定何时终止树的扩展，并确保每个叶节点都可以直接转换为原始动作。", "result": "在VirtualHome WAH-NL基准和真实机器人上的实验表明，STEP在长周期具身任务完成方面取得了高达34%（WAH-NL）和25%（真实机器人）的成功率，优于现有SOTA方法。", "conclusion": "STEP规划器通过其独特的跨层级子目标树结构和闭环模型，有效解决了机器人长周期具身任务规划中LLM推理能力不足的问题，显著提高了任务完成的成功率。", "translation": "能够执行可靠的长周期任务规划对于在现实世界环境中部署机器人至关重要。然而，直接使用大型语言模型（LLMs）作为动作序列生成器通常会导致成功率较低，因为它们对长周期具身任务的推理能力有限。在STEP框架中，我们通过一对闭环模型构建了一个子目标树：一个子目标分解模型和一个叶节点终止模型。在此框架内，我们开发了一个从粗到细的分层树结构。子目标分解模型利用基础LLM将复杂目标分解为可管理的子目标，从而扩展子目标树。叶节点终止模型根据环境状态提供实时反馈，确定何时终止树的扩展，并确保每个叶节点都可以直接转换为原始动作。在VirtualHome WAH-NL基准和真实机器人上进行的实验表明，STEP在长周期具身任务完成方面取得了高达34%（WAH-NL）和25%（真实机器人）的成功率，优于现有SOTA方法。", "summary": "本文提出了STEP规划器，旨在解决大型语言模型在长周期具身任务规划中推理能力不足导致成功率低的问题。STEP框架通过一对闭环模型（子目标分解模型和叶节点终止模型）构建了一个从粗到细的跨层级子目标树。子目标分解模型利用基础LLM分解复杂目标，而叶节点终止模型则提供实时反馈以确保叶节点可直接转换为原始动作。实验结果表明，STEP在虚拟和真实机器人环境中均显著提高了长周期具身任务的成功率，并优于现有最先进方法。", "keywords": "长周期任务规划, 具身机器人, 子目标树, 大型语言模型, 闭环模型", "comments": "STEP规划器通过引入跨层级的子目标树和一对闭环模型，为长周期具身任务规划提供了一个新颖且有效的解决方案。其创新点在于结合了LLM的分解能力和实时环境反馈，克服了LLM在处理复杂、长周期任务时的局限性。这种方法提高了机器人任务规划的可靠性和成功率，对于机器人实际部署具有重要意义。"}}
{"id": "2506.20921", "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach", "authors": ["Tong Zeng", "Srivathsan Badrinarayanan", "Janghoon Ock", "Cheng-Kai Lai", "Amir Barati Farimani"], "summary": "Chemical process optimization is crucial to maximize production efficiency\nand economic performance. Traditional methods, including gradient-based\nsolvers, evolutionary algorithms, and parameter grid searches, become\nimpractical when operating constraints are ill-defined or unavailable,\nrequiring engineers to rely on subjective heuristics to estimate feasible\nparameter ranges. To address this constraint definition bottleneck, we present\na multi-agent framework of large language model (LLM) agents that autonomously\ninfer operating constraints from minimal process descriptions, then\ncollaboratively guide optimization using the inferred constraints. Our\nAutoGen-based agentic framework employs OpenAI's o3 model, with specialized\nagents for constraint generation, parameter validation, simulation execution,\nand optimization guidance. Through two phases - autonomous constraint\ngeneration using embedded domain knowledge, followed by iterative multi-agent\noptimization - the framework eliminates the need for predefined operational\nbounds. Validated on the hydrodealkylation process across cost, yield, and\nyield-to-cost ratio metrics, the framework demonstrated competitive performance\nwith conventional optimization methods while achieving better computational\nefficiency, requiring fewer iterations to converge. Our approach converged in\nunder 20 minutes, achieving a 31-fold speedup over grid search. Beyond\ncomputational efficiency, the framework's reasoning-guided search demonstrates\nsophisticated process understanding, correctly identifying utility trade-offs,\nand applying domain-informed heuristics. This approach shows significant\npotential for optimization scenarios where operational constraints are poorly\ncharacterized or unavailable, particularly for emerging processes and retrofit\napplications.", "comment": "16 pages (main manuscript without references), 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20921v1", "categories": ["cs.LG", "cs.AI", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20921v1", "AI": {"title_translation": "LLM引导的多智能体化学过程优化", "tldr": "本文提出了一个基于LLM的多智能体框架，用于解决化学过程优化中操作约束不明确的问题。该框架能够自主推断约束并协作指导优化，无需预定义操作边界，实现了高效且性能优越的优化。", "motivation": "传统的化学过程优化方法在操作约束定义不清或不可用时效率低下，工程师需依赖主观经验。本文旨在解决这一“约束定义瓶颈”。", "method": "本文提出了一个基于AutoGen、采用OpenAI o3模型的多智能体LLM框架。该框架包含专门的智能体负责约束生成、参数验证、模拟执行和优化指导。它通过自主约束生成和迭代多智能体优化两个阶段，从最少的工艺描述中推断约束并指导优化，从而消除对预定义操作边界的需求。", "result": "在加氢脱烷基过程中的验证表明，该框架与传统优化方法性能相当，但计算效率更高，收敛所需的迭代次数更少。它在不到20分钟内收敛，比网格搜索加速31倍。此外，该框架展现出对过程的复杂理解，能正确识别效用权衡并应用领域知情的启发式方法。", "conclusion": "该方法在操作约束不明确或不可用的优化场景中，特别是对于新兴工艺和改造应用，显示出巨大的潜力。", "translation": "化学过程优化对于最大化生产效率和经济效益至关重要。传统方法，包括基于梯度的求解器、进化算法和参数网格搜索，在操作约束定义不清或不可用时变得不切实际，需要工程师依赖主观启发式方法来估计可行的参数范围。为了解决这个约束定义瓶颈，我们提出了一个大型语言模型（LLM）智能体多智能体框架，该框架能够从最少的工艺描述中自主推断操作约束，然后协同指导使用推断出的约束进行优化。我们基于AutoGen的智能体框架采用了OpenAI的o3模型，并设有专门的智能体用于约束生成、参数验证、模拟执行和优化指导。通过两个阶段——利用嵌入式领域知识进行自主约束生成，然后进行迭代多智能体优化——该框架消除了对预定义操作边界的需求。在加氢脱烷基过程的成本、产率和产率成本比指标上进行了验证，该框架表现出与传统优化方法相当的性能，同时实现了更好的计算效率，需要更少的迭代次数才能收敛。我们的方法在不到20分钟内收敛，比网格搜索加速了31倍。除了计算效率外，该框架的推理引导搜索展现了复杂的工艺理解，正确识别了效用权衡，并应用了领域知情的启发式方法。这种方法在操作约束特征不明确或不可用的优化场景中，特别是对于新兴工艺和改造应用，显示出巨大的潜力。", "summary": "本文提出了一个基于大型语言模型（LLM）的多智能体框架，旨在解决化学过程优化中操作约束定义不清的难题。该框架利用LLM智能体自主推断约束，并通过协作优化指导过程，无需预先定义操作边界。在加氢脱烷基过程中的验证表明，该方法在计算效率和性能上均优于传统方法，并能展现出对过程的深刻理解，尤其适用于约束信息不足的新兴或改造过程。", "keywords": "化学过程优化, 大型语言模型, 多智能体系统, 约束推断, 自动化优化", "comments": "该论文的创新点在于将LLM和多智能体系统应用于传统上依赖精确约束的化学过程优化，特别是解决了“约束定义瓶颈”问题。通过LLM的推理能力和多智能体的协作，实现了在信息不完全情况下的高效优化，为新兴和复杂过程的优化提供了新的范式。"}}
{"id": "2506.21329", "title": "Active Inference AI Systems for Scientific Discovery", "authors": ["Karthik Duraisamy"], "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21329v1", "categories": ["cs.AI", "physics.soc-ph", "68", "I.2"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21329v1", "AI": {"title_translation": "科学发现的主动推理AI系统", "tldr": "本文提出了一种主动推理AI系统架构，旨在通过弥合抽象、推理和现实差距，克服当前AI在科学发现中的局限性，并强调人类判断的不可或缺性。", "motivation": "当前人工智能系统在科学发现中受到其操作架构、脆弱的推理机制以及与实验现实脱节的根本限制。为了在AI驱动的科学领域取得进展，需要弥合抽象差距、推理差距和现实差距，而不是仅仅依赖模型大小、数据或测试时间计算。", "method": "本文定义了用于科学发现的主动推理AI系统，该系统具备以下特征：(i) 维护基于因果自监督基础模型的长期研究记忆；(ii) 配备贝叶斯护栏的符号或神经符号规划器；(iii) 发展持久的知识图谱，其中思考生成新的概念节点，推理建立因果边缘，并通过真实世界交互修剪错误连接并强化验证路径；(iv) 通过与高保真模拟器和自动化实验室的闭环交互来完善其内部表征，形成一个操作循环，即心理模拟指导行动，经验惊喜重塑理解。本质上，该架构通过使能反事实推理的内部模型与将假设根植于现实的外部验证之间的相互作用来促进发现。", "result": "Not mentioned in abstract", "conclusion": "本文提出了一种新的AI架构，旨在通过内部模型与外部验证的相互作用来实现科学发现，并强调由于模拟和实验反馈固有的模糊性以及潜在的不确定性，人类判断作为永久性架构组件是不可或缺的。", "translation": "人工智能的快速发展引发了对变革性科学发现的期待，然而当前的系统在操作架构、脆弱的推理机制以及与实验现实的分离方面仍存在根本性限制。在早期工作的基础上，我们认为AI驱动的科学进步现在取决于弥合三个基本差距——抽象差距、推理差距和现实差距——而不是依赖模型大小/数据/测试时间计算。科学推理需要支持行动和响应模拟的内部表征，区分相关性与机制的因果结构，以及持续校准。我们将用于科学发现的主动推理AI系统定义为：(i) 维护基于因果自监督基础模型的长期研究记忆；(ii) 配备贝叶斯护栏的符号或神经符号规划器；(iii) 发展持久的知识图谱，其中思考生成新的概念节点，推理建立因果边缘，真实世界交互修剪错误连接同时强化验证路径；(iv) 通过与高保真模拟器和自动化实验室的闭环交互来完善其内部表征——这是一个操作循环，其中心理模拟指导行动，经验惊喜重塑理解。本质上，我们概述了一种架构，其中发现产生于使能反事实推理的内部模型与将假设根植于现实的外部验证之间的相互作用。文章还认为，模拟和实验反馈中固有的模糊性以及潜在的不确定性使得人类判断不可或缺，它不是一个临时支架，而是一个永久的架构组成部分。", "summary": "本文提出了一种主动推理AI系统架构，旨在弥补当前AI在科学发现中存在的抽象、推理和现实差距。该系统通过整合基于因果自监督模型的长期研究记忆、带有贝叶斯护栏的规划器、能够动态演化的知识图谱以及与模拟器和自动化实验室的闭环交互来运作。该架构强调内部模型进行反事实推理与外部验证相结合，以实现科学发现。此外，文章指出，由于反馈的模糊性和不确定性，人类判断在系统中扮演着不可或缺的永久性角色。", "keywords": "主动推理AI, 科学发现, 知识图谱, 因果模型, 人类判断", "comments": "本文提出了一种新颖的AI系统概念架构，旨在克服现有AI在科学发现中的核心局限。其创新之处在于将主动推理框架、因果模型、知识图谱与人机闭环交互深度融合，特别是强调了人类判断作为核心组件的重要性，而非临时工具。这为未来构建更强大、更可靠的科学发现AI系统提供了重要的理论指导和设计思路，但其具体实现和效果仍有待进一步验证。"}}
{"id": "2506.21186", "title": "Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout", "authors": ["Apurva Shah", "Axel Abels", "Ann Nowé", "Tom Lenaerts"], "summary": "Perpetual voting addresses fairness in sequential collective decision-making\nby evaluating representational equity over time. However, existing perpetual\nvoting rules rely on full participation and complete approval information,\nassumptions that rarely hold in practice, where partial turnout is the norm. In\nthis work, we study the integration of Artificial Delegates,\npreference-learning agents trained to represent absent voters, into perpetual\nvoting systems. We examine how absenteeism affects fairness and\nrepresentativeness under various voting methods and evaluate the extent to\nwhich Artificial Delegates can compensate for missing participation. Our\nfindings indicate that while absenteeism significantly affects fairness,\nArtificial Delegates reliably mitigate these effects and enhance robustness\nacross diverse scenarios.", "comment": "The paper has been accepted at the ACM Collective Intelligence\n  Conference (CI 2025), August 4 to 6, 2025, San Diego, CA, USA", "pdf_url": "http://arxiv.org/pdf/2506.21186v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21186v1", "AI": {"title_translation": "人工代表解决部分投票率下永久投票的公平性问题", "tldr": "现有永久投票系统在部分投票率下存在公平性问题，本文提出人工代表来解决，并证明其能有效缓解不公平影响。", "motivation": "现有永久投票规则依赖于完全参与和完整的批准信息，这在实践中很少实现，因为部分投票率是常态，导致公平性问题。", "method": "研究了将“人工代表”（经过偏好学习训练以代表缺席选民的代理）整合到永久投票系统中。考察了缺席如何影响各种投票方法下的公平性和代表性，并评估了人工代表弥补缺失参与的程度。", "result": "缺席显著影响公平性，而人工代表能可靠地减轻这些影响并增强在不同情景下的鲁棒性。", "conclusion": "人工代表是解决永久投票中部分投票率导致的公平性问题的有效方法，能够提高系统的鲁棒性和公平性。", "translation": "永久投票通过评估随时间变化的代表性公平性来解决顺序集体决策中的公平性问题。然而，现有的永久投票规则依赖于完全参与和完整的批准信息，这些假设在实践中很少成立，因为部分投票率是常态。在这项工作中，我们研究了将“人工代表”（即经过偏好学习训练以代表缺席选民的代理）整合到永久投票系统中。我们考察了缺席如何影响各种投票方法下的公平性和代表性，并评估了人工代表在多大程度上可以弥补缺失的参与。我们的研究结果表明，虽然缺席显著影响公平性，但人工代表能够可靠地减轻这些影响，并增强在不同情景下的鲁棒性。", "summary": "本文探讨了在实际中常见的投票率不足情况下，永久投票系统面临的公平性挑战。为解决现有系统依赖完全参与的局限性，作者引入了“人工代表”——一种学习缺席选民偏好的智能代理。研究发现，尽管缺席会严重影响公平性，但人工代表能够有效缓解这些负面影响，并在多种场景下提升系统的稳健性。", "keywords": "永久投票, 公平性, 部分投票率, 人工代表, 偏好学习", "comments": "这项工作通过引入“人工代表”的概念，为解决现实世界中永久投票系统因部分投票率而导致的公平性问题提供了一种创新性的解决方案，具有重要的实践意义。"}}
{"id": "2506.21327", "title": "Enabling Bitcoin Smart Contracts on the Internet Computer", "authors": ["Ryan Croote", "Islam El-Ashi", "Thomas Locher", "Yvonne-Anne Pignolet"], "summary": "There is growing interest in providing programmatic access to the value\nlocked in Bitcoin, which famously offers limited programmability itself.\nVarious approaches have been put forth in recent years, with the vast majority\nof proposed mechanisms either building new functionality on top of Bitcoin or\nleveraging a bridging mechanism to enable smart contracts that make use of\n``wrapped'' bitcoins on entirely different platforms.\n  In this work, an architecture is presented that follows a different approach.\nThe architecture enables the execution of Turing-complete Bitcoin smart\ncontracts on the Internet Computer (IC), a blockchain platform for hosting and\nexecuting decentralized applications. Instead of using a bridge, IC and Bitcoin\nnodes interact directly, eliminating potential security risks that the use of a\nbridge entails. This integration requires novel concepts, in particular to\nreconcile the probabilistic nature of Bitcoin with the irreversibility of\nfinalized state changes on the IC, which may be of independent interest.\n  In addition to the presentation of the architecture, we provide evaluation\nresults based on measurements of the Bitcoin integration running on mainnet.\nThe evaluation results demonstrate that, with finalization in a few seconds and\nlow execution costs, this integration enables complex Bitcoin-based\ndecentralized applications that were not practically feasible or economically\nviable before.", "comment": "Published at ICDCS 2025, waiting for DOI", "pdf_url": "http://arxiv.org/pdf/2506.21327v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21327v1", "AI": {"title_translation": "在互联网计算机上实现比特币智能合约", "tldr": "该研究提出了一种在互联网计算机（IC）上直接执行图灵完备比特币智能合约的新架构，无需桥接，实现快速低成本的比特币DApp。", "motivation": "比特币本身的可编程性有限，但人们对其锁定的价值进行编程访问的兴趣日益增长。现有的方法大多是在比特币上构建新功能或通过桥接机制使用“包装”比特币，但这些方法存在局限性或安全风险。", "method": "论文提出了一种新架构，允许在互联网计算机（IC）上直接执行图灵完备的比特币智能合约。该架构通过IC和比特币节点之间的直接交互，而非使用桥接机制，从而消除了桥接带来的潜在安全风险。它还引入了新概念来协调比特币的概率性与IC上最终状态变化的不可逆性。", "result": "基于主网上的比特币集成测量评估结果显示，该集成能在几秒内完成最终确认，且执行成本低。", "conclusion": "这种集成使得以前在实践中不可行或经济上不可行的复杂比特币去中心化应用程序成为可能。", "translation": "比特币本身的可编程性有限，但人们对通过编程访问其锁定价值的兴趣日益增长。近年来提出了各种方法，其中绝大多数提出的机制要么在比特币之上构建新功能，要么利用桥接机制在完全不同的平台上启用使用“包装”比特币的智能合约。\n在这项工作中，提出了一种采用不同方法的架构。该架构使得图灵完备的比特币智能合约能够在互联网计算机（IC）上执行，互联网计算机是一个用于托管和执行去中心化应用程序的区块链平台。IC和比特币节点直接交互，而不是使用桥接，从而消除了使用桥接可能带来的安全风险。这种集成需要新颖的概念，特别是为了协调比特币的概率性与IC上最终状态变化的不可逆性，这可能具有独立的意义。\n除了介绍架构外，我们还提供了基于主网上运行的比特币集成测量结果的评估。评估结果表明，通过几秒钟内的最终确认和低执行成本，这种集成使得以前在实践中不可行或经济上不可行的复杂比特币去中心化应用程序成为可能。", "summary": "本文提出了一种在互联网计算机（IC）上实现图灵完备比特币智能合约的新颖架构。与现有依赖桥接或在比特币上构建新功能的方法不同，该架构通过IC和比特币节点直接交互，旨在消除桥接带来的安全风险。它还解决了比特币概率性与IC状态最终性之间的协调问题。评估结果表明，该集成具有快速最终确认和低成本的特点，为开发此前难以实现或不经济的复杂比特币去中心化应用提供了可能。", "keywords": "比特币智能合约, 互联网计算机, 无桥接集成, 去中心化应用, 图灵完备", "comments": "这篇论文的创新点在于提出了一个无需桥接机制就能在互联网计算机上直接执行比特币智能合约的架构，有效解决了传统桥接方案带来的安全隐患。通过直接节点交互和对概率性与最终性问题的协调，该方案显著提升了比特币可编程性和其在去中心化应用中的实用性，降低了复杂应用的开发门槛和成本。"}}
{"id": "2506.20850", "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision", "authors": ["Yuting He", "Shuo Li"], "summary": "Contrastive learning (CL) has become a cornerstone of self-supervised\npretraining (SSP) in foundation models, however, extending CL to pixel-wise\nrepresentation, crucial for medical vision, remains an open problem. Standard\nCL formulates SSP as a binary optimization problem (binary CL) where the\nexcessive pursuit of feature dispersion leads to an over-dispersion problem,\nbreaking pixel-wise feature correlation thus disrupting the intra-class\ndistribution. Our vector CL reformulates CL as a vector regression problem,\nenabling dispersion quantification in pixel-wise pretraining via modeling\nfeature distances in regressing displacement vectors. To implement this novel\nparadigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER\nestablishes an extendable vector-based self-learning, enforces a consistent\noptimization flow from vector regression to distance modeling, and leverages a\nvector pyramid architecture for granularity adaptation, thus preserving\npixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,\nspanning 2 dimensions and 4 modalities, show that COVER significantly improves\npixel-wise SSP, advancing generalizable medical visual foundation models.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20850v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20850v1", "AI": {"title_translation": "向量对比学习在医学视觉像素级预训练中的应用", "tldr": "提出向量对比学习（Vector CL）和COVER框架，通过将对比学习重构为向量回归问题，解决了标准对比学习在像素级预训练中的过度分散问题，显著提升了医学视觉基础模型的泛化能力。", "motivation": "标准对比学习（CL）在像素级表示方面存在过度分散问题，这会破坏像素级特征相关性并扰乱类内分布，而像素级表示对医学视觉至关重要，因此需要一种新的方法来解决这一挑战。", "method": "提出向量对比学习（vector CL），将CL重构为向量回归问题，通过建模回归位移向量中的特征距离来实现像素级预训练中的分散量化。为实现这一范式，提出COntrast in VEctor Regression (COVER) 框架，该框架建立了可扩展的基于向量的自学习，强制执行从向量回归到距离建模的一致优化流，并利用向量金字塔架构进行粒度适应，从而在自监督预训练（SSP）中保留像素级特征相关性。", "result": "在跨越2个维度和4种模态的8项任务中进行的广泛实验表明，COVER显著改进了像素级自监督预训练（SSP），推动了可泛化的医学视觉基础模型的发展。", "conclusion": "向量对比学习及其实现的COVER框架有效解决了标准对比学习在医学视觉像素级预训练中的挑战，显著提升了模型性能和泛化能力，为医学视觉基础模型的发展提供了新途径。", "translation": "对比学习（CL）已成为基础模型中自监督预训练（SSP）的基石，然而，将CL扩展到像素级表示（这对医学视觉至关重要）仍然是一个悬而未决的问题。标准CL将SSP表述为一个二元优化问题（二元CL），其中过度追求特征分散导致过度分散问题，破坏了像素级特征相关性，从而扰乱了类内分布。我们的向量CL将CL重构为一个向量回归问题，通过建模回归位移向量中的特征距离，实现了像素级预训练中的分散量化。为了实现这一新颖的范式，我们提出了向量回归中的对比（COVER）框架。COVER建立了一个可扩展的基于向量的自学习，强制执行从向量回归到距离建模的一致优化流，并利用向量金字塔架构进行粒度适应，从而在SSP中保留像素级特征相关性。在跨越2个维度和4种模态的8项任务中进行的广泛实验表明，COVER显著改进了像素级SSP，推动了可泛化的医学视觉基础模型。", "summary": "本文提出向量对比学习（vector CL）范式和COntrast in VEctor Regression (COVER) 框架，旨在解决标准对比学习在医学视觉像素级预训练中过度分散、破坏特征相关性的问题。通过将对比学习重构为向量回归问题，COVER框架能够量化特征分散并保留像素级特征相关性。实验证明，COVER显著提升了像素级自监督预训练的效果，促进了可泛化医学视觉基础模型的发展。", "keywords": "向量对比学习, 像素级预训练, 医学视觉, 自监督学习, 特征相关性", "comments": "这篇论文通过将对比学习从传统的二元优化问题重新定义为向量回归问题，提出了一个新颖的像素级预训练范式，解决了医学图像领域中像素级特征相关性被破坏的关键挑战。COVER框架的设计，特别是其向量金字塔架构，对于保持粒度适应性和特征相关性至关重要。其创新点在于对对比学习原理的深刻理解和在像素级应用的突破，对于推动医学视觉基础模型的发展具有重要意义。"}}
{"id": "2506.20746", "title": "Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers", "authors": ["Todd Nief", "David Reber", "Sean Richardson", "Ari Holtzman"], "summary": "When an LLM learns a relation during finetuning (e.g., new movie releases,\ncorporate mergers, etc.), where does this information go? Is it extracted when\nthe model processes an entity, recalled just-in-time before a prediction, or\nare there multiple separate heuristics? Existing localization approaches (e.g.\nactivation patching) are ill-suited for this analysis because they tend to\nreplace parts of the residual stream, potentially deleting information. To fill\nthis gap, we propose dynamic weight-grafting between fine-tuned and pre-trained\nlanguage models to show that fine-tuned language models both (1) extract\nrelation information learned during finetuning while processing entities and\n(2) ``recall\" this information in later layers while generating predictions. In\nsome cases, models need both of these pathways to correctly generate finetuned\ninformation while, in other cases, a single ``enrichment\" or ``recall\" pathway\nalone is sufficient. We examine the necessity and sufficiency of these\ninformation pathways, examining what layers they occur at, how much redundancy\nthey exhibit, and which model components are involved -- finding that the\n``recall\" pathway occurs via both task-specific attention mechanisms and a\nrelation extraction step in the output of the attention and the feedforward\nnetworks at the final layers before next token prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20746v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20746v1", "AI": {"title_translation": "关系抽取的多流机制：Transformer中的信息丰富与召回", "tldr": "本文通过动态权重嫁接方法，揭示了微调后的语言模型在处理实体时会提取关系信息，并在后续层中召回这些信息以进行预测，有时两种机制并存，有时单一机制即可。", "motivation": "现有定位方法（如激活补丁）不适用于分析大型语言模型（LLM）在微调过程中学习到的关系信息去向，因为它们可能会删除信息，从而无法准确揭示信息是在处理实体时被提取、在预测前被即时召回，还是存在多种独立的启发式方法。", "method": "本文提出了一种在微调模型和预训练模型之间进行“动态权重嫁接”的方法，以分析微调后的语言模型中关系信息的处理机制。", "result": "研究发现，微调后的语言模型在处理实体时会提取关系信息（“丰富”途径），并在生成预测时在后续层中“召回”这些信息。在某些情况下，模型需要这两种途径才能正确生成微调信息，而在其他情况下，单一的“丰富”或“召回”途径就足够了。此外，“召回”途径通过任务特定的注意力机制以及在最终层中注意力输出和前馈网络的输出中的关系抽取步骤发生。", "conclusion": "本文证明了微调后的语言模型通过“丰富”和“召回”这两种独立但有时协同的信息途径来处理和利用关系信息，并详细阐述了这些途径的发生位置、冗余度以及涉及的模型组件。", "translation": "当LLM在微调期间学习到一种关系（例如，新电影上映、公司合并等）时，这些信息去了哪里？它是在模型处理实体时被提取，在预测前即时召回，还是存在多种独立的启发式方法？现有定位方法（例如激活补丁）不适合这种分析，因为它们倾向于替换残差流的一部分，可能会删除信息。为了填补这一空白，我们提出了在微调语言模型和预训练语言模型之间进行动态权重嫁接，以表明微调语言模型（1）在处理实体时提取了微调期间学到的关系信息，并且（2）在生成预测时在后续层中“召回”了这些信息。在某些情况下，模型需要这两种途径才能正确生成微调信息，而在其他情况下，单一的“丰富”或“召回”途径就足够了。我们研究了这些信息途径的必要性和充分性，检查了它们发生的层、表现出的冗余量以及涉及的模型组件——发现“召回”途径通过任务特定的注意力机制以及在下一词元预测之前的最终层中注意力输出和前馈网络的输出中的关系抽取步骤发生。", "summary": "本文探讨了大型语言模型在微调后如何处理关系信息。通过引入动态权重嫁接方法，研究发现模型在处理实体时会“提取”关系信息，并在后续层“召回”这些信息以进行预测。有时这两种“丰富”和“召回”途径都必需，有时其中一种就足够。研究还详细分析了这些信息途径发生的层级、冗余性以及相关模型组件，揭示了“召回”途径通过注意力机制和最终层的网络输出进行关系抽取。", "keywords": "关系抽取, Transformer, 微调, 语言模型, 信息流", "comments": "本文创新性地提出了动态权重嫁接方法来剖析Transformer中关系信息的流向，解决了现有定位方法无法避免信息删除的局限性。其发现的“丰富”和“召回”双重信息途径，对于理解大型语言模型的内部机制及其微调后的行为具有重要意义，对未来模型设计和可解释性研究提供了新的视角。"}}
{"id": "2506.21309", "title": "Linear codes arising from the point-hyperplane geometry-Part I: the Segre embedding", "authors": ["Ilaria Cardinali", "Luca Giuzzi"], "summary": "Let $V$ be a vector space over the finite field $\\mathbb{F}_q$ with $q$\nelements and $\\Lambda$ be the image of the Segre geometry\n$\\mathrm{PG}(V)\\otimes\\mathrm{PG}(V^*)$ in $\\mathrm{PG}(V\\otimes V^*)$.\nConsider the subvariety $\\Lambda_{1}$ of $\\Lambda$ represented by the pure\ntensors $x\\otimes \\xi$ with $x\\in V$ and $\\xi\\in V^*$ such that $\\xi(x)=0$.\nRegarding $\\Lambda_1$ as a projective system of $\\mathrm{PG}(V\\otimes V^*)$, we\nstudy the linear code $\\mathcal{C}(\\Lambda_1)$ arising from it. The code\n$\\mathcal{C}(\\Lambda_1)$ is minimal code and we determine its basic parameters,\nitsfull weight list and its linear automorphism group. We also give a\ngeometrical characterization of its minimum and second lowest weight codewords\nas well as of some of the words of maximum weight.", "comment": "29 pages", "pdf_url": "http://arxiv.org/pdf/2506.21309v1", "categories": ["math.CO", "cs.DM", "cs.IT", "math.IT", "51E22, 94B05, 14M12"], "cate": "math.CO", "url": "http://arxiv.org/abs/2506.21309v1", "AI": {"title_translation": "线性码的产生源于点-超平面几何-第一部分：Segre嵌入", "tldr": "本文研究了由Segre几何的特定子簇$\\\\Lambda_1$生成的线性码$\\\\mathcal{C}(\\\\Lambda_1)$，确定了其基本参数、权重列表和自同构群，并对其特殊权重码字进行了几何表征。", "motivation": "该论文研究了从Segre几何的特定子簇$\\\\Lambda_1$产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$，其动机在于深入理解这类特殊几何结构生成的线性码的性质和参数。", "method": "论文将$\\\\Lambda_1$视为射影系统，研究了由此产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$。具体方法包括确定其基本参数、完整的权重列表和线性自同构群，并对其最小、第二低权重码字以及部分最大权重码字进行了几何表征。", "result": "代码$\\\\mathcal{C}(\\\\Lambda_1)$被证明是极小码。论文确定了其基本参数、完整的权重列表和线性自同构群。此外，还对最小和第二低权重码字以及部分最大权重码字进行了几何表征。", "conclusion": "论文成功地研究了从Segre几何的特定子簇$\\\\Lambda_1$产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$，并全面描述了其编码特性和几何结构。", "translation": "设$V$是有限域$\\\\mathbb{F}_q$上的一个向量空间，$\\\\Lambda$是Segre几何$\\\\mathrm{PG}(V)\\\\\\\\otimes\\\\\\\\mathrm{PG}(V^*)$在$\\\\mathrm{PG}(V\\\\\\\\otimes V^*)$中的像。考虑$\\\\Lambda$的一个子簇$\\\\Lambda_{1}$，它由纯张量$x\\\\\\\\otimes \\\\xi$表示，其中$x\\\\\\\\in V$和$\\\\xi\\\\\\\\in V^*$满足$\\\\xi(x)=0$。将$\\\\Lambda_1$视为$\\\\mathrm{PG}(V\\\\\\\\otimes V^*)$的一个射影系统，我们研究由此产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$。代码$\\\\mathcal{C}(\\\\Lambda_1)$是一个极小码，我们确定了它的基本参数、完整的权重列表和线性自同构群。我们还对其最小和第二低权重码字以及部分最大权重码字进行了几何表征。", "summary": "本文研究了从Segre几何的特定子簇$\\\\Lambda_1$（由满足$\\\\xi(x)=0$的纯张量$x\\\\\\\\otimes \\\\xi$构成）产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$。该码被证明是极小码。研究内容包括确定其基本编码参数、完整的权重分布以及线性自同构群。此外，论文还从几何角度对该码的最小、第二低以及部分最大权重码字进行了精确刻画。", "keywords": "线性码, Segre嵌入, 射影几何, 极小码, 权重列表", "comments": "这篇论文通过将代数几何中的Segre嵌入与编码理论相结合，为构建和分析新型线性码提供了一个具体的例子。其创新之处在于利用几何结构$\\\\Lambda_1$来定义码，并深入分析其编码属性，这对于理解码的结构和设计具有重要意义。特别地，对码字进行几何表征是其亮点，有助于从几何直观上理解码的性质。"}}
{"id": "2506.21216", "title": "Edge Clique Partition and Cover Beyond Independence", "authors": ["Fedor V. Fomin", "Petr A. Golovach", "Danil Sagunov", "Kirill Simonov"], "summary": "Covering and partitioning the edges of a graph into cliques are classical\nproblems at the intersection of combinatorial optimization and graph theory,\nhaving been studied through a range of algorithmic and complexity-theoretic\nlenses. Despite the well-known fixed-parameter tractability of these problems\nwhen parameterized by the total number of cliques, such a parameterization\noften fails to be meaningful for sparse graphs. In many real-world instances,\non the other hand, the minimum number of cliques in an edge cover or partition\ncan be very close to the size of a maximum independent set \\alpha(G).\n  Motivated by this observation, we investigate above \\alpha parameterizations\nof the edge clique cover and partition problems. Concretely, we introduce and\nstudy Edge Clique Cover Above Independent Set (ECC/\\alpha) and Edge Clique\nPartition Above Independent Set (ECP/\\alpha), where the goal is to cover or\npartition all edges of a graph using at most \\alpha(G) + k cliques, and k is\nthe parameter. Our main results reveal a distinct complexity landscape for the\ntwo variants. We show that ECP/\\alpha is fixed-parameter tractable, whereas\nECC/\\alpha is NP-complete for all k \\geq 2, yet can be solved in polynomial\ntime for k \\in {0,1}. These findings highlight intriguing differences between\nthe two problems when viewed through the lens of parameterization above a\nnatural lower bound.\n  Finally, we demonstrate that ECC/\\alpha becomes fixed-parameter tractable\nwhen parameterized by k + \\omega(G), where \\omega(G) is the size of a maximum\nclique of the graph G. This result is particularly relevant for sparse graphs,\nin which \\omega is typically small. For H-minor free graphs, we design a\nsubexponential algorithm of running time f(H)^{\\sqrt{k}}n^{O(1)}.", "comment": "An extended abstract of this paper appears in the proceedings of ESA\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21216v1", "categories": ["cs.DS", "cs.DM"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21216v1", "AI": {"title_translation": "边团划分与覆盖超越独立集", "tldr": "本文研究了边团覆盖和划分问题在“超独立集大小”参数化下的计算复杂性，发现边团划分问题是FPT，而边团覆盖问题大部分情况是NP完全，但在结合最大团大小作为参数时变为FPT。", "motivation": "经典图论问题中，边团覆盖和划分的参数化通常以总团数作为参数，但这对于稀疏图意义不大。然而，在许多实际场景中，所需团数接近最大独立集的大小。受此观察启发，作者研究了超越最大独立集大小的参数化。", "method": "引入并研究了两种新的参数化问题：超独立集边团覆盖（ECC/\\alpha）和超独立集边团划分（ECP/\\alpha），其目标是使用最多 \\alpha(G) + k 个团来覆盖或划分图的所有边，其中 k 是参数。", "result": "研究结果揭示了两种变体截然不同的复杂性：ECP/\\alpha 是固定参数可解（FPT）的；ECC/\\alpha 对于所有 k \\geq 2 都是 NP 完全的，但对于 k \\in {0,1} 可以在多项式时间内求解。此外，当参数化为 k + \\omega(G)（其中 \\omega(G) 是最大团的大小）时，ECC/\\alpha 变为 FPT，这对于稀疏图尤为重要。对于 H-minor-free 图，设计了一个次指数算法，运行时间为 f(H)^{\\sqrt{k}}n^{O(1)}。", "conclusion": "这些发现突出了边团覆盖和边团划分这两个问题在超越自然下界参数化视角下的显著差异。ECC/\\alpha 在特定参数化下对稀疏图具有可解性。", "translation": "将图的边覆盖和划分为团是组合优化和图论交叉领域的经典问题，已通过一系列算法和复杂性理论视角进行研究。尽管这些问题在以总团数作为参数时具有众所周知的固定参数可解性，但这种参数化对于稀疏图通常意义不大。另一方面，在许多实际实例中，边覆盖或划分中的最小团数可以非常接近最大独立集 \\alpha(G) 的大小。受此观察启发，我们研究了边团覆盖和划分问题在“超 \\alpha”参数化下的情况。具体来说，我们引入并研究了超独立集边团覆盖（ECC/\\alpha）和超独立集边团划分（ECP/\\alpha），其目标是使用最多 \\alpha(G) + k 个团来覆盖或划分图的所有边，其中 k 是参数。我们的主要结果揭示了这两种变体截然不同的复杂性图景。我们证明 ECP/\\alpha 是固定参数可解的，而 ECC/\\alpha 对于所有 k \\geq 2 都是 NP 完全的，但对于 k \\in {0,1} 可以在多项式时间内求解。这些发现突出了这两个问题在超越自然下界参数化视角下的有趣差异。\n最后，我们证明当以 k + \\omega(G) 作为参数时，ECC/\\alpha 变为固定参数可解的，其中 \\omega(G) 是图 G 的最大团的大小。这个结果对于稀疏图尤其相关，因为在稀疏图中 \\omega 通常很小。对于不含 H-次图的图，我们设计了一个运行时间为 f(H)^{\\sqrt{k}}n^{O(1)} 的次指数算法。", "summary": "本文探讨了经典图论问题——边团覆盖和边团划分——在新的参数化视角下的计算复杂性。鉴于传统参数化（总团数）对稀疏图的局限性，作者提出并研究了“超越最大独立集大小”（\\alpha(G) + k）的参数化。研究发现，边团划分问题（ECP/\\alpha）是固定参数可解的，而边团覆盖问题（ECC/\\alpha）在 k \\geq 2 时是 NP 完全的，但在 k \\in {0,1} 时可多项式时间求解。此外，当结合最大团大小（k + \\omega(G)）作为参数时，ECC/\\alpha 变为固定参数可解，这对于稀疏图具有重要意义。文章还为 H-minor-free 图设计了次指数算法。", "keywords": "边团划分, 边团覆盖, 参数化复杂性, 最大独立集, 稀疏图", "comments": "本文创新性地提出了将问题参数化为“超越最大独立集大小”这一新颖视角，有效解决了传统参数化在稀疏图上的局限性。通过对比分析两种变体（覆盖与划分）在不同参数化下的复杂性，揭示了它们之间深刻的计算差异。特别是在考虑稀疏图特性时，引入最大团大小作为参数的FPT结果具有重要的理论和实际价值。该研究为图论中的边覆盖和划分问题提供了新的理解和解决思路。"}}
{"id": "2506.21311", "title": "Estimating Technical Loss without Power Flows: A Practical, Data-Driven Approach for Loss Estimation in Distribution Grids", "authors": ["Mohini Bariya", "Genevieve Flaspohler"], "summary": "Electric grids in low- and middle-income countries (LMICs) across the world\nface an acute challenge. To support global decarbonisation efforts and raise\nmillions from energy poverty, these grids must shoulder substantial load growth\nwhile integrating distributed renewable generation. However, decades of rapid\nand poorly funded infrastructure expansions have led to national grids in many\nLMICs that are strained and weak, composed of aging, faulty, and undersized\ninfrastructure. A cause and symptom of this weakness is excessive technical\nloss within the grid infrastructure during energy delivery, particularly at the\ndistribution level; network losses are regularly estimated to be well over 20\npercent, compared to a baseline of 5 percent in higher-income nations.\nAddressing technical loss through targeted interventions is essential for\nbolstering grids' physical and economic strength. Unfortunately, current\napproaches for estimating and localizing technical loss require expensive,\nextensive power flow sensing, which is essentially absent in LMIC distribution\nsystems. We present a novel approach to technical loss estimation without power\nflows, which leverages more readily available voltage magnitude measurements at\nsparse locations in the grid. This estimator puts loss estimation and\nlocalization within reach for LMIC grids globally, and provides a critical tool\nfor the effective design, implementation, and evaluation of loss-reduction\ninterventions.", "comment": "6 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21311v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21311v1", "AI": {"title_translation": "在没有潮流的情况下估计技术损耗：一种针对配电网损耗估计的实用数据驱动方法", "tldr": "中低收入国家电网技术损耗高且难以估计，本文提出一种无需潮流数据、仅利用稀疏电压测量值即可估计技术损耗的新方法。", "motivation": "全球中低收入国家（LMICs）的电网面临严峻挑战，需承担巨大负荷增长并整合分布式可再生能源，但其基础设施薄弱、老化，导致技术损耗（尤其在配电层面）过高，常远超20%。现有损耗估计方法依赖昂贵且在LMIC电网中普遍缺乏的潮流传感数据，因此难以有效识别和解决技术损耗问题。", "method": "本研究提出了一种新颖的数据驱动方法，用于技术损耗估计。该方法无需传统的潮流数据，而是利用电网中稀疏位置更易获得的电压幅值测量数据来进行损耗估计和定位。", "result": "该方法使全球中低收入国家电网的技术损耗估计和定位成为可能。", "conclusion": "该方法为全球中低收入国家电网提供了关键工具，使其能够有效设计、实施和评估损耗降低干预措施，从而提升电网的物理和经济实力。", "translation": "全球中低收入国家（LMICs）的电网面临严峻挑战。为了支持全球脱碳努力并帮助数百万人摆脱能源贫困，这些电网必须承担巨大的负荷增长，同时整合分布式可再生能源发电。然而，数十年来快速且资金不足的基础设施扩张，导致许多中低收入国家的国家电网紧张而薄弱，由老化、故障和规模不足的基础设施组成。这种薄弱的一个原因和症状是能源输送过程中电网基础设施内部，特别是在配电层面，存在过度的技术损耗；与高收入国家5%的基线相比，电网损耗通常估计远超20%。通过有针对性的干预措施解决技术损耗对于增强电网的物理和经济实力至关重要。不幸的是，当前估计和定位技术损耗的方法需要昂贵、广泛的潮流传感，这在中低收入国家的配电系统中基本不存在。我们提出了一种无需潮流数据即可进行技术损耗估计的新颖方法，该方法利用电网中稀疏位置更易获得的电压幅值测量数据。这种估计器使全球中低收入国家电网的损耗估计和定位成为可能，并为有效设计、实施和评估损耗降低干预措施提供了关键工具。", "summary": "中低收入国家电网因基础设施薄弱而面临高技术损耗（常超20%），但现有损耗估计方法依赖其缺乏的昂贵潮流传感数据。本文提出一种创新的数据驱动方法，利用电网中稀疏位置的电压幅值测量数据即可估计技术损耗，无需潮流数据。该方法使中低收入国家电网能有效进行损耗估计和定位，为制定减损策略提供了关键工具。", "keywords": "技术损耗, 配电网, 数据驱动, 电压测量, 中低收入国家", "comments": "这项研究解决了中低收入国家电网在技术损耗估计方面的实际痛点，其创新之处在于提出了一种无需昂贵潮流传感数据、仅依赖更易获取的稀疏电压测量值的解决方案。这大大降低了损耗估计的门槛，对于提升这些地区电网的运行效率和经济效益具有重要意义。"}}
{"id": "2506.21043", "title": "Analysis of Null Related Beampattern Measures and Signal Quantization Effects for Linear Differential Microphone Arrays", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "summary": "A differential microphone array (DMA) offers enhanced capabilities to obtain\nsharp nulls at the cost of relatively broad peaks in the beam power pattern.\nThis can be used for applications that require nullification or attenuation of\ninterfering sources. To the best of our knowledge, the existing literature\nlacks measures that directly assess the efficacy of nulls, and null-related\nmeasures have not been investigated in the context of differential microphone\narrays (DMAs). This paper offers new insights about the utility of DMAs by\nproposing measures that characterize the nulls in their beam power patterns. We\ninvestigate the performance of differential beamformers by presenting and\nevaluating null-related measures namely null depth (ND) and Null Width (NW) as\na function of depth level relative to the beam power pattern maxima. A study of\nsignal quantization effects due to data acquisition for 1st, 2nd and 3rd order\nlinear DMAs and for different beampatterns i.e. dipole, cardioid, hypercardioid\nand supercardioid is presented. An analytical expression for the quantized\nbeamformed output for any general $ N^{th} $ order DMA is formulated.\nSimulation results of the variation of ND with number of quantization bits and\nthe variation of NW as a function of depth are also presented and inferences\nare drawn. Lab experiments are conducted in a fully anechoic room to support\nthe simulation results. The measured beampattern exhibits a pronounced null\ndepth, confirming the effectiveness of the experimental setup.", "comment": "10 pages, 15 Figures, 3 Tables", "pdf_url": "http://arxiv.org/pdf/2506.21043v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21043v1", "AI": {"title_translation": "线性差分麦克风阵列的零点相关波束图测量和信号量化效应分析", "tldr": "本文提出了评估差分麦克风阵列 (DMA) 波束图中零点性能的新测量方法（零点深度和零点宽度），并研究了信号量化对不同阶数和波束图类型DMA性能的影响，通过仿真和实验验证了结果。", "motivation": "现有文献缺乏直接评估零点有效性的测量方法，且未在差分麦克风阵列 (DMA) 的背景下研究零点相关测量。差分麦克风阵列能以较宽的波束峰值换取尖锐的零点，这对于需要消除或衰减干扰源的应用非常有用。", "method": "本文提出了表征DMA波束图中零点的新测量方法，即零点深度 (ND) 和零点宽度 (NW)。研究了信号量化对一阶、二阶和三阶线性DMA以及偶极子、心形、超心形和超指向性等不同波束图的影响。推导了任意N阶DMA量化波束形成输出的解析表达式。通过仿真研究了ND随量化位数的变化以及NW随深度水平的变化。在全消声室进行了实验室实验以支持仿真结果。", "result": "仿真结果展示了ND随量化位数的变化以及NW随深度水平的变化。实验室实验结果显示出显著的零点深度，证实了实验设置的有效性，并支持了仿真结果。", "conclusion": "本文提出了新的零点相关测量方法（零点深度和零点宽度），并成功地将它们应用于差分麦克风阵列。研究表明，这些测量方法对于评估DMA的性能非常有效，并且量化效应在实际应用中需要考虑。", "translation": "差分麦克风阵列（DMA）能够以相对较宽的波束功率图峰值为代价获得尖锐的零点。这可用于需要抵消或衰减干扰源的应用。据我们所知，现有文献缺乏直接评估零点有效性的测量方法，并且尚未在差分麦克风阵列（DMA）的背景下研究零点相关测量。本文通过提出表征其波束功率图中零点的测量方法，为DMA的实用性提供了新的见解。我们通过呈现和评估零点相关测量，即零点深度（ND）和零点宽度（NW），作为相对于波束功率图最大值的深度水平的函数，来研究差分波束形成器的性能。本文还研究了由于数据采集引起的一阶、二阶和三阶线性DMA以及偶极子、心形、超心形和超指向性等不同波束图的信号量化效应。本文推导出了任意N阶DMA量化波束形成输出的解析表达式。本文还给出了ND随量化位数变化以及NW随深度变化关系的仿真结果，并从中得出推论。在全消声室进行了实验室实验以支持仿真结果。测得的波束图显示出显著的零点深度，证实了实验设置的有效性。", "summary": "本文针对差分麦克风阵列（DMA）波束图中零点评估的现有空白，提出了零点深度（ND）和零点宽度（NW）两种新的测量方法。研究了这些零点相关测量在DMA中的性能，并分析了信号量化对不同阶数和波束图类型DMA的影响。文章推导了量化波束形成输出的解析表达式，并通过仿真和消声室实验验证了所提方法的有效性及量化效应。", "keywords": "差分麦克风阵列, 零点深度, 零点宽度, 信号量化, 波束图", "comments": "本文的创新点在于首次提出了专门用于评估差分麦克风阵列中零点性能的量化测量方法（零点深度和零点宽度），并系统地研究了信号量化对DMA性能的影响。这对于需要精确控制零点以抑制干扰的应用具有重要意义，填补了现有研究的空白。通过结合理论分析、仿真和实验验证，研究结果具有较高的可信度。"}}
{"id": "2506.20995", "title": "Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance", "authors": ["Akio Hayakawa", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji"], "summary": "We propose a novel step-by-step video-to-audio generation method that\nsequentially produces individual audio tracks, each corresponding to a specific\nsound event in the video. Our approach mirrors traditional Foley workflows,\naiming to capture all sound events induced by a given video comprehensively.\nEach generation step is formulated as a guided video-to-audio synthesis task,\nconditioned on a target text prompt and previously generated audio tracks. This\ndesign is inspired by the idea of concept negation from prior compositional\ngeneration frameworks. To enable this guided generation, we introduce a\ntraining framework that leverages pre-trained video-to-audio models and\neliminates the need for specialized paired datasets, allowing training on more\naccessible data. Experimental results demonstrate that our method generates\nmultiple semantically distinct audio tracks for a single input video, leading\nto higher-quality composite audio synthesis than existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20995v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20995v1", "AI": {"title_translation": "基于负面音频引导的逐步视频到音频合成", "tldr": "提出了一种新颖的逐步视频到音频生成方法，通过负面音频引导顺序生成独立的音轨，从而实现高质量的复合音频合成。", "motivation": "现有方法可能无法全面捕捉视频中所有声事件，或需要专门的配对数据集。本文旨在提出一种能够全面捕捉视频中所有声事件、且无需专门配对数据集的视频到音频生成方法。", "method": "提出了一种逐步视频到音频生成方法，该方法模仿传统的拟音工作流程。每个生成步骤都被表述为一个引导式视频到音频合成任务，以目标文本提示和先前生成的音轨为条件。引入了一个训练框架，利用预训练的视频到音频模型，并消除了对专门配对数据集的需求，允许在更易获取的数据上进行训练。", "result": "实验结果表明，该方法能够为单个输入视频生成多个语义上不同的音轨，并且比现有基线方法产生更高质量的复合音频合成。", "conclusion": "本文提出的基于负面音频引导的逐步视频到音频合成方法，能够有效地生成高质量的复合音频，并且在数据需求方面具有优势。", "translation": "我们提出了一种新颖的逐步视频到音频生成方法，该方法顺序生成独立的音轨，每个音轨对应视频中特定的声音事件。我们的方法模仿传统的拟音工作流程，旨在全面捕捉给定视频中所有由视频引发的声音事件。每个生成步骤都被表述为一个引导式视频到音频合成任务，以目标文本提示和先前生成的音轨为条件。这种设计灵感来源于先前组合生成框架中的概念否定思想。为了实现这种引导式生成，我们引入了一个训练框架，该框架利用预训练的视频到音频模型，并消除了对专门配对数据集的需求，允许在更易获取的数据上进行训练。实验结果表明，我们的方法能够为单个输入视频生成多个语义上不同的音轨，从而产生比现有基线方法更高质量的复合音频合成。", "summary": "本文提出了一种名为“逐步视频到音频合成”的新方法，该方法通过模仿拟音工作流，顺序生成视频中各声事件对应的独立音轨。每个生成步骤都受到目标文本提示和已生成音轨的引导，并借鉴了概念否定的思想。该方法利用预训练模型，无需特定配对数据集，可在更易获取的数据上训练。实验证明，该方法能为单个视频生成多个语义独立的音轨，合成的复合音频质量优于现有基线。", "keywords": "视频到音频合成, 逐步生成, 负面音频引导, 拟音工作流, 音频生成", "comments": "该论文的创新点在于其“逐步”和“负面音频引导”的生成范式，模仿了传统的拟音工作流程，这在视频到音频合成领域是一个新颖的视角。此外，它解决了专用配对数据集的需求问题，通过利用预训练模型和更易获取的数据进行训练，大大降低了数据门槛。其能够生成语义上不同的独立音轨，并最终合成高质量复合音频的能力，显示了其在多音源视频生成方面的潜力。"}}
{"id": "2506.21245", "title": "GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models", "authors": ["Qifei Cui", "Xinyu Lu"], "summary": "This work introduces a novel framework for brain tumor segmentation\nleveraging pre-trained GANs and Unet architectures. By combining a global\nanomaly detection module with a refined mask generation network, the proposed\nmodel accurately identifies tumor-sensitive regions and iteratively enhances\nsegmentation precision using adversarial loss constraints. Multi-modal MRI data\nand synthetic image augmentation are employed to improve robustness and address\nthe challenge of limited annotated datasets. Experimental results on the BraTS\ndataset demonstrate the effectiveness of the approach, achieving high\nsensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the\nbaseline. This scalable method minimizes the dependency on fully annotated\ndata, paving the way for practical real-world applications in clinical\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21245v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21245v1", "AI": {"title_translation": "GANet-Seg: 基于混合生成模型的脑肿瘤分割对抗学习", "tldr": "GANet-Seg是一个结合预训练GAN和Unet的脑肿瘤分割新框架，通过对抗学习提升精度，并利用多模态MRI和合成数据增强解决标注数据有限的问题，在BraTS数据集上表现出色。", "motivation": "脑肿瘤分割面临标注数据有限的挑战，且需要高精度和鲁棒性。本文旨在开发一种减少对完全标注数据依赖的有效分割方法。", "method": "本文提出了GANet-Seg框架，结合预训练GAN和Unet架构。它包含一个全局异常检测模块和一个精细掩膜生成网络，通过对抗损失约束迭代提升分割精度。使用多模态MRI数据和合成图像增强来提高鲁棒性并解决标注数据不足问题。", "result": "在BraTS数据集上的实验结果表明，该方法在病灶级Dice和HD95指标上均取得了比基线更高的灵敏度和准确性。", "conclusion": "GANet-Seg是一种可扩展的脑肿瘤分割方法，能有效减少对完全标注数据的依赖，为临床实践中的实际应用铺平了道路。", "translation": "这项工作引入了一个利用预训练GAN和Unet架构进行脑肿瘤分割的新颖框架。通过将全局异常检测模块与精细掩膜生成网络相结合，所提出的模型能够准确识别肿瘤敏感区域，并利用对抗性损失约束迭代地提高分割精度。采用多模态MRI数据和合成图像增强来提高鲁棒性并解决标注数据集有限的挑战。BraTS数据集上的实验结果表明，该方法在病灶级Dice和HD95指标上均表现出比基线更高的灵敏度和准确性。这种可扩展的方法最大限度地减少了对完全标注数据的依赖，为临床环境中的实际应用铺平了道路。", "summary": "GANet-Seg是一个创新的脑肿瘤分割框架，它融合了预训练的GAN和Unet模型。该方法通过结合异常检测和精细掩膜生成网络，并利用对抗性损失进行迭代优化，实现了高精度的肿瘤区域识别。为克服标注数据稀缺问题，研究采用了多模态MRI和合成数据增强。在BraTS数据集上的实验证明，GANet-Seg在分割灵敏度和准确性上均优于基线，展现了其在减少数据依赖方面的潜力，适用于实际临床应用。", "keywords": "脑肿瘤分割, 对抗学习, GAN, Unet, 医疗图像", "comments": "GANet-Seg的创新之处在于结合了GAN和Unet的优势，并通过对抗学习和合成数据增强有效解决了医疗图像分割中常见的标注数据稀缺问题。其可扩展性及其在BraTS数据集上的优异表现，使其在临床脑肿瘤分割领域具有重要的应用前景。"}}
{"id": "2506.21441", "title": "An evaluation of level of detail degradation in head-mounted display peripheries", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Martin Reddy"], "summary": "A paradigm for the design of systems that manage level of detail in virtual\nenvironments is proposed. As an example of the prototyping step in this\nparadigm, a user study was performed to evaluate the effectiveness of high\ndetail insets used with head-mounted displays. Ten subjects were given a simple\nsearch task that required the location and identification of a single target\nobject. All subjects used seven different displays (the independent variable),\nvarying in inset size and peripheral detail, to perform this task. Frame rate,\ntarget location, subject input method, and order of display use were all\ncontrolled. Primary dependent measures were search time on trials with correct\nidentification, and the percentage of all trials correctly identified. ANOVAs\nof the results showed that insetless, high detail displays did not lead to\nsignificantly different search times or accuracies than displays with insets.\nIn fact, only the insetless, low detail display returned significantly\ndifferent results. Further research is being performed to examine the effect of\nvarying task complexity, inset size, and level of detail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21441v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21441v1", "AI": {"title_translation": "头戴式显示器外围细节层次降级评估", "tldr": "本研究提出了一种在虚拟环境中管理细节层次的设计范式，并通过用户研究评估了头戴式显示器中高细节镶嵌的有效性。结果显示，对于简单搜索任务，无镶嵌的高细节显示器与有镶嵌的显示器在搜索时间或准确性上没有显著差异，只有无镶嵌的低细节显示器表现出显著差异。", "motivation": "本研究旨在提出一种在虚拟环境中管理细节层次的系统设计范式，并作为原型设计步骤的实例，评估头戴式显示器中高细节镶嵌的有效性。", "method": "研究采用用户研究方法，让10名受试者执行一个简单的目标搜索任务。受试者使用了七种不同的显示器（自变量），这些显示器在镶嵌尺寸和外围细节上有所不同。帧率、目标位置、受试者输入方法和显示器使用顺序均受到控制。主要因变量是正确识别试验的搜索时间和所有试验的正确识别百分比。数据通过方差分析（ANOVAs）进行分析。", "result": "方差分析结果显示，无镶嵌的高细节显示器与有镶嵌的显示器在搜索时间或准确性上没有显著差异。事实上，只有无镶嵌的低细节显示器产生了显著不同的结果。", "conclusion": "对于简单的搜索任务，在头戴式显示器中使用高细节镶嵌可能不会比无镶嵌的高细节显示器带来显著的性能提升。只有当细节层次完全降低且无镶嵌时，性能才会显著下降。", "translation": "本文提出了一种在虚拟环境中管理细节层次的系统设计范式。作为该范式中原型设计步骤的一个例子，我们进行了一项用户研究，以评估头戴式显示器中使用高细节镶嵌的有效性。十名受试者被分配了一个简单的搜索任务，要求定位和识别一个目标物体。所有受试者使用七种不同的显示器（自变量），这些显示器在镶嵌尺寸和外围细节上有所不同，来执行此任务。帧率、目标位置、受试者输入方法和显示器使用顺序都得到了控制。主要的因变量是正确识别试验的搜索时间，以及所有正确识别试验的百分比。结果的方差分析表明，无镶嵌的高细节显示器与有镶嵌的显示器在搜索时间或准确性上没有显著差异。事实上，只有无镶嵌的低细节显示器返回了显著不同的结果。正在进行进一步的研究，以检查不同任务复杂性、镶嵌尺寸和细节层次的影响。", "summary": "本研究提出了一种虚拟环境细节层次管理系统设计范式，并通过用户研究评估了头戴式显示器中高细节镶嵌的有效性。实验中，10名受试者在不同镶嵌和细节设置的显示器上执行简单搜索任务。结果显示，对于简单任务，无镶嵌的高细节显示器与带镶嵌的显示器在性能上无显著差异，仅无镶嵌的低细节显示器导致性能显著下降。这表明高细节镶嵌对简单任务的效用可能有限。", "keywords": "细节层次, 头戴式显示器, 虚拟环境, 用户研究, 外围视觉", "comments": "这项研究为头戴式显示器中细节层次管理的设计提供了初步的实证证据。其创新之处在于通过用户研究评估了高细节镶嵌的实际效果。重要性在于它挑战了高细节镶嵌在所有场景下都带来显著优势的假设，特别是对于简单任务。局限性在于研究仅限于简单的搜索任务，未来需要探索不同任务复杂度和参数（如镶嵌尺寸）的影响。"}}
{"id": "2506.21368", "title": "Real-time and personalized product recommendations for large e-commerce platforms", "authors": ["Matteo Tolloso", "Davide Bacciu", "Shahab Mokarizadeh", "Marco Varesi"], "summary": "We present a methodology to provide real-time and personalized product\nrecommendations for large e-commerce platforms, specifically focusing on\nfashion retail. Our approach aims to achieve accurate and scalable\nrecommendations with minimal response times, ensuring user satisfaction,\nleveraging Graph Neural Networks and parsimonious learning methodologies.\nExtensive experimentation with datasets from one of the largest e-commerce\nplatforms demonstrates the effectiveness of our approach in forecasting\npurchase sequences and handling multi-interaction scenarios, achieving\nefficient personalized recommendations under real-world constraints.", "comment": "This paper has been accepted for publication at the International\n  Conference on Artificial Neural Networks (ICANN) 2025. The final\n  authenticated version will be available for purchase through the publisher's\n  website. The conference proceedings will be published by Springer in the\n  Lecture Notes in Computer Science (LNCS) series", "pdf_url": "http://arxiv.org/pdf/2506.21368v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21368v1", "AI": {"title_translation": "针对大型电商平台的实时个性化商品推荐", "tldr": "提出一种利用图神经网络和简约学习方法为大型电商平台提供实时个性化商品推荐的新方法，实验证明其在预测购买序列和处理多交互场景方面的有效性。", "motivation": "为大型电商平台提供准确、可扩展、响应时间短的实时个性化商品推荐，以确保用户满意度。", "method": "采用图神经网络（Graph Neural Networks）和简约学习方法（parsimonious learning methodologies）。", "result": "在预测购买序列和处理多交互场景方面表现出有效性，并在真实世界约束下实现了高效的个性化推荐。", "conclusion": "该方法能够为大型电商平台提供高效的实时个性化商品推荐，有效处理复杂场景。", "translation": "我们提出了一种为大型电商平台提供实时个性化商品推荐的方法，特别侧重于时尚零售。我们的方法旨在通过最小的响应时间实现准确和可扩展的推荐，确保用户满意度，并利用图神经网络和简约学习方法。对来自最大电商平台之一的数据集进行的广泛实验表明，我们的方法在预测购买序列和处理多交互场景方面是有效的，在真实世界约束下实现了高效的个性化推荐。", "summary": "本文提出了一种针对大型电商平台（特别是时尚零售）的实时个性化商品推荐方法。该方法结合了图神经网络和简约学习，旨在实现高准确度、可扩展性及低响应时间。通过在大型电商平台数据集上的实验，证明了其在预测购买序列和处理多交互场景中的有效性，能够高效地提供个性化推荐。", "keywords": "实时推荐, 个性化推荐, 图神经网络, 电商平台, 简约学习", "comments": "这项研究的创新之处在于将图神经网络与简约学习相结合，以解决大型电商平台实时个性化推荐的挑战，特别是在时尚零售领域。其重要性体现在能够提供高效、准确且低延迟的推荐服务，这对于提升用户体验和平台收益至关重要。该方法在处理多交互场景和大规模数据集方面的表现也值得关注。"}}
{"id": "2506.20917", "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "authors": ["Zhengyan Shi"], "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "comment": "PhD Thesis", "pdf_url": "http://arxiv.org/pdf/2506.20917v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20917v1", "AI": {"title_translation": "优化下游任务的语言模型：一个后训练视角", "tldr": "本论文提出了一系列后训练方法，旨在提高语言模型在下游任务中的适应性、效率和鲁棒性，解决了现有微调方法的局限性。", "motivation": "语言模型在NLP中表现出色，但将其高效、鲁棒地适应特定任务仍然具有挑战性。随着模型规模和复杂性的增长，在标注数据上进行微调常常导致未标注数据利用不足、在小型任务特定数据集上过拟合以及产生显著的计算成本。这些限制阻碍了语言模型在实际开放式语言任务中的应用。", "method": "本论文提出了一系列优化语言模型以适应下游应用的方法：首先，探索从无标注数据中提取任务相关知识的策略，引入了一种超越现有半监督方法的持续预训练技术。其次，提出了一种参数高效的微调方法，显著降低了内存和计算成本，同时保持了竞争力。此外，还引入了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标注数据稀缺时，从而提高其在包括开放式生成在内的多种NLP任务中的性能。最后，开发了新的评估方法和基准，例如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。", "result": "通过在各种NLP任务中进行广泛的实证研究，结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其能够更好地适应广泛的应用。", "conclusion": "这些进展标志着在构建更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。", "translation": "语言模型（LMs）在自然语言处理（NLP）中展示了卓越的能力，然而，如何高效且稳健地将它们适应特定任务仍然是一个挑战。随着其规模和复杂性的增长，在标注数据上对语言模型进行微调往往会导致未标注数据利用不足，在小型任务特定数据集上过度拟合，并带来显著的计算成本。这些限制阻碍了它们在现实世界开放式语言任务中的应用。\n\n本论文提出了一系列方法，旨在更好地使语言模型适应下游应用。首先，我们探索了从无标注数据中提取任务相关知识的策略，引入了一种新颖的持续预训练技术，其性能优于最先进的半监督方法。其次，我们提出了一种参数高效的微调方法，该方法显著降低了内存和计算成本，同时保持了竞争力。我们还引入了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标注数据稀缺时，从而增强了它们在包括开放式生成在内的一系列NLP任务中的性能。最后，我们开发了新的评估方法和基准，例如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。\n\n通过在各种NLP任务中进行广泛的实证研究，我们的结果表明，这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使它们能够更好地适应广泛的应用。这些进步标志着在构建更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。", "summary": "本论文针对语言模型在下游任务适应性方面的挑战，提出了一系列后训练方法。这些方法包括利用无标注数据的持续预训练、参数高效的微调、改进的监督微调以处理数据稀缺情况，以及新的评估基准。实证研究表明，这些方法显著提升了语言模型的鲁棒性、效率和泛化能力，使其更适用于广泛的实际应用，并朝着通用人工智能迈进。", "keywords": "语言模型优化, 后训练, 下游任务, 参数高效微调, 持续预训练", "comments": "该论文从后训练的角度，系统性地解决了大型语言模型在下游任务适应性上的关键挑战，特别是针对现有微调方法中存在的计算成本高、数据利用不足和过拟合等问题。其创新点在于提出了一系列综合性的方法，包括利用无标注数据的持续预训练、参数高效微调以及改进的监督微调，这些方法协同作用，显著提升了模型的效率、鲁棒性和泛化能力。此外，引入新的评估方法也提升了对模型能力的全面评估。这项工作对于推动语言模型在实际应用中的落地具有重要意义，并对实现通用人工智能的目标做出了贡献。"}}
{"id": "2506.21478", "title": "SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture", "authors": ["Kehan Sui", "Jinxu Xiang", "Fang Jin"], "summary": "Singing voice synthesis (SVS) aims to generate expressive and high-quality\nvocals from musical scores, requiring precise modeling of pitch, duration, and\narticulation. While diffusion-based models have achieved remarkable success in\nimage and video generation, their application to SVS remains challenging due to\nthe complex acoustic and musical characteristics of singing, often resulting in\nartifacts that degrade naturalness. In this work, we propose SmoothSinger, a\nconditional diffusion model designed to synthesize high quality and natural\nsinging voices. Unlike prior methods that depend on vocoders as a final stage\nand often introduce distortion, SmoothSinger refines low-quality synthesized\naudio directly in a unified framework, mitigating the degradation associated\nwith two-stage pipelines. The model adopts a reference-guided dual-branch\narchitecture, using low-quality audio from any baseline system as a reference\nto guide the denoising process, enabling more expressive and context-aware\nsynthesis. Furthermore, it enhances the conventional U-Net with a parallel\nlow-frequency upsampling path, allowing the model to better capture pitch\ncontours and long term spectral dependencies. To improve alignment during\ntraining, we replace reference audio with degraded ground truth audio,\naddressing temporal mismatch between reference and target signals. Experiments\non the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that\nSmoothSinger achieves state-of-the-art results in both objective and subjective\nevaluations. Extensive ablation studies confirm its effectiveness in reducing\nartifacts and improving the naturalness of synthesized voices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21478v1", "categories": ["cs.SD", "cs.AI"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21478v1", "AI": {"title_translation": "SmoothSinger：一种具有多分辨率架构的歌唱语音合成条件扩散模型", "tldr": "SmoothSinger是一种条件扩散模型，用于生成高质量和自然的歌唱语音，通过直接细化低质量音频并采用参考引导的双分支架构，实现了最先进的性能。", "motivation": "歌唱语音合成（SVS）需要精确建模音高、持续时间和发音。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱复杂的声学和音乐特性，将其应用于SVS仍然具有挑战性，经常导致降低自然度的伪影。现有的方法依赖于声码器作为最终阶段，通常会引入失真。", "method": "我们提出了SmoothSinger，一个条件扩散模型，旨在合成高质量和自然的歌唱语音。与依赖声码器的先前方法不同，SmoothSinger在一个统一的框架中直接细化低质量的合成音频，减轻了两阶段管道相关的降级。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来引导去噪过程。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕捉音高轮廓和长期频谱依赖性。为了改善训练期间的对齐，我们用降级的真实音频替换参考音频，解决了参考信号和目标信号之间的时间不匹配。", "result": "在Opencpop数据集（一个大规模中文歌唱语料库）上的实验表明，SmoothSinger在客观和主观评估中都取得了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成语音自然度方面的有效性。", "conclusion": "SmoothSinger是一种有效的条件扩散模型，能够合成高质量和自然的歌唱语音，通过其独特的架构和训练策略，克服了传统SVS扩散模型中的挑战，并取得了最先进的性能。", "translation": "歌唱语音合成（SVS）旨在从乐谱中生成富有表现力的高质量人声，需要精确建模音高、持续时间和发音。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱复杂的声学和音乐特性，将其应用于SVS仍然具有挑战性，经常导致降低自然度的伪影。在这项工作中，我们提出了SmoothSinger，一个条件扩散模型，旨在合成高质量和自然的歌唱语音。与依赖声码器作为最终阶段并经常引入失真的先前方法不同，SmoothSinger在一个统一的框架中直接细化低质量的合成音频，减轻了两阶段管道相关的降级。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来引导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕捉音高轮廓和长期频谱依赖性。为了改善训练期间的对齐，我们用降级的真实音频替换参考音频，解决了参考信号和目标信号之间的时间不匹配。在Opencpop数据集（一个大规模中文歌唱语料库）上的实验表明，SmoothSinger在客观和主观评估中都取得了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成语音自然度方面的有效性。", "summary": "SmoothSinger是一种用于歌唱语音合成的条件扩散模型。它通过直接细化低质量音频、采用参考引导的双分支架构以及增强U-Net以捕捉音高和长期依赖性，解决了传统扩散模型在SVS中产生的伪影和自然度问题。该模型在训练中通过使用降级的真实音频作为参考来改善对齐。实验证明，SmoothSinger在中文歌唱数据集Opencpop上达到了最先进的性能，显著减少了伪影并提高了合成语音的自然度。", "keywords": "歌唱语音合成, 条件扩散模型, 多分辨率架构, 音频合成, 深度学习", "comments": "SmoothSinger的创新之处在于其统一的框架，直接对低质量音频进行细化，避免了两阶段管道的失真。其参考引导的双分支架构和增强的U-Net（带有并行低频上采样路径）是关键的技术贡献，有助于提高合成语音的表达力和自然度。通过在训练中替换参考音频为降级的真实音频，有效解决了时序不匹配问题。该研究在SVS领域取得了显著进展，为高质量歌唱语音合成提供了新的SOTA解决方案。"}}
{"id": "2506.21070", "title": "Inverse source problem with a posteriori interior measurements for space-time fractional diffusion equations", "authors": ["Kai Yu", "Zhiyuan Li", "Yikan Liu"], "summary": "This paper investigates an inverse source problem for space-time fractional\ndiffusion equations from a posteriori interior measurements. The uniqueness\nresult is established by the memory effect of fractional derivatives and the\nunique continuation property. For the numerical reconstruction, the inverse\nproblem is reformulated as an optimization problem with the Tikhonov\nregularization. We use the Levenberg-Marquardt method to identity the unknown\nsource from noisy measurements. Finally, we give some numerical examples to\nillustrate the efficiency and accuracy of the proposed algorithm.", "comment": "14 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.21070v1", "categories": ["math.NA", "cs.NA", "35R30, 35R11"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21070v1", "AI": {"title_translation": "具有后验内部测量的时空分数阶扩散方程逆源问题", "tldr": "本文研究了时空分数阶扩散方程的逆源问题，通过分数阶导数的记忆效应和唯一延拓性质证明了唯一性，并利用Tikhonov正则化和Levenberg-Marquardt方法进行了数值重建，数值示例验证了算法的有效性和准确性。", "motivation": "本文研究了时空分数阶扩散方程的逆源问题，旨在从后验内部测量中识别未知源。", "method": "研究通过分数阶导数的记忆效应和唯一延拓性质建立了唯一性结果。数值重建方面，将逆问题重新表述为带有Tikhonov正则化的优化问题，并使用Levenberg-Marquardt方法识别未知源。", "result": "建立了逆源问题的唯一性结果。通过数值示例验证了所提出算法的效率和准确性。", "conclusion": "所提出的算法能够有效地、准确地从噪声测量中识别时空分数阶扩散方程的未知源。", "translation": "本文研究了基于后验内部测量的时空分数阶扩散方程的逆源问题。通过分数阶导数的记忆效应和唯一延拓性质建立了唯一性结果。对于数值重建，将逆问题重新表述为带有Tikhonov正则化的优化问题。我们使用Levenberg-Marquardt方法从噪声测量中识别未知源。最后，我们给出了一些数值示例，以说明所提出算法的效率和准确性。", "summary": "本文探讨了时空分数阶扩散方程的逆源问题，利用分数阶导数的记忆效应和唯一延拓性质证明了唯一性。在数值重建方面，将该问题转化为一个Tikhonov正则化的优化问题，并采用Levenberg-Marquardt方法从噪声测量中识别未知源。数值实验结果证实了该算法的有效性和准确性。", "keywords": "逆源问题, 分数阶扩散方程, Tikhonov正则化, Levenberg-Marquardt方法, 唯一性", "comments": "该研究在分数阶扩散方程的逆源问题上具有重要意义，其创新性在于结合了分数阶导数的特性、优化方法和正则化技术来解决这一挑战性问题。所提出的算法在数值上表现出良好的效率和准确性。"}}
{"id": "2506.21266", "title": "KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks", "authors": ["Daniil Karol", "Elizaveta Artser", "Ilya Vlasov", "Yaroslav Golubev", "Hieke Keuning", "Anastasiia Birillo"], "summary": "Collecting data of students solving programming tasks is incredibly valuable\nfor researchers and educators. It allows verifying that the students correctly\napply the features and concepts they are taught, or finding students'\nmisconceptions. However, existing data collection tools have limitations, e.g.,\nno control over the granularity of the collected code, not collecting the\nspecific events of the programming environment used, and overall being hard to\nconfigure.\n  To overcome these limitations, we propose KOALA, a convenient and highly\nconfigurable tool for collecting code snapshots and feature usage from students\nsolving programming tasks in JetBrains IDEs. The plugin can be installed in\nIDEs and configured to provide the students with the necessary tasks, enable or\ndisable certain IDE features like code completion, and run surveys. During\nproblem solving, the plugin collects code snapshots at the configured\ngranularity, all IDE actions like running and debugging, as well as some data\nnot collected in prior works, like employed hotkeys and switching focus between\nfiles. The collected data is sent to the server that comes with the tool, where\nit is stored and can be converted to the standardized ProgSnap2 format. To\nshowcase the tool, we collected data from 28 students solving tasks in two\ncourses within the IDE, highlighting some insights from this data.", "comment": "Accepted to CompEd'25, 7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21266v1", "categories": ["cs.SE", "cs.CY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21266v1", "AI": {"title_translation": "KOALA：一个用于在解决编程任务时收集IDE数据的可配置工具", "tldr": "KOALA是一个高度可配置的工具，用于在学生解决编程任务时收集IDE数据，克服了现有工具在数据粒度控制和事件收集方面的局限性。", "motivation": "现有的编程任务数据收集工具存在局限性，例如无法控制代码收集的粒度、不收集特定编程环境事件以及难以配置。研究人员和教育工作者需要这些数据来验证学生对概念的应用或发现其误解。", "method": "本文提出了KOALA，一个方便且高度可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用数据。该插件可安装在IDE中，配置提供任务、启用/禁用IDE功能和运行调查。它收集配置粒度的代码快照、所有IDE操作（如运行、调试）、热键使用和文件焦点切换等数据。收集到的数据发送到配套服务器存储，并可转换为ProgSnap2格式。", "result": "为了展示该工具，研究人员从28名学生那里收集了在IDE中解决两个课程任务的数据，并从中获得了一些见解。", "conclusion": "KOALA是一个克服现有局限性的可配置工具，能够收集更细致和全面的学生编程行为数据，对研究人员和教育工作者有价值。", "translation": "收集学生解决编程任务的数据对研究人员和教育工作者来说非常有价值。它允许验证学生是否正确应用了所学的特性和概念，或者发现学生的错误概念。然而，现有数据收集工具存在局限性，例如无法控制所收集代码的粒度，不收集所使用的编程环境的特定事件，以及总体上难以配置。\n为了克服这些局限性，我们提出了KOALA，一个方便且高度可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用情况。该插件可以安装在IDE中，并配置为向学生提供必要的任务，启用或禁用某些IDE功能（如代码自动补全），以及运行调查。在解决问题期间，该插件会以配置的粒度收集代码快照，所有IDE操作（如运行和调试），以及一些先前工作中未收集到的数据，例如使用的快捷键和文件之间焦点切换。收集到的数据会发送到工具附带的服务器，在那里存储并可以转换为标准化的ProgSnap2格式。为了展示该工具，我们收集了28名学生在IDE中解决两门课程任务的数据，并强调了这些数据的一些见解。", "summary": "本文介绍了KOALA，一个为克服现有工具局限性而设计的高度可配置工具，用于在学生使用JetBrains IDE解决编程任务时收集详细的编程行为数据。KOALA能够以可配置的粒度收集代码快照、IDE操作、热键使用和文件焦点切换等数据，并将其存储为ProgSnap2格式。通过在28名学生中进行数据收集，展示了该工具的有效性及其在教育研究中的应用潜力。", "keywords": "IDE数据收集, 编程教育, 可配置工具, 学生行为分析, JetBrains IDE", "comments": "KOALA的创新之处在于其高度可配置性、能够收集更细粒度和更全面的IDE行为数据（如热键和焦点切换），以及支持将数据转换为ProgSnap2标准格式，这对于促进编程教育领域的数据共享和研究具有重要意义。它直接解决了现有工具在数据粒度控制和特定环境事件收集方面的不足。"}}
{"id": "2506.21417", "title": "Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation", "authors": ["Yunxiu Xu", "Siyu Wang", "Shoichi Hasegawa"], "summary": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device, designed\nwith thin strings and actuators attached to the fingernails, ensures minimal\nweight (1.55 g per finger) and preserves finger flexibility. Integrating the\nsoftware with a physics engine renders multiple types of haptic feedback (grip\nforce, collision, and sliding vibration feedback). We evaluated the device's\nperformance in pressure perception, slip feedback, typical dexterous\nmanipulation tasks, and daily operations, and we gathered user experience\nthrough subjective assessments. Our results show that participants could\nperceive and respond to pressure and vibration feedback. Through dexterous\nmanipulation experiments, we further demonstrated that these minimal haptic\ncues significantly improved virtual task efficiency, showcasing how lightweight\nhaptic feedback can enhance manipulation performance without complex\nmechanisms. The device's ability to preserve tactile sensations and minimize\nhindrance to real-world operations is a key advantage over glove-type haptic\ndevices. This research offers a potential solution for designing haptic\ninterfaces that balance lightweight construction, haptic feedback for dexterous\nmanipulation, and daily wearability.", "comment": "14 pages, 15 figures, 2 tables. Published in IEEE Transactions on\n  Haptics (Early Access)", "pdf_url": "http://arxiv.org/pdf/2506.21417v1", "categories": ["cs.HC", "H.5.2; I.3.6"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21417v1", "AI": {"title_translation": "轻量化指甲触觉设备：无阻碍指腹力和振动反馈，增强虚拟灵巧操作", "tldr": "该研究提出了一种轻量级指甲触觉设备，通过指甲提供力觉和振动反馈，显著提高了虚拟灵巧操作效率，同时不影响真实世界互动。", "motivation": "现有触觉设备在提供反馈的同时，常会阻碍真实世界的互动或增加设备重量。本研究旨在开发一种轻量化、不影响真实世界操作的触觉设备，以增强虚拟环境中的灵巧操作。", "method": "研究团队设计了一种轻量级（每根手指1.55克）可穿戴指尖触觉设备，通过连接到指甲的细线和执行器提供基于物理的触觉反馈。该设备集成了物理引擎，能够提供握力、碰撞和滑动振动等多种反馈。通过压力感知、滑移反馈、灵巧操作任务和日常操作进行评估，并收集了用户主观体验。", "result": "参与者能够感知并响应压力和振动反馈。通过灵巧操作实验，证明了这些微小的触觉提示显著提高了虚拟任务效率。该设备在保持触觉感知和最小化对真实世界操作的阻碍方面，优于手套式触觉设备。", "conclusion": "本研究提供了一种潜在的解决方案，用于设计平衡轻量化构造、灵巧操作触觉反馈和日常佩戴性的触觉界面。该轻量化指甲触觉设备能有效增强虚拟操作性能，同时不依赖复杂机制。", "translation": "本研究提出了一种轻量级、可穿戴的指尖触觉设备，为虚拟环境中的灵巧操作提供基于物理的触觉反馈，同时不阻碍真实世界互动。该设备采用连接到指甲的细线和执行器设计，确保了最小重量（每根手指1.55克），并保持了手指的灵活性。通过将软件与物理引擎集成，可提供多种类型的触觉反馈（握力、碰撞和滑动振动反馈）。我们评估了设备在压力感知、滑移反馈、典型灵巧操作任务和日常操作中的性能，并通过主观评估收集了用户体验。我们的结果表明，参与者能够感知并响应压力和振动反馈。通过灵巧操作实验，我们进一步证明这些微小的触觉提示显著提高了虚拟任务效率，展示了轻量级触觉反馈如何在没有复杂机制的情况下增强操作性能。该设备保持触觉感知并最小化对真实世界操作的阻碍的能力是其相对于手套式触觉设备的关键优势。这项研究为设计平衡轻量化结构、灵巧操作触觉反馈和日常佩戴性的触觉界面提供了一种潜在的解决方案。", "summary": "本研究介绍了一种创新的轻量化指甲触觉设备，该设备通过连接到指甲的细线和执行器，为虚拟环境中的灵巧操作提供力觉和振动反馈。其设计仅重1.55克/指，确保了手指灵活性和对真实世界操作的无阻碍。实验证明，该设备能有效传递压力和振动反馈，并显著提升虚拟任务效率。与传统手套式设备相比，其保持触觉感知和日常佩戴性的优势，使其成为一种平衡轻量化、高效反馈和实用性的触觉界面解决方案。", "keywords": "指甲触觉设备, 轻量化, 虚拟灵巧操作, 触觉反馈, 可穿戴设备", "comments": "该研究的创新之处在于其独特的指甲附着设计，实现了极低的重量和对指腹的无阻碍，解决了传统手套式触觉设备笨重和影响真实操作的问题。其重要性体现在为虚拟现实和增强现实中的灵巧操作提供了更自然、更沉浸的交互方式，具有广泛的应用潜力。"}}
{"id": "2506.21041", "title": "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling", "authors": ["Junwei You", "Pei Li", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Haotian Shi", "Bin Ran"], "summary": "Ensuring robust planning and decision-making under rare, diverse, and\nvisually degraded long-tail scenarios remains a fundamental challenge for\nautonomous driving in urban environments. This issue becomes more critical in\ncooperative settings, where vehicles and infrastructure jointly perceive and\nreason across complex environments. To address this challenge, we propose\nV2X-REALM, a vision-language model (VLM)-based framework with adaptive\nmultimodal learning for robust cooperative autonomous driving under long-tail\nscenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven\nlong-tail scenario generation and evaluation pipeline that leverages foundation\nmodels to synthesize realistic long-tail conditions such as snow and fog across\nvehicle- and infrastructure-side views, enriching training diversity\nefficiently; (ii) a gated multi-scenario adaptive attention module that\nmodulates the visual stream using scenario priors to recalibrate ambiguous or\ncorrupted features; and (iii) a multi-task scenario-aware contrastive learning\nobjective that improves multimodal alignment and promotes cross-scenario\nfeature separability. Extensive experiments demonstrate that V2X-REALM\nsignificantly outperforms existing baselines in robustness, semantic reasoning,\nsafety, and planning accuracy under complex, challenging driving conditions,\nadvancing the scalability of end-to-end cooperative autonomous driving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21041v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21041v1", "AI": {"title_translation": "V2X-REALM：基于视觉语言模型的鲁棒端到端协同自动驾驶与自适应长尾建模", "tldr": "V2X-REALM提出了一种基于视觉语言模型的框架，通过生成长尾场景、自适应注意力模块和对比学习，显著提升了协同自动驾驶在复杂长尾场景下的鲁棒性和性能。", "motivation": "在城市环境中，自动驾驶在罕见、多样化和视觉退化的长尾场景下，确保鲁棒的规划和决策仍然是一个基本挑战。在车辆和基础设施共同感知和推理的协同设置中，这个问题变得更加关键。", "method": "本文提出了V2X-REALM，一个基于视觉语言模型（VLM）的框架，采用自适应多模态学习，用于在长尾场景下实现鲁棒的协同自动驾驶。V2X-REALM引入了三项核心创新：(i) 一个提示驱动的长尾场景生成和评估管道，利用基础模型合成跨车辆和基础设施视角的真实长尾条件（如雪和雾），有效丰富训练多样性；(ii) 一个门控多场景自适应注意力模块，利用场景先验来调节视觉流，以重新校准模糊或损坏的特征；(iii) 一个多任务场景感知对比学习目标，以改善多模态对齐并促进跨场景特征的可分离性。", "result": "大量实验表明，V2X-REALM在复杂、具有挑战性的驾驶条件下，在鲁棒性、语义推理、安全性以及规划精度方面显著优于现有基线。", "conclusion": "V2X-REALM显著提升了在复杂长尾场景下协同自动驾驶的性能，并推动了端到端协同自动驾驶的可扩展性。", "translation": "确保在罕见、多样化和视觉退化的长尾场景下进行鲁棒的规划和决策，仍然是城市环境中自动驾驶面临的一个基本挑战。在车辆和基础设施共同感知和推理的协同设置中，这个问题变得更加关键。为了解决这一挑战，我们提出了V2X-REALM，一个基于视觉语言模型（VLM）的框架，采用自适应多模态学习，用于在长尾场景下实现鲁棒的协同自动驾驶。V2X-REALM引入了三项核心创新：(i) 一个提示驱动的长尾场景生成和评估管道，利用基础模型合成跨车辆和基础设施视角的真实长尾条件（如雪和雾），有效丰富训练多样性；(ii) 一个门控多场景自适应注意力模块，利用场景先验来调节视觉流，以重新校准模糊或损坏的特征；(iii) 一个多任务场景感知对比学习目标，以改善多模态对齐并促进跨场景特征的可分离性。大量实验表明，V2X-REALM在复杂、具有挑战性的驾驶条件下，在鲁棒性、语义推理、安全性以及规划精度方面显著优于现有基线，推动了端到端协同自动驾驶的可扩展性。", "summary": "V2X-REALM是一个基于视觉语言模型的协同自动驾驶框架，旨在解决长尾场景下的鲁棒性问题。它通过引入提示驱动的长尾场景生成、门控多场景自适应注意力模块和多任务场景感知对比学习目标三项核心创新，有效提升了模型在恶劣条件下的性能。实验证明，V2X-REALM在鲁棒性、语义推理、安全性和规划精度方面均显著超越现有基线，从而提高了端到端协同自动驾驶的实用性和可扩展性。", "keywords": "自动驾驶, 视觉语言模型, 长尾场景, 协同驾驶, 鲁棒性", "comments": "V2X-REALM的创新之处在于其结合视觉语言模型，并针对自动驾驶的长尾问题提出了系统性的解决方案，包括数据增强、特征校准和多模态对齐。特别是利用基础模型生成长尾场景，为解决数据稀缺性提供了高效途径，对提升自动驾驶在复杂现实世界中的鲁棒性具有重要意义。"}}
{"id": "2506.21393", "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding", "authors": ["Junwen Zhang", "Pu Chen", "Yin Zhang"], "summary": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.", "comment": "43 pages and 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.21393v1", "categories": ["cs.AI", "68T07 (Primary), 68T50, 68T30, 68T45 (Secondary)", "F.2.2; I.2.7; I.2.10"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21393v1", "AI": {"title_translation": "TableMoE：用于多模态表格理解中结构化专家推理的神经符号路由", "tldr": "TableMoE通过神经符号路由将表格元素动态路由到专业专家，显著提升了在复杂真实世界条件下对多模态表格的理解能力，并超越了现有SOTA模型。", "motivation": "在现实世界中，由于结构复杂性、符号密度和视觉退化（即WildStruct条件），对表格进行多模态理解极具挑战性。现有的多模态大型语言模型（MLLMs）在这种条件下表现不佳，导致性能受限和泛化能力差。", "method": "提出了TableMoE，这是一种神经符号专家连接器混合（MoCE）架构。它通过创新的神经符号路由机制，预测潜在的语义令牌角色（如表头、数据单元格），并利用符号推理图指导的置信度感知门控策略，将表格元素动态路由到专业专家（Table-to-HTML、Table-to-JSON、Table-to-Code）。为促进预训练，引入了大规模TableMoE-Align数据集（120万个表格-HTML-JSON-代码四元组）。为评估模型，策划并发布了四个挑战性的WildStruct基准测试：WMMFinQA、WMMTatQA、WMMTabDialog和WMMFinanceMath。", "result": "TableMoE显著超越了现有最先进的模型。广泛的消融研究验证了其核心组件，强调了神经符号路由和结构化专家对齐的关键作用。定性分析展示了TableMoE的可解释性和增强的鲁棒性。", "conclusion": "集成神经符号推理对于多模态表格理解是有效的，TableMoE通过这种方法增强了模型的鲁棒性和可解释性。", "translation": "在现实世界中对表格进行多模态理解具有挑战性，这是由于其结构复杂性、符号密度和视觉退化（模糊、倾斜、水印、不完整的结构或字体、多跨度或分层嵌套布局）。现有的多模态大型语言模型（MLLMs）在处理此类野外结构（WildStruct）条件时表现不佳，导致性能有限和泛化能力差。为了解决这些挑战，我们提出了TableMoE，这是一种神经符号专家连接器混合（MoCE）架构，专门用于对多模态表格数据进行鲁棒的结构化推理。TableMoE具有创新的神经符号路由机制，该机制预测潜在的语义令牌角色（例如，标题、数据单元格、轴、公式），并利用由符号推理图指导的置信度感知门控策略，将表格元素动态路由到专业专家（Table-to-HTML、Table-to-JSON、Table-to-Code）。为了促进有效的对齐驱动预训练，我们引入了大规模的TableMoE-Align数据集，该数据集包含来自金融、科学、生物医学和工业领域的120万个表格-HTML-JSON-代码四元组，专门用于模型预训练。为了进行评估，我们整理并发布了四个具有挑战性的WildStruct基准测试：WMMFinQA、WMMTatQA、WMMTabDialog和WMMFinanceMath，这些基准测试专门设计用于在现实世界的多模态退化和结构复杂性下对模型进行压力测试。实验结果表明，TableMoE显著超越了现有最先进的模型。广泛的消融研究验证了每个核心组件，强调了神经符号路由和结构化专家对齐的关键作用。通过定性分析，我们进一步展示了TableMoE的可解释性和增强的鲁棒性，强调了集成神经符号推理对多模态表格理解的有效性。", "summary": "本论文提出了TableMoE，一种神经符号专家连接器混合（MoCE）架构，旨在解决真实世界中复杂多模态表格理解的挑战。TableMoE引入创新的神经符号路由机制，根据预测的语义角色和符号推理图，将表格元素动态路由至专门专家。为支持模型训练和评估，论文还发布了大规模的TableMoE-Align数据集和四个全新的WildStruct基准测试。实验结果表明，TableMoE显著优于现有最先进的模型，验证了其神经符号推理集成在提升多模态表格理解能力方面的有效性。", "keywords": "多模态表格理解, 神经符号路由, 专家混合, TableMoE, WildStruct", "comments": "本文的创新点在于提出了TableMoE架构，特别是其神经符号路由机制，能够根据语义角色动态路由表格元素到专门专家，这在处理复杂和视觉退化的真实世界表格数据方面具有重要意义。此外，构建大规模的TableMoE-Align数据集和四个挑战性的WildStruct基准测试也为该领域的研究提供了宝贵的资源。其重要性在于有效提升了多模态表格理解在“野外结构”条件下的性能和泛化能力。"}}
{"id": "2506.21422", "title": "Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget", "authors": ["Kevin Kreutz", "Philipp Wiesner", "Monica Vitali"], "summary": "The carbon footprint of data centers has recently become a critical concern.\nSo far, most carbon-aware strategies have focused on leveraging the flexibility\nof scheduling decisions for batch processing by shifting the time and location\nof workload executions. However, such approaches cannot be applied to\nservice-oriented cloud applications, since they have to be reachable at every\npoint in time and often at low latencies. We propose a carbon-aware approach\nfor operating microservices under hourly carbon budgets. By choosing the most\nappropriate version and horizontal scaleout for each microservice, our strategy\nmaximizes user experience and revenue while staying within budget constraints.\nExperiments across various application configurations and carbon budgets\ndemonstrate that the approach adapts properly to changing workloads and carbon\nintensities.", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "pdf_url": "http://arxiv.org/pdf/2506.21422v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21422v1", "AI": {"title_translation": "碳感知微服务部署，以在预算内实现最佳用户体验", "tldr": "针对数据中心碳足迹问题，本文提出一种碳感知的微服务部署方法，通过选择合适的版本和水平扩展，在碳预算内最大化用户体验和收益，并能适应工作负载变化。", "motivation": "数据中心的碳足迹日益成为关键问题。现有的碳感知策略主要关注批处理，通过调整工作负载执行的时间和地点来利用调度灵活性，但这些方法不适用于需要始终可达且低延迟的服务导向型云应用。", "method": "提出一种在每小时碳预算下运行微服务的碳感知方法。通过为每个微服务选择最合适的版本和水平扩展，该策略在遵守预算限制的同时最大化用户体验和收益。", "result": "跨各种应用配置和碳预算进行的实验表明，该方法能够很好地适应不断变化的工作负载和碳强度。", "conclusion": "本文提出的碳感知微服务部署方法，通过智能选择微服务版本和扩展规模，在严格的碳预算下成功优化了用户体验和收益，并表现出对动态环境的良好适应性。", "translation": "数据中心的碳足迹最近已成为一个关键问题。迄今为止，大多数碳感知策略都侧重于利用调度决策的灵活性，通过改变工作负载执行的时间和位置来进行批处理。然而，此类方法无法应用于面向服务的云应用程序，因为它们必须在任何时间点都可达，并且通常需要低延迟。我们提出了一种碳感知方法，用于在每小时碳预算下运行微服务。通过为每个微服务选择最合适的版本和水平扩展，我们的策略在保持预算限制的同时最大化用户体验和收益。跨各种应用程序配置和碳预算进行的实验表明，该方法能够很好地适应不断变化的工作负载和碳强度。", "summary": "鉴于数据中心日益增长的碳足迹问题以及现有碳感知策略不适用于服务导向型应用的局限性，本文提出了一种新颖的碳感知微服务部署方法。该方法在每小时碳预算内运行微服务，通过智能选择微服务的版本和水平扩展，旨在最大化用户体验和收入。实验证明，该方法能有效适应动态的工作负载和碳强度变化。", "keywords": "碳感知, 微服务, 部署, 用户体验, 碳预算", "comments": "这篇论文的创新点在于将碳感知策略从传统的批处理领域扩展到了服务导向型微服务应用，解决了现有方法不适用于实时、低延迟服务的问题。其重要性在于提供了一种在环保约束下优化云服务性能和经济效益的实际解决方案。通过考虑微服务的版本选择和水平扩展，它提供了一个更细粒度的控制机制，以平衡碳足迹、用户体验和运营成本。"}}
{"id": "2506.20867", "title": "Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation", "authors": ["Ryosuke Kawamura", "Hideaki Hayashi", "Shunsuke Otake", "Noriko Takemura", "Hajime Nagahara"], "summary": "Dynamic facial expression recognition (DFER) is a task that estimates\nemotions from facial expression video sequences. For practical applications,\naccurately recognizing ambiguous facial expressions -- frequently encountered\nin in-the-wild data -- is essential. In this study, we propose MIDAS, a data\naugmentation method designed to enhance DFER performance for ambiguous facial\nexpression data using soft labels representing probabilities of multiple\nemotion classes. MIDAS augments training data by convexly combining pairs of\nvideo frames and their corresponding emotion class labels. This approach\nextends mixup to soft-labeled video data, offering a simple yet highly\neffective method for handling ambiguity in DFER. To evaluate MIDAS, we\nconducted experiments on both the DFEW dataset and FERV39k-Plus, a newly\nconstructed dataset that assigns soft labels to an existing DFER dataset. The\nresults demonstrate that models trained with MIDAS-augmented data achieve\nsuperior performance compared to the state-of-the-art method trained on the\noriginal dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20867v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20867v1", "AI": {"title_translation": "基于软标签数据增强的模糊动态面部表情识别增强", "tldr": "本文提出MIDAS，一种基于软标签的数据增强方法，旨在提高模糊动态面部表情识别的性能，并通过实验证明其优于现有SOTA方法。", "motivation": "在实际应用中，尤其是在野外数据中，准确识别模糊面部表情对于动态面部表情识别（DFER）至关重要。", "method": "本文提出MIDAS，一种数据增强方法，通过凸组合视频帧对及其对应的软标签（表示多个情感类别的概率）来扩充训练数据。该方法将mixup扩展到软标签视频数据，以处理DFER中的模糊性。", "result": "在DFEW数据集和新构建的FERV39k-Plus数据集上进行的实验表明，使用MIDAS增强数据训练的模型与在原始数据集上训练的最新方法相比，取得了卓越的性能。", "conclusion": "MIDAS是一种简单而高效的方法，能够有效处理动态面部表情识别中的模糊性，并显著提升识别性能。", "translation": "动态面部表情识别（DFER）是一项从面部表情视频序列中估计情绪的任务。对于实际应用而言，准确识别模糊的面部表情——这在野外数据中经常遇到——至关重要。在本研究中，我们提出了MIDAS，一种数据增强方法，旨在利用表示多个情绪类别概率的软标签，提升模糊面部表情数据的DFER性能。MIDAS通过凸组合视频帧对及其对应的表情类别标签来扩充训练数据。这种方法将mixup扩展到软标签视频数据，为处理DFER中的模糊性提供了一种简单而高效的方法。为了评估MIDAS，我们在DFEW数据集和FERV39k-Plus（一个新构建的、为现有DFER数据集分配软标签的数据集）上进行了实验。结果表明，使用MIDAS增强数据训练的模型与在原始数据集上训练的最新方法相比，取得了卓越的性能。", "summary": "本文提出MIDAS，一种基于软标签的数据增强方法，旨在提高动态面部表情识别（DFER）对模糊表情的识别能力。MIDAS通过凸组合视频帧及其对应的软标签来扩充训练数据，将mixup的思想扩展到软标签视频数据。实验结果表明，MIDAS增强的数据训练的模型在DFEW和FERV39k-Plus数据集上均优于现有SOTA方法，证明了其在处理模糊动态面部表情方面的有效性。", "keywords": "动态面部表情识别, 数据增强, 软标签, 模糊表情识别, mixup", "comments": "本文的创新点在于将mixup数据增强策略与软标签结合，应用于动态面部表情识别中模糊表情的处理，并引入了一个新的软标签数据集。这种方法简单高效，为解决实际场景中模糊表情识别的挑战提供了一条有前景的途径。"}}
{"id": "2506.20752", "title": "Characterization and Mitigation of Training Instabilities in Microscaling Formats", "authors": ["Huangyuan Su", "Mujin Kwun", "Stephanie Gil", "Sham Kakade", "Nikhil Anand"], "summary": "Training large language models is an expensive, compute-bound process that\nmust be repeated as models scale, algorithms improve, and new data is\ncollected. To address this, next-generation hardware accelerators increasingly\nsupport lower-precision arithmetic formats, such as the Microscaling (MX)\nformats introduced in NVIDIA's Blackwell architecture. These formats use a\nshared scale within blocks of parameters to extend representable range and\nperform forward/backward GEMM operations in reduced precision for efficiency\ngains. In this work, we investigate the challenges and viability of\nblock-scaled precision formats during model training. Across nearly one\nthousand language models trained from scratch -- spanning compute budgets from\n$2 \\times 10^{17}$ to $4.8 \\times 10^{19}$ FLOPs and sweeping over a broad\nrange of weight-activation precision combinations -- we consistently observe\nthat training in MX formats exhibits sharp, stochastic instabilities in the\nloss, particularly at larger compute scales. To explain this phenomenon, we\nconduct controlled experiments and ablations on a smaller proxy model that\nexhibits similar behavior as the language model, sweeping across architectural\nsettings, hyperparameters, and precision formats. These experiments motivate a\nsimple model in which multiplicative gradient bias introduced by the\nquantization of layer-norm affine parameters and a small fraction of\nactivations can trigger runaway divergence. Through \\emph{in situ} intervention\nexperiments on our proxy model, we demonstrate that instabilities can be\naverted or delayed by modifying precision schemes mid-training. Guided by these\nfindings, we evaluate stabilization strategies in the LLM setting and show that\ncertain hybrid configurations recover performance competitive with\nfull-precision training. We release our code at\nhttps://github.com/Hither1/systems-scaling.", "comment": "14 pages + appendices", "pdf_url": "http://arxiv.org/pdf/2506.20752v1", "categories": ["cs.LG", "cs.AR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20752v1", "AI": {"title_translation": "微缩格式训练不稳定性表征与缓解", "tldr": "本文研究了在NVIDIA Blackwell架构中引入的微缩（MX）格式训练大型语言模型时出现的训练不稳定性问题，并发现这些不稳定性是由层归一化仿射参数和少量激活的量化引入的乘法梯度偏差引起的。研究提出并验证了通过在训练中途修改精度方案或采用混合配置可以有效缓解这些不稳定性，使其性能与全精度训练相当。", "motivation": "训练大型语言模型成本高昂且计算密集，随着模型规模、算法改进和新数据收集，需要重复进行。为了解决这一问题，下一代硬件加速器越来越多地支持低精度算术格式，例如NVIDIA Blackwell架构中引入的微缩（MX）格式。本文旨在研究在模型训练中使用块缩放精度格式的挑战和可行性。", "method": "研究人员训练了近千个大型语言模型，涵盖了从 $2 \\times 10^{17}$ 到 $4.8 \\times 10^{19}$ FLOPs 的计算预算，并扫描了广泛的权重-激活精度组合，以观察训练中的行为。为了解释观察到的现象，他们在一个表现出类似行为的较小代理模型上进行了受控实验和消融研究，遍历了架构设置、超参数和精度格式。通过原位干预实验，验证了修改精度方案的效果。最后，在LLM设置中评估了稳定策略。", "result": "研究一致观察到，在MX格式下训练时，损失函数会出现剧烈、随机的不稳定性，尤其是在较大的计算规模下。受控实验和消融研究表明，不稳定性是由层归一化仿射参数和少量激活的量化引入的乘法梯度偏差触发的。通过在代理模型上进行原位干预实验，证明了通过在训练中途修改精度方案可以避免或延迟不稳定性。在LLM设置中，某些混合配置能够恢复与全精度训练相当的性能。", "conclusion": "在微缩（MX）格式下训练大型语言模型会引入由量化引起的乘法梯度偏差导致的不稳定性。通过在训练中途调整精度方案或采用特定的混合配置，可以有效缓解这些不稳定性，从而在保持计算效率的同时，实现与全精度训练相媲美的性能。", "translation": "训练大型语言模型是一个昂贵且受计算限制的过程，随着模型的扩展、算法的改进和新数据的收集，这一过程必须重复进行。为了解决这个问题，下一代硬件加速器越来越多地支持低精度算术格式，例如NVIDIA Blackwell架构中引入的微缩（MX）格式。这些格式在参数块内使用共享比例来扩展可表示范围，并以降低的精度执行前向/后向GEMM操作以提高效率。在这项工作中，我们研究了模型训练过程中块缩放精度格式的挑战和可行性。我们从头开始训练了近千个语言模型——涵盖了从 $2 \\times 10^{17}$ 到 $4.8 \\times 10^{19}$ FLOPs 的计算预算，并扫描了广泛的权重-激活精度组合——我们一致观察到，在MX格式下训练时，损失函数会出现剧烈、随机的不稳定性，尤其是在较大的计算规模下。为了解释这种现象，我们对一个表现出与语言模型相似行为的较小代理模型进行了受控实验和消融研究，遍历了架构设置、超参数和精度格式。这些实验促使我们建立了一个简单的模型，其中层归一化仿射参数和少量激活的量化引入的乘法梯度偏差可以触发失控发散。通过对我们的代理模型进行原位干预实验，我们证明通过在训练中途修改精度方案可以避免或延迟不稳定性。受这些发现的指导，我们在LLM设置中评估了稳定策略，并表明某些混合配置可以恢复与全精度训练相当的性能。我们的代码已在 https://github.com/Hither1/systems-scaling 发布。", "summary": "本文深入探讨了在NVIDIA Blackwell架构中引入的微缩（MX）格式训练大型语言模型时所面临的训练不稳定性问题。通过对近千个语言模型的广泛实验，研究人员发现MX格式训练会导致损失函数出现剧烈的不稳定性，特别是在大规模计算下。进一步的受控实验揭示，这些不稳定性源于层归一化仿射参数和少量激活的量化引入的乘法梯度偏差。研究证明，通过在训练过程中动态调整精度方案或采用混合精度配置，可以有效缓解或消除这些不稳定性，从而使低精度训练的性能与全精度训练相媲美，为高效训练大型模型提供了解决方案。", "keywords": "微缩格式, 训练不稳定性, 低精度训练, 大型语言模型, 梯度偏差", "comments": "这项工作在理解和解决低精度训练（特别是微缩格式）中的不稳定性方面具有重要意义。其创新之处在于通过大规模实验和受控消融研究，精确地定位了不稳定的根本原因——量化引入的梯度偏差。此外，提出的通过中途修改精度方案或混合配置来缓解不稳定性，为实际应用提供了具体的指导。这项研究对于推动下一代硬件加速器在大型模型训练中的高效利用至关重要，有助于降低计算成本并加速模型开发。其局限性可能在于，所提出的缓解策略是否在所有可能的模型架构和数据集上都能普遍有效，以及它们对最终模型质量的长期影响。"}}
{"id": "2506.21352", "title": "Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions", "authors": ["Le Vu Anh", "Mehmet Dik", "Nguyen Viet Anh"], "summary": "Persistent Laplacians are matrix operators that track how the shape and\nstructure of data transform across scales and are popularly adopted in biology,\nphysics, and machine learning. Their eigenvalues are concise descriptors of\ngeometric and topological features in a filtration. Although earlier work\nestablished global algebraic stability for these operators, the precise change\nin a single eigenvalue when one simplex, such as a vertex, edge, or triangle,\nis added has remained unknown. This is important because downstream tools,\nincluding heat-kernel signatures and spectral neural networks, depend directly\non these eigenvalues. We close this gap by proving a uniform Lipschitz bound:\nafter inserting one simplex, every up-persistent Laplacian eigenvalue can vary\nby at most twice the Euclidean norm of that simplex's boundary, independent of\nfiltration scale and complex size. This result delivers the first\neigenvalue-level robustness guarantee for spectral topological data analysis.\nIt guarantees that spectral features remain stable under local updates and\nenables reliable error control in dynamic data settings.", "comment": "16 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21352v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.MG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21352v1", "AI": {"title_translation": "单形插入下持久拉普拉斯特征值的Lipschitz界", "tldr": "本文证明了在插入一个单形后，每个向上持久拉普拉斯特征值的变化量最多是该单形边界欧几里得范数的两倍，与过滤尺度和复形大小无关，为谱拓扑数据分析提供了首个特征值层面的鲁棒性保证。", "motivation": "持久拉普拉斯算子在生物学、物理学和机器学习中被广泛采用，其特征值是几何和拓扑特征的简洁描述符。尽管早期工作建立了这些算子的全局代数稳定性，但当插入一个单形（如顶点、边或三角形）时，单个特征值的精确变化仍然未知。这很重要，因为下游工具（包括热核签名和谱神经网络）直接依赖于这些特征值。", "method": "通过数学证明，建立了统一的Lipschitz界。", "result": "证明了在插入一个单形后，每个向上持久拉普拉斯特征值的变化量最多是该单形边界欧几里得范数的两倍，且与过滤尺度和复形大小无关。", "conclusion": "该结果为谱拓扑数据分析提供了首个特征值层面的鲁棒性保证，确保了谱特征在局部更新下保持稳定，并使得动态数据设置中的误差控制变得可靠。", "translation": "持久拉普拉斯算子是跟踪数据形状和结构如何跨尺度转换的矩阵算子，在生物学、物理学和机器学习中被广泛采用。它们的特征值是过滤中几何和拓扑特征的简洁描述符。尽管早期工作建立了这些算子的全局代数稳定性，但当插入一个单形（如顶点、边或三角形）时，单个特征值的精确变化仍然未知。这很重要，因为下游工具（包括热核签名和谱神经网络）直接依赖于这些特征值。我们通过证明一个统一的Lipschitz界来弥补这一空白：在插入一个单形后，每个向上持久拉普拉斯特征值的变化量最多是该单形边界欧几里得范数的两倍，与过滤尺度和复形大小无关。这一结果为谱拓扑数据分析提供了首个特征值层面的鲁棒性保证。它保证了谱特征在局部更新下保持稳定，并使得动态数据设置中的误差控制变得可靠。", "summary": "本文研究了在数据中插入单个单形（如顶点、边或三角形）时，持久拉普拉斯特征值的变化。尽管持久拉普拉斯算子在多个领域有广泛应用，且其全局稳定性已知，但单个特征值的精确变化此前未被量化。作者通过数学证明，建立了统一的Lipschitz界，表明插入一个单形后，每个向上持久拉普拉斯特征值的变化量最大为该单形边界欧几里得范数的两倍，且与过滤尺度和复形大小无关。这一发现为谱拓扑数据分析提供了首个特征值层面的鲁棒性保证，确保了谱特征在数据动态更新时的稳定性，并有助于实现可靠的误差控制。", "keywords": "持久拉普拉斯, 特征值, Lipschitz界, 单形插入, 拓扑数据分析", "comments": "这项工作在拓扑数据分析领域具有重要意义，首次为持久拉普拉斯特征值在局部更新下的稳定性提供了量化保证。其创新之处在于建立了具体的Lipschitz界，解决了长期以来关于单个特征值精确变化的未决问题。这对于依赖谱特征进行下游分析（如热核签名和谱神经网络）的工具至关重要，能够提高动态数据分析的可靠性和误差控制能力。"}}
{"id": "2506.21418", "title": "Vantage Point Selection Algorithms for Bottleneck Capacity Estimation", "authors": ["Vikrant Ashvinkumar", "Rezaul Chowdhury", "Jie Gao", "Mayank Goswami", "Joseph S. B. Mitchell", "Valentin Polishchuk"], "summary": "Motivated by the problem of estimating bottleneck capacities on the Internet,\nwe formulate and study the problem of vantage point selection. We are given a\ngraph $G=(V, E)$ whose edges $E$ have unknown capacity values that are to be\ndiscovered. Probes from a vantage point, i.e, a vertex $v \\in V$, along\nshortest paths from $v$ to all other vertices, reveal bottleneck edge\ncapacities along each path. Our goal is to select $k$ vantage points from $V$\nthat reveal the maximum number of bottleneck edge capacities.\n  We consider both a non-adaptive setting where all $k$ vantage points are\nselected before any bottleneck capacity is revealed, and an adaptive setting\nwhere each vantage point selection instantly reveals bottleneck capacities\nalong all shortest paths starting from that point. In the non-adaptive setting,\nby considering a relaxed model where edge capacities are drawn from a random\npermutation (which still leaves the problem of maximizing the expected number\nof revealed edges NP-hard), we are able to give a $1-1/e$ approximate\nalgorithm. In the adaptive setting we work with the least permissive model\nwhere edge capacities are arbitrarily fixed but unknown. We compare with the\nbest solution for the particular input instance (i.e. by enumerating all\nchoices of $k$ tuples), and provide both lower bounds on instance optimal\napproximation algorithms and upper bounds for trees and planar graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21418v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21418v1", "AI": {"title_translation": "瓶颈容量估计的视点选择算法", "tldr": "本文研究了在互联网上估计瓶颈容量的视点选择问题，旨在从图中选择K个视点以揭示最大数量的瓶颈边容量，并提出了非自适应和自适应设置下的近似算法和界限。", "motivation": "为了解决互联网上瓶颈容量估计的问题。", "method": "本文将问题建模为在给定图G=(V, E)中选择k个视点，以揭示沿最短路径的最大数量的瓶颈边容量。研究了两种设置：非自适应设置（所有k个视点在容量揭示前选择）和自适应设置（每个视点选择后即时揭示容量）。在非自适应设置中，考虑了一个边容量从随机排列中抽取的宽松模型，并提出了一个1-1/e近似算法。在自适应设置中，采用边容量任意固定但未知的模型，并与特定输入实例的最佳解进行比较，为实例最优近似算法提供了下界，并为树和平面图提供了上界。", "result": "在非自适应设置中，对于边容量从随机排列中抽取的宽松模型，提出了一个1-1/e近似算法。在自适应设置中，为实例最优近似算法提供了下界，并为树和平面图提供了上界。", "conclusion": "本文通过对视点选择问题的形式化研究，在非自适应和自适应设置下分别提出了近似算法和理论界限，为互联网瓶颈容量估计提供了理论支持和算法基础。", "translation": "本文的动机是解决互联网上瓶颈容量估计的问题，我们因此提出并研究了视点选择问题。我们给定一个图$G=(V, E)$，其边的容量值未知，需要被发现。从一个视点，即顶点$v \notin V$沿着从$v$到所有其他顶点的最短路径进行探测，可以揭示每条路径上的瓶颈边容量。我们的目标是从$V$中选择$k$个视点，以揭示最大数量的瓶颈边容量。\n我们考虑了两种设置：非自适应设置，其中所有$k$个视点在任何瓶颈容量被揭示之前被选择；以及自适应设置，其中每个视点选择会立即揭示从该点开始的所有最短路径上的瓶颈容量。在非自适应设置中，通过考虑一个宽松模型，其中边容量从随机排列中抽取（这仍然使得最大化预期揭示边数的问题是NP-难的），我们能够给出一个1-1/e的近似算法。在自适应设置中，我们采用最不宽松的模型，其中边容量是任意固定但未知的。我们与特定输入实例的最佳解决方案（即通过枚举所有$k$元组的选择）进行比较，并为实例最优近似算法提供了下界，同时为树和平面图提供了上界。", "summary": "本文研究了为互联网瓶颈容量估计而进行的视点选择问题。作者将问题建模为在图中选择K个视点以最大化揭示的瓶颈边容量。研究分为非自适应和自适应两种设置：非自适应设置下，提出了一个1-1/e近似算法；自适应设置下，为实例最优近似算法提供了下界，并为特定图类型（树和平面图）提供了上界。", "keywords": "瓶颈容量估计, 视点选择, 非自适应算法, 自适应算法, 近似算法", "comments": "本文创新性地将瓶颈容量估计问题转化为图论中的视点选择问题，并针对非自适应和自适应两种不同信息获取模式进行了深入研究。其提出的近似算法和理论界限，尤其是对NP-难问题的近似解，具有重要的理论和实践意义。对特定图结构（树和平面图）的分析也增强了结果的实用性。"}}
{"id": "2506.21510", "title": "Joint Scheduling of DER under Demand Charges: Structure and Approximation", "authors": ["Ruixiao Yang", "Gulai Shen", "Ahmed S. Alahmed", "Chuchu Fan"], "summary": "We study the joint scheduling of behind-the-meter distributed energy\nresources (DERs), including flexible loads, renewable generation, and battery\nenergy storage systems, under net energy metering frameworks with demand\ncharges. The problem is formulated as a stochastic dynamic program aimed at\nmaximizing expected operational surplus while accounting for renewable\ngeneration uncertainty. We analytically characterize the structure of the\noptimal control policy and show that it admits a threshold-based form. However,\ndue to the strong temporal coupling of the storage and demand charge\nconstraints, the number of conditional branches in the policy scales\ncombinatorially with the scheduling horizon, as it requires a look-ahead over\nfuture states. To overcome the high computational complexity in the general\nformulation, an efficient approximation algorithm is proposed, which searches\nfor the peak demand under a mildly relaxed problem. We show that the algorithm\nscales linearly with the scheduling horizon. Extensive simulations using two\nopen-source datasets validate the proposed algorithm and compare its\nperformance against different DER control strategies, including a reinforcement\nlearning-based one. Under varying storage and tariff parameters, the results\nshow that the proposed algorithm outperforms various benchmarks in achieving a\nrelatively small solution gap compared to the theoretical upper bound.", "comment": "15 pages, 4 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21510v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21510v1", "AI": {"title_translation": "分布式能源在需量电价下的联合调度：结构与近似", "tldr": "研究了在需量电价下分布式能源（DERs）的联合调度问题，提出了一个高效的近似算法来解决其高计算复杂性，并通过仿真验证了其性能优于现有基准。", "motivation": "在净计量框架和需量电价下，对包括柔性负荷、可再生能源发电和电池储能系统在内的表后分布式能源（DERs）进行联合调度，以最大化预期运营盈余并应对可再生能源发电的不确定性。由于最优控制策略的组合式复杂性，需要开发高效的近似算法来克服计算挑战。", "method": "问题被公式化为一个随机动态规划，旨在最大化预期运营盈余，同时考虑可再生能源发电的不确定性。分析性地描述了最优控制策略的结构，并表明它具有基于阈值的形式。为了克服通用公式中的高计算复杂性，提出了一种高效的近似算法，该算法在轻度松弛问题下搜索峰值需求。该算法的计算复杂度与调度周期呈线性关系。通过使用两个开源数据集的广泛仿真来验证所提出的算法，并将其性能与包括强化学习在内的不同DER控制策略进行比较。", "result": "最优控制策略呈现基于阈值的形式，但由于储能和需量电价约束的强时间耦合，策略中的条件分支数量随调度周期呈组合式增长，导致计算复杂度高。提出的近似算法能够将计算复杂度降低到与调度周期呈线性关系。仿真结果表明，在不同的储能和电价参数下，所提出的算法在实现相对较小的解差距方面优于各种基准，与理论上限相比表现出色。", "conclusion": "该研究成功地为需量电价下的分布式能源联合调度问题提出了一种高效的近似算法，该算法在计算效率和性能上均优于现有方法，为实际应用提供了有效工具。", "translation": "我们研究了在具有需量电价的净计量框架下，包括柔性负荷、可再生能源发电和电池储能系统在内的表后分布式能源（DERs）的联合调度问题。该问题被公式化为一个随机动态规划，旨在最大化预期运营盈余，同时考虑可再生能源发电的不确定性。我们分析性地描述了最优控制策略的结构，并表明它具有基于阈值的形式。然而，由于储能和需量电价约束的强时间耦合，策略中的条件分支数量随调度周期呈组合式增长，因为它需要对未来状态进行展望。为了克服通用公式中的高计算复杂性，提出了一种高效的近似算法，该算法在轻度松弛问题下搜索峰值需求。我们表明该算法的计算复杂度与调度周期呈线性关系。使用两个开源数据集进行的广泛仿真验证了所提出的算法，并将其性能与包括基于强化学习的策略在内的不同DER控制策略进行了比较。在不同的储能和电价参数下，结果表明所提出的算法在实现与理论上限相比相对较小的解差距方面优于各种基准。", "summary": "本研究探讨了在需量电价下，包括柔性负荷、可再生能源和电池储能系统在内的分布式能源（DERs）的联合调度问题。该问题被建模为随机动态规划以最大化运营盈余，并发现最优策略具有基于阈值的结构，但存在组合式计算复杂性。为解决此问题，论文提出了一种高效的近似算法，该算法通过搜索松弛问题下的峰值需求，实现了与调度周期线性相关的计算复杂度。广泛的仿真结果证明，该算法在性能上优于多种现有DER控制策略，并能达到接近理论最优的解。", "keywords": "分布式能源调度, 需量电价, 随机动态规划, 近似算法, 储能系统", "comments": "本文创新性地解决了在需量电价下分布式能源联合调度的复杂性问题。通过深入分析最优控制策略的结构，并提出了一种计算效率高（线性复杂度）的近似算法，极大地提升了该问题在实际应用中的可行性。其对理论最优解的接近程度，以及在与强化学习等方法比较中的优越性，都凸显了该方法的实用价值和重要性。该研究对于推动智能电网中分布式能源的优化管理具有重要意义。"}}
{"id": "2506.21112", "title": "Point Cloud Environment-Based Channel Knowledge Map Construction", "authors": ["Yancheng Wang", "Wei Guo", "Guanying Chen", "Ye Zhang", "Shuguang Cui"], "summary": "Channel knowledge map (CKM) provides certain levels of channel state\ninformation (CSI) for an area of interest, serving as a critical enabler for\nenvironment-aware communications by reducing the overhead of frequent CSI\nacquisition. However, existing CKM construction schemes adopt over-simplified\nenvironment information, which significantly compromises their accuracy. To\naddress this issue, this work proposes a joint model- and data-driven approach\nto construct CKM by leveraging point cloud environmental data along with a few\nsamples of location-tagged channel information. First, we propose a novel point\nselector to identify subsets of point cloud that contain environmental\ninformation relevant to multipath channel gains, by constructing a set of\nco-focal ellipsoids based on different time of arrival (ToAs). Then, we trained\na neural channel gain estimator to learn the mapping between each selected\nsubset and its corresponding channel gain, using a real-world dataset we\ncollected through field measurements, comprising environmental point clouds and\ncorresponding channel data. Finally, experimental results demonstrate that: For\nCKM construction of power delay profile (PDP), the proposed method achieves a\nroot mean squared error (RMSE) of 2.95 dB, significantly lower than the 7.32 dB\nachieved by the conventional ray-tracing method; for CKM construction of\nreceived power values, i.e., radio map, it achieves an RMSE of 1.04 dB,\nsurpassing the Kriging interpolation method with an RMSE of 1.68 dB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21112v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21112v1", "AI": {"title_translation": "基于点云环境的信道知识图谱构建", "tldr": "该文提出一种结合点云数据的模型与数据驱动方法，用于构建精确的信道知识图谱，性能优于现有方法。", "motivation": "现有信道知识图谱(CKM)构建方案采用过于简化的环境信息，导致准确性显著降低。", "method": "本文提出一种模型驱动与数据驱动相结合的方法来构建CKM，利用点云环境数据和少量带位置标签的信道信息样本。首先，提出一种新颖的点选择器，通过基于不同到达时间（ToAs）构建共焦椭球来识别与多径信道增益相关的点云子集。然后，训练一个神经信道增益估计器，利用真实世界数据集学习选定子集与相应信道增益之间的映射。", "result": "对于功率延迟剖面(PDP)的CKM构建，所提方法实现了2.95 dB的均方根误差（RMSE），显著低于传统射线追踪方法实现的7.32 dB。对于接收功率值（即无线电图）的CKM构建，它实现了1.04 dB的RMSE，优于克里金插值法1.68 dB的RMSE。", "conclusion": "该方法通过有效利用点云环境数据，显著提高了功率延迟剖面(PDP)和接收功率值(无线电图)的信道知识图谱构建精度。", "translation": "信道知识图谱（CKM）为感兴趣区域提供一定程度的信道状态信息（CSI），通过减少频繁CSI获取的开销，成为环境感知通信的关键使能技术。然而，现有的CKM构建方案采用过于简化的环境信息，这显著损害了其准确性。为解决此问题，本文提出一种模型驱动与数据驱动相结合的方法，通过利用点云环境数据以及少量带位置标签的信道信息样本来构建CKM。首先，我们提出了一种新颖的点选择器，通过基于不同到达时间（ToAs）构建一组共焦椭球，来识别包含与多径信道增益相关的环境信息的点云子集。然后，我们利用通过实地测量收集的真实世界数据集（包含环境点云和相应的信道数据），训练了一个神经信道增益估计器，以学习每个选定子集与其相应信道增益之间的映射。最后，实验结果表明：对于功率延迟剖面（PDP）的CKM构建，所提方法实现了2.95 dB的均方根误差（RMSE），显著低于传统射线追踪方法实现的7.32 dB；对于接收功率值（即无线电图）的CKM构建，它实现了1.04 dB的RMSE，超过了克里金插值法1.68 dB的RMSE。", "summary": "信道知识图谱(CKM)对环境感知通信至关重要，但现有方法因环境信息简化而准确性不足。本文提出一种结合点云环境数据和信道样本的模型与数据驱动方法来构建CKM。该方法通过点选择器识别相关点云子集，并训练神经信道增益估计器。实验结果表明，所提方法在功率延迟剖面CKM构建中，RMSE为2.95 dB，优于传统射线追踪的7.32 dB；在接收功率值CKM构建中，RMSE为1.04 dB，优于克里金插值的1.68 dB，显著提高了CKM的准确性。", "keywords": "信道知识图谱, 点云, 环境感知通信, 机器学习, 信道建模", "comments": "本文的创新之处在于利用详细的点云数据和模型与数据相结合的方法，克服了现有CKM构建中环境模型过于简化的局限性。这显著提高了CKM的准确性，对环境感知通信至关重要。"}}
{"id": "2506.21499", "title": "Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising", "authors": ["Hojat Asgariandehkordi", "Mostafa Sharifzadeh", "Hassan Rivaz"], "summary": "Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by\ncombining echoes from multiple steered transmissions. While increasing the\nnumber of angles generally improves image quality, it drastically reduces the\nframe rate and can introduce blurring artifacts in fast-moving targets.\nMoreover, compounded images remain susceptible to noise, particularly when\nacquired with a limited number of transmissions. We propose a zero-shot\ndenoising framework tailored for low-angle CPWC acquisitions, which enhances\ncontrast without relying on a separate training dataset. The method divides the\navailable transmission angles into two disjoint subsets, each used to form\ncompound images that include higher noise levels. The new compounded images are\nthen used to train a deep model via a self-supervised residual learning scheme,\nenabling it to suppress incoherent noise while preserving anatomical\nstructures. Because angle-dependent artifacts vary between the subsets while\nthe underlying tissue response is similar, this physics-informed pairing allows\nthe network to learn to disentangle the inconsistent artifacts from the\nconsistent tissue signal. Unlike supervised methods, our model requires no\ndomain-specific fine-tuning or paired data, making it adaptable across\nanatomical regions and acquisition setups. The entire pipeline supports\nefficient training with low computational cost due to the use of a lightweight\narchitecture, which comprises only two convolutional layers. Evaluations on\nsimulation, phantom, and in vivo data demonstrate superior contrast enhancement\nand structure preservation compared to both classical and deep learning-based\ndenoising methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21499v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21499v1", "AI": {"title_translation": "轻量级物理信息零样本超声平面波去噪", "tldr": "本文提出了一种轻量级的物理信息零样本去噪框架，用于低角度超声相干平面波复合(CPWC)图像，通过自监督残差学习在不依赖外部训练数据的情况下提高对比度并保留结构。", "motivation": "超声相干平面波复合(CPWC)虽然能增强图像对比度，但增加角度会显著降低帧率并引入模糊伪影；此外，有限传输角度下的复合图像仍易受噪声影响。", "method": "该方法将可用的传输角度分为两个不相交的子集，每个子集用于形成包含更高噪声水平的复合图像。这些新的复合图像通过自监督残差学习方案训练一个深度模型，使其能够抑制非相干噪声同时保留解剖结构。通过物理信息配对，网络可以学习区分不一致的伪影和一致的组织信号。模型采用轻量级架构（仅包含两个卷积层），无需领域特定微调或配对数据。", "result": "在仿真、体模和体内数据上的评估表明，与经典和基于深度学习的去噪方法相比，该方法在对比度增强和结构保留方面表现出优越性。", "conclusion": "该轻量级物理信息零样本去噪框架能有效提高低角度超声CPWC图像的对比度并保留结构，且具有高适应性和计算效率，无需外部训练数据。", "translation": "超声相干平面波复合(CPWC)通过结合来自多个转向传输的回波来增强图像对比度。虽然增加角度数量通常能提高图像质量，但这会显著降低帧率，并可能在快速移动目标中引入模糊伪影。此外，复合图像仍然容易受到噪声影响，尤其是在传输次数有限的情况下采集时。我们提出了一种专为低角度CPWC采集定制的零样本去噪框架，该框架无需单独的训练数据集即可增强对比度。该方法将可用的传输角度分为两个不相交的子集，每个子集用于形成包含更高噪声水平的复合图像。然后，这些新的复合图像通过自监督残差学习方案训练一个深度模型，使其能够抑制非相干噪声同时保留解剖结构。由于角度依赖性伪影在子集之间有所不同，而底层组织响应相似，这种物理信息配对允许网络学习将不一致的伪影与一致的组织信号分离。与监督方法不同，我们的模型不需要领域特定的微调或配对数据，使其能够适应不同的解剖区域和采集设置。由于采用了轻量级架构（仅包含两个卷积层），整个管道支持高效训练且计算成本低。在仿真、体模和体内数据上的评估表明，与经典和基于深度学习的去噪方法相比，该方法在对比度增强和结构保留方面表现出优越性。", "summary": "本文提出了一种轻量级的物理信息零样本去噪框架，专门用于解决低角度超声CPWC图像的噪声问题。该方法通过将传输角度分成两组，自监督训练一个深度模型来识别并抑制噪声，同时保留组织结构。其创新之处在于无需外部训练数据或特定微调，且计算成本低。实验结果表明，该方法在对比度增强和结构保留方面优于现有去噪技术。", "keywords": "超声去噪, 零样本学习, 相干平面波复合, 自监督学习, 物理信息", "comments": "该论文提出了一种创新的零样本去噪方法，通过利用超声物理特性进行自监督学习，解决了传统监督方法对大量配对数据和领域特异性微调的需求。其轻量级架构和无需外部训练数据的特点，使其在实际应用中具有高度的适应性和效率，对于临床超声图像质量的提升具有重要意义。"}}
{"id": "2506.21456", "title": "Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "summary": "Previous work has demonstrated the utility of reductions in the level of\ndetail (LOD) in the periphery of head-tracked, large field of view displays.\nThis paper provides a psychophysically based model, centered around an eye/head\nmovement tradeoff, that explains the effectiveness of peripheral degradation\nand suggests how peripherally degraded displays should be designed. An\nexperiment evaluating the effect on search performance of the shape and area of\nthe high detail central area (inset) in peripherally degraded displays was\nperformed, results indicated that inset shape is not a significant factor in\nperformance. Inset area, however, was significant: performance with displays\nsubtending at least 30 degrees of horizontal and vertical angle was not\nsignificantly different from performance with an undegraded display. These\nresults agreed with the proposed model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21456v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21456v1", "AI": {"title_translation": "通过头部追踪的周边退化管理细节水平：一个模型和由此产生的设计原则", "tldr": "本文提出一个基于心理物理学的模型，解释了头部追踪大视场显示器中周边细节水平下降的有效性，并通过实验验证了中心高细节区域的面积而非形状对搜索性能的影响，并据此提出了设计原则。", "motivation": "之前的研究证明了在头部追踪的大视场显示器中，降低周边细节水平（LOD）的实用性。本文旨在提供一个基于心理物理学的模型，解释周边退化的有效性，并据此提出周边退化显示器的设计原则。", "method": "进行了一项实验，评估了周边退化显示器中高细节中心区域（内嵌区域）的形状和面积对搜索性能的影响。", "result": "实验结果表明，内嵌区域的形状对性能不是一个显著因素。然而，内嵌区域的面积是显著的：水平和垂直视角至少为30度的显示器，其性能与未退化显示器的性能没有显著差异。", "conclusion": "实验结果与本文提出的模型一致，证明了该模型能够解释周边细节退化的有效性并指导设计。", "translation": "以前的工作已经证明了在头部追踪、大视场显示器中，降低周边细节水平（LOD）的实用性。本文提供了一个基于心理物理学的模型，该模型围绕眼/头运动权衡，解释了周边退化的有效性，并提出了如何设计周边退化显示器。进行了一项实验，评估了周边退化显示器中高细节中心区域（内嵌区域）的形状和面积对搜索性能的影响，结果表明内嵌区域的形状对性能不是一个显著因素。然而，内嵌区域的面积是显著的：水平和垂直视角至少为30度的显示器，其性能与未退化显示器的性能没有显著差异。这些结果与所提出的模型一致。", "summary": "本文提出一个基于眼/头运动权衡的心理物理学模型，旨在解释头部追踪大视场显示器中周边细节水平下降的有效性，并指导其设计。通过实验验证了中心高细节区域的形状对搜索性能无显著影响，而面积则有显著影响，发现水平和垂直视角至少30度的显示器性能与未退化显示器相当。这些结果支持了所提出的模型。", "keywords": "细节水平管理, 头部追踪, 周边退化, 心理物理学模型, 显示器设计", "comments": "本文的创新之处在于提出了一个基于心理物理学的模型来解释周边细节退化的有效性，并通过实验验证了该模型，为大视场显示器的优化设计提供了具体的指导原则，特别是关于高细节中心区域的面积而非形状的重要性。这对于提升虚拟现实或增强现实等应用的用户体验和系统效率具有重要意义。"}}
{"id": "2506.20920", "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20920v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20920v1", "AI": {"title_translation": "FineWeb2：一个管道搞定一切——使预训练数据处理适应每种语言", "tldr": "本文介绍了一个基于FineWeb的新型预训练数据集整理管道，该管道可以自动适应任何语言，并用于创建比现有数据集性能更好的非英语语料库。该研究还引入了一种重新平衡数据集的方法，并最终扩展到1000多种语言，生成了20TB的FineWeb2多语言数据集。", "motivation": "预训练最先进的大型语言模型（LLMs）需要大量干净且多样化的文本数据。尽管高质量英语预训练数据集的开放开发取得了实质性进展，但训练高性能多语言LLMs仍然是一个挑战，这主要是因为为大量语言定制过滤和去重管道固有的困难。", "method": "本文引入了一个基于FineWeb的新预训练数据集整理管道，该管道可以自动适应任何语言。研究人员在九种不同语言上对管道设计选择进行了广泛的消融实验，并引入了一种简单且有原则的方法来重新平衡数据集，该方法考虑了重复计数和质量。最终，他们将管道扩展到1000多种语言，使用了近100个Common Crawl快照来生成FineWeb2。", "result": "该管道可用于创建比现有数据集性能更好的非英语语料库。此外，重新平衡数据集的方法提供了额外的性能提升。最终生成了FineWeb2，一个20TB（50亿文档）的多语言数据集。", "conclusion": "本文成功开发并扩展了一个可自动适应任何语言的预训练数据处理管道，并创建了大规模、高质量的多语言数据集FineWeb2，显著提升了多语言LLMs的性能。", "translation": "预训练最先进的大型语言模型（LLMs）需要大量干净且多样化的文本数据。尽管大型高质量英语预训练数据集的开放开发最近取得了实质性进展，但训练高性能多语言LLMs仍然是一个挑战，这在很大程度上是由于为大量语言定制过滤和去重管道固有的困难。在这项工作中，我们引入了一个基于FineWeb的新型预训练数据集整理管道，该管道可以自动适应以支持任何语言。我们在一组九种不同语言上广泛消融了我们的管道设计选择，并由一组有意义且信息丰富的评估任务指导，这些任务是通过基于可测量标准的新颖选择过程选出的。最终，我们表明我们的管道可用于创建比现有数据集产生更高性能模型的非英语语料库。我们还引入了一种直接且有原则的方法来重新平衡数据集，该方法同时考虑了重复计数和质量，提供了额外的性能提升。最后，我们使用近100个Common Crawl快照将我们的管道扩展到1000多种语言，以生产FineWeb2，这是一个新的20TB（50亿文档）多语言数据集，我们随同我们的管道、训练和评估代码库一起发布。", "summary": "本文提出了FineWeb2，一个可自动适应任何语言的预训练数据处理管道，旨在解决多语言LLM训练中数据处理的挑战。该管道基于FineWeb，并经过广泛的消融实验验证，能够生成比现有数据集性能更优的非英语语料库。此外，研究引入了一种考虑重复计数和质量的数据集重新平衡方法，进一步提升了模型性能。最终，该管道被扩展到1000多种语言，利用近100个Common Crawl快照生成了20TB的FineWeb2多语言数据集，并随代码库一同发布。", "keywords": "预训练数据, 多语言LLM, FineWeb2, 数据处理, 数据集平衡", "comments": "这项工作在解决多语言大型语言模型预训练数据处理的挑战方面具有重要意义。其创新之处在于开发了一个可自动适应多种语言的通用管道，极大地简化了多语言数据集的构建过程。发布大规模的FineWeb2数据集以及相关的代码库，将对多语言LLM的开放研究和发展产生积极影响。该方法通过系统性的消融实验和数据集重新平衡策略，确保了数据质量和模型性能的提升。"}}
{"id": "2506.21104", "title": "Robust space-time multiscale upscaling via multicontinuum homogenization for evolving perforated media", "authors": ["Wei Xie", "Viet Ha Hoang", "Yin Yang", "Yunqing Huang"], "summary": "Time-evolving perforated domains arise in many engineering and geoscientific\napplications, including reactive transport, particle deposition, and structural\ndegradation in porous media. Accurately capturing the macroscopic behavior of\nsuch systems poses significant computational challenges due to the dynamic\nfine-scale geometries. In this paper, we develop a robust and generalizable\nmultiscale modeling framework based on multicontinuum homogenization to derive\neffective macroscopic equations in shrinking domains. The method distinguishes\nmultiple continua according to the physical characteristics (e.g., channel\nwidths), and couples them via space-time local cell problems formulated on\nrepresentative volume elements. These local problems incorporate temporal\nderivatives and domain evolution, ensuring consistency with underlying\nfine-scale dynamics. The resulting upscaled system yields computable\nmacroscopic coefficients and is suitable for large-scale simulations. Several\nnumerical experiments are presented to validate the accuracy, efficiency, and\npotential applicability of the method to complex time-dependent engineering\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21104v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21104v1", "AI": {"title_translation": "演化穿孔介质中基于多连续体均匀化的鲁棒时空多尺度粗化", "tldr": "本文提出了一种基于多连续体均匀化的鲁棒多尺度建模框架，用于在收缩域中推导宏观有效方程，以解决动态细尺度几何形状带来的计算挑战。", "motivation": "由于多孔介质中反应传输、颗粒沉积和结构退化等应用中，时间演化的穿孔域具有动态的细尺度几何形状，准确捕捉其宏观行为存在显著的计算挑战。", "method": "本文开发了一种基于多连续体均匀化的鲁棒且可泛化的多尺度建模框架，用于在收缩域中推导有效的宏观方程。该方法根据物理特性（例如通道宽度）区分多个连续体，并通过在代表性体积单元上制定的时空局部单元问题将它们耦合。这些局部问题包含了时间导数和域演化，确保与底层细尺度动力学的一致性。", "result": "由此产生的粗化系统得到了可计算的宏观系数，并适用于大规模模拟。数值实验验证了该方法的准确性、效率以及在复杂瞬态工程问题中的潜在适用性。", "conclusion": "该方法成功地为时间演化的穿孔介质提供了鲁棒且高效的多尺度建模框架，能够准确捕捉宏观行为并适用于大规模模拟。", "translation": "时间演化的穿孔域出现在许多工程和地球科学应用中，包括多孔介质中的反应传输、颗粒沉积和结构退化。由于动态的细尺度几何形状，准确捕捉此类系统的宏观行为带来了显著的计算挑战。在本文中，我们开发了一种基于多连续体均匀化的鲁棒且可泛化的多尺度建模框架，以在收缩域中推导有效的宏观方程。该方法根据物理特性（例如通道宽度）区分多个连续体，并通过在代表性体积单元上制定的时空局部单元问题将它们耦合。这些局部问题包含了时间导数和域演化，确保与底层细尺度动力学的一致性。由此产生的粗化系统得到了可计算的宏观系数，并适用于大规模模拟。本文展示了几个数值实验，以验证该方法的准确性、效率以及在复杂瞬态工程问题中的潜在适用性。", "summary": "本文提出了一种基于多连续体均匀化的鲁棒时空多尺度建模框架，旨在解决时间演化穿孔介质中动态细尺度几何带来的计算挑战。该方法通过区分不同物理特性的连续体，并利用包含时间导数和域演化的时空局部单元问题进行耦合，从而在收缩域中推导出宏观有效方程。该框架产生的粗化系统具有可计算的宏观系数，适用于大规模模拟，并通过数值实验验证了其准确性、效率和广泛适用性。", "keywords": "多尺度粗化, 多连续体均匀化, 穿孔介质, 时空, 动态域", "comments": "该论文的创新点在于将多连续体均匀化方法应用于时间演化的穿孔介质，并考虑了域的动态演化和时间导数，这对于模拟复杂工程和地球科学问题具有重要意义。其提出的框架具有鲁棒性和泛化性，为处理动态细尺度几何问题提供了一个有效的计算工具。"}}
{"id": "2506.20816", "title": "Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers", "authors": ["Furkan Mumcu", "Yasin Yilmaz"], "summary": "Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input\ndesigns with limited noise budgets. While numerous successful attacks with\nsubtle modifications to original input have been proposed, defense techniques\nagainst these attacks are relatively understudied. Existing defense approaches\neither focus on improving DNN robustness by negating the effects of\nperturbations or use a secondary model to detect adversarial data. Although\nequally important, the attack detection approach, which is studied in this\nwork, provides a more practical defense compared to the robustness approach. We\nshow that the existing detection methods are either ineffective against the\nstate-of-the-art attack techniques or computationally inefficient for real-time\nprocessing. We propose a novel universal and efficient method to detect\nadversarial examples by analyzing the varying degrees of impact of attacks on\ndifferent DNN layers. {Our method trains a lightweight regression model that\npredicts deeper-layer features from early-layer features, and uses the\nprediction error to detect adversarial samples.} Through theoretical arguments\nand extensive experiments, we demonstrate that our detection method is highly\neffective, computationally efficient for real-time processing, compatible with\nany DNN architecture, and applicable across different domains, such as image,\nvideo, and audio.", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.17442", "pdf_url": "http://arxiv.org/pdf/2506.20816v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20816v1", "AI": {"title_translation": "通过网络层非均匀影响实现对抗性数据的通用高效检测", "tldr": "本文提出了一种新颖的通用高效方法，通过分析对抗性攻击对不同深度神经网络层的影响差异，来检测对抗性样本。", "motivation": "深度神经网络容易受到对抗性攻击，现有防御方法要么侧重于提高鲁棒性，要么使用次级模型检测对抗性数据，但这些方法对于最先进的攻击效果不佳或计算效率低下。本文旨在提供一种更实用、高效的攻击检测方法。", "method": "该方法训练一个轻量级回归模型，该模型从早期层特征预测更深层特征，并利用预测误差来检测对抗性样本。", "result": "实验证明，所提出的检测方法高效、计算效率高，适用于实时处理，兼容任何DNN架构，并可应用于图像、视频和音频等不同领域。", "conclusion": "本文提出了一种通过分析对抗性攻击对不同DNN层非均匀影响来检测对抗性样本的通用高效方法，该方法在理论和实验上均被证明是有效且实用的。", "translation": "深度神经网络（DNNs）以其对有限噪声预算的对抗性输入设计的脆弱性而闻名。虽然已经提出了许多通过对原始输入进行细微修改的成功攻击，但针对这些攻击的防御技术相对研究不足。现有的防御方法要么通过抵消扰动的影响来提高DNN的鲁棒性，要么使用辅助模型来检测对抗性数据。尽管同样重要，但本研究中探讨的攻击检测方法与鲁棒性方法相比，提供了一种更实用的防御。我们发现，现有检测方法要么对最先进的攻击技术无效，要么在实时处理方面计算效率低下。我们提出了一种新颖的通用高效方法，通过分析攻击对不同DNN层影响程度的差异来检测对抗性样本。我们的方法训练一个轻量级回归模型，该模型从早期层特征预测更深层特征，并利用预测误差来检测对抗性样本。通过理论论证和大量实验，我们证明了我们的检测方法高效、计算效率高，适用于实时处理，兼容任何DNN架构，并可应用于图像、视频和音频等不同领域。", "summary": "本文针对深度神经网络易受对抗性攻击的问题，提出了一种通用且高效的对抗性样本检测方法。该方法通过分析对抗性攻击对神经网络不同层产生的非均匀影响，训练一个轻量级回归模型，利用早期层特征预测更深层特征，并通过预测误差来识别对抗性数据。研究表明，该方法在效率、计算成本、兼容性和跨领域适用性方面均表现出色，为实时检测对抗性攻击提供了实用的解决方案。", "keywords": "对抗性样本检测, 深度神经网络, 非均匀影响, 轻量级回归模型, 实时处理", "comments": "该论文提出了一种新颖的对抗性样本检测视角，即利用攻击对网络层影响的非均匀性。其创新点在于通过轻量级回归模型预测层间特征并利用预测误差进行检测，显著提升了检测的效率和通用性，使其适用于实时处理和多种DNN架构及数据类型，具有较高的实用价值。"}}
{"id": "2506.21297", "title": "Exploring Micro Frontends: A Case Study Application in E-Commerce", "authors": ["Ricardo Hideki Hangai Kojo", "Luiz Fernando Corte Real", "Renato Cordeiro Ferreira", "Thatiane de Oliveira Rosa", "Alfredo Goldman"], "summary": "In the micro frontends architectural style, the frontend is divided into\nsmaller components, which can range from a simple button to an entire page. The\ngoal is to improve scalability, resilience, and team independence, albeit at\nthe cost of increased complexity and infrastructure demands. This paper seeks\nto understand when it is worth adopting micro frontends, particularly in the\ncontext of industry. To achieve this, we conducted an investigation into the\nstate of the art of micro frontends, based on both academic and gray\nliterature. We then implemented this architectural style in a marketplace for\nhandcrafted products, which already used microservices. Finally, we evaluated\nthe implementation through a semi-open questionnaire with the developers. At\nthe studied marketplace company, the need for architectural change arose due to\nthe tight coupling between their main system (a Java monolith) and a dedicated\nfrontend system. Additionally, there were deprecated technologies and poor\ndeveloper experience. To address these issues, the micro frontends architecture\nwas adopted, along with the API Gateway and Backend for Frontend patterns, and\ntechnologies such as Svelte and Fastify. Although the adoption of Micro\nFrontends was successful, it was not strictly necessary to meet the company's\nneeds. According to the analysis of the mixed questionnaire responses, other\nalternatives, such as a monolithic frontend, could have achieved comparable\nresults. What made adopting micro frontends the most convenient choice in the\ncompany's context was the monolith strangulation and microservices adoption,\nwhich facilitated implementation through infrastructure reuse and knowledge\nsharing between teams.", "comment": "11 pages, 2 figures (2 diagrams), submitted to the workshop AMP 2025", "pdf_url": "http://arxiv.org/pdf/2506.21297v1", "categories": ["cs.SE", "cs.DC", "D.2.11; D.2.13; D.2.7"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21297v1", "AI": {"title_translation": "探索微前端：一个电子商务中的案例研究应用", "tldr": "本文通过文献调查和在一个手工艺品电商平台上的案例研究，评估了微前端架构的适用性，发现尽管微前端成功实施，但对于满足公司需求并非绝对必要，其便利性主要得益于与现有微服务和单体解耦策略的协同。", "motivation": "研究旨在理解何时值得采用微前端架构，特别是在工业背景下，以解决现有系统紧密耦合、技术过时和开发者体验不佳等问题。", "method": "研究方法包括对微前端领域最新进展的调查（基于学术和灰色文献），然后在一个已使用微服务的电商平台实施微前端架构，并通过对开发者进行半开放式问卷调查来评估实施效果。", "result": "微前端架构的采用是成功的，但并非严格必要，单体前端等其他替代方案也能达到类似效果。微前端成为最便捷的选择，是因为单体解耦和微服务采用促进了基础设施重用和团队间知识共享。", "conclusion": "尽管微前端成功实施，但对于满足公司需求而言并非绝对必要。其在公司背景下成为最便捷的选择，是由于单体解耦和微服务采用，促进了实施过程中的基础设施重用和团队间知识共享。", "translation": "在微前端架构风格中，前端被划分为更小的组件，范围从简单的按钮到整个页面。目标是提高可扩展性、弹性和团队独立性，尽管代价是增加复杂性和基础设施需求。本文旨在理解何时值得采用微前端，特别是在工业背景下。为此，我们对微前端的最新技术状况进行了调查，基于学术和灰色文献。然后，我们在一个已经使用微服务的手工艺品市场中实现了这种架构风格。最后，我们通过对开发者进行半开放式问卷调查来评估了实施效果。在所研究的电商公司中，由于其主系统（Java 单体）与专用前端系统之间的紧密耦合，以及存在废弃技术和糟糕的开发者体验，因此出现了架构变革的需求。为了解决这些问题，采用了微前端架构，以及 API 网关和后端即前端模式，以及 Svelte 和 Fastify 等技术。尽管微前端的采用是成功的，但对于满足公司需求而言并非严格必要。根据对混合问卷答复的分析，其他替代方案，如单体前端，也可以达到类似的结果。在公司背景下，使采用微前端成为最便捷选择的原因是单体解耦和微服务采用，这通过基础设施重用和团队间知识共享促进了实施。", "summary": "本研究探讨了在电子商务领域采用微前端架构的适用性。通过文献回顾和在一个手工艺品电商平台上的案例研究，评估了微前端的实施效果。研究发现，尽管微前端成功地解决了现有系统耦合、技术过时和开发者体验问题，但其并非满足公司需求的唯一或严格必要的选择。其便利性主要源于与现有微服务和单体解耦策略的协同作用，实现了基础设施复用和知识共享。", "keywords": "微前端, 电子商务, 架构, 案例研究, 微服务", "comments": "本文通过案例研究深入探讨了微前端在真实工业环境中的应用，其创新之处在于不仅验证了微前端的可行性，还通过量化评估指出了其并非总是最佳或唯一选择，这为业界提供了宝贵的决策参考。论文强调了与现有架构（如微服务）的协同作用是成功实施的关键，而非微前端本身。局限性可能在于案例研究的通用性，以及问卷调查的主观性。"}}
{"id": "2506.21057", "title": "Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions", "authors": ["Zhuochen Miao", "Jun Lv", "Hongjie Fang", "Yang Jin", "Cewu Lu"], "summary": "Imitation learning has emerged as a powerful paradigm in robot manipulation,\nyet its generalization capability remains constrained by object-specific\ndependencies in limited expert demonstrations. To address this challenge, we\npropose knowledge-driven imitation learning, a framework that leverages\nexternal structural semantic knowledge to abstract object representations\nwithin the same category. We introduce a novel semantic keypoint graph as a\nknowledge template and develop a coarse-to-fine template-matching algorithm\nthat optimizes both structural consistency and semantic similarity. Evaluated\non three real-world robotic manipulation tasks, our method achieves superior\nperformance, surpassing image-based diffusion policies with only one-quarter of\nthe expert demonstrations. Extensive experiments further demonstrate its\nrobustness across novel objects, backgrounds, and lighting conditions. This\nwork pioneers a knowledge-driven approach to data-efficient robotic learning in\nreal-world settings. Code and more materials are available on\nhttps://knowledge-driven.github.io/.", "comment": "IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21057v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21057v1", "AI": {"title_translation": "知识驱动的模仿学习：实现跨多样化条件的泛化", "tldr": "本文提出了一种知识驱动的模仿学习框架，通过利用外部结构语义知识来抽象对象表示，从而显著提高机器人操作中模仿学习的泛化能力和数据效率。", "motivation": "模仿学习在机器人操作中表现出色，但其泛化能力受限于有限专家演示中的特定对象依赖性。", "method": "本文提出了知识驱动的模仿学习框架，利用外部结构语义知识抽象同类别内的对象表示。引入了一种新颖的语义关键点图作为知识模板，并开发了一种粗到细的模板匹配算法，优化结构一致性和语义相似性。", "result": "在三个真实世界机器人操作任务中，该方法表现优越，仅用四分之一的专家演示就超越了基于图像的扩散策略。实验进一步证明了其在新型对象、背景和光照条件下的鲁棒性。", "conclusion": "这项工作开创了一种知识驱动的方法，用于在真实世界环境中进行数据高效的机器人学习。", "translation": "模仿学习已成为机器人操作中一种强大的范式，但其泛化能力仍受限于有限专家演示中特定于对象的依赖性。为了解决这一挑战，我们提出了知识驱动的模仿学习，一个利用外部结构语义知识来抽象同一类别内对象表示的框架。我们引入了一种新颖的语义关键点图作为知识模板，并开发了一种粗到细的模板匹配算法，该算法优化了结构一致性和语义相似性。在三个真实世界机器人操作任务上进行评估，我们的方法取得了卓越的性能，仅用四分之一的专家演示就超越了基于图像的扩散策略。广泛的实验进一步证明了其在新型对象、背景和光照条件下的鲁棒性。这项工作开创了一种知识驱动的方法，用于在真实世界环境中进行数据高效的机器人学习。", "summary": "本文提出了一种知识驱动的模仿学习框架，旨在解决传统模仿学习在机器人操作中泛化能力受限的问题。通过引入语义关键点图作为知识模板并开发粗到细的模板匹配算法，该方法能够利用外部结构语义知识抽象对象表示。实验结果表明，该方法在真实世界任务中表现优越，显著提高了数据效率和对新型环境的鲁棒性。", "keywords": "模仿学习, 知识驱动, 机器人操作, 泛化, 数据效率", "comments": "该论文的创新之处在于将外部结构语义知识融入模仿学习，有效解决了传统方法泛化能力差和数据效率低的问题。通过引入语义关键点图和模板匹配算法，实现了对对象表示的抽象，使其能够更好地适应多样化的条件。这项工作为机器人学习在真实世界环境中的应用开辟了新的途径。"}}
{"id": "2506.21458", "title": "Spatial Mental Modeling from Limited Views", "authors": ["Baiqiao Yin", "Qineng Wang", "Pingyue Zhang", "Jianshu Zhang", "Kangrui Wang", "Zihan Wang", "Jieyu Zhang", "Keshigeyan Chandrasegaran", "Han Liu", "Ranjay Krishna", "Saining Xie", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.", "comment": "Preprint version", "pdf_url": "http://arxiv.org/pdf/2506.21458v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21458v1", "AI": {"title_translation": "有限视角下的空间心理建模", "tldr": "本文引入了MindCube基准测试，揭示了视觉语言模型在构建空间心理模型方面的不足，并提出“先映射后推理”方法结合强化学习显著提升了模型理解不可见空间的能力。", "motivation": "视觉语言模型（VLMs）在仅通过有限视角来想象完整场景方面存在显著缺陷，无法像人类那样形成空间心理模型来推断布局、透视和运动。现有的VLMs在此类任务上表现接近随机水平，因此需要一个专门的基准来暴露并解决这一关键差距。", "method": "研究引入了MindCube基准测试（包含21,154个问题和3,268张图像），系统评估了VLMs在构建鲁棒空间心理模型方面的表现，包括表征位置（认知映射）、方向（透视采纳）和动态（心理模拟）。为弥补不足，研究探索了三种方法：未见过的中间视图、自然语言推理链和认知地图。最终采用了一种协同方法——“先映射后推理”（map-then-reason），该方法联合训练模型首先生成认知地图，然后在此基础上进行推理。此外，还引入了强化学习进一步提升性能。", "result": "MindCube基准测试表明现有VLMs在此任务上表现接近随机。通过“先映射后推理”方法，模型准确率从37.8%显著提升至60.8%（提高了23.0%）。加入强化学习后，性能进一步提升至70.7%（总共提高了32.9%）。", "conclusion": "核心发现是，通过主动构建和利用内部结构化空间表征并结合灵活的推理过程来构建空间心理模型（即支架式方法），可以显著提高模型对不可观测空间的理解能力。", "translation": "人类能否像视觉语言模型（VLMs）一样，仅凭少量视图就能想象出完整的场景？人类会形成空间心理模型，即对不可见空间的内部表征，以便推断布局、透视和运动。我们新的MindCube基准测试包含3,268张图像上的21,154个问题，揭示了这一关键差距，现有VLMs在此基准上表现接近随机。我们使用MindCube系统评估了VLMs在构建鲁棒空间心理模型方面的能力，包括表征位置（认知映射）、方向（透视采纳）和动态（“假设”运动的心理模拟）。然后，我们探索了三种方法来帮助VLMs近似空间心理模型，包括未见过的中间视图、自然语言推理链和认知地图。显著的改进来自于一种协同方法，“先映射后推理”，该方法联合训练模型首先生成认知地图，然后在此基础上进行推理。通过训练模型对这些内部地图进行推理，我们将准确率从37.8%提高到60.8%（+23.0%）。添加强化学习将性能进一步推高到70.7%（+32.9%）。我们的关键见解是，这种空间心理模型的支架式构建方式——主动构建和利用内部结构化空间表征与灵活的推理过程相结合，显著提高了对不可观测空间的理解。", "summary": "本文研究视觉语言模型（VLMs）在仅凭有限视角构建完整场景空间心理模型的能力。通过引入MindCube基准测试，作者发现现有VLMs在此任务上表现不佳。为解决此问题，论文提出并验证了多种方法，其中“先映射后推理”的协同方法，即先生成认知地图再进行推理，结合强化学习，显著提升了VLMs理解不可观测空间的能力，将准确率从37.8%提升至70.7%。研究强调了构建和利用内部结构化空间表征对空间理解的重要性。", "keywords": "空间心理模型, 视觉语言模型, MindCube, 认知映射, 强化学习", "comments": "这篇论文的创新点在于提出了一个全新的基准测试MindCube，有效揭示了现有VLMs在空间心理建模方面的不足。其提出的“先映射后推理”方法，特别是结合强化学习，为VLMs学习更高级的空间推理能力提供了有效途径。这对于推动VLM在复杂真实世界场景理解和交互方面具有重要意义。"}}
{"id": "2506.21500", "title": "Devising a solution to the problems of Cancer awareness in Telangana", "authors": ["Priyanka Avhad", "Vedanti Kshirsagar", "Urvi Ranjan", "Mahek Nakhua"], "summary": "According to the data, the percent of women who underwent screening for\ncervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3\npercent, 0.3 percent and 2.3 percent respectively. Although early detection is\nthe only way to reduce morbidity and mortality, people have very low awareness\nabout cervical and breast cancer signs and symptoms and screening practices. We\ndeveloped an ML classification model to predict if a person is susceptible to\nbreast or cervical cancer based on demographic factors. We devised a system to\nprovide suggestions for the nearest hospital or Cancer treatment centres based\non the users location or address. In addition to this, we can integrate the\nhealth card to maintain medical records of all individuals and conduct\nawareness drives and campaigns. For ML classification models, we used decision\ntree classification and support vector classification algorithms for cervical\ncancer susceptibility and breast cancer susceptibility respectively. Thus, by\ndevising this solution we come one step closer to our goal which is spreading\ncancer awareness, thereby, decreasing the cancer mortality and increasing\ncancer literacy among the people of Telangana.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21500v1", "categories": ["cs.LG", "cs.CY", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21500v1", "AI": {"title_translation": "在特伦甘纳邦设计癌症意识问题的解决方案", "tldr": "该研究开发了一个机器学习模型和系统，旨在提高特伦甘纳邦的癌症意识并降低死亡率。", "motivation": "特伦甘纳邦宫颈癌、乳腺癌和口腔癌筛查率极低（分别为3.3%、0.3%和2.3%），尽管早期发现是降低发病率和死亡率的唯一途径，但人们对癌症症状和筛查实践的意识非常低。", "method": "开发了一个机器学习分类模型，用于根据人口统计学因素预测个人对乳腺癌或宫颈癌的易感性。使用决策树分类和支持向量分类算法分别用于宫颈癌和乳腺癌易感性预测。此外，设计了一个系统，根据用户位置提供最近的医院或癌症治疗中心建议，并可集成健康卡以维护医疗记录和开展宣传活动。", "result": "通过开发的机器学习模型可以预测个人对乳腺癌或宫颈癌的易感性。设计的系统能够根据用户位置提供最近的医院或癌症治疗中心建议，并有助于维护医疗记录和开展宣传活动。", "conclusion": "通过设计这个解决方案，可以更接近于实现传播癌症意识、降低癌症死亡率和提高特伦甘纳邦人民癌症知识的目标。", "translation": "根据数据显示，2020年特伦甘纳邦接受宫颈癌、乳腺癌和口腔癌筛查的女性比例分别为3.3%、0.3%和2.3%。尽管早期发现是降低发病率和死亡率的唯一途径，但人们对宫颈癌和乳腺癌的体征、症状以及筛查实践的意识非常低。我们开发了一个机器学习分类模型，根据人口统计学因素预测一个人是否易患乳腺癌或宫颈癌。我们设计了一个系统，根据用户的位置或地址提供最近的医院或癌症治疗中心的建议。除此之外，我们还可以整合健康卡以维护所有个人的医疗记录并开展宣传活动。对于机器学习分类模型，我们分别使用了决策树分类和支持向量分类算法来预测宫颈癌易感性和乳腺癌易感性。因此，通过设计这个解决方案，我们离我们的目标更近了一步，即传播癌症意识，从而降低癌症死亡率并提高特伦甘纳邦人民的癌症知识水平。", "summary": "本研究旨在解决印度特伦甘纳邦癌症意识低下导致筛查率极低的问题。作者开发了一个机器学习分类模型，利用人口统计学因素预测个人对乳腺癌或宫颈癌的易感性，其中宫颈癌采用决策树分类，乳腺癌采用支持向量分类。此外，还设计了一个系统，根据用户位置提供最近的癌症治疗中心建议，并计划集成健康卡以管理医疗记录和组织癌症宣传活动。该解决方案旨在提高癌症意识，从而降低死亡率并提高当地居民的癌症知识水平。", "keywords": "癌症意识, 机器学习, 特伦甘纳邦, 早期检测, 公共卫生", "comments": "该论文提出了一种结合机器学习预测和地理位置服务的实用解决方案，以应对印度特定地区癌症意识不足的挑战。其创新性在于将技术应用于公共卫生领域，旨在通过提高意识和提供便捷的医疗资源信息来改善癌症早期筛查和管理。"}}
{"id": "2506.21449", "title": "exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design", "authors": ["Maxim Moraru", "Weiyi Xia", "Zhuo Ye", "Feng Zhang", "Yongxin Yao", "Ying Wai Li", "Cai-Zhuang Wang"], "summary": "exa-AMD is a Python-based application designed to accelerate the discovery\nand design of functional materials by integrating AI/ML tools, materials\ndatabases, and quantum mechanical calculations into scalable, high-performance\nworkflows. The execution model of exa-AMD relies on Parsl, a task-parallel\nprogramming library that enables a flexible execution of tasks on any computing\nresource from laptops to supercomputers. By using Parsl, exa-AMD is able to\ndecouple the workflow logic from execution configuration, thereby empowering\nresearchers to scale their workflows without having to reimplement them for\neach system.", "comment": "We intend to publish the paper to the Journal of Open Source Software", "pdf_url": "http://arxiv.org/pdf/2506.21449v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21449v1", "AI": {"title_translation": "exa-AMD：一个加速AI辅助材料发现与设计的可扩展工作流", "tldr": "exa-AMD是一个基于Python的应用，利用Parsl库集成AI/ML、材料数据库和量子力学计算，创建可扩展的工作流，加速材料发现与设计，并实现工作流逻辑与执行配置的分离，方便研究人员在不同计算资源上扩展。", "motivation": "旨在加速功能材料的发现与设计。", "method": "exa-AMD是一个基于Python的应用，通过集成AI/ML工具、材料数据库和量子力学计算来构建可扩展、高性能的工作流。其执行模型依赖于Parsl（一个任务并行编程库），Parsl使得工作流逻辑与执行配置解耦，从而实现了在从笔记本电脑到超级计算机的任何计算资源上的灵活执行。", "result": "exa-AMD能够将工作流逻辑与执行配置解耦，使得研究人员无需为每个系统重新实现工作流即可对其进行扩展。", "conclusion": "exa-AMD使研究人员能够轻松地在各种计算资源上扩展其材料发现与设计工作流。", "translation": "exa-AMD是一个基于Python的应用程序，旨在通过将AI/ML工具、材料数据库和量子力学计算集成到可扩展、高性能的工作流中，从而加速功能材料的发现和设计。exa-AMD的执行模型依赖于Parsl，这是一个任务并行编程库，它可以在从笔记本电脑到超级计算机的任何计算资源上实现任务的灵活执行。通过使用Parsl，exa-AMD能够将工作流逻辑与执行配置解耦，从而使研究人员能够在不为每个系统重新实现工作流的情况下扩展其工作流。", "summary": "exa-AMD是一个利用Python和Parsl库开发的应用程序，它集成了AI/ML、材料数据库和量子力学计算，以创建一个可扩展、高性能的工作流。该工作流旨在加速功能材料的发现与设计。通过Parsl，exa-AMD实现了工作流逻辑与执行配置的解耦，使得研究人员能够在各种计算资源上灵活且便捷地扩展其材料发现工作流，而无需进行重复的实现工作。", "keywords": "材料发现, AI/ML, 可扩展工作流, Parsl, 量子力学计算", "comments": "该论文介绍的exa-AMD在AI辅助材料发现领域具有重要意义，其创新点在于利用Parsl库实现了工作流的解耦和高度可扩展性。这意味着研究人员可以更高效地利用不同计算资源进行材料研究，大大降低了在多平台部署和扩展工作流的复杂性。"}}
{"id": "2506.20877", "title": "THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion", "authors": ["Calin Teodor Ioan"], "summary": "Monocular depth estimation methods traditionally train deep models to infer\ndepth directly from RGB pixels. This implicit learning often overlooks explicit\nmonocular cues that the human visual system relies on, such as occlusion\nboundaries, shading, and perspective. Rather than expecting a network to\ndiscover these cues unaided, we present ThirdEye, a cue-aware pipeline that\ndeliberately supplies each cue through specialised, pre-trained, and frozen\nnetworks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)\nequipped with a key-value working-memory module that weights them by\nreliability. An adaptive-bins transformer head then produces a high-resolution\ndisparity map. Because the cue experts are frozen, ThirdEye inherits large\namounts of external supervision while requiring only modest fine-tuning. This\nextended version provides additional architectural detail, neuroscientific\nmotivation, and an expanded experimental protocol; quantitative results will\nappear in a future revision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20877v1", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20877v1", "AI": {"title_translation": "THIRDEYE：脑启发式多阶段融合的线索感知单目深度估计", "tldr": "ThirdEye 提出了一种脑启发式的多阶段融合框架，通过显式集成单目线索来改进单目深度估计，解决了传统方法忽视人类视觉系统依赖的显式线索的问题。", "motivation": "传统的单目深度估计方法通过深度模型从RGB像素中隐式推断深度，但这常常忽略了人类视觉系统所依赖的显式单目线索，如遮挡边界、阴影和透视。本文旨在通过明确提供这些线索来解决这个问题。", "method": "本文提出了ThirdEye，一个线索感知的管道，通过专门的、预训练和冻结的网络刻意提供每种线索。这些线索在一个三阶段的皮层层级（V1->V2->V3）中融合，该层级配备了键值工作记忆模块，根据可靠性对线索进行加权。然后，一个自适应箱变压器头部生成高分辨率的视差图。由于线索专家网络是冻结的，ThirdEye继承了大量的外部监督，同时只需要适度的微调。", "result": "定量结果将在未来的修订中出现。", "conclusion": "ThirdEye提出了一种通过脑启发式多阶段融合来显式整合单目线索的单目深度估计方法，该方法通过利用预训练和冻结的专家网络，能够继承大量外部监督并仅需适度微调。", "translation": "单目深度估计方法传统上训练深度模型直接从RGB像素推断深度。这种隐式学习常常忽视人类视觉系统所依赖的显式单目线索，例如遮挡边界、阴影和透视。我们没有期望网络在没有帮助的情况下发现这些线索，而是提出了ThirdEye，一个线索感知的管道，通过专门的、预训练和冻结的网络刻意提供每种线索。这些线索在一个三阶段的皮层层级（V1->V2->V3）中融合，该层级配备了键值工作记忆模块，根据可靠性对线索进行加权。然后，一个自适应箱变压器头部生成高分辨率的视差图。由于线索专家网络是冻结的，ThirdEye继承了大量的外部监督，同时只需要适度的微调。这个扩展版本提供了额外的架构细节、神经科学动机和扩展的实验协议；定量结果将在未来的修订中出现。", "summary": "ThirdEye提出了一种新颖的单目深度估计方法，该方法通过明确整合人类视觉系统所依赖的显式单目线索（如遮挡边界、阴影和透视）来克服传统方法的局限性。该方法利用预训练并冻结的专家网络提供这些线索，并通过一个脑启发式的三阶段皮层层级结构进行融合，其中包含一个基于可靠性加权的键值工作记忆模块。最终，一个自适应箱变压器头部生成高分辨率视差图。ThirdEye的创新之处在于其能够继承大量外部监督，同时仅需少量微调。", "keywords": "单目深度估计, 线索感知, 脑启发, 多阶段融合, THIRDEYE", "comments": "该论文的创新点在于其明确地将人类视觉系统所依赖的单目线索集成到深度估计流程中，并采用了脑启发式的多阶段融合架构。这种方法有望通过超越传统的隐式学习来提高深度估计的准确性。然而，本版本尚未提供定量结果，这是其当前的一个局限性。"}}
{"id": "2506.20771", "title": "Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models", "authors": ["Xinghao Dong", "Huchen Yang", "Jin-Long Wu"], "summary": "We propose a latent score-based generative AI framework for learning\nstochastic, non-local closure models and constitutive laws in nonlinear\ndynamical systems of computational mechanics. This work addresses a key\nchallenge of modeling complex multiscale dynamical systems without a clear\nscale separation, for which numerically resolving all scales is prohibitively\nexpensive, e.g., for engineering turbulent flows. While classical closure\nmodeling methods leverage domain knowledge to approximate subgrid-scale\nphenomena, their deterministic and local assumptions can be too restrictive in\nregimes lacking a clear scale separation. Recent developments of\ndiffusion-based stochastic models have shown promise in the context of closure\nmodeling, but their prohibitive computational inference cost limits practical\napplications for many real-world applications. This work addresses this\nlimitation by jointly training convolutional autoencoders with conditional\ndiffusion models in the latent spaces, significantly reducing the\ndimensionality of the sampling process while preserving essential physical\ncharacteristics. Numerical results demonstrate that the joint training approach\nhelps discover a proper latent space that not only guarantees small\nreconstruction errors but also ensures good performance of the diffusion model\nin the latent space. When integrated into numerical simulations, the proposed\nstochastic modeling framework via latent conditional diffusion models achieves\nsignificant computational acceleration while maintaining comparable predictive\naccuracy to standard diffusion models in physical spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20771v1", "categories": ["cs.LG", "math.DS", "physics.comp-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20771v1", "AI": {"title_translation": "随机和非局部闭合建模，用于通过潜在分数生成模型的非线性动力系统", "tldr": "该研究提出了一种基于潜在分数的生成式人工智能框架，用于非线性动力系统中的随机、非局部闭合建模，通过在潜在空间中联合训练扩散模型，显著降低了计算成本并保持了精度。", "motivation": "现有方法在处理没有明确尺度分离的复杂多尺度动力系统时，面临计算成本过高（如湍流）或假设过于严格的挑战。特别是，基于扩散的随机模型虽然有前景，但其计算推理成本过高限制了实际应用。", "method": "提出了一种潜在分数生成式AI框架。通过在潜在空间中联合训练卷积自编码器和条件扩散模型，显著降低了采样过程的维度，同时保留了重要的物理特性。", "result": "联合训练方法有助于发现一个合适的潜在空间，该空间不仅保证了小的重建误差，而且确保了扩散模型在潜在空间中的良好性能。当集成到数值模拟中时，所提出的随机建模框架实现了显著的计算加速，同时保持了与物理空间中标准扩散模型相当的预测精度。", "conclusion": "所提出的通过潜在条件扩散模型的随机建模框架，为非线性动力系统中的闭合建模提供了一种计算效率高且预测准确的解决方案，克服了现有方法的局限性，使其适用于实际应用。", "translation": "我们提出了一种基于潜在分数的生成式人工智能框架，用于计算力学非线性动力系统中随机、非局部闭合模型和本构律的学习。这项工作解决了对没有明确尺度分离的复杂多尺度动力系统进行建模的关键挑战，因为数值求解所有尺度会非常昂贵，例如工程湍流。虽然经典的闭合建模方法利用领域知识来近似亚网格尺度现象，但其确定性和局部假设在缺乏明确尺度分离的区域可能过于严格。最近基于扩散的随机模型在闭合建模方面已显示出前景，但其过高的计算推理成本限制了在许多实际应用中的实际应用。这项工作通过在潜在空间中联合训练卷积自编码器和条件扩散模型来解决这一限制，显著降低了采样过程的维度，同时保留了重要的物理特性。数值结果表明，联合训练方法有助于发现一个合适的潜在空间，该空间不仅保证了小的重建误差，而且确保了扩散模型在潜在空间中的良好性能。当集成到数值模拟中时，所提出的通过潜在条件扩散模型的随机建模框架实现了显著的计算加速，同时保持了与物理空间中标准扩散模型相当的预测精度。", "summary": "本论文提出了一种基于潜在分数的生成式人工智能框架，用于复杂非线性动力系统中的随机、非局部闭合建模。该框架通过在潜在空间中联合训练自编码器和条件扩散模型，解决了多尺度建模和现有扩散模型计算成本高昂的问题。这种方法显著降低了采样维度，实现了显著的计算加速，并保持了与标准方法相当的精度，使其适用于湍流等实际应用。", "keywords": "随机闭合建模, 潜在分数模型, 非线性动力系统, 扩散模型, 计算力学", "comments": "这项工作的创新之处在于通过利用潜在空间来解决扩散模型在闭合建模中的计算瓶颈，从而使其适用于湍流等复杂系统。联合训练方法对于发现最优潜在空间至关重要，为多尺度动力系统建模提供了一种更高效和准确的方法。"}}
{"id": "2506.21436", "title": "Succinct Preferential Attachment Graphs", "authors": ["Ziad Ismaili Alaoui", "Namrata", "Sebastian Wild"], "summary": "Computing over compressed data combines the space saving of data compression\nwith efficient support for queries directly on the compressed representation.\nSuch data structures are widely applied in text indexing and have been\nsuccessfully generalised to trees. For graphs, support for computing over\ncompressed data remains patchy; typical results in the area of succinct data\nstructures are restricted to a specific class of graphs and use the same,\nworst-case amount of space for any graph from this class.\n  In this work, we design a data structure whose space usage automatically\nimproves with the compressibility of the graph at hand, while efficiently\nsupporting navigational operations (simulating adjacency-list access).\nSpecifically, we show that the space usage approaches the instance-optimal\nspace when the graph is drawn according to the classic Barab\\'asi-Albert model\nof preferential-attachment graphs. Our data-structure techniques also work for\narbitrary graphs, guaranteeing a size asymptotically no larger than an\nentropy-compressed edge list. A key technical contribution is the careful\nanalysis of the instance-optimal space usage.", "comment": "WG 2025", "pdf_url": "http://arxiv.org/pdf/2506.21436v1", "categories": ["cs.DS", "cs.IT", "math.IT", "math.PR"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21436v1", "AI": {"title_translation": "简洁的优先连接图", "tldr": "该研究设计了一种新的数据结构，用于压缩图数据，使其空间使用量随图的可压缩性自动优化，并支持高效导航，尤其适用于优先连接图模型。", "motivation": "现有图的压缩数据结构支持不完善，通常限制于特定图类且空间使用量为最坏情况，无法根据图的可压缩性自动优化空间效率。", "method": "设计了一种新的数据结构，其空间使用量能随图的可压缩性自动改进，并能高效支持导航操作。通过分析Barabási-Albert优先连接图模型，证明其空间使用量接近实例最优。该技术也适用于任意图，并保证渐近地不大于熵压缩边列表的大小。", "result": "所设计的数据结构的空间使用量随图的可压缩性自动优化，对于Barabási-Albert优先连接图模型，空间使用量接近实例最优。该数据结构技术也适用于任意图，并能保证其大小渐近地不大于熵压缩边列表。", "conclusion": "该研究成功设计了一种空间自适应的图数据结构，解决了现有图压缩数据结构在空间效率和通用性方面的局限性，为高效处理压缩图数据提供了新方法。", "translation": "对压缩数据进行计算将数据压缩的空间节省与直接在压缩表示上高效支持查询结合起来。此类数据结构广泛应用于文本索引，并已成功推广到树。对于图，对压缩数据进行计算的支持仍然零散；简洁数据结构领域的典型结果仅限于特定类别的图，并且对于此类中的任何图都使用相同的最坏情况空间量。\n在这项工作中，我们设计了一种数据结构，其空间使用量随手头图的可压缩性自动改进，同时高效支持导航操作（模拟邻接列表访问）。具体来说，我们展示了当图根据经典的Barabási-Albert优先连接图模型绘制时，其空间使用量接近实例最优空间。我们的数据结构技术也适用于任意图，保证其大小渐近地不大于熵压缩边列表。一个关键的技术贡献是对实例最优空间使用量的仔细分析。", "summary": "本文提出了一种新的图数据结构，旨在解决现有图压缩数据结构在空间效率和通用性方面的局限性。该结构能够根据图的可压缩性自动优化空间使用，同时高效支持导航操作。研究表明，在经典的Barabási-Albert优先连接图模型下，其空间使用量接近实例最优。此外，该技术也适用于任意图，并能保证空间复杂度渐近地不大于熵压缩边列表。一个关键贡献是对实例最优空间使用量的深入分析。", "keywords": "简洁数据结构, 优先连接图, 图压缩, 空间效率, Barabási-Albert模型", "comments": "这项工作在图数据结构领域具有重要意义，它通过引入空间自适应性，解决了传统简洁数据结构在图压缩方面存在的局限性，即对特定图类和最坏情况空间使用的依赖。特别地，它将压缩计算的优势扩展到更广泛的图类型，并通过实例最优空间分析展现了其理论上的优越性。"}}
{"id": "2506.20799", "title": "Structural System Identification via Validation and Adaptation", "authors": ["Cristian López", "Keegan J. Moore"], "summary": "Estimating the governing equation parameter values is essential for\nintegrating experimental data with scientific theory to understand, validate,\nand predict the dynamics of complex systems. In this work, we propose a new\nmethod for structural system identification (SI), uncertainty quantification,\nand validation directly from data. Inspired by generative modeling frameworks,\na neural network maps random noise to physically meaningful parameters. These\nparameters are then used in the known equation of motion to obtain fake\naccelerations, which are compared to real training data via a mean square error\nloss. To simultaneously validate the learned parameters, we use independent\nvalidation datasets. The generated accelerations from these datasets are\nevaluated by a discriminator network, which determines whether the output is\nreal or fake, and guides the parameter-generator network. Analytical and real\nexperiments show the parameter estimation accuracy and model validation for\ndifferent nonlinear structural systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20799v1", "categories": ["math.DS", "cs.LG", "cs.SY", "eess.SY"], "cate": "math.DS", "url": "http://arxiv.org/abs/2506.20799v1", "AI": {"title_translation": "结构系统识别通过验证和适应", "tldr": "提出了一种基于生成模型和判别器网络的结构系统识别新方法，可直接从数据中估计参数并进行模型验证。", "motivation": "估计控制方程参数对于整合实验数据与科学理论以理解、验证和预测复杂系统的动态至关重要。", "method": "该方法受生成建模框架启发，使用一个神经网络将随机噪声映射到具有物理意义的参数。这些参数被用于已知的运动方程以生成“假”加速度，并通过均方误差损失与真实的训练数据进行比较。为了同时验证学习到的参数，引入了独立的验证数据集，由一个判别器网络评估生成的加速度是否真实或虚假，并指导参数生成器网络。", "result": "分析实验和真实实验结果表明了该方法在不同非线性结构系统上的参数估计准确性和模型验证能力。", "conclusion": "该方法能够准确估计结构系统参数并有效进行模型验证，为复杂系统的动力学理解和预测提供了新途径。", "translation": "估算控制方程参数值对于将实验数据与科学理论相结合以理解、验证和预测复杂系统的动力学至关重要。在这项工作中，我们提出了一种直接从数据进行结构系统识别（SI）、不确定性量化和验证的新方法。受生成建模框架的启发，一个神经网络将随机噪声映射到具有物理意义的参数。然后，这些参数用于已知的运动方程中以获得伪加速度，并通过均方误差损失与真实的训练数据进行比较。为了同时验证学习到的参数，我们使用独立的验证数据集。来自这些数据集生成的加速度由一个判别器网络评估，该网络判断输出是真实的还是伪造的，并指导参数生成器网络。分析和真实实验表明了不同非线性结构系统的参数估计准确性和模型验证能力。", "summary": "本文提出了一种新颖的结构系统识别方法，该方法融合了生成建模的思想，利用神经网络将噪声转化为物理参数，并通过运动方程生成预测值。通过结合均方误差损失和独立的判别器网络对验证数据集的评估，该方法不仅能估计系统参数，还能同时进行模型验证和不确定性量化。实验结果证实了其在非线性结构系统参数估计和模型验证方面的有效性。", "keywords": "结构系统识别, 生成建模, 神经网络, 参数估计, 模型验证", "comments": "该论文创新性地将生成对抗网络（GAN）的思想应用于结构系统识别领域，通过引入判别器网络实现了参数学习与模型验证的同步进行，提升了系统识别的可靠性。"}}
{"id": "2506.21123", "title": "Characterization of Rydberg-Atom Signal Reception of Dual-Frequency Signals Coupled with Two Energy Levels", "authors": ["Hao Wu", "Chongwu Xie", "Xinyuan Yao", "Kang-Da Wu", "Shanchi Wu", "Rui Ni", "Guo-Yong Xiang", "Chen Gong"], "summary": "Rydberg atomic sensors have been adopted for novel radio frequency (RF)\nmeasurement technique and the sensing capability for signals in multiple\nfrequencies makes it attractive for multi-user communication. However, unlike\ntraditional antennas where the signals in multiple frequencies are orthogonal,\nthe received signals of atomic sensors corresponding to different energy levels\nwill be downconverted to the baseband simultaneously, resulting in multi-user\ninterference. Thus, in this paper, we analyze the mutual interference\ncharacteristics of two RF signals with different carrier frequencies coupling\ndifferent energy levels. We introduce the joint response coefficient based on\nthe receiver characteristics and analyze the interference of one user to\nanother. We analyze the bit-error rate (BER) and symbol-error rate (SER) for\ntwo signals coupling two different energy levels. We also conduct experiments\nto validate the BER and SER results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21123v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21123v1", "AI": {"title_translation": "里德堡原子双频信号与双能级耦合接收特性研究", "tldr": "本文研究了里德堡原子传感器接收双频信号时，由于不同能级信号同时下变频到基带导致的多用户干扰问题，并分析了其相互干扰特性、误码率和符号误码率，并通过实验进行了验证。", "motivation": "里德堡原子传感器在射频测量和多频信号传感方面具有吸引力，但其接收多频信号时，不同频率信号对应不同能级，会同时下变频到基带，导致多用户干扰，这与传统天线多频信号正交不同。因此，需要分析这种干扰特性。", "method": "本文分析了不同载波频率的两个射频信号耦合不同能级时的相互干扰特性。引入了基于接收机特性的联合响应系数，分析了一个用户对另一个用户的干扰。分析了耦合两个不同能级的两个信号的误码率（BER）和符号误码率（SER）。并进行了实验验证BER和SER结果。", "result": "通过分析和实验，验证了里德堡原子传感器在双频信号接收中，不同能级耦合导致的相互干扰对误码率（BER）和符号误码率（SER）的影响。", "conclusion": "本文研究了里德堡原子传感器接收双频信号时，由于不同能级信号同时下变频到基带导致的多用户干扰问题，并通过理论分析和实验验证了其相互干扰特性以及对误码率和符号误码率的影响。", "translation": "里德堡原子传感器已被用于新型射频（RF）测量技术，其对多频信号的传感能力使其在多用户通信中具有吸引力。然而，与传统天线中多频信号正交不同，原子传感器接收到的对应不同能级的信号会同时下变频到基带，导致多用户干扰。因此，本文分析了两个不同载波频率的射频信号耦合不同能级时的相互干扰特性。我们引入了基于接收机特性的联合响应系数，并分析了一个用户对另一个用户的干扰。我们分析了两个耦合不同能级的信号的误码率（BER）和符号误码率（SER）。我们还进行了实验来验证BER和SER结果。", "summary": "本文研究了里德堡原子传感器在接收双频信号时面临的多用户干扰问题。与传统天线不同，里德堡原子传感器中不同频率的信号会同时下变频到基带，导致干扰。文章分析了两个不同载波频率信号耦合不同能级时的相互干扰特性，引入了联合响应系数，并详细分析了误码率（BER）和符号误码率（SER），最后通过实验验证了理论分析结果。", "keywords": "里德堡原子传感器, 双频信号, 多用户干扰, 误码率, 符号误码率", "comments": "这篇论文解决了里德堡原子传感器在多用户通信中面临的一个关键挑战——多用户干扰，这是由于其独特的信号下变频机制引起的。通过理论分析和实验验证，该研究为理解和减轻这种干扰提供了重要见解，对于推动里德堡原子传感器在实际通信系统中的应用具有重要意义。"}}
{"id": "2506.21535", "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation", "authors": ["Mohammed Baharoon", "Jun Ma", "Congyu Fang", "Augustin Toma", "Bo Wang"], "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21535v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21535v1", "AI": {"title_translation": "探索用于CT报告生成的3D多模态大语言模型的设计空间", "tldr": "该论文系统性地探索了用于CT报告生成的3D多模态大语言模型（MLLMs）的设计空间，并引入了两种基于知识的报告增强方法，在MICCAI 2024 AMOS-MM挑战赛中获得第二名。研究发现，LLM大小对放射学报告生成（RRG）影响有限，而分割掩码的使用能提高性能。", "motivation": "自动化放射学报告生成（RRG），特别是利用多模态大语言模型（MLLMs）进行3D CT报告生成。", "method": "系统地研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术。引入了两种基于知识的报告增强方法。在AMOS-MM数据集的1,687个病例上进行了实验。", "result": "1. 基于知识的报告增强方法将GREEN分数性能提高了高达10%。2. 在MICCAI 2024 AMOS-MM挑战赛中获得第二名。3. 在相同训练协议下，RRG在很大程度上与LLM的大小无关。4. 如果原始ViT是在较小体积上进行预训练的，则较大的体积大小并不总能提高性能。5. 使用分割掩码和CT体积一起可以提高性能。", "conclusion": "该研究探索了3D MLLMs在CT报告生成中的各种设计选择，强调了基于知识的增强和分割掩码的有效性，并指出LLM大小和预训练条件下的较大体积大小可能不是关键因素。", "translation": "多模态大语言模型（MLLMs）已成为自动化放射学报告生成（RRG）的一种有前景的方法。在这项工作中，我们系统地研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和用于3D CT报告生成的微调技术。我们还引入了两种基于知识的报告增强方法，将GREEN分数性能提高了高达10%，在MICCAI 2024 AMOS-MM挑战赛中获得第二名。我们对AMOS-MM数据集1,687个病例的结果表明，在相同的训练协议下，RRG在很大程度上与LLM的大小无关。我们还表明，如果原始ViT是在较小体积上进行预训练的，那么较大的体积大小并不总能提高性能。最后，我们表明使用分割掩码和CT体积一起可以提高性能。代码已在https://github.com/bowang-lab/AMOS-MM-Solution 公开。", "summary": "本文系统性地探索了用于3D CT放射学报告生成的3D多模态大语言模型（MLLMs）的设计空间，研究了包括视觉输入表示、投影器、LLMs和微调技术在内的多个组件。作者提出了两种基于知识的报告增强方法，使GREEN分数提高了10%，并在MICCAI 2024 AMOS-MM挑战赛中获得第二名。主要发现包括LLM大小对性能影响有限，当ViT在较小体积上预训练时，较大体积不总能提高性能，以及使用分割掩码与CT体积结合能显著提高性能。", "keywords": "3D MLLMs, CT报告生成, 放射学报告生成, 设计空间, 知识增强", "comments": "该论文对3D MLLMs在CT报告生成这一实际应用中的设计选择进行了系统性探索。所引入的基于知识的增强方法以及关于LLM大小和体积大小/预训练的经验性发现，为医学图像分析和多模态AI的未来研究和发展提供了宝贵的见解。代码的公开也具有重要贡献。"}}
{"id": "2506.20923", "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "pdf_url": "http://arxiv.org/pdf/2506.20923v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20923v1", "AI": {"title_translation": "KaLM-Embedding-V2：卓越的训练技术和数据启发了多功能嵌入模型", "tldr": "KaLM-Embedding-V2是一个紧凑的多功能文本嵌入模型，通过先进的训练技术和大规模高质量数据，在MTEB中表现优异，超越同等规模模型并媲美大型模型。", "motivation": "旨在通过利用卓越的训练技术和数据，开发一个在通用文本嵌入任务中表现出色的多功能紧凑型嵌入模型。", "method": "该模型移除了因果注意力掩码，采用全双向Transformer架构并结合简单有效的均值池化生成固定长度嵌入。训练采用多阶段流程：(i) 在大规模弱监督开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；(iii) 使用模型汤参数平均以实现鲁棒泛化。此外，引入了焦点式重加权机制以集中学习困难样本，并采用在线难负样本混合策略持续丰富难负样本。预训练数据超过20类，微调数据超过100类。", "result": "在海量文本嵌入基准（MTEB）中文和英文上的广泛评估表明，KaLM-Embedding-V2显著优于同等规模的其他模型。它能够与3倍、14倍、18倍和26倍大的嵌入模型竞争，为参数少于1B的多功能紧凑型嵌入模型树立了新标准。", "conclusion": "KaLM-Embedding-V2通过创新的架构、多阶段训练、难样本处理机制以及大规模多样化数据，实现了卓越的性能，证明了在紧凑模型尺寸下达到先进性能的可能性。", "translation": "在本文中，我们提出了KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，它通过利用卓越的训练技术和数据，在通用文本嵌入任务中取得了令人印象深刻的性能。我们的主要创新包括：(1) 为了更好地使架构与表示学习对齐，我们移除了因果注意力掩码，并采用了一个带有简单而有效的均值池化的全双向Transformer来生成固定长度的嵌入；(2) 我们采用了多阶段训练流程：(i) 在大规模弱监督开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；以及(iii) 模型汤参数平均以实现鲁棒泛化。此外，我们引入了一种焦点式重加权机制，将学习集中在困难样本上，以及一种在线难负样本混合策略，无需昂贵的离线挖掘即可持续丰富难负样本；(3) 我们收集了超过20类数据用于预训练，100类数据用于微调，以提升嵌入模型的性能和泛化能力。在海量文本嵌入基准（MTEB）中文和英文上的广泛评估表明，我们的模型显著优于同等规模的其他模型，并能与3倍、14倍、18倍和26倍大的嵌入模型竞争，为参数少于1B的多功能紧凑型嵌入模型树立了新标准。", "summary": "KaLM-Embedding-V2是一种新型紧凑型多功能文本嵌入模型，通过改进Transformer架构、采用多阶段训练（包括大规模预训练和高质量微调，并结合模型汤）、引入焦点式重加权和在线难负样本混合策略，以及利用多样化的大规模数据集，在MTEB基准测试中展现出卓越性能，超越同等规模模型并能与数倍大的模型竞争，为紧凑型嵌入模型设定了新标准。", "keywords": "文本嵌入, KaLM-Embedding-V2, 多阶段训练, 难负样本, MTEB", "comments": "该论文的创新点在于结合了架构改进（全双向Transformer与均值池化）、精细的多阶段训练策略（大规模预训练、高质量微调、模型汤）、以及有效的难样本处理机制（焦点式重加权、在线难负样本混合）。其重要性体现在证明了在保持模型紧凑（小于1B参数）的同时，通过优化训练技术和数据，可以实现与大型模型相当甚至超越同等规模模型的性能，这对于资源受限的应用场景具有重要意义。"}}
{"id": "2506.21206", "title": "Robust and efficient pre-processing techniques for particle-based methods including dynamic boundary generation", "authors": ["Niklas S. Neher", "Erik Faulhaber", "Sven Berger", "Christian Weißenfels", "Gregor J. Gassner", "Michael Schlottke-Lakemper"], "summary": "Obtaining high-quality particle distributions for stable and accurate\nparticle-based simulations poses significant challenges, especially for complex\ngeometries. We introduce a preprocessing technique for 2D and 3D geometries,\noptimized for smoothed particle hydrodynamics (SPH) and other particle-based\nmethods. Our pipeline begins with the generation of a resolution-adaptive point\ncloud near the geometry's surface employing a face-based neighborhood search.\nThis point cloud forms the basis for a signed distance field, enabling\nefficient, localized computations near surface regions. To create an initial\nparticle configuration, we apply a hierarchical winding number method for fast\nand accurate inside-outside segmentation. Particle positions are then relaxed\nusing an SPH-inspired scheme, which also serves to pack boundary particles.\nThis ensures full kernel support and promotes isotropic distributions while\npreserving the geometry interface. By leveraging the meshless nature of\nparticle-based methods, our approach does not require connectivity information\nand is thus straightforward to integrate into existing particle-based\nframeworks. It is robust to imperfect input geometries and memory-efficient\nwithout compromising performance. Moreover, our experiments demonstrate that\nwith increasingly higher resolution, the resulting particle distribution\nconverges to the exact geometry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21206v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21206v1", "AI": {"title_translation": "粒子方法中鲁棒高效的预处理技术，包括动态边界生成", "tldr": "本文介绍了一种针对2D和3D复杂几何体的鲁棒且高效的粒子预处理技术，旨在为SPH及其他粒子方法生成高质量的粒子分布。该方法能处理不完美的输入几何体，并确保粒子分布随分辨率提高而趋近精确几何体，从而支持稳定准确的粒子模拟。", "motivation": "为基于粒子的模拟（特别是复杂几何体）获取高质量的粒子分布以确保稳定性和准确性，带来了显著的挑战。", "method": "本文提出了一种预处理技术，其流程包括：首先，通过基于面的邻域搜索生成几何体表面附近的分辨率自适应点云；其次，该点云用于构建有符号距离场，以便在表面区域进行高效局部计算；接着，应用分层缠绕数方法进行快速准确的内外分割，以创建初始粒子配置；最后，使用受SPH启发的方案松弛粒子位置并填充边界粒子，确保完全核支持并促进各向同性分布，同时保留几何界面。该方法利用粒子方法的无网格特性，不需要连接信息，对不完善的输入几何体具有鲁棒性，并且内存效率高。", "result": "该技术能够生成确保完全核支持和各向同性分布的粒子配置，同时保留几何界面。它对不完善的输入几何体具有鲁棒性，并且内存效率高。实验表明，随着分辨率的提高，所得粒子分布会收敛到精确的几何体。", "conclusion": "本文提出了一种鲁棒且高效的预处理技术，能够为基于粒子的模拟生成高质量的粒子分布，克服了复杂几何体和不完善输入带来的挑战，从而实现了稳定和准确的模拟。", "translation": "为基于粒子的模拟（尤其是复杂几何体）获取高质量的粒子分布以确保稳定性和准确性，带来了显著的挑战。我们引入了一种针对2D和3D几何体的预处理技术，该技术针对光滑粒子流体动力学（SPH）及其他基于粒子的方法进行了优化。我们的流程始于在几何体表面附近通过基于面的邻域搜索生成分辨率自适应点云。该点云构成了有符号距离场的基础，从而能够在表面区域进行高效的局部计算。为了创建初始粒子配置，我们应用了分层缠绕数方法进行快速准确的内外分割。然后，使用受SPH启发的方案松弛粒子位置，该方案也用于填充边界粒子。这确保了完全核支持并促进了各向同性分布，同时保留了几何界面。通过利用基于粒子方法的无网格特性，我们的方法不需要连接信息，因此易于集成到现有的基于粒子框架中。它对不完善的输入几何体具有鲁棒性，并且在不影响性能的情况下具有内存效率。此外，我们的实验表明，随着分辨率的不断提高，所得粒子分布会收敛到精确的几何体。", "summary": "本文提出了一种鲁棒高效的预处理流程，用于为2D和3D几何体生成高质量的粒子分布，并针对SPH及其他粒子方法进行了优化。该方法通过生成分辨率自适应点云、构建有符号距离场、利用分层缠绕数方法进行内外分割，以及采用SPH启发式方案松弛和填充粒子位置。它不依赖网格连接信息，对不完善的输入几何体具有鲁棒性，且内存效率高。实验证明，随着分辨率的提高，生成的粒子分布能够收敛到精确的几何体，从而支持稳定准确的粒子模拟。", "keywords": "粒子方法, 预处理, 有符号距离场, SPH, 粒子分布", "comments": "本文的创新之处在于其全面的预处理流程，有效解决了粒子模拟中多个关键挑战，尤其体现在对不完善几何体的鲁棒性和无网格特性上。其重要性在于能够为基于粒子的模拟（特别是复杂形状）生成更稳定、更准确的初始粒子配置，极大地提升了模拟的质量和适用性。"}}
{"id": "2506.21300", "title": "An object-centric core metamodel for IoT-enhanced event logs", "authors": ["Yannis Bertrand", "Christian Imenkamp", "Lukas Malburg", "Matthias Ehrendorfer", "Marco Franceschetti", "Joscha Grüger", "Francesco Leotta", "Jürgen Mangler", "Ronny Seiger", "Agnes Koschmider", "Stefanie Rinderle-Ma", "Barbara Weber", "Estefania Serral"], "summary": "Advances in Internet-of-Things (IoT) technologies have prompted the\nintegration of IoT devices with business processes (BPs) in many organizations\nacross various sectors, such as manufacturing, healthcare and smart spaces. The\nproliferation of IoT devices leads to the generation of large amounts of IoT\ndata providing a window on the physical context of BPs, which facilitates the\ndiscovery of new insights about BPs using process mining (PM) techniques.\nHowever, to achieve these benefits, IoT data need to be combined with\ntraditional process (event) data, which is challenging due to the very\ndifferent characteristics of IoT and process data, for instance in terms of\ngranularity levels. Recently, several data models were proposed to integrate\nIoT data with process data, each focusing on different aspects of data\nintegration based on different assumptions and requirements. This fragmentation\nhampers data exchange and collaboration in the field of PM, e.g., making it\ntedious for researchers to share data. In this paper, we present a core model\nsynthesizing the most important features of existing data models. As the core\nmodel is based on common requirements, it greatly facilitates data sharing and\ncollaboration in the field. A prototypical Python implementation is used to\nevaluate the model against various use cases and demonstrate that it satisfies\nthese common requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21300v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21300v1", "AI": {"title_translation": "一个面向对象的物联网增强事件日志核心元模型", "tldr": "本文提出了一个核心元模型，用于整合物联网数据和流程数据，以解决现有数据模型碎片化的问题，从而促进过程挖掘领域的数据共享与协作。", "motivation": "物联网技术的发展使得物联网设备与业务流程的集成变得普遍，产生了大量的物联网数据，有助于通过过程挖掘发现新的业务洞察。然而，由于物联网数据和传统流程数据特性差异大（如粒度），两者结合面临挑战。现有数据模型碎片化，阻碍了数据交换和协作。", "method": "本文提出了一个核心元模型，该模型综合了现有数据模型最重要的特征，并基于共同的需求。通过一个原型Python实现，对该模型进行了评估，并针对各种用例进行了验证，证明其满足这些共同需求。", "result": "所提出的核心模型极大地促进了物联网增强事件日志领域的数据共享和协作，并且通过原型Python实现验证，该模型满足了共同的需求。", "conclusion": "本文提出的核心元模型解决了物联网数据与流程数据集成中现有数据模型碎片化的问题，通过提供一个基于共同需求的核心模型，极大地促进了数据共享和协作，为过程挖掘领域带来了便利。", "translation": "物联网（IoT）技术的进步促使物联网设备与各行各业（如制造业、医疗保健和智能空间）的业务流程（BP）进行整合。物联网设备的普及导致了大量物联网数据的生成，这些数据为业务流程的物理上下文提供了窗口，从而有助于使用过程挖掘（PM）技术发现关于业务流程的新洞察。然而，为了实现这些益处，物联网数据需要与传统流程（事件）数据相结合，这由于物联网数据和流程数据在粒度等方面的显著差异而具有挑战性。最近，提出了几种数据模型来整合物联网数据和流程数据，每个模型都基于不同的假设和要求，侧重于数据集成的不同方面。这种碎片化阻碍了过程挖掘领域的数据交换和协作，例如，使得研究人员共享数据变得繁琐。在本文中，我们提出了一个核心模型，该模型综合了现有数据模型最重要的特征。由于该核心模型基于共同的需求，它极大地促进了该领域的数据共享和协作。一个原型Python实现被用于根据各种用例评估该模型，并证明它满足这些共同需求。", "summary": "本文提出了一种面向对象的物联网增强事件日志核心元模型，旨在解决物联网数据与传统业务流程数据集成时面临的挑战，特别是现有数据模型碎片化的问题。该模型综合了现有数据模型的重要特征，并基于共同需求构建，旨在促进过程挖掘领域的数据共享与协作。一个原型Python实现验证了该模型在不同用例下满足通用要求。", "keywords": "物联网, 过程挖掘, 数据集成, 元模型, 事件日志", "comments": "该论文解决了物联网数据与传统流程数据集成中的一个关键问题，即数据模型碎片化。其创新点在于提出了一个综合现有模型特征的核心元模型，旨在提供一个通用的数据共享和协作基础。这对于过程挖掘领域的发展具有重要意义，因为它能有效降低数据交换的复杂性，促进跨领域研究。"}}
{"id": "2506.21063", "title": "Control of Marine Robots in the Era of Data-Driven Intelligence", "authors": ["Lin Hong", "Lu Liu", "Zhouhua Peng", "Fumin Zhang"], "summary": "The control of marine robots has long relied on model-based methods grounded\nin classical and modern control theory. However, the nonlinearity and\nuncertainties inherent in robot dynamics, coupled with the complexity of marine\nenvironments, have revealed the limitations of conventional control methods.\nThe rapid evolution of machine learning has opened new avenues for\nincorporating data-driven intelligence into control strategies, prompting a\nparadigm shift in the control of marine robots. This paper provides a review of\nrecent progress in marine robot control through the lens of this emerging\nparadigm. The review covers both individual and cooperative marine robotic\nsystems, highlighting notable achievements in data-driven control of marine\nrobots and summarizing open-source resources that support the development and\nvalidation of advanced control methods. Finally, several future perspectives\nare outlined to guide research toward achieving high-level autonomy for marine\nrobots in real-world applications. This paper aims to serve as a roadmap toward\nthe next-generation control framework of marine robots in the era of\ndata-driven intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21063v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21063v1", "AI": {"title_translation": "数据驱动智能时代下海洋机器人的控制", "tldr": "本文综述了数据驱动智能在海洋机器人控制中的最新进展，涵盖个体和协作系统，并展望了未来研究方向。", "motivation": "传统的基于模型的海洋机器人控制方法在处理非线性和不确定性方面存在局限性，而机器学习的快速发展为数据驱动智能控制提供了新机遇，推动了海洋机器人控制范式的转变。", "method": "本文通过新兴的数据驱动智能范式，综述了海洋机器人控制的最新进展，涵盖个体和协作系统，并总结了支持开发和验证先进控制方法的开源资源。", "result": "文章强调了数据驱动控制在海洋机器人领域的显著成就，并总结了相关开源资源。它还提出了未来研究方向，以实现海洋机器人在实际应用中的高级自主性。", "conclusion": "本文旨在为数据驱动智能时代下海洋机器人下一代控制框架的研究提供路线图。", "translation": "长期以来，海洋机器人的控制一直依赖于基于经典和现代控制理论的模型方法。然而，机器人动力学固有的非线性和不确定性，加上海洋环境的复杂性，揭示了传统控制方法的局限性。机器学习的快速发展为将数据驱动智能融入控制策略开辟了新途径，促使海洋机器人控制发生范式转变。本文从这一新兴范式的角度，综述了海洋机器人控制的最新进展。综述涵盖了单个和协作海洋机器人系统，重点介绍了数据驱动海洋机器人控制方面的显著成就，并总结了支持先进控制方法开发和验证的开源资源。最后，概述了几个未来展望，以指导研究实现海洋机器人在实际应用中的高级自主性。本文旨在作为数据驱动智能时代下海洋机器人下一代控制框架的路线图。", "summary": "本文综述了数据驱动智能在海洋机器人控制领域的最新进展，旨在应对传统模型方法的局限性。文章涵盖了数据驱动控制在个体和协作海洋机器人系统中的应用，突出了显著成就，并列举了开源资源。最后，提出了未来的研究方向，以期推动海洋机器人在实际应用中实现更高水平的自主性，为下一代控制框架提供路线图。", "keywords": "海洋机器人控制, 数据驱动, 机器学习, 综述, 自主性", "comments": "本文作为一篇综述性文章，全面梳理了数据驱动智能在海洋机器人控制领域的应用，指出了传统方法的局限性，并展望了未来发展方向，具有重要的指导意义。其价值在于为研究人员提供了一个清晰的路线图，加速了该领域向高水平自主性迈进。"}}
{"id": "2506.21532", "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets", "authors": ["Akshay Paruchuri", "Maryam Aziz", "Rohit Vartak", "Ayman Ali", "Best Uchehara", "Xin Liu", "Ishan Chatterjee", "Monica Agrawal"], "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat", "comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release", "pdf_url": "http://arxiv.org/pdf/2506.21532v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21532v1", "AI": {"title_translation": "“医生，你好吗？”：分析用户如何在大型对话式AI数据集中寻求健康信息", "tldr": "本文通过构建HealthChat-11K数据集并结合临床医生分类法，系统研究了用户在大型语言模型（LLM）中寻求健康信息的行为模式、潜在风险，并强调LLM在医疗保健支持能力方面需改进。", "motivation": "人们越来越多地通过交互式聊天机器人向大型语言模型（LLM）寻求医疗保健信息，但这些对话的性质和固有风险仍未得到充分探索。", "method": "通过过滤大型对话式AI数据集，构建了HealthChat-11K，一个包含1.1万个真实对话和2.5万条用户消息的精选数据集。然后，使用HealthChat-11K和一个由临床医生驱动的分类法，系统地研究了用户在21个不同健康专业中与LLM进行医疗信息查询时的互动。", "result": "分析揭示了用户如何以及为何寻求健康信息的性质的见解，例如常见交互、上下文不完整的情况、情感行为，以及可能诱导奉承的交互（例如，引导性问题）。", "conclusion": "强调了部署为对话式AI的LLM在医疗保健支持能力方面需要改进。", "translation": "人们越来越多地通过交互式聊天机器人向大型语言模型（LLM）寻求医疗保健信息，但这些对话的性质和固有风险仍未得到充分探索。在本文中，我们过滤了大型对话式AI数据集，以获得HealthChat-11K，这是一个包含1.1万个真实对话和2.5万条用户消息的精选数据集。我们使用HealthChat-11K和一个由临床医生驱动的用户与LLM在寻求医疗保健信息时交互方式的分类法，系统地研究了21个不同健康专业中的用户交互。我们的分析揭示了用户如何以及为何寻求健康信息的性质的见解，例如常见交互、上下文不完整的情况、情感行为，以及可能诱导奉承的交互（例如，引导性问题），这强调了部署为对话式AI的LLM在医疗保健支持能力方面需要改进。用于检索我们分析并将其组合成精选数据集的代码和工件可以在此处找到：https://github.com/yahskapar/HealthChat", "summary": "本文通过构建名为HealthChat-11K的1.1万个真实医疗对话数据集，并结合临床医生分类法，系统分析了用户在21个健康专业中如何与大型语言模型（LLM）进行健康信息查询。研究揭示了用户交互的常见模式、上下文缺失、情感表现以及可能导致LLM产生奉承性回应的交互方式（如引导性问题）。研究结果强调了当前LLM在提供医疗保健支持时存在不足，亟需改进其能力。", "keywords": "健康信息查询, 大型语言模型, 对话式AI, HealthChat-11K, 用户行为分析", "comments": "这项研究通过构建和分析一个大规模的真实医疗对话数据集（HealthChat-11K），填补了LLM在医疗健康信息查询领域对话性质和风险探索的空白。其创新之处在于使用了临床医生驱动的分类法来系统地研究用户行为，并揭示了LLM在医疗健康领域面临的具体挑战，如上下文理解和避免诱导奉承。这对于提升LLM在医疗领域的可靠性和安全性具有重要意义。"}}
{"id": "2506.21467", "title": "Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces", "authors": ["Michael Johnston", "Burkhard Ringlein", "Christoph Hagleitner", "Alessandro Pomponio", "Vassilis Vassiliadis", "Christian Pinto", "Srikumar Venugopal"], "summary": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21467v1", "categories": ["cs.DC", "C.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21467v1", "AI": {"title_translation": "高效可复用的基于发现空间的云配置搜索", "tldr": "提出“发现空间”抽象，解决大规模云资源配置优化问题，实现高效数据共享和知识复用，显著加速搜索过程。", "motivation": "在满足服务水平协议的前提下，以最低成本为给定工作负载寻找最优云资源配置是一个活跃的研究领域。由于云提供商和应用参数众多，配置空间巨大，包含数百万种部署选项，使得这一任务复杂且具有挑战性。", "method": "提出“发现空间”抽象，形式化描述工作负载配置问题，并具备结构化、健壮和分布式调查大型搜索空间所需的特性。同时描述了其具体实现，并展示了其在大型语言模型推理和大数据分析等多样化工作负载上的通用性。", "result": "该方法实现了在最优配置检测中，最佳优化器执行之间安全、透明的数据共享，提高了效率。此外，发现空间使知识在相似搜索空间之间进行转移和复用，将配置搜索速度提升了90%以上。", "conclusion": "发现空间抽象及其实现能够有效解决大规模云配置搜索的复杂性，通过数据共享和知识复用显著提高搜索效率和速度。", "translation": "在满足既定服务水平协议的前提下，以最低成本为给定工作负载寻找最优云资源配置是一个活跃的研究领域。将云提供商提供的计算、存储和服务的大量选择中适用的数十个参数与类似数量的应用程序特定参数相结合，导致配置空间包含数百万个部署选项。\n在本文中，我们提出了“发现空间”（Discovery Space），这是一种抽象，它形式化了工作负载配置问题的描述，并展现了对大型搜索空间进行结构化、健壮和分布式调查所需的一系列特性。我们描述了发现空间抽象的一个具体实现，并表明它可以在诸如大型语言模型推理和大数据分析等多样化工作负载中通用。\n我们证明了我们的方法能够在最佳优化器执行之间实现安全、透明的数据共享，从而提高大型搜索空间中最佳配置检测的效率。我们还展示了发现空间如何实现知识在相似搜索空间之间的转移和复用，从而使配置搜索速度提升90%以上。", "summary": "本文提出了“发现空间”抽象，旨在解决在海量配置选项中寻找最优云资源配置的挑战。该抽象形式化了工作负载配置问题，并支持对大型搜索空间的结构化、健壮和分布式调查。作者展示了其具体实现，并证明其在不同工作负载（如LLM推理和大数据分析）中的通用性。实验结果表明，该方法通过安全透明的数据共享提高了配置检测效率，并通过知识复用使搜索速度提升超过90%。", "keywords": "云配置, 发现空间, 优化, 知识复用, 大规模搜索", "comments": "这项工作通过引入“发现空间”这一抽象概念，为解决复杂的云资源配置优化问题提供了一个新颖且高效的框架。其创新点在于将配置问题形式化，并支持数据共享和知识复用，从而显著提升了搜索效率。特别是90%以上的速度提升，突显了其实用价值和潜在影响。该方法在通用性方面的展示也增加了其适用范围。"}}
{"id": "2506.20879", "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "summary": "Generation of images containing multiple humans, performing complex actions,\nwhile preserving their facial identities, is a significant challenge. A major\nfactor contributing to this is the lack of a a dedicated benchmark. To address\nthis, we introduce MultiHuman-Testbench, a novel benchmark for rigorously\nevaluating generative models for multi-human generation. The benchmark\ncomprises 1800 samples, including carefully curated text prompts, describing a\nrange of simple to complex human actions. These prompts are matched with a\ntotal of 5,550 unique human face images, sampled uniformly to ensure diversity\nacross age, ethnic background, and gender. Alongside captions, we provide\nhuman-selected pose conditioning images which accurately match the prompt. We\npropose a multi-faceted evaluation suite employing four key metrics to quantify\nface count, ID similarity, prompt alignment, and action detection. We conduct a\nthorough evaluation of a diverse set of models, including zero-shot approaches\nand training-based methods, with and without regional priors. We also propose\nnovel techniques to incorporate image and region isolation using human\nsegmentation and Hungarian matching, significantly improving ID similarity. Our\nproposed benchmark and key findings provide valuable insights and a\nstandardized tool for advancing research in multi-human image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20879v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20879v1", "AI": {"title_translation": "MultiHuman-Testbench：多人物图像生成基准测试", "tldr": "本文提出了MultiHuman-Testbench，一个用于评估多人物图像生成模型的新基准，解决了现有模型在生成多人物图像时面临的挑战，并提高了身份相似性。", "motivation": "生成包含多个执行复杂动作且保留面部身份的图像是一项重大挑战，主要原因是缺乏专门的基准测试。", "method": "本文引入了MultiHuman-Testbench，一个包含1800个样本（包括文本提示和5550个多样化的人脸图像）以及人工选择的姿态条件图像的新型基准。提出了一个多方面的评估套件，采用面部计数、ID相似性、提示对齐和动作检测四项关键指标。此外，还提出了利用人体分割和匈牙利匹配来整合图像和区域隔离的新技术。", "result": "对包括零样本和基于训练的方法在内的多种模型进行了全面评估，并展示了所提出的新技术显著提高了ID相似性。", "conclusion": "所提出的基准和主要发现为推进多人物图像生成研究提供了宝贵的见解和标准化工具。", "translation": "生成包含多个人物、执行复杂动作同时保留其面部身份的图像是一个重大挑战。导致这一问题的一个主要因素是缺乏专门的基准。为了解决这个问题，我们引入了MultiHuman-Testbench，这是一个用于严格评估多人物生成模型的生成模型的新型基准。该基准包含1800个样本，其中包括精心策划的文本提示，描述了从简单到复杂的一系列人类动作。这些提示与总共5550个独特的人脸图像匹配，这些图像均匀采样以确保年龄、种族背景和性别多样性。除了标题，我们还提供了人工选择的姿态条件图像，这些图像与提示准确匹配。我们提出了一个多方面的评估套件，采用四个关键指标来量化面部计数、ID相似性、提示对齐和动作检测。我们对包括零样本方法和基于训练的方法（有无区域先验）在内的多种模型进行了彻底评估。我们还提出了结合人体分割和匈牙利匹配来整合图像和区域隔离的新技术，显著提高了ID相似性。我们提出的基准和关键发现为推进多人物图像生成研究提供了宝贵的见解和标准化工具。", "summary": "本文介绍了MultiHuman-Testbench，一个用于评估多人物图像生成模型的新型基准。该基准包含1800个样本，包括多样化的文本提示、人脸图像和人工选择的姿态条件图像。研究者提出了一个包含面部计数、ID相似性、提示对齐和动作检测的四项关键指标的评估套件，并引入了结合人体分割和匈牙利匹配以提高ID相似性的新技术。该基准和相关发现旨在推动多人物图像生成领域的研究。", "keywords": "多人物图像生成, 基准测试, 面部身份, 姿态条件, 图像评估", "comments": "该论文的创新之处在于提出了首个专门针对多人物图像生成问题的综合基准MultiHuman-Testbench，填补了该领域基准测试的空白。其重要性体现在为未来的研究提供了一个标准化的评估工具，并提出了有效提高生成图像中人物身份相似性的新方法，对提升多人物图像生成质量具有重要意义。"}}
{"id": "2506.20790", "title": "Stochastic Parameter Decomposition", "authors": ["Lucius Bushnaq", "Dan Braun", "Lee Sharkey"], "summary": "A key step in reverse engineering neural networks is to decompose them into\nsimpler parts that can be studied in relative isolation. Linear parameter\ndecomposition -- a framework that has been proposed to resolve several issues\nwith current decomposition methods -- decomposes neural network parameters into\na sum of sparsely used vectors in parameter space. However, the current main\nmethod in this framework, Attribution-based Parameter Decomposition (APD), is\nimpractical on account of its computational cost and sensitivity to\nhyperparameters. In this work, we introduce \\textit{Stochastic Parameter\nDecomposition} (SPD), a method that is more scalable and robust to\nhyperparameters than APD, which we demonstrate by decomposing models that are\nslightly larger and more complex than was possible to decompose with APD. We\nalso show that SPD avoids other issues, such as shrinkage of the learned\nparameters, and better identifies ground truth mechanisms in toy models. By\nbridging causal mediation analysis and network decomposition methods, this\ndemonstration opens up new research possibilities in mechanistic\ninterpretability by removing barriers to scaling linear parameter decomposition\nmethods to larger models. We release a library for running SPD and reproducing\nour experiments at https://github.com/goodfire-ai/spd.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20790v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20790v1", "AI": {"title_translation": "随机参数分解", "tldr": "本文引入了随机参数分解（SPD），一种比现有方法更具可扩展性和鲁棒性的神经网络参数分解方法，有助于机制可解释性研究。", "motivation": "逆向工程神经网络的关键一步是将其分解为更简单的部分。现有的线性参数分解框架中的主要方法（APD）存在计算成本高和对超参数敏感的问题，限制了其在大模型上的应用。", "method": "本文提出了随机参数分解（SPD），该方法通过将神经网络参数分解为参数空间中稀疏使用的向量之和，并结合因果中介分析和网络分解方法，提高了可扩展性和对超参数的鲁棒性。", "result": "SPD比APD在计算上更具可扩展性和鲁棒性，能够分解比APD稍大和更复杂的模型。SPD还避免了学习参数的收缩等问题，并在玩具模型中更好地识别了真实机制。", "conclusion": "通过引入SPD并连接因果中介分析与网络分解方法，本文消除了线性参数分解方法扩展到更大模型的障碍，为机制可解释性研究开辟了新的可能性。", "translation": "逆向工程神经网络的一个关键步骤是将其分解为可以相对独立研究的更简单部分。线性参数分解——一个旨在解决当前分解方法中若干问题的框架——将神经网络参数分解为参数空间中稀疏使用向量的总和。然而，该框架中当前的主要方法，基于归因的参数分解（APD），因其计算成本和对超参数的敏感性而不切实际。在这项工作中，我们引入了“随机参数分解”（SPD），这是一种比APD更具可扩展性和对超参数更鲁棒的方法，我们通过分解比APD可能分解的模型稍大和更复杂的模型来证明了这一点。我们还表明，SPD避免了其他问题，例如学习参数的收缩，并且在玩具模型中更好地识别了真实机制。通过连接因果中介分析和网络分解方法，这一演示通过消除将线性参数分解方法扩展到更大模型的障碍，为机制可解释性开辟了新的研究可能性。我们发布了一个用于运行SPD和重现我们实验的库，网址为https://github.com/goodfire-ai/spd。", "summary": "本文提出了一种名为随机参数分解（SPD）的新方法，旨在解决现有神经网络参数分解方法（如APD）的计算成本和超参数敏感性问题。SPD能够更有效地分解大型复杂模型，避免了参数收缩，并提高了真实机制的识别能力。这项工作通过将因果中介分析与网络分解相结合，为机制可解释性领域提供了新的研究途径，并发布了相应的代码库。", "keywords": "神经网络分解, 随机参数分解, 机制可解释性, 线性参数分解, 因果中介分析", "comments": "本文的创新点在于提出了随机参数分解（SPD），有效解决了现有线性参数分解方法（如APD）在可扩展性和鲁棒性方面的局限性。通过引入SPD，并将其与因果中介分析相结合，该工作为神经网络的机制可解释性研究提供了实用的工具和新的方向，尤其是在处理大型复杂模型方面具有重要意义。"}}
{"id": "2506.21543", "title": "Detecting weighted hidden cliques", "authors": ["Urmisha Chatterjee", "Karissa Huang", "Ritabrata Karmakar", "B. R. Vinay Kumar", "Gábor Lugosi", "Nandan Malhotra", "Anirban Mandal", "Maruf Alam Tarafdar"], "summary": "We study a generalization of the classical hidden clique problem to graphs\nwith real-valued edge weights. Formally, we define a hypothesis testing\nproblem. Under the null hypothesis, edges of a complete graph on $n$ vertices\nare associated with independent and identically distributed edge weights from a\ndistribution $P$. Under the alternate hypothesis, $k$ vertices are chosen at\nrandom and the edge weights between them are drawn from a distribution $Q$,\nwhile the remaining are sampled from $P$. The goal is to decide, upon observing\nthe edge weights, which of the two hypotheses they were generated from. We\ninvestigate the problem under two different scenarios: (1) when $P$ and $Q$ are\ncompletely known, and (2) when there is only partial information of $P$ and\n$Q$. In the first scenario, we obtain statistical limits on $k$ when the two\nhypotheses are distinguishable, and when they are not. Additionally, in each of\nthe scenarios, we provide bounds on the minimal risk of the hypothesis testing\nproblem when $Q$ is not absolutely continuous with respect to $P$. We also\nprovide computationally efficient spectral tests that can distinguish the two\nhypotheses as long as $k=\\Omega(\\sqrt{n})$ in both the scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21543v1", "categories": ["math.ST", "cs.IT", "math.IT", "math.PR", "stat.TH", "62F03"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.21543v1", "AI": {"title_translation": "加权隐藏团的检测", "tldr": "本文研究了加权图中的隐藏团检测问题，将其建模为假设检验问题，并提出了在已知和部分已知分布情况下的统计界限和计算高效的谱测试方法。", "motivation": "本文旨在将经典的隐藏团问题推广到具有实值边权的图上，以解决在边权图上检测隐藏团的挑战。", "method": "研究方法是定义一个假设检验问题：零假设下边权来自分布P，备择假设下k个顶点间的边权来自分布Q，其余来自P。研究分为两种场景：P和Q完全已知，以及P和Q仅部分已知。在两种场景下，都提供了计算高效的谱测试方法来区分两种假设。", "result": "在第一种场景下，得到了两种假设可区分和不可区分时k的统计极限。在两种场景下，当Q不对P绝对连续时，提供了假设检验问题最小风险的界限。此外，在两种场景下，只要k=\\Omega(\\sqrt{n})，所提出的计算高效谱测试就能区分两种假设。", "conclusion": "论文成功地将经典隐藏团问题推广到加权图，并提出了在不同信息已知程度下有效的检测方法，特别是当隐藏团大小k达到\\Omega(\\sqrt{n})时，能够通过谱测试进行有效区分。", "translation": "我们研究了经典隐藏团问题向具有实值边权的图的推广。形式上，我们定义了一个假设检验问题。在零假设下，一个n个顶点的完全图的边权与来自分布P的独立同分布边权相关联。在备择假设下，随机选择k个顶点，它们之间的边权来自分布Q，而其余边权从P中采样。目标是在观察到边权后，判断它们是由哪种假设生成的。我们在两种不同场景下研究了这个问题：(1) 当P和Q完全已知时，以及 (2) 当P和Q只有部分信息时。在第一种场景中，我们获得了当两种假设可区分和不可区分时k的统计极限。此外，在每种场景中，当Q不对P绝对连续时，我们提供了假设检验问题最小风险的界限。我们还提供了计算高效的谱测试，在两种场景下，只要k=\\Omega(\\sqrt{n})，这些测试就能区分两种假设。", "summary": "本文探讨了加权图中的隐藏团检测问题，将其形式化为一个假设检验框架。研究了两种情况：当边权分布P和Q完全已知时，以及当它们仅部分已知时。研究工作包括确定在不同条件下隐藏团大小k的统计可区分性极限，以及在特定分布条件下的最小风险界限。更重要的是，论文提出了一种计算高效的谱测试方法，该方法在两种场景下均能有效检测隐藏团，只要隐藏团的大小k至少为\\Omega(\\sqrt{n})。", "keywords": "加权隐藏团, 假设检验, 谱测试, 统计极限, 图算法", "comments": "这篇论文通过将经典的隐藏团问题推广到加权图，并采用假设检验的框架，为该领域带来了新的视角。其创新点在于考虑了边权信息，并区分了完全已知和部分已知分布的场景。提出计算高效的谱测试，并在k=\\Omega(\\sqrt{n})的条件下提供检测保证，具有重要的理论和实践意义。这为处理更复杂的图数据提供了工具。"}}
{"id": "2506.20932", "title": "Thinning to improve two-sample discrepancy", "authors": ["Gleb Smirnov", "Roman Vershynin"], "summary": "The discrepancy between two independent samples \\(X_1,\\dots,X_n\\) and\n\\(Y_1,\\dots,Y_n\\) drawn from the same distribution on $\\mathbb{R}^d$ typically\nhas order \\(O(\\sqrt{n})\\) even in one dimension. We give a simple online\nalgorithm that reduces the discrepancy to \\(O(\\log^{2d} n)\\) by discarding a\nsmall fraction of the points.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.20932v1", "categories": ["math.PR", "cs.DS"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.20932v1", "AI": {"title_translation": "稀疏化以改善两样本差异", "tldr": "通过丢弃少量点，一个简单的在线算法可以将两样本差异从$O(\\sqrt{n})$降低到$O(\\log^{2d} n)$。", "motivation": "两独立样本之间的差异通常为$O(\\sqrt{n})$量级，即使在简单的一维情况下也是如此，这表明在比较或分析样本时存在显著的变异性或不匹配问题。", "method": "提出了一种简单的在线算法，通过丢弃一小部分点来减少样本间的差异。", "result": "该算法成功将两样本差异从典型的$O(\\sqrt{n})$降低到$O(\\log^{2d} n)$的量级。", "conclusion": "通过对样本进行稀疏化处理（丢弃少量点），可以显著降低两独立样本之间的统计差异，从而提高样本间的匹配或比较质量。", "translation": "从$\\mathbb{R}^d$上的相同分布中抽取的两个独立样本$X_1,\\dots,X_n$和$Y_1,\\dots,Y_n$之间的差异，即使在一维情况下，通常也具有$O(\\sqrt{n})$的量级。我们提供了一种简单的在线算法，通过丢弃一小部分点，将差异减少到$O(\\log^{2d} n)$。", "summary": "本论文提出了一种简单的在线算法，旨在解决从相同分布中抽取的两独立样本之间通常存在的$O(\\sqrt{n})$量级的差异问题。通过策略性地丢弃一小部分样本点，该算法能够显著地将样本差异降低到$O(\\log^{2d} n)$的量级，从而提高了样本间的匹配和比较效率。", "keywords": "两样本差异, 稀疏化, 在线算法, 复杂度降低, 统计学", "comments": "这项工作提出了一种创新且高效的方法来处理两样本差异问题。其创新点在于使用“稀疏化”（丢弃点）的策略来达到显著的差异降低效果，并且算法是“在线”的，这可能意味着其在实时应用中具有潜力。将差异从多项式量级降低到对数多项式量级是一个显著的改进，对需要高精度样本匹配的应用非常有价值。"}}
{"id": "2506.21208", "title": "Adversarial Training: Enhancing Out-of-Distribution Generalization for Learning Wireless Resource Allocation", "authors": ["Shengjie Liu", "Chenyang Yang"], "summary": "Deep neural networks (DNNs) have widespread applications for optimizing\nresource allocation. Yet, their performance is vulnerable to distribution\nshifts between training and test data, say channels. In this letter, we resort\nto adversarial training (AT) for enhancing out-of-distribution (OOD)\ngeneralizability of DNNs trained in unsupervised manner. We reformulate AT to\ncapture the OOD degradation, and propose a one-step gradient ascent method for\nAT. The proposed method is validated by optimizing hybrid precoding. Simulation\nresults showcase the enhanced OOD performance of multiple kinds of DNNs across\nvarious channel distributions, when only Rayleigh fading channels are used for\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21208v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21208v1", "AI": {"title_translation": "对抗训练：增强无线资源分配学习的分布外泛化能力", "tldr": "本文提出了一种改进的对抗训练方法，以增强深度神经网络在无线资源分配中对抗分布外变化的泛化能力，并通过混合预编码验证了其有效性。", "motivation": "深度神经网络在资源分配优化中广泛应用，但其性能容易受到训练和测试数据之间分布变化（如信道变化）的影响，导致泛化能力下降。", "method": "提出了一种改进的对抗训练 (AT) 方法，以增强无监督训练的深度神经网络的分布外 (OOD) 泛化能力。具体地，重新构建了 AT 以捕捉 OOD 性能下降，并提出了一种单步梯度上升方法进行 AT。该方法通过优化混合预编码进行验证。", "result": "仿真结果表明，当仅使用瑞利衰落信道进行训练时，所提出的方法能够增强多种深度神经网络在各种信道分布下的 OOD 性能。", "conclusion": "对抗训练可以有效提升深度神经网络在无线资源分配任务中面对信道分布变化的泛化能力。", "translation": "深度神经网络（DNNs）在优化资源分配方面有着广泛的应用。然而，它们的性能容易受到训练和测试数据之间分布变化（例如信道）的影响。在这封信中，我们采用对抗训练（AT）来增强以无监督方式训练的DNNs的分布外（OOD）泛化能力。我们重新构建了AT以捕捉OOD性能下降，并提出了一种用于AT的单步梯度上升方法。所提出的方法通过优化混合预编码得到了验证。仿真结果表明，当仅使用瑞利衰落信道进行训练时，多种DNNs在各种信道分布下都表现出增强的OOD性能。", "summary": "本文针对深度神经网络在无线资源分配中面临的分布外泛化能力差的问题，提出了一种改进的对抗训练方法。该方法通过重新构建对抗训练并采用单步梯度上升，旨在提升无监督训练的深度神经网络在不同信道分布下的鲁棒性。实验结果验证了所提方法能有效增强多种DNNs的分布外性能。", "keywords": "对抗训练, 分布外泛化, 无线资源分配, 深度神经网络, 混合预编码", "comments": "本文创新性地将对抗训练应用于无线资源分配中的深度神经网络，以解决其分布外泛化能力差的问题，这对于提升无线通信系统在复杂动态环境下的性能具有重要意义。提出的单步梯度上升方法简化了对抗训练过程。"}}
{"id": "2506.21191", "title": "Prompt-Guided Turn-Taking Prediction", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.", "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "pdf_url": "http://arxiv.org/pdf/2506.21191v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21191v1", "AI": {"title_translation": "提示引导的轮流预测", "tldr": "本文提出了一种新的提示引导模型，用于口语对话系统中的轮流预测，该模型可以根据文本提示动态控制轮流，并提高了预测精度。", "motivation": "轮流预测模型是口语对话系统和会话机器人中必不可少的部分。", "method": "本文提出了一种新颖的模型，可以通过文本提示动态控制轮流预测。该方法允许通过“更快”或“更平静”等指令进行直观和明确的控制，从而动态适应会话伙伴和上下文。所提出的模型基于Transformer的语音活动投影（VAP）模型，将文本提示嵌入整合到通道内Transformer和跨通道Transformer中。由于现有数据集中缺乏文本提示数据，研究人员利用大型语言模型（LLM）生成了合成提示句。", "result": "实验结果表明，所提出的模型提高了预测精度，并能根据文本提示有效地改变轮流时机行为。", "conclusion": "所提出的提示引导模型能够提高轮流预测的准确性，并允许根据文本提示动态调整轮流时机行为。", "translation": "轮流预测模型是口语对话系统和会话机器人中必不可少的部分。最近的方法利用基于Transformer的架构来连续实时地预测语音活动。在这项研究中，我们提出了一种新颖的模型，该模型可以通过文本提示动态控制轮流预测。这种方法允许通过“更快”或“更平静”等指令进行直观和明确的控制，从而动态适应会话伙伴和上下文。所提出的模型基于Transformer的语音活动投影（VAP）模型，将文本提示嵌入整合到通道内Transformer和跨通道Transformer中。我们使用超过950小时的人与人对话数据评估了我们方法的可行性。由于现有数据集中没有可用于所提出方法的文本提示数据，我们利用大型语言模型（LLM）生成了合成提示句。实验结果表明，所提出的模型提高了预测精度，并能根据文本提示有效地改变轮流时机行为。", "summary": "本文提出了一种新颖的、由文本提示引导的轮流预测模型，旨在使口语对话系统和会话机器人中的轮流行为能够动态调整。该模型基于Transformer的语音活动投影（VAP）架构，通过将文本提示嵌入集成到其内部，实现了对轮流时机的直观控制，如“更快”或“更平静”。为解决数据稀缺问题，研究人员利用大型语言模型生成了合成提示数据。在超过950小时的人与人对话数据上的实验表明，该模型不仅提高了预测准确性，还能有效地根据文本提示调整轮流时机行为。", "keywords": "轮流预测, 文本提示, Transformer, 对话系统, 大型语言模型", "comments": "该论文的创新点在于提出了一个可以通过文本提示动态控制轮流预测的模型，这为对话系统提供了更灵活和直观的交互方式。此外，利用大型语言模型生成合成训练数据以克服数据稀缺性问题，也体现了其方法上的新颖性。"}}
{"id": "2506.20696", "title": "IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "summary": "Elucidating the biomechanical behavior of the myocardium is crucial for\nunderstanding cardiac physiology, but cannot be directly inferred from clinical\nimaging and typically requires finite element (FE) simulations. However,\nconventional FE methods are computationally expensive and often fail to\nreproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed\nneural network (PINN) framework that integrates imaged motion consistency (IMC)\nwith FE modeling for patient-specific left ventricular (LV) biomechanics.\nCardiac motion is first estimated from MRI or echocardiography using either a\npre-trained attention-based network or an unsupervised cyclic-regularized\nnetwork, followed by extraction of motion modes. IMC-PINN-FE then rapidly\nestimates myocardial stiffness and active tension by fitting clinical pressure\nmeasurements, accelerating computation from hours to seconds compared to\ntraditional inverse FE. Based on these parameters, it performs FE modeling\nacross the cardiac cycle at 75x speedup. Through motion constraints, it matches\nimaged displacements more accurately, improving average Dice from 0.849 to\n0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE\nadvances previous PINN-FE models by introducing back-computation of material\nproperties and better motion fidelity. Using motion from a single subject to\nreconstruct shape modes also avoids the need for large datasets and improves\npatient specificity. IMC-PINN-FE offers a robust and efficient approach for\nrapid, personalized, and image-consistent cardiac biomechanical modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20696v1", "categories": ["physics.med-ph", "cs.AI", "eess.IV"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.20696v1", "AI": {"title_translation": "IMC-PINN-FE：一种结合图像运动一致性和生物力学参数估计的患者特异性左心室有限元建模的物理信息神经网络", "tldr": "IMC-PINN-FE是一种结合图像运动一致性的物理信息神经网络，能快速、准确地进行患者特异性左心室有限元建模和生物力学参数估计。", "motivation": "阐明心肌生物力学行为对理解心脏生理学至关重要，但无法直接从临床影像中推断；传统的有限元方法计算成本高且难以重现观察到的心脏运动。", "method": "提出IMC-PINN-FE框架，将图像运动一致性（IMC）与有限元建模相结合。首先使用预训练的注意力网络或无监督循环正则化网络从MRI或超声心动图估计心脏运动并提取运动模式。然后，IMC-PINN-FE通过拟合临床压力测量值快速估计心肌刚度和主动张力，并将计算时间从数小时缩短到数秒。基于这些参数，它以75倍的速度进行整个心动周期的有限元建模。", "result": "计算速度从数小时缩短到数秒（逆向有限元建模）；有限元建模速度提高75倍；通过运动约束，与图像位移匹配更准确，平均Dice系数从0.849提高到0.927；同时保留了真实的压力-容积行为。", "conclusion": "IMC-PINN-FE为快速、个性化和图像一致的心脏生物力学建模提供了一种稳健高效的方法，通过引入材料属性的反向计算和更好的运动保真度，改进了之前的PINN-FE模型。", "translation": "阐明心肌的生物力学行为对于理解心脏生理学至关重要，但无法直接从临床影像中推断，通常需要有限元（FE）模拟。然而，传统的有限元方法计算成本高，并且通常无法重现观察到的心脏运动。我们提出了IMC-PINN-FE，一个物理信息神经网络（PINN）框架，它将图像运动一致性（IMC）与有限元建模相结合，用于患者特异性左心室（LV）生物力学。心脏运动首先使用预训练的基于注意力网络或无监督循环正则化网络从MRI或超声心动图估计，然后提取运动模式。IMC-PINN-FE通过拟合临床压力测量值快速估计心肌刚度和主动张力，与传统逆向有限元相比，将计算时间从数小时加速到数秒。基于这些参数，它以75倍的速度在整个心动周期内执行有限元建模。通过运动约束，它更准确地匹配图像位移，将平均Dice系数从0.849提高到0.927，同时保持了真实的压力-容积行为。IMC-PINN-FE通过引入材料属性的反向计算和更好的运动保真度，改进了之前的PINN-FE模型。使用单个受试者的运动来重建形状模式也避免了对大型数据集的需求，并提高了患者特异性。IMC-PINN-FE为快速、个性化和图像一致的心脏生物力学建模提供了一种稳健高效的方法。", "summary": "IMC-PINN-FE是一个创新的物理信息神经网络框架，旨在解决传统有限元方法在心脏生物力学建模中计算昂贵且运动匹配不准确的问题。它通过整合图像运动一致性，能够从临床影像中快速估计心脏运动模式，并结合压力测量值高效地反演出心肌力学参数。该方法显著加速了患者特异性左心室有限元建模过程，并显著提高了模型与实际图像运动的匹配度，同时减少了对大量数据集的依赖，提升了患者特异性。", "keywords": "物理信息神经网络, 有限元建模, 心脏生物力学, 图像运动一致性, 患者特异性", "comments": "该论文提出了一种创新的结合了图像运动一致性的物理信息神经网络（IMC-PINN-FE），显著提升了患者特异性心脏生物力学建模的效率和准确性。其创新点在于将图像运动数据整合到PINN框架中，实现了材料属性的反向计算和更好的运动保真度。该方法将计算时间从数小时缩短到数秒，大大提高了临床应用的潜力。此外，通过利用单个受试者的运动数据，减少了对大型数据集的依赖，增强了患者特异性，这对个性化医疗具有重要意义。"}}
{"id": "2506.21049", "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "comment": "Accepted by ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21049v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21049v1", "AI": {"title_translation": "一种用于电商查询分类的半监督可伸缩统一框架", "tldr": "本文提出了一种名为SSUF的半监督可伸缩统一框架，通过知识增强、标签增强和结构增强模块，解决了电商查询分类中信息不足、对后验标签依赖高以及缺乏统一框架的问题，并在实验中显著优于现有模型。", "motivation": "电商查询分类面临以下挑战：查询短、缺乏上下文，标签间信息未被利用导致先验信息不足；现有方法依赖用户后验点击行为构建训练样本，导致马太效应；查询分类的子任务缺乏统一框架，导致算法优化效率低下。", "method": "本文提出了一种新颖的半监督可伸缩统一框架（SSUF），包含多个增强模块以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号，减少对后验标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块都高度可插拔，可根据子任务需要添加或移除输入特征。", "result": "广泛的离线和在线A/B实验结果表明，SSUF显著优于最先进的模型。", "conclusion": "SSUF通过其创新的多模块统一框架，有效解决了电商查询分类中的多项核心挑战，并取得了显著优于现有技术的性能，为该领域提供了一个高效且可扩展的解决方案。", "translation": "查询分类，包括意图和类别预测等多个子任务，对电商应用至关重要。电商查询通常较短且缺乏上下文，标签之间的信息无法利用，导致建模的先验信息不足。大多数现有工业查询分类方法依赖用户的后验点击行为来构建训练样本，导致马太效应的恶性循环。此外，查询分类的子任务缺乏统一框架，导致算法优化效率低下。\n在本文中，我们提出了一种新颖的半监督可伸缩统一框架（SSUF），包含多个增强模块以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号，减少对后验标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块都高度可插拔，可根据每个子任务的需要添加或移除输入特征。我们进行了广泛的离线和在线A/B实验，结果表明SSUF显著优于最先进的模型。", "summary": "本文提出了一种名为SSUF的半监督可伸缩统一框架，旨在解决电商查询分类中查询信息不足、对后验标签依赖性高以及缺乏统一框架的问题。SSUF包含知识增强、标签增强和结构增强三大模块，分别通过引入世界知识、利用标签语义和半监督信号以及增强标签关系来提升分类性能。该框架的模块化设计使其具有高度可插插拔性。实验结果表明，SSUF在离线和在线A/B测试中均显著优于现有SOTA模型。", "keywords": "电商查询分类, 半监督学习, 统一框架, 知识增强, 标签增强", "comments": "该论文的创新点在于提出了一个统一的、模块化的框架SSUF，解决了电商查询分类中普遍存在的短查询信息不足和对后验点击数据过度依赖的问题。通过引入知识增强、标签增强和结构增强等独立但可插拔的模块，SSUF提升了模型的泛化能力和优化效率。其半监督的特性也有效缓解了标注数据稀缺的问题。该框架在实际电商场景中具有重要的应用价值。"}}
{"id": "2506.20989", "title": "Can Gradient Descent Simulate Prompting?", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "comment": "14 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20989v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20989v1", "AI": {"title_translation": "梯度下降可以模拟提示吗？", "tldr": "本文提出了一种元训练语言模型的方法，使得梯度更新能够模拟提示的效果，从而使微调在某些任务上达到与提示相当的性能。", "motivation": "语言模型整合新信息主要有两种方式：改变提示或改变参数（如微调）。尽管参数更新没有长期存储成本，但在许多情况下，提示却显著更有效，尤其是在单示例泛化和逻辑推理方面。因此，本文旨在探讨是否可以通过修改模型，使微调能够模拟提示的效果。", "method": "本文描述了一种元训练语言模型的方法，使得梯度更新能够模拟对新信息进行条件化的效果。该方法利用了基于梯度的元学习工具，但使用语言模型自身的提示预测作为目标，从而消除了对真实标签的需求。", "result": "随后的梯度下降训练恢复了部分（有时是全部）提示模型的性能，在“逆转诅咒”任务上显示出改进，并且在单次梯度更新后能够回答关于文本段落的问题。", "conclusion": "这些结果表明，通过适当的初始化，梯度下降可以具有惊人的表达能力。本文的结果为长上下文建模提供了新的途径，并为基于梯度的学习的泛化能力提供了见解。", "translation": "将新信息整合到语言模型（LM）中有两种主要方式：改变其提示或改变其参数（例如通过微调）。参数更新不会产生模型更改的长期存储成本。然而，对于许多模型更新，提示明显更有效：提示模型可以从单个示例中稳健地泛化，并进行标准微调中不会出现的逻辑推理。那么，是否可以修改模型，使微调能够模拟提示呢？本文描述了一种元训练语言模型的方法，使得梯度更新能够模拟对新信息进行条件化的效果。我们的方法利用了基于梯度的元学习工具，但使用语言模型自身的提示预测作为目标，从而消除了对真实标签的需求。随后的梯度下降训练恢复了部分（有时是全部）提示模型的性能——在“逆转诅咒”任务上显示出改进，并在单次梯度更新后回答关于文本段落的问题。这些结果表明，通过适当的初始化，梯度下降可以具有惊人的表达能力。我们的结果为长上下文建模提供了新的途径，并为基于梯度的学习的泛化能力提供了见解。", "summary": "本文研究了梯度下降能否模拟语言模型中的提示效果。研究发现，尽管提示在某些任务上比微调更有效，但通过一种元训练方法，可以使梯度更新模拟提示对新信息的影响。该方法利用基于梯度的元学习，并以模型自身的提示预测为目标，无需真实标签。实验结果表明，这种方法能使梯度下降训练恢复部分甚至全部提示性能，并在“逆转诅咒”任务和单次更新后的文本问答中显示出改进。这表明适当初始化的梯度下降具有强大的表达能力，为长上下文建模和梯度学习的泛化提供了新思路。", "keywords": "梯度下降, 提示, 元学习, 语言模型, 微调", "comments": "本文的创新点在于提出了一种无需真实标签的元训练方法，使得微调能够模拟提示的效果，这在传统上被认为是两种截然不同的信息整合方式。其重要性体现在揭示了梯度下降的潜在表达能力，并为长上下文建模和理解基于梯度学习的泛化提供了新的研究方向。这种方法可能为未来语言模型的高效适应和知识注入提供新的范式。"}}
{"id": "2506.21241", "title": "On the coordinate system-dependence of the accuracy of symplectic numerical methods", "authors": ["Donát M. Takács", "Tamás Fülöp"], "summary": "Symplectic numerical methods have become a widely-used choice for the\naccurate simulation of Hamiltonian systems in various fields, including\ncelestial mechanics, molecular dynamics and robotics. Even though their\ncharacteristics are well-understood mathematically, relatively little attention\nhas been paid in general to the practical aspect of how the choice of\ncoordinates affects the accuracy of the numerical results, even though the\nconsequences can be computationally significant. The present article aims to\nfill this gap by giving a systematic overview of how coordinate transformations\ncan influence the results of simulations performed using symplectic methods. We\ngive a derivation for the non-invariance of the modified Hamiltonian of\nsymplectic methods under coordinate transformations, as well as a sufficient\ncondition for the non-preservation of a first integral corresponding to a\ncyclic coordinate for the symplectic Euler method. We also consider the\npossibility of finding order-compensating coordinate transformations that\nimprove the order of accuracy of a numerical method. Various numerical examples\nare presented throughout.", "comment": "24 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.21241v1", "categories": ["math.NA", "cs.NA", "physics.class-ph", "physics.comp-ph", "65P10, 70H15, 34A05"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21241v1", "AI": {"title_translation": "辛几何数值方法精度对坐标系的依赖性", "tldr": "辛几何数值方法的精度受到坐标系选择的影响，本文系统探讨了其原因和影响，并提出了提高精度的方法。", "motivation": "辛几何方法在哈密顿系统模拟中广泛应用，但坐标系选择对其精度影响的实际方面研究不足，尽管其计算后果显著。本文旨在填补这一空白。", "method": "本文系统地概述了坐标变换如何影响辛几何方法的模拟结果。具体方法包括推导辛几何方法修正哈密顿量在坐标变换下的非不变性，以及给出辛欧拉方法对应循环坐标第一积分不守恒的充分条件。文章还考虑了寻找能提高数值方法精度阶数的序补偿坐标变换，并提供了各种数值示例。", "result": "推导了辛几何方法修正哈密顿量在坐标变换下的非不变性，并给出了辛欧拉方法对应循环坐标第一积分不守恒的充分条件。探讨了序补偿坐标变换提高精度阶数的可能性。", "conclusion": "辛几何方法的精度确实依赖于坐标系的选择，这种依赖性体现在修正哈密顿量的非不变性和第一积分的非守恒上，同时存在通过特定坐标变换提高精度的方法。", "translation": "辛几何数值方法已成为各领域（包括天体力学、分子动力学和机器人学）中精确模拟哈密顿系统的广泛选择。尽管它们的特性在数学上已得到充分理解，但通常很少关注坐标选择如何影响数值结果精度的实际方面，尽管其后果在计算上可能非常显著。本文旨在通过系统地概述坐标变换如何影响使用辛几何方法进行的模拟结果来填补这一空白。我们推导了辛几何方法修正哈密顿量在坐标变换下的非不变性，以及辛欧拉方法对应循环坐标第一积分不守恒的充分条件。我们还考虑了寻找能够提高数值方法精度阶数的序补偿坐标变换的可能性。文中提供了各种数值示例。", "summary": "本文探讨了辛几何数值方法精度对坐标系的依赖性。研究发现，辛几何方法的修正哈密顿量在坐标变换下不具有不变性，并给出了辛欧拉方法中循环坐标对应第一积分不守恒的条件。文章还探讨了通过特定坐标变换提高数值方法精度阶数（序补偿）的可能性，并通过数值示例进行了验证，旨在弥补现有研究中对坐标选择实际影响关注不足的空白。", "keywords": "辛几何方法, 坐标变换, 精度, 哈密顿系统, 修正哈密顿量", "comments": "这篇论文的创新点在于系统地揭示了辛几何数值方法精度对坐标系选择的实际依赖性，填补了现有研究的空白。它不仅从理论上推导了修正哈密顿量的非不变性和第一积分的非守恒性，还提出了通过“序补偿”坐标变换来提高精度的方法，具有重要的理论和实际意义，尤其对于需要高精度哈密顿系统模拟的领域。"}}
{"id": "2506.20856", "title": "Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA", "authors": ["Fei Wang", "Baochun Li"], "summary": "Memorization in large language models (LLMs) makes them vulnerable to data\nextraction attacks. While pre-training memorization has been extensively\nstudied, fewer works have explored its impact in fine-tuning, particularly for\nLoRA fine-tuning, a widely adopted parameter-efficient method.\n  In this work, we re-examine memorization in fine-tuning and uncover a\nsurprising divergence from prior findings across different fine-tuning\nstrategies. Factors such as model scale and data duplication, which strongly\ninfluence memorization in pre-training and full fine-tuning, do not follow the\nsame trend in LoRA fine-tuning. Using a more relaxed similarity-based\nmemorization metric, we demonstrate that LoRA significantly reduces\nmemorization risks compared to full fine-tuning, while still maintaining strong\ntask performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20856v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20856v1", "AI": {"title_translation": "更精简的训练，更低的泄露：重新审视LoRA微调中大型语言模型的记忆化", "tldr": "本研究发现LoRA微调显著降低了大型语言模型的记忆化风险，同时保持了强大的任务性能，这与全量微调和预训练的记忆化趋势不同。", "motivation": "大型语言模型（LLMs）的记忆化使其容易受到数据提取攻击。虽然预训练中的记忆化已被广泛研究，但很少有工作探索其在微调中的影响，特别是对于广泛采用的参数高效方法LoRA微调。", "method": "本研究重新审视了微调中的记忆化，并使用了一种更宽松的基于相似度的记忆化度量标准。研究比较了LoRA微调与全量微调，并分析了模型规模和数据重复等因素对记忆化的影响。", "result": "研究发现LoRA微调的记忆化趋势与预训练和全量微调中的记忆化趋势存在显著差异。模型规模和数据重复等在预训练和全量微调中强烈影响记忆化的因素，在LoRA微调中不遵循相同的趋势。LoRA与全量微调相比显著降低了记忆化风险，同时仍保持了强大的任务性能。", "conclusion": "LoRA微调是一种有效的方法，可以显著降低大型语言模型的记忆化风险，同时不牺牲任务性能，这表明LoRA是提高模型安全性的有前途的策略。", "translation": "大型语言模型（LLMs）的记忆化使其容易受到数据提取攻击。虽然预训练中的记忆化已被广泛研究，但很少有工作探索其在微调中的影响，特别是对于广泛采用的参数高效方法LoRA微调。\n在本工作中，我们重新审视了微调中的记忆化，并揭示了不同微调策略下与先前发现的惊人差异。模型规模和数据重复等在预训练和全量微调中强烈影响记忆化的因素，在LoRA微调中不遵循相同的趋势。使用一种更宽松的基于相似度的记忆化度量标准，我们证明LoRA与全量微调相比显著降低了记忆化风险，同时仍保持了强大的任务性能。", "summary": "本研究深入探讨了大型语言模型（LLMs）在微调过程中的记忆化现象，特别关注了参数高效的LoRA微调方法。研究发现，LoRA微调的记忆化行为与预训练及全量微调存在显著差异，例如模型规模和数据重复等因素的影响趋势不同。通过采用更宽松的相似度记忆化度量，研究证明LoRA微调能够显著降低记忆化风险，同时保持优异的任务性能，从而为LLM的安全性提供了新的视角。", "keywords": "LLM, 记忆化, LoRA, 微调, 数据泄露", "comments": "这篇论文的创新之处在于它首次系统地深入探讨了LoRA微调中的记忆化问题，并揭示了其与传统微调和预训练的显著差异。其重要性在于为LLM的隐私和安全提供了新的见解，并指出LoRA不仅是参数高效的方法，也是一种有助于降低数据泄露风险的有效策略。这对于开发更安全、更可靠的LLM具有实际指导意义。"}}
{"id": "2506.20807", "title": "GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization", "authors": ["Martin Andrews", "Sam Witteveen"], "summary": "Optimizing GPU kernels for high performance is a complex task, often\ndemanding deep architectural knowledge, extensive profiling, and iterative\nexperimentation. This challenge is amplified when targeting newer or\nless-documented GPU architectures where traditional development aids are\nscarce. This paper introduces an LLM-powered \"GPU Kernel Scientist,\" an\nautomated methodology for iteratively refining accelerator kernels.\n  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)\nstrategically selecting promising prior code versions as a basis for new\niterations; (b) generating hypotheses for optimization experiments, based on\nexisting code and assimilated knowledge from general GPU literature; and (c)\nautonomously implementing these experiments through code modification and\nsubsequent submission to an external evaluation system, using only observed\ntiming data as performance feedback. We detail how this approach navigates the\nchallenges of the AMD MI300 target architecture and leverages LLMs to\ncompensate for limited domain-specific human expertise.\n  Since quantitative results from an ongoing performance competition were\nembargoed on paper submission date, we present the architectural design,\noperational workflow, and qualitative insights, highlighting the potential of\nLLM-driven agents to democratise and accelerate GPU kernel optimization,\nespecially in resource-constrained or rapidly evolving hardware environments.", "comment": "4 page paper plus Appendices. Accepted to the ES-FoMo \"Efficient\n  Systems for Foundation Models\" workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.20807v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20807v1", "AI": {"title_translation": "GPU核科学家：一个LLM驱动的迭代核优化框架", "tldr": "本文介绍了一个名为“GPU核科学家”的LLM驱动框架，用于自动化和迭代优化GPU核，尤其是在缺乏文档的新架构上。它通过LLM选择代码版本、生成优化假设并自主实现实验。", "motivation": "优化GPU核以获得高性能是一项复杂的任务，需要深入的架构知识、广泛的分析和迭代实验。当针对较新或文档较少的GPU架构时，这一挑战更加突出，因为传统开发辅助工具稀缺。", "method": "本文提出了一个LLM驱动的“GPU核科学家”自动化方法，用于迭代优化加速器核。该方法采用多阶段、演化过程：(a) 战略性地选择有前景的现有代码版本作为新迭代的基础；(b) 基于现有代码和从通用GPU文献中吸收的知识生成优化实验假设；(c) 通过代码修改自主实施这些实验，并提交给外部评估系统，仅使用观察到的时间数据作为性能反馈。该方法应对AMD MI300目标架构的挑战，并利用LLM弥补领域特定人类专业知识的不足。", "result": "由于正在进行的性能竞赛的定量结果在论文提交日期被禁运，本文主要介绍了架构设计、操作工作流程和定性见解。它强调了LLM驱动的代理在普及和加速GPU核优化方面的潜力，特别是在资源受限或快速发展的硬件环境中。", "conclusion": "LLM驱动的代理有潜力使GPU核优化民主化并加速，尤其是在资源受限或快速发展的硬件环境中，通过自动化迭代优化过程来弥补领域特定人类专业知识的不足。", "translation": "优化GPU核以获得高性能是一项复杂的任务，通常需要深厚的架构知识、广泛的性能分析和迭代实验。当针对较新或文档较少的GPU架构时，这一挑战会加剧，因为传统开发辅助工具稀缺。本文介绍了一个由大型语言模型（LLM）驱动的“GPU核科学家”，这是一种自动化方法，用于迭代地优化加速器核。\n我们的方法采用LLM进行多阶段的演化过程：(a) 战略性地选择有前景的先前代码版本作为新迭代的基础；(b) 基于现有代码和从通用GPU文献中吸收的知识生成优化实验假设；(c) 通过代码修改自主实施这些实验，并随后提交给外部评估系统，仅使用观察到的时间数据作为性能反馈。我们详细介绍了这种方法如何应对AMD MI300目标架构的挑战，并利用LLM来弥补有限的领域特定人类专业知识。\n由于正在进行的性能竞赛的定量结果在论文提交日期被禁运，我们展示了架构设计、操作工作流程和定性见解，强调了LLM驱动代理在普及和加速GPU核优化方面的潜力，特别是在资源受限或快速发展的硬件环境中。", "summary": "本文提出了一个名为“GPU核科学家”的LLM驱动框架，旨在自动化和加速GPU核优化过程。该框架通过LLM在多阶段演化过程中选择最佳代码版本、生成优化假设并自主执行实验。它特别适用于缺乏文档的新兴GPU架构，通过利用LLM弥补人类专业知识的不足。尽管定量结果因竞赛禁运而未公开，但论文详细描述了其架构设计、操作流程及定性潜力，强调了LLM在GPU优化领域的民主化和加速作用。", "keywords": "GPU优化, LLM, 核优化, 自动化, AMD MI300", "comments": "这篇论文提出了一种创新性的方法，利用LLM自动化GPU核优化，尤其是在新架构下缺乏专业知识的挑战性场景中。其多阶段、演化式的优化流程具有很强的实用价值。虽然缺乏具体的定量性能数据是一个遗憾，但其对LLM在复杂系统优化中作用的探索，以及对AMD MI300等新兴架构的关注，显示了其前瞻性和重要性。该框架有望降低GPU性能优化的门槛，加速新硬件的开发和应用。"}}
{"id": "2506.21077", "title": "CURL-SLAM: Continuous and Compact LiDAR Mapping", "authors": ["Kaicheng Zhang", "Shida Xu", "Yining Ding", "Xianwen Kong", "Sen Wang"], "summary": "This paper studies 3D LiDAR mapping with a focus on developing an updatable\nand localizable map representation that enables continuity, compactness and\nconsistency in 3D maps. Traditional LiDAR Simultaneous Localization and Mapping\n(SLAM) systems often rely on 3D point cloud maps, which typically require\nextensive storage to preserve structural details in large-scale environments.\nIn this paper, we propose a novel paradigm for LiDAR SLAM by leveraging the\nContinuous and Ultra-compact Representation of LiDAR (CURL) introduced in [1].\nOur proposed LiDAR mapping approach, CURL-SLAM, produces compact 3D maps\ncapable of continuous reconstruction at variable densities using CURL's\nspherical harmonics implicit encoding, and achieves global map consistency\nafter loop closure. Unlike popular Iterative Closest Point (ICP)-based LiDAR\nodometry techniques, CURL-SLAM formulates LiDAR pose estimation as a unique\noptimization problem tailored for CURL and extends it to local Bundle\nAdjustment (BA), enabling simultaneous pose refinement and map correction.\nExperimental results demonstrate that CURL-SLAM achieves state-of-the-art 3D\nmapping quality and competitive LiDAR trajectory accuracy, delivering\nsensor-rate real-time performance (10 Hz) on a CPU. We will release the\nCURL-SLAM implementation to the community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21077v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21077v1", "AI": {"title_translation": "CURL-SLAM：连续紧凑的激光雷达建图", "tldr": "CURL-SLAM提出了一种基于CURL的激光雷达SLAM新范式，实现了连续、紧凑且一致的3D地图，并具有高精度和实时性能。", "motivation": "传统的激光雷达同步定位与建图（SLAM）系统通常依赖于3D点云地图，这需要大量的存储空间来保留大规模环境中的结构细节。", "method": "本文提出了一种新的激光雷达SLAM范式，利用了[1]中引入的激光雷达连续超紧凑表示（CURL）。所提出的CURL-SLAM方法通过CURL的球谐函数隐式编码生成紧凑的3D地图，能够以可变密度进行连续重建，并在闭环后实现全局地图一致性。与流行的基于迭代最近点（ICP）的激光雷达里程计技术不同，CURL-SLAM将激光雷达位姿估计公式化为一个针对CURL量身定制的独特优化问题，并将其扩展到局部捆集调整（BA），从而实现同时的位姿细化和地图校正。", "result": "CURL-SLAM在CPU上实现了最先进的3D建图质量和具有竞争力的激光雷达轨迹精度，并提供了传感器速率的实时性能（10 Hz）。", "conclusion": "CURL-SLAM通过引入基于CURL的新型激光雷达SLAM范式，成功解决了传统点云地图存储量大、连续性不足的问题，实现了高效、高质量且实时运行的3D激光雷达建图。", "translation": "本文研究了3D激光雷达建图，重点是开发一种可更新和可定位的地图表示，以实现3D地图的连续性、紧凑性和一致性。传统的激光雷达同步定位与建图（SLAM）系统通常依赖于3D点云地图，这通常需要大量的存储空间来保留大规模环境中的结构细节。在本文中，我们通过利用[1]中引入的激光雷达连续超紧凑表示（CURL），提出了一种新的激光雷达SLAM范式。我们提出的激光雷达建图方法CURL-SLAM，使用CURL的球谐函数隐式编码，生成能够以可变密度进行连续重建的紧凑3D地图，并在闭环后实现全局地图一致性。与流行的基于迭代最近点（ICP）的激光雷达里程计技术不同，CURL-SLAM将激光雷达位姿估计公式化为一个针对CURL量身定制的独特优化问题，并将其扩展到局部捆集调整（BA），从而实现同时的位姿细化和地图校正。实验结果表明，CURL-SLAM在CPU上实现了最先进的3D建图质量和具有竞争力的激光雷达轨迹精度，并提供了传感器速率的实时性能（10 Hz）。我们将向社区发布CURL-SLAM的实现。", "summary": "CURL-SLAM提出了一种创新的激光雷达SLAM方法，旨在解决传统点云地图存储需求大的问题。该方法利用激光雷达连续超紧凑表示（CURL）来生成紧凑、可变密度且一致的3D地图。CURL-SLAM将位姿估计重新定义为针对CURL的优化问题，并结合局部捆集调整，实现了位姿和地图的同步优化。实验证明，CURL-SLAM在3D建图质量和轨迹精度上达到了先进水平，并能在CPU上实现实时性能。", "keywords": "激光雷达SLAM, 紧凑地图, CURL, 球谐函数, 实时性能", "comments": "CURL-SLAM的创新之处在于引入了CURL作为激光雷达地图表示，这显著提高了地图的紧凑性和连续性。通过将位姿估计公式化为针对CURL的优化问题，并结合局部BA，实现了更精确的位姿和地图修正。该方法在保证高精度的同时，实现了CPU上的实时性能，这对于实际应用具有重要意义。"}}
{"id": "2506.21506", "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jiménez Gutiérrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.", "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/", "pdf_url": "http://arxiv.org/pdf/2506.21506v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21506v1", "AI": {"title_translation": "Mind2Web 2：使用智能体作为评判者评估智能体搜索", "tldr": "Mind2Web 2是一个新的基准，用于评估复杂的智能体搜索系统，并引入了“智能体作为评判者”框架来自动评估答案。", "motivation": "现有的评估基准和方法已无法满足日益复杂的智能体搜索系统的评估需求，这些系统需要长时间的搜索和信息综合。", "method": "论文引入了Mind2Web 2，一个包含130个真实、高质量、长周期任务的基准，需要实时网页浏览和大量信息综合。同时，提出了“智能体作为评判者”框架，该框架基于树状评分标准设计，构建任务特定的评判智能体，以自动评估答案的正确性和来源归属。", "result": "对九个前沿智能体搜索系统和人类表现进行了全面评估。最佳系统OpenAI Deep Research在花费一半时间的情况下，能达到人类表现的50-70%。", "conclusion": "Mind2Web 2为开发和评估下一代智能体搜索系统提供了严谨的基础，并展示了当前智能体搜索系统的巨大潜力。", "translation": "智能体搜索系统，如深度研究系统，其中大型语言模型自主浏览网页、综合信息并返回带有全面引用的答案，代表着用户与网络规模信息交互方式的重大转变。尽管智能体搜索有望提高效率和认知卸载，但其日益增长的复杂性和开放性已超越了现有的评估基准和方法，这些基准和方法主要假设搜索范围短且答案是静态的。在本文中，我们引入了Mind2Web 2，这是一个包含130个真实、高质量、长周期任务的基准，需要实时网页浏览和大量信息综合，耗费了超过1000小时的人工劳动构建而成。为了解决评估时变和复杂答案的挑战，我们提出了一种新颖的“智能体作为评判者”框架。我们的方法基于树状评分标准设计构建任务特定的评判智能体，以自动评估答案的正确性和来源归属。我们对九个前沿智能体搜索系统和人类表现进行了全面评估，并进行了详细的错误分析，为未来的发展提供了见解。表现最佳的系统OpenAI Deep Research已经可以在花费一半时间的情况下达到人类表现的50-70%，显示出巨大的潜力。总而言之，Mind2Web 2为开发和评估下一代智能体搜索系统提供了严谨的基础。", "summary": "本文介绍了Mind2Web 2，一个用于评估复杂智能体搜索系统的新基准，包含130个需要实时网页浏览和信息综合的长周期任务。为解决评估挑战，论文提出了“智能体作为评判者”框架，通过构建任务特定评判智能体自动评估答案的正确性和来源归属。研究评估了九个前沿系统，发现最佳系统能达到人类表现的50-70%，表明智能体搜索潜力巨大，Mind2Web 2为未来研究奠定基础。", "keywords": "智能体搜索, 评估基准, Agent-as-a-Judge, Mind2Web 2, 大型语言模型", "comments": "本文通过引入Mind2Web 2基准和“智能体作为评判者”评估框架，解决了智能体搜索系统评估中的关键挑战，即现有方法无法应对其复杂性和开放性。其创新之处在于构建了大规模、真实场景的任务集以及自动化的评估机制，这对于推动该领域的发展至关重要。研究结果也提供了关于当前系统性能的宝贵见解。"}}
{"id": "2506.20442", "title": "When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact", "authors": ["Tianyao Shi", "Ritbik Kumar", "Inez Hua", "Yi Ding"], "summary": "Biodiversity loss is a critical planetary boundary, yet its connection to\ncomputing remains largely unexamined. Prior sustainability efforts in computing\nhave focused on carbon and water, overlooking biodiversity due to the lack of\nappropriate metrics and modeling frameworks. This paper presents the first\nend-to-end analysis of biodiversity impact from computing systems. We introduce\ntwo new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity\nIndex (OBI)--to quantify biodiversity impact across the lifecycle, and present\nFABRIC, a modeling framework that links computing workloads to biodiversity\nimpacts. Our evaluation highlights the need to consider biodiversity alongside\ncarbon and water in sustainable computing design and optimization. The code is\navailable at https://github.com/TianyaoShi/FABRIC.", "comment": "Accepted by HotCarbon' 25", "pdf_url": "http://arxiv.org/pdf/2506.20442v1", "categories": ["cs.CY", "cs.AR", "cs.DC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20442v1", "AI": {"title_translation": "当服务器遇到物种：计算对生物多样性影响的全生命周期视角", "tldr": "本文首次端到端分析了计算系统对生物多样性的影响，引入了EBI和OBI两个新指标以及FABRIC建模框架，强调在可持续计算设计中需同时考虑生物多样性、碳和水。", "motivation": "生物多样性丧失是一个关键的行星边界问题，但计算领域以往的可持续性努力主要关注碳和水，忽视了生物多样性，因为缺乏合适的衡量指标和建模框架。本文旨在填补这一空白。", "method": "本文提出了计算系统对生物多样性影响的首次端到端分析。引入了两个新指标：具身生物多样性指数（Embodied Biodiversity Index, EBI）和运行生物多样性指数（Operational Biodiversity Index, OBI），用于量化计算系统全生命周期的生物多样性影响。同时，提出了FABRIC建模框架，将计算工作负载与生物多样性影响联系起来。", "result": "评估结果强调，在可持续计算的设计和优化中，需要将生物多样性与碳和水一同考虑。代码已开源。", "conclusion": "计算系统对生物多样性的影响是真实存在的，并且需要被纳入可持续计算的设计和优化考量中，与碳和水同等重要。", "translation": "生物多样性丧失是一个关键的行星边界，然而其与计算的联系在很大程度上仍未被检验。计算领域以往的可持续性努力主要集中在碳和水，由于缺乏适当的衡量指标和建模框架，生物多样性被忽视了。本文首次对计算系统对生物多样性的影响进行了端到端分析。我们引入了两个新的指标——具身生物多样性指数（EBI）和运行生物多样性指数（OBI）——以量化全生命周期的生物多样性影响，并提出了FABRIC，一个将计算工作负载与生物多样性影响联系起来的建模框架。我们的评估强调，在可持续计算设计和优化中，需要将生物多样性与碳和水一同考虑。代码可在 https://github.com/TianyaoShi/FABRIC 获取。", "summary": "本文首次全面分析了计算系统对生物多样性的影响，指出以往研究忽视了这一领域。为解决缺乏衡量标准的问题，作者提出了具身生物多样性指数（EBI）和运行生物多样性指数（OBI）两个新指标，并开发了FABRIC建模框架，旨在量化并关联计算工作负载与生物多样性影响。研究强调，未来的可持续计算设计和优化应将生物多样性与碳、水消耗一同纳入考量。", "keywords": "生物多样性影响, 可持续计算, EBI, OBI, FABRIC", "comments": "本文的创新之处在于首次将生物多样性这一关键的行星边界问题与计算系统联系起来，并提出了具体的量化指标（EBI和OBI）和建模框架（FABRIC），为可持续计算领域拓展了新的研究维度。其重要性在于填补了现有研究的空白，为未来构建更全面的可持续计算策略提供了基础。"}}
{"id": "2506.20900", "title": "The Role of Cyclopean-Eye in Stereo Vision", "authors": ["Sherlon Almeida da Silva", "Davi Geiger", "Luiz Velho", "Moacir Antonelli Ponti"], "summary": "This work investigates the geometric foundations of modern stereo vision\nsystems, with a focus on how 3D structure and human-inspired perception\ncontribute to accurate depth reconstruction. We revisit the Cyclopean Eye model\nand propose novel geometric constraints that account for occlusions and depth\ndiscontinuities. Our analysis includes the evaluation of stereo feature\nmatching quality derived from deep learning models, as well as the role of\nattention mechanisms in recovering meaningful 3D surfaces. Through both\ntheoretical insights and empirical studies on real datasets, we demonstrate\nthat combining strong geometric priors with learned features provides internal\nabstractions for understanding stereo vision systems.", "comment": "arXiv admin note: text overlap with arXiv:2502.21280", "pdf_url": "http://arxiv.org/pdf/2506.20900v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20900v1", "AI": {"title_translation": "独眼视觉在立体视觉中的作用", "tldr": "本文探讨了独眼视觉模型在现代立体视觉中的几何基础，并结合深度学习和注意力机制，通过理论和实证研究证明了几何先验与学习特征结合的重要性。", "motivation": "现代立体视觉系统如何利用3D结构和受人类启发感知来准确重建深度。", "method": "重新审视独眼视觉模型，提出新的几何约束以处理遮挡和深度不连续性；评估深度学习模型衍生的立体特征匹配质量；分析注意力机制在恢复有意义3D表面中的作用。通过理论分析和真实数据集的实证研究。", "result": "结合强大的几何先验与学习特征，为理解立体视觉系统提供了内部抽象。", "conclusion": "结合几何先验与学习特征能够有效提升立体视觉系统的理解和深度重建能力。", "translation": "本文研究了现代立体视觉系统的几何基础，重点关注3D结构和受人类启发感知如何有助于准确的深度重建。我们重新审视了独眼视觉模型，并提出了新的几何约束，以解决遮挡和深度不连续性。我们的分析包括评估源自深度学习模型的立体特征匹配质量，以及注意力机制在恢复有意义的3D表面中的作用。通过理论洞察和对真实数据集的实证研究，我们证明了将强大的几何先验与学习特征相结合，为理解立体视觉系统提供了内部抽象。", "summary": "本文探讨了独眼视觉模型在现代立体视觉系统中的几何基础，旨在提高深度重建的准确性。研究提出了新的几何约束以处理遮挡和深度不连续，并评估了深度学习特征匹配质量和注意力机制的作用。通过理论和实证研究，论文证明了结合几何先验与学习特征对于理解立体视觉系统的重要性。", "keywords": "立体视觉, 独眼视觉, 深度重建, 几何先验, 深度学习", "comments": "这篇论文通过重新审视经典的独眼视觉模型并将其与现代深度学习技术相结合，提出了一种新颖的方法来处理立体视觉中的深度重建问题。其创新点在于将几何先验知识与学习到的特征相结合，这对于提升遮挡和深度不连续性区域的重建精度具有重要意义。"}}
{"id": "2506.21307", "title": "Guarding Offices with Maximum Dispersion", "authors": ["Sándor P. Fekete", "Kai Kobbe", "Dominik Krupke", "Joseph S. B. Mitchell", "Christian Rieck", "Christian Scheffer"], "summary": "We investigate the Dispersive Art Gallery Problem with vertex guards and\nrectangular visibility ($r$-visibility) for a class of orthogonal polygons that\nreflect the properties of real-world floor plans: these office-like polygons\nconsist of rectangular rooms and corridors. In the dispersive variant of the\nArt Gallery Problem, the objective is not to minimize the number of guards but\nto maximize the minimum geodesic $L_1$-distance between any two guards, called\nthe dispersion distance.\n  Our main contributions are as follows. We prove that determining whether a\nvertex guard set can achieve a dispersion distance of $4$ in office-like\npolygons is NP-complete, where vertices of the polygon are restricted to\ninteger coordinates. Additionally, we present a simple worst-case optimal\nalgorithm that guarantees a dispersion distance of $3$ in polynomial time. Our\ncomplexity result extends to polyominoes, resolving an open question posed by\nRieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be\nrational, we establish analogous results, proving that achieving a dispersion\ndistance of $2+\\varepsilon$ is NP-hard for any $\\varepsilon > 0$, while the\nclassic Art Gallery Problem remains solvable in polynomial time for this class\nof polygons. Furthermore, we give a straightforward polynomial-time algorithm\nthat computes worst-case optimal solutions with a dispersion distance of $2$.\n  On the other hand, for the more restricted class of hole-free independent\noffice-like polygons, we propose a dynamic programming approach that computes\noptimal solutions. Moreover, we demonstrate that the problem is practically\ntractable for arbitrary orthogonal polygons. To this end, we compare solvers\nbased on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently\ncompute optimal solutions for randomly generated instances with up to $1600$\nvertices in under $15$s.", "comment": "40 pages, 29 figures, to appear in the proceedings 50th International\n  Symposium on Mathematical Foundations of Computer Science (MFCS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.21307v1", "categories": ["cs.CG", "cs.DS", "F.2.2"], "cate": "cs.CG", "url": "http://arxiv.org/abs/2506.21307v1", "AI": {"title_translation": "以最大离散度守卫办公室", "tldr": "本文研究了办公室状正交多边形上的分散艺术画廊问题，旨在最大化守卫之间的最小测地L1距离，并提供了复杂性结果和算法。", "motivation": "本文旨在研究顶点守卫和矩形可见性（r-可见性）下分散艺术画廊问题，针对反映真实世界平面图特性的办公室状正交多边形，目标是最大化任意两个守卫之间的最小测地L1距离（称为分散距离），而非最小化守卫数量。", "method": "研究方法包括：证明了在某些条件下（如顶点为整数坐标）确定分散距离为4是NP完全的；提出了一种保证分散距离为3的多项式时间最坏情况最优算法；证明了当顶点坐标为有理数时，实现2+ε的分散距离是NP难的；提出了一种计算分散距离为2的最坏情况最优解的简单多项式时间算法；对于无孔独立办公室状多边形，提出了一种动态规划方法计算最优解；比较了基于SAT、CP和MIP公式的求解器，以评估实际可处理性。", "result": "主要结果包括：证明了在办公室状多边形中，当顶点坐标限制为整数时，确定顶点守卫集是否能实现4的分散距离是NP完全的；提出了一种能在多项式时间内保证3的分散距离的简单最坏情况最优算法；证明了当顶点坐标允许为有理数时，实现2+ε的分散距离对任何ε > 0都是NP难的；给出了一个计算分散距离为2的最坏情况最优解的直接多项式时间算法；对于无孔独立办公室状多边形，动态规划方法可以计算最优解；SAT求解器能够高效地计算随机生成实例的最优解，对于多达1600个顶点的实例，可在15秒内完成。", "conclusion": "本文全面分析了办公室状多边形上的分散艺术画廊问题，确立了其复杂性结果，并开发了高效算法，同时通过比较不同求解器证明了在某些情况下问题的实际可处理性。", "translation": "我们研究了顶点守卫和矩形可见性（r-可见性）下的分散艺术画廊问题，针对一类反映真实世界平面图特性的正交多边形：这些办公室状多边形由矩形房间和走廊组成。在分散艺术画廊问题中，目标不是最小化守卫的数量，而是最大化任意两个守卫之间的最小测地L1距离，这被称为分散距离。\n我们的主要贡献如下。我们证明了在办公室状多边形中，当多边形的顶点限制为整数坐标时，确定顶点守卫集是否能实现4的分散距离是NP完全的。此外，我们提出了一种简单的最坏情况最优算法，该算法能在多项式时间内保证3的分散距离。我们的复杂性结果扩展到多连块，解决了Rieck和Scheffer（CGTA 2024）提出的一个开放问题。当顶点坐标允许为有理数时，我们建立了类似的结论，证明了对于任何ε > 0，实现2+ε的分散距离是NP难的，而对于这类多边形，经典的艺术画廊问题仍然可以在多项式时间内解决。此外，我们给出了一种直接的多项式时间算法，可以计算分散距离为2的最坏情况最优解。\n另一方面，对于更受限制的无孔独立办公室状多边形，我们提出了一种动态规划方法来计算最优解。此外，我们证明了该问题对于任意正交多边形在实践中是可处理的。为此，我们比较了基于SAT、CP和MIP公式的求解器。值得注意的是，SAT求解器能够高效地计算随机生成实例的最优解，对于多达1600个顶点的实例，可在15秒内完成。", "summary": "本文研究了办公室状正交多边形中的分散艺术画廊问题，其目标是最大化顶点守卫之间的最小L1测地距离。研究证明了在特定条件下（如整数或有理坐标）实现某些分散距离（如4和2+ε）是NP完全或NP难的，并提出了能在多项式时间内保证分散距离为3和2的最坏情况最优算法。此外，对于更受限的无孔独立办公室状多边形，提出了动态规划方法，并通过实验证明了SAT求解器在处理大规模实例时的有效性。", "keywords": "分散艺术画廊问题,正交多边形,NP完全性,算法,守卫问题", "comments": "本文创新性地将分散艺术画廊问题应用于更贴近实际的办公室状多边形，并考虑了矩形可见性。其重要性在于不仅提供了理论上的复杂性分析（NP完全/难），还开发了实用的多项式时间算法，并证明了对于某些大规模实例，通过SAT求解器能够高效地找到最优解，这结合了理论深度与实践价值。解决了Rieck和Scheffer提出的开放问题也增加了其学术贡献。"}}
{"id": "2506.21207", "title": "Estimation of superconducting cavity bandwidth and detuning using a Luenberger observer", "authors": ["Bozo Richter", "Andrea Bellandi", "Julien Branlard", "Leon Speidel", "Annika Eichler"], "summary": "Enabled by progress in superconducting technology, several continuous wave\nlinear accelerators are foreseen in the next decade. For these machines, it is\nof crucial importance to track the main cavity parameters, such as the\nresonator bandwidth and detuning. The bandwidth yields information on the\nsuperconducting state of the cavity. The detuning should be minimized to limit\nthe required power to operate the cavity. The estimation of these parameters is\ncommonly implemented in the digital electronics of the Low-Level RF control\nsystem to minimize the computation delay. In this proceeding, we present a way\nto compute the bandwidth and detuning using a Luenberger observer. In contrast\nto previous methods, a state observer yields estimations at the native control\nsystem sample rate without explicitly filtering the input signals.\nAdditionally, the error convergence properties of the estimations can be\ncontrolled intuitively by adjusting gain parameters. Implementation\nconsiderations and test results on the derived observer are presented in the\nmanuscript.", "comment": "10 pages, 4 figures, to be published in APS Physical Review -\n  Accelerator and Beams", "pdf_url": "http://arxiv.org/pdf/2506.21207v1", "categories": ["physics.acc-ph", "cs.SY", "eess.SY"], "cate": "physics.acc-ph", "url": "http://arxiv.org/abs/2506.21207v1", "AI": {"title_translation": "使用Luenberger观测器估计超导腔带宽和失谐", "tldr": "本文提出使用Luenberger观测器估计超导腔的带宽和失谐，该方法无需显式滤波，并可直观控制误差收敛。", "motivation": "未来的连续波直线加速器需要精确跟踪超导腔的带宽和失谐参数，以了解超导状态并最小化运行功耗。", "method": "本文提出使用Luenberger观测器来计算超导腔的带宽和失谐。", "result": "该方法能够在控制系统原始采样率下提供估计值，无需显式滤波输入信号，并且可以通过调整增益参数直观地控制估计的误差收敛特性。论文中还介绍了实现考虑事项和测试结果。", "conclusion": "Luenberger观测器为超导腔带宽和失谐的估计提供了一种有效且可控的新方法，优于传统方法，有望提高直线加速器的运行效率和稳定性。", "translation": "借助于超导技术的进步，未来十年预计将出现数个连续波直线加速器。对于这些机器来说，跟踪主要腔体参数，如谐振器带宽和失谐，至关重要。带宽提供了腔体超导状态的信息。失谐应最小化以限制操作腔体所需的功率。这些参数的估计通常在低电平射频控制系统的数字电子设备中实现，以最小化计算延迟。在本论文中，我们提出了一种使用Luenberger观测器计算带宽和失谐的方法。与以前的方法相比，状态观测器能够在原始控制系统采样率下提供估计，而无需显式滤波输入信号。此外，通过调整增益参数，可以直观地控制估计的误差收敛特性。手稿中介绍了派生观测器的实现考虑和测试结果。", "summary": "本文介绍了一种利用Luenberger观测器估算超导腔带宽和失谐的新方法。该方法旨在满足未来连续波直线加速器对精确参数跟踪的需求，以优化腔体性能。与现有技术相比，Luenberger观测器无需对输入信号进行显式滤波，即可在控制系统的原始采样率下提供估计，并且其误差收敛特性可以通过增益参数直观地调整。论文中还讨论了该观测器的实现细节和测试结果。", "keywords": "超导腔, 带宽, 失谐, Luenberger观测器, 参数估计", "comments": "这篇论文的创新点在于引入Luenberger观测器来解决超导腔参数估计问题，其优势在于无需显式滤波和可控的误差收敛性，这对于实时控制系统来说非常重要。这种方法有望提高未来直线加速器的运行效率和稳定性。"}}
{"id": "2506.21325", "title": "Localization-Based Beam Focusing in Near-Field Communications", "authors": ["Nima Mozaffarikhosravi", "Prathapasinghe Dharmawansa", "Italo Atzeni"], "summary": "Shifting 6G-and-beyond wireless communication systems to higher frequency\nbands and the utilization of massive multiple-input multiple-output arrays will\nextend the near-field region, affecting beamforming and user localization\nschemes. In this paper, we propose a localization-based beam-focusing strategy\nthat leverages the dominant line-of-sight (LoS) propagation arising at mmWave\nand sub-THz frequencies. To support this approach, we analyze the 2D-MUSIC\nalgorithm for distance estimation by examining its spectrum in simplified,\ntractable setups with minimal numbers of antennas and users. Lastly, we compare\nthe proposed localization-based beam focusing, with locations estimated via\n2D-MUSIC, with zero forcing with pilot-based channel estimation in terms of\nuplink sum spectral efficiency. Our numerical results show that the proposed\nmethod becomes more effective under LoS-dominated propagation, short coherence\nblocks, and strong noise power arising at high carrier frequencies and with\nlarge bandwidths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21325v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21325v1", "AI": {"title_translation": "近场通信中基于定位的波束聚焦", "tldr": "针对6G及以上系统近场区域扩展对波束赋形和用户定位方案的影响，本文提出一种基于定位的波束聚焦策略，该策略利用毫米波和亚太赫兹频率下主导的视距传播，并通过分析2D-MUSIC算法进行距离估计。数值结果表明，该方法在视距主导传播、短相干块和高噪声功率条件下更有效。", "motivation": "6G及未来无线通信系统向更高频段发展以及大规模MIMO阵列的应用，将扩展近场区域，进而影响波束赋形和用户定位方案。", "method": "提出一种基于定位的波束聚焦策略，利用毫米波和亚太赫兹频率下主导的视距(LoS)传播。为支持此方法，分析了2D-MUSIC算法在简化设置下的距离估计性能。最后，将该方法（通过2D-MUSIC估计位置）与基于导频信道估计的零迫使方法在上行链路总频谱效率方面进行比较。", "result": "数值结果表明，所提出的方法在视距主导传播、短相干块以及高载波频率和大带宽下出现的强噪声功率条件下变得更有效。", "conclusion": "所提出的基于定位的波束聚焦策略在特定高频通信场景下，尤其是在视距传播和高噪声环境下，表现出优于传统方法的性能，为未来6G及以上系统的波束赋形提供了新思路。", "translation": "随着6G及更高版本无线通信系统转向更高频段以及大规模多输入多输出阵列的利用，近场区域将扩大，影响波束赋形和用户定位方案。本文提出一种基于定位的波束聚焦策略，该策略利用毫米波和亚太赫兹频率下主导的视距（LoS）传播。为了支持这种方法，我们通过在天线和用户数量最少、可处理的简化设置中检查2D-MUSIC算法的频谱，分析了其距离估计能力。最后，我们比较了所提出的基于定位的波束聚焦（通过2D-MUSIC估计位置）与基于导频信道估计的零迫使方法在上行链路总频谱效率方面的性能。我们的数值结果表明，所提出的方法在视距主导传播、短相干块以及高载波频率和大带宽下出现的强噪声功率条件下变得更有效。", "summary": "本文针对6G及未来通信中近场区域扩展对波束赋形的影响，提出了一种基于定位的波束聚焦策略。该策略利用毫米波和亚太赫兹频段的视距传播特性，并通过分析2D-MUSIC算法进行距离估计。研究将该方法与传统零迫使方法进行比较，结果表明其在视距主导、短相干块及高噪声功率等条件下具有更高的频谱效率。", "keywords": "近场通信, 波束聚焦, 定位, 2D-MUSIC, 6G, 毫米波, 视距传播", "comments": "这篇论文提出了一种新颖的基于定位的波束聚焦方法，特别适用于6G及以上系统在更高频段（毫米波和亚太赫兹）下近场通信的挑战。其创新点在于将用户定位与波束赋形紧密结合，并利用了高频段特有的LoS传播优势。通过分析2D-MUSIC算法并进行性能比较，研究验证了该方法在特定高噪声和LoS主导环境下的有效性，这对于未来无线通信系统的设计具有重要指导意义。"}}
{"id": "2506.21185", "title": "Out-of-Distribution Semantic Occupancy Prediction", "authors": ["Yuheng Zhang", "Mengfei Duan", "Kunyu Peng", "Yuhang Wang", "Ruiping Liu", "Fei Teng", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "summary": "3D Semantic Occupancy Prediction is crucial for autonomous driving, providing\na dense, semantically rich environmental representation. However, existing\nmethods focus on in-distribution scenes, making them susceptible to\nOut-of-Distribution (OoD) objects and long-tail distributions, which increases\nthe risk of undetected anomalies and misinterpretations, posing safety hazards.\nTo address these challenges, we introduce Out-of-Distribution Semantic\nOccupancy Prediction, targeting OoD detection in 3D voxel space. To fill the\ngaps in the dataset, we propose a Synthetic Anomaly Integration Pipeline that\ninjects synthetic anomalies while preserving realistic spatial and occlusion\npatterns, enabling the creation of two datasets: VAA-KITTI and VAA-KITTI-360.\nWe introduce OccOoD, a novel framework integrating OoD detection into 3D\nsemantic occupancy prediction, with Voxel-BEV Progressive Fusion (VBPF)\nleveraging an RWKV-based branch to enhance OoD detection via geometry-semantic\nfusion. Experimental results demonstrate that OccOoD achieves state-of-the-art\nOoD detection with an AuROC of 67.34% and an AuPRCr of 29.21% within a 1.2m\nregion, while maintaining competitive occupancy prediction performance. The\nestablished datasets and source code will be made publicly available at\nhttps://github.com/7uHeng/OccOoD.", "comment": "The established datasets and source code will be made publicly\n  available at https://github.com/7uHeng/OccOoD", "pdf_url": "http://arxiv.org/pdf/2506.21185v1", "categories": ["cs.CV", "cs.RO", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21185v1", "AI": {"title_translation": "域外语义占据预测", "tldr": "本文提出了一种新的框架OccOoD，用于在3D体素空间中进行域外（OoD）语义占据预测。通过引入合成异常数据生成流程和Voxel-BEV渐进式融合（VBPF）模块，OccOoD在OoD检测方面达到了最先进水平，同时保持了良好的占据预测性能，对于提升自动驾驶的安全性至关重要。", "motivation": "现有3D语义占据预测方法主要关注域内场景，对域外（OoD）对象和长尾分布敏感，这增加了未检测到异常和误解的风险，从而构成安全隐患。", "method": "1. 提出“合成异常整合流程”（Synthetic Anomaly Integration Pipeline），通过注入合成异常来创建数据集VAA-KITTI和VAA-KITTI-360，以弥补数据集中的空白。2. 引入OccOoD框架，将OoD检测集成到3D语义占据预测中，其中包含Voxel-BEV渐进式融合（VBPF）模块，该模块利用基于RWKV的分支通过几何-语义融合增强OoD检测。", "result": "OccOoD在1.2米区域内实现了最先进的OoD检测性能，AuROC为67.34%，AuPRCr为29.21%，同时保持了有竞争力的占据预测性能。", "conclusion": "该研究成功解决了3D语义占据预测中域外对象的检测挑战，通过新颖的OccOoD框架和合成数据生成方法，显著提升了自动驾驶环境感知的安全性，并为未来的研究提供了公开的数据集和源代码。", "translation": "3D语义占据预测对于自动驾驶至关重要，它提供了一种密集、语义丰富的环境表示。然而，现有方法主要关注域内场景，使其容易受到域外（OoD）对象和长尾分布的影响，这增加了未检测到异常和误解的风险，从而构成安全隐患。为了解决这些挑战，我们引入了域外语义占据预测，旨在3D体素空间中进行OoD检测。为了弥补数据集的不足，我们提出了一个合成异常整合流程，该流程注入合成异常，同时保留真实的G空间和遮挡模式，从而能够创建两个数据集：VAA-KITTI和VAA-KITTI-360。我们引入了OccOoD，一个将OoD检测集成到3D语义占据预测中的新型框架，其中Voxel-BEV渐进式融合（VBPF）模块利用基于RWKV的分支，通过几何-语义融合增强OoD检测。实验结果表明，OccOoD在1.2米区域内实现了最先进的OoD检测，AuROC为67.34%，AuPRCr为29.21%，同时保持了有竞争力的占据预测性能。所建立的数据集和源代码将在https://github.com/7uHeng/OccOoD公开可用。", "summary": "本文针对自动驾驶中3D语义占据预测现有方法对域外（OoD）对象识别不足的问题，提出了“域外语义占据预测”的概念。研究引入了“合成异常整合流程”以创建包含合成异常的数据集VAA-KITTI和VAA-KITTI-360，并提出了OccOoD框架，该框架将OoD检测融入3D语义占据预测，通过Voxel-BEV渐进式融合（VBPF）模块利用几何-语义融合增强OoD检测能力。实验证明，OccOoD在OoD检测方面达到了最先进水平，同时保持了良好的占据预测性能，为自动驾驶系统的安全性提供了重要保障。", "keywords": "语义占据预测, 域外检测, 自动驾驶, 合成数据, OccOoD", "comments": "这篇论文通过引入OoD检测的概念，解决了自动驾驶中3D语义占据预测对未知或异常物体识别不足的关键安全问题。其创新点在于提出了合成异常数据生成流程来弥补真实世界数据稀缺的挑战，以及设计了集成了OoD检测的OccOoD框架，特别是VBPF模块利用几何-语义融合提升检测精度。这对于提升自动驾驶系统的鲁棒性和安全性具有重要意义。"}}
{"id": "2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20993v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20993v1", "AI": {"title_translation": "SAC：一种用于测量和诱导LLM中人格特质并动态控制其强度的方法框架", "tldr": "本文提出SAC框架，通过扩展MPI并结合16PF模型，实现了对大型语言模型（LLMs）人格特质的精细化测量和动态强度控制，发现连续强度建模比二元开关更有效。", "motivation": "现有LLM人格模型主要基于“大五”人格框架，维度粗糙且缺乏对特质强度的控制机制。为了满足LLM在交互中展现类人个性的日益增长的期望，需要解决这些局限性。", "method": "作者通过将原使用“大五”模型的机器个性清单（MPI）扩展至包含16种人格因素（16PF）模型，从而实现对十六种不同特质的表达控制。同时，开发了一个名为“特定属性控制”（SAC）的结构化框架，用于评估和动态诱导LLM的特质强度。该方法引入了基于形容词的语义锚定来指导特质强度表达，并利用了跨越五个强度因素（频率、深度、阈值、努力和意愿）的行为问题。", "result": "实验发现，将强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，观察到目标特质强度的变化会系统性地影响密切相关的特质，方向符合心理学上的连贯性，这表明LLM内化了多维度的人格结构，而不是孤立地处理特质。", "conclusion": "这项工作为医疗保健、教育和面试等领域中受控且细致的人机交互开辟了新途径，使我们更接近真正类人的社交机器。", "translation": "近年来，大型语言模型（LLM）在广泛领域获得了显著关注。人们也越来越期望它们在交互中展现出类人个性。为了满足这一期望，许多研究提出了通过心理测量评估来建模LLM个性化。然而，大多数现有模型面临两大局限性：它们依赖于“大五”（OCEAN）框架，该框架仅提供粗略的个性维度；并且它们缺乏控制特质强度的机制。在本文中，我们通过扩展最初使用“大五”模型的机器个性清单（MPI），以纳入16种人格因素（16PF）模型，从而解决这一空白，实现对十六种不同特质的表达控制。我们还开发了一个名为“特定属性控制”（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。我们的方法引入了基于形容词的语义锚定来指导特质强度表达，并利用了跨越五个强度因素：频率、深度、阈值、努力和意愿的行为问题。通过实验，我们发现将强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，我们观察到目标特质强度的变化会系统性地影响密切相关的特质，方向符合心理学上的连贯性，这表明LLM内化了多维度的人格结构，而不是孤立地处理特质。我们的工作为医疗保健、教育和面试等领域中受控且细致的人机交互开辟了新途径，使我们更接近真正类人的社交机器。", "summary": "本文提出了SAC（特定属性控制）框架，旨在解决现有LLM人格模型在特质维度粗糙和强度控制方面的不足。通过将机器个性清单（MPI）扩展到16种人格因素（16PF）模型，并引入基于形容词的语义锚定和五种强度因素的行为问题，SAC实现了对LLM人格特质的精细化测量和动态强度控制。实验证明，连续的强度建模能提供更一致和可控的个性表达，并且LLM能够内化多维度的人格结构。该研究为实现更细致、类人的人机交互奠定了基础。", "keywords": "LLMs, 人格特质, 动态强度控制, SAC, 16PF", "comments": "这项工作具有显著的创新性，它超越了传统“大五”人格模型的局限性，引入了更细致的16PF模型，并且关键性地解决了人格特质强度动态控制的问题。通过提出SAC框架和连续强度建模，该研究为LLM展现更真实、可控的类人个性提供了有效途径，对于未来人机交互，尤其是在高敏感度领域（如医疗、教育）的应用具有重要意义。其发现LLM能内化多维度人格结构也为LLM的心理建模提供了新的洞察。"}}
{"id": "2506.21242", "title": "Runge--Kutta generalized Convolution Quadrature for sectorial problems", "authors": ["Jing Guo", "Maria Lopez-Fernandez"], "summary": "We study the application of the generalized convolution quadrature (gCQ)\nbased on Runge--Kutta methods to approximate the solution of an important class\nof sectorial problems. The gCQ generalizes Lubich's original convolution\nquadrature (CQ) to variable steps. High-order versions of the gCQ have been\ndeveloped in the last decade, relying on certain Runge--Kutta methods. The\nRunge--Kutta based gCQ has been studied so far in a rather general setting,\nwhich includes applications to boundary integral formulations of wave problems.\nThe available stability and convergence results for these new methods are\nsuboptimal compared to those known for the uniform-step CQ, both in terms of\nconvergence order and regularity requirements of the data. Here we focus on a\nspecial class of sectorial problems and prove that in these important\napplications it is possible to achieve the same order of convergence as for the\noriginal CQ, under the same regularity hypotheses on the data, and for very\ngeneral time meshes. In the particular case of data with some known algebraic\ntype of singularity, we also show how to choose an optimally graded time mesh\nto achieve convergence with maximal order, overcoming the well-known order\nreduction of the original CQ in these situations. An important advantage of the\ngCQ method is that it allows for a fast and memory-efficient implementation. We\ndescribe how the fast and oblivious Runge--Kutta based gCQ can be implemented\nand illustrate our theoretical results with several numerical experiments. The\ncodes implementing the examples in this article are available in [13].", "comment": "35 pages, 26 figures", "pdf_url": "http://arxiv.org/pdf/2506.21242v1", "categories": ["math.NA", "cs.NA", "65R20, 65L06, 65M15, 26A33, 35R11"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21242v1", "AI": {"title_translation": "龙格-库塔广义卷积积分法求解扇形问题", "tldr": "本文研究了基于龙格-库塔方法的广义卷积积分法（gCQ）在求解一类重要扇形问题中的应用，证明了其在特定条件下能达到与原始卷积积分法相同的收敛阶，并提出了处理数据奇异性的优化时间网格策略，同时强调了其快速高效的实现优势。", "motivation": "传统的基于龙格-库塔方法的广义卷积积分法（gCQ）在稳定性与收敛性方面，与均匀步长的原始卷积积分法（CQ）相比次优，尤其在收敛阶和数据正则性要求上存在不足。本文旨在解决这一问题，使其在特定应用中达到与原始CQ相同的性能。", "method": "本文研究了基于龙格-库塔方法的广义卷积积分法（gCQ）来近似求解扇形问题。特别地，针对具有代数类型奇异性的数据，提出了选择最优分级时间网格的方法。", "result": "对于一类特殊的扇形问题，本文证明了在相同的数据正则性假设下，基于Runge--Kutta的gCQ方法在非常通用的时间网格上可以达到与原始CQ相同的收敛阶。此外，对于具有代数类型奇异性的数据，通过选择最优分级时间网格，可以实现最大阶收敛，克服了原始CQ在此类情况下的阶数降低问题。该方法还具有快速和内存高效的实现优势。", "conclusion": "本文证明了基于龙格-库塔的广义卷积积分法（gCQ）在解决特定扇形问题时，可以达到与原始卷积积分法相同的收敛性能，并且能够通过优化时间网格来处理数据奇异性，实现最大阶收敛，同时具有高效的实现优势。", "translation": "我们研究了基于龙格-库塔方法的广义卷积积分法（gCQ）在近似求解一类重要扇形问题中的应用。gCQ将Lubich的原始卷积积分法（CQ）推广到变步长。在过去十年中，依赖于某些龙格-库塔方法，高阶版本的gCQ已经得到发展。迄今为止，基于龙格-库塔的gCQ已在一个相当通用的设置中进行了研究，其中包括应用于波问题的边界积分公式。与已知用于均匀步长CQ的结果相比，这些新方法的可用稳定性和收敛性结果是次优的，无论是在收敛阶还是数据正则性要求方面。在这里，我们专注于一类特殊的扇形问题，并证明在这些重要应用中，在相同的数据正则性假设下，并且对于非常通用的时间网格，可以实现与原始CQ相同的收敛阶。在数据具有某种已知代数类型奇异性的特殊情况下，我们还展示了如何选择最优分级时间网格以实现最大阶收敛，克服了原始CQ在这些情况下的众所周知的阶数降低问题。gCQ方法的一个重要优势是它允许快速和内存高效的实现。我们描述了如何实现基于龙格-库塔的快速且无感知的gCQ，并通过几个数值实验说明了我们的理论结果。本文中实现示例的代码可在[13]中获取。", "summary": "本文研究了基于龙格-库塔方法的广义卷积积分法（gCQ）在解决重要扇形问题中的应用。该方法是Lubich原始卷积积分法（CQ）的变步长泛化。针对现有gCQ在收敛阶和数据正则性方面次优的问题，本文证明了在特定扇形问题中，gCQ在相同数据正则性假设和通用时间网格下，能达到与原始CQ相同的收敛阶。对于具有代数奇异性的数据，本文还展示了如何选择最优分级时间网格以实现最大阶收敛，克服了原始CQ的阶数降低问题。此外，本文强调了gCQ方法快速且内存高效的实现优势，并通过数值实验验证了理论结果。", "keywords": "广义卷积积分, 龙格-库塔方法, 扇形问题, 收敛性, 时间网格", "comments": "本文的创新点在于证明了广义卷积积分法（gCQ）在特定扇形问题中能够克服其在一般设置下收敛性次优的限制，达到与原始卷积积分法（CQ）相同的最优收敛阶。特别是在处理数据奇异性时，通过设计最优分级时间网格来提高收敛阶，解决了原始CQ的阶数降低问题，这对于实际应用具有重要意义。此外，该方法还具有快速和内存高效的实现优势。"}}
{"id": "2506.20944", "title": "E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs", "authors": ["Van-Hoang Phan", "Long-Khanh Pham", "Dang Vu", "Anh-Duy Tran", "Minh-Son Dao"], "summary": "The rapid spread of misinformation in mobile and wireless networks presents\ncritical security challenges. This study introduces a training-free,\nretrieval-based multimodal fact verification system that leverages pretrained\nvision-language models and large language models for credibility assessment. By\ndynamically retrieving and cross-referencing trusted data sources, our approach\nmitigates vulnerabilities of traditional training-based models, such as\nadversarial attacks and data poisoning. Additionally, its lightweight design\nenables seamless edge device integration without extensive on-device\nprocessing. Experiments on two fact-checking benchmarks achieve SOTA results,\nconfirming its effectiveness in misinformation detection and its robustness\nagainst various attack vectors, highlighting its potential to enhance security\nin mobile and wireless communication environments.", "comment": "Accepted to AsiaCCS 2025 @ SCID", "pdf_url": "http://arxiv.org/pdf/2506.20944v1", "categories": ["cs.MM", "cs.CR"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.20944v1", "AI": {"title_translation": "E-FreeM2：基于多模态大语言模型的训练无关多尺度跨模态新闻验证", "tldr": "E-FreeM2是一个无需训练、基于检索的多模态事实核查系统，利用预训练视觉-语言模型和大型语言模型，有效检测虚假信息并抵御攻击，适用于移动和无线网络环境。", "motivation": "移动和无线网络中虚假信息的快速传播带来了严峻的安全挑战，传统基于训练的模型存在对抗性攻击和数据投毒等漏洞。", "method": "本研究提出了一个名为E-FreeM2的无需训练、基于检索的多模态事实核查系统。它利用预训练的视觉-语言模型和大型语言模型进行可信度评估，通过动态检索和交叉引用可信数据源来缓解传统训练模型的漏洞。此外，其轻量级设计支持在边缘设备上无缝集成。", "result": "在两个事实核查基准测试上取得了SOTA（State-of-the-Art）结果，证实了其在虚假信息检测方面的有效性以及对各种攻击向量的鲁棒性。", "conclusion": "E-FreeM2系统能够有效检测虚假信息并抵御攻击，有望增强移动和无线通信环境的安全性。", "translation": "移动和无线网络中虚假信息的快速传播带来了严峻的安全挑战。本研究引入了一种无需训练、基于检索的多模态事实核查系统，该系统利用预训练的视觉-语言模型和大型语言模型进行可信度评估。通过动态检索和交叉引用可信数据源，我们的方法减轻了传统基于训练模型的脆弱性，例如对抗性攻击和数据投毒。此外，其轻量级设计使得无需大量的设备端处理即可实现无缝的边缘设备集成。在两个事实核查基准测试上的实验取得了SOTA结果，证实了其在虚假信息检测方面的有效性以及对各种攻击向量的鲁棒性，突显了其在增强移动和无线通信环境安全性方面的潜力。", "summary": "本研究提出E-FreeM2，一个无需训练、基于检索的多模态事实核查系统，旨在解决移动和无线网络中的虚假信息传播问题。该系统利用预训练的视觉-语言模型和大型语言模型，通过动态检索和交叉引用可信数据源来评估信息可信度，从而规避了传统训练模型面临的对抗性攻击和数据投毒等风险。E-FreeM2设计轻量化，易于在边缘设备上部署。实验结果表明，该系统在事实核查基准测试中取得了最先进的性能，并展现出对多种攻击的鲁棒性，有望显著提升移动和无线通信环境的安全性。", "keywords": "多模态事实核查, 无需训练, 检索式, 虚假信息检测, MLLMs", "comments": "E-FreeM2的创新之处在于其“无需训练”和“基于检索”的范式，这有效规避了传统训练模型易受对抗性攻击和数据投毒的固有缺陷。其轻量级设计也使其能够无缝集成到边缘设备，具有重要的实际应用价值。在当前虚假信息泛滥的背景下，这项研究为提升网络安全和信息可信度提供了新的、高效的解决方案。"}}
{"id": "2506.21182", "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21182v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21182v1", "AI": {"title_translation": "维护 MTEB：迈向嵌入基准的长期可用性和可复现性", "tldr": "本文关注MTEB（大规模文本嵌入基准）的工程维护，以确保其长期可用性和可复现性，通过持续集成管道、设计选择和社区贡献策略实现。", "motivation": "MTEB已成为文本嵌入模型评估的标准平台，但之前的研究主要集中于核心基准方法。本文的动机是解决工程方面的问题，以确保MTEB的持续可复现性和可扩展性。", "method": "本文介绍了维护健壮的持续集成管道的方法，包括验证数据集完整性、自动化测试执行和评估基准结果的泛化性。详细阐述了增强可复现性和可用性的设计选择，并讨论了处理社区贡献以及通过新任务和数据集扩展基准的策略。", "result": "这些工程实践有助于MTEB变得更全面，同时保持质量和领域相关性。", "conclusion": "作者的经验为面临类似挑战（在机器学习评估框架中确保可复现性和可用性）的基准维护者提供了宝贵的见解。", "translation": "大规模文本嵌入基准（MTEB）已成为文本嵌入模型的一个标准评估平台。虽然先前的工作已经建立了核心基准方法，但本文专注于工程方面，以确保MTEB的持续可复现性和可扩展性。我们提出了维护健壮持续集成管道的方法，这些管道验证数据集完整性、自动化测试执行并评估基准结果的泛化性。我们详细介绍了共同增强可复现性和可用性的设计选择。此外，我们讨论了处理社区贡献以及通过新任务和数据集扩展基准的策略。这些工程实践在使MTEB变得更全面、同时保持质量并最终保持与该领域的相关性方面发挥了重要作用。我们的经验为面临在机器学习评估框架中确保可复现性和可用性方面类似挑战的基准维护者提供了宝贵的见解。MTEB存储库可在以下地址获取：https://github.com/embeddings-benchmark/mteb", "summary": "本文聚焦于大规模文本嵌入基准（MTEB）的工程维护，旨在确保其长期可用性和可复现性。作者详细介绍了如何通过建立健壮的持续集成管道来验证数据集、自动化测试并评估结果泛化性。文中还探讨了提升可复现性和可用性的设计选择，以及管理社区贡献和扩展基准的策略。这些工程实践对MTEB的规模化和质量维持至关重要，并为其他基准维护者提供了宝贵经验。", "keywords": "MTEB, 嵌入基准, 可复现性, 可用性, 持续集成", "comments": "本文的创新之处在于其将工程实践（如持续集成）应用于机器学习基准的维护，解决了长期可用性和可复现性这一关键但常被忽视的问题。这对于确保评估结果的可靠性和未来研究的进步至关重要。其重要性在于提供了一套实用的维护策略，对于任何大型、持续演进的机器学习评估平台都具有借鉴意义。"}}
{"id": "2506.21178", "title": "UAIbot: Beginner-friendly web-based simulator for interactive robotics learning and research", "authors": ["Johnata Brayan", "Armando Alves Neto", "Pavel Petrovič", "Gustavo M Freitas", "Vinicius Mariano Gonçalves"], "summary": "This paper presents UAIbot, a free and open-source web-based robotics\nsimulator designed to address the educational and research challenges\nconventional simulation platforms generally face. The Python and JavaScript\ninterfaces of UAIbot enable accessible hands-on learning experiences without\ncumbersome installations. By allowing users to explore fundamental mathematical\nand physical principles interactively, ranging from manipulator kinematics to\npedestrian flow dynamics, UAIbot provides an effective tool for deepening\nstudent understanding, facilitating rapid experimentation, and enhancing\nresearch dissemination.", "comment": "12 pages, 8 figures, submitted to Springer proceedings", "pdf_url": "http://arxiv.org/pdf/2506.21178v1", "categories": ["cs.RO", "68T40", "I.2.9; I.6.3"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21178v1", "AI": {"title_translation": "UAIbot：适用于交互式机器人学习和研究的初学者友好型网络模拟器", "tldr": "UAIbot是一个免费开源的网络机器人模拟器，旨在简化机器人学习和研究，提供无需安装的交互式体验。", "motivation": "解决传统模拟平台在教育和研究中面临的挑战，提供无障碍的动手学习体验，避免繁琐的安装。", "method": "UAIbot是一个免费、开源的网络机器人模拟器，具有Python和JavaScript接口，允许用户交互式探索从机械臂运动学到行人流动力学等基本数学和物理原理。", "result": "促进学生理解，加速实验进程，并增强研究成果的传播。", "conclusion": "UAIbot是一个有效的工具，可以深化学生理解，促进快速实验，并增强研究传播。", "translation": "本文介绍了UAIbot，一个免费开源的基于网络的机器人模拟器，旨在解决传统模拟平台普遍面临的教育和研究挑战。UAIbot的Python和JavaScript接口使得无需繁琐安装即可获得可访问的动手学习体验。通过允许用户交互式探索从机械臂运动学到行人流动力学等基本数学和物理原理，UAIbot为加深学生理解、促进快速实验和增强研究传播提供了有效工具。", "summary": "本文推出了UAIbot，一个免费开源的网络机器人模拟器，旨在克服传统模拟平台的教育和研究障碍。它提供Python和JavaScript接口，无需安装即可进行交互式学习和实验，帮助用户深入理解机器人学原理，加速研究进程。", "keywords": "机器人模拟器, 网络平台, 交互式学习, 开源, 机器人教育", "comments": "UAIbot的创新在于其网络化、开源和无需安装的特性，极大地降低了机器人学习和研究的门槛。这对于普及机器人教育和加速研究进展具有重要意义，尤其适合初学者和资源受限的环境。"}}
{"id": "2506.21536", "title": "PsyLite Technical Report", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21536v1", "categories": ["cs.AI", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21536v1", "AI": {"title_translation": "PsyLite 技术报告", "tldr": "针对现有AI心理咨询模型在安全性、场景处理和轻量化部署方面的不足，本研究提出了PsyLite，一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型，通过两阶段训练和创新性条件RAG，显著提升了专业性和安全性，并实现低内存部署。", "motivation": "现有AI驱动的心理咨询模型在对话安全、详细场景处理和轻量化部署方面存在不足。", "method": "本研究提出了PsyLite，一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型代理。它采用两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化）来增强模型的深度推理、心理咨询和安全对话能力。部署时使用Ollama和Open WebUI，并通过Pipelines创建自定义工作流。此外，设计了一种创新的条件RAG机制，在心理咨询中适时引入相声幽默元素以提升用户体验，并拒绝危险请求以加强对话安全。模型还使用量化技术（GGUF q4_k_m）实现低硬件部署。", "result": "PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中均优于基线模型。尤其在心理咨询专业性方面（CPsyCounE得分提高47.6%）和对话安全方面（安全得分提高2.4%）表现突出。模型通过量化技术仅需5GB内存即可运行，实现了低硬件部署。", "conclusion": "PsyLite为资源受限环境下的心理咨询应用提供了一个可行的解决方案，显著提升了AI心理咨询模型的专业性和安全性，同时实现了轻量化部署。", "translation": "随着数字技术的快速发展，AI驱动的心理咨询已逐渐成为心理健康领域的重要研究方向。然而，现有模型在对话安全、详细场景处理和轻量化部署方面仍存在不足。为解决这些问题，本研究提出了PsyLite，一个基于InternLM2.5-7B-chat基础模型开发的轻量级心理咨询大语言模型代理。通过两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），PsyLite增强了模型的深度推理能力、心理咨询能力和安全对话能力。部署时使用Ollama和Open WebUI，并使用Pipelines创建自定义工作流。创新性地设计了条件RAG机制，在心理咨询过程中适时引入相声幽默元素以提升用户体验，并拒绝危险请求以加强对话安全。评估结果表明，PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中均优于基线模型，尤其在心理咨询专业性（CPsyCounE得分提高47.6%）和对话安全（安全得分提高2.4%）方面表现突出。此外，该模型采用量化技术（GGUF q4_k_m）实现了低硬件部署（5GB内存即可运行），为资源受限环境下的心理咨询应用提供了可行的解决方案。", "summary": "本技术报告介绍了PsyLite，一个基于InternLM2.5-7B-chat的轻量级AI心理咨询大语言模型。针对现有模型在对话安全、场景处理和部署上的不足，PsyLite通过两阶段训练（混合蒸馏微调和ORPO优化）提升了推理、咨询和安全对话能力。其创新的条件RAG机制能适时引入幽默元素并拒绝不安全请求。评估显示，PsyLite在中文通用、心理咨询专业性和对话安全评估中均表现优异，尤其在专业性上提升显著。通过量化技术，该模型仅需5GB内存即可运行，为资源受限环境下的心理咨询应用提供了高效可行的方案。", "keywords": "心理咨询, 大语言模型, 轻量化部署, 对话安全, 条件RAG", "comments": "该论文提出了一种创新的轻量级心理咨询大语言模型PsyLite，其亮点在于结合了两阶段训练策略和条件RAG机制，不仅提升了模型的专业性和安全性，还通过引入幽默元素增强了用户体验。更重要的是，它实现了极低的硬件部署要求，解决了资源受限环境下的实际应用难题，具有重要的实用价值和推广潜力。"}}
{"id": "2506.20686", "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models", "authors": ["Hoa La", "Ahan Gupta", "Alex Morehead", "Jianlin Cheng", "Minjia Zhang"], "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.", "comment": "13 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20686v1", "categories": ["q-bio.BM", "cs.DC", "cs.LG", "cs.PF"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.20686v1", "AI": {"title_translation": "MegaFold: 用于加速蛋白质结构预测模型的系统级优化", "tldr": "MegaFold是一个跨平台系统，通过系统级优化显著加速了AlphaFold3等蛋白质结构预测模型的训练，减少了内存使用并提高了训练效率，同时支持更长的序列长度。", "motivation": "AlphaFold3（AF3）等蛋白质结构预测模型虽然推动了生物分子建模的边界，但其训练成本高昂，存在计算和内存密集型操作、2D注意力机制以及检索增强数据管道等问题，这些都严重阻碍了AF3训练的可扩展性。", "method": "MegaFold通过以下方法解决关键瓶颈：1. 提前缓存以消除检索增强数据管道中的GPU空闲时间。2. 基于Triton的内核，在异构设备上实现内存高效的EvoAttention。3. 对AF3中常见且关键的小型操作进行深度融合。", "result": "在NVIDIA H200和AMD MI250 GPU上的评估表明，MegaFold将AF3训练的峰值内存使用量降低了高达1.23倍，并将每次迭代的训练时间分别提高了高达1.73倍和1.62倍。更重要的是，MegaFold能够训练比PyTorch基线长1.35倍的序列长度而不会出现内存不足，显著提高了现代蛋白质折叠模型的可扩展性。", "conclusion": "MegaFold通过系统级优化，有效解决了蛋白质结构预测模型（如AlphaFold3）训练中的计算和内存瓶颈，显著提升了训练效率和可扩展性，使其能够处理更长的蛋白质序列。", "translation": "蛋白质结构预测模型，如AlphaFold3 (AF3)，通过将科学信息架构变化融入Transformer架构，推动了生物分子建模的前沿。然而，这些进步带来了巨大的系统成本，引入了：计算和内存密集型操作、2D注意力机制以及检索增强数据管道，这些共同阻碍了AF3训练的可扩展性。在这项工作中，我们提出了MegaFold，一个用于加速AF3训练的跨平台系统。MegaFold通过提前缓存来消除检索增强数据管道中的GPU空闲时间、基于Triton的内核在异构设备上实现内存高效的EvoAttention，以及对AF3中常见和关键的小型操作进行深度融合，解决了关键瓶颈。在NVIDIA H200和AMD MI250 GPU上的评估表明，MegaFold将AF3训练的峰值内存使用量降低了高达1.23倍，并将每次迭代的训练时间分别提高了高达1.73倍和1.62倍。更重要的是，MegaFold能够训练比PyTorch基线长1.35倍的序列长度而不会出现内存不足，显著提高了现代蛋白质折叠模型的可扩展性。我们已将代码开源在https://github.com/Supercomputing-System-AI-Lab/MegaFold/。", "summary": "本研究介绍了MegaFold，一个旨在加速AlphaFold3（AF3）等蛋白质结构预测模型训练的跨平台系统。针对AF3训练中计算和内存密集、2D注意力以及检索增强数据管道等瓶颈，MegaFold采用了提前缓存、基于Triton的内存高效内核以及深度融合等优化技术。实验结果表明，MegaFold显著降低了AF3训练的峰值内存使用量，提高了训练速度，并使得模型能够处理更长的蛋白质序列，从而极大地提升了现代蛋白质折叠模型的可扩展性。", "keywords": "蛋白质结构预测, AlphaFold3, 系统优化, MegaFold, 训练加速", "comments": "MegaFold的创新之处在于其系统级的优化方法，针对当前最先进的蛋白质结构预测模型（如AlphaFold3）的训练瓶颈提出了多方面的解决方案。其结合了数据管道优化（提前缓存）、算子优化（Triton内核、深度融合）以及跨平台支持，有效提升了训练效率和内存利用率。这项工作对于推动AI在生物分子领域的应用，特别是加速蛋白质结构预测模型的研发和部署具有重要意义。"}}
{"id": "2506.20911", "title": "FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing", "authors": ["Advait Gupta", "Rishie Raj", "Dang Nguyen", "Tianyi Zhou"], "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A$^*$\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A$^*$\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA$^*$'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A$^*$\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA$^*$ is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20911v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20911v1", "AI": {"title_translation": "FaSTA$^*$: 用于高效多轮图像编辑的快慢工具路径代理与子程序挖掘", "tldr": "FaSTA$^*$是一种结合LLM规划和A$^*$搜索的神经符号代理，通过挖掘和重用子程序，实现多轮图像编辑的成本效益和高效率。", "motivation": "解决复杂的多轮图像编辑任务，这些任务需要识别、修改多个对象，并找到成本效益高的工具路径。现有方法可能效率不高。", "method": "FaSTA$^*$结合了LLM进行快速、高层次的子任务规划，以及A$^*$搜索进行慢速、准确的工具使用和局部搜索。为了降低A$^*$的成本，系统通过LLM对成功的工具路径进行归纳推理，持续提取和优化常用子程序，并将其作为新工具重用。这形成了一种自适应的快慢规划：优先探索高层子程序，失败时才激活低层A$^*$搜索。", "result": "FaSTA$^*$在计算效率上显著优于现有图像编辑方法，同时在成功率方面与最先进的基线保持竞争力。", "conclusion": "FaSTA$^*$通过结合LLM的快速规划和A$^*$的慢速搜索，并通过子程序挖掘和重用，有效地解决了多轮图像编辑任务，实现了高效率和竞争性的成功率。", "translation": "我们开发了一种成本高效的神经符号代理，以解决具有挑战性的多轮图像编辑任务，例如“检测图像中的长凳并将其重新着色为粉红色。此外，为了更清晰的视图移除猫并将墙壁重新着色为黄色。”它结合了大型语言模型（LLM）的快速、高层次子任务规划与每个子任务的慢速、准确、工具使用和局部A$^*$搜索，以找到成本高效的工具路径——一系列对AI工具的调用。为了节省A$^*$在类似子任务上的成本，我们通过LLM对先前成功的工具路径进行归纳推理，持续提取/细化常用子程序，并将其作为新工具在自适应的快慢规划中用于未来的任务，其中首先探索更高级别的子程序，只有当它们失败时，才激活低级别的A$^*$搜索。可重用的符号子程序显著节省了应用于类似图像的相同类型子任务的探索成本，产生了一种类似人类的快慢工具路径代理“FaSTA$^*$”：首先由LLM尝试快速子任务规划，然后是每个子任务的基于规则的子程序选择，预计这可以覆盖大多数任务，而慢速A$^*$搜索仅在新颖和具有挑战性的子任务中触发。通过与最近的图像编辑方法进行比较，我们证明FaSTA$^*$在计算效率上显著更高，同时在成功率方面与最先进的基线保持竞争力。", "summary": "FaSTA$^*$是一个神经符号代理，专为高效多轮图像编辑而设计。它结合了LLM的高级规划和A$^*$的低级搜索，并通过挖掘和重用常用的工具路径子程序来显著提高效率。该方法通过优先使用预定义的子程序，仅在必要时才进行昂贵的A$^*$搜索，从而在保持高成功率的同时大幅降低了计算成本。", "keywords": "多轮图像编辑, 神经符号代理, 工具路径规划, 子程序挖掘, LLM, A$^*$搜索", "comments": "FaSTA$^*$的创新之处在于其“快慢”混合规划策略，以及通过LLM进行子程序挖掘和重用来优化昂贵的A$^*$搜索。这种结合了符号推理和神经模型的混合方法，提高了复杂多轮任务的效率，并展现了在AI代理中学习和重用经验的潜力。"}}
{"id": "2506.20810", "title": "FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs", "authors": ["Shashwat Khandelwal", "Jakoba Petri-Koenig", "Thomas B. Preußer", "Michaela Blott", "Shreejith Shanker"], "summary": "Recurrent neural networks (RNNs), particularly LSTMs, are effective for\ntime-series tasks like sentiment analysis and short-term stock prediction.\nHowever, their computational complexity poses challenges for real-time\ndeployment in resource constrained environments. While FPGAs offer a promising\nplatform for energy-efficient AI acceleration, existing tools mainly target\nfeed-forward networks, and LSTM acceleration typically requires full custom\nimplementation. In this paper, we address this gap by leveraging the\nopen-source and extensible FINN framework to enable the generalized deployment\nof LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open\nNeural Network Exchange (ONNX) specification to model the recurrent nature of\nLSTM computations, enabling support for mixed quantisation within them and\nfunctional verification of LSTM-based models. Furthermore, we introduce custom\ntransformations within the FINN compiler to map the quantised ONNX computation\ngraph to hardware blocks from the HLS kernel library of the FINN compiler and\nVitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM\nmodel for a mid-price stock prediction task using the widely used dataset and\ngenerating a corresponding hardware IP of the model using our flow, targeting\nthe XCZU7EV device. We show that the generated quantised ConvLSTM accelerator\nthrough our flow achieves a balance between performance (latency) and resource\nconsumption, while matching (or bettering) inference accuracy of\nstate-of-the-art models with reduced precision. We believe that the\ngeneralisable nature of the proposed flow will pave the way for\nresource-efficient RNN accelerator designs on FPGAs.", "comment": "9 pages, 6 figures, 5 tables, Accepted for publication in IEEE\n  FPL-2025 (https://2025.fpl.org/)", "pdf_url": "http://arxiv.org/pdf/2506.20810v1", "categories": ["cs.LG", "cs.AI", "cs.AR", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20810v1", "AI": {"title_translation": "FINN-GL: 面向FPGA加速LSTM的广义混合精度扩展", "tldr": "该论文提出FINN-GL，一个基于FINN框架的工具流，用于在FPGA上部署混合精度量化的LSTM，解决了RNN在资源受限环境中实时部署的计算复杂性挑战，并在股票预测任务中展示了性能、资源效率和准确性的平衡。", "motivation": "循环神经网络（RNN），特别是LSTM，在时间序列任务中表现出色，但其计算复杂性阻碍了在资源受限环境中的实时部署。现有FPGA加速工具主要针对前馈网络，LSTM加速通常需要完全定制实现，这构成了研究空白。", "method": "作者利用开源可扩展的FINN框架，通过ONNX规范中的Scan操作符来建模LSTM计算的循环特性，以支持混合精度量化和功能验证。此外，他们还在FINN编译器中引入了自定义转换，将量化的ONNX计算图映射到FINN编译器和Vitis HLS的HLS核库中的硬件块。", "result": "通过使用提出的工具流，为中期股票预测任务训练了一个量化的ConvLSTM模型，并生成了针对XCZU7EV设备的相应硬件IP。结果表明，生成的量化ConvLSTM加速器在性能（延迟）和资源消耗之间取得了平衡，同时在降低精度的前提下，其推理精度与最先进的模型相当或更优。", "conclusion": "本研究提出的广义工具流有望为FPGA上资源高效的RNN加速器设计铺平道路，解决了RNN在资源受限环境下实时部署的挑战。", "translation": "循环神经网络（RNN），特别是长短期记忆网络（LSTM），在情感分析和短期股票预测等时间序列任务中表现出色。然而，它们的计算复杂性给资源受限环境中的实时部署带来了挑战。尽管FPGA为节能AI加速提供了有前景的平台，但现有工具主要针对前馈网络，且LSTM加速通常需要完全定制实现。在本文中，我们通过利用开源和可扩展的FINN框架，弥补了这一空白，实现了LSTM在FPGA上的广义部署。具体来说，我们利用开放神经网络交换（ONNX）规范中的Scan操作符来建模LSTM计算的循环特性，从而支持其中的混合量化和基于LSTM模型的功能验证。此外，我们在FINN编译器中引入了自定义转换，将量化的ONNX计算图映射到FINN编译器和Vitis HLS的HLS核库中的硬件块。我们通过使用广泛使用的数据集训练一个用于中期股票预测任务的量化ConvLSTM模型，并使用我们的流程生成该模型的相应硬件IP，目标设备为XCZU7EV，从而验证了所提出的工具流。我们展示了通过我们的流程生成的量化ConvLSTM加速器在性能（延迟）和资源消耗之间取得了平衡，同时在降低精度的前提下，其推理精度与最先进的模型相当（或更优）。我们相信，所提出流程的通用性将为FPGA上资源高效的RNN加速器设计铺平道路。", "summary": "该论文介绍了FINN-GL，一个基于FINN框架的工具流，旨在解决循环神经网络（如LSTM）在FPGA上实时部署的计算复杂性挑战。它通过利用ONNX的Scan操作符来支持LSTM的混合精度量化，并引入自定义编译器转换将量化图映射到硬件。实验结果表明，该工具流生成的量化ConvLSTM加速器在性能、资源消耗和推理精度之间取得了良好平衡，为FPGA上资源高效的RNN加速器设计提供了通用方法。", "keywords": "FPGA加速, LSTM, 混合精度, FINN框架, 循环神经网络", "comments": "本文的创新点在于将FINN框架扩展到支持复杂循环结构如LSTM的混合精度量化和FPGA部署，通过利用ONNX的Scan操作符和自定义编译器转换，解决了现有工具对RNN支持不足的问题。其重要性在于为资源受限环境中的实时AI应用提供了高效的硬件加速方案，有望推动FPGA在RNN领域的广泛应用。"}}
{"id": "2506.21239", "title": "Exact Time-Varying Turnpikes for Dynamic Operation of District Heating Networks", "authors": ["Max Rose", "Hannes Gernandt", "Timm Faulwasser", "Johannes Schiffer"], "summary": "District heating networks (DHNs) are crucial for decarbonizing the heating\nsector. Yet, their efficient and reliable operation requires the coordination\nof multiple heat producers and the consideration of future demands. Predictive\nand optimization-based control is commonly used to address this task, but\nexisting results for DHNs do not account for time-varying problem aspects.\nSince the turnpike phenomenon can serve as a basis for model predictive control\ndesign and analysis, this paper examines its role in DHN optimization by\nanalyzing the underlying optimal control problem with time-varying prices and\ndemands. That is, we derive conditions for the existence of a unique\ntime-varying singular arc, which constitutes the time varying turnpike, and we\nprovide its closed-form expression. Additionally, we present converse turnpike\nresults showing a exact time-varying case implies strict dissipativity of the\noptimal control problem. A numerical example illustrates our findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21239v1", "categories": ["math.OC", "cs.SY", "eess.SY"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.21239v1", "AI": {"title_translation": "区域供热网络动态运行的精确时变转轨", "tldr": "本文研究了区域供热网络优化中的时变转轨现象，推导了唯一时变奇弧的存在条件和闭合形式表达式，并证明了精确时变转轨意味着最优控制问题的严格耗散性。", "motivation": "区域供热网络对供热部门脱碳至关重要，但其高效可靠运行需协调多个热源并考虑未来需求。现有预测和基于优化的控制方法未考虑时变问题。", "method": "通过分析具有时变价格和需求的最优控制问题，研究转轨现象在区域供热网络优化中的作用，并推导了唯一时变奇弧的存在条件及其闭合形式表达式，还提出了逆转轨结果。", "result": "推导了构成时变转轨的唯一时变奇弧的存在条件及其闭合形式表达式。展示了精确时变转轨情况意味着最优控制问题的严格耗散性。通过数值例子验证了发现。", "conclusion": "通过分析最优控制问题，证明了时变转轨现象在区域供热网络优化中的重要性，并为设计和分析模型预测控制提供了基础。", "translation": "区域供热网络（DHN）对于供热部门的脱碳至关重要。然而，它们的有效和可靠运行需要协调多个热力生产商并考虑未来的需求。预测和基于优化的控制通常用于解决此任务，但现有针对DHN的结果并未考虑时变问题方面。由于转轨现象可以作为模型预测控制设计和分析的基础，本文通过分析具有时变价格和需求的基本最优控制问题，探讨其在DHN优化中的作用。也就是说，我们推导了唯一时变奇弧（构成时变转轨）的存在条件，并提供了其闭合形式表达式。此外，我们提出了逆转轨结果，表明精确时变情况意味着最优控制问题的严格耗散性。一个数值例子说明了我们的发现。", "summary": "本文研究了区域供热网络优化中的时变转轨现象，以应对现有控制方法未能考虑时变问题的问题。作者通过分析具有时变价格和需求的最优控制问题，推导了构成时变转轨的唯一时变奇弧的存在条件及其闭合形式表达式。研究还提出了逆转轨结果，证明了精确时变转轨意味着最优控制问题的严格耗散性，并通过数值例子进行了说明。", "keywords": "区域供热网络, 时变转轨, 最优控制, 模型预测控制, 耗散性", "comments": "本文的创新点在于将转轨现象引入到区域供热网络的优化中，特别是在处理时变价格和需求方面。这为设计更精确、更鲁棒的模型预测控制提供了理论基础，有助于提高区域供热网络的运行效率和可靠性。"}}
{"id": "2506.21375", "title": "Integrating Movable Antennas and Intelligent Reflecting Surfaces for Coverage Enhancement", "authors": ["Ying Gao", "Qingqing Wu", "Weidong Mei", "Guangji Chen", "Wen Chen", "Ziyuan Zheng"], "summary": "This paper investigates an intelligent reflecting surface (IRS)-aided movable\nantenna (MA) system, where multiple IRSs cooperate with a multi-MA base station\nto extend wireless coverage to multiple designated target areas. The objective\nis to maximize the worst-case signal-to-noise ratio (SNR) across all locations\nwithin these areas through joint optimization of MA positions, IRS reflection\ncoefficients, and transmit beamforming. To achieve this while balancing the\nperformance-cost trade-off, we propose three coverage-enhancement schemes: the\narea-adaptive MA-IRS scheme, the area-adaptive MA-staIRS scheme, and the shared\nMA-staIRS scheme, where staIRS denotes static IRSs with reflection coefficients\nconfigured only once during installation. These schemes lead to challenging\nnon-convex optimization problems with implicit objective functions, which are\ndifficult to solve optimally. To address these problems, we propose a general\nalgorithmic framework that can be applied to solve each problem efficiently\nalbeit suboptimally. Simulation results demonstrate that: 1) the proposed\nMA-based schemes consistently outperform their fixed-position antenna\n(FPA)-based counterparts under both area-adaptive and static IRS\nconfigurations, with the area-adaptive MA-IRS scheme achieving the best\nworst-case SNR performance; 2) as transmit antennas are typically far fewer\nthan IRS elements, the area-adaptive MA-staIRS scheme may underperform the\nbaseline FPA scheme with area-adaptive IRSs in terms of the worst-case SNR, but\na modest increase in antenna number can reverse this trend; 3) under a fixed\ntotal cost, the optimal MA-to-IRS-element ratio for the worst-case SNR\nmaximization is empirically found to be proportional to the reciprocal of their\nunit cost ratio.", "comment": "13 pages, 8 figures, submitted to an IEEE journal for possible\n  publication on on May 8, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21375v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21375v1", "AI": {"title_translation": "集成可移动天线和智能反射面以增强覆盖范围", "tldr": "本文研究了IRS辅助的可移动天线系统，通过联合优化MA位置、IRS反射系数和发射波束成形来最大化目标区域的无线覆盖，并提出了三种方案和一种算法框架，仿真结果表明MA方案优于FPA方案。", "motivation": "本研究旨在通过联合优化可移动天线（MA）位置、智能反射面（IRS）反射系数和发射波束成形，最大化多个指定目标区域内的最差情况信号信噪比（SNR），从而扩展无线覆盖。", "method": "提出了三种覆盖增强方案：区域自适应MA-IRS方案、区域自适应MA-staIRS方案和共享MA-staIRS方案（其中staIRS指静态IRS）。针对由此产生的非凸优化问题，设计了一个通用的算法框架来高效地解决，尽管是次优解。", "result": "1) 所提出的基于MA的方案在各种IRS配置下均优于固定位置天线（FPA）方案，其中区域自适应MA-IRS方案实现了最佳的最差情况信噪比性能。2) 当发射天线数量远少于IRS单元时，区域自适应MA-staIRS方案可能不如基线FPA方案与区域自适应IRS的组合，但增加天线数量可改善。3) 在固定总成本下，最差情况信噪比最大化的最优MA与IRS单元比例与它们单位成本比的倒数成正比。", "conclusion": "本文证明了集成可移动天线和智能反射面可以显著增强无线覆盖。通过提出的多种方案和通用的算法框架，论文验证了MA方案相对于FPA方案的性能优势，并提供了关于MA与IRS单元之间成本效益比的实用指导。", "translation": "本文研究了一种智能反射面（IRS）辅助的可移动天线（MA）系统，其中多个IRS与多MA基站协作，将无线覆盖扩展到多个指定目标区域。目标是通过联合优化MA位置、IRS反射系数和发射波束成形，最大化这些区域内所有位置的最差情况信噪比（SNR）。为了在平衡性能与成本权衡的同时实现这一目标，我们提出了三种覆盖增强方案：区域自适应MA-IRS方案、区域自适应MA-staIRS方案和共享MA-staIRS方案，其中staIRS表示反射系数在安装时仅配置一次的静态IRS。这些方案导致具有隐式目标函数的挑战性非凸优化问题，难以找到最优解。为了解决这些问题，我们提出了一种通用的算法框架，可以有效地（尽管是次优地）应用于解决每个问题。仿真结果表明：1）所提出的基于MA的方案在区域自适应和静态IRS配置下均始终优于其基于固定位置天线（FPA）的对应方案，其中区域自适应MA-IRS方案实现了最佳的最差情况信噪比性能；2）由于发射天线通常远少于IRS单元，区域自适应MA-staIRS方案在最差情况信噪比方面可能低于基线FPA方案与区域自适应IRS的组合，但适度增加天线数量可以逆转此趋势；3）在固定总成本下，经验发现最差情况信噪比最大化的最优MA与IRS单元比例与其单位成本比的倒数成正比。", "summary": "本文研究了IRS辅助的可移动天线系统，旨在通过联合优化MA位置、IRS反射系数和发射波束成形来最大化多目标区域的无线覆盖（最差情况信噪比）。为平衡性能与成本，论文提出了三种覆盖增强方案（区域自适应MA-IRS、区域自适应MA-staIRS和共享MA-staIRS）并设计了一个通用的次优算法框架来解决由此产生的非凸优化问题。仿真结果表明，MA方案在覆盖性能上优于FPA方案，特别是区域自适应MA-IRS表现最佳，并揭示了MA与IRS单元的成本效益比例。", "keywords": "可移动天线, 智能反射面, 覆盖增强, 最差情况信噪比, 非凸优化", "comments": "这篇论文的创新点在于将可移动天线与智能反射面相结合，以解决无线覆盖增强问题，并考虑了性能与成本的权衡。提出的三种方案和通用的算法框架为实际部署提供了指导。尤其对成本效益比的分析，为系统设计提供了实用见解。"}}
{"id": "2506.21198", "title": "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation", "authors": ["Yihong Cao", "Jiaming Zhang", "Xu Zheng", "Hao Shi", "Kunyu Peng", "Hang Liu", "Kailun Yang", "Hui Zhang"], "summary": "Panoramic image processing is essential for omni-context perception, yet\nfaces constraints like distortions, perspective occlusions, and limited\nannotations. Previous unsupervised domain adaptation methods transfer knowledge\nfrom labeled pinhole data to unlabeled panoramic images, but they require\naccess to source pinhole data. To address these, we introduce a more practical\ntask, i.e., Source-Free Occlusion-Aware Seamless Segmentation (SFOASS), and\npropose its first solution, called UNconstrained Learning Omni-Context\nKnowledge (UNLOCK). Specifically, UNLOCK includes two key modules: Omni\nPseudo-Labeling Learning and Amodal-Driven Context Learning. While adapting\nwithout relying on source data or target labels, this framework enhances models\nto achieve segmentation with 360{\\deg} viewpoint coverage and occlusion-aware\nreasoning. Furthermore, we benchmark the proposed SFOASS task through both\nreal-to-real and synthetic-to-real adaptation settings. Experimental results\nshow that our source-free method achieves performance comparable to\nsource-dependent methods, yielding state-of-the-art scores of 10.9 in mAAP and\n11.6 in mAP, along with an absolute improvement of +4.3 in mAPQ over the\nsource-only method. All data and code will be made publicly available at\nhttps://github.com/yihong-97/UNLOCK.", "comment": "Accepted to ICCV 2025. All data and code will be made publicly\n  available at https://github.com/yihong-97/UNLOCK", "pdf_url": "http://arxiv.org/pdf/2506.21198v1", "categories": ["cs.CV", "cs.RO", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21198v1", "AI": {"title_translation": "解锁约束：无源遮挡感知无缝分割", "tldr": "本文提出了一种名为SFOASS的新型无源、遮挡感知无缝分割任务，并提供了首个解决方案UNLOCK，该方法在不依赖源数据或目标标签的情况下实现了与有源方法相当的性能。", "motivation": "全景图像处理对于全方位感知至关重要，但面临失真、透视遮挡和注释有限等挑战。现有的无监督域适应方法需要访问源针孔数据，这在实际应用中存在限制。", "method": "本文提出了“无源遮挡感知无缝分割 (SFOASS)”任务及其首个解决方案“无约束学习全方位知识 (UNLOCK)”。UNLOCK包含两个关键模块：全方位伪标签学习和非模态驱动上下文学习。该框架在不依赖源数据或目标标签的情况下，使模型能够实现360度视角的分割和遮挡感知推理。", "result": "实验结果表明，该无源方法实现了与有源方法相当的性能，在mAAP上达到10.9、mAP上达到11.6的最新分数，并且在mAPQ上相对于仅源方法有+4.3的绝对提升。", "conclusion": "本文成功引入并解决了无源遮挡感知无缝分割这一新任务，提出的UNLOCK框架在不依赖源数据或目标标签的情况下，在全景图像分割方面取得了与有源方法相当甚至超越的先进性能，显著提升了全方位感知能力。", "translation": "全景图像处理对于全方位感知至关重要，但面临失真、透视遮挡和注释有限等约束。以前的无监督域适应方法将知识从带标签的针孔数据转移到无标签的全景图像，但它们需要访问源针孔数据。为了解决这些问题，我们引入了一个更实用的任务，即无源遮挡感知无缝分割（SFOASS），并提出了其首个解决方案，名为无约束学习全方位知识（UNLOCK）。具体来说，UNLOCK包括两个关键模块：全方位伪标签学习和非模态驱动上下文学习。在不依赖源数据或目标标签进行适应的同时，该框架增强了模型，以实现360度视角的分割和遮挡感知推理。此外，我们通过真实到真实和合成到真实两种适应设置对所提出的SFOASS任务进行了基准测试。实验结果表明，我们的无源方法实现了与有源方法相当的性能，在mAAP上获得10.9、mAP上获得11.6的最新分数，并且在mAPQ上相对于仅源方法有+4.3的绝对提升。所有数据和代码将在https://github.com/yihong-97/UNLOCK 公开。", "summary": "本文针对全景图像处理中存在的失真、遮挡和标注限制，以及现有域适应方法对源数据的依赖问题，提出了一种新的“无源遮挡感知无缝分割 (SFOASS)”任务。研究者设计了首个解决方案UNLOCK框架，该框架通过“全方位伪标签学习”和“非模态驱动上下文学习”两个模块，实现了在无源数据和无目标标签的情况下，对全景图像进行360度视角覆盖和遮挡感知的分割。实验证明，UNLOCK在性能上与依赖源数据的方法相当，甚至在某些指标上有所超越，达到了最先进水平。", "keywords": "无源域适应, 全景分割, 遮挡感知, 无缝分割, UNLOCK", "comments": "该论文的创新点在于提出了一个更具实用性的“无源”域适应任务，即SFOASS，这解决了传统域适应方法对源数据依赖的限制。UNLOCK框架的设计，特别是其无源适应能力和对遮挡感知的关注，对于实际的全景图像处理和全方位感知具有重要意义。性能与有源方法相当甚至超越，证明了其方法的有效性和先进性。"}}
{"id": "2506.21222", "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21222v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21222v1", "AI": {"title_translation": "通过句法检索增强大型语言模型的自动术语提取", "tldr": "该研究提出了一种利用句法检索增强大型语言模型在少样本设置下进行自动术语提取（ATE）的方法，通过关注句法相似性来选择示例，从而提高了性能。", "motivation": "自动术语提取（ATE）对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLM）在多种NLP任务中取得了显著进展，但其在ATE领域的潜力尚未得到充分探索。", "method": "提出了一种基于检索的提示策略，用于少样本设置下的ATE。该策略根据句法而非语义相似性选择示例。这种句法检索方法与领域无关，并能为捕获术语边界提供更可靠的指导。", "result": "在三个专门的ATE基准测试上的实验表明，句法检索提高了F1分数。研究还分析了查询句子与其检索到的示例之间的词汇重叠如何影响性能。", "conclusion": "这些发现强调了在将大型语言模型应用于术语提取任务时，句法线索的重要性。", "translation": "自动术语提取（ATE）识别领域特定的表达，这些表达对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLM）已显著推动了各种自然语言处理任务，但其在ATE方面的潜力却鲜有研究。我们提出了一种基于检索的提示策略，在少样本设置下，该策略根据句法而非语义相似性选择示例。这种句法检索方法与领域无关，为捕获术语边界提供了更可靠的指导。我们在域内和跨域设置中评估了该方法，分析了查询句子与其检索到的示例之间的词汇重叠如何影响性能。在三个专门的ATE基准测试上的实验表明，句法检索提高了F1分数。这些发现强调了在将LLM应用于术语提取任务时句法线索的重要性。", "summary": "本文提出了一种新颖的少样本自动术语提取（ATE）方法，该方法利用大型语言模型（LLM）并采用句法检索策略。该策略根据句法相似性选择示例，具有领域无关性，并有助于识别术语边界。在三个ATE基准测试上的评估表明，所提出的句法检索显著提高了F1分数，强调了句法线索在将LLM应用于术语提取任务中的关键作用。", "keywords": "自动术语提取, 大型语言模型, 句法检索, 少样本学习, 自然语言处理", "comments": "该论文的创新之处在于提出了一种基于句法而非语义相似性的检索方法，用于在少样本设置下增强大型语言模型的自动术语提取能力。这种方法与领域无关，并且在捕获术语边界方面提供了更可靠的指导，为利用LLM处理结构化语言任务开辟了新的方向。"}}
{"id": "2506.21031", "title": "Large Language Models Acing Chartered Accountancy", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21031v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21031v1", "AI": {"title_translation": "大型语言模型在特许会计领域表现出色", "tldr": "本研究引入了CA-Ben基准测试，用于评估大型语言模型（LLMs）在印度特许会计领域的金融、法律和定量推理能力。结果显示，GPT-4o和Claude 3.5 Sonnet表现突出，但在数值计算和法律解释方面仍存在挑战。", "motivation": "大型语言模型（LLMs）正在显著改变金融实践，但它们在捕捉和应用领域特定金融知识方面的有效性尚不确定。为了填补印度金融领域的关键空白，本研究旨在评估LLMs在特许会计领域的金融、法律和定量推理能力。", "method": "本研究引入了CA-Ben，这是一个专门为评估LLMs金融、法律和定量推理能力而设计的特许会计基准测试。CA-Ben包含来自印度特许会计师协会（ICAI）严格考试的结构化问答数据集，涵盖基础、中级和高级CA课程阶段。研究使用标准化协议评估了六个主流LLMs，包括GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。", "result": "评估结果显示LLMs的性能存在差异。其中，Claude 3.5 Sonnet和GPT-4o表现优于其他模型，尤其是在概念和法律推理方面。然而，在数值计算和法律解释方面出现了显著挑战。", "conclusion": "研究结果强调了当前LLMs的优势和局限性。未来可通过混合推理和检索增强生成方法来改进LLMs，特别是在定量分析和准确的法律解释方面。", "translation": "先进的智能系统，特别是大型语言模型（LLMs），正通过自然语言处理（NLP）的进步显著改变金融实践。然而，这些模型捕捉和应用领域特定金融知识的程度仍不确定。为解决印度广阔金融背景下的一个关键空白，本文引入了CA-Ben，这是一个专门设计用于评估LLMs金融、法律和定量推理能力的特许会计基准测试。CA-Ben包含从印度特许会计师协会（ICAI）进行的严格考试中提取的结构化问答数据集，涵盖了基础、中级和高级CA课程阶段。研究使用标准化协议评估了六个主流LLMs，即GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。结果显示性能存在差异，其中Claude 3.5 Sonnet和GPT-4o表现优于其他模型，尤其是在概念和法律推理方面。在数值计算和法律解释方面出现了显著挑战。研究结果强调了当前LLMs的优势和局限性，并提出了未来通过混合推理和检索增强生成方法进行改进的建议，特别是针对定量分析和准确的法律解释。", "summary": "本研究旨在评估大型语言模型（LLMs）在印度特许会计领域的应用能力。为此，论文提出了CA-Ben基准测试，该测试基于印度特许会计师协会（ICAI）的考试数据。通过对包括GPT-4o和Claude 3.5 Sonnet在内的六个主流LLMs进行评估，研究发现顶级模型在概念和法律推理方面表现出色，但在处理数值计算和法律解释时仍面临挑战。研究强调了当前LLMs的潜力和局限性，并建议未来通过结合混合推理和检索增强生成技术来提升其性能。", "keywords": "大型语言模型, 特许会计, 金融知识, 基准测试, CA-Ben", "comments": "本研究通过引入CA-Ben基准测试，首次系统性地评估了大型语言模型在印度特许会计这一高度专业化领域的表现，填补了现有研究的空白。其创新之处在于利用真实世界的专业考试数据来构建评估体系，使得评估结果更具实际参考价值。研究揭示了当前LLMs在处理复杂金融和法律问题时的能力边界，特别是在数值计算和精确法律解释方面的不足，这为LLM的未来发展指明了方向，即需要更深入的领域知识集成和更强大的推理机制。"}}
{"id": "2506.21306", "title": "On Uniform Weighted Deep Polynomial approximation", "authors": ["Kingsley Yeon", "Steven B. Damelin"], "summary": "It is a classical result in rational approximation theory that certain\nnon-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be\nefficiently approximated using rational functions with root-exponential\nconvergence in terms of degrees of freedom \\cite{Sta, GN}. In contrast,\npolynomial approximations admit only algebraic convergence by Jackson's theorem\n\\cite{Lub2}. Recent work shows that composite polynomial architectures can\nrecover exponential approximation rates even without smoothness \\cite{KY}. In\nthis work, we introduce and analyze a class of weighted deep polynomial\napproximants tailored for functions with asymmetric behavior-growing unbounded\non one side and decaying on the other. By multiplying a learnable deep\npolynomial with a one-sided weight, we capture both local non-smoothness and\nglobal growth. We show numerically that this framework outperforms Taylor,\nChebyshev, and standard deep polynomial approximants, even when all use the\nsame number of parameters. To optimize these approximants in practice, we\npropose a stable graph-based parameterization strategy building on \\cite{Jar}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21306v1", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.NA", "stat.ML"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21306v1", "AI": {"title_translation": "关于均匀加权深度多项式逼近", "tldr": "本文引入并分析了一种针对具有不对称行为函数的加权深度多项式逼近器，即使在参数数量相同的情况下，也显示出优于标准方法的性能。", "motivation": "经典多项式逼近对于非光滑或奇异函数仅能实现代数收敛，而有理函数可实现根指数收敛。虽然近期工作表明复合多项式架构可以恢复指数收敛率，但本文旨在高效逼近具有不对称行为（一侧无界增长，另一侧衰减）和局部非光滑性的函数。", "method": "作者引入并分析了一类加权深度多项式逼近器。通过将可学习的深度多项式乘以单侧权重，以捕获局部非光滑性和全局增长。此外，提出了一种稳定的基于图的参数化策略进行优化。", "result": "数值结果表明，即使在所有方法使用相同数量参数的情况下，该框架也优于泰勒、切比雪夫和标准深度多项式逼近器。", "conclusion": "所提出的加权深度多项式逼近器对于具有不对称行为的函数是有效的，并且比现有方法表现出更好的性能。", "translation": "在有理逼近理论中，有一个经典结果表明，某些非光滑或奇异函数，例如 $|x|$ 和 $x^{1/p}$，可以使用具有根指数收敛的有理函数在自由度方面进行有效逼近 \ncite{Sta, GN}。相比之下，根据杰克逊定理 \ncite{Lub2}，多项式逼近仅允许代数收敛。最近的工作表明，复合多项式架构即使在没有平滑性时也能恢复指数逼近率 \ncite{KY}。在这项工作中，我们引入并分析了一类加权深度多项式逼近器，专为具有不对称行为（在一侧无界增长，在另一侧衰减）的函数量身定制。通过将可学习的深度多项式乘以单侧权重，我们捕获了局部非光滑性和全局增长。我们通过数值证明，即使所有方法使用相同数量的参数，该框架也优于泰勒、切比雪夫和标准深度多项式逼近器。为了在实践中优化这些逼近器，我们提出了一种基于 \ncite{Jar} 的稳定图基参数化策略。", "summary": "本文旨在解决非光滑或奇异函数逼近中传统多项式逼近仅能提供代数收敛的局限性。在深度多项式架构最近取得指数收敛率进展的基础上，作者提出了一类新颖的加权深度多项式逼近器。这些逼近器通过将可学习的深度多项式与单侧权重相结合，能够有效捕获局部非光滑性和全局不对称增长。数值实验表明，即使在参数数量相同的情况下，该新框架也超越了泰勒、切比雪夫和标准深度多项式逼近器的性能。此外，还引入了一种稳定的基于图的参数化策略以实现实际优化。", "keywords": "深度多项式逼近, 加权逼近, 非光滑函数, 不对称函数, 指数收敛", "comments": "本文通过将深度多项式与单侧权重相结合，提出了一种创新方法，有效解决了传统多项式逼近在处理具有不对称行为和非光滑性函数时的局限性。在相同参数数量下获得卓越性能的数值证据突显了其效率和实际应用潜力。"}}
{"id": "2506.21205", "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations", "authors": ["Elia Trevisan", "Khaled A. Mustafa", "Godert Notten", "Xinwei Wang", "Javier Alonso-Mora"], "summary": "Deploying mobile robots safely among humans requires the motion planner to\naccount for the uncertainty in the other agents' predicted trajectories. This\nremains challenging in traditional approaches, especially with arbitrarily\nshaped predictions and real-time constraints. To address these challenges, we\npropose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI),\na motion planner that incorporates uncertain future motions modelled with\npotentially non-Gaussian stochastic predictions. By leveraging MPPI's\ngradient-free nature, we propose a method that efficiently approximates the\njoint Collision Probability (CP) among multiple dynamic obstacles for several\nhundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This\nenables the rejection of samples exceeding a predefined CP threshold or the\nintegration of CP as a weighted objective within the navigation cost function.\nConsequently, DRA-MPPI mitigates the freezing robot problem while enhancing\nsafety. Real-world and simulated experiments with multiple dynamic obstacles\ndemonstrate DRA-MPPI's superior performance compared to state-of-the-art\napproaches, including Scenario-based Model Predictive Control (S-MPC), Frenet\nplanner, and vanilla MPPI.", "comment": "Accepted for presentation at IROS 2025. Submitted Version", "pdf_url": "http://arxiv.org/pdf/2506.21205v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21205v1", "AI": {"title_translation": "动态风险感知MPPI在人群中移动机器人中的应用：通过高效蒙特卡洛近似", "tldr": "本文提出了一种名为DRA-MPPI的运动规划器，通过高效的蒙特卡洛近似计算碰撞概率，使移动机器人在人群中安全导航，并解决了“机器人冻结”问题。", "motivation": "在人群中安全部署移动机器人，需要运动规划器考虑其他代理预测轨迹中的不确定性，这在传统方法中，尤其是在任意形状的预测和实时约束下，仍然具有挑战性。", "method": "提出了一种动态风险感知模型预测路径积分控制（DRA-MPPI）。该方法利用MPPI的无梯度特性，通过蒙特卡洛（MC）方法实时高效地近似多个动态障碍物之间的联合碰撞概率（CP），从而能够拒绝超过预定义CP阈值的样本或将CP作为加权目标整合到导航成本函数中。", "result": "DRA-MPPI缓解了“机器人冻结”问题，同时增强了安全性。在有多个动态障碍物的真实世界和模拟实验中，DRA-MPPI与现有最先进的方法（包括基于场景的模型预测控制（S-MPC）、Frenet规划器和香草MPPI）相比，表现出卓越的性能。", "conclusion": "DRA-MPPI通过高效的蒙特卡洛近似处理不确定性预测和实时约束，为移动机器人在人群中的安全导航提供了一种优越的解决方案，有效提高了安全性和鲁棒性。", "translation": "在人群中安全部署移动机器人，要求运动规划器考虑其他代理预测轨迹中的不确定性。这在传统方法中仍然具有挑战性，尤其是在任意形状的预测和实时约束下。为了解决这些挑战，我们提出了一种动态风险感知模型预测路径积分控制（DRA-MPPI），这是一种运动规划器，它结合了可能具有非高斯随机预测模型的不确定未来运动。通过利用MPPI的无梯度特性，我们提出了一种方法，通过蒙特卡洛（MC）方法实时高效地近似多个动态障碍物在数百条采样轨迹中的联合碰撞概率（CP）。这使得可以拒绝超过预定义CP阈值的样本，或者将CP作为加权目标整合到导航成本函数中。因此，DRA-MPPI缓解了机器人冻结问题，同时增强了安全性。在有多个动态障碍物的真实世界和模拟实验中，DRA-MPPI与现有最先进的方法（包括基于场景的模型预测控制（S-MPC）、Frenet规划器和香草MPPI）相比，表现出卓越的性能。", "summary": "本文提出了一种名为动态风险感知模型预测路径积分控制（DRA-MPPI）的运动规划器，旨在解决移动机器人在人群中安全导航时处理不确定性预测和实时约束的挑战。DRA-MPPI利用蒙特卡洛近似实时计算多障碍物联合碰撞概率，并将其用于样本筛选或成本函数加权，从而有效避免了“机器人冻结”问题并提升了安全性。实验结果表明，DRA-MPPI在复杂动态环境中优于现有先进方法。", "keywords": "移动机器人, 风险感知, MPPI, 蒙特卡洛近似, 碰撞概率", "comments": "这篇论文的创新点在于将蒙特卡洛近似方法高效地融入到MPPI框架中，以处理复杂且不确定的动态障碍物预测，特别是非高斯预测。这种方法有效地解决了传统运动规划器在密集人群中可能出现的“机器人冻结”问题，显著提升了移动机器人的安全性和实用性。其在实时性方面的突破也使其在实际部署中具有很高的应用潜力。"}}
{"id": "2504.15217", "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models", "authors": ["Yatong Bai", "Jonah Casebeer", "Somayeh Sojoudi", "Nicholas J. Bryan"], "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15217v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2504.15217v1", "AI": {"title_translation": "DRAGON：分布奖励优化扩散生成模型", "tldr": "DRAGON是一个灵活的框架，用于通过分布奖励微调媒体生成模型，其性能优于传统的RLHF/DPO，并在各种奖励函数下取得了高胜率，包括基于示范的奖励。", "motivation": "传统的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）方法在优化媒体生成模型方面缺乏灵活性。当前需要一个更通用的框架，能够优化评估单个示例或其分布的奖励函数。", "method": "DRAGON（Distributional RewArds for Generative OptimizatioN）框架被提出，它能够优化评估单个示例或其分布的奖励函数，兼容实例级、实例到分布和分布到分布的奖励。通过选择编码器（如CLAP）和参考示例来创建示范分布，构建新颖的奖励函数。DRAGON收集在线和策略内的生成，对其进行评分以构建正面和负面演示集，并利用两者的对比来最大化奖励。在评估中，该方法使用20种不同的奖励函数微调了一个音频领域的文本到音乐扩散模型，并比较了不同的FAD设置和消融实验。", "result": "在所有20个目标奖励中，DRAGON取得了81.45%的平均胜率。基于示范集的奖励函数能够增强生成效果，并与基于模型的奖励相当。在没有人类偏好注释训练的情况下，DRAGON通过适当的示范集实现了60.95%的人类投票音乐质量胜率。", "conclusion": "DRAGON为设计和优化奖励函数以提高人类感知质量提供了一种新方法，尤其是在不需要显式人类偏好训练的情况下。", "translation": "我们提出了DRAGON（Distributional RewArds for Generative OptimizatioN），这是一个用于微调媒体生成模型以达到预期结果的多功能框架。与传统的基于人类反馈的强化学习（RLHF）或成对偏好方法（如直接偏好优化（DPO））相比，DRAGON更加灵活。它能优化评估单个示例或其分布的奖励函数，使其兼容广泛的实例级、实例到分布和分布到分布奖励。利用这种多功能性，我们通过选择编码器和一组参考示例来创建示范分布，从而构建新颖的奖励函数。当使用跨模态编码器（如CLAP）时，参考示例可以是不同模态的（例如，文本与音频）。然后，DRAGON收集在线和策略内的生成，对它们进行评分以构建一个正面演示集和一个负面集，并利用这两个集合之间的对比来最大化奖励。为了进行评估，我们使用20种不同的奖励函数微调了一个音频领域的文本到音乐扩散模型，包括自定义音乐美学模型、CLAP分数、Vendi多样性和Frechet音频距离（FAD）。我们进一步比较了实例级（每首歌曲）和完整数据集FAD设置，同时消融了多个FAD编码器和参考集。在所有20个目标奖励中，DRAGON取得了81.45%的平均胜率。此外，基于示范集的奖励函数确实增强了生成效果，并且与基于模型的奖励相当。通过适当的示范集，DRAGON在未经人类偏好注释训练的情况下，实现了60.95%的人类投票音乐质量胜率。因此，DRAGON展示了一种设计和优化奖励函数以提高人类感知质量的新方法。声音示例可在https://ml-dragon.github.io/web 获取。", "summary": "DRAGON是一个多功能框架，用于微调媒体生成模型，通过优化评估单个示例或其分布的奖励函数，使其比RLHF或DPO更灵活。它能通过编码器和参考示例构建新颖的奖励函数，并通过对比正负生成集来最大化奖励。在对文本到音乐扩散模型的评估中，DRAGON在20种奖励函数上平均胜率达81.45%，且基于示范集的奖励函数能有效提升生成质量，在无需人类偏好训练的情况下，实现60.95%的人类投票音乐质量胜率。这为设计和优化奖励函数以提升人类感知质量提供了新途径。", "keywords": "扩散模型, 生成优化, 分布奖励, 微调, 媒体生成", "comments": "DRAGON的创新之处在于其对“分布奖励”的概念，使其能够处理更广泛的奖励函数类型，包括跨模态和基于参考分布的奖励，这比传统的RLHF和DPO更具普适性。它在无需人类偏好标注的情况下，通过构建示范集和对比学习实现高质量生成，这对于数据标注成本高昂的媒体生成领域具有重要意义。"}}
{"id": "2506.21036", "title": "An Information-Theoretic Analysis for Federated Learning under Concept Drift", "authors": ["Fu Peng", "Meng Zhang", "Ming Tang"], "summary": "Recent studies in federated learning (FL) commonly train models on static\ndatasets. However, real-world data often arrives as streams with shifting\ndistributions, causing performance degradation known as concept drift. This\npaper analyzes FL performance under concept drift using information theory and\nproposes an algorithm to mitigate the performance degradation. We model concept\ndrift as a Markov chain and introduce the \\emph{Stationary Generalization\nError} to assess a model's capability to capture characteristics of future\nunseen data. Its upper bound is derived using KL divergence and mutual\ninformation. We study three drift patterns (periodic, gradual, and random) and\ntheir impact on FL performance. Inspired by this, we propose an algorithm that\nregularizes the empirical risk minimization approach with KL divergence and\nmutual information, thereby enhancing long-term performance. We also explore\nthe performance-cost tradeoff by identifying a Pareto front. To validate our\napproach, we build an FL testbed using Raspberry Pi4 devices. Experimental\nresults corroborate with theoretical findings, confirming that drift patterns\nsignificantly affect performance. Our method consistently outperforms existing\napproaches for these three patterns, demonstrating its effectiveness in\nadapting concept drift in FL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21036v1", "categories": ["cs.LG", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21036v1", "AI": {"title_translation": "联邦学习中概念漂移的信息论分析", "tldr": "本文使用信息论分析了联邦学习在概念漂移下的性能，并提出了一种基于KL散度和互信息的算法来缓解性能下降，实验证明其优于现有方法。", "motivation": "现有联邦学习研究多基于静态数据集，但真实世界数据常伴随分布变化（概念漂移），导致性能下降。本文旨在分析并解决联邦学习在概念漂移下的性能退化问题。", "method": "本文将概念漂移建模为马尔可夫链，引入“稳态泛化误差”来评估模型捕获未来未见数据的能力，并使用KL散度和互信息推导出其上限。研究了三种漂移模式（周期性、渐进式、随机），并提出一种算法，通过KL散度和互信息对经验风险最小化方法进行正则化。此外，还通过识别帕累托前沿来探索性能-成本权衡。使用Raspberry Pi4设备构建了FL测试平台进行验证。", "result": "实验结果与理论发现一致，证实了漂移模式显著影响性能。所提出的方法在三种漂移模式下均持续优于现有方法。", "conclusion": "本文提出的基于信息论的方法能有效缓解联邦学习在概念漂移下的性能下降，并在实际测试中表现出优越性。", "translation": "最近的联邦学习 (FL) 研究通常在静态数据集上训练模型。然而，现实世界的数据通常以流的形式到达，并伴随着分布变化，这会导致性能下降，即概念漂移。本文使用信息论分析了概念漂移下 FL 的性能，并提出了一种算法来缓解性能下降。我们将概念漂移建模为马尔可夫链，并引入“稳态泛化误差”来评估模型捕获未来未见数据特征的能力。其上限使用 KL 散度和互信息推导得出。我们研究了三种漂移模式（周期性、渐进式和随机）及其对 FL 性能的影响。受此启发，我们提出了一种算法，通过 KL 散度和互信息对经验风险最小化方法进行正则化，从而提高长期性能。我们还通过识别帕累托前沿来探索性能-成本权衡。为了验证我们的方法，我们使用 Raspberry Pi4 设备构建了一个 FL 测试平台。实验结果与理论发现相符，证实了漂移模式显著影响性能。我们的方法在这三种模式下始终优于现有方法，证明了其在 FL 中适应概念漂移的有效性。", "summary": "本文针对联邦学习在概念漂移下性能下降的问题，提出了一种基于信息论的分析框架和相应的算法。通过将概念漂移建模为马尔可夫链，并引入稳态泛化误差，利用KL散度和互信息推导其上界。研究了周期性、渐进式和随机三种漂移模式的影响，并提出一种结合KL散度和互信息的正则化经验风险最小化算法。实验结果表明，该方法在多种漂移模式下均优于现有方法，有效提升了联邦学习在动态环境中的长期性能。", "keywords": "联邦学习, 概念漂移, 信息论, KL散度, 互信息", "comments": "本文的创新点在于首次将信息论引入联邦学习中的概念漂移分析，并提出了基于信息论度量的正则化方法。通过理论分析和实际验证相结合，增强了研究的严谨性。其对性能-成本权衡的探索也具有实际意义。该工作为联邦学习在真实动态环境下的应用提供了新的视角和解决方案，具有重要的理论和实践价值。"}}
{"id": "2506.20922", "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization", "authors": ["Ju-Hyeon Nam", "Dong-Hyun Moon", "Sang-Chul Lee"], "summary": "Image editing techniques have rapidly advanced, facilitating both innovative\nuse cases and malicious manipulation of digital images. Deep learning-based\nmethods have recently achieved high accuracy in pixel-level forgery\nlocalization, yet they frequently struggle with computational overhead and\nlimited representation power, particularly for subtle or complex tampering. In\nthis paper, we propose M2SFormer, a novel Transformer encoder-based framework\ndesigned to overcome these challenges. Unlike approaches that process spatial\nand frequency cues separately, M2SFormer unifies multi-frequency and\nmulti-scale attentions in the skip connection, harnessing global context to\nbetter capture diverse forgery artifacts. Additionally, our framework addresses\nthe loss of fine detail during upsampling by utilizing a global prior map, a\ncurvature metric indicating the difficulty of forgery localization, which then\nguides a difficulty-guided attention module to preserve subtle manipulations\nmore effectively. Extensive experiments on multiple benchmark datasets\ndemonstrate that M2SFormer outperforms existing state-of-the-art models,\noffering superior generalization in detecting and localizing forgeries across\nunseen domains.", "comment": "Accepted in International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20922v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20922v1", "AI": {"title_translation": "M2SFormer: 结合边缘感知难度引导的多光谱多尺度注意力图像伪造定位", "tldr": "M2SFormer是一个新的Transformer框架，通过统一多频率和多尺度注意力并利用难度引导注意力模块，解决了现有图像伪造定位方法计算开销大和表示能力有限的问题，并在多个基准数据集上取得了SOTA性能。", "motivation": "现有的深度学习图像伪造定位方法在像素级定位方面取得了高精度，但常面临计算开销大和表示能力有限的问题，尤其对于细微或复杂的篡改效果不佳。", "method": "本文提出了M2SFormer，一个基于Transformer编码器的新框架。它在跳跃连接中统一了多频率和多尺度注意力，利用全局上下文捕获多样化的伪造痕迹。此外，M2SFormer利用一个全局先验图（曲率度量，指示伪造定位难度）来指导一个难度引导注意力模块，以更有效地保留细微的操作，解决上采样过程中精细细节丢失的问题。", "result": "在多个基准数据集上进行的广泛实验表明，M2SFormer优于现有最先进的模型，在检测和定位未知领域伪造方面表现出卓越的泛化能力。", "conclusion": "M2SFormer通过其独特的多光谱多尺度注意力机制和边缘感知难度引导模块，有效克服了现有图像伪造定位方法的局限性，并在泛化能力和性能上超越了SOTA模型。", "translation": "图像编辑技术发展迅速，既促进了创新的应用，也导致了数字图像的恶意篡改。基于深度学习的方法最近在像素级伪造定位方面取得了高精度，但它们经常面临计算开销大和表示能力有限的问题，特别是对于细微或复杂的篡改。在本文中，我们提出了M2SFormer，一个新颖的基于Transformer编码器的框架，旨在克服这些挑战。与单独处理空间和频率线索的方法不同，M2SFormer在跳跃连接中统一了多频率和多尺度注意力，利用全局上下文更好地捕获多样化的伪造痕迹。此外，我们的框架通过利用全局先验图（一个指示伪造定位难度的曲率度量）来解决上采样过程中精细细节的丢失问题，然后该先验图指导一个难度引导注意力模块，以更有效地保留细微的操作。在多个基准数据集上进行的广泛实验表明，M2SFormer优于现有最先进的模型，在检测和定位未知领域伪造方面表现出卓越的泛化能力。", "summary": "M2SFormer是一个针对图像伪造定位的新型Transformer框架，旨在解决现有方法计算开销大和表示能力有限的问题。它通过在跳跃连接中统一多频率和多尺度注意力来捕获多样化的伪造痕迹，并利用一个难度引导注意力模块（基于全局先验图）来保留细微细节。实验证明M2SFormer在多个基准数据集上优于现有SOTA模型，并具有更强的泛化能力。", "keywords": "图像伪造定位, Transformer, 多尺度注意力, 多频率注意力, 难度引导注意力", "comments": "M2SFormer的创新之处在于其对多频率和多尺度注意力的统一处理，以及引入边缘感知难度引导机制，这有效地提升了模型对复杂和细微伪造的定位能力。其在Transformer框架下的应用，也体现了将先进的注意力机制应用于图像取证领域的潜力。"}}
{"id": "2506.20814", "title": "Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning", "authors": ["Jakub Piwko", "Jędrzej Ruciński", "Dawid Płudowski", "Antoni Zajko", "Patryzja Żak", "Mateusz Zacharecki", "Anna Kozak", "Katarzyna Woźnica"], "summary": "Ensemble learning has proven effective in boosting predictive performance,\nbut traditional methods such as bagging, boosting, and dynamic ensemble\nselection (DES) suffer from high computational cost and limited adaptability to\nheterogeneous data distributions. To address these limitations, we propose\nHellsemble, a novel and interpretable ensemble framework for binary\nclassification that leverages dataset complexity during both training and\ninference. Hellsemble incrementally partitions the dataset into circles of\ndifficulty by iteratively passing misclassified instances from simpler models\nto subsequent ones, forming a committee of specialised base learners. Each\nmodel is trained on increasingly challenging subsets, while a separate router\nmodel learns to assign new instances to the most suitable base model based on\ninferred difficulty. Hellsemble achieves strong classification accuracy while\nmaintaining computational efficiency and interpretability. Experimental results\non OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often\noutperforms classical ensemble methods. Our findings suggest that embracing\ninstance-level difficulty offers a promising direction for constructing\nefficient and robust ensemble systems.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20814v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20814v1", "AI": {"title_translation": "分而治之、专业化与路由：一种高效集成学习的新方法", "tldr": "Hellsemble是一种新的集成学习框架，通过根据数据复杂性划分数据集并训练专业化模型，实现高效、准确且可解释的二元分类，优于传统方法。", "motivation": "传统的集成学习方法（如Bagging、Boosting和DES）存在计算成本高和对异构数据分布适应性有限的问题。", "method": "本文提出了Hellsemble，一种新颖且可解释的二元分类集成框架。它在训练和推理过程中利用数据集复杂性，通过迭代地将简单模型错误分类的实例传递给后续模型，逐步将数据集划分为不同难度的“圈子”，从而形成一个由专业化基础学习器组成的委员会。每个模型都在难度递增的子集上进行训练，同时一个独立的路由模型学习根据推断的难度将新实例分配给最合适的基础模型。", "result": "Hellsemble在保持计算效率和可解释性的同时，实现了强大的分类准确性。在OpenML-CC18和Tabzilla基准测试上的实验结果表明，Hellsemble通常优于经典的集成方法。", "conclusion": "研究结果表明，利用实例级别的难度为构建高效且鲁棒的集成系统提供了一个有前景的方向。", "translation": "集成学习已被证明在提高预测性能方面是有效的，但Bagging、Boosting和动态集成选择（DES）等传统方法存在计算成本高和对异构数据分布适应性有限的问题。为了解决这些限制，我们提出了Hellsemble，一种新颖且可解释的二元分类集成框架，该框架在训练和推理过程中利用数据集复杂性。Hellsemble通过迭代地将简单模型错误分类的实例传递给后续模型，逐步将数据集划分为不同难度的“圈子”，从而形成一个由专业化基础学习器组成的委员会。每个模型都在难度递增的子集上进行训练，同时一个独立的路由模型学习根据推断的难度将新实例分配给最合适的基础模型。Hellsemble在保持计算效率和可解释性的同时，实现了强大的分类准确性。在OpenML-CC18和Tabzilla基准测试上的实验结果表明，Hellsemble通常优于经典的集成方法。我们的研究结果表明，利用实例级别的难度为构建高效且鲁棒的集成系统提供了一个有前景的方向。", "summary": "Hellsemble是一种用于二元分类的新型可解释集成学习框架，旨在解决传统方法计算成本高和适应性差的问题。它通过根据数据复杂性将数据集逐步划分为不同难度的子集，并训练专门的基础学习器委员会，同时使用路由模型将新实例分配给最合适的模型。实验证明，Hellsemble在保持效率和可解释性的同时，在分类准确性方面优于经典集成方法。", "keywords": "集成学习, 二元分类, 复杂性, 路由, 可解释性", "comments": "Hellsemble的创新点在于其“分而治之”的策略，通过实例级难度划分数据并训练专业化模型，以及引入路由机制，这为提高集成学习的效率、准确性和可解释性提供了一个新颖的视角。其对“实例级难度”的利用是值得关注的突破。"}}
{"id": "2506.21265", "title": "Active Disturbance Rejection Control for Trajectory Tracking of a Seagoing USV: Design, Simulation, and Field Experiments", "authors": ["Jelmer van der Saag", "Elia Trevisan", "Wouter Falkena", "Javier Alonso-Mora"], "summary": "Unmanned Surface Vessels (USVs) face significant control challenges due to\nuncertain environmental disturbances like waves and currents. This paper\nproposes a trajectory tracking controller based on Active Disturbance Rejection\nControl (ADRC) implemented on the DUS V2500. A custom simulation incorporating\nrealistic waves and current disturbances is developed to validate the\ncontroller's performance, supported by further validation through field tests\nin the harbour of Scheveningen, the Netherlands, and at sea. Simulation results\ndemonstrate that ADRC significantly reduces cross-track error across all tested\nconditions compared to a baseline PID controller but increases control effort\nand energy consumption. Field trials confirm these findings while revealing a\nfurther increase in energy consumption during sea trials compared to the\nbaseline.", "comment": "Accepted for presentation at IROS 2025. Submitted version", "pdf_url": "http://arxiv.org/pdf/2506.21265v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21265v1", "AI": {"title_translation": "自抗扰控制在远洋无人水面艇轨迹跟踪中的应用：设计、仿真与实地实验", "tldr": "本文提出了一种基于自抗扰控制（ADRC）的无人水面艇（USV）轨迹跟踪控制器，通过仿真和实地实验验证了其在减少横向跟踪误差方面的有效性，但代价是控制工作量和能耗增加。", "motivation": "无人水面艇（USV）在面对波浪和水流等不确定环境扰动时，面临着严峻的控制挑战。", "method": "本文提出了一种基于自抗扰控制（ADRC）的轨迹跟踪控制器，并将其应用于DUS V2500无人水面艇。开发了包含真实波浪和水流扰动的定制仿真平台以验证控制器性能，并通过在荷兰斯赫弗宁根港口和海上进行的实地测试进一步验证。", "result": "仿真结果表明，与基线PID控制器相比，ADRC在所有测试条件下显著减少了横向跟踪误差，但增加了控制工作量和能耗。实地试验证实了这些发现，并揭示了海上试验中能耗相比基线进一步增加。", "conclusion": "自抗扰控制（ADRC）能有效降低无人水面艇在环境扰动下的轨迹跟踪误差，尽管会增加控制工作量和能耗。", "translation": "无人水面艇（USVs）由于波浪和水流等不确定环境扰动而面临严峻的控制挑战。本文提出了一种基于自抗扰控制（ADRC）的轨迹跟踪控制器，并将其应用于DUS V2500。开发了一种包含真实波浪和水流扰动的定制仿真模型，以验证控制器的性能，并通过在荷兰斯赫弗宁根港口和海上进行的实地测试进一步验证。仿真结果表明，与基线PID控制器相比，ADRC在所有测试条件下显著减少了横向跟踪误差，但增加了控制工作量和能耗。实地试验证实了这些发现，同时揭示了海上试验中能耗相比基线进一步增加。", "summary": "本文针对无人水面艇（USV）在环境扰动下的轨迹跟踪难题，提出并验证了一种基于自抗扰控制（ADRC）的控制器。通过定制仿真和实地实验（包括港口和海上测试），研究发现ADRC能有效降低横向跟踪误差，性能优于传统PID控制器。然而，其代价是控制工作量和能源消耗的显著增加。", "keywords": "自抗扰控制, 无人水面艇, 轨迹跟踪, 环境扰动, 实地实验", "comments": "该论文的创新点在于将自抗扰控制（ADRC）应用于受环境扰动影响的无人水面艇轨迹跟踪，并通过仿真和实地实验进行了全面验证。其重要性在于证明了ADRC在复杂海洋环境下的有效性，为USV控制提供了新的思路。主要局限性在于ADRC虽然提高了控制精度，但能耗显著增加，这在实际应用中需要权衡。"}}
{"id": "2506.21349", "title": "Generalizable Neural Electromagnetic Inverse Scattering", "authors": ["Yizhe Cheng", "Chunxun Tian", "Haoru Wang", "Wentao Zhu", "Xiaoxuan Ma", "Yizhou Wang"], "summary": "Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in\napplications such as medical imaging, where the goal is to reconstruct the\nrelative permittivity from scattered electromagnetic field. This inverse\nprocess is inherently ill-posed and highly nonlinear, making it particularly\nchallenging. A recent machine learning-based approach, Img-Interiors, shows\npromising results by leveraging continuous implicit functions. However, it\nrequires case-specific optimization, lacks generalization to unseen data, and\nfails under sparse transmitter setups (e.g., with only one transmitter). To\naddress these limitations, we revisit EISP from a physics-informed perspective,\nreformulating it as a two stage inverse transmission-scattering process. This\nformulation reveals the induced current as a generalizable intermediate\nrepresentation, effectively decoupling the nonlinear scattering process from\nthe ill-posed inverse problem. Built on this insight, we propose the first\ngeneralizable physics-driven framework for EISP, comprising a current estimator\nand a permittivity solver, working in an end-to-end manner. The current\nestimator explicitly learns the induced current as a physical bridge between\nthe incident and scattered field, while the permittivity solver computes the\nrelative permittivity directly from the estimated induced current. This design\nenables data-driven training and generalizable feed-forward prediction of\nrelative permittivity on unseen data while maintaining strong robustness to\ntransmitter sparsity. Extensive experiments show that our method outperforms\nstate-of-the-art approaches in reconstruction accuracy, generalization, and\nrobustness. This work offers a fundamentally new perspective on electromagnetic\ninverse scattering and represents a major step toward cost-effective practical\nsolutions for electromagnetic imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21349v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21349v1", "AI": {"title_translation": "可泛化的神经电磁逆散射", "tldr": "本文提出了一种可泛化的物理驱动框架，用于解决电磁逆散射问题（EISP），通过将问题重新表述为两阶段过程并引入感应电流作为中间表示，显著提高了重建精度、泛化能力和对稀疏发射器设置的鲁棒性。", "motivation": "电磁逆散射问题（EISP）在医学成像等应用中至关重要，但由于其固有的病态性和高度非线性而极具挑战性。现有的机器学习方法（如Img-Interiors）虽然有前景，但存在需要针对特定案例优化、缺乏对未见数据的泛化能力以及在稀疏发射器设置下失效等局限性。", "method": "本文从物理信息角度重新审视EISP，将其重新表述为两阶段的逆透射-散射过程，并发现感应电流是可泛化的中间表示，有效解耦了非线性散射过程和病态逆问题。在此基础上，提出了第一个可泛化的物理驱动EISP框架，包含一个电流估计器和一个介电常数求解器。电流估计器明确学习感应电流作为入射场和散射场之间的物理桥梁，而介电常数求解器直接从估计的感应电流计算相对介电常数。", "result": "广泛的实验表明，该方法在重建精度、泛化能力和鲁棒性方面优于现有最先进的方法。", "conclusion": "这项工作为电磁逆散射提供了一个全新的视角，代表着向经济高效的电磁成像实用解决方案迈出了重要一步。", "translation": "解决电磁逆散射问题（EISP）在医学成像等应用中至关重要，其目标是从散射电磁场中重建相对介电常数。这种逆过程本质上是病态且高度非线性的，使其极具挑战性。最近一种基于机器学习的方法Img-Interiors通过利用连续隐式函数显示出有前景的结果。然而，它需要针对特定案例进行优化，缺乏对未见数据的泛化能力，并且在稀疏发射器设置下（例如，只有一个发射器）会失效。为了解决这些局限性，我们从物理信息角度重新审视EISP，将其重新表述为两阶段的逆透射-散射过程。这种表述揭示了感应电流是一种可泛化的中间表示，有效地将非线性散射过程与病态逆问题解耦。基于这一见解，我们提出了第一个可泛化的物理驱动EISP框架，包括一个电流估计器和一个介电常数求解器，以端到端的方式工作。电流估计器明确地将感应电流学习为入射场和散射场之间的物理桥梁，而介电常数求解器直接从估计的感应电流计算相对介电常数。这种设计使得在未见数据上进行数据驱动训练和可泛化的前向预测相对介电常数成为可能，同时对发射器稀疏性保持强大的鲁棒性。广泛的实验表明，我们的方法在重建精度、泛化能力和鲁棒性方面优于现有最先进的方法。这项工作为电磁逆散射提供了根本性的新视角，代表着向经济高效的电磁成像实用解决方案迈出了重要一步。", "summary": "本文针对电磁逆散射问题（EISP）中现有机器学习方法泛化能力差和稀疏发射器下失效的问题，提出了一个可泛化的物理驱动框架。通过将EISP重新表述为两阶段的逆透射-散射过程，并引入感应电流作为中间表示，该框架有效解耦了非线性散射与病态逆问题。所提出的方法包含一个电流估计器和一个介电常数求解器，实现了对未见数据的泛化预测和对稀疏发射器设置的鲁棒性。实验证明，该方法在重建精度、泛化性和鲁棒性上均优于现有技术，为电磁成像提供了新的解决方案。", "keywords": "电磁逆散射, 泛化, 物理信息, 感应电流, 神经网络", "comments": "这项工作通过引入感应电流作为可泛化的中间表示，并构建两阶段的物理驱动框架，为解决电磁逆散射问题提供了根本性的新视角。其创新点在于将复杂的非线性逆问题有效解耦，显著提升了模型对未见数据的泛化能力和在稀疏测量条件下的鲁棒性，克服了现有数据驱动方法的关键局限性。这对于推动电磁成像在实际应用中的成本效益和可行性具有重要意义。"}}
{"id": "2506.21288", "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21288v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21288v1", "AI": {"title_translation": "小型编码器在检测基础性方面可与大型解码器媲美", "tldr": "研究表明，轻量级编码器模型在检测LLM生成内容的基础性方面，表现可与大型LLM媲美，同时显著降低推理时间和资源消耗。", "motivation": "大型语言模型（LLMs）在外部上下文不足时易产生“幻觉”，即生成无根据的猜测或依赖内部知识，导致事实不一致和不可信。为确保LLMs输出的事实一致性和可信度，并在昂贵的答案生成之前减少推理时间和资源消耗，有必要开发一种机制来检测查询是否基于提供的文档上下文。", "method": "本研究使用RoBERTa和NomicBERT等轻量级、任务专用的编码器模型，在精选数据集上进行微调，以检测给定查询是否基于提供的文档上下文。随后，将这些编码器模型的性能与Llama3 8B和GPT4o等最先进的LLMs在基础性检测任务上的表现进行比较。", "result": "轻量级、任务专用的编码器模型（如RoBERTa和NomicBERT）在基础性检测方面，可以达到与Llama3 8B和GPT4o等最先进的LLMs相当的准确性。同时，这些编码器模型显著降低了几个数量级的推理延迟。", "conclusion": "轻量级编码器模型是检测LLM生成内容基础性的有效且高效的替代方案，它们能够在保持高准确性的同时，显著降低计算成本和推理时间，从而有助于确保LLMs的可靠性。", "translation": "将大型语言模型（LLMs）与外部上下文结合显著提高了它们在自然语言处理（NLP）任务中的性能。然而，当提供的上下文缺乏信息时，LLMs难以可靠地回答查询，经常诉诸于无根据的猜测或内部知识。基础性——即生成严格由上下文支持的响应——对于确保事实一致性和可信度至关重要。本研究侧重于在LLMs进行昂贵的答案生成之前，检测给定查询是否基于所提供的文档上下文。这种检测机制可以显著减少推理时间和资源消耗。我们表明，轻量级、任务专用的编码器模型，如RoBERTa和NomicBERT，在精选数据集上进行微调后，在基础性检测方面可以达到与Llama3 8B和GPT4o等最先进的LLMs相当的准确性，同时将推理延迟降低了几个数量级。代码可在：https://github.com/chandarlab/Hallucinate-less 获取。", "summary": "本研究探讨了在大型语言模型（LLMs）生成答案前，检测其输出是否基于给定上下文的重要性，以解决LLMs在上下文不足时产生幻觉的问题。研究发现，经过精心微调的轻量级编码器模型（如RoBERTa和NomicBERT）在检测基础性方面，能达到与Llama3 8B和GPT4o等大型LLMs相媲美的准确性，同时大幅降低推理时间和资源消耗，为确保LLMs的可靠性提供了一种高效的解决方案。", "keywords": "基础性检测, 大型语言模型, 编码器模型, 推理效率, 幻觉", "comments": "这篇论文的创新点在于证明了小型、任务专用的编码器模型在特定任务（如基础性检测）上可以超越或媲美大型通用LLMs的性能，尤其是在效率方面。这对于资源受限或需要低延迟的应用场景具有重要意义，提供了一种更经济、更快速的LLM辅助方案，有助于解决LLM幻觉问题。"}}
{"id": "2506.21463", "title": "Aligning Spoken Dialogue Models from User Interactions", "authors": ["Anne Wu", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez"], "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21463v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21463v1", "AI": {"title_translation": "从用户交互中校准口语对话模型", "tldr": "提出了一种新颖的偏好对齐框架，通过大规模数据集和离线对齐方法改进实时口语对话模型，使其更真实、安全、上下文对齐。", "motivation": "当前的偏好学习方法主要关注基于文本的语言模型，不适用于实时语音交互的复杂性（如中断、插话、无明确轮次分割）。", "method": "提出了一个新颖的偏好对齐框架；创建了一个包含超过150,000个偏好对的大规模数据集，数据来源于原始多轮语音对话，并用AI反馈进行标注；利用离线对齐方法微调了一个全双工自回归语音到语音模型；进行了广泛的实验和整体人工评估。", "result": "事实证明，对通用对话的反馈可以持续有效地改进口语对话模型，使其产生更真实、更安全、更上下文对齐的交互；发现各种动态之间良好校准的平衡对于自然的实时语音对话系统至关重要。", "conclusion": "论文强调了在实时语音对话系统中，各种动态（如中断、插话）之间良好校准的平衡的重要性，并通过提出的框架和方法有效提升了模型的表现。", "translation": "我们提出了一种新颖的偏好对齐框架，用于通过用户交互改进实时对话中的口语对话模型。当前的偏好学习方法主要关注基于文本的语言模型，不直接适用于实时语音交互的复杂性，后者具有更丰富的动态（例如中断、插话）且说话人轮次之间没有明确的分割。我们从原始多轮语音对话中创建了一个包含超过150,000个偏好对的大规模数据集，并用AI反馈进行标注，以涵盖语言内容和时间上下文变化的偏好。我们利用离线对齐方法微调了一个全双工自回归语音到语音模型。广泛的实验表明，对通用对话的反馈可以持续有效地改进口语对话模型，使其产生更真实、更安全、更上下文对齐的交互。我们部署了微调后的模型并进行了整体人工评估，以评估其在单轮对话之外的影响。我们的发现揭示了各种动态之间良好校准的平衡的重要性，这对于自然的实时语音对话系统至关重要。", "summary": "本文提出了一种新颖的偏好对齐框架，旨在通过用户交互改进实时口语对话模型。针对现有文本偏好学习方法不适用于实时语音复杂性的问题，研究团队构建了一个包含超过15万个偏好对的大规模语音对话数据集，并利用离线对齐技术微调了一个全双工语音到语音模型。实验结果表明，该方法能有效提升模型生成更真实、安全且上下文对齐的交互，并强调了实时语音对话中各种动态平衡的重要性。", "keywords": "语音对话模型, 偏好对齐, 用户交互, 实时对话, 全双工模型", "comments": "这篇论文的创新点在于将偏好对齐技术应用于实时语音对话领域，克服了现有方法主要面向文本的局限性。通过构建大规模语音偏好数据集和微调全双工语音模型，有效提升了实时对话系统的性能，使其更符合人类交互习惯。其对实时语音动态平衡的强调，为未来自然对话系统发展提供了重要见解。"}}
{"id": "2506.21314", "title": "A Sampling-Based Adaptive Rank Approach to the Wigner-Poisson System", "authors": ["Andrew Christlieb", "Sining Gong", "Jing-Mei Qiu", "Nanyi Zheng"], "summary": "We develop a mass-conserving, adaptive-rank solver for the 1D1V\nWigner-Poisson system. Our work is motivated by applications to the study of\nthe stopping power of $\\alpha$ particles at the National Ignition Facility\n(NIF). In this regime, electrons are in a warm dense state, requiring more than\na standard kinetic model. They are hot enough to neglect Pauli exclusion, yet\nquantum enough to require accounting for uncertainty. The Wigner-Poisson system\ncaptures these effects but presents challenges due to its nonlocal nature.\nBased on a second-order Strang splitting method, we first design a full-rank\nsolver with a structure-preserving Fourier update that ensures the intermediate\nsolutions remain real-valued (up to machine precision), improving upon previous\nmethods. Simulations demonstrate that the solutions exhibit a low rank\nstructure for moderate to high dimensionless Planck constants ($H \\ge 0.1$).\nThis observed low rank structure motivates the development of an adaptive-rank\nsolver, built on a Semi-Lagrangian adaptive-rank (SLAR) scheme for advection\nand an adaptive-rank, structure-preserving Fourier update for the Wigner\nintegral terms, with a rigorous proof of structure-preserving property\nprovided. Our solver achieves $O(N)$ complexity in both storage and computation\ntime, while preserving mass and maintaining momentum accuracy up to the\ntruncation error. The adaptive rank simulations are visually indistinguishable\nfrom the full-rank simulations in capturing solution structures. These results\nhighlight the potential of adaptive rank methods for high-dimensional\nWigner-Poisson simulations, paving the way toward fully kinetic studies of\nstopping power in warm dense plasmas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21314v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21314v1", "AI": {"title_translation": "一种基于采样的自适应秩方法求解Wigner-Poisson系统", "tldr": "本文开发了一种用于一维一速Wigner-Poisson系统的质量守恒、自适应秩求解器，该求解器具有$O(N)$的复杂度和高精度，能够有效模拟暖稠密等离子体中的量子效应，并为高维Wigner-Poisson模拟奠定基础。", "motivation": "该研究旨在开发一个用于一维一速Wigner-Poisson系统的质量守恒、自适应秩求解器，其动机是应用于国家点火装置（NIF）中α粒子阻止本领的研究。在该体系下，电子处于暖稠密状态，需要考虑量子不确定性效应。此外，模拟中观察到解在特定普朗克常数下表现出低秩结构，这进一步促使了自适应秩求解器的开发。", "method": "该方法基于二阶Strang分裂法，首先设计了一个具有结构保持傅里叶更新的全秩求解器，以确保中间解的实值性。随后，针对观察到的低秩结构，开发了一个自适应秩求解器，该求解器结合了用于对流的半拉格朗日自适应秩（SLAR）方案和用于Wigner积分项的自适应秩、结构保持傅里叶更新。论文还提供了结构保持性质的严格证明。", "result": "所开发的全秩求解器通过确保中间解的实值性改进了现有方法。模拟结果表明，对于中等到高无量纲普朗克常数（$H \text{≥} 0.1$），解呈现出低秩结构。自适应秩求解器在存储和计算时间上均实现了$O(N)$的复杂度，同时保持了质量守恒和动量精度达到截断误差。自适应秩模拟在捕获解结构方面与全秩模拟在视觉上无法区分。", "conclusion": "自适应秩方法在高维Wigner-Poisson模拟中展现出巨大潜力，为在暖稠密等离子体中进行完全动力学阻止本领研究铺平了道路。", "translation": "我们开发了一种用于一维一速Wigner-Poisson系统的质量守恒、自适应秩求解器。我们的工作动机是应用于国家点火装置（NIF）中α粒子阻止本领的研究。在此状态下，电子处于暖稠密状态，需要超越标准动力学模型的描述。它们足够热以至于可以忽略泡利不相容原理，但又足够量子化以至于需要考虑不确定性。Wigner-Poisson系统能够捕捉这些效应，但由于其非局部性而带来挑战。基于二阶Strang分裂方法，我们首先设计了一个全秩求解器，其具有结构保持的傅里叶更新，确保中间解保持实值（达到机器精度），从而改进了以前的方法。模拟表明，对于中等到高无量纲普朗克常数（$H \text{≥} 0.1$），解表现出低秩结构。观察到的低秩结构促使我们开发了一种自适应秩求解器，该求解器基于用于对流的半拉格朗日自适应秩（SLAR）方案和用于Wigner积分项的自适应秩、结构保持傅里叶更新构建，并提供了严格的结构保持性质证明。我们的求解器在存储和计算时间上都实现了$O(N)$的复杂度，同时保持质量守恒并确保动量精度达到截断误差。自适应秩模拟在捕获解结构方面与全秩模拟在视觉上难以区分。这些结果凸显了自适应秩方法在高维Wigner-Poisson模拟中的潜力，为在暖稠密等离子体中进行完全动力学阻止本领研究铺平了道路。", "summary": "本文提出了一种新颖的、质量守恒的$O(N)$复杂度自适应秩求解器，用于解决一维一速Wigner-Poisson系统，该研究动机源于暖稠密等离子体中的应用。该求解器基于二阶Strang分裂法和结构保持的傅里叶更新。利用在特定普朗克常数下观察到的解的低秩结构，作者开发了一种结合SLAR和自适应傅里叶更新的自适应秩方案。该求解器能高效地保持质量和动量精度，其结果与全秩模拟视觉上无异，展现了其在高维量子动力学研究中的巨大潜力。", "keywords": "Wigner-Poisson系统, 自适应秩, 暖稠密等离子体, 阻止本领, 量子动力学模型", "comments": "该论文引入了一种创新性的自适应秩方法来求解极具挑战性的Wigner-Poisson系统，这对暖稠密物质物理学至关重要。其核心创新在于利用观测到的解的低秩结构，开发出一种$O(N)$复杂度的求解器，显著提高了非局部量子动力学模型的计算效率。对结构保持性质的严格证明以及与全秩模拟在视觉上的无差别性，凸显了其鲁棒性和准确性，是迈向实用高维量子模拟的重要一步。"}}
{"id": "2506.21046", "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features", "authors": ["Shangbo Wu", "Yu-an Tan", "Ruinan Ma", "Wencong Ma", "Dehua Zhu", "Yuanzhang Li"], "summary": "The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA.", "comment": "14 pages, 9 figures, to appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21046v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21046v1", "AI": {"title_translation": "自监督视觉Transformer特征增强生成对抗性迁移能力", "tldr": "本文提出dSVA方法，利用自监督Vision Transformer（ViT）的特征（包括对比学习的全局结构特征和掩蔽图像建模的局部纹理特征），生成具有高黑盒迁移能力的对抗样本，并超越了现有先进方法。", "motivation": "现有对抗样本生成方法多依赖监督学习特征，而自监督学习和Transformer架构的协同作用启发研究者探索自监督Vision Transformer（ViT）表示是否能提升对抗性迁移能力，旨在提高黑盒对抗样本的泛化能力。", "method": "提出了dSVA——一种生成式双自监督ViT特征攻击，利用对比学习（CL）的全局结构特征和掩蔽图像建模（MIM）的局部纹理特征。设计了一个新的生成式训练框架，包含一个生成器来创建黑盒对抗样本，并通过利用自监督ViT的联合特征和注意力机制来训练生成器。", "result": "研究发现，CL和MIM使ViT能够关注不同的特征倾向，当它们协同利用时，能带来出色的对抗泛化能力。通过扰乱自监督ViT提取的双重深度特征，实现了对各种架构模型的显著黑盒迁移能力，并超越了现有最先进的方法。", "conclusion": "利用自监督ViT（CL和MIM）的双重特征可以有效提升生成对抗样本的黑盒迁移能力，超越现有技术。", "translation": "深度神经网络（DNN）的能力来源于从提供的数据中提取和解释特征。通过利用DNN中的中间特征而不是依赖硬标签，我们能够制作出更有效泛化的对抗性扰动，从而提升黑盒迁移能力。在以往的工作中，这些特征普遍来源于监督学习。受自监督学习和Transformer架构之间卓越协同作用的启发，本文探讨了利用自监督视觉Transformer（ViT）表示是否能提高对抗性迁移能力。我们提出了dSVA——一种生成式双自监督ViT特征攻击，它利用了来自对比学习（CL）的全局结构特征和来自掩蔽图像建模（MIM）的局部纹理特征，这是ViT的自监督学习范式双核。我们设计了一种新颖的生成式训练框架，该框架包含一个生成器来创建黑盒对抗样本，以及通过利用自监督ViT的联合特征和注意力机制来训练生成器的策略。我们的研究结果表明，CL和MIM使ViT能够关注不同的特征倾向，当它们协同利用时，能带来极大的对抗泛化能力。通过扰乱自监督ViT提取的双重深度特征，我们获得了对各种架构模型显著的黑盒迁移能力，其性能优于现有最先进的方法。代码可在https://github.com/spencerwooo/dSVA获取。", "summary": "本文提出dSVA方法，通过利用自监督视觉Transformer（ViT）的特征（包括对比学习的全局特征和掩蔽图像建模的局部特征），生成具有高黑盒迁移能力的对抗样本。研究发现，结合这两种自监督特征能显著提升对抗样本的泛化能力，并在各种模型架构上实现了超越现有先进技术的黑盒迁移效果。", "keywords": "对抗样本, 黑盒迁移, 自监督学习, 视觉Transformer, 特征攻击", "comments": "创新点在于首次将自监督ViT特征（特别是CL和MIM的双重特征）应用于生成对抗样本以提升黑盒迁移性。此方法通过利用模型深层特征而非标签，为提高对抗样本的泛化能力提供了新思路，对提升AI模型的鲁棒性具有重要意义。"}}
{"id": "2506.21338", "title": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification", "authors": ["Galvin Brice S. Lim", "Brian Godwin S. Lim", "Argel A. Bandala", "John Anthony C. Jose", "Timothy Scott C. Chu", "Edwin Sybingco"], "summary": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.21338v1", "categories": ["cs.LG", "cs.HC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21338v1", "AI": {"title_translation": "AGTCNet: 一种用于规范运动想象脑电图分类的图时域方法", "tldr": "AGTCNet是一种新型图时域模型，用于运动想象脑电图分类，解决了现有方法在捕捉复杂时空依赖方面的不足，实现了SOTA性能，且模型更小、推理更快，适合BCI部署。", "motivation": "脑机接口（BCI）技术尽管潜力巨大，但由于个体间和时间上的神经活动固有的复杂性和变异性，以及脑电图硬件的限制，开发主体不变和会话不变的BCI系统仍然是一个重大挑战。现有方法在捕捉多通道脑电图信号中复杂的时空依赖方面效率低下。", "method": "本研究引入了注意力图时域卷积网络（AGTCNet），这是一种新颖的图时域模型，用于运动想象EEG（MI-EEG）分类。AGTCNet利用EEG电极的地形配置作为归纳偏置，并整合图卷积注意力网络（GCAT）以共同学习富有表现力的时空EEG表示。", "result": "AGTCNet显著优于现有MI-EEG分类器，实现了最先进的性能，同时采用了紧凑的架构。模型尺寸减少49.87%，推理时间加快64.65%，输入EEG信号更短。在BCI Competition IV Dataset 2a上，主体独立分类的移动平均准确率为66.82%，主体特定分类微调后提高到82.88%。在EEG Motor Movement/Imagery Dataset上，4类和2类主体独立分类的移动平均准确率分别为64.14%和85.22%，主体特定分类进一步提高到72.13%和90.54%。", "conclusion": "AGTCNet在运动想象EEG分类中表现出卓越的性能和实用性，其紧凑的架构和高效的推理速度使其非常适合BCI部署。", "translation": "利用脑电图（EEG）的脑机接口（BCI）技术标志着一项变革性创新，使运动障碍个体能够平等地与环境互动。尽管其潜力巨大，但由于个体间和时间上的神经活动固有的复杂性和变异性，以及脑电图硬件的限制，开发主体不变和会话不变的BCI系统仍然是一个重大挑战。虽然先前的研究试图开发鲁棒的BCI系统，但现有方法在捕捉多通道脑电图信号中复杂的时空依赖方面仍然无效。本研究通过引入注意力图时域卷积网络（AGTCNet）解决了这一空白，AGTCNet是一种用于运动想象脑电图（MI-EEG）分类的新颖图时域模型。具体而言，AGTCNet利用脑电图电极的地形配置作为归纳偏置，并整合图卷积注意力网络（GCAT）以共同学习富有表现力的时空脑电图表示。所提出的模型显著优于现有MI-EEG分类器，实现了最先进的性能，同时采用了紧凑的架构，突显了其在BCI部署中的有效性和实用性。通过将模型尺寸减小49.87%，推理时间加快64.65%，以及更短的输入脑电图信号，AGTCNet在BCI Competition IV Dataset 2a上实现了主体独立分类的移动平均准确率为66.82%，当针对主体特定分类进行微调时，准确率进一步提高到82.88%。在EEG Motor Movement/Imagery Dataset上，AGTCNet在4类和2类主体独立分类中分别达到了64.14%和85.22%的移动平均准确率，对于主体特定分类，准确率进一步提高到72.13%和90.54%。", "summary": "本文提出了一种名为AGTCNet的注意力图时域卷积网络，用于运动想象EEG分类，旨在解决现有BCI系统在捕获复杂EEG时空依赖方面的不足。AGTCNet利用EEG电极的拓扑结构和图卷积注意力网络来学习有效的时空表示。实验结果表明，AGTCNet在多个数据集上均显著优于现有MI-EEG分类器，取得了最先进的性能，同时具有更小的模型尺寸和更快的推理速度，证明了其在BCI应用中的有效性和实用性。", "keywords": "运动想象脑电图, 脑机接口, 图卷积网络, 时空表示, AGTCNet", "comments": "AGTCNet的创新点在于结合了图神经网络和时域卷积，并利用EEG电极的拓扑结构作为归纳偏置，有效捕捉了EEG信号的复杂时空依赖。其在性能提升的同时，显著减小了模型尺寸并加快了推理速度，这对于BCI设备的实际部署具有重要意义。该研究为开发更高效、更实用的BCI系统提供了新思路。"}}
{"id": "2506.21250", "title": "ACTLLM: Action Consistency Tuned Large Language Model", "authors": ["Jing Bi", "Lianggong Bruce Wen", "Zhang Liu", "Chenliang Xu"], "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model),\na novel approach for robot manipulation in dynamic environments. Traditional\nvision-based systems often struggle to learn visual representations that excel\nin both task execution and spatial reasoning, thereby limiting their\nadaptability in dynamic environments. ACTLLM addresses these challenges by\nharnessing language to craft structured scene descriptors, providing a uniform\ninterface for both spatial understanding and task performance through flexible\nlanguage instructions. Moreover, we introduce a novel action consistency\nconstraint that aligns visual perception with corresponding actions, thereby\nenhancing the learning of actionable visual representations. Additionally, we\nhave reformulated the Markov decision process for manipulation tasks into a\nmulti-turn visual dialogue framework. This approach enables the modeling of\nlong-term task execution with enhanced contextual relevance derived from the\nhistory of task execution. During our evaluation, ACTLLM excels in diverse\nscenarios, proving its effectiveness on challenging vision-based robot\nmanipulation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21250v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21250v1", "AI": {"title_translation": "ACTLLM：动作一致性调整的大语言模型", "tldr": "ACTLLM是一个为动态环境中机器人操作设计的新型大语言模型，它利用语言生成结构化场景描述，引入动作一致性约束，并将MDP重构为多轮视觉对话，以提高视觉感知和任务执行的有效性。", "motivation": "传统的基于视觉的系统在学习既擅长任务执行又擅长空间推理的视觉表示方面存在困难，从而限制了它们在动态环境中的适应性。", "method": "ACTLLM通过利用语言来创建结构化场景描述，为空间理解和任务性能提供统一接口。此外，它引入了一种新颖的动作一致性约束，以对齐视觉感知与相应动作，从而增强可操作视觉表示的学习。该方法还将操作任务的马尔可夫决策过程重新构建为多轮视觉对话框架，以建模长期任务执行，并从任务执行历史中获得增强的上下文相关性。", "result": "ACTLLM在各种场景中表现出色，证明了其在具有挑战性的基于视觉的机器人操作任务上的有效性。", "conclusion": "该论文引入的ACTLLM通过语言驱动的场景描述、动作一致性约束和多轮视觉对话框架，显著提升了机器人在动态环境下的操作能力和视觉表示学习效果。", "translation": "本文介绍了ACTLLM（动作一致性调整的大语言模型），一种在动态环境中进行机器人操作的新颖方法。传统的基于视觉的系统通常难以学习在任务执行和空间推理方面都表现出色的视觉表示，从而限制了它们在动态环境中的适应性。ACTLLM通过利用语言来制作结构化场景描述来解决这些挑战，为通过灵活的语言指令进行空间理解和任务性能提供统一的界面。此外，我们引入了一种新颖的动作一致性约束，它将视觉感知与相应的动作对齐，从而增强了可操作视觉表示的学习。此外，我们还将操作任务的马尔可夫决策过程重新构建为多轮视觉对话框架。这种方法能够通过从任务执行历史中获得的增强上下文相关性来建模长期任务执行。在我们的评估中，ACTLLM在各种场景中表现出色，证明了其在具有挑战性的基于视觉的机器人操作任务上的有效性。", "summary": "ACTLLM是一种用于动态环境中机器人操作的新型大语言模型。它通过语言创建结构化场景描述来解决传统视觉系统在任务执行和空间推理上的不足。该模型引入了动作一致性约束来优化视觉感知与动作的对齐，并将马尔可夫决策过程重构为多轮视觉对话，以增强长期任务的上下文理解。实验证明ACTLLM在复杂的视觉机器人操作任务中表现优异。", "keywords": "ACTLLM, 机器人操作, 大语言模型, 动作一致性, 视觉对话", "comments": "ACTLLM的创新之处在于其结合大语言模型、动作一致性约束和多轮视觉对话框架来解决机器人操作中视觉表示学习和环境适应性的挑战。通过语言作为统一接口，它有效地桥接了高级推理和低级动作执行，这对于提升机器人在复杂动态环境中的自主性具有重要意义。该方法有望推动机器人领域在真实世界部署中的进步。"}}
{"id": "2506.20243", "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "authors": ["Papa Séga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2506.20243v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20243v1", "AI": {"title_translation": "CBF-AFA：基于块的多自监督学习融合用于自动流利度评估", "tldr": "提出了一种基于语音块的多自监督学习模型融合方法CBF-AFA，用于提高非母语使用者自动流利度评估的准确性，并在多个数据集上取得了显著提升。", "motivation": "自动流利度评估（AFA）仍然具有挑战性，特别是在捕捉非母语使用者语音节奏、停顿和不流利现象方面。", "method": "本文引入了一种基于语音块的方法，通过Silero VAD将语音分割成呼吸组块。该方法集成了Wav2Vec2、HuBERT和WavLM等自监督学习（SSL）模型，并通过可学习的加权机制融合SSL嵌入。融合后的嵌入辅以块级流利度标记（如语速、停顿持续时间、n-gram重复），并送入分层的CNN-BiLSTM框架以捕捉局部和长期依赖。", "result": "在Avalinguo和Speechocean762数据集上进行评估，与单SSL基线相比，在Speechocean762上F1分数提高了2.8，皮尔逊相关系数提高了6.2个点；在Avalinguo上F1分数提高了4.2，皮尔逊相关系数提高了4.0个点。该方法超越了基于Pyannote.audio的分割基线。", "conclusion": "这些发现强调了基于块的多SSL融合对于鲁棒流利度评估的有效性，但未来的工作应探索其对具有不规则韵律方言的泛化能力。", "translation": "自动流利度评估（AFA）仍然具有挑战性，尤其是在捕捉非母语使用者的语音节奏、停顿和不流利现象方面。我们引入了一种基于语音块的方法，该方法集成了自监督学习（SSL）模型（Wav2Vec2、HuBERT和WavLM），这些模型因其在语音、韵律和嘈杂语音建模方面的互补优势而被选中，并结合了分层的CNN-BiLSTM框架。语音使用Silero语音活动检测（Silero-VAD）分割成呼吸组块，从而实现细粒度的时序分析，同时减轻过度分割的伪影。SSL嵌入通过可学习的加权机制进行融合，平衡了声学和语言特征，并富含块级流利度标记（例如，语速、停顿持续时间、n-gram重复）。CNN-BiLSTM捕捉块间的局部和长期依赖。在Avalinguo和Speechocean762数据集上进行评估，我们的方法在Speechocean762上比单一SSL基线在F1分数上提高了2.8，皮尔逊相关系数提高了6.2个点，在Avalinguo上F1分数提高了4.2，皮尔逊相关系数提高了4.0个点，超越了基于Pyannote.audio的分割基线。这些发现强调了基于块的多SSL融合对于鲁棒流利度评估的有效性，尽管未来的工作应探索其对具有不规则韵律方言的泛化能力。", "summary": "本文提出了一种名为CBF-AFA的自动流利度评估新方法，通过将语音分割成呼吸组块，并融合Wav2Vec2、HuBERT和WavLM等多个自监督学习模型的嵌入，结合块级流利度标记和CNN-BiLSTM框架，以有效捕捉非母语使用者的语音节奏、停顿和不流利现象。实验结果表明，该方法在Avalinguo和Speechocean762数据集上均显著优于现有基线。", "keywords": "自动流利度评估, 自监督学习, 语音块, 多模型融合, 非母语语音", "comments": "该论文的创新点在于提出了“基于块的多SSL融合”方法，有效结合了不同SSL模型的互补优势，并通过细粒度的块级分析和上下文捕捉提升了自动流利度评估的准确性。其重要性在于解决了非母语者语音流利度评估中的关键挑战，特别是对语音节奏和不流利现象的捕捉。局限性在于未来需要探索其对具有不规则韵律方言的泛化能力。"}}
{"id": "2506.20936", "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling", "authors": ["Hao Zhang", "Haolan Xu", "Chun Feng", "Varun Jampani", "Narendra Ahuja"], "summary": "Skinning and rigging are fundamental components in animation, articulated\nobject reconstruction, motion transfer, and 4D generation. Existing approaches\npredominantly rely on Linear Blend Skinning (LBS), due to its simplicity and\ndifferentiability. However, LBS introduces artifacts such as volume loss and\nunnatural deformations, and it fails to model elastic materials like soft\ntissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty\ntissues). In this work, we propose PhysRig: a differentiable physics-based\nskinning and rigging framework that overcomes these limitations by embedding\nthe rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),\nwhich is simulated as a deformable soft-body structure driven by the animated\nskeleton. Our method leverages continuum mechanics and discretizes the object\nas particles embedded in an Eulerian background grid to ensure\ndifferentiability with respect to both material properties and skeletal motion.\nAdditionally, we introduce material prototypes, significantly reducing the\nlearning space while maintaining high expressiveness. To evaluate our\nframework, we construct a comprehensive synthetic dataset using meshes from\nObjaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object\ncategories and motion patterns. Our method consistently outperforms traditional\nLBS-based approaches, generating more realistic and physically plausible\nresults. Furthermore, we demonstrate the applicability of our framework in the\npose transfer task highlighting its versatility for articulated object\nmodeling.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20936v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20936v1", "AI": {"title_translation": "PhysRig：一种用于逼真关节对象建模的可微分基于物理的蒙皮与骨骼绑定框架", "tldr": "PhysRig是一个可微分的基于物理的蒙皮和骨骼绑定框架，通过将骨骼嵌入可变形软体结构中来克服传统LBS的体积损失和不自然变形问题，生成更逼真、物理上更合理的结果。", "motivation": "现有的蒙皮和骨骼绑定方法主要依赖线性混合蒙皮（LBS），但LBS会导致体积损失和不自然的变形，并且无法有效模拟弹性材料，如软组织、毛发和柔性附件。", "method": "论文提出了PhysRig，一个可微分的基于物理的蒙皮和骨骼绑定框架。该方法将刚性骨骼嵌入到体积表示（例如四面体网格）中，并将其模拟为由动画骨骼驱动的可变形软体结构。它利用连续介质力学，将对象离散化为嵌入欧拉背景网格中的粒子，以确保对材料属性和骨骼运动的可微分性。此外，引入了材料原型以减少学习空间并保持高表达性。", "result": "PhysRig在综合合成数据集上进行了评估，结果表明该方法始终优于传统的基于LBS的方法，生成了更逼真和物理上更合理的结果。它还展示了在姿态迁移任务中的适用性，突出了其在关节对象建模方面的多功能性。", "conclusion": "PhysRig框架通过引入物理模拟克服了传统LBS的局限性，能够生成更逼真、物理上更合理的变形，并具有广泛的应用潜力，尤其是在姿态迁移等关节对象建模任务中。", "translation": "蒙皮和骨骼绑定是动画、关节对象重建、动作迁移和4D生成中的基本组成部分。现有方法主要依赖线性混合蒙皮（LBS），因为它简单且可微分。然而，LBS会引入体积损失和不自然变形等伪影，并且无法模拟软组织、毛发和柔性附件（例如，象鼻、耳朵和脂肪组织）等弹性材料。在这项工作中，我们提出了PhysRig：一个可微分的基于物理的蒙皮和骨骼绑定框架，通过将刚性骨骼嵌入到体积表示（例如四面体网格）中来克服这些限制，该网格被模拟为由动画骨骼驱动的可变形软体结构。我们的方法利用连续介质力学，并将对象离散化为嵌入欧拉背景网格中的粒子，以确保对材料属性和骨骼运动的可微分性。此外，我们引入了材料原型，显著减少了学习空间，同时保持了高表达性。为了评估我们的框架，我们使用来自Objaverse、The Amazing Animals Zoo和MixaMo的网格构建了一个全面的合成数据集，涵盖了不同的对象类别和运动模式。我们的方法始终优于传统的基于LBS的方法，生成了更逼真和物理上更合理的结果。此外，我们展示了我们框架在姿态迁移任务中的适用性，突出了其在关节对象建模方面的多功能性。", "summary": "PhysRig是一种新颖的可微分物理蒙皮和骨骼绑定框架，旨在解决传统线性混合蒙皮（LBS）在处理体积损失、不自然变形和弹性材料方面的不足。通过将刚性骨骼嵌入到可变形的软体体积表示中并进行物理模拟，该框架利用连续介质力学和欧拉网格粒子离散化来实现对材料属性和骨骼运动的可微分性。此外，引入的材料原型优化了学习效率。实验证明，PhysRig在生成更逼真、物理上合理的变形方面优于LBS，并在姿态迁移等关节对象建模任务中展现出强大的多功能性。", "keywords": "蒙皮, 骨骼绑定, 物理模拟, 可微分, 软体变形", "comments": "这项工作通过引入物理模拟来改进蒙皮和骨骼绑定，解决了传统LBS的固有缺陷，特别是其在处理体积损失和弹性材料方面的局限性。其创新点在于将骨骼嵌入到可变形软体结构中并利用连续介质力学，同时保持了可微分性，这对于优化和学习至关重要。引入材料原型是另一个巧妙之处，有助于在保持表达性的同时减少计算复杂度。该框架在生成物理上更合理和逼真的动画方面具有重要意义，尤其适用于复杂生物体和弹性对象的建模。"}}
{"id": "2506.21412", "title": "Plasmonically Enhanced Flexural-Mode AlScN Nanoplate Resonator as Uncooled and Ultrafast IR Detector with High Responsivity", "authors": ["Aurelio Venditti", "Walter Gubinelli", "Enise F. Altin", "Luca Colombo", "Pietro Simeoni", "Benyamin Davaji", "Matteo Rinaldi"], "summary": "This letter introduces a novel class of miniaturized, uncooled, and\nultra-fast infrared (IR) resonant thermal detectors (RTDs) based on 30%-doped\nAluminum Scandium Nitride (AlScN) nanoplates. Exploiting high electromechanical\ncoupling, good thermal properties, and enhanced and selective IR absorption,\nthe presented device aims to demonstrate significant advancements over the\nstate-of-the-art IR RTDs. This single pixel combines compact footprint, high\nspectral selectivity and responsivity, reduced noise, and fast thermal\nresponse, allowing for the potential development of innovative IR thermal\nimagers through multi-pixel integration. The flexural nature of the actuated\nresonance mode eventually enables an interferometric optical readout, paving\nthe way towards achieving extremely low Noise Equivalent Power levels. These\nresults demonstrate a high IR responsivity of around 130 ppt/pW, a thermal time\nconstant of around 330 us, and a large out-of-plane displacement. This work\nrepresents the first experimental integration on a resonating platform of\nplasmonic absorbers that utilize AlScN as dielectric layer.", "comment": "This manuscript has been submitted to ACS Nano Letters for\n  consideration", "pdf_url": "http://arxiv.org/pdf/2506.21412v1", "categories": ["physics.ins-det", "cs.SY", "eess.SY", "physics.app-ph"], "cate": "physics.ins-det", "url": "http://arxiv.org/abs/2506.21412v1", "AI": {"title_translation": "等离子体增强弯曲模式AlScN纳米板谐振器作为非制冷超快高响应红外探测器", "tldr": "本文介绍了一种基于AlScN纳米板的新型小型化、非制冷、超快红外谐振热探测器，利用等离子体吸收和弯曲模式谐振实现了高响应度和快速响应，并有望用于创新的红外热成像。", "motivation": "目前的红外谐振热探测器存在性能限制。本文旨在通过引入基于AlScN纳米板的新型探测器，利用其高机电耦合、良好热性能和增强的红外吸收，实现对现有技术的显著改进，以开发出具有紧凑尺寸、高光谱选择性、高响应度、低噪声和快速热响应的器件。", "method": "本文介绍了一种基于30%掺杂的AlScN纳米板的新型小型化、非制冷、超快红外谐振热探测器。该器件利用AlScN的高机电耦合、良好热性能和增强的红外选择性吸收。谐振模式的弯曲特性支持干涉光学读出，并且首次在谐振平台上实验性地集成了以AlScN作为介电层的等离子体吸收器。", "result": "该探测器实现了约130 ppt/pW的高红外响应度，约330 µs的热时间常数，以及大的面外位移。这些结果证明了其在紧凑尺寸、高光谱选择性和响应度、低噪声和快速热响应方面的能力。", "conclusion": "本文成功演示了基于等离子体增强弯曲模式AlScN纳米板谐振器作为非制冷超快红外探测器，其高性能（高响应度、快响应时间）和集成等离子体吸收器的创新性，为开发新型红外热成像仪奠定了基础，并有望实现极低的噪声等效功率。", "translation": "这封信介绍了一种基于30%掺杂的氮化铝钪（AlScN）纳米板的新型小型化、非制冷、超快红外（IR）谐振热探测器（RTDs）。该器件利用高机电耦合、良好的热性能以及增强和选择性的红外吸收，旨在展示对现有红外RTD技术的显著进步。这种单像素结合了紧凑的尺寸、高光谱选择性和响应度、降低的噪声和快速的热响应，从而有可能通过多像素集成开发创新的红外热成像仪。驱动谐振模式的弯曲性质最终实现了干涉光学读出，为实现极低的噪声等效功率水平铺平了道路。这些结果展示了约130 ppt/pW的高红外响应度、约330 µs的热时间常数和大的面外位移。这项工作代表了首次在谐振平台上实验性地集成了以AlScN作为介电层的等离子体吸收器。", "summary": "本研究介绍了一种新型小型化、非制冷、超快红外谐振热探测器，其核心是30%掺杂的AlScN纳米板。该探测器利用AlScN优异的机电耦合、热性能和增强的红外吸收，结合等离子体吸收器和弯曲模式谐振，实现了高响应度（约130 ppt/pW）和快速热响应（约330 µs）。该器件具有紧凑尺寸、高光谱选择性和低噪声，为开发创新的多像素红外热成像仪和实现极低噪声等效功率提供了潜力。", "keywords": "AlScN纳米板, 红外探测器, 谐振器, 等离子体增强, 非制冷", "comments": "该论文的创新点在于首次将等离子体吸收器与AlScN纳米板谐振器结合，用于红外探测，并且利用了AlScN的高机电耦合特性。其重要性在于提供了一种高性能、小型化、非制冷且响应速度快的红外探测方案，有望推动红外热成像技术的发展。"}}
{"id": "2506.21384", "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.", "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "pdf_url": "http://arxiv.org/pdf/2506.21384v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21384v1", "AI": {"title_translation": "利用LLM辅助查询理解实现实时检索增强生成", "tldr": "提出Omni-RAG框架，通过LLM辅助查询理解，处理实时RAG系统中复杂、噪声和多意图的用户查询，提升系统鲁棒性和有效性。", "motivation": "实时检索增强生成（RAG）系统在处理嘈杂、模糊和包含多重意图的用户查询时面临显著挑战，现有系统通常难以应对此类复杂输入，因为它们多在更干净的数据上进行训练或评估。", "method": "论文引入了Omni-RAG框架，通过LLM辅助查询理解来预处理用户输入，包括三个关键模块：1. 深度查询理解与分解：利用LLM和定制提示去噪查询（如纠正拼写错误），并将多意图查询分解为结构化子查询。2. 意图感知知识检索：对每个子查询从语料库（如FineWeb使用OpenSearch）执行检索并聚合结果。3. 重排序与生成：重排序器（如BGE）在LLM（如Falcon-10B）使用思维链提示生成最终响应之前，优化文档选择。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "实时检索增强生成（RAG）系统在处理通常嘈杂、模糊且包含多重意图的用户查询时面临显著挑战。虽然RAG通过外部知识增强了大型语言模型（LLM），但当前的系统通常难以应对此类复杂输入，因为它们通常在更干净的数据上进行训练或评估。本文介绍了一种名为Omni-RAG的新颖框架，旨在提高RAG系统在实时、开放域设置中的鲁棒性和有效性。Omni-RAG采用LLM辅助查询理解来通过三个关键模块预处理用户输入：（1）深度查询理解和分解，该模块利用带有定制提示的LLM来去噪查询（例如，纠正拼写错误）并将多意图查询分解为结构化子查询；（2）意图感知知识检索，该模块从语料库（即使用OpenSearch的FineWeb）对每个子查询执行检索并聚合结果；（3）重排序与生成，在该模块中，重排序器（即BGE）在LLM（即Falcon-10B）使用思维链提示生成最终响应之前，优化文档选择。Omni-RAG旨在通过稳健地处理复杂和嘈杂的查询，弥合当前RAG能力与真实世界应用需求之间的差距，例如SIGIR 2025 LiveRAG挑战赛所强调的需求。", "summary": "论文提出了Omni-RAG框架，旨在解决实时RAG系统处理复杂、嘈杂和多意图用户查询的挑战。该框架通过LLM辅助查询理解，包含深度查询理解与分解（去噪和子查询分解）、意图感知知识检索（对子查询进行检索并聚合结果）以及重排序与生成（优化文档选择并生成最终响应）三个核心模块，以提升RAG系统在开放域环境中的鲁棒性和有效性。", "keywords": "RAG, LLM, 查询理解, 实时系统, 检索增强生成", "comments": "Omni-RAG的创新点在于其利用LLM进行深度查询理解和分解，从而能有效处理真实世界中常见的复杂、噪声和多意图查询，这是当前RAG系统面临的主要挑战。通过将查询预处理与意图感知检索和重排序相结合，该框架有望显著提升RAG在实时应用中的实用性和性能。"}}
{"id": "2506.21053", "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21053v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21053v1", "AI": {"title_translation": "MT2-CSD：一种用于会话立场检测的新数据集和多语义知识融合方法", "tldr": "本文提出了MT2-CSD数据集，一个用于多目标、多轮会话立场检测的大规模数据集，并引入了LLM-CRAN模型，该模型利用大语言模型提升会话理解，并在新数据集上取得了显著优于基线模型的性能。", "motivation": "当前立场检测研究多针对单一实例，难以有效建模社交媒体中典型的多方会话；此外，缺乏真实捕捉社交媒体互动动态的数据集，阻碍了会话立场检测的进一步发展。", "method": "本文引入了MT2-CSD，一个用于多目标、多轮会话立场检测的综合数据集，据称是目前最大的同类数据集，包含24,457个标注实例，并具有最大的会话深度。为应对这些挑战，本文提出了一种大语言模型增强会话关系注意力网络（LLM-CRAN），该网络利用大语言模型的推理能力来提高会话理解能力。", "result": "实验结果表明，LLM-CRAN在会话立场检测任务中显著优于强大的基线模型。", "conclusion": "MT2-CSD数据集的引入及其带来的挑战，以及LLM-CRAN模型的有效性，共同推动了多轮会话立场检测领域的研究进展，尤其在处理复杂会话深度方面表现出色。", "translation": "在当代社交媒体领域，自动立场检测对于意见挖掘至关重要，因为它综合并审查用户对有争议话题的看法，以揭示普遍趋势和情绪。传统的立场检测研究通常针对单个实例，从而限制了其建模真实社交媒体场景中典型的多方讨论的能力。这一缺点很大程度上源于缺乏真实捕捉社交媒体互动动态的数据集，从而阻碍了会话立场检测的进展。在本文中，我们介绍了MT2-CSD，一个用于多目标、多轮会话立场检测的综合数据集。据我们所知，MT2-CSD是目前可用于此目的的最大数据集，包含24,457个标注实例，并展现出最大的会话深度，从而为立场检测带来了新的挑战。为了应对这些挑战，我们提出了大语言模型增强会话关系注意力网络（LLM-CRAN），它利用LLM的推理能力来改进会话理解。我们进行了广泛的实验来评估LLM-CRAN在MT2-CSD数据集上的有效性。实验结果表明，LLM-CRAN在会话立场检测任务中显著优于强大的基线模型。", "summary": "本文针对传统立场检测研究在建模多方会话和数据集稀缺方面的不足，提出了MT2-CSD，一个目前最大的多目标、多轮会话立场检测数据集，包含24,457个实例。为应对新数据集带来的挑战，作者提出LLM-CRAN模型，该模型利用大语言模型增强会话理解能力。实验证明，LLM-CRAN在会话立场检测任务中显著优于基线模型。", "keywords": "会话立场检测, 数据集, 大语言模型, MT2-CSD, LLM-CRAN", "comments": "本文的创新点在于构建了目前最大的多目标、多轮会话立场检测数据集MT2-CSD，其大规模和深会话的特性为该领域带来了新的挑战和研究机会。同时，提出的LLM-CRAN模型巧妙地将大语言模型的推理能力融入会话理解，为解决复杂会话场景下的立场检测问题提供了有效途径，展示了LLM在特定任务上的巨大潜力。"}}
{"id": "2506.21326", "title": "A discontinuous in time Streamline Diffusion Virtual Element Method for Darcy-transport problem", "authors": ["R A Caraballo Diaz", "F Dassi"], "summary": "We present a first numerical study of transport phenomena involving\nchemically reactive species, modeled by advection-diffusion-reaction systems\nwith flow fields governed by Darcy's law. Among the various discretisation\napproaches, we consider the Streamline Diffusion method. Both the velocity\nfield and the species concentrations are computed using the Virtual Element\nMethod using a Discontinuous Galerkin scheme for time. An abstract error\nestimate has been derived using a special technique that utilizes Gauss-Radau\ninterpolation in conjunction with numerical integration. These theoretical\nfindings are supported by numerical experiments with arbitrary-order accuracy\nin both space and time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21326v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21326v1", "AI": {"title_translation": "达西输运问题的时间不连续流线扩散虚单元法", "tldr": "本文首次数值研究了达西输运问题中涉及化学反应物种的输运现象，提出并验证了一种时间不连续流线扩散虚单元法，实现了空间和时间上的任意阶精度。", "motivation": "研究涉及化学反应物种的输运现象，这些现象通过对流-扩散-反应系统建模，其流场由达西定律控制。", "method": "采用流线扩散法，结合虚单元法计算速度场和物种浓度，并使用时间上的不连续伽辽金格式。通过结合高斯-拉道插值和数值积分的特殊技术，推导了抽象误差估计。", "result": "理论上的抽象误差估计得到了数值实验的支持，实验结果显示在空间和时间上均达到了任意阶精度。", "conclusion": "所提出的时间不连续流线扩散虚单元法对于达西输运问题是有效的，并能实现任意阶精度。", "translation": "我们首次对涉及化学反应物种的输运现象进行了数值研究，这些现象通过对流-扩散-反应系统建模，其流场由达西定律控制。在各种离散化方法中，我们考虑了流线扩散法。速度场和物种浓度都使用虚单元法计算，时间上采用不连续伽辽金格式。利用高斯-拉道插值结合数值积分的特殊技术，推导出了一个抽象误差估计。这些理论发现得到了数值实验的支持，实验显示在空间和时间上均达到了任意阶精度。", "summary": "本文首次对涉及化学反应物种的对流-扩散-反应系统在达西定律控制流场下的输运现象进行了数值研究。研究提出了一种时间不连续流线扩散虚单元法，用于计算速度场和物种浓度。通过利用高斯-拉道插值和数值积分，推导出了抽象误差估计，并通过数值实验验证了该方法在空间和时间上均能达到任意阶精度。", "keywords": "流线扩散, 虚单元法, 不连续伽辽金, 达西输运, 对流-扩散-反应", "comments": "本文的创新点在于将流线扩散法与虚单元法以及时间上的不连续伽辽金格式相结合，应用于复杂的达西输运问题。其抽象误差估计的推导以及数值实验验证的任意阶精度，突显了其在反应输运高精度模拟方面的理论严谨性和实际潜力。"}}
{"id": "2506.21524", "title": "Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding", "authors": ["Libn Varghese", "Bhaskar Chaudhury", "Miral Shah", "Mainak Bandyopadhyay"], "summary": "The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase\nspace information using particle and grid data structures. High computational\ncosts in 2D and 3D device-scale PIC simulations necessitate parallelization,\nwith the Charge Deposition (CD) subroutine often becoming a bottleneck due to\nfrequent particle-grid interactions. Conventional methods mitigate dependencies\nby generating private grids for each core, but this approach faces scalability\nissues. We propose a novel approach based on a particle-thread binding strategy\nthat requires only four private grids per node in distributed memory systems or\nfour private grids in shared memory systems, enhancing CD scalability and\nperformance while maintaining conventional data structures and requiring\nminimal changes to existing PIC codes. This method ensures complete\naccessibility of grid data structure for concurrent threads and avoids\nsimultaneous access to particles within the same cell using additional\nfunctions and flags. Performance evaluations using a PIC benchmark for\nlow-temperature partially magnetized E x B discharge simulation on a shared\nmemory as well as a distributed memory system (1000 cores) demonstrate the\nmethod's scalability, and additionally, we show the method has little hardware\ndependency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21524v1", "categories": ["physics.comp-ph", "cs.DC", "physics.plasm-ph"], "cate": "physics.comp-ph", "url": "http://arxiv.org/abs/2506.21524v1", "AI": {"title_translation": "基于粒子-线程绑定的低温等离子体模拟静电粒子网格法基准测试与并行化", "tldr": "本文提出了一种基于粒子-线程绑定的新型并行化方法，用于解决粒子网格法（PIC）模拟中电荷沉积（CD）子例程的计算瓶颈和传统方法的扩展性问题，并在共享内存和分布式内存系统上展示了其良好的扩展性和性能。", "motivation": "粒子网格法（PIC）在二维和三维设备尺度的等离子体模拟中计算成本很高，其中电荷沉积（CD）子例程由于频繁的粒子-网格交互而成为瓶颈。传统的并行化方法通过为每个核心生成私有网格来缓解依赖性，但这种方法面临扩展性问题。", "method": "提出了一种基于粒子-线程绑定策略的新方法。该方法在分布式内存系统中每个节点仅需四个私有网格，在共享内存系统中也只需四个私有网格，从而增强了CD的扩展性和性能，同时保持了传统的粒子网格数据结构，并对现有PIC代码的改动最小。该方法通过附加函数和标志确保并发线程对网格数据结构的完全可访问性，并避免了同一单元内粒子之间的同时访问。", "result": "在共享内存和分布式内存系统（1000个核心）上，使用低温部分磁化E x B放电模拟的PIC基准测试进行了性能评估，结果表明该方法具有良好的扩展性，并且硬件依赖性很小。", "conclusion": "所提出的基于粒子-线程绑定的方法显著增强了电荷沉积（CD）的扩展性和性能，同时对现有PIC代码的修改量很小。", "translation": "粒子网格法（PIC）用于等离子体模拟，通过粒子和网格数据结构跟踪粒子相空间信息。二维和三维设备尺度的PIC模拟计算成本高昂，因此需要并行化，其中电荷沉积（CD）子例程由于频繁的粒子-网格交互而常常成为瓶颈。传统方法通过为每个核心生成私有网格来缓解依赖性，但这种方法面临扩展性问题。我们提出了一种基于粒子-线程绑定策略的新方法，在分布式内存系统中每个节点仅需四个私有网格，或在共享内存系统中仅需四个私有网格，从而增强了CD的扩展性和性能，同时保持了传统的粒子网格数据结构，并对现有PIC代码的改动最小。该方法确保并发线程对网格数据结构的完全可访问性，并通过附加函数和标志避免了同一单元内粒子之间的同时访问。使用低温部分磁化E x B放电模拟的PIC基准测试在共享内存以及分布式内存系统（1000个核心）上进行了性能评估，结果表明该方法具有良好的扩展性，并且硬件依赖性很小。", "summary": "本文针对粒子网格法（PIC）模拟中电荷沉积（CD）子例程的计算瓶颈和传统并行化方法的扩展性问题，提出了一种创新的粒子-线程绑定策略。该策略通过显著减少所需的私有网格数量（每个节点或系统仅四个）来提高CD的扩展性和性能，同时保持现有PIC代码的结构不变。通过在共享内存和分布式内存系统上对低温等离子体模拟进行基准测试，验证了该方法的有效性和低硬件依赖性。", "keywords": "粒子网格法, 并行化, 电荷沉积, 等离子体模拟, 扩展性", "comments": "该论文提出了一种新颖且高效的并行化策略，解决了粒子网格法（PIC）模拟中长期存在的电荷沉积（CD）计算瓶颈和传统并行化方法的扩展性问题。其创新点在于采用了粒子-线程绑定，并显著减少了私有网格的需求，从而在保持代码兼容性的同时实现了出色的扩展性和性能。这对于大规模等离子体模拟的计算效率提升具有重要意义。"}}
{"id": "2506.20939", "title": "AIR-VIEW: The Aviation Image Repository for Visibility Estimation of Weather, A Dataset and Benchmark", "authors": ["Chad Mourning", "Zhewei Wang", "Justin Murray"], "summary": "Machine Learning for aviation weather is a growing area of research for\nproviding low-cost alternatives for traditional, expensive weather sensors;\nhowever, in the area of atmospheric visibility estimation, publicly available\ndatasets, tagged with visibility estimates, of distances relevant for aviation,\nof diverse locations, of sufficient size for use in supervised learning, are\nabsent. This paper introduces a new dataset which represents the culmination of\na year-long data collection campaign of images from the FAA weather camera\nnetwork suitable for this purpose. We also present a benchmark when applying\nthree commonly used approaches and a general-purpose baseline when trained and\ntested on three publicly available datasets, in addition to our own, when\ncompared against a recently ratified ASTM standard.", "comment": "5 pages, meant as citation for dataset", "pdf_url": "http://arxiv.org/pdf/2506.20939v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20939v1", "AI": {"title_translation": "AIR-VIEW：用于天气能见度估计的航空图像存储库，一个数据集和基准", "tldr": "本文介绍了AIR-VIEW，一个用于航空天气能见度估计的新数据集和基准，以解决现有公开数据集的不足。", "motivation": "现有的公开数据集在航空相关距离、多样化地点和足够大的规模方面不足，无法用于监督学习进行大气能见度估计，而传统气象传感器成本高昂。", "method": "本文介绍了AIR-VIEW数据集，它是通过FAA气象相机网络一年多的数据收集活动形成的。同时，本文还提出了一个基准测试，将三种常用方法和一个通用基线模型在AIR-VIEW数据集和其他三个公开数据集上进行训练和测试，并与最近批准的ASTM标准进行比较。", "result": "本文提供了AIR-VIEW数据集，并建立了一个基准测试，展示了三种常用方法和一个通用基线模型在该数据集上的表现，并与ASTM标准进行了对比。具体性能结果未在摘要中提及。", "conclusion": "AIR-VIEW数据集及其伴随的基准测试为航空气象能见度估计领域的机器学习研究提供了急需的资源，解决了现有数据集的不足，并为未来研究提供了评估标准。", "translation": "机器学习在航空气象领域是一个不断发展的研究方向，旨在为传统昂贵的气象传感器提供低成本替代方案；然而，在大气能见度估计领域，缺乏适用于航空相关距离、地点多样、规模足够大以用于监督学习的公开可用且带有能见度估计标签的数据集。本文介绍了一个新的数据集，它代表了美国联邦航空管理局（FAA）气象相机网络一年期数据收集活动的成果，非常适合此目的。我们还在应用三种常用方法和一个通用基线模型时，提供了一个基准测试，这些模型除了在我们的数据集上进行训练和测试外，还在三个公开可用数据集上进行训练和测试，并与最近批准的ASTM标准进行了比较。", "summary": "本文推出了AIR-VIEW数据集，这是一个专为航空气象能见度估计设计的图像存储库，旨在解决当前缺乏高质量公开数据集的问题。该数据集汇集了FAA气象相机网络一年的数据。此外，论文还提出了一个基准测试，评估了三种常用方法和一个通用基线模型在AIR-VIEW及其他公开数据集上的性能，并与最新的ASTM标准进行了比较，为该领域的研究提供了重要的资源和评估框架。", "keywords": "航空气象, 能见度估计, 图像数据集, 机器学习, 基准测试", "comments": "该论文通过创建AIR-VIEW数据集，解决了航空气象能见度估计领域一个关键的资源短缺问题，即缺乏适合机器学习的带标签图像数据。其创新性在于提供了一个大规模、多样化且与航空相关的图像数据集，并建立了评估模型性能的基准，这对于推动该领域低成本气象解决方案的发展具有重要意义。"}}
{"id": "2506.20818", "title": "Demystifying Distributed Training of Graph Neural Networks for Link Prediction", "authors": ["Xin Huang", "Chul-Ho Lee"], "summary": "Graph neural networks (GNNs) are powerful tools for solving graph-related\nproblems. Distributed GNN frameworks and systems enhance the scalability of\nGNNs and accelerate model training, yet most are optimized for node\nclassification. Their performance on link prediction remains underexplored.\nThis paper demystifies distributed training of GNNs for link prediction by\ninvestigating the issue of performance degradation when each worker trains a\nGNN on its assigned partitioned subgraph without having access to the entire\ngraph. We discover that the main sources of the issue come from not only the\ninformation loss caused by graph partitioning but also the ways of drawing\nnegative samples during model training. While sharing the complete graph\ninformation with each worker resolves the issue and preserves link prediction\naccuracy, it incurs a high communication cost. We propose SpLPG, which\neffectively leverages graph sparsification to mitigate the issue of performance\ndegradation at a reduced communication cost. Experiment results on several\npublic real-world datasets demonstrate the effectiveness of SpLPG, which\nreduces the communication overhead by up to about 80% while mostly preserving\nlink prediction accuracy.", "comment": "Accepted by IEEE ICDCS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20818v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20818v1", "AI": {"title_translation": "揭秘用于链接预测的图神经网络分布式训练", "tldr": "本文揭示了分布式图神经网络在链接预测中性能下降的原因，并提出了SpLPG方法，通过图稀疏化有效降低通信成本并保持预测精度。", "motivation": "现有的分布式图神经网络框架大多针对节点分类进行优化，而其在链接预测任务上的性能尚未得到充分探索。当每个工作节点在没有完整图访问权限的情况下，仅在其分配的子图上训练GNN时，会遇到性能下降的问题。", "method": "本文首先研究了分布式GNN训练中链接预测性能下降的问题，发现其主要原因不仅包括图划分导致的信息丢失，还包括负样本的抽取方式。为了解决这个问题，本文提出了SpLPG方法，该方法有效利用图稀疏化来缓解性能下降问题，同时降低了通信成本。", "result": "在多个公共真实世界数据集上的实验结果表明，SpLPG方法有效降低了高达约80%的通信开销，同时基本保持了链接预测的准确性。", "conclusion": "通过揭示分布式GNN在链接预测中性能下降的原因，并提出SpLPG方法，本文成功地在保持链接预测准确性的同时，显著降低了分布式训练的通信成本。", "translation": "图神经网络（GNN）是解决图相关问题的强大工具。分布式GNN框架和系统增强了GNN的可扩展性并加速了模型训练，但大多数都针对节点分类进行了优化。它们在链接预测上的性能仍未得到充分探索。本文通过研究当每个工作节点在没有访问整个图的情况下，在其分配的子图上训练GNN时出现的性能下降问题，揭示了用于链接预测的GNN分布式训练。我们发现，该问题的主要来源不仅来自图划分造成的信息丢失，还来自模型训练过程中负样本的抽取方式。虽然与每个工作节点共享完整的图信息可以解决该问题并保持链接预测的准确性，但这会产生高昂的通信成本。我们提出了SpLPG，它有效地利用图稀疏化来缓解性能下降问题，同时降低了通信成本。在多个公共真实世界数据集上的实验结果表明了SpLPG的有效性，它将通信开销降低了约80%，同时基本保持了链接预测的准确性。", "summary": "本文探讨了分布式图神经网络（GNNs）在链接预测任务中遇到的性能下降问题。研究发现，性能下降主要源于图划分导致的信息丢失以及负样本抽取方式。虽然共享完整图信息可保持准确性，但通信成本高昂。为此，文章提出了SpLPG方法，该方法利用图稀疏化有效缓解了性能下降，同时显著降低了通信开销，在实验中将通信量减少了约80%并基本保持了链接预测的准确性。", "keywords": "图神经网络, 分布式训练, 链接预测, 图稀疏化, 性能下降", "comments": "本文深入分析了分布式GNN在链接预测中特有的性能下降问题，并提出了创新的图稀疏化方法SpLPG来解决通信开销与准确性之间的平衡。其贡献在于不仅揭示了问题根源（信息丢失和负采样），还提供了一个实用的、高效的解决方案，对大规模图数据上的链接预测任务具有重要意义。"}}
{"id": "2506.21453", "title": "Towards an Optimal Control Perspective of ResNet Training", "authors": ["Jens Püttschneider", "Simon Heilig", "Asja Fischer", "Timm Faulwasser"], "summary": "We propose a training formulation for ResNets reflecting an optimal control\nproblem that is applicable for standard architectures and general loss\nfunctions. We suggest bridging both worlds via penalizing intermediate outputs\nof hidden states corresponding to stage cost terms in optimal control. For\nstandard ResNets, we obtain intermediate outputs by propagating the state\nthrough the subsequent skip connections and the output layer. We demonstrate\nthat our training dynamic biases the weights of the unnecessary deeper residual\nlayers to vanish. This indicates the potential for a theory-grounded layer\npruning strategy.", "comment": "Accepted for presentation at the High-dimensional Learning Dynamics\n  (HiLD) workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21453v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21453v1", "AI": {"title_translation": "ResNet训练的最优控制视角", "tldr": "该论文提出了一种将ResNet训练视为最优控制问题的新方法，能够使不必要的深层权重消失，为理论驱动的层剪枝提供了潜力。", "motivation": "旨在为ResNet训练提出一种反映最优控制问题的公式，该公式适用于标准架构和通用损失函数，并有望实现基于理论的层剪枝策略。", "method": "提出了一种反映最优控制问题的ResNet训练公式。通过惩罚隐藏状态的中间输出来连接ResNet训练与最优控制，这些中间输出对应于最优控制中的阶段成本项。对于标准ResNet，中间输出通过后续的跳跃连接和输出层传播状态来获得。", "result": "训练动态促使不必要的更深残差层的权重消失。", "conclusion": "权重消失的现象表明了开发理论基础层剪枝策略的潜力。", "translation": "我们提出了一种ResNet训练公式，它反映了一个适用于标准架构和通用损失函数的最优控制问题。我们建议通过惩罚对应于最优控制中阶段成本项的隐藏状态的中间输出来连接这两个领域。对于标准ResNet，我们通过将状态通过后续的跳跃连接和输出层传播来获得中间输出。我们证明了我们的训练动态偏向于使不必要的更深残差层的权重消失。这表明了理论基础层剪枝策略的潜力。", "summary": "本文提出了一种将ResNet训练表述为最优控制问题的新颖方法。通过将中间隐藏状态的输出作为阶段成本进行惩罚，所提出的训练动态使得不必要的深层残差层权重消失，这为开发一种具有理论基础的层剪枝策略提供了可能性。", "keywords": "ResNet, 最优控制, 层剪枝, 训练动态, 深度学习", "comments": "该论文的创新之处在于将ResNet训练置于最优控制的框架下，这为理解和潜在改进网络架构，特别是在层剪枝方面，提供了理论基础。这种方法提供了一种超越经验剪枝方法的新颖视角。"}}
{"id": "2506.21445", "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21445v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21445v1", "AI": {"title_translation": "跨语言Text2Cypher：评估超越英语的基础模型", "tldr": "本文研究了基础LLM在多语言Text2Cypher任务上的表现，发现英语表现最佳，西班牙语次之，土耳其语最差，并发布了多语言测试集，强调了多语言查询生成中更具包容性评估的必要性。", "motivation": "现有的自然语言到数据库查询（如Text2Cypher）的研究主要集中在英语，对其他语言的评估有限，因此需要研究基础LLM在多语言Text2Cypher任务上的表现。", "method": "本文创建并发布了一个多语言测试集，通过将英语问题翻译成西班牙语和土耳其语，同时保留原始的Cypher查询，以实现公平的跨语言比较。研究使用标准化提示和指标评估了多个基础模型，并探索了将任务提示翻译成西班牙语和土耳其语的影响。", "result": "模型性能呈现一致的模式：英语最高，其次是西班牙语，土耳其语最低，这归因于训练数据可用性和语言特征的差异。此外，提示翻译对评估指标影响很小或没有影响。", "conclusion": "研究结果强调了在多语言查询生成中进行更具包容性的评估和开发的必要性。未来的工作包括模式本地化和跨不同语言的微调。", "translation": "大型语言模型的最新进展使得自然语言接口能够将用户问题转换为数据库查询，例如Text2SQL、Text2SPARQL和Text2Cypher。虽然这些接口增强了数据库的可访问性，但目前大多数研究只专注于英语，对其他语言的评估有限。本文研究了基础LLM在跨多种语言的Text2Cypher任务上的性能。我们通过将英语问题翻译成西班牙语和土耳其语，同时保留原始的Cypher查询，创建并发布了一个多语言测试集，从而实现了公平的跨语言比较。我们使用标准化提示和指标评估了多个基础模型。我们的结果显示出一致的性能模式：英语最高，其次是西班牙语，土耳其语最低。我们将其归因于训练数据可用性和语言特征的差异。此外，我们探讨了将任务提示翻译成西班牙语和土耳其语的影响。结果显示评估指标几乎没有变化，表明提示翻译影响很小。我们的发现强调了在多语言查询生成中进行更具包容性的评估和开发的必要性。未来的工作包括模式本地化和跨不同语言的微调。", "summary": "本文研究了基础大型语言模型在多语言Text2Cypher任务上的表现，旨在解决当前研究主要集中于英语的局限性。研究团队构建了一个包含英语、西班牙语和土耳其语的多语言测试集，并评估了多个基础模型。结果显示，模型在英语上的表现最佳，其次是西班牙语，土耳其语最差，这可能与训练数据和语言特性有关。研究还发现提示翻译对模型性能影响甚微。论文强调了在多语言查询生成领域进行更广泛评估和开发的必要性。", "keywords": "Text2Cypher, 多语言, 基础模型, 跨语言评估, 自然语言处理", "comments": "这篇论文通过构建多语言测试集并评估基础模型在非英语Text2Cypher任务上的性能，填补了现有研究的空白，具有重要意义。它揭示了当前LLM在多语言理解和生成方面的局限性，特别是对低资源语言。论文指出的训练数据和语言特性差异是导致性能差距的关键因素，为未来多语言LLM的开发提供了明确方向。其创新点在于首次系统性地评估了跨语言Text2Cypher，并明确指出提示翻译影响不大，这对于多语言NLP研究具有指导意义。"}}
{"id": "2506.21096", "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "comment": "Accepted by ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21096v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21096v1", "AI": {"title_translation": "DALR：多模态句子表示学习的双层对齐学习", "tldr": "DALR通过双层对齐学习解决了多模态句子表示中跨模态和模态内对齐的挑战，并在STS和TR任务上表现优异。", "motivation": "现有方法在粗粒度对齐上存在跨模态错位偏差和模态内语义发散的问题，导致句子表示质量显著下降。", "method": "提出DALR（多模态句子表示的双层对齐学习）。对于跨模态对齐，引入一致性学习模块，通过软化负样本并利用辅助任务的语义相似性实现细粒度对齐。对于模态内对齐，将排序蒸馏与全局模态内对齐学习结合，以捕获更复杂的句子关系并提高表示质量。", "result": "在语义文本相似性（STS）和迁移（TR）任务上的全面实验验证了方法的有效性，持续证明其优于最先进的基线。", "conclusion": "DALR通过双层对齐学习有效解决了多模态句子表示中跨模态错位偏差和模态内语义发散的挑战，显著提升了句子表示质量。", "translation": "以往的多模态句子表示学习方法取得了令人印象深刻的性能。然而，大多数方法侧重于在粗粒度级别对齐图像和文本，面临两个关键挑战：跨模态错位偏差和模态内语义发散，这些问题显著降低了句子表示质量。为了解决这些挑战，我们提出了DALR（多模态句子表示的双层对齐学习）。对于跨模态对齐，我们提出了一种一致性学习模块，该模块软化负样本并利用辅助任务的语义相似性来实现细粒度跨模态对齐。此外，我们认为句子关系超越了二元正负标签，呈现出更复杂的排序结构。为了更好地捕获这些关系并提高表示质量，我们将排序蒸馏与全局模态内对齐学习相结合。在语义文本相似性（STS）和迁移（TR）任务上的全面实验验证了我们方法的有效性，持续证明其优于最先进的基线。", "summary": "本文提出了DALR（多模态句子表示的双层对齐学习）模型，旨在解决现有方法在多模态句子表示学习中面临的跨模态错位偏差和模态内语义发散问题。DALR通过引入一致性学习模块实现细粒度跨模态对齐，并通过结合排序蒸馏和全局模态内对齐学习来捕获复杂的句子关系。实验结果表明，DALR在语义文本相似性（STS）和迁移（TR）任务上均优于现有最先进的方法。", "keywords": "多模态学习, 句子表示, 对齐学习, 语义相似性, 排序蒸馏", "comments": "该论文的创新点在于提出了双层对齐学习框架（DALR），同时解决了跨模态和模态内对齐的挑战。通过引入一致性学习和排序蒸馏，DALR能够实现更细致、更准确的多模态句子表示，对提升跨模态理解能力具有重要意义。"}}
{"id": "2506.21361", "title": "Efficient parameter-robust preconditioners for linear poroelasticity and elasticity in the primal formulation", "authors": ["Weizhang Huang", "Zhuoran Wang"], "summary": "Poroelasticity problems play an important role in various engineering,\ngeophysical, and biological applications. Their full discretization results in\na large-scale saddle-point system at each time step that is becoming singular\nfor locking cases and needs effective preconditioners for its fast iterative\nsolution. Instead of constructing spectrally equivalent ones, we develop\nnonsingular preconditioners so that the eigenvalues of the preconditioned\nsystem consist of a cluster around $1$ and an outlier in the order of\n$1/\\lambda$, where $\\lambda$ is a Lam\\'{e} constant that is large for locking\ncases. It is known that the convergence factor of GMRES is bounded by the\nradius of the cluster for this type of systems. Both two- and three-field block\ntriangular Schur complement preconditioners are studied. Upper bounds of the\nradius of the eigenvalue cluster for those systems are obtained and shown to be\nrelated to the inf-sup condition but independent of mesh size, time step, and\nlocking parameters, which reflects the robustness of the preconditioners with\nrespect to parameter variations. Moreover, the developed preconditioners do not\nneed to compute the Schur complement and neither require exact inversion of\ndiagonal blocks except the leading one. A locking-free weak Galerkin finite\nelement method and the implicit Euler scheme are used for the discretization of\nthe governing equation. Both two- and three-dimensional numerical results are\npresented to confirm the effectiveness and parameter-robustness of the\ndeveloped preconditioners.", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2506.21361v1", "categories": ["math.NA", "cs.NA", "65M60, 65F08, 65F10, 74F10"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21361v1", "AI": {"title_translation": "原始公式中线性多孔弹性与弹性的高效参数鲁棒预条件子", "tldr": "本文开发了一种高效的非奇异预条件子，用于解决多孔弹性问题中出现的鞍点系统，该预条件子在锁定情况下对参数变化具有鲁棒性，并能加速迭代求解。", "motivation": "多孔弹性问题在工程、地球物理和生物应用中扮演重要角色。其完全离散化会导致大规模鞍点系统，在锁定情况下可能变得奇异，需要有效的预条件子来实现快速迭代求解。", "method": "本文开发了非奇异预条件子，使得预条件系统的特征值围绕1形成簇，并有一个阶为$1/\\lambda$的离群值。研究了二场和三场块三角Schur补预条件子。所开发的预条件子无需计算Schur补，除了主对角块外也不需要精确求逆对角块。离散化采用了无锁定弱Galerkin有限元法和隐式Euler格式。", "result": "获得了特征值簇半径的上界，并表明其与inf-sup条件相关，但独立于网格尺寸、时间步长和锁定参数，这反映了预条件子对参数变化的鲁棒性。二维和三维数值结果均证实了所开发预条件子的有效性和参数鲁棒性。", "conclusion": "本文所开发的非奇异预条件子能够有效且参数鲁棒地解决多孔弹性问题中的大规模鞍点系统，尤其适用于锁定情况，从而加速了迭代求解过程。", "translation": "多孔弹性问题在各种工程、地球物理和生物应用中扮演重要角色。它们的完全离散化在每个时间步都产生一个大规模鞍点系统，该系统在锁定情况下会变得奇异，需要有效的预条件子来进行快速迭代求解。本文没有构建谱等效的预条件子，而是开发了非奇异预条件子，使得预条件系统的特征值由围绕1的簇和一个阶为$1/\\lambda$的离群值组成，其中$\\lambda$是Lamé常数，在锁定情况下较大。已知GMRES的收敛因子受此类系统特征值簇半径的限制。本文研究了二场和三场块三角Schur补预条件子。获得了这些系统特征值簇半径的上界，并表明其与inf-sup条件相关，但独立于网格尺寸、时间步长和锁定参数，这反映了预条件子对参数变化的鲁棒性。此外，所开发的预条件子无需计算Schur补，除了主对角块外也不需要精确求逆对角块。离散化控制方程采用了无锁定弱Galerkin有限元法和隐式Euler格式。二维和三维数值结果均证实了所开发预条件子的有效性和参数鲁棒性。", "summary": "本文针对多孔弹性问题离散化产生的大规模、在锁定情况下可能奇异的鞍点系统，提出了一种高效且参数鲁棒的非奇异预条件子。该预条件子使得预条件系统的特征值围绕1聚类，并通过理论分析和数值实验证明了其对网格尺寸、时间步长和锁定参数变化的鲁棒性。所提出的方法避免了Schur补的计算，并结合了无锁定弱Galerkin有限元法和隐式Euler格式，有效加速了迭代求解。", "keywords": "多孔弹性, 预条件子, 鞍点系统, 参数鲁棒性, 弱Galerkin有限元", "comments": "这项研究的创新之处在于开发了一种非奇异预条件子，它在解决多孔弹性问题中的鞍点系统时，能够克服锁定问题，并且对关键参数具有鲁棒性。其重要性在于提供了一种高效的数值方法，能够显著加速此类复杂物理问题的模拟，对于工程、地球物理和生物领域具有实际应用价值。特别是其无需精确计算Schur补的特性降低了计算成本。"}}
{"id": "2506.21347", "title": "Real-time Terrain Analysis for Off-road Autonomous Vehicles", "authors": ["Edwina Lewis", "Aditya Parameshwaran", "Laura Redmond", "Yue Wang"], "summary": "This research addresses critical autonomous vehicle control challenges\narising from road roughness variation, which induces course deviations and\npotential loss of road contact during steering operations. We present a novel\nreal-time road roughness estimation system employing Bayesian calibration\nmethodology that processes axle accelerations to predict terrain roughness with\nquantifiable confidence measures. The technical framework integrates a Gaussian\nprocess surrogate model with a simulated half-vehicle model, systematically\nprocessing vehicle velocity and road surface roughness parameters to generate\ncorresponding axle acceleration responses. The Bayesian calibration routine\nperforms inverse estimation of road roughness from observed accelerations and\nvelocities, yielding posterior distributions that quantify prediction\nuncertainty for adaptive risk management. Training data generation utilizes\nLatin Hypercube sampling across comprehensive velocity and roughness parameter\nspaces, while the calibrated model integrates seamlessly with a Simplex\ncontroller architecture to dynamically adjust velocity limits based on\nreal-time roughness predictions. Experimental validation on stochastically\ngenerated surfaces featuring varying roughness regions demonstrates robust\nreal-time characterization capabilities, with the integrated Simplex control\nstrategy effectively enhancing autonomous vehicle operational safety through\nproactive surface condition response. This innovative Bayesian framework\nestablishes a comprehensive foundation for mitigating roughness-related\noperational risks while simultaneously improving efficiency and safety margins\nin autonomous vehicle systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21347v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21347v1", "AI": {"title_translation": "越野自动驾驶汽车的实时地形分析", "tldr": "本研究提出了一种新颖的实时路面粗糙度估计系统，该系统采用贝叶斯校准方法，通过处理车轴加速度来预测地形粗糙度，并量化置信度，以增强越野自动驾驶汽车的运行安全性和效率。", "motivation": "解决自动驾驶汽车因路面粗糙度变化而导致的控制挑战，这些变化可能在转向操作期间引起路线偏差并可能导致与路面失去接触。", "method": "提出了一种新颖的实时路面粗糙度估计系统，采用贝叶斯校准方法处理车轴加速度。该系统集成了高斯过程替代模型与模拟半车辆模型，系统地处理车辆速度和路面粗糙度参数以生成相应的车轴加速度响应。贝叶斯校准程序从观测到的加速度和速度中逆向估计路面粗糙度，产生量化预测不确定性的后验分布。训练数据通过拉丁超立方体采样生成，校准后的模型与Simplex控制器架构无缝集成，根据实时粗糙度预测动态调整速度限制。", "result": "在具有不同粗糙度区域的随机生成表面上的实验验证表明，该系统具有强大的实时表征能力。集成的Simplex控制策略通过主动响应路面状况，有效增强了自动驾驶汽车的运行安全性。", "conclusion": "这种创新的贝叶斯框架为减轻与粗糙度相关的操作风险奠定了全面的基础，同时提高了自动驾驶系统的效率和安全裕度。", "translation": "这项研究解决了由路面粗糙度变化引起的自动驾驶汽车关键控制挑战，这些变化在转向操作期间会导致路线偏差和潜在的路面接触损失。我们提出了一种新颖的实时路面粗糙度估计系统，该系统采用贝叶斯校准方法，通过处理车轴加速度来预测地形粗糙度，并量化置信度。该技术框架将高斯过程替代模型与模拟半车辆模型相结合，系统地处理车辆速度和路面粗糙度参数以生成相应的车轴加速度响应。贝叶斯校准程序从观测到的加速度和速度中逆向估计路面粗糙度，产生量化预测不确定性的后验分布，以实现自适应风险管理。训练数据生成利用拉丁超立方体采样在全面的速度和粗糙度参数空间中进行，而校准后的模型与Simplex控制器架构无缝集成，根据实时粗糙度预测动态调整速度限制。在具有不同粗糙度区域的随机生成表面上的实验验证表明，该系统具有强大的实时表征能力，集成的Simplex控制策略通过主动响应路面状况，有效增强了自动驾驶汽车的运行安全性。这种创新的贝叶斯框架为减轻与粗糙度相关的操作风险奠定了全面的基础，同时提高了自动驾驶系统中的效率和安全裕度。", "summary": "本研究提出了一种用于越野自动驾驶汽车的实时路面粗糙度估计系统。该系统利用贝叶斯校准方法和高斯过程替代模型，通过分析车轴加速度和车辆速度来预测地形粗糙度及其不确定性。通过与Simplex控制器集成，该系统能动态调整速度限制以应对路面状况，从而提高自动驾驶汽车在复杂地形中的运行安全性和效率。实验验证表明其在实时表征和风险缓解方面的有效性。", "keywords": "实时地形分析, 自动驾驶汽车, 路面粗糙度估计, 贝叶斯校准, Simplex控制器", "comments": "该论文提出了一种创新的贝叶斯框架，用于实时路面粗糙度估计，其核心在于将贝叶斯校准与高斯过程替代模型相结合，并量化了预测不确定性，这对于自适应风险管理至关重要。将该系统与Simplex控制器集成，实现了对车辆速度的动态调整，显著提升了自动驾驶车辆在复杂地形下的安全性和效率。其创新性在于为解决越野环境中的路面粗糙度挑战提供了一个全面的、数据驱动的解决方案。"}}
{"id": "2506.20947", "title": "Hierarchical Sub-action Tree for Continuous Sign Language Recognition", "authors": ["Dejie Yang", "Zhu Xu", "Xinjie Gao", "Yang Liu"], "summary": "Continuous sign language recognition (CSLR) aims to transcribe untrimmed\nvideos into glosses, which are typically textual words. Recent studies indicate\nthat the lack of large datasets and precise annotations has become a bottleneck\nfor CSLR due to insufficient training data. To address this, some works have\ndeveloped cross-modal solutions to align visual and textual modalities.\nHowever, they typically extract textual features from glosses without fully\nutilizing their knowledge. In this paper, we propose the Hierarchical\nSub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge\nwith visual representation learning. By incorporating gloss-specific knowledge\nfrom large language models, our approach leverages textual information more\neffectively. Specifically, we construct an HST for textual information\nrepresentation, aligning visual and textual modalities step-by-step and\nbenefiting from the tree structure to reduce computational complexity.\nAdditionally, we impose a contrastive alignment enhancement to bridge the gap\nbetween the two modalities. Experiments on four datasets (PHOENIX-2014,\nPHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the\neffectiveness of our HST-CSLR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20947v1", "categories": ["cs.CV", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20947v1", "AI": {"title_translation": "用于连续手语识别的分层子动作树", "tldr": "本文提出了一种名为分层子动作树（HST）的新方法，通过结合大型语言模型的词汇知识来提高连续手语识别（CSLR）的性能，并有效对齐视觉和文本模态。", "motivation": "连续手语识别（CSLR）面临数据集不足和标注不精确的问题，导致训练数据不足。现有跨模态解决方案未能充分利用词汇知识。", "method": "本文提出了分层子动作树（HST），称为HST-CSLR，以有效地将词汇知识与视觉表示学习相结合。通过从大型语言模型中引入词汇特定知识，构建HST进行文本信息表示，逐步对齐视觉和文本模态，并利用树结构降低计算复杂度。此外，还引入对比对齐增强以弥合两种模态之间的差距。", "result": "在PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture四个数据集上的实验证明了HST-CSLR的有效性。", "conclusion": "本文提出的HST-CSLR方法通过有效利用词汇知识和分层对齐策略，显著提升了连续手语识别的性能。", "translation": "连续手语识别（CSLR）旨在将未剪辑的视频转录为通常为文本词汇的词汇。最近的研究表明，由于训练数据不足，大型数据集和精确标注的缺乏已成为CSLR的瓶颈。为解决此问题，一些工作开发了跨模态解决方案以对齐视觉和文本模态。然而，它们通常从词汇中提取文本特征，而未充分利用其知识。在本文中，我们提出了分层子动作树（Hierarchical Sub-action Tree，简称HST），称为HST-CSLR，以有效地将词汇知识与视觉表示学习相结合。通过整合来自大型语言模型的词汇特定知识，我们的方法更有效地利用了文本信息。具体来说，我们构建了一个用于文本信息表示的HST，逐步对齐视觉和文本模态，并受益于树结构以降低计算复杂度。此外，我们施加了对比对齐增强以弥合两种模态之间的差距。在四个数据集（PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture）上的实验证明了我们的HST-CSLR的有效性。", "summary": "本文针对连续手语识别（CSLR）中数据不足和词汇知识利用不足的问题，提出了一种名为分层子动作树（HST）的新方法，即HST-CSLR。该方法通过整合大型语言模型的词汇知识，并构建HST来表示文本信息，实现视觉与文本模态的逐步对齐，并利用树结构降低计算复杂度。同时，引入对比对齐增强以弥合模态间差距。实验结果表明，HST-CSLR在多个数据集上表现出有效性。", "keywords": "连续手语识别, 分层子动作树, 跨模态对齐, 词汇知识, 大型语言模型", "comments": "该论文的创新点在于提出了分层子动作树（HST）结构，并结合大型语言模型来更有效地利用词汇知识，解决CSLR中长期存在的训练数据不足和模态对齐问题。通过树结构降低计算复杂度的设计也具有实用价值。该方法为CSLR领域提供了一个新的、高效的跨模态学习范式。"}}
{"id": "2506.20849", "title": "Learning-Based Resource Management in Integrated Sensing and Communication Systems", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "In this paper, we tackle the task of adaptive time allocation in integrated\nsensing and communication systems equipped with radar and communication units.\nThe dual-functional radar-communication system's task involves allocating dwell\ntimes for tracking multiple targets and utilizing the remaining time for data\ntransmission towards estimated target locations. We introduce a novel\nconstrained deep reinforcement learning (CDRL) approach, designed to optimize\nresource allocation between tracking and communication under time budget\nconstraints, thereby enhancing target communication quality. Our numerical\nresults demonstrate the efficiency of our proposed CDRL framework, confirming\nits ability to maximize communication quality in highly dynamic environments\nwhile adhering to time constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20849v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20849v1", "AI": {"title_translation": "集成感知与通信系统中基于学习的资源管理", "tldr": "本文提出了一种新颖的约束深度强化学习（CDRL）方法，用于优化集成感知与通信系统中雷达跟踪与数据传输之间的时间分配，以提高通信质量。", "motivation": "在集成感知与通信系统中，需要解决自适应时间分配的任务，特别是在雷达和通信单元之间。目标是优化跟踪和通信之间的资源分配，以在时间预算限制下增强目标通信质量。", "method": "本文引入了一种新颖的约束深度强化学习（CDRL）方法，旨在优化在时间预算约束下跟踪和通信之间的资源分配。", "result": "数值结果表明，所提出的CDRL框架是有效的，并证实了它在高度动态环境中遵守时间限制的同时最大化通信质量的能力。", "conclusion": "所提出的CDRL框架能够有效地在集成感知与通信系统中，在时间约束下最大化通信质量，从而优化雷达跟踪与数据传输的资源分配。", "translation": "在本文中，我们解决了配备雷达和通信单元的集成感知与通信系统中的自适应时间分配任务。双功能雷达通信系统的任务涉及为跟踪多个目标分配驻留时间，并利用剩余时间向估计的目标位置进行数据传输。我们引入了一种新颖的约束深度强化学习（CDRL）方法，旨在优化在时间预算约束下跟踪和通信之间的资源分配，从而提高目标通信质量。我们的数值结果证明了我们提出的CDRL框架的效率，证实了它在高度动态环境中遵守时间限制的同时最大化通信质量的能力。", "summary": "本研究提出了一种创新的约束深度强化学习（CDRL）方法，以解决集成感知与通信系统中的自适应时间分配问题。该方法旨在优化雷达跟踪和数据传输之间的时间资源分配，确保在时间预算限制下最大限度地提高目标通信质量。数值结果验证了该CDRL框架的有效性，证明了其在动态环境中实现高效资源管理和提升通信性能的能力。", "keywords": "集成感知与通信, 资源管理, 深度强化学习, 时间分配, 雷达通信", "comments": "本文的创新点在于将约束深度强化学习应用于集成感知与通信系统中的时间资源管理问题，有效地平衡了雷达跟踪和数据传输的需求。这种方法有望在未来动态和资源受限的通信系统中发挥重要作用。"}}
{"id": "2506.21508", "title": "skLEP: A Slovak General Language Understanding Benchmark", "authors": ["Marek Šuppa", "Andrej Ridzik", "Daniel Hládek", "Tomáš Javůrek", "Viktória Ondrejová", "Kristína Sásiková", "Martin Tamajka", "Marián Šimko"], "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21508v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21508v1", "AI": {"title_translation": "skLEP：一个斯洛伐克通用语言理解基准", "tldr": "引入了skLEP，首个用于评估斯洛伐克自然语言理解模型的综合基准，包含九项任务，并发布了数据集、工具包和排行榜。", "motivation": "为了解决斯洛伐克自然语言理解（NLU）模型缺乏专门评估基准的问题，从而提供一个全面的模型能力评估工具，并促进该领域的研究。", "method": "研究人员引入了skLEP，一个专门为评估斯洛伐克NLU模型设计的综合基准。他们汇编了九项涵盖词汇级、句子对和文档级挑战的多样化任务。为了创建这个基准，他们整理了新的原创斯洛伐克语数据集，并精心翻译了已有的英语NLU资源。同时，他们还使用skLEP任务对各种斯洛伐克语特有、多语言和英语预训练语言模型进行了首次系统而广泛的评估。", "result": "引入了skLEP，这是第一个专门用于评估斯洛伐克自然语言理解（NLU）模型的综合基准。该基准包含九项多样化的任务，涵盖词汇级、句子对和文档级挑战。研究人员还对斯洛伐克语特有、多语言和英语预训练语言模型进行了首次系统而广泛的评估。此外，他们还发布了完整的基准数据、一个用于模型微调和评估的开源工具包，以及一个公共排行榜。", "conclusion": "该研究引入了skLEP，一个全面的斯洛伐克语NLU评估基准，并通过发布完整数据、开源工具包和公共排行榜，旨在促进斯洛伐克语NLU领域的可复现性并推动未来的研究。", "translation": "在这项工作中，我们介绍了skLEP，这是第一个专门为评估斯洛伐克自然语言理解（NLU）模型而设计的综合基准。我们编译了skLEP，使其包含九项多样化的任务，涵盖了词汇级、句子对和文档级挑战，从而提供了对模型能力的全面评估。为了创建这个基准，我们整理了为斯洛伐克语量身定制的全新原创数据集，并精心翻译了已有的英语NLU资源。在这篇论文中，我们还使用skLEP任务，首次对各种斯洛伐克语特有、多语言和英语预训练语言模型进行了系统而广泛的评估。最后，我们还发布了完整的基准数据、一个便于模型微调和评估的开源工具包，以及一个位于https://github.com/slovak-nlp/sklep的公共排行榜，以期促进斯洛伐克语NLU领域的可复现性并推动未来的研究。", "summary": "本论文介绍了skLEP，首个针对斯洛伐克语自然语言理解（NLU）模型的综合性评估基准。skLEP包含九项多样化任务，涵盖词汇、句子和文档级别，旨在全面评估模型能力。为构建该基准，研究者创建了新的斯洛伐克语数据集并翻译了现有英语NLU资源。论文还首次系统评估了多种斯洛伐克语、多语言及英语预训练语言模型。为促进可复现性和未来研究，完整的基准数据、开源工具包和公共排行榜已对外发布。", "keywords": "斯洛伐克语, 自然语言理解, 基准, skLEP, 语言模型评估", "comments": "这项工作的创新之处在于首次为斯洛伐克语NLU领域构建了一个全面且多样的评估基准，填补了该语言在NLU研究中的空白。通过提供九项不同粒度的任务、原创数据集和翻译资源，skLEP为斯洛伐克语NLU模型的发展提供了重要的基础设施。此外，开源工具包和公共排行榜的发布，极大地促进了研究的可复现性和社区协作，对推动斯洛伐克语NLU的未来研究具有重要意义。"}}
{"id": "2506.21098", "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "pdf_url": "http://arxiv.org/pdf/2506.21098v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21098v1", "AI": {"title_translation": "ComRAG：面向工业实时社区问答的动态向量存储检索增强生成", "tldr": "ComRAG是一个为工业实时社区问答设计的检索增强生成框架，它通过基于质心的记忆机制有效整合动态和静态知识，并在实际工业数据集上显著提升了性能。", "motivation": "社区问答（CQA）平台是重要的知识库，但现有方法在实时场景中未能充分利用外部知识、动态历史问答上下文，或缺乏适用于工业部署的记忆机制，导致难以有效利用历史交互和领域知识。", "method": "本文提出了ComRAG，一个用于实时工业CQA的检索增强生成框架。它通过一个为检索、生成和高效存储设计的基于质心的记忆机制，将静态知识与动态历史问答对相结合。", "result": "在三个工业CQA数据集上的评估显示，ComRAG持续优于所有基线：向量相似性提升高达25.9%，延迟降低8.7%至23.3%，迭代过程中块增长从20.23%降至2.06%。", "conclusion": "ComRAG通过其创新的动态向量存储和基于质心的记忆机制，有效解决了工业实时CQA中知识利用和效率的挑战，并在多项关键指标上取得了显著改进。", "translation": "社区问答（CQA）平台可被视为社区中重要的知识库，但如何有效利用历史交互和领域知识在实时场景中仍然是一个挑战。现有方法往往未充分利用外部知识，未能整合动态的历史问答上下文，或缺乏适用于工业部署的记忆机制。我们提出了ComRAG，一个用于实时工业CQA的检索增强生成框架，它通过一个为检索、生成和高效存储设计的基于质心的记忆机制，将静态知识与动态历史问答对相结合。在三个工业CQA数据集上的评估显示，ComRAG持续优于所有基线——向量相似性提升高达25.9%，延迟降低8.7%至23.3%，迭代过程中块增长从20.23%降至2.06%。", "summary": "本文提出了ComRAG，一个专门针对工业实时社区问答（CQA）的检索增强生成框架。该框架通过引入基于质心的记忆机制，有效整合了静态领域知识与动态历史问答对，旨在解决现有方法在知识利用、动态上下文整合和工业部署适应性方面的不足。实验结果表明，ComRAG在工业CQA数据集上显著优于基线，在向量相似性、延迟和数据块增长率方面均取得了显著改进，证明了其在实际应用中的高效性和优越性。", "keywords": "社区问答, 检索增强生成, 动态向量存储, 实时系统, 工业应用", "comments": "ComRAG的创新之处在于其动态向量存储和基于质心的记忆机制，这使其能够有效处理实时CQA场景中的动态知识更新和高效存储问题。其在工业数据集上的显著性能提升（尤其是在降低延迟和块增长方面）表明了其在实际工业部署中的巨大潜力。该方法为解决大规模、实时知识库的挑战提供了一个有前景的方向。"}}
{"id": "2506.21395", "title": "Optimal solutions employing an algebraic Variational Multiscale approach Part II: Application to Navier-Stokes", "authors": ["Suyash Shrestha", "Marc Gerritsma", "Gonzalo Rubio", "Steven Hulshoff", "Esteban Ferrer"], "summary": "This work presents a nonlinear extension of the high-order discretisation\nframework based on the Variational Multiscale (VMS) method previously\nintroduced for steady linear problems. Building on the concept of an optimal\nprojector defined via the symmetric part of the governing operator, we\ngeneralise the formulation to treat the 2D incompressible Navier-Stokes\nequations. The arroach maintains a clear separation between the resolved and\nunresolved scales, with the fine-scale contributions approximated through the\napproximate Fine-Scale Greens' function of the associated symmetric operator.\nThis enables a consistent variational treatment of the nonlinearity while\npreserving high-order accuracy. We show that the method yields numerical\nsolutions that closely approximate the optimal projection of the\ncontinuous/highly resolved solution and inherits desirable conservation\nproperties. Numerical results confirm the framework's robustness, accuracy, and\nits potential for application to a broad class of nonlinear multiscale\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21395v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21395v1", "AI": {"title_translation": "采用代数变分多尺度方法的最优解 第二部分：应用于纳维-斯托克斯方程", "tldr": "本文将高阶变分多尺度(VMS)离散化框架非线性扩展，并应用于二维不可压缩纳维-斯托克斯方程，结果显示该方法鲁棒、准确且具有良好的守恒性。", "motivation": "旨在将之前针对稳态线性问题引入的基于变分多尺度（VMS）方法的高阶离散化框架扩展到非线性问题，特别是处理二维不可压缩纳维-斯托克斯方程。", "method": "本文提出了一种基于变分多尺度（VMS）方法的高阶离散化框架的非线性扩展。该方法基于通过控制算子的对称部分定义的“最优投影器”概念，将公式推广以处理二维不可压缩纳维-斯托克斯方程。它保持了已解析尺度和未解析尺度之间的清晰分离，并通过相关对称算子的近似精细尺度格林函数来近似精细尺度贡献。", "result": "该方法能够对非线性进行一致的变分处理，同时保持高阶精度。数值解能紧密近似连续/高分辨率解的最优投影，并继承了理想的守恒性。数值结果证实了该框架的鲁棒性、准确性及其在广泛的非线性多尺度问题中的应用潜力。", "conclusion": "所提出的高阶变分多尺度方法在处理非线性多尺度问题，特别是纳维-斯托克斯方程时，表现出优异的鲁棒性、准确性及守恒性，具有广泛的应用前景。", "translation": "这项工作提出了高阶离散化框架的非线性扩展，该框架基于先前为稳态线性问题引入的变分多尺度（VMS）方法。我们基于通过控制算子的对称部分定义的最优投影器的概念，将该公式推广以处理二维不可压缩纳维-斯托克斯方程。该方法在已解析尺度和未解析尺度之间保持清晰的分离，其中精细尺度贡献通过相关对称算子的近似精细尺度格林函数进行近似。这使得非线性能够得到一致的变分处理，同时保持高阶精度。我们表明，该方法产生的数值解能紧密近似连续/高分辨率解的最优投影，并继承了理想的守恒性。数值结果证实了该框架的鲁棒性、准确性及其在广泛的非线性多尺度问题中的应用潜力。", "summary": "本文提出了一种高阶变分多尺度（VMS）方法的非线性扩展，用于求解二维不可压缩纳维-斯托克斯方程。该方法通过最优投影器概念，实现了已解析和未解析尺度间的清晰分离，并利用近似精细尺度格林函数处理精细尺度贡献，从而实现了高精度和一致的非线性变分处理。数值结果验证了其鲁棒性、准确性及良好的守恒特性，显示了其在非线性多尺度问题中的广泛应用潜力。", "keywords": "变分多尺度, 纳维-斯托克斯方程, 非线性问题, 高阶离散化, 数值模拟", "comments": "该论文通过将变分多尺度方法扩展到非线性问题，特别是在纳维-斯托克斯方程上的应用，展示了其在流体力学数值模拟领域的创新性。其保持高阶精度和良好守恒性的能力，以及对复杂非线性多尺度问题的处理潜力，是其重要贡献。"}}
{"id": "2506.21539", "title": "WorldVLA: Towards Autoregressive Action World Model", "authors": ["Jun Cen", "Chaohui Yu", "Hangjie Yuan", "Yuming Jiang", "Siteng Huang", "Jiayan Guo", "Xin Li", "Yibing Song", "Hao Luo", "Fan Wang", "Deli Zhao", "Hao Chen"], "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.", "comment": "Code: https://github.com/alibaba-damo-academy/WorldVLA", "pdf_url": "http://arxiv.org/pdf/2506.21539v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21539v1", "AI": {"title_translation": "WorldVLA：迈向自回归动作世界模型", "tldr": "WorldVLA是一种新的自回归动作世界模型，它将VLA模型和世界模型集成在一个框架中，实现动作和图像的理解与生成。它展示了动作模型和世界模型之间的相互增强作用，并提出了一种注意力掩码策略来解决自回归动作生成中的性能下降问题。", "motivation": "本文旨在通过将视觉-语言-动作（VLA）模型和世界模型整合到一个单一框架中，实现动作和图像理解与生成的统一。其目的是让世界模型通过学习环境的底层物理来改进动作生成，同时让动作模型通过基于图像观察生成动作来辅助视觉理解和生成。", "method": "WorldVLA将视觉-语言-动作（VLA）模型和世界模型整合到一个单一的自回归框架中。世界模型通过利用动作和图像理解来预测未来图像，以学习环境物理。动作模型则根据图像观察生成后续动作。为了解决自回归动作生成中性能下降的问题，本文提出了一种注意力掩码策略，即在生成当前动作时选择性地掩盖先前的动作。", "result": "WorldVLA的性能优于独立的动作模型和世界模型，突出了两者之间的相互增强作用。研究发现，动作模型在自回归生成动作序列时性能会下降，这归因于其动作预测泛化能力有限和错误传播。所提出的注意力掩码策略在动作块生成任务中显著提升了性能。", "conclusion": "本文提出了WorldVLA，一个有效的集成模型，用于统一动作和图像的理解与生成。它强调了结合动作模型和世界模型的优势，并通过新颖的注意力掩码策略解决了自回归动作生成中的挑战。", "translation": "我们提出了WorldVLA，一个自回归动作世界模型，它统一了动作和图像的理解与生成。我们的WorldVLA将视觉-语言-动作（VLA）模型和世界模型整合到一个单一框架中。世界模型通过利用动作和图像理解来预测未来的图像，目的是学习环境的底层物理以改进动作生成。同时，动作模型根据图像观察生成后续动作，辅助视觉理解，进而帮助世界模型的视觉生成。我们证明了WorldVLA优于独立的动作和世界模型，突出了世界模型和动作模型之间的相互增强。此外，我们发现动作模型在自回归生成动作序列时性能会下降。这种现象可以归因于模型动作预测的泛化能力有限，导致错误从早期动作传播到后续动作。为了解决这个问题，我们提出了一种注意力掩码策略，在生成当前动作时选择性地掩盖先前的动作，这在动作块生成任务中显示出显著的性能改进。", "summary": "WorldVLA是一种新颖的自回归动作世界模型，它在一个统一的框架中整合了视觉-语言-动作（VLA）模型和世界模型，以实现动作和图像的理解与生成。该模型通过世界模型预测未来图像来学习环境物理，并由动作模型生成基于图像观察的动作。研究表明WorldVLA优于独立的动作和世界模型，突出了两者的相互促进作用。针对自回归动作生成中存在的性能下降问题，作者提出了一种注意力掩码策略，通过选择性地掩盖先前的动作来有效提升动作块生成任务的性能。", "keywords": "WorldVLA, 自回归模型, 动作世界模型, 视觉-语言-动作, 注意力掩码", "comments": "这篇论文通过结合VLA和世界模型，提出了一种有趣且创新的集成方法，实现了动作和图像理解与生成的统一。动作模型和世界模型之间相互增强的概念具有重要意义。此外，识别并解决了自回归动作生成问题，并提出了注意力掩码策略，这展示了实用的问题解决能力。这项工作有助于实现更鲁棒和泛化能力更强的智能体学习。"}}
{"id": "2506.20960", "title": "OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs", "authors": ["Yiman Zhang", "Ziheng Luo", "Qiangyu Yan", "Wei He", "Borui Jiang", "Xinghao Chen", "Kai Han"], "summary": "In this paper, we introduce OmniEval, a benchmark for evaluating\nomni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,\nand textual inputs. Compared with existing benchmarks, our OmniEval has several\ndistinctive features: (i) Full-modal collaboration: We design evaluation tasks\nthat highlight the strong coupling between audio and video, requiring models to\neffectively leverage the collaborative perception of all modalities; (ii)\nDiversity of videos: OmniEval includes 810 audio-visual synchronized videos,\n285 Chinese videos and 525 English videos; (iii) Diversity and granularity of\ntasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended\nquestions and 1205 multiple-choice questions. These questions are divided into\n3 major task types and 12 sub-task types to achieve comprehensive evaluation.\nAmong them, we introduce a more granular video localization task named\nGrounding. Then we conduct experiments on OmniEval with several omni-modality\nmodels. We hope that our OmniEval can provide a platform for evaluating the\nability to construct and understand coherence from the context of all\nmodalities. Codes and data could be found at https://omnieval.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20960v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20960v1", "AI": {"title_translation": "OmniEval: 一个用于评估视觉、听觉和文本输入的万模态模型的基准", "tldr": "OmniEval是一个新的基准测试，用于评估能处理视觉、听觉和文本输入的万模态模型，通过设计强调全模态协作、视频多样性和任务多样性与细粒度的评估任务。", "motivation": "为了有效评估MiniCPM-O 2.6等万模态模型处理视觉、听觉和文本输入的能力，并弥补现有基准的不足。", "method": "本文介绍了OmniEval，一个用于评估万模态模型的基准。其特点包括：(i) 全模态协作：设计强调音频和视频强耦合的任务，要求模型有效利用所有模态的协同感知；(ii) 视频多样性：包含810个音视频同步视频（285个中文，525个英文）；(iii) 任务多样性和细粒度：包含2617个问答对（1412个开放式，1205个多项选择），分为3种主要任务类型和12种子任务类型，并引入了更细粒度的视频定位任务“Grounding”。随后，使用OmniEval对多个万模态模型进行了实验。", "result": "本文成功构建并推出了OmniEval基准，该基准具有全模态协作、视频多样性和任务多样性与细粒度等独特特征，并已用于对多种万模态模型进行实验评估。", "conclusion": "OmniEval旨在为评估模型从所有模态上下文中构建和理解连贯性的能力提供一个平台。", "translation": "在本文中，我们介绍了OmniEval，一个用于评估MiniCPM-O 2.6等万模态模型的基准，该基准包含视觉、听觉和文本输入。与现有基准相比，我们的OmniEval具有几个显著特点：(i) 全模态协作：我们设计的评估任务强调音频和视频之间的强耦合，要求模型有效利用所有模态的协同感知；(ii) 视频多样性：OmniEval包含810个音视频同步视频，其中285个中文视频和525个英文视频；(iii) 任务的多样性和细粒度：OmniEval包含2617个问答对，包括1412个开放式问题和1205个多项选择问题。这些问题分为3种主要任务类型和12种子任务类型，以实现全面评估。其中，我们引入了一个更细粒度的视频定位任务，名为Grounding。然后，我们使用OmniEval对几个万模态模型进行了实验。我们希望OmniEval能够为评估从所有模态上下文中构建和理解连贯性的能力提供一个平台。代码和数据可在https://omnieval.github.io/找到。", "summary": "本文提出了OmniEval，一个针对万模态模型（如MiniCPM-O 2.6）设计的综合评估基准，该基准支持视觉、听觉和文本输入。与现有基准不同，OmniEval强调全模态协作，包含多样化的音视频数据（810个视频，中英文皆有），并提供了细粒度的任务类型（2617个问答对，涵盖12种子任务，包括新的Grounding任务）。该基准旨在为评估模型理解和构建多模态上下文连贯性的能力提供平台。", "keywords": "万模态模型, 评估基准, 视觉, 听觉, 文本", "comments": "OmniEval的创新之处在于其强调“全模态协作”，这对于评估真正意义上的万模态模型至关重要，因为它迫使模型理解不同模态间的深层关联而非简单堆叠。同时，其任务的多样性和细粒度，特别是引入更精细的视频定位任务，提升了评估的全面性和深度。该基准有望推动万模态模型在复杂真实世界感知任务中的发展。"}}
{"id": "2506.20853", "title": "Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management", "authors": ["Ziyang Lu", "Subodh Kalia", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "The time allocation problem in multi-function cognitive radar systems focuses\non the trade-off between scanning for newly emerging targets and tracking the\npreviously detected targets. We formulate this as a multi-objective\noptimization problem and employ deep reinforcement learning to find\nPareto-optimal solutions and compare deep deterministic policy gradient (DDPG)\nand soft actor-critic (SAC) algorithms. Our results demonstrate the\neffectiveness of both algorithms in adapting to various scenarios, with SAC\nshowing improved stability and sample efficiency compared to DDPG. We further\nemploy the NSGA-II algorithm to estimate an upper bound on the Pareto front of\nthe considered problem. This work contributes to the development of more\nefficient and adaptive cognitive radar systems capable of balancing multiple\ncompeting objectives in dynamic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20853v1", "categories": ["cs.LG", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20853v1", "AI": {"title_translation": "认知雷达资源管理中的多目标强化学习", "tldr": "该研究将认知雷达时间分配问题建模为多目标优化问题，并使用深度强化学习（DDPG和SAC）寻找帕累托最优解，发现SAC在稳定性方面优于DDPG，并利用NSGA-II估计帕累托前沿。", "motivation": "解决多功能认知雷达系统中时间分配问题，即在扫描新目标和跟踪已检测目标之间进行权衡。", "method": "将问题表述为多目标优化问题，并采用深度强化学习方法（DDPG和SAC算法）寻找帕累托最优解。此外，使用NSGA-II算法估计帕累托前沿的上限。", "result": "两种算法（DDPG和SAC）在适应不同场景方面均有效，其中SAC在稳定性和样本效率方面优于DDPG。", "conclusion": "本工作有助于开发更高效、自适应的认知雷达系统，这些系统能够在动态环境中平衡多个相互竞争的目标。", "translation": "多功能认知雷达系统中的时间分配问题侧重于新出现目标的扫描与先前检测目标的跟踪之间的权衡。我们将此问题表述为多目标优化问题，并采用深度强化学习来寻找帕累托最优解，并比较了深度确定性策略梯度（DDPG）和软行动者-评论家（SAC）算法。我们的结果表明，两种算法在适应各种场景方面均有效，其中SAC与DDPG相比，显示出更高的稳定性和样本效率。我们进一步采用NSGA-II算法来估计所考虑问题的帕累托前沿的上限。这项工作有助于开发更高效、自适应的认知雷达系统，这些系统能够在动态环境中平衡多个相互竞争的目标。", "summary": "本文研究了多功能认知雷达系统中的时间分配问题，将其建模为多目标优化问题，并利用深度强化学习（DDPG和SAC）寻找帕累托最优解。研究发现SAC算法在稳定性与样本效率上优于DDPG。此外，本文还使用NSGA-II算法估算了帕累托前沿的上限，旨在开发更高效、自适应的认知雷达系统。", "keywords": "认知雷达, 多目标强化学习, 时间分配, 帕累托优化, 深度强化学习", "comments": "这项工作创新性地将多目标强化学习应用于认知雷达资源管理，解决了扫描和跟踪之间的权衡问题。通过比较DDPG和SAC算法，并指出SAC的优势，为实际系统部署提供了有价值的参考。结合NSGA-II估计帕累托前沿，进一步增强了解决方案的理论完整性。"}}
{"id": "2506.21538", "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval", "authors": ["Hani Alomari", "Anushka Sivakumar", "Andrew Zhang", "Chris Thomas"], "summary": "Cross-modal image-text retrieval is challenging because of the diverse\npossible associations between content from different modalities. Traditional\nmethods learn a single-vector embedding to represent semantics of each sample,\nbut struggle to capture nuanced and diverse relationships that can exist across\nmodalities. Set-based approaches, which represent each sample with multiple\nembeddings, offer a promising alternative, as they can capture richer and more\ndiverse relationships. In this paper, we show that, despite their promise,\nthese set-based representations continue to face issues including sparse\nsupervision and set collapse, which limits their effectiveness. To address\nthese challenges, we propose Maximal Pair Assignment Similarity to optimize\none-to-one matching between embedding sets which preserve semantic diversity\nwithin the set. We also introduce two loss functions to further enhance the\nrepresentations: Global Discriminative Loss to enhance distinction among\nembeddings, and Intra-Set Divergence Loss to prevent collapse within each set.\nOur method achieves state-of-the-art performance on MS-COCO and Flickr30k\nwithout relying on external data.", "comment": "Accepted at the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025 Main)", "pdf_url": "http://arxiv.org/pdf/2506.21538v1", "categories": ["cs.CV", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21538v1", "AI": {"title_translation": "最大匹配的重要性：防止表示坍塌以实现鲁棒的跨模态检索", "tldr": "本文提出了一种利用最大对分配相似度及两种新颖损失函数的新方法，以防止基于集合的跨模态检索中的表示坍塌，并取得了最先进的性能。", "motivation": "传统的单向量嵌入难以捕捉跨模态的细微和多样化关系。尽管基于集合的方法有潜力，但它们面临稀疏监督和集合坍塌的问题，限制了其有效性。", "method": "本文提出了最大对分配相似度（Maximal Pair Assignment Similarity）来优化嵌入集合之间的一对一匹配，以保留语义多样性。此外，还引入了全局判别损失（Global Discriminative Loss）来增强嵌入之间的区分度，以及集合内散度损失（Intra-Set Divergence Loss）来防止每个集合内部的坍塌。", "result": "该方法在MS-COCO和Flickr30k数据集上无需依赖外部数据就达到了最先进的性能。", "conclusion": "所提出的最大对分配相似度以及两种新的损失函数有效解决了基于集合的跨模态检索中的表示坍塌和稀疏监督问题，从而提高了性能。", "translation": "跨模态图像-文本检索具有挑战性，因为不同模态内容之间可能存在多种关联。传统方法学习单一向量嵌入来表示每个样本的语义，但难以捕捉跨模态存在的细微和多样化关系。基于集合的方法，即用多个嵌入表示每个样本，提供了一种有前景的替代方案，因为它们可以捕获更丰富和更多样化的关系。在本文中，我们表明，尽管基于集合的表示很有前景，但它们仍然面临稀疏监督和集合坍塌等问题，这限制了它们的有效性。为了解决这些挑战，我们提出了最大对分配相似度（Maximal Pair Assignment Similarity）来优化嵌入集合之间的一对一匹配，从而保留集合内的语义多样性。我们还引入了两个损失函数以进一步增强表示：全局判别损失（Global Discriminative Loss）以增强嵌入之间的区分度，以及集合内散度损失（Intra-Set Divergence Loss）以防止每个集合内部的坍塌。我们的方法在MS-COCO和Flickr30k数据集上无需依赖外部数据就达到了最先进的性能。", "summary": "本文旨在解决跨模态图像-文本检索中基于集合表示的局限性，特别是稀疏监督和表示坍塌问题。为此，它引入了最大对分配相似度，用于嵌入集合之间的一对一匹配，并提出了全局判别损失和集合内散度损失，以保持语义多样性并防止坍塌。该方法在标准基准测试中取得了最先进的性能。", "keywords": "跨模态检索, 表示坍塌, 基于集合的嵌入, 最大匹配, 损失函数", "comments": "该论文的创新之处在于解决了基于集合的跨模态检索中“集合坍塌”和“稀疏监督”的具体问题，这些问题对于捕捉细微关系至关重要。最大对分配相似度和两种新颖损失函数（全局判别损失和集合内散度损失）的引入直接解决了这些问题，从而产生了鲁棒且多样化的表示。无需外部数据即可达到最先进的性能，这突显了其有效性和实际适用性。"}}
{"id": "2506.21119", "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios.", "comment": "Accepted by ICONIP 2024", "pdf_url": "http://arxiv.org/pdf/2506.21119v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21119v1", "AI": {"title_translation": "Progtuning: 基于Transformer的语言模型的渐进式微调框架", "tldr": "Progtuning是一种新的渐进式微调框架，通过根据Transformer块的贡献逐步减少更新参数，从而优化资源分配并保持性能。", "motivation": "现有的微调和参数高效微调方法在更新参数时忽略了Transformer块贡献的不均衡性，导致计算资源分配效率低下，尤其是在模型尺寸不断增长的情况下。", "method": "本文提出了Progtuning框架，它结合了渐进式学习，并根据Transformer块的贡献，逐步减少需要更新的Transformer块的数量。", "result": "Progtuning优化了资源分配，减少了约25%的更新参数，同时保持了有竞争力的性能。它还与参数高效微调方法表现出高度的适应性，在各种适应场景中展示了出色的性能。", "conclusion": "Progtuning通过智能地减少更新参数，成功地提高了微调的效率，同时保持了性能，并且具有良好的兼容性。", "translation": "微调是一种在下游任务中利用基于Transformer的语言模型的有前景的技术。随着模型尺寸的不断增长，更新所有模型参数变得越来越昂贵。参数高效微调方法通过选择性地更新一小部分参数有效地解决了这个问题。然而，微调和大多数现有的参数高效微调方法需要更新与初始尺寸相同数量的参数，忽略了Transformer块之间贡献的不均衡性，导致计算资源分配效率极低。在本文中，我们提出了Progtuning，这是一种结合了渐进式学习的基于Transformer的语言模型的新型微调框架。具体来说，Progtuning根据贡献逐步减少需要更新的Transformer块的数量。值得注意的是，Progtuning优化了资源分配，减少了大约25%的更新参数，同时仍保持了有竞争力的性能。它还与参数高效微调方法表现出高度的适应性，在各种适应场景中展示了出色的性能。", "summary": "本文提出了Progtuning，一个针对基于Transformer的语言模型的渐进式微调框架。该方法通过根据Transformer块的贡献逐步减少更新的参数数量，解决了传统微调和参数高效微调中计算资源分配效率低下的问题。Progtuning能够优化资源分配，在减少约25%更新参数的同时保持竞争力性能，并与现有参数高效微调方法良好兼容。", "keywords": "Progtuning, 微调, Transformer, 参数高效, 渐进式学习", "comments": "Progtuning的创新之处在于其渐进式学习策略，它智能地利用了Transformer块贡献的不均衡性来优化资源分配，这对于处理日益增长的大型语言模型尤其重要。"}}
{"id": "2506.21405", "title": "An adaptive dynamical low-rank optimizer for solving kinetic parameter identification inverse problems", "authors": ["Lena Baumann", "Lukas Einkemmer", "Christian Klingenberg", "Jonas Kusch"], "summary": "The numerical solution of parameter identification inverse problems for\nkinetic equations can exhibit high computational and memory costs. In this\npaper, we propose a dynamical low-rank scheme for the reconstruction of the\nscattering parameter in the radiative transfer equation from a number of\nmacroscopic time-independent measurements. We first work through the PDE\nconstrained optimization procedure in a continuous setting and derive the\nadjoint equations using a Lagrangian reformulation. For the scattering\ncoefficient, a periodic B-spline approximation is introduced and a gradient\ndescent step for updating its coefficients is formulated. After the\ndiscretization, a dynamical low-rank approximation (DLRA) is applied. We make\nuse of the rank-adaptive basis update & Galerkin integrator and a line search\napproach for the adaptive refinement of the gradient descent step size and the\nDLRA tolerance. We show that the proposed scheme significantly reduces both\nmemory and computational cost. Numerical results computed with different\ninitial conditions validate the accuracy and efficiency of the proposed DLRA\nscheme compared to solutions computed with a full solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21405v1", "categories": ["math.NA", "cs.NA", "35Q49, 49M41, 65M22, 65M32"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21405v1", "AI": {"title_translation": "一种用于解决动力学参数识别逆问题的自适应动态低秩优化器", "tldr": "本文提出了一种自适应动态低秩近似（DLRA）方案，用于解决动力学参数识别逆问题，显著降低了计算和内存成本，并经验证具有高精度和效率。", "motivation": "动力学方程参数识别逆问题的数值解计算和内存成本高昂。", "method": "本文提出了一种动态低秩方案，用于从宏观时间无关测量中重建辐射传输方程中的散射参数。首先在连续设置中通过PDE约束优化过程，并使用拉格朗日重构推导出伴随方程。对于散射系数，引入周期B样条近似并制定了更新其系数的梯度下降步骤。离散化后，应用动态低秩近似（DLRA），并利用秩自适应基更新和伽辽金积分器，以及线搜索方法来自适应细化梯度下降步长和DLRA容差。", "result": "所提出的方案显著降低了内存和计算成本。通过不同初始条件计算的数值结果验证了所提出的DLRA方案与使用完整求解器计算的解决方案相比的准确性和效率。", "conclusion": "本文提出的自适应动态低秩近似（DLRA）方案能够有效且高效地解决动力学参数识别逆问题，显著降低了计算和内存成本，并保持了高精度。", "translation": "动力学方程参数识别逆问题的数值解可能表现出高计算和内存成本。在本文中，我们提出了一种动态低秩方案，用于从多个宏观时间无关测量中重建辐射传输方程中的散射参数。我们首先在连续设置中通过PDE约束优化过程，并使用拉格朗日重构推导出伴随方程。对于散射系数，引入了周期B样条近似，并制定了更新其系数的梯度下降步骤。在离散化之后，应用了动态低秩近似（DLRA）。我们利用秩自适应基更新和伽辽金积分器以及线搜索方法来对梯度下降步长和DLRA容差进行自适应细化。我们表明，所提出的方案显著降低了内存和计算成本。通过不同初始条件计算的数值结果验证了所提出的DLRA方案与使用完整求解器计算的解决方案相比的准确性和效率。", "summary": "本文提出了一种自适应动态低秩近似（DLRA）方案，旨在解决动力学方程参数识别逆问题所面临的高计算和内存成本。该方法通过PDE约束优化过程，结合拉格朗日重构推导伴随方程，并利用周期B样条近似和梯度下降更新散射系数。离散化后，应用DLRA，并通过秩自适应基更新、伽辽金积分器和线搜索方法自适应调整步长和容差。研究结果表明，该方案显著降低了计算和内存成本，且数值实验验证了其相较于完整求解器的高准确性和效率。", "keywords": "参数识别, 逆问题, 动态低秩近似, 辐射传输方程, 计算效率", "comments": "本文的创新点在于将动态低秩近似（DLRA）与自适应策略结合，有效地解决了动力学参数识别逆问题中的计算和内存瓶颈。通过引入秩自适应基更新和线搜索机制，该方案在保证精度的前提下，实现了计算效率的显著提升，对于需要处理大规模动力学逆问题的领域具有重要意义。"}}
{"id": "2506.20964", "title": "Evidence-based diagnostic reasoning with multi-agent copilot for human pathology", "authors": ["Chengkuan Chen", "Luca L. Weishaupt", "Drew F. K. Williamson", "Richard J. Chen", "Tong Ding", "Bowen Chen", "Anurag Vaidya", "Long Phi Le", "Guillaume Jaume", "Ming Y. Lu", "Faisal Mahmood"], "summary": "Pathology is experiencing rapid digital transformation driven by whole-slide\nimaging and artificial intelligence (AI). While deep learning-based\ncomputational pathology has achieved notable success, traditional models\nprimarily focus on image analysis without integrating natural language\ninstruction or rich, text-based context. Current multimodal large language\nmodels (MLLMs) in computational pathology face limitations, including\ninsufficient training data, inadequate support and evaluation for multi-image\nunderstanding, and a lack of autonomous, diagnostic reasoning capabilities. To\naddress these limitations, we introduce PathChat+, a new MLLM specifically\ndesigned for human pathology, trained on over 1 million diverse,\npathology-specific instruction samples and nearly 5.5 million question answer\nturns. Extensive evaluations across diverse pathology benchmarks demonstrated\nthat PathChat+ substantially outperforms the prior PathChat copilot, as well as\nboth state-of-the-art (SOTA) general-purpose and other pathology-specific\nmodels. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI\nsystem leveraging PathChat+ to autonomously evaluate gigapixel whole-slide\nimages (WSIs) through iterative, hierarchical diagnostic reasoning, reaching\nhigh accuracy on DDxBench, a challenging open-ended differential diagnosis\nbenchmark, while also capable of generating visually grounded,\nhumanly-interpretable summary reports.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20964v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20964v1", "AI": {"title_translation": "人类病理学中基于证据的诊断推理与多智能体副驾驶", "tldr": "论文介绍了PathChat+，一个专门为病理学设计的新型多模态大语言模型（MLLM），以及SlideSeek，一个利用PathChat+进行自主诊断推理的多智能体AI系统，显著提升了病理诊断的准确性和可解释性。", "motivation": "传统的计算病理模型主要侧重图像分析，缺乏自然语言指令和文本上下文整合。现有的多模态大语言模型（MLLMs）在计算病理学中存在训练数据不足、对多图像理解支持和评估不足以及缺乏自主诊断推理能力等局限性。", "method": "引入PathChat+，一个专门为人类病理学设计的新型多模态大语言模型（MLLM），该模型在超过100万个多样化的病理学特定指令样本和近550万个问答回合上进行训练。此外，提出了SlideSeek，一个利用PathChat+的、支持推理的多智能体AI系统，通过迭代、分层的诊断推理自主评估千兆像素的全玻片图像（WSIs）。", "result": "PathChat+在各种病理学基准测试中，显著优于先前的PathChat副驾驶以及最先进的通用模型和其他病理学专用模型。SlideSeek在具有挑战性的开放式鉴别诊断基准DDxBench上达到了高准确率，并能够生成视觉上接地、人类可解释的摘要报告。", "conclusion": "论文成功开发了PathChat+和SlideSeek，显著克服了当前计算病理学中多模态模型的局限性，实现了基于证据的自主诊断推理，提升了病理诊断的准确性和可解释性，推动了数字病理学的发展。", "translation": "病理学正在经历由全玻片成像和人工智能（AI）驱动的快速数字化转型。虽然基于深度学习的计算病理学取得了显著成功，但传统模型主要侧重于图像分析，而没有整合自然语言指令或丰富的文本上下文。当前计算病理学中的多模态大语言模型（MLLMs）面临局限性，包括训练数据不足、对多图像理解的支持和评估不足，以及缺乏自主诊断推理能力。为了解决这些局限性，我们引入了PathChat+，一个专门为人类病理学设计的新型MLLM，它在超过100万个多样化的病理学特定指令样本和近550万个问答回合上进行训练。在各种病理学基准测试中的广泛评估表明，PathChat+显著优于先前的PathChat副驾驶以及最先进的（SOTA）通用模型和其他病理学专用模型。此外，我们提出了SlideSeek，一个支持推理的多智能体AI系统，它利用PathChat+通过迭代、分层的诊断推理自主评估千兆像素的全玻片图像（WSIs），在具有挑战性的开放式鉴别诊断基准DDxBench上达到了高准确率，同时也能生成视觉上接地、人类可解释的摘要报告。", "summary": "本文针对计算病理学中传统模型和现有多模态大语言模型在语言整合、多图像理解及自主推理方面的不足，提出了PathChat+和SlideSeek。PathChat+是一个新型病理学专用MLLM，通过海量数据训练，在多个基准测试中表现优异。在此基础上，SlideSeek作为多智能体AI系统，能够对全玻片图像进行迭代、分层的自主诊断推理，并在鉴别诊断任务上取得高准确率，同时生成可解释的报告，显著推动了数字病理诊断的智能化和可靠性。", "keywords": "病理学, 多模态大语言模型, 诊断推理, 全玻片图像, 人工智能", "comments": "本文的创新之处在于结合了大型语言模型和多智能体系统，解决了传统计算病理学模型在语言理解和自主推理方面的局限。PathChat+的大规模病理学特定数据训练是其性能优越的关键。SlideSeek的多智能体、迭代推理机制是其实现高精度诊断和可解释报告的核心。这对于提升数字病理诊断的自动化和辅助医生决策具有重要意义。"}}
{"id": "2506.21170", "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21170v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21170v1", "AI": {"title_translation": "文本扩散建模的压缩平滑潜在空间", "tldr": "提出Cosmos，一种在压缩平滑潜在空间中进行文本扩散生成的方法，解决了现有文本生成模型的速度和连贯性问题，并实现了更快的推理速度和可比或更优的生成质量。", "motivation": "自回归语言模型解码慢且难以保持全局连贯性；扩散模型应用于文本生成受限于高维度的token级表示。", "method": "引入Cosmos，一种在为扩散量身定制的压缩、平滑潜在空间中操作的文本生成方法。该空间通过一个自编码器学习，该自编码器同时进行token级重建和与预训练语言编码器冻结激活的对齐。", "result": "文本表示可压缩8倍，同时保持与token级扩散模型相当的生成质量。增加潜在序列长度使Cosmos超越了基于扩散和自回归的基线模型。在四种生成任务（故事生成、问题生成、摘要、去毒）上评估，Cosmos实现了可比或更优的生成质量，并提供超过2倍的推理速度。", "conclusion": "Cosmos通过在压缩、平滑的潜在空间中进行文本扩散，有效克服了传统文本生成模型的局限性，在生成质量和推理速度上均表现出色。", "translation": "自回归语言模型主导了现代文本生成，但其顺序性引入了根本性限制：解码速度慢，并且难以保持全局连贯性。扩散模型通过实现并行生成和灵活控制提供了有前景的替代方案；然而，它们在文本生成中的应用受到token级表示高维度的阻碍。我们引入了Cosmos，一种新颖的文本生成方法，它完全在为扩散专门定制的压缩、平滑潜在空间中操作。这个空间通过一个自编码器学习，该自编码器同时进行token级重建和与预训练语言编码器冻结激活的对齐，从而提供强大的语义基础并实现有效的基于扰动的增强。经验上，我们证明文本表示可以被压缩8倍，同时保持与token级扩散模型相当的生成质量。此外，增加潜在序列长度使得Cosmos能够超越基于扩散和自回归的基线模型。我们在四种不同的生成任务上评估了Cosmos，包括故事生成、问题生成、摘要和去毒，并将其与各种生成范式进行比较。Cosmos实现了可比或更优的生成质量，同时提供超过2倍的推理速度。", "summary": "本文提出了Cosmos，一种新颖的文本扩散生成方法，它在学习到的压缩且平滑的潜在空间中操作，以解决传统自回归模型速度慢和全局连贯性差的问题，以及扩散模型在文本上高维度表示的挑战。Cosmos通过一个同时进行token重建和语义对齐的自编码器学习该潜在空间。实验证明，Cosmos能将文本表示压缩8倍，同时保持生成质量，并在增加潜在序列长度时超越现有基线，还在多项任务上实现了更优或相当的生成质量和超过2倍的推理速度。", "keywords": "文本扩散模型, 潜在空间, 文本生成, 自编码器, Cosmos", "comments": "Cosmos的创新之处在于其将文本扩散模型应用于一个专门学习的压缩平滑潜在空间，有效解决了高维度表示的挑战，并通过自编码器结合语义对齐确保了生成质量。其重要性体现在显著提升了文本生成的速度和效率，同时保持或超越了现有模型的生成质量，为未来高效、高质量的文本生成提供了新的方向。"}}
{"id": "2506.21455", "title": "An Iterative Methodology for Unitary Quantum Channel Search", "authors": ["Matthew M. Lin", "Hao-Wei Huang", "Bing-Ze Lu"], "summary": "In this paper, we propose an iterative algorithm using polar decomposition to\napproximate a channel characterized by a single unitary matrix based on\ninput-output quantum state pairs. In limited data, we state and prove that the\noptimal solution obtained from our method using one pair with a specific\nstructure will generate an equivalent class, significantly reducing the\ndimension of the searching space. Furthermore, we prove that the unitary\nmatrices describing the same channel differ by a complex number with modulus 1.\nWe rigorously prove our proposed algorithm can ultimately identify a critical\npoint, which is also a local minimum of the established objective function.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21455v1", "categories": ["math.NA", "cs.NA", "quant-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21455v1", "AI": {"title_translation": "单一量子信道搜索的迭代方法", "tldr": "本文提出了一种使用极分解的迭代算法，用于基于输入-输出量子态对来逼近单一酉矩阵表征的量子信道。该方法在有限数据下能显著减少搜索空间并被证明能收敛到局部最小值。", "motivation": "在有限数据下，基于输入-输出量子态对来近似由单个酉矩阵表征的量子信道。", "method": "提出了一种使用极分解的迭代算法。该方法利用一个具有特定结构的输入-输出量子态对来生成一个等价类，从而显著减小搜索空间的维度。", "result": "1. 在有限数据下，使用特定结构的输入-输出对，该方法获得的最佳解能生成一个等价类，显著减小搜索空间的维度。2. 描述相同信道的酉矩阵仅相差一个模为1的复数。3. 严格证明所提出的算法最终能够识别出一个临界点，该临界点也是所建立目标函数的局部最小值。", "conclusion": "本文提出的迭代算法能够有效地逼近单一酉矩阵表征的量子信道，并被严格证明在有限数据下能有效减少搜索空间并收敛到局部最小值。", "translation": "在本文中，我们提出了一种使用极分解的迭代算法，基于输入-输出量子态对来逼近由单个酉矩阵表征的信道。在有限数据下，我们阐述并证明，通过使用一个特定结构的输入-输出对，我们的方法获得的最佳解将生成一个等价类，从而显著减小搜索空间的维度。此外，我们证明了描述相同信道的酉矩阵仅相差一个模为1的复数。我们严格证明了我们提出的算法最终能够识别出一个临界点，该临界点也是所建立目标函数的局部最小值。", "summary": "本文提出了一种基于极分解的迭代算法，用于从输入-输出量子态对中近似单一酉矩阵表征的量子信道。研究证明，在有限数据条件下，通过使用特定结构的量子态对，该方法能够显著缩小搜索空间，并且算法能够收敛到目标函数的局部最小值，从而有效地识别量子信道。", "keywords": "迭代算法, 极分解, 酉量子信道, 量子态对, 搜索空间缩减", "comments": "本文的创新之处在于利用极分解进行迭代搜索，并严格证明了在有限数据下搜索空间的显著减少和算法的收敛性。这对于实际的量子信道表征，尤其是在数据受限的情况下，具有重要的理论和实践意义。"}}
{"id": "2506.20967", "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing", "authors": ["Lingling Cai", "Kang Zhao", "Hangjie Yuan", "Xiang Wang", "Yingya Zhang", "Kejie Huang"], "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in\nvideo generation. However, directly applying existing video editing methods to\nVideo DiTs often incurs substantial computational overhead, due to\nresource-intensive attention modification or finetuning. To alleviate this\nproblem, we present DFVEdit, an efficient zero-shot video editing method\ntailored for Video DiTs. DFVEdit eliminates the need for both attention\nmodification and fine-tuning by directly operating on clean latents via flow\ntransformation. To be more specific, we observe that editing and sampling can\nbe unified under the continuous flow perspective. Building upon this\nfoundation, we propose the Conditional Delta Flow Vector (CDFV) -- a\ntheoretically unbiased estimation of DFV -- and integrate Implicit Cross\nAttention (ICA) guidance as well as Embedding Reinforcement (ER) to further\nenhance editing quality. DFVEdit excels in practical efficiency, offering at\nleast 20x inference speed-up and 85\\% memory reduction on Video DiTs compared\nto attention-engineering-based editing methods. Extensive quantitative and\nqualitative experiments demonstrate that DFVEdit can be seamlessly applied to\npopular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art\nperformance on structural fidelity, spatial-temporal consistency, and editing\nquality.", "comment": "Zero-shot video editing", "pdf_url": "http://arxiv.org/pdf/2506.20967v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20967v1", "AI": {"title_translation": "DFVEdit：零样本视频编辑的条件增量流向量", "tldr": "DFVEdit是一种针对视频扩散Transformer（Video DiTs）的高效零样本视频编辑方法，通过流变换直接在潜在空间操作，避免了注意力修改和微调，显著提升了推理速度和内存效率。", "motivation": "将现有视频编辑方法直接应用于视频扩散Transformer（Video DiTs）会导致巨大的计算开销，原因在于其资源密集型的注意力修改或微调需求。", "method": "本文提出了DFVEdit，一种高效的零样本视频编辑方法，专为Video DiTs设计。DFVEdit通过流变换直接在干净的潜在空间上操作，从而无需进行注意力修改和微调。该方法将编辑和采样统一在连续流的视角下，并提出了条件增量流向量（CDFV）——一种理论上无偏的DFV估计。此外，DFVEdit还集成了隐式交叉注意力（ICA）引导和嵌入增强（ER）以进一步提升编辑质量。", "result": "DFVEdit在实际效率方面表现出色，与基于注意力工程的编辑方法相比，在Video DiTs上实现了至少20倍的推理速度提升和85%的内存减少。广泛的定量和定性实验表明，DFVEdit可以无缝应用于流行的Video DiTs（如CogVideoX和Wan2.1），并在结构保真度、时空一致性和编辑质量方面达到了最先进的性能。", "conclusion": "DFVEdit为视频扩散Transformer提供了一种高效、高质量的零样本视频编辑解决方案，显著提升了计算效率并保持了出色的编辑效果。", "translation": "视频扩散Transformer（Video DiTs）的出现标志着视频生成领域的一个里程碑。然而，由于资源密集型的注意力修改或微调，将现有视频编辑方法直接应用于Video DiTs通常会产生大量的计算开销。为了缓解这个问题，我们提出了DFVEdit，一种专为Video DiTs量身定制的高效零样本视频编辑方法。DFVEdit通过流变换直接在干净的潜在空间上操作，从而消除了对注意力修改和微调的需求。更具体地说，我们观察到编辑和采样可以在连续流的视角下统一。在此基础上，我们提出了条件增量流向量（CDFV）——一种理论上无偏的DFV估计——并集成了隐式交叉注意力（ICA）引导以及嵌入增强（ER）以进一步提高编辑质量。DFVEdit在实际效率方面表现出色，与基于注意力工程的编辑方法相比，在Video DiTs上提供了至少20倍的推理速度提升和85%的内存减少。广泛的定量和定性实验表明，DFVEdit可以无缝应用于流行的Video DiTs（例如CogVideoX和Wan2.1），在结构保真度、时空一致性和编辑质量方面达到了最先进的性能。", "summary": "DFVEdit是一种针对视频扩散Transformer（Video DiTs）的高效零样本视频编辑方法。它通过将编辑和采样统一在连续流视角下，并引入条件增量流向量（CDFV）以及隐式交叉注意力（ICA）和嵌入增强（ER），来避免传统方法中昂贵的注意力修改和微调。实验证明，DFVEdit在推理速度和内存使用上相比现有方法有显著提升（20倍加速，85%内存减少），并在多种Video DiTs上实现了卓越的编辑质量和一致性。", "keywords": "视频编辑, 视频扩散Transformer, 零样本, 流向量, 计算效率", "comments": "DFVEdit的创新点在于其通过流变换直接在潜在空间进行操作，巧妙地避开了Video DiTs中计算量大的注意力修改和微调，从而实现了显著的效率提升。这种基于“流”的统一编辑和采样视角，以及CDFV、ICA和ER的结合，为零样本视频编辑提供了一个高效且高质量的新范式。其在速度和内存上的优势使其在实际应用中具有重要价值。"}}
{"id": "2506.20886", "title": "Omniwise: Predicting GPU Kernels Performance with LLMs", "authors": ["Zixian Wang", "Cole Ramos", "Muhammad A. Awad", "Keith Lowery"], "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20886v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20886v1", "AI": {"title_translation": "Omniwise：使用大型语言模型预测GPU核性能", "tldr": "Omniwise利用大型语言模型（LLMs）无需执行代码即可预测GPU核性能，准确率高，且轻量级。", "motivation": "深度神经网络(DNNs)的快速发展需要高效的GPU性能，但现有的性能分析方法可能需要代码执行或特定工具。本文旨在提供一种新的、无需执行的预测方法。", "method": "本文引入了Omniwise，这是首个端到端、自监督的微调管道，将大型语言模型（LLMs）应用于GPU核性能预测。Omniwise模型无关且轻量级，即使使用小型3B参数模型也能获得良好结果。它能够直接从核代码预测关键性能指标，如内存带宽、缓存命中率、GFLOPs和算术强度，而无需代码执行或使用分析工具。", "result": "Omniwise在AMD MI250和MI300X架构上对GPU核的预测，有超过90%的预测在10%相对误差范围内。此外，还开发了在线推理服务器和Visual Studio Code插件，将基于LLM的性能预测无缝集成到开发人员的工作流程中。", "conclusion": "Omniwise提供了一种高效、准确且易于集成的GPU核性能预测新方法，通过利用大型语言模型显著简化了开发者的工作流程，无需代码执行即可获得关键性能指标。", "translation": "近年来，深度神经网络（DNNs）的快速发展彻底改变了人工智能，使模型在理解、生成和处理复杂数据方面具备了前所未有的能力。这些强大的架构已经改变了广泛的下游应用，解决了人类难以企及的任务。在本文中，我们介绍了Omniwise，这是第一个端到端、自监督的微调管道，将大型语言模型（LLMs）应用于GPU核性能预测——这是性能分析领域的一个新颖用例。Omniwise是模型无关且轻量级的，即使使用小型3B参数模型也能取得显著成果。它可以直接从核代码预测关键性能指标，包括内存带宽、缓存命中率、GFLOPs和算术强度，无需代码执行或分析工具。我们的方法在AMD MI250和MI300X架构上执行的GPU核上，实现了超过90%的预测在10%相对误差范围内。除了该管道，我们还开发了一个在线推理服务器和一个Visual Studio Code插件，将基于LLM的性能预测无缝集成到开发人员的工作流程中。", "summary": "Omniwise是一个创新的端到端、自监督微调管道，首次将大型语言模型（LLMs）应用于GPU核性能预测。该系统无需代码执行或分析工具，可直接从核代码预测内存带宽、缓存命中率等关键性能指标。Omniwise模型无关且轻量级，即使小型模型也能在AMD GPU上实现超过90%的预测在10%相对误差内的准确率。此外，还提供了在线推理服务器和VS Code插件以方便集成到开发工作流中。", "keywords": "GPU性能预测, 大型语言模型, Omniwise, 性能分析, 深度学习", "comments": "本文创新性地将LLMs应用于GPU性能预测，提供了一种无需代码执行的轻量级解决方案，显著提高了性能分析的效率和可访问性。其模型无关的特性和高精度是其重要亮点，同时提供的开发工具也极大地方便了实际应用。"}}
{"id": "2506.20694", "title": "Evaluating PDE discovery methods for multiscale modeling of biological signals", "authors": ["Andréa Ducos", "Audrey Denizot", "Thomas Guyet", "Hugues Berry"], "summary": "Biological systems are non-linear, include unobserved variables and the\nphysical principles that govern their dynamics are partly unknown. This makes\nthe characterization of their behavior very challenging. Notably, their\nactivity occurs on multiple interdependent spatial and temporal scales that\nrequire linking mechanisms across scales. To address the challenge of bridging\ngaps between scales, we leverage partial differential equations (PDE)\ndiscovery. PDE discovery suggests meso-scale dynamics characteristics from\nmicro-scale data. In this article, we present our framework combining\nparticle-based simulations and PDE discovery and conduct preliminary\nexperiments to assess equation discovery in controlled settings. We evaluate\nfive state-of-the-art PDE discovery methods on particle-based simulations of\ncalcium diffusion in astrocytes. The performances of the methods are evaluated\non both the form of the discovered equation and the forecasted temporal\nvariations of calcium concentration. Our results show that several methods\naccurately recover the diffusion term, highlighting the potential of PDE\ndiscovery for capturing macroscopic dynamics in biological systems from\nmicroscopic data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20694v1", "categories": ["q-bio.QM", "cs.AI"], "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.20694v1", "AI": {"title_translation": "评估用于生物信号多尺度建模的偏微分方程发现方法", "tldr": "本文评估了五种先进的偏微分方程（PDE）发现方法，通过粒子模拟来从微观数据中推断生物系统的介观尺度动力学，结果表明这些方法能够准确恢复扩散项，展示了PDE发现从微观数据捕捉宏观动力学的潜力。", "motivation": "生物系统具有非线性、包含未观测变量且其动力学物理原理部分未知，这使得描述其行为极具挑战性。此外，生物活动发生在多个相互依赖的时空尺度上，需要跨尺度连接机制。为了解决跨尺度鸿沟的挑战，本研究利用偏微分方程（PDE）发现。", "method": "本文提出了一个结合粒子模拟和PDE发现的框架，并在受控环境下进行初步实验以评估方程发现能力。具体来说，研究评估了五种最先进的PDE发现方法，针对星形胶质细胞中钙扩散的粒子模拟数据进行测试。方法的性能通过发现方程的形式和钙浓度预测的时间变化进行评估。", "result": "研究结果表明，有几种方法能够准确地恢复扩散项。", "conclusion": "偏微分方程（PDE）发现方法在从微观数据中捕捉生物系统的宏观动力学方面具有巨大潜力。", "translation": "生物系统是非线性的，包含未观测变量，并且其动力学所遵循的物理原理部分未知。这使得描述其行为变得非常具有挑战性。值得注意的是，它们的活动发生在多个相互依赖的空间和时间尺度上，需要跨尺度连接机制。为了解决弥合尺度之间差距的挑战，我们利用偏微分方程（PDE）发现。PDE发现从微观数据中推断出介观尺度的动力学特征。在本文中，我们提出了一个结合粒子模拟和PDE发现的框架，并进行了初步实验以评估受控环境下的方程发现能力。我们评估了五种最先进的PDE发现方法，针对星形胶质细胞中钙扩散的粒子模拟数据。方法的性能通过发现方程的形式和钙浓度预测的时间变化进行评估。我们的结果表明，有几种方法能够准确地恢复扩散项，突出了PDE发现从微观数据中捕捉生物系统宏观动力学的潜力。", "summary": "本研究旨在评估偏微分方程（PDE）发现方法在生物信号多尺度建模中的应用。鉴于生物系统行为描述的挑战性（非线性、未观测变量、未知物理原理及多尺度活动），研究提出了一个结合粒子模拟与PDE发现的框架，旨在从微观数据中推断介观尺度的动力学。通过对星形胶质细胞中钙扩散的粒子模拟数据，评估了五种先进PDE发现方法的性能，评估指标包括发现方程的形式和钙浓度的时间预测。结果显示，多种方法能准确恢复扩散项，证实了PDE发现从微观数据捕捉生物系统宏观动力学的潜力。", "keywords": "PDE发现, 多尺度建模, 生物信号, 钙扩散, 星形胶质细胞", "comments": "该论文提出了一种新颖的方法，通过结合粒子模拟和PDE发现来解决生物系统多尺度建模的挑战。其创新性在于利用PDE发现从微观数据中推断介观尺度动力学，这对于理解复杂生物过程至关重要。研究通过对钙扩散的模拟评估了多种SOTA方法，结果令人鼓舞，表明PDE发现有望成为连接不同尺度生物现象的有效工具。然而，目前仅限于初步实验和受控设置，未来的工作可能需要更复杂的生物系统和实验数据来验证其普适性。"}}
{"id": "2506.20977", "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging", "authors": ["Tao Liu", "Dafeng Zhang", "Gengchen Li", "Shizhuo Liu", "Yongqi Song", "Senmao Li", "Shiqi Yang", "Boqian Li", "Kai Wang", "Yaxing Wang"], "summary": "Face aging has become a crucial task in computer vision, with applications\nranging from entertainment to healthcare. However, existing methods struggle\nwith achieving a realistic and seamless transformation across the entire\nlifespan, especially when handling large age gaps or extreme head poses. The\ncore challenge lies in balancing age accuracy and identity preservation--what\nwe refer to as the Age-ID trade-off. Most prior methods either prioritize age\ntransformation at the expense of identity consistency or vice versa. In this\nwork, we address this issue by proposing a two-pass face aging framework, named\nCradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first\npass focuses on solving age accuracy by introducing an adaptive noise injection\n(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of\nage and gender for the given person as the textual condition. Also, by\nadjusting the noise level, we can control the strength of aging while allowing\nmore flexibility in transforming the face. However, identity preservation is\nweakly ensured here to facilitate stronger age transformations. In the second\npass, we enhance identity preservation while maintaining age-specific features\nby conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace\nand Rotate-CLIP. This pass allows for denoising the transformed image from the\nfirst pass, ensuring stronger identity preservation without compromising the\naging accuracy. Both passes are jointly trained in an end-to-end way. Extensive\nexperiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL\nprotocols, show that our Cradle2Cane outperforms existing face aging methods in\nage accuracy and identity consistency.", "comment": "30 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20977v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20977v1", "AI": {"title_translation": "从摇篮到拐杖：一种高保真寿命人脸老化两阶段框架", "tldr": "本文提出了一种名为Cradle2Cane的两阶段人脸老化框架，利用文本到图像扩散模型，通过平衡年龄准确性和身份保持，实现逼真且跨越整个生命周期的人脸老化。", "motivation": "现有的人脸老化方法在实现逼真、无缝的跨生命周期转换方面存在困难，尤其是在处理大年龄差距或极端头部姿势时。核心挑战在于平衡年龄准确性和身份保持，即“年龄-身份权衡”，大多数现有方法都牺牲其中一个来优先另一个。", "method": "本文提出了一种名为Cradle2Cane的两阶段人脸老化框架，该框架基于少量步骤的文本到图像（T2I）扩散模型。第一阶段通过引入自适应噪声注入（AdaNI）机制解决年龄准确性问题，该机制由年龄和性别提示作为文本条件引导，并通过调整噪声水平控制老化强度。第二阶段通过以两个身份感知嵌入（SVR-ArcFace和Rotate-CLIP）为条件，增强身份保持，同时保持年龄特定特征，对第一阶段的转换图像进行去噪。两个阶段共同进行端到端训练。", "result": "在CelebA-HQ测试数据集上进行的广泛实验，通过Face++和Qwen-VL协议评估，表明Cradle2Cane在年龄准确性和身份一致性方面优于现有的人脸老化方法。", "conclusion": "本文提出的Cradle2Cane两阶段框架有效解决了人脸老化中的年龄准确性与身份保持之间的权衡问题，实现了更逼真、更一致的跨生命周期人脸老化效果，并在实验中表现出超越现有方法的性能。", "translation": "人脸老化在计算机视觉中已成为一项关键任务，应用范围从娱乐到医疗保健。然而，现有方法在实现整个生命周期中逼真无缝的转换方面存在困难，尤其是在处理大年龄差距或极端头部姿势时。核心挑战在于平衡年龄准确性和身份保持——我们称之为年龄-身份权衡。大多数先前的方法要么以牺牲身份一致性为代价优先进行年龄转换，反之亦然。在这项工作中，我们通过提出一个名为Cradle2Cane的两阶段人脸老化框架来解决这个问题，该框架基于少量步骤的文本到图像（T2I）扩散模型。第一阶段通过引入自适应噪声注入（AdaNI）机制来解决年龄准确性问题。该机制通过包含给定人物的年龄和性别提示作为文本条件来引导。此外，通过调整噪声水平，我们可以控制老化强度，同时允许在人脸转换方面具有更大的灵活性。然而，为了促进更强的年龄转换，此处对身份保持的保障较弱。在第二阶段，我们通过以两个身份感知嵌入（IDEmb）：SVR-ArcFace和Rotate-CLIP为条件，增强身份保持，同时保持年龄特定特征。此阶段允许对第一阶段转换后的图像进行去噪，确保更强的身份保持而不损害老化准确性。两个阶段都以端到端的方式联合训练。在CelebA-HQ测试数据集上进行的广泛实验，通过Face++和Qwen-VL协议评估，表明我们的Cradle2Cane在年龄准确性和身份一致性方面优于现有的人脸老化方法。", "summary": "本文提出了一种名为Cradle2Cane的两阶段人脸老化框架，旨在解决现有方法在年龄准确性和身份保持之间权衡的难题。该框架基于文本到图像扩散模型，第一阶段利用自适应噪声注入（AdaNI）机制解决年龄准确性，第二阶段通过身份感知嵌入（SVR-ArcFace和Rotate-CLIP）增强身份保持。实验结果表明，Cradle2Cane在年龄准确性和身份一致性方面均优于现有方法，实现了高保真度的跨生命周期人脸老化。", "keywords": "人脸老化, 两阶段框架, 扩散模型, 身份保持, 年龄准确性", "comments": "这项工作通过创新的两阶段框架解决了人脸老化领域的一个核心挑战——年龄准确性与身份保持的权衡。利用扩散模型并引入自适应噪声注入和身份感知嵌入，展示了在复杂生成任务中精细控制不同属性的能力。其端到端训练和在极端情况下的表现潜力，使其在实际应用中具有重要价值。"}}
{"id": "2506.20893", "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20893v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20893v1", "AI": {"title_translation": "论输出分布重新加权对于有效类别遗忘的必要性", "tldr": "本文提出了一种名为RWFT的轻量级输出重加权遗忘方法，用于在不进行完全重新训练的情况下从分类器中删除整个类别。该方法通过重新分配被遗忘类别的预测概率质量来抵御成员推断攻击，并在实验中表现出与完全重新训练相当的性能，并显著优于现有最先进方法。", "motivation": "为了执行用户删除权和减轻有害或有偏见的预测，从训练好的模型中遗忘特定类别至关重要。完全重新训练成本高昂，而现有遗忘方法在预测来自被遗忘类别的样本时无法复制重新训练模型的行为，并且容易受到一种新的成员推断攻击（MIA-NN）的揭露。", "method": "本文提出了一种输出重加权遗忘方法RWFT，它是一种轻量级技术，无需完全重新训练即可从训练好的分类器中擦除整个类别。该方法通过简单地重新分配被遗忘类别样本的预测概率质量来抵御MIA-NN攻击。此外，还引入了一种基于总变异（TV）距离的新度量来量化残余泄漏。", "result": "通过与最先进的机器学习遗忘基线进行广泛实验，RWFT在先前用于评估的指标和本文提出的新指标上都与完全重新训练的结果相匹配。与最先进的方法相比，RWFT在先前使用的指标上提高了2.79%，在我们新的基于TV的指标上比现有最佳方法提高了111.45%。", "conclusion": "本文证明了输出分布重新加权对于有效类别遗忘的必要性，因为它可以抵御成员推断攻击并达到与完全重新训练相当的性能。", "translation": "在这项工作中，我们引入了一种输出重加权遗忘方法RWFT，这是一种轻量级技术，可以在不进行完全重新训练的情况下从训练好的分类器中擦除整个类别。从训练好的模型中遗忘特定类别对于执行用户删除权和减轻有害或有偏见的预测至关重要。完全重新训练成本高昂，而现有遗忘方法在预测来自被遗忘类别的样本时无法复制重新训练模型的行为。我们通过设计一种成员推断攻击的变体MIA-NN来证明这种失败，该攻击能够成功揭示任何这些方法的被遗忘类别。我们提出了一种简单的概率质量重新分配方案，用于被遗忘类别样本的预测，该方案对MIA-NN具有鲁棒性。我们还引入了一种基于总变异（TV）距离的预测概率新度量，以量化残余泄漏，从而防止未来的方法易受新攻击的影响。通过与最先进的机器学习遗忘基线进行广泛实验，我们表明我们的方法在先前工作用于评估的指标和我们在这项工作中提出的新指标上都与完全重新训练的结果相匹配。与最先进的方法相比，我们在先前使用的指标上获得了2.79%的提升，在我们新的基于TV的指标上比现有最佳方法获得了111.45%的提升。", "summary": "本文提出了一种名为RWFT的轻量级输出重加权遗忘方法，旨在有效从训练好的分类器中移除特定类别，以应对用户数据删除权和模型偏见问题。针对现有遗忘方法无法完全消除被遗忘类别信息，且易受新型成员推断攻击（MIA-NN）的挑战，RWFT通过重新分配被遗忘类别的预测概率质量来增强鲁棒性。同时，引入了一种基于总变异距离的新度量来量化遗忘效果。实验结果表明，RWFT在性能上与完全重新训练相当，并在多个指标上显著优于现有最先进的遗忘方法。", "keywords": "类别遗忘, 输出重加权, 成员推断攻击, 模型隐私, RWFT", "comments": "该论文的创新点在于提出了输出分布重新加权这一概念，并设计了RWFT方法来解决现有类别遗忘技术在隐私（抵抗成员推断攻击）和有效性（复制完全重新训练行为）方面的不足。引入新的TV距离度量也为评估遗忘效果提供了更严格的标准。其重要性体现在为实现“被遗忘权”提供了高效且鲁棒的解决方案，对负责任的AI发展具有积极意义。"}}
{"id": "2506.21199", "title": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image Segmentation and Classification", "authors": ["Shadman Sobhan", "Kazi Abrar Mahmud", "Abduz Zami"], "summary": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs.", "comment": "40 pages, 8 Tables, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2506.21199v1", "categories": ["cs.CV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21199v1", "AI": {"title_translation": "MedPrompt：用于医学图像分割和分类的LLM-CNN融合与权重路由", "tldr": "MedPrompt是一个统一框架，结合LLM进行任务规划和CNN进行图像处理，通过权重路由实现可扩展的医学图像分割和分类，在19个数据集上表现出色。", "motivation": "当前的医学图像分析系统通常是针对特定任务的，需要独立的模型进行分类和分割，并且缺乏支持用户定义工作流的灵活性。", "method": "MedPrompt是一个统一框架，结合了少量样本提示的大型语言模型（Llama-4-17B）用于高级任务规划，以及模块化卷积神经网络（DeepFusionLab）用于低级图像处理。LLM解释用户指令并生成结构化输出，以动态路由特定任务的预训练权重。这种权重路由方法避免了在添加新任务时重新训练整个框架，只需特定任务的权重。", "result": "MedPrompt在19个公共数据集上进行了评估，涵盖5种成像模式的12项任务。系统在解释和执行提示驱动指令方面达到了97%的端到端正确性，平均推理延迟为2.5秒。DeepFusionLab实现了具有竞争力的分割精度（例如，肺部Dice 0.9856）和强大的分类性能（结核病F1 0.9744）。", "conclusion": "MedPrompt通过结合LLM的可解释性与模块化CNN的效率，实现了可扩展的、提示驱动的医学成像。", "translation": "当前的医学图像分析系统通常是针对特定任务的，需要独立的模型进行分类和分割，并且缺乏支持用户定义工作流的灵活性。为了解决这些挑战，我们引入了MedPrompt，这是一个统一框架，它结合了用于高级任务规划的少量样本提示大型语言模型（Llama-4-17B）和用于低级图像处理的模块化卷积神经网络（DeepFusionLab）。LLM解释用户指令并生成结构化输出，以动态路由特定任务的预训练权重。这种权重路由方法避免了在添加新任务时重新训练整个框架——只需要特定任务的权重，从而增强了可扩展性和部署能力。我们在19个公共数据集上评估了MedPrompt，涵盖了5种成像模式的12项任务。该系统在解释和执行提示驱动指令方面达到了97%的端到端正确性，平均推理延迟为2.5秒，使其适用于近实时应用。DeepFusionLab实现了具有竞争力的分割精度（例如，肺部Dice 0.9856）和强大的分类性能（结核病F1 0.9744）。总的来说，MedPrompt通过结合LLM的可解释性与模块化CNN的效率，实现了可扩展的、提示驱动的医学成像。", "summary": "MedPrompt是一个创新的统一框架，旨在解决医学图像分析中任务特异性和缺乏灵活性的问题。它通过融合大型语言模型（LLM）进行高级任务规划和卷积神经网络（CNN）进行低级图像处理，实现了医学图像的分割和分类。该系统利用LLM动态路由任务特定的预训练权重，从而避免了在添加新任务时进行全面的模型再训练，显著提高了可扩展性。在19个数据集上的评估显示，MedPrompt在指令解释和执行方面达到97%的端到端正确性，平均推理延迟为2.5秒，同时DeepFusionLab在分割和分类任务上均表现出强大的性能。", "keywords": "医学图像分析, LLM, CNN, 图像分割, 图像分类", "comments": "MedPrompt的创新之处在于其LLM-CNN融合架构，特别是通过LLM进行权重路由以实现任务的动态适应性，这极大地提高了系统的可扩展性和灵活性，解决了传统医学图像分析系统任务特异性的痛点。其在多个数据集和任务上的验证表明了其实用价值和近实时应用潜力。"}}
{"id": "2506.21124", "title": "Quantum Adaptive Search: A Hybrid Quantum-Classical Algorithm for Global Optimization of Multivariate Functions", "authors": ["G. Intoccia", "U. Chirico", "V. Schiano Di Cola", "G. Pepe", "S. Cuomo"], "summary": "This work presents Quantum Adaptive Search (QAGS), a hybrid quantum-classical\nalgorithm for the global optimization of multivariate functions. The method\nemploys an adaptive mechanism that dynamically narrows the search space based\non a quantum-estimated probability distribution of the objective function. A\nquantum state encodes information about solution quality through an appropriate\ncomplex amplitude mapping, enabling the identification of the most promising\nregions, and thus progressively tightening the search bounds; then a classical\noptimizer performs local refinement of the solution. The analysis demonstrates\nthat QAGS ensures a contraction of the search space toward global optima, with\ncontrolled computational complexity. The numerical results on the benchmark\nfunctions show that, compared to the classical methods, QAGS achieves higher\naccuracy while offering advantages in both time and space complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21124v1", "categories": ["quant-ph", "cs.NA", "math.NA", "math.OC"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21124v1", "AI": {"title_translation": "量子自适应搜索：一种用于多元函数全局优化的混合量子-经典算法", "tldr": "QAGS是一种混合量子-经典算法，通过量子估计概率分布自适应缩小搜索空间，实现多元函数全局优化，相较于经典方法具有更高的精度和更好的时间和空间复杂度。", "motivation": "旨在提出一种用于多元函数全局优化的混合量子-经典算法。", "method": "提出量子自适应搜索（QAGS），该算法采用自适应机制，根据目标函数的量子估计概率分布动态缩小搜索空间。量子态通过复数幅度映射编码解的质量信息，识别最有希望的区域并逐步收紧搜索边界；然后经典优化器对解进行局部优化。", "result": "QAGS确保搜索空间向全局最优解收缩，并具有可控的计算复杂度。在基准函数上的数值结果表明，与经典方法相比，QAGS实现了更高的精度，并在时间和空间复杂度方面具有优势。", "conclusion": "QAGS是一种有效的混合量子-经典算法，用于多元函数的全局优化，相比经典方法具有更高的精度和更好的计算复杂度。", "translation": "这项工作提出了量子自适应搜索（QAGS），一种用于多元函数全局优化的混合量子-经典算法。该方法采用自适应机制，根据目标函数的量子估计概率分布动态缩小搜索空间。量子态通过适当的复数幅度映射编码解质量信息，从而能够识别最有希望的区域，并因此逐步收紧搜索边界；然后经典优化器对解进行局部细化。分析表明，QAGS确保搜索空间向全局最优解收缩，并具有可控的计算复杂度。基准函数上的数值结果表明，与经典方法相比，QAGS实现了更高的精度，同时在时间和空间复杂度方面都提供了优势。", "summary": "本文介绍了一种名为量子自适应搜索（QAGS）的混合量子-经典算法，用于多元函数的全局优化。该算法利用量子估计的概率分布自适应地缩小搜索空间，通过量子态编码解的质量信息来识别有希望的区域并逐步收紧搜索范围，最后由经典优化器进行局部细化。研究表明，QAGS能有效收敛至全局最优解，并具有可控的计算复杂度。与传统方法相比，QAGS在基准测试中表现出更高的精度以及在时间和空间复杂度上的优势。", "keywords": "量子自适应搜索, 全局优化, 混合量子-经典算法, 多元函数, 计算复杂度", "comments": "QAGS的创新之处在于其混合量子-经典架构和自适应搜索机制，通过量子方法指导经典优化，有效提升了全局优化的效率和精度。其在时间和空间复杂度上的优势表明了量子计算在解决复杂优化问题方面的潜力。"}}
{"id": "2506.20979", "title": "3D Scene-Camera Representation with Joint Camera Photometric Optimization", "authors": ["Weichen Dai", "Kangcheng Ma", "Jiaxin Wang", "Kecen Pan", "Yuhang Ming", "Hua Zhang", "Wanzeng Kong"], "summary": "Representing scenes from multi-view images is a crucial task in computer\nvision with extensive applications. However, inherent photometric distortions\nin the camera imaging can significantly degrade image quality. Without\naccounting for these distortions, the 3D scene representation may inadvertently\nincorporate erroneous information unrelated to the scene, diminishing the\nquality of the representation. In this paper, we propose a novel 3D\nscene-camera representation with joint camera photometric optimization. By\nintroducing internal and external photometric model, we propose a full\nphotometric model and corresponding camera representation. Based on\nsimultaneously optimizing the parameters of the camera representation, the\nproposed method effectively separates scene-unrelated information from the 3D\nscene representation. Additionally, during the optimization of the photometric\nparameters, we introduce a depth regularization to prevent the 3D scene\nrepresentation from fitting scene-unrelated information. By incorporating the\ncamera model as part of the mapping process, the proposed method constructs a\ncomplete map that includes both the scene radiance field and the camera\nphotometric model. Experimental results demonstrate that the proposed method\ncan achieve high-quality 3D scene representations, even under conditions of\nimaging degradation, such as vignetting and dirt.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20979v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20979v1", "AI": {"title_translation": "联合相机光度优化下的三维场景-相机表示", "tldr": "提出了一种联合相机光度优化的三维场景-相机表示方法，通过引入完整的相机光度模型和深度正则化，有效分离了相机畸变引入的非场景信息，从而在存在图像退化的情况下也能获得高质量的三维场景表示。", "motivation": "现有从多视角图像表示三维场景的方法，由于相机成像中固有的光度畸变，可能会将与场景无关的错误信息引入三维场景表示中，从而降低表示质量。", "method": "提出了一种新颖的联合相机光度优化的三维场景-相机表示方法。该方法引入了内部和外部光度模型，构建了一个完整的相机光度模型和对应的相机表示。通过同时优化相机表示的参数，有效分离了与场景无关的信息。此外，在光度参数优化过程中引入了深度正则化，以防止三维场景表示拟合与场景无关的信息。最终构建了一个包含场景辐射场和相机光度模型的完整映射。", "result": "实验结果表明，即使在存在晕影和污垢等成像退化条件下，所提出的方法也能实现高质量的三维场景表示。", "conclusion": "该论文提出了一种通过联合相机光度优化来提高三维场景表示质量的方法，成功地将相机引入的非场景信息与真实场景分离，并在存在成像退化的情况下取得了优异的表现。", "translation": "从多视角图像表示场景是计算机视觉中一个关键任务，具有广泛的应用。然而，相机成像中固有的光度畸变会显著降低图像质量。如果不考虑这些畸变，三维场景表示可能会无意中包含与场景无关的错误信息，从而降低表示质量。在本文中，我们提出了一种新颖的联合相机光度优化的三维场景-相机表示方法。通过引入内部和外部光度模型，我们提出了一个完整的光度模型和相应的相机表示。基于同时优化相机表示的参数，所提出的方法有效地将与场景无关的信息从三维场景表示中分离出来。此外，在光度参数优化过程中，我们引入了深度正则化，以防止三维场景表示拟合与场景无关的信息。通过将相机模型作为映射过程的一部分，所提出的方法构建了一个包含场景辐射场和相机光度模型的完整映射。实验结果表明，即使在晕影和污垢等成像退化条件下，所提出的方法也能实现高质量的三维场景表示。", "summary": "本文提出了一种新颖的联合相机光度优化的三维场景-相机表示方法，旨在解决相机光度畸变对三维场景表示质量的影响。通过引入完整的内部和外部光度模型以及深度正则化，该方法能够有效地将相机引入的非场景信息与真实三维场景表示分离。实验证明，即使在存在图像退化（如晕影和污垢）的情况下，该方法也能生成高质量的三维场景表示。", "keywords": "三维场景表示, 相机光度优化, 光度畸变, 深度正则化, 场景辐射场", "comments": "这篇论文通过将相机光度模型整合到三维场景表示的优化过程中，提供了一个创新的视角来解决图像畸变问题。其核心在于联合优化相机参数和场景表示，并引入深度正则化来提升鲁棒性，这对于在实际复杂成像条件下获得高质量三维重建具有重要意义。"}}
{"id": "2506.20898", "title": "Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction", "authors": ["Erfan Hajihashemi", "Yanning Shen"], "summary": "Online conformal prediction has demonstrated its capability to construct a\nprediction set for each incoming data point that covers the true label with a\npredetermined probability. To cope with potential distribution shift,\nmulti-model online conformal prediction has been introduced to select and\nleverage different models from a preselected candidate set. Along with the\nimproved flexibility, the choice of the preselected set also brings challenges.\nA candidate set that includes a large number of models may increase the\ncomputational complexity. In addition, the inclusion of irrelevant models with\npoor performance may negatively impact the performance and lead to\nunnecessarily large prediction sets. To address these challenges, we propose a\nnovel multi-model online conformal prediction algorithm that identifies a\nsubset of effective models at each time step by collecting feedback from a\nbipartite graph, which is refined upon receiving new data. A model is then\nselected from this subset to construct the prediction set, resulting in reduced\ncomputational complexity and smaller prediction sets. Additionally, we\ndemonstrate that using prediction set size as feedback, alongside model loss,\ncan significantly improve efficiency by constructing smaller prediction sets\nwhile still satisfying the required coverage guarantee. The proposed algorithms\nare proven to ensure valid coverage and achieve sublinear regret. Experiments\non real and synthetic datasets validate that the proposed methods construct\nsmaller prediction sets and outperform existing multi-model online conformal\nprediction approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20898v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20898v1", "AI": {"title_translation": "图结构反馈多模型集成在线共形预测", "tldr": "提出了一种基于图结构反馈的新型多模型在线共形预测算法，能动态选择有效模型子集，以降低计算复杂度和生成更小的预测集，同时保持覆盖率。", "motivation": "现有的多模型在线共形预测在处理分布偏移时，其预选模型集的选择面临挑战：模型数量过多会导致计算复杂性增加，而包含不相关模型则会负面影响性能并导致预测集过大。", "method": "提出了一种新颖的多模型在线共形预测算法。该算法通过从二分图中收集反馈来识别每个时间步的有效模型子集，该图会随着新数据的接收而细化。然后从该子集中选择一个模型来构建预测集。此外，除了模型损失外，还将预测集大小作为反馈来显著提高效率。", "result": "提出的算法能够构建更小的预测集，并被证明能确保有效的覆盖率并实现次线性遗憾。在真实和合成数据集上的实验验证了所提方法优于现有的多模型在线共形预测方法。", "conclusion": "本文提出了一种创新的图结构反馈多模型在线共形预测算法，有效解决了现有方法在计算复杂性和预测集大小方面的挑战，同时保证了预测的有效性和效率。", "translation": "在线共形预测已证明其能够为每个传入数据点构建一个预测集，该预测集以预定概率覆盖真实标签。为了应对潜在的分布偏移，引入了多模型在线共形预测，用于从预选的候选集中选择和利用不同的模型。在提高灵活性的同时，预选集的选择也带来了挑战。包含大量模型的候选集可能会增加计算复杂性。此外，包含性能不佳的不相关模型可能会对性能产生负面影响，并导致不必要的过大预测集。为了解决这些挑战，我们提出了一种新颖的多模型在线共形预测算法，该算法通过从二分图中收集反馈来识别每个时间步的有效模型子集，该图在接收新数据时会进行细化。然后从该子集中选择一个模型来构建预测集，从而降低了计算复杂性并减小了预测集。此外，我们证明了将预测集大小作为反馈，以及模型损失，可以通过构建更小的预测集来显著提高效率，同时仍满足所需的覆盖保证。所提出的算法被证明能够确保有效的覆盖率并实现次线性遗憾。在真实和合成数据集上的实验验证了所提出的方法构建了更小的预测集，并且优于现有的多模型在线共形预测方法。", "summary": "本文提出了一种新型的多模型在线共形预测算法，旨在解决现有方法中因模型集过大或包含不相关模型而导致的计算复杂性高和预测集过大的问题。该算法利用图结构反馈动态识别有效模型子集，并选择其中一个模型来构建预测集。通过将预测集大小作为额外反馈，进一步提高了效率。实验证明，该方法在保持有效覆盖率和次线性遗憾的同时，能生成更小的预测集，并优于现有方法。", "keywords": "在线共形预测, 多模型集成, 图结构反馈, 预测集, 模型选择", "comments": "这篇论文通过引入图结构反馈机制，创新性地解决了多模型在线共形预测中模型选择和效率的痛点。将预测集大小作为反馈信号，是提高算法效率的一个重要且实用的贡献。这种动态模型选择方法有望在实际应用中显著提升在线预测的性能和资源利用率。"}}
{"id": "2506.21275", "title": "Surrogate normal-forms for the numerical bifurcation and stability analysis of navier-stokes flows via machine learning", "authors": ["Alessandro Della Pia", "Dimitrios G. Patsatzis", "Gianluigi Rozza", "Lucia Russo", "Constantinos Siettos"], "summary": "Inspired by the Equation-Free multiscale modeling approach, we demonstrate\nhow the embed-learn-lift framework enables the construction of surrogate\nnormal-forms, namely minimal-dimensional reduced-order models (ROMs), from\nhigh-fidelity Navier-Stokes simulations. These surrogate models are then used\nfor efficient and accurate bifurcation and stability analysis. The framework\nproceeds in four steps. First, manifold learning reveals the intrinsic latent\ndimension of the high-dimensional spatio-temporal Navier-Stokes dynamics across\nparameter space. Second, we construct low-dimensional \"normal-form\" like ROMs\non this latent space using Gaussian Process Regression (GPR), capturing the\nemergent dynamics. Third, using these models, we apply numerical bifurcation\ntools to compute bifurcation diagrams and perform stability analysis in the\nlatent space. This includes tracing branches of limit cycles arising from\nAndronov-Hopf bifurcations - tasks intractable in full space due to\ncomputational cost. Finally, solving the pre-image problem allows\nreconstruction of the bifurcation structure in the original high-dimensional\nspace. We demonstrate the methodology on two canonical flows: wake flow past an\ninfinite circular cylinder and planar sudden-expansion channel flow. These\nexhibit Andronov-Hopf and pitchfork bifurcations, respectively, as Reynolds\nnumber increases. Our method identifies the latent dimensionality and\nconstructs GPR-based surrogate normal-forms that enable the tracing and\nstability analysis of bifurcating solutions, including limit cycles, their\nperiod, and stability via Floquet multipliers.", "comment": "26 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.21275v1", "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.21275v1", "AI": {"title_translation": "基于机器学习的纳维-斯托克斯流数值分岔和稳定性分析的代理范式", "tldr": "本文提出了一种基于机器学习的“嵌入-学习-提升”框架，通过构建代理范式（极小维度降阶模型）来对高维纳维-斯托克斯流进行高效准确的数值分岔和稳定性分析。", "motivation": "对高维纳维-斯托克斯流进行数值分岔和稳定性分析因计算成本高昂而难以在全空间中进行。", "method": "该框架分为四步：1. 流形学习揭示纳维-斯托克斯动力学的内在潜在维度；2. 利用高斯过程回归（GPR）在潜在空间中构建低维“范式”降阶模型；3. 使用这些模型，在潜在空间中应用数值分岔工具计算分岔图并进行稳定性分析；4. 解决原像问题以重建原始高维空间中的分岔结构。", "result": "该方法成功识别了潜在维度，并构建了基于GPR的代理范式，从而能够追踪和分析分岔解（包括极限环、其周期和稳定性），并在无限圆柱绕流和平面突扩通道流这两个典型流场上进行了验证，分别展示了Andronov-Hopf分岔和叉式分岔。", "conclusion": "基于机器学习的“嵌入-学习-提升”框架能够有效地构建代理范式，从而实现对高维纳维-斯托克斯流的计算效率高且准确的数值分岔和稳定性分析，解决了全空间分析计算成本过高的问题。", "translation": "受无方程多尺度建模方法的启发，本文展示了“嵌入-学习-提升”框架如何能够从高保真纳维-斯托克斯模拟中构建代理范式，即最小维度的降阶模型（ROMs）。这些代理模型随后被用于高效准确的分岔和稳定性分析。该框架分为四个步骤。首先，流形学习揭示了跨参数空间的高维时空纳维-斯托克斯动力学的内在潜在维度。其次，我们利用高斯过程回归（GPR）在该潜在空间中构建了低维“范式”类降阶模型，捕捉了涌现的动力学。第三，利用这些模型，我们应用数值分岔工具来计算潜在空间中的分岔图并进行稳定性分析。这包括追踪由Andronov-Hopf分岔产生的极限环分支——这些任务在全空间中因计算成本高昂而难以处理。最后，解决原像问题允许在原始高维空间中重建分岔结构。我们在两个典型流场上演示了该方法：无限圆柱绕流和平面突扩通道流。随着雷诺数的增加，它们分别表现出Andronov-Hopf分岔和叉式分岔。我们的方法识别了潜在维度并构建了基于GPR的代理范式，从而能够追踪和分析分岔解，包括极限环、它们的周期以及通过Floquet乘数确定的稳定性。", "summary": "本文提出了一种名为“嵌入-学习-提升”的机器学习框架，用于从高保真纳维-斯托克斯模拟中构建代理范式（最小维度降阶模型）。该框架通过流形学习识别潜在维度，利用高斯过程回归构建低维模型，进而在潜在空间中高效执行数值分岔和稳定性分析（包括极限环），最后将结果重建回原始高维空间。该方法在两个典型流场中得到了验证，有效解决了高维流体动力学分析的计算难题。", "keywords": "代理范式, 纳维-斯托克斯流, 数值分岔, 稳定性分析, 机器学习", "comments": "这项工作创新性地将机器学习（特别是流形学习和高斯过程回归）与传统的数值分岔分析相结合，为高维复杂流体动力学系统的分析提供了一种计算效率高的新途径。其“嵌入-学习-提升”框架概念清晰，有效地克服了全空间分析的计算瓶颈，对于理解和预测复杂流动的行为具有重要意义。"}}
{"id": "2506.21121", "title": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Lu Zhang", "Peiliang Li", "Shaojie Shen"], "summary": "Trajectory prediction for surrounding agents is a challenging task in\nautonomous driving due to its inherent uncertainty and underlying\nmultimodality. Unlike prevailing data-driven methods that primarily rely on\nsupervised learning, in this paper, we introduce a novel Graph-oriented Inverse\nReinforcement Learning (GoIRL) framework, which is an IRL-based predictor\nequipped with vectorized context representations. We develop a feature adaptor\nto effectively aggregate lane-graph features into grid space, enabling seamless\nintegration with the maximum entropy IRL paradigm to infer the reward\ndistribution and obtain the policy that can be sampled to induce multiple\nplausible plans. Furthermore, conditioned on the sampled plans, we implement a\nhierarchical parameterized trajectory generator with a refinement module to\nenhance prediction accuracy and a probability fusion strategy to boost\nprediction confidence. Extensive experimental results showcase our approach not\nonly achieves state-of-the-art performance on the large-scale Argoverse &\nnuScenes motion forecasting benchmarks but also exhibits superior\ngeneralization abilities compared to existing supervised models.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21121v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21121v1", "AI": {"title_translation": "GoIRL：面向图的逆强化学习用于多模态轨迹预测", "tldr": "GoIRL提出了一种基于逆强化学习（IRL）的框架，用于自动驾驶中的多模态轨迹预测，该框架结合了图特征聚合、分层轨迹生成和置信度融合，在Argoverse和nuScenes基准测试中取得了最先进的性能和更好的泛化能力。", "motivation": "自动驾驶中周围代理的轨迹预测由于其固有的不确定性和潜在的多模态性而具有挑战性。现有的数据驱动方法主要依赖于监督学习，本文旨在提出一种新颖的方法来解决这一问题。", "method": "本文引入了一种新颖的面向图的逆强化学习（GoIRL）框架，这是一种基于IRL的预测器，配备了向量化的上下文表示。它开发了一个特征适配器，将车道图特征有效地聚合到网格空间中，并与最大熵IRL范式集成，以推断奖励分布并获得可以采样以诱导多个合理计划的策略。此外，该方法还实现了分层参数化轨迹生成器，带有细化模块以提高预测精度，并使用概率融合策略以提高预测置信度。", "result": "GoIRL方法在大型Argoverse和nuScenes运动预测基准测试中不仅实现了最先进的性能，而且与现有监督模型相比，还表现出卓越的泛化能力。", "conclusion": "GoIRL框架通过结合逆强化学习、图特征聚合和分层轨迹生成，有效解决了自动驾驶中的多模态轨迹预测挑战，并在主要基准测试中取得了优异的性能和泛化能力。", "translation": "周围代理的轨迹预测是自动驾驶中一项具有挑战性的任务，因为其固有的不确定性和潜在的多模态性。与主要依赖监督学习的现有数据驱动方法不同，本文引入了一种新颖的面向图的逆强化学习（GoIRL）框架，这是一种基于IRL的预测器，配备了向量化的上下文表示。我们开发了一个特征适配器，以有效地将车道图特征聚合到网格空间中，从而实现与最大熵IRL范式的无缝集成，以推断奖励分布并获得可采样以诱导多个合理计划的策略。此外，以采样的计划为条件，我们实现了一个带有细化模块的分层参数化轨迹生成器，以提高预测精度，并使用概率融合策略以提高预测置信度。广泛的实验结果表明，我们的方法不仅在大型Argoverse和nuScenes运动预测基准测试中取得了最先进的性能，而且与现有监督模型相比，还表现出卓越的泛化能力。", "summary": "本文提出了一种名为GoIRL的面向图的逆强化学习框架，用于自动驾驶中的多模态轨迹预测。该框架通过特征适配器将车道图特征聚合到网格空间，并利用最大熵IRL推断奖励分布以生成多模态计划。在此基础上，采用分层轨迹生成器和概率融合策略来提高预测精度和置信度。实验证明，GoIRL在Argoverse和nuScenes基准测试中达到了最先进的性能，并展现出优异的泛化能力。", "keywords": "逆强化学习, 轨迹预测, 多模态, 图神经网络, 自动驾驶", "comments": "GoIRL的创新之处在于将逆强化学习引入多模态轨迹预测，并结合了图结构信息和分层生成策略。与传统的监督学习方法相比，其IRL范式能够更好地处理轨迹预测固有的不确定性和多模态性，并通过推断奖励分布来生成更合理的未来轨迹。其在大型基准测试上的优异表现和泛化能力证明了该方法的有效性和潜力。"}}
{"id": "2506.20983", "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation", "authors": ["Wenjie Xuan", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "summary": "Recent works favored dense signals (e.g., depth, DensePose), as an\nalternative to sparse signals (e.g., OpenPose), to provide detailed spatial\nguidance for pose-guided text-to-image generation. However, dense\nrepresentations raised new challenges, including editing difficulties and\npotential inconsistencies with textual prompts. This fact motivates us to\nrevisit sparse signals for pose guidance, owing to their simplicity and\nshape-agnostic nature, which remains underexplored. This paper proposes a novel\nSpatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust\ncontrollability for pose-guided image generation. Specifically, we extend\nOpenPose to a learnable spatial representation, making keypoint embeddings\ndiscriminative and expressive. Additionally, we introduce keypoint concept\nlearning, which encourages keypoint tokens to attend to the spatial positions\nof each keypoint, thus improving pose alignment. Experiments on animal- and\nhuman-centric image generation tasks demonstrate that our method outperforms\nrecent spatially controllable T2I generation approaches under sparse-pose\nguidance and even matches the performance of dense signal-based methods.\nMoreover, SP-Ctrl shows promising capabilities in diverse and cross-species\ngeneration through sparse signals. Codes will be available at\nhttps://github.com/DREAMXFAR/SP-Ctrl.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20983v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20983v1", "AI": {"title_translation": "重新思考稀疏信号在姿态引导的文本到图像生成中的应用", "tldr": "本文提出了一种名为Spatial-Pose ControlNet（SP-Ctrl）的新方法，通过扩展OpenPose并引入关键点概念学习，使稀疏姿态信号在文本到图像生成中具有强大的可控性，其性能优于现有稀疏姿态方法，甚至能与密集信号方法媲美。", "motivation": "当前研究倾向于使用密集信号（如深度、DensePose）为姿态引导的文本到图像生成提供详细的空间指导，但密集表示带来了编辑困难和与文本提示不一致的新挑战。这促使作者重新审视稀疏信号（如OpenPose）在姿态引导中的应用，因为它们具有简单性和形状无关性，且其潜力尚未被充分探索。", "method": "本文提出了一种新颖的Spatial-Pose ControlNet (SP-Ctrl)。具体而言，该方法将OpenPose扩展为可学习的空间表示，使关键点嵌入具有判别性和表达性。此外，它引入了关键点概念学习，鼓励关键点标记关注每个关键点的空间位置，从而改善姿态对齐。", "result": "在以动物和人类为中心的图像生成任务上的实验表明，SP-Ctrl在稀疏姿态引导下优于当前的空间可控T2I生成方法，甚至能与基于密集信号的方法性能相匹配。此外，SP-Ctrl在通过稀疏信号进行多样化和跨物种生成方面显示出良好的能力。", "conclusion": "通过重新利用稀疏姿态信号并提出SP-Ctrl，本文成功地解决了密集信号在姿态引导的文本到图像生成中面临的挑战，并展示了稀疏信号在提供强大可控性和实现高性能方面的潜力，甚至在多样化和跨物种生成中表现出色。", "translation": "最近的工作倾向于使用密集信号（例如深度、DensePose）作为稀疏信号（例如OpenPose）的替代品，为姿态引导的文本到图像生成提供详细的空间指导。然而，密集表示带来了新的挑战，包括编辑困难和与文本提示潜在的不一致性。这一事实促使我们重新审视稀疏信号在姿态引导中的应用，因为它们具有简单性和形状无关性，并且其潜力尚未被充分探索。本文提出了一种新颖的空间姿态控制网络（Spatial-Pose ControlNet，SP-Ctrl），使稀疏信号在姿态引导的图像生成中具有强大的可控性。具体而言，我们将OpenPose扩展为一种可学习的空间表示，使关键点嵌入具有判别性和表达性。此外，我们引入了关键点概念学习，鼓励关键点标记关注每个关键点的空间位置，从而改善姿态对齐。在以动物和人类为中心的图像生成任务上的实验表明，我们的方法在稀疏姿态引导下优于最近的空间可控T2I生成方法，甚至与基于密集信号的方法性能相匹配。此外，SP-Ctrl通过稀疏信号在多样化和跨物种生成方面显示出良好的能力。代码将在https://github.com/DREAMXFAR/SP-Ctrl提供。", "summary": "本文针对姿态引导的文本到图像生成中密集信号的挑战，提出了一种名为Spatial-Pose ControlNet (SP-Ctrl) 的新方法，旨在重新利用稀疏信号。SP-Ctrl通过将OpenPose扩展为可学习的空间表示并引入关键点概念学习，显著提升了稀疏姿态信号的可控性和对齐性。实验证明，该方法在稀疏姿态引导下表现优异，甚至能与密集信号方法相媲美，并在多样化和跨物种生成中展现出潜力。", "keywords": "稀疏信号, 姿态引导, 文本到图像生成, ControlNet, OpenPose", "comments": "该论文的创新点在于重新审视并有效利用了之前被认为不足的稀疏姿态信号，通过提出SP-Ctrl解决了密集信号的局限性。其重要性在于为姿态引导的文本到图像生成提供了一种更简单、更灵活且性能卓越的解决方案，尤其在编辑便利性和跨物种生成方面具有优势。"}}
{"id": "2506.21252", "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.", "comment": "ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2506.21252v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21252v1", "AI": {"title_translation": "智能体奖励基准：迈向真实世界多模态智能体在感知、规划和安全方面的统一奖励建模基准", "tldr": "提出了Agent-RewardBench，一个用于评估多模态大语言模型在真实世界多模态智能体中奖励建模能力的统一基准。", "motivation": "多模态智能体在缺乏外部反馈时难以自我纠正和泛化。奖励模型是一个有前景的方法，但目前缺乏针对智能体的奖励模型选择标准，因此迫切需要构建一个智能体奖励基准。", "method": "提出了Agent-RewardBench，一个旨在评估多模态大语言模型（MLLMs）奖励建模能力的基准。该基准具有三个关键特征：1) 涵盖感知、规划和安全等7个真实世界智能体场景的多维度评估；2) 步级奖励评估，提供更细粒度的性能视图；3) 适当的难度和高质量，通过从10个不同模型中采样、难度控制和人工验证确保数据完整性。", "result": "实验表明，即使是最先进的多模态模型也表现出有限的性能。", "conclusion": "迫切需要针对智能体奖励建模进行专门训练。", "translation": "随着多模态大语言模型（MLLMs）的进步，多模态智能体在网络导航和具身智能等真实世界任务中展现出前景。然而，由于缺乏外部反馈的限制，这些智能体在自我纠正和泛化方面面临困难。一个有前景的方法是使用奖励模型作为外部反馈，但目前尚不清楚如何为智能体选择奖励模型。因此，迫切需要建立一个针对智能体的奖励基准。为了解决这些挑战，我们提出了Agent-RewardBench，一个旨在评估MLLMs中奖励建模能力的基准。该基准具有三个关键特征：(1) 多维度和真实世界智能体场景评估。它涵盖了感知、规划和安全等7个场景；(2) 步级奖励评估。它允许在任务的单个步骤评估智能体能力，从而在规划过程中提供更细粒度的性能视图；以及(3) 适当的难度和高质量。我们从10个不同的模型中仔细采样，控制难度以保持任务挑战性，并进行人工验证以确保数据的完整性。实验表明，即使是最先进的多模态模型也表现出有限的性能，这凸显了对智能体奖励建模进行专门训练的需求。代码可在github上获取。", "summary": "本文提出了Agent-RewardBench，一个用于评估多模态大语言模型在真实世界多模态智能体中奖励建模能力的统一基准。该基准在感知、规划和安全等多个维度和步级进行评估，并确保数据的适当难度和高质量。实验结果表明，当前先进的多模态模型在奖励建模方面表现有限，强调了专门训练的必要性。", "keywords": "智能体奖励建模, 多模态大语言模型, 统一基准, 感知, 规划, 安全", "comments": "这项工作提出了一个急需的统一基准Agent-RewardBench，旨在解决多模态智能体在奖励建模方面的评估空白。其多维度、步级评估和高质量数据确保了评估的全面性和可靠性，对于推动多模态智能体在真实世界任务中的自校正和泛化能力具有重要意义。"}}
{"id": "2506.20986", "title": "EVA: Mixture-of-Experts Semantic Variant Alignment for Compositional Zero-Shot Learning", "authors": ["Xiao Zhang", "Yongqiang Ma", "Haodong Jing", "Nanning Zheng"], "summary": "Compositional Zero-Shot Learning (CZSL) investigates compositional\ngeneralization capacity to recognize unknown state-object pairs based on\nlearned primitive concepts. Existing CZSL methods typically derive primitives\nfeatures through a simple composition-prototype mapping, which is suboptimal\nfor a set of individuals that can be divided into distinct semantic subsets.\nMoreover, the all-to-one cross-modal primitives matching neglects compositional\ndivergence within identical states or objects, limiting fine-grained\nimage-composition alignment. In this study, we propose EVA, a\nMixture-of-Experts Semantic Variant Alignment framework for CZSL. Specifically,\nwe introduce domain-expert adaption, leveraging multiple experts to achieve\ntoken-aware learning and model high-quality primitive representations. To\nenable accurate compositional generalization, we further present semantic\nvariant alignment to select semantically relevant representation for\nimage-primitives matching. Our method significantly outperforms other\nstate-of-the-art CZSL methods on three popular benchmarks in both closed- and\nopen-world settings, demonstrating the efficacy of the proposed insight.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20986v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20986v1", "AI": {"title_translation": "EVA：专家混合语义变体对齐用于组合零样本学习", "tldr": "本文提出了EVA框架，通过专家混合和语义变体对齐，改进组合零样本学习中的原语表示和匹配，显著优于现有方法。", "motivation": "现有组合零样本学习（CZSL）方法在原语特征提取和跨模态匹配方面存在不足。简单的组合-原型映射次优，且一对多的跨模态原语匹配忽略了相同状态或对象内的组合差异，限制了细粒度的图像-组合对齐。", "method": "本文提出了EVA框架，一个专家混合语义变体对齐框架，用于CZSL。具体来说，引入了域专家自适应，利用多个专家实现令牌感知学习和建模高质量的原语表示。此外，提出了语义变体对齐，以选择语义相关的表示进行图像-原语匹配，从而实现准确的组合泛化。", "result": "EVA方法在三种流行的基准测试中，无论是在封闭世界还是开放世界设置下，都显著优于其他最先进的CZSL方法。", "conclusion": "提出的EVA框架通过专家混合和语义变体对齐，有效解决了CZSL中原语表示和匹配的挑战，并取得了优异的性能。", "translation": "组合零样本学习（CZSL）研究了基于学习到的原始概念识别未知状态-对象对的组合泛化能力。现有CZSL方法通常通过简单的组合-原型映射来推导原始特征，这对于可以分为不同语义子集的个体集合来说是次优的。此外，一对多的跨模态原始匹配忽略了相同状态或对象内的组合差异，限制了细粒度的图像-组合对齐。在本研究中，我们提出了EVA，一个用于CZSL的专家混合语义变体对齐框架。具体来说，我们引入了域专家自适应，利用多个专家实现令牌感知学习和建模高质量的原始表示。为了实现准确的组合泛化，我们进一步提出了语义变体对齐，以选择语义相关的表示进行图像-原始匹配。我们的方法在封闭世界和开放世界设置下，在三个流行的基准测试中显著优于其他最先进的CZSL方法，证明了所提出见解的有效性。", "summary": "本文提出了一个名为EVA的专家混合语义变体对齐框架，旨在解决组合零样本学习（CZSL）中现有方法在原语特征提取和图像-组合对齐方面的不足。EVA通过引入域专家自适应来学习高质量的原语表示，并通过语义变体对齐来选择最相关的表示进行匹配。实验结果表明，EVA在多个基准测试中显著超越了当前最先进的CZSL方法。", "keywords": "组合零样本学习, 专家混合, 语义变体对齐, 原语表示, 泛化能力", "comments": "这篇论文的创新点在于提出了一个专家混合（Mixture-of-Experts）的框架来处理组合零样本学习中的语义变体问题，并引入了域专家自适应和语义变体对齐这两个关键机制。这种方法能够更精细地建模和对齐原语概念，从而解决了现有方法在处理复杂语义组合时的局限性。其在多个基准测试上取得的显著优异性能，表明了该方法在提升CZSL泛化能力方面的重要性和有效性。"}}
{"id": "2506.20916", "title": "Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "Deep reinforcement learning has been extensively studied in decision-making\nprocesses and has demonstrated superior performance over conventional\napproaches in various fields, including radar resource management (RRM).\nHowever, a notable limitation of neural networks is their ``black box\" nature\nand recent research work has increasingly focused on explainable AI (XAI)\ntechniques to describe the rationale behind neural network decisions. One\npromising XAI method is local interpretable model-agnostic explanations (LIME).\nHowever, the sampling process in LIME ignores the correlations between\nfeatures. In this paper, we propose a modified LIME approach that integrates\ndeep learning (DL) into the sampling process, which we refer to as DL-LIME. We\nemploy DL-LIME within deep reinforcement learning for radar resource\nmanagement. Numerical results show that DL-LIME outperforms conventional LIME\nin terms of both fidelity and task performance, demonstrating superior\nperformance with both metrics. DL-LIME also provides insights on which factors\nare more important in decision making for radar resource management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20916v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20916v1", "AI": {"title_translation": "雷达资源管理的可解释人工智能：深度强化学习中的改进LIME", "tldr": "本文提出了一种名为DL-LIME的改进LIME方法，通过将深度学习集成到采样过程中，提高了雷达资源管理中深度强化学习的可解释性、保真度和任务性能。", "motivation": "深度强化学习在雷达资源管理等决策过程中表现出色，但其“黑箱”性质限制了对其决策原理的理解。现有的可解释人工智能（XAI）方法LIME在采样过程中忽略了特征之间的相关性。", "method": "本文提出了一种名为DL-LIME的改进LIME方法，该方法将深度学习（DL）集成到LIME的采样过程中。研究人员将DL-LIME应用于深度强化学习的雷达资源管理任务中。", "result": "数值结果表明，DL-LIME在保真度和任务性能方面均优于传统的LIME方法。DL-LIME还能揭示雷达资源管理决策中哪些因素更为重要。", "conclusion": "DL-LIME通过改进LIME的采样过程，成功提高了深度强化学习在雷达资源管理中决策的可解释性和性能，为理解“黑箱”模型提供了有效工具。", "translation": "深度强化学习在决策过程中得到了广泛研究，并在包括雷达资源管理（RRM）在内的各个领域中表现出优于传统方法的性能。然而，神经网络的一个显著局限性是其“黑箱”性质，最近的研究工作越来越关注可解释人工智能（XAI）技术，以描述神经网络决策背后的原理。一种有前景的XAI方法是局部可解释模型无关解释（LIME）。然而，LIME中的采样过程忽略了特征之间的相关性。在本文中，我们提出了一种改进的LIME方法，将深度学习（DL）集成到采样过程中，我们称之为DL-LIME。我们将DL-LIME应用于深度强化学习的雷达资源管理中。数值结果表明，DL-LIME在保真度和任务性能方面均优于传统的LIME，在两项指标上都表现出卓越的性能。DL-LIME还提供了关于哪些因素在雷达资源管理决策中更重要的见解。", "summary": "本文针对深度强化学习在雷达资源管理中“黑箱”决策的局限性，提出了一种改进的LIME方法——DL-LIME。DL-LIME通过将深度学习融入LIME的采样过程，有效解决了传统LIME忽略特征相关性的问题。实验结果表明，DL-LIME在提高解释的保真度和任务性能方面均优于传统LIME，并能揭示雷达资源管理决策中的关键影响因素，从而增强了深度强化学习的可解释性。", "keywords": "深度强化学习, 可解释人工智能, LIME, 雷达资源管理, DL-LIME", "comments": "本文的创新点在于提出了DL-LIME，通过将深度学习引入LIME的采样过程，解决了传统LIME在处理特征相关性方面的不足。这对于提高深度强化学习在实际应用中（如雷达资源管理）的透明度和可信度具有重要意义。该方法不仅提升了解释的准确性，还改善了模型的任务性能，为“黑箱”模型的解释提供了一个有效且实用的途径。"}}
{"id": "2506.21274", "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness", "comment": "(Submitted for publication)", "pdf_url": "http://arxiv.org/pdf/2506.21274v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21274v1", "AI": {"title_translation": "猫捉老鼠——虚假文本生成能否超越检测系统？", "tldr": "大型语言模型生成的虚假文本能否被检测系统持续有效识别？研究发现，在经典侦探小说风格的虚假文本生成中，Gemini模型的欺骗性有所增强，而GPT没有，表明即使模型越来越大，可靠检测仍可能可行，但新的模型架构可能会提高其欺骗性。", "motivation": "鉴于大型语言模型（LLMs）能够生成令人信服的“虚假文本”，并且检测这些文本的方法不断发展，本文旨在探讨这种“军备竞赛”是否会达到一个瓶颈，即生成模型的能力是否会超越检测系统。", "method": "研究人员通过检查统计分类器识别经典侦探小说风格的“虚假文本”的能力来解决这个问题。具体比较了Gemini和GPT模型在版本更新后的欺骗性。", "result": "在0.5版本更新后，Gemini模型生成欺骗性文本的能力有所增强，而GPT模型则没有表现出这种趋势。", "conclusion": "研究结果表明，即使面对不断增大的模型，虚假文本的可靠检测仍然是可行的。然而，新的模型架构可能会提高其欺骗性。", "translation": "大型语言模型可以在学术写作、产品评论和政治新闻等领域生成令人信服的“虚假文本”。许多方法已被研究用于检测人工智能生成的文本。虽然这似乎预示着一场无休止的“军备竞赛”，但我们注意到，更新的LLM使用了越来越多的参数、训练数据和能源，而相对简单的分类器则以适度的资源展示了良好的检测准确性。为了探讨模型击败检测器的能力是否因此达到一个平台期，我们检查了统计分类器识别经典侦探小说风格的“虚假文本”的能力。在0.5版本升级后，我们发现Gemini生成欺骗性文本的能力有所增强，而GPT则没有。这表明，即使对于越来越大的模型，虚假文本的可靠检测可能仍然可行，尽管新的模型架构可能会提高其欺骗性。", "summary": "本文探讨了大型语言模型（LLMs）生成的虚假文本与检测系统之间的“猫捉老鼠”式竞争。研究发现，尽管LLMs不断增大且资源消耗更多，但相对简单的分类器仍能有效检测。通过分析统计分类器识别经典侦探小说风格虚假文本的能力，结果显示Gemini在版本升级后生成欺骗性文本的能力有所提高，而GPT没有。这暗示着虚假文本的可靠检测即使面对大型模型仍可能实现，但新型模型架构可能增强其欺骗性。", "keywords": "虚假文本检测, 大型语言模型, 文本生成, Gemini, GPT", "comments": "本文通过具体实验对比了不同LLM在生成欺骗性文本方面的表现，为虚假文本检测的未来趋势提供了初步见解。其创新点在于将“猫捉老鼠”的博弈概念应用于LLM生成与检测领域，并指出模型架构而非单纯的模型规模可能是影响欺骗性的关键因素。这对于理解AI内容安全和发展更有效的检测策略具有重要意义。"}}
{"id": "2506.20988", "title": "Segment Anything in Pathology Images with Natural Language", "authors": ["Zhixuan Chen", "Junlin Hou", "Liqi Lin", "Yihui Wang", "Yequan Bie", "Xi Wang", "Yanning Zhou", "Ronald Cheong Kin Chan", "Hao Chen"], "summary": "Pathology image segmentation is crucial in computational pathology for\nanalyzing histological features relevant to cancer diagnosis and prognosis.\nHowever, current methods face major challenges in clinical applications due to\nlimited annotated data and restricted category definitions. To address these\nlimitations, we propose PathSegmentor, the first text-prompted segmentation\nfoundation model designed specifically for pathology images. We also introduce\nPathSeg , the largest and most comprehensive dataset for pathology\nsegmentation, built from 17 public sources and containing 275k image-mask-label\ntriples across 160 diverse categories. With PathSegmentor, users can perform\nsemantic segmentation using natural language prompts, eliminating the need for\nlaborious spatial inputs such as points or boxes. Extensive experiments\ndemonstrate that PathSegmentor outperforms specialized models with higher\naccuracy and broader applicability, while maintaining a compact architecture.\nIt significantly surpasses existing spatial- and text-prompted models by 0.145\nand 0.429 in overall Dice scores, respectively, showing strong robustness in\nsegmenting complex structures and generalizing to external datasets. Moreover,\nPathSegmentor's outputs enhance the interpretability of diagnostic models\nthrough feature importance estimation and imaging biomarker discovery, offering\npathologists evidence-based support for clinical decision-making. This work\nadvances the development of explainable AI in precision oncology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20988v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20988v1", "AI": {"title_translation": "使用自然语言在病理图像中分割一切", "tldr": "PathSegmentor是首个用于病理图像分割的文本提示基础模型，它通过自然语言提示实现语义分割，性能优于现有方法，并提升了诊断模型的可解释性。", "motivation": "当前的病理图像分割方法在临床应用中面临标注数据有限和类别定义受限的重大挑战。", "method": "提出了PathSegmentor，这是首个专为病理图像设计的文本提示分割基础模型。同时引入了PathSeg，一个最大、最全面的病理分割数据集，包含来自17个公共来源的27.5万个图像-掩码-标签三元组，涵盖160个不同类别。PathSegmentor允许用户使用自然语言提示进行语义分割，无需繁琐的空间输入（如点或框）。", "result": "PathSegmentor在准确性和适用性方面优于专用模型，并保持紧凑架构。在Dice分数上分别显著超越现有空间提示和文本提示模型0.145和0.429，在分割复杂结构和泛化到外部数据集方面表现出强大的鲁棒性。此外，PathSegmentor的输出通过特征重要性估计和影像生物标志物发现增强了诊断模型的可解释性，为病理学家提供循证支持。", "conclusion": "这项工作推动了精准肿瘤学中可解释人工智能的发展。", "translation": "病理图像分割在计算病理学中对于分析与癌症诊断和预后相关的组织学特征至关重要。然而，由于标注数据有限和类别定义受限，当前方法在临床应用中面临重大挑战。为了解决这些限制，我们提出了PathSegmentor，这是首个专为病理图像设计的文本提示分割基础模型。我们还引入了PathSeg，这是最大、最全面的病理分割数据集，其构建自17个公共来源，包含160个不同类别的27.5万个图像-掩码-标签三元组。通过PathSegmentor，用户可以使用自然语言提示执行语义分割，无需费力的空间输入，例如点或框。大量实验表明，PathSegmentor以更高的准确性和更广泛的适用性优于专用模型，同时保持紧凑的架构。它在总体Dice分数上分别显著超越现有空间提示和文本提示模型0.145和0.429，在分割复杂结构和泛化到外部数据集方面显示出强大的鲁棒性。此外，PathSegmentor的输出通过特征重要性估计和影像生物标志物发现增强了诊断模型的可解释性，为病理学家提供循证支持，从而辅助临床决策。这项工作推动了精准肿瘤学中可解释人工智能的发展。", "summary": "本研究提出PathSegmentor，首个专为病理图像设计的文本提示分割基础模型，旨在解决现有病理图像分割方法在临床应用中面临的标注数据有限和类别定义受限的挑战。该模型结合了PathSeg数据集，一个包含160个类别、27.5万个图像-掩码-标签三元组的大型综合数据集，允许用户通过自然语言提示进行语义分割，无需空间输入。实验证明，PathSegmentor在准确性、适用性和泛化能力上均显著优于现有模型，并能增强诊断模型的可解释性，为临床决策提供支持。", "keywords": "病理图像分割, 自然语言, 基础模型, 文本提示, 可解释AI", "comments": "这项工作具有显著的创新性，首次将文本提示基础模型引入病理图像分割领域，极大地简化了分割操作，并提高了效率。其构建的大规模PathSeg数据集也为该领域提供了宝贵的资源。PathSegmentor不仅提升了分割性能，更重要的是，它通过增强诊断模型的可解释性，为精准肿瘤学中的AI应用提供了更可靠的依据，具有重要的临床转化潜力。"}}
{"id": "2506.21285", "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.21285v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21285v1", "AI": {"title_translation": "Double-Checker：通过自我批判微调增强慢思考LLMs的推理能力", "tldr": "Double-Checker是一个通过自我批判微调来增强慢思考LLMs推理能力的框架，显著提升了其在推理基准上的表现，尤其是在AIME测试中。", "motivation": "慢思考大型语言模型（LLMs）虽然展现出类似反思的推理能力，但其生成信息性批判和完善先前解决方案的能力有限。", "method": "本文引入了Double-Checker框架，旨在通过促进明确的自我批判和对其先前解决方案的迭代完善来增强慢思考LLMs的推理能力。该框架通过对1,730个自我批判实例进行微调，使long-CoT LLMs能够在推理过程中迭代地批判和完善其输出，直到它们根据自我生成的批判将解决方案评估为正确。", "result": "Double-Checker在全面的推理基准测试中验证了其有效性，表明迭代自我批判显著增强了long-CoT LLMs的推理能力。值得注意的是，与原始的long-CoT LLMs相比，Double-Checker将具有挑战性的AIME基准测试中的pass@1性能从4.4%提高到18.2%。", "conclusion": "这些结果为开发更值得信赖、更有效的、能够进行结构化自我批判的LLMs指明了一个有前途的方向。", "translation": "虽然慢思考大型语言模型（LLMs）展现出类似反思的推理能力，通常被称为“顿悟时刻”，但它们生成信息性批判和完善先前解决方案的能力仍然有限。在本文中，我们引入了Double-Checker，这是一个旨在通过促进明确的自我批判和对其先前解决方案的迭代完善来增强慢思考LLMs推理能力的原则性框架。通过对我们精选的1,730个自我批判实例进行微调，Double-Checker使long-CoT LLMs能够在推理过程中迭代地批判和完善其输出，直到它们根据自我生成的批判将解决方案评估为正确。我们在全面的推理基准测试中验证了Double-Checker的有效性，表明迭代自我批判显著增强了long-CoT LLMs的推理能力。值得注意的是，我们的Double-Checker将具有挑战性的AIME基准测试中的pass@1性能从原始long-CoT LLMs的4.4%提高到18.2%。这些结果为开发更值得信赖、更有效的、能够进行结构化自我批判的LLMs指明了一个有前途的方向。", "summary": "本文提出了Double-Checker，一个旨在通过自我批判微调来提升慢思考LLMs推理能力的框架。通过对1,730个自我批判实例的精细训练，Double-Checker使LLMs能迭代地批判并完善其解决方案，直至自我评估为正确。实验结果表明，该方法显著提升了LLMs在多项推理基准上的表现，特别是在AIME测试中将pass@1性能从4.4%提高到18.2%，为开发更可靠且具自我批判能力的LLMs提供了新方向。", "keywords": "LLMs, 自我批判, 推理, 微调, Double-Checker", "comments": "Double-Checker的创新之处在于其通过“自我批判微调”来增强LLMs的推理能力，这是一种新颖且有效的提升模型反思和纠错能力的方法。该方法通过迭代地自我评估和完善输出，显著提升了模型在复杂推理任务上的性能，尤其是在数学推理方面。这对于构建更自主、更值得信赖的AI系统具有重要意义，因为它让LLMs能够像人类一样进行内部的反思和修正，而不是简单地输出首次生成的结果。"}}
{"id": "2506.20991", "title": "TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation", "authors": ["Chade Li", "Pengju Zhang", "Yihong Wu"], "summary": "The rapid advancement of 3D vision-language models (VLMs) has spurred\nsignificant interest in interactive point cloud processing tasks, particularly\nfor real-world applications. However, existing methods often underperform in\npoint-level tasks, such as segmentation, due to missing direct 3D-text\nalignment, limiting their ability to link local 3D features with textual\ncontext. To solve this problem, we propose TSDASeg, a Two-Stage model coupled\nwith a Direct cross-modal Alignment module and memory module for interactive\npoint cloud Segmentation. We introduce the direct cross-modal alignment module\nto establish explicit alignment between 3D point clouds and textual/2D image\ndata. Within the memory module, we employ multiple dedicated memory banks to\nseparately store text features, visual features, and their cross-modal\ncorrespondence mappings. These memory banks are dynamically leveraged through\nself-attention and cross-attention mechanisms to update scene-specific features\nbased on prior stored data, effectively addressing inconsistencies in\ninteractive segmentation results across diverse scenarios. Experiments\nconducted on multiple 3D instruction, reference, and semantic segmentation\ndatasets demonstrate that the proposed method achieves state-of-the-art\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20991v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20991v1", "AI": {"title_translation": "TSDASeg：一种用于交互式点云分割的直接对齐两阶段模型", "tldr": "TSDASeg提出了一种两阶段模型，通过直接跨模态对齐和记忆模块，解决了现有方法在交互式点云分割中缺乏3D-文本直接对齐的问题，并在多个数据集上达到了最先进的性能。", "motivation": "现有的3D视觉-语言模型在点级别任务（如分割）中表现不佳，因为它们缺少直接的3D-文本对齐，这限制了它们将局部3D特征与文本上下文关联起来的能力。", "method": "我们提出了TSDASeg，一个结合了直接跨模态对齐模块和记忆模块的两阶段模型。直接跨模态对齐模块用于建立3D点云与文本/2D图像数据之间的显式对齐。记忆模块使用多个专门的记忆库分别存储文本特征、视觉特征及其跨模态对应映射。这些记忆库通过自注意力和交叉注意力机制动态利用，根据先前存储的数据更新场景特定特征，以解决不同场景中交互式分割结果的不一致性。", "result": "在多个3D指令、参考和语义分割数据集上进行的实验表明，所提出的方法实现了最先进的性能。", "conclusion": "TSDASeg通过引入直接跨模态对齐和动态记忆模块，有效解决了交互式点云分割中3D-文本对齐不足的问题，并在各项任务中取得了领先的性能。", "translation": "3D视觉-语言模型（VLMs）的快速发展激发了人们对交互式点云处理任务的浓厚兴趣，尤其是在实际应用中。然而，现有方法在点级别任务（如分割）中往往表现不佳，原因在于缺少直接的3D-文本对齐，这限制了它们将局部3D特征与文本上下文关联的能力。为了解决这个问题，我们提出了TSDASeg，一个结合了直接跨模态对齐模块和记忆模块的两阶段模型，用于交互式点云分割。我们引入了直接跨模态对齐模块，以在3D点云和文本/2D图像数据之间建立显式对齐。在记忆模块内部，我们采用了多个专用记忆库，分别存储文本特征、视觉特征及其跨模态对应映射。这些记忆库通过自注意力和交叉注意力机制动态利用，根据先前存储的数据更新场景特定特征，从而有效解决了不同场景中交互式分割结果的不一致性。在多个3D指令、参考和语义分割数据集上进行的实验表明，所提出的方法实现了最先进的性能。", "summary": "TSDASeg是一种用于交互式点云分割的两阶段模型，旨在解决现有方法在3D-文本直接对齐方面的不足。该模型引入了直接跨模态对齐模块，以建立3D点云与文本/2D图像数据的显式关联，并设计了一个记忆模块，通过多记忆库和注意力机制动态更新场景特征，从而提高分割结果的一致性。实验证明，TSDASeg在多种3D分割任务上均取得了最先进的性能。", "keywords": "交互式点云分割, 3D-文本对齐, 两阶段模型, 记忆模块, 视觉-语言模型", "comments": "该论文的创新点在于提出了直接的3D-文本对齐机制以及一个动态记忆模块，以解决现有方法在交互式点云分割中缺乏局部特征与文本上下文关联的问题。这种方法通过显式对齐和记忆库来提升跨模态理解和结果的一致性，对于推动3D视觉-语言模型在实际点云处理任务中的应用具有重要意义。"}}
{"id": "2506.20927", "title": "Interpretable Representation Learning for Additive Rule Ensembles", "authors": ["Shahrzad Behzadimanesh", "Pierre Le Bodic", "Geoffrey I. Webb", "Mario Boley"], "summary": "Small additive ensembles of symbolic rules offer interpretable prediction\nmodels. Traditionally, these ensembles use rule conditions based on\nconjunctions of simple threshold propositions $x \\geq t$ on a single input\nvariable $x$ and threshold $t$, resulting geometrically in axis-parallel\npolytopes as decision regions. While this form ensures a high degree of\ninterpretability for individual rules and can be learned efficiently using the\ngradient boosting approach, it relies on having access to a curated set of\nexpressive and ideally independent input features so that a small ensemble of\naxis-parallel regions can describe the target variable well. Absent such\nfeatures, reaching sufficient accuracy requires increasing the number and\ncomplexity of individual rules, which diminishes the interpretability of the\nmodel. Here, we extend classical rule ensembles by introducing logical\npropositions with learnable sparse linear transformations of input variables,\ni.e., propositions of the form $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$, where\n$\\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as\ngeneral polytopes with oblique faces. We propose a learning method using\nsequential greedy optimization based on an iteratively reweighted formulation\nof logistic regression. Experimental results demonstrate that the proposed\nmethod efficiently constructs rule ensembles with the same test risk as\nstate-of-the-art methods while significantly reducing model complexity across\nten benchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20927v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20927v1", "AI": {"title_translation": "可解释的加性规则集成表示学习", "tldr": "本文通过引入可学习的稀疏线性变换，扩展了传统规则集成，使其能处理倾斜决策区域，从而在保持相同预测风险的同时显著降低模型复杂度。", "motivation": "传统的加性规则集成模型虽然可解释，但其基于单变量阈值的方法（轴平行多面体）依赖于高质量的输入特征。如果缺乏此类特征，为达到足够的准确性就需要增加规则的数量和复杂性，从而损害模型的可解释性。", "method": "提出了一种扩展经典规则集成的方法，引入了带有可学习稀疏线性变换的逻辑命题，即形式为 $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$ 的命题，从而实现具有倾斜面的更通用多面体决策区域。学习方法采用基于迭代重加权逻辑回归的序列贪婪优化。", "result": "实验结果表明，所提出的方法能够高效地构建规则集成，在十个基准数据集上达到与最新技术方法相同的测试风险，同时显著降低了模型复杂性。", "conclusion": "通过引入可学习的稀疏线性变换，本方法能够构建出在保持预测性能的同时，模型复杂度显著降低的可解释规则集成。", "translation": "加性符号规则的小型集成提供了可解释的预测模型。传统上，这些集成使用基于单个输入变量 $x$ 和阈值 $t$ 的简单阈值命题 $x \\geq t$ 的合取规则条件，几何上导致决策区域为轴平行多面体。虽然这种形式确保了单个规则的高度可解释性，并且可以使用梯度提升方法高效学习，但它依赖于访问一组经过精心策划的、表达能力强且理想情况下独立的输入特征，以便少量轴平行区域的集成能够很好地描述目标变量。在缺乏此类特征的情况下，达到足够的准确性需要增加单个规则的数量和复杂性，这会降低模型的可解释性。\n本文通过引入带有可学习稀疏线性变换的输入变量的逻辑命题，即形式为 $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$ 的命题，扩展了经典规则集成，其中 $\\mathbf{w}$ 是一个可学习的稀疏权重向量，从而使决策区域成为具有倾斜面的通用多面体。我们提出了一种基于迭代重加权逻辑回归的序列贪婪优化学习方法。实验结果表明，所提出的方法能够高效地构建规则集成，在十个基准数据集上达到与最新技术方法相同的测试风险，同时显著降低了模型复杂性。", "summary": "本文针对传统加性规则集成在缺乏高质量特征时可解释性下降的问题，提出了一种新的方法。该方法通过引入带有可学习稀疏线性变换的逻辑命题，使得决策区域能够形成具有倾斜面的通用多面体。采用基于迭代重加权逻辑回归的序列贪婪优化进行学习。实验证明，该方法在保持与现有技术相当的预测性能的同时，显著降低了模型复杂度，提升了模型的可解释性。", "keywords": "可解释性, 规则集成, 稀疏线性变换, 模型复杂度, 梯度提升", "comments": "这篇论文通过引入可学习的稀疏线性变换，创新性地解决了传统规则集成模型在特征不足时可解释性下降的问题。其核心贡献在于将轴平行决策边界扩展为倾斜边界，从而在不牺牲准确性的前提下显著简化了模型。这种方法对于需要高度可解释性但数据特征不尽理想的应用场景具有重要意义。"}}
{"id": "2506.21234", "title": "Real-Time ESFP: Estimating, Smoothing, Filtering, and Pose-Mapping", "authors": ["Qifei Cui", "Yuang Zhou", "Ruichen Deng"], "summary": "This paper presents ESFP, an end-to-end pipeline that converts monocular RGB\nvideo into executable joint trajectories for a low-cost 4-DoF desktop arm. ESFP\ncomprises four sequential modules. (1) Estimating: ROMP lifts each frame to a\n24-joint 3-D skeleton. (2) Smoothing: the proposed HPSTM-a sequence-to-sequence\nTransformer with self-attention-combines long-range temporal context with a\ndifferentiable forward-kinematics decoder, enforcing constant bone lengths and\nanatomical plausibility while jointly predicting joint means and full\ncovariances. (3) Filtering: root-normalized trajectories are variance-weighted\naccording to HPSTM's uncertainty estimates, suppressing residual noise. (4)\nPose-Mapping: a geometric retargeting layer transforms shoulder-elbow-wrist\ntriples into the uArm's polar workspace, preserving wrist orientation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21234v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21234v1", "AI": {"title_translation": "实时ESFP：估计、平滑、滤波和姿态映射", "tldr": "ESFP是一个将单目RGB视频转换为低成本四自由度桌面机械臂可执行关节轨迹的端到端管道，包含估计、平滑、滤波和姿态映射四个模块。", "motivation": "该论文旨在开发一个端到端管道，将单目RGB视频转换为低成本四自由度桌面机械臂的可执行关节轨迹，实现从视频到机械臂控制的自动化。", "method": "论文提出了ESFP管道，包含四个顺序模块：1. 估计: 使用ROMP将每一帧视频提升为24个关节的3D骨架。2. 平滑: 引入HPSTM（一种带有自注意力机制的序列到序列Transformer），结合长时间范围的时间上下文和可微分的正向运动学解码器，以确保骨骼长度恒定和解剖学合理性，同时共同预测关节均值和完整协方差。3. 滤波: 根据HPSTM的不确定性估计对根归一化轨迹进行方差加权，以抑制残余噪声。4. 姿态映射: 一个几何重定向层将肩-肘-腕三元组转换为uArm的极坐标工作空间，同时保留腕部方向。", "result": "论文展示了ESFP管道，能够将单目RGB视频转换为用于低成本4自由度桌面机械臂的可执行关节轨迹。该管道通过其四个模块（估计、平滑、滤波和姿态映射）实现了人体姿态到机械臂运动的实时转换。", "conclusion": "Not mentioned in abstract", "translation": "这篇论文提出了ESFP，一个将单目RGB视频转换为低成本四自由度桌面机械臂可执行关节轨迹的端到端管道。ESFP包含四个顺序模块。(1) 估计：ROMP将每一帧提升为24个关节的3D骨架。(2) 平滑：所提出的HPSTM——一个带有自注意力机制的序列到序列Transformer——结合了长范围时间上下文和可微分正向运动学解码器，强制执行恒定的骨骼长度和解剖学合理性，同时共同预测关节均值和完整协方差。(3) 滤波：根据HPSTM的不确定性估计对根归一化轨迹进行方差加权，以抑制残余噪声。(4) 姿态映射：一个几何重定向层将肩-肘-腕三元组转换为uArm的极坐标工作空间，同时保留腕部方向。", "summary": "ESFP是一个创新的端到端系统，它能够将单目RGB视频实时转换为低成本四自由度桌面机械臂的可执行关节轨迹。该系统由四个关键模块组成：首先，利用ROMP进行3D骨架估计；其次，通过提出的HPSTM（一个结合了时间上下文和可微分正向运动学解码器的Transformer）对关节轨迹进行平滑处理，确保解剖学合理性并预测不确定性；接着，利用HPSTM的不确定性进行方差加权滤波以抑制噪声；最后，通过几何重定向层将人体姿态映射到机械臂的工作空间，同时保持腕部方向。", "keywords": "实时姿态估计, 机器人控制, 单目视频, 运动映射, Transformer", "comments": "这篇论文提出了一种新颖的端到端管道ESFP，用于将人体运动实时映射到低成本机械臂。其创新点在于引入了HPSTM Transformer进行平滑处理，通过结合时间上下文、可微分正向运动学解码器以及对骨骼长度和解剖合理性的强制约束，显著提高了姿态估计的质量和鲁棒性，同时提供了不确定性估计用于后续滤波。这对于低成本人机交互和自动化任务具有重要意义。"}}
{"id": "2506.20941", "title": "Model State Arithmetic for Machine Unlearning", "authors": ["Keivan Rezaei", "Mehrdad Saberi", "Abhilasha Ravichander", "Soheil Feizi"], "summary": "Large language models are trained on massive corpora of web data, which may\ninclude private data, copyrighted material, factually inaccurate data, or data\nthat degrades model performance. Eliminating the influence of such problematic\ndatapoints through complete retraining -- by repeatedly pretraining the model\non datasets that exclude these specific instances -- is computationally\nprohibitive. For this reason, unlearning algorithms have emerged that aim to\neliminate the influence of particular datapoints, while otherwise preserving\nthe model -- at a low computational cost. However, precisely estimating and\nundoing the influence of individual datapoints has proved to be challenging. In\nthis work, we propose a new algorithm, MSA, for estimating and undoing the\ninfluence of datapoints -- by leveraging model checkpoints i.e. artifacts\ncapturing model states at different stages of pretraining. Our experimental\nresults demonstrate that MSA consistently outperforms existing machine\nunlearning algorithms across multiple benchmarks, models, and evaluation\nmetrics, suggesting that MSA could be an effective approach towards more\nflexible large language models that are capable of data erasure.", "comment": "Preprint. Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.20941v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20941v1", "AI": {"title_translation": "模型状态算术用于机器遗忘", "tldr": "本文提出了一种名为MSA的新算法，通过利用模型检查点来估计和消除数据点的影响，从而实现高效的机器遗忘，并优于现有算法。", "motivation": "大型语言模型在大量网络数据上训练，可能包含隐私、版权、不准确或降低性能的数据。通过完全重新训练来消除这些问题数据点的影响计算成本过高，因此需要低成本的遗忘算法。", "method": "本文提出了一种新的算法MSA（Model State Arithmetic），通过利用模型检查点（即捕获预训练不同阶段模型状态的工件）来估计和消除数据点的影响。", "result": "实验结果表明，MSA在多个基准、模型和评估指标上始终优于现有的机器遗忘算法。", "conclusion": "MSA可以成为一种有效的方法，实现更灵活、能够进行数据擦除的大型语言模型。", "translation": "大型语言模型在海量网络数据语料库上进行训练，其中可能包含私人数据、受版权保护的材料、事实不准确的数据或降低模型性能的数据。通过完全重新训练（即在排除这些特定实例的数据集上重复预训练模型）来消除此类问题数据点的影响，计算成本过高。因此，出现了旨在消除特定数据点影响，同时以低计算成本保留模型的遗忘算法。然而，精确估计和消除单个数据点的影响已被证明具有挑战性。在这项工作中，我们提出了一种新的算法MSA，通过利用模型检查点（即捕获预训练不同阶段模型状态的工件）来估计和消除数据点的影响。我们的实验结果表明，MSA在多个基准、模型和评估指标上始终优于现有的机器遗忘算法，这表明MSA可能是一种有效的方法，有助于实现更灵活、能够进行数据擦除的大型语言模型。", "summary": "本文针对大型语言模型中移除问题数据的挑战，提出了一种名为MSA的新型机器遗忘算法。该算法通过利用模型预训练过程中的检查点来高效地估计并消除特定数据点的影响。实验证明，MSA在多个场景下均优于现有遗忘算法，为实现可数据擦除的灵活大模型提供了有效途径。", "keywords": "机器遗忘, 大型语言模型, 模型检查点, 数据擦除, MSA", "comments": "这项工作在机器遗忘领域具有重要意义，特别是在大型语言模型背景下。其创新点在于利用模型检查点来解决传统重新训练成本高昂的问题，提供了一种更高效、更具可扩展性的数据擦除方法。MSA的提出有望促进更符合隐私、版权和数据质量要求的大模型的开发。"}}
{"id": "2506.21294", "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "authors": ["Bram Willemsen", "Gabriel Skantze"], "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.", "comment": "Accepted for publication at XLLM @ ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21294v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21294v1", "AI": {"title_translation": "使用自回归语言模型检测视觉接地对话中的指称表达", "tldr": "本文探讨了使用纯文本自回归语言模型从视觉接地对话中提取指称表达，发现即使是纯文本方法在语言上下文的帮助下也能有效，但该任务本质上是多模态的。", "motivation": "旨在探究仅凭语言上下文能在多大程度上帮助检测在对话视觉上下文中具有（视觉上可感知）指代物的提及。", "method": "采用纯文本、自回归语言建模方法。具体地，通过下一词元预测来划定文本中的提及范围边界，从而将预训练的大型语言模型（LLM）适配到对话中进行粗粒度提及范围标注。", "result": "研究结果表明，即使使用中等规模的LLM、相对较小的数据集和参数高效的微调，纯文本方法也能有效，这突出了语言上下文对于此任务的相对重要性。", "conclusion": "尽管纯文本方法有效，但作者认为该任务本质上是一个多模态问题，并讨论了单模态方法的根本局限性。", "translation": "在本文中，我们探索了使用一种仅基于文本的自回归语言建模方法，从视觉接地对话中提取指称表达。更具体地说，目标是调查仅凭语言上下文能在多大程度上帮助检测在对话视觉上下文中具有（视觉上可感知）指代物的提及。为此，我们调整了一个预训练的大型语言模型（LLM），通过下一词元预测在文本中划定提及范围边界，从而在展开的对话中执行相对粗粒度的提及范围标注。我们的发现表明，即使在使用中等规模的LLM、相对较小的数据集和参数高效的微调时，纯文本方法也能有效，这突出了语言上下文对于此任务的相对重要性。然而，我们认为该任务代表了一个固有的多模态问题，并讨论了单模态方法固有的局限性。", "summary": "本文研究了在视觉接地对话中，仅使用文本的自回归语言模型来检测指称表达的可行性。研究通过调整一个预训练LLM，利用下一词元预测进行提及范围标注。结果显示，即使在资源有限的情况下，纯文本方法也能有效识别指称表达，这强调了语言上下文的重要性。但作者也指出，指称表达检测本质上是多模态问题，并讨论了单模态方法的局限性。", "keywords": "指称表达检测, 视觉接地对话, 自回归语言模型, 纯文本方法, 多模态", "comments": "本文创新性地探索了在视觉接地对话中，纯文本语言模型在指称表达检测任务上的潜力，并强调了语言上下文的重要性。其重要性在于证明了在某些情况下，即使缺乏视觉信息，语言模型也能在多模态任务中取得一定效果。然而，作者也明确指出了纯模态方法的局限性，即该任务的本质是多模态的，这为未来的研究指明了方向，即需要结合视觉信息来解决该问题。"}}
{"id": "2506.20998", "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting", "authors": ["Yeon-Ji Song", "Jaein Kim", "Byung-Ju Kim", "Byoung-Tak Zhang"], "summary": "Novel view synthesis is a task of generating scenes from unseen perspectives;\nhowever, synthesizing dynamic scenes from blurry monocular videos remains an\nunresolved challenge that has yet to be effectively addressed. Existing novel\nview synthesis methods are often constrained by their reliance on\nhigh-resolution images or strong assumptions about static geometry and rigid\nscene priors. Consequently, their approaches lack robustness in real-world\nenvironments with dynamic object and camera motion, leading to instability and\ndegraded visual fidelity. To address this, we propose Motion-aware Dynamic View\nSynthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting\n(DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular\nvideos. Our model generates dense 3D Gaussians, restoring sharpness from blurry\nvideos and reconstructing detailed 3D geometry of the scene affected by dynamic\nmotion variations. Our model achieves robust performance in novel view\nsynthesis under dynamic blurry scenes and sets a new benchmark in realistic\nnovel view synthesis for blurry monocular video inputs.", "comment": "CVPRW 2025, Neural Fields Beyond Conventional Cameras", "pdf_url": "http://arxiv.org/pdf/2506.20998v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20998v1", "AI": {"title_translation": "DBMovi-GS：基于稀疏控制高斯泼溅的模糊单目视频动态视图合成", "tldr": "DBMovi-GS通过稀疏控制高斯泼溅技术，解决了从模糊单目视频合成动态场景新视图的挑战，提高了真实世界动态场景下的视觉质量。", "motivation": "现有新视图合成方法难以从模糊单目视频中合成动态场景，因为它们依赖高分辨率图像或对静态几何和刚性场景的强假设，导致在动态物体和相机运动的真实环境中表现不佳。", "method": "本文提出了DBMovi-GS，一种通过稀疏控制高斯泼溅技术，从模糊单目视频进行动态视图合成的方法。该模型生成密集的3D高斯，从模糊视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何。", "result": "模型在动态模糊场景下的新视图合成中实现了鲁棒性能，并在模糊单目视频输入下的真实感新视图合成方面树立了新基准。", "conclusion": "DBMovi-GS成功地解决了从模糊单目视频进行动态视图合成的挑战，通过其创新的方法提高了视觉质量和鲁棒性，并为该领域设定了新的性能标准。", "translation": "新视图合成是一项从未见过的视角生成场景的任务；然而，从模糊单目视频合成动态场景仍然是一个尚未有效解决的挑战。现有新视图合成方法通常受限于它们对高分辨率图像的依赖，或对静态几何和刚性场景的强假设。因此，它们的方法在具有动态物体和相机运动的真实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度下降。为了解决这个问题，我们提出了DBMovi-GS（Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting），这是一种旨在从模糊单目视频进行动态视图合成的方法。我们的模型生成密集的3D高斯，从模糊视频中恢复清晰度并重建受动态运动变化影响的场景的详细3D几何。我们的模型在动态模糊场景下的新视图合成中实现了鲁棒性能，并在模糊单目视频输入下的真实感新视图合成方面树立了新基准。", "summary": "本文提出了DBMovi-GS，一种利用稀疏控制高斯泼溅技术，从模糊单目视频合成动态场景新视图的方法。针对现有方法在动态、模糊真实环境中表现不佳的问题，DBMovi-GS能够生成密集的3D高斯，恢复视频清晰度并重建受动态影响的场景3D几何，从而实现鲁棒的动态新视图合成，并为该领域设定了新基准。", "keywords": "动态视图合成, 模糊单目视频, 高斯泼溅, 3D重建, DBMovi-GS", "comments": "DBMovi-GS的创新之处在于其首次有效地解决了从模糊单目视频进行动态视图合成的难题，特别是通过引入稀疏控制高斯泼溅来处理运动模糊和动态场景。这对于在非理想真实世界条件下进行3D重建和视图合成具有重要意义，克服了现有方法对高分辨率和静态场景的限制。"}}
{"id": "2506.20957", "title": "Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding", "authors": ["Jiameng Chen", "Xiantao Cai", "Jia Wu", "Wenbin Hu"], "summary": "Antibody design remains a critical challenge in therapeutic and diagnostic\ndevelopment, particularly for complex antigens with diverse binding interfaces.\nCurrent computational methods face two main limitations: (1) capturing\ngeometric features while preserving symmetries, and (2) generalizing novel\nantigen interfaces. Despite recent advancements, these methods often fail to\naccurately capture molecular interactions and maintain structural integrity. To\naddress these challenges, we propose \\textbf{AbMEGD}, an end-to-end framework\nintegrating \\textbf{M}ulti-scale \\textbf{E}quivariant \\textbf{G}raph\n\\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging\nadvanced geometric deep learning, AbMEGD combines atomic-level geometric\nfeatures with residue-level embeddings, capturing local atomic details and\nglobal sequence-structure interactions. Its E(3)-equivariant diffusion method\nensures geometric precision, computational efficiency, and robust\ngeneralizability for complex antigens. Furthermore, experiments using the\nSAbDab database demonstrate a 10.13\\% increase in amino acid recovery, 3.32\\%\nrise in improvement percentage, and a 0.062~\\AA\\ reduction in root mean square\ndeviation within the critical CDR-H3 region compared to DiffAb, a leading\nantibody design model. These results highlight AbMEGD's ability to balance\nstructural integrity with improved functionality, establishing a new benchmark\nfor sequence-structure co-design and affinity optimization. The code is\navailable at: https://github.com/Patrick221215/AbMEGD.", "comment": "9 pages, 4 figures, accepted at IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20957v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1; J.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20957v1", "AI": {"title_translation": "抗体设计与优化：基于多尺度等变图扩散模型实现精准复杂抗原结合", "tldr": "AbMEGD是一种基于多尺度等变图扩散的抗体设计新模型，显著提高了复杂抗原结合的准确性和功能性，优于现有方法。", "motivation": "当前的计算抗体设计方法在捕获几何特征、保持对称性以及泛化到新型抗原界面方面存在局限性，导致难以准确捕获分子相互作用并维持结构完整性。", "method": "本文提出了AbMEGD，一个端到端的框架，整合了多尺度等变图扩散（Multi-scale Equivariant Graph Diffusion）用于抗体序列和结构协同设计。它结合了原子级几何特征和残基级嵌入，并采用E(3)-等变扩散方法，以确保几何精度、计算效率和鲁棒的泛化能力。", "result": "与领先的抗体设计模型DiffAb相比，AbMEGD在SAbDab数据库上的实验表明，氨基酸恢复率提高了10.13%，改进百分比提高了3.32%，关键CDR-H3区域的均方根偏差降低了0.062 Å。", "conclusion": "AbMEGD成功平衡了结构完整性与功能性提升，为抗体序列-结构协同设计和亲和力优化建立了新的基准。", "translation": "抗体设计仍然是治疗和诊断开发中的一个关键挑战，特别是对于具有多样结合界面的复杂抗原。当前的计算方法面临两个主要限制：(1) 在保留对称性的同时捕获几何特征，以及 (2) 泛化到新的抗原界面。尽管最近取得了进展，但这些方法往往无法准确捕获分子相互作用并保持结构完整性。为了解决这些挑战，我们提出了 AbMEGD，这是一个端到端框架，整合了Multi-scale Equivariant Graph Diffusion（多尺度等变图扩散）用于抗体序列和结构协同设计。AbMEGD 利用先进的几何深度学习，将原子级几何特征与残基级嵌入相结合，捕获局部原子细节和全局序列-结构相互作用。其 E(3)-等变扩散方法确保了几何精度、计算效率以及对复杂抗原的鲁棒泛化能力。此外，使用 SAbDab 数据库进行的实验表明，与领先的抗体设计模型 DiffAb 相比，AbMEGD 在关键的 CDR-H3 区域内氨基酸恢复率提高了 10.13%，改进百分比提高了 3.32%，均方根偏差降低了 0.062 Å。这些结果突显了 AbMEGD 在平衡结构完整性与提高功能性方面的能力，为序列-结构协同设计和亲和力优化建立了新的基准。代码可在以下网址获取：https://github.com/Patrick221215/AbMEGD。", "summary": "本文介绍了AbMEGD，一个利用多尺度等变图扩散的端到端新框架，用于抗体序列和结构协同设计。它通过结合原子级几何特征与残基级嵌入，并采用E(3)-等变扩散方法，解决了现有方法的局限性。AbMEGD表现出优于现有模型（如DiffAb）的性能，在氨基酸恢复率、改进百分比和RMSD方面取得了显著提升，从而为抗体设计和亲和力优化设定了新标准。", "keywords": "抗体设计, 等变图扩散, 多尺度, 序列-结构协同设计, 抗原结合", "comments": "AbMEGD的创新在于其多尺度等变图扩散方法，结合了原子级和残基级信息，并引入了E(3)-等变性，这对于确保几何精度和泛化能力至关重要，特别是在复杂抗原结合方面。相对于领先模型的定量改进突显了其在治疗和诊断开发中的实际重要性。"}}
{"id": "2506.21360", "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.", "comment": "Accepted in CogSci 2025", "pdf_url": "http://arxiv.org/pdf/2506.21360v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21360v1", "AI": {"title_translation": "人工智能文学批评的结构主义方法：利用格雷马斯符号方阵处理大型语言模型", "tldr": "本文提出了GLASS框架，一个基于格雷马斯符号方阵的结构化分析框架，以增强大型语言模型进行深度文学分析的能力，并创建了首个相关数据集和量化指标，实验结果显示其表现出色，并成功应用于经典作品。", "motivation": "大型语言模型（LLMs）在理解和生成文本方面表现出色，但在为思想深刻、叙事复杂的作品提供专业文学批评方面存在困难。", "method": "本文提出了GLASS（Greimas Literary Analysis via Semiotic Square）框架，这是一个基于格雷马斯符号方阵（GSS）的结构化分析框架，旨在增强LLMs进行深度文学分析的能力。研究者提出了首个基于GSS的文学批评数据集，包含48部作品的详细分析，并使用“LLM即评判者”范式提出了基于GSS的文学批评的量化指标。", "result": "GLASS框架的结果与专家批评以及多个作品和LLMs的比较显示出高性能。此外，GLASS被应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究空白。", "conclusion": "这项研究提供了一个基于人工智能的文学研究和教育工具，为文学参与背后的认知机制提供了见解。", "translation": "大型语言模型（LLMs）在理解和生成文本方面表现出色，但在为思想深刻、叙事复杂的作品提供专业文学批评方面存在困难。本文提出了GLASS（Greimas Literary Analysis via Semiotic Square），一个基于格雷马斯符号方阵（GSS）的结构化分析框架，以增强LLMs进行深度文学分析的能力。GLASS有助于快速剖析叙事作品中的叙事结构和深层含义。我们提出了首个用于基于GSS的文学批评的数据集，其中包含48部作品的详细分析。然后，我们使用“LLM即评判者”范式提出了基于GSS的文学批评的量化指标。我们的框架结果，与多个作品和LLMs的专家批评进行比较，显示出高性能。最后，我们将GLASS应用于39部经典作品，产生了原创且高质量的分析，解决了现有研究空白。这项研究提供了一个基于人工智能的文学研究和教育工具，为文学参与背后的认知机制提供了见解。", "summary": "本文提出了一种名为GLASS（Greimas Literary Analysis via Semiotic Square）的结构化分析框架，旨在解决大型语言模型（LLMs）在提供专业文学批评方面的不足。GLASS基于格雷马斯符号方阵（GSS），能够帮助LLMs深入剖析叙事作品的结构和深层含义。研究者构建了首个基于GSS的文学批评数据集（包含48部作品），并引入了使用“LLM即评判者”范式的量化评估指标。实验结果表明，GLASS在与专家批评和不同LLMs的比较中表现出色。该框架成功应用于39部经典作品，生成了高质量的原创分析，填补了现有研究空白。这项工作为文学研究和教育提供了一个AI工具，并有助于理解文学认知机制。", "keywords": "大型语言模型, 文学批评, 格雷马斯符号方阵, 结构主义, 叙事分析", "comments": "本文的创新之处在于将结构主义的格雷马斯符号方阵引入到人工智能文学批评领域，为大型语言模型提供了一个系统化的分析框架。通过构建专门的数据集和量化指标，该研究为LLMs进行深度文学分析提供了新的路径，并展示了其在生成高质量文学分析方面的潜力。这对于文学研究和教育领域具有重要意义，尤其是在自动化和辅助分析复杂文本方面。"}}
{"id": "2506.21358", "title": "ToosiCubix: Monocular 3D Cuboid Labeling via Vehicle Part Annotations", "authors": ["Behrooz Nasihatkon", "Hossein Resani", "Amirreza Mehrzadian"], "summary": "Many existing methods for 3D cuboid annotation of vehicles rely on expensive\nand carefully calibrated camera-LiDAR or stereo setups, limiting their\naccessibility for large-scale data collection. We introduce ToosiCubix, a\nsimple yet powerful approach for annotating ground-truth cuboids using only\nmonocular images and intrinsic camera parameters. Our method requires only\nabout 10 user clicks per vehicle, making it highly practical for adding 3D\nannotations to existing datasets originally collected without specialized\nequipment. By annotating specific features (e.g., wheels, car badge,\nsymmetries) across different vehicle parts, we accurately estimate each\nvehicle's position, orientation, and dimensions up to a scale ambiguity (8\nDoF). The geometric constraints are formulated as an optimization problem,\nwhich we solve using a coordinate descent strategy, alternating between\nPerspective-n-Points (PnP) and least-squares subproblems. To handle common\nambiguities such as scale and unobserved dimensions, we incorporate\nprobabilistic size priors, enabling 9 DoF cuboid placements. We validate our\nannotations against the KITTI and Cityscapes3D datasets, demonstrating that our\nmethod offers a cost-effective and scalable solution for high-quality 3D cuboid\nannotation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21358v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21358v1", "AI": {"title_translation": "ToosiCubix：通过车辆部件标注进行单目3D长方体标注", "tldr": "ToosiCubix 是一种新的方法，仅使用单目图像和少量用户点击即可进行车辆3D长方体标注，提供了一种经济高效且可扩展的替代方案，以应对现有昂贵设置的局限性。", "motivation": "现有的车辆3D长方体标注方法依赖于昂贵且经过仔细校准的相机-LiDAR或立体设置，这限制了它们进行大规模数据采集的可及性。", "method": "本文引入了 ToosiCubix，一种仅使用单目图像和相机内参标注真实长方体的方法。该方法每个车辆仅需约10次用户点击，通过标注车辆不同部件的特定特征（例如车轮、汽车徽章、对称性），精确估计车辆的位置、方向和尺寸（8自由度，存在尺度模糊）。几何约束被表述为一个优化问题，通过坐标下降策略，在PnP（Perspective-n-Points）和最小二乘子问题之间交替求解。为了处理尺度和未观察到的尺寸等常见模糊性，该方法结合了概率尺寸先验，实现了9自由度的长方体放置。", "result": "通过在KITTI和Cityscapes3D数据集上验证，该方法为高质量3D长方体标注提供了一种经济高效且可扩展的解决方案。", "conclusion": "ToosiCubix 提供了一种经济高效且可扩展的解决方案，用于高质量的3D长方体标注，仅需单目图像和少量用户交互即可实现。", "translation": "许多现有的车辆3D长方体标注方法依赖于昂贵且经过仔细校准的相机-LiDAR或立体设置，这限制了它们进行大规模数据采集的可及性。我们引入了 ToosiCubix，一种简单而强大的方法，仅使用单目图像和相机内参即可标注真实长方体。我们的方法每个车辆仅需约10次用户点击，这使得它对于向最初未配备专业设备收集的现有数据集添加3D标注非常实用。通过标注车辆不同部件的特定特征（例如车轮、汽车徽章、对称性），我们准确估计了每个车辆的位置、方向和尺寸，但存在尺度模糊（8自由度）。几何约束被表述为一个优化问题，我们使用坐标下降策略求解，在PnP（Perspective-n-Points）和最小二乘子问题之间交替。为了处理尺度和未观察到的尺寸等常见模糊性，我们结合了概率尺寸先验，实现了9自由度的长方体放置。我们在KITTI和Cityscapes3D数据集上验证了我们的标注，表明我们的方法为高质量3D长方体标注提供了一种经济高效且可扩展的解决方案。", "summary": "ToosiCubix 是一种创新的单目3D长方体标注方法，旨在克服现有基于昂贵多传感器设置的局限性。该方法仅需单目图像和相机内参，通过用户对车辆特定部件的少量点击（约10次/车），结合几何约束优化（PnP和最小二乘交替）和概率尺寸先验，实现车辆位置、方向和尺寸的精确估计（8或9自由度）。实验证明，ToosiCubix 在KITTI和Cityscapes3D数据集上表现出色，提供了一种经济高效且可扩展的高质量3D标注方案。", "keywords": "单目3D标注, 长方体标注, 车辆部件, 成本效益, 可扩展性", "comments": "ToosiCubix 的主要创新在于通过仅使用单目图像和少量用户交互，显著降低了3D长方体标注的成本和复杂性，使其适用于大规模数据收集和现有数据集的增强。其结合PnP和最小二乘优化以及概率尺寸先验来解决单目视觉固有的尺度和深度模糊性是其方法的精妙之处。这对于自动驾驶和计算机视觉领域的数据标注具有重要意义，因为它提供了一个更易于部署和扩展的解决方案。"}}
{"id": "2506.21001", "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology", "authors": ["Qiuyi Qi", "Xin Li", "Ming Kong", "Zikang Xu", "Bingdi Chen", "Qiang Zhu", "S Kevin Zhou"], "summary": "Challenges such as the lack of high-quality annotations, long-tailed data\ndistributions, and inconsistent staining styles pose significant obstacles to\ntraining neural networks to detect abnormal cells in cytopathology robustly.\nThis paper proposes a style-aligned image composition (SAIC) method that\ncomposes high-fidelity and style-preserved pathological images to enhance the\neffectiveness and robustness of detection models. Without additional training,\nSAIC first selects an appropriate candidate from the abnormal cell bank based\non attribute guidance. Then, it employs a high-frequency feature reconstruction\nto achieve a style-aligned and high-fidelity composition of abnormal cells and\npathological backgrounds. Finally, it introduces a large vision-language model\nto filter high-quality synthesis images. Experimental results demonstrate that\nincorporating SAIC-synthesized images effectively enhances the performance and\nrobustness of abnormal cell detection for tail categories and styles, thereby\nimproving overall detection performance. The comprehensive quality evaluation\nfurther confirms the generalizability and practicality of SAIC in clinical\napplication scenarios. Our code will be released at\nhttps://github.com/Joey-Qi/SAIC.", "comment": "MIDL 2025 Oral", "pdf_url": "http://arxiv.org/pdf/2506.21001v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21001v1", "AI": {"title_translation": "风格对齐的图像合成用于细胞病理学中异常细胞的鲁棒检测", "tldr": "本文提出SAIC方法，通过合成高质量且风格一致的病理图像，以解决细胞病理学中异常细胞检测面临的数据挑战，显著提升了检测模型的性能和鲁棒性。", "motivation": "细胞病理学中异常细胞的鲁棒检测面临高质量标注缺乏、数据分布长尾和染色风格不一致等挑战，这些都阻碍了神经网络模型的有效训练。", "method": "本文提出了一种风格对齐的图像合成（SAIC）方法。该方法无需额外训练，首先根据属性指导从异常细胞库中选择合适的候选细胞；然后利用高频特征重建技术实现异常细胞与病理背景的风格对齐和高保真合成；最后引入大型视觉-语言模型来过滤高质量的合成图像。", "result": "实验结果表明，结合SAIC合成的图像能有效提升尾部类别和风格的异常细胞检测性能和鲁棒性，从而提高整体检测性能。全面的质量评估也进一步证实了SAIC在临床应用场景中的泛化性和实用性。", "conclusion": "SAIC方法通过合成高保真、风格对齐的图像，有效解决了细胞病理学中异常细胞检测面临的数据稀缺和风格不一致等挑战，显著提高了检测模型的性能和鲁棒性，并具有良好的临床应用前景。", "translation": "细胞病理学中异常细胞的鲁棒检测面临着高质量标注缺乏、数据分布长尾和染色风格不一致等挑战，这些都对训练神经网络构成了重大障碍。本文提出了一种风格对齐的图像合成（SAIC）方法，该方法合成高保真且风格保留的病理图像，以增强检测模型的有效性和鲁棒性。SAIC无需额外训练，首先根据属性指导从异常细胞库中选择合适的候选细胞。然后，它采用高频特征重建技术，实现异常细胞与病理背景的风格对齐和高保真合成。最后，引入大型视觉-语言模型来过滤高质量的合成图像。实验结果表明，结合SAIC合成的图像能有效提升尾部类别和风格的异常细胞检测性能和鲁棒性，从而提高整体检测性能。全面的质量评估进一步证实了SAIC在临床应用场景中的泛化性和实用性。我们的代码将在https://github.com/Joey-Qi/SAIC发布。", "summary": "本文针对细胞病理学中异常细胞检测面临的标注缺乏、长尾分布和风格不一致等挑战，提出了一种风格对齐的图像合成（SAIC）方法。SAIC通过属性指导选择细胞、高频特征重建实现风格对齐的高保真合成，并利用大型视觉-语言模型筛选高质量图像。实验证明，SAIC合成的图像能有效提升异常细胞检测模型在尾部类别和不同风格上的性能和鲁棒性，并具备在临床应用中的泛化性和实用性。", "keywords": "细胞病理学, 异常细胞检测, 图像合成, 风格对齐, 数据增强", "comments": "这篇论文通过提出SAIC方法，巧妙地解决了细胞病理学图像检测中数据稀缺和风格不一致的关键问题。其创新点在于结合了属性指导、高频特征重建和视觉-语言模型进行图像合成，有效提升了模型的鲁棒性和泛化能力，特别是在医疗图像分析这种对数据质量和多样性要求极高的领域，具有重要的实际应用价值。该方法无需额外训练的特点也增加了其实用性。"}}
{"id": "2506.20990", "title": "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes", "authors": ["Yifan Yang", "Zhen Zhang", "Rupak Vignesh Swaminathan", "Jing Liu", "Nathan Susanj", "Zheng Zhang"], "summary": "Fine-tuning vision language models (VLMs) has achieved remarkable performance\nacross various downstream tasks; yet, it requires access to model gradients\nthrough backpropagation (BP), making them unsuitable for memory-constrained,\ninference-only edge devices. To address this limitation, previous work has\nexplored various BP-free fine-tuning methods. However, these approaches often\nrely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)\noptimization, and often fail to achieve satisfactory performance. In this\npaper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)\napproach, specifically designed to enhance the performance of ZO VLM\nfine-tuning via a sharpness-aware warm-up training. SharpZO features a\ntwo-stage optimization process: a sharpness-aware ES stage that globally\nexplores and smooths the loss landscape to construct a strong initialization,\nfollowed by a fine-grained local search via sparse ZO optimization. The entire\noptimization relies solely on forward passes. Detailed theoretical analysis and\nextensive experiments on CLIP models demonstrate that SharpZO significantly\nimproves accuracy and convergence speed, achieving up to 7% average gain over\nstate-of-the-art forward-only methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20990v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20990v1", "AI": {"title_translation": "SharpZO：通过仅前向传播实现的混合锐度感知视觉语言模型提示微调", "tldr": "SharpZO提出了一种混合锐度感知零阶优化方法，用于视觉语言模型提示微调，仅依靠前向传播，显著提高了精度和收敛速度，优于现有仅前向传播方法。", "motivation": "现有的视觉语言模型（VLM）微调方法需要通过反向传播（BP）获取模型梯度，这不适用于内存受限的推理专用边缘设备。虽然有无需BP的微调方法，但它们通常依赖于高方差的演化策略（ES）或零阶（ZO）优化，并且性能不尽如人意。", "method": "本文提出了一种混合锐度感知零阶优化（SharpZO）方法。它通过锐度感知热身训练来增强ZO VLM微调的性能。SharpZO包含两阶段优化过程：首先是锐度感知ES阶段，用于全局探索和平滑损失景观以构建强大的初始化；其次是通过稀疏ZO优化进行细粒度局部搜索。整个优化过程仅依赖于前向传播。", "result": "SharpZO显著提高了精度和收敛速度，在CLIP模型上实现了比最先进的仅前向传播方法平均高达7%的增益。", "conclusion": "SharpZO通过结合锐度感知演化策略和稀疏零阶优化，为内存受限的边缘设备提供了高效且高性能的视觉语言模型提示微调解决方案，仅依赖于前向传播。", "translation": "视觉语言模型（VLM）的微调在各种下游任务中取得了显著的性能；然而，它需要通过反向传播（BP）访问模型梯度，这使得它们不适用于内存受限、仅推理的边缘设备。为了解决这一限制，以前的工作探索了各种无需BP的微调方法。然而，这些方法通常依赖于高方差的演化策略（ES）或零阶（ZO）优化，并且往往无法达到令人满意的性能。在本文中，我们提出了一种混合锐度感知零阶优化（SharpZO）方法，专门设计用于通过锐度感知热身训练来提高ZO VLM微调的性能。SharpZO具有两阶段优化过程：一个锐度感知ES阶段，用于全局探索和平滑损失景观以构建强大的初始化，然后是通过稀疏ZO优化进行细粒度局部搜索。整个优化过程仅依赖于前向传播。详细的理论分析和对CLIP模型的广泛实验表明，SharpZO显著提高了精度和收敛速度，比最先进的仅前向传播方法平均提高了高达7%。", "summary": "本文提出了一种名为SharpZO的混合锐度感知零阶优化方法，旨在解决视觉语言模型在内存受限设备上微调时对反向传播的依赖问题。SharpZO通过一个锐度感知演化策略进行全局探索和初始化，随后进行稀疏零阶优化进行局部搜索，整个过程仅依赖于前向传播。实验证明，SharpZO显著提升了VLM微调的精度和收敛速度，相较于现有仅前向传播方法有显著性能提升。", "keywords": "视觉语言模型, 零阶优化, 提示微调, 锐度感知, 仅前向传播", "comments": "SharpZO的创新之处在于其结合了锐度感知、演化策略和零阶优化，并实现了仅前向传播的微调，这对于边缘设备的应用具有重要意义。其两阶段优化策略能够有效平衡全局探索和局部优化，从而在无需梯度的前提下达到高性能。论文解决了VLM在资源受限环境下部署的关键挑战。"}}
{"id": "2506.21420", "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting", "authors": ["Taoyu Wu", "Yiyi Miao", "Zhuoxiao Li", "Haocheng Zhao", "Kang Dang", "Jionglong Su", "Limin Yu", "Haoang Li"], "summary": "Efficient three-dimensional reconstruction and real-time visualization are\ncritical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian\nSplatting (3DGS) has demonstrated remarkable performance in efficient 3D\nreconstruction and rendering. Most 3DGS-based Simultaneous Localization and\nMapping (SLAM) methods only rely on the appearance constraints for optimizing\nboth 3DGS and camera poses. However, in endoscopic scenarios, the challenges\ninclude photometric inconsistencies caused by non-Lambertian surfaces and\ndynamic motion from breathing affects the performance of SLAM systems. To\naddress these issues, we additionally introduce optical flow loss as a\ngeometric constraint, which effectively constrains both the 3D structure of the\nscene and the camera motion. Furthermore, we propose a depth regularisation\nstrategy to mitigate the problem of photometric inconsistencies and ensure the\nvalidity of 3DGS depth rendering in endoscopic scenes. In addition, to improve\nscene representation in the SLAM system, we improve the 3DGS refinement\nstrategy by focusing on viewpoints corresponding to Keyframes with suboptimal\nrendering quality frames, achieving better rendering results. Extensive\nexperiments on the C3VD static dataset and the StereoMIS dynamic dataset\ndemonstrate that our method outperforms existing state-of-the-art methods in\nnovel view synthesis and pose estimation, exhibiting high performance in both\nstatic and dynamic surgical scenes. The source code will be publicly available\nupon paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21420v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21420v1", "AI": {"title_translation": "内窥镜流式SLAM：基于流约束高斯溅射的实时内窥镜SLAM", "tldr": "本文提出EndoFlow-SLAM，一种新的内窥镜SLAM方法，通过引入光流损失和深度正则化来解决3DGS-SLAM在内窥镜场景中的光度不一致和动态运动问题，并在新视角合成和姿态估计方面优于现有方法。", "motivation": "在内窥镜等手术场景中，高效的三维重建和实时可视化至关重要。现有的基于3DGS的SLAM方法主要依赖外观约束，但在内窥镜场景中面临非朗伯表面导致的光度不一致和呼吸引起的动态运动等挑战，这些问题影响了SLAM系统的性能。", "method": "该方法引入光流损失作为几何约束，以有效约束场景的三维结构和相机运动。同时，提出深度正则化策略来减轻光度不一致问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，通过关注渲染质量欠佳的关键帧对应的视点，改进了3DGS细化策略，以改善场景表示和渲染结果。", "result": "在C3VD静态数据集和StereoMIS动态数据集上的大量实验表明，该方法在新颖视图合成和姿态估计方面均优于现有最先进方法，在静态和动态手术场景中均表现出高性能。", "conclusion": "EndoFlow-SLAM通过引入光流约束和深度正则化，有效解决了内窥镜场景中3DGS-SLAM面临的光度不一致和动态运动等挑战，并在三维重建和姿态估计方面取得了卓越的性能，从而提升了内窥镜手术的实时三维可视化能力。", "translation": "高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，三维高斯溅射（3DGS）在高效三维重建和渲染方面表现出卓越的性能。大多数基于3DGS的同步定位与建图（SLAM）方法仅依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致以及呼吸导致的动态运动影响SLAM系统的性能。为了解决这些问题，我们额外引入了光流损失作为几何约束，它有效地约束了场景的三维结构和相机运动。此外，我们提出了一种深度正则化策略，以减轻光度不一致问题并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改善SLAM系统中的场景表示，我们通过关注对应于渲染质量欠佳关键帧的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上的大量实验表明，我们的方法在新颖视图合成和姿态估计方面优于现有最先进方法，在静态和动态手术场景中均表现出高性能。源代码将在论文接受后公开。", "summary": "本文提出EndoFlow-SLAM，一种结合3D Gaussian Splatting技术用于内窥镜手术的实时SLAM系统。针对内窥镜场景中非朗伯表面导致的光度不一致和呼吸引起的动态运动等挑战，该方法创新性地引入光流损失作为几何约束，并提出深度正则化策略。此外，通过优化3DGS细化策略，提升了场景表示质量。实验结果表明，EndoFlow-SLAM在新视角合成和姿态估计方面均超越现有最先进方法，在静态和动态手术场景中均展现出卓越性能。", "keywords": "内窥镜SLAM, 3DGS, 光流, 深度正则化, 实时重建", "comments": "该论文的创新点在于将3DGS与光流约束和深度正则化相结合，以解决内窥镜SLAM中特有的光度不一致和动态运动问题。这对于提升手术场景下的实时三维重建和可视化具有重要意义，其结合几何约束来增强基于外观的3DGS是值得关注的方向。"}}
{"id": "2506.21002", "title": "Inverse Scene Text Removal", "authors": ["Takumi Yoshimatsu", "Shumpei Takezaki", "Seiichi Uchida"], "summary": "Scene text removal (STR) aims to erase textual elements from images. It was\noriginally intended for removing privacy-sensitiveor undesired texts from\nnatural scene images, but is now also appliedto typographic images. STR\ntypically detects text regions and theninpaints them. Although STR has advanced\nthrough neural networksand synthetic data, misuse risks have increased. This\npaper investi-gates Inverse STR (ISTR), which analyzes STR-processed images\nandfocuses on binary classification (detecting whether an image has un-dergone\nSTR) and localizing removed text regions. We demonstrate inexperiments that\nthese tasks are achievable with high accuracies, en-abling detection of\npotential misuse and improving STR. We also at-tempt to recover the removed\ntext content by training a text recognizerto understand its difficulty.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.21002v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21002v1", "AI": {"title_translation": "逆向场景文本移除", "tldr": "该论文研究逆向场景文本移除 (ISTR)，旨在检测图像是否经过STR处理并定位被移除的文本区域，以应对STR的滥用风险。", "motivation": "场景文本移除 (STR) 技术虽然通过神经网络和合成数据取得了进展，但其滥用风险也随之增加。因此，本研究的动机是开发一种能够分析STR处理过的图像，并检测图像是否经过STR处理以及定位被移除文本区域的方法，以识别潜在的滥用行为并改进STR技术。", "method": "本文研究逆向场景文本移除 (ISTR)，主要方法包括：1) 对图像进行二元分类，判断其是否经过STR处理；2) 定位图像中被移除的文本区域。此外，还尝试训练一个文本识别器来恢复被移除的文本内容，以评估其难度。", "result": "实验证明，二元分类和定位被移除文本区域的任务可以达到很高的准确率。恢复被移除文本内容的尝试也揭示了其难度。", "conclusion": "逆向场景文本移除（ISTR）技术在检测STR处理过的图像和定位被移除文本区域方面是可行的，并且能够达到高准确率，这有助于检测潜在的滥用并改进STR。恢复移除文本内容虽然困难，但也是一个值得探索的方向。", "translation": "场景文本移除（STR）旨在从图像中擦除文本元素。它最初是为了从自然场景图像中移除隐私敏感或不需要的文本，但现在也应用于排版图像。STR通常会检测文本区域，然后对其进行修复。尽管STR通过神经网络和合成数据取得了进展，但滥用风险也随之增加。本文研究逆向STR（ISTR），它分析经过STR处理的图像，并侧重于二元分类（检测图像是否经过STR处理）和定位被移除的文本区域。我们在实验中证明，这些任务可以以高准确率实现，从而能够检测潜在的滥用并改进STR。我们还尝试通过训练一个文本识别器来恢复被移除的文本内容，以了解其难度。", "summary": "这篇论文介绍了逆向场景文本移除（ISTR）技术，旨在解决场景文本移除（STR）技术可能带来的滥用风险。ISTR主要通过二元分类来判断图像是否经过STR处理，并能够高精度地定位被移除的文本区域。研究还探索了恢复被移除文本内容的可能性和难度。实验结果表明，ISTR在检测和定位方面表现出色，有助于识别潜在的STR滥用并促进STR技术的改进。", "keywords": "逆向场景文本移除, 场景文本移除, 文本检测, 图像修复, 滥用检测", "comments": "这篇论文提出了一种新颖且重要的研究方向——逆向场景文本移除（ISTR），它从“防御”的角度审视了场景文本移除（STR）技术的潜在滥用问题。其创新点在于将检测STR处理和定位被移除文本区域作为核心任务，并验证了其可行性。这对于提高图像内容安全性和规范AI技术使用具有重要意义。同时，尝试恢复文本内容也为未来的研究提供了新的思路。"}}
{"id": "2506.21003", "title": "Distilling Normalizing Flows", "authors": ["Steven Walton", "Valeriy Klyukin", "Maksim Artemev", "Denis Derkach", "Nikita Orlov", "Humphrey Shi"], "summary": "Explicit density learners are becoming an increasingly popular technique for\ngenerative models because of their ability to better model probability\ndistributions. They have advantages over Generative Adversarial Networks due to\ntheir ability to perform density estimation and having exact latent-variable\ninference. This has many advantages, including: being able to simply\ninterpolate, calculate sample likelihood, and analyze the probability\ndistribution. The downside of these models is that they are often more\ndifficult to train and have lower sampling quality.\n  Normalizing flows are explicit density models, that use composable bijective\nfunctions to turn an intractable probability function into a tractable one. In\nthis work, we present novel knowledge distillation techniques to increase\nsampling quality and density estimation of smaller student normalizing flows.\nWe seek to study the capacity of knowledge distillation in Compositional\nNormalizing Flows to understand the benefits and weaknesses provided by these\narchitectures. Normalizing flows have unique properties that allow for a\nnon-traditional forms of knowledge transfer, where we can transfer that\nknowledge within intermediate layers. We find that through this distillation,\nwe can make students significantly smaller while making substantial performance\ngains over a non-distilled student. With smaller models there is a\nproportionally increased throughput as this is dependent upon the number of\nbijectors, and thus parameters, in the network.", "comment": "Published in eLVM @ CVPR\n  (https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Walton_Distilling_Normalizing_Flows_CVPRW_2025_paper)", "pdf_url": "http://arxiv.org/pdf/2506.21003v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21003v1", "AI": {"title_translation": "蒸馏归一化流", "tldr": "本文提出了一种新的知识蒸馏技术，用于提高小型归一化流的采样质量和密度估计，发现蒸馏可以显著减小模型尺寸并提升性能和吞吐量。", "motivation": "显式密度学习器（如归一化流）因其更好地建模概率分布的能力，在生成模型中越来越受欢迎，但它们通常难以训练且采样质量较低。本研究旨在通过知识蒸馏解决这些问题，以提高小型归一化流的性能。", "method": "提出了新颖的知识蒸馏技术，并将其应用于组合归一化流（Compositional Normalizing Flows），探索了归一化流的独特属性，允许在中间层进行知识转移。", "result": "通过知识蒸馏，学生模型可以显著减小尺寸，同时在性能上相比未蒸馏的学生模型有实质性提升。较小的模型由于网络中双射器（bijectors）和参数数量的减少，吞吐量也相应增加。", "conclusion": "知识蒸馏是一种有效的方法，可以使归一化流模型更小、更快，同时保持甚至提高其采样质量和密度估计能力，证明了其在归一化流架构中的潜力。", "translation": "显式密度学习器因其更好地建模概率分布的能力，正成为生成模型中越来越流行的技术。它们相对于生成对抗网络具有优势，因为它们能够执行密度估计并具有精确的潜在变量推断。这带来了许多优点，包括：能够简单地插值、计算样本似然和分析概率分布。这些模型的缺点是它们通常更难训练且采样质量较低。\n归一化流是显式密度模型，它们使用可组合的双射函数将难以处理的概率函数转换为可处理的函数。在这项工作中，我们提出了新颖的知识蒸馏技术，以提高小型学生归一化流的采样质量和密度估计。我们旨在研究知识蒸馏在组合归一化流中的能力，以了解这些架构提供的优点和缺点。归一化流具有独特的特性，允许非传统的知识转移形式，我们可以在中间层转移知识。我们发现，通过这种蒸馏，我们可以使学生模型显著变小，同时相对于未蒸馏的学生模型取得实质性的性能提升。随着模型变小，吞吐量也成比例增加，因为这取决于网络中双射器（bijectors）的数量，从而取决于参数的数量。", "summary": "本文提出了一种针对归一化流的新型知识蒸馏方法，旨在解决其训练困难和采样质量低的缺点。研究发现，通过将知识从大型归一化流转移到小型学生模型，可以显著减小模型尺寸，同时大幅提高其采样质量和密度估计性能，并带来更高的吞吐量。这表明知识蒸馏是优化归一化流模型效率和性能的有效途径。", "keywords": "归一化流, 知识蒸馏, 生成模型, 密度估计, 模型压缩", "comments": "这项工作通过引入知识蒸馏来解决归一化流的实际应用挑战，即模型复杂度和采样效率。其创新之处在于利用了归一化流的独特架构特性，允许在中间层进行知识转移，这为模型压缩和性能提升提供了一条新颖且有效的路径。"}}
{"id": "2506.21443", "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "authors": ["Ali Şenol", "Garima Agrawal", "Huan Liu"], "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)\\-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk\\-sensitive scenarios. To\naddress these challenges, we present a Domain Knowledge (DK)\\-Enhanced LLM\nframework that integrates pretrained LLMs with structured, task\\-specific\ninsights to perform fraud and concept drift detection. The proposed\narchitecture consists of three main components: (1) a DK\\-LLM module to detect\nfake or deceptive conversations; (2) a drift detection unit (OCDD) to determine\nwhether a semantic shift has occurred; and (3) a second DK\\-LLM module to\nclassify the drift as either benign or fraudulent. We first validate the value\nof domain knowledge using a fake review dataset and then apply our full\nframework to SEConvo, a multiturn dialogue dataset that includes various types\nof fraud and spam attacks. Results show that our system detects fake\nconversations with high accuracy and effectively classifies the nature of\ndrift. Guided by structured prompts, the LLaMA\\-based implementation achieves\n98\\% classification accuracy. Comparative studies against zero\\-shot baselines\ndemonstrate that incorporating domain knowledge and drift awareness\nsignificantly improves performance, interpretability, and robustness in\nhigh\\-stakes NLP applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21443v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21443v1", "AI": {"title_translation": "领域知识增强型大型语言模型用于欺诈和概念漂移检测", "tldr": "本文提出一个领域知识增强型LLM框架，用于检测欺诈性对话和概念漂移，通过集成结构化领域知识显著提高了检测准确性和鲁棒性。", "motivation": "在动态平台上检测欺骗性对话日益困难，因为语言模式不断演变和概念漂移（语义或主题变化）的出现，这些变化会模糊恶意意图或模仿正常对话，使得准确分类具有挑战性。虽然大型语言模型（LLMs）在自然语言任务中表现出色，但在风险敏感场景中它们经常面临上下文歧义和幻觉问题。", "method": "本文提出了一个领域知识（DK）增强型LLM框架，将预训练的LLMs与结构化、任务特定的洞察相结合，以执行欺诈和概念漂移检测。该架构包含三个主要组件：1）一个DK-LLM模块用于检测虚假或欺骗性对话；2）一个漂移检测单元（OCDD）用于确定是否发生了语义漂移；3）第二个DK-LLM模块用于将漂移分类为良性或欺诈性。研究首先使用虚假评论数据集验证了领域知识的价值，然后将完整框架应用于SEConvo多轮对话数据集。", "result": "结果表明，该系统能够高精度地检测虚假对话，并有效分类漂移的性质。在结构化提示的指导下，基于LLaMA的实现达到了98%的分类准确率。与零样本基线进行的对比研究表明，整合领域知识和漂移感知显著提高了高风险NLP应用中的性能、可解释性和鲁棒性。", "conclusion": "整合领域知识和漂移感知能够显著提升大型语言模型在检测欺诈性对话和概念漂移方面的性能、可解释性和鲁棒性，尤其是在高风险的自然语言处理应用中。", "translation": "在动态平台上检测欺骗性对话因语言模式的演变和概念漂移（即随着时间推移改变交互上下文或意图的语义或主题转变）而日益困难。这些转变可能掩盖恶意意图或模仿正常对话，使得准确分类具有挑战性。尽管大型语言模型（LLMs）在自然语言任务中表现出强大的性能，但在风险敏感场景中它们经常面临上下文歧义和幻觉问题。为了解决这些挑战，我们提出了一个领域知识（DK）增强型LLM框架，该框架将预训练的LLMs与结构化、任务特定的洞察相结合，以执行欺诈和概念漂移检测。所提出的架构包含三个主要组件：(1) 一个DK-LLM模块用于检测虚假或欺骗性对话；(2) 一个漂移检测单元（OCDD）用于确定是否发生了语义漂移；以及 (3) 第二个DK-LLM模块用于将漂移分类为良性或欺诈性。我们首先使用一个虚假评论数据集验证了领域知识的价值，然后将我们的完整框架应用于SEConvo，一个包含各种类型欺诈和垃圾邮件攻击的多轮对话数据集。结果显示，我们的系统能够高精度地检测虚假对话，并有效分类漂移的性质。在结构化提示的指导下，基于LLaMA的实现达到了98%的分类准确率。与零样本基线进行的对比研究表明，整合领域知识和漂移感知显著提高了高风险NLP应用中的性能、可解释性和鲁棒性。", "summary": "本文提出了一种领域知识（DK）增强型大型语言模型（LLM）框架，旨在解决动态平台中欺诈性对话和概念漂移检测的挑战。该框架通过集成结构化、任务特定的领域知识来增强预训练LLMs，以应对上下文歧义和幻觉问题。其核心包含三个模块：检测欺骗性对话的DK-LLM、识别语义漂移的OCDD，以及分类漂移性质的第二个DK-LLM。实验证明，该系统在检测虚假对话和分类漂移方面表现出高准确性，其中基于LLaMA的实现达到了98%的分类精度。研究结果表明，引入领域知识和漂移感知能够显著提升高风险NLP应用中的性能、可解释性和鲁棒性。", "keywords": "领域知识, 大型语言模型, 欺诈检测, 概念漂移, 对话检测", "comments": "这项研究通过将领域知识与大型语言模型相结合，为风险敏感型自然语言处理任务提供了一个创新性解决方案，尤其是在欺诈检测和概念漂移识别方面。其模块化设计和在实际数据集上的高精度表现，凸显了领域知识在提升LLM性能和鲁棒性方面的关键作用，为未来高风险应用中的LLM部署提供了有价值的参考。"}}
{"id": "2506.21427", "title": "Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning", "authors": ["Prajwal Koirala", "Cody Fleming"], "summary": "Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21427v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21427v1", "AI": {"title_translation": "基于流的单步完成，实现高效和富有表现力的策略学习", "tldr": "提出SSCP，一种基于流匹配的生成策略，通过单步动作生成解决现有生成RL模型效率和稳定性问题，并在多种RL设置中表现出色。", "motivation": "现有的扩散模型和流匹配等生成模型在离线强化学习中虽然能捕捉丰富、多模态的动作分布，但其迭代采样导致推理成本高昂，且由于跨采样步骤的梯度传播导致训练不稳定。", "method": "提出单步完成策略（SSCP），这是一种生成式策略，通过增强的流匹配目标进行训练，以从中间流样本预测直接完成向量，从而实现准确的单步动作生成。SSCP在离策略actor-critic框架中运行，结合了生成模型的表达能力与单模态策略的训练和推理效率，无需冗长的反向传播链。该方法还扩展到目标条件RL，使扁平策略能够利用子目标结构而无需显式分层推理。", "result": "SSCP在速度和适应性方面比基于扩散的基线有显著提升，能有效扩展到离线、离线到在线以及在线RL设置。在标准离线RL和行为克隆基准测试中取得了优异的结果。", "conclusion": "SSCP是一个多功能、富有表现力且高效的深度强化学习和序列决策框架。", "translation": "扩散和流匹配等生成模型通过捕捉丰富、多模态的动作分布，为离线强化学习（RL）提供了富有表现力的策略，但其迭代采样引入了高昂的推理成本，并且由于跨采样步骤的梯度传播导致训练不稳定。我们提出了“单步完成策略”（SSCP），这是一种生成式策略，通过增强的流匹配目标进行训练，以从中间流样本预测直接完成向量，从而实现准确的单步动作生成。在离策略actor-critic框架中，SSCP结合了生成模型的表达能力与单模态策略的训练和推理效率，而无需冗长的反向传播链。我们的方法能有效扩展到离线、离线到在线以及在线RL设置，在速度和适应性方面比基于扩散的基线有显著提升。我们进一步将SSCP扩展到目标条件RL，使扁平策略能够利用子目标结构而无需显式分层推理。SSCP在标准离线RL和行为克隆基准测试中取得了优异的结果，使其成为深度RL和序列决策的多功能、富有表现力且高效的框架。", "summary": "本文提出单步完成策略（SSCP），旨在解决现有生成模型在离线强化学习中面临的推理效率低下和训练不稳定性问题。SSCP通过增强的流匹配目标进行训练，能够实现一次性、准确的动作生成。该策略在离策略actor-critic框架下运行，有效结合了生成模型的表达能力和单模态策略的效率。实验证明，SSCP在速度和适应性上优于扩散模型，并能有效应用于离线、离线到在线以及在线RL场景，甚至扩展到目标条件RL，在多个标准基准测试中表现出色，是一个多功能、高效的强化学习框架。", "keywords": "强化学习, 生成模型, 流匹配, 单步完成策略, 离线RL", "comments": "SSCP的创新点在于通过“单步完成”机制，巧妙地解决了生成模型在强化学习中迭代采样带来的效率和稳定性痛点。它在保持生成模型表达能力的同时，显著提升了推理速度和训练稳定性，这对于实际部署和应用具有重要意义。其在多种RL设置下的有效性和对目标条件RL的扩展，进一步展现了其通用性和潜力。"}}
{"id": "2506.21005", "title": "VisionGuard: Synergistic Framework for Helmet Violation Detection", "authors": ["Lam-Huy Nguyen", "Thinh-Phuc Nguyen", "Thanh-Hai Nguyen", "Gia-Huy Dinh", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Enforcing helmet regulations among motorcyclists is essential for enhancing\nroad safety and ensuring the effectiveness of traffic management systems.\nHowever, automatic detection of helmet violations faces significant challenges\ndue to environmental variability, camera angles, and inconsistencies in the\ndata. These factors hinder reliable detection of motorcycles and riders and\ndisrupt consistent object classification. To address these challenges, we\npropose VisionGuard, a synergistic multi-stage framework designed to overcome\nthe limitations of frame-wise detectors, especially in scenarios with class\nimbalance and inconsistent annotations. VisionGuard integrates two key\ncomponents: Adaptive Labeling and Contextual Expander modules. The Adaptive\nLabeling module is a tracking-based refinement technique that enhances\nclassification consistency by leveraging a tracking algorithm to assign\npersistent labels across frames and correct misclassifications. The Contextual\nExpander module improves recall for underrepresented classes by generating\nvirtual bounding boxes with appropriate confidence scores, effectively\naddressing the impact of data imbalance. Experimental results show that\nVisionGuard improves overall mAP by 3.1% compared to baseline detectors,\ndemonstrating its effectiveness and potential for real-world deployment in\ntraffic surveillance systems, ultimately promoting safety and regulatory\ncompliance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21005v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21005v1", "AI": {"title_translation": "VisionGuard：头盔违规检测的协同框架", "tldr": "VisionGuard是一个多阶段框架，通过自适应标注和上下文扩展模块，提高了头盔违规检测的准确性，解决了数据不一致和类别不平衡问题。", "motivation": "自动检测摩托车手头盔违规面临挑战，包括环境多变性、摄像机角度和数据不一致性，这些因素阻碍了可靠的摩托车和骑手检测以及一致的对象分类。现有的逐帧检测器在类别不平衡和标注不一致的情况下表现受限。", "method": "本文提出了VisionGuard，一个协同多阶段框架，包含两个关键组件：自适应标注模块和上下文扩展模块。自适应标注模块是一种基于跟踪的细化技术，通过利用跟踪算法在帧间分配持久标签并纠正错误分类来增强分类一致性。上下文扩展模块通过生成具有适当置信度分数的虚拟边界框来提高代表性不足类别的召回率，有效解决了数据不平衡的影响。", "result": "实验结果显示，VisionGuard相较于基线检测器，整体mAP提高了3.1%。", "conclusion": "VisionGuard展示了其有效性以及在交通监控系统中实际部署的潜力，最终有助于提高道路安全和法规遵从性。", "translation": "强制执行摩托车手头盔规定对于提高道路安全和确保交通管理系统的有效性至关重要。然而，由于环境多变性、摄像机角度和数据不一致性，头盔违规的自动检测面临着重大挑战。这些因素阻碍了摩托车和骑手的可靠检测，并扰乱了对象的一致分类。为了应对这些挑战，我们提出了VisionGuard，一个协同多阶段框架，旨在克服逐帧检测器的局限性，特别是在类别不平衡和标注不一致的场景中。VisionGuard集成了两个关键组件：自适应标注模块和上下文扩展模块。自适应标注模块是一种基于跟踪的细化技术，通过利用跟踪算法在帧间分配持久标签并纠正错误分类来增强分类一致性。上下文扩展模块通过生成具有适当置信度分数的虚拟边界框来提高代表性不足类别的召回率，有效解决了数据不平衡的影响。实验结果表明，与基线检测器相比，VisionGuard的整体mAP提高了3.1%，展示了其有效性以及在交通监控系统中实际部署的潜力，最终促进了安全和法规遵从性。", "summary": "本文提出了VisionGuard，一个用于头盔违规检测的协同多阶段框架，旨在解决现有逐帧检测器在环境多变性、数据不一致和类别不平衡下的局限性。VisionGuard整合了自适应标注模块（通过跟踪提高分类一致性）和上下文扩展模块（通过生成虚拟边界框解决数据不平衡）。实验证明，VisionGuard相较于基线检测器，整体mAP提升了3.1%，显示出其在交通监控中提高安全和合规性的潜力。", "keywords": "头盔违规检测, 协同框架, 交通安全, 目标检测, 数据不平衡", "comments": "VisionGuard通过结合跟踪和数据增强（虚拟边界框）来解决自动头盔检测中的关键挑战，即分类一致性和数据不平衡，这是一种创新且实用的方法。其模块化设计和在mAP上的提升表明了其在实际交通监控系统中的应用前景。"}}
{"id": "2506.21028", "title": "TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence", "authors": ["Feng Jiang", "Mangal Prakash", "Hehuan Ma", "Jianyuan Deng", "Yuzhi Guo", "Amina Mollaysa", "Tommaso Mansi", "Rui Liao", "Junzhou Huang"], "summary": "Molecular property prediction aims to learn representations that map chemical\nstructures to functional properties. While multimodal learning has emerged as a\npowerful paradigm to learn molecular representations, prior works have largely\noverlooked textual and taxonomic information of molecules for representation\nlearning. We introduce TRIDENT, a novel framework that integrates molecular\nSMILES, textual descriptions, and taxonomic functional annotations to learn\nrich molecular representations. To achieve this, we curate a comprehensive\ndataset of molecule-text pairs with structured, multi-level functional\nannotations. Instead of relying on conventional contrastive loss, TRIDENT\nemploys a volume-based alignment objective to jointly align tri-modal features\nat the global level, enabling soft, geometry-aware alignment across modalities.\nAdditionally, TRIDENT introduces a novel local alignment objective that\ncaptures detailed relationships between molecular substructures and their\ncorresponding sub-textual descriptions. A momentum-based mechanism dynamically\nbalances global and local alignment, enabling the model to learn both broad\nfunctional semantics and fine-grained structure-function mappings. TRIDENT\nachieves state-of-the-art performance on 11 downstream tasks, demonstrating the\nvalue of combining SMILES, textual, and taxonomic functional annotations for\nmolecular property prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21028v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21028v1", "AI": {"title_translation": "TRIDENT：结合分类学注释和局部对应关系的三模态分子表示学习", "tldr": "TRIDENT是一个新颖的框架，通过整合SMILES、文本描述和分类学功能注释，学习丰富的三模态分子表示，并在分子性质预测任务上实现了最先进的性能。", "motivation": "现有的分子表示学习方法在多模态学习中忽视了分子的文本和分类学信息，限制了学习到的分子表示的丰富性。", "method": "论文提出了TRIDENT框架，整合分子SMILES、文本描述和分类学功能注释来学习分子表示。为此，作者收集了一个包含分子-文本对和结构化、多级功能注释的综合数据集。TRIDENT采用基于体积的全局对齐目标，实现跨模态的软、几何感知对齐，并引入了新颖的局部对齐目标来捕捉分子子结构及其对应子文本描述之间的关系。通过基于动量的机制动态平衡全局和局部对齐。", "result": "TRIDENT在11个下游任务上取得了最先进的性能。", "conclusion": "结合SMILES、文本和分类学功能注释对于分子性质预测具有重要价值。", "translation": "分子性质预测旨在学习将化学结构映射到功能性质的表示。尽管多模态学习已成为学习分子表示的强大范式，但以往的工作在很大程度上忽视了分子的文本和分类学信息用于表示学习。我们引入了TRIDENT，一个新颖的框架，它整合了分子SMILES、文本描述和分类学功能注释来学习丰富的分子表示。为了实现这一点，我们整理了一个包含结构化、多级功能注释的分子-文本对的综合数据集。TRIDENT没有依赖传统的对比损失，而是采用基于体积的对齐目标，在全局层面联合对齐三模态特征，从而实现跨模态的软、几何感知对齐。此外，TRIDENT引入了一种新颖的局部对齐目标，捕捉分子子结构及其相应子文本描述之间的详细关系。基于动量的机制动态平衡全局和局部对齐，使模型能够学习广泛的功能语义和细粒度的结构-功能映射。TRIDENT在11个下游任务上取得了最先进的性能，证明了结合SMILES、文本和分类学功能注释对于分子性质预测的价值。", "summary": "本文提出TRIDENT，一个用于分子表示学习的三模态框架，它创新性地整合了分子SMILES、文本描述和分类学功能注释。TRIDENT通过构建综合数据集，并采用独特的基于体积的全局对齐和局部对齐目标来捕捉不同粒度的跨模态关系。实验结果表明，TRIDENT在多项分子性质预测任务上达到了最先进的性能，突显了结合多源信息在分子表示学习中的重要性。", "keywords": "分子表示学习, 多模态学习, 分子性质预测, 分类学注释, 局部对应", "comments": "TRIDENT的创新之处在于其三模态整合（SMILES、文本、分类学注释），以及提出的基于体积的全局对齐和局部对齐目标，超越了传统的对比学习，实现了更精细和全面的分子表示学习。其在多个下游任务上取得的SOTA性能证明了这种多源信息融合策略的有效性，为未来的分子表示学习提供了新思路。"}}
{"id": "2506.21547", "title": "SAM4D: Segment Anything in Camera and LiDAR Streams", "authors": ["Jianyun Xu", "Song Wang", "Ziqian Ni", "Chunyong Hu", "Sheng Yang", "Jianke Zhu", "Qiang Li"], "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.", "comment": "Accepted by ICCV2025, Project Page: https://SAM4D-Project.github.io", "pdf_url": "http://arxiv.org/pdf/2506.21547v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21547v1", "AI": {"title_translation": "SAM4D：在相机和激光雷达流中分割一切", "tldr": "SAM4D是一个多模态时序基础模型，通过引入UMPE和MCMA实现相机和激光雷达流的提示式分割，并开发了一个自动化数据引擎，能快速生成高质量伪标签，在Waymo-4DSeg上表现出强大的跨模态分割和数据标注潜力。", "motivation": "解决相机和激光雷达流中提示式分割的需求，并克服传统标注方法的数据瓶颈。", "method": "1. 引入统一多模态位置编码（UMPE）以在共享3D空间中对齐相机和激光雷达特征。2. 提出运动感知跨模态记忆注意力（MCMA）以增强时间一致性和长时域特征检索。3. 开发多模态自动化数据引擎，结合VFM驱动的视频掩膜、时空4D重建和跨模态掩膜融合，生成相机-激光雷达对齐的伪标签。", "result": "在构建的Waymo-4DSeg数据集上进行了广泛实验，结果表明SAM4D具有强大的跨模态分割能力和在数据标注方面的巨大潜力。", "conclusion": "SAM4D是一个有效且高效的多模态时序基础模型，能够实现跨相机和激光雷达流的提示式分割，并通过自动化数据引擎显著加速数据标注，为自动驾驶场景提供了强大的解决方案。", "translation": "我们提出了SAM4D，一个多模态和时序基础模型，旨在实现相机和激光雷达流的提示式分割。引入了统一多模态位置编码（UMPE）以在共享3D空间中对齐相机和激光雷达特征，从而实现无缝的跨模态提示和交互。此外，我们提出了运动感知跨模态记忆注意力（MCMA），它利用自我运动补偿来增强时间一致性和长时域特征检索，确保在动态变化的自动驾驶场景中实现鲁棒分割。为了避免标注瓶颈，我们开发了一个多模态自动化数据引擎，该引擎协同VFM驱动的视频掩膜、时空4D重建和跨模态掩膜融合。该框架生成的相机-激光雷达对齐伪标签的速度比人工标注快几个数量级，同时在点云表示中保留了VFM导出的语义保真度。我们在构建的Waymo-4DSeg上进行了广泛实验，这些实验证明了所提出的SAM4D强大的跨模态分割能力和在数据标注方面的巨大潜力。", "summary": "SAM4D是一个为相机和激光雷达流设计的提示式多模态时序分割模型。它通过统一多模态位置编码（UMPE）对齐跨模态特征，并利用运动感知跨模态记忆注意力（MCMA）提升时间一致性。为解决数据标注问题，该模型还开发了一个多模态自动化数据引擎，能高效生成高质量伪标签。实验证明SAM4D在跨模态分割和数据标注方面表现出色。", "keywords": "多模态分割, 激光雷达, 相机, 自动化标注, 自动驾驶", "comments": "SAM4D的创新性在于其整合了多模态（相机和激光雷达）和时序信息，并通过UMPE和MCMA实现高效的跨模态交互和时间一致性。其提出的自动化数据引擎是解决自动驾驶领域大规模数据标注瓶颈的关键突破，显著提高了标注效率和质量。这使得SAM4D在实际应用中具有重要潜力，尤其是在需要大量高质量标注数据的自动驾驶场景。"}}
{"id": "2506.21006", "title": "Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning", "authors": ["Tyler Ward", "Xiaoqin Wang", "Braxton McFarland", "Md Atik Ahamed", "Sahar Nozad", "Talal Arshad", "Hafsa Nebbache", "Jin Chen", "Abdullah Imran"], "summary": "Complete removal of cancer tumors with a negative specimen margin during\nlumpectomy is essential in reducing breast cancer recurrence. However, 2D\nspecimen radiography (SR), the current method used to assess intraoperative\nspecimen margin status, has limited accuracy, resulting in nearly a quarter of\npatients requiring additional surgery. To address this, we propose a novel deep\nlearning framework combining the Segment Anything Model (SAM) with\nForward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging\nboth local and global contrastive learning for patch-level classification of SR\nimages. After annotating SR images with regions of known maligancy,\nnon-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18\nbackbone with FFCL to classify margin status, then reconstruct coarse binary\nmasks to prompt SAM for refined tumor margin segmentation. Our approach\nachieved an AUC of 0.8455 for margin classification and segmented margins with\na 27.4% improvement in Dice similarity over baseline models, while reducing\ninference time to 47 milliseconds per image. These results demonstrate that\nFFCL-SAM significantly enhances both the speed and accuracy of intraoperative\nmargin assessment, with strong potential to reduce re-excision rates and\nimprove surgical outcomes in breast cancer treatment. Our code is available at\nhttps://github.com/tbwa233/FFCL-SAM/.", "comment": "19 pages, 7 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21006v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21006v1", "AI": {"title_translation": "乳腺癌保乳术切缘的SAM结合前向-前向对比学习检测", "tldr": "本文提出了一种结合SAM和前向-前向对比学习（FFCL）的深度学习框架，用于提高乳腺癌保乳术中切缘评估的速度和准确性，以减少二次手术率。", "motivation": "乳腺癌保乳术中肿瘤切除不完全（切缘阳性）是导致复发的重要原因。当前使用的2D标本射线照相（SR）准确性有限，导致约四分之一的患者需要二次手术。", "method": "提出了一种结合“万物分割模型”（SAM）与“前向-前向对比学习”（FFCL）的深度学习框架。首先，对SR图像进行已知恶性区域、非恶性组织和病理确认切缘的标注。然后，使用FFCL预训练ResNet-18骨干网络进行切缘状态分类。最后，重建粗略的二值掩码以提示SAM进行精细的肿瘤切缘分割。", "result": "该方法在切缘分类上取得了0.8455的AUC。在分割切缘方面，与基线模型相比，Dice相似系数提高了27.4%。此外，每张图像的推理时间减少到47毫秒。", "conclusion": "FFCL-SAM显著提高了术中切缘评估的速度和准确性，有望降低二次切除率并改善乳腺癌治疗的手术结果。", "translation": "乳腺癌保乳术中完整切除肿瘤并获得阴性切缘对于降低乳腺癌复发至关重要。然而，2D标本射线照相（SR）是目前用于评估术中标本切缘状态的方法，其准确性有限，导致近四分之一的患者需要额外手术。为了解决这个问题，我们提出了一种新颖的深度学习框架，将“万物分割模型”（SAM）与“前向-前向对比学习”（FFCL）相结合。FFCL是一种预训练策略，它利用局部和全局对比学习对SR图像进行补丁级分类。在对SR图像进行已知恶性区域、非恶性组织和病理确认切缘的标注后，我们使用FFCL预训练ResNet-18骨干网络以分类切缘状态，然后重建粗略的二值掩码以提示SAM进行精细的肿瘤切缘分割。我们的方法在切缘分类上取得了0.8455的AUC，并在分割切缘方面比基线模型提高了27.4%的Dice相似系数，同时将每张图像的推理时间减少到47毫秒。这些结果表明，FFCL-SAM显著提高了术中切缘评估的速度和准确性，具有降低二次切除率和改善乳腺癌治疗手术结果的巨大潜力。我们的代码可在https://github.com/tbwa233/FFCL-SAM/获取。", "summary": "本文提出了一种名为FFCL-SAM的深度学习框架，旨在提高乳腺癌保乳术中切缘评估的准确性和速度。该框架结合了前向-前向对比学习（FFCL）进行补丁级分类预训练，并利用Segment Anything Model（SAM）进行精细的肿瘤切缘分割。实验结果表明，FFCL-SAM在切缘分类和分割性能上均显著优于现有方法，并大幅缩短了推理时间，有望减少患者二次手术的需求。", "keywords": "乳腺癌, 保乳术, 切缘检测, 对比学习, SAM, 深度学习", "comments": "这项研究通过结合创新的前向-前向对比学习预训练策略和强大的SAM模型，有效解决了乳腺癌保乳术中切缘评估准确性低的临床痛点。其核心创新在于利用FFCL进行高效的特征学习和分类，并通过SAM实现高精度的分割，显著提升了术中决策的效率和可靠性。该方法对提高患者预后和降低医疗成本具有重要意义。"}}
{"id": "2506.21035", "title": "Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning", "authors": ["Haodong Lu", "Chongyang Zhao", "Jason Xue", "Lina Yao", "Kristen Moore", "Dong Gong"], "summary": "Continual learning (CL) with large pre-trained models is challenged by\ncatastrophic forgetting and task interference. Existing LoRA-based\nMixture-of-Experts (MoE) approaches mitigate forgetting by assigning and\nfreezing task-specific adapters, but suffer from interference, redundancy, and\nambiguous routing due to coarse adapter-level selection. However, this design\nintroduces three key challenges: 1) Interference: Activating full LoRA experts\nper input leads to subspace interference and prevents selective reuse of useful\ncomponents across tasks. 2) Redundancy: Newly added experts often duplicate or\ncontradict existing knowledge due to unnecessary activation of unrelated ranks\nand insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features\nacross tasks confuse the router, resulting in unstable expert assignments. As\nmore experts accumulate, earlier task routing degrades, accelerating\nforgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with\nself-activated and sparse rank activation for CL. Unlike mixing multiple\nlow-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,\neach treated as an independent expert, enabling fine-grained mixture of rank-1\nexpert utilization while mitigating interference and redundancy. To avoid\nambiguous routing, we propose that each rank-1 expert can infer its own\nrelevance via intermediate activations. Coupled with our proposed rank pruning\nand activation budgets, MoRA adaptively selects a sparse mixture of ranks per\ninput. We validate MoRA on continual learning tasks with CLIP and large\nlanguage models (LLMs), analyzing both in-domain learning and out-of-domain\nforgetting/generalization during fine-tuning. MoRA shows significant\neffectiveness on enhancing CL with PTMs, and improving generalization while\nmitigating forgetting.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.21035v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21035v1", "AI": {"title_translation": "循序渐进：通过自激活稀疏秩自适应学习的持续学习", "tldr": "大型预训练模型的持续学习面临灾难性遗忘和任务干扰。现有方法存在干扰、冗余和路由模糊问题。本文提出MoRA，一种自激活稀疏秩自适应学习方法，将每个秩-r更新分解为r个秩-1组件作为独立专家，实现细粒度选择，并通过自推断相关性和剪枝预算来缓解干扰、冗余和路由模糊，有效提升持续学习性能。", "motivation": "大型预训练模型的持续学习（CL）面临灾难性遗忘和任务干扰的挑战。现有的基于LoRA的专家混合（MoE）方法通过分配和冻结任务特定适配器来缓解遗忘，但由于粗粒度的适配器级别选择，它们仍面临干扰、冗余和模糊路由的问题。具体而言，对每个输入激活完整的LoRA专家会导致子空间干扰，阻碍跨任务有用组件的选择性重用，并导致新添加的专家由于不必要的无关秩激活和相关秩的不足重用而重复或矛盾现有知识。此外，任务间重叠的特征会混淆路由器，导致专家分配不稳定，随着专家数量的积累，早期任务路由性能下降，加速遗忘。", "method": "本文提出MoRA，一种用于持续学习的自激活稀疏秩自适应学习方法。与混合多个低秩矩阵不同，MoRA将每个秩-r更新分解为r个秩-1组件，每个组件被视为一个独立的专家，从而实现秩-1专家的细粒度混合利用，同时缓解干扰和冗余。为了避免模糊路由，我们提出每个秩-1专家可以通过中间激活来推断其自身的相关性。结合我们提出的秩剪枝和激活预算，MoRA能够为每个输入自适应地选择稀疏的秩混合。", "result": "MoRA在CLIP和大型语言模型（LLM）的持续学习任务中得到了验证，分析了微调过程中的域内学习和域外遗忘/泛化。MoRA在增强预训练模型的持续学习能力、提高泛化能力和减轻遗忘方面显示出显著的有效性。", "conclusion": "MoRA通过引入一种细粒度、自激活和稀疏的秩自适应学习方法，有效地解决了大型预训练模型在持续学习中面临的挑战，从而改善了泛化能力并减少了遗忘。", "translation": "大型预训练模型（PTMs）的持续学习（CL）面临灾难性遗忘和任务干扰的挑战。现有的基于LoRA的专家混合（MoE）方法通过分配和冻结任务特定适配器来缓解遗忘，但由于粗粒度的适配器级别选择，它们仍面临干扰、冗余和模糊路由的问题。然而，这种设计引入了三个关键挑战：1）干扰：每个输入激活完整的LoRA专家会导致子空间干扰，并阻止跨任务有用组件的选择性重用。2）冗余：由于不必要的无关秩激活和相关秩的不足重用，新添加的专家通常会复制或矛盾现有知识。3）模糊性：任务间重叠的特征会混淆路由器，导致专家分配不稳定。随着更多专家的积累，早期任务路由性能下降，加速遗忘。我们提出了MoRA，一种具有自激活和稀疏秩激活的秩自适应学习方法，用于持续学习。与混合多个低秩矩阵不同，MoRA将每个秩-r更新分解为r个秩-1组件，每个组件被视为一个独立的专家，从而实现秩-1专家利用的细粒度混合，同时缓解干扰和冗余。为了避免模糊路由，我们提出每个秩-1专家可以通过中间激活来推断其自身的相关性。结合我们提出的秩剪枝和激活预算，MoRA能够为每个输入自适应地选择稀疏的秩混合。我们在使用CLIP和大型语言模型（LLMs）的持续学习任务上验证了MoRA，分析了微调过程中的域内学习和域外遗忘/泛化。MoRA在增强预训练模型的持续学习能力、提高泛化能力和减轻遗忘方面显示出显著的有效性。", "summary": "本文提出MoRA，一种用于大型预训练模型持续学习的新方法，旨在解决现有LoRA-based MoE方法中的灾难性遗忘、任务干扰、冗余和路由模糊问题。MoRA将秩-r更新分解为独立的秩-1专家，实现细粒度的专家利用，并通过自激活和稀疏秩选择来缓解干扰和冗余。通过让每个秩-1专家推断自身相关性并结合秩剪枝和激活预算，MoRA有效地提高了持续学习的泛化能力并减轻了遗忘，在CLIP和大型语言模型上得到了验证。", "keywords": "持续学习, 专家混合, 低秩适应, 灾难性遗忘, 大型语言模型", "comments": "本文的创新点在于提出了MoRA，一种细粒度的秩自适应学习方法，通过将高秩更新分解为多个独立的秩-1专家，并允许这些专家自适应地选择性激活。这解决了现有LoRA-based MoE方法在持续学习中面临的干扰、冗余和路由模糊等核心挑战。其重要性在于，通过更精细的知识表示和选择机制，有效提升了大型预训练模型在面对新任务时的泛化能力，并显著减轻了灾难性遗忘，为持续学习领域提供了新的视角和解决方案。"}}
{"id": "2506.21552", "title": "Whole-Body Conditioned Egocentric Video Prediction", "authors": ["Yutong Bai", "Danny Tran", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik"], "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.", "comment": "Project Page: https://dannytran123.github.io/PEVA", "pdf_url": "http://arxiv.org/pdf/2506.21552v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21552v1", "AI": {"title_translation": "全身条件化自我中心视频预测", "tldr": "该研究训练模型，通过以相对3D身体姿态为条件，从人类动作预测自我中心视频，旨在模拟人类动作如何从第一视角塑造环境。", "motivation": "该论文旨在从人类视角出发，通过视频预测来解决建模复杂真实世界环境和具身智能体行为的挑战。", "method": "作者训练了名为PEVA的模型，该模型在给定过去视频和由相对3D身体姿态表示的动作的情况下，预测自我中心视频。模型通过身体关节层级结构化的运动姿态轨迹进行条件化。他们在一个名为Nymeria的大规模真实世界自我中心视频和身体姿态捕获数据集上训练了一个自回归条件扩散变换器。此外，他们还设计了一个分层评估协议。", "result": "该模型学会了模拟物理人类动作如何从第一人称视角塑造环境。分层评估协议能够对模型的具身预测和控制能力进行全面分析。", "conclusion": "这项工作代表了从人类视角通过视频预测来解决建模复杂真实世界环境和具身智能体行为挑战的初步尝试。", "translation": "我们训练模型，在给定过去的视频和以相对3D身体姿态表示的动作的情况下，预测自我中心视频（PEVA）。通过以身体关节层级结构化的运动姿态轨迹为条件，我们的模型学会了从第一人称视角模拟物理人类动作如何塑造环境。我们在Nymeria（一个大规模真实世界自我中心视频和身体姿态捕获数据集）上训练了一个自回归条件扩散变换器。我们进一步设计了一个分层评估协议，包含越来越具挑战性的任务，从而能够对模型的具身预测和控制能力进行全面分析。我们的工作代表了从人类视角通过视频预测来解决建模复杂真实世界环境和具身智能体行为挑战的初步尝试。", "summary": "本研究训练了一个名为PEVA的模型，用于根据过去的视频和人类的相对3D身体姿态来预测自我中心视频。模型利用身体姿态轨迹作为条件，旨在模拟人类动作如何从第一人称视角影响环境。研究团队在一个大型真实世界数据集Nymeria上训练了一个自回归条件扩散变换器，并设计了分层评估协议来全面分析模型的具身预测和控制能力。这项工作是解决复杂真实世界环境和具身智能体行为建模挑战的初步探索。", "keywords": "自我中心视频预测, 人类动作, 3D身体姿态, 扩散变换器, 具身智能体", "comments": "这项研究的创新之处在于通过结合全身姿态信息和扩散变换器来预测自我中心视频，这为理解和模拟人类与环境的交互提供了新视角。其重要性在于为具身智能体的行为预测和控制提供了基础，特别是在复杂真实世界场景下的应用潜力。通过大规模数据集和分层评估协议，确保了研究的严谨性和模型的全面分析能力。"}}
{"id": "2506.21008", "title": "The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion", "authors": ["Bang Gong", "Luchao Qi", "Jiaye Wu", "Zhicheng Fu", "Chunbo Song", "David W. Jacobs", "John Nicholson", "Roni Sengupta"], "summary": "We introduce the Aging Multiverse, a framework for generating multiple\nplausible facial aging trajectories from a single image, each conditioned on\nexternal factors such as environment, health, and lifestyle. Unlike prior\nmethods that model aging as a single deterministic path, our approach creates\nan aging tree that visualizes diverse futures. To enable this, we propose a\ntraining-free diffusion-based method that balances identity preservation, age\naccuracy, and condition control. Our key contributions include attention mixing\nto modulate editing strength and a Simulated Aging Regularization strategy to\nstabilize edits. Extensive experiments and user studies demonstrate\nstate-of-the-art performance across identity preservation, aging realism, and\nconditional alignment, outperforming existing editing and age-progression\nmodels, which often fail to account for one or more of the editing criteria. By\ntransforming aging into a multi-dimensional, controllable, and interpretable\nprocess, our approach opens up new creative and practical avenues in digital\nstorytelling, health education, and personalized visualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21008v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21008v1", "AI": {"title_translation": "衰老多重宇宙：通过免训练扩散生成条件感知面部衰老树", "tldr": "该研究引入“衰老多重宇宙”框架，通过免训练扩散方法从单张图像生成多条受环境、健康和生活方式等外部因素影响的 plausible 面部衰老路径，形成一个可视化多样化未来的衰老树，并在身份保持、年龄准确性和条件控制方面达到最先进水平。", "motivation": "现有的面部衰老模型通常将衰老视为单一的确定性路径，未能考虑外部因素如环境、健康和生活方式对衰老过程的多元影响，且在身份保持、年龄准确性和条件控制方面存在不足。", "method": "本文提出一种免训练的扩散方法，用于生成条件感知的面部衰老树。关键贡献包括：1. 注意力混合（attention mixing）以调节编辑强度。2. 模拟衰老正则化（Simulated Aging Regularization）策略以稳定编辑。该方法旨在平衡身份保持、年龄准确性和条件控制。", "result": "通过广泛的实验和用户研究，该方法在身份保持、衰老真实感和条件对齐方面展现出最先进的性能，优于现有未能充分考虑编辑标准的编辑和年龄进展模型。", "conclusion": "该方法将衰老转化为一个多维度、可控且可解释的过程，为数字故事讲述、健康教育和个性化可视化开辟了新的创意和实用途径。", "translation": "我们引入了“衰老多重宇宙”框架，该框架能够从单张图像生成多条合理的面部衰老轨迹，每条轨迹都受环境、健康和生活方式等外部因素的影响。与将衰老建模为单一确定性路径的现有方法不同，我们的方法创建了一个可视化多样化未来的衰老树。为实现这一点，我们提出了一种免训练的基于扩散的方法，该方法平衡了身份保持、年龄准确性和条件控制。我们的主要贡献包括用于调节编辑强度的注意力混合以及用于稳定编辑的模拟衰老正则化策略。广泛的实验和用户研究表明，在身份保持、衰老真实感和条件对齐方面，我们的方法达到了最先进的性能，优于现有常常未能考虑一个或多个编辑标准的编辑和年龄进展模型。通过将衰老转化为一个多维度、可控和可解释的过程，我们的方法在数字故事讲述、健康教育和个性化可视化方面开辟了新的创意和实用途径。", "summary": "本文提出了“衰老多重宇宙”框架，利用免训练扩散方法，从单一图像生成受外部因素（如环境、健康、生活方式）影响的多个 plausible 面部衰老轨迹，形成一个“衰老树”，以可视化多样化的未来。该方法通过注意力混合和模拟衰老正则化策略，有效平衡了身份保持、年龄准确性和条件控制，并在实验中展现出超越现有模型的顶尖性能，为面部衰老建模提供了多维度、可控和可解释的新范式。", "keywords": "面部衰老, 扩散模型, 条件生成, 免训练, 衰老树", "comments": "这项研究的创新之处在于它突破了传统面部衰老模型单一路径的限制，首次提出了“衰老多重宇宙”的概念，并利用免训练扩散模型生成条件感知的多样化衰老轨迹。其引入的注意力混合和模拟衰老正则化策略是关键的技术贡献，有效解决了多因素影响下的面部老化生成难题。这对于数字内容创作、健康预测和个性化服务具有重要意义。"}}
{"id": "2506.21468", "title": "TopK Language Models", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21468v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21468v1", "AI": {"title_translation": "TopK 语言模型", "tldr": "本文提出了一种名为 TopK 语言模型的新型 Transformer 架构，通过在选定层引入 TopK 激活函数，解决了稀疏自编码器（SAE）在语言模型可解释性方面存在的训练后置、特征不稳定和概念发现不明确等问题，实现了与 SAEs 相当的可解释性，同时保持模型能力，并促进了对语言模型学习过程的深入分析。", "motivation": "稀疏自编码器（SAE）是分析和解释 Transformer 语言模型（LM）激活空间的重要工具，但存在多项缺点，包括：训练后置导致无法确定概念发现失败的原因是 SAE 还是底层 LM；训练条件和架构选择影响 SAE 特征学习；以及特征稳定性差导致难以比较不同检查点间的 SAE 特征。", "method": "作者对 Transformer 架构进行了修改，在选定层引入了 TopK 激活函数，使模型的隐藏状态等同于 TopK SAE 的潜在特征。这种方法消除了对后置训练的需求，同时提供了与 SAE 相当的可解释性。", "result": "TopK 语言模型在模型大小、计算效率和可解释性之间取得了有利的权衡。尽管架构变化简单，TopK 语言模型保持了其原始能力，并提供了强大的可解释性优势。实验表明，TopK 语言模型学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并有助于详细分析跨检查点和层的神经元形成过程。", "conclusion": "TopK 语言模型提供了一种稳定可靠的工具，用于理解语言模型如何学习和表示概念，有望显著推动未来在模型可解释性和可控性方面的研究。", "translation": "稀疏自编码器（SAE）已成为分析和解释基于 Transformer 的语言模型（LM）激活空间的重要工具。然而，SAE 存在一些缺点，降低了其实用性和内部有效性。由于 SAE 是事后训练的，因此不清楚未能发现特定概念是 SAE 的问题，还是底层 LM 未表示该概念。训练条件和架构选择影响 SAE 学习哪些特征，这使得这个问题更加严重。在追踪 LM 在训练期间如何学习概念时，特征稳定性的缺乏也使得难以比较不同检查点之间的 SAE 特征。为了解决这些限制，我们对 Transformer 架构进行了修改，在选定层引入了 TopK 激活函数，使模型的隐藏状态等同于 TopK SAE 的潜在特征。这种方法消除了对事后训练的需求，同时提供了与 SAE 相当的可解释性。由此产生的 TopK LM 在模型大小、计算效率和可解释性之间提供了有利的权衡。尽管这种简单的架构改变，TopK LM 仍保持其原始能力，同时提供强大的可解释性益处。我们的实验表明，TopK LM 学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并促进对跨检查点和层的神经元形成过程的详细分析。这些特性使 TopK LM 成为理解语言模型如何学习和表示概念的稳定可靠工具，我们相信这将显著推动未来在模型可解释性和可控性方面的研究。", "summary": "本文提出了一种名为 TopK 语言模型的新型 Transformer 架构，旨在解决稀疏自编码器（SAE）在解释语言模型时面临的挑战，如训练后置性、特征不稳定和概念发现模糊性。通过在 Transformer 的选定层集成 TopK 激活函数，TopK LMs 直接在模型训练过程中生成与 TopK SAE 潜在特征等效的隐藏状态，从而无需事后训练。这种方法在保持原始模型能力的同时，提供了与 SAE 相当甚至更优的可解释性，并在模型大小、计算效率和可解释性之间实现了良好平衡。实验证明，TopK LMs 学习到的稀疏表示能够实现有效的神经元干预和对概念形成过程的深入分析，使其成为理解语言模型学习机制的稳定可靠工具，有望推动模型可解释性和可控性领域的发展。", "keywords": "TopK 语言模型, 可解释性, 稀疏自编码器, Transformer, 神经元干预", "comments": "这项工作具有重要的创新性，它将可解释性从一个独立的后处理步骤整合到模型架构本身。通过在训练阶段直接生成可解释的稀疏表示，TopK LMs 解决了传统 SAE 的主要痛点，如训练依赖性、特征稳定性和概念归属问题。这种内建的可解释性不仅提高了分析效率，也为未来更深入地理解和控制大型语言模型提供了坚实的基础，特别是在追踪概念学习和进行神经元干预方面展现出巨大潜力。"}}
{"id": "2506.21009", "title": "User-in-the-Loop View Sampling with Error Peaking Visualization", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Shohei Mori"], "summary": "Augmented reality (AR) provides ways to visualize missing view samples for\nnovel view synthesis. Existing approaches present 3D annotations for new view\nsamples and task users with taking images by aligning the AR display. This data\ncollection task is known to be mentally demanding and limits capture areas to\npre-defined small areas due to the ideal but restrictive underlying sampling\ntheory. To free users from 3D annotations and limited scene exploration, we\npropose using locally reconstructed light fields and visualizing errors to be\nremoved by inserting new views. Our results show that the error-peaking\nvisualization is less invasive, reduces disappointment in final results, and is\nsatisfactory with fewer view samples in our mobile view synthesis system. We\nalso show that our approach can contribute to recent radiance field\nreconstruction for larger scenes, such as 3D Gaussian splatting.", "comment": "Accepted at IEEE ICIP 2025, Project Page:\n  https://mediated-reality.github.io/projects/yasunaga_icip25/", "pdf_url": "http://arxiv.org/pdf/2506.21009v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21009v1", "AI": {"title_translation": "用户在环的视图采样与误差峰值可视化", "tldr": "本文提出一种用户在环的视图采样方法，通过可视化误差峰值来指导用户，减少了数据采集的负担并提高了效率。", "motivation": "现有的增强现实视图采样方法对用户来说心智负担重，且由于采样理论的限制，捕获区域小。为了让用户摆脱3D标注和有限的场景探索，作者提出了新的方法。", "method": "提出使用局部重建的光场，并通过可视化误差峰值来指导用户插入新的视图以消除误差。这种“误差峰值可视化”方法引导用户。", "result": "误差峰值可视化方法侵入性更小，减少了最终结果的失望，并且在移动视图合成系统中用更少的视图样本就能达到满意效果。该方法还有助于更大场景的辐射场重建（如3D高斯泼溅）。", "conclusion": "通过误差峰值可视化，可以有效改善用户在环的视图采样体验，提高数据采集效率和结果质量，并适用于更大场景的辐射场重建。", "translation": "增强现实（AR）提供了可视化缺失视图样本以进行新视图合成的方法。现有方法为新的视图样本提供3D标注，并要求用户通过对齐AR显示器来拍摄图像。众所周知，这种数据收集任务对心智要求很高，并且由于理想但限制性的底层采样理论，将捕获区域限制在预定义的小区域内。为了将用户从3D标注和有限的场景探索中解放出来，我们提出使用局部重建的光场并可视化通过插入新视图需要消除的误差。我们的结果表明，误差峰值可视化侵入性更小，减少了最终结果的失望，并且在我们的移动视图合成系统中，用更少的视图样本就能达到满意效果。我们还表明，我们的方法可以促进对更大场景的近期辐射场重建，例如3D高斯泼溅。", "summary": "本文针对增强现实中新视图合成的数据采集痛点，提出一种用户在环的视图采样方法。该方法利用局部重建的光场并可视化需要通过插入新视图消除的误差峰值，从而指导用户。实验结果表明，这种误差峰值可视化方法能有效降低用户负担，减少视图样本数量，并适用于大规模场景的辐射场重建，提高了数据采集的效率和用户满意度。", "keywords": "用户在环, 视图采样, 误差可视化, 增强现实, 辐射场重建", "comments": "本文的创新点在于提出了“误差峰值可视化”这一新颖的用户指导机制，有效解决了现有AR视图采样方法中用户心智负担重和采样区域受限的问题。通过将误差反馈给用户，实现了更智能、更高效的用户在环数据采集，对于移动视图合成和大规模辐射场重建具有重要意义。"}}
{"id": "2506.21037", "title": "RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment", "authors": ["Suorong Yang", "Peijia Li", "Furao Shen", "Jian Zhao"], "summary": "Modern deep architectures often rely on large-scale datasets, but training on\nthese datasets incurs high computational and storage overhead. Real-world\ndatasets often contain substantial redundancies, prompting the need for more\ndata-efficient training paradigms. Data selection has shown promise to mitigate\nredundancy by identifying the most representative samples, thereby reducing\ntraining costs without compromising performance. Existing methods typically\nrely on static scoring metrics or pretrained models, overlooking the combined\neffect of selected samples and their evolving dynamics during training. We\nintroduce the concept of epsilon-sample cover, which quantifies sample\nredundancy based on inter-sample relationships, capturing the intrinsic\nstructure of the dataset. Based on this, we reformulate data selection as a\nreinforcement learning (RL) process and propose RL-Selector, where a\nlightweight RL agent optimizes the selection policy by leveraging\nepsilon-sample cover derived from evolving dataset distribution as a reward\nsignal. Extensive experiments across benchmark datasets and diverse\narchitectures demonstrate that our method consistently outperforms existing\nstate-of-the-art baselines. Models trained with our selected datasets show\nenhanced generalization performance with improved training efficiency.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21037v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21037v1", "AI": {"title_translation": "RL-Selector：基于强化学习和冗余评估的数据选择方法", "tldr": "RL-Selector是一种通过强化学习和epsilon-sample cover进行数据选择的新方法，旨在减少大型数据集的冗余，提高训练效率和泛化性能，并优于现有基线。", "motivation": "现代深度学习模型依赖大规模数据集，但训练成本高昂且数据存在大量冗余。现有数据选择方法通常依赖静态指标或预训练模型，未能考虑样本组合效应及训练过程中的动态变化，因此需要更数据高效的训练范式。", "method": "本文提出了epsilon-sample cover概念来量化样本冗余，并基于此将数据选择重构为强化学习过程。RL-Selector通过一个轻量级强化学习代理，利用从动态数据集分布中导出的epsilon-sample cover作为奖励信号来优化数据选择策略。", "result": "在多个基准数据集和不同架构上的大量实验表明，该方法持续优于现有最先进的基线。使用RL-Selector选择的数据集训练的模型显示出更强的泛化性能和更高的训练效率。", "conclusion": "RL-Selector通过引入epsilon-sample cover和强化学习框架，有效解决了大规模数据集的冗余问题，显著提高了模型训练效率和泛化性能。", "translation": "现代深度架构通常依赖大规模数据集，但在此类数据集上进行训练会产生高昂的计算和存储开销。真实世界的数据集通常包含大量冗余，这促使人们需要更数据高效的训练范式。数据选择已显示出通过识别最具代表性的样本来减轻冗余的潜力，从而在不损害性能的情况下降低训练成本。现有方法通常依赖静态评分指标或预训练模型，忽视了所选样本的组合效应及其在训练期间的演变动态。我们引入了epsilon-sample cover的概念，它基于样本间关系量化样本冗余，捕获数据集的内在结构。在此基础上，我们将数据选择重构为强化学习（RL）过程，并提出了RL-Selector，其中一个轻量级RL代理通过利用从演变的数据集分布中导出的epsilon-sample cover作为奖励信号来优化选择策略。在基准数据集和不同架构上的大量实验表明，我们的方法持续优于现有最先进的基线。使用我们选择的数据集训练的模型显示出增强的泛化性能和改进的训练效率。", "summary": "RL-Selector是一种新颖的数据选择方法，旨在解决大规模数据集训练中的冗余和高成本问题。它引入了epsilon-sample cover来量化样本冗余，并将数据选择建模为一个强化学习过程。通过一个轻量级RL代理，利用动态数据集分布中的epsilon-sample cover作为奖励信号来优化数据选择策略。实验证明，RL-Selector在多个基准数据集上优于现有方法，能有效提高模型泛化能力和训练效率。", "keywords": "强化学习, 数据选择, 冗余评估, epsilon-sample cover, 训练效率", "comments": "本文的创新点在于引入了epsilon-sample cover来量化样本冗余，并将数据选择问题巧妙地转化为强化学习任务。这种动态的、基于训练过程演变的样本选择方式，克服了传统静态方法的局限性，有效提升了训练效率和模型性能，为大规模深度学习的数据处理提供了有前景的新范式。"}}
{"id": "2506.21495", "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21495v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21495v1", "AI": {"title_translation": "弥合LLMs的离线和在线强化学习", "tldr": "研究发现，在线和半在线强化学习方法在微调大型语言模型时，显著优于离线方法，并且DPO和GRPO表现相似。", "motivation": "探索强化学习方法在大型语言模型微调中，从离线到半在线再到完全在线场景的有效性，涵盖可验证和不可验证任务。", "method": "研究调查了在可验证的数学任务和不可验证的指令遵循任务上，将强化学习方法应用于LLM微调。具体比较了在线和半在线的直接偏好优化（DPO）和组奖励策略优化（GRPO）目标。", "result": "发现在线和半在线的DPO与GRPO在性能和收敛性上表现相似，并且它们都显著优于离线方法。此外，将可验证和不可验证奖励进行多任务联合训练可以提高两种任务类型的性能。", "conclusion": "在LLM微调中，在线和半在线强化学习方法比离线方法更有效，且DPO和GRPO表现相当；多任务学习可进一步提升性能。", "translation": "我们研究了强化学习方法在大型语言模型微调中从离线到半在线再到完全在线方案的有效性，涵盖可验证和不可验证任务。我们的实验包括在可验证的数学任务和不可验证的指令遵循任务上进行训练，并对两者进行了一系列基准评估。在这些设置中，我们广泛比较了在线和半在线的直接偏好优化（Direct Preference Optimization）和组奖励策略优化（Group Reward Policy Optimization）目标，并惊奇地发现这些变体在性能和收敛性上相似，它们都显著优于离线方法。我们提供了详细的训练动态和超参数选择策略分析，以实现最佳结果。最后，我们展示了将可验证和不可验证奖励进行多任务联合训练可以提高两种任务类型的性能。", "summary": "本文探讨了强化学习方法在大型语言模型微调中从离线到在线的有效性，涵盖可验证和不可验证任务。研究发现，在线和半在线的DPO与GRPO在性能和收敛性上表现相似，且均显著优于离线方法。此外，多任务联合训练可进一步提升LLM在不同任务类型上的性能。", "keywords": "强化学习, 大型语言模型, 微调, 在线学习, 离线学习", "comments": "本文的创新点在于系统地比较了不同在线程度的强化学习方法（离线、半在线、在线）在LLM微调中的表现，并发现在线方法显著优于离线方法，这对于LLM的持续学习和适应性非常重要。同时，DPO和GRPO的相似表现也提供了实际应用中的选择灵活性。多任务学习的发现也很有价值，表明可以同时优化不同类型的奖励。"}}
{"id": "2506.21011", "title": "Bridging Video Quality Scoring and Justification via Large Multimodal Models", "authors": ["Qizhi Xie", "Kun Yuan", "Yunpeng Qu", "Jiachao Gong", "Mingda Wu", "Ming Sun", "Chao Zhou", "Jihong Zhu"], "summary": "Classical video quality assessment (VQA) methods generate a numerical score\nto judge a video's perceived visual fidelity and clarity. Yet, a score fails to\ndescribe the video's complex quality dimensions, restricting its applicability.\nBenefiting from the linguistic output, adapting video large multimodal models\n(LMMs) to VQA via instruction tuning has the potential to address this issue.\nThe core of the approach lies in the video quality-centric instruction data.\nPrevious explorations mainly focus on the image domain, and their data\ngeneration processes heavily rely on human quality annotations and proprietary\nsystems, limiting data scalability and effectiveness. To address these\nchallenges, we propose the Score-based Instruction Generation (SIG) pipeline.\nSpecifically, SIG first scores multiple quality dimensions of an unlabeled\nvideo and maps scores to text-defined levels. It then explicitly incorporates a\nhierarchical Chain-of-Thought (CoT) to model the correlation between specific\ndimensions and overall quality, mimicking the human visual system's reasoning\nprocess. The automated pipeline eliminates the reliance on expert-written\nquality descriptions and proprietary systems, ensuring data scalability and\ngeneration efficiency. To this end, the resulting Score2Instruct (S2I) dataset\ncontains over 320K diverse instruction-response pairs, laying the basis for\ninstruction tuning. Moreover, to advance video LMMs' quality scoring and\njustification abilities simultaneously, we devise a progressive tuning strategy\nto fully unleash the power of S2I. Built upon SIG, we further curate a\nbenchmark termed S2I-Bench with 400 open-ended questions to better evaluate the\nquality justification capacity of video LMMs. Experimental results on the\nS2I-Bench and existing benchmarks indicate that our method consistently\nimproves quality scoring and justification capabilities across multiple video\nLMMs.", "comment": "15 pages, 4 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2506.21011v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21011v1", "AI": {"title_translation": "通过大型多模态模型连接视频质量评分与解释", "tldr": "本文提出了一种基于分数的指令生成（SIG）管道来创建大规模视频质量指令数据（S2I），以提升大型多模态模型的视频质量评分和解释能力。", "motivation": "经典的视频质量评估（VQA）方法只生成一个数值分数，无法描述视频复杂的质量维度，限制了其适用性。现有视频大型多模态模型（LMMs）在VQA上的应用，其数据生成过程主要依赖于人工标注和专有系统，限制了数据可扩展性和有效性。", "method": "提出了“基于分数的指令生成（SIG）”管道，该管道首先对未标注视频的多个质量维度进行评分并映射到文本级别，然后引入分层思维链（CoT）来模拟特定维度与整体质量之间的关联，模仿人类视觉系统推理。该自动化管道消除了对专家编写质量描述和专有系统的依赖。基于SIG，构建了Score2Instruct（S2I）数据集，包含超过320K个指令-响应对。设计了一种渐进式微调策略以同时提升视频LMMs的质量评分和解释能力。进一步策划了一个名为S2I-Bench的基准测试，包含400个开放式问题，用于评估视频LMMs的质量解释能力。", "result": "在S2I-Bench和现有基准测试上的实验结果表明，该方法持续改进了多个视频LMMs的质量评分和解释能力。", "conclusion": "通过提出的自动化指令生成管道和大规模数据集，本研究成功地弥合了视频质量评分和解释之间的鸿沟，显著提升了大型多模态模型在视频质量评估任务中的表现和解释能力。", "translation": "经典的视频质量评估（VQA）方法生成一个数值分数来判断视频感知的视觉保真度和清晰度。然而，一个分数无法描述视频复杂的质量维度，限制了其适用性。受益于语言输出，通过指令微调将视频大型多模态模型（LMMs）应用于VQA有潜力解决这个问题。该方法的核心在于以视频质量为中心的指令数据。之前的探索主要集中在图像领域，并且它们的数据生成过程严重依赖于人工质量标注和专有系统，限制了数据的可扩展性和有效性。为了应对这些挑战，我们提出了基于分数的指令生成（SIG）管道。具体而言，SIG首先对未标注视频的多个质量维度进行评分，并将分数映射到文本定义的级别。然后，它明确地引入了分层思维链（CoT）来模拟特定维度与整体质量之间的关联，模仿人类视觉系统的推理过程。该自动化管道消除了对专家编写的质量描述和专有系统的依赖，确保了数据的可扩展性和生成效率。为此，生成的Score2Instruct（S2I）数据集包含超过320K个多样化的指令-响应对，为指令微调奠定了基础。此外，为了同时提升视频LMMs的质量评分和解释能力，我们设计了一种渐进式微调策略，以充分发挥S2I的潜力。基于SIG，我们进一步策划了一个名为S2I-Bench的基准测试，包含400个开放式问题，以更好地评估视频LMMs的质量解释能力。在S2I-Bench和现有基准测试上的实验结果表明，我们的方法持续改进了多个视频LMMs的质量评分和解释能力。", "summary": "本文提出了一种新颖的基于分数的指令生成（SIG）管道，旨在解决传统视频质量评估（VQA）仅提供数值分数而缺乏解释性的问题。SIG通过对视频多维度评分并结合分层思维链（CoT），自动化生成大规模高质量指令数据（Score2Instruct, S2I），避免了对人工标注和专有系统的依赖。S2I数据集包含超过320K个指令-响应对，为视频大型多模态模型（LMMs）的指令微调提供了基础。研究还提出了一种渐进式微调策略，并构建了S2I-Bench基准测试来评估模型的质量解释能力。实验结果验证了该方法能显著提升视频LMMs的质量评分和解释能力。", "keywords": "视频质量评估, 大型多模态模型, 指令微调, 数据生成, 思维链", "comments": "本文的创新点在于提出了一个自动化的数据生成管道SIG，解决了传统视频质量评估缺乏解释性的问题，并克服了当前大型多模态模型在视频质量领域数据稀缺和依赖人工标注的局限性。通过引入分层思维链模拟人类推理过程，使得生成的指令数据更具高质量和多样性。构建大规模S2I数据集和S2I-Bench基准测试对推动视频LMMs在VQA领域的发展具有重要意义。"}}
{"id": "2506.21039", "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning", "authors": ["Jaebak Hwang", "Sanghyeon Lee", "Jeongmo Kim", "Seungyul Han"], "summary": "Long-horizon goal-conditioned tasks pose fundamental challenges for\nreinforcement learning (RL), particularly when goals are distant and rewards\nare sparse. While hierarchical and graph-based methods offer partial solutions,\nthey often suffer from subgoal infeasibility and inefficient planning. We\nintroduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL\nframework that enforces single-step subgoal reachability by structurally\nconstraining high-level decision-making. To enhance exploration, SSE employs a\ndecoupled exploration policy that systematically traverses underexplored\nregions of the goal space. Furthermore, a failure-aware path refinement, which\nrefines graph-based planning by dynamically adjusting edge costs according to\nobserved low-level success rates, thereby improving subgoal reliability.\nExperimental results across diverse long-horizon benchmarks demonstrate that\nSSE consistently outperforms existing goal-conditioned RL and hierarchical RL\napproaches in both efficiency and success rate.", "comment": "9 technical page followed by references and appendix", "pdf_url": "http://arxiv.org/pdf/2506.21039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21039v1", "AI": {"title_translation": "严格子目标执行：分层强化学习中可靠的长周期规划", "tldr": "严格子目标执行（SSE）是一种新的分层强化学习框架，通过结构化约束高层决策、解耦探索策略和故障感知路径优化，解决了长周期目标条件任务中子目标不可行和规划效率低下的问题，并在多个基准测试中表现出卓越的效率和成功率。", "motivation": "长周期目标条件任务对强化学习（RL）提出了根本性挑战，特别是当目标遥远且奖励稀疏时。现有的分层和基于图的方法虽然提供部分解决方案，但往往面临子目标不可行和规划效率低下的问题。", "method": "本研究引入了严格子目标执行（SSE），一个基于图的分层RL框架。它通过结构化约束高层决策来强制实现单步子目标可达性。为了增强探索，SSE采用解耦探索策略，系统地遍历目标空间中探索不足的区域。此外，它还引入了故障感知路径优化，通过根据观察到的低层成功率动态调整边缘成本来改进基于图的规划，从而提高子目标的可靠性。", "result": "实验结果表明，在各种长周期基准测试中，SSE在效率和成功率方面始终优于现有的目标条件RL和分层RL方法。", "conclusion": "严格子目标执行（SSE）框架通过其创新的设计，有效解决了长周期目标条件任务中的关键挑战，显著提高了分层强化学习的性能和可靠性。", "translation": "长周期目标条件任务对强化学习（RL）提出了根本性挑战，特别是当目标遥远且奖励稀疏时。虽然分层和基于图的方法提供部分解决方案，但它们常常面临子目标不可行和规划效率低下的问题。我们引入了严格子目标执行（SSE），一个基于图的分层RL框架，它通过结构化约束高层决策来强制实现单步子目标可达性。为了增强探索，SSE采用解耦探索策略，系统地遍历目标空间中探索不足的区域。此外，故障感知路径优化通过根据观察到的低层成功率动态调整边缘成本来改进基于图的规划，从而提高子目标的可靠性。在各种长周期基准测试中的实验结果表明，SSE在效率和成功率方面始终优于现有的目标条件RL和分层RL方法。", "summary": "严格子目标执行（SSE）是一个针对长周期目标条件任务的分层强化学习框架，旨在解决现有方法中子目标不可行和规划效率低下的问题。SSE通过结构化约束高层决策确保单步子目标可达性，采用解耦探索策略促进目标空间探索，并通过故障感知路径优化动态调整规划以提高子目标可靠性。实验证明，SSE在效率和成功率上均超越了现有方法。", "keywords": "分层强化学习, 子目标执行, 长周期规划, 目标条件RL, 稀疏奖励", "comments": "SSE的创新之处在于其“严格子目标执行”的概念，通过结构化约束高层决策来强制可达性，以及结合解耦探索和故障感知路径优化，共同解决了分层RL中的关键挑战。这对于处理复杂、稀疏奖励的长周期任务具有重要意义，有望提升实际应用中的机器人规划和控制能力。"}}
{"id": "2506.21497", "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21497v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21497v1", "AI": {"title_translation": "通过交互式大型语言模型对齐增强社交驱动对话中的用户参与度", "tldr": "该研究提出了一种通过将用户反应作为奖励信号来对齐交互式大型语言模型的方法，以增强社交驱动对话中的用户参与度。通过用户模拟器和i×MCTS收集数据，并使用DPO进行模型对齐，实验证明其能有效提升用户参与度。", "motivation": "先前的研究优化模型以推理相关知识或规划对话行为流程，但这些方法与用户参与度之间的关系是微妙的，并不能保证在社交驱动对话中的用户参与度。因此，需要一种更直接、更有效的方法来学习和提升用户参与度。", "method": "该方法将用户在互动后与对话意图相关的反应作为直接且相关的用户参与度指标，并以此作为奖励来对齐交互式大型语言模型（LLMs）。为此，研究开发了一个用户模拟器与目标LLMs进行互动，并利用i×MCTS（用于交互的蒙特卡洛树搜索）来探索用户与LLM系统之间的互动，从而收集包含高质量和低质量体验对的数据集。最后，通过直接偏好优化（DPO）来对齐交互式LLMs以实现高水平的用户参与度。", "result": "在两种社交驱动对话场景（情感支持对话和劝善）中进行的实验表明，所提出的方法能够有效增强交互式大型语言模型中的用户参与度。", "conclusion": "该方法在社交驱动对话场景中，能够有效增强交互式大型语言模型中的用户参与度。", "translation": "通过互动增强用户参与度在社交驱动的对话中扮演着至关重要的角色。尽管先前的研究已经优化了模型以推理相关知识或规划对话行为流程，但用户参与度与知识或对话行为之间的关系是微妙的，并不能保证在社交驱动对话中的用户参与度。为此，我们通过利用对话未来发展中的信号，使交互式大型语言模型能够学习用户参与度。具体而言，我们采用一个更直接、更相关的用户参与度指标，即用户在互动后与对话意图相关的反应，作为奖励来对齐交互式大型语言模型。为了实现这一点，我们开发了一个用户模拟器来与目标交互式大型语言模型互动，并通过i×MCTS（用于交互的蒙特卡洛树搜索）探索用户与交互式大型语言模型系统之间的互动。通过这种方式，我们使用i×MCTS收集了一个包含高质量和低质量体验对的数据集，并相应地通过直接偏好优化（DPO）对齐交互式大型语言模型以实现高水平的用户参与度。在两种社交驱动对话场景（情感支持对话和劝善）中进行的实验表明，我们的方法有效增强了交互式大型语言模型中的用户参与度。", "summary": "本论文提出了一种新颖的方法，通过直接将交互式大型语言模型（LLMs）与用户反应对齐，从而增强社交驱动对话中的用户参与度。与以往侧重于知识或对话行为的方法不同，本研究将互动后的用户反应作为奖励信号。论文引入了一个用户模拟器和i×MCTS来收集偏好数据，并利用这些数据通过直接偏好优化（DPO）来训练LLMs。在情感支持和劝善对话场景中的实验表明，该方法能够有效提升用户参与度。", "keywords": "用户参与度, 社交驱动对话, 交互式大型语言模型, 直接偏好优化, i×MCTS", "comments": "本研究的创新之处在于直接将用户反应作为LLM对齐的奖励信号，这解决了传统方法无法保证用户参与度的微妙问题。此外，利用用户模拟器和i×MCTS进行数据收集，为DPO的应用提供了可能，这也是一个显著的贡献。该研究的重要性在于提升了LLMs在社交语境中的实际应用价值。"}}
{"id": "2506.21012", "title": "FedSC: Federated Learning with Semantic-Aware Collaboration", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Jiahua Shi", "Jun Shen"], "summary": "Federated learning (FL) aims to train models collaboratively across clients\nwithout sharing data for privacy-preserving. However, one major challenge is\nthe data heterogeneity issue, which refers to the biased labeling preferences\nat multiple clients. A number of existing FL methods attempt to tackle data\nheterogeneity locally (e.g., regularizing local models) or globally (e.g.,\nfine-tuning global model), often neglecting inherent semantic information\ncontained in each client. To explore the possibility of using intra-client\nsemantically meaningful knowledge in handling data heterogeneity, in this\npaper, we propose Federated Learning with Semantic-Aware Collaboration (FedSC)\nto capture client-specific and class-relevant knowledge across heterogeneous\nclients. The core idea of FedSC is to construct relational prototypes and\nconsistent prototypes at semantic-level, aiming to provide fruitful class\nunderlying knowledge and stable convergence signals in a prototype-wise\ncollaborative way. On the one hand, FedSC introduces an inter-contrastive\nlearning strategy to bring instance-level embeddings closer to relational\nprototypes with the same semantics and away from distinct classes. On the other\nhand, FedSC devises consistent prototypes via a discrepancy aggregation manner,\nas a regularization penalty to constrain the optimization region of the local\nmodel. Moreover, a theoretical analysis for FedSC is provided to ensure a\nconvergence guarantee. Experimental results on various challenging scenarios\ndemonstrate the effectiveness of FedSC and the efficiency of crucial\ncomponents.", "comment": "12 pages, KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.21012v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21012v1", "AI": {"title_translation": "FedSC：语义感知协作的联邦学习", "tldr": "FedSC提出了一种新的联邦学习方法，通过构建语义级别的关系原型和一致原型来解决数据异构性问题，并引入了跨对比学习和差异聚合策略，同时提供了收敛性保证。", "motivation": "联邦学习在数据隐私保护下进行模型协作训练面临数据异构性（客户端数据标签偏置）的挑战。现有方法常忽略客户端固有的语义信息，因此需要探索利用客户端内部语义知识来处理数据异构性。", "method": "本文提出了FedSC（Federated Learning with Semantic-Aware Collaboration）。FedSC的核心思想是在语义层面构建关系原型（relational prototypes）和一致原型（consistent prototypes）。它引入了跨对比学习（inter-contrastive learning）策略，使实例级嵌入与相同语义的关系原型更接近，并远离不同类。此外，FedSC通过差异聚合（discrepancy aggregation）方式设计了一致原型，作为正则化惩罚来约束局部模型的优化区域。该方法还提供了收敛性理论分析。", "result": "在各种挑战性场景下的实验结果表明，FedSC的有效性以及其关键组件的效率。", "conclusion": "FedSC通过利用客户端内部的语义感知协作，有效地解决了联邦学习中的数据异构性问题，并提供了理论上的收敛性保证，实验证明其有效性和关键组件的效率。", "translation": "联邦学习（FL）旨在跨客户端协作训练模型，同时不共享数据以保护隐私。然而，一个主要的挑战是数据异构性问题，它指的是多个客户端的标签偏好存在偏差。许多现有的联邦学习方法试图在本地（例如，正则化本地模型）或全局（例如，微调全局模型）解决数据异构性问题，但往往忽略了每个客户端中固有的语义信息。为了探索利用客户端内部语义上有意义的知识来处理数据异构性的可能性，本文提出了语义感知协作的联邦学习（FedSC），以捕获异构客户端之间特定于客户端和与类别相关的知识。FedSC的核心思想是在语义层面构建关系原型和一致原型，旨在以原型协作的方式提供丰富的类别底层知识和稳定的收敛信号。一方面，FedSC引入了一种跨对比学习策略，使实例级嵌入更接近具有相同语义的关系原型，并远离不同的类别。另一方面，FedSC通过差异聚合方式设计了一致原型，作为正则化惩罚来约束局部模型的优化区域。此外，FedSC还提供了理论分析以确保收敛性。在各种挑战性场景下的实验结果证明了FedSC的有效性以及关键组件的效率。", "summary": "本文提出了一种名为FedSC的联邦学习新框架，旨在解决数据异构性问题。FedSC通过在语义层面构建关系原型和一致原型，利用客户端内部的语义信息。它结合了跨对比学习策略来优化实例嵌入，并使用差异聚合来正则化局部模型。研究还提供了理论收敛性保证，并通过实验验证了FedSC在不同场景下的有效性。", "keywords": "联邦学习, 数据异构性, 语义感知, 原型学习, 对比学习", "comments": "FedSC的创新之处在于其通过语义感知协作来解决联邦学习中的数据异构性问题，特别是引入了关系原型和一致原型，以及跨对比学习和差异聚合等机制，有效地利用了数据固有的语义信息。此外，提供理论收敛性保证也增加了其方法的可靠性。该工作对提升联邦学习在非独立同分布数据环境下的性能具有重要意义。"}}
{"id": "2506.21044", "title": "Efficient Skill Discovery via Regret-Aware Optimization", "authors": ["He Zhang", "Ming Zhou", "Shaopeng Zhai", "Ying Sun", "Hui Xiong"], "summary": "Unsupervised skill discovery aims to learn diverse and distinguishable\nbehaviors in open-ended reinforcement learning. For existing methods, they\nfocus on improving diversity through pure exploration, mutual information\noptimization, and learning temporal representation. Despite that they perform\nwell on exploration, they remain limited in terms of efficiency, especially for\nthe high-dimensional situations. In this work, we frame skill discovery as a\nmin-max game of skill generation and policy learning, proposing a regret-aware\nmethod on top of temporal representation learning that expands the discovered\nskill space along the direction of upgradable policy strength. The key insight\nbehind the proposed method is that the skill discovery is adversarial to the\npolicy learning, i.e., skills with weak strength should be further explored\nwhile less exploration for the skills with converged strength. As an\nimplementation, we score the degree of strength convergence with regret, and\nguide the skill discovery with a learnable skill generator. To avoid\ndegeneration, skill generation comes from an up-gradable population of skill\ngenerators. We conduct experiments on environments with varying complexities\nand dimension sizes. Empirical results show that our method outperforms\nbaselines in both efficiency and diversity. Moreover, our method achieves a 15%\nzero shot improvement in high-dimensional environments, compared to existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21044v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21044v1", "AI": {"title_translation": "通过后悔感知优化实现高效技能发现", "tldr": "现有无监督技能发现方法在效率上受限，尤其在高维环境中。本文提出一种后悔感知方法，将技能发现建模为最小-最大博弈，通过后悔来指导技能探索，提高了高维环境下的效率和多样性。", "motivation": "现有无监督技能发现方法（如纯探索、互信息优化、时间表征学习）虽然在探索方面表现良好，但在效率方面存在局限性，特别是在高维环境下。", "method": "将技能发现构建为技能生成和策略学习的最小-最大博弈。提出一种基于时间表征学习的后悔感知方法，沿着可升级策略强度的方向扩展发现的技能空间。核心思想是技能发现与策略学习是对抗的，即对强度弱的技能进行更多探索，对强度收敛的技能减少探索。具体实现通过后悔度量强度收敛程度，并使用可学习的技能生成器引导技能发现，同时通过可升级的技能生成器群体避免退化。", "result": "实验结果表明，该方法在效率和多样性方面均优于基线方法。此外，在高维环境中，该方法比现有方法实现了15%的零样本改进。", "conclusion": "本文提出的通过后悔感知优化实现高效技能发现的方法，有效解决了现有方法在效率上的局限性，并在高维环境中展现出显著的性能提升。", "translation": "无监督技能发现旨在在开放式强化学习中学习多样化和可区分的行为。现有方法侧重于通过纯粹探索、互信息优化和学习时间表征来提高多样性。尽管它们在探索方面表现良好，但在效率方面仍然有限，特别是对于高维情况。在这项工作中，我们将技能发现构建为技能生成和策略学习的最小-最大博弈，提出了一种基于时间表征学习的后悔感知方法，该方法沿着可升级策略强度的方向扩展发现的技能空间。该方法背后的关键见解是技能发现与策略学习是对抗的，即应进一步探索强度较弱的技能，而对强度已收敛的技能则减少探索。作为一种实现方式，我们用后悔来评估强度收敛的程度，并用可学习的技能生成器指导技能发现。为了避免退化，技能生成来自一个可升级的技能生成器群体。我们在不同复杂度和维度大小的环境中进行了实验。实证结果表明，我们的方法在效率和多样性方面均优于基线。此外，与现有方法相比，我们的方法在高维环境中实现了15%的零样本改进。", "summary": "本文提出一种名为“后悔感知优化”的高效无监督技能发现方法，旨在解决现有方法在高维环境中的效率瓶颈。该方法将技能发现建模为技能生成与策略学习的最小-最大博弈，并利用“后悔”机制来指导技能探索，即对策略强度较弱的技能进行更多探索。实验证明，该方法在效率和多样性上均优于现有基线，尤其在高维环境中实现了显著的零样本性能提升。", "keywords": "技能发现, 强化学习, 后悔感知, 高效性, 多样性", "comments": "这项工作通过引入“后悔感知”机制，将技能发现与策略学习的对抗性视角相结合，提供了一种新颖且高效的技能探索策略。其创新点在于利用后悔来量化策略强度收敛，从而智能地分配探索资源，有效解决了高维环境下的效率问题。15%的零样本改进是一个显著的成果，表明该方法具有较强的泛化能力和实用价值。"}}
{"id": "2506.21015", "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation", "authors": ["Qingyue Jiao", "Kangyu Zheng", "Yiyu Shi", "Zhiding Liang"], "summary": "Machine learning-assisted diagnosis is gaining traction in skin disease\ndetection, but training effective models requires large amounts of high-quality\ndata. Skin disease datasets often suffer from class imbalance, privacy\nconcerns, and object bias, making data augmentation essential. While classical\ngenerative models are widely used, they demand extensive computational\nresources and lengthy training time. Quantum computing offers a promising\nalternative, but existing quantum-based image generation methods can only yield\ngrayscale low-quality images. Through a novel classical-quantum latent space\nfusion technique, our work overcomes this limitation and introduces the first\nclassical-quantum generative adversarial network (GAN) capable of generating\ncolor medical images. Our model outperforms classical deep convolutional GANs\nand existing hybrid classical-quantum GANs in both image generation quality and\nclassification performance boost when used as data augmentation. Moreover, the\nperformance boost is comparable with that achieved using state-of-the-art\nclassical generative models, yet with over 25 times fewer parameters and 10\ntimes fewer training epochs. Such results suggest a promising future for\nquantum image generation as quantum hardware advances. Finally, we demonstrate\nthe robust performance of our model on real IBM quantum machine with hardware\nnoise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21015v1", "categories": ["cs.CV", "cs.LG", "quant-ph"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21015v1", "AI": {"title_translation": "HybridQ：用于皮肤病图像生成的混合经典-量子生成对抗网络", "tldr": "HybridQ是一种新的混合经典-量子GAN，能够生成彩色皮肤病医学图像，与经典GAN相比，在图像质量和数据增强方面表现更好，参数和训练周期显著减少。", "motivation": "皮肤病诊断中的机器学习模型需要大量高质量数据，但现有数据集存在类别不平衡、隐私问题和对象偏差，需要数据增强。经典生成模型计算资源需求大，训练时间长。现有量子图像生成方法只能生成低质量灰度图像。", "method": "通过一种新颖的经典-量子潜在空间融合技术，引入了第一个能够生成彩色医学图像的经典-量子生成对抗网络（HybridQ）。", "result": "HybridQ在图像生成质量和作为数据增强时的分类性能提升方面优于经典深度卷积GAN和现有混合经典-量子GAN。其性能提升与最先进的经典生成模型相当，但参数减少25倍以上，训练周期减少10倍。该模型在真实的IBM量子机器上表现出鲁棒性。", "conclusion": "量子图像生成具有广阔前景，随着量子硬件的发展，HybridQ展示了在生成高质量彩色医学图像方面的潜力，并且在资源效率方面优于经典方法。", "translation": "机器学习辅助诊断在皮肤病检测中越来越受欢迎，但训练有效的模型需要大量高质量数据。皮肤病数据集常面临类别不平衡、隐私问题和对象偏差，使得数据增强至关重要。虽然经典生成模型被广泛使用，但它们需要大量的计算资源和漫长的训练时间。量子计算提供了一个有前景的替代方案，但现有的基于量子的图像生成方法只能产生灰度低质量图像。通过一种新颖的经典-量子潜在空间融合技术，我们的工作克服了这一限制，并引入了第一个能够生成彩色医学图像的经典-量子生成对抗网络（GAN）。我们的模型在图像生成质量和作为数据增强时的分类性能提升方面均优于经典深度卷积GAN和现有混合经典-量子GAN。此外，其性能提升与使用最先进的经典生成模型所达到的效果相当，但参数减少了25倍以上，训练周期减少了10倍。这些结果表明，随着量子硬件的进步，量子图像生成具有广阔的前景。最后，我们展示了我们的模型在具有硬件噪声的真实IBM量子机器上的鲁棒性能。", "summary": "本研究提出了一种名为HybridQ的混合经典-量子生成对抗网络，旨在解决皮肤病图像数据集的局限性并提高数据增强效率。通过创新的经典-量子潜在空间融合技术，HybridQ首次实现了彩色医学图像的生成，克服了现有量子生成方法仅能产生低质量灰度图像的限制。实验结果表明，HybridQ在图像生成质量和作为数据增强时的分类性能提升方面优于传统GAN和现有混合量子GAN，且在性能与最先进经典模型相当的同时，显著减少了模型参数和训练时间。这表明量子图像生成在未来具有巨大潜力，并且该模型在真实量子硬件上表现出良好的鲁棒性。", "keywords": "混合生成对抗网络, 量子计算, 皮肤病图像生成, 数据增强, 经典-量子融合", "comments": "本文的创新点在于提出了首个能够生成彩色医学图像的混合经典-量子GAN，通过潜在空间融合技术克服了现有量子图像生成方法的局限性。其重要性体现在为医疗图像数据增强提供了一种高效且资源友好的新范式，特别是在数据稀缺和隐私敏感的皮肤病领域。模型在减少参数和训练时间方面的显著优势预示了量子计算在生成模型领域的广阔前景。"}}
{"id": "2506.21054", "title": "FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning", "authors": ["Fu Peng", "Ming Tang"], "summary": "In federated learning (FL), the data distribution of each client may change\nover time, introducing both temporal and spatial data heterogeneity, known as\nconcept drift. Data heterogeneity arises from three drift sources: real drift\n(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the\ninput distribution P(x)), and label drift (a shift in the label distribution\nP(y)). However, most existing FL methods addressing concept drift primarily\nfocus on real drift. When clients experience virtual or label drift, these\nmethods often fail to selectively retain useful historical knowledge, leading\nto catastrophic forgetting. A key challenge lies in distinguishing different\nsources of drift, as they require distinct adaptation strategies: real drift\ncalls for discarding outdated data, while virtual or label drift benefits from\nretaining historical data. Without explicitly identifying the drift sources, a\ngeneral adaptation strategy is suboptimal and may harm generalization. To\naddress this challenge, we propose FedDAA, a dynamic clustered FL framework\ndesigned to adapt to multi-source concept drift while preserving valuable\nhistorical knowledge. Specifically, FedDAA integrates three modules: a cluster\nnumber determination module to find the optimal number of clusters; a real\ndrift detection module to distinguish real drift from virtual/label drift; and\na concept drift adaptation module to adapt to new data while retaining useful\nhistorical information. We provide theoretical convergence guarantees, and\nexperiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over\nstate-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21054v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21054v1", "AI": {"title_translation": "联邦学习中概念漂移适应的动态客户端聚类：FedDAA", "tldr": "FedDAA是一种动态聚类联邦学习框架，通过区分真实漂移、虚拟漂移和标签漂移来适应多源概念漂移，并有效保留历史知识，显著提高了准确性。", "motivation": "联邦学习中客户端数据分布随时间变化导致概念漂移（包括真实漂移、虚拟漂移和标签漂移）。现有方法主要关注真实漂移，在面对虚拟或标签漂移时未能有效保留历史知识，导致灾难性遗忘。区分不同漂移源并采用不同适应策略是关键挑战。", "method": "提出FedDAA，一个动态聚类联邦学习框架。它包含三个模块：1) 聚类数量确定模块，用于找到最优聚类数；2) 真实漂移检测模块，用于区分真实漂移与虚拟/标签漂移；3) 概念漂移适应模块，用于适应新数据同时保留有用历史信息。该方法提供理论收敛保证。", "result": "实验表明，FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上比现有最先进方法实现了7.84%到8.52%的准确率提升。", "conclusion": "FedDAA通过动态客户端聚类和多源概念漂移适应策略，有效解决了联邦学习中的概念漂移问题，特别是在处理虚拟和标签漂移时能更好地保留历史知识，从而提高了模型性能。", "translation": "在联邦学习（FL）中，每个客户端的数据分布可能随时间变化，引入时间性和空间性数据异质性，即概念漂移。数据异质性源于三种漂移源：真实漂移（条件分布 P(y|x) 的变化）、虚拟漂移（输入分布 P(x) 的变化）和标签漂移（标签分布 P(y) 的变化）。然而，大多数现有解决概念漂移的联邦学习方法主要关注真实漂移。当客户端经历虚拟漂移或标签漂移时，这些方法往往无法选择性地保留有用的历史知识，导致灾难性遗忘。一个关键挑战在于区分不同来源的漂移，因为它们需要不同的适应策略：真实漂移需要丢弃过时数据，而虚拟漂移或标签漂移则受益于保留历史数据。如果没有明确识别漂移源，一般的适应策略是次优的，并可能损害泛化能力。为了解决这一挑战，我们提出了 FedDAA，一个动态聚类联邦学习框架，旨在适应多源概念漂移，同时保留有价值的历史知识。具体而言，FedDAA 集成了三个模块：一个聚类数量确定模块，用于找到最优聚类数量；一个真实漂移检测模块，用于区分真实漂移与虚拟/标签漂移；以及一个概念漂移适应模块，用于适应新数据同时保留有用的历史信息。我们提供了理论收敛保证，实验表明 FedDAA 在 Fashion-MNIST、CIFAR-10 和 CIFAR-100 上比现有最先进方法实现了 7.84% 到 8.52% 的准确率提升。", "summary": "本文提出了FedDAA，一个针对联邦学习中多源概念漂移（包括真实漂移、虚拟漂移和标签漂移）的动态聚类框架。FedDAA通过区分不同漂移源，并设计了聚类数确定、真实漂移检测和概念漂移适应三个模块，以在适应新数据的同时有效保留历史知识，避免灾难性遗忘。理论分析和实验结果均验证了FedDAA的有效性，其在多个数据集上显著优于现有SOTA方法。", "keywords": "联邦学习, 概念漂移, 动态聚类, 数据异质性, 灾难性遗忘", "comments": "FedDAA的创新点在于其能够区分并针对联邦学习中的多源概念漂移（真实漂移、虚拟漂移、标签漂移）采取不同的适应策略，特别是其保留历史知识的能力，有效解决了现有方法在虚拟或标签漂移下易发生灾难性遗忘的问题。这对于提升联邦学习在动态非IID环境下的鲁棒性和泛化能力具有重要意义。"}}
{"id": "2506.21521", "title": "Potemkin Understanding in Large Language Models", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21521v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21521v1", "AI": {"title_translation": "大型语言模型中的波将金式理解", "tldr": "大型语言模型在基准测试上的成功可能只是“波将金式理解”（虚假理解），而非真正的人类式理解，因为它们的内部概念表征存在不连贯性。", "motivation": "该论文旨在探讨用于评估大型语言模型（LLMs）的基准测试的有效性。作者质疑基于LLMs在特定问题集上的表现来推断其能力是否合理，并指出如果LLMs的误解方式与人类不同，那么这些基准测试的有效性就会受到质疑。", "method": "论文首先提出了一个形式化框架来解决这一问题。接着，它提出了两种量化“波将金式理解”的方法：一种是使用在三个领域中特别设计的基准测试，另一种是提供其普遍性下限的通用程序。", "result": "研究发现，“波将金式理解”在不同模型、任务和领域中普遍存在。这些失败不仅反映了不正确的理解，而且反映了概念表征中更深层次的内部不连贯性。", "conclusion": "大型语言模型在基准测试上的成功可能只是一种“波将金式理解”，即理解的假象，其答案与人类对概念的解释方式不符，这源于其内部概念表征的深层不连贯性。", "translation": "大型语言模型（LLMs）通常使用基准数据集进行评估。但是，基于LLM对一系列精心策划问题的回答来推断其能力，这种做法的合理性何在？本文首先提出了一个形式化框架来解决这个问题。关键在于，用于测试LLMs的基准测试——例如AP考试——也用于测试人类。然而，这带来了一个隐含的推论：只有当LLMs对概念的误解方式与人类的误解方式相似时，这些基准测试才是有效的。否则，基准测试的成功仅证明了波将金式理解：一种由与任何人类解释概念的方式不符的答案所驱动的理解幻觉。我们提出了两种量化波将金存在的程序：一种是使用在三个领域中特别设计的基准测试，另一种是提供其普遍性下限的通用程序。我们发现波将金式理解在各种模型、任务和领域中普遍存在。我们还发现这些失败不仅反映了不正确的理解，而且反映了概念表征中更深层次的内部不连贯性。", "summary": "该论文引入了一个形式化框架，旨在审视大型语言模型（LLMs）基准测试的有效性。文章提出，如果LLMs的误解方式未能反映人类的误解，那么它们在基准测试上的成功可能仅是“波将金式理解”——一种理解的假象。通过两种量化程序，包括一个专门设计的基准测试，研究发现“波将金式理解”在不同模型、任务和领域中普遍存在，这揭示了LLMs内部概念表征的深层不连贯性，而非仅仅是错误的理解。", "keywords": "大型语言模型, 基准测试, 波将金式理解, 概念不连贯性, 评估", "comments": "这篇论文提出了一个关于当前大型语言模型评估方法有效性的关键问题。它挑战了基准测试高分等同于真实人类式理解的假设，引入了“波将金式理解”这一概念。这强调了需要开发更复杂的评估指标，以深入探究LLM概念表征的内部一致性和与人类认知的对齐程度，超越表面性能的评估。"}}
{"id": "2506.21017", "title": "Multimodal Prompt Alignment for Facial Expression Recognition", "authors": ["Fuyan Ma", "Yiran He", "Bin Sun", "Shutao Li"], "summary": "Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs) like CLIP for various downstream tasks. Despite their success,\ncurrent VLM-based facial expression recognition (FER) methods struggle to\ncapture fine-grained textual-visual relationships, which are essential for\ndistinguishing subtle differences between facial expressions. To address this\nchallenge, we propose a multimodal prompt alignment framework for FER, called\nMPA-FER, that provides fine-grained semantic guidance to the learning process\nof prompted visual features, resulting in more precise and interpretable\nrepresentations. Specifically, we introduce a multi-granularity hard prompt\ngeneration strategy that utilizes a large language model (LLM) like ChatGPT to\ngenerate detailed descriptions for each facial expression. The LLM-based\nexternal knowledge is injected into the soft prompts by minimizing the feature\ndiscrepancy between the soft prompts and the hard prompts. To preserve the\ngeneralization abilities of the pretrained CLIP model, our approach\nincorporates prototype-guided visual feature alignment, ensuring that the\nprompted visual features from the frozen image encoder align closely with\nclass-specific prototypes. Additionally, we propose a cross-modal global-local\nalignment module that focuses on expression-relevant facial features, further\nimproving the alignment between textual and visual features. Extensive\nexperiments demonstrate our framework outperforms state-of-the-art methods on\nthree FER benchmark datasets, while retaining the benefits of the pretrained\nmodel and minimizing computational costs.", "comment": "To appear in ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.21017v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21017v1", "AI": {"title_translation": "多模态提示对齐用于面部表情识别", "tldr": "MPA-FER框架通过多模态提示对齐解决VLM在面部表情识别中难以捕捉细粒度文本-视觉关系的问题，引入LLM生成硬提示并与软提示对齐，同时进行原型引导的视觉特征对齐和跨模态全局-局部对齐，显著提升了FER性能。", "motivation": "当前基于视觉-语言模型（VLM）的面部表情识别（FER）方法难以捕获细粒度的文本-视觉关系，这对于区分面部表情的细微差异至关重要。", "method": "我们提出了一个名为MPA-FER的多模态提示对齐框架。该框架通过多粒度硬提示生成策略，利用大型语言模型（LLM）如ChatGPT生成每个面部表情的详细描述。LLM的外部知识通过最小化软提示和硬提示之间的特征差异来注入到软提示中。为了保留预训练CLIP模型的泛化能力，该方法结合了原型引导的视觉特征对齐，确保冻结图像编码器产生的提示视觉特征与特定类原型紧密对齐。此外，我们提出了一个跨模态全局-局部对齐模块，该模块关注与表情相关的面部特征，进一步改善了文本和视觉特征之间的对齐。", "result": "我们的框架在三个FER基准数据集上超越了最先进的方法，同时保留了预训练模型的优势并最小化了计算成本。", "conclusion": "MPA-FER框架通过有效解决细粒度文本-视觉关系捕获的挑战，显著提升了面部表情识别的性能，并证明了多模态提示对齐的有效性。", "translation": "提示学习已被广泛应用于高效地调整视觉-语言模型（VLM），如CLIP，以适应各种下游任务。尽管取得了成功，但当前基于VLM的面部表情识别（FER）方法难以捕获细粒度的文本-视觉关系，这对于区分面部表情的细微差异至关重要。为了解决这一挑战，我们提出了一个用于FER的多模态提示对齐框架，称为MPA-FER，它为提示视觉特征的学习过程提供细粒度的语义指导，从而产生更精确和可解释的表示。具体来说，我们引入了一种多粒度硬提示生成策略，该策略利用大型语言模型（LLM）如ChatGPT为每个面部表情生成详细描述。通过最小化软提示和硬提示之间的特征差异，将基于LLM的外部知识注入到软提示中。为了保留预训练CLIP模型的泛化能力，我们的方法结合了原型引导的视觉特征对齐，确保冻结图像编码器产生的提示视觉特征与特定类原型紧密对齐。此外，我们提出了一个跨模态全局-局部对齐模块，该模块关注与表情相关的面部特征，进一步改善了文本和视觉特征之间的对齐。大量的实验表明，我们的框架在三个FER基准数据集上超越了最先进的方法，同时保留了预训练模型的优势并最小化了计算成本。", "summary": "本文提出了一个名为MPA-FER的多模态提示对齐框架，旨在解决现有VLM在面部表情识别（FER）中难以捕捉细粒度文本-视觉关系的问题。该框架通过LLM生成多粒度硬提示，并将其知识注入软提示中。同时，它引入了原型引导的视觉特征对齐和跨模态全局-局部对齐模块，以提升特征表示的精确度和可解释性。实验证明，MPA-FER在多个FER数据集上优于现有方法，且保持了预训练模型的泛化能力并降低了计算成本。", "keywords": "面部表情识别, 多模态, 提示学习, 视觉-语言模型, CLIP", "comments": "该论文的创新点在于将LLM生成的细粒度语义知识与VLM的提示学习相结合，有效提升了FER任务中细微表情差异的识别能力。通过引入硬提示与软提示的对齐、原型引导的视觉特征对齐以及跨模态全局-局部对齐，系统地解决了现有VLM在细粒度文本-视觉关系捕获上的不足，为多模态FER提供了一个有效且高效的解决方案。"}}
{"id": "2506.21071", "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph", "authors": ["Jingwei Wang", "Zai Zhang", "Hao Qian", "Chunjing Gan", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "Bin Shi", "Bo Dong"], "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs.", "comment": "20 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.21071v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21071v1", "AI": {"title_translation": "利用知识图谱的高质量指令数据增强大型语言模型工具使用", "tldr": "提出利用知识图谱生成高质量指令数据，以显著提升大型语言模型的工具使用能力。", "motivation": "教会大型语言模型（LLMs）使用工具对于提升其解决问题能力和扩展应用至关重要，但由于需要深入理解工具功能和用户意图，有效使用工具具有挑战性。以往依赖LLM生成指令数据的方法质量通常不足。", "method": "提出一种新方法，利用知识图谱生成高质量指令数据。具体步骤包括：从知识图谱中提取查询路径并转化为用户查询；将实体之间的关系转化为可操作的工具；将每个查询的路径解析为详细的解决方案步骤，从而创建高质量指令数据。", "result": "实验表明，仅使用少量这种合成数据进行微调，即可显著提高LLM的工具利用率和整体能力。", "conclusion": "利用知识图谱生成高质量指令数据是一种有效增强大型语言模型工具使用和整体能力的方法。", "translation": "教大型语言模型（LLMs）使用工具对于提升其解决问题能力和扩展应用至关重要。然而，有效使用工具具有挑战性，因为它需要对工具功能和用户意图有深入的理解。以往的方法主要依赖LLM生成指令数据，但这些数据的质量往往不足。在本文中，我们提出了一种新方法，利用知识图谱生成LLM所需的高质量指令数据。知识图谱是人工策划的、富含语义信息的。我们首先从给定的知识图谱中提取各种查询路径，这些路径被转化为广泛的用户查询。然后，我们将实体之间的关系转化为可操作的工具，并将每个查询的路径解析为详细的解决方案步骤，从而创建高质量的指令数据。我们的实验表明，仅使用少量这种合成数据进行微调，即可显著提高LLM的工具利用率和整体能力。", "summary": "本论文提出了一种利用知识图谱生成高质量指令数据的新方法，旨在解决大型语言模型（LLMs）在工具使用方面面临的挑战。通过从知识图谱中提取并转化查询路径为用户查询，将实体关系转化为工具，并解析详细解决方案步骤，该方法能够创建优质的训练数据。实验结果表明，仅使用少量这种合成数据进行微调，即可显著提升LLMs的工具使用能力和整体性能。", "keywords": "大型语言模型, 工具使用, 知识图谱, 指令数据, 微调", "comments": "该论文的创新之处在于利用知识图谱结构化和语义丰富的特性来生成高质量的指令数据，克服了以往依赖LLM生成数据质量不足的局限性。这为提升LLM的实际问题解决能力提供了一个更可靠的途径，具有重要的意义。"}}
{"id": "2506.21018", "title": "LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection", "authors": ["Lei Hao", "Lina Xu", "Chang Liu", "Yanni Dong"], "summary": "Effective deep feature extraction via feature-level fusion is crucial for\nmultimodal object detection. However, previous studies often involve complex\ntraining processes that integrate modality-specific features by stacking\nmultiple feature-level fusion units, leading to significant computational\noverhead. To address this issue, we propose a new fusion detection baseline\nthat uses a single feature-level fusion unit to enable high-performance\ndetection, thereby simplifying the training process. Based on this approach, we\npropose a lightweight attention-guided self-modulation feature fusion network\n(LASFNet), which introduces a novel attention-guided self-modulation feature\nfusion (ASFF) module that adaptively adjusts the responses of fusion features\nat both global and local levels based on attention information from different\nmodalities, thereby promoting comprehensive and enriched feature generation.\nAdditionally, a lightweight feature attention transformation module (FATM) is\ndesigned at the neck of LASFNet to enhance the focus on fused features and\nminimize information loss. Extensive experiments on three representative\ndatasets demonstrate that, compared to state-of-the-art methods, our approach\nachieves a favorable efficiency-accuracy trade-off, reducing the number of\nparameters and computational cost by as much as 90% and 85%, respectively,\nwhile improving detection accuracy (mAP) by 1%-3%. The code will be\nopen-sourced at https://github.com/leileilei2000/LASFNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21018v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21018v1", "AI": {"title_translation": "LASFNet：一种用于多模态目标检测的轻量级注意力引导自调制特征融合网络", "tldr": "LASFNet提出了一种轻量级单特征融合单元，通过注意力引导的自调制和特征注意力转换模块，显著降低多模态目标检测的计算成本和参数量，同时提高检测精度。", "motivation": "现有多模态目标检测方法通过堆叠多个特征级融合单元，导致训练过程复杂和计算开销大。", "method": "提出LASFNet，一个基于单特征级融合单元的基线。LASFNet包含：新颖的注意力引导自调制特征融合（ASFF）模块，根据不同模态的注意力信息，自适应调整融合特征的全局和局部响应；轻量级特征注意力转换模块（FATM），在网络颈部增强对融合特征的关注并最小化信息损失。", "result": "在三个代表性数据集上，与SOTA方法相比，参数量和计算成本分别减少高达90%和85%，同时检测精度（mAP）提高1%-3%，实现了良好的效率-精度权衡。", "conclusion": "LASFNet通过其轻量级设计和创新的融合模块，显著提高了多模态目标检测的效率和精度，解决了现有方法的计算开销问题。", "translation": "通过特征级融合进行有效的深度特征提取对于多模态目标检测至关重要。然而，以往的研究通常涉及复杂的训练过程，通过堆叠多个特征级融合单元来整合模态特定特征，导致显著的计算开销。为了解决这个问题，我们提出了一种新的融合检测基线，该基线使用单个特征级融合单元来实现高性能检测，从而简化了训练过程。基于这种方法，我们提出了一种轻量级注意力引导自调制特征融合网络（LASFNet），它引入了一种新颖的注意力引导自调制特征融合（ASFF）模块，该模块根据来自不同模态的注意力信息，自适应地调整融合特征的全局和局部响应，从而促进全面和丰富的特征生成。此外，在LASFNet的颈部设计了一个轻量级特征注意力转换模块（FATM），以增强对融合特征的关注并最大程度地减少信息损失。在三个代表性数据集上进行的大量实验表明，与最先进的方法相比，我们的方法实现了良好的效率-精度权衡，将参数数量和计算成本分别降低了90%和85%，同时将检测精度（mAP）提高了1%-3%。代码将在https://github.com/leileilei2000/LASFNet开源。", "summary": "本文针对多模态目标检测中现有特征融合方法计算开销大的问题，提出了一种轻量级注意力引导自调制特征融合网络（LASFNet）。该网络采用单一特征级融合单元，并引入了注意力引导自调制特征融合（ASFF）模块和轻量级特征注意力转换（FATM）模块，以自适应调整融合特征并减少信息损失。实验证明，LASFNet在显著降低计算成本和参数量的同时，有效提升了检测精度。", "keywords": "多模态目标检测, 特征融合, 注意力机制, 轻量级网络, 自调制", "comments": "LASFNet的创新点在于其轻量化设计和高效的特征融合策略。通过引入单特征级融合单元、ASFF模块和FATM，它有效地解决了多模态目标检测中计算效率低下的问题，并在精度上有所提升，展现了良好的实用价值和潜在的广泛应用前景。"}}
{"id": "2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21545v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21545v1", "AI": {"title_translation": "语言模型训练中的数据效能", "tldr": "本文引入了“数据效能”概念，旨在优化语言模型训练数据的组织方式。提出了DELT范式，包含数据评分（如LQS）和数据排序（如FO），实验证明其在不增加数据或模型规模的情况下显著提升了LM性能。", "motivation": "当前语言模型训练研究主要集中在“数据效率”（选择最优数据子集）上，而“数据效能”（优化训练数据组织）这一方向相对未被充分探索。本文旨在通过探索数据效能来补充数据效率，从而最大化模型性能。", "method": "本文定义了数据效能，即通过优化训练数据组织来最大化性能。提出了一种通用范式DELT，包含数据评分、数据选择和数据排序三个组件。具体设计了可学习性-质量评分（LQS）作为数据评分的新实例，从梯度一致性角度考虑数据样本的可学习性和质量；设计了折叠排序（FO）作为数据排序的新颖实例，以解决模型遗忘和数据分布偏差问题。", "result": "实验验证了语言模型训练中的数据效能：1. 所提出的DELT的各种实例在不增加数据规模和模型大小的情况下，不同程度地提升了LM性能。2. 其中，LQS用于数据评分和Folding用于数据排序的组合实现了最显著的改进。3. 数据效能可以通过应用数据选择与数据效率协同实现。", "conclusion": "数据效能是语言模型训练中一个有前景的基础领域。", "translation": "数据是语言模型（LM）训练的基础。最近的研究致力于数据效率，旨在通过选择最小或最优的训练数据子集来最大化性能。数据过滤、采样和选择等技术在此领域发挥着关键作用。为了补充这一点，我们定义了数据效能，它侧重于通过优化训练数据的组织来最大化性能，并且相对未被充分探索。这项工作引入了一种通用的范式DELT，用于在LM训练中考虑数据效能，强调了训练数据组织的重要性。DELT包含三个组件：数据评分、数据选择和数据排序。在这些组件中，我们设计了可学习性-质量评分（LQS）作为数据评分的一个新实例，它从梯度一致性的角度考虑了每个数据样本的可学习性和质量。我们还设计了折叠排序（FO）作为数据排序的一个新颖实例，它解决了模型遗忘和数据分布偏差等问题。全面的实验验证了LM训练中的数据效能，表明：首先，所提出的DELT的各种实例在不增加数据规模和模型大小的情况下，不同程度地提升了LM性能。其次，在这些实例中，我们提出的用于数据评分的LQS和用于数据排序的折叠（Folding）的组合实现了最显著的改进。最后，通过应用数据选择，数据效能可以与数据效率一起实现。因此，我们认为数据效能是LM训练中一个有前景的基础领域。", "summary": "本文引入了“数据效能”这一新概念，旨在通过优化语言模型训练数据的组织方式来最大化性能，以补充现有的“数据效率”研究。作者提出了DELT通用范式，包含数据评分、数据选择和数据排序三个核心组件。特别地，论文设计了可学习性-质量评分（LQS）和折叠排序（FO），分别用于评估数据样本的学与质量以及解决模型遗忘和数据分布偏差。实验结果表明，DELT，尤其是LQS和FO的结合，能在不增加数据或模型规模的前提下显著提升LM性能，突显了数据效能作为LM训练中一个基础且有前景的研究方向。", "keywords": "数据效能, 语言模型训练, 数据组织, DELT, 可学习性-质量评分, 折叠排序", "comments": "这篇论文提出了一个新颖且重要的视角——“数据效能”，与现有专注于“数据效率”的研究形成互补。通过强调训练数据的组织而非仅仅选择，它为提高语言模型性能开辟了新的途径。所提出的DELT范式及其LQS和FO方法提供了实现这一目标的具体方案，并在不增加计算资源的情况下显示出显著的性能提升，这体现了其关键创新性。"}}
{"id": "2506.21022", "title": "Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation", "authors": ["Ze Wang", "Hao Chen", "Benran Hu", "Jiang Liu", "Ximeng Sun", "Jialian Wu", "Yusheng Su", "Xiaodong Yu", "Emad Barsoum", "Zicheng Liu"], "summary": "Image tokenization plays a critical role in reducing the computational\ndemands of modeling high-resolution images, significantly improving the\nefficiency of image and multimodal understanding and generation. Recent\nadvances in 1D latent spaces have reduced the number of tokens required by\neliminating the need for a 2D grid structure. In this paper, we further advance\ncompact discrete image representation by introducing 1D binary image latents.\nBy representing each image as a sequence of binary vectors, rather than using\ntraditional one-hot codebook tokens, our approach preserves high-resolution\ndetails while maintaining the compactness of 1D latents. To the best of our\nknowledge, our text-to-image models are the first to achieve competitive\nperformance in both diffusion and auto-regressive generation using just 128\ndiscrete tokens for images up to 1024x1024, demonstrating up to a 32-fold\nreduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary\nlatent space, coupled with simple model architectures, achieves marked\nimprovements in speed training and inference speed. Our text-to-image models\nallow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X\nGPUs, and the training can be completed within 200 GPU days. Our models achieve\ncompetitive performance compared to modern image generation models without any\nin-house private training data or post-training refinements, offering a\nscalable and efficient alternative to conventional tokenization methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21022v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21022v1", "AI": {"title_translation": "Instella-T2I: 突破一维离散潜在空间图像生成的极限", "tldr": "Instella-T2I引入一维二值图像潜在空间，用极少离散token实现高分辨率图像生成，大幅提升效率并达到SOTA性能。", "motivation": "图像标记化在降低高分辨率图像建模的计算需求方面起着关键作用，而现有方法仍有改进空间以进一步减少token数量并提升效率。", "method": "引入一维二值图像潜在空间，将每张图像表示为二值向量序列，而非传统one-hot码本token，从而在保持高分辨率细节的同时，维持一维潜在空间的紧凑性。", "result": "首次使用128个离散token实现高达1024x1024图像的扩散和自回归生成，达到具有竞争力的性能；与标准VQ-VAE相比，token数量减少高达32倍；显著提升了训练速度和推理速度；文本到图像模型可以在单个GPU节点（8个AMD MI300X GPU）上实现4096的全局批处理大小；训练可在200 GPU天内完成；在没有任何内部私有训练数据或后期训练优化的情况下，性能与现代图像生成模型相当。", "conclusion": "Instella-T2I提出的1D二值潜在空间结合简单模型架构，为传统tokenization方法提供了一种可扩展且高效的替代方案，在减少token数量和提高效率方面取得了显著进展，同时保持了竞争力。", "translation": "图像标记化在降低高分辨率图像建模的计算需求方面起着关键作用，显著提高了图像和多模态理解和生成的效率。一维潜在空间的最新进展通过消除对二维网格结构的需求，减少了所需的标记数量。在本文中，我们通过引入一维二值图像潜在空间，进一步推进了紧凑离散图像表示。通过将每张图像表示为二值向量序列，而不是使用传统的one-hot码本标记，我们的方法在保持一维潜在空间紧凑性的同时，保留了高分辨率细节。据我们所知，我们的文本到图像模型是第一个仅使用128个离散标记就能生成高达1024x1024图像并实现在扩散和自回归生成方面具有竞争力的性能的模型，与标准VQ-VAE相比，标记数量减少了高达32倍。所提出的一维二值潜在空间，结合简单的模型架构，显著提高了训练速度和推理速度。我们的文本到图像模型可以在单个GPU节点（配备8个AMD MI300X GPU）上实现4096的全局批处理大小，并且训练可以在200 GPU天内完成。我们的模型在没有任何内部私有训练数据或后期训练优化的情况下，与现代图像生成模型相比取得了有竞争力的性能，为传统的标记化方法提供了一种可扩展且高效的替代方案。", "summary": "本文提出了Instella-T2I，一种基于一维二值图像潜在空间的新型图像生成方法，旨在大幅减少高分辨率图像建模所需的token数量。通过将图像表示为二值向量序列，该方法在保持图像细节的同时，实现了仅用128个token生成1024x1024图像，相比传统VQ-VAE减少了32倍token。此外，该方法显著提升了训练和推理速度，并在不依赖私有数据或后期优化的前提下，达到了与现有先进模型相当的性能，为高效图像生成提供了可扩展的替代方案。", "keywords": "图像生成, 1D潜在空间, 二值表示, 文本到图像, 高效标记化", "comments": "本文的创新点在于引入了一维二值图像潜在空间，有效地将高分辨率图像表示为极少的离散token，这在计算效率上是一个重大突破。其重要性在于，在不牺牲性能的前提下，显著降低了图像生成模型的资源消耗，使得更大规模的模型训练和部署成为可能，尤其是在硬件资源有限或追求极致效率的场景下。"}}
{"id": "2506.21095", "title": "FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation", "authors": ["Xenia Heilmann", "Luca Corbucci", "Mattia Cerrato", "Anna Monreale"], "summary": "Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing clients' private data. However, fairness remains a key\nconcern, as biases in local clients' datasets can impact the entire federated\nsystem. Heterogeneous data distributions across clients may lead to models that\nare fairer for some clients than others. Although several fairness-enhancing\nsolutions are present in the literature, most focus on mitigating bias for a\nsingle sensitive attribute, typically binary, overlooking the diverse and\nsometimes conflicting fairness needs of different clients. This limited\nperspective can limit the effectiveness of fairness interventions for the\ndifferent clients. To support more robust and reproducible fairness research in\nFL, we aim to enable a consistent benchmarking of fairness-aware FL methods at\nboth the global and client levels. In this paper, we contribute in three ways:\n(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to\nevaluating fair FL methods under heterogeneous client bias; (2) we release four\nbias-heterogeneous datasets and corresponding benchmarks to compare fairness\nmitigation methods in a controlled environment; (3) we provide ready-to-use\nfunctions for evaluating fairness outcomes for these datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21095v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21095v1", "AI": {"title_translation": "FeDa4Fair：用于公平性评估的客户端级联邦数据集", "tldr": "本文介绍了FeDa4Fair，一个用于生成和评估联邦学习中具有客户端异构偏差的公平性数据集的库，旨在促进更稳健和可复现的公平性研究。", "motivation": "联邦学习中存在公平性问题，因为客户端本地数据集的偏差和数据分布的异构性可能导致模型对不同客户端的公平性表现不一。现有的公平性增强方案通常只关注单个敏感属性，忽略了不同客户端多样化且有时相互冲突的公平性需求。为了支持更稳健和可复现的联邦学习公平性研究，并实现全局和客户端级别公平性感知联邦学习方法的一致基准测试。", "method": "本文贡献了三点：1) 引入了FeDa4Fair，一个用于生成表格数据集的库，专门用于评估异构客户端偏差下的公平联邦学习方法；2) 发布了四个具有偏差异构性的数据集以及相应的基准，以便在受控环境中比较公平性缓解方法；3) 提供了用于评估这些数据集公平性结果的即用型函数。", "result": "本文引入了FeDa4Fair库，能够生成用于评估公平联邦学习方法在异构客户端偏差下表现的表格数据集。同时，发布了四个具有偏差异构性的数据集及其基准，为受控环境下比较公平性缓解方法提供了资源。此外，还提供了评估这些数据集公平性结果的即用型函数。", "conclusion": "本文通过引入FeDa4Fair库、发布偏差异构数据集和基准，以及提供评估函数，为联邦学习中公平性研究的基准测试和评估提供了重要工具，有助于推动该领域更稳健和可复现的研究。", "translation": "联邦学习（FL）实现了跨多个客户端的协作模型训练，而无需共享客户端的私有数据。然而，公平性仍然是一个关键问题，因为本地客户端数据集中存在的偏差可能会影响整个联邦系统。客户端之间异构的数据分布可能导致模型对某些客户端比对其他客户端更公平。尽管现有文献中存在一些增强公平性的解决方案，但大多数都集中于缓解单个敏感属性（通常是二进制）的偏差，而忽略了不同客户端多样化且有时相互冲突的公平性需求。这种有限的视角可能会限制公平性干预对不同客户端的有效性。为了支持联邦学习中更稳健和可复现的公平性研究，我们的目标是实现全局和客户端级别公平性感知联邦学习方法的一致基准测试。在本文中，我们通过三种方式做出了贡献：(1) 我们引入了FeDa4Fair，一个用于生成表格数据集的库，专门用于评估异构客户端偏差下的公平联邦学习方法；(2) 我们发布了四个具有偏差异构性的数据集和相应的基准，以便在受控环境中比较公平性缓解方法；(3) 我们提供了用于评估这些数据集公平性结果的即用型函数。", "summary": "本文介绍了FeDa4Fair，一个专门为联邦学习（FL）中公平性评估设计的库。鉴于FL中客户端数据异构性和偏差导致的公平性挑战以及现有解决方案的局限性，FeDa4Fair旨在通过生成具有异构客户端偏差的表格数据集来支持更稳健和可复现的公平性研究。该工作还发布了四个此类数据集及其基准，并提供了评估公平性结果的实用函数，为全局和客户端级别的公平性感知FL方法提供了一致的基准测试环境。", "keywords": "联邦学习, 公平性, 数据集, 偏差, 基准测试", "comments": "这项工作通过提供专门的数据集和评估工具，解决了联邦学习中公平性研究的一个关键痛点。其创新之处在于关注客户端层面的异构偏差，并提供了一个可复用、标准化的基准测试框架，这对于推动联邦学习领域公平性研究的进展至关重要。这有助于研究人员在更真实和受控的环境中比较和开发公平性缓解方法。"}}
{"id": "2506.21034", "title": "DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation", "authors": ["Wenzhou Lyu", "Jialing Lin", "Wenqi Ren", "Ruihao Xia", "Feng Qian", "Yang Tang"], "summary": "Commercial RGB-D cameras often produce noisy, incomplete depth maps for\nnon-Lambertian objects. Traditional depth completion methods struggle to\ngeneralize due to the limited diversity and scale of training data. Recent\nadvances exploit visual priors from pre-trained text-to-image diffusion models\nto enhance generalization in dense prediction tasks. However, we find that\nbiases arising from training-inference mismatches in the vanilla diffusion\nframework significantly impair depth completion performance. Additionally, the\nlack of distinct visual features in non-Lambertian regions further hinders\nprecise prediction. To address these issues, we propose \\textbf{DidSee}, a\ndiffusion-based framework for depth completion on non-Lambertian objects.\nFirst, we integrate a rescaled noise scheduler enforcing a zero terminal\nsignal-to-noise ratio to eliminate signal leakage bias. Second, we devise a\nnoise-agnostic single-step training formulation to alleviate error accumulation\ncaused by exposure bias and optimize the model with a task-specific loss.\nFinally, we incorporate a semantic enhancer that enables joint depth completion\nand semantic segmentation, distinguishing objects from backgrounds and yielding\nprecise, fine-grained depth maps. DidSee achieves state-of-the-art performance\non multiple benchmarks, demonstrates robust real-world generalization, and\neffectively improves downstream tasks such as category-level pose estimation\nand robotic grasping.Project page: https://wenzhoulyu.github.io/DidSee/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21034v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21034v1", "AI": {"title_translation": "DidSee：基于扩散的深度补全，用于材料无关的机器人感知和操作", "tldr": "DidSee 是一种新的基于扩散的深度补全框架，解决了传统方法在非朗伯物体上的不足，并通过引入新的噪声调度器、单步训练和语义增强器，实现了SOTA性能并改进了机器人任务。", "motivation": "商用RGB-D相机在非朗伯物体上生成噪声大、不完整的深度图。传统深度补全方法因训练数据有限而泛化能力差。尽管扩散模型有所进步，但香草扩散框架中的训练-推理不匹配偏差和非朗伯区域缺乏视觉特征进一步阻碍了精确预测。", "method": "提出了DidSee，一个基于扩散的深度补全框架，用于非朗伯物体。主要方法包括：1. 集成一个重新缩放的噪声调度器，强制零终端信噪比以消除信号泄漏偏差。2. 设计一个噪声无关的单步训练公式，以减轻暴露偏差引起的错误累积，并使用任务特定损失优化模型。3. 结合一个语义增强器，实现联合深度补全和语义分割，区分物体和背景，生成精确、细粒度的深度图。", "result": "DidSee 在多个基准测试中取得了最先进的性能，展示了强大的真实世界泛化能力，并有效改进了下游任务，如类别级姿态估计和机器人抓取。", "conclusion": "DidSee 通过创新的扩散模型改进和语义增强，有效解决了非朗伯物体深度补全的挑战，实现了卓越的性能和实际应用价值。", "translation": "商用RGB-D相机在非朗伯物体上通常会产生嘈杂、不完整的深度图。传统的深度补全方法由于训练数据的多样性和规模有限，难以泛化。最近的进展利用预训练文本到图像扩散模型的视觉先验来增强密集预测任务的泛化能力。然而，我们发现香草扩散框架中训练-推理不匹配引起的偏差会显著损害深度补全性能。此外，非朗伯区域缺乏独特的视觉特征进一步阻碍了精确预测。为了解决这些问题，我们提出了\\textbf{DidSee}，一个基于扩散的深度补全框架，用于非朗伯物体。首先，我们集成了一个重新缩放的噪声调度器，强制零终端信噪比以消除信号泄漏偏差。其次，我们设计了一个噪声无关的单步训练公式，以减轻暴露偏差引起的错误累积，并使用任务特定损失优化模型。最后，我们结合了一个语义增强器，实现了联合深度补全和语义分割，区分物体和背景，生成精确、细粒度的深度图。DidSee 在多个基准测试中取得了最先进的性能，展示了强大的真实世界泛化能力，并有效改进了下游任务，如类别级姿态估计和机器人抓取。项目页面：https://wenzhoulyu.github.io/DidSee/", "summary": "本文提出了DidSee，一个专为非朗伯物体深度补全设计的扩散模型框架。针对现有扩散模型在深度补全中存在的训练-推理偏差和非朗伯区域特征不足问题，DidSee引入了零终端信噪比的噪声调度器以消除信号泄漏，采用噪声无关的单步训练公式以减轻误差累积，并集成了语义增强器以实现联合深度补全与语义分割。实验证明，DidSee在多项基准测试中达到SOTA性能，并显著提升了机器人抓取等下游任务的效果。", "keywords": "深度补全, 扩散模型, 非朗伯物体, 机器人感知, 语义分割", "comments": "DidSee的创新之处在于其对扩散模型在深度补全任务中遇到的特定问题（如训练-推理偏差和非朗伯物体挑战）进行了深入分析并提出了针对性解决方案。通过结合噪声调度优化、单步训练策略和语义增强，该方法不仅提升了深度图的精度和泛化能力，还证明了其在实际机器人应用中的价值，对于提升机器人对复杂材质物体的感知能力具有重要意义。"}}
{"id": "2506.21102", "title": "Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning", "authors": ["David Debot", "Pietro Barbiero", "Gabriele Dominici", "Giuseppe Marra"], "summary": "Concept-Based Models (CBMs) are a class of deep learning models that provide\ninterpretability by explaining predictions through high-level concepts. These\nmodels first predict concepts and then use them to perform a downstream task.\nHowever, current CBMs offer interpretability only for the final task\nprediction, while the concept predictions themselves are typically made via\nblack-box neural networks. To address this limitation, we propose Hierarchical\nConcept Memory Reasoner (H-CMR), a new CBM that provides interpretability for\nboth concept and task predictions. H-CMR models relationships between concepts\nusing a learned directed acyclic graph, where edges represent logic rules that\ndefine concepts in terms of other concepts. During inference, H-CMR employs a\nneural attention mechanism to select a subset of these rules, which are then\napplied hierarchically to predict all concepts and the final task. Experimental\nresults demonstrate that H-CMR matches state-of-the-art performance while\nenabling strong human interaction through concept and model interventions. The\nformer can significantly improve accuracy at inference time, while the latter\ncan enhance data efficiency during training when background knowledge is\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21102v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21102v1", "AI": {"title_translation": "通过注意力引导图学习实现可解释的层次概念推理", "tldr": "本文提出了一种名为H-CMR的新型概念基模型，它通过学习有向无环图和神经注意力机制，为概念预测和任务预测都提供了可解释性，并达到了与现有技术相当的性能，同时支持强人机交互。", "motivation": "现有概念基模型（CBMs）仅为最终任务预测提供可解释性，而概念预测本身通常通过黑盒神经网络进行，缺乏透明度。本文旨在解决这一局限性，为概念和任务预测都提供可解释性。", "method": "本文提出了一种名为层次概念记忆推理器（H-CMR）的新型概念基模型。H-CMR通过学习一个有向无环图来建模概念之间的关系，其中边代表定义概念的逻辑规则。在推理过程中，H-CMR利用神经注意力机制选择这些规则的一个子集，然后分层应用这些规则来预测所有概念和最终任务。", "result": "实验结果表明，H-CMR在性能上与最先进的模型相当，同时通过概念和模型干预实现了强大的人机交互。概念干预可以在推理时显著提高准确性，而模型干预可以在有背景知识时提高训练期间的数据效率。", "conclusion": "H-CMR通过为概念和任务预测提供可解释性，并支持有效的人机交互，克服了现有概念基模型的局限性，同时保持了最先进的性能。", "translation": "概念基模型（CBMs）是一类深度学习模型，通过高级概念解释预测来提供可解释性。这些模型首先预测概念，然后利用它们执行下游任务。然而，当前的CBMs仅为最终任务预测提供可解释性，而概念预测本身通常通过黑盒神经网络进行。为了解决这一局限性，我们提出了层次概念记忆推理器（H-CMR），这是一种新型CBM，为概念和任务预测都提供了可解释性。H-CMR使用学习到的有向无环图来建模概念之间的关系，其中边表示定义其他概念的逻辑规则。在推理过程中，H-CMR采用神经注意力机制选择这些规则的一个子集，然后分层应用这些规则来预测所有概念和最终任务。实验结果表明，H-CMR在性能上与最先进的技术相当，同时通过概念和模型干预实现了强大的人机交互。前者可以在推理时显著提高准确性，而后者可以在有背景知识时提高训练期间的数据效率。", "summary": "本文提出了一种名为层次概念记忆推理器（H-CMR）的新型概念基模型，旨在解决现有概念基模型在概念预测层面缺乏可解释性的问题。H-CMR通过构建一个学习到的有向无环图来表示概念间的逻辑关系，并结合注意力机制分层应用这些规则进行概念和任务预测，从而实现了对两者的可解释性。实验证明，H-CMR在保持与现有技术相当性能的同时，显著增强了人机交互能力，通过概念和模型干预分别提升了推理准确性和训练数据效率。", "keywords": "概念基模型, 可解释性, 层次推理, 图学习, 注意力机制", "comments": "本文的创新之处在于提出了一种能够同时为概念和最终任务预测提供可解释性的CBM，这解决了现有CBMs的“黑盒”问题。通过引入注意力引导的图学习来建模概念间的层次关系，H-CMR不仅提高了模型的可解释性，还通过人机交互展示了实际应用潜力，例如提高推理准确性和数据效率。这对于可信AI的发展具有重要意义。"}}
{"id": "2506.21042", "title": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "summary": "Detectors often suffer from performance drop due to domain gap between\ntraining and testing data. Recent methods explore diffusion models applied to\ndomain generalization (DG) and adaptation (DA) tasks, but still struggle with\nlarge inference costs and have not yet fully leveraged the capabilities of\ndiffusion models. We propose to tackle these problems by extracting\nintermediate features from a single-step diffusion process, improving feature\ncollection and fusion to reduce inference time by 75% while enhancing\nperformance on source domains (i.e., Fitness). Then, we construct an\nobject-centered auxiliary branch by applying box-masked images with class\nprompts to extract robust and domain-invariant features that focus on object.\nWe also apply consistency loss to align the auxiliary and ordinary branch,\nbalancing fitness and generalization while preventing overfitting and improving\nperformance on target domains (i.e., Generalization). Furthermore, within a\nunified framework, standard detectors are guided by diffusion detectors through\nfeature-level and object-level alignment on source domains (for DG) and\nunlabeled target domains (for DA), thereby improving cross-domain detection\nperformance (i.e., Transferability). Our method achieves competitive results on\n3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO\ngeneralization benchmark demonstrate that our method maintains significant\nadvantages and show remarkable efficiency in large domain shifts and low-data\nscenarios. Our work shows the superiority of applying diffusion models to\ndomain generalized and adaptive detection tasks and offers valuable insights\nfor visual perception tasks across diverse domains. The code is available at\n\\href{https://github.com/heboyong/Fitness-Generalization-Transferability}{Fitness-Generalization-Transferability}.", "comment": "Accepted by ICCV2025. arXiv admin note: text overlap with\n  arXiv:2503.02101", "pdf_url": "http://arxiv.org/pdf/2506.21042v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21042v1", "AI": {"title_translation": "利用扩散模型提升域泛化和自适应检测：适应性、泛化性和可迁移性", "tldr": "本文提出了一种基于扩散模型的新方法，通过提取单步扩散中间特征、构建以对象为中心的辅助分支以及特征和对象级对齐，显著降低了推理成本，同时提高了域泛化和自适应检测的性能。", "motivation": "现有的检测器在训练和测试数据之间存在域间隙时性能会下降。尽管最近的方法将扩散模型应用于域泛化（DG）和域适应（DA）任务，但它们仍面临高昂的推理成本，并且未能充分利用扩散模型的能力。", "method": "本文提出通过从单步扩散过程中提取中间特征来提高特征收集和融合，从而将推理时间减少75%并增强源域性能（适应性）。通过应用带类别提示的框掩蔽图像构建以对象为中心的辅助分支，以提取鲁棒且域不变的特征。同时应用一致性损失来对齐辅助分支和普通分支，平衡适应性和泛化性，防止过拟合。在一个统一的框架内，通过在源域（用于DG）和未标记目标域（用于DA）上进行特征级和对象级对齐，引导标准检测器。", "result": "该方法在3个DA基准和5个DG基准上取得了有竞争力的结果。在COCO泛化基准上的实验表明，该方法在大的域偏移和低数据场景下保持显著优势并显示出卓越的效率。", "conclusion": "本文的工作展示了将扩散模型应用于域泛化和自适应检测任务的优越性，并为跨不同领域的视觉感知任务提供了宝贵的见解。", "translation": "检测器经常由于训练和测试数据之间的域间隙而导致性能下降。最近的方法探索了将扩散模型应用于域泛化（DG）和域适应（DA）任务，但仍然面临高昂的推理成本，并且尚未充分利用扩散模型的能力。我们提出通过从单步扩散过程中提取中间特征来解决这些问题，从而改进特征收集和融合，将推理时间减少75%，同时提高源域性能（即适应性）。然后，我们通过应用带类别提示的框掩蔽图像来构建一个以对象为中心的辅助分支，以提取鲁棒且域不变的特征，这些特征专注于对象。我们还应用一致性损失来对齐辅助分支和普通分支，平衡适应性和泛化性，同时防止过拟合并提高目标域性能（即泛化性）。此外，在一个统一的框架内，标准检测器通过在源域（用于DG）和未标记目标域（用于DA）上的特征级和对象级对齐，由扩散检测器引导，从而提高跨域检测性能（即可迁移性）。我们的方法在3个DA基准和5个DG基准上取得了有竞争力的结果。此外，在COCO泛化基准上的实验表明，我们的方法保持显著优势，并在大域偏移和低数据场景中显示出卓越的效率。我们的工作展示了将扩散模型应用于域泛化和自适应检测任务的优越性，并为跨不同领域的视觉感知任务提供了宝贵的见解。代码可在https://github.com/heboyong/Fitness-Generalization-Transferability 获取。", "summary": "本文提出了一种利用扩散模型解决域泛化（DG）和域适应（DA）中检测器性能下降及推理成本高昂的问题。通过从单步扩散中提取中间特征，显著降低了推理时间并提升了源域性能。引入以对象为中心的辅助分支，结合一致性损失，增强了特征的鲁棒性和域不变性，平衡了泛化性。此外，在一个统一框架内，通过特征级和对象级对齐，引导标准检测器进行跨域检测。实验证明，该方法在多个DG和DA基准上表现出色，尤其在域偏移大和数据量少的情况下效率显著。", "keywords": "扩散模型, 域泛化, 域适应, 目标检测, 跨域", "comments": "该论文的创新点在于有效利用扩散模型，通过提取单步扩散中间特征来大幅降低推理成本，并结合对象中心辅助分支和一致性损失，在统一框架下同时优化了检测器的适应性、泛化性和可迁移性。这种方法不仅提升了性能，还显著提高了效率，为跨域视觉感知任务提供了新的思路和强大的工具。"}}
{"id": "2506.21103", "title": "Learning to Skip the Middle Layers of Transformers", "authors": ["Tim Lawson", "Laurence Aitchison"], "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle.", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21103v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21103v1", "AI": {"title_translation": "学习跳过Transformer的中间层", "tldr": "本文提出了一种动态跳过Transformer中间层的新架构，旨在提高效率，但实验结果显示与更少层的密集基线相比，在验证交叉熵和FLOPs的权衡上没有改进。", "motivation": "现有条件计算方法常针对独立模块或独立跳过层，但解释性研究表明Transformer中间层冗余度更高，早期层聚合信息。受此启发，本文旨在通过跳过中间层来提高Transformer的效率，并减少“更简单”token的计算需求，期望形成多级表示层次。", "method": "提出了一种新的架构，该架构根据输入动态跳过数量可变的中间层。一个学习到的门控机制决定是否绕过对称的中心块跨度，一个门控注意力机制阻止后续token关注被跳过的token位置。残差范数通过“sandwich”或“perilayernorm”方案控制，门控稀疏性通过自适应正则化损失控制。", "result": "在所研究的规模下，与层数更少的密集基线相比，该方法在验证交叉熵和估计FLOPs之间的权衡方面未能实现改进。", "conclusion": "尽管提出了一种新颖的动态跳过Transformer中间层的架构，但在所测试的规模下，该方法未能达到预期的计算效率提升。", "translation": "条件计算是提高Transformer效率的常用策略。现有方法通常针对单个模块（例如，专家混合层）或独立跳过层。然而，可解释性研究表明，Transformer的中间层表现出更大的冗余性，并且早期层将信息聚合到token位置。受这些见解的启发，我们提出了一种新颖的架构，可以动态地从中间向外跳过可变数量的层。特别是，一个学习到的门控机制根据输入决定是否绕过对称的中心块跨度，一个门控注意力机制阻止后续token关注被跳过的token位置。残差范数通过“sandwich”或“perilayernorm”方案控制，门控稀疏性通过自适应正则化损失控制。我们旨在减少“更简单”token的计算需求，并可能促进一种新兴的多级表示层次结构，但在所研究的规模下，与层数更少的密集基线相比，我们的方法在验证交叉熵和估计FLOPs之间的权衡方面未能实现改进。我们已在https://github.com/tim-lawson/skip-middle 发布了我们的代码。", "summary": "本文提出了一种基于Transformer中间层冗余性观察的新型条件计算架构。该架构通过学习到的门控机制动态跳过对称的中间层，并结合门控注意力机制和残差范数控制策略，旨在提高效率并减少简单token的计算量。然而，实验结果显示，在测试规模下，该方法在计算效率与模型性能的权衡上未能超越传统的密集Transformer基线。", "keywords": "Transformer, 条件计算, 层跳过, 模型效率, 门控机制", "comments": "该论文的创新点在于其提出了一种新颖的动态跳过Transformer中间层的方法，这是基于对Transformer中间层冗余性的深入理解。它引入了门控机制和残差范数控制等技术来管理跳层行为。然而，其主要局限性在于，在所研究的规模下，该方法并未能实现预期的计算效率提升，这表明在实际应用中可能需要进一步的优化或在更大规模上进行验证。"}}
{"id": "2506.21045", "title": "Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling", "authors": ["Hansam Cho", "Seoung Bum Kim"], "summary": "Text-guided diffusion models have become essential for high-quality image\nsynthesis, enabling dynamic image editing. In image editing, two crucial\naspects are editability, which determines the extent of modification, and\nfaithfulness, which reflects how well unaltered elements are preserved.\nHowever, achieving optimal results is challenging because of the inherent\ntrade-off between editability and faithfulness. To address this, we propose\nFaithfulness Guidance and Scheduling (FGS), which enhances faithfulness with\nminimal impact on editability. FGS incorporates faithfulness guidance to\nstrengthen the preservation of input image information and introduces a\nscheduling strategy to resolve misalignment between editability and\nfaithfulness. Experimental results demonstrate that FGS achieves superior\nfaithfulness while maintaining editability. Moreover, its compatibility with\nvarious editing methods enables precise, high-quality image edits across\ndiverse tasks.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2506.21045v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21045v1", "AI": {"title_translation": "通过引导和调度提高基于扩散的图像编辑的保真度", "tldr": "提出FGS方法，通过保真度引导和调度策略，在图像编辑中平衡可编辑性和保真度，显著提高保真度。", "motivation": "文本引导扩散模型在图像编辑中面临可编辑性和保真度之间的固有权衡，难以同时实现最佳效果。", "method": "提出Faithfulness Guidance and Scheduling (FGS) 方法。FGS通过引入保真度引导来加强输入图像信息的保存，并采用调度策略解决可编辑性和保真度之间的错位问题。", "result": "实验结果表明，FGS在保持可编辑性的同时实现了卓越的保真度，并且与多种编辑方法兼容，能够实现精确、高质量的图像编辑。", "conclusion": "FGS有效解决了扩散模型图像编辑中可编辑性和保真度之间的权衡问题，显著提升了图像编辑的质量和精确性，并具有广泛的适用性。", "translation": "文本引导扩散模型已成为高质量图像合成的关键，实现了动态图像编辑。在图像编辑中，两个关键方面是可编辑性（决定修改程度）和保真度（反映未改变元素的保留程度）。然而，由于可编辑性和保真度之间固有的权衡，实现最佳结果具有挑战性。为了解决这个问题，我们提出了保真度引导和调度（FGS），它在对可编辑性影响最小的情况下增强保真度。FGS结合了保真度引导以加强输入图像信息的保存，并引入了调度策略以解决可编辑性和保真度之间的错位。实验结果表明，FGS在保持可编辑性的同时实现了卓越的保真度。此外，它与各种编辑方法的兼容性使得能够在不同任务中实现精确、高质量的图像编辑。", "summary": "本文提出了Faithfulness Guidance and Scheduling (FGS) 方法，旨在解决文本引导扩散模型在图像编辑中可编辑性与保真度之间的权衡问题。FGS通过引入保真度引导来增强对原始图像信息的保留，并采用调度策略来协调可编辑性和保真度。实验证明，FGS在保持良好可编辑性的同时显著提升了图像编辑的保真度，并且能够与多种编辑方法兼容，实现高质量的图像编辑。", "keywords": "扩散模型, 图像编辑, 保真度, 可编辑性, 引导和调度", "comments": "该论文提出了一种新颖的方法FGS，通过结合保真度引导和调度策略，有效地解决了扩散模型图像编辑中长期存在的保真度与可编辑性之间的矛盾。其创新性在于精细化地平衡了这两个关键指标，使得图像编辑既能实现大幅修改又能保留原始细节。方法的兼容性也增加了其实用价值。"}}
{"id": "2506.21107", "title": "Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges", "authors": ["Changxi Chi", "Jun Xia", "Yufei Huang", "Jingbo Zhou", "Siyuan Li", "Yunfan Liu", "Chang Yu", "Stan Z. Li"], "summary": "Estimating single-cell responses across various perturbations facilitates the\nidentification of key genes and enhances drug screening, significantly boosting\nexperimental efficiency. However, single-cell sequencing is a destructive\nprocess, making it impossible to capture the same cell's phenotype before and\nafter perturbation. Consequently, data collected under perturbed and\nunperturbed conditions are inherently unpaired. Existing methods either attempt\nto forcibly pair unpaired data using random sampling, or neglect the inherent\nrelationship between unperturbed and perturbed cells during the modeling. In\nthis work, we propose a framework based on Dual Diffusion Implicit Bridges\n(DDIB) to learn the mapping between different data distributions, effectively\naddressing the challenge of unpaired data. We further interpret this framework\nas a form of data augmentation. We integrate gene regulatory network (GRN)\ninformation to propagate perturbation signals in a biologically meaningful way,\nand further incorporate a masking mechanism to predict silent genes, improving\nthe quality of generated profiles. Moreover, gene expression under the same\nperturbation often varies significantly across cells, frequently exhibiting a\nbimodal distribution that reflects intrinsic heterogeneity. To capture this, we\nintroduce a more suitable evaluation metric. We propose Unlasting, dual\nconditional diffusion models that overcome the problem of unpaired single-cell\nperturbation data and strengthen the model's insight into perturbations under\nthe guidance of the GRN, with a dedicated mask model designed to improve\ngeneration quality by predicting silent genes. In addition, we introduce a\nbiologically grounded evaluation metric that better reflects the inherent\nheterogeneity in single-cell responses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21107v1", "categories": ["cs.LG", "q-bio.MN"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21107v1", "AI": {"title_translation": "Unlasting: 通过双条件扩散隐式桥接进行非配对单细胞多扰动估计", "tldr": "Unlasting使用双条件扩散模型解决单细胞多扰动数据非配对问题，并通过整合基因调控网络和掩码机制提高估计质量和异质性捕捉能力。", "motivation": "估计单细胞对各种扰动的反应对于识别关键基因和药物筛选至关重要，能显著提高实验效率。然而，单细胞测序的破坏性导致无法捕获同一细胞扰动前后表型，使得数据天生非配对。现有方法要么强制配对，要么忽略细胞间的内在关系。", "method": "本文提出了一个基于双扩散隐式桥接（DDIB）的框架Unlasting，用于学习不同数据分布间的映射，以解决非配对数据问题。该框架被解释为一种数据增强形式。它整合了基因调控网络（GRN）信息以生物学上有意义的方式传播扰动信号，并引入掩码机制来预测沉默基因。此外，为了捕捉单细胞响应的内在异质性，还引入了一个更合适的评估指标。", "result": "Unlasting成功克服了非配对单细胞扰动数据的问题，并在基因调控网络的指导下增强了模型对扰动的洞察力。通过专门设计的掩码模型预测沉默基因，提高了生成谱的质量。引入的生物学评估指标能更好地反映单细胞响应中固有的异质性。", "conclusion": "Unlasting通过其双条件扩散模型、基因调控网络整合、掩码机制和新的评估指标，有效地解决了单细胞多扰动数据非配对的挑战，并提高了扰动响应估计的准确性和生物学相关性。", "translation": "估计单细胞对各种扰动的反应有助于识别关键基因和加强药物筛选，显著提高实验效率。然而，单细胞测序是一个破坏性过程，使得在扰动前后捕获同一细胞的表型变得不可能。因此，在扰动和未扰动条件下收集的数据本质上是未配对的。现有方法要么尝试使用随机抽样强制配对未配对数据，要么在建模过程中忽略未扰动和扰动细胞之间的内在关系。在这项工作中，我们提出了一个基于双扩散隐式桥接（DDIB）的框架，以学习不同数据分布之间的映射，有效解决未配对数据的挑战。我们进一步将此框架解释为一种数据增强形式。我们整合了基因调控网络（GRN）信息，以生物学上有意义的方式传播扰动信号，并进一步结合掩码机制来预测沉默基因，从而提高生成谱的质量。此外，在相同扰动下，基因表达在不同细胞间往往差异显著，经常表现出反映内在异质性的双峰分布。为了捕捉这一点，我们引入了一个更合适的评估指标。我们提出了Unlasting，这是一种双条件扩散模型，它克服了单细胞扰动数据未配对的问题，并在GRN的指导下加强了模型对扰动的洞察力，同时设计了一个专门的掩码模型，通过预测沉默基因来提高生成质量。此外，我们引入了一个基于生物学原理的评估指标，可以更好地反映单细胞响应中固有的异质性。", "summary": "本文提出了Unlasting，一个基于双条件扩散隐式桥接（DDIB）的框架，旨在解决单细胞多扰动数据中固有的非配对问题。Unlasting通过学习不同数据分布之间的映射来估计单细胞对各种扰动的反应。该方法整合了基因调控网络（GRN）信息以生物学方式传播扰动信号，并引入掩码机制预测沉默基因，从而提高生成数据的质量。此外，为了更好地捕捉单细胞响应的内在异质性，Unlasting还引入了一个新的、更合适的评估指标。", "keywords": "单细胞扰动, 非配对数据, 扩散模型, 基因调控网络, 数据增强", "comments": "Unlasting的创新点在于利用双条件扩散模型有效处理单细胞扰动数据固有的非配对问题，这在现有方法中是一个重大挑战。通过整合基因调控网络和引入掩码机制，它不仅提高了估计的生物学合理性，也提升了生成数据的质量。引入新的评估指标以捕捉细胞间异质性，也显示了对生物学复杂性的深入理解。该框架有望提高单细胞药物筛选和关键基因识别的效率和准确性。"}}
{"id": "2506.21127", "title": "Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "summary": "The increasing automation of navigation for unmanned aerial vehicles (UAVs)\nhas exposed them to adversarial attacks that exploit vulnerabilities in\nreinforcement learning (RL) through sensor manipulation. Although existing\nrobust RL methods aim to mitigate such threats, their effectiveness has limited\ngeneralization to out-of-distribution shifts from the optimal value\ndistribution, as they are primarily designed to handle fixed perturbation. To\naddress this limitation, this paper introduces an antifragile RL framework that\nenhances adaptability to broader distributional shifts by incorporating a\nswitching mechanism based on discounted Thompson sampling (DTS). This mechanism\ndynamically selects among multiple robust policies to minimize adversarially\ninduced state-action-value distribution shifts. The proposed approach first\nderives a diverse ensemble of action robust policies by accounting for a range\nof perturbations in the policy space. These policies are then modeled as a\nmultiarmed bandit (MAB) problem, where DTS optimally selects policies in\nresponse to nonstationary Bernoulli rewards, effectively adapting to evolving\nadversarial strategies. Theoretical framework has also been provided where by\noptimizing the DTS to minimize the overall regrets due to distributional shift,\nresults in effective adaptation against unseen adversarial attacks thus\ninducing antifragility. Extensive numerical simulations validate the\neffectiveness of the proposed framework in complex navigation environments with\nmultiple dynamic three-dimensional obstacles and with stronger projected\ngradient descent (PGD) and spoofing attacks. Compared to conventional robust,\nnon-adaptive RL methods, the antifragile approach achieves superior\nperformance, demonstrating shorter navigation path lengths and a higher rate of\nconflict-free navigation trajectories compared to existing robust RL techniques", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21127v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21127v1", "AI": {"title_translation": "应对对抗性环境中无人机冲突消除的抗脆弱强化学习的鲁棒策略切换", "tldr": "提出了一种基于折扣汤普森采样的鲁棒策略切换机制，用于在对抗性环境中增强无人机导航的抗脆弱强化学习，以适应更广泛的分布变化。", "motivation": "现有的鲁棒强化学习方法在应对固定扰动时有效，但对最佳价值分布之外的分布变化泛化能力有限，无法有效处理传感器操纵等对抗性攻击导致的漏洞。", "method": "本文引入了一个抗脆弱强化学习框架，通过结合基于折扣汤普森采样（DTS）的切换机制来增强对更广泛分布变化的适应性。该机制动态选择多个鲁棒策略以最小化对抗性引起的状态-动作-价值分布变化。首先，通过考虑策略空间中的一系列扰动，推导出一组多样化的动作鲁棒策略。然后，将这些策略建模为多臂老虎机（MAB）问题，其中DTS根据非平稳伯努利奖励最优地选择策略，从而有效地适应不断演变的对抗性策略。还提供了理论框架，通过优化DTS以最小化由于分布变化引起的总遗憾，从而实现对未知对抗性攻击的有效适应。", "result": "广泛的数值模拟验证了所提出的框架在具有多个动态三维障碍物和更强的PGD（投影梯度下降）和欺骗攻击的复杂导航环境中的有效性。与传统的鲁棒、非自适应强化学习方法相比，抗脆弱方法表现出卓越的性能，导航路径长度更短，无冲突导航轨迹率更高。", "conclusion": "论文提出的抗脆弱强化学习框架，通过动态策略切换机制，能够有效应对复杂对抗环境中的分布变化和未知攻击，显著提升了无人机导航的鲁棒性和安全性。", "translation": "无人机（UAV）导航自动化程度的提高，使其暴露于利用传感器操纵来利用强化学习（RL）漏洞的对抗性攻击。尽管现有的鲁棒RL方法旨在缓解此类威胁，但它们的有效性对来自最优价值分布的分布外偏移的泛化能力有限，因为它们主要设计用于处理固定扰动。为了解决这一限制，本文引入了一个抗脆弱RL框架，通过结合基于折扣汤普森采样（DTS）的切换机制来增强对更广泛分布变化的适应性。该机制动态选择多个鲁棒策略，以最小化对抗性引起的状态-动作-价值分布偏移。所提出的方法首先通过考虑策略空间中的一系列扰动，推导出一组多样化的动作鲁棒策略。然后，将这些策略建模为多臂老虎机（MAB）问题，其中DTS根据非平稳伯努利奖励最优地选择策略，有效地适应不断演变的对抗性策略。还提供了理论框架，通过优化DTS以最小化由于分布偏移引起的总遗憾，从而实现对未知对抗性攻击的有效适应，从而产生抗脆弱性。广泛的数值模拟验证了所提出的框架在具有多个动态三维障碍物和更强的投影梯度下降（PGD）和欺骗攻击的复杂导航环境中的有效性。与传统的鲁棒、非自适应RL方法相比，抗脆弱方法实现了卓越的性能，与现有鲁棒RL技术相比，展示了更短的导航路径长度和更高的无冲突导航轨迹率。", "summary": "本文提出了一种针对无人机在对抗性环境中导航的抗脆弱强化学习框架。该框架通过引入基于折扣汤普森采样的动态策略切换机制，克服了传统鲁棒RL方法在处理分布外偏移时的局限性。它通过将多样化的鲁棒策略建模为多臂老虎机问题，并利用DTS进行最优选择，从而有效地适应不断演变的对抗性策略和未知攻击。实验结果表明，该方法在复杂导航环境中表现出优于现有方法的性能，实现了更短的导航路径和更高的无冲突率。", "keywords": "抗脆弱强化学习, 策略切换, 无人机冲突消除, 对抗性环境, 折扣汤普森采样", "comments": "这篇论文的创新点在于将抗脆弱性概念引入强化学习，并通过动态策略切换机制来应对对抗性环境中的未知和变化的扰动。它将策略选择问题转化为多臂老虎机问题，并利用折扣汤普森采样进行优化，这提供了一个新颖的适应性框架。这项工作对于提高无人机在复杂、不确定环境中的自主性和安全性具有重要意义。"}}
{"id": "2506.21055", "title": "Class-Agnostic Region-of-Interest Matching in Document Images", "authors": ["Demin Zhang", "Jiahao Lyu", "Zhijie Shen", "Yu Zhou"], "summary": "Document understanding and analysis have received a lot of attention due to\ntheir widespread application. However, existing document analysis solutions,\nsuch as document layout analysis and key information extraction, are only\nsuitable for fixed category definitions and granularities, and cannot achieve\nflexible applications customized by users. Therefore, this paper defines a new\ntask named ``Class-Agnostic Region-of-Interest Matching'' (``RoI-Matching'' for\nshort), which aims to match the customized regions in a flexible, efficient,\nmulti-granularity, and open-set manner. The visual prompt of the reference\ndocument and target document images are fed into our model, while the output is\nthe corresponding bounding boxes in the target document images. To meet the\nabove requirements, we construct a benchmark RoI-Matching-Bench, which sets\nthree levels of difficulties following real-world conditions, and propose the\nmacro and micro metrics to evaluate. Furthermore, we also propose a new\nframework RoI-Matcher, which employs a siamese network to extract multi-level\nfeatures both in the reference and target domains, and cross-attention layers\nto integrate and align similar semantics in different domains. Experiments show\nthat our method with a simple procedure is effective on RoI-Matching-Bench, and\nserves as the baseline for further research. The code is available at\nhttps://github.com/pd162/RoI-Matching.", "comment": "Accepted by ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.21055v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21055v1", "AI": {"title_translation": "文档图像中类别无关的感兴趣区域匹配", "tldr": "本文定义了一个名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务，旨在灵活、高效、多粒度地匹配文档图像中的自定义区域。为此，论文构建了一个基准数据集RoI-Matching-Bench，并提出了一个名为RoI-Matcher的框架，该框架采用Siamese网络和交叉注意力层。实验证明其方法有效，并可作为未来研究的基线。", "motivation": "现有的文档分析解决方案（如文档布局分析和关键信息提取）仅适用于固定的类别定义和粒度，无法实现用户定制的灵活应用。", "method": "本文定义了一个名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务。为此，论文构建了一个基准数据集RoI-Matching-Bench，并提出了宏观和微观评估指标。此外，还提出了一个名为RoI-Matcher的新框架，该框架采用Siamese网络提取参考和目标文档的多级特征，并使用交叉注意力层整合和对齐不同领域中的相似语义。", "result": "实验表明，本文提出的方法以简单的步骤在RoI-Matching-Bench上是有效的。", "conclusion": "本文提出的方法可以作为RoI-Matching任务的基线，为进一步的研究奠定基础。", "translation": "文档理解和分析因其广泛的应用而受到广泛关注。然而，现有的文档分析解决方案，例如文档布局分析和关键信息提取，仅适用于固定的类别定义和粒度，无法实现用户定制的灵活应用。因此，本文定义了一项名为“类别无关感兴趣区域匹配”（简称“RoI-Matching”）的新任务，旨在以灵活、高效、多粒度、开放集的方式匹配自定义区域。参考文档和目标文档图像的视觉提示被输入到我们的模型中，而输出是目标文档图像中对应的边界框。为了满足上述要求，我们构建了一个基准RoI-Matching-Bench，该基准根据真实世界条件设置了三个难度级别，并提出了宏观和微观指标进行评估。此外，我们还提出了一个新的框架RoI-Matcher，该框架采用Siamese网络提取参考和目标领域的多级特征，并采用交叉注意力层整合和对齐不同领域中的相似语义。实验表明，我们的方法以简单的步骤在RoI-Matching-Bench上是有效的，并可作为进一步研究的基线。代码可在https://github.com/pd162/RoI-Matching获取。", "summary": "本文针对现有文档分析解决方案缺乏灵活性的问题，提出了一项名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务，旨在实现文档图像中自定义区域的灵活、高效、多粒度匹配。为此，研究构建了RoI-Matching-Bench基准数据集和相应的评估指标，并提出RoI-Matcher框架，该框架利用Siamese网络提取多级特征并通过交叉注意力层进行语义对齐。实验证明该方法有效，并可作为未来研究的基线。", "keywords": "RoI-Matching, 类别无关, 文档理解, Siamese网络, 交叉注意力", "comments": "本文的创新点在于提出了一个全新的“类别无关感兴趣区域匹配”任务，这解决了现有文档分析方法在灵活性和用户定制方面的不足。其提出的基准数据集和RoI-Matcher框架为该领域的研究提供了有价值的起点和基线。该方法的“简单程序”特性也暗示了其潜在的实用性，但作为“基线”也表明未来可能存在进一步优化的空间。"}}
{"id": "2506.21129", "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks", "authors": ["Deepak Kumar Panda", "Adolfo Perrusquia", "Weisi Guo"], "summary": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21129v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21129v1", "AI": {"title_translation": "课程引导的反脆弱强化学习，用于观测空间攻击下的安全无人机冲突解除", "tldr": "该研究提出了一种反脆弱强化学习框架，通过模拟攻击者逐步增强观测空间扰动，使RL策略能够适应并泛化，从而在对抗性攻击下实现安全的无人机冲突解除。", "motivation": "强化学习（RL）策略部署在无人机导航等安全关键系统中时，容易受到观测空间中分布外（OOD）对抗性攻击的影响。这些攻击会导致分布偏移，显著降低价值估计，从而导致不安全或次优的决策，使现有策略脆弱。", "method": "该研究提出了一种反脆弱RL框架，旨在适应递增的对抗性扰动课程。该框架引入了一个模拟攻击者，该攻击者逐步增加观测空间扰动的强度，使RL智能体能够适应并泛化更广泛的OOD观测，并预测以前未见的攻击。理论上，将脆弱性定义为价值函数分布随扰动强度增加而单调发散的灾难性遗忘；将反脆弱性定义为这种价值偏移的有界性，并推导了稳定遗忘的适应条件。该方法通过使用Wasserstein距离最小化，在逐步扰动的观测中，通过迭代专家引导的批评者对齐来强制执行这些边界。", "result": "在涉及动态3D障碍物的无人机冲突解除场景中，反脆弱策略在面对PGD和GPS欺骗攻击时，始终优于标准和鲁棒RL基线，累积奖励提高了15%，冲突事件减少了30%以上。", "conclusion": "这些发现证明了反脆弱强化学习在具有演变威胁情景的环境中实现安全和弹性决策的实践和理论可行性。", "translation": "强化学习（RL）策略部署在安全关键系统（如动态空域中的无人机（UAV）导航）中，容易受到观测空间中分布外（OOD）对抗性攻击的影响。这些攻击会导致分布偏移，显著降低价值估计，从而导致不安全或次优的决策，使现有策略脆弱。为了解决这种脆弱性，我们提出了一种反脆弱RL框架，旨在适应递增的对抗性扰动课程。该框架引入了一个模拟攻击者，该攻击者逐步增加观测空间扰动的强度，使RL智能体能够适应并泛化更广泛的OOD观测，并预测以前未见的攻击。我们首先对脆弱性进行了理论表征，将灾难性遗忘正式定义为价值函数分布随扰动强度增加而单调发散。在此基础上，我们将反脆弱性定义为这种价值偏移的有界性，并推导了稳定遗忘的适应条件。我们的方法通过使用Wasserstein距离最小化，在逐步扰动的观测中，通过迭代专家引导的批评者对齐来强制执行这些边界。我们在涉及动态3D障碍物的无人机冲突解除场景中对该方法进行了实证评估。结果表明，当受到投影梯度下降（PGD）和GPS欺骗攻击时，反脆弱策略始终优于标准和鲁棒RL基线，累积奖励提高了15%，冲突事件减少了30%以上。这些发现证明了反脆弱强化学习在具有演变威胁情景的环境中实现安全和弹性决策的实践和理论可行性。", "summary": "本研究提出了一种名为“课程引导的反脆弱强化学习”的新框架，旨在解决强化学习（RL）策略在安全关键系统中（如无人机导航）面临的观测空间对抗性攻击导致的脆弱性问题。该框架通过引入一个模拟攻击者，逐步增加扰动强度，使RL智能体能够适应并泛化到广泛的分布外观测，从而预测并抵御未知攻击。研究首先从理论上定义了脆弱性（灾难性遗忘）和反脆弱性（价值偏移的有界性），并提出了在递增扰动下稳定遗忘的适应条件。通过迭代专家引导的批评者对齐和Wasserstein距离最小化，该方法在无人机冲突解除场景中进行了验证。实验结果表明，与现有基线相比，所提出的反脆弱策略在面对PGD和GPS欺骗攻击时表现出显著优越的性能，实现了更高的累积奖励和更少的冲突事件，证明了其在复杂威胁环境下实现安全和弹性决策的有效性。", "keywords": "反脆弱强化学习, 对抗性攻击, 无人机冲突解除, 观测空间攻击, 课程学习", "comments": "该论文的创新点在于提出了“反脆弱”强化学习的概念，并将其应用于安全关键系统，特别是无人机导航中的对抗性攻击防御。通过引入“课程引导”的模拟攻击者，逐步增加扰动强度，使得RL智能体能够主动适应并泛化，而不仅仅是被动抵抗。这种方法不仅解决了传统RL策略在面对OOD攻击时的脆弱性，还通过理论表征和实验验证了其有效性，具有重要的实践意义和理论贡献。特别是在价值函数分布的灾难性遗忘和反脆弱性定义上，提供了坚实的理论基础。"}}
{"id": "2506.21056", "title": "SAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification", "authors": ["Dinh-Khoi Vo", "Van-Loc Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Retrieving 3D objects in complex indoor environments using only a masked 2D\nimage and a natural language description presents significant challenges. The\nROOMELSA challenge limits access to full 3D scene context, complicating\nreasoning about object appearance, geometry, and semantics. These challenges\nare intensified by distorted viewpoints, textureless masked regions, ambiguous\nlanguage prompts, and noisy segmentation masks. To address this, we propose\nSAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification. SAMURAI\nintegrates CLIP-based semantic matching with shape-guided re-ranking derived\nfrom binary silhouettes of masked regions, alongside a robust majority voting\nstrategy. A dedicated preprocessing pipeline enhances mask quality by\nextracting the largest connected component and removing background noise. Our\nhybrid retrieval framework leverages both language and shape cues, achieving\ncompetitive performance on the ROOMELSA private test set. These results\nhighlight the importance of combining shape priors with language understanding\nfor robust open-world 3D object retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21056v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21056v1", "AI": {"title_translation": "SAMURAI：形状感知多模态三维物体识别检索", "tldr": "SAMURAI是一种新的方法，它利用形状和语言线索从带遮罩的2D图像和文本中检索3D物体，并在ROOMELSA挑战中表现出色。", "motivation": "在复杂的室内环境中，仅使用遮罩的2D图像和自然语言描述来检索3D物体面临巨大挑战，尤其是在ROOMELSA挑战中，由于缺乏完整的3D场景上下文、扭曲的视角、无纹理区域、模糊的语言提示和嘈杂的分割遮罩，这些挑战进一步加剧。", "method": "本文提出了SAMURAI框架，它将基于CLIP的语义匹配与从遮罩区域的二值轮廓导出的形状引导重排序相结合，并辅以鲁棒的多数投票策略。此外，还包含一个专门的预处理管道，通过提取最大连通分量和去除背景噪声来提高遮罩质量。该框架是一个利用语言和形状线索的混合检索系统。", "result": "在ROOMELSA私人测试集上取得了有竞争力的性能。", "conclusion": "研究结果强调了将形状先验与语言理解相结合对于鲁棒的开放世界3D物体检索的重要性。", "translation": "在复杂的室内环境中，仅使用遮罩的2D图像和自然语言描述来检索3D物体，带来了巨大的挑战。ROOMELSA挑战限制了对完整3D场景上下文的访问，使推理物体外观、几何形状和语义变得复杂。扭曲的视角、无纹理的遮罩区域、模糊的语言提示以及嘈杂的分割遮罩加剧了这些挑战。为了解决这个问题，我们提出了SAMURAI：形状感知多模态三维物体识别检索。SAMURAI将基于CLIP的语义匹配与从遮罩区域的二值轮廓导出的形状引导重排序相结合，并辅以鲁棒的多数投票策略。一个专门的预处理管道通过提取最大连通分量和去除背景噪声来提高遮罩质量。我们的混合检索框架利用语言和形状线索，在ROOMELSA私人测试集上取得了有竞争力的性能。这些结果突出了将形状先验与语言理解相结合对于鲁棒的开放世界3D物体检索的重要性。", "summary": "本文针对在复杂的室内环境中，仅使用遮罩的2D图像和自然语言描述进行3D物体检索的挑战性问题（特别是在ROOMELSA挑战的限制下）提出了解决方案。所提出的SAMURAI是一种混合多模态检索框架，它结合了基于CLIP的语义匹配、基于二值轮廓的形状引导重排序以及多数投票策略。该框架还包含一个专门的预处理管道以增强遮罩质量。SAMURAI利用语言和形状线索，在ROOMELSA私人测试集上表现出有竞争力的性能，从而强调了将形状先验与语言理解相结合对于鲁健的开放世界3D物体检索的关键作用。", "keywords": "3D物体检索, 多模态检索, 形状感知, CLIP, ROOMELSA", "comments": "SAMURAI的创新之处在于其混合方法，有效地将CLIP强大的语义理解与新颖的形状引导重排序机制相结合。通过预处理管道明确解决嘈杂遮罩和扭曲视角等挑战，增加了其实用性。其在ROOMELSA等挑战性基准测试中取得的有竞争力的性能表明了其在现实世界中进行鲁棒3D物体检索的潜力，尤其是在3D上下文有限的情况下。该论文强调了多模态融合的重要性，尤其是结合语言线索和经常被忽视的几何形状先验。"}}
{"id": "2506.21137", "title": "NaLaFormer: Norm-Aware Linear Attention for Transformer Models", "authors": ["Weikang Meng", "Yadan Luo", "Liangyu Huo", "Yaowei Wang", "Xin Li", "Zheng Zhang"], "summary": "Linear attention has emerged as a viable alternative to softmax attention by\nreducing complexity from quadratic to linear in sequence length. To preserve\ntwo fundamental properties of softmax, non-negativity and entropy reduction,\ncurrent works employ various linearly separatable kernel functions with $L1$\nnormalization instead of softmax operator. However, query norms are neglected\nby the normalization operation in linear attention, such degradation heavily\nleads to an entropy gap. Meanwhile, existing works inhibit negative values of\nquery and key vectors resulting in a missing inner-product interactions after\nbeing mapped. To address these dual challenges, we propose a novel Norm-Aware\nLinear Attention mechanism serving to restore norm-guided dynamic spikiness and\nrecover kernel-perturbed norm distributions. Specifically, we first decouple\nquery and key matrices into two components: norm and direction, to achieve\nnorm-aware spikiness control and norm consistency, respectively. We\nmathematically reveal that the extent of entropy reduction varies with the\nquery norm in softmax normalization, motivating a query-norm aware kernel\nfunction for dynamic control over entropy reduction. Furthermore, to ensure\nnorm consistency and enforce non-negativity constraints, we employ a\nnorm-preserving mapping to project all elements of the angular matrix into\npositive values, leveraging cosine similarity to inhibit dimensions with\nopposite directions. We conduct extensive experiments demonstrating that the\nNaLaFormer improves performance on vision and language tasks, enhancing both\nexpressiveness and efficiency by up to 4.2\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21137v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21137v1", "AI": {"title_translation": "NaLaFormer：面向Transformer模型的范数感知线性注意力机制", "tldr": "NaLaFormer提出了一种范数感知线性注意力机制，通过解耦查询和键矩阵以及范数保持映射来解决现有线性注意力中范数被忽略和负值抑制导致的问题，提升了Transformer在视觉和语言任务上的性能和效率。", "motivation": "现有线性注意力机制在L1归一化过程中忽略了查询范数，导致熵间隙；同时，对查询和键向量的负值抑制导致内积交互缺失。这些问题影响了线性注意力的非负性和熵减性质。", "method": "提出了一种新颖的范数感知线性注意力（NaLaFormer）机制。具体方法包括：1. 将查询和键矩阵解耦为范数和方向两部分，以实现范数感知的尖峰控制和范数一致性。2. 引入查询范数感知的核函数，以动态控制熵减程度。3. 采用范数保持映射将角度矩阵的所有元素投影为正值，并利用余弦相似度抑制方向相反的维度，以确保范数一致性并强制执行非负性约束。", "result": "NaLaFormer在视觉和语言任务上提高了性能，表达能力和效率提升高达4.2%。", "conclusion": "NaLaFormer通过解决现有线性注意力中范数被忽略和负值抑制的问题，成功恢复了范数引导的动态尖峰性和核扰动范数分布，显著提升了Transformer模型的性能和效率。", "translation": "线性注意力作为softmax注意力的可行替代方案，已将复杂性从序列长度的二次方降低到线性。为了保持softmax的两个基本属性——非负性和熵减，当前工作采用各种线性可分离的核函数进行L1归一化，而非softmax操作符。然而，线性注意力中的归一化操作忽略了查询范数，这种退化严重导致了熵间隙。同时，现有工作抑制了查询和键向量的负值，导致映射后缺少内积交互。为了解决这些双重挑战，我们提出了一种新颖的范数感知线性注意力机制，旨在恢复范数引导的动态尖峰性并恢复核扰动的范数分布。具体来说，我们首先将查询和键矩阵解耦为两个分量：范数和方向，分别实现范数感知的尖峰控制和范数一致性。我们通过数学揭示，softmax归一化中熵减的程度随查询范数而变化，这促使我们提出一个查询范数感知的核函数，用于动态控制熵减。此外，为了确保范数一致性并强制执行非负性约束，我们采用范数保持映射将角度矩阵的所有元素投影为正值，利用余弦相似度抑制方向相反的维度。我们进行了广泛的实验，证明NaLaFormer在视觉和语言任务上提高了性能，表达能力和效率提升高达4.2%。", "summary": "本文提出了NaLaFormer，一种范数感知线性注意力机制，旨在解决现有线性注意力中范数被忽略导致的熵间隙和负值抑制导致的内积交互缺失问题。NaLaFormer通过将查询和键矩阵解耦为范数和方向，并采用范数保持映射来恢复范数引导的动态尖峰性和核扰动范数分布。实验证明，NaLaFormer在视觉和语言任务上显著提升了Transformer模型的性能、表达能力和效率，最高可达4.2%。", "keywords": "线性注意力, Transformer, 范数感知, NaLaFormer, 熵减", "comments": "NaLaFormer的创新点在于其范数感知设计，通过解耦范数和方向以及引入范数保持映射，解决了线性注意力中长期存在的范数丢失和负值抑制问题。这不仅提升了模型的表达能力和效率，也为线性注意力机制的理论完善提供了新的思路。其对熵减程度与查询范数关系的数学揭示也具有重要意义。"}}
{"id": "2506.21076", "title": "PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image", "authors": ["Hongyu Yan", "Kunming Luo", "Weiyu Li", "Yixun Liang", "Shengming Li", "Jingwei Huang", "Chunchao Guo", "Ping Tan"], "summary": "3D characters play a crucial role in our daily entertainment. To improve the\nefficiency of 3D character modeling, recent image-based methods use two\nseparate models to achieve pose standardization and 3D reconstruction of the\nA-pose character. However, these methods are prone to generating distorted and\ndegraded images in the pose standardization stage due to self-occlusion and\nviewpoints, which further affects the geometric quality of the subsequent\nreconstruction process. To tackle these problems, we propose PoseMaster, an\nend-to-end controllable 3D character generation framework. Specifically, we\nunify pose transformation and 3D character generation into a flow-based 3D\nnative generation framework. To achieve accurate arbitrary-pose control, we\npropose to leverage the 3D body bones existing in the skeleton of an animatable\ncharacter as the pose condition. Furthermore, considering the specificity of\nmulti-condition control, we randomly empty the pose condition and the image\ncondition during training to improve the effectiveness and generalizability of\npose control. Finally, we create a high-quality pose-control dataset derived\nfrom realistic character animation data to make the model learning the implicit\nrelationships between skeleton and skinning weights. Extensive experiments show\nthat PoseMaster outperforms current state-of-the-art techniques in both\nqualitative and quantitative evaluations for A-pose character generation while\ndemonstrating its powerful ability to achieve precise control for arbitrary\nposes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21076v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21076v1", "AI": {"title_translation": "PoseMaster：从单张图像生成任意姿态的3D角色", "tldr": "PoseMaster是一个端到端框架，能从单张图像生成任意姿态的3D角色，解决了现有方法在姿态标准化和3D重建中产生的失真问题。", "motivation": "现有图像基方法使用两个独立模型进行姿态标准化和3D重建，但在姿态标准化阶段容易因自遮挡和视角导致图像失真和质量下降，进而影响后续重建的几何质量。", "method": "提出PoseMaster，一个端到端可控的3D角色生成框架。它将姿态转换和3D角色生成统一到一个基于流的3D原生生成框架中。通过利用可动画角色的骨骼中的3D身体骨骼作为姿态条件，实现精确的任意姿态控制。训练时随机清空姿态条件和图像条件以提高姿态控制的有效性和泛化性。创建了一个高质量的姿态控制数据集，该数据集来源于真实的字符动画数据，使模型能够学习骨骼和蒙皮权重之间的隐式关系。", "result": "PoseMaster在A姿态角色生成方面，无论是在定性还是定量评估中都优于当前的SOTA技术，并展示了其在实现任意姿态精确控制方面的强大能力。", "conclusion": "PoseMaster通过其统一的框架和创新的姿态控制机制，显著提高了从单张图像生成高质量、任意姿态3D角色的效率和准确性，克服了现有方法的局限性。", "translation": "3D角色在我们的日常娱乐中扮演着至关重要的角色。为了提高3D角色建模的效率，最近基于图像的方法使用两个独立的模型来实现A姿态角色的姿态标准化和3D重建。然而，这些方法在姿态标准化阶段容易因自遮挡和视角而产生扭曲和退化的图像，这进一步影响了后续重建过程的几何质量。为了解决这些问题，我们提出了PoseMaster，一个端到端可控的3D角色生成框架。具体来说，我们将姿态转换和3D角色生成统一到一个基于流的3D原生生成框架中。为了实现精确的任意姿态控制，我们提出利用可动画角色的骨架中存在的3D身体骨骼作为姿态条件。此外，考虑到多条件控制的特殊性，我们在训练期间随机清空姿态条件和图像条件，以提高姿态控制的有效性和泛化性。最后，我们创建了一个高质量的姿态控制数据集，该数据集来源于真实的字符动画数据，使模型能够学习骨骼和蒙皮权重之间的隐式关系。大量的实验表明，PoseMaster在A姿态角色生成方面，无论是在定性还是定量评估中都优于当前的最新技术，同时展示了其实现任意姿态精确控制的强大能力。", "summary": "PoseMaster是一个创新的端到端框架，旨在解决从单张图像生成3D角色时，现有方法因姿态标准化导致图像失真和几何质量下降的问题。它通过将姿态转换和3D生成整合到统一的流式框架中，并利用3D骨骼进行精确姿态控制，同时通过条件随机清空和高质量数据集训练来增强泛化能力。实验证明PoseMaster在A姿态生成和任意姿态控制方面均超越了现有技术。", "keywords": "3D角色生成, 姿态控制, 单图像, 端到端, 骨骼", "comments": "本文提出了一种创新的端到端3D角色生成框架PoseMaster，有效解决了现有方法中姿态标准化导致的图像失真和几何质量问题。其将姿态转换和3D生成统一的流式框架，以及利用3D骨骼进行姿态控制的策略是其主要创新点。此外，通过随机清空条件和构建高质量数据集，显著提升了模型的泛化能力和控制精度，对3D角色建模领域具有重要意义。"}}
{"id": "2506.21140", "title": "DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding", "authors": ["Ziwei Wang", "Hongbin Wang", "Tianwang Jia", "Xingyi He", "Siyang Li", "Dongrui Wu"], "summary": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform\nspontaneous/evoked neural activity into control commands for external\ncommunication. While convolutional neural networks (CNNs) remain the mainstream\nbackbone for EEG decoding, their inherently short receptive field makes it\ndifficult to capture long-range temporal dependencies and global inter-channel\nrelationships. Recent CNN-Transformer (Conformers) hybrids partially address\nthis issue, but most adopt a serial design, resulting in suboptimal integration\nof local and global features, and often overlook explicit channel-wise\nmodeling. To address these limitations, we propose DBConformer, a dual-branch\nconvolutional Transformer network tailored for EEG decoding. It integrates a\ntemporal Conformer to model long-range temporal dependencies and a spatial\nConformer to extract inter-channel interactions, capturing both temporal\ndynamics and spatial patterns in EEG signals. A lightweight channel attention\nmodule further refines spatial representations by assigning data-driven\nimportance to EEG channels. Extensive experiments on five motor imagery (MI)\ndatasets and two seizure detection datasets under three evaluation settings\ndemonstrate that DBConformer consistently outperforms 10 competitive baseline\nmodels, with over eight times fewer parameters than the high-capacity EEG\nConformer baseline. Further, the visualization results confirm that the\nfeatures extracted by DBConformer are physiologically interpretable and aligned\nwith sensorimotor priors in MI. The superior performance and interpretability\nof DBConformer make it reliable for robust and explainable EEG decoding. Code\nis publicized at https://github.com/wzwvv/DBConformer.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21140v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21140v1", "AI": {"title_translation": "DBConformer：用于脑电图解码的双分支卷积Transformer", "tldr": "提出DBConformer，一个双分支卷积Transformer网络，通过独立建模时序和空间特征，并结合通道注意力机制，在EEG解码任务上实现了卓越的性能和可解释性，同时参数量显著减少。", "motivation": "现有的用于EEG解码的CNN模型难以捕获长程时间依赖性和全局通道间关系，而串行设计的CNN-Transformer混合模型（Conformers）未能充分整合局部和全局特征，并且忽视了显式的通道建模。", "method": "提出DBConformer，一个双分支卷积Transformer网络。它包含一个时间Conformer用于建模长程时间依赖，以及一个空间Conformer用于提取通道间交互。此外，一个轻量级通道注意力模块通过数据驱动的方式为EEG通道分配重要性，进一步优化空间表示。", "result": "DBConformer在5个运动想象数据集和2个癫痫检测数据集上，在3种评估设置下，始终优于10个基线模型。其参数量比高容量的EEG Conformer基线少8倍以上。可视化结果证实DBConformer提取的特征具有生理学可解释性，并与运动想象中的感觉运动先验知识一致。", "conclusion": "DBConformer卓越的性能和可解释性使其在稳健和可解释的EEG解码中表现可靠。", "translation": "基于脑电图（EEG）的脑机接口（BCIs）将自发/诱发的神经活动转化为外部通信的控制命令。尽管卷积神经网络（CNNs）仍然是EEG解码的主流骨干网络，但其固有的短感受野使其难以捕获长程时间依赖性和全局通道间关系。最近的CNN-Transformer混合模型（Conformers）部分解决了这个问题，但大多数采用串行设计，导致局部和全局特征的整合次优，并且经常忽视显式的通道建模。为了解决这些局限性，我们提出了DBConformer，一个专为EEG解码设计的双分支卷积Transformer网络。它整合了一个时间Conformer来建模长程时间依赖，以及一个空间Conformer来提取通道间交互，从而捕获EEG信号中的时间动态和空间模式。一个轻量级通道注意力模块通过为EEG通道分配数据驱动的重要性，进一步细化空间表示。在五个运动想象（MI）数据集和两个癫痫检测数据集上，在三种评估设置下进行的广泛实验表明，DBConformer始终优于10个有竞争力的基线模型，且其参数量比高容量的EEG Conformer基线少八倍以上。此外，可视化结果证实DBConformer提取的特征具有生理学可解释性，并与运动想象中的感觉运动先验知识一致。DBConformer卓越的性能和可解释性使其在稳健和可解释的EEG解码中表现可靠。代码已在https://github.com/wzwvv/DBConformer公开。", "summary": "本文提出了DBConformer，一种新颖的双分支卷积Transformer网络，用于解决传统CNN在EEG解码中难以捕获长程依赖和通道间关系的问题，以及现有Conformer模型集成局部与全局特征不佳的局限性。DBConformer通过分离的时间和空间Conformer分支来独立处理时间动态和空间模式，并辅以通道注意力机制。实验证明，DBConformer在多个EEG数据集上性能显著优于现有模型，且参数量更少，同时提取的特征具有良好的生理学可解释性，为稳健和可解释的EEG解码提供了可靠方案。", "keywords": "EEG解码, 脑机接口, 卷积Transformer, 双分支网络, 通道注意力", "comments": "DBConformer的创新点在于其双分支结构，分别处理EEG信号的时序和空间特征，这种解耦设计比传统串行Conformer更能有效整合局部和全局信息。此外，引入轻量级通道注意力模块进一步提升了空间表示的精细度。该模型在参数效率和可解释性方面的优势，使其在实际BCI应用中具有重要潜力，有助于推动EEG解码技术的发展。"}}
{"id": "2506.21220", "title": "Complexity-aware fine-tuning", "authors": ["Andrey Goncharov", "Daniil Vyazhev", "Petr Sychev", "Edvard Khalafyan", "Alexey Zaytsev"], "summary": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21220v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21220v1", "AI": {"title_translation": "复杂度感知微调", "tldr": "提出一种基于熵的复杂度感知微调方法，能以更少数据实现与蒸馏相当的性能，并优于标准SFT。", "motivation": "通用大型语言模型（LLMs）通过监督微调（SFT）提升性能，但其效果有限。虽然通过蒸馏大型模型的思维链可以获得更好的结果，但这需要大量昂贵的计算和更多的数据，效率低下。", "method": "提出一种高效的微调方案，仅对通过熵识别的复杂数据使用推理。具体来说，通过单令牌答案熵（ROC AUC 0.73）将训练数据分为不同复杂度类别，然后对小型开放模型（约3B）通过SFT和蒸馏进行微调。", "result": "该方法显著优于标准SFT方法（平均准确率0.55 vs 0.43），并且在使用62%更少数据的情况下，实现了与蒸馏相当的性能（两者平均准确率均为0.55）。", "conclusion": "复杂度感知微调是一种有效且数据高效的LLM微调方法，它在性能上优于标准SFT并与蒸馏相当，同时显著减少了数据需求。", "translation": "通用大型语言模型（LLMs）经常通过监督微调（SFT）进行微调，以提高在特定领域的性能。通过蒸馏更大模型的思维链可以获得更好的结果，但这需要大量昂贵的调用和更多的数据。我们提出了一种新颖高效的微调蓝图，该蓝图仅对通过熵识别的复杂数据使用推理。具体来说，我们对两个小型开放模型（约3B）将训练数据通过单令牌答案熵（ROC AUC 0.73）分为复杂度类别，通过SFT和蒸馏微调大型语言模型（LLMs），并表明我们的流程显著优于标准SFT方法（平均准确率0.55 vs 0.43），并且在使用62%更少数据的情况下，提供了与蒸馏相当的性能（两者平均准确率均为0.55）。我们发布了代码和数据，以促进该方向的进一步研究。", "summary": "本文提出了一种新颖的“复杂度感知微调”方法，旨在提高大型语言模型（LLMs）微调的效率。该方法利用熵来识别训练数据中的复杂样本，并仅对这些复杂样本应用更耗资源的推理（蒸馏），而对简单样本使用标准监督微调（SFT）。实验结果表明，该方法在小型模型上显著优于传统的SFT方法，并在使用更少数据（减少62%）的情况下，达到了与昂贵的全量蒸馏相媲美的性能。", "keywords": "大型语言模型, 微调, 复杂度感知, 蒸馏, 熵", "comments": "这篇论文的创新点在于引入了“复杂度感知”的概念，通过数据复杂度分类来优化LLM的微调过程。这种方法有效地结合了SFT的效率和蒸馏的性能优势，同时显著降低了数据和计算成本，为高效LLM微调提供了新的思路。"}}
{"id": "2506.21080", "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception", "authors": ["Sanjoy Chowdhury", "Subrata Biswas", "Sayan Nag", "Tushar Nagarajan", "Calvin Murdock", "Ishwarya Ananthabhotla", "Yijun Qian", "Vamsi Krishna Ithapu", "Dinesh Manocha", "Ruohan Gao"], "summary": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21080v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21080v1", "AI": {"title_translation": "EgoAdapt: 自适应多感官蒸馏与策略学习，实现高效的第一人称感知", "tldr": "EgoAdapt是一个框架，通过自适应的多感官蒸馏和策略学习，显著提高了第一人称感知任务的效率，同时保持或超越了最先进的性能。", "motivation": "现代多感官第一人称感知模型计算成本高昂，难以在资源受限环境中部署。", "method": "提出EgoAdapt框架，通过自适应地执行跨模态蒸馏和策略学习，实现不同第一人称感知任务（如动作识别、说话人定位、行为预测）的高效推理。其策略模块可适应特定任务的动作空间。", "result": "在EPIC-Kitchens、EasyCom和Aria Everyday Activities三个数据集上，GMACs减少高达89.09%，参数减少高达82.02%，能耗降低高达9.6倍，同时性能与SOTA模型持平或超越。", "conclusion": "EgoAdapt通过自适应多感官蒸馏和策略学习，显著提高了第一人称感知任务的效率，使其更适合实际部署。", "translation": "现代感知模型，特别是为多感官第一人称任务设计的模型，已取得了卓越的性能，但通常伴随着巨大的计算成本。这些高要求对实际部署构成了挑战，尤其是在资源受限的环境中。在本文中，我们引入了EgoAdapt，一个自适应执行跨模态蒸馏和策略学习的框架，旨在实现不同第一人称感知任务（包括第一人称动作识别、主动说话人定位和行为预测）的高效推理。我们提出的策略模块可适应特定任务的动作空间，使其具有广泛的适用性。在EPIC-Kitchens、EasyCom和Aria Everyday Activities三个具有挑战性的第一人称数据集上的实验结果表明，我们的方法显著提高了效率，将GMACs降低了高达89.09%，参数降低了高达82.02%，能耗降低了高达9.6倍，同时性能与相应的最先进模型持平，并且在许多情况下表现更优。", "summary": "EgoAdapt是一个新颖的框架，旨在解决多感官第一人称感知模型计算成本高的问题。它通过自适应跨模态蒸馏和策略学习，显著提高了第一人称动作识别、说话人定位和行为预测等任务的推理效率。实验证明，EgoAdapt在保持甚至超越现有最佳性能的同时，大幅降低了计算资源消耗，使其更适用于资源受限的实际部署。", "keywords": "第一人称感知, 多感官蒸馏, 策略学习, 模型效率, 动作识别", "comments": "这篇论文通过引入自适应多感官蒸馏和策略学习，为第一人称感知任务的效率提升提供了一个创新方案。其核心贡献在于平衡了模型性能与计算成本，使其在边缘设备或资源受限场景下的实际部署成为可能，具有重要的应用价值。该方法的可扩展性（适应不同任务的动作空间）也是其亮点。"}}
{"id": "2506.21142", "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "summary": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21142v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21142v1", "AI": {"title_translation": "无人机网络攻击的生成对抗规避与分布外检测", "tldr": "该研究提出一个基于cGAN的框架来生成隐蔽的对抗性攻击以规避无人机入侵检测系统（IDS），并使用CVAE来有效检测这些隐蔽攻击。", "motivation": "传统的入侵检测系统（IDS）难以识别新型威胁，且常规的分布外（OOD）检测器难以区分隐蔽的对抗性攻击和真正的OOD事件，这使得无人机系统在缓解措施不足时容易受到攻击。", "method": "本文引入了一个基于条件生成对抗网络（cGAN）的框架来生成隐蔽的对抗性攻击，以规避IDS机制。首先，设计了一个鲁棒的多类别IDS分类器，该分类器在良性无人机遥测数据和已知网络攻击（包括DoS、FDI、MiTM和重放攻击）上进行训练。利用此分类器，cGAN扰动已知攻击以生成对抗性样本，这些样本被错误分类为良性，同时保留与OOD分布的统计相似性。这些对抗性样本被迭代优化以实现高隐蔽性和成功率。为了检测此类扰动，本文实现了一个条件变分自编码器（CVAE），利用负对数似然来将对抗性输入与真实的OOD样本分离。", "result": "比较评估表明，基于CVAE的遗憾分数在识别隐蔽对抗性威胁方面显著优于传统的基于马哈拉诺比斯距离的检测器。", "conclusion": "研究结果强调了先进概率建模对于加强IDS防御自适应、基于生成模型的网络入侵的重要性。", "translation": "无人机日益融入民用空域，凸显了对弹性智能入侵检测系统（IDS）的需求，因为传统的异常检测方法往往无法识别新型威胁。一种常见的方法是将不熟悉的攻击视为分布外（OOD）样本；然而，当缓解措施不足时，这会使系统容易受到攻击。此外，传统的OOD检测器难以区分隐蔽的对抗性攻击与真正的OOD事件。本文引入了一个基于条件生成对抗网络（cGAN）的框架，用于制作规避IDS机制的隐蔽对抗性攻击。我们首先设计了一个鲁棒的多类别IDS分类器，该分类器在良性无人机遥测数据和已知网络攻击（包括拒绝服务（DoS）、虚假数据注入（FDI）、中间人（MiTM）和重放攻击）上进行训练。利用此分类器，我们的cGAN扰动已知攻击以生成对抗性样本，这些样本被错误分类为良性，同时保留与OOD分布的统计相似性。这些对抗性样本被迭代优化以实现高隐蔽性和成功率。为了检测此类扰动，我们实现了一个条件变分自编码器（CVAE），利用负对数似然来分离对抗性输入与真实的OOD样本。比较评估表明，基于CVAE的遗憾分数在识别隐蔽对抗性威胁方面显著优于传统的基于马哈拉诺比斯距离的检测器。我们的发现强调了先进概率建模对于加强IDS防御自适应、基于生成模型的网络入侵的重要性。", "summary": "本文针对无人机网络攻击中传统入侵检测系统（IDS）和分布外（OOD）检测器在识别新型和隐蔽对抗性威胁方面的不足，提出了一种创新框架。该框架利用条件生成对抗网络（cGAN）生成能规避IDS且统计上类似OOD的隐蔽对抗性攻击，并使用条件变分自编码器（CVAE）通过负对数似然有效检测这些攻击。实验结果表明，基于CVAE的检测方法在识别隐蔽对抗性威胁方面显著优于传统方法，强调了先进概率建模在增强无人机IDS防御能力方面的重要性。", "keywords": "无人机网络安全, 入侵检测系统, 生成对抗网络, 分布外检测, 条件变分自编码器", "comments": "该论文的创新点在于同时利用生成对抗网络（cGAN）来创建高度隐蔽的对抗性攻击，并引入条件变分自编码器（CVAE）来有效检测这些由生成模型产生的复杂威胁。这种攻防兼备的视角对于提升无人机网络安全防御的鲁棒性具有重要意义，尤其是在面对日益智能化的网络攻击时。研究强调了高级概率建模在区分真实OOD事件和隐蔽攻击方面的潜力。"}}
{"id": "2506.21263", "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster", "authors": ["Ji Qi", "WenPeng Zhu", "Li Li", "Ming Wu", "YingJun Wu", "Wu He", "Xun Gao", "Jason Zeng", "Michael Heinrich"], "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21263v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21263v1", "AI": {"title_translation": "DiLoCoX：一种用于去中心化集群的低通信大规模训练框架", "tldr": "DiLoCoX是一种低通信量的大规模去中心化集群训练框架，可以在慢速网络上训练千亿级参数模型，显著提高训练速度。", "motivation": "分布式训练基础模型（特别是LLM）需要高通信量，因此高度依赖于具有快速可靠互连的中心化集群。当前挑战是在慢速网络上训练超过千亿参数的模型，并释放去中心化集群的潜力。", "method": "DiLoCoX框架结合了流水线并行（Pipeline Parallelism）、双优化器策略（Dual Optimizer Policy）、通信与本地训练的一步延迟重叠（One-Step-Delay Overlap of Communication and Local Training）以及自适应梯度压缩方案（Adaptive Gradient Compression Scheme）。并通过收敛性理论分析证明了一步延迟重叠和自适应梯度压缩方案的益处。", "result": "DiLoCoX能够在1Gbps网络上预训练107B的基础模型。与传统的AllReduce相比，DiLoCoX在分布式训练中实现了357倍的速度提升，同时模型收敛性退化可忽略不计。", "conclusion": "DiLoCoX是第一个成功应用于千亿级参数模型的去中心化训练框架，能够有效解决慢速网络下大规模模型训练的通信瓶颈。", "translation": "基础模型，特别是大型语言模型（LLM）的分布式训练，需要高水平的通信。因此，它高度依赖于具有快速可靠互连的中心化集群。我们能否在慢速网络上进行训练，从而在处理超过1000亿参数的模型时释放去中心化集群的潜力？在本文中，我们提出了DiLoCoX，一种低通信的大规模去中心化集群训练框架。它结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案。这种组合显著提高了参数规模和模型预训练的速度。我们通过收敛性理论分析证明了通信与本地训练的一步延迟重叠以及自适应梯度压缩方案的益处。通过实验，我们证明DiLoCoX能够在1Gbps网络上预训练一个107B的基础模型。与传统的AllReduce相比，DiLoCoX在分布式训练中可以实现357倍的速度提升，同时保持模型收敛性的可忽略不计的退化。据我们所知，这是第一个成功应用于超过1000亿参数模型的去中心化训练框架。", "summary": "DiLoCoX是一个针对大型模型在去中心化集群中进行低通信量训练的框架。它通过整合流水线并行、双优化器策略、一步延迟通信重叠和自适应梯度压缩，显著提升了参数规模和预训练速度。实验证明，DiLoCoX能在慢速网络上高效训练千亿级模型，相较于AllReduce实现了显著加速，且不影响收敛性。", "keywords": "去中心化训练, 低通信, 大规模模型, 流水线并行, 梯度压缩", "comments": "DiLoCoX的创新之处在于其结合多种策略有效降低了大规模模型在去中心化、低带宽网络环境下的通信开销，特别是其一步延迟重叠和自适应梯度压缩机制。这对于推动LLM等基础模型在更广泛、更具挑战性的网络条件下进行训练具有重要意义，打破了对昂贵中心化集群的依赖。"}}
{"id": "2506.21091", "title": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching", "authors": ["Mahmoud Tahmasebi", "Saif Huq", "Kevin Meehan", "Marion McAfee"], "summary": "Stereo matching has become an increasingly important component of modern\nautonomous systems. Developing deep learning-based stereo matching models that\ndeliver high accuracy while operating in real-time continues to be a major\nchallenge in computer vision. In the domain of cost-volume-based stereo\nmatching, accurate disparity estimation depends heavily on large-scale cost\nvolumes. However, such large volumes store substantial redundant information\nand also require computationally intensive aggregation units for processing and\nregression, making real-time performance unattainable. Conversely, small-scale\ncost volumes followed by lightweight aggregation units provide a promising\nroute for real-time performance, but lack sufficient information to ensure\nhighly accurate disparity estimation. To address this challenge, we propose the\nEnhanced Shuffle Mixer (ESM) to mitigate information loss associated with\nsmall-scale cost volumes. ESM restores critical details by integrating primary\nfeatures into the disparity upsampling unit. It quickly extracts features from\nthe initial disparity estimation and fuses them with image features. These\nfeatures are mixed by shuffling and layer splitting then refined through a\ncompact feature-guided hourglass network to recover more detailed scene\ngeometry. The ESM focuses on local contextual connectivity with a large\nreceptive field and low computational cost, leading to the reconstruction of a\nhighly accurate disparity map at real-time. The compact version of ESMStereo\nachieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX\nOrin.", "comment": "Under peer review", "pdf_url": "http://arxiv.org/pdf/2506.21091v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21091v1", "AI": {"title_translation": "ESMStereo: 增强型 ShuffleMixer 视差上采样，用于实时高精度立体匹配", "tldr": "ESMStereo提出了一种增强型Shuffle Mixer (ESM)来解决实时立体匹配中精度与速度的权衡问题，通过在小尺度代价体上进行特征融合和细化，实现了实时高精度视差估计。", "motivation": "在计算机视觉中，开发兼具高精度和实时性的深度学习立体匹配模型是一个重大挑战。大型代价体计算量大，难以实时；小型代价体虽然实时，但精度不足。", "method": "本文提出了增强型 Shuffle Mixer (ESM) 来缓解小尺度代价体带来的信息丢失。ESM 通过将主要特征集成到视差上采样单元中来恢复关键细节，快速从初始视差估计中提取特征并与图像特征融合。这些特征通过混洗和层分割进行混合，并通过紧凑的特征引导沙漏网络进行细化，以恢复更详细的场景几何。ESM 专注于具有大感受野和低计算成本的局部上下文连接。", "result": "ESMStereo 实现了实时高精度视差图重建。其紧凑版本在高端 GPU 上达到 116 FPS 的推理速度，在 AGX Orin 上达到 91 FPS。", "conclusion": "ESMStereo 通过其增强型 Shuffle Mixer (ESM) 有效解决了实时立体匹配中精度和速度的权衡问题，实现了在保持高精度的同时达到实时性能的目标。", "translation": "立体匹配已成为现代自主系统日益重要的组成部分。开发兼具高精度和实时性的深度学习立体匹配模型仍然是计算机视觉领域的一个主要挑战。在基于代价体的立体匹配领域，准确的视差估计严重依赖于大规模代价体。然而，这些大规模代价体存储了大量冗余信息，并且需要计算密集型的聚合单元进行处理和回归，从而无法实现实时性能。相反，小规模代价体加上轻量级聚合单元为实时性能提供了一条有前景的路线，但缺乏足够的信息来确保高精度的视差估计。为了解决这个挑战，我们提出了增强型 Shuffle Mixer (ESM) 来缓解与小规模代价体相关的信息丢失。ESM 通过将主要特征集成到视差上采样单元中来恢复关键细节。它快速从初始视差估计中提取特征并将其与图像特征融合。这些特征通过混洗和层分割进行混合，然后通过紧凑的特征引导沙漏网络进行细化，以恢复更详细的场景几何。ESM 专注于具有大感受野和低计算成本的局部上下文连接，从而实现了实时高精度视差图的重建。ESMStereo 的紧凑版本在高端 GPU 上实现了 116 FPS 的推理速度，在 AGX Orin 上实现了 91 FPS。", "summary": "ESMStereo 提出了一种名为增强型 Shuffle Mixer (ESM) 的方法，旨在解决深度学习立体匹配中实时性与高精度难以兼顾的问题。针对大型代价体计算量大、小型代价体信息不足的挑战，ESM 通过在视差上采样单元中集成和融合初始视差与图像特征，并利用混洗、层分割及紧凑的沙漏网络进行细化，有效恢复了小尺度代价体中的关键细节。该方法在保证低计算成本的同时，实现了高精度的实时视差图重建，在高端 GPU 上达到 116 FPS，在 AGX Orin 上达到 91 FPS。", "keywords": "立体匹配, 视差上采样, 实时, Shuffle Mixer, 深度学习", "comments": "ESMStereo 的创新在于其提出的 ESM 模块，巧妙地解决了小尺度代价体在实时立体匹配中精度不足的问题。通过有效的特征融合和精炼机制，它在保持实时性能的同时显著提升了视差估计的准确性，这对于自动驾驶等需要高精度和低延迟的应用至关重要。"}}
{"id": "2506.21144", "title": "Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion", "authors": ["Yuguang Zhang", "Kuangpu Guo", "Zhihe Lu", "Yunbo Wang", "Jian Liang"], "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, but is challenged by\nheterogeneity in data, computation, and communication. Pretrained\nvision-language models (VLMs), with their strong generalization and lightweight\ntuning via prompts, offer a promising solution. However, existing federated\nprompt-learning methods rely only on text prompts and overlook joint\nlabel-domain distribution shifts. In this paper, we propose a personalized FL\nframework based on dual-prompt learning and cross fusion, termed pFedDC.\nSpecifically, each client maintains both global and local prompts across vision\nand language modalities: global prompts capture common knowledge shared across\nthe federation, while local prompts encode client-specific semantics and domain\ncharacteristics. Meanwhile, a cross-fusion module is designed to adaptively\nintegrate prompts from different levels, enabling the model to generate\npersonalized representations aligned with each client's unique data\ndistribution. Extensive experiments across nine datasets with various types of\nheterogeneity show that pFedDC consistently outperforms state-of-the-art\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21144v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21144v1", "AI": {"title_translation": "通过双提示优化和交叉融合的个性化联邦学习", "tldr": "本文提出了一种名为pFedDC的个性化联邦学习（FL）框架，通过双提示优化和交叉融合来解决FL中的异质性问题，并在视觉-语言模型（VLM）中实现更优的性能。", "motivation": "联邦学习（FL）在去中心化客户端协作训练模型时面临数据、计算和通信异质性挑战。尽管预训练的视觉-语言模型（VLM）通过提示词微调具有良好的泛化能力，但现有联邦提示学习方法仅依赖文本提示，忽略了联合标签-域分布偏移问题。", "method": "本文提出了一个名为pFedDC的个性化联邦学习框架，它基于双提示学习和交叉融合。具体而言，每个客户端在视觉和语言模态中维护全局和局部提示：全局提示捕获联邦共享的通用知识，而局部提示编码客户端特定的语义和领域特征。同时，设计了一个交叉融合模块，以自适应地整合不同级别的提示，使模型能够生成与每个客户端独特数据分布对齐的个性化表示。", "result": "在九个具有各种异质性类型的数据集上进行的广泛实验表明，pFedDC始终优于最先进的方法。", "conclusion": "pFedDC框架通过双提示优化和交叉融合，有效解决了联邦学习中的异质性挑战，并在个性化联邦学习方面实现了优于现有方法的性能。", "translation": "联邦学习（FL）使得去中心化客户端无需共享本地数据即可进行协作模型训练，但面临数据、计算和通信异质性的挑战。预训练的视觉-语言模型（VLM）凭借其强大的泛化能力和轻量级提示词微调，提供了一个有前景的解决方案。然而，现有的联邦提示学习方法仅依赖于文本提示，并且忽略了联合标签-域分布偏移。在本文中，我们提出了一个基于双提示学习和交叉融合的个性化FL框架，命名为pFedDC。具体来说，每个客户端在视觉和语言模态中维护全局和局部提示：全局提示捕获联邦共享的通用知识，而局部提示编码客户端特定的语义和领域特征。同时，设计了一个交叉融合模块，以自适应地整合来自不同级别的提示，使模型能够生成与每个客户端独特数据分布对齐的个性化表示。在九个具有各种异质性类型的数据集上进行的广泛实验表明，pFedDC始终优于最先进的方法。", "summary": "本文提出了一个名为pFedDC的个性化联邦学习（FL）框架，旨在解决FL中的数据异质性问题。该框架利用视觉-语言模型（VLM）的双提示学习机制，为每个客户端维护全局和局部提示，分别捕获通用知识和客户端特定特征。此外，一个交叉融合模块被设计用于自适应地整合这些提示，从而生成与客户端数据分布对齐的个性化表示。实验结果表明，pFedDC在多个数据集上均优于现有先进方法。", "keywords": "联邦学习, 个性化联邦学习, 提示学习, 视觉-语言模型, 异质性", "comments": "本文的创新点在于将双提示学习（全局和局部提示）和交叉融合机制引入联邦学习，特别是在视觉-语言模型背景下，以有效应对数据异质性并实现模型个性化，超越了传统仅依赖文本提示的方法。这为构建更鲁棒和个性化的联邦学习系统迈出了重要一步。"}}
{"id": "2506.21277", "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context", "authors": ["Qize Yang", "Shimin Yao", "Weixuan Chen", "Shenghao Fu", "Detao Bai", "Jiaxing Zhao", "Boyuan Sun", "Bowen Yin", "Xihan Wei", "Jingren Zhou"], "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21277v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21277v1", "AI": {"title_translation": "HumanOmniV2: 从理解到基于上下文的全模态推理", "tldr": "本文提出了HumanOmniV2，一个利用强化学习和多维度奖励机制（包括上下文、格式、准确性和逻辑奖励）来提升多模态大型语言模型推理能力的方法，旨在解决现有模型中上下文理解不足和捷径问题，并引入了一个新的全模态基准测试IntentBench。", "motivation": "多模态大型语言模型（LLMs）需要深入理解和解释人类意图，这要求精细的推理能力。尽管强化学习（RL）在增强LLMs推理能力方面显示出潜力，但将其适应多模态数据和格式仍面临挑战。现有多模态推理模型存在全局上下文理解不足和捷径问题，导致模型错误解释上下文或忽略关键线索。", "method": "为了解决现有问题，论文强调模型需清晰理解多模态输入中的全局上下文。方法包括：1) 引入由大型语言模型（LLM）判断的上下文奖励，并结合格式和准确性奖励，以确保准确解释多模态上下文信息。2) 利用LLM评估逻辑奖励，以提高复杂推理能力，判断推理过程是否成功整合多模态信息与逻辑方法。3) 提出一个新的推理全模态基准测试IntentBench，用于评估模型理解复杂人类意图和情感的能力。", "result": "所提出的方法在多个全模态基准测试中，与其它开源全模态模型相比，展现出更高的性能。", "conclusion": "本文提出了HumanOmniV2框架，通过引入由LLM判断的多种奖励机制（上下文、格式、准确性和逻辑奖励），有效解决了多模态推理中全局上下文理解不足和捷径问题。同时，推出的IntentBench基准为评估模型理解复杂人类意图的能力提供了新工具。实验结果证明了该方法在全模态推理上的优越性。", "translation": "随着多模态大型语言模型的快速发展，深入理解和解释人类意图的能力已成为一项关键能力，这需要详细而周密的推理。在最近的研究中，强化学习（RL）已显示出增强大型语言模型（LLM）推理能力的潜力。然而，将RL适应多模态数据和格式所面临的挑战在很大程度上仍未解决。在本文中，我们指出了现有多模态推理模型中的两个问题：全局上下文理解不足和捷径问题。当模型错误地解释多模态上下文，导致不正确的答案时，就会发生上下文理解不足。当模型忽略多模态输入中的关键线索，直接回答查询而不考虑多模态信息时，就会出现捷径问题。为了解决这些问题，我们强调模型需要清晰地理解多模态输入中的全局上下文进行推理。这种全局上下文理解可以有效地防止模型忽略关键的多模态线索，并确保彻底的推理过程。为了确保准确解释多模态上下文信息，我们实施了一个由大型语言模型判断的上下文奖励，以及格式和准确性奖励。此外，为了提高复杂的推理能力，我们利用LLM评估逻辑奖励，判断推理过程是否成功地将多模态信息与逻辑方法相结合。我们还引入了一个推理全模态基准测试IntentBench，旨在评估模型理解复杂人类意图和情感的能力。我们提出的方法在多个全模态基准测试中，与其它开源全模态模型相比，展现出更高的性能。", "summary": "本文提出了HumanOmniV2，一个旨在增强多模态大语言模型推理能力的方法。它通过引入上下文奖励、格式奖励、准确性奖励和逻辑奖励（由LLM判断）来解决现有模型中全局上下文理解不足和捷径问题。此外，该研究还推出了一个新的全模态基准测试IntentBench，用于评估模型对复杂人类意图和情感的理解。实验结果表明，HumanOmniV2在多个全模态基准测试中优于其他开源模型。", "keywords": "多模态推理, 强化学习, 大型语言模型, 上下文理解, 全模态基准", "comments": "这项工作通过引入多维度奖励机制，特别是LLM判断的上下文和逻辑奖励，为强化学习在多模态推理中的应用提供了新思路，有效解决了现有模型的上下文理解不足和捷径问题。新提出的IntentBench基准也对未来研究提供了有价值的评估工具。其创新性在于将LLM的能力用于指导RL的奖励设计，从而更精细地优化多模态推理过程。"}}
{"id": "2506.21101", "title": "OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "AndyPian Wu", "Chaoyang Wang", "Chengjie Wang", "Taisong Jin", "SevenShu", "Yunsheng Wu", "Yongge Liu", "Rongrong Ji"], "summary": "As one of the earliest ancient languages, Oracle Bone Script (OBS)\nencapsulates the cultural records and intellectual expressions of ancient\ncivilizations. Despite the discovery of approximately 4,500 OBS characters,\nonly about 1,600 have been deciphered. The remaining undeciphered ones, with\ntheir complex structure and abstract imagery, pose significant challenges for\ninterpretation. To address these challenges, this paper proposes a novel\ntwo-stage semantic typography framework, named OracleFusion. In the first\nstage, this approach leverages the Multimodal Large Language Model (MLLM) with\nenhanced Spatial Awareness Reasoning (SAR) to analyze the glyph structure of\nthe OBS character and perform visual localization of key components. In the\nsecond stage, we introduce Oracle Structural Vector Fusion (OSVF),\nincorporating glyph structure constraints and glyph maintenance constraints to\nensure the accurate generation of semantically enriched vector fonts. This\napproach preserves the objective integrity of the glyph structure, offering\nvisually enhanced representations that assist experts in deciphering OBS.\nExtensive qualitative and quantitative experiments demonstrate that\nOracleFusion outperforms state-of-the-art baseline models in terms of\nsemantics, visual appeal, and glyph maintenance, significantly enhancing both\nreadability and aesthetic quality. Furthermore, OracleFusion provides\nexpert-like insights on unseen oracle characters, making it a valuable tool for\nadvancing the decipherment of OBS.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21101v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21101v1", "AI": {"title_translation": "甲骨融合：以结构约束语义字体辅助甲骨文释读", "tldr": "OracleFusion是一个两阶段的语义字体框架，利用多模态大语言模型和结构向量融合技术，辅助甲骨文释读，显著提升了可读性和美观性，并能为专家提供未见字符的洞察。", "motivation": "甲骨文作为最古老的文字之一，承载着古代文明的文化记录和思想表达。然而，已发现的约4500个甲骨文中有近1600个已被释读，剩余未释读的字符结构复杂、意象抽象，给释读带来了巨大挑战。", "method": "本文提出了一个名为OracleFusion的新型两阶段语义字体框架。第一阶段，该方法利用增强空间感知推理（SAR）的多模态大语言模型（MLLM）分析甲骨文的字形结构并进行关键组件的视觉定位。第二阶段，引入甲骨结构向量融合（OSVF），结合字形结构约束和字形维护约束，以确保生成语义丰富的矢量字体。", "result": "广泛的定性和定量实验表明，OracleFusion在语义、视觉吸引力和字形维护方面优于最先进的基线模型，显著增强了可读性和美学质量。此外，OracleFusion能为未见甲骨字符提供专家级的洞察。", "conclusion": "OracleFusion是一个有价值的工具，能够推进甲骨文的释读工作，通过提供视觉增强的表示来辅助专家，并为未见字符提供专家级的洞察。", "translation": "甲骨文作为最早的古老语言之一，承载着古代文明的文化记录和思想表达。尽管已发现大约4500个甲骨文字符，但只有约1600个已被释读。剩余未释读的字符，其复杂的结构和抽象的意象，给解释带来了巨大的挑战。为了解决这些挑战，本文提出了一种新颖的两阶段语义字体框架，命名为OracleFusion。在第一阶段，该方法利用增强空间感知推理（SAR）的多模态大语言模型（MLLM）来分析甲骨文字符的字形结构并执行关键组件的视觉定位。在第二阶段，我们引入了甲骨结构向量融合（OSVF），结合字形结构约束和字形维护约束，以确保准确生成语义丰富的矢量字体。这种方法保留了字形结构的客观完整性，提供了视觉增强的表示，辅助专家释读甲骨文。广泛的定性和定量实验表明，OracleFusion在语义、视觉吸引力和字形维护方面优于最先进的基线模型，显著增强了可读性和美学质量。此外，OracleFusion为未见甲骨文字符提供了专家般的见解，使其成为推进甲骨文释读工作的宝贵工具。", "summary": "本文提出了一种名为OracleFusion的新型两阶段语义字体框架，旨在解决甲骨文（OBS）释读的挑战。该框架首先利用多模态大语言模型（MLLM）分析OBS字符的字形结构并进行视觉定位，然后通过甲骨结构向量融合（OSVF）生成受结构约束的语义丰富矢量字体。实验证明，OracleFusion在语义、视觉吸引力和字形维护方面优于现有模型，显著提高了OBS的可读性和美学质量，并能为专家提供未见字符的洞察，是推进OBS释读的宝贵工具。", "keywords": "甲骨文释读, 语义字体, 多模态大语言模型, 结构约束, 甲骨结构向量融合", "comments": "该论文提出了一种创新的方法，将先进的AI模型（MLLM）与结构约束相结合，用于甲骨文的辅助释读。其创新之处在于结合了视觉分析和语义字体生成，以解决古文字释读中的复杂结构和抽象意象问题。通过提供视觉增强的表示和专家级洞察，该工具对甲骨文研究具有重要意义。"}}
{"id": "2506.21146", "title": "Linearity-based neural network compression", "authors": ["Silas Dobler", "Florian Lemmerich"], "summary": "In neural network compression, most current methods reduce unnecessary\nparameters by measuring importance and redundancy. To augment already highly\noptimized existing solutions, we propose linearity-based compression as a novel\nway to reduce weights in a neural network. It is based on the intuition that\nwith ReLU-like activation functions, neurons that are almost always activated\nbehave linearly, allowing for merging of subsequent layers. We introduce the\ntheory underlying this compression and evaluate our approach experimentally.\nOur novel method achieves a lossless compression down to 1/4 of the original\nmodel size in over the majority of tested models. Applying our method on\nalready importance-based pruned models shows very little interference between\ndifferent types of compression, demonstrating the option of successful\ncombination of techniques. Overall, our work lays the foundation for a new type\nof compression method that enables smaller and ultimately more efficient neural\nnetwork models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21146v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21146v1", "AI": {"title_translation": "基于线性的神经网络压缩", "tldr": "本文提出了一种基于线性的新型神经网络压缩方法，通过利用ReLU激活函数下近似线性行为的神经元来合并层，实现了在大多数模型中高达1/4的无损压缩，并可与其他压缩技术结合使用。", "motivation": "为了增强现有的高度优化的神经网络压缩解决方案，并进一步减少神经网络中的冗余参数，因为大多数当前方法主要通过测量重要性和冗余来减少参数。", "method": "提出了一种基于线性的神经网络压缩方法。其核心思想是，对于ReLU类激活函数，几乎总是激活的神经元表现出线性行为，这允许合并后续层。该方法包括理论介绍和实验评估。", "result": "在大多数测试模型中，实现了高达1/4原始模型大小的无损压缩。将该方法应用于已进行基于重要性剪枝的模型时，显示出不同类型压缩之间干扰极小，证明了技术成功组合的可能性。", "conclusion": "本文为一种新型的神经网络压缩方法奠定了基础，该方法能够实现更小、最终更高效的神经网络模型。", "translation": "在神经网络压缩领域，大多数现有方法通过衡量重要性和冗余来减少不必要的参数。为了增强现有高度优化的解决方案，我们提出了一种基于线性的压缩方法，作为减少神经网络权重的创新方式。它基于这样一种直觉：对于ReLU类激活函数，几乎总是激活的神经元表现出线性行为，从而允许合并后续层。我们介绍了这种压缩方法的基础理论，并对我们的方法进行了实验评估。我们的新方法在大多数测试模型中实现了高达原始模型大小1/4的无损压缩。将我们的方法应用于已经基于重要性修剪的模型，显示出不同类型压缩之间干扰极小，证明了成功组合技术的可能性。总的来说，我们的工作为一种新型的压缩方法奠定了基础，该方法能够实现更小、最终更高效的神经网络模型。", "summary": "本文提出了一种新颖的基于线性的神经网络压缩方法，旨在通过利用ReLU类激活函数下近似线性行为的神经元来合并后续层，从而有效减少模型大小。该方法在理论上得到阐述并经过实验验证，在大多数测试模型中实现了高达1/4的无损压缩。此外，它能与现有的基于重要性的剪枝方法良好结合，为构建更小、更高效的神经网络模型提供了新途径。", "keywords": "神经网络压缩, 线性, ReLU, 模型大小, 无损压缩", "comments": "这项工作引入了一种新颖的神经网络压缩视角，即利用神经元的线性行为而非传统的参数重要性或冗余度量。其创新点在于将线性行为与层合并结合起来，实现了显著的无损压缩。该方法与现有技术的兼容性也增加了其实用价值，为神经网络的部署和效率提升提供了新的可能性。"}}
{"id": "2506.21109", "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection", "authors": ["Luosheng Xu", "Dalin Zhang", "Zhaohui Song"], "summary": "Remote sensing change detection is essential for monitoring urban expansion,\ndisaster assessment, and resource management, offering timely, accurate, and\nlarge-scale insights into dynamic landscape transformations. While deep\nlearning has revolutionized change detection, the increasing complexity and\ncomputational demands of modern models have not necessarily translated into\nsignificant accuracy gains. Instead of following this trend, this study\nexplores a more efficient approach, focusing on lightweight models that\nmaintain high accuracy while minimizing resource consumption, which is an\nessential requirement for on-satellite processing. To this end, we propose\nFlickCD, which means quick flick then get great results, pushing the boundaries\nof the performance-resource trade-off. FlickCD introduces an Enhanced\nDifference Module (EDM) to amplify critical feature differences between\ntemporal phases while suppressing irrelevant variations such as lighting and\nweather changes, thereby reducing computational costs in the subsequent change\ndecoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion\nBlocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global\nSelf-Attention (EGSA) to efficiently capture semantic information at multiple\nscales, preserving both coarse- and fine-grained changes. Extensive experiments\non four benchmark datasets demonstrate that FlickCD reduces computational and\nstorage overheads by more than an order of magnitude while achieving\nstate-of-the-art (SOTA) performance or incurring only a minor (<1\\% F1)\naccuracy trade-off. The implementation code is publicly available at\nhttps://github.com/xulsh8/FlickCD.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.21109v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21109v1", "AI": {"title_translation": "突破权衡界限：紧凑而有效的遥感变化检测", "tldr": "本文提出了一种名为FlickCD的轻量级模型，用于遥感变化检测，它在显著降低计算和存储开销的同时，保持了最先进的性能或仅有微小精度损失。", "motivation": "深度学习在遥感变化检测中面临模型复杂性和计算需求增加但精度提升不显著的问题，尤其是在星载处理中需要高效且资源消耗小的模型。", "method": "本文提出了FlickCD模型，它包含一个增强差异模块（EDM），用于放大关键特征差异并抑制无关变化，从而降低后续变化解码器的计算成本。此外，FlickCD解码器结合了局部-全局融合块，利用移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）来高效捕获多尺度语义信息，保留粗粒度和细粒度变化。", "result": "在四个基准数据集上的广泛实验表明，FlickCD将计算和存储开销降低了一个数量级以上，同时实现了最先进（SOTA）的性能或仅导致微小的（<1% F1）精度权衡。", "conclusion": "FlickCD模型成功地在遥感变化检测中实现了性能与资源消耗之间的有效权衡，为轻量级、高效率的模型提供了解决方案，特别适用于资源受限的场景如星载处理。", "translation": "遥感变化检测对于监测城市扩张、灾害评估和资源管理至关重要，它能及时、准确、大规模地洞察动态景观变化。虽然深度学习彻底改变了变化检测，但现代模型日益增长的复杂性和计算需求并未必然带来显著的精度提升。本研究没有追随这一趋势，而是探索了一种更有效的方法，专注于在最小化资源消耗的同时保持高精度的轻量级模型，这是星载处理的基本要求。为此，我们提出了FlickCD，意为快速一瞥即可获得出色结果，它突破了性能-资源权衡的界限。FlickCD引入了一个增强差异模块（EDM），以放大时间阶段之间的关键特征差异，同时抑制光照和天气变化等无关变异，从而降低后续变化解码器中的计算成本。此外，FlickCD解码器结合了局部-全局融合块，利用移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）来高效捕获多尺度语义信息，保留粗粒度和细粒度变化。在四个基准数据集上的广泛实验表明，FlickCD将计算和存储开销降低了一个数量级以上，同时实现了最先进（SOTA）的性能或仅导致微小的（<1% F1）精度权衡。实现代码已在https://github.com/xulsh8/FlickCD公开。", "summary": "本研究针对遥感变化检测领域深度学习模型日益增长的复杂性和资源消耗问题，提出了一种名为FlickCD的轻量级模型。FlickCD通过引入增强差异模块（EDM）来高效处理特征差异，并利用局部-全局融合块（包含SWSA和EGSA）来捕获多尺度语义信息。实验证明，FlickCD在显著减少计算和存储开销的同时，仍能保持最先进的性能或仅有极小的精度损失，尤其适用于资源受限的星载处理场景。", "keywords": "遥感变化检测, 轻量级模型, FlickCD, 计算效率, 性能-资源权衡", "comments": "本文的创新点在于提出了FlickCD模型，它通过独特的设计（EDM和Local-Global Fusion Blocks）成功地在遥感变化检测领域实现了计算效率和检测精度之间的卓越平衡。其重要性体现在为资源受限环境（如星载处理）提供了切实可行的解决方案，挑战了当前深度学习模型盲目追求复杂度的趋势。"}}
{"id": "2506.21158", "title": "Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design", "authors": ["Hampus Gummesson Svensson", "Ola Engkvist", "Jon Paul Janet", "Christian Tyrchan", "Morteza Haghir Chehreghani"], "summary": "In many real-world applications, evaluating the goodness of instances is\noften costly and time-consuming, e.g., human feedback and physics simulations,\nin contrast to proposing new instances. In particular, this is even more\ncritical in reinforcement learning, as new interactions with the environment\n(i.e., new instances) need to be evaluated to provide a reward signal to learn\nfrom. As sufficient exploration is crucial, learning from a diverse mini-batch\ncan have a large impact and help mitigate mode collapse. In this paper, we\nintroduce diverse mini-batch selection for reinforcement learning and propose\nto use determinantal point processes for this task. We study this framework in\nthe context of a real-world problem, namely drug discovery. We experimentally\nstudy how our proposed framework can improve the effectiveness of chemical\nexploration in de novo drug design, where finding diverse and high-quality\nsolutions is essential. We conduct a comprehensive evaluation with three\nwell-established molecular generation oracles over numerous generative steps.\nOur experiments conclude that our diverse mini-batch selection framework can\nsubstantially improve the diversity of the solutions, while still obtaining\nsolutions of high quality. In drug discovery, such outcome can potentially lead\nto fulfilling unmet medication needs faster.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21158v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21158v1", "AI": {"title_translation": "强化学习中多样化小批量选择用于从头药物设计中的高效化学探索", "tldr": "本论文提出了一种基于行列式点过程的强化学习多样化小批量选择框架，用于从头药物设计中的高效化学探索，实验证明该方法在保持高质量的同时显著提高了解决方案的多样性。", "motivation": "在许多实际应用中，评估实例的优劣成本高昂且耗时，尤其是在强化学习中，新的环境交互需要评估以提供奖励信号。为了进行充分探索并避免模式崩溃，从多样化的小批量中学习至关重要。", "method": "本文引入了强化学习中的多样化小批量选择方法，并提出使用行列式点过程（Determinantal Point Processes）来完成此任务。该框架应用于从头药物设计中的化学探索。", "result": "实验结果表明，所提出的多样化小批量选择框架能够显著提高解决方案的多样性，同时仍能获得高质量的解决方案。", "conclusion": "多样化小批量选择框架能有效提升从头药物设计中化学探索的效率和多样性，有助于更快满足未满足的药物需求。", "translation": "在许多实际应用中，评估实例的优劣往往是成本高昂且耗时的，例如人工反馈和物理模拟，这与提出新实例形成对比。特别是，这在强化学习中更为关键，因为需要评估与环境的新交互（即新实例）以提供学习所需的奖励信号。由于充分探索至关重要，从多样化的小批量中学习可以产生巨大影响并有助于缓解模式崩溃。在本文中，我们引入了强化学习中的多样化小批量选择，并提出使用行列式点过程来完成此任务。我们在一个实际问题，即药物发现的背景下研究了该框架。我们通过实验研究了我们提出的框架如何提高从头药物设计中化学探索的有效性，其中找到多样化和高质量的解决方案至关重要。我们使用三个成熟的分子生成预言机进行了多次生成步骤的全面评估。我们的实验得出结论，我们的多样化小批量选择框架可以显著提高解决方案的多样性，同时仍能获得高质量的解决方案。在药物发现中，这样的结果可能潜在地导致更快地满足未满足的药物需求。", "summary": "本研究提出了一种在强化学习中进行多样化小批量选择的新框架，利用行列式点过程来解决实例评估成本高昂的问题，尤其是在从头药物设计领域。该方法旨在促进高效的化学探索，通过确保学习批次的多样性来避免模式崩溃。实验证明，该框架在保持高解决方案质量的同时，显著提高了生成分子的多样性，有望加速新药的发现。", "keywords": "强化学习, 多样化小批量选择, 行列式点过程, 从头药物设计, 化学探索", "comments": "该论文通过引入行列式点过程进行强化学习中的多样化小批量选择，为解决高成本评估和模式崩溃问题提供了一个新颖的视角。将其应用于从头药物设计这一实际且重要的领域，突出了其潜在的应用价值。通过平衡多样性和质量，该方法有望提升药物发现的效率。"}}
{"id": "2506.21328", "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts", "authors": ["Jiajie Yang"], "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing.", "comment": "15 pages,4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21328v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21328v1", "AI": {"title_translation": "潜在原型路由：实现专家混合模型中近乎完美的负载均衡", "tldr": "提出潜在原型路由（LPR）框架，显著改善专家混合（MoE）模型中的专家负载不平衡问题，实现近乎完美的负载均衡。", "motivation": "当前的专家混合（MoE）系统存在严重的负载不平衡问题，即在训练和推理过程中只有一小部分专家被持续激活，导致模型容量和计算资源严重未被充分利用。", "method": "通过聚类视角重新审视专家路由，并提出了潜在原型路由（LPR），这是一个新颖的路由框架，它推广了现有方法，并在不损害下游性能的情况下促进平衡的专家利用。", "result": "在多个开源MoE模型（包括DeepSeek-V3、Qwen3-MoE和Mixtral）上，LPR将专家负载的基尼系数平均从0.70降低到0.035，并将最小-最大专家负载比从1e-6提高到0.70，实现了近乎完美的负载均衡。", "conclusion": "潜在原型路由（LPR）能够显著解决MoE模型中的负载不平衡问题，实现近乎完美的负载均衡，同时不影响模型性能，有效提升资源利用率。", "translation": "专家混合（MoE）架构已成为有效扩展大型语言模型（LLM）的关键策略。然而，当前的MoE系统存在严重的负载不平衡问题，在训练和推理过程中只有一小部分专家被持续激活，导致模型容量和计算资源严重未被充分利用。在这项工作中，我们通过聚类视角重新审视专家路由，并提出了潜在原型路由（LPR），这是一个新颖的路由框架，它推广了现有方法，同时在不损害下游性能的情况下促进平衡的专家利用。在多个开源MoE模型（包括DeepSeek-V3、Qwen3-MoE和Mixtral）上进行的大量实验表明，LPR将专家负载的基尼系数平均从0.70降低到0.035，将最小-最大专家负载比从1e-6提高到0.70，实现了近乎完美的负载均衡。", "summary": "这篇论文提出了一种名为“潜在原型路由”（LPR）的新型专家路由框架，旨在解决大型语言模型中专家混合（MoE）架构普遍存在的负载不平衡问题。LPR通过聚类视角优化专家分配，确保专家得到更平衡的利用，同时不影响模型性能。实验证明，LPR能显著降低专家负载的基尼系数并提高最小-最大负载比，从而在多种MoE模型中实现近乎完美的负载均衡，有效提升资源利用率。", "keywords": "专家混合, 负载均衡, 潜在原型路由, 大型语言模型, 路由", "comments": "这篇论文的创新点在于从聚类视角重新审视专家路由，并提出了LPR框架，有效解决了MoE模型中长期存在的负载不平衡问题。其重要性在于通过提高专家利用率，能够更高效地利用LLM的计算资源和模型容量，对于MoE架构的实际部署和扩展具有重要意义。"}}
{"id": "2506.21116", "title": "IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes", "authors": ["Yujia Liang", "Jile Jiao", "Zhicheng Wang", "Xuetao Feng", "Zixuan Ye", "Yuan Wang", "Hao Lu"], "summary": "Video Large Language Models (VideoLLMs) have demonstrated remarkable\nunderstanding capabilities, but are found struggling to tackle multi-shot\nscenarios,e.g., video clips with varying camera angles or scene changes. This\nchallenge can render failures such as instance identity forgetting and key\nframe negligence. In this work, we first attribute the challenge to the lack of\nmulti-shot annotations among existing datasets and therefore we introduce a new\ndataset termed MultiClip-Bench, featuring dense descriptions and\ninstruction-based question-answering pairs tailored for multi-shot scenarios.\nWe empirically find that the training set significantly boosts the multi-shot\nperformance, while the testing benchmark provides a reliable measure of the\nmodel capability in multi-shot scenarios. By further analyzing and discovering\nthat current models only encode instance features in a discrete or lossy\nmanner, at the risk of missing identity information, we then contribute a new\nmodel IPFormer-VideoLLM. Its key idea is the injection of instance-level\nfeatures as instance prompts through an efficient attention-based connector.\nThis allows for the aggregation of instance-specific information across scenes.\nExperiments demonstrate that our proposed dataset and model not only enhance\nthe multi-scene video understanding significantly, but also offer distinct\nadvantages across various video benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21116v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21116v1", "AI": {"title_translation": "IPFormer-VideoLLM：增强多镜头场景的多模态视频理解", "tldr": "IPFormer-VideoLLM通过引入新的数据集MultiClip-Bench和IPFormer-VideoLLM模型，解决了现有VideoLLM在多镜头场景中理解能力不足的问题，显著提升了多场景视频理解性能。", "motivation": "现有的视频大语言模型（VideoLLMs）在处理多镜头场景（如具有不同摄像机角度或场景变化的视频片段）时表现不佳，导致实例身份遗忘和关键帧忽略等问题。作者将这一挑战归因于现有数据集中缺乏多镜头标注。", "method": "首先，引入了一个名为MultiClip-Bench的新数据集，该数据集具有密集的描述和针对多镜头场景的基于指令的问答对。其次，分析发现当前模型以离散或有损的方式编码实例特征，可能丢失身份信息，因此提出了IPFormer-VideoLLM模型。该模型的关键思想是通过高效的基于注意力的连接器，将实例级特征作为实例提示注入，从而聚合跨场景的实例特定信息。", "result": "实验证明，所提出的数据集和模型不仅显著增强了多场景视频理解能力，而且在各种视频基准测试中也表现出明显的优势。", "conclusion": "本文通过引入专为多镜头场景设计的新数据集和新模型IPFormer-VideoLLM，有效解决了现有VideoLLM在复杂多镜头场景理解中的局限性，显著提升了视频理解性能。", "translation": "视频大语言模型（VideoLLMs）展现出卓越的理解能力，但在处理多镜头场景时（例如，具有不同摄像机角度或场景变化的视频片段）却发现它们力不从心。这一挑战可能导致诸如实例身份遗忘和关键帧忽略等问题。在这项工作中，我们首先将这一挑战归因于现有数据集中缺乏多镜头标注，因此我们引入了一个名为MultiClip-Bench的新数据集，该数据集具有密集的描述和专门为多镜头场景定制的基于指令的问答对。我们凭经验发现，训练集显著提升了多镜头性能，而测试基准则为模型在多镜头场景中的能力提供了可靠的衡量标准。通过进一步分析并发现当前模型仅以离散或有损的方式编码实例特征，存在丢失身份信息的风险，我们随后贡献了一个新模型IPFormer-VideoLLM。其核心思想是通过高效的基于注意力的连接器，将实例级特征作为实例提示注入。这使得跨场景的实例特定信息得以聚合。实验表明，我们提出的数据集和模型不仅显著增强了多场景视频理解能力，而且在各种视频基准测试中也提供了独特的优势。", "summary": "本文针对现有视频大语言模型（VideoLLMs）在多镜头场景理解中表现不佳的问题，提出了两项主要贡献：一是创建了新的多镜头场景数据集MultiClip-Bench，包含密集的描述和问答对，以弥补现有数据集中多镜头标注的不足；二是开发了IPFormer-VideoLLM模型，该模型通过注意力连接器注入实例级特征作为提示，有效聚合跨场景的实例信息，解决了现有模型在实例特征编码上的缺陷。实验证明，所提出的数据集和模型显著提升了多场景视频理解能力，并在多个视频基准上表现出色。", "keywords": "视频大语言模型, 多镜头场景, 实例级特征, 数据集, 视频理解", "comments": "该论文的创新点在于同时解决了多镜头视频理解的数据和模型两方面问题。通过构建专门的MultiClip-Bench数据集，弥补了现有数据集在多镜头标注上的不足，为多镜头场景的训练和评估提供了基础。同时，IPFormer-VideoLLM模型通过注入实例级特征作为提示，有效地聚合了跨场景的实例信息，解决了现有模型在处理实例身份连续性方面的挑战。这种数据和模型协同创新的方法，使其在多场景视频理解方面取得了显著进展，具有重要的研究价值和应用前景。"}}
{"id": "2506.21117", "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization", "authors": ["Jan Ackermann", "Jonas Kulhanek", "Shengqu Cai", "Haofei Xu", "Marc Pollefeys", "Gordon Wetzstein", "Leonidas Guibas", "Songyou Peng"], "summary": "In dynamic 3D environments, accurately updating scene representations over\ntime is crucial for applications in robotics, mixed reality, and embodied AI.\nAs scenes evolve, efficient methods to incorporate changes are needed to\nmaintain up-to-date, high-quality reconstructions without the computational\noverhead of re-optimizing the entire scene. This paper introduces CL-Splats,\nwhich incrementally updates Gaussian splatting-based 3D representations from\nsparse scene captures. CL-Splats integrates a robust change-detection module\nthat segments updated and static components within the scene, enabling focused,\nlocal optimization that avoids unnecessary re-computation. Moreover, CL-Splats\nsupports storing and recovering previous scene states, facilitating temporal\nsegmentation and new scene-analysis applications. Our extensive experiments\ndemonstrate that CL-Splats achieves efficient updates with improved\nreconstruction quality over the state-of-the-art. This establishes a robust\nfoundation for future real-time adaptation in 3D scene reconstruction tasks.", "comment": "ICCV 2025, Project Page: https://cl-splats.github.io", "pdf_url": "http://arxiv.org/pdf/2506.21117v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21117v1", "AI": {"title_translation": "CL-Splats：基于局部优化的三维高斯泼溅持续学习", "tldr": "CL-Splats是一种增量更新高斯泼溅三维表示的方法，通过变化检测和局部优化实现高效更新和高质量重建，适用于动态3D环境。", "motivation": "在动态三维环境中，精确地随时间更新场景表示对于机器人、混合现实和具身AI应用至关重要。随着场景的演变，需要高效的方法来整合变化，以保持最新、高质量的重建，同时避免重新优化整个场景的计算开销。", "method": "本文提出了CL-Splats，它从稀疏场景捕获中增量更新基于高斯泼溅的三维表示。CL-Splats集成了一个鲁棒的变化检测模块，该模块能分割场景中更新和静态的组件，从而实现聚焦的局部优化，避免不必要的重新计算。此外，CL-Splats支持存储和恢复先前的场景状态。", "result": "广泛的实验表明，CL-Splats实现了高效更新，并且重建质量优于现有技术。", "conclusion": "CL-Splats为未来3D场景重建任务中的实时适应奠定了坚实的基础。", "translation": "在动态三维环境中，随时间准确更新场景表示对于机器人、混合现实和具身AI应用至关重要。随着场景的演变，需要高效的方法来整合变化，以保持最新、高质量的重建，同时避免重新优化整个场景的计算开销。本文介绍了CL-Splats，它从稀疏场景捕获中增量更新基于高斯泼溅的三维表示。CL-Splats集成了一个鲁棒的变化检测模块，该模块能分割场景中更新和静态的组件，从而实现聚焦的局部优化，避免不必要的重新计算。此外，CL-Splats支持存储和恢复先前的场景状态，有助于时间分割和新的场景分析应用。我们的广泛实验表明，CL-Splats实现了高效更新，并且重建质量优于现有技术。这为未来三维场景重建任务中的实时适应奠定了坚实的基础。", "summary": "CL-Splats是一种用于动态3D环境中持续学习高斯泼溅三维表示的方法。它通过引入一个鲁棒的变化检测模块来识别场景中的更新和静态部分，从而实现聚焦的局部优化，避免了对整个场景的重新计算，提高了更新效率和重建质量。该方法还支持存储和恢复场景历史状态，为实时3D场景重建提供了新途径。", "keywords": "持续学习, 高斯泼溅, 局部优化, 3D重建, 动态场景", "comments": "本文的创新点在于将变化检测与局部优化相结合，应用于高斯泼溅的持续学习，显著降低了动态场景更新的计算成本，并提高了重建质量。其支持历史状态恢复的功能也为未来的场景分析应用提供了潜力。这对于需要实时、高效3D场景表示的应用领域具有重要意义。"}}
{"id": "2506.21408", "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference", "authors": ["Colin Samplawski", "Adam D. Cobb", "Manoj Acharya", "Ramneet Kaur", "Susmit Jha"], "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.", "comment": "Accepted at UAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21408v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21408v1", "AI": {"title_translation": "通过随机变分子空间推断实现大型语言模型的可扩展贝叶斯低秩适应", "tldr": "本文提出了ScalaBL，一种新的贝叶斯低秩适应方法，通过在低维子空间中进行推断，显著减少了额外参数需求，并能扩展到更大的LLM，同时保持与现有技术相当的性能，解决了贝叶斯LLM的可扩展性问题。", "motivation": "尽管大型语言模型（LLMs）广泛使用，但它们存在幻觉错误信息和校准不佳的问题，这使得LLM的不确定性量化至关重要，尤其是在自动驾驶和医疗保健等高风险领域。现有基于贝叶斯深度学习的方法通过对微调模型的低秩适应（LoRA）参数进行推断，虽然有效，但由于需要额外的参数，难以扩展到更大的LLM。", "method": "本文提出了可扩展贝叶斯低秩适应（ScalaBL），通过随机变分子空间推断实现。该方法在LoRA秩r的r维子空间中进行贝叶斯推断。通过将LoRA参数重新用作投影矩阵，能够将来自子空间的样本映射到LLM的完整权重空间。所有参数都使用随机变分推断进行学习。", "result": "ScalaBL在仅需要约1000个额外参数的情况下，能够与最先进的方法达到竞争性性能。此外，它能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是先前工作的四倍。", "conclusion": "本文提出的ScalaBL方法通过在低维子空间中进行贝叶斯推断，成功解决了大型语言模型贝叶斯不确定性量化的可扩展性问题，并在参数效率和模型规模方面取得了显著的改进，同时保持了高性能。", "translation": "尽管大型语言模型（LLMs）广泛使用，但它们存在幻觉错误信息和校准不佳的问题。这使得这些模型的不确定性量化至关重要，尤其是在自动驾驶和医疗保健等高风险领域。先前的工作通过对微调模型的低秩适应（LoRA）参数进行推断，使得基于贝叶斯深度学习的方法解决了这个问题，变得更易处理。然而，这些方法由于需要比LoRA更多的额外参数，难以扩展到更大的LLM。在这项工作中，我们提出了通过随机变分子空间推断实现的可扩展贝叶斯低秩适应（ScalaBL）。我们对LoRA秩r的r维子空间进行贝叶斯推断。通过将LoRA参数重新用作投影矩阵，我们能够将来自此子空间的样本映射到LLM的完整权重空间。这使我们能够使用随机变分推断学习我们方法的所有参数。尽管我们的子空间维度较低，但我们能够以仅需要约1000个额外参数的代价，实现与最先进方法相当的性能。此外，它使我们能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是先前工作的四倍。", "summary": "本文提出了一种名为ScalaBL的可扩展贝叶斯低秩适应方法，旨在解决大型语言模型的不确定性量化问题及其现有贝叶斯方法的可扩展性挑战。ScalaBL通过在低维子空间中进行贝叶斯推断，并将LoRA参数用作投影矩阵，实现了将子空间样本映射到LLM的全权重空间。该方法仅需约1000个额外参数，即可达到与现有技术相当的性能，并成功扩展到比以往更大的贝叶斯LLM。", "keywords": "贝叶斯深度学习, 大型语言模型, 低秩适应, 不确定性量化, 随机变分推断", "comments": "本文的创新点在于提出了ScalaBL，通过利用低维子空间推断和重新利用LoRA参数作为投影矩阵，显著降低了贝叶斯LLM的额外参数需求，并提高了其可扩展性。这对于在资源受限的环境下实现LLM的不确定性量化具有重要意义，尤其是在高风险应用领域。其能够扩展到更大模型的能力是该研究的一个重要突破。"}}
{"id": "2506.21240", "title": "Zero-Shot Learning for Obsolescence Risk Forecasting", "authors": ["Elie Saad", "Aya Mrabah", "Mariem Besbes", "Marc Zolghadri", "Victor Czmil", "Claude Baron", "Vincent Bourgeois"], "summary": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21240v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21240v1", "AI": {"title_translation": "零样本学习在报废风险预测中的应用", "tldr": "本文提出了一种利用零样本学习（ZSL）和大型语言模型（LLMs）来预测电子元件报废风险的新方法，以应对数据稀缺问题，并在实际数据集中展现了有效性。", "motivation": "电子元件报废给相关行业带来了巨大的挑战，导致成本增加和系统安全与可用性中断。准确的报废风险预测至关重要，但受限于可靠数据的缺乏。", "method": "本文提出了一种新颖的方法，利用零样本学习（ZSL）结合大型语言模型（LLMs）来预测报废风险，通过利用表格数据集中的领域特定知识来解决数据限制。该方法应用于两个真实世界数据集。", "result": "该方法在两个真实世界数据集中展示了有效的风险预测能力。对四种LLM的比较评估强调了为特定预测任务选择合适模型的重要性。", "conclusion": "利用零样本学习和大型语言模型可以有效预测报废风险，且选择合适的LLM对于特定的预测任务至关重要。", "translation": "电子元件报废给依赖电子元件的行业带来了巨大的挑战，导致成本增加以及系统安全性和可用性的中断。准确的报废风险预测至关重要，但受到可靠数据缺乏的阻碍。本文提出了一种利用零样本学习（ZSL）与大型语言模型（LLMs）相结合来预测报废风险的新方法，通过利用表格数据集中的领域特定知识来解决数据限制。该方法应用于两个真实世界数据集，展示了有效的风险预测能力。对四种LLM的比较评估强调了为特定预测任务选择合适模型的重要性。", "summary": "本文针对电子元件报废风险预测中数据匮乏的挑战，提出了一种创新性的零样本学习（ZSL）方法，结合大型语言模型（LLMs）和领域特定表格数据进行风险预测。该方法在实际数据集中表现出有效性，并且研究强调了为特定预测任务选择合适LLM的重要性。", "keywords": "零样本学习, 报废风险预测, 大型语言模型, 数据稀缺, 电子元件", "comments": "该论文的创新点在于将零样本学习和大型语言模型应用于传统上数据稀缺的报废风险预测领域，为解决数据限制问题提供了一种新颖的思路。其重要性在于能够帮助行业提高预测准确性，减少成本和系统中断。论文还通过比较评估不同LLM，为实际应用提供了选型指导。"}}
{"id": "2506.21132", "title": "Learning to See in the Extremely Dark", "authors": ["Hai Jiang", "Binhao Guan", "Zhen Liu", "Xiaohong Liu", "Jian Yu", "Zheng Liu", "Songchen Han", "Shuaicheng Liu"], "summary": "Learning-based methods have made promising advances in low-light RAW image\nenhancement, while their capability to extremely dark scenes where the\nenvironmental illuminance drops as low as 0.0001 lux remains to be explored due\nto the lack of corresponding datasets. To this end, we propose a\npaired-to-paired data synthesis pipeline capable of generating well-calibrated\nextremely low-light RAW images at three precise illuminance ranges of 0.01-0.1\nlux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB\nreferences to comprise a large-scale paired dataset named\nSee-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement\napproaches. Furthermore, we propose a diffusion-based framework that leverages\nthe generative ability and intrinsic denoising property of diffusion models to\nrestore visually pleasing results from extremely low-SNR RAW inputs, in which\nan Adaptive Illumination Correction Module (AICM) and a color consistency loss\nare introduced to ensure accurate exposure correction and color restoration.\nExtensive experiments on the proposed SIED and publicly available benchmarks\ndemonstrate the effectiveness of our method. The code and dataset are available\nat https://github.com/JianghaiSCU/SIED.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21132v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21132v1", "AI": {"title_translation": "在极暗环境下学习可见", "tldr": "针对极暗场景下（低至0.0001 lux）学习型低光RAW图像增强方法因缺乏数据集而受限的问题，本文提出了一种配对数据合成管线，生成了大规模的SIED数据集，并提出了一种基于扩散模型的框架，结合自适应光照校正模块（AICM）和色彩一致性损失，有效提升了极低信噪比RAW输入的图像质量。", "motivation": "现有的学习型方法在极低光照（低至0.0001 lux）的极暗场景下的RAW图像增强能力尚未被充分探索，主要原因是缺乏相应的训练数据集。", "method": "本文提出了一种配对数据合成管线，能够生成在0.01-0.1 lux、0.001-0.01 lux和0.0001-0.001 lux三个精确照度范围内的校准良好的极低光RAW图像，并结合高质量sRGB参考图像构建了大规模的SIED数据集。此外，本文提出了一种基于扩散模型的框架，利用扩散模型的生成能力和内在去噪特性来从极低信噪比的RAW输入中恢复视觉上令人满意的结果，并引入了自适应光照校正模块（AICM）和色彩一致性损失以确保准确的曝光校正和色彩恢复。", "result": "在所提出的SIED数据集和公开基准上的大量实验证明了本文方法的有效性。", "conclusion": "本文提出的数据合成管线和基于扩散模型的方法能够有效解决极暗场景下RAW图像增强的数据稀缺问题，并显著提升图像的视觉质量。", "translation": "学习型方法在低光RAW图像增强方面取得了可喜的进展，但其在环境照度低至0.0001 lux的极暗场景中的能力仍有待探索，原因是缺乏相应的数据集。为此，我们提出了一种配对数据合成管线，能够生成在0.01-0.1 lux、0.001-0.01 lux和0.0001-0.001 lux三个精确照度范围内的校准良好的极低光RAW图像，并结合高质量sRGB参考图像，构成了一个名为See-in-the-Extremely-Dark (SIED) 的大规模配对数据集，用于评估低光RAW图像增强方法。此外，我们提出了一种基于扩散模型的框架，该框架利用扩散模型的生成能力和内在去噪特性，从极低信噪比的RAW输入中恢复视觉上令人满意的结果，其中引入了自适应光照校正模块（AICM）和色彩一致性损失，以确保准确的曝光校正和色彩恢复。在所提出的SIED数据集和公开基准上的大量实验证明了我们方法的有效性。代码和数据集可在https://github.com/JianghaiSCU/SIED获取。", "summary": "本文针对极暗场景下低光RAW图像增强中数据集缺乏的问题，提出了一种创新的配对数据合成管线，构建了大规模的SIED数据集。在此基础上，提出了一种基于扩散模型的框架，该框架结合了自适应光照校正模块（AICM）和色彩一致性损失，能够有效地从极低信噪比的RAW输入中恢复高质量的图像。实验结果验证了所提方法的有效性。", "keywords": "低光增强, RAW图像, 扩散模型, 数据合成, 极暗场景", "comments": "本文的创新点在于通过合成数据解决了极暗场景下图像增强的数据稀缺性问题，并提出了一个专门针对此类挑战性条件优化的扩散模型。SIED数据集的创建是其重要贡献。"}}
{"id": "2506.21474", "title": "Logios : An open source Greek Polytonic Optical Character Recognition system", "authors": ["Perifanos Konstantinos", "Goutsos Dionisis"], "summary": "In this paper, we present an Optical Character Recognition (OCR) system\nspecifically designed for the accurate recognition and digitization of Greek\npolytonic texts. By leveraging the combined strengths of convolutional layers\nfor feature extraction and recurrent layers for sequence learning, our system\naddresses the unique challenges posed by Greek polytonic scripts. This approach\naims to overcome the limitations of traditional OCR methods, offering\nsignificant improvements in accuracy and efficiency. We release the underlying\nmodel as an open-source library and make our OCR platform available for\nacademic use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21474v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21474v1", "AI": {"title_translation": "Logios：一个开源的希腊语多音素光学字符识别系统", "tldr": "Logios是一个开源的希腊语多音素OCR系统，结合了卷积层和循环层，提高了识别准确性和效率。", "motivation": "旨在克服传统OCR方法在处理希腊语多音素文本时的局限性，提高识别准确性和效率。", "method": "结合卷积层进行特征提取和循环层进行序列学习，以处理希腊语多音素文字的独特挑战。", "result": "显著提高了准确性和效率，并以开源库形式发布了底层模型，OCR平台可供学术使用。", "conclusion": "Logios系统成功地为希腊语多音素文本提供了一个高准确度和效率的OCR解决方案，并且是开源的，有利于学术界使用和发展。", "translation": "在本文中，我们介绍了一个光学字符识别（OCR）系统，该系统专门设计用于希腊语多音素文本的准确识别和数字化。通过利用卷积层进行特征提取和循环层进行序列学习的综合优势，我们的系统解决了希腊语多音素文字带来的独特挑战。这种方法旨在克服传统OCR方法的局限性，在准确性和效率方面提供了显著的改进。我们以开源库的形式发布了底层模型，并使我们的OCR平台可供学术使用。", "summary": "本文提出了Logios，一个开源的希腊语多音素光学字符识别（OCR）系统。该系统结合了卷积层和循环层，有效处理希腊语多音素文本的复杂性，显著提升了识别的准确性和效率。其底层模型已开源，平台可供学术使用。", "keywords": "光学字符识别, 希腊语多音素, 深度学习, 开源, 文本数字化", "comments": "该论文的创新之处在于其专门针对希腊语多音素文本的OCR系统设计，并结合了深度学习中的卷积层和循环层来解决这一特定挑战。其开源性质极大地促进了学术研究和应用发展，为希腊语文献的数字化提供了重要工具。"}}
{"id": "2506.21135", "title": "YOLO-FDA: Integrating Hierarchical Attention and Detail Enhancement for Surface Defect Detection", "authors": ["Jiawei Hu"], "summary": "Surface defect detection in industrial scenarios is both crucial and\ntechnically demanding due to the wide variability in defect types, irregular\nshapes and sizes, fine-grained requirements, and complex material textures.\nAlthough recent advances in AI-based detectors have improved performance,\nexisting methods often suffer from redundant features, limited detail\nsensitivity, and weak robustness under multiscale conditions. To address these\nchallenges, we propose YOLO-FDA, a novel YOLO-based detection framework that\nintegrates fine-grained detail enhancement and attention-guided feature fusion.\nSpecifically, we adopt a BiFPN-style architecture to strengthen bidirectional\nmultilevel feature aggregation within the YOLOv5 backbone. To better capture\nfine structural changes, we introduce a Detail-directional Fusion Module (DDFM)\nthat introduces a directional asymmetric convolution in the second-lowest layer\nto enrich spatial details and fuses the second-lowest layer with low-level\nfeatures to enhance semantic consistency. Furthermore, we propose two novel\nattention-based fusion strategies, Attention-weighted Concatenation (AC) and\nCross-layer Attention Fusion (CAF) to improve contextual representation and\nreduce feature noise. Extensive experiments on benchmark datasets demonstrate\nthat YOLO-FDA consistently outperforms existing state-of-the-art methods in\nterms of both accuracy and robustness across diverse types of defects and\nscales.", "comment": "14 pages, 6 figures. Submitted to The 8th Chinese Conference on\n  Pattern Recognition and Computer Vision", "pdf_url": "http://arxiv.org/pdf/2506.21135v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21135v1", "AI": {"title_translation": "YOLO-FDA：集成层级注意力与细节增强的表面缺陷检测", "tldr": "YOLO-FDA是一种新的基于YOLO的检测框架，通过细节增强和注意力引导的特征融合，解决了工业表面缺陷检测中现有方法冗余特征、细节敏感度有限和多尺度鲁棒性差的问题，并在基准数据集上超越了现有SOTA方法。", "motivation": "工业场景中的表面缺陷检测至关重要且技术要求高，因为缺陷类型多样、形状尺寸不规则、要求精细且材质纹理复杂。尽管基于AI的检测器有所改进，但现有方法常存在特征冗余、细节敏感度有限以及多尺度条件下鲁棒性差的问题。", "method": "本文提出了YOLO-FDA，一个新颖的基于YOLO的检测框架，集成了细粒度细节增强和注意力引导的特征融合。具体来说，采用BiFPN风格的架构来加强YOLOv5骨干网络内的双向多级特征聚合。为更好地捕获精细结构变化，引入了细节方向融合模块（DDFM），该模块在第二低层引入定向非对称卷积以丰富空间细节，并将第二低层与低级特征融合以增强语义一致性。此外，提出了两种新颖的基于注意力的融合策略：注意力加权拼接（AC）和跨层注意力融合（CAF），以改善上下文表示并减少特征噪声。", "result": "在基准数据集上的大量实验表明，YOLO-FDA在各种缺陷类型和尺度下，在准确性和鲁棒性方面始终优于现有的最先进方法。", "conclusion": "YOLO-FDA通过创新的细节增强和注意力引导的特征融合策略，有效解决了工业表面缺陷检测中的挑战，并在准确性和鲁棒性上达到了领先水平，证明了其在实际应用中的潜力。", "translation": "工业场景中的表面缺陷检测既关键又技术要求高，因为缺陷类型广泛多变、形状和尺寸不规则、精细化要求以及复杂的材料纹理。尽管近期基于AI的检测器取得了进展，提高了性能，但现有方法通常存在特征冗余、细节敏感度有限以及多尺度条件下鲁棒性弱的问题。为了应对这些挑战，我们提出了YOLO-FDA，一个新颖的基于YOLO的检测框架，它集成了细粒度细节增强和注意力引导的特征融合。具体来说，我们采用了一种BiFPN风格的架构来加强YOLOv5骨干网络内的双向多级特征聚合。为了更好地捕获精细结构变化，我们引入了一个细节方向融合模块（DDFM），该模块在第二低层引入定向非对称卷积以丰富空间细节，并将第二低层与低级特征融合以增强语义一致性。此外，我们提出了两种新颖的基于注意力的融合策略，即注意力加权拼接（AC）和跨层注意力融合（CAF），以改善上下文表示并减少特征噪声。在基准数据集上的大量实验表明，YOLO-FDA在各种缺陷类型和尺度下，在准确性和鲁棒性方面始终优于现有的最先进方法。", "summary": "YOLO-FDA是一个针对工业表面缺陷检测提出的新型YOLO框架。它旨在解决现有方法在处理多变缺陷时面临的特征冗余、细节敏感度不足和鲁棒性差的问题。通过采用BiFPN风格的特征聚合、引入DDFM模块增强细节捕获，以及提出AC和CAF两种注意力融合策略，YOLO-FDA显著提升了上下文表示并降低了特征噪声。实验证明，该方法在准确性和鲁棒性上均超越了现有SOTA方法。", "keywords": "表面缺陷检测, YOLO-FDA, 细节增强, 注意力机制, 特征融合", "comments": "该论文提出的YOLO-FDA框架在解决工业表面缺陷检测的挑战方面具有创新性。其核心贡献在于结合了细粒度细节增强和注意力引导的特征融合，特别是DDFM模块和两种新型注意力融合策略（AC和CAF）的设计，有效提升了模型对精细缺陷的识别能力和多尺度鲁棒性。这对于实际工业应用中高精度、高鲁棒性的缺陷检测具有重要意义。"}}
{"id": "2506.21291", "title": "Improved seeding strategies for k-means and k-GMM", "authors": ["Guillaume Carrière", "Frédéric Cazals"], "summary": "We revisit the randomized seeding techniques for k-means clustering and k-GMM\n(Gaussian Mixture model fitting with Expectation-Maximization), formalizing\ntheir three key ingredients: the metric used for seed sampling, the number of\ncandidate seeds, and the metric used for seed selection. This analysis yields\nnovel families of initialization methods exploiting a lookahead\nprinciple--conditioning the seed selection to an enhanced coherence with the\nfinal metric used to assess the algorithm, and a multipass strategy to tame\ndown the effect of randomization.\n  Experiments show a consistent constant factor improvement over classical\ncontenders in terms of the final metric (SSE for k-means, log-likelihood for\nk-GMM), at a modest overhead. In particular, for k-means, our methods improve\non the recently designed multi-swap strategy, which was the first one to\noutperform the greedy k-means++ seeding.\n  Our experimental analysis also shed light on subtle properties of k-means\noften overlooked, including the (lack of) correlations between the SSE upon\nseeding and the final SSE, the variance reduction phenomena observed in\niterative seeding methods, and the sensitivity of the final SSE to the pool\nsize for greedy methods.\n  Practically, our most effective seeding methods are strong candidates to\nbecome one of the--if not the--standard techniques. From a theoretical\nperspective, our formalization of seeding opens the door to a new line of\nanalytical approaches.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.21291v1", "categories": ["cs.LG", "F.2; G.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21291v1", "AI": {"title_translation": "改进的k-means和k-GMM播种策略", "tldr": "本文重新审视并改进了k-means和k-GMM的随机播种技术，通过引入前瞻性原则和多通道策略，实现了在最终评估指标上的显著性能提升，并揭示了k-means的一些细微特性。", "motivation": "本文旨在改进k-means聚类和k-GMM（高斯混合模型拟合）的随机播种技术，通过形式化其关键组成部分，以期在性能上超越现有方法，特别是最近设计的multi-swap策略和贪婪的k-means++播种方法。", "method": "作者重新审视了k-means和k-GMM的随机播种技术，并形式化了其三个关键要素：用于种子采样的度量、候选种子的数量以及用于种子选择的度量。在此基础上，提出了利用“前瞻性原则”（根据最终评估算法的度量标准增强种子选择的一致性）和“多通道策略”（以驯服随机化效应）的新型初始化方法家族。", "result": "实验结果表明，与经典竞争者相比，所提出的方法在最终度量（k-means的SSE，k-GMM的对数似然）方面实现了持续的常数因子改进，且开销适中。特别是对于k-means，这些方法改进了最近设计的multi-swap策略，后者是第一个超越贪婪k-means++播种的方法。实验分析还揭示了k-means的一些常被忽视的微妙特性，包括播种时的SSE与最终SSE之间（缺乏）相关性、迭代播种方法中观察到的方差减少现象，以及最终SSE对贪婪方法池大小的敏感性。", "conclusion": "本文提出的最有效的播种方法有望成为标准技术之一。从理论角度看，播种的形式化为新的分析方法打开了大门。", "translation": "我们重新审视了k-means聚类和k-GMM（通过期望最大化进行高斯混合模型拟合）的随机播种技术，并形式化了其三个关键组成部分：用于种子采样的度量、候选种子的数量以及用于种子选择的度量。这项分析产生了利用前瞻性原则（根据用于评估算法的最终度量增强种子选择的一致性）和多通道策略（以驯服随机化效应）的新型初始化方法家族。\n实验表明，在最终度量（k-means的SSE，k-GMM的对数似然）方面，与经典竞争者相比，在适度的开销下，实现了持续的常数因子改进。特别是对于k-means，我们的方法改进了最近设计的multi-swap策略，这是第一个超越贪婪k-means++播种的方法。\n我们的实验分析还揭示了k-means的一些常被忽视的细微特性，包括播种时的SSE与最终SSE之间（缺乏）相关性、迭代播种方法中观察到的方差减少现象，以及最终SSE对贪婪方法池大小的敏感性。\n实际上，我们最有效的播种方法是成为标准技术之一（如果不是唯一）的有力候选者。从理论角度看，我们对播种的形式化为新的分析方法打开了大门。", "summary": "本文重新审视了k-means和k-GMM的随机播种技术，并对其关键组成部分进行了形式化。在此基础上，提出了基于前瞻性原则和多通道策略的新型初始化方法。实验证明，这些方法在最终评估指标上取得了显著且持续的改进，性能超越了现有的先进播种策略，并揭示了k-means的一些深层特性。这些新方法有望成为业界标准，并为播种理论研究开辟了新方向。", "keywords": "k-means, k-GMM, 播种策略, 初始化, 聚类", "comments": "本文的创新之处在于对k-means和k-GMM随机播种技术的深入形式化，并由此提出了结合“前瞻性原则”和“多通道策略”的新型初始化方法。其重要性体现在两方面：实践上，它提供了在性能上超越现有顶级方法的播种策略，有望成为新的工业标准；理论上，这种形式化为播种算法的分析打开了新的研究路径。此外，实验分析还揭示了k-means的一些常被忽视的特性，增加了对算法行为的理解。"}}
{"id": "2506.21150", "title": "Tree-based Semantic Losses: Application to Sparsely-supervised Large Multi-class Hyperspectral Segmentation", "authors": ["Junwen Wang", "Oscar Maccormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "summary": "Hyperspectral imaging (HSI) shows great promise for surgical applications,\noffering detailed insights into biological tissue differences beyond what the\nnaked eye can perceive. Refined labelling efforts are underway to train vision\nsystems to distinguish large numbers of subtly varying classes. However,\ncommonly used learning methods for biomedical segmentation tasks penalise all\nerrors equivalently and thus fail to exploit any inter-class semantics in the\nlabel space. In this work, we introduce two tree-based semantic loss functions\nwhich take advantage of a hierarchical organisation of the labels. We further\nincorporate our losses in a recently proposed approach for training with\nsparse, background-free annotations. Extensive experiments demonstrate that our\nproposed method reaches state-of-the-art performance on a sparsely annotated\nHSI dataset comprising $107$ classes organised in a clinically-defined semantic\ntree structure. Furthermore, our method enables effective detection of\nout-of-distribution (OOD) pixels without compromising segmentation performance\non in-distribution (ID) pixels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21150v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21150v1", "AI": {"title_translation": "基于树的语义损失：应用于稀疏监督的大规模多类别高光谱分割", "tldr": "该研究引入了两种基于树的语义损失函数，利用标签的层次结构，以提高在稀疏标注的大规模多类别高光谱分割任务中的性能，并能有效检测分布外像素。", "motivation": "现有生物医学分割任务的学习方法对所有错误同等惩罚，未能利用标签空间中的类间语义信息，尤其是在处理大规模、类别细微差异的高光谱图像分割时，这限制了高光谱成像在手术应用中的潜力。", "method": "本文提出了两种基于树的语义损失函数，这些函数利用了标签的层次组织结构。这些损失函数被整合到一种处理稀疏、无背景标注的训练方法中。", "result": "该方法在包含107个类别、按临床定义语义树结构组织的高光谱图像稀疏标注数据集上达到了最先进的性能。此外，该方法在不损害分布内像素分割性能的情况下，还能有效检测分布外（OOD）像素。", "conclusion": "基于树的语义损失函数能够有效利用标签的层次结构，显著提升大规模多类别高光谱分割的性能，并增强了对分布外像素的检测能力。", "translation": "高光谱成像（HSI）在外科应用中展现出巨大潜力，能够提供超越肉眼所能感知的生物组织差异的详细信息。目前正在进行精细的标注工作，以训练视觉系统区分大量细微变化的类别。然而，生物医学分割任务中常用的学习方法对所有错误都进行同等惩罚，因此未能利用标签空间中的任何类间语义信息。在这项工作中，我们引入了两种基于树的语义损失函数，它们利用了标签的层次组织结构。我们进一步将我们的损失函数整合到最近提出的一种使用稀疏、无背景标注进行训练的方法中。大量的实验表明，我们提出的方法在一个稀疏标注的高光谱图像数据集上达到了最先进的性能，该数据集包含107个类别，这些类别以临床定义的语义树结构进行组织。此外，我们的方法能够在不损害分布内（ID）像素分割性能的情况下，有效检测分布外（OOD）像素。", "summary": "该论文针对大规模多类别高光谱图像分割中现有方法未能利用类间语义信息的问题，提出了两种基于树的语义损失函数。这些损失函数利用标签的层次结构，并被整合到处理稀疏标注的训练框架中。实验证明，该方法在包含107个类别的临床定义高光谱数据集上达到了最先进的性能，并能有效识别分布外像素，同时保持对分布内像素的分割精度。", "keywords": "语义损失, 高光谱分割, 树结构, 稀疏监督, 分布外检测", "comments": "本文的创新点在于引入了基于树的语义损失函数，有效地利用了标签的层次结构，解决了传统损失函数对所有错误同等惩罚而忽略类间语义的问题。这对于处理类别众多且关系复杂的生物医学高光谱图像分割任务具有重要意义。此外，该方法能够检测分布外像素，增加了其实用性和鲁棒性，在实际外科应用中具有潜在价值。"}}
{"id": "2506.21546", "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation", "authors": ["Xinzhuo Li", "Adheesh Juvekar", "Xingyou Liu", "Muntasir Wahed", "Kiet A. Nguyen", "Ismini Lourentzou"], "summary": "Recent progress in vision-language segmentation has significantly advanced\ngrounded visual understanding. However, these models often exhibit\nhallucinations by producing segmentation masks for objects not grounded in the\nimage content or by incorrectly labeling irrelevant regions. Existing\nevaluation protocols for segmentation hallucination primarily focus on label or\ntextual hallucinations without manipulating the visual context, limiting their\ncapacity to diagnose critical failures. In response, we introduce\nHalluSegBench, the first benchmark specifically designed to evaluate\nhallucinations in visual grounding through the lens of counterfactual visual\nreasoning. Our benchmark consists of a novel dataset of 1340 counterfactual\ninstance pairs spanning 281 unique object classes, and a set of newly\nintroduced metrics that quantify hallucination sensitivity under visually\ncoherent scene edits. Experiments on HalluSegBench with state-of-the-art\nvision-language segmentation models reveal that vision-driven hallucinations\nare significantly more prevalent than label-driven ones, with models often\npersisting in false segmentation, highlighting the need for counterfactual\nreasoning to diagnose grounding fidelity.", "comment": "Project webpage: https://plan-lab.github.io/hallusegbench/", "pdf_url": "http://arxiv.org/pdf/2506.21546v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21546v1", "AI": {"title_translation": "HalluSegBench：用于分割幻觉评估的反事实视觉推理", "tldr": "HalluSegBench是一个新的基准，用于通过反事实视觉推理评估视觉-语言分割模型中的分割幻觉。它包含一个新数据集和新指标，实验表明视觉驱动的幻觉比标签驱动的更普遍。", "motivation": "当前的视觉-语言分割模型常出现幻觉，即为图像中不存在或不相关的对象生成分割掩码。现有评估协议主要关注标签或文本幻觉，未能通过操纵视觉上下文来诊断关键故障，因此需要一种新的方法来评估视觉接地中的幻觉。", "method": "我们引入了HalluSegBench，这是第一个专门设计用于通过反事实视觉推理评估视觉接地中幻觉的基准。它包含一个由1340个反事实实例对组成的新数据集（涵盖281个独特的对象类别），以及一套新引入的指标，用于量化视觉连贯场景编辑下的幻觉敏感性。", "result": "在HalluSegBench上对最先进的视觉-语言分割模型进行的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型经常持续出现错误的分割。", "conclusion": "本研究强调了需要反事实推理来诊断视觉接地保真度，以解决视觉-语言分割模型中普遍存在的视觉驱动幻觉问题。", "translation": "最近视觉-语言分割的进展显著推动了基础视觉理解。然而，这些模型经常表现出幻觉，即为图像内容中未接地的对象生成分割掩码，或错误地标记不相关的区域。现有的分割幻觉评估协议主要关注标签或文本幻觉，而没有操纵视觉上下文，这限制了它们诊断关键故障的能力。因此，我们引入了HalluSegBench，这是第一个专门设计用于通过反事实视觉推理来评估视觉接地中幻觉的基准。我们的基准包含一个由1340个反事实实例对组成的新数据集，涵盖281个独特的对象类别，以及一套新引入的指标，用于量化视觉连贯场景编辑下的幻觉敏感性。在HalluSegBench上对最先进的视觉-语言分割模型进行的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型经常持续出现错误的分割，这突出表明需要反事实推理来诊断接地保真度。", "summary": "HalluSegBench是一个新颖的基准，旨在解决视觉-语言分割模型中普遍存在的分割幻觉问题。它通过引入一个包含1340个反事实实例对的数据集和一套新的评估指标，利用反事实视觉推理来诊断模型对视觉上下文变化的敏感性。实验结果揭示，视觉驱动的幻觉比标签驱动的更为普遍，强调了通过反事实方法评估模型视觉接地能力的重要性。", "keywords": "分割幻觉, 反事实视觉推理, 视觉-语言分割, HalluSegBench, 视觉接地", "comments": "该论文的创新之处在于首次提出了一个基于反事实视觉推理的基准HalluSegBench，专门用于评估视觉-语言分割模型中的分割幻觉。通过引入新的数据集和指标，它能够更深入地诊断模型对视觉上下文变化的敏感性，揭示了视觉驱动幻觉的普遍性，这对于推动鲁棒的视觉接地理解至关重要。"}}
{"id": "2506.21151", "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels", "authors": ["Aida Moafi", "Danial Moafi", "Evgeny M. Mirkes", "Gerry P. McCann", "Abbas S. Alatrany", "Jayanth R. Arnold", "Mostafa Mehdipour Ghazi"], "summary": "The accurate segmentation of myocardial scars from cardiac MRI is essential\nfor clinical assessment and treatment planning. In this study, we propose a\nrobust deep-learning pipeline for fully automated myocardial scar detection and\nsegmentation by fine-tuning state-of-the-art models. The method explicitly\naddresses challenges of label noise from semi-automatic annotations, data\nheterogeneity, and class imbalance through the use of Kullback-Leibler loss and\nextensive data augmentation. We evaluate the model's performance on both acute\nand chronic cases and demonstrate its ability to produce accurate and smooth\nsegmentations despite noisy labels. In particular, our approach outperforms\nstate-of-the-art models like nnU-Net and shows strong generalizability in an\nout-of-distribution test set, highlighting its robustness across various\nimaging conditions and clinical tasks. These results establish a reliable\nfoundation for automated myocardial scar quantification and support the broader\nclinical adoption of deep learning in cardiac imaging.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21151v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21151v1", "AI": {"title_translation": "用于心脏MRI心肌疤痕分割的鲁棒深度学习，解决标签噪声问题", "tldr": "本研究提出了一种鲁棒的深度学习方法，用于心脏MRI中自动分割心肌疤痕，有效解决了半自动标注带来的标签噪声、数据异质性和类别不平衡等挑战，并在性能上超越了现有最先进的模型。", "motivation": "从心脏MRI中准确分割心肌疤痕对于临床评估和治疗规划至关重要。", "method": "本研究通过微调最先进的模型，提出了一种鲁棒的深度学习管道，用于全自动心肌疤痕检测和分割。该方法通过使用Kullback-Leibler损失和广泛的数据增强，明确解决了半自动标注产生的标签噪声、数据异质性和类别不平衡等挑战。", "result": "该模型在急性和慢性病例上均表现出色，即使在存在噪声标签的情况下也能生成准确且平滑的分割结果。特别是，该方法优于nnU-Net等最先进的模型，并在分布外测试集上显示出强大的泛化能力，突出了其在各种成像条件和临床任务中的鲁棒性。", "conclusion": "这些结果为自动化心肌疤痕量化奠定了可靠基础，并支持深度学习在心脏成像中更广泛的临床应用。", "translation": "从心脏MRI中准确分割心肌疤痕对于临床评估和治疗规划至关重要。在本研究中，我们提出了一种鲁棒的深度学习管道，通过微调最先进的模型，实现全自动的心肌疤痕检测和分割。该方法通过使用Kullback-Leibler损失和广泛的数据增强，明确解决了半自动标注产生的标签噪声、数据异质性和类别不平衡等挑战。我们在急性和慢性病例上评估了模型的性能，并展示了其在存在噪声标签的情况下也能生成准确且平滑的分割结果的能力。特别是，我们的方法优于nnU-Net等最先进的模型，并在分布外测试集上显示出强大的泛化能力，突出了其在各种成像条件和临床任务中的鲁棒性。这些结果为自动化心肌疤痕量化奠定了可靠基础，并支持深度学习在心脏成像中更广泛的临床应用。", "summary": "本研究提出了一种鲁棒的深度学习管道，用于心脏MRI中全自动的心肌疤痕分割。该方法通过引入Kullback-Leibler损失和广泛的数据增强，有效解决了半自动标注带来的标签噪声、数据异质性和类别不平衡问题。实验证明，该模型在急性和慢性病例中均能生成准确平滑的分割结果，并优于现有最先进的模型，在分布外测试集上显示出强大的泛化能力和鲁棒性，为心肌疤痕的自动化量化和深度学习在心脏成像中的临床应用提供了可靠基础。", "keywords": "心肌疤痕分割, 深度学习, 标签噪声, 心脏MRI, 鲁棒性", "comments": "该论文的创新点在于其鲁棒的深度学习管道能够有效处理心脏MRI心肌疤痕分割中常见的标签噪声、数据异质性和类别不平衡问题。通过超越现有最先进模型并展现出强大的泛化能力，该研究为深度学习在心脏影像领域的临床应用奠定了坚实基础，具有重要的临床价值。"}}
{"id": "2506.21152", "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image", "authors": ["Pufan Li", "Bi'an Du", "Wei Hu"], "summary": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To takle these issues, we present a novel method that seamlessly\nintegrates geometry and perception priors without requiring additional model\ntraining to reconstruct detailed 3D objects from a single image. Specifically,\nwe train three different Gaussian branches initialized from the geometry prior,\nperception prior and Gaussian noise, respectively. The geometry prior captures\nthe rough 3D shapes, while the perception prior utilizes the 2D pretrained\ndiffusion model to enhance multiview information. Subsequently, we refine 3D\nGaussian branches through mutual interaction between geometry and perception\npriors, further enhanced by a reprojection-based strategy that enforces depth\nconsistency. Experiments demonstrate the higher-fidelity reconstruction results\nof our method, outperforming existing methods on novel view synthesis and 3D\nreconstruction, demonstrating robust and consistent 3D object generation.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21152v1", "categories": ["cs.CV", "68", "I.4.0"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21152v1", "AI": {"title_translation": "几何与感知引导的高斯模型：从单幅图像生成多视角一致的3D对象", "tldr": "本文提出了一种新颖的方法，通过结合几何和感知先验来从单幅图像生成多视角一致且细节丰富的3D对象，无需额外训练，并在新视角合成和3D重建方面表现优异。", "motivation": "从单视角图像生成逼真的3D对象需要自然的视觉效果、3D一致性以及捕捉未见区域多种可能解释的能力。现有方法（如微调2D扩散模型或直接通过网络推理或3D高斯Splatting生成3D信息）通常存在多视角一致性差和几何细节不足的问题。", "method": "本方法无需额外的模型训练，通过无缝整合几何和感知先验来重建详细的3D对象。具体地，训练三个不同的高斯分支，分别从几何先验、感知先验和高斯噪声初始化。几何先验捕获粗略的3D形状，而感知先验利用2D预训练扩散模型增强多视角信息。随后，通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。", "result": "实验证明，该方法重建结果具有更高的保真度，在新视角合成和3D重建方面优于现有方法，展示了鲁棒且一致的3D对象生成能力。", "conclusion": "本文提出了一种结合几何和感知先验的新方法，有效解决了单视角3D生成中多视角一致性和几何细节不足的问题，实现了高质量的3D重建和新视角合成。", "translation": "从单视角图像生成逼真的3D对象需要自然的视觉效果、3D一致性以及捕捉未见区域多种可能解释的能力。现有方法通常依赖于微调预训练的2D扩散模型或通过快速网络推理或3D高斯Splatting直接生成3D信息，但其结果普遍存在多视角一致性差和几何细节不足的问题。为了解决这些问题，我们提出了一种新颖的方法，无需额外的模型训练，即可无缝整合几何和感知先验，从单幅图像重建详细的3D对象。具体地，我们训练了三个不同的高斯分支，分别从几何先验、感知先验和高斯噪声初始化。几何先验捕获粗略的3D形状，而感知先验利用2D预训练扩散模型来增强多视角信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高的保真度重建结果，在新视角合成和3D重建方面优于现有方法，展示了鲁棒且一致的3D对象生成。", "summary": "本文提出了一种无需额外训练的单视角3D生成方法，通过整合几何与感知先验来解决现有方案中多视角一致性差和几何细节不足的问题。该方法初始化三个高斯分支（几何先验、感知先验、高斯噪声），利用几何先验获取粗略形状，感知先验结合2D扩散模型增强多视角信息，并通过两者相互作用和重投影策略细化高斯分支，最终实现高保真度的多视角一致3D对象生成，在新视角合成和3D重建上超越现有方法。", "keywords": "单视角3D生成, 高斯Splatting, 几何先验, 感知先验, 多视角一致性", "comments": "该论文的创新点在于无需额外模型训练，通过巧妙地结合几何先验（粗略形状）和感知先验（2D扩散模型增强多视角信息），并引入高斯噪声分支，形成多分支高斯模型进行3D重建。其通过分支间的相互作用和重投影深度一致性增强策略，有效提升了单视角3D生成的多视角一致性和几何细节，解决了现有方法的痛点。这种无训练整合多种先验的思路具有较高的研究价值。"}}
{"id": "2506.21343", "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language Models", "authors": ["Jingyao Li", "Hao Sun", "Zile Qiao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Hong Xu", "Jiaya Jia"], "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21343v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21343v1", "AI": {"title_translation": "DynamicBench：评估大型语言模型中的实时报告生成", "tldr": "DynamicBench是一个新的基准测试，用于评估LLM处理和生成实时报告的能力，它采用双路径检索，并在无文档和有文档场景下表现优于GPT4o。", "motivation": "传统的LLM基准测试未能捕捉到当代应用中实时信息处理的动态需求，因此需要一个新的基准来评估LLM处理最新数据的能力。", "method": "本文提出了DynamicBench，一个旨在评估LLM存储和处理实时数据能力的基准。它采用双路径检索管道，整合了网络搜索和本地报告数据库，并要求领域特定知识。DynamicBench通过在提供或不提供外部文档的场景中评估模型，来衡量其独立处理最新信息或利用上下文增强的能力。此外，还引入了一个先进的报告生成系统来管理动态信息合成。", "result": "实验结果证实了该方法的有效性，其方法实现了最先进的性能，在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%。", "conclusion": "DynamicBench是一个有效且性能卓越的基准测试和报告生成系统，能够评估并提升大型语言模型在实时动态信息处理方面的能力，并在多项任务中超越现有最先进模型。", "translation": "大型语言模型（LLM）的传统基准测试通常依赖于通过讲故事或表达意见进行的静态评估，这未能捕捉到当代应用中实时信息处理的动态需求。为了解决这一限制，我们提出了DynamicBench，一个旨在评估LLM存储和处理最新数据能力的基准。DynamicBench利用双路径检索管道，整合了网络搜索和本地报告数据库。它需要领域特定知识，以确保在专业领域内生成准确的报告。通过在提供或不提供外部文档的场景中评估模型，DynamicBench有效地衡量了它们独立处理最新信息或利用上下文增强的能力。此外，我们引入了一个先进的报告生成系统，擅长管理动态信息合成。我们的实验结果证实了我们方法的有效性，我们的方法实现了最先进的性能，在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%。代码和数据将公开可用。", "summary": "本文介绍了DynamicBench，一个旨在评估大型语言模型（LLM）实时报告生成能力的基准测试。针对传统静态评估的局限性，DynamicBench采用双路径检索管道，结合网络搜索和本地数据库，并在有无外部文档的场景下测试LLM处理最新信息的能力。研究还提出了一个先进的报告生成系统。实验结果表明，DynamicBench方法在性能上超越了GPT4o，验证了其在动态信息处理方面的有效性。", "keywords": "大型语言模型, 实时报告生成, 基准测试, 动态评估, DynamicBench", "comments": "该论文的创新之处在于提出了一个针对LLM实时信息处理能力的动态评估基准DynamicBench，弥补了传统静态评估的不足。其采用的双路径检索管道（网络搜索与本地数据库结合）以及在不同文档提供情况下的评估方式，能够更全面地衡量LLM处理动态信息和利用上下文的能力。此外，超越GPT4o的性能表明了其方法的有效性和重要性。"}}
{"id": "2506.21165", "title": "Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition", "authors": ["Longkun Zou", "Kangjun Liu", "Ke Chen", "Kailing Guo", "Kui Jia", "Yaowei Wang"], "summary": "Learning semantic representations from point sets of 3D object shapes is\noften challenged by significant geometric variations, primarily due to\ndifferences in data acquisition methods. Typically, training data is generated\nusing point simulators, while testing data is collected with distinct 3D\nsensors, leading to a simulation-to-reality (Sim2Real) domain gap that limits\nthe generalization ability of point classifiers. Current unsupervised domain\nadaptation (UDA) techniques struggle with this gap, as they often lack robust,\ndomain-insensitive descriptors capable of capturing global topological\ninformation, resulting in overfitting to the limited semantic patterns of the\nsource domain. To address this issue, we introduce a novel Topology-Aware\nModeling (TAM) framework for Sim2Real UDA on object point clouds. Our approach\nmitigates the domain gap by leveraging global spatial topology, characterized\nby low-level, high-frequency 3D structures, and by modeling the topological\nrelations of local geometric features through a novel self-supervised learning\ntask. Additionally, we propose an advanced self-training strategy that combines\ncross-domain contrastive learning with self-training, effectively reducing the\nimpact of noisy pseudo-labels and enhancing the robustness of the adaptation\nprocess. Experimental results on three public Sim2Real benchmarks validate the\neffectiveness of our TAM framework, showing consistent improvements over\nstate-of-the-art methods across all evaluated tasks. The source code of this\nwork will be available at https://github.com/zou-longkun/TAG.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21165v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21165v1", "AI": {"title_translation": "拓扑感知建模用于无监督模拟到现实点云识别", "tldr": "本文提出了一种新颖的拓扑感知建模（TAM）框架，通过利用全局空间拓扑和先进的自训练策略，有效解决了无监督模拟到现实（Sim2Real）点云识别中的领域鸿沟问题，并优于现有最先进方法。", "motivation": "三维物体形状点集的语义表示学习面临显著的几何变化挑战，主要是由于数据采集方法不同。训练数据通常由点模拟器生成，而测试数据则由不同的三维传感器收集，导致模拟到现实（Sim2Real）的领域鸿沟，这限制了点分类器的泛化能力。当前的无监督域适应（UDA）技术难以弥补这一鸿沟，因为它们通常缺乏能够捕获全局拓扑信息的鲁棒、域无关描述符，导致对源域有限语义模式的过拟合。", "method": "我们引入了一个新颖的拓扑感知建模（TAM）框架，用于物体点云的Sim2Real UDA。该方法通过利用以低级、高频三维结构为特征的全局空间拓扑，并通过新颖的自监督学习任务建模局部几何特征的拓扑关系来弥补领域鸿沟。此外，我们提出了一种先进的自训练策略，将跨域对比学习与自训练相结合，有效减少了噪声伪标签的影响，增强了适应过程的鲁棒性。", "result": "在三个公共Sim2Real基准上的实验结果验证了我们TAM框架的有效性，在所有评估任务中均显示出比现有最先进方法的一致改进。", "conclusion": "拓扑感知建模（TAM）框架通过有效利用全局空间拓扑和先进的自训练策略，成功解决了无监督模拟到现实点云识别中的领域鸿沟问题，显著提升了点分类器的泛化能力。", "translation": "从三维物体形状点集中学习语义表示经常受到显著几何变化的挑战，这主要源于数据采集方法的差异。通常，训练数据通过点模拟器生成，而测试数据则由不同的三维传感器收集，导致模拟到现实（Sim2Real）的领域鸿沟，从而限制了点分类器的泛化能力。当前的无监督域适应（UDA）技术难以应对这一鸿沟，因为它们往往缺乏能够捕获全局拓扑信息的鲁棒、域无关描述符，导致对源域有限语义模式的过拟合。为了解决这个问题，我们引入了一种新颖的拓扑感知建模（TAM）框架，用于物体点云的Sim2Real UDA。我们的方法通过利用以低级、高频三维结构为特征的全局空间拓扑，并通过新颖的自监督学习任务建模局部几何特征的拓扑关系来弥补领域鸿沟。此外，我们提出了一种先进的自训练策略，将跨域对比学习与自训练相结合，有效减少了噪声伪标签的影响，增强了适应过程的鲁棒性。在三个公共Sim2Real基准上的实验结果验证了我们TAM框架的有效性，在所有评估任务中均显示出比现有最先进方法的一致改进。这项工作的源代码将在https://github.com/zou-longkun/TAG.git提供。", "summary": "本文针对无监督模拟到现实（Sim2Real）点云识别中存在的领域鸿沟问题，提出了一种名为拓扑感知建模（TAM）的新框架。该框架通过利用全局空间拓扑信息和通过自监督学习建模局部几何特征的拓扑关系来弥补领域差异。此外，它还结合了跨域对比学习和自训练的先进策略，以减少伪标签噪声并增强适应性。实验证明，TAM在多个Sim2Real基准上均优于现有最先进方法，有效提升了点分类器的泛化能力。", "keywords": "拓扑感知建模, 无监督域适应, 点云识别, 模拟到现实, 自监督学习", "comments": "本文的创新点在于提出了一个拓扑感知建模框架，通过显式地利用全局空间拓扑和局部几何特征的拓扑关系来解决Sim2Real领域鸿沟，这为无监督点云识别提供了一个新的视角。其结合自监督学习和先进自训练策略的设计，有效提升了模型对噪声的鲁棒性，并在实际应用中展现出优越的性能，对于提升点云分类器在不同采集环境下的泛化能力具有重要意义。"}}
{"id": "2506.21184", "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding", "authors": ["Minghao Qin", "Yan Shu", "Peitian Zhang", "Kun Lun", "Huaying Yuan", "Juenjie Zhou", "Shitao Xiao", "Bo Zhao", "Zheng Liu"], "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.", "comment": "14 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.21184v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21184v1", "AI": {"title_translation": "面向成本效益型长视频理解的任务感知KV压缩", "tldr": "Video-X^2L通过任务感知的双层KV压缩和选择性重载，显著提高了长视频理解的效率和性能，无需额外训练。", "motivation": "现有多模态大型语言模型（MLLMs）在长视频理解（LVU）中面临巨大的计算成本挑战，且现有KV压缩方法在高压缩比下存在严重信息损失。", "method": "本文提出了Video-X^2L，它包含两个关键操作：\n1. 双层KV压缩：在MLLM的预填充阶段，生成两种类型的压缩KVs：低压缩KVs（L-KVs）以捕获细粒度视频细节，高压缩KVs（H-KVs）以提供紧凑的视频表示。\n2. 选择性KV重载：在MLLM的解码阶段，Video-X^2L选择性地为最关键的视频块重载L-KVs，同时为其他不那么重要的部分使用H-KVs。这使得MLLM能够充分利用任务特定的信息，同时保持整体的紧凑性。该方法无需额外训练，且直接兼容现有KV可压缩的MLLMs。", "result": "Video-X^2L在VideoMME、MLVU、LongVideoBench和VNBench等多种流行的LVU基准测试中进行了评估。实验结果表明，Video-X^2L以巨大的优势超越了现有KV压缩方法，同时大幅节省了计算成本。", "conclusion": "Video-X^2L是一个简单而有效的方法，能够灵活地为每个长视频理解任务保留关键视频信息，并在性能和计算成本效益上取得显著提升。", "translation": "长视频理解（LVU）仍然是现有多模态大型语言模型（MLLMs）面临的严峻挑战，这主要是由于其高昂的计算成本。最近的方法探索了KV压缩来缓解这个问题，但它们在高压缩比下往往会遭受严重的信息损失。在本文中，我们引入了Video-X^2L，它能灵活地为每个LVU任务保留关键的视频信息。Video-X^2L涉及两个关键操作。第一个是双层KV压缩。在MLLM的预填充阶段，Video-X^2L生成两种类型的压缩KVs：低压缩KVs（L-KVs）以捕获细粒度视频细节，高压缩KVs（H-KVs）以提供紧凑的视频表示。第二个是选择性KV重载。在MLLM的解码阶段，Video-X^2L选择性地为最关键的视频块重载L-KVs，同时为其他不那么重要的部分使用H-KVs。这使得MLLM能够充分利用任务特定的信息，同时保持整体的紧凑性。Video-X^2L简单而有效：它无需额外训练，且直接兼容现有KV可压缩的MLLMs。我们在各种流行的LVU基准测试中评估了Video-X^2L，包括VideoMME、MLVU、LongVideoBench和VNBench。我们的实验结果表明，Video-X^2L以巨大的优势超越了现有KV压缩方法，同时大幅节省了计算成本。", "summary": "本文提出了Video-X^2L，一种针对长视频理解(LVU)中多模态大型语言模型(MLLM)高计算成本问题的任务感知KV压缩方法。Video-X^2L通过双层KV压缩（生成细粒度L-KVs和紧凑H-KVs）和选择性KV重载（根据任务重要性动态加载不同KVs）来灵活保留关键视频信息。该方法无需额外训练，且与现有MLLM兼容，在多个LVU基准测试中表现出显著优于现有KV压缩方法的性能，并大幅降低了计算成本。", "keywords": "KV compression, Long video understanding, MLLMs, Task-aware, Cost-effective", "comments": "Video-X^2L的创新之处在于其“任务感知”的双层KV压缩和选择性重载机制，有效解决了高压缩比下信息丢失的问题，同时保持了与现有模型的兼容性且无需额外训练，这使其具有很高的实用价值和应用潜力。"}}
{"id": "2506.21355", "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning", "authors": ["Melanie Rieff", "Maya Varma", "Ossian Rabow", "Subathra Adithan", "Julie Kim", "Ken Chang", "Hannah Lee", "Nidhi Rohatgi", "Christian Bluethgen", "Mohamed S. Muneer", "Jean-Benoit Delbrouck", "Michael Moor"], "summary": "Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, example ordering exhibits a recency bias,\ni.e., placing the most relevant example last can lead to substantial\nperformance improvements by up to 71%. Our findings highlight critical\nlimitations and biases in current MLLMs when learning multimodal medical tasks\nfrom context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21355v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21355v1", "AI": {"title_translation": "SMMILE：一个专家驱动的多模态医学情境学习基准", "tldr": "SMMILE引入了首个专家驱动的多模态医学情境学习基准，评估了MLLM在此类任务上的表现，发现现有模型表现不佳，且对不相关示例和示例顺序敏感。", "motivation": "尽管多模态情境学习（ICL）在医学等领域具有巨大潜力，但其探索不足。临床医生常遇到需要从有限示例中学习的专业任务。虽然多模态大型语言模型（MLLMs）在医学视觉问答（VQA）方面取得进展，但其从情境中学习多模态任务的能力尚不清楚。", "method": "引入了SMMILE，首个专家驱动的多模态医学任务ICL基准。11位医学专家策划了111个问题（517个问答-图像三元组），涵盖6个医学专业和13种成像模式。进一步引入了SMMILE++，一个包含1038个排列问题的增强变体。对15个MLLM进行了全面评估。", "result": "大多数模型在医学任务中的多模态ICL能力表现为中等到差。在开放式评估中，ICL在SMMILE上仅比零样本平均提高8%，在SMMILE++上提高9.4%。发现模型容易受到不相关情境示例的影响：即使单个噪声或不相关示例也能使性能下降高达9.5%。此外，示例顺序表现出近因偏见，即把最相关的示例放在最后可以带来高达71%的显著性能提升。", "conclusion": "研究结果强调了当前MLLM在从情境中学习多模态医学任务时存在的关键局限性和偏见。", "translation": "多模态情境学习（ICL）尽管在医学等领域具有巨大潜力，但仍未得到充分探索。临床医生经常遇到需要从有限示例中适应的各种专业任务，例如从少量相关先例中获取见解或考虑一组受限的鉴别诊断。尽管多模态大型语言模型（MLLMs）在医学视觉问答（VQA）方面取得了进展，但它们从情境中学习多模态任务的能力在很大程度上是未知的。我们引入了SMMILE，这是第一个专家驱动的医学任务多模态ICL基准。十一位医学专家策划了问题，每个问题都包含一个多模态查询和多模态情境示例作为任务演示。SMMILE包含111个问题（517个问答-图像三元组），涵盖6个医学专业和13种成像模式。我们进一步引入了SMMILE++，一个包含1038个排列问题的增强变体。对15个MLLM的全面评估表明，大多数模型在医学任务中表现出中等到差的多模态ICL能力。在开放式评估中，ICL在SMMILE上仅比零样本平均提高8%，在SMMILE++上提高9.4%。我们观察到模型对不相关情境示例的敏感性：即使单个噪声或不相关示例也能使性能下降高达9.5%。此外，示例排序表现出近因偏见，即把最相关的示例放在最后可以带来高达71%的显著性能提升。我们的研究结果强调了当前MLLM在从情境中学习多模态医学任务时存在的关键局限性和偏见。", "summary": "本研究介绍了SMMILE，一个由11位医学专家构建的、首个专家驱动的多模态医学情境学习（ICL）基准，包含111个问题和517个问答-图像三元组，涵盖6个医学专业和13种成像模式。同时提出了增强版SMMILE++。通过对15个多模态大型语言模型（MLLMs）的评估发现，现有模型在医学多模态ICL任务中表现普遍不佳，ICL相对于零样本的提升有限。研究还揭示了MLLMs对不相关情境示例的敏感性以及示例顺序的近因偏见，凸显了当前MLLMs在医学情境学习中的局限性。", "keywords": "多模态情0境学习, 医学AI, 基准测试, MLLMs, SMMILE", "comments": "该论文的创新之处在于构建了首个专家驱动的多模态医学ICL基准SMMILE及其增强版SMMILE++，填补了该领域基准测试的空白。其重要性在于揭示了当前MLLMs在医学多模态情境学习方面的不足和存在的偏见，为未来的模型改进提供了明确的方向。特别是对不相关示例敏感性和近因偏见的发现，对设计更鲁棒和有效的ICL策略具有指导意义。"}}
{"id": "2506.21367", "title": "rQdia: Regularizing Q-Value Distributions With Image Augmentation", "authors": ["Sam Lerman", "Jing Bi"], "summary": "rQdia regularizes Q-value distributions with augmented images in pixel-based\ndeep reinforcement learning. With a simple auxiliary loss, that equalizes these\ndistributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks\nrespectively in the MuJoCo Continuous Control Suite from pixels, and\nData-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured\nin both sample efficiency and longer-term training. Moreover, the addition of\nrQdia finally propels model-free continuous control from pixels over the state\nencoding baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21367v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21367v1", "AI": {"title_translation": "rQdia：使用图像增强正则化Q值分布", "tldr": "rQdia通过图像增强和简单的辅助损失正则化Q值分布，显著提升了像素级深度强化学习（如DrQ、SAC、Rainbow）的性能，并使无模型连续控制超越了状态编码基线。", "motivation": "该研究旨在通过正则化Q值分布来提升像素级深度强化学习的性能。", "method": "rQdia通过图像增强来正则化像素级深度强化学习中的Q值分布。它使用一个简单的辅助损失函数，通过均方误差（MSE）来均衡这些分布。", "result": "rQdia在MuJoCo连续控制套件中，分别提升了DrQ在9/12任务和SAC在10/12任务上的表现，并在26个Atari街机环境中的18个提升了Data-Efficient Rainbow的性能。性能提升体现在样本效率和长期训练。此外，rQdia的加入使得像素级的无模型连续控制超越了状态编码基线。", "conclusion": "rQdia通过正则化Q值分布，显著提升了现有像素级深度强化学习算法的性能，并使无模型连续控制从像素输入超越了状态编码基线。", "translation": "rQdia通过增强图像来正则化基于像素的深度强化学习中的Q值分布。通过一个简单的辅助损失函数，该函数通过均方误差（MSE）来均衡这些分布，rQdia在MuJoCo连续控制套件中分别提升了DrQ在12个任务中的9个和SAC在12个任务中的10个的表现，并提升了Data-Efficient Rainbow在26个Atari街机环境中的18个。性能提升体现在样本效率和长期训练。此外，rQdia的加入最终使基于像素的无模型连续控制超越了状态编码基线。", "summary": "rQdia是一种在像素级深度强化学习中，通过图像增强和基于MSE的辅助损失来正则化Q值分布的方法。该方法显著提升了DrQ、SAC和Data-Efficient Rainbow在多个基准测试（MuJoCo和Atari）上的性能，并在样本效率和长期训练方面均有增益。值得注意的是，rQdia使无模型连续控制从像素输入超越了状态编码基线。", "keywords": "Q值分布正则化, 图像增强, 深度强化学习, 无模型控制, 像素级控制", "comments": "rQdia的创新在于将图像增强与Q值分布正则化结合，通过一个简单的辅助损失实现了显著的性能提升。其重要性体现在它不仅提升了现有算法的效率和长期表现，还使像素级的无模型连续控制达到了超越状态编码基线的里程碑，这对于实际应用中从原始像素输入进行控制具有重要意义。"}}
{"id": "2506.21188", "title": "GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding", "authors": ["Zijun Lin", "Shuting He", "Cheston Tan", "Bihan Wen"], "summary": "Sequential grounding in 3D point clouds (SG3D) refers to locating sequences\nof objects by following text instructions for a daily activity with detailed\nsteps. Current 3D visual grounding (3DVG) methods treat text instructions with\nmultiple steps as a whole, without extracting useful temporal information from\neach step. However, the instructions in SG3D often contain pronouns such as\n\"it\", \"here\" and \"the same\" to make language expressions concise. This requires\ngrounding methods to understand the context and retrieve relevant information\nfrom previous steps to correctly locate object sequences. Due to the lack of an\neffective module for collecting related historical information,\nstate-of-the-art 3DVG methods face significant challenges in adapting to the\nSG3D task. To fill this gap, we propose GroundFlow -- a plug-in module for\ntemporal reasoning on 3D point cloud sequential grounding. Firstly, we\ndemonstrate that integrating GroundFlow improves the task accuracy of 3DVG\nbaseline methods by a large margin (+7.5\\% and +10.2\\%) in the SG3D benchmark,\neven outperforming a 3D large language model pre-trained on various datasets.\nFurthermore, we selectively extract both short-term and long-term step\ninformation based on its relevance to the current instruction, enabling\nGroundFlow to take a comprehensive view of historical information and maintain\nits temporal understanding advantage as step counts increase. Overall, our work\nintroduces temporal reasoning capabilities to existing 3DVG models and achieves\nstate-of-the-art performance in the SG3D benchmark across five datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21188v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21188v1", "AI": {"title_translation": "GroundFlow：一个用于3D点云序列定位中时序推理的即插即用模块", "tldr": "GroundFlow是一个即插即用模块，通过引入时序推理能力，显著提高了3D点云序列定位的准确性，解决了现有方法无法有效处理多步骤指令和指代词的问题。", "motivation": "现有的3D视觉定位（3DVG）方法在处理多步骤文本指令时，未能有效提取时序信息，也无法理解指令中包含的指代词（如“它”、“这里”等），导致在3D点云序列定位（SG3D）任务中面临挑战，无法正确追踪和定位对象序列。", "method": "我们提出了GroundFlow，这是一个即插即用模块，用于3D点云序列定位中的时序推理。该模块能够选择性地提取与当前指令相关的短期和长期步骤信息，从而全面利用历史信息，并随着步骤数量的增加保持其时序理解优势。", "result": "将GroundFlow集成到3DVG基线方法中，在SG3D基准测试中显著提高了任务准确性（+7.5%和+10.2%），甚至优于在各种数据集上预训练的3D大型语言模型。它在五个数据集上实现了最先进的性能。", "conclusion": "我们的工作为现有3DVG模型引入了时序推理能力，并在SG3D基准测试中实现了最先进的性能。", "translation": "3D点云中的序列定位（SG3D）是指通过遵循详细步骤的日常活动文本指令来定位一系列对象。当前的3D视觉定位（3DVG）方法将多步骤文本指令作为一个整体处理，而没有从每个步骤中提取有用的时序信息。然而，SG3D中的指令通常包含代词，如“它”、“这里”和“相同”，以使语言表达简洁。这要求定位方法理解上下文并从之前的步骤中检索相关信息，以正确地定位对象序列。由于缺乏有效模块来收集相关的历史信息，最先进的3DVG方法在适应SG3D任务时面临重大挑战。为了填补这一空白，我们提出了GroundFlow——一个用于3D点云序列定位中时序推理的即插即用模块。首先，我们证明了在SG3D基准测试中，集成GroundFlow显著提高了3DVG基线方法的任务准确性（+7.5%和+10.2%），甚至优于在各种数据集上预训练的3D大型语言模型。此外，我们根据与当前指令的相关性选择性地提取短期和长期步骤信息，使GroundFlow能够全面查看历史信息，并随着步骤数量的增加保持其时序理解优势。总的来说，我们的工作为现有3DVG模型引入了时序推理能力，并在五个数据集的SG3D基准测试中实现了最先进的性能。", "summary": "本文提出了GroundFlow，一个针对3D点云序列定位（SG3D）的即插即用模块，旨在解决现有3D视觉定位（3DVG）方法在处理多步骤指令和指代词时缺乏时序推理能力的问题。GroundFlow通过选择性地提取短期和长期历史信息，有效理解上下文并追踪对象序列。实验结果表明，GroundFlow显著提升了基线方法的准确性，甚至超越了预训练的3D大型语言模型，并在五个数据集上达到了SG3D任务的最先进性能。该工作成功地为3DVG模型引入了时序推理能力。", "keywords": "3D点云, 序列定位, 时序推理, 即插即用模块, 视觉定位", "comments": "GroundFlow的创新之处在于其作为“即插即用”模块的设计，能够轻松集成到现有3DVG模型中，从而赋予它们处理复杂序列指令的能力。它通过选择性地利用短期和长期历史信息，有效解决了多步骤指令中上下文依赖和指代词理解的关键挑战。该模块的引入显著提升了SG3D任务的性能，并超越了大型预训练模型，这突显了其在时序推理方面的有效性和重要性。"}}
{"id": "2506.21371", "title": "MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators", "authors": ["Vasileios Leon", "Georgios Makris", "Sotirios Xydis", "Kiamal Pekmestzi", "Dimitrios Soudris"], "summary": "Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has\nestablished them as the defacto approach for providing advanced Machine\nLearning tasks with excellent accuracy. Targeting low-power DNN computing, this\npaper examines the interplay of fine-grained error resilience of DNN workloads\nin collaboration with hardware approximation techniques, to achieve higher\nlevels of energy efficiency. Utilizing the state-of-the-art ROUP approximate\nmultipliers, we systematically explore their fine-grained distribution across\nthe network according to our layer-, filter-, and kernel-level approaches, and\nexamine their impact on accuracy and energy. We use the ResNet-8 model on the\nCIFAR-10 dataset to evaluate our approximations. The proposed solution delivers\nup to 54% energy gains in exchange for up to 4% accuracy loss, compared to the\nbaseline quantized model, while it provides 2x energy gains with better\naccuracy versus the state-of-the-art DNN approximations.", "comment": "Presented at the 13th IEEE LASCAS Conference", "pdf_url": "http://arxiv.org/pdf/2506.21371v1", "categories": ["cs.LG", "cs.AR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21371v1", "AI": {"title_translation": "MAx-DNN：用于节能型DNN硬件加速器的多级算术近似", "tldr": "MAx-DNN通过在层、滤波器和核级别应用细粒度近似乘法器，显著提高了DNN硬件加速器的能效，同时将精度损失控制在可接受范围内。", "motivation": "深度神经网络（DNN）架构的快速增长使其成为提供高级机器学习任务的事实标准，但其计算需求高。本研究旨在通过硬件近似技术实现低功耗DNN计算，以提高能效。", "method": "本研究利用最先进的ROUP近似乘法器，系统地探索了其在网络中细粒度的分布，并提出了层级、滤波器级和核级的方法。通过检查DNN工作负载的细粒度错误弹性与硬件近似技术之间的相互作用，以实现更高的能效。使用ResNet-8模型在CIFAR-10数据集上进行评估。", "result": "与基线量化模型相比，所提出的解决方案实现了高达54%的能耗增益，而精度损失最高为4%。与最先进的DNN近似方法相比，它提供了2倍的能耗增益，同时保持了更好的精度。", "conclusion": "MAx-DNN通过在DNN硬件加速器中引入多级算术近似，显著提高了能效，同时将精度损失控制在可接受的范围内，甚至优于现有最先进的近似方法。", "translation": "如今，深度神经网络（DNN）架构的快速发展已使其成为提供具有出色准确性的高级机器学习任务的事实方法。针对低功耗DNN计算，本文研究了DNN工作负载的细粒度错误弹性与硬件近似技术之间的相互作用，以实现更高水平的能效。我们利用最先进的ROUP近似乘法器，系统地探索了它们在网络中根据我们的层级、滤波器级和核级方法的细粒度分布，并检查了它们对准确性和能耗的影响。我们使用CIFAR-10数据集上的ResNet-8模型来评估我们的近似。与基线量化模型相比，所提出的解决方案实现了高达54%的能耗增益，而精度损失最高为4%，同时与最先进的DNN近似方法相比，它提供了2倍的能耗增益，并具有更好的准确性。", "summary": "本文提出MAx-DNN，一种用于节能型DNN硬件加速器的多级算术近似方法。该方法利用ROUP近似乘法器，在层、滤波器和核级别上系统地探索其细粒度分布，以利用DNN工作负载的错误弹性。在ResNet-8模型和CIFAR-10数据集上的评估显示，与基线量化模型相比，MAx-DNN可实现高达54%的能耗增益，精度损失最大为4%；与现有最先进的DNN近似方法相比，可实现2倍能耗增益且精度更优。", "keywords": "DNN, 硬件加速器, 算术近似, 能效, 多级近似", "comments": "该论文的创新点在于提出了多级（层、滤波器、核）的细粒度算术近似方法，并系统地探索了近似乘法器在DNN中的分布。其重要性体现在显著提高了DNN硬件加速器的能效，同时将精度损失控制在可接受的范围内，甚至优于现有技术，这对于边缘计算和低功耗AI设备具有重要意义。"}}
{"id": "2506.21085", "title": "CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions", "authors": ["Yangzhe Peng", "Kaiyuan Gao", "Liang He", "Yuheng Cong", "Haiguang Liu", "Kun He", "Lijun Wu"], "summary": "Molecular docking plays a crucial role in predicting the binding mode of\nligands to target proteins, and covalent interactions, which involve the\nformation of a covalent bond between the ligand and the target, are\nparticularly valuable due to their strong, enduring binding nature. However,\nmost existing docking methods and deep learning approaches hardly account for\nthe formation of covalent bonds and the associated structural changes. To\naddress this gap, we introduce a comprehensive benchmark for covalent docking,\nCovDocker, which is designed to better capture the complexities of covalent\nbinding. We decompose the covalent docking process into three main tasks:\nreactive location prediction, covalent reaction prediction, and covalent\ndocking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,\nwe establish baseline performances and demonstrate the effectiveness of the\nbenchmark in accurately predicting interaction sites and modeling the molecular\ntransformations involved in covalent binding. These results confirm the role of\nthe benchmark as a rigorous framework for advancing research in covalent drug\ndesign. It underscores the potential of data-driven approaches to accelerate\nthe discovery of selective covalent inhibitors and addresses critical\nchallenges in therapeutic development.", "comment": "Accepted to KDD 2025 Research Track", "pdf_url": "http://arxiv.org/pdf/2506.21085v1", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.21085v1", "AI": {"title_translation": "CovDocker：用任务、数据集和解决方案基准测试共价药物设计", "tldr": "CovDocker是一个用于共价对接的新基准，旨在解决现有方法在处理共价键方面的局限性，提供任务、数据集和解决方案以推进共价药物设计。", "motivation": "现有的大多数分子对接方法和深度学习方法难以考虑共价键的形成和相关的结构变化，这限制了共价药物设计的发展。", "method": "将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接。通过调整Uni-Mol和Chemformer等先进模型来建立基线性能。", "result": "建立了基线性能，并证明了该基准在准确预测相互作用位点和模拟共价结合中涉及的分子转化方面的有效性。", "conclusion": "CovDocker基准为推进共价药物设计研究提供了一个严谨的框架，并强调了数据驱动方法在加速选择性共价抑制剂发现和解决治疗开发关键挑战方面的潜力。", "translation": "分子对接在预测配体与靶蛋白的结合模式中起着关键作用，而共价相互作用，即配体与靶点之间形成共价键，因其强效、持久的结合性质而特别有价值。然而，大多数现有的对接方法和深度学习方法很难考虑到共价键的形成和相关的结构变化。为了弥补这一空白，我们引入了一个全面的共价对接基准CovDocker，旨在更好地捕捉共价结合的复杂性。我们将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接。通过调整Uni-Mol和Chemformer等先进模型，我们建立了基线性能，并证明了该基准在准确预测相互作用位点和模拟共价结合中涉及的分子转化方面的有效性。这些结果证实了该基准作为推进共价药物设计研究的严谨框架的作用。它强调了数据驱动方法在加速选择性共价抑制剂发现方面的潜力，并解决了治疗开发中的关键挑战。", "summary": "CovDocker通过引入一个全面的基准，解决了当前分子对接方法在处理共价键方面的局限性。它将共价对接分解为反应位点预测、共价反应预测和共价对接三个任务。通过调整先进模型，CovDocker证明了其在预测相互作用位点和模拟分子转化方面的有效性，为推进共价药物设计提供了一个严谨的框架，并加速了选择性共价抑制剂的发现。", "keywords": "共价药物设计, 分子对接, 基准测试, 共价结合, CovDocker", "comments": "CovDocker通过为共价对接提供一个结构化的基准，弥补了传统对接方法在该领域长期存在的不足，具有创新性。它将共价对接过程分解为具体任务，并利用先进模型建立基线，为未来的共价药物设计研究奠定了坚实基础，解决了药物发现中一个关键需求。"}}
{"id": "2506.21374", "title": "Pay Attention to Small Weights", "authors": ["Chao Zhou", "Tom Jacobs", "Advait Gadhikar", "Rebekka Burkholz"], "summary": "Finetuning large pretrained neural networks is known to be\nresource-intensive, both in terms of memory and computational cost. To mitigate\nthis, a common approach is to restrict training to a subset of the model\nparameters. By analyzing the relationship between gradients and weights during\nfinetuning, we observe a notable pattern: large gradients are often associated\nwith small-magnitude weights. This correlation is more pronounced in finetuning\nsettings than in training from scratch. Motivated by this observation, we\npropose NANOADAM, which dynamically updates only the small-magnitude weights\nduring finetuning and offers several practical advantages: first, this\ncriterion is gradient-free -- the parameter subset can be determined without\ngradient computation; second, it preserves large-magnitude weights, which are\nlikely to encode critical features learned during pretraining, thereby reducing\nthe risk of catastrophic forgetting; thirdly, it permits the use of larger\nlearning rates and consistently leads to better generalization performance in\nexperiments. We demonstrate this for both NLP and vision tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21374v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21374v1", "AI": {"title_translation": "关注小权重", "tldr": "论文提出NANOADAM，一种动态更新预训练模型中小型权重的方法，以减少微调时的资源消耗，同时提高泛化性能并减少灾难性遗忘。", "motivation": "微调大型预训练神经网络资源消耗大（内存和计算成本高）。现有方法是限制训练参数子集。作者通过分析发现微调时大梯度常与小权重相关，受此启发提出新方法。", "method": "提出NANOADAM方法，在微调过程中动态地只更新小幅度的权重。该方法无需梯度计算即可确定参数子集，能保留大权重以减少灾难性遗忘，并允许使用更大的学习率。", "result": "NANOADAM在实验中持续带来更好的泛化性能，并在NLP和视觉任务中得到验证。", "conclusion": "通过只更新小权重，NANOADAM有效降低了微调的资源消耗，同时提升了模型性能，并减少了灾难性遗忘。", "translation": "微调大型预训练神经网络在内存和计算成本方面都已知是资源密集型的。为了缓解这一问题，一种常见的方法是将训练限制在模型参数的一个子集上。通过分析微调过程中梯度与权重之间的关系，我们观察到一个显著的模式：大梯度通常与小幅度的权重相关。这种相关性在微调设置中比从头开始训练时更为明显。受此观察的启发，我们提出了NANOADAM，它在微调过程中动态地只更新小幅度的权重，并提供了几个实际优势：首先，这个标准是无梯度的——参数子集可以在不计算梯度的情况下确定；其次，它保留了大幅度的权重，这些权重可能编码了预训练期间学到的关键特征，从而降低了灾难性遗忘的风险；第三，它允许使用更大的学习率，并在实验中持续带来更好的泛化性能。我们在自然语言处理（NLP）和视觉任务中都证明了这一点。", "summary": "本文针对大型预训练模型微调时资源消耗大的问题，提出了一种名为NANOADAM的新方法。该方法基于一个观察，即微调时大梯度常与小幅度权重相关。NANOADAM动态地只更新模型中的小幅度权重，从而实现无梯度确定更新参数、保留关键特征以减少灾难性遗忘、并允许使用更大学习率，最终在NLP和视觉任务中展现出更好的泛化性能。", "keywords": "微调, 预训练模型, NANOADAM, 资源效率, 泛化性能", "comments": "这篇论文通过深入分析梯度与权重的关系，提出了一种新颖且高效的微调策略。其创新点在于利用小权重与大梯度的关联性，设计出一种无梯度、能有效减少灾难性遗忘并提升泛化性能的方法。NANOADAM的提出对于资源受限的深度学习应用具有重要意义，提供了一种更经济且高效的模型微调范式。"}}
{"id": "2506.21382", "title": "Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection", "authors": ["Zhi Zheng", "Bochuan Zhou", "Yuping Song"], "summary": "Cryptocurrency transaction fraud detection faces the dual challenges of\nincreasingly complex transaction patterns and severe class imbalance.\nTraditional methods rely on manual feature engineering and struggle to capture\ntemporal and structural dependencies in transaction networks. This paper\nproposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that\nenhances detection performance through three modules: (1) designing an advanced\ntemporal embedding module that fuses multi-scale time difference features with\nperiodic position encoding; (2) constructing a temporal-aware triple attention\nmechanism that jointly optimizes structural, temporal, and global context\nattention; (3) employing weighted BCE loss to address class imbalance.\nExperiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT\nachieves an AUC of 0.9130, representing a 9.2% improvement over the best\ntraditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This\nmethod not only validates the enhancement effect of temporal awareness and\ntriple attention mechanisms on graph neural networks, but also provides\nfinancial institutions with more reliable fraud detection tools, with its\ndesign principles generalizable to other temporal graph anomaly detection\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21382v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21382v1", "AI": {"title_translation": "用于加密货币交易欺诈检测的时间感知图注意力网络", "tldr": "本文提出了一种名为ATGAT的增强型时间感知图注意力网络，通过融合时间特征、构建三重注意力机制和使用加权BCE损失来解决加密货币交易欺诈检测中复杂的交易模式和类别不平衡问题，并在Elliptic++数据集上取得了显著的性能提升。", "motivation": "加密货币交易欺诈检测面临交易模式日益复杂和类别严重不平衡的双重挑战。传统方法依赖手动特征工程，难以捕捉交易网络中的时间依赖性和结构依赖性。", "method": "本文提出了一种增强型时间感知图注意力网络（ATGAT），通过以下三个模块提升检测性能：1) 设计先进的时间嵌入模块，融合多尺度时间差特征与周期位置编码；2) 构建时间感知三重注意力机制，共同优化结构、时间及全局上下文注意力；3) 采用加权BCE损失以解决类别不平衡问题。", "result": "在Elliptic++加密货币数据集上的实验表明，ATGAT的AUC达到0.9130，比最佳传统方法XGBoost提高了9.2%，比GCN提高了12.0%，比标准GAT提高了10.0%。", "conclusion": "该方法不仅验证了时间感知和三重注意力机制对图神经网络的增强效果，还为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广到其他时间图异常检测任务。", "translation": "加密货币交易欺诈检测面临日益复杂的交易模式和严重的类别不平衡的双重挑战。传统方法依赖于手动特征工程，难以捕捉交易网络中的时间和结构依赖性。本文提出了一种增强型时间感知图注意力网络（ATGAT），通过三个模块提高检测性能：(1) 设计一个先进的时间嵌入模块，融合多尺度时间差特征与周期位置编码；(2) 构建一个时间感知三重注意力机制，共同优化结构、时间及全局上下文注意力；(3) 采用加权BCE损失以解决类别不平衡。在Elliptic++加密货币数据集上的实验表明，ATGAT的AUC达到0.9130，比最佳传统方法XGBoost提高了9.2%，比GCN提高了12.0%，比标准GAT提高了10.0%。该方法不仅验证了时间感知和三重注意力机制对图神经网络的增强效果，还为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广到其他时间图异常检测任务。", "summary": "本文提出了一种增强型时间感知图注意力网络（ATGAT），旨在解决加密货币交易欺诈检测中复杂的交易模式和严重的类别不平衡问题。ATGAT通过融合多尺度时间差特征和周期位置编码的时间嵌入模块、共同优化结构、时间及全局上下文注意力的时间感知三重注意力机制，以及采用加权BCE损失来提升性能。实验证明，ATGAT在Elliptic++数据集上取得了显著优于传统方法和现有图神经网络的欺诈检测效果，验证了其创新机制的有效性，并为金融机构提供了更可靠的工具。", "keywords": "加密货币欺诈检测, 图注意力网络, 时间感知, 类别不平衡, 三重注意力", "comments": "该论文的创新点在于提出了增强型时间感知图注意力网络（ATGAT），特别是在时间特征融合（多尺度时间差与周期位置编码）和三重注意力机制（结构、时间、全局上下文）方面，有效解决了加密货币交易欺诈中时间依赖性和复杂模式捕捉的难题。其性能提升显著，且方法具有一定的通用性，可应用于其他时间图异常检测任务，具有重要的实践意义。"}}
{"id": "2506.21209", "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models", "authors": ["Louis Kerner", "Michel Meintz", "Bihe Zhao", "Franziska Boenisch", "Adam Dziedzic"], "summary": "State-of-the-art text-to-image models like Infinity generate photorealistic\nimages at an unprecedented speed. These models operate in a bitwise\nautoregressive manner over a discrete set of tokens that is practically\ninfinite in size. However, their impressive generative power comes with a\ngrowing risk: as their outputs increasingly populate the Internet, they are\nlikely to be scraped and reused as training data-potentially by the very same\nmodels. This phenomenon has been shown to lead to model collapse, where\nrepeated training on generated content, especially from the models' own\nprevious versions, causes a gradual degradation in performance. A promising\nmitigation strategy is watermarking, which embeds human-imperceptible yet\ndetectable signals into generated images-enabling the identification of\ngenerated content. In this work, we introduce BitMark, a robust bitwise\nwatermarking framework for Infinity. Our method embeds a watermark directly at\nthe bit level of the token stream across multiple scales (also referred to as\nresolutions) during Infinity's image generation process. Our bitwise watermark\nsubtly influences the bits to preserve visual fidelity and generation speed\nwhile remaining robust against a spectrum of removal techniques. Furthermore,\nit exhibits high radioactivity, i.e., when watermarked generated images are\nused to train another image generative model, this second model's outputs will\nalso carry the watermark. The radioactive traces remain detectable even when\nonly fine-tuning diffusion or image autoregressive models on images watermarked\nwith our BitMark. Overall, our approach provides a principled step toward\npreventing model collapse in image generative models by enabling reliable\ndetection of generated outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21209v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21209v1", "AI": {"title_translation": "BitMark for Infinity：位级自回归图像生成模型的水印技术", "tldr": "为了防止像Infinity这样的位级自回归图像生成模型因其输出被重复用于训练而导致模型崩溃，本文引入了BitMark，一种鲁棒的位级水印框架。BitMark在生成过程中直接在比特层面嵌入水印，同时保持视觉保真度和生成速度，并具有高放射性，即使被用于训练新模型，水印仍可追溯。", "motivation": "当前最先进的文本到图像模型（如Infinity）能够以极快的速度生成逼真的图像，但其强大的生成能力也带来了日益增长的风险：随着生成内容在互联网上大量出现，它们很可能被抓取并重新用作训练数据，这可能导致模型崩溃，即在生成内容（特别是模型自身先前版本的内容）上重复训练会导致性能逐渐下降。水印技术是一种有前景的缓解策略，它能将人眼不可见但可检测的信号嵌入到生成的图像中，从而实现对生成内容的识别。", "method": "本文介绍了BitMark，一个针对Infinity的鲁棒位级水印框架。我们的方法在Infinity的图像生成过程中，直接在令牌流的比特级别跨多个尺度（也称为分辨率）嵌入水印。我们的位级水印巧妙地影响比特，以保持视觉保真度和生成速度，同时对各种移除技术保持鲁棒性。", "result": "BitMark对一系列移除技术表现出鲁棒性。此外，它还具有高放射性，即当带有水印的生成图像被用于训练另一个图像生成模型时，这个第二模型的输出也将带有水印。即使仅使用我们BitMark加水印的图像对扩散模型或图像自回归模型进行微调，放射性痕迹仍然可检测。", "conclusion": "总的来说，我们的方法通过实现对生成输出的可靠检测，为防止图像生成模型中的模型崩溃提供了原则性的一步。", "translation": "最先进的文本到图像模型，如Infinity，能以空前的速度生成逼真的图像。这些模型以位级自回归的方式操作，作用于实际上无限大小的离散令牌集。然而，它们令人印象深刻的生成能力伴随着日益增长的风险：随着它们的输出越来越多地充斥互联网，它们很可能被抓取并重新用作训练数据——甚至可能被同一模型利用。这种现象已被证明会导致模型崩溃，即在生成内容上重复训练，特别是来自模型自身先前版本的内容，会导致性能逐渐下降。一种有前景的缓解策略是水印技术，它将人眼不可见但可检测的信号嵌入到生成的图像中，从而能够识别生成内容。在这项工作中，我们引入了BitMark，一个针对Infinity的鲁棒位级水印框架。我们的方法在Infinity的图像生成过程中，直接在令牌流的比特级别跨多个尺度（也称为分辨率）嵌入水印。我们的位级水印巧妙地影响比特，以保持视觉保真度和生成速度，同时对各种移除技术保持鲁棒性。此外，它还表现出高放射性，即当带有水印的生成图像被用于训练另一个图像生成模型时，这个第二模型的输出也将带有水印。即使仅使用我们BitMark加水印的图像对扩散模型或图像自回归模型进行微调，放射性痕迹仍然可检测。总的来说，我们的方法通过实现对生成输出的可靠检测，为防止图像生成模型中的模型崩溃提供了原则性的一步。", "summary": "鉴于像Infinity这样的位级自回归图像生成模型面临因其生成内容被重复用于训练而导致模型崩溃的风险，本文提出了一种名为BitMark的鲁棒位级水印框架。BitMark在图像生成过程中直接在令牌流的比特层面嵌入水印，能够在保持视觉保真度和生成速度的同时，有效抵抗水印移除攻击。其核心创新在于“放射性”特性，即使用BitMark加水印的图像训练出的新模型，其生成内容仍会带有可检测的水印痕迹，即使是微调模型也能保持可检测性。这项工作为通过可靠检测生成内容来防止图像生成模型中的模型崩溃迈出了重要一步。", "keywords": "水印技术, 图像生成模型, 模型崩溃, 位级自回归, 放射性", "comments": "本论文的创新点在于提出了BitMark水印框架，特别是其“放射性”特性。这意味着水印不仅能标识原始生成内容，还能在这些内容被用于训练新的生成模型时，将水印传播到新模型的输出中。这一点对于防止模型崩溃至关重要，因为它提供了一种长期且深层次的追溯机制，有助于维护未来生成模型的质量和数据来源的透明度。这种超越传统水印能力的设计，使其在应对AI生成内容滥用和模型退化方面具有显著的重要性。"}}
{"id": "2506.21387", "title": "Early Stopping Tabular In-Context Learning", "authors": ["Jaris Küken", "Lennart Purucker", "Frank Hutter"], "summary": "Tabular foundation models have shown strong performance across various\ntabular learning tasks via in-context learning, offering robust generalization\nwithout any downstream finetuning. However, their inference-time costs remain\nhigh, particularly for larger datasets. To address this, we propose\nearly-stopping the in-context learning process. We achieve this by dynamically\nevaluating whether to stop in-context learning after each Transformer encoder\nlayer. Once stopped, we decode the embedding using a pre-trained layer-wise\ndecoder. Experiments across 34 small classification tasks size show that early\nstopping in-context learning accelerates inference by up to x1.3 with\nnegligible degradation in predictive performance. To assess scalability, we\nfurther evaluate our method on five larger classification tasks, achieving\nspeedups of up to x2.2. Our results demonstrate the potential of early exiting\nas an effective and practical strategy for improving the efficiency of tabular\nin-context learning.", "comment": "ICML Workshop Paper", "pdf_url": "http://arxiv.org/pdf/2506.21387v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21387v1", "AI": {"title_translation": "表格上下文学习的提前停止", "tldr": "本文提出了一种提前停止策略，通过在每个Transformer编码器层后动态评估是否停止上下文学习，从而显著加速表格上下文学习的推理过程，同时保持预测性能。", "motivation": "表格基础模型通过上下文学习在各种表格学习任务中表现出色，但其推理时间成本很高，特别是对于大型数据集。", "method": "我们提出了提前停止上下文学习过程。通过在每个Transformer编码器层之后动态评估是否停止上下文学习来实现。一旦停止，我们使用预训练的逐层解码器解码嵌入。", "result": "在34个小型分类任务上的实验表明，提前停止上下文学习可将推理速度提高达1.3倍，同时预测性能下降可忽略不计。在5个大型分类任务上进一步评估，速度提升高达2.2倍。", "conclusion": "我们的结果表明，提前退出是提高表格上下文学习效率的一种有效且实用的策略。", "translation": "表格基础模型通过上下文学习在各种表格学习任务中表现出强大的性能，无需任何下游微调即可提供强大的泛化能力。然而，它们的推理时间成本仍然很高，特别是对于大型数据集。为了解决这个问题，我们提出了提前停止上下文学习过程。我们通过在每个Transformer编码器层之后动态评估是否停止上下文学习来实现这一点。一旦停止，我们使用预训练的逐层解码器解码嵌入。在34个小型分类任务上的实验表明，提前停止上下文学习可将推理速度提高达1.3倍，同时预测性能下降可忽略不计。为了评估可扩展性，我们进一步在五个大型分类任务上评估了我们的方法，实现了高达2.2倍的速度提升。我们的结果表明，提前退出是提高表格上下文学习效率的一种有效且实用的策略。", "summary": "本文针对表格基础模型在上下文学习中推理成本高的问题，提出了一种提前停止上下文学习的策略。该方法通过在Transformer编码器层后动态评估停止点，并使用预训练解码器解码嵌入。实验结果显示，在小型分类任务上可加速1.3倍且性能无显著下降，在大型任务上可加速高达2.2倍，证明了提前退出在提高表格上下文学习效率方面的潜力。", "keywords": "表格上下文学习, 提前停止, 推理加速, Transformer编码器", "comments": "该论文提出了一种创新的方法来解决表格上下文学习中推理时间成本高的问题，即采用提前停止策略。这种方法通过动态评估停止点，有效平衡了计算效率和模型性能，对于实际应用具有重要意义。其创新性在于将早期退出机制应用于表格领域的上下文学习，为未来基础模型在资源受限环境下的部署提供了新的思路。"}}
{"id": "2506.21233", "title": "ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation", "authors": ["Xiwei Xuan", "Ziquan Deng", "Kwan-Liu Ma"], "summary": "Training-free open-vocabulary semantic segmentation (OVS) aims to segment\nimages given a set of arbitrary textual categories without costly model\nfine-tuning. Existing solutions often explore attention mechanisms of\npre-trained models, such as CLIP, or generate synthetic data and design complex\nretrieval processes to perform OVS. However, their performance is limited by\nthe capability of reliant models or the suboptimal quality of reference sets.\nIn this work, we investigate the largely overlooked data quality problem for\nthis challenging dense scene understanding task, and identify that a\nhigh-quality reference set can significantly benefit training-free OVS. With\nthis observation, we introduce a data-quality-oriented framework, comprising a\ndata pipeline to construct a reference set with well-paired segment-text\nembeddings and a simple similarity-based retrieval to unveil the essential\neffect of data. Remarkably, extensive evaluations on ten benchmark datasets\ndemonstrate that our method outperforms all existing training-free OVS\napproaches, highlighting the importance of data-centric design for advancing\nOVS without training. Our code is available at https://github.com/xiweix/ReME .", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21233v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21233v1", "AI": {"title_translation": "ReME：一个用于免训练开放词汇分割的数据中心框架", "tldr": "ReME通过关注高质量的参考数据集和简单的检索方法，显著提升了免训练开放词汇分割的性能，超越了现有方法。", "motivation": "现有的免训练开放词汇语义分割（OVS）方法受限于所依赖模型的能力或参考集质量不佳。本文旨在解决这一被忽视的数据质量问题。", "method": "本研究提出了一个以数据质量为导向的框架ReME，包含一个数据管道用于构建具有良好配对的片段-文本嵌入的参考集，并采用简单的基于相似度的检索方法。", "result": "在十个基准数据集上的广泛评估表明，ReME方法优于所有现有的免训练OVS方法。", "conclusion": "数据中心设计对于无需训练的开放词汇分割的进步至关重要。", "translation": "免训练开放词汇语义分割（OVS）旨在在不进行昂贵的模型微调的情况下，根据一组任意文本类别对图像进行分割。现有解决方案通常探索预训练模型（如CLIP）的注意力机制，或生成合成数据并设计复杂的检索过程来执行OVS。然而，它们的性能受限于所依赖模型的能力或参考集质量不佳。在这项工作中，我们调查了这项具有挑战性的密集场景理解任务中在很大程度上被忽视的数据质量问题，并发现高质量的参考集可以显著有益于免训练OVS。基于这一观察，我们引入了一个以数据质量为导向的框架，包括一个用于构建具有良好配对的片段-文本嵌入的参考集的数据管道，以及一个简单的基于相似度的检索方法，以揭示数据的本质影响。值得注意的是，在十个基准数据集上的广泛评估表明，我们的方法优于所有现有的免训练OVS方法，突出了以数据为中心的设计对于无需训练推进OVS的重要性。我们的代码可在https://github.com/xiweix/ReME 获取。", "summary": "ReME是一个新颖的以数据为中心的框架，用于免训练开放词汇语义分割。它解决了现有方法因参考集质量不佳而受到的限制。通过构建一个包含良好配对的片段-文本嵌入的高质量参考集，并采用简单的基于相似度的检索，ReME显著提升了性能。评估结果表明，它超越了所有当前的免训练OVS方法，强调了数据质量在此任务中的重要性。", "keywords": "免训练OVS, 数据中心, 语义分割, 参考集, 数据质量", "comments": "本文的创新之处在于将研究重点从复杂的模型设计或检索过程转移到数据质量这一基本方面。这种以数据为中心的方法简化了过程，同时取得了卓越的成果，为免训练开放词汇分割提供了一个新的有效方向。"}}
{"id": "2506.21411", "title": "Distributed Cross-Channel Hierarchical Aggregation for Foundation Models", "authors": ["Aristeidis Tsaris", "Isaac Lyngaas", "John Lagregren", "Mohamed Wahib", "Larry York", "Prasanna Balaprakash", "Dan Lu", "Feiyi Wang", "Xiao Wang"], "summary": "Vision-based scientific foundation models hold significant promise for\nadvancing scientific discovery and innovation. This potential stems from their\nability to aggregate images from diverse sources such as varying physical\ngroundings or data acquisition systems and to learn spatio-temporal\ncorrelations using transformer architectures. However, tokenizing and\naggregating images can be compute-intensive, a challenge not fully addressed by\ncurrent distributed methods. In this work, we introduce the Distributed\nCross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets\nwith a large number of channels across image modalities. Our method is\ncompatible with any model-parallel strategy and any type of vision transformer\narchitecture, significantly improving computational efficiency. We evaluated\nD-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated\nwith tensor parallelism and model sharding, our approach achieved up to a 75%\nreduction in memory usage and more than doubled sustained throughput on up to\n1,024 AMD GPUs on the Frontier Supercomputer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21411v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21411v1", "AI": {"title_translation": "分布式跨通道分层聚合基础模型", "tldr": "本文提出了分布式跨通道分层聚合（D-CHAG）方法，用于视觉科学基础模型，显著提高了多通道图像数据的计算效率并减少了内存使用。", "motivation": "当前分布式方法未能充分解决视觉科学基础模型在对图像进行标记和聚合时计算密集的问题，特别是对于具有大量通道的数据集。", "method": "本文引入了分布式跨通道分层聚合（D-CHAG）方法，专为具有大量图像模态通道的数据集设计。该方法兼容任何模型并行策略和任何类型的视觉Transformer架构。通过与张量并行和模型分片集成，在超光谱成像和天气预报任务上进行了评估。", "result": "当与张量并行和模型分片集成时，D-CHAG在多达1,024个AMD GPU上实现了高达75%的内存使用量减少，并在Frontier超级计算机上将持续吞吐量提高了一倍以上。", "conclusion": "D-CHAG显著提高了视觉基础模型处理大型多通道数据集的计算效率和内存利用率，并在超级计算机上展示了强大的可扩展性。", "translation": "基于视觉的科学基础模型在推进科学发现和创新方面具有巨大潜力。这种潜力源于它们能够聚合来自不同来源（例如不同的物理基础或数据采集系统）的图像，并使用Transformer架构学习时空相关性。然而，对图像进行标记化和聚合可能计算密集，这是一个当前分布式方法尚未完全解决的挑战。在这项工作中，我们引入了分布式跨通道分层聚合（D-CHAG）方法，专为具有大量图像模态通道的数据集设计。我们的方法兼容任何模型并行策略和任何类型的视觉Transformer架构，显著提高了计算效率。我们在超光谱成像和天气预报任务上评估了D-CHAG。当与张量并行和模型分片集成时，我们的方法在Frontier超级计算机上使用多达1,024个AMD GPU时，实现了高达75%的内存使用量减少，并将持续吞吐量提高了一倍以上。", "summary": "本文提出了一种名为分布式跨通道分层聚合（D-CHAG）的新方法，旨在解决基于视觉的科学基础模型在处理多通道图像数据时面临的计算挑战。D-CHAG兼容多种模型并行策略和视觉Transformer架构，能够显著提高计算效率并减少内存使用。在超光谱成像和天气预报任务上的评估表明，该方法在大型GPU集群上实现了显著的性能提升。", "keywords": "分布式聚合, 基础模型, 视觉Transformer, 计算效率, 高光谱成像", "comments": "该论文为训练大型视觉基础模型（特别是处理高维数据如高光谱图像的模型）引入了一项重要的优化。D-CHAG方法与现有并行策略的兼容性以及在超级计算机上实现的显著性能提升（内存减少、吞吐量增加）突显了其实用价值和加速科学发现的潜力。"}}
{"id": "2506.21237", "title": "DiMPLe -- Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation", "authors": ["Umaima Rahman", "Mohammad Yaqub", "Dwarikanath Mahapatra"], "summary": "We introduce DiMPLe (Disentangled Multi-Modal Prompt Learning), a novel\napproach to disentangle invariant and spurious features across vision and\nlanguage modalities in multi-modal learning. Spurious correlations in visual\ndata often hinder out-of-distribution (OOD) performance. Unlike prior methods\nfocusing solely on image features, DiMPLe disentangles features within and\nacross modalities while maintaining consistent alignment, enabling better\ngeneralization to novel classes and robustness to distribution shifts. Our\nmethod combines three key objectives: (1) mutual information minimization\nbetween invariant and spurious features, (2) spurious feature regularization,\nand (3) contrastive learning on invariant features. Extensive experiments\ndemonstrate DiMPLe demonstrates superior performance compared to CoOp-OOD, when\naveraged across 11 diverse datasets, and achieves absolute gains of 15.27 in\nbase class accuracy and 44.31 in novel class accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21237v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21237v1", "AI": {"title_translation": "DiMPLe -- 解耦多模态提示学习：通过不变性和虚假特征分离增强分布外对齐", "tldr": "DiMPLe是一种新的多模态提示学习方法，通过解耦不变性和虚假特征来提高OOD性能和泛化能力。", "motivation": "视觉数据中的虚假相关性通常会阻碍分布外（OOD）性能。现有方法主要关注图像特征，未能有效解决跨模态的特征解耦问题，导致泛化能力和对分布偏移的鲁棒性不足。", "method": "DiMPLe通过在视觉和语言模态中解耦不变特征和虚假特征，同时保持一致的对齐。它结合了三个关键目标：1) 不变特征和虚假特征之间的互信息最小化；2) 虚假特征正则化；3) 在不变特征上进行对比学习。", "result": "DiMPLe在11个不同数据集上平均表现优于CoOp-OOD，在基类准确率上绝对提高了15.27，在新类准确率上绝对提高了44.31。", "conclusion": "DiMPLe通过有效解耦多模态特征，显著提升了模型在分布外场景下的泛化能力和对分布偏移的鲁棒性。", "translation": "我们引入了DiMPLe（解耦多模态提示学习），这是一种在多模态学习中解耦视觉和语言模态之间不变特征和虚假特征的新方法。视觉数据中的虚假相关性通常会阻碍分布外（OOD）性能。与以往仅关注图像特征的方法不同，DiMPLe在模态内部和跨模态解耦特征，同时保持一致的对齐，从而能够更好地泛化到新类别并对分布偏移具有鲁棒性。我们的方法结合了三个关键目标：（1）不变特征和虚假特征之间的互信息最小化，（2）虚假特征正则化，以及（3）在不变特征上进行对比学习。广泛的实验表明，DiMPLe在11个不同数据集上的平均性能优于CoOp-OOD，并在基类准确率上获得了15.27的绝对增益，在新类准确率上获得了44.31的绝对增益。", "summary": "DiMPLe是一种新颖的多模态提示学习方法，旨在通过解耦视觉和语言模态中的不变特征和虚假特征来提高模型在分布外（OOD）场景下的性能。该方法结合了互信息最小化、虚假特征正则化和不变特征上的对比学习三个核心目标，以实现更好的泛化能力和对分布偏移的鲁棒性。实验证明，DiMPLe在多个数据集上显著优于现有方法，尤其在新类准确率上表现突出。", "keywords": "多模态学习, 提示学习, 分布外泛化, 特征解耦, 虚假相关性", "comments": "DiMPLe的创新之处在于其对多模态学习中不变和虚假特征的精细解耦，这超越了以往仅关注单一模态特征的方法。通过在模态内部和跨模态进行解耦，并结合精心设计的三个目标函数，它有效提升了模型在面对分布外数据时的泛化能力和鲁棒性，特别是对新类别的识别性能有显著提升，这对于实际应用中的模型可靠性至关重要。"}}
{"id": "2506.21249", "title": "Temporal Rate Reduction Clustering for Human Motion Segmentation", "authors": ["Xianghan Meng", "Zhengyu Tong", "Zhiyuan Huang", "Chun-Guang Li"], "summary": "Human Motion Segmentation (HMS), which aims to partition videos into\nnon-overlapping human motions, has attracted increasing research attention\nrecently. Existing approaches for HMS are mainly dominated by subspace\nclustering methods, which are grounded on the assumption that high-dimensional\ntemporal data align with a Union-of-Subspaces (UoS) distribution. However, the\nframes in video capturing complex human motions with cluttered backgrounds may\nnot align well with the UoS distribution. In this paper, we propose a novel\napproach for HMS, named Temporal Rate Reduction Clustering\n($\\text{TR}^2\\text{C}$), which jointly learns structured representations and\naffinity to segment the frame sequences in video. Specifically, the structured\nrepresentations learned by $\\text{TR}^2\\text{C}$ maintain temporally consistent\nand align well with a UoS structure, which is favorable for the HMS task. We\nconduct extensive experiments on five benchmark HMS datasets and achieve\nstate-of-the-art performances with different feature extractors.", "comment": "The paper is accepted by ICCV 2025. The first two authors are equally\n  contributed", "pdf_url": "http://arxiv.org/pdf/2506.21249v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21249v1", "AI": {"title_translation": "用于人体运动分割的时序速率降低聚类", "tldr": "本文提出了一种名为时序速率降低聚类（TR²C）的新型人体运动分割（HMS）方法，旨在解决现有子空间聚类方法在处理复杂运动时UoS假设不适配的问题，并在多个基准数据集上取得了最先进的性能。", "motivation": "现有的人体运动分割（HMS）方法主要基于子空间聚类，其假设高维时序数据与子空间联合（UoS）分布对齐。然而，在捕捉复杂人体运动和杂乱背景的视频中，帧可能无法很好地与UoS分布对齐，这限制了现有方法的性能。", "method": "本文提出了一种新颖的HMS方法，名为时序速率降低聚类（TR²C）。该方法联合学习结构化表示和亲和力，以分割视频中的帧序列。TR²C学习到的结构化表示能够保持时间上的一致性，并与UoS结构良好对齐，这有利于HMS任务。", "result": "在五个基准HMS数据集上进行了广泛实验，TR²C使用不同的特征提取器均取得了最先进的性能。", "conclusion": "所提出的时序速率降低聚类（TR²C）方法通过学习鲁棒且时间上一致的表示，有效解决了传统基于UoS的人体运动分割方法的局限性，从而实现了卓越的分割性能。", "translation": "人体运动分割（HMS）旨在将视频划分为不重叠的人体运动，最近引起了越来越多的研究关注。现有的HMS方法主要由子空间聚类方法主导，这些方法基于高维时序数据与子空间联合（UoS）分布对齐的假设。然而，捕捉复杂人体运动和杂乱背景的视频帧可能无法很好地与UoS分布对齐。在本文中，我们提出了一种新颖的HMS方法，名为时序速率降低聚类（TR²C），它联合学习结构化表示和亲和力以分割视频中的帧序列。具体而言，TR²C学习的结构化表示保持时间上的一致性并与UoS结构良好对齐，这有利于HMS任务。我们在五个基准HMS数据集上进行了广泛的实验，并使用不同的特征提取器取得了最先进的性能。", "summary": "本文提出了一种名为时序速率降低聚类（TR²C）的新型人体运动分割（HMS）方法，旨在解决现有子空间聚类方法中UoS假设在复杂背景下不适用的问题。TR²C通过联合学习结构化表示和亲和力来分割视频帧序列，其学习到的表示保持时间上的一致性并与UoS结构良好对齐。实验证明，TR²C在五个基准HMS数据集上取得了最先进的性能。", "keywords": "人体运动分割, 时序速率降低聚类, 子空间聚类, 结构化表示, 视频分割", "comments": "该论文通过提出TR²C方法，巧妙地解决了现有HMS子空间聚类方法中UoS假设的局限性。其创新点在于联合学习结构化表示和亲和力，使得学习到的表示既保持时间一致性又能更好地适应UoS结构。在多个基准数据集上取得最先进的性能，充分证明了该方法的有效性和重要性。"}}
{"id": "2506.21429", "title": "Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort", "authors": ["Franco Rugolon", "Thomas Jack Samuels", "Stephan Hau", "Lennart Högman"], "summary": "This study investigates the efficacy of using multimodal machine learning\ntechniques to detect deception in dyadic interactions, focusing on the\nintegration of data from both the deceiver and the deceived. We compare early\nand late fusion approaches, utilizing audio and video data - specifically,\nAction Units and gaze information - across all possible combinations of\nmodalities and participants. Our dataset, newly collected from Swedish native\nspeakers engaged in truth or lie scenarios on emotionally relevant topics,\nserves as the basis for our analysis. The results demonstrate that\nincorporating both speech and facial information yields superior performance\ncompared to single-modality approaches. Moreover, including data from both\nparticipants significantly enhances deception detection accuracy, with the best\nperformance (71%) achieved using a late fusion strategy applied to both\nmodalities and participants. These findings align with psychological theories\nsuggesting differential control of facial and vocal expressions during initial\ninteractions. As the first study of its kind on a Scandinavian cohort, this\nresearch lays the groundwork for future investigations into dyadic\ninteractions, particularly within psychotherapy settings.", "comment": "40 pages, 2 figures, 2 tables. To be submitted in Behavior Research\n  Methods", "pdf_url": "http://arxiv.org/pdf/2506.21429v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21429v1", "AI": {"title_translation": "使用多模态机器学习在两人对话中进行欺骗检测：一项针对瑞典人群的研究", "tldr": "本研究使用多模态机器学习（音频和视频）在瑞典人群中检测两人对话中的欺骗行为。结果显示，结合说话者和被欺骗者的多模态数据（特别是晚期融合）能显著提高欺骗检测准确率，达到71%。", "motivation": "本研究旨在探索使用多模态机器学习技术提高两人互动中欺骗检测的有效性，并关注整合欺骗者和被欺骗者双方数据的重要性。", "method": "研究采用多模态机器学习方法，整合音频和视频数据（特别是动作单元和凝视信息），比较了早期融合和晚期融合策略。数据集是新收集的瑞典母语者在情感相关主题的真实或谎言情景中的数据。分析涵盖了所有可能的模态和参与者组合。", "result": "结果表明，结合语音和面部信息的多模态方法优于单一模态方法。此外，纳入双方参与者的数据显著提高了欺骗检测的准确性。最佳性能（71%）是通过对所有模态和参与者应用晚期融合策略实现的。", "conclusion": "研究结果与心理学理论一致，表明在初始互动中面部和声音表情的差异化控制。作为首个针对斯堪的纳维亚人群的同类研究，它为未来在两人互动（特别是心理治疗环境）中的研究奠定了基础。", "translation": "本研究旨在探讨使用多模态机器学习技术在两人互动中检测欺骗的有效性，重点关注整合欺骗者和被欺骗者双方的数据。我们比较了早期和晚期融合方法，利用音频和视频数据——特别是动作单元和凝视信息——跨所有可能的模态和参与者组合。我们的数据集是新收集的瑞典母语者在情感相关主题的真实或谎言情景中的数据，作为我们分析的基础。结果表明，结合语音和面部信息比单一模态方法表现更优越。此外，纳入双方参与者的数据显著提高了欺骗检测的准确性，最佳性能（71%）是通过对所有模态和参与者应用晚期融合策略实现的。这些发现与心理学理论一致，表明在初始互动中面部和声音表情的差异化控制。作为首个针对斯堪的纳维亚人群的同类研究，这项研究为未来对两人互动，特别是在心理治疗环境中的调查奠定了基础。", "summary": "本研究探讨了在两人互动中使用多模态机器学习检测欺骗的有效性，并首次在瑞典人群中进行。研究比较了早期和晚期融合策略，利用欺骗者和被欺骗者的音频（语音）和视频（动作单元、凝视）数据。结果显示，结合语音和面部信息的多模态方法以及纳入双方参与者的数据显著提高了欺骗检测的准确性，最佳准确率为71%（采用晚期融合策略）。这些发现支持了心理学理论，并为未来在心理治疗等情境下的两人互动研究奠定了基础。", "keywords": "欺骗检测, 多模态机器学习, 两人互动, 晚期融合, 瑞典人群", "comments": "这项研究的创新之处在于首次在斯堪的纳维亚人群中进行双人互动中的欺骗检测，并强调了结合欺骗者和被欺骗者双方多模态数据（音频和视频）的重要性。其发现对于理解欺骗行为的心理学机制具有价值，并为未来在心理治疗等实际应用中的欺骗检测研究提供了基础。"}}
{"id": "2506.21260", "title": "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic", "authors": ["Munish Monga", "Vishal Chudasama", "Pankaj Wasnik", "Biplab Banerjee"], "summary": "Real-world object detection systems, such as those in autonomous driving and\nsurveillance, must continuously learn new object categories and simultaneously\nadapt to changing environmental conditions. Existing approaches, Class\nIncremental Object Detection (CIOD) and Domain Incremental Object Detection\n(DIOD) only address one aspect of this challenge. CIOD struggles in unseen\ndomains, while DIOD suffers from catastrophic forgetting when learning new\nclasses, limiting their real-world applicability. To overcome these\nlimitations, we introduce Dual Incremental Object Detection (DuIOD), a more\npractical setting that simultaneously handles class and domain shifts in an\nexemplar-free manner. We propose DuET, a Task Arithmetic-based model merging\nframework that enables stable incremental learning while mitigating sign\nconflicts through a novel Directional Consistency Loss. Unlike prior methods,\nDuET is detector-agnostic, allowing models like YOLO11 and RT-DETR to function\nas real-time incremental object detectors. To comprehensively evaluate both\nretention and adaptation, we introduce the Retention-Adaptability Index (RAI),\nwhich combines the Average Retention Index (Avg RI) for catastrophic forgetting\nand the Average Generalization Index for domain adaptability into a common\nground. Extensive experiments on the Pascal Series and Diverse Weather Series\ndemonstrate DuET's effectiveness, achieving a +13.12% RAI improvement while\npreserving 89.3% Avg RI on the Pascal Series (4 tasks), as well as a +11.39%\nRAI improvement with 88.57% Avg RI on the Diverse Weather Series (3 tasks),\noutperforming existing methods.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21260v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21260v1", "AI": {"title_translation": "DuET：通过无样本任务算术的双重增量目标检测", "tldr": "DuET提出了一种双重增量目标检测（DuIOD）方法，通过无样本的任务算术框架同时处理类别和域的变化，并在多个数据集上优于现有方法。", "motivation": "现有的类别增量目标检测（CIOD）和域增量目标检测（DIOD）方法分别在未见过的域中表现不佳或遭受灾难性遗忘，限制了它们在需要持续学习新类别并适应不断变化环境的真实世界目标检测系统中的应用。", "method": "我们引入了双重增量目标检测（DuIOD）设置，它以无样本的方式同时处理类别和域的转变。我们提出了DuET，一个基于任务算术的模型合并框架，通过新颖的方向一致性损失来缓解符号冲突，实现稳定的增量学习。DuET是检测器无关的，并引入了保留-适应性指数（RAI）来全面评估保留和适应性。", "result": "DuET在Pascal系列（4个任务）上实现了+13.12%的RAI改进，同时保留了89.3%的平均保留指数（Avg RI）；在多样天气系列（3个任务）上实现了+11.39%的RAI改进，保留了88.57%的平均保留指数（Avg RI），均优于现有方法。", "conclusion": "DuET通过其基于任务算术的模型合并框架和方向一致性损失，有效解决了双重增量目标检测的挑战，并在保留和适应性方面取得了显著改进，证明了其在实际应用中的优越性。", "translation": "真实世界的目标检测系统，例如自动驾驶和监控中的系统，必须持续学习新的目标类别并同时适应不断变化的环境条件。现有的方法，即类别增量目标检测（CIOD）和域增量目标检测（DIOD）仅解决了这一挑战的一个方面。CIOD在未见过的域中表现不佳，而DIOD在学习新类别时遭受灾难性遗忘，限制了它们的实际应用。为了克服这些限制，我们引入了双重增量目标检测（DuIOD），这是一种更实用的设置，可以以无样本的方式同时处理类别和域的转变。我们提出了DuET，一个基于任务算术的模型合并框架，通过新颖的方向一致性损失来缓解符号冲突，实现稳定的增量学习。与之前的方法不同，DuET是检测器无关的，允许YOLO11和RT-DETR等模型作为实时增量目标检测器。为了全面评估保留和适应性，我们引入了保留-适应性指数（RAI），它将用于灾难性遗忘的平均保留指数（Avg RI）和用于域适应性的平均泛化指数结合到一个共同的基础中。在Pascal系列和多样天气系列上的大量实验证明了DuET的有效性，在Pascal系列（4个任务）上实现了+13.12%的RAI改进，同时保留了89.3%的平均保留指数（Avg RI），以及在多样天气系列（3个任务）上实现了+11.39%的RAI改进，保留了88.57%的平均保留指数（Avg RI），均优于现有方法。", "summary": "本研究提出了DuET，一个基于任务算术的框架，用于解决双重增量目标检测（DuIOD）问题，该问题要求系统同时学习新类别并适应不同的环境域，且无需存储旧样本。DuET通过引入方向一致性损失来稳定增量学习并缓解符号冲突，并且具有检测器无关的特性。为了全面评估，论文还提出了保留-适应性指数（RAI）。实验结果表明，DuET在多个数据集上显著优于现有方法，在保留旧知识和适应新域方面均表现出色。", "keywords": "增量学习, 目标检测, 任务算术, 域适应, 灾难性遗忘", "comments": "DuET的创新之处在于其提出了双重增量目标检测（DuIOD）这一更贴近实际的设置，并利用任务算术框架以无样本方式解决该问题。其检测器无关的特性大大增加了方法的普适性。此外，引入保留-适应性指数（RAI）为增量学习的评估提供了一个更全面的衡量标准，对于后续研究具有指导意义。"}}
{"id": "2506.21270", "title": "Video Virtual Try-on with Conditional Diffusion Transformer Inpainter", "authors": ["Cheng Zou", "Senlin Cheng", "Bolei Xu", "Dandan Zheng", "Xiaobo Li", "Jingdong Chen", "Ming Yang"], "summary": "Video virtual try-on aims to naturally fit a garment to a target person in\nconsecutive video frames. It is a challenging task, on the one hand, the output\nvideo should be in good spatial-temporal consistency, on the other hand, the\ndetails of the given garment need to be preserved well in all the frames.\nNaively using image-based try-on methods frame by frame can get poor results\ndue to severe inconsistency. Recent diffusion-based video try-on methods,\nthough very few, happen to coincide with a similar solution: inserting temporal\nattention into image-based try-on model to adapt it for video try-on task,\nwhich have shown improvements but there still exist inconsistency problems. In\nthis paper, we propose ViTI (Video Try-on Inpainter), formulate and implement\nvideo virtual try-on as a conditional video inpainting task, which is different\nfrom previous methods. In this way, we start with a video generation problem\ninstead of an image-based try-on problem, which from the beginning has a better\nspatial-temporal consistency. Specifically, at first we build a video\ninpainting framework based on Diffusion Transformer with full 3D\nspatial-temporal attention, and then we progressively adapt it for video\ngarment inpainting, with a collection of masking strategies and multi-stage\ntraining. After these steps, the model can inpaint the masked garment area with\nappropriate garment pixels according to the prompt with good spatial-temporal\nconsistency. Finally, as other try-on methods, garment condition is added to\nthe model to make sure the inpainted garment appearance and details are as\nexpected. Both quantitative and qualitative experimental results show that ViTI\nis superior to previous works.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21270v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21270v1", "AI": {"title_translation": "基于条件扩散Transformer修复器的视频虚拟试穿", "tldr": "本文提出ViTI，一种基于条件扩散Transformer修复器的视频虚拟试穿方法，通过将视频试穿任务建模为条件视频修复，实现了更好的时空一致性和服装细节保留。", "motivation": "视频虚拟试穿任务具有挑战性，需要良好的时空一致性和服装细节保留。传统基于图像的方法逐帧处理会导致严重不一致。近期扩散基视频试穿方法虽有改进，但仍存在不一致问题。", "method": "本文提出ViTI（Video Try-on Inpainter），将视频虚拟试穿建模为条件视频修复任务。该方法构建了一个基于Diffusion Transformer的视频修复框架，并使用全3D时空注意力。通过一系列掩码策略和多阶段训练，逐步将其应用于视频服装修复。最终，模型加入了服装条件以确保修复后的服装外观和细节符合预期。", "result": "定量和定性实验结果表明ViTI优于现有工作。", "conclusion": "ViTI通过将视频试穿建模为条件视频修复任务，有效解决了时空一致性和服装细节保留问题，并优于现有方法。", "translation": "视频虚拟试穿旨在将服装自然地适配到连续视频帧中的目标人物。这是一项具有挑战性的任务，一方面，输出视频应具有良好的时空一致性；另一方面，给定服装的细节需要在所有帧中得到很好的保留。简单地逐帧使用基于图像的试穿方法会由于严重的不一致性而得到糟糕的结果。最近基于扩散的视频试穿方法，尽管数量很少，但碰巧与类似的解决方案不谋而合：将时间注意力插入基于图像的试穿模型中，以使其适应视频试穿任务，这虽然显示出改进，但仍然存在不一致性问题。在本文中，我们提出了ViTI（Video Try-on Inpainter），将视频虚拟试穿公式化并实现为条件视频修复任务，这与以前的方法不同。通过这种方式，我们从视频生成问题而不是基于图像的试穿问题开始，这从一开始就具有更好的时空一致性。具体来说，我们首先构建了一个基于Diffusion Transformer的视频修复框架，该框架具有完整的3D时空注意力，然后我们通过一系列掩码策略和多阶段训练，逐步将其适应于视频服装修复。经过这些步骤后，模型可以根据提示使用适当的服装像素修复被遮罩的服装区域，并具有良好的时空一致性。最后，像其他试穿方法一样，将服装条件添加到模型中，以确保修复后的服装外观和细节符合预期。定量和定性实验结果均表明ViTI优于现有工作。", "summary": "视频虚拟试穿是一项要求高时空一致性和服装细节保留的挑战性任务。针对现有方法的不足，本文提出ViTI，将视频虚拟试穿重新定义为条件视频修复任务。ViTI基于带有全3D时空注意力的Diffusion Transformer构建，并通过多阶段训练和掩码策略逐步适应服装修复，最终加入服装条件以确保细节和外观。实验证明，ViTI在定量和定性上均优于现有方法，有效提升了视频虚拟试穿的效果。", "keywords": "视频虚拟试穿, 扩散Transformer, 视频修复, 时空一致性", "comments": "本文的创新点在于将视频虚拟试穿任务重新定义为条件视频修复任务，这与以往将图像试穿方法适配到视频或简单添加时间注意力的方法不同。通过利用带有全3D时空注意力的Diffusion Transformer，从视频生成角度出发，有效提升了时空一致性，这是该方法的核心优势。"}}
{"id": "2506.21461", "title": "A Keyword-Based Technique to Evaluate Broad Question Answer Script", "authors": ["Tamim Al Mahmud", "Md Gulzar Hussain", "Sumaiya Kabir", "Hasnain Ahmad", "Mahmudus Sobhan"], "summary": "Evaluation is the method of assessing and determining the educational system\nthrough various techniques such as verbal or viva-voice test, subjective or\nobjective written test. This paper presents an efficient solution to evaluate\nthe subjective answer script electronically. In this paper, we proposed and\nimplemented an integrated system that examines and evaluates the written answer\nscript. This article focuses on finding the keywords from the answer script and\nthen compares them with the keywords that have been parsed from both open and\nclosed domain. The system also checks the grammatical and spelling errors in\nthe answer script. Our proposed system tested with answer scripts of 100\nstudents and gives precision score 0.91.", "comment": "ACM Conference Proceedings (9 Pages)", "pdf_url": "http://arxiv.org/pdf/2506.21461v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21461v1", "AI": {"title_translation": "一种基于关键词的宽泛问题答案脚本评估技术", "tldr": "本文提出了一种基于关键词的集成系统，用于电子评估主观答案脚本，该系统通过比对关键词、检查语法和拼写错误，并在100份学生答案脚本上实现了0.91的准确率。", "motivation": "为了提供一种高效的电子评估主观答案脚本的解决方案，以改进教育系统中的评估方法。", "method": "本文提出并实现了一个集成系统，该系统从答案脚本中查找关键词，然后将其与从开放域和封闭域解析的关键词进行比较，并检查答案脚本中的语法和拼写错误。", "result": "该系统用100名学生的答案脚本进行了测试，并获得了0.91的准确率分数。", "conclusion": "本文提出了一种高效的、基于关键词的集成系统，用于电子评估主观答案脚本，并取得了令人满意的准确率。", "translation": "评估是通过各种技术（例如口头或口试、主观或客观书面测试）评估和确定教育系统的方法。本文提出了一种有效的方法，可以电子方式评估主观答案脚本。在本文中，我们提出并实现了一个检查和评估书面答案脚本的集成系统。本文重点从答案脚本中查找关键词，然后将其与从开放域和封闭域解析的关键词进行比较。该系统还检查答案脚本中的语法和拼写错误。我们提出的系统用100名学生的答案脚本进行了测试，并给出了0.91的准确率分数。", "summary": "本文介绍了一种基于关键词的集成系统，用于电子评估主观答案脚本。该系统识别答案中的关键词，将其与特定领域关键词进行比较，并纠正语法和拼写错误。该系统在100份学生答案脚本上进行了测试，取得了0.91的准确率，为自动化评估提供了一种高效的解决方案。", "keywords": "关键词评估, 答案脚本评估, 电子评估, 主观测试, 准确率", "comments": "本文提出了一种实用且高效的方法，旨在自动化主观答案脚本的评估，这是教育评估中一项具有挑战性的任务。其创新之处在于将关键词匹配与语法和拼写检查相结合的集成系统。0.91的准确率表明其在实际应用中的潜力，有望显著改进手动评估过程。然而，摘要中没有详细说明关键词提取的方法或用于比较和错误检查的具体算法，这可能是理解其完整技术深度的局限性。"}}
{"id": "2506.21276", "title": "WordCon: Word-level Typography Control in Scene Text Rendering", "authors": ["Wenda Shi", "Yiren Song", "Zihan Rao", "Dengming Zhang", "Jiaming Liu", "Xingxing Zou"], "summary": "Achieving precise word-level typography control within generated images\nremains a persistent challenge. To address it, we newly construct a word-level\ncontrolled scene text dataset and introduce the Text-Image Alignment (TIA)\nframework. This framework leverages cross-modal correspondence between text and\nlocal image regions provided by grounding models to enhance the Text-to-Image\n(T2I) model training. Furthermore, we propose WordCon, a hybrid\nparameter-efficient fine-tuning (PEFT) method. WordCon reparameterizes\nselective key parameters, improving both efficiency and portability. This\nallows seamless integration into diverse pipelines, including artistic text\nrendering, text editing, and image-conditioned text rendering. To further\nenhance controllability, the masked loss at the latent level is applied to\nguide the model to concentrate on learning the text region in the image, and\nthe joint-attention loss provides feature-level supervision to promote\ndisentanglement between different words. Both qualitative and quantitative\nresults demonstrate the superiority of our method to the state of the art. The\ndatasets and source code will be available for academic use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21276v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21276v1", "AI": {"title_translation": "WordCon：场景文本渲染中的词级排版控制", "tldr": "本文提出了WordCon，一种混合参数高效微调方法，结合新的词级受控数据集和文本-图像对齐（TIA）框架，旨在解决生成图像中精确词级排版控制的挑战，并实现了超越现有技术的性能。", "motivation": "在生成图像中实现精确的词级排版控制仍然是一个持续的挑战。", "method": "本文构建了一个新的词级受控场景文本数据集，并引入了文本-图像对齐（TIA）框架，该框架利用接地模型提供的文本与局部图像区域之间的跨模态对应关系来增强文本到图像（T2I）模型训练。此外，提出了WordCon，一种混合参数高效微调（PEFT）方法，通过重新参数化选择性关键参数来提高效率和可移植性。为了增强可控性，还应用了潜在层面的掩蔽损失来引导模型集中学习图像中的文本区域，以及联合注意力损失提供特征级监督以促进不同单词之间的解耦。", "result": "定性和定量结果均表明该方法优于现有技术。", "conclusion": "WordCon方法通过构建新的词级受控数据集、引入文本-图像对齐（TIA）框架以及提出混合参数高效微调（PEFT）技术，有效解决了生成图像中词级排版控制的挑战，并取得了优于现有技术的性能。", "translation": "在生成图像中实现精确的词级排版控制仍然是一个持续的挑战。为了解决这个问题，我们新构建了一个词级受控场景文本数据集，并引入了文本-图像对齐（TIA）框架。该框架利用了接地模型提供的文本与局部图像区域之间的跨模态对应关系，以增强文本到图像（T2I）模型的训练。此外，我们提出了WordCon，一种混合参数高效微调（PEFT）方法。WordCon重新参数化了选择性关键参数，提高了效率和可移植性。这使得它能够无缝集成到各种管道中，包括艺术文本渲染、文本编辑和图像条件文本渲染。为了进一步增强可控性，我们应用了潜在层面的掩蔽损失来引导模型集中学习图像中的文本区域，并且联合注意力损失提供了特征级监督以促进不同单词之间的解耦。定性和定量结果都表明我们的方法优于现有技术。数据集和源代码将可供学术使用。", "summary": "本文针对生成图像中词级排版控制的难题，提出了WordCon方法。该方法通过构建新的词级受控场景文本数据集，并引入文本-图像对齐（TIA）框架来增强Text-to-Image模型训练。WordCon本身是一种混合参数高效微调（PEFT）方法，它重新参数化了关键参数以提高效率和可移植性，并结合了掩蔽损失和联合注意力损失以增强对文本区域的学习和不同单词的解耦。实验结果表明，该方法在词级排版控制方面取得了优于现有技术的表现。", "keywords": "词级排版控制, 场景文本渲染, 参数高效微调, 文本-图像对齐, WordCon", "comments": "本文创新性地结合了新的数据集构建、跨模态对齐框架（TIA）和混合参数高效微调（WordCon），有效解决了图像生成中的词级排版控制难题。其提出的损失函数（掩蔽损失和联合注意力损失）进一步增强了模型对文本区域的聚焦和不同单词的解耦能力，提升了可控性。该方法的效率和可移植性也使其在艺术文本渲染、文本编辑等多种应用场景中具有广泛潜力。"}}
{"id": "2506.21465", "title": "Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage", "authors": ["Gavin Lee Goodship", "Luis Miralles-Pechuan", "Stephen O'Sullivan"], "summary": "Extended Stability Runge-Kutta (ESRK) methods are crucial for solving\nlarge-scale computational problems in science and engineering, including\nweather forecasting, aerodynamic analysis, and complex biological modelling.\nHowever, balancing accuracy, stability, and computational efficiency remains\nchallenging, particularly for high-order, low-storage schemes. This study\nintroduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)\napproach for automated heuristic discovery, optimising low-storage ESRK\nmethods. Unlike traditional approaches that rely on manually designed\nheuristics or exhaustive numerical searches, our method leverages GA-driven\nmutations for search-space exploration and an RL-inspired state transition\nmechanism to refine heuristic selection dynamically. This enables systematic\nparameter reduction, preserving fourth-order accuracy while significantly\nimproving computational efficiency.The proposed GA-RL heuristic optimisation\nframework is validated through rigorous testing on benchmark problems,\nincluding the 1D and 2D Brusselator systems and the steady-state Navier-Stokes\nequations. The best-performing heuristic achieves a 25\\% reduction in IPOPT\nruntime compared to traditional ESRK optimisation processes while maintaining\nnumerical stability and accuracy. These findings demonstrate the potential of\nadaptive heuristic discovery to improve resource efficiency in high-fidelity\nsimulations and broaden the applicability of low-storage Runge-Kutta methods in\nreal-world computational fluid dynamics, physics simulations, and other\ndemanding fields. This work establishes a new paradigm in heuristic\noptimisation for numerical methods, opening pathways for further exploration\nusing Deep RL and AutoML-based heuristic search", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21465v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21465v1", "AI": {"title_translation": "优化四阶龙格-库塔方法：一种提高效率和降低存储的动态启发式方法", "tldr": "本研究提出了一种混合遗传算法（GA）和强化学习（RL）的动态启发式方法，用于优化低存储扩展稳定性龙格-库塔（ESRK）方法，显著提高了计算效率并保持了数值稳定性。", "motivation": "扩展稳定性龙格-库塔（ESRK）方法在解决大规模计算问题中至关重要，但平衡精度、稳定性和计算效率，特别是对于高阶、低存储方案，仍然具有挑战性。", "method": "本研究引入了一种混合遗传算法（GA）和强化学习（RL）方法，用于自动化启发式发现，以优化低存储ESRK方法。该方法利用GA驱动的变异进行搜索空间探索，并采用受RL启发的态转移机制动态地完善启发式选择，从而系统地减少参数并保持四阶精度。", "result": "所提出的GA-RL启发式优化框架在基准问题（包括一维和二维Brusselator系统以及稳态Navier-Stokes方程）上进行了严格测试。性能最佳的启发式方法与传统ESRK优化过程相比，将IPOPT运行时减少了25%，同时保持了数值稳定性和精度。", "conclusion": "本研究表明自适应启发式发现有潜力提高高保真模拟中的资源效率，并拓宽低存储龙格-库塔方法在实际计算流体力学、物理模拟和其他要求苛刻领域中的适用性。这项工作为数值方法的启发式优化建立了新范式。", "translation": "扩展稳定性龙格-库塔（ESRK）方法对于解决科学和工程中的大规模计算问题至关重要，包括天气预报、空气动力学分析和复杂的生物建模。然而，平衡精度、稳定性和计算效率仍然具有挑战性，特别是对于高阶、低存储方案。本研究引入了一种混合遗传算法（GA）和强化学习（RL）方法，用于自动化启发式发现，以优化低存储ESRK方法。与依赖手动设计启发式或穷举数值搜索的传统方法不同，我们的方法利用GA驱动的变异进行搜索空间探索，并采用受RL启发的态转移机制动态地完善启发式选择。这使得系统地减少参数成为可能，同时保持四阶精度并显著提高计算效率。所提出的GA-RL启发式优化框架通过对基准问题（包括一维和二维Brusselator系统以及稳态Navier-Stokes方程）的严格测试进行了验证。性能最佳的启发式方法与传统ESRK优化过程相比，将IPOPT运行时减少了25%，同时保持了数值稳定性和精度。这些发现表明自适应启发式发现有潜力提高高保真模拟中的资源效率，并拓宽低存储龙格-库塔方法在实际计算流体力学、物理模拟和其他要求苛刻领域中的适用性。这项工作为数值方法的启发式优化建立了新范式，为使用深度强化学习和基于AutoML的启发式搜索进一步探索开辟了道路。", "summary": "本研究提出了一种创新的混合遗传算法（GA）和强化学习（RL）框架，用于自动化优化低存储扩展稳定性龙格-库塔（ESRK）方法。该方法通过GA进行搜索空间探索，并利用RL动态优化启发式选择，旨在克服传统方法在平衡精度、稳定性和计算效率方面的挑战。实验结果表明，与传统方法相比，该框架能将IPOPT运行时减少25%，同时保持数值稳定性和精度，为高保真模拟中的资源效率提升和Runge-Kutta方法在实际应用中的推广提供了新范式。", "keywords": "龙格-库塔方法, 遗传算法, 强化学习, 启发式优化, 计算效率", "comments": "这篇论文的创新点在于将遗传算法（GA）和强化学习（RL）这两种强大的优化技术结合起来，用于自动化地发现和优化数值方法中的启发式策略，特别是针对低存储的四阶Runge-Kutta方法。这种动态启发式方法避免了手动设计启发式的局限性，并实现了显著的计算效率提升。它为数值方法的优化提供了一个新的范式，并预示了深度强化学习和AutoML在该领域应用的巨大潜力，对于计算流体力学、物理模拟等计算密集型领域具有重要意义。"}}
{"id": "2506.21287", "title": "HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation", "authors": ["Diego Biagini", "Nassir Navab", "Azade Farshad"], "summary": "Surgical Video Synthesis has emerged as a promising research direction\nfollowing the success of diffusion models in general-domain video generation.\nAlthough existing approaches achieve high-quality video generation, most are\nunconditional and fail to maintain consistency with surgical actions and\nphases, lacking the surgical understanding and fine-grained guidance necessary\nfor factual simulation. We address these challenges by proposing HieraSurg, a\nhierarchy-aware surgical video generation framework consisting of two\nspecialized diffusion models. Given a surgical phase and an initial frame,\nHieraSurg first predicts future coarse-grained semantic changes through a\nsegmentation prediction model. The final video is then generated by a\nsecond-stage model that augments these temporal segmentation maps with\nfine-grained visual features, leading to effective texture rendering and\nintegration of semantic information in the video space. Our approach leverages\nsurgical information at multiple levels of abstraction, including surgical\nphase, action triplets, and panoptic segmentation maps. The experimental\nresults on Cholecystectomy Surgical Video Generation demonstrate that the model\nsignificantly outperforms prior work both quantitatively and qualitatively,\nshowing strong generalization capabilities and the ability to generate higher\nframe-rate videos. The model exhibits particularly fine-grained adherence when\nprovided with existing segmentation maps, suggesting its potential for\npractical surgical applications.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21287v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21287v1", "AI": {"title_translation": "HieraSurg：分层感知扩散模型用于手术视频生成", "tldr": "HieraSurg是一个分层感知的扩散模型框架，通过两个专门的扩散模型生成手术视频，解决了现有方法在手术动作和阶段一致性方面的不足，并能生成高质量、高帧率的手术视频。", "motivation": "现有的手术视频生成方法大多是无条件的，并且未能保持与手术动作和阶段的一致性，缺乏手术理解和细粒度指导，无法进行真实模拟。", "method": "HieraSurg是一个分层感知的手术视频生成框架，包含两个专门的扩散模型。给定手术阶段和初始帧，首先通过分割预测模型预测未来的粗粒度语义变化。然后，第二阶段模型通过细粒度视觉特征增强这些时间分割图，生成最终视频，实现有效的纹理渲染和语义信息整合。该方法利用了多层抽象的手术信息，包括手术阶段、动作三元组和全景分割图。", "result": "在胆囊切除术视频生成上的实验结果表明，该模型在定量和定性上都显著优于现有工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有分割图时，模型表现出特别细粒度的依从性。", "conclusion": "HieraSurg通过其分层感知的方法解决了手术视频生成中保持一致性和细粒度指导的挑战，其优异的性能和对现有分割图的细粒度依从性表明了其在实际手术应用中的巨大潜力。", "translation": "手术视频合成已成为一个有前景的研究方向，这得益于扩散模型在通用领域视频生成方面的成功。尽管现有方法实现了高质量的视频生成，但大多数是无条件的，并且未能保持与手术动作和阶段的一致性，缺乏真实模拟所需的手术理解和细粒度指导。我们通过提出HieraSurg来解决这些挑战，HieraSurg是一个分层感知的手术视频生成框架，由两个专门的扩散模型组成。给定手术阶段和初始帧，HieraSurg首先通过分割预测模型预测未来的粗粒度语义变化。然后，最终视频由第二阶段模型生成，该模型通过细粒度视觉特征增强这些时间分割图，从而在视频空间中实现有效的纹理渲染和语义信息整合。我们的方法利用了多个抽象层次的手术信息，包括手术阶段、动作三元组和全景分割图。在胆囊切除术视频生成上的实验结果表明，该模型在定量和定性上都显著优于现有工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有分割图时，模型表现出特别细粒度的依从性，表明其在实际手术应用中的潜力。", "summary": "HieraSurg是一个新颖的分层感知扩散模型框架，旨在解决现有手术视频生成方法在保持手术动作和阶段一致性方面的不足。该框架包含两个专门的扩散模型：第一个模型预测未来的粗粒度语义变化，第二个模型则在此基础上生成包含细粒度视觉特征的最终视频。通过利用手术阶段、动作三元组和全景分割图等多层次抽象信息，HieraSurg能够生成高质量、高帧率的手术视频。实验结果表明，该模型在性能上显著优于现有技术，并具有强大的泛化能力和实际应用潜力。", "keywords": "扩散模型, 手术视频生成, 分层感知, 语义分割, 视频合成", "comments": "HieraSurg的创新之处在于其分层感知的双阶段扩散模型设计，有效解决了手术视频生成中关键的语义一致性问题。通过整合多层次的手术信息，模型不仅提升了生成视频的真实感和质量，还展现了优异的泛化能力和生成高帧率视频的能力。这对于需要高精度和上下文感知的手术模拟和训练场景具有重要意义。"}}
{"id": "2506.21502", "title": "Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems", "authors": ["Francesco Vitale", "Nicola Dall'Ora", "Sebastiano Gaiardelli", "Enrico Fraccaroli", "Nicola Mazzocca", "Franco Fummi"], "summary": "Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring\nsystem dependability and operational efficiency by accurately detecting\nanomalies and identifying their root causes. However, the manual modeling of\nfaulty behaviors often demands extensive domain expertise and produces models\nthat are complex, error-prone, and difficult to interpret. To address this\nchallenge, we present a novel unsupervised fault diagnosis methodology that\nintegrates collective anomaly detection in multivariate time series, process\nmining, and stochastic simulation. Initially, collective anomalies are detected\nfrom low-level sensor data using multivariate time-series analysis. These\nanomalies are then transformed into structured event logs, enabling the\ndiscovery of interpretable process models through process mining. By\nincorporating timing distributions into the extracted Petri nets, the approach\nsupports stochastic simulation of faulty behaviors, thereby enhancing root\ncause analysis and behavioral understanding. The methodology is validated using\nthe Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart\nmanufacturing. Experimental results demonstrate its effectiveness in modeling,\nsimulating, and classifying faulty behaviors in CPSs. This enables the creation\nof comprehensive fault dictionaries that support predictive maintenance and the\ndevelopment of digital twins for industrial environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21502v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21502v1", "AI": {"title_translation": "流程挖掘驱动的建模与仿真以增强信息物理系统中的故障诊断", "tldr": "本文提出了一种结合多变量时间序列异常检测、流程挖掘和随机仿真的无监督故障诊断方法，旨在解决信息物理系统（CPS）中手动建模故障行为的复杂性和错误问题，并在机械臂数据集上验证了其有效性。", "motivation": "信息物理系统（CPS）中的故障诊断对于确保系统可靠性和运行效率至关重要。然而，手动建模故障行为通常需要广泛的领域专业知识，并且生成的模型复杂、易错且难以解释。", "method": "本文提出了一种新颖的无监督故障诊断方法，该方法集成了多变量时间序列中的集体异常检测、流程挖掘和随机仿真。首先，利用多变量时间序列分析从低级传感器数据中检测集体异常。然后，将这些异常转换为结构化事件日志，通过流程挖掘发现可解释的流程模型。通过将时间分布纳入提取的Petri网中，该方法支持故障行为的随机仿真，从而增强了根本原因分析和行为理解。", "result": "该方法在智能制造领域广泛认可的机械臂数据集（RoAD）上进行了验证。实验结果表明，该方法在信息物理系统（CPS）中建模、仿真和分类故障行为方面是有效的。", "conclusion": "这项研究能够创建全面的故障字典，支持预测性维护和工业环境中数字孪生的开发。", "translation": "信息物理系统（CPS）中的故障诊断对于通过准确检测异常并识别其根本原因来确保系统可靠性和运行效率至关重要。然而，手动建模故障行为通常需要广泛的领域专业知识，并且生成的模型复杂、易错且难以解释。为了解决这一挑战，我们提出了一种新颖的无监督故障诊断方法，该方法集成了多变量时间序列中的集体异常检测、流程挖掘和随机仿真。最初，利用多变量时间序列分析从低级传感器数据中检测集体异常。然后，将这些异常转换为结构化事件日志，通过流程挖掘发现可解释的流程模型。通过将时间分布纳入提取的Petri网中，该方法支持故障行为的随机仿真，从而增强了根本原因分析和行为理解。该方法在智能制造领域广泛认可的机械臂数据集（RoAD）上进行了验证。实验结果表明，该方法在信息物理系统（CPS）中建模、仿真和分类故障行为方面是有效的。这使得能够创建全面的故障字典，支持预测性维护和工业环境中数字孪生的开发。", "summary": "本文提出了一种创新的无监督故障诊断方法，旨在克服信息物理系统（CPS）中手动建模故障行为的挑战。该方法通过整合多变量时间序列异常检测、流程挖掘和随机仿真，从低级传感器数据中识别并分析故障。首先检测集体异常并将其转换为事件日志，然后利用流程挖掘构建可解释的Petri网模型，并通过随机仿真增强根本原因分析。在机械臂数据集上的验证表明，该方法能有效地建模、仿真和分类CPS中的故障行为，从而支持预测性维护和数字孪生。", "keywords": "故障诊断, 信息物理系统, 流程挖掘, 随机仿真, 异常检测", "comments": "该论文提出了一种结合多源技术的创新故障诊断方法，特别是在将流程挖掘应用于异常事件日志以构建可解释模型方面具有新颖性。它有效地解决了手动故障建模的复杂性和错误问题，并通过随机仿真增强了诊断能力。该方法对于提升信息物理系统的可靠性，特别是支持预测性维护和数字孪生技术的发展，具有重要的实践意义。"}}
{"id": "2506.21312", "title": "Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing", "authors": ["Lars Möllenbrok", "Behnood Rasti", "Begüm Demir"], "summary": "The development of continual learning (CL) methods, which aim to learn new\ntasks in a sequential manner from the training data acquired continuously, has\ngained great attention in remote sensing (RS). The existing CL methods in RS,\nwhile learning new tasks, enhance robustness towards catastrophic forgetting.\nThis is achieved by using a large number of labeled training samples, which is\ncostly and not always feasible to gather in RS. To address this problem, we\npropose a novel continual self-supervised learning method in the context of\nmasked autoencoders (denoted as CoSMAE). The proposed CoSMAE consists of two\ncomponents: i) data mixup; and ii) model mixup knowledge distillation. Data\nmixup is associated with retaining information on previous data distributions\nby interpolating images from the current task with those from the previous\ntasks. Model mixup knowledge distillation is associated with distilling\nknowledge from past models and the current model simultaneously by\ninterpolating their model weights to form a teacher for the knowledge\ndistillation. The two components complement each other to regularize the MAE at\nthe data and model levels to facilitate better generalization across tasks and\nreduce the risk of catastrophic forgetting. Experimental results show that\nCoSMAE achieves significant improvements of up to 4.94% over state-of-the-art\nCL methods applied to MAE. Our code is publicly available at:\nhttps://git.tu-berlin.de/rsim/CoSMAE.", "comment": "Accepted to IEEE Geoscience and Remote Sensing Letters. Our code is\n  available at https://git.tu-berlin.de/rsim/CoSMAE", "pdf_url": "http://arxiv.org/pdf/2506.21312v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21312v1", "AI": {"title_translation": "遥感领域基于掩码自编码器的持续自监督学习", "tldr": "提出CoSMAE，一种持续自监督学习方法，通过数据混合和模型混合知识蒸馏解决遥感持续学习中标签数据昂贵的问题，同时减少灾难性遗忘，性能显著提升。", "motivation": "现有的遥感持续学习方法需要大量昂贵的标注训练样本来增强对灾难性遗忘的鲁棒性，这在遥感领域并不总是可行。", "method": "提出CoSMAE，一种持续自监督学习方法，包含两个组件：1) 数据混合：通过将当前任务图像与之前任务图像进行插值来保留旧数据分布信息；2) 模型混合知识蒸馏：通过插值过去模型和当前模型的权重来形成一个教师模型进行知识蒸馏。这两个组件在数据和模型层面规范化MAE，以促进更好的跨任务泛化并降低灾难性遗忘的风险。", "result": "实验结果表明，CoSMAE比应用于MAE的最先进持续学习方法实现了显著改进，性能提升高达4.94%。", "conclusion": "CoSMAE通过其独特的数据混合和模型混合知识蒸馏机制，有效解决了遥感持续学习中对大量标注数据的依赖问题，并显著提高了模型在持续学习场景下的泛化能力和对灾难性遗忘的抵抗力。", "translation": "持续学习（CL）方法旨在从连续获取的训练数据中顺序学习新任务，其发展在遥感（RS）领域受到了广泛关注。遥感领域现有的CL方法在学习新任务的同时，通过使用大量标注训练样本来增强对灾难性遗忘的鲁棒性，但这成本高昂且在遥感领域并非总是可行。为了解决这个问题，我们提出了一种在掩码自编码器（MAE）背景下的新型持续自监督学习方法（记作CoSMAE）。所提出的CoSMAE由两个组件组成：i）数据混合；ii）模型混合知识蒸馏。数据混合通过将当前任务的图像与之前任务的图像进行插值来保留先前数据分布的信息。模型混合知识蒸馏通过插值过去模型和当前模型的权重来形成一个教师模型，从而同时从它们中蒸馏知识。这两个组件相互补充，在数据和模型层面规范化MAE，以促进更好的跨任务泛化并降低灾难性遗忘的风险。实验结果表明，CoSMAE比应用于MAE的最先进CL方法实现了高达4.94%的显著改进。我们的代码已公开可用：https://git.tu-berlin.de/rsim/CoSMAE。", "summary": "本文提出CoSMAE，一种用于遥感领域的新型持续自监督学习方法，旨在解决现有持续学习方法对大量昂贵标注数据的依赖问题。CoSMAE通过数据混合和模型混合知识蒸馏两个核心组件，在数据和模型层面规范化掩码自编码器，有效保留旧知识、促进跨任务泛化并减轻灾难性遗忘。实验证明，CoSMAE在性能上显著优于现有先进方法。", "keywords": "持续学习, 自监督学习, 掩码自编码器, 遥感, 灾难性遗忘", "comments": "这项工作通过引入数据混合和模型混合知识蒸馏，巧妙地将持续学习与自监督学习结合起来，解决了遥感领域获取大量标注数据困难的核心痛点。其创新性在于在无监督或弱监督场景下提升了持续学习的效率和鲁棒性，对于资源受限的遥感应用具有重要意义。"}}
{"id": "2506.21550", "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale", "authors": ["Xiaona Zhou", "Constantin Brif", "Ismini Lourentzou"], "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21550v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21550v1", "AI": {"title_translation": "mTSBench：大规模多元时间序列异常检测与模型选择基准测试", "tldr": "mTSBench是一个大型基准平台，用于评估多元时间序列异常检测和无监督模型选择方法，发现没有单一最佳检测器，且现有模型选择方法仍有很大提升空间。", "motivation": "多元时间序列异常检测(MTS-AD)在医疗、网络安全和工业监控等领域至关重要，但由于复杂的变量间依赖、时间动态性和稀疏的异常标签，仍然具有挑战性。此外，缺乏一个大规模、统一的基准来评估和比较MTS-AD方法和无监督模型选择技术。", "method": "引入了mTSBench，这是迄今为止最大的MTS-AD和无监督模型选择基准平台。它涵盖了19个数据集、12个不同应用领域的344个带标签时间序列，并评估了24种异常检测方法（包括基于大型语言模型的方法）。mTSBench还在标准化条件下系统地测试了无监督模型选择技术。", "result": "研究结果证实，没有单一的检测器能在所有数据集上表现出色，这强调了模型选择的重要性。然而，即使是最先进的模型选择方法也远非最优，这揭示了关键的差距。", "conclusion": "mTSBench提供了一个统一的评估套件，以实现严格、可复现的比较，并促进自适应异常检测和鲁棒模型选择领域的未来进展。", "translation": "多元时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于复杂的变量间依赖、时间动态性和稀疏的异常标签，仍然具有挑战性。我们引入了mTSBench，这是迄今为止最大的MTS-AD和无监督模型选择基准平台，涵盖了19个数据集和12个不同应用领域的344个带标签时间序列。mTSBench评估了24种异常检测方法，包括用于多元时间序列的基于大型语言模型（LLM）的检测器，并在标准化条件下系统地测试了无监督模型选择技术。与先前的研究结果一致，我们的结果证实没有单一的检测器能在所有数据集上表现出色，这强调了模型选择的重要性。然而，即使是最先进的选择方法也远非最优，这揭示了关键的差距。mTSBench提供了一个统一的评估套件，以实现严格、可复现的比较，并促进自适应异常检测和鲁棒模型选择领域的未来进展。", "summary": "mTSBench是一个全新的大规模基准平台，专为多元时间序列异常检测（MTS-AD）及无监督模型选择而设计。该平台整合了19个数据集、344个时间序列，并评估了24种不同的异常检测方法（包括LLM-based）。研究发现，没有单一的异常检测器能普适最优，且当前的模型选择技术仍有显著提升空间。mTSBench旨在提供一个标准化的评估环境，以推动MTS-AD领域的未来研究。", "keywords": "多元时间序列异常检测, 模型选择, 基准测试, 无监督学习, 大型语言模型", "comments": "这篇论文通过引入mTSBench，解决了多元时间序列异常检测领域缺乏大规模、标准化基准测试平台的问题。其创新点在于集合了大量数据集和检测方法，并首次将LLM-based检测器纳入考量。论文强调了模型选择的重要性，并指出现有选择方法的不足，为未来的研究指明了方向。mTSBench的建立对于促进MTS-AD领域的严格比较和可复现性具有重要意义。"}}
{"id": "2506.21154", "title": "Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation", "authors": ["He Li", "Haoang Chi", "Mingyu Liu", "Wanrong Huang", "Liyang Xu", "Wenjing Yang"], "summary": "The real world naturally has dimensions of time and space. Therefore,\nestimating the counterfactual outcomes with spatial-temporal attributes is a\ncrucial problem. However, previous methods are based on classical statistical\nmodels, which still have limitations in performance and generalization. This\npaper proposes a novel framework for estimating counterfactual outcomes with\nspatial-temporal attributes using the Transformer, exhibiting stronger\nestimation ability. Under mild assumptions, the proposed estimator within this\nframework is consistent and asymptotically normal. To validate the\neffectiveness of our approach, we conduct simulation experiments and real data\nexperiments. Simulation experiments show that our estimator has a stronger\nestimation capability than baseline methods. Real data experiments provide a\nvaluable conclusion to the causal effect of conflicts on forest loss in\nColombia. The source code is available at\nhttps://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.", "comment": "24 pages, accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21154v1", "categories": ["stat.ME", "cs.AI", "cs.LG"], "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.21154v1", "AI": {"title_translation": "基于Transformer的时空反事实结果估计", "tldr": "本文提出了一种基于Transformer的新颖框架，用于估计具有时空属性的反事实结果，克服了传统方法的局限性，并在模拟和真实数据实验中展示了更强的估计能力，应用于分析哥伦比亚冲突对森林损失的因果效应。", "motivation": "现实世界具有时空维度，估计具有时空属性的反事实结果是一个关键问题。然而，现有基于经典统计模型的方法在性能和泛化能力上存在局限性。", "method": "本文提出了一种利用Transformer的新颖框架来估计具有时空属性的反事实结果。在该框架下，所提出的估计器在温和假设下具有一致性和渐近正态性。通过模拟实验和真实数据实验验证了其有效性。", "result": "模拟实验表明，所提出的估计器比基线方法具有更强的估计能力。真实数据实验为哥伦比亚冲突对森林损失的因果效应提供了有价值的结论。", "conclusion": "基于Transformer的方法能够有效且更准确地估计时空反事实结果，并在复杂现实世界问题中展现出强大的因果效应分析能力。", "translation": "现实世界自然具有时间和空间的维度。因此，估计具有时空属性的反事实结果是一个关键问题。然而，以前的方法基于经典的统计模型，在性能和泛化方面仍然存在局限性。本文提出了一种利用Transformer估计具有时空属性的反事实结果的新颖框架，该框架展现出更强的估计能力。在温和的假设下，该框架内提出的估计器是一致且渐近正态的。为了验证我们方法的有效性，我们进行了模拟实验和真实数据实验。模拟实验表明我们的估计器比基线方法具有更强的估计能力。真实数据实验为哥伦比亚冲突对森林损失的因果效应提供了有价值的结论。源代码可在 https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master 获取。", "summary": "本文提出了一种新颖的基于Transformer的时空反事实结果估计框架，旨在克服传统统计模型在性能和泛化能力上的局限性。该框架下的估计器具有理论上的良好性质（一致性和渐近正态性），并通过模拟实验证明其优于现有基线方法。此外，真实世界数据实验成功应用于分析哥伦比亚冲突对森林损失的因果效应，验证了其在实际问题中的有效性。", "keywords": "时空反事实估计, Transformer, 因果推断, 估计器, 森林损失", "comments": "该论文创新性地将Transformer模型引入时空反事实结果估计领域，解决了传统统计模型在性能和泛化方面的不足。其理论保证和在模拟及真实数据集上的优异表现，特别是应用于实际社会经济问题（如冲突对森林损失的影响），显示了其重要的应用价值和广阔前景。"}}
{"id": "2506.21316", "title": "DrishtiKon: Multi-Granular Visual Grounding for Text-Rich Document Images", "authors": ["Badri Vishal Kasuba", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "summary": "Visual grounding in text-rich document images is a critical yet underexplored\nchallenge for document intelligence and visual question answering (VQA)\nsystems. We present \\drishtikon, a multi-granular visual grounding framework\ndesigned to enhance interpretability and trust in VQA for complex, multilingual\ndocuments. Our approach integrates robust multi-lingual OCR, large language\nmodels, and a novel region matching algorithm to accurately localize answer\nspans at block, line, word, and point levels. We curate a new benchmark from\nthe CircularsVQA test set, providing fine-grained, human-verified annotations\nacross multiple granularities. Extensive experiments demonstrate that our\nmethod achieves state-of-the-art grounding accuracy, with line-level\ngranularity offering the best trade-off between precision and recall. Ablation\nstudies further highlight the benefits of multi-block and multi-line reasoning.\nComparative evaluations with leading vision-language models reveal the\nlimitations of current VLMs in precise localization, underscoring the\neffectiveness of our structured, alignment-based approach. Our findings pave\nthe way for more robust and interpretable document understanding systems in\nreal-world, text-centric scenarios. Code and dataset has been made available at\nhttps://github.com/kasuba-badri-vishal/DhrishtiKon.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.21316v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21316v1", "AI": {"title_translation": "DrishtiKon: 多粒度视觉定位用于文本密集型文档图像", "tldr": "DrishtiKon是一个多粒度视觉定位框架，用于文本密集型文档图像，通过集成OCR、LLM和区域匹配算法，实现了SOTA的定位精度，提高了VQA系统的可解释性。", "motivation": "文本密集型文档图像中的视觉定位对于文档智能和视觉问答（VQA）系统来说是一个关键但尚未充分探索的挑战。现有视觉-语言模型（VLM）在精确局部化方面存在局限性，需要提高VQA系统的可解释性和信任度。", "method": "提出了DrishtiKon框架，该框架集成了鲁棒的多语言OCR、大型语言模型（LLM）和一种新颖的区域匹配算法，以在块、行、单词和点级别准确地定位答案跨度。同时，从CircularsVQA测试集策划了一个新的基准，提供了多粒度的人工验证注释。", "result": "该方法在视觉定位精度上达到了最先进的水平，其中行级粒度在精度和召回率之间提供了最佳权衡。消融研究突出了多块和多行推理的优势。与领先的视觉-语言模型（VLM）的比较评估揭示了当前VLM在精确局部化方面的局限性，并强调了该结构化、基于对齐的方法的有效性。", "conclusion": "本研究的发现为在真实世界、以文本为中心的场景中构建更鲁棒和可解释的文档理解系统铺平了道路。", "translation": "文本密集型文档图像中的视觉定位是文档智能和视觉问答（VQA）系统的一个关键但尚未充分探索的挑战。我们提出了\\drishtikon，一个多粒度视觉定位框架，旨在增强复杂多语言文档中VQA的可解释性和信任度。我们的方法集成了鲁棒的多语言OCR、大型语言模型和一种新颖的区域匹配算法，以在块、行、单词和点级别准确地定位答案跨度。我们从CircularsVQA测试集策划了一个新的基准，提供了跨多个粒度的细粒度、人工验证的注释。广泛的实验表明，我们的方法达到了最先进的定位精度，其中行级粒度在精度和召回率之间提供了最佳权衡。消融研究进一步突出了多块和多行推理的优势。与领先的视觉-语言模型（VLM）的比较评估揭示了当前VLM在精确局部化方面的局限性，强调了我们结构化、基于对齐的方法的有效性。我们的发现为在真实世界、以文本为中心的场景中构建更鲁棒和可解释的文档理解系统铺平了道路。代码和数据集已在https://github.com/kasuba-badri-vishal/DhrishtiKon 提供。", "summary": "本文提出了DrishtiKon，一个用于文本密集型文档图像的多粒度视觉定位框架，旨在提高视觉问答（VQA）系统的可解释性和信任度。该框架结合了多语言OCR、大型语言模型和新颖的区域匹配算法，能够在块、行、单词和点级别精确定位答案跨度。通过在CircularsVQA数据集上构建的新基准进行实验，DrishtiKon在定位精度上达到了最先进的水平，尤其是在行级粒度上表现出最佳的精度-召回率权衡。研究还揭示了当前视觉-语言模型在精确局部化方面的不足，并证明了该结构化、基于对齐方法的优越性，为构建更鲁棒、可解释的文档理解系统奠定了基础。", "keywords": "视觉定位, 文档图像, 视觉问答, 多粒度, 文本密集", "comments": "这项工作通过引入多粒度视觉定位和结合OCR、LLM及新颖区域匹配算法，有效解决了文本密集型文档图像中视觉问答的挑战，显著提升了定位精度和系统可解释性。其创新点在于多粒度定位和对现有VLM局限性的揭示，为未来文档理解系统提供了新的方向，具有重要的实践意义。"}}
{"id": "2506.21551", "title": "Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test", "authors": ["Ziyue Li", "Chenrui Fan", "Tianyi Zhou"], "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21551v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21551v1", "AI": {"title_translation": "在LLM预训练中何处寻找Grokking？无需测试即可监控记忆到泛化", "tldr": "本研究首次在大型语言模型预训练中发现了“Grokking”现象，并通过分析模型内部动态（即训练样本路径的演变）揭示了其从记忆到泛化的机制。研究还提出了两种新的度量标准，可以在不进行微调和测试的情况下预测泛化性能。", "motivation": "Grokking现象（即训练损失收敛后测试性能仍持续提升）的发现使得泛化机制和推理等新兴能力变得神秘。先前的研究大多在小型模型和特定任务上进行，本研究旨在首次在大型语言模型（LLM）的预训练过程中探索Grokking现象。", "method": "研究在一个7B大型语言模型（OLMoE）的一次性预训练过程中，对检查点进行Grokking研究。计算训练损失并在包括数学推理、代码生成、常识/领域特定知识检索等多样化基准任务上评估泛化能力。通过调查LLM内部动态，特别是训练样本的路径（即跨层的专家选择）来揭示Grokking的“泛化涌现”。开发了两种新颖的度量标准来量化路径距离和单个路径的复杂性。", "result": "研究首次证实Grokking现象在大规模基础模型预训练中仍然存在，尽管不同数据可能异步进入Grokking阶段。发现训练样本的路径从随机、实例特定演变为更结构化且样本间可共享，并且样本路径的复杂性在损失收敛后有所降低。这表明发生了从记忆到泛化的转换，为延迟泛化提供了机制解释。开发的两种新度量标准能够预测各种下游任务上的泛化改进。", "conclusion": "Grokking现象确实发生在大规模LLM预训练中，其背后机制是训练样本路径从随机到结构化、可共享的演变，体现了从记忆到泛化的转变。新提出的路径度量标准能有效且高效地在不进行微调和测试的情况下监控泛化性能。理论上，更结构化的路径可以降低模型复杂性并提高泛化界限。", "translation": "Grokking，即训练损失收敛后测试性能仍持续提升的现象，最近在神经网络训练中被观察到，这使得泛化机制以及推理等新兴能力变得神秘。虽然之前的研究通常在少量玩具或高度特定的任务上训练小型模型数千个epoch，但我们首次对7B大型语言模型（LLM），即OLMoE的一次性预训练过程中的检查点进行Grokking研究。我们计算了训练损失，并在包括数学推理、代码生成以及常识/领域特定知识检索任务在内的多样化基准任务上评估了泛化能力。\n我们的研究首次验证了Grokking现象在大规模基础模型预训练中仍然发生，尽管不同数据可能异步进入Grokking阶段。我们通过调查LLM的内部动态，进一步揭示了Grokking的“泛化涌现”机制。具体来说，我们发现训练样本的路径（即跨层的专家选择）在Grokking过程中从随机、实例特定演变为更结构化且样本间可共享。此外，尽管损失已收敛，但样本路径的复杂性却降低了。这些表明发生了从记忆到泛化的转换，为延迟泛化提供了机制解释。在本研究中，我们开发了两种新颖的度量标准来量化路径距离和单个路径的复杂性。我们展示了它们预测各种下游任务泛化改进的能力。它们高效、计算简单，并且仅依赖于训练数据。因此，它们对预训练具有实用价值，使我们能够在不进行微调和测试的情况下监控泛化性能。理论上，我们表明更结构化的路径降低了模型复杂性并提高了泛化界限。", "summary": "本研究首次在7B大型语言模型（OLMoE）的预训练过程中，对“Grokking”现象进行了深入探究。研究发现，Grokking现象在大规模基础模型预训练中确实存在，并通过分析训练样本在模型内部的路径演变，揭示了其从记忆到泛化的机制。具体而言，样本路径从随机变得更结构化和可共享，复杂性降低。此外，研究提出了两种新颖的度量标准，能够高效且准确地在不进行微调和测试的情况下预测模型的泛化性能，为LLM的预训练监控提供了实用工具。", "keywords": "Grokking, LLM预训练, 泛化, 记忆, 路径分析", "comments": "这项研究的创新之处在于它是首次在大规模LLM预训练中探索Grokking现象，并提供了其从记忆到泛化的内部机制解释，即通过训练样本路径的演变。其提出的两种新度量标准具有重要的实践价值，可以在不依赖下游任务测试的情况下，高效地监控模型泛化能力，这对于LLM的预训练优化具有显著意义。"}}
{"id": "2506.21317", "title": "LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "summary": "Current vision-language models (VLMs) are well-adapted for general visual\nunderstanding tasks. However, they perform inadequately when handling complex\nvisual tasks related to human poses and actions due to the lack of specialized\nvision-language instruction-following data. We introduce a method for\ngenerating such data by integrating human keypoints with traditional visual\nfeatures such as captions and bounding boxes, enabling more precise\nunderstanding of human-centric scenes. Our approach constructs a dataset\ncomprising 200,328 samples tailored to fine-tune models for human-centric\ntasks, focusing on three areas: conversation, detailed description, and complex\nreasoning. We establish an Extended Human Pose and Action Understanding\nBenchmark (E-HPAUB) to assess model performance on human pose and action\nunderstanding. We fine-tune the LLaVA-1.5-7B model using this dataset and\nevaluate our resulting LLaVA-Pose model on the benchmark, achieving significant\nimprovements. Experimental results show an overall improvement of 33.2%\ncompared to the original LLaVA-1.5-7B model. These findings highlight the\neffectiveness of keypoint-integrated data in enhancing multimodal models for\nhuman-centric visual understanding. Code is available at\nhttps://github.com/Ody-trek/LLaVA-Pose.", "comment": "arXiv admin note: substantial text overlap with arXiv:2409.09306", "pdf_url": "http://arxiv.org/pdf/2506.21317v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21317v1", "AI": {"title_translation": "LLaVA-Pose：通过关键点集成指令微调增强人体姿态和动作理解", "tldr": "LLaVA-Pose通过整合人体关键点数据对现有视觉-语言模型进行指令微调，显著提升了模型在人体姿态和动作理解方面的性能，相较于原始LLaVA-1.5-7B模型提升了33.2%。", "motivation": "当前视觉-语言模型（VLMs）在处理与人体姿态和动作相关的复杂视觉任务时表现不佳，原因是缺乏专门的视觉-语言指令遵循数据。", "method": "该研究通过将人体关键点与传统视觉特征（如图像标题和边界框）相结合，生成了专门的视觉-语言指令遵循数据。基于此方法构建了一个包含200,328个样本的数据集，用于针对以人为中心的任务（对话、详细描述、复杂推理）进行模型微调。同时，建立了一个扩展人体姿态和动作理解基准（E-HPAUB）来评估模型性能。使用此数据集对LLaVA-1.5-7B模型进行微调，得到了LLaVA-Pose模型。", "result": "实验结果显示，LLaVA-Pose模型在E-HPAUB基准测试中取得了显著改进，相较于原始LLaVA-1.5-7B模型，整体性能提升了33.2%。", "conclusion": "关键点集成数据能够有效增强多模态模型在以人为中心的视觉理解任务中的性能。", "translation": "当前的视觉-语言模型（VLMs）已很好地适应了通用视觉理解任务。然而，由于缺乏专门的视觉-语言指令遵循数据，它们在处理与人体姿态和动作相关的复杂视觉任务时表现不足。我们引入了一种生成此类数据的方法，通过将人体关键点与传统视觉特征（如图像标题和边界框）相结合，从而实现对以人为中心场景更精确的理解。我们的方法构建了一个包含200,328个样本的数据集，专门用于微调以人为中心的任务模型，重点关注三个领域：对话、详细描述和复杂推理。我们建立了一个扩展人体姿态和动作理解基准（E-HPAUB）来评估模型在人体姿态和动作理解方面的性能。我们使用此数据集对LLaVA-1.5-7B模型进行微调，并在基准测试中评估了我们得到的LLaVA-Pose模型，取得了显著改进。实验结果显示，与原始LLaVA-1.5-7B模型相比，整体性能提升了33.2%。这些发现突出表明，关键点集成数据在增强多模态模型以实现以人为中心的视觉理解方面是有效的。代码可在https://github.com/Ody-trek/LLaVA-Pose获取。", "summary": "该研究提出LLaVA-Pose模型，旨在解决现有视觉-语言模型在人体姿态和动作理解方面表现不足的问题。通过整合人体关键点与传统视觉特征，研究团队构建了一个包含20万样本的专用数据集，并基于此数据集对LLaVA-1.5-7B模型进行微调。为评估模型性能，还建立了一个扩展人体姿态和动作理解基准（E-HPAUB）。实验结果表明，LLaVA-Pose在人体姿态和动作理解方面取得了显著提升，相比原始模型整体性能提高了33.2%，验证了关键点集成数据在增强以人为中心的视觉理解方面的有效性。", "keywords": "人体姿态理解, 动作理解, 视觉-语言模型, 关键点集成, 指令微调", "comments": "该论文的创新点在于提出了将人体关键点集成到视觉-语言指令微调数据中的方法，有效弥补了现有VLM在人体姿态和动作理解方面的不足。通过构建大规模的专用数据集和评估基准，为该领域的研究提供了宝贵的资源和明确的评估标准。33.2%的显著性能提升凸显了其方法的有效性和重要性。"}}
{"id": "2409.18017", "title": "Transferring disentangled representations: bridging the gap between synthetic and real images", "authors": ["Jacopo Dapueto", "Nicoletta Noceti", "Francesca Odone"], "summary": "Developing meaningful and efficient representations that separate the\nfundamental structure of the data generation mechanism is crucial in\nrepresentation learning. However, Disentangled Representation Learning has not\nfully shown its potential on real images, because of correlated generative\nfactors, their resolution and limited access to ground truth labels.\nSpecifically on the latter, we investigate the possibility of leveraging\nsynthetic data to learn general-purpose disentangled representations applicable\nto real data, discussing the effect of fine-tuning and what properties of\ndisentanglement are preserved after the transfer. We provide an extensive\nempirical study to address these issues. In addition, we propose a new\ninterpretable intervention-based metric, to measure the quality of factors\nencoding in the representation. Our results indicate that some level of\ndisentanglement, transferring a representation from synthetic to real data, is\npossible and effective.", "comment": "Accepted to NeurIPS, 2024", "pdf_url": "http://arxiv.org/pdf/2409.18017v3", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2409.18017v3", "AI": {"title_translation": "解耦表示迁移：弥合合成图像与真实图像之间的鸿沟", "tldr": "本文研究了将解耦表示从合成数据迁移到真实数据的可能性和有效性，并提出了一种新的可解释的度量方法。", "motivation": "解耦表示学习在真实图像上尚未充分发挥其潜力，原因在于生成因素的相关性、分辨率以及对真实标签的有限访问。本文旨在解决这一问题，探索利用合成数据学习适用于真实数据的通用解耦表示。", "method": "本文研究了利用合成数据学习通用解耦表示，并探讨了微调的效果以及迁移后解耦属性的保留情况。此外，还提出了一种新的基于干预的可解释度量标准，用于衡量表示中编码因素的质量。", "result": "研究结果表明，将解耦表示从合成数据迁移到真实数据是可能且有效的，能在一定程度上实现解耦。", "conclusion": "通过将解耦表示从合成数据迁移到真实数据，可以有效弥合合成图像与真实图像之间的差距，实现一定程度的解耦表示学习。", "translation": "开发有意义且高效的表示，以分离数据生成机制的基本结构，在表示学习中至关重要。然而，解耦表示学习尚未在真实图像上充分发挥其潜力，原因在于生成因素的相关性、其分辨率以及对真实标签的有限访问。特别是在后者方面，我们研究了利用合成数据学习适用于真实数据的通用解耦表示的可能性，讨论了微调的效果以及迁移后解耦的哪些属性得以保留。我们提供了广泛的实证研究来解决这些问题。此外，我们提出了一种新的可解释的基于干预的度量标准，用于衡量表示中因子编码的质量。我们的结果表明，将表示从合成数据迁移到真实数据，一定程度的解耦是可能且有效的。", "summary": "本文探讨了将解耦表示从合成数据迁移到真实数据的可行性与有效性，旨在克服解耦表示学习在真实图像上遇到的挑战，如相关生成因素和真实标签的获取限制。研究通过实证分析了微调的影响及解耦属性的保留情况，并引入了一种新的基于干预的可解释度量标准来评估表示质量。结果表明，这种迁移方法能够实现一定程度的解耦，并取得了良好的效果。", "keywords": "解耦表示, 合成数据, 真实图像, 迁移学习, 度量标准", "comments": "本文的创新点在于探索了将解耦表示从合成数据迁移到真实数据，这为解决真实世界数据中解耦表示学习的挑战提供了一个有前景的方向。提出的新度量标准也增加了研究的严谨性和可解释性。这项工作对于推动解耦表示学习在实际应用中的发展具有重要意义。"}}
{"id": "2506.21330", "title": "Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models", "authors": ["Haoyang Wu", "Tsun-Hsuan Wang", "Mathias Lechner", "Ramin Hasani", "Jennifer A. Eckhoff", "Paul Pak", "Ozanan R. Meireles", "Guy Rosman", "Yutong Ban", "Daniela Rus"], "summary": "Surgical workflow analysis is essential in robot-assisted surgeries, yet the\nlong duration of such procedures poses significant challenges for comprehensive\nvideo analysis. Recent approaches have predominantly relied on transformer\nmodels; however, their quadratic attention mechanism restricts efficient\nprocessing of lengthy surgical videos. In this paper, we propose a novel\nhierarchical input-dependent state space model that leverages the linear\nscaling property of state space models to enable decision making on full-length\nvideos while capturing both local and global dynamics. Our framework\nincorporates a temporally consistent visual feature extractor, which appends a\nstate space model head to a visual feature extractor to propagate temporal\ninformation. The proposed model consists of two key modules: a\nlocal-aggregation state space model block that effectively captures intricate\nlocal dynamics, and a global-relation state space model block that models\ntemporal dependencies across the entire video. The model is trained using a\nhybrid discrete-continuous supervision strategy, where both signals of discrete\nphase labels and continuous phase progresses are propagated through the\nnetwork. Experiments have shown that our method outperforms the current\nstate-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on\nMICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available\nafter paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21330v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21330v1", "AI": {"title_translation": "全局手术阶段识别与分层输入依赖状态空间模型", "tldr": "提出一种分层输入依赖状态空间模型，用于全局手术阶段识别，解决了长视频处理中Transformer模型的效率问题，并显著优于现有SOTA方法。", "motivation": "机器人辅助手术中的手术流程分析至关重要，但长时间的手术视频给全面视频分析带来挑战。现有方法（如Transformer模型）的二次注意力机制限制了对长手术视频的有效处理。", "method": "提出一种新颖的分层输入依赖状态空间模型，利用状态空间模型的线性扩展特性，实现全长视频的决策，并捕捉局部和全局动态。该框架包含一个时间一致的视觉特征提取器，将状态空间模型头部附加到视觉特征提取器上以传播时间信息。模型由两个关键模块组成：一个局部聚合状态空间模型块（捕捉局部动态）和一个全局关系状态空间模型块（建模整个视频的时间依赖性）。模型采用混合离散-连续监督策略进行训练，其中离散阶段标签和连续阶段进度信号都通过网络传播。", "result": "该方法在Cholec80数据集上优于现有SOTA方法2.8%，在MICCAI2016数据集上优于4.3%，在Heichole数据集上优于12.9%。", "conclusion": "所提出的分层输入依赖状态空间模型有效解决了长手术视频分析的挑战，并在多个数据集上取得了显著优于现有最先进方法的性能，证明了其在全局手术阶段识别中的有效性。", "translation": "手术流程分析在机器人辅助手术中至关重要，然而此类手术的长时间特性给全面的视频分析带来了巨大挑战。最近的方法主要依赖于Transformer模型；然而，它们的二次注意力机制限制了对冗长手术视频的有效处理。在本文中，我们提出了一种新颖的分层输入依赖状态空间模型，该模型利用状态空间模型的线性缩放特性，能够在全长视频上进行决策，同时捕捉局部和全局动态。我们的框架包含一个时间一致的视觉特征提取器，它将一个状态空间模型头部附加到视觉特征提取器上以传播时间信息。所提出的模型由两个关键模块组成：一个有效捕捉复杂局部动态的局部聚合状态空间模型块，以及一个建模整个视频时间依赖性的全局关系状态空间模型块。该模型采用混合离散-连续监督策略进行训练，其中离散阶段标签和连续阶段进度的两种信号都通过网络传播。实验表明，我们的方法在Cholec80数据集上以2.8%的显著优势、在MICCAI2016数据集上以4.3%的优势、在Heichole数据集上以12.9%的优势超越了目前的最新技术。代码将在论文接收后公开发布。", "summary": "本文提出一种新颖的分层输入依赖状态空间模型（HIDS），旨在解决机器人辅助手术中长视频分析的挑战。该模型利用状态空间模型的线性扩展特性，能够处理全长手术视频并捕捉局部与全局时间动态。HIDS包含一个时间一致的视觉特征提取器，并由局部聚合和全局关系两个状态空间模型块构成。通过混合离散-连续监督策略训练，实验结果表明HIDS在Cholec80、MICCAI2016和Heichole等数据集上显著超越了现有最先进方法，证明了其在全局手术阶段识别中的优越性。", "keywords": "手术阶段识别, 状态空间模型, 机器人辅助手术, 视频分析, 分层模型", "comments": "该论文的创新点在于引入了分层输入依赖状态空间模型来解决长手术视频分析中Transformer模型效率低下的问题。通过结合局部和全局动态捕捉能力，并利用状态空间模型的线性扩展特性，该方法有效地提高了手术阶段识别的准确性，并在多个基准数据集上取得了显著的性能提升，对机器人辅助手术的自动化分析具有重要意义。"}}
{"id": "2506.20672", "title": "The final solution of the Hitchhiker's problem #5", "authors": ["Matjaž Omladič", "Martin Vuk", "Aljaž Zalar"], "summary": "A recent survey, nicknamed \"Hitchhiker's Guide\", J.J. Arias-Garc{\\i}a, R.\nMesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and\nSystems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the\ndependence modeling community in spite of the lack of statistical\ninterpretation of quasi-copulas. In our previous work (arXiv:2410.19339,\naccepted in Fuzzy Sets and Systems), we addressed the question of extreme\nvalues of the mass distribution associated with multivariate quasi-copulas.\nUsing a linear programming approach, we were able to solve Open Problem 5 of\nthe \"Guide\" up to dimension d = 17 and disprove a recent conjecture on the\nsolution to that problem. In this paper, we use an analytical approach to\nprovide a complete answer to the original question.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2506.20672v1", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20672v1", "AI": {"title_translation": "《搭便车问题 #5》的最终解决方案", "tldr": "本文使用解析方法彻底解决了准联结函数中“搭便车指南”的开放问题5。", "motivation": "尽管准联结函数缺乏统计解释，但近期一项名为“搭便车指南”的调查提高了准联结函数问题在依赖建模领域的评价。作者之前的研究解决了多元准联结函数相关质量分布的极值问题，并解决了“指南”中的开放问题5（维度d=17），但仍需一个完整的答案。", "method": "本文采用解析方法来提供对原始问题的完整答案。先前的研究（作者的旧作）曾使用线性规划方法。", "result": "本文使用解析方法，对“搭便车指南”中的开放问题5提供了一个完整的答案。先前的研究（作者的旧作）曾通过线性规划方法解决该问题至维度d=17，并驳斥了一个关于该问题解决方案的最新猜想。", "conclusion": "本文通过解析方法，最终提供了“搭便车指南”中开放问题5的完整解决方案。", "translation": "近期一项名为“搭便车指南”的调查（J.J. Arias-García, R. Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and Systems 393 (2020) 1-28）尽管准联结函数缺乏统计解释，但仍在依赖建模社区中提高了准联结函数问题的评价。在我们之前的工作中（arXiv:2410.19339，已在Fuzzy Sets and Systems接受），我们解决了与多元准联结函数相关的质量分布极值问题。通过线性规划方法，我们能够将“指南”中的开放问题5解决到维度d = 17，并驳斥了关于该问题解决方案的一个最新猜想。在本文中，我们使用解析方法来为原始问题提供一个完整的答案。", "summary": "本文针对准联结函数领域中“搭便车指南”提出的开放问题5，采用解析方法提供了一个完整的解决方案。该问题涉及多元准联结函数相关质量分布的极值。作者在先前的工作中曾使用线性规划方法解决了该问题至维度d=17，并驳斥了相关猜想，而本文则通过新的分析方法给出了最终且全面的答案。", "keywords": "准联结函数, 依赖建模, 开放问题5, 解析方法, 极值", "comments": "本文的创新之处在于，它通过纯粹的解析方法，为之前只能通过计算（如线性规划）或有限维度解决的复杂问题（开放问题5）提供了完整的、普遍适用的答案。这不仅解决了特定问题，也可能为准联结函数理论提供更深层次的理解和新的分析工具。"}}
{"id": "2506.21348", "title": "PanSt3R: Multi-view Consistent Panoptic Segmentation", "authors": ["Lojze Zust", "Yohann Cabon", "Juliette Marrie", "Leonid Antsfeld", "Boris Chidlovskii", "Jerome Revaud", "Gabriela Csurka"], "summary": "Panoptic segmentation of 3D scenes, involving the segmentation and\nclassification of object instances in a dense 3D reconstruction of a scene, is\na challenging problem, especially when relying solely on unposed 2D images.\nExisting approaches typically leverage off-the-shelf models to extract\nper-frame 2D panoptic segmentations, before optimizing an implicit geometric\nrepresentation (often based on NeRF) to integrate and fuse the 2D predictions.\nWe argue that relying on 2D panoptic segmentation for a problem inherently 3D\nand multi-view is likely suboptimal as it fails to leverage the full potential\nof spatial relationships across views. In addition to requiring camera\nparameters, these approaches also necessitate computationally expensive\ntest-time optimization for each scene. Instead, in this work, we propose a\nunified and integrated approach PanSt3R, which eliminates the need for\ntest-time optimization by jointly predicting 3D geometry and multi-view\npanoptic segmentation in a single forward pass. Our approach builds upon recent\nadvances in 3D reconstruction, specifically upon MUSt3R, a scalable multi-view\nversion of DUSt3R, and enhances it with semantic awareness and multi-view\npanoptic segmentation capabilities. We additionally revisit the standard\npost-processing mask merging procedure and introduce a more principled approach\nfor multi-view segmentation. We also introduce a simple method for generating\nnovel-view predictions based on the predictions of PanSt3R and vanilla 3DGS.\nOverall, the proposed PanSt3R is conceptually simple, yet fast and scalable,\nand achieves state-of-the-art performance on several benchmarks, while being\norders of magnitude faster than existing methods.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21348v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21348v1", "AI": {"title_translation": "PanSt3R: 多视角一致的全景分割", "tldr": "PanSt3R是一种新的方法，用于从未定位的2D图像进行3D全景分割，通过一次前向传播联合预测3D几何和多视角全景分割，无需测试时优化，且比现有方法更快、更优。", "motivation": "在仅依靠未定位的2D图像进行3D场景全景分割是一个具有挑战性的问题。现有方法通常依赖于2D全景分割，然后优化隐式几何表示（如NeRF）来融合2D预测，这种方法是次优的，因为它未能充分利用跨视图的空间关系，并且需要相机参数和计算成本高昂的测试时优化。", "method": "本文提出了一种统一的集成方法PanSt3R，通过在一次前向传播中联合预测3D几何和多视角全景分割，消除了测试时优化的需要。该方法建立在3D重建的最新进展之上，特别是MUSt3R（DUSt3R的可扩展多视角版本），并增强了语义感知和多视角全景分割能力。此外，还重新审视了标准的后处理掩码合并过程，并引入了一种更原则性的多视角分割方法，以及一种基于PanSt3R和3DGS预测生成新视角预测的简单方法。", "result": "所提出的PanSt3R概念简单、快速且可扩展，在多个基准测试中实现了最先进的性能，同时比现有方法快几个数量级。", "conclusion": "PanSt3R提供了一种新颖、高效且有效的解决方案，用于从未定位的2D图像进行3D场景全景分割，显著优于现有方法，解决了测试时优化和计算效率低下的问题。", "translation": "3D场景的全景分割，涉及场景密集3D重建中对象实例的分割和分类，是一个具有挑战性的问题，尤其是在仅依靠未定位的2D图像时。现有方法通常利用现成的模型提取每帧2D全景分割，然后优化隐式几何表示（通常基于NeRF）来整合和融合2D预测。我们认为，对于一个本质上是3D和多视角的问题，依赖2D全景分割可能是次优的，因为它未能充分利用跨视图的空间关系。除了需要相机参数外，这些方法还需要对每个场景进行计算成本高昂的测试时优化。相反，在这项工作中，我们提出了一种统一的集成方法PanSt3R，通过在一次前向传播中联合预测3D几何和多视角全景分割，消除了测试时优化的需要。我们的方法建立在3D重建的最新进展之上，特别是基于DUSt3R的可扩展多视角版本MUSt3R，并增强了语义感知和多视角全景分割能力。我们还重新审视了标准的后处理掩码合并过程，并引入了一种更原则性的多视角分割方法。我们还介绍了一种基于PanSt3R和普通3DGS的预测生成新视角预测的简单方法。总的来说，所提出的PanSt3R概念简单，但快速且可扩展，在多个基准测试中实现了最先进的性能，同时比现有方法快几个数量级。", "summary": "PanSt3R提出了一种创新的方法，用于从无姿态的2D图像进行3D场景全景分割。与现有依赖2D分割和耗时测试时优化的方法不同，PanSt3R通过一次前向传播联合预测3D几何和多视角全景分割，实现了无测试时优化。该方法基于MUSt3R并增强了语义能力，同时改进了多视角分割的后处理。PanSt3R在概念上简单、快速且可扩展，在多项基准测试中达到了最先进的性能，并显著提高了速度。", "keywords": "全景分割, 3D场景, 多视角, 深度学习, 三维重建", "comments": "PanSt3R的创新之处在于其统一且集成的框架，它通过单次前向传播同时处理3D几何和多视角全景分割，从而避免了传统方法中耗时的测试时优化。这种方法不仅提高了效率，还在性能上超越了现有技术。它利用了3D重建的最新进展，并对多视角分割的后处理进行了改进，使其在实际应用中更具吸引力。"}}
{"id": "2506.21356", "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models", "authors": ["Hongbo Liu", "Jingwen He", "Yi Jin", "Dian Zheng", "Yuhao Dong", "Fan Zhang", "Ziqi Huang", "Yinan He", "Yangguang Li", "Weichao Chen", "Yu Qiao", "Wanli Ouyang", "Shengjie Zhao", "Ziwei Liu"], "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\n\\textbf{ShotBench}, a comprehensive benchmark specifically designed for\ncinematic language understanding. It features over 3.5k expert-annotated QA\npairs from images and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60\\%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\n\\textbf{ShotQA}, a large-scale multimodal dataset comprising approximately 70k\ncinematic QA pairs. Leveraging ShotQA, we develop \\textbf{ShotVL} through\nsupervised fine-tuning and Group Relative Policy Optimization. ShotVL\nsignificantly outperforms all existing open-source and proprietary models on\nShotBench, establishing new \\textbf{state-of-the-art} performance. We\nopen-source our models, data, and code to foster rapid progress in this crucial\narea of AI-driven cinematic understanding and generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21356v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21356v1", "AI": {"title_translation": "ShotBench：视觉语言模型中的专家级电影理解", "tldr": "本文介绍了ShotBench，一个用于评估视觉语言模型电影理解能力的基准测试，并发现现有模型存在显著局限性。为解决此问题，作者构建了ShotQA数据集并开发了ShotVL模型，该模型在ShotBench上取得了最先进的性能。", "motivation": "现有的视觉语言模型（VLMs）在理解电影中细致的电影语法方面能力有限，且缺乏鲁棒的评估方法。这一关键空白限制了细粒度视觉理解和AI辅助视频生成的精度。", "method": "1. 引入ShotBench，一个包含3.5k专家标注问答对的电影语言理解基准，数据来源于200多部电影，涵盖八个电影摄影维度。2. 在ShotBench上评估24个领先的VLM，揭示其局限性。3. 构建ShotQA，一个包含约70k电影问答对的大规模多模态数据集。4. 利用ShotQA，通过监督微调和群组相对策略优化开发ShotVL模型。", "result": "1. 对24个领先VLM的评估显示，即使表现最佳的模型平均准确率也低于60%，尤其在细粒度视觉线索和复杂空间推理方面表现不佳。2. ShotVL在ShotBench上显著优于所有现有开源和专有模型，建立了新的最先进性能。", "conclusion": "现有视觉语言模型在电影理解方面存在显著局限性。通过引入ShotBench基准、构建ShotQA数据集和开发ShotVL模型，本文成功提升了AI在电影理解和生成方面的能力，并为该领域的未来研究奠定了基础。", "translation": "电影摄影作为电影最基本的视觉语言，对于传达叙事、情感和美学质量至关重要。尽管最近的视觉语言模型（VLMs）展示了强大的通用视觉理解能力，但它们在理解个体镜头中嵌入的细致电影语法方面的熟练程度仍未被充分探索，并且缺乏鲁棒的评估。这一关键空白限制了细粒度视觉理解和AI辅助视频生成的精度。为了解决这个问题，我们引入了ShotBench，一个专门为电影语言理解设计的综合基准。它包含来自图像和视频片段的3.5k多个专家标注的问答对，这些问答对经过精心策划，来源于200多部著名（主要是奥斯卡提名）电影，并涵盖了八个关键的电影摄影维度。我们对24个领先的VLM在ShotBench上的评估揭示了它们存在的显著局限性：即使是表现最佳的模型，其平均准确率也低于60%，尤其在细粒度视觉线索和复杂空间推理方面表现挣扎。为了促进该领域的进步，我们构建了ShotQA，一个包含大约70k电影问答对的大规模多模态数据集。利用ShotQA，我们通过监督微调和群组相对策略优化开发了ShotVL。ShotVL在ShotBench上显著优于所有现有开源和专有模型，建立了新的最先进性能。我们开源了我们的模型、数据和代码，以促进AI驱动的电影理解和生成这一关键领域的快速发展。", "summary": "本文介绍了ShotBench，一个专门用于评估视觉语言模型（VLMs）电影理解能力的综合基准。研究发现现有VLMs在此方面存在显著不足。为弥补这一差距，作者构建了大规模电影问答数据集ShotQA，并基于此开发了ShotVL模型。ShotVL在ShotBench上取得了突破性进展，显著超越了现有模型，确立了电影理解的新技术水平，并开源了相关资源以推动领域发展。", "keywords": "电影理解, 视觉语言模型, ShotBench, ShotQA, ShotVL", "comments": "本文通过引入首个专门针对电影语言理解的基准ShotBench，填补了视觉语言模型在电影领域评估的空白。它不仅揭示了现有模型的局限性，还通过构建大规模数据集ShotQA和开发新模型ShotVL，为该领域提供了重要的资源和解决方案，展示了其在推动AI电影理解和生成方面的重要性和创新性。"}}
{"id": "2506.21357", "title": "CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations", "authors": ["Julian Lorenz", "Mrunmai Phatak", "Robin Schön", "Katja Ludwig", "Nico Hörmann", "Annemarie Friedrich", "Rainer Lienhart"], "summary": "2D scene graphs provide a structural and explainable framework for scene\nunderstanding. However, current work still struggles with the lack of accurate\nscene graph data. To overcome this data bottleneck, we present CoPa-SG, a\nsynthetic scene graph dataset with highly precise ground truth and exhaustive\nrelation annotations between all objects. Moreover, we introduce parametric and\nproto-relations, two new fundamental concepts for scene graphs. The former\nprovides a much more fine-grained representation than its traditional\ncounterpart by enriching relations with additional parameters such as angles or\ndistances. The latter encodes hypothetical relations in a scene graph and\ndescribes how relations would form if new objects are placed in the scene.\nUsing CoPa-SG, we compare the performance of various scene graph generation\nmodels. We demonstrate how our new relation types can be integrated in\ndownstream applications to enhance planning and reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21357v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21357v1", "AI": {"title_translation": "CoPa-SG：具有参数化和原型关系图的密集场景图", "tldr": "提出了CoPa-SG数据集和参数化、原型关系，以解决场景图数据不足问题并增强表示和推理能力。", "motivation": "当前二维场景图工作面临缺乏准确场景图数据的瓶颈，且现有关系表示不够细致。", "method": "提出了CoPa-SG合成场景图数据集，具有精确的地面真值和详尽的关系标注。引入了两种新的基本概念：参数化关系（用角度或距离等参数丰富关系）和原型关系（编码假设关系，描述新对象加入时关系如何形成）。", "result": "使用CoPa-SG比较了各种场景图生成模型的性能。展示了新的关系类型如何集成到下游应用中以增强规划和推理能力。", "conclusion": "CoPa-SG数据集和引入的参数化、原型关系能有效解决场景图数据不足问题，并提升场景图的表示细粒度和下游应用的规划推理能力。", "translation": "二维场景图为场景理解提供了一个结构化且可解释的框架。然而，目前的工作仍在努力解决缺乏准确场景图数据的问题。为了克服这一数据瓶颈，我们提出了CoPa-SG，一个合成场景图数据集，具有高度精确的地面真值和所有对象之间详尽的关系标注。此外，我们引入了参数化关系和原型关系，这是场景图的两个新的基本概念。前者通过用角度或距离等额外参数丰富关系，提供了比传统对应物更细粒度的表示。后者在场景图中编码假设关系，并描述了如果将新对象放置在场景中，关系将如何形成。使用CoPa-SG，我们比较了各种场景图生成模型的性能。我们展示了如何将我们的新关系类型集成到下游应用程序中，以增强规划和推理能力。", "summary": "本文针对二维场景图面临的准确数据稀缺问题，提出了CoPa-SG合成数据集，该数据集包含高精度真值和详尽的对象间关系标注。同时，文章引入了两种创新关系概念：参数化关系，通过增加角度、距离等参数实现更精细的关系描述；以及原型关系，用于编码假设性关系，预测新对象加入场景后的关系形成。研究利用CoPa-SG数据集评估了多种场景图生成模型的性能，并证明了新关系类型能有效提升下游应用的规划和推理能力。", "keywords": "场景图, CoPa-SG, 参数化关系, 原型关系, 数据集", "comments": "本文通过引入CoPa-SG数据集和参数化/原型关系，有效解决了场景图领域的数据稀缺和关系表示粒度不足的问题。特别是参数化关系和原型关系的概念，为场景图的表达能力和动态推理提供了新的视角，对于提升机器人规划和AI推理等下游应用的性能具有重要意义，展现了良好的创新性。"}}
{"id": "2506.20697", "title": "scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection", "authors": ["Zhen Yuan", "Shaoqing Jiao", "Yihang Xiao", "Jiajie Peng"], "summary": "The advent of single-cell multi-omics technologies has enabled the\nsimultaneous profiling of diverse omics layers within individual cells.\nIntegrating such multimodal data provides unprecedented insights into cellular\nidentity, regulatory processes, and disease mechanisms. However, it remains\nchallenging, as current methods often rely on selecting highly variable genes\nor peaks during preprocessing, which may inadvertently discard crucial\nbiological information. Here, we present scMamba, a foundation model designed\nto integrate single-cell multi-omics data without the need for prior feature\nselection while preserving genomic positional information. scMamba introduces a\npatch-based cell tokenization strategy that treats genomics regions as words\n(tokens) and cells as sentences. Building upon the concept of state space\nduality, scMamba distills rich biological insights from high-dimensional,\nsparse single-cell multi-omics data. Additionally, our novel contrastive\nlearning approach, enhanced with cosine similarity regularization, enables\nsuperior alignment across omics layers compared to traditional methods.\nSystematic benchmarking across multiple datasets demonstrates that scMamba\nsignificantly outperforms state-of-the-art methods in preserving biological\nvariation, aligning omics layers, and enhancing key downstream tasks such as\nclustering, cell type annotation, and trajectory inference. Our findings\nposition scMamba as a powerful tool for large-scale single-cell multi-omics\nintegration, capable of handling large-scale atlases and advancing biological\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20697v1", "categories": ["q-bio.CB", "cs.LG"], "cate": "q-bio.CB", "url": "http://arxiv.org/abs/2506.20697v1", "AI": {"title_translation": "scMamba：一种超越高变特征选择的可扩展单细胞多组学整合基础模型", "tldr": "scMamba是一种新的基础模型，能够无需预先特征选择地整合单细胞多组学数据，并显著优于现有方法。", "motivation": "现有的单细胞多组学整合方法通常依赖于选择高变基因或峰，这可能会无意中丢弃关键的生物学信息，导致数据整合的挑战。", "method": "scMamba引入了一种基于补丁的细胞标记策略，将基因组区域视为词（tokens），细胞视为句子。它基于状态空间对偶性，从高维稀疏的单细胞多组学数据中提取生物学见解。此外，它采用了一种新颖的对比学习方法，并通过余弦相似度正则化进行增强，以实现更好的组学层对齐。", "result": "在多个数据集上的系统基准测试表明，scMamba在保留生物变异、对齐组学层以及增强下游任务（如聚类、细胞类型注释和轨迹推断）方面，显著优于现有最先进的方法。", "conclusion": "scMamba被定位为一种强大的工具，用于大规模单细胞多组学整合，能够处理大型图谱并推动生物发现。", "translation": "单细胞多组学技术的出现使得在单个细胞内同时分析不同的组学层成为可能。整合此类多模态数据为细胞特性、调控过程和疾病机制提供了前所未有的见解。然而，这仍然具有挑战性，因为当前方法在预处理过程中通常依赖于选择高变基因或峰，这可能会无意中丢弃关键的生物学信息。在此，我们提出了scMamba，一个旨在整合单细胞多组学数据的基础模型，无需预先进行特征选择，同时保留基因组位置信息。scMamba引入了一种基于补丁的细胞标记策略，将基因组区域视为词（tokens），细胞视为句子。基于状态空间对偶性的概念，scMamba从高维、稀疏的单细胞多组学数据中提炼出丰富的生物学见解。此外，我们新颖的对比学习方法，通过余弦相似度正则化增强，与传统方法相比，能够实现卓越的组学层对齐。在多个数据集上的系统基准测试表明，scMamba在保留生物变异、对齐组学层和增强关键下游任务（如聚类、细胞类型注释和轨迹推断）方面，显著优于最先进的方法。我们的发现将scMamba定位为一种强大工具，用于大规模单细胞多组学整合，能够处理大型图谱并推动生物发现。", "summary": "scMamba是一个用于单细胞多组学整合的基础模型，其创新之处在于无需预先进行高变特征选择，同时保留了基因组位置信息。它采用独特的补丁式细胞标记策略和状态空间对偶性来处理高维稀疏数据，并通过增强的对比学习实现卓越的组学层对齐。基准测试表明，scMamba在保留生物变异和提升下游任务表现方面优于现有方法，使其成为大规模多组学整合的强大工具。", "keywords": "单细胞多组学, 基础模型, 特征选择, 对比学习, scMamba", "comments": "scMamba的创新之处在于其“超越高变特征选择”的能力，这解决了当前方法中可能丢失关键生物信息的痛点。其将基因组区域视为词、细胞视为句子的标记化策略，以及结合状态空间对偶性和对比学习的设计，使其能够有效处理高维稀疏的多组学数据。该模型的可扩展性使其有望应用于大规模单细胞图谱的分析，对于推动生物发现具有重要意义。"}}
{"id": "2506.20764", "title": "Control and optimization for Neural Partial Differential Equations in Supervised Learning", "authors": ["Alain Bensoussan", "Minh-Binh Tran", "Bangjie Wang"], "summary": "Although there is a substantial body of literature on control and\noptimization problems for parabolic and hyperbolic systems, the specific\nproblem of controlling and optimizing the coefficients of the associated\noperators within such systems has not yet been thoroughly explored. In this\nwork, we aim to initiate a line of research in control theory focused on\noptimizing and controlling the coefficients of these operators-a problem that\nnaturally arises in the context of neural networks and supervised learning.\n  In supervised learning, the primary objective is to transport initial data\ntoward target data through the layers of a neural network. We propose a novel\nperspective: neural networks can be interpreted as partial differential\nequations (PDEs). From this viewpoint, the control problem traditionally\nstudied in the context of ordinary differential equations (ODEs) is\nreformulated as a control problem for PDEs, specifically targeting the\noptimization and control of coefficients in parabolic and hyperbolic operators.\nTo the best of our knowledge, this specific problem has not yet been\nsystematically addressed in the control theory of PDEs.\n  To this end, we propose a dual system formulation for the control and\noptimization problem associated with parabolic PDEs, laying the groundwork for\nthe development of efficient numerical schemes in future research. We also\nprovide a theoretical proof showing that the control and optimization problem\nfor parabolic PDEs admits minimizers. Finally, we investigate the control\nproblem associated with hyperbolic PDEs and prove the existence of solutions\nfor a corresponding approximated control problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20764v1", "categories": ["math.OC", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20764v1", "AI": {"title_translation": "监督学习中神经偏微分方程的控制与优化", "tldr": "本文提出将神经网络解释为偏微分方程（PDEs），并解决了PDE系数的控制与优化问题，证明了抛物线和双曲线方程情况下极小值/解的存在性。", "motivation": "尽管关于抛物线和双曲线系统的控制和优化问题已有大量文献，但在神经网络和监督学习背景下，控制和优化相关算子系数的具体问题尚未得到彻底探索。PDE控制理论中尚未系统地解决这一特定问题。", "method": "本文提出将神经网络解释为偏微分方程。将传统的常微分方程（ODE）控制问题重新表述为针对抛物线和双曲线算子系数优化的偏微分方程控制问题。对于抛物线PDEs，提出了对偶系统公式。对于双曲线PDEs，研究了相关控制问题并证明了近似控制问题解的存在性。", "result": "为抛物线PDEs的控制和优化问题提出了对偶系统公式，为未来开发高效数值方案奠定了基础。理论证明了抛物线PDEs的控制和优化问题存在极小值。证明了与双曲线PDEs相关的近似控制问题解的存在性。", "conclusion": "本文开创了在神经网络背景下控制和优化偏微分方程算子系数的研究方向，为抛物线和双曲线情况提供了极小值/解的理论存在性证明，为未来的数值方案奠定了基础。", "translation": "尽管关于抛物线和双曲线系统的控制和优化问题已有大量文献，但在此类系统中控制和优化相关算子系数的具体问题尚未得到彻底探索。在这项工作中，我们旨在开创控制理论中的一个研究方向，专注于优化和控制这些算子的系数——这个问题自然地出现在神经网络和监督学习的背景下。在监督学习中，主要目标是通过神经网络的层将初始数据传输到目标数据。我们提出了一种新颖的视角：神经网络可以被解释为偏微分方程（PDEs）。从这个角度来看，传统上在常微分方程（ODEs）背景下研究的控制问题被重新表述为偏微分方程的控制问题，特别是针对抛物线和双曲线算子中系数的优化和控制。据我们所知，PDE控制理论中尚未系统地解决这个具体问题。为此，我们为与抛物线PDE相关的控制和优化问题提出了一个对偶系统公式，为未来研究中开发高效数值方案奠定了基础。我们还提供了理论证明，表明抛物线PDE的控制和优化问题存在极小值。最后，我们研究了与双曲线PDE相关的控制问题，并证明了相应近似控制问题解的存在性。", "summary": "本文提出将神经网络解释为偏微分方程（PDEs），以解决监督学习中偏微分方程算子系数控制和优化的未探索问题。它将传统的基于常微分方程的控制问题重新表述为偏微分方程控制问题。作者为抛物线PDEs开发了对偶系统公式，并证明了极小值的存在性，同时证明了双曲线PDEs近似控制问题解的存在性，为未来的数值方法奠定了理论基础。", "keywords": "神经网络, 偏微分方程, 控制理论, 优化, 监督学习", "comments": "本文通过将神经网络与偏微分方程控制理论相结合，提供了一个新颖的理论视角，特别关注了系数优化这一未充分探索的领域。其主要创新在于将监督学习问题重新构建在偏微分方程框架内，并提供了基础性的存在性证明，这对于开发新的神经网络优化算法至关重要。"}}
{"id": "2506.21364", "title": "CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection", "authors": ["Zhixin Cheng", "Jiacheng Deng", "Xinjun Li", "Xiaotian Yin", "Bohao Liao", "Baoqun Yin", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Detection-free methods typically follow a coarse-to-fine pipeline, extracting\nimage and point cloud features for patch-level matching and refining dense\npixel-to-point correspondences. However, differences in feature channel\nattention between images and point clouds may lead to degraded matching\nresults, ultimately impairing registration accuracy. Furthermore, similar\nstructures in the scene could lead to redundant correspondences in cross-modal\nmatching. To address these issues, we propose Channel Adaptive Adjustment\nModule (CAA) and Global Optimal Selection Module (GOS). CAA enhances\nintra-modal features and suppresses cross-modal sensitivity, while GOS replaces\nlocal selection with global optimization. Experiments on RGB-D Scenes V2 and\n7-Scenes demonstrate the superiority of our method, achieving state-of-the-art\nperformance in image-to-point cloud registration.", "comment": "ICCV 2025 accepted", "pdf_url": "http://arxiv.org/pdf/2506.21364v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21364v1", "AI": {"title_translation": "CA-I2P: 通道自适应配准网络与全局最优选择", "tldr": "提出CA-I2P，通过通道自适应调整和全局最优选择模块，解决了图像到点云配准中特征通道差异和相似结构冗余对应的问题，实现了最先进的性能。", "motivation": "现有无检测方法在图像与点云特征通道注意力差异和场景中相似结构导致的冗余对应问题，导致配准精度下降。", "method": "提出通道自适应调整模块（CAA）增强模态内特征并抑制跨模态敏感性，以及全局最优选择模块（GOS）用全局优化取代局部选择。", "result": "在RGB-D Scenes V2和7-Scenes数据集上，我们的方法在图像到点云配准中取得了最先进的性能。", "conclusion": "CA-I2P通过其提出的CAA和GOS模块，有效解决了图像到点云配准中的关键挑战，显著提高了配准精度。", "translation": "无检测方法通常遵循从粗到精的流程，提取图像和点云特征进行补丁级匹配并细化密集的像素到点对应。然而，图像和点云之间特征通道注意力的差异可能导致匹配结果退化，最终损害配准精度。此外，场景中的相似结构可能导致跨模态匹配中出现冗余对应。为了解决这些问题，我们提出了通道自适应调整模块（CAA）和全局最优选择模块（GOS）。CAA增强模态内特征并抑制跨模态敏感性，而GOS用全局优化取代局部选择。在RGB-D Scenes V2和7-Scenes上的实验证明了我们方法的优越性，在图像到点云配准中取得了最先进的性能。", "summary": "本文提出CA-I2P，一个用于图像到点云配准的通道自适应配准网络。该网络通过通道自适应调整模块（CAA）解决特征通道注意力差异问题，并通过全局最优选择模块（GOS）处理相似结构导致的冗余对应。实验证明，CA-I2P在图像到点云配准任务上达到了最先进的性能。", "keywords": "图像到点云配准, 通道自适应, 全局最优选择, 跨模态匹配, 特征学习", "comments": "这篇论文通过引入通道自适应调整模块和全局最优选择模块，创新性地解决了图像到点云配准中特征差异和冗余对应的问题。这种方法提升了跨模态匹配的鲁棒性和准确性，对于需要高精度多模态数据融合的应用具有重要意义。"}}
{"id": "2506.20779", "title": "Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon", "authors": ["Tongtong Liang", "Dan Qiao", "Yu-Xiang Wang", "Rahul Parhi"], "summary": "We study the implicit bias of flatness / low (loss) curvature and its effects\non generalization in two-layer overparameterized ReLU networks with\nmultivariate inputs -- a problem well motivated by the minima stability and\nedge-of-stability phenomena in gradient-descent training. Existing work either\nrequires interpolation or focuses only on univariate inputs. This paper\npresents new and somewhat surprising theoretical results for multivariate\ninputs. On two natural settings (1) generalization gap for flat solutions, and\n(2) mean-squared error (MSE) in nonparametric function estimation by stable\nminima, we prove upper and lower bounds, which establish that while flatness\ndoes imply generalization, the resulting rates of convergence necessarily\ndeteriorate exponentially as the input dimension grows. This gives an\nexponential separation between the flat solutions vis-\\`a-vis low-norm\nsolutions (i.e., weight decay), which knowingly do not suffer from the curse of\ndimensionality. In particular, our minimax lower bound construction, based on a\nnovel packing argument with boundary-localized ReLU neurons, reveals how flat\nsolutions can exploit a kind of ''neural shattering'' where neurons rarely\nactivate, but with high weight magnitudes. This leads to poor performance in\nhigh dimensions. We corroborate these theoretical findings with extensive\nnumerical simulations. To the best of our knowledge, our analysis provides the\nfirst systematic explanation for why flat minima may fail to generalize in high\ndimensions.", "comment": "Comments Welcome!", "pdf_url": "http://arxiv.org/pdf/2506.20779v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20779v1", "AI": {"title_translation": "ReLU神经网络的稳定最小值受维度灾难影响：神经破碎现象", "tldr": "本文研究发现，对于多变量输入的ReLU神经网络，平坦解（低损失曲率）虽然有助于泛化，但其收敛速度会随输入维度的增加呈指数级恶化，这揭示了“神经破碎”现象导致其在高维空间表现不佳。", "motivation": "现有工作在研究ReLU神经网络中平坦度/低损失曲率的隐式偏差及其对泛化影响时，要么需要插值，要么只关注单变量输入。本文旨在研究多变量输入情况，这由梯度下降训练中的最小值稳定性和边缘稳定性现象充分激发。", "method": "本文通过理论分析，针对两种自然设置（1）平坦解的泛化差距，以及（2）稳定最小值在非参数函数估计中的均方误差（MSE），证明了上下界。通过一种基于边界局部化ReLU神经元的新颖填充论证，构建了极小极大下界。", "result": "研究发现，尽管平坦度确实能提升泛化能力，但其收敛速度必然随输入维度的增长呈指数级恶化。这导致平坦解与低范数解（例如权重衰减）之间存在指数级分离，后者已知不受维度灾难影响。特别是，神经破碎现象揭示了平坦解如何利用神经元极少激活但权重幅值较高的情况，导致在高维空间中性能不佳。理论发现得到了数值模拟的证实。", "conclusion": "本文首次系统地解释了为什么平坦最小值在高维空间中可能无法泛化，揭示了平坦解的泛化能力在高维情况下会遭遇维度灾难，并提出了“神经破碎”作为其根本原因。", "translation": "我们研究了平坦度/低（损失）曲率的隐式偏差及其在具有多变量输入的双层过参数化ReLU网络中对泛化的影响——这个问题得到了梯度下降训练中最小值稳定性和边缘稳定性现象的充分启发。现有工作要么需要插值，要么只关注单变量输入。本文为多变量输入提供了新的、有些令人惊讶的理论结果。在两种自然设置下：（1）平坦解的泛化差距，以及（2）稳定最小值在非参数函数估计中的均方误差（MSE），我们证明了上下界，这些界限表明，虽然平坦度确实意味着泛化，但其导致的收敛速度必然随输入维度的增长呈指数级恶化。这导致平坦解与低范数解（即权重衰减）之间存在指数级分离，后者已知不受维度灾难影响。特别是，我们基于一种新的边界局部化ReLU神经元填充论证构建的极小极大下界，揭示了平坦解如何利用一种“神经破碎”现象，即神经元很少激活，但具有高权重幅值。这导致在高维空间中性能不佳。我们通过大量的数值模拟证实了这些理论发现。据我们所知，我们的分析首次系统地解释了为什么平坦最小值在高维空间中可能无法泛化。", "summary": "本文研究了多变量输入下ReLU神经网络中平坦解的泛化能力。研究发现，尽管平坦度有助于泛化，但其收敛速度会随输入维度的增加呈指数级恶化，即遭遇维度灾难。这种现象被称为“神经破碎”，表现为神经元激活稀疏但权重幅值高，导致高维性能不佳。这与低范数解形成对比，后者不受维度灾难影响。本文首次系统地解释了平坦最小值在高维空间中泛化失败的原因，并通过理论分析和数值模拟进行了验证。", "keywords": "ReLU神经网络, 平坦最小值, 维度灾难, 神经破碎, 泛化", "comments": "本文创新性地揭示了ReLU神经网络中平坦最小值在高维空间下泛化性能不佳的深层原因，即“神经破碎”现象和维度灾难。它弥补了现有研究主要关注单变量输入或需要插值的不足，为理解深度学习模型的泛化行为提供了重要的理论见解。研究结果对优化算法和模型设计具有指导意义，尤其是在处理高维数据时，需要重新审视对平坦解的偏好。"}}
{"id": "2506.21369", "title": "GenFlow: Interactive Modular System for Image Generation", "authors": ["Duc-Hung Nguyen", "Huu-Phuc Huynh", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21369v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21369v1", "AI": {"title_translation": "GenFlow：交互式模块化图像生成系统", "tldr": "GenFlow是一个模块化系统，通过直观的界面和智能助手，使所有技能水平的用户都能轻松进行图像生成和生成艺术创作。", "motivation": "生成艺术虽然具有无限的创意潜力，但由于需要高级架构概念和计算工作流程方面的技术专业知识，其全部潜力尚未得到充分发挥。本研究旨在弥合这一差距，让所有用户都能轻松进行生成艺术创作。", "method": "本研究提出了GenFlow，一个新颖的模块化框架。该系统具有一个基于节点的编辑器，用于无缝定制，并配备了一个由自然语言处理驱动的智能助手。GenFlow通过自动化部署过程和最小化技术障碍，将复杂的工作流程创建转化为直观且易于访问的体验。", "result": "一项用户研究表明，GenFlow能够优化工作流程，减少任务完成时间，并通过其直观的界面和自适应功能增强用户理解。这些结果表明GenFlow重新定义了生成艺术领域的可访问性和效率。", "conclusion": "GenFlow是一个开创性的解决方案，它通过提供直观、高效和易于访问的工具，重新定义了生成艺术领域的可访问性和效率，使得尖端生成艺术工具能够被所有人使用。", "translation": "生成艺术开启了无限的创意可能性，但由于高级架构概念和计算工作流程所需的技术专业知识，其全部潜力尚未得到充分发挥。为了弥合这一差距，我们提出了GenFlow，一个新颖的模块化框架，它使所有技能水平的用户都能够精确、轻松地生成图像。GenFlow具有一个基于节点的编辑器，用于无缝定制，以及一个由自然语言处理驱动的智能助手，它将工作流程创建的复杂性转化为直观且易于访问的体验。通过自动化部署过程和最小化技术障碍，我们的框架使尖端生成艺术工具可供所有人使用。一项用户研究表明，GenFlow能够通过其直观的界面和自适应功能优化工作流程，减少任务完成时间，并增强用户理解。这些结果将GenFlow定位为一个开创性的解决方案，它重新定义了生成艺术领域的可访问性和效率。", "summary": "GenFlow是一个创新的模块化系统，旨在降低生成艺术的创作门槛。它通过提供一个基于节点的直观编辑器和由自然语言处理驱动的智能助手，简化了复杂的图像生成工作流程。该系统自动化部署并减少技术障碍，使得所有技能水平的用户都能轻松创建生成艺术。用户研究证实，GenFlow能优化工作流程、缩短任务时间并提升用户理解，从而在生成艺术领域实现了更高的可访问性和效率。", "keywords": "GenFlow, 生成艺术, 模块化系统, 图像生成, 用户界面", "comments": "GenFlow的创新之处在于其将复杂的生成艺术工作流程通过模块化、节点式编辑和NLP智能助手进行了极大的简化和用户友好化。这对于降低生成艺术的入门门槛，推广其应用具有重要意义。该系统通过自动化部署和减少技术障碍，使得更多非专业用户也能接触和使用前沿的生成艺术工具，有望推动该领域的普及和发展。其用户研究结果也验证了其在优化效率和提升用户体验方面的有效性。"}}
{"id": "2506.21398", "title": "FastRef:Fast Prototype Refinement for Few-Shot Industrial Anomaly Detection", "authors": ["Long Tian", "Yufei Li", "Yuyang Dai", "Wenchao Chen", "Xiyang Liu", "Bo Chen"], "summary": "Few-shot industrial anomaly detection (FS-IAD) presents a critical challenge\nfor practical automated inspection systems operating in data-scarce\nenvironments. While existing approaches predominantly focus on deriving\nprototypes from limited normal samples, they typically neglect to\nsystematically incorporate query image statistics to enhance prototype\nrepresentativeness. To address this issue, we propose FastRef, a novel and\nefficient prototype refinement framework for FS-IAD. Our method operates\nthrough an iterative two-stage process: (1) characteristic transfer from query\nfeatures to prototypes via an optimizable transformation matrix, and (2)\nanomaly suppression through prototype alignment. The characteristic transfer is\nachieved through linear reconstruction of query features from prototypes, while\nthe anomaly suppression addresses a key observation in FS-IAD that unlike\nconventional IAD with abundant normal prototypes, the limited-sample setting\nmakes anomaly reconstruction more probable. Therefore, we employ optimal\ntransport (OT) for non-Gaussian sampled features to measure and minimize the\ngap between prototypes and their refined counterparts for anomaly suppression.\nFor comprehensive evaluation, we integrate FastRef with three competitive\nprototype-based FS-IAD methods: PatchCore, FastRecon, WinCLIP, and AnomalyDINO.\nExtensive experiments across four benchmark datasets of MVTec, ViSA, MPDD and\nRealIAD demonstrate both the effectiveness and computational efficiency of our\napproach under 1/2/4-shots.", "comment": "18pages, 7figures, 6tables", "pdf_url": "http://arxiv.org/pdf/2506.21398v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21398v1", "AI": {"title_translation": "FastRef：用于少样本工业异常检测的快速原型细化", "tldr": "FastRef提出了一种快速原型细化框架，通过查询特征的特性转移和异常抑制来解决少样本工业异常检测中原型代表性不足的问题，并在多个基准数据集上表现出有效性和计算效率。", "motivation": "现有少样本工业异常检测方法主要关注从有限的正常样本中派生原型，但通常忽略系统地结合查询图像统计数据来增强原型代表性，这在数据稀缺的实际自动化检测系统中是一个关键挑战。", "method": "本文提出了FastRef，一种新颖高效的少样本工业异常检测原型细化框架。该方法通过迭代的两阶段过程进行操作：(1) 通过可优化的变换矩阵将查询特征的特性转移到原型；(2) 通过原型对齐进行异常抑制。特性转移通过原型对查询特征进行线性重构实现。异常抑制则通过使用最优传输(OT)来测量和最小化原型与其细化对应物之间的差距，以处理非高斯采样特征，因为在少样本设置下异常重构的可能性更高。", "result": "FastRef已与PatchCore, FastRecon, WinCLIP, 和 AnomalyDINO这四种有竞争力的基于原型的少样本工业异常检测方法集成。在MVTec, ViSA, MPDD和RealIAD四个基准数据集上，在1/2/4-shots设置下进行了广泛实验，结果证明了该方法的有效性和计算效率。", "conclusion": "FastRef通过其创新的原型细化框架，有效解决了少样本工业异常检测中原型代表性不足的问题，并在实际应用中展现出良好的性能和效率。", "translation": "少样本工业异常检测（FS-IAD）对在数据稀缺环境中运行的实际自动化检测系统提出了严峻挑战。虽然现有方法主要侧重于从有限的正常样本中提取原型，但它们通常忽略系统地结合查询图像统计数据来增强原型的代表性。为了解决这个问题，我们提出了FastRef，一种新颖高效的FS-IAD原型细化框架。我们的方法通过迭代的两阶段过程进行操作：(1) 通过可优化的变换矩阵将查询特征的特性转移到原型；(2) 通过原型对齐进行异常抑制。特性转移通过原型对查询特征进行线性重构实现，而异常抑制则解决了FS-IAD中的一个关键观察，即与具有丰富正常原型的传统IAD不同，有限样本设置使得异常重构的可能性更大。因此，我们采用最优传输（OT）处理非高斯采样特征，以测量和最小化原型与其细化对应物之间的差距，从而抑制异常。为了进行全面评估，我们将FastRef与三种有竞争力的基于原型的FS-IAD方法集成：PatchCore、FastRecon、WinCLIP和AnomalyDINO。在MVTec、ViSA、MPDD和RealIAD四个基准数据集上，在1/2/4-shots设置下进行了广泛实验，结果证明了我们方法的有效性和计算效率。", "summary": "FastRef提出了一种新颖高效的原型细化框架，用于解决少样本工业异常检测（FS-IAD）中原型代表性不足的问题。该方法采用两阶段迭代过程：一是通过可优化变换矩阵将查询特征特性转移到原型，二是通过最优传输（OT）进行原型对齐以抑制异常。实验结果表明，FastRef在多个基准数据集上与现有FS-IAD方法集成后，在有限样本设置下具有显著的有效性和计算效率。", "keywords": "少样本异常检测, 原型细化, 工业检测, 特性转移, 最优传输", "comments": "FastRef的创新点在于其通过迭代的两阶段过程精炼原型，特别是引入了“特性转移”和“异常抑制”的概念。通过结合查询图像统计数据来增强原型代表性，并利用最优传输处理非高斯特征，解决了少样本设置下异常重构概率增大的关键问题。这对于数据稀缺的工业应用具有重要意义，因为它提高了少样本异常检测的准确性和效率。"}}
{"id": "2506.21278", "title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution", "authors": ["Lukas Sablica", "Kurt Hornik"], "summary": "We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21278v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21278v1", "AI": {"title_translation": "使用高效球面柯西分布的超球面变分自编码器", "tldr": "本文提出了一种使用球面柯西（spCauchy）潜在分布的新型变分自编码器（VAE）架构，它提供了更自然的超球面表示，解决了现有方法（如vMF）的数值不稳定性，并实现了高效稳定的训练。", "motivation": "传统的变分自编码器（VAE）在使用高斯潜在空间或von Mises-Fisher（vMF）分布时存在局限性：高斯分布不适合超球面数据，而vMF分布存在数值不稳定性，且两者都可能导致过度正则化。因此，需要一种更自然、稳定、富有表现力的潜在分布来改进超球面VAE。", "method": "本文提出了一种新颖的VAE架构，该架构采用球面柯西（spCauchy）潜在分布。它通过莫比乌斯变换实现了完全可微分且高效的重参数化技巧，并利用快速收敛的幂级数计算KL散度，从而避免了超几何函数比值评估中可能出现的下溢或溢出问题。", "result": "所提出的球面柯西（spCauchy）分布为潜在变量提供了更自然的超球面表示，能更好地捕获方向性数据，同时保持灵活性。其重尾特性防止了过度正则化，确保了高效的潜在空间利用和更具表现力的表示。此外，spCauchy规避了vMF固有的数值不稳定性，实现了稳定且可扩展的训练。", "conclusion": "球面柯西（spCauchy）分布是变分自编码器（VAE）的一个引人注目的替代方案，在高维生成建模中提供了理论优势和实际效率。", "translation": "我们提出了一种新颖的变分自编码器（VAE）架构，该架构采用球面柯西（spCauchy）潜在分布。与传统的欧几里得高斯潜在空间或广泛使用的von Mises-Fisher（vMF）分布不同，spCauchy为潜在变量提供了更自然的超球面表示，更好地捕获方向性数据，同时保持灵活性。其重尾特性防止了过度正则化，确保了高效的潜在空间利用，同时提供了更具表现力的表示。此外，spCauchy规避了vMF固有的数值不稳定性，这些不稳定性源于涉及贝塞尔函数的归一化常数计算。相反，它通过莫比乌斯变换实现了完全可微分且高效的重参数化技巧，从而实现稳定和可扩展的训练。KL散度可以通过快速收敛的幂级数计算，消除了与超几何函数比值评估相关的下溢或溢出问题。这些特性使spCauchy成为VAE的一个引人注目的替代方案，在高维生成建模中提供了理论优势和实际效率。", "summary": "本文介绍了一种新颖的变分自编码器（VAE）架构，其核心是使用球面柯西（spCauchy）潜在分布。这种方法为潜在变量提供了更自然、更具表现力的超球面表示，能够有效地捕捉方向性数据。与传统的高斯或von Mises-Fisher（vMF）分布相比，spCauchy的重尾特性避免了过度正则化，从而实现了高效的潜在空间利用。同时，它通过莫比乌斯变换实现了完全可微分的重参数化技巧，并通过稳定的幂级数计算KL散度，成功解决了vMF分布的数值不稳定性问题。这些优势使spCauchy成为高维生成建模中一个理论上优越且实际高效的选择。", "keywords": "超球面VAE, 球面柯西分布, 潜在空间, 重参数化技巧, 生成建模", "comments": "本文的创新之处在于将球面柯西（spCauchy）分布引入变分自编码器（VAE）的潜在空间，以解决现有方法（如高斯和vMF）在超球面数据建模中的局限性。其提出的重参数化技巧和KL散度计算方法显著提升了模型的数值稳定性和训练效率，对于处理方向性数据和高维生成建模具有重要意义。"}}
{"id": "2506.21401", "title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction", "authors": ["Zhirui Gao. Renjiao Yi", "Yaqiao Dai", "Xuening Zhu", "Wei Chen", "Chenyang Zhu", "Kai Xu"], "summary": "This paper presents an end-to-end framework for reconstructing 3D parametric\ncurves directly from multi-view edge maps. Contrasting with existing two-stage\nmethods that follow a sequential ``edge point cloud reconstruction and\nparametric curve fitting'' pipeline, our one-stage approach optimizes 3D\nparametric curves directly from 2D edge maps, eliminating error accumulation\ncaused by the inherent optimization gap between disconnected stages. However,\nparametric curves inherently lack suitability for rendering-based multi-view\noptimization, necessitating a complementary representation that preserves their\ngeometric properties while enabling differentiable rendering. We propose a\nnovel bi-directional coupling mechanism between parametric curves and\nedge-oriented Gaussian components. This tight correspondence formulates a\ncurve-aware Gaussian representation, \\textbf{CurveGaussian}, that enables\ndifferentiable rendering of 3D curves, allowing direct optimization guided by\nmulti-view evidence. Furthermore, we introduce a dynamically adaptive topology\noptimization framework during training to refine curve structures through\nlinearization, merging, splitting, and pruning operations. Comprehensive\nevaluations on the ABC dataset and real-world benchmarks demonstrate our\none-stage method's superiority over two-stage alternatives, particularly in\nproducing cleaner and more robust reconstructions. Additionally, by directly\noptimizing parametric curves, our method significantly reduces the parameter\ncount during training, achieving both higher efficiency and superior\nperformance compared to existing approaches.", "comment": "Code: https://github.com/zhirui-gao/Curve-Gaussian Accepted by ICCV\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21401v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21401v1", "AI": {"title_translation": "曲线感知高斯泼溅用于三维参数曲线重建", "tldr": "提出一种名为CurveGaussian的单阶段端到端框架，通过曲线感知高斯表示实现从多视图边缘图直接重建三维参数曲线，克服了传统两阶段方法的误差累积问题，并实现了更高效和卓越的性能。", "motivation": "现有的三维参数曲线重建方法是两阶段的，先重建边缘点云再拟合曲线，导致误差累积和优化鸿沟。此外，参数曲线本身不适合基于渲染的多视图优化。", "method": "本文提出一个端到端的单阶段框架，直接从2D边缘图优化3D参数曲线。核心是引入参数曲线与边缘导向高斯分量之间的双向耦合机制，形成“CurveGaussian”表示，实现3D曲线的可微分渲染，从而允许由多视图证据引导的直接优化。此外，训练过程中还引入了动态自适应拓扑优化框架（通过线性化、合并、分割和修剪操作）来完善曲线结构。", "result": "在ABC数据集和真实世界基准测试中，该单阶段方法优于两阶段替代方案，尤其在生成更清晰、更鲁棒的重建方面。通过直接优化参数曲线，显著减少了训练过程中的参数数量，实现了更高的效率和卓越的性能。", "conclusion": "本文提出的单阶段CurveGaussian框架通过直接从2D边缘图优化3D参数曲线，有效解决了传统两阶段方法的误差累积问题，并在效率和重建质量上均超越现有方法。", "translation": "本文提出一个端到端框架，用于直接从多视图边缘图重建三维参数曲线。与现有遵循“边缘点云重建和参数曲线拟合”顺序管道的两阶段方法形成对比，我们的一阶段方法直接从2D边缘图优化3D参数曲线，消除了由不连贯阶段之间固有的优化差距引起的误差累积。然而，参数曲线本质上不适合基于渲染的多视图优化，因此需要一种互补的表示，既能保留其几何属性又能实现可微分渲染。我们提出了一种新颖的参数曲线与边缘导向高斯分量之间的双向耦合机制。这种紧密的对应关系形成了一种曲线感知高斯表示，\\textbf{CurveGaussian}，它使得3D曲线的可微分渲染成为可能，从而允许由多视图证据引导的直接优化。此外，我们在训练过程中引入了一个动态自适应拓扑优化框架，通过线性化、合并、分割和修剪操作来完善曲线结构。在ABC数据集和真实世界基准上的全面评估表明，我们的一阶段方法优于两阶段替代方案，特别是在生成更清晰、更鲁棒的重建方面。此外，通过直接优化参数曲线，我们的方法显著减少了训练期间的参数数量，与现有方法相比，实现了更高的效率和卓越的性能。", "summary": "本文提出了一种名为CurveGaussian的创新性单阶段框架，用于从多视图边缘图直接重建三维参数曲线。该方法通过引入参数曲线与边缘导向高斯分量的双向耦合机制，克服了传统两阶段方法中存在的误差累积和优化差距问题，实现了3D曲线的可微分渲染及直接优化。结合动态自适应拓扑优化，CurveGaussian在重建质量、效率和鲁棒性方面均显著优于现有方法。", "keywords": "三维参数曲线重建, 高斯泼溅, 单阶段方法, 可微分渲染, CurveGaussian", "comments": "该论文的创新点在于提出了一个端到端的单阶段框架，直接优化3D参数曲线，避免了传统两阶段方法的误差累积。核心贡献是“CurveGaussian”表示，它巧妙地结合了参数曲线和高斯泼溅的优势，实现了3D曲线的可微分渲染。此外，动态拓扑优化也增强了方法的实用性。这项工作为3D曲线重建提供了一个高效且鲁棒的新范式。"}}
{"id": "2506.21416", "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation", "authors": ["Bowen Chen", "Mengyi Zhao", "Haomiao Sun", "Li Chen", "Xu Wang", "Kang Du", "Xinglong Wu"], "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.", "comment": "Project Page: https://bytedance.github.io/XVerse Github Link:\n  https://github.com/bytedance/XVerse", "pdf_url": "http://arxiv.org/pdf/2506.21416v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21416v1", "AI": {"title_translation": "XVerse：通过DiT调制实现身份和语义属性的多主体一致性控制", "tldr": "XVerse是一个新的多主体图像生成模型，通过将参考图像转换为文本流调制的偏移量，解决了多主体文本到图像生成中身份和语义属性控制的挑战，实现了高保真、可编辑的多主体图像合成。", "motivation": "在文本到图像生成中，特别是对于多个主体，实现对主体身份和语义属性（姿态、风格、光照）的细粒度控制，往往会损害扩散变换器（DiTs）的可编辑性和连贯性。许多现有方法会引入伪影或遭受属性纠缠问题。", "method": "我们提出了一个新颖的多主体受控生成模型XVerse。通过将参考图像转换为针对特定token的文本流调制的偏移量，XVerse可以在不干扰图像潜在表示或特征的情况下，对特定主体进行精确且独立的控制。", "result": "XVerse提供了高保真、可编辑的多主体图像合成，并能对个体主体特征和语义属性进行稳健控制。", "conclusion": "这项进展显著提高了个性化和复杂场景的生成能力。", "translation": "在文本到图像生成中，特别是对于多个主体，实现对主体身份和语义属性（姿态、风格、光照）的细粒度控制，往往会损害扩散变换器（DiTs）的可编辑性和连贯性。许多方法会引入伪影或遭受属性纠缠。为了克服这些挑战，我们提出了一个新颖的多主体受控生成模型XVerse。通过将参考图像转换为针对特定token的文本流调制的偏移量，XVerse可以在不干扰图像潜在表示或特征的情况下，对特定主体进行精确且独立的控制。因此，XVerse提供了高保真、可编辑的多主体图像合成，并能对个体主体特征和语义属性进行稳健控制。这项进展显著提高了个性化和复杂场景的生成能力。", "summary": "本文提出了一种名为XVerse的新型多主体受控生成模型，旨在解决在文本到图像生成中，尤其是在多主体场景下，对主体身份和语义属性进行细粒度控制时，扩散变换器（DiTs）的可编辑性和连贯性受损，以及现有方法引入伪影和属性纠缠的问题。XVerse通过将参考图像转换为token特异性文本流调制的偏移量，实现了对特定主体的精确和独立控制，且不干扰图像的潜在表示或特征。最终，XVerse能够生成高保真、可编辑的多主体图像，并对个体主体的特征和语义属性进行稳健控制，从而显著提升了个性化和复杂场景的生成能力。", "keywords": "XVerse, 多主体控制, DiT调制, 身份控制, 语义属性", "comments": "XVerse通过其独特的“参考图像到偏移量”的转换机制，实现了对多主体身份和语义属性的精细控制，解决了DiTs在多主体生成中的一致性和可编辑性难题。这种不干扰图像潜在表示或特征的方法，是其创新点所在，对于个性化和复杂场景的图像生成具有重要意义。"}}
{"id": "2506.20831", "title": "Efficacy of Temporal Fusion Transformers for Runoff Simulation", "authors": ["Sinan Rasiya Koya", "Tirthankar Roy"], "summary": "Combining attention with recurrence has shown to be valuable in sequence\nmodeling, including hydrological predictions. Here, we explore the strength of\nTemporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks\nin rainfall-runoff modeling. We train ten randomly initialized models, TFT and\nLSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five\nsubsets of the Caravan dataset, each representing catchments in the US,\nAustralia, Brazil, Great Britain, and Chile. Then, the performance of the\nmodels, their variability regarding the catchment attributes, and the\ndifference according to the datasets are assessed. Our findings show that TFT\nslightly outperforms LSTM, especially in simulating the midsection and peak of\nhydrographs. Furthermore, we show the ability of TFT to handle longer sequences\nand why it can be a better candidate for higher or larger catchments. Being an\nexplainable AI technique, TFT identifies the key dynamic and static variables,\nproviding valuable scientific insights. However, both TFT and LSTM exhibit a\nconsiderable drop in performance with the Caravan dataset, indicating possible\ndata quality issues. Overall, the study highlights the potential of TFT in\nimproving hydrological modeling and understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20831v1", "categories": ["physics.geo-ph", "cs.LG", "stat.AP"], "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.20831v1", "AI": {"title_translation": "时间融合转换器在径流模拟中的效能", "tldr": "本研究评估了时间融合转换器（TFT）在降雨径流模拟中相对于长短期记忆（LSTM）网络的性能，发现TFT略优于LSTM，尤其是在模拟径流曲线的中段和峰值方面，并且能处理更长的序列，提供可解释的科学见解。", "motivation": "结合注意力机制和循环网络在序列建模（包括水文预测）中已被证明很有价值。本研究旨在探索时间融合转换器（TFT）在降雨径流建模中相对于长短期记忆（LSTM）网络的优势。", "method": "研究训练了十个随机初始化的时间融合转换器（TFT）和长短期记忆（LSTM）模型，应用于美国531个CAMELS流域。实验还使用Caravan数据集的五个子集（分别代表美国、澳大利亚、巴西、英国和智利的流域）重复进行。随后，评估了模型的性能、其在流域属性方面的变异性以及不同数据集之间的差异。", "result": "研究发现，时间融合转换器（TFT）略微优于长短期记忆（LSTM）网络，尤其是在模拟径流曲线的中段和峰值。此外，TFT能够处理更长的序列，并且可能更适合更高或更大的流域。作为一种可解释的人工智能技术，TFT能识别关键的动态和静态变量。然而，TFT和LSTM在Caravan数据集上的性能均显著下降，这可能表明数据质量存在问题。", "conclusion": "这项研究总体上突出了时间融合转换器（TFT）在改进水文建模和理解方面的潜力。", "translation": "结合注意力机制与循环网络在序列建模（包括水文预测）中已显示出其价值。在此，我们探讨了时间融合转换器（TFTs）在降雨径流建模中相对于长短期记忆（LSTM）网络的优势。我们训练了十个随机初始化的模型，包括TFT和LSTM，应用于美国531个CAMELS流域。我们使用Caravan数据集的五个子集重复了实验，每个子集代表美国、澳大利亚、巴西、英国和智利的流域。然后，评估了模型的性能、其在流域属性方面的变异性以及根据数据集的不同而产生的差异。我们的研究结果表明，TFT略微优于LSTM，尤其是在模拟径流曲线的中段和峰值。此外，我们展示了TFT处理更长序列的能力，以及为什么它可能是更高或更大流域的更好选择。作为一种可解释的人工智能技术，TFT能够识别关键的动态和静态变量，提供有价值的科学见解。然而，TFT和LSTM在Caravan数据集上的性能都出现了显著下降，这表明可能存在数据质量问题。总的来说，这项研究强调了TFT在改进水文建模和理解方面的潜力。", "summary": "本研究评估了时间融合转换器（TFT）在降雨径流模拟中的效能，并将其与长短期记忆（LSTM）网络进行比较。研究在美国CAMELS流域和全球Caravan数据集的子集上训练并测试了这两种模型。结果显示，TFT在性能上略优于LSTM，特别是在模拟水文曲线的中段和峰值，并且能够处理更长序列，提供可解释的变量洞察。尽管TFT具有优势，但两种模型在Caravan数据集上均表现出性能下降，可能与数据质量问题有关。总体而言，本研究强调了TFT在提升水文建模和理解方面的潜力。", "keywords": "时间融合转换器, 径流模拟, LSTM, 水文建模, 可解释AI", "comments": "该论文的创新之处在于将时间融合转换器（TFT）应用于水文径流模拟，并与传统的LSTM进行对比。TFT在处理长序列和提供可解释性方面的能力是其重要优势，这对于科学洞察和模型理解非常有价值。然而，论文也指出了在不同数据集上性能下降的问题，提示未来研究需要关注数据质量对模型泛化能力的影响。"}}
{"id": "2506.20839", "title": "Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys", "authors": ["Jing Luo", "Yejun Gu", "Yanfei Wang", "Xiaolong Ma", "Jaafar. A El-Awady"], "summary": "Machine learning has significantly advanced the understanding and application\nof structural materials, with an increasing emphasis on integrating existing\ndata and quantifying uncertainties in predictive modeling. This study presents\na comprehensive methodology utilizing a mixed density network (MDN) model,\ntrained on extensive experimental data from literature. This approach uniquely\npredicts the distribution of dislocation density, inferred as a latent\nvariable, and the resulting stress distribution at the grain level. The\nincorporation of statistical parameters of those predicted distributions into a\ndislocation-mediated plasticity model allows for accurate stress-strain\npredictions with explicit uncertainty quantification. This strategy not only\nimproves the accuracy and reliability of mechanical property predictions but\nalso plays a vital role in optimizing alloy design, thereby facilitating the\ndevelopment of new materials in a rapidly evolving industry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20839v1", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.20839v1", "AI": {"title_translation": "FCC合金中位错塑性与应力-应变响应预测的不确定性感知机器学习框架", "tldr": "本研究提出一种不确定性感知的机器学习框架，利用混合密度网络（MDN）模型预测FCC合金的位错塑性与应力-应变响应，并量化不确定性，以改进材料设计。", "motivation": "机器学习在结构材料的理解和应用方面取得了显著进展，但越来越强调整合现有数据和量化预测模型中的不确定性。本研究旨在通过提高机械性能预测的准确性和可靠性，并优化合金设计来解决这一需求。", "method": "本研究提出了一种综合方法，利用在大量实验数据上训练的混合密度网络（MDN）模型。该方法独特地预测位错密度（推断为潜在变量）的分布以及晶粒水平上的应力分布。将这些预测分布的统计参数纳入位错介导的塑性模型中，从而实现了具有明确不确定性量化的精确应力-应变预测。", "result": "该策略不仅提高了机械性能预测的准确性和可靠性，而且在优化合金设计方面发挥了至关重要的作用，从而促进了快速发展行业中新材料的开发。", "conclusion": "所提出的不确定性感知机器学习框架显著提高了机械性能预测的准确性和可靠性，并在优化合金设计、促进新材料开发方面发挥关键作用。", "translation": "机器学习在结构材料的理解和应用方面取得了显著进展，对整合现有数据和量化预测模型中的不确定性越来越重视。本研究提出了一种综合方法，利用在大量实验数据上训练的混合密度网络（MDN）模型。该方法独特地预测位错密度（推断为潜在变量）的分布以及晶粒水平上的应力分布。将这些预测分布的统计参数纳入位错介导的塑性模型中，从而实现了具有明确不确定性量化的精确应力-应变预测。该策略不仅提高了机械性能预测的准确性和可靠性，而且在优化合金设计方面发挥了至关重要的作用，从而促进了快速发展行业中新材料的开发。", "summary": "本研究提出了一种新颖的机器学习框架，利用混合密度网络（MDN）模型预测FCC合金中的位错塑性与应力-应变响应。通过预测位错密度和应力分布，并将其统计参数整合到位错介导的塑性模型中，该框架实现了具有明确不确定性量化的精确应力-应变预测。这显著提高了机械性能预测的准确性和可靠性，并有助于优化合金设计和新材料开发。", "keywords": "机器学习, 不确定性量化, 位错塑性, 应力-应变, FCC合金, 混合密度网络", "comments": "该论文的创新点在于利用混合密度网络（MDN）模型对位错密度和应力分布进行不确定性量化，并将其与位错介导的塑性模型相结合。这种明确的不确定性量化对于材料的可靠设计和开发至关重要，为材料科学领域的预测建模提供了更稳健的方法。"}}
{"id": "2506.21430", "title": "HyperSORT: Self-Organising Robust Training with hyper-networks", "authors": ["Samuel Joutard", "Marijn Stollenga", "Marc Balle Sanchez", "Mohammad Farid Azampour", "Raphael Prevost"], "summary": "Medical imaging datasets often contain heterogeneous biases ranging from\nerroneous labels to inconsistent labeling styles. Such biases can negatively\nimpact deep segmentation networks performance. Yet, the identification and\ncharacterization of such biases is a particularly tedious and challenging task.\nIn this paper, we introduce HyperSORT, a framework using a hyper-network\npredicting UNets' parameters from latent vectors representing both the image\nand annotation variability. The hyper-network parameters and the latent vector\ncollection corresponding to each data sample from the training set are jointly\nlearned. Hence, instead of optimizing a single neural network to fit a dataset,\nHyperSORT learns a complex distribution of UNet parameters where low density\nareas can capture noise-specific patterns while larger modes robustly segment\norgans in differentiated but meaningful manners. We validate our method on two\n3D abdominal CT public datasets: first a synthetically perturbed version of the\nAMOS dataset, and TotalSegmentator, a large scale dataset containing real\nunknown biases and errors. Our experiments show that HyperSORT creates a\nstructured mapping of the dataset allowing the identification of relevant\nsystematic biases and erroneous samples. Latent space clusters yield UNet\nparameters performing the segmentation task in accordance with the underlying\nlearned systematic bias. The code and our analysis of the TotalSegmentator\ndataset are made available: https://github.com/ImFusionGmbH/HyperSORT", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21430v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21430v1", "AI": {"title_translation": "HyperSORT：基于超网络的自组织鲁棒训练", "tldr": "HyperSORT是一个使用超网络学习UNet参数复杂分布的框架，用于处理医学图像数据集中存在的偏置和错误标签，从而实现鲁棒的分割和偏置识别。", "motivation": "医学影像数据集常包含异质性偏置，如错误标签和不一致的标注风格，这些偏置会严重影响深度分割网络的性能。然而，识别和表征这些偏置是一项繁琐且具挑战性的任务。", "method": "论文提出了HyperSORT框架，该框架利用一个超网络从代表图像和标注变异性的潜在向量中预测UNet的参数。超网络参数和对应每个训练数据样本的潜在向量集合是联合学习的。与优化单个神经网络拟合数据集不同，HyperSORT学习UNet参数的复杂分布，其中低密度区域可以捕获噪声特异性模式，而较大模式则以区分但有意义的方式稳健地分割器官。该方法在两个3D腹部CT公共数据集上进行了验证：一个是合成扰动版的AMOS数据集，另一个是包含真实未知偏置和错误的大规模TotalSegmentator数据集。", "result": "实验表明，HyperSORT创建了数据集的结构化映射，从而能够识别相关的系统性偏置和错误样本。潜在空间聚类产生的UNet参数能够根据底层学习到的系统性偏置执行分割任务。", "conclusion": "HyperSORT能够有效处理医学影像数据中的异质性偏置，通过学习UNet参数的复杂分布，不仅实现了鲁棒的分割，还能识别并映射数据集中的系统性偏置和错误样本。", "translation": "医学影像数据集通常包含异质性偏置，从错误的标签到不一致的标注风格。这些偏置会对深度分割网络的性能产生负面影响。然而，识别和表征这些偏置是一项特别繁琐和具有挑战性的任务。在本文中，我们引入了HyperSORT，这是一个使用超网络从代表图像和标注变异性的潜在向量中预测UNet参数的框架。超网络参数和对应训练集中每个数据样本的潜在向量集合是联合学习的。因此，HyperSORT不是优化单个神经网络来拟合数据集，而是学习UNet参数的复杂分布，其中低密度区域可以捕获噪声特异性模式，而较大模式则以差异化但有意义的方式稳健地分割器官。我们在两个3D腹部CT公共数据集上验证了我们的方法：首先是AMOS数据集的一个合成扰动版本，其次是TotalSegmentator，一个包含真实未知偏置和错误的大规模数据集。我们的实验表明，HyperSORT创建了数据集的结构化映射，从而能够识别相关的系统性偏置和错误样本。潜在空间聚类产生的UNet参数能够根据底层学习到的系统性偏置执行分割任务。代码和我们对TotalSegmentator数据集的分析已可用：https://github.com/ImFusionGmbH/HyperSORT", "summary": "HyperSORT是一种新颖的框架，通过利用超网络和联合学习策略，解决了医学影像数据集中存在的异质性偏置（如错误标签和不一致标注）对深度分割网络性能的负面影响。该方法不直接优化单一网络，而是学习UNet参数的复杂分布，使得网络能够捕获噪声模式并鲁棒地执行分割任务。实验证明，HyperSORT能有效识别数据集中的系统性偏置和错误样本，并通过潜在空间聚类生成适应特定偏置的分割参数。", "keywords": "HyperSORT, 超网络, 鲁棒训练, 医学影像分割, 数据偏置, UNet", "comments": "这篇论文提出了一种创新的方法来处理医学影像数据中常见的标注偏置和错误，这是一个重要的实际问题。通过引入超网络来学习UNet参数的分布，而不是单个网络，HyperSORT能够更灵活地适应数据中的变异性。其能够识别数据集中的系统性偏置和错误样本的特性，对于数据质量控制和模型解释性具有重要意义。这种方法有望提高深度学习模型在真实世界医疗数据上的鲁棒性和泛化能力。"}}
{"id": "2506.21444", "title": "Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation", "authors": ["Sweta Banerjee", "Viktoria Weiss", "Taryn A. Donovan", "Rutger A. Fick", "Thomas Conrad", "Jonas Ammeling", "Nils Porsche", "Robert Klopfleisch", "Christopher Kaltenecker", "Katharina Breininger", "Marc Aubreville", "Christof A. Bertram"], "summary": "Atypical mitoses mark a deviation in the cell division process that can be an\nindependent prognostically relevant marker for tumor malignancy. However, their\nidentification remains challenging due to low prevalence, at times subtle\nmorphological differences from normal mitoses, low inter-rater agreement among\npathologists, and class imbalance in datasets. Building on the Atypical Mitosis\ndataset for Breast Cancer (AMi-Br), this study presents a comprehensive\nbenchmark comparing deep learning approaches for automated atypical mitotic\nfigure (AMF) classification, including baseline models, foundation models with\nlinear probing, and foundation models fine-tuned with low-rank adaptation\n(LoRA). For rigorous evaluation, we further introduce two new hold-out AMF\ndatasets - AtNorM-Br, a dataset of mitoses from the The TCGA breast cancer\ncohort, and AtNorM-MD, a multi-domain dataset of mitoses from the MIDOG++\ntraining set. We found average balanced accuracy values of up to 0.8135,\n0.7696, and 0.7705 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and\nAtNorM-MD datasets, respectively, with the results being particularly good for\nLoRA-based adaptation of the Virchow-line of foundation models. Our work shows\nthat atypical mitosis classification, while being a challenging problem, can be\neffectively addressed through the use of recent advances in transfer learning\nand model fine-tuning techniques. We make available all code and data used in\nthis paper in this github repository:\nhttps://github.com/DeepMicroscopy/AMi-Br_Benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21444v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21444v1", "AI": {"title_translation": "深度学习和视觉基础模型在非典型与正常有丝分裂分类中的基准测试及跨数据集评估", "tldr": "本研究对深度学习和视觉基础模型在非典型有丝分裂分类中的表现进行了基准测试，引入了新的数据集进行跨领域评估，并发现迁移学习和模型微调技术（特别是LoRA）能有效解决这一挑战性问题。", "motivation": "非典型有丝分裂是肿瘤恶性程度的独立预后相关标志物，但其识别具有挑战性，原因包括患病率低、与正常有丝分裂形态差异细微、病理学家间判读一致性低以及数据集中类别不平衡。", "method": "本研究基于Atypical Mitosis dataset for Breast Cancer (AMi-Br)数据集，对用于自动化非典型有丝分裂（AMF）分类的深度学习方法进行了全面基准测试，包括基线模型、带有线性探测的基础模型以及使用低秩适应（LoRA）进行微调的基础模型。此外，引入了两个新的保留AMF数据集：AtNorM-Br（来自TCGA乳腺癌队列）和AtNorM-MD（来自MIDOG++训练集的多领域数据集）进行严格评估。", "result": "研究发现在域内AMi-Br数据集上平均平衡准确率最高可达0.8135，在域外AtNorM-Br和AtNorM-MD数据集上分别为0.7696和0.7705。其中，基于LoRA对Virchow系列基础模型的适应效果尤为出色。", "conclusion": "非典型有丝分裂分类是一个具有挑战性的问题，但通过利用迁移学习和模型微调技术（特别是LoRA）的最新进展，可以有效地解决。", "translation": "非典型有丝分裂标志着细胞分裂过程的偏差，其可以作为肿瘤恶性程度的独立预后相关标志物。然而，由于患病率低、与正常有丝分裂形态有时差异细微、病理学家之间判读一致性低以及数据集中类别不平衡等原因，其识别仍然具有挑战性。本研究以乳腺癌非典型有丝分裂数据集（AMi-Br）为基础，对自动化非典型有丝分裂（AMF）分类的深度学习方法进行了全面基准测试，包括基线模型、带有线性探测的基础模型以及使用低秩适应（LoRA）进行微调的基础模型。为了进行严格评估，我们进一步引入了两个新的保留AMF数据集——AtNorM-Br，一个来自TCGA乳腺癌队列的有丝分裂数据集，以及AtNorM-MD，一个来自MIDOG++训练集的多领域有丝分裂数据集。我们发现在域内AMi-Br数据集上平均平衡准确率最高可达0.8135，在域外AtNorM-Br和AtNorM-MD数据集上分别为0.7696和0.7705，其中基于LoRA对Virchow系列基础模型的适应效果尤为出色。我们的工作表明，非典型有丝分裂分类虽然是一个具有挑战性的问题，但通过利用迁移学习和模型微调技术的最新进展可以有效地解决。本研究中使用的所有代码和数据均可在以下GitHub仓库获取：https://github.com/DeepMicroscopy/AMi-Br_Benchmark。", "summary": "本研究旨在解决非典型有丝分裂识别的挑战，对深度学习和视觉基础模型在非典型与正常有丝分裂分类中的性能进行了全面基准测试。研究使用了现有数据集并引入了两个新的跨领域数据集进行评估。结果显示，迁移学习和模型微调技术，特别是基于LoRA的基础模型适应，能有效提升分类准确率，为非典型有丝分裂的自动化识别提供了有效方案。", "keywords": "非典型有丝分裂, 深度学习, 视觉基础模型, 迁移学习, LoRA", "comments": "这项研究的重要性在于它系统地评估了前沿的深度学习和视觉基础模型在病理学中一个具有挑战性的分类任务上的表现。通过引入新的、多领域的数据集进行跨数据集评估，增强了研究结果的泛化性。特别是，发现LoRA等高效微调技术在解决类别不平衡和形态差异细微的问题上表现出色，为未来的病理图像分析提供了有价值的指导。公开代码和数据也促进了研究的透明度和可复现性。"}}
{"id": "2506.21446", "title": "Controllable 3D Placement of Objects with Scene-Aware Diffusion Models", "authors": ["Mohamed Omran", "Dimitris Kalatzis", "Jens Petersen", "Amirhossein Habibian", "Auke Wiggers"], "summary": "Image editing approaches have become more powerful and flexible with the\nadvent of powerful text-conditioned generative models. However, placing objects\nin an environment with a precise location and orientation still remains a\nchallenge, as this typically requires carefully crafted inpainting masks or\nprompts. In this work, we show that a carefully designed visual map, combined\nwith coarse object masks, is sufficient for high quality object placement. We\ndesign a conditioning signal that resolves ambiguities, while being flexible\nenough to allow for changing of shapes or object orientations. By building on\nan inpainting model, we leave the background intact by design, in contrast to\nmethods that model objects and background jointly. We demonstrate the\neffectiveness of our method in the automotive setting, where we compare\ndifferent conditioning signals in novel object placement tasks. These tasks are\ndesigned to measure edit quality not only in terms of appearance, but also in\nterms of pose and location accuracy, including cases that require non-trivial\nshape changes. Lastly, we show that fine location control can be combined with\nappearance control to place existing objects in precise locations in a scene.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21446v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21446v1", "AI": {"title_translation": "基于场景感知扩散模型的可控三维物体放置", "tldr": "本文提出了一种利用视觉地图和粗略物体掩码结合修复扩散模型，实现图像中精确、灵活的3D物体放置的方法，并在汽车环境中展示了其有效性。", "motivation": "现有的图像编辑方法在精确的3D物体放置方面存在挑战，因为这通常需要精心制作的修复掩码或提示，而且这些方法经常同时建模物体和背景，可能改变背景。", "method": "作者提出了一种方法，使用精心设计的视觉地图结合粗略的物体掩码作为修复扩散模型的条件信号。这种信号能够解决歧义，并允许灵活地改变物体的形状或方向。该设计确保了背景保持不变。", "result": "该方法在汽车环境中得到了有效验证，通过比较不同条件信号在新型物体放置任务中的表现。评估的编辑质量不仅包括外观，还包括姿态和位置精度，甚至涵盖了需要非平凡形状变化的情况。此外，还展示了精细的位置控制可以与外观控制相结合，用于将现有物体精确放置在场景中。", "conclusion": "该研究展示了其方法能够实现可控且精确的3D物体放置，同时保持背景不变，并且能够结合精细的位置和外观控制。", "translation": "随着强大的文本条件生成模型的出现，图像编辑方法变得更加强大和灵活。然而，以精确的位置和方向在环境中放置物体仍然是一个挑战，因为这通常需要精心制作的修复掩码或提示。在这项工作中，我们表明精心设计的视觉地图与粗略的物体掩码相结合，足以实现高质量的物体放置。我们设计了一种条件信号，它能解决歧义，同时足够灵活，允许改变形状或物体方向。通过建立在修复模型的基础上，我们有意保持背景不变，这与联合建模物体和背景的方法形成对比。我们在汽车环境中展示了我们方法的有效性，在新的物体放置任务中比较了不同的条件信号。这些任务旨在不仅从外观方面，而且从姿态和位置精度方面衡量编辑质量，包括需要非平凡形状变化的情况。最后，我们展示了精细的位置控制可以与外观控制相结合，以将现有物体精确放置在场景中的特定位置。", "summary": "本文提出了一种利用场景感知扩散模型实现图像中可控三维物体放置的新方法。它通过引入基于视觉地图和粗略物体掩码的条件信号来解决精确物体定位的挑战，该信号在形状和方向上提供了灵活性，同时保留了背景。该方法基于修复模型构建，并在汽车环境中进行了演示，显示出高质量的物体放置效果，具有准确的姿态和位置，并能够将精细的位置控制与外观控制相结合。", "keywords": "3D物体放置, 扩散模型, 图像编辑, 场景感知, 修复", "comments": "本文的创新之处在于利用“精心设计的视觉地图”和“粗略物体掩码”作为条件信号，实现了精确的3D物体放置，这比传统依赖复杂掩码或提示的方法有了显著改进。该方法专注于保持背景不变，并在实际汽车环境中进行验证，增加了其重要性。其处理非平凡形状变化以及结合位置与外观控制的能力，突显了其灵活性和实用性。"}}
{"id": "2506.20910", "title": "Faster Fixed-Point Methods for Multichain MDPs", "authors": ["Matthew Zurek", "Yudong Chen"], "summary": "We study value-iteration (VI) algorithms for solving general (a.k.a.\nmultichain) Markov decision processes (MDPs) under the average-reward\ncriterion, a fundamental but theoretically challenging setting. Beyond the\ndifficulties inherent to all average-reward problems posed by the lack of\ncontractivity and non-uniqueness of solutions to the Bellman operator, in the\nmultichain setting an optimal policy must solve the navigation subproblem of\nsteering towards the best connected component, in addition to optimizing\nlong-run performance within each component. We develop algorithms which better\nsolve this navigational subproblem in order to achieve faster convergence for\nmultichain MDPs, obtaining improved rates of convergence and sharper measures\nof complexity relative to prior work. Many key components of our results are of\npotential independent interest, including novel connections between\naverage-reward and discounted problems, optimal fixed-point methods for\ndiscounted VI which extend to general Banach spaces, new sublinear convergence\nrates for the discounted value error, and refined suboptimality decompositions\nfor multichain MDPs. Overall our results yield faster convergence rates for\ndiscounted and average-reward problems and expand the theoretical foundations\nof VI approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20910v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20910v1", "AI": {"title_translation": "多链MDPs的更快不动点方法", "tldr": "本文研究了多链马尔可夫决策过程（MDPs）在平均奖励准则下的值迭代（VI）算法，并开发了能更快收敛的新算法，提高了收敛速度并扩展了VI方法的理论基础。", "motivation": "研究平均奖励准则下的多链马尔可夫决策过程（MDPs）是一个基础但理论上具有挑战性的问题。由于贝尔曼算子缺乏收缩性和解的非唯一性，以及多链设置中需要解决导航子问题，导致现有方法存在收敛速度慢等困难。", "method": "开发了新的算法，这些算法更好地解决了多链MDPs中的导航子问题，以实现更快的收敛速度。方法包括平均奖励问题与折扣问题之间的新颖联系，以及适用于一般巴纳赫空间的折扣VI最优不动点方法。", "result": "获得了相对于现有工作更高的收敛速度和更精确的复杂性度量。具体成果包括：平均奖励问题与折扣问题之间的新颖联系，适用于一般巴纳赫空间的折扣VI最优不动点方法，折扣值误差的新次线性收敛率，以及多链MDPs的精细次优分解。", "conclusion": "本研究的结果为折扣和平均奖励问题带来了更快的收敛速度，并扩展了值迭代（VI）方法的理论基础。", "translation": "我们研究了在平均奖励准则下解决通用（又称多链）马尔可夫决策过程（MDPs）的值迭代（VI）算法，这是一个基础但理论上具有挑战性的设置。除了贝尔曼算子缺乏收缩性和解的非唯一性给所有平均奖励问题带来的固有困难之外，在多链设置中，最优策略除了优化每个组件内的长期性能外，还必须解决导航子问题，即转向最佳连接组件。我们开发了能够更好解决这一导航子问题的算法，以实现多链MDPs的更快收敛，从而相对于先前工作获得了更高的收敛速度和更精确的复杂性度量。我们结果中的许多关键组件都具有潜在的独立兴趣，包括平均奖励问题与折扣问题之间的新颖联系，适用于一般巴纳赫空间的折扣VI最优不动点方法，折扣值误差的新次线性收敛率，以及多链MDPs的精细次优分解。总的来说，我们的结果为折扣和平均奖励问题带来了更快的收敛速度，并扩展了VI方法的理论基础。", "summary": "本文研究了平均奖励准则下多链马尔可夫决策过程（MDPs）的值迭代（VI）算法。针对该设置中贝尔曼算子缺乏收缩性、解非唯一性以及导航子问题带来的挑战，作者开发了新的算法，能够更好地解决导航子问题，从而实现更快的收敛。研究成果包括提高了收敛速度和复杂度度量，发现了平均奖励问题与折扣问题之间的新联系，提出了适用于一般巴纳赫空间的折扣VI最优不动点方法，得到了折扣值误差的新次线性收敛率，并对多链MDPs进行了精细的次优分解。总体而言，这些结果加快了折扣和平均奖励问题的收敛速度，并扩展了VI方法的理论基础。", "keywords": "多链MDPs, 值迭代, 平均奖励, 不动点方法, 收敛速度", "comments": "本文在解决多链马尔可夫决策过程（MDPs）的平均奖励问题方面取得了显著进展，通过开发新的算法有效提升了值迭代（VI）的收敛速度。其创新性在于提出了一种更好地解决导航子问题的方法，并建立了平均奖励问题与折扣问题之间的新颖联系，这对于理论研究和实际应用都具有重要意义。此外，将最优不动点方法扩展到一般巴纳赫空间，以及发现新的次线性收敛率和精细的次优分解，都进一步丰富了VI方法的理论基础。这些贡献有望为解决更复杂的决策问题提供更高效的工具。"}}
{"id": "2506.21451", "title": "A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario", "authors": ["Cyrus Addy", "Ajay Kumar Gurumadaiah", "Yixiang Gao", "Kwame Awuah-Offei"], "summary": "Underground mining operations face significant safety challenges that make\nemergency response capabilities crucial. While robots have shown promise in\nassisting with search and rescue operations, their effectiveness depends on\nreliable miner detection capabilities. Deep learning algorithms offer potential\nsolutions for automated miner detection, but require comprehensive training\ndatasets, which are currently lacking for underground mining environments. This\npaper presents a novel thermal imaging dataset specifically designed to enable\nthe development and validation of miner detection systems for potential\nemergency applications. We systematically captured thermal imagery of various\nmining activities and scenarios to create a robust foundation for detection\nalgorithms. To establish baseline performance metrics, we evaluated several\nstate-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,\nand RT-DETR on our dataset. While not exhaustive of all possible emergency\nsituations, this dataset serves as a crucial first step toward developing\nreliable thermal-based miner detection systems that could eventually be\ndeployed in real emergency scenarios. This work demonstrates the feasibility of\nusing thermal imaging for miner detection and establishes a foundation for\nfuture research in this critical safety application.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21451v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21451v1", "AI": {"title_translation": "针对多样化场景的井下矿工检测综合数据集", "tldr": "本文提出了一个用于井下矿工检测的综合热成像数据集，并评估了SOTA目标检测算法，以解决现有训练数据不足的问题，为紧急情况下的矿工检测系统奠定基础。", "motivation": "井下采矿作业面临重大安全挑战，应急响应能力至关重要。虽然机器人有望协助搜救，但其有效性取决于可靠的矿工检测能力。深度学习算法提供了自动化矿工检测的潜在解决方案，但目前缺乏用于井下采矿环境的全面训练数据集。", "method": "本文提出了一个新颖的热成像数据集，专门用于开发和验证矿工检测系统，以应对潜在的紧急应用。研究人员系统地捕获了各种采矿活动和场景的热图像，以建立检测算法的坚实基础。为了建立基线性能指标，研究人员评估了包括YOLOv8、YOLOv10、YOLO11和RT-DETR在内的几种最先进的目标检测算法在该数据集上的表现。", "result": "该工作证明了使用热成像进行矿工检测的可行性。该数据集是开发可靠的基于热成像的矿工检测系统的关键第一步。评估了多个SOTA目标检测算法并建立了基线性能指标。", "conclusion": "本文证明了使用热成像进行矿工检测的可行性，并为未来在此关键安全应用领域的研究奠定了基础，旨在最终部署到真实的紧急场景中。", "translation": "井下采矿作业面临重大的安全挑战，这使得应急响应能力至关重要。虽然机器人在协助搜救行动中显示出前景，但其有效性取决于可靠的矿工检测能力。深度学习算法为自动化矿工检测提供了潜在的解决方案，但目前缺乏用于井下采矿环境的全面训练数据集。本文提出了一个新颖的热成像数据集，专门用于开发和验证矿工检测系统，以应对潜在的紧急应用。我们系统地捕获了各种采矿活动和场景的热图像，以建立检测算法的坚实基础。为了建立基线性能指标，我们评估了包括YOLOv8、YOLOv10、YOLO11和RT-DETR在内的几种最先进的目标检测算法在该数据集上的表现。虽然未能涵盖所有可能的紧急情况，但该数据集是开发可靠的基于热成像的矿工检测系统的关键第一步，这些系统最终可能部署在真实的紧急场景中。这项工作证明了使用热成像进行矿工检测的可行性，并为未来在此关键安全应用领域的研究奠定了基础。", "summary": "本文针对井下矿工检测领域缺乏综合训练数据集的问题，提出了一个新颖的热成像数据集。该数据集通过系统捕获多样化的采矿活动和场景图像构建，旨在支持开发和验证用于紧急情况的矿工检测系统。研究人员利用该数据集评估了包括YOLOv8、YOLOv10、YOLO11和RT-DETR在内的先进目标检测算法，并建立了基线性能。这项工作证明了热成像在矿工检测中的可行性，并为未来在该关键安全应用领域的研究奠定了基础。", "keywords": "井下矿工检测, 热成像, 数据集, 目标检测, 采矿安全", "comments": "这篇论文通过构建一个专门的热成像数据集，解决了井下矿工检测领域数据稀缺的关键问题，具有重要的实际应用价值。其创新点在于专注于热成像数据，这对于光照不足或烟雾弥漫的井下环境尤为重要。通过评估多种SOTA模型，为后续研究提供了有价值的基线。该数据集的发布将极大地推动井下安全和应急响应技术的发展。"}}
{"id": "2506.21452", "title": "Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency", "authors": ["Kaiyu Song", "Hanjiang Lai"], "summary": "Classifier-free guidance (CFG) succeeds in condition diffusion models that\nuse a guidance scale to balance the influence of conditional and unconditional\nterms. A high guidance scale is used to enhance the performance of the\nconditional term. However, the high guidance scale often results in\noversaturation and unrealistic artifacts. In this paper, we introduce a new\nperspective based on low-frequency signals, identifying the accumulation of\nredundant information in these signals as the key factor behind oversaturation\nand unrealistic artifacts. Building on this insight, we propose low-frequency\nimproved classifier-free guidance (LF-CFG) to mitigate these issues.\nSpecifically, we introduce an adaptive threshold-based measurement to pinpoint\nthe locations of redundant information. We determine a reasonable threshold by\nanalyzing the change rate of low-frequency information between prior and\ncurrent steps. We then apply a down-weight strategy to reduce the impact of\nredundant information in the low-frequency signals. Experimental results\ndemonstrate that LF-CFG effectively alleviates oversaturation and unrealistic\nartifacts across various diffusion models, including Stable Diffusion-XL,\nStable Diffusion 2.1, 3.0, 3.5, and SiT-XL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21452v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21452v1", "AI": {"title_translation": "通过低频重新思考无分类器引导中的过饱和现象", "tldr": "本文提出了一种名为LF-CFG的新方法，通过识别并减少低频信号中冗余信息的影响，有效缓解了无分类器引导（CFG）在高引导尺度下导致的过饱和和不真实伪影问题。", "motivation": "无分类器引导（CFG）在高引导尺度下能够增强条件项的性能，但常常导致图像过饱和和不真实伪影。本文旨在解决这一问题。", "method": "本文提出了一种基于低频信号的新视角，认为低频信号中冗余信息的积累是导致过饱和和不真实伪影的关键因素。在此基础上，提出低频改进无分类器引导（LF-CFG）方法。具体而言，该方法引入了一种基于自适应阈值的测量方法来定位冗余信息，通过分析先前和当前步骤之间低频信息的变化率来确定合理的阈值，然后应用降权策略来减少低频信号中冗余信息的影响。", "result": "实验结果表明，LF-CFG能够有效缓解各种扩散模型（包括Stable Diffusion-XL, Stable Diffusion 2.1, 3.0, 3.5, 和SiT-XL）中的过饱和和不真实伪影问题。", "conclusion": "通过关注低频信号中的冗余信息并对其进行降权处理，LF-CFG成功解决了无分类器引导在高引导尺度下产生的过饱和和不真实伪影，提升了条件扩散模型的图像生成质量。", "translation": "无分类器引导（CFG）成功地应用于条件扩散模型，它使用引导尺度来平衡条件项和无条件项的影响。高引导尺度用于增强条件项的性能。然而，高引导尺度常常导致过饱和和不真实伪影。在本文中，我们引入了一种基于低频信号的新视角，将这些信号中冗余信息的积累识别为导致过饱和和不真实伪影的关键因素。基于这一见解，我们提出了低频改进无分类器引导（LF-CFG）来缓解这些问题。具体而言，我们引入了一种基于自适应阈值的测量方法来精确定位冗余信息的位置。我们通过分析先前和当前步骤之间低频信息的变化率来确定合理的阈值。然后，我们应用降权策略来减少低频信号中冗余信息的影响。实验结果表明，LF-CFG能够有效缓解各种扩散模型（包括Stable Diffusion-XL、Stable Diffusion 2.1、3.0、3.5和SiT-XL）中的过饱和和不真实伪影问题。", "summary": "本文针对无分类器引导（CFG）在高引导尺度下易导致图像过饱和和不真实伪影的问题，提出了一种新的解释：低频信号中冗余信息的积累是根本原因。基于此，论文引入了低频改进无分类器引导（LF-CFG）方法。LF-CFG通过自适应阈值测量定位冗余信息，并根据低频信息的变化率确定阈值，随后采用降权策略减少冗余信息的影响。实验证明，LF-CFG在多种扩散模型上有效改善了过饱和及伪影问题。", "keywords": "无分类器引导, 扩散模型, 过饱和, 低频信号, 图像生成", "comments": "本文的创新点在于从低频信号的角度重新审视了无分类器引导中的过饱和问题，并提出了一个基于信息冗余的新颖解释。通过针对性地处理低频信号中的冗余信息，LF-CFG提供了一种有效的解决方案，具有良好的普适性，对提升扩散模型生成图像的质量具有重要意义。"}}
{"id": "2506.20928", "title": "Active Learning for Manifold Gaussian Process Regression", "authors": ["Yuanxing Cheng", "Lulu Kang", "Yiwei Wang", "Chun Liu"], "summary": "This paper introduces an active learning framework for manifold Gaussian\nProcess (GP) regression, combining manifold learning with strategic data\nselection to improve accuracy in high-dimensional spaces. Our method jointly\noptimizes a neural network for dimensionality reduction and a Gaussian process\nregressor in the latent space, supervised by an active learning criterion that\nminimizes global prediction error. Experiments on synthetic data demonstrate\nsuperior performance over randomly sequential learning. The framework\nefficiently handles complex, discontinuous functions while preserving\ncomputational tractability, offering practical value for scientific and\nengineering applications. Future work will focus on scalability and\nuncertainty-aware manifold learning.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20928v1", "categories": ["stat.ML", "cs.LG", "62", "G.3"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20928v1", "AI": {"title_translation": "流形高斯过程回归的主动学习", "tldr": "提出一种结合流形学习和主动学习的框架，用于高维空间中的高斯过程回归，能有效提高精度并处理复杂函数。", "motivation": "在处理高维数据时，通过结合流形学习和策略性数据选择来提高高斯过程回归的预测准确性。", "method": "本文引入了一个用于流形高斯过程（GP）回归的主动学习框架。该方法联合优化一个用于降维的神经网络和一个在潜在空间中的高斯过程回归器，并由一个最小化全局预测误差的主动学习准则进行监督。", "result": "在合成数据上的实验表明，该方法在性能上优于随机序贯学习。该框架能够高效处理复杂、不连续的函数，同时保持计算上的可行性。", "conclusion": "该主动学习框架在流形高斯过程回归中表现出色，能有效提高高维空间中的预测精度并处理复杂函数，具有实际应用价值。", "translation": "本文介绍了一种用于流形高斯过程（GP）回归的主动学习框架，该框架结合了流形学习和策略性数据选择，以提高高维空间中的精度。我们的方法联合优化了一个用于降维的神经网络和一个在潜在空间中的高斯过程回归器，并由一个最小化全局预测误差的主动学习准则进行监督。在合成数据上的实验表明，其性能优于随机序贯学习。该框架能够高效处理复杂、不连续的函数，同时保持计算上的可行性，为科学和工程应用提供了实际价值。未来的工作将侧重于可扩展性和不确定性感知流形学习。", "summary": "本文提出了一种针对流形高斯过程回归的主动学习框架，旨在通过结合流形学习和智能数据选择来提升高维数据的预测精度。该框架通过一个主动学习准则，协同优化神经网络进行降维和高斯过程回归器在潜在空间中的表现。实验证明，该方法在处理复杂、不连续函数方面优于随机学习，并保持了计算效率，具有广泛的实际应用潜力。", "keywords": "主动学习, 流形高斯过程回归, 降维, 高维空间, 数据选择", "comments": "该论文的创新点在于将主动学习与流形高斯过程回归相结合，通过联合优化神经网络降维和GP回归器，有效提升了高维空间中的预测精度。其处理复杂、不连续函数的能力以及计算可行性是其重要价值所在。未来的可扩展性和不确定性感知学习将是进一步提升该框架实用性的关键。"}}
{"id": "2506.21469", "title": "Evaluation of Traffic Signals for Daily Traffic Pattern", "authors": ["Mohammad Shokrolah Shirazi", "Hung-Fu Chang"], "summary": "The turning movement count data is crucial for traffic signal design,\nintersection geometry planning, traffic flow, and congestion analysis. This\nwork proposes three methods called dynamic, static, and hybrid configuration\nfor TMC-based traffic signals. A vision-based tracking system is developed to\nestimate the TMC of six intersections in Las Vegas using traffic cameras. The\nintersection design, route (e.g. vehicle movement directions), and signal\nconfiguration files with compatible formats are synthesized and imported into\nSimulation of Urban MObility for signal evaluation with realistic data. The\ninitial experimental results based on estimated waiting times indicate that the\ncycle time of 90 and 120 seconds works best for all intersections. In addition,\nfour intersections show better performance for dynamic signal timing\nconfiguration, and the other two with lower performance have a lower ratio of\ntotal vehicle count to total lanes of the intersection leg. Since daily traffic\nflow often exhibits a bimodal pattern, we propose a hybrid signal method that\nswitches between dynamic and static methods, adapting to peak and off-peak\ntraffic conditions for improved flow management. So, a built-in traffic\ngenerator module creates vehicle routes for 4 hours, including peak hours, and\na signal design module produces signal schedule cycles according to static,\ndynamic, and hybrid methods. Vehicle count distributions are weighted\ndifferently for each zone (i.e., West, North, East, South) to generate diverse\ntraffic patterns. The extended experimental results for 6 intersections with 4\nhours of simulation time imply that zone-based traffic pattern distributions\naffect signal design selection. Although the static method works great for\nevenly zone-based traffic distribution, the hybrid method works well for highly\nweighted traffic at intersection pairs of the West-East and North-South zones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21469v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21469v1", "AI": {"title_translation": "日常交通模式下交通信号灯的评估", "tldr": "该研究评估了动态、静态和混合交通信号控制方法，利用转向流量计数（TMC）数据进行模拟，发现混合方法在双峰交通模式和加权交通中表现良好。", "motivation": "转向流量计数（TMC）数据对于交通信号设计、交叉口几何规划、交通流和拥堵分析至关重要。日常交通流常呈现双峰模式，因此需要更有效的信号管理方法。", "method": "该研究提出了三种基于TMC的交通信号配置方法：动态、静态和混合。开发了一个基于视觉的跟踪系统，利用交通摄像头估算拉斯维加斯六个交叉口的TMC数据。将交叉口设计、路线和信号配置文件合成为兼容格式并导入到城市交通模拟软件（SUMO）中，使用真实数据进行信号评估。提出了一种混合信号方法，可在动态和静态方法之间切换，以适应高峰和非高峰交通状况。内置的交通生成模块创建了包含高峰时段在内的4小时车辆路线，信号设计模块根据静态、动态和混合方法生成信号调度周期。对每个区域（即西、北、东、南）的车辆计数分布进行不同加权，以生成多样化的交通模式。", "result": "初步实验结果表明，90秒和120秒的周期时间对所有交叉口效果最佳（基于估计的等待时间）。四个交叉口在动态信号配时配置下表现更好，而另外两个性能较低的交叉口其总车辆数与交叉口支路总车道数的比率较低。扩展实验结果表明，基于区域的交通模式分布会影响信号设计选择。静态方法适用于均匀的基于区域的交通分布，而混合方法对于西-东和北-南区域交叉口对的高加权交通表现良好。", "conclusion": "交通信号方法的选择（静态、动态、混合）取决于具体的交通模式分布，其中混合方法对于双峰模式和不均匀分布的交通是有效的。", "translation": "转向流量计数数据对于交通信号设计、交叉口几何规划、交通流和拥堵分析至关重要。本研究提出了三种基于TMC的交通信号灯方法，分别称为动态、静态和混合配置。开发了一个基于视觉的跟踪系统，利用交通摄像头估算拉斯维加斯六个交叉口的TMC。将交叉口设计、路线（例如车辆移动方向）和兼容格式的信号配置文件合成并导入城市交通模拟软件（SUMO）中，用于真实数据下的信号评估。基于估计等待时间的初步实验结果表明，90秒和120秒的周期时间对所有交叉口效果最佳。此外，四个交叉口在动态信号配时配置下表现更好，而另外两个性能较低的交叉口其总车辆数与交叉口总车道数的比率较低。由于日常交通流通常呈现双峰模式，我们提出了一种混合信号方法，可在动态和静态方法之间切换，以适应高峰和非高峰交通状况，从而改善交通流管理。因此，一个内置的交通生成模块创建了4小时的车辆路线，包括高峰时段，一个信号设计模块根据静态、动态和混合方法生成信号调度周期。对每个区域（即西、北、东、南）的车辆计数分布进行不同加权，以生成多样化的交通模式。对六个交叉口进行4小时模拟的扩展实验结果表明，基于区域的交通模式分布会影响信号设计选择。尽管静态方法对于均匀的基于区域的交通分布效果很好，但混合方法对于西-东和北-南区域交叉口对的高加权交通效果良好。", "summary": "本文评估了静态、动态和混合交通信号控制方法，这些方法基于转向流量计数（TMC）数据。研究开发了一个基于视觉的系统，用于估算拉斯维加斯六个交叉口的TMC，并在SUMO中进行模拟验证。针对日常交通流的双峰特性，本文提出了一种混合信号方法，该方法能够在高峰和非高峰时段动态切换静态和动态控制策略。实验结果表明，90至120秒的周期时间效果最佳，并且虽然静态方法适用于均匀交通分布，但混合方法在处理高加权交通模式时表现更优，强调了基于区域的交通分布在信号设计选择中的重要性。", "keywords": "交通信号, 转向流量计数, 混合方法, 交通模拟, 日常交通模式", "comments": "该论文引入了一种实用的基于视觉的TMC估算系统，具有实际应用价值。提出的混合信号方法能够适应日常交通的双峰模式，在灵活性和潜在效率方面优于纯静态或纯动态方法，具有创新性。利用SUMO进行仿真验证，增加了研究结果的可信度。"}}
{"id": "2506.20930", "title": "Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "Ya-Chuan Chen"], "summary": "We propose a hybrid quantum-classical reinforcement learning framework for\nsector rotation in the Taiwan stock market. Our system employs Proximal Policy\nOptimization (PPO) as the backbone algorithm and integrates both classical\narchitectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,\nQASA) as policy and value networks. An automated feature engineering pipeline\nextracts financial indicators from capital share data to ensure consistent\nmodel input across all configurations. Empirical backtesting reveals a key\nfinding: although quantum-enhanced models consistently achieve higher training\nrewards, they underperform classical models in real-world investment metrics\nsuch as cumulative return and Sharpe ratio. This discrepancy highlights a core\nchallenge in applying reinforcement learning to financial domains -- namely,\nthe mismatch between proxy reward signals and true investment objectives. Our\nanalysis suggests that current reward designs may incentivize overfitting to\nshort-term volatility rather than optimizing risk-adjusted returns. This issue\nis compounded by the inherent expressiveness and optimization instability of\nquantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We\ndiscuss the implications of this reward-performance gap and propose directions\nfor future improvement, including reward shaping, model regularization, and\nvalidation-based early stopping. Our work offers a reproducible benchmark and\ncritical insights into the practical challenges of deploying quantum\nreinforcement learning in real-world finance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20930v1", "categories": ["quant-ph", "cs.LG", "q-fin.CP"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.20930v1", "AI": {"title_translation": "量子强化学习交易代理在台湾股市的行业轮动应用", "tldr": "本文提出了一个用于台湾股市行业轮动的混合量子-经典强化学习框架，发现量子增强模型在训练奖励上表现更好，但在实际投资指标（如累积回报和夏普比率）上不如经典模型，这揭示了强化学习在金融领域中代理奖励信号与真实投资目标不匹配的核心挑战。", "motivation": "探索量子强化学习在金融领域的应用，特别是台湾股市的行业轮动，并评估其性能。解决强化学习应用于金融领域时代理奖励信号与真实投资目标不匹配的问题。", "method": "提出一个混合量子-经典强化学习框架，以PPO为核心算法。结合经典架构（LSTM, Transformer）和量子增强模型（QNN, QRWKV, QASA）作为策略和价值网络。使用自动化特征工程从资本份额数据中提取金融指标。", "result": "经验回测显示，量子增强模型在训练奖励上始终更高，但在累积回报和夏普比率等实际投资指标上表现不如经典模型。这种差异突显了强化学习应用于金融领域时代理奖励信号与真实投资目标不匹配的核心挑战。分析表明，当前的奖励设计可能激励对短期波动过拟合，而非优化风险调整后的回报。", "conclusion": "量子增强模型在金融强化学习中面临奖励与性能差距的挑战，主要原因是代理奖励信号与真实投资目标不匹配，以及NISQ约束下量子电路的固有表达性和优化不稳定性。文章为部署量子强化学习提供了可复现的基准和关键见解，并提出了未来的改进方向。", "translation": "我们提出了一个用于台湾股市行业轮动的混合量子-经典强化学习框架。我们的系统采用近端策略优化（PPO）作为核心算法，并集成了经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）作为策略和价值网络。自动化特征工程管道从资本份额数据中提取金融指标，以确保所有配置的模型输入一致。经验回测揭示了一个关键发现：尽管量子增强模型始终实现更高的训练奖励，但在累积回报和夏普比率等实际投资指标上，它们的表现不如经典模型。这种差异突显了将强化学习应用于金融领域的核心挑战——即代理奖励信号与真实投资目标之间的不匹配。我们的分析表明，当前的奖励设计可能激励对短期波动过拟合，而不是优化风险调整后的回报。这个问题因噪声中等规模量子（NISQ）约束下量子电路固有的表达性和优化不稳定性而加剧。我们讨论了这种奖励-性能差距的影响，并提出了未来改进的方向，包括奖励塑形、模型正则化和基于验证的早期停止。我们的工作提供了一个可复现的基准，并为在实际金融中部署量子强化学习的实际挑战提供了关键见解。", "summary": "本文提出了一个用于台湾股市行业轮动的混合量子-经典强化学习框架。该框架以PPO为基础，结合了经典（LSTM, Transformer）和量子增强（QNN, QRWKV, QASA）模型。研究发现，尽管量子模型在训练奖励上表现优异，但在实际投资指标（如回报和夏普比率）上却不及经典模型。这揭示了强化学习在金融应用中代理奖励与真实目标不匹配的挑战，并指出当前的奖励设计可能导致过拟合短期波动。文章讨论了这种性能差距，并提出了改进策略，为量子强化学习在金融领域的实际部署提供了重要见解和可复现的基准。", "keywords": "量子强化学习, 行业轮动, 台湾股市, 金融应用, 奖励不匹配", "comments": "本文创新性地将量子强化学习应用于股票市场行业轮动，并提供了量子增强模型与经典模型在实际金融应用中的对比基准。其重要性在于揭示了量子强化学习在金融领域应用时面临的核心挑战，即代理奖励与真实投资目标之间的不匹配问题，以及NISQ量子电路的局限性。研究指出了当前奖励设计可能导致过拟合短期波动的问题，为未来该领域的研究提供了明确的改进方向，如奖励塑形和正则化，具有重要的实践指导意义。"}}
{"id": "2506.20933", "title": "Lower Bounds on the Size of Markov Equivalence Classes", "authors": ["Erik Jahn", "Frederick Eberhardt", "Leonard J. Schulman"], "summary": "Causal discovery algorithms typically recover causal graphs only up to their\nMarkov equivalence classes unless additional parametric assumptions are made.\nThe sizes of these equivalence classes reflect the limits of what can be\nlearned about the underlying causal graph from purely observational data. Under\nthe assumptions of acyclicity, causal sufficiency, and a uniform model prior,\nMarkov equivalence classes are known to be small on average. In this paper, we\nshow that this is no longer the case when any of these assumptions is relaxed.\nSpecifically, we prove exponentially large lower bounds for the expected size\nof Markov equivalence classes in three settings: sparse random directed acyclic\ngraphs, uniformly random acyclic directed mixed graphs, and uniformly random\ndirected cyclic graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20933v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20933v1", "AI": {"title_translation": "马尔可夫等价类大小的下界", "tldr": "本文证明，当放宽无环性、因果充分性或均匀模型先验假设时，马尔可夫等价类的预期大小会呈指数级增长。", "motivation": "因果发现算法通常只能恢复到马尔可夫等价类，这些等价类的大小反映了从纯观测数据中学习底层因果图的局限性。现有研究发现，在特定假设下，这些等价类平均较小，但本文旨在探索放宽这些假设时的情况。", "method": "通过理论证明，在三种特定设置下（稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图），计算马尔可夫等价类预期大小的指数级下界。", "result": "证明了当放宽无环性、因果充分性或均匀模型先验中的任何一个假设时，马尔可夫等价类的预期大小不再是平均较小的，而是可以达到指数级下界。具体在稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图这三种图模型中得到了验证。", "conclusion": "放宽因果发现中的标准假设（无环性、因果充分性、均匀模型先验）会导致马尔可夫等价类的大小显著增加，这表明在这些更一般的设置下，从观测数据中进行因果推断的难度更大。", "translation": "因果发现算法通常只能恢复到它们的马尔可夫等价类，除非做出额外的参数假设。这些等价类的大小反映了仅从观测数据中可以学到关于底层因果图的局限性。在无环性、因果充分性和均匀模型先验的假设下，已知马尔可夫等价类平均较小。在本文中，我们表明当这些假设中的任何一个被放宽时，情况就不再如此。具体来说，我们证明了在三种设置下，马尔可夫等价类预期大小的指数级大下界：稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图。", "summary": "本文探讨了在放宽标准假设（无环性、因果充分性、均匀模型先验）时，因果发现中马尔可夫等价类大小的变化。研究发现，不同于在标准假设下平均较小的情况，当任何一个假设被放宽时，马尔可夫等价类的预期大小会呈指数级增长，并在稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图这三种特定设置中得到了指数级下界的证明。这揭示了在更复杂的因果模型中，从观测数据中识别因果结构所面临的挑战。", "keywords": "马尔可夫等价类, 因果发现, 下界, 有向无环图, 有向循环图", "comments": "这篇论文对于理解因果发现算法的局限性具有重要意义。它挑战了在理想假设下马尔可夫等价类平均较小的传统观点，揭示了在更现实或更复杂的图模型中，因果识别的难度会显著增加。其结果对于指导因果推断实践和设计新的因果发现算法具有指导作用，特别是当无法满足某些严格假设时。"}}
{"id": "2506.21476", "title": "Global and Local Entailment Learning for Natural World Imagery", "authors": ["Srikumar Sastry", "Aayush Dhakal", "Eric Xing", "Subash Khanal", "Nathan Jacobs"], "summary": "Learning the hierarchical structure of data in vision-language models is a\nsignificant challenge. Previous works have attempted to address this challenge\nby employing entailment learning. However, these approaches fail to model the\ntransitive nature of entailment explicitly, which establishes the relationship\nbetween order and semantics within a representation space. In this work, we\nintroduce Radial Cross-Modal Embeddings (RCME), a framework that enables the\nexplicit modeling of transitivity-enforced entailment. Our proposed framework\noptimizes for the partial order of concepts within vision-language models. By\nleveraging our framework, we develop a hierarchical vision-language foundation\nmodel capable of representing the hierarchy in the Tree of Life. Our\nexperiments on hierarchical species classification and hierarchical retrieval\ntasks demonstrate the enhanced performance of our models compared to the\nexisting state-of-the-art models. Our code and models are open-sourced at\nhttps://vishu26.github.io/RCME/index.html.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21476v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21476v1", "AI": {"title_translation": "自然世界图像的全局和局部蕴涵学习", "tldr": "该论文引入了径向跨模态嵌入（RCME）框架，用于明确建模视觉-语言模型中数据的传递性蕴涵，以学习层次结构。实验证明，RCME在层次物种分类和层次检索任务中表现优于现有SOTA模型。", "motivation": "在视觉-语言模型中学习数据的层次结构是一个重大挑战。现有方法在处理蕴涵学习时，未能明确建模蕴涵的传递性，而这种传递性对于在表示空间中建立顺序和语义关系至关重要。", "method": "该论文提出了径向跨模态嵌入（RCME）框架，旨在明确建模传递性强制的蕴涵。该框架优化了视觉-语言模型中概念的部分顺序。通过RCME，研究人员开发了一个能够表示生命之树层次结构的层次视觉-语言基础模型。", "result": "在层次物种分类和层次检索任务上的实验表明，与现有最先进的模型相比，RCME模型的性能得到了增强。", "conclusion": "RCME框架通过明确建模传递性蕴涵，有效解决了视觉-语言模型中数据层次结构学习的挑战，并在相关任务中取得了优于现有SOTA模型的性能，证明了其在表示复杂层次结构方面的潜力。", "translation": "学习视觉-语言模型中数据的层次结构是一个重大挑战。以前的工作试图通过使用蕴涵学习来解决这个挑战。然而，这些方法未能明确地建模蕴涵的传递性，而传递性在表示空间中建立了顺序和语义之间的关系。在这项工作中，我们引入了径向跨模态嵌入（RCME），这是一个能够明确建模传递性强制蕴涵的框架。我们提出的框架优化了视觉-语言模型中概念的部分顺序。通过利用我们的框架，我们开发了一个能够表示生命之树中层次结构的层次视觉-语言基础模型。我们在层次物种分类和层次检索任务上的实验表明，与现有最先进的模型相比，我们的模型性能有所提高。我们的代码和模型已在 https://vishu26.github.io/RCME/index.html 开源。", "summary": "该论文提出了径向跨模态嵌入（RCME）框架，以解决视觉-语言模型中数据层次结构学习的挑战。RCME通过明确建模蕴涵的传递性，优化了概念的部分顺序，弥补了现有蕴涵学习方法未能处理传递性的不足。基于RCME，研究人员开发了一个层次视觉-语言基础模型，并成功应用于表示“生命之树”的层次结构。实验结果表明，RCME模型在层次物种分类和层次检索任务上均优于现有的最先进模型。", "keywords": "蕴涵学习, 层次结构, 视觉-语言模型, 传递性, 径向跨模态嵌入", "comments": "该论文的创新点在于明确建模了蕴涵的传递性，这对于准确表示视觉-语言模型中的层次结构至关重要。RCME框架的提出为处理复杂数据层次结构提供了一种有效的新方法。将该方法应用于“生命之树”的表示，展示了其在生物学等真实世界数据中的巨大应用潜力。此外，代码和模型的开源也促进了相关领域的研究进展。"}}
{"id": "2506.20935", "title": "Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics", "authors": ["Hsin-Hsiung Huang", "Hayden Hampton"], "summary": "Forecasting geopolitical conflict from data sources like the Global Database\nof Events, Language, and Tone (GDELT) is a critical challenge for national\nsecurity. The inherent sparsity, burstiness, and overdispersion of such data\ncause standard deep learning models, including the Temporal Fusion Transformer\n(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,\na hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)\ncompetition by overcoming these limitations. Designed to bridge this gap, our\nmodel employs a two-stage process: first, a TFT captures complex temporal\ndynamics to generate multi-quantile forecasts. These quantiles then serve as\ninformed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),\nwhich performs principled spatiotemporal smoothing and uncertainty\nquantification. In a case study forecasting conflict dynamics in the Middle\nEast and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,\nshowing a superior ability to predict the timing and magnitude of bursty event\nperiods, particularly at long-range horizons. This work offers a robust\nframework for generating more reliable and actionable intelligence from\nchallenging event data, with all code and workflows made publicly available to\nensure reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20935v1", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO", "37M10, 62M10, 62P25, 65Y20"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20935v1", "AI": {"title_translation": "地缘政治事件预测：基于稀疏时间融合转换器和高斯过程混合模型在中东和美国冲突动态中的案例研究", "tldr": "本文提出了一种名为STFT-VNNGP的混合架构，结合了稀疏时间融合转换器（TFT）和变分最近邻高斯过程（VNNGP），旨在解决地缘政治事件数据（如GDELT）在长期预测中存在的稀疏性、突发性和过度分散性问题。该模型在预测中东和美国冲突动态的案例研究中表现出色，尤其是在长期预测突发事件时期的时机和规模方面，优于独立的TFT。", "motivation": "从全球事件、语言和语调数据库（GDELT）等数据源预测地缘政治冲突对国家安全至关重要。然而，此类数据固有的稀疏性、突发性和过度分散性使得包括时间融合转换器（TFT）在内的标准深度学习模型难以产生可靠的长期预测，这构成了当前的挑战。", "method": "本文引入了STFT-VNNGP，这是一种混合架构，采用两阶段过程：首先，时间融合转换器（TFT）捕获复杂的时间动态以生成多分位数预测；其次，这些多分位数作为变分最近邻高斯过程（VNNGP）的有效输入，VNNGP执行原则性的时空平滑和不确定性量化。", "result": "在预测中东和美国冲突动态的案例研究中，STFT-VNNGP持续优于独立的TFT，在预测突发事件时期的时机和规模方面，尤其是在长期预测中表现出卓越的能力。该模型赢得了2023年威胁检测算法（ATD）竞赛。", "conclusion": "这项工作提供了一个强大的框架，可以从具有挑战性的地缘政治事件数据中生成更可靠、更具可操作性的情报。所有代码和工作流程均已公开，以确保可复现性。", "translation": "从全球事件、语言和语调数据库（GDELT）等数据源预测地缘政治冲突是国家安全面临的关键挑战。此类数据固有的稀疏性、突发性和过度分散性导致包括时间融合转换器（TFT）在内的标准深度学习模型产生不可靠的长期预测。我们引入了STFT-VNNGP，这是一种混合架构，通过克服这些局限性赢得了2023年威胁检测算法（ATD）竞赛。我们的模型旨在弥补这一差距，采用两阶段过程：首先，TFT捕获复杂的时间动态以生成多分位数预测。然后，这些分位数作为变分最近邻高斯过程（VNNGP）的有效输入，VNNGP执行原则性的时空平滑和不确定性量化。在中东和美国冲突动态预测的案例研究中，STFT-VNNGP持续优于独立的TFT，在预测突发事件时期的时机和规模方面，尤其是在长期预测中表现出卓越的能力。这项工作为从具有挑战性的事件数据中生成更可靠、更具可操作性的情报提供了一个强大的框架，所有代码和工作流程均已公开，以确保可复现性。", "summary": "本文提出了一种名为STFT-VNNGP的混合模型，它结合了时间融合转换器（TFT）和变分最近邻高斯过程（VNNGP），旨在解决地缘政治事件数据（如GDELT）在长期预测中面临的稀疏性、突发性和过度分散性挑战。该模型采用两阶段方法：TFT负责捕获时间动态并生成多分位数预测，VNNGP则利用这些预测进行时空平滑和不确定性量化。在预测中东和美国冲突动态的案例研究中，STFT-VNNGP在预测突发事件的时机和规模方面表现出卓越的长期预测能力，并持续优于独立的TFT，为生成更可靠和可操作的情报提供了鲁棒框架。", "keywords": "地缘政治事件预测, 时间融合转换器, 高斯过程, 冲突动态, GDELT数据", "comments": "这项研究通过结合深度学习模型（TFT）和概率模型（VNNGP）的优势，创新性地解决了地缘政治事件数据预测中的核心挑战，特别是在处理稀疏、突发和过度分散数据方面。其重要性在于为国家安全领域提供了更可靠的长期预测工具。该模型在ATD竞赛中获胜证明了其有效性，而代码和工作流程的公开性进一步增强了其研究价值和可复现性。"}}
{"id": "2506.21484", "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning", "authors": ["Tajamul Ashraf", "Janibul Bashir"], "summary": "We focus on the source-free domain adaptive object detection (SF-DAOD)\nproblem when source data is unavailable during adaptation and the model must\nadapt to an unlabeled target domain. The majority of approaches for the problem\nemploy a self-supervised approach using a student-teacher (ST) framework where\npseudo-labels are generated via a source-pretrained model for further\nfine-tuning. We observe that the performance of a student model often degrades\ndrastically, due to the collapse of the teacher model, primarily caused by high\nnoise in pseudo-labels, resulting from domain bias, discrepancies, and a\nsignificant domain shift across domains. To obtain reliable pseudo-labels, we\npropose a Target-based Iterative Query-Token Adversarial Network (TITAN), which\nseparates the target images into two subsets: those similar to the source\n(easy) and those dissimilar (hard). We propose a strategy to estimate variance\nto partition the target domain. This approach leverages the insight that higher\ndetection variances correspond to higher recall and greater similarity to the\nsource domain. Also, we incorporate query-token-based adversarial modules into\na student-teacher baseline framework to reduce the domain gaps between two\nfeature representations. Experiments conducted on four natural imaging datasets\nand two challenging medical datasets have substantiated the superior\nperformance of TITAN compared to existing state-of-the-art (SOTA)\nmethodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7\npercent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,\nrespectively.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21484v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21484v1", "AI": {"title_translation": "TITAN: 基于查询令牌的领域自适应对抗学习", "tldr": "针对源数据不可用的领域自适应目标检测问题，TITAN通过分离目标域的“简单”和“困难”子集，并引入基于查询令牌的对抗模块，显著提升了伪标签的可靠性和模型的性能。", "motivation": "在源数据不可用的领域自适应目标检测(SF-DAOD)中，现有的学生-教师(ST)框架常因伪标签的高噪声（源于领域偏差、差异和显著的领域偏移）导致教师模型崩溃，进而使学生模型性能大幅下降。", "method": "提出了一种基于目标的迭代查询令牌对抗网络(TITAN)。该方法将目标图像分为“简单”（与源域相似）和“困难”（与源域不相似）两个子集，并提出一种利用检测方差估计来划分目标域的策略，认为高检测方差对应高召回率和与源域的更高相似性。此外，将基于查询令牌的对抗模块整合到学生-教师基线框架中，以减少两种特征表示之间的领域差距。", "result": "在四个自然图像数据集和两个挑战性医学数据集上，TITAN的表现优于现有最先进的方法。在C2F、C2B、S2C和K2C基准测试中，mAP分别比当前SOTA提升了+22.7%、+22.2%、+21.1%和+3.7%。", "conclusion": "TITAN通过其创新的目标域划分策略和查询令牌对抗模块，有效解决了SF-DAOD中伪标签噪声问题，显著提升了模型在多种数据集上的性能，证明了其优越性。", "translation": "我们关注源数据在自适应过程中不可用且模型必须适应未标记目标域的无源领域自适应目标检测（SF-DAOD）问题。解决该问题的大多数方法采用学生-教师（ST）框架下的自监督方法，通过源预训练模型生成伪标签以进行进一步微调。我们观察到，由于教师模型的崩溃，学生模型的性能通常会急剧下降，这主要是由伪标签中的高噪声引起的，而高噪声又源于领域偏差、差异以及跨领域显著的领域偏移。为了获得可靠的伪标签，我们提出了一种基于目标的迭代查询令牌对抗网络（TITAN），它将目标图像分为两个子集：与源域相似的（简单）和不相似的（困难）。我们提出了一种估计方差来划分目标域的策略。这种方法利用了高检测方差对应高召回率和与源域更高相似性的见解。此外，我们将基于查询令牌的对抗模块整合到学生-教师基线框架中，以减少两种特征表示之间的领域差距。在四个自然图像数据集和两个具有挑战性的医学数据集上进行的实验证实了TITAN优于现有最先进（SOTA）方法。我们报告在C2F、C2B、S2C和K2C基准测试中，mAP分别比当前SOTA提高了+22.7、+22.2、+21.1和+3.7个百分点。", "summary": "本文针对源数据不可用的领域自适应目标检测(SF-DAOD)中伪标签噪声导致的学生模型性能下降问题，提出了一种名为TITAN的新方法。TITAN通过将目标域图像划分为“简单”和“困难”子集，并利用检测方差进行划分，以生成更可靠的伪标签。同时，它将基于查询令牌的对抗模块融入学生-教师框架，以缩小领域差距。实验结果表明，TITAN在多个自然和医学数据集上均显著超越了现有最先进的方法。", "keywords": "领域自适应, 目标检测, 对抗学习, 伪标签, 源数据不可用", "comments": "TITAN的创新点在于其目标域的“简单”和“困难”子集划分策略，以及利用检测方差来估计与源域的相似性，这有效缓解了伪标签噪声问题。此外，引入查询令牌对抗模块进一步缩小了领域差距。这项工作在无源域自适应目标检测领域取得了显著进展，特别是在提升伪标签可靠性方面具有重要意义。其在多个数据集上大幅提升性能，证明了方法的有效性和普适性。"}}
{"id": "2506.21486", "title": "Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection", "authors": ["Tobias J. Riedlinger", "Kira Maag", "Hanno Gottschalk"], "summary": "Deep neural networks have set the state-of-the-art in computer vision tasks\nsuch as bounding box detection and semantic segmentation. Object detectors and\nsegmentation models assign confidence scores to predictions, reflecting the\nmodel's uncertainty in object detection or pixel-wise classification. However,\nthese confidence estimates are often miscalibrated, as their architectures and\nloss functions are tailored to task performance rather than probabilistic\nfoundation. Even with well calibrated predictions, object detectors fail to\nquantify uncertainty outside detected bounding boxes, i.e., the model does not\nmake a probability assessment of whether an area without detected objects is\ntruly free of obstacles. This poses a safety risk in applications such as\nautomated driving, where uncertainty in empty areas remains unexplored. In this\nwork, we propose an object detection model grounded in spatial statistics.\nBounding box data matches realizations of a marked point process, commonly used\nto describe the probabilistic occurrence of spatial point events identified as\nbounding box centers, where marks are used to describe the spatial extension of\nbounding boxes and classes. Our statistical framework enables a\nlikelihood-based training and provides well-defined confidence estimates for\nwhether a region is drivable, i.e., free of objects. We demonstrate the\neffectiveness of our method through calibration assessments and evaluation of\nperformance.", "comment": "15 pages, 4 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21486v1", "categories": ["cs.CV", "cs.LG", "math.PR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21486v1", "AI": {"title_translation": "迈向可靠的空闲空间检测：用于目标检测的条件标记点过程", "tldr": "本文提出了一种基于空间统计学的目标检测模型，利用条件标记点过程来可靠地量化未检测区域（空闲空间）的不确定性，从而提高自动驾驶等应用的安全性。", "motivation": "当前深度学习目标检测模型在检测性能上表现出色，但其置信度估计常常不准确，并且无法量化未检测区域（即边界框之外的区域）的不确定性。这在自动驾驶等安全关键应用中构成了风险，因为模型未能评估无障碍区域是否真正安全。", "method": "本文提出了一种基于空间统计学的方法。将边界框数据视为标记点过程的实现，其中边界框中心是空间点事件，而标记则描述了边界框的空间扩展和类别。这种统计框架支持基于似然的训练，并为区域是否可行驶（即无障碍）提供了明确定义的置信度估计。", "result": "通过校准评估和性能评估，证明了所提出方法的有效性。", "conclusion": "本文提出的基于条件标记点过程的目标检测模型，为检测空闲空间提供了可靠的置信度估计，解决了现有模型在量化未检测区域不确定性方面的不足，从而提高了自动驾驶等应用的安全性和可靠性。", "translation": "深度神经网络在边界框检测和语义分割等计算机视觉任务中取得了最先进的成果。目标检测器和分割模型为其预测分配置信度分数，反映了模型在目标检测或像素级分类中的不确定性。然而，这些置信度估计常常不准确，因为它们的架构和损失函数是为任务性能而非概率基础量身定制的。即使预测经过良好校准，目标检测器也无法量化检测到的边界框之外的不确定性，即模型没有对没有检测到对象的区域是否真正没有障碍物进行概率评估。这在自动驾驶等应用中构成了安全风险，因为空闲区域的不确定性仍未被探索。在这项工作中，我们提出了一种基于空间统计学的目标检测模型。边界框数据与标记点过程的实现相匹配，该过程通常用于描述被识别为边界框中心的空间点事件的概率发生，其中标记用于描述边界框和类别的空间扩展。我们的统计框架支持基于似然的训练，并为区域是否可行驶（即无障碍）提供了明确定义的置信度估计。我们通过校准评估和性能评估证明了我们方法的有效性。", "summary": "本文提出了一种基于空间统计学的目标检测模型，利用条件标记点过程来解决现有深度学习模型在量化未检测区域不确定性方面的不足。通过将边界框数据视为标记点过程的实现，该方法能够进行基于似然的训练，并为“空闲”或可行驶区域提供明确定义的置信度估计。实验结果表明，该方法在校准和性能方面均有效，有助于提高自动驾驶等安全关键应用的可靠性。", "keywords": "目标检测, 空间统计, 条件标记点过程, 不确定性量化, 空闲空间检测", "comments": "该论文的创新之处在于将空间统计学中的条件标记点过程引入到目标检测领域，解决了现有深度学习模型无法可靠量化未检测区域不确定性的关键问题。这对于自动驾驶等需要高安全性的应用尤为重要。其贡献在于提供了一个具有明确概率基础的框架，能够为“空闲”空间提供置信度评估，弥补了传统目标检测器只关注“有物体”区域的不足。"}}
{"id": "2506.21509", "title": "Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration", "authors": ["Jiahe Chen", "Jiaying He", "Qian Shao", "Qiyuan Chen", "Jiahe Ying", "Hongxia Xu", "Jintai Chen", "Jianwei Zheng", "Jian Wu"], "summary": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21509v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21509v1", "AI": {"title_translation": "缓解大型视觉-语言模型幻觉的动态Logits校准方法", "tldr": "提出动态Logits校准（DLC）框架，通过在推理时动态调整输出logits，有效减少大型视觉-语言模型（LVLMs）的幻觉。", "motivation": "大型视觉-语言模型（LVLMs）尽管在多模态理解方面取得进展，但常受幻觉困扰，即生成与视觉输入矛盾的文本。现有免训练解码策略存在局限性，包括使用静态约束、低效（需多次前向传播）以及过度干预导致细节丢失。", "method": "论文引入动态Logits校准（DLC），一种新型免训练解码框架。在解码阶段，DLC逐步利用CLIP评估输入图像与生成文本序列的语义对齐。通过动态更新的上下文基线评估候选token的相对视觉优势（RVA），自适应调整输出logits以偏向视觉接地（visually grounded）的token。此外，一个由实时上下文对齐分数指导的自适应加权机制平衡视觉引导，同时确保文本输出的整体质量。", "result": "在不同基准和LVLM架构（如LLaVA、InstructBLIP和MiniGPT-4）上进行的广泛实验表明，DLC显著减少了幻觉，优于现有方法，并通过避免多次前向传播保持了高推理效率。", "conclusion": "论文提出了一种有效且高效的解码时解决方案来缓解幻觉，从而提高LVLMs的可靠性以用于更多实践。", "translation": "大型视觉-语言模型（LVLMs）在多模态理解方面取得了显著进展，但它们经常受到幻觉的困扰——即生成与视觉输入矛盾的文本。现有的免训练解码策略存在关键局限性，包括使用静态约束，这些约束在生成过程中不适应语义漂移；由于需要多次前向传播而导致效率低下；以及由于过于僵硬的干预规则而导致细节退化。为了克服这些挑战，本文引入了动态Logits校准（DLC），这是一种新颖的免训练解码框架，旨在推理时动态地将文本生成与视觉证据对齐。在解码阶段，DLC逐步利用CLIP评估输入图像与生成文本序列之间的语义对齐。然后，根据动态更新的上下文基线评估候选token的相对视觉优势（RVA），自适应地调整输出logits以偏向视觉接地的token。此外，一个由实时上下文对齐分数指导的自适应加权机制，在确保文本输出整体质量的同时，仔细平衡了视觉引导。在各种基准和不同LVLM架构（如LLaVA、InstructBLIP和MiniGPT-4）上进行的广泛实验表明，DLC显著减少了幻觉，优于当前方法，同时通过避免多次前向传播保持了高推理效率。总的来说，我们提出了一种有效且高效的解码时解决方案来缓解幻觉，从而增强了LVLMs在更多实践中的可靠性。代码将在Github上发布。", "summary": "本论文提出了一种名为动态Logits校准（DLC）的新型免训练解码框架，旨在解决大型视觉-语言模型（LVLMs）中常见的幻觉问题。DLC在推理时通过逐步利用CLIP评估视觉-文本对齐，并基于动态更新的上下文基线调整候选token的相对视觉优势（RVA），自适应地调整输出logits以优先选择视觉上更合理的token。结合自适应加权机制，DLC在显著减少LVLM幻觉的同时，保持了高效的推理速度，并在多种LVLM架构上展现出优于现有方法的性能。", "keywords": "大型视觉-语言模型, 幻觉缓解, 动态Logits校准, 免训练解码, CLIP", "comments": "该论文的创新点在于提出了一种无需训练的、动态的解码时解决方案来缓解LVLM的幻觉问题。与现有静态或低效的方法不同，DLC能够自适应地调整，避免了多次前向传播，从而提高了效率。通过将视觉证据与文本生成动态对齐，DLC有效提升了LVLMs的可靠性和实用性，是一个重要的进展。"}}
{"id": "2506.21513", "title": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation", "authors": ["Wentao Hu", "Shunkai Li", "Ziqiao Peng", "Haoxian Zhang", "Fan Shi", "Xiaoqiang Liu", "Pengfei Wan", "Di Zhang", "Hui Tian"], "summary": "Creating high-quality, generalizable speech-driven 3D talking heads remains a\npersistent challenge. Previous methods achieve satisfactory results for fixed\nviewpoints and small-scale audio variations, but they struggle with large head\nrotations and out-of-distribution (OOD) audio. Moreover, they are constrained\nby the need for time-consuming, identity-specific training. We believe the core\nissue lies in the lack of sufficient 3D priors, which limits the extrapolation\ncapabilities of synthesized talking heads. To address this, we propose\nGGTalker, which synthesizes talking heads through a combination of\ngeneralizable priors and identity-specific adaptation. We introduce a two-stage\nPrior-Adaptation training strategy to learn Gaussian head priors and adapt to\nindividual characteristics. We train Audio-Expression and Expression-Visual\npriors to capture the universal patterns of lip movements and the general\ndistribution of head textures. During the Customized Adaptation, individual\nspeaking styles and texture details are precisely modeled. Additionally, we\nintroduce a color MLP to generate fine-grained, motion-aligned textures and a\nBody Inpainter to blend rendered results with the background, producing\nindistinguishable, photorealistic video frames. Comprehensive experiments show\nthat GGTalker achieves state-of-the-art performance in rendering quality, 3D\nconsistency, lip-sync accuracy, and training efficiency.", "comment": "ICCV 2025, Project page: https://vincenthu19.github.io/GGTalker/", "pdf_url": "http://arxiv.org/pdf/2506.21513v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21513v1", "AI": {"title_translation": "GGTalker：基于可泛化高斯先验和身份特异性适应的说话人头部合成", "tldr": "GGTalker提出了一种结合可泛化高斯先验和身份特异性适应的两阶段训练策略，解决了高质量、可泛化的语音驱动3D说话人头部合成中大头部旋转、OOD音频和耗时训练的挑战，实现了SOTA的渲染质量、3D一致性、唇形同步精度和训练效率。", "motivation": "当前方法在固定视角和小规模音频变化下效果尚可，但在大头部旋转和域外（OOD）音频方面表现不佳，且受限于耗时的身份特异性训练。核心问题在于缺乏足够的3D先验，限制了合成说话人头部的外推能力。", "method": "GGTalker通过结合可泛化先验和身份特异性适应来合成说话人头部。它引入了两阶段的“先验-适应”训练策略：学习高斯头部先验并适应个体特征。具体包括训练“音频-表情”和“表情-视觉”先验以捕捉唇部动作的普遍模式和头部纹理的普遍分布。在定制适应阶段，精确建模个体说话风格和纹理细节。此外，引入了颜色MLP生成精细、运动对齐的纹理，并使用Body Inpainter将渲染结果与背景融合，生成逼真的视频帧。", "result": "GGTalker在渲染质量、3D一致性、唇形同步精度和训练效率方面均达到了最先进的性能。", "conclusion": "GGTalker通过引入可泛化高斯先验和身份特异性适应的两阶段训练策略，成功解决了高质量、可泛化语音驱动3D说话人头部合成的挑战，并在多个关键指标上实现了最先进的性能。", "translation": "创建高质量、可泛化的语音驱动3D说话人头部仍然是一个持续的挑战。以前的方法在固定视角和小规模音频变化下取得了令人满意的结果，但它们在大头部旋转和域外（OOD）音频方面表现不佳。此外，它们受到耗时的身份特异性训练的限制。我们认为核心问题在于缺乏足够的3D先验，这限制了合成说话人头部的外推能力。为了解决这个问题，我们提出了GGTalker，它通过结合可泛化先验和身份特异性适应来合成说话人头部。我们引入了两阶段的“先验-适应”训练策略来学习高斯头部先验并适应个体特征。我们训练“音频-表情”和“表情-视觉”先验以捕捉唇部动作的普遍模式和头部纹理的普遍分布。在定制适应阶段，精确建模个体说话风格和纹理细节。此外，我们引入了一个颜色MLP来生成精细、运动对齐的纹理，并引入一个Body Inpainter将渲染结果与背景融合，生成无法区分的逼真视频帧。全面的实验表明，GGTalker在渲染质量、3D一致性、唇形同步精度和训练效率方面均达到了最先进的性能。", "summary": "GGTalker提出了一种新颖的语音驱动3D说话人头部合成系统，旨在解决现有方法在大头部旋转、域外音频和耗时训练方面的局限性。该系统引入了两阶段的“先验-适应”训练策略，首先学习通用的高斯头部先验（包括音频-表情和表情-视觉先验），然后进行身份特异性适应以捕捉个体特征。GGTalker还结合了颜色MLP生成精细纹理和Body Inpainter进行背景融合，最终实现了高渲染质量、3D一致性、准确的唇形同步和高训练效率。", "keywords": "说话人头部合成, 3D先验, 高斯先验, 身份适应, 语音驱动", "comments": "GGTalker的创新之处在于其两阶段的“先验-适应”训练策略，它有效地结合了通用先验知识和个性化适应能力，解决了传统方法在泛化性和训练效率上的痛点。特别是引入高斯头部先验来增强3D外推能力，以及颜色MLP和Body Inpainter用于提高视觉真实感，使其在高质量语音驱动3D说话人头部合成领域具有重要意义。"}}
{"id": "2506.21514", "title": "G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation", "authors": ["Mohammed Rakib", "Arunkumar Bagavathi"], "summary": "Multimodal learning aims to leverage information from diverse data modalities\nto achieve more comprehensive performance. However, conventional multimodal\nmodels often suffer from modality imbalance, where one or a few modalities\ndominate model optimization, leading to suboptimal feature representation and\nunderutilization of weak modalities. To address this challenge, we introduce\nGradient-Guided Distillation (G$^{2}$D), a knowledge distillation framework\nthat optimizes the multimodal model with a custom-built loss function that\nfuses both unimodal and multimodal objectives. G$^{2}$D further incorporates a\ndynamic sequential modality prioritization (SMP) technique in the learning\nprocess to ensure each modality leads the learning process, avoiding the\npitfall of stronger modalities overshadowing weaker ones. We validate G$^{2}$D\non multiple real-world datasets and show that G$^{2}$D amplifies the\nsignificance of weak modalities while training and outperforms state-of-the-art\nmethods in classification and regression tasks. Our code is available at\nhttps://github.com/rAIson-Lab/G2D.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21514v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21514v1", "AI": {"title_translation": "G$^{2}$D: 梯度引导蒸馏提升多模态学习", "tldr": "G$^{2}$D是一种梯度引导蒸馏框架，通过自定义损失函数和动态模态优先级技术解决多模态学习中的模态不平衡问题，提升弱模态的重要性并超越现有SOTA方法。", "motivation": "传统多模态模型存在模态不平衡问题，即部分模态在模型优化中占据主导，导致特征表示不佳和弱模态利用不足。", "method": "本文引入了梯度引导蒸馏 (G$^{2}$D) 框架，该框架通过自定义损失函数融合单模态和多模态目标来优化模型。G$^{2}$D还结合了动态顺序模态优先级 (SMP) 技术，确保每个模态都能主导学习过程，避免强模态掩盖弱模态。", "result": "G$^{2}$D在多个真实世界数据集上得到了验证，结果表明G$^{2}$D在训练过程中增强了弱模态的重要性，并在分类和回归任务中超越了现有最先进的方法。", "conclusion": "G$^{2}$D框架通过解决多模态学习中的模态不平衡问题，显著提升了弱模态的利用率和整体模型性能，在分类和回归任务上表现优异。", "translation": "多模态学习旨在利用不同数据模态的信息以实现更全面的性能。然而，传统的多模态模型常受模态不平衡问题困扰，即一个或少数模态在模型优化中占据主导，导致次优的特征表示和弱模态的利用不足。为解决这一挑战，我们引入了梯度引导蒸馏 (G$^{2}$D)，这是一个知识蒸馏框架，它通过融合单模态和多模态目标的自定义损失函数来优化多模态模型。G$^{2}$D在学习过程中进一步整合了动态顺序模态优先级 (SMP) 技术，以确保每个模态都能主导学习过程，避免强模态掩盖弱模态的弊端。我们在多个真实世界数据集上验证了G$^{2}$D，并表明G$^{2}$D在训练时增强了弱模态的重要性，并在分类和回归任务中超越了最先进的方法。我们的代码可在 https://github.com/rAIson-Lab/G2D 获取。", "summary": "本文提出G$^{2}$D，一个梯度引导蒸馏框架，旨在解决多模态学习中的模态不平衡问题。G$^{2}$D通过结合自定义损失函数（融合单模态与多模态目标）和动态顺序模态优先级（SMP）技术，确保弱模态在训练中得到充分利用。实验证明，G$^{2}$D在多个真实数据集上能有效提升弱模态的重要性，并在分类和回归任务中超越现有SOTA方法。", "keywords": "多模态学习, 梯度引导蒸馏, 模态不平衡, 知识蒸馏, 模态优先级", "comments": "G$^{2}$D的创新之处在于其结合了知识蒸馏、自定义损失函数以及动态模态优先级技术，有效解决了多模态学习中长期存在的模态不平衡问题。通过提升弱模态的重要性，该方法有望在实际应用中更全面地利用多源数据，具有重要的理论和实践意义。"}}
{"id": "2506.21520", "title": "MADrive: Memory-Augmented Driving Scene Modeling", "authors": ["Polina Karpikova", "Daniil Selikhanovych", "Kirill Struminsky", "Ruslan Musaev", "Maria Golitsyna", "Dmitry Baranchuk"], "summary": "Recent advances in scene reconstruction have pushed toward highly realistic\nmodeling of autonomous driving (AD) environments using 3D Gaussian splatting.\nHowever, the resulting reconstructions remain closely tied to the original\nobservations and struggle to support photorealistic synthesis of significantly\naltered or novel driving scenarios. This work introduces MADrive, a\nmemory-augmented reconstruction framework designed to extend the capabilities\nof existing scene reconstruction methods by replacing observed vehicles with\nvisually similar 3D assets retrieved from a large-scale external memory bank.\nSpecifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg}\ncar videos captured in the wild and present a retrieval module that finds the\nmost similar car instances in the memory bank, reconstructs the corresponding\n3D assets from video, and integrates them into the target scene through\norientation alignment and relighting. The resulting replacements provide\ncomplete multi-view representations of vehicles in the scene, enabling\nphotorealistic synthesis of substantially altered configurations, as\ndemonstrated in our experiments. Project page:\nhttps://yandex-research.github.io/madrive/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21520v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21520v1", "AI": {"title_translation": "MADrive：记忆增强驾驶场景建模", "tldr": "MADrive是一个记忆增强的重建框架，通过从外部记忆库检索和集成3D车辆资产，实现对自动驾驶场景中车辆的逼真替换和新场景合成。", "motivation": "现有的3D高斯泼溅场景重建方法虽然能实现高度逼真的自动驾驶环境建模，但重建结果与原始观测紧密相关，难以支持显著改变或新颖驾驶场景的光真实感合成。", "method": "本工作提出了MADrive，一个记忆增强的重建框架。它通过从大规模外部记忆库中检索视觉上相似的3D资产来替换观察到的车辆。具体来说，该框架发布了MAD-Cars数据集（包含约7万个野外捕获的360度汽车视频），并提出了一个检索模块，该模块能从记忆库中找到最相似的汽车实例，从视频中重建相应的3D资产，并通过方向对齐和重新打光将其集成到目标场景中。", "result": "替换后的车辆提供了场景中完整的、多视图的车辆表示，实验证明这使得能够光真实感地合成显著改变的配置。", "conclusion": "MADrive通过引入记忆增强的车辆替换机制，显著扩展了现有场景重建方法的能力，实现了对自动驾驶场景中车辆的逼真合成和新场景生成。", "translation": "自动驾驶（AD）环境的场景重建近期取得了进展，3D高斯泼溅技术实现了高度逼真的建模。然而，由此产生的重建结果仍然与原始观测紧密相关，难以支持显著改变或新颖驾驶场景的光真实感合成。本工作引入了MADrive，一个记忆增强的重建框架，旨在通过用从大型外部记忆库中检索到的视觉相似的3D资产替换观察到的车辆，来扩展现有场景重建方法的能力。具体来说，我们发布了MAD-Cars，一个包含约7万个在野外捕获的360度汽车视频的精选数据集，并提出了一个检索模块，该模块能在记忆库中找到最相似的汽车实例，从视频中重建相应的3D资产，并通过方向对齐和重新打光将其集成到目标场景中。由此产生的替换提供了场景中车辆的完整多视图表示，使得能够光真实感地合成显著改变的配置，正如我们在实验中所展示的。项目页面：https://yandex-research.github.io/madrive/", "summary": "MADrive是一个创新的记忆增强重建框架，旨在解决现有3D场景重建方法在自动驾驶环境中合成显著改变或新颖场景时的局限性。该框架通过从一个名为MAD-Cars的大规模外部记忆库中检索并集成逼真的3D车辆资产来替换原始场景中的车辆。MADrive包含一个检索模块，能够找到最相似的车辆实例，从视频重建其3D模型，并通过方向对齐和重新打光将其无缝集成到目标场景中。实验证明，这种方法能够实现对车辆配置的光真实感合成，从而扩展了自动驾驶场景建模的能力。", "keywords": "场景重建, 记忆增强, 3D高斯泼溅, 自动驾驶, 车辆合成", "comments": "MADrive的创新之处在于其记忆增强的方法，通过引入外部大规模3D资产库来克服传统重建方法对原始观测的依赖。这为自动驾驶场景的灵活编辑和新颖场景生成提供了新的可能性。MAD-Cars数据集的发布也为相关研究提供了宝贵的资源。"}}
{"id": "2506.21079", "title": "Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games", "authors": ["Yann Kerzreho"], "summary": "This paper introduces a new approach for approximating the learning dynamics\nof multiple reinforcement learning (RL) agents interacting in a finite-state\nMarkov game. The idea is to rescale the learning process by simultaneously\nreducing the learning rate and increasing the update frequency, effectively\ntreating the agent's parameters as a slow-evolving variable influenced by the\nfast-mixing game state. Under mild assumptions-ergodicity of the state process\nand continuity of the updates-we prove the convergence of this rescaled process\nto an ordinary differential equation (ODE). This ODE provides a tractable,\ndeterministic approximation of the agent's learning dynamics. An implementation\nof the framework is available at\\,:\nhttps://github.com/yannKerzreho/MarkovGameApproximation", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21079v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21079v1", "AI": {"title_translation": "有限状态马尔可夫博弈中多智能体学习动力学的均匀化", "tldr": "通过重标度学习过程，将有限状态马尔可夫博弈中的多智能体学习动力学近似为常微分方程。", "motivation": "旨在解决多智能体强化学习在有限状态马尔可夫博弈中学习动力学复杂且难以分析的问题，提供一个可处理的近似方法。", "method": "该论文引入了一种新的方法，通过同时减小学习率和增加更新频率来重标度学习过程。这有效地将智能体的参数视为受快速混合博弈状态影响的慢演变变量。在状态过程遍历性和更新连续性的温和假设下，证明了这种重标度过程收敛到一个常微分方程（ODE）。", "result": "证明了重标度过程收敛到一个常微分方程（ODE）。该ODE为智能体的学习动力学提供了一个可处理的、确定性的近似。", "conclusion": "通过重标度学习过程，可以将多智能体学习动力学近似为一个可处理的、确定性的常微分方程，从而简化了对复杂系统行为的分析。", "translation": "本文介绍了一种新的方法，用于近似在有限状态马尔可夫博弈中交互的多个强化学习（RL）智能体的学习动力学。其思想是通过同时减小学习率和增加更新频率来重标度学习过程，有效地将智能体的参数视为受快速混合博弈状态影响的慢演变变量。在温和的假设下——状态过程的遍历性和更新的连续性——我们证明了这种重标度过程收敛到一个常微分方程（ODE）。这个ODE为智能体的学习动力学提供了一个可处理的、确定性的近似。该框架的实现可在以下网址获取：https://github.com/yannKerzreho/MarkovGameApproximation", "summary": "本文提出一种新方法，通过重标度学习过程（减小学习率、增加更新频率）来近似有限状态马尔可夫博弈中多智能体学习动力学。该方法将智能体参数视为慢演变变量，并证明在特定条件下，重标度过程收敛到一个常微分方程（ODE），从而为学习动力学提供可处理的确定性近似。", "keywords": "多智能体学习, 马尔可夫博弈, 学习动力学, 常微分方程, 均匀化", "comments": "这项工作通过将复杂的离散多智能体学习动力学近似为连续的常微分方程，极大地简化了分析。这种“均匀化”方法在理论上具有重要意义，因为它提供了一个数学上更易处理的模型来理解和预测多智能体系统的行为，对于理解学习算法的收敛性和稳定性具有潜在价值。"}}
{"id": "2506.21526", "title": "WAFT: Warping-Alone Field Transforms for Optical Flow", "authors": ["Yihan Wang", "Jia Deng"], "summary": "We introduce Warping-Alone Field Transforms (WAFT), a simple and effective\nmethod for optical flow. WAFT is similar to RAFT but replaces cost volume with\nhigh-resolution warping, achieving better accuracy with lower memory cost. This\ndesign challenges the conventional wisdom that constructing cost volumes is\nnecessary for strong performance. WAFT is a simple and flexible\nmeta-architecture with minimal inductive biases and reliance on custom designs.\nCompared with existing methods, WAFT ranks 1st on Spring and KITTI benchmarks,\nachieves the best zero-shot generalization on KITTI, while being up to 4.1x\nfaster than methods with similar performance. Code and model weights are\navailable at https://github.com/princeton-vl/WAFT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21526v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21526v1", "AI": {"title_translation": "WAFT：用于光流的独立形变场变换", "tldr": "WAFT是一种新的光流方法，通过高分辨率形变取代成本体，实现了更高的精度、更低的内存消耗，并在基准测试中表现优异。", "motivation": "该研究旨在挑战传统观念，即构建成本体对于实现光流的强大性能是必需的，并提供一种简单、有效、低内存成本且高性能的光流方法。", "method": "本文引入了独立形变场变换（WAFT），这是一种用于光流的简单灵活的元架构。它类似于RAFT，但用高分辨率形变取代了成本体。", "result": "WAFT在Spring和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化能力，并且比性能相似的方法快4.1倍。", "conclusion": "WAFT证明了即使没有构建成本体，也能实现强大的光流性能，提供了一种简单、灵活、高效且高性能的光流解决方案。", "translation": "我们引入了独立形变场变换（WAFT），这是一种简单有效的光流方法。WAFT与RAFT相似，但用高分辨率形变取代了成本体，以更低的内存成本实现了更高的精度。这种设计挑战了构建成本体对于实现强大性能是必要的传统观念。WAFT是一种简单灵活的元架构，具有最小的归纳偏置和对自定义设计的依赖。与现有方法相比，WAFT在Spring和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化能力，同时比性能相似的方法快4.1倍。代码和模型权重可在https://github.com/princeton-vl/WAFT获取。", "summary": "本文提出了一种名为WAFT（Warping-Alone Field Transforms）的光流方法。WAFT通过采用高分辨率形变而非成本体，显著提高了精度并降低了内存消耗。该方法挑战了光流领域中构建成本体是必要条件的传统认知。WAFT被设计为一种简单灵活的元架构，具有最小的归纳偏置。实验结果表明，WAFT在Spring和KITTI基准测试中均位列第一，并在KITTI上展现出卓越的零样本泛化能力，同时比同等性能的方法快4.1倍。", "keywords": "光流, 形变场变换, 成本体, 零样本泛化, 元架构", "comments": "WAFT的创新之处在于其挑战了光流领域中成本体构建的传统范式，通过引入高分辨率形变实现了性能和效率的显著提升。其“独立形变”的设计思路简化了架构，减少了归纳偏置，使其具有更强的灵活性和泛化能力。在性能上，它在多个主流基准测试中均表现出色，尤其在速度和零样本泛化方面具有重要优势，这对于实际应用具有重要意义。"}}
{"id": "2506.21541", "title": "StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning", "authors": ["Chuxin Wang", "Yixin Zha", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Recently, Mamba-based methods have demonstrated impressive performance in\npoint cloud representation learning by leveraging State Space Model (SSM) with\nthe efficient context modeling ability and linear complexity. However, these\nmethods still face two key issues that limit the potential of SSM: Destroying\nthe adjacency of 3D points during SSM processing and failing to retain\nlong-sequence memory as the input length increases in downstream tasks. To\naddress these issues, we propose StruMamba3D, a novel paradigm for\nself-supervised point cloud representation learning. It enjoys several merits.\nFirst, we design spatial states and use them as proxies to preserve spatial\ndependencies among points. Second, we enhance the SSM with a state-wise update\nstrategy and incorporate a lightweight convolution to facilitate interactions\nbetween spatial states for efficient structure modeling. Third, our method\nreduces the sensitivity of pre-trained Mamba-based models to varying input\nlengths by introducing a sequence length-adaptive strategy. Experimental\nresults across four downstream tasks showcase the superior performance of our\nmethod. In addition, our method attains the SOTA 95.1% accuracy on ModelNet40\nand 92.75% accuracy on the most challenging split of ScanObjectNN without\nvoting strategy.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21541v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21541v1", "AI": {"title_translation": "StruMamba3D：探索结构化Mamba用于自监督点云表示学习", "tldr": "StruMamba3D通过引入空间状态、状态更新策略和序列长度自适应策略，解决了现有Mamba模型在点云处理中破坏邻接性和长序列记忆不足的问题，并在多项下游任务中取得了SOTA性能。", "motivation": "现有基于Mamba的点云表示学习方法存在两个主要问题：一是在SSM处理过程中破坏了3D点的邻接性；二是随着下游任务输入长度的增加，难以保留长序列记忆，从而限制了SSM的潜力。", "method": "本文提出StruMamba3D，一种新的自监督点云表示学习范式。它通过设计空间状态作为代理来保留点之间的空间依赖性；通过状态更新策略增强SSM，并结合轻量级卷积促进空间状态间的交互以实现高效结构建模；通过引入序列长度自适应策略，降低预训练Mamba模型对不同输入长度的敏感性。", "result": "在四项下游任务中的实验结果表明，StruMamba3D表现优越。此外，该方法在ModelNet40上达到了95.1%的SOTA准确率，在ScanObjectNN最具挑战性的分割上达到了92.75%的准确率（无需投票策略）。", "conclusion": "StruMamba3D成功解决了现有Mamba模型在点云处理中的关键问题，并在多个基准测试中展现出卓越的性能和最先进的准确率，证明了其在自监督点云表示学习领域的有效性和潜力。", "translation": "最近，基于Mamba的方法通过利用状态空间模型（SSM）及其高效的上下文建模能力和线性复杂度，在点云表示学习中展现出令人印象深刻的性能。然而，这些方法仍然面临两个限制SSM潜力的关键问题：在SSM处理过程中破坏了3D点的邻接性，以及在下游任务中随着输入长度的增加未能保留长序列记忆。为了解决这些问题，我们提出了StruMamba3D，一种用于自监督点云表示学习的新范式。它具有多项优点。首先，我们设计了空间状态并将其用作代理，以保留点之间的空间依赖性。其次，我们通过状态更新策略增强了SSM，并结合了轻量级卷积以促进空间状态之间的交互，从而实现高效的结构建模。第三，我们的方法通过引入序列长度自适应策略，降低了预训练Mamba模型对不同输入长度的敏感性。在四项下游任务中的实验结果展示了我们方法的卓越性能。此外，我们的方法在ModelNet40上达到了95.1%的SOTA准确率，在ScanObjectNN最具挑战性的分割上达到了92.75%的准确率（无需投票策略）。", "summary": "本文提出StruMamba3D，一种新颖的自监督点云表示学习框架，旨在解决现有Mamba模型在点云处理中破坏3D点邻接性和长序列记忆不足的问题。StruMamba3D通过引入空间状态来保留空间依赖性，采用状态更新策略和轻量级卷积进行高效结构建模，并通过序列长度自适应策略提高对不同输入长度的鲁棒性。实验证明，StruMamba3D在多项下游任务中表现优异，并在ModelNet40和ScanObjectNN上取得了SOTA准确率。", "keywords": "Mamba, 点云, 自监督学习, 状态空间模型, 表示学习", "comments": "StruMamba3D的创新之处在于其针对Mamba模型在点云处理中固有缺陷的精准优化。通过引入空间状态、精细化的状态更新机制以及对序列长度的自适应处理，该工作显著提升了Mamba模型在三维数据上的表现力。其在多个基准测试中达到SOTA性能，证明了其重要性和有效性，为点云表示学习领域提供了一个有前景的新方向。"}}
{"id": "2506.21544", "title": "DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion", "authors": ["Yansong Qu", "Shaohui Dai", "Xinyang Li", "Yuze Wang", "You Shen", "Liujuan Cao", "Rongrong Ji"], "summary": "Reconstructing 3D objects from a single image is a long-standing challenge,\nespecially under real-world occlusions. While recent diffusion-based view\nsynthesis models can generate consistent novel views from a single RGB image,\nthey generally assume fully visible inputs and fail when parts of the object\nare occluded. This leads to inconsistent views and degraded 3D reconstruction\nquality. To overcome this limitation, we propose an end-to-end framework for\nocclusion-aware multi-view generation. Our method directly synthesizes six\nstructurally consistent novel views from a single partially occluded image,\nenabling downstream 3D reconstruction without requiring prior inpainting or\nmanual annotations. We construct a self-supervised training pipeline using the\nPix2Gestalt dataset, leveraging occluded-unoccluded image pairs and\npseudo-ground-truth views to teach the model structure-aware completion and\nview consistency. Without modifying the original architecture, we fully\nfine-tune the view synthesis model to jointly learn completion and multi-view\ngeneration. Additionally, we introduce the first benchmark for occlusion-aware\nreconstruction, encompassing diverse occlusion levels, object categories, and\nmask patterns. This benchmark provides a standardized protocol for evaluating\nfuture methods under partial occlusions. Our code is available at\nhttps://github.com/Quyans/DeOcc123.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21544v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21544v1", "AI": {"title_translation": "DeOcc-1-to-3：通过自监督多视角扩散从单张图像进行3D去遮挡", "tldr": "DeOcc-1-to-3提出了一种端到端框架，通过自监督多视角扩散从单张部分遮挡图像生成一致的新视角，从而实现高质量的3D去遮挡重建。", "motivation": "从单张图像重建3D物体是一个长期存在的挑战，尤其是在真实世界遮挡下。现有的基于扩散的视角合成模型在物体部分被遮挡时会失效，导致视角不一致和3D重建质量下降。", "method": "提出一个端到端的遮挡感知多视角生成框架，直接从单张部分遮挡图像合成六个结构一致的新视角。利用Pix2Gestalt数据集构建自监督训练流程，使用遮挡-未遮挡图像对和伪真值视角来训练模型学习结构感知补全和视角一致性。通过微调视角合成模型，使其联合学习补全和多视角生成。此外，引入了首个用于遮挡感知重建的基准。", "result": "模型能够从单张部分遮挡图像合成六个结构一致的新视角，无需预先修复或手动标注，从而实现高质量的3D重建。还创建了第一个遮挡感知重建基准，涵盖了不同的遮挡水平、物体类别和遮罩模式，为未来方法的评估提供了标准化协议。", "conclusion": "该论文提出了一个名为DeOcc-1-to-3的端到端框架，通过自监督多视角扩散从单张部分遮挡图像进行3D去遮挡，并生成一致的新视角，有效解决了现有方法在遮挡情况下的局限性。同时，还首次引入了一个用于遮挡感知重建的标准化基准。", "translation": "从单张图像重建3D物体是一个长期存在的挑战，尤其是在真实世界的遮挡下。虽然最近基于扩散的视角合成模型可以从单张RGB图像生成一致的新视角，但它们通常假设输入是完全可见的，并且在物体部分被遮挡时会失效。这会导致视角不一致和3D重建质量下降。为了克服这一限制，我们提出了一个用于遮挡感知多视角生成的端到端框架。我们的方法直接从单张部分遮挡图像合成六个结构一致的新视角，从而实现下游的3D重建，而无需预先修复或手动标注。我们使用Pix2Gestalt数据集构建了一个自监督训练流程，利用遮挡-未遮挡图像对和伪真值视角来教导模型结构感知补全和视角一致性。在不修改原始架构的情况下，我们完全微调了视角合成模型，以联合学习补全和多视角生成。此外，我们引入了第一个用于遮挡感知重建的基准，涵盖了不同的遮挡水平、物体类别和遮罩模式。这个基准为未来在部分遮挡下的方法评估提供了标准化协议。我们的代码可在https://github.com/Quyans/DeOcc123获取。", "summary": "本论文提出了DeOcc-1-to-3，一个用于从单张部分遮挡图像进行3D去遮挡的端到端框架。该方法通过自监督多视角扩散，直接合成六个结构一致的新视角，从而在无需预先修复或手动标注的情况下实现高质量的3D重建。作者利用Pix2Gestalt数据集构建了自监督训练流程，并微调了视角合成模型以联合学习补全和多视角生成。此外，论文还首次引入了一个用于遮挡感知重建的标准化基准。", "keywords": "3D去遮挡, 单图像重建, 多视角扩散, 自监督学习, 遮挡感知重建", "comments": "该论文在处理3D重建中的真实世界遮挡问题上迈出了重要一步，通过提出的自监督多视角扩散框架，有效解决了现有扩散模型在遮挡情况下的局限性。其创新之处在于无需预先修复或手动标注即可直接从遮挡图像生成一致的新视角，并联合学习补全和多视角生成。此外，引入首个遮挡感知重建基准对于推动该领域未来的研究和评估具有重要意义。"}}
{"id": "2506.21549", "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark", "authors": ["Alex Costanzino", "Pierluigi Zama Ramirez", "Luigi Lella", "Matteo Ragaglia", "Alessandro Oliva", "Giuseppe Lisanti", "Luigi Di Stefano"], "summary": "We propose SiM3D, the first benchmark considering the integration of\nmultiview and multimodal information for comprehensive 3D anomaly detection and\nsegmentation (ADS), where the task is to produce a voxel-based Anomaly Volume.\nMoreover, SiM3D focuses on a scenario of high interest in manufacturing:\nsingle-instance anomaly detection, where only one object, either real or\nsynthetic, is available for training. In this respect, SiM3D stands out as the\nfirst ADS benchmark that addresses the challenge of generalising from synthetic\ntraining data to real test data. SiM3D includes a novel multimodal multiview\ndataset acquired using top-tier industrial sensors and robots. The dataset\nfeatures multiview high-resolution images (12 Mpx) and point clouds (7M points)\nfor 333 instances of eight types of objects, alongside a CAD model for each\ntype. We also provide manually annotated 3D segmentation GTs for anomalous test\nsamples. To establish reference baselines for the proposed multiview 3D ADS\ntask, we adapt prominent singleview methods and assess their performance using\nnovel metrics that operate on Anomaly Volumes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21549v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21549v1", "AI": {"title_translation": "SiM3D：单实例多视角多模态多设置3D异常检测基准", "tldr": "SiM3D是一个新的3D异常检测与分割基准，首次整合了多视角和多模态信息，并专注于制造业中单实例和合成-真实数据泛化的挑战，提供了一个新的数据集和评估方法。", "motivation": "现有的3D异常检测和分割基准未能充分整合多视角和多模态信息，也未有效解决制造业中常见的单实例异常检测场景以及从合成数据到真实数据泛化的问题。", "method": "本文提出了SiM3D基准，用于全面的3D异常检测和分割。它包含一个新颖的多模态多视角数据集，该数据集使用顶级工业传感器和机器人采集，包括高分辨率图像（12 Mpx）和点云（7M点），涵盖8种对象类型的333个实例，并附带每种类型的CAD模型。同时，还提供了异常测试样本的手动标注3D分割真值。为了建立多视角3D ADS任务的参考基线，研究人员调整了著名的单视角方法，并使用操作于异常体积的新型度量标准评估其性能。", "result": "本文提出了SiM3D，这是第一个整合多视角和多模态信息进行全面3D异常检测和分割的基准。它专注于单实例异常检测场景，并首次解决了从合成训练数据泛化到真实测试数据的挑战。SiM3D提供了一个包含高分辨率图像和点云的新型多模态多视角数据集，以及手动标注的3D分割真值。此外，还为所提出的多视角3D ADS任务建立了参考基线，并引入了基于异常体积的新型评估指标。", "conclusion": "SiM3D提供了一个至关重要的新基准和数据集，通过整合多样化的数据源并解决单实例学习和合成-真实泛化等实际挑战，推动了3D异常检测和分割领域的发展，尤其对制造业应用具有重要意义。", "translation": "我们提出了SiM3D，这是第一个考虑整合多视角和多模态信息以进行全面3D异常检测和分割（ADS）的基准，其任务是生成基于体素的异常体积。此外，SiM3D专注于制造业中备受关注的场景：单实例异常检测，即仅有一个对象（无论是真实的还是合成的）可用于训练。在这方面，SiM3D是第一个解决从合成训练数据泛化到真实测试数据挑战的ADS基准。SiM3D包含一个使用顶级工业传感器和机器人采集的新型多模态多视角数据集。该数据集包含八种对象类型的333个实例的多视角高分辨率图像（12 Mpx）和点云（7M点），以及每种类型的CAD模型。我们还为异常测试样本提供了手动标注的3D分割真值。为了为所提出的多视角3D ADS任务建立参考基线，我们调整了著名的单视角方法，并使用操作于异常体积的新型度量标准评估其性能。", "summary": "SiM3D是一个新颖的3D异常检测与分割（ADS）基准，首次整合了多视角和多模态信息。它专注于制造业中的单实例异常检测场景，并解决了从合成数据到真实数据泛化的挑战。SiM3D包含一个使用工业级传感器获取的独特数据集，该数据集拥有高分辨率图像、点云数据和CAD模型，并提供了手动标注的3D分割真值。该基准还为多视角3D ADS任务建立了参考基线，并引入了基于异常体积的新度量标准。", "keywords": "3D异常检测, 多视角, 多模态, 单实例, 基准", "comments": "SiM3D是一个重要的贡献，因为它填补了3D异常检测领域的一个关键空白，即多视角、多模态数据集成以及对单实例和合成-真实泛化挑战的关注。其提供的高质量数据集和详细标注对于推动该领域的研究具有重大价值，尤其是在工业应用中。"}}
{"id": "2506.21460", "title": "Wild refitting for black box prediction", "authors": ["Martin J. Wainwright"], "summary": "We describe and analyze a computionally efficient refitting procedure for\ncomputing high-probability upper bounds on the instance-wise mean-squared\nprediction error of penalized nonparametric estimates based on least-squares\nminimization. Requiring only a single dataset and black box access to the\nprediction method, it consists of three steps: computing suitable residuals,\nsymmetrizing and scaling them with a pre-factor $\\rho$, and using them to\ndefine and solve a modified prediction problem recentered at the current\nestimate. We refer to it as wild refitting, since it uses Rademacher residual\nsymmetrization as in a wild bootstrap variant. Under relatively mild conditions\nallowing for noise heterogeneity, we establish a high probability guarantee on\nits performance, showing that the wild refit with a suitably chosen wild noise\nscale $\\rho$ gives an upper bound on prediction error. This theoretical\nanalysis provides guidance into the design of such procedures, including how\nthe residuals should be formed, the amount of noise rescaling in the wild\nsub-problem needed for upper bounds, and the local stability properties of the\nblock-box procedure. We illustrate the applicability of this procedure to\nvarious problems, including non-rigid structure-from-motion recovery with\nstructured matrix penalties; plug-and-play image restoration with deep neural\nnetwork priors; and randomized sketching with kernel methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21460v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21460v1", "AI": {"title_translation": "黑箱预测的野性再拟合", "tldr": "本文提出了一种名为“野性再拟合”的计算高效程序，用于计算基于最小二乘法的惩罚非参数估计的实例级均方预测误差的高概率上限，仅需单个数据集和对预测方法的黑箱访问。", "motivation": "需要为基于最小二乘法的惩罚非参数估计计算实例级均方预测误差的高概率上限，尤其是在只能进行黑箱访问的情况下。", "method": "该方法称为“野性再拟合”，仅需单个数据集和对预测方法的黑箱访问。它包含三个步骤：计算合适的残差，使用预因子$\\rho$对其进行对称化和缩放，以及使用它们定义和解决以当前估计为中心修正的预测问题。它使用Rademacher残差对称化，类似于野性自举变体。", "result": "在允许噪声异质性的相对温和条件下，该方法在高概率下保证了其性能，表明经过适当选择的野性噪声尺度$\\rho$的野性再拟合可以提供预测误差的上限。理论分析为此类程序的设计提供了指导，包括残差的形成方式、上限所需的野性子问题中噪声重缩放的量以及黑箱程序的局部稳定性特性。该程序适用于非刚性运动结构恢复、即插即用图像修复和核方法随机草图等问题。", "conclusion": "野性再拟合程序提供了一种计算高效且具有理论保证的方法，用于确定黑箱预测模型的预测误差上限，并为设计此类程序提供了有价值的指导。", "translation": "我们描述并分析了一种计算高效的再拟合程序，用于计算基于最小二乘法惩罚非参数估计的实例级均方预测误差的高概率上限。该程序仅需单个数据集和对预测方法的黑箱访问，它包括三个步骤：计算合适的残差，使用预因子$\\rho$对其进行对称化和缩放，以及使用它们定义和解决以当前估计为中心修正的预测问题。我们将其称为野性再拟合，因为它使用Rademacher残差对称化，类似于野性自举变体。在允许噪声异质性的相对温和条件下，我们建立了其性能的高概率保证，表明经过适当选择的野性噪声尺度$\\rho$的野性再拟合可以提供预测误差的上限。这项理论分析为此类程序的设计提供了指导，包括残差的形成方式、上限所需的野性子问题中噪声重缩放的量以及黑箱程序的局部稳定性特性。我们举例说明了该程序在各种问题中的适用性，包括使用结构化矩阵惩罚的非刚性运动结构恢复；使用深度神经网络先验的即插即用图像修复；以及使用核方法的随机草图。", "summary": "本文介绍并分析了一种计算高效的“野性再拟合”程序，用于计算基于最小二乘法的惩罚非参数估计的实例级均方预测误差的高概率上限。该程序仅需单个数据集和对预测方法的黑箱访问，其核心步骤包括残差计算、残差对称化与缩放，以及解决修正的预测问题。通过理论分析，研究表明在温和条件下，该方法能提供预测误差的有效上限，并为程序设计提供指导。该方法已成功应用于非刚性运动结构恢复、即插即用图像修复和核方法随机草图等多个领域。", "keywords": "野性再拟合, 黑箱预测, 预测误差上限, 非参数估计, Rademacher对称化", "comments": "该论文的创新之处在于提出了一种计算高效的“野性再拟合”程序，能够为黑箱预测模型提供高概率的实例级均方预测误差上限。其重要性在于，它提供了一种在复杂模型中进行不确定性量化的方法，并且其理论分析为设计鲁棒的预测误差边界程序提供了清晰的指导。该方法仅依赖于黑箱访问和单个数据集，使其具有广泛的适用性。"}}
{"id": "2506.21511", "title": "Gaussian Invariant Markov Chain Monte Carlo", "authors": ["Michalis K. Titsias", "Angelos Alexopoulos", "Siran Liu", "Petros Dellaportas"], "summary": "We develop sampling methods, which consist of Gaussian invariant versions of\nrandom walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and\nsecond order Hessian or Manifold MALA. Unlike standard RWM and MALA we show\nthat Gaussian invariant sampling can lead to ergodic estimators with improved\nstatistical efficiency. This is due to a remarkable property of Gaussian\ninvariance that allows us to obtain exact analytical solutions to the Poisson\nequation for Gaussian targets. These solutions can be used to construct\nefficient and easy to use control variates for variance reduction of estimators\nunder any intractable target. We demonstrate the new samplers and estimators in\nseveral examples, including high dimensional targets in latent Gaussian models\nwhere we compare against several advanced methods and obtain state-of-the-art\nresults. We also provide theoretical results regarding geometric ergodicity,\nand an optimal scaling analysis that shows the dependence of the optimal\nacceptance rate on the Gaussianity of the target.", "comment": "29, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21511v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21511v1", "AI": {"title_translation": "高斯不变马尔可夫链蒙特卡罗", "tldr": "开发了高斯不变的MCMC采样方法，提高了统计效率，并给出了理论和实践验证。", "motivation": "现有标准随机游走 Metropolis (RWM) 和 Metropolis 调整 Langevin 算法 (MALA) 在统计效率上存在局限性，本研究旨在开发能够提高统计效率的新型采样方法。", "method": "开发了随机游走 Metropolis (RWM)、Metropolis 调整 Langevin 算法 (MALA) 和二阶 Hessian 或流形 MALA 的高斯不变版本。利用高斯不变性获得高斯目标的泊松方程精确解析解，并用这些解构建高效易用的控制变量，以减少任何难以处理目标下的估计器方差。", "result": "高斯不变采样方法能够产生具有改进统计效率的遍历估计器。新的采样器和估计器在多个示例中表现出色，包括高维潜在高斯模型，并与现有先进方法相比获得了最先进的结果。提供了关于几何遍历性的理论结果，以及显示最优接受率对目标高斯性依赖关系的最优尺度分析。", "conclusion": "高斯不变采样方法能够显著提高估计器的统计效率，通过利用高斯不变性获得的泊松方程精确解来构建有效的控制变量，从而实现方差减少。该方法在理论和实践中均表现出优越性。", "translation": "我们开发了采样方法，包括随机游走 Metropolis (RWM)、Metropolis 调整 Langevin 算法 (MALA) 和二阶 Hessian 或流形 MALA 的高斯不变版本。与标准 RWM 和 MALA 不同，我们展示了高斯不变采样可以产生具有改进统计效率的遍历估计器。这归因于高斯不变性的一种显著特性，该特性使我们能够获得高斯目标的泊松方程的精确解析解。这些解可用于构建高效且易于使用的控制变量，以减少任何难以处理的目标下的估计器方差。我们在几个示例中演示了新的采样器和估计器，包括潜在高斯模型中的高维目标，在这些模型中我们与几种先进方法进行了比较并获得了最先进的结果。我们还提供了关于几何遍历性的理论结果，以及显示最优接受率对目标高斯性依赖关系的最优尺度分析。", "summary": "本文提出了一系列高斯不变的马尔可夫链蒙特卡罗（MCMC）采样方法，包括RWM、MALA及其二阶变体。这些方法利用高斯不变性，能够获得泊松方程的精确解析解，进而构建高效的控制变量以显著提高估计器的统计效率并减少方差。通过在高维潜在高斯模型等多个示例中的验证，证明了其优于现有先进方法的性能，并提供了几何遍历性和最优尺度分析的理论支持。", "keywords": "马尔可夫链蒙特卡罗, 高斯不变性, 统计效率, 控制变量, 泊松方程", "comments": "这项工作创新性地引入了高斯不变性到MCMC采样中，通过利用其独特的数学性质，实现了对泊松方程的精确求解，从而能够构建有效的方差削减技术。这对于提高MCMC方法的统计效率具有重要意义，尤其是在处理高维和复杂目标分布时。其理论分析和在实际应用中的优异表现，使其成为MCMC领域的一个重要进展。"}}
{"id": "2506.21331", "title": "Automatic Reviewers Assignment to a Research Paper Based on Allied References and Publications Weight", "authors": ["Tamim Al Mahmud", "B M Mainul Hossain", "Dilshad Ara"], "summary": "Everyday, a vast stream of research documents is submitted to conferences,\nanthologies, journals, newsletters, annual reports, daily papers, and various\nperiodicals. Many such publications use independent external specialists to\nreview submissions. This process is called peer review, and the reviewers are\ncalled referees. However, it is not always possible to pick the best referee\nfor reviewing. Moreover, new research fields are emerging in every sector, and\nthe number of research papers is increasing dramatically. To review all these\npapers, every journal assigns a small team of referees who may not be experts\nin all areas. For example, a research paper in communication technology should\nbe reviewed by an expert from the same field. Thus, efficiently selecting the\nbest reviewer or referee for a research paper is a big challenge.\n  In this research, we propose and implement program that uses a new strategy\nto automatically select the best reviewers for a research paper. Every research\npaper contains references at the end, usually from the same area. First, we\ncollect the references and count authors who have at least one paper in the\nreferences. Then, we automatically browse the web to extract research topic\nkeywords. Next, we search for top researchers in the specific topic and count\ntheir h-index, i10-index, and citations for the first n authors. Afterward, we\nrank the top n authors based on a score and automatically browse their\nhomepages to retrieve email addresses. We also check their co-authors and\ncolleagues online and discard them from the list. The remaining top n authors,\ngenerally professors, are likely the best referees for reviewing the research\npaper.", "comment": "IEEE Conference Proceedings (5 Pages)", "pdf_url": "http://arxiv.org/pdf/2506.21331v1", "categories": ["cs.DL", "cs.CV"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.21331v1", "AI": {"title_translation": "基于相关参考文献和出版物权重的研究论文自动审稿人分配", "tldr": "本文提出并实现了一种程序，通过分析研究论文的参考文献、作者指标（h指数、i10指数、引用量）和附属关系，自动为研究论文分配审稿人。", "motivation": "随着研究论文数量的急剧增加和新研究领域的不断涌现，高效且准确地为研究论文选择最佳专家审稿人成为一个重大挑战。当前的同行评审过程难以确保分配的审稿人是特定领域的真正专家。", "method": "该研究提出并实现了一个程序，用于自动选择研究论文的最佳审稿人。具体方法包括：首先收集论文末尾的参考文献；然后统计在参考文献中至少有一篇论文的作者；接着自动浏览网络提取研究主题关键词；之后搜索特定主题的顶尖研究人员，并统计前n位作者的h指数、i10指数和引用量；随后根据得分对这n位作者进行排名；最后自动浏览他们的主页以获取电子邮件地址，并检查并排除共同作者和同事，从而确定剩余的顶尖作者为最佳审稿人。", "result": "未在摘要中提及", "conclusion": "该研究提出的程序通过利用相关参考文献、作者指标和在线信息，为研究论文自动选择最佳审稿人提供了一种新策略，旨在解决高效准确分配审稿人的挑战。", "translation": "每天，大量的研究文档被提交给会议、文集、期刊、通讯、年度报告、日报和各种期刊。许多此类出版物使用独立的外部专家来评审投稿。这个过程被称为同行评审，审稿人被称为评审员。然而，并非总是能够为评审选择最佳评审员。此外，每个领域都在出现新的研究领域，研究论文的数量正在急剧增加。为了评审所有这些论文，每个期刊都分配一个由少数评审员组成的小团队，他们可能不是所有领域的专家。例如，一篇关于通信技术的研究论文应该由来自同一领域的专家进行评审。因此，高效选择研究论文的最佳评审员是一个巨大的挑战。\n在这项研究中，我们提出并实现了一个程序，该程序使用一种新策略来自动选择研究论文的最佳审稿人。每篇研究论文末尾都包含参考文献，通常来自同一领域。首先，我们收集参考文献并统计在参考文献中至少有一篇论文的作者。然后，我们自动浏览网页以提取研究主题关键词。接下来，我们搜索特定主题的顶尖研究人员，并统计前n位作者的h指数、i10指数和引用量。之后，我们根据得分对前n位作者进行排名，并自动浏览他们的主页以获取电子邮件地址。我们还会在线检查他们的共同作者和同事，并将其从列表中排除。剩余的前n位作者，通常是教授，很可能是评审研究论文的最佳审稿人。", "summary": "本研究旨在解决为不断增长的研究论文量高效分配专家审稿人的挑战。论文提出并实现了一个程序，通过分析研究论文的参考文献、提取研究主题关键词，并识别顶尖研究人员，然后根据其h指数、i10指数和引用量等指标进行评估和排名，来自动选择最佳审稿人。该系统还会自动获取这些作者的电子邮件地址，并排除共同作者或同事，以确定最合适的审稿人。", "keywords": "自动审稿人分配, 同行评审, 研究论文, 文献计量学, 作者指标", "comments": "该论文的创新点在于将参考文献分析与量化作者指标（如h指数、i10指数和引用量）以及在线信息（如主页、共同作者）相结合，以实现自动化审稿人选择。这种方法旨在提高分配审稿人的相关性和专业性。潜在的局限性可能包括对公开在线数据的依赖，这些数据可能不完整或过时，以及自动获取电子邮件地址可能涉及的伦理考量。"}}
