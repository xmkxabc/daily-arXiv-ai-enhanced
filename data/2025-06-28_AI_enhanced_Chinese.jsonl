{"id": "2506.20770", "title": "Perry: A High-level Framework for Accelerating Cyber Deception Experimentation", "authors": ["Brian Singer", "Yusuf Saquib", "Lujo Bauer", "Vyas Sekar"], "summary": "Cyber deception aims to distract, delay, and detect network attackers with\nfake assets such as honeypots, decoy credentials, or decoy files. However,\ntoday, it is difficult for operators to experiment, explore, and evaluate\ndeception approaches. Existing tools and platforms have non-portable and\ncomplex implementations that are difficult to modify and extend. We address\nthis pain point by introducing Perry, a high-level framework that accelerates\nthe design and exploration of deception what-if scenarios. Perry has two\ncomponents: a high-level abstraction layer for security operators to specify\nattackers and deception strategies, and an experimentation module to run these\nattackers and defenders in realistic emulated networks. To translate these\nhigh-level specifications we design four key modules for Perry: 1) an action\nplanner that translates high-level actions into low-level implementations, 2)\nan observability module to translate low-level telemetry into high-level\nobservations, 3) an environment state service that enables environment agnostic\nstrategies, and 4) an attack graph service to reason about how attackers could\nexplore an environment. We illustrate that Perry's abstractions reduce the\nimplementation effort of exploring a wide variety of deception defenses,\nattackers, and environments. We demonstrate the value of Perry by emulating 55\nunique deception what-if scenarios and illustrate how these experiments enable\noperators to shed light on subtle tradeoffs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20770v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20770v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "Perry：一个加速网络欺骗实验的高级框架", "tldr": "Perry是一个高级框架，旨在通过提供抽象层和实验模块来加速网络欺骗策略的设计和探索，解决了现有工具复杂且难以修改的问题。", "motivation": "现有网络欺骗工具和平台实现复杂且不可移植，难以修改和扩展，导致操作员难以实验、探索和评估欺骗方法。", "method": "论文引入了Perry框架，包含两个核心组件：一个高级抽象层用于安全操作员指定攻击者和欺骗策略，以及一个实验模块用于在真实的模拟网络中运行这些攻击者和防御者。为实现高级规范的转换，Perry设计了四个关键模块：动作规划器、可观察性模块、环境状态服务和攻击图服务。", "result": "Perry的抽象层减少了探索各种欺骗防御、攻击者和环境的实现工作。通过模拟55个独特的欺骗假设情景，展示了Perry的价值，并揭示了这些实验如何帮助操作员理解微妙的权衡。", "conclusion": "Perry框架通过提供高级抽象和实验能力，显著降低了网络欺骗实验的复杂性，使操作员能够更有效地设计、探索和评估欺骗策略，并理解其中的权衡。", "translation": "网络欺骗旨在通过蜜罐、诱饵凭证或诱饵文件等虚假资产来分散、延迟和检测网络攻击者。然而，目前操作员难以实验、探索和评估欺骗方法。现有工具和平台实现复杂且不可移植，难以修改和扩展。我们通过引入Perry来解决这一痛点，Perry是一个高级框架，可加速欺骗假设情景的设计和探索。Perry包含两个组件：一个高级抽象层，供安全操作员指定攻击者和欺骗策略；以及一个实验模块，用于在真实的模拟网络中运行这些攻击者和防御者。为了转换这些高级规范，我们为Perry设计了四个关键模块：1）一个动作规划器，将高级动作转换为低级实现；2）一个可观察性模块，将低级遥测数据转换为高级观察结果；3）一个环境状态服务，支持环境无关的策略；以及4）一个攻击图服务，用于推断攻击者如何探索环境。我们证明了Perry的抽象层减少了探索各种欺骗防御、攻击者和环境的实现工作。我们通过模拟55个独特的欺骗假设情景来展示Perry的价值，并阐明这些实验如何使操作员能够揭示微妙的权衡。", "summary": "本文介绍了Perry，一个高级框架，旨在简化和加速网络欺骗策略的实验与评估。针对现有工具复杂且难以修改的痛点，Perry提供了一个抽象层，允许安全操作员定义攻击者和欺骗策略，并包含一个实验模块用于在模拟网络中执行。通过动作规划、可观察性、环境状态和攻击图服务等四个关键模块，Perry能够将高级规范转换为具体实现。实验证明，Perry显著降低了欺骗防御探索的复杂性，并通过模拟55个场景揭示了策略权衡。", "keywords": "网络欺骗, 实验框架, 高级抽象, 安全评估, 蜜罐", "comments": "Perry框架通过引入高级抽象层和模块化设计，有效地解决了网络欺骗实验中工具复杂性和可扩展性差的问题。其创新点在于将欺骗策略的设计和实验过程进行了高级封装，使得安全操作员能够更专注于策略本身而非底层实现细节。这对于加速网络安全领域的研发和提升防御能力具有重要意义。该框架通过模拟大量假设情景来验证其价值，展现了其在实际应用中的潜力。"}}
{"id": "2506.20800", "title": "SIMulator: SIM Tracing on a (Pico-)Budget", "authors": ["Gabriel K. Gegenhuber", "Philipp É. Frenzel", "Adrian Dabrowski"], "summary": "SIM tracing -- the ability to inspect, modify, and relay communication\nbetween a SIM card and modem -- has become a significant technique in cellular\nnetwork research. It enables essential security- and development-related\napplications such as fuzzing communication interfaces, extracting session keys,\nmonitoring hidden SIM activity (e.g., proactive SIM commands or over-the-air\nupdates), and facilitating scalable, distributed measurement platforms through\nSIM reuse. Traditionally, achieving these capabilities has relied on\nspecialized hardware, which can pose financial and logistical burdens for\nresearchers, particularly those new to the field. In this work, we show that\nfull SIM tracing functionality can be achieved using only simple, widely\navailable components, such as UART interfaces and GPIO ports. We port these\ncapabilities to low-cost microcontrollers, exemplified by the Raspberry Pi Pico\n(4~USD). Unlike other approaches, it dramatically reduces hardware complexity\nby electrically decoupling the SIM and the modem and only transferring on APDU\nlevel. By significantly reducing hardware requirements and associated costs, we\naim to make SIM tracing techniques accessible to a broader community of\nresearchers and hobbyists, fostering wider exploration and experimentation in\ncellular network research.", "comment": "Accepted Poster at WiSec 2025", "pdf_url": "http://arxiv.org/pdf/2506.20800v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20800v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "SIMulator：低成本（Pico-）SIM卡追踪", "tldr": "研究人员开发了一种使用低成本、通用组件（如树莓派Pico）进行SIM卡追踪的方法，极大地降低了硬件复杂性和成本，使SIM追踪技术更易于获取。", "motivation": "传统的SIM卡追踪依赖于专业硬件，这给研究人员带来了财务和物流负担，特别是对于该领域的新手。", "method": "通过使用UART接口和GPIO端口等简单、广泛可用的组件，并将其移植到低成本微控制器上（如树莓派Pico，4美元）。通过电气解耦SIM卡和调制解调器，并仅在APDU级别传输数据，从而显著降低了硬件复杂性。", "result": "实现了完整的SIM卡追踪功能，仅使用简单、广泛可用的组件和低成本微控制器。", "conclusion": "通过显著降低硬件要求和相关成本，旨在使SIM卡追踪技术更广泛地应用于研究人员和爱好者社区，促进蜂窝网络研究中更广泛的探索和实验。", "translation": "SIM卡追踪——检查、修改和中继SIM卡与调制解调器之间通信的能力——已成为蜂窝网络研究中的一项重要技术。它支持重要的安全和开发相关应用，例如模糊测试通信接口、提取会话密钥、监控隐藏的SIM卡活动（例如主动SIM卡命令或空中更新），并通过SIM卡复用促进可扩展的分布式测量平台。传统上，实现这些功能依赖于专用硬件，这可能给研究人员，特别是该领域的新手带来财务和物流负担。在这项工作中，我们展示了仅使用简单、广泛可用的组件，如UART接口和GPIO端口，就可以实现完整的SIM卡追踪功能。我们将这些功能移植到低成本微控制器上，例如树莓派Pico（4美元）。与其他方法不同，它通过电气解耦SIM卡和调制解调器并仅在APDU级别传输，显著降低了硬件复杂性。通过显著降低硬件要求和相关成本，我们旨在使SIM卡追踪技术更广泛地应用于研究人员和爱好者社区，促进蜂窝网络研究中更广泛的探索和实验。", "summary": "本文介绍了一种名为“SIMulator”的创新方法，旨在降低SIM卡追踪技术的门槛。传统SIM卡追踪需要昂贵的专用硬件，而SIMulator利用简单、通用组件（如UART和GPIO）以及低成本微控制器（如树莓派Pico）实现完整的SIM卡追踪功能。其关键在于通过电气解耦SIM卡和调制解调器，并在APDU级别进行数据传输，从而大幅简化了硬件设计。这项工作旨在使SIM卡追踪技术对更广泛的研究人员和爱好者群体更易于获取，从而推动蜂窝网络研究领域的进一步探索和实验。", "keywords": "SIM卡追踪, 蜂窝网络, 低成本硬件, 树莓派Pico, APDU", "comments": "本文的创新点在于提出了一个低成本、易于实现的SIM卡追踪解决方案，解决了传统方法中专用硬件昂贵且复杂的痛点。通过利用通用组件和巧妙的电气解耦设计，极大地降低了SIM卡追踪的门槛，对于推动蜂窝网络安全研究和开发具有重要意义。其重要性在于能够让更多资源有限的研究人员和业余爱好者参与到SIM卡相关的研究中，从而促进该领域的知识普及和技术创新。"}}
{"id": "2506.20806", "title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis", "authors": ["Zhonghao Zhan", "Huichi Zhou", "Hamed Haddadi"], "summary": "Graph Neural Networks (GNNs) show great promise for Network Intrusion\nDetection Systems (NIDS), particularly in IoT environments, but suffer\nperformance degradation due to distribution drift and lack robustness against\nrealistic adversarial attacks. Current robustness evaluations often rely on\nunrealistic synthetic perturbations and lack demonstrations on systematic\nanalysis of different kinds of adversarial attack, which encompass both\nblack-box and white-box scenarios. This work proposes a novel approach to\nenhance GNN robustness and generalization by employing Large Language Models\n(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These\nagents scrutinize graph structures derived from network flow data, identifying\nand potentially mitigating suspicious or adversarially perturbed elements\nbefore GNN processing. Our experiments, using a framework designed for\nrealistic evaluation and testing with a variety of adversarial attacks\nincluding a dataset collected from physical testbed experiments, demonstrate\nthat integrating LLM analysis can significantly improve the resilience of\nGNN-based NIDS against challenges, showcasing the potential of LLM agent as a\ncomplementary layer in intrusion detection architectures.", "comment": "Poster accepted at the 10th IEEE European Symposium on Security and\n  Privacy (Euro S&P 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20806v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20806v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "海报：通过基于代理的分析增强GNN在网络入侵检测中的鲁棒性", "tldr": "本研究提出了一种新颖的方法，利用大型语言模型（LLMs）作为代理，在GNN处理网络入侵检测数据之前，分析并缓解对抗性扰动，从而提高GNN的鲁棒性和泛化能力。", "motivation": "图神经网络（GNNs）在网络入侵检测系统（NIDS）中显示出巨大潜力，特别是在IoT环境中，但它们面临着由于分布漂移导致的性能下降以及对现实对抗性攻击缺乏鲁棒性的问题。当前的鲁棒性评估通常依赖不切实际的合成扰动，并且缺乏对不同类型对抗性攻击（包括黑盒和白盒场景）进行系统分析的演示。", "method": "本研究提出了一种新颖的方法，通过在代理管道中利用大型语言模型（LLMs）作为模拟网络安全专家代理，来增强GNN的鲁棒性和泛化能力。这些代理在GNN处理之前，审查从网络流数据派生出的图结构，识别并可能缓解可疑或对抗性扰动的元素。", "result": "实验使用一个为现实评估设计的框架，并利用包括从物理测试台实验收集的数据集在内的各种对抗性攻击进行测试，结果表明集成LLM分析可以显著提高基于GNN的NIDS对抗挑战的弹性。", "conclusion": "本研究展示了LLM代理作为入侵检测架构中互补层的潜力，能够显著提高GNN在网络入侵检测中对抗现实世界挑战的鲁棒性。", "translation": "图神经网络（GNNs）在网络入侵检测系统（NIDS）中显示出巨大潜力，特别是在IoT环境中，但它们面临着由于分布漂移导致的性能下降以及对现实对抗性攻击缺乏鲁棒性的问题。当前的鲁棒性评估通常依赖不切实际的合成扰动，并且缺乏对不同类型对抗性攻击（包括黑盒和白盒场景）进行系统分析的演示。本研究提出了一种新颖的方法，通过在代理管道中利用大型语言模型（LLMs）作为模拟网络安全专家代理，来增强GNN的鲁棒性和泛化能力。这些代理在GNN处理之前，审查从网络流数据派生出的图结构，识别并可能缓解可疑或对抗性扰动的元素。我们的实验使用一个为现实评估设计的框架，并利用包括从物理测试台实验收集的数据集在内的各种对抗性攻击进行测试，结果表明集成LLM分析可以显著提高基于GNN的NIDS对抗挑战的弹性，展示了LLM代理作为入侵检测架构中互补层的潜力。", "summary": "本研究旨在解决图神经网络（GNNs）在网络入侵检测系统（NIDS）中面临的鲁棒性问题，尤其是在面对现实对抗性攻击和分布漂移时。为此，论文提出了一种新颖的方法，利用大型语言模型（LLMs）作为代理，在GNN处理网络流数据之前，对图结构进行分析，以识别并缓解可疑或对抗性扰动的元素。实验结果表明，这种集成LLM分析的方法显著提高了基于GNN的NIDS的弹性，证明了LLM代理在入侵检测架构中作为补充层的有效潜力。", "keywords": "GNN, 网络入侵检测, LLM, 对抗性攻击, 鲁棒性", "comments": "该论文的创新点在于将大型语言模型（LLMs）引入到GNN的预处理阶段，作为代理来增强其在网络入侵检测中的鲁棒性。这种方法通过在GNN处理数据之前识别并缓解对抗性扰动，有效地解决了现有GNN在现实攻击下性能下降的问题。其重要性在于为GNN在安全领域的应用提供了一种新的增强策略，特别是在面对日益复杂的网络威胁时。通过使用现实世界的对抗性攻击和物理测试台数据集进行评估，增加了研究结果的可信度。"}}
{"id": "2506.20872", "title": "Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research", "authors": ["Osama Zafar", "Rosemarie Santa González", "Mina Namazi", "Alfonso Morales", "Erman Ayday"], "summary": "Data-driven agriculture, which integrates technology and data into\nagricultural practices, has the potential to improve crop yield, disease\nresilience, and long-term soil health. However, privacy concerns, such as\nadverse pricing, discrimination, and resource manipulation, deter farmers from\nsharing data, as it can be used against them. To address this barrier, we\npropose a privacy-preserving framework that enables secure data sharing and\ncollaboration for research and development while mitigating privacy risks. The\nframework combines dimensionality reduction techniques (like Principal\nComponent Analysis (PCA)) and differential privacy by introducing Laplacian\nnoise to protect sensitive information. The proposed framework allows\nresearchers to identify potential collaborators for a target farmer and train\npersonalized machine learning models either on the data of identified\ncollaborators via federated learning or directly on the aggregated\nprivacy-protected data. It also allows farmers to identify potential\ncollaborators based on similarities. We have validated this on real-life\ndatasets, demonstrating robust privacy protection against adversarial attacks\nand utility performance comparable to a centralized system. We demonstrate how\nthis framework can facilitate collaboration among farmers and help researchers\npursue broader research objectives. The adoption of the framework can empower\nresearchers and policymakers to leverage agricultural data responsibly, paving\nthe way for transformative advances in data-driven agriculture. By addressing\ncritical privacy challenges, this work supports secure data integration,\nfostering innovation and sustainability in agricultural systems.", "comment": "arXiv admin note: text overlap with arXiv:2409.06069", "pdf_url": "http://arxiv.org/pdf/2506.20872v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20872v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "赋能数字农业：一个用于数据共享和协作研究的隐私保护框架", "tldr": "本文提出了一个结合降维和差分隐私的框架，旨在解决数字农业中数据共享的隐私问题，从而实现安全协作和模型训练。该框架在真实数据集上得到了验证，证明了其强大的隐私保护能力和实用性能。", "motivation": "数据驱动的农业有潜力提高作物产量、疾病抵抗力和土壤健康。然而，隐私担忧，如不利定价、歧视和资源操纵，阻碍农民分享数据，因为这些数据可能被用来对付他们。", "method": "本文提出了一个隐私保护框架，旨在实现安全的数据共享和协作研究，同时减轻隐私风险。该框架结合了降维技术（如主成分分析PCA）和通过引入拉普拉斯噪声实现的差分隐私来保护敏感信息。它允许研究人员识别目标农民的潜在合作者，并通过联邦学习在已识别合作者的数据上或直接在聚合的隐私保护数据上训练个性化机器学习模型。此外，该框架还允许农民根据相似性识别潜在合作者。", "result": "该框架已在真实数据集上进行了验证，展示了对对抗性攻击的强大隐私保护能力，以及与中心化系统相当的实用性能。研究表明，该框架能够促进农民之间的协作，并帮助研究人员实现更广泛的研究目标。", "conclusion": "该框架的采用可以赋能研究人员和政策制定者负责任地利用农业数据，为数据驱动农业的变革性进步铺平道路。通过解决关键的隐私挑战，这项工作支持安全的数据集成，促进农业系统的创新和可持续性。", "translation": "数据驱动的农业，将技术和数据整合到农业实践中，有潜力提高作物产量、疾病抵抗力和长期土壤健康。然而，隐私担忧，如不利定价、歧视和资源操纵，阻碍农民分享数据，因为这些数据可能被用来对付他们。为了解决这一障碍，我们提出了一个隐私保护框架，该框架能够实现安全的数据共享和研究开发协作，同时减轻隐私风险。该框架结合了降维技术（如主成分分析（PCA））和通过引入拉普拉斯噪声实现的差分隐私来保护敏感信息。所提出的框架允许研究人员识别目标农民的潜在合作者，并通过联邦学习在已识别合作者的数据上或直接在聚合的隐私保护数据上训练个性化机器学习模型。它还允许农民根据相似性识别潜在合作者。我们已在真实数据集上验证了这一点，展示了对对抗性攻击的强大隐私保护能力，以及与中心化系统相当的实用性能。我们展示了该框架如何促进农民之间的协作，并帮助研究人员追求更广泛的研究目标。该框架的采用可以赋能研究人员和政策制定者负责任地利用农业数据，为数据驱动农业的变革性进步铺平道路。通过解决关键的隐私挑战，这项工作支持安全的数据集成，促进农业系统的创新和可持续性。", "summary": "本文提出了一个隐私保护框架，旨在解决数字农业中数据共享的隐私障碍。该框架创新性地结合了降维技术（如PCA）和差分隐私（通过引入拉普拉斯噪声），以实现安全的数据共享和协作研究。它不仅支持研究人员识别潜在合作者并在此基础上训练个性化机器学习模型，也允许农民根据相似性寻找合作伙伴。该框架在真实数据集上的验证结果显示，其具备强大的隐私保护能力，同时保持了与中心化系统相当的实用性能。这项工作有望促进农业数据的负责任利用，从而推动数据驱动农业的创新和可持续发展。", "keywords": "数字农业, 隐私保护, 数据共享, 差分隐私, 联邦学习", "comments": "这项工作创新性地结合了降维技术和差分隐私来解决数字农业中数据共享的实际隐私问题，这对于推动农业数字化转型至关重要。其在真实数据集上的验证增强了方案的实用性和可信度。该框架不仅关注技术实现，还考虑了农民和研究人员的实际协作需求，具有重要的应用前景和政策指导意义。"}}
{"id": "2506.21490", "title": "Ad-Hoc Human-AI Coordination Challenge", "authors": ["Tin Dizdarević", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Jakob Nicolaus Foerster"], "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21490v1", "categories": ["cs.AI", "cs.HC", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21490v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "临时人机协作挑战", "tldr": "引入AH2AC2挑战和人类代理，以解决人机协作中昂贵且难以复现的人类评估问题，并提供Hanabi游戏的基线结果和数据集。", "motivation": "在实际应用中实现AI代理和人类之间的无缝协作至关重要，但目前仍是一个重大的开放挑战。Hanabi作为人机协作的理想测试平台，其使用却因人类评估的成本高昂和难以复现而受到限制。", "method": "引入了临时人机协作挑战（AH2AC2），通过在大规模人类数据集上开发“人类代理”，使其成为AH2AC2中鲁棒、廉价且可复现的类人评估伙伴。同时，开源了一个包含3,079场游戏的受限人类游戏数据集，并提供了一个受控评估系统以确保公平评估。", "result": "提供了两人和三人Hanabi场景的基线结果。", "conclusion": "通过引入AH2AC2和人类代理，本研究为克服人机协作中昂贵且难以复现的人类评估问题提供了一个创新且可复现的解决方案，并促进了数据高效方法的发展。", "translation": "实现AI代理和人类之间的无缝协作对于实际应用至关重要，但这仍然是一个重大的开放挑战。花火（Hanabi）是一款具有不完美信息、受限通信、心智理论要求和协调行动的合作纸牌游戏——使其成为人机协作的理想测试平台。然而，由于人类评估的挑战，其在人机交互中的使用受到了限制。在这项工作中，我们引入了临时人机协作挑战（Ad-Hoc Human-AI Coordination Challenge, AH2AC2）来克服昂贵且难以复现的人类评估的限制。我们基于大规模人类数据集开发了“人类代理”（human proxy agents），它们在AH2AC2中作为鲁棒、廉价且可复现的类人评估伙伴。为了鼓励数据高效方法的发展，我们开源了一个包含3,079场游戏的数据集，并刻意限制了可用的人类游戏数据量。我们展示了两人和三人花火场景的基线结果。为确保公平评估，我们通过受控评估系统托管代理，而不是公开发布它们。代码可在https://github.com/FLAIROx/ah2ac2获取。", "summary": "这项工作提出了临时人机协作挑战（AH2AC2），旨在解决人机协作领域中昂贵且难以复现的人类评估问题。通过在大规模人类数据集上开发“人类代理”，AH2AC2提供了一种鲁棒、廉价且可复现的评估方法。研究者开源了一个受限的人类游戏数据集，并展示了Hanabi游戏的基线结果，以促进数据高效方法的开发。", "keywords": "人机协作, 人类代理, Hanabi, 评估, 数据集", "comments": "这项工作创新性地提出了“人类代理”的概念和AH2AC2挑战，有效解决了人机协作研究中人类评估成本高、复现性差的关键瓶颈。通过提供可控的评估环境和受限数据集，它有望加速数据高效型人机协作AI的发展。其重要性在于为未来的人机协作研究提供了一个标准化的、可扩展的评估框架。"}}
{"id": "2506.20782", "title": "Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing", "authors": ["Marc Bara"], "summary": "We present the first theoretical framework for applying spiking neural\nnetworks (SNNs) to synthetic aperture radar (SAR) interferometric phase\nunwrapping. Despite extensive research in both domains, our comprehensive\nliterature review confirms that SNNs have never been applied to phase\nunwrapping, representing a significant gap in current methodologies. As Earth\nobservation data volumes continue to grow exponentially (with missions like\nNISAR expected to generate 100PB in two years) energy-efficient processing\nbecomes critical for sustainable data center operations. SNNs, with their\nevent-driven computation model, offer potential energy savings of 30-100x\ncompared to conventional approaches while maintaining comparable accuracy. We\ndevelop spike encoding schemes specifically designed for wrapped phase data,\npropose SNN architectures that leverage the spatial propagation nature of phase\nunwrapping, and provide theoretical analysis of computational complexity and\nconvergence properties. Our framework demonstrates how the temporal dynamics\ninherent in SNNs can naturally model the spatial continuity constraints\nfundamental to phase unwrapping. This work opens a new research direction at\nthe intersection of neuromorphic computing and SAR interferometry, offering a\ncomplementary approach to existing algorithms that could enable more\nsustainable large-scale InSAR processing.", "comment": "8 pages, 2 figures, patent pending", "pdf_url": "http://arxiv.org/pdf/2506.20782v1", "categories": ["cs.NE", "cs.ET", "cs.LG", "eess.SP", "68T07, 94A08", "I.2.6; G.1.6; B.7.1"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20782v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "用于SAR干涉相位解缠的脉冲神经网络：一种节能处理的理论框架", "tldr": "该研究提出了首个将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，旨在为大规模InSAR处理提供节能且可持续的方法。", "motivation": "随着地球观测数据量呈指数级增长，数据中心的可持续运营对节能处理的需求变得至关重要。传统的处理方法能耗高，而脉冲神经网络（SNNs）具有显著的节能潜力（30-100倍），同时保持可比的精度，但此前从未应用于相位解缠。", "method": "本研究为缠绕相位数据开发了脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并对计算复杂度和收敛性进行了理论分析。该框架展示了SNN固有的时间动力学如何自然地模拟相位解缠中空间连续性约束。", "result": "该框架展示了脉冲神经网络（SNNs）固有的时间动力学能够自然地模拟相位解缠中基本的空间连续性约束。", "conclusion": "这项工作为神经形态计算和SAR干涉测量交叉领域开辟了新的研究方向，为现有算法提供了一种补充方法，有望实现更可持续的大规模InSAR处理。", "translation": "我们提出了首个将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架。尽管这两个领域都有广泛的研究，但我们全面的文献综述证实，SNNs从未应用于相位解缠，这代表了当前方法论中的一个显著空白。随着地球观测数据量继续呈指数级增长（像NISAR这样的任务预计在两年内生成100PB数据），节能处理对于可持续的数据中心运营变得至关重要。SNNs凭借其事件驱动的计算模型，与传统方法相比，在保持可比精度的同时，可提供30-100倍的潜在节能。我们开发了专门为缠绕相位数据设计的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并提供了计算复杂度和收敛特性的理论分析。我们的框架展示了SNN中固有的时间动力学如何自然地模拟相位解缠中基本的空间连续性约束。这项工作为神经形态计算和SAR干涉测量交叉领域开辟了新的研究方向，为现有算法提供了一种补充方法，有望实现更可持续的大规模InSAR处理。", "summary": "本研究首次提出了将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架。鉴于地球观测数据量激增对节能处理的需求，SNNs因其显著的节能潜力而备受关注。论文开发了专门的脉冲编码方案和SNN架构，并进行了理论分析，证明SNN的时间动力学能有效模拟相位解缠的空间连续性约束。这项工作为神经形态计算与SAR干涉测量结合开辟了新方向，有望实现更可持续的大规模InSAR处理。", "keywords": "脉冲神经网络, SAR干涉测量, 相位解缠, 节能处理, 理论框架", "comments": "这项工作具有显著的创新性，因为它首次将脉冲神经网络引入SAR干涉相位解缠领域，填补了现有方法论的空白。其重要性在于，它提出了一种极具前景的节能处理方案，这对于处理指数级增长的地球观测数据至关重要，有望推动大规模InSAR处理的可持续发展。该研究为神经形态计算在地球科学应用中开辟了新的研究方向。"}}
{"id": "2506.20679", "title": "Establishing validated standards for Home and Work location Detection", "authors": ["Silvia de Sojo", "Lorenzo Lucchini", "Ollin D. Langle-Chimal", "Samuel P. Fraiberger", "Laura Alessandretti"], "summary": "Smartphone location data have transformed urban mobility research, providing\nunprecedented insights into how people navigate and interact in cities.\nHowever, leveraging location data at scale presents methodological challenges.\nAccurately identifying individuals' home and work locations is critical for a\nrange of applications, including commuting analysis, unemployment estimation,\nand urban accessibility studies. Despite their widespread use, home-work\ndetection methods lack a standardized framework that accounts for differing\ndata quality and that is validated against ground-truth observations. This\nlimits the comparability and reproducibility of results across studies and\ndatasets. In this paper, we present HoWDe, a robust algorithm for identifying\nhome and work locations from mobility data, explicitly designed to handle\nmissing data and varying data quality across individuals. Using two unique\nground-truth datasets comprising over 5100 individuals from more than 80\ncountries, HoWDe achieves home and work detection accuracies of up to 97% and\n88%, respectively, with consistent performance across countries and demographic\ngroups. We examine how parameter choices shape the trade-off between accuracy\nand user retention, and demonstrate how these methodological decisions\ninfluence downstream applications such as employment estimation and commuting\npattern analysis. By supporting in-house pre-processing through a transparent\nand validated pipeline, HoWDe also facilitates the sharing of\nprivacy-preserving mobility data. Together, our tools and findings establish\nmethodological standards that support more robust, scalable, and reproducible\nmobility research at both individual and urban scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20679v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20679v1", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "建立家庭和工作地点检测的验证标准", "tldr": "HoWDe是一种鲁棒算法，用于从移动数据中识别家庭和工作地点，解决了现有方法缺乏标准化框架和地面真实验证的问题，并在全球范围内实现了高精度。", "motivation": "智能手机位置数据为城市交通研究带来了变革，但大规模利用位置数据存在方法学挑战。准确识别个人家庭和工作地点对于通勤分析、失业率估计和城市可达性研究至关重要。然而，现有的家庭-工作地点检测方法缺乏考虑数据质量差异并经过地面真实观测验证的标准化框架，这限制了研究结果的可比性和可重复性。", "method": "本论文提出了HoWDe，一种鲁棒的算法，用于从移动数据中识别家庭和工作地点。该算法明确设计用于处理缺失数据和个体间不同的数据质量。研究使用了两个独特的地面真实数据集，包含来自80多个国家的5100多名个体。", "result": "HoWDe在家庭和工作地点检测方面分别达到了高达97%和88%的准确率，并且在不同国家和人口群体中表现一致。研究还探讨了参数选择如何影响准确性和用户保留之间的权衡，并展示了这些方法学决策如何影响下游应用，如就业估计和通勤模式分析。", "conclusion": "HoWDe工具和研究结果建立了方法学标准，支持在个体和城市层面进行更稳健、可扩展和可重复的移动性研究。它通过透明和经过验证的管道支持内部预处理，促进了隐私保护移动数据的共享。", "translation": "智能手机位置数据已经改变了城市出行研究，为人们如何在城市中导航和互动提供了前所未有的洞察。然而，大规模利用位置数据带来了方法学挑战。准确识别个人的家庭和工作地点对于一系列应用至关重要，包括通勤分析、失业估计和城市可达性研究。尽管被广泛使用，但家庭-工作地点检测方法缺乏一个考虑不同数据质量并经过地面真实观测验证的标准化框架。这限制了研究结果在不同研究和数据集之间的可比性和可重复性。在本文中，我们提出了HoWDe，一种从移动数据中识别家庭和工作地点的鲁棒算法，明确设计用于处理缺失数据和个体间不同的数据质量。通过使用两个独特的地面真实数据集，包含来自80多个国家的5100多名个体，HoWDe在家庭和工作地点检测方面分别达到了高达97%和88%的准确率，并且在不同国家和人口群体中表现一致。我们研究了参数选择如何影响准确性和用户保留之间的权衡，并展示了这些方法学决策如何影响下游应用，如就业估计和通勤模式分析。通过透明和经过验证的管道支持内部预处理，HoWDe还促进了隐私保护移动数据的共享。总而言之，我们的工具和发现建立了方法学标准，支持在个体和城市层面进行更稳健、可扩展和可重复的移动性研究。", "summary": "本研究提出了一种名为HoWDe的鲁棒算法，用于从智能手机移动数据中准确识别家庭和工作地点。该算法旨在解决现有方法缺乏标准化和地面真实验证的问题，并能有效处理数据缺失和质量差异。通过在包含来自80多个国家的5100多名个体的地面真实数据集上进行验证，HoWDe在家庭和工作地点检测方面分别实现了高达97%和88%的准确率，并在不同群体中表现一致。该研究还探讨了参数选择对准确性和数据保留的影响，并强调了其对就业估计和通勤分析等下游应用的重要性。HoWDe通过提供透明且经过验证的预处理流程，为移动性研究建立了新的方法学标准，从而支持更可靠、可扩展和可重复的分析。", "keywords": "家庭工作地点检测, 移动数据, HoWDe, 验证标准, 准确性", "comments": "HoWDe的创新之处在于其明确设计用于处理移动数据中的缺失和质量差异，并首次通过大规模多国地面真实数据集进行了严格验证。这解决了现有方法在标准化、可比性和可重复性方面的关键局限性。其高准确率和在不同人口群体中的一致性表现，使其在城市规划、交通分析和人口统计等领域具有重要应用价值。此外，它支持隐私保护数据共享的潜力也值得关注。"}}
{"id": "2506.20754", "title": "Domain Knowledge in Requirements Engineering: A Systematic Mapping Study", "authors": ["Marina Araújo", "Júlia Araújo", "Romeu Oliveira", "Lucas Romao", "Marcos Kalinowski"], "summary": "[Context] Domain knowledge is recognized as a key component for the success\nof Requirements Engineering (RE), as it provides the conceptual support needed\nto understand the system context, ensure alignment with stakeholder needs, and\nreduce ambiguity in requirements specification. Despite its relevance, the\nscientific literature still lacks a systematic consolidation of how domain\nknowledge can be effectively used and operationalized in RE. [Goal] This paper\naddresses this gap by offering a comprehensive overview of existing\ncontributions, including methods, techniques, and tools to incorporate domain\nknowledge into RE practices. [Method] We conducted a systematic mapping study\nusing a hybrid search strategy that combines database searches with iterative\nbackward and forward snowballing. [Results] In total, we found 75 papers that\nmet our inclusion criteria. The analysis highlights the main types of\nrequirements addressed, the most frequently considered quality attributes, and\nrecurring challenges in the formalization, acquisition, and long-term\nmaintenance of domain knowledge. The results provide support for researchers\nand practitioners in identifying established approaches and unresolved issues.\nThe study also outlines promising directions for future research, emphasizing\nthe development of scalable, automated, and sustainable solutions to integrate\ndomain knowledge into RE processes. [Conclusion] The study contributes by\nproviding a comprehensive overview that helps to build a conceptual and\nmethodological foundation for knowledge-driven requirements engineering.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20754v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20754v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "需求工程中的领域知识：一项系统映射研究", "tldr": "该研究通过系统映射，提供了关于如何在需求工程中有效利用领域知识的全面概述，识别了现有方法、挑战和未来研究方向。", "motivation": "尽管领域知识对于需求工程的成功至关重要，但科学文献中仍缺乏关于如何有效使用和操作化领域知识的系统整合。本研究旨在填补这一空白，提供现有贡献的全面概述。", "method": "进行了一项系统映射研究，采用了混合搜索策略，结合了数据库搜索与迭代式前向和后向滚雪球方法。", "result": "共发现了75篇符合纳入标准的论文。分析强调了所解决的主要需求类型、最常考虑的质量属性，以及领域知识在形式化、获取和长期维护中反复出现的挑战。研究结果为研究人员和实践者识别既定方法和未解决问题提供了支持，并为未来研究指明了有前景的方向。", "conclusion": "本研究通过提供全面的概述，有助于为知识驱动的需求工程构建概念和方法论基础。", "translation": "[背景] 领域知识被认为是需求工程（RE）成功的关键组成部分，因为它提供了理解系统上下文、确保与利益相关者需求一致以及减少需求规范模糊性所需的概念支持。尽管其相关性，科学文献仍然缺乏对领域知识如何在需求工程中有效使用和操作化的系统整合。[目标] 本文通过提供现有贡献的全面概述，包括将领域知识纳入需求工程实践的方法、技术和工具，来解决这一空白。[方法] 我们采用混合搜索策略，结合数据库搜索与迭代式前向和后向滚雪球，进行了一项系统映射研究。[结果] 总共找到了75篇符合我们纳入标准的论文。分析强调了所解决的主要需求类型、最常考虑的质量属性，以及领域知识在形式化、获取和长期维护中反复出现的挑战。研究结果为研究人员和实践者识别既定方法和未解决问题提供了支持。该研究还概述了未来研究的有前景方向，强调开发可扩展、自动化和可持续的解决方案，以将领域知识整合到需求工程过程中。[结论] 该研究通过提供全面的概述，有助于为知识驱动的需求工程构建概念和方法论基础。", "summary": "本系统映射研究旨在弥补需求工程（RE）领域知识应用系统整合的空白。通过对75篇相关论文的分析，该研究概述了现有方法、技术和工具，并识别了领域知识在RE中面临的主要挑战，如形式化、获取和维护。研究结果为研究人员和实践者提供了关于现有方法和未解决问题的洞察，并指出了未来研究的方向，特别是在开发可扩展、自动化和可持续的领域知识集成方案方面。本研究为知识驱动的需求工程奠定了概念和方法论基础。", "keywords": "领域知识, 需求工程, 系统映射研究, 知识管理, 需求规范", "comments": "这项研究通过系统映射，有效地整合了需求工程领域中关于领域知识的现有文献，填补了该领域系统性总结的空白。其价值在于为研究人员和实践者提供了全面的知识图谱，不仅识别了成熟的方法，也清晰地指出了当前面临的挑战和未来值得探索的方向，特别是对自动化和可持续解决方案的强调，具有前瞻性。"}}
{"id": "2506.20801", "title": "IMA-Catcher: An IMpact-Aware Nonprehensile Catching Framework based on Combined Optimization and Learning", "authors": ["Francesco Tassi", "Jianzhuang Zhao", "Gustavo J. G. Lahr", "Luna Gava", "Marco Monforte", "Arren Glover", "Chiara Bartolozzi", "Arash Ajoudani"], "summary": "Robotic catching of flying objects typically generates high impact forces\nthat might lead to task failure and potential hardware damages. This is\naccentuated when the object mass to robot payload ratio increases, given the\nstrong inertial components characterizing this task. This paper aims to address\nthis problem by proposing an implicitly impact-aware framework that\naccomplishes the catching task in both pre- and post-catching phases. In the\nfirst phase, a motion planner generates optimal trajectories that minimize\ncatching forces, while in the second, the object's energy is dissipated\nsmoothly, minimizing bouncing. In particular, in the pre-catching phase, a\nreal-time optimal planner is responsible for generating trajectories of the\nend-effector that minimize the velocity difference between the robot and the\nobject to reduce impact forces during catching. In the post-catching phase, the\nrobot's position, velocity, and stiffness trajectories are generated based on\nhuman demonstrations when catching a series of free-falling objects with\nunknown masses. A hierarchical quadratic programming-based controller is used\nto enforce the robot's constraints (i.e., joint and torque limits) and create a\nstack of tasks that minimizes the reflected mass at the end-effector as a\nsecondary objective. The initial experiments isolate the problem along one\ndimension to accurately study the effects of each contribution on the metrics\nproposed. We show how the same task, without velocity matching, would be\ninfeasible due to excessive joint torques resulting from the impact. The\naddition of reflected mass minimization is then investigated, and the catching\nheight is increased to evaluate the method's robustness. Finally, the setup is\nextended to catching along multiple Cartesian axes, to prove its generalization\nin space.", "comment": "25 pages, 17 figures, accepted by International Journal of Robotics\n  Research (IJRR)", "pdf_url": "http://arxiv.org/pdf/2506.20801v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20801v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "IMA-Catcher: 基于组合优化与学习的冲击感知非抓取式捕获框架", "tldr": "本文提出了IMA-Catcher，一个冲击感知的机器人捕获框架，通过优化和学习在捕获前减少冲击力，并在捕获后平稳耗散能量，以应对高速捕获中产生的高冲击问题。", "motivation": "机器人捕获飞行物体时会产生高冲击力，可能导致任务失败和潜在的硬件损坏，尤其是在物体质量与机器人有效载荷比增加时。", "method": "本文提出了一个隐式冲击感知框架，分为捕获前和捕获后两个阶段。在捕获前阶段，一个实时最优规划器生成末端执行器轨迹，旨在最小化机器人与物体之间的速度差以减少冲击力。在捕获后阶段，基于人类演示生成机器人的位置、速度和刚度轨迹，用于捕获未知质量的自由落体物体。同时，使用一个分层二次规划控制器来强制执行机器人的关节和扭矩限制，并将最小化末端执行器反射质量作为次要目标。实验首先在一维环境中进行，以研究各贡献的影响，然后通过增加捕获高度来评估方法的鲁棒性，最后扩展到多维笛卡尔轴捕获，以证明其泛化能力。", "result": "实验结果表明，在没有速度匹配的情况下，任务会因冲击导致的关节扭矩过大而不可行。该方法通过加入反射质量最小化，并在增加捕获高度的情况下，证明了其鲁棒性。最终，该设置扩展到多维笛卡尔轴捕获，证明了其在空间中的泛化能力。", "conclusion": "本文提出的IMA-Catcher框架通过结合优化和学习，有效解决了机器人捕获中的高冲击力问题，提高了捕获任务的成功率和安全性。", "translation": "机器人捕获飞行物体通常会产生高冲击力，这可能导致任务失败和潜在的硬件损坏。当物体质量与机器人有效载荷之比增加时，考虑到表征此任务的强大惯性分量，这一点尤为突出。本文旨在通过提出一种隐式冲击感知框架来解决这个问题，该框架在捕获前和捕获后阶段都能完成捕获任务。在第一阶段，运动规划器生成最小化捕获力的最优轨迹，而在第二阶段，物体的能量被平稳耗散，从而最大程度地减少反弹。特别是在捕获前阶段，实时最优规划器负责生成末端执行器的轨迹，该轨迹最小化机器人与物体之间的速度差，以减少捕获期间的冲击力。在捕获后阶段，机器人的位置、速度和刚度轨迹是根据人类演示生成的，用于捕获一系列未知质量的自由落体物体。分层二次规划控制器用于强制执行机器人的约束（即关节和扭矩限制），并创建一系列任务，将末端执行器的反射质量最小化作为次要目标。初步实验将问题隔离到一维，以准确研究每个贡献对所提出指标的影响。我们展示了在没有速度匹配的情况下，由于冲击导致的关节扭矩过大，同一任务将变得不可行。然后研究了反射质量最小化的加入，并增加了捕获高度以评估方法的鲁棒性。最后，设置扩展到沿多个笛卡尔轴的捕获，以证明其在空间中的泛化能力。", "summary": "本文提出IMA-Catcher，一个基于优化与学习的冲击感知非抓取式机器人捕获框架，旨在解决高速捕获中高冲击力导致的任务失败和硬件损坏问题。该框架在捕获前通过优化速度匹配来减少冲击力，在捕获后利用人类演示和分层二次规划控制器平稳耗散能量并最小化反射质量。实验证明，该方法有效降低了冲击，提高了捕获的鲁棒性和泛化性。", "keywords": "机器人捕获, 冲击感知, 优化, 学习, 非抓取式", "comments": "该论文创新性地结合了优化和学习方法来解决机器人高速捕获中的核心问题——冲击力。通过在捕获前阶段进行速度匹配优化和在捕获后阶段引入反射质量最小化，有效降低了冲击并提高了系统稳定性。其分层控制器的设计也考虑了实际机器人约束，增强了实用性。该研究对于开发更鲁棒、更安全的机器人捕获系统具有重要意义。"}}
{"id": "2506.20748", "title": "Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots", "authors": ["Jingshu Li", "Zicheng Zhu", "Renwen Zhang", "Yi-Chieh Lee"], "summary": "Chatbots are increasingly integrated into people's lives and are widely used\nto help people. Recently, there has also been growing interest in the reverse\ndirection-humans help chatbots-due to a wide range of benefits including better\nchatbot performance, human well-being, and collaborative outcomes. However,\nlittle research has explored the factors that motivate people to help chatbots.\nTo address this gap, we draw on the Computers Are Social Actors (CASA)\nframework to examine how chatbot anthropomorphism-including human-like\nidentity, emotional expression, and non-verbal expression-influences human\nempathy toward chatbots and their subsequent prosocial behaviors and\nintentions. We also explore people's own interpretations of their prosocial\nbehaviors toward chatbots. We conducted an online experiment (N = 244) in which\nchatbots made mistakes in a collaborative image labeling task and explained the\nreasons to participants. We then measured participants' prosocial behaviors and\nintentions toward the chatbots. Our findings revealed that human identity and\nemotional expression of chatbots increased participants' prosocial behavior and\nintention toward chatbots, with empathy mediating these effects. Qualitative\nanalysis further identified two motivations for participants' prosocial\nbehaviors: empathy for the chatbot and perceiving the chatbot as human-like. We\ndiscuss the implications of these results for understanding and promoting human\nprosocial behaviors toward chatbots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20748v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20748v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "探索聊天机器人拟人化和人类同理心对人类对聊天机器人亲社会行为的影响", "tldr": "研究发现聊天机器人的拟人化（如人类身份和情感表达）能通过引发人类同理心来增加人类对聊天机器人的亲社会行为。", "motivation": "现有研究很少探讨促使人们帮助聊天机器人的因素，尽管人类帮助聊天机器人能带来性能提升和用户福祉等益处。", "method": "采用“计算机是社会行动者（CASA）”框架，通过一项在线实验（N=244），让聊天机器人在协作图像标注任务中犯错并解释原因，然后测量参与者对聊天机器人的亲社会行为和意图。还进行了定性分析。", "result": "聊天机器人的人类身份和情感表达增加了参与者对聊天机器人的亲社会行为和意图，其中同理心起中介作用。定性分析揭示了亲社会行为的两个动机：对聊天机器人的同理心和将聊天机器人视为类人。", "conclusion": "研究结果有助于理解和促进人类对聊天机器人的亲社会行为。", "translation": "聊天机器人越来越多地融入人们的生活，并被广泛用于帮助人们。最近，人们对相反的方向——人类帮助聊天机器人——的兴趣也日益增长，因为它带来了广泛的益处，包括更好的聊天机器人性能、人类福祉和协作成果。然而，很少有研究探讨促使人们帮助聊天机器人的因素。为了弥补这一空白，我们借鉴“计算机是社会行动者（CASA）”框架，研究聊天机器人拟人化——包括类人身份、情感表达和非语言表达——如何影响人类对聊天机器人的同理心及其随后的亲社会行为和意图。我们还探讨了人们对自己对聊天机器人亲社会行为的解释。我们进行了一项在线实验（N = 244），在实验中，聊天机器人在一项协作图像标注任务中犯了错误并向参与者解释了原因。然后我们测量了参与者对聊天机器人的亲社会行为和意图。我们的发现表明，聊天机器人的人类身份和情感表达增加了参与者对聊天机器人的亲社会行为和意图，其中同理心介导了这些影响。定性分析进一步确定了参与者亲社会行为的两个动机：对聊天机器人的同理心和将聊天机器人视为类人。我们讨论了这些结果对理解和促进人类对聊天机器人的亲社会行为的启示。", "summary": "本研究基于“计算机是社会行动者”框架，探讨了聊天机器人拟人化（人类身份、情感表达）如何通过引发人类同理心来促进人类对聊天机器人的亲社会行为和意图。在线实验（N=244）显示，聊天机器人的人类身份和情感表达显著提升了用户的亲社会行为，且同理心是关键的中介因素。定性分析进一步揭示了同理心和类人感知是促使人们帮助聊天机器人的主要动机。", "keywords": "聊天机器人拟人化, 人类同理心, 亲社会行为, CASA框架, 人机交互", "comments": "这项研究通过引入人类对聊天机器人“反向帮助”的概念，扩展了人机交互领域。它创新性地运用了CASA框架来解释拟人化和同理心在促进行为中的作用，为设计更具协作性和用户参与度的AI系统提供了重要启示。"}}
{"id": "2506.20762", "title": "Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks", "authors": ["Shisheng Hu", "Jie Gao", "Xue Qin", "Conghao Zhou", "Xinyu Huang", "Mushu Li", "Mingcheng He", "Xuemin Shen"], "summary": "In this paper, we propose a novel drift-adaptive slicing-based resource\nmanagement scheme for cooperative integrated sensing and communication (ISAC)\nnetworks. Particularly, we establish two network slices to provide sensing and\ncommunication services, respectively. In the large-timescale planning for the\nslices, we partition the sensing region of interest (RoI) of each mobile device\nand reserve network resources accordingly, facilitating low-complexity\ndistance-based sensing target assignment in small timescales. To cope with the\nnon-stationary spatial distributions of mobile devices and sensing targets,\nwhich can result in the drift in modeling the distributions and ineffective\nplanning decisions, we construct digital twins (DTs) of the slices. In each DT,\na drift-adaptive statistical model and an emulation function are developed for\nthe spatial distributions in the corresponding slice, which facilitates\nclosed-form decision-making and efficient validation of a planning decision,\nrespectively. Numerical results show that the proposed drift-adaptive\nslicing-based resource management scheme can increase the service satisfaction\nratio by up to 18% and reduce resource consumption by up to 13.1% when compared\nwith benchmark schemes.", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking", "pdf_url": "http://arxiv.org/pdf/2506.20762v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.20762v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "协同ISAC网络中基于漂移自适应切片的资源管理", "tldr": "本文提出了一种新颖的基于漂移自适应切片的协同集成感知与通信（ISAC）网络资源管理方案，通过构建数字孪生来应对移动设备和感知目标的非平稳空间分布，显著提高了服务满意度并降低了资源消耗。", "motivation": "为了应对移动设备和感知目标的非平稳空间分布导致建模漂移和规划决策无效的问题。", "method": "本文提出了一种新颖的漂移自适应切片式资源管理方案，用于协同ISAC网络。该方案建立了感知和通信两个网络切片，并在大时间尺度规划中划分感知区域并预留资源。为应对非平稳空间分布，为每个切片构建了数字孪生（DT），其中包含漂移自适应统计模型和仿真功能，以实现闭式决策和高效验证。", "result": "数值结果表明，与基准方案相比，所提出的漂移自适应切片式资源管理方案可以将服务满意度提高多达18%，并将资源消耗降低多达13.1%。", "conclusion": "所提出的漂移自适应切片式资源管理方案能够有效提升协同ISAC网络的服务满意度并降低资源消耗，通过引入数字孪生和漂移自适应模型成功应对了非平稳空间分布带来的挑战。", "translation": "在本文中，我们提出了一种新颖的漂移自适应切片式资源管理方案，用于协同集成感知与通信（ISAC）网络。特别是，我们建立了两个网络切片，分别提供感知和通信服务。在切片的大时间尺度规划中，我们划分了每个移动设备的感兴趣感知区域（RoI），并相应地预留了网络资源，从而促进了小时间尺度下基于距离的低复杂度感知目标分配。为了应对移动设备和感知目标的非平稳空间分布（这可能导致建模分布的漂移和无效的规划决策），我们构建了切片的数字孪生（DTs）。在每个DT中，分别为相应切片中的空间分布开发了漂移自适应统计模型和仿真功能，这分别有助于闭式决策和规划决策的有效验证。数值结果表明，与基准方案相比，所提出的漂移自适应切片式资源管理方案可以将服务满意度提高多达18%，并将资源消耗降低多达13.1%。", "summary": "本文提出了一种用于协同ISAC网络的新型漂移自适应切片式资源管理方案。该方案通过建立感知和通信切片，并在大时间尺度上进行资源规划。为解决移动设备和感知目标非平稳空间分布导致的建模漂移问题，引入数字孪生技术，在每个数字孪生中开发漂移自适应统计模型和仿真功能。数值结果证明，该方案能显著提高服务满意度并降低资源消耗。", "keywords": "漂移自适应, 切片, 资源管理, ISAC, 数字孪生", "comments": "该论文的创新点在于引入了“漂移自适应”的概念，并通过数字孪生技术来应对ISAC网络中移动设备和感知目标的非平稳空间分布问题。这种方法有效地解决了传统资源管理方案在动态环境中可能出现的建模漂移和规划失效问题，具有重要的理论和实际意义。其提出的切片化和数字孪生结合的方案，为未来ISAC网络的智能化管理提供了新的思路。"}}
{"id": "2506.20813", "title": "Entropic additive energy and entropy inequalities for sums and products", "authors": ["Rupert Li", "Lampros Gavalakis", "Ioannis Kontoyiannis"], "summary": "Following a growing number of studies that, over the past 15 years, have\nestablished entropy inequalities via ideas and tools from additive\ncombinatorics, in this work we obtain a number of new bounds for the\ndifferential entropy of sums, products, and sum-product combinations of\ncontinuous random variables. Partly motivated by recent work by Goh on the\ndiscrete entropic version of the notion of \"additive energy\", we introduce the\nadditive energy of pairs of continuous random variables and prove various\nversions of the statement that \"the additive energy is large if and only if the\nentropy of the sum is small\", along with a version of the\nBalog-Szemer\\'edi-Gowers theorem for differential entropy. Then, motivated in\npart by recent work by M\\'ath\\'e and O'Regan, we establish a series of new\ndifferential entropy inequalities for products and sum-product combinations of\ncontinuous random variables. In particular, we prove a new, general, ring\nPl\\\"unnecke-Ruzsa entropy inequality. We briefly return to the case of discrete\nentropy and provide a characterization of discrete random variables with \"large\ndoubling\", analogous to Tao's Freiman-type inverse sumset theory for the case\nof small doubling. Finally, we consider the natural entropic analog of the\nErd\\\"os-Szemer\\'edi sum-product phenomenon for integer-valued random variables.\nWe show that, if it does hold, then the range of parameters for which it does\nwould necessarily be significantly more restricted than its anticipated\ncombinatorial counterpart.", "comment": "26 pages, no figures", "pdf_url": "http://arxiv.org/pdf/2506.20813v1", "categories": ["cs.IT", "math.CO", "math.IT", "94A17 (Primary) 11B13 (Secondary)"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20813v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "熵加性能量与和积的熵不等式", "tldr": "本文引入了连续随机变量的熵加性能量，证明了其与和的熵之间的关系，建立了积与和积组合的新的熵不等式，并讨论了离散熵和熵和积现象。", "motivation": "本研究的动机源于过去15年里，通过加性组合学的方法和工具建立熵不等式的研究日益增多，以及Goh关于“加性能量”离散熵版本的近期工作，还有Máthé和O'Regan关于积的熵不等式的近期工作。", "method": "本文引入了连续随机变量对的加性能量，并证明了“加性能量大当且仅当和的熵小”的各种版本，以及微分熵的Balog-Szemerédi-Gowers定理的一个版本。同时，建立了连续随机变量的积及和积组合的一系列新的微分熵不等式，特别是证明了一个新的、普适的环Plünnecke-Ruzsa熵不等式。此外，还对具有“大倍增”的离散随机变量进行了特征描述，并考虑了整数值随机变量的Erdős-Szemerédi和积现象的熵模拟。", "result": "获得了连续随机变量的和、积以及和积组合的微分熵的许多新界限；证明了“加性能量大当且仅当和的熵小”的各种版本；建立了微分熵的Balog-Szemerédi-Gowers定理的一个版本；建立了一系列新的积及和积组合的微分熵不等式，包括一个新的、普适的环Plünnecke-Ruzsa熵不等式；提供了具有“大倍增”的离散随机变量的特征；并指出，如果熵Erdős-Szemerédi和积现象成立，其参数范围将比其组合对应物受到更显著的限制。", "conclusion": "本文建立了连续随机变量的熵加性能量与熵之间的关系及新的熵不等式，将加性组合学的概念扩展到熵领域。研究还对离散熵和熵和积现象提供了见解，并发现熵和积现象的参数范围比其组合对应物更受限制。", "translation": "在过去15年里，越来越多的研究通过加性组合学的思想和工具建立了熵不等式，在此背景下，本工作为连续随机变量的和、积以及和积组合的微分熵获得了许多新的界限。部分受Goh关于“加性能量”概念的离散熵版本近期工作启发，我们引入了连续随机变量对的加性能量，并证明了“加性能量大当且仅当和的熵小”这一论断的各种版本，以及微分熵的Balog-Szemerédi-Gowers定理的一个版本。接着，部分受Máthé和O'Regan近期工作的启发，我们为连续随机变量的积及和积组合建立了一系列新的微分熵不等式。特别地，我们证明了一个新的、普适的环Plünnecke-Ruzsa熵不等式。我们简要地回到离散熵的情况，并提供了具有“大倍增”的离散随机变量的特征，这类似于Tao针对小倍增情况的Freiman型逆和集理论。最后，我们考虑了整数值随机变量的Erdős-Szemerédi和积现象的自然熵模拟。我们表明，如果它确实成立，那么其成立的参数范围必然比其预期的组合对应物受到更显著的限制。", "summary": "本文将加性组合学扩展到熵领域，引入了连续随机变量的熵加性能量，并揭示了其与和的熵之间的反向关系，包括一个Balog-Szemerédi-Gowers定理的类比。研究还为积与和积组合建立了新的微分熵不等式，其中值得注意的是一个普适的环Plünnecke-Ruzsa不等式。此外，本文还对具有“大倍增”的离散随机变量进行了特征描述，并分析了熵Erdős-Szemerédi和积现象，发现其参数范围比组合对应物更为受限。", "keywords": "熵加性能量, 熵不等式, 和积现象, 加性组合学, 微分熵", "comments": "本文通过将加性组合学的核心概念（如加性能量和Balog-Szemerédi-Gowers定理）扩展到连续随机变量的微分熵领域，做出了重要贡献。环Plünnecke-Ruzsa熵不等式的引入是一项显著的创新。对熵和积现象的探索及其与组合版本的比较，突出了熵背景下重要的差异和潜在限制。这项工作连接了两个不同的数学领域，为分析随机变量的结构提供了新的工具和见解。"}}
{"id": "2506.20673", "title": "ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data", "authors": ["Yongqian Sun", "Xijie Pan", "Xiao Xiong", "Lei Tao", "Jiaju Wang", "Shenglin Zhang", "Yuan Yuan", "Yuqi Li", "Kunlin Jian"], "summary": "Network failure diagnosis is challenging yet critical for high-performance\ncomputing (HPC) systems. Existing methods cannot be directly applied to HPC\nscenarios due to data heterogeneity and lack of accuracy. This paper proposes a\nnovel framework, called ClusterRCA, to localize culprit nodes and determine\nfailure types by leveraging multimodal data. ClusterRCA extracts features from\ntopologically connected network interface controller (NIC) pairs to analyze the\ndiverse, multimodal data in HPC systems. To accurately localize culprit nodes\nand determine failure types, ClusterRCA combines classifier-based and\ngraph-based approaches. A failure graph is constructed based on the output of\nthe state classifier, and then it performs a customized random walk on the\ngraph to localize the root cause. Experiments on datasets collected by a\ntop-tier global HPC device vendor show ClusterRCA achieves high accuracy in\ndiagnosing network failure for HPC systems. ClusterRCA also maintains robust\nperformance across different application scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20673v1", "categories": ["cs.DC", "cs.AI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20673v1", "date": "2025-06-17", "updated": "2025-06-17", "AI": {"title_translation": "ClusterRCA：使用多模态数据在HPC系统中进行网络故障诊断", "tldr": "ClusterRCA是一个用于HPC系统网络故障诊断的新框架，它结合了分类器和图方法，利用多模态数据来高精度地定位故障节点并确定故障类型。", "motivation": "网络故障诊断对高性能计算（HPC）系统来说既具有挑战性又至关重要。现有方法由于数据异构性和准确性不足，无法直接应用于HPC场景。", "method": "本文提出了一个名为ClusterRCA的新框架，通过利用多模态数据来定位故障节点并确定故障类型。ClusterRCA从拓扑连接的网络接口控制器（NIC）对中提取特征，以分析HPC系统中多样化的多模态数据。为了准确地定位故障节点并确定故障类型，ClusterRCA结合了基于分类器和基于图的方法。它根据状态分类器的输出构建一个故障图，然后在图上执行定制的随机游走来定位根本原因。", "result": "在由一家顶级全球HPC设备供应商收集的数据集上进行的实验表明，ClusterRCA在诊断HPC系统网络故障方面取得了高精度，并且在不同应用场景中保持了鲁棒的性能。", "conclusion": "ClusterRCA为HPC系统提供了一种高精度、鲁棒的网络故障诊断方法，有效解决了现有方法在数据异构性和准确性方面的不足。", "translation": "网络故障诊断对于高性能计算（HPC）系统来说既具有挑战性又至关重要。由于数据异构性和准确性不足，现有方法无法直接应用于HPC场景。本文提出了一种新颖的框架，名为ClusterRCA，通过利用多模态数据来定位故障节点并确定故障类型。ClusterRCA从拓扑连接的网络接口控制器（NIC）对中提取特征，以分析HPC系统中多样化的多模态数据。为了准确地定位故障节点并确定故障类型，ClusterRCA结合了基于分类器和基于图的方法。它根据状态分类器的输出构建一个故障图，然后在图上执行定制的随机游走来定位根本原因。在由一家顶级全球HPC设备供应商收集的数据集上进行的实验表明，ClusterRCA在诊断HPC系统网络故障方面取得了高精度。ClusterRCA还在不同应用场景中保持了鲁棒的性能。", "summary": "ClusterRCA是一个针对HPC系统网络故障诊断的新型框架，旨在克服现有方法在数据异构性和准确性方面的不足。它通过从NIC对中提取特征来处理多模态数据，并结合分类器和图方法。具体而言，它构建故障图并执行定制的随机游走来定位根本原因。实验证明，ClusterRCA在HPC系统网络故障诊断中表现出高精度和鲁棒性。", "keywords": "网络故障诊断, HPC系统, 多模态数据, 故障定位, 随机游走", "comments": "ClusterRCA的创新之处在于其结合了分类器和图方法的混合诊断策略，并能有效处理HPC系统中复杂的多模态数据。其重要性体现在解决了HPC系统网络故障诊断的关键挑战，提升了诊断的准确性和鲁棒性。该方法对于维护大型复杂计算系统的稳定运行具有实际价值。"}}
{"id": "2506.20763", "title": "A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion", "authors": ["Y. Navidtehrani", "C. Betegón", "E. Martínez-Pañeda"], "summary": "We present a novel, generalised formulation to treat coupled structural\nintegrity problems by combining phase field and multi-physics modelling. The\napproach exploits the versatility of the heat transfer equation and is\ntherefore well suited to be adopted in commercial finite element packages,\nrequiring only integration point-level implementation. This aspect is\ndemonstrated here by implementing coupled, multi-variable phenomena through\nsimple \\texttt{UMAT} and \\texttt{UMATHT} subroutines in the finite element\npackage \\texttt{Abaqus}. The generalised theoretical and computational\nframework presented is particularised to four problems of engineering and\nscientific relevance: thermo-mechanical fracture, hydraulic fracture,\nhydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are\nconsidered. The results reveal a very good agreement with experimental data,\nand existing numerical and analytical solutions.The user subroutines developed\nare made freely available at https://mechmat.web.ox.ac.uk/codes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20763v1", "categories": ["cs.CE", "cs.NA", "math.NA", "physics.app-ph"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20763v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "耦合问题相场建模的通用框架：在热机械断裂、水力压裂、氢脆和腐蚀中的应用", "tldr": "本文提出了一个结合相场和多物理场建模的通用框架，用于处理耦合结构完整性问题，并演示了其在商用有限元软件中的实现，成功应用于热机械断裂、水力压裂、氢辅助开裂和金属腐蚀等问题，结果与实验数据和现有解决方案高度吻合。", "motivation": "解决耦合结构完整性问题，并提供一个通用的、易于在商用有限元软件包中实现的建模方法。", "method": "提出了一种结合相场和多物理场建模的通用公式，该方法利用热传递方程的通用性，并通过在有限元软件包Abaqus中实现简单的UMAT和UMATHT子程序来演示其在集成点级别的实现。该框架被应用于热机械断裂、水力压裂、氢辅助开裂和金属腐蚀四种工程和科学相关问题。", "result": "所提出的框架在2D和3D问题上的结果显示与实验数据以及现有的数值和解析解具有非常好的一致性。", "conclusion": "本文提出了一个通用的理论和计算框架，能够有效地处理各种耦合结构完整性问题，并通过在商用有限元软件包中的实现展示了其广泛的适用性和准确性。", "translation": "我们提出了一种新颖的通用公式，通过结合相场和多物理场建模来处理耦合结构完整性问题。该方法利用了热传递方程的通用性，因此非常适合在商用有限元软件包中采用，仅需要集成点级别的实现。通过在有限元软件包Abaqus中通过简单的UMAT和UMATHT子程序实现耦合多变量现象，本文展示了这一点。所提出的通用理论和计算框架被具体应用于四个具有工程和科学相关性的问题：热机械断裂、水力压裂、氢辅助开裂和金属腐蚀。考虑了2D和3D问题。结果显示与实验数据以及现有的数值和解析解具有非常好的一致性。开发的用户子程序可在https://mechmat.web.ox.ac.uk/codes免费获取。", "summary": "本文提出了一个创新的通用框架，结合相场和多物理场建模来解决复杂的耦合结构完整性问题。该方法基于热传递方程的通用性，易于在Abaqus等商用有限元软件中通过用户子程序实现。研究将此框架成功应用于热机械断裂、水力压裂、氢辅助开裂和金属腐蚀等多种工程问题，并在2D和3D条件下验证了其结果与实验数据及现有解决方案的高度吻合，展示了其广泛的适用性和准确性。", "keywords": "相场建模, 耦合问题, 有限元, 结构完整性, 多物理场", "comments": "本文的创新点在于提出了一个高度通用且易于在商用有限元软件中实现的耦合问题相场建模框架。通过利用热传递方程的通用性，并提供具体的UMAT/UMATHT子程序实现，极大地降低了复杂多物理场耦合问题建模的门槛。其对多种实际工程问题的成功应用及其与实验数据的高度吻合，证明了该方法的实用性和鲁棒性。"}}
{"id": "2506.20685", "title": "Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan", "Naima Iltaf", "Ihtesham ul Islam"], "summary": "Federated Learning (FL) has emerged as a transformative paradigm for\ndistributed machine learning while preserving data privacy. However, existing\napproaches predominantly focus on model heterogeneity and aggregation\ntechniques, largely overlooking the fundamental impact of dataset size\ncharacteristics on federated training dynamics. This paper introduces\nSize-Based Adaptive Federated Learning (SAFL), a novel progressive training\nframework that systematically organizes federated learning based on dataset\nsize characteristics across heterogeneous multi-modal data. Our comprehensive\nexperimental evaluation across 13 diverse datasets spanning 7 modalities\n(vision, text, time series, audio, sensor, medical vision, and multimodal)\nreveals critical insights: 1) an optimal dataset size range of 1000-1500\nsamples for federated learning effectiveness; 2) a clear modality performance\nhierarchy with structured data (time series, sensor) significantly\noutperforming unstructured data (text, multimodal); and 3) systematic\nperformance degradation for large datasets exceeding 2000 samples. SAFL\nachieves an average accuracy of 87.68% across all datasets, with structured\ndata modalities reaching 99%+ accuracy. The framework demonstrates superior\ncommunication efficiency, reducing total data transfer to 7.38 GB across 558\ncommunications while maintaining high performance. Our real-time monitoring\nframework provides unprecedented insights into system resource utilization,\nnetwork efficiency, and training dynamics. This work fills critical gaps in\nunderstanding how data characteristics should drive federated learning\nstrategies, providing both theoretical insights and practical guidance for\nreal-world FL deployments in neural network and learning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20685v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20685v1", "date": "2025-06-24", "updated": "2025-06-24", "AI": {"title_translation": "渐进式规模自适应联邦学习：异构多模态数据系统的综合框架", "tldr": "本文提出了SAFL，一个基于数据集大小对联邦学习进行系统组织的新型渐进式训练框架，解决了现有联邦学习方法忽视数据集大小对训练动态影响的问题，并在多模态数据上取得了优异的性能和通信效率。", "motivation": "现有的联邦学习方法主要关注模型异构性和聚合技术，但很大程度上忽略了数据集大小特性对联邦训练动态的根本性影响。", "method": "本文提出了基于大小的自适应联邦学习（SAFL），这是一个新颖的渐进式训练框架，它根据异构多模态数据的数据集大小特性系统地组织联邦学习。", "result": "实验结果显示：1) 联邦学习效果的最佳数据集大小范围为1000-1500样本；2) 结构化数据（时间序列、传感器）的性能明显优于非结构化数据（文本、多模态）；3) 超过2000样本的大数据集性能会系统性下降。SAFL在所有数据集上平均准确率达到87.68%，结构化数据模态达到99%+准确率，并将总数据传输量减少到7.38 GB（558次通信）。", "conclusion": "这项工作填补了理解数据特性如何驱动联邦学习策略的关键空白，为实际的联邦学习部署提供了理论见解和实践指导。", "translation": "联邦学习（FL）已成为一种在保护数据隐私的同时实现分布式机器学习的变革性范式。然而，现有方法主要关注模型异构性和聚合技术，很大程度上忽视了数据集大小特性对联邦训练动态的根本性影响。本文引入了基于大小的自适应联邦学习（SAFL），这是一个新颖的渐进式训练框架，它根据异构多模态数据的数据集大小特性系统地组织联邦学习。我们对跨越7种模态（视觉、文本、时间序列、音频、传感器、医学视觉和多模态）的13个不同数据集进行的综合实验评估揭示了关键见解：1) 联邦学习效果的最佳数据集大小范围为1000-1500样本；2) 存在清晰的模态性能层次结构，其中结构化数据（时间序列、传感器）的性能显著优于非结构化数据（文本、多模态）；3) 超过2000样本的大数据集性能会系统性下降。SAFL在所有数据集上平均准确率达到87.68%，其中结构化数据模态达到99%+的准确率。该框架展示了卓越的通信效率，在558次通信中将总数据传输量减少到7.38 GB，同时保持了高性能。我们的实时监控框架为系统资源利用率、网络效率和训练动态提供了前所未有的洞察。这项工作填补了理解数据特性应如何驱动联邦学习策略的关键空白，为神经网络和学习系统在实际联邦学习部署中提供了理论见解和实践指导。", "summary": "本文提出了一种名为“基于大小的自适应联邦学习”（SAFL）的新型渐进式训练框架，旨在解决现有联邦学习（FL）方法忽略数据集大小对训练动态影响的问题。SAFL根据数据集大小特性系统地组织联邦学习，并在13个数据集和7种模态上进行了全面评估。实验发现，1000-1500样本是联邦学习的最佳数据集大小范围，结构化数据性能优于非结构化数据，且大数据集性能会下降。SAFL实现了高准确率（平均87.68%），显著提高了通信效率，并提供了系统资源和训练动态的实时洞察，为实际FL部署提供了宝贵的理论和实践指导。", "keywords": "联邦学习, 数据集大小, 多模态数据, 自适应学习, 通信效率", "comments": "本文的创新点在于首次系统地研究了数据集大小对联邦学习性能和效率的影响，并提出了SAFL框架来解决这一问题。其重要性在于填补了联邦学习领域的一个关键空白，为未来的FL部署提供了基于数据特性的优化策略。特别是对最佳数据集大小范围的发现以及不同模态性能的揭示，具有重要的实践指导意义。该研究不仅提供了理论见解，还通过实时监控框架提供了实际操作层面的支持。"}}
{"id": "2506.20982", "title": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers", "authors": ["Martin Ruskov"], "summary": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings.", "comment": "accepted at D-SAIL Workshop - Transformative Curriculum Design:\n  Digitalization, Sustainability, and AI Literacy for 21st Century Learning", "pdf_url": "http://arxiv.org/pdf/2506.20982v1", "categories": ["cs.CY", "cs.RO", "K.3.1"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20982v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "我们的编程冒险：使用大型语言模型为学龄前儿童的实体编程机器人定制叙事", "tldr": "该研究提出了一种使用大型语言模型为学龄前儿童的实体编程机器人Cubetto生成个性化故事的方法，旨在帮助教师创建定制化教学内容，同时避免儿童直接接触LLM。", "motivation": "在教育领域平衡使用大型语言模型（LLMs）面临挑战，尤其对于理解技术能力有限且易受影响的年幼儿童，他们还面临过度屏幕时间的问题。本研究旨在探索如何利用LLMs的能力，同时规避这些风险，特别是为学龄前儿童的实体编程机器人提供个性化叙事。", "method": "研究团队采用行动研究方法，开发了一种早期形式化的流程，用于为实体编程机器人Cubetto快速原型设计游戏故事。他们利用LLMs生成个性化故事，并使用开放权重模型确保结果可复现。该方法是模型无关的，通过测试5种不同的LLMs进行了验证。研究记录了过程、材料、提示以及学习经验和成果，并测试了模型在4种不同任务场景下的表现。", "result": "研究发现，LLMs生成的故事成功达到了作为教师辅助工具的预期目的。然而，在测试中也遇到了内容一致性和幻觉问题，研究记录了相应的评估过程以及克服这些问题的尝试（部分成功）。重要的是，该过程不直接让儿童接触LLMs，而是帮助教师轻松开发基于儿童偏好主题的个性化叙事。", "conclusion": "研究人员认为他们的方法适用于学龄前课堂，并计划在真实的教育环境中进行进一步的实验。该方法通过间接使用LLMs，成功地为学龄前儿童的实体编程机器人提供了个性化叙事支持，同时解决了直接接触LLMs的潜在风险。", "translation": "在教育领域平衡使用大型语言模型（LLMs）是一个挑战，因为存在对技术理解不足和受众易受影响的固有风险。对于年幼儿童来说尤其如此，他们已知在普遍的屏幕时间方面存在困难。我们与名为Cubetto的实体编程机器人合作，提出了一种方法，通过在个性化故事准备中使用LLMs来利用其能力，这对于学龄前儿童适应指挥机器人的实践是必要的。我们进行行动研究，开发了一个早期形式化的流程，用于快速原型设计Cubetto的游戏故事。我们的方法既有可复现的结果，因为它使用了开放权重模型，又是模型无关的，因为我们用5种不同的LLMs进行了测试。我们一方面记录了过程、使用的材料和提示，另一方面记录了学习经验和成果。我们认为生成对于将结果用作教师辅助工具的预期目的是成功的。在4种不同任务场景下测试模型时，我们遇到了内容一致性和幻觉问题，并记录了相应的评估过程和克服这些问题的尝试（有些成功，有些不成功）。重要的是，这个过程不会让儿童直接接触LLMs。相反，这项技术用于帮助教师轻松开发关于儿童偏好主题的个性化叙事。我们相信我们的方法适用于学龄前课堂，我们计划在真实的教育环境中进一步实验。", "summary": "本研究提出了一种利用大型语言模型（LLMs）为学龄前儿童的实体编程机器人Cubetto生成个性化教学叙事的方法。旨在解决LLMs在儿童教育中应用面临的风险和屏幕时间问题。通过行动研究和多模型测试，该方法帮助教师快速创建定制化故事，虽然遇到了一致性和幻觉问题，但仍被视为有效的教师辅助工具，且避免了儿童直接接触LLMs。研究认为此方法适用于学龄前教育，并计划进行实地验证。", "keywords": "大型语言模型, 个性化叙事, 实体编程机器人, 学龄前教育, 教师辅助", "comments": "该论文的创新点在于提出了一个间接使用LLMs的教育框架，旨在为学龄前儿童提供个性化学习体验，同时规避了LLMs直接暴露给低龄儿童的潜在风险。其重要性体现在为实体编程教育引入了定制化叙事，有助于提升学习兴趣和效率。然而，研究也明确指出了LLMs在生成内容时的一致性和幻觉问题，这表明在实际应用中仍需教师进行干预和修正，是未来研究需要克服的限制。"}}
{"id": "2506.20728", "title": "Distributed Lyapunov Functions for Nonlinear Networks", "authors": ["Yiming Wang", "Arthur N. Montanari", "Adilson E. Motter"], "summary": "Nonlinear networks are often multistable, exhibiting coexisting stable states\nwith competing regions of attraction (ROAs). As a result, ROAs can have complex\n\"tentacle-like\" morphologies that are challenging to characterize analytically\nor computationally. In addition, the high dimensionality of the state space\nprohibits the automated construction of Lyapunov functions using\nstate-of-the-art optimization methods, such as sum-of-squares (SOS)\nprogramming. In this letter, we propose a distributed approach for the\nconstruction of Lyapunov functions based solely on local information. To this\nend, we establish an augmented comparison lemma that characterizes the\nexistence conditions of partial Lyapunov functions, while also accounting for\nresidual effects caused by the associated dimensionality reduction. These\ntheoretical results allow us to formulate an SOS optimization that iteratively\nconstructs such partial functions, whose aggregation forms a composite Lyapunov\nfunction. The resulting composite function provides accurate convex\napproximations of both the volumes and shapes of the ROAs. We validate our\nmethod on networks of van der Pol and Ising oscillators, demonstrating its\neffectiveness in characterizing high-dimensional systems with non-convex ROAs.", "comment": "Codes are available at our GitHub repository\n  https://github.com/YimingSci/Distributed-Lya-Func", "pdf_url": "http://arxiv.org/pdf/2506.20728v1", "categories": ["eess.SY", "cond-mat.dis-nn", "cs.SY", "math.DS"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20728v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "非线性网络的分布式李雅普诺夫函数", "tldr": "本文提出了一种基于局部信息的分布式方法来构建李雅普诺夫函数，以准确近似高维非线性网络中复杂吸引域的体积和形状，有效解决了传统方法在高维系统中的局限性。", "motivation": "非线性网络通常是多稳态的，其吸引域（ROAs）形态复杂且难以分析或计算表征。此外，状态空间的高维性使得使用现有最先进的优化方法（如平方和（SOS）编程）自动构建李雅普诺夫函数变得不可能。", "method": "本文提出了一种基于局部信息的分布式李雅普诺夫函数构建方法。通过建立一个增广比较引理，该引理刻画了部分李雅普诺夫函数的存在条件并考虑了维度降低的残余效应。基于此理论结果，研究者制定了一个SOS优化问题，迭代地构建这些部分函数，并将它们聚合形成一个复合李雅普诺夫函数。", "result": "所构建的复合李雅普诺夫函数能够对吸引域的体积和形状提供准确的凸近似。该方法在van der Pol和Ising振荡器网络上的验证表明，其在刻画具有非凸吸引域的高维系统方面是有效的。", "conclusion": "本文成功提出了一种分布式方法，用于构建非线性网络的李雅普诺夫函数，克服了高维状态空间下吸引域表征的挑战，并能准确近似吸引域的体积和形状，为复杂非线性系统的稳定性分析提供了新工具。", "translation": "非线性网络通常是多稳态的，表现出共存的稳定状态和相互竞争的吸引域（ROA）。因此，吸引域可能具有复杂的“触手状”形态，难以进行分析或计算表征。此外，状态空间的高维性使得使用最先进的优化方法（如平方和（SOS）编程）自动构建李雅普诺夫函数变得不可能。在本文中，我们提出了一种基于局部信息的分布式方法来构建李雅普诺夫函数。为此，我们建立了一个增广比较引理，它刻画了部分李雅普诺夫函数的存在条件，同时考虑了由相关维度降低引起的残余效应。这些理论结果使我们能够制定一个SOS优化问题，迭代地构建这些部分函数，它们的聚合形成一个复合李雅普诺夫函数。由此产生的复合函数提供了对吸引域的体积和形状的精确凸近似。我们在van der Pol和Ising振荡器网络上验证了我们的方法，证明了其在刻画具有非凸吸引域的高维系统方面的有效性。", "summary": "本文针对非线性网络中吸引域（ROA）难以表征的问题，提出了一种基于局部信息的分布式李雅普诺夫函数构建方法。该方法通过建立增广比较引理来确定部分李雅普诺夫函数的存在条件，并利用平方和（SOS）优化迭代地构建这些部分函数，最终聚合形成一个复合李雅普诺夫函数。实验结果表明，该复合函数能够准确地凸近似ROA的体积和形状，并在高维非凸ROA系统中展现出有效性。", "keywords": "分布式李雅普诺夫函数, 非线性网络, 吸引域, 平方和优化, 局部信息", "comments": "本文的创新之处在于提出了一个分布式框架来构建李雅普诺夫函数，有效解决了高维非线性系统中吸引域表征的复杂性问题。通过利用局部信息和SOS优化，该方法克服了传统方法的局限性，为非线性系统的稳定性分析提供了新的工具。其贡献在于能够准确近似吸引域的形状和体积，对控制理论和复杂系统分析具有重要意义。"}}
{"id": "2506.20741", "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20741v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20741v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "OTSurv：一种用于生存预测的异质性感知最优传输多实例学习框架", "tldr": "OTSurv是一个新的多实例学习（MIL）框架，它利用最优传输（OT）来解决全玻片图像（WSIs）生存预测中存在的病理异质性问题，通过引入全局长尾和局部不确定性感知约束，实现了最先进的性能和高可解释性。", "motivation": "现有的多实例学习（MIL）方法在利用全玻片图像（WSIs）进行生存预测时，未能明确捕捉病理异质性，包括全局的长尾形态分布和局部的瓦片级预测不确定性。", "method": "提出OTSurv框架，将生存预测表述为一个异质性感知最优传输问题。该问题包含两个核心约束：1）全局长尾约束，通过调节传输质量分配来建模先验形态分布，以避免模式崩溃和过度均匀性；2）局部不确定性感知约束，通过逐步增加总传输质量来优先处理高置信度补丁并抑制噪声。该框架将初始OT问题重铸为非平衡OT形式，并使用高效且硬件友好的矩阵缩放算法求解。", "result": "OTSurv在六个流行的基准测试中取得了新的最先进结果，平均C-index绝对提高了3.6%。此外，OTSurv在对数秩检验中达到了统计显著性。", "conclusion": "OTSurv提供高可解释性，使其成为数字病理学中生存预测的强大工具。", "translation": "使用全玻片图像（WSIs）进行生存预测可以被表述为一个多实例学习（MIL）问题。然而，现有的MIL方法往往未能明确捕捉WSI内的病理异质性，无论是全局的（通过长尾形态分布），还是局部的（通过瓦片级预测不确定性）。最优传输（OT）通过结合边缘分布约束，提供了一种建模这种异质性的原则性方法。基于这一见解，我们提出了OTSurv，一个从最优传输角度出发的新型MIL框架。具体来说，OTSurv将生存预测表述为一个异质性感知的OT问题，并带有两个约束：（1）全局长尾约束，通过调节传输质量分配来建模先验形态分布，以避免模式崩溃和过度均匀性；（2）局部不确定性感知约束，通过逐步提高总传输质量来优先处理高置信度补丁并抑制噪声。然后，我们将最初的OT问题，通过这些约束进行增强，重铸为一种非平衡OT形式，可以使用高效且硬件友好的矩阵缩放算法求解。经验上，OTSurv在六个流行的基准测试中取得了新的最先进结果，平均C-index绝对提高了3.6%。此外，OTSurv在对数秩检验中达到了统计显著性，并提供了高可解释性，使其成为数字病理学中生存预测的强大工具。我们的代码可在https://github.com/Y-Research-SBU/OTSurv获得。", "summary": "OTSurv是一种新颖的多实例学习框架，用于利用全玻片图像进行生存预测。它通过异质性感知最优传输方法，解决了现有MIL方法未能捕捉病理异质性的问题。该框架引入了全局长尾约束和局部不确定性感知约束，有效建模了形态分布并处理了预测不确定性。实验结果表明，OTSurv在多个基准测试中显著提升了生存预测的性能和可解释性。", "keywords": "多实例学习, 生存预测, 最优传输, 病理异质性, 全玻片图像", "comments": "OTSurv的创新之处在于将最优传输理论引入多实例学习框架，以解决全玻片图像中复杂的病理异质性问题。通过引入全局长尾和局部不确定性感知约束，它不仅提高了预测精度，还增强了模型的可解释性，这对于临床应用具有重要价值。其在多个基准测试中取得的最先进结果证明了其有效性和潜力。"}}
{"id": "2506.20702", "title": "The Singapore Consensus on Global AI Safety Research Priorities", "authors": ["Yoshua Bengio", "Tegan Maharaj", "Luke Ong", "Stuart Russell", "Dawn Song", "Max Tegmark", "Lan Xue", "Ya-Qin Zhang", "Stephen Casper", "Wan Sie Lee", "Sören Mindermann", "Vanessa Wilfred", "Vidhisha Balachandran", "Fazl Barez", "Michael Belinsky", "Imane Bello", "Malo Bourgon", "Mark Brakel", "Siméon Campos", "Duncan Cass-Beggs", "Jiahao Chen", "Rumman Chowdhury", "Kuan Chua Seah", "Jeff Clune", "Juntao Dai", "Agnes Delaborde", "Nouha Dziri", "Francisco Eiras", "Joshua Engels", "Jinyu Fan", "Adam Gleave", "Noah Goodman", "Fynn Heide", "Dan Hendrycks", "Cyrus Hodes", "Bryan Low Kian Hsiang", "Minlie Huang", "Sami Jawhar", "Wang Jingyu", "Adam Tauman Kalai", "Meindert Kamphuis", "Mohan Kankanhalli", "Subhash Kantamneni", "Mathias Bonde Kirk", "Thomas Kwa", "Jeffrey Ladish", "Kwok-Yan Lam", "Wan Lee Sie", "Taewhi Lee", "Xiaojian Li", "Jiajun Liu", "Chaochao Lu", "Yifan Mai", "Richard Mallah", "Julian Michael", "Nick Moës", "Simon Möller", "Kihyuk Nam", "Kwan Yee Ng", "Mark Nitzberg", "Besmira Nushi", "Seán O hÉigeartaigh", "Alejandro Ortega", "Pierre Peigné", "James Petrie", "Benjamin Prud'Homme", "Reihaneh Rabbany", "Nayat Sanchez-Pi", "Sarah Schwettmann", "Buck Shlegeris", "Saad Siddiqui", "Aradhana Sinha", "Martín Soto", "Cheston Tan", "Dong Ting", "Robert Trager", "Brian Tse", "Anthony Tung K. H.", "Vanessa Wilfred", "John Willes", "Denise Wong", "Wei Xu", "Rongwu Xu", "Yi Zeng", "HongJiang Zhang", "Djordje Žikelić"], "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).", "comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report", "pdf_url": "http://arxiv.org/pdf/2506.20702v1", "categories": ["cs.AI", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20702v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "新加坡全球人工智能安全研究优先事项共识", "tldr": "新加坡会议汇集全球AI科学家，旨在识别并整合AI安全研究重点，并将其分为开发、评估和控制三大领域，以构建可信赖的AI生态系统。", "motivation": "确保AI安全（可信、可靠、安全）对于构建信任生态系统至关重要，这有助于人们自信地拥抱AI，最大化创新空间，同时避免负面影响。", "method": "通过举办“2025新加坡人工智能会议（SCAI）：国际人工智能安全科学交流”，汇集全球AI科学家，以识别和综合AI安全研究优先事项。该报告基于Yoshua Bengio主持的国际AI安全报告，并采纳了纵深防御模型。", "result": "形成了一份报告，将AI安全研究领域组织为三类：创建可信AI系统面临的挑战（开发）、评估其风险面临的挑战（评估）以及部署后监控和干预面临的挑战（控制）。", "conclusion": "报告通过对AI安全研究领域进行结构化分类（开发、评估、控制），为全球AI安全研究提供了优先事项和框架，旨在促进可信赖AI生态系统的构建。", "translation": "快速发展的人工智能能力和自主性预示着巨大的变革前景，但也引发了关于如何确保人工智能安全，即值得信赖、可靠和安全的热烈讨论。因此，建立一个值得信赖的生态系统至关重要——它有助于人们自信地拥抱人工智能，为创新提供最大的空间，同时避免负面影响。“2025新加坡人工智能会议（SCAI）：人工智能安全国际科学交流”旨在通过汇集全球人工智能科学家，识别和综合人工智能安全研究优先事项，从而支持这一领域的研究。这份由此产生的报告以Yoshua Bengio主持并由33个政府支持的《国际人工智能安全报告》为基础。通过采用纵深防御模型，本报告将人工智能安全研究领域分为三类：创建可信赖人工智能系统面临的挑战（开发）、评估其风险面临的挑战（评估）以及部署后监控和干预面临的挑战（控制）。", "summary": "这篇报告基于“2025新加坡人工智能会议”的成果，旨在应对AI能力快速发展带来的安全挑战。它通过汇集全球AI科学家，识别并综合了AI安全研究的优先事项，并将这些领域划分为开发、评估和控制三大类，以构建一个可信赖、可靠且安全的AI生态系统。", "keywords": "AI安全, 研究优先事项, 新加坡共识, 可信AI, 纵深防御模型", "comments": "这份报告的重要性在于它提供了一个全球性的、结构化的AI安全研究框架，通过新加坡会议汇集了国际共识，并在此基础上将复杂的AI安全挑战划分为可操作的“开发、评估、控制”三大领域，这对于指导未来的AI安全研究和政策制定具有重要意义。"}}
{"id": "2506.20683", "title": "Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG", "authors": ["Alexander Selivanov", "Philip Müller", "Özgün Turgut", "Nil Stolt-Ansó", "Daniel Rückert"], "summary": "An electrocardiogram (ECG) is a widely used, cost-effective tool for\ndetecting electrical abnormalities in the heart. However, it cannot directly\nmeasure functional parameters, such as ventricular volumes and ejection\nfraction, which are crucial for assessing cardiac function. Cardiac magnetic\nresonance (CMR) is the gold standard for these measurements, providing detailed\nstructural and functional insights, but is expensive and less accessible. To\nbridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive\nLearning), a multimodal contrastive learning framework that enhances ECG\nrepresentations by integrating spatio-temporal information from CMR. PTACL uses\nglobal patient-level contrastive loss and local temporal-level contrastive\nloss. The global loss aligns patient-level representations by pulling ECG and\nCMR embeddings from the same patient closer together, while pushing apart\nembeddings from different patients. Local loss enforces fine-grained temporal\nalignment within each patient by contrasting encoded ECG segments with\ncorresponding encoded CMR frames. This approach enriches ECG representations\nwith diagnostic information beyond electrical activity and transfers more\ninsights between modalities than global alignment alone, all without\nintroducing new learnable weights. We evaluate PTACL on paired ECG-CMR data\nfrom 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL\nachieves better performance in two clinically relevant tasks: (1) retrieving\npatients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac\nfunction parameters, such as ventricular volumes and ejection fraction. Our\nresults highlight the potential of PTACL to enhance non-invasive cardiac\ndiagnostics using ECG. The code is available at:\nhttps://github.com/alsalivan/ecgcmr", "comment": "accepted to MICCAI 2025 (Springer LNCS)", "pdf_url": "http://arxiv.org/pdf/2506.20683v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20683v1", "date": "2025-06-24", "updated": "2025-06-24", "AI": {"title_translation": "心脏MRI和心电图联合表征的全局与局部对比学习", "tldr": "该研究提出了PTACL，一个多模态对比学习框架，通过结合CMR的时空信息来增强ECG表征，从而在心脏表型检索和功能参数预测方面优于基线方法。", "motivation": "心电图（ECG）成本低廉但无法直接测量心脏功能参数，而心脏磁共振（CMR）虽是金标准但昂贵且不易获取。本研究旨在弥合这一差距，通过整合CMR信息来增强ECG的诊断能力。", "method": "本研究提出了PTACL（Patient and Temporal Alignment Contrastive Learning），一个多模态对比学习框架。它使用全局患者级对比损失来对齐来自同一患者的ECG和CMR嵌入，并推开不同患者的嵌入。同时，使用局部时间级对比损失，通过对比编码的ECG片段与对应的编码CMR帧，来强制执行每个患者内部的精细时间对齐，且不引入新的可学习权重。", "result": "在英国生物样本库中27,951名受试者的配对ECG-CMR数据上评估了PTACL。与基线方法相比，PTACL在以下两个临床相关任务中取得了更好的性能：1）检索具有相似心脏表型的患者；2）预测CMR衍生的心脏功能参数，如心室容积和射血分数。", "conclusion": "PTACL框架有潜力利用ECG增强非侵入性心脏诊断，通过结合CMR信息显著丰富了ECG表征的诊断信息，超越了单纯的电活动，并实现了模态间更深入的洞察力转移。", "translation": "心电图（ECG）是一种广泛使用、经济高效的工具，用于检测心脏的电异常。然而，它无法直接测量功能参数，如心室容积和射血分数，这些参数对于评估心脏功能至关重要。心脏磁共振（CMR）是这些测量的金标准，提供详细的结构和功能洞察，但成本高昂且可及性较低。为了弥合这一差距，我们提出了PTACL（Patient and Temporal Alignment Contrastive Learning），一个多模态对比学习框架，通过整合来自CMR的时空信息来增强ECG表征。PTACL使用全局患者级对比损失和局部时间级对比损失。全局损失通过将同一患者的ECG和CMR嵌入拉近，同时推开不同患者的嵌入来对齐患者级表征。局部损失通过对比编码的ECG片段与相应的编码CMR帧，在每个患者内部强制执行精细的时间对齐。这种方法用超出电活动的诊断信息丰富了ECG表征，并且比单独的全局对齐在模态之间转移了更多的洞察力，所有这些都没有引入新的可学习权重。我们在英国生物样本库中27,951名受试者的配对ECG-CMR数据上评估了PTACL。与基线方法相比，PTACL在两个临床相关任务中取得了更好的性能：1）检索具有相似心脏表型的患者；2）预测CMR衍生的心脏功能参数，如心室容积和射血分数。我们的结果突出了PTACL利用ECG增强非侵入性心脏诊断的潜力。代码可在：https://github.com/alsalivan/ecgcmr 获得。", "summary": "本研究提出了一种名为PTACL的多模态对比学习框架，旨在结合心脏磁共振（CMR）数据来增强心电图（ECG）的诊断能力，以弥补ECG在功能参数测量上的不足和CMR的高成本与低可及性。PTACL通过全局患者级和局部时间级对比损失，实现了ECG与CMR在患者和时间层面的对齐，从而丰富了ECG的表征。实验结果表明，PTACL在检索相似心脏表型患者和预测CMR衍生的心脏功能参数方面，均优于现有基线方法，突显了其在非侵入性心脏诊断中的应用潜力。", "keywords": "对比学习, 心电图, 心脏磁共振, 多模态, 心脏功能", "comments": "该论文的创新点在于提出了PTACL框架，通过结合全局和局部对比学习，有效地将昂贵的CMR数据中的空间和时间信息融入到经济的ECG表征中，显著提升了ECG的诊断价值。这种跨模态信息融合的方法，在不引入额外可学习参数的情况下，提高了心脏表型检索和功能参数预测的准确性，对于推动非侵入性心脏诊断具有重要意义。"}}
{"id": "2506.20783", "title": "Precise Near-Field Beam Training with DFT Codebook based on Amplitude-only Measurement", "authors": ["Zijun Wang", "Shawn Tsai", "Rama Kiran", "Rui Zhang"], "summary": "Extremely large antenna arrays (ELAAs) operating in high-frequency bands have\nspurred the development of near-field communication, driving advancements in\nbeam training and signal processing design. In this work, we present a\nlow-complexity near-field beam training scheme that fully utilizes the\nconventional discrete Fourier transform (DFT) codebook designed for far-field\nusers. We begin by analyzing the received beam pattern in the near field and\nderive closed-form expressions for the beam width and central gain. These\nanalytical results enable the definition of an angle-dependent, modified\nRayleigh distance, which effectively distinguishes near-field and far-field\nuser regimes. Building on the analysis, we develop a direct and computationally\nefficient method to estimate user distance, with a complexity of O(1), and\nfurther improve its accuracy through a simple refinement. Simulation results\ndemonstrate significant gains in both single- and multi-user settings, with up\nto 2.38 dB SNR improvement over exhaustive search. To further enhance\nestimation accuracy, we additionally propose a maximum likelihood estimation\n(MLE) based refinement method, leveraging the Rician distribution of signal\namplitudes and achieving accuracy close to the Cramer--Rao bound (CRB).\nSimulation shows the single-user and multi-user achievable rates can both\napproach those obtained with ideal channel state information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20783v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20783v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基于幅度测量的DFT码本精确近场波束训练", "tldr": "本文提出了一种低复杂度的近场波束训练方案，利用现有DFT码本，通过分析近场波束模式、提出O(1)复杂度的距离估计方法和基于MLE的精确细化，实现了接近理想CSI的性能。", "motivation": "极大规模天线阵列（ELAAs）在高频段运行促进了近场通信的发展，对波束训练和信号处理设计提出了新挑战。", "method": "本文提出了一种低复杂度的近场波束训练方案，充分利用了为远场用户设计的传统离散傅里叶变换（DFT）码本。首先分析了近场接收波束模式，推导了波束宽度和中心增益的闭合表达式。这些分析结果使得能够定义一个与角度相关的修正瑞利距离，有效区分近场和远场用户区域。在此基础上，开发了一种直接且计算高效的用户距离估计方法，复杂度为O(1)，并通过简单的细化提高了精度。为进一步提高估计精度，额外提出了一种基于最大似然估计（MLE）的细化方法，利用信号幅度的莱斯分布，实现了接近克拉默-拉奥下界（CRB）的精度。", "result": "仿真结果表明，在单用户和多用户设置中均获得了显著增益，信噪比（SNR）比穷举搜索提高了高达2.38 dB。仿真显示，单用户和多用户可达速率均能接近通过理想信道状态信息（CSI）获得的结果。", "conclusion": "所提出的低复杂度近场波束训练方案，通过利用传统DFT码本和先进的距离估计及细化方法，实现了接近理想信道状态信息的性能，显著提高了近场通信的波束训练效率和精度。", "translation": "极大规模天线阵列（ELAAs）在高频段运行，促进了近场通信的发展，推动了波束训练和信号处理设计的进步。在这项工作中，我们提出了一种低复杂度的近场波束训练方案，该方案充分利用了为远场用户设计的传统离散傅里叶变换（DFT）码本。我们首先分析了近场中的接收波束模式，并推导了波束宽度和中心增益的闭合表达式。这些分析结果使得能够定义一个与角度相关的修正瑞利距离，该距离有效地区分了近场和远场用户区域。在此分析的基础上，我们开发了一种直接且计算高效的用户距离估计方法，其复杂度为O(1)，并通过简单的细化进一步提高了其精度。仿真结果表明，在单用户和多用户设置中均获得了显著增益，信噪比（SNR）比穷举搜索提高了高达2.38 dB。为了进一步提高估计精度，我们额外提出了一种基于最大似然估计（MLE）的细化方法，该方法利用信号幅度的莱斯分布，实现了接近克拉默-拉奥下界（CRB）的精度。仿真显示，单用户和多用户可达速率均能接近通过理想信道状态信息获得的结果。", "summary": "该论文提出了一种创新的低复杂度近场波束训练方案，该方案巧妙地利用了为远场设计的传统DFT码本。通过对近场波束模式的深入分析，定义了修正瑞利距离以区分近远场用户。核心贡献在于开发了一种计算效率极高的O(1)复杂度用户距离估计方法，并辅以简单的细化。为追求更高精度，还引入了基于莱斯分布的MLE细化方法，使其性能接近CRB。仿真结果证实了该方案在单用户和多用户场景下的显著性能提升，信噪比增益高达2.38 dB，且可达速率接近理想信道信息水平。", "keywords": "近场通信, 波束训练, DFT码本, 幅度测量, 极大规模天线阵列", "comments": "该论文的创新点在于其能够将为远场设计的传统DFT码本应用于复杂的近场环境，从而在保持低复杂度的同时实现高精度波束训练。特别是O(1)复杂度的距离估计方法和基于MLE的精确细化，是其显著的技术贡献。这种方法为未来大规模MIMO系统在近场区域的实际部署提供了有前景的解决方案，尤其是在资源受限的应用中。"}}
{"id": "2506.20703", "title": "Generative Blocks World: Moving Things Around in Pictures", "authors": ["Vaibhav Vavilala", "Seemandhar Jain", "Rahul Vasanth", "D. A. Forsyth", "Anand Bhattad"], "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.", "comment": "23 pages, 16 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.20703v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20703v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "生成式积木世界：在图片中移动物体", "tldr": "该论文介绍了“生成式积木世界”，通过操作简单的几何抽象来编辑生成图像的场景。该方法将场景表示为凸面3D基元的组合，并通过基于流的方法生成图像，该方法以深度和纹理提示为条件，从而提高了视觉保真度、可编辑性和组合泛化能力。", "motivation": "该论文旨在通过操纵简单的几何抽象来与生成图像的场景进行交互，允许编辑者移动整个结构或小细节，并解决现有纹理一致性技术的不足。", "method": "该方法将场景表示为凸面3D基元的组合，同一个场景可以用不同数量的基元表示。场景几何编辑后，图像通过基于流的方法生成，该方法以深度和纹理提示为条件。纹理提示考虑了修改后的3D基元，超越了现有键值缓存技术提供的纹理一致性。", "result": "该方法允许精确的物体和相机移动，并在很大程度上保留了所描绘物体的身份。定量和定性实验表明，该方法在视觉保真度、可编辑性和组合泛化方面优于现有工作。", "conclusion": "该论文提出的“生成式积木世界”方法通过操纵简单的几何抽象，有效实现了对生成图像场景的交互和编辑，在视觉保真度、可编辑性和组合泛化方面均优于现有方法。", "translation": "我们描述了生成式积木世界，通过操纵简单的几何抽象来与生成图像的场景进行交互。我们的方法将场景表示为凸面3D基元的组合，同一个场景可以用不同数量的基元表示，允许编辑者移动整个结构或小细节。一旦场景几何被编辑，图像将通过基于流的方法生成，该方法以深度和纹理提示为条件。我们的纹理提示考虑了修改后的3D基元，超越了现有键值缓存技术提供的纹理一致性。这些纹理提示(a)允许精确的物体和相机移动，(b)在很大程度上保留了所描绘物体的身份。定量和定性实验表明，我们的方法在视觉保真度、可编辑性和组合泛化方面优于现有工作。", "summary": "本文提出了“生成式积木世界”，一种通过操纵简单3D几何基元来编辑生成图像场景的方法。该方法将场景表示为可变数量的凸面3D基元，支持从整体到细节的编辑。图像生成采用基于流的方法，并结合了创新的纹理提示，该提示考虑了修改后的3D基元，有效提升了纹理一致性。实验证明，该方法在视觉保真度、可编辑性和组合泛化能力上均优于现有技术，同时能准确移动物体和相机并保持物体身份。", "keywords": "生成式积木世界, 图像编辑, 3D基元, 纹理一致性, 流模型", "comments": "该论文的创新之处在于其“生成式积木世界”概念，通过将场景抽象为可操作的3D基元，极大地提升了生成图像的可编辑性。特别是其改进的纹理提示机制，超越了传统的缓存技术，在编辑过程中保持了卓越的纹理一致性和物体身份，这对于实现逼真且可控的图像编辑至关重要。其对视觉保真度、可编辑性和组合泛化能力的全面提升，使其在图像生成和编辑领域具有重要意义。"}}
{"id": "2506.21074", "title": "CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via Dynamic Frame Rate", "authors": ["Hankun Wang", "Yiwei Guo", "Chongtian Shao", "Bohan Li", "Xie Chen", "Kai Yu"], "summary": "Neural speech codecs have been widely used in audio compression and various\ndownstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which\nallocate the same number of tokens to every equal-duration slice. However,\nspeech is inherently non-uniform in temporal information density. As a result,\nmany tokens are wasted on steady-state segments like long vowels and silences.\nTo address this mismatch, we present CodecSlime, a plugin-style method for\ncompressing temporal redundancy through supporting dynamic frame rate (DFR) on\nneural speech codecs for the first time. Our method is unsupervised and\narchitecture-agnostic, combining two key innovations, ScheDFR and\nMelt-and-Cool, for adapting inference and training, respectively. When\nintegrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR\n($\\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to\n46% relative to conventional FFR baselines with the same model architecture and\nsimilar bitrates, while other metrics are also competitive. CodecSlime also\nenables flexible trade-offs between reconstruction quality and bitrate: a\nsingle model supports inference at multiple frame rates and consistently\noutperforms FFR models at the corresponding frame rates. Audio samples are\navailable at https://acadarmeria.github.io/codecslime/.", "comment": "16 pages, 5 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2506.21074v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21074v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CodecSlime：通过动态帧率实现神经语音编解码器的时间冗余压缩", "tldr": "神经语音编解码器由于固定帧率，在稳态语音段上浪费了大量token。CodecSlime首次引入动态帧率（DFR）来压缩时间冗余，从而在相同或更低的比特率下实现更好的重建质量或更低的比特率。", "motivation": "当前主流的神经语音编解码器采用固定帧率（FFR），为每个等时长片段分配相同数量的token。然而，语音在时间信息密度上本质上是非均匀的。因此，许多token被浪费在像长元音和静音这样的稳态段上。", "method": "CodecSlime是一种插件式方法，首次通过支持神经语音编解码器上的动态帧率（DFR）来压缩时间冗余。该方法是无监督和架构无关的，结合了ScheDFR和Melt-and-Cool两项关键创新，分别用于推理和训练的适应。", "result": "当集成到典型的VQ-GAN编解码器主干并以40 Hz DFR（约600 bps）运行时，CodecSlime的重建WER相对于具有相同模型架构和相似比特率的传统FFR基线降低了高达46%，同时其他指标也具有竞争力。CodecSlime还支持重建质量和比特率之间的灵活权衡：单个模型支持以多个帧率进行推理，并在相应帧率下始终优于FFR模型。", "conclusion": "CodecSlime通过支持动态帧率有效减少了神经语音编解码器的时间冗余，将重建词错误率（WER）降低了高达46%，并实现了质量与比特率之间的灵活权衡，显著优于固定帧率模型。", "translation": "神经语音编解码器已广泛应用于音频压缩和各种下游任务。当前主流编解码器是固定帧率（FFR），它们为每个等时长片段分配相同数量的token。然而，语音在时间信息密度上本质上是非均匀的。因此，许多token被浪费在像长元音和静音这样的稳态段上。为了解决这种不匹配，我们提出了CodecSlime，这是一种插件式方法，首次通过支持神经语音编解码器上的动态帧率（DFR）来压缩时间冗余。我们的方法是无监督和架构无关的，结合了ScheDFR和Melt-and-Cool两项关键创新，分别用于推理和训练的适应。当集成到典型的VQ-GAN编解码器主干并以40 Hz DFR（约600 bps）运行时，CodecSlime的重建WER相对于具有相同模型架构和相似比特率的传统FFR基线降低了高达46%，同时其他指标也具有竞争力。CodecSlime还支持重建质量和比特率之间的灵活权衡：单个模型支持以多个帧率进行推理，并在相应帧率下始终优于FFR模型。音频样本可在https://acadarmeria.github.io/codecslime/获取。", "summary": "CodecSlime通过引入动态帧率（DFR）方法，解决了固定帧率（FFR）神经语音编解码器在均匀语音段上浪费token的低效率问题。这种无监督、架构无关的插件方法，结合了ScheDFR和Melt-and-Cool，显著减少了时间冗余。实验表明，CodecSlime在相似比特率下，相对于FFR基线，重建WER降低了高达46%，并允许灵活的质量-比特率权衡。", "keywords": "神经语音编解码器, 动态帧率, 时间冗余, 语音压缩, VQ-GAN", "comments": "CodecSlime引入了一种新颖且实用的方法，通过解决时间冗余问题来提高神经语音编解码器的效率。其插件式、无监督和架构无关的特性使其具有高度适应性。词错误率（WER）的显著降低以及灵活的比特率-质量权衡表明了其在语音压缩和相关任务中的潜在影响。"}}
{"id": "2506.20677", "title": "Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions", "authors": ["Shrinivass Arunachalam Balasubramanian"], "summary": "Sorting is an essential operation in computer science with direct\nconsequences on the performance of large scale data systems, real-time systems,\nand embedded computation. However, no sorting algorithm is optimal under all\ndistributions of data. The new adaptive hybrid sorting paradigm proposed in\nthis paper is the paradigm that automatically selects the most effective\nsorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time\nmonitoring of patterns in input data. The architecture begins by having a\nfeature extraction module to compute significant parameters such as data\nvolume, value range and entropy. These parameters are sent to a decision engine\ninvolving Finite State Machine and XGBoost classifier to aid smart and\neffective in choosing the optimal sorting strategy. It implements Counting Sort\non small key ranges, Radix Sort on large range structured input with\nlow-entropy keys and QuickSort on general purpose sorting. The experimental\nfindings of both synthetic and real life dataset confirm that the proposed\nsolution is actually inclined to excel significantly by comparison in execution\ntime, flexibility and the efficiency of conventional static sorting algorithms.\nThe proposed framework provides a scalable, high perhaps and applicable to a\nwide range of data processing operations like big data analytics, edge\ncomputing, and systems with hardware limitations.", "comment": "11 Pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20677v1", "categories": ["cs.DS", "cs.DB", "cs.PF"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20677v1", "date": "2025-06-22", "updated": "2025-06-22", "AI": {"title_translation": "自适应混合排序：针对不同数据分布的动态策略选择以实现最佳排序", "tldr": "本文提出一种自适应混合排序范式，通过实时监测输入数据模式，动态选择计数排序、基数排序或快速排序，以优化不同数据分布下的排序性能。", "motivation": "现有的排序算法无法在所有数据分布下都达到最优性能，因此需要一种能根据数据特性动态选择最佳排序策略的方案。", "method": "提出一种自适应混合排序范式。该架构包含特征提取模块（计算数据量、值范围、熵等参数）和决策引擎（包含有限状态机和XGBoost分类器）。决策引擎根据提取的参数智能选择最佳排序策略：小键范围使用计数排序，大范围低熵结构化输入使用基数排序，通用排序使用快速排序。", "result": "在合成数据集和真实数据集上的实验结果表明，所提出的解决方案在执行时间、灵活性和效率方面明显优于传统的静态排序算法。", "conclusion": "所提出的框架提供了一个可扩展、高性能且适用于大数据分析、边缘计算和硬件受限系统等广泛数据处理操作的解决方案。", "translation": "排序是计算机科学中一项基本操作，直接影响大规模数据系统、实时系统和嵌入式计算的性能。然而，没有一种排序算法能在所有数据分布下都达到最优。本文提出的新型自适应混合排序范式，是一种能根据输入数据模式的实时监测，自动选择最有效的排序算法（计数排序、基数排序或快速排序）的范式。该架构首先通过特征提取模块计算数据量、值范围和熵等重要参数。这些参数被发送到包含有限状态机和XGBoost分类器的决策引擎，以辅助智能有效地选择最优排序策略。它在小键范围上实现计数排序，在大范围低熵结构化输入上实现基数排序，在通用排序上实现快速排序。合成数据集和真实数据集的实验结果证实，所提出的解决方案在执行时间、灵活性和效率方面，与传统的静态排序算法相比，确实表现出显著的优势。所提出的框架提供了一个可扩展、高性能且适用于大数据分析、边缘计算和硬件受限系统等广泛数据处理操作的解决方案。", "summary": "本文提出了一种名为“自适应混合排序”的新范式，旨在解决现有排序算法无法在所有数据分布下都达到最优性能的问题。该方法通过特征提取模块实时监测输入数据的特性（如数据量、值范围和熵），并利用包含有限状态机和XGBoost分类器的决策引擎，动态选择最适合的排序算法（计数排序、基数排序或快速排序）。实验结果表明，该自适应方法在执行时间、灵活性和效率方面显著优于传统静态排序算法，并适用于大数据分析和边缘计算等多种应用场景。", "keywords": "自适应排序, 混合排序, 动态策略选择, 数据分布, XGBoost", "comments": "这篇论文的创新点在于提出了一个动态自适应的排序策略选择框架，通过实时分析数据特性来优化排序性能，而非依赖单一的静态算法。其结合特征提取和机器学习（XGBoost）进行决策的思路具有前瞻性，提升了排序算法的普适性和效率，对于需要处理多样化数据分布的系统具有重要意义。"}}
{"id": "2506.20817", "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation", "authors": ["Ali Tourani", "Fatemeh Nazary", "Yashar Deldjoo"], "summary": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec", "comment": "20 pages, 6 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.20817v1", "categories": ["cs.IR", "cs.MM"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20817v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "RAG-VisualRec：一个用于推荐系统中视觉和文本增强检索增强生成开放资源", "tldr": "本文介绍了RAG-VisualRec，一个结合LLM生成的剧情描述和预告片视觉嵌入，以增强电影推荐系统中的检索增强生成和协同过滤的开放资源，尤其在元数据有限的情况下表现出色。", "motivation": "电影领域有限的元数据（如标题、类型）阻碍了鲁棒推荐系统的生成，因此需要开发多模态推荐系统来解决这一挑战。", "method": "引入了一个结合LLM生成的剧情描述和预告片衍生的视觉嵌入的统一管道，支持检索增强生成（RAG）和协同过滤。核心方法包括数据增强步骤（将稀疏元数据转化为更丰富的文本信号）和融合策略（如PCA、CCA）来整合视觉线索。", "result": "实验评估表明，与单模态基线相比，基于CCA的融合显著提高了召回率。LLM驱动的重排序步骤进一步改善了NDCG，特别是在文本数据有限的情况下。", "conclusion": "通过发布RAG-VisualRec框架，作者旨在邀请进一步探索针对冷启动、新颖性聚焦和领域特定设置的多模态推荐技术。", "translation": "本文旨在解决电影领域开发多模态推荐系统所面临的挑战，在该领域，有限的元数据（例如，标题、类型）通常会阻碍鲁棒推荐的生成。我们引入了一种资源，它将LLM生成的剧情描述与预告片衍生的视觉嵌入结合在一个统一的管道中，支持检索增强生成（RAG）和协同过滤。我们方法的核心是一个数据增强步骤，它将稀疏元数据转换为更丰富的文本信号，同时采用融合策略（例如，PCA、CCA）来整合视觉线索。实验评估表明，与单模态基线相比，基于CCA的融合显著提高了召回率，而LLM驱动的重排序步骤进一步改善了NDCG，特别是在文本数据有限的情况下。通过发布此框架，我们邀请进一步探索针对冷启动、新颖性聚焦和领域特定设置的多模态推荐技术。所有代码、数据和详细文档均公开可用：https://github.com/RecSys-lab/RAG-VisualRec", "summary": "本文提出了RAG-VisualRec，一个针对电影推荐的多模态开放资源。它通过结合LLM生成的剧情描述和预告片视觉嵌入，解决了元数据稀疏的问题。该框架利用数据增强和CCA等融合策略，在RAG和协同过滤中提升了推荐性能，尤其在召回率和NDCG方面表现出色。研究成果旨在推动冷启动、新颖性及特定领域的多模态推荐技术发展。", "keywords": "多模态推荐, 检索增强生成, 视觉嵌入, 数据增强, 协同过滤", "comments": "该论文通过结合文本和视觉信息，为多模态推荐系统提供了一个有价值的开放资源，尤其在解决电影领域元数据稀疏问题上具有创新性。其数据增强和多模态融合策略是亮点，为提升推荐质量提供了有效途径。该资源的开放性将有助于社区进一步研究和开发。"}}
{"id": "2506.20747", "title": "Towards Probabilistic Question Answering Over Tabular Data", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20747v1", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20747v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "迈向表格数据的概率问答", "tldr": "提出LUCARIO基准和框架，用于表格数据的概率问答，通过贝叶斯网络和LLM实现混合推理，显著优于基线。", "motivation": "现有表格数据问答系统（如NL2SQL）在事实性问题上表现良好，但在需要不确定性推理的概率性问题上表现不足。", "method": "引入新的基准LUCARIO和一个概率问答框架。该方法从表格中推断贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。", "result": "实验结果表明，与基线相比有显著改进。", "conclusion": "强调了混合符号-神经推理的优势。", "translation": "当前针对表格数据的问答（QA）方法，例如NL2SQL系统，在答案可以直接从表格中检索的事实性问题上表现良好。然而，它们在需要不确定性推理的概率性问题上表现不足。在本文中，我们引入了一个新的基准LUCARIO和一个用于大型表格数据概率问答的框架。我们的方法从表格中推断贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。实证结果表明，与基线相比有显著改进，突出了混合符号-神经推理的优势。", "summary": "本文针对现有表格数据问答系统在处理概率性问题上的不足，提出了一个新的基准LUCARIO和一个概率问答框架。该框架通过从表格中构建贝叶斯网络，将自然语言问题转化为概率查询，并结合大型语言模型生成答案，实现了表格数据的概率问答。实验证明，该方法在性能上显著优于现有基线，展现了混合符号-神经推理的有效性。", "keywords": "表格数据问答, 概率问答, 贝叶斯网络, 大型语言模型, LUCARIO", "comments": "这篇论文的创新点在于提出了一个专门针对表格数据概率问答的框架和基准，解决了现有系统在处理不确定性推理方面的局限性。其结合贝叶斯网络（符号推理）和大型语言模型（神经推理）的方法是一种有前景的混合方法，有望在复杂问答场景中取得突破。"}}
{"id": "2506.20777", "title": "Inverse initial data reconstruction for Maxwell's equations via time-dimensional reduction method", "authors": ["Thuy T. Le", "Cong B. Van", "Trong D. Dang", "Loc H. Nguyen"], "summary": "We study an inverse problem for the time-dependent Maxwell system in an\ninhomogeneous and anisotropic medium. The objective is to recover the initial\nelectric field $\\mathbf{E}_0$ in a bounded domain $\\Omega \\subset\n\\mathbb{R}^3$, using boundary measurements of the electric field and its normal\nderivative over a finite time interval. Informed by practical constraints, we\nadopt an under-determined formulation of Maxwell's equations that avoids the\nneed for initial magnetic field data and charge density information. To address\nthis inverse problem, we develop a time-dimension reduction approach by\nprojecting the electric field onto a finite-dimensional Legendre\npolynomial-exponential basis in time. This reformulates the original space-time\nproblem into a sequence of spatial systems for the projection coefficients. The\nreconstruction is carried out using the quasi-reversibility method within a\nminimum-norm framework, which accommodates the inherent non-uniqueness of the\nunder-determined setting. We prove a convergence theorem that ensures the\nquasi-reversibility solution approximates the true solution as the noise and\nregularization parameters vanish. Numerical experiments in a fully\nthree-dimensional setting validate the method's performance. The reconstructed\ninitial electric field remains accurate even with $10\\%$ noise in the data,\ndemonstrating the robustness and applicability of the proposed approach to\nrealistic inverse electromagnetic problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20777v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20777v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "麦克斯韦方程组初始数据逆重构的时间维度归约方法", "tldr": "本文提出一种时间维度归约方法，通过边界测量在非均匀各向异性介质中重构麦克斯韦方程组的初始电场，即使在有噪声数据下也表现出鲁棒性。", "motivation": "在非均匀和各向异性介质中，需要通过边界测量重构麦克斯韦方程组的初始电场。现有的方法可能需要初始磁场数据和电荷密度信息，而本文旨在避免这些需求，采用欠定公式。", "method": "1. 采用麦克斯韦方程组的欠定公式，避免初始磁场数据和电荷密度信息。 2. 开发时间维度归约方法，将电场投影到有限维勒让德多项式-指数基上。 3. 将原始时空问题转化为一系列空间系统。 4. 使用最小范数框架内的准可逆性方法进行重构。 5. 证明了收敛定理。", "result": "证明了准可逆解在噪声和正则化参数消失时逼近真实解。在完全三维设置下的数值实验验证了方法的性能。即使数据中存在10%的噪声，重构的初始电场仍然准确。", "conclusion": "提出的时间维度归约方法能够鲁棒且准确地重构麦克斯韦方程组的初始电场，适用于实际的逆电磁问题，即使在存在噪声的情况下。", "translation": "我们研究了非均匀和各向异性介质中随时间变化的麦克斯韦系统的一个逆问题。目标是利用在有限时间间隔内的电场及其法向导数的边界测量，恢复有界域 $\\Omega \\subset \\mathbb{R}^3$ 中的初始电场 $\\mathbf{E}_0$。根据实际约束，我们采用了麦克斯韦方程组的欠定公式，避免了对初始磁场数据和电荷密度信息的需要。为了解决这个逆问题，我们开发了一种时间维度归约方法，通过将电场投影到时间上的有限维勒让德多项式-指数基上。这使得原始时空问题被重新表述为一系列针对投影系数的空间系统。重构是在最小范数框架内使用准可逆性方法进行的，该方法适应了欠定设置固有的非唯一性。我们证明了一个收敛定理，确保当噪声和正则化参数消失时，准可逆解逼近真实解。在完全三维设置下的数值实验验证了该方法的性能。即使数据中存在10%的噪声，重构的初始电场仍然准确，这表明所提出方法对实际逆电磁问题的鲁棒性和适用性。", "summary": "本文研究了在非均匀和各向异性介质中利用边界测量重构麦克斯韦方程组初始电场的逆问题。为避免对初始磁场和电荷密度数据的需求，作者提出了一种时间维度归约方法，将电场投影到有限维基上，并将时空问题转化为空间系统序列。重构采用最小范数框架下的准可逆性方法。数值实验表明，该方法即使在10%噪声下也能准确重构初始电场，验证了其鲁棒性和实用性。", "keywords": "麦克斯韦方程组, 逆问题, 初始数据重构, 时间维度归约, 准可逆性方法", "comments": "这篇论文的创新之处在于提出了一个时间维度归约方法来解决麦克斯韦方程组的初始数据逆问题，特别是采用了欠定公式以避免对初始磁场和电荷密度数据的依赖，这在实际应用中具有重要意义。该方法通过将复杂的时空问题转化为一系列空间问题，并结合准可逆性方法处理非唯一性，提高了计算效率和鲁棒性。其在噪声数据下的良好表现进一步证实了其实用价值。"}}
{"id": "2506.20945", "title": "A Multi-Stage Framework for Multimodal Controllable Speech Synthesis", "authors": ["Rui Niu", "Weihao Wu", "Jie Chen", "Long Ma", "Zhiyong Wu"], "summary": "Controllable speech synthesis aims to control the style of generated speech\nusing reference input, which can be of various modalities. Existing face-based\nmethods struggle with robustness and generalization due to data quality\nconstraints, while text prompt methods offer limited diversity and fine-grained\ncontrol. Although multimodal approaches aim to integrate various modalities,\ntheir reliance on fully matched training data significantly constrains their\nperformance and applicability. This paper proposes a 3-stage multimodal\ncontrollable speech synthesis framework to address these challenges. For face\nencoder, we use supervised learning and knowledge distillation to tackle\ngeneralization issues. Furthermore, the text encoder is trained on both\ntext-face and text-speech data to enhance the diversity of the generated\nspeech. Experimental results demonstrate that this method outperforms\nsingle-modal baseline methods in both face based and text prompt based speech\nsynthesis, highlighting its effectiveness in generating high-quality speech.", "comment": "Accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2506.20945v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.20945v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种多模态可控语音合成的多阶段框架", "tldr": "本文提出了一种三阶段多模态可控语音合成框架，通过结合监督学习、知识蒸馏和多源数据训练，解决了现有方法在鲁棒性、泛化性和多样性方面的局限性，实现了高质量的语音合成。", "motivation": "现有的基于面部的方法受数据质量限制，在鲁棒性和泛化性方面表现不佳；文本提示方法在多样性和细粒度控制方面有限；多模态方法则受限于对完全匹配训练数据的依赖。", "method": "本文提出一个三阶段多模态可控语音合成框架。针对面部编码器，采用监督学习和知识蒸馏解决泛化性问题。文本编码器则在文本-面部和文本-语音数据上进行训练，以增强生成语音的多样性。", "result": "实验结果表明，该方法在基于面部和基于文本提示的语音合成方面，均优于单模态基线方法。", "conclusion": "该多阶段多模态框架有效解决了现有可控语音合成方法的局限性，能够生成高质量的语音。", "translation": "可控语音合成旨在利用各种模态的参考输入来控制生成语音的风格。现有的基于面部的方法由于数据质量限制，在鲁棒性和泛化性方面存在困难，而文本提示方法提供的多样性和细粒度控制有限。尽管多模态方法旨在整合各种模态，但它们对完全匹配训练数据的依赖显著限制了其性能和适用性。本文提出一个三阶段多模态可控语音合成框架来解决这些挑战。对于面部编码器，我们使用监督学习和知识蒸馏来解决泛化问题。此外，文本编码器在文本-面部和文本-语音数据上进行训练，以增强生成语音的多样性。实验结果表明，该方法在基于面部和基于文本提示的语音合成方面均优于单模态基线方法，突出了其在生成高质量语音方面的有效性。", "summary": "本文针对现有可控语音合成方法在鲁棒性、泛化性和多样性上的不足，提出了一种三阶段多模态框架。该框架通过对面部编码器采用监督学习和知识蒸馏，并对文本编码器利用多源数据训练，有效提升了合成语音的质量和多样性，并在实验中验证了其优于单模态基线方法的性能。", "keywords": "可控语音合成, 多模态, 多阶段框架, 知识蒸馏, 语音合成", "comments": "该论文的创新点在于提出了一个多阶段框架，并通过结合多种技术（监督学习、知识蒸馏、多源数据训练）来解决多模态可控语音合成中的关键挑战，特别是针对数据质量和匹配度的问题。这对于提升语音合成的实用性和泛化能力具有重要意义。"}}
{"id": "2506.21073", "title": "Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G Networks", "authors": ["Ilias Papalamprou", "Nikolaos Fotos", "Nikolaos Chatzivasileiadis", "Anna Angelogianni", "Dimosthenis Masouros", "Dimitrios Soudris"], "summary": "The advent of 5G and beyond has brought increased performance networks,\nfacilitating the deployment of services closer to the user. To meet performance\nrequirements such services require specialized hardware, such as Field\nProgrammable Gate Arrays (FPGAs). However, FPGAs are often deployed in\nunprotected environments, leaving the user's applications vulnerable to\nmultiple attacks. With the rise of quantum computing, which threatens the\nintegrity of widely-used cryptographic algorithms, the need for a robust\nsecurity infrastructure is even more crucial. In this paper we introduce a\nhybrid hardware-software solution utilizing remote attestation to securely\nconfigure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms\nfor enhanced security. Additionally, to enable trustworthiness across the whole\nedge computing continuum, our solution integrates a blockchain infrastructure,\nensuring the secure storage of any security evidence. We evaluate the proposed\nsecure configuration process under different PQC algorithms in two FPGA\nfamilies, showcasing only 2% overheard compared to the non PQC approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21073v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21073v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "B5G网络中可信FPGA的后量子和基于区块链的认证", "tldr": "本论文提出了一种混合软硬件解决方案，利用远程认证、后量子密码算法和区块链基础设施，为B5G网络中部署在非保护环境下的FPGA提供安全配置，并实现了低开销。", "motivation": "5G及未来网络的发展需要FPGA等专用硬件来满足性能要求，但FPGA常部署在非保护环境中，易受攻击。量子计算的兴起对现有密码算法构成威胁，因此需要更强大的安全基础设施。", "method": "论文提出了一种混合软硬件解决方案，利用远程认证安全配置FPGA，并集成了后量子密码（PQC）算法以增强安全性。此外，为确保整个边缘计算连续体的可信度，解决方案还集成了区块链基础设施，用于安全存储任何安全证据。", "result": "在两种FPGA系列中，使用不同PQC算法对所提出的安全配置过程进行评估，结果显示与非PQC方法相比，开销仅为2%。", "conclusion": "该研究成功地提出并验证了一种针对B5G网络中FPGA的安全配置方案，该方案结合了后量子密码和区块链技术，在保证安全性的同时保持了较低的性能开销，有效应对了量子计算威胁和FPGA部署环境的安全挑战。", "translation": "5G及未来网络的出现带来了性能提升的网络，促进了服务更接近用户的部署。为了满足性能要求，这些服务需要专用硬件，例如现场可编程门阵列（FPGA）。然而，FPGA通常部署在非保护环境中，使用户的应用程序容易受到多种攻击。随着量子计算的兴起，它威胁着广泛使用的加密算法的完整性，对健壮安全基础设施的需求变得更加关键。本文介绍了一种混合软硬件解决方案，利用远程认证安全配置FPGA，同时集成后量子密码（PQC）算法以增强安全性。此外，为了实现整个边缘计算连续体的可信度，我们的解决方案集成了一个区块链基础设施，确保任何安全证据的安全存储。我们在两种FPGA系列中，使用不同PQC算法对所提出的安全配置过程进行了评估，结果显示与非PQC方法相比，开销仅为2%。", "summary": "本文针对B5G网络中FPGA在非保护环境下易受攻击且面临量子计算威胁的问题，提出了一种混合软硬件安全配置方案。该方案结合了远程认证、后量子密码算法和区块链技术，旨在安全配置FPGA并安全存储安全证据。实验评估表明，与非PQC方法相比，该方案仅产生2%的开销，证明了其在提供增强安全性的同时具有高效性。", "keywords": "后量子密码, 区块链, FPGA, 远程认证, B5G网络", "comments": "该论文的创新点在于将后量子密码与区块链技术结合应用于FPGA的远程认证和安全配置，有效应对了量子计算威胁和非信任环境下的硬件安全挑战。其低开销的评估结果也显示了方案的实用性。"}}
{"id": "2506.20915", "title": "ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models", "authors": ["Mina Namazi", "Alexander Nemecek", "Erman Ayday"], "summary": "As the deployment of large language models (LLMs) grows in sensitive domains,\nensuring the integrity of their computational provenance becomes a critical\nchallenge, particularly in regulated sectors such as healthcare, where strict\nrequirements are applied in dataset usage. We introduce ZKPROV, a novel\ncryptographic framework that enables zero-knowledge proofs of LLM provenance.\nIt allows users to verify that a model is trained on a reliable dataset without\nrevealing sensitive information about it or its parameters. Unlike prior\napproaches that focus on complete verification of the training process\n(incurring significant computational cost) or depend on trusted execution\nenvironments, ZKPROV offers a distinct balance. Our method cryptographically\nbinds a trained model to its authorized training dataset(s) through\nzero-knowledge proofs while avoiding proof of every training step. By\nleveraging dataset-signed metadata and compact model parameter commitments,\nZKPROV provides sound and privacy-preserving assurances that the result of the\nLLM is derived from a model trained on the claimed authorized and relevant\ndataset. Experimental results demonstrate the efficiency and scalability of the\nZKPROV in generating this proof and verifying it, achieving a practical\nsolution for real-world deployments. We also provide formal security\nguarantees, proving that our approach preserves dataset confidentiality while\nensuring trustworthy dataset provenance.", "comment": "12 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.20915v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20915v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ZKPROV：一种用于大型语言模型数据集溯源的零知识方法", "tldr": "ZKPROV是一种零知识证明框架，用于验证大型语言模型是否在可靠数据集上训练，同时保护数据隐私和模型参数信息，且比现有方法更高效实用。", "motivation": "随着大型语言模型（LLM）在敏感领域的部署，确保其计算溯源的完整性成为一个关键挑战，特别是在医疗保健等受监管部门，对数据集使用有严格要求。现有方法要么计算成本高昂（完全验证训练过程），要么依赖可信执行环境。", "method": "ZKPROV通过零知识证明将训练好的模型与其授权训练数据集进行加密绑定，避免了对每个训练步骤的证明。它利用数据集签名的元数据和紧凑的模型参数承诺，提供可靠且保护隐私的保证，证明LLM的结果源自于在声明的授权相关数据集上训练的模型。", "result": "实验结果表明ZKPROV在生成和验证证明方面具有效率和可扩展性，为实际部署提供了实用的解决方案。该方法还提供了形式化的安全保证，证明其在确保可信数据集溯源的同时保留了数据集的机密性。", "conclusion": "ZKPROV提供了一种实用、高效且保护隐私的零知识方法，用于验证大型语言模型的数据集溯源，解决了在敏感领域部署LLM时的数据完整性挑战。", "translation": "随着大型语言模型（LLM）在敏感领域的部署日益增多，确保其计算溯源的完整性成为一个关键挑战，特别是在医疗保健等受监管部门，对数据集的使用有严格要求。我们引入了ZKPROV，一个新颖的加密框架，能够实现LLM溯源的零知识证明。它允许用户验证模型是否在可靠数据集上训练，而无需透露有关数据集或模型参数的敏感信息。与之前专注于训练过程的完整验证（导致显著的计算成本）或依赖可信执行环境的方法不同，ZKPROV提供了独特的平衡。我们的方法通过零知识证明将训练好的模型与其授权训练数据集加密绑定，同时避免证明每个训练步骤。通过利用数据集签名的元数据和紧凑的模型参数承诺，ZKPROV提供了可靠且保护隐私的保证，确保LLM的结果源自于在声明的授权和相关数据集上训练的模型。实验结果表明ZKPROV在生成和验证此证明方面具有效率和可扩展性，实现了实际部署的实用解决方案。我们还提供了形式化的安全保证，证明我们的方法在确保可信数据集溯源的同时保留了数据集的机密性。", "summary": "ZKPROV是一个新颖的零知识证明框架，旨在解决大型语言模型在敏感领域部署时的数据集溯源完整性挑战。它允许用户在不泄露敏感数据或模型参数的情况下，验证模型是否在授权数据集上训练。与现有高成本或依赖可信环境的方法不同，ZKPROV通过加密绑定模型与数据集，并利用数据集签名元数据和模型参数承诺，提供高效、可扩展且隐私保护的解决方案，确保LLM结果的可靠溯源。该方法已通过实验验证其效率和实用性，并提供了形式化的安全保证。", "keywords": "零知识证明, 数据集溯源, 大型语言模型, 隐私保护, 加密框架", "comments": "ZKPROV的创新之处在于其在保护隐私和计算效率之间找到了一个独特的平衡点，解决了LLM在敏感领域部署的关键痛点。它避免了对整个训练过程进行昂贵的验证，而是专注于数据集与模型的绑定，这对于实际应用具有重要意义。形式化的安全保证也增加了其可信度。"}}
{"id": "2506.20834", "title": "Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher", "authors": ["Tomas Gallo Aquino", "Victoria Liu", "Habiba Azab", "Raissa Mathura", "Andrew J Watrous", "Eleonora Bartoli", "Benjamin Y Hayden", "Paul Sajda", "Sameer A Sheth", "Nuttida Rungratsameetaweemana"], "summary": "Transfer learning enhances the training of novel sensory and decision models\nby employing rich feature representations from large, pre-trained teacher\nmodels. Cognitive neuroscience shows that the human brain creates\nlow-dimensional, abstract representations for efficient sensorimotor coding.\nImportantly, the brain can learn these representations with significantly fewer\ndata points and less computational power than artificial models require. We\nintroduce Brain2Model Transfer Learning (B2M), a framework where neural\nactivity from human sensory and decision-making tasks acts as the teacher model\nfor training artificial neural networks. We propose two B2M strategies: (1)\nBrain Contrastive Transfer, which aligns brain activity and network activations\nthrough a contrastive objective; and (2) Brain Latent Transfer, which projects\nlatent dynamics from similar cognitive tasks onto student networks via\nsupervised regression of brain-derived features. We validate B2M in\nmemory-based decision-making with a recurrent neural network and scene\nreconstruction for autonomous driving with a variational autoencoder. The\nresults show that student networks benefiting from brain-based transfer\nconverge faster and achieve higher predictive accuracy than networks trained in\nisolation. Our findings indicate that the brain's representations are valuable\nfor artificial learners, paving the way for more efficient learning of complex\ndecision-making representations, which would be costly or slow through purely\nartificial training.", "comment": "15 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.20834v1", "categories": ["cs.NE", "cs.ET", "q-bio.NC"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20834v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "脑到模型迁移：以人类神经活动为教师训练感觉和决策模型", "tldr": "提出Brain2Model迁移学习框架，利用人类神经活动作为教师模型，显著加速人工神经网络训练并提高准确性。", "motivation": "人脑能以显著更少的数据和计算资源学习低维抽象表示，而现有的人工模型需要大量数据和计算且训练成本高昂。本文旨在利用人脑的这种高效学习能力来改进人工模型的训练，使其更高效、更经济。", "method": "本文引入了Brain2Model Transfer Learning (B2M) 框架，其中人类感觉和决策任务中的神经活动充当训练人工神经网络的教师模型。该框架提出两种策略：1) 脑对比迁移（Brain Contrastive Transfer），通过对比目标对齐大脑活动和网络激活；2) 脑潜在迁移（Brain Latent Transfer），通过对脑源特征进行监督回归，将类似认知任务的潜在动态投射到学生网络。研究在基于记忆的决策（使用循环神经网络）和自动驾驶场景重建（使用变分自编码器）中验证了B2M。", "result": "受益于基于大脑迁移的学生网络比单独训练的网络收敛更快，并达到更高的预测准确性。", "conclusion": "大脑的表示对人工学习者很有价值，为更有效地学习复杂决策表示铺平了道路，而这些表示通过纯人工训练将是昂贵或缓慢的。", "translation": "迁移学习通过利用大型预训练教师模型的丰富特征表示来增强新型感觉和决策模型的训练。认知神经科学表明，人脑为高效的感觉运动编码创建了低维、抽象的表示。重要的是，与人工智能模型所需的相比，大脑能够以显著更少的数据点和更低的计算能力学习这些表示。我们引入了脑到模型迁移学习（B2M），这是一个框架，其中人类感觉和决策任务中的神经活动充当训练人工神经网络的教师模型。我们提出了两种B2M策略：（1）脑对比迁移，它通过对比目标对齐大脑活动和网络激活；（2）脑潜在迁移，它通过对脑源特征的监督回归，将类似认知任务的潜在动态投射到学生网络上。我们在基于记忆的决策（使用循环神经网络）和自动驾驶场景重建（使用变分自编码器）中验证了B2M。结果表明，受益于基于大脑迁移的学生网络比单独训练的网络收敛更快，并达到更高的预测准确性。我们的发现表明，大脑的表示对人工学习者很有价值，为更有效地学习复杂决策表示铺平了道路，而这些表示通过纯人工训练将是昂贵或缓慢的。", "summary": "本文提出了Brain2Model Transfer Learning (B2M) 框架，旨在利用人类神经活动作为教师模型来训练人工神经网络。该框架包含脑对比迁移和脑潜在迁移两种策略，旨在将人脑高效学习的低维、抽象表示能力迁移到人工模型中。实验结果表明，B2M能显著加速学生网络的收敛并提高预测准确性，证明了人脑表示对于构建更高效、更经济的人工智能模型具有重要价值。", "keywords": "脑到模型迁移, 神经活动, 迁移学习, 人工神经网络, 决策模型", "comments": "这篇论文提出了一种新颖的迁移学习范式，即“脑到模型迁移”，其创新点在于将人脑的神经活动作为“教师”来指导人工神经网络的学习。这突破了传统迁移学习中仅使用人工模型作为教师的限制，利用了人脑在数据效率和计算效率上的优势。其重要性在于为未来构建更接近生物智能、更高效的人工智能系统提供了新的思路，尤其是在复杂决策和感知任务中，可以显著降低训练成本和时间。"}}
{"id": "2506.20695", "title": "Malicious earworms and useful memes, how the far-right surfs on TikTok audio trends", "authors": ["Marloes Geboers", "Marcus Bösch"], "summary": "With its features of remix, TikTok is the designated platform for meme-making\nand dissemination. Creative combinations of video, emoji, and filters allow for\nan endless stream of memes and trends animated by sound. The platform has\nfocused its moderation on upholding physical safety, hence investing in the\ndetection of harmful challenges. In response to the DSA, TikTok implemented\nopt-outs for personalized feeds and features allowing users to report illegal\ncontent. At the same time, the platform remains subject to scrutiny. Centering\non the role of sound and its intersections with ambiguous memes, the presented\nresearch probed right-wing extremist formations relating to the 2024 German\nstate elections. The analysis evidences how the TikTok sound infrastructure\naffords a sustained presence of xenophobic content, often cloaked through\nvernacular modes of communication. These cloaking practices benefit from a\nsound infrastructure that affords the ongoing posting of user-generated sounds\nthat instantly spread through the use-this-sound button. Importantly, these\nsounds are often not clearly recognizable as networkers of extremist content.\nSongs that do contain hateful lyrics are not eligible for personalized feeds,\nhowever, they remain online where they profit from intersecting with benign\nmeme trends, rendering them visible in search results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20695v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20695v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "恶意耳虫与有用迷因：极右翼如何利用TikTok音频趋势", "tldr": "本研究探讨了极右翼势力如何在TikTok上利用音频趋势传播仇外内容，即使这些内容通过模糊的迷因和用户生成的声音进行伪装，并绕过平台的部分审查机制。", "motivation": "TikTok作为模因制作和传播的平台，其声音基础设施被发现允许仇外内容的持续存在，即使平台已加强内容审核。本研究旨在探讨极右翼极端主义组织如何利用TikTok的声音功能及其与模糊模因的交叉点，尤其是在2024年德国州选举背景下。", "method": "该研究聚焦于声音的作用及其与模糊迷因的交叉点，通过分析与2024年德国州选举相关的右翼极端主义形成，来揭示TikTok声音基础设施如何助长仇外内容的传播。", "result": "分析表明，TikTok的声音基础设施使得仇外内容能够持续存在，这些内容通常通过本土化的交流方式进行伪装。这些伪装手段受益于一个允许用户生成声音持续发布并迅速传播（通过“使用此声音”按钮）的声音基础设施。这些声音往往不易被识别为极端主义内容的传播者。包含仇恨歌词的歌曲虽然不符合个性化推送的条件，但仍在线上存在，并与良性模因趋势交叉，使其在搜索结果中可见。", "conclusion": "TikTok的声音基础设施为极右翼势力提供了一个传播仇外内容的有效渠道，即使平台实施了审核措施，通过模糊的模因和用户生成的声音，这些内容仍能持续存在并被发现。", "translation": "凭借其混音功能，TikTok是模因制作和传播的指定平台。视频、表情符号和滤镜的创意组合，使得由声音驱动的模因和趋势源源不断。该平台一直专注于维护人身安全，因此投资于有害挑战的检测。为了响应《数字服务法案》（DSA），TikTok实施了个性化推送的退出选项，并提供了允许用户举报非法内容的功能。与此同时，该平台仍受到严格审查。本研究以声音的作用及其与模糊模因的交叉点为中心，探讨了与2024年德国州选举相关的右翼极端主义形成。分析表明，TikTok的声音基础设施如何使得仇外内容持续存在，这些内容通常通过本土化的交流方式进行伪装。这些伪装做法得益于一个声音基础设施，该基础设施允许用户生成的声音持续发布，并通过“使用此声音”按钮即时传播。重要的是，这些声音通常无法清晰识别为极端主义内容的网络传播者。确实包含仇恨歌词的歌曲不符合个性化推送的条件，但它们仍在线上存在，并受益于与良性模因趋势的交叉，使其在搜索结果中可见。", "summary": "本研究分析了极右翼势力如何在TikTok上利用其独特的声音基础设施和模因传播机制来扩散仇外内容。尽管TikTok已加强内容审核并引入新功能以应对非法内容，但研究发现，通过模糊的模因、用户生成的声音以及与良性趋势的交叉，仇外信息仍能持续存在并被发现，尤其是在2024年德国州选举的背景下。", "keywords": "TikTok, 极右翼, 音频趋势, 模因, 仇外内容", "comments": "该研究揭示了社交媒体平台在内容审核方面面临的复杂挑战，特别是当恶意内容通过模糊和非传统方式（如音频趋势和模因）传播时。它强调了仅仅关注显性有害内容不足以遏制极端主义，需要更深入理解内容在不同媒介中的伪装和传播机制。创新点在于其聚焦TikTok的声音基础设施和模因文化，揭示了这些技术特性如何被滥用。"}}
{"id": "2506.20759", "title": "Agile Management for Machine Learning: A Systematic Mapping Study", "authors": ["Lucas Romao", "Hugo Villamizar", "Romeu Oliveira", "Silvio Alonso", "Marcos Kalinowski"], "summary": "[Context] Machine learning (ML)-enabled systems are present in our society,\ndriving significant digital transformations. The dynamic nature of ML\ndevelopment, characterized by experimental cycles and rapid changes in data,\nposes challenges to traditional project management. Agile methods, with their\nflexibility and incremental delivery, seem well-suited to address this\ndynamism. However, it is unclear how to effectively apply these methods in the\ncontext of ML-enabled systems, where challenges require tailored approaches.\n[Goal] Our goal is to outline the state of the art in agile management for\nML-enabled systems. [Method] We conducted a systematic mapping study using a\nhybrid search strategy that combines database searches with backward and\nforward snowballing iterations. [Results] Our study identified 27 papers\npublished between 2008 and 2024. From these, we identified eight frameworks and\ncategorized recommendations and practices into eight key themes, such as\nIteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable\nModel. The main challenge identified across studies was accurate effort\nestimation for ML-related tasks. [Conclusion] This study contributes by mapping\nthe state of the art and identifying open gaps in the field. While relevant\nwork exists, more robust empirical evaluation is still needed to validate these\ncontributions.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20759v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20759v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "机器学习的敏捷管理：一项系统性映射研究", "tldr": "本研究通过系统性映射研究，概述了机器学习系统敏捷管理的现状，识别了八个框架和八个关键主题，并指出主要挑战是ML任务的准确工作量估算，强调需要更多实证评估。", "motivation": "机器学习（ML）系统开发具有实验周期和数据快速变化的动态性，对传统项目管理构成挑战。尽管敏捷方法似乎适合，但如何有效应用于ML系统尚不明确，需要定制化方法。因此，本研究旨在概述ML系统敏捷管理的现状。", "method": "本研究采用系统性映射研究方法，结合数据库搜索以及前向和后向滚雪球迭代的混合搜索策略。", "result": "研究识别了2008年至2024年间发表的27篇论文。从中识别出八个框架，并将推荐和实践分为八个关键主题，例如迭代灵活性、创新性ML特定工件和最小可行模型。研究中发现的主要挑战是ML相关任务的准确工作量估算。", "conclusion": "本研究通过绘制该领域的现状图并识别开放差距做出了贡献。尽管存在相关工作，但仍需要更强有力的实证评估来验证这些贡献。", "translation": "[背景] 机器学习（ML）驱动的系统存在于我们的社会中，推动着重大的数字化转型。ML开发动态性强，以实验周期和数据快速变化为特征，给传统项目管理带来了挑战。敏捷方法以其灵活性和增量交付，似乎非常适合应对这种动态性。然而，目前尚不清楚如何在ML驱动系统背景下有效应用这些方法，因为这些挑战需要定制化的方法。\n[目标] 我们的目标是概述ML驱动系统敏捷管理的最新现状。[方法] 我们采用了一种系统性映射研究方法，该方法结合了数据库搜索与前向和后向滚雪球迭代的混合搜索策略。[结果] 我们的研究识别了2008年至2024年间发表的27篇论文。从中，我们识别了八个框架，并将推荐和实践分为八个关键主题，例如迭代灵活性、创新性ML特定工件和最小可行模型。研究中发现的主要挑战是ML相关任务的准确工作量估算。[结论] 本研究通过绘制该领域的现状图并识别开放差距做出了贡献。尽管存在相关工作，但仍需要更强有力的实证评估来验证这些贡献。", "summary": "本文进行了一项系统性映射研究，旨在概述机器学习（ML）系统敏捷管理的现状。研究通过混合搜索策略筛选了2008年至2024年间的27篇论文，识别出八个敏捷管理框架，并归纳了八个关键主题，如迭代灵活性和ML特定工件。研究发现，ML任务的准确工作量估算是主要挑战。研究贡献在于描绘了该领域的现状并指出了现有差距，强调未来需要更多实证评估。", "keywords": "敏捷管理, 机器学习, 系统性映射研究, 项目管理, 软件工程", "comments": "这项研究通过系统性映射，清晰地描绘了机器学习项目敏捷管理的当前图景，识别了现有框架和实践，并突出了工作量估算这一核心挑战，为未来的研究和实践提供了宝贵的起点。其创新之处在于将敏捷方法论与ML的独特动态性相结合进行审视，填补了这一交叉领域的知识空白。"}}
{"id": "2506.20804", "title": "Online Planning for Cooperative Air-Ground Robot Systems with Unknown Fuel Requirements", "authors": ["Ritvik Agarwal", "Behnoushsadat Hatami", "Alvika Gautam", "Parikshit Maini"], "summary": "We consider an online variant of the fuel-constrained UAV routing problem\nwith a ground-based mobile refueling station (FCURP-MRS), where targets incur\nunknown fuel costs. We develop a two-phase solution: an offline heuristic-based\nplanner computes initial UAV and UGV paths, and a novel online planning\nalgorithm that dynamically adjusts rendezvous points based on real-time fuel\nconsumption during target processing. Preliminary Gazebo simulations\ndemonstrate the feasibility of our approach in maintaining UAV-UGV path\nvalidity, ensuring mission completion. Link to video:\nhttps://youtu.be/EmpVj-fjqNY", "comment": "Submitted to RSS (MRS Workshop)", "pdf_url": "http://arxiv.org/pdf/2506.20804v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20804v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "具有未知燃料需求的空地机器人协同系统在线规划", "tldr": "提出一种在线规划方法，用于解决空地机器人协同系统中，无人机在未知燃料成本下进行任务，并通过地面移动加油站补给的问题，通过两阶段方案和在线算法动态调整加油点，仿真验证了可行性。", "motivation": "解决燃料受限的无人机路径规划问题，特别是当目标燃料成本未知时，需要一个在线规划方案来动态调整空中无人机和地面加油站的汇合点。", "method": "采用两阶段解决方案：首先，一个离线启发式规划器计算初始无人机（UAV）和地面车辆（UGV）路径；其次，一个新颖的在线规划算法根据目标处理期间的实时燃料消耗动态调整汇合点。", "result": "初步的Gazebo仿真结果表明，所提出的方法在保持无人机-地面车辆路径有效性、确保任务完成方面是可行的。", "conclusion": "该研究成功开发并验证了一种在线规划方法，能够有效解决具有未知燃料需求的空地机器人协同系统的燃料补给和任务完成问题。", "translation": "我们考虑了燃料受限无人机路径规划问题（FCURP-MRS）的一个在线变体，其中包含一个地面移动加油站，且目标会产生未知的燃料成本。我们开发了一个两阶段解决方案：一个离线启发式规划器计算初始无人机和地面车辆路径，以及一个新颖的在线规划算法，根据目标处理期间的实时燃料消耗动态调整汇合点。初步的Gazebo仿真演示了我们方法在保持无人机-地面车辆路径有效性、确保任务完成方面的可行性。视频链接：https://youtu.be/EmpVj-fjqNY", "summary": "本文提出了一种针对空地机器人协同系统的在线规划方法，以解决燃料受限无人机路由问题中目标燃料成本未知的情况。该方法包含一个离线启发式规划器用于初始路径规划，以及一个新颖的在线算法根据实时燃料消耗动态调整无人机与地面加油站的汇合点。初步的Gazebo仿真验证了该方法在维持路径有效性和确保任务完成方面的可行性。", "keywords": "在线规划, 空地机器人系统, 燃料受限无人机, 移动加油站, 动态调整", "comments": "该研究的创新点在于提出了一个两阶段的在线规划方法，特别是在线算法能够根据实时燃料消耗动态调整汇合点，有效应对了未知燃料成本的挑战。这种动态调整能力对于实际应用中的鲁棒性至关重要。"}}
{"id": "2506.20884", "title": "\"TikTok, Do Your Thing\": User Reactions to Social Surveillance in the Public Sphere", "authors": ["Meira Gilbert", "Miranda Wei", "Lindah Kotut"], "summary": "''TikTok, Do Your Thing'' is a viral trend where users attempt to identify\nstrangers they see in public via information crowd-sourcing. The trend started\nas early as 2021 and users typically engage with it for romantic purposes\n(similar to a ''Missed Connections'' personal advertisement). This practice\nincludes acts of surveillance and identification in the public sphere, although\nby peers rather than governments or corporations. To understand users'\nreactions to this trend we conducted a qualitative analysis of 60 TikTok videos\nand 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were\nsuccessfully identified. We also find that while there were comments expressing\ndisapproval (n=310), more than double the number expressed support (n=883).\nSupportive comments demonstrated genuine interest and empathy, reflecting\nevolving conceptions of community and algorithmic engagement. On the other\nhand, disapproving comments highlighted concerns about inappropriate\nrelationships, stalking, consent, and gendered double standards. We discuss\nthese insights in relation to the normalization of interpersonal surveillance,\nonline stalking, and as an evolution of social surveillance to offer a new\nperspective on user perceptions surrounding interpersonal surveillance and\nidentification in the public sphere.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20884v1", "categories": ["cs.HC", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20884v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "“抖音，发挥你的作用”：用户对公共领域社会监控的反应", "tldr": "一项对TikTok上“抖音，发挥你的作用”趋势的定性分析显示，尽管存在担忧，但用户对这种基于人群的陌生人识别行为普遍持支持态度，反映了人际监控的常态化。", "motivation": "了解用户对“抖音，发挥你的作用”这一病毒式趋势的反应，该趋势涉及在公共领域通过众包方式识别陌生人，通常出于浪漫目的。", "method": "对60个TikTok视频和1,901条用户评论进行了定性分析。", "result": "在审查的60个视频中，有19个人被成功识别。支持评论（n=883）是反对评论（n=310）的两倍多。支持评论表现出兴趣和同情，反映了社区和算法参与观念的演变；反对评论则关注不当关系、跟踪、同意和性别双重标准。", "conclusion": "本研究讨论了人际监控、在线跟踪的常态化，以及社会监控的演变，为用户在公共领域对人际监控和识别的看法提供了新视角。", "translation": "“抖音，发挥你的作用”是一个病毒式趋势，用户试图通过信息众包来识别他们在公共场合看到的陌生人。该趋势早在2021年就开始了，用户通常出于浪漫目的参与其中（类似于“寻人启事”个人广告）。这种做法包括在公共领域的监控和识别行为，尽管是由同伴而非政府或公司进行的。为了了解用户对这一趋势的反应，我们对60个TikTok视频和1,901条用户评论进行了定性分析。在审查的60个视频中，我们发现有19个人被成功识别。我们还发现，尽管有表示反对的评论（n=310），但表示支持的评论（n=883）是其两倍多。支持性评论表现出真正的兴趣和同情，反映了社区和算法参与观念的演变。另一方面，反对性评论则强调了对不当关系、跟踪、同意和性别双重标准的担忧。我们结合人际监控、在线跟踪的常态化以及社会监控的演变来讨论这些见解，从而为用户在公共领域对人际监控和识别的看法提供新视角。", "summary": "本研究对TikTok上的“抖音，发挥你的作用”病毒式趋势进行了定性分析，该趋势涉及通过众包识别公共场合的陌生人。通过分析60个视频和1901条评论，研究发现尽管存在对跟踪和隐私的担忧，但用户对这种行为的支持度远超反对度，表明了人际监控的日益常态化以及社区和算法互动观念的转变。", "keywords": "TikTok, 社会监控, 用户反应, 众包, 隐私", "comments": "这项研究通过定性分析揭示了社交媒体上人际监控的复杂用户反应，特别是在TikTok这一热门平台上。其创新之处在于关注“同伴监控”而非传统意义上的政府或企业监控，并揭示了用户对隐私和社区观念的演变。研究的重要性在于它触及了数字时代隐私边界模糊化的现实，以及用户在参与此类趋势时的内在矛盾心理。然而，研究的局限性可能在于其定性方法和样本量，可能无法完全代表所有TikTok用户的普遍看法。"}}
{"id": "2506.21406", "title": "Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees", "authors": ["Tommaso Bonato", "Daniele De Sensi", "Salvatore Di Girolamo", "Abdulla Bataineh", "David Hewson", "Duncan Roweth", "Torsten Hoefler"], "summary": "Network latency severely impacts the performance of applications running on\nsupercomputers. Adaptive routing algorithms route packets over different\navailable paths to reduce latency and improve network utilization. However, if\na switch routes packets belonging to the same network flow on different paths,\nthey might arrive at the destination out-of-order due to differences in the\nlatency of these paths. For some transport protocols like TCP, QUIC, and RoCE,\nout-of-order (OOO) packets might cause large performance drops or significantly\nincrease CPU utilization. In this work, we propose flowcut switching, a new\nadaptive routing algorithm that provides high-performance in-order packet\ndelivery. Differently from existing solutions like flowlet switching, which are\nbased on the assumption of bursty traffic and that might still reorder packets,\nflowcut switching guarantees in-order delivery under any network conditions,\nand is effective also for non-bursty traffic, as it is often the case for RDMA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21406v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.21406v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Flowcut Switching：高性能自适应路由与顺序交付保证", "tldr": "Flowcut Switching是一种新的自适应路由算法，它能确保在任何网络条件下数据包的按序交付，解决了现有自适应路由可能导致乱序包的问题，尤其适用于非突发流量。", "motivation": "自适应路由算法虽然能通过在不同可用路径上路由数据包来减少网络延迟并提高网络利用率，但如果同一网络流的数据包通过不同路径传输，它们可能会因路径延迟差异而乱序到达目的地。对于TCP、QUIC和RoCE等传输协议，乱序（OOO）数据包可能导致性能大幅下降或显著增加CPU利用率。", "method": "本文提出了一种名为flowcut switching的新型自适应路由算法。与现有解决方案（如flowlet switching）不同，flowcut switching不依赖于突发流量的假设，且能在任何网络条件下保证数据包的按序交付，同时对非突发流量（如RDMA）也有效。", "result": "Flowcut switching能够提供高性能的按序数据包交付。它在任何网络条件下都能保证按序交付，并且对非突发流量（如RDMA）也有效。", "conclusion": "Flowcut switching是一种能够解决自适应路由中乱序包问题的创新方法，它通过保证在任何网络条件下的按序交付，为高性能计算环境中的网络通信提供了更可靠和高效的解决方案，尤其适用于对乱序敏感的应用和非突发流量。", "translation": "网络延迟严重影响超级计算机上运行应用程序的性能。自适应路由算法通过在不同可用路径上路由数据包来减少延迟并提高网络利用率。然而，如果交换机将属于同一网络流的数据包路由到不同路径，由于这些路径的延迟差异，它们可能会乱序到达目的地。对于TCP、QUIC和RoCE等某些传输协议，乱序（OOO）数据包可能导致性能大幅下降或显著增加CPU利用率。在这项工作中，我们提出了flowcut switching，这是一种新的自适应路由算法，可提供高性能的按序数据包交付。与现有解决方案（如flowlet switching）不同，后者基于突发流量的假设并且可能仍然会重新排序数据包，flowcut switching在任何网络条件下都能保证按序交付，并且对非突发流量也有效，例如RDMA通常就是这种情况。", "summary": "本文提出了一种名为Flowcut Switching的新型自适应路由算法，旨在解决现有自适应路由在超级计算机网络中可能导致数据包乱序的问题。该算法承诺在任何网络条件下提供高性能的按序数据包交付，并且对非突发流量（如RDMA）同样有效，克服了传统方法（如Flowlet Switching）对突发流量的依赖及其潜在的乱序问题，从而提升了对乱序敏感协议的性能和CPU利用率。", "keywords": "Flowcut Switching, 自适应路由, 按序交付, 网络延迟, 超级计算机", "comments": "该论文的创新点在于提出了Flowcut Switching，一种能够保证在任何网络条件下按序交付数据包的自适应路由算法。这解决了现有自适应路由方案在处理对乱序敏感协议（如TCP、QUIC、RoCE）时可能导致的性能下降和CPU利用率增加的问题。其重要性在于，它为高性能计算环境（特别是RDMA应用）提供了更可靠和高效的网络通信，因为该方案对非突发流量也有效，且不依赖于突发流量的假设，这在网络通信中具有广泛的应用潜力。"}}
{"id": "2506.21078", "title": "Constant Modulus Waveforms for IoT-Centric Integrated Sensing and Communications", "authors": ["Tian Han", "Shalanika Dayarathna", "Rajitha Senanayake", "Peter Smith", "Aryan Kaushik", "Alain Mourad", "Richard A. Stirling-Gallacher", "Jamie Evans"], "summary": "Integrated sensing and communications (ISAC) is considered a key enabler to\nsupport application scenarios such as the Internet-of-Things (IoT) in which\nboth communications and sensing play significant roles. Multi-carrier\nwaveforms, such as orthogonal frequency division multiplexing (OFDM), have been\nconsidered as good candidates for ISAC due to their high communications data\nrate and good time bandwidth property for sensing. Nevertheless, their high\npeak-to-average-power-ratio (PAPR) values lead to either performance\ndegradation or an increase in system complexity. This can make OFDM unsuitable\nfor IoT applications with insufficient resources in terms of power, system\ncomplexity, hardware size or cost. This article provides IoT-centric constant\nmodulus waveform designs that leverage the advantage of unit PAPR and thus are\nmore suitable in resource-limited scenarios. More specifically, several\nsingle-carrier frequency and/or phase-modulated waveforms are considered. A\ncomprehensive discussion on their radar sensing and communications performance\nis conducted based on performance metrics, including the radar ambiguity\nfunction, the bandwidth property, the data rate, and the communications\nreceiver complexity.", "comment": "Submitted for publication to IEEE Communications Standards Magazine", "pdf_url": "http://arxiv.org/pdf/2506.21078v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21078v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于以物联网为中心的集成传感与通信的恒模波形", "tldr": "OFDM在ISAC中PAPR高不适合IoT，本文提出并分析了恒模波形作为替代方案，以适应资源受限的IoT设备。", "motivation": "集成传感与通信（ISAC）是物联网（IoT）应用的关键技术，但传统多载波波形（如OFDM）的高峰均功率比（PAPR）导致性能下降或系统复杂性增加，使其不适用于功率、系统复杂性、硬件尺寸或成本受限的IoT应用。", "method": "本文提出了以物联网为中心的恒模波形设计，利用其单位高峰均功率比（PAPR）的优势。具体考虑了几种单载波频率和/或相位调制波形，并基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等性能指标，对其雷达传感和通信性能进行了全面讨论。", "result": "论文对所提出的恒模波形的雷达传感和通信性能进行了综合讨论，并基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等多个性能指标进行了评估。", "conclusion": "恒模波形因其单位PAPR特性，比传统OFDM波形更适合资源受限的IoT场景中的集成传感与通信（ISAC）应用。", "translation": "集成传感与通信（ISAC）被认为是支持物联网（IoT）等应用场景的关键使能技术，在这些场景中，通信和传感都扮演着重要角色。多载波波形，例如正交频分复用（OFDM），因其高通信数据速率和良好的传感时间带宽特性，被认为是ISAC的良好候选。然而，其高峰均功率比（PAPR）值导致性能下降或系统复杂性增加。这使得OFDM可能不适合在功率、系统复杂性、硬件尺寸或成本方面资源不足的物联网应用。本文提供了以物联网为中心的恒模波形设计，利用单位PAPR的优势，因此更适合资源受限的场景。更具体地说，本文考虑了几种单载波频率和/或相位调制波形。基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等性能指标，对它们的雷达传感和通信性能进行了全面讨论。", "summary": "本文针对物联网（IoT）中集成传感与通信（ISAC）应用中OFDM波形高峰均功率比（PAPR）不适用于资源受限设备的问题，提出并设计了以IoT为中心的恒模波形。通过利用恒模波形单位PAPR的优势，使其更适合资源有限的IoT场景。文章详细讨论了几种单载波频率和/或相位调制波形的雷达传感和通信性能，并基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等关键指标进行了评估。", "keywords": "恒模波形, 集成传感与通信, 物联网, 高峰均功率比, 单载波调制", "comments": "这篇论文的创新点在于提出了适用于IoT场景的恒模波形设计，解决了传统OFDM波形在资源受限设备中PAPR过高的问题。其重要性在于为未来IoT-ISAC系统的波形选择提供了新的方向，有望降低系统复杂性和成本。"}}
{"id": "2506.20674", "title": "Scalable GPU Performance Variability Analysis framework", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Seth Ockerman", "Amal Gueroudji", "Line Pouchard", "Tanzima Z. Islam"], "summary": "Analyzing large-scale performance logs from GPU profilers often requires\nterabytes of memory and hours of runtime, even for basic summaries. These\nconstraints prevent timely insight and hinder the integration of performance\nanalytics into automated workflows. Existing analysis tools typically process\ndata sequentially, making them ill-suited for HPC workflows with growing trace\ncomplexity and volume. We introduce a distributed data analysis framework that\nscales with dataset size and compute availability. Rather than treating the\ndataset as a single entity, our system partitions it into independently\nanalyzable shards and processes them concurrently across MPI ranks. This design\nreduces per-node memory pressure, avoids central bottlenecks, and enables\nlow-latency exploration of high-dimensional trace data. We apply the framework\nto end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate\nits ability to diagnose performance variability, and uncover the impact of\nmemory transfer latency on GPU kernel behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20674v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20674v1", "date": "2025-06-17", "updated": "2025-06-17", "AI": {"title_translation": "可扩展的GPU性能变异性分析框架", "tldr": "该论文介绍了一个分布式数据分析框架，用于解决大规模GPU性能日志分析中内存和运行时限制的问题，通过数据分片和并行处理实现可扩展性，并成功诊断了HPC和AI工作负载中的性能变异性。", "motivation": "分析来自GPU性能分析器的大规模性能日志通常需要大量的内存和运行时，即使是基本摘要也需要太字节内存和数小时运行时间。这些限制阻碍了及时洞察和性能分析集成到自动化工作流中。现有分析工具通常顺序处理数据，不适用于日益增长的跟踪复杂性和数据量的HPC工作流。", "method": "我们引入了一个分布式数据分析框架，该框架可随数据集大小和计算可用性进行扩展。该系统不将数据集视为一个单一实体，而是将其划分为可独立分析的分片，并跨MPI等级并行处理它们。这种设计减少了每个节点的内存压力，避免了中心瓶颈，并实现了对高维跟踪数据的低延迟探索。", "result": "该框架应用于真实HPC和AI工作负载的端到端Nsight Compute跟踪，展示了其诊断性能变异性的能力，并揭示了内存传输延迟对GPU内核行为的影响。", "conclusion": "该分布式数据分析框架有效解决了大规模GPU性能日志分析的挑战，通过其可扩展的设计，能够成功诊断性能变异性并揭示关键性能因素，从而为HPC和AI工作负载提供及时洞察。", "translation": "分析来自GPU性能分析器的大规模性能日志通常需要太字节的内存和数小时的运行时间，即使是基本摘要也如此。这些限制阻碍了及时洞察，并阻碍了性能分析集成到自动化工作流中。现有分析工具通常顺序处理数据，使其不适合处理日益增长的跟踪复杂性和数据量的HPC工作流。我们引入了一个分布式数据分析框架，该框架可随数据集大小和计算可用性进行扩展。该系统不将数据集视为一个单一实体，而是将其划分为可独立分析的分片，并跨MPI等级并行处理它们。这种设计减少了每个节点的内存压力，避免了中心瓶颈，并实现了对高维跟踪数据的低延迟探索。我们将该框架应用于真实HPC和AI工作负载的端到端Nsight Compute跟踪，展示了其诊断性能变异性的能力，并揭示了内存传输延迟对GPU内核行为的影响。", "summary": "本文提出了一个可扩展的分布式数据分析框架，旨在解决大规模GPU性能日志分析中遇到的内存和运行时限制。该框架通过将数据集分片并利用MPI并行处理，有效降低了内存压力并消除了中心瓶颈，从而实现了对高维跟踪数据的低延迟探索。实验证明，该框架能成功诊断HPC和AI工作负载中的GPU性能变异性，并揭示了内存传输延迟对GPU内核行为的具体影响。", "keywords": "GPU性能, 分布式分析, 性能变异性, HPC, AI工作负载", "comments": "这项工作通过引入分布式数据分析框架，创新性地解决了大规模GPU性能分析面临的内存和运行时瓶颈问题。其将数据集分片并并行处理的方法，有效提升了分析的可扩展性和效率，对于HPC和AI等需要处理海量性能数据的领域具有重要意义。该框架能够诊断性能变异性并揭示深层性能因素，为优化GPU工作负载提供了宝贵工具。"}}
{"id": "2506.20773", "title": "A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers", "authors": ["Stephen T. Castonguay", "Joshua B. Fernandes", "Michael A. Puso", "Sylvie Aubry"], "summary": "An efficient numerical framework is presented for modeling viscoelasticity\nand permanent set of polymers. It is based on the hereditary integral form of\ntransient network theory, in which polymer chains belong to distinct networks\neach with different natural equilibrium states. Chains continually detach from\npreviously formed networks and reattach to new networks in a state of zero\nstress. The free energy of these networks is given in terms of the deformation\ngradient relative to the configuration at which the network was born. A\ndecomposition of the kernel for various free energies allows for a recurrence\nrelationship to be established, bypassing the need to integrate over all time\nhistory. The technique is established for both highly compressible and nearly\nincompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and\nOgden-Hill material models. Multiple examples are presented showing the ability\nto handle rate-dependent response and residual strains under complex loading\nhistories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20773v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20773v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "聚合物永久变形和粘弹性响应的遗传积分瞬态网络建模方法", "tldr": "本文提出了一种高效的数值框架，用于建模聚合物的粘弹性和永久变形。该方法基于瞬态网络理论的遗传积分形式，通过链段的不断解离和重新连接来描述材料行为，并能处理各种复杂的加载历史。", "motivation": "本文旨在为聚合物的粘弹性和永久变形提供一个高效的数值建模框架。", "method": "该方法基于瞬态网络理论的遗传积分形式。聚合物链属于不同的网络，每个网络具有不同的自然平衡状态。链段不断从旧网络解离并以零应力状态重新连接到新网络。网络的自由能由相对于其形成时的构型变形梯度表示。通过对各种自由能核进行分解，建立了递归关系，从而避免了对整个时间历史进行积分。该技术适用于高度可压缩和近似不可压缩材料，并使用了neo-Hookean、Blatz-Ko、Yeoh和Ogden-Hill等材料模型。", "result": "该框架能够处理速率依赖性响应和复杂加载历史下的残余应变。", "conclusion": "Not mentioned in abstract", "translation": "本文提出了一种用于模拟聚合物粘弹性和永久变形的高效数值框架。它基于瞬态网络理论的遗传积分形式，其中聚合物链属于不同的网络，每个网络具有不同的自然平衡状态。链段不断从先前形成的网络中解离，并以零应力状态重新连接到新网络。这些网络的自由能根据相对于网络诞生时的构型变形梯度给出。各种自由能的核分解允许建立递归关系，从而避免了对所有时间历史进行积分。该技术通过使用neo-Hookean、Blatz-Ko、Yeoh和Ogden-Hill材料模型，适用于高度可压缩和近似不可压缩材料。本文提供了多个示例，展示了在复杂加载历史下处理速率依赖性响应和残余应变的能力。", "summary": "本文提出了一种基于遗传积分瞬态网络理论的数值框架，用于高效建模聚合物的粘弹性和永久变形。该模型通过描述聚合物链在不同网络间的解离与重连接过程来捕捉材料行为，并利用自由能核分解建立递归关系，避免了对完整时间历史的积分。该方法适用于多种材料模型，并成功展示了其在处理速率依赖性响应和复杂加载历史下残余应变方面的能力。", "keywords": "聚合物, 粘弹性, 永久变形, 瞬态网络理论, 遗传积分", "comments": "该论文提出了一种创新的数值框架，通过结合遗传积分和瞬态网络理论，有效地解决了聚合物粘弹性和永久变形的建模挑战。其通过递归关系避免了对整个时间历史的积分，显著提高了计算效率。该方法适用于多种材料模型，并能处理复杂的加载条件，具有重要的实际应用价值。"}}
{"id": "2506.20693", "title": "E-ABIN: an Explainable module for Anomaly detection in BIological Networks", "authors": ["Ugo Lomoio", "Tommaso Mazza", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "summary": "The increasing availability of large-scale omics data calls for robust\nanalytical frameworks capable of handling complex gene expression datasets\nwhile offering interpretable results. Recent advances in artificial\nintelligence have enabled the identification of aberrant molecular patterns\ndistinguishing disease states from healthy controls. Coupled with improvements\nin model interpretability, these tools now support the identification of genes\npotentially driving disease phenotypes. However, current approaches to gene\nanomaly detection often remain limited to single datasets and lack accessible\ngraphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable\nframework for Anomaly detection in Biological Networks. E-ABIN combines\nclassical machine learning and graph-based deep learning techniques within a\nunified, user-friendly platform, enabling the detection and interpretation of\nanomalies from gene expression or methylation-derived networks. By integrating\nalgorithms such as Support Vector Machines, Random Forests, Graph Autoencoders\n(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a\nhigh predictive accuracy while maintaining interpretability. We demonstrate the\nutility of E-ABIN through case studies of bladder cancer and coeliac disease,\nwhere it effectively uncovers biologically relevant anomalies and offers\ninsights into disease mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20693v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20693v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "E-ABIN：一种用于生物网络异常检测的可解释模块", "tldr": "E-ABIN是一个可解释的框架，结合了经典机器学习和图深度学习，用于在生物网络中检测和解释基因异常，解决了现有方法在多数据集和用户界面上的局限性，并在癌症和乳糜泻案例中展示了其有效性。", "motivation": "现有的基因异常检测方法通常局限于单一数据集，并且缺乏易于访问的图形界面，无法充分利用日益增长的大规模组学数据并提供可解释的结果。", "method": "本文引入了E-ABIN框架，它在一个统一、用户友好的平台中结合了经典机器学习（如支持向量机、随机森林）和基于图的深度学习技术（如图自编码器、图对抗属性网络），用于检测和解释基因表达或甲基化衍生网络中的异常。", "result": "E-ABIN在膀胱癌和乳糜泻的案例研究中展示了其效用，有效地揭示了生物学相关的异常，并为疾病机制提供了见解，同时保持了高预测准确性和可解释性。", "conclusion": "E-ABIN提供了一个通用、可解释且用户友好的框架，能够高效准确地在生物网络中进行异常检测和解释，从而为理解疾病机制提供了新的工具。", "translation": "随着大规模组学数据可用性的增加，需要强大的分析框架来处理复杂的基因表达数据集，同时提供可解释的结果。人工智能的最新进展使得能够识别区分疾病状态与健康对照的异常分子模式。结合模型可解释性的改进，这些工具现在支持识别可能驱动疾病表型的基因。然而，当前的基因异常检测方法通常仍局限于单一数据集，并且缺乏易于访问的图形界面。在此，我们引入了E-ABIN，一个用于生物网络异常检测的通用、可解释框架。E-ABIN在一个统一、用户友好的平台中结合了经典机器学习和基于图的深度学习技术，能够检测和解释来自基因表达或甲基化衍生网络中的异常。通过整合支持向量机、随机森林、图自编码器（GAEs）和图对抗属性网络（GAANs）等算法，E-ABIN在保持可解释性的同时确保了高预测准确性。我们通过膀胱癌和乳糜泻的案例研究展示了E-ABIN的实用性，它有效地揭示了生物学相关的异常，并为疾病机制提供了见解。", "summary": "E-ABIN是一个新颖的可解释框架，旨在解决现有基因异常检测方法在处理复杂组学数据时缺乏多数据集支持和用户界面的问题。该框架整合了经典机器学习（如SVM、RF）和图深度学习（如GAEs、GAANs）技术，在一个统一的平台中实现对生物网络中异常的高精度检测和解释。E-ABIN在膀胱癌和乳糜泻的案例研究中被证明能够有效识别生物学相关异常并提供疾病机制洞察。", "keywords": "异常检测, 生物网络, 可解释AI, 基因表达, 组学数据", "comments": "E-ABIN的创新之处在于其结合了经典机器学习和图深度学习方法，并特别强调了结果的可解释性，这在生物医学领域至关重要。它还通过提供用户友好的平台解决了现有工具在可用性上的局限。该框架的通用性使其有望应用于更广泛的生物网络异常检测场景。"}}
{"id": "2506.20780", "title": "Noise-Tolerant Hybrid Approach for Data-Driven Predictive Control", "authors": ["Mahmood Mazare", "Hossein Ramezani"], "summary": "This paper focuses on a key challenge in hybrid data-driven predictive\ncontrol: the effect of measurement noise on Hankel matrices. While noise is\nhandled in direct and indirect methods, hybrid approaches often overlook its\nimpact during trajectory estimation. We propose a Noise-Tolerant Data-Driven\nPredictive Control (NTDPC) framework that integrates singular value\ndecomposition to separate system dynamics from noise within reduced-order\nHankel matrices. This enables accurate prediction with shorter data horizons\nand lower computational effort. A sensitivity index is introduced to support\nhorizon selection under different noise levels. Simulation results indicate\nimproved robustness and efficiency compared to existing hybrid methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20780v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20780v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "噪声容忍的混合数据驱动预测控制方法", "tldr": "提出一种噪声容忍的数据驱动预测控制框架（NTDPC），利用SVD处理测量噪声，提高预测准确性和效率。", "motivation": "混合数据驱动预测控制中，测量噪声对Hankel矩阵的影响是一个关键挑战，现有混合方法在轨迹估计过程中常忽视其影响。", "method": "提出噪声容忍数据驱动预测控制（NTDPC）框架，该框架通过集成奇异值分解（SVD）在降阶Hankel矩阵中分离系统动力学与噪声。同时引入一个灵敏度指标，以支持在不同噪声水平下的预测范围选择。", "result": "仿真结果表明，与现有混合方法相比，NTDPC提高了鲁棒性和效率。它能以更短的数据范围和更低的计算量实现精确预测。", "conclusion": "该论文成功开发了一种噪声容忍的混合数据驱动预测控制框架，有效解决了测量噪声问题，提升了控制性能。", "translation": "本文重点关注混合数据驱动预测控制中的一个关键挑战：测量噪声对Hankel矩阵的影响。虽然直接和间接方法能够处理噪声，但混合方法在轨迹估计过程中常常忽视其影响。我们提出了一种噪声容忍的数据驱动预测控制（NTDPC）框架，该框架集成了奇异值分解，以在降阶Hankel矩阵中将系统动力学与噪声分离。这使得在更短的数据范围和更低的计算量下实现精确预测成为可能。引入了一个灵敏度指标，以支持在不同噪声水平下的预测范围选择。仿真结果表明，与现有混合方法相比，该方法提高了鲁棒性和效率。", "summary": "本文提出一种噪声容忍的数据驱动预测控制（NTDPC）框架，旨在解决混合数据驱动预测控制中测量噪声对Hankel矩阵的影响问题。该框架通过在降阶Hankel矩阵中应用奇异值分解，有效分离系统动力学与噪声，从而在更短数据范围和更低计算量下实现精确预测。此外，引入了灵敏度指标以辅助不同噪声水平下的预测范围选择。仿真结果验证了NTDPC在鲁棒性和效率上优于现有混合方法。", "keywords": "数据驱动预测控制, 噪声容忍, Hankel矩阵, 奇异值分解, 混合方法", "comments": "该论文的创新点在于提出了一个噪声容忍的混合数据驱动预测控制框架，通过集成奇异值分解有效解决了测量噪声对Hankel矩阵的影响，这在混合方法中常被忽视。其重要性在于提升了数据驱动预测控制在实际应用中的鲁棒性和效率，尤其是在存在测量噪声的环境下。引入的灵敏度指标也为实际应用提供了有益的指导。"}}
{"id": "2506.20756", "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.20756v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20756v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "StereoDiff：立体与扩散协同用于视频深度估计", "tldr": "StereoDiff结合立体匹配和视频扩散模型，解决了视频深度估计中动静态区域的不同一致性要求，实现了SOTA性能。", "motivation": "现有的视频深度估计方法是图像深度估计的简单扩展，未能有效处理视频中动态和静态区域根本不同的时间一致性要求。静态区域可从立体匹配中获得全局3D线索，而动态区域则需从大规模数据中学习以确保平滑过渡。", "method": "StereoDiff是一种两阶段视频深度估计器。它将主要用于静态区域的立体匹配与用于动态区域保持深度平滑过渡的视频深度扩散相结合。通过频域分析数学地证明了立体匹配和视频深度扩散的互补优势。", "result": "StereoDiff在零样本、真实世界、动态视频深度基准（包括室内和室外）上取得了最先进（SoTA）的性能，显示出其在视频深度估计中卓越的一致性和准确性。", "conclusion": "StereoDiff通过协同立体匹配和视频深度扩散，有效解决了视频中静态和动态区域不同的深度一致性要求，从而显著提高了视频深度估计的性能。", "translation": "最近的视频深度估计方法通过遵循图像深度估计的范式，即通常使用海量数据微调预训练的视频扩散模型，取得了很好的性能。然而，我们认为视频深度估计并非图像深度估计的简单扩展。视频中动态和静态区域的时间一致性要求根本不同。静态区域（通常是背景）的一致视频深度可以通过跨所有帧的立体匹配更有效地实现，这提供了更强的全局3D线索。而动态区域的一致性由于违反三角测量约束，仍然需要从大规模视频深度数据中学习以确保平滑过渡。基于这些见解，我们引入了StereoDiff，这是一种两阶段视频深度估计器，它将主要用于静态区域的立体匹配与用于保持动态区域深度平滑过渡的视频深度扩散相结合。我们通过频域分析数学地证明了立体匹配和视频深度扩散如何通过频域分析提供互补优势，突出了它们协同作用在捕捉两者优势方面的有效性。在零样本、真实世界、动态视频深度基准（包括室内和室外）上的实验结果表明，StereoDiff的性能达到了SoTA，展示了其在视频深度估计中卓越的一致性和准确性。", "summary": "StereoDiff是一种新型的两阶段视频深度估计器，旨在解决现有方法在处理视频中动静态区域不同深度一致性要求时的不足。它创新性地结合了立体匹配（用于静态区域提供全局3D线索）和视频深度扩散（用于动态区域确保平滑过渡），并通过频域分析证明了其互补优势。实验结果表明，StereoDiff在零样本、真实世界的动态视频深度基准上实现了最先进的性能，显著提高了视频深度估计的一致性和准确性。", "keywords": "视频深度估计, 立体匹配, 扩散模型, 时间一致性, StereoDiff", "comments": "该论文的创新点在于认识到视频深度估计中动静态区域一致性要求的差异，并提出了一个结合立体匹配和视频扩散的两阶段协同框架。这种方法有效利用了立体匹配的全局3D线索和扩散模型在动态区域平滑过渡上的优势，解决了传统方法的局限性。通过数学分析和SOTA性能验证，该工作为视频深度估计提供了一个新颖且高效的解决方案，具有重要的研究价值和应用潜力。"}}
{"id": "2506.20737", "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation", "authors": ["Gurusha Juneja", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "summary": "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20737v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20737v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "MAGPIE：一个用于多智能体上下文隐私评估的数据集", "tldr": "本文提出了一个新数据集MAGPIE，用于评估大型语言模型（LLM）代理在多智能体协作中对上下文隐私的理解和维护能力，并发现当前SOTA模型在此方面表现不佳。", "motivation": "随着基于LLM的智能体在协作任务中的广泛部署，隐私问题变得至关重要，因为这些智能体经常访问专有工具和机密数据库。现有的隐私评估基准不足以评估LLM智能体在复杂多轮对话中对上下文隐私的理解和维护能力。", "method": "本文首先提出了一个名为MAGPIE的新基准数据集，包含15个领域内的158个真实高风险场景，这些场景旨在权衡任务完成和隐私保护。然后，使用该数据集评估了包括GPT-4o和Claude-2.7-Sonnet在内的SOTA LLM，测试它们对上下文私有数据的理解以及在不侵犯用户隐私的情况下进行协作的能力。", "result": "实验表明，当前模型（包括GPT-4o和Claude-2.7-Sonnet）缺乏对上下文隐私的鲁棒理解，分别有25.2%和43.6%的时间将私有数据错误分类为可共享数据。在多轮对话中，即使有明确的隐私指示，这些模型仍在59.9%和50.5%的案例中泄露了私人信息。此外，多智能体系统在71%的场景中未能完成任务。", "conclusion": "当前模型未能同时实现上下文隐私保护和协作任务解决的目标。", "translation": "大型语言模型（LLM）代理的普及导致了代理间协作的日益增多，用于调度、谈判、资源分配等任务。在此类系统中，隐私至关重要，因为代理通常访问专有工具和需要严格保密的领域特定数据库。本文研究了基于LLM的代理是否表现出对上下文隐私的理解。并且，如果收到指示，这些系统是否在非对抗性多轮对话中保留推理时用户隐私。现有评估LLM代理上下文隐私的基准主要评估单轮、低复杂度的任务，其中私有信息可以很容易地被排除。我们首先提出了一个基准——MAGPIE，包含15个领域内的158个真实高风险场景。这些场景的设计使得完全排除私有数据会阻碍任务完成，但无限制的信息共享可能导致重大损失。然后，我们评估了当前最先进的LLM（a）它们对上下文私有数据的理解和（b）它们在不侵犯用户隐私的情况下进行协作的能力。实证实验表明，当前模型，包括GPT-4o和Claude-2.7-Sonnet，缺乏对上下文隐私的鲁棒理解，分别有25.2%和43.6%的时间将私有数据错误分类为可共享数据。在多轮对话中，即使在明确的隐私指示下，这些模型仍在59.9%和50.5%的案例中泄露了私人信息。此外，多智能体系统在71%的场景中未能完成任务。这些结果强调，当前模型并未同时实现上下文隐私保护和协作任务解决的目标。", "summary": "本文针对LLM代理在多智能体协作中面临的上下文隐私挑战，提出了一个名为MAGPIE的新型高风险场景数据集，用于评估LLM代理对隐私的理解和保护能力。通过对GPT-4o和Claude-2.7-Sonnet等SOTA模型的评估发现，现有模型在识别和保护上下文隐私方面表现不佳，在多轮对话中频繁泄露信息，并且在需要隐私保护的协作任务中完成率较低，表明当前模型在隐私保护与任务协作之间存在不一致性。", "keywords": "多智能体系统, 上下文隐私, LLM代理, 数据集, 隐私评估", "comments": "本文的创新之处在于提出了一个专门针对多智能体上下文隐私评估的高风险、真实场景数据集MAGPIE，填补了现有基准的空白。其重要性在于揭示了当前SOTA LLM在处理复杂隐私情境时的显著不足，对未来多智能体系统的设计和部署具有重要的指导意义。研究结果强调了在开发LLM代理时需要更深入地考虑隐私对齐，而不仅仅是任务完成。"}}
{"id": "2506.20688", "title": "Building Lightweight Semantic Segmentation Models for Aerial Images Using Dual Relation Distillation", "authors": ["Minglong Li", "Lianlei Shan", "Weiqiang Wang", "Ke Lv", "Bin Luo", "Si-Bao Chen"], "summary": "Recently, there have been significant improvements in the accuracy of CNN\nmodels for semantic segmentation. However, these models are often heavy and\nsuffer from low inference speed, which limits their practical application. To\naddress this issue, knowledge distillation has emerged as a promising approach\nto achieve a good trade-off between segmentation accuracy and efficiency. In\nthis paper, we propose a novel dual relation distillation (DRD) technique that\ntransfers both spatial and channel relations in feature maps from a cumbersome\nmodel (teacher) to a compact model (student). Specifically, we compute spatial\nand channel relation maps separately for the teacher and student models, and\nthen align corresponding relation maps by minimizing their distance. Since the\nteacher model usually learns more information and collects richer spatial and\nchannel correlations than the student model, transferring these correlations\nfrom the teacher to the student can help the student mimic the teacher better\nin terms of feature distribution, thus improving the segmentation accuracy of\nthe student model. We conduct comprehensive experiments on three segmentation\ndatasets, including two widely adopted benchmarks in the remote sensing field\n(Vaihingen and Potsdam datasets) and one popular benchmark in general scene\n(Cityscapes dataset). The experimental results demonstrate that our novel\ndistillation framework can significantly boost the performance of the student\nnetwork without incurring extra computational overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20688v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20688v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "使用双重关系蒸馏构建轻量级航空图像语义分割模型", "tldr": "本文提出了一种新颖的双重关系蒸馏（DRD）技术，通过从繁琐的教师模型向紧凑的学生模型传输空间和通道关系，显著提高了轻量级语义分割模型的性能，且不增加计算开销。", "motivation": "现有的CNN语义分割模型虽然精度高，但通常模型庞大且推理速度慢，限制了其在实际应用中的部署。知识蒸馏是一种有前景的方法，可以在分割精度和效率之间取得良好平衡。", "method": "本文提出了一种新颖的双重关系蒸馏（DRD）技术。该技术将特征图中的空间和通道关系从繁琐的教师模型传输到紧凑的学生模型。具体来说，分别计算教师和学生模型的空间和通道关系图，然后通过最小化它们之间的距离来对齐相应的关系图。", "result": "在三个分割数据集（包括Vaihingen、Potsdam和Cityscapes）上进行了综合实验。实验结果表明，所提出的新蒸馏框架可以显著提升学生网络的性能，且不会带来额外的计算开销。", "conclusion": "双重关系蒸馏（DRD）技术能够有效地将教师模型的丰富空间和通道相关性转移给学生模型，从而在不增加计算成本的情况下，显著提高轻量级学生模型在语义分割任务上的准确性，解决了模型效率与精度之间的权衡问题。", "translation": "最近，CNN模型在语义分割精度方面取得了显著进步。然而，这些模型通常很重且推理速度慢，这限制了它们的实际应用。为了解决这个问题，知识蒸馏已成为一种有前景的方法，可以在分割精度和效率之间取得良好平衡。在本文中，我们提出了一种新颖的双重关系蒸馏（DRD）技术，该技术将特征图中的空间和通道关系从繁琐的模型（教师）传输到紧凑的模型（学生）。具体来说，我们分别为教师和学生模型计算空间和通道关系图，然后通过最小化它们之间的距离来对齐相应的关系图。由于教师模型通常比学生模型学习到更多信息并收集更丰富的空间和通道关联，因此将这些关联从教师传输到学生可以帮助学生在特征分布方面更好地模仿教师，从而提高学生模型的分割精度。我们在三个分割数据集上进行了综合实验，其中包括遥感领域中广泛采用的两个基准（Vaihingen和Potsdam数据集）和一个通用场景中的流行基准（Cityscapes数据集）。实验结果表明，我们新颖的蒸馏框架可以显著提升学生网络的性能，而不会带来额外的计算开销。", "summary": "本文针对现有语义分割CNN模型在效率上的不足，提出了一种新颖的双重关系蒸馏（DRD）技术。该方法通过将教师模型的空间和通道关系图传输并对齐到学生模型，帮助轻量级学生模型更好地模仿教师的特征分布，从而提高其分割精度。实验证明，DRD框架能在不增加计算成本的情况下显著提升学生网络的性能，适用于航空图像及通用场景的语义分割。", "keywords": "语义分割, 知识蒸馏, 双重关系蒸馏, 轻量级模型, 航空图像", "comments": "这项工作在知识蒸馏领域提出了一种创新的双重关系蒸馏方法，同时考虑了空间和通道关系，这比传统的基于特征图或对数的方法更全面。它有效地解决了轻量级模型在语义分割中精度与效率的权衡问题，对于资源受限的实际应用具有重要意义。该方法在多个遥感和通用场景数据集上的验证，也证明了其泛化能力和有效性。"}}
{"id": "2506.20798", "title": "Physical Limits of Entanglement-Based Quantum Key Distribution over Long-Distance Satellite Links", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saif Al-Kuwari", "Khalid Qaraqe"], "summary": "Entanglement-based quantum key distribution (QKD) protocols, such as E91 and\nBBM92, offer strong information-theoretic security and are naturally suited for\nsatellite-to-satellite QKD (SatQKD) links. However, implementing these\nprotocols over long-distance inter-satellite free-space optical (FSO) channels\nposes critical physical-layer challenges that are not addressed in the existing\nliterature. In particular, photon losses due to beam divergence, pointing\nerrors, and background noise can severely degrade the key generation rate and\nquantum bit error rate (QBER), especially under narrow receiver field-of-view\n(FoV) constraints. This paper presents a comprehensive performance analysis of\nentanglement-based inter-satellite QKD, focusing on photon-level modeling and\nthe impact of practical impairments. We develop analytical expressions for\nsignal detection probabilities, background photon influence, multi-pair\nemissions, and QBER, incorporating key parameters such as link distance,\ntransmitter tracking jitter, receiver misalignment, and photon pair generation\nrate. Simulation results reveal the nonlinear sensitivity of system performance\nto tracking error and FoV limitations, and highlight optimal parameter regimes\nthat jointly maximize secret key rate while maintaining QBER below acceptable\nthresholds. The proposed model provides actionable design insights for reliable\nand efficient deployment of entanglement-based SatQKD systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20798v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20798v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基于纠缠的远距离卫星链路量子密钥分发的物理限制", "tldr": "本文研究了基于纠缠的卫星量子密钥分发(SatQKD)在远距离星间链路中的物理限制，特别是光子损耗、指向误差和背景噪声的影响，并提供了全面的性能分析和设计指导。", "motivation": "现有的文献没有解决在远距离星间自由空间光(FSO)信道中实现基于纠缠的QKD协议所面临的关键物理层挑战，特别是由于光束发散、指向误差和背景噪声引起的光子损耗会严重降低密钥生成速率和量子误码率(QBER)。", "method": "本文对基于纠缠的星间QKD进行了全面的性能分析，侧重于光子级建模和实际损伤（如光束发散、指向误差和背景噪声）的影响。开发了信号检测概率、背景光子影响、多对发射和QBER的解析表达式，并纳入了链路距离、发射器跟踪抖动、接收器失准和光子对生成速率等关键参数。通过仿真验证了模型。", "result": "仿真结果揭示了系统性能对跟踪误差和视场(FoV)限制的非线性敏感性，并突出了在保持QBER低于可接受阈值的同时共同最大化秘密密钥率的最佳参数范围。", "conclusion": "所提出的模型为可靠高效部署基于纠缠的卫星量子密钥分发(SatQKD)系统提供了可操作的设计见解。", "translation": "基于纠缠的量子密钥分发（QKD）协议，如E91和BBM92，提供了强大的信息论安全性，并且天然适用于卫星到卫星的QKD（SatQKD）链路。然而，在远距离星间自由空间光（FSO）信道上实现这些协议带来了现有文献中未解决的关键物理层挑战。特别是，由于光束发散、指向误差和背景噪声导致的光子损耗会严重降低密钥生成速率和量子误码率（QBER），尤其是在窄接收器视场（FoV）限制下。本文对基于纠缠的星间QKD进行了全面的性能分析，重点关注光子级建模和实际损伤的影响。我们开发了信号检测概率、背景光子影响、多对发射和QBER的解析表达式，其中包含了链路距离、发射器跟踪抖动、接收器失准和光子对生成速率等关键参数。仿真结果揭示了系统性能对跟踪误差和FoV限制的非线性敏感性，并突出了在保持QBER低于可接受阈值的同时共同最大化秘密密钥率的最佳参数范围。所提出的模型为可靠高效部署基于纠缠的SatQKD系统提供了可操作的设计见解。", "summary": "本文对基于纠缠的星间量子密钥分发(SatQKD)系统在远距离自由空间光(FSO)链路中的性能进行了深入分析，旨在解决现有研究中未充分探讨的物理层挑战，包括光子损耗、指向误差和背景噪声。研究通过建立光子级模型和解析表达式，详细分析了这些实际损伤对密钥生成速率和量子误码率(QBER)的影响。仿真结果揭示了系统性能对跟踪误差和接收器视场的非线性敏感性，并确定了在满足QBER要求下实现最大秘密密钥率的最佳参数配置。该研究为未来基于纠缠的SatQKD系统的设计和可靠部署提供了重要的理论指导。", "keywords": "量子密钥分发, 卫星通信, 纠缠, 自由空间光, 物理限制", "comments": "该论文的创新点在于对基于纠缠的星间QKD在实际远距离链路中的物理限制进行了深入的光子级建模和分析，填补了现有文献在处理此类挑战方面的空白。其重要性在于为未来可靠高效的卫星量子通信系统设计提供了关键的理论基础和实践指导，特别是针对光子损耗和指向误差等实际工程问题。"}}
{"id": "2506.20875", "title": "3DGH: 3D Head Generation with Composable Hair and Face", "authors": ["Chengan He", "Junxuan Li", "Tobias Kirschstein", "Artem Sevastopolsky", "Shunsuke Saito", "Qingyang Tan", "Javier Romero", "Chen Cao", "Holly Rushmeier", "Giljoo Nam"], "summary": "We present 3DGH, an unconditional generative model for 3D human heads with\ncomposable hair and face components. Unlike previous work that entangles the\nmodeling of hair and face, we propose to separate them using a novel data\nrepresentation with template-based 3D Gaussian Splatting, in which deformable\nhair geometry is introduced to capture the geometric variations across\ndifferent hairstyles. Based on this data representation, we design a 3D\nGAN-based architecture with dual generators and employ a cross-attention\nmechanism to model the inherent correlation between hair and face. The model is\ntrained on synthetic renderings using carefully designed objectives to\nstabilize training and facilitate hair-face separation. We conduct extensive\nexperiments to validate the design choice of 3DGH, and evaluate it both\nqualitatively and quantitatively by comparing with several state-of-the-art 3D\nGAN methods, demonstrating its effectiveness in unconditional full-head image\nsynthesis and composable 3D hairstyle editing. More details will be available\non our project page: https://c-he.github.io/projects/3dgh/.", "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://c-he.github.io/projects/3dgh/", "pdf_url": "http://arxiv.org/pdf/2506.20875v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20875v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "3DGH：可组合发型和面部的三维头部生成", "tldr": "3DGH是一个生成模型，可以生成具有可组合发型和面部组件的三维人头，解决了以往方法中头发和面部纠缠的问题。", "motivation": "以往的工作将头发和面部的建模纠缠在一起，限制了三维头部生成的可组合性。", "method": "提出了一种使用基于模板的三维高斯溅射的新型数据表示，引入可变形头发几何体以捕获发型变化。基于此，设计了具有双生成器的三维GAN架构，并采用交叉注意力机制建模头发和面部之间的相关性。模型在合成渲染图上训练，并使用精心设计的目标函数以稳定训练和促进头发-面部分离。", "result": "在无条件全头图像合成和可组合三维发型编辑方面表现出有效性。通过与几种最先进的三维GAN方法进行定性和定量比较，验证了其设计选择和有效性。", "conclusion": "3DGH成功地提出了一个新颖的无条件生成模型，用于生成具有解耦和可组合头发与面部组件的三维人头，在性能和编辑能力上优于现有方法。", "translation": "我们提出了3DGH，一个用于生成具有可组合发型和面部组件的三维人头的无条件生成模型。与以往将头发和面部建模纠缠在一起的工作不同，我们提出使用一种基于模板的三维高斯溅射的新型数据表示来分离它们，其中引入了可变形头发几何体以捕获不同发型之间的几何变化。基于这种数据表示，我们设计了一种具有双生成器的三维GAN架构，并采用交叉注意力机制来模拟头发和面部之间固有的相关性。该模型在合成渲染图上进行训练，并采用精心设计的目标函数以稳定训练并促进头发-面部分离。我们进行了广泛的实验来验证3DGH的设计选择，并通过与几种最先进的三维GAN方法进行比较，对其进行了定性和定量评估，证明了其在无条件全头图像合成和可组合三维发型编辑方面的有效性。更多详情将在我们的项目页面上提供：https://c-he.github.io/projects/3dgh/。", "summary": "3DGH提出了一种新颖的无条件三维人头生成模型，该模型将头发和面部组件解耦。它采用基于模板的三维高斯溅射表示（包含可变形头发几何体）以及具有双生成器和交叉注意力机制的三维GAN架构。3DGH在合成数据上训练，能够有效地进行全头合成和可组合发型编辑，优于现有三维GAN方法。", "keywords": "三维头部生成, 生成模型, 高斯溅射, 可组合, 头发-面部分离", "comments": "该论文的创新之处在于成功地将三维头部生成中的头发和面部组件解耦，这对于实现更可控和可组合的三维资产创建具有重要意义。在三维高斯溅射中引入可变形头发几何体以及使用带有交叉注意力机制的双生成器GAN是其关键的技术贡献。"}}
{"id": "2506.21090", "title": "Post-training for Deepfake Speech Detection", "authors": ["Wanying Ge", "Xin Wang", "Xuechen Liu", "Junichi Yamagishi"], "summary": "We introduce a post-training approach that adapts self-supervised learning\n(SSL) models for deepfake speech detection by bridging the gap between general\npre-training and domain-specific fine-tuning. We present AntiDeepfake models, a\nseries of post-trained models developed using a large-scale multilingual speech\ndataset containing over 56,000 hours of genuine speech and 18,000 hours of\nspeech with various artifacts in over one hundred languages. Experimental\nresults show that the post-trained models already exhibit strong robustness and\ngeneralization to unseen deepfake speech. When they are further fine-tuned on\nthe Deepfake-Eval-2024 dataset, these models consistently surpass existing\nstate-of-the-art detectors that do not leverage post-training. Model\ncheckpoints and source code are available online.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21090v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21090v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "深度伪造语音检测的后训练", "tldr": "本文介绍了一种后训练方法，通过弥合通用预训练和领域特定微调之间的差距，使自监督学习模型适应深度伪造语音检测。", "motivation": "为提升深度伪造语音检测模型的鲁棒性和泛化能力，弥补通用预训练与领域特定微调之间的差距。", "method": "引入一种后训练方法，使自监督学习（SSL）模型适应深度伪造语音检测。开发了AntiDeepfake模型，使用包含超过56,000小时真实语音和18,000小时带伪造痕迹语音的大规模多语言数据集进行后训练。", "result": "后训练模型对未见过的深度伪造语音表现出强大的鲁棒性和泛化能力。在Deepfake-Eval-2024数据集上进一步微调后，这些模型持续超越现有不使用后训练的最先进检测器。", "conclusion": "后训练方法显著提升了深度伪造语音检测模型的性能，使其在鲁棒性和泛化能力上优于现有技术。", "translation": "我们引入了一种后训练方法，通过弥合通用预训练和领域特定微调之间的差距，使自监督学习（SSL）模型适应深度伪造语音检测。我们提出了AntiDeepfake模型，这是一系列使用大规模多语言语音数据集开发的后训练模型，该数据集包含超过一百种语言的56,000小时真实语音和18,000小时带有各种伪造痕迹的语音。实验结果表明，后训练模型已经对未见过的深度伪造语音表现出强大的鲁棒性和泛化能力。当它们在Deepfake-Eval-2024数据集上进一步微调时，这些模型持续超越现有不利用后训练的最先进检测器。模型检查点和源代码已在线提供。", "summary": "本文提出了一种针对深度伪造语音检测的后训练方法，旨在弥合自监督学习模型在通用预训练和领域特定微调之间的鸿沟。研究团队开发了AntiDeepfake模型，利用一个大规模多语言数据集（包含超过5.6万小时真实语音和1.8万小时伪造语音）进行后训练。实验证明，这些后训练模型对未知深度伪造语音展现出卓越的鲁棒性和泛化能力，并在Deepfake-Eval-2024数据集上进一步微调后，性能超越了当前不采用后训练的最先进检测器。", "keywords": "深度伪造语音检测, 后训练, 自监督学习, AntiDeepfake, 鲁棒性", "comments": "这篇论文的创新点在于提出了“后训练”的概念，有效地将大规模通用预训练模型与特定领域的深度伪造检测任务相结合，弥补了二者之间的差距。通过利用大规模多语言数据集进行后训练，显著提升了模型的鲁棒性和泛化能力，为深度伪造语音检测领域提供了一种新的、更有效的方法。其成果超越了现有SOTA，具有重要的实践意义。"}}
{"id": "2506.20687", "title": "Review of Three Variants of the k-d Tree", "authors": ["Russell A. Brown"], "summary": "The original description of the k-d tree recognized that rebalancing\ntechniques, such as used to build an AVL tree or a red-black tree, are not\napplicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is\nnecessary to find the median of a set of data for each recursive subdivision of\nthat set. The sort or selection used to find the median, and the technique used\nto partition the set about that median, strongly influence the computational\ncomplexity of building a k-d tree. This article describes and contrasts three\nvariants of the k-d tree that differ in their technique used to partition the\nset, and compares the performance of those variants. In addition, dual-threaded\nexecution is proposed and analyzed for one of the three variants.", "comment": "29 pages, 11 figures, one listing, one table", "pdf_url": "http://arxiv.org/pdf/2506.20687v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20687v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "k-d树三种变体的综述", "tldr": "本文综述并对比了k-d树的三种变体，重点关注它们的分区技术和计算复杂性，并提出并分析了其中一种变体的双线程执行。", "motivation": "k-d树的原始描述指出，传统的再平衡技术不适用于k-d树，因此需要通过寻找中位数和数据集分区来构建平衡的k-d树。用于寻找中位数和分区的技术会强烈影响构建k-d树的计算复杂性，这促使了对不同变体的分析和比较。", "method": "本文描述并对比了k-d树的三种变体，这些变体在数据集分区技术上有所不同。论文比较了这些变体的性能，并为其中一种变体提出了双线程执行方案并进行了分析。", "result": "本文对比了k-d树三种变体的性能，并对其中一种变体的双线程执行进行了提案和分析。", "conclusion": "Not mentioned in abstract", "translation": "k-d树的原始描述认识到，像用于构建AVL树或红黑树那样的再平衡技术不适用于k-d树。因此，为了构建一个平衡的k-d树，有必要在每次递归细分数据集时找到该数据集的中位数。用于寻找中位数以及围绕该中位数分区数据集的排序或选择技术，强烈影响构建k-d树的计算复杂性。本文描述并对比了k-d树的三种变体，它们在用于分区数据集的技术上有所不同，并比较了这些变体的性能。此外，还针对这三种变体中的一种提出了双线程执行并进行了分析。", "summary": "本文综述了k-d树的三种变体，这些变体主要在数据分区技术上有所区别。鉴于k-d树无法应用传统再平衡技术，构建平衡树需依赖中位数查找和数据集分区，而这些技术显著影响其计算复杂性。论文描述、对比并评估了这些变体的性能，同时还为其中一种变体提出并分析了双线程执行方案。", "keywords": "k-d树, 数据结构, 分区, 计算复杂性, 性能", "comments": "这篇论文关注了k-d树的一个核心性能方面：分区策略。通过分析现有变体并提出像双线程执行这样的优化方案，对于实际应用中构建效率至关重要的k-d树实现具有重要价值。"}}
{"id": "2506.20844", "title": "The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "summary": "Scientific fact-checking aims to determine the veracity of scientific claims\nby retrieving and analysing evidence from research literature. The problem is\ninherently more complex than general fact-checking since it must accommodate\nthe evolving nature of scientific knowledge, the structural complexity of\nacademic literature and the challenges posed by long-form, multimodal\nscientific expression. However, existing approaches focus on simplified\nversions of the problem based on small-scale datasets consisting of abstracts\nrather than full papers, thereby avoiding the distinct challenges associated\nwith processing complete documents. This paper examines the limitations of\ncurrent scientific fact-checking systems and reveals the many potential\nfeatures and resources that could be exploited to advance their performance. It\nidentifies key research challenges within evidence retrieval, including (1)\nevidence-driven retrieval that addresses semantic limitations and topic\nimbalance (2) time-aware evidence retrieval with citation tracking to mitigate\noutdated information, (3) structured document parsing to leverage long-range\ncontext, (4) handling complex scientific expressions, including tables,\nfigures, and domain-specific terminology and (5) assessing the credibility of\nscientific literature. Preliminary experiments were conducted to substantiate\nthese challenges and identify potential solutions. This perspective paper aims\nto advance scientific fact-checking with a specialised IR system tailored for\nreal-world applications.", "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR'25)", "pdf_url": "http://arxiv.org/pdf/2506.20844v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20844v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "科学事实核查的下一阶段：从复杂结构化学术论文中进行高级证据检索", "tldr": "当前的科学事实核查系统在处理复杂的完整学术论文时面临挑战。本文揭示了证据检索的关键挑战，并提出需要一个专门的信息检索系统来推进科学事实核查。", "motivation": "现有科学事实核查方法侧重于简化问题和小型数据集（摘要），未能有效处理完整学术论文的结构复杂性、科学知识的演变性质以及多模态科学表达。因此，需要开发更先进的系统来克服这些局限性并提升性能。", "method": "本文审视了当前科学事实核查系统的局限性，揭示了可利用的潜在特征和资源。它识别了证据检索中的五大关键研究挑战（包括语义、时间、结构化解析、复杂表达和可信度评估），并进行了初步实验来证实这些挑战并确定潜在解决方案。这是一篇旨在通过专业信息检索系统推进科学事实核查的观点论文。", "result": "本文识别了科学事实核查中证据检索的五大关键研究挑战，并指出需要一个专门的信息检索系统来克服这些挑战。初步实验证实了这些挑战的存在并提供了潜在的解决方案方向。", "conclusion": "本文旨在通过开发一个为实际应用量身定制的专业信息检索系统，来推进科学事实核查的领域，从而克服当前系统在处理复杂学术论文方面的局限性。", "translation": "科学事实核查旨在通过从研究文献中检索和分析证据来确定科学主张的真实性。这个问题比一般的事实核查本质上更复杂，因为它必须适应科学知识的演变性质、学术文献的结构复杂性以及长篇、多模态科学表达带来的挑战。然而，现有方法侧重于基于小规模数据集（由摘要而非完整论文组成）的简化问题版本，从而避免了处理完整文档所带来的独特挑战。本文审视了当前科学事实核查系统的局限性，并揭示了许多可用于提升其性能的潜在特征和资源。它指出了证据检索中的关键研究挑战，包括 (1) 解决语义限制和主题不平衡的证据驱动检索，(2) 结合引文跟踪以缓解过时信息的时间感知证据检索，(3) 利用长程上下文的结构化文档解析，(4) 处理复杂的科学表达，包括表格、图表和领域特定术语，以及 (5) 评估科学文献的可信度。进行了初步实验以证实这些挑战并确定潜在解决方案。这篇观点论文旨在通过一个为实际应用量身定制的专业信息检索系统来推进科学事实核查。", "summary": "这篇观点论文探讨了当前科学事实核查系统的局限性，指出它们未能有效处理完整学术论文的复杂性。文章识别了高级证据检索中的五大关键挑战，包括语义和时间问题、结构化文档解析、复杂科学表达处理以及可信度评估。论文强调，需要开发一个专门的信息检索系统，以应对这些挑战并推进科学事实核查在实际应用中的发展。初步实验支持了所识别的挑战和潜在解决方案。", "keywords": "科学事实核查, 证据检索, 信息检索, 学术论文, 文档解析", "comments": "该论文的创新之处在于将科学事实核查的焦点从简化的摘要层面转移到更具挑战性和现实意义的完整、复杂学术文档。它系统性地识别了证据检索中的关键技术和概念障碍，为未来构建鲁棒的、面向实际应用的科学事实核查系统提供了路线图。其对结构化解析、时间感知检索和多模态内容的强调尤其具有前瞻性。"}}
{"id": "2506.20793", "title": "Multi-lingual Functional Evaluation for Large Language Models", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20793v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20793v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "大型语言模型的多语言功能评估", "tldr": "本文创建了新的多语言功能基准（CL-GSM Symbolic和CL-IFEval），通过将现有英文基准翻译成五种语言，以更好地评估大型语言模型在多语言环境下的实际性能和鲁棒性。研究发现，静态基准可能无法完全捕捉功能性能，且模型在不同语言间的鲁棒性差异显著。", "motivation": "大型语言模型的多语言能力评估通常依赖静态数据基准，但这些评估往往无法充分理解模型在多语言环境下的实际性能和鲁棒性。", "method": "研究人员通过将现有的英文功能基准模板翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种额外语言，创建了多语言功能基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）。", "result": "研究结果表明，一些静态多语言基准在捕捉功能性能方面与新基准差异显著（例如，M-GSM与CL-GSM Symbolic在英语、法语和西班牙语中的性能分别下降24%、17%和18%；Belebele与CL-IFEval在不同语言间有15-24%的性能下降；M-MMLU与CL-IFEval仅有0.5%至3%的性能下降）。同时发现，模型在不同语言间的鲁棒性差异显著，某些语言（如阿拉伯语、英语）在评估迭代中表现最为稳定。", "conclusion": "静态多语言基准可能无法完全反映大型语言模型的实际功能性能和鲁棒性。新创建的多语言功能基准提供了更深入的评估视角，揭示了模型在不同语言环境下的性能差异和鲁棒性问题。", "translation": "大型语言模型的多语言能力通常通过静态数据基准进行评估，例如Belebele、M-MMLU和M-GSM。然而，这些评估往往未能充分理解模型在多语言环境下的实际性能和鲁棒性。为此，我们创建了多语言功能基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）——通过将现有英文功能基准模板翻译成五种额外的语言，这些语言涵盖了自然语言处理（NLP）可用资源的范围：法语、西班牙语、印地语、阿拉伯语和约鲁巴语。我们的结果显示，一些静态多语言基准比其他基准更能紧密地捕捉功能性能（即，在不同模型中，M-GSM与CL-GSM Symbolic在英语、法语和西班牙语中的性能分别下降24%、17%和18%；类似地，Belebele与CL-IFEval在不同语言间有15-24%的性能下降，而M-MMLU与CL-IFEval之间仅有0.5%至3%的性能下降）。同样，我们发现模型在不同语言间的鲁棒性差异显著，某些语言（例如阿拉伯语、英语）在评估迭代中表现最为稳定。", "summary": "本文针对现有静态多语言基准未能充分评估大型语言模型实际性能和鲁棒性的问题，提出了新的多语言功能基准CL-GSM Symbolic和CL-IFEval。通过将英文功能基准翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语，研究发现静态基准与功能性能之间存在显著差异，且模型在不同语言间的鲁棒性表现不一，揭示了当前多语言LLM评估的局限性。", "keywords": "多语言评估, 大型语言模型, 功能基准, 鲁棒性, 跨语言", "comments": "本文的创新点在于提出了新的多语言功能评估基准，这对于更准确地理解大型语言模型在真实多语言场景下的表现至关重要。其重要性体现在揭示了现有静态基准的局限性，并为未来LLM的多语言能力评估提供了新的方向和工具。研究结果表明，仅依赖静态基准可能会高估或误判模型在实际应用中的表现。"}}
{"id": "2506.20809", "title": "Boundary integral equation analysis for spheroidal suspensions", "authors": ["Leo Crowder", "Tianyue Li", "Eduardo Corona", "Shravan Veerapaneni"], "summary": "In this work, we provide a fast, spectrally accurate method for the\nevaluation of boundary integral operators (BIOs) on a suspension of prolate and\noblate spheroids. We first derive formulas for the standard layer potential\noperators for the Laplace equation applied to an expansion of the integral\ndensities in the appropriate spheroidal harmonic basis. These then lead to\nanalytical expressions in solid harmonics that allow spectrally accurate\nevaluation of near-field particle interactions. Finally, a standard quadrature\nscheme is used to evaluate smooth, far-field interactions; these are then\naccelerated using the fast multipole method.\n  Through a number of numerical test cases, we verify the accuracy and\nefficiency of our BIO evaluation framework for dense, polydisperse suspensions\nof spheroids. Through the use of standard formulas linking Stokes and Laplace\npotentials, we show our scheme can be readily applied to problems involving\nparticulate suspension flows. For both Laplace and Stokes, our method allows us\nto evaluate BIOs for suspensions up to hundreds of particles on a single\nprocessor.", "comment": "Submitted to Journal of Computational Physics, June 2025", "pdf_url": "http://arxiv.org/pdf/2506.20809v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20809v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "扁球体悬浮液的边界积分方程分析", "tldr": "本文提出了一种快速、谱精确的方法，用于评估扁球体悬浮液上的边界积分算子（BIOs），并验证了其在拉普拉斯和斯托克斯问题中的准确性和效率。", "motivation": "本文旨在提供一种快速、谱精确的方法，用于评估长扁球体和扁扁球体悬浮液上的边界积分算子（BIOs）。", "method": "该方法首先推导了在适当扁球谐波基中展开积分密度的拉普拉斯方程标准层势算子的公式。这些公式进而得到固体谐波中的解析表达式，从而实现近场粒子相互作用的谱精确评估。远场相互作用则使用标准正交方案进行评估，并通过快速多极子方法加速。该方案通过连接斯托克斯和拉普拉斯势的标准公式，可应用于涉及颗粒悬浮流的问题。", "result": "通过大量数值测试案例，验证了该BIO评估框架在稠密、多分散扁球体悬浮液中的准确性和效率。对于拉普拉斯和斯托克斯问题，该方法能够在单个处理器上评估多达数百个粒子的悬浮液的BIOs。", "conclusion": "该研究提出了一种高效、精确的边界积分算子评估框架，适用于扁球体悬浮液，并能有效处理拉普拉斯和斯托克斯问题，显示出良好的可扩展性。", "translation": "在这项工作中，我们提供了一种快速、谱精确的方法，用于评估长扁球体和扁扁球体悬浮液上的边界积分算子（BIOs）。我们首先推导了将积分密度在适当的扁球谐波基中展开后应用于拉普拉斯方程的标准层势算子的公式。这些公式进而得到了固体谐波中的解析表达式，从而实现了近场粒子相互作用的谱精确评估。最后，使用标准正交方案评估平滑的远场相互作用；这些相互作用随后通过快速多极子方法加速。\n通过大量的数值测试案例，我们验证了我们的BIO评估框架在稠密、多分散的扁球体悬浮液中的准确性和效率。通过使用连接斯托克斯和拉普拉斯势的标准公式，我们表明我们的方案可以很容易地应用于涉及颗粒悬浮流的问题。对于拉普拉斯和斯托克斯问题，我们的方法允许我们在单个处理器上评估多达数百个粒子的悬浮液的BIOs。", "summary": "本文提出了一种用于评估扁球体悬浮液边界积分算子（BIOs）的快速、谱精确方法。该方法结合了基于扁球谐波和固体谐波的解析表达式用于近场相互作用，以及结合快速多极子方法的标准正交方案用于远场相互作用。数值测试验证了其在稠密、多分散悬浮液中的准确性和效率，并且该方案通过连接斯托克斯和拉普拉斯势的公式，能够处理颗粒悬浮流问题，并在单个处理器上支持多达数百个粒子。", "keywords": "边界积分方程, 扁球体, 快速多极子方法, 拉普拉斯, 斯托克斯", "comments": "这项工作提出了一种新颖且高效的计算框架，用于处理复杂形状粒子（扁球体）的边界积分算子问题。其创新点在于结合了不同尺度的分析和数值方法（近场解析表达式与远场FMM加速），实现了谱精确性和计算效率的平衡。该方法对理解和模拟流体中颗粒悬浮液的行为具有重要意义，尤其是在需要高精度计算的领域。"}}
{"id": "2506.21086", "title": "PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching", "authors": ["Guillem Cortès-Sebastià", "Benjamin Martin", "Emilio Molina", "Xavier Serra", "Romain Hennequin"], "summary": "This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)\nsystem designed specifically around spectral peaks. This novel system is\ndesigned to leverage the sparse spectral coordinates typically computed by\ntraditional peak-based AFP methods. PeakNetFP performs hierarchical point\nfeature extraction techniques similar to the computer vision model PointNet++,\nand is trained using contrastive learning like in the state-of-the-art deep\nlearning AFP, NeuralFP. This combination allows PeakNetFP to outperform\nconventional AFP systems and achieves comparable performance to NeuralFP when\nhandling challenging time-stretched audio data. In extensive evaluation,\nPeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging\nfrom 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:\ncompared to NeuralFP, it has 100 times fewer parameters and uses 11 times\nsmaller input data. These features make PeakNetFP a lightweight and efficient\nsolution for AFP tasks where time stretching is involved. Overall, this system\nrepresents a promising direction for future AFP technologies, as it\nsuccessfully merges the lightweight nature of peak-based AFP with the\nadaptability and pattern recognition capabilities of neural network-based\napproaches, paving the way for more scalable and efficient solutions in the\nfield.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21086v1", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.3.1; H.3.3; H.3.4"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21086v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PeakNetFP：基于峰值的神经音频指纹识别，对极端时间拉伸具有鲁棒性", "tldr": "PeakNetFP是一种新型的神经音频指纹识别系统，它结合了传统峰值方法的稀疏性与神经网络的适应性，在处理时间拉伸音频时性能优异，并且参数更少、输入数据更小。", "motivation": "传统的音频指纹识别方法在处理极端时间拉伸的音频数据时可能表现不佳。该研究旨在开发一种对时间拉伸具有鲁棒性的高效神经音频指纹识别系统。", "method": "PeakNetFP是第一个专门围绕频谱峰值设计的神经音频指纹识别系统。它利用传统基于峰值的AFP方法计算的稀疏频谱坐标。系统采用类似于PointNet++的分层点特征提取技术，并使用对比学习进行训练，类似于NeuralFP。", "result": "PeakNetFP在处理时间拉伸音频数据时，性能优于传统AFP系统，并与最先进的NeuralFP表现相当。在50%到200%的时间拉伸因子范围内，其Top-1命中率保持在90%以上。与NeuralFP相比，PeakNetFP的参数数量减少了100倍，输入数据量减少了11倍。", "conclusion": "PeakNetFP成功地将基于峰值的AFP的轻量级特性与基于神经网络的方法的适应性和模式识别能力相结合，为未来的AFP技术提供了有前景的方向，有望实现更具可扩展性和效率的解决方案。", "translation": "这项工作引入了PeakNetFP，这是第一个专门围绕频谱峰值设计的神经音频指纹识别（AFP）系统。这个新颖的系统旨在利用传统基于峰值的AFP方法通常计算的稀疏频谱坐标。PeakNetFP执行类似于计算机视觉模型PointNet++的分层点特征提取技术，并使用对比学习进行训练，如同最先进的深度学习AFP系统NeuralFP。这种组合使得PeakNetFP在处理具有挑战性的时间拉伸音频数据时，性能优于传统AFP系统，并实现了与NeuralFP相当的性能。在广泛的评估中，PeakNetFP在50%到200%的时间拉伸因子范围内保持了超过90%的Top-1命中率。此外，PeakNetFP提供了显著的效率优势：与NeuralFP相比，它的参数数量减少了100倍，输入数据量减少了11倍。这些特性使得PeakNetFP成为涉及时间拉伸的AFP任务的轻量级高效解决方案。总的来说，该系统代表了未来AFP技术的一个有前景的方向，因为它成功地将基于峰值的AFP的轻量级特性与基于神经网络的方法的适应性和模式识别能力相结合，为该领域更具可扩展性和效率的解决方案铺平了道路。", "summary": "PeakNetFP是一种创新的神经音频指纹识别（AFP）系统，它首次专门利用音频频谱峰值。该系统结合了PointNet++的分层特征提取和对比学习，旨在应对极端时间拉伸。实验证明，PeakNetFP在时间拉伸音频上性能优越，与NeuralFP相当，且在效率上具有显著优势，参数量和输入数据量大幅减少，使其成为轻量级高效的AFP解决方案。", "keywords": "音频指纹识别, 神经网络, 频谱峰值, 时间拉伸, PeakNetFP", "comments": "PeakNetFP的创新之处在于将传统的基于峰值的音频指纹识别的轻量级特性与神经网络的强大模式识别能力相结合，特别是在处理时间拉伸方面。其显著的效率提升（参数减少100倍，输入数据减少11倍）使其在实际应用中具有很高的价值和潜力，为音频指纹识别领域提供了一个可扩展且高效的新范式。"}}
{"id": "2506.21414", "title": "Accelerating GNN Training through Locality-aware Dropout and Merge", "authors": ["Gongjian Sun", "Mingyu Yan", "Dengke Han", "Runzhen Xue", "Duo Wang", "Xiaochun Ye", "Dongrui Fan"], "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in graph\nlearning and are widely adopted across various critical domains. However, the\nirregular connectivity between vertices leads to inefficient neighbor\naggregation, resulting in substantial irregular and coarse-grained DRAM\naccesses. This lack of data locality presents significant challenges for\nexecution platforms, ultimately degrading performance. While previous\naccelerator designs have leveraged on-chip memory and data access scheduling\nstrategies to address this issue, they still inevitably access features at\nirregular addresses from DRAM. In this work, we propose LiGNN, a hardware-based\nsolution that improves data locality by applying dropout and merge techniques\nduring neighbor aggregation to accelerate GNN training. Unlike conventional\nalgorithm-level dropout methods that primarily aim to improve accuracy while\noverlooking hardware costs, LiGNN introduces a locality-aware feature dropout\nmechanism. This approach selectively drops node features with data locality\nawareness, effectively reducing irregular DRAM accesses without compromising\nmodel accuracy. Moreover, by leveraging detailed knowledge of memory layout and\norganization-including critical alignment constraints-LiGNN strategically\nmerges memory accesses during neighbor aggregation at the DRAM row level,\nguided by GNN-level semantics. This optimization significantly improves data\nlocality with minimal additional cost. Under the commonly adopted 0.5 dropout\nrate, LiGNN outperforms state-of-the-art methods, delivering a 1.48~3.02x\nspeedup, reducing DRAM accesses by 34%~55%, and lowering DRAM row activations\nby 59%~82%, all while maintaining model accuracy.", "comment": "under review in TPDS. extend version of DATE 2025", "pdf_url": "http://arxiv.org/pdf/2506.21414v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21414v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过局部性感知Dropout和合并加速GNN训练", "tldr": "LiGNN是一种硬件加速方案，通过局部性感知的特征丢弃和DRAM行级内存访问合并，显著提升GNN训练速度并减少DRAM访问，同时保持模型精度。", "motivation": "图神经网络（GNN）在图学习中取得巨大成功，但顶点间的不规则连接导致低效的邻居聚合，产生大量不规则和粗粒度的DRAM访问，缺乏数据局部性，从而降低性能。现有加速器设计未能完全解决不规则DRAM访问问题。", "method": "本研究提出了LiGNN，一种硬件加速方案，通过在邻居聚合过程中应用局部性感知Dropout和合并技术来提高数据局部性。LiGNN引入了一种局部性感知的特征丢弃机制，选择性地丢弃节点特征，有效减少不规则DRAM访问而不损害模型精度。此外，LiGNN利用内存布局和组织知识，在DRAM行级别战略性地合并邻居聚合期间的内存访问。", "result": "在常用的0.5丢弃率下，LiGNN比现有最佳方法提速1.48~3.02倍，将DRAM访问减少34%~55%，DRAM行激活减少59%~82%，同时保持模型精度。", "conclusion": "LiGNN通过硬件级的局部性感知Dropout和内存访问合并，有效解决了GNN训练中数据局部性差导致性能下降的问题，显著加速了GNN训练并降低了DRAM能耗，同时保持了模型精度。", "translation": "图神经网络（GNNs）在图学习中取得了显著成功，并被广泛应用于各种关键领域。然而，顶点之间不规则的连接导致低效的邻居聚合，从而产生大量的非规则和粗粒度的DRAM访问。这种数据局部性的缺乏给执行平台带来了重大挑战，最终降低了性能。虽然之前的加速器设计利用了片上内存和数据访问调度策略来解决这个问题，但它们仍然不可避免地从DRAM以不规则地址访问特征。在这项工作中，我们提出了LiGNN，一种基于硬件的解决方案，通过在邻居聚合期间应用丢弃（dropout）和合并（merge）技术来提高数据局部性，从而加速GNN训练。与主要旨在提高精度而忽视硬件成本的传统算法级丢弃方法不同，LiGNN引入了一种局部性感知的特征丢弃机制。这种方法选择性地丢弃具有数据局部性意识的节点特征，在不损害模型精度的情况下有效减少不规则DRAM访问。此外，通过利用内存布局和组织（包括关键对齐约束）的详细知识，LiGNN在GNN级语义的指导下，在DRAM行级别战略性地合并邻居聚合期间的内存访问。这种优化以最小的额外成本显著提高了数据局部性。在常用的0.5丢弃率下，LiGNN优于现有最先进的方法，实现了1.48~3.02倍的加速，将DRAM访问减少了34%~55%，并将DRAM行激活降低了59%~82%，同时保持了模型精度。", "summary": "本论文提出LiGNN，一种硬件加速方案，旨在解决GNN训练中因不规则连接导致的低效邻居聚合和DRAM访问问题。LiGNN通过引入局部性感知特征丢弃机制，选择性地减少不规则DRAM访问，并在DRAM行级别战略性合并内存访问，以提高数据局部性。实验结果表明，LiGNN在保持模型精度的同时，显著加速了GNN训练，并大幅减少了DRAM访问和行激活。", "keywords": "GNN训练, 数据局部性, 硬件加速, Dropout, 内存合并", "comments": "LiGNN的创新之处在于将硬件加速与算法优化相结合，特别是引入了“局部性感知”的Dropout机制，这与传统Dropout仅关注精度不同，它同时考虑了硬件效率。此外，DRAM行级内存访问合并也极大地提升了数据局部性，有效解决了GNN训练中长期存在的内存访问效率低下的问题。这项工作为未来的GNN加速器设计提供了新的思路。"}}
{"id": "2506.20926", "title": "CodeGuard: A Generalized and Stealthy Backdoor Watermarking for Generative Code Models", "authors": ["Haoxuan Li", "Jiale Zhang", "Xiaobing Sun", "Xiapu Luo"], "summary": "Generative code models (GCMs) significantly enhance development efficiency\nthrough automated code generation and code summarization. However, building and\ntraining these models require computational resources and time, necessitating\neffective digital copyright protection to prevent unauthorized leaks and\nmisuse. Backdoor watermarking, by embedding hidden identifiers, simplifies\ncopyright verification by breaking the model's black-box nature. Current\nbackdoor watermarking techniques face two main challenges: first, limited\ngeneralization across different tasks and datasets, causing fluctuating\nverification rates; second, insufficient stealthiness, as watermarks are easily\ndetected and removed by automated methods. To address these issues, we propose\nCodeGuard, a novel watermarking method combining attention mechanisms with\ndistributed trigger embedding strategies. Specifically, CodeGuard employs\nattention mechanisms to identify watermark embedding positions, ensuring\nverifiability. Moreover, by using homomorphic character replacement, it avoids\nmanual detection, while distributed trigger embedding reduces the likelihood of\nautomated detection. Experimental results demonstrate that CodeGuard achieves\nup to 100% watermark verification rates in both code summarization and code\ngeneration tasks, with no impact on the primary task performance. In terms of\nstealthiness, CodeGuard performs exceptionally, with a maximum detection rate\nof only 0.078 against ONION detection methods, significantly lower than\nbaseline methods.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.20926v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20926v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CodeGuard：一种用于生成式代码模型的通用且隐蔽的后门水印技术", "tldr": "CodeGuard是一种新型后门水印技术，通过结合注意力机制和分布式触发器嵌入，解决了生成式代码模型水印在通用性和隐蔽性方面的挑战，实现了高验证率和低检测率。", "motivation": "生成式代码模型（GCMs）的训练成本高昂，需要有效的数字版权保护以防止未经授权的泄露和滥用。现有的后门水印技术存在两个主要挑战：一是跨任务和数据集的泛化能力有限，导致验证率波动；二是隐蔽性不足，水印容易被自动化方法检测和移除。", "method": "本文提出了CodeGuard，一种结合注意力机制和分布式触发器嵌入策略的新型水印方法。CodeGuard利用注意力机制识别水印嵌入位置以确保可验证性；通过同形字符替换避免人工检测；并通过分布式触发器嵌入降低自动化检测的可能性。", "result": "实验结果表明，CodeGuard在代码摘要和代码生成任务中均实现了高达100%的水印验证率，且不影响主要任务性能。在隐蔽性方面，CodeGuard表现出色，针对ONION检测方法的最高检测率仅为0.078，远低于基线方法。", "conclusion": "CodeGuard成功解决了生成式代码模型后门水印在通用性和隐蔽性方面的挑战，实现了高验证率和低检测率，同时保持了模型的主要任务性能。", "translation": "生成式代码模型（GCMs）通过自动化代码生成和代码摘要显著提升了开发效率。然而，构建和训练这些模型需要大量的计算资源和时间，因此需要有效的数字版权保护来防止未经授权的泄露和滥用。后门水印技术通过嵌入隐藏标识符，打破了模型的黑盒性质，从而简化了版权验证。当前后门水印技术面临两大挑战：首先，在不同任务和数据集上的泛化能力有限，导致验证率波动；其次，隐蔽性不足，水印容易被自动化方法检测和移除。为了解决这些问题，我们提出了CodeGuard，一种结合注意力机制和分布式触发器嵌入策略的新型水印方法。具体而言，CodeGuard采用注意力机制来识别水印嵌入位置，确保可验证性。此外，通过使用同形字符替换，它避免了人工检测，而分布式触发器嵌入则降低了自动化检测的可能性。实验结果表明，CodeGuard在代码摘要和代码生成任务中均实现了高达100%的水印验证率，且不影响主要任务性能。在隐蔽性方面，CodeGuard表现出色，针对ONION检测方法的最高检测率仅为0.078，显著低于基线方法。", "summary": "本文提出了一种名为CodeGuard的新型后门水印技术，旨在解决生成式代码模型版权保护中现有水印方法在泛化性和隐蔽性上的不足。CodeGuard结合了注意力机制以精确识别水印嵌入位置，并采用同形字符替换和分布式触发器嵌入策略来增强水印的隐蔽性，使其难以被人工或自动化方法检测。实验证明，CodeGuard在代码摘要和代码生成任务中均能达到近乎完美的水印验证率，同时对模型主要性能无影响，并且在隐蔽性方面显著优于现有基线方法。", "keywords": "生成式代码模型, 后门水印, CodeGuard, 隐蔽性, 泛化性", "comments": "CodeGuard的创新点在于其结合注意力机制、同形字符替换和分布式触发器嵌入，有效提升了后门水印的通用性和隐蔽性，这对于保护生成式代码模型的知识产权具有重要意义。其在不影响模型核心性能的前提下，实现了高验证率和极低的检测率，展现了该方法的实用性和先进性。"}}
{"id": "2506.20918", "title": "Metadata Enrichment of Long Text Documents using Large Language Models", "authors": ["Manika Lamba", "You Peng", "Sophie Nikolov", "Glen Layne-Worthey", "J. Stephen Downie"], "summary": "In this project, we semantically enriched and enhanced the metadata of long\ntext documents, theses and dissertations, retrieved from the HathiTrust Digital\nLibrary in English published from 1920 to 2020 through a combination of manual\nefforts and large language models. This dataset provides a valuable resource\nfor advancing research in areas such as computational social science, digital\nhumanities, and information science. Our paper shows that enriching metadata\nusing LLMs is particularly beneficial for digital repositories by introducing\nadditional metadata access points that may not have originally been foreseen to\naccommodate various content types. This approach is particularly effective for\nrepositories that have significant missing data in their existing metadata\nfields, enhancing search results and improving the accessibility of the digital\nrepository.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20918v1", "categories": ["cs.DL", "cs.ET", "cs.IR"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.20918v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用大型语言模型对长文本文件进行元数据丰富", "tldr": "本文结合人工和大型语言模型，对HathiTrust数字图书馆的长文本文档元数据进行了语义丰富和增强，提高了数字存储库的搜索结果和可访问性。", "motivation": "该研究旨在提高HathiTrust数字图书馆中长文本文档（论文和学位论文）的元数据质量，以适应各种内容类型，并解决现有元数据字段中大量缺失数据的问题，从而促进计算社会科学、数字人文和信息科学等领域的研究。", "method": "通过结合人工努力和大型语言模型，对从HathiTrust数字图书馆检索到的1920年至2020年间出版的英文长文本文档、论文和学位论文的元数据进行语义丰富和增强。", "result": "该方法为数字存储库引入了额外的元数据访问点，增强了搜索结果，并提高了数字存储库的可访问性。所创建的数据集为计算社会科学、数字人文和信息科学等领域的研究提供了宝贵资源。", "conclusion": "使用大型语言模型丰富元数据对数字存储库特别有益，尤其是在现有元数据缺失严重的情况下，能够显著提高搜索效率和可访问性。", "translation": "在这个项目中，我们结合人工努力和大型语言模型，对从HathiTrust数字图书馆检索到的1920年至2020年间出版的英文长文本文档、论文和学位论文的元数据进行了语义丰富和增强。这个数据集为计算社会科学、数字人文和信息科学等领域的研究提供了宝贵的资源。我们的论文表明，使用大型语言模型丰富元数据对于数字存储库特别有益，它引入了最初可能未预见到的额外元数据访问点，以适应各种内容类型。这种方法对于现有元数据字段中存在大量缺失数据的存储库尤其有效，能够增强搜索结果并提高数字存储库的可访问性。", "summary": "本文介绍了通过结合人工和大型语言模型，对HathiTrust数字图书馆中1920-2020年间的英文长文本文档（包括论文和学位论文）元数据进行语义丰富和增强的项目。研究表明，使用大型语言模型丰富元数据对数字存储库非常有益，尤其是在元数据缺失严重的情况下，能够增加访问点、提高搜索结果和可访问性。所创建的数据集对计算社会科学、数字人文和信息科学等领域的研究具有重要价值。", "keywords": "元数据丰富, 大型语言模型, 数字图书馆, HathiTrust, 长文本文档", "comments": "该论文通过结合人工和大型语言模型，为数字图书馆的元数据丰富提供了一个实用的解决方案，尤其突出了LLM在处理现有数据缺失问题上的潜力。其创新性在于将LLM应用于特定领域的元数据增强，为数字人文和信息科学研究提供了新的工具和数据集。"}}
{"id": "2506.20971", "title": "Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)", "authors": ["Shihui Feng", "Huilin Zhang", "Dragan Gašević"], "summary": "In this study, we analyze 2,398 research articles published between 2020 and\n2024 across eight core venues related to the field of Artificial Intelligence\nin Education (AIED). Using a three-step knowledge co-occurrence network\nanalysis, we analyze the knowledge structure of the field, the evolving\nknowledge clusters, and the emerging frontiers. Our findings reveal that AIED\nresearch remains strongly technically focused, with sustained themes such as\nintelligent tutoring systems, learning analytics, and natural language\nprocessing, alongside rising interest in large language models (LLMs) and\ngenerative artificial intelligence (GenAI). By tracking the bridging keywords\nover the past five years, we identify four emerging frontiers in AIED--LLMs,\nGenAI, multimodal learning analytics, and human-AI collaboration. The current\nresearch interests in GenAI are centered around GAI-driven personalization,\nself-regulated learning, feedback, assessment, motivation, and ethics.The key\nresearch interests and emerging frontiers in AIED reflect a growing emphasis on\nco-adaptive, human-centered AI for education. This study provides the first\nlarge-scale field-level mapping of AIED's transformation in the GenAI era and\nsheds light on the future research development and educational practices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20971v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20971v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "AIED 将走向何方？关键主题与新兴前沿（2020-2024）", "tldr": "本研究分析了2020-2024年间2398篇教育人工智能（AIED）领域的论文，揭示了其技术导向性、持续主题和LLMs、GenAI等新兴前沿，并强调了以人为中心的AI发展趋势。", "motivation": "该研究旨在通过分析大量研究文章，揭示教育人工智能（AIED）领域的知识结构、演变中的知识群集和新兴前沿，尤其是在生成式人工智能（GenAI）时代，并提供首次大规模的领域级映射，以指导未来的研究发展和教育实践。", "method": "本研究分析了2020年至2024年间在八个核心场所发表的2,398篇教育人工智能（AIED）研究文章。采用三步知识共现网络分析方法，分析了该领域的知识结构、演变中的知识群集和新兴前沿。", "result": "研究发现AIED研究仍然高度关注技术，持续主题包括智能辅导系统、学习分析和自然语言处理。同时，对大型语言模型（LLMs）和生成式人工智能（GenAI）的兴趣日益增长。通过追踪桥接关键词，确定了AIED的四个新兴前沿：LLMs、GenAI、多模态学习分析和人机协作。GenAI目前的研究兴趣集中在GAI驱动的个性化、自主学习、反馈、评估、动机和伦理方面。", "conclusion": "AIED的关键研究兴趣和新兴前沿反映了对共同适应的、以人为中心的教育人工智能日益增长的重视。本研究首次提供了AIED在GenAI时代转型的大规模领域级映射，为未来的研究发展和教育实践提供了启示。", "translation": "本研究分析了2020年至2024年间在八个与教育人工智能（AIED）领域相关的核心场所发表的2,398篇研究文章。我们使用三步知识共现网络分析方法，分析了该领域的知识结构、演变中的知识群集以及新兴前沿。我们的研究结果显示，AIED研究仍然高度关注技术，持续的主题包括智能辅导系统、学习分析和自然语言处理，同时对大型语言模型（LLMs）和生成式人工智能（GenAI）的兴趣日益增长。通过追踪过去五年中的桥接关键词，我们确定了AIED的四个新兴前沿——LLMs、GenAI、多模态学习分析和人机协作。目前GenAI的研究兴趣集中在GAI驱动的个性化、自主学习、反馈、评估、动机和伦理方面。AIED的关键研究兴趣和新兴前沿反映了对共同适应的、以人为中心的教育人工智能日益增长的重视。本研究首次对AIED在GenAI时代转型进行了大规模领域级映射，并为未来的研究发展和教育实践提供了启示。", "summary": "该研究通过对2020-2024年间2398篇教育人工智能（AIED）领域论文进行三步知识共现网络分析，揭示了AIED研究的技术导向性，识别出智能辅导系统、学习分析和自然语言处理等持续主题，以及大型语言模型（LLMs）、生成式人工智能（GenAI）、多模态学习分析和人机协作等新兴前沿。研究强调AIED正日益重视协同适应的、以人为中心的教育AI，并首次提供了AIED在GenAI时代转型的大规模领域级映射，为未来研究和实践指明方向。", "keywords": "AIED, LLMs, GenAI, 知识共现网络分析, 新兴前沿", "comments": "本研究通过对大量文献的系统性分析，提供了对教育人工智能（AIED）领域在GenAI时代转型前沿的及时和全面洞察。其创新之处在于首次提供了大规模的领域级映射，并明确指出了新兴前沿，这对指导未来AIED的研究方向和教育实践具有重要价值。数据量大且分析方法严谨，增强了研究的可靠性。"}}
{"id": "2506.20851", "title": "Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach", "authors": ["Srikar Reddy Gadusu", "Larry Callahan", "Samir Lababidi", "Arunasri Nishtala", "Sophia Healey", "Hande McGinty"], "summary": "As data and knowledge expand rapidly, adopting systematic methodologies for\nontology generation has become crucial. With the daily increases in data\nvolumes and frequent content changes, the demand for databases to store and\nretrieve information for the creation of knowledge graphs has become\nincreasingly urgent. The previously established Knowledge Acquisition and\nRepresentation Methodology (KNARM) outlines a systematic approach to address\nthese challenges and create knowledge graphs. However, following this\nmethodology highlights the existing challenge of seamlessly integrating Neo4j\ndatabases with the Web Ontology Language (OWL). Previous attempts to integrate\ndata from Neo4j into an ontology have been discussed, but these approaches\noften require an understanding of description logics (DL) syntax, which may not\nbe familiar to many users. Thus, a more accessible method is necessary to\nbridge this gap. This paper presents a user-friendly approach that utilizes\nPython and its rdflib library to support ontology development. We showcase our\nnovel approach through a Neo4j database we created by integrating data from the\nFood and Drug Administration (FDA) Adverse Event Reporting System (FAERS)\ndatabase. Using this dataset, we developed a Python script that automatically\ngenerates the required classes and their axioms, facilitating a smoother\nintegration process. This approach offers a practical solution to the\nchallenges of ontology generation in the context of rapidly growing adverse\ndrug event datasets, supporting improved drug safety monitoring and public\nhealth decision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20851v1", "categories": ["cs.SE", "cs.AI", "cs.DB"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20851v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "通过自动化集成数据生成可靠不良事件档案（GRAPH-AID）：一种半自动化本体构建方法", "tldr": "本研究提出了一种用户友好的半自动化方法，利用Python和rdflib库，将Neo4j数据库中的不良事件数据集成到OWL本体中，以克服现有方法对描述逻辑语法的要求，支持药物安全监测和公共卫生决策。", "motivation": "随着数据和知识的快速增长以及数据内容的频繁变化，本体生成和知识图谱构建面临挑战。具体而言，将Neo4j数据库与Web本体语言（OWL）无缝集成存在困难，且现有方法通常需要用户理解描述逻辑（DL）语法，这对于许多用户而言是一个障碍。", "method": "本研究提出了一种用户友好的方法，利用Python及其rdflib库来支持本体开发。通过一个集成了美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）数据的Neo4j数据库，开发了一个Python脚本，该脚本能自动生成所需的类及其公理，从而促进Neo4j数据与OWL本体的集成。", "result": "该方法成功地将FDA不良事件报告系统（FAERS）的数据从Neo4j数据库集成到OWL本体中，并自动生成了所需的类及其公理，展示了其在快速增长的不良药物事件数据集背景下本体生成的实用性。", "conclusion": "本研究提供了一种实用的解决方案，克服了在快速增长的不良药物事件数据集背景下本体生成所面临的挑战，支持了改进的药物安全监测和公共卫生决策。", "translation": "随着数据和知识的迅速扩展，采用系统化的本体生成方法变得至关重要。随着数据量的日常增加和内容的频繁变化，对存储和检索信息以创建知识图谱的数据库的需求变得日益紧迫。先前建立的知识获取与表示方法（KNARM）概述了一种系统化的方法来应对这些挑战并创建知识图谱。然而，遵循这种方法凸显了将Neo4j数据库与Web本体语言（OWL）无缝集成的现有挑战。之前已讨论过将Neo4j数据集成到本体中的尝试，但这些方法通常需要理解描述逻辑（DL）语法，这可能对许多用户不熟悉。因此，需要一种更易于访问的方法来弥补这一差距。本文提出了一种用户友好的方法，该方法利用Python及其rdflib库来支持本体开发。我们通过一个我们创建的Neo4j数据库展示了我们的新颖方法，该数据库通过集成来自美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）数据库的数据而构建。利用此数据集，我们开发了一个Python脚本，该脚本自动生成所需的类及其公理，从而促进了更顺畅的集成过程。这种方法为在快速增长的不良药物事件数据集背景下本体生成所面临的挑战提供了一个实用解决方案，支持改进的药物安全监测和公共卫生决策。", "summary": "本论文提出了一种名为GRAPH-AID的半自动化本体构建方法，旨在解决将Neo4j数据库中的不良事件数据无缝集成到OWL本体的挑战。针对现有方法需要用户理解复杂描述逻辑语法的问题，研究开发了一个利用Python和rdflib库的用户友好型脚本。该脚本能够自动从Neo4j数据库（包含FDA不良事件报告系统数据）生成本体所需的类和公理，从而简化了本体开发和数据集成过程。该方法为在不断增长的不良药物事件数据背景下，提高药物安全监测和公共卫生决策提供了实用支持。", "keywords": "本体构建, Neo4j, OWL, Python, 不良事件, 数据集成", "comments": "该研究的创新之处在于提出了一种用户友好的半自动化本体构建方法，克服了传统本体集成方法对描述逻辑语法的较高要求。通过利用Python和rdflib库，并针对实际的不良事件报告数据进行应用，该方法显著降低了本体开发的门槛，尤其是在处理大规模、动态变化的数据集时，具有重要的实用价值。它为药物安全监测和公共卫生领域的数据集成和知识表示提供了有效工具。"}}
{"id": "2506.20812", "title": "Model-Based Real-Time Pose and Sag Estimation of Overhead Power Lines Using LiDAR for Drone Inspection", "authors": ["Alexandre Girard", "Steven A. Parkison", "Philippe Hamelin"], "summary": "Drones can inspect overhead power lines while they remain energized,\nsignificantly simplifying the inspection process. However, localizing a drone\nrelative to all conductors using an onboard LiDAR sensor presents several\nchallenges: (1) conductors provide minimal surface for LiDAR beams limiting the\nnumber of conductor points in a scan, (2) not all conductors are consistently\ndetected, and (3) distinguishing LiDAR points corresponding to conductors from\nother objects, such as trees and pylons, is difficult. This paper proposes an\nestimation approach that minimizes the error between LiDAR measurements and a\nsingle geometric model representing the entire conductor array, rather than\ntracking individual conductors separately. Experimental results, using data\nfrom a power line drone inspection, demonstrate that this method achieves\naccurate tracking, with a solver converging under 50 ms per frame, even in the\npresence of partial observations, noise, and outliers. A sensitivity analysis\nshows that the estimation approach can tolerate up to twice as many outlier\npoints as valid conductors measurements.", "comment": "Submitted to IEEE case 2025", "pdf_url": "http://arxiv.org/pdf/2506.20812v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20812v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基于模型的使用激光雷达进行无人机输电线路姿态和弧垂实时估计", "tldr": "提出一种基于模型的激光雷达估计方法，用于无人机输电线路的实时姿态和弧垂估计，解决了传统方法中对单个导线跟踪的挑战，实现快速准确的定位。", "motivation": "使用无人机载激光雷达传感器定位输电线路面临多项挑战：导线反射面小导致点云数据有限；并非所有导线都能被稳定检测；以及难以区分导线与其他物体（如树木和电线杆）的激光雷达点。", "method": "提出一种估计方法，通过最小化激光雷达测量值与代表整个导线阵列的单一几何模型之间的误差，而不是单独跟踪每根导线。", "result": "该方法实现了准确的跟踪，求解器在每帧50毫秒内收敛，即使在存在部分观测、噪声和异常值的情况下也能表现良好。敏感性分析表明，该估计方法可以容忍多达两倍于有效导线测量值的异常点。", "conclusion": "该研究提出了一种新颖的基于模型的估计方法，有效解决了无人机激光雷达输电线路检测中的挑战，实现了实时、准确和鲁棒的姿态和弧垂估计。", "translation": "无人机可以在输电线路带电时进行检查，大大简化了检查过程。然而，使用机载激光雷达传感器相对于所有导线定位无人机存在几个挑战：（1）导线为激光雷达光束提供的表面很小，限制了扫描中导线点的数量；（2）并非所有导线都能被持续检测到；（3）难以区分对应于导线的激光雷达点与其他物体（如树木和电线杆）的激光雷达点。本文提出了一种估计方法，通过最小化激光雷达测量值与代表整个导线阵列的单一几何模型之间的误差，而不是单独跟踪每根导线。使用输电线路无人机检查数据进行的实验结果表明，该方法实现了准确的跟踪，求解器在每帧50毫秒内收敛，即使在存在部分观测、噪声和异常值的情况下也能表现良好。敏感性分析表明，该估计方法可以容忍多达两倍于有效导线测量值的异常点。", "summary": "本文提出一种新颖的基于模型的估计方法，旨在解决使用无人机载激光雷达传感器进行输电线路姿态和弧垂估计时的挑战。该方法通过将激光雷达测量值与表示整个导线阵列的单一几何模型进行匹配来最小化误差，从而避免了对单根导线的独立跟踪。实验结果表明，该方法能够实现准确且实时的跟踪，即使在存在噪声、异常值和不完整数据的情况下，求解器也能快速收敛，并且对异常点具有很高的容忍度。", "keywords": "激光雷达, 无人机检测, 输电线路, 姿态估计, 弧垂估计", "comments": "这篇论文通过引入一个单一的几何模型来表示整个导线阵列，而不是单独跟踪每根导线，提供了一种创新的方法来解决无人机激光雷达输电线路检测中的关键挑战。这种模型驱动的方法提高了在稀疏和噪声数据下的鲁棒性和效率，对于实时应用至关重要。其在处理异常值方面的能力也显著增强了实际部署的可靠性。"}}
{"id": "2506.20952", "title": "Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment", "authors": ["Kyosuke Ishibashi", "Atsushi Saito", "Zin Y. Tun", "Lucas Ray", "Megan C. Coram", "Akihiro Sakurai", "Allison M. Okamura", "Ko Yamamoto"], "summary": "Human crowd simulation in virtual reality (VR) is a powerful tool with\npotential applications including emergency evacuation training and assessment\nof building layout. While haptic feedback in VR enhances immersive experience,\nits effect on walking behavior in dense and dynamic pedestrian flows is\nunknown. Through a user study, we investigated how haptic feedback changes user\nwalking motion in crowded pedestrian flows in VR. The results indicate that\nhaptic feedback changed users' collision avoidance movements, as measured by\nincreased walking trajectory length and change in pelvis angle. The\ndisplacements of users' lateral position and pelvis angle were also increased\nin the instantaneous response to a collision with a non-player character (NPC),\neven when the NPC was inside the field of view. Haptic feedback also enhanced\nusers' awareness and visual exploration when an NPC approached from the side\nand back. Furthermore, variation in walking speed was increased by the haptic\nfeedback. These results suggested that the haptic feedback enhanced users'\nsensitivity to a collision in VR environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20952v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20952v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "触觉反馈对动态虚拟现实行人环境中避让行为和视觉探索的影响", "tldr": "本研究通过用户实验发现，在虚拟现实拥挤行人流中，触觉反馈会改变用户的避让行为和视觉探索，提高其对碰撞的敏感性。", "motivation": "虚拟现实中的人群模拟在应急疏散训练和建筑布局评估等领域具有潜在应用。尽管触觉反馈能增强VR沉浸感，但其对密集动态行人流中行走行为的影响尚不明确。", "method": "通过一项用户研究，调查了触觉反馈如何改变用户在虚拟现实拥挤行人流中的行走运动。", "result": "结果表明，触觉反馈改变了用户的碰撞避让动作，表现为行走轨迹长度增加和骨盆角度变化；即使NPC在视野内，用户对与NPC碰撞的瞬时反应中，其侧向位置和骨盆角度的位移也增加了；触觉反馈还增强了当NPC从侧面和后面接近时用户的意识和视觉探索；此外，行走速度的变化也因触觉反馈而增加。", "conclusion": "这些结果表明，触觉反馈增强了用户在虚拟现实环境中对碰撞的敏感性。", "translation": "虚拟现实（VR）中的人群模拟是一个强大的工具，在应急疏散训练和建筑布局评估等方面具有潜在应用。虽然VR中的触觉反馈能增强沉浸式体验，但其对密集动态行人流中行走行为的影响尚不明确。通过一项用户研究，我们调查了触觉反馈如何改变用户在VR拥挤行人流中的行走运动。结果表明，触觉反馈改变了用户的碰撞避让动作，表现为行走轨迹长度增加和骨盆角度变化。即使NPC在视野内，用户对与NPC碰撞的瞬时反应中，其侧向位置和骨盆角度的位移也增加了。触觉反馈还增强了当NPC从侧面和后面接近时用户的意识和视觉探索。此外，行走速度的变化也因触觉反馈而增加。这些结果表明，触觉反馈增强了用户在VR环境中对碰撞的敏感性。", "summary": "本研究旨在探究触觉反馈对虚拟现实（VR）中拥挤行人流中用户行走行为的影响。通过用户实验，研究发现触觉反馈显著改变了用户的碰撞避让动作，包括增加行走轨迹长度、改变骨盆角度，并增强了对非玩家角色（NPC）碰撞的瞬时反应（包括侧向位置和骨盆角度的位移），即使NPC在视野内。此外，触觉反馈还提升了用户在NPC从侧后方接近时的意识和视觉探索，并增加了行走速度的变化。研究结果表明，触觉反馈能有效提高用户在VR环境中对碰撞的敏感性。", "keywords": "触觉反馈, 虚拟现实, 避让行为, 行人模拟, 碰撞敏感性", "comments": "本研究创新性地探讨了触觉反馈在VR行人模拟中的具体效果，填补了该领域的部分空白。其发现对于提升VR沉浸感和模拟真实行为具有重要意义，尤其是在应急训练和建筑设计评估等应用中。研究通过量化指标（如轨迹长度、骨盆角度）验证了触觉反馈对行为的影响，具有一定的严谨性。"}}
{"id": "2506.20965", "title": "Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment", "authors": ["Craig Steven Wright"], "summary": "This paper integrates Austrian capital theory with repeated game theory to\nexamine strategic miner behaviour under different institutional conditions in\nblockchain systems. It shows that when protocol rules are mutable, effective\ntime preference rises, undermining rational long-term planning and cooperative\nequilibria. Using formal game-theoretic analysis and Austrian economic\nprinciples, the paper demonstrates how mutable protocols shift miner incentives\nfrom productive investment to political rent-seeking and influence games. The\noriginal Bitcoin protocol is interpreted as an institutional anchor: a fixed\nrule-set enabling calculability and low time preference. Drawing on the work of\nBohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability\nis essential for restoring strategic coherence, entrepreneurial confidence, and\nsustainable network equilibrium.", "comment": "Approximately 10,770 words, 0 figure, 0 table. Submitted to The\n  Quarterly Journal of Austrian Economics", "pdf_url": "http://arxiv.org/pdf/2506.20965v1", "categories": ["econ.GN", "cs.CR", "cs.GT", "cs.NI", "q-fin.EC", "q-fin.GN", "91B42, 91A25, 91B50", "K.4.4; J.4; C.2.4"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.20965v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "矿工理性行为、协议稳定性与时间偏好：比特币激励环境的奥地利学派与博弈论分析", "tldr": "该论文结合奥地利资本理论和重复博弈论，分析了区块链系统中矿工的策略行为，指出可变协议会提高时间偏好，导致矿工转向寻租，而协议的不可变性对于维护网络稳定至关重要。", "motivation": "本文旨在结合奥地利资本理论与重复博弈论，探讨区块链系统中矿工的策略行为，并分析协议规则可变性对长期规划、合作均衡以及矿工激励的影响。", "method": "本文整合了奥地利资本理论与重复博弈论，并运用形式化的博弈论分析和奥地利经济学原理来论证其观点。", "result": "研究表明，当协议规则可变时，有效时间偏好会上升，从而破坏理性的长期规划和合作均衡。此外，可变协议会将矿工的激励从生产性投资转向政治寻租和影响力博弈。原始比特币协议被视为一个固定规则集，能够实现可计算性和低时间偏好。", "conclusion": "协议的不可变性对于恢复战略一致性、创业信心和可持续网络均衡至关重要。", "translation": "本文将奥地利资本理论与重复博弈论相结合，考察了区块链系统在不同制度条件下的矿工策略行为。研究表明，当协议规则可变时，有效时间偏好上升，从而破坏了理性的长期规划和合作均衡。通过形式化的博弈论分析和奥地利经济学原理，本文论证了可变协议如何将矿工的激励从生产性投资转向政治寻租和影响力博弈。原始比特币协议被解释为一个制度锚点：一个固定的规则集，能够实现可计算性和低时间偏好。借鉴庞巴维克、米塞斯和哈耶克的研究成果，本文提出协议的不可变性对于恢复战略一致性、创业信心和可持续网络均衡至关重要。", "summary": "该论文结合奥地利资本理论和重复博弈论，分析了区块链系统中矿工的策略行为。研究发现，可变的协议规则会提高矿工的有效时间偏好，导致长期规划受损和合作均衡被破坏，并将矿工激励从生产性投资转向寻租。论文强调，比特币原始协议的固定规则集是稳定性的关键。最终得出结论，协议的不可变性对于维护区块链网络的战略一致性、创业信心和可持续均衡至关重要。", "keywords": "矿工行为, 协议稳定性, 时间偏好, 奥地利学派, 博弈论", "comments": "该论文通过将奥地利经济学（时间偏好、资本理论）与博弈论相结合，为分析区块链矿工行为提供了一种新颖的跨学科方法。其强调协议不可变性作为关键制度锚点的观点，为去中心化系统的基本设计原则提供了强有力的理论论证，将经济理论与实际的区块链治理联系起来，具有重要意义。"}}
{"id": "2506.21126", "title": "Semantic-aware Digital Twin for AI-based CSI Acquisition", "authors": ["Jiajia Guo", "Yiming Cui", "Shi Jin"], "summary": "Artificial intelligence (AI) substantially enhances channel state information\n(CSI) acquisition performance but is limited by its reliance on single-modality\ninformation and deployment challenges, particularly in dataset collection. This\npaper investigates the use of semantic-aware digital twin (DT) to enhance\nAI-based CSI acquisition. We first briefly introduce the motivation and recent\nadvancements in AI-driven CSI acquisition and semantic-aware DT employment for\nair interfaces. Then, we thoroughly explore how semantic-aware DT can bolster\nAI-based CSI acquisition. We categorizes the semantic-aware DT for AI-based CSI\nacquisition into two classes: enhancing AI-based CSI acquisition through\nintegration with DT and using DT to aid AI-based CSI deployment. Potential\nintegration frameworks are introduced in detail. Finally, we conclude by\noutlining potential research directions within the semantic-aware DT-assisted\nAI-based CSI acquisition.", "comment": "This article has been accepted by IEEE Communications Standards\n  Magazine", "pdf_url": "http://arxiv.org/pdf/2506.21126v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21126v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于基于AI的CSI获取的语义感知数字孪生", "tldr": "本文探讨了语义感知数字孪生如何通过解决其局限性并辅助部署来增强基于AI的信道状态信息(CSI)获取。", "motivation": "当前基于人工智能(AI)的信道状态信息(CSI)获取性能虽有显著提升，但受限于对单一模态信息的依赖以及部署挑战，特别是在数据集收集方面。", "method": "本文研究了语义感知数字孪生(DT)在增强基于AI的CSI获取中的应用。首先介绍了AI驱动CSI获取和语义感知DT在空口应用中的动机和进展，然后深入探讨了语义感知DT如何增强基于AI的CSI获取。研究将语义感知DT用于AI-based CSI获取分为两类：通过与DT集成增强AI-based CSI获取，以及使用DT辅助AI-based CSI部署。文中详细介绍了潜在的集成框架。", "result": "摘要中未提及具体结果。", "conclusion": "本文通过概述语义感知数字孪生辅助的基于AI的CSI获取的潜在研究方向作为总结。", "translation": "人工智能(AI)显著提升了信道状态信息(CSI)获取性能，但受限于对单一模态信息的依赖以及部署挑战，特别是在数据集收集方面。本文研究了语义感知数字孪生(DT)在增强基于AI的CSI获取中的应用。我们首先简要介绍了AI驱动CSI获取和语义感知DT在空口应用中的动机和最新进展。然后，我们深入探讨了语义感知DT如何增强基于AI的CSI获取。我们将用于基于AI的CSI获取的语义感知DT分为两类：通过与DT集成增强AI-based CSI获取，以及使用DT辅助AI-based CSI部署。文中详细介绍了潜在的集成框架。最后，我们通过概述语义感知DT辅助的基于AI的CSI获取的潜在研究方向作为总结。", "summary": "本文探讨了语义感知数字孪生(DT)如何增强基于AI的信道状态信息(CSI)获取，旨在克服AI在CSI获取中面临的单一模态信息依赖和部署挑战。研究将语义感知DT的应用分为两类：通过集成DT来增强AI-based CSI获取，以及利用DT辅助AI-based CSI的部署，并详细介绍了潜在的集成框架。文章最后指出了该领域未来的研究方向。", "keywords": "语义感知数字孪生, AI, CSI获取, 无线通信, 部署", "comments": "本文提出了一种将语义感知数字孪生与基于AI的CSI获取相结合的探索性框架，旨在解决AI在无线通信中实际应用所面临的数据集收集和部署等挑战。其创新性在于探索了数字孪生与人工智能在无线通信领域协同作用的新途径，为提升CSI获取性能提供了新的视角。"}}
{"id": "2506.20675", "title": "Utility-Driven Speculative Decoding for Mixture-of-Experts", "authors": ["Anish Saxena", "Po-An Tsai", "Hritvik Taneja", "Aamer Jaleel", "Moinuddin Qureshi"], "summary": "GPU memory bandwidth is the main bottleneck for low-latency Large Language\nModel (LLM) inference. Speculative decoding leverages idle GPU compute by using\na lightweight drafter to propose K tokens, which the LLM verifies in parallel,\nboosting token throughput. In conventional dense LLMs, all model weights are\nfetched each iteration, so speculation adds no latency overhead. Emerging\nMixture of Experts (MoE) models activate only a subset of weights per token,\ngreatly reducing data movement. However, we show that speculation is\nineffective for MoEs: draft tokens collectively activate more weights,\nincreasing data movement and verification time by 2-3x. When token throughput\ngains fail to offset this overhead, speculation causes slowdowns up to 1.5x,\nmaking it infeasible. Even when useful, the optimal K varies by task, model,\nand even between requests and iterations. Thus, despite widespread use in dense\nLLMs, speculation remains impractical in leading MoEs.\n  We present Cascade, a utility-driven framework that selectively enables\nspeculation to avoid slowdowns and dynamically tunes K to accelerate MoE\nserving. Cascade uses a lightweight metric, speculation utility, the ratio of\ntoken gains to verification cost, which shows iteration-level locality,\nenabling periodic decisions via short test and longer set phases. For each\nrequest, Cascade disables speculation if utility drops below one during\ntesting, and when utility exceeds one, tests multiple K-values to choose the\nutility-maximizing K for the set phase. We implement Cascade in vLLM and\nevaluate it on five popular MoEs with workloads spanning code, math,\nextraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and\nimproves throughput by 7-14% over static K, making speculative decoding\npractical for MoEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20675v1", "categories": ["cs.DC", "cs.AI", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20675v1", "date": "2025-06-17", "updated": "2025-06-17", "AI": {"title_translation": "面向混合专家模型的效用驱动推测解码", "tldr": "推测解码对混合专家模型（MoE）无效，因为它会增加数据移动和验证时间。本文提出了Cascade框架，通过动态调整推测解码参数，使其适用于MoE模型，从而限制性能下降并提高吞吐量。", "motivation": "GPU内存带宽是大型语言模型（LLM）推理的主要瓶颈。推测解码通过利用空闲GPU计算来提高密集型LLM的吞吐量。然而，本文发现推测解码对MoE模型无效，因为它会导致数据移动和验证时间增加2-3倍，可能导致高达1.5倍的减速。此外，最佳的推测K值因任务、模型甚至请求和迭代而异。因此，尽管在密集型LLM中广泛使用，推测解码在领先的MoE模型中仍然不实用。本文旨在解决这一问题，使推测解码在MoE模型中变得实用。", "method": "本文提出了Cascade，一个效用驱动的框架，用于选择性地启用推测解码以避免减速，并动态调整K值以加速MoE服务。Cascade使用一个轻量级指标——推测效用，即令牌增益与验证成本之比，该指标显示出迭代级别的局部性，通过短测试阶段和长设置阶段实现周期性决策。对于每个请求，如果测试期间效用降至1以下，Cascade会禁用推测解码；当效用超过1时，它会测试多个K值，以选择在设置阶段实现效用最大化的K值。该框架在vLLM中实现，并在五个流行的MoE模型上进行了评估，涵盖了代码、数学、提取和混合任务的工作负载。", "result": "Cascade将减速限制在5%（相比之下，未经优化的推测解码可能导致1.5倍的减速），并且比静态K值提高了7-14%的吞吐量。", "conclusion": "Cascade框架使得推测解码在混合专家模型（MoE）中变得实用。", "translation": "GPU内存带宽是低延迟大型语言模型（LLM）推理的主要瓶颈。推测解码通过使用轻量级草稿器提出K个令牌来利用空闲GPU计算，LLM并行验证这些令牌，从而提高令牌吞吐量。在传统的密集型LLM中，每次迭代都会获取所有模型权重，因此推测不会增加延迟开销。新兴的混合专家（MoE）模型每个令牌仅激活部分权重，大大减少了数据移动。然而，我们表明推测解码对MoE模型无效：草稿令牌共同激活更多权重，使数据移动和验证时间增加2-3倍。当令牌吞吐量增益无法抵消此开销时，推测会导致高达1.5倍的减速，使其不可行。即使有用，最佳的K值也因任务、模型，甚至请求和迭代而异。因此，尽管在密集型LLM中广泛使用，推测解码在领先的MoE模型中仍然不实用。\n我们提出了Cascade，一个效用驱动的框架，它选择性地启用推测解码以避免减速，并动态调整K值以加速MoE服务。Cascade使用一个轻量级指标——推测效用，即令牌增益与验证成本之比，该指标显示出迭代级别的局部性，通过短测试阶段和长设置阶段实现周期性决策。对于每个请求，如果测试期间效用降至1以下，Cascade会禁用推测解码；当效用超过1时，它会测试多个K值，以选择在设置阶段实现效用最大化的K值。我们在vLLM中实现了Cascade，并在五个流行的MoE模型上进行了评估，涵盖了代码、数学、提取和混合任务的工作负载。Cascade将减速限制在5%（相比1.5倍），并比静态K值提高了7-14%的吞吐量，使推测解码对MoE模型变得实用。", "summary": "推测解码在密集型大型语言模型（LLM）中有效，但本文指出其在混合专家（MoE）模型中效率低下，因为它会导致数据移动和验证时间增加，从而降低性能。为解决此问题，论文提出了Cascade框架。Cascade是一个效用驱动的系统，它通过引入“推测效用”这一轻量级指标，动态地决定何时启用推测解码以及如何优化草稿令牌数量（K值）。通过周期性的测试和设置阶段，Cascade能够根据效用值自适应地调整策略，从而避免减速并提高吞吐量。实验结果表明，Cascade成功将MoE模型中的推测解码所导致的减速限制在5%以内，并相对于静态K值方案实现了7-14%的吞吐量提升，使得推测解码在MoE模型中变得可行和实用。", "keywords": "推测解码, 混合专家模型, LLM推理, GPU内存, 吞吐量优化", "comments": "这项工作的创新之处在于识别了推测解码在MoE模型中的局限性，并提出了一个基于效用的动态自适应框架来解决这一问题。其重要性在于，它将一种高效的推理优化技术（推测解码）扩展到了新兴且日益重要的MoE模型架构上，这对于提升MoE模型的实际应用性能至关重要。通过动态调整K值和启用/禁用推测解码的策略，Cascade能够适应不同的任务和工作负载，这是一个非常巧妙且实用的设计。"}}
{"id": "2506.21362", "title": "Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation", "authors": ["Chang Liu", "Yixin Wang", "Moontae Lee"], "summary": "Efficient access to high-quality information is vital for online platforms.\nTo promote more useful information, users not only create new content but also\nevaluate existing content, often through helpfulness voting. Although\naggregated votes help service providers rank their user content, these votes\nare often biased by disparate accessibility per position and the cascaded\ninfluence of prior votes. For a fairer assessment of information quality, we\npropose the Counterfactual Voting Adjustment (CVA), a causal framework that\naccounts for the context in which individual votes are cast. Through\npreliminary and semi-synthetic experiments, we show that CVA effectively models\nthe position and herding biases, accurately recovering the predefined content\nquality. In a real experiment, we demonstrate that reranking content based on\nthe learned quality by CVA exhibits stronger alignment with both user sentiment\nand quality evaluation assessed by GPT-4o, outperforming system rankings based\non aggregated votes and model-based rerankings without causal inference. Beyond\nthe individual quality inference, our embeddings offer comparative insights\ninto the behavioral dynamics of expert user groups across 120 major\nStackExchange communities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21362v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21362v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "反事实投票调整：用于在线平台中基于有用性评估的质量评估和更公平投票", "tldr": "本文提出反事实投票调整（CVA），一个因果框架，用于纠正在线有用性投票中的位置和羊群效应偏差，以实现更公平的内容质量评估和排名，并显示出与用户情感和GPT-4o评估的更好一致性。", "motivation": "在线平台中，用户对内容的有用性投票常常受到位置差异和先前投票级联影响的偏差，导致信息质量评估不公平。研究动机是为了促进更有用的信息，并实现对内容质量的更公平评估。", "method": "提出了反事实投票调整（CVA），这是一个考虑个体投票所处上下文的因果框架。", "result": "CVA有效地建模了位置偏差和羊群效应偏差。在初步和半合成实验中，CVA准确恢复了预定义的内容质量。在真实实验中，基于CVA学习到的质量对内容进行重新排名，与用户情感和GPT-4o评估的质量表现出更强的一致性，优于基于汇总投票的系统排名和不含因果推断的模型重新排名。CVA的嵌入还提供了对120个主要StackExchange社区中专家用户群体行为动态的比较性见解。", "conclusion": "CVA通过有效缓解投票偏差，为在线平台中更公平的质量评估和内容排名提供了一种稳健的方法，从而提高了与实际质量和用户情感的一致性，并提供了对用户行为的洞察。", "translation": "在线平台高效获取高质量信息至关重要。为了推广更有用的信息，用户不仅创建新内容，还会评估现有内容，通常通过有用性投票。尽管汇总投票有助于服务提供商对用户内容进行排名，但这些投票常常因不同位置的可访问性差异和先前投票的级联影响而产生偏差。为了更公平地评估信息质量，我们提出了反事实投票调整（CVA），这是一个考虑个体投票所处上下文的因果框架。通过初步和半合成实验，我们表明CVA有效地建模了位置偏差和羊群效应偏差，准确地恢复了预定义的内容质量。在真实实验中，我们证明基于CVA学习到的质量对内容进行重新排名，与用户情感和GPT-4o评估的质量表现出更强的一致性，优于基于汇总投票的系统排名和不含因果推断的模型重新排名。除了个体质量推断，我们的嵌入还提供了对120个主要StackExchange社区中专家用户群体行为动态的比较性见解。", "summary": "本文提出了反事实投票调整（CVA），一个旨在解决在线平台中有用性投票中固有偏差（位置偏差和羊群效应偏差）的因果框架。CVA旨在提供更公平的信息质量评估。通过实验，CVA展示了其准确建模偏差和恢复内容质量的能力。当应用于真实世界的内容重新排名时，CVA与用户情感和基于AI的质量评估相比传统方法表现出卓越的一致性。此外，其嵌入还提供了对用户行为动态的洞察。", "keywords": "反事实投票调整, 因果推断, 偏差校正, 在线平台, 内容质量", "comments": "本文的创新之处在于应用因果框架（CVA）来缓解在线投票系统中常见的偏差（位置和羊群效应），这对于公平的内容质量评估至关重要。其重要性体现在它证明了内容排名与用户情感和高级AI评估（GPT-4o）的更好一致性，表明其在增强在线平台效用方面的实际适用性。利用因果推断将真实质量从语境偏差中分离出来是一项重要贡献。"}}
{"id": "2506.20699", "title": "On Context-Content Uncertainty Principle", "authors": ["Xin Li"], "summary": "The Context-Content Uncertainty Principle (CCUP) proposes that inference\nunder uncertainty is governed by an entropy asymmetry between context and\ncontent: high-entropy contexts must be interpreted through alignment with\nlow-entropy, structured content. In this paper, we develop a layered\ncomputational framework that derives operational principles from this\nfoundational asymmetry. At the base level, CCUP formalizes inference as\ndirectional entropy minimization, establishing a variational gradient that\nfavors content-first structuring. Building upon this, we identify four\nhierarchical layers of operational principles: (\\textbf{L1}) \\emph{Core\nInference Constraints}, including structure-before-specificity, asymmetric\ninference flow, cycle-consistent bootstrapping, and conditional compression,\nall shown to be mutually reducible; (\\textbf{L2}) \\emph{Resource Allocation\nPrinciples}, such as precision-weighted attention, asymmetric learning rates,\nand attractor-based memory encoding; (\\textbf{L3}) \\emph{Temporal Bootstrapping\nDynamics}, which organize learning over time via structure-guided curricula;\nand (\\textbf{L4}) \\emph{Spatial Hierarchical Composition}, which integrates\nthese mechanisms into self-organizing cycles of memory, inference, and\nplanning. We present formal equivalence theorems, a dependency lattice among\nprinciples, and computational simulations demonstrating the efficiency gains of\nCCUP-aligned inference. This work provides a unified theoretical foundation for\nunderstanding how brains and machines minimize uncertainty through recursive\nstructure-specificity alignment. The brain is not just an inference machine. It\nis a cycle-consistent entropy gradient resolver, aligning structure and\nspecificity via path-dependent, content-seeded simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20699v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20699v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "论语境-内容不确定性原理", "tldr": "语境-内容不确定性原理（CCUP）提出了一种分层的计算框架，通过上下文和内容之间的熵不对称来解释不确定性下的推理，并展示了其在理解大脑和机器如何最小化不确定性方面的效率提升。", "motivation": "该研究的动机是提供一个统一的理论基础，以理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性。", "method": "本文开发了一个分层的计算框架，该框架从语境和内容之间的熵不对称性推导出操作原理。它将推理形式化为定向熵最小化，并识别了四个层次的操作原则：核心推理约束、资源分配原则、时间自举动力学和空间分层组合。研究还提出了形式等价定理、原则间的依赖格和计算模拟。", "result": "研究结果表明，通过计算模拟证明了与CCUP对齐的推理可以获得效率提升。", "conclusion": "CCUP为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。大脑不仅仅是一个推理机器，它还是一个循环一致的熵梯度解析器，通过路径依赖、内容播种的模拟来对齐结构和特异性。", "translation": "语境-内容不确定性原理（CCUP）提出，不确定性下的推理受语境和内容之间熵不对称性的支配：高熵语境必须通过与低熵、结构化内容的对齐来解释。在本文中，我们开发了一个分层的计算框架，从这种基本不对称性中推导出操作原理。在基础层面，CCUP将推理形式化为定向熵最小化，建立了一个有利于内容优先结构的变分梯度。在此基础上，我们确定了四个层级的操作原则：（L1）核心推理约束，包括结构优先于特异性、不对称推理流、循环一致自举和条件压缩，所有这些都被证明可以相互归约；（L2）资源分配原则，如精度加权注意力、不对称学习率和基于吸引子的记忆编码；（L3）时间自举动力学，通过结构引导的课程随时间组织学习；以及（L4）空间分层组合，将这些机制整合到记忆、推理和规划的自组织循环中。我们提出了形式等价定理、原则间的依赖格以及计算模拟，展示了与CCUP对齐的推理所带来的效率提升。这项工作为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。大脑不仅仅是一个推理机器。它是一个循环一致的熵梯度解析器，通过路径依赖、内容播种的模拟来对齐结构和特异性。", "summary": "本文提出了语境-内容不确定性原理（CCUP），一个基于语境与内容之间熵不对称性的计算框架，用于理解不确定性下的推理。CCUP将推理定义为定向熵最小化，并提出了四个层次的操作原则，涵盖了从核心推理机制到资源分配、时间学习和空间组合的各个方面。通过形式定理和计算模拟，该研究展示了CCUP对齐推理的效率优势，并为大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。", "keywords": "语境-内容不确定性原理, 熵不对称性, 推理, 计算框架, 不确定性最小化", "comments": "本文提出了一个新颖的语境-内容不确定性原理，并构建了一个分层的计算框架来解释不确定性下的推理。其创新之处在于将熵不对称性作为核心，并将其应用于大脑和机器的统一理论中，这对于理解智能系统的信息处理机制具有重要意义。提出的四层操作原则为未来的研究提供了详细的指导。"}}
{"id": "2506.20819", "title": "DPLib: A Standard Benchmark Library for Distributed Power System Analysis and Optimization", "authors": ["Milad Hasanzadeh", "Amin Kargarian"], "summary": "\\textit{DPLib} is an open-source MATLAB-based benchmark library created to\nsupport research and development in distributed and decentralized power system\nanalysis and optimization. Distributed and decentralized methods offer\nscalability, privacy preservation, and resilience to single points of failure,\nmaking them increasingly important for modern power systems. However, unlike\ncentralized tools such as MATPOWER, no general-purpose, reproducible data\nlibrary package currently exists for distributed power system studies. DPLib\nfills this gap by providing a standard power system library featuring over 20\nmulti-region benchmark test cases of varying sizes, along with a graph-based\npartitioning toolkit that decomposes any MATPOWER test system into multiple\nelectrically coherent regions. The partitioning toolkit, an easy-to-use MATLAB\ncode, generates standardized \\texttt{.mat} and \\texttt{.m} files, along with\nregion visualizations for intuitive understanding. We also provide modular,\neasy-to-use distributed optimal power flow (OPF) solvers: an alternating\ndirection method of multipliers(ADMM)-based DC-OPF solver implemented in\nYALMIP, and an ADMM-based AC-OPF solver leveraging IPOPT. These solvers\nvalidate the generated test systems for distributed optimization applications.\nNumerical results validate the generated test cases, establishing DPLib as a\nfoundation for reproducible distributed power system research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20819v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20819v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "DPLib：一个用于分布式电力系统分析和优化的标准基准库", "tldr": "DPLib是一个开源的MATLAB基准库，旨在支持分布式电力系统分析和优化研究，填补了该领域缺乏通用、可复现数据库的空白。", "motivation": "目前，与MATPOWER等集中式工具不同，分布式电力系统研究缺乏通用的、可复现的数据库软件包。分布式和去中心化方法对于现代电力系统日益重要，因为它们提供了可扩展性、隐私保护和对单点故障的弹性。", "method": "DPLib提供了一个标准电力系统库，包含20多个不同大小的多区域基准测试用例，以及一个基于图的划分工具包，可将任何MATPOWER测试系统分解为多个电气连贯的区域。此外，DPLib还提供了模块化、易于使用的分布式最优潮流（OPF）求解器：一个基于ADMM的DC-OPF求解器和一个基于ADMM的AC-OPF求解器。", "result": "数值结果验证了所生成的测试用例。", "conclusion": "DPLib的创建为可复现的分布式电力系统研究奠定了基础。", "translation": "DPLib是一个开源的、基于MATLAB的基准库，旨在支持分布式和去中心化电力系统分析与优化的研究与开发。分布式和去中心化方法提供了可扩展性、隐私保护以及对单点故障的弹性，使其对现代电力系统日益重要。然而，与MATPOWER等集中式工具不同，目前分布式电力系统研究尚缺乏通用、可复现的数据库软件包。DPLib通过提供一个标准电力系统库来填补这一空白，该库包含20多个不同大小的多区域基准测试用例，以及一个基于图的划分工具包，该工具包能够将任何MATPOWER测试系统分解为多个电气连贯的区域。该划分工具包是一个易于使用的MATLAB代码，可生成标准化的.mat和.m文件，并提供区域可视化以便直观理解。我们还提供了模块化、易于使用的分布式最优潮流（OPF）求解器：一个在YALMIP中实现的基于交替方向乘子法（ADMM）的DC-OPF求解器，以及一个利用IPOPT的基于ADMM的AC-OPF求解器。这些求解器验证了所生成的测试系统适用于分布式优化应用。数值结果验证了所生成的测试用例，从而确立了DPLib作为可复现分布式电力系统研究的基础。", "summary": "DPLib是一个开源的MATLAB基准库，旨在解决分布式电力系统分析和优化领域缺乏标准化、可复现数据工具的痛点。它提供了20多个多区域基准测试用例，一个用于系统分解的图划分工具包，以及基于ADMM的分布式最优潮流求解器。通过数值验证，DPLib为可复现的分布式电力系统研究奠定了基础。", "keywords": "分布式电力系统, 基准库, 优化, MATLAB, ADMM", "comments": "DPLib的创新之处在于填补了分布式电力系统研究中缺乏通用、可复现数据基准库的空白。其重要性在于为分布式电力系统分析和优化提供了一个标准化的研究平台，促进了该领域的可复现性和进一步发展。通过提供多区域测试用例和分解工具，它降低了研究门槛，并支持了更复杂的分布式算法开发和验证。"}}
{"id": "2506.20757", "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20757v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20757v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "ConViTac：使用对比表示对齐视觉-触觉融合", "tldr": "ConViTac通过对比学习和跨模态注意力，提高了视觉-触觉融合的特征对齐，在材料分类和抓取预测任务中表现优于现有技术。", "motivation": "机器人视觉和触觉是互补的感知模态，但现有方法在融合视觉-触觉表示时，直接组合（如特征加法和拼接）导致特征集成不佳。", "method": "本文提出了ConViTac，一个视觉-触觉表示学习网络。其核心是对比嵌入条件（CEC）机制，该机制利用通过自监督对比学习预训练的对比编码器，将视觉和触觉输入投射到统一的潜在嵌入中。这些嵌入通过跨模态注意力耦合视觉-触觉特征融合，以对齐统一表示并增强下游任务性能。", "result": "ConViTac在实际应用中优于当前最先进的方法。所提出的CEC机制有效，在材料分类和抓取预测任务中将准确率提高了高达12.0%。", "conclusion": "ConViTac及其提出的对比嵌入条件（CEC）机制能够有效对齐视觉-触觉特征并提高下游任务的性能，超越了现有技术。", "translation": "视觉和触觉是机器人最基本的两种感觉模态，它们提供互补信息，增强感知和操作任务。以往的研究试图共同学习视觉-触觉表示以提取更有意义的信息。然而，这些方法通常依赖直接组合，例如特征加法和拼接，进行模态融合，这往往导致特征集成不佳。在本文中，我们提出了ConViTac，一个视觉-触觉表示学习网络，旨在利用对比表示增强融合过程中的特征对齐。我们的主要贡献是对比嵌入条件（CEC）机制，该机制利用通过自监督对比学习预训练的对比编码器，将视觉和触觉输入投射到统一的潜在嵌入中。这些嵌入通过跨模态注意力耦合视觉-触觉特征融合，旨在对齐统一表示并提高下游任务的性能。我们进行了广泛的实验，证明了ConViTac在实际应用中优于当前最先进的方法，并验证了我们提出的CEC机制的有效性，该机制在材料分类和抓取预测任务中将准确率提高了高达12.0%。", "summary": "本文提出了ConViTac，一个用于视觉-触觉表示学习的网络，旨在通过引入对比嵌入条件（CEC）机制来解决现有融合方法中特征集成不佳的问题。CEC利用自监督对比学习预训练的编码器将视觉和触觉输入映射到统一的潜在空间，并通过跨模态注意力实现特征对齐。实验证明ConViTac在材料分类和抓取预测等下游任务上优于现有技术，并显著提升了性能。", "keywords": "视觉-触觉融合, 对比学习, 表示学习, 跨模态注意力, 机器人感知", "comments": "该论文的创新点在于引入了对比学习来增强视觉和触觉模态之间的特征对齐，克服了传统直接融合方法的局限性。CEC机制通过统一的潜在嵌入空间和跨模态注意力，有效地提升了多模态融合的质量和下游任务的性能。这对于提升机器人感知和操作能力具有重要意义。"}}
{"id": "2506.20815", "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications", "authors": ["Xinye Tang", "Haijun Zhai", "Chaitanya Belwal", "Vineeth Thayanithi", "Philip Baumann", "Yogesh K Roy"], "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20815v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20815v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "领域特定AI应用的动态上下文感知提示推荐", "tldr": "本文提出一个动态上下文感知提示推荐系统，通过结合查询分析、知识增强、技能组织与排名，为领域特定AI应用生成高质量提示，并在真实世界数据上表现出高实用性和相关性。", "motivation": "大型语言模型（LLM）驱动的应用对用户提示的质量高度敏感，但尤其对于领域特定应用而言，创建高质量提示常常具有挑战性。", "method": "本文提出了一个新颖的动态上下文感知提示推荐系统。该解决方案结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名来生成相关且可操作的提示建议。系统利用行为遥测和两阶段分层推理过程来动态选择和排序相关技能，并使用预定义和自适应模板（通过少样本学习增强）合成提示。", "result": "在真实世界数据集上的实验表明，该方法实现了高实用性和相关性，并通过自动化和专家评估得到验证。", "conclusion": "该研究成功开发了一个动态上下文感知提示推荐系统，有效解决了领域特定AI应用中高质量提示生成困难的问题，并通过实验证明了其有效性和实用性。", "translation": "大型语言模型（LLM）驱动的应用对用户提示的质量高度敏感，但尤其对于领域特定应用而言，创建高质量提示常常具有挑战性。本文提出了一个新颖的动态上下文感知提示推荐系统，用于领域特定AI应用。我们的解决方案结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名，以生成相关且可操作的提示建议。该系统利用行为遥测和两阶段分层推理过程来动态选择和排序相关技能，并使用预定义和自适应模板（通过少样本学习增强）合成提示。在真实世界数据集上的实验表明，我们的方法实现了高实用性和相关性，并通过自动化和专家评估得到验证。", "summary": "本文介绍了一个针对领域特定AI应用的动态上下文感知提示推荐系统。该系统通过整合上下文查询分析、检索增强知识、分层技能组织和自适应技能排名等技术，旨在解决大型语言模型应用中提示质量对性能影响大且难于创建的问题。它利用行为遥测和两阶段推理过程动态选择和排序技能，并结合少样本学习从预定义和自适应模板生成提示。实验结果表明，该系统在真实世界数据上表现出高实用性和相关性。", "keywords": "提示推荐, 动态上下文感知, 领域特定AI, LLM, 提示工程", "comments": "本文提出了一种创新的方法来解决领域特定AI应用中提示工程的挑战。其创新点在于结合了多项技术，包括上下文感知、知识增强、分层技能管理和自适应学习，以实现动态和个性化的提示推荐。这对于提升LLM在垂直领域的应用效率和用户体验具有重要意义。"}}
{"id": "2506.20689", "title": "U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "summary": "Artificial intelligence, including deep learning models, will play a\ntransformative role in automated medical image analysis for the diagnosis of\ncardiac disorders and their management. Automated accurate delineation of\ncardiac images is the first necessary initial step for the quantification and\nautomated diagnosis of cardiac disorders. In this paper, we propose a deep\nlearning based enhanced UNet model, U-R-Veda, which integrates convolution\ntransformations, vision transformer, residual links, channel-attention, and\nspatial attention, together with edge-detection based skip-connections for an\naccurate fully-automated semantic segmentation of cardiac magnetic resonance\n(CMR) images. The model extracts local-features and their interrelationships\nusing a stack of combination convolution blocks, with embedded channel and\nspatial attention in the convolution block, and vision transformers. Deep\nembedding of channel and spatial attention in the convolution block identifies\nimportant features and their spatial localization. The combined edge\ninformation with channel and spatial attention as skip connection reduces\ninformation-loss during convolution transformations. The overall model\nsignificantly improves the semantic segmentation of CMR images necessary for\nimproved medical image analysis. An algorithm for the dual attention module\n(channel and spatial attention) has been presented. Performance results show\nthat U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The\nmodel outperforms the accuracy attained by other models, based on DSC and HD\nmetrics, especially for the delineation of right-ventricle and\nleft-ventricle-myocardium.", "comment": "15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.20689v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20689v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "U-R-VEDA：整合UNET、残差连接、边缘和双重注意力以及视觉Transformer，用于CMR的精确语义分割", "tldr": "提出U-R-VEDA模型，结合UNet、Transformer、残差连接和双重注意力，显著提升CMR图像的语义分割精度。", "motivation": "人工智能在自动化医学图像分析中具有变革性作用，而心脏图像的精确自动描绘是心脏疾病量化和自动诊断的首要步骤。", "method": "提出U-R-VEDA深度学习模型，该模型是增强型UNet，整合了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力以及基于边缘检测的跳跃连接，用于CMR图像的精确全自动语义分割。模型通过结合卷积块、嵌入通道和空间注意力以及视觉Transformer来提取局部特征及其相互关系。结合边缘信息与通道和空间注意力作为跳跃连接，减少了卷积变换过程中的信息损失。", "result": "U-R-VEDA模型在DSC指标下达到了平均95.2%的准确率。该模型在DSC和HD指标下，尤其是在右心室和左心室心肌的描绘方面，优于其他模型的准确性。", "conclusion": "U-R-VEDA模型显著提高了CMR图像的语义分割精度，这对于改进医学图像分析至关重要，并且在分割性能上优于现有模型。", "translation": "人工智能，包括深度学习模型，将在心脏疾病诊断和管理中的自动化医学图像分析方面发挥变革性作用。心脏图像的自动化精确描绘是心脏疾病量化和自动化诊断的首要必要步骤。在本文中，我们提出了一种基于深度学习的增强型UNet模型U-R-Veda，它整合了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力以及基于边缘检测的跳跃连接，用于心脏磁共振（CMR）图像的精确全自动语义分割。该模型通过一系列组合卷积块提取局部特征及其相互关系，其中卷积块中嵌入了通道和空间注意力，并结合了视觉Transformer。卷积块中通道和空间注意力的深度嵌入识别了重要的特征及其空间定位。结合边缘信息与通道和空间注意力作为跳跃连接，减少了卷积变换过程中的信息损失。整个模型显著改善了CMR图像的语义分割，这对于改进医学图像分析是必要的。本文还介绍了一种双重注意力模块（通道和空间注意力）的算法。性能结果显示，U-R-Veda在DSC指标下达到了平均95.2%的准确率。该模型在DSC和HD指标下，尤其是在右心室和左心室心肌的描绘方面，优于其他模型的准确性。", "summary": "本文提出了一种名为U-R-VEDA的增强型UNet深度学习模型，旨在实现心脏磁共振（CMR）图像的精确全自动语义分割。该模型巧妙地结合了卷积变换、视觉Transformer、残差连接、双重注意力（通道和空间）以及基于边缘检测的跳跃连接，以有效提取局部特征并减少信息损失。实验结果表明，U-R-VEDA在DSC指标下达到了95.2%的平均准确率，并在右心室和左心室心肌的描绘方面表现出超越其他模型的优越性能，显著提升了医学图像分析的准确性。", "keywords": "语义分割, 心脏磁共振, UNet, 视觉Transformer, 注意力机制", "comments": "U-R-VEDA模型的创新之处在于其多组件集成策略，结合了UNet的结构优势、Transformer的全局建模能力、残差连接的信息流优化、双重注意力的特征增强以及边缘检测的细节保留。这种混合架构有效地解决了医学图像分割中局部细节与全局上下文理解的挑战，并通过明确引入边缘信息来弥补传统跳跃连接可能的信息损失，提升了分割精度，尤其是在复杂的心脏结构描绘上。"}}
{"id": "2506.20823", "title": "Compact Analytical Model for Real-Time Evaluation of OAM-Based Inter-Satellite Links", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna"], "summary": "This paper presents an efficient analytical framework for evaluating the\nperformance of inter-satellite communication systems utilizing orbital angular\nmomentum (OAM) beams under pointing errors. An accurate analytical model is\nfirst developed to characterize intermodal crosstalk caused by beam\nmisalignment in OAM-based inter-satellite links. Building upon this model, we\nderive efficient expressions to analyze and optimize system performance in\nterms of bit error rate (BER). Unlike traditional Monte Carlo-based methods\nthat are computationally intensive, the proposed approach offers accurate\nperformance predictions. This enables a substantial decrease in computation\ntime while maintaining high accuracy, thanks to the use of analytical\nexpressions for both crosstalk and BER. This fast and accurate evaluation\ncapability is particularly critical for dynamic low Earth orbit (LEO) satellite\nconstellations, where network topology and channel conditions change rapidly,\nrequiring real-time link adaptation. Furthermore, we systematically design and\nevaluate asymmetric OAM mode sets, which significantly outperform symmetric\nconfigurations in the presence of pointing errors. Our results also reveal key\ninsights into the interaction between beam divergence, tracking accuracy, and\nlink distance, demonstrating that the proposed framework enables real-time\noptimization of system parameters with high fidelity. The analytical findings\nare rigorously validated against extensive Monte Carlo simulations, confirming\ntheir practical applicability for high-mobility optical wireless systems such\nas LEO satellite networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20823v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20823v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "紧凑型分析模型用于实时评估基于OAM的星间链路", "tldr": "本文提出了一种紧凑的分析模型，用于快速准确地评估基于OAM的星间链路在指向误差下的性能，并支持实时优化。", "motivation": "传统的蒙特卡洛方法计算量大，难以满足动态低地球轨道（LEO）卫星星座中快速变化的拓扑和信道条件对实时链路适应性的需求。", "method": "开发了一个精确的分析模型来表征OAM星间链路中由波束失准引起的模间串扰；基于该模型，推导出高效的表达式来分析和优化误码率（BER）方面的系统性能；系统地设计和评估了非对称OAM模式集。", "result": "所提出的分析方法提供了准确的性能预测，显著减少了计算时间，同时保持了高精度；非对称OAM模式集在存在指向误差的情况下显著优于对称配置；研究结果揭示了波束发散、跟踪精度和链路距离之间相互作用的关键见解，实现了系统参数的实时高保真优化。", "conclusion": "所提出的分析模型能够快速准确地评估和优化基于OAM的星间链路性能，其分析结果通过蒙特卡洛仿真得到严格验证，证实了其在LEO卫星网络等高移动性光无线系统中的实际适用性。", "translation": "本文提出了一种高效的分析框架，用于评估在指向误差下利用轨道角动量（OAM）光束的星间通信系统的性能。首先，开发了一个精确的分析模型来表征基于OAM的星间链路中由光束失准引起的模间串扰。在此模型的基础上，我们推导出了高效的表达式，以误码率（BER）为指标分析和优化系统性能。与计算密集型的传统蒙特卡洛方法不同，所提出的方法提供了准确的性能预测。这使得计算时间大幅减少，同时由于使用了串扰和BER的分析表达式而保持了高精度。这种快速准确的评估能力对于动态低地球轨道（LEO）卫星星座尤其关键，因为在这些星座中，网络拓扑和信道条件变化迅速，需要实时链路适应。此外，我们系统地设计和评估了非对称OAM模式集，这些模式集在存在指向误差的情况下显著优于对称配置。我们的结果还揭示了光束发散、跟踪精度和链路距离之间相互作用的关键见解，表明所提出的框架能够以高保真度实现系统参数的实时优化。分析结果通过广泛的蒙特卡洛仿真进行了严格验证，证实了它们在LEO卫星网络等高移动性光无线系统中的实际适用性。", "summary": "本文提出了一种紧凑高效的分析模型，用于实时评估基于轨道角动量（OAM）的星间链路在指向误差下的性能。该模型通过表征模间串扰并推导误码率表达式，实现了比传统蒙特卡洛方法更快的计算速度和更高的精度。研究还设计并验证了性能更优的非对称OAM模式集，并揭示了影响链路性能的关键参数间的相互作用，为LEO卫星网络等动态光无线系统提供了实时优化能力。", "keywords": "OAM, 星间链路, 分析模型, 指向误差, 实时评估", "comments": "本文的创新之处在于提出了一个紧凑且高效的分析模型，解决了传统蒙特卡洛方法在评估OAM星间链路性能时计算量大的问题。其重要性体现在为动态变化的LEO卫星星座提供了实时性能评估和参数优化的能力，这对于未来高移动性光无线通信系统的设计和部署具有重要意义。通过引入非对称OAM模式集，进一步提升了系统在实际指向误差下的鲁棒性。"}}
{"id": "2506.20901", "title": "Data Visualization for Improving Financial Literacy: A Systematic Review", "authors": ["Meng Du", "Robert Amor", "Kwan-Liu Ma", "Burkhard C. Wünsche"], "summary": "Financial literacy empowers individuals to make informed and effective\nfinancial decisions, improving their overall financial well-being and security.\nHowever, for many people understanding financial concepts can be daunting and\nonly half of US adults are considered financially literate. Data visualization\nsimplifies these concepts, making them accessible and engaging for learners of\nall ages. This systematic review analyzes 37 research papers exploring the use\nof data visualization and visual analytics in financial education and literacy\nenhancement. We classify these studies into five key areas: (1) the evolution\nof visualization use across time and space, (2) motivations for using\nvisualization tools, (3) the financial topics addressed and instructional\napproaches used, (4) the types of tools and technologies applied, and (5) how\nthe effectiveness of teaching interventions was evaluated. Furthermore, we\nidentify research gaps and highlight opportunities for advancing financial\nliteracy. Our findings offer practical insights for educators and professionals\nto effectively utilize or design visual tools for financial literacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20901v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20901v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "数据可视化在提升金融素养方面的应用：一项系统综述", "tldr": "本系统综述分析了37篇研究论文，探讨了数据可视化在金融教育中的应用，并将其分为五个关键领域，旨在识别研究空白并为教育者和专业人士提供实用见解，以提高金融素养。", "motivation": "许多人觉得理解金融概念很困难，且只有一半的美国成年人被认为是具备金融素养的。数据可视化能够简化这些概念，使其更易于理解和吸引人。", "method": "本研究进行了一项系统综述，分析了37篇探讨数据可视化和视觉分析在金融教育及素养提升中应用的研究论文。这些研究被分为五个关键领域进行分类：可视化使用的时间和空间演变、使用可视化工具的动机、涉及的金融主题和教学方法、应用的工具和技术类型，以及教学干预有效性的评估方式。", "result": "研究结果识别了数据可视化在金融素养领域的研究空白，并强调了进一步发展的机会。此外，研究发现为教育者和专业人士提供了实用见解，以有效利用或设计视觉工具来提升金融素养。", "conclusion": "本系统综述的结论是，通过分析现有研究，识别了数据可视化在金融素养领域的研究空白和发展机遇，并为教育者和专业人士提供了利用视觉工具提升金融素养的实用指导。", "translation": "金融素养使个人能够做出明智有效的金融决策，从而改善他们的整体财务福祉和安全。然而，对许多人来说，理解金融概念可能令人望而生畏，而且只有一半的美国成年人被认为是具备金融素养的。数据可视化简化了这些概念，使所有年龄段的学习者都能轻松接触并参与其中。本系统综述分析了37篇研究论文，探讨了数据可视化和视觉分析在金融教育和素养提升中的应用。我们将这些研究分为五个关键领域：(1) 可视化使用随时间和空间的演变，(2) 使用可视化工具的动机，(3) 所涉及的金融主题和所使用的教学方法，(4) 所应用的工具和技术类型，以及 (5) 教学干预有效性的评估方式。此外，我们还识别了研究空白，并强调了提升金融素养的机会。我们的研究结果为教育者和专业人士有效利用或设计视觉工具以提升金融素养提供了实用见解。", "summary": "本系统综述探讨了数据可视化在提升金融素养方面的应用。针对金融概念理解困难及金融素养普遍不足的问题，研究分析了37篇相关论文，并将其归类为五个方面：可视化演变、使用动机、主题与方法、工具与技术以及效果评估。研究旨在识别现有研究空白并提供实用见解，以帮助教育者和专业人士有效利用视觉工具促进金融素养的提升。", "keywords": "数据可视化, 金融素养, 系统综述, 金融教育, 视觉分析", "comments": "该论文通过系统综述的方法，全面梳理了数据可视化在金融素养领域的研究现状，其创新之处在于对现有文献进行了细致的分类和归纳，并明确指出了研究空白和未来发展方向。这对于推动金融教育领域的数据可视化应用具有重要的指导意义。其局限性可能在于仅限于现有文献的分析，未涉及实证研究。"}}
{"id": "2506.21174", "title": "Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4", "authors": ["Jongyeon Park", "Joonhee Lee", "Do-Hyeon Lim", "Hong Kook Kim", "Hyeongcheol Geum", "Jeong Eun Lim"], "summary": "This technical report presents submission systems for Task 4 of the DCASE\n2025 Challenge. This model incorporates additional audio features (spectral\nroll-off and chroma features) into the embedding feature extracted from the\nmel-spectral feature to im-prove the classification capabilities of an\naudio-tagging model in the spatial semantic segmentation of sound scenes (S5)\nsystem. This approach is motivated by the fact that mixed audio often contains\nsubtle cues that are difficult to capture with mel-spectrograms alone. Thus,\nthese additional features offer alterna-tive perspectives for the model.\nSecond, an agent-based label correction system is applied to the outputs\nprocessed by the S5 system. This system reduces false positives, improving the\nfinal class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.\nFinally, we refine the training dataset to enhance the classi-fication accuracy\nof low-performing classes by removing irrele-vant samples and incorporating\nexternal data. That is, audio mix-tures are generated from a limited number of\ndata points; thus, even a small number of out-of-class data points could\ndegrade model performance. The experiments demonstrate that the submit-ted\nsystems employing these approaches relatively improve CA-SDRi by up to 14.7%\ncompared to the baseline of DCASE 2025 Challenge Task 4.", "comment": "DCASE 2025 challenge Task4, 5 pages", "pdf_url": "http://arxiv.org/pdf/2506.21174v1", "categories": ["eess.AS", "cs.LG"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21174v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DCASE 2025挑战任务4中通过丰富音频特征和基于代理的错误校正改进空间语义分割的性能", "tldr": "该技术报告介绍了DCASE 2025挑战任务4的提交系统，通过整合额外的音频特征、应用基于代理的标签校正系统以及优化训练数据集来提高空间语义分割的性能，相对于基线系统，CA-SDRi指标提高了14.7%。", "motivation": "为了提高声景空间语义分割（S5）系统中音频标注模型的分类能力，因为混合音频中包含仅靠梅尔谱图难以捕捉的细微线索。此外，还需要减少假阳性并提升低性能类别的分类精度。", "method": "1. 将额外的音频特征（频谱滚降和色度特征）整合到从梅尔谱图特征中提取的嵌入特征中。2. 应用基于代理的标签校正系统对S5系统处理的输出进行校正，以减少假阳性。3. 通过移除不相关样本和纳入外部数据来优化训练数据集，以增强低性能类别的分类准确性。", "result": "与DCASE 2025挑战任务4的基线系统相比，所提交的系统在CA-SDRi指标上相对提高了高达14.7%。", "conclusion": "通过结合丰富的音频特征、基于代理的错误校正和优化的训练数据集，能够显著提高声景空间语义分割系统的性能，并在DCASE 2025挑战任务4中取得了显著的改进。", "translation": "本技术报告介绍了DCASE 2025挑战任务4的提交系统。该模型将额外的音频特征（频谱滚降和色度特征）整合到从梅尔谱图特征中提取的嵌入特征中，以提高声景空间语义分割（S5）系统中音频标注模型的分类能力。这种方法的原因是，混合音频通常包含仅靠梅尔谱图难以捕捉的细微线索。因此，这些额外的特征为模型提供了替代视角。其次，将基于代理的标签校正系统应用于S5系统处理的输出。该系统减少了假阳性，提高了最终的类别感知信号失真比改善（CA-SDRi）指标。最后，我们通过移除不相关样本和纳入外部数据来优化训练数据集，以提高低性能类别的分类准确性。也就是说，音频混合物是从有限的数据点生成的；因此，即使少量的不属于类内的数据点也可能降低模型性能。实验表明，采用这些方法的提交系统与DCASE 2025挑战任务4的基线系统相比，CA-SDRi相对提高了高达14.7%。", "summary": "本文介绍了DCASE 2025挑战任务4的提交系统，旨在提高声景空间语义分割的性能。该系统通过整合频谱滚降和色度等额外音频特征来增强分类能力，并引入基于代理的标签校正系统以减少假阳性并提升CA-SDRi指标。此外，研究还优化了训练数据集，移除了不相关样本并加入了外部数据，以改善低性能类别的分类准确性。实验结果显示，相对于基线系统，这些方法使CA-SDRi指标提高了14.7%。", "keywords": "空间语义分割, 音频特征, 错误校正, DCASE 2025, CA-SDRi", "comments": "该论文通过多方面策略提升了声景空间语义分割的性能，其创新点在于结合了多源音频特征、引入了基于代理的后处理校正机制，并对数据进行了细致的清洗和增强。这些方法协同作用，有效解决了复杂声学场景中分类精度和假阳性问题，为DCASE挑战提供了有竞争力的解决方案。其系统性的优化方法值得借鉴。"}}
{"id": "2506.20761", "title": "A Framework for Building Data Structures from Communication Protocols", "authors": ["Alexandr Andoni", "Shunhua Jiang", "Omri Weinstein"], "summary": "We present a general framework for designing efficient data structures for\nhigh-dimensional pattern-matching problems ($\\exists \\;? i\\in[n], f(x_i,y)=1$)\nthrough communication models in which $f(x,y)$ admits sublinear communication\nprotocols with exponentially-small error. Specifically, we reduce the data\nstructure problem to the Unambiguous Arthur-Merlin (UAM) communication\ncomplexity of $f(x,y)$ under product distributions.\n  We apply our framework to the Partial Match problem (a.k.a, matching with\nwildcards), whose underlying communication problem is sparse set-disjointness.\nWhen the database consists of $n$ points in dimension $d$, and the number of\n$\\star$'s in the query is at most $w = c\\log n \\;(\\ll d)$, the fastest known\nlinear-space data structure (Cole, Gottlieb and Lewenstein, STOC'04) had query\ntime $t \\approx 2^w = n^c$, which is nontrivial only when $c<1$. By contrast,\nour framework produces a data structure with query time $n^{1-1/(c \\log^2 c)}$\nand space close to linear.\n  To achieve this, we develop a one-sided $\\epsilon$-error communication\nprotocol for Set-Disjointness under product distributions with\n$\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$ complexity, improving on the\nclassical result of Babai, Frankl and Simon (FOCS'86). Building on this\nprotocol, we show that the Unambiguous AM communication complexity of\n$w$-Sparse Set-Disjointness with $\\epsilon$-error under product distributions\nis $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$, independent of the ambient dimension\n$d$, which is crucial for the Partial Match result. Our framework sheds further\nlight on the power of data-dependent data structures, which is instrumental for\nreducing to the (much easier) case of product distributions.", "comment": "53 pages, STOC 2025", "pdf_url": "http://arxiv.org/pdf/2506.20761v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20761v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "从通信协议构建数据结构的一个框架", "tldr": "本文提出了一个通用框架，通过通信模型设计高效的高维模式匹配数据结构，并将其应用于部分匹配问题，显著提高了查询时间，同时改进了集合不交性问题的通信协议。", "motivation": "旨在为高维模式匹配问题设计高效的数据结构，特别是通过利用通信模型来解决这些问题。", "method": "提出一个通用框架，将数据结构问题归结为乘积分布下函数 $f(x,y)$ 的无歧义阿瑟-梅林（UAM）通信复杂度。具体地，开发了一种在乘积分布下具有 $\tilde{\theta}(\\sqrt{d\\log(1/\\epsilon)})$ 复杂度的单边 $\\epsilon$-误差集合不交性通信协议，并在此基础上证明了 $w$-稀疏集合不交性在乘积分布下的无歧义 AM 通信复杂度为 $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$。", "result": "将该框架应用于部分匹配问题，在查询中通配符数量为 $w = c\\log n$ 时，实现了查询时间 $n^{1-1/(c \\log^2 c)}$ 和接近线性的空间，显著优于现有最快的线性空间数据结构（查询时间为 $n^c$）。同时，改进了 Babai, Frankl 和 Simon (FOCS'86) 关于集合不交性通信协议的经典结果。", "conclusion": "该框架揭示了数据依赖型数据结构的强大能力，这对于简化到（更容易的）乘积分布情况至关重要。", "translation": "我们提出了一个通用框架，用于通过通信模型设计高效的高维模式匹配问题的数据结构（$\\\\exists \\\\;? i\\\\in[n], f(x_i,y)=1$），其中 $f(x,y)$ 允许具有指数级小误差的亚线性通信协议。具体而言，我们将数据结构问题归结为乘积分布下 $f(x,y)$ 的无歧义阿瑟-梅林（UAM）通信复杂度。\n我们将我们的框架应用于部分匹配问题（又称带通配符匹配），其底层通信问题是稀疏集合不交性。当数据库包含 $d$ 维的 $n$ 个点，并且查询中的星号数量至多为 $w = c\\log n \\\\;(\\\\ll d)$ 时，已知最快的线性空间数据结构（Cole, Gottlieb 和 Lewenstein, STOC'04）的查询时间约为 $2^w = n^c$，这仅在 $c<1$ 时才非平凡。相比之下，我们的框架生成的数据结构具有 $n^{1-1/(c \\log^2 c)}$ 的查询时间以及接近线性的空间。\n为了实现这一点，我们开发了一种在乘积分布下具有 $\\\\tilde{\\Theta}(\\\\sqrt{d\\\\log(1/\\\\epsilon)})$ 复杂度的单边 $\\\\epsilon$-误差集合不交性通信协议，改进了 Babai, Frankl 和 Simon (FOCS'86) 的经典结果。在此协议的基础上，我们表明在乘积分布下具有 $\\\\epsilon$-误差的 $w$-稀疏集合不交性的无歧义 AM 通信复杂度为 $\\\\tilde{O}(\\\\sqrt{w \\\\log(1/\\\\epsilon)})$，这与环境维度 $d$ 无关，这对于部分匹配结果至关重要。我们的框架进一步阐明了数据依赖型数据结构的强大能力，这对于简化到（更容易的）乘积分布情况至关重要。", "summary": "本文提出了一种通用的框架，用于通过通信协议设计高维模式匹配问题的有效数据结构。该框架将数据结构问题转化为无歧义阿瑟-梅林（UAM）通信复杂度问题，并将其成功应用于部分匹配问题，显著提高了查询效率。为实现此目标，作者开发了一种改进的集合不交性通信协议，其复杂度优于现有成果。研究强调了数据依赖型数据结构在处理乘积分布情况下的重要性。", "keywords": "数据结构, 通信协议, 模式匹配, 部分匹配, 集合不交性", "comments": "该论文的创新点在于提出了一个通用的框架，将数据结构问题与通信复杂度理论相结合，特别是利用了UAM通信复杂度。其重要性体现在为高维模式匹配问题提供了更高效的解决方案，并通过改进底层通信协议（如集合不交性）为相关领域带来了理论上的进步。该方法对于处理数据依赖型数据结构具有启发意义。"}}
{"id": "2506.20854", "title": "Towards Two-Stage Counterfactual Learning to Rank", "authors": ["Shashank Gupta", "Yiming Liao", "Maarten de Rijke"], "summary": "Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.", "comment": "Accepted at ICTIR 2025 (co-located with SIGIR 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20854v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20854v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "面向两阶段反事实排序学习", "tldr": "针对大规模排序中单阶段反事实排序学习(CLTR)的不切实际性，本文提出了一种新颖的两阶段CLTR估计器和联合优化方法，用于同时训练候选生成器和排序器，并在半合成基准上验证了其有效性。", "motivation": "现有反事实排序学习（CLTR）方法采用单阶段策略，在处理数百万文档的实际大规模排序系统时效率低下且不切实际。尽管实际系统采用两阶段架构，但现有针对两阶段离线排序系统的CLTR方法存在局限性（例如，只考虑前1个排序设置，且排序器固定），并且缺乏一种能够联合训练排序器和候选生成器的CLTR方法。", "method": "本文提出了一种两阶段CLTR估计器，该估计器考虑了两个阶段之间的交互作用，并能够离线估计两种策略的联合价值。此外，论文还提出了一种新颖的联合优化方法，用于分别训练候选生成器和排序器策略。", "result": "在半合成基准上的实验结果表明，所提出的联合CLTR方法比基线方法更有效。", "conclusion": "据作者所知，本文首次提出了一种针对两阶段排序系统的反事实排序学习估计器和学习方法，弥补了现有文献中在实际大规模应用方面的一个关键空白。", "translation": "反事实排序学习 (CLTR) 旨在从用户交互中学习排序策略，同时纠正交互数据中固有的偏差，例如位置偏差。现有的 CLTR 方法假设采用单一的排序策略，从整个文档候选集中选择前 K 个排序结果。在实际应用中，候选文档集可能达到数百万，这使得单一阶段的排序策略不切实际。为了扩展到数百万文档，实际的排序系统被设计成两阶段模式，即先有候选生成器，后有排序器。现有的针对两阶段离线排序系统的 CLTR 方法只考虑了前 1 个排序设置，并且只专注于训练候选生成器，而排序器是固定的。现有文献中缺乏一种用于联合训练排序器和候选生成器的 CLTR 方法。在本文中，我们提出了一种两阶段 CLTR 估计器，该估计器考虑了两个阶段之间的交互，并离线估计了两种策略的联合价值。此外，我们提出了一种新颖的联合优化方法，分别训练候选和排序器策略。据我们所知，我们是第一个提出两阶段排序的 CLTR 估计器和学习方法。在半合成基准上的实验结果表明，所提出的联合 CLTR 方法优于基线方法。", "summary": "本论文旨在解决现有反事实排序学习（CLTR）方法在处理大规模真实世界排序系统时的局限性，因为这些系统通常包含数百万文档且采用两阶段架构。针对现有CLTR方法无法有效处理两阶段系统，尤其是缺乏联合训练候选生成器和排序器的方法，本文提出了一种新颖的两阶段CLTR估计器。该估计器考虑了两个阶段间的交互，并能离线估计两策略的联合价值。此外，论文还引入了一种联合优化方法来训练这两个阶段的策略。在半合成基准上的实验结果验证了所提出联合CLTR方法的有效性。", "keywords": "反事实排序学习, 两阶段排序, 偏差校正, 候选生成, 排序器优化", "comments": "本文通过将反事实排序学习（CLTR）扩展到两阶段架构，并提出联合优化方法，成功地弥合了CLTR理论与大规模实际应用之间的鸿沟，是该领域的一项重要创新。其核心贡献在于首次提出了针对两阶段排序的CLTR估计器和学习方法，特别强调了阶段间交互和联合价值估计，这对于提升真实世界排序系统的性能至关重要。一个潜在的局限性是实验仅在“半合成基准”上进行，其结果可能无法完全代表复杂生产环境中的表现。"}}
{"id": "2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "comment": "main paper is 14 pages", "pdf_url": "http://arxiv.org/pdf/2506.20803v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20803v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "构思-执行差距：大型语言模型生成与人类研究想法的执行结果", "tldr": "本研究发现，尽管大型语言模型（LLM）生成的科研想法在构思阶段可能被认为更具新颖性，但在实际执行后，其研究质量（新颖性、兴奋度、有效性和整体评价）显著低于人类专家提出的想法，揭示了LLM在生成真正有效科研想法方面的局限性。", "motivation": "大型语言模型（LLMs）在加速科学研究流程方面显示出潜力，特别是在生成新颖研究想法方面。然而，先前研究发现LLM生成的想法在构思阶段被认为比人类专家想法更具新颖性，但一个好的想法不仅要新颖，还应在执行后产生更好的研究成果。因此，本研究旨在测试AI生成的想法是否能带来更好的研究成果，以弥补构思与执行之间的差距。", "method": "本研究进行了一项执行研究，招募了43位专家研究人员，随机分配他们执行由专家撰写或由LLM生成的想法。每位专家花费超过100小时实施想法，并撰写了一篇4页的短论文记录实验。所有执行的项目随后由专家NLP研究人员进行盲审。通过比较想法在执行前后的评审分数来评估。", "result": "与执行前相比，LLM生成的想法在执行后，其分数在所有评估指标（新颖性、兴奋度、有效性和整体评价；p < 0.05）上均显著下降，降幅远大于专家撰写的想法。这弥合了在构思阶段观察到的LLM与人类想法之间的差距。在比较执行研究的汇总评审分数时，甚至发现在许多指标上排名发生了逆转，人类想法得分高于LLM想法。", "conclusion": "本研究揭示了LLM在生成真正有效研究想法方面的局限性，并强调了在缺乏执行结果的情况下评估研究想法的挑战。大型语言模型在构思阶段看似新颖的想法，在实际执行后表现出明显的“构思-执行差距”，其研究成果质量低于人类专家。", "translation": "大型语言模型（LLMs）在加速科学研究流程方面显示出潜力。这一过程的一个关键能力是生成新颖的研究想法，并且先前的研究发现，在某些情况下，LLM生成的想法被判断为比人类专家的想法更具新颖性。然而，一个好的想法不应仅仅看起来新颖，它还应该在执行后产生更好的研究成果。为了测试AI生成的想法是否能带来更好的研究成果，我们进行了一项执行研究，招募了43位专家研究人员来执行随机分配的想法，这些想法要么由专家撰写，要么由LLM生成。每位专家花费超过100小时实施该想法，并撰写了一篇4页的短论文来记录实验。所有执行的项目随后都由专家NLP研究人员进行盲审。比较想法在执行前后的评审分数，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性和整体评价；p < 0.05）上的分数下降幅度显著大于专家撰写的想法，从而弥合了在构思阶段观察到的LLM与人类想法之间的差距。在比较执行研究的汇总评审分数时，我们甚至观察到在许多指标上排名发生了逆转，人类想法得分高于LLM想法。这种构思-执行差距凸显了当前LLM在生成真正有效研究想法方面的局局限性，以及在缺乏执行结果的情况下评估研究想法的挑战。", "summary": "本研究探讨了大型语言模型（LLM）生成的科研想法与人类专家想法在实际执行后的质量差异。通过一项招募43位专家研究人员执行随机分配想法的实验，并对执行成果进行盲审，研究发现，尽管LLM想法在构思阶段可能被认为更具新颖性，但在执行后，其在所有评估指标（新颖性、兴奋度、有效性、整体评价）上的分数均显著低于人类想法，揭示了LLM在生成“可执行”且“有效”研究想法方面的局限性，并强调了评估研究想法时执行结果的重要性。", "keywords": "大型语言模型, 科研想法, 执行研究, 构思-执行差距, 研究质量", "comments": "这项研究非常创新，它挑战了当前对LLM在科学研究中作用的普遍乐观预期。它不仅关注了想法的生成，更进一步深入到想法的“可执行性”和“执行结果”，填补了LLM研究领域的一个重要空白。其发现对于理解LLM在复杂、长期任务（如科研）中的真实能力具有重要意义，并为未来LLM辅助研究工具的开发提供了宝贵启示。"}}
{"id": "2506.21324", "title": "Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning", "authors": ["Jiechen Chen", "Bipin Rajendran", "Osvaldo Simeone"], "summary": "Neuromorphic and quantum computing have recently emerged as promising\nparadigms for advancing artificial intelligence, each offering complementary\nstrengths. Neuromorphic systems built on spiking neurons excel at processing\ntime-series data efficiently through sparse, event-driven computation,\nconsuming energy only upon input events. Quantum computing, on the other hand,\nleverages superposition and entanglement to explore feature spaces that are\nexponentially large in the number of qubits. Hybrid approaches combining these\nparadigms have begun to show potential, but existing quantum spiking models\nhave important limitations. Notably, prior quantum spiking neuron\nimplementations rely on classical memory mechanisms on single qubits, requiring\nrepeated measurements to estimate firing probabilities, and they use\nconventional backpropagation on classical simulators for training. Here we\npropose a stochastic quantum spiking (SQS) neuron model that addresses these\nchallenges. The SQS neuron uses multi-qubit quantum circuits to realize a\nspiking unit with internal quantum memory, enabling event-driven probabilistic\nspike generation in a single shot. Furthermore, we outline how networks of SQS\nneurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a\nhardware-friendly local learning rule, eliminating the need for global\nclassical backpropagation. The proposed SQSNN model fuses the time-series\nefficiency of neuromorphic computing with the exponentially large inner state\nspace of quantum computing, paving the way for quantum spiking neural networks\nthat are modular, scalable, and trainable on quantum hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21324v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21324v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "具有量子记忆和局部学习的随机量子脉冲神经网络", "tldr": "本文提出了一种新的随机量子脉冲（SQS）神经元模型及其网络（SQSNNs），该模型利用多量子比特电路实现内部量子记忆，并采用硬件友好的局部学习规则进行训练，克服了现有量子脉冲模型依赖经典记忆和反向传播的局限性，为可扩展的量子脉冲神经网络铺平了道路。", "motivation": "现有的量子脉冲神经元实现存在局限性，它们依赖于单量子比特上的经典记忆机制，需要重复测量来估计脉冲概率，并且训练时使用经典模拟器上的传统反向传播算法。", "method": "本文提出了一种随机量子脉冲（SQS）神经元模型，该模型使用多量子比特量子电路实现具有内部量子记忆的脉冲单元，从而能够单次事件驱动地生成概率性脉冲。此外，还提出了SQS神经元网络（SQSNNs）可以通过硬件友好的局部学习规则进行训练，从而消除了对全局经典反向传播的需求。", "result": "提出的SQS神经元模型实现了单次事件驱动的概率性脉冲生成，并具有内部量子记忆。SQSNNs可以通过局部学习规则进行训练，融合了神经形态计算的时间序列效率和量子计算的指数级大内部状态空间。", "conclusion": "所提出的SQSNN模型融合了神经形态计算的时间序列效率与量子计算的指数级大内部状态空间，为构建模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络铺平了道路。", "translation": "神经形态计算和量子计算最近已成为推动人工智能发展的有前景的范式，各自提供了互补的优势。基于脉冲神经元的神经形态系统擅长通过稀疏、事件驱动的计算有效地处理时间序列数据，仅在输入事件发生时才消耗能量。另一方面，量子计算利用叠加和纠缠来探索在量子比特数量上呈指数级增长的特征空间。结合这些范式的混合方法已开始显示出潜力，但现有的量子脉冲模型存在重要局限性。值得注意的是，先前的量子脉冲神经元实现依赖于单量子比特上的经典记忆机制，需要重复测量来估计脉冲概率，并且它们使用经典模拟器上的传统反向传播进行训练。本文提出了一种随机量子脉冲（SQS）神经元模型来解决这些挑战。SQS神经元使用多量子比特量子电路来实现具有内部量子记忆的脉冲单元，从而能够单次事件驱动地生成概率性脉冲。此外，我们概述了如何通过硬件友好的局部学习规则训练SQS神经元网络（称为SQSNNs），从而消除了对全局经典反向传播的需求。所提出的SQSNN模型融合了神经形态计算的时间序列效率与量子计算的指数级大内部状态空间，为构建模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络铺平了道路。", "summary": "本文提出了一种随机量子脉冲（SQS）神经元模型及其网络（SQSNNs），旨在克服现有量子脉冲模型的局限性。SQS神经元利用多量子比特量子电路实现具有内部量子记忆的脉冲单元，能够单次实现事件驱动的概率性脉冲生成。此外，SQSNNs采用硬件友好的局部学习规则进行训练，无需传统的全局反向传播。该模型成功融合了神经形态计算的时间序列处理效率和量子计算的指数级状态空间探索能力，为构建可在量子硬件上进行训练的模块化、可扩展的量子脉冲神经网络奠定了基础。", "keywords": "随机量子脉冲神经网络, 量子记忆, 局部学习, 神经形态计算, 量子计算", "comments": "本文的创新之处在于提出了带有内部量子记忆的随机量子脉冲神经元，并通过局部学习规则实现了量子脉冲神经网络的训练，避免了传统方法中对经典记忆和全局反向传播的依赖。这对于在量子硬件上实现高效且可扩展的量子神经网络具有重要意义，是量子人工智能领域的一个关键进展。"}}
{"id": "2506.20890", "title": "Multicontinuum Homogenization for Poroelasticity Model", "authors": ["Dmitry Ammosov", "Mohammed Al-Kobaisi", "Yalchin Efendiev"], "summary": "In this paper, we derive multicontinuum poroelasticity models using the\nmulticontinuum homogenization method. Poroelasticity models are widely used in\nmany areas of science and engineering to describe coupled flow and mechanics\nprocesses in porous media. However, in many applications, the properties of\nporoelastic media possess high contrast, presenting serious computational\nchallenges. It is well known that standard homogenization approaches often fail\nto give an accurate solution due to the lack of macroscopic parameters.\nMulticontinuum approaches allow us to consider such cases by defining several\naverage states known as continua. In the field of poroelasticity,\nmultiple-network models arising from the multiple porous media theory are\nrepresentatives of these approaches. In this work, we extend previous findings\nby deriving the generalized multicontinuum poroelasticity model. We apply the\nrecently developed multicontinuum homogenization method and provide a rigorous\nderivation of multicontinuum equations. For this purpose, we formulate coupled\nconstraint cell problems in oversampled regions to consider different\nhomogenized effects. Then, we obtain a multicontinuum expansion of the\nfine-scale fields and derive the multicontinuum model supposing the smoothness\nof macroscopic variables. We present the most general version of equations and\nthe simplified ones based on our numerical experiments. Numerical results are\npresented for different heterogeneous media cases and demonstrate the high\naccuracy of our proposed multicontinuum models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20890v1", "categories": ["math.NA", "cs.CE", "cs.NA", "physics.comp-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20890v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "多连续体均质化孔隙弹性模型", "tldr": "本文提出了一种广义多连续体孔隙弹性模型，通过多连续体均质化方法解决标准均质化方法在处理高对比度孔隙弹性介质时面临的计算挑战和精度问题，并通过数值实验证明了其高精度。", "motivation": "孔隙弹性模型在多孔介质中描述流固耦合过程被广泛应用，但在高对比度介质中存在严重的计算挑战。标准均质化方法因缺乏宏观参数而无法提供准确解。", "method": "本文采用多连续体均质化方法，推导了广义多连续体孔隙弹性模型。具体方法包括：在过采样区域建立耦合约束单元问题以考虑不同的均质化效应；获得精细尺度场的多连续体展开；假设宏观变量的平滑性来推导多连续体模型。", "result": "推导出了最通用的方程版本和基于数值实验的简化版本。数值结果表明，所提出的多连续体模型在不同非均匀介质情况下具有高精度。", "conclusion": "所提出的多连续体孔隙弹性模型能够有效解决高对比度多孔介质的计算挑战，并提供高精度的解决方案。", "translation": "在本文中，我们使用多连续体均质化方法推导了多连续体孔隙弹性模型。孔隙弹性模型在科学和工程的许多领域被广泛用于描述多孔介质中的耦合流体和力学过程。然而，在许多应用中，孔隙弹性介质的性质具有高对比度，带来了严重的计算挑战。众所周知，由于缺乏宏观参数，标准均质化方法通常无法给出准确的解。多连续体方法通过定义几个称为连续体的平均状态，使我们能够考虑这种情况。在孔隙弹性领域，源自多孔介质理论的多网络模型是这些方法的代表。在这项工作中，我们通过推导广义多连续体孔隙弹性模型来扩展之前的研究发现。我们应用了最近开发的多连续体均质化方法，并提供了多连续体方程的严格推导。为此，我们在过采样区域中建立了耦合约束单元问题，以考虑不同的均质化效应。然后，我们获得了精细尺度场的多连续体展开，并在假设宏观变量平滑性的前提下推导了多连续体模型。我们根据数值实验提出了最通用的方程版本和简化版本。数值结果展示了不同非均匀介质情况下的应用，并证明了我们提出的多连续体模型具有高精度。", "summary": "本文提出了一种基于多连续体均质化方法的广义多连续体孔隙弹性模型，旨在解决标准均质化方法在处理高对比度孔隙弹性介质时面临的计算挑战和精度不足问题。研究通过在过采样区域建立耦合约束单元问题、获取精细尺度场的多连续体展开以及假设宏观变量平滑性等步骤，严格推导了多连续体方程。数值实验结果验证了所提出模型在不同非均匀介质情况下的高精度。", "keywords": "多连续体, 均质化, 孔隙弹性, 多孔介质, 非均匀介质", "comments": "本文通过引入多连续体均质化方法，有效地解决了标准均质化方法在处理高对比度孔隙弹性介质时的局限性，具有重要的理论和应用价值。其创新点在于提出了一种广义的多连续体孔隙弹性模型，并通过严格的推导和数值验证，展示了其在复杂介质中的高精度。"}}
{"id": "2506.21167", "title": "A Hierarchical Deep Learning Approach for Minority Instrument Detection", "authors": ["Dylan Sechet", "Francesca Bugiotti", "Matthieu Kowalski", "Edouard d'Hérouville", "Filip Langiewicz"], "summary": "Identifying instrument activities within audio excerpts is vital in music\ninformation retrieval, with significant implications for music cataloging and\ndiscovery. Prior deep learning endeavors in musical instrument recognition have\npredominantly emphasized instrument classes with ample data availability.\nRecent studies have demonstrated the applicability of hierarchical\nclassification in detecting instrument activities in orchestral music, even\nwith limited fine-grained annotations at the instrument level. Based on the\nHornbostel-Sachs classification, such a hierarchical classification system is\nevaluated using the MedleyDB dataset, renowned for its diversity and richness\nconcerning various instruments and music genres. This work presents various\nstrategies to integrate hierarchical structures into models and tests a new\nclass of models for hierarchical music prediction. This study showcases more\nreliable coarse-level instrument detection by bridging the gap between detailed\ninstrument identification and group-level recognition, paving the way for\nfurther advancements in this domain.", "comment": "International Conference on Digital Audio Effects (DAFx)", "pdf_url": "http://arxiv.org/pdf/2506.21167v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21167v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种用于少数乐器检测的分层深度学习方法", "tldr": "本研究提出了一种分层深度学习方法，用于检测音乐中的乐器活动，特别是在数据稀缺的少数乐器类别中，通过弥合乐器级别和组级别识别之间的差距，实现了更可靠的粗粒度乐器检测。", "motivation": "在音乐信息检索中，识别音频片段中的乐器活动至关重要，对音乐编目和发现具有重要意义。然而，以往的深度学习在乐器识别方面主要侧重于数据充足的乐器类别，而对细粒度标注有限的少数乐器识别存在挑战。", "method": "本研究基于Hornbostel-Sachs分类系统，利用MedleyDB数据集评估了一种分层分类系统。该工作提出了多种策略将分层结构整合到模型中，并测试了一类新的分层音乐预测模型。", "result": "本研究通过弥合详细乐器识别和组级别识别之间的差距，展示了更可靠的粗粒度乐器检测。", "conclusion": "该研究通过引入分层深度学习方法，有效地解决了少数乐器检测中的数据稀缺问题，为该领域的进一步发展奠定了基础，实现了更可靠的粗粒度乐器检测。", "translation": "在音乐信息检索中，识别音频片段中的乐器活动至关重要，对音乐编目和发现具有重要意义。以往的音乐乐器识别深度学习研究主要强调数据充足的乐器类别。最近的研究表明，即使在乐器级别的细粒度标注有限的情况下，分层分类在管弦乐器活动检测中也具有适用性。本研究基于霍恩博斯特尔-萨克斯分类法，使用以其多样性和乐器、音乐流派丰富性而闻名的MedleyDB数据集评估了这种分层分类系统。这项工作提出了将分层结构整合到模型中的各种策略，并测试了一类新的分层音乐预测模型。本研究通过弥合详细乐器识别和组级别识别之间的差距，展示了更可靠的粗粒度乐器检测，为该领域的进一步发展铺平了道路。", "summary": "本论文提出了一种新的分层深度学习方法，用于解决音乐信息检索中少数乐器检测的数据稀缺问题。研究基于Hornbostel-Sachs分类系统，在MedleyDB数据集上评估了分层分类系统，并提出了将分层结构整合到模型中的策略。结果表明，该方法能够实现更可靠的粗粒度乐器检测，弥合了详细乐器识别和组级别识别之间的差距。", "keywords": "分层深度学习, 乐器检测, 音乐信息检索, Hornbostel-Sachs分类, MedleyDB数据集", "comments": "这项研究的创新之处在于提出了一种分层深度学习方法来解决音乐信息检索中少数乐器检测的数据稀缺问题。通过利用分层分类系统和整合分层结构到模型中，该研究在有限的细粒度标注下实现了更可靠的粗粒度乐器检测，对音乐编目和发现具有重要意义，并为该领域的进一步发展提供了新的方向。"}}
{"id": "2506.21487", "title": "OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits", "authors": ["Maryam Ghane", "Amir M. Hajisadeghi", "Hamid R. Zarandi"], "summary": "This paper presents OptGM, an optimized gate merging method designed to\nmitigate negative bias temperature instability (NBTI) in digital circuits.\nFirst, the proposed approach effectively identifies NBTI-critical internal\nnodes, defined as those with a signal probability exceeding a predefined\nthreshold. Next, based on the proposed optimized algorithm, the sensitizer gate\n(which drives the critical node) and the sensitive gate (which is fed by it)\nare merged into a new complex gate. This complex gate preserves the original\nlogic while eliminating NBTI-critical nodes. Finally, to evaluate the\neffectiveness of OptGM, we assess it on several combinational and sequential\nbenchmark circuits. Simulation results demonstrate that, on average, the number\nof NBTI-critical transistors (i.e., PMOS transistors connected to critical\nnodes), NBTI-induced delay degradation, and the total transistor count are\nreduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances\nperformance per cost (PPC) by 12.8% on average, with minimal area overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21487v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21487v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "OptGM：一种优化门合并方法以减轻数字电路中的NBTI效应", "tldr": "OptGM是一种优化的门合并方法，通过识别并合并NBTI关键节点相关的门，有效减轻数字电路中的NBTI效应，显著降低NBTI关键晶体管数量和延迟退化。", "motivation": "数字电路中负偏置温度不稳定性（NBTI）问题导致性能下降，需要一种有效的方法来减轻其影响。", "method": "OptGM首先识别信号概率超过预定义阈值的NBTI关键内部节点。然后，基于优化算法，将驱动关键节点的敏感门和被关键节点馈送的敏感门合并成一个新的复杂门，该复杂门保留原始逻辑并消除NBTI关键节点。", "result": "OptGM在组合和时序基准电路上进行评估。仿真结果显示，NBTI关键晶体管数量平均减少89.29%，NBTI引起的延迟退化平均减少23.87%，总晶体管数量平均减少6.47%。此外，性能成本（PPC）平均提高12.8%，面积开销极小。", "conclusion": "OptGM是一种有效的NBTI缓解方法，能够显著减少NBTI关键晶体管、降低延迟退化，并提高性能成本比，同时保持较低的面积开销。", "translation": "本文提出OptGM，一种优化的门合并方法，旨在减轻数字电路中的负偏置温度不稳定性（NBTI）。首先，所提出的方法有效识别NBTI关键内部节点，这些节点被定义为信号概率超过预定义阈值的节点。其次，基于所提出的优化算法，将敏感门（驱动关键节点的门）和被关键节点馈送的敏感门合并成一个新的复杂门。这个复杂门保留了原始逻辑，同时消除了NBTI关键节点。最后，为了评估OptGM的有效性，我们在几个组合和时序基准电路上对其进行了评估。仿真结果表明，NBTI关键晶体管（即连接到关键节点的PMOS晶体管）的数量、NBTI引起的延迟退化以及总晶体管数量平均分别减少了89.29%、23.87%和6.47%。此外，OptGM平均将性能成本（PPC）提高了12.8%，且面积开销极小。", "summary": "本文提出OptGM，一种优化的门合并方法，旨在减轻数字电路中的NBTI效应。该方法通过识别NBTI关键内部节点，并将驱动和被馈送这些关键节点的门合并成新的复杂门，从而消除关键节点并保持原有逻辑。实验结果表明，OptGM显著减少了NBTI关键晶体管数量、延迟退化和总晶体管数量，并提高了性能成本比，同时保持最小的面积开销。", "keywords": "NBTI, 门合并, 数字电路, 可靠性, 延迟退化", "comments": "该论文提出了一种新颖的门合并方法OptGM来解决数字电路中的NBTI效应。其创新点在于识别NBTI关键节点并进行有针对性的门合并，有效降低了NBTI关键晶体管数量和延迟退化，同时保持了较低的面积开销和提高了性能成本比，显示出其在可靠性设计中的重要性。"}}
{"id": "2506.20931", "title": "SPA: Towards More Stealth and Persistent Backdoor Attacks in Federated Learning", "authors": ["Chengcheng Zhu", "Ye Li", "Bosen Rao", "Jiale Zhang", "Yunlong Mao", "Sheng Zhong"], "summary": "Federated Learning (FL) has emerged as a leading paradigm for\nprivacy-preserving distributed machine learning, yet the distributed nature of\nFL introduces unique security challenges, notably the threat of backdoor\nattacks. Existing backdoor strategies predominantly rely on end-to-end label\nsupervision, which, despite their efficacy, often results in detectable feature\ndisentanglement and limited persistence. In this work, we propose a novel and\nstealthy backdoor attack framework, named SPA, which fundamentally departs from\ntraditional approaches by leveraging feature-space alignment rather than direct\ntrigger-label association. Specifically, SPA reduces representational distances\nbetween backdoor trigger features and target class features, enabling the\nglobal model to misclassify trigger-embedded inputs with high stealth and\npersistence. We further introduce an adaptive, adversarial trigger optimization\nmechanism, utilizing boundary-search in the feature space to enhance attack\nlongevity and effectiveness, even against defensive FL scenarios and non-IID\ndata distributions. Extensive experiments on various FL benchmarks demonstrate\nthat SPA consistently achieves high attack success rates with minimal impact on\nmodel utility, maintains robustness under challenging participation and data\nheterogeneity conditions, and exhibits persistent backdoor effects far\nexceeding those of conventional techniques. Our results call urgent attention\nto the evolving sophistication of backdoor threats in FL and emphasize the\npressing need for advanced, feature-level defense techniques.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2506.20931v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20931v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SPA：迈向联邦学习中更隐蔽和持久的后门攻击", "tldr": "本文提出了一种名为SPA的新型隐蔽后门攻击框架，通过特征空间对齐而非直接标签关联，实现联邦学习中更隐蔽、持久且对防御机制鲁棒的后门攻击。", "motivation": "联邦学习（FL）面临后门攻击的独特安全挑战。现有后门策略主要依赖端到端标签监督，但这会导致可检测的特征解耦和有限的持久性。", "method": "本文提出了一种名为SPA的新型隐蔽后门攻击框架。它通过利用特征空间对齐而非直接触发器-标签关联来减少后门触发器特征与目标类特征之间的表征距离，从而使全局模型能够以高隐蔽性和持久性错误分类嵌入触发器的输入。此外，还引入了一种自适应的对抗性触发器优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性，即使面对防御性FL场景和非IID数据分布也能奏效。", "result": "在各种FL基准上的大量实验表明，SPA始终以最小的模型效用影响实现高攻击成功率，在挑战性的参与和数据异构条件下保持鲁棒性，并展现出远超传统技术的持久后门效应。", "conclusion": "本文结果呼吁紧急关注FL中后门威胁日益增长的复杂性，并强调迫切需要先进的特征级防御技术。", "translation": "联邦学习（FL）已成为一种领先的保护隐私的分布式机器学习范式，然而FL的分布式特性带来了独特的安全挑战，特别是后门攻击的威胁。现有的后门策略主要依赖端到端标签监督，尽管它们有效，但通常会导致可检测的特征解耦和有限的持久性。在这项工作中，我们提出了一种新颖且隐蔽的后门攻击框架，名为SPA，它通过利用特征空间对齐而非直接触发器-标签关联，从根本上脱离了传统方法。具体来说，SPA减少了后门触发器特征和目标类特征之间的表征距离，使全局模型能够以高隐蔽性和持久性错误分类嵌入触发器的输入。我们进一步引入了一种自适应的对抗性触发器优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性，即使面对防御性FL场景和非IID数据分布也能奏效。在各种FL基准上的大量实验表明，SPA始终以最小的模型效用影响实现高攻击成功率，在挑战性的参与和数据异构条件下保持鲁棒性，并展现出远超传统技术的持久后门效应。我们的结果呼吁紧急关注FL中后门威胁日益增长的复杂性，并强调迫切需要先进的特征级防御技术。", "summary": "本文提出了一种名为SPA的新型后门攻击框架，旨在解决联邦学习中现有后门攻击可检测性高和持久性差的问题。SPA通过在特征空间中对齐后门触发器和目标类别特征，而非依赖传统的触发器-标签关联，实现了更高的隐蔽性和持久性。它还引入了自适应对抗性触发器优化机制，以增强攻击在防御和非IID数据环境下的鲁棒性。实验证明，SPA在保持模型效用的同时，能达到高攻击成功率并展现出卓越的持久性，揭示了联邦学习中后门威胁的复杂演变。", "keywords": "联邦学习, 后门攻击, 特征空间对齐, 隐蔽性, 持久性", "comments": "SPA通过引入特征空间对齐而非传统的标签关联，显著提升了联邦学习中后门攻击的隐蔽性和持久性，这是一种重要的创新。其对防御机制和非IID数据分布的鲁棒性也增强了其威胁性。这项工作的重要性在于它揭示了现有防御可能不足以应对更高级的特征级攻击，并强调了开发相应防御技术的紧迫性。"}}
{"id": "2506.21246", "title": "From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting", "authors": ["Giorgos Demosthenous", "Chryssis Georgiou", "Eliada Polydorou"], "summary": "This study investigates the impact of data source diversity on the\nperformance of cryptocurrency forecasting models by integrating various data\ncategories, including technical indicators, on-chain metrics, sentiment and\ninterest metrics, traditional market indices, and macroeconomic indicators. We\nintroduce the Crypto100 index, representing the top 100 cryptocurrencies by\nmarket capitalization, and propose a novel feature reduction algorithm to\nidentify the most impactful and resilient features from diverse data sources.\nOur comprehensive experiments demonstrate that data source diversity\nsignificantly enhances the predictive performance of forecasting models across\ndifferent time horizons. Key findings include the paramount importance of\non-chain metrics for both short-term and long-term predictions, the growing\nrelevance of traditional market indices and macroeconomic indicators for\nlonger-term forecasts, and substantial improvements in model accuracy when\ndiverse data sources are utilized. These insights help demystify the short-term\nand long-term driving factors of the cryptocurrency market and lay the\ngroundwork for developing more accurate and resilient forecasting models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21246v1", "categories": ["q-fin.PM", "cs.AI", "cs.ET", "cs.LG", "q-fin.ST"], "cate": "q-fin.PM", "url": "http://arxiv.org/abs/2506.21246v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从链上到宏观：评估数据源多样性在加密货币市场预测中的重要性", "tldr": "本研究探讨了数据源多样性如何显著提升加密货币市场预测模型的性能，发现链上指标至关重要，传统市场和宏观指标对长期预测日益相关。", "motivation": "旨在通过整合多种数据类别，探究数据源多样性对加密货币预测模型性能的影响，并揭示加密货币市场短期和长期驱动因素。", "method": "整合了技术指标、链上指标、情绪和兴趣指标、传统市场指数以及宏观经济指标等多种数据类别。引入了Crypto100指数，并提出了一种新的特征降维算法来识别最具影响力且稳健的特征。", "result": "数据源多样性显著提升了不同时间跨度下预测模型的性能。链上指标对短期和长期预测都至关重要。传统市场指数和宏观经济指标对长期预测的相关性日益增长。利用多样化数据源可显著提高模型准确性。", "conclusion": "数据源多样性对于构建更准确、更具韧性的加密货币市场预测模型至关重要，并且有助于理解市场驱动因素。", "translation": "本研究通过整合多种数据类别，包括技术指标、链上指标、情绪和兴趣指标、传统市场指数以及宏观经济指标，调查了数据源多样性对加密货币预测模型性能的影响。我们引入了代表市值前100种加密货币的Crypto100指数，并提出了一种新颖的特征降维算法，以从多样化的数据源中识别出最具影响力且稳健的特征。我们的综合实验表明，数据源多样性显著提升了不同时间跨度下预测模型的预测性能。主要发现包括：链上指标对短期和长期预测都至关重要；传统市场指数和宏观经济指标对长期预测的相关性日益增长；以及利用多样化数据源可显著提高模型准确性。这些见解有助于揭示加密货币市场的短期和长期驱动因素，并为开发更准确、更具韧性的预测模型奠定了基础。", "summary": "本研究探讨了数据源多样性对加密货币市场预测模型性能的重要性。通过整合链上数据、宏观经济指标等多种数据，并引入Crypto100指数及新型特征降维算法，实验证明数据源多样性显著提升了预测准确性。研究强调了链上指标的关键作用以及传统市场和宏观指标对长期预测日益增长的相关性，为构建更稳健的预测模型提供了基础。", "keywords": "加密货币预测, 数据源多样性, 链上指标, 宏观经济指标, 特征降维", "comments": "这篇论文的创新点在于系统地评估了不同类型数据源（特别是链上和宏观数据）对加密货币预测的综合影响，并提出了Crypto100指数和特征降维算法。其重要性在于为开发更全面、更准确的加密货币预测模型提供了实证依据和方法论。"}}
{"id": "2506.20980", "title": "Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs", "authors": ["Ziyu Zheng", "Yaming Yang", "Ziyu Guan", "Wei Zhao", "Weigang Lu"], "summary": "Real-world networks usually have a property of node heterophily, that is, the\nconnected nodes usually have different features or different labels. This\nheterophily issue has been extensively studied in homogeneous graphs but\nremains under-explored in heterogeneous graphs, where there are multiple types\nof nodes and edges. Capturing node heterophily in heterogeneous graphs is very\nchallenging since both node/edge heterogeneity and node heterophily should be\ncarefully taken into consideration. Existing methods typically convert\nheterogeneous graphs into homogeneous ones to learn node heterophily, which\nwill inevitably lose the potential heterophily conveyed by heterogeneous\nrelations. To bridge this gap, we propose Relation-Aware Separation of\nHomophily and Heterophily (RASH), a novel contrastive learning framework that\nexplicitly models high-order semantics of heterogeneous interactions and\nadaptively separates homophilic and heterophilic patterns. Particularly, RASH\nintroduces dual heterogeneous hypergraphs to encode multi-relational bipartite\nsubgraphs and dynamically constructs homophilic graphs and heterophilic graphs\nbased on relation importance. A multi-relation contrastive loss is designed to\nalign heterogeneous and homophilic/heterophilic views by maximizing mutual\ninformation. In this way, RASH simultaneously resolves the challenges of\nheterogeneity and heterophily in heterogeneous graphs. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of RASH across various\ndownstream tasks. The code is available at:\nhttps://github.com/zhengziyu77/RASH.", "comment": "accepted by KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.20980v1", "categories": ["cs.SI", "cs.AI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20980v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "增强同质性-异质性分离：异构图中的关系感知学习", "tldr": "RASH是一个新的对比学习框架，用于在异构图中有效分离同质性和异质性模式，通过双重异构超图和多关系对比损失来解决异构性和异质性挑战，并在基准数据集上表现出色。", "motivation": "真实世界网络存在节点异质性，即相连节点特征或标签不同。同质图中的异质性问题已广泛研究，但在异构图（包含多种节点和边类型）中仍未充分探索。现有方法通常将异构图转换为同质图来学习节点异质性，但这会丢失异构关系所蕴含的潜在异质性。因此，需要一种能够同时考虑节点/边异构性和节点异质性的方法来捕获异构图中的节点异质性。", "method": "本文提出了关系感知同质性与异质性分离（RASH）框架，这是一种新颖的对比学习方法。RASH通过引入双重异构超图来编码多关系二分图，并基于关系重要性动态构建同质图和异质图。此外，设计了一种多关系对比损失，通过最大化互信息来对齐异构视图以及同质/异质视图，从而显式建模异构交互的高阶语义并自适应地分离同质和异质模式。", "result": "RASH框架同时解决了异构图中的异构性和异质性挑战。在基准数据集上进行的广泛实验证明了RASH在各种下游任务中的有效性。", "conclusion": "RASH通过其新颖的关系感知对比学习框架，成功地在异构图中实现了同质性与异质性的有效分离，解决了异构性和异质性共存的挑战，并在多个下游任务中取得了显著的性能提升。", "translation": "真实世界网络通常具有节点异质性，即相连的节点通常具有不同的特征或不同的标签。这种异质性问题已在同质图中得到广泛研究，但在异构图（其中存在多种类型的节点和边）中仍未充分探索。在异构图中捕获节点异质性非常具有挑战性，因为节点/边异构性和节点异质性都应仔细考虑。现有方法通常将异构图转换为同质图来学习节点异质性，这不可避免地会丢失异构关系所传达的潜在异质性。为了弥补这一差距，我们提出了关系感知同质性与异质性分离（RASH），这是一种新颖的对比学习框架，它明确地建模异构交互的高阶语义并自适应地分离同质和异质模式。特别是，RASH引入了双重异构超图来编码多关系二分图，并根据关系重要性动态构建同质图和异质图。设计了一种多关系对比损失，通过最大化互信息来对齐异构视图以及同质/异质视图。通过这种方式，RASH同时解决了异构图中异构性和异质性的挑战。在基准数据集上进行的广泛实验证明了RASH在各种下游任务中的有效性。代码可在：https://github.com/zhengziyu77/RASH 获取。", "summary": "本文提出RASH（关系感知同质性与异质性分离），一个针对异构图的对比学习框架，旨在解决异构图中节点异质性捕获的挑战。RASH通过引入双重异构超图和设计多关系对比损失，有效建模异构交互的高阶语义，并自适应地分离同质和异质模式，从而同时处理异构性和异质性问题。实验证明其在多个下游任务上的有效性。", "keywords": "异构图, 节点异质性, 对比学习, 同质性分离, 关系感知", "comments": "该论文提出了一种新颖的对比学习方法RASH，用于解决异构图中同质性与异质性分离的难题。其创新点在于引入双重异构超图来编码多关系信息，并动态构建同质/异质图，同时设计了多关系对比损失来最大化互信息，有效避免了现有方法在异构到同质转换中信息丢失的问题。这项工作对于理解和处理复杂异构网络中的异质性模式具有重要意义，并为相关下游任务提供了强大的工具。"}}
{"id": "2506.20869", "title": "Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation", "authors": ["Md Toufique Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Ayman Asad Khan", "Mika Saari", "Pekka Abrahamsson"], "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.", "comment": "Accepted as a full paper to the 51st Euromicro Conference on Software\n  Engineering and Advanced Applications (SEAA 2025). 9 pages, 4 figures. This\n  is the preprint version and not the final camera ready version", "pdf_url": "http://arxiv.org/pdf/2506.20869v1", "categories": ["cs.SE", "cs.AI", "cs.IR", "D.2.11; I.2.6; H.3.3"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20869v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "为实际应用设计、开发和评估RAG系统", "tldr": "本文介绍了为实际场景开发的五个RAG应用，评估了用户反馈，并总结了12条关于RAG系统在实际应用中的经验教训。", "motivation": "RAG系统是LLM结合外部知识的关键方法，但目前缺乏基于真实用例、通过用户评估且系统记录经验的实证研究。本文旨在填补这一空白，通过开发和评估实际RAG应用来提供经验。", "method": "开发了五个针对治理、网络安全、农业、工业研究和医学诊断等领域特定真实场景的RAG应用。每个系统都包含多语言OCR、通过向量嵌入的语义检索和领域适应的LLM，并通过本地服务器或云API部署。通过包含100名参与者的网络评估，从六个维度（易用性、相关性、透明度、响应性、准确性和推荐可能性）评估了系统。根据用户反馈和开发经验，记录了12条关键经验教训。", "result": "开发了五个领域特定的RAG应用，并成功进行了包含100名参与者的网络评估，评估了系统在六个维度上的表现。研究总结了12条关键经验教训，强调了影响RAG系统在实践中可靠性和可用性的技术、操作和道德挑战。", "conclusion": "本文通过开发和评估真实的RAG系统，提供了宝贵的实证数据和经验教训，揭示了在实际应用中部署RAG系统所面临的挑战，并为未来的RAG系统工程提供了指导和实践见解。", "translation": "检索增强生成（RAG）系统正成为将大型语言模型（LLM）与外部知识相结合的关键方法，以解决事实准确性和上下文相关性方面的局限性。然而，目前缺乏基于真实用例、通过普通用户参与评估并伴有系统性经验教训记录的RAG实现开发方面的实证研究。本文介绍了为治理、网络安全、农业、工业研究和医学诊断等真实场景开发的五个领域特定RAG应用。每个系统都集成了多语言OCR、通过向量嵌入的语义检索和领域适应的LLM，并通过本地服务器或云API部署以满足不同的用户需求。一项涉及总共100名参与者的基于网络的评估从六个维度对系统进行了评估：(i)易用性、(ii)相关性、(iii)透明度、(iv)响应性、(v)准确性，以及(vi)推荐可能性。根据用户反馈和我们的开发经验，我们记录了十二条关键经验教训，强调了影响RAG系统在实践中可靠性和可用性的技术、操作和道德挑战。", "summary": "本文介绍了为治理、网络安全、农业、工业研究和医学诊断等五个真实场景开发的RAG系统。这些系统集成了多语言OCR、向量嵌入的语义检索和领域适应的LLM。通过100名用户参与的在线评估，从易用性、相关性、透明度、响应性、准确性和推荐可能性六个维度对系统进行了评估。研究总结了12条关键经验教训，涵盖了RAG系统在实际应用中面临的技术、操作和道德挑战，为RAG系统的实际部署提供了宝贵的见解。", "keywords": "RAG系统, 大型语言模型, 真实应用, 用户评估, 经验教训", "comments": "本文的创新之处在于其强调了RAG系统在真实世界应用中的工程实践和用户评估，填补了现有研究中缺乏实证数据的空白。通过跨多个领域的具体案例和系统性的经验教训总结，为RAG系统的设计、开发和评估提供了宝贵的实践指导。特别重要的是，它不仅关注技术挑战，还触及了操作和道德层面的问题，这对于RAG系统的实际落地具有重要意义。"}}
{"id": "2506.20954", "title": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard Sensing", "authors": ["Xueming Liu", "Lin Li", "Xiang Zhou", "Qingrui Zhang", "Tianjiang Hu"], "summary": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations.", "comment": "8 Pages, 7 figures. Accepted by RA-L", "pdf_url": "http://arxiv.org/pdf/2506.20954v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20954v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多旋翼系统基于机载感知的协同环绕飞行", "tldr": "本文提出了一种多旋翼系统协同环绕框架，利用机载感知技术实现对移动目标的跟踪和包围，无需外部定位系统，并在遮挡环境下表现出鲁棒性和容错性。", "motivation": "旨在解决多旋翼系统在不依赖外部定位系统的情况下，对移动目标进行协同环绕跟踪的挑战，特别是在视觉遮挡和系统部分故障的复杂环境中。", "method": "采用异构感知策略评估旋翼-旋翼和旋翼-目标交互关系；开发改进的卡尔曼滤波器融合视觉-惯性里程计和距离测量以提高旋翼间相对定位精度；设计事件触发的分布式卡尔曼滤波器结合邻居测量和估计的相对位置，在视觉遮挡下实现鲁棒的目标状态估计；构建基于振荡器自主编队飞行策略的协同环绕控制器。", "result": "通过广泛的室内外实验验证了所提出环绕框架在遮挡环境中的高效性；通过旋翼故障实验，突出显示了该框架固有的容错特性。", "conclusion": "所提出的协同环绕框架能够使多旋翼系统在不依赖外部定位系统的情况下，有效且鲁棒地包围和跟踪移动目标，即使在视觉遮挡和部分系统故障的情况下也能保持性能，展现了在搜救行动中的潜在应用价值。", "translation": "本文提出了一种多旋翼系统协同环绕框架，用于包围和跟踪移动目标，无需依赖外部定位系统。利用异构感知策略和相应的状态估计算法，评估了旋翼-旋翼和旋翼-目标交互之间的独特关系。开发了一种改进的卡尔曼滤波器，融合视觉-惯性里程计与距离测量，以提高旋翼间相对定位的精度。设计了一种事件触发的分布式卡尔曼滤波器，通过结合邻居测量和估计的旋翼间相对位置，在视觉遮挡下实现鲁棒的目标状态估计。利用估计结果，构建了一个协同环绕控制器，利用基于振荡器的自主编队飞行策略。我们进行了广泛的室内外实验，以验证所提出的环绕框架在遮挡环境中的效率。此外，一项旋翼故障实验突出了所提出框架固有的容错特性，强调了其在搜救行动中的部署潜力。", "summary": "本文提出了一种创新的多旋翼系统协同环绕框架，该框架不依赖外部定位系统，通过异构感知、改进的卡尔曼滤波器和事件触发的分布式卡尔曼滤波器实现对移动目标的鲁棒跟踪和包围。它利用基于振荡器的编队策略进行控制，并通过实验证明了其在遮挡环境下的高效性和固有的容错性，预示了其在搜救等领域的应用前景。", "keywords": "多旋翼系统, 协同环绕, 机载感知, 卡尔曼滤波, 容错性", "comments": "本文的创新之处在于提出了一种不依赖外部定位的协同环绕框架，并在遮挡环境下验证了其鲁棒性。特别是在状态估计方面，结合了改进的卡尔曼滤波和事件触发的分布式卡尔曼滤波，有效处理了相对定位和目标估计问题。此外，其展示的容错性增加了实际部署的可靠性，对于搜救等应用具有重要意义。"}}
{"id": "2506.21195", "title": "Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact", "authors": ["Neha Raghuvanshi"], "summary": "Have you wondered how cross-functional teams balance between maximizing value\nthat users derive and business growth leading to win-win situations? This case\nstudy shows how User Experience Research (UXR) and Data Science teams used\nmixed methods research to strategically influence Product Led Growth (PLG) for\na Password Manager used by million+ users, thus allowing our users, internal\nteams, and business to win. The audience will take away practical\nlessons/techniques related to leveraging mixed methods to: a. Maximize user\nvalue while meeting business growth goals b. Influence cross-functional teams\nc. Measure user and business impact This case study can be easily tied to the\nUXR Point of view pyramid (POV) [2] that represents a methodological approach\nto construct a POV and further dives into actioning POV to create measurable\nuser and business impact.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21195v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21195v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "有意义地跟随用户，产品增长随之而来：一项将用户体验视角与增长联系起来并产生可衡量影响的混合方法案例研究", "tldr": "本案例研究展示了用户体验研究（UXR）和数据科学团队如何利用混合方法研究，战略性地影响产品主导增长（PLG），从而实现用户价值最大化和业务增长的平衡。", "motivation": "跨职能团队如何在最大化用户价值和实现业务增长之间取得平衡，从而实现双赢局面？", "method": "用户体验研究（UXR）和数据科学团队对一个拥有百万级用户的密码管理器采用了混合方法研究，并结合用户体验视角金字塔（POV）的方法论，以战略性地影响产品主导增长（PLG）。", "result": "通过所采用的方法，用户、内部团队和业务都获得了成功，实现了用户价值和业务影响的可衡量。", "conclusion": "本案例研究提供了利用混合方法来最大化用户价值同时实现业务增长目标、影响跨职能团队以及衡量用户和业务影响的实用经验和技术。", "translation": "您是否曾好奇跨职能团队如何在最大化用户所获得价值和业务增长之间取得平衡，从而实现双赢局面？本案例研究展示了用户体验研究（UXR）和数据科学团队如何使用混合方法研究，战略性地影响一个拥有百万级用户密码管理器的产品主导增长（PLG），从而让我们的用户、内部团队和业务都获得成功。受众将从中获得利用混合方法的实用经验/技术，以：a. 在满足业务增长目标的同时最大化用户价值 b. 影响跨职能团队 c. 衡量用户和业务影响。本案例研究可以很容易地与用户体验研究视角金字塔（POV）[2]联系起来，该金字塔代表了一种构建视角的方法论，并进一步深入探讨如何将视角付诸行动以创造可衡量的用户和业务影响。", "summary": "本案例研究探讨了跨职能团队如何通过结合用户体验研究（UXR）和数据科学的混合方法，在一个拥有百万级用户的密码管理器中战略性地推动产品主导增长（PLG）。研究展示了如何平衡用户价值与业务增长，实现用户、团队和业务的三方共赢，并提供了衡量用户和业务影响的实用技术和经验，这些都与用户体验研究视角金字塔（POV）的方法论紧密关联。", "keywords": "混合方法, 用户体验研究（UXR）, 产品主导增长（PLG）, 用户价值, 业务增长", "comments": "本研究的创新之处在于它将用户体验视角（UX POV）与实际的产品增长和可衡量的业务影响相结合，通过混合方法案例研究的形式提供了实用的指导。其重要性在于为跨职能团队提供了一个清晰的框架，以平衡用户价值和业务目标，这在当今以用户为中心的产品开发中至关重要。研究强调了用户体验研究和数据科学在实现双赢局面中的关键作用。"}}
{"id": "2506.21134", "title": "Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations", "authors": ["Jacopo Bufalino", "Jose Luis Martin-Navarro", "Mario Di Francesco", "Tuomas Aura"], "summary": "Kubernetes has emerged as the de facto standard for container orchestration.\nUnfortunately, its increasing popularity has also made it an attractive target\nfor malicious actors. Despite extensive research on securing Kubernetes, little\nattention has been paid to the impact of network configuration on the security\nof application deployments. This paper addresses this gap by conducting a\ncomprehensive analysis of network misconfigurations in a Kubernetes cluster\nwith specific reference to lateral movement. Accordingly, we carried out an\nextensive evaluation of 287 open-source applications belonging to six different\norganizations, ranging from IT companies and public entities to non-profits. As\na result, we identified 634 misconfigurations, well beyond what could be found\nby solutions in the state of the art. We responsibly disclosed our findings to\nthe concerned organizations and engaged in a discussion to assess their\nseverity. As of now, misconfigurations affecting more than thirty applications\nhave been fixed with the mitigations we proposed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21134v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21134v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "内部工作：防御Kubernetes集群的网络错误配置", "tldr": "研究发现Kubernetes集群中普遍存在网络错误配置，可能导致横向移动攻击，并提出了修复方案。", "motivation": "尽管Kubernetes安全性研究广泛，但对网络配置对应用部署安全的影响关注不足，特别是横向移动方面。", "method": "对来自六个不同组织的287个开源应用程序进行了广泛评估，以分析Kubernetes集群中的网络错误配置。", "result": "识别出634个网络错误配置，远超现有解决方案的发现能力；向相关组织披露了发现，并已修复了30多个受影响应用的错误配置。", "conclusion": "通过分析和披露，证明了网络错误配置在Kubernetes集群中的普遍性和严重性，并成功推动了部分修复。", "translation": "Kubernetes已成为容器编排的事实标准。不幸的是，其日益增长的普及也使其成为恶意行为者的诱人目标。尽管对保护Kubernetes进行了广泛研究，但很少关注网络配置对应用程序部署安全的影响。本文通过对Kubernetes集群中的网络错误配置进行全面分析，特别是针对横向移动，解决了这一空白。因此，我们对来自六个不同组织（从IT公司和公共实体到非营利组织）的287个开源应用程序进行了广泛评估。结果，我们识别出634个错误配置，远远超出了现有解决方案所能发现的范围。我们负责任地向相关组织披露了我们的发现，并进行了讨论以评估其严重性。截至目前，影响30多个应用程序的错误配置已通过我们提出的缓解措施得到修复。", "summary": "本文针对Kubernetes集群中网络配置对应用安全的影响这一研究空白，特别是横向移动风险，进行了全面分析。通过评估287个开源应用，发现了634个网络错误配置，并成功推动了其中30多个应用的修复，强调了网络配置在Kubernetes安全中的关键作用。", "keywords": "Kubernetes安全, 网络错误配置, 横向移动, 容器编排, 应用安全", "comments": "这篇论文通过对实际开源应用的广泛分析，揭示了Kubernetes网络配置中普遍存在的安全漏洞，这对于提升Kubernetes集群的实际安全性具有重要意义。其负责任的披露和推动修复的行动也值得肯定。"}}
{"id": "2506.21370", "title": "Cluster-Aware Two-Stage Method for Fast Iterative MIMO Detection in LEO Satellite Communications", "authors": ["Jiuyu Liu", "Yi Ma", "Qihao Peng", "Rahim Tafazolli"], "summary": "In this paper, a cluster-aware two-stage multiple-input multiple-output\n(MIMO) detection method is proposed for direct-to-cell satellite\ncommunications. The method achieves computational efficiency by exploiting a\ndistinctive property of satellite MIMO channels: users within the same\ngeographical cluster exhibit highly correlated channel characteristics due to\ntheir physical proximity, which typically impedes convergence in conventional\niterative MIMO detectors. The proposed method implements a two-stage strategy\nthat first eliminates intra-cluster interference using computationally\nefficient small matrix inversions, then utilizes these pre-computed matrices to\naccelerate standard iterative MIMO detectors such as Gauss-Seidel (GS) and\nsymmetric successive over-relaxation (SSOR) for effective inter-cluster\ninterference cancellation. Computer simulations demonstrate that the proposed\nmethod achieves more than 12 times faster convergence under perfect channel\nstate information. Even when accounting for channel estimation errors, the\nmethod maintains 9 times faster convergence, demonstrating its robustness and\neffectiveness for next-generation satellite MIMO communications.", "comment": "This work has been accepted by IEEE/CIC ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2506.21370v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21370v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "考虑簇的双阶段方法，用于LEO卫星通信中的快速迭代MIMO检测", "tldr": "本文提出了一种考虑簇的双阶段MIMO检测方法，用于LEO卫星通信，通过利用卫星MIMO信道的特性，显著加速了迭代检测器的收敛，即使存在信道估计误差也表现出鲁棒性。", "motivation": "传统迭代MIMO检测器在卫星MIMO信道中，由于同一地理簇内用户信道特性高度相关，通常会阻碍收敛。本文旨在利用这一特性，提高计算效率并加速收敛。", "method": "本文提出了一种双阶段策略。第一阶段，通过计算高效的小矩阵求逆来消除簇内干扰；第二阶段，利用这些预计算的矩阵加速标准迭代MIMO检测器（如高斯-赛德尔和对称逐次超松弛），以有效消除簇间干扰。", "result": "计算机仿真表明，在完美信道状态信息下，所提方法实现了超过12倍的收敛速度提升。即使考虑信道估计误差，该方法也能保持9倍的收敛速度提升。", "conclusion": "所提出的方法通过利用卫星MIMO信道的特性，有效解决了传统迭代MIMO检测器在收敛方面的挑战，表现出鲁棒性和有效性，适用于下一代卫星MIMO通信。", "translation": "本文提出了一种考虑簇的双输入多输出（MIMO）检测方法，用于直连蜂窝卫星通信。该方法通过利用卫星MIMO信道的独特特性来实现计算效率：同一地理簇内的用户由于物理接近而表现出高度相关的信道特性，这通常会阻碍传统迭代MIMO检测器的收敛。所提出的方法采用双阶段策略，首先使用计算高效的小矩阵求逆消除簇内干扰，然后利用这些预计算的矩阵加速标准迭代MIMO检测器，如高斯-赛德尔（GS）和对称逐次超松弛（SSOR），以有效消除簇间干扰。计算机仿真表明，在完美信道状态信息下，所提出的方法实现了超过12倍的收敛速度。即使考虑到信道估计误差，该方法也能保持9倍的收敛速度，这表明其在下一代卫星MIMO通信中的鲁棒性和有效性。", "summary": "本文针对LEO卫星通信中的MIMO检测，提出了一种考虑簇的双阶段方法。该方法利用卫星MIMO信道中同簇用户信道高度相关的特性，首先通过小矩阵求逆消除簇内干扰，然后利用预计算的矩阵加速标准迭代检测器来消除簇间干扰。仿真结果显示，该方法在完美信道信息下收敛速度提升12倍以上，在存在信道估计误差时也能提升9倍，展现了其在下一代卫星MIMO通信中的高效性和鲁棒性。", "keywords": "MIMO检测, LEO卫星通信, 迭代检测, 双阶段方法, 簇感知", "comments": "该论文的创新点在于提出了一个双阶段的MIMO检测方法，巧妙地利用了卫星MIMO信道中同簇信道相关的特性。通过将干扰消除分解为簇内和簇间两个阶段，并引入小矩阵求逆来提高计算效率，有效解决了传统迭代检测器在收敛性上的问题。其在收敛速度上的显著提升，即使在非理想条件下也保持了良好性能，显示了其在实际低轨卫星MIMO通信系统中的应用潜力。"}}
{"id": "2506.20938", "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks", "authors": ["Joshua H. Davis", "Daniel Nichols", "Ishan Khillan", "Abhinav Bhatele"], "summary": "GPGPU architectures have become significantly diverse in recent years, which\nhas led to an emergence of a variety of specialized programming models and\nsoftware stacks to support them. While portable execution models exist, they\nstill require significant developer effort to port to and optimize for\ndifferent hardware architectures. Recent advances in large language models\n(LLMs) can help us reduce some of this programmer burden. In this paper, we\npresent a novel benchmark and testing framework, ParEval-Repo, which can be\nused to evaluate the efficacy of LLM-based approaches in automatically\ntranslating entire codebases across GPGPU execution models. ParEval-Repo\nincludes several scientific computing and AI mini-applications in a range of\nprogramming models, and levels of repository complexity. We use ParEval-Repo to\nevaluate a range of state-of-the-art open-source and commercial LLMs, with both\na non-agentic and a top-down agentic approach. We assess code generated by the\nLLMs and approaches in terms of compilability, functional correctness,\ncategories of build errors, and the cost of translation in terms of the number\nof inference tokens. Our results demonstrate that LLM translation of scientific\napplications is feasible for small programs but difficulty with generating\nfunctional build systems and cross-file dependencies pose challenges in scaling\nto larger codebases.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20938v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20938v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ParEval-Repo: 一个用于评估大型语言模型在仓库级高性能计算翻译任务中的基准套件", "tldr": "ParEval-Repo是一个新基准，用于评估LLM自动翻译GPGPU代码库的有效性。研究发现LLM对小型程序翻译可行，但处理大型代码库的构建系统和跨文件依赖存在困难。", "motivation": "近年来，GPGPU架构日益多样化，导致出现了多种专门的编程模型和软件栈。尽管存在可移植的执行模型，但将代码移植并优化到不同硬件架构仍需要大量开发人员的努力。大型语言模型的最新进展有望减轻部分程序员负担。", "method": "本文提出了一个名为ParEval-Repo的新型基准和测试框架，用于评估基于LLM的方法在自动翻译整个GPGPU执行模型代码库方面的有效性。ParEval-Repo包含多个科学计算和AI迷你应用程序，涵盖多种编程模型和仓库复杂性级别。研究使用ParEval-Repo评估了一系列最先进的开源和商业LLM，包括非代理和自顶向下代理方法。评估了LLM生成的代码在可编译性、功能正确性、构建错误类别以及翻译成本（推理令牌数量）方面的表现。", "result": "研究结果表明，LLM翻译科学应用程序对于小型程序是可行的，但生成功能性构建系统和处理跨文件依赖方面的困难，给扩展到更大的代码库带来了挑战。", "conclusion": "尽管大型语言模型在小型程序的高性能计算代码翻译方面显示出潜力，但它们在处理复杂构建系统和跨文件依赖方面的局限性，使得将这些技术扩展到大型、实际的代码库仍然是一个重大挑战。", "translation": "近年来，GPGPU架构变得日益多样化，这导致了各种专门的编程模型和软件栈的出现以支持它们。尽管存在可移植的执行模型，但它们仍然需要开发人员付出巨大的努力才能移植到不同的硬件架构并进行优化。大型语言模型（LLM）的最新进展可以帮助我们减轻程序员的这种负担。在本文中，我们提出了一个新的基准和测试框架ParEval-Repo，它可以用于评估基于LLM的方法在自动翻译整个GPGPU执行模型代码库方面的有效性。ParEval-Repo包含一系列编程模型和仓库复杂性级别的多个科学计算和AI迷你应用程序。我们使用ParEval-Repo评估了一系列最先进的开源和商业LLM，采用了非代理和自顶向下代理两种方法。我们从可编译性、功能正确性、构建错误类别以及翻译成本（推理令牌数量）方面评估了LLM和方法生成的代码。我们的结果表明，LLM翻译科学应用程序对于小型程序是可行的，但生成功能性构建系统和跨文件依赖方面的困难给扩展到更大的代码库带来了挑战。", "summary": "本文介绍了ParEval-Repo，一个用于评估大型语言模型（LLM）在GPGPU代码库翻译任务中表现的新基准套件。该基准包含不同复杂度的科学计算和AI迷你应用程序。研究使用ParEval-Repo评估了多种LLM，并发现LLM在翻译小型程序方面是可行的，但在处理功能性构建系统和跨文件依赖时面临挑战，这限制了它们扩展到大型代码库的能力。", "keywords": "LLM, 代码翻译, GPGPU, 基准测试, 高性能计算", "comments": "ParEval-Repo的创新之处在于它提供了一个专门的基准，用于评估LLM在实际且复杂的仓库级高性能计算代码翻译任务中的能力。它关注的难点，如构建系统和跨文件依赖，是代码翻译中真实存在的挑战，也揭示了当前LLM在处理这类复杂工程任务时的局限性。这项工作为未来LLM在软件工程领域的应用提供了重要的方向和改进空间。"}}
{"id": "2506.20743", "title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "summary": "Foundation models (FMs) are catalyzing a transformative shift in materials\nscience (MatSci) by enabling scalable, general-purpose, and multimodal AI\nsystems for scientific discovery. Unlike traditional machine learning models,\nwhich are typically narrow in scope and require task-specific engineering, FMs\noffer cross-domain generalization and exhibit emergent capabilities. Their\nversatility is especially well-suited to materials science, where research\nchallenges span diverse data types and scales. This survey provides a\ncomprehensive overview of foundation models, agentic systems, datasets, and\ncomputational tools supporting this growing field. We introduce a task-driven\ntaxonomy encompassing six broad application areas: data extraction,\ninterpretation and Q\\&A; atomistic simulation; property prediction; materials\nstructure, design and discovery; process planning, discovery, and optimization;\nand multiscale modeling. We discuss recent advances in both unimodal and\nmultimodal FMs, as well as emerging large language model (LLM) agents.\nFurthermore, we review standardized datasets, open-source tools, and autonomous\nexperimental platforms that collectively fuel the development and integration\nof FMs into research workflows. We assess the early successes of foundation\nmodels and identify persistent limitations, including challenges in\ngeneralizability, interpretability, data imbalance, safety concerns, and\nlimited multimodal fusion. Finally, we articulate future research directions\ncentered on scalable pretraining, continual learning, data governance, and\ntrustworthiness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20743v1", "categories": ["cs.LG", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20743v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "人工智能在材料科学中的应用综述：基础模型、LLM智能体、数据集和工具", "tldr": "本综述全面概述了基础模型、LLM智能体、数据集和工具如何推动材料科学领域的发展，并讨论了其应用、挑战和未来方向。", "motivation": "基础模型正在通过实现可扩展、通用和多模态的AI系统，催化材料科学领域的变革。本综述旨在全面概述支持这一新兴领域的基础模型、智能体系统、数据集和计算工具。", "method": "本综述提供了一个全面的概述，引入了一个任务驱动的分类法，涵盖了六个广泛的应用领域，并讨论了单模态和多模态基础模型以及新兴的大型语言模型（LLM）智能体的最新进展。此外，还回顾了标准化数据集、开源工具和自主实验平台。", "result": "本综述评估了基础模型的早期成功，并指出了持续存在的局限性，包括泛化能力、可解释性、数据不平衡、安全问题和有限的多模态融合方面的挑战。", "conclusion": "未来研究方向将集中在可扩展的预训练、持续学习、数据治理和可信度上。", "translation": "基础模型（FMs）正在通过实现可扩展、通用和多模态的AI系统，催化材料科学（MatSci）领域的变革。与传统上范围狭窄且需要针对特定任务进行工程设计的机器学习模型不同，基础模型提供跨领域泛化能力并展现出涌现能力。它们的通用性特别适合材料科学，因为该领域的研究挑战涵盖了多样化的数据类型和规模。本综述全面概述了支持这一新兴领域的基础模型、智能体系统、数据集和计算工具。我们引入了一个任务驱动的分类法，涵盖六个广泛的应用领域：数据提取、解释和问答；原子模拟；属性预测；材料结构、设计和发现；工艺规划、发现和优化；以及多尺度建模。我们讨论了单模态和多模态基础模型以及新兴的大型语言模型（LLM）智能体的最新进展。此外，我们回顾了标准化数据集、开源工具和自主实验平台，它们共同推动了基础模型在研究工作流程中的开发和集成。我们评估了基础模型的早期成功，并指出了持续存在的局限性，包括泛化能力、可解释性、数据不平衡、安全问题和有限的多模态融合方面的挑战。最后，我们阐明了以可扩展预训练、持续学习、数据治理和可信度为中心的未来研究方向。", "summary": "本综述全面探讨了人工智能，特别是基础模型和大型语言模型（LLM）智能体，在材料科学领域的应用与影响。文章首先阐述了基础模型在材料科学中实现可扩展、通用和多模态AI系统的变革潜力。随后，提出了一个任务驱动的分类法，将应用划分为六个核心领域。文中详细讨论了单模态和多模态基础模型以及LLM智能体的最新进展，并回顾了相关数据集、开源工具和自主实验平台。最后，评估了基础模型的初期成功，指出了泛化、可解释性、数据不平衡、安全和多模态融合等方面的局限性，并展望了未来在可扩展预训练、持续学习、数据治理和可信度等方向的研究。", "keywords": "材料科学, 人工智能, 基础模型, LLM智能体, 综述", "comments": "这篇综述及时地总结了AI，特别是基础模型和LLM在材料科学中的应用现状，为研究人员提供了全面的视角。其创新之处在于提出了任务驱动的分类法，有助于系统理解该领域的应用范围。重要性体现在它不仅指出了现有技术的潜力，也坦诚地揭示了当前面临的挑战和局限性，为未来的研究指明了方向。"}}
{"id": "2506.20701", "title": "Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models", "authors": ["Vineet Jain", "Kusha Sareen", "Mohammad Pedramfar", "Siamak Ravanbakhsh"], "summary": "Adapting a pretrained diffusion model to new objectives at inference time\nremains an open problem in generative modeling. Existing steering methods\nsuffer from inaccurate value estimation, especially at high noise levels, which\nbiases guidance. Moreover, information from past runs is not reused to improve\nsample quality, resulting in inefficient use of compute. Inspired by the\nsuccess of Monte Carlo Tree Search, we address these limitations by casting\ninference-time alignment as a search problem that reuses past computations. We\nintroduce a tree-based approach that samples from the reward-aligned target\ndensity by propagating terminal rewards back through the diffusion chain and\niteratively refining value estimates with each additional generation. Our\nproposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact\nsamples from the target distribution in the limit of infinite rollouts, and its\ngreedy variant, Diffusion Tree Search (DTS$^\\star$), performs a global search\nfor high reward samples. On MNIST and CIFAR-10 class-conditional generation,\nDTS matches the FID of the best-performing baseline with up to $10\\times$ less\ncompute. In text-to-image generation and language completion tasks, DTS$^\\star$\neffectively searches for high reward samples that match best-of-N with up to\n$5\\times$ less compute. By reusing information from previous generations, we\nget an anytime algorithm that turns additional compute into steadily better\nsamples, providing a scalable approach for inference-time alignment of\ndiffusion models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20701v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20701v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "扩散树采样：扩散模型推理时对齐的可扩展方法", "tldr": "引入了扩散树采样（DTS），一种受蒙特卡罗树搜索启发的、用于扩散模型高效可扩展推理时对齐的方法，显著降低了计算成本，同时保持或提高了样本质量。", "motivation": "预训练扩散模型在推理时适应新目标时，现有引导方法存在价值估计不准确的问题，尤其是在高噪声水平下，这会使引导产生偏差。此外，过去运行的信息没有被重新利用来提高样本质量，导致计算资源使用效率低下。", "method": "受蒙特卡罗树搜索的启发，将推理时对齐视为一个重用过去计算的搜索问题。提出了扩散树采样（DTS），这是一种基于树的方法，通过将终端奖励通过扩散链传播回来，并随着每次额外的生成迭代地细化价值估计，从而从奖励对齐的目标密度中采样。其贪婪变体，扩散树搜索（DTS$^\\star$），对高奖励样本进行全局搜索。", "result": "在MNIST和CIFAR-10的类条件生成任务中，DTS以高达10倍的更少计算量匹配了最佳基线的FID。在文本到图像生成和语言补全任务中，DTS$^\\star$有效地搜索了高奖励样本，以高达5倍的更少计算量匹配了最佳N中选择的效果。通过重用信息，该算法可以随着额外计算的增加持续改进样本质量。", "conclusion": "DTS通过重用前几代的信息，为扩散模型的推理时对齐提供了一种可扩展且高效的方法，使得额外的计算能够转化为持续更好的样本。", "translation": "预训练扩散模型在推理时适应新目标仍然是生成建模中的一个开放问题。现有的引导方法存在价值估计不准确的问题，尤其是在高噪声水平下，这会使引导产生偏差。此外，过去运行的信息没有被重新利用来提高样本质量，导致计算资源使用效率低下。受蒙特卡罗树搜索成功的启发，我们通过将推理时对齐视为一个重用过去计算的搜索问题来解决这些限制。我们引入了一种基于树的方法，通过将终端奖励通过扩散链传播回来，并随着每次额外的生成迭代地细化价值估计，从而从奖励对齐的目标密度中采样。我们提出的方法，扩散树采样（DTS），在无限次展开的极限下，从目标分布中产生渐近精确的样本；其贪婪变体，扩散树搜索（DTS$^\nstar$），对高奖励样本进行全局搜索。在MNIST和CIFAR-10的类条件生成任务中，DTS以高达10倍的更少计算量匹配了最佳基线的FID。在文本到图像生成和语言补全任务中，DTS$^\nstar$有效地搜索了高奖励样本，以高达5倍的更少计算量匹配了最佳N中选择的效果。通过重用前几代的信息，我们得到了一个随时可用的算法，它将额外的计算转化为持续更好的样本，为扩散模型的推理时对齐提供了一种可扩展的方法。", "summary": "本文介绍了扩散树采样（DTS）及其贪婪变体DTS$^\nstar$，这是一种受蒙特卡罗树搜索启发的新方法，旨在解决预训练扩散模型在推理时高效对齐新目标的挑战。通过将对齐视为一个搜索问题并在树结构中重用过去的计算，DTS提供了渐近精确的样本，而DTS$^\nstar$则寻找高奖励样本。这些方法在各种生成任务中展示了显著的计算节省（DTS高达10倍，DTS$^\nstar$高达5倍），同时保持或提高了性能，为扩散模型引导提供了一种可扩展且随时可用的解决方案。", "keywords": "扩散模型, 推理时对齐, 蒙特卡罗树搜索, 生成建模, 计算效率", "comments": "这篇论文通过借鉴蒙特卡罗树搜索（一种在强化学习和游戏AI中成熟的技术），为扩散模型引导提出了一种创新方法。其主要创新在于将推理时对齐构建为一个搜索问题，并利用树结构重用计算，这解决了现有方法中价值估计不准确和计算效率低下的常见问题。该方法“随时可用”的特性（即更多计算带来更好样本）是实现可扩展部署的一个重要实际优势。这项工作可能为更高效、更鲁棒地控制扩散模型输出铺平道路。"}}
{"id": "2506.20882", "title": "Resilience Through Escalation: A Graph-Based PACE Architecture for Satellite Threat Response", "authors": ["Anouar Boumeftah", "Sarah McKenzie-Picot", "Peter Klimas", "Gunes Karabulut Kurt"], "summary": "Satellite systems increasingly face operational risks from jamming,\ncyberattacks, and electromagnetic disruptions. Traditional redundancy\nstrategies often fail against dynamic, multi-vector threats. This paper\nintroduces a resilience-by-design framework grounded in the PACE (Primary,\nAlternate, Contingency, Emergency) methodology, originally developed for\ntactical communications in military operations, adapting it to satellite\nsystems through a layered state-transition model informed by threat scoring\nframeworks such as CVSS, DREAD, and NASA's risk matrix. We define a dynamic\nresilience index to quantify system adaptability and implement three PACE\nvariants: static, adaptive, and softmax-based decision models, to evaluate\nresilience under diverse disruption scenarios. The proposed approach highlights\nthe effectiveness of lightweight, decision-aware fallback mechanisms in\nimproving survivability and operational continuity for next-generation space\nassets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20882v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20882v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "通过升级实现韧性：一种用于卫星威胁响应的基于图的PACE架构", "tldr": "本文提出了一种基于PACE方法（源于军事战术通信）的韧性设计框架，用于增强卫星系统应对动态多向量威胁的韧性。通过分层状态转换模型和威胁评分框架，并引入动态韧性指数，评估了静态、自适应和基于softmax的PACE变体在不同中断场景下的表现，证明了轻量级、决策感知的后备机制能有效提高下一代空间资产的生存能力和运行连续性。", "motivation": "卫星系统日益面临来自干扰、网络攻击和电磁中断的操作风险。传统的冗余策略在应对动态、多向量威胁时往往失效。", "method": "本文引入了一种基于PACE（Primary, Alternate, Contingency, Emergency）方法论的韧性设计框架，并将其应用于卫星系统。该框架通过分层状态转换模型，并结合CVSS、DREAD和NASA风险矩阵等威胁评分框架来指导。研究定义了一个动态韧性指数来量化系统适应性，并实现了三种PACE变体：静态、自适应和基于softmax的决策模型，以评估在不同中断场景下的韧性。", "result": "该方法突出了轻量级、决策感知的后备机制在提高下一代空间资产的生存能力和运行连续性方面的有效性。", "conclusion": "轻量级、决策感知的后备机制能有效提高下一代空间资产的生存能力和运行连续性，从而增强卫星系统应对动态多向量威胁的韧性。", "translation": "卫星系统日益面临来自干扰、网络攻击和电磁中断的操作风险。传统的冗余策略在应对动态、多向量威胁时往往失效。本文引入了一种基于PACE（Primary, Alternate, Contingency, Emergency）方法论的韧性设计框架，该方法最初是为军事行动中的战术通信开发的，通过受威胁评分框架（如CVSS、DREAD和NASA风险矩阵）启发的分层状态转换模型，将其应用于卫星系统。我们定义了一个动态韧性指数来量化系统适应性，并实现了三种PACE变体：静态、自适应和基于softmax的决策模型，以评估在不同中断场景下的韧性。所提出的方法突出了轻量级、决策感知的后备机制在提高下一代空间资产的生存能力和运行连续性方面的有效性。", "summary": "本文提出了一种基于PACE（Primary, Alternate, Contingency, Emergency）方法论的韧性设计框架，旨在解决卫星系统在面对动态、多向量威胁时传统冗余策略失效的问题。该框架通过分层状态转换模型并结合威胁评分框架，为卫星系统提供韧性保障。研究定义了动态韧性指数，并评估了静态、自适应和基于softmax的PACE决策模型在不同中断场景下的表现，结果表明轻量级、决策感知的后备机制能有效提升下一代空间资产的生存能力和运行连续性。", "keywords": "卫星系统, 韧性, PACE架构, 威胁响应, 后备机制", "comments": "本文将源自军事战术通信的PACE方法论创造性地应用于卫星系统韧性设计，并结合威胁评分框架和动态韧性指数进行量化评估，提供了一种新颖的、系统性的方法来应对复杂的空间威胁。其创新点在于将成熟的军事策略与空间系统需求相结合，并通过多模型评估验证了轻量级后备机制的有效性，对于提升未来空间资产的生存能力和操作连续性具有重要意义。"}}
{"id": "2506.20786", "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20786v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20786v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "AI驱动的MRI脑肿瘤分割基准测试", "tldr": "本研究对多种基于提示的医学图像分割模型在脑肿瘤数据集上进行了基准测试，发现虽然在理想提示下部分模型表现优异，但由于实际操作的限制，nnU-Net仍是实际应用中的主导。", "motivation": "现有研究缺乏对新兴通用可提示模型（如SAM系列）及其医学变体在通用医学数据集上，针对不同提示质量的评估和比较。", "method": "研究使用Segment Anything Model (SAM)、Segment Anything Model 2 (SAM 2)、MedSAM、SAM-Med-3D和nnU-Net在BraTS 2023成人胶质瘤和儿科数据集上进行零样本推理，评估点和边界框两种提示类型在多种提示质量下的表现。此外，还在儿科数据集上对SAM、SAM 2、MedSAM和SAM-Med-3D进行了微调，以扩展模型和提示评估。", "result": "在提供极其准确的边界框提示时，SAM和SAM 2的Dice分数分别高达0.894和0.893，超过了nnU-Net的分割性能。然而，由于提供高度准确提示的不切实际性，nnU-Net仍然是主要的医学图像分割网络。微调后点提示的性能显著提高，但仍未能实现比边界框或nnU-Net更好的分割效果。", "conclusion": "尽管在理想的高精度边界框提示下，部分基于提示的模型（如SAM和SAM 2）在脑肿瘤分割上能超越nnU-Net，但由于实际应用中难以提供此类高精度提示，nnU-Net仍是当前医学图像分割的主导模型。微调可以显著改善点提示性能，但要达到最优分割效果仍需进一步研究。", "translation": "医学图像分割极大地辅助了医学诊断，其中基于U-Net的架构和nnU-Net提供了最先进的性能。近年来，出现了许多通用的可提示模型及其医学变体，但目前在通用医学数据集上，针对不同提示质量对这些模型进行评估和比较的工作尚缺乏。本研究使用Segment Anything Model (SAM)、Segment Anything Model 2 (SAM 2)、MedSAM、SAM-Med-3D和nnU-Net，在BraTS 2023成人胶质瘤和儿科数据集上，针对点和边界框两种提示类型，在多种提示质量下进行零样本推理。其中几个模型表现出有前景的Dice分数，特别是SAM和SAM 2在给定极其准确的边界框提示时，分别取得了高达0.894和0.893的分数，这超出了nnU-Net的分割性能。然而，由于提供高度准确提示给模型的不切实际性，nnU-Net仍然是主要的医学图像分割网络。通过在儿科数据集上对SAM、SAM 2、MedSAM和SAM-Med-3D进行微调，扩展了模型和提示的评估和比较。微调后点提示性能的提升是显著的，并显示出未来研究的潜力，但仍未能实现比边界框或nnU-Net更好的分割效果。", "summary": "本研究对AI驱动的MRI脑肿瘤分割模型进行了基准测试，比较了包括SAM系列模型和nnU-Net在内的多种模型在BraTS 2023数据集上的表现。结果表明，尽管SAM和SAM 2在获得极高精度边界框提示时能超越nnU-Net，但由于实际应用中的限制，nnU-Net仍是主流选择。微调能显著提升点提示性能，但尚未超越边界框或nnU-Net。", "keywords": "脑肿瘤分割, 提示模型, nnU-Net, SAM, 性能基准", "comments": "本文对新兴的通用可提示模型在医学图像分割领域的实际应用潜力进行了有价值的评估。其创新点在于系统性地比较了不同提示质量对模型性能的影响。研究揭示了虽然这些模型在理想条件下表现出色，但在实际临床场景中，如何高效获取高质量提示仍是一个挑战。这强调了nnU-Net等无需精细提示的模型在当前实践中的重要性，并为未来可提示模型的研究方向（如提升点提示鲁棒性和实用性）提供了见解。"}}
{"id": "2506.20949", "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation", "authors": ["Chenkai Sun", "Denghui Zhang", "ChengXiang Zhai", "Heng Ji"], "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20949v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20949v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "超越反应式安全：通过长周期模拟实现风险感知型LLM对齐", "tldr": "提出一个框架和数据集，用于通过长周期模拟评估和改进大型语言模型在社会决策中的长期安全对齐。", "motivation": "鉴于语言模型代理对高风险社会决策日益增长的影响，确保其有益影响需要理解其建议的深远影响。现有安全方法可能仅限于反应式安全，未能预见间接和长期的危害。", "method": "提出一个概念验证框架，该框架能预测模型生成的建议如何在宏观层面随时间在社会系统中传播，从而实现更强大的对齐。引入一个包含100个间接危害场景的数据集，用于测试模型预见看似无害的用户提示带来的不利、不明显结果的能力。", "result": "在新的数据集上实现了超过20%的改进，并且在现有安全基准（AdvBench, SafeRLHF, WildGuardMix）上，相对于强基线，平均胜率超过70%。", "conclusion": "本研究提出的方法为构建更安全的代理提供了一个有前景的方向，通过长期模拟和风险感知对齐来超越反应式安全。", "translation": "鉴于基于语言模型的代理对公共政策到医疗保健等高风险社会决策日益增长的影响，确保其有益影响需要理解其建议的深远影响。我们提出了一个概念验证框架，该框架能预测模型生成的建议如何在宏观层面随时间在社会系统中传播，从而实现更强大的对齐。为了评估语言模型的长期安全意识，我们还引入了一个包含100个间接危害场景的数据集，用于测试模型预见看似无害的用户提示带来的不利、不明显结果的能力。我们的方法不仅在新数据集上实现了超过20%的改进，而且在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）上，相对于强基线，平均胜率超过70%，这表明了构建更安全代理的一个有前景的方向。", "summary": "该研究提出了一个名为“超越反应式安全”的风险感知型LLM对齐框架，通过长周期模拟预测模型建议在社会系统中的长期影响。为评估模型长期安全意识，引入了包含100个间接危害场景的新数据集。该方法在新数据集上提升20%，并在现有安全基准上对强基线表现出超过70%的胜率，为开发更安全的AI代理提供了新方向。", "keywords": "LLM对齐, 风险感知, 长期模拟, 安全, 间接危害", "comments": "这项工作创新性地将LLM安全对齐的关注点从短期反应式安全扩展到长期、宏观层面的风险感知。通过引入长周期模拟框架和间接危害数据集，解决了现有安全评估的局限性，特别是在预见非显而易见的链式反应方面。其在性能上的显著提升表明了该方向的巨大潜力，对于构建更负责任和安全的AI系统具有重要意义。"}}
{"id": "2506.20897", "title": "Development of MR spectral analysis method robust against static magnetic field inhomogeneity", "authors": ["Shuki Maruyama", "Hidenori Takeshima"], "summary": "Purpose:To develop a method that enhances the accuracy of spectral analysis\nin the presence of static magnetic field B0 inhomogeneity. Methods:The authors\nproposed a new spectral analysis method utilizing a deep learning model trained\non modeled spectra that consistently represent the spectral variations induced\nby B0 inhomogeneity. These modeled spectra were generated from the B0 map and\nmetabolite ratios of the healthy human brain. The B0 map was divided into a\npatch size of subregions, and the separately estimated metabolites and baseline\ncomponents were averaged and then integrated. The quality of the modeled\nspectra was visually and quantitatively evaluated against the measured spectra.\nThe analysis models were trained using measured, simulated, and modeled\nspectra. The performance of the proposed method was assessed using mean squared\nerrors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)\nof the metabolite ratios were also compared to LCModel when analyzing the\nphantom spectra acquired under two types of B0 inhomogeneity. Results:The\nmodeled spectra exhibited broadened and narrowed spectral peaks depending on\nthe B0 inhomogeneity and were quantitatively close to the measured spectra. The\nanalysis model trained using measured spectra with modeled spectra improved\nMSEs by 49.89% compared to that trained using measured spectra alone, and by\n26.66% compared to that trained using measured spectra with simulated spectra.\nThe performance improved as the number of modeled spectra increased from 0 to\n1,000. This model showed significantly lower MAPEs than LCModel under both\ntypes of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep\nlearning model using the modeled spectra was developed. The results suggest\nthat the proposed method has the potential to improve the accuracy of spectral\nanalysis by increasing the training samples of spectra.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20897v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20897v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "开发一种对静磁场不均匀性具有鲁棒性的MR光谱分析方法", "tldr": "开发了一种基于深度学习的MR光谱分析方法，利用建模光谱进行训练，显著提高了在静磁场不均匀性下的分析准确性，优于现有方法。", "motivation": "为了提高在静磁场B0不均匀性存在下光谱分析的准确性。", "method": "作者提出了一种新的光谱分析方法，该方法利用深度学习模型，并使用能够一致表示B0不均匀性引起的光谱变化的建模光谱进行训练。这些建模光谱是根据健康人脑的B0图和代谢物比率生成的。B0图被划分为子区域，分别估计的代谢物和基线成分被平均并整合。建模光谱的质量通过视觉和定量方式与测量光谱进行评估。分析模型使用测量、模拟和建模光谱进行训练。通过代谢物比率的均方误差（MSEs）评估所提方法的性能，并与在两种B0不均匀性下采集的体模光谱分析中的LCModel的平均绝对百分比误差（MAPEs）进行比较。", "result": "建模光谱根据B0不均匀性表现出加宽和变窄的光谱峰，并且在定量上接近测量光谱。使用测量光谱和建模光谱训练的分析模型，与单独使用测量光谱训练的模型相比，MSEs提高了49.89%；与使用测量光谱和模拟光谱训练的模型相比，MSEs提高了26.66%。随着建模光谱数量从0增加到1,000，性能有所提高。该模型在两种B0不均匀性下均显示出比LCModel显著更低的MAPEs。", "conclusion": "开发了一种使用建模光谱进行训练的新的光谱分析深度学习模型。结果表明，所提出的方法通过增加光谱训练样本，有潜力提高光谱分析的准确性。", "translation": "目的：开发一种在静磁场B0不均匀性存在下提高光谱分析准确性的方法。方法：作者提出了一种新的光谱分析方法，该方法利用一个深度学习模型，该模型通过对一致表示B0不均匀性引起的光谱变化的建模光谱进行训练。这些建模光谱是根据健康人脑的B0图和代谢物比率生成的。B0图被划分为补丁大小的子区域，并且分别估计的代谢物和基线成分被平均然后整合。建模光谱的质量通过视觉和定量方式与测量光谱进行评估。分析模型使用测量、模拟和建模光谱进行训练。所提出方法的性能通过代谢物比率的均方误差（MSEs）进行评估。在分析在两种B0不均匀性下采集的体模光谱时，代谢物比率的平均绝对百分比误差（MAPEs）也与LCModel进行了比较。结果：建模光谱根据B0不均匀性表现出加宽和变窄的光谱峰，并且在定量上接近测量光谱。与单独使用测量光谱训练的模型相比，使用测量光谱和建模光谱训练的分析模型将MSEs提高了49.89%；与使用测量光谱和模拟光谱训练的模型相比，提高了26.66%。性能随着建模光谱数量从0增加到1,000而提高。该模型在两种B0不均匀性下均显示出比LCModel显著更低的MAPEs。结论：开发了一种使用建模光谱进行训练的新的光谱分析深度学习模型。结果表明，所提出的方法通过增加光谱训练样本，有潜力提高光谱分析的准确性。", "summary": "本研究旨在开发一种在静磁场B0不均匀性下提高MR光谱分析准确性的方法。作者提出了一种基于深度学习的新型光谱分析方法，该方法利用健康人脑的B0图和代谢物比率生成建模光谱，以模拟并表示B0不均匀性引起的光谱变化。这些建模光谱与测量光谱在视觉和定量上均表现出高度一致性。研究表明，使用建模光谱进行训练显著提高了深度学习模型的性能，与仅使用测量光谱或结合模拟光谱训练的模型相比，代谢物比率的均方误差分别降低了49.89%和26.66%。该方法在两种B0不均匀性下均显示出比传统LCModel更低的平均绝对百分比误差。结果表明，通过增加训练样本，该方法有望显著提高光谱分析的准确性。", "keywords": "MR光谱分析, B0不均匀性, 深度学习, 建模光谱, 准确性", "comments": "本文的创新点在于提出了利用建模光谱来训练深度学习模型，以应对MR光谱分析中静磁场不均匀性带来的挑战。这种方法有效地扩大了训练数据集，显著提高了模型在复杂磁场环境下的鲁棒性和准确性。其重要性在于为MR光谱分析提供了一种更精确、更可靠的工具，这对于临床诊断和研究具有重要意义。该方法通过实验结果证明了其优于现有方法的性能，具有实际应用潜力。"}}
{"id": "2506.20858", "title": "Doppler Estimation and Compensation Techniques in LoRa Direct-to-Satellite Communications", "authors": ["Jamil Farhat", "Gianni Pasolini", "Enrico Paolini", "Muhammad Asad Ullah", "Richard Demo Souza"], "summary": "Within the LPWAN framework, the LoRa modulation adopted by LoRaWAN technology\nhas garnered significant interest as a connectivity solution for IoT\napplications due to its ability to offer low-cost, low-power, and long-range\ncommunications. One emerging use case of LoRa is DtS connectivity, which\nextends coverage to remote areas for supporting IoT operations. The satellite\nIoT industry mainly prefers LEO because it has lower launch costs and less path\nloss compared to Geostationary orbit. However, a major drawback of LEO\nsatellites is the impact of the Doppler effect caused by their mobility.\nEarlier studies have confirmed that the Doppler effect significantly degrades\nthe LoRa DtS performance. In this paper, we propose four frameworks for Doppler\nestimation and compensation in LoRa DtS connectivity and numerically compare\nthe performance against the ideal scenario without the Doppler effect.\nFurthermore, we investigate the trade-offs among these frameworks by analyzing\nthe interplay between spreading factor, and other key parameters related to the\nDoppler effect. The results provide insights into how to achieve robust LoRa\nconfigurations for DtS connectivity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20858v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20858v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "LoRa直连卫星通信中的多普勒估计与补偿技术", "tldr": "本文提出了四种LoRa直连卫星通信中的多普勒估计与补偿框架，以解决LEO卫星移动性导致的多普勒效应对性能的显著影响，并分析了其性能和权衡。", "motivation": "低地球轨道（LEO）卫星的移动性导致的多普勒效应会显著降低LoRa直连卫星（DtS）通信的性能。", "method": "提出了四种用于LoRa直连卫星（DtS）连接中多普勒估计和补偿的框架，并将其性能与没有多普勒效应的理想场景进行了数值比较。此外，还通过分析扩频因子和与多普勒效应相关的其他关键参数之间的相互作用，研究了这些框架之间的权衡。", "result": "研究结果为如何实现LoRa直连卫星通信的鲁棒配置提供了见解。", "conclusion": "所提出的多普勒估计和补偿框架有助于实现LoRa直连卫星通信的鲁棒配置，以应对多普勒效应。", "translation": "在LPWAN框架内，LoRaWAN技术所采用的LoRa调制因其提供低成本、低功耗和长距离通信的能力，作为物联网应用的连接解决方案受到了广泛关注。LoRa的一个新兴用例是直连卫星（DtS）连接，它将覆盖范围扩展到偏远地区以支持物联网操作。卫星物联网行业主要偏爱低地球轨道（LEO），因为它与地球静止轨道相比具有更低的发射成本和更小的路径损耗。然而，LEO卫星的一个主要缺点是其移动性引起的多普勒效应的影响。早期的研究已经证实，多普勒效应会显著降低LoRa DtS的性能。在本文中，我们提出了四种用于LoRa DtS连接中多普勒估计和补偿的框架，并将其性能与没有多普勒效应的理想场景进行了数值比较。此外，我们通过分析扩频因子以及与多普勒效应相关的其他关键参数之间的相互作用，研究了这些框架之间的权衡。结果提供了关于如何实现LoRa DtS连接的鲁棒配置的见解。", "summary": "本文针对低地球轨道（LEO）卫星移动性导致的多普勒效应显著降低LoRa直连卫星（DtS）通信性能的问题，提出了四种多普勒估计与补偿框架。研究通过数值比较了这些框架在理想场景下的性能，并分析了扩频因子与其他多普勒相关参数之间的权衡，旨在为实现LoRa DtS通信的鲁棒配置提供指导。", "keywords": "LoRa, 直连卫星通信, 多普勒效应, 多普勒补偿, 低地球轨道", "comments": "该论文解决了LoRa直连卫星通信中的一个关键挑战——多普勒效应，这对于LEO卫星物联网应用至关重要。通过提出并比较多种估计和补偿框架，为实际系统设计提供了有价值的工程指导，特别是关于参数权衡的分析，具有创新性和实用性。"}}
{"id": "2506.20946", "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models", "authors": ["Donggoo Kang", "Jangyeong Kim", "Dasol Jeong", "Junyoung Choi", "Jeonga Wi", "Hyunmin Lee", "Joonho Gwon", "Joonki Paik"], "summary": "Current texture synthesis methods, which generate textures from fixed\nviewpoints, suffer from inconsistencies due to the lack of global context and\ngeometric understanding. Meanwhile, recent advancements in video generation\nmodels have demonstrated remarkable success in achieving temporally consistent\nvideos. In this paper, we introduce VideoTex, a novel framework for seamless\ntexture synthesis that leverages video generation models to address both\nspatial and temporal inconsistencies in 3D textures. Our approach incorporates\ngeometry-aware conditions, enabling precise utilization of 3D mesh structures.\nAdditionally, we propose a structure-wise UV diffusion strategy, which enhances\nthe generation of occluded areas by preserving semantic information, resulting\nin smoother and more coherent textures. VideoTex not only achieves smoother\ntransitions across UV boundaries but also ensures high-quality, temporally\nstable textures across video frames. Extensive experiments demonstrate that\nVideoTex outperforms existing methods in texture fidelity, seam blending, and\nstability, paving the way for dynamic real-time applications that demand both\nvisual quality and temporal coherence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20946v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.7; I.4.10; I.2.10"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20946v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用几何感知扩散和时间视频模型实现一致的零样本3D纹理合成", "tldr": "VideoTex利用视频生成模型和几何感知扩散，解决了3D纹理合成中的空间和时间不一致问题，生成高质量、稳定的纹理。", "motivation": "当前纹理合成方法因缺乏全局上下文和几何理解，在固定视角下生成纹理时存在不一致性。", "method": "本文引入VideoTex框架，利用视频生成模型解决3D纹理的空间和时间不一致性。该方法结合几何感知条件以精确利用3D网格结构，并提出结构化UV扩散策略，通过保留语义信息增强遮挡区域生成，从而产生更平滑、更连贯的纹理。", "result": "VideoTex实现了UV边界间的平滑过渡，并确保了视频帧间高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法。", "conclusion": "VideoTex为需要视觉质量和时间连贯性的动态实时应用铺平了道路。", "translation": "当前从固定视角生成纹理的方法，由于缺乏全局上下文和几何理解，存在不一致性问题。同时，视频生成模型的最新进展在实现时间上一致的视频方面取得了显著成功。在本文中，我们引入了VideoTex，一个用于无缝纹理合成的新颖框架，它利用视频生成模型来解决3D纹理中的空间和时间不一致性。我们的方法结合了几何感知条件，能够精确利用3D网格结构。此外，我们提出了一种结构化UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而产生更平滑、更连贯的纹理。VideoTex不仅实现了UV边界间的平滑过渡，还确保了视频帧间高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法，为需要视觉质量和时间连贯性的动态实时应用铺平了道路。", "summary": "VideoTex是一个新颖的3D纹理合成框架，它利用视频生成模型和几何感知条件来解决当前方法中存在的空间和时间不一致性。通过引入结构化UV扩散策略，VideoTex能生成更平滑、连贯的纹理，尤其是在遮挡区域。实验证明，VideoTex在纹理质量、接缝处理和时间稳定性上均优于现有技术，为实时动态应用提供了高质量的视觉和时间连贯性。", "keywords": "3D纹理合成, 视频生成模型, 几何感知扩散, 时间一致性, 零样本", "comments": "该论文通过将视频生成模型引入3D纹理合成，巧妙地解决了传统方法中存在的空间和时间不一致性问题，具有创新性。其引入的几何感知条件和结构化UV扩散策略，有效提升了纹理质量，尤其是在复杂几何体的处理上。VideoTex的成功为实时3D渲染和动态应用开辟了新的可能性，前景广阔。"}}
{"id": "2506.21386", "title": "Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings", "authors": ["Ghazal Al-Shwayyat", "Omer Nezih Gerek"], "summary": "Arabic dialect recognition presents a significant challenge in speech\ntechnology due to the linguistic diversity of Arabic and the scarcity of large\nannotated datasets, particularly for underrepresented dialects. This research\ninvestigates hybrid modeling strategies that integrate classical signal\nprocessing techniques with deep learning architectures to address this problem\nin low-resource scenarios. Two hybrid models were developed and evaluated: (1)\nMel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural\nNetwork (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with\na Recurrent Neural Network (RNN). The models were trained on a dialect-filtered\nsubset of the Common Voice Arabic dataset, with dialect labels assigned based\non speaker metadata. Experimental results demonstrate that the MFCC + CNN\narchitecture achieved superior performance, with an accuracy of 91.2% and\nstrong precision, recall, and F1-scores, significantly outperforming the\nWavelet + RNN configuration, which achieved an accuracy of 66.5%. These\nfindings highlight the effectiveness of leveraging spectral features with\nconvolutional models for Arabic dialect recognition, especially when working\nwith limited labeled data. The study also identifies limitations related to\ndataset size, potential regional overlaps in labeling, and model optimization,\nproviding a roadmap for future research. Recommendations for further\nimprovement include the adoption of larger annotated corpora, integration of\nself-supervised learning techniques, and exploration of advanced neural\narchitectures such as Transformers. Overall, this research establishes a strong\nbaseline for future developments in Arabic dialect recognition within\nresource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21386v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21386v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "混合深度学习和信号处理在低资源环境下的阿拉伯语方言识别", "tldr": "本研究将信号处理与深度学习相结合，以解决低资源环境下阿拉伯语方言识别的挑战。MFCC+CNN模型表现最佳，准确率达91.2%，优于DWT+RNN。", "motivation": "由于阿拉伯语的语言多样性以及大型标注数据集的稀缺性，特别是对于代表性不足的方言，阿拉伯语方言识别在语音技术中面临重大挑战。", "method": "开发并评估了两种混合模型：(1) 梅尔频率倒谱系数 (MFCC) 与卷积神经网络 (CNN) 结合，以及 (2) 离散小波变换 (DWT) 特征与循环神经网络 (RNN) 结合。模型在Common Voice 阿拉伯语数据集的方言过滤子集上进行训练。", "result": "MFCC + CNN 架构取得了卓越的性能，准确率达到 91.2%，并具有强大的精确率、召回率和 F1 分数，显著优于小波 + RNN 配置，后者准确率为 66.5%。", "conclusion": "MFCC + CNN 架构在低资源环境下对阿拉伯语方言识别是有效的。研究指出了数据集大小、潜在的地域标签重叠以及模型优化等局限性，并为未来的研究提出了建议，包括采用更大的标注语料库、整合自监督学习技术和探索更先进的神经网络架构。这项研究为资源受限环境下阿拉伯语方言识别的未来发展奠定了坚实的基础。", "translation": "阿拉伯语方言识别在语音技术中面临重大挑战，原因在于阿拉伯语的语言多样性以及大型标注数据集的稀缺性，特别是对于代表性不足的方言。本研究探讨了混合建模策略，该策略将经典信号处理技术与深度学习架构相结合，以解决低资源场景下的这一问题。开发并评估了两种混合模型：(1) 梅尔频率倒谱系数 (MFCC) 与卷积神经网络 (CNN) 结合，以及 (2) 离散小波变换 (DWT) 特征与循环神经网络 (RNN) 结合。这些模型在 Common Voice 阿拉伯语数据集的方言过滤子集上进行训练，方言标签根据说话人元数据分配。实验结果表明，MFCC + CNN 架构取得了卓越的性能，准确率达到 91.2%，并具有强大的精确率、召回率和 F1 分数，显著优于小波 + RNN 配置，后者准确率为 66.5%。这些发现强调了利用频谱特征与卷积模型进行阿拉伯语方言识别的有效性，尤其是在处理有限标注数据时。该研究还指出了与数据集大小、潜在的地域标签重叠以及模型优化相关的局限性，为未来的研究提供了路线图。进一步改进的建议包括采用更大的标注语料库、整合自监督学习技术以及探索更先进的神经网络架构，例如 Transformer。总的来说，这项研究为资源受限环境下阿拉伯语方言识别的未来发展奠定了坚实的基础。", "summary": "本研究旨在解决低资源环境下阿拉伯语方言识别的挑战，提出并评估了两种结合信号处理与深度学习的混合模型：MFCC+CNN和DWT+RNN。实验结果表明，MFCC+CNN模型表现最优，准确率达到91.2%，证明了频谱特征与卷积网络结合的有效性。研究还指出了当前方法的局限性，并为未来的研究方向提供了建议，包括使用更大的数据集和探索更先进的神经网络架构。", "keywords": "阿拉伯语方言识别, 深度学习, 信号处理, 低资源, MFCC, CNN", "comments": "这篇论文为低资源阿拉伯语方言识别问题提供了一个实用的混合深度学习方法。MFCC与CNN的结合表现出显著的潜力。论文中对局限性的识别以及对未来研究方向（特别是自监督学习和Transformer）的建议，为该领域的进一步改进提供了清晰的路径，奠定了坚实的基础。"}}
{"id": "2506.20828", "title": "Practical and Accurate Local Edge Differentially Private Graph Algorithms", "authors": ["Pranay Mundra", "Charalampos Papamanthou", "Julian Shun", "Quanquan C. Liu"], "summary": "The rise of massive networks across diverse domains necessitates\nsophisticated graph analytics, often involving sensitive data and raising\nprivacy concerns. This paper addresses these challenges using local\ndifferential privacy (LDP), which enforces privacy at the individual level,\nwhere no third-party entity is trusted, unlike centralized models that assume a\ntrusted curator. We introduce novel LDP algorithms for two fundamental graph\nstatistics: k-core decomposition and triangle counting. Our approach leverages\ninput-dependent private graph properties, specifically the degeneracy and\nmaximum degree of the graph, to improve theoretical utility. Unlike prior\nmethods, our error bounds are determined by the maximum degree rather than the\ntotal number of edges, resulting in significantly tighter guarantees. For\ntriangle counting, we improve upon the work of Imola, Murakami, and\nChaudhury~\\cite{IMC21locally, IMC21communication}, which bounds error in terms\nof edge count. Instead, our algorithm achieves bounds based on graph degeneracy\nby leveraging a private out-degree orientation, a refined variant of Eden et\nal.'s randomized response technique~\\cite{ELRS23, and a novel analysis,\nyielding stronger guarantees than prior work. Beyond theoretical gains, we are\nthe first to evaluate local DP algorithms in a distributed simulation, unlike\nprior work tested on a single processor. Experiments on real-world graphs show\nsubstantial accuracy gains: our k-core decomposition achieves errors within 3x\nof exact values, far outperforming the 131x error in the baseline of Dhulipala\net al.~\\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative\napproximation errors by up to six orders of magnitude, while maintaining\ncompetitive runtime.", "comment": "To appear in VLDB 2025", "pdf_url": "http://arxiv.org/pdf/2506.20828v1", "categories": ["cs.DS", "cs.CR", "cs.DB"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20828v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "实用且精确的局部边差分隐私图算法", "tldr": "本文提出并评估了新的局部差分隐私（LDP）图算法，用于k-核分解和三角形计数，通过利用图的退化度和最大度，显著提高了准确性并收紧了误差界限。", "motivation": "大规模网络中的敏感数据涉及隐私问题，需要先进的图分析方法。传统的中心化模型假设存在可信的策展人，但局部差分隐私（LDP）在个体层面强制执行隐私，无需信任第三方实体，因此本文旨在解决LDP在图算法中的挑战。", "method": "本文提出了针对k-核分解和三角形计数的局部差分隐私（LDP）算法。该方法利用输入依赖的私有图属性，特别是图的退化度和最大度，以提高理论效用。对于三角形计数，该算法通过利用私有出度定向（Eden et al.随机响应技术的改进变体）和新颖的分析，实现了基于图退化度的误差界限。此外，本文首次在分布式模拟中评估了局部DP算法。", "result": "本文的算法实现了比先前工作更强的理论保证，误差界限由最大度而非总边数决定。对于k-核分解，误差在精确值的3倍以内，远优于基线的131倍误差。对于三角形计数，乘法近似误差减少了高达六个数量级，同时保持了有竞争力的运行时性能。", "conclusion": "本文提出了实用且精确的局部差分隐私图算法，通过利用图的内在属性，显著提高了k-核分解和三角形计数的准确性，并在分布式环境中进行了有效验证，为敏感图数据的隐私保护分析提供了更强的保证。", "translation": "大规模网络在不同领域的兴起，需要复杂的图分析，这通常涉及敏感数据并引发隐私问题。本文使用局部差分隐私（LDP）来解决这些挑战，LDP在个体层面强制执行隐私，不像假设有可信策展人的中心化模型那样，不信任任何第三方实体。我们为两种基本图统计：k-核分解和三角形计数引入了新颖的LDP算法。我们的方法利用输入依赖的私有图属性，特别是图的退化度和最大度，以提高理论效用。与现有方法不同，我们的误差界限由最大度而不是总边数决定，从而产生了显著更紧密的保证。对于三角形计数，我们改进了Imola、Murakami和Chaudhury的工作，该工作将误差限制在边数方面。相反，我们的算法通过利用私有出度定向（Eden et al.随机响应技术的改进变体）和新颖的分析，实现了基于图退化度的界限，从而获得了比先前工作更强的保证。除了理论上的收益，我们首次在分布式模拟中评估了局部DP算法，这与先前在单个处理器上测试的工作不同。在真实世界图上的实验显示出显著的准确性提升：我们的k-核分解误差在精确值的3倍以内，远优于Dhulipala et al.基线的131倍误差。我们的三角形计数算法将乘法近似误差减少了高达六个数量级，同时保持了有竞争力的运行时性能。", "summary": "本文提出了一种新的局部差分隐私（LDP）图算法，用于k-核分解和三角形计数，旨在解决大规模网络中敏感数据分析的隐私问题。与现有方法不同，该算法通过利用图的退化度和最大度等输入依赖的私有属性，显著收紧了误差界限。实验结果表明，在k-核分解和三角形计数任务上，该算法在分布式模拟中表现出显著的准确性提升，误差远低于现有基线，同时保持了高效的运行时间。", "keywords": "局部差分隐私, 图算法, k-核分解, 三角形计数, 隐私保护", "comments": "本文的创新之处在于首次将局部差分隐私应用于k-核分解和三角形计数等核心图算法，并利用图的内在属性（如退化度和最大度）来优化误差界限，从而实现了显著的准确性提升。此外，本文首次在分布式模拟中验证了LDP图算法，增强了其实用性。其重要性在于为在大规模敏感图数据上进行隐私保护分析提供了更实用和精确的解决方案。"}}
{"id": "2506.20963", "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora", "authors": ["Fangyuan Zhang", "Zhengjun Huang", "Yingli Zhou", "Qintian Guo", "Zhixun Li", "Wensheng Luo", "Di Jiang", "Yixiang Fang", "Xiaofang Zhou"], "summary": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20963v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20963v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EraRAG：面向不断增长语料库的高效增量检索增强生成", "tldr": "EraRAG是一种新型的多层图检索增强生成（Graph-RAG）框架，支持高效、可扩展的动态更新，解决了现有Graph-RAG在动态环境中需要昂贵全图重建的问题。", "motivation": "现有基于图的检索增强生成（Graph-RAG）方法通常假设语料库是静态的，当有新文档到来时，需要进行昂贵的全图重建，这限制了它们在动态、不断演进环境中的可扩展性。", "method": "EraRAG引入了一种新颖的多层Graph-RAG框架。它利用基于超平面的局部敏感哈希（LSH）将原始语料库分区并组织成层次图结构，从而实现新数据的高效局部插入，而无需破坏现有拓扑结构。该设计消除了再训练或昂贵重新计算的需要。", "result": "实验表明，与现有Graph-RAG系统相比，EraRAG在大型基准测试中将更新时间和token消耗降低了一个数量级，同时提供了卓越的准确性性能。", "conclusion": "EraRAG为需要在持续增长语料库上运行的RAG系统提供了一条实用的前进道路，弥合了检索效率和适应性之间的差距。", "translation": "基于图的检索增强生成（Graph-RAG）通过在外部语料库上构建检索结构来增强大型语言模型（LLMs）。然而，现有方法通常假设语料库是静态的，当有新文档到来时，需要进行昂贵的全图重建，这限制了它们在动态、不断演进环境中的可扩展性。为了解决这些限制，我们引入了EraRAG，一种新颖的多层Graph-RAG框架，支持高效和可扩展的动态更新。我们的方法利用基于超平面的局部敏感哈希（LSH）将原始语料库分区并组织成层次图结构，从而实现新数据的高效局部插入，而无需破坏现有拓扑结构。该设计消除了再训练或昂贵重新计算的需要，同时保持了高检索准确性和低延迟。大型基准测试的实验表明，与现有Graph-RAG系统相比，EraRAG将更新时间和token消耗降低了一个数量级，同时提供了卓越的准确性性能。这项工作为必须在持续增长语料库上运行的RAG系统提供了一条实用的前进道路，弥合了检索效率和适应性之间的差距。我们的代码和数据可在https://github.com/EverM0re/EraRAG-Official 获取。", "summary": "EraRAG是一个创新的多层图检索增强生成（Graph-RAG）框架，旨在解决现有Graph-RAG系统在处理动态、不断增长的语料库时效率低下的问题。通过采用基于超平面的局部敏感哈希（LSH）来构建层次图结构，EraRAG实现了新数据的高效增量插入，避免了昂贵的全图重建和再训练。实验证明，EraRAG在更新时间、token消耗和检索准确性方面均优于现有系统，为RAG系统在动态环境中的应用提供了可扩展的解决方案。", "keywords": "检索增强生成, 动态更新, 图神经网络, 局部敏感哈希, 可扩展性", "comments": "这项工作提出了一个解决Graph-RAG在动态语料库中扩展性问题的创新方法。通过引入基于LSH的多层图结构，它巧妙地实现了高效的增量更新，避免了传统方法的全图重建成本。这种设计在保持高检索性能的同时，显著提高了系统的实用性和部署潜力，尤其对于需要实时处理数据增长的应用场景具有重要意义。"}}
{"id": "2506.20821", "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "comment": "Preprint Copy", "pdf_url": "http://arxiv.org/pdf/2506.20821v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20821v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "MultiFinRAG：一种优化的多模态检索增强生成（RAG）金融问答框架", "tldr": "MultiFinRAG是一种专为金融问答设计的多模态检索增强生成（RAG）框架，通过优化多模态信息提取和检索策略，在处理包含文本、表格和图像的复杂金融文档时，比ChatGPT-4o（免费版）的准确率高出19个百分点，且可在商用硬件上运行。", "motivation": "金融文档（如10-K、10-Q和投资者演示文稿）篇幅巨大且包含文本、表格、图表等多种模态信息。对这类内容进行问答通常需要跨模态的联合推理，而传统的大型语言模型（LLMs）和检索增强生成（RAG）管道因令牌限制、布局丢失和碎片化的跨模态上下文而难以有效处理。", "method": "MultiFinRAG首先通过将表格和图表图像分批发送给轻量级、量化的开源多模态LLM进行多模态提取，生成结构化JSON输出和简洁的文本摘要。然后，这些输出与叙述性文本一起，通过模态感知的相似度阈值进行嵌入和索引，以实现精确检索。最后，采用分层回退策略，在必要时动态地将上下文从纯文本升级到文本+表格+图像，从而实现跨模态推理并减少不相关上下文。", "result": "MultiFinRAG在涉及文本、表格、图像和组合多模态推理的复杂金融问答任务上，比ChatGPT-4o（免费版）的准确率高出19个百分点，并且可以在商用硬件上运行。", "conclusion": "MultiFinRAG是一个有效且高效的框架，能够处理复杂的多模态金融问答任务，显著优于现有主流模型，证明了其在金融领域应用的潜力和价值。", "translation": "金融文档——如10-K、10-Q和投资者演示文稿——跨越数百页，并结合了多样化的模态，包括密集的叙述性文本、结构化表格和复杂图表。对这类内容进行问答通常需要跨模态的联合推理，这由于令牌限制、布局丢失和碎片化的跨模态上下文而对传统的大型语言模型（LLMs）和检索增强生成（RAG）管道造成压力。我们引入了MultiFinRAG，一个专为金融问答构建的检索增强生成框架。MultiFinRAG首先通过将表格和图表图像分批发送到轻量级、量化的开源多模态LLM进行多模态提取，该LLM生成结构化JSON输出和简洁的文本摘要。这些输出与叙述性文本一起，通过模态感知的相似度阈值进行嵌入和索引，以实现精确检索。然后，分层回退策略在必要时动态地从纯文本上下文升级到文本+表格+图像上下文，从而实现跨模态推理同时减少不相关上下文。尽管在商用硬件上运行，MultiFinRAG在涉及文本、表格、图像和组合多模态推理的复杂金融问答任务上，比ChatGPT-4o（免费版）的准确率高出19个百分点。", "summary": "本研究提出了MultiFinRAG，一个专为处理复杂金融文档问答而设计的优化多模态检索增强生成（RAG）框架。针对传统LLM和RAG在处理跨模态金融数据时面临的挑战，MultiFinRAG通过创新的多模态提取（利用轻量级MLLM生成结构化和文本摘要）、模态感知嵌入索引以及动态分层回退检索策略，有效整合文本、表格和图像信息。实验结果表明，MultiFinRAG在商用硬件上运行，却能在复杂金融问答任务中比ChatGPT-4o（免费版）实现19个百分点的准确率提升。", "keywords": "金融问答, 多模态RAG, 检索增强生成, 金融文档, 跨模态推理", "comments": "MultiFinRAG的创新性在于其为金融领域定制的多模态RAG框架，特别解决了金融文档固有的复杂性和多模态性。其通过轻量级MLLM进行多模态信息提取和智能的分层检索策略，有效克服了现有模型在处理长篇、多模态金融数据时的局限性。在商用硬件上实现显著优于ChatGPT-4o的性能，凸显了其在实际应用中的高效性和实用价值。这对于金融分析、风险评估等需要精确信息检索和推理的场景具有重要意义。"}}
{"id": "2506.21512", "title": "Assessing an evolutionary search engine for small language models, prompts, and evaluation metrics", "authors": ["Cláudio Lúcio do Val Lopes", "Lucca Machado"], "summary": "The concurrent optimization of language models and instructional prompts\npresents a significant challenge for deploying efficient and effective AI\nsystems, particularly when balancing performance against computational costs\nlike token usage. This paper introduces and assesses a bi-objective\nevolutionary search engine designed to navigate this complex space, focusing\nspecifically on Small Language Models (SLMs). We employ the NSGA-II algorithm\nand prompt grammar to simultaneously optimize for task accuracy and token\nefficiency across some reasoning tasks. Our results successfully identify\ndiverse, high-performing model-prompt combinations, quantitatively revealing\nthe critical trade-off between the two objectives. This research highlights\ntask-specific affinities between particular SLMs and prompt structures (e.g.,\ninstructions, context, chain of thought). The generated practical Pareto fronts\noffer decision-makers a portfolio of optimized solutions adaptable to their\nspecific constraints. This automated approach moves beyond traditional manual\ntuning, providing a foundational framework for discovering effective human-AI\ninteraction patterns.", "comment": "14 pages, 1 figure, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.21512v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21512v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "评估用于小型语言模型、提示和评估指标的进化搜索引擎", "tldr": "本文介绍并评估了一种双目标进化搜索引擎，用于同时优化小型语言模型和提示，以平衡任务准确性和令牌效率。", "motivation": "语言模型和指令提示的同步优化在部署高效、有效的AI系统时面临重大挑战，尤其是在平衡性能与计算成本（如令牌使用）时。", "method": "本文引入并评估了一种双目标进化搜索引擎，专门针对小型语言模型（SLMs）。该研究采用NSGA-II算法和提示语法，在推理任务中同时优化任务准确性和令牌效率。", "result": "研究成功识别了多样化、高性能的模型-提示组合，定量揭示了两个目标之间的关键权衡。研究还强调了特定SLM与提示结构（如指令、上下文、思维链）之间的任务特异性亲和力。", "conclusion": "生成的实用帕累托前沿为决策者提供了可根据其特定约束调整的优化解决方案组合。这种自动化方法超越了传统的手动调优，为发现有效的人机交互模式提供了基础框架。", "translation": "语言模型和指令提示的同步优化对部署高效、有效的AI系统提出了重大挑战，尤其是在平衡性能与计算成本（如令牌使用）时。本文介绍并评估了一种双目标进化搜索引擎，旨在解决这一复杂空间，并特别关注小型语言模型（SLMs）。我们采用NSGA-II算法和提示语法，在某些推理任务中同时优化任务准确性和令牌效率。我们的结果成功识别了多样化、高性能的模型-提示组合，定量揭示了两个目标之间的关键权衡。这项研究强调了特定SLM与提示结构（例如，指令、上下文、思维链）之间任务特异性亲和力。生成的实用帕累托前沿为决策者提供了一系列可根据其特定约束进行调整的优化解决方案组合。这种自动化方法超越了传统的手动调优，为发现有效的人机交互模式提供了基础框架。", "summary": "本文介绍了一种用于小型语言模型（SLMs）和提示的双目标进化搜索引擎，旨在同时优化任务准确性和令牌效率。通过采用NSGA-II算法和提示语法，该研究成功识别了高性能的模型-提示组合，并揭示了性能与计算成本之间的权衡。此方法提供了一个自动化框架，用于生成适应特定约束的优化解决方案，从而改进人机交互模式。", "keywords": "进化搜索, 小型语言模型, 提示优化, NSGA-II, 双目标优化", "comments": "这项研究的创新之处在于提出了一个双目标进化搜索引擎，用于同时优化小型语言模型和提示，以解决性能和计算成本之间的权衡问题。通过自动化方法，它超越了传统的手动调优，为发现有效的人机交互模式提供了基础框架，对于部署高效AI系统具有重要意义。"}}
{"id": "2506.20940", "title": "Two-dimensional greedy randomized Kaczmarz methods for solving large-scale linear systems", "authors": ["Tao Li", "Meng-Long Xiao", "Xin-Fang Zhang"], "summary": "In this paper, we consider a novel two-dimensional randomized Kaczmarz method\nand its improved version with simple random sampling, which chooses two active\nrows with probability proportional to the square of their cross-product-like\nconstant, for solving large-scale linear systems. From the greedy selection\nstrategy with grasping two larger entries of the residual vector at each\niteration, we then devise a two-dimensional greedy randomized Kaczmarz method.\nTo improve the above methods further, motivated by the semi-randomized Kaczmarz\nmethod and Chebyshev's law of large numbers, we propose a two-dimensional\nsemi-randomized Kaczmarz method and its modified version with simple random\nsampling, which is particularly advantageous for big data problems.\nTheoretically, we prove that the proposed methods converge to the unique\nleast-norm solution of the consistent linear systems. Numerical results on some\npractical applications illustrate the superiority of the proposed methods\ncompared with some existing ones in terms of computing time.", "comment": "arXiv admin note: text overlap with arXiv:2506.16106", "pdf_url": "http://arxiv.org/pdf/2506.20940v1", "categories": ["math.NA", "cs.NA", "65F10, 65F20, 94A08"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20940v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "求解大规模线性系统的二维贪婪随机Kaczmarz方法", "tldr": "本文提出并分析了几种新型二维Kaczmarz方法，包括随机、贪婪随机和半随机变体，用于求解大规模线性系统。理论上证明了它们的收敛性，数值结果显示其在计算时间上优于现有方法。", "motivation": "为了有效求解大规模线性系统，并进一步改进现有Kaczmarz方法的性能，特别是针对大数据问题。", "method": "本文提出并研究了多种二维Kaczmarz方法：\n1. 二维随机Kaczmarz方法及其改进版本（通过简单随机抽样选择两行，概率与交叉积常数的平方成正比）。\n2. 二维贪婪随机Kaczmarz方法（通过贪婪选择策略，每次迭代选取残差向量中两个较大项）。\n3. 受半随机Kaczmarz方法和切比雪夫大数定律启发，提出了二维半随机Kaczmarz方法及其改进版本（适用于大数据问题）。", "result": "理论上，证明了所提出的方法收敛到一致线性系统的唯一最小范数解。数值结果表明，在计算时间方面，所提出的方法优于一些现有方法。", "conclusion": "所提出的二维随机、贪婪随机和半随机Kaczmarz方法能够有效且高效地求解大规模线性系统，并在理论上保证收敛性，在实践中展现出卓越的计算性能。", "translation": "在本文中，我们考虑了一种新颖的二维随机Kaczmarz方法及其通过简单随机抽样改进的版本，该方法以与其叉积类常数平方成比例的概率选择两个活跃行，用于求解大规模线性系统。根据每次迭代中抓住残差向量中两个较大项的贪婪选择策略，我们随后设计了一种二维贪婪随机Kaczmarz方法。为了进一步改进上述方法，受半随机Kaczmarz方法和切比雪夫大数定律的启发，我们提出了一种二维半随机Kaczmarz方法及其通过简单随机抽样修改的版本，这对于大数据问题尤其有利。理论上，我们证明了所提出的方法收敛到一致线性系统的唯一最小范数解。在一些实际应用中的数值结果表明，与现有方法相比，所提出的方法在计算时间方面具有优越性。", "summary": "本文介绍了几种新颖的二维Kaczmarz方法，用于高效求解大规模线性系统。这些方法包括二维随机Kaczmarz、二维贪婪随机Kaczmarz以及受半随机Kaczmarz和切比雪夫大数定律启发的二维半随机Kaczmarz及其变体。文章理论证明了这些方法能收敛到一致线性系统的唯一最小范数解，并通过数值实验验证了它们在计算时间上优于现有算法，特别适用于大数据场景。", "keywords": "Kaczmarz方法, 大规模线性系统, 二维, 贪婪随机, 半随机", "comments": "本文的创新点在于将Kaczmarz方法推广到二维选择策略，并结合了贪婪、随机和半随机思想，这对于处理大规模线性系统，特别是大数据问题，具有重要意义。理论收敛性证明和数值性能提升，使其在实际应用中具有潜在的优势。"}}
{"id": "2506.21269", "title": "Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou", "authors": ["Pengfei Fan", "Yuli Zhang", "Xinheng Wang", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Shangbo Wang"], "summary": "This study presents and publicly releases the Suzhou Urban Road Acoustic\nDataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive\ndata-acquisition protocols and annotation guidelines to ensure transparency and\nreproducibility of the experimental workflow. To model the coupling between\nvehicular noise and driving speed, we propose a bimodal-feature-fusion deep\nconvolutional neural network (BMCNN). During preprocessing, an adaptive\ndenoising and normalization strategy is applied to suppress environmental\nbackground interference; in the network architecture, parallel branches extract\nMel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,\nwhich are subsequently fused via a cross-modal attention mechanism in the\nintermediate feature space to fully exploit time-frequency information.\nExperimental results demonstrate that BMCNN achieves a classification accuracy\nof 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic\ndataset. Ablation studies and robustness tests on the Suzhou dataset further\nvalidate the contributions of each module to performance improvement and\noverfitting mitigation. The proposed acoustics-based speed classification\nmethod can be integrated into smart-city traffic management systems for\nreal-time noise monitoring and speed estimation, thereby optimizing traffic\nflow control, reducing roadside noise pollution, and supporting sustainable\nurban planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21269v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21269v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "整合车辆声学数据以增强城市交通管理：以苏州为例的速度分类研究", "tldr": "本研究发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种双模态特征融合深度卷积神经网络（BMCNN），用于车辆噪声与驾驶速度的耦合建模，实现了高效的速度分类，可应用于智能城市交通管理。", "motivation": "为了建模车辆噪声和驾驶速度之间的耦合关系，并利用声学数据实现城市交通管理中的实时噪声监测和速度估计，从而优化交通流控制、减少路边噪音污染并支持可持续城市规划。", "method": "本研究发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一个双模态特征融合深度卷积神经网络（BMCNN）。预处理阶段采用自适应去噪和归一化策略抑制环境背景干扰；网络架构中，并行分支提取梅尔频率倒谱系数（MFCCs）和小波包能量特征，并通过跨模态注意力机制在中间特征空间融合，以充分利用时频信息。", "result": "BMCNN在SZUR-Acoustic Dataset上实现了87.56%的分类准确率，在公共IDMT-Traffic数据集上达到了96.28%。消融研究和鲁棒性测试进一步验证了各模块对性能提升和过拟合缓解的贡献。", "conclusion": "所提出的基于声学的速度分类方法可以整合到智能城市交通管理系统中，用于实时噪声监测和速度估计，从而优化交通流控制，减少路边噪音污染，并支持可持续城市规划。", "translation": "本研究发布并公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），该数据集附有全面的数据采集协议和注释指南，以确保实验工作流程的透明性和可复现性。为了建模车辆噪声与驾驶速度之间的耦合关系，我们提出了一种双模态特征融合深度卷积神经网络（BMCNN）。在预处理过程中，采用自适应去噪和归一化策略来抑制环境背景干扰；在网络架构中，并行分支提取梅尔频率倒谱系数（MFCCs）和小波包能量特征，这些特征随后通过中间特征空间中的跨模态注意力机制进行融合，以充分利用时频信息。实验结果表明，BMCNN在SZUR-Acoustic Dataset上实现了87.56%的分类准确率，在公共IDMT-Traffic数据集上达到了96.28%。在苏州数据集上进行的消融研究和鲁棒性测试进一步验证了每个模块对性能改进和缓解过拟合的贡献。所提出的基于声学的速度分类方法可以整合到智能城市交通管理系统中，用于实时噪声监测和速度估计，从而优化交通流控制、减少路边噪音污染并支持可持续城市规划。", "summary": "本研究公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种名为BMCNN的双模态特征融合深度卷积神经网络，用于建模车辆噪声与行驶速度的耦合关系。BMCNN通过自适应去噪、MFCCs和小波包能量特征提取及跨模态注意力融合，在速度分类任务上表现出色，尤其在SZUR-Acoustic和IDMT-Traffic数据集上分别达到87.56%和96.28%的准确率。该方法可应用于智能城市交通管理系统，实现实时噪声监测和速度估计，以优化交通流、减少噪音污染并支持可持续城市规划。", "keywords": "车辆声学数据, 速度分类, 深度学习, 交通管理, 城市规划", "comments": "该研究通过发布新的数据集和提出创新的双模态特征融合深度学习模型，为城市交通管理中的声学数据应用提供了重要基础。其创新点在于结合了MFCCs和小波包能量特征，并通过跨模态注意力机制进行融合，有效提升了速度分类的准确性和鲁棒性。该方法具有实际应用潜力，可帮助构建更智能、更可持续的城市交通系统。"}}
{"id": "2506.20981", "title": "PrivacyGo: Privacy-Preserving Ad Measurement with Multidimensional Intersection", "authors": ["Jian Du", "Haohao Qian", "Shikun Zhang", "Wen-jie Lu", "Donghang Lu", "Yongchuan Niu", "Bo Jiang", "Yongjun Zhao", "Qiang Yan"], "summary": "This paper tackles the challenging and practical problem of multi-identifier\nprivate user profile matching for privacy-preserving ad measurement, a\ncornerstone of modern advertising analytics. We introduce a comprehensive\ncryptographic framework leveraging reversed Oblivious Pseudorandom Functions\n(OPRF) and novel blind key rotation techniques to support secure matching\nacross multiple identifiers. Our design prevents cross-identifier linkages and\nincludes a differentially private mechanism to obfuscate intersection sizes,\nmitigating risks such as membership inference attacks.\n  We present a concrete construction of our protocol that achieves both strong\nprivacy guarantees and high efficiency. It scales to large datasets, offering a\npractical and scalable solution for privacy-centric applications like secure ad\nconversion tracking. By combining rigorous cryptographic principles with\ndifferential privacy, our work addresses a critical need in the advertising\nindustry, setting a new standard for privacy-preserving ad measurement\nframeworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20981v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20981v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PrivacyGo：多维交集下的隐私保护广告测量", "tldr": "PrivacyGo提出了一种结合反向不经意伪随机函数和盲密钥轮换技术，并加入差分隐私机制的加密框架，用于多标识符隐私保护广告测量，实现了高效且可扩展的解决方案。", "motivation": "现代广告分析中，多标识符私有用户档案匹配是一个具有挑战性的实际问题，尤其是在隐私保护广告测量方面。当前的解决方案面临跨标识符链接和成员推断攻击的风险。", "method": "本文提出了一种全面的加密框架，利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术，以支持多标识符之间的安全匹配。该设计能防止跨标识符链接，并包含一个差分隐私机制来混淆交集大小，从而减轻成员推断攻击等风险。", "result": "所提出的协议实现了强大的隐私保证和高效率，能够扩展到大型数据集，为安全广告转化跟踪等以隐私为中心的应用程序提供了一个实用且可扩展的解决方案。", "conclusion": "本文结合严格的密码学原理与差分隐私，解决了广告行业的一个关键需求，为隐私保护广告测量框架设定了新标准。", "translation": "本文解决了多标识符私有用户档案匹配这一具有挑战性且实际的问题，这是隐私保护广告测量（现代广告分析的基石）的核心。我们引入了一个全面的加密框架，利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术，以支持跨多个标识符的安全匹配。我们的设计可防止跨标识符链接，并包含一个差分隐私机制来混淆交集大小，从而减轻成员推断攻击等风险。我们提出了一个具体的协议构建，实现了强大的隐私保证和高效率。它能够扩展到大型数据集，为安全广告转化跟踪等以隐私为中心的应用程序提供了一个实用且可扩展的解决方案。通过将严格的密码学原理与差分隐私相结合，我们的工作解决了广告行业的一个关键需求，为隐私保护广告测量框架设定了新标准。", "summary": "PrivacyGo提出了一种创新的加密框架，用于解决多标识符隐私保护广告测量中的用户档案匹配问题。该框架结合了反向不经意伪随机函数（OPRF）和盲密钥轮换技术，确保了跨标识符的安全匹配，并利用差分隐私机制来保护交集大小，有效抵御了成员推断攻击。该方案不仅提供了强大的隐私保障，还具备高效率和可扩展性，为广告行业的安全转化跟踪等应用提供了实用且可行的解决方案，为隐私保护广告测量设立了新标准。", "keywords": "隐私保护, 广告测量, OPRF, 差分隐私, 多标识符匹配", "comments": "该论文的创新点在于结合了反向不经意伪随机函数（OPRF）、盲密钥轮换技术和差分隐私，以实现多标识符下的隐私保护广告测量。其重要性在于解决了广告行业中用户数据匹配的隐私挑战，尤其是在大规模数据集下的实用性和可扩展性，这对于当前数据隐私日益受关注的环境至关重要。该框架为未来的隐私保护广告技术发展提供了坚实的基础。"}}
{"id": "2506.21537", "title": "ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers", "authors": ["Nicholas S. DiBrita", "Jason Han", "Tirthak Patel"], "summary": "Research in quantum machine learning has recently proliferated due to the\npotential of quantum computing to accelerate machine learning. An area of\nmachine learning that has not yet been explored is neural ordinary differential\nequation (neural ODE) based residual neural networks (ResNets), which aim to\nimprove the effectiveness of neural networks using the principles of ordinary\ndifferential equations. In this work, we present our insights about why analog\nRydberg atom quantum computers are especially well-suited for ResNets. We also\nintroduce ResQ, a novel framework to optimize the dynamics of Rydberg atom\nquantum computers to solve classification problems in machine learning using\nanalog quantum neural ODEs.", "comment": "ResQ will appear in the Proceedings of the IEEE International\n  Conference on Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2506.21537v1", "categories": ["quant-ph", "cs.CV", "cs.ET"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21537v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ResQ：一种在模拟里德堡原子量子计算机上实现残差神经网络的新颖框架", "tldr": "ResQ是一个新框架，用于在模拟里德堡原子量子计算机上实现基于神经ODE的残差神经网络，以解决机器学习分类问题。", "motivation": "量子机器学习正在快速发展，但基于神经ODE的残差神经网络（ResNets）在量子计算领域的探索尚不足。本研究旨在填补这一空白，并探讨模拟里德堡原子量子计算机为何特别适用于ResNets。", "method": "本文提出了ResQ框架，该框架利用模拟量子神经ODE优化里德堡原子量子计算机的动力学，以解决机器学习中的分类问题。", "result": "ResQ框架旨在利用模拟里德堡原子量子计算机解决机器学习中的分类问题。", "conclusion": "Not mentioned in abstract", "translation": "量子机器学习研究最近因量子计算加速机器学习的潜力而迅速发展。机器学习中一个尚未被探索的领域是基于神经常微分方程（neural ODE）的残差神经网络（ResNets），它旨在利用常微分方程的原理提高神经网络的有效性。在这项工作中，我们提出了关于模拟里德堡原子量子计算机为何特别适合ResNets的见解。我们还介绍了ResQ，这是一个新颖的框架，用于优化里德堡原子量子计算机的动力学，以使用模拟量子神经ODE解决机器学习中的分类问题。", "summary": "本文介绍了ResQ，一个新颖的框架，旨在利用模拟里德堡原子量子计算机实现基于神经常微分方程（neural ODE）的残差神经网络（ResNets）。该研究探讨了模拟里德堡原子量子计算机适用于ResNets的原因，并提出了ResQ框架，通过优化量子计算机的动力学来解决机器学习中的分类问题。", "keywords": "量子机器学习, 残差神经网络, 里德堡原子量子计算机, 神经常微分方程, ResQ", "comments": "这篇论文的创新点在于将神经ODE 기반的残差神经网络与模拟里德堡原子量子计算机结合，并提出了一个名为ResQ的新颖框架。它探索了一个量子机器学习中尚未被充分研究的领域，并为在特定量子硬件上实现这类网络提供了潜在的解决方案。"}}
{"id": "2506.20992", "title": "Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty", "authors": ["Craig Steven Wright"], "summary": "This paper develops a formal game-theoretic model to examine how protocol\nmutability disrupts cooperative mining behaviour in blockchain systems. Using a\nrepeated game framework with stochastic rule shocks, we show that even minor\nuncertainty in institutional rules increases time preference and induces\nstrategic deviation. Fixed-rule environments support long-term investment and\nstable equilibrium strategies; in contrast, mutable protocols lead to\nshort-termism, higher discounting, and collapse of coordinated engagement.\nSimulation results identify instability zones in the parameter space where\nrational mining gives way to extractive or arbitrage conduct. These findings\nsupport an Austrian economic interpretation: calculability requires rule\nstability. Institutional noise undermines the informational basis for\nproductive action. We conclude that protocol design must be treated as a\nconstitutional economic constraint, not a discretionary variable, if\nsustainable cooperation is to emerge in decentralised systems.", "comment": "40 pages, submitted to QJAE", "pdf_url": "http://arxiv.org/pdf/2506.20992v1", "categories": ["econ.GN", "cs.CE", "cs.CY", "cs.GT", "cs.SI", "q-fin.EC", "91A05, 91B42, 68M14, 91B62", "J.4; C.2.4; K.4.1; F.1.1"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.20992v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "制度性噪音、策略性偏差与跨期崩溃：协议不确定性下矿工行为的正式模型", "tldr": "本文通过一个正式的博弈论模型，研究了区块链协议的不确定性如何破坏合作挖矿行为，发现即使是轻微的规则不确定性也会导致矿工的短期行为和合作关系的瓦解。", "motivation": "研究协议可变性如何扰乱区块链系统中的合作挖矿行为，并探讨其对矿工行为和系统稳定性的影响。", "method": "开发了一个正式的博弈论模型，并采用具有随机规则冲击的重复博弈框架进行分析和模拟。", "result": "即使是制度规则上的微小不确定性也会增加时间偏好并诱导策略性偏差。固定规则环境支持长期投资和稳定的均衡策略，而可变协议则导致短期主义、更高的贴现和协调参与的崩溃。模拟结果识别出参数空间中的不稳定区域，其中理性挖矿让位于掠夺性或套利行为。", "conclusion": "如果要在去中心化系统中实现可持续合作，协议设计必须被视为一种宪法经济约束，而不是一个可自由裁量的变量，因为可计算性需要规则稳定性且制度性噪音会破坏生产性行动的信息基础。", "translation": "本文开发了一个正式的博弈论模型，以研究协议可变性如何破坏区块链系统中的合作挖矿行为。通过使用一个具有随机规则冲击的重复博弈框架，我们表明，即使是制度规则上的微小不确定性也会增加时间偏好并诱导策略性偏差。固定规则环境支持长期投资和稳定的均衡策略；相反，可变协议导致短期主义、更高的贴现和协调参与的崩溃。模拟结果识别出参数空间中的不稳定区域，其中理性挖矿让位于掠夺性或套利行为。这些发现支持奥地利经济学的解释：可计算性需要规则稳定性。制度性噪音破坏了生产性行动的信息基础。我们得出结论，如果要在去中心化系统中实现可持续合作，协议设计必须被视为一种宪法经济约束，而不是一个可自由裁量的变量。", "summary": "本文构建了一个正式的博弈论模型，以探讨区块链协议的不确定性如何影响矿工的合作行为。研究发现，协议规则的不确定性，即使是轻微的，也会导致矿工产生短期偏好和策略性偏差，进而破坏长期投资和合作稳定性。通过模拟，论文识别了理性挖矿行为转向掠夺或套利行为的不稳定区域。最终，论文强调协议设计应被视为去中心化系统实现可持续合作的宪法经济约束。", "keywords": "区块链, 协议不确定性, 矿工行为, 博弈论, 合作瓦解", "comments": "本文创新性地将博弈论应用于区块链协议的稳定性分析，强调了制度规则稳定性对去中心化系统长期合作的重要性。其结论对区块链协议的设计和治理提供了重要的理论指导，具有较强的现实意义。然而，模型可能未能完全捕捉现实世界中矿工行为的复杂性，且其奥地利经济学解释的适用性可能需要更广泛的讨论。"}}
{"id": "2506.20883", "title": "Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance", "authors": ["Kyanna Dagenais", "Istvan David"], "summary": "Model-driven engineering problems often require complex model transformations\n(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of\nsuch problems include model synchronization, automated model repair, and design\nspace exploration. Manually developing complex MTs is an error-prone and often\ninfeasible process. Reinforcement learning (RL) is an apt way to alleviate\nthese issues. In RL, an autonomous agent explores the state space through trial\nand error to identify beneficial sequences of actions, such as MTs. However, RL\nmethods exhibit performance issues in complex problems. In these situations,\nhuman guidance can be of high utility. In this paper, we present an approach\nand technical framework for developing complex MT sequences through RL, guided\nby potentially uncertain human advice. Our framework allows user-defined MTs to\nbe mapped onto RL primitives, and executes them as RL programs to find optimal\nMT sequences. Our evaluation shows that human guidance, even if uncertain,\nsubstantially improves RL performance, and results in more efficient\ndevelopment of complex MTs. Through a trade-off between the certainty and\ntimeliness of human advice, our method takes a step towards RL-driven\nhuman-in-the-loop engineering methods.", "comment": "Accepted for ACM/IEEE MODELS'25", "pdf_url": "http://arxiv.org/pdf/2506.20883v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20883v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "复杂模型转换：基于不确定人类指导的强化学习", "tldr": "本文提出一种利用强化学习（RL）并结合不确定人类指导的方法，以更高效地开发复杂的模型转换（MTs）。", "motivation": "模型驱动工程中手动开发复杂的模型转换（MTs）容易出错且通常不可行。尽管强化学习（RL）可以缓解这些问题，但其在复杂问题中表现出性能瓶颈。因此，需要引入人类指导来提升RL在复杂MT开发中的表现。", "method": "本文提出了一种方法和技术框架，通过强化学习（RL）在可能不确定的人类建议指导下开发复杂的MT序列。该框架允许将用户定义的MTs映射到RL原语，并将其作为RL程序执行，以找到最优的MT序列。", "result": "评估结果表明，即使是不确定的人类指导也能显著提高RL性能，并使得复杂MT的开发更加高效。", "conclusion": "通过在人类建议的确定性和及时性之间进行权衡，本文提出的方法是朝着RL驱动的人机协作工程方法迈出的一步。", "translation": "模型驱动工程问题通常需要复杂的模型转换（MTs），即需要大量序列链式连接的MTs。这类问题的相关例子包括模型同步、自动化模型修复和设计空间探索。手动开发复杂的MTs是一个容易出错且通常不可行的过程。强化学习（RL）是缓解这些问题的合适方法。在RL中，自主代理通过试错探索状态空间，以识别有益的动作序列，例如MTs。然而，RL方法在复杂问题中表现出性能问题。在这些情况下，人类指导具有很高的实用价值。在本文中，我们提出了一种通过强化学习开发复杂MT序列的方法和技术框架，该框架由可能不确定的人类建议指导。我们的框架允许将用户定义的MTs映射到RL原语上，并将其作为RL程序执行以找到最佳MT序列。我们的评估表明，人类指导，即使不确定，也能显著提高RL性能，并导致更高效的复杂MT开发。通过在人类建议的确定性和及时性之间进行权衡，我们的方法朝着RL驱动的人机协作工程方法迈进了一步。", "summary": "本文旨在解决模型驱动工程中复杂模型转换（MTs）手动开发过程中的高错误率和不可行性问题。研究提出了一种新颖的方法，将强化学习（RL）与可能不确定的人类指导相结合，以自动化和优化MT序列的开发。该框架能够将用户定义的MTs映射为RL原语并执行。实验评估证明，即使是不确定的人类指导也能显著提升RL的性能和复杂MT的开发效率，从而推动了RL驱动的人机协作工程方法的发展。", "keywords": "模型转换, 强化学习, 人类指导, 模型驱动工程, 人机协作", "comments": "本文的创新点在于将不确定的人类指导整合到强化学习中，以解决复杂模型转换的挑战。这种人机协作（human-in-the-loop）策略对于纯粹的强化学习难以应对复杂性问题至关重要，它利用了人类专业知识（即使不精确）来提供有价值的指导。对“不确定”指导的关注尤其具有洞察力，它承认了人类输入在现实世界中的局限性。"}}
{"id": "2506.20966", "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends", "authors": ["Tian-Yu Xiang", "Ao-Qun Jin", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Sheng-Bin Duan", "Fu-Chao Xie", "Wen-Kai Wang", "Si-Cheng Wang", "Ling-Yun Li", "Tian Tu", "Zeng-Guang Hou"], "summary": "Vision-language-action (VLA) models extend vision-language models (VLM) by\nintegrating action generation modules for robotic manipulation. Leveraging\nstrengths of VLM in vision perception and instruction understanding, VLA models\nexhibit promising generalization across diverse manipulation tasks. However,\napplications demanding high precision and accuracy reveal performance gaps\nwithout further adaptation. Evidence from multiple domains highlights the\ncritical role of post-training to align foundational models with downstream\napplications, spurring extensive research on post-training VLA models. VLA\nmodel post-training aims to address the challenge of improving an embodiment's\nability to interact with the environment for the given tasks, analogous to the\nprocess of humans motor skills acquisition. Accordingly, this paper reviews\npost-training strategies for VLA models through the lens of human motor\nlearning, focusing on three dimensions: environments, embodiments, and tasks. A\nstructured taxonomy is introduced aligned with human learning mechanisms: (1)\nenhancing environmental perception, (2) improving embodiment awareness, (3)\ndeepening task comprehension, and (4) multi-component integration. Finally, key\nchallenges and trends in post-training VLA models are identified, establishing\na conceptual framework to guide future research. This work delivers both a\ncomprehensive overview of current VLA model post-training methods from a human\nmotor learning perspective and practical insights for VLA model development.\n(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20966v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20966v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "VLA模型后训练与人类运动学习的并行：进展、挑战与趋势", "tldr": "本文从人类运动学习的角度综述了VLA模型后训练策略，并提出了一个分类法，以指导未来研究。", "motivation": "VLA模型在复杂操作任务中展现出泛化能力，但在需要高精度和准确性的应用中存在性能差距。后训练对于使基础模型与下游应用对齐至关重要，这类似于人类运动技能习得的过程。", "method": "本文从人类运动学习的角度，通过环境、具身和任务三个维度，回顾了VLA模型的后训练策略。引入了一个与人类学习机制对齐的结构化分类法，包括：(1) 增强环境感知，(2) 提高具身意识，(3) 深化任务理解，以及 (4) 多组件集成。", "result": "本文识别了VLA模型后训练中的关键挑战和趋势，并建立了一个概念框架以指导未来的研究。", "conclusion": "这项工作提供了从人类运动学习视角对当前VLA模型后训练方法的全面概述，以及对VLA模型开发的实用见解。", "translation": "视觉-语言-动作 (VLA) 模型通过整合用于机器人操作的动作生成模块，扩展了视觉-语言模型 (VLM)。VLA 模型利用 VLM 在视觉感知和指令理解方面的优势，在各种操作任务中展现出有前景的泛化能力。然而，需要高精度和准确性的应用在没有进一步适应的情况下会暴露出性能差距。来自多个领域的证据强调了后训练在使基础模型与下游应用对齐方面的关键作用，这促进了对后训练 VLA 模型的广泛研究。VLA 模型后训练旨在解决如何提高具身对给定任务与环境交互能力的问题，这类似于人类运动技能习得的过程。因此，本文从人类运动学习的角度，通过环境、具身和任务三个维度，回顾了 VLA 模型的后训练策略。引入了一个与人类学习机制对齐的结构化分类法：(1) 增强环境感知，(2) 提高具身意识，(3) 深化任务理解，以及 (4) 多组件集成。最后，识别了 VLA 模型后训练中的关键挑战和趋势，建立了一个概念框架以指导未来的研究。这项工作提供了从人类运动学习视角对当前 VLA 模型后训练方法的全面概述，以及对 VLA 模型开发的实用见解。", "summary": "本文综述了视觉-语言-动作 (VLA) 模型的后训练策略，旨在弥补其在精度方面的不足。作者从人类运动学习的角度，通过环境、具身和任务三个维度进行分析，并提出了一个结构化分类法。该研究识别了VLA后训练的挑战和趋势，为未来研究提供了概念框架和实用见解。", "keywords": "VLA模型, 后训练, 人类运动学习, 机器人操作, 分类法", "comments": "本文的创新之处在于将VLA模型的后训练与人类运动学习进行类比，提供了一个新颖且直观的视角来理解和分类VLA模型的适应性策略。这种跨领域的方法有助于系统化地分析现有方法，并为未来的研究指明方向。其提出的分类法具有很强的指导意义。"}}
{"id": "2506.21201", "title": "Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices", "authors": ["Zihao You", "Michael Crabb"], "summary": "The consumption of subtitles via TVs, laptops and smartphones has the\npotential to marginalize people based on their complex accessibility needs. The\ncurrent one-size-fits-all approach to this accessibility aid is no longer fit\nfor purpose and work is required to look at how it can be adapted to be\npersonalised for individual users based on individual context, content, and\nconsumption habits. People with Aphasia, for example, encounter significant\nchallenges in understanding subtitle texts.\n  We see our work as a call to action for more inclusive practices, focusing on\nhow the thoughts and opinions of people with aphasia can be included in media\nresearch. Our work investigates how to develop future media solutions for\npeople with aphasia to create a more inclusive media viewing environment. We\nbelieve the key to this is appropriate prototyping tools and methods to allow\nequitable inclusion in the system design process.", "comment": "3 pages, 1 figure, Access InContext Workshop at CHI 2025 on 26th of\n  April", "pdf_url": "http://arxiv.org/pdf/2506.21201v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21201v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "失语症患者的字幕媒体改编：持续存在的无障碍障碍和新兴设计实践", "tldr": "本研究呼吁为失语症患者开发个性化和包容性的字幕媒体解决方案，强调在系统设计过程中纳入他们的意见。", "motivation": "现有的字幕“一刀切”方法无法满足具有复杂无障碍需求的人群，特别是失语症患者，他们在理解字幕文本方面面临重大挑战。因此，需要研究如何根据个人情况、内容和消费习惯对字幕进行个性化调整，以避免边缘化。", "method": "本研究致力于调查如何为失语症患者开发未来的媒体解决方案，以创建更具包容性的媒体观看环境。其关键在于使用合适的原型工具和方法，以实现在系统设计过程中的公平包容。", "result": "Not mentioned in abstract", "conclusion": "本研究呼吁采取更具包容性的实践，重点关注如何将失语症患者的观点和意见纳入媒体研究，以开发更具包容性的媒体观看环境。", "translation": "通过电视、笔记本电脑和智能手机消费字幕有可能因其复杂的无障碍需求而边缘化人群。当前这种“一刀切”的无障碍辅助方法已不再适用，需要研究如何根据个人用户、上下文、内容和消费习惯进行个性化调整。例如，失语症患者在理解字幕文本方面遇到了重大挑战。\n我们将我们的工作视为一项行动号召，旨在推动更具包容性的实践，重点关注如何将失语症患者的思想和意见纳入媒体研究。我们的工作旨在研究如何为失语症患者开发未来的媒体解决方案，以创建更具包容性的媒体观看环境。我们认为，实现这一目标的关键是适当的原型工具和方法，以实现在系统设计过程中的公平包容。", "summary": "本研究关注字幕媒体对失语症患者造成的无障碍障碍，指出当前“一刀切”的字幕方法不适用，呼吁开发个性化且包容的媒体解决方案。文章强调将失语症患者的意见纳入媒体研究和系统设计过程的重要性，旨在通过合适的原型工具和方法，为他们创建更具包容性的媒体观看环境。", "keywords": "失语症, 字幕, 无障碍, 个性化, 包容性设计", "comments": "本文创新性地关注了失语症患者这一特定群体在字幕媒体消费中的无障碍问题，并提出了个性化和包容性设计的必要性。其重要性在于呼吁将受众群体（特别是残障人士）纳入设计过程，而非仅仅提供统一的辅助功能，这对于未来无障碍技术的发展具有指导意义。文章的局限性在于抽象中未提供具体的研究方法或初步结果，更多是提出问题和研究方向。"}}
{"id": "2506.20904", "title": "Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL", "authors": ["Matthew Zurek", "Guy Zamir", "Yudong Chen"], "summary": "We study offline reinforcement learning in average-reward MDPs, which\npresents increased challenges from the perspectives of distribution shift and\nnon-uniform coverage, and has been relatively underexamined from a theoretical\nperspective. While previous work obtains performance guarantees under\nsingle-policy data coverage assumptions, such guarantees utilize additional\ncomplexity measures which are uniform over all policies, such as the uniform\nmixing time. We develop sharp guarantees depending only on the target policy,\nspecifically the bias span and a novel policy hitting radius, yielding the\nfirst fully single-policy sample complexity bound for average-reward offline\nRL. We are also the first to handle general weakly communicating MDPs,\ncontrasting restrictive structural assumptions made in prior work. To achieve\nthis, we introduce an algorithm based on pessimistic discounted value iteration\nenhanced by a novel quantile clipping technique, which enables the use of a\nsharper empirical-span-based penalty function. Our algorithm also does not\nrequire any prior parameter knowledge for its implementation. Remarkably, we\nshow via hard examples that learning under our conditions requires coverage\nassumptions beyond the stationary distribution of the target policy,\ndistinguishing single-policy complexity measures from previously examined\ncases. We also develop lower bounds nearly matching our main result.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20904v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20904v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "平均奖励离线强化学习中的最优单策略样本复杂度和瞬态覆盖", "tldr": "本文首次为平均奖励离线RL提供了单策略样本复杂性界限，解决了分布偏移和非均匀覆盖的挑战，并引入了一种新算法，无需先验参数知识。", "motivation": "平均奖励MDPs中的离线强化学习在分布偏移和非均匀覆盖方面存在挑战，且从理论角度来看相对未被充分检验。现有工作在单策略数据覆盖下提供性能保证时，依赖于对所有策略统一的复杂性度量，而非仅针对目标策略。", "method": "本文提出了基于悲观折扣值迭代并辅以新颖分位数剪裁技术的算法，该技术实现了更尖锐的基于经验跨度的惩罚函数。该算法无需任何先验参数知识。研究通过目标策略的偏差跨度（bias span）和新颖的策略命中半径（policy hitting radius）来开发尖锐的保证。", "result": "本文首次获得了平均奖励离线RL的完全单策略样本复杂性界限。首次处理了通用的弱通信MDPs。通过困难示例证明在本文条件下学习需要超越目标策略平稳分布的覆盖假设。开发了几乎匹配主要结果的下界。", "conclusion": "本文为平均奖励离线RL提供了首个完全单策略样本复杂性界限，通过引入新颖的理论工具和算法，成功解决了分布偏移和非均匀覆盖的挑战，并证明了单策略复杂性度量与以往情况的区别，揭示了学习所需的更深层次覆盖假设。", "translation": "我们研究平均奖励MDPs中的离线强化学习，这在分布偏移和非均匀覆盖方面提出了更大的挑战，并且从理论角度来看相对未被充分检验。虽然之前的工作在单策略数据覆盖假设下获得了性能保证，但这些保证利用了对所有策略统一的额外复杂性度量，例如统一混合时间。我们开发了仅取决于目标策略的尖锐保证，特别是偏差跨度（bias span）和新颖的策略命中半径（policy hitting radius），从而产生了平均奖励离线RL的第一个完全单策略样本复杂性界限。我们也是第一个处理通用弱通信MDPs的研究，这与先前工作中做出的限制性结构假设形成对比。为了实现这一点，我们引入了一种基于悲观折扣值迭代并辅以新颖分位数剪裁技术的算法，这使得能够使用更尖锐的基于经验跨度的惩罚函数。我们的算法在实施时也不需要任何先验参数知识。值得注意的是，我们通过困难示例表明，在我们条件下学习需要超越目标策略平稳分布的覆盖假设，从而将单策略复杂性度量与先前研究的情况区分开来。我们还开发了几乎匹配我们主要结果的下界。", "summary": "本文针对平均奖励MDPs中的离线强化学习，解决了分布偏移和非均匀覆盖的理论挑战。通过引入基于目标策略的偏差跨度和新颖策略命中半径的尖锐保证，并提出一种结合悲观折扣值迭代与分位数剪裁的算法，首次实现了完全单策略样本复杂性界限，并能处理通用弱通信MDPs，且无需先验参数知识。研究还揭示了单策略复杂性需要超越目标策略平稳分布的覆盖假设，并提供了匹配的下界。", "keywords": "离线强化学习, 平均奖励MDPs, 样本复杂性, 单策略, 偏差跨度", "comments": "本文在平均奖励离线强化学习领域取得了显著进展，首次提出了完全单策略样本复杂性界限，避免了以往工作中对所有策略统一的复杂性度量，更具实际意义。其引入的新颖理论工具（如策略命中半径）和算法（悲观折扣值迭代与分位数剪裁）具有创新性，且无需先验参数知识，提升了实用性。此外，对弱通信MDPs的处理也扩展了适用范围。"}}
{"id": "2506.20994", "title": "Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe", "authors": ["Måns I. Andersson", "Martin Karp", "Niclas Jansson", "Stefano Markidis"], "summary": "With the emergence of new high-performance computing (HPC) accelerators, such\nas Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures\nhas become a major challenge for HPC application developers. The increasing\nhardware diversity in HPC systems often necessitates the development of\narchitecture-specific code, hindering the sustainability of large-scale\nscientific applications. In this work, we leverage DaCe, a data-centric\nparallel programming framework, to automate the generation of high-performance\nkernels. DaCe enables automatic code generation for multicore processors and\nvarious accelerators, reducing the burden on developers who would otherwise\nneed to rewrite code for each new architecture. Our study demonstrates DaCe's\ncapabilities by applying its automatic code generation to a critical\ncomputational kernel used in Computational Fluid Dynamics (CFD). Specifically,\nwe focus on Neko, a Fortran-based solver that employs the spectral-element\nmethod, which relies on small tensor operations. We detail the formulation of\nthis computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)\nrepresentation and discuss how this approach facilitates high-performance code\ngeneration. Additionally, we outline the workflow for seamlessly integrating\nDaCe's generated code into the Neko solver. Our results highlight the\nportability and performance of the generated code across multiple platforms,\nincluding Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive\nperformance results. By demonstrating the potential of automatic code\ngeneration, we emphasise the feasibility of using portable solutions to ensure\nthe long-term sustainability of large-scale scientific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20994v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20994v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "适用于计算流体动力学代码的DaCe可移植高性能内核生成", "tldr": "本文利用DaCe框架，实现了计算流体动力学（CFD）代码的高性能内核自动化生成，解决了多硬件架构下的代码移植和性能优化挑战，并展示了其在不同GPU平台上的可移植性和竞争力性能。", "motivation": "随着新型高性能计算（HPC）加速器的出现，高效地针对多样化硬件架构成为HPC应用开发者的主要挑战。HPC系统中日益增长的硬件多样性常常需要开发针对特定架构的代码，这阻碍了大型科学应用的可持续性。", "method": "本研究利用数据中心并行编程框架DaCe来自动化生成高性能内核。通过将DaCe的自动代码生成应用于计算流体动力学（CFD）中关键的计算内核，特别是Fortran基的Neko求解器（采用谱元法），并详细阐述了使用DaCe的Stateful Dataflow Multigraph (SDFG) 表示来构建计算内核的方法，以及将DaCe生成的代码无缝集成到Neko求解器的工作流程。", "result": "生成的代码在包括Nvidia GH200、Nvidia A100和AMD MI250X GPU在内的多个平台上展现出良好的可移植性和具有竞争力的性能。", "conclusion": "通过展示自动代码生成的潜力，本研究强调了使用可移植解决方案确保大型科学应用长期可持续性的可行性。", "translation": "随着Nvidia和AMD GPU等新型高性能计算（HPC）加速器的出现，高效地针对多样化硬件架构已成为HPC应用开发者的主要挑战。HPC系统中日益增长的硬件多样性常常需要开发针对特定架构的代码，这阻碍了大型科学应用的可持续性。在这项工作中，我们利用DaCe（一个以数据为中心的并行编程框架）来自动化生成高性能内核。DaCe能够为多核处理器和各种加速器自动生成代码，从而减轻了开发者为每种新架构重写代码的负担。我们的研究通过将DaCe的自动代码生成应用于计算流体动力学（CFD）中使用的关键计算内核，展示了DaCe的能力。具体来说，我们专注于Neko，一个基于Fortran的求解器，它采用谱元法，依赖于小型张量操作。我们详细阐述了使用DaCe的Stateful Dataflow Multigraph (SDFG) 表示来构建这个计算内核，并讨论了这种方法如何促进高性能代码生成。此外，我们概述了将DaCe生成的代码无缝集成到Neko求解器中的工作流程。我们的结果突出了生成代码在多个平台上的可移植性和性能，包括Nvidia GH200、Nvidia A100和AMD MI250X GPU，并取得了有竞争力的性能结果。通过展示自动代码生成的潜力，我们强调了使用可移植解决方案确保大型科学应用长期可持续性的可行性。", "summary": "本文利用数据中心并行编程框架DaCe，实现了计算流体动力学（CFD）代码高性能内核的自动化生成，旨在解决HPC领域日益增长的硬件多样性带来的代码开发和维护挑战。研究通过将DaCe应用于基于Fortran的Neko求解器中的关键计算内核，展示了其在多核处理器和多种加速器上自动生成代码的能力。结果表明，DaCe生成的代码在包括Nvidia和AMD GPU在内的多种平台上具有良好的可移植性和竞争力性能，证明了自动化代码生成在保障大型科学应用长期可持续性方面的潜力。", "keywords": "DaCe, 高性能计算, 自动代码生成, 计算流体动力学, 可移植性", "comments": "该论文的创新点在于利用DaCe这一数据中心并行编程框架，有效地解决了高性能计算（HPC）领域中异构硬件架构下代码移植和性能优化的难题。它通过自动化内核生成，显著减轻了开发者为不同架构重写代码的负担，提升了大型科学应用（如CFD）的可持续性。其重要性在于为未来HPC应用开发提供了一种更高效、更具可移植性的解决方案。"}}
{"id": "2506.20745", "title": "Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates", "authors": ["C. Betegón", "C. Rodríguez", "E. Martínez-Pañeda", "R. M. McMeeking"], "summary": "The exceptional adhesion properties of biological fibrillar structures --\nsuch as those found in geckos -- have inspired the development of synthetic\nadhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated\nsuperior pull-off strength compared to other geometries. In this study, we\nemploy a computational approach based on a Dugdale cohesive zone model to\nanalyze the detachment behavior of these fibrils when adhered to a rigid\nsubstrate. The results provide complete pull-off curves, revealing that the\nseparation process is inherently unstable under load control, regardless of\nwhether detachment initiates at the fibril edge or center. Our findings show\nthat fibrils with a wide, thin mushroom cap effectively reduce stress\nconcentrations and promote central detachment, leading to enhanced adhesion.\nHowever, detachment from the center is not observed in all geometries, whereas\nedge detachment can occur under certain conditions in all cases. Additionally,\nwe investigate the impact of adhesion defects at the fibril center, showing\nthat they can significantly reduce pull-off strength, particularly at high\nvalues of the dimensionless parameter \\c{hi}. These insights contribute to the\noptimization of bio-inspired adhesives and microstructured surfaces for various\nengineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20745v1", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cs.CE", "physics.app-ph", "physics.bio-ph"], "cate": "cond-mat.soft", "url": "http://arxiv.org/abs/2506.20745v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "蘑菇状纤维粘附到刚性基底的脱离强度", "tldr": "本研究利用计算方法分析了蘑菇状纤维粘附到刚性基底时的脱离行为，发现分离过程在载荷控制下不稳定，宽而薄的蘑菇状帽可以增强粘附力，且粘附缺陷会显著降低脱离强度。", "motivation": "壁虎等生物纤维结构卓越的粘附特性启发了合成粘附表面的开发。其中，蘑菇状纤维相比其他几何形状表现出更优异的脱离强度，因此本研究旨在深入理解其脱离行为。", "method": "本研究采用基于Dugdale内聚力区模型的计算方法，分析了蘑菇状纤维粘附到刚性基底时的脱离行为。", "result": "研究结果提供了完整的脱离曲线，揭示了分离过程在载荷控制下本质上是不稳定的，无论脱离是从纤维边缘还是中心开始。研究发现，具有宽而薄的蘑菇状帽的纤维能有效减少应力集中并促进中心脱离，从而增强粘附力。然而，并非所有几何形状都观察到中心脱离，而在某些条件下所有情况下都可能发生边缘脱离。此外，粘附缺陷会显著降低脱离强度，尤其是在无量纲参数\\c{hi}值较高时。", "conclusion": "这些研究结果有助于优化仿生粘合剂和微结构表面，以应用于各种工程领域。", "translation": "生物纤维结构（如壁虎的纤维结构）卓越的粘附特性启发了合成粘附表面的开发。其中，蘑菇状纤维相比其他几何形状表现出更优异的脱离强度。在本研究中，我们采用基于Dugdale内聚力区模型的计算方法，分析了这些纤维粘附到刚性基底时的脱离行为。结果提供了完整的脱离曲线，揭示了分离过程在载荷控制下本质上是不稳定的，无论脱离是从纤维边缘还是中心开始。我们的研究发现，具有宽而薄的蘑菇状帽的纤维能有效减少应力集中并促进中心脱离，从而增强粘附力。然而，并非所有几何形状都观察到中心脱离，而在某些条件下所有情况下都可能发生边缘脱离。此外，我们研究了纤维中心粘附缺陷的影响，结果表明它们会显著降低脱离强度，尤其是在无量纲参数\\c{hi}值较高时。这些见解有助于优化仿生粘合剂和微结构表面，以应用于各种工程领域。", "summary": "本研究利用基于Dugdale内聚力区模型的计算方法，深入分析了蘑菇状纤维粘附到刚性基底时的脱离行为。研究发现，分离过程在载荷控制下本质上不稳定。具有宽而薄的蘑菇状帽的纤维能有效减少应力集中，促进中心脱离，从而增强粘附力。然而，中心脱离并非在所有几何形状中都出现，而边缘脱离在特定条件下总是可能发生。此外，纤维中心的粘附缺陷会显著降低脱离强度。这些发现为优化仿生粘合剂和微结构表面提供了重要见解。", "keywords": "蘑菇状纤维, 脱离强度, 计算模型, 粘附, 仿生材料", "comments": "该研究通过计算方法深入探讨了蘑菇状纤维的脱离机制，创新性地揭示了分离过程的不稳定性以及帽形结构对粘附性能的影响。对于仿生粘合剂的设计和优化具有重要的指导意义，特别是其对缺陷敏感性的分析，为实际应用提供了宝贵的参考。"}}
{"id": "2506.20705", "title": "On Convolutions, Intrinsic Dimension, and Diffusion Models", "authors": ["Kin Kwan Leung", "Rasa Hosseinzadeh", "Gabriel Loaiza-Ganem"], "summary": "The manifold hypothesis asserts that data of interest in high-dimensional\nambient spaces, such as image data, lies on unknown low-dimensional\nsubmanifolds. Diffusion models (DMs) -- which operate by convolving data with\nprogressively larger amounts of Gaussian noise and then learning to revert this\nprocess -- have risen to prominence as the most performant generative models,\nand are known to be able to learn distributions with low-dimensional support.\nFor a given datum in one of these submanifolds, we should thus intuitively\nexpect DMs to have implicitly learned its corresponding local intrinsic\ndimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari\net al. (2024b) recently showed that this is indeed the case by linking this LID\nto the rate of change of the log marginal densities of the DM with respect to\nthe amount of added noise, resulting in an LID estimator known as FLIPD. LID\nestimators such as FLIPD have a plethora of uses, among others they quantify\nthe complexity of a given datum, and can be used to detect outliers,\nadversarial examples and AI-generated text. FLIPD achieves state-of-the-art\nperformance at LID estimation, yet its theoretical underpinnings are incomplete\nsince Kamkari et al. (2024b) only proved its correctness under the highly\nunrealistic assumption of affine submanifolds. In this work we bridge this gap\nby formally proving the correctness of FLIPD under realistic assumptions.\nAdditionally, we show that an analogous result holds when Gaussian convolutions\nare replaced with uniform ones, and discuss the relevance of this result.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20705v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20705v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "关于卷积、内在维度和扩散模型", "tldr": "本文通过在更真实的假设下证明FLIPD估计器的正确性，弥补了扩散模型中局部内在维度（LID）估计的理论空白，并展示了均匀卷积的类似结果。", "motivation": "FLIPD在局部内在维度（LID）估计方面达到了最先进的性能，但其理论基础不完整，因为之前的研究仅在高度不现实的仿射子流形假设下证明了其正确性。本文旨在弥补这一理论空白，并在更现实的假设下形式化证明FLIPD的正确性。", "method": "本文通过形式化证明了FLIPD在现实假设下的正确性。此外，研究还展示了当高斯卷积被均匀卷积取代时，一个类似的结果仍然成立。", "result": "成功在现实假设下形式化证明了FLIPD（一种局部内在维度估计器）的正确性。同时，研究还发现当高斯卷积被均匀卷积替换时，一个类似的结果也成立。", "conclusion": "本文弥补了FLIPD局部内在维度估计器理论基础的空白，通过在现实假设下证明其正确性，并将其适用性扩展到均匀卷积，从而增强了其在量化数据复杂性、检测异常值和AI生成内容等应用中的可靠性。", "translation": "流形假设断言，高维环境空间中感兴趣的数据（例如图像数据）位于未知的低维子流形上。扩散模型（DMs）通过将数据与逐渐增大的高斯噪声进行卷积，然后学习逆转这一过程，已成为性能最佳的生成模型，并已知能够学习具有低维支持的分布。对于这些子流形中的给定数据，我们直观地期望DMs能够隐式学习其对应的局部内在维度（LID），即其所属子流形的维度。Kamkari 等人（2024b）最近通过将LID与DM的对数边际密度相对于添加噪声量的变化率联系起来，证明了确实如此，从而产生了名为FLIPD的LID估计器。FLIPD等LID估计器用途广泛，它们可以量化给定数据的复杂性，并可用于检测异常值、对抗性样本和AI生成文本。FLIPD在LID估计方面取得了最先进的性能，但其理论基础不完整，因为Kamkari 等人（2024b）仅在高度不现实的仿射子流形假设下证明了其正确性。在这项工作中，我们通过在现实假设下形式化证明FLIPD的正确性来弥补这一空白。此外，我们还表明，当高斯卷积被均匀卷积取代时，一个类似的结果仍然成立，并讨论了这一结果的相关性。", "summary": "本文解决了扩散模型中局部内在维度（LID）估计器FLIPD的理论基础不完整的问题。之前的研究仅在不现实的仿射子流形假设下证明了FLIPD的正确性。本工作通过在更现实的假设下形式化证明FLIPD的正确性来弥补这一理论空白。此外，研究还证明了当高斯卷积被均匀卷积取代时，一个类似的LID估计结果也成立，扩展了FLIPD的理论适用性。", "keywords": "卷积, 内在维度, 扩散模型, FLIPD, 流形假设", "comments": "本文的创新之处在于它弥补了FLIPD这一先进LID估计器在理论基础上的重要空白，使其在更广泛、更真实的场景下具备了形式化的正确性证明。这对于依赖LID进行数据复杂性量化、异常检测和AI内容识别的应用具有重要意义，增强了FLIPD的可靠性和实用性。同时，对均匀卷积的扩展也展现了其理论的普适性。"}}
{"id": "2506.20987", "title": "Optimal Parameter Design for Power Electronic Converters Using a Probabilistic Learning-Based Stochastic Surrogate Model", "authors": ["Akash Mahajan", "Shivam Chaturvedi", "Srijita Das", "Wencong Su", "Van-Hai Bui"], "summary": "The selection of optimal design for power electronic converter parameters\ninvolves balancing efficiency and thermal constraints to ensure high\nperformance without compromising safety. This paper introduces a\nprobabilistic-learning-based stochastic surrogate modeling framework to address\nthis challenge and significantly reduce the time required during the design\nphase. The approach begins with a neural network classifier that evaluates the\nfeasibility of parameter configurations, effectively filtering out unsafe\nand/or impractical inputs. Subsequently, a probabilistic prediction model\nestimates the converter's efficiency and temperature while quantifying\nprediction uncertainty, providing both performance insights and reliability\nmetrics. Finally, a heuristic optimization-based model is employed to optimize\na multi-objective function that maximizes efficiency while adhering to thermal\nconstraints. The optimization process incorporates penalty terms to discourage\nsolutions that violate practical thresholds, ensuring actionable and realistic\nrecommendations. An advanced heuristic optimization method is used to find the\noptimal solution and is compared with several well-known search algorithms,\nincluding Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Simulated\nAnnealing (SA), Tabu-Search (TS), and Stochastic Hill Climbing (SHC). The\nresults demonstrate significant improvements in predictive accuracy and\noptimization outcomes, offering a robust solution for advancing power\nelectronics design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20987v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20987v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于概率学习随机代理模型的电力电子变换器最佳参数设计", "tldr": "本文提出了一种基于概率学习的随机代理建模框架，用于电力电子变换器的最佳参数设计，以平衡效率和热约束，并显著缩短设计时间。", "motivation": "电力电子变换器参数的最佳设计需要在效率和热约束之间取得平衡，以确保高性能和安全性。此外，传统设计过程耗时较长，需要显著缩短设计时间。", "method": "本文引入了一种基于概率学习的随机代理建模框架。首先，使用神经网络分类器评估参数配置的可行性，过滤不安全/不切实际的输入。其次，概率预测模型估计变换器的效率和温度，并量化预测不确定性。最后，采用基于启发式优化的模型来优化多目标函数，以在满足热约束的同时最大化效率，并加入惩罚项以避免不切实际的解决方案。该方法与遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）、禁忌搜索（TS）和随机爬山（SHC）等多种已知搜索算法进行了比较。", "result": "结果表明，该方法在预测精度和优化结果方面均有显著改进。", "conclusion": "该框架为推进电力电子设计提供了一个鲁棒的解决方案。", "translation": "电力电子变换器参数的最佳设计涉及平衡效率和热约束，以确保高性能而不损害安全性。本文介绍了一种基于概率学习的随机代理建模框架，以解决这一挑战并显著减少设计阶段所需的时间。该方法首先通过一个神经网络分类器评估参数配置的可行性，有效过滤掉不安全和/或不切实际的输入。随后，一个概率预测模型估计变换器的效率和温度，同时量化预测不确定性，提供性能洞察和可靠性指标。最后，采用基于启发式优化的模型来优化一个多目标函数，该函数在遵守热约束的同时最大化效率。优化过程结合了惩罚项，以阻止违反实际阈值的解决方案，确保可操作和实际的建议。使用先进的启发式优化方法寻找最优解，并与几种知名搜索算法进行比较，包括遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）、禁忌搜索（TS）和随机爬山（SHC）。结果表明，在预测精度和优化结果方面有显著改进，为推进电力电子设计提供了鲁棒的解决方案。", "summary": "本文提出了一种基于概率学习的随机代理建模框架，用于电力电子变换器的最佳参数设计。该框架通过神经网络分类器筛选可行参数，接着利用概率预测模型估计效率和温度并量化不确定性，最后通过启发式优化模型在满足热约束下最大化效率。该方法显著缩短了设计时间，并在预测精度和优化结果上优于多种现有优化算法，为电力电子设计提供了一个鲁棒且高效的解决方案。", "keywords": "电力电子变换器, 参数设计, 概率学习, 随机代理模型, 启发式优化", "comments": "该论文的创新之处在于结合了概率学习和随机代理建模，以解决电力电子变换器参数设计的复杂性。通过引入神经网络分类器进行可行性过滤和概率预测模型量化不确定性，提高了设计的效率和鲁棒性。与传统优化算法的比较也验证了其优越性，为工业应用提供了有价值的工具。"}}
{"id": "2506.20795", "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20795v1", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20795v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基础模型与基于骨架的方法在人机交互手势识别中的比较如何？", "tldr": "本研究比较了基础模型（V-JEPA、Gemini Flash 2.0）与基于骨架的方法（HD-GCN）在人机交互手势识别中的表现，并引入了NUGGET数据集。结果显示HD-GCN性能最佳，但V-JEPA表现接近，预示着基础模型可能简化系统。Gemini在零样本设置下表现不佳。", "motivation": "手势是人机非语言交流的关键，尤其在嘈杂环境中。传统的深度学习手势识别依赖于复杂的、针对特定任务的架构。本研究的动机在于探究视觉基础模型（VFMs）和视觉语言模型（VLMs）是否能凭借其强大的泛化能力，通过取代专用任务模块来降低系统复杂性。", "method": "本研究调查了如何将VFM（V-JEPA）和VLM（Gemini Flash 2.0）应用于动态全身手势识别，并将其与表现顶尖的基于骨架的方法（HD-GCN）进行了比较。为评估这些手势识别方法，研究引入了一个专为内部物流环境中人机通信定制的数据集NUGGET。", "result": "实验结果显示，HD-GCN取得了最佳性能。然而，V-JEPA通过一个简单的、针对特定任务的分类头也取得了接近的性能。相比之下，Gemini Flash 2.0在零样本设置下，仅凭文本描述难以区分手势。", "conclusion": "尽管传统的基于骨架的方法（HD-GCN）目前表现最佳，但V-JEPA等基础模型在手势识别方面显示出降低系统复杂性的潜力，尤其作为共享多任务模型。对于Gemini这类模型，需要进一步研究适合手势的输入表示形式。", "translation": "手势使非语言人机通信成为可能，尤其是在敏捷生产等嘈杂环境中。传统的基于深度学习的手势识别依赖于使用图像、视频或骨架姿态估计作为输入的特定任务架构。与此同时，视觉基础模型（VFMs）和视觉语言模型（VLMs）凭借其强大的泛化能力，有望通过取代专用任务模块来降低系统复杂性。本研究调查了这些模型在动态全身手势识别中的应用，比较了V-JEPA（最先进的VFM）、Gemini Flash 2.0（多模态VLM）和HD-GCN（表现顶尖的基于骨架的方法）。我们引入了NUGGET数据集，该数据集专为内部物流环境中的人机通信而定制，用于评估不同的手势识别方法。在我们的实验中，HD-GCN取得了最佳性能，但V-JEPA通过一个简单的、针对特定任务的分类头也取得了接近的性能——从而为通过将其用作共享多任务模型来降低系统复杂性铺平了可能的道路。相比之下，Gemini在零样本设置下，仅凭文本描述难以区分手势，这突出表明需要进一步研究适合手势的输入表示形式。", "summary": "本研究评估了基础模型（V-JEPA、Gemini Flash 2.0）与基于骨架的方法（HD-GCN）在人机交互中动态全身手势识别方面的性能。研究引入了NUGGET数据集进行评估。结果显示，HD-GCN表现最佳，但V-JEPA通过简单的分类头也取得了接近的性能，表明其在简化系统复杂性方面的潜力。Gemini Flash 2.0在零样本设置下，仅依赖文本描述时识别手势表现不佳，这强调了对手势合适输入表示进行进一步研究的必要性。", "keywords": "手势识别, 基础模型, 人机交互, 骨架方法, 多模态模型", "comments": "本论文对新兴的基础模型与现有基于骨架的方法在人机交互手势识别中的应用进行了及时比较。V-JEPA虽非最先进，但仅通过简单分类头便取得接近的竞争力，这一发现具有重要意义，预示着未来人机交互系统可能实现更通用、更低复杂度的发展。Gemini Flash 2.0在零样本文本描述下的识别困境，揭示了当前视觉语言模型在手势理解方面的一个关键局限，并强调了更精细的多模态整合或专用手势表示的必要性。此外，NUGGET数据集的引入也是对该领域的一项宝贵贡献。"}}
{"id": "2506.21215", "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?", "authors": ["Haoang Chi", "He Li", "Wenjing Yang", "Feng Liu", "Long Lan", "Xiaoguang Ren", "Tongliang Liu", "Bo Han"], "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.", "comment": "24 pages, accepted at NeurIPS 2024", "pdf_url": "http://arxiv.org/pdf/2506.21215v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21215v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "揭示大型语言模型中的因果推理：现实还是幻象？", "tldr": "研究发现大型语言模型（LLMs）仅能进行浅层（一级）因果推理，缺乏真正的人类级（二级）推理能力。通过引入新的基准CausalProbe-2024和提出G^2-Reasoner方法，证明了LLMs的局限性并提供了一种增强其因果推理能力的新途径。", "motivation": "大型语言模型的因果推理能力对于其迈向强人工智能至关重要。尽管LLMs似乎展示了理解上下文因果关系的能力，但仍不清楚它们是否进行真正类似人类的因果推理。", "method": "研究方法上，深入探讨了基于Transformer的LLMs的自回归机制，揭示其并非内在因果。实验上，引入了一个新的因果问答基准CausalProbe-2024，其语料库对于LLMs来说是新鲜且几乎未见的。为弥合与二级因果推理的差距，提出G^2-Reasoner方法，该方法将通用知识和目标导向提示融入LLMs的因果推理过程。", "result": "LLMs仅能执行浅层（一级）因果推理，这主要归因于其参数中嵌入的因果知识，但缺乏真正类人（二级）因果推理的能力。LLMs在CausalProbe-2024上的表现与早期基准相比显著下降，表明它们主要进行一级因果推理。实验证明G^2-Reasoner显著增强了LLMs的因果推理能力，特别是在新鲜和反事实的上下文中。", "conclusion": "这项工作为LLMs超越一级、迈向二级、实现真正因果推理指明了一条新路径。", "translation": "因果推理能力对于推动大型语言模型（LLMs）迈向强大的人工智能至关重要。尽管多功能LLMs似乎已经展示了理解上下文因果关系并提供符合因果律的响应的能力，但它们是否执行真正类似人类的因果推理仍不清楚。然而，目前的证据表明情况恰恰相反。具体来说，LLMs仅能执行浅层（一级）因果推理，这主要归因于其参数中嵌入的因果知识，但它们缺乏真正类人（二级）因果推理的能力。为了支持这一假设，在方法论上，我们深入探讨了基于Transformer的LLMs的自回归机制，揭示它并非内在因果。在经验上，我们引入了一个新的因果问答基准CausalProbe-2024，其语料库对于所研究的LLMs来说是新鲜且几乎未见的。LLMs在CausalProbe-2024上的表现与早期基准相比显著下降，这表明它们主要进行一级因果推理。为了弥合与二级因果推理的差距，我们从人类推理通常由通用知识和预期目标促进的事实中获得启发。我们提出了G^2-Reasoner，一种将通用知识和目标导向提示融入LLMs因果推理过程的方法。实验表明G^2-Reasoner显著增强了LLMs的因果推理能力，特别是在新鲜和反事实的上下文中。这项工作为LLMs超越一级、迈向二级、实现真正因果推理指明了一条新路径。", "summary": "本研究旨在探究大型语言模型（LLMs）是否具备真正的因果推理能力。研究发现LLMs目前仅能进行浅层（一级）因果推理，这源于其参数中的嵌入知识，而非深层的人类级（二级）推理。通过分析Transformer的自回归机制，并引入新基准CausalProbe-2024，证实了LLMs在处理新颖因果情境时的性能下降。为提升LLMs的因果推理能力，论文提出了G^2-Reasoner方法，该方法通过整合通用知识和目标导向提示来模拟人类推理过程。实验结果表明，G^2-Reasoner显著增强了LLMs在新鲜和反事实语境下的因果推理能力，为LLMs向更高级别因果推理发展开辟了新途径。", "keywords": "因果推理, 大型语言模型, CausalProbe-2024, G^2-Reasoner, 二级因果推理", "comments": "这项研究深入探讨了LLMs因果推理能力的本质局限性，区分了其现有的浅层（一级）能力与人类深层（二级）推理的差异，具有重要的理论意义。通过提出新的评估基准CausalProbe-2024和创新方法G^2-Reasoner，为量化和提升LLMs的因果推理能力提供了具体工具和方向。G^2-Reasoner的提出，特别是其结合通用知识和目标导向提示的思路，为未来LLMs向更高级智能发展提供了有益的启发。"}}
{"id": "2506.21162", "title": "A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation", "authors": ["Shuwei Xing", "Derek W. Cool", "David Tessier", "Elvis C. S. Chen", "Terry M. Peters", "Aaron Fenster"], "summary": "3D ultrasound (US) imaging has shown significant benefits in enhancing the\noutcomes of percutaneous liver tumour ablation. Its clinical integration is\ncrucial for transitioning 3D US into the therapeutic domain. However,\nchallenges of tumour identification in US images continue to hinder its broader\nadoption. In this work, we propose a novel framework for integrating 3D US into\nthe standard ablation workflow. We present a key component, a clinically viable\n2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to\nreduce registration complexity. To facilitate efficient verification of the\nregistration workflow, we also propose an intuitive multimodal image\nvisualization technique. In our study, 2D US-CT/MRI registration achieved a\nlandmark distance error of approximately 2-4 mm with a runtime of 0.22s per\nimage pair. Additionally, non-rigid registration reduced the mean alignment\nerror by approximately 40% compared to rigid registration. Results demonstrated\nthe efficacy of the proposed 2D US-CT/MRI registration workflow. Our\nintegration framework advanced the capabilities of 3D US imaging in improving\npercutaneous tumour ablation, demonstrating the potential to expand the\ntherapeutic role of 3D US in clinical interventions.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21162v1", "categories": ["eess.IV", "cs.AI"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21162v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "整合三维超声用于经皮肝肿瘤消融的新颖框架", "tldr": "提出一种将3D超声整合到肝肿瘤消融工作流程的新框架，通过2D超声-CT/MRI配准实现，有效提高了肿瘤消融的精度和效率。", "motivation": "3D超声对经皮肝肿瘤消融有益，但肿瘤识别困难阻碍其临床应用，需要将其整合到治疗领域。", "method": "提出了一个将3D超声整合到标准消融工作流程的新框架，核心是利用3D超声作为中介的2D超声-CT/MRI配准方法以降低复杂性，并提出了一种多模态图像可视化技术以验证配准工作流。", "result": "2D US-CT/MRI配准的标志点距离误差约为2-4毫米，每对图像运行时间0.22秒；非刚性配准将平均对齐误差比刚性配准降低了约40%。", "conclusion": "所提出的2D US-CT/MRI配准工作流是有效的，该整合框架提升了3D超声在改善经皮肿瘤消融方面的能力，并展示了扩展3D超声在临床干预中治疗作用的潜力。", "translation": "三维超声（US）成像在提高经皮肝肿瘤消融效果方面显示出显著益处。其临床整合对于将三维超声转化为治疗领域至关重要。然而，超声图像中肿瘤识别的挑战持续阻碍其更广泛的应用。在这项工作中，我们提出了一种将三维超声整合到标准消融工作流程的新颖框架。我们提出了一个关键组件，即一种临床可行的二维超声-CT/MRI配准方法，该方法利用三维超声作为中介以降低配准复杂性。为了促进配准工作流程的有效验证，我们还提出了一种直观的多模态图像可视化技术。在我们的研究中，二维超声-CT/MRI配准实现了约2-4毫米的标志点距离误差，每对图像的运行时间为0.22秒。此外，与刚性配准相比，非刚性配准将平均对齐误差降低了约40%。结果证明了所提出的二维超声-CT/MRI配准工作流程的有效性。我们的整合框架提升了三维超声成像在改善经皮肿瘤消融方面的能力，展示了扩展三维超声在临床干预中治疗作用的潜力。", "summary": "本文提出了一种新颖的框架，旨在将3D超声成像技术整合到经皮肝肿瘤消融的标准工作流程中。为了解决3D超声在肿瘤识别方面的挑战并促进其临床应用，该框架的核心是一个临床可行的2D超声-CT/MRI配准方法，该方法利用3D超声作为中介以简化配准过程。研究结果表明，该配准方法具有高精度（2-4毫米误差）和高效率（0.22秒运行时间），并且非刚性配准显著降低了对齐误差。这表明所提出的整合框架能够有效提升3D超声在改善经皮肿瘤消融方面的能力，具有重要的临床应用潜力。", "keywords": "3D超声, 肝肿瘤消融, 图像配准, 多模态图像, 介入治疗", "comments": "本文提出了一种创新的方法，通过引入3D超声作为中介来简化2D超声与CT/MRI图像的配准，有效解决了3D超声在肝肿瘤消融中肿瘤识别的难题。其贡献在于提供了一个实用且高效的临床整合框架，显著提升了经皮肿瘤消融的精度和效率，有望扩大3D超声在介入治疗中的应用。"}}
{"id": "2506.20863", "title": "Quantum-Accelerated Wireless Communications: Concepts, Connections, and Implications", "authors": ["Naoki Ishikawa", "Giuseppe Thadeu Freitas de Abreu", "Petar Popovski", "Robert W. Heath Jr"], "summary": "Quantum computing is poised to redefine the algorithmic foundations of\ncommunication systems. While quantum superposition and entanglement enable\nquadratic or exponential speedups for specific problems, identifying use cases\nwhere these advantages yield engineering benefits is, however, still\nnontrivial. This article presents the fundamentals of quantum computing in a\nstyle familiar to the communications society, outlining the current limits of\nfault-tolerant quantum computing and uncovering a mathematical harmony between\nquantum and wireless systems, which makes the topic more enticing to wireless\nresearchers. Based on a systematic review of pioneering and state-of-the-art\nstudies, we distill common design trends for the research and development of\nquantum-accelerated communication systems and highlight lessons learned. The\nkey insight is that classical heuristics can sharpen certain quantum\nparameters, underscoring the complementary strengths of classical and quantum\ncomputing. This article aims to catalyze interdisciplinary research at the\nfrontier of quantum information processing and future communication systems.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20863v1", "categories": ["eess.SP", "quant-ph"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20863v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "量子加速无线通信：概念、连接和影响", "tldr": "本文探讨量子计算如何应用于无线通信，揭示量子与经典计算的互补优势，旨在促进跨学科研究。", "motivation": "量子计算有望重塑通信系统的算法基础，但如何将其优势转化为实际工程效益仍具挑战性。本文旨在为通信领域研究者阐明量子计算基础及其与无线系统的数学关联，以促进相关研究。", "method": "本文通过以通信领域研究者熟悉的方式介绍量子计算基础知识，概述容错量子计算的当前局限性，并揭示量子与无线系统之间的数学和谐。此外，还系统回顾了前沿研究，提炼了量子加速通信系统的设计趋势并总结了经验教训。", "result": "关键发现是经典启发式算法可以优化某些量子参数，突显了经典计算和量子计算的互补优势。", "conclusion": "本文旨在促进量子信息处理与未来通信系统前沿领域的跨学科研究。", "translation": "量子计算有望重新定义通信系统的算法基础。虽然量子叠加和纠缠能为特定问题带来二次方或指数级的加速，但识别这些优势能够产生工程效益的用例仍然并非易事。本文以通信领域熟悉的方式介绍了量子计算的基础知识，概述了容错量子计算的当前局限性，并揭示了量子与无线系统之间的数学和谐，这使得该主题对无线研究人员更具吸引力。基于对开创性和最先进研究的系统回顾，我们提炼出量子加速通信系统研究与开发的常见设计趋势，并强调了所汲取的经验教训。关键的洞察是经典启发式算法可以优化某些量子参数，突显了经典和量子计算的互补优势。本文旨在催化量子信息处理和未来通信系统前沿领域的跨学科研究。", "summary": "本文探讨了量子计算如何应用于无线通信系统，旨在弥合量子计算与通信领域之间的知识鸿沟。文章介绍了量子计算的基础，分析了容错量子计算的局限性，并揭示了量子与无线系统之间的数学联系。通过系统回顾现有研究，作者提炼出量子加速通信系统的设计趋势和经验教训，并强调了经典启发式算法在优化量子参数方面的互补优势。该研究旨在促进量子信息处理与未来通信系统领域的跨学科合作。", "keywords": "量子计算, 无线通信, 跨学科研究, 量子加速, 经典启发式算法", "comments": "这篇论文通过将复杂的量子计算概念以通信领域研究者熟悉的方式进行阐述，有效地降低了跨学科研究的门槛。其创新之处在于不仅探讨了量子加速通信的潜力，更强调了经典与量子计算的互补性，为实际应用提供了更具操作性的视角。这对于推动量子技术在无线通信领域的落地具有重要意义。"}}
{"id": "2506.21272", "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character", "authors": ["Jiayi Zheng", "Xiaodong Cun"], "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon\nvideos from a single child's drawing, while faithfully preserving its unique\nartistic style. Unlike previous storytelling methods that primarily focus on\ncharacter consistency and basic motion, FairyGen explicitly disentangles\ncharacter modeling from stylized background generation and incorporates\ncinematic shot design to support expressive and coherent storytelling. Given a\nsingle character sketch, we first employ an MLLM to generate a structured\nstoryboard with shot-level descriptions that specify environment settings,\ncharacter actions, and camera perspectives. To ensure visual consistency, we\nintroduce a style propagation adapter that captures the character's visual\nstyle and applies it to the background, faithfully retaining the character's\nfull visual identity while synthesizing style-consistent scenes. A shot design\nmodule further enhances visual diversity and cinematic quality through frame\ncropping and multi-view synthesis based on the storyboard. To animate the\nstory, we reconstruct a 3D proxy of the character to derive physically\nplausible motion sequences, which are then used to fine-tune an MMDiT-based\nimage-to-video diffusion model. We further propose a two-stage motion\ncustomization adapter: the first stage learns appearance features from\ntemporally unordered frames, disentangling identity from motion; the second\nstage models temporal dynamics using a timestep-shift strategy with frozen\nidentity weights. Once trained, FairyGen directly renders diverse and coherent\nvideo scenes aligned with the storyboard. Extensive experiments demonstrate\nthat our system produces animations that are stylistically faithful,\nnarratively structured natural motion, highlighting its potential for\npersonalized and engaging story animation. The code will be available at\nhttps://github.com/GVCLab/FairyGen", "comment": "Project Page: https://jayleejia.github.io/FairyGen/ ; Code:\n  https://github.com/GVCLab/FairyGen", "pdf_url": "http://arxiv.org/pdf/2506.21272v1", "categories": ["cs.GR", "cs.CV", "cs.MM"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21272v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FairyGen：从单个儿童绘画角色生成故事卡通视频", "tldr": "FairyGen是一个自动系统，能从单个儿童绘画角色生成故事驱动的卡通视频，同时忠实保留其独特艺术风格。", "motivation": "与之前主要关注角色一致性和基本运动的讲故事方法不同，FairyGen旨在解决现有方法在风格化背景生成和电影镜头设计方面的不足，以支持更具表现力和连贯性的故事讲述。", "method": "FairyGen首先利用MLLM从单个角色草图生成带镜头描述的结构化故事板。为确保视觉一致性，引入了风格传播适配器，将角色视觉风格应用于背景。镜头设计模块通过画面裁剪和多视角合成增强视觉多样性和电影质量。为动画化故事，系统重建角色的3D代理以导出物理上合理的运动序列，并用于微调基于MMDiT的图像到视频扩散模型。此外，提出了一个两阶段运动定制适配器，第一阶段从无序帧中学习外观特征以解耦身份和运动，第二阶段使用时间步移位策略建模时间动态。", "result": "FairyGen系统生成的动画在风格上忠实于原始绘画，叙事结构化，并具有自然的运动。", "conclusion": "FairyGen系统展示了其在个性化和引人入胜的故事动画方面的巨大潜力。", "translation": "我们提出了FairyGen，一个从单个儿童绘画生成故事驱动卡通视频的自动化系统，同时忠实保留其独特的艺术风格。与之前主要关注角色一致性和基本运动的讲故事方法不同，FairyGen明确地将角色建模与风格化背景生成解耦，并融入电影镜头设计以支持富有表现力和连贯性的故事讲述。给定一个单个角色草图，我们首先采用多模态大型语言模型（MLLM）生成一个结构化的故事板，其中包含指定环境设置、角色动作和摄像机视角的镜头级描述。为确保视觉一致性，我们引入了一个风格传播适配器，该适配器捕捉角色的视觉风格并将其应用于背景，忠实地保留角色的完整视觉特征，同时合成风格一致的场景。一个镜头设计模块通过基于故事板的画面裁剪和多视角合成进一步增强视觉多样性和电影质量。为了动画化故事，我们重建角色的3D代理以导出物理上合理的运动序列，然后用于微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一个两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动解耦；第二阶段使用带有冻结身份权重的timestep-shift策略建模时间动态。一旦训练完成，FairyGen直接渲染与故事板对齐的各种连贯视频场景。大量的实验表明，我们的系统生成的动画在风格上忠实、叙事结构化且具有自然运动，突显了其在个性化和引人入胜的故事动画方面的潜力。代码将在https://github.com/GVCLab/FairyGen提供。", "summary": "FairyGen是一个创新的自动化系统，能够将儿童的单个绘画角色转化为风格忠实且叙事驱动的卡通视频。它通过解耦角色建模与风格化背景生成、整合电影镜头设计、利用MLLM生成故事板、以及通过3D代理和MMDiT扩散模型实现逼真动画来突破传统限制。该系统还引入了风格传播适配器和两阶段运动定制适配器，以确保视觉一致性和自然运动。实验证明，FairyGen能生成高质量、个性化且引人入胜的动画。", "keywords": "卡通视频生成, 儿童绘画, 风格迁移, 故事驱动动画, 扩散模型", "comments": "FairyGen的创新之处在于其能够从单个儿童绘画中生成复杂的、故事驱动的卡通视频，同时忠实地保留原始艺术风格。它通过将角色建模与背景生成解耦、引入电影镜头设计以及采用先进的扩散模型和运动定制技术，显著提升了自动化故事动画的质量和表现力。这对于个性化内容创作和教育领域具有重要意义。"}}
{"id": "2506.21448", "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing", "authors": ["Huadai Liu", "Jialei Wang", "Kaicheng Luo", "Wen Wang", "Qian Chen", "Zhou Zhao", "Wei Xue"], "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\n\\textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT)\nreasoning to enable stepwise, interactive audio generation and editing for\nvideos. Our approach decomposes the process into three complementary stages:\nfoundational foley generation that creates semantically coherent soundscapes,\ninteractive object-centric refinement through precise user interactions, and\ntargeted editing guided by natural language instructions. At each stage, a\nmultimodal large language model generates contextually aligned CoT reasoning\nthat guides a unified audio foundation model. Furthermore, we introduce\n\\textbf{AudioCoT}, a comprehensive dataset with structured reasoning\nannotations that establishes connections between visual content, textual\ndescriptions, and sound synthesis. Experiments demonstrate that ThinkSound\nachieves state-of-the-art performance in video-to-audio generation across both\naudio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio\nbenchmark. The demo page is available at https://ThinkSound-Demo.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21448v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21448v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ThinkSound：多模态大型语言模型中用于音频生成和编辑的思维链推理", "tldr": "ThinkSound是一个利用思维链推理的多模态大语言模型框架，用于视频音频的逐步生成和编辑，实现了SOTA性能。", "motivation": "尽管端到端视频到音频生成有所改进，但仍难以生成真实捕捉视觉内容细微差别的高保真音频，这需要对视觉动态、声学环境和时间关系进行复杂的推理。", "method": "提出ThinkSound框架，利用思维链（CoT）推理实现视频音频的逐步交互式生成和编辑。该方法分解为三个阶段：基础拟音生成、交互式以对象为中心的细化和自然语言指导的定向编辑。在每个阶段，多模态大语言模型生成CoT推理来指导统一的音频基础模型。此外，引入了AudioCoT数据集，用于连接视觉内容、文本描述和声音合成。", "result": "ThinkSound在视频到音频生成方面实现了最先进的性能，无论是在音频指标还是CoT指标上，并且在分布外Movie Gen Audio基准测试中表现出色。", "conclusion": "ThinkSound框架通过结合思维链推理和多模态大语言模型，显著提升了视频音频生成和编辑的质量和交互性，达到了SOTA水平。", "translation": "尽管端到端视频到音频生成已大大改进，但生成能真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。像创意产业的专业人士一样，这种生成需要对视觉动态、声学环境和时间关系等项目进行复杂的推理。我们提出了\\textbf{ThinkSound}，一个新颖的框架，它利用思维链（CoT）推理来实现视频的逐步、交互式音频生成和编辑。我们的方法将过程分解为三个互补的阶段：创建语义连贯声景的基础拟音生成，通过精确用户交互进行的交互式以对象为中心的细化，以及由自然语言指令引导的定向编辑。在每个阶段，多模态大型语言模型生成与上下文对齐的CoT推理，以指导统一的音频基础模型。此外，我们引入了\\textbf{AudioCoT}，一个包含结构化推理注释的综合数据集，它建立了视觉内容、文本描述和声音合成之间的联系。实验表明，ThinkSound在视频到音频生成方面在音频指标和CoT指标上都达到了最先进的性能，并在分布外Movie Gen Audio基准测试中表现出色。演示页面可在https://ThinkSound-Demo.github.io获取。", "summary": "ThinkSound是一个创新的多模态大语言模型框架，通过引入思维链（CoT）推理，解决了视频到音频生成中高保真和细致推理的挑战。它将生成过程分为基础拟音、交互式细化和自然语言编辑三个阶段，并由CoT推理指导音频基础模型。该研究还提出了AudioCoT数据集。实验证明ThinkSound在视频到音频生成方面达到了最先进的性能。", "keywords": "视频到音频生成, 思维链推理, 多模态大语言模型, 音频编辑, AudioCoT", "comments": "ThinkSound的创新之处在于将思维链推理引入多模态大语言模型，以实现更精细、可控和交互式的视频音频生成和编辑。这种分阶段的方法模拟了专业人士的工作流，提高了生成音频的真实性和细节表现。引入AudioCoT数据集也为未来的研究提供了宝贵的资源。"}}
{"id": "2506.20906", "title": "Almost Tight Additive Guarantees for \\boldmath $k$-Edge-Connectivity", "authors": ["Nikhil Kumar", "Chaitanya Swamy"], "summary": "We consider the \\emph{$k$-edge connected spanning subgraph} (kECSS) problem,\nwhere we are given an undirected graph $G = (V, E)$ with nonnegative edge costs\n$\\{c_e\\}_{e\\in E}$, and we seek a minimum-cost \\emph{$k$-edge connected}\nsubgraph $H$ of $G$. For even $k$, we present a polytime algorithm that\ncomputes a $(k-2)$-edge connected subgraph of cost at most the optimal value\n$LP^*$ of the natural LP-relaxation for kECSS; for odd $k$, we obtain a\n$(k-3)$-edge connected subgraph of cost at most $LP^*$. Since kECSS is APX-hard\nfor all $k\\geq 2$, our results are nearly optimal. They also significantly\nimprove upon the recent work of Hershkowitz et al., both in terms of solution\nquality and the simplicity of algorithm and its analysis. Our techniques also\nyield an alternate guarantee, where we obtain a $(k-1)$-edge connected subgraph\nof cost at most $1.5\\cdot LP^*$; with unit edge costs, the cost guarantee\nimproves to $(1+\\frac{4}{3k})\\cdot LP^*$, which improves upon the\nstate-of-the-art approximation for unit edge costs, but with a unit loss in\nedge connectivity.\n  Our kECSS-result also yields results for the \\emph{$k$-edge connected\nspanning multigraph} (kECSM) problem, where multiple copies of an edge can be\nselected: we obtain a $(1+2/k)$-approximation algorithm for even $k$, and a\n$(1+3/k)$-approximation algorithm for odd $k$.\n  Our techniques extend to the degree-bounded versions of kECSS and kECSM,\nwherein we also impose degree lower- and upper- bounds on the nodes. We obtain\nthe same cost and connectivity guarantees for these degree-bounded versions\nwith an additive violation of (roughly) $2$ for the degree bounds. These are\nthe first results for degree-bounded \\{kECSS,kECSM\\} of the form where the cost\nof the solution obtained is at most the optimum, and the connectivity\nconstraints are violated by an additive constant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20906v1", "categories": ["cs.DS", "F.2.2; G.2"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20906v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "$k$-边连通性近似紧加性保证", "tldr": "该论文为 $k$-边连通生成子图 (kECSS) 和多图 (kECSM) 问题提供了多项式时间算法，实现了接近最优的加性连通性保证，成本至多或略高于 LP 松弛最优值。", "motivation": "该论文研究 $k$-边连通生成子图 (kECSS) 问题，该问题是 APX-hard。目标是找到一个成本最小的 $k$-边连通子图。作者旨在改进现有工作并首次解决带度限制的版本。", "method": "对于 kECSS，作者提出了一种多项式时间算法。对于偶数 $k$，算法计算一个 $(k-2)$-边连通子图；对于奇数 $k$，计算一个 $(k-3)$-边连通子图，其成本均至多为 LP 松弛最优值 $LP^*$。他们还提供了一种替代保证，即获得一个 $(k-1)$-边连通子图，成本至多为 $1.5\text{·}LP^*$；对于单位边成本，该保证提高到 $(1+\\frac{4}{3k})\\text{·}LP^*$。他们的技术也适用于 kECSM 问题，对于偶数 $k$ 获得 $(1+2/k)$-近似算法，对于奇数 $k$ 获得 $(1+3/k)$-近似算法。此外，这些技术扩展到带度限制的 kECSS 和 kECSM 版本，在度限制上存在约 2 的加性违反。", "result": "对于 kECSS：偶数 $k$ 获得 $(k-2)$-边连通子图，成本 $\\le LP^*$；奇数 $k$ 获得 $(k-3)$-边连通子图，成本 $\\le LP^*$。替代方案是 $(k-1)$-边连通子图，成本 $\\le 1.5\\text{·}LP^*$；单位成本下，成本 $\\le (1+\\frac{4}{3k})\\text{·}LP^*$。对于 kECSM：偶数 $k$ 获得 $(1+2/k)$-近似，奇数 $k$ 获得 $(1+3/k)$-近似。对于带度限制的版本，获得了与 kECSS/kECSM 相同的成本和连通性保证，但在度限制上存在约 2 的加性违反。", "conclusion": "该算法为 kECSS 提供了接近最优的加性保证，并显著改进了现有工作。它们还为 kECSM 带来了结果，并且是首次解决带度限制版本的问题，其解决方案成本至多为最优值，连通性约束仅受到加性常数违反。", "translation": "我们考虑 $k$-边连通生成子图 (kECSS) 问题，其中给定一个无向图 $G = (V, E)$ 具有非负边成本 $\\{c_e\\}_{e\\in E}$，我们寻求 $G$ 中成本最小的 $k$-边连通子图 $H$。对于偶数 $k$，我们提出了一种多项式时间算法，该算法计算一个 $(k-2)$-边连通子图，其成本至多为 kECSS 的自然 LP-松弛的最优值 $LP^*$；对于奇数 $k$，我们获得一个 $(k-3)$-边连通子图，其成本至多为 $LP^*$。由于 kECSS 对于所有 $k\\geq 2$ 都是 APX-hard 的，我们的结果几乎是最优的。它们还在解决方案质量以及算法及其分析的简单性方面显著改进了 Hershkowitz 等人的最新工作。我们的技术也产生了另一种保证，即我们获得一个 $(k-1)$-边连通子图，其成本至多为 $1.5\\text{·}LP^*$；对于单位边成本，成本保证提高到 $(1+\\frac{4}{3k})\\text{·}LP^*$，这改进了单位边成本的最先进近似，但边连通性损失了一个单位。\n我们的 kECSS 结果也为 $k$-边连通生成多图 (kECSM) 问题带来了结果，其中可以选择边的多个副本：对于偶数 $k$，我们获得了 $(1+2/k)$-近似算法，对于奇数 $k$，我们获得了 $(1+3/k)$-近似算法。\n我们的技术扩展到 kECSS 和 kECSM 的度受限版本，其中我们还对节点施加了度下限和上限。对于这些度受限版本，我们获得了相同的成本和连通性保证，但在度限制方面存在（大约）2 的加性违反。这些是度受限 \\{kECSS,kECSM\\} 的第一批结果，其形式是获得的解决方案成本至多为最优值，并且连通性约束被加性常数违反。", "summary": "本论文解决了 APX-hard 的 $k$-边连通生成子图 (kECSS) 和多图 (kECSM) 问题。作者提出了多项式时间算法，在成本至多或略高于 LP 松弛最优值的情况下，实现了接近最优的边连通性加性保证。具体而言，对于 kECSS，偶数 $k$ 获得 $(k-2)$-边连通性，奇数 $k$ 获得 $(k-3)$-边连通性，成本均不超过 $LP^*$。他们还提供了一种替代方案，即 $(k-1)$-边连通解，成本为 $1.5\\text{·}LP^*$，对于单位成本则改进为 $(1+4/(3k))\\text{·}LP^*$。对于 kECSM，偶数 $k$ 获得 $(1+2/k)$-近似，奇数 $k$ 获得 $(1+3/k)$-近似。此外，其技术扩展到这些问题的度受限版本，在度限制上存在小的加性违反，代表了此类变体的首批结果。", "keywords": "k-边连通性, 生成子图, LP松弛, 近似算法, 度受限", "comments": "该论文的创新之处在于，对于 NP-hard 问题如 kECSS，通过将成本控制在 LP 最优值或其附近，同时仅对连通性进行小常数放松，实现了接近最优的加性保证。将其扩展到带度限制的版本也是一项重要贡献，提供了此类形式的首批结果。文中提及算法和分析的简洁性也是一个优点。"}}
{"id": "2506.20978", "title": "Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality", "authors": ["Naihe Feng", "Yi Sui", "Shiyi Hou", "Jesse C. Cresswell", "Ga Wu"], "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.", "comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment", "pdf_url": "http://arxiv.org/pdf/2506.20978v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20978v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于条件保形事实性的检索增强生成响应质量评估", "tldr": "Conformal-RAG利用保形预测为RAG生成的响应子声明提供统计质量保证，无需真实标签，且比直接CP应用于LLMs效果更好。", "motivation": "现有RAG研究主要关注整体问答准确性，但忽视了子声明质量；现有自动评估方法缺乏概率保证或需要真实标签。", "method": "提出Conformal-RAG框架，它结合保形预测(CP)和RAG机制的内部信息，为响应质量提供统计保证。该方法确保跨多个子域的组条件覆盖，且无需手动标记保形集。", "result": "Conformal-RAG在相同可靠性保证下，比直接将CP应用于LLMs多保留高达60%的高质量子声明。它为精炼子声明的质量提供统计保证，无需真实答案。", "conclusion": "Conformal-RAG通过利用CP和RAG内部信息，有效解决了现有RAG评估方法的局限性，提供有统计保证的响应质量评估方法，并在保持可靠性的前提下显著提高了高质量子声明的保留率。", "translation": "现有关于检索增强生成（RAG）的研究主要集中在提高整体问答准确性，却常常忽视生成响应中子声明的质量。近期旨在提高RAG可信度的方法，例如通过自动评估指标，缺乏概率保证或需要真实答案。为了解决这些局限性，我们提出了Conformal-RAG，一个受最近保形预测（CP）在大型语言模型（LLMs）应用启发的新颖框架。Conformal-RAG利用CP和RAG机制的内部信息，为响应质量提供统计保证。它确保跨多个子域的组条件覆盖，且无需手动标记保形集，使其适用于复杂的RAG应用。与现有RAG自动评估方法相比，Conformal-RAG为精炼子声明的质量提供统计保证，确保响应可靠性而无需真实答案。此外，我们的实验表明，通过利用RAG系统的信息，Conformal-RAG在保持相同可靠性保证的情况下，比直接将CP应用于LLMs多保留高达60%的响应中的高质量子声明。", "summary": "本文提出了Conformal-RAG框架，旨在解决现有检索增强生成（RAG）系统在评估响应子声明质量时缺乏统计保证和需要真实标签的问题。Conformal-RAG结合了保形预测（CP）和RAG内部信息，提供响应质量的统计保证，确保组条件覆盖，且无需手动标记。实验证明，Conformal-RAG在保持可靠性的同时，能比直接应用CP保留更多高质量子声明。", "keywords": "检索增强生成, 保形预测, 响应质量评估, 事实性, 统计保证", "comments": "这篇论文的创新点在于将保形预测应用于RAG的响应质量评估，特别是针对子声明的质量保证，解决了现有方法缺乏统计保证和对真实标签依赖的问题。其利用RAG内部信息来优化CP应用，提高了高质量子声明的保留率，对于提升RAG系统的可信度和实用性具有重要意义。"}}
{"id": "2506.20822", "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "authors": ["Quintin Myers", "Yanjun Gao"], "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20822v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20822v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "揭示LLMs中隐藏的暴力倾向：通过行为小插曲进行人口统计学分析", "tldr": "本研究首次使用经验证的社会科学工具（VBVQ）评估LLMs在处理日常冲突中的暴力倾向和潜在偏见。结果显示LLMs的表面文本生成与其内部暴力偏好存在差异，且其暴力倾向在不同人口统计学特征上表现出变化，这与现有犯罪学和社会科学发现相悖。", "motivation": "大型语言模型（LLMs）被越来越多地提议用于在线暴力内容的检测和响应，但它们对道德模糊、真实世界场景的推理能力尚未得到充分检验。", "method": "本研究首次使用经验证的社会科学工具“暴力行为小插曲问卷”（VBVQ）来评估LLMs，该问卷旨在衡量人类对日常冲突的反应。为评估潜在偏见，研究引入了基于角色的提示，在美国境内改变了种族、年龄和地理身份。研究在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLMs。", "result": "研究揭示了两个关键发现：（1）LLMs的表面文本生成常常与其内部对暴力反应的偏好存在分歧；（2）它们的暴力倾向因人口统计学特征而异，且经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "conclusion": "LLMs在处理道德模糊情境时，其内部暴力倾向可能与其表面输出不一致，并且这些倾向表现出人口统计学偏见，这挑战了现有社会科学的发现，并暗示了LLMs在检测和响应暴力内容方面的复杂性和潜在风险。", "translation": "大型语言模型（LLMs）被越来越多地提议用于在线暴力内容的检测和响应，但它们对道德模糊、真实世界场景的推理能力尚未得到充分检验。我们首次提出一项研究，使用一个经过验证的社会科学工具来评估LLMs，该工具旨在衡量人类对日常冲突的反应，即暴力行为小插曲问卷（VBVQ）。为了评估潜在偏见，我们引入了基于角色的提示，在美国境内改变了种族、年龄和地理身份。在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLMs。我们的研究揭示了两个关键发现：（1）LLMs的表面文本生成常常与其内部对暴力反应的偏好存在分歧；（2）它们的暴力倾向因人口统计学特征而异，且经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "summary": "本研究首次使用暴力行为小插曲问卷（VBVQ）这一经验证的社会科学工具，评估了LLMs在处理日常冲突中的推理能力。通过引入基于角色的提示来模拟不同人口统计学特征，研究发现LLMs的表面文本输出与其内部暴力倾向存在差异，且这些倾向在不同人口群体中表现出与现有社会科学理论相悖的偏见。这揭示了LLMs在理解和响应道德模糊情境时的复杂性和潜在风险。", "keywords": "LLMs, 暴力倾向, 行为小插曲, 人口统计学分析, 偏见", "comments": "这项研究的创新之处在于首次将经过验证的社会科学测量工具（VBVQ）应用于LLMs的评估，以揭示其在道德模糊情境下的隐藏暴力倾向。通过引入人口统计学偏见的分析，它强调了LLMs在处理敏感内容时可能存在的深层偏见，这对于LLMs在内容审核和风险评估应用中的部署具有重要意义。研究结果对LLMs的可靠性和公平性提出了新的挑战。"}}
{"id": "2506.21025", "title": "An energy-stable parametric finite element method for the Willmore flow in three dimensions", "authors": ["Weizhu Bao", "Yifei Li", "Dongmin Wang"], "summary": "This work develops novel energy-stable parametric finite element methods\n(ES-PFEM) for the Willmore flow and curvature-dependent geometric gradient\nflows of surfaces in three dimensions. The key to achieving the energy\nstability lies in the use of two novel geometric identities: (i) a reformulated\nvariational form of the normal velocity field, and (ii) incorporation of the\ntemporal evolution of the mean curvature into the governing equations. These\nidentities enable the derivation of a new variational formulation. By using the\nparametric finite element method, an implicit fully discrete scheme is\nsubsequently developed, which maintains the energy dissipative property at the\nfully discrete level. Based on the ES-PFEM, comprehensive insights into the\ndesign of ES-PFEM for general curvature-dependent geometric gradient flows and\na new understanding of mesh quality improvement in PFEM are provided. In\nparticular, we develop the first PFEM for the Gauss curvature flow of surfaces.\nFurthermore, a tangential velocity control methodology is applied to improve\nthe mesh quality and enhance the robustness of the proposed numerical method.\nExtensive numerical experiments confirm that the proposed method preserves\nenergy dissipation properties and maintain good mesh quality in the surface\nevolution under the Willmore flow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21025v1", "categories": ["math.NA", "cs.NA", "65M60, 65M12, 35K55, 53C44"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21025v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "三维Willmore流的一种能量稳定参数有限元方法", "tldr": "本文提出了一种新的能量稳定参数有限元方法（ES-PFEM），用于三维Willmore流和曲率相关几何梯度流，通过引入两个新的几何恒等式和切向速度控制，实现了能量耗散性和良好的网格质量。", "motivation": "为了解决三维Willmore流和曲率相关几何梯度流中实现能量稳定性的挑战。", "method": "开发了能量稳定参数有限元方法（ES-PFEM），其关键在于使用了两个新的几何恒等式：(i) 法向速度场的重新公式化变分形式，以及 (ii) 将平均曲率的时间演化纳入控制方程。在此基础上，推导出了新的变分公式，并开发了隐式全离散方案。此外，应用了切向速度控制方法来改善网格质量。", "result": "所提出的方法在全离散层面保持了能量耗散特性。提供了关于设计ES-PFEM用于一般曲率相关几何梯度流的全面见解，并对PFEM中的网格质量改进有了新的理解。首次开发了用于高斯曲率流的PFEM。数值实验证实该方法在Willmore流下的表面演化中保持了能量耗散特性和良好的网格质量。", "conclusion": "提出的能量稳定参数有限元方法在三维Willmore流的数值模拟中表现出色，能够保持能量耗散特性并维持良好的网格质量。", "translation": "这项工作开发了用于三维Willmore流和曲率相关几何梯度流的新型能量稳定参数有限元方法（ES-PFEM）。实现能量稳定性的关键在于使用了两个新的几何恒等式：(i) 法向速度场的重新公式化变分形式，以及 (ii) 将平均曲率的时间演化纳入控制方程。这些恒等式使得能够推导出一种新的变分公式。通过使用参数有限元方法，随后开发了一种隐式全离散方案，该方案在全离散层面保持了能量耗散特性。基于ES-PFEM，提供了关于设计用于一般曲率相关几何梯度流的ES-PFEM的全面见解，并对PFEM中的网格质量改进有了新的理解。特别是，我们开发了第一个用于表面高斯曲率流的PFEM。此外，应用了切向速度控制方法来改善网格质量并增强所提出的数值方法的鲁棒性。大量的数值实验证实，所提出的方法在Willmore流下的表面演化中保持了能量耗散特性并维持了良好的网格质量。", "summary": "本文提出了一种用于三维Willmore流和曲率相关几何梯度流的能量稳定参数有限元方法（ES-PFEM）。该方法通过引入两个创新的几何恒等式（重新公式化的法向速度变分形式和平均曲率时间演化纳入）来确保能量稳定性，并推导了新的变分公式和隐式全离散方案。此外，结合切向速度控制技术以优化网格质量。数值实验验证了该方法在保持能量耗散性和网格质量方面的有效性，并首次将其应用于高斯曲率流。", "keywords": "Willmore流, 参数有限元方法, 能量稳定, 几何梯度流, 网格质量", "comments": "该论文的创新点在于引入了两个新颖的几何恒等式，从而实现了三维Willmore流的能量稳定参数有限元方法。同时，通过切向速度控制进一步提升了网格质量和方法的鲁棒性。其重要性体现在为处理复杂几何梯度流提供了稳定高效的数值工具，并扩展了PFEM在曲率流领域的应用，特别是首次应用于高斯曲率流。抽象中未明确提及方法的局限性。"}}
{"id": "2506.21298", "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation", "authors": ["Atharva Mehta", "Shivam Chauhan", "Monojit Choudhury"], "summary": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21298v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21298v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "探索低资源音乐生成中的适配器设计权衡", "tldr": "本文研究了针对低资源音乐生成的适配器设计权衡，发现不同适配器类型各有优劣，并确定了中等大小适配器的最佳平衡点，同时比较了MusicGen和Mustango两种模型的性能。", "motivation": "微调大型音乐生成模型计算成本高昂。参数高效微调（PEFT）技术，特别是基于适配器的方法，虽有潜力，但其设计选择（架构、位置、大小）尚不明确，尤其是在低资源音乐流派中，何种组合能产生最佳适配器及其原因尚不清楚。", "method": "本文通过研究两种AI音乐模型（MusicGen和Mustango）在两种音乐流派（印度斯坦古典音乐和土耳其马卡姆音乐）上的各种适配器配置来回答上述问题。", "result": "卷积型适配器擅长捕捉精细的局部音乐细节（如装饰音和短旋律片段）；Transformer型适配器能更好地保持对结构化即兴创作至关重要的长程依赖；中等大小的适配器（40M参数）在表达能力和质量之间实现了最佳平衡；Mustango（基于扩散的模型）生成更多样化且更符合输入提示的输出，但在音符稳定性、节奏对齐和美学方面有所欠缺，且计算密集、训练时间长；MusicGen（自回归模型）训练更快、效率更高，能产生更高质量的输出，但生成内容冗余度略高。", "conclusion": "本研究揭示了低资源音乐生成中适配器设计的明显权衡，并分析了不同适配器类型和两种AI音乐模型（扩散型与自回归型）的性能特征，指出中等大小的适配器能达到最优平衡。", "translation": "微调大规模音乐生成模型，如MusicGen和Mustango，是一个计算成本高昂的过程，通常需要更新数十亿参数，因此需要大量的硬件资源。参数高效微调（PEFT）技术，特别是基于适配器的方法，已成为一种有前景的替代方案，它能够以最少的训练参数进行适应，同时保持模型性能。然而，适配器的设计选择，包括其架构、位置和大小，数量众多，目前尚不清楚在给定低资源音乐流派的情况下，这些组合中的哪一种会产生最佳适配器以及原因。在本文中，我们试图通过研究两种AI音乐模型（MusicGen和Mustango）在两种流派：印度斯坦古典音乐和土耳其马卡姆音乐上的各种适配器配置来回答这个问题。\n我们的研究结果揭示了明显的权衡：基于卷积的适配器擅长捕捉精细的局部音乐细节，如装饰音和短旋律片段，而基于Transformer的适配器能更好地保持对结构化即兴创作至关重要的长程依赖。此外，我们分析了不同适配器规模下的计算资源需求，表明中等大小的适配器（40M参数）在表达能力和质量之间实现了最佳平衡。此外，我们发现Mustango（一种基于扩散的模型）生成更多样化且更符合输入提示的输出，但在音符稳定性、节奏对齐和美学方面有所欠缺。同时，它计算密集，需要显著更长的训练时间。相比之下，像MusicGen这样的自回归模型提供更快的训练速度和更高的效率，并且可以产生更高质量的输出，但其生成内容冗余度略高。", "summary": "本文探讨了在低资源环境下，针对MusicGen和Mustango等大型音乐生成模型使用参数高效微调（PEFT）中的适配器设计权衡。研究比较了不同适配器配置（如卷积型和Transformer型）在捕捉音乐细节和保持长程依赖方面的表现，并分析了不同适配器规模的计算需求，发现中等大小的适配器（40M参数）能达到性能与资源消耗的最佳平衡。此外，论文还对比了Mustango（扩散模型）和MusicGen（自回归模型）在生成多样性、质量、效率和资源需求方面的特点。", "keywords": "适配器设计, 音乐生成, 低资源, PEFT, 模型权衡", "comments": "本文的创新之处在于系统性地探索了在低资源音乐生成领域中适配器设计的具体权衡，并首次对比了不同适配器类型和两种主流AI音乐模型（自回归与扩散）在此背景下的性能差异。这对于优化音乐生成模型的部署和降低计算成本具有重要指导意义。"}}
{"id": "2506.21069", "title": "TEMPEST-LoRa: Cross-Technology Covert Communication", "authors": ["Xieyang Sun", "Yuanqing Zheng", "Wei Xi", "Zuhao Chen", "Zhizhen Chen", "Han Hao", "Zhiping Jiang", "Sheng Zhong"], "summary": "Electromagnetic (EM) covert channels pose significant threats to computer and\ncommunications security in air-gapped networks. Previous works exploit EM\nradiation from various components (e.g., video cables, memory buses, CPUs) to\nsecretly send sensitive information. These approaches typically require the\nattacker to deploy highly specialized receivers near the victim, which limits\ntheir real-world impact. This paper reports a new EM covert channel,\nTEMPEST-LoRa, that builds on Cross-Technology Covert Communication (CTCC),\nwhich could allow attackers to covertly transmit EM-modulated secret data from\nair-gapped networks to widely deployed operational LoRa receivers from afar. We\nreveal the potential risk and demonstrate the feasibility of CTCC by tackling\npractical challenges involved in manipulating video cables to precisely\ngenerate the EM leakage that could readily be received by third-party\ncommercial LoRa nodes/gateways. Experiment results show that attackers can\nreliably decode secret data modulated by the EM leakage from a video cable at a\nmaximum distance of 87.5m or a rate of 21.6 kbps. We note that the secret data\ntransmission can be performed with monitors turned off (therefore covertly).", "comment": "15 pages, 19 figures, and this paper has been accepted to ACM CCS\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21069v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21069v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TEMPEST-LoRa: 跨技术秘密通信", "tldr": "本文介绍了一种名为TEMPEST-LoRa的新型电磁秘密信道，它利用交叉技术秘密通信（CTCC），允许攻击者将气隙网络中的秘密数据传输到远距离的LoRa接收器，并展示了通过操纵视频线缆实现远距离和高速数据传输的可行性。", "motivation": "电磁（EM）秘密信道对气隙网络中的计算机和通信安全构成重大威胁。以往的方法需要攻击者在受害者附近部署高度专业化的接收器，这限制了其实际影响。本文旨在解决这一局限性，并揭示利用广泛部署的LoRa接收器进行秘密通信的潜在风险。", "method": "本文提出了一种名为TEMPEST-LoRa的新型电磁秘密信道，它基于交叉技术秘密通信（CTCC）。该方法通过操纵视频线缆来精确生成电磁泄漏，这些泄漏可以被第三方商用LoRa节点/网关接收。作者通过实验证明了CTCC的可行性，并展示了数据传输能力。", "result": "实验结果表明，攻击者可以可靠地解码来自视频线缆电磁泄漏调制下的秘密数据。最大传输距离达到87.5米，或最高传输速率达到21.6 kbps。此外，秘密数据传输可以在显示器关闭的情况下进行，从而实现隐蔽性。", "conclusion": "本文揭示了交叉技术秘密通信（CTCC）的潜在风险，并证明了利用视频线缆的电磁泄漏向远距离的商用LoRa接收器进行秘密数据传输的可行性，这表明了气隙网络安全面临的新威胁。", "translation": "电磁（EM）秘密信道对气隙网络中的计算机和通信安全构成重大威胁。以往的工作利用各种组件（如视频线缆、内存总线、CPU）的电磁辐射来秘密发送敏感信息。这些方法通常要求攻击者在受害者附近部署高度专业化的接收器，这限制了其实际影响。本文报告了一种新的电磁秘密信道——TEMPEST-LoRa，它建立在交叉技术秘密通信（CTCC）的基础上，可以允许攻击者将电磁调制的秘密数据从气隙网络秘密传输到远距离的广泛部署的LoRa接收器。我们揭示了CTCC的潜在风险，并通过解决操纵视频线缆以精确生成可被第三方商用LoRa节点/网关轻易接收的电磁泄漏所涉及的实际挑战，证明了其可行性。实验结果表明，攻击者可以可靠地解码由视频线缆的电磁泄漏调制的秘密数据，最大距离可达87.5米或速率可达21.6 kbps。我们注意到秘密数据传输可以在显示器关闭的情况下进行（因此是隐蔽的）。", "summary": "本文介绍了一种名为TEMPEST-LoRa的新型电磁秘密信道，旨在解决传统秘密通信方法对专用接收器依赖的局限性。通过利用交叉技术秘密通信（CTCC），该方法能够将气隙网络中的秘密数据，通过操纵视频线缆产生的电磁泄漏，传输到远距离的商用LoRa接收器。实验证明了这种方法的有效性，实现了87.5米的最大传输距离或21.6 kbps的最高传输速率，且可在显示器关闭时隐蔽进行，揭示了CTCC带来的新安全风险。", "keywords": "秘密通信, 电磁泄漏, LoRa, 气隙网络, 交叉技术通信", "comments": "本文提出了一种创新的秘密通信方法，通过利用现有设备的电磁泄漏与广泛部署的LoRa技术相结合，显著降低了攻击的门槛和可检测性。其创新之处在于利用了“交叉技术”的概念，使得无需专门接收设备即可进行远距离秘密通信。这对于气隙网络的安全防护提出了新的挑战，具有重要的实际意义。"}}
{"id": "2506.21426", "title": "Evolution and determinants of firm-level systemic risk in local production networks", "authors": ["Anna Mancini", "Balázs Lengyel", "Riccardo Di Clemente", "Giulio Cimini"], "summary": "Recent crises like the COVID-19 pandemic and geopolitical tensions have\nexposed vulnerabilities and caused disruptions of supply chains, leading to\nproduct shortages, increased costs, and economic instability. This has prompted\nincreasing efforts to assess systemic risk, namely the effects of firm\ndisruptions on entire economies. However, the ability of firms to react to\ncrises by rewiring their supply links has been largely overlooked, limiting our\nunderstanding of production networks resilience. Here we study dynamics and\ndeterminants of firm-level systemic risk in the Hungarian production network\nfrom 2015 to 2022. We use as benchmark a heuristic maximum entropy null model\nthat generates an ensemble of production networks at equilibrium, by preserving\nthe total input (demand) and output (supply) of each firm at the sector level.\nWe show that the fairly stable set of firms with highest systemic risk\nundergoes a structural change during COVID-19, as those enabling economic\nexchanges become key players in the economy -- a result which is not reproduced\nby the null model. Although the empirical systemic risk aligns well with the\nnull value until the onset of the pandemic, it becomes significantly smaller\nafterwards as the adaptive behavior of firms leads to a more resilient economy.\nFurthermore, firms' international trade volume (being a subject of disruption)\nbecomes a significant predictor of their systemic risk. However, international\nlinks cannot provide an unequivocal explanation for the observed trends, as\nimports and exports have opposing effects on local systemic risk through the\nsupply and demand channels.", "comment": "15 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21426v1", "categories": ["physics.soc-ph", "cs.SI", "econ.GN", "physics.data-an", "q-fin.EC", "q-fin.RM"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.21426v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "地方生产网络中企业层面系统性风险的演变与决定因素", "tldr": "研究了匈牙利生产网络中企业层面系统性风险的动态和决定因素，发现疫情期间企业适应性行为降低了系统性风险，并指出国际贸易量是重要预测因素。", "motivation": "近期危机（如COVID-19大流行和地缘政治紧张）暴露了供应链脆弱性并导致经济不稳定，促使人们评估系统性风险。然而，企业通过重组供应链接来应对危机的能力被忽视，这限制了对生产网络韧性的理解。", "method": "研究了2015年至2022年匈牙利生产网络中企业层面系统性风险的动态和决定因素。使用启发式最大熵零模型作为基准，该模型通过保留每个企业在部门层面的总投入（需求）和产出（供应）来生成均衡生产网络集合。", "result": "1. 具有最高系统性风险的企业集合在COVID-19期间发生了结构性变化，促成经济交换的企业成为经济中的关键参与者，而零模型无法重现此结果。2. 在疫情爆发前，经验系统性风险与零模型值吻合良好，但之后显著减小，表明企业适应性行为使经济更具韧性。3. 企业的国际贸易量成为其系统性风险的重要预测因素。4. 国际联系无法明确解释观察到的趋势，因为进出口通过供需渠道对地方系统性风险产生相反影响。", "conclusion": "企业在危机中的适应性行为（如供应链重组）显著降低了系统性风险，增强了经济韧性。国际贸易量是系统性风险的预测因素，但其影响复杂且不明确。", "translation": "近期危机，如COVID-19大流行和地缘政治紧张，暴露了供应链的脆弱性并导致了中断，进而引发了产品短缺、成本增加和经济不稳定。这促使人们日益努力评估系统性风险，即企业中断对整个经济体的影响。然而，企业通过重组其供应链接来应对危机的能力在很大程度上被忽视，这限制了我们对生产网络韧性的理解。本文研究了2015年至2022年匈牙利生产网络中企业层面系统性风险的动态和决定因素。我们使用启发式最大熵零模型作为基准，该模型通过保留每个企业在部门层面的总投入（需求）和产出（供应）来生成一组处于均衡状态的生产网络。我们发现，在COVID-19期间，系统性风险最高的企业集合（一个相当稳定的群体）经历了结构性变化，因为那些促成经济交换的企业成为了经济中的关键参与者——这一结果并未被零模型重现。尽管在疫情爆发之前，经验系统性风险与零值吻合良好，但之后其显著减小，因为企业的适应性行为使得经济更具韧性。此外，企业的国际贸易量（作为中断的对象）成为其系统性风险的重要预测因素。然而，国际联系无法为观察到的趋势提供明确的解释，因为进出口通过供需渠道对地方系统性风险产生相反影响。", "summary": "本文研究了2015年至2022年匈牙利生产网络中企业层面系统性风险的演变和决定因素。研究发现，在COVID-19疫情期间，高系统性风险企业的结构发生了变化，且企业通过适应性行为（如重组供应链）显著降低了整体系统性风险，提升了经济韧性。此外，国际贸易量是系统性风险的重要预测因素，但进出口对系统性风险的影响是复杂且对立的。", "keywords": "系统性风险, 生产网络, 企业适应性, 供应链韧性, COVID-19", "comments": "这篇论文通过引入企业适应性行为来研究生产网络中的系统性风险，填补了现有研究中对企业自适应能力忽视的空白，提高了对经济韧性的理解。其使用匈牙利生产网络的实证数据，并结合最大熵零模型进行对比分析，验证了企业在危机中的动态响应对系统性风险的积极影响，具有重要的政策启示意义。"}}
{"id": "2506.21014", "title": "Boosting Vulnerability Detection with Inter-function Multilateral Association Insights", "authors": ["Shaojian Qiu", "Mengyang Huang", "Jiahao Cheng"], "summary": "Vulnerability detection is a crucial yet challenging technique for ensuring\nthe security of software systems. Currently, most deep learning-based\nvulnerability detection methods focus on stand-alone functions, neglecting the\ncomplex inter-function interrelations, particularly the multilateral\nassociations. This oversight can fail to detect vulnerabilities in these\ninterrelations. To address this gap, we present an Inter-Function Multilateral\nAssociation analysis framework for Vulnerability Detection (IFMA-VD). The\ncornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and\nutilizing hyperedge convolution to extract multilateral association features.\nSpecifically, we first parse functions into a code property graph to generate\nintra-function features. Following this, we construct a code behavior\nhypergraph by segmenting the program dependency graph to isolate and encode\nbehavioral features into hyperedges. Finally, we utilize a hypergraph network\nto capture the multilateral association knowledge for augmenting vulnerability\ndetection. We evaluate IFMA-VD on three widely used vulnerability datasets and\ndemonstrate improvements in F-measure and Recall compared to baseline methods.\nAdditionally, we illustrate that multilateral association features can boost\ncode feature representation and validate the effectiveness of IFMA-VD on\nreal-world datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21014v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21014v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "提升漏洞检测：基于函数间多边关联洞察", "tldr": "本文提出了IFMA-VD框架，通过构建代码行为超图和利用超边卷积，有效提升了软件漏洞检测的性能，尤其解决了现有方法忽视函数间多边关联的问题。", "motivation": "目前大多数基于深度学习的漏洞检测方法侧重于独立函数，忽视了复杂的函数间相互关系，特别是多边关联，这可能导致无法检测到这些关系中的漏洞。", "method": "本文提出了一个用于漏洞检测的函数间多边关联分析框架（IFMA-VD）。其核心在于构建代码行为超图并利用超边卷积来提取多边关联特征。具体步骤包括：首先将函数解析为代码属性图以生成函数内特征；然后通过分割程序依赖图来构建代码行为超图，将行为特征编码到超边中；最后利用超图网络捕获多边关联知识以增强漏洞检测。", "result": "IFMA-VD 在三个广泛使用的漏洞数据集上进行了评估，与基线方法相比，F-measure 和 Recall 均有所改进。此外，研究表明多边关联特征可以增强代码特征表示，并验证了 IFMA-VD 在真实世界数据集上的有效性。", "conclusion": "通过结合函数间多边关联洞察和超图分析，所提出的 IFMA-VD 框架显著增强了漏洞检测能力，弥补了先前方法的一个关键局限性。", "translation": "漏洞检测是确保软件系统安全的关键但具有挑战性的技术。目前，大多数基于深度学习的漏洞检测方法侧重于独立函数，忽视了复杂的函数间相互关系，特别是多边关联。这种疏忽可能导致无法检测到这些相互关系中的漏洞。为了弥补这一空白，我们提出了一个用于漏洞检测的函数间多边关联分析框架（IFMA-VD）。IFMA-VD 的基石在于构建代码行为超图并利用超边卷积来提取多边关联特征。具体来说，我们首先将函数解析为代码属性图以生成函数内特征。在此之后，我们通过分割程序依赖图来构建代码行为超图，以隔离并将行为特征编码到超边中。最后，我们利用超图网络捕获多边关联知识，以增强漏洞检测。我们在三个广泛使用的漏洞数据集上评估了 IFMA-VD，并展示了与基线方法相比，F-measure 和 Recall 方面的改进。此外，我们还说明了多边关联特征可以增强代码特征表示，并验证了 IFMA-VD 在真实世界数据集上的有效性。", "summary": "本文提出了 IFMA-VD，一个用于漏洞检测的新颖框架，旨在解决现有深度学习方法忽略函数间多边关联的局限性。IFMA-VD 通过构建代码行为超图并使用超边卷积来提取这些关键特征，并将其与来自代码属性图的函数内特征相结合。在三个数据集上进行评估，IFMA-VD 显示出 F-measure 和 Recall 的改进，证明了结合多边关联洞察以增强代码特征表示和漏洞检测的有效性。", "keywords": "漏洞检测, 函数间关联, 超图, 深度学习, 代码分析", "comments": "本文通过使用超图明确建模函数间多边关联，提出了一种新颖的方法，这相对于仅关注单个函数的方法是一个重大进步。利用超图网络捕获复杂关系具有创新性，并且与理解跨多个函数的软件漏洞高度相关。这可能导致更健壮和准确的漏洞检测工具。"}}
{"id": "2506.20969", "title": "ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation", "authors": ["Shruti Bansal", "Wenshan Wang", "Yifei Liu", "Parv Maheshwari"], "summary": "Autonomous systems rely on sensors to estimate the environment around them.\nHowever, cameras, LiDARs, and RADARs have their own limitations. In nighttime\nor degraded environments such as fog, mist, or dust, thermal cameras can\nprovide valuable information regarding the presence of objects of interest due\nto their heat signature. They make it easy to identify humans and vehicles that\nare usually at higher temperatures compared to their surroundings. In this\npaper, we focus on the adaptation of thermal cameras for robotics and\nautomation, where the biggest hurdle is the lack of data. Several multi-modal\ndatasets are available for driving robotics research in tasks such as scene\nsegmentation, object detection, and depth estimation, which are the cornerstone\nof autonomous systems. However, they are found to be lacking in thermal\nimagery. Our paper proposes a solution to augment these datasets with synthetic\nthermal data to enable widespread and rapid adaptation of thermal cameras. We\nexplore the use of conditional diffusion models to convert existing RGB images\nto thermal images using self-attention to learn the thermal properties of\nreal-world objects.", "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.20969v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20969v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ThermalDiffusion：用于自主导航的视觉到热图像到图像转换", "tldr": "该论文提出了一种使用条件扩散模型将RGB图像转换为热图像的方法，以解决自主系统中热图像数据不足的问题，从而增强多模态数据集。", "motivation": "自主系统依赖传感器感知环境，但现有相机、激光雷达和雷达在夜间或恶劣环境下存在局限性。热像仪能提供有价值的信息，但机器人和自动化领域缺乏足够的热图像数据。现有针对自动驾驶的多模态数据集也缺乏热图像，这阻碍了热像仪的广泛应用。", "method": "本文提出使用条件扩散模型，利用自注意力机制学习真实世界物体的热特性，将现有RGB图像转换为热图像，从而合成热数据来扩充数据集。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "自主系统依赖传感器来估计周围环境。然而，相机、激光雷达和雷达都有其自身的局限性。在夜间或恶劣环境，如雾、薄雾或灰尘中，热像仪由于物体的热特征，可以提供关于感兴趣物体存在的宝贵信息。它们使得识别通常比周围环境温度更高的人类和车辆变得容易。在本文中，我们专注于热像仪在机器人和自动化领域的应用，其中最大的障碍是数据的缺乏。有几个多模态数据集可用于自动驾驶机器人研究，涉及场景分割、目标检测和深度估计等任务，这些是自主系统的基石。然而，这些数据集被发现缺乏热图像。我们的论文提出了一种解决方案，通过合成热数据来增强这些数据集，以实现热像仪的广泛和快速适应。我们探索了使用条件扩散模型，利用自注意力机制学习真实世界物体的热特性，将现有RGB图像转换为热图像。", "summary": "本论文提出“ThermalDiffusion”方法，旨在解决自主导航系统中热图像数据不足的问题。针对现有传感器在恶劣环境下的局限性以及多模态数据集中热图像的缺乏，研究者利用条件扩散模型，结合自注意力机制，将RGB图像转换为合成热图像，以扩充现有数据集，从而促进热像仪在机器人和自动化领域的广泛应用。", "keywords": "ThermalDiffusion, 图像到图像转换, 热图像, 自主导航, 扩散模型", "comments": "该论文的创新点在于利用条件扩散模型进行视觉到热图像的转换，以解决热图像数据稀缺的痛点。这对于推动热像仪在全天候自主导航中的应用具有重要意义，尤其是在数据驱动的深度学习时代。其方法通过合成数据来增强现有数据集，提供了一个实用的解决方案，有望加速相关技术的发展和部署。"}}
{"id": "2506.21319", "title": "Multimodal LLMs for Visualization Reconstruction and Understanding", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21319v1", "categories": ["cs.HC", "cs.CV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21319v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多模态大语言模型用于可视化重建与理解", "tldr": "提出了一种新的数据集和专门训练的多模态可视化大语言模型，通过结合图表图像和矢量化表示来改进可视化理解、数据提取和图表重建。", "motivation": "当前的多模态大语言模型在自然图像理解方面表现出色，但在可视化理解上存在困难，因为它们无法解码数据到视觉的映射规则并提取结构化信息。", "method": "研究人员提出了一个新的数据集，并训练了专门用于可视化理解的多模态可视化大语言模型。该方法将图表图像与其对应的矢量化表示、编码方案和数据特征相结合。提出的矢量格式能够紧凑而准确地重建可视化内容。", "result": "实验结果表明，在数据提取准确性和图表重建质量方面都有显著改进。", "conclusion": "通过专门的数据集和模型训练，多模态大语言模型能够显著提升对可视化的理解、数据提取和重建能力，克服了现有模型在这方面的局限性。", "translation": "可视化对于数据交流至关重要，但理解它们需要同时理解视觉元素及其底层数据关系。当前的多模态大模型虽然在自然图像理解方面表现出色，但在可视化方面却举步维艰，因为它们无法解码数据到视觉的映射规则并提取结构化信息。为了解决这些挑战，我们提出了一个新颖的数据集并训练了专门用于理解的多模态可视化大语言模型。我们的方法将图表图像与它们相应的矢量化表示、编码方案和数据特征相结合。所提出的矢量格式能够紧凑而准确地重建可视化内容。实验结果表明，在数据提取准确性和图表重建质量方面都有显著改进。", "summary": "本文针对当前多模态大语言模型在可视化理解上的局限性，提出了一种新颖的方法。通过构建一个结合图表图像、矢量化表示、编码方案和数据特征的专用数据集，并训练了专门的多模态可视化大语言模型，显著提升了数据提取的准确性和图表重建的质量，实现了对可视化内容的有效理解和重建。", "keywords": "多模态大语言模型, 可视化理解, 数据提取, 图表重建, 矢量化表示", "comments": "这项工作通过引入专门的数据集和训练方法，有效地解决了现有通用多模态大模型在处理可视化数据时面临的挑战，特别是在解码数据到视觉映射和结构化信息提取方面的不足。其创新点在于结合了矢量化表示，这对于精确重建和理解图表至关重要。这对于数据分析和人机交互领域具有重要意义。"}}
{"id": "2506.21093", "title": "Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection", "authors": ["Li Fan", "Peng Wang", "Jing Yang", "Cong Shen"], "summary": "Transformers have shown potential in solving wireless communication problems,\nparticularly via in-context learning (ICL), where models adapt to new tasks\nthrough prompts without requiring model updates. However, prior ICL-based\nTransformer models rely on deep architectures with many layers to achieve\nsatisfactory performance, resulting in substantial storage and computational\ncosts. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a\nCoT-enhanced shallow Transformer framework for wireless symbol detection. By\nintroducing autoregressive latent reasoning steps within the hidden space,\nCHOOSE significantly improves the reasoning capacity of shallow models (1-2\nlayers) without increasing model depth. This design enables lightweight\nTransformers to achieve detection performance comparable to much deeper models,\nmaking them well-suited for deployment on resource-constrained mobile devices.\nExperimental results demonstrate that our approach outperforms conventional\nshallow Transformers and achieves performance comparable to that of deep\nTransformers, while maintaining storage and computational efficiency. This\nrepresents a promising direction for implementing Transformer-based algorithms\nin wireless receivers with limited computational resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21093v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21093v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "思维链增强的浅层Transformer用于无线符号检测", "tldr": "提出了一种名为CHOOSE的思维链增强浅层Transformer框架，用于无线符号检测，它在不增加模型深度的情况下显著提升了浅层模型的推理能力，使其在资源受限设备上也能实现与深层模型相当的性能。", "motivation": "现有的基于上下文学习（ICL）的Transformer模型需要深层架构才能达到满意性能，导致巨大的存储和计算成本，这限制了它们在资源受限设备上的部署。", "method": "本文提出CHOOSE（CHain Of thOught Symbol dEtection），一个思维链增强的浅层Transformer框架。通过在隐藏空间中引入自回归潜在推理步骤，CHOOSE显著提升了浅层模型（1-2层）的推理能力，而无需增加模型深度。", "result": "实验结果表明，CHOOSE优于传统的浅层Transformer，并且在保持存储和计算效率的同时，实现了与深层Transformer相当的检测性能。", "conclusion": "该方法为在计算资源有限的无线接收器中部署基于Transformer的算法提供了一个有前景的方向。", "translation": "Transformer在解决无线通信问题，特别是在上下文学习（ICL）方面展现出潜力，其中模型通过提示适应新任务而无需更新模型。然而，先前的基于ICL的Transformer模型依赖于具有多层的深层架构才能实现令人满意的性能，导致巨大的存储和计算成本。在这项工作中，我们提出了CHain Of thOught Symbol dEtection（CHOOSE），一个思维链增强的浅层Transformer框架，用于无线符号检测。通过在隐藏空间中引入自回归潜在推理步骤，CHOOSE在不增加模型深度的情况下显著提升了浅层模型（1-2层）的推理能力。这种设计使得轻量级Transformer能够实现与更深层模型相当的检测性能，使其非常适合部署在资源受限的移动设备上。实验结果表明，我们的方法优于传统的浅层Transformer，并在保持存储和计算效率的同时，实现了与深层Transformer相当的性能。这代表了在计算资源有限的无线接收器中实现基于Transformer算法的一个有前景的方向。", "summary": "本文提出了一种名为CHOOSE的思维链增强浅层Transformer框架，用于无线符号检测。针对现有基于ICL的Transformer模型深度大、成本高的问题，CHOOSE通过在隐藏空间引入自回归潜在推理步骤，显著提升了浅层模型（1-2层）的推理能力，使其在不增加模型深度的情况下，能达到与深层模型相当的性能。实验证明，CHOOSE在保持高效的同时，性能优于传统浅层模型并媲美深层模型，为资源受限的无线设备部署Transformer算法提供了可行方案。", "keywords": "思维链, 浅层Transformer, 无线符号检测, 上下文学习, 资源受限设备", "comments": "这项工作的创新点在于将思维链（CoT）思想引入到浅层Transformer中，以增强其推理能力，从而在保持模型轻量化的同时，达到与深层模型相当的性能。这对于资源受限的移动设备部署Transformer模型具有重要意义，解决了传统深度Transformer计算成本高昂的问题。"}}
{"id": "2506.21033", "title": "BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services", "authors": ["Zhaojiacheng Zhou", "Hongze Liu", "Shijing Yuan", "Hanning Zhang", "Jiong Lou", "Chentao Wu", "Jie Li"], "summary": "The hallucination problem of Large Language Models (LLMs) has increasingly\ndrawn attention. Augmenting LLMs with external knowledge is a promising\nsolution to address this issue. However, due to privacy and security concerns,\na vast amount of downstream task-related knowledge remains dispersed and\nisolated across various \"silos,\" making it difficult to access. To bridge this\nknowledge gap, we propose a blockchain-based external knowledge framework that\ncoordinates multiple knowledge silos to provide reliable foundational knowledge\nfor large model retrieval while ensuring data security. Technically, we distill\nknowledge from local data into prompts and execute transactions and records on\nthe blockchain. Additionally, we introduce a reputation mechanism and\ncross-validation to ensure knowledge quality and provide incentives for\nparticipation. Furthermore, we design a query generation framework that\nprovides a direct API interface for large model retrieval. To evaluate the\nperformance of our proposed framework, we conducted extensive experiments on\nvarious knowledge sources. The results demonstrate that the proposed framework\nachieves efficient LLM service knowledge sharing in blockchain environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21033v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21033v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "BLOCKS：区块链支持的跨孤岛知识共享，实现高效LLM服务", "tldr": "BLOCKS是一个基于区块链的框架，通过协调分散的知识孤岛，为大型语言模型（LLMs）提供可靠的外部知识，以解决幻觉问题，同时确保数据安全和知识质量。", "motivation": "大型语言模型（LLMs）的幻觉问题日益受到关注。通过外部知识增强LLMs是解决此问题的一个有前景的方案。然而，由于隐私和安全问题，大量下游任务相关的知识分散并隔离在不同的“孤岛”中，难以访问。", "method": "我们提出了一个基于区块链的外部知识框架，协调多个知识孤岛，为大型模型检索提供可靠的基础知识，同时确保数据安全。技术上，我们从本地数据中提取知识到提示中，并在区块链上执行交易和记录。此外，我们引入了声誉机制和交叉验证来确保知识质量并激励参与。我们还设计了一个查询生成框架，为大型模型检索提供直接的API接口。", "result": "实验结果表明，所提出的框架在区块链环境中实现了高效的LLM服务知识共享。", "conclusion": "该论文提出了BLOCKS框架，一个基于区块链的解决方案，用于解决LLM的幻觉问题，通过安全地汇集和共享跨孤岛的外部知识，并引入机制确保知识质量和参与激励，最终实现了高效的LLM服务知识共享。", "translation": "大型语言模型（LLMs）的幻觉问题日益受到关注。通过外部知识增强LLMs是解决此问题的一个有前景的方案。然而，由于隐私和安全问题，大量下游任务相关的知识分散并隔离在各种“孤岛”中，难以访问。为了弥补这一知识鸿沟，我们提出了一个基于区块链的外部知识框架，该框架协调多个知识孤岛，为大型模型检索提供可靠的基础知识，同时确保数据安全。技术上，我们从本地数据中提取知识到提示中，并在区块链上执行交易和记录。此外，我们引入了声誉机制和交叉验证来确保知识质量并激励参与。此外，我们设计了一个查询生成框架，为大型模型检索提供直接的API接口。为了评估我们提出的框架的性能，我们对各种知识源进行了广泛的实验。结果表明，所提出的框架在区块链环境中实现了高效的LLM服务知识共享。", "summary": "本文提出了BLOCKS框架，一个基于区块链的解决方案，旨在通过安全、高效地共享分散在不同“孤岛”中的外部知识来解决大型语言模型（LLMs）的幻觉问题。该框架利用区块链记录知识交易，通过提示从本地数据中提取知识，并结合声誉机制和交叉验证确保知识质量和激励参与。它还提供了一个查询生成API接口，实验证明其在区块链环境下实现了高效的LLM服务知识共享。", "keywords": "区块链, 知识共享, 大型语言模型, 幻觉问题, 数据安全", "comments": "该论文的创新点在于将区块链技术应用于解决LLM的知识幻觉问题，通过构建一个去中心化的知识共享框架，有效克服了传统知识共享中存在的隐私和安全障碍。声誉机制和交叉验证的引入，是确保共享知识质量和激励用户参与的关键，这对于构建一个可持续的知识生态系统至关重要。该方法为未来LLM的知识增强和应用提供了新的思路，特别是在需要处理敏感或分布式数据的场景下具有重要潜力。"}}
{"id": "2506.20729", "title": "Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset", "authors": ["Zhiqi Gao", "Tianyi Li", "Yurii Kvasiuk", "Sai Chaitanya Tadepalli", "Maja Rudolph", "Daniel J. H. Chung", "Frederic Sala", "Moritz Münchmeyer"], "summary": "Large language models (LLMs) have shown strong capabilities in complex\nreasoning, and test-time scaling techniques can enhance their performance with\ncomparably low cost. Many of these methods have been developed and evaluated on\nmathematical reasoning benchmarks such as AIME. This paper investigates whether\nthe lessons learned from these benchmarks generalize to the domain of advanced\ntheoretical physics. We evaluate a range of common test-time scaling methods on\nthe TPBench physics dataset and compare their effectiveness with results on\nAIME. To better leverage the structure of physics problems, we develop a novel,\nsymbolic weak-verifier framework to improve parallel scaling results. Our\nempirical results demonstrate that this method significantly outperforms\nexisting test-time scaling approaches on TPBench. We also evaluate our method\non AIME, confirming its effectiveness in solving advanced mathematical\nproblems. Our findings highlight the power of step-wise symbolic verification\nfor tackling complex scientific problems.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20729v1", "categories": ["cs.LG", "astro-ph.CO", "cs.AI", "hep-ph", "hep-th"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20729v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "理论物理中的测试时缩放技术——TPBench数据集上的方法比较", "tldr": "本文研究了大型语言模型（LLMs）在理论物理领域中的测试时缩放技术，并在TPBench数据集上比较了各种方法。作者提出了一种新颖的符号化弱验证器框架，该框架在TPBench上显著优于现有方法，并对AIME数据集也有效，强调了分步符号验证在解决复杂科学问题中的强大作用。", "motivation": "大型语言模型在复杂推理方面表现出色，测试时缩放技术能以低成本提升其性能。然而，许多此类方法主要在数学推理基准（如AIME）上开发和评估。本文旨在探究这些经验是否能推广到高级理论物理领域，并寻找在该领域有效的测试时缩放方法。", "method": "本文在TPBench物理数据集上评估了一系列常见的测试时缩放方法，并将其有效性与AIME上的结果进行比较。为了更好地利用物理问题的结构，研究开发了一种新颖的、符号化的弱验证器框架，以改进并行缩放结果。该新方法也R在AIME数据集上进行了评估。", "result": "所提出的新颖符号化弱验证器框架在TPBench数据集上显著优于现有的测试时缩放方法。该方法在AIME数据集上也证实了其在解决高级数学问题方面的有效性。", "conclusion": "分步符号验证对于解决复杂的科学问题具有强大的能力。", "translation": "大型语言模型（LLMs）在复杂推理方面展现出强大的能力，而测试时缩放技术能以相对较低的成本提升其性能。许多此类方法已在AIME等数学推理基准上开发和评估。本文研究从这些基准中学到的经验是否能推广到高级理论物理领域。我们在TPBench物理数据集上评估了一系列常见的测试时缩放方法，并将其有效性与AIME上的结果进行比较。为了更好地利用物理问题的结构，我们开发了一种新颖的、符号化的弱验证器框架，以改进并行缩放结果。我们的实证结果表明，该方法在TPBench上显著优于现有的测试时缩放方法。我们还在AIME上评估了我们的方法，证实了其在解决高级数学问题方面的有效性。我们的发现强调了分步符号验证在解决复杂科学问题方面的强大能力。", "summary": "本文探讨了大型语言模型（LLMs）的测试时缩放技术在理论物理领域的适用性，旨在将数学推理基准（如AIME）的经验推广到TPBench物理数据集。研究评估了现有方法，并引入了一种新颖的、利用物理问题结构的符号化弱验证器框架。实证结果表明，该新方法在TPBench上显著优于现有方法，并在AIME上同样有效，突出了分步符号验证在解决复杂科学问题中的强大作用。", "keywords": "大型语言模型, 测试时缩放, 理论物理, TPBench, 符号验证", "comments": "该论文的创新之处在于将通常用于数学问题的测试时缩放技术应用于理论物理领域。其开发了一种专门针对物理问题结构的新颖符号化弱验证器框架，并通过在TPBench新数据集上的实证验证，证明了其优于现有方法的性能，这是重要的贡献。它解决了LLM推理能力在不同领域泛化性的关键问题。"}}
{"id": "2506.21302", "title": "Coordinated Control of Autonomous Vehicles for Traffic Density Reduction at a Signalized Junction: An MPC Approach", "authors": ["Rudra Sen", "Subashish Datta"], "summary": "The effective and safe management of traffic is a key issue due to the rapid\nadvancement of the urban transportation system. Connected autonomous vehicles\n(CAVs) possess the capability to connect with each other and adjacent\ninfrastructure, presenting novel opportunities for enhancing traffic flow and\ncoordination. This work proposes a dual-mode model predictive control (MPC)\narchitecture that tackles two interrelated issues: mitigating traffic density\nat signalized junctions and facilitating seamless, cooperative lane changes in\nhigh-density traffic conditions. The objective of this work is to facilitate\nresponsive decision-making for CAVs, thereby enhancing the efficiency and\nsafety of urban mobility. Moreover, we ensure recursive feasibility and\nconvergence of the proposed MPC scheme by the integration of an\nonline-calculated maximal control invariant terminal set. Finally, the efficacy\nof the proposed approach is validated through numerical simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21302v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21302v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "信号交叉口交通密度降低的自动驾驶车辆协同控制：一种MPC方法", "tldr": "本文提出了一种双模模型预测控制（MPC）架构，用于联网自动驾驶车辆（CAVs）在信号交叉口降低交通密度并实现协作车道变换，并通过数值模拟验证了其有效性。", "motivation": "由于城市交通系统的快速发展，有效和安全地管理交通是一个关键问题。联网自动驾驶车辆（CAVs）为增强交通流和协调提供了新的机会。本工作的目标是通过促进CAVs的响应式决策，从而提高城市出行的效率和安全性，特别是在信号交叉口交通密度和高密度交通条件下的协作车道变换问题。", "method": "本文提出了一种双模模型预测控制（MPC）架构，用于解决信号交叉口交通密度缓解和高密度交通条件下的无缝协作车道变换问题。通过整合在线计算的最大控制不变终端集，确保了所提出MPC方案的递归可行性和收敛性。", "result": "所提出方法的有效性通过数值模拟得到了验证。", "conclusion": "本文提出的双模MPC方案通过缓解信号交叉口的交通密度和促进协作车道变换，有效提升了城市出行的效率和安全性，并经数值模拟验证。", "translation": "由于城市交通系统的快速发展，有效和安全地管理交通是一个关键问题。联网自动驾驶车辆（CAVs）具备相互连接和与相邻基础设施连接的能力，为增强交通流和协调提供了新的机会。这项工作提出了一种双模模型预测控制（MPC）架构，解决了两个相互关联的问题：减轻信号交叉口的交通密度和在高密度交通条件下促进无缝、协作的车道变换。这项工作的目标是促进CAVs的响应式决策，从而提高城市出行的效率和安全性。此外，我们通过整合在线计算的最大控制不变终端集，确保了所提出的MPC方案的递归可行性和收敛性。最后，通过数值模拟验证了所提出方法的有效性。", "summary": "本文提出了一种针对联网自动驾驶车辆（CAVs）的双模模型预测控制（MPC）架构，旨在解决信号交叉口的交通密度缓解问题，并促进高密度交通条件下的无缝协作车道变换。该方法通过促进CAVs的响应式决策，从而提高城市出行的效率和安全性。为确保方案的递归可行性和收敛性，集成了在线计算的最大控制不变终端集。所提出的方法已通过数值模拟验证其有效性。", "keywords": "联网自动驾驶车辆, 模型预测控制, 交通密度降低, 信号交叉口, 协作车道变换", "comments": "本文的创新之处在于提出了一种专门针对信号交叉口联网自动驾驶车辆的双模MPC架构，同时解决了交通密度降低和协作车道变换的问题。通过整合在线计算的最大控制不变终端集来确保递归可行性和收敛性，增加了方案的鲁棒性。其重要性在于利用CAVs的能力来实现更智能的城市交通管理。"}}
{"id": "2506.20832", "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "pdf_url": "http://arxiv.org/pdf/2506.20832v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20832v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "利用视觉-语言模型选择扩散模型生成的可靠超分辨率样本", "tldr": "本文提出一个利用视觉-语言模型从扩散模型生成的超分辨率图像中选择最可靠样本的框架，并通过新的可信度分数（TWS）进行评估，该方法比传统指标更能反映信息保真度。", "motivation": "超分辨率是一个病态逆问题，存在多种可行解。回归式SR模型难以平衡保真度和感知质量，常引入伪影，在关键信息应用中造成歧义。扩散模型能生成多样化的SR图像，但从中选择最可靠的解决方案仍是挑战。", "method": "本文引入一个鲁棒、自动化的框架，利用视觉-语言模型（VLM，如BLIP-2, GPT-4o）的语义推理能力，通过结构化查询评估语义正确性、视觉质量和伪影存在，从而从扩散模型生成的SR图像集中识别最可靠的样本。排名靠前的SR候选样本将被集成以获得单一可靠输出。为评估VLM选择的有效性，作者提出了一种新颖的混合度量“可信度分数”（TWS），该分数基于CLIP嵌入的语义相似性、边缘图SSIM的结构完整性和多级小波分解的伪影敏感性三个互补组件来量化SR可靠性。", "result": "实验证明，TWS在模糊图像和自然图像中都与人类偏好高度相关，并且VLM引导的选择始终产生高TWS值。与PSNR、LPIPS等未能反映信息保真度的传统指标相比，该方法提供了一种原则性、可扩展和可推广的解决方案，能更好地反映信息保真度，以应对扩散SR空间的不确定性。", "conclusion": "本工作通过将输出与人类期望和语义正确性对齐，为生成式SR中的可信度设定了新基准，提供了一种克服扩散SR不确定性的有效方法。", "translation": "超分辨率（SR）是一个病态的逆问题，对于给定的低分辨率图像，存在许多可行的解决方案。一方面，回归式SR模型旨在平衡保真度和感知质量以产生单一解决方案，但这种权衡常常引入伪影，在识别数字或字母等信息关键应用中造成歧义。另一方面，扩散模型生成多样化的SR图像，但从这组图像中选择最可靠的解决方案仍然是一个挑战。本文引入了一个鲁棒、自动化的框架，通过利用视觉-语言模型（VLM）的语义推理能力，从扩散模型生成的图像集中识别最可靠的SR样本。具体而言，BLIP-2、GPT-4o及其变体等VLM通过结构化查询来评估语义正确性、视觉质量和伪影存在。然后将排名靠前的SR候选样本进行集成，以经济高效的方式获得单一的可靠输出。为了严格评估VLM选择样本的有效性，我们提出了一种新颖的“可信度分数”（TWS），这是一个混合度量，它基于三个互补组件量化SR可靠性：通过CLIP嵌入的语义相似性、使用边缘图SSIM的结构完整性，以及通过多级小波分解的伪影敏感性。我们通过实验证明，TWS在模糊图像和自然图像中都与人类偏好高度相关，并且VLM引导的选择始终产生高TWS值。与PSNR、LPIPS等未能反映信息保真度的传统指标相比，我们的方法为解决扩散SR空间的不确定性提供了一种原则性、可扩展和可推广的解决方案。通过将输出与人类期望和语义正确性对齐，这项工作为生成式SR中的可信度设定了新基准。", "summary": "本文提出一个利用视觉-语言模型（VLM）从扩散模型生成的超分辨率（SR）图像集中选择最可靠样本的新框架。该框架通过VLM评估SR样本的语义正确性、视觉质量和伪影，并将高排名样本集成。为验证VLM选择的有效性，作者引入了一种新颖的混合度量“可信度分数”（TWS），该分数综合考虑语义相似性、结构完整性和伪影敏感性。实验证明，TWS与人类偏好高度相关，且VLM引导的选择能持续获得高TWS值，表明该方法在信息保真度方面优于传统SR评估指标。", "keywords": "超分辨率, 扩散模型, 视觉-语言模型, 可信度分数, 样本选择", "comments": "创新点：首次将视觉-语言模型应用于扩散模型生成的超分辨率图像的选择和评估，解决了生成多样性但缺乏可靠性选择的痛点。重要性：提出了一个衡量超分辨率图像“可信度”的新颖混合指标TWS，弥补了传统指标在评估信息保真度上的不足，为生成式SR的评估和选择提供了更符合人类感知的标准。"}}
{"id": "2506.21230", "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "authors": ["Junhao Shi", "Zhaoye Fei", "Siyin Wang", "Qipeng Guo", "Jingjing Gong", "Xipeng QIu"], "summary": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks\nbut struggle with complex scenarios involving unfamiliar environments and\nmulti-step goals. Current approaches rely on environment-agnostic imitation\nlearning that disconnects instructions from environmental contexts, causing\nmodels to struggle with context-sensitive instructions and rely on\nsupplementary cues rather than visual reasoning during long-horizon\ninteractions. In this work, we propose World-Aware Planning Narrative\nEnhancement (WAP), a framework that infuses LVLMs with comprehensive\nenvironmental understanding through four cognitive capabilities (visual\nappearance modeling, spatial reasoning, functional abstraction, and syntactic\ngrounding) while developing and evaluating models using only raw visual\nobservations through curriculum learning. Evaluations on the EB-ALFRED\nbenchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a\n60.7 absolute improvement in task success rates, particularly in commonsense\nreasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced\nopen-source models outperform proprietary systems like GPT-4o and\nClaude-3.5-Sonnet by a large margin.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21230v1", "categories": ["cs.AI", "cs.RO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21230v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "世界感知规划叙事增强大型视觉-语言模型规划器", "tldr": "大型视觉-语言模型（LVLMs）在复杂具身规划中表现不佳。本文提出WAP框架，通过注入环境理解能力和课程学习，显著提升LVLMs的任务成功率，并使其超越了GPT-4o等专有模型。", "motivation": "大型视觉-语言模型（LVLMs）在具身规划任务中面临挑战，尤其是在不熟悉的环境和多步骤目标下。现有方法依赖于环境无关的模仿学习，导致指令与环境脱节，模型难以处理上下文敏感指令，并过度依赖补充线索而非视觉推理。", "method": "本文提出了世界感知规划叙事增强（WAP）框架，通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法基础）为LVLMs注入全面的环境理解。模型开发和评估仅使用原始视觉观测，并采用课程学习。", "result": "在EB-ALFRED基准测试中，Qwen2.5-VL的任务成功率绝对提高了60.7%，其中常识推理提高了60.0%，长距离规划提高了70.0%。增强后的开源模型在很大程度上优于GPT-4o和Claude-3.5-Sonnet等专有系统。", "conclusion": "WAP框架通过注入全面的环境理解和采用课程学习，显著提升了LVLMs在具身规划任务上的性能，使其在复杂场景下表现更优，并超越了现有SOTA专有模型。", "translation": "大型视觉-语言模型（LVLMs）在具身规划任务中显示出前景，但在涉及不熟悉环境和多步骤目标的复杂场景中表现不佳。当前方法依赖于环境无关的模仿学习，这使得指令与环境上下文脱节，导致模型难以处理上下文敏感指令，并在长时间交互过程中依赖补充线索而非视觉推理。在这项工作中，我们提出了世界感知规划叙事增强（WAP），这是一个通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法基础）为LVLMs注入全面环境理解的框架，同时仅通过课程学习使用原始视觉观测来开发和评估模型。对EB-ALFRED基准的评估表明，任务成功率显著提高，Qwen2.5-VL的绝对成功率提高了60.7%，特别是在常识推理（+60.0）和长距离规划（+70.0）方面。值得注意的是，我们增强的开源模型在很大程度上优于GPT-4o和Claude-3.5-Sonnet等专有系统。", "summary": "本文提出了世界感知规划叙事增强（WAP）框架，旨在解决大型视觉-语言模型（LVLMs）在复杂具身规划任务中环境理解和视觉推理不足的问题。WAP通过整合视觉外观建模、空间推理、功能抽象和句法基础四种认知能力，并结合课程学习，显著提升了LVLMs在未知环境和多步骤目标下的表现。在EB-ALFRED基准测试中，WAP使Qwen2.5-VL的任务成功率大幅提升，并使得开源模型超越了GPT-4o和Claude-3.5-Sonnet等专有系统。", "keywords": "大型视觉-语言模型, 具身规划, 世界感知, 认知能力, 课程学习", "comments": "该研究通过引入“世界感知规划叙事增强”（WAP）框架，解决了LVLMs在具身规划中缺乏环境理解和视觉推理能力的核心问题。其创新点在于将四种认知能力（视觉外观建模、空间推理、功能抽象、句法基础）系统地融入LVLMs，并通过课程学习仅利用原始视觉观测进行训练。这一方法不仅显著提升了模型在复杂场景下的性能，还在开源模型上实现了超越专有SOTA模型的突破，为具身智能领域的发展提供了重要的方向和潜力。"}}
{"id": "2506.21171", "title": "Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations", "authors": ["Jing Yang", "Qunliang Xing", "Mai Xu", "Minglang Qiao"], "summary": "Joint Photographic Experts Group (JPEG) achieves data compression by\nquantizing Discrete Cosine Transform (DCT) coefficients, which inevitably\nintroduces compression artifacts. Most existing JPEG quality enhancement\nmethods operate in the pixel domain, suffering from the high computational\ncosts of decoding. Consequently, direct enhancement of JPEG images in the DCT\ndomain has gained increasing attention. However, current DCT-domain methods\noften exhibit limited performance. To address this challenge, we identify two\ncritical types of correlations within the DCT coefficients of JPEG images.\nBuilding on this insight, we propose an Advanced DCT-domain JPEG Quality\nEnhancement (AJQE) method that fully exploits these correlations. The AJQE\nmethod enables the adaptation of numerous well-established pixel-domain models\nto the DCT domain, achieving superior performance with reduced computational\ncomplexity. Compared to the pixel-domain counterparts, the DCT-domain models\nderived by our method demonstrate a 0.35 dB improvement in PSNR and a 60.5%\nincrease in enhancement throughput on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21171v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21171v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在DCT中发掘宝藏：通过利用潜在相关性推进JPEG图像质量增强", "tldr": "该研究提出了一种新的DCT域JPEG质量增强方法（AJQE），通过利用DCT系数中的潜在相关性，实现了比现有像素域方法更高的性能和更低的计算成本。", "motivation": "JPEG图像在离散余弦变换（DCT）系数量化过程中会引入压缩伪影。大多数现有像素域的JPEG质量增强方法计算成本高昂。因此，直接在DCT域进行增强受到了关注，但现有DCT域方法的性能有限。", "method": "该研究识别了JPEG图像DCT系数中的两种关键相关性。基于此，提出了一种先进的DCT域JPEG质量增强（AJQE）方法，充分利用这些相关性。AJQE方法能够将许多成熟的像素域模型应用于DCT域。", "result": "与像素域对应方法相比，通过该方法导出的DCT域模型在PSNR上平均提高了0.35 dB，增强吞吐量平均增加了60.5%。", "conclusion": "通过在DCT域中识别并利用潜在相关性，所提出的AJQE方法显著提高了JPEG图像质量增强的性能，并降低了计算复杂性，证明了DCT域增强的优越性。", "translation": "联合图像专家组（JPEG）通过量化离散余弦变换（DCT）系数实现数据压缩，这不可避免地引入了压缩伪影。大多数现有的JPEG质量增强方法在像素域操作，但受限于解码的高计算成本。因此，直接在DCT域对JPEG图像进行增强受到了越来越多的关注。然而，当前的DCT域方法往往性能有限。为了解决这一挑战，我们识别了JPEG图像DCT系数中的两种关键相关性。基于这一洞察，我们提出了一种先进的DCT域JPEG质量增强（AJQE）方法，该方法充分利用了这些相关性。AJQE方法使得许多成熟的像素域模型能够适应DCT域，以更低的计算复杂性实现卓越的性能。与像素域对应方法相比，我们方法导出的DCT域模型在PSNR上平均提高了0.35 dB，增强吞吐量平均增加了60.5%。", "summary": "本文提出了一种名为AJQE的先进DCT域JPEG质量增强方法，旨在解决现有像素域方法计算成本高和DCT域方法性能有限的问题。通过识别并充分利用JPEG图像DCT系数中的两种关键相关性，AJQE方法成功地将成熟的像素域模型迁移到DCT域，从而在提高图像质量的同时显著降低了计算开销。实验结果表明，该方法在PSNR和增强吞吐量方面均优于传统的像素域方法。", "keywords": "JPEG质量增强, DCT域, 压缩伪影, 图像处理, 相关性", "comments": "该论文的创新点在于识别并利用了DCT系数中的潜在相关性，从而成功地将像素域的先进模型应用于DCT域，实现了性能提升和计算效率的优化。这为JPEG图像质量增强提供了一个新的、高效的途径，对于资源受限的应用场景具有重要意义。"}}
{"id": "2506.20970", "title": "Co-Design of Sensing, Communications, and Control for Low-Altitude Wireless Networks", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Fan Liu", "Yuanhao Cui"], "summary": "The rapid advancement of Internet of Things (IoT) services and the evolution\ntoward the sixth generation (6G) have positioned unmanned aerial vehicles\n(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This\nwork investigates the co-design of integrated sensing, communication, and\ncontrol ($\\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite\nblocklength (FBL) transmission. In particular, the UAVs continuously monitor\nthe state of the field robots and transmit their observations to the robot\ncontroller to ensure stable control while cooperating to localize an unknown\nsensing target (ST). To this end, a weighted optimization problem is first\nformulated by jointly considering the control and localization performance in\nterms of the linear quadratic regulator (LQR) cost and the determinant of the\nFisher information matrix (FIM), respectively. The resultant problem,\noptimizing resource allocations, the UAVs' deployment positions, and multi-user\nscheduling, is non-convex. To circumvent this challenge, we first derive a\nclosed-form expression of the LQR cost with respect to other variables.\nSubsequently, the non-convex optimization problem is decomposed into a series\nof sub-problems by leveraging the alternating optimization (AO) approach, in\nwhich the difference of convex functions (DC) programming and projected\ngradient descent (PGD) method are employed to obtain an efficient near-optimal\nsolution. Furthermore, the convergence and computational complexity of the\nproposed algorithm are thoroughly analyzed. Extensive simulation results are\npresented to validate the effectiveness of our proposed approach compared to\nthe benchmark schemes and reveal the trade-off between control and sensing\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20970v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20970v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "低空无线网络中感知、通信与控制的协同设计", "tldr": "本文研究了多无人机合作系统中集成感知、通信和控制的协同设计，旨在实现稳定的机器人控制和未知目标定位，并提出了一种基于交替优化和凸差分规划的非凸问题求解方法。", "motivation": "物联网服务和6G技术的快速发展使得无人机成为低空无线网络（LAWNs）的关键使能者。本研究旨在解决多无人机合作系统中，如何在有限块长传输条件下，协同设计感知、通信和控制（SC²），以确保机器人控制稳定并同时定位未知感知目标。", "method": "研究首先将控制和定位性能（分别用LQR成本和FIM行列式衡量）联合考虑，建立了一个加权优化问题，该问题是非凸的，涉及资源分配、无人机部署位置和多用户调度。为解决此问题，首先推导了LQR成本的闭式表达式。随后，利用交替优化（AO）方法将非凸问题分解为一系列子问题，并采用凸差分（DC）规划和投影梯度下降（PGD）方法获得高效的次优解。此外，还对所提算法的收敛性和计算复杂度进行了分析。", "result": "广泛的仿真结果验证了所提出方法相对于基准方案的有效性，并揭示了控制与感知性能之间的权衡。", "conclusion": "本文成功地提出了低空无线网络中感知、通信与控制的协同设计方案，通过优化资源分配、无人机部署和调度，实现了多无人机合作系统中机器人控制的稳定性和未知目标定位，并验证了其有效性。", "translation": "物联网（IoT）服务的快速发展以及向第六代（6G）的演进，已将无人机（UAV）定位为低空无线网络（LAWNs）的关键使能者。本工作研究了多无人机合作系统中，集成感知、通信和控制（SC²）在有限块长（FBL）传输条件下的协同设计。具体而言，无人机持续监测地面机器人的状态并将其观测结果传输给机器人控制器，以确保稳定控制，同时协同定位一个未知感知目标（ST）。为此，首先通过联合考虑线性二次调节器（LQR）成本和费舍尔信息矩阵（FIM）的行列式来衡量控制和定位性能，从而构建了一个加权优化问题。由此产生的优化资源分配、无人机部署位置和多用户调度的问题是非凸的。为了规避这一挑战，我们首先推导了LQR成本相对于其他变量的闭式表达式。随后，通过利用交替优化（AO）方法，将非凸优化问题分解为一系列子问题，其中采用凸差分（DC）规划和投影梯度下降（PGD）方法来获得高效的次优解。此外，还彻底分析了所提出算法的收敛性和计算复杂度。本文提供了大量的仿真结果，以验证我们提出的方法与基准方案相比的有效性，并揭示了控制和感知性能之间的权衡。", "summary": "本文针对低空无线网络中的多无人机合作系统，研究了集成感知、通信与控制（SC²）的协同设计，特别是在有限块长传输条件下。研究目标是确保地面机器人控制的稳定性，并同时实现未知感知目标的定位。为此，作者提出了一个联合考虑控制性能（LQR成本）和定位性能（FIM行列式）的加权非凸优化问题，该问题涉及资源分配、无人机部署和多用户调度。为解决此非凸问题，文中首先推导了LQR成本的闭式表达式，然后采用交替优化（AO）框架，结合凸差分（DC）规划和投影梯度下降（PGD）方法，得到了高效的次优解。仿真结果验证了所提方法的有效性，并分析了控制与感知性能的权衡。", "keywords": "无人机, 低空无线网络, 感知-通信-控制协同设计, 有限块长传输, 非凸优化", "comments": "本文的创新点在于将感知、通信和控制进行协同设计，以解决低空无线网络中多无人机合作系统的复杂问题，尤其是在有限块长传输的约束下。通过将LQR成本和FIM行列式结合进行优化，并巧妙地利用交替优化、DC规划和PGD方法来处理非凸问题，提供了一个实用的解决方案。这项工作对于未来6G环境下的无人机应用，如机器人控制和目标定位，具有重要的理论和实际意义。"}}
{"id": "2506.21425", "title": "IDGraphs: Intrusion Detection and Analysis Using Stream Compositing", "authors": ["Pin Ren", "Yan Gao", "Zhichun Li", "Yan Chen", "Benjamin Watson"], "summary": "Traffic anomalies and attacks are commonplace in today's networks and\nidentifying them rapidly and accurately is critical for large network\noperators. For a statistical intrusion detection system (IDS), it is crucial to\ndetect at the flow-level for accurate detection and mitigation. However,\nexisting IDS systems offer only limited support for 1) interactively examining\ndetected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)\nand discovering correlated attacks. These problems are becoming even more acute\nas the traffic on today's high-speed routers continues to grow.\n  IDGraphs is an interactive visualization system for intrusion detection that\naddresses these challenges. The central visualization in the system is a\nflow-level trace plotted with time on the horizontal axis and aggregated number\nof unsuccessful connections on the vertical axis. We then summarize a stack of\ntens or hundreds of thousands of these traces using the Histographs [RW05]\ntechnique, which maps data frequency at each pixel to brightness. Users may\nthen interactively query the summary view, performing analysis by highlighting\nsubsets of the traces. For example, brushing a linked correlation matrix view\nhighlights traces with similar patterns, revealing distributed attacks that are\ndifficult to detect using standard statistical analysis.\n  We apply IDGraphs system to a real network router data-set with 179M\nflow-level records representing a total traffic of 1.16TB. The system\nsuccessfully detects and analyzes a variety of attacks and anomalies, including\nport scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21425v1", "categories": ["cs.GR", "cs.CR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21425v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "IDGraphs：使用流合成的入侵检测与分析", "tldr": "IDGraphs是一个交互式可视化系统，用于入侵检测，它通过流级跟踪的可视化和Histographs技术来检测和分析各种网络攻击和异常，解决了现有IDS的局限性。", "motivation": "现有入侵检测系统在交互式检查入侵、分析蠕虫传播模式和发现相关攻击方面支持有限，且面对高速路由器流量增长时问题更加突出，因此需要一种新的解决方案。", "method": "IDGraphs是一个交互式可视化系统，其核心是流级跟踪的可视化，以时间为横轴，聚合的未成功连接数为纵轴。系统使用Histographs技术总结大量跟踪，将数据频率映射到亮度，并允许用户交互式查询和分析摘要视图，例如通过关联矩阵视图突出显示相似模式的跟踪以发现分布式攻击。", "result": "IDGraphs系统成功应用于包含1.79亿流级记录的真实网络路由器数据集，并成功检测并分析了多种攻击和异常，包括端口扫描、蠕虫爆发、隐蔽TCP SYN洪水和一些分布式攻击。", "conclusion": "IDGraphs系统能够有效解决现有入侵检测系统面临的挑战，提供了一种准确、快速地检测和分析网络流量异常和攻击的方法。", "translation": "当今网络中，流量异常和攻击司空见惯，对于大型网络运营商而言，快速准确地识别它们至关重要。对于统计入侵检测系统（IDS）而言，在流级别进行检测对于准确检测和缓解至关重要。然而，现有IDS系统在以下方面提供的支持有限：1）交互式检查已检测到的入侵和异常，2）分析蠕虫传播模式，3）发现相关攻击。随着当今高速路由器上的流量持续增长，这些问题变得更加严峻。\nIDGraphs是一个交互式可视化系统，用于入侵检测，旨在解决这些挑战。该系统的核心可视化是一个流级跟踪，以时间为横轴，聚合的未成功连接数为纵轴绘制。然后，我们使用Histographs [RW05] 技术总结了数万甚至数十万条此类跟踪，该技术将每个像素的数据频率映射到亮度。用户可以交互式查询摘要视图，通过突出显示跟踪的子集进行分析。例如，刷选链接的关联矩阵视图会突出显示具有相似模式的跟踪，从而揭示使用标准统计分析难以检测的分布式攻击。\n我们将IDGraphs系统应用于一个真实的网络路由器数据集，该数据集包含1.79亿条流级记录，总流量为1.16TB。该系统成功检测并分析了各种攻击和异常，包括端口扫描、蠕虫爆发、隐蔽TCP SYN洪水以及一些分布式攻击。", "summary": "IDGraphs是一个创新的交互式可视化系统，旨在解决现有入侵检测系统在检测和分析网络入侵方面的局限性。它通过可视化流级跟踪并利用Histographs技术汇总大量数据，使用户能够交互式地识别和分析各种攻击模式，包括难以发现的分布式攻击。该系统在真实网络数据上的应用证明了其在检测端口扫描、蠕虫爆发和TCP SYN洪水等攻击方面的有效性。", "keywords": "入侵检测, 流量分析, 可视化, 流合成, 网络安全", "comments": "IDGraphs的创新之处在于其将交互式可视化与流级数据分析相结合，特别是引入了Histographs技术来高效处理和总结大规模网络流量数据，从而有效解决了传统IDS在分析复杂和分布式攻击方面的不足。其对实际大型网络数据集的应用验证了其有效性和实用性。"}}
{"id": "2506.21118", "title": "Courcelle's Theorem for Lipschitz Continuity", "authors": ["Tatsuya Gima", "Soh Kumabe", "Yuichi Yoshida"], "summary": "Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida\n(FOCS'23), measures the stability of an algorithm against small input\nperturbations. Algorithms with small Lipschitz continuity are desirable, as\nthey ensure reliable decision-making and reproducible scientific research.\nSeveral studies have proposed Lipschitz continuous algorithms for various\ncombinatorial optimization problems, but these algorithms are problem-specific,\nrequiring a separate design for each problem.\n  To address this issue, we provide the first algorithmic meta-theorem in the\nfield of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz\ncontinuous analogue of Courcelle's theorem, which offers Lipschitz continuous\nalgorithms for problems on bounded-treewidth graphs. Specifically, we consider\nthe problem of finding a vertex set in a graph that maximizes or minimizes the\ntotal weight, subject to constraints expressed in monadic second-order logic\n(MSO_2). We show that for any $\\varepsilon>0$, there exists a $(1\\pm\n\\varepsilon)$-approximation algorithm for the problem with a polylogarithmic\nLipschitz constant on bounded treewidth graphs. On such graphs, our result\noutperforms most existing Lipschitz continuous algorithms in terms of\napproximability and/or Lipschitz continuity. Further, we provide similar\nresults for problems on bounded-clique-width graphs subject to constraints\nexpressed in MSO_1. Additionally, we construct a Lipschitz continuous version\nof Baker's decomposition using our meta-theorem as a subroutine.", "comment": "ESA 2025, 27 pages", "pdf_url": "http://arxiv.org/pdf/2506.21118v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21118v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于Lipschitz连续性的Courcelle定理", "tldr": "本文提出了Lipschitz连续算法领域的第一个算法元定理，可以看作是Courcelle定理的Lipschitz连续模拟，为有界树宽和有界团宽图上满足MSO逻辑约束的问题提供了具有多对数Lipschitz常数的近似算法。", "motivation": "Lipschitz连续性衡量算法对输入扰动的稳定性，具有小Lipschitz常数的算法是理想的。然而，现有Lipschitz连续算法都是针对特定问题设计的。本文旨在提供一个通用的算法元定理来解决这一问题。", "method": "本文提出了Lipschitz连续算法的第一个算法元定理，该定理是Courcelle定理的Lipschitz连续模拟。具体地，针对在有界树宽图上，在满足MSO_2逻辑约束下最大化或最小化总权重的顶点集问题，以及在有界团宽图上满足MSO_1逻辑约束的问题，应用此元定理。此外，利用该元定理作为子程序，构建了Baker分解的Lipschitz连续版本。", "result": "对于有界树宽图上满足MSO_2约束的问题，本文提出了一个具有多对数Lipschitz常数的$(1\\pm \\varepsilon)$-近似算法。该结果在近似性和/或Lipschitz连续性方面优于大多数现有Lipschitz连续算法。对于有界团宽图上满足MSO_1约束的问题，也得到了类似的结果。此外，成功构建了Baker分解的Lipschitz连续版本。", "conclusion": "本文提供了Lipschitz连续算法领域的第一个算法元定理，为在有界树宽和有界团宽图上设计稳定的近似算法提供了一个通用框架，并超越了现有针对特定问题的算法。", "translation": "Lipschitz连续性算法由Kumabe和Yoshida（FOCS'23）引入，衡量算法对抗小输入扰动的稳定性。具有小Lipschitz连续性的算法是理想的，因为它们确保了可靠的决策和可重复的科学研究。一些研究已经提出了针对各种组合优化问题的Lipschitz连续算法，但这些算法是针对特定问题的，每个问题都需要单独设计。\n为了解决这个问题，我们提供了Lipschitz连续算法领域的第一个算法元定理。我们的结果可以看作是Courcelle定理的Lipschitz连续模拟，它为有界树宽图上的问题提供了Lipschitz连续算法。具体来说，我们考虑在图上找到一个顶点集，该顶点集在满足单二阶逻辑（MSO_2）表达的约束下最大化或最小化总权重的问题。我们表明，对于任何$\\varepsilon>0$，存在一个$(1\\pm \\varepsilon)$-近似算法，其在有界树宽图上的Lipschitz常数为多对数。在这些图上，我们的结果在近似性和/或Lipschitz连续性方面优于大多数现有Lipschitz连续算法。此外，我们为受MSO_1表达约束的有界团宽图上的问题提供了类似的结果。此外，我们利用我们的元定理作为子程序，构建了Baker分解的Lipschitz连续版本。", "summary": "本文提出了Lipschitz连续算法领域的第一个算法元定理，该定理是Courcelle定理的Lipschitz连续模拟。它为在有界树宽和有界团宽图上，受单二阶逻辑（MSO）约束的问题，提供了具有多对数Lipschitz常数的$(1\\pm \\varepsilon)$-近似算法。这项工作通过提供一个通用的框架来解决现有Lipschitz连续算法的特定问题限制，并在稳定性和近似质量方面优于现有方法。该元定理还被用于构建Baker分解的Lipschitz连续版本。", "keywords": "Lipschitz连续性, 算法元定理, Courcelle定理, 有界树宽, 单二阶逻辑", "comments": "本文通过引入一个元定理，为Lipschitz连续算法领域做出了重要贡献，将范式从特定问题设计转变为更统一的框架。这可能加速在结构化图上为各种组合问题开发稳定算法。与Courcelle定理的类比突出了其理论深度和潜在影响。"}}
{"id": "2506.21032", "title": "RecCoT: Enhancing Recommendation via Chain-of-Thought", "authors": ["Shuo Yang", "Jiangxia Cao", "Haipeng Li", "Yuqi Mao", "Shuchao Pang"], "summary": "In real-world applications, users always interact with items in multiple\naspects, such as through implicit binary feedback (e.g., clicks, dislikes, long\nviews) and explicit feedback (e.g., comments, reviews). Modern recommendation\nsystems (RecSys) learn user-item collaborative signals from these implicit\nfeedback signals as a large-scale binary data-streaming, subsequently\nrecommending other highly similar items based on users' personalized historical\ninteractions. However, from this collaborative-connection perspective, the\nRecSys does not focus on the actual content of the items themselves but instead\nprioritizes higher-probability signals of behavioral co-occurrence among items.\nConsequently, under this binary learning paradigm, the RecSys struggles to\nunderstand why a user likes or dislikes certain items. To alleviate it, some\nworks attempt to utilize the content-based reviews to capture the semantic\nknowledge to enhance recommender models. However, most of these methods focus\non predicting the ratings of reviews, but do not provide a human-understandable\nexplanation.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.21032v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21032v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "RecCoT：通过思维链增强推荐系统", "tldr": "现有推荐系统难以解释用户偏好，因为它们侧重于二元反馈和行为共现，而非物品内容或提供人类可理解的解释。", "motivation": "现代推荐系统主要依赖隐式二元反馈学习用户-物品协作信号，推荐相似物品。然而，这种方法不关注物品内容，而是优先考虑行为共现信号，导致系统难以理解用户喜欢或不喜欢某些物品的原因。现有利用内容评论的方法也多侧重于预测评分，未能提供人类可理解的解释。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在实际应用中，用户总是通过多个方面与物品进行互动，例如通过隐式二元反馈（如点击、不点赞、长时间观看）和显式反馈（如评论、评价）。现代推荐系统（RecSys）从这些隐式反馈信号中学习用户-物品协作信号，作为大规模的二元数据流，随后根据用户的个性化历史互动推荐其他高度相似的物品。然而，从这种协作连接的角度来看，推荐系统不关注物品本身的实际内容，而是优先考虑物品之间行为共现的更高概率信号。因此，在这种二元学习范式下，推荐系统难以理解用户喜欢或不喜欢某些物品的原因。为了缓解这个问题，一些工作尝试利用基于内容的评论来捕获语义知识，以增强推荐模型。然而，这些方法大多侧重于预测评论的评分，但没有提供人类可理解的解释。", "summary": "现代推荐系统主要依赖隐式二元反馈来学习协作信号并推荐相似物品。然而，这种范式侧重于行为共现而非物品内容，导致系统难以理解用户偏好的深层原因。尽管一些方法尝试利用内容评论来捕获语义知识，但它们通常只预测评分，未能提供人类可理解的解释。", "keywords": "推荐系统, 思维链, 隐式反馈, 解释性, 用户偏好", "comments": "该论文指出了当前推荐系统的一个关键局限性：缺乏可解释性以及对用户偏好的理解，而不仅仅是简单的共现。标题中提及的“思维链”方法预示着一种创新途径，有望弥补这一空白，从而实现更透明、更用户友好的推荐。论文的动机阐述清晰。"}}
{"id": "2506.20876", "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20876v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20876v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "少做决策，多做沟通：论医学领域端到端事实核查的构念效度", "tldr": "医学领域的端到端事实核查系统仍未被广泛使用，本研究揭示了其在应用中面临的挑战，并提出应将其视为交互式沟通问题。", "motivation": "医学决策的重要性以及批判性评估大量医学文献的挑战，促使人们对在公共卫生和医学领域采用自动化事实核查系统产生了兴趣。然而，尽管有这些需求和技术进步，端到端事实核查系统在医学领域仍未被广泛使用。本研究旨在理解这一现象，并探讨其构念效度。", "method": "本研究首次通过分析临床专家如何合成医学证据来验证社交媒体上的真实声明，以理解端到端事实核查的上限。", "result": "研究揭示了在医学领域应用端到端事实核查的根本挑战，包括：难以将“野外”声明与临床试验形式的科学证据联系起来；不明确声明中的歧义以及意图不匹配；以及本质上主观的真实性标签。", "conclusion": "研究认为，事实核查应被视为一个交互式沟通问题来处理和评估，而非一个简单的端到端过程。", "translation": "技术进步已在自动事实核查等曾被视为具有挑战性的任务上取得了具体进展。鉴于医学决策的高风险性质以及批判性评估大量多样化医学文献的挑战，人们对在公共卫生和医学领域采用这些系统的兴趣日益增长。循证医学与每个人息息相关，但其本质高度专业化，使得大多数用户的医学素养不足以充分驾驭该领域。医学沟通的此类问题为端到端事实核查代理奠定了基础：针对当前医学文献核查声明并返回有证据支持的结论。然而，此类系统仍未被广泛使用。为了理解这一点，我们首次研究了临床专家如何通过综合医学证据来验证社交媒体上的真实声明。在寻找这一上限的过程中，我们揭示了端到端事实核查应用于医学时面临的根本挑战：将“野外”声明与临床试验形式的科学证据联系起来的困难；不明确声明中混杂着不匹配意图的歧义；以及本质上主观的真实性标签。我们认为，事实核查应被视为一个交互式沟通问题来处理和评估，而非一个端到端过程。", "summary": "本研究探讨了医学领域端到端事实核查系统未被广泛使用的原因。通过分析临床专家如何验证社交媒体上的医学声明，研究揭示了当前端到端方法面临的挑战，包括连接声明与证据的困难、声明的歧义以及真实性标签的主观性。文章主张将事实核查视为一个交互式沟通问题，而非简单的端到端过程。", "keywords": "端到端事实核查, 医学, 构念效度, 医疗沟通, 临床专家", "comments": "该论文对当前流行的端到端AI解决方案在复杂且高风险领域（如医学）的应用提出了批判性视角。它强调了在处理真实世界、模糊和主观信息时，纯粹的自动化决策的局限性，并提出了一种更注重交互和沟通的范式，这对于未来AI在医疗健康领域的负责任发展具有重要指导意义。"}}
{"id": "2506.21065", "title": "Entropy-stable in- and outflow boundary conditions for the compressible Navier-Stokes equations", "authors": ["Magnus Svärd", "Anita Gjesteland"], "summary": "We propose inflow and outflow boundary conditions for the compressible\nNavier-Stokes equations and prove that they allow a priori estimates of the\nentropy, mass and total energy. Furthermore, we demonstrate how to approximate\nthese boundary conditions in conjunction with an entropy-stable finite-volume\nscheme. The method is also applicable to other types of entropy-stable schemes.\nFinally, we carry out some numerical computations with the finite-volume scheme\nand demonstrate their robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21065v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21065v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可压缩Navier-Stokes方程的熵稳定入流和出流边界条件", "tldr": "本文提出了可压缩Navier-Stokes方程的熵稳定入流和出流边界条件，证明了它们允许对熵、质量和总能量进行先验估计，并展示了如何将它们与熵稳定有限体积格式结合应用，通过数值计算验证了其鲁棒性。", "motivation": "本文旨在为可压缩Navier-Stokes方程提出入流和出流边界条件，并证明这些条件允许对熵、质量和总能量进行先验估计，同时展示如何将这些条件与熵稳定的有限体积格式结合使用，并通过数值计算验证其鲁棒性。", "method": "本文提出了可压缩Navier-Stokes方程的入流和出流边界条件。研究人员证明了这些条件允许对熵、质量和总能量进行先验估计。此外，他们展示了如何将这些边界条件与熵稳定的有限体积格式结合进行近似，并指出该方法也适用于其他类型的熵稳定格式。最后，通过使用有限体积格式进行数值计算来验证其鲁棒性。", "result": "所提出的边界条件允许对可压缩Navier-Stokes方程的熵、质量和总能量进行先验估计。与熵稳定的有限体积格式结合使用时，这些边界条件的近似方法通过数值计算验证了其鲁棒性。", "conclusion": "本文成功提出了并证明了可压缩Navier-Stokes方程的鲁棒熵稳定入流和出流边界条件，这些条件与熵稳定格式兼容，并允许重要的先验估计。", "translation": "我们提出了可压缩Navier-Stokes方程的入流和出流边界条件，并证明它们允许对熵、质量和总能量进行先验估计。此外，我们展示了如何将这些边界条件与熵稳定的有限体积格式结合起来进行近似。该方法也适用于其他类型的熵稳定格式。最后，我们使用有限体积格式进行了一些数值计算，并证明了它们的鲁棒性。", "summary": "本文针对可压缩Navier-Stokes方程引入了新型熵稳定的入流和出流边界条件。研究证明这些条件能够实现熵、质量和总能量的先验估计。文中详细阐述了将这些条件与熵稳定有限体积格式（以及其他熵稳定格式）结合进行近似的方法，并通过数值计算验证了其鲁棒性。", "keywords": "熵稳定, 边界条件, Navier-Stokes方程, 有限体积格式, 可压缩流", "comments": "本文的创新之处在于提出了能够保证熵稳定并允许先验估计的边界条件，这对于可压缩流体数值模拟的稳定性和准确性至关重要。同时，展示了这些条件与有限体积格式的兼容性及其鲁棒性，也是一项重要的贡献。"}}
{"id": "2506.21440", "title": "Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform", "authors": ["Maxime Leiber", "Yosra Marnissi", "Axel Barrau", "Sylvain Meignen", "Laurent Massoulié"], "summary": "The short-time Fourier transform (STFT) is widely used for analyzing\nnon-stationary signals. However, its performance is highly sensitive to its\nparameters, and manual or heuristic tuning often yields suboptimal results. To\novercome this limitation, we propose a unified differentiable formulation of\nthe STFT that enables gradient-based optimization of its parameters. This\napproach addresses the limitations of traditional STFT parameter tuning\nmethods, which often rely on computationally intensive discrete searches. It\nenables fine-tuning of the time-frequency representation (TFR) based on any\ndesired criterion. Moreover, our approach integrates seamlessly with neural\nnetworks, allowing joint optimization of the STFT parameters and network\nweights. The efficacy of the proposed differentiable STFT in enhancing TFRs and\nimproving performance in downstream tasks is demonstrated through experiments\non both simulated and real-world data.", "comment": "DSTFT, STFT, spectrogram, time-frequency, IEEE Transactions on Signal\n  Processing, 10 pages", "pdf_url": "http://arxiv.org/pdf/2506.21440v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21440v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可学习自适应时频表示通过可微分短时傅里叶变换", "tldr": "提出一种可微分短时傅里叶变换(STFT)公式，实现STFT参数的梯度优化，解决了传统STFT参数调优的局限性，并能与神经网络联合优化，有效提升时频表示和下游任务性能。", "motivation": "传统的短时傅里叶变换（STFT）在分析非平稳信号时，其性能对参数高度敏感，且手动或启发式调优常导致次优结果。传统的参数调优方法依赖计算密集型离散搜索，存在局限性。", "method": "本文提出了一种统一的可微分短时傅里叶变换（STFT）公式，该公式允许通过梯度优化来调整STFT参数。这种方法可以根据任意所需标准对时频表示（TFR）进行微调，并且能够与神经网络无缝集成，实现STFT参数和网络权重的联合优化。", "result": "所提出的可微分STFT在增强时频表示（TFRs）和改进下游任务性能方面表现出有效性，并通过在模拟和真实世界数据上的实验得到了证明。", "conclusion": "通过引入可微分STFT，可以有效地解决传统STFT参数调优的局限性，实现参数的梯度优化和与神经网络的联合优化，从而显著提升时频表示的质量和下游任务的性能。", "translation": "短时傅里叶变换（STFT）被广泛用于分析非平稳信号。然而，其性能对其参数高度敏感，并且手动或启发式调优常常导致次优结果。为了克服这一限制，我们提出了一种统一的可微分STFT公式，该公式能够实现其参数的基于梯度的优化。这种方法解决了传统STFT参数调优方法的局限性，这些方法通常依赖于计算密集型的离散搜索。它使得能够根据任何所需标准对时频表示（TFR）进行微调。此外，我们的方法与神经网络无缝集成，允许STFT参数和网络权重的联合优化。通过在模拟和真实世界数据上的实验，证明了所提出的可微分STFT在增强TFRs和改进下游任务性能方面的有效性。", "summary": "本文提出了一种统一的可微分短时傅里叶变换（STFT）公式，旨在解决传统STFT参数敏感且调优困难的问题。该方法允许通过梯度优化来调整STFT参数，从而避免了计算密集型的离散搜索，并能根据特定标准微调时频表示（TFR）。此外，它能与神经网络无缝集成，实现参数和网络权重的联合优化。实验证明，该方法能有效提升TFR质量和下游任务性能。", "keywords": "短时傅里叶变换, 可微分, 时频表示, 梯度优化, 神经网络", "comments": "这篇论文的创新点在于提出了STFT的可微分形式，从而将传统上需要手动或启发式调优的参数纳入到梯度优化框架中。这一方法不仅解决了STFT参数敏感性的问题，还使其能够与深度学习模型进行端到端联合优化，极大地扩展了STFT在各种信号处理和机器学习任务中的应用潜力。其重要性在于为信号的时频分析提供了一个更灵活、更优化的工具，特别是对于非平稳信号的处理。"}}
{"id": "2506.21106", "title": "PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction", "authors": ["Felipe Castaño", "Eduardo Fidalgo", "Enrique Alegre", "Rocio Alaiz-Rodríguez", "Raul Orduna", "Francesco Zola"], "summary": "Phishing attacks pose a significant cybersecurity threat, evolving rapidly to\nbypass detection mechanisms and exploit human vulnerabilities. This paper\nintroduces PhishKey to address the challenges of adaptability, robustness, and\nefficiency. PhishKey is a novel phishing detection method using automatic\nfeature extraction from hybrid sources. PhishKey combines character-level\nprocessing with Convolutional Neural Networks (CNN) for URL classification, and\na Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at\nthe word level. CAPE reduces noise and ensures complete sample processing\navoiding crop operations on the input data. The predictions from both modules\nare integrated using a soft-voting ensemble to achieve more accurate and\nreliable classifications. Experimental evaluations on four state-of-the-art\ndatasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1\nScore and shows strong resistance to adversarial manipulations such as\ninjection attacks with minimal performance degradation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21106v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21106v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PhishKey：一种使用自适应HTML组件提取增强网络钓鱼检测的新型基于质心的方法", "tldr": "PhishKey是一种结合URL CNN分类和HTML质心提取的新型网络钓鱼检测方法，通过软投票集成提高准确性和鲁棒性，在多个数据集上表现出色。", "motivation": "网络钓鱼攻击构成重大网络安全威胁，且快速演变以规避检测机制并利用人类弱点。现有方法面临适应性、鲁棒性和效率方面的挑战。", "method": "本文引入PhishKey，这是一种新型网络钓鱼检测方法，通过从混合源进行自动特征提取。PhishKey结合了字符级处理与卷积神经网络 (CNN) 用于URL分类，以及一个基于质心的关键组件网络钓鱼提取器 (CAPE) 用于词级别的HTML内容处理。CAPE旨在减少噪声并确保完整样本处理，避免对输入数据进行裁剪操作。来自两个模块的预测通过软投票集成进行整合，以实现更准确和可靠的分类。", "result": "在四个先进数据集上进行的实验评估证明了PhishKey的有效性。它实现了高达98.70%的F1分数，并显示出对注入攻击等对抗性操纵的强大抵抗力，性能下降最小。", "conclusion": "PhishKey通过结合URL和HTML内容分析，并采用集成学习，能够有效且鲁棒地检测网络钓鱼攻击，即使面对对抗性操纵也能保持高性能。", "translation": "网络钓鱼攻击构成重大的网络安全威胁，它们快速演变以规避检测机制并利用人类的脆弱性。本文引入PhishKey以解决适应性、鲁棒性和效率方面的挑战。PhishKey是一种新颖的网络钓鱼检测方法，它利用从混合源自动提取特征。PhishKey结合了字符级处理与卷积神经网络（CNN）用于URL分类，以及一个基于质心的关键组件网络钓鱼提取器（CAPE）用于词级别的HTML内容。CAPE减少了噪声并确保了完整的样本处理，避免了对输入数据进行裁剪操作。来自两个模块的预测通过软投票集成进行整合，以实现更准确和可靠的分类。在四个最先进数据集上的实验评估证明了PhishKey的有效性。它实现了高达98.70%的F1分数，并显示出对注入攻击等对抗性操纵的强大抵抗力，性能下降最小。", "summary": "PhishKey是一种新颖的网络钓鱼检测系统，旨在解决现有方法的适应性、鲁棒性和效率挑战。它通过结合基于CNN的URL字符级分类和基于质心的HTML内容词级提取（CAPE）来自动提取混合源特征。PhishKey利用软投票集成融合两个模块的预测结果，以提高检测的准确性和可靠性。实验结果表明，PhishKey在多个数据集上表现出色，F1分数高达98.70%，并对对抗性攻击具有强大的抵抗力。", "keywords": "网络钓鱼检测, 质心方法, HTML组件提取, 卷积神经网络, 软投票集成", "comments": "PhishKey的创新之处在于其混合方法，结合了URL的字符级CNN处理和HTML内容的词级质心提取，并通过软投票集成增强了性能。其对对抗性攻击的鲁棒性是其重要优势，这在当前复杂的网络安全环境中至关重要。"}}
{"id": "2506.21138", "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE", "authors": ["Abdelkarim El-Hajjami", "Camille Salinesi"], "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21138v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21138v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "合成需求有多好？评估LLM为AI4RE生成的数据集", "tldr": "本文介绍了Synthline v1，一种增强的产品线方法，用于生成合成需求数据，旨在解决AI4RE领域公共标记数据集的短缺问题。研究评估了提示策略、自动提示优化和后生成策展对数据质量的影响，并表明合成需求在特定任务上可以与人工编写的需求相媲美甚至超越。", "motivation": "人工智能在需求工程（AI4RE）领域的发展面临公共可用、带标签的需求数据集严重短缺的障碍。大型语言模型（LLMs）提供了生成合成数据的潜力，但如何系统地控制和优化生成需求的质量仍未得到充分探索。", "method": "本文提出了Synthline v1，一种增强的产品线方法，用于生成合成需求数据，该方法在早期v0版本的基础上增加了高级生成策略和策展技术。研究通过四个研究问题，评估了提示策略、自动化提示优化（使用PACE）和后生成策展如何影响四种分类任务（缺陷检测、功能性与非功能性、质量与非质量、安全性与非安全性）的数据质量。", "result": "多样本提示显著提高了效用和多样性，F1分数提高了6到44个点。使用PACE进行自动提示优化产生了任务依赖的结果，在功能分类上大幅提高（+32.5点），但在其他任务上降低了性能。基于相似性的策展提高了多样性，但通常损害了分类性能。最重要的是，合成需求在特定任务上可以与人工编写的需求相媲美或超越，在安全性（+7.8点）和缺陷分类（+15.4点）方面，合成数据甚至优于人工数据。", "conclusion": "合成需求能够匹配甚至在某些特定任务上超越人工编写的需求，为通过系统化的合成生成来缓解数据集稀缺性提供了一条可行的途径。这些发现为AI4RE提供了实用的见解。", "translation": "公共可用、带标签的需求数据集的短缺仍然是推进需求工程人工智能（AI4RE）的主要障碍。虽然大型语言模型为合成数据生成提供了有前景的能力，但控制和优化生成需求质量的系统方法仍未得到充分探索。本文提出了Synthline v1，一种增强的产品线方法，用于生成合成需求数据，该方法通过高级生成策略和策展技术扩展了我们早期的v0版本。我们调查了四个研究问题，评估了提示策略、自动化提示优化和后生成策展如何影响四种分类任务（缺陷检测、功能性与非功能性、质量与非质量、安全性与非安全性）的数据质量。我们的评估表明，多样本提示显著提高了单样本生成的效用和多样性，F1分数提高了6到44个点。使用PACE（提示行动者-评论家编辑）进行自动化提示优化产生了任务依赖的结果，大大改善了功能分类（+32.5点），但降低了其他任务的性能。有趣的是，基于相似性的策展提高了多样性，但通常损害了分类性能，这表明一些冗余可能有助于机器学习模型。最重要的是，我们的结果表明，合成需求在特定任务上可以与人工编写的需求相媲美或超越，其中合成数据在安全性（+7.8点）和缺陷分类（+15.4点）方面超越了人工数据。这些发现为AI4RE提供了实用的见解，并为通过系统化的合成生成来缓解数据集稀缺性描绘了一条可行的路径。", "summary": "本文介绍了Synthline v1，一个用于生成合成需求数据的增强型产品线方法，旨在解决AI4RE领域中标记数据集的稀缺问题。研究通过四个分类任务评估了不同的LLM提示策略、自动化提示优化和后生成策展对数据质量的影响。结果显示，多样本提示显著提升数据质量和多样性，且合成数据在特定任务（如安全和缺陷分类）上表现优于人工数据。这为通过系统生成合成数据以缓解数据集不足提供了可行方案。", "keywords": "合成需求, LLM, AI4RE, 数据集生成, 提示工程", "comments": "本文创新性地提出了Synthline v1框架，结合了先进的生成策略和策展技术来优化LLM生成的合成需求数据质量。其重要性在于为解决AI4RE领域长期存在的数据集稀缺问题提供了一条有前景的路径。研究不仅验证了合成数据在某些任务上的有效性，甚至超越了人工数据，还深入探讨了不同生成和优化策略对数据质量的影响，为未来LLM在需求工程中的应用提供了宝贵的实践指导。"}}
{"id": "2506.21016", "title": "Fault-Tolerant Spacecraft Attitude Determination using State Estimation Techniques", "authors": ["B. Chidambaram", "A. Hilbert", "M. Silva"], "summary": "The extended and unscented Kalman filter, and the particle filter provide a\nrobust framework for fault-tolerant attitude estimation on spacecraft. This\npaper explores how each filter performs for a large satellite in a low earth\norbit. Additionally, various techniques, built on these filters, for fault\ndetection, isolation and recovery from erroneous sensor measurements, are\nanalyzed. Key results from this analysis include filter performance for various\nfault modes.", "comment": "8 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.21016v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21016v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "容错航天器姿态确定中的状态估计算法", "tldr": "本文探讨了扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波在航天器容错姿态估计中的性能，并分析了基于这些滤波器的故障检测、隔离和恢复技术。", "motivation": "为航天器提供一个鲁棒的容错姿态估计框架。", "method": "采用扩展卡尔曼滤波 (EKF)、无迹卡尔曼滤波 (UKF) 和粒子滤波 (PF) 进行姿态估计，并在此基础上分析了用于故障检测、隔离和恢复的各种技术。", "result": "分析的关键结果包括各种故障模式下滤波器的性能。", "conclusion": "Not mentioned in abstract", "translation": "扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波为航天器容错姿态估计提供了一个鲁棒的框架。本文探讨了每种滤波器在低地球轨道大型卫星上的性能。此外，还分析了基于这些滤波器构建的各种用于从错误传感器测量中进行故障检测、隔离和恢复的技术。此分析的关键结果包括各种故障模式下滤波器的性能。", "summary": "本文研究了扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波在大型低轨卫星容错姿态估计中的应用。研究分析了这些滤波器在不同故障模式下的性能，并探讨了基于这些滤波器实现的故障检测、隔离和恢复技术。", "keywords": "姿态确定, 容错, 卡尔曼滤波, 粒子滤波, 故障检测", "comments": "该研究通过评估多种状态估计算法在容错航天器姿态确定中的表现，并结合故障检测、隔离和恢复技术，为航天器在复杂故障环境下的可靠运行提供了重要的技术分析。其创新点在于将成熟的滤波技术应用于航天器故障容错场景，并详细分析了其在不同故障模式下的性能。"}}
{"id": "2506.21322", "title": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory", "authors": ["Hong Wang", "Natalia Calvo-Barajas", "Katie Winkle", "Ginevra Castellano"], "summary": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2506.21322v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21322v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "“我该相信谁？”：家庭医疗机器人与人类记忆矛盾时的用户解读与决策", "tldr": "本研究探讨了当家庭医疗机器人提供的信息与用户记忆矛盾时，用户如何解读信息并做出决策，以及机器人透明度和社交性对其信任的影响。结果显示，透明度影响用户对信息差异的解释，且用户倾向于过度信任机器人。", "motivation": "随着智能医疗机器人在家庭环境中部署的可能性日益增加，当机器人提供的信息与用户记忆矛盾时，如何处理用户信任和决策成为一个挑战。", "method": "本研究采用2x2被试间在线研究设计，共有176名参与者。参与者观看一个Furhat机器人作为家庭医疗助手的视频，该机器人建议虚构用户在与用户记忆不同的时间服药。研究考察了机器人透明度和社交性对用户解读、决策和感知信任的影响。", "result": "机器人透明度影响了用户对信息差异的解释：低透明度机器人下，用户最常假设是自己记错了时间；高透明度机器人下，参与者更倾向于将差异归因于外部因素（如伴侣修改了机器人信息）。此外，参与者表现出过度信任倾向，即使怀疑系统故障或第三方干扰，也常优先采纳机器人建议而非用户记忆。", "conclusion": "研究结果强调了机器人系统中透明度机制的影响，家庭环境中多用户机器人系统访问控制的复杂性和重要性，以及用户在医疗等敏感领域过度依赖机器人的潜在风险。", "translation": "“我该相信谁？”：家庭医疗机器人与人类记忆矛盾时的用户解读与决策\n\n摘要：\n机器人提供物理辅助、心理支持和日常健康管理的能力的进步，使得在不久的将来在家庭环境中部署智能医疗机器人变得越来越可行。然而，当这些机器人提供的信息与用户的记忆相矛盾时，就会出现挑战，引发用户信任和决策方面的担忧。本文提出了一项研究，探讨了当机器人提供冲突信息时，改变机器人的透明度和社交性水平如何影响用户的解读、决策和感知信任。在一项2x2被试间在线研究中，176名参与者观看了Furhat机器人作为家庭医疗助手的视频，该机器人建议一个虚构用户在与用户记忆不同的时间服药。结果表明，机器人透明度影响了用户对信息差异的解读：对于低透明度机器人，最常见的假设是用户没有正确记住时间，而对于高透明度机器人，参与者更有可能将差异归因于外部因素，例如伴侣或另一个家庭成员修改了机器人的信息。此外，参与者表现出过度信任的倾向，即使怀疑系统故障或第三方干扰，也常常优先采纳机器人的建议而非用户的记忆。这些发现强调了透明度机制在机器人系统中的影响，与家庭环境中部署的多用户机器人系统访问控制相关的复杂性和重要性，以及用户在医疗等敏感领域过度依赖机器人的潜在风险。", "summary": "本研究探讨了当家庭医疗机器人提供的信息与用户记忆冲突时，机器人透明度和社交性对用户解读、决策和信任的影响。通过一项有176名参与者参与的在线实验，结果显示机器人透明度会影响用户对信息差异的归因，即低透明度导致用户怀疑自身记忆，高透明度则归因于外部因素。研究还发现，用户普遍存在过度信任机器人的倾向，即使面对潜在故障或第三方干扰，仍优先采纳机器人建议。这凸显了透明度机制、多用户系统访问控制的重要性，并警示了用户在医疗领域过度依赖机器人的风险。", "keywords": "家庭医疗机器人, 用户信任, 透明度, 决策, 过度信任", "comments": "该研究揭示了在人机交互，特别是医疗健康领域中，机器人透明度对用户信任和决策的关键影响。其创新之处在于通过实验量化了透明度对信息差异解释的影响，并揭示了用户对机器人的过度信任倾向。这项工作对于未来家庭医疗机器人的设计，尤其是系统访问控制和避免用户过度依赖方面，具有重要的指导意义。"}}
{"id": "2506.21308", "title": "Balancing Privacy and Utility in Correlated Data: A Study of Bayesian Differential Privacy", "authors": ["Martin Lange", "Patricia Guerra-Balboa", "Javier Parra-Arnau", "Thorsten Strufe"], "summary": "Privacy risks in differentially private (DP) systems increase significantly\nwhen data is correlated, as standard DP metrics often underestimate the\nresulting privacy leakage, leaving sensitive information vulnerable. Given the\nubiquity of dependencies in real-world databases, this oversight poses a\ncritical challenge for privacy protections. Bayesian differential privacy (BDP)\nextends DP to account for these correlations, yet current BDP mechanisms\nindicate notable utility loss, limiting its adoption.\n  In this work, we address whether BDP can be realistically implemented in\ncommon data structures without sacrificing utility -- a key factor for its\napplicability. By analyzing arbitrary and structured correlation models,\nincluding Gaussian multivariate distributions and Markov chains, we derive\npractical utility guarantees for BDP. Our contributions include theoretical\nlinks between DP and BDP and a novel methodology for adapting DP mechanisms to\nmeet the BDP requirements. Through evaluations on real-world databases, we\ndemonstrate that our novel theorems enable the design of BDP mechanisms that\nmaintain competitive utility, paving the way for practical privacy-preserving\ndata practices in correlated settings.", "comment": "This is the extended version of the paper accepted in the Proceedings\n  of the VLDB Endowment (PVLDB), 2025. The code used for our experiments is\n  accessible in https://github.com/lange-martin/privacy-utility-bdp", "pdf_url": "http://arxiv.org/pdf/2506.21308v1", "categories": ["cs.CR", "cs.IT", "math.IT", "68P27"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21308v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关联数据中隐私与效用的平衡：一项关于贝叶斯差分隐私的研究", "tldr": "解决了贝叶斯差分隐私在关联数据中应用时效用损失大的问题，通过新方法实现了在保持效用下对关联数据的隐私保护。", "motivation": "标准差分隐私在关联数据中隐私风险高，而贝叶斯差分隐私（BDP）虽能考虑关联性，但现有机制导致显著的效用损失，限制了其应用。本研究旨在探究BDP如何在不牺牲效用的情况下应用于常见数据结构。", "method": "通过分析任意和结构化的关联模型（包括高斯多元分布和马尔可夫链），推导了BDP的实用效用保证。贡献包括建立了DP和BDP之间的理论联系，并提出了一种新颖的方法来调整DP机制以满足BDP要求。", "result": "在真实世界数据库上的评估表明，他们的新定理能够设计出保持竞争性效用的BDP机制。", "conclusion": "这项工作为在关联数据环境中实现实用的隐私保护数据实践铺平了道路。", "translation": "差分隐私（DP）系统中，当数据存在关联时，隐私风险显著增加，因为标准的DP度量通常低估了由此产生的隐私泄露，使敏感信息容易受到攻击。鉴于现实世界数据库中依赖关系的普遍性，这种疏忽对隐私保护构成了严峻挑战。贝叶斯差分隐私（BDP）扩展了DP以考虑这些关联，但当前的BDP机制表现出显著的效用损失，限制了其采用。\n在这项工作中，我们探讨了BDP是否可以在不牺牲效用的情况下在常见数据结构中实际实现——这是其适用性的一个关键因素。通过分析任意和结构化的关联模型，包括高斯多元分布和马尔可夫链，我们推导了BDP的实用效用保证。我们的贡献包括DP和BDP之间的理论联系，以及一种调整DP机制以满足BDP要求的新颖方法。通过对真实世界数据库的评估，我们证明了我们的新定理能够设计出保持竞争性效用的BDP机制，为在关联设置中实现实用的隐私保护数据实践铺平了道路。", "summary": "本文研究了在关联数据中应用贝叶斯差分隐私（BDP）时效用损失的问题。针对标准差分隐私在关联数据中隐私泄露评估不足的挑战，作者通过分析多种关联模型，推导了BDP的实用效用保证。研究提出了一种新方法，将标准差分隐私机制调整以满足BDP要求，并在真实世界数据上验证了其设计的BDP机制能够在保持竞争性效用的同时提供隐私保护，从而推动了关联数据中隐私保护实践的落地。", "keywords": "贝叶斯差分隐私, 关联数据, 隐私保护, 效用", "comments": "本文解决了贝叶斯差分隐私在实际应用中面临的效用损失这一关键挑战，其创新点在于提出了新颖的理论和方法，使得BDP能够在关联数据中实现高效用隐私保护。这对于提升差分隐私在复杂现实世界数据中的实用性具有重要意义。"}}
{"id": "2506.21072", "title": "Bridding OT and PaaS in Edge-to-Cloud Continuum", "authors": ["Carlos J Barrios", "Yves Denneulin"], "summary": "The Operational Technology Platform as a Service (OTPaaS) initiative provides\na structured framework for the efficient management and storage of data. It\nensures excellent response times while improving security, reliability, data\nand technology sovereignty, robustness, and energy efficiency, which are\ncrucial for industrial transformation and data sovereignty. This paper\nillustrates successful deployment, adaptable application management, and\nvarious integration components catering to Edge and Cloud environments. It\nleverages the advantages of the Platform as a Service model and highlights key\nchallenges that have been addressed for specific use cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21072v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21072v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "将OT和PaaS桥接到边缘到云的连续体中", "tldr": "论文介绍了运营技术即服务平台（OTPaaS），这是一个为工业转型提供高效数据管理、存储、安全、可靠性、数据主权和能效的框架，并展示了其在边缘和云环境中的成功部署和应用管理。", "motivation": "工业转型和数据主权对数据管理、存储、安全性、可靠性、数据和技术主权、鲁棒性及能源效率有高要求，OTPaaS旨在提供一个结构化框架来满足这些关键需求。", "method": "本文提出了运营技术即服务平台（OTPaaS）倡议，该倡议提供了一个结构化的数据高效管理和存储框架。它利用了平台即服务（PaaS）模型的优势，并阐述了成功的部署、可适应的应用程序管理以及针对边缘和云环境的各种集成组件，同时解决了特定用例的关键挑战。", "result": "OTPaaS实现了成功的部署、可适应的应用管理和针对边缘及云环境的多种集成组件。它显著提升了数据管理和存储的效率，并改善了响应时间、安全性、可靠性、数据和技术主权、鲁棒性以及能源效率。", "conclusion": "运营技术即服务平台（OTPaaS）通过提供一个结构化框架并解决关键挑战，成功地将运营技术（OT）与平台即服务（PaaS）模型结合，为边缘到云的连续体中的数据高效管理和存储提供了解决方案，并提升了多项关键性能指标，对工业转型和数据主权至关重要。", "translation": "运营技术即服务平台 (OTPaaS) 倡议提供了一个结构化的框架，用于高效的数据管理和存储。它确保了出色的响应时间，同时提高了安全性、可靠性、数据和技术主权、鲁棒性以及能源效率，这些对于工业转型和数据主权至关重要。本文阐述了成功的部署、可适应的应用程序管理以及满足边缘和云环境的各种集成组件。它利用了平台即服务模型的优势，并强调了针对特定用例已解决的关键挑战。", "summary": "本文介绍了运营技术即服务平台（OTPaaS）倡议，该倡议提供了一个结构化框架，旨在高效管理和存储数据，同时提升响应时间、安全性、可靠性、数据和技术主权、鲁棒性以及能源效率。OTPaaS对于工业转型和数据主权至关重要，论文展示了其在边缘和云环境中的成功部署、灵活的应用管理和多种集成组件，并通过利用PaaS模型解决了特定用例中的关键挑战。", "keywords": "运营技术, 平台即服务, 边缘到云, 数据管理, 工业转型", "comments": "这篇论文通过提出OTPaaS，有效地将运营技术（OT）与平台即服务（PaaS）模型结合，为边缘到云的连续体中的数据管理和存储提供了一个全面的解决方案。其创新点在于强调了数据主权、能源效率等工业转型中的关键要素，并展示了实际部署和对挑战的解决，显示了其在工业物联网和云计算融合领域的潜在重要性。"}}
{"id": "2506.21311", "title": "Estimating Technical Loss without Power Flows: A Practical, Data-Driven Approach for Loss Estimation in Distribution Grids", "authors": ["Mohini Bariya", "Genevieve Flaspohler"], "summary": "Electric grids in low- and middle-income countries (LMICs) across the world\nface an acute challenge. To support global decarbonisation efforts and raise\nmillions from energy poverty, these grids must shoulder substantial load growth\nwhile integrating distributed renewable generation. However, decades of rapid\nand poorly funded infrastructure expansions have led to national grids in many\nLMICs that are strained and weak, composed of aging, faulty, and undersized\ninfrastructure. A cause and symptom of this weakness is excessive technical\nloss within the grid infrastructure during energy delivery, particularly at the\ndistribution level; network losses are regularly estimated to be well over 20\npercent, compared to a baseline of 5 percent in higher-income nations.\nAddressing technical loss through targeted interventions is essential for\nbolstering grids' physical and economic strength. Unfortunately, current\napproaches for estimating and localizing technical loss require expensive,\nextensive power flow sensing, which is essentially absent in LMIC distribution\nsystems. We present a novel approach to technical loss estimation without power\nflows, which leverages more readily available voltage magnitude measurements at\nsparse locations in the grid. This estimator puts loss estimation and\nlocalization within reach for LMIC grids globally, and provides a critical tool\nfor the effective design, implementation, and evaluation of loss-reduction\ninterventions.", "comment": "6 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21311v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21311v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "无需潮流计算的技术损耗估算：一种针对配电网损耗估算的实用数据驱动方法", "tldr": "发展中国家电网技术损耗高且缺乏测量数据，本文提出一种无需潮流计算，仅利用稀疏电压测量数据估算技术损耗的新方法，为LMIC电网的损耗估算和定位提供了关键工具。", "motivation": "全球低收入和中等收入国家（LMICs）的电网面临着巨大的技术损耗挑战，其损耗率远高于高收入国家。现有技术损耗估算方法依赖于昂贵且在LMIC电网中普遍缺乏的潮流传感数据，这阻碍了有效的损耗干预措施。因此，迫切需要一种无需潮流计算的实用损耗估算方法。", "method": "本文提出了一种新颖的技术损耗估算方法，该方法无需进行潮流计算。它利用电网中稀疏位置更容易获得的电压幅值测量数据来进行损耗估算和定位。", "result": "该估算器使全球低收入和中等收入国家电网的技术损耗估算和定位成为可能，并为损耗降低干预措施的有效设计、实施和评估提供了关键工具。", "conclusion": "该研究提供了一种无需昂贵潮流传感数据即可估算和定位配电网技术损耗的实用方法，特别适用于低收入和中等收入国家电网。这为加强这些电网的物理和经济实力，以及有效实施减损措施提供了关键支持。", "translation": "全球低收入和中等收入国家（LMICs）的电网面临着严峻的挑战。为了支持全球脱碳努力并将数百万人从能源贫困中解救出来，这些电网必须在整合分布式可再生能源的同时承担巨大的负荷增长。然而，数十年来快速且资金不足的基础设施扩张，导致许多LMICs的国家电网紧张而脆弱，由老化、故障和规模不足的基础设施组成。这种脆弱性的一个原因和表现就是能源输送过程中电网基础设施内部，尤其是在配电层面的过度技术损耗；与高收入国家5%的基线相比，这些电网的损耗通常估计远超20%。通过有针对性的干预措施解决技术损耗对于增强电网的物理和经济实力至关重要。不幸的是，目前估算和定位技术损耗的方法需要昂贵、广泛的潮流传感，而这在LMIC配电系统中基本不存在。我们提出了一种无需潮流计算的技术损耗估算新方法，该方法利用电网中稀疏位置更容易获得的电压幅值测量数据。这种估算器使全球LMIC电网的损耗估算和定位成为可能，并为损耗降低干预措施的有效设计、实施和评估提供了关键工具。", "summary": "本文关注低收入和中等收入国家（LMICs）电网面临的严重技术损耗问题，这些电网因基础设施薄弱而导致损耗远高于发达国家。针对现有损耗估算方法依赖昂贵且稀缺的潮流测量数据，该研究提出了一种创新方法。新方法无需潮流计算，而是利用电网中稀疏位置更容易获得的电压幅值测量数据来估算和定位技术损耗。该方法旨在为全球LMIC电网提供一个实用工具，以有效设计、实施和评估减损干预措施，从而增强电网的物理和经济韧性。", "keywords": "技术损耗, 配电网, 数据驱动, 电压测量, 低收入和中等收入国家", "comments": "该论文提出了一种创新且实用的方法，解决了低收入和中等收入国家（LMICs）电网在技术损耗估算方面的数据限制问题。其核心创新在于无需传统的潮流计算，转而利用更易获得的稀疏电压幅值测量数据。这对于基础设施薄弱、缺乏先进传感设备的LMIC电网具有重要意义，能够有效支持其减损干预措施的设计与实施，从而提升电网效率和韧性。然而，抽象中未详细说明所采用的具体数据驱动模型或其对测量数据稀疏性的鲁棒性。"}}
{"id": "2506.20841", "title": "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization", "authors": ["Ha Min Son", "Shahbaz Rezaei", "Xin Liu"], "summary": "Semi-supervised domain generalization (SSDG) aims to solve the problem of\ngeneralizing to out-of-distribution data when only a few labels are available.\nDue to label scarcity, applying domain generalization methods often\nunderperform. Consequently, existing SSDG methods combine semi-supervised\nlearning methods with various regularization terms. However, these methods do\nnot explicitly regularize to learn domains invariant representations across all\ndomains, which is a key goal for domain generalization. To address this, we\nintroduce FixCLR. Inspired by success in self-supervised learning, we change\ntwo crucial components to adapt contrastive learning for explicit domain\ninvariance regularization: utilization of class information from pseudo-labels\nand using only a repelling term. FixCLR can also be added on top of most\nexisting SSDG and semi-supervised methods for complementary performance\nimprovements. Our research includes extensive experiments that have not been\npreviously explored in SSDG studies. These experiments include benchmarking\ndifferent improvements to semi-supervised methods, evaluating the performance\nof pretrained versus non-pretrained models, and testing on datasets with many\ndomains. Overall, FixCLR proves to be an effective SSDG method, especially when\ncombined with other semi-supervised methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20841v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20841v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "FixCLR：半监督域泛化的负类对比学习", "tldr": "FixCLR引入负类对比学习，通过显式域不变性正则化来改进半监督域泛化（SSDG），尤其是在标签稀缺的情况下。", "motivation": "半监督域泛化（SSDG）在数据分布外泛化时，由于标签稀缺，现有方法往往表现不佳。这些方法未能明确地正则化以学习跨所有域的域不变表示，而这正是域泛化的关键目标。", "method": "本文引入了FixCLR。受自监督学习的启发，FixCLR修改了对比学习的两个关键组件，以实现显式的域不变性正则化：利用伪标签的类别信息和仅使用排斥项。FixCLR还可以与大多数现有的SSDG和半监督方法结合使用，以获得互补的性能改进。", "result": "研究包括了SSDG研究中以前未曾探索过的广泛实验，包括对半监督方法不同改进的基准测试、预训练模型与非预训练模型性能的评估，以及在多域数据集上的测试。FixCLR被证明是一种有效的SSDG方法，特别是与其他半监督方法结合使用时。", "conclusion": "FixCLR被证明是一种有效的半监督域泛化（SSDG）方法，尤其是在与其他半监督方法结合使用时，它通过显式的域不变性正则化解决了现有方法的不足。", "translation": "半监督域泛化（SSDG）旨在解决当只有少量标签可用时，泛化到分布外数据的问题。由于标签稀缺，应用域泛化方法通常表现不佳。因此，现有的SSDG方法将半监督学习方法与各种正则化项结合起来。然而，这些方法没有明确地正则化以学习跨所有域的域不变表示，而这正是域泛化的一个关键目标。为了解决这个问题，我们引入了FixCLR。受自监督学习成功的启发，我们改变了两个关键组件，以使对比学习适应显式的域不变性正则化：利用伪标签的类别信息和仅使用排斥项。FixCLR还可以添加到大多数现有的SSDG和半监督方法之上，以实现互补的性能改进。我们的研究包括了SSDG研究中以前未曾探索过的广泛实验。这些实验包括对半监督方法不同改进的基准测试、评估预训练模型与非预训练模型的性能，以及在多域数据集上进行多域数据集的测试。总的来说，FixCLR被证明是一种有效的SSDG方法，特别是当与其它半监督方法结合使用时。", "summary": "本文提出FixCLR，一种针对半监督域泛化（SSDG）的新方法，旨在解决现有方法在标签稀缺下无法有效学习域不变表示的问题。FixCLR通过修改对比学习的组件，即利用伪标签的类别信息和仅使用排斥项，实现显式的域不变性正则化。实验证明，FixCLR是一种有效的SSDG方法，并且可以与现有半监督方法结合使用以进一步提升性能，尤其适用于多域数据集和评估预训练模型。", "keywords": "半监督域泛化, 对比学习, 域不变性, 伪标签, FixCLR", "comments": "FixCLR的创新之处在于将负类对比学习引入半监督域泛化，并通过显式正则化来学习域不变表示，解决了传统SSDG方法在标签稀缺下的性能瓶颈。其模块化设计允许与现有方法结合，增强了其实用性和影响力。该研究还通过广泛且新颖的实验设置，对SSDG领域进行了深入探索。"}}
{"id": "2506.21310", "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems", "authors": ["Pauline Speckmann", "Mario Nadj", "Christian Janiesch"], "summary": "Although several post-hoc methods for explainable AI have been developed,\nmost are static and neglect the user perspective, limiting their effectiveness\nfor the target audience. In response, we developed the interactive explainable\nintelligent system called IXAII that offers explanations from four explainable\nAI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored\nviews for five user groups and gives users agency over the explanations'\ncontent and their format. We evaluated IXAII through interviews with experts\nand lay users. Our results indicate that IXAII, which provides different\nexplanations with multiple visualization options, is perceived as helpful to\nincrease transparency. By bridging the gaps between explainable AI methods,\ninteractivity, and practical implementation, we provide a novel perspective on\nAI explanation practices and human-AI interaction.", "comment": "9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track", "pdf_url": "http://arxiv.org/pdf/2506.21310v1", "categories": ["cs.AI", "cs.SE", "K.6.3 Software Management"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21310v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "IXAII：一个用于决策支持系统的交互式可解释人工智能界面", "tldr": "IXAII是一个交互式可解释AI系统，它整合了多种解释方法，提供定制化视图和用户控制，旨在提高透明度并促进人机交互。", "motivation": "现有的事后可解释AI方法大多是静态的，并且忽视了用户视角，这限制了它们对目标受众的有效性。", "method": "我们开发了一个名为IXAII的交互式可解释智能系统，它提供了来自LIME、SHAP、Anchors和DiCE四种可解释AI方法的解释。该原型为五种用户组提供了定制化视图，并赋予用户对解释内容和格式的控制权。我们通过专家和普通用户的访谈评估了IXAII。", "result": "我们的结果表明，IXAII通过提供具有多种可视化选项的不同解释，被认为有助于提高透明度。", "conclusion": "通过弥合可解释AI方法、交互性和实际实现之间的差距，我们为AI解释实践和人机交互提供了一个新颖的视角。", "translation": "尽管已经开发了几种事后可解释AI方法，但大多数都是静态的，并且忽视了用户视角，这限制了它们对目标受众的有效性。为此，我们开发了一个名为IXAII的交互式可解释智能系统，它提供了来自LIME、SHAP、Anchors和DiCE四种可解释AI方法的解释。我们的原型为五种用户组提供了定制化视图，并赋予用户对解释内容和格式的控制权。我们通过专家和普通用户的访谈评估了IXAII。我们的结果表明，IXAII通过提供具有多种可视化选项的不同解释，被认为有助于提高透明度。通过弥合可解释AI方法、交互性和实际实现之间的差距，我们为AI解释实践和人机交互提供了一个新颖的视角。", "summary": "本文介绍了一个名为IXAII的交互式可解释人工智能界面，旨在解决现有可解释AI方法静态且忽视用户视角的问题。IXAII整合了LIME、SHAP、Anchors和DiCE等四种解释方法，为不同用户群体提供定制化的视图，并允许用户控制解释的内容和格式。通过专家和普通用户访谈评估，结果显示IXAII有助于提高AI系统的透明度。该研究为AI解释实践和人机交互提供了新颖的视角，弥合了可解释AI方法、交互性与实际应用之间的鸿沟。", "keywords": "可解释AI, 交互式界面, 决策支持系统, LIME, SHAP", "comments": "该论文的创新点在于其交互性设计和用户中心视角，这在当前静态可解释AI方法为主流的背景下显得尤为重要。通过整合多种解释方法并提供定制化视图，IXAII显著提升了可解释AI的实用性和用户体验，对于促进AI在决策支持系统中的透明度和信任度具有重要意义。"}}
{"id": "2506.21245", "title": "GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models", "authors": ["Qifei Cui", "Xinyu Lu"], "summary": "This work introduces a novel framework for brain tumor segmentation\nleveraging pre-trained GANs and Unet architectures. By combining a global\nanomaly detection module with a refined mask generation network, the proposed\nmodel accurately identifies tumor-sensitive regions and iteratively enhances\nsegmentation precision using adversarial loss constraints. Multi-modal MRI data\nand synthetic image augmentation are employed to improve robustness and address\nthe challenge of limited annotated datasets. Experimental results on the BraTS\ndataset demonstrate the effectiveness of the approach, achieving high\nsensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the\nbaseline. This scalable method minimizes the dependency on fully annotated\ndata, paving the way for practical real-world applications in clinical\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21245v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21245v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GANet-Seg：基于混合生成模型的脑肿瘤对抗学习分割", "tldr": "GANet-Seg是一个利用预训练GAN和Unet的脑肿瘤分割新框架，通过对抗学习和合成数据增强，减少对完全标注数据的依赖，并在BraTS数据集上表现出色。", "motivation": "为了开发一种准确、鲁棒的脑肿瘤分割方法，并解决标注数据集有限的挑战。", "method": "提出GANet-Seg框架，结合预训练GAN和Unet架构。该模型通过全局异常检测模块和精细掩膜生成网络，利用对抗性损失约束迭代提高分割精度。同时采用多模态MRI数据和合成图像增强来提高鲁棒性并解决标注数据不足的问题。", "result": "在BraTS数据集上的实验结果表明，该方法有效，在病灶级Dice和HD95指标上均达到比基线更高的灵敏度和准确性。", "conclusion": "GANet-Seg是一种可扩展的方法，最大限度地减少了对完全标注数据的依赖，为临床环境中的实际应用铺平了道路。", "translation": "这项工作引入了一种利用预训练GAN和Unet架构进行脑肿瘤分割的新颖框架。通过将全局异常检测模块与精细掩膜生成网络相结合，所提出的模型能够准确识别肿瘤敏感区域，并利用对抗性损失约束迭代增强分割精度。研究中采用了多模态MRI数据和合成图像增强，以提高鲁棒性并解决带注释数据集有限的挑战。在BraTS数据集上的实验结果表明，该方法有效，在病灶级Dice和HD95指标上均达到比基线更高的灵敏度和准确性。这种可扩展的方法最大限度地减少了对完全标注数据的依赖，为临床环境中的实际实际应用铺平了道路。", "summary": "GANet-Seg是一个新颖的脑肿瘤分割框架，它结合了预训练的GAN和Unet架构，并集成了一个全局异常检测模块和精细掩膜生成网络。该模型利用对抗性损失约束来提高分割精度，并通过多模态MRI数据和合成图像增强来解决有限标注数据集的问题。在BraTS数据集上的实验证明，该方法在灵敏度和准确性方面优于基线，为临床应用提供了一种可扩展且对标注数据依赖性较低的解决方案。", "keywords": "脑肿瘤分割, 对抗学习, GAN, Unet, 数据增强", "comments": "该论文的创新之处在于将GAN和Unet与异常检测及合成数据增强相结合，有效解决了医学图像领域标注数据稀缺的关键问题，使其在临床应用中具有高度实用性。"}}
{"id": "2506.21043", "title": "Analysis of Null Related Beampattern Measures and Signal Quantization Effects for Linear Differential Microphone Arrays", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "summary": "A differential microphone array (DMA) offers enhanced capabilities to obtain\nsharp nulls at the cost of relatively broad peaks in the beam power pattern.\nThis can be used for applications that require nullification or attenuation of\ninterfering sources. To the best of our knowledge, the existing literature\nlacks measures that directly assess the efficacy of nulls, and null-related\nmeasures have not been investigated in the context of differential microphone\narrays (DMAs). This paper offers new insights about the utility of DMAs by\nproposing measures that characterize the nulls in their beam power patterns. We\ninvestigate the performance of differential beamformers by presenting and\nevaluating null-related measures namely null depth (ND) and Null Width (NW) as\na function of depth level relative to the beam power pattern maxima. A study of\nsignal quantization effects due to data acquisition for 1st, 2nd and 3rd order\nlinear DMAs and for different beampatterns i.e. dipole, cardioid, hypercardioid\nand supercardioid is presented. An analytical expression for the quantized\nbeamformed output for any general $ N^{th} $ order DMA is formulated.\nSimulation results of the variation of ND with number of quantization bits and\nthe variation of NW as a function of depth are also presented and inferences\nare drawn. Lab experiments are conducted in a fully anechoic room to support\nthe simulation results. The measured beampattern exhibits a pronounced null\ndepth, confirming the effectiveness of the experimental setup.", "comment": "10 pages, 15 Figures, 3 Tables", "pdf_url": "http://arxiv.org/pdf/2506.21043v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21043v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "线性差分麦克风阵列的零点相关波束图测量和信号量化效应分析", "tldr": "本文提出了表征差分麦克风阵列（DMA）零点的新测量方法（零点深度和零点宽度），研究了信号量化对DMA的影响，并通过仿真和实验室实验验证了结果。", "motivation": "现有文献缺乏直接评估差分麦克风阵列（DMA）零点效率的测量方法，并且尚未在DMA的背景下研究零点相关测量。本文旨在通过解决这一空白，提供关于DMA效用的新见解。", "method": "提出了表征DMA波束功率图中零点的新测量方法：零点深度（ND）和零点宽度（NW），并研究了它们作为深度水平函数的性能。研究了由于数据采集对一阶、二阶和三阶线性DMA以及不同波束图（偶极、心形、超心形和超指向性）的信号量化效应。推导了任意通用N阶DMA的量化波束形成输出的解析表达式。通过仿真和在全消声室中的实验室实验验证了研究结果。", "result": "仿真结果展示了零点深度随量化位数的变化以及零点宽度随深度的变化。实验室实验结果支持了仿真结果，测得的波束图显示出明显的零点深度，证实了实验设置的有效性。", "conclusion": "本文成功提出并评估了差分麦克风阵列的零点相关测量方法，研究了信号量化效应，并验证了研究结果，展示了DMA在零点消除应用中的实用性。", "translation": "差分麦克风阵列（DMA）能够以相对较宽的波束功率图峰值为代价，获得更尖锐的零点，从而增强能力。这可用于需要消除或衰减干扰源的应用。据我们所知，现有文献缺乏直接评估零点效率的测量方法，并且尚未在差分麦克风阵列（DMA）的背景下研究零点相关测量。本文通过提出表征其波束功率图中零点的测量方法，提供了关于DMA效用的新见解。我们通过展示和评估零点相关测量，即零点深度（ND）和零点宽度（NW），作为相对于波束功率图最大值的深度水平函数，来研究差分波束形成器的性能。本文还介绍了由于数据采集对一阶、二阶和三阶线性DMA以及不同波束图（即偶极、心形、超心形和超指向性）的信号量化效应的研究。本文推导了任意通用N阶DMA的量化波束形成输出的解析表达式。还介绍了零点深度随量化位数变化的仿真结果和零点宽度随深度变化的仿真结果，并得出了推论。在全消声室中进行了实验室实验以支持仿真结果。测得的波束图显示出明显的零点深度，证实了实验设置的有效性。", "summary": "本文针对差分麦克风阵列（DMA）中零点评估缺乏专门测量方法的问题，提出了新的测量指标：零点深度（ND）和零点宽度（NW），以表征DMA波束功率图中的零点。研究探讨了信号量化效应对不同阶线性DMA和各种波束图的影响，并推导了量化输出的解析表达式。通过仿真和消声室实验验证了所提出的测量方法和研究发现，证实了DMA在干扰消除中的有效性。", "keywords": "差分麦克风阵列, 零点深度, 零点宽度, 波束图, 信号量化", "comments": "本文的创新之处在于引入了专门用于表征差分麦克风阵列中零点的新颖测量方法（零点深度和零点宽度），填补了现有文献的空白。其重要性在于提供了一种更直接、更定量的方式来评估DMA在消除干扰方面的性能，这对于需要精确干扰衰减的应用至关重要。研究中包含信号量化效应的分析以及通过仿真和实验进行的验证，增加了研究结果的鲁棒性。"}}
{"id": "2506.21441", "title": "An evaluation of level of detail degradation in head-mounted display peripheries", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Martin Reddy"], "summary": "A paradigm for the design of systems that manage level of detail in virtual\nenvironments is proposed. As an example of the prototyping step in this\nparadigm, a user study was performed to evaluate the effectiveness of high\ndetail insets used with head-mounted displays. Ten subjects were given a simple\nsearch task that required the location and identification of a single target\nobject. All subjects used seven different displays (the independent variable),\nvarying in inset size and peripheral detail, to perform this task. Frame rate,\ntarget location, subject input method, and order of display use were all\ncontrolled. Primary dependent measures were search time on trials with correct\nidentification, and the percentage of all trials correctly identified. ANOVAs\nof the results showed that insetless, high detail displays did not lead to\nsignificantly different search times or accuracies than displays with insets.\nIn fact, only the insetless, low detail display returned significantly\ndifferent results. Further research is being performed to examine the effect of\nvarying task complexity, inset size, and level of detail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21441v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21441v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "头戴式显示器外围细节层次降级的评估", "tldr": "本研究评估了头戴式显示器中高细节插图对用户在搜索任务中表现的影响。结果显示，高细节显示器无论有无插图，在搜索时间或准确性上均无显著差异，仅无插图的低细节显示器表现出显著不同。", "motivation": "提出一种用于管理虚拟环境中细节层次的系统设计范例，并作为该范例中原型步骤的一个例子，评估头戴式显示器中使用高细节插图的有效性。", "method": "进行了一项用户研究，十名受试者执行简单的目标搜索和识别任务。受试者使用七种不同配置（在插图大小和外围细节上有所不同）的头戴式显示器。实验控制了帧率、目标位置、输入方法和显示器使用顺序。通过方差分析（ANOVAs）评估了搜索时间和正确识别率这两个主要因变量。", "result": "ANOVA结果显示，无插图的高细节显示器与有插图的显示器相比，在搜索时间或准确性上没有显著差异。事实上，只有无插图的低细节显示器返回了显著不同的结果。", "conclusion": "在所测试的任务和条件下，头戴式显示器中高细节插图的使用并未显著改善用户在搜索任务中的表现。仅无插图的低细节显示器表现出显著差异，这表明并非所有细节层次的降级策略都对性能产生负面影响，或并非所有高细节的提供都能带来性能提升。", "translation": "提出了一个用于管理虚拟环境中细节层次的系统设计范例。作为该范例中原型步骤的一个例子，进行了一项用户研究，以评估与头戴式显示器一起使用的高细节插图的有效性。十名受试者被分配了一个简单的搜索任务，要求定位和识别一个单一的目标对象。所有受试者都使用了七种不同的显示器（自变量），这些显示器在插图大小和外围细节方面有所不同，以执行此任务。帧率、目标位置、受试者输入方法和显示器使用顺序都得到了控制。主要的因变量是正确识别试验的搜索时间，以及所有正确识别试验的百分比。结果的方差分析（ANOVAs）显示，无插图的高细节显示器与有插图的显示器相比，在搜索时间或准确性上没有显著差异。事实上，只有无插图的低细节显示器返回了显著不同的结果。正在进行进一步的研究，以检查不同任务复杂性、插图大小和细节层次的影响。", "summary": "本研究提出了一种虚拟环境细节层次管理系统的设计范式，并以头戴式显示器为例，通过用户研究评估了高细节插图的有效性。实验中，10名受试者使用七种不同配置的显示器完成简单搜索任务。结果表明，高细节显示器无论是否包含插图，在搜索时间或准确性上均无显著差异，仅无插图的低细节显示器表现出显著不同。这提示在头戴式显示器中，并非所有细节层次的增加都能带来性能提升。", "keywords": "头戴式显示器, 细节层次, 虚拟环境, 用户研究, 性能评估", "comments": "该研究通过严格的实验设计，对头戴式显示器中细节层次管理策略的实际效果进行了量化评估。其创新点在于提出了一个设计范式并进行了原型验证。结果表明，在特定任务下，高细节插图并未带来预期的性能提升，这挑战了直觉并对虚拟现实系统设计具有重要指导意义，提示开发者需谨慎评估细节优化策略。研究的局限性在于任务简单，未来工作将探索更复杂场景。"}}
{"id": "2506.20995", "title": "Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance", "authors": ["Akio Hayakawa", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji"], "summary": "We propose a novel step-by-step video-to-audio generation method that\nsequentially produces individual audio tracks, each corresponding to a specific\nsound event in the video. Our approach mirrors traditional Foley workflows,\naiming to capture all sound events induced by a given video comprehensively.\nEach generation step is formulated as a guided video-to-audio synthesis task,\nconditioned on a target text prompt and previously generated audio tracks. This\ndesign is inspired by the idea of concept negation from prior compositional\ngeneration frameworks. To enable this guided generation, we introduce a\ntraining framework that leverages pre-trained video-to-audio models and\neliminates the need for specialized paired datasets, allowing training on more\naccessible data. Experimental results demonstrate that our method generates\nmultiple semantically distinct audio tracks for a single input video, leading\nto higher-quality composite audio synthesis than existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20995v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20995v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "负面音频引导的逐步视频到音频合成", "tldr": "提出一种受拟音启发的分步视频到音频合成方法，通过负面音频引导生成高质量的独立音轨。", "motivation": "旨在全面捕捉视频中由特定声音事件引起的所有音效，并克服对专用配对数据集的需求。", "method": "提出一种新颖的逐步视频到音频生成方法，按顺序生成与视频中特定声音事件对应的独立音轨。每个生成步骤都被表述为引导式视频到音频合成任务，以目标文本提示和先前生成的音轨为条件。该设计受概念否定思想的启发，并引入了一个训练框架，利用预训练的视频到音频模型，无需专门的配对数据集。", "result": "该方法为单个输入视频生成了多个语义上不同的音轨，并产生了比现有基线更高质量的复合音频合成。", "conclusion": "该研究成功开发了一种分步视频到音频合成方法，能够生成高质量、语义独立的音轨，从而在无需专用配对数据集的情况下实现卓越的复合音频合成。", "translation": "我们提出了一种新颖的逐步视频到音频生成方法，该方法按顺序生成独立的音轨，每个音轨对应视频中的特定声音事件。我们的方法模仿传统的拟音工作流程，旨在全面捕捉给定视频引起的所有声音事件。每个生成步骤都被表述为引导式视频到音频合成任务，以目标文本提示和先前生成的音轨为条件。这种设计灵感来源于先前组合生成框架中的概念否定思想。为了实现这种引导式生成，我们引入了一个训练框架，该框架利用预训练的视频到音频模型，并消除了对专用配对数据集的需求，从而允许在更易获取的数据上进行训练。实验结果表明，我们的方法为单个输入视频生成了多个语义上不同的音轨，从而产生了比现有基线更高质量的复合音频合成。", "summary": "本文提出一种新颖的分步视频到音频合成方法，通过模仿传统拟音工作流程，顺序生成与视频中声音事件对应的独立音轨。该方法将每个步骤视为引导式合成任务，并利用受概念否定启发的训练框架，在无需专用配对数据集的情况下，利用预训练模型进行训练。实验证明，该方法能为单个视频生成多个语义独立的音轨，并产生优于现有基线的复合音频质量。", "keywords": "视频到音频合成, 拟音, 负面音频引导, 音频生成, 分步生成", "comments": "这篇论文的创新点在于其分步生成独立音轨的方法，这与传统的拟音工作流程相呼应，提高了合成音频的精细度和质量。特别值得注意的是，它利用概念否定思想进行引导式生成，并且能够利用预训练模型并在无需专门配对数据集的情况下进行训练，这大大降低了数据门槛，使其方法更具实用性。"}}
{"id": "2506.21175", "title": "On Minimizing Wiggle in Stacked Area Charts", "authors": ["Alexander Dobler", "Martin Nöllenburg"], "summary": "Stacked area charts are a widely used visualization technique for numerical\ntime series. The x-axis represents time, and the time series are displayed as\nhorizontal, variable-height layers stacked on top of each other. The height of\neach layer corresponds to the time series values at each time point. The main\naesthetic criterion for optimizing the readability of stacked area charts is\nthe amount of vertical change of the borders between the time series in the\nvisualization, called wiggle. While many heuristic algorithms have been\ndeveloped to minimize wiggle, the computational complexity of minimizing wiggle\nhas not been formally analyzed. In this paper, we show that different variants\nof wiggle minimization are NP-hard and even hard to approximate. We also\npresent an exact mixed-integer linear programming formulation and compare its\nperformance with a state-of-the-art heuristic in an experimental evaluation.\nLastly, we consider a special case of wiggle minimization that corresponds to\nthe fundamentally interesting and natural problem of ordering a set of numbers\nas to minimize their sum of absolute prefix sums. We show several complexity\nresults for this problem that imply some of the mentioned hardness results for\nwiggle minimization.", "comment": "19 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21175v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21175v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "堆叠面积图中最小化摆动", "tldr": "本文证明了堆叠面积图中最小化摆动问题是NP难的，并且难以近似，同时提出了精确的混合整数线性规划（MILP）公式，并分析了一个特殊情况的复杂性。", "motivation": "堆叠面积图是一种广泛使用的数值时间序列可视化技术，其可读性的主要美学标准是时间序列之间边界的垂直变化量，即“摆动”。尽管已开发出许多启发式算法来最小化摆动，但最小化摆动的计算复杂性尚未得到正式分析。", "method": "本文正式分析了摆动最小化问题的计算复杂性，证明了不同变体是NP难的且难以近似。提出了一个精确的混合整数线性规划（MILP）公式，并与最先进的启发式算法进行了实验比较。最后，考虑了一个特殊情况，即最小化一组数字的绝对前缀和之和的问题，并展示了其复杂性结果。", "result": "本文证明了摆动最小化问题的不同变体是NP难的，甚至难以近似。提出了一个精确的混合整数线性规划（MILP）公式，并在实验评估中将其性能与最先进的启发式算法进行了比较。此外，还展示了最小化绝对前缀和之和这一特殊情况的复杂性结果，这些结果也暗示了摆动最小化的一些难度。", "conclusion": "本文正式确立了堆叠面积图中摆动最小化问题的计算复杂性，证明其为NP难且难以近似。同时提供了一个精确的混合整数线性规划解决方案，并通过分析一个相关的基础问题深化了对该问题的理解。", "translation": "堆叠面积图是一种广泛使用的数值时间序列可视化技术。X轴表示时间，时间序列以水平、高度可变的层叠方式显示。每层的高度对应于每个时间点的时间序列值。优化堆叠面积图可读性的主要美学标准是可视化中时间序列之间边界的垂直变化量，称为“摆动”。虽然已经开发了许多启发式算法来最小化摆动，但最小化摆动的计算复杂性尚未得到正式分析。在本文中，我们证明了摆动最小化问题的不同变体是NP难的，甚至难以近似。我们还提出了一个精确的混合整数线性规划公式，并在实验评估中将其性能与最先进的启发式算法进行了比较。最后，我们考虑了摆动最小化的一种特殊情况，它对应于对一组数字进行排序以最小化其绝对前缀和之和这一根本有趣且自然的问题。我们展示了该问题的几个复杂性结果，这些结果也暗示了摆动最小化的一些难度。", "summary": "本文深入探讨了堆叠面积图中“摆动”最小化的问题，该问题关乎图表的可读性。此前，尽管存在多种启发式算法，但其计算复杂性未被正式分析。本研究首次证明了摆动最小化问题的不同变体是NP难的，甚至难以近似。为此，作者提出了一个精确的混合整数线性规划（MILP）公式，并与现有最佳启发式算法进行了实验比较。此外，文章还分析了一个相关的特殊情况——最小化一组数字的绝对前缀和之和，并揭示了其复杂性，从而进一步支持了摆动最小化问题的难度结论。", "keywords": "堆叠面积图, 摆动最小化, NP难, 混合整数线性规划, 计算复杂性", "comments": "本文的创新之处在于首次对堆叠面积图中的“摆动”最小化问题进行了严格的计算复杂性分析，填补了该领域的一个空白。证明其为NP难且难以近似，对未来算法设计具有重要指导意义。提出的混合整数线性规划（MILP）公式为解决该问题提供了一个精确的方法，尽管可能计算成本较高。将该问题与“最小化绝对前缀和之和”这一基础数学问题联系起来，也展现了深刻的理论洞察力。"}}
{"id": "2506.21368", "title": "Real-time and personalized product recommendations for large e-commerce platforms", "authors": ["Matteo Tolloso", "Davide Bacciu", "Shahab Mokarizadeh", "Marco Varesi"], "summary": "We present a methodology to provide real-time and personalized product\nrecommendations for large e-commerce platforms, specifically focusing on\nfashion retail. Our approach aims to achieve accurate and scalable\nrecommendations with minimal response times, ensuring user satisfaction,\nleveraging Graph Neural Networks and parsimonious learning methodologies.\nExtensive experimentation with datasets from one of the largest e-commerce\nplatforms demonstrates the effectiveness of our approach in forecasting\npurchase sequences and handling multi-interaction scenarios, achieving\nefficient personalized recommendations under real-world constraints.", "comment": "This paper has been accepted for publication at the International\n  Conference on Artificial Neural Networks (ICANN) 2025. The final\n  authenticated version will be available for purchase through the publisher's\n  website. The conference proceedings will be published by Springer in the\n  Lecture Notes in Computer Science (LNCS) series", "pdf_url": "http://arxiv.org/pdf/2506.21368v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21368v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "大型电商平台的实时个性化产品推荐", "tldr": "该研究提出一种针对大型电商平台（特别是时尚零售）的实时个性化产品推荐方法，该方法利用图神经网络和简约学习，在预测购买序列和处理多交互场景方面表现出有效性，实现了高效的个性化推荐。", "motivation": "为大型电商平台提供准确、可扩展、响应时间短的实时个性化产品推荐，以确保用户满意度。", "method": "采用图神经网络（Graph Neural Networks）和简约学习（parsimonious learning methodologies）。", "result": "在大型电商平台数据集上的广泛实验表明，该方法在预测购买序列和处理多交互场景方面有效，并在实际约束下实现了高效的个性化推荐。", "conclusion": "该方法能够为大型电商平台提供有效且高效的实时个性化产品推荐。", "translation": "我们提出了一种为大型电商平台提供实时个性化产品推荐的方法，特别关注时尚零售。我们的方法旨在通过最小的响应时间实现准确和可扩展的推荐，确保用户满意度，并利用图神经网络和简约学习方法。通过来自最大电商平台之一的数据集进行广泛实验，证明了我们方法在预测购买序列和处理多交互场景方面的有效性，在实际约束下实现了高效的个性化推荐。", "summary": "这篇论文提出了一种针对大型电商平台（特别是时尚零售）的实时个性化产品推荐方法。该方法结合了图神经网络和简约学习，旨在实现高准确度、高可扩展性和低响应时间。实验结果表明，该方法在预测购买序列和处理多交互场景方面表现出色，能够在真实世界约束下提供高效的个性化推荐。", "keywords": "实时推荐, 个性化推荐, 电商平台, 图神经网络, 简约学习", "comments": "这项研究的创新之处在于结合了图神经网络和简约学习来解决大型电商平台实时个性化推荐的挑战。其重要性在于能够在大规模数据集和严格的实时性要求下提供高效且准确的推荐，这对于提升用户体验和电商销售额至关重要。"}}
{"id": "2506.20917", "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "authors": ["Zhengyan Shi"], "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "comment": "PhD Thesis", "pdf_url": "http://arxiv.org/pdf/2506.20917v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20917v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "优化下游任务的语言模型：一个后训练视角", "tldr": "本论文提出了优化语言模型后训练的方法，使其在下游任务中更鲁棒、高效和泛化。", "motivation": "当前大型语言模型在适应特定任务时面临挑战，原因包括对无标签数据利用不足、在小数据集上过拟合以及计算成本高昂，这些限制阻碍了它们在真实世界语言任务中的应用。", "method": "本论文提出了一系列方法：首先，引入了一种新颖的持续预训练技术，用于从无标签数据中提取任务相关知识；其次，提出了一种参数高效的微调方法，以降低内存和计算成本；再次，改进了监督微调方法，使语言模型更好地遵循指令，尤其是在标签数据稀缺时；最后，开发了新的评估方法和基准，如多跳空间推理任务，以全面评估语言模型的能力和适应性。", "result": "通过在各种NLP任务中进行广泛的实证研究，结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更能适应广泛的应用。", "conclusion": "这些进展标志着在构建更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。", "translation": "语言模型（LMs）在自然语言处理（NLP）中展现出卓越的能力，但如何高效、鲁棒地将其适应特定任务仍然是一个挑战。随着其规模和复杂性的增长，在有标签数据上微调语言模型通常会低效利用可用的无标签数据，导致在小型任务特定数据集上过拟合，并带来显著的计算成本。这些局限性阻碍了它们在真实世界开放式语言任务中的应用。\n本论文提出了一系列方法，以更好地使语言模型适应下游应用。首先，我们探索了从无标签数据中提取任务相关知识的策略，引入了一种新颖的持续预训练技术，该技术优于最先进的半监督方法。其次，我们提出了一种参数高效的微调方法，该方法在保持竞争力性能的同时，显著降低了内存和计算成本。我们还引入了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标签数据稀缺时，从而提高了它们在一系列NLP任务（包括开放式生成）中的性能。最后，我们开发了新的评估方法和基准，例如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。\n通过在各种NLP任务中进行广泛的实证研究，我们的结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更能适应广泛的应用。这些进展标志着在构建更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。", "summary": "本论文旨在解决大型语言模型（LMs）在下游任务中高效和鲁棒适应的挑战。它提出了一系列新颖的后训练方法，包括一种利用无标签数据的持续预训练技术、一种参数高效的微调方法以及改进的监督微调方法以更好地遵循指令。研究还引入了新的评估基准。实证研究表明，这些方法显著增强了语言模型在各种NLP任务中的鲁棒性、效率和泛化能力，从而推动了通用人工智能的发展。", "keywords": "语言模型, 微调, 后训练, 无标签数据, 下游任务", "comments": "该论文在优化语言模型的全面后训练方法上具有创新性，解决了数据利用、计算成本和稀缺标签数据适应性等关键问题。其专注于提高鲁棒性、效率和泛化能力以适应真实世界应用，对于语言模型的实际部署和通用人工智能的进步至关重要。"}}
{"id": "2506.21070", "title": "Inverse source problem with a posteriori interior measurements for space-time fractional diffusion equations", "authors": ["Kai Yu", "Zhiyuan Li", "Yikan Liu"], "summary": "This paper investigates an inverse source problem for space-time fractional\ndiffusion equations from a posteriori interior measurements. The uniqueness\nresult is established by the memory effect of fractional derivatives and the\nunique continuation property. For the numerical reconstruction, the inverse\nproblem is reformulated as an optimization problem with the Tikhonov\nregularization. We use the Levenberg-Marquardt method to identity the unknown\nsource from noisy measurements. Finally, we give some numerical examples to\nillustrate the efficiency and accuracy of the proposed algorithm.", "comment": "14 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.21070v1", "categories": ["math.NA", "cs.NA", "35R30, 35R11"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21070v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "具有后验内部测量的时空分数阶扩散方程的逆源问题", "tldr": "本文研究了时空分数阶扩散方程的逆源问题，通过分数阶导数的记忆效应和唯一延拓性质证明了唯一性，并利用Tikhonov正则化和Levenberg-Marquardt方法进行数值重建，数值例子验证了算法的有效性和准确性。", "motivation": "本文旨在研究时空分数阶扩散方程的逆源问题。", "method": "通过分数阶导数的记忆效应和唯一延拓性质建立了唯一性结果。对于数值重建，将逆问题重新表述为一个带有Tikhonov正则化的优化问题，并使用Levenberg-Marquardt方法从噪声测量中识别未知源。", "result": "建立了逆源问题的唯一性结果，并通过数值例子说明了所提出算法的效率和准确性。", "conclusion": "所提出的算法在从噪声测量中识别时空分数阶扩散方程的未知源方面是有效和准确的。", "translation": "本文研究了具有后验内部测量的时空分数阶扩散方程的逆源问题。通过分数阶导数的记忆效应和唯一延拓性质建立了唯一性结果。对于数值重建，将逆问题重新表述为带有Tikhonov正则化的优化问题。我们使用Levenberg-Marquardt方法从噪声测量中识别未知源。最后，我们给出了一些数值例子来说明所提出算法的效率和准确性。", "summary": "本文研究了时空分数阶扩散方程的逆源问题，利用分数阶导数的记忆效应和唯一延拓性质证明了唯一性。在数值重建方面，该问题被转化为一个带有Tikhonov正则化的优化问题，并采用Levenberg-Marquardt方法从噪声数据中识别未知源。数值实验验证了所提算法的有效性和准确性。", "keywords": "逆源问题, 分数阶扩散方程, 唯一性, Tikhonov正则化, Levenberg-Marquardt方法", "comments": "本文的创新点在于结合分数阶导数的记忆效应和唯一延拓性质来解决时空分数阶扩散方程的逆源问题的唯一性，并提出了一个基于Tikhonov正则化和Levenberg-Marquardt方法的数值重建框架，为解决这类复杂逆问题提供了有效途径。"}}
{"id": "2506.21478", "title": "SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture", "authors": ["Kehan Sui", "Jinxu Xiang", "Fang Jin"], "summary": "Singing voice synthesis (SVS) aims to generate expressive and high-quality\nvocals from musical scores, requiring precise modeling of pitch, duration, and\narticulation. While diffusion-based models have achieved remarkable success in\nimage and video generation, their application to SVS remains challenging due to\nthe complex acoustic and musical characteristics of singing, often resulting in\nartifacts that degrade naturalness. In this work, we propose SmoothSinger, a\nconditional diffusion model designed to synthesize high quality and natural\nsinging voices. Unlike prior methods that depend on vocoders as a final stage\nand often introduce distortion, SmoothSinger refines low-quality synthesized\naudio directly in a unified framework, mitigating the degradation associated\nwith two-stage pipelines. The model adopts a reference-guided dual-branch\narchitecture, using low-quality audio from any baseline system as a reference\nto guide the denoising process, enabling more expressive and context-aware\nsynthesis. Furthermore, it enhances the conventional U-Net with a parallel\nlow-frequency upsampling path, allowing the model to better capture pitch\ncontours and long term spectral dependencies. To improve alignment during\ntraining, we replace reference audio with degraded ground truth audio,\naddressing temporal mismatch between reference and target signals. Experiments\non the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that\nSmoothSinger achieves state-of-the-art results in both objective and subjective\nevaluations. Extensive ablation studies confirm its effectiveness in reducing\nartifacts and improving the naturalness of synthesized voices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21478v1", "categories": ["cs.SD", "cs.AI"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21478v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SmoothSinger：一种具有多分辨率架构的歌唱语音合成条件扩散模型", "tldr": "SmoothSinger是一个条件扩散模型，通过直接在统一框架中优化低质量音频，并采用参考引导的双分支架构和增强的U-Net，实现了高质量和自然的歌唱语音合成，在Opencpop数据集上达到SOTA。", "motivation": "歌唱语音合成 (SVS) 需要精确建模音高、持续时间和发音，但基于扩散的模型在SVS中的应用仍具挑战性，常导致不自然的人工痕迹。现有方法依赖声码器作为最终阶段，容易引入失真。", "method": "提出SmoothSinger，一个条件扩散模型，用于合成高质量和自然的歌唱语音。它在统一框架中直接细化低质量合成音频，以减轻两阶段管道的退化。模型采用参考引导的双分支架构，使用低质量音频作为参考指导去噪过程。此外，它通过并行低频上采样路径增强了传统的U-Net，以更好地捕捉音高轮廓和长期频谱依赖。训练时，用降级的真实音频替代参考音频，以解决时间不匹配问题。", "result": "在Opencpop数据集上的实验表明，SmoothSinger在客观和主观评估中均取得了最先进的结果。广泛的消融研究证实了其在减少人工痕迹和提高合成语音自然度方面的有效性。", "conclusion": "SmoothSinger通过其创新的架构和训练策略，成功解决了歌唱语音合成中扩散模型应用面临的挑战，实现了高质量和自然度的SVS，并达到了最先进的性能。", "translation": "歌唱语音合成 (SVS) 旨在从乐谱生成富有表现力的高质量人声，需要对音高、持续时间和发音进行精确建模。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱复杂的声学和音乐特性，它们在SVS中的应用仍然具有挑战性，经常导致降低自然度的人工痕迹。在这项工作中，我们提出了 SmoothSinger，一个条件扩散模型，旨在合成高质量和自然的歌唱语音。与依赖声码器作为最终阶段并经常引入失真的先前方法不同，SmoothSinger 在统一框架中直接细化低质量合成音频，减轻了两阶段管道相关的退化。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来指导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的 U-Net，使模型能够更好地捕捉音高轮廓和长期频谱依赖。为了在训练期间改善对齐，我们用降级的真实音频替换参考音频，解决了参考信号和目标信号之间的时间不匹配。在大型中文歌唱语料库 Opencpop 数据集上的实验表明，SmoothSinger 在客观和主观评估中均取得了最先进的结果。广泛的消融研究证实了其在减少人工痕迹和提高合成语音自然度方面的有效性。", "summary": "SmoothSinger是一个用于歌唱语音合成的条件扩散模型，旨在克服现有扩散模型在SVS中生成人工痕迹和两阶段管道引入失真的问题。它通过直接在统一框架中细化低质量音频，并采用参考引导的双分支架构和增强的U-Net（带有并行低频上采样路径），实现了高质量和自然的歌唱声音。在Opencpop数据集上的实验证明，SmoothSinger在客观和主观评估中均达到了最先进的性能，并有效减少了人工痕迹，提高了合成语音的自然度。", "keywords": "歌唱语音合成, 条件扩散模型, 多分辨率架构, 音频细化, SmoothSinger", "comments": "SmoothSinger的创新之处在于其统一的直接音频细化框架，避免了传统声码器引入的失真。其参考引导的双分支架构和改进的U-Net（特别是并行低频上采样路径）是关键，有助于更好地捕捉音乐特性，如音高。解决了训练中参考与目标信号时间不匹配的问题也提升了模型性能。这项工作对于推动扩散模型在复杂音频生成领域的应用具有重要意义。"}}
{"id": "2506.21211", "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models", "authors": ["Quanming Liu", "Xupeng Bu", "Zhichao Yan", "Ru Li"], "summary": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21211v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21211v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "T³：基于多级树的大语言模型自动程序修复", "tldr": "本文提出了T³框架，它结合大语言模型和树搜索，显著提高了自动程序修复中生成候选修复方案的精度，并为优化修复策略提供了指导。", "motivation": "尽管大语言模型（LLMs）和思维链（CoT）技术在推理能力上取得了显著进展，但由于自动程序修复（APR）任务的复杂逻辑和多步推理需求，CoT技术在APR领域的应用仍然不足。", "method": "本文系统评估了几种常见的CoT技术在APR任务中的表现，并提出了创新的T³框架。T³将LLMs强大的推理能力与树搜索相结合。", "result": "T³框架有效提高了生成候选修复方案的精度，并为APR任务中优化样本选择和修复策略提供了宝贵的指导。", "conclusion": "T³框架为实现高效的自动化调试建立了一个强大的基础。", "translation": "自动程序修复（APR）是软件开发和维护中的一项核心技术，旨在通过最少的人工干预实现自动化缺陷修复。近年来，大型语言模型（LLMs）和思维链（CoT）技术的巨大进步显著增强了这些模型的推理能力。然而，由于所需的复杂逻辑和多步推理能力，CoT技术在APR领域的应用仍然不足。本研究系统评估了几种常见的CoT技术在APR任务中的表现，并提出了一个创新的T³框架，该框架将LLMs强大的推理能力与树搜索相结合，有效提高了生成候选修复方案的精度。此外，T³为优化APR任务中的样本选择和修复策略提供了宝贵的指导，为实现高效的自动化调试建立了一个强大的框架。", "summary": "本文提出了一个名为T³的创新框架，旨在通过结合大语言模型（LLMs）的强大推理能力和树搜索技术，解决自动程序修复（APR）中思维链（CoT）技术应用不足的问题。研究评估了现有CoT技术在APR中的表现，并展示了T³如何有效提高候选修复方案的生成精度，同时为优化APR任务中的样本选择和修复策略提供了指导，从而建立了一个高效自动化调试的强大框架。", "keywords": "自动程序修复, 大语言模型, 思维链, 树搜索, 自动化调试", "comments": "该论文的创新点在于将大语言模型（LLMs）的推理能力与树搜索技术相结合，以解决自动程序修复（APR）中复杂的多步推理挑战。这种结合不仅提高了修复方案的生成精度，还为APR策略优化提供了新思路，对自动化调试领域具有重要意义。"}}
{"id": "2506.21030", "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner", "authors": ["Zhou Tianxing", "Wang Zhirui", "Ao Haojia", "Chen Guangyan", "Xing Boyang", "Cheng Jingwen", "Yang Yi", "Yue Yufeng"], "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21030v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21030v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "STEP Planner：构建跨层级子目标树作为具身长周期任务规划器", "tldr": "STEP Planner通过构建跨层级子目标树，解决了LLM在长周期具身任务规划中成功率低的问题，并在虚拟和真实机器人实验中取得了优于SOTA的性能。", "motivation": "在现实世界环境中部署机器人需要可靠的长周期任务规划能力。然而，直接使用大型语言模型（LLMs）作为动作序列生成器，由于其对长周期具身任务推理能力的限制，通常导致成功率较低。", "method": "STEP框架通过一对闭环模型构建子目标树：一个子目标分解模型和一个叶节点终止模型。该框架开发了一个从粗到细的层级树结构。子目标分解模型利用基础LLM将复杂目标分解为可管理的子目标，从而扩展子目标树。叶节点终止模型根据环境状态提供实时反馈，决定何时终止树的扩展，并确保每个叶节点可以直接转换为原始动作。", "result": "在VirtualHome WAH-NL基准测试和真实机器人上进行的实验表明，STEP在长周期具身任务完成方面取得了高达34%（WAH-NL）和25%（真实机器人）的成功率，优于现有最先进（SOTA）的方法。", "conclusion": "STEP Planner通过构建跨层级子目标树，有效提升了机器人在长周期具身任务中的规划成功率，解决了LLM在该领域应用中的局限性，并展现出超越现有方法的性能。", "translation": "可靠的长周期任务规划能力对于在现实世界环境中部署机器人至关重要。然而，直接使用大型语言模型（LLMs）作为动作序列生成器，由于其对长周期具身任务推理能力的限制，通常导致成功率较低。在STEP框架中，我们通过一对闭环模型构建子目标树：一个子目标分解模型和一个叶节点终止模型。在此框架内，我们开发了一个从粗到细的层级树结构。子目标分解模型利用基础LLM将复杂目标分解为可管理的子目标，从而扩展子目标树。叶节点终止模型根据环境状态提供实时反馈，决定何时终止树的扩展，并确保每个叶节点可以直接转换为原始动作。在VirtualHome WAH-NL基准测试和真实机器人上进行的实验表明，STEP在长周期具身任务完成方面取得了高达34%（WAH-NL）和25%（真实机器人）的成功率，优于现有最先进（SOTA）的方法。", "summary": "STEP Planner提出了一种用于具身长周期任务规划的新方法，通过构建一个跨层级子目标树来解决传统LLM在处理此类任务时成功率低的问题。该框架包含一个利用LLM进行子目标分解的模型和一个基于环境反馈的叶节点终止模型，形成闭环控制，确保子目标可执行。实验结果表明，STEP在虚拟和真实机器人环境中均显著提高了长周期任务的完成成功率，超越了现有最先进的方法。", "keywords": "长周期任务规划, 具身智能, 子目标树, 大型语言模型, 机器人规划", "comments": "这篇论文的创新点在于提出了一个结合LLM和实时反馈的闭环系统，通过构建层级子目标树来应对具身长周期任务规划的挑战。其通过子目标分解和叶节点终止的协同作用，有效地弥补了LLM在复杂推理上的不足，并实现了可观的性能提升，对于具身AI领域具有重要意义。"}}
{"id": "2506.21333", "title": "A Systematic Review of Human-AI Co-Creativity", "authors": ["Saloni Singh", "Koen Hndriks", "Drik Heylen", "Kim Baraka"], "summary": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21333v1", "categories": ["cs.HC", "cs.AI", "I.2.11"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21333v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人机协同创造的系统综述", "tldr": "本文对62篇关于人机协同创造系统的论文进行了系统综述，识别了关键设计维度和24项设计考量，发现高用户控制度能提升满意度，自适应主动系统能增强协作，并指出了该领域的现有差距。", "motivation": "为了给未来人机协同创造系统的设计提供有价值且高效的基础，本研究旨在通过系统文献综述来总结现有设计考量。", "method": "本研究对62篇关于协同创造系统的论文进行了系统文献综述，这些论文涵盖视觉艺术、设计和写作等应用领域，其中AI不仅是工具，更是创造过程中的积极协作者。", "result": "研究识别了与系统设计相关的几个关键维度：创造过程的阶段、创造性任务、系统的主动行为、用户控制、系统实体化和AI模型类型。研究发现，提供高用户控制的系统能带来更高的满意度、信任和对创造成果更强的归属感。此外，自适应且情境敏感的主动系统可以增强协作。研究还提取了24项设计考量，强调鼓励用户外化思考以及增加系统社交存在感和透明度以培养信任的价值。", "conclusion": "尽管近期取得了进展，但在人机协同创造领域仍存在重要空白，例如对问题澄清等早期创造阶段的支持有限，以及用户适应AI系统相关的挑战。", "translation": "协同创造社区在开发更复杂和定制化的系统以支持和增强人类创造力方面取得了显著进展。先前的设计考量可以作为未来系统宝贵而高效的基础。为了支持这项工作，我们对62篇关于协同创造系统的论文进行了系统文献综述。这些论文涵盖了视觉艺术、设计和写作等广泛的应用领域，其中AI不仅作为工具，而且作为创造过程中的积极合作者。通过这次综述，我们确定了几个与系统设计相关的关键维度：创造过程的阶段、创造性任务、系统的主动行为、用户控制、系统实体化和AI模型类型。我们的研究结果表明，提供高用户控制的系统能带来更高的满意度、信任和对创造成果更强的归属感。此外，自适应且情境敏感的主动系统可以增强协作。我们还提取了24项设计考量，强调了鼓励用户外化思考以及增加系统社交存在感和透明度以培养信任的价值。尽管近期取得了进展，但仍存在重要的空白，例如对问题澄清等早期创造阶段的支持有限，以及用户适应AI系统相关的挑战。", "summary": "本文对62篇关于人机协同创造系统的文献进行了系统综述。研究旨在为未来系统设计提供基础，识别了创造过程阶段、任务、系统主动性、用户控制、系统实体化和AI模型类型等关键设计维度。研究发现，高用户控制能提升用户满意度和归属感，而自适应主动系统能增强协作。此外，还提取了24项设计考量，强调了外部化思考和系统透明度的重要性。研究同时指出了当前领域在早期创造阶段支持和用户适应性方面的不足。", "keywords": "人机协同创造, 系统综述, 协同创造系统, 用户控制, 设计考量", "comments": "这篇系统综述为理解人机协同创造领域的设计挑战和机遇提供了全面的视角。其创新之处在于系统性地梳理了现有研究，并提炼出关键的设计维度和具体的设计考量，对未来协同创造系统的开发具有重要的指导意义。论文强调了用户控制和系统主动性的平衡，以及信任建立的重要性，这些都是人机协作中不可或缺的因素。然而，论文也指出了当前研究在支持早期创造阶段和用户适应性方面的局限性，为未来的研究指明了方向。"}}
{"id": "2506.21309", "title": "Linear codes arising from the point-hyperplane geometry-Part I: the Segre embedding", "authors": ["Ilaria Cardinali", "Luca Giuzzi"], "summary": "Let $V$ be a vector space over the finite field $\\mathbb{F}_q$ with $q$\nelements and $\\Lambda$ be the image of the Segre geometry\n$\\mathrm{PG}(V)\\otimes\\mathrm{PG}(V^*)$ in $\\mathrm{PG}(V\\otimes V^*)$.\nConsider the subvariety $\\Lambda_{1}$ of $\\Lambda$ represented by the pure\ntensors $x\\otimes \\xi$ with $x\\in V$ and $\\xi\\in V^*$ such that $\\xi(x)=0$.\nRegarding $\\Lambda_1$ as a projective system of $\\mathrm{PG}(V\\otimes V^*)$, we\nstudy the linear code $\\mathcal{C}(\\Lambda_1)$ arising from it. The code\n$\\mathcal{C}(\\Lambda_1)$ is minimal code and we determine its basic parameters,\nitsfull weight list and its linear automorphism group. We also give a\ngeometrical characterization of its minimum and second lowest weight codewords\nas well as of some of the words of maximum weight.", "comment": "29 pages", "pdf_url": "http://arxiv.org/pdf/2506.21309v1", "categories": ["math.CO", "cs.DM", "cs.IT", "math.IT", "51E22, 94B05, 14M12"], "cate": "math.CO", "url": "http://arxiv.org/abs/2506.21309v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "点-超平面几何产生的线性码——第一部分：Segre嵌入", "tldr": "本文研究了由Segre嵌入中一个特定子簇产生的线性码，确定了其基本参数、权重列表和自同构群，并对其码字进行了几何刻画。", "motivation": "研究由Segre几何中的特定子簇$\\\\Lambda_1$产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$的性质。", "method": "通过将$\\\\Lambda_1$视为射影系统，研究了由此产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$。具体方法包括确定其基本参数、完整的权重列表和线性自同构群，并对其最小、次低权重码字以及部分最大权重码字进行了几何刻画。", "result": "研究发现码$\\\\mathcal{C}(\\\\Lambda_1)$是极小码。文章确定了它的基本参数、完整的权重列表和线性自同构群。同时，对它的最小、次低权重码字以及部分最大权重码字给出了几何刻画。", "conclusion": "由Segre嵌入中特定子簇产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$是极小码，其各项基本参数、权重列表和自同构群已被确定，且其关键码字得到了几何刻画。", "translation": "设$V$是有限域$\\\\mathbb{F}_q$上的向量空间，$\\\\Lambda$是Segre几何$\\\\mathrm{PG}(V)\\\\otimes\\\\mathrm{PG}(V^*)$在$\\\\mathrm{PG}(V\\\\otimes V^*)$中的像。考虑$\\\\Lambda$的子簇$\\\\Lambda_{1}$，它由纯张量$x\\\\otimes \\\\xi$表示，其中$x\\\\in V$和$\\\\xi\\\\in V^*$满足$\\\\xi(x)=0$。将$\\\\Lambda_1$视为$\\\\mathrm{PG}(V\\\\otimes V^*)$的一个射影系统，我们研究由此产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$。码$\\\\mathcal{C}(\\\\Lambda_1)$是极小码，我们确定了它的基本参数、完整的权重列表和线性自同构群。我们还对其最小和次低权重码字以及部分最大权重码字进行了几何刻画。", "summary": "本文研究了由Segre几何中一个特定子簇$\\\\Lambda_1$（由满足$\\\\xi(x)=0$的纯张量$x\\\\otimes \\\\xi$构成）产生的线性码$\\\\mathcal{C}(\\\\Lambda_1)$。该研究确定了此极小码的基本参数、完整的权重列表和线性自同构群，并对其最小、次低以及部分最大权重码字进行了几何刻画。", "keywords": "线性码, Segre嵌入, 点-超平面几何, 极小码, 权重列表", "comments": "这篇论文在编码理论和代数几何的交叉领域做出了贡献，通过深入分析由Segre嵌入特定子簇产生的线性码，揭示了其重要的代数和几何性质。确定码的基本参数、权重列表和自同构群对于理解码的结构和性能至关重要，而几何刻画则提供了直观的理解。"}}
{"id": "2506.21327", "title": "Enabling Bitcoin Smart Contracts on the Internet Computer", "authors": ["Ryan Croote", "Islam El-Ashi", "Thomas Locher", "Yvonne-Anne Pignolet"], "summary": "There is growing interest in providing programmatic access to the value\nlocked in Bitcoin, which famously offers limited programmability itself.\nVarious approaches have been put forth in recent years, with the vast majority\nof proposed mechanisms either building new functionality on top of Bitcoin or\nleveraging a bridging mechanism to enable smart contracts that make use of\n``wrapped'' bitcoins on entirely different platforms.\n  In this work, an architecture is presented that follows a different approach.\nThe architecture enables the execution of Turing-complete Bitcoin smart\ncontracts on the Internet Computer (IC), a blockchain platform for hosting and\nexecuting decentralized applications. Instead of using a bridge, IC and Bitcoin\nnodes interact directly, eliminating potential security risks that the use of a\nbridge entails. This integration requires novel concepts, in particular to\nreconcile the probabilistic nature of Bitcoin with the irreversibility of\nfinalized state changes on the IC, which may be of independent interest.\n  In addition to the presentation of the architecture, we provide evaluation\nresults based on measurements of the Bitcoin integration running on mainnet.\nThe evaluation results demonstrate that, with finalization in a few seconds and\nlow execution costs, this integration enables complex Bitcoin-based\ndecentralized applications that were not practically feasible or economically\nviable before.", "comment": "Published at ICDCS 2025, waiting for DOI", "pdf_url": "http://arxiv.org/pdf/2506.21327v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21327v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在互联网计算机上实现比特币智能合约", "tldr": "本文提出了一种新架构，使比特币智能合约可以直接在互联网计算机（IC）上运行，通过节点直接交互而非桥接，提高了效率和安全性，并支持复杂的比特币去中心化应用。", "motivation": "比特币本身的可编程性有限，但市场对利用其价值进行编程访问的需求日益增长。现有方案（如在比特币之上构建新功能或通过桥接机制使用“封装”比特币）存在安全风险。", "method": "本文提出了一种新架构，使图灵完备的比特币智能合约能够在互联网计算机（IC）上执行。该架构通过IC和比特币节点之间的直接交互实现，避免了桥接机制带来的潜在安全风险。此外，它引入了新概念来协调比特币的概率性与IC上最终状态更改的不可逆性。", "result": "基于主网上的比特币集成测量结果显示，该集成可以在几秒内完成最终确定，且执行成本低。", "conclusion": "这种集成使得以前不切实际或不经济的复杂比特币去中心化应用成为可能。", "translation": "人们对比特币中锁定的价值进行编程访问的兴趣日益增长，众所周知，比特币本身提供的可编程性有限。近年来，各种方法层出不穷，绝大多数提出的机制要么在比特币之上构建新功能，要么利用桥接机制在完全不同的平台上实现使用“封装”比特币的智能合约。\n在这项工作中，提出了一种采用不同方法的新架构。该架构使图灵完备的比特币智能合约能够在互联网计算机（IC）上执行，IC是一个用于托管和执行去中心化应用程序的区块链平台。IC和比特币节点之间直接交互，而不是使用桥接，从而消除了使用桥接可能带来的潜在安全风险。这种集成需要新颖的概念，特别是要协调比特币的概率性与IC上最终状态更改的不可逆性，这可能具有独立的意义。\n除了架构的呈现，我们还提供了基于主网上比特币集成测量结果的评估。评估结果表明，通过几秒钟内的最终确定和低执行成本，这种集成使得以前不切实际或不经济的复杂比特币去中心化应用程序成为可能。", "summary": "本文提出了一种在互联网计算机（IC）上直接执行图灵完备比特币智能合约的新架构。该方案通过IC和比特币节点间的直接交互，避免了传统桥接机制带来的安全风险。为解决比特币的概率性与IC状态更改的不可逆性之间的冲突，文中引入了新颖概念。评估结果显示，该集成具有快速最终确定和低成本的优势，从而使复杂的比特币去中心化应用得以实现，克服了以往的实用性和经济性障碍。", "keywords": "比特币智能合约, 互联网计算机, 直接集成, 去中心化应用, 区块链互操作性", "comments": "这项工作通过实现互联网计算机与比特币的直接集成，提供了一种无需桥接即可在比特币上构建智能合约的创新方法，解决了现有方案的安全和效率问题。其核心创新在于协调了两种不同区块链的特性（比特币的概率性与IC的不可逆性）。这对于扩展比特币的实用性和推动更复杂的去中心化金融应用具有重要意义。"}}
{"id": "2506.20921", "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach", "authors": ["Tong Zeng", "Srivathsan Badrinarayanan", "Janghoon Ock", "Cheng-Kai Lai", "Amir Barati Farimani"], "summary": "Chemical process optimization is crucial to maximize production efficiency\nand economic performance. Traditional methods, including gradient-based\nsolvers, evolutionary algorithms, and parameter grid searches, become\nimpractical when operating constraints are ill-defined or unavailable,\nrequiring engineers to rely on subjective heuristics to estimate feasible\nparameter ranges. To address this constraint definition bottleneck, we present\na multi-agent framework of large language model (LLM) agents that autonomously\ninfer operating constraints from minimal process descriptions, then\ncollaboratively guide optimization using the inferred constraints. Our\nAutoGen-based agentic framework employs OpenAI's o3 model, with specialized\nagents for constraint generation, parameter validation, simulation execution,\nand optimization guidance. Through two phases - autonomous constraint\ngeneration using embedded domain knowledge, followed by iterative multi-agent\noptimization - the framework eliminates the need for predefined operational\nbounds. Validated on the hydrodealkylation process across cost, yield, and\nyield-to-cost ratio metrics, the framework demonstrated competitive performance\nwith conventional optimization methods while achieving better computational\nefficiency, requiring fewer iterations to converge. Our approach converged in\nunder 20 minutes, achieving a 31-fold speedup over grid search. Beyond\ncomputational efficiency, the framework's reasoning-guided search demonstrates\nsophisticated process understanding, correctly identifying utility trade-offs,\nand applying domain-informed heuristics. This approach shows significant\npotential for optimization scenarios where operational constraints are poorly\ncharacterized or unavailable, particularly for emerging processes and retrofit\napplications.", "comment": "16 pages (main manuscript without references), 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20921v1", "categories": ["cs.LG", "cs.AI", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20921v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LLM引导的多智能体化学过程优化", "tldr": "本文提出一个基于大型语言模型（LLM）的多智能体框架，用于在操作约束不明确或缺失的情况下自动推断约束并引导化学过程优化，该方法在计算效率和理解复杂过程方面表现出色。", "motivation": "传统的化学过程优化方法（如梯度求解器、进化算法和参数网格搜索）在操作约束定义不清或缺失时变得不切实际，导致工程师需要依赖主观启发式方法来估计可行参数范围。本文旨在解决这一约束定义瓶颈。", "method": "本文提出了一个基于大型语言模型（LLM）的多智能体框架。该框架基于AutoGen，并使用OpenAI的o3模型，包含专门用于约束生成、参数验证、模拟执行和优化引导的智能体。该方法分两个阶段：首先是利用嵌入式领域知识自主生成约束，然后是迭代的多智能体优化。此框架无需预定义的操作界限。", "result": "该框架在加氢脱烷基过程的成本、产率和产率成本比指标上进行了验证，结果表明其性能与传统优化方法相当，同时计算效率更高，收敛所需的迭代次数更少。该方法在20分钟内收敛，比网格搜索快31倍。此外，该框架的推理引导搜索展现了复杂的流程理解能力，能够正确识别效用权衡并应用领域启发式方法。", "conclusion": "该方法在操作约束特征不明确或不可用的优化场景中，特别是对于新兴流程和改造应用，显示出巨大的潜力。", "translation": "化学过程优化对于最大限度地提高生产效率和经济效益至关重要。传统的优化方法，包括基于梯度的求解器、进化算法和参数网格搜索，在操作约束定义不清或不可用时变得不切实际，需要工程师依赖主观启发式方法来估计可行的参数范围。为了解决这个约束定义瓶颈，我们提出了一个由大型语言模型（LLM）智能体组成的多智能体框架，该框架能够从最少的流程描述中自主推断操作约束，然后协同引导优化。我们基于AutoGen的智能体框架采用了OpenAI的o3模型，并设有专门用于约束生成、参数验证、模拟执行和优化引导的智能体。通过两个阶段——利用嵌入式领域知识自主生成约束，随后进行迭代的多智能体优化——该框架消除了对预定义操作边界的需求。在加氢脱烷基过程的成本、产率和产率成本比指标上进行验证后，该框架展示了与传统优化方法相当的性能，同时实现了更好的计算效率，所需的迭代次数更少。我们的方法在20分钟内收敛，比网格搜索快31倍。除了计算效率，该框架的推理引导搜索还展示了复杂的流程理解能力，能够正确识别效用权衡并应用领域启发式方法。这种方法在操作约束特征不明确或不可用的优化场景中，特别是对于新兴流程和改造应用，显示出巨大的潜力。", "summary": "本文提出了一种LLM引导的多智能体框架，旨在解决化学过程优化中操作约束定义不清的问题。该框架利用LLM智能体自主推断约束，并通过多智能体协作引导优化。该方法在加氢脱烷基过程上进行了验证，结果显示其在计算效率和对复杂过程的理解方面均优于传统方法，尤其适用于约束条件缺失或不明确的场景。", "keywords": "LLM, 多智能体, 化学过程优化, 约束推断, AutoGen", "comments": "这项研究通过引入LLM引导的多智能体方法来解决化学过程优化中长期存在的约束定义挑战，具有显著的创新性。其能够从少量信息中自主推断操作约束，并结合领域知识进行优化，这对于新兴和改造过程尤其重要。该方法在计算效率上的显著提升，以及对复杂过程的理解能力，预示着其在工业应用中的巨大潜力。"}}
{"id": "2506.20746", "title": "Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers", "authors": ["Todd Nief", "David Reber", "Sean Richardson", "Ari Holtzman"], "summary": "When an LLM learns a relation during finetuning (e.g., new movie releases,\ncorporate mergers, etc.), where does this information go? Is it extracted when\nthe model processes an entity, recalled just-in-time before a prediction, or\nare there multiple separate heuristics? Existing localization approaches (e.g.\nactivation patching) are ill-suited for this analysis because they tend to\nreplace parts of the residual stream, potentially deleting information. To fill\nthis gap, we propose dynamic weight-grafting between fine-tuned and pre-trained\nlanguage models to show that fine-tuned language models both (1) extract\nrelation information learned during finetuning while processing entities and\n(2) ``recall\" this information in later layers while generating predictions. In\nsome cases, models need both of these pathways to correctly generate finetuned\ninformation while, in other cases, a single ``enrichment\" or ``recall\" pathway\nalone is sufficient. We examine the necessity and sufficiency of these\ninformation pathways, examining what layers they occur at, how much redundancy\nthey exhibit, and which model components are involved -- finding that the\n``recall\" pathway occurs via both task-specific attention mechanisms and a\nrelation extraction step in the output of the attention and the feedforward\nnetworks at the final layers before next token prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20746v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20746v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "多重关系抽取流：Transformer中的丰富与回忆", "tldr": "本文通过动态权重嫁接方法研究了大型语言模型（LLM）在微调过程中学习到的关系信息如何在模型内部被提取和回忆，发现信息通过“丰富”和“回忆”两种路径传递。", "motivation": "现有定位方法（如激活修补）不适用于分析大型语言模型（LLM）在微调过程中学习到的关系信息在模型内部的去向，因为它们可能会删除信息。为了填补这一空白，本文旨在探究关系信息在LLM中是如何被提取和回忆的。", "method": "本文提出了一种在微调模型和预训练模型之间进行动态权重嫁接的方法。通过这种方法，研究者分析了信息流的必要性和充分性，以及它们发生的层级、冗余程度和涉及的模型组件。", "result": "研究发现，微调后的语言模型在处理实体时会提取在微调过程中学到的关系信息，并在稍后的层中在生成预测时“回忆”这些信息。在某些情况下，模型需要这两种路径才能正确生成微调信息，而在其他情况下，单个“丰富”或“回忆”路径就足够了。“回忆”路径通过任务特定的注意力机制以及在预测前最终层中注意力输出和前馈网络的中的关系抽取步骤发生。", "conclusion": "LLM在微调过程中学习到的关系信息通过“丰富”（提取）和“回忆”两种途径在模型内部传递，并且这两种路径在不同情况下可能单独或协同作用，以确保信息的正确生成。", "translation": "当大型语言模型（LLM）在微调过程中学习到一种关系（例如，新电影上映、公司合并等）时，这些信息去向何处？是在模型处理实体时被提取，还是在预测前及时被回忆，或者存在多个独立的启发式方法？现有的定位方法（例如激活修补）不适合这种分析，因为它们倾向于替换残差流的一部分，可能会删除信息。为了填补这一空白，我们提出了微调和预训练语言模型之间的动态权重嫁接方法，以表明微调后的语言模型（1）在处理实体时提取在微调过程中学到的关系信息，并且（2）在生成预测时在后续层中“回忆”这些信息。在某些情况下，模型需要这两种途径才能正确生成微调信息，而在其他情况下，单个“丰富”或“回忆”途径就足够了。我们检查了这些信息途径的必要性和充分性，检查了它们发生的层级，它们表现出的冗余程度，以及涉及的模型组件——发现“回忆”途径通过任务特定的注意力机制以及在下一个词元预测前的最终层中注意力输出和前馈网络中的关系抽取步骤发生。", "summary": "本文研究了大型语言模型（LLM）在微调期间学习到的关系信息如何在模型内部被处理。通过引入一种动态权重嫁接方法，作者发现LLM在处理实体时会“提取”关系信息，并在生成预测时“回忆”这些信息。研究表明，“丰富”（提取）和“回忆”这两种信息路径都对正确生成微调信息至关重要，并且在不同情境下可能单独或协同作用。论文进一步探讨了这些路径在模型层级中的发生位置、冗余性以及涉及的模型组件，揭示了“回忆”路径通过注意力机制和前馈网络在最终层发挥作用。", "keywords": "关系抽取, Transformer, 大型语言模型, 微调, 信息路径", "comments": "本文通过提出创新的“动态权重嫁接”方法，弥补了现有信息定位方法在分析LLM内部信息流向方面的不足。这一方法不仅揭示了LLM在处理关系信息时存在“提取”和“回忆”两种路径，还详细分析了这些路径在模型中的具体作用机制和位置，对于理解Transformer模型如何存储和利用知识具有重要意义。其发现对于改进模型微调策略和提高模型可解释性具有潜在价值。"}}
{"id": "2506.21186", "title": "Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout", "authors": ["Apurva Shah", "Axel Abels", "Ann Nowé", "Tom Lenaerts"], "summary": "Perpetual voting addresses fairness in sequential collective decision-making\nby evaluating representational equity over time. However, existing perpetual\nvoting rules rely on full participation and complete approval information,\nassumptions that rarely hold in practice, where partial turnout is the norm. In\nthis work, we study the integration of Artificial Delegates,\npreference-learning agents trained to represent absent voters, into perpetual\nvoting systems. We examine how absenteeism affects fairness and\nrepresentativeness under various voting methods and evaluate the extent to\nwhich Artificial Delegates can compensate for missing participation. Our\nfindings indicate that while absenteeism significantly affects fairness,\nArtificial Delegates reliably mitigate these effects and enhance robustness\nacross diverse scenarios.", "comment": "The paper has been accepted at the ACM Collective Intelligence\n  Conference (CI 2025), August 4 to 6, 2025, San Diego, CA, USA", "pdf_url": "http://arxiv.org/pdf/2506.21186v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21186v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人工代理解决部分投票率下永久投票中的公平性问题", "tldr": "本文研究了将人工代理（代表缺席选民的偏好学习智能体）整合到永久投票系统中，以解决部分投票率导致的公平性问题，结果表明人工代理能有效缓解缺席对公平性的影响并增强鲁棒性。", "motivation": "现有的永久投票规则依赖于完全参与和完整的批准信息，这在实际中很少发生，因为部分投票率是常态，导致公平性问题。", "method": "作者研究了将人工代理（经过训练以代表缺席选民的偏好学习智能体）整合到永久投票系统中。他们检查了缺席对各种投票方法下的公平性和代表性的影响，并评估了人工代理在多大程度上可以弥补缺失的参与。", "result": "研究结果表明，虽然缺席显著影响公平性，但人工代理能可靠地缓解这些影响，并在不同场景下增强系统的鲁棒性。", "conclusion": "人工代理能够有效解决永久投票中因部分投票率引起的公平性问题，提高系统的鲁棒性。", "translation": "永久投票通过评估随时间变化的代表性公平性来解决顺序集体决策中的公平性问题。然而，现有的永久投票规则依赖于完全参与和完整的批准信息，这些假设在实践中很少成立，因为部分投票率是常态。在这项工作中，我们研究了将人工代理（经过训练以代表缺席选民的偏好学习智能体）整合到永久投票系统中。我们检查了缺席对各种投票方法下的公平性和代表性的影响，并评估了人工代理在多大程度上可以弥补缺失的参与。我们的研究结果表明，虽然缺席显著影响公平性，但人工代理能可靠地缓解这些影响，并在不同场景下增强鲁棒性。", "summary": "本文研究了在永久投票系统中整合人工代理（代表缺席选民的偏好学习智能体），以解决实际中普遍存在的部分投票率导致的公平性问题。研究发现，缺席会对公平性产生显著影响，而人工代理能够有效地缓解这些负面影响，并在各种场景下增强系统的稳健性。", "keywords": "永久投票, 公平性, 部分投票率, 人工代理, 偏好学习", "comments": "这项工作通过引入“人工代理”来解决现实世界中集体决策系统面临的实际问题，即部分投票率。其创新点在于利用机器学习代理来模拟缺席选民的偏好，从而提高投票系统的公平性和鲁棒性，具有重要的实践意义。"}}
{"id": "2506.21510", "title": "Joint Scheduling of DER under Demand Charges: Structure and Approximation", "authors": ["Ruixiao Yang", "Gulai Shen", "Ahmed S. Alahmed", "Chuchu Fan"], "summary": "We study the joint scheduling of behind-the-meter distributed energy\nresources (DERs), including flexible loads, renewable generation, and battery\nenergy storage systems, under net energy metering frameworks with demand\ncharges. The problem is formulated as a stochastic dynamic program aimed at\nmaximizing expected operational surplus while accounting for renewable\ngeneration uncertainty. We analytically characterize the structure of the\noptimal control policy and show that it admits a threshold-based form. However,\ndue to the strong temporal coupling of the storage and demand charge\nconstraints, the number of conditional branches in the policy scales\ncombinatorially with the scheduling horizon, as it requires a look-ahead over\nfuture states. To overcome the high computational complexity in the general\nformulation, an efficient approximation algorithm is proposed, which searches\nfor the peak demand under a mildly relaxed problem. We show that the algorithm\nscales linearly with the scheduling horizon. Extensive simulations using two\nopen-source datasets validate the proposed algorithm and compare its\nperformance against different DER control strategies, including a reinforcement\nlearning-based one. Under varying storage and tariff parameters, the results\nshow that the proposed algorithm outperforms various benchmarks in achieving a\nrelatively small solution gap compared to the theoretical upper bound.", "comment": "15 pages, 4 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21510v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21510v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "需求费用下分布式能源的联合调度：结构与近似", "tldr": "本文研究了在需求费用下对分布式能源（DERs）进行联合调度的问题，提出了一个基于阈值的最优控制策略，并针对其高计算复杂性开发了一种高效的近似算法，通过模拟验证了该算法的优越性。", "motivation": "在具有需求费用的净计量框架下，对包括灵活负荷、可再生能源发电和电池储能系统在内的电表后分布式能源进行联合调度，以最大化预期运营盈余，同时考虑到可再生能源发电的不确定性。由于储能和需求费用约束的强时间耦合，导致最优控制策略的计算复杂度很高，需要克服这一挑战。", "method": "将问题表述为随机动态规划，旨在最大化预期运营盈余。分析性地描述了最优控制策略的结构，表明其具有基于阈值的形式。为解决高计算复杂性，提出了一种高效的近似算法，该算法在轻微放宽的问题下搜索峰值需求，并展示其计算复杂度与调度周期呈线性关系。", "result": "提出的近似算法的计算复杂度与调度周期呈线性关系。通过使用两个开源数据集进行的广泛模拟，验证了该算法的有效性，并显示其在不同的储能和费率参数下，相比于理论上限，能够实现相对较小的解决方案差距，性能优于包括强化学习在内的多种基准DER控制策略。", "conclusion": "论文成功地提出了一个高效的近似算法来解决需求费用下分布式能源的联合调度问题，克服了最优控制策略因强时间耦合导致的高计算复杂性，并在广泛模拟中证明了其在性能上优于现有基准方法。", "translation": "我们研究了在具有需求费用的净计量框架下，电表后分布式能源（DERs）的联合调度，包括柔性负荷、可再生能源发电和电池储能系统。该问题被表述为一个随机动态规划，旨在最大化预期运营盈余，同时考虑可再生能源发电的不确定性。我们分析性地描述了最优控制策略的结构，并表明它具有基于阈值的形式。然而，由于储能和需求费用约束的强时间耦合，策略中的条件分支数量随调度周期呈组合式增长，因为它需要对未来状态进行展望。为了克服通用公式中的高计算复杂性，提出了一种高效的近似算法，该算法在轻微放宽的问题下搜索峰值需求。我们表明该算法的计算复杂度与调度周期呈线性关系。使用两个开源数据集进行的广泛模拟验证了所提出的算法，并将其性能与不同的DER控制策略（包括基于强化学习的策略）进行了比较。在不同的储能和费率参数下，结果表明，与理论上限相比，所提出的算法在实现相对较小的解决方案差距方面优于各种基准。", "summary": "本文研究了在需求费用下，包括灵活负荷、可再生能源和电池储能系统在内的分布式能源的联合调度问题。通过将其建模为随机动态规划，作者分析了最优控制策略的阈值结构。鉴于该策略因强时间耦合导致的高计算复杂性，论文提出了一种高效的近似算法，该算法在轻微放宽的问题下搜索峰值需求，并展示了其计算复杂度与调度周期呈线性关系。通过广泛的模拟，该算法在性能上超越了多种现有策略，并实现了接近理论最优的解决方案。", "keywords": "分布式能源调度, 需求费用, 随机动态规划, 近似算法, 电池储能", "comments": "本文创新性地解决了在需求费用下分布式能源联合调度的计算复杂性问题，通过提出一种高效的近似算法，使其在实际应用中更具可行性。其对最优控制策略结构的分析也具有重要的理论价值。该方法在模拟中表现出色，为未来的能源管理提供了有益的参考。"}}
{"id": "2506.20850", "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision", "authors": ["Yuting He", "Shuo Li"], "summary": "Contrastive learning (CL) has become a cornerstone of self-supervised\npretraining (SSP) in foundation models, however, extending CL to pixel-wise\nrepresentation, crucial for medical vision, remains an open problem. Standard\nCL formulates SSP as a binary optimization problem (binary CL) where the\nexcessive pursuit of feature dispersion leads to an over-dispersion problem,\nbreaking pixel-wise feature correlation thus disrupting the intra-class\ndistribution. Our vector CL reformulates CL as a vector regression problem,\nenabling dispersion quantification in pixel-wise pretraining via modeling\nfeature distances in regressing displacement vectors. To implement this novel\nparadigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER\nestablishes an extendable vector-based self-learning, enforces a consistent\noptimization flow from vector regression to distance modeling, and leverages a\nvector pyramid architecture for granularity adaptation, thus preserving\npixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,\nspanning 2 dimensions and 4 modalities, show that COVER significantly improves\npixel-wise SSP, advancing generalizable medical visual foundation models.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20850v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20850v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "用于医学视觉中像素级预训练的向量对比学习", "tldr": "向量对比学习（COVER）通过解决标准对比学习中的过度分散问题，显著改进了医学视觉中的像素级预训练。", "motivation": "将对比学习扩展到像素级表示对于医学视觉至关重要，但仍然是一个开放问题。标准对比学习存在过度分散问题，破坏了像素级特征相关性并扰乱了类内分布。", "method": "该研究将对比学习重新表述为向量回归问题，通过建模位移向量中的特征距离来实现像素级预训练中的分散量化。为此，提出了COntrast in VEctor Regression (COVER)框架。COVER建立可扩展的基于向量的自学习，强制从向量回归到距离建模的一致优化流，并利用向量金字塔架构进行粒度适应，从而在自监督预训练中保留像素级特征相关性。", "result": "在涵盖2个维度和4种模态的8项任务中进行的广泛实验表明，COVER显著改进了像素级自监督预训练，推动了可泛化的医学视觉基础模型发展。", "conclusion": "COVER显著改进了像素级自监督预训练，并推动了可泛化的医学视觉基础模型的发展。", "translation": "对比学习（CL）已成为基础模型中自监督预训练（SSP）的基石，然而，将CL扩展到像素级表示（这对医学视觉至关重要）仍然是一个开放问题。标准CL将SSP表述为二元优化问题（二元CL），其中过度追求特征分散导致过度分散问题，破坏了像素级特征相关性，从而扰乱了类内分布。我们的向量CL将CL重新表述为向量回归问题，通过在回归位移向量中建模特征距离，从而实现像素级预训练中的分散量化。为了实现这一新范式，我们提出了COntrast in VEctor Regression (COVER)框架。COVER建立了可扩展的基于向量的自学习，强制从向量回归到距离建模的一致优化流，并利用向量金字塔架构进行粒度适应，从而在SSP中保留像素级特征相关性。在涵盖2个维度和4种模态的8项任务中进行的广泛实验表明，COVER显著改进了像素级SSP，推动了可泛化的医学视觉基础模型发展。", "summary": "标准对比学习在医学视觉的像素级预训练中存在过度分散问题。该研究提出向量对比学习，将其重构为向量回归问题，并引入COVER框架，通过建立向量自学习、优化流和向量金字塔架构来保留像素级特征相关性。实验证明COVER在多任务和多模态上显著提升了像素级自监督预训练效果。", "keywords": "对比学习, 像素级预训练, 医学视觉, 自监督预训练, 向量回归", "comments": "该论文的创新点在于将对比学习重新表述为向量回归问题，并提出了COVER框架来解决标准对比学习中像素级表示的过度分散问题。这对于医学视觉中像素级预训练至关重要，有助于推动可泛化的医学视觉基础模型的发展。"}}
{"id": "2506.21329", "title": "Active Inference AI Systems for Scientific Discovery", "authors": ["Karthik Duraisamy"], "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21329v1", "categories": ["cs.AI", "physics.soc-ph", "68", "I.2"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21329v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于科学发现的主动推理人工智能系统", "tldr": "本文提出了一种主动推理AI系统架构，旨在通过弥合抽象、推理和现实差距，克服当前AI在科学发现中的局限性，强调内部模型与外部验证的结合以及人类判断的不可或缺性。", "motivation": "当前人工智能系统在科学发现方面受到其操作架构、脆弱的推理机制以及与实验现实脱节的根本性限制。作者认为，AI驱动的科学进步取决于弥合抽象、推理和现实这三个基本差距。", "method": "本文定义了用于科学发现的主动推理AI系统，其特点包括：(i) 维护基于因果自监督基础模型的长期研究记忆；(ii) 配备贝叶斯护栏的符号或神经符号规划器；(iii) 发展持久的知识图谱，通过思考生成新概念节点，推理建立因果边缘，并通过真实世界交互修正和强化连接；(iv) 通过与高保真模拟器和自动化实验室的闭环交互来完善内部表示。核心在于内部模型（实现反事实推理）与外部验证（将假设根植于现实）的相互作用。", "result": "Not mentioned in abstract", "conclusion": "科学发现源于使反事实推理成为可能内部模型与将假设建立在现实中的外部验证之间的相互作用。此外，模拟和实验反馈中固有的模糊性以及潜在的不确定性使得人类判断不可或缺，它不是一个临时支架，而是一个永久的架构组成部分。", "translation": "人工智能的快速发展引发了对变革性科学发现的期望，然而，当前的系统仍然受到其操作架构、脆弱的推理机制以及与实验现实脱节的根本性限制。在前人工作的基础上，我们认为，AI驱动的科学进步现在取决于弥合三个基本差距——抽象差距、推理差距和现实差距——而不是模型规模/数据/测试时间计算。科学推理需要支持行动和响应模拟的内部表示、区分相关性与机制的因果结构以及持续校准。我们将用于科学发现的主动推理人工智能系统定义为：(i) 维护基于因果自监督基础模型的长期研究记忆；(ii) 配备贝叶斯护栏的符号或神经符号规划器；(iii) 发展持久的知识图谱，其中思考生成新概念节点，推理建立因果边缘，真实世界交互修剪错误连接同时强化已验证的路径；(iv) 通过与高保真模拟器和自动化实验室的闭环交互来完善其内部表示——这是一个操作循环，其中心理模拟指导行动，经验惊喜重塑理解。本质上，我们概述了一种架构，其中发现产生于能够进行反事实推理的内部模型与将假设建立在现实中的外部验证之间的相互作用。文章还认为，模拟和实验反馈中固有的模糊性以及潜在的不确定性使得人类判断不可或缺，它不是一个临时支架，而是一个永久的架构组成部分。", "summary": "本文提出了一种用于科学发现的主动推理AI系统架构，旨在解决当前AI在科学探索中面临的抽象、推理和现实差距。该系统整合了基于因果自监督模型的长期记忆、带贝叶斯约束的规划器、动态增长的知识图谱以及与模拟器和自动化实验室的闭环交互，强调通过内部反事实推理模型与外部真实验证的协同作用来促进发现。论文还特别指出，鉴于反馈的模糊性和不确定性，人类判断应作为AI系统不可或缺的永久组成部分。", "keywords": "主动推理, 科学发现, 人工智能系统, 因果模型, 知识图谱", "comments": "该论文提出了一种创新且全面的AI架构，旨在克服当前AI在科学发现中的核心限制。其亮点在于强调了弥合“抽象、推理和现实”三大差距，并通过整合因果模型、神经符号方法、知识图谱和闭环实验反馈等多种先进AI范式，构建了一个自我完善的系统。特别值得注意的是，论文将人类判断提升为AI系统中的永久架构组件，而非临时辅助，这对于未来人机协作的科学研究具有重要指导意义，体现了对AI局限性的深刻理解和对复杂科学问题的务实态度。"}}
{"id": "2506.21499", "title": "Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising", "authors": ["Hojat Asgariandehkordi", "Mostafa Sharifzadeh", "Hassan Rivaz"], "summary": "Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by\ncombining echoes from multiple steered transmissions. While increasing the\nnumber of angles generally improves image quality, it drastically reduces the\nframe rate and can introduce blurring artifacts in fast-moving targets.\nMoreover, compounded images remain susceptible to noise, particularly when\nacquired with a limited number of transmissions. We propose a zero-shot\ndenoising framework tailored for low-angle CPWC acquisitions, which enhances\ncontrast without relying on a separate training dataset. The method divides the\navailable transmission angles into two disjoint subsets, each used to form\ncompound images that include higher noise levels. The new compounded images are\nthen used to train a deep model via a self-supervised residual learning scheme,\nenabling it to suppress incoherent noise while preserving anatomical\nstructures. Because angle-dependent artifacts vary between the subsets while\nthe underlying tissue response is similar, this physics-informed pairing allows\nthe network to learn to disentangle the inconsistent artifacts from the\nconsistent tissue signal. Unlike supervised methods, our model requires no\ndomain-specific fine-tuning or paired data, making it adaptable across\nanatomical regions and acquisition setups. The entire pipeline supports\nefficient training with low computational cost due to the use of a lightweight\narchitecture, which comprises only two convolutional layers. Evaluations on\nsimulation, phantom, and in vivo data demonstrate superior contrast enhancement\nand structure preservation compared to both classical and deep learning-based\ndenoising methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21499v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21499v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "轻量级物理信息零样本超声平面波去噪", "tldr": "提出了一种轻量级、物理信息引导的零样本超声去噪框架，通过自监督学习在不依赖额外训练数据的情况下，有效提升低角度CPWC图像的对比度并保留结构。", "motivation": "超声相干平面波复合（CPWC）虽能增强图像对比度，但增加角度会降低帧率并引入运动伪影；在传输次数有限的情况下，复合图像易受噪声影响。", "method": "该方法针对低角度CPWC采集，将可用传输角度分为两个不相交的子集，每个子集用于形成包含更高噪声的复合图像。然后，这些新的复合图像通过自监督残差学习方案训练一个深度模型，使其能够抑制非相干噪声并保留解剖结构。利用子集间角度依赖伪影不同而底层组织响应相似的物理信息配对，使网络能够区分不一致的伪影和一致的组织信号。模型采用轻量级架构（仅包含两个卷积层），无需领域特定微调或配对数据。", "result": "在仿真、体模和体内数据上的评估表明，与经典和基于深度学习的去噪方法相比，该方法在对比度增强和结构保留方面表现出卓越的性能。", "conclusion": "该零样本去噪框架通过利用物理信息和自监督学习，为低角度CPWC超声图像去噪提供了一种高效、自适应且计算成本低廉的解决方案，显著提升了图像质量。", "translation": "超声相干平面波复合（CPWC）通过结合来自多个转向传输的回波来增强图像对比度。虽然增加角度数量通常能改善图像质量，但这会大幅降低帧率并可能在快速移动目标中引入模糊伪影。此外，复合图像仍然容易受到噪声影响，尤其是在传输次数有限的情况下采集时。我们提出了一种专为低角度CPWC采集量身定制的零样本去噪框架，该框架无需单独的训练数据集即可增强对比度。该方法将可用的传输角度分为两个不相交的子集，每个子集用于形成包含更高噪声水平的复合图像。然后，这些新的复合图像通过自监督残差学习方案训练一个深度模型，使其能够抑制非相干噪声，同时保留解剖结构。由于角度依赖性伪影在子集之间有所不同，而底层组织响应相似，这种物理信息引导的配对使得网络能够学习将不一致的伪影与一致的组织信号分离。与监督方法不同，我们的模型无需领域特定的微调或配对数据，使其能够适应不同的解剖区域和采集设置。由于采用了轻量级架构（仅包含两个卷积层），整个流程支持高效训练，计算成本低。在仿真、体模和体内数据上的评估表明，与经典和基于深度学习的去噪方法相比，该方法在对比度增强和结构保留方面表现出卓越的性能。", "summary": "本研究提出了一种轻量级、物理信息引导的零样本去噪框架，专为低角度超声相干平面波复合（CPWC）图像设计。该方法通过将传输角度分为两个子集，并利用它们形成的带噪图像进行自监督残差学习，从而训练一个仅包含两个卷积层的深度模型。这种方法无需外部训练数据或领域特定微调，能够有效抑制非相干噪声，同时保留解剖结构。实验结果表明，该框架在对比度增强和结构保留方面优于现有去噪方法。", "keywords": "超声去噪, 零样本学习, 相干平面波复合, 自监督学习, 物理信息引导", "comments": "该论文的创新点在于提出了一个零样本、物理信息引导的自监督去噪框架，解决了传统监督学习方法对大量标记数据依赖的问题。其轻量级架构和无需微调的特性使其在实际超声应用中具有很高的普适性和效率。通过巧妙地利用图像物理特性来生成训练数据，避免了采集昂贵且耗时的配对数据集。"}}
{"id": "2506.21112", "title": "Point Cloud Environment-Based Channel Knowledge Map Construction", "authors": ["Yancheng Wang", "Wei Guo", "Guanying Chen", "Ye Zhang", "Shuguang Cui"], "summary": "Channel knowledge map (CKM) provides certain levels of channel state\ninformation (CSI) for an area of interest, serving as a critical enabler for\nenvironment-aware communications by reducing the overhead of frequent CSI\nacquisition. However, existing CKM construction schemes adopt over-simplified\nenvironment information, which significantly compromises their accuracy. To\naddress this issue, this work proposes a joint model- and data-driven approach\nto construct CKM by leveraging point cloud environmental data along with a few\nsamples of location-tagged channel information. First, we propose a novel point\nselector to identify subsets of point cloud that contain environmental\ninformation relevant to multipath channel gains, by constructing a set of\nco-focal ellipsoids based on different time of arrival (ToAs). Then, we trained\na neural channel gain estimator to learn the mapping between each selected\nsubset and its corresponding channel gain, using a real-world dataset we\ncollected through field measurements, comprising environmental point clouds and\ncorresponding channel data. Finally, experimental results demonstrate that: For\nCKM construction of power delay profile (PDP), the proposed method achieves a\nroot mean squared error (RMSE) of 2.95 dB, significantly lower than the 7.32 dB\nachieved by the conventional ray-tracing method; for CKM construction of\nreceived power values, i.e., radio map, it achieves an RMSE of 1.04 dB,\nsurpassing the Kriging interpolation method with an RMSE of 1.68 dB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21112v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21112v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于点云环境的信道知识图谱构建", "tldr": "本文提出了一种结合模型和数据驱动的方法，利用点云环境数据和少量信道信息样本来构建信道知识图谱，显著提高了精度。", "motivation": "现有的信道知识图谱（CKM）构建方案采用过于简化的环境信息，这严重影响了其准确性，导致频繁获取信道状态信息（CSI）的开销较大。", "method": "本文提出了一种结合模型和数据驱动的方法来构建信道知识图谱（CKM）。首先，设计了一种点选择器，通过构建基于不同到达时间（ToA）的同焦点椭球体，识别包含与多径信道增益相关的环境信息的点云子集。然后，利用实地测量收集的包含环境点云和相应信道数据的真实世界数据集，训练了一个神经网络信道增益估计器，以学习每个选定子集与其对应信道增益之间的映射。", "result": "对于功率延迟剖面（PDP）的CKM构建，所提方法的均方根误差（RMSE）为2.95 dB，显著低于传统射线追踪方法的7.32 dB。对于接收功率值（即无线电地图）的CKM构建，其RMSE为1.04 dB，优于克里金插值方法的1.68 dB。", "conclusion": "结合模型和数据驱动的方法，利用点云环境数据构建信道知识图谱，能够显著提高信道知识图谱的构建精度，优于传统的射线追踪和克里金插值方法。", "translation": "信道知识图谱（CKM）为感兴趣区域提供一定水平的信道状态信息（CSI），通过减少频繁CSI获取的开销，成为环境感知通信的关键使能技术。然而，现有的CKM构建方案采用过于简化的环境信息，这显著影响了它们的准确性。为了解决这个问题，本文提出了一种结合模型和数据驱动的方法，通过利用点云环境数据以及少量位置标记的信道信息样本来构建CKM。首先，我们提出了一种新颖的点选择器，通过构建一组基于不同到达时间（ToA）的同焦点椭球体，来识别包含与多径信道增益相关的环境信息的点云子集。然后，我们利用通过实地测量收集的真实世界数据集（包含环境点云和相应的信道数据），训练了一个神经网络信道增益估计器，以学习每个选定子集与其对应信道增益之间的映射。最后，实验结果表明：对于功率延迟剖面（PDP）的CKM构建，所提方法的均方根误差（RMSE）为2.95 dB，显著低于传统射线追踪方法实现的7.32 dB；对于接收功率值（即无线电地图）的CKM构建，其RMSE为1.04 dB，超越了克里金插值方法的1.68 dB。", "summary": "本文提出了一种新颖的结合模型和数据驱动的信道知识图谱（CKM）构建方法，旨在解决现有方法因环境信息简化导致精度不足的问题。该方法利用点云环境数据和少量位置标记的信道信息。核心在于引入一个点选择器，通过构建同焦点椭球体来筛选与多径信道增益相关的点云子集，随后训练一个神经网络估计器学习点云与信道增益的映射。实验结果表明，该方法在功率延迟剖面CKM构建和无线电地图构建方面均显著优于传统方法，分别实现了2.95 dB和1.04 dB的RMSE。", "keywords": "信道知识图谱, 点云, 环境感知通信, 深度学习, 信道状态信息", "comments": "本文的创新点在于将点云环境数据引入信道知识图谱的构建中，并结合了模型驱动的点选择与数据驱动的神经网络估计，有效地提升了CKM的精度。这种方法为环境感知通信提供了一种更准确的CSI获取手段，对于未来无线通信系统的设计具有重要意义。"}}
{"id": "2506.21456", "title": "Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "summary": "Previous work has demonstrated the utility of reductions in the level of\ndetail (LOD) in the periphery of head-tracked, large field of view displays.\nThis paper provides a psychophysically based model, centered around an eye/head\nmovement tradeoff, that explains the effectiveness of peripheral degradation\nand suggests how peripherally degraded displays should be designed. An\nexperiment evaluating the effect on search performance of the shape and area of\nthe high detail central area (inset) in peripherally degraded displays was\nperformed, results indicated that inset shape is not a significant factor in\nperformance. Inset area, however, was significant: performance with displays\nsubtending at least 30 degrees of horizontal and vertical angle was not\nsignificantly different from performance with an undegraded display. These\nresults agreed with the proposed model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21456v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21456v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过头部追踪的周边降级管理细节水平：一个模型和设计原则", "tldr": "本文提出了一个基于眼/头运动权衡的心理物理学模型，解释了头部追踪大视场显示器中周边细节降级的有效性，并通过实验验证了模型，并提出了设计原则。", "motivation": "先前工作表明在头部追踪的大视场显示器中，降低周边细节水平（LOD）是有用的，但需要一个解释其有效性并指导设计的模型。", "method": "本文提出了一个基于眼/头运动权衡的心理物理学模型，解释周边降级的有效性并指导设计。此外，还进行了一项实验，评估了周边降级显示器中高细节中心区域（内嵌区域）的形状和面积对搜索性能的影响。", "result": "实验结果表明，内嵌区域的形状对性能没有显著影响。然而，内嵌区域的面积是显著因素：当显示器内嵌区域的水平和垂直视角至少为30度时，其性能与未降级显示器的性能没有显著差异。这些结果与提出的模型一致。", "conclusion": "本文提出的心理物理学模型能够解释周边细节降级的有效性，并且实验结果验证了该模型，为周边降级显示器的设计提供了指导原则。", "translation": "先前的工作已经证明了在头部追踪的大视场显示器周边区域中降低细节水平（LOD）的实用性。本文提供了一个基于心理物理学的模型，该模型以眼/头运动权衡为中心，解释了周边降级的有效性，并提出了周边降级显示器应如何设计。进行了一项实验，评估了周边降级显示器中高细节中心区域（内嵌区域）的形状和面积对搜索性能的影响。结果表明，内嵌区域的形状对性能不是一个显著因素。然而，内嵌区域的面积是显著的：当显示器内嵌区域的水平和垂直视角至少为30度时，其性能与未降级显示器的性能没有显著差异。这些结果与所提出的模型一致。", "summary": "本文提出了一个基于眼/头运动权衡的心理物理学模型，以解释头部追踪大视场显示器中周边细节降级的有效性，并为设计周边降级显示器提供指导。通过实验评估内嵌区域形状和面积对搜索性能的影响，发现形状不重要，而内嵌区域面积至少达到30度视角时，性能与未降级显示器相当，验证了模型的有效性。", "keywords": "细节水平管理, 周边降级, 头部追踪显示器, 心理物理学模型, 搜索性能", "comments": "这项研究通过建立心理物理学模型并进行实验验证，为头部追踪大视场显示器中高效管理细节水平提供了理论基础和实用的设计原则。其创新点在于将眼/头运动权衡纳入模型，并明确了高细节区域的最小有效尺寸，这对于优化虚拟现实和增强现实等应用的用户体验和系统性能具有重要意义。"}}
{"id": "2506.21216", "title": "Edge Clique Partition and Cover Beyond Independence", "authors": ["Fedor V. Fomin", "Petr A. Golovach", "Danil Sagunov", "Kirill Simonov"], "summary": "Covering and partitioning the edges of a graph into cliques are classical\nproblems at the intersection of combinatorial optimization and graph theory,\nhaving been studied through a range of algorithmic and complexity-theoretic\nlenses. Despite the well-known fixed-parameter tractability of these problems\nwhen parameterized by the total number of cliques, such a parameterization\noften fails to be meaningful for sparse graphs. In many real-world instances,\non the other hand, the minimum number of cliques in an edge cover or partition\ncan be very close to the size of a maximum independent set \\alpha(G).\n  Motivated by this observation, we investigate above \\alpha parameterizations\nof the edge clique cover and partition problems. Concretely, we introduce and\nstudy Edge Clique Cover Above Independent Set (ECC/\\alpha) and Edge Clique\nPartition Above Independent Set (ECP/\\alpha), where the goal is to cover or\npartition all edges of a graph using at most \\alpha(G) + k cliques, and k is\nthe parameter. Our main results reveal a distinct complexity landscape for the\ntwo variants. We show that ECP/\\alpha is fixed-parameter tractable, whereas\nECC/\\alpha is NP-complete for all k \\geq 2, yet can be solved in polynomial\ntime for k \\in {0,1}. These findings highlight intriguing differences between\nthe two problems when viewed through the lens of parameterization above a\nnatural lower bound.\n  Finally, we demonstrate that ECC/\\alpha becomes fixed-parameter tractable\nwhen parameterized by k + \\omega(G), where \\omega(G) is the size of a maximum\nclique of the graph G. This result is particularly relevant for sparse graphs,\nin which \\omega is typically small. For H-minor free graphs, we design a\nsubexponential algorithm of running time f(H)^{\\sqrt{k}}n^{O(1)}.", "comment": "An extended abstract of this paper appears in the proceedings of ESA\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21216v1", "categories": ["cs.DS", "cs.DM"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21216v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "边团划分与覆盖超越独立集", "tldr": "本文研究了边团覆盖和划分问题在最大独立集大小之上的参数化，揭示了两种变体不同的复杂性，并展示了在额外参数下边团覆盖问题的固定参数可解性。", "motivation": "经典的边团覆盖和划分问题，当以团的总数作为参数时，对于稀疏图通常不适用。在许多实际情况下，边覆盖或划分中的最小团数可能非常接近最大独立集的大小α(G)。受此观察启发，作者研究了边团覆盖和划分问题在α之上的参数化。", "method": "作者引入并研究了超越独立集的边团覆盖（ECC/α）和超越独立集的边团划分（ECP/α）问题，目标是使用至多α(G) + k个团来覆盖或划分图的所有边，其中k是参数。他们分析了这些变体的复杂性。此外，他们还证明了当以k + ω(G)（其中ω(G)是图G的最大团大小）作为参数时，ECC/α是固定参数可解的，并为H-minor自由图设计了一个次指数算法。", "result": "ECP/α是固定参数可解的。ECC/α对于所有k ≥ 2都是NP完全的，但对于k ∈ {0,1}可以在多项式时间内解决。当以k + ω(G)作为参数时，ECC/α变为固定参数可解。对于H-minor自由图，设计了一个运行时间为f(H)^√k * n^O(1)的次指数算法。", "conclusion": "这些发现突出了在超越自然下界（α(G)）的参数化视角下，边团覆盖和边团划分这两个问题之间存在的显著差异。ECC/α的k + ω(G)参数化结果对于稀疏图尤其重要。", "translation": "将图的边覆盖和划分为团是组合优化和图论交叉领域的经典问题，已经通过一系列算法和复杂性理论视角进行了研究。尽管这些问题在以团的总数作为参数时具有众所周知的固定参数可解性，但这种参数化对于稀疏图通常意义不大。另一方面，在许多实际情况下，边覆盖或划分中的最小团数可能非常接近最大独立集的大小α(G)。\n受此观察启发，我们研究了边团覆盖和划分问题在α之上的参数化。具体来说，我们引入并研究了超越独立集的边团覆盖（ECC/α）和超越独立集的边团划分（ECP/α），其目标是使用至多α(G) + k个团来覆盖或划分图的所有边，其中k是参数。我们的主要结果揭示了两种变体截然不同的复杂性图景。我们证明了ECP/α是固定参数可解的，而ECC/α对于所有k ≥ 2都是NP完全的，但对于k ∈ {0,1}可以在多项式时间内解决。这些发现突出了在超越自然下界（α(G)）的参数化视角下，这两个问题之间存在的引人入胜的差异。\n最后，我们证明了当以k + ω(G)作为参数时，ECC/α变为固定参数可解的，其中ω(G)是图G的最大团的大小。这一结果对于稀疏图尤其重要，因为在稀疏图中ω通常很小。对于H-minor自由图，我们设计了一个运行时间为f(H)^√k * n^O(1)的次指数算法。", "summary": "该论文通过引入一种新的“超越α(G)”（最大独立集大小）的参数化方法，重新审视了经典的边团覆盖（ECC/α）和边团划分（ECP/α）问题。研究发现，ECP/α是固定参数可解的，而ECC/α对于k ≥ 2是NP完全的，但在k ∈ {0,1}时可在多项式时间内解决。此外，论文还指出，当以k + ω(G)（最大团大小）作为参数时，ECC/α变为固定参数可解，这对于稀疏图尤为重要，并为H-minor自由图提供了一个次指数算法。这项工作揭示了在这新参数化视角下两种问题截然不同的复杂性特征。", "keywords": "边团覆盖, 边团划分, 固定参数可解性, 独立集, 图论", "comments": "本文提出了一种新颖的参数化方法，即“超越独立集”，这对于稀疏图比传统的总团数参数化更具意义。它揭示了在这一新视角下，边团覆盖和边团划分问题之间存在显著的复杂性差异，特别是ECC/α的NP完全性与ECP/α的FPT特性形成对比。针对k + ω(G)的参数化结果具有重要的实践价值，尤其适用于ω较小的稀疏图。"}}
{"id": "2506.20920", "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20920v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20920v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FineWeb2: 一个可扩展所有语言的管道——将预训练数据处理适应到每种语言", "tldr": "该论文介绍了FineWeb2，一个用于创建高质量多语言预训练数据集的新数据管道，能够适应1000多种语言，并展示了其能提高模型性能。", "motivation": "尽管高质量英语预训练数据集的开放开发取得了显著进展，但训练高性能多语言大型语言模型（LLM）仍然是一个挑战，主要原因是为大量语言定制过滤和去重管道存在固有的困难。", "method": "该研究引入了一种基于FineWeb的新型预训练数据整理管道，该管道能够自动适应并支持任何语言。通过在一组九种不同语言上广泛消融管道设计选择，并基于可衡量标准选择评估任务，验证了其有效性。此外，还引入了一种直接且有原则的数据集重新平衡方法，该方法同时考虑了重复计数和质量。最终，该管道被扩展到1000多种语言，使用了近100个Common Crawl快照，从而生成了FineWeb2数据集。", "result": "该管道能够创建非英语语料库，这些语料库产生的模型比现有数据集性能更高。所引入的数据集重新平衡方法提供了额外的性能提升。最终生成了FineWeb2，一个20TB（50亿文档）的多语言数据集，并随同管道、训练和评估代码库一起发布。", "conclusion": "该论文成功开发并扩展了一个语言无关的管道FineWeb2，用于创建高质量的多语言预训练数据集，显著提高了跨多种语言的LLM性能，并开源了数据集和相关代码库。", "translation": "预训练最先进的大型语言模型（LLM）需要大量干净多样化的文本数据。虽然高质量英语预训练数据集的开放开发最近取得了实质性进展，但训练高性能多语言LLM仍然是一个挑战，这主要是由于为大量语言定制过滤和去重管道固有的困难。在这项工作中，我们引入了一种基于FineWeb的新预训练数据集整理管道，该管道可以自动适应以支持任何语言。我们在一组九种不同语言上广泛消融了我们的管道设计选择，这些选择由一套有意义且信息丰富的评估任务指导，这些任务是通过基于可衡量标准的新颖选择过程选定的。最终，我们表明我们的管道可用于创建非英语语料库，这些语料库产生的模型比以前的数据集性能更高。我们还引入了一种直接且有原则的方法来重新平衡数据集，该方法考虑了重复计数和质量，从而提供了额外的性能提升。最后，我们使用近100个Common Crawl快照将我们的管道扩展到1000多种语言，从而生成了FineWeb2，这是一个新的20TB（50亿文档）多语言数据集，我们将其与我们的管道、训练和评估代码库一起发布。", "summary": "该论文提出了FineWeb2，一个新颖的、语言无关的预训练数据整理管道，旨在克服为大型语言模型（LLM）创建高质量多语言数据集的挑战。该管道基于FineWeb，能够自动调整过滤和去重过程以适应任何语言。通过对九种语言进行广泛的消融研究和引入新的数据集重新平衡技术，作者证明FineWeb2生成的非英语语料库能够使模型性能优于现有数据集。该管道已扩展到1000多种语言，生成了20TB、50亿文档的FineWeb2数据集，并随管道和代码库一起发布。", "keywords": "多语言LLM, 预训练数据, 数据整理, FineWeb2, 语言适应, 数据集重新平衡", "comments": "该论文通过提供一个可扩展且语言无关的数据处理管道，对多语言LLM预训练领域做出了重要贡献。其自动适应能力和有原则的重新平衡方法具有创新性，解决了开发超越英语的高性能LLM的关键瓶颈。大规模FineWeb2数据集和相关代码库的发布对于开源社区来说非常有价值，将促进多语言AI领域的进一步研究和发展。"}}
{"id": "2506.21104", "title": "Robust space-time multiscale upscaling via multicontinuum homogenization for evolving perforated media", "authors": ["Wei Xie", "Viet Ha Hoang", "Yin Yang", "Yunqing Huang"], "summary": "Time-evolving perforated domains arise in many engineering and geoscientific\napplications, including reactive transport, particle deposition, and structural\ndegradation in porous media. Accurately capturing the macroscopic behavior of\nsuch systems poses significant computational challenges due to the dynamic\nfine-scale geometries. In this paper, we develop a robust and generalizable\nmultiscale modeling framework based on multicontinuum homogenization to derive\neffective macroscopic equations in shrinking domains. The method distinguishes\nmultiple continua according to the physical characteristics (e.g., channel\nwidths), and couples them via space-time local cell problems formulated on\nrepresentative volume elements. These local problems incorporate temporal\nderivatives and domain evolution, ensuring consistency with underlying\nfine-scale dynamics. The resulting upscaled system yields computable\nmacroscopic coefficients and is suitable for large-scale simulations. Several\nnumerical experiments are presented to validate the accuracy, efficiency, and\npotential applicability of the method to complex time-dependent engineering\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21104v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21104v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "演化穿孔介质中基于多连续体均匀化的鲁棒时空多尺度粗化", "tldr": "本文提出了一种基于多连续体均匀化的鲁棒多尺度建模框架，用于处理时变穿孔介质，通过推导宏观方程来解决动态精细尺度几何带来的计算挑战。", "motivation": "由于动态精细尺度几何形状，准确捕捉时变穿孔域的宏观行为带来了显著的计算挑战。这些域广泛存在于工程和地球科学应用中。", "method": "本文开发了一种基于多连续体均匀化的多尺度建模框架。该方法根据物理特性（如通道宽度）区分多个连续体，并通过在代表性体积单元上制定的时空局部单元问题将它们耦合起来。这些局部问题结合了时间导数和域演化，确保与底层精细尺度动力学的一致性。", "result": "该方法产生了一个鲁棒且可推广的粗化系统，具有可计算的宏观系数，适用于大规模模拟。数值实验验证了该方法的准确性、效率和潜在适用性。", "conclusion": "所开发的多连续体均匀化框架为时变穿孔介质的多尺度建模提供了一种准确、高效且适用的方法，能够实现大规模模拟。", "translation": "时变穿孔域出现在许多工程和地球科学应用中，包括多孔介质中的反应输运、颗粒沉积和结构退化。由于动态的精细尺度几何形状，准确捕捉此类系统的宏观行为带来了显著的计算挑战。在本文中，我们开发了一种基于多连续体均匀化的鲁棒且可推广的多尺度建模框架，以推导收缩域中的有效宏观方程。该方法根据物理特性（例如通道宽度）区分多个连续体，并通过在代表性体积单元上制定的时空局部单元问题将它们耦合起来。这些局部问题结合了时间导数和域演化，确保与底层精细尺度动力学的一致性。由此产生的粗化系统产生可计算的宏观系数，适用于大规模模拟。本文提出了几个数值实验，以验证该方法对复杂时变工程问题的准确性、效率和潜在适用性。", "summary": "本文提出了一种鲁棒且可推广的多尺度建模框架，利用多连续体均匀化处理时变穿孔介质。该框架通过推导有效的宏观方程，解决了动态精细尺度几何带来的计算难题。该方法根据物理特性区分多个连续体，并通过包含时间导数和域演化的时空局部单元问题进行耦合。由此产生的粗化系统具有可计算的宏观系数，适用于大规模模拟，其准确性、效率和适用性已通过数值实验验证。", "keywords": "多尺度建模, 多连续体均匀化, 时变介质, 穿孔域, 粗化", "comments": "该论文的创新之处在于将多连续体均匀化方法扩展到时变穿孔介质，特别解决了动态精细尺度几何问题并整合了时间导数。这增强了多尺度建模在各种工程和地球科学领域中对复杂、演化系统的适用性。其鲁棒性和大规模模拟的适用性是其主要优势。"}}
{"id": "2506.21266", "title": "KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks", "authors": ["Daniil Karol", "Elizaveta Artser", "Ilya Vlasov", "Yaroslav Golubev", "Hieke Keuning", "Anastasiia Birillo"], "summary": "Collecting data of students solving programming tasks is incredibly valuable\nfor researchers and educators. It allows verifying that the students correctly\napply the features and concepts they are taught, or finding students'\nmisconceptions. However, existing data collection tools have limitations, e.g.,\nno control over the granularity of the collected code, not collecting the\nspecific events of the programming environment used, and overall being hard to\nconfigure.\n  To overcome these limitations, we propose KOALA, a convenient and highly\nconfigurable tool for collecting code snapshots and feature usage from students\nsolving programming tasks in JetBrains IDEs. The plugin can be installed in\nIDEs and configured to provide the students with the necessary tasks, enable or\ndisable certain IDE features like code completion, and run surveys. During\nproblem solving, the plugin collects code snapshots at the configured\ngranularity, all IDE actions like running and debugging, as well as some data\nnot collected in prior works, like employed hotkeys and switching focus between\nfiles. The collected data is sent to the server that comes with the tool, where\nit is stored and can be converted to the standardized ProgSnap2 format. To\nshowcase the tool, we collected data from 28 students solving tasks in two\ncourses within the IDE, highlighting some insights from this data.", "comment": "Accepted to CompEd'25, 7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21266v1", "categories": ["cs.SE", "cs.CY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21266v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "KOALA：一个用于在解决编程任务时收集IDE数据的可配置工具", "tldr": "KOALA是一个高度可配置的工具，用于从学生在JetBrains IDE中解决编程任务时收集代码快照和功能使用数据，克服了现有数据收集工具的局限性。", "motivation": "现有数据收集工具在代码粒度控制、特定编程环境事件收集以及配置难度方面存在局限性，导致研究人员和教育工作者难以有效收集学生解决编程任务的数据。", "method": "KOALA是一个JetBrains IDE插件，可配置以提供任务、启用/禁用IDE功能和运行调查。它收集配置粒度的代码快照、所有IDE操作（如运行和调试）、热键使用和文件焦点切换等数据。收集到的数据发送到配套服务器，存储并可转换为ProgSnap2格式。", "result": "该工具已用于收集28名学生在两个课程中解决IDE任务的数据，并从中获得了一些见解。", "conclusion": "KOALA成功克服了现有数据收集工具的局限性，能够方便、高度可配置地收集学生在JetBrains IDE中解决编程任务的详细数据，并展示了从这些数据中获取洞察的能力。", "translation": "收集学生解决编程任务的数据对研究人员和教育工作者来说非常有价值。它有助于验证学生是否正确应用了所学的功能和概念，或发现学生的错误概念。然而，现有数据收集工具存在局限性，例如无法控制收集代码的粒度、不收集所用编程环境的特定事件，以及整体配置困难。\n为了克服这些局限性，我们提出了KOALA，一个方便且高度可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用情况。该插件可以安装在IDE中，并配置为向学生提供必要的任务、启用或禁用某些IDE功能（如代码补全），以及运行调查。在解决问题期间，该插件会以配置的粒度收集代码快照、所有IDE操作（如运行和调试），以及一些先前工作中未收集的数据，例如使用的热键和文件之间的焦点切换。收集到的数据会发送到工具附带的服务器，在那里存储并可以转换为标准化的ProgSnap2格式。为了展示该工具，我们收集了28名学生在IDE中解决两门课程任务的数据，并强调了这些数据的一些见解。", "summary": "KOALA是一个为研究人员和教育工作者设计的可配置工具，旨在解决现有编程任务数据收集工具的局限性。它作为一个JetBrains IDE插件，能够以可控的粒度收集学生在解决编程任务时的代码快照、IDE操作、热键使用和文件焦点切换等详细数据。KOALA支持任务配置、IDE功能控制和调查运行，并将收集的数据存储在配套服务器上，可转换为ProgSnap2格式。该工具已成功用于从28名学生那里收集数据，并从中获得了有价值的见解。", "keywords": "KOALA, IDE数据收集, 编程任务, JetBrains, 可配置工具", "comments": "KOALA的创新之处在于其高度可配置性，解决了现有工具在数据粒度控制和特定事件收集方面的不足。它能够收集更细致、更全面的学生编程行为数据，特别是热键使用和文件焦点切换等先前未被广泛收集的数据，这对于深入理解学生学习过程和发现编程误区具有重要意义。该工具的插件形式和数据标准化能力也增强了其实用性和互操作性。"}}
{"id": "2506.21041", "title": "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling", "authors": ["Junwei You", "Pei Li", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Haotian Shi", "Bin Ran"], "summary": "Ensuring robust planning and decision-making under rare, diverse, and\nvisually degraded long-tail scenarios remains a fundamental challenge for\nautonomous driving in urban environments. This issue becomes more critical in\ncooperative settings, where vehicles and infrastructure jointly perceive and\nreason across complex environments. To address this challenge, we propose\nV2X-REALM, a vision-language model (VLM)-based framework with adaptive\nmultimodal learning for robust cooperative autonomous driving under long-tail\nscenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven\nlong-tail scenario generation and evaluation pipeline that leverages foundation\nmodels to synthesize realistic long-tail conditions such as snow and fog across\nvehicle- and infrastructure-side views, enriching training diversity\nefficiently; (ii) a gated multi-scenario adaptive attention module that\nmodulates the visual stream using scenario priors to recalibrate ambiguous or\ncorrupted features; and (iii) a multi-task scenario-aware contrastive learning\nobjective that improves multimodal alignment and promotes cross-scenario\nfeature separability. Extensive experiments demonstrate that V2X-REALM\nsignificantly outperforms existing baselines in robustness, semantic reasoning,\nsafety, and planning accuracy under complex, challenging driving conditions,\nadvancing the scalability of end-to-end cooperative autonomous driving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21041v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21041v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "V2X-REALM：基于视觉语言模型的鲁棒端到端协同自动驾驶与自适应长尾建模", "tldr": "V2X-REALM是一个基于视觉语言模型的框架，通过引入场景生成、自适应注意力模块和对比学习，显著提升了协同自动驾驶在长尾复杂场景下的鲁棒性和性能。", "motivation": "在城市环境中，自动驾驶车辆在稀有、多样且视觉退化的长尾场景下，确保鲁棒的规划和决策仍然是一个基本挑战。在协同设置中，车辆和基础设施共同感知和推理复杂环境时，这个问题变得更加关键。", "method": "本文提出了V2X-REALM，一个基于视觉语言模型（VLM）的框架，该框架采用自适应多模态学习，以实现长尾场景下鲁棒的协同自动驾驶。V2X-REALM引入了三个核心创新点：1) 一个提示驱动的长尾场景生成和评估流程，利用基础模型合成真实的（如雪、雾）长尾条件，以丰富训练多样性；2) 一个门控多场景自适应注意力模块，利用场景先验调节视觉流以重新校准模糊或损坏的特征；3) 一个多任务场景感知对比学习目标，以改善多模态对齐并促进跨场景特征可分离性。", "result": "大量实验表明，V2X-REALM在复杂、具有挑战性的驾驶条件下，在鲁棒性、语义推理、安全性以及规划精度方面显著优于现有基线。", "conclusion": "V2X-REALM提升了端到端协同自动驾驶的可扩展性。", "translation": "确保在稀有、多样和视觉退化的长尾场景下鲁棒的规划和决策，仍然是城市环境中自动驾驶面临的一个基本挑战。在协同设置中，当车辆和基础设施在复杂环境中共同感知和推理时，这个问题变得更加关键。为了解决这一挑战，我们提出了V2X-REALM，一个基于视觉语言模型（VLM）的框架，该框架采用自适应多模态学习，以实现长尾场景下鲁棒的协同自动驾驶。V2X-REALM引入了三个核心创新点：(i) 一个提示驱动的长尾场景生成和评估流程，该流程利用基础模型合成真实的（如雪和雾）长尾条件，涵盖车辆端和基础设施端视图，从而有效丰富训练多样性；(ii) 一个门控多场景自适应注意力模块，该模块利用场景先验来调节视觉流，以重新校准模糊或损坏的特征；(iii) 一个多任务场景感知对比学习目标，该目标改善了多模态对齐并促进了跨场景特征的可分离性。大量实验表明，V2X-REALM在复杂、具有挑战性的驾驶条件下，在鲁棒性、语义推理、安全性以及规划精度方面显著优于现有基线，从而提升了端到端协同自动驾驶的可扩展性。", "summary": "V2X-REALM是一个针对协同自动驾驶中长尾场景挑战提出的视觉语言模型（VLM）框架。它通过提示驱动的场景生成、门控多场景自适应注意力模块和多任务场景感知对比学习这三大创新，有效解决了稀有和视觉退化场景下的鲁棒性问题。实验证明，V2X-REALM在鲁棒性、语义推理、安全性和规划精度方面均显著优于现有方法，提升了端到端协同自动驾驶的可扩展性。", "keywords": "协同自动驾驶, 长尾场景, 视觉语言模型, 鲁棒性, 端到端", "comments": "V2X-REALM的创新性在于其结合了视觉语言模型，并针对长尾场景提出了多方面的解决方案，包括利用基础模型进行场景生成，以及通过自适应注意力和对比学习来增强特征处理和对齐。这对于提高自动驾驶在复杂和不常见条件下的性能和安全性具有重要意义，尤其是在协同自动驾驶领域。"}}
{"id": "2506.21417", "title": "Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation", "authors": ["Yunxiu Xu", "Siyu Wang", "Shoichi Hasegawa"], "summary": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device, designed\nwith thin strings and actuators attached to the fingernails, ensures minimal\nweight (1.55 g per finger) and preserves finger flexibility. Integrating the\nsoftware with a physics engine renders multiple types of haptic feedback (grip\nforce, collision, and sliding vibration feedback). We evaluated the device's\nperformance in pressure perception, slip feedback, typical dexterous\nmanipulation tasks, and daily operations, and we gathered user experience\nthrough subjective assessments. Our results show that participants could\nperceive and respond to pressure and vibration feedback. Through dexterous\nmanipulation experiments, we further demonstrated that these minimal haptic\ncues significantly improved virtual task efficiency, showcasing how lightweight\nhaptic feedback can enhance manipulation performance without complex\nmechanisms. The device's ability to preserve tactile sensations and minimize\nhindrance to real-world operations is a key advantage over glove-type haptic\ndevices. This research offers a potential solution for designing haptic\ninterfaces that balance lightweight construction, haptic feedback for dexterous\nmanipulation, and daily wearability.", "comment": "14 pages, 15 figures, 2 tables. Published in IEEE Transactions on\n  Haptics (Early Access)", "pdf_url": "http://arxiv.org/pdf/2506.21417v1", "categories": ["cs.HC", "H.5.2; I.3.6"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21417v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "轻量级指甲触觉设备：无障碍指尖力与振动反馈，增强虚拟灵巧操作", "tldr": "一种轻量级（指甲佩戴）触觉设备，提供力与振动反馈，显著提高虚拟操作效率，同时不影响真实世界互动。", "motivation": "旨在提供不阻碍真实世界互动的虚拟灵巧操作触觉反馈，解决传统手套式设备的局限性。", "method": "开发了一种轻量级（每根手指1.55克）可穿戴指尖触觉设备，通过附着在指甲上的细绳和执行器实现。该设备集成了软件与物理引擎，提供握力、碰撞和滑动振动反馈。通过压力感知、滑动反馈、灵巧操作任务和日常操作评估其性能，并收集用户主观体验。", "result": "参与者能够感知并响应压力和振动反馈。微小的触觉提示显著提高了虚拟任务效率。该设备保留了触觉感知，最大限度地减少了对真实世界操作的阻碍，优于手套式触觉设备。", "conclusion": "这项研究为设计平衡了轻量化结构、灵巧操作触觉反馈和日常佩戴性的触觉界面提供了一个潜在的解决方案。", "translation": "这项研究提出了一种轻量级、可穿戴的指尖触觉设备，可在虚拟环境中为灵巧操作提供基于物理的触觉反馈，同时不阻碍真实世界的互动。该设备采用附着在指甲上的细绳和执行器设计，确保了最小的重量（每根手指1.55克）并保持了手指的灵活性。将软件与物理引擎集成，可呈现多种类型的触觉反馈（握力、碰撞和滑动振动反馈）。我们评估了设备在压力感知、滑动反馈、典型的灵巧操作任务和日常操作中的性能，并通过主观评估收集了用户体验。我们的结果表明，参与者能够感知并响应压力和振动反馈。通过灵巧操作实验，我们进一步证明这些微小的触觉提示显著提高了虚拟任务效率，展示了轻量级触觉反馈如何在没有复杂机制的情况下增强操作性能。该设备在保留触觉感知和最大限度减少对真实世界操作的阻碍方面，相对于手套式触觉设备具有关键优势。这项研究为设计平衡了轻量化结构、灵巧操作触觉反馈和日常佩戴性的触觉界面提供了一个潜在的解决方案。", "summary": "本研究介绍了一种创新的轻量级（1.55克/指）指甲佩戴式触觉设备，旨在为虚拟环境中的灵巧操作提供物理基础的力与振动反馈，同时不干扰真实世界互动。该设备通过将细绳和执行器连接到指甲实现，并与物理引擎集成以提供多种反馈。实验结果表明，该设备能有效传递压力和振动反馈，并显著提升虚拟任务效率。其主要优势在于轻量化设计和对真实世界操作的无阻碍，为平衡佩戴性、反馈效果和日常可用性的触觉界面设计提供了新途径。", "keywords": "触觉设备, 指甲佩戴, 虚拟现实, 灵巧操作, 轻量级", "comments": "该研究通过将触觉反馈设备创新性地附着在指甲上，有效解决了传统手套式设备阻碍指尖触觉和真实世界操作的问题，实现了轻量化和高可用性。其在虚拟操作效率提升方面的实证结果，展示了简洁机制下触觉反馈的巨大潜力，为未来人机交互界面的设计提供了新的思路。"}}
{"id": "2506.21352", "title": "Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions", "authors": ["Le Vu Anh", "Mehmet Dik", "Nguyen Viet Anh"], "summary": "Persistent Laplacians are matrix operators that track how the shape and\nstructure of data transform across scales and are popularly adopted in biology,\nphysics, and machine learning. Their eigenvalues are concise descriptors of\ngeometric and topological features in a filtration. Although earlier work\nestablished global algebraic stability for these operators, the precise change\nin a single eigenvalue when one simplex, such as a vertex, edge, or triangle,\nis added has remained unknown. This is important because downstream tools,\nincluding heat-kernel signatures and spectral neural networks, depend directly\non these eigenvalues. We close this gap by proving a uniform Lipschitz bound:\nafter inserting one simplex, every up-persistent Laplacian eigenvalue can vary\nby at most twice the Euclidean norm of that simplex's boundary, independent of\nfiltration scale and complex size. This result delivers the first\neigenvalue-level robustness guarantee for spectral topological data analysis.\nIt guarantees that spectral features remain stable under local updates and\nenables reliable error control in dynamic data settings.", "comment": "16 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21352v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.MG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21352v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "持久拉普拉斯特征值在单单纯形插入下的利普希茨界", "tldr": "本文证明了在插入一个单纯形后，持久拉普拉斯特征值的变化有一个统一的利普希茨界，为谱拓扑数据分析提供了首次特征值层面的鲁棒性保证。", "motivation": "尽管先前的研究已经确立了持久拉普拉斯算子的全局代数稳定性，但当插入一个顶点、边或三角形等单一单纯形时，单个特征值的精确变化仍是未知的。这很重要，因为热核签名和谱神经网络等下游工具直接依赖于这些特征值。", "method": "通过证明一个统一的利普希茨界来解决这个问题。", "result": "在插入一个单纯形后，每个向上持久拉普拉斯特征值的变化最大为其边界欧几里得范数的两倍，且与过滤尺度和复形大小无关。", "conclusion": "该结果为谱拓扑数据分析提供了首次特征值层面的鲁棒性保证。它确保了在局部更新下谱特征保持稳定，并能在动态数据设置中实现可靠的误差控制。", "translation": "持久拉普拉斯算子是矩阵算子，用于追踪数据形状和结构如何跨尺度转换，并广泛应用于生物学、物理学和机器学习。它们的特征值是过滤中几何和拓扑特征的简洁描述符。尽管早期的工作已经确立了这些算子的全局代数稳定性，但当插入一个顶点、边或三角形等单一单纯形时，单个特征值的精确变化仍然未知。这很重要，因为包括热核签名和谱神经网络在内的下游工具直接依赖于这些特征值。我们通过证明一个统一的利普希茨界来弥补这一空白：在插入一个单纯形后，每个向上持久拉普拉斯特征值的变化最大为其边界欧几里得范数的两倍，且与过滤尺度和复形大小无关。这一结果为谱拓扑数据分析提供了首次特征值层面的鲁棒性保证。它保证了谱特征在局部更新下保持稳定，并能在动态数据设置中实现可靠的误差控制。", "summary": "本文研究了持久拉普拉斯特征值在数据中插入单一单纯形时的变化情况。尽管持久拉普拉斯算子已被广泛应用于多个领域，但单个特征值在局部扰动下的精确变化一直不明确。研究通过证明一个统一的利普希茨界，揭示了插入一个单纯形后，持久拉普拉斯特征值的变化量受限于该单纯形边界欧几里得范数的两倍。这一发现首次为谱拓扑数据分析提供了特征值层面的鲁棒性保证，确保了谱特征在局部更新下的稳定性，并支持在动态数据环境中进行可靠的误差控制。", "keywords": "持久拉普拉斯, 特征值, 利普希茨界, 单纯形插入, 拓扑数据分析", "comments": "这项工作填补了持久拉普拉斯特征值在局部扰动下精确变化认知的空白，首次提供了特征值层面的鲁棒性保证。这对于依赖这些特征值的下游工具（如热核签名和谱神经网络）的可靠性至关重要，特别是在动态数据分析中具有重要的实际应用价值。其创新点在于建立了明确的利普希茨界，量化了特征值的稳定性。"}}
{"id": "2506.21422", "title": "Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget", "authors": ["Kevin Kreutz", "Philipp Wiesner", "Monica Vitali"], "summary": "The carbon footprint of data centers has recently become a critical concern.\nSo far, most carbon-aware strategies have focused on leveraging the flexibility\nof scheduling decisions for batch processing by shifting the time and location\nof workload executions. However, such approaches cannot be applied to\nservice-oriented cloud applications, since they have to be reachable at every\npoint in time and often at low latencies. We propose a carbon-aware approach\nfor operating microservices under hourly carbon budgets. By choosing the most\nappropriate version and horizontal scaleout for each microservice, our strategy\nmaximizes user experience and revenue while staying within budget constraints.\nExperiments across various application configurations and carbon budgets\ndemonstrate that the approach adapts properly to changing workloads and carbon\nintensities.", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "pdf_url": "http://arxiv.org/pdf/2506.21422v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21422v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "碳感知微服务部署，以预算内实现最佳用户体验", "tldr": "提出一种碳感知微服务部署方法，通过选择合适的版本和横向扩展，在碳预算内最大化用户体验和收入。", "motivation": "数据中心的碳足迹已成为关键问题。现有碳感知策略主要针对批处理，无法应用于需要实时可达和低延迟的服务导向型云应用。", "method": "提出一种碳感知方法，用于在每小时碳预算下运行微服务。该策略通过为每个微服务选择最合适的版本和横向扩展，在预算约束内最大化用户体验和收入。", "result": "跨各种应用配置和碳预算的实验表明，该方法能正确适应不断变化的工作负载和碳强度。", "conclusion": "该碳感知微服务部署方法能够在预算内有效管理碳足迹，同时优化用户体验和收入，并能适应动态环境。", "translation": "数据中心的碳足迹最近已成为一个关键问题。迄今为止，大多数碳感知策略都专注于利用批处理调度决策的灵活性，通过改变工作负载执行的时间和地点。然而，此类方法无法应用于面向服务的云应用程序，因为它们必须在任何时间点都可达，并且通常需要低延迟。我们提出了一种碳感知方法，用于在每小时碳预算下运行微服务。通过为每个微服务选择最合适的版本和横向扩展，我们的策略在保持预算约束的同时，最大化用户体验和收入。跨各种应用配置和碳预算的实验表明，该方法能够正确适应不断变化的工作负载和碳强度。", "summary": "本文提出了一种针对服务导向型云应用的碳感知微服务部署策略，以应对数据中心日益增长的碳足迹问题。不同于传统批处理的碳调度方法，该策略通过为每个微服务智能选择版本和横向扩展，旨在每小时碳预算内最大化用户体验和收入。实验证明，该方法能有效适应动态的工作负载和碳强度变化。", "keywords": "碳感知, 微服务, 用户体验, 部署, 碳足迹", "comments": "这项研究的创新之处在于将碳感知策略应用于对可用性和延迟有严格要求的微服务架构，填补了现有碳感知方法在这方面的空白。它提供了一种实际可行的方案，平衡了环境可持续性、用户体验和经济效益。其潜在局限性可能在于需要精确的碳强度预测和复杂的配置管理。"}}
{"id": "2506.20752", "title": "Characterization and Mitigation of Training Instabilities in Microscaling Formats", "authors": ["Huangyuan Su", "Mujin Kwun", "Stephanie Gil", "Sham Kakade", "Nikhil Anand"], "summary": "Training large language models is an expensive, compute-bound process that\nmust be repeated as models scale, algorithms improve, and new data is\ncollected. To address this, next-generation hardware accelerators increasingly\nsupport lower-precision arithmetic formats, such as the Microscaling (MX)\nformats introduced in NVIDIA's Blackwell architecture. These formats use a\nshared scale within blocks of parameters to extend representable range and\nperform forward/backward GEMM operations in reduced precision for efficiency\ngains. In this work, we investigate the challenges and viability of\nblock-scaled precision formats during model training. Across nearly one\nthousand language models trained from scratch -- spanning compute budgets from\n$2 \\times 10^{17}$ to $4.8 \\times 10^{19}$ FLOPs and sweeping over a broad\nrange of weight-activation precision combinations -- we consistently observe\nthat training in MX formats exhibits sharp, stochastic instabilities in the\nloss, particularly at larger compute scales. To explain this phenomenon, we\nconduct controlled experiments and ablations on a smaller proxy model that\nexhibits similar behavior as the language model, sweeping across architectural\nsettings, hyperparameters, and precision formats. These experiments motivate a\nsimple model in which multiplicative gradient bias introduced by the\nquantization of layer-norm affine parameters and a small fraction of\nactivations can trigger runaway divergence. Through \\emph{in situ} intervention\nexperiments on our proxy model, we demonstrate that instabilities can be\naverted or delayed by modifying precision schemes mid-training. Guided by these\nfindings, we evaluate stabilization strategies in the LLM setting and show that\ncertain hybrid configurations recover performance competitive with\nfull-precision training. We release our code at\nhttps://github.com/Hither1/systems-scaling.", "comment": "14 pages + appendices", "pdf_url": "http://arxiv.org/pdf/2506.20752v1", "categories": ["cs.LG", "cs.AR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20752v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "微缩放格式中训练不稳定性的表征与缓解", "tldr": "本文研究了在微缩放（MX）格式下训练大型语言模型时出现的训练不稳定性，发现这是由量化引入的梯度偏差引起的，并提出了通过混合精度配置来缓解这些不稳定性，使其性能接近全精度训练。", "motivation": "大型语言模型训练成本高昂且计算密集，下一代硬件加速器支持低精度算术格式（如NVIDIA Blackwell架构中的微缩放MX格式）以提高效率。然而，在这些格式下训练模型可能面临挑战，需要对其进行表征和缓解。", "method": "研究人员训练了近一千个大型语言模型，涵盖不同计算预算和权重-激活精度组合，观察MX格式下的训练行为。为了解释不稳定现象，他们在一个较小的代理模型上进行了受控实验和消融研究，扫掠了架构设置、超参数和精度格式。通过原位干预实验，验证了修改训练中精度方案的有效性。", "result": "在MX格式下训练时，尤其是在较大的计算规模下，训练损失表现出尖锐、随机的不稳定性。实验表明，由层归一化仿射参数和少量激活的量化引入的乘性梯度偏差可以触发失控发散。通过原位干预，展示了通过在训练中修改精度方案可以避免或延迟不稳定性。在LLM设置中，某些混合配置能够恢复与全精度训练相当的性能。", "conclusion": "微缩放（MX）格式在大型语言模型训练中存在显著的不稳定性，这些不稳定性主要源于量化引入的乘性梯度偏差。然而，通过在训练中采用混合精度配置等策略，可以有效缓解这些不稳定性，使模型性能达到与全精度训练相当的水平。", "translation": "训练大型语言模型是一个昂贵且计算密集的过程，随着模型规模的扩大、算法的改进和新数据的收集，这一过程需要重复进行。为了解决这个问题，下一代硬件加速器越来越多地支持低精度算术格式，例如NVIDIA Blackwell架构中引入的微缩放（MX）格式。这些格式在参数块内使用共享比例因子来扩展可表示范围，并以降低的精度执行前向/后向GEMM操作以提高效率。在这项工作中，我们调查了模型训练期间块缩放精度格式的挑战和可行性。在从头开始训练的近一千个语言模型中——计算预算从$2 \\times 10^{17}$到$4.8 \\times 10^{19}$ FLOPs，并涵盖了广泛的权重-激活精度组合——我们始终观察到MX格式的训练表现出尖锐、随机的损失不稳定性，尤其是在较大的计算规模下。为了解释这种现象，我们对一个表现出与语言模型相似行为的较小代理模型进行了受控实验和消融研究，扫掠了架构设置、超参数和精度格式。这些实验促使我们建立了一个简单的模型，其中层归一化仿射参数和少量激活的量化引入的乘性梯度偏差可以触发失控发散。通过对我们的代理模型进行原位干预实验，我们证明通过在训练中修改精度方案可以避免或延迟不稳定性。在这些发现的指导下，我们在LLM设置中评估了稳定策略，并表明某些混合配置可以恢复与全精度训练相当的性能。我们已在https://github.com/Hither1/systems-scaling发布了代码。", "summary": "本研究深入探讨了在NVIDIA Blackwell架构中引入的微缩放（MX）格式在大型语言模型（LLM）训练中出现的训练不稳定性。通过对大量LLM的广泛实验，作者发现MX格式训练会导致损失出现尖锐、随机的不稳定性。进一步的受控实验揭示，这种不稳定性源于量化层归一化仿射参数和部分激活时引入的乘性梯度偏差，该偏差可导致训练发散。为解决此问题，研究提出并通过实验验证了在训练中动态调整精度方案的有效性，特别是采用混合配置，能够显著缓解不稳定性并使模型性能恢复至与全精度训练相当的水平。", "keywords": "微缩放, 训练不稳定性, 大型语言模型, 低精度, 量化", "comments": "该论文的创新点在于系统性地表征了微缩放格式在训练大型语言模型时引入的不稳定性，并明确指出了其根本原因——量化引入的乘性梯度偏差。其重要性在于为未来低精度训练硬件和算法的设计提供了关键见解。通过提出并验证混合精度配置作为缓解策略，为实际应用提供了可行的解决方案。研究规模庞大，训练了近千个模型，增强了结论的可靠性。"}}
{"id": "2506.20799", "title": "Structural System Identification via Validation and Adaptation", "authors": ["Cristian López", "Keegan J. Moore"], "summary": "Estimating the governing equation parameter values is essential for\nintegrating experimental data with scientific theory to understand, validate,\nand predict the dynamics of complex systems. In this work, we propose a new\nmethod for structural system identification (SI), uncertainty quantification,\nand validation directly from data. Inspired by generative modeling frameworks,\na neural network maps random noise to physically meaningful parameters. These\nparameters are then used in the known equation of motion to obtain fake\naccelerations, which are compared to real training data via a mean square error\nloss. To simultaneously validate the learned parameters, we use independent\nvalidation datasets. The generated accelerations from these datasets are\nevaluated by a discriminator network, which determines whether the output is\nreal or fake, and guides the parameter-generator network. Analytical and real\nexperiments show the parameter estimation accuracy and model validation for\ndifferent nonlinear structural systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20799v1", "categories": ["math.DS", "cs.LG", "cs.SY", "eess.SY"], "cate": "math.DS", "url": "http://arxiv.org/abs/2506.20799v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "结构系统识别通过验证和适应", "tldr": "本文提出一种新的基于生成建模框架的结构系统识别方法，通过神经网络从数据中直接进行参数估计、不确定性量化和模型验证，并使用独立的验证数据集和判别器网络进行同步验证。", "motivation": "整合实验数据与科学理论以理解、验证和预测复杂系统的动力学时，估计控制方程参数值至关重要。", "method": "提出一种新的结构系统识别（SI）、不确定性量化和验证方法。该方法受生成建模框架启发，使用神经网络将随机噪声映射到物理有意义的参数。这些参数用于已知的运动方程以生成“假”加速度，并与真实训练数据通过均方误差损失进行比较。为同时验证学习到的参数，使用独立的验证数据集，这些数据集生成的加速度由判别器网络评估，判别器判断输出是真实还是虚假，并指导参数生成器网络。", "result": "分析和真实实验表明了不同非线性结构系统的参数估计准确性和模型验证能力。", "conclusion": "该方法能够准确估计参数并有效验证模型，为结构系统识别提供了一个有前景的途径。", "translation": "估计控制方程参数值对于整合实验数据与科学理论以理解、验证和预测复杂系统的动力学至关重要。在这项工作中，我们提出了一种新的结构系统识别（SI）、不确定性量化和直接从数据中进行验证的方法。受生成建模框架的启发，神经网络将随机噪声映射到具有物理意义的参数。然后，这些参数用于已知的运动方程以获得“假”加速度，通过均方误差损失将其与真实训练数据进行比较。为了同时验证学习到的参数，我们使用独立的验证数据集。来自这些数据集生成的加速度由判别器网络评估，该网络确定输出是真实还是虚假，并指导参数生成器网络。分析和真实实验表明了不同非线性结构系统的参数估计准确性和模型验证能力。", "summary": "本文提出一种新的基于生成建模框架的结构系统识别方法，该方法通过神经网络将噪声转换为物理参数，并利用已知的运动方程生成加速度。通过均方误差与真实数据比较进行训练，并结合独立的验证数据集和判别器网络进行参数验证，实现了从数据中直接进行参数估计、不确定性量化和模型验证。实验证明了其在非线性结构系统中的准确性和有效性。", "keywords": "结构系统识别, 不确定性量化, 神经网络, 生成模型, 模型验证", "comments": "该研究创新性地将生成对抗网络（GAN）的思想引入结构系统识别领域，通过生成器和判别器的协同作用，实现了参数估计与模型验证的同步进行，提升了数据驱动型系统识别的可靠性。"}}
{"id": "2506.20867", "title": "Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation", "authors": ["Ryosuke Kawamura", "Hideaki Hayashi", "Shunsuke Otake", "Noriko Takemura", "Hajime Nagahara"], "summary": "Dynamic facial expression recognition (DFER) is a task that estimates\nemotions from facial expression video sequences. For practical applications,\naccurately recognizing ambiguous facial expressions -- frequently encountered\nin in-the-wild data -- is essential. In this study, we propose MIDAS, a data\naugmentation method designed to enhance DFER performance for ambiguous facial\nexpression data using soft labels representing probabilities of multiple\nemotion classes. MIDAS augments training data by convexly combining pairs of\nvideo frames and their corresponding emotion class labels. This approach\nextends mixup to soft-labeled video data, offering a simple yet highly\neffective method for handling ambiguity in DFER. To evaluate MIDAS, we\nconducted experiments on both the DFEW dataset and FERV39k-Plus, a newly\nconstructed dataset that assigns soft labels to an existing DFER dataset. The\nresults demonstrate that models trained with MIDAS-augmented data achieve\nsuperior performance compared to the state-of-the-art method trained on the\noriginal dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20867v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20867v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "增强基于软标签的数据增强的模糊动态面部表情识别", "tldr": "提出MIDAS，一种基于软标签数据增强方法，旨在提高模糊动态面部表情识别（DFER）性能。", "motivation": "在实际应用中，准确识别野外数据中常见的模糊面部表情对于动态面部表情识别（DFER）至关重要。", "method": "本研究提出MIDAS，一种数据增强方法。该方法通过凸组合视频帧对及其对应的软标签（代表多个情感类别的概率）来扩充训练数据，从而将mixup扩展到软标签视频数据，以增强模糊面部表情数据的DFER性能。", "result": "实验结果表明，使用MIDAS增强数据训练的模型在DFEW数据集和新构建的FERV39k-Plus数据集上，比在原始数据集上训练的现有最先进方法表现出更优越的性能。", "conclusion": "MIDAS是一种简单而高效的方法，能够有效处理动态面部表情识别中的模糊性，并显著提升模型性能。", "translation": "动态面部表情识别（DFER）是一项从面部表情视频序列中估计情感的任务。对于实际应用而言，准确识别野外数据中频繁遇到的模糊面部表情至关重要。在本研究中，我们提出了MIDAS，一种旨在通过使用表示多个情感类别概率的软标签来增强模糊面部表情数据的DFER性能的数据增强方法。MIDAS通过凸组合视频帧对及其对应的情感类别标签来扩充训练数据。这种方法将mixup扩展到软标签视频数据，为处理DFER中的模糊性提供了一种简单而高效的方法。为了评估MIDAS，我们在DFEW数据集和FERV39k-Plus（一个将软标签分配给现有DFER数据集的新构建数据集）上进行了实验。结果表明，使用MIDAS增强数据训练的模型比在原始数据集上训练的现有最先进方法取得了更优越的性能。", "summary": "本研究提出MIDAS，一种基于软标签的数据增强方法，旨在提升动态面部表情识别（DFER）中对模糊表情的识别能力。MIDAS通过凸组合视频帧及其软标签来扩展mixup，以有效处理模糊性。实验结果表明，在DFEW和FERV39k-Plus数据集上，使用MIDAS增强数据训练的模型性能优于现有最先进方法。", "keywords": "动态面部表情识别, 数据增强, 软标签, 模糊表情, mixup", "comments": "该论文提出了一种新颖的数据增强方法MIDAS，通过引入软标签和扩展mixup机制，有效解决了动态面部表情识别中模糊表情的挑战。其创新性在于将软标签与数据增强结合，为处理情感模糊性提供了一个简洁而高效的方案。文中提及构建新的FERV39k-Plus数据集也值得关注，为该领域研究提供了新的资源。"}}
{"id": "2506.21393", "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding", "authors": ["Junwen Zhang", "Pu Chen", "Yin Zhang"], "summary": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.", "comment": "43 pages and 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.21393v1", "categories": ["cs.AI", "68T07 (Primary), 68T50, 68T30, 68T45 (Secondary)", "F.2.2; I.2.7; I.2.10"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21393v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TableMoE：多模态表格理解中结构化专家推理的神经符号路由", "tldr": "TableMoE 是一种新型的神经符号 MLLM，用于鲁棒的多模态表格理解，它使用专家路由和大型预训练数据集，在具有挑战性的真实世界基准上超越了最先进的模型。", "motivation": "现有的大型多模态语言模型（MLLMs）难以处理复杂、视觉退化的真实世界表格（WildStruct 条件），因为其结构复杂性、符号密度和视觉退化，导致性能有限和泛化能力差。", "method": "本文提出了 TableMoE，这是一种神经符号的连接专家混合（MoCE）架构，专门用于对多模态表格数据进行鲁棒、结构化推理。TableMoE 的核心是创新的神经符号路由机制，该机制预测潜在的语义标记角色（如标题、数据单元格、轴、公式），并利用由符号推理图指导的置信度感知门控策略，将表格元素动态路由到专门的专家（Table-to-HTML、Table-to-JSON、Table-to-Code）。为了促进有效的对齐驱动预训练，研究人员引入了大规模的 TableMoE-Align 数据集，包含 120 万个表格-HTML-JSON-代码四元组。为了评估模型，他们整理并发布了四个具有挑战性的 WildStruct 基准：WMMFinQA、WMMTatQA、WMMTabDialog 和 WMMFinanceMath。", "result": "实验结果表明，TableMoE 显著超越了现有的最先进模型。广泛的消融研究验证了每个核心组件，强调了神经符号路由和结构化专家对齐的关键作用。定性分析进一步展示了 TableMoE 的可解释性和增强的鲁棒性。", "conclusion": "将神经符号推理集成到多模态表格理解中是有效且鲁棒的，并且能够提高可解释性。", "translation": "多模态表格理解在现实世界中面临挑战，因为其结构复杂性、符号密度和视觉退化（模糊、倾斜、水印、不完整的结构或字体、多跨度或分层嵌套布局）。现有的多模态大型语言模型（MLLMs）在这些“野外结构”（WildStruct）条件下表现不佳，导致性能有限和泛化能力差。为了解决这些挑战，我们提出了TableMoE，这是一种神经符号的连接专家混合（MoCE）架构，专门用于对多模态表格数据进行鲁棒、结构化推理。TableMoE具有创新的神经符号路由机制，可以预测潜在的语义标记角色（例如，标题、数据单元格、轴、公式），并使用由符号推理图指导的置信度感知门控策略，将表格元素动态路由到专业专家（Table-to-HTML、Table-to-JSON、Table-to-Code）。为了促进有效的对齐驱动预训练，我们引入了大规模的TableMoE-Align数据集，该数据集包含来自金融、科学、生物医学和工业领域的120万个表格-HTML-JSON-代码四元组，专门用于模型预训练。为了进行评估，我们整理并发布了四个具有挑战性的WildStruct基准：WMMFinQA、WMMTatQA、WMMTabDialog和WMMFinanceMath，这些基准专门设计用于在现实世界的多模态退化和结构复杂性下对模型进行压力测试。实验结果表明，TableMoE显著超越了现有的最先进模型。广泛的消融研究验证了每个核心组件，强调了神经符号路由和结构化专家对齐的关键作用。通过定性分析，我们进一步展示了TableMoE的可解释性和增强的鲁棒性，强调了集成神经符号推理在多模态表格理解中的有效性。", "summary": "本文提出了 TableMoE，这是一种神经符号的连接专家混合（MoCE）架构，旨在解决复杂且退化的真实世界条件下（WildStruct）多模态表格理解的挑战。TableMoE 采用新颖的神经符号路由机制，该机制预测语义标记角色，并利用置信度感知门控策略将表格元素动态路由到专业专家。作者还提出了用于预训练的大规模 TableMoE-Align 数据集和四个具有挑战性的 WildStruct 基准用于评估。实验结果表明，TableMoE 显著优于现有最先进的模型，消融研究强调了其神经符号路由和专家对齐组件的重要性，展示了增强的鲁棒性和可解释性。", "keywords": "多模态表格理解, 神经符号推理, 专家混合, TableMoE, WildStruct", "comments": "本文通过解决“野外”条件下多模态表格理解的实际挑战，做出了重大贡献，这些挑战经常困扰现有的大型多模态语言模型。核心创新在于神经符号路由机制，它巧妙地结合了神经网络能力和符号推理，以实现更鲁棒和可解释的表格处理。大规模 TableMoE-Align 数据集和具有挑战性的 WildStruct 基准的引入对于推动该领域的研究也至关重要，为未来的工作提供了宝贵的资源。对可解释性和鲁棒性的强调是一个强项，超越了单纯的性能指标。"}}
{"id": "2506.21535", "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation", "authors": ["Mohammed Baharoon", "Jun Ma", "Congyu Fang", "Augustin Toma", "Bo Wang"], "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21535v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21535v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "探索用于CT报告生成的3D多模态大语言模型设计空间", "tldr": "本文系统研究了用于CT报告生成的3D多模态大语言模型（MLLMs）的设计空间，并提出了知识增强方法，在MICCAI 2024 AMOS-MM挑战赛中取得第二名。研究发现，在相同训练协议下，放射报告生成（RRG）与LLM大小基本无关；若ViT在较小体积上预训练，则较大体积不总能提高性能；使用分割掩码可提升性能。", "motivation": "自动化放射学报告生成（RRG）是多模态大语言模型（MLLMs）的一个有前景的应用方向。本文旨在系统地探索用于3D CT报告生成的3D MLLMs的设计空间。", "method": "本文系统调查了3D MLLMs的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术。此外，还引入了两种基于知识的报告增强方法。研究在AMOS-MM数据集的1,687个病例上进行了评估。", "result": "基于知识的报告增强方法将GREEN分数提高了10%，在MICCAI 2024 AMOS-MM挑战赛中获得第二名。结果表明，在相同的训练协议下，放射学报告生成（RRG）在很大程度上独立于LLM的大小。研究还发现，如果原始ViT是在较小体积上预训练的，那么更大的体积并不总是能提高性能。最后，使用分割掩码与CT体积一起可以提高性能。", "conclusion": "本研究表明，通过系统探索3D MLLMs的设计空间并引入知识增强方法，可以显著提高CT报告生成的性能。关键发现包括LLM大小对RRG影响有限、体积大小与ViT预训练的关联性以及分割掩码的有效性。", "translation": "多模态大语言模型（MLLMs）已成为自动化放射学报告生成（RRG）的一种有前景的方法。在这项工作中，我们系统地研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和用于3D CT报告生成的微调技术。我们还引入了两种基于知识的报告增强方法，将GREEN分数提高了10%，在MICCAI 2024 AMOS-MM挑战赛中获得第二名。我们在AMOS-MM数据集的1,687个病例上的结果表明，在相同的训练协议下，RRG在很大程度上独立于LLM的大小。我们还表明，如果原始ViT是在较小体积上预训练的，那么更大的体积并不总是能提高性能。最后，我们表明使用分割掩码与CT体积一起可以提高性能。代码已公开在https://github.com/bowang-lab/AMOS-MM-Solution", "summary": "本文系统探索了用于CT报告生成的3D多模态大语言模型（MLLMs）的设计空间，涵盖了视觉输入、投影器、LLMs和微调技术。研究引入了两种基于知识的报告增强方法，显著提升了性能，并在MICCAI 2024 AMOS-MM挑战赛中获得第二名。主要发现包括RRG与LLM大小的独立性、体积大小在ViT预训练背景下的细微影响，以及结合分割掩码的益处。", "keywords": "3D MLLMs, CT报告生成, 放射学, 设计空间, 知识增强", "comments": "该论文系统性地探索了3D MLLMs的设计空间，为放射学报告生成领域提供了宝贵的指导。关于LLM大小独立性和ViT预训练对体积大小影响的发现，挑战了现有的一些普遍假设，具有重要的理论和实践意义。知识增强方法的引入也展示了创新性。"}}
{"id": "2506.21123", "title": "Characterization of Rydberg-Atom Signal Reception of Dual-Frequency Signals Coupled with Two Energy Levels", "authors": ["Hao Wu", "Chongwu Xie", "Xinyuan Yao", "Kang-Da Wu", "Shanchi Wu", "Rui Ni", "Guo-Yong Xiang", "Chen Gong"], "summary": "Rydberg atomic sensors have been adopted for novel radio frequency (RF)\nmeasurement technique and the sensing capability for signals in multiple\nfrequencies makes it attractive for multi-user communication. However, unlike\ntraditional antennas where the signals in multiple frequencies are orthogonal,\nthe received signals of atomic sensors corresponding to different energy levels\nwill be downconverted to the baseband simultaneously, resulting in multi-user\ninterference. Thus, in this paper, we analyze the mutual interference\ncharacteristics of two RF signals with different carrier frequencies coupling\ndifferent energy levels. We introduce the joint response coefficient based on\nthe receiver characteristics and analyze the interference of one user to\nanother. We analyze the bit-error rate (BER) and symbol-error rate (SER) for\ntwo signals coupling two different energy levels. We also conduct experiments\nto validate the BER and SER results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21123v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21123v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Rydberg原子双频信号与双能级耦合的接收特性研究", "tldr": "本文分析并验证了Rydberg原子传感器接收双频信号时，不同能级耦合引起的相互干扰和误码率。", "motivation": "Rydberg原子传感器在多用户通信中具有多频传感潜力，但与传统天线不同，其接收的多频信号会同时下变频到基带，导致多用户干扰。本文旨在解决这一干扰问题。", "method": "本文分析了两个不同载波频率的射频信号耦合不同能级时的相互干扰特性。引入了基于接收器特性的联合响应系数，分析了一个用户对另一个用户的干扰。计算并实验验证了两个耦合不同能级的信号的误码率(BER)和符号错误率(SER)。", "result": "本文分析了两个耦合不同能级的信号的误码率(BER)和符号错误率(SER)，并通过实验验证了这些结果。", "conclusion": "本研究表征了Rydberg原子传感器在接收与不同能级耦合的双频信号时的干扰和误码率，为它们在多用户通信中的性能提供了见解。", "translation": "Rydberg原子传感器已被用于新颖的射频(RF)测量技术，其对多频信号的传感能力使其在多用户通信中具有吸引力。然而，与传统天线中多频信号正交不同，原子传感器接收到的对应不同能级的信号会同时下变频到基带，导致多用户干扰。因此，本文分析了两个不同载波频率的射频信号耦合不同能级时的相互干扰特性。我们基于接收器特性引入了联合响应系数，并分析了一个用户对另一个用户的干扰。我们分析了两个耦合不同能级的信号的误码率(BER)和符号错误率(SER)。我们还进行了实验来验证BER和SER的结果。", "summary": "本文研究了Rydberg原子传感器在接收与不同能级耦合的双频信号时出现的多用户干扰。它分析了相互干扰特性，引入了联合响应系数，并评估了此类信号的误码率(BER)和符号错误率(SER)。文中还提供了BER和SER结果的实验验证。", "keywords": "Rydberg原子传感器, 多用户干扰, 双频信号, 误码率, 符号错误率", "comments": "本文解决了Rydberg原子传感器在多用户通信应用中的一个关键挑战（多用户干扰），这是迈向实际部署的重要一步。理论分析（联合响应系数、BER/SER）与实验验证的结合增强了研究结果的说服力。"}}
{"id": "2506.21418", "title": "Vantage Point Selection Algorithms for Bottleneck Capacity Estimation", "authors": ["Vikrant Ashvinkumar", "Rezaul Chowdhury", "Jie Gao", "Mayank Goswami", "Joseph S. B. Mitchell", "Valentin Polishchuk"], "summary": "Motivated by the problem of estimating bottleneck capacities on the Internet,\nwe formulate and study the problem of vantage point selection. We are given a\ngraph $G=(V, E)$ whose edges $E$ have unknown capacity values that are to be\ndiscovered. Probes from a vantage point, i.e, a vertex $v \\in V$, along\nshortest paths from $v$ to all other vertices, reveal bottleneck edge\ncapacities along each path. Our goal is to select $k$ vantage points from $V$\nthat reveal the maximum number of bottleneck edge capacities.\n  We consider both a non-adaptive setting where all $k$ vantage points are\nselected before any bottleneck capacity is revealed, and an adaptive setting\nwhere each vantage point selection instantly reveals bottleneck capacities\nalong all shortest paths starting from that point. In the non-adaptive setting,\nby considering a relaxed model where edge capacities are drawn from a random\npermutation (which still leaves the problem of maximizing the expected number\nof revealed edges NP-hard), we are able to give a $1-1/e$ approximate\nalgorithm. In the adaptive setting we work with the least permissive model\nwhere edge capacities are arbitrarily fixed but unknown. We compare with the\nbest solution for the particular input instance (i.e. by enumerating all\nchoices of $k$ tuples), and provide both lower bounds on instance optimal\napproximation algorithms and upper bounds for trees and planar graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21418v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21418v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "瓶颈容量估计的视点选择算法", "tldr": "本文研究了互联网瓶颈容量估计中的视点选择问题，提出了在非自适应和自适应设置下的算法，并给出了近似性能保证和界限。", "motivation": "论文的动机是解决互联网上的瓶颈容量估计问题。具体目标是从图中选择 $k$ 个视点，以揭示最大数量的瓶颈边容量。", "method": "论文将问题建模为在给定图 $G=(V, E)$ 中选择 $k$ 个视点，通过从这些视点沿最短路径探测来揭示瓶颈边容量。研究了两种设置：1. 非自适应设置：在揭示任何容量之前选择所有 $k$ 个视点。在此设置下，针对边容量从随机排列中抽取的宽松模型，提出了一个 $1-1/e$ 近似算法。2. 自适应设置：每次视点选择后立即揭示瓶颈容量。在此设置下，采用边容量任意固定但未知的模型，并提供了实例最优近似算法的下界以及针对树和平面图的上界。", "result": "在非自适应设置中，对于边容量从随机排列中抽取的模型，提供了一个 $1-1/e$ 近似算法。在自适应设置中，对于边容量任意固定但未知的模型，给出了实例最优近似算法的下界，并为树和平面图提供了上界。", "conclusion": "论文为瓶颈容量估计的视点选择问题提供了理论框架和算法，在非自适应和自适应两种设置及不同模型下取得了近似性能保证和边界，对网络测量中的资源优化具有指导意义。", "translation": "标题翻译：瓶颈容量估计的视点选择算法\n\n摘要翻译：受互联网瓶颈容量估计问题的启发，我们提出并研究了视点选择问题。我们给定一个图 $G=(V, E)$，其边 $E$ 具有待发现的未知容量值。从一个视点（即一个顶点 $v \\in V$）沿着从 $v$ 到所有其他顶点的最短路径进行探测，可以揭示每条路径上的瓶颈边容量。我们的目标是从 $V$ 中选择 $k$ 个视点，以揭示最大数量的瓶颈边容量。\n\n我们考虑了两种设置：非自适应设置，其中在揭示任何瓶颈容量之前选择所有 $k$ 个视点；以及自适应设置，其中每次视点选择会立即揭示从该点开始的所有最短路径上的瓶颈容量。在非自适应设置中，通过考虑一个宽松模型（其中边容量从随机排列中抽取，这仍然使得最大化揭示边期望数量的问题是NP-难的），我们能够给出一个 $1-1/e$ 近似算法。在自适应设置中，我们采用最不宽松的模型，其中边容量是任意固定但未知的。我们与特定输入实例的最佳解决方案（即通过枚举所有 $k$ 元组的选择）进行比较，并提供了实例最优近似算法的下界以及针对树和平面图的上界。", "summary": "这篇论文探讨了互联网瓶颈容量估计中的视点选择问题。作者将问题建模为在给定图中选择 $k$ 个视点，以最大化通过最短路径探测所能揭示的瓶颈边容量数量。论文研究了非自适应和自适应两种设置。在非自适应设置下，提出了一个 $1-1/e$ 近似算法；在自适应设置下，为任意固定但未知容量的模型提供了实例最优近似算法的下界以及针对特定图类型（如树和平面图）的上界。", "keywords": "瓶颈容量估计, 视点选择, 近似算法, 网络测量, 图论", "comments": "这篇论文通过将互联网瓶颈容量估计问题抽象为图论中的视点选择问题，提供了一个新颖的建模方法。其创新之处在于区分了非自适应和自适应两种探测策略，并在不同假设下提供了理论上的近似保证和界限，这对于理解和解决网络测量中的资源优化问题具有重要意义。"}}
{"id": "2506.20923", "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "pdf_url": "http://arxiv.org/pdf/2506.20923v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20923v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "KaLM-Embedding-V2：卓越的训练技术和数据启发了多功能嵌入模型", "tldr": "KaLM-Embedding-V2通过优越的训练技术和大规模数据，实现了在通用文本嵌入任务中超越同类模型并媲美大型模型的性能，设定了紧凑型嵌入模型的新标准。", "motivation": "旨在通过利用卓越的训练技术和数据，开发一个多功能且紧凑的文本嵌入模型，以在通用文本嵌入任务中取得令人印象深刻的性能。", "method": "该模型通过以下创新实现：1) 架构改进，移除因果注意力掩码，采用全双向Transformer和均值池化生成固定长度嵌入。2) 多阶段训练流程，包括在大规模弱监督开源语料库上预训练，在高质量检索和非检索数据集上微调，以及使用模型融合（model-soup）进行参数平均以增强泛化能力。3) 引入焦点式重加权机制以集中学习困难样本，并采用在线难负样本混合策略持续丰富难负样本。4) 收集超过20类数据用于预训练，100类数据用于微调，以提升性能和泛化能力。", "result": "在海量文本嵌入基准（MTEB）中文和英文评估中，KaLM-Embedding-V2显著优于同等大小的其他模型，并能与3倍、14倍、18倍和26倍大的嵌入模型竞争，为参数小于1B的多功能紧凑型嵌入模型树立了新标准。", "conclusion": "KaLM-Embedding-V2通过其创新的架构、多阶段训练方法、先进的学习策略和丰富的数据集，成功地在通用文本嵌入任务中实现了卓越的性能，并为紧凑型嵌入模型设立了新基准，证明了优越的训练技术和数据在构建高效能模型中的关键作用。", "translation": "在本文中，我们提出了KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，它通过利用卓越的训练技术和数据，在通用文本嵌入任务中取得了令人印象深刻的性能。我们的主要创新包括：(1) 为了更好地使架构与表示学习对齐，我们移除了因果注意力掩码，并采用了带有简单而有效的均值池化的全双向Transformer，以生成固定长度的嵌入；(2) 我们采用了多阶段训练流程：(i) 在大规模弱监督开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；以及 (iii) 使用模型融合（model-soup）参数平均以实现鲁棒的泛化能力。此外，我们引入了一种焦点式重加权机制，将学习集中在困难样本上，以及一种在线难负样本混合策略，无需昂贵的离线挖掘即可持续丰富难负样本；(3) 我们收集了超过20类数据用于预训练，100类数据用于微调，以提升嵌入模型的性能和泛化能力。在海量文本嵌入基准（MTEB）中文和英文上的广泛评估表明，我们的模型显著优于同等大小的其他模型，并能与3倍、14倍、18倍和26倍大的嵌入模型竞争，为参数小于1B的多功能紧凑型嵌入模型树立了新标准。", "summary": "KaLM-Embedding-V2是一种紧凑且多功能的文本嵌入模型，通过优化架构（全双向Transformer和均值池化）、采用多阶段训练（预训练、微调、模型融合）、引入焦点式重加权和在线难负样本混合策略，并利用大规模多样化数据集进行训练。该模型在MTEB中英文基准测试中表现出色，超越了同等大小的模型，并能与远大于其参数量的模型相媲美，为紧凑型高性能嵌入模型树立了新标准。", "keywords": "文本嵌入, KaLM-Embedding-V2, 多阶段训练, Transformer, 难负样本", "comments": "这篇论文通过结合架构创新（全双向Transformer）、精细的多阶段训练流程（预训练、微调、模型融合）、先进的学习策略（焦点重加权、在线难负样本）以及大规模多样化数据，成功地构建了一个在性能和效率之间取得极佳平衡的嵌入模型。其创新性在于将多种先进技术有效整合，显著提升了紧凑型模型的竞争力，使其能够与大型模型匹敌，这对于资源受限的应用场景具有重要意义。"}}
{"id": "2506.21206", "title": "Robust and efficient pre-processing techniques for particle-based methods including dynamic boundary generation", "authors": ["Niklas S. Neher", "Erik Faulhaber", "Sven Berger", "Christian Weißenfels", "Gregor J. Gassner", "Michael Schlottke-Lakemper"], "summary": "Obtaining high-quality particle distributions for stable and accurate\nparticle-based simulations poses significant challenges, especially for complex\ngeometries. We introduce a preprocessing technique for 2D and 3D geometries,\noptimized for smoothed particle hydrodynamics (SPH) and other particle-based\nmethods. Our pipeline begins with the generation of a resolution-adaptive point\ncloud near the geometry's surface employing a face-based neighborhood search.\nThis point cloud forms the basis for a signed distance field, enabling\nefficient, localized computations near surface regions. To create an initial\nparticle configuration, we apply a hierarchical winding number method for fast\nand accurate inside-outside segmentation. Particle positions are then relaxed\nusing an SPH-inspired scheme, which also serves to pack boundary particles.\nThis ensures full kernel support and promotes isotropic distributions while\npreserving the geometry interface. By leveraging the meshless nature of\nparticle-based methods, our approach does not require connectivity information\nand is thus straightforward to integrate into existing particle-based\nframeworks. It is robust to imperfect input geometries and memory-efficient\nwithout compromising performance. Moreover, our experiments demonstrate that\nwith increasingly higher resolution, the resulting particle distribution\nconverges to the exact geometry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21206v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21206v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于粒子方法的鲁棒高效预处理技术，包括动态边界生成", "tldr": "本文提出了一种用于2D和3D几何体的鲁棒高效预处理技术，旨在为SPH和其他粒子方法生成高质量的粒子分布，即使面对复杂或不完美的输入几何体也能有效工作。", "motivation": "为稳定准确的粒子基模拟获取高质量的粒子分布，特别是对于复杂几何体，面临重大挑战。", "method": "该方法首先生成几何体表面附近的分辨率自适应点云，然后利用该点云构建符号距离场。接着，应用分层缠绕数方法进行快速准确的内外分割，并使用SPH启发式方案松弛粒子位置，同时实现边界粒子填充，确保核函数支持和各向同性分布。", "result": "该技术能够生成高质量的粒子分布，即使对于不完美的输入几何体也具有鲁棒性，并且内存效率高。实验表明，随着分辨率的提高，所得粒子分布收敛于精确的几何形状。", "conclusion": "所提出的预处理方法不依赖连接信息，易于集成到现有粒子框架中。它对不完美的输入几何体具有鲁棒性，且内存高效，同时不影响性能，并能确保粒子分布在更高分辨率下收敛到精确几何体。", "translation": "为稳定准确的粒子基模拟获取高质量的粒子分布，特别是对于复杂几何体，面临重大挑战。我们介绍了一种针对2D和3D几何体的预处理技术，该技术针对光滑粒子流体动力学（SPH）和其他粒子基方法进行了优化。我们的流程始于在几何体表面附近生成分辨率自适应点云，该过程采用基于面的邻域搜索。该点云构成了符号距离场的基础，从而能够在表面区域附近进行高效的局部计算。为了创建初始粒子配置，我们应用了一种分层缠绕数方法进行快速准确的内外分割。然后，使用SPH启发式方案松弛粒子位置，该方案也用于填充边界粒子。这确保了完整的核函数支持并促进了各向同性分布，同时保留了几何界面。通过利用粒子基方法的无网格特性，我们的方法不需要连接信息，因此易于集成到现有的粒子基框架中。它对不完美的输入几何体具有鲁棒性，并且内存高效，而不影响性能。此外，我们的实验表明，随着分辨率的不断提高，所得粒子分布收敛于精确的几何形状。", "summary": "本文提出了一种针对2D和3D复杂几何体的鲁棒高效预处理技术，旨在为SPH及其他粒子基模拟生成高质量的粒子分布。该方法通过生成分辨率自适应点云、构建符号距离场、应用分层缠绕数进行内外分割，并采用SPH启发式方案松弛粒子及填充边界粒子，确保了粒子分布的均匀性和几何界面的保持。该技术不依赖网格连接信息，易于集成，对不完美输入具有鲁棒性，且内存高效，并能确保高分辨率下粒子分布收敛于精确几何体。", "keywords": "粒子方法, 预处理, SPH, 粒子分布, 几何体处理", "comments": "该论文提出了一种创新的粒子预处理流程，其亮点在于解决了复杂几何体下粒子分布质量的挑战。方法结合了点云、符号距离场和改进的粒子松弛技术，实现了对不完美输入几何体的鲁棒性，并且无需网格连接信息，这大大简化了与现有粒子框架的集成。其高效性和对几何体收敛性的证明，使其在计算流体力学等领域具有重要应用潜力。"}}
{"id": "2506.20816", "title": "Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers", "authors": ["Furkan Mumcu", "Yasin Yilmaz"], "summary": "Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input\ndesigns with limited noise budgets. While numerous successful attacks with\nsubtle modifications to original input have been proposed, defense techniques\nagainst these attacks are relatively understudied. Existing defense approaches\neither focus on improving DNN robustness by negating the effects of\nperturbations or use a secondary model to detect adversarial data. Although\nequally important, the attack detection approach, which is studied in this\nwork, provides a more practical defense compared to the robustness approach. We\nshow that the existing detection methods are either ineffective against the\nstate-of-the-art attack techniques or computationally inefficient for real-time\nprocessing. We propose a novel universal and efficient method to detect\nadversarial examples by analyzing the varying degrees of impact of attacks on\ndifferent DNN layers. {Our method trains a lightweight regression model that\npredicts deeper-layer features from early-layer features, and uses the\nprediction error to detect adversarial samples.} Through theoretical arguments\nand extensive experiments, we demonstrate that our detection method is highly\neffective, computationally efficient for real-time processing, compatible with\nany DNN architecture, and applicable across different domains, such as image,\nvideo, and audio.", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.17442", "pdf_url": "http://arxiv.org/pdf/2506.20816v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20816v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "神经网络层非均匀影响下的对抗性数据通用高效检测", "tldr": "提出了一种通过分析对抗性攻击对DNN不同层的影响来高效检测对抗性样本的新方法。", "motivation": "深度神经网络（DNNs）容易受到对抗性攻击，现有防御方法（提高DNN鲁棒性或使用辅助模型检测）要么对最新攻击无效，要么计算效率低下，无法进行实时处理。本研究旨在提供一种更实用、高效的攻击检测方法。", "method": "本文提出了一种新颖的通用高效方法来检测对抗性样本，通过分析攻击对不同DNN层产生的不同程度的影响。具体而言，该方法训练一个轻量级回归模型，该模型从早期层特征预测更深层特征，并利用预测误差来检测对抗性样本。", "result": "该检测方法被证明是高效的、计算效率高（适用于实时处理）、与任何DNN架构兼容，并且适用于图像、视频和音频等不同领域。", "conclusion": "本文提出了一种通用且高效的对抗性数据检测方法，通过分析对抗性攻击对网络层的非均匀影响，能够有效应对现有攻击并满足实时处理需求，具有广泛的应用前景。", "translation": "深度神经网络（DNN）对具有有限噪声预算的对抗性输入设计非常脆弱。虽然已经提出了许多对原始输入进行细微修改的成功攻击，但针对这些攻击的防御技术相对研究不足。现有的防御方法要么通过消除扰动的影响来提高DNN的鲁棒性，要么使用辅助模型来检测对抗性数据。尽管同样重要，但本研究中探讨的攻击检测方法与鲁棒性方法相比，提供了一种更实用的防御。我们发现，现有检测方法要么对最先进的攻击技术无效，要么计算效率低下，无法进行实时处理。我们提出了一种新颖的通用高效方法，通过分析攻击对不同DNN层产生的不同程度的影响来检测对抗性样本。我们的方法训练了一个轻量级回归模型，该模型从早期层特征预测更深层特征，并利用预测误差来检测对抗性样本。通过理论论证和大量实验，我们证明我们的检测方法高效、计算效率高（适用于实时处理）、与任何DNN架构兼容，并且适用于图像、视频和音频等不同领域。", "summary": "本文提出了一种新颖、通用且高效的对抗性数据检测方法，旨在解决深度神经网络在对抗性攻击下的脆弱性问题。该方法通过分析对抗性攻击对深度神经网络不同层产生的非均匀影响来识别对抗性样本，具体实现为训练一个轻量级回归模型，利用早期层特征预测深层特征并基于预测误差进行检测。实验证明，该方法在效果和计算效率上均表现出色，适用于实时处理，并兼容多种DNN架构及跨模态数据。", "keywords": "对抗性样本检测, 深度神经网络, 特征预测, 实时处理, 通用防御", "comments": "本文提出了一种新颖的对抗性样本检测视角，即利用攻击对网络层非均匀影响的特性。其创新点在于通过轻量级回归模型预测层间特征并利用预测误差进行检测，避免了传统防御方法的局限性。该方法的通用性和高效性对于实际部署具有重要意义，尤其是在实时处理场景下。"}}
{"id": "2506.21297", "title": "Exploring Micro Frontends: A Case Study Application in E-Commerce", "authors": ["Ricardo Hideki Hangai Kojo", "Luiz Fernando Corte Real", "Renato Cordeiro Ferreira", "Thatiane de Oliveira Rosa", "Alfredo Goldman"], "summary": "In the micro frontends architectural style, the frontend is divided into\nsmaller components, which can range from a simple button to an entire page. The\ngoal is to improve scalability, resilience, and team independence, albeit at\nthe cost of increased complexity and infrastructure demands. This paper seeks\nto understand when it is worth adopting micro frontends, particularly in the\ncontext of industry. To achieve this, we conducted an investigation into the\nstate of the art of micro frontends, based on both academic and gray\nliterature. We then implemented this architectural style in a marketplace for\nhandcrafted products, which already used microservices. Finally, we evaluated\nthe implementation through a semi-open questionnaire with the developers. At\nthe studied marketplace company, the need for architectural change arose due to\nthe tight coupling between their main system (a Java monolith) and a dedicated\nfrontend system. Additionally, there were deprecated technologies and poor\ndeveloper experience. To address these issues, the micro frontends architecture\nwas adopted, along with the API Gateway and Backend for Frontend patterns, and\ntechnologies such as Svelte and Fastify. Although the adoption of Micro\nFrontends was successful, it was not strictly necessary to meet the company's\nneeds. According to the analysis of the mixed questionnaire responses, other\nalternatives, such as a monolithic frontend, could have achieved comparable\nresults. What made adopting micro frontends the most convenient choice in the\ncompany's context was the monolith strangulation and microservices adoption,\nwhich facilitated implementation through infrastructure reuse and knowledge\nsharing between teams.", "comment": "11 pages, 2 figures (2 diagrams), submitted to the workshop AMP 2025", "pdf_url": "http://arxiv.org/pdf/2506.21297v1", "categories": ["cs.SE", "cs.DC", "D.2.11; D.2.13; D.2.7"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21297v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "探索微前端：电子商务中的案例研究应用", "tldr": "本文通过对现有文献的调查和在一个已采用微服务的电商平台进行案例研究，评估了微前端架构的适用性。研究发现，虽然微前端的采用是成功的，但并非总是必需的，其便利性主要源于与现有微服务和单体解耦策略的协同作用。", "motivation": "了解在何种情况下值得采用微前端架构，特别是在工业环境中。研究动机源于一家电商平台面临的挑战，包括其主系统与前端系统之间的紧密耦合、技术过时以及开发者体验不佳等问题。", "method": "首先，对微前端的最新技术现状进行了调查，包括学术文献和灰色文献。接着，在一个已使用微服务的电商平台中实施了微前端架构，并结合了API网关和前端后端（BFF）模式，以及Svelte和Fastify等技术。最后，通过对开发人员进行半开放式问卷调查来评估实施效果。", "result": "微前端的采用是成功的，但并非严格必要以满足公司需求。分析混合问卷反馈表明，其他替代方案（如单体前端）可能也能达到类似效果。在公司背景下，采用微前端最方便的原因是单体解耦和微服务的采用，这通过基础设施重用和团队间的知识共享促进了实施。", "conclusion": "微前端架构的成功采用并非总是严格必需的，其他替代方案可能也能达到类似效果。然而，当公司已有单体解耦和微服务等现有架构模式时，微前端的采用会变得更加便利和高效，因为它能促进基础设施重用和知识共享。", "translation": "在微前端架构风格中，前端被划分为更小的组件，范围可以从简单的按钮到整个页面。其目标是提高可扩展性、弹性以及团队独立性，尽管代价是增加了复杂性和基础设施需求。本文旨在理解何时值得采纳微前端，特别是在工业背景下。为此，我们对微前端的最新技术现状进行了调查，基于学术和灰色文献。随后，我们在一个已使用微服务的手工制品市场中实施了这种架构风格。最后，我们通过对开发人员进行半开放式问卷调查来评估了此次实施。在所研究的市场公司中，由于其主系统（一个Java单体）与专用前端系统之间紧密耦合，以及存在废弃技术和糟糕的开发者体验，因此出现了架构变革的需求。为解决这些问题，采用了微前端架构，同时结合了API网关和前端后端（Backend for Frontend）模式，以及Svelte和Fastify等技术。尽管微前端的采用是成功的，但它并非严格必要以满足公司的需求。根据对混合问卷反馈的分析，其他替代方案，例如单体前端，可能也能达到类似效果。在公司背景下，使微前端成为最便捷选择的原因是单体解耦和微服务的采用，这通过基础设施重用和团队间的知识共享促进了实施。", "summary": "本文探讨了微前端架构在电子商务领域的应用，并通过一个真实案例研究评估了其适用性。研究首先调查了微前端的最新进展，随后在一个已采用微服务的电商平台实施了微前端，并使用问卷调查对开发者进行了评估。结果显示，尽管微前端的实施是成功的，但并非公司解决现有架构问题的唯一或严格必需的方案。其便利性主要体现在与现有微服务架构和单体解耦策略的协同作用，这有助于基础设施复用和知识共享。", "keywords": "微前端, 电子商务, 案例研究, 架构, 微服务", "comments": "该论文通过实际案例研究，深入探讨了微前端在工业场景下的适用性，而非仅仅停留在理论层面。其创新之处在于，它不仅验证了微前端的可行性，更重要的是，它指出了微前端并非总是最佳或唯一方案，而是其价值会随着现有架构基础（如微服务和单体解耦）而显著提升。这为企业在考虑微前端转型时提供了宝贵的实践指导，强调了背景和协同效应的重要性。"}}
{"id": "2506.21057", "title": "Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions", "authors": ["Zhuochen Miao", "Jun Lv", "Hongjie Fang", "Yang Jin", "Cewu Lu"], "summary": "Imitation learning has emerged as a powerful paradigm in robot manipulation,\nyet its generalization capability remains constrained by object-specific\ndependencies in limited expert demonstrations. To address this challenge, we\npropose knowledge-driven imitation learning, a framework that leverages\nexternal structural semantic knowledge to abstract object representations\nwithin the same category. We introduce a novel semantic keypoint graph as a\nknowledge template and develop a coarse-to-fine template-matching algorithm\nthat optimizes both structural consistency and semantic similarity. Evaluated\non three real-world robotic manipulation tasks, our method achieves superior\nperformance, surpassing image-based diffusion policies with only one-quarter of\nthe expert demonstrations. Extensive experiments further demonstrate its\nrobustness across novel objects, backgrounds, and lighting conditions. This\nwork pioneers a knowledge-driven approach to data-efficient robotic learning in\nreal-world settings. Code and more materials are available on\nhttps://knowledge-driven.github.io/.", "comment": "IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21057v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21057v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "知识驱动的模仿学习：实现跨多样化条件的泛化", "tldr": "本文提出了一种知识驱动的模仿学习框架，通过利用外部结构化语义知识来提高机器人操作的泛化能力和数据效率。", "motivation": "模仿学习在机器人操作中表现强大，但其泛化能力受限于有限专家演示中对特定对象的依赖。", "method": "本文提出了知识驱动的模仿学习框架，该框架利用外部结构化语义知识来抽象同一类别内的对象表示。具体地，引入了一种新颖的语义关键点图作为知识模板，并开发了一种粗到细的模板匹配算法，该算法优化了结构一致性和语义相似性。", "result": "在三项真实世界机器人操作任务中，该方法取得了卓越的性能，仅用四分之一的专家演示就超越了基于图像的扩散策略。广泛的实验进一步证明了其在新型对象、背景和光照条件下的鲁棒性。", "conclusion": "这项工作开创了一种知识驱动的方法，用于在真实世界环境中实现数据高效的机器人学习。", "translation": "模仿学习已成为机器人操作中一种强大的范式，但其泛化能力仍受限于有限专家演示中的对象特定依赖。为了解决这一挑战，我们提出了知识驱动的模仿学习，一个利用外部结构化语义知识来抽象同一类别内对象表示的框架。我们引入了一种新颖的语义关键点图作为知识模板，并开发了一种粗到细的模板匹配算法，该算法优化了结构一致性和语义相似性。在三项真实世界机器人操作任务中进行评估，我们的方法取得了卓越的性能，仅用四分之一的专家演示就超越了基于图像的扩散策略。广泛的实验进一步证明了其在新型对象、背景和光照条件下的鲁棒性。这项工作开创了一种知识驱动的方法，用于在真实世界环境中实现数据高效的机器人学习。代码和更多材料可在 https://knowledge-driven.github.io/ 获取。", "summary": "本文提出了一种名为“知识驱动的模仿学习”的新框架，旨在解决传统模仿学习在机器人操作中泛化能力受限的问题。该方法利用外部结构化语义知识，通过引入语义关键点图作为知识模板和开发粗到细的模板匹配算法，实现了对同一类别内对象表示的抽象。实验证明，该方法在真实世界机器人任务中表现优越，显著减少了所需的专家演示数量，并展现出对新对象、背景和光照条件的强大鲁棒性，为数据高效的机器人学习开辟了新途径。", "keywords": "模仿学习, 知识驱动, 泛化, 机器人操作, 语义关键点图", "comments": "这项工作通过引入外部语义知识和创新的语义关键点图，有效解决了模仿学习中泛化能力和数据效率的挑战。其在仅需少量专家演示的情况下超越了现有方法，并在多样化条件下表现出鲁棒性，这对于实际机器人部署具有重要意义。该研究为知识驱动的机器人学习奠定了基础。"}}
{"id": "2506.21436", "title": "Succinct Preferential Attachment Graphs", "authors": ["Ziad Ismaili Alaoui", "Namrata", "Sebastian Wild"], "summary": "Computing over compressed data combines the space saving of data compression\nwith efficient support for queries directly on the compressed representation.\nSuch data structures are widely applied in text indexing and have been\nsuccessfully generalised to trees. For graphs, support for computing over\ncompressed data remains patchy; typical results in the area of succinct data\nstructures are restricted to a specific class of graphs and use the same,\nworst-case amount of space for any graph from this class.\n  In this work, we design a data structure whose space usage automatically\nimproves with the compressibility of the graph at hand, while efficiently\nsupporting navigational operations (simulating adjacency-list access).\nSpecifically, we show that the space usage approaches the instance-optimal\nspace when the graph is drawn according to the classic Barab\\'asi-Albert model\nof preferential-attachment graphs. Our data-structure techniques also work for\narbitrary graphs, guaranteeing a size asymptotically no larger than an\nentropy-compressed edge list. A key technical contribution is the careful\nanalysis of the instance-optimal space usage.", "comment": "WG 2025", "pdf_url": "http://arxiv.org/pdf/2506.21436v1", "categories": ["cs.DS", "cs.IT", "math.IT", "math.PR"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21436v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "简洁的优先连接图", "tldr": "本文设计了一种新的图数据结构，其空间使用量能根据图的可压缩性自动优化，尤其适用于优先连接图，同时支持高效导航操作。", "motivation": "现有压缩图数据结构通常局限于特定图类，并且对该类中的所有图都使用相同的最坏情况空间量。研究的动机在于设计一种空间使用量能随图可压缩性自动改进，并能高效支持查询操作的图数据结构。", "method": "本文设计了一种新的数据结构，其空间使用量能够随着给定图的可压缩性自动优化。研究方法包括分析该数据结构在经典Barabási-Albert优先连接图模型下的性能，证明其空间使用量接近实例最优空间，并验证其对任意图的适用性，保证大小不大于熵压缩的边列表。关键技术贡献是对实例最优空间使用量的仔细分析。", "result": "设计的数据结构在图根据经典Barabási-Albert优先连接图模型绘制时，其空间使用量接近实例最优空间。对于任意图，该数据结构保证其大小渐近不大于熵压缩的边列表。此外，它还能高效支持导航操作（模拟邻接表访问）。", "conclusion": "本文成功设计了一种图数据结构，该结构能够根据图的可压缩性自动优化空间使用，并在优先连接图上实现了接近实例最优的空间效率，同时保持了高效的导航操作支持。", "translation": "对压缩数据进行计算结合了数据压缩的空间节省和直接在压缩表示上高效支持查询的优点。这类数据结构广泛应用于文本索引，并已成功推广到树。对于图，对压缩数据进行计算的支持仍然不完善；简洁数据结构领域的典型结果仅限于特定类别的图，并且对该类别中的任何图都使用相同的最坏情况空间量。\n在这项工作中，我们设计了一种数据结构，其空间使用量会随着当前图的可压缩性自动提高，同时有效支持导航操作（模拟邻接表访问）。具体来说，我们表明当图根据经典的Barabási-Albert优先连接图模型绘制时，其空间使用量接近实例最优空间。我们的数据结构技术也适用于任意图，保证其大小渐近不大于熵压缩的边列表。一个关键的技术贡献是对实例最优空间使用量的仔细分析。", "summary": "本文提出了一种新颖的图数据结构，旨在克服现有简洁图结构在空间效率上的局限性。该数据结构能够根据图的可压缩性自动调整其空间使用量，特别是在Barabási-Albert优先连接图上实现了接近实例最优的空间效率，并对任意图保证了渐近高效的空间占用（不大于熵压缩的边列表）。同时，它能高效支持图的导航操作。", "keywords": "简洁数据结构, 优先连接图, 图压缩, 实例最优空间", "comments": "这项工作的创新之处在于其设计的图数据结构能够实现“实例最优”的空间使用，即空间效率随图的可压缩性而提高，而非依赖于最坏情况或特定图类。这对于处理具有不同可压缩性的实际图数据具有重要意义。对实例最优空间使用量的严格分析是其重要的技术贡献。"}}
{"id": "2506.21449", "title": "exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design", "authors": ["Maxim Moraru", "Weiyi Xia", "Zhuo Ye", "Feng Zhang", "Yongxin Yao", "Ying Wai Li", "Cai-Zhuang Wang"], "summary": "exa-AMD is a Python-based application designed to accelerate the discovery\nand design of functional materials by integrating AI/ML tools, materials\ndatabases, and quantum mechanical calculations into scalable, high-performance\nworkflows. The execution model of exa-AMD relies on Parsl, a task-parallel\nprogramming library that enables a flexible execution of tasks on any computing\nresource from laptops to supercomputers. By using Parsl, exa-AMD is able to\ndecouple the workflow logic from execution configuration, thereby empowering\nresearchers to scale their workflows without having to reimplement them for\neach system.", "comment": "We intend to publish the paper to the Journal of Open Source Software", "pdf_url": "http://arxiv.org/pdf/2506.21449v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21449v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "exa-AMD：一个用于加速AI辅助材料发现与设计的可扩展工作流", "tldr": "exa-AMD是一个基于Python的应用，利用Parsl加速AI辅助的材料发现与设计，实现从笔记本到超级计算机的可扩展工作流。", "motivation": "旨在加速功能材料的发现与设计。", "method": "exa-AMD是一个基于Python的应用，通过整合AI/ML工具、材料数据库和量子力学计算来创建可扩展、高性能的工作流。其执行模型依赖于Parsl，一个任务并行编程库，使得工作流逻辑与执行配置解耦，从而能在各种计算资源上灵活扩展。", "result": "exa-AMD能够实现工作流在从笔记本到超级计算机的任何计算资源上的灵活执行和扩展，无需为每个系统重新实现工作流。", "conclusion": "exa-AMD通过其可扩展的工作流和对Parsl的利用，显著简化并加速了AI辅助的材料发现与设计过程。", "translation": "exa-AMD是一个基于Python的应用程序，旨在通过将AI/ML工具、材料数据库和量子力学计算集成到可扩展、高性能的工作流中，来加速功能材料的发现和设计。exa-AMD的执行模型依赖于Parsl，这是一个任务并行编程库，它能够使任务在从笔记本电脑到超级计算机的任何计算资源上灵活执行。通过使用Parsl，exa-AMD能够将工作流逻辑与执行配置解耦，从而使研究人员能够在不为每个系统重新实现工作流的情况下扩展其工作流。", "summary": "exa-AMD是一个Python应用程序，旨在通过整合AI/ML工具、材料数据库和量子力学计算来加速功能材料的发现与设计。它利用Parsl任务并行库，实现了工作流逻辑与执行配置的分离，从而使研究人员能够在各种计算资源上轻松扩展其工作流，无需重复开发。", "keywords": "材料发现, AI/ML, 可扩展工作流, Parsl, 量子力学计算", "comments": "exa-AMD的创新之处在于其通过Parsl库实现了AI辅助材料发现工作流的高度可扩展性和灵活性，这对于在不同计算环境下部署和加速材料研究至关重要。它解决了在多变计算资源上重新实现工作流的痛点。"}}
{"id": "2506.20771", "title": "Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models", "authors": ["Xinghao Dong", "Huchen Yang", "Jin-Long Wu"], "summary": "We propose a latent score-based generative AI framework for learning\nstochastic, non-local closure models and constitutive laws in nonlinear\ndynamical systems of computational mechanics. This work addresses a key\nchallenge of modeling complex multiscale dynamical systems without a clear\nscale separation, for which numerically resolving all scales is prohibitively\nexpensive, e.g., for engineering turbulent flows. While classical closure\nmodeling methods leverage domain knowledge to approximate subgrid-scale\nphenomena, their deterministic and local assumptions can be too restrictive in\nregimes lacking a clear scale separation. Recent developments of\ndiffusion-based stochastic models have shown promise in the context of closure\nmodeling, but their prohibitive computational inference cost limits practical\napplications for many real-world applications. This work addresses this\nlimitation by jointly training convolutional autoencoders with conditional\ndiffusion models in the latent spaces, significantly reducing the\ndimensionality of the sampling process while preserving essential physical\ncharacteristics. Numerical results demonstrate that the joint training approach\nhelps discover a proper latent space that not only guarantees small\nreconstruction errors but also ensures good performance of the diffusion model\nin the latent space. When integrated into numerical simulations, the proposed\nstochastic modeling framework via latent conditional diffusion models achieves\nsignificant computational acceleration while maintaining comparable predictive\naccuracy to standard diffusion models in physical spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20771v1", "categories": ["cs.LG", "math.DS", "physics.comp-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20771v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基于潜在得分生成模型的非线性动力系统随机非局部闭合建模", "tldr": "提出了一种基于潜在得分生成AI框架，用于非线性动力系统的随机非局部闭合建模，通过在潜在空间中联合训练自编码器和扩散模型，显著降低计算成本，同时保持预测精度。", "motivation": "现有方法在缺乏明确尺度分离的多尺度动力系统建模中存在局限性。经典闭合模型假设过于严格，而扩散模型的计算成本过高，限制了实际应用。", "method": "提出一个潜在得分生成AI框架，通过在潜在空间中联合训练卷积自编码器（convolutional autoencoders）和条件扩散模型（conditional diffusion models）。这种方法旨在显著降低采样过程的维度，同时保留物理特性。", "result": "联合训练方法成功发现了合适的潜在空间，确保了较小的重建误差和潜在空间中扩散模型的良好性能。当集成到数值模拟中时，该框架显著加速了计算，并保持了与物理空间中标准扩散模型相当的预测精度。", "conclusion": "通过在潜在空间中联合训练自编码器和条件扩散模型，可以有效地为非线性动力系统构建随机非局部闭合模型，克服了传统方法和现有扩散模型在计算效率上的限制，同时保持了高精度。", "translation": "我们提出了一种潜在得分生成AI框架，用于计算力学中非线性动力系统的随机、非局部闭合模型和本构定律的学习。这项工作解决了在没有明确尺度分离的情况下建模复杂多尺度动力系统的关键挑战，因为数值求解所有尺度是极其昂贵的，例如工程湍流。虽然经典闭合建模方法利用领域知识来近似亚网格尺度现象，但其确定性和局部假设在缺乏明确尺度分离的区域可能过于严格。扩散基随机模型的最新发展在闭合建模方面展现出前景，但其过高的计算推理成本限制了许多实际应用的落地。这项工作通过在潜在空间中联合训练卷积自编码器和条件扩散模型来解决这一限制，显著降低了采样过程的维度，同时保留了基本的物理特性。数值结果表明，联合训练方法有助于发现一个合适的潜在空间，该空间不仅保证了小的重建误差，而且确保了潜在空间中扩散模型的良好性能。当集成到数值模拟中时，所提出的通过潜在条件扩散模型实现的随机建模框架在保持与物理空间中标准扩散模型相当的预测精度的同时，实现了显著的计算加速。", "summary": "这篇论文提出了一种基于潜在得分生成AI的框架，用于非线性动力系统的随机非局部闭合建模。针对传统方法在多尺度系统建模中存在的局限性以及现有扩散模型计算成本高昂的问题，作者通过在潜在空间中联合训练卷积自编码器和条件扩散模型，有效降低了计算复杂度，同时保持了模型的预测精度。数值结果表明，该方法在计算效率和准确性方面均表现出色，为复杂多尺度动力系统建模提供了新的解决方案。", "keywords": "随机闭合模型, 潜在得分生成模型, 非线性动力系统, 扩散模型, 自编码器", "comments": "这项工作创新性地将自编码器与条件扩散模型结合，在潜在空间进行闭合建模，有效解决了多尺度动力系统建模中计算成本过高的问题。其重要性在于为工程湍流等复杂系统提供了更高效、更准确的数值模拟方法，有望推动计算力学领域的发展。"}}
{"id": "2506.21500", "title": "Devising a solution to the problems of Cancer awareness in Telangana", "authors": ["Priyanka Avhad", "Vedanti Kshirsagar", "Urvi Ranjan", "Mahek Nakhua"], "summary": "According to the data, the percent of women who underwent screening for\ncervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3\npercent, 0.3 percent and 2.3 percent respectively. Although early detection is\nthe only way to reduce morbidity and mortality, people have very low awareness\nabout cervical and breast cancer signs and symptoms and screening practices. We\ndeveloped an ML classification model to predict if a person is susceptible to\nbreast or cervical cancer based on demographic factors. We devised a system to\nprovide suggestions for the nearest hospital or Cancer treatment centres based\non the users location or address. In addition to this, we can integrate the\nhealth card to maintain medical records of all individuals and conduct\nawareness drives and campaigns. For ML classification models, we used decision\ntree classification and support vector classification algorithms for cervical\ncancer susceptibility and breast cancer susceptibility respectively. Thus, by\ndevising this solution we come one step closer to our goal which is spreading\ncancer awareness, thereby, decreasing the cancer mortality and increasing\ncancer literacy among the people of Telangana.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21500v1", "categories": ["cs.LG", "cs.CY", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21500v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "制定解决特伦甘纳邦癌症意识问题的方案", "tldr": "针对特伦甘纳邦癌症意识和筛查率低的问题，本文开发了一个基于机器学习的系统，用于预测癌症易感性、提供医院建议并整合健康记录，以提高癌症意识和降低死亡率。", "motivation": "特伦甘纳邦宫颈癌、乳腺癌和口腔癌的筛查率极低（2020年分别为3.3%、0.3%和2.3%）。尽管早期检测是降低发病率和死亡率的唯一途径，但当地民众对癌症症状和筛查实践的认识非常低。", "method": "开发了一个机器学习分类模型，根据人口统计学因素预测个人对乳腺癌或宫颈癌的易感性，分别使用了决策树分类和支持向量分类算法。同时，设计了一个系统，根据用户位置提供最近的医院或癌症治疗中心建议，并计划整合健康卡以维护个人医疗记录和开展癌症宣传活动。", "result": "开发了一个基于机器学习的癌症易感性预测模型，并设计了一个提供医院建议和整合医疗记录的综合系统。", "conclusion": "通过开发此解决方案，有助于传播癌症意识，从而降低癌症死亡率并提高特伦甘纳邦人民的癌症素养。", "translation": "根据数据显示，2020年特伦甘纳邦接受宫颈癌、乳腺癌和口腔癌筛查的女性比例分别为3.3%、0.3%和2.3%。尽管早期检测是降低发病率和死亡率的唯一途径，但人们对宫颈癌和乳腺癌的体征、症状以及筛查实践的认识非常低。我们开发了一个机器学习分类模型，根据人口统计学因素预测一个人是否易患乳腺癌或宫颈癌。我们设计了一个系统，根据用户的位置或地址提供最近的医院或癌症治疗中心的建议。除此之外，我们还可以整合健康卡来维护所有个体的医疗记录，并开展意识宣传活动。对于机器学习分类模型，我们分别使用决策树分类和支持向量分类算法来预测宫颈癌易感性和乳腺癌易感性。因此，通过设计这个解决方案，我们离实现我们的目标又近了一步，即传播癌症意识，从而降低癌症死亡率并提高特伦甘纳邦人民的癌症素养。", "summary": "本文旨在解决印度特伦甘纳邦癌症筛查率低和公众癌症意识不足的问题。研究团队开发了一个机器学习分类模型，利用人口统计学因素预测个体对乳腺癌和宫颈癌的易感性，并分别采用了决策树和支持向量机算法。此外，还设计了一个系统，能够根据用户位置推荐最近的癌症治疗中心，并计划整合健康卡以管理医疗记录和组织癌症宣传活动。该解决方案旨在通过提高癌症意识来降低癌症死亡率并提升公众的癌症素养。", "keywords": "癌症意识, 机器学习, 癌症筛查, 特伦甘纳邦, 易感性预测", "comments": "该论文提出了一种结合机器学习和信息系统来解决公共健康问题的创新方法。其重要性在于直接针对特伦甘纳邦癌症筛查率低和意识不足的实际痛点。通过预测易感性和提供个性化医院建议，有望提高早期诊断率。此外，整合健康卡和开展宣传活动的设计也体现了全面性。然而，抽象中未提及模型的具体性能指标（如准确率、召回率等），也未详细说明数据来源和规模，这可能是未来研究需要补充的方面。"}}
{"id": "2506.20877", "title": "THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion", "authors": ["Calin Teodor Ioan"], "summary": "Monocular depth estimation methods traditionally train deep models to infer\ndepth directly from RGB pixels. This implicit learning often overlooks explicit\nmonocular cues that the human visual system relies on, such as occlusion\nboundaries, shading, and perspective. Rather than expecting a network to\ndiscover these cues unaided, we present ThirdEye, a cue-aware pipeline that\ndeliberately supplies each cue through specialised, pre-trained, and frozen\nnetworks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)\nequipped with a key-value working-memory module that weights them by\nreliability. An adaptive-bins transformer head then produces a high-resolution\ndisparity map. Because the cue experts are frozen, ThirdEye inherits large\namounts of external supervision while requiring only modest fine-tuning. This\nextended version provides additional architectural detail, neuroscientific\nmotivation, and an expanded experimental protocol; quantitative results will\nappear in a future revision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20877v1", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20877v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "THIRDEYE：基于脑启发式多阶段融合的线索感知单目深度估计", "tldr": "ThirdEye是一个利用预训练网络提供的显式单目线索，并通过脑启发式多阶段融合来估计深度的系统，解决了传统方法忽视人类视觉系统依赖的线索问题。", "motivation": "传统的单目深度估计方法通过隐式学习从RGB像素推断深度，常常忽略了人类视觉系统所依赖的显式单目线索（如遮挡边界、阴影和透视）。本文的动机是设计一个能够主动利用这些显式线索的深度估计系统。", "method": "ThirdEye是一个线索感知的流水线，通过专门的、预训练且冻结的网络提供显式线索。这些线索在一个三阶段皮层层级（V1->V2->V3）中融合，该层级配备了键值工作记忆模块，根据可靠性对线索进行加权。然后，一个自适应分箱Transformer头部生成高分辨率视差图。由于线索专家网络是冻结的，ThirdEye继承了大量的外部监督，同时只需要适度的微调。", "result": "定量结果将在未来的修订版中提供。", "conclusion": "Not mentioned in abstract", "translation": "单目深度估计方法传统上训练深度模型直接从RGB像素推断深度。这种隐式学习常常忽略了人类视觉系统所依赖的显式单目线索，例如遮挡边界、阴影和透视。我们没有期望网络在没有帮助的情况下发现这些线索，而是提出了ThirdEye，一个线索感知的流水线，通过专门的、预训练且冻结的网络主动提供每种线索。这些线索在一个三阶段皮层层级（V1->V2->V3）中融合，该层级配备了键值工作记忆模块，根据可靠性对它们进行加权。然后，一个自适应分箱Transformer头部生成高分辨率视差图。由于线索专家网络是冻结的，ThirdEye继承了大量的外部监督，同时只需要适度的微调。这个扩展版本提供了额外的架构细节、神经科学动机和扩展的实验协议；定量结果将在未来的修订版中出现。", "summary": "ThirdEye是一种新型的单目深度估计算法，旨在解决传统方法忽视人类视觉系统所利用的显式线索的问题。它通过预训练且冻结的专家网络主动提供遮挡边界、阴影和透视等线索，并在一个受大脑启发的V1->V2->V3三阶段皮层层级中融合这些线索，其中包含一个基于可靠性加权的键值工作记忆模块。最后，一个自适应分箱Transformer头部生成高分辨率视差图。该方法通过利用外部监督减少了微调需求。", "keywords": "单目深度估计, 线索感知, 脑启发, 多阶段融合, ThirdEye", "comments": "该论文的创新点在于其脑启发式的多阶段融合架构和显式线索感知的方法，这与传统深度学习的隐式学习形成对比。通过利用预训练的线索专家网络并将其冻结，该系统能够继承大量外部监督，从而减少了自身训练的需求，这在效率和性能上可能具有重要意义。然而，抽象中明确指出定量结果将在未来版本中提供，这意味着目前尚无法评估其实际性能。"}}
{"id": "2506.21458", "title": "Spatial Mental Modeling from Limited Views", "authors": ["Baiqiao Yin", "Qineng Wang", "Pingyue Zhang", "Jianshu Zhang", "Kangrui Wang", "Zihan Wang", "Jieyu Zhang", "Keshigeyan Chandrasegaran", "Han Liu", "Ranjay Krishna", "Saining Xie", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.", "comment": "Preprint version", "pdf_url": "http://arxiv.org/pdf/2506.21458v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21458v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从有限视角进行空间心理建模", "tldr": "视觉语言模型（VLMs）在从有限视角进行空间推理方面表现不佳；本研究引入了MindCube基准测试，并提出“先映射后推理”的方法，通过构建内部空间模型显著提高了VLMs的性能。", "motivation": "现有视觉语言模型（VLMs）无法像人类一样仅凭少量视图想象完整的场景，在空间推理任务中表现出接近随机的性能。人类会形成空间心理模型来推断布局、透视和运动。本研究旨在揭示并解决VLMs在这方面的关键差距。", "method": "本研究引入了MindCube基准测试，包含21,154个问题和3,268张图像，用于系统评估VLMs构建稳健空间心理模型的能力，包括表示位置（认知映射）、方向（视角采纳）和动态（心理模拟“假设”运动）。研究探索了三种方法来帮助VLMs近似空间心理模型：未见的中间视图、自然语言推理链和认知地图。其中，“先映射后推理”的协同方法效果显著，该方法联合训练模型，使其首先生成认知地图，然后在此基础上进行推理。通过添加强化学习，性能得到进一步提升。", "result": "现有VLMs在MindCube基准测试中表现出接近随机的性能。通过“先映射后推理”的方法，准确率从37.8%提高到60.8%（+23.0%）。加入强化学习后，性能进一步提升至70.7%（+32.9%）。", "conclusion": "本研究的关键见解是，通过主动构建和利用具有灵活推理过程的内部结构化空间表示（即空间心理模型），能够显著提高视觉语言模型对不可观察空间的理解能力。", "translation": "视觉语言模型（VLMs）能否像人类一样，仅凭少量视图就能想象出完整的场景？人类会形成空间心理模型，即未见空间的内部表征，以便对布局、透视和运动进行推理。我们新的MindCube基准测试包含3,268张图像上的21,154个问题，揭示了这一关键差距，现有VLMs在此基准上表现出接近随机的性能。我们使用MindCube系统评估了VLMs通过表示位置（认知映射）、方向（视角采纳）和动态（“假设”运动的心理模拟）来构建稳健空间心理模型的能力。然后，我们探索了三种方法来帮助VLMs近似空间心理模型，包括未见的中间视图、自然语言推理链和认知地图。显著的改进来自于一种协同方法——“先映射后推理”，该方法联合训练模型，使其首先生成认知地图，然后在此基础上进行推理。通过训练模型对这些内部地图进行推理，我们将准确率从37.8%提高到60.8%（+23.0%）。添加强化学习将性能进一步推高到70.7%（+32.9%）。我们的关键见解是，这种空间心理模型的脚手架，即主动构建和利用具有灵活推理过程的内部结构化空间表示，显著提高了对不可观察空间的理解。", "summary": "视觉语言模型（VLMs）在仅凭有限视图进行空间推理方面存在显著不足，无法像人类那样形成完整的空间心理模型。为解决此问题，本研究引入了MindCube，一个包含2万余问题的全新基准测试，揭示了现有VLMs在此任务上的低效表现。研究系统评估了VLMs构建空间心理模型的能力，并探索了多种改进方法。其中，“先映射后推理”的协同策略效果最佳，它通过联合训练模型生成内部认知地图并在此基础上进行推理，将准确率从37.8%提升至60.8%。进一步结合强化学习，性能可达70.7%。这表明，主动构建和利用内部结构化空间表示对于增强VLMs对不可观察空间的理解至关重要。", "keywords": "视觉语言模型, 空间推理, 心理模型, 认知地图, MindCube", "comments": "本论文通过引入MindCube基准测试，量化了当前视觉语言模型在复杂空间推理方面的关键局限性，具有重要的诊断价值。其提出的“先映射后推理”方法，灵感来源于人类认知过程，通过强制模型构建内部空间表示，为提升AI的空间理解能力提供了一个创新且有效的路径。显著的性能提升证明了这种结合显式空间建模与高级训练技术（如强化学习）的潜力，对于推动视觉语言模型在更复杂、更接近人类的推理任务上的发展具有重要意义。"}}
{"id": "2506.20696", "title": "IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "summary": "Elucidating the biomechanical behavior of the myocardium is crucial for\nunderstanding cardiac physiology, but cannot be directly inferred from clinical\nimaging and typically requires finite element (FE) simulations. However,\nconventional FE methods are computationally expensive and often fail to\nreproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed\nneural network (PINN) framework that integrates imaged motion consistency (IMC)\nwith FE modeling for patient-specific left ventricular (LV) biomechanics.\nCardiac motion is first estimated from MRI or echocardiography using either a\npre-trained attention-based network or an unsupervised cyclic-regularized\nnetwork, followed by extraction of motion modes. IMC-PINN-FE then rapidly\nestimates myocardial stiffness and active tension by fitting clinical pressure\nmeasurements, accelerating computation from hours to seconds compared to\ntraditional inverse FE. Based on these parameters, it performs FE modeling\nacross the cardiac cycle at 75x speedup. Through motion constraints, it matches\nimaged displacements more accurately, improving average Dice from 0.849 to\n0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE\nadvances previous PINN-FE models by introducing back-computation of material\nproperties and better motion fidelity. Using motion from a single subject to\nreconstruct shape modes also avoids the need for large datasets and improves\npatient specificity. IMC-PINN-FE offers a robust and efficient approach for\nrapid, personalized, and image-consistent cardiac biomechanical modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20696v1", "categories": ["physics.med-ph", "cs.AI", "eess.IV"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.20696v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "IMC-PINN-FE：一种用于患者特异性左心室有限元建模的物理信息神经网络，具有图像运动一致性和生物力学参数估计功能", "tldr": "IMC-PINN-FE是一种新的PINN框架，结合图像运动一致性和有限元建模，可快速、准确地对患者特异性左心室生物力学进行建模和参数估计。", "motivation": "阐明心肌的生物力学行为对理解心脏生理至关重要，但临床成像无法直接推断，且传统的有限元方法计算成本高昂，且往往无法重现观察到的心脏运动。", "method": "本文提出IMC-PINN-FE，一个物理信息神经网络（PINN）框架，它将图像运动一致性（IMC）与有限元（FE）建模相结合，用于患者特异性左心室（LV）生物力学。首先，使用预训练的基于注意力网络或无监督循环正则化网络从MRI或超声心动图估计心脏运动，然后提取运动模式。IMC-PINN-FE通过拟合临床压力测量值，快速估计心肌刚度和主动张力，将计算时间从数小时缩短到数秒。基于这些参数，它以75倍的速度在心动周期内执行有限元建模。", "result": "相较于传统逆向有限元，计算速度从数小时缩短到数秒，FE建模速度提升75倍。通过运动约束，与图像位移匹配更准确，平均Dice系数从0.849提高到0.927，同时保持真实的压力-容积行为。通过单个受试者的运动重建形状模式，避免了对大型数据集的需求，并提高了患者特异性。", "conclusion": "IMC-PINN-FE为快速、个性化和图像一致的心脏生物力学建模提供了一种稳健高效的方法。它通过引入材料属性的反向计算和更好的运动保真度，改进了以前的PINN-FE模型。", "translation": "阐明心肌的生物力学行为对于理解心脏生理至关重要，但无法直接从临床成像中推断，并且通常需要有限元（FE）模拟。然而，传统的有限元方法计算成本高昂，并且往往无法重现观察到的心脏运动。我们提出了IMC-PINN-FE，一个物理信息神经网络（PINN）框架，它将图像运动一致性（IMC）与有限元建模相结合，用于患者特异性左心室（LV）生物力学。心脏运动首先使用预训练的基于注意力网络或无监督循环正则化网络从MRI或超声心动图估计，然后提取运动模式。IMC-PINN-FE随后通过拟合临床压力测量值快速估计心肌刚度和主动张力，与传统逆向有限元相比，将计算时间从数小时缩短到数秒。基于这些参数，它以75倍的速度在心动周期内执行有限元建模。通过运动约束，它更准确地匹配图像位移，将平均Dice系数从0.849提高到0.927，同时保持真实的压力-容积行为。IMC-PINN-FE通过引入材料属性的反向计算和更好的运动保真度，改进了以前的PINN-FE模型。使用单个受试者的运动重建形状模式也避免了对大型数据集的需求，并提高了患者特异性。IMC-PINN-FE为快速、个性化和图像一致的心脏生物力学建模提供了一种稳健高效的方法。", "summary": "本文提出IMC-PINN-FE，一个结合物理信息神经网络、图像运动一致性和有限元建模的新框架，旨在解决传统有限元方法在患者特异性左心室生物力学建模中计算效率低和运动匹配不准确的问题。该方法通过从临床影像中估计心脏运动模式，并结合压力测量，快速估计心肌参数，显著加速计算并提高图像位移匹配精度（Dice系数从0.849提升至0.927）。IMC-PINN-FE无需大量数据集，增强了患者特异性，提供了一种高效、个性化且图像一致的心脏生物力学建模方案。", "keywords": "物理信息神经网络, 有限元建模, 左心室生物力学, 图像运动一致性, 患者特异性", "comments": "IMC-PINN-FE的创新之处在于将图像运动一致性引入到物理信息神经网络中，以提高有限元模型的准确性和效率。它解决了传统FE方法计算成本高昂和运动匹配不足的痛点，通过快速参数估计和高效建模，实现了患者特异性心脏生物力学分析。其优势在于显著的计算加速（从数小时到数秒）和更高的运动保真度，同时减少了对大量训练数据的依赖。"}}
{"id": "2506.21208", "title": "Adversarial Training: Enhancing Out-of-Distribution Generalization for Learning Wireless Resource Allocation", "authors": ["Shengjie Liu", "Chenyang Yang"], "summary": "Deep neural networks (DNNs) have widespread applications for optimizing\nresource allocation. Yet, their performance is vulnerable to distribution\nshifts between training and test data, say channels. In this letter, we resort\nto adversarial training (AT) for enhancing out-of-distribution (OOD)\ngeneralizability of DNNs trained in unsupervised manner. We reformulate AT to\ncapture the OOD degradation, and propose a one-step gradient ascent method for\nAT. The proposed method is validated by optimizing hybrid precoding. Simulation\nresults showcase the enhanced OOD performance of multiple kinds of DNNs across\nvarious channel distributions, when only Rayleigh fading channels are used for\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21208v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21208v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "对抗训练：增强无线资源分配学习的域外泛化能力", "tldr": "该文提出一种基于对抗训练的方法，用于提升深度神经网络在无线资源分配中对域外数据的泛化能力，并通过优化混合预编码验证了其有效性。", "motivation": "深度神经网络在资源分配优化中广泛应用，但其性能易受训练和测试数据之间分布偏移（如信道变化）的影响。", "method": "本文采用对抗训练（AT）来增强无监督训练的深度神经网络的域外（OOD）泛化能力。具体地，重新构建了AT以捕捉OOD性能下降，并提出了一种一步梯度上升法进行AT。所提出的方法通过优化混合预编码进行验证。", "result": "仿真结果表明，当仅使用瑞利衰落信道进行训练时，所提出的方法在各种信道分布下显著提升了多种深度神经网络的OOD性能。", "conclusion": "对抗训练能够有效增强深度神经网络在无线资源分配任务中的域外泛化能力，即使训练数据有限，也能在不同信道分布下保持良好性能。", "translation": "深度神经网络（DNNs）在优化资源分配方面有着广泛的应用。然而，它们的性能容易受到训练和测试数据之间分布偏移（例如信道）的影响。在这封信中，我们采用对抗训练（AT）来增强以无监督方式训练的DNNs的域外（OOD）泛化能力。我们重新构建了AT以捕捉OOD性能下降，并提出了一种一步梯度上升法进行AT。所提出的方法通过优化混合预编码得到了验证。仿真结果表明，当仅使用瑞利衰落信道进行训练时，多种深度神经网络在各种信道分布下都表现出增强的OOD性能。", "summary": "本文提出了一种基于对抗训练（AT）的新方法，旨在解决深度神经网络在无线资源分配任务中面临的训练与测试数据分布偏移问题，从而提升其域外（OOD）泛化能力。作者重新构建了AT以捕捉OOD退化，并引入一步梯度上升法。通过混合预编码的优化验证，仿真结果证实该方法显著增强了多种DNN在不同信道分布下的OOD性能，即使训练数据仅限于瑞利衰落信道。", "keywords": "对抗训练, 域外泛化, 无线资源分配, 深度神经网络, 混合预编码", "comments": "这篇论文通过引入对抗训练来解决深度神经网络在无线资源分配中域外泛化能力不足的问题，具有创新性。它针对数据分布偏移这一常见挑战提供了解决方案，尤其是在无线通信这种信道环境多变的领域，其重要性不言而喻。一步梯度上升法的提出也简化了训练过程。"}}
{"id": "2506.21191", "title": "Prompt-Guided Turn-Taking Prediction", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.", "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "pdf_url": "http://arxiv.org/pdf/2506.21191v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21191v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "提示词引导的对话轮次预测", "tldr": "该研究提出了一种基于Transformer的新型对话轮次预测模型，该模型可以通过文本提示（如“更快”、“更平静”）进行动态控制，提高了预测准确性并能有效调整轮次转换时机。", "motivation": "对话轮次预测模型是语音对话系统和会话机器人中的重要组成部分。当前的方法虽然利用基于Transformer的架构实时连续预测语音活动，但缺乏通过直观指令进行动态控制的能力。本研究旨在解决这一问题，通过文本提示实现对轮次预测的动态控制。", "method": "本研究提出了一种新模型，它建立在基于Transformer的语音活动预测（VAP）模型之上，并将文本提示嵌入到通道内Transformer和跨通道Transformer中。研究利用了超过950小时的人机对话数据进行评估，并由于现有数据集中缺乏文本提示数据，因此使用大型语言模型（LLM）生成了合成提示句。", "result": "实验结果表明，所提出的模型提高了预测准确性，并能根据文本提示有效地改变轮次转换的时机行为。", "conclusion": "该模型成功实现了通过文本提示动态控制对话轮次预测，提高了准确性和适应性。", "translation": "对话轮次预测模型是语音对话系统和会话机器人中的重要组成部分。最近的方法利用基于Transformer的架构来持续实时地预测语音活动。在这项研究中，我们提出了一种新颖的模型，该模型能够通过文本提示动态控制对话轮次预测。这种方法允许通过“更快”或“更平静”等指令进行直观和明确的控制，从而动态适应对话伙伴和上下文。所提出的模型建立在基于Transformer的语音活动预测（VAP）模型之上，将文本提示嵌入到通道内Transformer和跨通道Transformer中。我们使用超过950小时的人机对话数据评估了我们方法的可行性。由于现有数据集中没有可用于所提出方法的文本提示数据，我们利用大型语言模型（LLM）生成了合成提示句。实验结果表明，所提出的模型提高了预测准确性，并能根据文本提示有效地改变对话轮次转换的时机行为。", "summary": "本文提出了一种新颖的、基于Transformer的对话轮次预测模型，该模型允许通过“更快”或“更平静”等文本提示进行动态控制。该模型在Transformer-based语音活动预测（VAP）模型的基础上，将文本提示嵌入到通道内和跨通道Transformer中。研究在超过950小时的人机对话数据上进行了评估，并利用大型语言模型生成了合成提示数据。实验结果表明，该模型不仅提高了预测准确性，还能根据文本提示有效调整轮次转换的时机行为。", "keywords": "轮次预测, 文本提示, Transformer, 语音活动预测, 对话系统", "comments": "该研究的创新之处在于引入了通过文本提示动态、直观地控制对话轮次转换的能力，这对于构建更自然、更具适应性的对话系统具有重要意义。此外，利用大型语言模型生成合成提示数据，也为解决特定类型数据稀缺问题提供了一种实用的方法。"}}
{"id": "2506.21049", "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "comment": "Accepted by ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21049v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21049v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "电子商务查询分类的半监督可扩展统一框架", "tldr": "提出SSUF半监督统一框架，通过知识、标签和结构增强模块解决电商查询分类中信息不足、依赖点击行为和缺乏统一框架的问题，效果显著优于SOTA模型。", "motivation": "现有电商查询分类方法面临查询短、上下文不足、标签间信息无法利用导致先验信息不足的问题；多数方法依赖用户点击行为构建训练样本，导致马太效应；子任务缺乏统一框架，算法优化效率低。", "method": "提出半监督可扩展统一框架（SSUF），包含多个增强模块：知识增强模块利用世界知识增强查询表示；标签增强模块利用标签语义和半监督信号减少对后验标签的依赖；结构增强模块基于复杂标签关系增强标签表示。每个模块高度可插拔，可根据子任务增删输入特征。", "result": "广泛的离线和在线A/B实验结果表明，SSUF显著优于现有最先进的模型。", "conclusion": "SSUF有效解决了电商查询分类中的多项挑战，并通过其统一和模块化的设计，实现了显著的性能提升。", "translation": "查询分类，包括意图和类别预测等多个子任务，对电子商务应用至关重要。电子商务查询通常较短且缺乏上下文，并且标签之间的信息无法利用，导致建模的先验信息不足。大多数现有的工业查询分类方法依赖于用户的后验点击行为来构建训练样本，导致马太效应的恶性循环。此外，查询分类的子任务缺乏统一的框架，导致算法优化效率低下。\n在本文中，我们提出了一种新颖的半监督可扩展统一框架（SSUF），其中包含多个增强模块，以统一查询分类任务。知识增强模块利用世界知识来增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号来减少对后验标签的依赖。结构增强模块根据复杂的标签关系增强标签表示。每个模块都高度可插拔，可以根据每个子任务的需要添加或删除输入特征。我们进行了广泛的离线和在线A/B实验，结果表明SSUF显著优于最先进的模型。", "summary": "本文提出了一种名为SSUF的半监督可扩展统一框架，旨在解决电子商务查询分类中存在的查询信息不足、过度依赖用户点击行为以及缺乏统一优化框架的问题。SSUF包含知识增强、标签增强和结构增强三大模块，分别通过引入世界知识、利用标签语义和半监督信号以及增强标签关系表示来提升分类效果。该框架具有高度可插拔性，并经实验证明，其性能显著优于现有先进模型。", "keywords": "查询分类, 半监督学习, 统一框架, 电子商务, 信息增强", "comments": "这篇论文的创新点在于提出了一个统一的半监督框架来解决电商查询分类的多个痛点。其模块化的设计（知识、标签、结构增强）使得框架具有高度的灵活性和可扩展性，能够适应不同的子任务需求。通过减少对后验点击行为的依赖和利用半监督信号，有效缓解了数据稀疏和马太效应问题，对于工业界应用具有重要意义。"}}
{"id": "2506.20989", "title": "Can Gradient Descent Simulate Prompting?", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "comment": "14 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20989v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20989v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "梯度下降能否模拟提示？", "tldr": "研究表明，通过元训练语言模型，梯度下降可以模拟提示的效果，从而在某些任务上恢复甚至超越提示模型的性能。", "motivation": "语言模型整合新信息主要有两种方式：提示（prompting）和参数更新（如微调）。尽管提示在泛化和逻辑推理方面更有效，但参数更新没有长期存储成本。本文旨在探讨能否修改模型，使微调能够模拟提示的效果，以结合两者的优势。", "method": "本文描述了一种元训练语言模型的方法，使其梯度更新能够模拟对新信息进行条件化的效果。该方法利用基于梯度的元学习工具，但使用语言模型自身提示的预测作为目标，从而无需真实标签。", "result": "随后的梯度下降训练恢复了部分（有时是全部）提示模型的性能。具体表现为在“逆转诅咒”任务上显示出改进，并在单次梯度更新后能够回答关于文本段落的问题。", "conclusion": "研究结果表明，通过适当的初始化，梯度下降可以具有惊人的表达能力。这些发现为长上下文建模提供了新途径，并深入了解了基于梯度的学习的泛化能力。", "translation": "语言模型（LM）整合新信息主要有两种方式：改变其提示或改变其参数（例如通过微调）。参数更新不会产生模型更改的长期存储成本。然而，对于许多模型更新而言，提示明显更有效：提示模型可以从单个示例中稳健地泛化，并进行标准微调下不会出现的逻辑推理。那么，模型能否被修改，使得微调能够模拟提示呢？本文描述了一种元训练语言模型的方法，使得梯度更新能够模拟对新信息进行条件化的效果。我们的方法利用了基于梯度的元学习工具，但使用LM自身提示的预测作为目标，从而无需真实标签。随后的梯度下降训练恢复了部分（有时是全部）提示模型的性能——在“逆转诅咒”任务上显示出改进，并在单次梯度更新后回答了关于文本段落的问题。这些结果表明，通过适当的初始化，梯度下降可以具有惊人的表达能力。我们的结果为长上下文建模提供了新途径，并深入了解了基于梯度的学习的泛化能力。", "summary": "本文探讨了梯度下降能否模拟语言模型中提示的效果。研究提出了一种元训练语言模型的方法，使其通过梯度更新模仿提示对新信息的适应。该方法利用基于梯度的元学习，并以模型自身的提示预测为目标，无需真实标签。实验结果显示，通过这种方式，梯度下降训练能恢复提示模型的性能，并在特定任务上展现出改进，表明适当初始化下梯度下降的强大表达能力，为长上下文建模和梯度学习的泛化提供了新见解。", "keywords": "梯度下降, 提示, 微调, 元学习, 语言模型", "comments": "这篇论文的创新点在于提出了一个新颖的元训练框架，使得参数更新（微调）能够模仿提示（prompting）的效果，从而结合了两者的优势。它解决了提示有效但微调成本高的问题，通过让模型学习如何通过梯度更新来“内化”提示信息，这对于理解模型学习机制和开发更高效的语言模型更新方法具有重要意义。特别是，它无需真实标签，仅依赖模型自身的提示预测作为目标，这大大降低了数据需求。"}}
{"id": "2506.21241", "title": "On the coordinate system-dependence of the accuracy of symplectic numerical methods", "authors": ["Donát M. Takács", "Tamás Fülöp"], "summary": "Symplectic numerical methods have become a widely-used choice for the\naccurate simulation of Hamiltonian systems in various fields, including\ncelestial mechanics, molecular dynamics and robotics. Even though their\ncharacteristics are well-understood mathematically, relatively little attention\nhas been paid in general to the practical aspect of how the choice of\ncoordinates affects the accuracy of the numerical results, even though the\nconsequences can be computationally significant. The present article aims to\nfill this gap by giving a systematic overview of how coordinate transformations\ncan influence the results of simulations performed using symplectic methods. We\ngive a derivation for the non-invariance of the modified Hamiltonian of\nsymplectic methods under coordinate transformations, as well as a sufficient\ncondition for the non-preservation of a first integral corresponding to a\ncyclic coordinate for the symplectic Euler method. We also consider the\npossibility of finding order-compensating coordinate transformations that\nimprove the order of accuracy of a numerical method. Various numerical examples\nare presented throughout.", "comment": "24 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.21241v1", "categories": ["math.NA", "cs.NA", "physics.class-ph", "physics.comp-ph", "65P10, 70H15, 34A05"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21241v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于辛数值方法精度对坐标系的依赖性", "tldr": "本文系统性地探讨了坐标系选择如何影响辛数值方法的精度，并给出了修正哈密顿量非不变性和第一积分非守恒的推导。", "motivation": "辛数值方法在模拟哈密顿系统中应用广泛，但关于坐标系选择如何影响其精度这一实际问题，却鲜有关注。本文旨在填补这一空白。", "method": "本文系统性地概述了坐标变换如何影响辛数值方法模拟结果。具体方法包括：推导了辛方法修正哈密顿量在坐标变换下的非不变性；给出了辛欧拉方法对应循环坐标第一积分非守恒的充分条件；并探讨了寻找能提高数值方法精度的阶补偿坐标变换的可能性。", "result": "研究表明，辛方法的修正哈密顿量在坐标变换下是非不变的。对于辛欧拉方法，给出了对应循环坐标的第一积分非守恒的充分条件。文中还提出了寻找阶补偿坐标变换以提高精度的方法。", "conclusion": "本文系统地分析了辛数值方法精度对坐标系的依赖性，揭示了修正哈密顿量的非不变性和第一积分的非守恒性，并探讨了通过坐标变换提高精度的方法，强调了坐标系选择在实际应用中的重要性。", "translation": "辛数值方法已成为各种领域（包括天体力学、分子动力学和机器人技术）中哈密顿系统精确模拟的广泛选择。尽管它们的特性在数学上已得到充分理解，但通常很少有人关注坐标系选择如何影响数值结果的精度这一实际问题，尽管其后果可能在计算上非常显著。本文旨在通过系统地概述坐标变换如何影响使用辛方法进行的模拟结果来填补这一空白。我们推导了辛方法修正哈密顿量在坐标变换下的非不变性，以及辛欧拉方法对应循环坐标第一积分非守恒的充分条件。我们还考虑了寻找能够提高数值方法精度阶数的阶补偿坐标变换的可能性。文中提供了各种数值例子。", "summary": "本文系统地探讨了坐标系选择对辛数值方法精度的影响。研究推导了修正哈密顿量在坐标变换下的非不变性，并给出了辛欧拉方法中循环坐标第一积分非守恒的充分条件。此外，文章还探讨了通过阶补偿坐标变换来提升数值方法精度的可能性，并通过数值例子验证了这些发现。", "keywords": "辛数值方法, 坐标系依赖性, 修正哈密顿量, 第一积分, 精度", "comments": "本文填补了辛数值方法在实际应用中关于坐标系选择影响精度这一研究空白，具有重要的实践意义。其创新之处在于系统性地分析了修正哈密顿量的非不变性和第一积分的非守恒性，并提出了通过坐标变换优化精度的思路。"}}
{"id": "2506.21300", "title": "An object-centric core metamodel for IoT-enhanced event logs", "authors": ["Yannis Bertrand", "Christian Imenkamp", "Lukas Malburg", "Matthias Ehrendorfer", "Marco Franceschetti", "Joscha Grüger", "Francesco Leotta", "Jürgen Mangler", "Ronny Seiger", "Agnes Koschmider", "Stefanie Rinderle-Ma", "Barbara Weber", "Estefania Serral"], "summary": "Advances in Internet-of-Things (IoT) technologies have prompted the\nintegration of IoT devices with business processes (BPs) in many organizations\nacross various sectors, such as manufacturing, healthcare and smart spaces. The\nproliferation of IoT devices leads to the generation of large amounts of IoT\ndata providing a window on the physical context of BPs, which facilitates the\ndiscovery of new insights about BPs using process mining (PM) techniques.\nHowever, to achieve these benefits, IoT data need to be combined with\ntraditional process (event) data, which is challenging due to the very\ndifferent characteristics of IoT and process data, for instance in terms of\ngranularity levels. Recently, several data models were proposed to integrate\nIoT data with process data, each focusing on different aspects of data\nintegration based on different assumptions and requirements. This fragmentation\nhampers data exchange and collaboration in the field of PM, e.g., making it\ntedious for researchers to share data. In this paper, we present a core model\nsynthesizing the most important features of existing data models. As the core\nmodel is based on common requirements, it greatly facilitates data sharing and\ncollaboration in the field. A prototypical Python implementation is used to\nevaluate the model against various use cases and demonstrate that it satisfies\nthese common requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21300v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21300v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种面向物联网增强事件日志的以对象为中心的核心元模型", "tldr": "该论文提出了一种以对象为中心的核心元模型，用于整合物联网数据与过程事件数据，从而促进流程挖掘领域的数据共享和协作。", "motivation": "物联网技术的发展使得物联网设备与业务流程的整合日益增多，产生了大量物联网数据，有助于通过流程挖掘技术发现新的业务洞察。然而，由于物联网数据和传统过程数据（事件数据）的特性差异（如粒度级别），二者的结合面临挑战。现有数据模型的分散性阻碍了流程挖掘领域的数据交换和协作。", "method": "本文提出了一种核心模型，该模型综合了现有数据模型最重要的特性，并基于共同需求构建。通过一个原型Python实现来评估该模型，并证明其满足这些共同需求。", "result": "该核心模型极大地促进了流程挖掘领域的数据共享和协作。通过原型Python实现，证明了该模型满足了各种用例的共同需求。", "conclusion": "本文提出的以对象为中心的核心元模型通过整合现有数据模型的关键特性，有效解决了物联网数据与过程数据集成中的挑战，显著促进了流程挖掘领域的数据共享与协作。", "translation": "物联网（IoT）技术的进步促使物联网设备与各行各业（如制造业、医疗保健和智能空间）的业务流程（BPs）进行集成。物联网设备的普及产生了大量的物联网数据，为业务流程的物理上下文提供了窗口，这有助于利用流程挖掘（PM）技术发现关于业务流程的新见解。然而，为了实现这些益处，物联网数据需要与传统过程（事件）数据相结合，这由于物联网数据和过程数据截然不同的特性（例如粒度级别）而具有挑战性。最近，提出了几种数据模型来整合物联网数据与过程数据，每种模型都基于不同的假设和要求，侧重于数据集成的不同方面。这种碎片化阻碍了流程挖掘领域的数据交换和协作，例如，使得研究人员共享数据变得繁琐。在本文中，我们提出了一种核心模型，该模型综合了现有数据模型中最重要的特性。由于该核心模型基于共同需求，因此它极大地促进了该领域的数据共享和协作。使用原型Python实现对该模型进行了评估，以应对各种用例，并证明它满足这些共同需求。", "summary": "该论文旨在解决物联网数据与传统流程数据在流程挖掘中集成时面临的挑战，这些挑战源于数据特性差异和现有模型碎片化。为此，文章提出了一种以对象为中心的核心元模型，该模型综合了现有数据模型的关键特性，并基于通用需求构建。通过一个原型Python实现，该模型在多种用例中进行了评估，并被证明能够有效促进流程挖掘领域的数据共享与协作。", "keywords": "物联网, 流程挖掘, 数据集成, 元模型, 事件日志", "comments": "该论文的创新之处在于提出了一种综合性的核心元模型，以解决物联网数据与流程数据集成中的碎片化问题，这对于推动物联网背景下的流程挖掘至关重要。其价值在于显著提升了数据交换和协作效率，为该领域的研究和应用奠定了更好的基础。"}}
{"id": "2506.21063", "title": "Control of Marine Robots in the Era of Data-Driven Intelligence", "authors": ["Lin Hong", "Lu Liu", "Zhouhua Peng", "Fumin Zhang"], "summary": "The control of marine robots has long relied on model-based methods grounded\nin classical and modern control theory. However, the nonlinearity and\nuncertainties inherent in robot dynamics, coupled with the complexity of marine\nenvironments, have revealed the limitations of conventional control methods.\nThe rapid evolution of machine learning has opened new avenues for\nincorporating data-driven intelligence into control strategies, prompting a\nparadigm shift in the control of marine robots. This paper provides a review of\nrecent progress in marine robot control through the lens of this emerging\nparadigm. The review covers both individual and cooperative marine robotic\nsystems, highlighting notable achievements in data-driven control of marine\nrobots and summarizing open-source resources that support the development and\nvalidation of advanced control methods. Finally, several future perspectives\nare outlined to guide research toward achieving high-level autonomy for marine\nrobots in real-world applications. This paper aims to serve as a roadmap toward\nthe next-generation control framework of marine robots in the era of\ndata-driven intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21063v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21063v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "数据驱动智能时代海洋机器人的控制", "tldr": "本文回顾了数据驱动智能时代海洋机器人控制的最新进展，旨在为下一代控制框架提供路线图，以克服传统方法的局限性。", "motivation": "传统的基于模型的海洋机器人控制方法因机器人动力学的非线性、不确定性以及海洋环境的复杂性而暴露出局限性。机器学习的快速发展为将数据驱动智能融入控制策略提供了新途径，促使海洋机器人控制范式转变。", "method": "本文通过回顾数据驱动智能这一新兴范式，综述了海洋机器人控制的最新进展。内容涵盖个体和协同海洋机器人系统，并总结了支持高级控制方法开发和验证的开源资源。", "result": "综述了数据驱动智能在海洋机器人控制方面的显著成就，涵盖个体和协同系统，并总结了相关开源资源。提出了未来展望，以指导实现海洋机器人在实际应用中的高级自主性。", "conclusion": "本文旨在为数据驱动智能时代海洋机器人的下一代控制框架提供一份路线图，以期实现海洋机器人在实际应用中的高水平自主性。", "translation": "海洋机器人的控制长期以来一直依赖于基于模型的方法，这些方法以经典和现代控制理论为基础。然而，机器人动力学固有的非线性和不确定性，加上海洋环境的复杂性，已经揭示了传统控制方法的局限性。机器学习的快速发展为将数据驱动智能融入控制策略开辟了新途径，促使海洋机器人控制发生范式转变。本文回顾了在这一新兴范式下海洋机器人控制的最新进展。综述涵盖了个体和协同海洋机器人系统，强调了数据驱动型海洋机器人控制方面的显著成就，并总结了支持高级控制方法开发和验证的开源资源。最后，概述了几个未来展望，以指导研究实现海洋机器人在实际应用中的高水平自主性。本文旨在为数据驱动智能时代海洋机器人的下一代控制框架提供一份路线图。", "summary": "本文回顾了数据驱动智能在海洋机器人控制领域的最新进展，旨在应对传统基于模型方法在处理非线性、不确定性和复杂海洋环境方面的局限性。该综述涵盖了数据驱动控制在个体和协同海洋机器人系统中的显著成就，并总结了相关开源资源。文章还提出了未来研究方向，旨在为实现海洋机器人在实际应用中的高水平自主性提供路线图。", "keywords": "海洋机器人, 数据驱动控制, 机器学习, 控制策略, 自主性", "comments": "这篇论文通过系统回顾数据驱动智能在海洋机器人控制中的应用，填补了传统控制方法在复杂海洋环境下的局限性，具有重要的指导意义。其创新之处在于将机器学习范式引入海洋机器人控制，并提供了全面的现状分析和未来展望，对该领域的研究具有重要的推动作用。"}}
{"id": "2506.21543", "title": "Detecting weighted hidden cliques", "authors": ["Urmisha Chatterjee", "Karissa Huang", "Ritabrata Karmakar", "B. R. Vinay Kumar", "Gábor Lugosi", "Nandan Malhotra", "Anirban Mandal", "Maruf Alam Tarafdar"], "summary": "We study a generalization of the classical hidden clique problem to graphs\nwith real-valued edge weights. Formally, we define a hypothesis testing\nproblem. Under the null hypothesis, edges of a complete graph on $n$ vertices\nare associated with independent and identically distributed edge weights from a\ndistribution $P$. Under the alternate hypothesis, $k$ vertices are chosen at\nrandom and the edge weights between them are drawn from a distribution $Q$,\nwhile the remaining are sampled from $P$. The goal is to decide, upon observing\nthe edge weights, which of the two hypotheses they were generated from. We\ninvestigate the problem under two different scenarios: (1) when $P$ and $Q$ are\ncompletely known, and (2) when there is only partial information of $P$ and\n$Q$. In the first scenario, we obtain statistical limits on $k$ when the two\nhypotheses are distinguishable, and when they are not. Additionally, in each of\nthe scenarios, we provide bounds on the minimal risk of the hypothesis testing\nproblem when $Q$ is not absolutely continuous with respect to $P$. We also\nprovide computationally efficient spectral tests that can distinguish the two\nhypotheses as long as $k=\\Omega(\\sqrt{n})$ in both the scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21543v1", "categories": ["math.ST", "cs.IT", "math.IT", "math.PR", "stat.TH", "62F03"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.21543v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "检测加权隐藏团", "tldr": "本文研究加权图中隐藏团的检测问题，将其建模为假设检验。在P和Q已知或部分已知的情况下，获得了可区分性的统计极限和最小风险界限，并提出了当隐藏团大小k达到$\\Omega(\\sqrt{n})$时计算高效的谱测试方法。", "motivation": "本文旨在将经典的隐藏团问题推广到具有实值边权的图，并将其形式化为一个假设检验问题。具体来说，目标是根据观察到的边权，判断它们是来自一个完全随机的图，还是存在一个由特定分布生成边权的隐藏子图（即隐藏团）。", "method": "研究采用假设检验框架。在零假设下，所有边权来自分布P；在备择假设下，k个顶点的边权来自分布Q，其余来自P。研究分两种场景进行：1) 分布P和Q完全已知；2) 分布P和Q只有部分信息。为了区分两种假设，论文提出了计算高效的谱测试方法。", "result": "在P和Q完全已知的情况下，论文获得了当两种假设可区分和不可区分时k的统计极限。当Q对P非绝对连续时，在两种场景下都给出了假设检验的最小风险界限。此外，在两种场景下，只要k达到$\\Omega(\\sqrt{n})$，所提出的计算高效的谱测试方法就能够区分两种假设。", "conclusion": "本研究成功地将经典的隐藏团问题推广到加权图，并通过严谨的假设检验框架，在不同信息完备度下，给出了区分加权隐藏团的统计界限和计算效率高的解决方案，特别是当隐藏团大小满足一定条件时。", "translation": "我们研究了经典隐藏团问题在具有实值边权图上的推广。形式上，我们定义了一个假设检验问题。在零假设下，n个顶点的完全图的边权来自独立同分布P。在备择假设下，随机选择k个顶点，它们之间的边权来自分布Q，而其余边权从P中采样。目标是根据观察到的边权，判断它们是由哪种假设生成的。我们在两种不同的场景下研究了这个问题：(1) 当P和Q完全已知时，以及 (2) 当P和Q只有部分信息时。在第一种场景中，我们获得了当两种假设可区分和不可区分时k的统计极限。此外，在每种场景中，当Q对P非绝对连续时，我们给出了假设检验问题的最小风险界限。我们还提供了计算高效的谱测试方法，在两种场景下，只要k=$ \\Omega(\\sqrt{n})$，就能区分两种假设。", "summary": "本文研究了加权隐藏团的检测问题，将其形式化为一种假设检验。在零假设下，图的边权服从分布P；在备择假设下，存在一个由k个顶点组成的子图，其边权服从分布Q，其余边权服从P。研究涵盖了P和Q已知以及部分已知两种场景。结果包括了可区分性的统计极限、最小风险界限，并提出了在k达到$\\Omega(\\sqrt{n})$时计算高效的谱测试方法。", "keywords": "加权隐藏团, 假设检验, 谱测试, 统计极限, 图论", "comments": "这篇论文将经典的隐藏团问题推广到更具挑战性的加权图设置，具有重要的理论和实际意义。通过引入假设检验框架，并区分P和Q已知与部分已知两种情况，使得研究更加全面。特别地，提出计算高效的谱测试方法并在k=$\\Omega(\\sqrt{n})$时实现区分，为解决这一复杂问题提供了实用的算法工具。"}}
{"id": "2506.21467", "title": "Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces", "authors": ["Michael Johnston", "Burkhard Ringlein", "Christoph Hagleitner", "Alessandro Pomponio", "Vassilis Vassiliadis", "Christian Pinto", "Srikumar Venugopal"], "summary": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21467v1", "categories": ["cs.DC", "C.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21467v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用发现空间进行高效可重用云配置搜索", "tldr": "本文提出了“发现空间”抽象，旨在形式化和高效解决复杂的云配置问题，通过实现数据共享和知识重用，将配置搜索速度提高90%以上。", "motivation": "寻找以最低成本部署给定工作负载并满足服务水平协议的最佳云资源集是当前活跃的研究领域。由于云提供商和应用程序参数的结合导致配置空间庞大，包含数百万种部署选项，因此找到最优配置极具挑战性。", "method": "本文提出了“发现空间”（Discovery Space），这是一种抽象，用于形式化工作负载配置问题的描述，并具备对大型搜索空间进行结构化、健壮和分布式调查所需的特性。文中描述了其具体实现，并证明其可推广到大型语言模型推理和大数据分析等多种工作负载。", "result": "该方法能够实现在最佳优化器执行之间安全、透明地共享数据，从而提高了大型搜索空间中最佳配置检测的效率。此外，“发现空间”还实现了在类似搜索空间之间知识的转移和重用，使配置搜索速度提升了90%以上。", "conclusion": "“发现空间”提供了一个高效且可重用的云配置搜索框架，通过实现数据和知识共享，显著提高了优化效率和搜索速度。", "translation": "寻找以最低成本部署给定工作负载并满足既定服务水平协议的最佳云资源集是一个活跃的研究领域。将云提供商提供的大量计算、存储和服务中的数十个参数与类似数量的特定于应用程序的参数相结合，导致配置空间具有数百万种部署选项。\n在本文中，我们提出了“发现空间”（Discovery Space），这是一种抽象，它形式化了工作负载配置问题的描述，并展示了对大型搜索空间进行结构化、健壮和分布式调查所需的一组特性。我们描述了“发现空间”抽象的一个具体实现，并表明它可推广到各种工作负载，例如大型语言模型推理和大数据分析。\n我们证明了我们的方法能够在最佳优化器执行之间安全、透明地共享数据，从而提高大型搜索空间中最佳配置检测的效率。我们还展示了“发现空间”如何实现在类似搜索空间之间知识的转移和重用，使配置搜索速度提高90%以上。", "summary": "本文引入了“发现空间”（Discovery Space），这是一种抽象，旨在形式化并高效解决复杂的云配置问题。面对数百万种潜在部署选项，该方法能够对大型搜索空间进行结构化探索，促进优化器之间安全的数据共享，并推动类似配置之间的知识重用。所提出的方法可推广到大型语言模型推理和大数据分析等多种工作负载，通过利用知识转移，将配置搜索速度提升了90%以上。", "keywords": "云配置, 优化, 发现空间, 知识重用, 搜索空间", "comments": "该论文的创新之处在于通过“发现空间”的形式化方法来解决云配置问题，并明确地实现了搜索执行过程中的知识重用和数据共享。这通过显著减少冗余计算并加速最优配置的发现，解决了现代云部署复杂性和规模带来的关键挑战。所展示的显著加速效果充分证明了其在实际应用中的巨大价值。"}}
{"id": "2506.20790", "title": "Stochastic Parameter Decomposition", "authors": ["Lucius Bushnaq", "Dan Braun", "Lee Sharkey"], "summary": "A key step in reverse engineering neural networks is to decompose them into\nsimpler parts that can be studied in relative isolation. Linear parameter\ndecomposition -- a framework that has been proposed to resolve several issues\nwith current decomposition methods -- decomposes neural network parameters into\na sum of sparsely used vectors in parameter space. However, the current main\nmethod in this framework, Attribution-based Parameter Decomposition (APD), is\nimpractical on account of its computational cost and sensitivity to\nhyperparameters. In this work, we introduce \\textit{Stochastic Parameter\nDecomposition} (SPD), a method that is more scalable and robust to\nhyperparameters than APD, which we demonstrate by decomposing models that are\nslightly larger and more complex than was possible to decompose with APD. We\nalso show that SPD avoids other issues, such as shrinkage of the learned\nparameters, and better identifies ground truth mechanisms in toy models. By\nbridging causal mediation analysis and network decomposition methods, this\ndemonstration opens up new research possibilities in mechanistic\ninterpretability by removing barriers to scaling linear parameter decomposition\nmethods to larger models. We release a library for running SPD and reproducing\nour experiments at https://github.com/goodfire-ai/spd.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20790v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20790v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "随机参数分解", "tldr": "本文提出了一种名为随机参数分解（SPD）的新方法，旨在解决现有线性参数分解方法（如归因参数分解APD）在逆向工程神经网络时计算成本高和对超参数敏感的问题。SPD比APD更具可扩展性和鲁棒性，能分解更大更复杂的模型，并避免了参数收缩等问题，为机械可解释性研究开辟了新途径。", "motivation": "逆向工程神经网络的关键一步是将其分解为更简单的部分。现有的线性参数分解框架中的主要方法，归因参数分解（APD），因其计算成本高和对超参数敏感而变得不实用。", "method": "本文引入了随机参数分解（SPD），这是一种比APD更具可扩展性且对超参数更鲁棒的方法。该方法通过连接因果中介分析和网络分解方法来实现。", "result": "SPD能够分解比APD更大、更复杂的模型。它还避免了学习参数收缩等其他问题，并在玩具模型中更好地识别了真实机制。", "conclusion": "SPD通过消除线性参数分解方法扩展到更大模型的障碍，为机械可解释性研究开辟了新的可能性。这通过连接因果中介分析和网络分解方法得以实现。", "translation": "逆向工程神经网络的一个关键步骤是将其分解为可以相对独立研究的更简单部分。线性参数分解——一个已被提出的用于解决当前分解方法中若干问题的框架——将神经网络参数分解为参数空间中稀疏使用的向量之和。然而，该框架中当前的主要方法，归因参数分解（APD），因其计算成本和对超参数的敏感性而变得不实用。在这项工作中，我们引入了随机参数分解（SPD），这是一种比APD更具可扩展性且对超参数更鲁棒的方法，我们通过分解比APD可能分解的模型略大且更复杂的模型来证明这一点。我们还表明，SPD避免了其他问题，例如学习参数的收缩，并且在玩具模型中更好地识别了真实机制。通过连接因果中介分析和网络分解方法，这一演示通过消除将线性参数分解方法扩展到更大模型的障碍，为机械可解释性开辟了新的研究可能性。我们发布了一个用于运行SPD和复现我们实验的库，地址为 https://github.com/goodfire-ai/spd。", "summary": "本文提出随机参数分解（SPD），旨在克服现有线性参数分解方法（如APD）在逆向工程神经网络时面临的计算成本高和超参数敏感性问题。SPD被证明比APD更具可扩展性和鲁棒性，能够有效分解更大、更复杂的神经网络模型，并避免了学习参数收缩等常见问题。通过将因果中介分析与网络分解相结合，SPD为将线性参数分解方法应用于大型模型提供了可行途径，从而为机械可解释性研究开辟了新的机遇。", "keywords": "随机参数分解, 线性参数分解, 神经网络, 机械可解释性, 逆向工程", "comments": "SPD的创新之处在于其对现有线性参数分解方法的重大改进，特别是在解决可扩展性和鲁棒性方面的痛点。这使得机械可解释性研究能够扩展到更复杂、更大规模的神经网络模型，是该领域的一个重要进展。论文还提供了开源库，有助于研究的复现和进一步探索，体现了开放科学的精神。"}}
{"id": "2506.21532", "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets", "authors": ["Akshay Paruchuri", "Maryam Aziz", "Rohit Vartak", "Ayman Ali", "Best Uchehara", "Xin Liu", "Ishan Chatterjee", "Monica Agrawal"], "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat", "comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release", "pdf_url": "http://arxiv.org/pdf/2506.21532v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21532v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "\"医生，你好吗？\": 分析用户如何在大型会话式AI数据集中寻求健康信息", "tldr": "本文通过构建HealthChat-11K数据集并结合临床医生分类法，系统研究了用户如何从大型语言模型中寻求健康信息，揭示了上下文不完整、情感行为和诱导奉承等问题，强调了改进LLM在医疗保健支持方面能力的必要性。", "motivation": "人们越来越多地通过交互式聊天机器人从大型语言模型（LLM）中寻求医疗保健信息，然而这些对话的性质和固有风险在很大程度上仍未被探索。", "method": "作者过滤了大规模会话式AI数据集，构建了一个包含1.1万个真实世界对话和2.5万条用户消息的精选数据集HealthChat-11K。随后，他们利用HealthChat-11K和一个由临床医生驱动的分类法，系统地研究了用户在21个不同健康专业领域与LLM互动以寻求医疗保健信息的方式。", "result": "分析揭示了用户如何以及为何寻求健康信息的本质，包括常见的交互模式、上下文不完整的情况、情感行为，以及可能诱导奉承的交互（例如，引导性问题）。", "conclusion": "研究结果强调了部署为会话式AI的大型语言模型在医疗保健支持能力方面需要改进。", "translation": "人们越来越多地通过交互式聊天机器人从大型语言模型（LLM）中寻求医疗保健信息，然而这些对话的性质和固有风险在很大程度上仍未被探索。在本文中，我们过滤了大规模会话式AI数据集，以获得HealthChat-11K，这是一个包含1.1万个真实世界对话和2.5万条用户消息的精选数据集。我们使用HealthChat-11K和一个由临床医生驱动的用户与LLM互动寻求医疗保健信息的分类法，系统地研究了21个不同健康专业领域的用户交互。我们的分析揭示了用户如何以及为何寻求健康信息的本质，例如常见的交互、上下文不完整的情况、情感行为，以及可能导致奉承的交互（例如，引导性问题），这强调了部署为会话式AI的LLM在医疗保健支持能力方面需要改进。获取我们的分析并将其组合成精选数据集的代码和工件可以在这里找到：https://github.com/yahskapar/HealthChat", "summary": "本文旨在探究用户通过聊天机器人从大型语言模型（LLM）获取医疗健康信息的未探索性质和潜在风险。研究人员通过筛选大规模会话式AI数据集，构建了一个名为HealthChat-11K的精选数据集，其中包含1.1万个真实世界的医疗对话。利用此数据集和临床医生驱动的分类法，他们系统地分析了21个健康专业领域中用户的交互行为。分析结果揭示了用户寻求健康信息的方式和原因，识别出诸如上下文不完整、情感行为以及可能诱导LLM产生奉承的交互等问题，从而强调了提升LLM在医疗保健支持能力方面的迫切需求。", "keywords": "健康信息寻求, 大型语言模型, 会话式AI, 医疗聊天机器人, HealthChat-11K", "comments": "本文通过创建HealthChat-11K这一新颖、大规模且精心策划的数据集，为分析LLM在医疗保健相关对话中的表现做出了重要贡献。该数据集与临床医生驱动的分类法相结合，使得对用户在这一关键领域行为和挑战的系统深入研究成为可能。关于上下文不完整和奉承现象的发现尤为重要，它们直接指出了当前LLM在提供可靠和安全的医疗保健支持方面的不足之处，为未来负责任的医疗AI研究和开发指明了方向。"}}
{"id": "2506.21207", "title": "Estimation of superconducting cavity bandwidth and detuning using a Luenberger observer", "authors": ["Bozo Richter", "Andrea Bellandi", "Julien Branlard", "Leon Speidel", "Annika Eichler"], "summary": "Enabled by progress in superconducting technology, several continuous wave\nlinear accelerators are foreseen in the next decade. For these machines, it is\nof crucial importance to track the main cavity parameters, such as the\nresonator bandwidth and detuning. The bandwidth yields information on the\nsuperconducting state of the cavity. The detuning should be minimized to limit\nthe required power to operate the cavity. The estimation of these parameters is\ncommonly implemented in the digital electronics of the Low-Level RF control\nsystem to minimize the computation delay. In this proceeding, we present a way\nto compute the bandwidth and detuning using a Luenberger observer. In contrast\nto previous methods, a state observer yields estimations at the native control\nsystem sample rate without explicitly filtering the input signals.\nAdditionally, the error convergence properties of the estimations can be\ncontrolled intuitively by adjusting gain parameters. Implementation\nconsiderations and test results on the derived observer are presented in the\nmanuscript.", "comment": "10 pages, 4 figures, to be published in APS Physical Review -\n  Accelerator and Beams", "pdf_url": "http://arxiv.org/pdf/2506.21207v1", "categories": ["physics.acc-ph", "cs.SY", "eess.SY"], "cate": "physics.acc-ph", "url": "http://arxiv.org/abs/2506.21207v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "超导腔带宽和失谐的Luenberger观测器估计", "tldr": "本文提出使用Luenberger观测器估计超导腔带宽和失谐，该方法无需显式滤波即可在原始采样率下实现，且误差收敛可控。", "motivation": "未来十年将出现多台连续波直线加速器，这些机器需要精确跟踪超导腔的带宽和失谐，以获取腔体超导状态信息并最小化运行功耗。现有参数估计方法在计算延迟方面存在局限。", "method": "本文提出了一种使用Luenberger观测器来计算超导腔带宽和失谐的方法。与现有方法不同，该状态观测器能在控制系统原始采样率下提供估计，无需显式滤波输入信号，并且可以通过调整增益参数直观地控制估计的误差收敛特性。", "result": "论文中展示了所推导观测器的实现考量和测试结果。", "conclusion": "Luenberger观测器能够有效且高效地估计超导腔的带宽和失谐，具有无需滤波和误差收敛可控的优点，适用于未来的连续波直线加速器。", "translation": "随着超导技术的进步，未来十年预计将出现多台连续波直线加速器。对于这些机器，跟踪主腔参数（如谐振腔带宽和失谐）至关重要。带宽提供了腔体超导状态的信息。应尽量减小失谐以限制操作腔体所需的功率。这些参数的估计通常在低电平射频（LLRF）控制系统的数字电子设备中实现，以最大限度地减少计算延迟。在本论文中，我们提出了一种使用Luenberger观测器计算带宽和失谐的方法。与以前的方法不同，状态观测器可以在原始控制系统采样率下产生估计，而无需显式滤波输入信号。此外，通过调整增益参数，可以直观地控制估计的误差收敛特性。手稿中介绍了派生观测器的实现考虑和测试结果。", "summary": "本文提出了一种基于Luenberger观测器的新方法，用于估计超导腔的带宽和失谐。该方法旨在满足未来连续波直线加速器对精确腔体参数跟踪的需求。与传统方法相比，其主要优势在于能够以控制系统原始采样率进行估计，无需对输入信号进行显式滤波，并且允许通过调整增益参数直观地控制估计误差的收敛性。论文详细介绍了该观测器的实现考虑并展示了测试结果。", "keywords": "超导腔, 带宽估计, 失谐估计, Luenberger观测器, LLRF", "comments": "这项工作通过引入Luenberger观测器为超导腔参数估计提供了一种新颖且高效的解决方案。其创新之处在于无需显式滤波即可在原生采样率下进行估计，并提供了直观的误差收敛控制，这对于需要低延迟和高精度的低电平射频（LLRF）系统至关重要。"}}
{"id": "2506.20879", "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "summary": "Generation of images containing multiple humans, performing complex actions,\nwhile preserving their facial identities, is a significant challenge. A major\nfactor contributing to this is the lack of a a dedicated benchmark. To address\nthis, we introduce MultiHuman-Testbench, a novel benchmark for rigorously\nevaluating generative models for multi-human generation. The benchmark\ncomprises 1800 samples, including carefully curated text prompts, describing a\nrange of simple to complex human actions. These prompts are matched with a\ntotal of 5,550 unique human face images, sampled uniformly to ensure diversity\nacross age, ethnic background, and gender. Alongside captions, we provide\nhuman-selected pose conditioning images which accurately match the prompt. We\npropose a multi-faceted evaluation suite employing four key metrics to quantify\nface count, ID similarity, prompt alignment, and action detection. We conduct a\nthorough evaluation of a diverse set of models, including zero-shot approaches\nand training-based methods, with and without regional priors. We also propose\nnovel techniques to incorporate image and region isolation using human\nsegmentation and Hungarian matching, significantly improving ID similarity. Our\nproposed benchmark and key findings provide valuable insights and a\nstandardized tool for advancing research in multi-human image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20879v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20879v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "MultiHuman-Testbench：多人物图像生成基准测试", "tldr": "本文提出了MultiHuman-Testbench，一个用于评估多人物图像生成模型的新型基准，解决了现有缺乏专用基准的问题，并提出了新的评估方法和技术以提高身份相似性。", "motivation": "生成包含多个执行复杂动作且保留面部身份的人物的图像是一个重大挑战。造成这一挑战的主要原因是缺乏一个专用的基准测试。", "method": "引入了MultiHuman-Testbench，一个包含1800个样本（包括文本提示和5,550个独特人脸图像）以及人工选择的姿态条件图像的新型基准。提出了一个多方面的评估套件，采用面部计数、ID相似性、提示对齐和动作检测四项关键指标。此外，还提出了利用人体分割和匈牙利匹配来整合图像和区域隔离的新技术。", "result": "对包括零样本方法和基于训练的方法在内的多种模型进行了彻底评估。提出的新技术显著提高了ID相似性。", "conclusion": "提出的基准和关键发现为推进多人物图像生成研究提供了宝贵的见解和标准化工具。", "translation": "生成包含多个执行复杂动作且保留面部身份的人物的图像是一个重大挑战。造成这一挑战的主要原因是缺乏一个专用的基准测试。为了解决这个问题，我们引入了MultiHuman-Testbench，一个用于严格评估多人物生成模型的生成模型的新型基准。该基准包含1800个样本，包括精心策划的文本提示，描述了从简单到复杂的人类动作。这些提示与总共5,550张独特的人脸图像匹配，这些图像经过均匀采样，以确保年龄、种族背景和性别多样性。除了说明文字，我们还提供了人工选择的姿态条件图像，这些图像准确匹配提示。我们提出了一个多方面的评估套件，采用面部计数、ID相似性、提示对齐和动作检测四项关键指标。我们对包括零样本方法和基于训练的方法在内的多种模型进行了彻底评估，无论是否包含区域先验。我们还提出了利用人体分割和匈牙利匹配来整合图像和区域隔离的新技术，显著提高了ID相似性。我们提出的基准和关键发现为推进多人物图像生成研究提供了宝贵的见解和标准化工具。", "summary": "本文介绍了MultiHuman-Testbench，这是一个专门针对多人物图像生成任务的新型基准测试。该基准包含1800个多样化样本，包括文本提示和大量人脸图像，并辅以人工选择的姿态条件图像。研究者提出了一个包含四项关键指标的多方面评估框架，并开发了结合人体分割和匈牙利匹配的新技术，以提高生成图像中人物身份的相似性。通过对多种模型进行评估，该研究为多人物图像生成领域提供了一个标准化的评估工具和有价值的见解。", "keywords": "多人物生成, 图像生成, 基准测试, 面部身份, 评估指标", "comments": "本文的创新之处在于提出了首个专门用于多人物图像生成的基准测试MultiHuman-Testbench，填补了该领域评估工具的空白。其重要性在于提供了一个标准化的评估框架和数据集，这将极大地促进多人物图像生成技术的发展和比较。同时，提出的结合人体分割和匈牙利匹配的新技术也为提高生成图像中人物身份的保持性提供了有效途径。"}}
{"id": "2506.21185", "title": "Out-of-Distribution Semantic Occupancy Prediction", "authors": ["Yuheng Zhang", "Mengfei Duan", "Kunyu Peng", "Yuhang Wang", "Ruiping Liu", "Fei Teng", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "summary": "3D Semantic Occupancy Prediction is crucial for autonomous driving, providing\na dense, semantically rich environmental representation. However, existing\nmethods focus on in-distribution scenes, making them susceptible to\nOut-of-Distribution (OoD) objects and long-tail distributions, which increases\nthe risk of undetected anomalies and misinterpretations, posing safety hazards.\nTo address these challenges, we introduce Out-of-Distribution Semantic\nOccupancy Prediction, targeting OoD detection in 3D voxel space. To fill the\ngaps in the dataset, we propose a Synthetic Anomaly Integration Pipeline that\ninjects synthetic anomalies while preserving realistic spatial and occlusion\npatterns, enabling the creation of two datasets: VAA-KITTI and VAA-KITTI-360.\nWe introduce OccOoD, a novel framework integrating OoD detection into 3D\nsemantic occupancy prediction, with Voxel-BEV Progressive Fusion (VBPF)\nleveraging an RWKV-based branch to enhance OoD detection via geometry-semantic\nfusion. Experimental results demonstrate that OccOoD achieves state-of-the-art\nOoD detection with an AuROC of 67.34% and an AuPRCr of 29.21% within a 1.2m\nregion, while maintaining competitive occupancy prediction performance. The\nestablished datasets and source code will be made publicly available at\nhttps://github.com/7uHeng/OccOoD.", "comment": "The established datasets and source code will be made publicly\n  available at https://github.com/7uHeng/OccOoD", "pdf_url": "http://arxiv.org/pdf/2506.21185v1", "categories": ["cs.CV", "cs.RO", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21185v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "分布外语义占据预测", "tldr": "现有3D语义占据预测方法在分布外（OoD）物体上表现不佳。本文提出了OccOoD框架和VAA-KITTI、VAA-KITTI-360数据集，以解决此问题，实现了最先进的OoD检测性能，同时保持了竞争力占据预测能力。", "motivation": "现有的3D语义占据预测方法主要关注分布内场景，容易受到分布外（OoD）物体和长尾分布的影响，这增加了未检测到的异常和误判的风险，从而对自动驾驶构成安全隐患。", "method": "本文提出了“分布外语义占据预测”的概念。为弥补数据集空白，引入了“合成异常集成管道”来创建VAA-KITTI和VAA-KITTI-360两个数据集，通过注入合成异常同时保留真实的几何和遮挡模式。此外，还提出了一个名为“OccOoD”的新型框架，将OoD检测集成到3D语义占据预测中，其中包含“体素-BEV渐进融合（VBPF）”模块，该模块利用基于RWKV的分支通过几何-语义融合来增强OoD检测。", "result": "实验结果表明，OccOoD在1.2米区域内实现了最先进的OoD检测，AuROC为67.34%，AuPRCr为29.21%，同时保持了有竞争力的占据预测性能。", "conclusion": "本文成功引入了一个新颖的框架（OccOoD）和数据集（VAA-KITTI、VAA-KITTI-360），实现了分布外语义占据预测，显著提高了自动驾驶的OoD检测性能，增强了安全性。", "translation": "3D语义占据预测对于自动驾驶至关重要，它提供了密集、语义丰富的环境表示。然而，现有方法侧重于分布内场景，使其容易受到分布外（OoD）物体和长尾分布的影响，这增加了未检测到的异常和误解的风险，从而构成安全隐患。为了解决这些挑战，我们引入了分布外语义占据预测，旨在3D体素空间中进行OoD检测。为了填补数据集的空白，我们提出了一种合成异常集成管道，该管道在保留真实空间和遮挡模式的同时注入合成异常，从而能够创建两个数据集：VAA-KITTI和VAA-KITTI-360。我们引入了OccOoD，一个将OoD检测集成到3D语义占据预测中的新型框架，其中Voxel-BEV渐进融合（VBPF）利用基于RWKV的分支通过几何-语义融合来增强OoD检测。实验结果表明，OccOoD在1.2米区域内实现了最先进的OoD检测，AuROC为67.34%，AuPRCr为29.21%，同时保持了有竞争力的占据预测性能。已建立的数据集和源代码将在https://github.com/7uHeng/OccOoD 公开。", "summary": "本文旨在解决自动驾驶中3D语义占据预测对分布外（OoD）物体识别不足的挑战。为此，研究提出了一种“合成异常集成管道”以构建VAA-KITTI和VAA-KITTI-360两个新数据集，其中注入了合成异常。核心贡献是“OccOoD”框架，它将OoD检测集成到3D语义占据预测中，并利用包含RWKV分支的“体素-BEV渐进融合（VBPF）”模块来增强OoD检测。实验证明，OccOoD在保持占据预测性能的同时，实现了最先进的OoD检测。", "keywords": "分布外, 语义占据预测, 自动驾驶, 异常检测, 3D体素", "comments": "该论文通过解决自动驾驶中经常被忽视的分布外场景，为提高自动驾驶安全性指明了一个关键方向。合成异常数据集的创建对于解决该领域数据稀缺性问题具有创新性，而所提出的OccOoD框架及其VBPF和基于RWKV的分支为集成OoD检测提供了一种新颖的架构设计。同时关注OoD检测和保持占据预测性能是其一大亮点。"}}
{"id": "2506.21325", "title": "Localization-Based Beam Focusing in Near-Field Communications", "authors": ["Nima Mozaffarikhosravi", "Prathapasinghe Dharmawansa", "Italo Atzeni"], "summary": "Shifting 6G-and-beyond wireless communication systems to higher frequency\nbands and the utilization of massive multiple-input multiple-output arrays will\nextend the near-field region, affecting beamforming and user localization\nschemes. In this paper, we propose a localization-based beam-focusing strategy\nthat leverages the dominant line-of-sight (LoS) propagation arising at mmWave\nand sub-THz frequencies. To support this approach, we analyze the 2D-MUSIC\nalgorithm for distance estimation by examining its spectrum in simplified,\ntractable setups with minimal numbers of antennas and users. Lastly, we compare\nthe proposed localization-based beam focusing, with locations estimated via\n2D-MUSIC, with zero forcing with pilot-based channel estimation in terms of\nuplink sum spectral efficiency. Our numerical results show that the proposed\nmethod becomes more effective under LoS-dominated propagation, short coherence\nblocks, and strong noise power arising at high carrier frequencies and with\nlarge bandwidths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21325v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21325v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "近场通信中基于定位的波束聚焦", "tldr": "针对6G及以上高频段近场通信，提出一种基于定位（使用2D-MUSIC算法）的波束聚焦策略，数值结果表明其在视距主导、短相干块和强噪声环境下更有效。", "motivation": "6G及以上无线通信系统向更高频段发展以及大规模MIMO阵列的应用将扩展近场区域，进而影响波束赋形和用户定位方案。", "method": "提出一种利用毫米波和亚太赫兹频率下主导视距传播的基于定位的波束聚焦策略。为支持此方法，分析了用于距离估计的2D-MUSIC算法，并通过在简化、易处理的设置中检查其频谱。将该方法与基于导频信道估计的零迫使方案在下行链路总谱效率方面进行了比较。", "result": "数值结果表明，所提出的方法在视距主导传播、短相干块以及高载波频率和大带宽下出现的强噪声功率条件下更有效。", "conclusion": "基于定位的波束聚焦策略（利用2D-MUSIC进行定位）在特定高频段和噪声条件下，比传统的基于导频的零迫使方案表现出更好的频谱效率，特别是在视距主导的环境中。", "translation": "将6G及以上无线通信系统转移到更高频段以及大规模多输入多输出阵列的利用将扩展近场区域，影响波束赋形和用户定位方案。在本文中，我们提出了一种基于定位的波束聚焦策略，该策略利用毫米波和亚太赫兹频率下出现的主导视距（LoS）传播。为了支持这种方法，我们通过在最小天线和用户数量的简化、易处理的设置中检查其频谱，分析了用于距离估计的2D-MUSIC算法。最后，我们比较了所提出的基于定位的波束聚焦（通过2D-MUSIC估计位置）与基于导频信道估计的零迫使在下行链路总谱效率方面的性能。我们的数值结果表明，所提出的方法在视距主导传播、短相干块以及高载波频率和大带宽下出现的强噪声功率条件下变得更有效。", "summary": "本文针对6G及以上高频段近场通信中波束赋形和用户定位面临的挑战，提出了一种基于定位的波束聚焦策略。该策略利用毫米波和亚太赫兹频率下的视距传播优势，并通过分析2D-MUSIC算法进行距离估计。数值结果表明，在视距主导、短相干块和强噪声环境下，所提出的方法比传统的基于导频的零迫使方案在下行链路总谱效率方面更有效。", "keywords": "近场通信, 波束聚焦, 定位, 2D-MUSIC, 6G, 视距传播", "comments": "这篇论文提出了一种创新的方法，将用户定位与波束聚焦相结合，以应对6G及以上高频段近场通信的挑战。其亮点在于利用了高频段特有的视距传播特性，并结合了2D-MUSIC算法进行定位。研究结果表明了其在特定条件下的优势，这对于未来高频通信系统的设计具有重要指导意义。"}}
{"id": "2506.20932", "title": "Thinning to improve two-sample discrepancy", "authors": ["Gleb Smirnov", "Roman Vershynin"], "summary": "The discrepancy between two independent samples \\(X_1,\\dots,X_n\\) and\n\\(Y_1,\\dots,Y_n\\) drawn from the same distribution on $\\mathbb{R}^d$ typically\nhas order \\(O(\\sqrt{n})\\) even in one dimension. We give a simple online\nalgorithm that reduces the discrepancy to \\(O(\\log^{2d} n)\\) by discarding a\nsmall fraction of the points.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.20932v1", "categories": ["math.PR", "cs.DS"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.20932v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "稀疏化以改善两样本差异", "tldr": "提出一种在线算法，通过丢弃少量数据点，将两样本差异从O(√n)显著降低到O(log^(2d) n)。", "motivation": "两个独立样本之间的差异通常为O(√n)，即使在低维度下也是如此，这表明样本差异较大，需要被改善。", "method": "本文提出了一个简单的在线算法，通过丢弃一小部分数据点来减少样本差异。", "result": "该算法成功地将两样本差异从典型的O(√n)量级降低到O(log^(2d) n)。", "conclusion": "通过一个简单的在线算法并丢弃一小部分数据点，可以显著地改善两个独立样本之间的差异。", "translation": "摘要：从$\\\\mathbb{R}^d$上的相同分布中抽取的两个独立样本\\(X_1,\\dots,X_n\\)和\\(Y_1,\\dots,Y_n\\)之间的差异，即使在一维情况下，通常也具有\\(O(\\\\sqrt{n})\\)的量级。我们提出了一种简单的在线算法，通过丢弃一小部分点，将差异降低到\\(O(\\\\log^{2d} n)\\\\)。", "summary": "本文针对两个独立样本之间普遍存在的O(√n)量级差异问题，提出了一种简单的在线算法。该算法通过策略性地丢弃一小部分样本点，成功地将样本差异显著降低至O(log^(2d) n)，从而有效地提升了样本间的匹配度。", "keywords": "两样本差异, 稀疏化, 在线算法, 差异降低, 数据抽样", "comments": "这项工作通过一个简单的在线算法，显著改善了传统两样本差异的收敛速度，从多项式级别降低到对数级别，这在处理大数据集时可能具有重要意义。其创新之处在于通过“稀疏化”或“丢弃”部分点来优化差异，这是一种新颖且高效的方法。"}}
{"id": "2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20993v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20993v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SAC：一种用于测量和诱导大型语言模型人格特质并进行动态强度控制的框架", "tldr": "本文提出了SAC框架，通过扩展16PF模型并引入动态强度控制，实现对LLM人格特质更精细的测量和诱导。", "motivation": "现有的大型语言模型（LLM）人格建模方法主要依赖于粗粒度的“大五”（OCEAN）框架，并且缺乏对特质强度的有效控制机制。", "method": "本文通过扩展机器个性清单（MPI）以整合16人格因素（16PF）模型，从而实现了对十六种不同特质的表达性控制。同时，开发了名为特定属性控制（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。该方法引入了基于形容词的语义锚定来指导特质强度表达，并利用行为问题在五个强度因子（频率、深度、阈值、努力、意愿）上进行评估。", "result": "实验发现，将特质强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，研究观察到目标特质强度的变化会系统地影响心理学上密切相关的特质，这表明LLM内化了多维人格结构，而非孤立地处理特质。", "conclusion": "该工作为医疗保健、教育和面试等领域受控和细致的人机交互开辟了新途径，使我们离真正的类人社交机器更近一步。", "translation": "大型语言模型（LLM）近年来在广泛领域获得了显著关注。人们也越来越期望它们在交互过程中展现出类人的人格。为了满足这一期望，许多研究提出了通过心理测量评估来建模LLM人格的方法。然而，大多数现有模型面临两个主要限制：它们依赖于“大五”（OCEAN）框架，该框架仅提供粗粒度的人格维度；并且它们缺乏控制特质强度的机制。在本文中，我们通过扩展最初使用“大五”模型的机器个性清单（MPI），以整合16人格因素（16PF）模型来解决这一空白，从而实现对十六种不同特质的表达性控制。我们还开发了一个名为特定属性控制（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。我们的方法引入了基于形容词的语义锚定来指导特质强度表达，并利用行为问题在五个强度因子：频率、深度、阈值、努力和意愿上进行评估。通过实验，我们发现将强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，我们观察到目标特质强度的变化会系统地影响心理学上密切相关的特质，这表明LLM内化了多维人格结构，而不是孤立地处理特质。我们的工作为医疗保健、教育和面试等领域受控和细致的人机交互开辟了新途径，使我们离真正的类人社交机器更近一步。", "summary": "本文提出了SAC（特定属性控制）框架，旨在解决现有LLM人格建模方法在特质粒度和强度控制方面的不足。通过将机器个性清单（MPI）扩展至16人格因素（16PF）模型，并引入基于形容词的语义锚定和五个强度因子，SAC实现了对LLM人格特质的精细测量和动态强度诱导。研究表明，连续谱的强度建模比二元切换更有效，且LLM能内化多维人格结构。该工作为实现更 nuanced 的人机交互提供了新途径。", "keywords": "LLM人格, 16PF模型, SAC框架, 特质强度控制, 人机交互", "comments": "这项工作通过引入16PF模型和动态强度控制机制，显著提升了LLM人格建模的精细度和可控性。特别创新的是其提出的基于形容词的语义锚定和五个强度因子，这为量化和诱导人格特质提供了实用的方法。研究结果表明LLM能够理解并内化复杂的人格结构，这对于开发更具情商和适应性的AI具有重要意义。该框架在医疗、教育等领域的应用潜力巨大，有助于实现更自然、更有效的AI交互。"}}
{"id": "2506.21242", "title": "Runge--Kutta generalized Convolution Quadrature for sectorial problems", "authors": ["Jing Guo", "Maria Lopez-Fernandez"], "summary": "We study the application of the generalized convolution quadrature (gCQ)\nbased on Runge--Kutta methods to approximate the solution of an important class\nof sectorial problems. The gCQ generalizes Lubich's original convolution\nquadrature (CQ) to variable steps. High-order versions of the gCQ have been\ndeveloped in the last decade, relying on certain Runge--Kutta methods. The\nRunge--Kutta based gCQ has been studied so far in a rather general setting,\nwhich includes applications to boundary integral formulations of wave problems.\nThe available stability and convergence results for these new methods are\nsuboptimal compared to those known for the uniform-step CQ, both in terms of\nconvergence order and regularity requirements of the data. Here we focus on a\nspecial class of sectorial problems and prove that in these important\napplications it is possible to achieve the same order of convergence as for the\noriginal CQ, under the same regularity hypotheses on the data, and for very\ngeneral time meshes. In the particular case of data with some known algebraic\ntype of singularity, we also show how to choose an optimally graded time mesh\nto achieve convergence with maximal order, overcoming the well-known order\nreduction of the original CQ in these situations. An important advantage of the\ngCQ method is that it allows for a fast and memory-efficient implementation. We\ndescribe how the fast and oblivious Runge--Kutta based gCQ can be implemented\nand illustrate our theoretical results with several numerical experiments. The\ncodes implementing the examples in this article are available in [13].", "comment": "35 pages, 26 figures", "pdf_url": "http://arxiv.org/pdf/2506.21242v1", "categories": ["math.NA", "cs.NA", "65R20, 65L06, 65M15, 26A33, 35R11"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21242v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Runge--Kutta 广义卷积积分法求解扇形问题", "tldr": "本文研究了基于Runge--Kutta方法的广义卷积积分法（gCQ）在求解扇形问题上的应用，证明其能达到与原始卷积积分法相同的收敛阶，并能有效处理数据奇异性，实现快速高效的计算。", "motivation": "原始的卷积积分法（CQ）在均匀步长下表现良好，但其基于Runge--Kutta方法的高阶广义卷积积分法（gCQ）在一般设置下的稳定性和收敛性结果不如原始CQ，在收敛阶和数据正则性要求方面都次优。本文旨在解决gCQ在特定问题类别中的次优表现。", "method": "研究了基于Runge--Kutta方法的广义卷积积分法（gCQ）在求解一类重要的扇形问题中的应用。证明了在这些扇形问题中，gCQ在相同数据正则性假设和非常通用的时间网格下，可以达到与原始CQ相同的收敛阶。对于具有代数型奇异性的数据，展示了如何选择最优分级时间网格以实现最大阶收敛，克服了原始CQ在这些情况下的阶数降低问题。描述了Runge--Kutta基的gCQ的快速且无感知实现方法。通过数值实验验证了理论结果。", "result": "证明了Runge--Kutta基的gCQ在扇形问题中可以实现与原始CQ相同的收敛阶，且对数据正则性要求相同，并适用于非常通用的时间网格。对于具有代数型奇异性的数据，通过选择最优分级时间网格，gCQ能够实现最大阶收敛，克服了原始CQ在这些情况下的阶数降低问题。该gCQ方法允许快速且内存高效的实现。数值实验证实了理论结果。", "conclusion": "本文证明了基于Runge--Kutta方法的广义卷积积分法（gCQ）在求解一类重要的扇形问题时，能够达到与原始卷积积分法相同的最优收敛阶，即使对于具有奇异性的数据也能通过优化时间网格实现最大阶收敛，并且该方法具有快速和内存高效的实现优势。", "translation": "我们研究了基于Runge--Kutta方法的广义卷积积分法（gCQ）在逼近一类重要扇形问题解中的应用。gCQ将Lubich的原始卷积积分法（CQ）推广到可变步长。在过去十年中，基于某些Runge--Kutta方法，已经开发出了高阶版本的gCQ。迄今为止，基于Runge--Kutta的gCQ已在相当通用的设置中进行研究，其中包括波浪问题的边界积分公式应用。与均匀步长CQ的已知结果相比，这些新方法的可用稳定性和收敛性结果次优，无论是在收敛阶还是数据正则性要求方面。本文我们专注于一类特殊的扇形问题，并证明在这些重要应用中，在相同的数据正则性假设下，以及在非常通用的时间网格下，可以实现与原始CQ相同的收敛阶。在数据具有某些已知代数型奇异性的特殊情况下，我们还展示了如何选择最优分级时间网格以实现最大阶收敛，克服了原始CQ在这些情况下的众所周知的阶数降低问题。gCQ方法的一个重要优点是它允许快速且内存高效的实现。我们描述了如何实现快速且无感知的基于Runge--Kutta的gCQ，并通过几个数值实验说明了我们的理论结果。本文中实现示例的代码可在[13]中获取。", "summary": "本文研究了基于Runge--Kutta方法的广义卷积积分法（gCQ）在扇形问题中的应用。尽管gCQ在一般情况下收敛性次优，但作者证明了在特定扇形问题上，gCQ可以在通用时间网格和相同数据正则性假设下，达到与原始卷积积分法（CQ）相同的收敛阶。特别地，对于具有奇异性的数据，通过选择最优分级时间网格，gCQ能克服原始CQ的阶数降低问题，实现最大阶收敛。此外，该方法具有快速和内存高效的实现优势，并通过数值实验验证了理论结果。", "keywords": "广义卷积积分法, Runge--Kutta方法, 扇形问题, 收敛阶, 奇异性数据", "comments": "该研究的创新之处在于，它解决了Runge--Kutta基的广义卷积积分法（gCQ）在特定应用场景中收敛性次优的问题，证明了在扇形问题中可以实现与原始卷积积分法相同的最优收敛阶。尤其值得注意的是，它提出了处理数据奇异性的方法，通过优化时间网格来避免阶数降低，这在实际应用中非常重要。gCQ的快速和内存高效的实现也增加了其实用价值。"}}
{"id": "2506.20856", "title": "Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA", "authors": ["Fei Wang", "Baochun Li"], "summary": "Memorization in large language models (LLMs) makes them vulnerable to data\nextraction attacks. While pre-training memorization has been extensively\nstudied, fewer works have explored its impact in fine-tuning, particularly for\nLoRA fine-tuning, a widely adopted parameter-efficient method.\n  In this work, we re-examine memorization in fine-tuning and uncover a\nsurprising divergence from prior findings across different fine-tuning\nstrategies. Factors such as model scale and data duplication, which strongly\ninfluence memorization in pre-training and full fine-tuning, do not follow the\nsame trend in LoRA fine-tuning. Using a more relaxed similarity-based\nmemorization metric, we demonstrate that LoRA significantly reduces\nmemorization risks compared to full fine-tuning, while still maintaining strong\ntask performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20856v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20856v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "精益训练，更低泄露：LoRA 下 LLM 微调中记忆化的再探讨", "tldr": "LoRA 微调显著降低了大型语言模型的记忆化风险，且不受模型规模和数据重复性的影响，同时保持了良好的任务性能。", "motivation": "大型语言模型（LLM）中的记忆化使其容易受到数据提取攻击。虽然预训练记忆化已被广泛研究，但在微调阶段，特别是 LoRA 微调中，其影响尚未得到充分探索。", "method": "本文重新审视了微调中的记忆化，并使用一种更宽松的基于相似度的记忆化度量方法，比较了 LoRA 微调与完全微调。", "result": "与预训练和完全微调不同，模型规模和数据重复等因素在 LoRA 微调中对记忆化的影响趋势不一致。LoRA 微调相比完全微调显著降低了记忆化风险，同时保持了强大的任务性能。", "conclusion": "LoRA 是一种有效降低 LLM 微调中记忆化风险的方法，同时能维持性能。", "translation": "大型语言模型（LLM）中的记忆化使其容易受到数据提取攻击。虽然预训练记忆化已被广泛研究，但很少有工作探索其在微调中的影响，特别是对于 LoRA 微调这种广泛采用的参数高效方法。\n在这项工作中，我们重新审视了微调中的记忆化，并发现了与不同微调策略先前研究结果的惊人差异。模型规模和数据重复等在预训练和完全微调中强烈影响记忆化的因素，在 LoRA 微调中并不遵循相同的趋势。使用一种更宽松的基于相似度的记忆化度量，我们证明 LoRA 相比完全微调显著降低了记忆化风险，同时仍保持了强大的任务性能。", "summary": "本文重新审视了大型语言模型微调中的记忆化问题，特别关注了参数高效的 LoRA 方法。研究发现，与预训练和完全微调不同，LoRA 微调中的记忆化受模型规模和数据重复的影响较小，并且与完全微调相比，LoRA 能显著降低记忆化风险，同时保持优秀的任务性能。", "keywords": "LLM, LoRA, 微调, 记忆化, 数据泄露", "comments": "这项工作揭示了 LoRA 微调在记忆化方面与传统微调和预训练的显著差异，强调了 LoRA 在降低数据泄露风险方面的潜力，这对于LLM的安全性和隐私性具有重要意义。"}}
{"id": "2506.20807", "title": "GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization", "authors": ["Martin Andrews", "Sam Witteveen"], "summary": "Optimizing GPU kernels for high performance is a complex task, often\ndemanding deep architectural knowledge, extensive profiling, and iterative\nexperimentation. This challenge is amplified when targeting newer or\nless-documented GPU architectures where traditional development aids are\nscarce. This paper introduces an LLM-powered \"GPU Kernel Scientist,\" an\nautomated methodology for iteratively refining accelerator kernels.\n  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)\nstrategically selecting promising prior code versions as a basis for new\niterations; (b) generating hypotheses for optimization experiments, based on\nexisting code and assimilated knowledge from general GPU literature; and (c)\nautonomously implementing these experiments through code modification and\nsubsequent submission to an external evaluation system, using only observed\ntiming data as performance feedback. We detail how this approach navigates the\nchallenges of the AMD MI300 target architecture and leverages LLMs to\ncompensate for limited domain-specific human expertise.\n  Since quantitative results from an ongoing performance competition were\nembargoed on paper submission date, we present the architectural design,\noperational workflow, and qualitative insights, highlighting the potential of\nLLM-driven agents to democratise and accelerate GPU kernel optimization,\nespecially in resource-constrained or rapidly evolving hardware environments.", "comment": "4 page paper plus Appendices. Accepted to the ES-FoMo \"Efficient\n  Systems for Foundation Models\" workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.20807v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20807v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "GPU核科学家：一个由LLM驱动的迭代内核优化框架", "tldr": "本文介绍了一个名为“GPU核科学家”的LLM驱动框架，用于自动化迭代优化GPU内核，特别适用于新或文档较少的架构。", "motivation": "优化GPU内核以实现高性能是一项复杂任务，需要深入的架构知识和大量实验，尤其是在针对较新或文档较少的GPU架构时，挑战更大。现有传统开发辅助工具稀缺，因此需要一种自动化解决方案。", "method": "“GPU核科学家”框架利用LLM进行多阶段进化过程：(a) 战略性选择有前景的现有代码版本；(b) 基于现有代码和GPU文献知识生成优化假设；(c) 通过代码修改并利用外部评估系统的时序数据反馈，自主执行实验。该方法旨在解决AMD MI300架构的挑战，并弥补领域专业知识的不足。", "result": "由于定量结果在论文提交时被禁止公开，本文主要展示了该架构的设计、操作工作流程和定性见解。研究强调了LLM驱动的代理在民主化和加速GPU内核优化方面的潜力，尤其是在资源受限或硬件快速发展的环境中。", "conclusion": "LLM驱动的代理具有民主化和加速GPU内核优化的潜力，特别是在资源有限或硬件环境快速演进的情况下。", "translation": "优化GPU内核以实现高性能是一项复杂的任务，通常需要深入的架构知识、广泛的性能分析和迭代实验。当针对较新或文档较少的GPU架构时，传统开发辅助工具稀缺，这一挑战尤为突出。本文介绍了一种由LLM驱动的“GPU核科学家”，这是一种用于迭代优化加速器内核的自动化方法。\n我们的方法采用LLM进行多阶段、进化的过程：(a) 战略性地选择有前景的先前代码版本作为新迭代的基础；(b) 基于现有代码和从通用GPU文献中吸收的知识，生成优化实验的假设；(c) 通过代码修改并随后提交到外部评估系统，仅使用观察到的时间数据作为性能反馈，自主实施这些实验。我们详细介绍了该方法如何应对AMD MI300目标架构的挑战，并利用LLM弥补有限的领域特定人类专业知识。\n由于在论文提交日期，正在进行的性能竞赛的定量结果被禁止公开，因此我们展示了架构设计、操作工作流程和定性见解，强调了LLM驱动的代理在民主化和加速GPU内核优化方面的潜力，特别是在资源受限或快速发展的硬件环境中。", "summary": "本文提出了一个名为“GPU核科学家”的LLM驱动自动化框架，旨在迭代优化GPU内核。该框架通过利用LLM选择代码版本、生成优化假设以及基于性能反馈自主执行实验，解决了GPU内核优化的复杂性，尤其是在面对新型架构时。尽管定量结果尚未公开，论文详细阐述了其架构设计和工作流程，强调了该框架在严苛硬件环境中民主化和加速GPU内核优化的巨大潜力。", "keywords": "GPU内核优化, LLM, 迭代优化, 自动化框架, 加速器内核", "comments": "该论文创新性地将大型语言模型（LLM）应用于GPU内核优化这一高度专业化且传统上依赖人工的任务。其核心优势在于自动化迭代实验过程，并弥补了领域专业知识的不足，这对于新兴或文档较少的架构尤为重要。尽管缺乏具体的定量结果是一个限制，但所提出的框架设计和定性见解表明，它在民主化高性能计算方面具有显著潜力。"}}
{"id": "2506.21077", "title": "CURL-SLAM: Continuous and Compact LiDAR Mapping", "authors": ["Kaicheng Zhang", "Shida Xu", "Yining Ding", "Xianwen Kong", "Sen Wang"], "summary": "This paper studies 3D LiDAR mapping with a focus on developing an updatable\nand localizable map representation that enables continuity, compactness and\nconsistency in 3D maps. Traditional LiDAR Simultaneous Localization and Mapping\n(SLAM) systems often rely on 3D point cloud maps, which typically require\nextensive storage to preserve structural details in large-scale environments.\nIn this paper, we propose a novel paradigm for LiDAR SLAM by leveraging the\nContinuous and Ultra-compact Representation of LiDAR (CURL) introduced in [1].\nOur proposed LiDAR mapping approach, CURL-SLAM, produces compact 3D maps\ncapable of continuous reconstruction at variable densities using CURL's\nspherical harmonics implicit encoding, and achieves global map consistency\nafter loop closure. Unlike popular Iterative Closest Point (ICP)-based LiDAR\nodometry techniques, CURL-SLAM formulates LiDAR pose estimation as a unique\noptimization problem tailored for CURL and extends it to local Bundle\nAdjustment (BA), enabling simultaneous pose refinement and map correction.\nExperimental results demonstrate that CURL-SLAM achieves state-of-the-art 3D\nmapping quality and competitive LiDAR trajectory accuracy, delivering\nsensor-rate real-time performance (10 Hz) on a CPU. We will release the\nCURL-SLAM implementation to the community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21077v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21077v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CURL-SLAM：连续紧凑激光雷达建图", "tldr": "CURL-SLAM提出了一种基于CURL的激光雷达建图新范式，实现了连续、紧凑、一致的三维地图，并在CPU上达到实时性能。", "motivation": "传统激光雷达SLAM系统通常依赖于三维点云地图，这需要大量的存储空间来保留大规模环境中的结构细节，导致地图不够紧凑。", "method": "CURL-SLAM利用激光雷达连续超紧凑表示（CURL）的球谐函数隐式编码，生成紧凑且可变密度连续重建的三维地图，并在闭环后实现全局地图一致性。它将激光雷达姿态估计公式化为针对CURL的独特优化问题，并扩展到局部束调整（BA），实现姿态细化和地图校正。", "result": "CURL-SLAM实现了最先进的三维建图质量和有竞争力的激光雷达轨迹精度，并在CPU上达到了传感器速率实时性能（10 Hz）。", "conclusion": "CURL-SLAM通过引入基于CURL的新范式，有效解决了传统激光雷达SLAM在地图存储、连续性和一致性方面的挑战，并展现出卓越的性能和实时处理能力。", "translation": "这篇论文研究了三维激光雷达建图，重点在于开发一种可更新、可定位的地图表示，以实现三维地图的连续性、紧凑性和一致性。传统的激光雷达同时定位与建图（SLAM）系统通常依赖于三维点云地图，这通常需要大量的存储空间来保留大规模环境中的结构细节。在这篇论文中，我们通过利用[1]中介绍的激光雷达连续超紧凑表示（CURL），提出了一种新颖的激光雷达SLAM范式。我们提出的激光雷达建图方法，CURL-SLAM，利用CURL的球谐函数隐式编码，生成能够以可变密度连续重建的紧凑三维地图，并在闭环后实现全局地图一致性。与流行的基于迭代最近点（ICP）的激光雷达里程计技术不同，CURL-SLAM将激光雷达姿态估计公式化为针对CURL量身定制的独特优化问题，并将其扩展到局部束调整（BA），从而实现同步姿态细化和地图校正。实验结果表明，CURL-SLAM实现了最先进的三维建图质量和有竞争力的激光雷达轨迹精度，并在CPU上提供了传感器速率的实时性能（10 Hz）。我们将向社区发布CURL-SLAM的实现。", "summary": "CURL-SLAM提出一种新颖的激光雷达SLAM范式，通过利用CURL（Continuous and Ultra-compact Representation of LiDAR）的球谐函数隐式编码，解决了传统点云地图存储量大和连续性差的问题。该方法能够生成紧凑、可变密度连续重建的三维地图，并在闭环后实现全局一致性。CURL-SLAM还将激光雷达姿态估计设计为针对CURL的优化问题，并结合局部束调整实现姿态和地图的同步校正。实验结果表明，CURL-SLAM在三维建图质量、轨迹精度和CPU实时性能方面均达到先进水平。", "keywords": "激光雷达SLAM, CURL, 紧凑地图, 连续重建, 球谐函数", "comments": "该论文的创新点在于引入CURL作为激光雷达地图的表示方式，显著提升了地图的紧凑性和连续性，并克服了传统点云地图存储大的缺点。其提出的针对CURL的姿态优化和局部束调整策略也具有独特性。在CPU上实现实时性能，表明其具有较好的实用潜力。"}}
{"id": "2506.20442", "title": "When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact", "authors": ["Tianyao Shi", "Ritbik Kumar", "Inez Hua", "Yi Ding"], "summary": "Biodiversity loss is a critical planetary boundary, yet its connection to\ncomputing remains largely unexamined. Prior sustainability efforts in computing\nhave focused on carbon and water, overlooking biodiversity due to the lack of\nappropriate metrics and modeling frameworks. This paper presents the first\nend-to-end analysis of biodiversity impact from computing systems. We introduce\ntwo new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity\nIndex (OBI)--to quantify biodiversity impact across the lifecycle, and present\nFABRIC, a modeling framework that links computing workloads to biodiversity\nimpacts. Our evaluation highlights the need to consider biodiversity alongside\ncarbon and water in sustainable computing design and optimization. The code is\navailable at https://github.com/TianyaoShi/FABRIC.", "comment": "Accepted by HotCarbon' 25", "pdf_url": "http://arxiv.org/pdf/2506.20442v1", "categories": ["cs.CY", "cs.AR", "cs.DC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20442v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "当服务器遇到物种：计算对生物多样性影响的全生命周期视角", "tldr": "本文首次对计算系统对生物多样性的影响进行了端到端分析，提出了新的衡量指标和建模框架FABRIC，强调在可持续计算设计中需将生物多样性与碳和水一同考虑。", "motivation": "生物多样性丧失是一个关键的地球边界问题，但其与计算领域之间的联系却在很大程度上未被研究。先前的计算可持续性努力主要关注碳和水，由于缺乏合适的衡量标准和建模框架，生物多样性一直被忽视。", "method": "本文提出了首次端到端分析计算系统对生物多样性影响的方法。引入了两个新的指标——具身生物多样性指数（Embodied Biodiversity Index, EBI）和运行生物多样性指数（Operational Biodiversity Index, OBI），以量化生命周期内的生物多样性影响。同时，提出了一个名为FABRIC的建模框架，将计算工作负载与生物多样性影响联系起来。", "result": "评估结果强调，在可持续计算设计和优化中，需要将生物多样性与碳和水一同考虑。", "conclusion": "本研究首次全面分析了计算系统对生物多样性的影响，并提出了量化和建模工具，得出结论认为生物多样性应被纳入可持续计算的设计和优化考量中，与碳和水同等重要。", "translation": "生物多样性丧失是一个关键的地球边界问题，但其与计算领域之间的联系却在很大程度上未被研究。先前的计算可持续性努力主要关注碳和水，由于缺乏合适的衡量标准和建模框架，生物多样性一直被忽视。本文首次对计算系统对生物多样性的影响进行了端到端分析。我们引入了两个新的指标——具身生物多样性指数（EBI）和运行生物多样性指数（OBI）——以量化生命周期内的生物多样性影响，并提出了FABRIC，一个将计算工作负载与生物多样性影响联系起来的建模框架。我们的评估强调，在可持续计算设计和优化中，需要将生物多样性与碳和水一同考虑。代码可在https://github.com/TianyaoShi/FABRIC获取。", "summary": "本文首次全面探讨了计算系统对生物多样性的影响，指出以往的可持续计算研究忽视了这一方面。为填补空白，研究引入了具身生物多样性指数（EBI）和运行生物多样性指数（OBI）两个新指标，并开发了FABRIC建模框架，用于量化和关联计算活动与生物多样性影响。研究结果强调了在可持续计算设计中，将生物多样性与碳和水同等考虑的必要性。", "keywords": "生物多样性影响, 可持续计算, 全生命周期分析, EBI, OBI, FABRIC", "comments": "本文的创新之处在于首次将生物多样性与计算领域联系起来，并提出了具体的量化指标（EBI和OBI）和建模框架（FABRIC），填补了该领域的一个重要空白。这对于推动更全面的可持续计算具有重要意义，促使行业在关注碳足迹和水资源消耗的同时，也考虑其对生物多样性的影响。"}}
{"id": "2506.21239", "title": "Exact Time-Varying Turnpikes for Dynamic Operation of District Heating Networks", "authors": ["Max Rose", "Hannes Gernandt", "Timm Faulwasser", "Johannes Schiffer"], "summary": "District heating networks (DHNs) are crucial for decarbonizing the heating\nsector. Yet, their efficient and reliable operation requires the coordination\nof multiple heat producers and the consideration of future demands. Predictive\nand optimization-based control is commonly used to address this task, but\nexisting results for DHNs do not account for time-varying problem aspects.\nSince the turnpike phenomenon can serve as a basis for model predictive control\ndesign and analysis, this paper examines its role in DHN optimization by\nanalyzing the underlying optimal control problem with time-varying prices and\ndemands. That is, we derive conditions for the existence of a unique\ntime-varying singular arc, which constitutes the time varying turnpike, and we\nprovide its closed-form expression. Additionally, we present converse turnpike\nresults showing a exact time-varying case implies strict dissipativity of the\noptimal control problem. A numerical example illustrates our findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21239v1", "categories": ["math.OC", "cs.SY", "eess.SY"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.21239v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "区域供热网络动态运行的精确时变转轨", "tldr": "本文研究了区域供热网络优化中的时变转轨现象，推导了其存在条件和闭式表达式，并展示了其与耗散性的关系。", "motivation": "区域供热网络对供热部门脱碳至关重要，但其高效可靠运行需要协调多个热源并考虑未来需求。现有基于预测和优化的控制方法未考虑时变问题。", "method": "本文通过分析具有时变价格和需求的最优控制问题，研究了转轨现象在区域供热网络优化中的作用。具体地，推导了唯一时变奇异弧（即时变转轨）存在的条件及其闭式表达式。此外，还提出了逆向转轨结果，表明精确时变情况意味着最优控制问题的严格耗散性。", "result": "推导了唯一时变奇异弧（构成时变转轨）存在的条件，并提供了其闭式表达式。提出了逆向转轨结果，表明精确时变情况意味着最优控制问题的严格耗散性。通过数值例子验证了研究结果。", "conclusion": "本文为区域供热网络优化提供了精确的时变转轨，可作为模型预测控制设计和分析的基础，并揭示了其与最优控制问题严格耗散性的关系。", "translation": "区域供热网络（DHNs）对于供热部门的脱碳至关重要。然而，其高效可靠的运行需要协调多个热生产者并考虑未来的需求。基于预测和优化的控制方法通常用于解决这项任务，但现有的DHNs研究结果没有考虑时变问题。由于转轨现象可以作为模型预测控制设计和分析的基础，本文通过分析具有时变价格和需求的基本最优控制问题，研究了其在DHN优化中的作用。也就是说，我们推导了唯一时变奇异弧存在的条件，该奇异弧构成了时变转轨，并提供了其闭式表达式。此外，我们还提出了逆向转轨结果，表明精确时变情况意味着最优控制问题的严格耗散性。一个数值例子说明了我们的发现。", "summary": "本文研究了区域供热网络（DHNs）在时变条件下的高效运行优化问题。针对现有控制方法未能考虑时变因素的不足，论文利用转轨现象作为理论基础，分析了DHNs的最优控制问题。研究推导了唯一时变奇异弧（即时变转轨）的存在条件及其闭式表达式，并进一步揭示了精确时变转轨与最优控制问题严格耗散性之间的关系。这些发现为DHNs的模型预测控制设计和分析提供了新的理论工具，并通过数值案例进行了验证。", "keywords": "区域供热网络, 转轨现象, 最优控制, 时变, 耗散性", "comments": "本文的创新之处在于将时变转轨理论应用于区域供热网络的优化，解决了现有方法在处理时变因素上的局限性。这为DHNs的动态运行提供了更精确的理论基础，有助于提高其能源效率和脱碳潜力。该研究通过严谨的数学推导，不仅丰富了最优控制理论在实际工程中的应用，也为未来DHNs的智能控制提供了新的视角。"}}
{"id": "2506.20900", "title": "The Role of Cyclopean-Eye in Stereo Vision", "authors": ["Sherlon Almeida da Silva", "Davi Geiger", "Luiz Velho", "Moacir Antonelli Ponti"], "summary": "This work investigates the geometric foundations of modern stereo vision\nsystems, with a focus on how 3D structure and human-inspired perception\ncontribute to accurate depth reconstruction. We revisit the Cyclopean Eye model\nand propose novel geometric constraints that account for occlusions and depth\ndiscontinuities. Our analysis includes the evaluation of stereo feature\nmatching quality derived from deep learning models, as well as the role of\nattention mechanisms in recovering meaningful 3D surfaces. Through both\ntheoretical insights and empirical studies on real datasets, we demonstrate\nthat combining strong geometric priors with learned features provides internal\nabstractions for understanding stereo vision systems.", "comment": "arXiv admin note: text overlap with arXiv:2502.21280", "pdf_url": "http://arxiv.org/pdf/2506.20900v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20900v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "独眼巨人眼在立体视觉中的作用", "tldr": "本文探讨了独眼巨人眼模型在现代立体视觉系统中的几何基础，并结合深度学习和注意力机制，证明了几何先验与学习特征结合能有效提升深度重建。", "motivation": "探讨现代立体视觉系统中的几何基础，特别是3D结构和受人类启发的感知如何促进准确的深度重建。", "method": "重新审视独眼巨人眼模型，提出新的几何约束以处理遮挡和深度不连续性。评估深度学习模型导出的立体特征匹配质量，以及注意力机制在恢复有意义的3D表面中的作用。通过理论分析和真实数据集的实证研究。", "result": "结合强大的几何先验与学习特征为理解立体视觉系统提供了内部抽象。", "conclusion": "结合强大的几何先验与学习特征为理解立体视觉系统提供了内部抽象。", "translation": "这项工作研究了现代立体视觉系统的几何基础，重点关注3D结构和受人类启发的感知如何有助于准确的深度重建。我们重新审视了独眼巨人眼模型，并提出了新的几何约束，以解释遮挡和深度不连续性。我们的分析包括评估来自深度学习模型的立体特征匹配质量，以及注意力机制在恢复有意义的3D表面中的作用。通过理论见解和真实数据集的实证研究，我们证明了将强大的几何先验与学习特征相结合，为理解立体视觉系统提供了内部抽象。", "summary": "本文深入研究了现代立体视觉系统的几何原理，并重新审视了独眼巨人眼模型。研究引入了新的几何约束来处理遮挡和深度不连续性，并评估了深度学习模型产生的立体特征匹配质量以及注意力机制在3D表面恢复中的作用。通过理论和实验，论文指出结合强几何先验和学习特征能有效提升对立体视觉系统的理解和深度重建。", "keywords": "立体视觉, 独眼巨人眼, 深度重建, 几何先验, 深度学习", "comments": "这篇论文通过重新审视经典的独眼巨人眼模型，并将其与现代深度学习技术（如深度学习特征匹配和注意力机制）相结合，为立体视觉的深度重建提供了新的视角。其创新点在于强调了几何先验在数据驱动方法中的重要性，揭示了将传统几何理论与现代AI技术融合的潜力，可能为更鲁棒和准确的立体视觉系统奠定基础。"}}
{"id": "2506.21506", "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jiménez Gutiérrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.", "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/", "pdf_url": "http://arxiv.org/pdf/2506.21506v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21506v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Mind2Web 2：使用代理作为评判者评估代理式搜索", "tldr": "本文介绍了Mind2Web 2，一个用于评估代理式搜索系统的新基准，以及一个新颖的“代理作为评判者”框架，以解决现有评估方法的不足。研究发现，最先进的系统已能达到人类性能的50-70%。", "motivation": "现有评估基准和方法已无法满足日益复杂的代理式搜索（如深度研究系统）的需求，这些系统涉及自主网络浏览和信息综合，且其答案是时变和开放式的，而当前方法多假设短搜索周期和静态答案。", "method": "论文引入了Mind2Web 2，一个包含130个真实、高质量、长周期任务的基准，这些任务需要实时网络浏览和大量信息综合。为评估时变和复杂答案，提出了一种新颖的“代理作为评判者”框架，该框架基于树状结构评分标准构建特定任务的判别代理，以自动评估答案的正确性和来源归属。研究对九个前沿代理式搜索系统和人类表现进行了综合评估，并进行了详细的错误分析。", "result": "最佳表现系统OpenAI Deep Research在花费一半时间的情况下，已能达到人类性能的50-70%。", "conclusion": "Mind2Web 2为开发和评估下一代代理式搜索系统提供了坚实的基础。", "translation": "代理式搜索，例如深度研究系统，其中大型语言模型自主浏览网页、综合信息并返回全面且有引文支持的答案，代表了用户与网络规模信息交互方式的重大转变。虽然有望提高效率和减轻认知负担，但代理式搜索日益增长的复杂性和开放性已超越了现有的评估基准和方法，这些方法大多假设短搜索周期和静态答案。在本文中，我们引入了Mind2Web 2，一个包含130个真实、高质量、长周期任务的基准，这些任务需要实时网络浏览和广泛的信息综合，耗费了超过1000小时的人工劳动构建。为了解决评估时变和复杂答案的挑战，我们提出了一种新颖的“代理作为评判者”框架。我们的方法基于树状结构评分标准构建特定任务的判别代理，以自动评估答案的正确性和来源归属。我们对九个前沿代理式搜索系统和人类表现进行了综合评估，并进行了详细的错误分析，以期为未来的发展提供见解。表现最佳的系统OpenAI Deep Research在花费一半时间的情况下，已能达到人类性能的50-70%，显示出巨大的潜力。总而言之，Mind2Web 2为开发和评估下一代代理式搜索系统提供了坚实的基础。", "summary": "本文针对现有评估方法无法适应复杂、开放式代理式搜索的挑战，提出了Mind2Web 2，这是一个包含130个真实、长周期任务的新基准。为有效评估时变和复杂答案，论文引入了“代理作为评判者”框架，利用基于树状评分标准的判别代理自动评估答案正确性和来源归属。通过对九个前沿系统和人类表现的综合评估，研究发现最佳系统OpenAI Deep Research已能以一半时间达到人类性能的50-70%，展示了代理式搜索的巨大潜力。Mind2Web 2旨在为未来代理式搜索系统的开发和评估提供严格的基础。", "keywords": "代理式搜索, 评估基准, 代理作为评判者, 大型语言模型, Mind2Web 2", "comments": "Mind2Web 2的创新之处在于其构建了一个大规模、高质量、长周期任务的基准，并提出了“代理作为评判者”这一新颖的评估框架，有效解决了代理式搜索系统评估中答案时变性和复杂性的难题。这项工作对于推动下一代代理式搜索系统的发展和标准化评估具有重要意义，尤其是在强调自动化评估和真实世界任务复杂性方面。"}}
{"id": "2506.21198", "title": "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation", "authors": ["Yihong Cao", "Jiaming Zhang", "Xu Zheng", "Hao Shi", "Kunyu Peng", "Hang Liu", "Kailun Yang", "Hui Zhang"], "summary": "Panoramic image processing is essential for omni-context perception, yet\nfaces constraints like distortions, perspective occlusions, and limited\nannotations. Previous unsupervised domain adaptation methods transfer knowledge\nfrom labeled pinhole data to unlabeled panoramic images, but they require\naccess to source pinhole data. To address these, we introduce a more practical\ntask, i.e., Source-Free Occlusion-Aware Seamless Segmentation (SFOASS), and\npropose its first solution, called UNconstrained Learning Omni-Context\nKnowledge (UNLOCK). Specifically, UNLOCK includes two key modules: Omni\nPseudo-Labeling Learning and Amodal-Driven Context Learning. While adapting\nwithout relying on source data or target labels, this framework enhances models\nto achieve segmentation with 360{\\deg} viewpoint coverage and occlusion-aware\nreasoning. Furthermore, we benchmark the proposed SFOASS task through both\nreal-to-real and synthetic-to-real adaptation settings. Experimental results\nshow that our source-free method achieves performance comparable to\nsource-dependent methods, yielding state-of-the-art scores of 10.9 in mAAP and\n11.6 in mAP, along with an absolute improvement of +4.3 in mAPQ over the\nsource-only method. All data and code will be made publicly available at\nhttps://github.com/yihong-97/UNLOCK.", "comment": "Accepted to ICCV 2025. All data and code will be made publicly\n  available at https://github.com/yihong-97/UNLOCK", "pdf_url": "http://arxiv.org/pdf/2506.21198v1", "categories": ["cs.CV", "cs.RO", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21198v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "解锁约束：无源遮挡感知无缝分割", "tldr": "本文提出了一种名为SFOASS的无源遮挡感知无缝分割任务及其首个解决方案UNLOCK，该方案无需源数据或目标标签，即可在全景图像上实现与有源方法相当的分割性能，并具有遮挡感知能力。", "motivation": "全景图像处理在全方位感知中至关重要，但面临扭曲、透视遮挡和标注受限等挑战。现有的无监督域适应方法需要访问源针孔数据，这在实际应用中是一种限制。", "method": "本文引入了无源遮挡感知无缝分割（SFOASS）任务，并提出了第一个解决方案UNconstrained Learning Omni-Context Knowledge (UNLOCK)。UNLOCK框架包括两个关键模块：全方位伪标签学习（Omni Pseudo-Labeling Learning）和非模态驱动上下文学习（Amodal-Driven Context Learning）。该方法无需依赖源数据或目标标签，即可使模型实现360度视角覆盖和遮挡感知推理的分割。", "result": "实验结果表明，所提出的无源方法实现了与依赖源数据方法相当的性能，在mAAP上达到10.9，mAP上达到11.6，均为最先进水平，并且相对于仅使用源数据的方法，mAPQ有+4.3的绝对提升。", "conclusion": "本文提出的无源遮挡感知无缝分割方法UNLOCK在全景图像处理中表现出色，实现了与有源方法相当的性能，并在没有源数据或目标标签的情况下，有效解决了全景图像分割中的遮挡和视角覆盖问题。", "translation": "全景图像处理对于全方位感知至关重要，但面临扭曲、透视遮挡和标注受限等约束。以前的无监督域适应方法将知识从带标签的针孔数据转移到无标签的全景图像，但它们需要访问源针孔数据。为了解决这些问题，我们引入了一个更实用的任务，即无源遮挡感知无缝分割（Source-Free Occlusion-Aware Seamless Segmentation, SFOASS），并提出了其第一个解决方案，名为无约束学习全方位知识（UNconstrained Learning Omni-Context Knowledge, UNLOCK）。具体而言，UNLOCK包括两个关键模块：全方位伪标签学习（Omni Pseudo-Labeling Learning）和非模态驱动上下文学习（Amodal-Driven Context Learning）。在不依赖源数据或目标标签的情况下进行适应，该框架增强了模型，以实现360度视角覆盖和遮挡感知推理的分割。此外，我们通过真实到真实和合成到真实两种适应设置，对所提出的SFOASS任务进行了基准测试。实验结果表明，我们的无源方法实现了与依赖源数据方法相当的性能，在mAAP上达到10.9，mAP上达到11.6，均为最先进水平，并且相对于仅使用源数据的方法，mAPQ有+4.3的绝对提升。所有数据和代码将公开在https://github.com/yihong-97/UNLOCK。", "summary": "本文针对全景图像处理中存在的扭曲、遮挡和标注限制以及现有域适应方法对源数据的依赖问题，提出了无源遮挡感知无缝分割（SFOASS）任务及其首个解决方案UNLOCK。UNLOCK包含全方位伪标签学习和非模态驱动上下文学习模块，使其能够在不依赖源数据或目标标签的情况下，实现360度视角覆盖和遮挡感知分割。实验证明，UNLOCK在性能上可与依赖源数据的方法媲美，并取得了最先进的结果。", "keywords": "无源分割, 全景图像, 遮挡感知, 域适应, 无监督学习", "comments": "该论文的创新点在于提出了无源遮挡感知无缝分割这一更实用的任务，并提供了首个解决方案UNLOCK。其核心优势在于无需访问源数据和目标标签即可实现高性能的全景图像分割，这大大降低了实际应用的门槛。通过集成全方位伪标签学习和非模态驱动上下文学习，UNLOCK有效地解决了全景图像中的扭曲、遮挡和视角覆盖问题，为全景感知领域提供了一个重要且实用的进展。"}}
{"id": "2506.21375", "title": "Integrating Movable Antennas and Intelligent Reflecting Surfaces for Coverage Enhancement", "authors": ["Ying Gao", "Qingqing Wu", "Weidong Mei", "Guangji Chen", "Wen Chen", "Ziyuan Zheng"], "summary": "This paper investigates an intelligent reflecting surface (IRS)-aided movable\nantenna (MA) system, where multiple IRSs cooperate with a multi-MA base station\nto extend wireless coverage to multiple designated target areas. The objective\nis to maximize the worst-case signal-to-noise ratio (SNR) across all locations\nwithin these areas through joint optimization of MA positions, IRS reflection\ncoefficients, and transmit beamforming. To achieve this while balancing the\nperformance-cost trade-off, we propose three coverage-enhancement schemes: the\narea-adaptive MA-IRS scheme, the area-adaptive MA-staIRS scheme, and the shared\nMA-staIRS scheme, where staIRS denotes static IRSs with reflection coefficients\nconfigured only once during installation. These schemes lead to challenging\nnon-convex optimization problems with implicit objective functions, which are\ndifficult to solve optimally. To address these problems, we propose a general\nalgorithmic framework that can be applied to solve each problem efficiently\nalbeit suboptimally. Simulation results demonstrate that: 1) the proposed\nMA-based schemes consistently outperform their fixed-position antenna\n(FPA)-based counterparts under both area-adaptive and static IRS\nconfigurations, with the area-adaptive MA-IRS scheme achieving the best\nworst-case SNR performance; 2) as transmit antennas are typically far fewer\nthan IRS elements, the area-adaptive MA-staIRS scheme may underperform the\nbaseline FPA scheme with area-adaptive IRSs in terms of the worst-case SNR, but\na modest increase in antenna number can reverse this trend; 3) under a fixed\ntotal cost, the optimal MA-to-IRS-element ratio for the worst-case SNR\nmaximization is empirically found to be proportional to the reciprocal of their\nunit cost ratio.", "comment": "13 pages, 8 figures, submitted to an IEEE journal for possible\n  publication on on May 8, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21375v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21375v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "集成可移动天线和智能反射面以增强覆盖范围", "tldr": "该论文研究了一种结合可移动天线（MA）和智能反射面（IRS）的系统，旨在通过联合优化MA位置、IRS反射系数和发射波束成形来最大化目标区域的最差情况信噪比（SNR），从而扩展无线覆盖。文章提出了三种覆盖增强方案和一个通用算法框架，并仿真表明MA方案优于固定位置天线（FPA）方案。", "motivation": "该论文旨在研究一种智能反射面（IRS）辅助的可移动天线（MA）系统，其中多个IRS与多MA基站协作，以将无线覆盖范围扩展到多个指定目标区域。目标是通过联合优化MA位置、IRS反射系数和发射波束成形，最大化这些区域内所有位置的最差情况信噪比（SNR）。", "method": "本文研究了IRS辅助的可移动天线（MA）系统，旨在通过联合优化MA位置、IRS反射系数和发射波束成形来最大化目标区域的最差情况信噪比（SNR）。为平衡性能与成本，提出了三种覆盖增强方案：区域自适应MA-IRS、区域自适应MA-staIRS和共享MA-staIRS。针对这些非凸优化问题，提出了一个通用的算法框架进行高效（尽管次优）求解。", "result": "1. 所提出的基于MA的方案在区域自适应和静态IRS配置下始终优于基于固定位置天线（FPA）的对应方案，其中区域自适应MA-IRS方案实现了最佳的最差情况SNR性能。\n2. 由于发射天线数量通常远少于IRS单元，区域自适应MA-staIRS方案在最差情况SNR方面可能不如结合区域自适应IRS的基线FPA方案，但适度增加天线数量可以扭转这一趋势。\n3. 在固定总成本下，经验发现用于最差情况SNR最大化的最优MA与IRS单元比例与其单位成本比的倒数成正比。", "conclusion": "本文证明，结合可移动天线和智能反射面能有效增强无线覆盖，特别是在最差情况信噪比方面，MA方案显著优于固定位置天线方案。研究还揭示了天线数量和成本对系统性能的影响，为实际部署提供了优化方向。", "translation": "本文研究了一种智能反射面（IRS）辅助的可移动天线（MA）系统，其中多个IRS与一个多MA基站协作，以将无线覆盖范围扩展到多个指定目标区域。目标是通过联合优化MA位置、IRS反射系数和发射波束成形，最大化这些区域内所有位置的最差情况信噪比（SNR）。为了在平衡性能-成本权衡的同时实现这一目标，我们提出了三种覆盖增强方案：区域自适应MA-IRS方案、区域自适应MA-staIRS方案和共享MA-staIRS方案，其中staIRS表示在安装时仅配置一次反射系数的静态IRS。这些方案导致具有隐式目标函数的挑战性非凸优化问题，难以找到最优解。为了解决这些问题，我们提出了一种通用的算法框架，可以有效地（尽管不是最优地）解决每个问题。仿真结果表明：1）所提出的基于MA的方案在区域自适应和静态IRS配置下始终优于其基于固定位置天线（FPA）的对应方案，其中区域自适应MA-IRS方案实现了最佳的最差情况SNR性能；2）由于发射天线通常远少于IRS单元，区域自适应MA-staIRS方案在最差情况SNR方面可能不如基线FPA方案与区域自适应IRS结合，但天线数量的适度增加可以扭转这一趋势；3）在固定总成本下，经验发现用于最差情况SNR最大化的最优MA与IRS单元比例与其单位成本比的倒数成正比。", "summary": "本文研究了一种IRS辅助的可移动天线（MA）系统，旨在扩展无线覆盖并最大化目标区域的最差情况信噪比。论文提出了三种新型覆盖增强方案（区域自适应MA-IRS、区域自适应MA-staIRS和共享MA-staIRS）以及一个通用的算法框架来解决由此产生的非凸优化问题。仿真结果表明，基于MA的方案显著优于固定位置天线方案，其中区域自适应MA-IRS方案表现最佳。研究还深入探讨了天线数量对性能的影响以及成本与性能之间的权衡，为最优系统设计提供了见解。", "keywords": "可移动天线, 智能反射面, 覆盖增强, 最差情况信噪比, 联合优化", "comments": "该论文创新性地将可移动天线与智能反射面相结合，以解决无线覆盖扩展这一未来通信系统的关键挑战。对MA位置、IRS系数和发射波束成形的联合优化方法复杂但前景广阔。提出的算法框架能够处理非凸问题，具有实用价值，即便其结果次优。关于成本-性能权衡的经验发现对于实际部署具有重要的参考意义。"}}
{"id": "2506.21307", "title": "Guarding Offices with Maximum Dispersion", "authors": ["Sándor P. Fekete", "Kai Kobbe", "Dominik Krupke", "Joseph S. B. Mitchell", "Christian Rieck", "Christian Scheffer"], "summary": "We investigate the Dispersive Art Gallery Problem with vertex guards and\nrectangular visibility ($r$-visibility) for a class of orthogonal polygons that\nreflect the properties of real-world floor plans: these office-like polygons\nconsist of rectangular rooms and corridors. In the dispersive variant of the\nArt Gallery Problem, the objective is not to minimize the number of guards but\nto maximize the minimum geodesic $L_1$-distance between any two guards, called\nthe dispersion distance.\n  Our main contributions are as follows. We prove that determining whether a\nvertex guard set can achieve a dispersion distance of $4$ in office-like\npolygons is NP-complete, where vertices of the polygon are restricted to\ninteger coordinates. Additionally, we present a simple worst-case optimal\nalgorithm that guarantees a dispersion distance of $3$ in polynomial time. Our\ncomplexity result extends to polyominoes, resolving an open question posed by\nRieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be\nrational, we establish analogous results, proving that achieving a dispersion\ndistance of $2+\\varepsilon$ is NP-hard for any $\\varepsilon > 0$, while the\nclassic Art Gallery Problem remains solvable in polynomial time for this class\nof polygons. Furthermore, we give a straightforward polynomial-time algorithm\nthat computes worst-case optimal solutions with a dispersion distance of $2$.\n  On the other hand, for the more restricted class of hole-free independent\noffice-like polygons, we propose a dynamic programming approach that computes\noptimal solutions. Moreover, we demonstrate that the problem is practically\ntractable for arbitrary orthogonal polygons. To this end, we compare solvers\nbased on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently\ncompute optimal solutions for randomly generated instances with up to $1600$\nvertices in under $15$s.", "comment": "40 pages, 29 figures, to appear in the proceedings 50th International\n  Symposium on Mathematical Foundations of Computer Science (MFCS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.21307v1", "categories": ["cs.CG", "cs.DS", "F.2.2"], "cate": "cs.CG", "url": "http://arxiv.org/abs/2506.21307v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "以最大离散度守卫办公室", "tldr": "该论文研究了办公楼多边形中的离散美术馆问题，证明了某些离散距离的NP完全性，提供了其他距离的多项式时间算法，并展示了SAT求解器的实际可行性。", "motivation": "该研究旨在解决离散美术馆问题，目标是最大化任意两个守卫之间的最小测地L1距离（称为离散距离），而不是最小化守卫数量。研究对象是反映真实世界平面图特性的办公楼式正交多边形，这些多边形由矩形房间和走廊组成。", "method": "研究证明了在办公楼式多边形中，确定顶点守卫集能否达到4的离散距离（顶点坐标为整数）是NP完全的，并提出了一个多项式时间的最坏情况最优算法，保证离散距离为3。对于顶点坐标允许为有理数的情况，证明了达到2+ε的离散距离是NP难的，并给出了一个多项式时间算法来计算离散距离为2的最坏情况最优解。对于无孔独立办公楼式多边形，提出了动态规划方法来计算最优解。此外，通过比较SAT、CP和MIP公式的求解器，证明了该问题对于任意正交多边形在实践中是可处理的。", "result": "1. 证明了在办公楼式多边形中，确定顶点守卫集能否达到4的离散距离（顶点坐标为整数）是NP完全的。2. 提出了一个简单的最坏情况最优算法，保证在多项式时间内达到3的离散距离。3. 复杂性结果扩展到多聚骨牌，解决了Rieck和Scheffer提出的一个开放问题。4. 证明了当顶点坐标为有理数时，达到2+ε的离散距离是NP难的。5. 提供了一个直接的多项式时间算法，计算离散距离为2的最坏情况最优解。6. 对于无孔独立办公楼式多边形，动态规划方法能够计算最优解。7. SAT求解器能够高效地计算随机生成实例的最优解，对于多达1600个顶点的实例，耗时不到15秒。", "conclusion": "该论文为办公楼式多边形在r-可见性下的离散美术馆问题提供了复杂性结果（NP完全性）和多项式时间算法。同时，通过使用SAT求解器，展示了该问题在处理大型实例时的实际可行性。", "translation": "我们研究了在具有顶点守卫和矩形可见性（r-可见性）的离散美术馆问题，针对一类反映真实世界平面图特性的正交多边形：这些办公楼式多边形由矩形房间和走廊组成。在离散美术馆问题的变体中，目标不是最小化守卫数量，而是最大化任意两个守卫之间的最小测地L1距离，称为离散距离。\n我们的主要贡献如下。我们证明了在办公楼式多边形中，确定顶点守卫集能否达到4的离散距离是NP完全的，其中多边形的顶点被限制为整数坐标。此外，我们提出了一个简单的最坏情况最优算法，保证在多项式时间内达到3的离散距离。我们的复杂性结果扩展到多聚骨牌，解决了Rieck和Scheffer（CGTA 2024）提出的一个开放问题。当顶点坐标允许为有理数时，我们建立了类似的结果，证明了对于任意ε > 0，达到2+ε的离散距离是NP难的，而经典美术馆问题对于这类多边形仍然可以在多项式时间内解决。此外，我们给出了一个直接的多项式时间算法，可以计算离散距离为2的最坏情况最优解。\n另一方面，对于更受限制的无孔独立办公楼式多边形，我们提出了一种动态规划方法来计算最优解。此外，我们证明了该问题对于任意正交多边形在实践中是可处理的。为此，我们比较了基于SAT、CP和MIP公式的求解器。值得注意的是，SAT求解器能够高效地计算随机生成实例的最优解，对于多达1600个顶点的实例，耗时不到15秒。", "summary": "本论文研究了在办公楼式正交多边形中，使用顶点守卫和r-可见性的离散美术馆问题。研究确立了达到特定离散距离（整数坐标下为4，有理坐标下为2+ε）的NP完全性，并为其他距离（3和2）提供了多项式时间算法。对于更受限的情况，论文采用了动态规划方法，并通过比较SAT、CP和MIP求解器，展示了该问题对于任意正交多边形在实践中的可处理性，其中SAT求解器在大规模实例上表现出色。", "keywords": "离散美术馆问题, 正交多边形, NP完全性, 算法, SAT求解器", "comments": "该论文对离散美术馆问题做出了重要贡献，扩展了复杂性结果，并为一类实用的多边形提供了高效算法。解决了开放问题以及使用现代求解器证明了实际可行性是其显著的创新点和重要性所在。"}}
{"id": "2506.21222", "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21222v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21222v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过句法检索增强大语言模型的自动术语提取", "tldr": "该研究提出了一种基于句法检索的提示策略，用于在少样本设置下使用大型语言模型进行自动术语提取（ATE），并在三个专业ATE基准测试中表现出F1分数提升。", "motivation": "自动术语提取（ATE）对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLMs）已显著推动了各种NLP任务，但其在ATE方面的潜力尚未得到充分研究。", "method": "本研究提出了一种基于检索的提示策略，在少样本设置下，根据句法而非语义相似性选择演示。这种句法检索方法与领域无关，并为捕获术语边界提供了更可靠的指导。该方法在域内和跨域设置中进行了评估，并分析了查询句子与其检索示例之间的词汇重叠如何影响性能。", "result": "在三个专业ATE基准测试上的实验表明，句法检索提高了F1分数。", "conclusion": "研究结果强调了在将大型语言模型应用于术语提取任务时，句法线索的重要性。", "translation": "自动术语提取（ATE）识别领域特定的表达，这些表达对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLMs）已显著推动了各种NLP任务，但其在ATE方面的潜力尚未得到充分研究。我们提出了一种基于检索的提示策略，在少样本设置下，根据句法而非语义相似性选择演示。这种句法检索方法与领域无关，并为捕获术语边界提供了更可靠的指导。我们在域内和跨域设置中评估了该方法，分析了查询句子与其检索示例之间的词汇重叠如何影响性能。在三个专业ATE基准测试上的实验表明，句法检索提高了F1分数。这些发现强调了在将大型语言模型应用于术语提取任务时，句法线索的重要性。", "summary": "本研究探讨了大型语言模型（LLMs）在自动术语提取（ATE）中的应用潜力，提出了一种新颖的基于句法检索的提示策略。该策略在少样本学习环境下，通过选择句法相似而非语义相似的演示示例，为LLMs提供了更可靠的术语边界识别指导。实验结果表明，该方法在多个ATE基准测试中显著提升了F1分数，突出了句法信息在适应LLMs进行术语提取任务时的关键作用。", "keywords": "自动术语提取, 大型语言模型, 句法检索, 少样本学习, 术语边界", "comments": "这篇论文的创新点在于提出了一个新颖的、领域无关的句法检索方法，用于增强LLMs在少样本ATE任务中的表现。它成功地将LLMs的强大能力应用于一个之前未充分探索的领域，并通过强调句法线索的重要性，为未来LLMs在术语提取及相关NLP任务中的应用提供了新的视角和方向。"}}
{"id": "2506.21031", "title": "Large Language Models Acing Chartered Accountancy", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21031v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21031v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "大型语言模型在特许会计领域表现出色", "tldr": "本研究引入了CA-Ben基准，用于评估大型语言模型在印度特许会计领域的金融、法律和定量推理能力，发现GPT-4o和Claude 3.5 Sonnet表现较好，但在数值计算和法律解释方面仍存在挑战。", "motivation": "尽管大型语言模型（LLMs）正在重塑金融实践，但它们在捕捉和应用领域特定金融知识方面的有效性仍不确定。为填补印度金融背景下的关键空白，本研究旨在评估LLMs在特许会计领域的表现。", "method": "本研究引入了CA-Ben，一个专门设计用于评估LLMs金融、法律和定量推理能力的特许会计基准。CA-Ben包含来源于印度特许会计师协会（ICAI）严格考试的结构化问答数据集。研究评估了六个主流LLMs（GPT 4o, LLAMA 3.3 70B, LLAMA 3.1 405B, MISTRAL Large, Claude 3.5 Sonnet, 和 Microsoft Phi 4），并采用标准化协议进行评估。", "result": "评估结果显示LLMs的性能存在差异，其中Claude 3.5 Sonnet和GPT-4o表现优于其他模型，尤其在概念和法律推理方面。然而，在数值计算和法律解释方面出现了显著挑战。", "conclusion": "研究结果强调了当前大型语言模型的优势和局限性，并建议未来通过混合推理和检索增强生成方法进行改进，特别是在定量分析和准确的法律解释方面。", "translation": "先进的智能系统，特别是大型语言模型（LLMs），正通过自然语言处理（NLP）的进步显著重塑金融实践。然而，这些模型有效捕捉和应用领域特定金融知识的程度仍不确定。为填补广阔的印度金融背景下的关键空白，本文引入了CA-Ben，一个专门设计用于评估LLMs金融、法律和定量推理能力的特许会计基准。CA-Ben包含来源于印度特许会计师协会（ICAI）严格考试的结构化问答数据集，涵盖基础、中级和高级特许会计课程阶段。研究使用标准化协议评估了六个主流LLMs，即GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。结果显示性能存在差异，其中Claude 3.5 Sonnet和GPT-4o表现优于其他模型，尤其在概念和法律推理方面。在数值计算和法律解释方面出现了显著挑战。研究结果强调了当前LLMs的优势和局限性，并建议未来通过混合推理和检索增强生成方法进行改进，特别是在定量分析和准确的法律解释方面。", "summary": "本文介绍了CA-Ben，一个针对印度特许会计领域的全新基准，旨在评估大型语言模型（LLMs）的金融、法律和定量推理能力。该基准的数据集来源于印度特许会计师协会的考试。研究评估了GPT 4o、LLAMA 3.3、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4等六个主流LLMs，发现Claude 3.5 Sonnet和GPT-4o表现出色，尤其在概念和法律推理方面。然而，LLMs在数值计算和法律解释上仍面临挑战，研究建议未来可采用混合推理和检索增强生成方法进行改进。", "keywords": "大型语言模型, 特许会计, 金融推理, 基准测试, CA-Ben", "comments": "该论文通过引入特定领域的CA-Ben基准，解决了大型语言模型在专业金融（特许会计）领域应用有效性评估的空白。其创新之处在于构建了一个结合金融、法律和定量推理的综合性数据集，并明确指出了当前LLMs在处理复杂数值计算和精确法律解释方面的局限性，为未来LLM在专业领域的优化提供了明确方向。这对于推动LLMs在金融等高风险领域的实际应用具有重要意义。"}}
{"id": "2506.21306", "title": "On Uniform Weighted Deep Polynomial approximation", "authors": ["Kingsley Yeon", "Steven B. Damelin"], "summary": "It is a classical result in rational approximation theory that certain\nnon-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be\nefficiently approximated using rational functions with root-exponential\nconvergence in terms of degrees of freedom \\cite{Sta, GN}. In contrast,\npolynomial approximations admit only algebraic convergence by Jackson's theorem\n\\cite{Lub2}. Recent work shows that composite polynomial architectures can\nrecover exponential approximation rates even without smoothness \\cite{KY}. In\nthis work, we introduce and analyze a class of weighted deep polynomial\napproximants tailored for functions with asymmetric behavior-growing unbounded\non one side and decaying on the other. By multiplying a learnable deep\npolynomial with a one-sided weight, we capture both local non-smoothness and\nglobal growth. We show numerically that this framework outperforms Taylor,\nChebyshev, and standard deep polynomial approximants, even when all use the\nsame number of parameters. To optimize these approximants in practice, we\npropose a stable graph-based parameterization strategy building on \\cite{Jar}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21306v1", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.NA", "stat.ML"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21306v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于均匀加权深度多项式逼近", "tldr": "本文引入并分析了一类加权深度多项式逼近器，专门用于逼近具有不对称行为的函数，并在数值上证明其优于传统方法。", "motivation": "经典的理性逼近理论表明，非光滑或奇异函数可以被有理函数高效逼近，收敛速度为根指数级，而多项式逼近仅能实现代数收敛。虽然最近的工作表明复合多项式结构可以恢复指数逼近率，但针对具有不对称行为（一侧无界增长，另一侧衰减）的函数，仍需要更高效的逼近方法。", "method": "本文引入并分析了一类加权深度多项式逼近器，通过将一个可学习的深度多项式与一个单侧权重相乘，以捕获局部非光滑性和全局增长。为了在实践中优化这些逼近器，提出了一种基于图的稳定参数化策略。", "result": "数值结果表明，该框架在参数数量相同的情况下，优于泰勒、切比雪夫和标准深度多项式逼近器。", "conclusion": "通过引入加权深度多项式逼近器和稳定的参数化策略，可以有效地逼近具有不对称行为的非光滑函数，并达到比传统多项式逼近方法更好的性能。", "translation": "在有理逼近理论中，一个经典结果是，某些非光滑或奇异函数，如 $|x|$ 和 $x^{1/p}$，可以使用有理函数高效逼近，其自由度方面具有根指数收敛速度 \n\\cite{Sta, GN}。相比之下，根据杰克逊定理 \n\\cite{Lub2}，多项式逼近仅能实现代数收敛。最近的工作表明，复合多项式结构即使在没有光滑性的情况下也能恢复指数逼近率 \n\\cite{KY}。在这项工作中，我们引入并分析了一类加权深度多项式逼近器，专门针对具有不对称行为——一侧无界增长，另一侧衰减的函数。通过将一个可学习的深度多项式与一个单侧权重相乘，我们捕获了局部非光滑性和全局增长。我们数值表明，即使所有方法使用相同数量的参数，该框架也优于泰勒、切比雪夫和标准深度多项式逼近器。为了在实践中优化这些逼近器，我们提出了一种基于图的稳定参数化策略，该策略建立在 \n\\cite{Jar} 的基础上。", "summary": "本文提出并分析了一种新的加权深度多项式逼近器，旨在高效逼近具有不对称行为（如一侧增长一侧衰减）的非光滑函数。通过结合深度多项式和单侧权重，该方法能够同时处理局部非光滑性和全局增长特性。数值实验表明，与传统的泰勒、切比雪夫和标准深度多项式逼近器相比，即使在参数数量相同的情况下，该方法也能取得更优的性能。此外，文章还提出了一种稳定的图基参数化策略以优化实际应用。", "keywords": "深度多项式逼近, 加权逼近, 非光滑函数, 指数收敛, 参数化策略", "comments": "本文的创新之处在于引入了加权深度多项式逼近器来处理非光滑和具有不对称行为的函数，这在传统多项式逼近中是一个挑战。通过结合深度学习的优势和加权方法，该研究为处理特定类型的复杂函数逼近问题提供了新的视角和有效工具。其数值结果表明了该方法的优越性，这对于实际应用具有重要意义。"}}
{"id": "2506.20944", "title": "E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs", "authors": ["Van-Hoang Phan", "Long-Khanh Pham", "Dang Vu", "Anh-Duy Tran", "Minh-Son Dao"], "summary": "The rapid spread of misinformation in mobile and wireless networks presents\ncritical security challenges. This study introduces a training-free,\nretrieval-based multimodal fact verification system that leverages pretrained\nvision-language models and large language models for credibility assessment. By\ndynamically retrieving and cross-referencing trusted data sources, our approach\nmitigates vulnerabilities of traditional training-based models, such as\nadversarial attacks and data poisoning. Additionally, its lightweight design\nenables seamless edge device integration without extensive on-device\nprocessing. Experiments on two fact-checking benchmarks achieve SOTA results,\nconfirming its effectiveness in misinformation detection and its robustness\nagainst various attack vectors, highlighting its potential to enhance security\nin mobile and wireless communication environments.", "comment": "Accepted to AsiaCCS 2025 @ SCID", "pdf_url": "http://arxiv.org/pdf/2506.20944v1", "categories": ["cs.MM", "cs.CR"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.20944v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "E-FreeM2: 通过多模态大语言模型实现高效免训练多尺度跨模态新闻验证", "tldr": "E-FreeM2是一个免训练、基于检索的多模态事实验证系统，利用预训练视觉-语言模型和大型语言模型，通过动态检索和交叉引用可信数据源来检测虚假信息，并在移动和无线网络中实现SOTA性能。", "motivation": "移动和无线网络中虚假信息的快速传播带来了严峻的安全挑战，传统的基于训练的模型容易受到对抗性攻击和数据投毒。", "method": "本研究提出了一个名为E-FreeM2的免训练、基于检索的多模态事实验证系统。该系统利用预训练的视觉-语言模型和大型语言模型进行可信度评估，通过动态检索和交叉引用可信数据源来规避传统训练模型的脆弱性。其轻量级设计使其能够无缝集成到边缘设备中，无需大量的设备内处理。", "result": "该系统在两个事实核查基准测试中取得了SOTA（State-of-the-Art）结果，证实了其在虚假信息检测方面的有效性以及对各种攻击向量的鲁棒性。", "conclusion": "E-FreeM2系统通过其免训练、轻量级和鲁棒的特性，在移动和无线通信环境中具有增强安全性的巨大潜力。", "translation": "移动和无线网络中虚假信息的快速传播带来了严峻的安全挑战。本研究引入了一种免训练、基于检索的多模态事实验证系统，该系统利用预训练的视觉-语言模型和大型语言模型进行可信度评估。通过动态检索和交叉引用可信数据源，我们的方法减轻了传统基于训练模型的脆弱性，例如对抗性攻击和数据投毒。此外，其轻量级设计使得无需大量的设备内处理即可实现边缘设备的无缝集成。在两个事实核查基准测试上的实验取得了SOTA结果，证实了其在虚假信息检测方面的有效性以及对各种攻击向量的鲁棒性，突出了其在移动和无线通信环境中增强安全的潜力。", "summary": "E-FreeM2是一个创新的免训练、基于检索的多模态事实验证系统，旨在应对移动和无线网络中的虚假信息问题。它利用预训练的视觉-语言模型和大型语言模型，通过动态检索和交叉引用可信数据源来评估信息可信度，从而规避了传统训练模型易受攻击的缺点。该系统设计轻量，易于集成到边缘设备。实验证明，E-FreeM2在事实核查基准测试中达到了最先进的性能，并展现出对各种攻击的强大鲁棒性，有望显著提升移动和无线通信环境的安全性。", "keywords": "多模态事实验证, 免训练, 检索式, 大型语言模型, 虚假信息检测", "comments": "这篇论文的创新点在于提出了一个“免训练”的多模态事实验证系统，这显著降低了模型对大量标注数据的依赖，并增强了对对抗性攻击和数据投毒的鲁棒性。其轻量级设计使其适用于资源受限的边缘设备，具有很高的实用价值。在多模态大模型快速发展的背景下，这种将预训练MLLMs与检索机制结合的方式，为虚假信息检测提供了新的思路。"}}
{"id": "2506.21182", "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21182v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21182v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "维护MTEB：迈向嵌入基准的长期可用性和可复现性", "tldr": "本文关注MTEB基准的工程维护，以确保其长期可复现性和可用性。", "motivation": "MTEB已成为文本嵌入模型的标准评估平台，但需要解决工程方面的问题，以确保其持续的可复现性和可扩展性。", "method": "维护健壮的持续集成管道来验证数据集完整性、自动化测试执行和评估基准结果的泛化能力；详细说明增强可复现性和可用性的设计选择；处理社区贡献和扩展基准的策略。", "result": "这些工程实践有助于MTEB变得更全面，同时保持质量和相关性。", "conclusion": "他们的经验为面临类似挑战的基准维护者提供了宝贵的见解，以确保机器学习评估框架的可复现性和可用性。", "translation": "大规模文本嵌入基准（MTEB）已成为文本嵌入模型的标准评估平台。虽然之前的工作已经建立了核心基准方法，但本文侧重于确保MTEB持续可复现性和可扩展性的工程方面。我们介绍了我们维护健壮的持续集成管道的方法，这些管道验证数据集完整性、自动化测试执行并评估基准结果的泛化能力。我们详细介绍了共同增强可复现性和可用性的设计选择。此外，我们讨论了处理社区贡献以及用新任务和数据集扩展基准的策略。这些工程实践在使MTEB变得更全面、同时保持质量并最终保持该领域的相关性方面发挥了重要作用。我们的经验为面临类似挑战的基准维护者提供了宝贵的见解，以确保机器学习评估框架中的可复现性和可用性。MTEB存储库可在：https://github.com/embeddings-benchmark/mteb 访问。", "summary": "本文探讨了大规模文本嵌入基准（MTEB）的工程维护，旨在确保其长期可用性和可复现性。作者详细介绍了建立健壮的持续集成管道、优化设计选择以及管理社区贡献的策略，这些措施共同提升了基准的质量、广度和相关性。研究经验为其他机器学习评估框架的维护者提供了宝贵参考。", "keywords": "MTEB, 文本嵌入, 基准测试, 可复现性, 工程维护", "comments": "本文的创新之处在于将重点从基准方法论转移到其工程维护和可持续性上，这对于任何大型、持续发展的评估平台都至关重要。其重要性体现在确保MTEB作为行业标准的长期可靠性和可用性，并为其他基准项目提供了实践经验和指导。"}}
{"id": "2506.21178", "title": "UAIbot: Beginner-friendly web-based simulator for interactive robotics learning and research", "authors": ["Johnata Brayan", "Armando Alves Neto", "Pavel Petrovič", "Gustavo M Freitas", "Vinicius Mariano Gonçalves"], "summary": "This paper presents UAIbot, a free and open-source web-based robotics\nsimulator designed to address the educational and research challenges\nconventional simulation platforms generally face. The Python and JavaScript\ninterfaces of UAIbot enable accessible hands-on learning experiences without\ncumbersome installations. By allowing users to explore fundamental mathematical\nand physical principles interactively, ranging from manipulator kinematics to\npedestrian flow dynamics, UAIbot provides an effective tool for deepening\nstudent understanding, facilitating rapid experimentation, and enhancing\nresearch dissemination.", "comment": "12 pages, 8 figures, submitted to Springer proceedings", "pdf_url": "http://arxiv.org/pdf/2506.21178v1", "categories": ["cs.RO", "68T40", "I.2.9; I.6.3"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21178v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "UAIbot: 适用于交互式机器人学习和研究的初学者友好型网络模拟器", "tldr": "UAIbot是一个免费开源的网络机器人模拟器，旨在简化机器人学习和研究，通过Python和JavaScript接口提供无需安装的交互式实践体验。", "motivation": "传统的机器人模拟平台面临教育和研究挑战，安装繁琐且不易上手，限制了学生和研究人员的实践学习和快速实验。", "method": "UAIbot通过提供基于Python和JavaScript的Web接口，实现无需复杂安装即可访问的实践学习体验。它允许用户交互式地探索从机械臂运动学到行人流动力学等基础数学和物理原理。", "result": "UAIbot为学生提供了深化理解的有效工具，促进了快速实验，并增强了研究成果的传播。它使交互式学习成为可能，并帮助用户探索基础原理。", "conclusion": "UAIbot通过其免费、开源和易于访问的Web平台，成功解决了传统机器人模拟器的痛点，为机器人学习和研究提供了一个有效且用户友好的工具。", "translation": "本文介绍了UAIbot，一个免费开源的网络机器人模拟器，旨在解决传统模拟平台普遍面临的教育和研究挑战。UAIbot的Python和JavaScript接口使得无需繁琐安装即可获得便捷的实践学习体验。通过允许用户交互式探索从机械臂运动学到行人流动力学等基础数学和物理原理，UAIbot为深化学生理解、促进快速实验和增强研究传播提供了一个有效的工具。", "summary": "本文介绍了UAIbot，一个免费开源的Web机器人模拟器。它通过提供Python和JavaScript接口，解决了传统模拟器安装复杂的问题，使初学者能够轻松进行交互式机器人学习和研究。UAIbot旨在通过实践操作，帮助用户深入理解机器人学中的基本原理，促进快速实验，并提升研究成果的普及性。", "keywords": "机器人模拟器, 网络平台, 交互式学习, 开源, 机器人教育", "comments": "UAIbot的创新之处在于其“初学者友好”和“网络化”的特性，大大降低了机器人模拟学习和研究的门槛。其免费开源的模式也促进了知识的普及和社区协作。这种基于Web的无需安装的解决方案对于教育和快速原型开发具有重要意义。"}}
{"id": "2506.20686", "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models", "authors": ["Hoa La", "Ahan Gupta", "Alex Morehead", "Jianlin Cheng", "Minjia Zhang"], "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.", "comment": "13 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20686v1", "categories": ["q-bio.BM", "cs.DC", "cs.LG", "cs.PF"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.20686v1", "date": "2025-06-24", "updated": "2025-06-24", "AI": {"title_translation": "MegaFold：加速蛋白质结构预测模型的系统级优化", "tldr": "MegaFold通过系统级优化，显著加速并扩展了AlphaFold3等蛋白质结构预测模型的训练，解决了其高昂的系统成本和可扩展性问题。", "motivation": "AlphaFold3等蛋白质结构预测模型虽然在生物分子建模方面取得了突破，但其计算和内存密集型操作、2D注意力机制以及检索增强数据管道带来了高昂的系统成本，严重阻碍了训练的可扩展性。", "method": "MegaFold通过以下系统级优化来解决关键瓶颈：1. 采用提前缓存（ahead-of-time caching）消除检索增强数据管道中的GPU空闲时间。2. 使用基于Triton的核（Triton-based kernels）实现异构设备上内存高效的EvoAttention。3. 对AlphaFold3中常见和关键的小型操作进行深度融合（deep fusion）。", "result": "在NVIDIA H200和AMD MI250 GPU上的评估显示，MegaFold将AlphaFold3训练的峰值内存使用量减少了高达1.23倍。它将每次迭代的训练时间分别提高了高达1.73倍和1.62倍。更重要的是，与PyTorch基线相比，MegaFold能够在不耗尽内存的情况下训练1.35倍更长的序列长度。", "conclusion": "MegaFold显著提高了现代蛋白质折叠模型（如AlphaFold3）的可扩展性。", "translation": "蛋白质结构预测模型，如AlphaFold3 (AF3)，通过将科学知情的架构变化融入Transformer架构，推动了生物分子建模的前沿。然而，这些进步带来了高昂的系统成本，包括：计算和内存密集型操作、2D注意力机制以及检索增强数据管道，这些共同阻碍了AF3训练的可扩展性。在这项工作中，我们提出了MegaFold，一个跨平台系统，用于加速AF3训练。MegaFold通过提前缓存（ahead-of-time caching）消除检索增强数据管道中的GPU空闲时间，基于Triton的核（Triton-based kernels）实现异构设备上内存高效的EvoAttention，以及对AF3中常见和关键的小型操作进行深度融合，解决了关键瓶颈。在NVIDIA H200和AMD MI250 GPU上的评估表明，MegaFold将AF3训练的峰值内存使用量减少了高达1.23倍，并将每次迭代的训练时间分别提高了高达1.73倍和1.62倍。更重要的是，与PyTorch基线相比，MegaFold能够在不耗尽内存的情况下训练1.35倍更长的序列长度，显著提高了现代蛋白质折叠模型的可扩展性。我们的代码已在https://github.com/Supercomputing-System-AI-Lab/MegaFold/ 开源。", "summary": "MegaFold是一个为加速AlphaFold3等蛋白质结构预测模型训练而设计的跨平台系统。它通过引入多项系统级优化来解决现有模型训练中高昂的计算和内存成本问题，包括提前缓存以减少GPU空闲时间、基于Triton的内存高效EvoAttention核以及对关键操作的深度融合。实验证明，MegaFold显著降低了内存消耗，提升了训练速度，并支持更长序列的训练，从而极大地增强了现代蛋白质折叠模型的可扩展性。", "keywords": "蛋白质结构预测, AlphaFold3, 系统优化, 训练加速, 内存效率", "comments": "MegaFold的创新之处在于其针对蛋白质结构预测模型（尤其是AlphaFold3）训练中的特定系统瓶颈，提出了全面且高效的优化方案。通过在数据管道、内存管理和操作融合方面的改进，它直接解决了这些模型高昂的训练成本和可扩展性挑战。这对于推动生物分子建模的进一步研究和应用至关重要，因为它使得训练更复杂、更精确的模型变得更加可行。其跨平台支持也增加了其实际应用价值。"}}
{"id": "2506.20810", "title": "FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs", "authors": ["Shashwat Khandelwal", "Jakoba Petri-Koenig", "Thomas B. Preußer", "Michaela Blott", "Shreejith Shanker"], "summary": "Recurrent neural networks (RNNs), particularly LSTMs, are effective for\ntime-series tasks like sentiment analysis and short-term stock prediction.\nHowever, their computational complexity poses challenges for real-time\ndeployment in resource constrained environments. While FPGAs offer a promising\nplatform for energy-efficient AI acceleration, existing tools mainly target\nfeed-forward networks, and LSTM acceleration typically requires full custom\nimplementation. In this paper, we address this gap by leveraging the\nopen-source and extensible FINN framework to enable the generalized deployment\nof LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open\nNeural Network Exchange (ONNX) specification to model the recurrent nature of\nLSTM computations, enabling support for mixed quantisation within them and\nfunctional verification of LSTM-based models. Furthermore, we introduce custom\ntransformations within the FINN compiler to map the quantised ONNX computation\ngraph to hardware blocks from the HLS kernel library of the FINN compiler and\nVitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM\nmodel for a mid-price stock prediction task using the widely used dataset and\ngenerating a corresponding hardware IP of the model using our flow, targeting\nthe XCZU7EV device. We show that the generated quantised ConvLSTM accelerator\nthrough our flow achieves a balance between performance (latency) and resource\nconsumption, while matching (or bettering) inference accuracy of\nstate-of-the-art models with reduced precision. We believe that the\ngeneralisable nature of the proposed flow will pave the way for\nresource-efficient RNN accelerator designs on FPGAs.", "comment": "9 pages, 6 figures, 5 tables, Accepted for publication in IEEE\n  FPL-2025 (https://2025.fpl.org/)", "pdf_url": "http://arxiv.org/pdf/2506.20810v1", "categories": ["cs.LG", "cs.AI", "cs.AR", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20810v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "FINN-GL：面向FPGA加速LSTM的广义混合精度扩展", "tldr": "本文通过扩展FINN框架，为FPGA上的LSTM部署提供了广义的混合精度支持，实现了性能、资源和精度之间的良好平衡。", "motivation": "循环神经网络（RNNs），特别是LSTM，在时间序列任务中表现出色，但其计算复杂性限制了在资源受限环境中进行实时部署。现有FPGA加速工具主要针对前馈网络，LSTM加速通常需要完全定制实现，因此存在一个空白。", "method": "作者利用开源且可扩展的FINN框架，通过ONNX的Scan操作符来建模LSTM的循环特性，并支持混合量化。此外，引入了FINN编译器中的自定义转换，将量化的ONNX计算图映射到FINN编译器HLS内核库和Vitis HLS的硬件块。", "result": "通过在股票预测任务上验证量化ConvLSTM模型，生成的硬件IP在性能（延迟）和资源消耗之间取得了平衡，同时在降低精度的前提下，其推理精度与现有最先进模型相当或更优。", "conclusion": "所提出的通用化流程将为FPGA上资源高效的RNN加速器设计铺平道路。", "translation": "循环神经网络（RNNs），特别是长短期记忆网络（LSTMs），对于情感分析和短期股票预测等时间序列任务非常有效。然而，其计算复杂性对资源受限环境中的实时部署构成了挑战。虽然FPGA为能源高效的AI加速提供了一个有前景的平台，但现有工具主要针对前馈网络，LSTM加速通常需要完全定制实现。本文通过利用开源且可扩展的FINN框架，弥补了这一空白，实现了LSTM在FPGA上的通用部署。具体而言，我们利用开放神经网络交换（ONNX）规范中的Scan操作符来建模LSTM计算的循环特性，从而支持其中的混合量化以及基于LSTM模型的功能验证。此外，我们在FINN编译器中引入了自定义转换，将量化的ONNX计算图映射到FINN编译器HLS内核库和Vitis HLS的硬件块。我们通过训练一个量化的ConvLSTM模型进行股票中间价预测任务（使用广泛使用的数据集），并使用我们的流程生成该模型的相应硬件IP（目标设备为XCZU7EV），从而验证了所提出的工具流程。结果表明，通过我们的流程生成的量化ConvLSTM加速器在性能（延迟）和资源消耗之间取得了平衡，同时在降低精度的前提下，其推理精度与现有最先进模型相当（或更优）。我们相信，所提出流程的通用性将为FPGA上资源高效的RNN加速器设计铺平道路。", "summary": "本文提出了一种基于FINN框架的通用混合精度扩展（FINN-GL），旨在解决LSTM在FPGA上部署的计算复杂性和现有工具不足的问题。通过利用ONNX的Scan操作符和FINN编译器的自定义转换，该方法支持LSTM的混合量化，并能将量化图映射到FPGA硬件。实验结果表明，该加速器在性能、资源利用和推理精度之间实现了高效平衡，并有望推动FPGA上资源高效RNN加速器的发展。", "keywords": "FPGA, LSTM, 混合精度, FINN, RNN加速", "comments": "本文的创新点在于将FINN框架扩展到支持循环神经网络（RNNs），特别是LSTM的混合精度部署，这填补了现有FPGA加速工具主要针对前馈网络的空白。其提出的通用化流程，结合ONNX的Scan操作符和FINN编译器的定制转换，为在资源受限的FPGA上实现高效的LSTM加速提供了可行方案。这项工作对于推动边缘AI和嵌入式系统中时间序列任务的实时处理具有重要意义。"}}
{"id": "2506.21265", "title": "Active Disturbance Rejection Control for Trajectory Tracking of a Seagoing USV: Design, Simulation, and Field Experiments", "authors": ["Jelmer van der Saag", "Elia Trevisan", "Wouter Falkena", "Javier Alonso-Mora"], "summary": "Unmanned Surface Vessels (USVs) face significant control challenges due to\nuncertain environmental disturbances like waves and currents. This paper\nproposes a trajectory tracking controller based on Active Disturbance Rejection\nControl (ADRC) implemented on the DUS V2500. A custom simulation incorporating\nrealistic waves and current disturbances is developed to validate the\ncontroller's performance, supported by further validation through field tests\nin the harbour of Scheveningen, the Netherlands, and at sea. Simulation results\ndemonstrate that ADRC significantly reduces cross-track error across all tested\nconditions compared to a baseline PID controller but increases control effort\nand energy consumption. Field trials confirm these findings while revealing a\nfurther increase in energy consumption during sea trials compared to the\nbaseline.", "comment": "Accepted for presentation at IROS 2025. Submitted version", "pdf_url": "http://arxiv.org/pdf/2506.21265v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21265v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "远洋USV轨迹跟踪的主动抗扰控制：设计、仿真与实地实验", "tldr": "本文提出了一种基于主动抗扰控制（ADRC）的无人水面艇（USV）轨迹跟踪控制器，并通过定制仿真和实地实验验证了其在减少横向跟踪误差方面的有效性，但代价是更高的能量消耗。", "motivation": "无人水面艇（USV）在波浪和水流等不确定环境扰动下，其控制面临巨大挑战，因此需要一种能有效应对这些扰动的轨迹跟踪控制器。", "method": "本文提出了一种基于主动抗扰控制（ADRC）的轨迹跟踪控制器，并将其应用于DUS V2500无人艇。通过开发包含真实波浪和水流扰动的定制仿真模型来验证控制器性能，并通过在荷兰斯赫弗宁根港和海上进行的实地测试进一步验证。", "result": "仿真结果表明，在所有测试条件下，ADRC相比基线PID控制器显著降低了横向跟踪误差，但增加了控制工作量和能量消耗。实地试验证实了这些发现，并揭示了海上试验期间能量消耗相比基线进一步增加。", "conclusion": "ADRC控制器能有效减少无人水面艇在环境扰动下的轨迹跟踪误差，但其代价是更高的能量消耗。", "translation": "无人水面艇（USV）由于波浪和水流等不确定环境扰动而面临严峻的控制挑战。本文提出了一种基于主动抗扰控制（ADRC）的轨迹跟踪控制器，并在DUS V2500上实现。开发了一个包含真实波浪和水流扰动的定制仿真系统来验证控制器的性能，并通过在荷兰斯赫弗宁根港和海上进行的实地测试进一步验证。仿真结果表明，与基线PID控制器相比，ADRC在所有测试条件下显著减少了横向跟踪误差，但增加了控制工作量和能量消耗。实地试验证实了这些发现，同时揭示了海上试验期间能量消耗相比基线进一步增加。", "summary": "本文针对无人水面艇在不确定环境扰动下的轨迹跟踪难题，提出了一种基于主动抗扰控制（ADRC）的解决方案。通过定制仿真和实地实验（包括港口和海上测试），研究验证了ADRC控制器在显著降低横向跟踪误差方面的有效性。然而，研究也指出，ADRC的应用会导致控制工作量和能量消耗的增加，尤其是在海上环境中。", "keywords": "主动抗扰控制, 轨迹跟踪, 无人水面艇, 环境扰动, 实地实验", "comments": "这篇论文通过结合仿真和实地实验，验证了ADRC在复杂海洋环境下对USV轨迹跟踪的有效性，其创新性在于将ADRC应用于实际USV并量化了其性能提升及代价。这项工作对于未来USV在复杂环境下的自主导航和控制具有重要意义，同时也指出了高能耗是ADRC在实际应用中需要解决的局限性。"}}
{"id": "2506.20911", "title": "FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing", "authors": ["Advait Gupta", "Rishie Raj", "Dang Nguyen", "Tianyi Zhou"], "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A$^*$\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A$^*$\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA$^*$'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A$^*$\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA$^*$ is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20911v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20911v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FaSTA*: 结合子程序挖掘的快慢工具路径智能体，用于高效多轮图像编辑", "tldr": "FaSTA*是一个高效的神经符号智能体，通过结合LLM的快速规划和A*的慢速搜索，并挖掘可重用子程序，实现高效的多轮图像编辑，同时显著提高计算效率。", "motivation": "解决复杂的多轮图像编辑任务，并开发一个成本高效的神经符号智能体。", "method": "本文提出FaSTA*，一个成本高效的神经符号智能体，用于处理多轮图像编辑任务。它结合大型语言模型（LLMs）进行快速的高级子任务规划，以及每子任务的慢速、精确的工具使用和局部A*搜索，以找到成本高效的工具路径。为了节省A*搜索在相似子任务上的成本，FaSTA*通过LLMs对先前成功的工具路径进行归纳推理，持续提取和优化常用子程序，并将其作为新工具在未来的任务中重用，实现自适应的快慢规划。", "result": "FaSTA*在计算效率上显著优于现有图像编辑方法，同时在成功率方面与最先进的基线保持竞争力。", "conclusion": "该研究成功开发并验证了FaSTA*，一个在多轮图像编辑任务中既高效又具有竞争力的神经符号智能体，其通过结合LLM规划和A*搜索，并引入子程序挖掘和重用机制，显著提升了性能。", "translation": "我们开发了一种成本高效的神经符号智能体，以解决“检测图像中的长凳并将其重新着色为粉红色。此外，删除猫以获得更清晰的视图并将墙壁重新着色为黄色”等具有挑战性的多轮图像编辑任务。它结合了大型语言模型（LLMs）的快速、高级子任务规划与每个子任务的慢速、精确的工具使用和局部A*搜索，以找到一个成本高效的工具路径——一系列对AI工具的调用。为了节省A*在相似子任务上的成本，我们通过LLMs对先前成功的工具路径进行归纳推理，持续提取/优化常用子程序，并将其作为新工具在未来的任务中重用，实现自适应的快慢规划。在这种规划中，首先探索更高级别的子程序，只有当它们失败时，才会激活低级别的A*搜索。可重用的符号子程序显著节省了应用于相似图像的相同类型子任务的探索成本，从而产生了一个类人化的快慢工具路径智能体“FaSTA*”：首先由LLMs尝试快速子任务规划，然后是基于规则的每子任务子程序选择，预计这可以覆盖大多数任务，而慢速A*搜索仅针对新颖和具有挑战性的子任务触发。通过与最近的图像编辑方法进行比较，我们证明FaSTA*在计算效率上显著更高，同时在成功率方面与最先进的基线保持竞争力。", "summary": "本文提出了一种名为FaSTA*的成本高效神经符号智能体，用于处理复杂的多轮图像编辑任务。FaSTA*结合了大型语言模型进行快速高层子任务规划和A*搜索进行精确的底层工具使用。其核心创新在于通过LLM对成功工具路径进行子程序挖掘，将常用操作封装并重用，从而显著降低了重复任务的探索成本。实验结果表明，FaSTA*在计算效率上远超现有方法，同时在成功率上与最先进的基线持平。", "keywords": "多轮图像编辑, 神经符号智能体, 大型语言模型, A*搜索, 子程序挖掘", "comments": "FaSTA*的创新性在于其结合了LLM的高级规划能力和A*的精确搜索，并通过子程序挖掘和重用机制实现了显著的效率提升。这种快慢结合的神经符号方法，模拟了人类解决问题的过程，即先尝试已知或高层策略，失败后再进行细致探索，这在复杂多步骤任务中具有很高的应用价值和潜力。"}}
{"id": "2506.21536", "title": "PsyLite Technical Report", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21536v1", "categories": ["cs.AI", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21536v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PsyLite 技术报告", "tldr": "PsyLite是一个基于InternLM2.5-7B-chat的轻量级AI心理咨询大语言模型，通过两阶段训练和创新的条件RAG，显著提升了心理咨询能力、对话安全性和部署效率。", "motivation": "现有AI驱动心理咨询模型在对话安全、详细场景处理和轻量化部署方面存在不足。", "method": "本研究提出了PsyLite，一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型代理。采用两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化）以增强模型深度推理、心理咨询和安全对话能力。部署时使用Ollama和Open WebUI创建自定义工作流，并设计了创新的条件RAG，以在心理咨询中适时引入相声幽默元素并拒绝危险请求。模型还利用量化技术（GGUF q4_k_m）实现低硬件部署。", "result": "PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中均优于基线模型。尤其在CPsyCounE得分提升47.6%，对话安全（\\safe{}得分提升2.4%）。模型仅需5GB内存即可运行。", "conclusion": "PsyLite为资源受限环境下的心理咨询应用提供了可行的解决方案，并在专业性、安全性和轻量化部署方面表现出色。", "translation": "随着数字技术的飞速发展，AI驱动的心理咨询逐渐成为心理健康领域的重要研究方向。然而，现有模型在对话安全、详细场景处理和轻量化部署方面仍存在不足。为解决这些问题，本研究提出了PsyLite，一个基于InternLM2.5-7B-chat基础模型开发的轻量级心理咨询大语言模型代理。通过两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），PsyLite增强了模型的深度推理能力、心理咨询能力和安全对话能力。部署使用Ollama和Open WebUI后，通过Pipelines创建了自定义工作流。设计了一种创新的条件RAG，在心理咨询过程中适时引入相声幽默元素以增强用户体验，并拒绝危险请求以加强对话安全。评估结果显示，PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中均优于基线模型，尤其在心理咨询专业性（CPsyCounE得分提升47.6%）和对话安全（\\safe{}得分提升2.4%）方面表现突出。此外，该模型利用量化技术（GGUF q4_k_m）实现了低硬件部署（5GB内存即可运行），为资源受限环境下的心理咨询应用提供了可行的解决方案。", "summary": "PsyLite是一个基于InternLM2.5-7B-chat开发的轻量级AI心理咨询大语言模型。它通过两阶段训练（混合蒸馏和ORPO优化）提升了深度推理、心理咨询和安全对话能力。其创新之处在于引入了条件RAG以在咨询中加入幽默元素并增强安全性。评估显示，PsyLite在专业性、安全性和通用评估方面均优于基线模型，且仅需5GB内存即可部署，为资源受限环境下的心理咨询提供了高效可行的方案。", "keywords": "心理咨询, 大语言模型, 轻量化部署, 对话安全, 条件RAG", "comments": "PsyLite的创新之处在于其结合了两阶段训练策略和条件RAG，特别是在心理咨询中引入相声幽默元素以提升用户体验，同时兼顾对话安全。其轻量化部署（仅需5GB内存）是其重要优势，使得AI心理咨询能在资源受限环境中广泛应用。这对于推动AI心理健康服务普及具有重要意义。"}}
{"id": "2506.21349", "title": "Generalizable Neural Electromagnetic Inverse Scattering", "authors": ["Yizhe Cheng", "Chunxun Tian", "Haoru Wang", "Wentao Zhu", "Xiaoxuan Ma", "Yizhou Wang"], "summary": "Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in\napplications such as medical imaging, where the goal is to reconstruct the\nrelative permittivity from scattered electromagnetic field. This inverse\nprocess is inherently ill-posed and highly nonlinear, making it particularly\nchallenging. A recent machine learning-based approach, Img-Interiors, shows\npromising results by leveraging continuous implicit functions. However, it\nrequires case-specific optimization, lacks generalization to unseen data, and\nfails under sparse transmitter setups (e.g., with only one transmitter). To\naddress these limitations, we revisit EISP from a physics-informed perspective,\nreformulating it as a two stage inverse transmission-scattering process. This\nformulation reveals the induced current as a generalizable intermediate\nrepresentation, effectively decoupling the nonlinear scattering process from\nthe ill-posed inverse problem. Built on this insight, we propose the first\ngeneralizable physics-driven framework for EISP, comprising a current estimator\nand a permittivity solver, working in an end-to-end manner. The current\nestimator explicitly learns the induced current as a physical bridge between\nthe incident and scattered field, while the permittivity solver computes the\nrelative permittivity directly from the estimated induced current. This design\nenables data-driven training and generalizable feed-forward prediction of\nrelative permittivity on unseen data while maintaining strong robustness to\ntransmitter sparsity. Extensive experiments show that our method outperforms\nstate-of-the-art approaches in reconstruction accuracy, generalization, and\nrobustness. This work offers a fundamentally new perspective on electromagnetic\ninverse scattering and represents a major step toward cost-effective practical\nsolutions for electromagnetic imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21349v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21349v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可泛化神经电磁逆散射", "tldr": "本文提出一种可泛化的物理驱动神经框架，用于解决电磁逆散射问题（EISP）。该框架利用感应电流作为中间表示，有效克服了现有方法在泛化能力和稀疏发射器设置下的局限性，并在重建精度、泛化能力和鲁棒性方面优于现有技术。", "motivation": "解决电磁逆散射问题（EISP）固有的病态性和高度非线性挑战，特别是针对现有机器学习方法（如Img-Interiors）存在的需要案例特定优化、缺乏对未知数据的泛化能力以及在稀疏发射器设置（例如只有一个发射器）下失效的局限性。", "method": "本文从物理信息角度重新审视EISP，将其重新表述为两阶段逆透射-散射过程，并发现感应电流是可泛化的中间表示。在此基础上，提出首个可泛化的物理驱动EISP框架，包含一个电流估计器和一个介电常数求解器。电流估计器学习感应电流作为入射场和散射场之间的物理桥梁，介电常数求解器直接从估计的感应电流计算相对介电常数。", "result": "实验表明，本文方法在重建精度、泛化能力和鲁棒性方面均优于现有最先进的方法，并且对发射器稀疏性保持强大的鲁棒性。", "conclusion": "该工作为电磁逆散射提供了一个全新的视角，代表着向实现成本效益高的电磁成像实用解决方案迈出了重要一步。", "translation": "解决电磁逆散射问题（EISP）在医学成像等应用中至关重要，其目标是从散射电磁场中重建相对介电常数。这个逆过程本质上是病态且高度非线性的，这使得它特别具有挑战性。最近一种基于机器学习的方法Img-Interiors通过利用连续隐函数显示出有希望的结果。然而，它需要针对特定案例进行优化，缺乏对未知数据的泛化能力，并且在稀疏发射器设置下（例如只有一个发射器）会失效。为了解决这些局限性，我们从物理信息的角度重新审视EISP，将其重新表述为两阶段逆透射-散射过程。这种表述揭示了感应电流作为一种可泛化的中间表示，有效地将非线性散射过程与病态逆问题解耦。基于这一见解，我们提出了第一个可泛化的物理驱动EISP框架，它包括一个电流估计器和一个介电常数求解器，以端到端的方式工作。电流估计器明确地学习感应电流作为入射场和散射场之间的物理桥梁，而介电常数求解器直接从估计的感应电流计算相对介电常数。这种设计使得对未知数据进行数据驱动训练和可泛化的前向预测相对介电常数成为可能，同时对发射器稀疏性保持强大的鲁棒性。大量的实验表明，我们的方法在重建精度、泛化能力和鲁棒性方面均优于现有最先进的方法。这项工作为电磁逆散射提供了一个全新的视角，代表着向实现经济高效的电磁成像实用解决方案迈出了重要一步。", "summary": "本文提出一种可泛化的神经电磁逆散射框架，旨在克服现有机器学习方法在电磁逆散射问题（EISP）中泛化能力差和对稀疏发射器设置不鲁棒的局限性。通过将EISP重新构想为两阶段过程，并利用感应电流作为可泛化的中间表示，该框架通过电流估计器和介电常数求解器实现了端到端的介电常数预测。实验证明，该方法在重建精度、泛化能力和鲁棒性方面均优于现有技术。", "keywords": "电磁逆散射, 泛化能力, 物理驱动, 感应电流, 稀疏发射器", "comments": "该论文通过引入物理驱动的感应电流作为中间表示，巧妙地解耦了电磁逆散射问题中的非线性和病态性，从而显著提升了模型的泛化能力和对稀疏数据设置的鲁棒性。这种结合物理洞察与深度学习的方法是解决复杂逆问题的创新范式。"}}
{"id": "2506.21288", "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21288v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21288v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "小型编码器在检测“接地性”方面可与大型解码器媲美", "tldr": "研究表明，轻量级编码器模型在检测大型语言模型（LLMs）回答的“接地性”（即是否基于给定上下文）方面，能达到与最先进LLMs相当的准确度，同时大幅降低推理时间和资源消耗。", "motivation": "大型语言模型在处理外部上下文时表现出色，但当上下文信息不足时，它们容易产生不基于事实的推测或使用内部知识，导致“幻觉”问题。为了确保事实一致性和可信度，“接地性”（即回答严格基于上下文）至关重要。因此，在LLM生成答案之前检测查询的“接地性”可以显著减少推理时间和资源消耗。", "method": "本研究通过在精选数据集上微调轻量级、任务特定的编码器模型（如RoBERTa和NomicBERT），来检测给定查询是否基于提供的文档。", "result": "结果显示，与Llama3 8B和GPT4o等最先进的LLM相比，这些轻量级编码器模型在接地性检测方面达到了可媲美的准确度，同时将推理延迟降低了几个数量级。", "conclusion": "轻量级、任务特定的编码器模型可以作为高效且准确的替代方案，用于在昂贵的LLM答案生成之前检测查询的“接地性”。", "translation": "将大型语言模型（LLM）与外部上下文结合显著提升了它们在自然语言处理（NLP）任务中的性能。然而，当提供的上下文信息不足时，LLM难以可靠地回答查询，常常诉诸于不基于事实的推测或内部知识。“接地性”（即生成严格由上下文支持的回答）对于确保事实一致性和可信度至关重要。本研究侧重于在LLM进行昂贵的答案生成之前，检测给定查询是否基于提供的文档。这种检测机制可以显著减少推理时间和资源消耗。我们展示了轻量级、任务特定的编码器模型，如RoBERTa和NomicBERT，通过在精选数据集上进行微调，可以在接地性检测方面达到与Llama3 8B和GPT4o等最先进的LLM相当的准确度，同时将推理延迟降低了几个数量级。代码可在：https://github.com/chandarlab/Hallucinate-less 获取。", "summary": "本研究旨在解决大型语言模型（LLMs）在上下文信息不足时容易产生“幻觉”的问题，提出在LLM生成回答前检测查询的“接地性”。研究发现，通过在精选数据集上微调轻量级编码器模型（如RoBERTa和NomicBERT），可以在接地性检测任务上达到与先进LLMs（如Llama3 8B和GPT4o）相当的准确度，同时大幅度降低推理时间和资源消耗，证明小型编码器是更高效的选择。", "keywords": "接地性检测, 大型语言模型, 轻量级编码器, RoBERTa, NomicBERT, 幻觉", "comments": "这篇论文的创新点在于证明了在特定任务（接地性检测）上，轻量级编码器模型可以与大型、资源密集型的LLMs相媲美，甚至在效率上远超LLMs。这对于实际部署和资源受限的应用具有重要意义，提供了一种成本效益高的解决方案来提高LLM的可靠性和一致性。它挑战了“越大越好”的普遍观念，并为LLM应用中的“幻觉”问题提供了一个实用的预处理方案。"}}
{"id": "2506.21314", "title": "A Sampling-Based Adaptive Rank Approach to the Wigner-Poisson System", "authors": ["Andrew Christlieb", "Sining Gong", "Jing-Mei Qiu", "Nanyi Zheng"], "summary": "We develop a mass-conserving, adaptive-rank solver for the 1D1V\nWigner-Poisson system. Our work is motivated by applications to the study of\nthe stopping power of $\\alpha$ particles at the National Ignition Facility\n(NIF). In this regime, electrons are in a warm dense state, requiring more than\na standard kinetic model. They are hot enough to neglect Pauli exclusion, yet\nquantum enough to require accounting for uncertainty. The Wigner-Poisson system\ncaptures these effects but presents challenges due to its nonlocal nature.\nBased on a second-order Strang splitting method, we first design a full-rank\nsolver with a structure-preserving Fourier update that ensures the intermediate\nsolutions remain real-valued (up to machine precision), improving upon previous\nmethods. Simulations demonstrate that the solutions exhibit a low rank\nstructure for moderate to high dimensionless Planck constants ($H \\ge 0.1$).\nThis observed low rank structure motivates the development of an adaptive-rank\nsolver, built on a Semi-Lagrangian adaptive-rank (SLAR) scheme for advection\nand an adaptive-rank, structure-preserving Fourier update for the Wigner\nintegral terms, with a rigorous proof of structure-preserving property\nprovided. Our solver achieves $O(N)$ complexity in both storage and computation\ntime, while preserving mass and maintaining momentum accuracy up to the\ntruncation error. The adaptive rank simulations are visually indistinguishable\nfrom the full-rank simulations in capturing solution structures. These results\nhighlight the potential of adaptive rank methods for high-dimensional\nWigner-Poisson simulations, paving the way toward fully kinetic studies of\nstopping power in warm dense plasmas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21314v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21314v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于采样自适应秩方法的Wigner-Poisson系统", "tldr": "本文开发了一种针对一维一速Wigner-Poisson系统的质量守恒、自适应秩求解器，用于研究暖密物质中α粒子的阻止本领。该求解器实现了O(N)的计算复杂度，并能有效捕捉溶液结构，与全秩模拟结果在视觉上无异。", "motivation": "该工作的动机是研究国家点火装置（NIF）中α粒子的阻止本领。在此状态下，电子处于暖密态，需要Wigner-Poisson系统来描述，因为它能捕捉量子不确定性效应。Wigner-Poisson系统的非局部性带来了挑战。此外，模拟中观察到的解的低秩结构也促使开发自适应秩求解器。", "method": "首先，基于二阶Strang分裂方法，设计了一个全秩求解器，该求解器采用结构保持傅里叶更新，确保中间解保持实值。其次，基于对解低秩结构的观察，开发了一个自适应秩求解器。该求解器结合了用于对流的半拉格朗日自适应秩（SLAR）方案和用于Wigner积分项的自适应秩、结构保持傅里叶更新，并提供了严格的结构保持性质证明。该求解器在存储和计算时间上均实现了O(N)的复杂度，同时保持质量守恒和动量精度。", "result": "全秩求解器通过确保实值中间解，改进了现有方法。模拟表明，对于中等到高维无量纲普朗克常数（H ≥ 0.1），解表现出低秩结构。自适应秩模拟在捕获解结构方面与全秩模拟在视觉上无法区分。该求解器在存储和计算时间上均达到了O(N)复杂度，同时保持质量守恒并维持动量精度。", "conclusion": "这些结果突出了自适应秩方法在高维Wigner-Poisson模拟中的潜力，为在暖密等离子体中进行阻止本领的完全动力学研究铺平了道路。", "translation": "我们开发了一个用于一维一速Wigner-Poisson系统的质量守恒、自适应秩求解器。我们的工作动机是应用于研究国家点火装置 (NIF) 中 $\\alpha$ 粒子的阻止本领。在这种情况下，电子处于暖密态，需要超出标准动力学模型的描述。它们足够热，可以忽略泡利不相容原理，但又足够量子化，需要考虑不确定性。Wigner-Poisson系统捕获了这些效应，但由于其非局部性带来了挑战。基于二阶Strang分裂方法，我们首先设计了一个全秩求解器，该求解器具有结构保持傅里叶更新，确保中间解保持实值（达到机器精度），改进了以前的方法。模拟表明，对于中等到高维无量纲普朗克常数 ($H \\ge 0.1$)，解表现出低秩结构。观察到的这种低秩结构促使我们开发了一种自适应秩求解器，该求解器基于用于对流的半拉格朗日自适应秩 (SLAR) 方案和用于Wigner积分项的自适应秩、结构保持傅里叶更新，并提供了结构保持性质的严格证明。我们的求解器在存储和计算时间上都达到了 $O(N)$ 复杂度，同时保持质量守恒并保持动量精度达到截断误差。自适应秩模拟在捕获解结构方面与全秩模拟在视觉上无法区分。这些结果凸显了自适应秩方法在高维Wigner-Poisson模拟中的潜力，为在暖密等离子体中进行阻止本领的完全动力学研究铺平了道路。", "summary": "本文提出了一种用于1D1V Wigner-Poisson系统的质量守恒、自适应秩求解器，旨在研究暖密等离子体中α粒子的阻止本领。研究首先开发了一个基于二阶Strang分裂和结构保持傅里叶更新的全秩求解器。鉴于在特定普朗克常数下观察到的解的低秩结构，作者进一步提出了一个自适应秩求解器，该求解器利用半拉格朗日自适应秩（SLAR）方案和自适应秩、结构保持傅里叶更新。新求解器在存储和计算上均实现了O(N)复杂度，同时保持质量守恒和动量精度，并且其结果与全秩模拟在视觉上保持一致，展示了其在高维量子动力学模拟中的巨大潜力。", "keywords": "Wigner-Poisson系统, 自适应秩, 暖密物质, 阻止本领, 量子动力学模拟", "comments": "该论文在Wigner-Poisson系统求解方面取得了显著进展，尤其适用于暖密物质应用。其创新之处在于开发了一种自适应秩求解器，该求解器利用了观察到的解的低秩结构，从而实现了令人印象深刻的O(N)复杂度。这种高效率与质量守恒和精度相结合，使其成为复杂量子动力学模拟的强大工具，克服了系统非局部性带来的挑战。其在高维模拟方面的潜力是主要亮点。"}}
{"id": "2506.21463", "title": "Aligning Spoken Dialogue Models from User Interactions", "authors": ["Anne Wu", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez"], "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21463v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21463v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从用户交互中对齐语音对话模型", "tldr": "本文提出了一个新颖的偏好对齐框架，通过用户交互改进实时语音对话模型。针对现有文本方法不适用于实时语音的复杂性，作者构建了一个包含15万+偏好对的大规模数据集，并使用离线对齐方法微调了一个全双工自回归语音到语音模型。实验证明该方法能有效提升对话模型的真实性、安全性和上下文对齐性，并强调了动态平衡对自然实时语音对话系统的重要性。", "motivation": "当前偏好学习方法主要集中于文本语言模型，不直接适用于实时语音交互的复杂性，例如更丰富的动态（如打断、插话）以及说话者轮次之间没有明确的分割。因此，需要一种新的框架来改善实时对话中的语音对话模型。", "method": "本文提出了一个新颖的偏好对齐框架。首先，构建了一个包含超过15万个偏好对的大规模数据集，这些数据来源于原始多轮语音对话，并用AI反馈进行标注，以覆盖语言内容和时间上下文的偏好。接着，利用离线对齐方法对一个全双工自回归语音到语音模型进行微调。最后，部署微调后的模型并进行整体的人工评估，以评估其在单轮对话之外的影响。", "result": "广泛的实验表明，对通用对话的反馈可以持续有效地改进语音对话模型，使其产生更真实、更安全、更符合上下文的交互。人工评估也证实了其在多轮对话中的积极影响。", "conclusion": "研究结果揭示了在各种动态之间实现良好平衡的重要性，这对于自然的实时语音对话系统至关重要。", "translation": "我们提出了一个新颖的偏好对齐框架，旨在通过用户交互改进实时对话中的语音对话模型。当前的偏好学习方法主要关注基于文本的语言模型，不直接适用于实时语音交互的复杂性，后者具有更丰富的动态（例如打断、插话）且说话者轮次之间没有明确的分割。我们从原始多轮语音对话中创建了一个包含超过15万个偏好对的大规模数据集，并用AI反馈进行标注，以覆盖语言内容和时间上下文的偏好。我们利用离线对齐方法对一个全双工自回归语音到语音模型进行微调。广泛的实验表明，对通用对话的反馈可以持续有效地改进语音对话模型，使其产生更真实、更安全、更符合上下文的交互。我们部署了微调后的模型并进行了整体的人工评估，以评估其在单轮对话之外的影响。我们的发现揭示了在各种动态之间实现良好平衡的重要性，这对于自然的实时语音对话系统至关重要。", "summary": "本文提出了一种新颖的偏好对齐框架，旨在通过用户交互提升实时语音对话模型的性能。针对现有文本偏好学习方法在处理实时语音复杂性方面的不足，研究者构建了一个包含超过15万个偏好对的大规模数据集，并利用离线对齐技术对一个全双工自回归语音到语音模型进行了微调。实验结果表明，该方法能够有效提升语音对话模型的真实性、安全性及上下文对齐能力。研究强调了在各种对话动态之间实现平衡对于构建自然实时语音对话系统的重要性。", "keywords": "语音对话模型, 偏好对齐, 实时交互, 语音到语音, 用户交互", "comments": "本文的创新点在于提出了一个专门针对实时语音对话的偏好对齐框架，解决了现有文本偏好学习方法无法处理语音交互复杂性的问题。通过构建大规模的语音偏好数据集和微调全双工语音到语音模型，为提升语音对话系统的自然度和实用性提供了有效途径。其对“各种动态之间平衡”的强调，对于未来语音AI系统的发展具有指导意义。"}}
{"id": "2506.21205", "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations", "authors": ["Elia Trevisan", "Khaled A. Mustafa", "Godert Notten", "Xinwei Wang", "Javier Alonso-Mora"], "summary": "Deploying mobile robots safely among humans requires the motion planner to\naccount for the uncertainty in the other agents' predicted trajectories. This\nremains challenging in traditional approaches, especially with arbitrarily\nshaped predictions and real-time constraints. To address these challenges, we\npropose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI),\na motion planner that incorporates uncertain future motions modelled with\npotentially non-Gaussian stochastic predictions. By leveraging MPPI's\ngradient-free nature, we propose a method that efficiently approximates the\njoint Collision Probability (CP) among multiple dynamic obstacles for several\nhundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This\nenables the rejection of samples exceeding a predefined CP threshold or the\nintegration of CP as a weighted objective within the navigation cost function.\nConsequently, DRA-MPPI mitigates the freezing robot problem while enhancing\nsafety. Real-world and simulated experiments with multiple dynamic obstacles\ndemonstrate DRA-MPPI's superior performance compared to state-of-the-art\napproaches, including Scenario-based Model Predictive Control (S-MPC), Frenet\nplanner, and vanilla MPPI.", "comment": "Accepted for presentation at IROS 2025. Submitted Version", "pdf_url": "http://arxiv.org/pdf/2506.21205v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21205v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "拥挤环境中移动机器人的动态风险感知MPPI：通过高效蒙特卡洛近似", "tldr": "本文提出DRA-MPPI，一种新的运动规划器，利用高效蒙特卡洛近似实时处理人群中机器人运动的不确定性，提高安全性并避免“机器人冻结”问题。", "motivation": "在人群中安全部署移动机器人，需要运动规划器考虑其他代理预测轨迹中的不确定性。传统方法在处理任意形状的预测和实时约束时面临挑战。", "method": "本文提出动态风险感知模型预测路径积分控制（DRA-MPPI）。该方法利用MPPI的无梯度特性，通过蒙特卡洛（MC）方法高效近似多个动态障碍物之间的联合碰撞概率（CP），实时处理数百条采样轨迹。这使得可以拒绝超过预定义CP阈值的样本，或将CP作为导航成本函数中的加权目标。", "result": "DRA-MPPI缓解了“机器人冻结”问题，同时提高了安全性。在真实世界和模拟实验中，DRA-MPPI与现有最先进方法（包括基于场景的模型预测控制（S-MPC）、Frenet规划器和普通MPPI）相比，表现出卓越的性能。", "conclusion": "DRA-MPPI通过有效处理不确定性预测和实时约束，显著提升了移动机器人在拥挤环境中的安全性和导航能力，优于现有方法。", "translation": "在人类环境中安全部署移动机器人，需要运动规划器考虑其他代理预测轨迹中的不确定性。这在传统方法中仍然具有挑战性，特别是在任意形状的预测和实时约束下。为了解决这些挑战，我们提出了一种动态风险感知模型预测路径积分控制（DRA-MPPI），这是一种运动规划器，它结合了可能具有非高斯随机预测模型的不确定未来运动。通过利用MPPI的无梯度特性，我们提出了一种方法，可以通过蒙特卡洛（MC）方法实时有效地近似多个动态障碍物之间的联合碰撞概率（CP），针对数百条采样轨迹进行计算。这使得可以拒绝超过预定义CP阈值的样本，或将CP作为导航成本函数中的加权目标。因此，DRA-MPPI缓解了机器人冻结问题，同时增强了安全性。与最先进的方法（包括基于场景的模型预测控制（S-MPC）、Frenet规划器和普通MPPI）相比，DRA-MPPI在多个动态障碍物下的真实世界和模拟实验中表现出卓越的性能。", "summary": "本文提出了一种名为动态风险感知模型预测路径积分控制（DRA-MPPI）的运动规划器，旨在解决移动机器人在人群中导航时处理不确定性预测和实时约束的挑战。DRA-MPPI利用蒙特卡洛方法高效近似联合碰撞概率，从而能够拒绝高风险轨迹或将其纳入导航成本，有效避免了“机器人冻结”问题并提高了安全性。实验结果表明，DRA-MPPI在复杂动态环境中表现优于现有先进方法。", "keywords": "移动机器人, 风险感知, MPPI, 蒙特卡洛近似, 碰撞概率", "comments": "该论文提出了一种创新的风险感知运动规划方法DRA-MPPI，通过高效的蒙特卡洛近似解决了传统方法在处理非高斯不确定性预测和实时约束方面的难题。其核心贡献在于将碰撞概率有效地融入到MPPI框架中，从而显著提高了机器人在拥挤环境中的安全性并避免了保守行为（“机器人冻结”）。该方法的实时性和与现有方法的对比优势，使其在实际应用中具有重要潜力。"}}
{"id": "2506.21036", "title": "An Information-Theoretic Analysis for Federated Learning under Concept Drift", "authors": ["Fu Peng", "Meng Zhang", "Ming Tang"], "summary": "Recent studies in federated learning (FL) commonly train models on static\ndatasets. However, real-world data often arrives as streams with shifting\ndistributions, causing performance degradation known as concept drift. This\npaper analyzes FL performance under concept drift using information theory and\nproposes an algorithm to mitigate the performance degradation. We model concept\ndrift as a Markov chain and introduce the \\emph{Stationary Generalization\nError} to assess a model's capability to capture characteristics of future\nunseen data. Its upper bound is derived using KL divergence and mutual\ninformation. We study three drift patterns (periodic, gradual, and random) and\ntheir impact on FL performance. Inspired by this, we propose an algorithm that\nregularizes the empirical risk minimization approach with KL divergence and\nmutual information, thereby enhancing long-term performance. We also explore\nthe performance-cost tradeoff by identifying a Pareto front. To validate our\napproach, we build an FL testbed using Raspberry Pi4 devices. Experimental\nresults corroborate with theoretical findings, confirming that drift patterns\nsignificantly affect performance. Our method consistently outperforms existing\napproaches for these three patterns, demonstrating its effectiveness in\nadapting concept drift in FL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21036v1", "categories": ["cs.LG", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21036v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "联邦学习在概念漂移下的信息论分析", "tldr": "本文使用信息论分析了联邦学习在概念漂移下的性能，并提出了一种基于KL散度和互信息的算法，该算法通过实验证明能有效提高长期性能并优于现有方法。", "motivation": "现有联邦学习模型常在静态数据集上训练，但在真实世界中数据流分布会发生变化（概念漂移），导致模型性能下降。", "method": "本文利用信息论分析联邦学习在概念漂移下的性能，将概念漂移建模为马尔可夫链，并引入“稳态泛化误差”来评估模型捕获未来数据的能力，其上界通过KL散度和互信息推导。研究了周期性、渐进式和随机三种漂移模式。受此启发，提出了一种算法，通过KL散度和互信息对经验风险最小化方法进行正则化，以提高长期性能。还探讨了性能-成本权衡并构建了树莓派4联邦学习测试平台进行验证。", "result": "实验结果证实了理论发现，确认概念漂移模式显著影响联邦学习性能。所提出的方法在三种漂移模式下均持续优于现有方法。", "conclusion": "所提出的基于信息论的联邦学习概念漂移适应方法，通过对经验风险最小化进行正则化，能够有效提高长期性能并优于现有方法。", "translation": "近期联邦学习（FL）研究通常在静态数据集上训练模型。然而，真实世界的数据常以流的形式到达，且分布不断变化，导致性能下降，这被称为概念漂移。本文利用信息论分析了联邦学习在概念漂移下的性能，并提出了一种缓解性能下降的算法。我们将概念漂移建模为马尔可夫链，并引入“稳态泛化误差”来评估模型捕获未来未知数据特征的能力。其上界通过KL散度和互信息推导得出。我们研究了三种漂移模式（周期性、渐进式和随机）及其对联邦学习性能的影响。受此启发，我们提出了一种算法，该算法利用KL散度和互信息对经验风险最小化方法进行正则化，从而提高长期性能。我们还通过识别帕累托前沿来探索性能-成本权衡。为了验证我们的方法，我们使用树莓派4设备构建了一个联邦学习测试平台。实验结果与理论发现相符，证实了漂移模式显著影响性能。我们的方法在这三种模式下始终优于现有方法，证明了其在联邦学习中适应概念漂移的有效性。", "summary": "本文针对联邦学习在真实世界数据流中面临的概念漂移导致的性能下降问题，引入信息论进行分析。研究将概念漂移建模为马尔可夫链，并定义稳态泛化误差，利用KL散度和互信息推导其上界。在此基础上，提出了一种新的算法，通过KL散度和互信息正则化经验风险最小化，以提升长期性能。实验在树莓派4测试平台上进行，结果表明概念漂移模式对性能影响显著，且所提方法在多种漂移模式下均优于现有方案，证明了其在联邦学习中适应概念漂移的有效性。", "keywords": "联邦学习, 概念漂移, 信息论, KL散度, 互信息", "comments": "这篇论文通过引入信息论来分析联邦学习中的概念漂移问题，提供了一个新颖的理论视角。提出稳态泛化误差及其上界，并基于此设计正则化算法，具有理论和实践意义。使用树莓派搭建测试平台进行验证，增加了其实验的可信度。该研究在解决联邦学习在动态真实世界环境中适应性差的局限性方面迈出了重要一步。"}}
{"id": "2506.20814", "title": "Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning", "authors": ["Jakub Piwko", "Jędrzej Ruciński", "Dawid Płudowski", "Antoni Zajko", "Patryzja Żak", "Mateusz Zacharecki", "Anna Kozak", "Katarzyna Woźnica"], "summary": "Ensemble learning has proven effective in boosting predictive performance,\nbut traditional methods such as bagging, boosting, and dynamic ensemble\nselection (DES) suffer from high computational cost and limited adaptability to\nheterogeneous data distributions. To address these limitations, we propose\nHellsemble, a novel and interpretable ensemble framework for binary\nclassification that leverages dataset complexity during both training and\ninference. Hellsemble incrementally partitions the dataset into circles of\ndifficulty by iteratively passing misclassified instances from simpler models\nto subsequent ones, forming a committee of specialised base learners. Each\nmodel is trained on increasingly challenging subsets, while a separate router\nmodel learns to assign new instances to the most suitable base model based on\ninferred difficulty. Hellsemble achieves strong classification accuracy while\nmaintaining computational efficiency and interpretability. Experimental results\non OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often\noutperforms classical ensemble methods. Our findings suggest that embracing\ninstance-level difficulty offers a promising direction for constructing\nefficient and robust ensemble systems.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20814v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20814v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "分而治之、专业化与路由：一种高效集成学习的新方法", "tldr": "Hellsemble是一种新的集成学习框架，通过将数据集按难度划分并训练专业化模型，实现了高效且准确的二元分类，优于传统方法。", "motivation": "传统集成学习方法（如 bagging, boosting, DES）存在计算成本高和对异构数据分布适应性有限的问题。", "method": "本文提出Hellsemble框架，用于二元分类。该框架通过迭代地将错误分类的实例从较简单的模型传递给后续模型，逐步将数据集划分为难度圈，形成一个由专业化基础学习器组成的委员会。每个模型都在难度递增的子集上进行训练，同时一个独立的路由模型学习根据推断的难度将新实例分配给最合适的基础模型。", "result": "Hellsemble在保持计算效率和可解释性的同时，实现了强大的分类精度。在OpenML-CC18和Tabzilla基准测试上的实验结果表明，Hellsemble通常优于经典集成方法。", "conclusion": "拥抱实例级难度为构建高效、鲁棒的集成系统提供了有前景的方向。", "translation": "集成学习已被证明在提高预测性能方面有效，但传统的袋装法、提升法和动态集成选择（DES）等方法存在计算成本高和对异构数据分布适应性有限的问题。为了解决这些限制，我们提出了 Hellsemble，一种新颖且可解释的二元分类集成框架，它在训练和推理过程中都利用了数据集的复杂性。Hellsemble 通过迭代地将错误分类的实例从较简单的模型传递给后续模型，从而将数据集逐步划分为难度圈，形成一个由专业化基础学习器组成的委员会。每个模型都在难度递增的子集上进行训练，同时一个独立的路由模型学习根据推断的难度将新实例分配给最合适的基础模型。Hellsemble 在保持计算效率和可解释性的同时，实现了强大的分类精度。在 OpenML-CC18 和 Tabzilla 基准测试上的实验结果表明，Hellsemble 通常优于经典的集成方法。我们的研究结果表明，拥抱实例级难度为构建高效且鲁棒的集成系统提供了有前景的方向。", "summary": "本文提出了一种名为 Hellsemble 的新型可解释集成学习框架，旨在解决传统集成方法计算成本高和对异构数据适应性差的问题。Hellsemble 通过将数据集按难度递增地划分为子集，并训练一系列专业化的基础学习器，同时利用一个路由模型将新实例分配给最合适的模型。实验证明，Hellsemble 在保持计算效率和可解释性的同时，在分类精度上优于传统集成方法。", "keywords": "集成学习, 二元分类, 实例难度, 模型专业化, 计算效率", "comments": "Hellsemble 的创新之处在于其“分而治之”的策略，通过实例级难度划分和专业化模型训练，有效提升了集成学习的效率和对复杂数据的处理能力。其可解释性和路由机制是亮点。"}}
{"id": "2506.21412", "title": "Plasmonically Enhanced Flexural-Mode AlScN Nanoplate Resonator as Uncooled and Ultrafast IR Detector with High Responsivity", "authors": ["Aurelio Venditti", "Walter Gubinelli", "Enise F. Altin", "Luca Colombo", "Pietro Simeoni", "Benyamin Davaji", "Matteo Rinaldi"], "summary": "This letter introduces a novel class of miniaturized, uncooled, and\nultra-fast infrared (IR) resonant thermal detectors (RTDs) based on 30%-doped\nAluminum Scandium Nitride (AlScN) nanoplates. Exploiting high electromechanical\ncoupling, good thermal properties, and enhanced and selective IR absorption,\nthe presented device aims to demonstrate significant advancements over the\nstate-of-the-art IR RTDs. This single pixel combines compact footprint, high\nspectral selectivity and responsivity, reduced noise, and fast thermal\nresponse, allowing for the potential development of innovative IR thermal\nimagers through multi-pixel integration. The flexural nature of the actuated\nresonance mode eventually enables an interferometric optical readout, paving\nthe way towards achieving extremely low Noise Equivalent Power levels. These\nresults demonstrate a high IR responsivity of around 130 ppt/pW, a thermal time\nconstant of around 330 us, and a large out-of-plane displacement. This work\nrepresents the first experimental integration on a resonating platform of\nplasmonic absorbers that utilize AlScN as dielectric layer.", "comment": "This manuscript has been submitted to ACS Nano Letters for\n  consideration", "pdf_url": "http://arxiv.org/pdf/2506.21412v1", "categories": ["physics.ins-det", "cs.SY", "eess.SY", "physics.app-ph"], "cate": "physics.ins-det", "url": "http://arxiv.org/abs/2506.21412v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "等离子体增强挠曲模式AlScN纳米板谐振器作为非制冷超快高响应红外探测器", "tldr": "该研究介绍了一种基于等离子体增强的AlScN纳米板谐振器，作为一种小型化、非制冷、超快且高响应的红外探测器。", "motivation": "现有红外谐振热探测器（RTDs）的性能有待提升，尤其是在尺寸、响应速度、噪声和响应度方面。本文旨在通过利用AlScN纳米板的高机电耦合、良好热性能和增强的红外吸收，实现显著的性能突破。", "method": "该研究引入了一种基于30%掺杂的氮化铝钪（AlScN）纳米板的新型小型化、非制冷、超快红外谐振热探测器（RTDs）。该器件利用AlScN的高机电耦合、良好热性能和增强的红外吸收。此外，其弯曲模式的致动谐振特性支持干涉光学读出，并首次在谐振平台上集成了等离子体吸收器，以AlScN作为介电层。", "result": "该设备展示了约130 ppt/pW的高红外响应度，约330微秒的热时间常数，以及较大的面外位移。它结合了紧凑的尺寸、高光谱选择性和响应度、降低的噪声以及快速的热响应。", "conclusion": "该工作首次在谐振平台上实验性地集成了利用AlScN作为介电层的等离子体吸收器，并证明了基于AlScN纳米板谐振器的红外探测器在实现小型化、非制冷、超快和高响应方面具有巨大潜力，为创新的红外热成像仪发展铺平了道路。", "translation": "这封信介绍了一种基于30%掺杂的氮化铝钪（AlScN）纳米板的新型小型化、非制冷、超快红外（IR）谐振热探测器（RTDs）。利用高机电耦合、良好热性能以及增强和选择性红外吸收，所提出的器件旨在展示相较于现有红外RTDs的显著进步。这种单像素结合了紧凑的尺寸、高光谱选择性和响应度、降低的噪声以及快速的热响应，从而为通过多像素集成开发创新的红外热成像仪提供了潜力。致动谐振模式的弯曲性质最终实现了干涉光学读出，为实现极低噪声等效功率水平铺平了道路。这些结果展示了约130 ppt/pW的高红外响应度、约330微秒的热时间常数以及较大的面外位移。这项工作代表了首次在谐振平台上实验性地集成利用AlScN作为介电层的等离子体吸收器。", "summary": "该论文介绍了一种新型的、基于等离子体增强的挠曲模式AlScN纳米板谐振器，作为一种小型化、非制冷、超快且高响应的红外探测器。该器件利用AlScN的高机电耦合和热性能，并结合等离子体吸收器实现增强的红外吸收。实验结果显示，该探测器具有高红外响应度（约130 ppt/pW）和快速热响应（约330微秒），并支持干涉光学读出，为开发高性能红外热成像仪提供了潜力。", "keywords": "AlScN纳米板, 红外探测器, 谐振器, 等离子体增强, 非制冷", "comments": "这项工作的创新之处在于首次将等离子体吸收器与AlScN纳米板谐振器结合，创造出一种新型的红外探测器。其重要性在于实现了小型化、非制冷、超快且高响应的性能，有望推动红外热成像技术的发展。该方法利用了AlScN优异的材料特性和等离子体增强效应，为未来高性能传感器设计提供了新思路。"}}
{"id": "2506.20922", "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization", "authors": ["Ju-Hyeon Nam", "Dong-Hyun Moon", "Sang-Chul Lee"], "summary": "Image editing techniques have rapidly advanced, facilitating both innovative\nuse cases and malicious manipulation of digital images. Deep learning-based\nmethods have recently achieved high accuracy in pixel-level forgery\nlocalization, yet they frequently struggle with computational overhead and\nlimited representation power, particularly for subtle or complex tampering. In\nthis paper, we propose M2SFormer, a novel Transformer encoder-based framework\ndesigned to overcome these challenges. Unlike approaches that process spatial\nand frequency cues separately, M2SFormer unifies multi-frequency and\nmulti-scale attentions in the skip connection, harnessing global context to\nbetter capture diverse forgery artifacts. Additionally, our framework addresses\nthe loss of fine detail during upsampling by utilizing a global prior map, a\ncurvature metric indicating the difficulty of forgery localization, which then\nguides a difficulty-guided attention module to preserve subtle manipulations\nmore effectively. Extensive experiments on multiple benchmark datasets\ndemonstrate that M2SFormer outperforms existing state-of-the-art models,\noffering superior generalization in detecting and localizing forgeries across\nunseen domains.", "comment": "Accepted in International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20922v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20922v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "M2SFormer：结合边缘感知难度指导的多光谱多尺度注意力图像伪造定位", "tldr": "M2SFormer是一个基于Transformer的框架，通过统一多频和多尺度注意力以及引入难度引导的注意力模块来解决图像伪造定位中的计算开销和表示能力限制，在基准数据集上表现优于现有SOTA模型。", "motivation": "现有的深度学习图像伪造定位方法存在计算开销大、表示能力有限的问题，尤其对于细微或复杂的篡改效果不佳。", "method": "本文提出了M2SFormer，一个基于Transformer编码器的新框架。它在跳跃连接中统一了多频和多尺度注意力，并利用一个全局先验图（曲率度量）来指示伪造定位的难度，然后引导一个难度引导的注意力模块以更有效地保留细微操作。", "result": "M2SFormer在多个基准数据集上超越了现有的最先进模型，在检测和定位未见领域伪造方面提供了卓越的泛化能力。", "conclusion": "M2SFormer通过其新颖的架构和注意力机制，有效解决了图像伪造定位的挑战，并在泛化能力和性能上超越了现有SOTA方法。", "translation": "图像编辑技术发展迅速，既促进了创新应用，也导致了数字图像的恶意篡改。基于深度学习的方法最近在像素级伪造定位方面取得了高精度，但它们经常面临计算开销大和表示能力有限的问题，特别是对于细微或复杂的篡改。在本文中，我们提出了M2SFormer，一个新颖的基于Transformer编码器的框架，旨在克服这些挑战。与单独处理空间和频率线索的方法不同，M2SFormer在跳跃连接中统一了多频和多尺度注意力，利用全局上下文更好地捕获不同的伪造痕迹。此外，我们的框架通过利用全局先验图（一个指示伪造定位难度的曲率度量）来解决上采样过程中精细细节的丢失问题，该先验图随后引导一个难度引导的注意力模块，以更有效地保留细微操作。在多个基准数据集上的大量实验表明，M2SFormer优于现有的最先进模型，在检测和定位未见领域伪造方面提供了卓越的泛化能力。", "summary": "M2SFormer是一个新颖的基于Transformer编码器的框架，旨在解决现有深度学习方法在图像伪造定位中面临的计算开销和表示能力有限的挑战。它通过在跳跃连接中统一多频和多尺度注意力来捕获多样的伪造痕迹，并利用一个难度引导的注意力模块来保留细微细节，该模块由指示伪造定位难度的全局先验图指导。实验证明M2SFormer在多个基准数据集上超越了现有SOTA模型，并提供了卓越的泛化能力。", "keywords": "图像伪造定位, Transformer, 多尺度注意力, 多频注意力, 难度引导", "comments": "M2SFormer的创新之处在于其将多频和多尺度注意力统一在跳跃连接中，以及引入了基于曲率度量的难度引导注意力模块。这种结合使得模型能够更全面地捕获伪造痕迹，并有效保留细微细节，从而提高了对复杂和未见伪造的定位能力和泛化性。这是一个重要的进步，对于数字图像取证领域具有实际价值。"}}
{"id": "2504.15217", "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models", "authors": ["Yatong Bai", "Jonah Casebeer", "Somayeh Sojoudi", "Nicholas J. Bryan"], "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15217v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2504.15217v1", "date": "2025-04-21", "updated": "2025-04-21", "AI": {"title_translation": "DRAGON：分布奖励优化扩散生成模型", "tldr": "DRAGON是一个灵活的框架，用于微调媒体生成模型，通过优化针对个体示例或其分布的奖励函数来提高生成质量，并在实验中取得了显著的胜率，甚至无需人类偏好标注即可提升人类感知质量。", "motivation": "传统的强化学习与人类反馈（RLHF）或配对偏好方法（如直接偏好优化DPO）在微调媒体生成模型时不够灵活，无法优化评估个体示例或其分布的多种奖励函数。", "method": "DRAGON（Distributional RewArds for Generative OptimizatioN）是一个通用的框架，它比传统的RLHF或DPO更灵活，能够优化评估个体示例或其分布的奖励函数。它兼容多种奖励类型：实例到实例、实例到分布、分布到分布。该方法通过选择编码器和参考示例来构建示例分布（当使用跨模态编码器如CLAP时，参考示例可以是不同模态的）。然后，DRAGON收集在线和策略内生成内容，对其进行评分以构建正面示范集和负面示范集，并利用两组之间的对比来最大化奖励。", "result": "DRAGON在20种不同的目标奖励函数上实现了81.45%的平均胜率。基于示例集的奖励函数确实增强了生成效果，并且与基于模型的奖励效果相当。通过适当的示例集，DRAGON在没有人类偏好标注训练的情况下，实现了60.95%的人类投票音乐质量胜率。", "conclusion": "DRAGON展示了一种设计和优化奖励函数的新方法，以提高人类感知质量。", "translation": "我们提出了DRAGON（分布奖励生成优化），一个用于微调媒体生成模型以达到预期结果的通用框架。与传统的基于人类反馈的强化学习（RLHF）或配对偏好方法（如直接偏好优化DPO）相比，DRAGON更加灵活。它既可以优化评估单个示例的奖励函数，也可以优化评估示例分布的奖励函数，这使得它与各种实例到实例、实例到分布以及分布到分布的奖励兼容。利用这种多功能性，我们通过选择编码器和一组参考示例来创建示例分布，从而构建了新颖的奖励函数。当使用CLAP等跨模态编码器时，参考示例可以是不同模态的（例如，文本与音频）。然后，DRAGON收集在线和策略内生成内容，对其进行评分以构建一个正面示范集和一个负面示范集，并利用两组之间的对比来最大化奖励。为了进行评估，我们使用20种不同的奖励函数微调了一个音频领域的文本到音乐扩散模型，其中包括一个自定义音乐美学模型、CLAP分数、Vendi多样性和Frechet音频距离（FAD）。我们还比较了实例级（每首歌曲）和全数据集FAD设置，同时消除了多个FAD编码器和参考集的影响。在所有20个目标奖励中，DRAGON实现了81.45%的平均胜率。此外，基于示例集的奖励函数确实增强了生成效果，并且与基于模型的奖励效果相当。通过适当的示例集，DRAGON在没有人类偏好标注训练的情况下，实现了60.95%的人类投票音乐质量胜率。因此，DRAGON展示了一种设计和优化奖励函数以提高人类感知质量的新方法。声音示例可在https://ml-dragon.github.io/web查阅。", "summary": "DRAGON是一种用于微调媒体生成模型的灵活框架，它通过优化评估个体示例或其分布的奖励函数来克服传统RLHF和DPO的局限性。该方法通过构建示例分布、收集在线生成内容并利用正负样本对比来最大化奖励。在对文本到音乐扩散模型的评估中，DRAGON在20种不同奖励函数上取得了81.45%的平均胜率，并且在不依赖人类偏好标注的情况下，通过适当的示例集实现了60.95%的人类投票音乐质量胜率，展示了其在提升人类感知质量方面的潜力。", "keywords": "生成模型, 扩散模型, 奖励优化, DRAGON, 分布式奖励", "comments": "DRAGON的创新之处在于其对奖励函数的灵活性，能够处理个体示例和分布层面的奖励，这显著拓宽了生成模型优化的可能性。其无需人类偏好标注即可提升人类感知质量的能力，对于降低训练成本和提高模型实用性具有重要意义。该方法通过构建示例分布和对比学习的方式来优化扩散模型，提供了一个新的范式。"}}
{"id": "2506.21384", "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.", "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "pdf_url": "http://arxiv.org/pdf/2506.21384v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21384v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用LLM辅助查询理解实现实时检索增强生成", "tldr": "Omni-RAG是一个利用LLM辅助查询理解的框架，旨在提高实时RAG系统处理复杂和嘈杂查询的鲁棒性和有效性。", "motivation": "实时检索增强生成（RAG）系统在处理嘈杂、模糊和多意图的用户查询时面临巨大挑战，因为现有系统通常在更干净的数据上训练或评估，难以应对复杂的输入。", "method": "论文提出了Omni-RAG框架，通过LLM辅助查询理解来预处理用户输入。该框架包含三个核心模块：1) 深度查询理解与分解，利用LLM和定制提示进行查询去噪并将多意图查询分解为结构化子查询；2) 意图感知知识检索，对每个子查询从语料库中执行检索并聚合结果；3) 重排序与生成，使用重排序器优化文档选择，然后LLM利用思维链提示生成最终响应。", "result": "Not mentioned in abstract", "conclusion": "Omni-RAG框架通过LLM辅助的查询理解，能够鲁棒地处理复杂和嘈杂的查询，从而弥合了当前RAG系统能力与实时开放域应用需求之间的差距。", "translation": "实时检索增强生成（RAG）系统在处理通常嘈杂、模糊且包含多个意图的用户查询时面临重大挑战。尽管RAG通过外部知识增强大型语言模型（LLMs），但当前系统通常难以处理此类复杂输入，因为它们通常在更干净的数据上进行训练或评估。本文介绍了一种新颖的框架Omni-RAG，旨在提高RAG系统在实时、开放域环境中的鲁棒性和有效性。Omni-RAG采用LLM辅助的查询理解，通过三个关键模块预处理用户输入：(1) 深度查询理解与分解，该模块利用LLM和定制提示来消除查询中的噪声（例如，纠正拼写错误）并将多意图查询分解为结构化子查询；(2) 意图感知知识检索，该模块对每个子查询从语料库（即使用OpenSearch的FineWeb）中执行检索并聚合结果；(3) 重排序与生成，其中重排序器（即BGE）在LLM（即Falcon-10B）使用思维链提示生成最终响应之前，精炼文档选择。Omni-RAG旨在通过鲁棒地处理复杂和嘈杂的查询，弥合当前RAG能力与实际应用需求之间的差距，例如SIGIR 2025 LiveRAG挑战赛所强调的需求。", "summary": "本文提出了一种名为Omni-RAG的新型框架，旨在解决实时RAG系统在处理嘈杂、模糊和多意图用户查询时面临的挑战。Omni-RAG通过LLM辅助的查询理解，包含深度查询理解与分解、意图感知知识检索以及重排序与生成三个模块，从而能够有效去噪、分解复杂查询、进行精准检索并生成高质量响应，以提升RAG系统在真实世界开放域环境中的鲁棒性和有效性。", "keywords": "RAG, 大型语言模型, 查询理解, 检索增强生成, 实时系统", "comments": "Omni-RAG的创新点在于其多模块的LLM辅助查询理解方法，特别是将复杂查询分解为结构化子查询的能力，这对于提高RAG系统在真实世界嘈杂环境中的表现至关重要。该框架通过整合去噪、分解、意图感知检索和重排序等步骤，提供了一个全面的解决方案，以应对现有RAG系统处理复杂输入的局限性。其重要性体现在其直接针对LiveRAG挑战等实际应用需求，有望推动RAG技术在实际部署中的发展。"}}
{"id": "2506.21053", "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21053v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21053v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MT2-CSD：一种用于会话立场检测的新数据集和多语义知识融合方法", "tldr": "本文介绍了MT2-CSD，一个用于多目标、多轮会话立场检测的大型数据集，并提出了LLM-CRAN模型，该模型利用大型语言模型提升会话理解能力，在MT2-CSD数据集上表现优于现有基线模型。", "motivation": "在当代社交媒体中，自动立场检测对于意见挖掘至关重要，但传统研究主要针对独立实例，限制了其对多方讨论的建模能力。这一不足主要源于缺乏真实捕捉社交媒体互动动态的数据集，阻碍了会话立场检测的进展。", "method": "本文介绍了MT2-CSD，一个用于多目标、多轮会话立场检测的综合数据集，其包含24,457个标注实例，具有最大的会话深度。为解决挑战，提出了大型语言模型增强的会话关系注意力网络（LLM-CRAN），利用LLM的推理能力改进会话理解。", "result": "实验结果表明，LLM-CRAN在会话立场检测任务中显著优于强大的基线模型。", "conclusion": "本文引入了迄今为止最大的会话立场检测数据集MT2-CSD，并提出了LLM-CRAN模型，该模型利用LLM的推理能力有效提升了会话理解和立场检测性能，解决了现有数据集稀缺和模型能力不足的问题。", "translation": "在当代社交媒体领域，自动立场检测对于意见挖掘至关重要，因为它综合并审查用户对有争议话题的观点，以揭示主流趋势和情绪。传统的立场检测研究通常针对单个实例，从而限制了其对真实社交媒体场景中典型的多方讨论进行建模的能力。这一缺点主要源于缺乏真实捕捉社交媒体互动动态的数据集，阻碍了会话立场检测的进展。在本文中，我们介绍了MT2-CSD，一个用于多目标、多轮会话立场检测的综合数据集。据我们所知，MT2-CSD是目前用于此目的的最大数据集，包含24,457个标注实例，并展现出最大的会话深度，从而为立场检测带来了新的挑战。为了应对这些挑战，我们提出了大型语言模型增强的会话关系注意力网络（LLM-CRAN），该网络利用LLM的推理能力来改善会话理解。我们进行了广泛的实验来评估LLM-CRAN在MT2-CSD数据集上的有效性。实验结果表明，LLM-CRAN在会话立场检测任务中显著优于强大的基线模型。", "summary": "本文针对社交媒体中会话立场检测的数据集稀缺和模型能力不足问题，提出了MT2-CSD，一个迄今为止最大的多目标、多轮会话立场检测数据集。为应对数据集带来的新挑战，研究引入了LLM-CRAN模型，该模型通过利用大型语言模型（LLM）的推理能力来增强会话理解。实验证明，LLM-CRAN在MT2-CSD数据集上的表现显著优于现有的强基线模型。", "keywords": "会话立场检测, MT2-CSD, LLM-CRAN, 大型语言模型, 数据集", "comments": "该论文的创新点在于构建了迄今为止最大的多目标、多轮会话立场检测数据集MT2-CSD，解决了该领域长期存在的数据稀缺问题。同时，提出的LLM-CRAN模型巧妙地结合了大型语言模型的推理能力，为会话理解和立场检测提供了新的视角和有效方法。其重要性在于推动了会话立场检测领域的研究进展，为更真实、更复杂的社交媒体互动分析提供了基础数据和高性能模型。"}}
{"id": "2506.21326", "title": "A discontinuous in time Streamline Diffusion Virtual Element Method for Darcy-transport problem", "authors": ["R A Caraballo Diaz", "F Dassi"], "summary": "We present a first numerical study of transport phenomena involving\nchemically reactive species, modeled by advection-diffusion-reaction systems\nwith flow fields governed by Darcy's law. Among the various discretisation\napproaches, we consider the Streamline Diffusion method. Both the velocity\nfield and the species concentrations are computed using the Virtual Element\nMethod using a Discontinuous Galerkin scheme for time. An abstract error\nestimate has been derived using a special technique that utilizes Gauss-Radau\ninterpolation in conjunction with numerical integration. These theoretical\nfindings are supported by numerical experiments with arbitrary-order accuracy\nin both space and time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21326v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21326v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "达西输运问题中基于时间不连续流线扩散的虚单元法", "tldr": "本文首次数值研究了一种时间不连续的流线扩散虚单元法，用于模拟达西流场下的对流-扩散-反应系统，并通过数值实验验证了理论误差估计的任意阶精度。", "motivation": "研究涉及化学反应物种的输运现象，这些现象由达西定律控制的流场下的对流-扩散-反应系统建模。", "method": "采用流线扩散法，结合虚单元法计算速度场和物种浓度，并使用时间不连续伽辽金（Discontinuous Galerkin）方案处理时间。通过结合高斯-拉道插值和数值积分的特殊技术推导了抽象误差估计。", "result": "理论发现得到了数值实验的支持，这些实验在空间和时间上都显示出任意阶精度。", "conclusion": "所提出的时间不连续流线扩散虚单元法能够有效且准确地模拟达西-输运问题中的对流-扩散-反应系统，并在空间和时间上都达到了任意阶精度。", "translation": "我们首次对涉及化学反应物种的输运现象进行了数值研究，这些现象由达西定律控制的流场下的对流-扩散-反应系统建模。在各种离散化方法中，我们考虑了流线扩散法。速度场和物种浓度都使用虚单元法计算，时间方面采用不连续伽辽金方案。通过结合高斯-拉道插值和数值积分的特殊技术，推导出了抽象误差估计。这些理论发现得到了数值实验的支持，数值实验在空间和时间上都显示出任意阶精度。", "summary": "本文首次对达西定律控制下的对流-扩散-反应系统中的化学反应物种输运现象进行了数值研究。作者提出了一种时间不连续的流线扩散虚单元法，用于计算速度场和物种浓度。该方法在时间离散上采用了不连续伽辽金方案。通过结合高斯-拉道插值和数值积分，推导出了抽象误差估计，并通过数值实验验证了该方法在空间和时间上的任意阶精度。", "keywords": "Streamline Diffusion, Virtual Element Method, Discontinuous Galerkin, Darcy-transport, Advection-diffusion-reaction", "comments": "这项工作创新性地将流线扩散法、虚单元法和时间不连续伽辽金方案结合起来，用于解决复杂的达西输运问题。其理论误差估计和数值实验验证了方法的精度，为相关领域的数值模拟提供了新的高效工具。"}}
{"id": "2506.21046", "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features", "authors": ["Shangbo Wu", "Yu-an Tan", "Ruinan Ma", "Wencong Ma", "Dehua Zhu", "Yuanzhang Li"], "summary": "The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA.", "comment": "14 pages, 9 figures, to appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21046v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21046v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "提升生成式对抗迁移性与自监督视觉Transformer特征", "tldr": "本文提出dSVA，一种利用自监督ViT（结合对比学习和掩蔽图像建模）提取的全局和局部特征来生成对抗样本的方法，显著提升了黑盒对抗迁移性，超越了现有技术。", "motivation": "之前的对抗样本生成方法主要依赖监督学习的特征。受自监督学习和Transformer架构之间协同作用的启发，本文旨在探索利用自监督Vision Transformer (ViT) 表示能否改善对抗迁移性。", "method": "本文提出dSVA，一种生成式双自监督ViT特征攻击方法。它利用ViT的对比学习（CL）的全局结构特征和掩蔽图像建模（MIM）的局部纹理特征。设计了一个新的生成式训练框架，包含一个生成器来创建黑盒对抗样本，并通过利用自监督ViT的联合特征和注意力机制来训练生成器。", "result": "研究发现，CL和MIM使ViT能够关注不同的特征倾向，当两者结合利用时，能显著提升对抗泛化能力。通过扰动自监督ViT提取的双重深度特征，实现了对各种架构模型的卓越黑盒迁移性，性能优于现有技术。", "conclusion": "结合自监督ViT的全局（CL）和局部（MIM）特征能够显著提升对抗样本的黑盒迁移性，证明了自监督学习在对抗攻击领域的潜力。", "translation": "深度神经网络（DNNs）的能力源于从提供的数据中提取和解释特征。通过利用DNN中的中间特征而非依赖硬标签，我们能够生成更有效泛化的对抗性扰动，从而提升黑盒迁移性。在之前的工作中，这些特征普遍来自于监督学习。受自监督学习和Transformer架构之间卓越协同作用的启发，本文探讨了利用自监督视觉Transformer（ViT）表示是否能改善对抗迁移性。我们提出了dSVA——一种生成式双自监督ViT特征攻击，它利用了来自对比学习（CL）的全局结构特征和来自掩蔽图像建模（MIM）的局部纹理特征，这是ViT的自监督学习范式组合。我们设计了一个新颖的生成式训练框架，该框架包含一个生成器来创建黑盒对抗样本，以及通过利用自监督ViT的联合特征和注意力机制来训练生成器的策略。我们的研究结果表明，CL和MIM使ViT能够关注不同的特征倾向，当两者结合利用时，能够显著提升对抗泛化能力。通过扰动自监督ViT提取的双重深度特征，我们获得了对各种架构模型的卓越黑盒迁移性，其性能优于现有技术。代码可在https://github.com/spencerwooo/dSVA 获取。", "summary": "本文提出dSVA，一种新颖的生成式对抗攻击方法，旨在提升黑盒对抗迁移性。该方法创新性地利用自监督Vision Transformer (ViT) 的双重特征：来自对比学习的全局结构特征和来自掩蔽图像建模的局部纹理特征。通过设计一个生成器训练框架，dSVA能够生成高度泛化的对抗样本。实验证明，结合这两种自监督ViT特征能显著提高对抗攻击的迁移性，超越了现有最佳方法，展示了自监督学习在对抗攻击中的巨大潜力。", "keywords": "对抗攻击, 黑盒迁移性, 自监督学习, Vision Transformer, 生成式对抗网络", "comments": "这篇论文的创新点在于首次将自监督学习（特别是ViT的CL和MIM双范式）应用于提升生成式对抗攻击的黑盒迁移性。通过利用ViT提取的更丰富、更具鲁棒性的特征，成功克服了传统方法对硬标签或监督特征的依赖，极大地提升了对抗样本的泛化能力。其方法dSVA为未来对抗攻击和防御的研究开辟了新途径，特别是在自监督模型日益普及的背景下。"}}
{"id": "2506.21250", "title": "ACTLLM: Action Consistency Tuned Large Language Model", "authors": ["Jing Bi", "Lianggong Bruce Wen", "Zhang Liu", "Chenliang Xu"], "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model),\na novel approach for robot manipulation in dynamic environments. Traditional\nvision-based systems often struggle to learn visual representations that excel\nin both task execution and spatial reasoning, thereby limiting their\nadaptability in dynamic environments. ACTLLM addresses these challenges by\nharnessing language to craft structured scene descriptors, providing a uniform\ninterface for both spatial understanding and task performance through flexible\nlanguage instructions. Moreover, we introduce a novel action consistency\nconstraint that aligns visual perception with corresponding actions, thereby\nenhancing the learning of actionable visual representations. Additionally, we\nhave reformulated the Markov decision process for manipulation tasks into a\nmulti-turn visual dialogue framework. This approach enables the modeling of\nlong-term task execution with enhanced contextual relevance derived from the\nhistory of task execution. During our evaluation, ACTLLM excels in diverse\nscenarios, proving its effectiveness on challenging vision-based robot\nmanipulation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21250v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21250v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ACTLLM：动作一致性调优的大型语言模型", "tldr": "ACTLLM是一种新型机器人操作方法，利用语言和动作一致性在动态环境中实现高效视觉机器人操作。", "motivation": "传统基于视觉的系统难以学习在任务执行和空间推理方面都表现出色的视觉表示，从而限制了它们在动态环境中的适应性。", "method": "引入ACTLLM，通过语言生成结构化的场景描述符，为空间理解和任务性能提供统一接口。引入新的动作一致性约束，使视觉感知与相应动作对齐，以增强可操作视觉表示的学习。将操作任务的马尔可夫决策过程重新构建为多轮视觉对话框架，以建模长期任务执行。", "result": "ACTLLM在各种场景中表现出色，证明了其在具有挑战性的基于视觉的机器人操作任务中的有效性。", "conclusion": "ACTLLM通过结合语言指导和动作一致性，有效解决了动态环境中机器人操作的挑战，提升了视觉机器人的任务执行和空间推理能力。", "translation": "本文介绍了ACTLLM（动作一致性调优的大型语言模型），这是一种在动态环境中进行机器人操作的新颖方法。传统的基于视觉的系统通常难以学习在任务执行和空间推理方面都表现出色的视觉表示，从而限制了它们在动态环境中的适应性。ACTLLM通过利用语言来创建结构化的场景描述符来解决这些挑战，为空间理解和通过灵活的语言指令执行任务提供统一的接口。此外，我们引入了一种新颖的动作一致性约束，使视觉感知与相应的动作对齐，从而增强了可操作视觉表示的学习。此外，我们还将操作任务的马尔可夫决策过程重新构建为多轮视觉对话框架。这种方法能够通过从任务执行历史中获得增强的上下文相关性来建模长期任务执行。在我们的评估中，ACTLLM在各种场景中表现出色，证明了其在具有挑战性的基于视觉的机器人操作任务中的有效性。", "summary": "ACTLLM是一种创新的大型语言模型，专为动态环境中的机器人操作设计。它通过利用语言生成结构化场景描述符，并引入动作一致性约束来对齐视觉感知与动作，从而克服了传统视觉系统在学习有效视觉表示上的局限性。该方法还将马尔可夫决策过程重构为多轮视觉对话，以增强长期任务执行的上下文关联性。实验证明ACTLLM在复杂的视觉机器人操作任务中表现卓越。", "keywords": "机器人操作, 大型语言模型, 动作一致性, 视觉机器人, 动态环境", "comments": "这篇论文的创新点在于结合了大型语言模型与机器人操作，特别是通过引入“动作一致性约束”来优化视觉表示的学习，以及将MDP重构为“多轮视觉对话”以处理长期任务。这种方法有效地弥补了传统视觉系统在动态环境适应性和空间推理上的不足，为机器人操作提供了一个更灵活、更具上下文感知能力的解决方案。"}}
{"id": "2506.21338", "title": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification", "authors": ["Galvin Brice S. Lim", "Brian Godwin S. Lim", "Argel A. Bandala", "John Anthony C. Jose", "Timothy Scott C. Chu", "Edwin Sybingco"], "summary": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.21338v1", "categories": ["cs.LG", "cs.HC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21338v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "AGTCNet: 一种用于规范运动想象脑电图分类的图时域方法", "tldr": "AGTCNet是一种新颖的图时域模型，用于运动想象EEG分类，通过利用电极拓扑结构和图卷积注意力网络，实现了最先进的性能，同时具有更小的模型尺寸和更快的推理速度，解决了脑机接口系统中EEG信号时空依赖性捕获的挑战。", "motivation": "脑机接口（BCI）技术在帮助运动障碍人士方面具有巨大潜力，但开发与受试者和会话无关的BCI系统仍面临挑战，因为个体间和时间上神经网络活动的复杂性和变异性，以及EEG硬件的限制。现有方法在捕获多通道EEG信号中复杂的时空依赖性方面效率低下。", "method": "本研究引入了注意力图时域卷积网络（AGTCNet），这是一种新颖的图时域模型，用于运动想象EEG（MI-EEG）分类。AGTCNet利用EEG电极的拓扑配置作为归纳偏置，并集成了图卷积注意力网络（GCAT）来共同学习富有表现力的时空EEG表示。", "result": "AGTCNet显著优于现有MI-EEG分类器，实现了最先进的性能，同时采用紧凑的架构。模型尺寸减少了49.87%，推理时间加快了64.65%，并且使用更短的输入EEG信号。在BCI Competition IV Dataset 2a上，AGTCNet在受试者独立分类中实现了66.82%的移动平均准确率，经受试者特定微调后提高到82.88%。在EEG Motor Movement/Imagery Dataset上，AGTCNet在4分类和2分类受试者独立分类中分别达到了64.14%和85.22%的移动平均准确率，受试者特定分类进一步提高到72.13%和90.54%。", "conclusion": "AGTCNet通过有效捕获EEG信号的时空依赖性，并在紧凑的架构下实现卓越的性能，证明了其在BCI部署中的有效性和实用性。", "translation": "脑机接口（BCI）技术利用脑电图（EEG）标志着一项变革性创新，使运动障碍人士能够平等地参与环境。尽管其潜力巨大，但由于个体之间和随时间变化的神经活动固有的复杂性和变异性，以及EEG硬件限制，开发受试者不变和会话不变的BCI系统仍然是一个重大挑战。虽然先前的研究试图开发鲁棒的BCI系统，但现有方法在捕获多通道EEG信号中复杂的时空依赖性方面仍然无效。本研究通过引入注意力图时域卷积网络（AGTCNet）来解决这一差距，这是一种用于运动想象EEG（MI-EEG）分类的新型图时域模型。具体来说，AGTCNet利用EEG电极的拓扑配置作为归纳偏置，并集成了图卷积注意力网络（GCAT）来共同学习富有表现力的时空EEG表示。所提出的模型显著优于现有MI-EEG分类器，实现了最先进的性能，同时采用紧凑的架构，凸显了其在BCI部署中的有效性和实用性。模型尺寸减少了49.87%，推理时间加快了64.65%，并使用更短的输入EEG信号，AGTCNet在BCI Competition IV Dataset 2a上针对受试者独立分类实现了66.82%的移动平均准确率，当针对受试者特定分类进行微调时，准确率进一步提高到82.88%。在EEG Motor Movement/Imagery Dataset上，AGTCNet在4分类和2分类受试者独立分类中分别实现了64.14%和85.22%的移动平均准确率，受试者特定分类进一步提高到72.13%和90.54%。", "summary": "本研究提出了一种名为AGTCNet的新型图时域卷积网络，用于运动想象EEG（MI-EEG）分类。该模型通过利用EEG电极的拓扑结构作为归纳偏置，并结合图卷积注意力网络，有效地捕获了多通道EEG信号中复杂的时空依赖性。实验结果表明，AGTCNet在保持紧凑架构的同时，显著超越了现有MI-EEG分类器，实现了最先进的性能，并且在模型尺寸、推理速度和所需输入信号长度方面均有显著优化，验证了其在脑机接口应用中的有效性和实用性。", "keywords": "运动想象, 脑电图分类, 图时域网络, 脑机接口, AGTCNet", "comments": "AGTCNet的创新之处在于其将EEG电极的拓扑配置作为归纳偏置，并结合图卷积注意力网络来学习时空EEG表示，有效解决了传统方法难以捕捉复杂时空依赖性的问题。其重要性体现在不仅提升了分类准确率，还在模型紧凑性和推理效率上取得了显著进步，这对于实际的脑机接口系统部署具有重要的实践意义。"}}
{"id": "2506.21453", "title": "Towards an Optimal Control Perspective of ResNet Training", "authors": ["Jens Püttschneider", "Simon Heilig", "Asja Fischer", "Timm Faulwasser"], "summary": "We propose a training formulation for ResNets reflecting an optimal control\nproblem that is applicable for standard architectures and general loss\nfunctions. We suggest bridging both worlds via penalizing intermediate outputs\nof hidden states corresponding to stage cost terms in optimal control. For\nstandard ResNets, we obtain intermediate outputs by propagating the state\nthrough the subsequent skip connections and the output layer. We demonstrate\nthat our training dynamic biases the weights of the unnecessary deeper residual\nlayers to vanish. This indicates the potential for a theory-grounded layer\npruning strategy.", "comment": "Accepted for presentation at the High-dimensional Learning Dynamics\n  (HiLD) workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21453v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21453v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "面向ResNet训练的最优控制视角", "tldr": "将ResNet训练构建为最优控制问题，通过惩罚中间输出使不必要的深层权重消失，为剪枝提供理论基础。", "motivation": "旨在为ResNet训练提供一种新的、基于最优控制的视角和公式，以解决标准架构和通用损失函数下的训练问题。", "method": "提出将ResNet训练表述为一个最优控制问题，通过惩罚隐藏状态的中间输出（对应于最优控制中的阶段成本项）来连接ResNet和最优控制。具体地，对于标准ResNet，通过后续的跳跃连接和输出层来获取中间输出。", "result": "训练动态导致不必要的更深残差层的权重趋于消失。", "conclusion": "这种训练方法为基于理论的层剪枝策略提供了潜力。", "translation": "我们提出了一种ResNet训练公式，反映了一个适用于标准架构和通用损失函数的最优控制问题。我们建议通过惩罚对应于最优控制中阶段成本项的隐藏状态中间输出来连接这两个领域。对于标准ResNet，我们通过后续的跳跃连接和输出层传播状态来获取中间输出。我们证明了我们的训练动态会使不必要的更深残差层的权重消失。这表明了基于理论的层剪枝策略的潜力。", "summary": "该论文提出将ResNet训练重新构建为一个最优控制问题，通过在训练过程中惩罚中间输出（作为阶段成本）来促使不必要的深层残差层权重趋于消失。这种方法为开发理论上更严谨的层剪枝策略提供了可能性。", "keywords": "ResNet训练, 最优控制, 层剪枝, 权重消失", "comments": "该研究通过将ResNet训练与最优控制理论相结合，提供了一个新颖的视角。其创新点在于引入了中间输出惩罚机制，并证明了其对层剪枝的潜在益处，这可能为神经网络压缩提供新的理论基础。"}}
{"id": "2506.20936", "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling", "authors": ["Hao Zhang", "Haolan Xu", "Chun Feng", "Varun Jampani", "Narendra Ahuja"], "summary": "Skinning and rigging are fundamental components in animation, articulated\nobject reconstruction, motion transfer, and 4D generation. Existing approaches\npredominantly rely on Linear Blend Skinning (LBS), due to its simplicity and\ndifferentiability. However, LBS introduces artifacts such as volume loss and\nunnatural deformations, and it fails to model elastic materials like soft\ntissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty\ntissues). In this work, we propose PhysRig: a differentiable physics-based\nskinning and rigging framework that overcomes these limitations by embedding\nthe rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),\nwhich is simulated as a deformable soft-body structure driven by the animated\nskeleton. Our method leverages continuum mechanics and discretizes the object\nas particles embedded in an Eulerian background grid to ensure\ndifferentiability with respect to both material properties and skeletal motion.\nAdditionally, we introduce material prototypes, significantly reducing the\nlearning space while maintaining high expressiveness. To evaluate our\nframework, we construct a comprehensive synthetic dataset using meshes from\nObjaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object\ncategories and motion patterns. Our method consistently outperforms traditional\nLBS-based approaches, generating more realistic and physically plausible\nresults. Furthermore, we demonstrate the applicability of our framework in the\npose transfer task highlighting its versatility for articulated object\nmodeling.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20936v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20936v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PhysRig：基于可微分物理的蒙皮和绑定框架，用于逼真关节对象建模", "tldr": "本文提出了PhysRig，一个可微分的基于物理的蒙皮和绑定框架，通过将骨架嵌入到可变形软体结构中，克服了传统线性混合蒙皮（LBS）的局限性，生成更逼真、物理上更合理的结果。", "motivation": "现有蒙皮和绑定方法（如线性混合蒙皮LBS）存在体积损失、不自然变形等伪影，且无法有效建模软组织、毛发等弹性材料。", "method": "提出PhysRig框架，将刚性骨架嵌入体积表示（如四面体网格），并模拟为由动画骨架驱动的可变形软体结构。该方法利用连续介质力学，将对象离散化为嵌入欧拉背景网格中的粒子，以确保对材料属性和骨骼运动都可微分。此外，引入材料原型以减少学习空间。", "result": "PhysRig在综合合成数据集上持续优于传统的基于LBS的方法，生成更逼真且物理上更合理的结果。该框架在姿态迁移任务中也展现了其适用性。", "conclusion": "PhysRig提供了一个有效且通用的解决方案，解决了传统蒙皮和绑定方法在处理弹性材料和生成物理逼真变形方面的局限性，尤其适用于关节对象建模。", "translation": "蒙皮和绑定是动画、关节对象重建、运动迁移和4D生成中的基本组成部分。现有方法主要依赖线性混合蒙皮（LBS），因为它简单且可微分。然而，LBS引入了体积损失和不自然变形等伪影，并且无法建模弹性材料，如软组织、毛发和柔性附肢（例如，象鼻、耳朵和脂肪组织）。在这项工作中，我们提出了PhysRig：一个可微分的基于物理的蒙皮和绑定框架，通过将刚性骨架嵌入到体积表示（例如，四面体网格）中来克服这些限制，该体积表示被模拟为由动画骨架驱动的可变形软体结构。我们的方法利用连续介质力学，将对象离散化为嵌入欧拉背景网格中的粒子，以确保对材料属性和骨骼运动都可微分。此外，我们引入了材料原型，显著减少了学习空间，同时保持了高表达性。为了评估我们的框架，我们使用来自Objaverse、The Amazing Animals Zoo和MixaMo的网格构建了一个全面的合成数据集，涵盖了不同的对象类别和运动模式。我们的方法始终优于传统的基于LBS的方法，生成更逼真且物理上更合理的结果。此外，我们展示了我们框架在姿态迁移任务中的适用性，突出了其在关节对象建模方面的多功能性。", "summary": "本文介绍了PhysRig，一个创新的可微分物理蒙皮和绑定框架，旨在克服传统线性混合蒙皮（LBS）在处理弹性材料和生成逼真变形方面的局限性。PhysRig通过将骨架嵌入可变形软体结构并利用连续介质力学进行物理模拟，实现了对材料属性和骨骼运动的可微分性，同时通过引入材料原型优化了学习空间。实验证明，PhysRig在生成更逼真、物理上更合理的结果方面显著优于LBS，并在姿态迁移等任务中展现了广泛的适用性，为关节对象建模提供了新的解决方案。", "keywords": "蒙皮, 绑定, 可微分物理, 软体模拟, 关节对象建模", "comments": "PhysRig的创新之处在于将可微分物理模拟引入蒙皮和绑定过程，克服了传统LBS在处理复杂变形和弹性材料方面的固有局限性。通过嵌入骨架到体积软体中并利用连续介质力学，它能够生成更物理逼真的效果。该方法的可微分性对于优化和学习非常重要。其在多种数据集上的优越表现和在姿态迁移中的应用前景，使其在动画、VR/AR和数字人建模等领域具有重要意义。"}}
{"id": "2506.20243", "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "authors": ["Papa Séga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2506.20243v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20243v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "CBF-AFA：基于分块的多自监督学习融合用于自动流利度评估", "tldr": "CBF-AFA提出了一种基于分块的多自监督学习（SSL）融合方法，用于自动流利度评估，通过整合Wav2Vec2、HuBERT和WavLM模型并结合分层CNN-BiLSTM框架，显著提高了非母语使用者流利度评估的准确性。", "motivation": "自动流利度评估（AFA）仍然具有挑战性，尤其是在捕捉非母语使用者的语速、停顿和不流利现象方面。", "method": "本文提出了一种基于分块的方法，将Wav2Vec2、HuBERT和WavLM等自监督学习（SSL）模型与分层CNN-BiLSTM框架集成。语音通过Silero语音活动检测（Silero-VAD）被分割成呼吸组分块，以实现细粒度时间分析并减少过度分割伪影。SSL嵌入通过可学习的加权机制融合，平衡声学和语言特征，并辅以分块级的流利度标记（如语速、停顿持续时间、n-gram重复）。CNN-BiLSTM用于捕捉分块之间的局部和长期依赖关系。", "result": "在Avalinguo和Speechocean762数据集上进行评估，与单SSL基线相比，我们的方法在Speechocean762上F1分数提高了2.8点，Pearson相关性提高了6.2点；在Avalinguo上F1分数提高了4.2点，Pearson相关性提高了4.0点，并且超越了基于Pyannote.audio的分割基线。", "conclusion": "这些发现突出了基于分块的多SSL融合在鲁棒流利度评估中的有效性，尽管未来的工作应探索其在具有不规则韵律的方言上的泛化能力。", "translation": "自动流利度评估（AFA）仍然具有挑战性，尤其是在捕捉非母语使用者的语速、停顿和不流利现象方面。我们引入了一种基于分块的方法，该方法集成了自监督学习（SSL）模型（Wav2Vec2、HuBERT和WavLM），这些模型因其在语音、韵律和嘈杂语音建模方面的互补优势而被选择，并结合了分层CNN-BiLSTM框架。语音使用Silero语音活动检测（Silero-VAD）被分割成呼吸组分块，从而实现细粒度时间分析，同时减轻过度分割的伪影。SSL嵌入通过可学习的加权机制进行融合，平衡声学和语言特征，并辅以分块级的流利度标记（例如，语速、停顿持续时间、n-gram重复）。CNN-BiLSTM捕捉分块之间的局部和长期依赖关系。在Avalinguo和Speechocean762数据集上进行评估，与单SSL基线相比，我们的方法在Speechocean762上F1分数提高了2.8点，Pearson相关性提高了6.2点，在Avalinguo上F1分数提高了4.2点，Pearson相关性提高了4.0点，并且超越了基于Pyannote.audio的分割基线。这些发现突出了基于分块的多SSL融合在鲁棒流利度评估中的有效性，尽管未来的工作应探索其在具有不规则韵律的方言上的泛化能力。", "summary": "本文提出CBF-AFA，一种用于自动流利度评估的基于分块的多自监督学习（SSL）融合方法。该方法将Wav2Vec2、HuBERT和WavLM等SSL模型与分层CNN-BiLSTM框架结合，通过Silero-VAD将语音分割成呼吸组分块进行细粒度分析。SSL嵌入通过加权机制融合，并结合分块级流利度标记。实验结果表明，该方法在Avalinguo和Speechocean762数据集上均显著优于单SSL基线和基于Pyannote.audio的分割基线，验证了其在鲁棒流利度评估方面的有效性。", "keywords": "自动流利度评估, 自监督学习, 语音分块, 多模态融合, 深度学习", "comments": "该论文的创新点在于提出了基于分块的多自监督学习融合方法，有效结合了不同SSL模型的互补优势，并通过分层网络和细粒度特征增强，显著提升了自动流利度评估的性能。其在处理非母语使用者语音特点方面的表现尤为突出，具有重要的实际应用价值。未来的工作可以进一步探索其在不同方言和更复杂语境下的泛化能力。"}}
{"id": "2506.21445", "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21445v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21445v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "跨语言Text2Cypher：评估超越英语的基础模型", "tldr": "研究评估了基础LLM在多语言Text2Cypher任务上的表现，发现英语表现最好，西班牙语次之，土耳其语最差，并发布了多语言测试集。", "motivation": "现有研究主要关注英语的自然语言到数据库查询转换（如Text2Cypher），而对其他语言的评估有限，因此需要调查基础LLM在多语言Text2Cypher任务上的性能。", "method": "创建并发布了一个多语言测试集，通过将英语问题翻译成西班牙语和土耳其语，同时保留原始Cypher查询，以实现公平的跨语言比较。使用标准化提示和指标评估了多个基础模型。", "result": "结果显示一致的性能模式：英语最高，其次是西班牙语，土耳其语最低。这归因于训练数据可用性和语言特征的差异。此外，将任务提示翻译成西班牙语和土耳其语对评估指标几乎没有影响，表明提示翻译影响很小。", "conclusion": "研究结果强调了在多语言查询生成中进行更具包容性的评估和开发的必要性。未来的工作包括模式本地化和跨不同语言的微调。", "translation": "大型语言模型的最新进展使得自然语言接口能够将用户问题翻译成数据库查询，例如Text2SQL、Text2SPARQL和Text2Cypher。虽然这些接口增强了数据库的可访问性，但目前大多数研究仅关注英语，对其他语言的评估有限。本文研究了基础LLM在多语言Text2Cypher任务上的性能。我们通过将英语问题翻译成西班牙语和土耳其语，同时保留原始Cypher查询，创建并发布了一个多语言测试集，从而实现了公平的跨语言比较。我们使用标准化提示和指标评估了多个基础模型。我们的结果显示出一致的性能模式：英语表现最高，其次是西班牙语，土耳其语最低。我们将其归因于训练数据可用性和语言特征的差异。此外，我们探讨了将任务提示翻译成西班牙语和土耳其语的影响。结果显示评估指标几乎没有变化，表明提示翻译影响很小。我们的发现强调了在多语言查询生成中进行更具包容性的评估和开发的必要性。未来的工作包括模式本地化和跨不同语言的微调。", "summary": "本文研究了基础大型语言模型在多语言Text2Cypher任务上的表现，旨在弥补现有研究主要集中于英语的不足。研究构建了一个包含英语、西班牙语和土耳其语的跨语言测试集，并评估了多个基础模型。结果显示，模型性能在英语上最高，西班牙语次之，土耳其语最低，这与训练数据和语言特性有关。此外，研究发现提示翻译对性能影响不大。论文强调了未来在多语言查询生成领域需要更具包容性的评估和开发。", "keywords": "Text2Cypher, 多语言, 基础模型, 数据库查询, 跨语言评估", "comments": "这篇论文通过创建多语言测试集并评估基础模型在Text2Cypher任务上的表现，填补了现有研究主要集中于英语的空白，具有重要的创新性。其发现揭示了不同语言间性能差异的原因，并指出了未来多语言查询生成领域发展方向，对提升数据库可访问性具有重要意义。"}}
{"id": "2506.21096", "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "comment": "Accepted by ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21096v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21096v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DALR：用于多模态句子表示学习的双层对齐学习", "tldr": "DALR提出双层对齐学习框架，通过细粒度跨模态对齐和模态内排序蒸馏解决多模态句子表示学习中的对齐问题，并在STS和TR任务上超越了现有SOTA方法。", "motivation": "现有多模态句子表示学习方法主要在粗粒度上对齐图像和文本，面临跨模态错位偏差和模态内语义差异两大挑战，严重降低了句子表示质量。", "method": "提出DALR（用于多模态句子表示的双层对齐学习）框架。对于跨模态对齐，设计了一致性学习模块，通过软化负样本并利用辅助任务的语义相似性实现细粒度对齐。此外，通过将排序蒸馏与全局模态内对齐学习相结合，以更好地捕捉句子间复杂的排序关系，提升表示质量。", "result": "在语义文本相似度（STS）和迁移（TR）任务上的综合实验验证了所提方法的有效性，并持续展现出优于现有最先进基线的性能。", "conclusion": "DALR框架通过双层对齐学习有效解决了多模态句子表示学习中的挑战，在语义文本相似度（STS）和迁移（TR）任务上均表现出优越性。", "translation": "以前的多模态句子表示学习方法已经取得了令人印象深刻的性能。然而，大多数方法都侧重于在粗粒度级别上对齐图像和文本，面临两个关键挑战：跨模态错位偏差和模态内语义差异，这显著降低了句子表示质量。为了解决这些挑战，我们提出了DALR（用于多模态句子表示的双层对齐学习）。对于跨模态对齐，我们提出了一致性学习模块，该模块软化负样本并利用辅助任务的语义相似性来实现细粒度的跨模态对齐。此外，我们认为句子关系超越了二元正负标签，呈现出更复杂的排序结构。为了更好地捕捉这些关系并提高表示质量，我们将排序蒸馏与全局模态内对齐学习相结合。在语义文本相似度（STS）和迁移（TR）任务上的综合实验验证了我们方法的有效性，并持续展示出其优于最先进基线的性能。", "summary": "本文提出了DALR（双层对齐学习）框架，旨在解决现有多模态句子表示学习方法中粗粒度对齐所导致的跨模态错位偏差和模态内语义差异问题。DALR引入了一致性学习模块，通过软化负样本并利用辅助任务的语义相似性，实现了细粒度跨模态对齐。同时，为了更好地捕捉句子间复杂的排序关系，DALR将排序蒸馏与全局模态内对齐学习相结合。实验结果表明，DALR在语义文本相似度（STS）和迁移（TR）任务上均表现出优于现有先进基线的性能。", "keywords": "多模态学习, 句子表示, 对齐学习, 语义相似度, 排序蒸馏", "comments": "DALR的创新点在于其提出的双层对齐学习策略，特别是细粒度跨模态对齐中对负样本的处理和辅助任务的利用，以及模态内对齐中引入排序蒸馏来捕捉更复杂的句子关系。这些方法有效解决了多模态表示学习中的关键挑战，对于提升句子表示质量和多模态理解具有重要意义。"}}
{"id": "2506.21361", "title": "Efficient parameter-robust preconditioners for linear poroelasticity and elasticity in the primal formulation", "authors": ["Weizhang Huang", "Zhuoran Wang"], "summary": "Poroelasticity problems play an important role in various engineering,\ngeophysical, and biological applications. Their full discretization results in\na large-scale saddle-point system at each time step that is becoming singular\nfor locking cases and needs effective preconditioners for its fast iterative\nsolution. Instead of constructing spectrally equivalent ones, we develop\nnonsingular preconditioners so that the eigenvalues of the preconditioned\nsystem consist of a cluster around $1$ and an outlier in the order of\n$1/\\lambda$, where $\\lambda$ is a Lam\\'{e} constant that is large for locking\ncases. It is known that the convergence factor of GMRES is bounded by the\nradius of the cluster for this type of systems. Both two- and three-field block\ntriangular Schur complement preconditioners are studied. Upper bounds of the\nradius of the eigenvalue cluster for those systems are obtained and shown to be\nrelated to the inf-sup condition but independent of mesh size, time step, and\nlocking parameters, which reflects the robustness of the preconditioners with\nrespect to parameter variations. Moreover, the developed preconditioners do not\nneed to compute the Schur complement and neither require exact inversion of\ndiagonal blocks except the leading one. A locking-free weak Galerkin finite\nelement method and the implicit Euler scheme are used for the discretization of\nthe governing equation. Both two- and three-dimensional numerical results are\npresented to confirm the effectiveness and parameter-robustness of the\ndeveloped preconditioners.", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2506.21361v1", "categories": ["math.NA", "cs.NA", "65M60, 65F08, 65F10, 74F10"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21361v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "原公式中线性多孔弹性与弹性高效参数鲁棒预处理器", "tldr": "为解决多孔弹性问题离散化导致的大规模鞍点系统在锁定情况下的奇异性问题，本文开发了参数鲁棒的非奇异预处理器，其特征值分布有助于GMRES快速收敛，并通过数值实验验证了其有效性。", "motivation": "多孔弹性问题在工程、地球物理和生物应用中很重要，其完全离散化导致大规模鞍点系统，在锁定情况下会变得奇异，需要有效的预处理器进行快速迭代求解。", "method": "开发非奇异预处理器，使其预处理系统的特征值由围绕1的簇和数量级为1/λ的异常值组成。研究了二场和三场块三角Schur补预处理器。推导了特征值簇半径的上界，并证明其与inf-sup条件相关但与网格尺寸、时间步长和锁定参数无关。预处理器不需要计算Schur补，也不需要除首个块外的对角块的精确求逆。使用无锁定弱Galerkin有限元方法和隐式欧拉格式进行离散化。", "result": "预处理系统的特征值由围绕1的簇和数量级为1/λ的异常值组成，这有助于GMRES的收敛。获得了特征值簇半径的上界，并证明其与inf-sup条件相关但与网格尺寸、时间步长和锁定参数无关，这反映了预处理器对参数变化的鲁棒性。数值结果证实了所开发预处理器的有效性和参数鲁棒性。", "conclusion": "本文开发的高效参数鲁棒预处理器能有效解决多孔弹性问题离散化产生的大规模奇异鞍点系统，且对参数变化具有鲁棒性，为快速迭代求解提供了有效工具。", "translation": "多孔弹性问题在各种工程、地球物理和生物应用中扮演着重要角色。它们的完全离散化在每个时间步都会产生一个大规模的鞍点系统，该系统在锁定情况下会变得奇异，需要有效的预处理器来进行快速迭代求解。我们没有构建谱等价的预处理器，而是开发了非奇异预处理器，使得预处理系统的特征值由围绕1的簇和数量级为1/λ的异常值组成，其中λ是Lamé常数，在锁定情况下很大。已知GMRES的收敛因子受此类系统特征值簇半径的限制。本文研究了二场和三场块三角Schur补预处理器。获得了这些系统特征值簇半径的上界，并表明其与inf-sup条件相关，但独立于网格尺寸、时间步长和锁定参数，这反映了预处理器对参数变化的鲁棒性。此外，所开发的预处理器不需要计算Schur补，也不需要除首个块外的对角块的精确求逆。控制方程的离散化使用了无锁定弱Galerkin有限元方法和隐式欧拉格式。二维和三维数值结果都证实了所开发预处理器的有效性和参数鲁棒性。", "summary": "本文针对多孔弹性问题离散化后在锁定情况下出现的奇异大规模鞍点系统，提出了一种高效的参数鲁棒非奇异预处理器。该预处理器的设计使得预处理系统的特征值聚集在1附近，有助于GMRES方法的快速收敛。研究了二场和三场块三角Schur补预处理器，并证明其特征值簇半径的上界与网格尺寸、时间步长和锁定参数无关，体现了其对参数变化的鲁棒性。该方法无需计算Schur补，且除了首个块外，无需精确逆对角块。通过二维和三维数值实验验证了所开发预处理器的有效性和鲁棒性。", "keywords": "多孔弹性, 预处理器, 鞍点系统, 参数鲁棒性, GMRES", "comments": "该研究通过开发非奇异预处理器，巧妙地解决了多孔弹性问题在锁定情况下离散化所产生的奇异性难题。其创新点在于通过控制预处理系统特征值的分布来保证GMRES算法的快速收敛，并且实现了对网格尺寸、时间步长和锁定参数的鲁棒性，这对于实际工程应用具有重要意义。此外，预处理器无需精确求逆和计算Schur补的特性也提升了其计算效率和实用性。"}}
{"id": "2506.21524", "title": "Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding", "authors": ["Libn Varghese", "Bhaskar Chaudhury", "Miral Shah", "Mainak Bandyopadhyay"], "summary": "The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase\nspace information using particle and grid data structures. High computational\ncosts in 2D and 3D device-scale PIC simulations necessitate parallelization,\nwith the Charge Deposition (CD) subroutine often becoming a bottleneck due to\nfrequent particle-grid interactions. Conventional methods mitigate dependencies\nby generating private grids for each core, but this approach faces scalability\nissues. We propose a novel approach based on a particle-thread binding strategy\nthat requires only four private grids per node in distributed memory systems or\nfour private grids in shared memory systems, enhancing CD scalability and\nperformance while maintaining conventional data structures and requiring\nminimal changes to existing PIC codes. This method ensures complete\naccessibility of grid data structure for concurrent threads and avoids\nsimultaneous access to particles within the same cell using additional\nfunctions and flags. Performance evaluations using a PIC benchmark for\nlow-temperature partially magnetized E x B discharge simulation on a shared\nmemory as well as a distributed memory system (1000 cores) demonstrate the\nmethod's scalability, and additionally, we show the method has little hardware\ndependency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21524v1", "categories": ["physics.comp-ph", "cs.DC", "physics.plasm-ph"], "cate": "physics.comp-ph", "url": "http://arxiv.org/abs/2506.21524v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "低温等离子体模拟中静电粒子-网格法的基准测试与粒子-线程绑定并行化", "tldr": "本文提出了一种基于粒子-线程绑定策略的静电粒子-网格法（PIC）并行化新方法，该方法通过减少私有网格数量来解决传统方法中电荷沉积（CD）子程序的瓶颈和可扩展性问题，并在共享内存和分布式内存系统上验证了其可扩展性和低硬件依赖性。", "motivation": "粒子-网格法（PIC）在2D和3D设备级等离子体模拟中计算成本高昂，尤其电荷沉积（CD）子程序由于频繁的粒子-网格交互而成为并行化的瓶颈。传统并行化方法通过为每个核心生成私有网格来缓解依赖性，但这种方法面临可扩展性问题。", "method": "本文提出了一种基于粒子-线程绑定策略的新方法。该方法在分布式内存系统或共享内存系统中每个节点仅需要四个私有网格，从而增强了CD的可扩展性和性能。它保持了传统的粒子和网格数据结构，并对现有PIC代码的修改量极小。此外，该方法通过附加函数和标志确保并发线程对网格数据结构的完全可访问性，并避免了同一单元内粒子的同时访问。", "result": "在共享内存和分布式内存系统（1000个核心）上使用低温部分磁化E x B放电模拟的PIC基准测试进行了性能评估，结果表明该方法具有良好的可扩展性，并且硬件依赖性很小。", "conclusion": "所提出的粒子-线程绑定并行化方法有效解决了传统PIC模拟中电荷沉积子程序的可扩展性瓶颈，通过减少私有网格数量，在保持传统数据结构和最小化代码修改的同时，显著提升了性能，并展现出良好的可扩展性和低硬件依赖性。", "translation": "粒子-网格（PIC）方法用于等离子体模拟，利用粒子和网格数据结构跟踪粒子相空间信息。2D和3D设备级PIC模拟的高计算成本使得并行化成为必要，其中电荷沉积（CD）子程序由于频繁的粒子-网格交互而常常成为瓶颈。传统方法通过为每个核心生成私有网格来缓解依赖性，但这种方法面临可扩展性问题。我们提出了一种基于粒子-线程绑定策略的新方法，在分布式内存系统中每个节点仅需要四个私有网格，或在共享内存系统中需要四个私有网格，从而增强了CD的可扩展性和性能，同时保持了传统数据结构并对现有PIC代码的修改量极小。该方法确保并发线程对网格数据结构的完全可访问性，并使用附加函数和标志避免了同一单元内粒子的同时访问。在共享内存以及分布式内存系统（1000个核心）上使用低温部分磁化E x B放电模拟的PIC基准测试进行了性能评估，结果表明该方法具有可扩展性，此外，我们还展示了该方法几乎没有硬件依赖性。", "summary": "本文针对低温等离子体模拟中粒子-网格法（PIC）的计算成本和电荷沉积（CD）子程序瓶颈，提出了一种新颖的粒子-线程绑定并行化策略。与传统为每个核心生成私有网格的方法不同，该方法仅需每个节点或系统四个私有网格，显著提升了CD的可扩展性和性能，同时保持了现有代码结构的兼容性。性能评估在共享和分布式内存系统上验证了其在千核规模下的可扩展性及低硬件依赖性。", "keywords": "粒子-网格法, 并行化, 粒子-线程绑定, 等离子体模拟, 电荷沉积", "comments": "这项研究提出了一种创新的粒子-线程绑定策略，有效解决了传统PIC并行化中电荷沉积子程序的可扩展性瓶颈。其创新之处在于大幅减少了所需的私有网格数量（仅四个），这对于分布式和共享内存系统都是一个显著的改进。该方法的重要性在于它能在最小化对现有PIC代码修改的同时，实现高性能和高可扩展性，这对于实际应用具有重要意义。此外，其低硬件依赖性也增强了方法的普适性。"}}
{"id": "2506.20818", "title": "Demystifying Distributed Training of Graph Neural Networks for Link Prediction", "authors": ["Xin Huang", "Chul-Ho Lee"], "summary": "Graph neural networks (GNNs) are powerful tools for solving graph-related\nproblems. Distributed GNN frameworks and systems enhance the scalability of\nGNNs and accelerate model training, yet most are optimized for node\nclassification. Their performance on link prediction remains underexplored.\nThis paper demystifies distributed training of GNNs for link prediction by\ninvestigating the issue of performance degradation when each worker trains a\nGNN on its assigned partitioned subgraph without having access to the entire\ngraph. We discover that the main sources of the issue come from not only the\ninformation loss caused by graph partitioning but also the ways of drawing\nnegative samples during model training. While sharing the complete graph\ninformation with each worker resolves the issue and preserves link prediction\naccuracy, it incurs a high communication cost. We propose SpLPG, which\neffectively leverages graph sparsification to mitigate the issue of performance\ndegradation at a reduced communication cost. Experiment results on several\npublic real-world datasets demonstrate the effectiveness of SpLPG, which\nreduces the communication overhead by up to about 80% while mostly preserving\nlink prediction accuracy.", "comment": "Accepted by IEEE ICDCS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20818v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20818v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "揭秘图神经网络在链接预测中的分布式训练", "tldr": "本文揭示了图神经网络在链接预测中分布式训练的性能下降问题，并提出了一种名为SpLPG的方法，通过图稀疏化有效降低通信成本，同时保持预测精度。", "motivation": "尽管分布式图神经网络（GNNs）框架和系统提高了GNN的扩展性和训练速度，但大多数都针对节点分类进行了优化，其在链接预测上的性能尚未得到充分探索。当每个工作节点在没有访问整个图的情况下训练其分配的子图时，链接预测的性能会下降，这是本研究的动机。", "method": "本文通过调查每个工作节点在没有访问整个图的情况下训练其分配的子图时性能下降的问题，揭示了图神经网络在链接预测中分布式训练的奥秘。研究发现，问题的主要来源不仅是图划分导致的信息丢失，还包括模型训练过程中负样本的抽取方式。为了解决这个问题，论文提出了SpLPG，该方法有效利用图稀疏化来缓解性能下降问题，并降低了通信成本。", "result": "在多个公共真实世界数据集上的实验结果表明，SpLPG是有效的，它将通信开销降低了约80%，同时大部分保持了链接预测的准确性。", "conclusion": "本文揭示了分布式图神经网络在链接预测中性能下降的原因，并提出了一种名为SpLPG的有效解决方案，通过图稀疏化显著降低了通信成本，同时保持了预测精度，解决了在分布式训练中访问完整图信息导致的通信开销过高的问题。", "translation": "图神经网络（GNNs）是解决图相关问题的强大工具。分布式GNN框架和系统增强了GNN的可扩展性并加速了模型训练，但大多数都针对节点分类进行了优化。它们在链接预测上的性能仍未得到充分探索。本文通过调查每个工作节点在没有访问整个图的情况下训练其分配的子图时性能下降的问题，揭示了图神经网络在链接预测中分布式训练的奥秘。我们发现，问题的主要来源不仅是图划分导致的信息丢失，还包括模型训练过程中负样本的抽取方式。虽然与每个工作节点共享完整的图信息可以解决问题并保持链接预测的准确性，但这会产生很高的通信成本。我们提出了SpLPG，它有效利用图稀疏化来缓解性能下降问题，并降低了通信成本。在多个公共真实世界数据集上的实验结果表明了SpLPG的有效性，它将通信开销降低了约80%，同时大部分保持了链接预测的准确性。", "summary": "本文研究了图神经网络在链接预测任务中进行分布式训练时遇到的性能下降问题。研究发现，性能下降主要源于图划分导致的信息丢失以及负样本抽取方式。虽然共享完整图信息可解决精度问题，但会带来高昂的通信成本。为此，论文提出了一种名为SpLPG的方法，通过图稀疏化有效缓解了性能下降，并显著降低了通信开销。实验证明，SpLPG在保持链接预测准确性的同时，能将通信开销降低高达80%。", "keywords": "图神经网络, 分布式训练, 链接预测, 图稀疏化, 性能下降", "comments": "这篇论文的创新点在于深入分析了分布式GNN在链接预测中性能下降的具体原因，并提出了一个实用的解决方案SpLPG。通过引入图稀疏化来优化分布式训练中的通信效率，SpLPG有效地平衡了性能与成本，对于推动GNN在实际大规模应用中的落地具有重要意义。"}}
{"id": "2506.20939", "title": "AIR-VIEW: The Aviation Image Repository for Visibility Estimation of Weather, A Dataset and Benchmark", "authors": ["Chad Mourning", "Zhewei Wang", "Justin Murray"], "summary": "Machine Learning for aviation weather is a growing area of research for\nproviding low-cost alternatives for traditional, expensive weather sensors;\nhowever, in the area of atmospheric visibility estimation, publicly available\ndatasets, tagged with visibility estimates, of distances relevant for aviation,\nof diverse locations, of sufficient size for use in supervised learning, are\nabsent. This paper introduces a new dataset which represents the culmination of\na year-long data collection campaign of images from the FAA weather camera\nnetwork suitable for this purpose. We also present a benchmark when applying\nthree commonly used approaches and a general-purpose baseline when trained and\ntested on three publicly available datasets, in addition to our own, when\ncompared against a recently ratified ASTM standard.", "comment": "5 pages, meant as citation for dataset", "pdf_url": "http://arxiv.org/pdf/2506.20939v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20939v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "AIR-VIEW：用于天气能见度估计的航空图像库，一个数据集和基准", "tldr": "本文介绍了AIR-VIEW，一个用于航空天气能见度估计的新型大型数据集和基准，旨在解决现有公开数据集的不足。", "motivation": "为航空天气提供低成本的机器学习替代方案是研究热点，但在大气能见度估计领域，缺乏适用于监督学习的、足够大、多样化且标注了能见度估计的公开数据集。", "method": "本文通过为期一年的数据收集活动，从FAA天气摄像头网络中收集图像，构建了AIR-VIEW数据集。同时，在AIR-VIEW和三个公开数据集上，应用了三种常用方法和一个通用基线模型进行训练和测试，并与近期批准的ASTM标准进行了比较，建立了基准。", "result": "本文成功引入了一个新的航空图像数据集AIR-VIEW，该数据集是为期一年的数据收集活动的成果，旨在用于天气能见度估计。此外，还提供了一个基准，该基准在AIR-VIEW和三个公开数据集上，应用了三种常用方法和一个通用基线，并与ASTM标准进行了比较。", "conclusion": "本文通过引入AIR-VIEW数据集和相关基准，为航空天气能见度估计的机器学习研究提供了急需的公共资源和评估框架，有助于推动该领域低成本解决方案的发展。", "translation": "机器学习在航空天气领域是一个不断发展的研究方向，旨在为传统的昂贵天气传感器提供低成本替代方案；然而，在大气能见度估计领域，缺乏适用于监督学习的、标注了航空相关距离能见度估计的、地点多样化且规模足够的公开数据集。本文介绍了一个新数据集，该数据集代表了为期一年的FAA天气摄像头网络图像数据收集活动的成果，适用于此目的。我们还提出了一个基准，该基准在训练和测试时，除了我们自己的数据集外，还在三个公开数据集上应用了三种常用方法和一个通用基线，并与近期批准的ASTM标准进行了比较。", "summary": "本文针对航空大气能见度估计领域缺乏适用于机器学习的公开数据集的问题，引入了AIR-VIEW数据集。该数据集通过一年期的FAA天气摄像头网络图像收集而成，旨在满足监督学习的需求。此外，论文还提出了一个基准，利用AIR-VIEW和三个现有公开数据集，评估了三种常用机器学习方法和一个通用基线，并将结果与近期批准的ASTM标准进行了比较，为该领域提供了重要的资源和评估工具。", "keywords": "航空天气, 能见度估计, 数据集, 机器学习, 基准", "comments": "该论文通过创建并公开一个大规模、高质量的航空图像数据集AIR-VIEW，显著解决了航空天气能见度估计领域长期存在的数据稀缺问题，这是推动该领域机器学习应用的关键一步。同时，提供的基准有助于标准化模型评估，促进后续研究的进展和比较。这项工作对于开发低成本的替代方案，以取代昂贵的传统天气传感器具有重要意义。"}}
{"id": "2506.21508", "title": "skLEP: A Slovak General Language Understanding Benchmark", "authors": ["Marek Šuppa", "Andrej Ridzik", "Daniel Hládek", "Tomáš Javůrek", "Viktória Ondrejová", "Kristína Sásiková", "Martin Tamajka", "Marián Šimko"], "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21508v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21508v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "skLEP：一个斯洛伐克通用语言理解基准", "tldr": "引入了skLEP，首个斯洛伐克NLU综合基准，包含9项任务，并评估了多种预训练模型，同时发布了数据和工具以促进研究。", "motivation": "现有NLU模型缺乏专门针对斯洛伐克语的综合评估基准。", "method": "构建了skLEP，包含9项涵盖词级别、句子对级别和文档级别的多样化任务。策划了新的斯洛伐克语原创数据集，并精心翻译了已有的英语NLU资源。使用skLEP任务系统地评估了多种斯洛伐克语特有、多语言和英语预训练语言模型。发布了完整的基准数据、开源工具包和公共排行榜。", "result": "首次对多种斯洛伐克语特有、多语言和英语预训练语言模型进行了系统和广泛的评估。", "conclusion": "skLEP作为首个斯洛伐克NLU综合基准，通过提供多样化的任务、数据集、评估结果和开源资源，旨在促进斯洛伐克语NLU领域的可复现性并推动未来的研究。", "translation": "在这项工作中，我们介绍了skLEP，这是第一个专门为评估斯洛伐克自然语言理解（NLU）模型而设计的综合基准。我们编译了skLEP，使其包含九项涵盖词级别、句子对级别和文档级别挑战的多样化任务，从而提供了对模型能力的全面评估。为了创建这个基准，我们策划了针对斯洛伐克语量身定制的全新原创数据集，并精心翻译了已有的英语NLU资源。在本文中，我们还首次使用skLEP任务对各种斯洛伐克语特有、多语言和英语预训练语言模型进行了系统而广泛的评估。最后，我们还发布了完整的基准数据、一个便于模型微调和评估的开源工具包，以及一个公共排行榜（网址：https://github.com/slovak-nlp/sklep），希望以此促进斯洛伐克NLU领域的可复现性并推动未来的研究。", "summary": "本文推出了skLEP，这是首个专门用于评估斯洛伐克语自然语言理解（NLU）模型的综合基准。skLEP包含九项多样化的任务，涵盖词级别、句子对级别和文档级别挑战，并通过结合原创斯洛伐克语数据集和翻译的英语NLU资源构建。研究人员使用skLEP对斯洛伐克语特有、多语言和英语预训练语言模型进行了首次系统评估。为了促进斯洛伐克语NLU领域的可复现性和研究发展，作者还发布了完整的基准数据、开源工具包和公共排行榜。", "keywords": "斯洛伐克语NLU, 语言理解基准, skLEP, 预训练模型, 自然语言处理", "comments": "skLEP的创新之处在于它是首个针对斯洛伐克语的综合NLU基准，填补了该领域的一个空白。其重要性体现在它为斯洛伐克语NLU模型提供了一个标准化的评估框架，并通过发布数据和工具促进了社区研究。这对于低资源语言的NLP发展尤为关键。"}}
{"id": "2506.21098", "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "pdf_url": "http://arxiv.org/pdf/2506.21098v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21098v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ComRAG：用于工业实时社区问答的动态向量存储检索增强生成", "tldr": "ComRAG是一个新的检索增强生成框架，通过动态向量存储和基于质心的记忆机制，显著提高了工业实时社区问答的性能和效率。", "motivation": "现有社区问答（CQA）方法在实时利用历史交互和领域知识方面存在挑战，常未充分利用外部知识，未能整合动态历史QA上下文，或缺乏适用于工业部署的记忆机制。", "method": "本文提出ComRAG，一个用于实时工业CQA的检索增强生成框架。它通过一个为检索、生成和高效存储设计的基于质心的记忆机制，将静态知识与动态历史QA对集成。", "result": "在三个工业CQA数据集上，ComRAG持续优于所有基线，向量相似度提高高达25.9%，延迟降低8.7%至23.3%，迭代过程中块增长从20.23%降低到2.06%。", "conclusion": "ComRAG显著提升了工业实时CQA的性能和效率，解决了现有方法的局限性，特别是在动态知识利用和存储优化方面。", "translation": "社区问答 (CQA) 平台可被视为社区中重要的知识库，但如何有效利用历史交互和领域知识仍是一个挑战。现有方法通常未充分利用外部知识，未能整合动态历史问答上下文，或缺乏适用于工业部署的记忆机制。我们提出了 ComRAG，一个用于实时工业 CQA 的检索增强生成框架，它通过一个为检索、生成和高效存储设计的基于质心的记忆机制，将静态知识与动态历史问答对集成。在三个工业 CQA 数据集上的评估显示，ComRAG 持续优于所有基线——向量相似度提高高达 25.9%，延迟降低 8.7% 至 23.3%，并且在迭代过程中块增长从 20.23% 降低到 2.06%。", "summary": "本文提出了ComRAG，一个针对工业实时社区问答（CQA）的检索增强生成框架。该框架通过创新的基于质心的记忆机制，有效整合静态知识和动态历史问答对，以克服现有CQA方法在利用外部知识和动态上下文方面的不足。实验结果表明，ComRAG在向量相似度、延迟和存储效率方面均显著优于现有基线，为工业CQA提供了高效且高性能的解决方案。", "keywords": "社区问答, 检索增强生成, 动态向量存储, 实时CQA, 质心记忆机制", "comments": "ComRAG的创新之处在于其动态向量存储和基于质心的记忆机制，这使其能够有效处理实时变化的社区问答数据，并显著提升了工业应用中的效率和性能。它解决了现有RAG系统在动态知识更新和存储效率方面的挑战。"}}
{"id": "2506.21395", "title": "Optimal solutions employing an algebraic Variational Multiscale approach Part II: Application to Navier-Stokes", "authors": ["Suyash Shrestha", "Marc Gerritsma", "Gonzalo Rubio", "Steven Hulshoff", "Esteban Ferrer"], "summary": "This work presents a nonlinear extension of the high-order discretisation\nframework based on the Variational Multiscale (VMS) method previously\nintroduced for steady linear problems. Building on the concept of an optimal\nprojector defined via the symmetric part of the governing operator, we\ngeneralise the formulation to treat the 2D incompressible Navier-Stokes\nequations. The arroach maintains a clear separation between the resolved and\nunresolved scales, with the fine-scale contributions approximated through the\napproximate Fine-Scale Greens' function of the associated symmetric operator.\nThis enables a consistent variational treatment of the nonlinearity while\npreserving high-order accuracy. We show that the method yields numerical\nsolutions that closely approximate the optimal projection of the\ncontinuous/highly resolved solution and inherits desirable conservation\nproperties. Numerical results confirm the framework's robustness, accuracy, and\nits potential for application to a broad class of nonlinear multiscale\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21395v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21395v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "采用代数变分多尺度方法的优化解 第二部分：在Navier-Stokes方程中的应用", "tldr": "该工作将变分多尺度（VMS）方法扩展到非线性问题，特别是2D不可压缩Navier-Stokes方程，通过最优投影和格林函数近似实现了高阶精度、守恒性，并展示了其鲁棒性和准确性。", "motivation": "该研究旨在将先前针对稳态线性问题引入的基于变分多尺度（VMS）方法的高阶离散化框架，扩展到非线性问题，特别是处理2D不可压缩Navier-Stokes方程。", "method": "该方法是变分多尺度（VMS）方法的非线性扩展，通过定义在控制算子对称部分上的最优投影器来泛化公式。它通过相关对称算子的近似细尺度格林函数来近似细尺度贡献，从而在保持高阶精度的同时，对非线性进行一致的变分处理。", "result": "该方法产生的数值解能够紧密近似连续/高分辨率解的最优投影，并继承了理想的守恒特性。数值结果证实了该框架的鲁棒性、准确性及其在广泛非线性多尺度问题中的应用潜力。", "conclusion": "该框架对于非线性多尺度问题具有鲁棒性、准确性，并具有广泛的应用潜力。", "translation": "这项工作提出了对先前针对稳态线性问题引入的基于变分多尺度（VMS）方法的高阶离散化框架的非线性扩展。在通过控制算子对称部分定义的最优投影概念的基础上，我们将公式推广到处理2D不可压缩Navier-Stokes方程。该方法在已解析尺度和未解析尺度之间保持清晰的分离，其中细尺度贡献通过相关对称算子的近似细尺度格林函数进行近似。这使得非线性能够进行一致的变分处理，同时保持高阶精度。我们表明，该方法产生的数值解能够紧密近似连续/高分辨率解的最优投影，并继承了理想的守恒特性。数值结果证实了该框架的鲁棒性、准确性及其在广泛非线性多尺度问题中的应用潜力。", "summary": "本研究将变分多尺度（VMS）方法从线性稳态问题推广到非线性问题，并应用于2D不可压缩Navier-Stokes方程。该方法利用最优投影器和近似细尺度格林函数，实现了已解析和未解析尺度的清晰分离，并能一致处理非线性，同时保持高阶精度。数值结果表明，该方法能产生接近最优投影的解，具有良好的守恒性、鲁棒性和准确性，适用于多种非线性多尺度问题。", "keywords": "变分多尺度方法, Navier-Stokes方程, 非线性问题, 最优投影, 数值模拟", "comments": "该论文的创新点在于将高阶变分多尺度（VMS）方法成功推广到非线性问题，特别是Navier-Stokes方程。通过引入最优投影器和细尺度格林函数近似，该方法在保持高阶精度的同时，有效处理了非线性，并展现出良好的守恒性。这对于解决复杂的非线性多尺度问题具有重要意义。"}}
{"id": "2506.21347", "title": "Real-time Terrain Analysis for Off-road Autonomous Vehicles", "authors": ["Edwina Lewis", "Aditya Parameshwaran", "Laura Redmond", "Yue Wang"], "summary": "This research addresses critical autonomous vehicle control challenges\narising from road roughness variation, which induces course deviations and\npotential loss of road contact during steering operations. We present a novel\nreal-time road roughness estimation system employing Bayesian calibration\nmethodology that processes axle accelerations to predict terrain roughness with\nquantifiable confidence measures. The technical framework integrates a Gaussian\nprocess surrogate model with a simulated half-vehicle model, systematically\nprocessing vehicle velocity and road surface roughness parameters to generate\ncorresponding axle acceleration responses. The Bayesian calibration routine\nperforms inverse estimation of road roughness from observed accelerations and\nvelocities, yielding posterior distributions that quantify prediction\nuncertainty for adaptive risk management. Training data generation utilizes\nLatin Hypercube sampling across comprehensive velocity and roughness parameter\nspaces, while the calibrated model integrates seamlessly with a Simplex\ncontroller architecture to dynamically adjust velocity limits based on\nreal-time roughness predictions. Experimental validation on stochastically\ngenerated surfaces featuring varying roughness regions demonstrates robust\nreal-time characterization capabilities, with the integrated Simplex control\nstrategy effectively enhancing autonomous vehicle operational safety through\nproactive surface condition response. This innovative Bayesian framework\nestablishes a comprehensive foundation for mitigating roughness-related\noperational risks while simultaneously improving efficiency and safety margins\nin autonomous vehicle systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21347v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21347v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "越野自动驾驶汽车的实时地形分析", "tldr": "本研究提出了一种新颖的实时路面粗糙度估计系统，利用贝叶斯校准方法处理车轴加速度，以提高自动驾驶车辆在崎岖路面上的安全性和效率。", "motivation": "本研究旨在解决自动驾驶车辆在路面粗糙度变化时面临的控制挑战，这些变化可能导致转向操作期间的行驶偏差和潜在的失控。", "method": "该研究提出了一个实时路面粗糙度估计系统，采用贝叶斯校准方法处理车轴加速度来预测地形粗糙度。技术框架集成了高斯过程替代模型和模拟半车辆模型，系统地处理车辆速度和路面粗糙度参数以生成车轴加速度响应。贝叶斯校准程序通过观测到的加速度和速度逆向估计路面粗糙度，生成量化预测不确定性的后验分布。训练数据通过拉丁超立方采样生成，校准后的模型与Simplex控制器架构无缝集成，根据实时粗糙度预测动态调整速度限制。", "result": "在随机生成的不同粗糙度区域的路面上进行的实验验证表明，该系统具有强大的实时表征能力。集成Simplex控制策略通过主动响应路面状况，有效增强了自动驾驶车辆的运行安全性。", "conclusion": "该创新的贝叶斯框架为减轻与粗糙度相关的操作风险奠定了全面基础，同时提高了自动驾驶系统的效率和安全裕度。", "translation": "本研究旨在解决路面粗糙度变化引起的自动驾驶车辆关键控制挑战，这些变化会导致转向操作期间的行驶偏差和潜在的失控。我们提出了一种新颖的实时路面粗糙度估计系统，该系统采用贝叶斯校准方法，处理车轴加速度以可量化的置信度预测地形粗糙度。该技术框架将高斯过程替代模型与模拟半车辆模型相结合，系统地处理车辆速度和路面粗糙度参数以生成相应的车轴加速度响应。贝叶斯校准程序根据观测到的加速度和速度进行路面粗糙度的逆向估计，生成量化预测不确定性的后验分布，用于自适应风险管理。训练数据的生成利用拉丁超立方采样在全面的速度和粗糙度参数空间中进行，而校准后的模型与Simplex控制器架构无缝集成，根据实时粗糙度预测动态调整速度限制。在具有不同粗糙度区域的随机生成路面上进行的实验验证表明，该系统具有鲁棒的实时表征能力，集成Simplex控制策略通过主动响应路面状况，有效增强了自动驾驶车辆的运行安全性。这种创新的贝叶斯框架为减轻与粗糙度相关的操作风险奠定了全面基础，同时提高了自动驾驶系统中的效率和安全裕度。", "summary": "本研究提出了一种用于越野自动驾驶车辆的实时路面粗糙度估计系统，以解决因路面粗糙度变化导致的控制挑战。该系统采用贝叶斯校准方法，结合高斯过程模型和半车辆模型，通过分析车轴加速度来预测地形粗糙度并量化不确定性。实验证明，该方法能有效增强自动驾驶车辆在崎岖路面上的操作安全性和效率。", "keywords": "实时地形分析, 自动驾驶车辆, 路面粗糙度, 贝叶斯校准, Simplex控制器", "comments": "该论文的创新点在于提出了一个将贝叶斯校准与高斯过程和半车辆模型相结合的实时路面粗糙度估计框架。这种方法不仅能够预测粗糙度，还能量化预测的不确定性，这对于自动驾驶车辆的风险管理至关重要。与Simplex控制器的集成展示了其实用性，为提高越野自动驾驶车辆的安全性提供了坚实的基础。"}}
{"id": "2506.20849", "title": "Learning-Based Resource Management in Integrated Sensing and Communication Systems", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "In this paper, we tackle the task of adaptive time allocation in integrated\nsensing and communication systems equipped with radar and communication units.\nThe dual-functional radar-communication system's task involves allocating dwell\ntimes for tracking multiple targets and utilizing the remaining time for data\ntransmission towards estimated target locations. We introduce a novel\nconstrained deep reinforcement learning (CDRL) approach, designed to optimize\nresource allocation between tracking and communication under time budget\nconstraints, thereby enhancing target communication quality. Our numerical\nresults demonstrate the efficiency of our proposed CDRL framework, confirming\nits ability to maximize communication quality in highly dynamic environments\nwhile adhering to time constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20849v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20849v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "学习型综合传感与通信系统资源管理", "tldr": "本文提出了一种基于约束深度强化学习(CDRL)的方法，用于优化集成传感与通信系统中跟踪和通信之间的时间分配，以提高通信质量。", "motivation": "在配备雷达和通信单元的集成传感与通信系统中，需要解决自适应时间分配任务，即在跟踪多个目标和向估计目标位置传输数据之间进行时间分配，以提高目标通信质量。", "method": "引入了一种新颖的约束深度强化学习（CDRL）方法，旨在优化在时间预算约束下跟踪和通信之间的资源分配。", "result": "数值结果表明，所提出的CDRL框架是高效的，并证实了其在高度动态环境中遵守时间约束的同时最大化通信质量的能力。", "conclusion": "CDRL方法能够有效地优化集成传感与通信系统中的资源分配，从而在动态环境中提高通信质量。", "translation": "本文旨在解决配备雷达和通信单元的集成传感与通信系统中自适应时间分配的任务。双功能雷达通信系统的任务涉及为跟踪多个目标分配驻留时间，并利用剩余时间向估计的目标位置传输数据。我们引入了一种新颖的约束深度强化学习（CDRL）方法，旨在优化在时间预算约束下跟踪和通信之间的资源分配，从而提高目标通信质量。我们的数值结果证明了我们提出的CDRL框架的效率，证实了其在高度动态环境中遵守时间约束的同时最大化通信质量的能力。", "summary": "本文研究了集成传感与通信系统中的自适应时间分配问题，该系统需在目标跟踪和数据传输之间进行资源分配。为解决此问题，作者提出了一种新颖的约束深度强化学习（CDRL）方法，旨在优化时间预算约束下的资源分配，以提高通信质量。数值结果验证了CDRL框架的有效性，表明其能在动态环境下最大化通信质量并遵守时间限制。", "keywords": "集成传感与通信, 资源管理, 深度强化学习, 时间分配", "comments": "这篇论文的创新点在于将约束深度强化学习应用于集成传感与通信系统中的资源管理，解决了在有限时间预算下同时优化目标跟踪和数据传输的复杂问题。这种方法对于提高未来通信系统的效率和性能具有重要意义，尤其是在动态和资源受限的环境中。"}}
{"id": "2506.20947", "title": "Hierarchical Sub-action Tree for Continuous Sign Language Recognition", "authors": ["Dejie Yang", "Zhu Xu", "Xinjie Gao", "Yang Liu"], "summary": "Continuous sign language recognition (CSLR) aims to transcribe untrimmed\nvideos into glosses, which are typically textual words. Recent studies indicate\nthat the lack of large datasets and precise annotations has become a bottleneck\nfor CSLR due to insufficient training data. To address this, some works have\ndeveloped cross-modal solutions to align visual and textual modalities.\nHowever, they typically extract textual features from glosses without fully\nutilizing their knowledge. In this paper, we propose the Hierarchical\nSub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge\nwith visual representation learning. By incorporating gloss-specific knowledge\nfrom large language models, our approach leverages textual information more\neffectively. Specifically, we construct an HST for textual information\nrepresentation, aligning visual and textual modalities step-by-step and\nbenefiting from the tree structure to reduce computational complexity.\nAdditionally, we impose a contrastive alignment enhancement to bridge the gap\nbetween the two modalities. Experiments on four datasets (PHOENIX-2014,\nPHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the\neffectiveness of our HST-CSLR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20947v1", "categories": ["cs.CV", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20947v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "连续手语识别的分层子动作树", "tldr": "提出分层子动作树(HST-CSLR)模型，结合大语言模型中的手语词汇知识，通过分层对齐和对比增强，有效提升连续手语识别性能。", "motivation": "连续手语识别(CSLR)面临大数据集和精确标注不足的瓶颈，现有跨模态方案未能充分利用手语词汇的知识。", "method": "提出分层子动作树(HST)，称为HST-CSLR，旨在有效地将手语词汇知识与视觉表示学习相结合。通过整合来自大型语言模型的手语词汇特有知识，构建HST进行文本信息表示，并逐步对齐视觉和文本模态，同时利用树结构降低计算复杂度。此外，还引入对比对齐增强来弥合两种模态之间的差距。", "result": "在PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture四个数据集上的实验证明了HST-CSLR的有效性。", "conclusion": "本文提出的HST-CSLR模型，通过有效结合大语言模型中的手语词汇知识和分层对齐策略，显著提升了连续手语识别的性能，并解决了数据不足和知识利用不充分的问题。", "translation": "连续手语识别（CSLR）旨在将未剪辑的视频转录为通常是文本词汇的手语词（glosses）。最近的研究表明，由于训练数据不足，大型数据集和精确标注的缺乏已成为CSLR的瓶颈。为了解决这个问题，一些工作开发了跨模态解决方案来对齐视觉和文本模态。然而，它们通常从手语词汇中提取文本特征，但没有充分利用其知识。在本文中，我们提出了分层子动作树（Hierarchical Sub-action Tree，HST），称为HST-CSLR，以有效地将手语词汇知识与视觉表示学习结合起来。通过整合来自大型语言模型的手语词汇特有知识，我们的方法更有效地利用了文本信息。具体来说，我们构建了一个HST用于文本信息表示，逐步对齐视觉和文本模态，并受益于树结构以降低计算复杂度。此外，我们施加了对比对齐增强以弥合两种模态之间的差距。在四个数据集（PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture）上的实验证明了我们的HST-CSLR的有效性。", "summary": "本文针对连续手语识别(CSLR)中数据集不足和手语词汇知识利用不充分的问题，提出了一种名为分层子动作树(HST-CSLR)的新方法。该方法通过整合来自大语言模型的手语词汇特有知识，构建HST进行文本表示，并逐步对齐视觉与文本模态，同时利用树结构降低计算复杂度。此外，引入对比对齐增强以弥合模态间隙。实验结果表明，HST-CSLR在多个数据集上表现出有效性。", "keywords": "连续手语识别, 分层子动作树, 手语词汇知识, 跨模态对齐, 大语言模型", "comments": "本文的创新点在于提出了分层子动作树(HST)结构，并首次有效地将大型语言模型中的手语词汇知识引入到连续手语识别中，从而克服了传统方法对词汇知识利用不足的缺陷。树结构的应用也有效地降低了计算复杂度，提升了效率。"}}
{"id": "2506.20853", "title": "Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management", "authors": ["Ziyang Lu", "Subodh Kalia", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "The time allocation problem in multi-function cognitive radar systems focuses\non the trade-off between scanning for newly emerging targets and tracking the\npreviously detected targets. We formulate this as a multi-objective\noptimization problem and employ deep reinforcement learning to find\nPareto-optimal solutions and compare deep deterministic policy gradient (DDPG)\nand soft actor-critic (SAC) algorithms. Our results demonstrate the\neffectiveness of both algorithms in adapting to various scenarios, with SAC\nshowing improved stability and sample efficiency compared to DDPG. We further\nemploy the NSGA-II algorithm to estimate an upper bound on the Pareto front of\nthe considered problem. This work contributes to the development of more\nefficient and adaptive cognitive radar systems capable of balancing multiple\ncompeting objectives in dynamic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20853v1", "categories": ["cs.LG", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20853v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "认知雷达资源管理中的多目标强化学习", "tldr": "本文利用多目标深度强化学习解决认知雷达的时间分配问题，通过比较DDPG和SAC算法，发现SAC在稳定性和样本效率上表现更优。", "motivation": "多功能认知雷达系统中的时间分配问题需要在扫描新出现的目标和跟踪已检测到的目标之间进行权衡。", "method": "将问题表述为多目标优化问题；采用深度强化学习（DDPG和SAC）寻找帕累托最优解；并使用NSGA-II算法估计帕累托前沿的上限。", "result": "DDPG和SAC两种算法在适应不同场景方面均有效；SAC在稳定性和样本效率方面优于DDPG。", "conclusion": "这项工作有助于开发更高效、更自适应的认知雷达系统，能够在动态环境中平衡多个相互竞争的目标。", "translation": "多功能认知雷达系统中的时间分配问题侧重于扫描新出现的目标和跟踪先前检测到的目标之间的权衡。我们将此问题表述为多目标优化问题，并采用深度强化学习来寻找帕累托最优解，比较了深度确定性策略梯度（DDPG）和软行动者-评论家（SAC）算法。我们的结果表明，这两种算法在适应各种场景方面均有效，其中SAC与DDPG相比，显示出更高的稳定性和样本效率。我们进一步采用NSGA-II算法来估计所考虑问题的帕累托前沿的上限。这项工作有助于开发更高效、更自适应的认知雷达系统，能够在动态环境中平衡动态环境中的多个竞争目标。", "summary": "本文解决了认知雷达中的多目标时间分配问题，平衡了新目标扫描和现有目标跟踪。它将此问题表述为多目标优化问题，并使用深度强化学习（DDPG和SAC）来寻找帕累托最优解。结果表明，两种算法均有效，其中SAC在稳定性和样本效率方面优于DDPG。文中还使用NSGA-II估计帕累托前沿的上限，有助于开发更具适应性的雷达系统。", "keywords": "多目标强化学习, 认知雷达, 资源管理, 时间分配, 帕累托最优", "comments": "本文创新性地将多目标深度强化学习应用于关键的雷达资源管理问题，特别是时间分配。DDPG和SAC的比较提供了实际见解，SAC表现出明显的优势。使用NSGA-II提供帕累托前沿的上限也是一个很好的基准。这种方法可以显著提高未来认知雷达系统在复杂动态环境中的适应性和效率。"}}
{"id": "2506.21538", "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval", "authors": ["Hani Alomari", "Anushka Sivakumar", "Andrew Zhang", "Chris Thomas"], "summary": "Cross-modal image-text retrieval is challenging because of the diverse\npossible associations between content from different modalities. Traditional\nmethods learn a single-vector embedding to represent semantics of each sample,\nbut struggle to capture nuanced and diverse relationships that can exist across\nmodalities. Set-based approaches, which represent each sample with multiple\nembeddings, offer a promising alternative, as they can capture richer and more\ndiverse relationships. In this paper, we show that, despite their promise,\nthese set-based representations continue to face issues including sparse\nsupervision and set collapse, which limits their effectiveness. To address\nthese challenges, we propose Maximal Pair Assignment Similarity to optimize\none-to-one matching between embedding sets which preserve semantic diversity\nwithin the set. We also introduce two loss functions to further enhance the\nrepresentations: Global Discriminative Loss to enhance distinction among\nembeddings, and Intra-Set Divergence Loss to prevent collapse within each set.\nOur method achieves state-of-the-art performance on MS-COCO and Flickr30k\nwithout relying on external data.", "comment": "Accepted at the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025 Main)", "pdf_url": "http://arxiv.org/pdf/2506.21538v1", "categories": ["cs.CV", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21538v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "最大匹配至关重要：防止表示坍塌以实现鲁棒的跨模态检索", "tldr": "该论文提出了一种新的方法，通过最大对分配相似性和两种损失函数来防止基于集合的跨模态检索中的表示坍塌，并在MS-COCO和Flickr30k数据集上取得了最先进的性能。", "motivation": "传统的单向量嵌入难以捕捉跨模态间细致和多样化的关系。尽管基于集合的方法很有前景，但它们面临稀疏监督和集合坍塌等问题，限制了其有效性。", "method": "提出了“最大对分配相似性”来优化嵌入集合之间的一对一匹配，以保持集合内的语义多样性。引入了两种损失函数：全局判别损失（Global Discriminative Loss）以增强嵌入之间的区别，以及集合内散度损失（Intra-Set Divergence Loss）以防止集合内部的坍塌。", "result": "该方法在MS-COCO和Flickr30k数据集上取得了最先进的性能，且不依赖外部数据。", "conclusion": "通过引入最大对分配相似性以及全局判别损失和集合内散度损失，该方法成功解决了基于集合的跨模态检索中表示坍塌的问题，并显著提升了检索性能。", "translation": "跨模态图像-文本检索具有挑战性，因为不同模态内容之间可能存在多种关联。传统方法学习单一向量嵌入来表示每个样本的语义，但难以捕捉跨模态间细致和多样化的关系。基于集合的方法将每个样本表示为多个嵌入，提供了一种有前景的替代方案，因为它们可以捕捉更丰富、更多样的关系。在本文中，我们表明，尽管基于集合的表示很有前景，但它们仍然面临稀疏监督和集合坍塌等问题，这限制了它们的有效性。为了解决这些挑战，我们提出了最大对分配相似性（Maximal Pair Assignment Similarity）来优化嵌入集合之间的一对一匹配，从而保持集合内的语义多样性。我们还引入了两种损失函数来进一步增强表示：全局判别损失（Global Discriminative Loss）以增强嵌入之间的区别，以及集合内散度损失（Intra-Set Divergence Loss）以防止每个集合内部的坍塌。我们的方法在MS-COCO和Flickr30k数据集上取得了最先进的性能，且不依赖外部数据。", "summary": "该论文通过引入最大对分配相似性来优化嵌入集合之间的一对一匹配，并提出两种新的损失函数（全局判别损失和集合内散度损失），解决了基于集合的跨模态检索中表示坍塌和稀疏监督的问题。这些创新旨在保持和增强语义多样性，最终在MS-COCO和Flickr30k数据集上实现了最先进的性能。", "keywords": "跨模态检索, 表示坍塌, 基于集合的嵌入, 最大匹配, 损失函数", "comments": "该论文的创新点在于针对基于集合的跨模态表示中特有的稀疏监督和表示坍塌问题，提出了新颖的解决方案。通过引入最大对分配相似性以及专门设计的两种损失函数，它有效地提升了跨模态检索的鲁棒性和精确性，对该领域具有重要意义。"}}
{"id": "2506.21119", "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios.", "comment": "Accepted by ICONIP 2024", "pdf_url": "http://arxiv.org/pdf/2506.21119v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21119v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Progtuning：面向Transformer语言模型的渐进式微调框架", "tldr": "Progtuning是一种新的微调框架，通过渐进式地减少更新的Transformer块数量，优化资源分配并减少参数更新，同时保持性能。", "motivation": "现有微调和大多数参数高效微调方法在更新参数时忽略了Transformer块间贡献不均，导致计算资源分配效率低下，且随着模型增大，更新所有参数成本高昂。", "method": "本文提出了Progtuning，一个结合渐进式学习的微调框架。Progtuning根据Transformer块的贡献，渐进式地减少需要更新的Transformer块数量。", "result": "Progtuning优化了资源分配，减少了约25%的更新参数，同时保持了有竞争力的性能。它还与参数高效微调方法表现出高适应性，在各种适应场景下表现出色。", "conclusion": "Progtuning通过渐进式地选择性更新Transformer块，有效解决了大规模语言模型微调中的资源分配效率问题，并在减少参数的同时保持了性能。", "translation": "微调是利用基于Transformer的语言模型进行下游任务的一种有前景的技术。随着模型规模的不断增长，更新所有模型参数变得越来越昂贵。参数高效微调方法通过选择性地更新一小部分参数有效地解决了这个问题。然而，微调和大多数现有参数高效微调方法需要更新与初始规模相同数量的参数，忽略了Transformer块之间不平等的贡献，导致计算资源分配极其低效。在本文中，我们提出了Progtuning，这是一种结合渐进式学习的、用于基于Transformer的语言模型的新型微调框架。具体来说，Progtuning根据贡献渐进式地减少更新的Transformer块数量。值得注意的是，Progtuning优化了资源分配，并减少了大约25%的更新参数，同时仍保持了有竞争力的性能。它还与参数高效微调方法表现出高度适应性，在各种适应场景中展示了出色的性能。", "summary": "本文提出Progtuning，一种针对Transformer语言模型的渐进式微调框架。该方法通过根据贡献度渐进式地减少更新的Transformer块数量，解决了传统微调和参数高效微调方法中资源分配低效的问题。实验结果表明，Progtuning在减少约25%更新参数的同时，仍能保持竞争性性能，并与现有参数高效微调方法具有良好的兼容性。", "keywords": "Progtuning, 微调, Transformer, 语言模型, 参数高效", "comments": "Progtuning的创新之处在于引入了渐进式学习的理念，根据Transformer块的贡献度动态调整更新参数的数量，解决了现有微调方法资源分配不均的问题。这种方法在保持性能的同时显著降低了计算成本，对于大型语言模型的实际应用具有重要意义。其与现有参数高效微调方法的兼容性也增强了其普适性。"}}
{"id": "2506.21405", "title": "An adaptive dynamical low-rank optimizer for solving kinetic parameter identification inverse problems", "authors": ["Lena Baumann", "Lukas Einkemmer", "Christian Klingenberg", "Jonas Kusch"], "summary": "The numerical solution of parameter identification inverse problems for\nkinetic equations can exhibit high computational and memory costs. In this\npaper, we propose a dynamical low-rank scheme for the reconstruction of the\nscattering parameter in the radiative transfer equation from a number of\nmacroscopic time-independent measurements. We first work through the PDE\nconstrained optimization procedure in a continuous setting and derive the\nadjoint equations using a Lagrangian reformulation. For the scattering\ncoefficient, a periodic B-spline approximation is introduced and a gradient\ndescent step for updating its coefficients is formulated. After the\ndiscretization, a dynamical low-rank approximation (DLRA) is applied. We make\nuse of the rank-adaptive basis update & Galerkin integrator and a line search\napproach for the adaptive refinement of the gradient descent step size and the\nDLRA tolerance. We show that the proposed scheme significantly reduces both\nmemory and computational cost. Numerical results computed with different\ninitial conditions validate the accuracy and efficiency of the proposed DLRA\nscheme compared to solutions computed with a full solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21405v1", "categories": ["math.NA", "cs.NA", "35Q49, 49M41, 65M22, 65M32"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21405v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种用于求解动力学参数识别逆问题的自适应动态低秩优化器", "tldr": "本文提出了一种自适应动态低秩近似（DLRA）方案，用于从宏观测量中重建辐射传输方程中的散射参数，该方案显著降低了计算和内存成本，并被证明是准确和高效的。", "motivation": "动力学方程的参数识别逆问题的数值解具有高计算和内存成本。", "method": "该方法首先在连续设置中通过PDE约束优化过程，并使用拉格朗日重构推导出伴随方程。对于散射系数，引入周期B样条逼近并制定了更新其系数的梯度下降步骤。离散化后，应用动态低秩近似（DLRA）。利用秩自适应基更新和伽辽金积分器，以及用于梯度下降步长和DLRA容差自适应改进的线搜索方法。", "result": "所提出的方案显著降低了内存和计算成本。使用不同初始条件计算的数值结果验证了所提出的DLRA方案与使用完整求解器计算的解决方案相比的准确性和效率。", "conclusion": "所提出的自适应动态低秩优化器在解决动力学参数识别逆问题时，能够显著降低计算和内存成本，并保持高精度和效率。", "translation": "动力学方程参数识别逆问题的数值解可能表现出高计算和内存成本。在本文中，我们提出了一种动态低秩方案，用于从一些宏观时间无关测量中重建辐射传输方程中的散射参数。我们首先在连续设置中通过PDE约束优化过程，并使用拉格朗日重构推导出伴随方程。对于散射系数，引入周期B样条逼近，并制定了更新其系数的梯度下降步骤。离散化后，应用动态低秩近似（DLRA）。我们利用秩自适应基更新和伽辽金积分器，以及用于梯度下降步长和DLRA容差自适应改进的线搜索方法。我们证明了所提出的方案显著降低了内存和计算成本。使用不同初始条件计算的数值结果验证了所提出的DLRA方案与使用完整求解器计算的解决方案相比的准确性和效率。", "summary": "本文提出了一种自适应动态低秩近似（DLRA）方案，用于解决动力学方程中参数识别逆问题的高计算和内存成本。该方案通过PDE约束优化、拉格朗日重构、B样条逼近和梯度下降步骤来重建辐射传输方程中的散射参数。在离散化后，应用DLRA，并结合秩自适应基更新、伽辽金积分器和线搜索方法进行自适应改进。研究结果表明，该方案显著降低了计算和内存成本，并且在准确性和效率上优于传统的完整求解器。", "keywords": "动态低秩近似, 参数识别, 逆问题, 辐射传输方程, 计算成本", "comments": "本文的创新点在于将动态低秩近似（DLRA）与自适应机制相结合，有效地解决了动力学参数识别逆问题中计算和内存成本高昂的挑战。特别是引入秩自适应基更新和线搜索方法，使得优化过程更加高效和鲁棒。这项工作对于处理大规模动力学逆问题具有重要的实际意义。"}}
{"id": "2506.21539", "title": "WorldVLA: Towards Autoregressive Action World Model", "authors": ["Jun Cen", "Chaohui Yu", "Hangjie Yuan", "Yuming Jiang", "Siteng Huang", "Jiayan Guo", "Xin Li", "Yibing Song", "Hao Luo", "Fan Wang", "Deli Zhao", "Hao Chen"], "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.", "comment": "Code: https://github.com/alibaba-damo-academy/WorldVLA", "pdf_url": "http://arxiv.org/pdf/2506.21539v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21539v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "世界VLA：迈向自回归动作世界模型", "tldr": "WorldVLA是一个统一动作和图像理解与生成的自回归动作世界模型，它整合了VLA模型和世界模型，并通过注意力掩码策略解决了自回归动作生成中的性能下降问题。", "motivation": "现有模型可能无法有效统一动作和图像的理解与生成，且自回归动作生成存在误差传播导致性能下降的问题，需要一个能同时解决这些问题并提升性能的框架。", "method": "提出了WorldVLA，一个自回归动作世界模型，它将视觉-语言-动作（VLA）模型和世界模型集成在一个框架中。世界模型通过利用动作和图像理解来预测未来图像，以学习环境的潜在物理特性并改进动作生成。动作模型根据图像观察生成后续动作，辅助视觉理解并反过来帮助世界模型的视觉生成。为了解决自回归动作序列生成性能下降的问题，提出了一种注意力掩码策略，在生成当前动作时选择性地掩盖先前的动作。", "result": "WorldVLA在性能上优于独立的动作模型和世界模型，这表明世界模型和动作模型之间存在相互增强。然而，发现自回归生成动作序列时动作模型性能会下降，这归因于模型动作预测泛化能力有限导致误差传播。提出的注意力掩码策略在动作块生成任务中显示出显著的性能提升。", "conclusion": "WorldVLA成功地将动作和图像理解与生成统一起来，并通过世界模型和动作模型之间的相互增强取得了优越的性能。同时，通过引入注意力掩码策略，有效解决了自回归动作生成中的误差传播问题，提升了模型的泛化能力和性能。", "translation": "我们提出了WorldVLA，一个自回归动作世界模型，它统一了动作和图像的理解与生成。我们的WorldVLA将视觉-语言-动作（VLA）模型和世界模型集成在一个单一的框架中。世界模型通过利用动作和图像理解来预测未来图像，目的是学习环境的潜在物理特性以改进动作生成。同时，动作模型根据图像观察生成后续动作，辅助视觉理解并反过来帮助世界模型的视觉生成。我们证明了WorldVLA优于独立的动作模型和世界模型，突出了世界模型和动作模型之间的相互增强。此外，我们发现当以自回归方式生成动作序列时，动作模型的性能会下降。这种现象可归因于模型对动作预测的泛化能力有限，导致误差从早期动作传播到后续动作。为了解决这个问题，我们提出了一种注意力掩码策略，在生成当前动作时选择性地掩盖先前的动作，这在动作块生成任务中显示出显著的性能提升。", "summary": "WorldVLA是一个创新的自回归动作世界模型，它在一个统一的框架中结合了视觉-语言-动作（VLA）模型和世界模型。该模型旨在通过世界模型预测未来图像来学习环境物理特性以改进动作生成，并通过动作模型生成后续动作来辅助视觉理解。研究表明，WorldVLA性能优于独立模型，且其内部的世界模型和动作模型相互促进。针对自回归动作生成中存在的误差传播导致的性能下降问题，研究提出了一种注意力掩码策略，显著提升了动作块生成任务的性能。", "keywords": "自回归动作世界模型, WorldVLA, 视觉-语言-动作模型, 注意力掩码, 动作生成", "comments": "这篇论文通过提出WorldVLA，有效地将VLA模型与世界模型融合，实现了动作和图像理解与生成的统一，这是一个重要的创新点。模型内部的世界模型和动作模型之间的相互增强机制是其性能优越的关键。更值得注意的是，论文识别并成功解决了自回归动作生成中常见的误差累积问题，通过提出新颖的注意力掩码策略，显著提升了模型的实用性和鲁棒性，对未来的自回归模型设计具有指导意义。"}}
{"id": "2506.20960", "title": "OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs", "authors": ["Yiman Zhang", "Ziheng Luo", "Qiangyu Yan", "Wei He", "Borui Jiang", "Xinghao Chen", "Kai Han"], "summary": "In this paper, we introduce OmniEval, a benchmark for evaluating\nomni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,\nand textual inputs. Compared with existing benchmarks, our OmniEval has several\ndistinctive features: (i) Full-modal collaboration: We design evaluation tasks\nthat highlight the strong coupling between audio and video, requiring models to\neffectively leverage the collaborative perception of all modalities; (ii)\nDiversity of videos: OmniEval includes 810 audio-visual synchronized videos,\n285 Chinese videos and 525 English videos; (iii) Diversity and granularity of\ntasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended\nquestions and 1205 multiple-choice questions. These questions are divided into\n3 major task types and 12 sub-task types to achieve comprehensive evaluation.\nAmong them, we introduce a more granular video localization task named\nGrounding. Then we conduct experiments on OmniEval with several omni-modality\nmodels. We hope that our OmniEval can provide a platform for evaluating the\nability to construct and understand coherence from the context of all\nmodalities. Codes and data could be found at https://omnieval.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20960v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20960v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "OmniEval：一个用于评估视觉、听觉和文本输入的全模态模型的基准", "tldr": "OmniEval是一个新的基准，用于评估处理视觉、听觉和文本输入的全模态模型，具有全模态协作、视频多样性和任务多样性等特点。", "motivation": "现有的基准可能无法有效评估需要视觉、听觉和文本输入协同的全模态模型，因此需要一个能突出模态间强耦合的评估平台。", "method": "本文介绍了OmniEval基准，其特点包括：(i) 全模态协作：设计强调音视频强耦合的任务，要求模型有效利用所有模态的协作感知；(ii) 视频多样性：包含810个音视频同步视频（285个中文，525个英文）；(iii) 任务多样性和粒度：包含2617个问答对（1412个开放式问题，1205个多项选择问题），分为3大类12子类任务，并引入了更细粒度的视频定位任务Grounding。", "result": "在OmniEval上对几种全模态模型进行了实验。具体实验结果细节未在摘要中提及。", "conclusion": "作者希望OmniEval能为评估模型构建和理解所有模态上下文连贯性的能力提供一个平台。", "translation": "在本文中，我们介绍了 OmniEval，这是一个用于评估 MiniCPM-O 2.6 等全模态模型的基准，该基准包含视觉、听觉和文本输入。与现有基准相比，我们的 OmniEval 具有几个显著特点：(i) 全模态协作：我们设计的评估任务突出了音频和视频之间的强耦合，要求模型有效利用所有模态的协作感知；(ii) 视频多样性：OmniEval 包含 810 个音视频同步视频，其中 285 个中文视频和 525 个英文视频；(iii) 任务多样性和粒度：OmniEval 包含 2617 个问答对，其中包括 1412 个开放式问题和 1205 个多项选择问题。这些问题分为 3 种主要任务类型和 12 种子任务类型，以实现全面评估。其中，我们引入了一个更细粒度的视频定位任务，名为 Grounding。然后，我们使用几种全模态模型在 OmniEval 上进行了实验。我们希望我们的 OmniEval 能够为评估从所有模态上下文中构建和理解连贯性的能力提供一个平台。代码和数据可在 https://omnieval.github.io/ 找到。", "summary": "本文提出了OmniEval，一个专为评估全模态模型（涵盖视觉、听觉、文本输入）设计的基准。它通过强调全模态协作、提供多样化的音视频数据（包括中英文）以及细致多样的任务类型（如视频定位Grounding），旨在全面评估模型在多模态上下文理解和连贯性构建方面的能力。作者在OmniEval上对多种全模态模型进行了实验。", "keywords": "全模态模型, 评估基准, 视觉, 听觉, 文本", "comments": "该论文的创新点在于提出了一个更全面的全模态评估基准OmniEval，特别强调了音频与视频之间的强耦合，并引入了更细粒度的任务类型，弥补了现有基准在评估全模态模型协同感知能力方面的不足。其包含中英文视频和多样化任务的设计，使其具有较高的实用性和普适性。"}}
{"id": "2506.21170", "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21170v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21170v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "文本扩散建模的压缩平滑潜在空间", "tldr": "Cosmos是一个新的文本扩散模型，它在一个压缩平滑的潜在空间中操作，实现了更快的推理速度和可比或更优的生成质量。", "motivation": "自回归语言模型存在解码速度慢和难以维持全局连贯性的局限性。扩散模型在文本生成中的应用受到token级表示高维度的阻碍。", "method": "本文引入了Cosmos，一种在为扩散量身定制的压缩、平滑潜在空间中进行文本生成的新方法。该潜在空间通过一个自动编码器学习，该编码器同时进行token级重建和与预训练语言编码器冻结激活的对齐。", "result": "文本表示可以被压缩8倍，同时保持与token级扩散模型相当的生成质量。增加潜在序列长度使Cosmos超越了扩散基线和自回归基线。在故事生成、问题生成、摘要和去毒化等四种生成任务上，Cosmos实现了可比或更优的生成质量，并提供超过2倍的推理速度。", "conclusion": "Cosmos通过在压缩平滑的潜在空间中进行操作，有效地解决了现有文本生成模型在速度和连贯性方面的局限性，实现了在保持或提升生成质量的同时显著提高推理速度。", "translation": "自回归语言模型主导了现代文本生成，但其顺序性带来了根本性限制：解码速度慢，且难以保持全局连贯性。扩散模型通过实现并行生成和灵活控制提供了有前景的替代方案；然而，它们在文本生成中的应用受到token级表示高维度的阻碍。我们引入了Cosmos，一种在专门为扩散量身定制的压缩、平滑潜在空间中完全操作的文本生成新方法。这个空间是使用一个自动编码器学习的，该自动编码器同时进行token级重建和与预训练语言编码器冻结激活的对齐，从而提供强大的语义基础并实现有效的基于扰动的增强。经验上，我们证明文本表示可以被压缩8倍，同时保持与token级扩散模型相当的生成质量。此外，增加潜在序列长度使Cosmos超越了基于扩散和自回归的基线。我们在包括故事生成、问题生成、摘要和去毒化在内的四种多样化生成任务上评估了Cosmos，并将其与各种生成范式进行了比较。Cosmos实现了可比或更优的生成质量，同时提供了超过2倍的推理速度。", "summary": "Cosmos是一种新的文本扩散模型，旨在解决传统自回归模型速度慢和连贯性差的问题，以及扩散模型在高维度文本表示上的挑战。它通过学习一个压缩、平滑的潜在空间来操作，该空间由一个同时进行token级重建和语义对齐的自动编码器生成。实验证明，Cosmos能将文本表示压缩8倍，同时保持生成质量，并在多项生成任务上超越现有基线，推理速度提升超过2倍。", "keywords": "文本扩散建模, 潜在空间, 自动编码器, 文本生成, 推理速度", "comments": "Cosmos的创新点在于将文本扩散模型引入到一个压缩且平滑的潜在空间中，这有效地克服了高维度表示的挑战，并显著提升了推理速度。该方法为文本生成领域提供了一个新的高效范式，尤其适用于需要并行生成和快速推理的应用场景。"}}
{"id": "2506.21455", "title": "An Iterative Methodology for Unitary Quantum Channel Search", "authors": ["Matthew M. Lin", "Hao-Wei Huang", "Bing-Ze Lu"], "summary": "In this paper, we propose an iterative algorithm using polar decomposition to\napproximate a channel characterized by a single unitary matrix based on\ninput-output quantum state pairs. In limited data, we state and prove that the\noptimal solution obtained from our method using one pair with a specific\nstructure will generate an equivalent class, significantly reducing the\ndimension of the searching space. Furthermore, we prove that the unitary\nmatrices describing the same channel differ by a complex number with modulus 1.\nWe rigorously prove our proposed algorithm can ultimately identify a critical\npoint, which is also a local minimum of the established objective function.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21455v1", "categories": ["math.NA", "cs.NA", "quant-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21455v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "幺正量子信道搜索的迭代方法", "tldr": "本文提出了一种基于极分解的迭代算法，用于从输入输出量子态对中近似幺正量子信道，并从理论上证明了其在有限数据下的搜索空间约简和收敛性。", "motivation": "该研究旨在开发一种迭代方法，通过输入输出量子态对来近似由单个幺正矩阵表征的量子信道。", "method": "本文提出了一种使用极分解的迭代算法，通过输入输出量子态对来近似由单个幺正矩阵表征的量子信道。在有限数据下，该方法利用一个具有特定结构的量子态对来生成一个等价类，从而显著减小搜索空间的维度。", "result": "研究证明，在有限数据下，使用特定结构的输入输出量子态对，从该方法获得的最佳解将生成一个等价类，从而显著减小搜索空间的维度。此外，还证明了描述同一信道的幺正矩阵仅相差一个模为1的复数。论文严格证明了所提出的算法最终可以识别一个临界点，该点也是所建立目标函数的局部最小值。", "conclusion": "该迭代算法能够有效地近似幺正量子信道，并在理论上被证明能够收敛到目标函数的局部最小值，同时在有限数据情况下显著优化了搜索效率。", "translation": "在本文中，我们提出了一种使用极分解的迭代算法，用于基于输入输出量子态对来近似由单个幺正矩阵表征的信道。在有限数据的情况下，我们阐述并证明了使用一个具有特定结构的量子态对从我们方法获得的最佳解将生成一个等价类，从而显著减小搜索空间的维度。此外，我们证明了描述相同信道的幺正矩阵相差一个模为1的复数。我们严格证明了我们提出的算法最终可以识别一个临界点，该点也是所建立目标函数的局部最小值。", "summary": "本文提出了一种基于极分解的迭代算法，旨在通过输入输出量子态对来近似由单个幺正矩阵表示的量子信道。研究证明，在数据有限时，利用特定结构的输入输出对，该方法能生成等价类，从而大幅缩小搜索空间。此外，论文还证明了描述同一信道的幺正矩阵仅相差一个模为1的复数，并严格证明了所提算法能够收敛到目标函数的局部最小值。", "keywords": "幺正量子信道, 迭代算法, 极分解, 量子态对, 搜索空间约简", "comments": "该论文的创新点在于提出了一个利用极分解的迭代算法来解决幺正量子信道近似问题，并在理论上证明了其在有限数据下的搜索空间优化和收敛性。特别是，通过生成等价类来显著减小搜索空间维度，显示了其在实际应用中的潜在效率提升。"}}
{"id": "2506.20964", "title": "Evidence-based diagnostic reasoning with multi-agent copilot for human pathology", "authors": ["Chengkuan Chen", "Luca L. Weishaupt", "Drew F. K. Williamson", "Richard J. Chen", "Tong Ding", "Bowen Chen", "Anurag Vaidya", "Long Phi Le", "Guillaume Jaume", "Ming Y. Lu", "Faisal Mahmood"], "summary": "Pathology is experiencing rapid digital transformation driven by whole-slide\nimaging and artificial intelligence (AI). While deep learning-based\ncomputational pathology has achieved notable success, traditional models\nprimarily focus on image analysis without integrating natural language\ninstruction or rich, text-based context. Current multimodal large language\nmodels (MLLMs) in computational pathology face limitations, including\ninsufficient training data, inadequate support and evaluation for multi-image\nunderstanding, and a lack of autonomous, diagnostic reasoning capabilities. To\naddress these limitations, we introduce PathChat+, a new MLLM specifically\ndesigned for human pathology, trained on over 1 million diverse,\npathology-specific instruction samples and nearly 5.5 million question answer\nturns. Extensive evaluations across diverse pathology benchmarks demonstrated\nthat PathChat+ substantially outperforms the prior PathChat copilot, as well as\nboth state-of-the-art (SOTA) general-purpose and other pathology-specific\nmodels. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI\nsystem leveraging PathChat+ to autonomously evaluate gigapixel whole-slide\nimages (WSIs) through iterative, hierarchical diagnostic reasoning, reaching\nhigh accuracy on DDxBench, a challenging open-ended differential diagnosis\nbenchmark, while also capable of generating visually grounded,\nhumanly-interpretable summary reports.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20964v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20964v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于证据的病理诊断推理与多智能体副驾驶", "tldr": "本文介绍了PathChat+，一种新型多模态大语言模型（MLLM），专为人类病理学设计，解决了现有模型在整合语言指令、处理多图像理解和自主诊断推理方面的局限性。在此基础上，提出了SlideSeek，一个多智能体AI系统，能够通过迭代、分层的诊断推理自主评估全玻片图像（WSIs），并在具有挑战性的诊断基准测试中表现出色。", "motivation": "传统的计算病理学模型主要侧重于图像分析，但缺乏整合自然语言指令或丰富的文本上下文的能力。当前多模态大语言模型（MLLMs）在计算病理学领域面临训练数据不足、对多图像理解支持和评估不足以及缺乏自主诊断推理能力的限制。为了解决这些问题，本研究旨在开发一种能够进行基于证据的诊断推理并具备自主诊断能力的AI系统。", "method": "本研究引入了PathChat+，一种新型多模态大语言模型（MLLM），专为人类病理学设计，通过超过100万个多样化的病理学特定指令样本和近550万个问答回合进行训练。此外，还提出了SlideSeek，一个推理驱动的多智能体AI系统，它利用PathChat+通过迭代、分层的诊断推理自主评估千兆像素的全玻片图像（WSIs）。", "result": "PathChat+在多项病理学基准测试中显著优于先前的PathChat副驾驶以及最先进的通用模型和其他病理学专用模型。SlideSeek在具有挑战性的开放式鉴别诊断基准DDxBench上达到了高准确率，并且能够生成视觉化、人类可解释的摘要报告。", "conclusion": "PathChat+和SlideSeek的开发和成功验证表明，结合大规模病理学特定数据训练的多模态大语言模型和多智能体推理系统，能够显著提升计算病理学在诊断推理方面的能力，并为病理学家提供有效的辅助诊断工具。", "translation": "病理学正在经历由全玻片成像和人工智能（AI）驱动的快速数字化转型。尽管基于深度学习的计算病理学取得了显著成功，但传统模型主要专注于图像分析，而没有整合自然语言指令或丰富的文本上下文。当前计算病理学中的多模态大语言模型（MLLMs）面临局限性，包括训练数据不足、对多图像理解的支持和评估不足，以及缺乏自主诊断推理能力。为了解决这些局限性，我们引入了PathChat+，一种专为人类病理学设计的新型MLLM，它在超过100万个多样化的病理学特定指令样本和近550万个问答回合上进行训练。在各种病理学基准测试中的广泛评估表明，PathChat+显著优于先前的PathChat副驾驶以及最先进的通用模型和其他病理学专用模型。此外，我们提出了SlideSeek，一个推理驱动的多智能体AI系统，它利用PathChat+通过迭代、分层的诊断推理自主评估千兆像素的全玻片图像（WSIs），在具有挑战性的开放式鉴别诊断基准DDxBench上达到了高准确率，同时还能生成视觉化、人类可解释的摘要报告。", "summary": "本研究针对计算病理学中传统模型缺乏语言整合和现有多模态大语言模型（MLLMs）在数据、多图像理解及自主推理方面的不足，提出了两项创新。首先，开发了PathChat+，一个在海量病理学专用数据上训练的新型MLLM，其在多项基准测试中表现优异。其次，构建了SlideSeek，一个基于PathChat+的多智能体AI系统，能够自主进行迭代、分层的全玻片图像诊断推理，并在鉴别诊断任务中取得高准确率，同时生成可解释的报告，显著提升了病理学诊断的AI辅助能力。", "keywords": "病理学, 人工智能, 多模态大语言模型, 诊断推理, 全玻片图像", "comments": "该论文在计算病理学领域取得了重要进展，通过引入PathChat+解决了当前MLLMs在病理学数据和多模态理解方面的核心痛点。更具创新性的是，SlideSeek多智能体系统的提出，使其能够进行迭代、分层的诊断推理，这模仿了人类病理学家的思考过程，是AI从图像识别走向复杂诊断推理的关键一步。生成可解释报告的能力也极大地增强了AI在临床应用中的可信度和实用性。"}}
{"id": "2506.20886", "title": "Omniwise: Predicting GPU Kernels Performance with LLMs", "authors": ["Zixian Wang", "Cole Ramos", "Muhammad A. Awad", "Keith Lowery"], "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20886v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20886v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "Omniwise：使用大型语言模型预测GPU内核性能", "tldr": "Omniwise是一个端到端、自监督的微调流程，首次将大型语言模型（LLMs）应用于GPU内核性能预测，无需实际执行代码即可准确预测性能指标，并在AMD GPU上表现出色。", "motivation": "深度神经网络（DNNs）的快速发展彻底改变了人工智能，但其强大的架构需要高效的GPU性能。为了解决GPU内核性能预测的挑战，本研究旨在引入一种无需代码执行或分析工具的新方法。", "method": "本研究提出了Omniwise，一个端到端、自监督的微调流程，将大型语言模型（LLMs）应用于GPU内核性能预测。Omniwise是模型无关且轻量级的，可以直接从内核代码预测内存带宽、缓存命中率、GFLOPs和算术强度等关键性能指标，无需执行代码或使用分析工具。", "result": "Omniwise在AMD MI250和MI300X架构上执行的GPU内核预测中，超过90%的预测结果在10%的相对误差范围内。它能够预测关键性能指标，包括内存带宽、缓存命中率、GFLOPs和算术强度。", "conclusion": "Omniwise成功地将大型语言模型应用于GPU内核性能预测，提供了一种无需代码执行的、准确且轻量级的解决方案，并能无缝集成到开发者的工作流程中，极大地提升了性能分析的效率。", "translation": "近年来，深度神经网络（DNNs）的快速发展彻底改变了人工智能，使模型具备了前所未有的理解、生成和处理复杂数据的能力。这些强大的架构已经改变了广泛的下游应用，解决了人类力所不能及的任务。在本文中，我们介绍了Omniwise，这是第一个端到端、自监督的微调流程，将大型语言模型（LLMs）应用于GPU内核性能预测——这是性能分析中的一个新颖用例。Omniwise是模型无关且轻量级的，即使使用一个小型3B参数模型也能取得显著结果。它可以直接从内核代码预测关键性能指标，包括内存带宽、缓存命中率、GFLOPs和算术强度，而无需代码执行或分析工具。我们的方法在AMD MI250和MI300X架构上执行的GPU内核上，实现了超过90%的预测在10%的相对误差范围内。除了该流程，我们还开发了一个在线推理服务器和一个Visual Studio Code插件，将基于LLM的性能预测无缝集成到开发人员的工作流程中。", "summary": "Omniwise是首个将大型语言模型（LLMs）应用于GPU内核性能预测的端到端、自监督微调流程。它能够直接从内核代码预测内存带宽、缓存命中率、GFLOPs和算术强度等关键性能指标，无需实际执行代码或使用分析工具。该方法在AMD MI250和MI300X架构上的GPU内核上，超过90%的预测结果在10%的相对误差内。此外，Omniwise还提供了在线推理服务器和VS Code插件，便于开发者集成使用。", "keywords": "GPU性能预测, 大型语言模型, 自监督学习, 性能分析, Omniwise", "comments": "本文提出Omniwise，首次将大型语言模型（LLMs）应用于GPU内核性能预测，这是一个显著的创新点。其优势在于无需实际执行代码即可预测性能，大大节省了时间和资源。此外，该方法模型无关且轻量级，即使小型模型也能取得良好效果，并通过集成开发环境插件提升了实用性。这对于GPU性能优化和开发流程具有重要意义。"}}
{"id": "2506.20967", "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing", "authors": ["Lingling Cai", "Kang Zhao", "Hangjie Yuan", "Xiang Wang", "Yingya Zhang", "Kejie Huang"], "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in\nvideo generation. However, directly applying existing video editing methods to\nVideo DiTs often incurs substantial computational overhead, due to\nresource-intensive attention modification or finetuning. To alleviate this\nproblem, we present DFVEdit, an efficient zero-shot video editing method\ntailored for Video DiTs. DFVEdit eliminates the need for both attention\nmodification and fine-tuning by directly operating on clean latents via flow\ntransformation. To be more specific, we observe that editing and sampling can\nbe unified under the continuous flow perspective. Building upon this\nfoundation, we propose the Conditional Delta Flow Vector (CDFV) -- a\ntheoretically unbiased estimation of DFV -- and integrate Implicit Cross\nAttention (ICA) guidance as well as Embedding Reinforcement (ER) to further\nenhance editing quality. DFVEdit excels in practical efficiency, offering at\nleast 20x inference speed-up and 85\\% memory reduction on Video DiTs compared\nto attention-engineering-based editing methods. Extensive quantitative and\nqualitative experiments demonstrate that DFVEdit can be seamlessly applied to\npopular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art\nperformance on structural fidelity, spatial-temporal consistency, and editing\nquality.", "comment": "Zero-shot video editing", "pdf_url": "http://arxiv.org/pdf/2506.20967v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20967v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DFVEdit：零样本视频编辑的条件增量流向量", "tldr": "DFVEdit是一种高效的零样本视频编辑方法，专为Video DiTs设计，通过流变换直接操作潜在空间，无需注意力修改或微调，显著提升了编辑速度和内存效率。", "motivation": "现有视频编辑方法直接应用于Video Diffusion Transformers (Video DiTs) 会产生巨大的计算开销，因为它们需要资源密集型的注意力修改或微调。", "method": "提出了DFVEdit，一种高效的零样本视频编辑方法。它通过流变换直接操作干净的潜在空间，无需注意力修改和微调。该方法基于编辑和采样可以统一在连续流的视角下，并提出了条件增量流向量 (CDFV) 作为DFV的理论无偏估计。此外，还集成了隐式交叉注意力 (ICA) 指导和嵌入强化 (ER) 以提高编辑质量。", "result": "DFVEdit在实用效率方面表现出色，与基于注意力工程的编辑方法相比，在Video DiTs上实现了至少20倍的推理加速和85%的内存减少。在结构保真度、时空一致性和编辑质量方面达到了最先进的性能，并可无缝应用于流行的Video DiTs (如CogVideoX和Wan2.1)。", "conclusion": "DFVEdit是一种高效、高性能的零样本视频编辑方法，通过创新的流变换方法解决了Video DiTs的计算开销问题，并在多个方面实现了最先进的性能。", "translation": "视频扩散Transformer (Video DiTs) 的出现标志着视频生成领域的一个里程碑。然而，将现有视频编辑方法直接应用于Video DiTs通常会产生巨大的计算开销，因为它们需要资源密集型的注意力修改或微调。为了缓解这个问题，我们提出了DFVEdit，一种专为Video DiTs量身定制的高效零样本视频编辑方法。DFVEdit通过流变换直接操作干净的潜在空间，从而无需注意力修改和微调。更具体地说，我们观察到编辑和采样可以在连续流的视角下统一。在此基础上，我们提出了条件增量流向量 (CDFV)——DFV的理论无偏估计——并集成了隐式交叉注意力 (ICA) 指导以及嵌入强化 (ER) 以进一步提高编辑质量。DFVEdit在实用效率方面表现出色，与基于注意力工程的编辑方法相比，在Video DiTs上实现了至少20倍的推理加速和85%的内存减少。广泛的定量和定性实验表明，DFVEdit可以无缝应用于流行的Video DiTs（例如CogVideoX和Wan2.1），在结构保真度、时空一致性和编辑质量方面达到了最先进的性能。", "summary": "DFVEdit是一种专为Video Diffusion Transformers (Video DiTs) 设计的高效零样本视频编辑方法。它通过引入条件增量流向量 (CDFV) 和直接在潜在空间进行流变换，避免了传统方法所需的注意力修改和微调，显著降低了计算开销。该方法集成了隐式交叉注意力指导和嵌入强化以提升编辑质量，并在推理速度和内存效率上取得了显著提升，实现了20倍加速和85%内存减少，同时在多个Video DiTs上达到了最先进的编辑性能。", "keywords": "视频编辑, 零样本, 视频扩散Transformer, 流向量, 计算效率", "comments": "这篇论文通过提出DFVEdit，为Video DiTs的视频编辑提供了一种创新且高效的解决方案。其核心创新在于将编辑和采样统一在连续流视角下，并引入条件增量流向量，从而避免了传统方法中计算量大的注意力修改和微调。这极大地提升了推理速度和内存效率，解决了Video DiTs在实际应用中的主要瓶颈。该方法的零样本特性和对现有Video DiTs的无缝兼容性也增加了其实用价值。"}}
{"id": "2506.21199", "title": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image Segmentation and Classification", "authors": ["Shadman Sobhan", "Kazi Abrar Mahmud", "Abduz Zami"], "summary": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs.", "comment": "40 pages, 8 Tables, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2506.21199v1", "categories": ["cs.CV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21199v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MedPrompt：用于医学图像分割和分类的LLM-CNN融合与权重路由", "tldr": "MedPrompt是一个统一的框架，结合LLM进行高级任务规划和CNN进行低级图像处理，通过权重路由实现可扩展的医学图像分割和分类，避免了为新任务重新训练整个框架，并在19个数据集上表现出色。", "motivation": "当前的医学图像分析系统通常是任务特定的，需要独立的模型进行分类和分割，并且缺乏支持用户定义工作流的灵活性。", "method": "MedPrompt是一个统一的框架，它将一个少样本提示的大型语言模型（Llama-4-17B）用于高级任务规划，并与一个模块化卷积神经网络（DeepFusionLab）用于低级图像处理相结合。LLM解释用户指令并生成结构化输出以动态路由任务特定的预训练权重，这种权重路由方法避免了在添加新任务时重新训练整个框架，只需任务特定权重。", "result": "MedPrompt在19个公共数据集上进行了评估，涵盖5种成像模式的12项任务。系统在解释和执行提示驱动指令方面达到了97%的端到端正确性，平均推理延迟为2.5秒。DeepFusionLab在肺部分割方面达到了0.9856的Dice系数，在结核病分类方面达到了0.9744的F1分数。", "conclusion": "MedPrompt通过结合LLM的可解释性和模块化CNN的效率，实现了可扩展的、提示驱动的医学成像。", "translation": "当前的医学图像分析系统通常是任务特定的，需要独立的模型进行分类和分割，并且缺乏支持用户定义工作流的灵活性。为了解决这些挑战，我们引入了MedPrompt，一个统一的框架，它结合了一个少样本提示的大型语言模型（Llama-4-17B）用于高级任务规划，并与一个模块化卷积神经网络（DeepFusionLab）用于低级图像处理。LLM解释用户指令并生成结构化输出以动态路由任务特定的预训练权重。这种权重路由方法避免了在添加新任务时重新训练整个框架——只需要任务特定的权重，从而增强了可扩展性和部署便利性。我们在19个公共数据集上评估了MedPrompt，涵盖了5种成像模式的12项任务。该系统在解释和执行提示驱动指令方面达到了97%的端到端正确性，平均推理延迟为2.5秒，使其适用于近实时应用。DeepFusionLab实现了具有竞争力的分割精度（例如，肺部Dice系数0.9856）和强大的分类性能（结核病F1分数0.9744）。总的来说，MedPrompt通过结合LLM的可解释性与模块化CNN的效率，实现了可扩展的、提示驱动的医学成像。", "summary": "MedPrompt是一个创新的统一框架，旨在解决现有医学图像分析系统任务特定和缺乏灵活性的问题。它通过融合大型语言模型（LLM）进行高级任务规划和卷积神经网络（CNN）DeepFusionLab进行低级图像处理，并引入了权重路由机制，实现了动态任务适应而无需整体重新训练。该系统在多达19个数据集、12项任务和5种成像模式上进行了广泛评估，展示了97%的指令执行正确率和2.5秒的低延迟，同时在分割和分类任务上取得了有竞争力的性能，为可扩展、提示驱动的医学图像分析提供了有效方案。", "keywords": "医学图像分析, LLM, CNN, 图像分割, 图像分类", "comments": "MedPrompt的创新点在于其LLM-CNN融合架构和权重路由机制，这显著提高了医学图像分析系统的灵活性和可扩展性，解决了传统方法需要为每个任务单独训练模型的痛点。其支持用户定义工作流的能力也为实际应用带来了便利。该方法有望推动医学影像诊断的自动化和个性化发展。"}}
{"id": "2506.21124", "title": "Quantum Adaptive Search: A Hybrid Quantum-Classical Algorithm for Global Optimization of Multivariate Functions", "authors": ["G. Intoccia", "U. Chirico", "V. Schiano Di Cola", "G. Pepe", "S. Cuomo"], "summary": "This work presents Quantum Adaptive Search (QAGS), a hybrid quantum-classical\nalgorithm for the global optimization of multivariate functions. The method\nemploys an adaptive mechanism that dynamically narrows the search space based\non a quantum-estimated probability distribution of the objective function. A\nquantum state encodes information about solution quality through an appropriate\ncomplex amplitude mapping, enabling the identification of the most promising\nregions, and thus progressively tightening the search bounds; then a classical\noptimizer performs local refinement of the solution. The analysis demonstrates\nthat QAGS ensures a contraction of the search space toward global optima, with\ncontrolled computational complexity. The numerical results on the benchmark\nfunctions show that, compared to the classical methods, QAGS achieves higher\naccuracy while offering advantages in both time and space complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21124v1", "categories": ["quant-ph", "cs.NA", "math.NA", "math.OC"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21124v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "量子自适应搜索：一种用于多元函数全局优化的混合量子-经典算法", "tldr": "QAGS是一种混合量子-经典算法，通过自适应缩小搜索空间和局部经典优化来高效地全局优化多元函数。", "motivation": "解决多元函数的全局优化问题，并提高优化效率和准确性。", "method": "QAGS采用自适应机制，根据目标函数的量子估计概率分布动态缩小搜索空间。量子态通过复振幅映射编码解的质量信息，以识别最有前景的区域并逐步收紧搜索边界。随后，经典优化器对解进行局部细化。", "result": "数值结果表明，与经典方法相比，QAGS在基准函数上实现了更高的精度，并在时间和空间复杂度方面具有优势。", "conclusion": "QAGS确保搜索空间向全局最优收缩，计算复杂度可控，且在精度、时间和空间复杂度方面均优于经典方法。", "translation": "这篇工作提出了量子自适应搜索（QAGS），一种用于多元函数全局优化的混合量子-经典算法。该方法采用一种自适应机制，根据目标函数的量子估计概率分布动态缩小搜索空间。量子态通过适当的复振幅映射编码解的质量信息，从而能够识别最有前景的区域，并因此逐步收紧搜索边界；然后，经典优化器对解进行局部细化。分析表明，QAGS确保搜索空间向全局最优收缩，且计算复杂度可控。在基准函数上的数值结果显示，与经典方法相比，QAGS实现了更高的精度，同时在时间和空间复杂度方面具有优势。", "summary": "本文提出了一种名为量子自适应搜索（QAGS）的混合量子-经典算法，用于多元函数的全局优化。该算法通过量子估计的概率分布自适应地缩小搜索空间，并利用量子态编码解的质量信息来识别有前景的区域，随后由经典优化器进行局部精修。QAGS被证明能有效收敛至全局最优，且具有受控的计算复杂度，并在数值实验中展现出比传统方法更高的精度以及时间和空间复杂度优势。", "keywords": "量子优化, 全局优化, 混合算法, 量子自适应搜索, 多元函数", "comments": "QAGS的创新之处在于结合了量子计算的潜力（通过量子态编码和量子估计概率分布）与经典优化器的局部搜索能力。这种混合方法有望克服纯经典方法在处理高维复杂优化问题时的局限性，特别是在全局收敛性和计算效率方面。其自适应缩小搜索空间机制是提高效率的关键。"}}
{"id": "2506.20893", "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20893v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20893v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "论输出分布重加权对有效类别遗忘的必要性", "tldr": "本文提出了一种名为RWFT的轻量级输出重加权遗忘方法，用于从训练好的分类器中擦除整个类别，无需完全重新训练，并能有效防止成员推断攻击，且在各项指标上表现优异。", "motivation": "从训练模型中遗忘特定类别对于强制执行用户删除权和减轻有害或有偏见的预测至关重要。完全重新训练成本高昂，且现有遗忘方法在预测未遗忘类别样本时无法复制重新训练模型的行为，并且容易受到成员推断攻击。", "method": "引入了一种名为RWFT的输出重加权遗忘方法，这是一种轻量级技术，可在不完全重新训练的情况下从训练好的分类器中擦除整个类别。该方法通过对遗忘类别样本的预测概率质量进行简单的重新分配，使其对成员推断攻击MIA-NN具有鲁棒性。此外，还引入了一个基于总变差（TV）距离的预测概率新度量来量化残余泄漏。", "result": "RWFT在现有评估指标和新提出的TV-based指标上都与完全重新训练的结果相匹配。与现有最佳方法相比，在之前使用的指标上提高了2.79%，在新提出的基于TV的指标上提高了111.45%。", "conclusion": "输出分布重加权对于实现有效的类别遗忘是必要的，它能够使遗忘模型更好地模拟完全重新训练的行为，并有效抵御成员推断攻击，同时提供了一种量化残余泄漏的新方法。", "translation": "在这项工作中，我们引入了一种输出重加权遗忘方法RWFT，这是一种轻量级技术，可以在不完全重新训练的情况下从训练好的分类器中擦除整个类别。从训练模型中遗忘特定类别对于强制执行用户删除权和减轻有害或有偏见的预测至关重要。完全重新训练成本高昂，而现有的遗忘方法在预测未遗忘类别样本时无法复制重新训练模型的行为。我们通过设计一种成员推断攻击的变体MIA-NN来证明这种失败，MIA-NN可以成功揭示任何这些方法的未遗忘类别。我们提出了一种对遗忘类别样本的预测概率质量进行简单重新分配的方法，该方法对MIA-NN具有鲁棒性。我们还引入了一个基于总变差（TV）距离的预测概率新度量，以量化残余泄漏，从而防止未来的方法容易受到新攻击的影响。通过对最先进的机器遗忘基线进行大量实验，我们表明我们的方法在先前工作使用的评估指标和我们在这项工作中提出的新指标上都与完全重新训练的结果相匹配。与最先进的方法相比，我们比现有最佳方法在先前使用的指标上提高了2.79%，在我们新的基于TV的指标上提高了111.45%。", "summary": "本文提出了一种名为RWFT的轻量级输出重加权遗忘方法，旨在无需完全重新训练即可从分类器中有效擦除特定类别。该方法通过重新分配遗忘类别的预测概率质量，提高了对成员推断攻击的鲁棒性。作者还引入了一个基于总变差距离的新度量来评估残余泄漏。实验结果表明，RWFT在各项指标上均能媲美完全重新训练，并且显著优于现有最先进的机器遗忘方法。", "keywords": "机器遗忘, 输出重加权, 类别遗忘, 成员推断攻击, 数据隐私", "comments": "本文创新性地提出了输出分布重加权的概念来解决机器遗忘中遗忘类别行为复制和成员推断攻击鲁棒性的问题。通过引入新的评估指标，进一步推动了该领域对遗忘效果量化的理解。其轻量级特性和优越的性能使其在实际应用中具有重要潜力，尤其是在数据隐私和模型伦理方面。"}}
{"id": "2506.20977", "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging", "authors": ["Tao Liu", "Dafeng Zhang", "Gengchen Li", "Shizhuo Liu", "Yongqi Song", "Senmao Li", "Shiqi Yang", "Boqian Li", "Kai Wang", "Yaxing Wang"], "summary": "Face aging has become a crucial task in computer vision, with applications\nranging from entertainment to healthcare. However, existing methods struggle\nwith achieving a realistic and seamless transformation across the entire\nlifespan, especially when handling large age gaps or extreme head poses. The\ncore challenge lies in balancing age accuracy and identity preservation--what\nwe refer to as the Age-ID trade-off. Most prior methods either prioritize age\ntransformation at the expense of identity consistency or vice versa. In this\nwork, we address this issue by proposing a two-pass face aging framework, named\nCradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first\npass focuses on solving age accuracy by introducing an adaptive noise injection\n(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of\nage and gender for the given person as the textual condition. Also, by\nadjusting the noise level, we can control the strength of aging while allowing\nmore flexibility in transforming the face. However, identity preservation is\nweakly ensured here to facilitate stronger age transformations. In the second\npass, we enhance identity preservation while maintaining age-specific features\nby conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace\nand Rotate-CLIP. This pass allows for denoising the transformed image from the\nfirst pass, ensuring stronger identity preservation without compromising the\naging accuracy. Both passes are jointly trained in an end-to-end way. Extensive\nexperiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL\nprotocols, show that our Cradle2Cane outperforms existing face aging methods in\nage accuracy and identity consistency.", "comment": "30 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20977v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20977v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从摇篮到拐杖：一种用于高保真度全生命周期人脸老化的两阶段框架", "tldr": "现有的人脸老化方法难以实现逼真和身份保留，尤其是在大年龄差距下。本文提出了Cradle2Cane，一个基于两阶段扩散模型的框架，通过自适应噪声注入和身份感知嵌入来平衡年龄准确性和身份保留，并优于现有方法。", "motivation": "人脸老化是计算机视觉中的一项关键任务，但现有方法在整个生命周期内实现逼真和无缝转换时面临挑战，尤其是在处理大年龄差距或极端头部姿势时。核心挑战在于平衡年龄准确性和身份保留（即“年龄-身份权衡”），大多数现有方法在此方面存在偏颇。", "method": "本文提出了一个名为Cradle2Cane的两阶段人脸老化框架，基于少步文本到图像（T2I）扩散模型。第一阶段通过引入自适应噪声注入（AdaNI）机制解决年龄准确性问题，该机制由年龄和性别提示作为文本条件引导，并允许通过调整噪声水平控制老化强度。此阶段弱化身份保留以促进更强的年龄转换。第二阶段通过将模型条件化于SVR-ArcFace和Rotate-CLIP两种身份感知嵌入（IDEmb），增强身份保留并保持年龄特定特征，同时对第一阶段的图像进行去噪。两个阶段以端到端方式联合训练。", "result": "在CelebA-HQ测试数据集上进行的广泛实验，通过Face++和Qwen-VL协议评估，表明Cradle2Cane在年龄准确性和身份一致性方面均优于现有的人脸老化方法。", "conclusion": "Cradle2Cane框架成功地解决了人脸老化中的年龄-身份权衡问题，通过两阶段方法实现了卓越的年龄准确性和身份一致性。", "translation": "人脸老化已成为计算机视觉中的一项关键任务，其应用范围从娱乐到医疗保健。然而，现有方法难以在整个生命周期内实现逼真和无缝的转换，尤其是在处理大年龄差距或极端头部姿势时。核心挑战在于平衡年龄准确性和身份保留——我们称之为年龄-身份权衡。大多数现有方法要么以牺牲身份一致性为代价优先进行年龄转换，要么反之。在这项工作中，我们通过提出一个名为Cradle2Cane的两阶段人脸老化框架来解决这个问题，该框架基于少步文本到图像（T2I）扩散模型。第一阶段通过引入自适应噪声注入（AdaNI）机制来解决年龄准确性问题。该机制通过包含给定人物的年龄和性别提示描述作为文本条件进行引导。此外，通过调整噪声水平，我们可以控制老化强度，同时在人脸转换中提供更大的灵活性。然而，在此阶段身份保留被弱化，以促进更强的年龄转换。在第二阶段，我们通过将模型条件化于两个身份感知嵌入（IDEmb）：SVR-ArcFace和Rotate-CLIP，来增强身份保留，同时保持年龄特定特征。此阶段允许对第一阶段转换后的图像进行去噪，确保更强的身份保留而不损害老化准确性。两个阶段都以端到端方式联合训练。在CelebA-HQ测试数据集上进行的广泛实验，通过Face++和Qwen-VL协议进行评估，表明我们的Cradle2Cane在年龄准确性和身份一致性方面优于现有的人脸老化方法。", "summary": "本研究提出了一种名为Cradle2Cane的两阶段人脸老化框架，旨在解决现有方法在全生命周期人脸老化中面临的“年龄-身份权衡”挑战。该框架基于文本到图像扩散模型，并通过两个阶段实现平衡。第一阶段利用自适应噪声注入（AdaNI）机制，结合年龄和性别提示，优先实现年龄准确性。第二阶段则通过引入SVR-ArcFace和Rotate-CLIP两种身份感知嵌入，增强图像的身份保留，同时维持年龄特征。两个阶段联合训练，实验结果表明Cradle2Cane在年龄准确性和身份一致性方面均优于现有方法，为高保真度人脸老化提供了有效解决方案。", "keywords": "人脸老化, 扩散模型, 身份保留, 年龄准确性, 两阶段框架", "comments": "这项工作通过引入一个创新的两阶段框架来解决人脸老化中的核心挑战——年龄准确性与身份保留之间的权衡。其亮点在于结合了T2I扩散模型，并设计了独特的AdaNI机制和身份感知嵌入，有效地实现了平衡。这种分阶段处理复杂任务的方法值得借鉴，并且其在处理大年龄差距和极端姿势方面的潜力是重要的进步。"}}
{"id": "2506.20694", "title": "Evaluating PDE discovery methods for multiscale modeling of biological signals", "authors": ["Andréa Ducos", "Audrey Denizot", "Thomas Guyet", "Hugues Berry"], "summary": "Biological systems are non-linear, include unobserved variables and the\nphysical principles that govern their dynamics are partly unknown. This makes\nthe characterization of their behavior very challenging. Notably, their\nactivity occurs on multiple interdependent spatial and temporal scales that\nrequire linking mechanisms across scales. To address the challenge of bridging\ngaps between scales, we leverage partial differential equations (PDE)\ndiscovery. PDE discovery suggests meso-scale dynamics characteristics from\nmicro-scale data. In this article, we present our framework combining\nparticle-based simulations and PDE discovery and conduct preliminary\nexperiments to assess equation discovery in controlled settings. We evaluate\nfive state-of-the-art PDE discovery methods on particle-based simulations of\ncalcium diffusion in astrocytes. The performances of the methods are evaluated\non both the form of the discovered equation and the forecasted temporal\nvariations of calcium concentration. Our results show that several methods\naccurately recover the diffusion term, highlighting the potential of PDE\ndiscovery for capturing macroscopic dynamics in biological systems from\nmicroscopic data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20694v1", "categories": ["q-bio.QM", "cs.AI"], "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.20694v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "评估用于生物信号多尺度建模的偏微分方程发现方法", "tldr": "本研究评估了五种最先进的偏微分方程（PDE）发现方法，通过粒子模拟的星形胶质细胞中钙扩散来验证其在从微观数据中发现宏观生物系统动力学方面的潜力。", "motivation": "生物系统具有非线性、包含未观测变量、物理原理部分未知且其活动发生在多尺度上，这使得表征其行为极具挑战性。为了解决跨尺度连接的难题，本文利用偏微分方程（PDE）发现。", "method": "本文提出了一个结合粒子模拟和PDE发现的框架，并在受控环境下进行了初步实验，以评估方程发现的性能。研究评估了五种最先进的PDE发现方法，针对星形胶质细胞中钙扩散的粒子模拟数据。方法的性能从发现方程的形式和钙浓度的时间变化预测两方面进行评估。", "result": "研究结果表明，有几种方法能够准确地恢复扩散项。", "conclusion": "研究结果突出了偏微分方程发现从微观数据中捕获生物系统宏观动力学的潜力。", "translation": "生物系统是非线性的，包含未观测变量，且支配其动力学的物理原理部分未知。这使得表征其行为极具挑战性。值得注意的是，它们的活动发生在多个相互依赖的空间和时间尺度上，需要跨尺度连接机制。为了解决弥合尺度间隙的挑战，我们利用偏微分方程（PDE）发现。PDE发现从微观数据中推断介观尺度的动力学特征。在本文中，我们提出了结合粒子模拟和PDE发现的框架，并进行了初步实验以评估受控环境下的方程发现。我们评估了五种最先进的PDE发现方法，针对星形胶质细胞中钙扩散的粒子模拟。方法的性能从发现方程的形式和钙浓度的时间变化预测两方面进行评估。我们的结果表明，有几种方法能够准确地恢复扩散项，突出了PDE发现从微观数据中捕获生物系统宏观动力学的潜力。", "summary": "本研究旨在评估偏微分方程（PDE）发现方法在生物信号多尺度建模中的应用。鉴于生物系统固有的复杂性、非线性和多尺度特性，研究提出了一个结合粒子模拟和PDE发现的框架。通过对星形胶质细胞中钙扩散的粒子模拟数据，评估了五种主流PDE发现方法在方程形式恢复和时间变化预测方面的表现。结果表明，多种方法能够准确识别扩散项，证明了PDE发现从微观数据中提取生物系统宏观动力学的可行性与潜力。", "keywords": "PDE发现, 多尺度建模, 生物信号, 钙扩散, 星形胶质细胞", "comments": "该论文的创新点在于将PDE发现方法应用于多尺度生物系统建模，并通过粒子模拟提供了一个受控的评估环境。其重要性在于为从微观数据推断宏观生物动力学提供了一种有前景的工具，有助于克服生物系统复杂性带来的建模挑战。目前仅限于初步实验和钙扩散的评估，未来的工作可能需要扩展到更复杂的生物过程和更广泛的数据集。"}}
{"id": "2506.21275", "title": "Surrogate normal-forms for the numerical bifurcation and stability analysis of navier-stokes flows via machine learning", "authors": ["Alessandro Della Pia", "Dimitrios G. Patsatzis", "Gianluigi Rozza", "Lucia Russo", "Constantinos Siettos"], "summary": "Inspired by the Equation-Free multiscale modeling approach, we demonstrate\nhow the embed-learn-lift framework enables the construction of surrogate\nnormal-forms, namely minimal-dimensional reduced-order models (ROMs), from\nhigh-fidelity Navier-Stokes simulations. These surrogate models are then used\nfor efficient and accurate bifurcation and stability analysis. The framework\nproceeds in four steps. First, manifold learning reveals the intrinsic latent\ndimension of the high-dimensional spatio-temporal Navier-Stokes dynamics across\nparameter space. Second, we construct low-dimensional \"normal-form\" like ROMs\non this latent space using Gaussian Process Regression (GPR), capturing the\nemergent dynamics. Third, using these models, we apply numerical bifurcation\ntools to compute bifurcation diagrams and perform stability analysis in the\nlatent space. This includes tracing branches of limit cycles arising from\nAndronov-Hopf bifurcations - tasks intractable in full space due to\ncomputational cost. Finally, solving the pre-image problem allows\nreconstruction of the bifurcation structure in the original high-dimensional\nspace. We demonstrate the methodology on two canonical flows: wake flow past an\ninfinite circular cylinder and planar sudden-expansion channel flow. These\nexhibit Andronov-Hopf and pitchfork bifurcations, respectively, as Reynolds\nnumber increases. Our method identifies the latent dimensionality and\nconstructs GPR-based surrogate normal-forms that enable the tracing and\nstability analysis of bifurcating solutions, including limit cycles, their\nperiod, and stability via Floquet multipliers.", "comment": "26 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.21275v1", "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.21275v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过机器学习代理范式进行纳维-斯托克斯流的数值分岔与稳定性分析", "tldr": "本文提出了一种基于机器学习的“嵌入-学习-提升”框架，通过构建代理范式（低维降阶模型）来高效准确地分析高维纳维-斯托克斯流的分岔和稳定性，解决了全空间计算成本过高的问题。", "motivation": "在高维空间中对纳维-斯托克斯流进行分岔和稳定性分析因计算成本过高而难以实现，本文旨在通过构建代理范式来克服这一挑战，实现高效准确的分析。", "method": "该方法遵循“嵌入-学习-提升”框架的四个步骤：1) 流形学习揭示高维纳维-斯托克斯动力学的内在潜在维度；2) 使用高斯过程回归（GPR）在该潜在空间构建低维“范式”降阶模型；3) 利用这些模型，在潜在空间应用数值分岔工具计算分岔图并进行稳定性分析，包括追踪由Andronov-Hopf分岔引起的极限环；4) 解决原像问题以在原始高维空间中重建分岔结构。该方法在圆柱绕流和平面突扩流两种典型流动上进行了验证。", "result": "该方法成功识别了潜在维度，并构建了基于GPR的代理范式，从而能够追踪和稳定性分析分岔解，包括极限环、其周期及其通过Floquet乘子的稳定性。在圆柱绕流和平面突扩流两种典型流动上展示了该方法，它们分别呈现Andronov-Hopf和叉状分岔。", "conclusion": "本文提出的基于机器学习的代理范式方法，能够从高保真纳维-斯托克斯模拟中构建有效的降阶模型，从而实现高维流动的分岔和稳定性分析，解决了传统方法计算成本高昂的难题，提供了一种高效准确的分析工具。", "translation": "受无方程多尺度建模方法的启发，本文展示了“嵌入-学习-提升”框架如何能够从高保真纳维-斯托克斯模拟中构建代理范式，即最小维降阶模型（ROMs）。这些代理模型随后被用于高效准确的分岔和稳定性分析。该框架分为四个步骤。首先，流形学习揭示了跨参数空间的高维时空纳维-斯托克斯动力学的内在潜在维度。其次，我们使用高斯过程回归（GPR）在该潜在空间上构建了低维的“范式”类降阶模型，捕获了涌现动力学。第三，使用这些模型，我们应用数值分岔工具在潜在空间中计算分岔图并进行稳定性分析。这包括追踪由Andronov-Hopf分岔引起的极限环分支——这些任务在全空间中因计算成本过高而难以处理。最后，解决原像问题允许在原始高维空间中重建分岔结构。我们在两种典型流动上演示了该方法：无限圆柱体后的尾流和平面突扩通道流。随着雷诺数的增加，它们分别呈现Andronov-Hopf和叉状分岔。我们的方法识别了潜在维度并构建了基于GPR的代理范式，从而能够追踪和稳定性分析分岔解，包括极限环、其周期及其通过Floquet乘子的稳定性。", "summary": "本文提出了一种基于机器学习的“嵌入-学习-提升”框架，用于从高保真纳维-斯托克斯模拟中构建代理范式（最小维降阶模型），以实现高效准确的数值分岔和稳定性分析。该框架通过流形学习识别潜在维度，然后利用高斯过程回归在该潜在空间构建低维模型，再应用数值分岔工具进行分析，并最终在原始高维空间重建结果。该方法成功应用于圆柱绕流和平面突扩流，能够识别潜在维度并分析包括极限环在内的分岔解的稳定性，解决了高维流体动力学分析中计算成本高昂的问题。", "keywords": "纳维-斯托克斯流, 分岔分析, 机器学习, 降阶模型, 高斯过程回归", "comments": "该论文的创新点在于将流形学习和高斯过程回归等机器学习技术融入到流体力学分岔分析中，有效地解决了高维纳维-斯托克斯方程在全空间进行分岔和稳定性分析时计算成本过高的问题。通过构建低维代理范式，不仅提高了分析效率，还使得原本难以处理的复杂动力学行为（如极限环）的追踪和稳定性分析成为可能。该方法为复杂系统的高效数值分析提供了一个有价值的新范式。"}}
{"id": "2506.20898", "title": "Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction", "authors": ["Erfan Hajihashemi", "Yanning Shen"], "summary": "Online conformal prediction has demonstrated its capability to construct a\nprediction set for each incoming data point that covers the true label with a\npredetermined probability. To cope with potential distribution shift,\nmulti-model online conformal prediction has been introduced to select and\nleverage different models from a preselected candidate set. Along with the\nimproved flexibility, the choice of the preselected set also brings challenges.\nA candidate set that includes a large number of models may increase the\ncomputational complexity. In addition, the inclusion of irrelevant models with\npoor performance may negatively impact the performance and lead to\nunnecessarily large prediction sets. To address these challenges, we propose a\nnovel multi-model online conformal prediction algorithm that identifies a\nsubset of effective models at each time step by collecting feedback from a\nbipartite graph, which is refined upon receiving new data. A model is then\nselected from this subset to construct the prediction set, resulting in reduced\ncomputational complexity and smaller prediction sets. Additionally, we\ndemonstrate that using prediction set size as feedback, alongside model loss,\ncan significantly improve efficiency by constructing smaller prediction sets\nwhile still satisfying the required coverage guarantee. The proposed algorithms\nare proven to ensure valid coverage and achieve sublinear regret. Experiments\non real and synthetic datasets validate that the proposed methods construct\nsmaller prediction sets and outperform existing multi-model online conformal\nprediction approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20898v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20898v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "图结构反馈多模型集成在线共形预测", "tldr": "提出一种新的多模型在线共形预测算法，通过图结构反馈动态选择有效模型子集，以降低计算复杂性并生成更小的预测集，同时保持覆盖率。", "motivation": "现有的多模型在线共形预测面临挑战：候选模型集过大导致计算复杂性增加，以及包含不相关或性能差的模型会负面影响性能并导致过大的预测集。", "method": "提出一种新的多模型在线共形预测算法。该算法通过收集来自二分图的反馈，在每个时间步识别一个有效的模型子集，并随新数据进行细化。然后从该子集中选择一个模型来构建预测集。此外，证明了使用预测集大小作为反馈（除了模型损失）可以显著提高效率。", "result": "提出的算法被证明能确保有效的覆盖率并实现次线性遗憾。在真实和合成数据集上的实验验证了所提出的方法能够构建更小的预测集，并优于现有的多模型在线共形预测方法。", "conclusion": "本文提出了一种创新的多模型在线共形预测算法，通过动态选择有效模型子集和利用预测集大小作为反馈，有效解决了现有方法的局限性，在保证覆盖率的同时显著提高了效率并减小了预测集。", "translation": "在线共形预测已证明其有能力为每个传入数据点构建一个预测集，该预测集以预定概率覆盖真实标签。为了应对潜在的分布漂移，引入了多模型在线共形预测，从预选的候选集中选择和利用不同的模型。除了提高灵活性之外，预选集的选择也带来了挑战。包含大量模型的候选集可能会增加计算复杂性。此外，包含性能差的不相关模型可能会对性能产生负面影响，并导致不必要的大预测集。为了解决这些挑战，我们提出了一种新颖的多模型在线共形预测算法，该算法通过收集来自二分图的反馈，在每个时间步识别一个有效的模型子集，该二分图在接收新数据时得到完善。然后从该子集中选择一个模型来构建预测集，从而降低了计算复杂性并减小了预测集。此外，我们证明了使用预测集大小作为反馈，以及模型损失，可以通过构建更小的预测集来显著提高效率，同时仍满足所需的覆盖保证。所提出的算法被证明能确保有效的覆盖率并实现次线性遗憾。在真实和合成数据集上的实验验证了所提出的方法构建了更小的预测集，并优于现有的多模型在线共形预测方法。", "summary": "本文提出了一种新颖的多模型在线共形预测算法，旨在解决现有方法中候选模型集过大导致的计算复杂性和无效模型影响预测集大小的问题。该算法通过基于二分图反馈动态识别并选择有效的模型子集来构建预测集，从而显著降低了计算成本并生成了更紧凑的预测集。研究还发现，将预测集大小作为反馈信号（除模型损失外）能进一步提升效率。理论分析和实验结果均表明，该方法在保证预测覆盖率的同时，实现了更优的性能和更小的预测集。", "keywords": "在线共形预测, 多模型集成, 图结构反馈, 预测集, 分布漂移", "comments": "这篇论文通过引入图结构反馈机制，为多模型在线共形预测提供了一种新颖且高效的解决方案。其创新点在于动态选择有效模型子集，并利用预测集大小作为反馈，有效解决了计算复杂性和预测集过大的问题。这对于实际应用中需要处理大量候选模型和应对数据分布变化的场景具有重要意义。"}}
{"id": "2506.20979", "title": "3D Scene-Camera Representation with Joint Camera Photometric Optimization", "authors": ["Weichen Dai", "Kangcheng Ma", "Jiaxin Wang", "Kecen Pan", "Yuhang Ming", "Hua Zhang", "Wanzeng Kong"], "summary": "Representing scenes from multi-view images is a crucial task in computer\nvision with extensive applications. However, inherent photometric distortions\nin the camera imaging can significantly degrade image quality. Without\naccounting for these distortions, the 3D scene representation may inadvertently\nincorporate erroneous information unrelated to the scene, diminishing the\nquality of the representation. In this paper, we propose a novel 3D\nscene-camera representation with joint camera photometric optimization. By\nintroducing internal and external photometric model, we propose a full\nphotometric model and corresponding camera representation. Based on\nsimultaneously optimizing the parameters of the camera representation, the\nproposed method effectively separates scene-unrelated information from the 3D\nscene representation. Additionally, during the optimization of the photometric\nparameters, we introduce a depth regularization to prevent the 3D scene\nrepresentation from fitting scene-unrelated information. By incorporating the\ncamera model as part of the mapping process, the proposed method constructs a\ncomplete map that includes both the scene radiance field and the camera\nphotometric model. Experimental results demonstrate that the proposed method\ncan achieve high-quality 3D scene representations, even under conditions of\nimaging degradation, such as vignetting and dirt.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20979v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20979v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "具有联合相机光度优化的三维场景-相机表示", "tldr": "本文提出了一种新颖的三维场景-相机表示方法，通过联合优化相机光度模型来有效去除图像畸变带来的场景无关信息，从而在存在图像退化的情况下也能获得高质量的三维场景表示。", "motivation": "从多视图图像表示场景是计算机视觉中的关键任务，但相机成像固有的光度畸变会显著降低图像质量，导致三维场景表示包含错误信息，从而降低表示质量。", "method": "提出了一种新颖的三维场景-相机表示，通过联合相机光度优化。引入内部和外部光度模型，构建了一个完整的光度模型和相应的相机表示。通过同时优化相机表示参数，有效分离了场景无关信息。此外，在光度参数优化过程中引入深度正则化，以防止三维场景表示拟合场景无关信息。该方法将相机模型作为映射过程的一部分，构建了一个包含场景辐射场和相机光度模型的完整映射。", "result": "实验结果表明，即使在渐晕和污垢等成像退化条件下，所提出的方法也能实现高质量的三维场景表示。", "conclusion": "通过联合相机光度优化和深度正则化，该方法能够有效处理相机光度畸变，从而在存在图像退化的情况下也能获得高质量的三维场景表示。", "translation": "从多视图图像表示场景是计算机视觉中的一项关键任务，具有广泛的应用。然而，相机成像中固有的光度畸变会显著降低图像质量。如果不考虑这些畸变，三维场景表示可能会无意中包含与场景无关的错误信息，从而降低表示质量。在本文中，我们提出了一种具有联合相机光度优化的新型三维场景-相机表示方法。通过引入内部和外部光度模型，我们提出了一个完整的光度模型和相应的相机表示。通过同时优化相机表示的参数，所提出的方法有效地将与场景无关的信息从三维场景表示中分离出来。此外，在光度参数优化过程中，我们引入了深度正则化，以防止三维场景表示拟合与场景无关的信息。通过将相机模型作为映射过程的一部分，所提出的方法构建了一个包含场景辐射场和相机光度模型的完整映射。实验结果表明，即使在渐晕和污垢等成像退化条件下，所提出的方法也能实现高质量的三维场景表示。", "summary": "本文提出了一种新颖的三维场景-相机表示方法，旨在解决相机光度畸变对三维场景表示质量的影响。该方法通过引入内部和外部光度模型，并进行联合相机光度优化，能够有效分离图像中的场景无关信息。此外，通过引入深度正则化，进一步防止三维场景表示拟合错误信息。实验证明，该方法即使在成像退化条件下也能获得高质量的三维场景表示。", "keywords": "三维场景表示, 相机光度优化, 图像畸变, 深度正则化, 多视图图像", "comments": "该论文创新性地将相机光度模型纳入三维场景表示的映射过程中，并通过联合优化和深度正则化有效解决了图像畸变对场景重建质量的影响。这对于实际应用中，尤其是在复杂或受损成像条件下获取高质量三维数据具有重要意义。"}}
{"id": "2506.21252", "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.", "comment": "ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2506.21252v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21252v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Agent-RewardBench：迈向真实世界多模态智能体在感知、规划和安全方面的统一奖励建模基准", "tldr": "提出Agent-RewardBench，一个用于评估多模态大语言模型在真实世界智能体任务中奖励建模能力的统一基准，发现当前模型性能有限，需专门训练。", "motivation": "多模态大语言模型（MLLMs）在真实世界任务中展现潜力，但缺乏外部反馈导致智能体难以自我纠正和泛化。奖励模型是一个有前景的方法，但缺乏如何为智能体选择奖励模型的明确指导，因此迫切需要构建一个针对智能体的奖励基准。", "method": "提出Agent-RewardBench基准。该基准具有三个关键特征：1) 涵盖感知、规划和安全等7个真实世界智能体场景的多维度评估；2) 步级奖励评估，提供更细粒度的性能视图；3) 适当难度和高质量，通过从10个不同模型中采样、难度控制和人工验证确保数据完整性。", "result": "实验表明，即使是最先进的多模态模型也表现出有限的性能。", "conclusion": "这凸显了在智能体奖励建模方面进行专门训练的必要性。", "translation": "随着多模态大语言模型（MLLMs）的进步，多模态智能体在网络导航和具身智能等真实世界任务中展现出前景。然而，由于缺乏外部反馈的限制，这些智能体在自我纠正和泛化方面面临困难。一个有前景的方法是使用奖励模型作为外部反馈，但目前尚不清楚如何为智能体选择奖励模型。因此，迫切需要建立一个针对智能体的奖励基准。为解决这些挑战，我们提出了Agent-RewardBench，一个旨在评估MLLMs中奖励建模能力的基准。该基准具有三个关键特征：(1) 多维度和真实世界智能体场景评估。它涵盖感知、规划和安全，包含7个场景；(2) 步级奖励评估。它允许评估任务中每个单独步骤的智能体能力，在规划过程中提供更细粒度的性能视图；(3) 适当难度和高质量。我们仔细从10个不同的模型中采样，通过难度控制来保持任务挑战性，并通过人工验证来确保数据的完整性。实验表明，即使是最先进的多模态模型也表现出有限的性能，这凸显了在智能体奖励建模方面进行专门训练的必要性。代码可在github上获取。", "summary": "该论文提出了Agent-RewardBench，一个用于评估多模态大语言模型在真实世界多模态智能体中奖励建模能力的统一基准。该基准涵盖感知、规划和安全等多个维度，提供步级评估，并确保高质量和适当难度。实验结果表明，现有最先进模型在该基准上表现有限，强调了专门训练智能体奖励模型的必要性。", "keywords": "多模态智能体, 奖励建模, 基准测试, 感知, 规划, 安全", "comments": "该论文提出的Agent-RewardBench填补了多模态智能体奖励建模评估的空白，其多维度、步级评估和高质量数据确保了基准的全面性和有效性。发现现有模型性能不足，为未来研究指明了方向，具有重要的实践意义。"}}
{"id": "2506.21121", "title": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Lu Zhang", "Peiliang Li", "Shaojie Shen"], "summary": "Trajectory prediction for surrounding agents is a challenging task in\nautonomous driving due to its inherent uncertainty and underlying\nmultimodality. Unlike prevailing data-driven methods that primarily rely on\nsupervised learning, in this paper, we introduce a novel Graph-oriented Inverse\nReinforcement Learning (GoIRL) framework, which is an IRL-based predictor\nequipped with vectorized context representations. We develop a feature adaptor\nto effectively aggregate lane-graph features into grid space, enabling seamless\nintegration with the maximum entropy IRL paradigm to infer the reward\ndistribution and obtain the policy that can be sampled to induce multiple\nplausible plans. Furthermore, conditioned on the sampled plans, we implement a\nhierarchical parameterized trajectory generator with a refinement module to\nenhance prediction accuracy and a probability fusion strategy to boost\nprediction confidence. Extensive experimental results showcase our approach not\nonly achieves state-of-the-art performance on the large-scale Argoverse &\nnuScenes motion forecasting benchmarks but also exhibits superior\ngeneralization abilities compared to existing supervised models.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21121v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21121v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GoIRL：面向图的逆强化学习用于多模态轨迹预测", "tldr": "本文提出了一种名为GoIRL的图导向逆强化学习框架，用于自动驾驶中的多模态轨迹预测，并在大型基准测试中取得了最先进的性能和更好的泛化能力。", "motivation": "自动驾驶中周围代理的轨迹预测由于其固有的不确定性和潜在的多模态性而具有挑战性。", "method": "本文提出了一种新颖的GoIRL（Graph-oriented Inverse Reinforcement Learning）框架，这是一种基于IRL的预测器，配备矢量化上下文表示。开发了一个特征适配器，将车道图特征有效地聚合到网格空间中，并与最大熵IRL范式集成，以推断奖励分布并获得可以采样以诱导多个合理计划的策略。此外，以采样的计划为条件，实现了一个分层参数化轨迹生成器，带有细化模块以提高预测精度，并采用概率融合策略以提高预测置信度。", "result": "该方法在大型Argoverse和nuScenes运动预测基准测试中不仅取得了最先进的性能，而且与现有监督模型相比，展现出卓越的泛化能力。", "conclusion": "GoIRL框架通过结合逆强化学习和图导向特征表示，有效解决了自动驾驶中的多模态轨迹预测难题，并在性能和泛化能力上超越了现有方法。", "translation": "周围代理的轨迹预测由于其固有的不确定性和潜在的多模态性，在自动驾驶中是一项具有挑战性的任务。与主要依赖监督学习的现有数据驱动方法不同，在本文中，我们引入了一种新颖的图导向逆强化学习（GoIRL）框架，这是一种基于IRL的预测器，配备了矢量化上下文表示。我们开发了一个特征适配器，以有效地将车道图特征聚合到网格空间中，从而实现与最大熵IRL范式的无缝集成，以推断奖励分布并获得可以采样以诱导多个合理计划的策略。此外，以采样的计划为条件，我们实现了一个分层参数化轨迹生成器，带有细化模块以提高预测精度，并采用概率融合策略以提高预测置信度。广泛的实验结果表明，我们的方法不仅在大型Argoverse和nuScenes运动预测基准测试中取得了最先进的性能，而且与现有监督模型相比，展现出卓越的泛化能力。", "summary": "本文提出GoIRL，一个面向图的逆强化学习框架，用于自动驾驶中的多模态轨迹预测。该框架利用特征适配器将车道图特征整合到网格空间，并通过最大熵IRL推断奖励分布以生成多模态计划。在此基础上，通过分层轨迹生成器和概率融合策略提升预测精度和置信度。实验证明，GoIRL在Argoverse和nuScenes基准测试中表现出最先进的性能和优越的泛化能力。", "keywords": "轨迹预测, 逆强化学习, 图神经网络, 自动驾驶, 多模态", "comments": "本文的创新之处在于将逆强化学习（IRL）与图神经网络相结合，用于解决自动驾驶中的多模态轨迹预测问题。与传统的监督学习方法不同，GoIRL通过IRL推断奖励函数，从而更好地捕捉潜在的驾驶意图和多样的行为模式。其分层轨迹生成器和概率融合策略也有效提升了预测的准确性和置信度，展现了IRL在复杂动态环境预测中的潜力。"}}
{"id": "2506.20983", "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation", "authors": ["Wenjie Xuan", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "summary": "Recent works favored dense signals (e.g., depth, DensePose), as an\nalternative to sparse signals (e.g., OpenPose), to provide detailed spatial\nguidance for pose-guided text-to-image generation. However, dense\nrepresentations raised new challenges, including editing difficulties and\npotential inconsistencies with textual prompts. This fact motivates us to\nrevisit sparse signals for pose guidance, owing to their simplicity and\nshape-agnostic nature, which remains underexplored. This paper proposes a novel\nSpatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust\ncontrollability for pose-guided image generation. Specifically, we extend\nOpenPose to a learnable spatial representation, making keypoint embeddings\ndiscriminative and expressive. Additionally, we introduce keypoint concept\nlearning, which encourages keypoint tokens to attend to the spatial positions\nof each keypoint, thus improving pose alignment. Experiments on animal- and\nhuman-centric image generation tasks demonstrate that our method outperforms\nrecent spatially controllable T2I generation approaches under sparse-pose\nguidance and even matches the performance of dense signal-based methods.\nMoreover, SP-Ctrl shows promising capabilities in diverse and cross-species\ngeneration through sparse signals. Codes will be available at\nhttps://github.com/DREAMXFAR/SP-Ctrl.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20983v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20983v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "重新思考用于姿态引导文本到图像生成的稀疏信号", "tldr": "本文提出SP-Ctrl，一个基于稀疏信号（如OpenPose）的ControlNet，解决了密集信号在姿态引导文本到图像生成中的问题，实现了与密集信号相当甚至超越现有稀疏信号方法的性能，并支持多样化和跨物种生成。", "motivation": "现有姿态引导文本到图像生成方法多采用密集信号（如深度、DensePose），但存在编辑困难和与文本提示不一致的问题。稀疏信号（如OpenPose）因其简单性和形状无关性而未被充分探索，这促使作者重新审视并利用稀疏信号。", "method": "本文提出了一种新颖的空间姿态控制网络（Spatial-Pose ControlNet, SP-Ctrl）。具体而言，SP-Ctrl将OpenPose扩展为可学习的空间表示，使关键点嵌入更具判别性和表达性。此外，SP-Ctrl引入了关键点概念学习，鼓励关键点令牌关注每个关键点的空间位置，从而改善姿态对齐。", "result": "在以动物和人类为中心的图像生成任务上的实验表明，所提出的方法在稀疏姿态引导下优于近期空间可控的T2I生成方法，甚至能与基于密集信号的方法性能相匹配。此外，SP-Ctrl通过稀疏信号在多样化和跨物种生成方面显示出前景广阔的能力。", "conclusion": "本文成功地重新利用了稀疏信号进行姿态引导的文本到图像生成，通过提出的SP-Ctrl，不仅解决了密集信号的挑战，还在性能上达到了甚至超越了现有方法，并在多样性和跨物种生成方面展现了巨大潜力。", "translation": "最近的工作倾向于使用密集信号（例如，深度、DensePose）作为稀疏信号（例如，OpenPose）的替代品，为姿态引导的文本到图像生成提供详细的空间指导。然而，密集表示带来了新的挑战，包括编辑困难和与文本提示潜在的不一致性。这一事实促使我们重新审视用于姿态引导的稀疏信号，因为它们具有简单性和形状无关的特性，而这仍然未被充分探索。本文提出了一种新颖的空间姿态控制网络（Spatial-Pose ControlNet, SP-Ctrl），为稀疏信号赋予了强大的可控性，用于姿态引导的图像生成。具体而言，我们将OpenPose扩展为可学习的空间表示，使关键点嵌入具有判别性和表达性。此外，我们引入了关键点概念学习，这鼓励关键点令牌关注每个关键点的空间位置，从而改善姿态对齐。在以动物和人类为中心的图像生成任务上的实验表明，我们的方法在稀疏姿态引导下优于近期空间可控的T2I生成方法，甚至能与基于密集信号的方法性能相匹配。此外，SP-Ctrl通过稀疏信号在多样化和跨物种生成方面显示出前景广阔的能力。代码将在https://github.com/DREAMXFAR/SP-Ctrl 提供。", "summary": "本文针对姿态引导文本到图像生成中密集信号存在的编辑困难和文本不一致问题，提出重新审视并利用稀疏信号。为此，作者开发了Spatial-Pose ControlNet (SP-Ctrl)，它通过将OpenPose扩展为可学习的空间表示并引入关键点概念学习，显著提升了稀疏信号的可控性和姿态对齐能力。实验证明，SP-Ctrl在稀疏姿态引导下表现优异，甚至能与密集信号方法媲美，并展现了在多样化和跨物种生成方面的强大潜力。", "keywords": "稀疏信号, 姿态引导生成, 文本到图像, ControlNet, 关键点学习", "comments": "这篇论文通过重新思考并有效利用稀疏信号，为姿态引导的文本到图像生成领域提供了一个创新且实用的解决方案。其创新点在于克服了传统密集信号的缺点，并证明了稀疏信号在特定场景下可以达到甚至超越密集信号的性能。SP-Ctrl的设计，特别是可学习的OpenPose扩展和关键点概念学习，是提升稀疏信号表达力和控制力的关键。这项工作对于推动T2I生成在更广泛应用场景中的发展具有重要意义，尤其是在需要灵活编辑和多样化生成的场景。"}}
{"id": "2506.21274", "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness", "comment": "(Submitted for publication)", "pdf_url": "http://arxiv.org/pdf/2506.21274v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21274v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "猫鼠游戏——伪造文本生成能否超越检测系统？", "tldr": "研究发现，Gemini生成欺骗性文本的能力有所增强，而GPT则没有，这表明即使对于更大的模型，伪造文本的可靠检测可能仍然可行，但新的模型架构可能会提高其欺骗性。", "motivation": "大型语言模型能够生成令人信服的“伪造文本”，而检测这些文本的方法也在不断发展，这似乎预示着一场“军备竞赛”。该研究旨在探讨模型超越检测器的能力是否会达到一个平台期，特别是考虑到较简单分类器在资源有限的情况下也能达到良好的检测准确率。", "method": "研究人员通过检查统计分类器识别“伪造文本”的能力来探究模型能否击败检测器，这些伪造文本以经典侦探小说的风格生成。他们比较了Gemini和GPT模型在0.5版本升级后生成欺骗性文本的能力变化。", "result": "在0.5版本更新后，Gemini模型生成欺骗性文本的能力有所增强，而GPT模型则没有表现出这种增强。", "conclusion": "研究结果表明，即使对于越来越大的模型，可靠的伪造文本检测可能仍然可行。然而，新的模型架构可能会提高其欺骗性，这可能对未来的检测构成挑战。", "translation": "大型语言模型可以在学术写作、产品评论和政治新闻等领域生成令人信服的“伪造文本”。人们已经研究了许多检测人工智能生成文本的方法。虽然这似乎预示着一场无休止的“军备竞赛”，但我们注意到，较新的大型语言模型使用越来越多的参数、训练数据和能源，而相对简单的分类器则以适度的资源展示了良好的检测准确性。为了探讨模型击败检测器的能力是否会因此达到一个平台期，我们研究了统计分类器识别经典侦探小说风格的“伪造文本”的能力。在0.5版本更新后，我们发现Gemini生成欺骗性文本的能力有所增强，而GPT则没有。这表明，即使对于越来越大的模型，可靠的伪造文本检测可能仍然可行，尽管新的模型架构可能会提高它们的欺骗性。", "summary": "本文探讨了大型语言模型（LLMs）生成“伪造文本”与检测系统之间的“猫鼠游戏”。文章旨在探究LLMs欺骗检测器的能力是否会达到一个平台期，尤其考虑到简单的分类器也能有效检测。通过测试统计分类器在经典侦探小说风格的伪造文本上的表现，研究发现Gemini在0.5版本更新后生成欺骗性文本的能力有所提升，而GPT则没有。这表明，即使面对更大的模型，可靠的伪造文本检测可能仍具可行性，尽管新型模型架构可能会增加欺骗性。", "keywords": "伪造文本, 大型语言模型, 文本检测, 欺骗性, 统计分类器", "comments": "该论文探讨了当前大型语言模型时代一个非常重要且紧迫的问题。其创新之处在于审视了“军备竞赛”的动态，并具体比较了不同LLM（Gemini与GPT）在特定文本风格下欺骗能力的演变。检测可能仍可行这一发现意义重大，但对新架构可能带来挑战的警告是未来研究的关键考量。"}}
{"id": "2506.20916", "title": "Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "Deep reinforcement learning has been extensively studied in decision-making\nprocesses and has demonstrated superior performance over conventional\napproaches in various fields, including radar resource management (RRM).\nHowever, a notable limitation of neural networks is their ``black box\" nature\nand recent research work has increasingly focused on explainable AI (XAI)\ntechniques to describe the rationale behind neural network decisions. One\npromising XAI method is local interpretable model-agnostic explanations (LIME).\nHowever, the sampling process in LIME ignores the correlations between\nfeatures. In this paper, we propose a modified LIME approach that integrates\ndeep learning (DL) into the sampling process, which we refer to as DL-LIME. We\nemploy DL-LIME within deep reinforcement learning for radar resource\nmanagement. Numerical results show that DL-LIME outperforms conventional LIME\nin terms of both fidelity and task performance, demonstrating superior\nperformance with both metrics. DL-LIME also provides insights on which factors\nare more important in decision making for radar resource management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20916v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20916v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "雷达资源管理中的可解释人工智能：深度强化学习中改进的 LIME", "tldr": "本文提出了一种名为 DL-LIME 的改进 LIME 方法，它将深度学习集成到采样过程中，以解决传统 LIME 忽略特征相关性的问题。数值结果表明，DL-LIME 在雷达资源管理中，在保真度和任务性能方面均优于传统 LIME。", "motivation": "深度强化学习在雷达资源管理 (RRM) 等决策过程中表现出色，但其神经网络的“黑箱”性质限制了其可解释性。现有的可解释人工智能 (XAI) 方法如 LIME 在采样过程中忽略了特征间的相关性，需要一种改进的方法来提供更好的解释性。", "method": "本文提出了一种名为 DL-LIME 的改进 LIME 方法。DL-LIME 将深度学习 (DL) 集成到 LIME 的采样过程中，以解决传统 LIME 忽略特征相关性的问题。该方法应用于深度强化学习中的雷达资源管理。", "result": "数值结果表明，DL-LIME 在保真度和任务性能方面均优于传统 LIME。DL-LIME 在这两项指标上都表现出卓越的性能。此外，DL-LIME 还能揭示在雷达资源管理决策中哪些因素更重要。", "conclusion": "DL-LIME 是一种改进的 LIME 方法，通过将深度学习集成到采样过程中，有效解决了传统 LIME 的局限性，并在雷达资源管理中实现了更好的解释性和性能。", "translation": "深度强化学习在决策过程中得到了广泛研究，并在包括雷达资源管理 (RRM) 在内的各个领域中表现出优于传统方法的性能。然而，神经网络的一个显著局限性是其“黑箱”性质，最近的研究工作越来越关注可解释人工智能 (XAI) 技术，以描述神经网络决策背后的原理。一种很有前景的 XAI 方法是局部可解释模型无关解释 (LIME)。然而，LIME 中的采样过程忽略了特征之间的相关性。在本文中，我们提出了一种改进的 LIME 方法，该方法将深度学习 (DL) 集成到采样过程中，我们称之为 DL-LIME。我们将 DL-LIME 应用于深度强化学习中的雷达资源管理。数值结果表明，DL-LIME 在保真度和任务性能方面均优于传统 LIME，在这两项指标上均表现出卓越的性能。DL-LIME 还提供了关于哪些因素在雷达资源管理决策中更重要的见解。", "summary": "本文针对深度强化学习在雷达资源管理中的“黑箱”问题，提出了一种改进的 LIME (局部可解释模型无关解释) 方法，命名为 DL-LIME。传统 LIME 在采样时忽略特征相关性，而 DL-LIME 通过将深度学习集成到采样过程中来解决这一问题。实验结果表明，DL-LIME 在保真度和任务性能上均优于传统 LIME，并且能有效揭示雷达资源管理决策中的关键因素。", "keywords": "可解释人工智能, 深度强化学习, 雷达资源管理, LIME, DL-LIME", "comments": "本文的创新点在于提出了 DL-LIME，通过将深度学习引入 LIME 的采样过程，解决了传统 LIME 忽略特征相关性的问题，从而提高了可解释性和性能。这对于将深度强化学习应用于关键决策领域（如雷达资源管理）具有重要意义，因为它能帮助理解模型决策，增强信任度。"}}
{"id": "2506.20986", "title": "EVA: Mixture-of-Experts Semantic Variant Alignment for Compositional Zero-Shot Learning", "authors": ["Xiao Zhang", "Yongqiang Ma", "Haodong Jing", "Nanning Zheng"], "summary": "Compositional Zero-Shot Learning (CZSL) investigates compositional\ngeneralization capacity to recognize unknown state-object pairs based on\nlearned primitive concepts. Existing CZSL methods typically derive primitives\nfeatures through a simple composition-prototype mapping, which is suboptimal\nfor a set of individuals that can be divided into distinct semantic subsets.\nMoreover, the all-to-one cross-modal primitives matching neglects compositional\ndivergence within identical states or objects, limiting fine-grained\nimage-composition alignment. In this study, we propose EVA, a\nMixture-of-Experts Semantic Variant Alignment framework for CZSL. Specifically,\nwe introduce domain-expert adaption, leveraging multiple experts to achieve\ntoken-aware learning and model high-quality primitive representations. To\nenable accurate compositional generalization, we further present semantic\nvariant alignment to select semantically relevant representation for\nimage-primitives matching. Our method significantly outperforms other\nstate-of-the-art CZSL methods on three popular benchmarks in both closed- and\nopen-world settings, demonstrating the efficacy of the proposed insight.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20986v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20986v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EVA：用于组合零样本学习的专家混合语义变体对齐", "tldr": "EVA提出了一种专家混合语义变体对齐框架，通过域专家适应和语义变体对齐，显著提高了组合零样本学习的性能。", "motivation": "现有的组合零样本学习方法通常通过简单的组合原型映射来获取原始特征，这对于可分为不同语义子集的个体来说是次优的。此外，一对多的跨模态原始匹配忽略了相同状态或对象内的组合差异，限制了细粒度的图像-组合对齐。", "method": "本文提出EVA，一个用于组合零样本学习的专家混合语义变体对齐框架。具体而言，引入了域专家适应，利用多个专家实现令牌感知学习和建模高质量的原始表示。为了实现准确的组合泛化，进一步提出了语义变体对齐，以选择语义相关的表示进行图像-原始匹配。", "result": "EVA在三种流行的基准测试中，在封闭世界和开放世界设置下，均显著优于其他最先进的组合零样本学习方法。", "conclusion": "所提出的EVA框架通过其域专家适应和语义变体对齐机制，有效解决了现有组合零样本学习方法的局限性，并显著提高了性能。", "translation": "组合零样本学习（CZSL）研究了基于学习到的原始概念识别未知状态-对象对的组合泛化能力。现有CZSL方法通常通过简单的组合原型映射来获取原始特征，这对于可分为不同语义子集的个体来说是次优的。此外，一对多的跨模态原始匹配忽略了相同状态或对象内的组合差异，限制了细粒度的图像-组合对齐。在本研究中，我们提出了EVA，一个用于CZSL的专家混合语义变体对齐框架。具体而言，我们引入了域专家适应，利用多个专家实现令牌感知学习和建模高质量的原始表示。为了实现准确的组合泛化，我们进一步提出了语义变体对齐，以选择语义相关的表示进行图像-原始匹配。我们的方法在三种流行的基准测试中，在封闭世界和开放世界设置下，均显著优于其他最先进的CZSL方法，证明了所提出见解的有效性。", "summary": "本文针对组合零样本学习（CZSL）中现有方法在处理语义子集和组合差异方面的不足，提出了一种名为EVA的专家混合语义变体对齐框架。EVA通过引入域专家适应来实现令牌感知学习和高质量的原始表示，并利用语义变体对齐来选择语义相关的表示进行图像-原始匹配，从而实现准确的组合泛化。实验结果表明，EVA在多个基准测试中显著优于现有最先进的CZSL方法。", "keywords": "组合零样本学习, 专家混合, 语义变体对齐, 域专家适应, 零样本学习", "comments": "该论文通过引入“专家混合”和“语义变体对齐”的概念，为组合零样本学习提供了创新的解决方案。特别是，域专家适应能够更好地处理不同语义子集，而语义变体对齐则解决了细粒度图像-组合对齐的问题，这些都是现有方法面临的关键挑战。其在多个基准测试上的显著性能提升，凸显了该方法的有效性和重要性。"}}
{"id": "2506.21285", "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.21285v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21285v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "双重检查器：通过自我批判微调增强慢思考大型语言模型的推理能力", "tldr": "引入Double-Checker框架，通过自我批判微调显著提升慢思考大型语言模型（LLMs）的推理能力。", "motivation": "慢思考大型语言模型（LLMs）虽然展现出反思性推理能力，但它们生成有信息量的批判和完善先前解决方案的能力有限。", "method": "本文引入了Double-Checker框架，通过在1,730个精心策划的自我批判实例上进行微调，使长链思维（long-CoT）LLMs能够在推理过程中迭代地批判和完善其输出，直到它们在自我生成的批判下评估解决方案是正确的。", "result": "Double-Checker在全面的推理基准测试中表现出显著效果，迭代自我批判显著增强了长链思维（long-CoT）LLMs的推理能力。特别地，在具有挑战性的AIME基准测试中，其pass@1性能从原始长链思维（long-CoT）LLMs的4.4%提高到18.2%。", "conclusion": "这些结果表明，开发能够进行结构化自我批判的、更值得信赖和有效的LLMs是一个有前景的方向。", "translation": "尽管慢思考大型语言模型（LLMs）表现出类似反思的推理能力，通常被称为“顿悟时刻”，但它们生成有信息量的批判和完善先前解决方案的能力仍然有限。在本文中，我们引入了Double-Checker，一个旨在通过促进明确的自我批判和对其先前解决方案的迭代完善来增强慢思考LLMs推理能力的原则性框架。通过在我们精心策划的1,730个自我批判实例上进行微调，Double-Checker使长CoT LLMs能够在推理过程中迭代地批判和完善其输出，直到它们在自我生成的批判下评估其解决方案是正确的。我们在全面的推理基准测试套件中验证了Double-Checker的功效，证明迭代自我批判显著增强了长CoT LLMs的推理能力。值得注意的是，与原始的长CoT LLMs相比，我们的Double-Checker在具有挑战性的AIME基准测试中将pass@1性能从4.4%提高到18.2%。这些结果突出了开发更值得信赖和有效的、能够进行结构化自我批判的LLMs的一个有前景的方向。", "summary": "本文提出了Double-Checker框架，旨在通过自我批判微调提升慢思考大型语言模型（LLMs）的推理能力。该框架通过在1,730个自我批判实例上进行微调，使LLMs能够迭代地批判并完善其输出，直到自我评估为正确。实验结果表明，Double-Checker显著提高了长链思维（long-CoT）LLMs在各种推理基准上的性能，尤其是在AIME测试中将pass@1从4.4%提升至18.2%，展示了构建更可靠、能自我批判LLMs的潜力。", "keywords": "大型语言模型, 自我批判, 微调, 推理能力, 慢思考LLM", "comments": "这篇论文的创新点在于引入了一个“自我批判”的微调机制，使LLMs能够像人类一样进行反思和迭代改进，而不是仅仅生成一次性答案。这种方法对于提升LLMs在复杂推理任务上的可靠性和准确性具有重要意义，尤其是在需要多步骤思考和错误纠正的场景中。其方法论清晰，并通过具体性能提升（如AIME上的显著提升）验证了其有效性，为未来开发更智能、更值得信赖的LLMs提供了新思路。"}}
{"id": "2506.20988", "title": "Segment Anything in Pathology Images with Natural Language", "authors": ["Zhixuan Chen", "Junlin Hou", "Liqi Lin", "Yihui Wang", "Yequan Bie", "Xi Wang", "Yanning Zhou", "Ronald Cheong Kin Chan", "Hao Chen"], "summary": "Pathology image segmentation is crucial in computational pathology for\nanalyzing histological features relevant to cancer diagnosis and prognosis.\nHowever, current methods face major challenges in clinical applications due to\nlimited annotated data and restricted category definitions. To address these\nlimitations, we propose PathSegmentor, the first text-prompted segmentation\nfoundation model designed specifically for pathology images. We also introduce\nPathSeg , the largest and most comprehensive dataset for pathology\nsegmentation, built from 17 public sources and containing 275k image-mask-label\ntriples across 160 diverse categories. With PathSegmentor, users can perform\nsemantic segmentation using natural language prompts, eliminating the need for\nlaborious spatial inputs such as points or boxes. Extensive experiments\ndemonstrate that PathSegmentor outperforms specialized models with higher\naccuracy and broader applicability, while maintaining a compact architecture.\nIt significantly surpasses existing spatial- and text-prompted models by 0.145\nand 0.429 in overall Dice scores, respectively, showing strong robustness in\nsegmenting complex structures and generalizing to external datasets. Moreover,\nPathSegmentor's outputs enhance the interpretability of diagnostic models\nthrough feature importance estimation and imaging biomarker discovery, offering\npathologists evidence-based support for clinical decision-making. This work\nadvances the development of explainable AI in precision oncology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20988v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20988v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用自然语言在病理图像中分割一切", "tldr": "PathSegmentor是一个基于自然语言提示的病理图像分割基础模型，解决了数据标注和类别定义限制，并在PathSeg数据集上表现优异，提升了诊断可解释性。", "motivation": "当前病理图像分割方法在临床应用中面临标注数据有限和类别定义受限的挑战。", "method": "提出了PathSegmentor，首个针对病理图像的文本提示分割基础模型。同时构建了PathSeg数据集，包含17个公共来源的27.5万个图像-掩膜-标签三元组，涵盖160个类别。PathSegmentor允许用户使用自然语言提示进行语义分割，无需繁琐的空间输入，例如点或框。", "result": "PathSegmentor在准确性和适用性方面优于专用模型，并保持紧凑架构。在整体Dice分数上，它分别比现有空间提示模型和文本提示模型高出0.145和0.429。在分割复杂结构和泛化到外部数据集方面表现出强大的鲁棒性。其输出通过特征重要性估计和成像生物标志物发现，增强了诊断模型的可解释性。", "conclusion": "这项工作推动了精准肿瘤学中可解释AI的发展，为病理学家提供了循证支持。", "translation": "病理图像分割在计算病理学中至关重要，用于分析与癌症诊断和预后相关的组织学特征。然而，当前方法在临床应用中面临标注数据有限和类别定义受限的重大挑战。为了解决这些局限性，我们提出了PathSegmentor，这是首个专为病理图像设计的文本提示分割基础模型。我们还引入了PathSeg，这是迄今为止最大、最全面的病理分割数据集，由17个公共来源构建，包含160个不同类别的27.5万个图像-掩膜-标签三元组。通过PathSegmentor，用户可以使用自然语言提示执行语义分割，无需繁琐的空间输入，例如点或框。大量实验表明，PathSegmentor以更高的准确性和更广泛的适用性优于专用模型，同时保持紧凑的架构。在整体Dice分数上，它分别显著超越现有空间提示模型和文本提示模型0.145和0.429，在分割复杂结构和泛化到外部数据集方面显示出强大的鲁棒性。此外，PathSegmentor的输出通过特征重要性估计和成像生物标志物发现，增强了诊断模型的可解释性，为病理学家提供循证支持以辅助临床决策。这项工作推动了精准肿瘤学中可解释AI的发展。", "summary": "PathSegmentor是一个开创性的文本提示分割基础模型，专为病理图像分析设计，旨在克服现有方法在标注数据和类别定义上的局限。该模型结合了大规模的PathSeg数据集（包含27.5万个图像-掩膜-标签三元组和160个类别），允许用户通过自然语言提示进行精确的语义分割。实验证明，PathSegmentor在准确性、适用性和泛化能力上均显著优于现有模型，并能增强诊断模型的可解释性，为精准肿瘤学中的可解释AI发展做出贡献。", "keywords": "病理图像分割, 自然语言处理, 基础模型, PathSegmentor, 可解释AI", "comments": "这项工作通过引入首个文本提示的病理图像分割基础模型PathSegmentor和迄今最大的病理分割数据集PathSeg，显著推动了计算病理学领域的发展。其创新之处在于利用自然语言提示进行分割，极大地降低了对繁琐空间输入的依赖，解决了标注数据稀缺和类别定义受限的痛点。此外，该模型在提升诊断模型可解释性方面的潜力，对于临床决策支持和精准肿瘤学具有重要意义。"}}
{"id": "2506.20927", "title": "Interpretable Representation Learning for Additive Rule Ensembles", "authors": ["Shahrzad Behzadimanesh", "Pierre Le Bodic", "Geoffrey I. Webb", "Mario Boley"], "summary": "Small additive ensembles of symbolic rules offer interpretable prediction\nmodels. Traditionally, these ensembles use rule conditions based on\nconjunctions of simple threshold propositions $x \\geq t$ on a single input\nvariable $x$ and threshold $t$, resulting geometrically in axis-parallel\npolytopes as decision regions. While this form ensures a high degree of\ninterpretability for individual rules and can be learned efficiently using the\ngradient boosting approach, it relies on having access to a curated set of\nexpressive and ideally independent input features so that a small ensemble of\naxis-parallel regions can describe the target variable well. Absent such\nfeatures, reaching sufficient accuracy requires increasing the number and\ncomplexity of individual rules, which diminishes the interpretability of the\nmodel. Here, we extend classical rule ensembles by introducing logical\npropositions with learnable sparse linear transformations of input variables,\ni.e., propositions of the form $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$, where\n$\\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as\ngeneral polytopes with oblique faces. We propose a learning method using\nsequential greedy optimization based on an iteratively reweighted formulation\nof logistic regression. Experimental results demonstrate that the proposed\nmethod efficiently constructs rule ensembles with the same test risk as\nstate-of-the-art methods while significantly reducing model complexity across\nten benchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20927v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20927v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可解释的加性规则集成表示学习", "tldr": "本文通过引入可学习的稀疏线性变换，扩展了传统规则集成，以在保持相同预测性能的同时显著降低模型复杂度。", "motivation": "传统的加性规则集成模型使用基于单输入变量阈值（轴平行多面体）的规则条件，虽然可解释性高且学习效率高，但其性能高度依赖于预先准备好的富有表达力的输入特征。如果缺乏此类特征，为了达到足够的准确性，需要增加规则的数量和复杂性，从而降低模型的可解释性。", "method": "提出通过引入带有可学习稀疏线性变换的逻辑命题（形式为 $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$），来扩展经典的规则集成，从而实现具有倾斜面的决策区域。采用基于逻辑回归的迭代重加权公式的序贯贪婪优化方法进行学习。", "result": "实验结果表明，所提出的方法能够高效地构建规则集成，在十个基准数据集上与最先进的方法达到相同的测试风险，同时显著降低了模型复杂度。", "conclusion": "通过引入可学习的稀疏线性变换，本文提出的方法在保持高预测性能的同时，有效解决了传统规则集成在缺乏高质量特征时可解释性下降的问题，并显著降低了模型复杂度。", "translation": "符号规则的小型加性集成提供了可解释的预测模型。传统上，这些集成使用基于单个输入变量x和阈值t的简单阈值命题$x \\geq t$的合取形式的规则条件，从而在几何上形成轴平行多面体作为决策区域。虽然这种形式确保了单个规则的高度可解释性，并且可以使用梯度提升方法有效地学习，但它依赖于拥有精心策划的一组富有表达力且理想独立的输入特征，以便少量的轴平行区域集成能够很好地描述目标变量。在缺乏此类特征的情况下，达到足够的准确性需要增加单个规则的数量和复杂性，这会降低模型的可解释性。在此，我们通过引入具有可学习的输入变量稀疏线性变换的逻辑命题，即形式为$\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$的命题来扩展经典规则集成，其中$\\mathbf{w}$是可学习的稀疏权重向量，从而使决策区域成为具有倾斜面的通用多面体。我们提出了一种基于逻辑回归的迭代重加权公式的序贯贪婪优化学习方法。实验结果表明，所提出的方法能够高效地构建规则集成，在十个基准数据集上与最先进的方法具有相同的测试风险，同时显著降低了模型复杂度。", "summary": "本文针对传统加性规则集成模型在缺乏高质量输入特征时解释性下降的问题，提出了一种新的方法。该方法通过引入带有可学习稀疏线性变换的逻辑命题，使得决策区域能够形成具有倾斜面的通用多面体，而非传统的轴平行多面体。文中采用基于迭代重加权逻辑回归的序贯贪婪优化算法进行模型学习。实验证明，新方法在保持与现有先进方法相同预测性能的同时，显著降低了模型复杂度，提升了模型的可解释性。", "keywords": "加性规则集成, 可解释性, 稀疏线性变换, 倾斜决策边界, 模型复杂度", "comments": "本文的创新点在于将传统规则集成的轴平行决策边界扩展为倾斜边界，通过引入可学习的稀疏线性变换，使得模型能够在特征不理想的情况下依然保持较高的可解释性和预测性能。这种方法有效地平衡了模型的准确性与可解释性，对于实际应用中特征工程受限的场景具有重要意义。"}}
{"id": "2506.20991", "title": "TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation", "authors": ["Chade Li", "Pengju Zhang", "Yihong Wu"], "summary": "The rapid advancement of 3D vision-language models (VLMs) has spurred\nsignificant interest in interactive point cloud processing tasks, particularly\nfor real-world applications. However, existing methods often underperform in\npoint-level tasks, such as segmentation, due to missing direct 3D-text\nalignment, limiting their ability to link local 3D features with textual\ncontext. To solve this problem, we propose TSDASeg, a Two-Stage model coupled\nwith a Direct cross-modal Alignment module and memory module for interactive\npoint cloud Segmentation. We introduce the direct cross-modal alignment module\nto establish explicit alignment between 3D point clouds and textual/2D image\ndata. Within the memory module, we employ multiple dedicated memory banks to\nseparately store text features, visual features, and their cross-modal\ncorrespondence mappings. These memory banks are dynamically leveraged through\nself-attention and cross-attention mechanisms to update scene-specific features\nbased on prior stored data, effectively addressing inconsistencies in\ninteractive segmentation results across diverse scenarios. Experiments\nconducted on multiple 3D instruction, reference, and semantic segmentation\ndatasets demonstrate that the proposed method achieves state-of-the-art\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20991v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20991v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TSDASeg: 一种用于交互式点云分割的直接对齐两阶段模型", "tldr": "现有交互式点云分割方法因缺乏直接三维-文本对齐而表现不佳。本文提出TSDASeg，一个带有直接跨模态对齐模块和记忆模块的两阶段模型，解决了这一问题，并在多个数据集上取得了最先进的性能。", "motivation": "现有的交互式点云处理方法，特别是针对分割等点级别任务，由于缺少直接的三维-文本对齐，无法有效地将局部三维特征与文本语境关联，导致性能不佳。", "method": "本文提出了TSDASeg，一个结合了直接跨模态对齐模块和记忆模块的两阶段模型，用于交互式点云分割。直接跨模态对齐模块用于在三维点云和文本/二维图像数据之间建立显式对齐。记忆模块内包含多个专用记忆库，分别存储文本特征、视觉特征及其跨模态对应映射，并通过自注意力机制和交叉注意力机制动态利用这些记忆库来更新场景特定特征，从而解决不同场景下交互式分割结果的不一致性。", "result": "在多个三维指令、参考和语义分割数据集上进行的实验表明，所提出的TSDASeg方法实现了最先进的性能。", "conclusion": "TSDASeg通过引入直接的三维-文本对齐和记忆模块，有效解决了交互式点云分割中现有方法的局限性，并在多个数据集上取得了最先进的性能，证明了其在实际应用中的有效性。", "translation": "三维视觉-语言模型（VLMs）的快速发展激发了人们对交互式点云处理任务的极大兴趣，尤其是在实际应用中。然而，现有方法在点级别任务（如分割）中往往表现不佳，因为缺少直接的三维-文本对齐，这限制了它们将局部三维特征与文本语境关联起来的能力。为了解决这个问题，我们提出了TSDASeg，这是一个两阶段模型，结合了直接跨模态对齐模块和记忆模块，用于交互式点云分割。我们引入了直接跨模态对齐模块，以在三维点云和文本/二维图像数据之间建立显式对齐。在记忆模块中，我们采用了多个专用记忆库来分别存储文本特征、视觉特征及其跨模态对应映射。这些记忆库通过自注意力机制和交叉注意力机制动态利用，根据预先存储的数据更新场景特定特征，有效地解决了不同场景下交互式分割结果的不一致性。在多个三维指令、参考和语义分割数据集上进行的实验表明，所提出的方法实现了最先进的性能。", "summary": "TSDASeg是一种新颖的两阶段模型，专为交互式点云分割设计。它通过引入一个直接跨模态对齐模块和一个记忆模块，解决了现有方法缺乏直接三维-文本对齐的局限性。对齐模块显式地将三维点云与文本/二维图像关联起来，而记忆模块动态地存储和更新跨模态特征，以确保在不同场景下的一致性。实验结果表明，TSDASeg在各种三维分割数据集上实现了最先进的性能。", "keywords": "交互式点云分割, 三维-文本对齐, 两阶段模型, 记忆模块, 跨模态", "comments": "该论文通过引入直接三维-文本对齐，解决了交互式点云分割中的一个关键局限性，这是一项重要的创新。利用专用记忆模块来处理跨模态特征并确保场景之间的一致性也是一个巧妙的设计。所提出的TSDASeg模型似乎是解决实际三维视觉-语言应用的强大方案。"}}
{"id": "2506.21294", "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "authors": ["Bram Willemsen", "Gabriel Skantze"], "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.", "comment": "Accepted for publication at XLLM @ ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21294v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21294v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用自回归语言模型检测视觉接地对话中的指称表达", "tldr": "本文探讨了使用纯文本自回归语言模型从视觉接地对话中提取指称表达，发现即使是文本-only方法在有限资源下也有效，但指出该任务本质上是多模态的。", "motivation": "旨在探究仅凭语言上下文能在多大程度上辅助检测视觉接地对话中的指称表达，即那些在视觉语境中有对应指代物的提及。", "method": "采用纯文本、自回归语言建模方法。具体地，通过适应预训练的大型语言模型（LLM），利用下一词元预测来粗粒度地标注对话中提及片段的边界，从而提取指称表达。", "result": "研究发现，即使使用中等规模的LLM、相对较小的数据集和参数高效的微调，纯文本方法仍然有效，这突显了语言上下文在该任务中的相对重要性。", "conclusion": "尽管纯文本方法有效，但作者认为该任务本质上是一个多模态问题，并讨论了单模态方法的根本局限性。", "translation": "在本文中，我们探讨了使用纯文本、自回归语言建模方法从视觉接地对话中提取指称表达。更具体地说，目标是调查仅凭语言上下文能在多大程度上为检测在对话视觉上下文中具有（视觉上可感知）指代物的提及提供信息。为此，我们调整了一个预训练的大型语言模型（LLM），通过下一词元预测在文本中划定提及片段边界，从而在展开的对话中执行相对粗粒度的提及片段标注。我们的发现表明，即使使用中等规模的LLM、相对较小的数据集和参数高效的微调，纯文本方法也可以是有效的，这突出了语言上下文对于此任务的相对重要性。然而，我们认为该任务代表了一个固有的多模态问题，并讨论了单模态方法固有的局限性。", "summary": "本研究探索了一种纯文本、自回归语言模型方法，用于从视觉接地对话中提取指称表达。通过调整预训练的LLM并利用下一词元预测进行提及片段标注，作者旨在评估语言上下文在检测视觉指代物方面的作用。结果表明，即使在资源有限的情况下，纯文本方法也表现出有效性，强调了语言上下文的重要性。然而，论文也指出该任务本质上是多模态的，并讨论了单模态方法的局限性。", "keywords": "指称表达检测, 视觉接地对话, 自回归语言模型, 纯文本方法, 多模态问题", "comments": "本文的创新之处在于其反直觉地探索了在视觉接地对话中，仅依赖语言上下文来识别指称表达的可能性，而非直接采用多模态方法。研究结果显示了纯文本方法的有效性，这对于资源受限或需要快速部署的场景具有一定启发性。然而，论文也清醒地认识到并指出了其单模态方法的根本局限性，即该任务本质上是多模态的，这为未来的研究指明了方向。"}}
{"id": "2506.21234", "title": "Real-Time ESFP: Estimating, Smoothing, Filtering, and Pose-Mapping", "authors": ["Qifei Cui", "Yuang Zhou", "Ruichen Deng"], "summary": "This paper presents ESFP, an end-to-end pipeline that converts monocular RGB\nvideo into executable joint trajectories for a low-cost 4-DoF desktop arm. ESFP\ncomprises four sequential modules. (1) Estimating: ROMP lifts each frame to a\n24-joint 3-D skeleton. (2) Smoothing: the proposed HPSTM-a sequence-to-sequence\nTransformer with self-attention-combines long-range temporal context with a\ndifferentiable forward-kinematics decoder, enforcing constant bone lengths and\nanatomical plausibility while jointly predicting joint means and full\ncovariances. (3) Filtering: root-normalized trajectories are variance-weighted\naccording to HPSTM's uncertainty estimates, suppressing residual noise. (4)\nPose-Mapping: a geometric retargeting layer transforms shoulder-elbow-wrist\ntriples into the uArm's polar workspace, preserving wrist orientation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21234v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21234v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "实时ESFP：估计、平滑、滤波和姿态映射", "tldr": "本文提出了ESFP，一个端到端管道，能将单目RGB视频转换为低成本四自由度桌面机械臂的可执行关节轨迹。", "motivation": "本文旨在将单目RGB视频转换为低成本四自由度桌面机械臂的可执行关节轨迹。", "method": "本文提出了ESFP，一个包含四个顺序模块的端到端管道：1) 估计：ROMP将每帧提升为24个关节的3D骨架。2) 平滑：提出的HPSTM（一个带有自注意力机制的序列到序列Transformer）结合了长距离时间上下文和可微分正向运动学解码器，强制保持骨骼长度不变和解剖学合理性，同时联合预测关节均值和完整协方差。3) 滤波：根据HPSTM的不确定性估计对根归一化轨迹进行方差加权，以抑制残余噪声。4) 姿态映射：一个几何重定向层将肩-肘-腕三元组转换为uArm的极坐标工作空间，同时保持腕部方向。", "result": "本文成功提出了ESFP，一个能够将单目RGB视频转换为低成本四自由度桌面机械臂可执行关节轨迹的端到端管道，并详细描述了其四个组成模块及其功能。", "conclusion": "Not mentioned in abstract", "translation": "本文提出了ESFP，一个端到端管道，能将单目RGB视频转换为低成本四自由度桌面机械臂的可执行关节轨迹。ESFP包含四个顺序模块。(1) 估计：ROMP将每帧提升为24个关节的3D骨架。(2) 平滑：提出的HPSTM——一个带有自注意力机制的序列到序列Transformer——结合了长距离时间上下文和可微分正向运动学解码器，强制保持骨骼长度不变和解剖学合理性，同时联合预测关节均值和完整协方差。(3) 滤波：根据HPSTM的不确定性估计对根归一化轨迹进行方差加权，以抑制残余噪声。(4) 姿态映射：一个几何重定向层将肩-肘-腕三元组转换为uArm的极坐标工作空间，同时保持腕部方向。", "summary": "本文介绍了一个名为ESFP的实时端到端管道，旨在将单目RGB视频转换为低成本四自由度桌面机械臂的可执行关节轨迹。ESFP由四个模块组成：首先，ROMP模块将视频帧转换为3D骨架；其次，HPSTM（一个序列到序列Transformer）负责平滑关节轨迹并预测均值和协方差，同时确保解剖学合理性；接着，滤波模块根据不确定性估计抑制噪声；最后，姿态映射层将处理后的骨架姿态转换为机械臂在极坐标工作空间内的动作，并保持腕部方向。", "keywords": "ESFP, 实时, 姿态估计, 机械臂控制, 运动平滑", "comments": "该论文的创新点在于提出了一个完整的端到端管道ESFP，用于将人体运动视频实时转换为机械臂的控制指令。特别是HPSTM模块，通过结合Transformer和可微分正向运动学解码器，在平滑轨迹的同时保持了生物力学约束，这一点非常有意义。整个系统旨在实现低成本机械臂的视频控制，具有潜在的实用价值。"}}
{"id": "2506.20941", "title": "Model State Arithmetic for Machine Unlearning", "authors": ["Keivan Rezaei", "Mehrdad Saberi", "Abhilasha Ravichander", "Soheil Feizi"], "summary": "Large language models are trained on massive corpora of web data, which may\ninclude private data, copyrighted material, factually inaccurate data, or data\nthat degrades model performance. Eliminating the influence of such problematic\ndatapoints through complete retraining -- by repeatedly pretraining the model\non datasets that exclude these specific instances -- is computationally\nprohibitive. For this reason, unlearning algorithms have emerged that aim to\neliminate the influence of particular datapoints, while otherwise preserving\nthe model -- at a low computational cost. However, precisely estimating and\nundoing the influence of individual datapoints has proved to be challenging. In\nthis work, we propose a new algorithm, MSA, for estimating and undoing the\ninfluence of datapoints -- by leveraging model checkpoints i.e. artifacts\ncapturing model states at different stages of pretraining. Our experimental\nresults demonstrate that MSA consistently outperforms existing machine\nunlearning algorithms across multiple benchmarks, models, and evaluation\nmetrics, suggesting that MSA could be an effective approach towards more\nflexible large language models that are capable of data erasure.", "comment": "Preprint. Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.20941v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20941v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "模型状态算术用于机器遗忘", "tldr": "本文提出了一种名为MSA的新算法，通过利用模型检查点来有效估计和消除大型语言模型中特定数据点的影响，解决了传统重训练计算成本高的问题，并在多个基准测试中优于现有算法。", "motivation": "大型语言模型在训练过程中可能包含私有、受版权保护、不准确或降低性能的数据。通过完全重训练来消除这些问题数据点的影响计算成本过高，因此需要低成本的遗忘算法。然而，精确估计和消除单个数据点的影响一直是一个挑战。", "method": "本文提出了一种名为MSA（Model State Arithmetic）的新算法，通过利用模型检查点（即捕获预训练不同阶段模型状态的工件）来估计和消除数据点的影响。", "result": "实验结果表明，MSA在多个基准测试、模型和评估指标上始终优于现有的机器遗忘算法。", "conclusion": "MSA可以成为一种有效的方法，以实现更灵活的、能够进行数据擦除的大型语言模型。", "translation": "大型语言模型在海量网络数据语料库上进行训练，其中可能包括私有数据、受版权保护的材料、事实不准确的数据或降低模型性能的数据。通过完全重训练——即在排除这些特定实例的数据集上重复预训练模型——来消除此类问题数据点的影响，在计算上是 prohibitive 的。因此，出现了旨在消除特定数据点影响，同时以低计算成本保留模型其他部分的遗忘算法。然而，精确估计和撤销单个数据点的影响已被证明具有挑战性。在这项工作中，我们提出了一种新的算法，MSA，用于估计和撤销数据点的影响——通过利用模型检查点，即捕获预训练不同阶段模型状态的工件。我们的实验结果表明，MSA在多个基准测试、模型和评估指标上始终优于现有的机器遗忘算法，这表明MSA可能是一种有效的方法，可用于实现更灵活的、能够进行数据擦除的大型语言模型。", "summary": "本文提出了一种名为MSA（模型状态算术）的新型机器遗忘算法，旨在解决大型语言模型中移除特定数据点影响的计算成本问题。该算法通过利用模型预训练过程中的检查点来精确估计并消除有害数据的影响。实验结果表明，MSA在多个基准测试中表现优于现有遗忘算法，为实现更灵活、支持数据擦除的大型语言模型提供了有效途径。", "keywords": "机器遗忘, 大型语言模型, 模型检查点, 数据擦除, MSA", "comments": "这项工作提出了一种新颖的方法，通过利用模型检查点来解决机器遗忘中的核心挑战——精确估计和消除数据点影响。其创新性在于将模型状态的演变融入到遗忘过程中，有效降低了计算成本并提高了遗忘效果。鉴于数据隐私和模型可控性的日益重要，这项研究对于开发负责任和可信赖的AI系统具有重要意义。"}}
{"id": "2506.21360", "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.", "comment": "Accepted in CogSci 2025", "pdf_url": "http://arxiv.org/pdf/2506.21360v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21360v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人工智能文学批评的结构主义方法：利用格雷马斯符号方阵处理大型语言模型", "tldr": "本文提出GLASS框架，一个基于格雷马斯符号方阵的结构化分析框架，旨在增强大型语言模型进行深度文学分析的能力，并构建了首个相关数据集和评估指标，实验证明其表现优异。", "motivation": "大型语言模型（LLMs）在理解和生成文本方面表现出色，但难以对具有深刻思想和复杂叙事的文学作品提供专业的文学批评。", "method": "本文提出了GLASS（通过符号方阵进行的格雷马斯文学分析）框架，这是一个基于格雷马斯符号方阵（GSS）的结构化分析框架。研究构建了首个用于GSS文学批评的数据集（包含48部作品的详细分析），并提出了使用“LLM作为评判者”范式进行GSS文学批评的量化指标。", "result": "GLASS框架的结果与专家批评以及多个作品和大型语言模型的比较显示出高性能。此外，将GLASS应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究空白。", "conclusion": "这项研究提供了一个基于人工智能的文学研究和教育工具，并为文学参与背后的认知机制提供了见解。", "translation": "大型语言模型（LLMs）在理解和生成文本方面表现出色，但难以对具有深刻思想和复杂叙事的文学作品提供专业的文学批评。本文提出了GLASS（通过符号方阵进行的格雷马斯文学分析），一个基于格雷马斯符号方阵（GSS）的结构化分析框架，旨在增强LLMs进行深度文学分析的能力。GLASS有助于快速剖析叙事作品中的叙事结构和深层含义。我们提出了首个用于GSS文学批评的数据集，其中包含对48部作品的详细分析。然后，我们提出了使用“LLM作为评判者”范式进行GSS文学批评的量化指标。我们的框架结果与专家批评以及多个作品和LLMs的比较显示出高性能。最后，我们将GLASS应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究空白。这项研究提供了一个基于人工智能的文学研究和教育工具，为文学参与背后的认知机制提供了见解。", "summary": "本文针对大型语言模型（LLMs）在专业文学批评方面的不足，提出了一种名为GLASS的结构化分析框架。GLASS基于格雷马斯符号方阵（GSS），旨在提升LLMs进行深度文学分析的能力。研究构建了首个GSS文学批评数据集，并引入了量化评估指标。实验结果表明，GLASS框架在与专家批评和多种LLMs的比较中表现出色，并成功应用于经典作品，产生了高质量的原创分析，为AI文学研究和教育提供了新工具。", "keywords": "大型语言模型, 文学批评, 格雷马斯符号方阵, 结构主义, 人工智能", "comments": "本文的创新之处在于将结构主义文学理论中的格雷马斯符号方阵引入到人工智能文学批评领域，并为大型语言模型提供了具体的分析框架GLASS。通过构建首个GSS文学批评数据集和量化评估指标，为LLMs进行深度文学分析提供了可操作的工具和评估标准，填补了现有研究空白。其重要性在于为AI文学研究和教育提供了新的可能性，并有助于理解文学参与的认知机制。"}}
{"id": "2506.20957", "title": "Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding", "authors": ["Jiameng Chen", "Xiantao Cai", "Jia Wu", "Wenbin Hu"], "summary": "Antibody design remains a critical challenge in therapeutic and diagnostic\ndevelopment, particularly for complex antigens with diverse binding interfaces.\nCurrent computational methods face two main limitations: (1) capturing\ngeometric features while preserving symmetries, and (2) generalizing novel\nantigen interfaces. Despite recent advancements, these methods often fail to\naccurately capture molecular interactions and maintain structural integrity. To\naddress these challenges, we propose \\textbf{AbMEGD}, an end-to-end framework\nintegrating \\textbf{M}ulti-scale \\textbf{E}quivariant \\textbf{G}raph\n\\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging\nadvanced geometric deep learning, AbMEGD combines atomic-level geometric\nfeatures with residue-level embeddings, capturing local atomic details and\nglobal sequence-structure interactions. Its E(3)-equivariant diffusion method\nensures geometric precision, computational efficiency, and robust\ngeneralizability for complex antigens. Furthermore, experiments using the\nSAbDab database demonstrate a 10.13\\% increase in amino acid recovery, 3.32\\%\nrise in improvement percentage, and a 0.062~\\AA\\ reduction in root mean square\ndeviation within the critical CDR-H3 region compared to DiffAb, a leading\nantibody design model. These results highlight AbMEGD's ability to balance\nstructural integrity with improved functionality, establishing a new benchmark\nfor sequence-structure co-design and affinity optimization. The code is\navailable at: https://github.com/Patrick221215/AbMEGD.", "comment": "9 pages, 4 figures, accepted at IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20957v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1; J.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20957v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于多尺度等变图扩散模型用于精确复杂抗原结合的抗体设计与优化", "tldr": "AbMEGD是一个新的多尺度等变图扩散模型，用于抗体序列和结构协同设计，在复杂抗原结合方面表现优于现有方法。", "motivation": "抗体设计在治疗和诊断开发中仍是关键挑战，特别是对于具有多样结合界面的复杂抗原。当前计算方法面临两大限制：(1) 在保留对称性的同时捕获几何特征，以及 (2) 泛化新型抗原界面。现有方法常未能准确捕获分子相互作用并维持结构完整性。", "method": "提出AbMEGD框架，一个端到端的多尺度等变图扩散（Multi-scale Equivariant Graph Diffusion）模型，用于抗体序列和结构协同设计。AbMEGD结合了原子级几何特征与残基级嵌入，捕获局部原子细节和全局序列-结构相互作用。其E(3)等变扩散方法确保了几何精度、计算效率和对复杂抗原的鲁棒泛化能力。", "result": "在SAbDab数据库上的实验表明，与领先的抗体设计模型DiffAb相比，AbMEGD在关键CDR-H3区域的氨基酸恢复率提高了10.13%，改进百分比提升了3.32%，均方根偏差降低了0.062Å。", "conclusion": "AbMEGD能够平衡结构完整性与改进的功能性，为序列-结构协同设计和亲和力优化建立了新基准。", "translation": "抗体设计在治疗和诊断开发中仍是一个关键挑战，特别是对于具有多样结合界面的复杂抗原。当前的计算方法面临两大主要限制：(1) 在保留对称性的同时捕获几何特征，以及 (2) 泛化新型抗原界面。尽管最近取得了进展，但这些方法往往未能准确捕获分子相互作用并维持结构完整性。为了解决这些挑战，我们提出了AbMEGD，一个端到端框架，它集成了多尺度等变图扩散（Multi-scale Equivariant Graph Diffusion）用于抗体序列和结构协同设计。AbMEGD利用先进的几何深度学习技术，将原子级几何特征与残基级嵌入相结合，从而捕获局部原子细节和全局序列-结构相互作用。其E(3)等变扩散方法确保了几何精度、计算效率以及对复杂抗原的鲁棒泛化能力。此外，使用SAbDab数据库进行的实验表明，与领先的抗体设计模型DiffAb相比，AbMEGD在关键的CDR-H3区域氨基酸恢复率提高了10.13%，改进百分比提升了3.32%，均方根偏差降低了0.062Å。这些结果突出了AbMEGD在平衡结构完整性与改进功能性方面的能力，为序列-结构协同设计和亲和力优化建立了新基准。代码可在：https://github.com/Patrick221215/AbMEGD 获取。", "summary": "本研究提出了AbMEGD，一个端到端的多尺度等变图扩散模型，用于解决复杂抗原结合中的抗体序列和结构协同设计挑战。该模型结合了原子级和残基级特征，并采用E(3)等变扩散方法，以确保几何精度和泛化能力。实验结果显示，AbMEGD在氨基酸恢复率、改进百分比和均方根偏差方面优于现有领先模型，为抗体设计和亲和力优化设立了新标准。", "keywords": "抗体设计, 等变图扩散, 序列-结构协同设计, 复杂抗原, 几何深度学习", "comments": "AbMEGD的创新点在于其结合了多尺度等变图扩散方法，实现了抗体序列和结构的协同设计，并利用E(3)等变性确保了几何精度和泛化能力。这对于处理复杂抗原的挑战尤其重要。其在关键CDR-H3区域的显著性能提升，表明该模型在平衡结构完整性和功能性方面取得了突破，为计算抗体设计领域设定了新的基准。"}}
{"id": "2506.20998", "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting", "authors": ["Yeon-Ji Song", "Jaein Kim", "Byung-Ju Kim", "Byoung-Tak Zhang"], "summary": "Novel view synthesis is a task of generating scenes from unseen perspectives;\nhowever, synthesizing dynamic scenes from blurry monocular videos remains an\nunresolved challenge that has yet to be effectively addressed. Existing novel\nview synthesis methods are often constrained by their reliance on\nhigh-resolution images or strong assumptions about static geometry and rigid\nscene priors. Consequently, their approaches lack robustness in real-world\nenvironments with dynamic object and camera motion, leading to instability and\ndegraded visual fidelity. To address this, we propose Motion-aware Dynamic View\nSynthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting\n(DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular\nvideos. Our model generates dense 3D Gaussians, restoring sharpness from blurry\nvideos and reconstructing detailed 3D geometry of the scene affected by dynamic\nmotion variations. Our model achieves robust performance in novel view\nsynthesis under dynamic blurry scenes and sets a new benchmark in realistic\nnovel view synthesis for blurry monocular video inputs.", "comment": "CVPRW 2025, Neural Fields Beyond Conventional Cameras", "pdf_url": "http://arxiv.org/pdf/2506.20998v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20998v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DBMovi-GS：通过稀疏控制高斯泼溅从模糊单目视频进行动态视图合成", "tldr": "DBMovi-GS提出了一种新方法，通过生成密集的3D高斯并恢复清晰度，解决了从模糊单目视频进行动态场景新视图合成的挑战。", "motivation": "现有的新视图合成方法受限于对高分辨率图像的依赖或对静态几何和刚性场景的强假设，导致在动态物体和相机运动的真实世界环境中缺乏鲁棒性，表现不稳定且视觉保真度下降。", "method": "我们提出了Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS)。我们的模型生成密集的3D高斯，从模糊视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何。", "result": "我们的模型在动态模糊场景下的新视图合成中实现了鲁棒性能，并为模糊单目视频输入设置了真实新视图合成的新基准。", "conclusion": "DBMovi-GS有效地解决了从模糊单目视频进行动态视图合成的挑战，通过生成密集的3D高斯并恢复清晰度，重建详细的3D几何，从而在动态模糊场景中实现鲁棒性能并设定了新的基准。", "translation": "新视图合成是一项从未知视角生成场景的任务；然而，从模糊单目视频合成动态场景仍然是一个尚未有效解决的挑战。现有新视图合成方法通常受限于对高分辨率图像的依赖或对静态几何和刚性场景的强假设。因此，它们的方法在具有动态物体和相机运动的真实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度下降。为了解决这个问题，我们提出了通过稀疏控制高斯泼溅从模糊单目视频进行运动感知动态视图合成（DBMovi-GS），这是一种专为从模糊单目视频进行动态视图合成而设计的方法。我们的模型生成密集的3D高斯，从模糊视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何。我们的模型在动态模糊场景下的新视图合成中取得了鲁棒性能，并为模糊单目视频输入设置了真实新视图合成的新基准。", "summary": "DBMovi-GS是一种针对从模糊单目视频进行动态视图合成的新方法，旨在解决现有技术在动态模糊场景中鲁棒性差的问题。该模型通过生成密集的3D高斯来恢复视频清晰度并重建受动态运动影响的详细3D几何，从而在动态模糊场景的新视图合成中表现出强大的性能，并为该领域设立了新的基准。", "keywords": "动态视图合成, 模糊单目视频, 高斯泼溅, 3D重建, 运动恢复", "comments": "DBMovi-GS的创新之处在于其能够直接从模糊的单目视频中进行动态视图合成，这对于现有方法来说是一个未解决的挑战。通过利用稀疏控制的高斯泼溅技术生成密集的3D高斯并恢复清晰度，该方法显著提升了在复杂真实世界环境中的鲁棒性和视觉保真度，为动态场景的新视图合成带来了重要进展。"}}
{"id": "2506.21358", "title": "ToosiCubix: Monocular 3D Cuboid Labeling via Vehicle Part Annotations", "authors": ["Behrooz Nasihatkon", "Hossein Resani", "Amirreza Mehrzadian"], "summary": "Many existing methods for 3D cuboid annotation of vehicles rely on expensive\nand carefully calibrated camera-LiDAR or stereo setups, limiting their\naccessibility for large-scale data collection. We introduce ToosiCubix, a\nsimple yet powerful approach for annotating ground-truth cuboids using only\nmonocular images and intrinsic camera parameters. Our method requires only\nabout 10 user clicks per vehicle, making it highly practical for adding 3D\nannotations to existing datasets originally collected without specialized\nequipment. By annotating specific features (e.g., wheels, car badge,\nsymmetries) across different vehicle parts, we accurately estimate each\nvehicle's position, orientation, and dimensions up to a scale ambiguity (8\nDoF). The geometric constraints are formulated as an optimization problem,\nwhich we solve using a coordinate descent strategy, alternating between\nPerspective-n-Points (PnP) and least-squares subproblems. To handle common\nambiguities such as scale and unobserved dimensions, we incorporate\nprobabilistic size priors, enabling 9 DoF cuboid placements. We validate our\nannotations against the KITTI and Cityscapes3D datasets, demonstrating that our\nmethod offers a cost-effective and scalable solution for high-quality 3D cuboid\nannotation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21358v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21358v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ToosiCubix: 基于车辆部件标注的单目3D长方体标注", "tldr": "ToosiCubix 是一种仅使用单目图像和少量用户点击即可进行车辆3D长方体标注的方法，比现有方法更经济且可扩展。", "motivation": "许多现有的车辆3D长方体标注方法依赖于昂贵且经过仔细校准的相机-激光雷达或立体设置，这限制了它们进行大规模数据采集的可及性。", "method": "ToosiCubix 是一种仅使用单目图像和相机内参进行真值长方体标注的方法。它通过用户对车辆特定特征（如车轮、车标、对称性）的大约10次点击来估计车辆的位置、方向和尺寸（8自由度）。几何约束被表述为一个优化问题，通过坐标下降策略（在PnP和最小二乘子问题之间交替）解决。为了处理尺度和未观测尺寸等常见歧义，该方法结合了概率尺寸先验，从而实现了9自由度的长方体放置。", "result": "该方法能够准确估计车辆的位置、方向和尺寸（8自由度，加入先验后9自由度）。它在KITTI和Cityscapes3D数据集上进行了验证，结果表明该方法为高质量3D长方体标注提供了一种经济高效且可扩展的解决方案。", "conclusion": "ToosiCubix 提供了一种实用、经济高效且可扩展的解决方案，仅使用单目图像即可实现高质量的3D长方体标注，克服了以往方法的局限性。", "translation": "许多现有的车辆3D长方体标注方法依赖于昂贵且经过仔细校准的相机-激光雷达或立体设置，这限制了它们进行大规模数据采集的可及性。我们引入了 ToosiCubix，这是一种简单而强大的方法，仅使用单目图像和相机内参即可标注真值长方体。我们的方法每辆车只需约10次用户点击，这使得它对于向最初未配备专用设备收集的现有数据集添加3D标注非常实用。通过对不同车辆部件上的特定特征（例如车轮、车标、对称性）进行标注，我们能够准确估计每辆车的位置、方向和尺寸，但存在尺度模糊性（8自由度）。几何约束被表述为一个优化问题，我们使用坐标下降策略解决，在透视-n-点（PnP）和最小二乘子问题之间交替。为了处理尺度和未观测尺寸等常见歧义，我们引入了概率尺寸先验，从而实现了9自由度的长方体放置。我们在KITTI和Cityscapes3D数据集上验证了我们的标注，结果表明我们的方法为高质量3D长方体标注提供了一种经济高效且可扩展的解决方案。", "summary": "ToosiCubix 是一种新颖的车辆3D长方体标注方法，它仅使用单目图像和最少的用户输入（每辆车约10次点击）。该方法通过将几何约束表述为优化问题，并利用坐标下降策略（交替进行PnP和最小二乘子问题）来解决，从而克服了昂贵多传感器设置的局限性。通过整合概率尺寸先验，ToosiCubix 实现了9自由度的长方体放置。该方法在KITTI和Cityscapes3D数据集上进行了验证，证明了其在高质量3D标注方面的成本效益和可扩展性。", "keywords": "3D长方体标注, 单目, 车辆, 数据标注, 优化", "comments": "本文的主要创新在于提出了一种仅使用单目图像和少量用户点击即可进行3D长方体标注的方法，极大地降低了数据采集的成本和复杂性。这使得大规模3D数据标注变得更加可行和普及。该方法通过将问题转化为优化并结合先验知识来解决尺度模糊性，显示了其在实际应用中的潜力。其重要性在于为自动驾驶和计算机视觉领域的数据集构建提供了更经济、更高效的解决方案。虽然需要用户点击，但数量极少，使其具有很高的实用性。"}}
{"id": "2506.20990", "title": "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes", "authors": ["Yifan Yang", "Zhen Zhang", "Rupak Vignesh Swaminathan", "Jing Liu", "Nathan Susanj", "Zheng Zhang"], "summary": "Fine-tuning vision language models (VLMs) has achieved remarkable performance\nacross various downstream tasks; yet, it requires access to model gradients\nthrough backpropagation (BP), making them unsuitable for memory-constrained,\ninference-only edge devices. To address this limitation, previous work has\nexplored various BP-free fine-tuning methods. However, these approaches often\nrely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)\noptimization, and often fail to achieve satisfactory performance. In this\npaper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)\napproach, specifically designed to enhance the performance of ZO VLM\nfine-tuning via a sharpness-aware warm-up training. SharpZO features a\ntwo-stage optimization process: a sharpness-aware ES stage that globally\nexplores and smooths the loss landscape to construct a strong initialization,\nfollowed by a fine-grained local search via sparse ZO optimization. The entire\noptimization relies solely on forward passes. Detailed theoretical analysis and\nextensive experiments on CLIP models demonstrate that SharpZO significantly\nimproves accuracy and convergence speed, achieving up to 7% average gain over\nstate-of-the-art forward-only methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20990v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20990v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SharpZO：通过仅前向传播实现的混合锐度感知视觉语言模型提示调优", "tldr": "SharpZO是一种混合锐度感知零阶优化方法，通过两阶段前向传播过程，显著提高了视觉语言模型在内存受限设备上的调优性能和收敛速度。", "motivation": "现有的视觉语言模型（VLM）微调方法需要通过反向传播获取模型梯度，这使得它们不适用于内存受限的边缘设备。虽然已有一些无反向传播的微调方法，但它们通常依赖于高方差的进化策略或零阶优化，并且性能不佳。", "method": "本文提出了一种混合锐度感知零阶优化（SharpZO）方法。该方法包含一个两阶段优化过程：首先是锐度感知的进化策略（ES）阶段，用于全局探索和平滑损失景观以构建强大的初始化；随后是稀疏零阶优化进行的细粒度局部搜索。整个优化过程仅依赖于前向传播。", "result": "SharpZO在CLIP模型上的实验表明，它显著提高了准确性和收敛速度，比最先进的仅前向传播方法平均提高了高达7%的性能。", "conclusion": "SharpZO通过其混合锐度感知零阶优化方法，有效解决了在内存受限设备上微调视觉语言模型的挑战，并在仅前向传播的设置下取得了显著的性能提升和更快的收敛速度。", "translation": "视觉语言模型（VLM）的微调在各种下游任务中取得了卓越的性能；然而，它需要通过反向传播（BP）访问模型梯度，这使得它们不适用于内存受限、仅推理的边缘设备。为了解决这一限制，先前的工作探索了各种无BP的微调方法。然而，这些方法通常依赖于高方差的进化策略（ES）或零阶（ZO）优化，并且往往未能达到令人满意的性能。在本文中，我们提出了一种混合锐度感知零阶优化（SharpZO）方法，专门设计用于通过锐度感知热身训练来增强ZO VLM微调的性能。SharpZO具有两阶段优化过程：一个锐度感知ES阶段，用于全局探索和平滑损失景观以构建强大的初始化，随后通过稀疏ZO优化进行细粒度的局部搜索。整个优化过程仅依赖于前向传播。详细的理论分析和对CLIP模型的广泛实验表明，SharpZO显著提高了准确性和收敛速度，比最先进的仅前向传播方法平均提高了高达7%的增益。", "summary": "本文提出了一种名为SharpZO的混合锐度感知零阶优化方法，旨在解决视觉语言模型在内存受限边缘设备上进行微调时需要反向传播的问题。SharpZO采用两阶段优化：首先通过锐度感知进化策略进行全局探索和初始化，然后通过稀疏零阶优化进行局部搜索。该方法仅需前向传播，并在CLIP模型上展示出显著的准确性提升和更快的收敛速度，平均性能优于现有仅前向传播方法达7%。", "keywords": "视觉语言模型, 提示调优, 零阶优化, 锐度感知, 仅前向传播", "comments": "SharpZO通过结合锐度感知和两阶段优化策略，为内存受限设备上的VLM微调提供了一个创新的解决方案。其“仅前向传播”的特性使其在实际应用中具有重要价值，尤其是在边缘计算场景。论文在理论分析和实验验证方面都做得很好，结果令人信服。"}}
{"id": "2506.21001", "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology", "authors": ["Qiuyi Qi", "Xin Li", "Ming Kong", "Zikang Xu", "Bingdi Chen", "Qiang Zhu", "S Kevin Zhou"], "summary": "Challenges such as the lack of high-quality annotations, long-tailed data\ndistributions, and inconsistent staining styles pose significant obstacles to\ntraining neural networks to detect abnormal cells in cytopathology robustly.\nThis paper proposes a style-aligned image composition (SAIC) method that\ncomposes high-fidelity and style-preserved pathological images to enhance the\neffectiveness and robustness of detection models. Without additional training,\nSAIC first selects an appropriate candidate from the abnormal cell bank based\non attribute guidance. Then, it employs a high-frequency feature reconstruction\nto achieve a style-aligned and high-fidelity composition of abnormal cells and\npathological backgrounds. Finally, it introduces a large vision-language model\nto filter high-quality synthesis images. Experimental results demonstrate that\nincorporating SAIC-synthesized images effectively enhances the performance and\nrobustness of abnormal cell detection for tail categories and styles, thereby\nimproving overall detection performance. The comprehensive quality evaluation\nfurther confirms the generalizability and practicality of SAIC in clinical\napplication scenarios. Our code will be released at\nhttps://github.com/Joey-Qi/SAIC.", "comment": "MIDL 2025 Oral", "pdf_url": "http://arxiv.org/pdf/2506.21001v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21001v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "细胞病理学中用于异常细胞鲁棒检测的风格对齐图像合成", "tldr": "提出SAIC方法，通过合成风格对齐的病理图像，提高异常细胞检测的鲁棒性和整体性能。", "motivation": "高质量标注的缺乏、长尾数据分布以及不一致的染色风格对训练神经网络鲁棒地检测细胞病理学中的异常细胞构成了重大障碍。", "method": "本文提出了一种风格对齐图像合成（SAIC）方法。该方法无需额外训练，首先根据属性指导从异常细胞库中选择合适的候选细胞；然后，采用高频特征重建技术实现异常细胞与病理背景的风格对齐和高保真合成；最后，引入大型视觉-语言模型来过滤高质量的合成图像。", "result": "实验结果表明，结合SAIC合成的图像能有效增强针对长尾类别和不同风格的异常细胞检测性能和鲁棒性，从而提升整体检测效果。全面的质量评估进一步证实了SAIC在临床应用场景中的通用性和实用性。", "conclusion": "SAIC方法通过合成高保真和风格对齐的病理图像，有效提升了细胞病理学中异常细胞检测的性能和鲁棒性，并具有良好的临床实用性。", "translation": "细胞病理学中高质量标注的缺乏、长尾数据分布以及不一致的染色风格等挑战，对训练神经网络鲁棒地检测异常细胞构成了重大障碍。本文提出了一种风格对齐图像合成（SAIC）方法，该方法合成高保真且风格保留的病理图像，以增强检测模型的有效性和鲁棒性。SAIC无需额外训练，首先根据属性指导从异常细胞库中选择合适的候选细胞。然后，它采用高频特征重建技术实现异常细胞与病理背景的风格对齐和高保真合成。最后，它引入大型视觉-语言模型来过滤高质量的合成图像。实验结果表明，结合SAIC合成的图像能有效增强针对长尾类别和不同风格的异常细胞检测性能和鲁棒性，从而提升整体检测效果。全面的质量评估进一步证实了SAIC在临床应用场景中的通用性和实用性。我们的代码将在https://github.com/Joey-Qi/SAIC 发布。", "summary": "针对细胞病理学中异常细胞检测面临的标注稀缺、数据长尾和染色风格不一致等挑战，本文提出了一种风格对齐图像合成（SAIC）方法。SAIC通过结合属性指导下的细胞选择、高频特征重建进行风格对齐的高保真合成，以及大型视觉-语言模型进行质量过滤，生成高质量的合成病理图像。实验证明，SAIC合成的图像显著提升了异常细胞检测模型对长尾类别和不同风格的性能与鲁棒性，展现了其在临床应用中的普适性和实用价值。", "keywords": "异常细胞检测, 细胞病理学, 图像合成, 风格对齐, 鲁棒性", "comments": "SAIC的创新之处在于其通过合成高保真、风格对齐的病理图像来解决细胞病理学中异常细胞检测面临的数据稀缺和风格不一致问题。该方法巧妙地结合了属性指导、高频特征重建以及大型视觉-语言模型进行质量控制，为医疗图像分析领域提供了新的思路。其重要性体现在能有效提升医疗诊断的准确性和鲁棒性，尤其是在处理罕见病变或多样化样本时。"}}
{"id": "2506.21443", "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "authors": ["Ali Şenol", "Garima Agrawal", "Huan Liu"], "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21443v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21443v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "领域知识增强型LLM用于欺诈和概念漂移检测", "tldr": "提出一个领域知识增强型LLM框架，用于在动态平台检测欺诈对话和概念漂移，显著提高了检测准确性、可解释性和鲁棒性。", "motivation": "在动态平台检测欺骗性对话因语言模式演变和概念漂移（语义或主题变化）而日益困难。这些变化会模糊恶意意图或模仿正常对话，使准确分类具有挑战性。虽然大型语言模型（LLM）在自然语言任务中表现出色，但在风险敏感场景中常受上下文模糊性和幻觉困扰。", "method": "提出了一个领域知识（DK）增强型LLM框架，该框架将预训练的LLM与结构化、任务特定的洞察相结合，以执行欺诈和概念漂移检测。该架构包括三个主要组件：(1) DK-LLM模块用于检测虚假或欺骗性对话；(2) 漂移检测单元（OCDD）用于确定是否发生语义漂移；以及(3) 第二个DK-LLM模块用于将漂移分类为良性或欺诈性。", "result": "首先使用虚假评论数据集验证了领域知识的价值，然后将完整框架应用于SEConvo数据集。结果表明，该系统能高精度检测虚假对话并有效分类漂移的性质。在结构化提示引导下，基于LLaMA的实现达到了98%的分类准确率。与零样本基线的对比研究表明，结合领域知识和漂移感知显著提高了高风险NLP应用中的性能、可解释性及鲁棒性。", "conclusion": "结合领域知识和漂移感知可以显著提高大型语言模型在高风险自然语言处理应用中检测欺诈和概念漂移的性能、可解释性和鲁棒性。", "translation": "在动态平台上检测欺骗性对话由于不断演变的语言模式和概念漂移（CD）——即随着时间推移改变交互上下文或意图的语义或主题变化——而日益困难。这些变化会模糊恶意意图或模仿正常对话，使准确分类具有挑战性。虽然大型语言模型（LLM）在自然语言任务中表现出色，但在风险敏感场景中它们经常面临上下文模糊性和幻觉问题。为了解决这些挑战，我们提出了一个领域知识（DK）增强型LLM框架，该框架将预训练的LLM与结构化、任务特定的洞察相结合，以执行欺诈和概念漂移检测。所提出的架构包括三个主要组件：(1) 一个DK-LLM模块用于检测虚假或欺骗性对话；(2) 一个漂移检测单元（OCDD）用于确定是否发生语义漂移；以及(3) 第二个DK-LLM模块用于将漂移分类为良性或欺诈性。我们首先使用一个虚假评论数据集验证了领域知识的价值，然后将我们的完整框架应用于SEConvo，这是一个多轮对话数据集，包含各种类型的欺诈和垃圾邮件攻击。结果显示，我们的系统能够高精度检测虚假对话并有效分类漂移的性质。在结构化提示的引导下，基于LLaMA的实现达到了98%的分类准确率。与零样本基线的对比研究表明，结合领域知识和漂移感知显著提高了高风险NLP应用中的性能、可解释性和鲁棒性。", "summary": "本研究提出了一个领域知识（DK）增强型LLM框架，旨在解决动态平台中欺诈对话和概念漂移检测的挑战。该框架包含一个用于检测欺骗性对话的DK-LLM模块、一个用于识别语义漂移的OCDD单元，以及一个用于分类漂移性质的第二个DK-LLM模块。实验结果表明，该系统能够高精度检测虚假对话，有效分类漂移，并且基于LLaMA的实现达到了98%的准确率，证明了领域知识和漂移感知在提高高风险NLP应用性能、可解释性和鲁棒性方面的显著优势。", "keywords": "欺诈检测, 概念漂移, 大型语言模型, 领域知识, 自然语言处理", "comments": "该论文的创新点在于将领域知识与大型语言模型结合，以应对高风险场景中欺诈检测和概念漂移的挑战。通过模块化设计，它不仅解决了LLM在上下文模糊性和幻觉方面的局限性，还提出了一个有效识别和分类概念漂移的方法。这对于动态平台上的安全性和内容管理具有重要意义，其高准确率和鲁棒性显示了该方法的实际应用潜力。"}}
{"id": "2506.21420", "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting", "authors": ["Taoyu Wu", "Yiyi Miao", "Zhuoxiao Li", "Haocheng Zhao", "Kang Dang", "Jionglong Su", "Limin Yu", "Haoang Li"], "summary": "Efficient three-dimensional reconstruction and real-time visualization are\ncritical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian\nSplatting (3DGS) has demonstrated remarkable performance in efficient 3D\nreconstruction and rendering. Most 3DGS-based Simultaneous Localization and\nMapping (SLAM) methods only rely on the appearance constraints for optimizing\nboth 3DGS and camera poses. However, in endoscopic scenarios, the challenges\ninclude photometric inconsistencies caused by non-Lambertian surfaces and\ndynamic motion from breathing affects the performance of SLAM systems. To\naddress these issues, we additionally introduce optical flow loss as a\ngeometric constraint, which effectively constrains both the 3D structure of the\nscene and the camera motion. Furthermore, we propose a depth regularisation\nstrategy to mitigate the problem of photometric inconsistencies and ensure the\nvalidity of 3DGS depth rendering in endoscopic scenes. In addition, to improve\nscene representation in the SLAM system, we improve the 3DGS refinement\nstrategy by focusing on viewpoints corresponding to Keyframes with suboptimal\nrendering quality frames, achieving better rendering results. Extensive\nexperiments on the C3VD static dataset and the StereoMIS dynamic dataset\ndemonstrate that our method outperforms existing state-of-the-art methods in\nnovel view synthesis and pose estimation, exhibiting high performance in both\nstatic and dynamic surgical scenes. The source code will be publicly available\nupon paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21420v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21420v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EndoFlow-SLAM：基于流约束高斯泼溅的实时内窥镜SLAM", "tldr": "EndoFlow-SLAM引入光流损失和深度正则化策略，改进3D高斯泼溅(3DGS)以克服内窥镜SLAM中的光度不一致和动态运动问题，在实时三维重建和位姿估计方面优于现有方法。", "motivation": "在内窥镜等手术场景中，高效的三维重建和实时可视化至关重要。现有的基于3D高斯泼溅（3DGS）的同步定位与建图（SLAM）方法主要依赖外观约束，但在内窥镜场景中，非朗伯表面引起的光度不一致和呼吸引起的动态运动会影响SLAM系统性能。", "method": "该方法引入光流损失作为几何约束，有效约束场景的三维结构和相机运动。此外，提出了一种深度正则化策略来缓解光度不一致问题，并确保3DGS深度渲染在内窥镜场景中的有效性。为了改善SLAM系统中的场景表示，该研究还改进了3DGS细化策略，关注渲染质量欠佳的关键帧视角，以获得更好的渲染结果。", "result": "在C3VD静态数据集和StereoMIS动态数据集上的大量实验表明，该方法在新视角合成和位姿估计方面优于现有最先进的方法，在静态和动态手术场景中均表现出高性能。", "conclusion": "EndoFlow-SLAM通过引入光流约束和深度正则化等策略，有效解决了内窥镜SLAM中光度不一致和动态运动的挑战，实现了高效的实时三维重建和精准的位姿估计，为手术场景提供了优越的SLAM解决方案。", "translation": "高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，三维高斯泼溅（3DGS）在高效三维重建和渲染方面表现出卓越的性能。大多数基于3DGS的同步定位与建图（SLAM）方法仅依赖外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括由非朗伯表面引起的光度不一致和呼吸导致的动态运动影响SLAM系统的性能。为了解决这些问题，我们额外引入了光流损失作为几何约束，有效约束了场景的三维结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致问题并确保3DGS深度渲染在内窥镜场景中的有效性。另外，为了改善SLAM系统中的场景表示，我们通过关注对应于渲染质量欠佳关键帧的视角来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上的大量实验表明，我们的方法在新视角合成和位姿估计方面优于现有最先进的方法，在静态和动态手术场景中均表现出高性能。源代码将在论文接受后公开。", "summary": "EndoFlow-SLAM是一种为内窥镜手术场景设计的实时SLAM系统，旨在解决现有3DGS-based SLAM方法在内窥镜中面临的光度不一致和动态运动挑战。该方法通过引入光流损失作为几何约束，以及提出深度正则化策略来优化3D结构和相机运动，并改进3DGS细化策略以增强场景表示。实验结果表明，EndoFlow-SLAM在静态和动态手术场景中，其新视角合成和位姿估计性能均优于现有先进方法。", "keywords": "内窥镜SLAM, 3D高斯泼溅, 光流, 实时, 姿态估计", "comments": "该论文的创新点在于将光流损失和深度正则化策略引入到基于3DGS的SLAM系统中，以应对内窥镜手术中特有的光度不一致和动态运动问题。这对于提升内窥镜实时三维重建和导航的准确性和鲁棒性具有重要意义。其在动态场景下的优越表现也突显了其在实际应用中的潜力。"}}
{"id": "2506.21003", "title": "Distilling Normalizing Flows", "authors": ["Steven Walton", "Valeriy Klyukin", "Maksim Artemev", "Denis Derkach", "Nikita Orlov", "Humphrey Shi"], "summary": "Explicit density learners are becoming an increasingly popular technique for\ngenerative models because of their ability to better model probability\ndistributions. They have advantages over Generative Adversarial Networks due to\ntheir ability to perform density estimation and having exact latent-variable\ninference. This has many advantages, including: being able to simply\ninterpolate, calculate sample likelihood, and analyze the probability\ndistribution. The downside of these models is that they are often more\ndifficult to train and have lower sampling quality.\n  Normalizing flows are explicit density models, that use composable bijective\nfunctions to turn an intractable probability function into a tractable one. In\nthis work, we present novel knowledge distillation techniques to increase\nsampling quality and density estimation of smaller student normalizing flows.\nWe seek to study the capacity of knowledge distillation in Compositional\nNormalizing Flows to understand the benefits and weaknesses provided by these\narchitectures. Normalizing flows have unique properties that allow for a\nnon-traditional forms of knowledge transfer, where we can transfer that\nknowledge within intermediate layers. We find that through this distillation,\nwe can make students significantly smaller while making substantial performance\ngains over a non-distilled student. With smaller models there is a\nproportionally increased throughput as this is dependent upon the number of\nbijectors, and thus parameters, in the network.", "comment": "Published in eLVM @ CVPR\n  (https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Walton_Distilling_Normalizing_Flows_CVPRW_2025_paper)", "pdf_url": "http://arxiv.org/pdf/2506.21003v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21003v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "蒸馏归一化流", "tldr": "本文提出了一种新的知识蒸馏技术，用于提高小型归一化流模型的采样质量和密度估计，使其在显著缩小模型尺寸的同时获得显著的性能提升。", "motivation": "显式密度学习器（如归一化流）在生成模型中越来越受欢迎，因为它们能够更好地建模概率分布，并且相对于生成对抗网络具有密度估计和精确潜在变量推断的优势。然而，这些模型通常训练难度更大，采样质量较低。", "method": "本文提出了新颖的知识蒸馏技术，以提高小型学生归一化流的采样质量和密度估计。研究了组合归一化流中知识蒸馏的能力，并利用归一化流的独特属性，允许在中间层进行非传统的知识转移。", "result": "通过知识蒸馏，可以显著缩小学生模型的尺寸，同时相对于未经蒸馏的学生模型获得实质性的性能提升。更小的模型会带来成比例的吞吐量增加，因为这取决于网络中双射器（即参数）的数量。", "conclusion": "知识蒸馏可以显著提高小型归一化流模型的性能和效率。", "translation": "显式密度学习器正成为生成模型中越来越流行的技术，因为它们能够更好地建模概率分布。由于它们能够执行密度估计并具有精确的潜在变量推断，因此它们比生成对抗网络具有优势。这有许多优点，包括：能够简单地进行插值、计算样本似然和分析概率分布。这些模型的缺点是它们通常更难训练且采样质量较低。\n归一化流是显式密度模型，它们使用可组合的双射函数将难以处理的概率函数转换为可处理的概率函数。在这项工作中，我们提出了新颖的知识蒸馏技术，以提高小型学生归一化流的采样质量和密度估计。我们旨在研究组合归一化流中知识蒸馏的能力，以了解这些架构提供的优点和缺点。归一化流具有独特的属性，允许非传统的知识转移形式，我们可以在中间层转移该知识。我们发现，通过这种蒸馏，我们可以使学生模型显著缩小，同时比未经蒸馏的学生模型获得实质性的性能提升。随着模型变小，吞吐量成比例增加，因为这取决于网络中双射器（即参数）的数量。", "summary": "本文针对显式密度学习器（尤其是归一化流）训练困难和采样质量较低的问题，提出了一种新颖的知识蒸馏技术。研究发现，利用归一化流独特的中间层知识转移特性，可以将知识从大型教师模型蒸馏到小型学生模型。实验结果表明，通过这种蒸馏方法，学生模型在尺寸显著减小的同时，在性能上取得了实质性提升，并带来了更高的吞吐量。", "keywords": "归一化流, 知识蒸馏, 生成模型, 密度估计, 模型压缩", "comments": "该研究的创新之处在于探索了归一化流中非传统的知识转移形式，即在中间层进行知识蒸馏。这对于模型压缩和提高生成模型效率具有重要意义，能够使得更小、更快的模型达到与大型模型相当的性能。"}}
{"id": "2506.21002", "title": "Inverse Scene Text Removal", "authors": ["Takumi Yoshimatsu", "Shumpei Takezaki", "Seiichi Uchida"], "summary": "Scene text removal (STR) aims to erase textual elements from images. It was\noriginally intended for removing privacy-sensitiveor undesired texts from\nnatural scene images, but is now also appliedto typographic images. STR\ntypically detects text regions and theninpaints them. Although STR has advanced\nthrough neural networksand synthetic data, misuse risks have increased. This\npaper investi-gates Inverse STR (ISTR), which analyzes STR-processed images\nandfocuses on binary classification (detecting whether an image has un-dergone\nSTR) and localizing removed text regions. We demonstrate inexperiments that\nthese tasks are achievable with high accuracies, en-abling detection of\npotential misuse and improving STR. We also at-tempt to recover the removed\ntext content by training a text recognizerto understand its difficulty.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.21002v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21002v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "逆向场景文本移除", "tldr": "本文研究逆向场景文本移除（ISTR），旨在检测图像是否经过场景文本移除（STR）处理，定位被移除的文本区域，并尝试恢复移除的文本内容。", "motivation": "场景文本移除（STR）技术虽然有所进步，但也带来了滥用风险。为了检测潜在的滥用并改进STR技术，需要一种能够分析STR处理过的图像，并识别是否经过STR处理以及移除文本区域的方法。", "method": "本文研究逆向场景文本移除（ISTR）。方法包括两个主要任务：1) 二分类：检测图像是否经过STR处理。2) 定位：定位被移除的文本区域。此外，还尝试训练一个文本识别器来恢复被移除的文本内容，以评估其难度。", "result": "实验表明，检测图像是否经过STR处理和定位被移除文本区域的任务可以高精度实现。恢复被移除文本内容的尝试也揭示了其难度。", "conclusion": "逆向场景文本移除（ISTR）任务，包括检测图像是否经过STR处理和定位被移除文本区域，是可行的且能达到高精度，这有助于检测潜在的滥用并改进STR技术。恢复被移除的文本内容具有挑战性。", "translation": "场景文本移除（STR）旨在从图像中擦除文本元素。它最初是为了从自然场景图像中移除隐私敏感或不需要的文本，但现在也应用于版式图像。STR通常检测文本区域然后进行修复。尽管STR通过神经网络和合成数据取得了进展，但滥用风险也随之增加。本文研究逆向STR（ISTR），它分析经过STR处理的图像，并专注于二分类（检测图像是否经过STR处理）和定位被移除的文本区域。我们在实验中证明这些任务可以高精度实现，从而能够检测潜在的滥用并改进STR。我们还尝试通过训练一个文本识别器来恢复被移除的文本内容，以了解其难度。", "summary": "本文提出并研究了逆向场景文本移除（ISTR）任务，旨在应对场景文本移除（STR）技术可能带来的滥用风险。ISTR主要包括检测图像是否经过STR处理的二分类任务，以及定位被移除文本区域的任务。实验证明，这些任务可以高精度完成，有助于识别STR的潜在滥用并改进现有STR方法。此外，研究还尝试恢复被移除的文本内容，并评估了该任务的难度。", "keywords": "逆向场景文本移除, 场景文本移除, 文本检测, 图像修复, 滥用检测", "comments": "这项工作具有重要的实际意义，因为它直接解决了文本移除技术可能带来的伦理和滥用问题。通过提出逆向检测和定位的方法，为图像取证和内容真实性验证提供了新的视角。尝试恢复文本内容虽然具有挑战性，但为未来的研究指明了方向。其创新性在于从“移除”的对立面“检测移除”进行研究，填补了该领域的空白。"}}
{"id": "2506.21427", "title": "Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning", "authors": ["Prajwal Koirala", "Cody Fleming"], "summary": "Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21427v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21427v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于流的单步完成，实现高效且富有表现力的策略学习", "tldr": "提出SSCP，一种基于流匹配的生成策略，实现单步动作生成，结合生成模型的表达能力与单峰策略的效率，适用于多种RL设置。", "motivation": "现有生成模型（如扩散和流匹配）在离线强化学习中虽然能捕捉多模态动作分布，但其迭代采样导致高推理成本和训练不稳定（梯度传播）。", "method": "提出“单步完成策略”（SSCP），这是一种生成策略，通过增强的流匹配目标进行训练，以从中间流样本预测直接完成向量，从而实现准确的单次动作生成。SSCP在异策略演员-评论家框架中结合了生成模型的表达能力与单峰策略的训练和推理效率，无需长反向传播链。", "result": "该方法有效地扩展到离线RL、离线到在线RL和在线RL设置，与基于扩散的基线相比，在速度和适应性方面提供了显著提升。进一步扩展到目标条件RL，使扁平策略能够利用子目标结构。在标准离线RL和行为克隆基准上取得了良好结果。", "conclusion": "SSCP被定位为一个多功能、富有表现力且高效的深度RL和序列决策框架。", "translation": "生成模型，如扩散模型和流匹配模型，通过捕捉丰富、多模态的动作分布，为离线强化学习（RL）提供了富有表现力的策略。然而，它们的迭代采样引入了高昂的推理成本和由于跨采样步骤的梯度传播导致的训练不稳定。我们提出了“单步完成策略”（SSCP），这是一种生成策略，通过增强的流匹配目标进行训练，以从中间流样本预测直接完成向量，从而实现准确的单次动作生成。在异策略演员-评论家框架中，SSCP将生成模型的表达能力与单峰策略的训练和推理效率相结合，而无需冗长的反向传播链。我们的方法能有效地扩展到离线RL、离线到在线RL和在线RL设置，与基于扩散的基线相比，在速度和适应性方面提供了显著提升。我们进一步将SSCP扩展到目标条件RL，使扁平策略能够利用子目标结构，而无需显式的分层推理。SSCP在标准离线RL和行为克隆基准上取得了良好结果，将其定位为深度RL和序列决策的通用、富有表现力且高效的框架。", "summary": "本文提出了单步完成策略（SSCP），一种基于增强流匹配目标的生成式策略，用于解决离线强化学习中现有生成模型（如扩散模型）因迭代采样导致的高推理成本和训练不稳定问题。SSCP能够从中间流样本直接预测完成向量，实现高效的单次动作生成。该方法在异策略演员-评论家框架下，结合了生成模型的表达能力与单峰策略的效率，并在离线、离线到在线及在线RL设置中展现出优越的速度和适应性。SSCP还在目标条件RL和各项基准测试中表现出色，被认为是一个多功能、高效的深度RL框架。", "keywords": "强化学习, 生成模型, 流匹配, 单步完成, 策略学习", "comments": "SSCP的创新之处在于通过改进流匹配目标，实现了生成式策略的单步动作生成，有效解决了传统生成模型在强化学习中面临的效率和稳定性挑战。这使其能够同时拥有生成模型的表达能力和单峰策略的效率，显著提升了实际应用价值。其在多种RL设置下的普适性和对目标条件RL的扩展也增加了其重要性。"}}
{"id": "2506.21028", "title": "TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence", "authors": ["Feng Jiang", "Mangal Prakash", "Hehuan Ma", "Jianyuan Deng", "Yuzhi Guo", "Amina Mollaysa", "Tommaso Mansi", "Rui Liao", "Junzhou Huang"], "summary": "Molecular property prediction aims to learn representations that map chemical\nstructures to functional properties. While multimodal learning has emerged as a\npowerful paradigm to learn molecular representations, prior works have largely\noverlooked textual and taxonomic information of molecules for representation\nlearning. We introduce TRIDENT, a novel framework that integrates molecular\nSMILES, textual descriptions, and taxonomic functional annotations to learn\nrich molecular representations. To achieve this, we curate a comprehensive\ndataset of molecule-text pairs with structured, multi-level functional\nannotations. Instead of relying on conventional contrastive loss, TRIDENT\nemploys a volume-based alignment objective to jointly align tri-modal features\nat the global level, enabling soft, geometry-aware alignment across modalities.\nAdditionally, TRIDENT introduces a novel local alignment objective that\ncaptures detailed relationships between molecular substructures and their\ncorresponding sub-textual descriptions. A momentum-based mechanism dynamically\nbalances global and local alignment, enabling the model to learn both broad\nfunctional semantics and fine-grained structure-function mappings. TRIDENT\nachieves state-of-the-art performance on 11 downstream tasks, demonstrating the\nvalue of combining SMILES, textual, and taxonomic functional annotations for\nmolecular property prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21028v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21028v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TRIDENT：结合分类学注释和局部对应关系的三模态分子表示学习", "tldr": "TRIDENT是一个三模态分子表示学习框架，通过整合SMILES、文本和分类学注释，并利用全局和局部对齐目标，在分子性质预测任务上实现了最先进的性能。", "motivation": "现有的多模态分子表示学习方法忽略了分子的文本和分类学信息，而分子性质预测需要学习将化学结构映射到功能性质的表示。", "method": "TRIDENT是一个新颖的框架，它整合了分子SMILES、文本描述和分类学功能注释来学习分子表示。它通过策划一个包含结构化、多级功能注释的分子-文本对数据集来实现。该框架采用基于体积的对齐目标进行全局三模态特征对齐，并引入了捕获分子子结构与其对应子文本描述之间关系的局部对齐目标。一个基于动量的机制动态平衡全局和局部对齐。", "result": "TRIDENT在11项下游任务上实现了最先进的性能。", "conclusion": "结合SMILES、文本和分类学功能注释对于分子性质预测具有重要价值。", "translation": "分子性质预测旨在学习将化学结构映射到功能性质的表示。尽管多模态学习已成为学习分子表示的强大范式，但现有工作在很大程度上忽视了分子的文本和分类学信息用于表示学习。我们引入了TRIDENT，一个新颖的框架，它整合了分子SMILES、文本描述和分类学功能注释来学习丰富的分子表示。为此，我们策划了一个包含结构化、多级功能注释的分子-文本对的综合数据集。TRIDENT没有依赖传统的对比损失，而是采用基于体积的对齐目标来联合对齐全局级别的三模态特征，从而实现跨模态的软性、几何感知对齐。此外，TRIDENT引入了一种新颖的局部对齐目标，捕获分子子结构与其对应子文本描述之间的详细关系。一个基于动量的机制动态平衡全局和局部对齐，使模型能够学习广泛的功能语义和细粒度的结构-功能映射。TRIDENT在11项下游任务上实现了最先进的性能，证明了结合SMILES、文本和分类学功能注释对于分子性质预测的价值。", "summary": "TRIDENT是一个用于分子表示学习的新型三模态框架，它创新性地整合了分子SMILES、文本描述和分类学功能注释。该框架通过构建一个包含多级功能注释的分子-文本数据集，并采用基于体积的全局对齐目标以及新颖的局部对齐目标来捕捉细粒度关系。一个动量机制用于平衡不同层次的对齐。TRIDENT在多项下游任务中取得了最先进的性能，强调了多模态信息融合在分子性质预测中的重要性。", "keywords": "分子表示学习, 多模态学习, 分子性质预测, 分类学注释, 局部对应", "comments": "TRIDENT的创新之处在于其整合了之前被忽视的文本和分类学信息，并引入了基于体积的全局对齐和局部对齐目标，以实现更精细和全面的分子表示学习。其动量平衡机制也提升了模型的学习能力。这项工作的重要性在于为分子性质预测提供了一个更全面、更强大的多模态学习范式，有望推动该领域的发展。"}}
{"id": "2506.21005", "title": "VisionGuard: Synergistic Framework for Helmet Violation Detection", "authors": ["Lam-Huy Nguyen", "Thinh-Phuc Nguyen", "Thanh-Hai Nguyen", "Gia-Huy Dinh", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Enforcing helmet regulations among motorcyclists is essential for enhancing\nroad safety and ensuring the effectiveness of traffic management systems.\nHowever, automatic detection of helmet violations faces significant challenges\ndue to environmental variability, camera angles, and inconsistencies in the\ndata. These factors hinder reliable detection of motorcycles and riders and\ndisrupt consistent object classification. To address these challenges, we\npropose VisionGuard, a synergistic multi-stage framework designed to overcome\nthe limitations of frame-wise detectors, especially in scenarios with class\nimbalance and inconsistent annotations. VisionGuard integrates two key\ncomponents: Adaptive Labeling and Contextual Expander modules. The Adaptive\nLabeling module is a tracking-based refinement technique that enhances\nclassification consistency by leveraging a tracking algorithm to assign\npersistent labels across frames and correct misclassifications. The Contextual\nExpander module improves recall for underrepresented classes by generating\nvirtual bounding boxes with appropriate confidence scores, effectively\naddressing the impact of data imbalance. Experimental results show that\nVisionGuard improves overall mAP by 3.1% compared to baseline detectors,\ndemonstrating its effectiveness and potential for real-world deployment in\ntraffic surveillance systems, ultimately promoting safety and regulatory\ncompliance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21005v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21005v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "VisionGuard：头盔违规检测的协同框架", "tldr": "VisionGuard是一个多阶段框架，通过自适应标注和上下文扩展模块，解决了头盔违规自动检测中数据不一致和类别不平衡的挑战，将mAP提高了3.1%。", "motivation": "摩托车手头盔法规的执行对道路安全至关重要，但自动检测面临环境多变性、摄像机角度和数据不一致等挑战，这些因素阻碍了摩托车和骑手的可靠检测，并干扰了对象分类的一致性。现有的逐帧检测器在类别不平衡和标注不一致的情况下存在局限性。", "method": "提出VisionGuard，一个协同的多阶段框架，旨在克服逐帧检测器的局限性。它集成了两个关键组件：自适应标注模块和上下文扩展模块。自适应标注模块是一种基于跟踪的细化技术，通过利用跟踪算法在帧之间分配持久标签并纠正错误分类来增强分类一致性。上下文扩展模块通过生成具有适当置信度分数的虚拟边界框来提高代表性不足类别的召回率，有效解决了数据不平衡的影响。", "result": "实验结果表明，与基线检测器相比，VisionGuard将整体mAP提高了3.1%。", "conclusion": "VisionGuard证明了其在交通监控系统中实际部署的有效性和潜力，最终促进了安全和法规遵守。", "translation": "强制执行摩托车手头盔法规对于提高道路安全和确保交通管理系统的有效性至关重要。然而，由于环境多变性、摄像机角度和数据不一致等因素，头盔违规的自动检测面临重大挑战。这些因素阻碍了摩托车和骑手的可靠检测，并干扰了对象分类的一致性。为了应对这些挑战，我们提出了VisionGuard，一个协同的多阶段框架，旨在克服逐帧检测器的局限性，特别是在类别不平衡和标注不一致的场景中。VisionGuard集成了两个关键组件：自适应标注和上下文扩展模块。自适应标注模块是一种基于跟踪的细化技术，通过利用跟踪算法在帧之间分配持久标签并纠正错误分类来增强分类一致性。上下文扩展模块通过生成具有适当置信度分数的虚拟边界框来提高代表性不足类别的召回率，有效解决了数据不平衡的影响。实验结果表明，与基线检测器相比，VisionGuard将整体mAP提高了3.1%，证明了其在交通监控系统中实际部署的有效性和潜力，最终促进了安全和法规遵守。", "summary": "VisionGuard是一个用于头盔违规检测的协同多阶段框架，旨在解决自动检测中环境变异性、数据不一致性和类别不平衡等挑战。该框架包含自适应标注模块（通过跟踪提高分类一致性）和上下文扩展模块（通过生成虚拟边界框改善稀有类别召回率）。实验证明，VisionGuard将整体mAP提高了3.1%，显示出其在实际交通监控系统中的应用潜力，有助于提升道路安全和法规遵守。", "keywords": "头盔违规检测, 交通安全, 目标检测, 跟踪, 数据不平衡", "comments": "这篇论文通过提出VisionGuard框架，有效解决了头盔违规自动检测中长期存在的分类一致性差和数据不平衡问题。其创新点在于结合了跟踪算法进行标签细化（自适应标注模块）和生成虚拟边界框以增强稀有类别召回率（上下文扩展模块）。这使得检测系统在复杂多变的环境下更鲁棒、更准确。该框架对于提升交通监控的自动化水平和道路安全具有重要意义。"}}
{"id": "2506.21547", "title": "SAM4D: Segment Anything in Camera and LiDAR Streams", "authors": ["Jianyun Xu", "Song Wang", "Ziqian Ni", "Chunyong Hu", "Sheng Yang", "Jianke Zhu", "Qiang Li"], "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.", "comment": "Accepted by ICCV2025, Project Page: https://SAM4D-Project.github.io", "pdf_url": "http://arxiv.org/pdf/2506.21547v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21547v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SAM4D：在相机和激光雷达流中分割一切", "tldr": "SAM4D是一个多模态时间基础模型，用于相机和激光雷达流的可提示分割，通过统一位置编码和运动感知注意力增强跨模态和时间一致性，并提出自动化数据引擎加速伪标签生成。", "motivation": "现有分割模型在多模态数据对齐和时间一致性方面存在挑战，尤其是在动态自动驾驶场景中。此外，数据标注是瓶颈，需要更高效的标注方法。", "method": "该论文引入了统一多模态位置编码（UMPE）以对齐相机和激光雷达特征到共享3D空间，实现跨模态提示和交互。提出了运动感知跨模态记忆注意力（MCMA）以利用自我运动补偿增强时间一致性和长距离特征检索。此外，开发了一个多模态自动化数据引擎，通过协同VFM驱动的视频masklets、时空4D重建和跨模态masklet融合来生成相机-激光雷达对齐的伪标签。", "result": "在构建的Waymo-4DSeg数据集上进行了广泛实验，结果表明SAM4D具有强大的跨模态分割能力和在数据标注方面的巨大潜力。其自动化数据引擎生成伪标签的速度比人工标注快几个数量级，同时保持了语义保真度。", "conclusion": "SAM4D通过其创新的多模态和时间处理机制，显著提升了相机和激光雷达流的分割能力，并为解决数据标注瓶颈提供了一个高效的解决方案。", "translation": "我们提出了SAM4D，一个多模态和时间基础模型，旨在实现相机和激光雷达流的可提示分割。引入了统一多模态位置编码（UMPE），以在共享的3D空间中对齐相机和激光雷达特征，从而实现无缝的跨模态提示和交互。此外，我们提出了运动感知跨模态记忆注意力（MCMA），它利用自我运动补偿来增强时间一致性和长距离特征检索，确保在动态变化的自动驾驶场景中实现鲁棒的分割。为了避免标注瓶颈，我们开发了一个多模态自动化数据引擎，该引擎协同VFM驱动的视频masklets、时空4D重建和跨模态masklet融合。该框架生成的相机-激光雷达对齐伪标签的速度比人工标注快几个数量级，同时在点云表示中保留了VFM衍生的语义保真度。我们在构建的Waymo-4DSeg数据集上进行了广泛实验，这些实验证明了所提出的SAM4D强大的跨模态分割能力和在数据标注方面的巨大潜力。", "summary": "SAM4D是一个针对相机和激光雷达流的可提示分割的多模态时间基础模型。它通过统一多模态位置编码（UMPE）实现特征对齐和跨模态交互，并利用运动感知跨模态记忆注意力（MCMA）增强时间一致性。为了解决标注瓶颈，SAM4D还引入了一个多模态自动化数据引擎，能够高效生成高质量的相机-激光雷达对齐伪标签。在Waymo-4DSeg数据集上的实验表明，SAM4D在跨模态分割和数据标注方面表现出强大的能力和巨大潜力。", "keywords": "多模态分割, 激光雷达, 相机, 基础模型, 伪标签生成", "comments": "SAM4D的创新点在于其多模态（相机+LiDAR）融合策略和时间一致性处理，特别是UMPE和MCMA的设计。更重要的是，它提出了一个自动化数据引擎，显著加速了高质量伪标签的生成，这对于解决自动驾驶领域昂贵且耗时的数据标注问题具有重要意义。该模型在基础模型框架下，结合了分割和数据生成能力，具有很高的实用价值和广阔的应用前景。"}}
{"id": "2506.21035", "title": "Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning", "authors": ["Haodong Lu", "Chongyang Zhao", "Jason Xue", "Lina Yao", "Kristen Moore", "Dong Gong"], "summary": "Continual learning (CL) with large pre-trained models is challenged by\ncatastrophic forgetting and task interference. Existing LoRA-based\nMixture-of-Experts (MoE) approaches mitigate forgetting by assigning and\nfreezing task-specific adapters, but suffer from interference, redundancy, and\nambiguous routing due to coarse adapter-level selection. However, this design\nintroduces three key challenges: 1) Interference: Activating full LoRA experts\nper input leads to subspace interference and prevents selective reuse of useful\ncomponents across tasks. 2) Redundancy: Newly added experts often duplicate or\ncontradict existing knowledge due to unnecessary activation of unrelated ranks\nand insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features\nacross tasks confuse the router, resulting in unstable expert assignments. As\nmore experts accumulate, earlier task routing degrades, accelerating\nforgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with\nself-activated and sparse rank activation for CL. Unlike mixing multiple\nlow-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,\neach treated as an independent expert, enabling fine-grained mixture of rank-1\nexpert utilization while mitigating interference and redundancy. To avoid\nambiguous routing, we propose that each rank-1 expert can infer its own\nrelevance via intermediate activations. Coupled with our proposed rank pruning\nand activation budgets, MoRA adaptively selects a sparse mixture of ranks per\ninput. We validate MoRA on continual learning tasks with CLIP and large\nlanguage models (LLMs), analyzing both in-domain learning and out-of-domain\nforgetting/generalization during fine-tuning. MoRA shows significant\neffectiveness on enhancing CL with PTMs, and improving generalization while\nmitigating forgetting.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.21035v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21035v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "循序渐进：通过自激活稀疏秩自适应学习实现持续学习", "tldr": "MoRA提出了一种新的持续学习方法，通过将低秩更新分解为稀疏激活的秩1专家来解决大预训练模型中灾难性遗忘和任务干扰的问题，并在CLIP和LLM上表现出显著效果。", "motivation": "现有基于LoRA的专家混合（MoE）方法在处理大型预训练模型的持续学习时，面临灾难性遗忘和任务干扰的问题。具体而言，这些方法存在三个关键挑战：1）干扰：对每个输入激活完整的LoRA专家会导致子空间干扰，并阻碍跨任务选择性重用有用组件。2）冗余：由于不必要地激活不相关的秩和不充分地重用相关的秩，新添加的专家经常重复或与现有知识冲突。3）歧义：任务之间的重叠特征会混淆路由器，导致专家分配不稳定，随着专家数量的增加，早期任务路由会退化，加速遗忘。", "method": "我们提出了MoRA，一种具有自激活和稀疏秩激活的秩自适应学习方法，用于持续学习。与混合多个低秩矩阵不同，MoRA将每个秩-r更新分解为r个秩-1组件，每个组件被视为独立的专家，从而实现秩-1专家的细粒度混合利用，同时减轻干扰和冗余。为了避免模糊路由，我们提出每个秩-1专家可以通过中间激活推断其自身的相关性。结合我们提出的秩剪枝和激活预算，MoRA为每个输入自适应地选择稀疏的秩混合。", "result": "MoRA在CLIP和大型语言模型（LLMs）的持续学习任务上得到了验证，分析了微调过程中的域内学习和域外遗忘/泛化。MoRA在增强预训练模型的持续学习、提高泛化能力和减轻遗忘方面显示出显著的有效性。", "conclusion": "MoRA是一种有效的持续学习方法，能够显著增强预训练模型的持续学习能力，提高泛化性能并有效减轻遗忘，从而克服了现有方法面临的干扰、冗余和歧义等挑战。", "translation": "带有大型预训练模型（PTMs）的持续学习（CL）面临着灾难性遗忘和任务干扰的挑战。现有的基于LoRA的专家混合（MoE）方法通过分配和冻结特定任务的适配器来减轻遗忘，但由于粗粒度的适配器级别选择，它们遭受干扰、冗余和模糊路由的问题。然而，这种设计引入了三个关键挑战：1）干扰：每次输入激活完整的LoRA专家会导致子空间干扰，并阻止跨任务选择性重用有用组件。2）冗余：由于不必要地激活不相关的秩和不充分地重用相关的秩，新添加的专家经常重复或与现有知识冲突。3）歧义：任务之间重叠的特征会混淆路由器，导致专家分配不稳定。随着更多专家的积累，早期任务的路由会退化，加速遗忘。我们提出了MoRA，一种具有自激活和稀疏秩激活的秩自适应学习方法，用于持续学习。与混合多个低秩矩阵不同，MoRA将每个秩-r更新分解为r个秩-1组件，每个组件被视为独立的专家，从而实现秩-1专家的细粒度混合利用，同时减轻干扰和冗余。为了避免模糊路由，我们提出每个秩-1专家可以通过中间激活推断其自身的相关性。结合我们提出的秩剪枝和激活预算，MoRA为每个输入自适应地选择稀疏的秩混合。我们在CLIP和大型语言模型（LLMs）的持续学习任务上验证了MoRA，分析了微调过程中的域内学习和域外遗忘/泛化。MoRA在增强预训练模型的持续学习、提高泛化能力和减轻遗忘方面显示出显著的有效性。", "summary": "该论文提出MoRA（Mixture-of-Rank Adaptive learning），一种针对大型预训练模型持续学习的新方法，旨在解决现有LoRA-based MoE方法面临的灾难性遗忘、任务干扰、冗余和路由歧义问题。MoRA将每个低秩更新分解为独立的秩-1专家，通过自激活和稀疏秩选择机制，实现细粒度的专家利用，并结合秩剪枝和激活预算来避免模糊路由。实验证明，MoRA在CLIP和LLM的持续学习任务中，显著提升了持续学习效果，改善了泛化能力并有效减轻了遗忘。", "keywords": "持续学习, 预训练模型, 秩自适应, 专家混合, 灾难性遗忘", "comments": "该论文的创新点在于提出了MoRA，通过将低秩更新分解为更细粒度的秩-1专家，并引入自激活和稀疏选择机制，有效解决了现有LoRA-based MoE在持续学习中面临的干扰、冗余和路由歧义问题。这种细粒度的专家管理方式，以及通过中间激活推断专家相关性的设计，具有较高的创新性。MoRA在保持预训练模型性能的同时，显著缓解了灾难性遗忘，对于大模型在不断变化的环境中进行持续学习具有重要意义。"}}
{"id": "2506.21006", "title": "Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning", "authors": ["Tyler Ward", "Xiaoqin Wang", "Braxton McFarland", "Md Atik Ahamed", "Sahar Nozad", "Talal Arshad", "Hafsa Nebbache", "Jin Chen", "Abdullah Imran"], "summary": "Complete removal of cancer tumors with a negative specimen margin during\nlumpectomy is essential in reducing breast cancer recurrence. However, 2D\nspecimen radiography (SR), the current method used to assess intraoperative\nspecimen margin status, has limited accuracy, resulting in nearly a quarter of\npatients requiring additional surgery. To address this, we propose a novel deep\nlearning framework combining the Segment Anything Model (SAM) with\nForward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging\nboth local and global contrastive learning for patch-level classification of SR\nimages. After annotating SR images with regions of known maligancy,\nnon-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18\nbackbone with FFCL to classify margin status, then reconstruct coarse binary\nmasks to prompt SAM for refined tumor margin segmentation. Our approach\nachieved an AUC of 0.8455 for margin classification and segmented margins with\na 27.4% improvement in Dice similarity over baseline models, while reducing\ninference time to 47 milliseconds per image. These results demonstrate that\nFFCL-SAM significantly enhances both the speed and accuracy of intraoperative\nmargin assessment, with strong potential to reduce re-excision rates and\nimprove surgical outcomes in breast cancer treatment. Our code is available at\nhttps://github.com/tbwa233/FFCL-SAM/.", "comment": "19 pages, 7 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21006v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21006v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "乳腺癌保乳术切缘的SAM融合前向-前向对比学习检测", "tldr": "该研究提出了一种结合SAM和前向-前向对比学习的深度学习框架FFCL-SAM，用于提高乳腺癌保乳术中切缘评估的速度和准确性，以减少再次手术率。", "motivation": "当前用于评估乳腺癌保乳术中切缘状态的二维标本X射线摄影（SR）准确性有限，导致近四分之一的患者需要再次手术，因此需要更准确、快速的方法来评估切缘。", "method": "提出了一种结合Segment Anything Model (SAM) 和 Forward-Forward Contrastive Learning (FFCL) 的新型深度学习框架。该方法首先对SR图像进行标注，然后使用FFCL预训练ResNet-18骨干网络进行切缘状态分类，最后通过重建粗略的二值掩码来提示SAM进行精细的肿瘤切缘分割。", "result": "该方法在切缘分类方面实现了0.8455的AUC；在切缘分割方面，Dice相似度比基线模型提高了27.4%；同时，每张图像的推理时间减少到47毫秒。", "conclusion": "FFCL-SAM显著提高了术中切缘评估的速度和准确性，具有降低再次切除率和改善乳腺癌治疗手术结果的巨大潜力。", "translation": "**标题：** 融合SAM的前向-前向对比学习在乳腺癌保乳术切缘检测中的应用\n\n**摘要：** 在保乳术中完整切除肿瘤并获得阴性标本切缘对于降低乳腺癌复发至关重要。然而，二维标本X射线摄影（SR）作为目前用于评估术中标本切缘状态的方法，其准确性有限，导致近四分之一的患者需要再次手术。为解决此问题，我们提出了一种新颖的深度学习框架，将Segment Anything Model (SAM) 与前向-前向对比学习 (FFCL) 相结合。FFCL是一种预训练策略，利用局部和全局对比学习对SR图像进行补丁级分类。在对已知恶性、非恶性组织和病理证实切缘的SR图像进行标注后，我们使用FFCL预训练一个ResNet-18骨干网络来分类切缘状态，然后重建粗略的二值掩码以提示SAM进行精细的肿瘤切缘分割。我们的方法在切缘分类方面实现了0.8455的AUC，在切缘分割方面的Dice相似度比基线模型提高了27.4%，同时将每张图像的推理时间减少到47毫秒。这些结果表明，FFCL-SAM显著提高了术中切缘评估的速度和准确性，具有降低再次切除率和改善乳腺癌治疗手术结果的巨大潜力。我们的代码可在https://github.com/tbwa233/FFCL-SAM/获取。", "summary": "该论文提出了一种名为FFCL-SAM的深度学习框架，旨在提高乳腺癌保乳术中切缘评估的准确性和效率。该框架结合了前向-前向对比学习（FFCL）进行图像补丁分类预训练，并利用Segment Anything Model (SAM) 进行精细的肿瘤切缘分割。实验结果表明，FFCL-SAM在切缘分类和分割性能上均显著优于基线模型，并大幅缩短了推理时间，有望减少患者的再次手术率并改善治疗效果。", "keywords": "乳腺癌, 保乳术, 切缘检测, 对比学习, SAM, 深度学习", "comments": "这篇论文的创新点在于将前向-前向对比学习（FFCL）与强大的通用分割模型SAM相结合，以解决乳腺癌保乳术中切缘评估的实际临床难题。FFCL的预训练策略能够有效利用局部和全局信息进行补丁级分类，而SAM的引入则能提供高精度的分割能力。该方法不仅在准确性上取得了显著提升（AUC 0.8455，Dice相似度提高27.4%），还在推理速度上实现了突破（47毫秒/图像），这对于术中快速决策至关重要。其潜在的临床影响是巨大的，有望显著降低患者的再次手术率，减轻患者负担并优化医疗资源。"}}
{"id": "2506.21468", "title": "TopK Language Models", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21468v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21468v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TopK 语言模型", "tldr": "本文引入了一种新的Transformer架构修改，即TopK语言模型，通过在选定层集成TopK激活函数，消除了对稀疏自编码器（SAEs）的后训练需求，同时提供了可比的解释性，并能保持原始模型能力。", "motivation": "稀疏自编码器（SAEs）是分析和解释Transformer语言模型激活空间的重要工具，但存在局限性：它们是事后训练的，导致不清楚概念未被发现是SAE的问题还是底层LM未表示该概念；训练条件和架构选择会影响SAE学习的特征；特征稳定性不足导致难以比较不同检查点间的SAE特征，从而阻碍了对LM学习概念过程的追踪。", "method": "本文提出对Transformer架构进行修改，在选定的层中引入TopK激活函数，使得模型的隐藏状态等同于TopK稀疏自编码器的潜在特征。这种方法消除了对SAE进行事后训练的需要，同时提供了与SAE相当的解释性。", "result": "TopK语言模型在模型大小、计算效率和可解释性之间提供了有利的权衡。尽管架构改动简单，TopK语言模型仍能保持其原有能力，并提供强大的可解释性优势。实验表明，TopK语言模型学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并有助于详细分析跨检查点和层的神经元形成过程。", "conclusion": "TopK语言模型提供了一种稳定可靠的工具，用于理解语言模型如何学习和表示概念，有望显著推动未来在模型可解释性和可控性方面的研究。", "translation": "稀疏自编码器（SAEs）已成为分析和解释基于Transformer的语言模型（LMs）激活空间的重要工具。然而，SAEs存在一些缺点，降低了它们的实用性和内部有效性。由于SAEs是事后训练的，因此不清楚未能发现特定概念是SAE的问题，还是底层语言模型未表示该概念。训练条件和架构选择影响SAE学习哪些特征，这使得问题更加严重。在追踪语言模型在训练过程中如何学习概念时，特征稳定性的缺乏也使得难以比较不同检查点之间的SAE特征。为了解决这些局限性，我们引入了一种对Transformer架构的修改，在选定的层中加入了TopK激活函数，使模型的隐藏状态等同于TopK SAE的潜在特征。这种方法消除了对事后训练的需求，同时提供了与SAEs相当的可解释性。由此产生的TopK语言模型在模型大小、计算效率和可解释性之间提供了有利的权衡。尽管进行了这种简单的架构改变，TopK语言模型仍保持了其原有能力，同时提供了强大的可解释性优势。我们的实验表明，TopK语言模型学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并有助于详细分析跨检查点和层的神经元形成过程。这些特性使得TopK语言模型成为理解语言模型如何学习和表示概念的稳定可靠的工具，我们相信这将显著推动未来在模型可解释性和可控性方面的研究。", "summary": "本文针对稀疏自编码器（SAEs）在语言模型解释性方面存在的局限性，提出了一种名为TopK语言模型的新架构。通过在Transformer模型的选定层中直接集成TopK激活函数，该方法将模型的隐藏状态等同于TopK SAE的潜在特征，从而消除了SAE的后训练需求，并提供了与SAEs相当的可解释性。实验证明，TopK语言模型在保持原始能力的同时，提供了模型大小、计算效率和可解释性之间的良好平衡，并且其稀疏表示有助于实现成功的神经元干预和细致的神经元形成过程分析，有望成为理解语言模型学习和表示概念的稳定工具。", "keywords": "TopK语言模型, 稀疏自编码器, 语言模型解释性, Transformer架构, 神经元干预", "comments": "TopK语言模型提出了一种新颖且实用的方法，通过直接修改Transformer架构来解决稀疏自编码器（SAEs）的固有缺陷，特别是事后训练的限制和特征稳定性问题。其创新之处在于将可解释性直接嵌入到模型训练过程中，而非作为后处理步骤。这不仅提高了效率，也增强了特征的内在有效性和可比性。该方法在保持模型性能的同时，显著提升了可解释性，为理解大型语言模型内部工作机制提供了更稳定和可靠的工具，对于未来模型可控性和透明度的研究具有重要意义。"}}
{"id": "2506.21552", "title": "Whole-Body Conditioned Egocentric Video Prediction", "authors": ["Yutong Bai", "Danny Tran", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik"], "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.", "comment": "Project Page: https://dannytran123.github.io/PEVA", "pdf_url": "http://arxiv.org/pdf/2506.21552v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21552v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "全身条件下的自我中心视频预测", "tldr": "该论文训练了一个模型（PEVA），通过以人体姿态为条件，预测第一人称视角视频，并使用了一个新的评估协议来分析其能力。", "motivation": "该研究旨在从人类视角出发，通过视频预测来模拟复杂的真实世界环境和具身智能体的行为。", "method": "作者训练了一个自回归条件扩散变换器（PEVA），通过过去视频和相对3D身体姿态表示的动作进行条件化，以预测自我中心视频。模型在大型真实世界自我中心视频和身体姿态捕获数据集Nymeria上进行训练。他们还设计了一个分层评估协议。", "result": "该模型学会了模拟物理人体动作如何从第一人称视角塑造环境。他们设计的分层评估协议能够全面分析模型的具身预测和控制能力。", "conclusion": "这项工作代表了从人类视角出发，通过视频预测来建模复杂真实世界环境和具身智能体行为挑战的初步尝试。", "translation": "我们训练模型（PEVA）根据过去的视频和由相对3D身体姿态表示的动作来预测自我中心视频。通过以身体关节层次结构化的运动姿态轨迹为条件，我们的模型学会了从第一人称视角模拟物理人体动作如何塑造环境。我们在Nymeria（一个包含大量真实世界自我中心视频和身体姿态捕捉数据的大规模数据集）上训练了一个自回归条件扩散变换器。我们进一步设计了一个具有逐渐增加挑战性任务的分层评估协议，从而能够全面分析模型的具身预测和控制能力。我们的工作代表了从人类视角出发，通过视频预测来解决建模复杂真实世界环境和具身智能体行为挑战的初步尝试。", "summary": "该论文介绍了一种名为PEVA的模型，旨在通过以人体动作（表示为相对3D身体姿态）为条件，预测自我中心视频。该模型利用运动姿态轨迹来模拟人类行为如何影响环境。PEVA是一个自回归条件扩散变换器，在大型数据集Nymeria上进行训练。为了全面评估模型的具身预测和控制能力，研究者还开发了一个分层评估协议。这项工作是解决从人类视角建模复杂真实世界环境和具身智能体行为的初步探索。", "keywords": "自我中心视频预测, 身体姿态, 扩散模型, 具身智能体, 视频生成", "comments": "这项工作的创新在于将全身姿态作为条件来预测自我中心视频，这为模拟具身智能体在复杂环境中的行为提供了新的视角。通过使用扩散模型和大规模真实世界数据集，该研究为未来更高级的具身AI系统奠定了基础。分层评估协议的设计也值得称赞，它允许对模型的不同能力进行细致分析。"}}
{"id": "2506.21008", "title": "The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion", "authors": ["Bang Gong", "Luchao Qi", "Jiaye Wu", "Zhicheng Fu", "Chunbo Song", "David W. Jacobs", "John Nicholson", "Roni Sengupta"], "summary": "We introduce the Aging Multiverse, a framework for generating multiple\nplausible facial aging trajectories from a single image, each conditioned on\nexternal factors such as environment, health, and lifestyle. Unlike prior\nmethods that model aging as a single deterministic path, our approach creates\nan aging tree that visualizes diverse futures. To enable this, we propose a\ntraining-free diffusion-based method that balances identity preservation, age\naccuracy, and condition control. Our key contributions include attention mixing\nto modulate editing strength and a Simulated Aging Regularization strategy to\nstabilize edits. Extensive experiments and user studies demonstrate\nstate-of-the-art performance across identity preservation, aging realism, and\nconditional alignment, outperforming existing editing and age-progression\nmodels, which often fail to account for one or more of the editing criteria. By\ntransforming aging into a multi-dimensional, controllable, and interpretable\nprocess, our approach opens up new creative and practical avenues in digital\nstorytelling, health education, and personalized visualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21008v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21008v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "衰老多重宇宙：通过免训练扩散生成条件感知面部衰老树", "tldr": "提出“衰老多重宇宙”框架，通过免训练扩散方法，从单张图像生成多条受外部因素影响的 plausible 面部衰老轨迹，形成“衰老树”，优于现有确定性方法。", "motivation": "现有方法将衰老建模为单一确定性路径，而本文旨在生成受环境、健康和生活方式等外部因素影响的多种 plausible 面部衰老轨迹。", "method": "提出一种免训练的基于扩散的方法，以平衡身份保留、年龄准确性和条件控制。主要贡献包括用于调节编辑强度的注意力混合（attention mixing）和用于稳定编辑的模拟衰老正则化（Simulated Aging Regularization）策略。", "result": "在身份保留、衰老真实感和条件对齐方面表现出最先进的性能，优于现有编辑和年龄进展模型，这些模型通常未能考虑一个或多个编辑标准。", "conclusion": "将衰老转化为一个多维度、可控且可解释的过程，为数字故事讲述、健康教育和个性化可视化开辟了新的创意和实用途径。", "translation": "我们引入了衰老多重宇宙（Aging Multiverse），这是一个从单张图像生成多条 plausible 面部衰老轨迹的框架，每条轨迹都以环境、健康和生活方式等外部因素为条件。与将衰老建模为单一确定性路径的现有方法不同，我们的方法创建了一个可视化多样化未来的衰老树。为了实现这一点，我们提出了一种免训练的基于扩散的方法，该方法平衡了身份保留、年龄准确性和条件控制。我们的主要贡献包括用于调节编辑强度的注意力混合（attention mixing）和用于稳定编辑的模拟衰老正则化（Simulated Aging Regularization）策略。广泛的实验和用户研究表明，在身份保留、衰老真实感和条件对齐方面，该方法表现出最先进的性能，优于现有编辑和年龄进展模型，这些模型通常未能考虑一个或多个编辑标准。通过将衰老转化为一个多维度、可控且可解释的过程，我们的方法为数字故事讲述、健康教育和个性化可视化开辟了新的创意和实用途径。", "summary": "本文提出“衰老多重宇宙”框架，利用免训练扩散方法，从单张图像生成多条受外部因素（如环境、健康、生活方式）影响的 plausible 面部衰老轨迹，形成“衰老树”，以可视化多样化的未来。该方法通过注意力混合和模拟衰老正则化策略，平衡身份保留、年龄准确性和条件控制。实验和用户研究表明，在身份保留、衰老真实感和条件对齐方面，其性能优于现有模型。该方法将衰老转化为多维度、可控且可解释的过程，为数字故事讲述、健康教育和个性化可视化等领域开辟了新途径。", "keywords": "面部衰老, 扩散模型, 条件感知, 衰老树, 免训练", "comments": "本文提出了“衰老树”这一新颖概念，通过整合外部因素，超越了传统的确定性衰老模型，提供了一种更真实、更多功能的衰老模拟方法。“免训练扩散”方法的提出尤具创新性，降低了计算成本，使模型更具实用性。该研究强调平衡身份保留、年龄准确性和条件控制，并结合注意力混合和模拟衰老正则化等特定技术，展现了其方法论的稳健性。其在数字故事讲述和健康教育方面的应用潜力巨大，凸显了这项研究的实际影响力。"}}
{"id": "2506.21495", "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21495v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21495v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LLM离线与在线强化学习的桥接", "tldr": "本文研究了LLM微调中强化学习方法从离线到半在线再到完全在线的有效性，发现在线和半在线方法优于离线方法，并且多任务学习能进一步提升性能。", "motivation": "调查强化学习方法在LLM微调中，从离线到半在线再到完全在线场景的有效性。", "method": "实验涵盖可验证数学和不可验证指令遵循任务；比较了在线和半在线的直接偏好优化（Direct Preference Optimization）和群组奖励策略优化（Group Reward Policy Optimization）目标；分析了训练动态和超参数选择策略；尝试了可验证和不可验证奖励的多任务学习。", "result": "在线和半在线的DPO和GRPO变体表现和收敛性相似，且都显著优于离线方法。可验证和不可验证奖励的多任务学习能共同提升两种任务类型的性能。", "conclusion": "在线和半在线的强化学习方法在LLM微调中比离线方法更有效，且通过多任务学习可以进一步提升性能。", "translation": "我们研究了强化学习方法在微调大型语言模型时，从离线到半在线再到完全在线模式，针对可验证和不可验证任务的有效性。我们的实验涵盖了可验证数学和不可验证指令遵循的训练，并对两者进行了一系列基准评估。在这些设置中，我们广泛比较了在线和半在线的直接偏好优化（Direct Preference Optimization）和群组奖励策略优化（Group Reward Policy Optimization）目标，并惊讶地发现这些变体之间性能和收敛性相似，它们都显著优于离线方法。我们提供了训练动态和超参数选择策略的详细分析，以实现最佳结果。最后，我们展示了同时使用可验证和不可验证奖励进行多任务处理可以共同提高两种任务类型的性能。", "summary": "本文探讨了强化学习方法在大型语言模型微调中，从离线到在线不同阶段的有效性。研究对比了在线和半在线的DPO与GRPO目标，发现在线方法显著优于离线方法，且在线和半在线方法性能相似。此外，通过可验证和不可验证任务的多任务学习，模型性能得到进一步提升。", "keywords": "强化学习, 大型语言模型, 在线学习, 离线学习, 微调", "comments": "本文的创新点在于系统地比较了不同在线程度的强化学习方法在LLM微调中的表现，并发现在线方法优于离线方法，同时指出了多任务学习的潜力。研究结果对于LLM的实际部署和持续学习具有重要指导意义。"}}
{"id": "2506.21037", "title": "RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment", "authors": ["Suorong Yang", "Peijia Li", "Furao Shen", "Jian Zhao"], "summary": "Modern deep architectures often rely on large-scale datasets, but training on\nthese datasets incurs high computational and storage overhead. Real-world\ndatasets often contain substantial redundancies, prompting the need for more\ndata-efficient training paradigms. Data selection has shown promise to mitigate\nredundancy by identifying the most representative samples, thereby reducing\ntraining costs without compromising performance. Existing methods typically\nrely on static scoring metrics or pretrained models, overlooking the combined\neffect of selected samples and their evolving dynamics during training. We\nintroduce the concept of epsilon-sample cover, which quantifies sample\nredundancy based on inter-sample relationships, capturing the intrinsic\nstructure of the dataset. Based on this, we reformulate data selection as a\nreinforcement learning (RL) process and propose RL-Selector, where a\nlightweight RL agent optimizes the selection policy by leveraging\nepsilon-sample cover derived from evolving dataset distribution as a reward\nsignal. Extensive experiments across benchmark datasets and diverse\narchitectures demonstrate that our method consistently outperforms existing\nstate-of-the-art baselines. Models trained with our selected datasets show\nenhanced generalization performance with improved training efficiency.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21037v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21037v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "RL-Selector：基于冗余评估的强化学习引导数据选择", "tldr": "RL-Selector利用强化学习和epsilon-sample cover进行数据选择，以减少训练冗余并提高效率，同时增强模型泛化能力。", "motivation": "现代深度架构依赖大规模数据集，导致高计算和存储开销。真实世界数据集通常包含大量冗余，需要更数据高效的训练范式。现有数据选择方法通常依赖静态评分指标或预训练模型，忽视了所选样本的组合效应及其在训练过程中的演变动态。", "method": "引入了epsilon-sample cover的概念，通过量化样本间的关系来评估样本冗余，捕获数据集的内在结构。在此基础上，将数据选择重新定义为强化学习（RL）过程，并提出了RL-Selector。其中，一个轻量级RL代理利用从演变数据集分布中获得的epsilon-sample cover作为奖励信号，来优化选择策略。", "result": "在基准数据集和不同架构上的大量实验表明，该方法始终优于现有的最先进基线。使用我们选择的数据集训练的模型显示出增强的泛化性能和更高的训练效率。", "conclusion": "RL-Selector通过强化学习引导的数据选择，有效解决了大规模数据集训练中的冗余问题，提高了训练效率和模型泛化能力。", "translation": "现代深度架构通常依赖于大规模数据集，但在此类数据集上进行训练会导致高昂的计算和存储开销。真实世界数据集通常包含大量冗余，这促使人们需要更数据高效的训练范式。数据选择已显示出通过识别最具代表性的样本来减轻冗余的潜力，从而在不损害性能的情况下降低训练成本。现有方法通常依赖静态评分指标或预训练模型，忽视了所选样本的组合效应及其在训练过程中的演变动态。我们引入了epsilon-sample cover的概念，它基于样本间关系量化样本冗余，捕捉数据集的内在结构。在此基础上，我们将数据选择重新表述为强化学习（RL）过程，并提出了RL-Selector，其中一个轻量级RL代理通过利用从演变数据集分布中获得的epsilon-sample cover作为奖励信号来优化选择策略。在基准数据集和不同架构上的大量实验表明，我们的方法始终优于现有最先进的基线。使用我们选择的数据集训练的模型显示出增强的泛化性能和更高的训练效率。", "summary": "RL-Selector是一种通过强化学习引导的数据选择方法，旨在解决大规模数据集训练中的冗余问题。该方法引入了epsilon-sample cover来量化样本冗余并捕捉数据集的内在结构，将数据选择建模为强化学习过程。一个轻量级RL代理利用epsilon-sample cover作为奖励信号来动态优化数据选择策略。实验结果表明，RL-Selector在提高训练效率和模型泛化性能方面优于现有最先进的方法。", "keywords": "数据选择, 强化学习, 冗余评估, epsilon-sample cover, 深度学习", "comments": "该论文的创新点在于引入了epsilon-sample cover来量化样本冗余，并巧妙地将数据选择问题转化为一个强化学习任务。这种动态的数据选择策略能够适应训练过程中数据集分布的变化，克服了传统静态方法的局限性，对于在大规模数据集上实现高效且高性能的深度学习模型训练具有重要意义。"}}
{"id": "2506.21009", "title": "User-in-the-Loop View Sampling with Error Peaking Visualization", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Shohei Mori"], "summary": "Augmented reality (AR) provides ways to visualize missing view samples for\nnovel view synthesis. Existing approaches present 3D annotations for new view\nsamples and task users with taking images by aligning the AR display. This data\ncollection task is known to be mentally demanding and limits capture areas to\npre-defined small areas due to the ideal but restrictive underlying sampling\ntheory. To free users from 3D annotations and limited scene exploration, we\npropose using locally reconstructed light fields and visualizing errors to be\nremoved by inserting new views. Our results show that the error-peaking\nvisualization is less invasive, reduces disappointment in final results, and is\nsatisfactory with fewer view samples in our mobile view synthesis system. We\nalso show that our approach can contribute to recent radiance field\nreconstruction for larger scenes, such as 3D Gaussian splatting.", "comment": "Accepted at IEEE ICIP 2025, Project Page:\n  https://mediated-reality.github.io/projects/yasunaga_icip25/", "pdf_url": "http://arxiv.org/pdf/2506.21009v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21009v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用户在环的视图采样与误差峰值可视化", "tldr": "该论文提出了一种新的AR中新视图合成的视图采样方法，通过可视化误差来指导用户，使数据收集过程更轻松高效，并适用于大场景。", "motivation": "现有的增强现实（AR）视图采样方法对用户心理要求高，需要3D注释，并且由于采样理论的限制，捕获区域小。本文的动机是让用户摆脱这些限制，实现更轻松高效的新视图合成数据收集。", "method": "本文提出利用局部重建的光场，并通过“误差峰值可视化”来指导用户插入新的视图以消除误差，从而进行视图采样。", "result": "误差峰值可视化侵入性更小，减少了最终结果的失望，并且在移动视图合成系统中用更少的视图样本就能达到满意效果。此外，该方法还可用于更大场景的辐射场重建，如3D高斯泼溅。", "conclusion": "所提出的误差峰值可视化方法提高了新视图合成中视图采样的用户体验和效率，使其更省力并适用于更大场景。", "translation": "增强现实（AR）提供了可视化缺失视图样本以进行新视图合成的方法。现有方法为新视图样本提供3D注释，并要求用户通过对齐AR显示器来拍摄图像。众所周知，这种数据收集任务对精神要求很高，并且由于理想但限制性的底层采样理论，将捕获区域限制在预定义的小区域。为了将用户从3D注释和有限的场景探索中解放出来，我们建议使用局部重建的光场并可视化通过插入新视图要消除的误差。我们的结果表明，误差峰值可视化侵入性更小，减少了最终结果的失望，并且在我们的移动视图合成系统中，使用更少的视图样本就令人满意。我们还表明，我们的方法可以为更大场景的最新辐射场重建做出贡献，例如3D高斯泼溅。", "summary": "本文介绍了一种用于增强现实中新视图合成的用户在环视图采样方法。为克服当前依赖3D注释且对用户要求高、限制性强的方法，作者提出利用局部重建的光场和“误差峰值可视化”来引导用户。研究表明，该方法侵入性更小，提高了用户满意度，所需样本量更少，并且适用于3D高斯泼溅等更大场景的重建技术。", "keywords": "视图采样, 增强现实, 新视图合成, 误差可视化, 光场, 3D高斯泼溅", "comments": "这篇论文通过将数据收集从显式3D注释转变为直观的误差可视化，解决了AR中新视图合成数据收集的实际挑战。这一创新显著改善了用户体验和效率，使过程更易于访问并可扩展到更大的场景，这对于实际的AR应用至关重要。"}}
{"id": "2506.21497", "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21497v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21497v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过交互式LLM对齐增强社交驱动对话中的用户参与度", "tldr": "通过利用用户意图反馈并结合用户模拟器和i×MCTS，本研究使用DPO对齐交互式LLM，从而有效提升社交驱动对话中的用户参与度。", "motivation": "现有研究虽优化了LLM以推理相关知识或规划对话行为流，但用户参与度与知识或对话行为之间的关系微妙，无法保证社交驱动对话中的用户参与度。", "method": "本文通过利用对话未来发展的信号，使交互式LLM学习用户参与度。具体而言，采用用户交互后与对话意图相关的反应作为奖励信号来对齐交互式LLM。为此，开发了一个用户模拟器，并利用i×MCTS（交互式蒙特卡洛树搜索）来探索用户与LLM系统之间的交互，从而收集包含高质量和低质量体验的数据集，并使用直接偏好优化（DPO）对齐交互式LLM以实现高水平的用户参与度。", "result": "在情感支持对话和劝善等两种社交驱动对话场景中进行的实验表明，该方法有效地增强了交互式LLM中的用户参与度。", "conclusion": "本文提出的通过用户意图反馈和i×MCTS结合DPO对齐交互式LLM的方法，能够显著提升社交驱动对话中的用户参与度。", "translation": "标题：通过交互式LLM对齐增强社交驱动对话中的用户参与度\n\n摘要：通过交互增强用户参与度在社交驱动对话中扮演着重要角色。虽然先前的研究已经优化了模型以推理相关知识或规划对话行为流，但用户参与度与知识或对话行为之间的关系是微妙的，并不能保证社交驱动对话中的用户参与度。为此，我们通过利用对话未来发展的信号，使交互式LLM能够学习用户参与度。具体而言，我们采用一个更直接和相关的用户参与度指标，即交互后用户与对话意图相关的反应，作为奖励来对齐交互式LLM。为了实现这一点，我们开发了一个用户模拟器与目标交互式LLM进行交互，并通过i×MCTS（交互式蒙特卡洛树搜索）探索用户与交互式LLM系统之间的交互。通过这种方式，我们收集了一个包含通过i×MCTS获得的更高和更低质量体验对的数据集，并相应地通过直接偏好优化（DPO）对齐交互式LLM以实现高水平的用户参与度。在两种社交驱动对话场景（情感支持对话和劝善）中进行的实验表明，我们的方法有效地增强了交互式LLM中的用户参与度。", "summary": "本文提出一种通过交互式LLM对齐来增强社交驱动对话中用户参与度的方法。针对现有方法无法保证用户参与度的问题，作者利用对话未来发展中的用户意图反馈作为奖励信号，通过开发用户模拟器和i×MCTS来探索用户与LLM的交互，从而收集高质量和低质量体验数据。随后，利用直接偏好优化（DPO）对齐交互式LLM。实验证明，该方法在情感支持和劝善对话场景中有效提升了用户参与度。", "keywords": "用户参与度, 社交对话, LLM对齐, i×MCTS, 直接偏好优化", "comments": "本文的创新点在于引入了“用户交互后与对话意图相关的反应”作为直接的用户参与度指标，并通过用户模拟器和i×MCTS模拟交互过程来收集偏好数据，进而利用DPO对齐LLM。这种方法更直接地关注了用户行为反馈，而非仅仅依赖知识或对话行为规划，为提升社交对话LLM的实用性和用户体验提供了新的视角。其局限性可能在于用户模拟器的真实性以及所收集偏好数据的规模和多样性。"}}
{"id": "2506.21039", "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning", "authors": ["Jaebak Hwang", "Sanghyeon Lee", "Jeongmo Kim", "Seungyul Han"], "summary": "Long-horizon goal-conditioned tasks pose fundamental challenges for\nreinforcement learning (RL), particularly when goals are distant and rewards\nare sparse. While hierarchical and graph-based methods offer partial solutions,\nthey often suffer from subgoal infeasibility and inefficient planning. We\nintroduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL\nframework that enforces single-step subgoal reachability by structurally\nconstraining high-level decision-making. To enhance exploration, SSE employs a\ndecoupled exploration policy that systematically traverses underexplored\nregions of the goal space. Furthermore, a failure-aware path refinement, which\nrefines graph-based planning by dynamically adjusting edge costs according to\nobserved low-level success rates, thereby improving subgoal reliability.\nExperimental results across diverse long-horizon benchmarks demonstrate that\nSSE consistently outperforms existing goal-conditioned RL and hierarchical RL\napproaches in both efficiency and success rate.", "comment": "9 technical page followed by references and appendix", "pdf_url": "http://arxiv.org/pdf/2506.21039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21039v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "严格子目标执行：分层强化学习中可靠的长周期规划", "tldr": "本文提出了严格子目标执行（SSE），一个基于图的分层强化学习框架，通过结构化约束高层决策来确保单步子目标可达性，并结合了解耦探索策略和故障感知路径优化，以解决长周期任务中的子目标不可行性和规划效率低下问题，实验证明其在效率和成功率上均优于现有方法。", "motivation": "长周期目标条件任务对强化学习（RL）提出了根本性挑战，尤其是在目标距离遥远且奖励稀疏的情况下。现有的分层和基于图的方法虽然提供部分解决方案，但通常存在子目标不可行性和规划效率低下的问题。", "method": "本文引入了严格子目标执行（SSE），一个基于图的分层RL框架。SSE通过结构化约束高层决策来强制执行单步子目标可达性。为增强探索，SSE采用解耦探索策略，系统地遍历目标空间中探索不足的区域。此外，它还包含一个故障感知路径优化机制，通过根据观察到的低层成功率动态调整边缘成本来改进基于图的规划，从而提高子目标的可靠性。", "result": "在各种长周期基准测试中的实验结果表明，SSE在效率和成功率方面均持续优于现有的目标条件RL和分层RL方法。", "conclusion": "严格子目标执行（SSE）框架通过强制执行单步子目标可达性、采用解耦探索策略和故障感知路径优化，有效解决了长周期任务中子目标不可行性和规划效率低下的问题，显著提高了分层强化学习的性能。", "translation": "长周期目标条件任务对强化学习（RL）提出了根本性挑战，尤其是在目标距离遥远且奖励稀疏的情况下。尽管分层和基于图的方法提供部分解决方案，但它们常常面临子目标不可行性和规划效率低下的问题。我们引入了严格子目标执行（SSE），一个基于图的分层RL框架，它通过结构化约束高层决策来强制执行单步子目标可达性。为了增强探索，SSE采用了一种解耦探索策略，系统地遍历目标空间中探索不足的区域。此外，故障感知路径优化通过根据观察到的低层成功率动态调整边缘成本来改进基于图的规划，从而提高了子目标的可靠性。在各种长周期基准测试中的实验结果表明，SSE在效率和成功率方面均持续优于现有的目标条件RL和分层RL方法。", "summary": "本文提出了一种名为严格子目标执行（SSE）的基于图的分层强化学习框架，旨在解决长周期目标条件任务中子目标不可行性和规划效率低下的问题。SSE通过结构化约束确保单步子目标可达性，并结合了解耦探索策略以促进对未探索区域的遍历，以及故障感知路径优化机制以根据低层成功率动态调整规划。实验证明，SSE在效率和成功率方面均优于现有方法。", "keywords": "分层强化学习, 子目标执行, 长周期规划, 图基方法, 稀疏奖励", "comments": "该论文的创新点在于提出了“严格子目标执行”的概念，通过结构化约束强制保证子目标的可达性，并结合了新颖的解耦探索策略和故障感知路径优化，有效提升了分层强化学习在长周期稀疏奖励任务中的性能和可靠性。这对于实际应用中需要稳定长周期规划的场景具有重要意义。"}}
{"id": "2506.21011", "title": "Bridging Video Quality Scoring and Justification via Large Multimodal Models", "authors": ["Qizhi Xie", "Kun Yuan", "Yunpeng Qu", "Jiachao Gong", "Mingda Wu", "Ming Sun", "Chao Zhou", "Jihong Zhu"], "summary": "Classical video quality assessment (VQA) methods generate a numerical score\nto judge a video's perceived visual fidelity and clarity. Yet, a score fails to\ndescribe the video's complex quality dimensions, restricting its applicability.\nBenefiting from the linguistic output, adapting video large multimodal models\n(LMMs) to VQA via instruction tuning has the potential to address this issue.\nThe core of the approach lies in the video quality-centric instruction data.\nPrevious explorations mainly focus on the image domain, and their data\ngeneration processes heavily rely on human quality annotations and proprietary\nsystems, limiting data scalability and effectiveness. To address these\nchallenges, we propose the Score-based Instruction Generation (SIG) pipeline.\nSpecifically, SIG first scores multiple quality dimensions of an unlabeled\nvideo and maps scores to text-defined levels. It then explicitly incorporates a\nhierarchical Chain-of-Thought (CoT) to model the correlation between specific\ndimensions and overall quality, mimicking the human visual system's reasoning\nprocess. The automated pipeline eliminates the reliance on expert-written\nquality descriptions and proprietary systems, ensuring data scalability and\ngeneration efficiency. To this end, the resulting Score2Instruct (S2I) dataset\ncontains over 320K diverse instruction-response pairs, laying the basis for\ninstruction tuning. Moreover, to advance video LMMs' quality scoring and\njustification abilities simultaneously, we devise a progressive tuning strategy\nto fully unleash the power of S2I. Built upon SIG, we further curate a\nbenchmark termed S2I-Bench with 400 open-ended questions to better evaluate the\nquality justification capacity of video LMMs. Experimental results on the\nS2I-Bench and existing benchmarks indicate that our method consistently\nimproves quality scoring and justification capabilities across multiple video\nLMMs.", "comment": "15 pages, 4 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2506.21011v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21011v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过大型多模态模型连接视频质量评分与解释", "tldr": "本文提出了一种名为SIG的管道，用于自动生成视频质量评估的指令数据（Score2Instruct数据集），并开发了一种渐进式调优策略和S2I-Bench基准，以提升大型多模态模型在视频质量评分和解释方面的能力。", "motivation": "传统的视频质量评估（VQA）方法仅生成数值分数，无法描述视频复杂的质量维度，限制了其适用性。现有的指令数据生成方法主要集中在图像领域，且高度依赖人工标注和专有系统，导致数据可扩展性和效率受限。", "method": "本文提出了基于分数的指令生成（Score-based Instruction Generation, SIG）管道。SIG首先对未标注视频的多个质量维度进行评分，并将其映射到文本定义的级别。然后，它明确地结合了分层思维链（Chain-of-Thought, CoT）来模拟人类视觉系统，建模特定维度与整体质量之间的关联。该自动化管道消除了对专家编写质量描述和专有系统的依赖。在此基础上，生成了包含超过320K个指令-响应对的Score2Instruct (S2I) 数据集。此外，为同时提升视频大型多模态模型（LMMs）的质量评分和解释能力，本文设计了一种渐进式调优策略。基于SIG，进一步构建了一个包含400个开放式问题的S2I-Bench基准，以更好地评估视频LMMs的质量解释能力。", "result": "在S2I-Bench和现有基准上的实验结果表明，该方法持续改进了多个视频大型多模态模型的质量评分和解释能力。", "conclusion": "本文提出的SIG管道及其生成的S2I数据集和S2I-Bench基准，有效提升了大型多模态模型在视频质量评估中的评分和解释能力，解决了传统方法和现有数据生成方法的局限性。", "translation": "经典的视频质量评估（VQA）方法生成一个数值分数来判断视频感知的视觉保真度和清晰度。然而，一个分数无法描述视频复杂的质量维度，限制了其适用性。受益于语言输出，通过指令调优将视频大型多模态模型（LMMs）应用于VQA，有潜力解决这个问题。该方法的核心在于以视频质量为中心的指令数据。之前的探索主要集中在图像领域，其数据生成过程高度依赖人工质量标注和专有系统，限制了数据的可扩展性和有效性。为了解决这些挑战，我们提出了基于分数的指令生成（SIG）管道。具体来说，SIG首先对未标注视频的多个质量维度进行评分，并将分数映射到文本定义的级别。然后，它明确地结合了分层思维链（CoT）来建模特定维度与整体质量之间的关联，模仿人类视觉系统的推理过程。该自动化管道消除了对专家编写质量描述和专有系统的依赖，确保了数据的可扩展性和生成效率。为此，生成的Score2Instruct (S2I) 数据集包含超过320K个多样化的指令-响应对，为指令调优奠定了基础。此外，为了同时提升视频LMMs的质量评分和解释能力，我们设计了一种渐进式调优策略以充分释放S2I的潜力。在SIG的基础上，我们进一步策划了一个名为S2I-Bench的基准，包含400个开放式问题，以更好地评估视频LMMs的质量解释能力。S2I-Bench和现有基准上的实验结果表明，我们的方法持续改进了多个视频LMMs的质量评分和解释能力。", "summary": "本文针对传统视频质量评估（VQA）仅提供数值分数而缺乏解释性的问题，提出了一种基于分数的指令生成（SIG）管道。该管道能够自动化生成大规模视频质量评估指令数据（Score2Instruct数据集），并结合分层思维链（CoT）来模拟人类推理过程。此外，本文还设计了渐进式调优策略和S2I-Bench基准，以提升大型多模态模型（LMMs）在视频质量评分和解释方面的能力。实验证明，该方法显著改善了多个视频LMMs的性能。", "keywords": "视频质量评估, 大型多模态模型, 指令调优, 数据生成, 思维链", "comments": "本文的创新点在于提出了一个自动化的数据生成管道SIG，解决了视频质量评估领域高质量指令数据稀缺的问题，并摆脱了对人工标注和专有系统的依赖，极大地提升了数据生成的可扩展性和效率。通过引入分层思维链（CoT），模型能够更好地模拟人类的推理过程，从而提供更具解释性的视频质量评估。这项工作对于推动大型多模态模型在视频理解和评价方面的应用具有重要意义。"}}
{"id": "2506.21044", "title": "Efficient Skill Discovery via Regret-Aware Optimization", "authors": ["He Zhang", "Ming Zhou", "Shaopeng Zhai", "Ying Sun", "Hui Xiong"], "summary": "Unsupervised skill discovery aims to learn diverse and distinguishable\nbehaviors in open-ended reinforcement learning. For existing methods, they\nfocus on improving diversity through pure exploration, mutual information\noptimization, and learning temporal representation. Despite that they perform\nwell on exploration, they remain limited in terms of efficiency, especially for\nthe high-dimensional situations. In this work, we frame skill discovery as a\nmin-max game of skill generation and policy learning, proposing a regret-aware\nmethod on top of temporal representation learning that expands the discovered\nskill space along the direction of upgradable policy strength. The key insight\nbehind the proposed method is that the skill discovery is adversarial to the\npolicy learning, i.e., skills with weak strength should be further explored\nwhile less exploration for the skills with converged strength. As an\nimplementation, we score the degree of strength convergence with regret, and\nguide the skill discovery with a learnable skill generator. To avoid\ndegeneration, skill generation comes from an up-gradable population of skill\ngenerators. We conduct experiments on environments with varying complexities\nand dimension sizes. Empirical results show that our method outperforms\nbaselines in both efficiency and diversity. Moreover, our method achieves a 15%\nzero shot improvement in high-dimensional environments, compared to existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21044v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21044v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过后悔感知优化实现高效技能发现", "tldr": "本文提出了一种名为“后悔感知优化”的新型无监督技能发现方法，将技能发现框架化为技能生成和策略学习的极小极大博弈，以提高高维环境下的效率和多样性。", "motivation": "现有无监督技能发现方法在探索方面表现良好，但在效率上仍有限制，尤其是在高维情况下。", "method": "将技能发现框架化为技能生成和策略学习的极小极大博弈，并提出一种基于时间表征学习的后悔感知方法。该方法根据策略强度收敛程度（通过后悔衡量）调整探索方向，对强度较弱的技能进行更多探索。通过可学习的技能生成器指导技能发现，并利用可升级的技能生成器群体避免退化。", "result": "实验结果表明，该方法在效率和多样性方面均优于现有基线方法。在高维环境中，该方法比现有方法实现了15%的零样本改进。", "conclusion": "该研究提出了一种高效且多样化的无监督技能发现方法，通过后悔感知优化和极小极大博弈框架，显著提升了高维环境下的性能和探索效率。", "translation": "无监督技能发现旨在在开放式强化学习中学习多样且可区分的行为。现有方法侧重于通过纯粹探索、互信息优化和学习时间表征来提高多样性。尽管它们在探索方面表现良好，但在效率方面仍有限制，尤其是在高维情况下。在这项工作中，我们将技能发现框定为技能生成和策略学习的极小极大博弈，提出了一种基于时间表征学习的后悔感知方法，该方法沿着可升级策略强度的方向扩展发现的技能空间。该方法背后的关键见解是技能发现与策略学习是对抗性的，即对强度较弱的技能应进行更多探索，而对强度收敛的技能则进行较少探索。作为一种实现，我们用后悔来衡量强度收敛的程度，并用一个可学习的技能生成器来指导技能发现。为避免退化，技能生成来自一个可升级的技能生成器群体。我们在不同复杂度和维度大小的环境中进行了实验。经验结果表明，我们的方法在效率和多样性方面均优于基线。此外，与现有方法相比，我们的方法在高维环境中实现了15%的零样本改进。", "summary": "本文提出了一种名为“后悔感知优化”的高效无监督技能发现方法，旨在解决现有方法在效率上的局限性，尤其是在高维环境中。该方法将技能发现建模为技能生成与策略学习的极小极大博弈，并利用后悔机制指导技能探索，优先发现强度较弱但有潜力的技能。实验证明，该方法在效率和多样性上均优于现有基线，并在高维环境中取得了显著的零样本性能提升。", "keywords": "技能发现, 后悔感知, 极小极大博弈, 强化学习, 高效探索", "comments": "这篇论文通过将技能发现问题转化为一个极小极大博弈，并引入“后悔感知”机制来动态调整探索强度，为无监督技能发现提供了一个新颖且高效的视角。其创新点在于利用策略强度的收敛程度（通过后悔衡量）来指导技能生成，从而避免了无效探索并提高了效率，尤其在高维环境中的零样本改进显示了其潜力。"}}
{"id": "2506.21012", "title": "FedSC: Federated Learning with Semantic-Aware Collaboration", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Jiahua Shi", "Jun Shen"], "summary": "Federated learning (FL) aims to train models collaboratively across clients\nwithout sharing data for privacy-preserving. However, one major challenge is\nthe data heterogeneity issue, which refers to the biased labeling preferences\nat multiple clients. A number of existing FL methods attempt to tackle data\nheterogeneity locally (e.g., regularizing local models) or globally (e.g.,\nfine-tuning global model), often neglecting inherent semantic information\ncontained in each client. To explore the possibility of using intra-client\nsemantically meaningful knowledge in handling data heterogeneity, in this\npaper, we propose Federated Learning with Semantic-Aware Collaboration (FedSC)\nto capture client-specific and class-relevant knowledge across heterogeneous\nclients. The core idea of FedSC is to construct relational prototypes and\nconsistent prototypes at semantic-level, aiming to provide fruitful class\nunderlying knowledge and stable convergence signals in a prototype-wise\ncollaborative way. On the one hand, FedSC introduces an inter-contrastive\nlearning strategy to bring instance-level embeddings closer to relational\nprototypes with the same semantics and away from distinct classes. On the other\nhand, FedSC devises consistent prototypes via a discrepancy aggregation manner,\nas a regularization penalty to constrain the optimization region of the local\nmodel. Moreover, a theoretical analysis for FedSC is provided to ensure a\nconvergence guarantee. Experimental results on various challenging scenarios\ndemonstrate the effectiveness of FedSC and the efficiency of crucial\ncomponents.", "comment": "12 pages, KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.21012v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21012v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FedSC：语义感知协作的联邦学习", "tldr": "FedSC通过构建语义级关系原型和一致原型来解决联邦学习中的数据异质性问题，从而在保护隐私的同时实现有效的协作训练。", "motivation": "联邦学习在保护隐私方面存在数据异质性挑战，现有方法常忽略客户端固有的语义信息。本文旨在探索利用客户端内部语义信息来处理数据异质性问题。", "method": "本文提出了FedSC（Federated Learning with Semantic-Aware Collaboration）方法。其核心思想是在语义层面构建关系原型（relational prototypes）和一致原型（consistent prototypes）。具体地，FedSC引入了一种跨对比学习策略，使实例级嵌入与相同语义的关系原型更接近，并远离不同类别。此外，FedSC通过差异聚合方式设计了一致原型，作为正则化惩罚来约束局部模型的优化区域。文章还提供了FedSC的理论收敛性分析。", "result": "在各种具有挑战性的场景下进行的实验结果表明，FedSC的有效性以及其关键组件的效率。", "conclusion": "FedSC通过引入语义感知协作，有效地解决了联邦学习中的数据异质性问题，并通过构建关系原型和一致原型，提供了丰富的类别底层知识和稳定的收敛信号，并得到了理论收敛性保证。", "translation": "联邦学习（FL）旨在跨客户端协同训练模型，同时不共享数据以保护隐私。然而，一个主要的挑战是数据异质性问题，这指的是多个客户端的标签偏好存在偏差。许多现有的FL方法尝试在本地（例如，正则化局部模型）或全局（例如，微调全局模型）解决数据异质性问题，但往往忽略了每个客户端中固有的语义信息。为了探索利用客户端内部语义有意义的知识来处理数据异质性的可能性，本文提出了语义感知协作的联邦学习（FedSC），以捕获异构客户端的客户端特定和类别相关知识。FedSC的核心思想是在语义层面构建关系原型和一致原型，旨在以原型协作的方式提供丰富的类别底层知识和稳定的收敛信号。一方面，FedSC引入了一种跨对比学习策略，使实例级嵌入更接近具有相同语义的关系原型，并远离不同的类别。另一方面，FedSC通过差异聚合方式设计了一致原型，作为正则化惩罚来约束局部模型的优化区域。此外，本文提供了FedSC的理论分析以确保收敛性。在各种具有挑战性场景下的实验结果证明了FedSC的有效性和关键组件的效率。", "summary": "本文提出了FedSC（Federated Learning with Semantic-Aware Collaboration），旨在解决联邦学习中的数据异质性问题。FedSC通过在语义层面构建关系原型和一致原型，利用客户端内部的语义信息。它引入了跨对比学习策略来优化实例嵌入，并利用一致原型作为正则化项。理论分析和实验结果均验证了FedSC在处理数据异质性方面的有效性和收敛性。", "keywords": "联邦学习, 数据异质性, 语义感知, 原型学习, 协作训练", "comments": "该论文创新性地将语义信息融入联邦学习框架，通过语义级原型来解决数据异质性问题，这为联邦学习的进一步发展提供了新的视角。其提出的跨对比学习和一致原型设计具有新颖性，并提供了理论收敛性保证，增强了方法的可靠性。"}}
{"id": "2506.21521", "title": "Potemkin Understanding in Large Language Models", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21521v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21521v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "大型语言模型中的波将金式理解", "tldr": "本文提出并量化了大型语言模型中“波将金式理解”现象，即模型在基准测试上的成功可能只是理解的假象，源于与人类理解方式不符的内部不一致性。", "motivation": "大型语言模型（LLMs）经常通过基准数据集进行评估，但作者质疑这种评估能否真正反映LLM的能力，特别是如果LLMs的错误理解方式与人类不同。", "method": "本文首先提出了一个形式化框架来解决评估有效性问题。然后，提出了两种量化“波将金式理解”存在的程序：一种是使用在三个领域专门设计的基准测试，另一种是提供其普遍性下限的通用程序。", "result": "研究发现，“波将金式理解”在不同模型、任务和领域中普遍存在。这些失败不仅反映了不正确的理解，还反映了概念表征中更深层次的内部不一致性。", "conclusion": "基准测试的成功可能仅是大型语言模型“波将金式理解”的体现，即一种理解的假象，其答案与人类对概念的解释方式不符，这揭示了模型内部概念表征的深层不连贯性。", "translation": "大型语言模型（LLMs）通常使用基准数据集进行评估。但如何证明基于LLM对一组精心策划问题的回答来推断其能力的合理性？本文首先提出了一个形式化框架来解决这个问题。关键在于，用于测试LLMs的基准测试（如AP考试）也用于测试人类。然而，这带来了一个隐含的推论：只有当LLMs对概念的误解方式与人类的误解方式相似时，这些基准测试才是有效的。否则，在基准测试上的成功仅表明波将金式理解：一种理解的假象，由与任何人类解释概念的方式不符的答案所驱动。我们提出了两种量化波将金式现象存在的程序：一种是在三个领域使用专门设计的基准测试，另一种是提供其普遍性下限的通用程序。我们发现波将金式现象在不同模型、任务和领域中普遍存在。我们还发现，这些失败不仅反映了不正确的理解，还反映了概念表征中更深层次的内部不一致性。", "summary": "本文探讨了大型语言模型（LLMs）在基准测试上的表现是否真正反映了其理解能力。作者提出了“波将金式理解”的概念，指出如果LLMs的错误理解方式与人类不同，那么基准测试的成功可能只是理解的假象。通过引入形式化框架和两种量化程序，包括使用专门设计的基准测试和通用程序，研究发现“波将金式理解”在各种模型、任务和领域中普遍存在，并且揭示了模型内部概念表征的深层不一致性。", "keywords": "大型语言模型, 波将金式理解, 基准测试, 模型评估, 概念表征", "comments": "本文创新性地提出了“波将金式理解”这一概念，深刻质疑了当前大型语言模型基准测试的有效性。它提醒我们，模型在表面上的成功可能掩盖了其深层理解的缺陷和内部不一致性，对LLMs的评估和未来发展方向具有重要指导意义。"}}
{"id": "2506.21054", "title": "FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning", "authors": ["Fu Peng", "Ming Tang"], "summary": "In federated learning (FL), the data distribution of each client may change\nover time, introducing both temporal and spatial data heterogeneity, known as\nconcept drift. Data heterogeneity arises from three drift sources: real drift\n(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the\ninput distribution P(x)), and label drift (a shift in the label distribution\nP(y)). However, most existing FL methods addressing concept drift primarily\nfocus on real drift. When clients experience virtual or label drift, these\nmethods often fail to selectively retain useful historical knowledge, leading\nto catastrophic forgetting. A key challenge lies in distinguishing different\nsources of drift, as they require distinct adaptation strategies: real drift\ncalls for discarding outdated data, while virtual or label drift benefits from\nretaining historical data. Without explicitly identifying the drift sources, a\ngeneral adaptation strategy is suboptimal and may harm generalization. To\naddress this challenge, we propose FedDAA, a dynamic clustered FL framework\ndesigned to adapt to multi-source concept drift while preserving valuable\nhistorical knowledge. Specifically, FedDAA integrates three modules: a cluster\nnumber determination module to find the optimal number of clusters; a real\ndrift detection module to distinguish real drift from virtual/label drift; and\na concept drift adaptation module to adapt to new data while retaining useful\nhistorical information. We provide theoretical convergence guarantees, and\nexperiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over\nstate-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21054v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21054v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FedDAA：联邦学习中概念漂移适应的动态客户端聚类", "tldr": "FedDAA通过动态客户端聚类和多源漂移检测，解决了联邦学习中由于概念漂移导致的历史知识遗忘问题，并显著提升了性能。", "motivation": "在联邦学习中，客户端数据分布随时间变化导致概念漂移，引入时空数据异质性。现有方法主要关注真实漂移，忽视虚拟漂移和标签漂移，导致灾难性遗忘，且无法区分不同漂移源以采取恰当的适应策略。", "method": "提出FedDAA，一个动态聚类联邦学习框架，旨在适应多源概念漂移并保留历史知识。它包含三个模块：集群数量确定模块（寻找最优集群数）、真实漂移检测模块（区分真实漂移与虚拟/标签漂移）和概念漂移适应模块（适应新数据同时保留有用历史信息）。", "result": "FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上比现有最先进方法提高了7.84%到8.52%的准确率，并提供了理论收敛保证。", "conclusion": "FedDAA通过有效区分和适应多源概念漂移，并在保留有用历史知识的同时，显著提升了联邦学习在数据漂移环境下的性能。", "translation": "在联邦学习（FL）中，每个客户端的数据分布可能随时间变化，引入了时间和空间上的数据异质性，这被称为概念漂移。数据异质性源于三种漂移来源：真实漂移（条件分布P(y|x)的变化）、虚拟漂移（输入分布P(x)的变化）和标签漂移（标签分布P(y)的变化）。然而，大多数现有解决概念漂移的联邦学习方法主要关注真实漂移。当客户端经历虚拟漂移或标签漂移时，这些方法往往无法选择性地保留有用的历史知识，导致灾难性遗忘。一个关键挑战在于区分不同来源的漂移，因为它们需要不同的适应策略：真实漂移需要丢弃过时数据，而虚拟漂移或标签漂移则受益于保留历史数据。如果不明确识别漂移来源，通用的适应策略将是次优的，并可能损害泛化能力。为了解决这一挑战，我们提出了FedDAA，一个动态聚类联邦学习框架，旨在适应多源概念漂移，同时保留有价值的历史知识。具体而言，FedDAA集成了三个模块：一个集群数量确定模块来寻找最优集群数量；一个真实漂移检测模块来区分真实漂移与虚拟/标签漂移；以及一个概念漂移适应模块来适应新数据同时保留有用的历史信息。我们提供了理论收敛保证，实验表明FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上比现有最先进方法提高了7.84%到8.52%的准确率。", "summary": "本文提出了FedDAA，一个针对联邦学习中多源概念漂移的动态聚类框架。它通过区分真实漂移、虚拟漂移和标签漂移，并采用不同的适应策略（丢弃或保留历史数据），有效解决了现有方法在处理非真实漂移时灾难性遗忘的问题。FedDAA包含集群数量确定、真实漂移检测和概念漂移适应三个模块，并在多个数据集上取得了显著的准确率提升。", "keywords": "联邦学习, 概念漂移, 动态聚类, 数据异质性, 灾难性遗忘", "comments": "FedDAA的创新点在于其能够区分联邦学习中不同类型的概念漂移（真实、虚拟、标签），并针对性地采取不同的适应策略，从而有效避免了灾难性遗忘，并更好地保留了历史知识。这种细致的漂移处理方式是现有方法所欠缺的，对于提升联邦学习在动态数据环境下的鲁棒性和泛化能力具有重要意义。"}}
{"id": "2506.21015", "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation", "authors": ["Qingyue Jiao", "Kangyu Zheng", "Yiyu Shi", "Zhiding Liang"], "summary": "Machine learning-assisted diagnosis is gaining traction in skin disease\ndetection, but training effective models requires large amounts of high-quality\ndata. Skin disease datasets often suffer from class imbalance, privacy\nconcerns, and object bias, making data augmentation essential. While classical\ngenerative models are widely used, they demand extensive computational\nresources and lengthy training time. Quantum computing offers a promising\nalternative, but existing quantum-based image generation methods can only yield\ngrayscale low-quality images. Through a novel classical-quantum latent space\nfusion technique, our work overcomes this limitation and introduces the first\nclassical-quantum generative adversarial network (GAN) capable of generating\ncolor medical images. Our model outperforms classical deep convolutional GANs\nand existing hybrid classical-quantum GANs in both image generation quality and\nclassification performance boost when used as data augmentation. Moreover, the\nperformance boost is comparable with that achieved using state-of-the-art\nclassical generative models, yet with over 25 times fewer parameters and 10\ntimes fewer training epochs. Such results suggest a promising future for\nquantum image generation as quantum hardware advances. Finally, we demonstrate\nthe robust performance of our model on real IBM quantum machine with hardware\nnoise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21015v1", "categories": ["cs.CV", "cs.LG", "quant-ph"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21015v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HybridQ：用于皮肤病图像生成的混合经典-量子生成对抗网络", "tldr": "HybridQ是一种混合经典-量子生成对抗网络，通过新的潜在空间融合技术生成彩色皮肤病图像，在性能上优于现有模型，且参数和训练时间更少，显示了量子图像生成的潜力。", "motivation": "皮肤病诊断的机器学习模型需要大量高质量数据，但现有数据集存在类别不平衡、隐私和对象偏差问题。经典生成模型计算资源消耗大、训练时间长。现有量子图像生成方法只能产生低质量灰度图像。", "method": "本文提出了一种新颖的经典-量子潜在空间融合技术，并在此基础上构建了首个能够生成彩色医学图像的混合经典-量子生成对抗网络 (HybridQ)。", "result": "HybridQ在图像生成质量和作为数据增强时的分类性能提升方面均优于经典的深度卷积GAN和现有混合经典-量子GAN。其性能提升与最先进的经典生成模型相当，但参数量减少了25倍以上，训练周期减少了10倍。该模型在真实IBM量子机器上也能表现出稳健的性能。", "conclusion": "研究结果表明，随着量子硬件的进步，量子图像生成具有广阔的前景。", "translation": "机器学习辅助诊断在皮肤病检测中越来越受到关注，但训练有效的模型需要大量高质量数据。皮肤病数据集常面临类别不平衡、隐私问题和对象偏差，使得数据增强变得至关重要。虽然经典生成模型被广泛使用，但它们需要大量的计算资源和漫长的训练时间。量子计算提供了一个有前景的替代方案，但现有的基于量子的图像生成方法只能产生灰度低质量图像。通过一种新颖的经典-量子潜在空间融合技术，我们的工作克服了这一限制，并引入了第一个能够生成彩色医学图像的经典-量子生成对抗网络（GAN）。我们的模型在图像生成质量和作为数据增强时的分类性能提升方面均优于经典的深度卷积GAN和现有混合经典-量子GAN。此外，性能提升与使用最先进的经典生成模型所达到的效果相当，但参数量减少了25倍以上，训练周期减少了10倍。这些结果预示着随着量子硬件的进步，量子图像生成将拥有光明的前景。最后，我们展示了我们的模型在真实IBM量子机器上带硬件噪声的稳健性能。", "summary": "本文提出了HybridQ，这是一种新颖的混合经典-量子生成对抗网络，利用经典-量子潜在空间融合技术生成彩色医学图像，以解决皮肤病数据集的数据稀缺问题。该模型在图像生成质量和分类性能提升方面超越了传统和现有混合GAN，且显著减少了参数和训练时间，展现了量子图像生成在医学领域的巨大潜力，并在真实量子硬件上验证了其鲁棒性。", "keywords": "生成对抗网络, 量子计算, 皮肤病图像, 数据增强, 混合模型", "comments": "本文的创新点在于首次将经典与量子计算有效融合，通过独特的潜在空间融合技术实现了彩色医学图像的生成，克服了现有量子图像生成方法的局限性。其重要性在于为医学图像数据增强提供了一种高效且资源需求更低的替代方案，尤其是在量子硬件仍在发展初期的背景下，其性能表现令人鼓舞。该研究为量子图像生成领域的发展开辟了新的方向。"}}
{"id": "2506.21071", "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph", "authors": ["Jingwei Wang", "Zai Zhang", "Hao Qian", "Chunjing Gan", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "Bin Shi", "Bo Dong"], "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs.", "comment": "20 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.21071v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21071v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用知识图谱的高质量指令数据增强大型语言模型工具使用能力", "tldr": "本文提出了一种利用知识图谱生成高质量指令数据的方法，以显著提升大型语言模型（LLMs）的工具使用能力。", "motivation": "教导大型语言模型（LLMs）使用工具对于提升其解决问题的能力和扩展应用至关重要。然而，有效使用工具具有挑战性，因为它需要深入理解工具功能和用户意图。以往的方法主要依赖LLMs生成指令数据，但这些数据的质量往往不足。", "method": "本文提出了一种利用知识图谱生成高质量指令数据的新方法。首先，从知识图谱中提取各种查询路径，并将其转化为广泛的用户查询。然后，将实体之间的关系转化为可操作的工具，并将每个查询的路径解析为详细的解决方案步骤，从而创建高质量的指令数据。", "result": "实验表明，仅用一小部分这种合成数据进行微调，就能显著提高LLMs的工具利用率和整体能力。", "conclusion": "通过利用知识图谱生成高质量指令数据，可以有效提升大型语言模型的工具使用和整体能力。", "translation": "教导大型语言模型（LLMs）使用工具对于提升其解决问题的能力和扩展应用至关重要。然而，有效使用工具具有挑战性，因为它需要深入理解工具功能和用户意图。以往的方法主要依赖LLMs生成指令数据，但这些数据的质量往往不足。在本文中，我们提出了一种利用知识图谱生成高质量指令数据的新方法。知识图谱是人工整理的、富含语义信息的数据集。我们首先从给定的知识图谱中提取各种查询路径，这些路径被转化为广泛的用户查询。然后，我们将实体之间的关系转化为可操作的工具，并将每个查询的路径解析为详细的解决方案步骤，从而创建高质量的指令数据。我们的实验表明，仅用一小部分这种合成数据进行微调，就能显著提高LLMs的工具利用率和整体能力。", "summary": "本文提出了一种新颖的方法，通过利用知识图谱生成高质量的指令数据来增强大型语言模型（LLMs）的工具使用能力。该方法通过从知识图谱中提取查询路径并将其转化为用户查询，再将实体关系转化为工具，并将查询路径解析为详细的解决方案步骤，从而创建出高质量的训练数据。实验证明，即使使用少量这种合成数据进行微调，也能显著提升LLMs的工具利用率和整体性能。", "keywords": "大型语言模型, 工具使用, 指令数据, 知识图谱, 微调", "comments": "该论文的创新点在于利用知识图谱这一高质量、人工整理的语义信息源来生成LLM的工具使用指令数据，而非依赖LLM自身生成，从而解决了传统方法数据质量不足的问题。其重要性在于为提升LLM的实际应用能力提供了一条有效途径，特别是在需要精确理解工具功能和用户意图的场景下。"}}
{"id": "2506.21017", "title": "Multimodal Prompt Alignment for Facial Expression Recognition", "authors": ["Fuyan Ma", "Yiran He", "Bin Sun", "Shutao Li"], "summary": "Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs) like CLIP for various downstream tasks. Despite their success,\ncurrent VLM-based facial expression recognition (FER) methods struggle to\ncapture fine-grained textual-visual relationships, which are essential for\ndistinguishing subtle differences between facial expressions. To address this\nchallenge, we propose a multimodal prompt alignment framework for FER, called\nMPA-FER, that provides fine-grained semantic guidance to the learning process\nof prompted visual features, resulting in more precise and interpretable\nrepresentations. Specifically, we introduce a multi-granularity hard prompt\ngeneration strategy that utilizes a large language model (LLM) like ChatGPT to\ngenerate detailed descriptions for each facial expression. The LLM-based\nexternal knowledge is injected into the soft prompts by minimizing the feature\ndiscrepancy between the soft prompts and the hard prompts. To preserve the\ngeneralization abilities of the pretrained CLIP model, our approach\nincorporates prototype-guided visual feature alignment, ensuring that the\nprompted visual features from the frozen image encoder align closely with\nclass-specific prototypes. Additionally, we propose a cross-modal global-local\nalignment module that focuses on expression-relevant facial features, further\nimproving the alignment between textual and visual features. Extensive\nexperiments demonstrate our framework outperforms state-of-the-art methods on\nthree FER benchmark datasets, while retaining the benefits of the pretrained\nmodel and minimizing computational costs.", "comment": "To appear in ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.21017v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21017v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多模态提示对齐用于面部表情识别", "tldr": "提出MPA-FER框架，通过多粒度硬提示生成、LLM知识注入、原型引导视觉特征对齐和跨模态全局-局部对齐，提升VLM在面部表情识别中的性能，超越SOTA方法。", "motivation": "当前基于VLM的面部表情识别（FER）方法难以捕捉细粒度的文本-视觉关系，这对于区分面部表情的细微差异至关重要。", "method": "提出MPA-FER多模态提示对齐框架，为提示视觉特征学习提供细粒度语义指导。具体包括：1. 多粒度硬提示生成策略，利用LLM（如ChatGPT）为每个面部表情生成详细描述。2. 通过最小化软提示与硬提示之间的特征差异，将LLM外部知识注入软提示。3. 引入原型引导的视觉特征对齐，确保提示视觉特征与类别特定原型对齐，以保留预训练CLIP模型的泛化能力。4. 提出跨模态全局-局部对齐模块，关注表情相关的面部特征，进一步改善文本和视觉特征的对齐。", "result": "该框架在三个FER基准数据集上优于最先进的方法，同时保留了预训练模型的优势并最小化了计算成本。", "conclusion": "MPA-FER通过多模态提示对齐，有效解决了VLM在面部表情识别中难以捕捉细粒度文本-视觉关系的问题，实现了更精确和可解释的表示，并在性能上超越了现有SOTA方法。", "translation": "提示学习已被广泛采用，以有效地将CLIP等视觉-语言模型（VLMs）应用于各种下游任务。尽管取得了成功，但当前基于VLM的面部表情识别（FER）方法难以捕捉细粒度的文本-视觉关系，这对于区分面部表情的细微差异至关重要。为了解决这一挑战，我们提出了一种用于FER的多模态提示对齐框架，称为MPA-FER，它为提示视觉特征的学习过程提供了细粒度的语义指导，从而产生更精确和可解释的表示。具体来说，我们引入了一种多粒度硬提示生成策略，该策略利用大型语言模型（LLM）如ChatGPT为每个面部表情生成详细描述。通过最小化软提示与硬提示之间的特征差异，将基于LLM的外部知识注入软提示中。为了保留预训练CLIP模型的泛化能力，我们的方法结合了原型引导的视觉特征对齐，确保来自冻结图像编码器的提示视觉特征与类别特定原型紧密对齐。此外，我们提出了一种跨模态全局-局部对齐模块，该模块专注于表情相关的面部特征，进一步改善了文本和视觉特征之间的对齐。广泛的实验表明，我们的框架在三个FER基准数据集上优于最先进的方法，同时保留了预训练模型的优势并最小化了计算成本。", "summary": "本文提出MPA-FER，一个用于面部表情识别的多模态提示对齐框架。它通过LLM生成细粒度硬提示来指导软提示学习，并结合原型引导的视觉特征对齐和跨模态全局-局部对齐模块，以增强VLM捕获细微表情差异的能力。实验证明，MPA-FER在多个基准数据集上超越现有SOTA方法，同时保持高效性。", "keywords": "面部表情识别, 提示学习, 多模态对齐, 视觉-语言模型, 大型语言模型", "comments": "该论文创新性地将LLM生成的细粒度语义信息融入VLM的提示学习中，并通过多重对齐机制（硬提示-软提示、视觉特征-原型、全局-局部跨模态）解决了现有VLM在FER中难以捕捉细微表情差异的问题。其方法不仅提升了性能，还保持了预训练模型的泛化能力和计算效率，为VLM在细粒度识别任务中的应用提供了新的思路。"}}
{"id": "2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21545v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21545v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "语言模型训练中的数据效能", "tldr": "本文提出了数据效能的概念，并引入了DELT框架，通过优化训练数据组织来提升语言模型性能，特别是通过LQS和FO方法。", "motivation": "现有研究主要关注数据效率（选择最小/最优数据子集），而数据效能（优化训练数据组织）相对未被充分探索，但对提升语言模型性能同样重要。", "method": "提出了一个通用的数据效能范式DELT，包含数据评分、数据选择和数据排序三个组件。其中，设计了学习质量评分（LQS）作为数据评分的新实例，从梯度一致性角度考虑数据样本的学习能力和质量；设计了折叠排序（FO）作为数据排序的新实例，解决模型遗忘和数据分布偏差问题。", "result": "1. DELT的各种实例在不增加数据规模和模型大小的情况下，不同程度地提升了语言模型性能。\n2. LQS与FO的组合实现了最显著的性能提升。\n3. 数据效能可以与数据效率（通过数据选择）结合实现。", "conclusion": "数据效能是语言模型训练中一个有前景的基础领域。", "translation": "数据是语言模型（LM）训练的基础。最近的研究致力于数据效率，旨在通过选择最小或最优的训练数据子集来最大化性能。数据过滤、采样和选择等技术在此领域发挥着关键作用。作为补充，我们定义了数据效能，它侧重于通过优化训练数据的组织来最大化性能，而这方面相对未被充分探索。这项工作引入了一个通用范式DELT，用于在LM训练中考虑数据效能，它强调了训练数据组织的重要性。DELT包括三个组件：数据评分、数据选择和数据排序。在这些组件中，我们设计了学习能力-质量评分（LQS）作为数据评分的一个新实例，它从梯度一致性的角度考虑了每个数据样本的学习能力和质量。我们还设计了折叠排序（FO）作为数据排序的一个新实例，它解决了模型遗忘和数据分布偏差等问题。全面的实验验证了LM训练中的数据效能，结果表明：首先，所提出的DELT的各种实例在不增加数据规模和模型大小的情况下，不同程度地提升了LM性能。其次，在这些实例中，我们提出的用于数据评分的LQS和用于数据排序的折叠排序的组合实现了最显著的改进。最后，数据效能可以通过应用数据选择与数据效率一起实现。因此，我们认为数据效能是LM训练中一个有前景的基础领域。", "summary": "本文提出了“数据效能”的概念，旨在通过优化训练数据组织来提升语言模型性能。为此，作者引入了DELT通用范式，包含数据评分、数据选择和数据排序。特别地，他们设计了学习质量评分（LQS）和折叠排序（FO）两种新方法。实验证明，DELT及其组件能在不增加模型大小和数据规模的情况下有效提升LM性能，尤其是LQS与FO的结合效果最佳，并可与数据效率方法结合使用。", "keywords": "数据效能, 语言模型, DELT, 数据组织, LQS, 折叠排序", "comments": "本文创新性地提出了“数据效能”的概念，将研究焦点从数据“量”的筛选转移到数据“组织”的优化，为语言模型训练提供了新的视角。DELT框架及其LQS和FO组件为实现数据效能提供了具体方法，尤其是在不增加计算资源的情况下提升模型性能，具有重要的实践意义。"}}
{"id": "2506.21018", "title": "LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection", "authors": ["Lei Hao", "Lina Xu", "Chang Liu", "Yanni Dong"], "summary": "Effective deep feature extraction via feature-level fusion is crucial for\nmultimodal object detection. However, previous studies often involve complex\ntraining processes that integrate modality-specific features by stacking\nmultiple feature-level fusion units, leading to significant computational\noverhead. To address this issue, we propose a new fusion detection baseline\nthat uses a single feature-level fusion unit to enable high-performance\ndetection, thereby simplifying the training process. Based on this approach, we\npropose a lightweight attention-guided self-modulation feature fusion network\n(LASFNet), which introduces a novel attention-guided self-modulation feature\nfusion (ASFF) module that adaptively adjusts the responses of fusion features\nat both global and local levels based on attention information from different\nmodalities, thereby promoting comprehensive and enriched feature generation.\nAdditionally, a lightweight feature attention transformation module (FATM) is\ndesigned at the neck of LASFNet to enhance the focus on fused features and\nminimize information loss. Extensive experiments on three representative\ndatasets demonstrate that, compared to state-of-the-art methods, our approach\nachieves a favorable efficiency-accuracy trade-off, reducing the number of\nparameters and computational cost by as much as 90% and 85%, respectively,\nwhile improving detection accuracy (mAP) by 1%-3%. The code will be\nopen-sourced at https://github.com/leileilei2000/LASFNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21018v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21018v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LASFNet：一种用于多模态目标检测的轻量级注意力引导自调制特征融合网络", "tldr": "LASFNet提出了一种轻量级注意力引导自调制特征融合网络，通过单一融合单元显著减少计算开销，同时提高多模态目标检测的精度。", "motivation": "当前多模态目标检测中，通过特征级融合进行深度特征提取至关重要，但现有方法常涉及复杂的训练过程，通过堆叠多个特征级融合单元集成模态特定特征，导致显著的计算开销。", "method": "本研究提出了一种新的融合检测基线，其使用单一特征级融合单元实现高性能检测，从而简化训练过程。在此基础上，提出了轻量级注意力引导自调制特征融合网络（LASFNet），引入了新颖的注意力引导自调制特征融合（ASFF）模块，该模块根据来自不同模态的注意力信息，在全局和局部层面自适应调整融合特征的响应。此外，在LASFNet的颈部设计了一个轻量级特征注意力转换模块（FATM），以增强对融合特征的关注并最小化信息损失。", "result": "在三个代表性数据集上的广泛实验表明，与最先进的方法相比，LASFNet实现了有利的效率-精度权衡，参数数量和计算成本分别减少了90%和85%，同时检测精度（mAP）提高了1%-3%。", "conclusion": "LASFNet通过其轻量级设计和创新的融合策略，显著提升了多模态目标检测的效率和精度，为该领域提供了一个高性能且计算友好的解决方案。", "translation": "通过特征级融合进行有效的深度特征提取对于多模态目标检测至关重要。然而，以往的研究通常涉及复杂的训练过程，通过堆叠多个特征级融合单元来整合模态特定特征，导致显著的计算开销。为了解决这个问题，我们提出了一种新的融合检测基线，其使用单一特征级融合单元来实现高性能检测，从而简化训练过程。在此基础上，我们提出了一种轻量级注意力引导自调制特征融合网络（LASFNet），它引入了一种新颖的注意力引导自调制特征融合（ASFF）模块，该模块根据来自不同模态的注意力信息，在全局和局部层面自适应调整融合特征的响应，从而促进全面和丰富的特征生成。此外，在LASFNet的颈部设计了一个轻量级特征注意力转换模块（FATM），以增强对融合特征的关注并最小化信息损失。在三个代表性数据集上的广泛实验表明，与最先进的方法相比，我们的方法实现了有利的效率-精度权衡，参数数量和计算成本分别减少了90%和85%，同时检测精度（mAP）提高了1%-3%。代码将开源于https://github.com/leileilei2000/LASFNet。", "summary": "本论文提出LASFNet，一种轻量级注意力引导自调制特征融合网络，旨在解决多模态目标检测中现有方法计算开销大的问题。LASFNet通过一个单一的特征级融合单元，结合注意力引导自调制特征融合（ASFF）模块和轻量级特征注意力转换模块（FATM），显著简化了训练过程，并提高了特征融合的效率和质量。实验结果表明，该方法在保持甚至提升检测精度的同时，大幅减少了参数量和计算成本，实现了卓越的效率-精度平衡。", "keywords": "多模态目标检测, 特征融合, 轻量级网络, 注意力机制, 自调制", "comments": "LASFNet的创新之处在于提出了单一特征级融合单元的基线，以及引入了ASFF模块和FATM模块，显著降低了多模态目标检测的计算复杂性，同时保持或提高了检测精度。其轻量化设计对于资源受限的应用场景具有重要意义，展现了在效率和性能之间取得良好平衡的潜力。"}}
{"id": "2506.21095", "title": "FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation", "authors": ["Xenia Heilmann", "Luca Corbucci", "Mattia Cerrato", "Anna Monreale"], "summary": "Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing clients' private data. However, fairness remains a key\nconcern, as biases in local clients' datasets can impact the entire federated\nsystem. Heterogeneous data distributions across clients may lead to models that\nare fairer for some clients than others. Although several fairness-enhancing\nsolutions are present in the literature, most focus on mitigating bias for a\nsingle sensitive attribute, typically binary, overlooking the diverse and\nsometimes conflicting fairness needs of different clients. This limited\nperspective can limit the effectiveness of fairness interventions for the\ndifferent clients. To support more robust and reproducible fairness research in\nFL, we aim to enable a consistent benchmarking of fairness-aware FL methods at\nboth the global and client levels. In this paper, we contribute in three ways:\n(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to\nevaluating fair FL methods under heterogeneous client bias; (2) we release four\nbias-heterogeneous datasets and corresponding benchmarks to compare fairness\nmitigation methods in a controlled environment; (3) we provide ready-to-use\nfunctions for evaluating fairness outcomes for these datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21095v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21095v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FeDa4Fair：用于公平性评估的客户端级联邦数据集", "tldr": "FeDa4Fair是一个库，用于生成表格数据集，以评估异构客户端偏差下的联邦学习公平性方法，并发布了四个偏置异构数据集和相应的基准。", "motivation": "联邦学习中的公平性是一个关键问题，因为本地客户端数据集中的偏差会影响整个联邦系统。现有公平性增强解决方案大多只关注单一敏感属性的偏差缓解，忽略了不同客户端多样且有时冲突的公平性需求，这限制了干预措施的有效性。为了支持更稳健和可复现的联邦学习公平性研究，需要一致地基准测试全局和客户端级别的公平性感知联邦学习方法。", "method": "该论文通过以下三方面做出贡献：1) 引入FeDa4Fair库，用于生成表格数据集，以评估异构客户端偏差下的公平联邦学习方法；2) 发布了四个偏置异构数据集和相应的基准，用于在受控环境中比较公平性缓解方法；3) 提供了用于评估这些数据集公平性结果的即用型函数。", "result": "该研究发布了四个偏置异构数据集和相应的基准，以在受控环境中比较公平性缓解方法，并提供了用于评估公平性结果的即用型函数。", "conclusion": "该论文通过提供FeDa4Fair库、偏置异构数据集和评估函数，旨在支持联邦学习中更稳健和可复现的公平性研究，并实现公平性感知联邦学习方法在全球和客户端级别的一致基准测试。", "translation": "联邦学习（FL）能够在不共享客户端私有数据的情况下，实现多个客户端之间的协作模型训练。然而，公平性仍然是一个关键问题，因为本地客户端数据集中的偏差可能会影响整个联邦系统。客户端之间异构的数据分布可能导致模型对某些客户端比对其他客户端更公平。尽管文献中存在几种增强公平性的解决方案，但大多数都集中于缓解单一敏感属性（通常是二元）的偏差，而忽略了不同客户端多样化且有时相互冲突的公平性需求。这种有限的视角可能会限制公平性干预措施对不同客户端的有效性。为了支持联邦学习中更稳健和可重现的公平性研究，我们的目标是在全局和客户端级别实现对公平性感知FL方法的一致基准测试。在本文中，我们以三种方式做出贡献：(1) 我们引入了FeDa4Fair，一个用于生成表格数据集的库，专门用于评估异构客户端偏差下的公平FL方法；(2) 我们发布了四个偏置异构数据集和相应的基准，用于在受控环境中比较公平性缓解方法；(3) 我们提供了用于评估这些数据集公平性结果的即用型函数。", "summary": "该论文提出了FeDa4Fair，一个用于评估联邦学习（FL）中公平性的库，旨在解决现有公平性研究未能充分考虑客户端间异构偏差的问题。FeDa4Fair通过生成专门的表格数据集来模拟异构客户端偏差，并发布了四个偏置异构数据集及其基准，以支持在受控环境中对公平性缓解方法进行鲁棒且可复现的评估。此外，该工作还提供了用于评估这些数据集公平性结果的即用型函数。", "keywords": "联邦学习, 公平性评估, 数据集, 客户端偏差, FeDa4Fair", "comments": "该论文通过引入FeDa4Fair库和相关数据集，为联邦学习中的公平性研究提供了一个重要工具和基准。其创新之处在于关注客户端级别的异构偏差，这在现有研究中常被忽视。通过提供受控环境和即用型函数，该工作有助于提升联邦学习公平性研究的严谨性和可复现性。"}}
{"id": "2506.21022", "title": "Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation", "authors": ["Ze Wang", "Hao Chen", "Benran Hu", "Jiang Liu", "Ximeng Sun", "Jialian Wu", "Yusheng Su", "Xiaodong Yu", "Emad Barsoum", "Zicheng Liu"], "summary": "Image tokenization plays a critical role in reducing the computational\ndemands of modeling high-resolution images, significantly improving the\nefficiency of image and multimodal understanding and generation. Recent\nadvances in 1D latent spaces have reduced the number of tokens required by\neliminating the need for a 2D grid structure. In this paper, we further advance\ncompact discrete image representation by introducing 1D binary image latents.\nBy representing each image as a sequence of binary vectors, rather than using\ntraditional one-hot codebook tokens, our approach preserves high-resolution\ndetails while maintaining the compactness of 1D latents. To the best of our\nknowledge, our text-to-image models are the first to achieve competitive\nperformance in both diffusion and auto-regressive generation using just 128\ndiscrete tokens for images up to 1024x1024, demonstrating up to a 32-fold\nreduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary\nlatent space, coupled with simple model architectures, achieves marked\nimprovements in speed training and inference speed. Our text-to-image models\nallow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X\nGPUs, and the training can be completed within 200 GPU days. Our models achieve\ncompetitive performance compared to modern image generation models without any\nin-house private training data or post-training refinements, offering a\nscalable and efficient alternative to conventional tokenization methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21022v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21022v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Instella-T2I：突破一维离散潜在空间图像生成的极限", "tldr": "Instella-T2I引入一维二值图像潜在表示，显著减少了文本到图像生成所需的离散token数量（低至128个），同时保持高分辨率细节和竞争力，大幅提升了训练和推理效率。", "motivation": "图像token化对于降低高分辨率图像建模的计算需求至关重要，能显著提高图像和多模态理解及生成的效率。近期一维潜在空间通过消除二维网格结构减少了所需token数量，但仍有提升空间，尤其是在保持高分辨率细节和进一步压缩token数量方面。", "method": "本论文通过引入一维二值图像潜在表示，进一步推进了紧凑离散图像表示。该方法将每张图像表示为一系列二值向量，而非传统的独热码本token。这种方法在保持一维潜在空间紧凑性的同时，保留了高分辨率细节。", "result": "Instella-T2I的文本到图像模型是首个在使用128个离散token（适用于高达1024x1024的图像）的情况下，在扩散和自回归生成方面均达到竞争性性能的模型。与标准VQ-VAE相比，token数量减少了高达32倍。所提出的一维二值潜在空间与简单的模型架构相结合，显著提高了训练和推理速度。模型允许在单个GPU节点（配备8个AMD MI300X GPU）上实现4096的全局批处理大小，训练可在200个GPU日内完成。模型在没有内部私有训练数据或后期训练优化的情况下，实现了与现代图像生成模型相当的性能。", "conclusion": "Instella-T2I提供了一种可扩展且高效的替代传统token化方法，通过引入一维二值图像潜在表示，极大地减少了文本到图像生成所需的token数量，同时保持了高分辨率细节和竞争性性能，并在训练和推理速度上取得了显著提升。", "translation": "图像token化在降低高分辨率图像建模的计算需求方面起着关键作用，显著提高了图像和多模态理解与生成的效率。一维潜在空间的最新进展通过消除对二维网格结构的需求，减少了所需的token数量。在本文中，我们通过引入一维二值图像潜在表示，进一步推动了紧凑离散图像表示。通过将每张图像表示为一系列二值向量，而不是使用传统的独热码本token，我们的方法在保持一维潜在空间紧凑性的同时保留了高分辨率细节。据我们所知，我们的文本到图像模型是第一个仅使用128个离散token（适用于高达1024x1024的图像）就能在扩散和自回归生成方面实现竞争性性能的模型，与标准VQ-VAE相比，token数量减少了高达32倍。所提出的一维二值潜在空间，结合简单的模型架构，显著提高了训练和推理速度。我们的文本到图像模型允许在单个配备8个AMD MI300X GPU的GPU节点上实现4096的全局批处理大小，并且训练可以在200个GPU日内完成。我们的模型在没有任何内部私有训练数据或后期训练优化的情况下，实现了与现代图像生成模型相当的性能，为传统token化方法提供了一种可扩展且高效的替代方案。", "summary": "Instella-T2I提出了一种新颖的一维二值图像潜在表示方法，旨在大幅减少文本到图像生成所需的离散token数量，同时保持高分辨率细节。该方法将图像编码为二值向量序列，而非传统的独热码本token。实验表明，Instella-T2I模型仅用128个token即可在1024x1024图像上实现与现有扩散和自回归模型相当的性能，相较于标准VQ-VAE，token数量减少了32倍。此外，该模型显著提升了训练和推理速度，并在不依赖私有数据或后期优化的前提下展现出强大竞争力，提供了一种高效且可扩展的图像token化替代方案。", "keywords": "图像生成, 离散潜在空间, 一维二值表示, Token化, 文本到图像", "comments": "Instella-T2I的创新点在于引入了一维二值图像潜在空间，这是一种非常激进且有效的token化方法。它成功地将高分辨率图像的表示压缩到极低的128个离散token，这在计算效率上是一个巨大的飞跃，尤其是在训练和推理速度方面。其无需私有数据和后期优化即可达到竞争性性能的特点，表明了其方法的鲁棒性和普适性。这对于推动大规模图像生成模型的发展具有重要意义，降低了资源需求，使其更易于部署和研究。"}}
{"id": "2506.21102", "title": "Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning", "authors": ["David Debot", "Pietro Barbiero", "Gabriele Dominici", "Giuseppe Marra"], "summary": "Concept-Based Models (CBMs) are a class of deep learning models that provide\ninterpretability by explaining predictions through high-level concepts. These\nmodels first predict concepts and then use them to perform a downstream task.\nHowever, current CBMs offer interpretability only for the final task\nprediction, while the concept predictions themselves are typically made via\nblack-box neural networks. To address this limitation, we propose Hierarchical\nConcept Memory Reasoner (H-CMR), a new CBM that provides interpretability for\nboth concept and task predictions. H-CMR models relationships between concepts\nusing a learned directed acyclic graph, where edges represent logic rules that\ndefine concepts in terms of other concepts. During inference, H-CMR employs a\nneural attention mechanism to select a subset of these rules, which are then\napplied hierarchically to predict all concepts and the final task. Experimental\nresults demonstrate that H-CMR matches state-of-the-art performance while\nenabling strong human interaction through concept and model interventions. The\nformer can significantly improve accuracy at inference time, while the latter\ncan enhance data efficiency during training when background knowledge is\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21102v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21102v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过注意力引导图学习的可解释分层概念推理", "tldr": "本文提出H-CMR，一种新的概念基础模型，通过学习有向无环图和注意力机制，为概念和任务预测提供可解释性，并达到SOTA性能，支持人机交互。", "motivation": "现有概念基础模型（CBMs）仅对最终任务预测提供可解释性，而概念本身的预测通常由黑盒神经网络完成，缺乏透明度。", "method": "本文提出分层概念记忆推理器（H-CMR），这是一种新的CBM。H-CMR通过学习一个有向无环图来建模概念之间的关系，其中边表示定义概念的逻辑规则。在推理过程中，H-CMR利用神经网络注意力机制选择这些规则的一个子集，然后分层应用这些规则来预测所有概念和最终任务。", "result": "实验结果表明，H-CMR在性能上与最先进的模型相当。它还通过概念和模型干预实现了强大的人机交互，前者可以显著提高推理时的准确性，后者可以在背景知识可用时提高训练期间的数据效率。", "conclusion": "H-CMR通过提供对概念和任务预测的可解释性，并保持高性能，解决了现有CBMs的局限性，同时通过人机交互增强了模型的实用性和效率。", "translation": "概念基础模型（CBMs）是一类深度学习模型，通过高级概念解释预测来提供可解释性。这些模型首先预测概念，然后利用它们执行下游任务。然而，当前的CBMs仅为最终任务预测提供可解释性，而概念预测本身通常通过黑盒神经网络进行。为了解决这一局限性，我们提出了分层概念记忆推理器（H-CMR），这是一种新的CBM，它为概念和任务预测都提供了可解释性。H-CMR使用学习到的有向无环图来建模概念之间的关系，其中边表示定义其他概念的逻辑规则。在推理过程中，H-CMR采用神经注意力机制来选择这些规则的一个子集，然后分层应用这些规则来预测所有概念和最终任务。实验结果表明，H-CMR的性能与最先进的技术相匹配，同时通过概念和模型干预实现了强大的人机交互。前者可以在推理时显著提高准确性，而后者可以在背景知识可用时提高训练期间的数据效率。", "summary": "本文提出H-CMR，一种新型概念基础模型，旨在解决现有CBMs在概念预测层面缺乏可解释性的问题。H-CMR通过学习一个有向无环图来表示概念间的逻辑关系，并利用注意力机制选择相关规则进行分层推理，从而同时为概念和最终任务预测提供可解释性。实验证明H-CMR性能与SOTA相当，并能通过人机交互（概念/模型干预）提升推理准确性和训练数据效率。", "keywords": "概念基础模型, 可解释性, 分层推理, 注意力机制, 图学习", "comments": "H-CMR的创新点在于将可解释性扩展到概念预测层面，而非仅限于最终任务预测。通过引入注意力引导的图学习来建模概念间的层次关系，它提供了一种透明的推理路径。其支持人机交互的特性，尤其是在推理时通过概念干预提升准确性和训练时通过模型干预提升数据效率的潜力，使其在实际应用中具有重要价值。"}}
{"id": "2506.21034", "title": "DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation", "authors": ["Wenzhou Lyu", "Jialing Lin", "Wenqi Ren", "Ruihao Xia", "Feng Qian", "Yang Tang"], "summary": "Commercial RGB-D cameras often produce noisy, incomplete depth maps for\nnon-Lambertian objects. Traditional depth completion methods struggle to\ngeneralize due to the limited diversity and scale of training data. Recent\nadvances exploit visual priors from pre-trained text-to-image diffusion models\nto enhance generalization in dense prediction tasks. However, we find that\nbiases arising from training-inference mismatches in the vanilla diffusion\nframework significantly impair depth completion performance. Additionally, the\nlack of distinct visual features in non-Lambertian regions further hinders\nprecise prediction. To address these issues, we propose \\textbf{DidSee}, a\ndiffusion-based framework for depth completion on non-Lambertian objects.\nFirst, we integrate a rescaled noise scheduler enforcing a zero terminal\nsignal-to-noise ratio to eliminate signal leakage bias. Second, we devise a\nnoise-agnostic single-step training formulation to alleviate error accumulation\ncaused by exposure bias and optimize the model with a task-specific loss.\nFinally, we incorporate a semantic enhancer that enables joint depth completion\nand semantic segmentation, distinguishing objects from backgrounds and yielding\nprecise, fine-grained depth maps. DidSee achieves state-of-the-art performance\non multiple benchmarks, demonstrates robust real-world generalization, and\neffectively improves downstream tasks such as category-level pose estimation\nand robotic grasping.Project page: https://wenzhoulyu.github.io/DidSee/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21034v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21034v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DidSee：基于扩散的深度补全用于材料无关的机器人感知与操作", "tldr": "DidSee是一个基于扩散的深度补全框架，专门用于解决非朗伯体物体的深度图噪声和不完整问题。它通过引入重新缩放的噪声调度器、单步训练和语义增强器来解决现有扩散模型的偏差和泛化问题，实现了最先进的性能，并提升了机器人感知和操作能力。", "motivation": "商用RGB-D相机为非朗伯体物体生成的深度图通常噪声大且不完整。传统深度补全方法因训练数据多样性和规模有限而难以泛化。虽然预训练扩散模型能增强泛化能力，但其训练-推理不匹配导致的偏差（如信号泄漏和曝光偏差）以及非朗伯区域缺乏独特视觉特征，显著损害了深度补全性能。", "method": "DidSee是一个基于扩散的非朗伯体物体深度补全框架。它主要包含：1. 集成一个重新缩放的噪声调度器，强制零终端信噪比以消除信号泄漏偏差。2. 设计一个与噪声无关的单步训练公式，以减轻由曝光偏差引起的误差累积，并使用任务特定损失进行优化。3. 结合一个语义增强器，实现联合深度补全和语义分割，以区分物体和背景，生成精确、细粒度的深度图。", "result": "DidSee在多个基准测试中取得了最先进的性能，展示了强大的真实世界泛化能力，并有效改进了下游任务，例如类别级姿态估计和机器人抓取。", "conclusion": "DidSee通过改进扩散模型，有效解决了非朗伯体物体深度补全的挑战，实现了最先进的性能，并显著提升了机器人感知和操作等下游任务的效率和准确性。", "translation": "商用RGB-D相机通常会为非朗伯体物体生成噪声大、不完整的深度图。传统深度补全方法由于训练数据的多样性和规模有限，难以泛化。最近的进展利用预训练文本到图像扩散模型中的视觉先验来增强密集预测任务的泛化能力。然而，我们发现香草扩散框架中由训练-推理不匹配引起的偏差显著损害了深度补全性能。此外，非朗伯区域缺乏独特的视觉特征进一步阻碍了精确预测。为了解决这些问题，我们提出了\\textbf{DidSee}，一个基于扩散的非朗伯体物体深度补全框架。首先，我们整合了一个重新缩放的噪声调度器，强制零终端信噪比以消除信号泄漏偏差。其次，我们设计了一个与噪声无关的单步训练公式，以减轻由曝光偏差引起的误差累积，并使用任务特定损失优化模型。最后，我们结合了一个语义增强器，实现联合深度补全和语义分割，区分物体和背景，并产生精确、细粒度的深度图。DidSee在多个基准测试中取得了最先进的性能，展示了强大的真实世界泛化能力，并有效改进了下游任务，例如类别级姿态估计和机器人抓取。项目页面：https://wenzhoulyu.github.io/DidSee/", "summary": "本论文提出DidSee，一个新颖的基于扩散的框架，旨在为非朗伯体物体提供鲁棒的深度补全。为了解决现有RGB-D相机深度图噪声大、传统方法泛化能力差以及香草扩散模型中训练-推理不匹配导致的偏差等问题，DidSee集成了重新缩放的噪声调度器、与噪声无关的单步训练公式和语义增强器。这种方法有效缓解了信号泄漏和曝光偏差，并实现了联合深度补全和语义分割。DidSee在多个基准测试中达到了最先进的性能，展现了强大的真实世界泛化能力，并显著提升了如姿态估计和抓取等下游机器人感知和操作任务。", "keywords": "深度补全, 扩散模型, 非朗伯体, 机器人感知, 语义分割", "comments": "DidSee创新性地应用并改进了扩散模型进行深度补全，特别解决了现实世界机器人应用中常见的非朗伯表面这一挑战性问题。其在解决扩散模型偏差（信号泄漏、曝光偏差）和整合语义信息方面的贡献具有重要意义。在下游机器人任务中表现出的改进突显了其实用价值。"}}
{"id": "2506.21103", "title": "Learning to Skip the Middle Layers of Transformers", "authors": ["Tim Lawson", "Laurence Aitchison"], "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle.", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21103v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21103v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "学习跳过Transformer的中间层", "tldr": "提出一种动态跳过Transformer中间层的新架构，但实验结果显示与基线模型相比，计算效率和性能权衡没有提升。", "motivation": "现有条件计算方法效率不高，且可解释性研究表明Transformer中间层冗余度更高，早期层聚合信息。因此，目标是减少“简单”token的计算需求并可能促进多级表示层次。", "method": "提出一种新颖的架构，动态地从中间向外跳过可变数量的层。一个学习到的门控机制根据输入决定是否绕过对称的中心块，一个门控注意力机制阻止后续token关注跳过的token位置。残差范数通过“三明治”或“perilayernorm”方案控制，门控稀疏性通过自适应正则化损失控制。", "result": "在所研究的规模下，与层数较少的密集基线模型相比，该方法在验证交叉熵和估计FLOPs之间的权衡方面没有实现改进。", "conclusion": "尽管提出了新颖的跳层策略，但在当前实验规模下，该方法未能提升Transformer的计算效率与性能的权衡。", "translation": "条件计算是提高Transformer效率的流行策略。现有方法通常针对单个模块（例如，专家混合层）或独立地跳过层。然而，可解释性研究表明Transformer的中间层表现出更大的冗余性，并且早期层将信息聚合到token位置。受这些见解的启发，我们提出了一种新颖的架构，该架构动态地从中间向外跳过可变数量的层。特别是，一个学习到的门控机制根据输入决定是否绕过对称的中心块，一个门控注意力机制阻止后续token关注跳过的token位置。残差范数通过“三明治”或“perilayernorm”方案控制，门控稀疏性通过自适应正则化损失控制。我们旨在减少“简单”token的计算需求并可能促进新兴的多级表示层次结构，但在所研究的规模下，与层数较少的密集基线模型相比，我们的方法在验证交叉熵和估计FLOPs之间的权衡方面没有实现改进。我们已在https://github.com/tim-lawson/skip-middle 发布了代码。", "summary": "这篇论文提出了一种新的Transformer架构，旨在通过动态跳过其中间层来提高计算效率。受中间层冗余和早期层信息聚合的启发，该方法利用学习到的门控机制决定跳过中心块，并使用门控注意力防止对跳过位置的关注。尽管目标是减少计算和促进层次化表示，但实验结果表明，在所测试的规模下，该方法并未在计算量和模型性能之间实现比密集基线模型更好的权衡。", "keywords": "Transformer, 条件计算, 层跳过, 模型效率, 门控机制", "comments": "该论文提出了一种创新的、基于可解释性研究的Transformer层跳过策略，即从中间向外跳层，这与以往独立跳层或针对特定模块的方法不同。然而，其主要局限在于实验结果未能证明在计算效率和性能上优于现有基线，这可能限制了其在实际应用中的直接价值，但也为未来研究提供了宝贵的负面结果和设计方向。"}}
{"id": "2506.21042", "title": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "summary": "Detectors often suffer from performance drop due to domain gap between\ntraining and testing data. Recent methods explore diffusion models applied to\ndomain generalization (DG) and adaptation (DA) tasks, but still struggle with\nlarge inference costs and have not yet fully leveraged the capabilities of\ndiffusion models. We propose to tackle these problems by extracting\nintermediate features from a single-step diffusion process, improving feature\ncollection and fusion to reduce inference time by 75% while enhancing\nperformance on source domains (i.e., Fitness). Then, we construct an\nobject-centered auxiliary branch by applying box-masked images with class\nprompts to extract robust and domain-invariant features that focus on object.\nWe also apply consistency loss to align the auxiliary and ordinary branch,\nbalancing fitness and generalization while preventing overfitting and improving\nperformance on target domains (i.e., Generalization). Furthermore, within a\nunified framework, standard detectors are guided by diffusion detectors through\nfeature-level and object-level alignment on source domains (for DG) and\nunlabeled target domains (for DA), thereby improving cross-domain detection\nperformance (i.e., Transferability). Our method achieves competitive results on\n3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO\ngeneralization benchmark demonstrate that our method maintains significant\nadvantages and show remarkable efficiency in large domain shifts and low-data\nscenarios. Our work shows the superiority of applying diffusion models to\ndomain generalized and adaptive detection tasks and offers valuable insights\nfor visual perception tasks across diverse domains. The code is available at\n\\href{https://github.com/heboyong/Fitness-Generalization-Transferability}{Fitness-Generalization-Transferability}.", "comment": "Accepted by ICCV2025. arXiv admin note: text overlap with\n  arXiv:2503.02101", "pdf_url": "http://arxiv.org/pdf/2506.21042v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21042v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用扩散模型提升域泛化和自适应检测：适应性、泛化性和可迁移性", "tldr": "本文提出了一种利用扩散模型的新方法，通过从单步扩散中提取中间特征、构建以对象为中心的辅助分支以及引导标准检测器，以提高域泛化和自适应检测的性能，同时显著降低推理成本。", "motivation": "现有的检测器在训练和测试数据之间存在域差距时性能会下降。最近将扩散模型应用于域泛化（DG）和域适应（DA）任务的方法，仍然存在推理成本高的问题，并且未能充分利用扩散模型的潜力。", "method": "本文通过以下方法解决问题：1) 从单步扩散过程中提取中间特征，改进特征收集和融合，从而将推理时间减少75%，并提高源域性能（适应性）。2) 通过对带有类别提示的框掩蔽图像应用以对象为中心的辅助分支，提取鲁棒和域不变的特征。同时应用一致性损失来对齐辅助分支和普通分支，平衡适应性和泛化性，防止过拟合，并提高目标域性能（泛化性）。3) 在统一框架内，通过在源域（针对DG）和未标记目标域（针对DA）上的特征级和对象级对齐，由扩散检测器引导标准检测器，从而提高跨域检测性能（可迁移性）。", "result": "本文方法在3个DA基准和5个DG基准上取得了有竞争力的结果。此外，在COCO泛化基准上的实验表明，该方法在大域偏移和低数据场景下保持显著优势，并显示出卓越的效率。", "conclusion": "本文工作展示了将扩散模型应用于域泛化和自适应检测任务的优越性，并为跨不同领域的视觉感知任务提供了宝贵的见解。", "translation": "检测器常常由于训练和测试数据之间的域差距而导致性能下降。最近的方法探索将扩散模型应用于域泛化（DG）和域适应（DA）任务，但仍然面临高昂的推理成本，并且尚未充分利用扩散模型的能力。我们提出通过从单步扩散过程中提取中间特征来解决这些问题，改进特征收集和融合，从而将推理时间减少75%，同时提高源域性能（即适应性）。然后，我们通过对带有类别提示的框掩蔽图像应用以对象为中心的辅助分支来提取鲁棒和域不变的聚焦于对象的特征。我们还应用一致性损失来对齐辅助分支和普通分支，平衡适应性和泛化性，同时防止过拟合并提高目标域性能（即泛化性）。此外，在一个统一的框架内，通过在源域（针对DG）和未标记目标域（针对DA）上的特征级和对象级对齐，标准检测器受到扩散检测器的引导，从而提高跨域检测性能（即可迁移性）。我们的方法在3个DA基准和5个DG基准上取得了有竞争力的结果。此外，在COCO泛化基准上的实验表明，我们的方法保持显著优势，并在大域偏移和低数据场景中显示出卓越的效率。我们的工作展示了将扩散模型应用于域泛化和自适应检测任务的优越性，并为跨不同领域的视觉感知任务提供了宝贵的见解。代码可在https://github.com/heboyong/Fitness-Generalization-Transferability 获取。", "summary": "本文提出了一种利用扩散模型的新颖方法，旨在解决域泛化（DG）和域适应（DA）任务中检测器的性能下降问题以及扩散模型推理成本高、能力未充分利用的挑战。该方法通过从单步扩散中提取中间特征以提高源域性能并大幅降低推理时间（适应性），构建以对象为中心的辅助分支和应用一致性损失来增强目标域的泛化能力（泛化性），并通过特征级和对象级对齐引导标准检测器以提升跨域检测性能（可迁移性）。实验证明，该方法在多个DG和DA基准上表现出色，尤其在大域偏移和低数据场景下展现出显著优势和高效率。", "keywords": "扩散模型, 域泛化, 域适应, 目标检测, 特征学习", "comments": "本文创新性地将扩散模型应用于域泛化和自适应检测，并有效解决了现有方法推理成本高的问题。通过引入“适应性”、“泛化性”和“可迁移性”三个核心概念，并设计相应的技术模块（单步扩散特征提取、对象中心辅助分支、特征/对象级对齐），实现了性能的全面提升。特别是在效率方面的提升（推理时间减少75%）和在大域偏移、低数据场景下的表现，使其具有重要的实际应用价值。该研究为扩散模型在视觉感知任务中的应用提供了新的视角和宝贵见解。"}}
{"id": "2506.21107", "title": "Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges", "authors": ["Changxi Chi", "Jun Xia", "Yufei Huang", "Jingbo Zhou", "Siyuan Li", "Yunfan Liu", "Chang Yu", "Stan Z. Li"], "summary": "Estimating single-cell responses across various perturbations facilitates the\nidentification of key genes and enhances drug screening, significantly boosting\nexperimental efficiency. However, single-cell sequencing is a destructive\nprocess, making it impossible to capture the same cell's phenotype before and\nafter perturbation. Consequently, data collected under perturbed and\nunperturbed conditions are inherently unpaired. Existing methods either attempt\nto forcibly pair unpaired data using random sampling, or neglect the inherent\nrelationship between unperturbed and perturbed cells during the modeling. In\nthis work, we propose a framework based on Dual Diffusion Implicit Bridges\n(DDIB) to learn the mapping between different data distributions, effectively\naddressing the challenge of unpaired data. We further interpret this framework\nas a form of data augmentation. We integrate gene regulatory network (GRN)\ninformation to propagate perturbation signals in a biologically meaningful way,\nand further incorporate a masking mechanism to predict silent genes, improving\nthe quality of generated profiles. Moreover, gene expression under the same\nperturbation often varies significantly across cells, frequently exhibiting a\nbimodal distribution that reflects intrinsic heterogeneity. To capture this, we\nintroduce a more suitable evaluation metric. We propose Unlasting, dual\nconditional diffusion models that overcome the problem of unpaired single-cell\nperturbation data and strengthen the model's insight into perturbations under\nthe guidance of the GRN, with a dedicated mask model designed to improve\ngeneration quality by predicting silent genes. In addition, we introduce a\nbiologically grounded evaluation metric that better reflects the inherent\nheterogeneity in single-cell responses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21107v1", "categories": ["cs.LG", "q-bio.MN"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21107v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Unlasting: 基于双条件扩散隐式桥接的无配对单细胞多扰动估计", "tldr": "Unlasting提出了一种基于双条件扩散隐式桥接的模型，用于解决无配对单细胞多扰动数据的估计问题，并通过整合基因调控网络和掩码机制来提高生成质量，并引入新的评估指标。", "motivation": "估计单细胞对各种扰动的反应对于识别关键基因和增强药物筛选至关重要，能显著提高实验效率。然而，单细胞测序是破坏性的，无法捕获同一细胞扰动前后的表型，导致数据固有的无配对性。现有方法要么强制配对，要么忽略细胞间的固有关系。", "method": "本文提出了Unlasting框架，基于双扩散隐式桥接（DDIB）来学习不同数据分布之间的映射，有效处理无配对数据。该框架被解释为一种数据增强形式。它整合了基因调控网络（GRN）信息以生物学上有意义的方式传播扰动信号，并结合掩码机制预测沉默基因以提高生成质量。此外，引入了一个更合适的评估指标来捕获细胞间在相同扰动下基因表达的固有异质性（双峰分布）。", "result": "Not mentioned in abstract", "conclusion": "Unlasting是一种双条件扩散模型，它克服了无配对单细胞扰动数据的问题，并在基因调控网络的指导下增强了模型对扰动的洞察力，通过专门设计的掩码模型预测沉默基因以提高生成质量。此外，它引入了一个生物学基础的评估指标，能更好地反映单细胞反应中固有的异质性。", "translation": "估计单细胞对各种扰动的反应有助于识别关键基因并增强药物筛选，显著提高实验效率。然而，单细胞测序是一个破坏性的过程，使得捕获同一细胞扰动前后的表型成为不可能。因此，在扰动和未扰动条件下收集的数据是固有的无配对的。现有方法要么尝试使用随机采样强制配对无配对数据，要么在建模过程中忽略未扰动和扰动细胞之间的固有关系。在这项工作中，我们提出了一个基于双扩散隐式桥接（DDIB）的框架来学习不同数据分布之间的映射，有效地解决了无配对数据的挑战。我们进一步将此框架解释为一种数据增强形式。我们整合了基因调控网络（GRN）信息，以生物学上有意义的方式传播扰动信号，并进一步结合掩码机制来预测沉默基因，提高生成谱的质量。此外，相同扰动下的基因表达在不同细胞之间通常差异显著，经常表现出反映内在异质性的双峰分布。为了捕捉这一点，我们引入了一个更合适的评估指标。我们提出了Unlasting，一种双条件扩散模型，它克服了无配对单细胞扰动数据的问题，并在GRN的指导下加强了模型对扰动的洞察力，其专门设计的掩码模型旨在通过预测沉默基因来提高生成质量。此外，我们引入了一个具有生物学基础的评估指标，可以更好地反映单细胞反应中固有的异质性。", "summary": "Unlasting是一种创新的双条件扩散模型，旨在解决单细胞多扰动数据固有的无配对问题。通过利用双扩散隐式桥接（DDIB），该模型能够学习不同数据分布之间的映射，并将其视为一种数据增强策略。为了提高估计的生物学相关性和生成质量，Unlasting整合了基因调控网络（GRN）信息以传播扰动信号，并引入了掩码机制来预测沉默基因。此外，为了准确捕捉单细胞反应的内在异质性，该研究还提出了一种新的、更合适的评估指标。", "keywords": "单细胞, 扰动, 扩散模型, 无配对数据, 基因调控网络", "comments": "Unlasting的创新之处在于其利用双条件扩散隐式桥接（DDIB）来处理单细胞扰动数据中普遍存在的无配对问题，这在生物学数据分析中是一个重大挑战。通过整合基因调控网络（GRN）信息，该模型能够以生物学上更合理的方式理解和传播扰动信号，增强了模型的生物学解释性。引入掩码机制预测沉默基因以及针对异质性设计新的评估指标，都体现了对单细胞数据复杂性的深入理解和细致处理。这项工作有望显著提升单细胞扰动分析的准确性和效率。"}}
{"id": "2506.21045", "title": "Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling", "authors": ["Hansam Cho", "Seoung Bum Kim"], "summary": "Text-guided diffusion models have become essential for high-quality image\nsynthesis, enabling dynamic image editing. In image editing, two crucial\naspects are editability, which determines the extent of modification, and\nfaithfulness, which reflects how well unaltered elements are preserved.\nHowever, achieving optimal results is challenging because of the inherent\ntrade-off between editability and faithfulness. To address this, we propose\nFaithfulness Guidance and Scheduling (FGS), which enhances faithfulness with\nminimal impact on editability. FGS incorporates faithfulness guidance to\nstrengthen the preservation of input image information and introduces a\nscheduling strategy to resolve misalignment between editability and\nfaithfulness. Experimental results demonstrate that FGS achieves superior\nfaithfulness while maintaining editability. Moreover, its compatibility with\nvarious editing methods enables precise, high-quality image edits across\ndiverse tasks.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2506.21045v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21045v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过引导和调度提高基于扩散的图像编辑保真度", "tldr": "本文提出FGS，一种新的方法，通过引入保真度引导和调度策略，在保持可编辑性的同时显著提高基于扩散的图像编辑的保真度。", "motivation": "在基于扩散的图像编辑中，可编辑性（修改程度）和保真度（未改变元素的保留程度）之间存在固有的权衡，导致难以获得最佳结果。本文旨在解决这一问题，在最小化对可编辑性影响的同时增强保真度。", "method": "本文提出了保真度引导和调度（FGS）方法。FGS结合了保真度引导来加强输入图像信息的保存，并引入了调度策略来解决可编辑性和保真度之间的错位问题。", "result": "实验结果表明，FGS在保持可编辑性的同时，实现了卓越的保真度。此外，FGS与各种编辑方法兼容，能够在不同任务中实现精确、高质量的图像编辑。", "conclusion": "FGS通过引入保真度引导和调度策略，有效解决了扩散模型图像编辑中保真度和可编辑性之间的权衡问题，显著提高了图像编辑的质量和精确性，并具有广泛的兼容性。", "translation": "文本引导的扩散模型已成为高质量图像合成的关键，实现了动态图像编辑。在图像编辑中，两个关键方面是可编辑性（决定修改程度）和保真度（反映未改变元素的保留程度）。然而，由于可编辑性和保真度之间固有的权衡，实现最佳结果具有挑战性。为了解决这个问题，我们提出了保真度引导和调度（FGS），它在最小化对可编辑性影响的同时增强了保真度。FGS结合了保真度引导以加强输入图像信息的保存，并引入了调度策略以解决可编辑性和保真度之间的错位。实验结果表明，FGS在保持可编辑性的同时实现了卓越的保真度。此外，它与各种编辑方法的兼容性使其能够在不同任务中实现精确、高质量的图像编辑。", "summary": "本文提出了一种名为FGS（Faithfulness Guidance and Scheduling）的新方法，旨在解决基于扩散的图像编辑中可编辑性和保真度之间的权衡问题。FGS通过引入保真度引导来增强原始图像信息的保留，并采用调度策略来协调可编辑性和保真度。实验证明，FGS在保持良好可编辑性的同时，显著提高了图像编辑的保真度，并且能够兼容多种编辑方法，实现高质量的图像编辑。", "keywords": "扩散模型, 图像编辑, 保真度, 引导, 调度", "comments": "该论文通过引入保真度引导和调度策略，巧妙地解决了扩散模型图像编辑中长期存在的保真度与可编辑性之间的矛盾，为提高图像编辑质量提供了一个有效且通用的解决方案。其创新性在于提出了一种平衡机制，而不是简单地牺牲其中一个方面。"}}
{"id": "2506.21127", "title": "Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "summary": "The increasing automation of navigation for unmanned aerial vehicles (UAVs)\nhas exposed them to adversarial attacks that exploit vulnerabilities in\nreinforcement learning (RL) through sensor manipulation. Although existing\nrobust RL methods aim to mitigate such threats, their effectiveness has limited\ngeneralization to out-of-distribution shifts from the optimal value\ndistribution, as they are primarily designed to handle fixed perturbation. To\naddress this limitation, this paper introduces an antifragile RL framework that\nenhances adaptability to broader distributional shifts by incorporating a\nswitching mechanism based on discounted Thompson sampling (DTS). This mechanism\ndynamically selects among multiple robust policies to minimize adversarially\ninduced state-action-value distribution shifts. The proposed approach first\nderives a diverse ensemble of action robust policies by accounting for a range\nof perturbations in the policy space. These policies are then modeled as a\nmultiarmed bandit (MAB) problem, where DTS optimally selects policies in\nresponse to nonstationary Bernoulli rewards, effectively adapting to evolving\nadversarial strategies. Theoretical framework has also been provided where by\noptimizing the DTS to minimize the overall regrets due to distributional shift,\nresults in effective adaptation against unseen adversarial attacks thus\ninducing antifragility. Extensive numerical simulations validate the\neffectiveness of the proposed framework in complex navigation environments with\nmultiple dynamic three-dimensional obstacles and with stronger projected\ngradient descent (PGD) and spoofing attacks. Compared to conventional robust,\nnon-adaptive RL methods, the antifragile approach achieves superior\nperformance, demonstrating shorter navigation path lengths and a higher rate of\nconflict-free navigation trajectories compared to existing robust RL techniques", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21127v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21127v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "对抗环境下无人机冲突避免的鲁棒策略切换抗脆弱强化学习", "tldr": "本文提出了一种基于折扣汤普森采样（DTS）的抗脆弱强化学习框架，通过动态切换多个鲁棒策略来应对无人机在对抗环境中面临的广义分布偏移和未见过的攻击，显著提高了导航性能。", "motivation": "现有的鲁棒强化学习方法在处理固定扰动方面有效，但对于来自最优价值分布的分布外偏移，其泛化能力有限，无法有效应对由传感器操纵引起的对抗性攻击对无人机导航自动化造成的漏洞。", "method": "本文引入了一个抗脆弱强化学习框架，通过结合基于折扣汤普森采样（DTS）的切换机制来增强对更广泛分布偏移的适应性。该机制动态地从多个鲁棒策略中选择，以最小化对抗引起的S-A-V分布偏移。首先，通过考虑策略空间中的一系列扰动，推导出一组多样化的动作鲁棒策略。然后，将这些策略建模为多臂赌博机（MAB）问题，其中DTS根据非平稳伯努利奖励进行最优策略选择，从而有效地适应不断演变的对抗策略。还提供了理论框架，通过优化DTS来最小化由于分布偏移引起的总遗憾，从而实现对未见过的对抗攻击的有效适应。", "result": "广泛的数值模拟验证了所提出的框架在具有多个动态三维障碍物和更强的PGD及欺骗攻击的复杂导航环境中的有效性。与传统的鲁棒、非自适应强化学习方法相比，该抗脆弱方法表现出卓越的性能，具有更短的导航路径长度和更高的无冲突导航轨迹率。", "conclusion": "本文提出的抗脆弱强化学习框架通过策略切换机制，能够有效应对无人机在对抗环境中面临的广义分布偏移和未见过的攻击，显著提高了其鲁棒性和导航性能。", "translation": "无人机（UAV）导航自动化程度的提高使其面临对抗性攻击，这些攻击通过传感器操纵利用强化学习（RL）中的漏洞。尽管现有的鲁棒RL方法旨在减轻此类威胁，但它们的有效性在泛化到来自最优价值分布的分布外偏移时受到限制，因为它们主要设计用于处理固定扰动。为了解决这一限制，本文引入了一个抗脆弱RL框架，通过结合基于折扣汤普森采样（DTS）的切换机制，增强了对更广泛分布偏移的适应性。该机制动态地在多个鲁棒策略中选择，以最小化对抗性引起的S-A-V（状态-动作-价值）分布偏移。所提出的方法首先通过考虑策略空间中的一系列扰动，推导出一组多样化的动作鲁棒策略。然后，将这些策略建模为多臂赌博机（MAB）问题，其中DTS根据非平稳伯努利奖励进行最优策略选择，从而有效地适应不断演变的对抗策略。还提供了理论框架，通过优化DTS以最小化由于分布偏移引起的总遗憾，从而实现对未见过的对抗攻击的有效适应，进而引入抗脆弱性。广泛的数值模拟验证了所提出的框架在具有多个动态三维障碍物和更强的PGD（投影梯度下降）和欺骗攻击的复杂导航环境中的有效性。与传统的鲁棒、非自适应RL方法相比，该抗脆弱方法实现了卓越的性能，与现有鲁棒RL技术相比，展示了更短的导航路径长度和更高的无冲突导航轨迹率。", "summary": "本文针对无人机在对抗环境中因传感器操纵导致的强化学习漏洞，提出了一种抗脆弱强化学习框架。该框架引入基于折扣汤普森采样（DTS）的策略切换机制，动态选择多组鲁棒策略，以应对广义的分布偏移和未见过的对抗攻击。通过将策略选择建模为多臂赌博机问题并优化DTS，该方法显著提升了无人机在复杂导航环境中的鲁棒性和冲突避免能力，表现出比现有鲁棒RL方法更优的性能。", "keywords": "抗脆弱强化学习, 策略切换, 无人机冲突避免, 对抗环境, 折扣汤普森采样", "comments": "该论文的创新点在于将抗脆弱性概念引入强化学习，并利用折扣汤普森采样实现策略的动态切换，以应对非固定和不断演变的对抗性攻击。这种方法超越了传统鲁棒RL仅处理固定扰动的局限性，在无人机等关键应用领域具有重要意义。通过将策略选择建模为MAB问题，并提供理论保障，提升了方法的严谨性。其在复杂环境下的优越性能也验证了其有效性。"}}
{"id": "2506.21129", "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks", "authors": ["Deepak Kumar Panda", "Adolfo Perrusquia", "Weisi Guo"], "summary": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21129v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21129v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "课程引导的反脆弱强化学习，用于观测空间攻击下无人机安全避碰", "tldr": "本文提出了一种反脆弱强化学习框架，通过模拟攻击者逐步增加扰动，使RL智能体能够适应观测空间攻击，从而在安全关键系统中实现更安全、更有弹性的决策。", "motivation": "强化学习（RL）策略在安全关键系统（如无人机导航）中容易受到观测空间中的分布外（OOD）对抗性攻击。这些攻击会导致分布偏移，显著降低价值估计，从而导致不安全或次优的决策，使现有策略变得脆弱。", "method": "提出了一种反脆弱强化学习框架，旨在适应递增的对抗性扰动课程。该框架引入了一个模拟攻击者，逐步增加观测空间扰动的强度，使RL智能体能够适应并泛化到更广泛的OOD观测，并预测以前未见的攻击。理论上，将脆弱性定义为随着扰动强度增加，价值函数分布的单调发散（灾难性遗忘），并将反脆弱性定义为这种价值偏移的有界性，并推导出稳定遗忘的适应条件。通过使用Wasserstein距离最小化，在逐步扰动的观测上进行迭代的专家引导评论家对齐来强制执行这些界限。该方法在涉及动态3D障碍的无人机避碰场景中进行了实证评估。", "result": "反脆弱策略在经受投影梯度下降（PGD）和GPS欺骗攻击时，始终优于标准和鲁棒RL基线，实现了高达15%的累积奖励提升和超过30%的冲突事件减少。", "conclusion": "这些发现证明了反脆弱强化学习在具有不断演变威胁场景的环境中实现安全和弹性决策的实践和理论可行性。", "translation": "强化学习（RL）策略部署在安全关键系统，如动态空域中的无人机（UAV）导航，容易受到观测空间中的分布外（OOD）对抗性攻击。这些攻击会导致分布偏移，显著降低价值估计，从而导致不安全或次优的决策，使现有策略变得脆弱。为了解决这种脆弱性，我们提出了一种反脆弱RL框架，旨在适应递增的对抗性扰动课程。该框架引入了一个模拟攻击者，逐步增加观测空间扰动的强度，这使得RL智能体能够适应并泛化到更广泛的OOD观测，并预测以前未见的攻击。我们首先对脆弱性进行了理论表征，正式将灾难性遗忘定义为随着扰动强度增加，价值函数分布的单调发散。在此基础上，我们将反脆弱性定义为这种价值偏移的有界性，并推导出稳定遗忘的适应条件。我们的方法通过使用Wasserstein距离最小化，在逐步扰动的观测上进行迭代的专家引导评论家对齐来强制执行这些界限。我们在涉及动态3D障碍的无人机避碰场景中对该方法进行了实证评估。结果表明，在经受投影梯度下降（PGD）和GPS欺骗攻击时，反脆弱策略始终优于标准和鲁棒RL基线，实现了高达15%的累积奖励提升和超过30%的冲突事件减少。这些发现证明了反脆弱强化学习在具有不断演变威胁场景的环境中实现安全和弹性决策的实践和理论可行性。", "summary": "本文提出了一种课程引导的反脆弱强化学习框架，以解决安全关键系统中强化学习策略对观测空间对抗性攻击的脆弱性。该框架引入一个模拟攻击者，逐步增加扰动强度，使RL智能体能够适应并泛化到新的攻击。研究者对脆弱性进行了理论表征，将其定义为价值函数分布的灾难性遗忘，并定义了反脆弱性为价值偏移的有界性。该方法通过迭代专家引导的评论家对齐，利用Wasserstein距离最小化来强制执行这些界限。在无人机避碰场景中的实验结果表明，该反脆弱策略在面对PGD和GPS欺骗攻击时，显著优于现有基线，实现了更高的累积奖励和更少的冲突事件，从而证明了其在安全弹性决策中的实用性和理论可行性。", "keywords": "反脆弱强化学习, 无人机避碰, 对抗性攻击, 观测空间, 课程学习", "comments": "该论文的创新点在于提出了“反脆弱强化学习”的概念，旨在使RL策略不仅能抵抗攻击，还能在面对递增的对抗性扰动时变得更强。通过引入模拟攻击者和理论上对脆弱性与反脆弱性的定义，并结合Wasserstein距离最小化进行评论家对齐，提供了一种新颖且有效的方法来提升RL在安全关键系统中的鲁棒性和弹性。尤其是在无人机避碰场景中的出色表现，凸显了其在实际应用中的巨大潜力。"}}
{"id": "2506.21055", "title": "Class-Agnostic Region-of-Interest Matching in Document Images", "authors": ["Demin Zhang", "Jiahao Lyu", "Zhijie Shen", "Yu Zhou"], "summary": "Document understanding and analysis have received a lot of attention due to\ntheir widespread application. However, existing document analysis solutions,\nsuch as document layout analysis and key information extraction, are only\nsuitable for fixed category definitions and granularities, and cannot achieve\nflexible applications customized by users. Therefore, this paper defines a new\ntask named ``Class-Agnostic Region-of-Interest Matching'' (``RoI-Matching'' for\nshort), which aims to match the customized regions in a flexible, efficient,\nmulti-granularity, and open-set manner. The visual prompt of the reference\ndocument and target document images are fed into our model, while the output is\nthe corresponding bounding boxes in the target document images. To meet the\nabove requirements, we construct a benchmark RoI-Matching-Bench, which sets\nthree levels of difficulties following real-world conditions, and propose the\nmacro and micro metrics to evaluate. Furthermore, we also propose a new\nframework RoI-Matcher, which employs a siamese network to extract multi-level\nfeatures both in the reference and target domains, and cross-attention layers\nto integrate and align similar semantics in different domains. Experiments show\nthat our method with a simple procedure is effective on RoI-Matching-Bench, and\nserves as the baseline for further research. The code is available at\nhttps://github.com/pd162/RoI-Matching.", "comment": "Accepted by ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.21055v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21055v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "文档图像中类别无关的感兴趣区域匹配", "tldr": "现有文档分析解决方案缺乏灵活性。本文提出了一项名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务，旨在以灵活、高效、多粒度和开放集的方式匹配自定义区域。为此，构建了基准测试RoI-Matching-Bench，并提出了基于孪生网络和交叉注意力的RoI-Matcher框架。实验证明该方法有效，可作为后续研究的基线。", "motivation": "现有文档分析解决方案（如文档版面分析和关键信息提取）仅适用于固定的类别定义和粒度，无法实现用户定制的灵活应用。", "method": "本文定义了一项新任务，即“类别无关感兴趣区域匹配”（RoI-Matching），旨在以灵活、高效、多粒度和开放集的方式匹配自定义区域。模型输入为参考文档和目标文档图像的视觉提示，输出为目标文档图像中对应的边界框。为满足需求，构建了包含三个难度级别的基准测试RoI-Matching-Bench，并提出了宏观和微观评估指标。此外，还提出了一个新的框架RoI-Matcher，该框架采用孪生网络来提取参考域和目标域中的多级特征，并利用交叉注意力层来整合和对齐不同域中的相似语义。", "result": "实验表明，本文提出的方法RoI-Matcher在RoI-Matching-Bench上效果显著，且过程简单。", "conclusion": "本文提出的方法可以作为RoI-Matching任务的基线，供进一步研究。", "translation": "文档理解和分析因其广泛应用而受到广泛关注。然而，现有的文档分析解决方案，如文档版面分析和关键信息提取，仅适用于固定的类别定义和粒度，无法实现用户定制的灵活应用。因此，本文定义了一项名为“类别无关感兴趣区域匹配”（简称“RoI-Matching”）的新任务，旨在以灵活、高效、多粒度和开放集的方式匹配自定义区域。参考文档和目标文档图像的视觉提示被输入到我们的模型中，输出是目标文档图像中相应的边界框。为了满足上述要求，我们构建了一个基准测试RoI-Matching-Bench，该基准测试根据实际情况设置了三个难度级别，并提出了宏观和微观指标进行评估。此外，我们还提出了一个新的框架RoI-Matcher，该框架采用孪生网络来提取参考域和目标域中的多级特征，并利用交叉注意力层来整合和对齐不同域中的相似语义。实验表明，我们的方法过程简单，在RoI-Matching-Bench上是有效的，并可作为进一步研究的基线。代码可在https://github.com/pd162/RoI-Matching 获取。", "summary": "为解决现有文档分析方案缺乏用户定制灵活性问题，本文提出了“类别无关感兴趣区域匹配”（RoI-Matching）的新任务，旨在实现灵活、高效、多粒度和开放集的定制区域匹配。为此，论文构建了RoI-Matching-Bench基准测试，并提出了基于孪生网络和交叉注意力的RoI-Matcher框架。实验证明RoI-Matcher简单且有效，可作为该任务的基线。", "keywords": "文档理解, 感兴趣区域匹配, 类别无关, 孪生网络, 文档分析", "comments": "本文通过定义一个全新的“类别无关感兴趣区域匹配”任务，有效解决了现有文档分析在灵活性和用户定制方面的局限性。其创新之处在于提出了开放集、多粒度的匹配范式，并为此任务构建了专门的基准测试和评估指标，这对于该领域后续研究具有重要的奠基意义。RoI-Matcher框架结合孪生网络和交叉注意力，提供了一个简洁而有效的解决方案。"}}
{"id": "2506.21137", "title": "NaLaFormer: Norm-Aware Linear Attention for Transformer Models", "authors": ["Weikang Meng", "Yadan Luo", "Liangyu Huo", "Yaowei Wang", "Xin Li", "Zheng Zhang"], "summary": "Linear attention has emerged as a viable alternative to softmax attention by\nreducing complexity from quadratic to linear in sequence length. To preserve\ntwo fundamental properties of softmax, non-negativity and entropy reduction,\ncurrent works employ various linearly separatable kernel functions with $L1$\nnormalization instead of softmax operator. However, query norms are neglected\nby the normalization operation in linear attention, such degradation heavily\nleads to an entropy gap. Meanwhile, existing works inhibit negative values of\nquery and key vectors resulting in a missing inner-product interactions after\nbeing mapped. To address these dual challenges, we propose a novel Norm-Aware\nLinear Attention mechanism serving to restore norm-guided dynamic spikiness and\nrecover kernel-perturbed norm distributions. Specifically, we first decouple\nquery and key matrices into two components: norm and direction, to achieve\nnorm-aware spikiness control and norm consistency, respectively. We\nmathematically reveal that the extent of entropy reduction varies with the\nquery norm in softmax normalization, motivating a query-norm aware kernel\nfunction for dynamic control over entropy reduction. Furthermore, to ensure\nnorm consistency and enforce non-negativity constraints, we employ a\nnorm-preserving mapping to project all elements of the angular matrix into\npositive values, leveraging cosine similarity to inhibit dimensions with\nopposite directions. We conduct extensive experiments demonstrating that the\nNaLaFormer improves performance on vision and language tasks, enhancing both\nexpressiveness and efficiency by up to 4.2\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21137v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21137v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "NaLaFormer：面向Transformer模型的范数感知线性注意力", "tldr": "现有的线性注意力机制忽略了查询范数，导致熵间隙和内积交互缺失。NaLaFormer提出了一种范数感知线性注意力机制，通过解耦查询和键矩阵、引入范数感知核函数和范数保留映射来解决这些问题，从而在视觉和语言任务上提高了性能和效率。", "motivation": "现有的线性注意力机制通过L1范数归一化忽略了查询范数，导致熵间隙。同时，现有方法抑制了查询和键向量的负值，导致映射后内积交互缺失。本文旨在恢复范数引导的动态尖峰性和恢复核扰动的范数分布。", "method": "我们提出了一种范数感知线性注意力（NaLaFormer）机制。具体地，首先将查询和键矩阵解耦为范数和方向两个分量，分别实现范数感知的尖峰性控制和范数一致性。其次，引入了一种查询范数感知的核函数，以动态控制熵减。此外，采用范数保留映射将角度矩阵的所有元素投影为正值，并利用余弦相似度抑制方向相反的维度，以确保范数一致性并强制执行非负性约束。", "result": "NaLaFormer在视觉和语言任务上提高了性能，表达能力和效率均提升高达4.2%。", "conclusion": "NaLaFormer通过解决线性注意力中查询范数被忽略和负值抑制的问题，有效提升了Transformer模型在视觉和语言任务上的性能和效率。", "translation": "线性注意力已成为softmax注意力的可行替代方案，将复杂度从序列长度的二次方降低到线性。为了保持softmax的两个基本特性：非负性和熵减，当前工作采用各种线性可分离核函数和L1范数归一化而非softmax操作。然而，线性注意力中的归一化操作忽略了查询范数，这种退化严重导致了熵间隙。同时，现有工作抑制了查询和键向量的负值，导致映射后缺少内积交互。为了解决这些双重挑战，我们提出了一种新颖的范数感知线性注意力机制，旨在恢复范数引导的动态尖峰性并恢复核扰动的范数分布。具体地，我们首先将查询和键矩阵解耦为范数和方向两个分量，分别实现范数感知的尖峰性控制和范数一致性。我们从数学上揭示了熵减的程度随softmax归一化中查询范数的变化而变化，这促使我们设计了一个查询范数感知的核函数，用于动态控制熵减。此外，为了确保范数一致性并强制执行非负性约束，我们采用范数保留映射将角度矩阵的所有元素投影为正值，利用余弦相似度抑制方向相反的维度。我们进行了广泛的实验，证明NaLaFormer在视觉和语言任务上提高了性能，表达能力和效率均提升高达4.2%。", "summary": "本文提出了一种名为NaLaFormer的范数感知线性注意力机制，旨在解决现有线性注意力中查询范数被忽略导致的熵间隙以及负值抑制导致的内积交互缺失问题。NaLaFormer通过将查询和键矩阵解耦为范数和方向分量，引入查询范数感知的核函数以动态控制熵减，并采用范数保留映射确保范数一致性和非负性。实验证明，NaLaFormer在视觉和语言任务上提高了性能、表达能力和效率，提升高达4.2%。", "keywords": "线性注意力, Transformer, 范数感知, 熵减, NaLaFormer", "comments": "NaLaFormer的创新点在于其对线性注意力中范数问题的深入洞察和解决方案。通过解耦、范数感知核函数和范数保留映射，它不仅解决了现有方法的局限性，还提升了模型的表达能力和效率，为Transformer模型的优化提供了新的思路。"}}
{"id": "2506.21056", "title": "SAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification", "authors": ["Dinh-Khoi Vo", "Van-Loc Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Retrieving 3D objects in complex indoor environments using only a masked 2D\nimage and a natural language description presents significant challenges. The\nROOMELSA challenge limits access to full 3D scene context, complicating\nreasoning about object appearance, geometry, and semantics. These challenges\nare intensified by distorted viewpoints, textureless masked regions, ambiguous\nlanguage prompts, and noisy segmentation masks. To address this, we propose\nSAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification. SAMURAI\nintegrates CLIP-based semantic matching with shape-guided re-ranking derived\nfrom binary silhouettes of masked regions, alongside a robust majority voting\nstrategy. A dedicated preprocessing pipeline enhances mask quality by\nextracting the largest connected component and removing background noise. Our\nhybrid retrieval framework leverages both language and shape cues, achieving\ncompetitive performance on the ROOMELSA private test set. These results\nhighlight the importance of combining shape priors with language understanding\nfor robust open-world 3D object retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21056v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21056v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SAMURAI：用于3D物体识别的形状感知多模态检索", "tldr": "SAMURAI通过结合CLIP语义匹配和形状引导重排序，解决了在复杂室内环境中仅使用2D图像和自然语言描述检索3D物体的挑战。", "motivation": "在复杂室内环境中，仅使用遮罩的2D图像和自然语言描述检索3D物体存在显著挑战，尤其是在ROOMELSA挑战中，由于缺乏完整的3D场景上下文、扭曲的视角、无纹理的遮罩区域、模糊的语言提示和嘈杂的分割掩模，使得推理物体外观、几何和语义变得复杂。", "method": "本文提出了SAMURAI（Shape-Aware Multimodal Retrieval for 3D Object Identification），它将基于CLIP的语义匹配与从遮罩区域的二值轮廓导出的形状引导重排序相结合，并辅以鲁棒的多数投票策略。此外，一个专门的预处理管道通过提取最大连通分量和去除背景噪声来提高掩模质量。该混合检索框架利用语言和形状线索。", "result": "SAMURAI在ROOMELSA私人测试集上取得了有竞争力的性能。", "conclusion": "这些结果强调了将形状先验与语言理解相结合对于鲁士开放世界3D物体检索的重要性。", "translation": "仅使用遮罩的2D图像和自然语言描述在复杂室内环境中检索3D物体带来了显著挑战。ROOMELSA挑战限制了对完整3D场景上下文的访问，使推理物体外观、几何和语义变得复杂。这些挑战因扭曲的视角、无纹理的遮罩区域、模糊的语言提示和嘈杂的分割掩模而加剧。为了解决这个问题，我们提出了SAMURAI：用于3D物体识别的形状感知多模态检索。SAMURAI将基于CLIP的语义匹配与从遮罩区域的二值轮廓导出的形状引导重排序以及鲁棒的多数投票策略相结合。一个专门的预处理管道通过提取最大连通分量和去除背景噪声来提高掩模质量。我们的混合检索框架利用语言和形状线索，在ROOMELSA私人测试集上取得了有竞争力的性能。这些结果突出了将形状先验与语言理解相结合对于鲁士开放世界3D物体检索的重要性。", "summary": "本文提出了SAMURAI，一个用于3D物体识别的形状感知多模态检索框架。该框架旨在解决在复杂室内环境中仅使用2D遮罩图像和自然语言描述进行3D物体检索的难题。SAMURAI结合了CLIP的语义匹配和基于二值轮廓的形状引导重排序，并辅以多数投票策略。它还包括一个预处理管道来优化掩模质量。该方法通过融合语言和形状线索，在ROOMELSA挑战中表现出色，证明了形状先验与语言理解结合对于鲁棒3D物体检索的关键作用。", "keywords": "3D物体检索, 多模态检索, 形状感知, CLIP, ROOMELSA", "comments": "该论文的创新点在于其混合检索框架，巧妙地结合了基于CLIP的语言语义理解和形状引导的几何信息，以克服2D图像和语言描述在3D物体检索中的局限性。特别是，利用二值轮廓进行形状引导重排序和多数投票策略，以及专门的掩模预处理，都增强了系统的鲁棒性。这种多模态融合的方法对于开放世界3D物体检索具有重要意义。"}}
{"id": "2506.21220", "title": "Complexity-aware fine-tuning", "authors": ["Andrey Goncharov", "Daniil Vyazhev", "Petr Sychev", "Edvard Khalafyan", "Alexey Zaytsev"], "summary": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21220v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21220v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "复杂度感知微调", "tldr": "提出一种新的高效微调LLM的方法，通过熵识别复杂数据，并仅对复杂数据进行推理，从而在显著减少数据量的情况下达到与蒸馏相当的性能，并优于标准SFT。", "motivation": "通用大型语言模型（LLMs）通过监督微调（SFT）来提高特定领域性能，但通过蒸馏大型模型的思维链虽然能获得更好结果，却需要大量的昂贵调用和更多的数据，效率低下。", "method": "本文提出了一种高效微调的新蓝图，该蓝图仅对通过熵识别的复杂数据使用推理。具体方法是在两个小型开源模型（约3B）上，通过单令牌答案熵（ROC AUC 0.73）将训练数据分为复杂性类别，并通过SFT和蒸馏对LLMs进行微调。", "result": "我们的方法显著优于标准SFT方法（平均准确率0.55 对 0.43），并且在数据量减少62%的情况下，提供了与蒸馏相当的性能（两者平均准确率均为0.55）。", "conclusion": "该研究提出了一种高效的LLM微调方法，通过选择性地对复杂数据进行处理，实现了与数据密集型蒸馏方法相当的性能，同时显著降低了数据需求，并优于标准SFT。", "translation": "通用大型语言模型（LLMs）经常通过监督微调（SFT）进行微调，以提高在特定领域的性能。通过蒸馏更大模型的思维链可以获得更好的结果，但代价是大量的昂贵调用和更多的数据。我们提出了一种高效微调的新颖蓝图，该蓝图仅对通过熵识别的复杂数据使用推理。具体来说，在两个小型开源模型（约3B）上，我们通过单令牌答案熵（ROC AUC 0.73）将训练数据分为复杂性类别，通过SFT和蒸馏对大型语言模型（LLMs）进行微调，并表明我们的管道显著优于标准SFT方法（平均准确率0.55对0.43），并且在数据量减少62%的情况下，提供了与蒸馏相当的性能（两者平均准确率均为0.55）。我们发布了代码和数据，以促进该方向的进一步研究。", "summary": "本文提出了一种新颖的“复杂度感知微调”方法，旨在提高大型语言模型（LLMs）的微调效率。该方法通过熵值识别训练数据中的复杂样本，并仅对这些复杂数据应用推理，从而避免了传统蒸馏方法所需的大量计算和数据。实验结果表明，该方法在显著减少62%数据量的情况下，其性能优于标准监督微调（SFT），并能达到与完整蒸馏方法相当的准确率。", "keywords": "LLM微调, 复杂度感知, 熵, 数据效率, 知识蒸馏", "comments": "该论文的创新点在于引入了“复杂度感知”的概念，通过熵来智能地选择性应用推理，从而显著提高了LLM微调的效率。这对于资源受限或需要快速部署LLM的场景具有重要意义，因为它在保持性能的同时大幅减少了数据和计算开销。"}}
{"id": "2506.21140", "title": "DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding", "authors": ["Ziwei Wang", "Hongbin Wang", "Tianwang Jia", "Xingyi He", "Siyang Li", "Dongrui Wu"], "summary": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform\nspontaneous/evoked neural activity into control commands for external\ncommunication. While convolutional neural networks (CNNs) remain the mainstream\nbackbone for EEG decoding, their inherently short receptive field makes it\ndifficult to capture long-range temporal dependencies and global inter-channel\nrelationships. Recent CNN-Transformer (Conformers) hybrids partially address\nthis issue, but most adopt a serial design, resulting in suboptimal integration\nof local and global features, and often overlook explicit channel-wise\nmodeling. To address these limitations, we propose DBConformer, a dual-branch\nconvolutional Transformer network tailored for EEG decoding. It integrates a\ntemporal Conformer to model long-range temporal dependencies and a spatial\nConformer to extract inter-channel interactions, capturing both temporal\ndynamics and spatial patterns in EEG signals. A lightweight channel attention\nmodule further refines spatial representations by assigning data-driven\nimportance to EEG channels. Extensive experiments on five motor imagery (MI)\ndatasets and two seizure detection datasets under three evaluation settings\ndemonstrate that DBConformer consistently outperforms 10 competitive baseline\nmodels, with over eight times fewer parameters than the high-capacity EEG\nConformer baseline. Further, the visualization results confirm that the\nfeatures extracted by DBConformer are physiologically interpretable and aligned\nwith sensorimotor priors in MI. The superior performance and interpretability\nof DBConformer make it reliable for robust and explainable EEG decoding. Code\nis publicized at https://github.com/wzwvv/DBConformer.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21140v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21140v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DBConformer：用于脑电图解码的双分支卷积Transformer", "tldr": "DBConformer是一种双分支卷积Transformer网络，用于脑电图解码，通过集成时间Conformer和空间Conformer来捕获长程时间依赖性和通道间交互，并在多个数据集上表现出优越的性能和可解释性。", "motivation": "现有的卷积神经网络（CNNs）在脑电图（EEG）解码中难以捕获长程时间依赖性和全局通道间关系，因为其感受野较短。而目前的CNN-Transformer混合模型（Conformers）大多采用串行设计，导致局部和全局特征的整合不理想，并且常常忽略显式的通道建模。", "method": "本文提出了DBConformer，一种专为脑电图解码设计的双分支卷积Transformer网络。它集成了一个时间Conformer来建模长程时间依赖性，以及一个空间Conformer来提取通道间交互，从而捕获脑电信号中的时间动态和空间模式。一个轻量级通道注意力模块通过为脑电图通道分配数据驱动的重要性来进一步优化空间表示。", "result": "在五个运动想象（MI）数据集和两个癫痫检测数据集上的广泛实验表明，DBConformer在三种评估设置下始终优于10个有竞争力的基线模型，且参数量比高容量的EEG Conformer基线少八倍以上。此外，可视化结果证实DBConformer提取的特征具有生理可解释性，并与运动想象中的感觉运动先验一致。", "conclusion": "DBConformer的卓越性能和可解释性使其在稳健和可解释的脑电图解码中具有可靠性。", "translation": "基于脑电图（EEG）的脑机接口（BCIs）将自发/诱发的神经活动转化为外部通信的控制命令。虽然卷积神经网络（CNNs）仍然是脑电图解码的主流骨干网络，但其固有的短感受野使其难以捕获长程时间依赖性和全局通道间关系。最近的CNN-Transformer混合模型（Conformers）部分解决了这个问题，但大多数采用串行设计，导致局部和全局特征的整合不理想，并且常常忽略显式的通道建模。为了解决这些限制，我们提出了DBConformer，一种专为脑电图解码设计的双分支卷积Transformer网络。它集成了一个时间Conformer来建模长程时间依赖性，以及一个空间Conformer来提取通道间交互，从而捕获脑电信号中的时间动态和空间模式。一个轻量级通道注意力模块通过为脑电图通道分配数据驱动的重要性来进一步优化空间表示。在五个运动想象（MI）数据集和两个癫痫检测数据集上的广泛实验表明，DBConformer在三种评估设置下始终优于10个有竞争力的基线模型，且参数量比高容量的EEG Conformer基线少八倍以上。此外，可视化结果证实DBConformer提取的特征具有生理可解释性，并与运动想象中的感觉运动先验一致。DBConformer的卓越性能和可解释性使其在稳健和可解释的脑电图解码中具有可靠性。代码已在https://github.com/wzwvv/DBConformer上公开。", "summary": "该论文提出了DBConformer，一种用于脑电图（EEG）解码的双分支卷积Transformer网络。针对现有CNN和Conformer模型在捕获长程时间依赖性、全局通道关系以及特征整合方面的不足，DBConformer通过并行的时间Conformer和空间Conformer分别处理EEG信号的时间动态和空间模式，并结合通道注意力模块优化空间表示。实验结果表明，DBConformer在多个运动想象和癫痫检测数据集上表现出优于现有基线模型的性能，且参数量显著减少，同时提取的特征具有良好的生理可解释性。", "keywords": "脑电图解码, 卷积Transformer, 双分支网络, 时间依赖性, 空间交互", "comments": "DBConformer的创新点在于其双分支结构，能够并行处理时间依赖性和空间交互，并结合轻量级通道注意力，有效解决了传统CNN和串行Conformer在EEG解码中遇到的挑战。其在保持高性能的同时显著减少参数量，并提供可解释的特征，这对于脑机接口的应用至关重要，提高了模型的实用性和可靠性。"}}
{"id": "2506.21076", "title": "PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image", "authors": ["Hongyu Yan", "Kunming Luo", "Weiyu Li", "Yixun Liang", "Shengming Li", "Jingwei Huang", "Chunchao Guo", "Ping Tan"], "summary": "3D characters play a crucial role in our daily entertainment. To improve the\nefficiency of 3D character modeling, recent image-based methods use two\nseparate models to achieve pose standardization and 3D reconstruction of the\nA-pose character. However, these methods are prone to generating distorted and\ndegraded images in the pose standardization stage due to self-occlusion and\nviewpoints, which further affects the geometric quality of the subsequent\nreconstruction process. To tackle these problems, we propose PoseMaster, an\nend-to-end controllable 3D character generation framework. Specifically, we\nunify pose transformation and 3D character generation into a flow-based 3D\nnative generation framework. To achieve accurate arbitrary-pose control, we\npropose to leverage the 3D body bones existing in the skeleton of an animatable\ncharacter as the pose condition. Furthermore, considering the specificity of\nmulti-condition control, we randomly empty the pose condition and the image\ncondition during training to improve the effectiveness and generalizability of\npose control. Finally, we create a high-quality pose-control dataset derived\nfrom realistic character animation data to make the model learning the implicit\nrelationships between skeleton and skinning weights. Extensive experiments show\nthat PoseMaster outperforms current state-of-the-art techniques in both\nqualitative and quantitative evaluations for A-pose character generation while\ndemonstrating its powerful ability to achieve precise control for arbitrary\nposes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21076v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21076v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PoseMaster：从单张图像生成任意姿态的3D角色", "tldr": "PoseMaster是一个端到端可控的3D角色生成框架，能从单张图像生成任意姿态的3D角色，解决了现有方法中姿态标准化导致的图像失真和几何质量问题。", "motivation": "为了提高3D角色建模效率，现有基于图像的方法使用两个独立模型进行姿态标准化和3D重建，但在姿态标准化阶段易产生扭曲和退化的图像，影响后续重建的几何质量。", "method": "我们提出了PoseMaster，一个端到端可控的3D角色生成框架。它将姿态变换和3D角色生成统一到基于流的3D原生生成框架中。为实现精确的任意姿态控制，我们利用可动画角色骨架中的3D身体骨骼作为姿态条件。此外，在训练过程中随机清空姿态条件和图像条件，以提高姿态控制的有效性和泛化性。最后，我们创建了一个高质量的姿态控制数据集，源自真实的字符动画数据，使模型学习骨骼和蒙皮权重之间的隐含关系。", "result": "大量的实验表明，PoseMaster在A姿态角色生成方面，无论在定性还是定量评估中，都优于当前的最新技术，同时展示了其对任意姿态实现精确控制的强大能力。", "conclusion": "PoseMaster成功地解决了现有方法在3D角色生成中遇到的挑战，通过其端到端框架和精确的姿态控制能力，显著提升了从单张图像生成高质量任意姿态3D角色的效果。", "translation": "3D角色在我们的日常娱乐中扮演着至关重要的角色。为了提高3D角色建模的效率，最近基于图像的方法使用两个独立的模型来实现A姿态角色的姿态标准化和3D重建。然而，这些方法在姿态标准化阶段由于自遮挡和视角问题，容易生成扭曲和退化的图像，这进一步影响了后续重建过程的几何质量。为了解决这些问题，我们提出了PoseMaster，一个端到端可控的3D角色生成框架。具体而言，我们将姿态变换和3D角色生成统一到一个基于流的3D原生生成框架中。为了实现精确的任意姿态控制，我们建议利用可动画角色骨架中存在的3D身体骨骼作为姿态条件。此外，考虑到多条件控制的特殊性，我们在训练过程中随机清空姿态条件和图像条件，以提高姿态控制的有效性和泛化性。最后，我们创建了一个高质量的姿态控制数据集，源自真实的字符动画数据，使模型学习骨骼和蒙皮权重之间的隐含关系。大量的实验表明，PoseMaster在A姿态角色生成方面，无论在定性还是定量评估中，都优于当前的最新技术，同时展示了其对任意姿态实现精确控制的强大能力。", "summary": "PoseMaster是一个端到端可控的3D角色生成框架，旨在从单张图像生成任意姿态的3D角色。它通过将姿态变换和3D角色生成统一到基于流的3D原生框架中，解决了现有方法在姿态标准化阶段的图像失真和几何质量问题。该方法利用3D身体骨骼作为姿态条件实现精确控制，并通过随机清空条件来提高泛化性，同时构建了高质量的姿态控制数据集。实验证明，PoseMaster在生成A姿态角色和实现任意姿态精确控制方面均优于现有技术。", "keywords": "3D角色生成, 姿态控制, 单图像, 端到端, 骨骼条件", "comments": "PoseMaster的创新之处在于其端到端的框架设计，将姿态变换和3D生成无缝整合，有效解决了传统分阶段方法的弊端。利用3D骨骼作为姿态条件实现了高精度的任意姿态控制，而随机清空条件的训练策略则显著增强了模型的泛化能力。高质量数据集的构建也为此类研究提供了宝贵的资源。"}}
{"id": "2506.21263", "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster", "authors": ["Ji Qi", "WenPeng Zhu", "Li Li", "Ming Wu", "YingJun Wu", "Wu He", "Xun Gao", "Jason Zeng", "Michael Heinrich"], "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21263v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21263v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DiLoCoX：一种用于去中心化集群的低通信大规模训练框架", "tldr": "DiLoCoX是一个低通信、大规模的去中心化集群训练框架，它结合了多种策略，使得在慢速网络上对千亿参数级模型进行分布式训练成为可能，并实现了显著的加速。", "motivation": "基础模型（特别是大型语言模型）的分布式训练需要高水平的通信，高度依赖于具有快速可靠互连的中心化集群。当前面临的挑战是如何在慢速网络上对超过千亿参数的模型进行训练，以释放去中心化集群的潜力。", "method": "本文提出了DiLoCoX，一个低通信大规模去中心化集群训练框架。它结合了流水线并行（Pipeline Parallelism）、双优化器策略（Dual Optimizer Policy）、通信与本地训练的一步延迟重叠（One-Step-Delay Overlap of Communication and Local Training）以及自适应梯度压缩方案（Adaptive Gradient Compression Scheme）。同时，通过收敛性理论分析证明了一步延迟重叠和自适应梯度压缩方案的有效性。", "result": "DiLoCoX能够在1Gbps的网络上预训练一个107B的基础模型。与传统的AllReduce相比，DiLoCoX在分布式训练中实现了357倍的速度提升，同时模型的收敛性退化可以忽略不计。", "conclusion": "DiLoCoX是首个成功应用于万亿参数级以上模型的去中心化训练框架，它极大地提高了参数规模和模型预训练速度，使得在慢速网络上进行大规模模型训练成为可能。", "translation": "基础模型，特别是大型语言模型（LLMs）的分布式训练需要高水平的通信。因此，它高度依赖于具有快速可靠互连的中心化集群。我们能否在慢速网络上进行训练，从而在处理超过千亿参数的模型时释放去中心化集群的潜力？在本文中，我们提出了DiLoCoX，一个低通信大规模去中心化集群训练框架。它结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案。这种组合显著提高了参数规模和模型预训练的速度。我们通过收敛性理论分析证明了通信与本地训练的一步延迟重叠以及自适应梯度压缩方案的益处。经验上，我们证明了DiLoCoX能够在1Gbps的网络上预训练一个107B的基础模型。与传统的AllReduce相比，DiLoCoX在分布式训练中实现了357倍的速度提升，同时模型的收敛性退化可以忽略不计。据我们所知，这是第一个成功应用于超过千亿参数模型的去中心化训练框架。", "summary": "DiLoCoX是一种创新的低通信大规模去中心化集群训练框架，旨在解决大型语言模型在慢速网络上进行分布式训练的挑战。该框架通过结合流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案，显著提高了模型预训练的参数规模和速度。理论分析证明了其核心组件的收敛性，并通过实验证明DiLoCoX能在1Gbps网络上高效预训练107B模型，实现357倍于AllReduce的加速，同时保持良好收敛性。这是首个成功应用于千亿参数级以上模型的去中心化训练框架。", "keywords": "去中心化训练, 低通信, 大规模模型, 流水线并行, 梯度压缩", "comments": "DiLoCoX的创新性在于其整合多种优化策略，以实现在低带宽去中心化网络上对超大规模模型进行高效训练。其理论分析和显著的经验加速（357x）证明了其在解决分布式训练通信瓶颈方面的巨大潜力。这项工作为未来在资源受限环境下部署和训练巨型模型开辟了新途径，具有重要的实践意义。"}}
{"id": "2506.21142", "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "summary": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21142v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21142v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "无人机网络攻击的生成对抗规避与域外检测", "tldr": "本文提出一种基于cGAN生成无人机网络对抗攻击并利用CVAE进行检测的框架，旨在增强入侵检测系统（IDS）抵御新型隐蔽威胁的能力。", "motivation": "传统的入侵检测系统（IDS）和域外（OOD）检测器在识别新型、隐蔽的无人机网络攻击时面临挑战，难以区分隐蔽的对抗性攻击与真实的OOD事件，导致系统易受攻击。", "method": "本文提出一个基于条件生成对抗网络（cGAN）的框架来生成隐蔽的对抗性攻击，以规避IDS机制。首先设计一个鲁棒的多类别IDS分类器，在良性无人机遥测数据和已知网络攻击上进行训练。然后，cGAN利用该分类器扰动已知攻击，生成被误分类为良性但统计上仍类似于OOD分布的对抗性样本，并进行迭代优化。为检测这些扰动，引入一个条件变分自编码器（CVAE），利用负对数似然将对抗性输入与真实的OOD样本分离。", "result": "比较评估表明，基于CVAE的悔恨分数在识别隐蔽对抗性威胁方面显著优于传统的基于马哈拉诺比斯距离的检测器。", "conclusion": "研究强调了先进概率建模对于增强IDS抵御自适应、基于生成模型的网络入侵的重要性。", "translation": "无人机日益融入民用空域，凸显了对弹性智能入侵检测系统（IDS）的需求，因为传统的异常检测方法往往无法识别新型威胁。一种常见的方法是将不熟悉的攻击视为域外（OOD）样本；然而，当缓解措施不足时，这会使系统容易受到攻击。此外，传统的OOD检测器难以区分隐蔽的对抗性攻击和真实的OOD事件。本文介绍了一种基于条件生成对抗网络（cGAN）的框架，用于制作规避IDS机制的隐蔽对抗性攻击。我们首先设计了一个鲁棒的多类别IDS分类器，该分类器在良性无人机遥测数据和已知网络攻击（包括拒绝服务（DoS）、虚假数据注入（FDI）、中间人（MiTM）和重放攻击）上进行训练。使用该分类器，我们的cGAN扰动已知攻击以生成对抗性样本，这些样本被错误地分类为良性，同时保留了与OOD分布的统计相似性。这些对抗性样本经过迭代细化以实现高隐蔽性和成功率。为了检测此类扰动，我们实现了一个条件变分自编码器（CVAE），利用负对数似然将对抗性输入与真实的OOD样本分离。比较评估表明，基于CVAE的悔恨分数在识别隐蔽对抗性威胁方面显著优于传统的基于马哈拉诺比斯距离的检测器。我们的发现强调了先进概率建模对于增强IDS抵御自适应、基于生成模型的网络入侵的重要性。", "summary": "本文针对无人机网络攻击中传统入侵检测系统和域外检测器无法有效识别新型隐蔽威胁的问题，提出了一种基于cGAN生成对抗性攻击样本并利用CVAE进行检测的框架。该框架首先训练一个IDS分类器，然后利用cGAN生成能规避IDS且统计上类似OOD的隐蔽对抗样本。为检测这些样本，引入CVAE，并通过负对数似然区分对抗性输入与真实OOD样本。实验证明，CVAE在检测隐蔽对抗性威胁方面优于传统方法，强调了先进概率建模在增强IDS能力中的重要性。", "keywords": "无人机网络攻击, 生成对抗网络, 域外检测, 入侵检测系统, 条件变分自编码器", "comments": "该论文创新性地结合了生成对抗网络（cGAN）来制造隐蔽的无人机网络攻击样本，并引入条件变分自编码器（CVAE）来检测这些高级对抗性威胁，解决了传统IDS和OOD检测器的局限性。其重要性在于提升了无人机IDS对新型、自适应网络攻击的鲁棒性，为未来网络安全防御提供了新的思路。"}}
{"id": "2506.21080", "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception", "authors": ["Sanjoy Chowdhury", "Subrata Biswas", "Sayan Nag", "Tushar Nagarajan", "Calvin Murdock", "Ishwarya Ananthabhotla", "Yijun Qian", "Vamsi Krishna Ithapu", "Dinesh Manocha", "Ruohan Gao"], "summary": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21080v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21080v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EgoAdapt：面向高效以自我为中心的感知的自适应多感官蒸馏与策略学习", "tldr": "EgoAdapt是一个框架，通过自适应跨模态蒸馏和策略学习，显著提高了以自我为中心感知任务（如动作识别、说话者定位、行为预测）的效率，同时保持或超越了最先进模型的性能。", "motivation": "现代多感官以自我为中心的感知模型性能卓越，但计算成本高昂，难以在资源受限环境中部署。", "method": "本文提出了EgoAdapt框架，通过自适应地执行跨模态蒸馏和策略学习，实现不同以自我为中心感知任务的高效推理。其策略模块可适应特定任务的动作空间，具有广泛适用性。", "result": "在EPIC-Kitchens、EasyCom和Aria Everyday Activities三个数据集上，EgoAdapt显著提高了效率，GMACs降低高达89.09%，参数减少高达82.02%，能耗降低高达9.6倍，同时性能与最先进模型持平或超越。", "conclusion": "EgoAdapt通过自适应多感官蒸馏和策略学习，有效地解决了以自我为中心感知模型计算成本高的问题，实现了高效且高性能的推理，使其更适用于实际部署。", "translation": "现代感知模型，特别是那些为多感官以自我为中心任务设计的模型，已经取得了显著的性能，但通常伴随着巨大的计算成本。这些高要求对实际部署构成了挑战，尤其是在资源受限的环境中。在本文中，我们引入了EgoAdapt，一个自适应执行跨模态蒸馏和策略学习的框架，旨在为不同的以自我为中心感知任务（包括以自我为中心的动作识别、主动说话者定位和行为预测）实现高效推理。我们提出的策略模块可适应特定任务的动作空间，使其具有广泛适用性。在EPIC-Kitchens、EasyCom和Aria Everyday Activities三个具有挑战性的以自我为中心数据集上的实验结果表明，我们的方法显著提高了效率，将GMACs降低高达89.09%，参数减少高达82.02%，能耗降低高达9.6倍，同时性能与相应的最先进模型持平，并且在许多情况下表现更优。", "summary": "EgoAdapt是一个为解决现代以自我为中心感知模型高计算成本问题而设计的框架。它通过自适应跨模态蒸馏和策略学习，显著提升了以自我为中心动作识别、主动说话者定位和行为预测等任务的推理效率，同时保持或超越了现有最先进模型的性能。实验证明，EgoAdapt在计算资源和能耗方面均有大幅优化，使其更适合资源受限环境下的实际部署。", "keywords": "以自我为中心感知, 多感官, 蒸馏, 策略学习, 效率", "comments": "该论文提出的EgoAdapt框架在解决以自我为中心感知模型的计算效率问题上具有重要意义。其创新点在于结合了自适应跨模态蒸馏和策略学习，使得模型在保持高性能的同时，大幅降低了GMACs、参数和能耗，这对于资源受限的实际部署场景（如可穿戴设备）至关重要。该方法的普适性（适用于不同任务）也增强了其价值。"}}
{"id": "2506.21277", "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context", "authors": ["Qize Yang", "Shimin Yao", "Weixuan Chen", "Shenghao Fu", "Detao Bai", "Jiaxing Zhao", "Boyuan Sun", "Bowen Yin", "Xihan Wei", "Jingren Zhou"], "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21277v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21277v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HumanOmniV2：从理解到结合上下文的全模态推理", "tldr": "本文提出了HumanOmniV2，一种通过引入上下文奖励和逻辑奖励来解决多模态大语言模型中上下文理解不足和捷径问题的方法，并引入了一个新的基准IntentBench，以提高全模态推理能力。", "motivation": "随着多模态大语言模型的快速发展，深入理解和解释人类意图的能力变得至关重要，这需要详细和周密的推理。尽管强化学习在增强大语言模型的推理能力方面展现了潜力，但将其适应多模态数据和格式的挑战仍未得到充分解决。现有模型存在两个问题：全局上下文理解不足和捷径问题。", "method": "为了解决上下文理解不足和捷径问题，本文强调模型需要对多模态输入中的全局上下文有清晰的理解。通过大语言模型判断的上下文奖励、格式奖励和准确性奖励来确保多模态上下文信息的准确解释。此外，利用大语言模型评估逻辑奖励，以提高复杂的推理能力。论文还引入了一个名为IntentBench的推理全模态基准，旨在评估模型理解复杂人类意图和情感的能力。", "result": "本文提出的方法在多个全模态基准测试中，相比其他开源全模态模型，展现出更先进的性能。", "conclusion": "本文通过引入上下文奖励和逻辑奖励，并提出新的评估基准IntentBench，有效解决了多模态推理中的上下文理解不足和捷径问题，显著提升了全模态模型的推理能力。", "translation": "随着多模态大语言模型的快速发展，深入理解和解释人类意图的能力已成为一项关键能力，这需要详细而周密的推理。在最近的研究中，强化学习（RL）已展示出增强大语言模型（LLMs）推理能力的潜力。然而，将RL适应多模态数据和格式的相关挑战在很大程度上仍未解决。在本文中，我们识别了现有多模态推理模型中的两个问题：全局上下文理解不足和捷径问题。当模型误解多模态上下文时，可能发生上下文理解不足，导致不正确的答案。捷径问题发生时，模型会忽略多模态输入中的关键线索，直接回答查询而不考虑多模态信息。为了解决这些问题，我们强调模型必须在清晰理解多模态输入中全局上下文的情况下进行推理。这种全局上下文理解可以有效防止模型忽略关键多模态线索，并确保彻底的推理过程。为了确保多模态上下文信息的准确解释，我们实现了由大语言模型判断的上下文奖励，以及格式和准确性奖励。此外，为了提高复杂推理能力，我们采用LLM来评估逻辑奖励，判断推理过程是否成功地将多模态信息与逻辑方法相结合。我们还引入了一个推理全模态基准IntentBench，旨在评估模型理解复杂人类意图和情感的能力。我们提出的方法在多个全模态基准测试中，相比其他开源全模态模型，展现出更先进的性能。", "summary": "本文提出了HumanOmniV2，旨在解决多模态大语言模型在理解人类意图时存在的全局上下文理解不足和捷径问题。研究强调了全局上下文理解的重要性，并引入了由大语言模型判断的上下文奖励、格式奖励、准确性奖励以及逻辑奖励来增强模型的推理能力。此外，论文还提出了一个名为IntentBench的新型全模态推理基准。实验结果表明，HumanOmniV2在多个全模态基准测试中表现优于其他开源模型。", "keywords": "全模态推理, 上下文理解, 强化学习, 大语言模型, IntentBench", "comments": "本文创新性地将强化学习中的奖励机制应用于多模态大语言模型，特别是引入了由LLM判断的上下文奖励和逻辑奖励，以解决多模态推理中的核心问题。提出的IntentBench基准也对评估复杂人类意图理解具有重要意义。该工作对于提升多模态LLM的鲁棒性和准确性具有重要价值。"}}
{"id": "2506.21144", "title": "Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion", "authors": ["Yuguang Zhang", "Kuangpu Guo", "Zhihe Lu", "Yunbo Wang", "Jian Liang"], "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, but is challenged by\nheterogeneity in data, computation, and communication. Pretrained\nvision-language models (VLMs), with their strong generalization and lightweight\ntuning via prompts, offer a promising solution. However, existing federated\nprompt-learning methods rely only on text prompts and overlook joint\nlabel-domain distribution shifts. In this paper, we propose a personalized FL\nframework based on dual-prompt learning and cross fusion, termed pFedDC.\nSpecifically, each client maintains both global and local prompts across vision\nand language modalities: global prompts capture common knowledge shared across\nthe federation, while local prompts encode client-specific semantics and domain\ncharacteristics. Meanwhile, a cross-fusion module is designed to adaptively\nintegrate prompts from different levels, enabling the model to generate\npersonalized representations aligned with each client's unique data\ndistribution. Extensive experiments across nine datasets with various types of\nheterogeneity show that pFedDC consistently outperforms state-of-the-art\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21144v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21144v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过双提示优化和交叉融合的个性化联邦学习", "tldr": "pFedDC 是一种个性化联邦学习框架，通过双提示学习和交叉融合解决了联邦学习中的数据异构性问题。", "motivation": "联邦学习面临数据、计算和通信异构性挑战。现有联邦提示学习方法仅依赖文本提示，且忽略联合标签域分布漂移。", "method": "本文提出了 pFedDC 框架，该框架基于双提示学习和交叉融合。每个客户端维护视觉和语言模态的全局和局部提示，其中全局提示捕获通用知识，局部提示编码客户端特定语义和域特征。此外，还设计了一个交叉融合模块，用于自适应整合不同级别的提示，以生成与每个客户端独特数据分布对齐的个性化表示。", "result": "在九个具有各种类型异构性的数据集上进行的广泛实验表明，pFedDC 始终优于最先进的方法。", "conclusion": "pFedDC 通过双提示优化和交叉融合，有效解决了联邦学习中的异构性问题，并实现了优越的个性化性能。", "translation": "联邦学习（FL）允许在去中心化客户端之间进行协作模型训练，而无需共享本地数据，但面临数据、计算和通信异构性的挑战。预训练视觉-语言模型（VLMs）凭借其强大的泛化能力和通过提示进行的轻量级调优，提供了一种有前景的解决方案。然而，现有的联邦提示学习方法仅依赖于文本提示，并且忽略了联合标签域分布漂移。在本文中，我们提出了一种基于双提示学习和交叉融合的个性化联邦学习框架，称之为 pFedDC。具体而言，每个客户端在视觉和语言模态中维护全局和局部提示：全局提示捕获联邦中共享的通用知识，而局部提示编码客户端特定的语义和域特征。同时，设计了一个交叉融合模块，以自适应地整合来自不同级别的提示，使模型能够生成与每个客户端独特数据分布对齐的个性化表示。在九个具有各种类型异构性的数据集上进行的广泛实验表明，pFedDC 始终优于最先进的方法。", "summary": "本文提出了 pFedDC，一个基于双提示优化和交叉融合的个性化联邦学习框架，旨在解决联邦学习中的数据异构性问题。该框架允许每个客户端维护视觉和语言模态的全局和局部提示，并通过交叉融合模块自适应整合这些提示，生成个性化表示。实验证明 pFedDC 在多种异构数据集上优于现有SOTA方法。", "keywords": "联邦学习, 个性化, 提示学习, 异构性, 视觉-语言模型", "comments": "该论文的创新点在于结合了预训练视觉-语言模型的提示学习能力，并引入了双提示（全局和局部）以及交叉融合机制来处理联邦学习中的客户端异构性，特别是考虑了联合标签域分布漂移。这种方法为联邦学习的个性化和鲁棒性提供了新的思路。"}}
{"id": "2506.21091", "title": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching", "authors": ["Mahmoud Tahmasebi", "Saif Huq", "Kevin Meehan", "Marion McAfee"], "summary": "Stereo matching has become an increasingly important component of modern\nautonomous systems. Developing deep learning-based stereo matching models that\ndeliver high accuracy while operating in real-time continues to be a major\nchallenge in computer vision. In the domain of cost-volume-based stereo\nmatching, accurate disparity estimation depends heavily on large-scale cost\nvolumes. However, such large volumes store substantial redundant information\nand also require computationally intensive aggregation units for processing and\nregression, making real-time performance unattainable. Conversely, small-scale\ncost volumes followed by lightweight aggregation units provide a promising\nroute for real-time performance, but lack sufficient information to ensure\nhighly accurate disparity estimation. To address this challenge, we propose the\nEnhanced Shuffle Mixer (ESM) to mitigate information loss associated with\nsmall-scale cost volumes. ESM restores critical details by integrating primary\nfeatures into the disparity upsampling unit. It quickly extracts features from\nthe initial disparity estimation and fuses them with image features. These\nfeatures are mixed by shuffling and layer splitting then refined through a\ncompact feature-guided hourglass network to recover more detailed scene\ngeometry. The ESM focuses on local contextual connectivity with a large\nreceptive field and low computational cost, leading to the reconstruction of a\nhighly accurate disparity map at real-time. The compact version of ESMStereo\nachieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX\nOrin.", "comment": "Under peer review", "pdf_url": "http://arxiv.org/pdf/2506.21091v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21091v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ESMStereo：用于实时和精确立体匹配的增强型ShuffleMixer视差上采样", "tldr": "ESMStereo提出增强型Shuffle Mixer (ESM) 解决实时立体匹配中小型代价体信息损失问题，通过特征融合和精炼实现高精度和实时性能，在高端GPU上达116 FPS。", "motivation": "现代自主系统中，开发同时具备高精度和实时性能的深度学习立体匹配模型是一个重大挑战。大型代价体虽然精确但计算成本高难以实时；小型代价体虽然实时但精度不足。", "method": "论文提出了增强型Shuffle Mixer (ESM) 来弥补小型代价体的信息损失。ESM通过将主要特征整合到视差上采样单元中来恢复关键细节。它快速从初始视差估计中提取特征并与图像特征融合，通过混洗和层分割混合这些特征，然后通过一个紧凑的特征引导沙漏网络进行精炼，以恢复更详细的场景几何。ESM关注局部上下文连接性，具有大感受野和低计算成本。", "result": "ESMStereo实现了高精度视差图的实时重建。紧凑版ESMStereo在高端GPU上达到116 FPS的推理速度，在AGX Orin上达到91 FPS。", "conclusion": "ESMStereo成功解决了立体匹配中实现实时性能和高精度并存的挑战，通过其创新的ESM模块有效缓解了小型代价体的信息损失问题。", "translation": "立体匹配已成为现代自主系统日益重要的组成部分。开发能够提供高精度同时实时运行的基于深度学习的立体匹配模型仍然是计算机视觉领域的一大挑战。在基于代价体的立体匹配领域，精确的视差估计严重依赖于大规模代价体。然而，这些大规模代价体存储了大量的冗余信息，并且需要计算密集型的聚合单元进行处理和回归，使得实时性能无法实现。相反，小型代价体随后跟随着轻量级聚合单元为实时性能提供了一条有前景的途径，但缺乏足够的信息来确保高精度的视差估计。为了解决这一挑战，我们提出了增强型Shuffle Mixer (ESM) 来缓解与小型代价体相关的信息损失。ESM通过将主要特征整合到视差上采样单元中来恢复关键细节。它快速从初始视差估计中提取特征并将其与图像特征融合。这些特征通过混洗和层分割进行混合，然后通过一个紧凑的特征引导沙漏网络进行精炼，以恢复更详细的场景几何。ESM专注于具有大感受野和低计算成本的局部上下文连接性，从而实现了高精度视差图的实时重建。ESMStereo的紧凑版本在高端GPU上实现了116 FPS的推理速度，在AGX Orin上实现了91 FPS。", "summary": "本文提出了ESMStereo，一种用于实时高精度立体匹配的模型，旨在解决现有方法在实时性和精度之间的权衡问题。通过引入增强型Shuffle Mixer (ESM) 模块，ESMStereo能够有效弥补小型代价体带来的信息损失，通过特征融合、混洗和精炼恢复关键细节。该方法在保证高精度的同时，实现了显著的实时性能，其紧凑版在高端GPU上可达116 FPS。", "keywords": "立体匹配, 视差上采样, 实时性能, ShuffleMixer, 深度学习", "comments": "ESMStereo的创新点在于其提出的ESM模块，巧妙地解决了小型代价体在实时立体匹配中精度不足的问题。通过有效的特征融合和精炼策略，该模型在保持计算效率的同时，显著提升了视差估计的细节恢复能力。其在嵌入式设备上的出色表现（如AGX Orin上的91 FPS）也凸显了其在实际应用中的巨大潜力，对于自主系统等对实时性要求高的场景具有重要意义。"}}
{"id": "2506.21146", "title": "Linearity-based neural network compression", "authors": ["Silas Dobler", "Florian Lemmerich"], "summary": "In neural network compression, most current methods reduce unnecessary\nparameters by measuring importance and redundancy. To augment already highly\noptimized existing solutions, we propose linearity-based compression as a novel\nway to reduce weights in a neural network. It is based on the intuition that\nwith ReLU-like activation functions, neurons that are almost always activated\nbehave linearly, allowing for merging of subsequent layers. We introduce the\ntheory underlying this compression and evaluate our approach experimentally.\nOur novel method achieves a lossless compression down to 1/4 of the original\nmodel size in over the majority of tested models. Applying our method on\nalready importance-based pruned models shows very little interference between\ndifferent types of compression, demonstrating the option of successful\ncombination of techniques. Overall, our work lays the foundation for a new type\nof compression method that enables smaller and ultimately more efficient neural\nnetwork models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21146v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21146v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于线性度的神经网络压缩", "tldr": "本文提出一种新的基于线性度的神经网络压缩方法，可以在不损失性能的情况下将模型大小压缩到原始的1/4，并且可以与现有压缩技术结合使用。", "motivation": "为了增强已高度优化的现有神经网络压缩解决方案，本文提出了一种新颖的基于线性度的方法来减少神经网络权重。", "method": "本文提出了一种基于线性度的神经网络压缩方法。该方法基于ReLU类激活函数中几乎总是激活的神经元表现出线性的直觉，从而允许合并后续层。论文介绍了其理论基础并进行了实验评估。", "result": "该新方法在大多数测试模型上实现了无损压缩，将模型大小减少到原始的1/4。将该方法应用于已通过基于重要性剪枝的模型时，显示出与其他压缩类型之间很少的干扰，表明可以成功组合多种技术。", "conclusion": "这项工作为一种新型的压缩方法奠定了基础，该方法能够实现更小、最终更高效的神经网络模型。", "translation": "在神经网络压缩领域，当前大多数方法通过衡量重要性和冗余度来减少不必要的参数。为了增强已高度优化的现有解决方案，我们提出了一种基于线性度的压缩方法，作为减少神经网络权重的创新方式。它基于这样一种直觉：对于ReLU类激活函数，几乎总是激活的神经元表现出线性，从而允许合并后续层。我们介绍了这种压缩背后的理论，并对我们的方法进行了实验评估。我们这种新颖的方法在大多数测试模型中实现了无损压缩，将模型大小减小到原始的1/4。将我们的方法应用于已通过基于重要性剪枝的模型时，显示出不同类型的压缩之间干扰很小，这表明成功结合多种技术的可能性。总的来说，我们的工作为一种新型的压缩方法奠定了基础，该方法能够实现更小、最终更高效的神经网络模型。", "summary": "本文提出一种新颖的基于线性度的神经网络压缩方法，旨在通过利用ReLU类激活函数中几乎总是激活的神经元的线性特性来合并后续层，从而有效减少模型权重。实验证明，该方法在大多数模型上能实现高达1/4的无损压缩，并且可以与现有的基于重要性的剪枝方法兼容并成功结合，为构建更小、更高效的神经网络模型提供了新途径。", "keywords": "神经网络压缩, 线性度, 模型剪枝, ReLU, 模型合并", "comments": "这项工作提出了一种创新的神经网络压缩视角，即利用神经元的线性行为进行层合并，这与传统的基于重要性/冗余度的剪枝方法不同。其创新点在于从神经元激活的内在特性出发，提供了一种新的压缩机制。该方法实现了显著的无损压缩比例，并且能够与其他压缩技术兼容，这极大地增强了其在实际应用中的潜力。它为未来神经网络模型的小型化和效率提升提供了新的研究方向。"}}
{"id": "2506.21101", "title": "OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "AndyPian Wu", "Chaoyang Wang", "Chengjie Wang", "Taisong Jin", "SevenShu", "Yunsheng Wu", "Yongge Liu", "Rongrong Ji"], "summary": "As one of the earliest ancient languages, Oracle Bone Script (OBS)\nencapsulates the cultural records and intellectual expressions of ancient\ncivilizations. Despite the discovery of approximately 4,500 OBS characters,\nonly about 1,600 have been deciphered. The remaining undeciphered ones, with\ntheir complex structure and abstract imagery, pose significant challenges for\ninterpretation. To address these challenges, this paper proposes a novel\ntwo-stage semantic typography framework, named OracleFusion. In the first\nstage, this approach leverages the Multimodal Large Language Model (MLLM) with\nenhanced Spatial Awareness Reasoning (SAR) to analyze the glyph structure of\nthe OBS character and perform visual localization of key components. In the\nsecond stage, we introduce Oracle Structural Vector Fusion (OSVF),\nincorporating glyph structure constraints and glyph maintenance constraints to\nensure the accurate generation of semantically enriched vector fonts. This\napproach preserves the objective integrity of the glyph structure, offering\nvisually enhanced representations that assist experts in deciphering OBS.\nExtensive qualitative and quantitative experiments demonstrate that\nOracleFusion outperforms state-of-the-art baseline models in terms of\nsemantics, visual appeal, and glyph maintenance, significantly enhancing both\nreadability and aesthetic quality. Furthermore, OracleFusion provides\nexpert-like insights on unseen oracle characters, making it a valuable tool for\nadvancing the decipherment of OBS.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21101v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21101v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "OracleFusion：通过结构约束语义排版辅助甲骨文破译", "tldr": "OracleFusion利用MLLM和结构约束语义排版辅助甲骨文破译，超越现有模型并提供专家级见解。", "motivation": "甲骨文作为最早的古老语言之一，约4500个字符中仅有1600个被破译。未破译的字符结构复杂、图像抽象，给解读带来巨大挑战。", "method": "本文提出了一种名为OracleFusion的新颖两阶段语义排版框架。第一阶段利用增强空间感知推理（SAR）的多模态大语言模型（MLLM）分析甲骨文字形结构并进行关键组件的视觉定位。第二阶段引入甲骨文结构向量融合（OSVF），结合字形结构约束和字形维护约束，确保准确生成语义丰富的矢量字体。", "result": "广泛的定性和定量实验表明，OracleFusion在语义、视觉吸引力和字形维护方面均优于最先进的基线模型，显著提升了可读性和美学质量。此外，OracleFusion能为未见的甲骨文字符提供专家般的见解。", "conclusion": "OracleFusion是一种有价值的工具，可推动甲骨文的破译工作。", "translation": "甲骨文作为最早的古老语言之一，承载着古代文明的文化记录和思想表达。尽管已发现约4500个甲骨文字符，但只有约1600个被破译。其余未破译的字符，其复杂的结构和抽象的图像，给解读带来了巨大的挑战。为了应对这些挑战，本文提出了一种新颖的两阶段语义排版框架，名为OracleFusion。在第一阶段，该方法利用增强空间感知推理（SAR）的多模态大语言模型（MLLM）分析甲骨文字形结构并进行关键组件的视觉定位。在第二阶段，我们引入甲骨文结构向量融合（OSVF），结合字形结构约束和字形维护约束，以确保准确生成语义丰富的矢量字体。这种方法保持了字形结构的客观完整性，提供了视觉增强的表示，辅助专家破译甲骨文。广泛的定性和定量实验表明，OracleFusion在语义、视觉吸引力和字形维护方面均优于最先进的基线模型，显著提升了可读性和美学质量。此外，OracleFusion能为未见的甲骨文字符提供专家般的见解，使其成为推动甲骨文破译工作的宝贵工具。", "summary": "本文提出OracleFusion，一个两阶段语义排版框架，旨在辅助甲骨文的破译。它首先利用MLLM进行字形结构分析和组件定位，然后通过OSVF生成结构约束的语义矢量字体。实验证明，OracleFusion在语义、视觉和字形维护方面优于现有模型，显著提高了可读性和美学质量，并能为未见字符提供专家级见解，是推动甲骨文破译的宝贵工具。", "keywords": "甲骨文破译, 语义排版, 多模态大语言模型, 结构约束, 矢量字体", "comments": "该论文提出了一种创新的结合多模态大语言模型与结构约束语义排版的方法来辅助甲骨文破译，具有重要的文化和历史意义。其亮点在于引入了结构约束以保持字形完整性，并能为专家提供未见字符的见解，这对于古文字研究领域是一个重大进步。"}}
{"id": "2506.21328", "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts", "authors": ["Jiajie Yang"], "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing.", "comment": "15 pages,4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21328v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21328v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "潜在原型路由：在专家混合模型中实现近乎完美的负载均衡", "tldr": "本文提出潜在原型路由（LPR）框架，通过集群视角解决专家混合（MoE）模型中严重的专家负载不平衡问题，实现了近乎完美的负载均衡，同时不影响性能。", "motivation": "当前专家混合（MoE）系统存在严重的负载不平衡问题，导致模型容量和计算资源大量未被利用。只有一小部分专家在训练和推理过程中持续被激活。", "method": "我们从聚类视角重新审视专家路由，并提出了潜在原型路由（LPR），这是一个新颖的路由框架，它在推广现有方法的同时，促进了平衡的专家利用，且不损害下游性能。", "result": "在多个开源MoE模型（包括DeepSeek-V3、Qwen3-MoE和Mixtral）上的广泛实验表明，LPR将专家负载的Gini系数平均从0.70降低到0.035，将最小-最大专家负载比从1e-6提高到0.70，实现了近乎完美的负载均衡。", "conclusion": "潜在原型路由（LPR）通过从聚类视角重新设计专家路由，显著解决了MoE模型中的负载不平衡问题，实现了卓越的资源利用率，同时保持了模型性能。", "translation": "专家混合（MoE）架构已成为高效扩展大型语言模型（LLMs）的关键策略。然而，当前的MoE系统面临严重的负载不平衡问题，即在训练和推理过程中，只有一小部分专家持续被激活，导致模型容量和计算资源的大量未被充分利用。在这项工作中，我们从聚类的角度重新审视了专家路由，并提出了潜在原型路由（LPR），这是一种新颖的路由框架，它概括了现有方法，同时促进了平衡的专家利用，而不会损害下游性能。在多个开源MoE模型（包括DeepSeek-V3、Qwen3-MoE和Mixtral）上的大量实验表明，LPR平均将专家负载的Gini系数从0.70降低到0.035，并将最小-最大专家负载比从1e-6提高到0.70，实现了近乎完美的负载均衡。", "summary": "本文提出了一种名为潜在原型路由（LPR）的新型专家混合（MoE）模型路由框架，旨在解决现有MoE架构中严重的专家负载不平衡问题。LPR从聚类视角出发，通过推广现有方法，有效地促进了专家资源的平衡利用，同时不影响模型性能。实验结果表明，LPR在多种MoE模型上显著降低了专家负载的Gini系数，并大幅提升了最小-最大专家负载比，实现了近乎完美的负载均衡，从而提高了计算资源的利用率。", "keywords": "专家混合, 负载均衡, 路由, 潜在原型, 大型语言模型", "comments": "该论文通过引入潜在原型路由（LPR），为MoE模型中的负载均衡问题提供了一个创新且有效的解决方案。其将专家路由与聚类思想相结合的视角是新颖的，并且通过量化指标（Gini系数和最小-最大比率）证明了其方法的显著优势，对提高大型语言模型训练和推理的效率具有重要意义。该方法在不损害性能的前提下，极大地提升了资源利用率，具有很高的实用价值。"}}
{"id": "2506.21158", "title": "Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design", "authors": ["Hampus Gummesson Svensson", "Ola Engkvist", "Jon Paul Janet", "Christian Tyrchan", "Morteza Haghir Chehreghani"], "summary": "In many real-world applications, evaluating the goodness of instances is\noften costly and time-consuming, e.g., human feedback and physics simulations,\nin contrast to proposing new instances. In particular, this is even more\ncritical in reinforcement learning, as new interactions with the environment\n(i.e., new instances) need to be evaluated to provide a reward signal to learn\nfrom. As sufficient exploration is crucial, learning from a diverse mini-batch\ncan have a large impact and help mitigate mode collapse. In this paper, we\nintroduce diverse mini-batch selection for reinforcement learning and propose\nto use determinantal point processes for this task. We study this framework in\nthe context of a real-world problem, namely drug discovery. We experimentally\nstudy how our proposed framework can improve the effectiveness of chemical\nexploration in de novo drug design, where finding diverse and high-quality\nsolutions is essential. We conduct a comprehensive evaluation with three\nwell-established molecular generation oracles over numerous generative steps.\nOur experiments conclude that our diverse mini-batch selection framework can\nsubstantially improve the diversity of the solutions, while still obtaining\nsolutions of high quality. In drug discovery, such outcome can potentially lead\nto fulfilling unmet medication needs faster.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21158v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21158v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "强化学习中用于从头药物设计中高效化学探索的多样化小批量选择", "tldr": "本文提出了一种在强化学习中使用行列式点过程进行多样化小批量选择的方法，以提高从头药物设计中的化学探索效率，实验证明能显著提升解决方案的多样性并保持高质量。", "motivation": "在许多实际应用中，评估实例的质量通常成本高昂且耗时，特别是在强化学习中，新的环境交互需要被评估以提供奖励信号。为了有效探索并缓解模式崩溃，从多样化的小批量中学习至关重要。", "method": "本文引入了用于强化学习的多样化小批量选择，并提出使用行列式点过程来完成此任务。该框架在药物发现的真实世界问题中进行了研究。", "result": "实验结果表明，所提出的多样化小批量选择框架可以显著提高解决方案的多样性，同时仍能获得高质量的解决方案。", "conclusion": "我们的多样化小批量选择框架能够显著提高解决方案的多样性，同时保持高品质。在药物发现中，这样的结果可能潜在地更快地满足未满足的药物需求。", "translation": "在许多实际应用中，评估实例的质量通常成本高昂且耗时，例如人工反馈和物理模拟，这与提出新实例形成对比。特别是，这在强化学习中更为关键，因为需要评估与环境的新交互（即新实例）以提供学习所需的奖励信号。由于充分探索至关重要，从多样化的小批量中学习可以产生巨大影响并有助于缓解模式崩溃。在本文中，我们引入了用于强化学习的多样化小批量选择，并提出使用行列式点过程来完成此任务。我们在真实世界问题（即药物发现）的背景下研究了这个框架。我们通过实验研究了我们提出的框架如何提高从头药物设计中化学探索的有效性，其中找到多样化和高质量的解决方案至关重要。我们使用三个成熟的分子生成预言机进行了全面的评估，涉及大量的生成步骤。我们的实验得出结论，我们的多样化小批量选择框架可以显著提高解决方案的多样性，同时仍能获得高质量的解决方案。在药物发现中，这样的结果可能潜在地更快地满足未满足的药物需求。", "summary": "本研究提出了一种在强化学习中进行多样化小批量选择的新框架，旨在解决实例评估成本高昂和模式崩溃的问题。该方法利用行列式点过程来确保小批量的多样性，并将其应用于从头药物设计中的化学探索。通过与现有分子生成模型的广泛实验表明，该框架能显著提高生成解决方案的多样性，同时保持高质量，从而加速药物发现过程。", "keywords": "强化学习, 多样化小批量选择, 行列式点过程, 从头药物设计, 化学探索", "comments": "这项工作通过引入行列式点过程进行多样化小批量选择，为强化学习中的高效探索提供了一个新颖的视角，尤其是在高成本评估领域如药物设计中。其创新性在于将多样性纳入学习过程，有效缓解了模式崩溃问题，并有望加速新药的发现。该方法在实际应用中具有重要价值。"}}
{"id": "2506.21109", "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection", "authors": ["Luosheng Xu", "Dalin Zhang", "Zhaohui Song"], "summary": "Remote sensing change detection is essential for monitoring urban expansion,\ndisaster assessment, and resource management, offering timely, accurate, and\nlarge-scale insights into dynamic landscape transformations. While deep\nlearning has revolutionized change detection, the increasing complexity and\ncomputational demands of modern models have not necessarily translated into\nsignificant accuracy gains. Instead of following this trend, this study\nexplores a more efficient approach, focusing on lightweight models that\nmaintain high accuracy while minimizing resource consumption, which is an\nessential requirement for on-satellite processing. To this end, we propose\nFlickCD, which means quick flick then get great results, pushing the boundaries\nof the performance-resource trade-off. FlickCD introduces an Enhanced\nDifference Module (EDM) to amplify critical feature differences between\ntemporal phases while suppressing irrelevant variations such as lighting and\nweather changes, thereby reducing computational costs in the subsequent change\ndecoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion\nBlocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global\nSelf-Attention (EGSA) to efficiently capture semantic information at multiple\nscales, preserving both coarse- and fine-grained changes. Extensive experiments\non four benchmark datasets demonstrate that FlickCD reduces computational and\nstorage overheads by more than an order of magnitude while achieving\nstate-of-the-art (SOTA) performance or incurring only a minor (<1\\% F1)\naccuracy trade-off. The implementation code is publicly available at\nhttps://github.com/xulsh8/FlickCD.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.21109v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21109v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "突破权衡界限：紧凑而有效的遥感变化检测", "tldr": "FlickCD是一种新的轻量级遥感变化检测模型，在大幅降低计算和存储开销的同时，保持了最先进的性能。", "motivation": "尽管深度学习彻底改变了变化检测，但现代模型日益增长的复杂性和计算需求并未带来显著的精度提升。本研究旨在探索一种更高效的方法，专注于轻量级模型，以在最小化资源消耗的同时保持高精度，这对于卫星处理至关重要。", "method": "本研究提出了FlickCD模型，它引入了增强差异模块（EDM）来放大时间相位间的关键特征差异，同时抑制光照和天气变化等无关变异，从而降低后续变化解码器的计算成本。此外，FlickCD解码器结合了局部-全局融合块，利用移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）来有效捕获多尺度语义信息，保留粗粒度和细粒度的变化。", "result": "在四个基准数据集上的大量实验表明，FlickCD将计算和存储开销降低了一个数量级以上，同时实现了最先进（SOTA）的性能或仅带来微小的（<1% F1）精度权衡。", "conclusion": "FlickCD通过在大幅降低资源消耗的同时保持高精度，成功突破了遥感变化检测领域的性能-资源权衡界限，特别适合星载处理。", "translation": "遥感变化检测对于监测城市扩张、灾害评估和资源管理至关重要，它能为动态景观转变提供及时、准确和大规模的洞察。尽管深度学习彻底改变了变化检测，但现代模型日益增长的复杂性和计算需求并未必然转化为显著的精度提升。本研究并未追随这一趋势，而是探索了一种更高效的方法，专注于轻量级模型，以在最小化资源消耗的同时保持高精度，这对于卫星处理是必不可少的。为此，我们提出了FlickCD，意为“快速一瞥即可获得出色结果”，它突破了性能-资源权衡的界限。FlickCD引入了一个增强差异模块（EDM）来放大时间相位间的关键特征差异，同时抑制光照和天气变化等无关变异，从而降低后续变化解码器的计算成本。此外，FlickCD解码器结合了局部-全局融合块，利用移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）来有效捕获多尺度语义信息，保留粗粒度和细粒度的变化。在四个基准数据集上的大量实验表明，FlickCD将计算和存储开销降低了一个数量级以上，同时实现了最先进（SOTA）的性能或仅带来微小的（<1% F1）精度权衡。实现代码已在https://github.com/xulsh8/FlickCD 公开。", "summary": "该研究提出了一种名为FlickCD的轻量级遥感变化检测模型，旨在解决现有深度学习模型计算复杂性高但精度提升有限的问题。FlickCD通过引入增强差异模块（EDM）来有效识别关键特征差异，并利用包含移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）的局部-全局融合块来捕获多尺度语义信息。实验结果表明，FlickCD在大幅降低计算和存储开销的同时，实现了与最先进模型相当的性能。", "keywords": "遥感变化检测, 轻量级模型, 性能-资源权衡, FlickCD, 深度学习", "comments": "该论文的创新点在于其FlickCD模型在遥感变化检测领域对性能-资源权衡的突破。通过设计轻量级但高效的架构，FlickCD显著降低了计算和存储成本，同时保持了高精度，这对于资源受限的星载处理应用具有重要意义。EDM和局部-全局融合块的设计是其成功的关键。"}}
{"id": "2506.21116", "title": "IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes", "authors": ["Yujia Liang", "Jile Jiao", "Zhicheng Wang", "Xuetao Feng", "Zixuan Ye", "Yuan Wang", "Hao Lu"], "summary": "Video Large Language Models (VideoLLMs) have demonstrated remarkable\nunderstanding capabilities, but are found struggling to tackle multi-shot\nscenarios,e.g., video clips with varying camera angles or scene changes. This\nchallenge can render failures such as instance identity forgetting and key\nframe negligence. In this work, we first attribute the challenge to the lack of\nmulti-shot annotations among existing datasets and therefore we introduce a new\ndataset termed MultiClip-Bench, featuring dense descriptions and\ninstruction-based question-answering pairs tailored for multi-shot scenarios.\nWe empirically find that the training set significantly boosts the multi-shot\nperformance, while the testing benchmark provides a reliable measure of the\nmodel capability in multi-shot scenarios. By further analyzing and discovering\nthat current models only encode instance features in a discrete or lossy\nmanner, at the risk of missing identity information, we then contribute a new\nmodel IPFormer-VideoLLM. Its key idea is the injection of instance-level\nfeatures as instance prompts through an efficient attention-based connector.\nThis allows for the aggregation of instance-specific information across scenes.\nExperiments demonstrate that our proposed dataset and model not only enhance\nthe multi-scene video understanding significantly, but also offer distinct\nadvantages across various video benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21116v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21116v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "IPFormer-VideoLLM：增强多镜头场景下的多模态视频理解", "tldr": "IPFormer-VideoLLM 引入了新的 MultiClip-Bench 数据集和 IPFormer-VideoLLM 模型，通过实例级特征注入，显著提升了视频大语言模型在多镜头场景下的视频理解能力，解决了现有模型在处理复杂场景时实例识别遗忘和关键帧忽视的问题。", "motivation": "现有的视频大语言模型在处理多镜头场景（如包含不同摄像机角度或场景变化的视频片段）时表现不佳，导致实例身份遗忘和关键帧忽视等问题。本文认为这是由于现有数据集中缺乏多镜头标注造成的，并且当前模型编码实例特征的方式是离散或有损的，存在丢失身份信息的风险。", "method": "1. 引入了一个新的数据集 MultiClip-Bench，该数据集具有密集的描述和针对多镜头场景的基于指令的问题-回答对。2. 提出了一个新的模型 IPFormer-VideoLLM，其核心思想是通过一个高效的基于注意力的连接器，将实例级特征作为实例提示注入，从而聚合跨场景的实例特定信息。", "result": "实验表明，所提出的数据集和模型不仅显著增强了多场景视频理解能力，而且在各种视频基准测试中也展现出明显的优势。训练集显著提升了多镜头性能，测试基准提供了可靠的模型能力衡量标准。", "conclusion": "IPFormer-VideoLLM 通过引入专门的数据集 MultiClip-Bench 和创新的模型架构，有效解决了视频大语言模型在多镜头场景理解中的挑战，显著提升了模型的性能和鲁棒性。", "translation": "视频大语言模型（VideoLLMs）展现了卓越的理解能力，但在处理多镜头场景时（例如，包含不同摄像机角度或场景变化的视频片段）却发现它们力不从心。这一挑战可能导致实例身份遗忘和关键帧忽视等失败。在这项工作中，我们首先将这一挑战归因于现有数据集中缺乏多镜头标注，因此我们引入了一个名为 MultiClip-Bench 的新数据集，该数据集具有密集的描述和专为多镜头场景量身定制的基于指令的问题-回答对。我们凭经验发现，训练集显著提升了多镜头性能，而测试基准则提供了衡量模型在多镜头场景中能力的可靠方法。通过进一步分析和发现当前模型仅以离散或有损的方式编码实例特征，存在丢失身份信息的风险，我们随后贡献了一个新模型 IPFormer-VideoLLM。其核心思想是通过一个高效的基于注意力的连接器，将实例级特征作为实例提示注入。这允许聚合跨场景的实例特定信息。实验表明，我们提出的数据集和模型不仅显著增强了多场景视频理解能力，而且在各种视频基准测试中也提供了明显的优势。", "summary": "本文针对视频大语言模型在多镜头场景理解中面临的挑战，提出了两项关键贡献：首先，引入了 MultiClip-Bench 数据集，该数据集专门为多镜头场景设计，包含丰富的描述和问答对，旨在弥补现有数据集中多镜头标注的不足。其次，提出了 IPFormer-VideoLLM 模型，该模型通过一个注意力连接器将实例级特征注入为实例提示，有效聚合跨场景的实例信息，解决了现有模型在处理多镜头时实例识别困难和信息丢失的问题。实验证明，该数据集和模型显著提升了多场景视频理解能力，并在多个视频基准上表现出色。", "keywords": "视频大语言模型, 多镜头理解, 数据集, 实例级特征, IPFormer-VideoLLM", "comments": "这项工作在解决视频大语言模型处理复杂多镜头场景时的核心挑战方面具有创新性。通过引入专门的数据集 MultiClip-Bench，它弥补了现有数据标注的空白，为模型学习多镜头理解提供了宝贵资源。同时，IPFormer-VideoLLM 模型通过实例级特征注入的机制，有效解决了实例身份遗忘和信息丢失的问题，这对于提升视频理解的精细度和鲁棒性至关重要。其创新点在于从数据和模型两个层面同时进行改进，形成协同效应。"}}
{"id": "2506.21408", "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference", "authors": ["Colin Samplawski", "Adam D. Cobb", "Manoj Acharya", "Ramneet Kaur", "Susmit Jha"], "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.", "comment": "Accepted at UAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21408v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21408v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过随机变分子空间推理实现大规模语言模型的可扩展贝叶斯低秩适应", "tldr": "本文提出了ScalaBL，一种新的可扩展贝叶斯低秩适应方法，通过在低维子空间中进行推理，显著减少了大型语言模型不确定性量化所需的额外参数，同时保持了竞争力并提高了可扩展性。", "motivation": "大型语言模型（LLMs）普遍存在幻觉和校准不良问题，使得在自动驾驶和医疗保健等高风险领域中，对这些模型进行不确定性量化变得至关重要。然而，现有基于贝叶斯深度学习的方法，尽管通过低秩适应（LoRA）参数使问题更易处理，但由于需要额外的参数，难以扩展到更大的LLMs。", "method": "本文提出了可扩展贝叶斯低秩适应（ScalaBL），通过随机变分子空间推理（Stochastic Variational Subspace Inference）在LoRA秩r的r维子空间中执行贝叶斯推理。该方法通过将LoRA参数重新用作投影矩阵，能够将子空间中的样本映射到LLM的完整权重空间，并利用随机变分推理学习所有参数。", "result": "尽管子空间维度较低，但ScalaBL与最先进的方法实现了竞争性的性能，并且仅需要大约1000个额外的参数。此外，它还能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是先前工作的四倍。", "conclusion": "ScalaBL通过在低维子空间中进行贝叶斯推理，有效地解决了现有贝叶斯LLM方法在扩展性方面的参数限制问题，显著提高了贝叶斯LLM的可扩展性，同时保持了高性能。", "translation": "尽管大型语言模型（LLMs）被广泛使用，但它们已知会产生错误信息并校准不良。这使得这些模型的不确定性量化变得至关重要，尤其是在自主和医疗保健等高风险领域。先前的工作通过对微调模型的低秩适应（LoRA）参数进行推理，使基于贝叶斯深度学习的方法更具可行性。然而，这些方法虽然有效，但由于与LoRA相比需要额外的参数，因此难以扩展到更大的LLMs。在这项工作中，我们提出了通过随机变分子空间推理实现的可扩展贝叶斯低秩适应（ScalaBL）。我们对LoRA秩r的r维子空间进行贝叶斯推理。通过将LoRA参数重新用作投影矩阵，我们能够将该子空间中的样本映射到LLM的完整权重空间。这使我们能够使用随机变分推理学习我们方法的所有参数。尽管我们的子空间维度较低，但我们能够与最先进的方法达到具有竞争力的性能，同时仅需要约1000个额外参数。此外，它使我们能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是先前工作的四倍。", "summary": "本文提出了可扩展贝叶斯低秩适应（ScalaBL），一种针对大型语言模型（LLMs）不确定性量化的新方法。针对现有贝叶斯LLM方法扩展性差的问题，ScalaBL通过在低维子空间中进行贝叶斯推理，并巧妙地利用LoRA参数作为投影矩阵，实现了将子空间样本映射到LLM的全权重空间。实验证明，ScalaBL仅需少量额外参数即可达到与现有先进方法相当的性能，并成功扩展到比以往更大的贝叶斯LLM。", "keywords": "贝叶斯深度学习, 大型语言模型, 不确定性量化, 低秩适应, 随机变分推理", "comments": "这项工作的创新之处在于，它通过在低维子空间中进行推理，并重新利用LoRA参数作为投影，有效地解决了贝叶斯LLM在不确定性量化方面面临的参数扩展性问题。其重要性在于，它使得LLMs在需要高置信度和不确定性量化的高风险应用中（如自动驾驶和医疗）更具实用性。通过显著减少额外参数的需求，ScalaBL为构建更大、更实用的贝叶斯LLM铺平了道路。"}}
{"id": "2506.21117", "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization", "authors": ["Jan Ackermann", "Jonas Kulhanek", "Shengqu Cai", "Haofei Xu", "Marc Pollefeys", "Gordon Wetzstein", "Leonidas Guibas", "Songyou Peng"], "summary": "In dynamic 3D environments, accurately updating scene representations over\ntime is crucial for applications in robotics, mixed reality, and embodied AI.\nAs scenes evolve, efficient methods to incorporate changes are needed to\nmaintain up-to-date, high-quality reconstructions without the computational\noverhead of re-optimizing the entire scene. This paper introduces CL-Splats,\nwhich incrementally updates Gaussian splatting-based 3D representations from\nsparse scene captures. CL-Splats integrates a robust change-detection module\nthat segments updated and static components within the scene, enabling focused,\nlocal optimization that avoids unnecessary re-computation. Moreover, CL-Splats\nsupports storing and recovering previous scene states, facilitating temporal\nsegmentation and new scene-analysis applications. Our extensive experiments\ndemonstrate that CL-Splats achieves efficient updates with improved\nreconstruction quality over the state-of-the-art. This establishes a robust\nfoundation for future real-time adaptation in 3D scene reconstruction tasks.", "comment": "ICCV 2025, Project Page: https://cl-splats.github.io", "pdf_url": "http://arxiv.org/pdf/2506.21117v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21117v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CL-Splats：基于局部优化的持续高斯泼溅学习", "tldr": "CL-Splats通过局部优化和变化检测，高效地持续更新3D高斯泼溅场景表示，无需重新优化整个场景。", "motivation": "在动态3D环境中，实时准确更新场景表示对于机器人、混合现实和具身AI应用至关重要。需要高效的方法来整合变化，以维护高质量的重建，同时避免重新优化整个场景的计算开销。", "method": "本文提出了CL-Splats，它从稀疏场景捕获中增量更新基于高斯泼溅的3D表示。CL-Splats集成了一个鲁棒的变化检测模块，该模块能分割场景中更新和静态的组件，从而实现集中的局部优化，避免不必要的重复计算。此外，CL-Splats支持存储和恢复先前的场景状态，有助于时间分割和新的场景分析应用。", "result": "广泛的实验表明，CL-Splats实现了高效的更新，并且重建质量优于现有技术。", "conclusion": "CL-Splats为未来3D场景重建任务中的实时适应性奠定了坚实的基础。", "translation": "在动态3D环境中，随时间准确更新场景表示对于机器人、混合现实和具身AI应用至关重要。随着场景的演变，需要高效的方法来整合变化，以维护最新的高质量重建，同时避免重新优化整个场景的计算开销。本文介绍了CL-Splats，它从稀疏场景捕获中增量更新基于高斯泼溅的3D表示。CL-Splats集成了一个鲁棒的变化检测模块，该模块能分割场景中更新和静态的组件，从而实现集中的局部优化，避免不必要的重复计算。此外，CL-Splats支持存储和恢复先前的场景状态，有助于时间分割和新的场景分析应用。我们广泛的实验表明，CL-Splats实现了高效的更新，并且重建质量优于现有技术。这为未来3D场景重建任务中的实时适应性奠定了坚实的基础。", "summary": "CL-Splats是一种用于动态3D环境的持续学习方法，它通过增量更新高斯泼溅表示来高效地维护高质量的场景重建。该方法采用变化检测模块进行局部优化，避免全场景重新计算，并支持场景状态的存储和恢复。实验证明其更新效率和重建质量均优于现有技术，为实时3D场景适应性提供了基础。", "keywords": "持续学习, 高斯泼溅, 3D重建, 局部优化, 变化检测", "comments": "这篇论文通过引入局部优化和变化检测模块，解决了动态3D场景中高斯泼溅表示的持续更新效率问题，避免了传统方法中全场景重新优化的巨大计算开销。其创新点在于对场景变化进行精细化管理，并支持历史状态恢复，这对于实时应用和场景分析具有重要意义。该方法有望推动3D重建技术在动态环境中的实用化。"}}
{"id": "2506.21240", "title": "Zero-Shot Learning for Obsolescence Risk Forecasting", "authors": ["Elie Saad", "Aya Mrabah", "Mariem Besbes", "Marc Zolghadri", "Victor Czmil", "Claude Baron", "Vincent Bourgeois"], "summary": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21240v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21240v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "零样本学习用于过时风险预测", "tldr": "本文提出一种利用零样本学习（ZSL）和大型语言模型（LLMs）预测电子元件过时风险的方法，以解决数据稀缺问题，并在真实数据集上验证了其有效性。", "motivation": "电子元件过时给相关产业带来成本增加和系统中断等挑战，准确的过时风险预测至关重要，但现有方法受限于可靠数据的缺乏。", "method": "本文提出一种新颖的方法，利用零样本学习（ZSL）和大型语言模型（LLMs）来预测过时风险。该方法通过从表格数据集中利用领域特定知识来解决数据限制。", "result": "该方法在两个真实世界数据集中展示了有效的风险预测能力。对四种LLM的比较评估强调了为特定预测任务选择正确模型的重要性。", "conclusion": "零样本学习结合大型语言模型能够有效预测电子元件过时风险，并且针对具体预测任务选择合适的LLM对模型性能至关重要。", "translation": "电子元件过时给依赖电子元件的行业带来了严峻挑战，导致成本增加并扰乱了系统的安全性与可用性。准确的过时风险预测至关重要，但由于缺乏可靠数据而受到阻碍。本文提出了一种利用零样本学习（ZSL）与大型语言模型（LLMs）预测过时风险的新方法，旨在通过利用表格数据集中的领域特定知识来解决数据限制问题。该方法应用于两个真实世界数据集，展示了有效的风险预测能力。对四种LLM的比较评估强调了为特定预测任务选择正确模型的重要性。", "summary": "本文提出一种新颖的零样本学习（ZSL）结合大型语言模型（LLMs）的方法，用于预测电子元件的过时风险。该方法通过利用表格数据中的领域特定知识来克服数据不足的挑战，并在两个真实世界数据集中验证了其有效性。研究还指出，针对特定预测任务选择合适的LLM至关重要。", "keywords": "过时风险, 零样本学习, 大型语言模型, 预测, 数据限制", "comments": "该研究的创新点在于将零样本学习和大型语言模型应用于过时风险预测这一数据稀缺的领域，提供了一种解决实际工业挑战的新途径。其重要性在于能够帮助企业降低成本、保障系统安全与可用性。"}}
{"id": "2506.21474", "title": "Logios : An open source Greek Polytonic Optical Character Recognition system", "authors": ["Perifanos Konstantinos", "Goutsos Dionisis"], "summary": "In this paper, we present an Optical Character Recognition (OCR) system\nspecifically designed for the accurate recognition and digitization of Greek\npolytonic texts. By leveraging the combined strengths of convolutional layers\nfor feature extraction and recurrent layers for sequence learning, our system\naddresses the unique challenges posed by Greek polytonic scripts. This approach\naims to overcome the limitations of traditional OCR methods, offering\nsignificant improvements in accuracy and efficiency. We release the underlying\nmodel as an open-source library and make our OCR platform available for\nacademic use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21474v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21474v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Logios：一个开源的希腊语多音素光学字符识别系统", "tldr": "Logios是一个开源的希腊语多音素OCR系统，结合卷积层和循环层，旨在提高希腊语多音素文本的识别准确性和效率。", "motivation": "传统的OCR方法在处理希腊语多音素文本时存在局限性，本系统旨在解决希腊语多音素文本的准确识别和数字化所带来的独特挑战。", "method": "该系统结合了卷积层进行特征提取和循环层进行序列学习，以处理希腊语多音素文字的独特挑战。", "result": "该系统显著提高了识别准确性和效率。研究者发布了底层模型作为开源库，并提供了OCR平台供学术使用。", "conclusion": "Logios系统通过结合卷积层和循环层，有效克服了传统OCR方法的局限性，在希腊语多音素文本的准确识别和数字化方面取得了显著改进。", "translation": "本文介绍了一个专门为希腊语多音素文本的准确识别和数字化而设计的光学字符识别（OCR）系统。通过利用卷积层进行特征提取和循环层进行序列学习的综合优势，我们的系统解决了希腊语多音素文字所带来的独特挑战。这种方法旨在克服传统OCR方法的局限性，在准确性和效率方面提供了显著改进。我们将底层模型作为一个开源库发布，并使我们的OCR平台可供学术使用。", "summary": "Logios是一个专门为希腊语多音素文本设计的开源OCR系统。它结合了卷积层进行特征提取和循环层进行序列学习，旨在克服传统OCR方法的局限性，显著提高希腊语多音素文本的识别准确性和效率。该系统已作为开源库发布，并提供平台供学术使用。", "keywords": "希腊语多音素, OCR, 光学字符识别, 深度学习, 开源", "comments": "该论文的创新之处在于其专门针对希腊语多音素文字的OCR解决方案，并结合了深度学习中的卷积层和循环层来处理其复杂性。作为开源项目发布，将极大地促进相关领域的研究和应用。"}}
{"id": "2506.21132", "title": "Learning to See in the Extremely Dark", "authors": ["Hai Jiang", "Binhao Guan", "Zhen Liu", "Xiaohong Liu", "Jian Yu", "Zheng Liu", "Songchen Han", "Shuaicheng Liu"], "summary": "Learning-based methods have made promising advances in low-light RAW image\nenhancement, while their capability to extremely dark scenes where the\nenvironmental illuminance drops as low as 0.0001 lux remains to be explored due\nto the lack of corresponding datasets. To this end, we propose a\npaired-to-paired data synthesis pipeline capable of generating well-calibrated\nextremely low-light RAW images at three precise illuminance ranges of 0.01-0.1\nlux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB\nreferences to comprise a large-scale paired dataset named\nSee-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement\napproaches. Furthermore, we propose a diffusion-based framework that leverages\nthe generative ability and intrinsic denoising property of diffusion models to\nrestore visually pleasing results from extremely low-SNR RAW inputs, in which\nan Adaptive Illumination Correction Module (AICM) and a color consistency loss\nare introduced to ensure accurate exposure correction and color restoration.\nExtensive experiments on the proposed SIED and publicly available benchmarks\ndemonstrate the effectiveness of our method. The code and dataset are available\nat https://github.com/JianghaiSCU/SIED.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21132v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21132v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在极暗环境下学习可见", "tldr": "本文提出了一个数据合成管道，用于生成极低光RAW图像数据集SIED，并提出了一种基于扩散模型的框架，通过自适应光照校正模块和颜色一致性损失来增强极低信噪比图像，有效解决了极暗场景下的图像增强问题。", "motivation": "现有的基于学习的方法在极暗场景（照度低至0.0001 lux）下的RAW图像增强能力尚未被充分探索，主要原因是缺乏相应的极暗场景数据集。", "method": "本文提出了一个成对数据合成管道，能够生成三种精确照度范围（0.01-0.1 lux, 0.001-0.01 lux, 0.0001-0.001 lux）的校准良好的极低光RAW图像，并结合高质量sRGB参考图像，构建了一个名为See-in-the-Extremely-Dark (SIED)的大规模成对数据集。此外，本文还提出了一种基于扩散模型的框架，利用扩散模型的生成能力和内在去噪特性，从极低信噪比的RAW输入中恢复视觉上令人满意的结果，该框架引入了自适应光照校正模块（AICM）和颜色一致性损失，以确保准确的曝光校正和颜色恢复。", "result": "在所提出的SIED数据集和公开基准数据集上进行的广泛实验证明了本文方法的有效性。", "conclusion": "本文成功构建了极暗场景数据集SIED并提出了一种有效的基于扩散模型的图像增强框架，解决了极暗环境下图像增强的挑战。", "translation": "基于学习的方法在低光RAW图像增强方面取得了可喜的进展，但由于缺乏相应的数据集，它们在环境照度低至0.0001 lux的极暗场景中的能力仍有待探索。为此，我们提出了一个成对数据合成管道，能够生成三种精确照度范围（0.01-0.1 lux、0.001-0.01 lux和0.0001-0.001 lux）的校准良好的极低光RAW图像，并结合高质量的sRGB参考图像，构成了一个名为See-in-the-Extremely-Dark (SIED)的大规模成对数据集，用于评估低光RAW图像增强方法。此外，我们提出了一种基于扩散模型的框架，该框架利用扩散模型的生成能力和内在去噪特性，从极低信噪比的RAW输入中恢复视觉上令人满意的结果，其中引入了自适应光照校正模块（AICM）和颜色一致性损失，以确保准确的曝光校正和颜色恢复。在所提出的SIED和公开可用基准数据集上进行的广泛实验证明了我们方法的有效性。代码和数据集可在https://github.com/JianghaiSCU/SIED上获取。", "summary": "本文针对极暗场景下（照度低至0.0001 lux）RAW图像增强缺乏数据集和有效方法的问题，提出了一个成对数据合成管道，构建了大规模极低光RAW图像数据集SIED。同时，提出了一种基于扩散模型的框架，该框架通过引入自适应光照校正模块和颜色一致性损失，能够从极低信噪比的RAW输入中恢复高质量图像。实验证明了该方法在SIED和公共基准上的有效性。", "keywords": "极暗图像增强, RAW图像, 数据集合成, 扩散模型, 低光照", "comments": "本文的创新点在于构建了首个针对极暗场景的大规模成对数据集SIED，填补了该领域数据稀缺的空白。同时，将扩散模型应用于极低信噪比的RAW图像增强，并引入了AICM和颜色一致性损失，有效利用了扩散模型的去噪和生成能力，为极暗环境下的图像恢复提供了新的解决方案，具有重要的实用价值。"}}
{"id": "2506.21291", "title": "Improved seeding strategies for k-means and k-GMM", "authors": ["Guillaume Carrière", "Frédéric Cazals"], "summary": "We revisit the randomized seeding techniques for k-means clustering and k-GMM\n(Gaussian Mixture model fitting with Expectation-Maximization), formalizing\ntheir three key ingredients: the metric used for seed sampling, the number of\ncandidate seeds, and the metric used for seed selection. This analysis yields\nnovel families of initialization methods exploiting a lookahead\nprinciple--conditioning the seed selection to an enhanced coherence with the\nfinal metric used to assess the algorithm, and a multipass strategy to tame\ndown the effect of randomization.\n  Experiments show a consistent constant factor improvement over classical\ncontenders in terms of the final metric (SSE for k-means, log-likelihood for\nk-GMM), at a modest overhead. In particular, for k-means, our methods improve\non the recently designed multi-swap strategy, which was the first one to\noutperform the greedy k-means++ seeding.\n  Our experimental analysis also shed light on subtle properties of k-means\noften overlooked, including the (lack of) correlations between the SSE upon\nseeding and the final SSE, the variance reduction phenomena observed in\niterative seeding methods, and the sensitivity of the final SSE to the pool\nsize for greedy methods.\n  Practically, our most effective seeding methods are strong candidates to\nbecome one of the--if not the--standard techniques. From a theoretical\nperspective, our formalization of seeding opens the door to a new line of\nanalytical approaches.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.21291v1", "categories": ["cs.LG", "F.2; G.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21291v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "k-means和k-GMM改进的种子点初始化策略", "tldr": "本文提出并实验验证了k-means和k-GMM新的初始化方法，通过前瞻性原则和多通道策略，在最终度量指标上优于现有方法。", "motivation": "现有k-means和k-GMM随机初始化技术存在改进空间，需要更有效的种子点初始化方法来提高聚类性能。", "method": "重新审视了随机种子点初始化技术，形式化了其三个关键要素：种子采样使用的度量、候选种子点数量和种子点选择使用的度量。在此基础上，提出了利用“前瞻性原则”（根据评估算法的最终度量来调整种子点选择）和“多通道策略”（减少随机性影响）的新型初始化方法。", "result": "实验表明，在最终度量指标（k-means的SSE，k-GMM的对数似然）上，相对于经典竞争者有持续的常数因子改进，开销适中。特别是对于k-means，该方法优于最近设计的multi-swap策略，后者是第一个超越贪婪k-means++初始化的方法。实验分析还揭示了k-means的一些细微特性，包括种子点初始化时的SSE与最终SSE之间的相关性（或缺乏）、迭代种子点方法中观察到的方差减少现象，以及最终SSE对贪婪方法池大小的敏感性。", "conclusion": "作者提出的最有效的种子点初始化方法有望成为标准技术。从理论角度看，种子点初始化的形式化为新的分析方法打开了大门。", "translation": "我们重新审视了k-means聚类和k-GMM（使用期望最大化进行高斯混合模型拟合）的随机种子点初始化技术，并形式化了它们的三个关键要素：用于种子采样的度量、候选种子点的数量以及用于种子点选择的度量。这项分析产生了利用前瞻性原则——使种子点选择与用于评估算法的最终度量保持更高的一致性——和多通道策略来驯服随机性影响的新型初始化方法。\n实验表明，在最终度量指标（k-means的SSE，k-GMM的对数似然）上，相对于经典竞争者，以适度的开销实现了持续的常数因子改进。特别是对于k-means，我们的方法改进了最近设计的multi-swap策略，这是第一个在性能上超越贪婪k-means++初始化的方法。\n我们的实验分析还揭示了k-means经常被忽视的一些细微特性，包括种子点初始化时的SSE与最终SSE之间的相关性（或缺乏）、在迭代种子点方法中观察到的方差减少现象，以及最终SSE对贪婪方法池大小的敏感性。\n实际上，我们最有效的种子点初始化方法是成为标准技术之一（如果不是唯一）的有力候选。从理论角度来看，我们对种子点初始化的形式化为新的分析方法开辟了道路。", "summary": "本文重新审视了k-means和k-GMM的随机种子点初始化技术，并形式化了其关键要素。在此基础上，提出了基于“前瞻性原则”和“多通道策略”的新型初始化方法。实验证明，这些方法在最终聚类质量上显著优于现有技术，并揭示了k-means的一些深层特性。这些方法有望成为新的标准初始化策略，并为理论分析奠定基础。", "keywords": "k-means, k-GMM, 种子点初始化, 聚类, 前瞻性原则", "comments": "本文通过对k-means和k-GMM随机种子点初始化技术的深入分析和形式化，提出了具有创新性的“前瞻性原则”和“多通道策略”。其创新点在于将种子点选择与最终评估指标更紧密地结合，并有效降低了随机性影响。实验结果表明其在性能上的显著提升，尤其是在k-means领域首次超越了k-means++和multi-swap等先进方法，这具有重要的实践意义。此外，对k-means特性的深入洞察也为后续研究提供了宝贵的经验。"}}
{"id": "2506.21135", "title": "YOLO-FDA: Integrating Hierarchical Attention and Detail Enhancement for Surface Defect Detection", "authors": ["Jiawei Hu"], "summary": "Surface defect detection in industrial scenarios is both crucial and\ntechnically demanding due to the wide variability in defect types, irregular\nshapes and sizes, fine-grained requirements, and complex material textures.\nAlthough recent advances in AI-based detectors have improved performance,\nexisting methods often suffer from redundant features, limited detail\nsensitivity, and weak robustness under multiscale conditions. To address these\nchallenges, we propose YOLO-FDA, a novel YOLO-based detection framework that\nintegrates fine-grained detail enhancement and attention-guided feature fusion.\nSpecifically, we adopt a BiFPN-style architecture to strengthen bidirectional\nmultilevel feature aggregation within the YOLOv5 backbone. To better capture\nfine structural changes, we introduce a Detail-directional Fusion Module (DDFM)\nthat introduces a directional asymmetric convolution in the second-lowest layer\nto enrich spatial details and fuses the second-lowest layer with low-level\nfeatures to enhance semantic consistency. Furthermore, we propose two novel\nattention-based fusion strategies, Attention-weighted Concatenation (AC) and\nCross-layer Attention Fusion (CAF) to improve contextual representation and\nreduce feature noise. Extensive experiments on benchmark datasets demonstrate\nthat YOLO-FDA consistently outperforms existing state-of-the-art methods in\nterms of both accuracy and robustness across diverse types of defects and\nscales.", "comment": "14 pages, 6 figures. Submitted to The 8th Chinese Conference on\n  Pattern Recognition and Computer Vision", "pdf_url": "http://arxiv.org/pdf/2506.21135v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21135v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "YOLO-FDA：融合分层注意力与细节增强的表面缺陷检测", "tldr": "YOLO-FDA是一种新的基于YOLO的表面缺陷检测框架，通过细节增强和注意力引导的特征融合，解决了现有方法在多尺度条件下的冗余特征、细节敏感性不足和鲁棒性差的问题，并在基准数据集上表现出SOTA性能。", "motivation": "工业场景中的表面缺陷检测具有挑战性，因为缺陷类型多变、形状不规则、尺寸各异、要求精细且材料纹理复杂。尽管AI检测器有所改进，但现有方法常存在冗余特征、细节敏感性有限以及多尺度条件下鲁棒性差的问题。", "method": "本文提出了YOLO-FDA框架，一个基于YOLO的检测框架，其核心在于集成细粒度细节增强和注意力引导的特征融合。具体方法包括：1. 采用BiFPN风格的架构，加强YOLOv5骨干网络内的双向多级特征聚合。2. 引入细节方向融合模块（DDFM），在倒数第二层引入方向性不对称卷积以丰富空间细节，并将倒数第二层与低级特征融合以增强语义一致性。3. 提出两种新颖的基于注意力的融合策略：注意力加权拼接（AC）和跨层注意力融合（CAF），以改善上下文表示并减少特征噪声。", "result": "在基准数据集上的大量实验表明，YOLO-FDA在各种缺陷类型和尺度上，在准确性和鲁棒性方面始终优于现有的最先进方法。", "conclusion": "YOLO-FDA通过其创新性的细节增强和注意力引导特征融合策略，有效解决了工业表面缺陷检测的挑战，并在性能上超越了现有SOTA方法。", "translation": "工业场景中的表面缺陷检测既关键又具技术挑战性，因为缺陷类型广泛多样、形状和尺寸不规则、要求精细且材料纹理复杂。尽管近期基于AI的检测器取得了进展，但现有方法常受限于冗余特征、细节敏感性不足以及多尺度条件下的鲁棒性差。为解决这些挑战，我们提出了YOLO-FDA，一种新颖的基于YOLO的检测框架，它集成了细粒度细节增强和注意力引导的特征融合。具体而言，我们采用BiFPN风格的架构来加强YOLOv5骨干网络内的双向多级特征聚合。为了更好地捕获精细结构变化，我们引入了一个细节方向融合模块（DDFM），该模块在倒数第二层引入方向性不对称卷积以丰富空间细节，并将倒数第二层与低级特征融合以增强语义一致性。此外，我们提出了两种新颖的基于注意力的融合策略，即注意力加权拼接（AC）和跨层注意力融合（CAF），以改善上下文表示并减少特征噪声。在基准数据集上的大量实验表明，YOLO-FDA在各种缺陷类型和尺度上，在准确性和鲁棒性方面始终优于现有的最先进方法。", "summary": "本文提出了YOLO-FDA，一个基于YOLO的表面缺陷检测新框架，旨在解决工业场景中缺陷检测的挑战，如冗余特征、细节敏感性不足和多尺度鲁棒性差。YOLO-FDA通过集成BiFPN风格的双向特征聚合、引入细节方向融合模块（DDFM）以增强细节捕捉，以及提出注意力加权拼接（AC）和跨层注意力融合（CAF）两种注意力融合策略来优化特征表示。实验结果表明，YOLO-FDA在准确性和鲁棒性方面均超越了现有SOTA方法。", "keywords": "表面缺陷检测, YOLO, 细节增强, 注意力机制, 特征融合", "comments": "YOLO-FDA的创新点在于其结合了细节增强模块（DDFM）和多种注意力机制（AC, CAF），以优化YOLOv5骨干网络的特征表示，特别针对工业缺陷检测中对精细细节捕捉和多尺度鲁棒性的高要求。BiFPN风格的集成也进一步强化了特征聚合能力，使其在复杂工业场景中表现出卓越性能。"}}
{"id": "2506.21546", "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation", "authors": ["Xinzhuo Li", "Adheesh Juvekar", "Xingyou Liu", "Muntasir Wahed", "Kiet A. Nguyen", "Ismini Lourentzou"], "summary": "Recent progress in vision-language segmentation has significantly advanced\ngrounded visual understanding. However, these models often exhibit\nhallucinations by producing segmentation masks for objects not grounded in the\nimage content or by incorrectly labeling irrelevant regions. Existing\nevaluation protocols for segmentation hallucination primarily focus on label or\ntextual hallucinations without manipulating the visual context, limiting their\ncapacity to diagnose critical failures. In response, we introduce\nHalluSegBench, the first benchmark specifically designed to evaluate\nhallucinations in visual grounding through the lens of counterfactual visual\nreasoning. Our benchmark consists of a novel dataset of 1340 counterfactual\ninstance pairs spanning 281 unique object classes, and a set of newly\nintroduced metrics that quantify hallucination sensitivity under visually\ncoherent scene edits. Experiments on HalluSegBench with state-of-the-art\nvision-language segmentation models reveal that vision-driven hallucinations\nare significantly more prevalent than label-driven ones, with models often\npersisting in false segmentation, highlighting the need for counterfactual\nreasoning to diagnose grounding fidelity.", "comment": "Project webpage: https://plan-lab.github.io/hallusegbench/", "pdf_url": "http://arxiv.org/pdf/2506.21546v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21546v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HalluSegBench：用于分割幻觉评估的反事实视觉推理", "tldr": "提出HalluSegBench，首个用于评估视觉-语言分割模型幻觉的基准，通过反事实视觉推理揭示视觉驱动幻觉比标签驱动更普遍。", "motivation": "现有视觉-语言分割模型经常产生幻觉，即为图像中不存在的对象生成分割掩码或错误标记无关区域。现有评估协议主要关注标签或文本幻觉，但未操纵视觉上下文，限制了诊断关键故障的能力。", "method": "引入HalluSegBench，这是第一个专门设计用于通过反事实视觉推理评估视觉基础中幻觉的基准。该基准包含一个由1340对反事实实例（涵盖281个独特对象类别）组成的新数据集，以及一套量化视觉连贯场景编辑下幻觉敏感度的新指标。", "result": "在HalluSegBench上对最先进的视觉-语言分割模型进行的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型通常会持续错误的分割。", "conclusion": "强调了需要反事实推理来诊断视觉基础的保真度。", "translation": "视觉-语言分割的最新进展显著推动了扎根视觉理解。然而，这些模型经常通过为不基于图像内容的对象生成分割掩码或错误标记无关区域来表现出幻觉。现有的分割幻觉评估协议主要关注标签或文本幻觉，而没有操纵视觉上下文，这限制了它们诊断关键故障的能力。为此，我们引入了HalluSegBench，这是第一个专门设计用于通过反事实视觉推理来评估视觉基础中幻觉的基准。我们的基准包含一个由1340对反事实实例组成的新数据集，涵盖281个独特的对象类别，以及一套新引入的指标，用于量化在视觉连贯场景编辑下的幻觉敏感度。在HalluSegBench上对最先进的视觉-语言分割模型进行的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型通常会持续错误的分割，这突出表明需要反事实推理来诊断基础的保真度。", "summary": "本文介绍了HalluSegBench，一个用于评估视觉-语言分割模型中视觉基础幻觉的首个基准。针对现有评估方法无法充分诊断视觉幻觉的问题，HalluSegBench利用反事实视觉推理，构建了一个包含1340对反事实实例的新数据集和一套新指标。实验结果显示，视觉驱动的幻觉比标签驱动的幻觉更普遍，模型常出现持续性错误分割，突显了反事实推理在诊断视觉基础保真度方面的重要性。", "keywords": "分割幻觉, 反事实推理, 视觉-语言分割, 基准测试, HalluSegBench", "comments": "该论文的创新点在于首次引入反事实视觉推理来评估视觉-语言分割中的幻觉，填补了现有评估协议的空白。其重要性在于揭示了视觉驱动幻觉的普遍性，并提出了更有效的诊断工具，对未来鲁棒视觉-语言模型的发展具有指导意义。"}}
{"id": "2506.21150", "title": "Tree-based Semantic Losses: Application to Sparsely-supervised Large Multi-class Hyperspectral Segmentation", "authors": ["Junwen Wang", "Oscar Maccormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "summary": "Hyperspectral imaging (HSI) shows great promise for surgical applications,\noffering detailed insights into biological tissue differences beyond what the\nnaked eye can perceive. Refined labelling efforts are underway to train vision\nsystems to distinguish large numbers of subtly varying classes. However,\ncommonly used learning methods for biomedical segmentation tasks penalise all\nerrors equivalently and thus fail to exploit any inter-class semantics in the\nlabel space. In this work, we introduce two tree-based semantic loss functions\nwhich take advantage of a hierarchical organisation of the labels. We further\nincorporate our losses in a recently proposed approach for training with\nsparse, background-free annotations. Extensive experiments demonstrate that our\nproposed method reaches state-of-the-art performance on a sparsely annotated\nHSI dataset comprising $107$ classes organised in a clinically-defined semantic\ntree structure. Furthermore, our method enables effective detection of\nout-of-distribution (OOD) pixels without compromising segmentation performance\non in-distribution (ID) pixels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21150v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21150v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于树的语义损失：应用于稀疏监督的大规模多类别高光谱图像分割", "tldr": "本文提出了两种基于树的语义损失函数，用于稀疏监督的高光谱图像分割，利用标签的层次结构，在包含107个类别的稀疏标注高光谱数据集上达到了最先进的性能，并能有效检测OOD像素。", "motivation": "高光谱成像在外科应用中潜力巨大，但现有的生物医学分割方法对所有错误同等惩罚，未能利用标签空间中的类间语义信息，导致在区分大量细微变化的类别时效果不佳。", "method": "本文提出了两种基于树的语义损失函数，利用标签的层次结构。这些损失函数被整合到一个最近提出的用于稀疏、无背景注释训练的方法中。", "result": "在包含107个类别的临床定义语义树结构高光谱图像数据集上，本文提出的方法达到了最先进的性能。此外，该方法能够在不影响分布内（ID）像素分割性能的情况下，有效检测分布外（OOD）像素。", "conclusion": "本文提出的基于树的语义损失函数，通过利用标签的层次结构，显著提升了稀疏监督大规模多类别高光谱图像分割的性能，并增强了对分布外像素的检测能力。", "translation": "高光谱成像（HSI）在外科应用中展现出巨大潜力，能够提供超越肉眼所能感知的生物组织差异的详细信息。目前正在进行精细的标注工作，以训练视觉系统区分大量细微变化的类别。然而，生物医学分割任务中常用的学习方法对所有错误给予同等惩罚，因此未能利用标签空间中的任何类间语义。在这项工作中，我们引入了两种基于树的语义损失函数，它们利用了标签的层次组织。我们进一步将我们的损失函数整合到一个最近提出的利用稀疏、无背景注释进行训练的方法中。大量的实验表明，我们提出的方法在包含107个类别、以临床定义语义树结构组织的高光谱图像稀疏标注数据集上达到了最先进的性能。此外，我们的方法能够在不影响分布内（ID）像素分割性能的情况下，有效检测分布外（OOD）像素。", "summary": "本文针对高光谱成像（HSI）在外科应用中细致区分大量类别的挑战，提出了两种基于树的语义损失函数。这些损失函数利用标签的层次结构，解决了传统方法忽视类间语义信息的问题。结合稀疏标注训练方法，本研究在高光谱数据集上实现了最先进的分割性能，并有效检测了分布外像素，为生物医学图像分割提供了新途径。", "keywords": "高光谱图像分割, 语义损失, 树结构, 稀疏监督, 分布外检测", "comments": "这项工作通过引入基于树的语义损失，巧妙地解决了高光谱图像分割中类别多且细微、传统损失函数无法利用类间语义的痛点。其创新性在于将标签的层次结构融入损失函数设计，这对于处理大规模多类别分类问题尤为重要。同时，该方法在稀疏标注数据集上的SOTA表现以及OOD检测能力，进一步凸显了其实用价值和潜在应用前景。"}}
{"id": "2506.21151", "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels", "authors": ["Aida Moafi", "Danial Moafi", "Evgeny M. Mirkes", "Gerry P. McCann", "Abbas S. Alatrany", "Jayanth R. Arnold", "Mostafa Mehdipour Ghazi"], "summary": "The accurate segmentation of myocardial scars from cardiac MRI is essential\nfor clinical assessment and treatment planning. In this study, we propose a\nrobust deep-learning pipeline for fully automated myocardial scar detection and\nsegmentation by fine-tuning state-of-the-art models. The method explicitly\naddresses challenges of label noise from semi-automatic annotations, data\nheterogeneity, and class imbalance through the use of Kullback-Leibler loss and\nextensive data augmentation. We evaluate the model's performance on both acute\nand chronic cases and demonstrate its ability to produce accurate and smooth\nsegmentations despite noisy labels. In particular, our approach outperforms\nstate-of-the-art models like nnU-Net and shows strong generalizability in an\nout-of-distribution test set, highlighting its robustness across various\nimaging conditions and clinical tasks. These results establish a reliable\nfoundation for automated myocardial scar quantification and support the broader\nclinical adoption of deep learning in cardiac imaging.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21151v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21151v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "针对心脏MRI中含噪声标签的心肌瘢痕分割的鲁棒深度学习", "tldr": "本研究提出了一种鲁棒的深度学习方法，用于在存在噪声标签的情况下，准确分割心脏MRI中的心肌瘢痕，并优于现有技术。", "motivation": "准确分割心脏MRI中的心肌瘢痕对于临床评估和治疗规划至关重要。", "method": "本研究提出一个鲁棒的深度学习流程，通过微调最先进的模型，明确解决了半自动标注、数据异质性和类别不平衡带来的标签噪声挑战。具体方法包括使用Kullback-Leibler损失和大量数据增强。", "result": "模型在急性病例和慢性病例上都表现出准确和平滑的分割，即使存在噪声标签。该方法优于nnU-Net等最先进模型，并在分布外测试集上显示出强大的泛化能力，突出了其在各种成像条件和临床任务中的鲁棒性。", "conclusion": "这些结果为自动化心肌瘢痕量化奠定了可靠基础，并支持深度学习在心脏成像中更广泛的临床应用。", "translation": "心肌瘢痕在心脏MRI中的精确分割对于临床评估和治疗规划至关重要。在这项研究中，我们提出了一种鲁棒的深度学习流程，通过微调最先进的模型，实现全自动心肌瘢痕检测和分割。该方法通过使用Kullback-Leibler损失和广泛的数据增强，明确解决了半自动标注、数据异质性和类别不平衡带来的标签噪声挑战。我们在急性病例和慢性病例上评估了模型的性能，并展示了其在存在噪声标签的情况下也能产生准确和平滑分割的能力。特别是，我们的方法优于nnU-Net等最先进模型，并在分布外测试集上显示出强大的泛化能力，突出了其在各种成像条件和临床任务中的鲁棒性。这些结果为自动化心肌瘢痕量化奠定了可靠基础，并支持深度学习在心脏成像中更广泛的临床应用。", "summary": "本研究提出了一种针对心脏MRI中肌瘢痕分割的鲁棒深度学习方法。该方法通过微调现有模型并结合Kullback-Leibler损失和数据增强，有效解决了半自动标注中的标签噪声、数据异质性和类别不平衡问题。实验证明，即使在噪声标签下，该方法也能实现准确平滑的分割，并且在泛化能力和性能上优于现有技术，为心肌瘢痕的自动化量化和深度学习在心脏成像领域的临床应用奠定了基础。", "keywords": "心肌瘢痕分割, 深度学习, 标签噪声, 心脏MRI, 鲁棒性", "comments": "这篇论文的创新点在于其鲁棒的深度学习流程，特别是在处理医疗图像中常见的标签噪声（来自半自动标注）、数据异质性和类别不平衡问题上。通过采用KL损失和数据增强，该方法显著提升了模型在真实世界数据中的实用性。其超越nnU-Net并展现出强大泛化能力的表现，使其在临床应用中具有重要意义，尤其是在需要高精度和可靠性的诊断领域。"}}
{"id": "2506.21343", "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language Models", "authors": ["Jingyao Li", "Hao Sun", "Zile Qiao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Hong Xu", "Jiaya Jia"], "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21343v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21343v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DynamicBench：评估大型语言模型中的实时报告生成", "tldr": "DynamicBench是一个新的基准测试，用于评估LLM处理实时信息和生成报告的能力，其方法在文档有无的情况下均超越GPT4o。", "motivation": "传统的大型语言模型（LLM）基准测试依赖于静态评估，无法捕捉当代应用中实时信息处理的动态需求。为了解决这一限制，本文提出了DynamicBench。", "method": "DynamicBench采用双路径检索管道，整合了网络搜索和本地报告数据库。它需要领域特定知识，并在提供或不提供外部文档的场景中评估模型处理最新信息或利用上下文增强的能力。此外，本文还引入了一个先进的报告生成系统，能够管理动态信息合成。", "result": "实验结果证实了该方法的有效性，其方法在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%，达到了最先进的性能。", "conclusion": "本文提出的DynamicBench及其相关方法有效解决了大型语言模型在实时报告生成方面的评估空白，并在实验中展现出卓越的性能。", "translation": "大型语言模型（LLM）的传统基准测试通常依赖于通过讲故事或表达意见进行的静态评估，这未能捕捉当代应用中实时信息处理的动态需求。为了解决这一限制，我们提出了DynamicBench，一个旨在评估LLM存储和处理最新数据熟练程度的基准测试。DynamicBench利用双路径检索管道，整合了网络搜索和本地报告数据库。它需要领域特定知识，以确保在专业领域内准确生成报告。通过在提供或不提供外部文档的场景中评估模型，DynamicBench有效地衡量了它们独立处理最新信息或利用上下文增强的能力。此外，我们引入了一个先进的报告生成系统，擅长管理动态信息合成。我们的实验结果证实了我们方法的有效性，我们的方法在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%，达到了最先进的性能。代码和数据将公开可用。", "summary": "本文介绍了DynamicBench，一个旨在评估大型语言模型（LLM）实时报告生成能力的新型基准测试。针对传统静态评估的局限性，DynamicBench采用双路径检索管道，结合网络搜索和本地报告数据库，并要求领域特定知识。它在有无外部文档的场景下评估LLM处理最新信息的能力。实验结果表明，该方法实现了最先进的性能，在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%。", "keywords": "DynamicBench, 大型语言模型, 实时报告生成, 基准测试, 信息处理", "comments": "该论文通过引入DynamicBench，创新性地解决了现有LLM基准测试未能有效评估实时信息处理能力的痛点。其双路径检索管道和对领域特定知识的要求，使其能够更真实地模拟实际应用场景。超越GPT4o的性能表明了该方法的有效性和重要性，对于推动LLM在动态信息处理领域的应用具有重要意义。"}}
{"id": "2506.21152", "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image", "authors": ["Pufan Li", "Bi'an Du", "Wei Hu"], "summary": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To takle these issues, we present a novel method that seamlessly\nintegrates geometry and perception priors without requiring additional model\ntraining to reconstruct detailed 3D objects from a single image. Specifically,\nwe train three different Gaussian branches initialized from the geometry prior,\nperception prior and Gaussian noise, respectively. The geometry prior captures\nthe rough 3D shapes, while the perception prior utilizes the 2D pretrained\ndiffusion model to enhance multiview information. Subsequently, we refine 3D\nGaussian branches through mutual interaction between geometry and perception\npriors, further enhanced by a reprojection-based strategy that enforces depth\nconsistency. Experiments demonstrate the higher-fidelity reconstruction results\nof our method, outperforming existing methods on novel view synthesis and 3D\nreconstruction, demonstrating robust and consistent 3D object generation.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21152v1", "categories": ["cs.CV", "68", "I.4.0"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21152v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "几何与感知引导的高斯用于单图多视角一致性三维生成", "tldr": "本文提出一种新方法，通过结合几何和感知先验，从单张图像生成多视角一致且细节丰富的3D对象，无需额外模型训练。", "motivation": "现有从单视图图像生成3D对象的方法通常存在多视角一致性差和几何细节不足的问题，且难以捕捉未见区域的多种合理解释。", "method": "本文提出一种新颖的方法，无需额外模型训练即可无缝整合几何和感知先验，以从单张图像重建详细的3D对象。具体地，训练三个不同高斯分支，分别从几何先验、感知先验和高斯噪声初始化。几何先验用于捕获粗略的3D形状，感知先验利用2D预训练扩散模型增强多视角信息。随后，通过几何和感知先验之间的相互作用以及基于重投影的策略加强深度一致性，进一步细化3D高斯分支。", "result": "实验证明，该方法实现了更高保真度的重建结果，在新视图合成和3D重建方面优于现有方法，展示了鲁棒且一致的3D对象生成。", "conclusion": "本文提出的几何与感知引导的高斯方法有效解决了单视图3D生成中多视角一致性和几何细节不足的问题，实现了高质量的3D对象重建。", "translation": "从单视图图像生成逼真的3D对象需要自然的视觉外观、3D一致性以及捕捉未见区域多种合理解释的能力。现有方法通常依赖于微调预训练的2D扩散模型或通过快速网络推理或3D高斯散射直接生成3D信息，但它们的结果普遍存在多视角一致性差和几何细节不足的问题。为了解决这些问题，我们提出了一种新颖的方法，该方法无需额外的模型训练即可无缝整合几何和感知先验，以从单张图像重建详细的3D对象。具体而言，我们训练了三个不同高斯分支，分别从几何先验、感知先验和高斯噪声初始化。几何先验捕获粗略的3D形状，而感知先验则利用2D预训练扩散模型来增强多视角信息。随后，我们通过几何和感知先验之间的相互作用进一步细化3D高斯分支，并通过基于重投影的策略加强深度一致性。实验表明，我们的方法实现了更高保真度的重建结果，在新型视图合成和3D重建方面优于现有方法，展示了鲁棒且一致的3D对象生成。", "summary": "本文提出一种创新的单视图3D生成方法，旨在解决现有技术在多视角一致性和几何细节方面的不足。该方法通过整合几何和感知先验，并利用三个独立初始化的高斯分支（分别来自几何先验、感知先验和高斯噪声）进行3D对象重建，且无需额外的模型训练。几何先验用于捕捉粗略3D形状，感知先验则通过2D预训练扩散模型增强多视角信息。通过两者间的相互作用和基于重投影的深度一致性策略，对3D高斯分支进行细化。实验结果表明，该方法在3D重建和新视图合成方面表现出更高保真度和一致性，优于现有方法。", "keywords": "3D生成, 单视图, 高斯散射, 多视角一致性, 几何先验, 感知先验", "comments": "这篇论文的创新点在于它无需额外的模型训练，而是通过巧妙地结合几何先验和感知先验来增强3D高斯模型，从而解决了单视图3D生成中长期存在的多视角不一致和细节缺失问题。其引入的三个高斯分支以及几何与感知之间的相互作用机制是关键，特别是利用2D预训练扩散模型作为感知先验来增强多视角信息，这提供了一个有效的解决方案。"}}
{"id": "2506.21165", "title": "Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition", "authors": ["Longkun Zou", "Kangjun Liu", "Ke Chen", "Kailing Guo", "Kui Jia", "Yaowei Wang"], "summary": "Learning semantic representations from point sets of 3D object shapes is\noften challenged by significant geometric variations, primarily due to\ndifferences in data acquisition methods. Typically, training data is generated\nusing point simulators, while testing data is collected with distinct 3D\nsensors, leading to a simulation-to-reality (Sim2Real) domain gap that limits\nthe generalization ability of point classifiers. Current unsupervised domain\nadaptation (UDA) techniques struggle with this gap, as they often lack robust,\ndomain-insensitive descriptors capable of capturing global topological\ninformation, resulting in overfitting to the limited semantic patterns of the\nsource domain. To address this issue, we introduce a novel Topology-Aware\nModeling (TAM) framework for Sim2Real UDA on object point clouds. Our approach\nmitigates the domain gap by leveraging global spatial topology, characterized\nby low-level, high-frequency 3D structures, and by modeling the topological\nrelations of local geometric features through a novel self-supervised learning\ntask. Additionally, we propose an advanced self-training strategy that combines\ncross-domain contrastive learning with self-training, effectively reducing the\nimpact of noisy pseudo-labels and enhancing the robustness of the adaptation\nprocess. Experimental results on three public Sim2Real benchmarks validate the\neffectiveness of our TAM framework, showing consistent improvements over\nstate-of-the-art methods across all evaluated tasks. The source code of this\nwork will be available at https://github.com/zou-longkun/TAG.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21165v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21165v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于无监督模拟到现实点云识别的拓扑感知建模", "tldr": "本文提出了一种名为拓扑感知建模（TAM）的新框架，用于解决3D点云识别中模拟到现实的领域差距问题，通过利用全局空间拓扑和局部几何特征的拓扑关系，并结合先进的自训练策略，在多个基准测试中取得了优于现有方法的性能。", "motivation": "由于数据采集方法的差异，3D物体形状点集学习语义表示面临显著的几何变异挑战，导致模拟数据与真实数据之间存在模拟到现实（Sim2Real）领域差距，这限制了点分类器的泛化能力。现有无监督域适应（UDA）技术难以应对这一差距，因为它们缺乏能够捕获全局拓扑信息的鲁棒、领域不敏感的描述符，导致对源域有限语义模式的过拟合。", "method": "本文提出了一种新颖的拓扑感知建模（TAM）框架，用于解决对象点云的Sim2Real UDA问题。该方法通过利用低级、高频3D结构表征的全局空间拓扑，并通过新颖的自监督学习任务建模局部几何特征的拓扑关系来缩小领域差距。此外，还提出了一种先进的自训练策略，结合跨域对比学习和自训练，有效减少了噪声伪标签的影响并增强了适应过程的鲁棒性。", "result": "在三个公共Sim2Real基准测试上的实验结果验证了TAM框架的有效性，在所有评估任务中均显示出对现有最先进方法的一致改进。", "conclusion": "通过引入拓扑感知建模（TAM）框架，该研究成功地解决了3D点云识别中的模拟到现实领域差距问题，并显著提高了无监督域适应的性能。", "translation": "从3D物体形状的点集中学习语义表示通常受到显著几何变化的挑战，这主要是由于数据采集方法的差异。通常，训练数据是使用点模拟器生成的，而测试数据是使用不同的3D传感器收集的，这导致了模拟到现实（Sim2Real）的领域差距，限制了点分类器的泛化能力。当前的无监督域适应（UDA）技术难以应对这一差距，因为它们通常缺乏能够捕获全局拓扑信息的鲁棒、领域不敏感的描述符，导致对源域有限语义模式的过拟合。为了解决这个问题，我们引入了一种新颖的拓扑感知建模（TAM）框架，用于对象点云的Sim2Real UDA。我们的方法通过利用由低级、高频3D结构表征的全局空间拓扑，并通过新颖的自监督学习任务建模局部几何特征的拓扑关系来缩小领域差距。此外，我们提出了一种先进的自训练策略，结合跨域对比学习和自训练，有效减少了噪声伪标签的影响并增强了适应过程的鲁棒性。在三个公共Sim2Real基准测试上的实验结果验证了我们TAM框架的有效性，在所有评估任务中均显示出对现有最先进方法的一致改进。这项工作的源代码将在https://github.com/zou-longkun/TAG.git上提供。", "summary": "本研究提出了一种名为拓扑感知建模（TAM）的新框架，旨在解决3D点云识别中模拟到现实（Sim2Real）的领域差距问题。针对现有无监督域适应（UDA）方法在处理几何变异和缺乏全局拓扑信息描述符方面的不足，TAM框架通过利用全局空间拓扑和建模局部几何特征的拓扑关系来缓解领域差距。此外，该框架结合了跨域对比学习和自训练的先进自训练策略，以提高适应过程的鲁棒性并减少噪声伪标签的影响。实验结果表明，TAM在多个Sim2Real基准测试上均优于现有最先进方法。", "keywords": "点云识别, 模拟到现实, 无监督域适应, 拓扑感知建模, 领域差距", "comments": "该论文通过引入拓扑感知建模，有效地解决了点云Sim2Real领域适应中全局拓扑信息缺失的关键问题，并结合创新的自监督和自训练策略，显著提升了模型在跨域识别上的泛化能力。其创新点在于对拓扑信息的深度利用和对伪标签噪声的鲁棒性处理。"}}
{"id": "2506.21355", "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning", "authors": ["Melanie Rieff", "Maya Varma", "Ossian Rabow", "Subathra Adithan", "Julie Kim", "Ken Chang", "Hannah Lee", "Nidhi Rohatgi", "Christian Bluethgen", "Mohamed S. Muneer", "Jean-Benoit Delbrouck", "Michael Moor"], "summary": "Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, example ordering exhibits a recency bias,\ni.e., placing the most relevant example last can lead to substantial\nperformance improvements by up to 71%. Our findings highlight critical\nlimitations and biases in current MLLMs when learning multimodal medical tasks\nfrom context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21355v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21355v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SMMILE：一个专家驱动的多模态医学上下文学习基准", "tldr": "引入SMMILE，首个专家驱动的医学多模态上下文学习基准，评估发现当前多模态大语言模型在此任务上表现不佳，且易受无关示例和示例顺序影响。", "motivation": "尽管多模态上下文学习（ICL）在医学等领域潜力巨大，但其探索不足，特别是多模态大语言模型在医学任务中从上下文中学习的能力尚不明确。", "method": "引入了SMMILE，一个由11位医学专家策划的医学多模态上下文学习基准，包含111个问题（517个问答-图像三元组），涵盖6个医学专业和13种成像模式。还推出了增强版SMMILE++。对15个多模态大语言模型进行了全面评估。", "result": "大多数模型在医学多模态上下文学习能力上表现中等或较差。上下文学习对零样本的平均提升仅为SMMILE上的8%和SMMILE++上的9.4%。无关上下文示例会降低性能，单个噪声或无关示例可使性能下降高达9.5%。示例排序存在近因偏差，即将最相关的示例放在最后可使性能大幅提升高达71%。", "conclusion": "当前多模态大语言模型在从上下文学习多模态医学任务时存在严重的局限性和偏差。", "translation": "尽管多模态上下文学习（ICL）在医学等领域具有巨大潜力，但其探索不足。临床医生日常会遇到需要从有限示例中适应的各种专业任务，例如从少量相关既往病例中获取见解或考虑一组受限的鉴别诊断。虽然多模态大语言模型（MLLMs）在医学视觉问答（VQA）方面取得了进展，但它们从上下文中学习多模态任务的能力在很大程度上是未知的。\n我们介绍了SMMILE，这是第一个专家驱动的医学任务多模态ICL基准。十一位医学专家策划了问题，每个问题都包含一个多模态查询和多模态上下文示例作为任务演示。SMMILE包含111个问题（517个问答-图像三元组），涵盖6个医学专业和13种成像模式。我们进一步介绍了SMMILE++，这是一个包含1038个排列问题的增强变体。对15个MLLM的全面评估表明，大多数模型在医学任务中的多模态ICL能力表现中等或较差。在开放式评估中，ICL在SMMILE上比零样本平均仅提升8%，在SMMILE++上提升9.4%。我们观察到对不相关上下文示例的敏感性：即使是单个噪声或不相关的示例也能使性能下降高达9.5%。此外，示例排序表现出近因偏差，即将最相关的示例放在最后可以使性能大幅提升高达71%。我们的发现强调了当前MLLM在从上下文中学习多模态医学任务时存在的关键局限性和偏差。", "summary": "本文引入了SMMILE，首个由专家驱动的医学多模态上下文学习基准，旨在评估多模态大语言模型在医学领域从上下文学习的能力。该基准包含由医学专家精心策划的问题和示例，并推出了增强版SMMILE++。通过对15个MLLM的广泛评估，研究发现当前模型在医学多模态上下文学习方面表现不佳，且易受不相关示例干扰，同时示例排序存在近因偏差，提示了当前模型在此任务上的显著局限性。", "keywords": "多模态上下文学习, 医学人工智能, 基准测试, 多模态大语言模型, 近因偏差", "comments": "本文的创新之处在于构建了首个专家驱动的医学多模态上下文学习基准SMMILE，填补了该领域评估工具的空白。其重要性在于揭示了当前多模态大语言模型在医学上下文学习中存在的关键局限性，特别是在处理无关示例和示例顺序方面，为未来模型改进提供了明确的方向。"}}
{"id": "2506.21184", "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding", "authors": ["Minghao Qin", "Yan Shu", "Peitian Zhang", "Kun Lun", "Huaying Yuan", "Juenjie Zhou", "Shitao Xiao", "Bo Zhao", "Zheng Liu"], "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.", "comment": "14 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.21184v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21184v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "任务感知型KV压缩，用于经济高效的长视频理解", "tldr": "Video-X^2L提出了一种任务感知的双层KV压缩和选择性KV重载方法，显著提升了长视频理解的效率和性能，且无需额外训练。", "motivation": "现有的大型多模态语言模型（MLLMs）在长视频理解（LVU）方面面临巨大的计算成本挑战，尽管KV压缩方法有所探索，但在高压缩比下常导致严重信息损失。", "method": "本文引入了Video-X^2L，它包含两个关键操作：1) 双层KV压缩：在MLLM的预填充阶段生成低压缩（L-KVs）和高压缩（H-KVs）两种KVs，分别捕获细粒度细节和提供紧凑表示。2) 选择性KV重载：在MLLM的解码阶段，选择性地为最关键的视频块重载L-KVs，而对不重要的部分使用H-KVs，从而在保持整体紧凑性的同时充分利用任务特定信息。", "result": "Video-X^2L在VideoMME、MLVU、LongVideoBench和VNBench等多个流行LVU基准测试中进行了评估。实验结果表明，Video-X^2L在大幅节省计算成本的同时，以巨大优势超越了现有的KV压缩方法。", "conclusion": "Video-X^2L是一种简单而有效的方法，它无需额外训练，并直接兼容现有的KV可压缩MLLMs，显著提升了长视频理解的性能和效率。", "translation": "长视频理解（LVU）对于现有的大型多模态语言模型（MLLMs）来说仍然是一个严峻的挑战，这主要是由于其高昂的计算成本。最近的方法探索了KV压缩来缓解这个问题，但它们在高压缩比下常常遭受严重的信息损失。在本文中，我们介绍了Video-X^2L，它能灵活地为每个LVU任务保留关键的视频信息。Video-X^2L涉及两个关键操作。第一个称为双层KV压缩。在MLLM的预填充阶段，Video-X^2L生成两种类型的压缩KVs：低压缩KVs（L-KVs）用于捕获细粒度的视频细节，高压缩KVs（H-KVs）用于提供紧凑的视频表示。第二个称为选择性KV重载。在MLLM的解码阶段，Video-X^2L选择性地为最关键的视频块重载L-KVs，同时对其他不那么重要的部分使用H-KVs。这使得MLLM能够充分利用任务特定信息，同时保持整体的紧凑性。Video-X^2L简单而有效：它无需额外训练，并直接兼容现有的KV可压缩MLLMs。我们使用各种流行的LVU基准测试对Video-X^2L进行了评估，包括VideoMME、MLVU、LongVideoBench和VNBench。我们的实验结果表明，Video-X^2L以巨大的优势超越了现有的KV压缩方法，同时大幅节省了计算成本。", "summary": "本文提出了一种名为Video-X^2L的新方法，旨在解决长视频理解中大型多模态语言模型（MLLMs）面临的高计算成本和信息损失问题。Video-X^2L通过引入双层KV压缩（生成低压缩和高压缩KVs）和选择性KV重载机制（根据视频块的重要性选择性加载KVs）来优化KV压缩。这种方法能够在不进行额外训练的情况下，有效保留关键视频信息，显著降低计算成本，并在多个长视频理解基准测试中表现优于现有KV压缩技术。", "keywords": "长视频理解, KV压缩, 多模态大型语言模型, 计算成本, 任务感知", "comments": "Video-X^2L的创新之处在于其“任务感知”的双层KV压缩和选择性重载机制，这有效地解决了长视频理解中计算效率与信息保留之间的矛盾。其无需额外训练即可兼容现有模型的特点，大大降低了其应用门槛，使其具有很强的实用价值和潜在影响力。"}}
{"id": "2506.21367", "title": "rQdia: Regularizing Q-Value Distributions With Image Augmentation", "authors": ["Sam Lerman", "Jing Bi"], "summary": "rQdia regularizes Q-value distributions with augmented images in pixel-based\ndeep reinforcement learning. With a simple auxiliary loss, that equalizes these\ndistributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks\nrespectively in the MuJoCo Continuous Control Suite from pixels, and\nData-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured\nin both sample efficiency and longer-term training. Moreover, the addition of\nrQdia finally propels model-free continuous control from pixels over the state\nencoding baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21367v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21367v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "rQdia：使用图像增强正则化Q值分布", "tldr": "rQdia通过图像增强和简单的辅助损失来正则化Q值分布，显著提升了像素级深度强化学习算法（如DrQ、SAC和Data-Efficient Rainbow）在MuJoCo和Atari环境中的表现，并使无模型连续控制超越了状态编码基线。", "motivation": "在像素级深度强化学习中，为了提升现有算法的性能和样本效率，并使无模型的连续控制能够超越基于状态编码的基线，作者提出了正则化Q值分布的方法。", "method": "rQdia通过使用增强图像和引入一个简单的辅助损失来正则化Q值分布，该损失通过均方误差（MSE）来均衡这些分布。", "result": "rQdia在MuJoCo连续控制套件中，分别将DrQ和SAC在像素级任务上的表现提升了9/12和10/12；在Atari街机环境中，将Data-Efficient Rainbow的表现提升了18/26。这些提升体现在样本效率和长期训练中。此外，rQdia的加入最终使得无模型的像素级连续控制超越了状态编码基线。", "conclusion": "rQdia通过正则化Q值分布，显著提升了多种像素级深度强化学习算法的性能和效率，并成功使无模型的像素级连续控制超越了传统的状态编码基线。", "translation": "rQdia通过增强图像在基于像素的深度强化学习中正则化Q值分布。通过一个简单的辅助损失，即通过均方误差（MSE）均衡这些分布，rQdia在MuJoCo连续控制套件中，分别将DrQ和SAC在像素级任务上的表现提升了9/12和10/12，并在26个Atari街机环境中提升了Data-Efficient Rainbow的表现18个。性能提升体现在样本效率和长期训练中。此外，rQdia的加入最终使得无模型的像素级连续控制超越了状态编码基线。", "summary": "rQdia是一种用于像素级深度强化学习的方法，它利用图像增强技术通过简单的均方误差辅助损失来正则化Q值分布。该方法显著提升了DrQ和SAC在MuJoCo连续控制任务上的表现，并改善了Data-Efficient Rainbow在Atari环境中的性能。rQdia不仅提高了算法的样本效率和长期训练效果，还首次使无模型像素级连续控制超越了状态编码基线。", "keywords": "rQdia, Q值分布, 图像增强, 深度强化学习, 像素级控制", "comments": "本文的创新点在于提出了一个简单而有效的辅助损失，通过正则化Q值分布来利用图像增强，从而显著提升了像素级深度强化学习的性能。其重要性在于，它不仅在多个基准测试中取得了优异成绩，更重要的是，它推动了无模型像素级连续控制超越了传统的基于状态编码的基线，为该领域的发展开辟了新方向。"}}
{"id": "2506.21371", "title": "MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators", "authors": ["Vasileios Leon", "Georgios Makris", "Sotirios Xydis", "Kiamal Pekmestzi", "Dimitrios Soudris"], "summary": "Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has\nestablished them as the defacto approach for providing advanced Machine\nLearning tasks with excellent accuracy. Targeting low-power DNN computing, this\npaper examines the interplay of fine-grained error resilience of DNN workloads\nin collaboration with hardware approximation techniques, to achieve higher\nlevels of energy efficiency. Utilizing the state-of-the-art ROUP approximate\nmultipliers, we systematically explore their fine-grained distribution across\nthe network according to our layer-, filter-, and kernel-level approaches, and\nexamine their impact on accuracy and energy. We use the ResNet-8 model on the\nCIFAR-10 dataset to evaluate our approximations. The proposed solution delivers\nup to 54% energy gains in exchange for up to 4% accuracy loss, compared to the\nbaseline quantized model, while it provides 2x energy gains with better\naccuracy versus the state-of-the-art DNN approximations.", "comment": "Presented at the 13th IEEE LASCAS Conference", "pdf_url": "http://arxiv.org/pdf/2506.21371v1", "categories": ["cs.LG", "cs.AR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21371v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MAx-DNN：用于节能DNN硬件加速器的多级算术近似", "tldr": "MAx-DNN提出了一种多级算术近似方法，通过在层、滤波器和核级别分布近似乘法器，显著提高深度神经网络硬件加速器的能效，同时保持可接受的精度。", "motivation": "深度神经网络（DNN）架构的快速增长使其成为提供高级机器学习任务的事实方法，但其计算需求高。本文旨在通过利用DNN工作负载的细粒度错误弹性与硬件近似技术相结合，实现更高级别的能效，从而解决低功耗DNN计算的需求。", "method": "本文利用最先进的ROUP近似乘法器，系统地探索其在网络中的细粒度分布，采用层级、滤波器级和核级方法。通过在CIFAR-10数据集上使用ResNet-8模型评估了这些近似的影响。", "result": "所提出的解决方案相比于基线量化模型，在牺牲高达4%的精度损失的情况下，实现了高达54%的能量增益；与最先进的DNN近似方法相比，则提供了2倍的能量增益和更好的精度。", "conclusion": "MAx-DNN通过多级算术近似，有效地提高了深度神经网络硬件加速器的能效，并在保持可接受的精度损失的同时，优于现有的先进近似方法。", "translation": "如今，深度神经网络（DNN）架构的快速增长已使其成为提供高精度高级机器学习任务的事实方法。针对低功耗DNN计算，本文研究了DNN工作负载的细粒度错误弹性与硬件近似技术之间的相互作用，以实现更高水平的能效。我们利用最先进的ROUP近似乘法器，系统地探索了它们在网络中根据我们的层级、滤波器级和核级方法的细粒度分布，并检查了它们对精度和能耗的影响。我们使用CIFAR-10数据集上的ResNet-8模型来评估我们的近似。所提出的解决方案与基线量化模型相比，以高达4%的精度损失换取了高达54%的能量增益，同时与最先进的DNN近似方法相比，提供了2倍的能量增益和更好的精度。", "summary": "本文提出MAx-DNN，一种利用多级算术近似来提高深度神经网络（DNN）硬件加速器能效的方法。通过系统地在层、滤波器和核级别分布ROUP近似乘法器，研究了其对精度和能耗的影响。在ResNet-8和CIFAR-10数据集上的评估表明，该方法在可接受的精度损失下显著降低了能耗，并且在能效方面优于现有先进的DNN近似技术。", "keywords": "深度神经网络, 硬件加速器, 算术近似, 能效, ROUP乘法器", "comments": "本文的创新点在于提出了多级（层、滤波器、核）的细粒度近似方法，这比传统的粗粒度近似更精细和灵活。通过在不同粒度级别上应用近似乘法器，可以更好地平衡精度和能效之间的权衡。其重要性在于为未来节能型AI硬件设计提供了新的思路，尤其是在资源受限的边缘设备上部署DNN时具有重要意义。"}}
{"id": "2506.21188", "title": "GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding", "authors": ["Zijun Lin", "Shuting He", "Cheston Tan", "Bihan Wen"], "summary": "Sequential grounding in 3D point clouds (SG3D) refers to locating sequences\nof objects by following text instructions for a daily activity with detailed\nsteps. Current 3D visual grounding (3DVG) methods treat text instructions with\nmultiple steps as a whole, without extracting useful temporal information from\neach step. However, the instructions in SG3D often contain pronouns such as\n\"it\", \"here\" and \"the same\" to make language expressions concise. This requires\ngrounding methods to understand the context and retrieve relevant information\nfrom previous steps to correctly locate object sequences. Due to the lack of an\neffective module for collecting related historical information,\nstate-of-the-art 3DVG methods face significant challenges in adapting to the\nSG3D task. To fill this gap, we propose GroundFlow -- a plug-in module for\ntemporal reasoning on 3D point cloud sequential grounding. Firstly, we\ndemonstrate that integrating GroundFlow improves the task accuracy of 3DVG\nbaseline methods by a large margin (+7.5\\% and +10.2\\%) in the SG3D benchmark,\neven outperforming a 3D large language model pre-trained on various datasets.\nFurthermore, we selectively extract both short-term and long-term step\ninformation based on its relevance to the current instruction, enabling\nGroundFlow to take a comprehensive view of historical information and maintain\nits temporal understanding advantage as step counts increase. Overall, our work\nintroduces temporal reasoning capabilities to existing 3DVG models and achieves\nstate-of-the-art performance in the SG3D benchmark across five datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21188v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21188v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GroundFlow：一个用于三维点云序列化定位中时序推理的插件模块", "tldr": "GroundFlow是一个插件模块，通过有效整合短期和长期历史信息，显著提升了三维点云序列化定位（SG3D）任务中现有三维视觉定位（3DVG）方法的性能，解决了现有方法在处理带有代词的指令时缺乏时序推理能力的问题。", "motivation": "现有三维视觉定位（3DVG）方法在处理包含多步骤指令的文本时，未能有效提取时序信息，尤其是在指令中包含“它”、“这里”和“相同”等代词时，需要理解上下文并从之前的步骤中检索相关信息。由于缺乏有效的历史信息收集模块，现有3DVG方法难以适应三维点云序列化定位（SG3D）任务。", "method": "本文提出了GroundFlow，一个用于三维点云序列化定位中时序推理的插件模块。GroundFlow能够选择性地提取与当前指令相关的短期和长期步骤信息，从而全面利用历史信息。", "result": "GroundFlow模块显著提升了基线3DVG方法在SG3D基准测试中的任务准确率（分别提高了7.5%和10.2%），甚至超越了在各种数据集上预训练的三维大型语言模型。GroundFlow在步骤数量增加时，仍能保持其时序理解优势，并在五个数据集的SG3D基准测试中取得了最先进的性能。", "conclusion": "GroundFlow模块成功地为现有3DVG模型引入了时序推理能力，并在三维点云序列化定位任务中实现了最先进的性能。", "translation": "三维点云序列化定位（SG3D）是指通过遵循详细步骤的日常活动文本指令来定位一系列对象。当前的三维视觉定位（3DVG）方法将包含多个步骤的文本指令作为一个整体处理，而没有从每个步骤中提取有用的时序信息。然而，SG3D中的指令通常包含“它”、“这里”和“相同”等代词，以使语言表达简洁。这要求定位方法理解上下文并从之前的步骤中检索相关信息，以正确地定位对象序列。由于缺乏有效的模块来收集相关的历史信息，最先进的3DVG方法在适应SG3D任务时面临重大挑战。为了填补这一空白，我们提出了GroundFlow——一个用于三维点云序列化定位中时序推理的插件模块。首先，我们证明了在SG3D基准测试中，集成GroundFlow可以大幅提高3DVG基线方法的任务准确率（+7.5%和+10.2%），甚至优于在各种数据集上预训练的三维大型语言模型。此外，我们根据其与当前指令的相关性，选择性地提取短期和长期步骤信息，使GroundFlow能够全面审视历史信息，并在步骤数量增加时保持其时序理解优势。总的来说，我们的工作为现有3DVG模型引入了时序推理能力，并在五个数据集的SG3D基准测试中取得了最先进的性能。", "summary": "本文提出了GroundFlow，一个用于三维点云序列化定位（SG3D）的插件模块，旨在解决现有三维视觉定位（3DVG）方法在处理多步骤文本指令时缺乏时序推理能力的问题。GroundFlow通过选择性地提取和整合短期及长期历史信息，显著提升了基线3DVG模型在SG3D任务上的准确性，并在多项基准测试中超越了预训练的三维大型语言模型，实现了最先进的性能。", "keywords": "三维点云, 时序推理, 序列化定位, 视觉定位, 插件模块", "comments": "GroundFlow的创新之处在于其作为插件模块，能够为现有3DVG模型引入急需的时序推理能力，有效解决了SG3D任务中代词和上下文理解的挑战。其通过选择性地整合短期和长期历史信息的方法，使其在处理复杂序列指令时表现出色，具有重要的实际应用价值。"}}
{"id": "2506.21374", "title": "Pay Attention to Small Weights", "authors": ["Chao Zhou", "Tom Jacobs", "Advait Gadhikar", "Rebekka Burkholz"], "summary": "Finetuning large pretrained neural networks is known to be\nresource-intensive, both in terms of memory and computational cost. To mitigate\nthis, a common approach is to restrict training to a subset of the model\nparameters. By analyzing the relationship between gradients and weights during\nfinetuning, we observe a notable pattern: large gradients are often associated\nwith small-magnitude weights. This correlation is more pronounced in finetuning\nsettings than in training from scratch. Motivated by this observation, we\npropose NANOADAM, which dynamically updates only the small-magnitude weights\nduring finetuning and offers several practical advantages: first, this\ncriterion is gradient-free -- the parameter subset can be determined without\ngradient computation; second, it preserves large-magnitude weights, which are\nlikely to encode critical features learned during pretraining, thereby reducing\nthe risk of catastrophic forgetting; thirdly, it permits the use of larger\nlearning rates and consistently leads to better generalization performance in\nexperiments. We demonstrate this for both NLP and vision tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21374v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21374v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关注小权重", "tldr": "针对大型预训练模型微调的资源密集问题，本文提出NANOADAM，通过动态更新小权重来降低成本，同时提高泛化性能并减少灾难性遗忘。", "motivation": "微调大型预训练神经网络存在内存和计算成本高的资源密集问题。通过分析微调过程中梯度和权重的关系，发现大梯度常与小权重相关，尤其在微调设置中更为显著，这促使作者提出一种新的微调策略。", "method": "提出NANOADAM方法，该方法在微调过程中动态地仅更新小幅度的权重。其优点包括：参数子集确定无需梯度计算（无梯度准则），保留大幅度权重以减少灾难性遗忘，允许使用更大的学习率。", "result": "NANOADAM在实验中持续获得更好的泛化性能，并在NLP和视觉任务中得到了验证。", "conclusion": "NANOADAM通过关注并动态更新小权重，有效解决了大型模型微调的资源密集问题，同时提高了性能并减少了灾难性遗忘，提供了一种高效且实用的微调策略。", "translation": "微调大型预训练神经网络在内存和计算成本方面都是资源密集型的。为了缓解这种情况，一种常见的方法是将训练限制在模型参数的一个子集上。通过分析微调过程中梯度和权重之间的关系，我们观察到一个显著的模式：大梯度通常与小幅度的权重相关。这种相关性在微调设置中比从头开始训练时更为明显。受此观察启发，我们提出了NANOADAM，它在微调过程中动态地仅更新小幅度的权重，并提供了几个实际优势：首先，这个标准是无梯度的——可以在不计算梯度的情况下确定参数子集；其次，它保留了大幅度的权重，这些权重可能编码了预训练期间学到的关键特征，从而降低了灾难性遗忘的风险；第三，它允许使用更大的学习率，并在实验中持续带来更好的泛化性能。我们在NLP和视觉任务中都证明了这一点。", "summary": "本文针对大型预训练模型微调中存在的资源密集问题，通过观察到微调时大梯度常与小权重相关这一现象，提出了一种名为NANOADAM的新方法。NANOADAM在微调时仅动态更新小幅度的权重，无需梯度计算即可确定更新参数，有效保留了预训练学到的关键特征以减少灾难性遗忘，并支持更大的学习率，从而在NLP和视觉任务中展现出更优的泛化性能。", "keywords": "模型微调, 小权重, NANOADAM, 资源效率, 灾难性遗忘", "comments": "这篇论文通过一个简洁而新颖的观察（大梯度与小权重相关）提出了一种高效的微调策略。其创新点在于利用这一观察来设计一个无梯度且能有效减少灾难性遗忘的方法，显著降低了微调的资源消耗，同时提升了模型性能。这对于大型模型在资源受限环境下的部署和应用具有重要意义。"}}
{"id": "2506.21085", "title": "CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions", "authors": ["Yangzhe Peng", "Kaiyuan Gao", "Liang He", "Yuheng Cong", "Haiguang Liu", "Kun He", "Lijun Wu"], "summary": "Molecular docking plays a crucial role in predicting the binding mode of\nligands to target proteins, and covalent interactions, which involve the\nformation of a covalent bond between the ligand and the target, are\nparticularly valuable due to their strong, enduring binding nature. However,\nmost existing docking methods and deep learning approaches hardly account for\nthe formation of covalent bonds and the associated structural changes. To\naddress this gap, we introduce a comprehensive benchmark for covalent docking,\nCovDocker, which is designed to better capture the complexities of covalent\nbinding. We decompose the covalent docking process into three main tasks:\nreactive location prediction, covalent reaction prediction, and covalent\ndocking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,\nwe establish baseline performances and demonstrate the effectiveness of the\nbenchmark in accurately predicting interaction sites and modeling the molecular\ntransformations involved in covalent binding. These results confirm the role of\nthe benchmark as a rigorous framework for advancing research in covalent drug\ndesign. It underscores the potential of data-driven approaches to accelerate\nthe discovery of selective covalent inhibitors and addresses critical\nchallenges in therapeutic development.", "comment": "Accepted to KDD 2025 Research Track", "pdf_url": "http://arxiv.org/pdf/2506.21085v1", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.21085v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CovDocker：用任务、数据集和解决方案基准测试共价药物设计", "tldr": "CovDocker是一个用于共价对接的综合基准测试平台，旨在解决现有对接方法难以处理共价键形成的问题。它将共价对接分解为三个任务，并利用先进模型建立了基线性能，以推动共价药物设计研究。", "motivation": "现有的大多数分子对接方法和深度学习方法很难考虑共价键的形成及其相关的结构变化，这限制了它们在预测配体与靶蛋白共价结合模式方面的应用。为了弥补这一空白，本研究引入了一个全面的共价对接基准测试平台CovDocker。", "method": "本研究引入了CovDocker，一个用于共价对接的综合基准测试平台，旨在更好地捕捉共价结合的复杂性。它将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接。通过调整Uni-Mol和Chemformer等最先进的模型，建立了基线性能。", "result": "CovDocker基准测试在准确预测相互作用位点和建模共价结合中涉及的分子转化方面展现了有效性。这些结果证实了该基准测试作为推进共价药物设计研究的严谨框架的作用。", "conclusion": "本研究推出的CovDocker基准测试为共价药物设计提供了一个严谨的框架，并强调了数据驱动方法在加速选择性共价抑制剂发现和解决治疗开发中关键挑战的潜力。", "translation": "分子对接在预测配体与靶蛋白的结合模式方面发挥着关键作用，而共价相互作用，即配体与靶标之间形成共价键，因其强大的、持久的结合性质而特别有价值。然而，大多数现有的对接方法和深度学习方法很难考虑共价键的形成及其相关的结构变化。为了弥补这一空白，我们引入了一个全面的共价对接基准测试平台CovDocker，旨在更好地捕捉共价结合的复杂性。我们将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接。通过调整Uni-Mol和Chemformer等最先进的模型，我们建立了基线性能，并展示了该基准测试在准确预测相互作用位点和建模共价结合中涉及的分子转化方面的有效性。这些结果证实了该基准测试作为推进共价药物设计研究的严谨框架的作用。它强调了数据驱动方法在加速选择性共价抑制剂发现和解决治疗开发中关键挑战的潜力。", "summary": "CovDocker是一个为共价药物设计而开发的综合基准测试平台，旨在解决现有分子对接方法在处理共价键形成方面的不足。该平台将共价对接流程细分为反应位点预测、共价反应预测和共价对接三个核心任务。研究者通过整合并调整Uni-Mol和Chemformer等先进模型，建立了基线性能，并验证了CovDocker在准确预测相互作用位点和模拟分子转化方面的有效性。该基准测试被确认为推动共价药物设计研究的有力框架，有望加速选择性共价抑制剂的发现。", "keywords": "共价药物设计, 分子对接, 基准测试, CovDocker, 深度学习", "comments": "CovDocker的创新之处在于其首次为共价对接提供了一个全面且结构化的基准测试，填补了现有对接方法在处理共价相互作用方面的空白。通过将复杂的共价对接过程分解为多个子任务，并利用先进的深度学习模型建立基线，它为未来的研究提供了一个清晰的评估框架和发展方向。这对于加速共价药物的发现和开发具有重要意义。"}}
{"id": "2506.21382", "title": "Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection", "authors": ["Zhi Zheng", "Bochuan Zhou", "Yuping Song"], "summary": "Cryptocurrency transaction fraud detection faces the dual challenges of\nincreasingly complex transaction patterns and severe class imbalance.\nTraditional methods rely on manual feature engineering and struggle to capture\ntemporal and structural dependencies in transaction networks. This paper\nproposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that\nenhances detection performance through three modules: (1) designing an advanced\ntemporal embedding module that fuses multi-scale time difference features with\nperiodic position encoding; (2) constructing a temporal-aware triple attention\nmechanism that jointly optimizes structural, temporal, and global context\nattention; (3) employing weighted BCE loss to address class imbalance.\nExperiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT\nachieves an AUC of 0.9130, representing a 9.2% improvement over the best\ntraditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This\nmethod not only validates the enhancement effect of temporal awareness and\ntriple attention mechanisms on graph neural networks, but also provides\nfinancial institutions with more reliable fraud detection tools, with its\ndesign principles generalizable to other temporal graph anomaly detection\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21382v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21382v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于加密货币交易欺诈检测的时间感知图注意力网络", "tldr": "提出了一种名为ATGAT的时间感知图注意力网络，通过融合多尺度时间特征和三重注意力机制，有效提升了加密货币交易欺诈检测的性能，并在Elliptic++数据集上取得了显著优于传统方法和GNN的成果。", "motivation": "加密货币交易欺诈检测面临日益复杂的交易模式和严重的类别不平衡挑战。传统方法依赖手动特征工程，难以捕获交易网络中的时间和结构依赖性。", "method": "本文提出了一种增强型时间感知图注意力网络（ATGAT），通过三个模块提升检测性能：(1) 设计先进的时间嵌入模块，融合多尺度时间差特征与周期性位置编码；(2) 构建时间感知三重注意力机制，联合优化结构、时间及全局上下文注意力；(3) 采用加权BCE损失解决类别不平衡问题。", "result": "在Elliptic++加密货币数据集上的实验表明，ATGAT的AUC达到0.9130，比最佳传统方法XGBoost提高了9.2%，比GCN提高了12.0%，比标准GAT提高了10.0%。", "conclusion": "该方法不仅验证了时间感知和三重注意力机制对图神经网络的增强效果，还为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广到其他时间图异常检测任务。", "translation": "加密货币交易欺诈检测面临着日益复杂的交易模式和严重的类别不平衡的双重挑战。传统方法依赖于手动特征工程，并且难以捕获交易网络中的时间和结构依赖关系。本文提出了一种增强型时间感知图注意力网络（ATGAT），通过三个模块提升检测性能：(1) 设计了一个先进的时间嵌入模块，融合了多尺度时间差特征和周期性位置编码；(2) 构建了一个时间感知三重注意力机制，联合优化了结构、时间以及全局上下文注意力；(3) 采用加权BCE损失来解决类别不平衡问题。在Elliptic++加密货币数据集上的实验表明，ATGAT的AUC达到了0.9130，比最佳传统方法XGBoost提高了9.2%，比GCN提高了12.0%，比标准GAT提高了10.0%。该方法不仅验证了时间感知和三重注意力机制对图神经网络的增强效果，还为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广到其他时间图异常检测任务。", "summary": "本文提出了一种名为增强型时间感知图注意力网络（ATGAT）的新方法，旨在解决加密货币交易欺诈检测中复杂的交易模式和类别不平衡问题。ATGAT通过一个先进的时间嵌入模块、一个时间感知三重注意力机制和一个加权BCE损失来提升性能。实验结果显示，ATGAT在Elliptic++数据集上取得了0.9130的AUC，显著优于传统方法和现有图神经网络模型，证明了其在欺诈检测中的有效性和普适性。", "keywords": "Cryptocurrency fraud detection, Graph Attention Network, Temporal awareness, Class imbalance, Anomaly detection", "comments": "该论文通过引入时间感知和三重注意力机制，显著提升了图神经网络在加密货币欺诈检测中的性能，解决了传统方法难以处理的时间和结构依赖性问题，并有效应对了类别不平衡。其模块化设计和在其他时间图异常检测任务中的推广潜力是其重要创新点。"}}
{"id": "2506.21387", "title": "Early Stopping Tabular In-Context Learning", "authors": ["Jaris Küken", "Lennart Purucker", "Frank Hutter"], "summary": "Tabular foundation models have shown strong performance across various\ntabular learning tasks via in-context learning, offering robust generalization\nwithout any downstream finetuning. However, their inference-time costs remain\nhigh, particularly for larger datasets. To address this, we propose\nearly-stopping the in-context learning process. We achieve this by dynamically\nevaluating whether to stop in-context learning after each Transformer encoder\nlayer. Once stopped, we decode the embedding using a pre-trained layer-wise\ndecoder. Experiments across 34 small classification tasks size show that early\nstopping in-context learning accelerates inference by up to x1.3 with\nnegligible degradation in predictive performance. To assess scalability, we\nfurther evaluate our method on five larger classification tasks, achieving\nspeedups of up to x2.2. Our results demonstrate the potential of early exiting\nas an effective and practical strategy for improving the efficiency of tabular\nin-context learning.", "comment": "ICML Workshop Paper", "pdf_url": "http://arxiv.org/pdf/2506.21387v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21387v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "早期停止表格上下文学习", "tldr": "针对表格基础模型在上下文学习中推理成本高的问题，提出了一种早期停止策略，通过动态评估在每个Transformer编码器层后是否停止，显著加速了推理，同时保持了性能。", "motivation": "表格基础模型通过上下文学习在各种表格学习任务中表现出强大的性能，无需任何下游微调即可提供强大的泛化能力。然而，它们的推理时间成本仍然很高，特别是对于更大的数据集。", "method": "提出了一种早期停止上下文学习过程的方法。通过在每个Transformer编码器层之后动态评估是否停止上下文学习来实现。一旦停止，使用预训练的逐层解码器解码嵌入。", "result": "在34个小型分类任务上的实验表明，早期停止上下文学习可以将推理速度提高高达1.3倍，而预测性能的下降可以忽略不计。在5个大型分类任务上，实现了高达2.2倍的加速。", "conclusion": "早期退出是一种有效且实用的策略，可以提高表格上下文学习的效率。", "translation": "表格基础模型通过上下文学习在各种表格学习任务中表现出强大的性能，无需任何下游微调即可提供强大的泛化能力。然而，它们的推理时间成本仍然很高，特别是对于更大的数据集。为了解决这个问题，我们提出了早期停止上下文学习过程。我们通过在每个Transformer编码器层之后动态评估是否停止上下文学习来实现这一点。一旦停止，我们使用预训练的逐层解码器解码嵌入。在34个小型分类任务上的实验表明，早期停止上下文学习可以将推理速度提高高达1.3倍，而预测性能的下降可以忽略不计。为了评估可扩展性，我们进一步在五个大型分类任务上评估了我们的方法，实现了高达2.2倍的加速。我们的结果证明了早期退出作为一种有效且实用的策略，在提高表格上下文学习效率方面的潜力。", "summary": "本文提出了一种早期停止表格上下文学习的方法，旨在解决表格基础模型推理成本高的问题。该方法通过在每个Transformer编码器层后动态判断是否停止，并使用预训练解码器解码，从而显著加速了推理过程。实验证明，在小型数据集上可提速1.3倍且性能无明显下降，在大型数据集上可提速高达2.2倍，显示了其作为提高表格上下文学习效率的有效性。", "keywords": "表格基础模型, 上下文学习, 早期停止, 推理效率, Transformer", "comments": "这项工作通过引入早期停止策略，有效地解决了表格基础模型在推理时的高成本问题，尤其是在处理大型数据集时。其创新点在于动态评估和逐层解码器的结合，实现了效率与性能的良好平衡。这对于实际应用中部署大规模表格上下文学习模型具有重要意义，因为它能显著降低计算资源消耗。"}}
{"id": "2506.21209", "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models", "authors": ["Louis Kerner", "Michel Meintz", "Bihe Zhao", "Franziska Boenisch", "Adam Dziedzic"], "summary": "State-of-the-art text-to-image models like Infinity generate photorealistic\nimages at an unprecedented speed. These models operate in a bitwise\nautoregressive manner over a discrete set of tokens that is practically\ninfinite in size. However, their impressive generative power comes with a\ngrowing risk: as their outputs increasingly populate the Internet, they are\nlikely to be scraped and reused as training data-potentially by the very same\nmodels. This phenomenon has been shown to lead to model collapse, where\nrepeated training on generated content, especially from the models' own\nprevious versions, causes a gradual degradation in performance. A promising\nmitigation strategy is watermarking, which embeds human-imperceptible yet\ndetectable signals into generated images-enabling the identification of\ngenerated content. In this work, we introduce BitMark, a robust bitwise\nwatermarking framework for Infinity. Our method embeds a watermark directly at\nthe bit level of the token stream across multiple scales (also referred to as\nresolutions) during Infinity's image generation process. Our bitwise watermark\nsubtly influences the bits to preserve visual fidelity and generation speed\nwhile remaining robust against a spectrum of removal techniques. Furthermore,\nit exhibits high radioactivity, i.e., when watermarked generated images are\nused to train another image generative model, this second model's outputs will\nalso carry the watermark. The radioactive traces remain detectable even when\nonly fine-tuning diffusion or image autoregressive models on images watermarked\nwith our BitMark. Overall, our approach provides a principled step toward\npreventing model collapse in image generative models by enabling reliable\ndetection of generated outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21209v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21209v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "BitMark for Infinity：比特级自回归图像生成模型的水印技术", "tldr": "针对Infinity等比特级自回归图像生成模型，研究人员提出了BitMark水印框架，通过在比特层面嵌入水印，有效防止模型崩溃，并具有高放射性，即使训练新的模型也能检测到水印。", "motivation": "现有的文本到图像生成模型（如Infinity）能生成逼真的图像，但其输出被重复用作训练数据会导致“模型崩溃”，性能逐渐下降。水印技术是一种有前途的缓解策略，可以识别生成内容。", "method": "本文引入了BitMark，一个针对Infinity的鲁棒比特级水印框架。该方法在Infinity图像生成过程中，直接在令牌流的比特级别跨多个尺度嵌入水印。BitMark通过微妙地影响比特来保持视觉保真度和生成速度。", "result": "BitMark水印对多种移除技术具有鲁棒性。它还表现出高放射性，即当带有水印的生成图像被用于训练另一个图像生成模型时，第二个模型的输出也将携带该水印，即使仅对扩散模型或图像自回归模型进行微调，水印痕迹仍然可检测。", "conclusion": "BitMark提供了一种原则性的方法，通过可靠地检测生成输出，从而有效防止图像生成模型中的模型崩溃。", "translation": "像Infinity这样的最先进文本到图像模型以史无前例的速度生成逼真的图像。这些模型以比特级自回归方式在离散的、实际上无限大小的令牌集上运行。然而，它们令人印象深刻的生成能力伴随着日益增长的风险：随着它们的输出越来越多地充斥互联网，它们很可能被抓取并重新用作训练数据——甚至可能被同一模型使用。这种现象已被证明会导致模型崩溃，即重复使用生成内容进行训练，尤其是来自模型自身先前版本的生成内容，会导致性能逐渐下降。一种有前景的缓解策略是水印技术，它将人眼不可察觉但可检测的信号嵌入到生成图像中，从而能够识别生成内容。在这项工作中，我们引入了BitMark，一个针对Infinity的鲁棒比特级水印框架。我们的方法在Infinity的图像生成过程中，直接在令牌流的比特级别跨多个尺度（也称为分辨率）嵌入水印。我们的比特级水印微妙地影响比特，以保持视觉保真度和生成速度，同时对一系列移除技术保持鲁棒性。此外，它还表现出高放射性，即当带有水印的生成图像被用于训练另一个图像生成模型时，第二个模型的输出也将携带该水印。即使仅使用我们的BitMark水印的图像对扩散或图像自回归模型进行微调，放射性痕迹仍然可检测。总的来说，我们的方法通过实现对生成输出的可靠检测，为防止图像生成模型中的模型崩溃提供了原则性的一步。", "summary": "本文提出了BitMark，一种针对Infinity等比特级自回归图像生成模型的鲁棒水印框架。BitMark在图像生成过程中直接在比特级别嵌入水印，旨在防止模型崩溃——一种由于生成内容被重复用于训练而导致的性能下降现象。该水印技术在保持图像质量和生成速度的同时，对移除攻击具有鲁棒性，并展现出“放射性”，即即使使用带水印的图像训练新的模型，水印仍可被检测到，从而实现对生成内容的可靠识别。", "keywords": "水印, 模型崩溃, 图像生成模型, 比特级自回归, 内容识别", "comments": "这篇论文解决了当前AI生成内容面临的一个重要问题，即“模型崩溃”。BitMark的创新之处在于其比特级的水印嵌入方式，以及其“放射性”特性，这使得水印不仅能识别原始生成内容，还能追踪其被用于训练新模型后的传播，这对于内容溯源和版权保护具有重要意义。该方法兼顾了水印的隐蔽性、鲁棒性和检测性，是防止AI模型生态系统退化的一个关键进展。"}}
{"id": "2506.21233", "title": "ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation", "authors": ["Xiwei Xuan", "Ziquan Deng", "Kwan-Liu Ma"], "summary": "Training-free open-vocabulary semantic segmentation (OVS) aims to segment\nimages given a set of arbitrary textual categories without costly model\nfine-tuning. Existing solutions often explore attention mechanisms of\npre-trained models, such as CLIP, or generate synthetic data and design complex\nretrieval processes to perform OVS. However, their performance is limited by\nthe capability of reliant models or the suboptimal quality of reference sets.\nIn this work, we investigate the largely overlooked data quality problem for\nthis challenging dense scene understanding task, and identify that a\nhigh-quality reference set can significantly benefit training-free OVS. With\nthis observation, we introduce a data-quality-oriented framework, comprising a\ndata pipeline to construct a reference set with well-paired segment-text\nembeddings and a simple similarity-based retrieval to unveil the essential\neffect of data. Remarkably, extensive evaluations on ten benchmark datasets\ndemonstrate that our method outperforms all existing training-free OVS\napproaches, highlighting the importance of data-centric design for advancing\nOVS without training. Our code is available at https://github.com/xiweix/ReME .", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21233v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21233v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ReME：一个用于免训练开放词汇分割的数据中心框架", "tldr": "ReME是一个数据中心框架，通过构建高质量的参考集和简单的相似性检索，显著提升了免训练开放词汇语义分割的性能，超越了现有方法。", "motivation": "现有的免训练开放词汇语义分割（OVS）解决方案性能受限于其依赖模型的能力或参考集的次优质量。本文旨在解决这一被忽视的数据质量问题，以提升OVS性能。", "method": "本文提出了一个以数据质量为导向的框架ReME，包含一个数据管道用于构建具有良好配对片段-文本嵌入的参考集，以及一个简单的基于相似性的检索方法来揭示数据的本质作用。", "result": "在十个基准数据集上进行的广泛评估表明，ReME方法优于所有现有的免训练OVS方法。", "conclusion": "高质量的参考集和以数据为中心的设计对于推进免训练开放词汇语义分割至关重要。", "translation": "免训练开放词汇语义分割（OVS）旨在在不进行昂贵的模型微调的情况下，根据一组任意的文本类别对图像进行分割。现有解决方案通常探索预训练模型（如CLIP）的注意力机制，或者生成合成数据并设计复杂的检索过程来执行OVS。然而，它们的性能受限于所依赖模型的能力或参考集的次优质量。在这项工作中，我们研究了针对这一具有挑战性的密集场景理解任务中，在很大程度上被忽视的数据质量问题，并发现高质量的参考集可以显著有益于免训练OVS。基于这一观察，我们引入了一个以数据质量为导向的框架，包括一个数据管道来构建具有良好配对片段-文本嵌入的参考集，以及一个简单的基于相似性的检索来揭示数据的本质作用。值得注意的是，在十个基准数据集上的广泛评估表明，我们的方法优于所有现有的免训练OVS方法，突显了以数据为中心的设计对于无需训练即可推进OVS的重要性。我们的代码可在https://github.com/xiweix/ReME 获取。", "summary": "该论文提出了ReME，一个用于免训练开放词汇语义分割的数据中心框架。针对现有方法受限于模型能力或参考集质量的问题，ReME关注数据质量，通过构建高质量的参考集和采用简单的相似性检索，显著提升了分割性能。实验证明，ReME在多个基准数据集上超越了现有所有免训练OVS方法，强调了数据中心设计的重要性。", "keywords": "开放词汇分割, 免训练, 数据中心, 语义分割, 参考集", "comments": "本文的创新点在于将研究重心从复杂的模型设计转向了数据质量，特别是在免训练开放词汇分割这一领域，强调了高质量参考集的重要性。这种“数据中心”的方法为OVS提供了一个新的视角和有效的解决方案，其简单而有效的设计理念值得关注。"}}
{"id": "2506.21411", "title": "Distributed Cross-Channel Hierarchical Aggregation for Foundation Models", "authors": ["Aristeidis Tsaris", "Isaac Lyngaas", "John Lagregren", "Mohamed Wahib", "Larry York", "Prasanna Balaprakash", "Dan Lu", "Feiyi Wang", "Xiao Wang"], "summary": "Vision-based scientific foundation models hold significant promise for\nadvancing scientific discovery and innovation. This potential stems from their\nability to aggregate images from diverse sources such as varying physical\ngroundings or data acquisition systems and to learn spatio-temporal\ncorrelations using transformer architectures. However, tokenizing and\naggregating images can be compute-intensive, a challenge not fully addressed by\ncurrent distributed methods. In this work, we introduce the Distributed\nCross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets\nwith a large number of channels across image modalities. Our method is\ncompatible with any model-parallel strategy and any type of vision transformer\narchitecture, significantly improving computational efficiency. We evaluated\nD-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated\nwith tensor parallelism and model sharding, our approach achieved up to a 75%\nreduction in memory usage and more than doubled sustained throughput on up to\n1,024 AMD GPUs on the Frontier Supercomputer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21411v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21411v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "分布式跨通道分层聚合用于基础模型", "tldr": "本文提出D-CHAG，一种分布式跨通道分层聚合方法，旨在解决视觉基础模型中图像标记化和聚合的计算密集问题，显著提高计算效率，减少内存使用，并提高吞吐量。", "motivation": "视觉科学基础模型在科学发现和创新中潜力巨大，但当前分布式方法未能充分解决图像标记化和聚合带来的计算密集挑战。", "method": "本文引入分布式跨通道分层聚合（D-CHAG）方法，专为具有大量通道的图像模态数据集设计。该方法兼容任何模型并行策略和任何类型的视觉Transformer架构，并可与张量并行和模型分片集成。", "result": "D-CHAG在超光谱成像和天气预报任务上进行了评估。当与张量并行和模型分片集成时，在Frontier超级计算机上使用多达1024个AMD GPU时，内存使用量减少高达75%，持续吞吐量增加一倍以上。", "conclusion": "D-CHAG通过提高计算效率、减少内存使用和增加吞吐量，有效解决了视觉基础模型中图像聚合的计算密集挑战，从而推动科学发现和创新。", "translation": "基于视觉的科学基础模型在推动科学发现和创新方面具有巨大潜力。这种潜力源于它们能够聚合来自不同来源（如不同物理基础或数据采集系统）的图像，并使用Transformer架构学习时空相关性。然而，对图像进行标记化和聚合可能计算密集，这是当前分布式方法尚未完全解决的挑战。在这项工作中，我们引入了分布式跨通道分层聚合（D-CHAG）方法，该方法专为具有大量图像模态通道的数据集设计。我们的方法兼容任何模型并行策略和任何类型的视觉Transformer架构，显著提高了计算效率。我们在高光谱成像和天气预报任务上评估了D-CHAG。当与张量并行和模型分片集成时，我们的方法在Frontier超级计算机上使用多达1024个AMD GPU时，内存使用量减少高达75%，持续吞吐量增加了一倍以上。", "summary": "本文提出了一种名为分布式跨通道分层聚合（D-CHAG）的新方法，旨在解决视觉基础模型在处理多通道图像数据时面临的计算密集问题。D-CHAG兼容多种模型并行策略和视觉Transformer架构，并通过实验证明其能显著提高计算效率、减少内存使用，并在大规模GPU集群上提升吞吐量，特别适用于高光谱成像和天气预报等任务。", "keywords": "分布式聚合, 基础模型, 视觉Transformer, 计算效率, 超光谱成像", "comments": "该论文提出了一种创新的分布式聚合方法D-CHAG，有效解决了视觉基础模型在处理多通道图像数据时面临的计算效率和内存瓶颈问题。其兼容性强，可与现有并行策略和Transformer架构结合，并在超大规模GPU集群上展现出显著的性能提升，对推动大规模视觉基础模型的发展具有重要意义。"}}
{"id": "2506.21237", "title": "DiMPLe -- Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation", "authors": ["Umaima Rahman", "Mohammad Yaqub", "Dwarikanath Mahapatra"], "summary": "We introduce DiMPLe (Disentangled Multi-Modal Prompt Learning), a novel\napproach to disentangle invariant and spurious features across vision and\nlanguage modalities in multi-modal learning. Spurious correlations in visual\ndata often hinder out-of-distribution (OOD) performance. Unlike prior methods\nfocusing solely on image features, DiMPLe disentangles features within and\nacross modalities while maintaining consistent alignment, enabling better\ngeneralization to novel classes and robustness to distribution shifts. Our\nmethod combines three key objectives: (1) mutual information minimization\nbetween invariant and spurious features, (2) spurious feature regularization,\nand (3) contrastive learning on invariant features. Extensive experiments\ndemonstrate DiMPLe demonstrates superior performance compared to CoOp-OOD, when\naveraged across 11 diverse datasets, and achieves absolute gains of 15.27 in\nbase class accuracy and 44.31 in novel class accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21237v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21237v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DiMPLe -- 解耦多模态提示学习：通过不变性和虚假特征分离增强分布外对齐", "tldr": "DiMPLe通过解耦视觉和语言模态中的不变性和虚假特征，提高了多模态学习的分布外泛化能力。", "motivation": "视觉数据中的虚假相关性阻碍了分布外（OOD）性能，现有方法主要关注图像特征，未能有效解决跨模态的特征解耦问题。", "method": "DiMPLe结合了三个关键目标：1) 不变特征和虚假特征之间的互信息最小化；2) 虚假特征正则化；3) 不变特征上的对比学习。它在模态内和跨模态解耦特征，同时保持一致的对齐。", "result": "DiMPLe在11个不同数据集上的平均表现优于CoOp-OOD，在基类准确率上取得了15.27的绝对增益，在新类准确率上取得了44.31的绝对增益。", "conclusion": "DiMPLe通过有效解耦多模态数据中的不变性和虚假特征，显著提升了模型在分布外场景下的泛化能力和鲁棒性。", "translation": "我们引入了DiMPLe（解耦多模态提示学习），这是一种在多模态学习中解耦视觉和语言模态中的不变特征和虚假特征的新方法。视觉数据中的虚假相关性通常会阻碍分布外（OOD）性能。与以往仅关注图像特征的方法不同，DiMPLe在模态内部和跨模态解耦特征，同时保持一致的对齐，从而能够更好地泛化到新类别并对分布偏移具有鲁棒性。我们的方法结合了三个关键目标：（1）不变特征和虚假特征之间的互信息最小化，（2）虚假特征正则化，以及（3）不变特征上的对比学习。广泛的实验表明，DiMPLe在11个不同数据集上的平均表现优于CoOp-OOD，并在基类准确率上取得了15.27的绝对增益，在新类准确率上取得了44.31的绝对增益。", "summary": "本文提出了DiMPLe，一种解耦多模态提示学习方法，旨在解决多模态学习中视觉和语言模态间的虚假相关性问题。DiMPLe通过在模态内和跨模态分离不变和虚假特征，并结合互信息最小化、虚假特征正则化和对比学习，显著提升了模型在分布外场景下对新类别的泛化能力和对分布偏移的鲁棒性。实验证明DiMPLe在多项数据集上表现优异，尤其在新旧类别准确率上取得显著提升。", "keywords": "多模态学习, 特征解耦, 分布外泛化, 提示学习, 虚假相关性", "comments": "DiMPLe的创新之处在于其首次提出了在多模态学习中同时在模态内和跨模态解耦不变和虚假特征的方法，这对于提升模型在复杂真实世界场景中的分布外泛化能力和鲁棒性至关重要。其三项关键目标的设计也体现了对特征鲁棒性和判别性的深刻理解。"}}
{"id": "2506.21429", "title": "Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort", "authors": ["Franco Rugolon", "Thomas Jack Samuels", "Stephan Hau", "Lennart Högman"], "summary": "This study investigates the efficacy of using multimodal machine learning\ntechniques to detect deception in dyadic interactions, focusing on the\nintegration of data from both the deceiver and the deceived. We compare early\nand late fusion approaches, utilizing audio and video data - specifically,\nAction Units and gaze information - across all possible combinations of\nmodalities and participants. Our dataset, newly collected from Swedish native\nspeakers engaged in truth or lie scenarios on emotionally relevant topics,\nserves as the basis for our analysis. The results demonstrate that\nincorporating both speech and facial information yields superior performance\ncompared to single-modality approaches. Moreover, including data from both\nparticipants significantly enhances deception detection accuracy, with the best\nperformance (71%) achieved using a late fusion strategy applied to both\nmodalities and participants. These findings align with psychological theories\nsuggesting differential control of facial and vocal expressions during initial\ninteractions. As the first study of its kind on a Scandinavian cohort, this\nresearch lays the groundwork for future investigations into dyadic\ninteractions, particularly within psychotherapy settings.", "comment": "40 pages, 2 figures, 2 tables. To be submitted in Behavior Research\n  Methods", "pdf_url": "http://arxiv.org/pdf/2506.21429v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21429v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用多模态机器学习在两人交流中检测欺骗：一项针对瑞典人群的研究", "tldr": "本研究使用多模态机器学习（音频、视频、动作单元、凝视）在瑞典两人互动中检测欺骗，发现结合欺骗者和被欺骗者的数据以及多模态（特别是晚期融合）显著提高了检测准确性。", "motivation": "本研究旨在调查使用多模态机器学习技术在两人互动中检测欺骗的有效性，重点是整合来自欺骗者和被欺骗者的数据。", "method": "研究采用多模态机器学习方法，比较了早期和晚期融合策略。数据来源包括音频和视频信息（特别是动作单元和凝视信息），并结合了欺骗者和被欺骗者双方的数据。数据集是新收集的，来自参与情感相关话题的真实或谎言情景的瑞典母语使用者。", "result": "结果表明，结合语音和面部信息比单一模态方法表现更优。此外，包含来自两位参与者的数据显著提高了欺骗检测准确性，最佳性能（71%）是通过对两种模态和参与者应用晚期融合策略实现的。", "conclusion": "这些发现与心理学理论一致，表明在初次互动中面部和声音表达的差异控制。作为针对斯堪的纳维亚人群的首次同类研究，这项研究为未来对两人互动（特别是在心理治疗环境中）的调查奠定了基础。", "translation": "这项研究调查了使用多模态机器学习技术在两人互动中检测欺骗的有效性，重点是整合来自欺骗者和被欺骗者的数据。我们比较了早期和晚期融合方法，利用音频和视频数据——特别是动作单元和凝视信息——跨所有可能的模态和参与者组合。我们的数据集是新收集的，来自参与情感相关话题的真实或谎言情景的瑞典母语使用者，作为我们分析的基础。结果表明，与单模态方法相比，结合语音和面部信息能产生更优越的性能。此外，包含来自两位参与者的数据显著提高了欺骗检测的准确性，最佳性能（71%）是通过对两种模态和参与者应用晚期融合策略实现的。这些发现与心理学理论一致，这些理论表明在初次互动中面部和声音表达的差异控制。作为针对斯堪的纳维亚人群的首次同类研究，这项研究为未来对两人互动（特别是在心理治疗环境中）的调查奠定了基础。", "summary": "本研究探索了在两人互动中应用多模态机器学习进行欺骗检测，整合了来自欺骗者和被欺骗者双方的音频（语音）和视频（动作单元、凝视）数据。研究使用了新的瑞典数据集，并比较了早期和晚期融合方法。结果表明，结合语音和面部信息以及两位参与者的数据显著提高了检测准确性，其中晚期融合方法达到了71%的最佳性能。这些发现支持了关于表达控制的心理学理论，并为未来在两人互动（特别是心理治疗）背景下的研究奠定了基础。", "keywords": "欺骗检测, 多模态机器学习, 两人互动, 音视频分析, 晚期融合", "comments": "该论文具有创新性，因为它首次在斯堪的纳维亚人群中研究多模态欺骗检测，并强调了在两人交流中整合双方参与者数据的重要性，这是一种细致入微的方法。该任务达到了71%的准确率，是一个很好的基准。其在心理治疗环境中的潜在应用尤其值得关注。"}}
{"id": "2506.21249", "title": "Temporal Rate Reduction Clustering for Human Motion Segmentation", "authors": ["Xianghan Meng", "Zhengyu Tong", "Zhiyuan Huang", "Chun-Guang Li"], "summary": "Human Motion Segmentation (HMS), which aims to partition videos into\nnon-overlapping human motions, has attracted increasing research attention\nrecently. Existing approaches for HMS are mainly dominated by subspace\nclustering methods, which are grounded on the assumption that high-dimensional\ntemporal data align with a Union-of-Subspaces (UoS) distribution. However, the\nframes in video capturing complex human motions with cluttered backgrounds may\nnot align well with the UoS distribution. In this paper, we propose a novel\napproach for HMS, named Temporal Rate Reduction Clustering\n($\\text{TR}^2\\text{C}$), which jointly learns structured representations and\naffinity to segment the frame sequences in video. Specifically, the structured\nrepresentations learned by $\\text{TR}^2\\text{C}$ maintain temporally consistent\nand align well with a UoS structure, which is favorable for the HMS task. We\nconduct extensive experiments on five benchmark HMS datasets and achieve\nstate-of-the-art performances with different feature extractors.", "comment": "The paper is accepted by ICCV 2025. The first two authors are equally\n  contributed", "pdf_url": "http://arxiv.org/pdf/2506.21249v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21249v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于人体运动分割的时间速率降低聚类", "tldr": "针对人体运动分割，本文提出了一种名为TR2C的新方法，通过学习结构化表示和亲和力来解决传统子空间聚类在复杂背景下UoS假设不佳的问题，并在基准数据集上取得了最先进的性能。", "motivation": "现有的人体运动分割（HMS）方法主要由子空间聚类方法主导，这些方法基于高维时间数据与子空间联合（UoS）分布对齐的假设。然而，在具有杂乱背景的视频中捕获复杂人体运动的帧可能无法很好地与UoS分布对齐，这限制了现有方法的性能。", "method": "本文提出了一种新颖的HMS方法，命名为时间速率降低聚类（Temporal Rate Reduction Clustering, TR2C）。TR2C通过联合学习结构化表示和亲和力来分割视频中的帧序列。具体而言，TR2C学习到的结构化表示能够保持时间上的一致性，并良好地与UoS结构对齐，这有利于HMS任务。", "result": "TR2C在五个基准HMS数据集上进行了广泛的实验，并使用不同的特征提取器取得了最先进的性能。", "conclusion": "TR2C通过学习时间一致且与UoS结构良好对齐的结构化表示，有效解决了复杂背景下人体运动分割的挑战，并在多个基准数据集上展现出卓越的性能。", "translation": "人体运动分割（HMS）旨在将视频划分为非重叠的人体运动，最近引起了越来越多的研究关注。现有的HMS方法主要由子空间聚类方法主导，这些方法基于高维时间数据与子空间联合（UoS）分布对齐的假设。然而，在具有杂乱背景的视频中捕获复杂人体运动的帧可能无法很好地与UoS分布对齐。在本文中，我们提出了一种新颖的HMS方法，命名为时间速率降低聚类（Temporal Rate Reduction Clustering, TR2C），该方法联合学习结构化表示和亲和力以分割视频中的帧序列。具体而言，TR2C学习到的结构化表示保持时间上的一致性，并良好地与UoS结构对齐，这有利于HMS任务。我们在五个基准HMS数据集上进行了广泛的实验，并使用不同的特征提取器取得了最先进的性能。", "summary": "本文提出了一种新颖的人体运动分割（HMS）方法——时间速率降低聚类（TR2C），旨在解决传统子空间聚类方法在复杂背景下UoS假设失效的问题。TR2C通过联合学习时间一致且与UoS结构良好对齐的结构化表示以及亲和力，实现对视频帧序列的有效分割。实验结果表明，TR2C在多个基准HMS数据集上均达到了最先进的性能。", "keywords": "人体运动分割, 时间速率降低聚类, 子空间聚类, 结构化表示, 视频分析", "comments": "这篇论文的创新点在于提出了TR2C，它通过联合学习结构化表示和亲和力来克服传统子空间聚类方法在处理复杂人体运动和杂乱背景时UoS假设失效的局限性。其核心思想是学习到时间一致且与UoS结构良好对齐的表示，这对于提高人体运动分割的准确性至关重要。论文通过在多个基准数据集上取得最先进的性能，验证了其方法的有效性和普适性。"}}
{"id": "2506.21260", "title": "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic", "authors": ["Munish Monga", "Vishal Chudasama", "Pankaj Wasnik", "Biplab Banerjee"], "summary": "Real-world object detection systems, such as those in autonomous driving and\nsurveillance, must continuously learn new object categories and simultaneously\nadapt to changing environmental conditions. Existing approaches, Class\nIncremental Object Detection (CIOD) and Domain Incremental Object Detection\n(DIOD) only address one aspect of this challenge. CIOD struggles in unseen\ndomains, while DIOD suffers from catastrophic forgetting when learning new\nclasses, limiting their real-world applicability. To overcome these\nlimitations, we introduce Dual Incremental Object Detection (DuIOD), a more\npractical setting that simultaneously handles class and domain shifts in an\nexemplar-free manner. We propose DuET, a Task Arithmetic-based model merging\nframework that enables stable incremental learning while mitigating sign\nconflicts through a novel Directional Consistency Loss. Unlike prior methods,\nDuET is detector-agnostic, allowing models like YOLO11 and RT-DETR to function\nas real-time incremental object detectors. To comprehensively evaluate both\nretention and adaptation, we introduce the Retention-Adaptability Index (RAI),\nwhich combines the Average Retention Index (Avg RI) for catastrophic forgetting\nand the Average Generalization Index for domain adaptability into a common\nground. Extensive experiments on the Pascal Series and Diverse Weather Series\ndemonstrate DuET's effectiveness, achieving a +13.12% RAI improvement while\npreserving 89.3% Avg RI on the Pascal Series (4 tasks), as well as a +11.39%\nRAI improvement with 88.57% Avg RI on the Diverse Weather Series (3 tasks),\noutperforming existing methods.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21260v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21260v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DuET：通过无样本任务算术的双增量目标检测", "tldr": "DuET提出了一种双增量目标检测（DuIOD）的新范式，旨在同时处理类别和域的增量学习，且无需样本。它引入了基于任务算术的模型合并框架DuET和新的评估指标RAI，实验证明其在保留能力和适应性方面均优于现有方法。", "motivation": "现有目标检测系统在现实世界中需要持续学习新类别并适应环境变化，但现有的类别增量目标检测（CIOD）和域增量目标检测（DIOD）方法各自存在局限性：CIOD在未见过的域中表现不佳，而DIOD在学习新类别时面临灾难性遗忘。为了解决这些问题，论文提出了更实用的双增量目标检测（DuIOD）设置，以同时处理类别和域的变化。", "method": "论文提出了DuET，一个基于任务算术的模型合并框架，该框架通过新颖的方向一致性损失（Directional Consistency Loss）来缓解符号冲突，从而实现稳定的增量学习。DuET是检测器无关的，能够将YOLO11和RT-DETR等模型用作实时增量目标检测器。为了全面评估模型的保留能力和适应性，论文引入了保留-适应性指数（RAI），该指数结合了用于灾难性遗忘的平均保留指数（Avg RI）和用于域适应性的平均泛化指数。", "result": "在Pascal系列（4个任务）和多样化天气系列（3个任务）上的大量实验表明DuET的有效性。DuET在Pascal系列上实现了+13.12%的RAI提升，同时保持了89.3%的Avg RI；在多样化天气系列上实现了+11.39%的RAI提升，同时保持了88.57%的Avg RI，均优于现有方法。", "conclusion": "DuET有效解决了双增量目标检测的挑战，通过其基于任务算术的框架和新评估指标，在同时处理类别和域变化方面表现出色，显著优于现有方法。", "translation": "现实世界中的目标检测系统，例如自动驾驶和监控系统中的，必须持续学习新的目标类别并同时适应不断变化的环境条件。现有的方法，即类别增量目标检测（CIOD）和域增量目标检测（DIOD）仅解决了这一挑战的一个方面。CIOD在未见过的域中表现不佳，而DIOD在学习新类别时遭受灾难性遗忘，这限制了它们在现实世界中的适用性。为了克服这些限制，我们引入了双增量目标检测（DuIOD），这是一种更实用的设置，可以以无样本的方式同时处理类别和域的漂移。我们提出了DuET，一个基于任务算术的模型合并框架，该框架通过新颖的方向一致性损失（Directional Consistency Loss）来缓解符号冲突，从而实现稳定的增量学习。与现有方法不同，DuET是检测器无关的，允许YOLO11和RT-DETR等模型作为实时增量目标检测器运行。为了全面评估保留能力和适应性，我们引入了保留-适应性指数（RAI），该指数将用于灾难性遗忘的平均保留指数（Avg RI）和用于域适应性的平均泛化指数结合到一个共同的基础中。在Pascal系列和多样化天气系列上的大量实验证明了DuET的有效性，在Pascal系列（4个任务）上实现了+13.12%的RAI提升，同时保持了89.3%的Avg RI，以及在多样化天气系列（3个任务）上实现了+11.39%的RAI提升，同时保持了88.57%的Avg RI，均优于现有方法。", "summary": "DuET解决了现有增量目标检测方法在同时处理类别和域变化方面的局限性。论文提出了双增量目标检测（DuIOD）这一更实用的设置，并引入了DuET，一个基于任务算术、检测器无关的模型合并框架，该框架利用方向一致性损失来稳定学习。此外，还提出了保留-适应性指数（RAI）以全面评估模型性能。实验结果表明，DuET在多项任务中显著提高了RAI，并有效保持了对旧知识的保留能力，性能优于现有方法。", "keywords": "双增量目标检测, 任务算术, 无样本, 灾难性遗忘, 域适应", "comments": "这篇论文通过引入双增量目标检测（DuIOD）这一新颖且更符合现实的设定，解决了当前增量目标检测领域的关键挑战。DuET作为其提出的解决方案，其基于任务算术的模型合并框架具有创新性，并且其检测器无关的特性大大增强了实用性。此外，引入保留-适应性指数（RAI）为全面评估增量学习系统提供了一个新的、更完善的度量标准，具有重要的理论和实践价值。"}}
{"id": "2506.21461", "title": "A Keyword-Based Technique to Evaluate Broad Question Answer Script", "authors": ["Tamim Al Mahmud", "Md Gulzar Hussain", "Sumaiya Kabir", "Hasnain Ahmad", "Mahmudus Sobhan"], "summary": "Evaluation is the method of assessing and determining the educational system\nthrough various techniques such as verbal or viva-voice test, subjective or\nobjective written test. This paper presents an efficient solution to evaluate\nthe subjective answer script electronically. In this paper, we proposed and\nimplemented an integrated system that examines and evaluates the written answer\nscript. This article focuses on finding the keywords from the answer script and\nthen compares them with the keywords that have been parsed from both open and\nclosed domain. The system also checks the grammatical and spelling errors in\nthe answer script. Our proposed system tested with answer scripts of 100\nstudents and gives precision score 0.91.", "comment": "ACM Conference Proceedings (9 Pages)", "pdf_url": "http://arxiv.org/pdf/2506.21461v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21461v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "评估开放式问题答案的基于关键词的技术", "tldr": "本文提出了一种基于关键词的电子系统，用于自动评估主观题的答案，并检查语法和拼写错误，在100份答卷上取得了0.91的准确率。", "motivation": "旨在提供一种高效的电子解决方案，用于评估主观题的答卷，以改进教育系统的评估方法。", "method": "提出并实现了一个集成系统，通过从答卷中提取关键词，并与从开放和封闭领域解析的关键词进行比较来评估书面答卷。该系统还检查答卷中的语法和拼写错误。", "result": "该系统在100名学生的答卷上进行了测试，并获得了0.91的准确率。", "conclusion": "该系统能够有效且高精度地评估主观题答卷。", "translation": "评估是通过各种技术（如口头或口试、主观或客观书面测试）评估和确定教育系统的方法。本文提出了一种高效的电子评估主观答卷的解决方案。在本文中，我们提出并实现了一个检查和评估书面答卷的集成系统。本文重点关注从答卷中查找关键词，然后将其与从开放域和封闭域解析的关键词进行比较。该系统还检查答卷中的语法和拼写错误。我们提出的系统在100名学生的答卷上进行了测试，并给出了0.91的准确率。", "summary": "本文提出了一种基于关键词的电子系统，旨在高效评估主观题的答卷。该系统通过提取答卷中的关键词并与预设关键词进行比较来评分，同时还检查语法和拼写错误。实验结果显示，该系统在处理100份学生答卷时达到了0.91的准确率，证明了其在自动化评估方面的有效性。", "keywords": "关键词评估, 主观题, 电子评估, 自动评分, 答卷分析", "comments": "该研究提出了一种实用的方法来自动化主观题的评估，这在传统教育评估中是一个挑战。其创新点在于结合了关键词匹配和语法拼写检查，提高了评估的效率和准确性。然而，该方法可能在处理语义复杂或表达方式多样的答案时面临局限性，且0.91的准确率虽高，但仍需考虑其在不同学科和问题类型上的泛化能力。"}}
{"id": "2506.21270", "title": "Video Virtual Try-on with Conditional Diffusion Transformer Inpainter", "authors": ["Cheng Zou", "Senlin Cheng", "Bolei Xu", "Dandan Zheng", "Xiaobo Li", "Jingdong Chen", "Ming Yang"], "summary": "Video virtual try-on aims to naturally fit a garment to a target person in\nconsecutive video frames. It is a challenging task, on the one hand, the output\nvideo should be in good spatial-temporal consistency, on the other hand, the\ndetails of the given garment need to be preserved well in all the frames.\nNaively using image-based try-on methods frame by frame can get poor results\ndue to severe inconsistency. Recent diffusion-based video try-on methods,\nthough very few, happen to coincide with a similar solution: inserting temporal\nattention into image-based try-on model to adapt it for video try-on task,\nwhich have shown improvements but there still exist inconsistency problems. In\nthis paper, we propose ViTI (Video Try-on Inpainter), formulate and implement\nvideo virtual try-on as a conditional video inpainting task, which is different\nfrom previous methods. In this way, we start with a video generation problem\ninstead of an image-based try-on problem, which from the beginning has a better\nspatial-temporal consistency. Specifically, at first we build a video\ninpainting framework based on Diffusion Transformer with full 3D\nspatial-temporal attention, and then we progressively adapt it for video\ngarment inpainting, with a collection of masking strategies and multi-stage\ntraining. After these steps, the model can inpaint the masked garment area with\nappropriate garment pixels according to the prompt with good spatial-temporal\nconsistency. Finally, as other try-on methods, garment condition is added to\nthe model to make sure the inpainted garment appearance and details are as\nexpected. Both quantitative and qualitative experimental results show that ViTI\nis superior to previous works.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21270v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21270v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于条件扩散Transformer修复器的视频虚拟试穿", "tldr": "本文提出ViTI，一个将视频虚拟试穿视为条件视频修复任务的新方法，通过扩散Transformer实现，解决了现有方法的时空不一致问题并提升了试穿效果。", "motivation": "视频虚拟试穿是一个挑战性任务，要求输出视频具有良好的时空一致性，并保留服装细节。现有图像基或扩散基方法在帧间一致性上存在问题。", "method": "本文提出ViTI（Video Try-on Inpainter），将视频虚拟试穿建模为条件视频修复任务。首先，构建基于具有3D时空注意力的扩散Transformer的视频修复框架；然后，通过一系列遮罩策略和多阶段训练，逐步将其应用于视频服装修复，以在遮罩区域填充适当的服装像素并保持良好的时空一致性；最后，加入服装条件以确保修复后的服装外观和细节符合预期。", "result": "定量和定性实验结果表明，ViTI优于现有工作。", "conclusion": "ViTI通过将视频虚拟试穿重新定义为条件视频修复任务，并利用扩散Transformer和多阶段训练，有效地解决了时空一致性问题，显著提升了视频虚拟试穿的效果。", "translation": "视频虚拟试穿旨在将一件服装自然地适配到连续视频帧中的目标人物。这是一项具有挑战性的任务，一方面，输出视频应具有良好的时空一致性；另一方面，给定服装的细节需要在所有帧中得到很好的保留。简单地逐帧使用基于图像的试穿方法会因为严重的不一致性而得到糟糕的结果。最近基于扩散的视频试穿方法，尽管数量很少，却不约而同地采用了一种相似的解决方案：在基于图像的试穿模型中插入时间注意力以使其适应视频试穿任务，这虽然有所改进，但仍然存在不一致性问题。在本文中，我们提出了ViTI（Video Try-on Inpainter），将视频虚拟试穿表述并实现为条件视频修复任务，这与以往的方法不同。通过这种方式，我们从视频生成问题而不是基于图像的试穿问题开始，这从一开始就具有更好的时空一致性。具体来说，我们首先构建了一个基于具有完整3D时空注意力的扩散Transformer的视频修复框架，然后通过一系列遮罩策略和多阶段训练，逐步将其应用于视频服装修复。经过这些步骤后，模型可以根据提示使用适当的服装像素修复遮罩的服装区域，并具有良好的时空一致性。最后，像其他试穿方法一样，将服装条件添加到模型中，以确保修复后的服装外观和细节符合预期。定量和定性实验结果均表明ViTI优于现有工作。", "summary": "本文提出了一种名为ViTI的视频虚拟试穿新方法，将该任务重新定义为条件视频修复。针对现有方法在时空一致性上的不足，ViTI基于扩散Transformer构建了一个具有完整3D时空注意力的视频修复框架，并通过多阶段训练和遮罩策略逐步适应服装修复。该方法从视频生成角度出发，有效解决了帧间不一致性问题，并能根据服装条件生成高质量的试穿视频，实验证明其性能优于现有技术。", "keywords": "视频虚拟试穿, 条件扩散Transformer, 视频修复, 时空一致性, ViTI", "comments": "ViTI的创新之处在于将视频虚拟试穿任务从传统的图像或简单视频适应方法，转换到更根本的条件视频修复问题。通过利用扩散Transformer的3D时空注意力，它从根本上解决了视频任务中常见的时空不一致性问题，这比简单地添加时间注意力更具优势。多阶段训练和遮罩策略的结合也显示了其在处理复杂视频内容方面的鲁棒性。"}}
{"id": "2506.21465", "title": "Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage", "authors": ["Gavin Lee Goodship", "Luis Miralles-Pechuan", "Stephen O'Sullivan"], "summary": "Extended Stability Runge-Kutta (ESRK) methods are crucial for solving\nlarge-scale computational problems in science and engineering, including\nweather forecasting, aerodynamic analysis, and complex biological modelling.\nHowever, balancing accuracy, stability, and computational efficiency remains\nchallenging, particularly for high-order, low-storage schemes. This study\nintroduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)\napproach for automated heuristic discovery, optimising low-storage ESRK\nmethods. Unlike traditional approaches that rely on manually designed\nheuristics or exhaustive numerical searches, our method leverages GA-driven\nmutations for search-space exploration and an RL-inspired state transition\nmechanism to refine heuristic selection dynamically. This enables systematic\nparameter reduction, preserving fourth-order accuracy while significantly\nimproving computational efficiency.The proposed GA-RL heuristic optimisation\nframework is validated through rigorous testing on benchmark problems,\nincluding the 1D and 2D Brusselator systems and the steady-state Navier-Stokes\nequations. The best-performing heuristic achieves a 25\\% reduction in IPOPT\nruntime compared to traditional ESRK optimisation processes while maintaining\nnumerical stability and accuracy. These findings demonstrate the potential of\nadaptive heuristic discovery to improve resource efficiency in high-fidelity\nsimulations and broaden the applicability of low-storage Runge-Kutta methods in\nreal-world computational fluid dynamics, physics simulations, and other\ndemanding fields. This work establishes a new paradigm in heuristic\noptimisation for numerical methods, opening pathways for further exploration\nusing Deep RL and AutoML-based heuristic search", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21465v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21465v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "优化四阶龙格-库塔方法：一种提高效率和降低存储的动态启发式方法", "tldr": "本研究提出一种结合遗传算法和强化学习的混合方法，用于自动发现启发式规则，以优化低存储的四阶Runge-Kutta方法，显著提高计算效率并保持精度，超越了传统方法。", "motivation": "扩展稳定性龙格-库塔（ESRK）方法在解决大规模计算问题中至关重要，但平衡精度、稳定性和计算效率，特别是对于高阶、低存储方案，仍然是一个挑战。传统方法依赖于手动设计的启发式或穷举数值搜索，效率低下。", "method": "本研究引入了一种混合遗传算法（GA）和强化学习（RL）的方法，用于自动化启发式发现，以优化低存储的ESRK方法。该方法利用GA驱动的变异进行搜索空间探索，并采用受RL启发的状态转换机制动态改进启发式选择，从而实现系统性地减少参数，同时保持四阶精度。", "result": "最佳启发式方法在IPOPT运行时方面比传统ESRK优化过程减少了25%，同时保持了数值稳定性和精度。", "conclusion": "自适应启发式发现能够显著提高高精度模拟的资源效率，并扩大低存储Runge-Kutta方法在实际应用中的适用性。这项工作为数值方法的启发式优化建立了新范式，并为未来探索开辟了途径。", "translation": "扩展稳定性龙格-库塔（ESRK）方法对于解决科学和工程中的大规模计算问题至关重要，包括天气预报、气动分析和复杂的生物建模。然而，平衡精度、稳定性与计算效率仍然是一个挑战，特别是对于高阶、低存储方案。本研究引入了一种混合遗传算法（GA）和强化学习（RL）方法，用于自动化启发式发现，以优化低存储的ESRK方法。与依赖手动设计启发式或穷举数值搜索的传统方法不同，我们的方法利用GA驱动的变异进行搜索空间探索，并采用受RL启发的状态转换机制动态改进启发式选择。这使得系统性地减少参数成为可能，同时保持四阶精度并显著提高计算效率。所提出的GA-RL启发式优化框架通过对基准问题进行严格测试得到验证，包括一维和二维Brusselator系统以及稳态Navier-Stokes方程。与传统ESRK优化过程相比，性能最佳的启发式方法在IPOPT运行时减少了25%，同时保持了数值稳定性和精度。这些发现证明了自适应启发式发现提高高保真模拟资源效率的潜力，并拓宽了低存储龙格-库塔方法在实际计算流体动力学、物理模拟和其他高要求领域的适用性。这项工作为数值方法的启发式优化建立了新范式，为使用深度强化学习和基于AutoML的启发式搜索开辟了进一步探索的途径。", "summary": "本文提出一种新颖的混合遗传算法（GA）和强化学习（RL）方法，用于优化低存储的四阶Runge-Kutta（ESRK）方法，以解决大规模计算中平衡精度、稳定性与效率的挑战。该方法通过GA进行搜索空间探索，并利用RL动态优化启发式规则，实现了参数的系统性减少，同时保持四阶精度。实验结果表明，该方法使计算运行时减少了25%，显著提高了效率，并为数值方法的启发式优化开辟了新途径。", "keywords": "龙格-库塔方法, 启发式优化, 遗传算法, 强化学习, 计算效率", "comments": "这项研究通过结合遗传算法和强化学习，为数值方法的启发式优化提供了一个创新的自动化范式。它克服了传统手动设计或穷举搜索的局限性，在提高计算效率的同时保持了高精度，对于大规模科学和工程模拟具有重要意义。该方法在资源效率方面的提升，特别是25%的运行时减少，展示了其在实际应用中的巨大潜力。此外，它为未来结合深度强化学习和AutoML的启发式搜索奠定了基础，具有前瞻性。"}}
{"id": "2506.21276", "title": "WordCon: Word-level Typography Control in Scene Text Rendering", "authors": ["Wenda Shi", "Yiren Song", "Zihan Rao", "Dengming Zhang", "Jiaming Liu", "Xingxing Zou"], "summary": "Achieving precise word-level typography control within generated images\nremains a persistent challenge. To address it, we newly construct a word-level\ncontrolled scene text dataset and introduce the Text-Image Alignment (TIA)\nframework. This framework leverages cross-modal correspondence between text and\nlocal image regions provided by grounding models to enhance the Text-to-Image\n(T2I) model training. Furthermore, we propose WordCon, a hybrid\nparameter-efficient fine-tuning (PEFT) method. WordCon reparameterizes\nselective key parameters, improving both efficiency and portability. This\nallows seamless integration into diverse pipelines, including artistic text\nrendering, text editing, and image-conditioned text rendering. To further\nenhance controllability, the masked loss at the latent level is applied to\nguide the model to concentrate on learning the text region in the image, and\nthe joint-attention loss provides feature-level supervision to promote\ndisentanglement between different words. Both qualitative and quantitative\nresults demonstrate the superiority of our method to the state of the art. The\ndatasets and source code will be available for academic use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21276v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21276v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "WordCon：场景文本渲染中的词级排版控制", "tldr": "提出了WordCon，一种混合PEFT方法，用于解决生成图像中词级排版控制的挑战，通过新的数据集、TIA框架和特定损失函数，实现了优于现有技术的效果。", "motivation": "在生成的图像中实现精确的词级排版控制仍然是一个持续存在的挑战。", "method": "1. 构建了一个新的词级受控场景文本数据集。2. 引入了文本-图像对齐（TIA）框架，该框架利用接地模型提供的文本和局部图像区域之间的跨模态对应关系来增强文本到图像（T2I）模型的训练。3. 提出了WordCon，一种混合参数高效微调（PEFT）方法，通过重新参数化选择性关键参数来提高效率和可移植性。4. 应用潜在层面的掩蔽损失来引导模型专注于学习图像中的文本区域。5. 应用联合注意力损失提供特征级监督，以促进不同单词之间的解耦。", "result": "定性和定量结果均表明该方法优于现有技术。", "conclusion": "通过构建新的数据集、引入TIA框架和提出WordCon（一种混合PEFT方法），并结合特定的损失函数，成功解决了生成图像中词级排版控制的挑战，并实现了最先进的性能。", "translation": "在生成的图像中实现精确的词级排版控制仍然是一个持续存在的挑战。为了解决这个问题，我们新建了一个词级受控场景文本数据集，并引入了文本-图像对齐（TIA）框架。该框架利用接地模型提供的文本和局部图像区域之间的跨模态对应关系，以增强文本到图像（T2I）模型的训练。此外，我们提出了WordCon，一种混合参数高效微调（PEFT）方法。WordCon重新参数化选择性关键参数，提高了效率和可移植性。这使得它能够无缝集成到各种管道中，包括艺术文本渲染、文本编辑和图像条件文本渲染。为了进一步增强可控性，应用了潜在层面的掩蔽损失来引导模型集中学习图像中的文本区域，并且联合注意力损失提供了特征级监督，以促进不同单词之间的解耦。定性和定量结果均表明我们的方法优于现有技术。数据集和源代码将可用于学术用途。", "summary": "本文旨在解决生成图像中词级排版控制的难题。为此，作者构建了一个新的词级受控场景文本数据集，并提出了文本-图像对齐（TIA）框架，该框架利用跨模态对应关系增强T2I模型训练。此外，还引入了WordCon，一种混合参数高效微调（PEFT）方法，通过重新参数化关键参数提升效率和可移植性，使其可集成到多种文本渲染和编辑流程中。为了增强控制力，模型还采用了潜在层面的掩蔽损失和联合注意力损失。实验结果表明，该方法在定性和定量上均优于现有技术。", "keywords": "词级排版控制, 场景文本渲染, 参数高效微调, 文本-图像对齐, WordCon", "comments": "这篇论文通过引入新的数据集、创新的TIA框架以及高效的WordCon PEFT方法，显著提升了生成图像中词级排版控制的能力。其对效率和可移植性的关注，以及通过特定损失函数（掩蔽损失和联合注意力损失）增强可控性的设计，都体现了其创新性。该方法在艺术文本渲染、文本编辑等领域的潜在应用价值很高，解决了文本生成领域的一个重要痛点。"}}
{"id": "2506.21502", "title": "Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems", "authors": ["Francesco Vitale", "Nicola Dall'Ora", "Sebastiano Gaiardelli", "Enrico Fraccaroli", "Nicola Mazzocca", "Franco Fummi"], "summary": "Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring\nsystem dependability and operational efficiency by accurately detecting\nanomalies and identifying their root causes. However, the manual modeling of\nfaulty behaviors often demands extensive domain expertise and produces models\nthat are complex, error-prone, and difficult to interpret. To address this\nchallenge, we present a novel unsupervised fault diagnosis methodology that\nintegrates collective anomaly detection in multivariate time series, process\nmining, and stochastic simulation. Initially, collective anomalies are detected\nfrom low-level sensor data using multivariate time-series analysis. These\nanomalies are then transformed into structured event logs, enabling the\ndiscovery of interpretable process models through process mining. By\nincorporating timing distributions into the extracted Petri nets, the approach\nsupports stochastic simulation of faulty behaviors, thereby enhancing root\ncause analysis and behavioral understanding. The methodology is validated using\nthe Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart\nmanufacturing. Experimental results demonstrate its effectiveness in modeling,\nsimulating, and classifying faulty behaviors in CPSs. This enables the creation\nof comprehensive fault dictionaries that support predictive maintenance and the\ndevelopment of digital twins for industrial environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21502v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21502v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于过程挖掘的建模与仿真以增强信息物理系统故障诊断", "tldr": "本文提出了一种无监督故障诊断方法，结合多元时间序列异常检测、过程挖掘和随机仿真，以解决信息物理系统故障行为手动建模的复杂性，并在机械臂数据集上验证了其有效性。", "motivation": "信息物理系统 (CPS) 中的故障诊断对于确保系统可靠性和运行效率至关重要。然而，手动建模故障行为通常需要大量领域专业知识，并且生成的模型复杂、容易出错且难以解释。本文旨在解决这一挑战。", "method": "该方法是一种新颖的无监督故障诊断方法，集成了多元时间序列中的集体异常检测、过程挖掘和随机仿真。首先，利用多元时间序列分析从低级传感器数据中检测集体异常。然后，将这些异常转换为结构化事件日志，通过过程挖掘发现可解释的过程模型。通过将时间分布纳入提取的Petri网，该方法支持故障行为的随机仿真，从而增强了根本原因分析和行为理解。", "result": "该方法在机器人臂数据集 (RoAD) 上进行了验证。实验结果表明，该方法在信息物理系统故障行为的建模、仿真和分类方面是有效的。这有助于创建全面的故障字典，支持预测性维护和工业环境数字孪生的开发。", "conclusion": "本文提出了一种基于过程挖掘的无监督故障诊断方法，通过集成异常检测、过程挖掘和随机仿真，有效解决了信息物理系统故障行为建模的挑战，并为预测性维护和数字孪生提供了支持。", "translation": "信息物理系统（CPS）中的故障诊断对于通过准确检测异常和识别其根本原因来确保系统可靠性和运行效率至关重要。然而，手动建模故障行为通常需要广泛的领域专业知识，并且产生的模型复杂、容易出错且难以解释。为了解决这一挑战，我们提出了一种新颖的无监督故障诊断方法，该方法集成了多元时间序列中的集体异常检测、过程挖掘和随机仿真。最初，利用多元时间序列分析从低级传感器数据中检测集体异常。然后，将这些异常转换为结构化事件日志，通过过程挖掘发现可解释的过程模型。通过将时间分布纳入提取的Petri网，该方法支持故障行为的随机仿真，从而增强了根本原因分析和行为理解。该方法使用机器人臂数据集（RoAD）（智能制造领域广泛认可的基准）进行了验证。实验结果表明，该方法在信息物理系统故障行为的建模、仿真和分类方面是有效的。这使得能够创建全面的故障字典，支持预测性维护和工业环境数字孪生的开发。", "summary": "本文提出了一种新颖的无监督故障诊断方法，旨在解决信息物理系统（CPS）中手动建模故障行为的复杂性。该方法整合了多元时间序列的集体异常检测、过程挖掘和随机仿真。它首先从传感器数据中识别异常，将其转换为事件日志，然后通过过程挖掘发现可解释的Petri网模型，并进行随机仿真以增强根本原因分析。该方法在机器人臂数据集上得到验证，结果显示其在建模、仿真和分类CPS故障行为方面的有效性，支持预测性维护和数字孪生开发。", "keywords": "故障诊断, 过程挖掘, 随机仿真, 信息物理系统, 异常检测", "comments": "该论文提出了一种创新的无监督故障诊断框架，通过结合数据驱动的异常检测、过程挖掘和随机仿真，有效解决了传统手动建模的局限性。其亮点在于将低级传感器数据转化为高层过程模型，并支持动态仿真，这对于理解复杂系统的故障机制具有重要意义。在智能制造和工业物联网背景下，该方法为实现预测性维护和数字孪生提供了有力的技术支持。局限性可能在于对数据质量和事件日志构建的依赖性，以及Petri网模型在极端复杂系统中的可伸缩性。"}}
{"id": "2506.21287", "title": "HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation", "authors": ["Diego Biagini", "Nassir Navab", "Azade Farshad"], "summary": "Surgical Video Synthesis has emerged as a promising research direction\nfollowing the success of diffusion models in general-domain video generation.\nAlthough existing approaches achieve high-quality video generation, most are\nunconditional and fail to maintain consistency with surgical actions and\nphases, lacking the surgical understanding and fine-grained guidance necessary\nfor factual simulation. We address these challenges by proposing HieraSurg, a\nhierarchy-aware surgical video generation framework consisting of two\nspecialized diffusion models. Given a surgical phase and an initial frame,\nHieraSurg first predicts future coarse-grained semantic changes through a\nsegmentation prediction model. The final video is then generated by a\nsecond-stage model that augments these temporal segmentation maps with\nfine-grained visual features, leading to effective texture rendering and\nintegration of semantic information in the video space. Our approach leverages\nsurgical information at multiple levels of abstraction, including surgical\nphase, action triplets, and panoptic segmentation maps. The experimental\nresults on Cholecystectomy Surgical Video Generation demonstrate that the model\nsignificantly outperforms prior work both quantitatively and qualitatively,\nshowing strong generalization capabilities and the ability to generate higher\nframe-rate videos. The model exhibits particularly fine-grained adherence when\nprovided with existing segmentation maps, suggesting its potential for\npractical surgical applications.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21287v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21287v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HieraSurg：层级感知扩散模型用于外科手术视频生成", "tldr": "HieraSurg是一个层级感知的扩散模型，用于生成高质量的外科手术视频，解决了现有模型缺乏手术理解和精细指导的问题，并通过两阶段扩散模型整合了粗粒度语义变化和细粒度视觉特征。", "motivation": "现有外科手术视频生成方法大多是无条件的，无法保持与手术动作和阶段的一致性，缺乏手术理解和精细指导，导致模拟不够真实。", "method": "提出HieraSurg框架，包含两个专门的扩散模型。首先，给定手术阶段和初始帧，一个分割预测模型预测未来粗粒度语义变化。然后，第二个模型通过细化这些时间分割图的视觉特征，生成最终视频，实现有效的纹理渲染和语义信息整合。该方法利用了手术阶段、动作三元组和全景分割图等多层次抽象的手术信息。", "result": "在胆囊切除术视频生成上的实验结果表明，该模型在定量和定性上均显著优于现有工作，显示出强大的泛化能力和生成更高帧率视频的能力。在提供现有分割图时，模型表现出特别精细的依从性。", "conclusion": "HieraSurg通过整合多层级手术信息，能够生成高质量、高帧率且与手术过程高度一致的视频，在外科手术模拟和实际应用中具有巨大潜力。", "translation": "外科手术视频合成已成为一个有前景的研究方向，这得益于扩散模型在通用领域视频生成方面的成功。尽管现有方法实现了高质量的视频生成，但大多数是无条件的，未能保持与手术动作和阶段的一致性，缺乏外科理解和精细指导，这对于真实的模拟是必需的。我们通过提出HieraSurg来解决这些挑战，HieraSurg是一个层级感知的外科手术视频生成框架，由两个专门的扩散模型组成。给定一个手术阶段和初始帧，HieraSurg首先通过一个分割预测模型预测未来的粗粒度语义变化。然后，第二个模型通过用细粒度视觉特征增强这些时间分割图来生成最终视频，从而在视频空间中实现有效的纹理渲染和语义信息整合。我们的方法利用了多层抽象的手术信息，包括手术阶段、动作三元组和全景分割图。在胆囊切除术外科手术视频生成上的实验结果表明，该模型在定量和定性上均显著优于现有工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有分割图时，模型表现出特别精细的依从性，这表明其在实际外科手术应用中的潜力。", "summary": "HieraSurg是一个创新的层级感知扩散模型框架，旨在解决现有外科手术视频生成模型在手术一致性和精细指导方面的不足。它通过两阶段扩散过程，首先预测粗粒度语义变化，然后整合细粒度视觉特征，利用多层次手术信息（如阶段、动作、分割图）生成高质量、与手术过程一致的视频。实验证明，HieraSurg在性能上显著优于现有方法，并展现出在实际外科应用中的巨大潜力。", "keywords": "外科手术视频生成, 扩散模型, 层级感知, 语义分割, 视频合成", "comments": "HieraSurg的创新之处在于其“层级感知”的设计，通过结合粗粒度语义预测和细粒度视觉渲染，有效地解决了传统扩散模型在外科手术视频生成中缺乏手术理解和一致性的问题。这种分阶段、多层次信息整合的方法显著提升了生成视频的真实性和实用性，为外科手术模拟和训练提供了更可靠的工具。"}}
{"id": "2506.21550", "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale", "authors": ["Xiaona Zhou", "Constantin Brif", "Ismini Lourentzou"], "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21550v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21550v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "mTSBench：大规模多元时间序列异常检测和模型选择基准测试", "tldr": "mTSBench是一个大规模多元时间序列异常检测和模型选择基准，发现没有单一检测器表现最佳，且现有模型选择方法仍有不足。", "motivation": "多元时间序列异常检测在医疗、网络安全和工业监控等领域至关重要，但由于复杂的变量间依赖、时间动态和稀疏的异常标签而极具挑战性。", "method": "本文引入了mTSBench，这是迄今为止最大的多元时间序列异常检测（MTS-AD）和无监督模型选择基准，涵盖19个数据集和12个不同应用领域的344个带标签时间序列。mTSBench评估了24种异常检测方法，包括基于大型语言模型（LLM）的检测器，并在标准化条件下系统地基准测试了无监督模型选择技术。", "result": "研究结果证实没有单一检测器在所有数据集上表现出色，强调了模型选择的重要性。然而，即使是最先进的选择方法也远未达到最优，揭示了关键的不足。", "conclusion": "mTSBench提供了一个统一的评估套件，旨在实现严谨、可复现的比较，并促进自适应异常检测和鲁棒模型选择的未来发展。", "translation": "多元时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于复杂的变量间依赖、时间动态和稀疏的异常标签，仍然极具挑战性。我们引入了mTSBench，这是迄今为止最大的MTS-AD和无监督模型选择基准，涵盖19个数据集和12个不同应用领域的344个带标签时间序列。mTSBench评估了24种异常检测方法，包括基于大型语言模型（LLM）的多元时间序列检测器，并在标准化条件下系统地基准测试了无监督模型选择技术。与先前的研究结果一致，我们的结果证实没有单一检测器在所有数据集上表现出色，强调了模型选择的重要性。然而，即使是最先进的选择方法也远未达到最优，揭示了关键的不足。mTSBench提供了一个统一的评估套件，旨在实现严谨、可复现的比较，并促进自适应异常检测和鲁棒模型选择的未来发展。", "summary": "本文介绍了mTSBench，一个迄今为止最大的多元时间序列异常检测（MTS-AD）和无监督模型选择基准。它包含了344个时间序列、19个数据集和12个应用领域，并评估了24种检测方法（包括LLM-based）。研究发现没有单一检测器能普遍表现最佳，且当前的模型选择方法仍有显著提升空间。mTSBench旨在为MTS-AD提供统一、可复现的评估平台，以推动该领域的未来研究。", "keywords": "多元时间序列异常检测, 模型选择, 基准测试, 大型语言模型, 无监督学习", "comments": "mTSBench的创新之处在于其作为迄今为止最大的多元时间序列异常检测基准，并首次将基于大型语言模型的检测器纳入评估范围，这对于推动该领域的发展具有重要意义。该工作揭示了当前异常检测和模型选择方法的局限性，特别是没有“一刀切”的解决方案，且现有模型选择技术仍不理想，为未来的研究指明了方向。其提供的统一评估套件将有助于领域内研究的严谨性和可复现性。"}}
{"id": "2506.21312", "title": "Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing", "authors": ["Lars Möllenbrok", "Behnood Rasti", "Begüm Demir"], "summary": "The development of continual learning (CL) methods, which aim to learn new\ntasks in a sequential manner from the training data acquired continuously, has\ngained great attention in remote sensing (RS). The existing CL methods in RS,\nwhile learning new tasks, enhance robustness towards catastrophic forgetting.\nThis is achieved by using a large number of labeled training samples, which is\ncostly and not always feasible to gather in RS. To address this problem, we\npropose a novel continual self-supervised learning method in the context of\nmasked autoencoders (denoted as CoSMAE). The proposed CoSMAE consists of two\ncomponents: i) data mixup; and ii) model mixup knowledge distillation. Data\nmixup is associated with retaining information on previous data distributions\nby interpolating images from the current task with those from the previous\ntasks. Model mixup knowledge distillation is associated with distilling\nknowledge from past models and the current model simultaneously by\ninterpolating their model weights to form a teacher for the knowledge\ndistillation. The two components complement each other to regularize the MAE at\nthe data and model levels to facilitate better generalization across tasks and\nreduce the risk of catastrophic forgetting. Experimental results show that\nCoSMAE achieves significant improvements of up to 4.94% over state-of-the-art\nCL methods applied to MAE. Our code is publicly available at:\nhttps://git.tu-berlin.de/rsim/CoSMAE.", "comment": "Accepted to IEEE Geoscience and Remote Sensing Letters. Our code is\n  available at https://git.tu-berlin.de/rsim/CoSMAE", "pdf_url": "http://arxiv.org/pdf/2506.21312v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21312v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "遥感中基于掩码自编码器的持续自监督学习", "tldr": "本文提出CoSMAE，一种用于遥感的持续自监督学习方法，通过数据混合和模型混合知识蒸馏，在不依赖大量标注数据的情况下，有效缓解灾难性遗忘，并显著提升了性能。", "motivation": "现有的遥感持续学习（CL）方法在学习新任务时，虽然能增强对灾难性遗忘的鲁棒性，但需要大量标注训练样本，这在遥感领域成本高昂且不总是可行。", "method": "提出了一种名为CoSMAE的持续自监督学习方法，该方法基于掩码自编码器（MAE），包含两个主要组件：1）数据混合：通过插值当前任务图像与之前任务图像来保留先前数据分布的信息；2）模型混合知识蒸馏：通过插值过去模型和当前模型的权重来形成一个教师模型，从而同时从它们中提取知识。这两个组件相互补充，在数据和模型层面规范MAE，以促进更好的跨任务泛化并降低灾难性遗忘的风险。", "result": "实验结果表明，CoSMAE比应用于MAE的现有最先进的持续学习方法实现了显著改进，性能提升高达4.94%。", "conclusion": "CoSMAE通过数据混合和模型混合知识蒸馏，在数据和模型层面有效规范了掩码自编码器，从而在遥感持续学习中，无需大量标注样本即可有效缓解灾难性遗忘并提升跨任务泛化能力。", "translation": "持续学习（CL）方法的发展在遥感（RS）领域受到了广泛关注，其目标是以序列方式从持续获取的训练数据中学习新任务。遥感中现有的CL方法在学习新任务时，增强了对灾难性遗忘的鲁棒性。这通常通过使用大量标注训练样本来实现，但这在遥感中成本高昂且不总是可行。为了解决这个问题，我们提出了一种在掩码自编码器背景下的新型持续自监督学习方法（记为CoSMAE）。所提出的CoSMAE包含两个组件：i）数据混合；ii）模型混合知识蒸馏。数据混合与通过插值当前任务的图像与先前任务的图像来保留先前数据分布信息有关。模型混合知识蒸馏与通过插值过去模型和当前模型的权重来形成一个用于知识蒸馏的教师模型，从而同时从它们中提取知识有关。这两个组件相互补充，在数据和模型层面规范MAE，以促进更好的跨任务泛化并降低灾难性遗忘的风险。实验结果表明，CoSMAE比应用于MAE的最先进的CL方法实现了显著改进，性能提升高达4.94%。我们的代码已公开可用：https://git.tu-berlin.de/rsim/CoSMAE。", "summary": "本文针对遥感领域持续学习中对大量标注数据的依赖问题，提出了一种名为CoSMAE的持续自监督学习方法。CoSMAE基于掩码自编码器，并通过数据混合和模型混合知识蒸馏两个核心组件来缓解灾难性遗忘。数据混合通过插值不同任务数据来保留旧知识，模型混合知识蒸馏则通过组合新旧模型权重来指导学习。实验证明，CoSMAE在性能上优于现有方法，提升高达4.94%，有效提高了跨任务泛化能力。", "keywords": "持续学习, 自监督学习, 掩码自编码器, 遥感, 灾难性遗忘", "comments": "CoSMAE的创新之处在于将自监督学习与持续学习相结合，并通过独特的数据混合和模型混合知识蒸馏策略，在不依赖大量标注数据的情况下有效解决了灾难性遗忘问题。这对于遥感这种数据标注成本高昂的领域具有重要意义。该方法通过在数据和模型双重层面进行正则化，为持续学习提供了一种新的范式。"}}
{"id": "2506.21551", "title": "Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test", "authors": ["Ziyue Li", "Chenrui Fan", "Tianyi Zhou"], "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21551v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21551v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在LLM预训练中何处寻找Grokking？无需测试即可监控从记忆到泛化", "tldr": "Grokking现象在大型语言模型预训练中存在，且与内部通路结构化和复杂性降低有关，可以通过新指标无需测试进行监控。", "motivation": "Grokking现象（训练损失收敛后测试性能仍持续提升）在神经网络训练中被发现，使得泛化和推理等新兴能力的机制变得神秘。之前的研究多集中于小型模型和特定任务，缺乏对大型语言模型预训练中Grokking现象的深入研究。", "method": "本研究首次在7B大型语言模型（LLM），即OLMoE的一次性预训练检查点上进行Grokking研究。研究计算训练损失并在多种基准任务上评估泛化能力。通过调查LLM内部动态，特别是训练样本的通路（跨层专家选择）演变，来揭示Grokking的机制。此外，开发了两种新颖的指标来量化通路距离和单个通路的复杂性，并验证了它们预测泛化改进的能力。理论上，还证明了更结构化的通路能降低模型复杂性并改善泛化界限。", "result": "本研究首次验证了Grokking现象在大规模基础模型预训练中仍然存在，尽管不同数据可能异步进入Grokking阶段。研究发现训练样本的通路从随机、实例特定演变为更结构化和样本间可共享，且样本通路的复杂性在损失收敛后降低，这表明了从记忆到泛化的转换。开发的新指标能够高效、简单地预测不同下游任务上的泛化改进，且仅依赖于训练数据，因此具有在不进行微调和测试的情况下监控泛化性能的实用价值。理论上，更结构化的通路能降低模型复杂性并改善泛化界限。", "conclusion": "Grokking现象在大型语言模型预训练中普遍存在，其机制在于内部通路从记忆到泛化的转换，表现为通路结构化和复杂性降低。通过新开发的内部指标，可以在不依赖测试的情况下有效监控模型的泛化性能。", "translation": "Grokking，即训练损失收敛后测试性能仍持续提升的现象，最近在神经网络训练中被观察到，这使得泛化以及推理等新兴能力的机制变得神秘。虽然之前的研究通常在少量玩具或高度特定的任务上训练小型模型数千个周期，但我们首次对7B大型语言模型（LLM），即OLMoE，在一次性预训练过程中的检查点上进行了Grokking研究。我们计算了训练损失，并在包括数学推理、代码生成以及常识/领域特定知识检索任务在内的多种基准任务上评估了泛化能力。\n我们的研究首次验证了Grokking现象在大规模基础模型预训练中依然发生，尽管不同数据可能异步进入Grokking阶段。我们通过调查LLM的内部动态，进一步揭示了Grokking的“泛化涌现”机制。具体来说，我们发现训练样本的通路（即跨层的专家选择）在Grokking过程中从随机、实例特定演变为更结构化且样本间可共享。此外，尽管损失已收敛，样本通路的复杂性却降低了。这些迹象表明了从记忆到泛化（memorization-to-generalization）的转换，为延迟泛化提供了机制解释。在本研究中，我们开发了两种新颖的指标来量化通路距离和单个通路的复杂性。我们展示了它们预测不同下游任务泛化改进的能力。它们高效、计算简单，且仅依赖于训练数据。因此，它们对预训练具有实用价值，使我们能够在不进行微调和测试的情况下监控泛化性能。理论上，我们证明了更结构化的通路能降低模型复杂性并改善泛化界限。", "summary": "本文首次在7B大型语言模型（OLMoE）的预训练过程中研究了Grokking现象，发现它确实存在于大规模基础模型中。研究揭示了Grokking的内部机制：训练样本的通路（专家选择）从随机变为结构化和可共享，且通路复杂性降低，这标志着从记忆到泛化的转变。为监控这一过程，作者提出了两种基于训练数据的新指标，能够有效预测泛化性能，从而实现在不进行微调和测试的情况下监控预训练模型的泛化能力。理论分析也支持了通路结构化对泛化能力的提升作用。", "keywords": "Grokking, 大型语言模型, 预训练, 泛化, 通路分析", "comments": "这篇论文的创新之处在于将Grokking现象的研究从小型模型和玩具任务扩展到大规模LLM的预训练阶段，并首次证实了其存在。更重要的是，它深入探讨了Grokking的内部机制，通过分析模型内部的“通路”演变，提供了从记忆到泛化转换的机械性解释。提出的新指标具有重要的实践价值，为LLM预训练提供了一种无需额外测试即可监控泛化能力的方法，这对于优化大型模型训练过程具有显著意义。"}}
{"id": "2506.21316", "title": "DrishtiKon: Multi-Granular Visual Grounding for Text-Rich Document Images", "authors": ["Badri Vishal Kasuba", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "summary": "Visual grounding in text-rich document images is a critical yet underexplored\nchallenge for document intelligence and visual question answering (VQA)\nsystems. We present \\drishtikon, a multi-granular visual grounding framework\ndesigned to enhance interpretability and trust in VQA for complex, multilingual\ndocuments. Our approach integrates robust multi-lingual OCR, large language\nmodels, and a novel region matching algorithm to accurately localize answer\nspans at block, line, word, and point levels. We curate a new benchmark from\nthe CircularsVQA test set, providing fine-grained, human-verified annotations\nacross multiple granularities. Extensive experiments demonstrate that our\nmethod achieves state-of-the-art grounding accuracy, with line-level\ngranularity offering the best trade-off between precision and recall. Ablation\nstudies further highlight the benefits of multi-block and multi-line reasoning.\nComparative evaluations with leading vision-language models reveal the\nlimitations of current VLMs in precise localization, underscoring the\neffectiveness of our structured, alignment-based approach. Our findings pave\nthe way for more robust and interpretable document understanding systems in\nreal-world, text-centric scenarios. Code and dataset has been made available at\nhttps://github.com/kasuba-badri-vishal/DhrishtiKon.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.21316v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21316v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DrishtiKon: 文本丰富文档图像的多粒度视觉定位", "tldr": "DrishtiKon是一个多粒度视觉定位框架，用于提高文本丰富文档图像中VQA的可解释性和准确性。", "motivation": "文本丰富文档图像中的视觉定位是文档智能和视觉问答（VQA）系统中的一个关键但尚未充分探索的挑战。现有视觉-语言模型（VLM）在精确本地化方面存在局限性。", "method": "提出了DrishtiKon框架，该框架集成了强大的多语言OCR、大型语言模型和新颖的区域匹配算法，以在块、行、单词和点级别准确本地化答案跨度。同时，从CircularsVQA测试集中整理了一个新的基准数据集，提供了细粒度、人工验证的注释。", "result": "该方法实现了最先进的定位精度，其中行级别粒度在精度和召回率之间提供了最佳权衡。消融研究进一步突出了多块和多行推理的好处。与领先的视觉-语言模型进行比较评估，揭示了当前VLM在精确本地化方面的局限性，并突显了DrishtiKon结构化、基于对齐的方法的有效性。", "conclusion": "DrishtiKon为在现实世界的文本中心场景中构建更稳健和可解释的文档理解系统铺平了道路。", "translation": "文本丰富文档图像中的视觉定位是文档智能和视觉问答（VQA）系统中的一个关键但尚未充分探索的挑战。我们提出了\\drishtikon，一个多粒度视觉定位框架，旨在增强复杂多语言文档中VQA的可解释性和信任度。我们的方法集成了强大的多语言OCR、大型语言模型和新颖的区域匹配算法，以在块、行、单词和点级别准确本地化答案跨度。我们从CircularsVQA测试集中整理了一个新的基准数据集，提供了跨多个粒度的细粒度、人工验证的注释。大量的实验表明，我们的方法实现了最先进的定位精度，其中行级别粒度在精度和召回率之间提供了最佳权衡。消融研究进一步强调了多块和多行推理的好处。与领先的视觉-语言模型的比较评估揭示了当前VLM在精确本地化方面的局限性，突显了我们结构化、基于对齐的方法的有效性。我们的发现为在现实世界的文本中心场景中构建更稳健和可解释的文档理解系统铺平了道路。代码和数据集已在https://github.com/kasuba-badri-vishal/DhrishtiKon 提供。", "summary": "本文提出了DrishtiKon，一个用于文本丰富文档图像的多粒度视觉定位框架，旨在提高视觉问答（VQA）系统的可解释性和信任度。该框架结合了多语言OCR、大型语言模型和新颖的区域匹配算法，能够在块、行、单词和点等不同粒度级别精确本地化答案。通过创建新的基准数据集并进行广泛实验，DrishtiKon展示了最先进的定位精度，特别是在行级别粒度上表现出最佳的精度与召回率平衡，并优于现有视觉-语言模型，为更稳健的文档理解系统奠定了基础。", "keywords": "视觉定位, 文档图像, 多粒度, 视觉问答, OCR", "comments": "这篇论文的创新点在于提出了一个多粒度视觉定位框架DrishtiKon，能够处理文本丰富的文档图像，并在不同粒度级别（块、行、单词、点）进行精确的答案定位。其结合多语言OCR、LLM和新颖区域匹配算法的方法是独到的。该研究通过解决当前VLM在精确本地化方面的局限性，显著提升了文档理解和VQA系统的性能和可解释性，对于现实世界的文本中心应用具有重要意义。"}}
{"id": "2506.21154", "title": "Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation", "authors": ["He Li", "Haoang Chi", "Mingyu Liu", "Wanrong Huang", "Liyang Xu", "Wenjing Yang"], "summary": "The real world naturally has dimensions of time and space. Therefore,\nestimating the counterfactual outcomes with spatial-temporal attributes is a\ncrucial problem. However, previous methods are based on classical statistical\nmodels, which still have limitations in performance and generalization. This\npaper proposes a novel framework for estimating counterfactual outcomes with\nspatial-temporal attributes using the Transformer, exhibiting stronger\nestimation ability. Under mild assumptions, the proposed estimator within this\nframework is consistent and asymptotically normal. To validate the\neffectiveness of our approach, we conduct simulation experiments and real data\nexperiments. Simulation experiments show that our estimator has a stronger\nestimation capability than baseline methods. Real data experiments provide a\nvaluable conclusion to the causal effect of conflicts on forest loss in\nColombia. The source code is available at\nhttps://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.", "comment": "24 pages, accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21154v1", "categories": ["stat.ME", "cs.AI", "cs.LG"], "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.21154v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于Transformer的时空反事实结果估计", "tldr": "本文提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，解决了传统统计模型在性能和泛化方面的局限性，并在模拟和真实数据实验中表现出更强的估计能力。", "motivation": "在时空维度下估计反事实结果是一个关键问题。然而，以往的方法基于经典统计模型，在性能和泛化方面仍存在局限性。", "method": "本文提出了一种基于Transformer的新颖框架，用于估计具有时空属性的反事实结果。在该框架下，所提出的估计器在温和假设下具有一致性和渐近正态性。", "result": "模拟实验表明，所提出的估计器比基线方法具有更强的估计能力。真实数据实验为哥伦比亚冲突对森林损失的因果效应提供了有价值的结论。", "conclusion": "基于Transformer的方法在时空反事实结果估计方面表现出更强的估计能力，并且可以用于分析实际的因果效应，例如哥伦比亚冲突对森林损失的影响。", "translation": "现实世界天然具有时间和空间维度。因此，估计具有时空属性的反事实结果是一个关键问题。然而，以往的方法基于经典统计模型，在性能和泛化方面仍存在局限性。本文提出了一种基于Transformer的新颖框架，用于估计具有时空属性的反事实结果，展现出更强的估计能力。在温和假设下，该框架内提出的估计器具有一致性和渐近正态性。为了验证我们方法的有效性，我们进行了模拟实验和真实数据实验。模拟实验表明，我们的估计器比基线方法具有更强的估计能力。真实数据实验为哥伦比亚冲突对森林损失的因果效应提供了有价值的结论。源代码可在https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master获取。", "summary": "本文针对时空反事实结果估计问题，提出了一种基于Transformer的新型框架，旨在克服传统统计模型的性能和泛化局限性。该框架下的估计器在温和假设下具有一致性和渐近正态性。通过模拟和真实数据实验（例如分析哥伦比亚冲突对森林损失的因果效应），验证了其优于基线方法的估计能力和有效性。", "keywords": "Transformer, 反事实估计, 时空, 因果推断, 森林损失", "comments": "该论文创新性地将Transformer模型应用于时空反事实结果估计，解决了传统统计模型的局限性。其重要性在于提升了复杂时空因果推断的准确性和泛化能力，并提供了实际应用案例，具有较高的研究价值和实践意义。"}}
{"id": "2409.18017", "title": "Transferring disentangled representations: bridging the gap between synthetic and real images", "authors": ["Jacopo Dapueto", "Nicoletta Noceti", "Francesca Odone"], "summary": "Developing meaningful and efficient representations that separate the\nfundamental structure of the data generation mechanism is crucial in\nrepresentation learning. However, Disentangled Representation Learning has not\nfully shown its potential on real images, because of correlated generative\nfactors, their resolution and limited access to ground truth labels.\nSpecifically on the latter, we investigate the possibility of leveraging\nsynthetic data to learn general-purpose disentangled representations applicable\nto real data, discussing the effect of fine-tuning and what properties of\ndisentanglement are preserved after the transfer. We provide an extensive\nempirical study to address these issues. In addition, we propose a new\ninterpretable intervention-based metric, to measure the quality of factors\nencoding in the representation. Our results indicate that some level of\ndisentanglement, transferring a representation from synthetic to real data, is\npossible and effective.", "comment": "Accepted to NeurIPS, 2024", "pdf_url": "http://arxiv.org/pdf/2409.18017v3", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2409.18017v3", "date": "2024-09-26", "updated": "2024-12-06", "AI": {"title_translation": "解耦表示迁移：弥合合成图像与真实图像之间的鸿沟", "tldr": "研究如何将从合成数据中学到的解耦表示迁移到真实图像，并提出新的评估指标，结果表明这种迁移是可行且有效的。", "motivation": "解耦表示学习在真实图像上未能充分发挥潜力，因为生成因素相关、分辨率问题以及难以获取真实标签。本文旨在利用合成数据学习通用解耦表示并应用于真实数据，以弥合合成与真实图像之间的差距。", "method": "通过广泛的实证研究，探讨了利用合成数据学习通用解耦表示并应用于真实数据的可能性，讨论了微调的效果以及迁移后解耦属性的保留情况。此外，提出了一种新的基于干预的可解释度量来衡量表示中因子编码的质量。", "result": "结果表明，将解耦表示从合成数据迁移到真实数据在一定程度上是可能且有效的。", "conclusion": "从合成数据迁移解耦表示到真实数据是可行且有效的，这有助于解决真实图像上解耦表示学习的挑战。", "translation": "开发有意义且高效的表示，以分离数据生成机制的基本结构，在表示学习中至关重要。然而，解耦表示学习尚未在真实图像上充分展示其潜力，原因在于生成因素相关性、其分辨率以及真实标签的有限获取。特别是针对后者，我们研究了利用合成数据学习适用于真实数据的通用解耦表示的可能性，讨论了微调的效果以及迁移后解耦属性的保留情况。我们提供了广泛的实证研究来解决这些问题。此外，我们提出了一种新的可解释的基于干预的度量，用于衡量表示中因子编码的质量。我们的结果表明，将解耦表示从合成数据迁移到真实数据，在一定程度上是可能且有效的。", "summary": "该研究探讨了如何将从合成数据中学到的解耦表示迁移到真实图像，以克服真实图像中解耦学习面临的挑战，如相关生成因素和缺乏真实标签。文章通过广泛的实证研究，分析了微调和解耦属性的保留情况，并提出了一种新的基于干预的可解释度量来评估因子编码质量。研究结果表明，将解耦表示从合成数据迁移到真实数据是可行且有效的，为弥合合成与真实图像之间的差距提供了新途径。", "keywords": "解耦表示学习, 合成数据, 真实图像, 迁移学习, 评估指标", "comments": "该论文的创新点在于提出了将解耦表示从合成数据迁移到真实图像的策略，并引入了新的评估指标。这对于解决真实世界数据中解耦表示学习的挑战具有重要意义，尤其是在真实标签稀缺的场景下。其贡献在于证明了在一定程度上，合成数据可以作为真实数据解耦表示学习的有效替代或辅助。"}}
{"id": "2506.21317", "title": "LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "summary": "Current vision-language models (VLMs) are well-adapted for general visual\nunderstanding tasks. However, they perform inadequately when handling complex\nvisual tasks related to human poses and actions due to the lack of specialized\nvision-language instruction-following data. We introduce a method for\ngenerating such data by integrating human keypoints with traditional visual\nfeatures such as captions and bounding boxes, enabling more precise\nunderstanding of human-centric scenes. Our approach constructs a dataset\ncomprising 200,328 samples tailored to fine-tune models for human-centric\ntasks, focusing on three areas: conversation, detailed description, and complex\nreasoning. We establish an Extended Human Pose and Action Understanding\nBenchmark (E-HPAUB) to assess model performance on human pose and action\nunderstanding. We fine-tune the LLaVA-1.5-7B model using this dataset and\nevaluate our resulting LLaVA-Pose model on the benchmark, achieving significant\nimprovements. Experimental results show an overall improvement of 33.2%\ncompared to the original LLaVA-1.5-7B model. These findings highlight the\neffectiveness of keypoint-integrated data in enhancing multimodal models for\nhuman-centric visual understanding. Code is available at\nhttps://github.com/Ody-trek/LLaVA-Pose.", "comment": "arXiv admin note: substantial text overlap with arXiv:2409.09306", "pdf_url": "http://arxiv.org/pdf/2506.21317v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21317v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LLaVA-Pose：通过关键点集成指令微调增强人体姿态和动作理解", "tldr": "LLaVA-Pose通过整合人体关键点数据对VLM进行指令微调，显著提升了模型在人体姿态和动作理解上的表现，相比LLaVA-1.5-7B有33.2%的提升。", "motivation": "当前视觉语言模型（VLMs）在处理复杂的人体姿态和动作相关视觉任务时表现不足，主要原因是缺乏专门的视觉语言指令遵循数据。", "method": "提出了一种通过将人体关键点与传统视觉特征（如图像标题和边界框）结合来生成专业数据的方法。构建了一个包含200,328个样本的数据集，用于微调模型，专注于对话、详细描述和复杂推理。建立了扩展人体姿态和动作理解基准（E-HPAUB）来评估模型性能。使用该数据集微调了LLaVA-1.5-7B模型，并将其命名为LLaVA-Pose。", "result": "LLaVA-Pose模型在E-HPAUB基准测试中取得了显著改进，相比原始的LLaVA-1.5-7B模型，整体性能提升了33.2%。", "conclusion": "实验结果表明，关键点集成数据在增强多模态模型进行以人为中心的视觉理解方面是有效的。", "translation": "当前的视觉语言模型（VLMs）已很好地适应了通用视觉理解任务。然而，由于缺乏专门的视觉语言指令遵循数据，它们在处理与人体姿态和动作相关的复杂视觉任务时表现不足。我们引入了一种通过将人体关键点与传统视觉特征（如图像标题和边界框）相结合来生成此类数据的方法，从而能够更精确地理解以人为中心的场景。我们的方法构建了一个包含200,328个样本的数据集，专门用于微调以人为中心任务的模型，重点关注三个领域：对话、详细描述和复杂推理。我们建立了扩展人体姿态和动作理解基准（E-HPAUB）来评估模型在人体姿态和动作理解方面的性能。我们使用该数据集微调了LLaVA-1.5-7B模型，并在基准测试中评估了我们生成的LLaVA-Pose模型，取得了显著改进。实验结果显示，与原始的LLaVA-1.5-7B模型相比，整体性能提升了33.2%。这些发现突出了关键点集成数据在增强多模态模型以实现以人为中心的视觉理解方面的有效性。代码可在https://github.com/Ody-trek/LLaVA-Pose 获取。", "summary": "本文针对现有视觉语言模型在人体姿态和动作理解任务上的不足，提出LLaVA-Pose模型。通过创新性地将人体关键点与传统视觉特征结合，生成了包含20万余样本的专业指令遵循数据集，并构建了E-HPAUB基准。基于该数据集对LLaVA-1.5-7B进行微调后，LLaVA-Pose在人体中心视觉理解任务上实现了33.2%的性能提升，验证了关键点集成数据在增强多模态模型方面的有效性。", "keywords": "视觉语言模型, 人体姿态, 动作理解, 关键点集成, 指令微调", "comments": "这篇论文的创新点在于提出了将人体关键点集成到视觉-语言指令微调数据中的方法，从而专门解决了现有VLM在人体姿态和动作理解方面的局限性。通过构建大规模的专业数据集和新的评估基准（E-HPAUB），为该领域的研究提供了宝贵的资源和评估工具。其显著的性能提升证明了这种数据集成策略的有效性，对于推动以人为中心的视觉理解任务具有重要意义。"}}
{"id": "2506.20672", "title": "The final solution of the Hitchhiker's problem #5", "authors": ["Matjaž Omladič", "Martin Vuk", "Aljaž Zalar"], "summary": "A recent survey, nicknamed \"Hitchhiker's Guide\", J.J. Arias-Garc{\\i}a, R.\nMesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and\nSystems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the\ndependence modeling community in spite of the lack of statistical\ninterpretation of quasi-copulas. In our previous work (arXiv:2410.19339,\naccepted in Fuzzy Sets and Systems), we addressed the question of extreme\nvalues of the mass distribution associated with multivariate quasi-copulas.\nUsing a linear programming approach, we were able to solve Open Problem 5 of\nthe \"Guide\" up to dimension d = 17 and disprove a recent conjecture on the\nsolution to that problem. In this paper, we use an analytical approach to\nprovide a complete answer to the original question.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2506.20672v1", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20672v1", "date": "2025-05-20", "updated": "2025-05-20", "AI": {"title_translation": "搭便车者问题#5的最终解决方案", "tldr": "本文利用分析方法，对“搭便车者指南”中关于多元拟合联结函数质量分布的极端值问题（开放问题5）提供了完整的解决方案，此前该问题曾通过线性规划方法部分解决并推翻了一个猜想。", "motivation": "尽管拟合联结函数缺乏统计解释，但近期一项名为“搭便车者指南”的调查提高了其在依赖性建模领域中的关注度。作者先前的研究部分解决了指南中的开放问题5，但仍需一个完整的答案。", "method": "本文采用分析方法来提供对原始问题的完整解答。先前的相关工作曾使用线性规划方法。", "result": "本文利用分析方法，为“搭便车者指南”中的开放问题5提供了完整的答案。此前，作者通过线性规划方法已将该问题解决至维度d=17，并推翻了一个关于该问题解决方案的近期猜想。", "conclusion": "本文成功地通过分析方法，为“搭便车者指南”中的开放问题5提供了完整的解决方案。", "translation": "近期一项名为“搭便车者指南”的调查（J.J. Arias-García, R. Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and Systems 393 (2020) 1-28）提高了拟合联结函数问题在依赖性建模社区中的评价，尽管拟合联结函数缺乏统计解释。在我们之前的工作中（arXiv:2410.19339，已在Fuzzy Sets and Systems接受），我们解决了与多元拟合联结函数相关的质量分布极端值问题。通过线性规划方法，我们能够将“指南”中的开放问题5解决至维度d=17，并推翻了一个关于该问题解决方案的近期猜想。在本文中，我们使用分析方法为原始问题提供了完整的答案。", "summary": "本研究旨在为“搭便车者指南”中关于多元拟合联结函数质量分布极端值的开放问题5提供一个完整的分析解决方案。该问题因近期一项调查而备受关注。作者之前的研究曾利用线性规划方法部分解决了该问题，并推翻了相关猜想，但仅限于特定维度。本文通过采用纯粹的分析方法，最终为该长期未决的问题提供了全面的解答。", "keywords": "拟合联结函数, 搭便车者指南, 开放问题5, 分析方法, 依赖性建模", "comments": "本文的创新之处在于其采用分析方法彻底解决了之前只能通过线性规划部分解决的问题，并且推翻了相关猜想。这对于依赖性建模领域中拟合联结函数的研究具有重要意义，尤其是在缺乏统计解释的情况下提供了一个完整的理论解决方案。"}}
{"id": "2506.21330", "title": "Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models", "authors": ["Haoyang Wu", "Tsun-Hsuan Wang", "Mathias Lechner", "Ramin Hasani", "Jennifer A. Eckhoff", "Paul Pak", "Ozanan R. Meireles", "Guy Rosman", "Yutong Ban", "Daniela Rus"], "summary": "Surgical workflow analysis is essential in robot-assisted surgeries, yet the\nlong duration of such procedures poses significant challenges for comprehensive\nvideo analysis. Recent approaches have predominantly relied on transformer\nmodels; however, their quadratic attention mechanism restricts efficient\nprocessing of lengthy surgical videos. In this paper, we propose a novel\nhierarchical input-dependent state space model that leverages the linear\nscaling property of state space models to enable decision making on full-length\nvideos while capturing both local and global dynamics. Our framework\nincorporates a temporally consistent visual feature extractor, which appends a\nstate space model head to a visual feature extractor to propagate temporal\ninformation. The proposed model consists of two key modules: a\nlocal-aggregation state space model block that effectively captures intricate\nlocal dynamics, and a global-relation state space model block that models\ntemporal dependencies across the entire video. The model is trained using a\nhybrid discrete-continuous supervision strategy, where both signals of discrete\nphase labels and continuous phase progresses are propagated through the\nnetwork. Experiments have shown that our method outperforms the current\nstate-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on\nMICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available\nafter paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21330v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21330v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于分层输入依赖状态空间模型的整体手术阶段识别", "tldr": "本文提出了一种新型分层输入依赖状态空间模型，用于整体手术阶段识别，该模型能有效处理长视频，并在多个数据集上显著优于现有SOTA方法。", "motivation": "手术工作流分析在机器人辅助手术中至关重要，但手术时长导致全面的视频分析面临巨大挑战，特别是变压器模型由于其二次注意力机制难以有效处理长时间的手术视频。", "method": "本文提出了一种新型分层输入依赖状态空间模型，利用状态空间模型的线性缩放特性，实现对全长视频的决策，并捕获局部和全局动态。该框架包含一个时间一致的视觉特征提取器，它在视觉特征提取器后附加一个状态空间模型头部以传播时间信息。该模型由两个关键模块组成：一个局部聚合状态空间模型块，有效捕获复杂的局部动态；一个全局关系状态空间模型块，建模整个视频的时间依赖性。模型采用混合离散-连续监督策略进行训练，其中离散阶段标签和连续阶段进度信号都通过网络传播。", "result": "实验表明，我们的方法在Cholec80数据集上超越现有最先进方法2.8%，在MICCAI2016数据集上超越4.3%，在Heichole数据集上超越12.9%。", "conclusion": "本文提出的分层输入依赖状态空间模型有效解决了长手术视频分析的挑战，通过高效捕获局部和全局动态，在整体手术阶段识别方面取得了卓越的性能。", "translation": "手术工作流分析在机器人辅助手术中至关重要，但此类手术的长时间特性对全面的视频分析构成了重大挑战。最近的方法主要依赖于变压器模型；然而，它们的二次注意力机制限制了对冗长手术视频的有效处理。在本文中，我们提出了一种新型分层输入依赖状态空间模型，该模型利用状态空间模型的线性缩放特性，能够对全长视频进行决策，同时捕获局部和全局动态。我们的框架包含一个时间一致的视觉特征提取器，它在视觉特征提取器后附加一个状态空间模型头部以传播时间信息。所提出的模型由两个关键模块组成：一个局部聚合状态空间模型块，有效捕获复杂的局部动态；以及一个全局关系状态空间模型块，建模整个视频的时间依赖性。该模型采用混合离散-连续监督策略进行训练，其中离散阶段标签和连续阶段进度信号都通过网络传播。实验表明，我们的方法在Cholec80数据集上以2.8%的显著优势超越了当前最先进的方法，在MICCAI2016数据集上以4.3%的优势超越，在Heichole数据集上以12.9%的优势超越。代码将在论文接收后公开发布。", "summary": "本文提出了一种新颖的分层输入依赖状态空间模型，用于整体手术阶段识别，旨在克服传统变压器模型处理长手术视频的效率限制。该模型利用状态空间模型的线性缩放特性，能够捕捉视频的局部和全局动态。其核心包含一个时间一致的视觉特征提取器以及两个专门的状态空间模块：一个用于局部聚合，另一个用于全局关系建模。通过混合离散-连续监督策略进行训练，该方法在Cholec80、MICCAI2016和Heichole等数据集上均显著优于现有最先进的方法。", "keywords": "手术阶段识别, 状态空间模型, 手术工作流分析, 长视频分析, 分层模型", "comments": "该论文的创新之处在于将状态空间模型应用于手术视频分析，尤其有效解决了长时序视频分析中变压器模型存在的二次复杂度问题。其分层结构设计和混合监督策略是亮点，能够同时捕获精细的局部动态和宏观的全局时间依赖。实验结果的显著提升表明了该方法在机器人辅助手术领域实际应用的重要性和潜力。"}}
{"id": "2506.21348", "title": "PanSt3R: Multi-view Consistent Panoptic Segmentation", "authors": ["Lojze Zust", "Yohann Cabon", "Juliette Marrie", "Leonid Antsfeld", "Boris Chidlovskii", "Jerome Revaud", "Gabriela Csurka"], "summary": "Panoptic segmentation of 3D scenes, involving the segmentation and\nclassification of object instances in a dense 3D reconstruction of a scene, is\na challenging problem, especially when relying solely on unposed 2D images.\nExisting approaches typically leverage off-the-shelf models to extract\nper-frame 2D panoptic segmentations, before optimizing an implicit geometric\nrepresentation (often based on NeRF) to integrate and fuse the 2D predictions.\nWe argue that relying on 2D panoptic segmentation for a problem inherently 3D\nand multi-view is likely suboptimal as it fails to leverage the full potential\nof spatial relationships across views. In addition to requiring camera\nparameters, these approaches also necessitate computationally expensive\ntest-time optimization for each scene. Instead, in this work, we propose a\nunified and integrated approach PanSt3R, which eliminates the need for\ntest-time optimization by jointly predicting 3D geometry and multi-view\npanoptic segmentation in a single forward pass. Our approach builds upon recent\nadvances in 3D reconstruction, specifically upon MUSt3R, a scalable multi-view\nversion of DUSt3R, and enhances it with semantic awareness and multi-view\npanoptic segmentation capabilities. We additionally revisit the standard\npost-processing mask merging procedure and introduce a more principled approach\nfor multi-view segmentation. We also introduce a simple method for generating\nnovel-view predictions based on the predictions of PanSt3R and vanilla 3DGS.\nOverall, the proposed PanSt3R is conceptually simple, yet fast and scalable,\nand achieves state-of-the-art performance on several benchmarks, while being\norders of magnitude faster than existing methods.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21348v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21348v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PanSt3R：多视角一致全景分割", "tldr": "PanSt3R提出了一种统一且集成的3D全景分割方法，无需测试时优化，比现有方法快几个数量级，并在多个基准上达到最先进的性能。", "motivation": "现有的3D场景全景分割方法通常依赖于2D全景分割，然后进行昂贵的测试时优化来融合预测，这未能充分利用跨视图的空间关系，且计算成本高昂。", "method": "本文提出了PanSt3R，一种统一且集成的3D全景分割方法。它在单次前向传播中联合预测3D几何和多视角全景分割，消除了测试时优化的需求。PanSt3R基于MUSt3R（DUSt3R的可扩展多视图版本）并增强了语义感知和多视图全景分割能力。此外，它改进了标准的后处理掩码合并程序，并引入了一种基于PanSt3R和Vanilla 3DGS生成新视角预测的简单方法。", "result": "PanSt3R在多个基准测试中取得了最先进的性能，并且比现有方法快了几个数量级。", "conclusion": "PanSt3R是一种概念简单、快速、可扩展的3D全景分割方法，通过统一的端到端预测，解决了现有方法的效率和性能瓶颈，实现了卓越的性能。", "translation": "3D场景的全景分割，涉及场景密集3D重建中对象实例的分割和分类，是一个具有挑战性的问题，尤其是在仅依赖未定位的2D图像时。现有方法通常利用现成的模型提取每帧2D全景分割，然后优化隐式几何表示（通常基于NeRF）以整合和融合2D预测。我们认为，对于一个本质上是3D和多视角的问题，依赖2D全景分割可能不是最优的，因为它未能充分利用跨视图空间关系的全部潜力。除了需要相机参数外，这些方法还需要对每个场景进行计算成本高昂的测试时优化。相反，在这项工作中，我们提出了一种统一且集成的方法PanSt3R，它通过在单次前向传播中联合预测3D几何和多视角全景分割，消除了测试时优化的需求。我们的方法建立在3D重建的最新进展之上，特别是基于MUSt3R（DUSt3R的可扩展多视图版本），并增强了语义感知和多视角全景分割能力。我们还重新审视了标准的后处理掩码合并过程，并引入了一种更原则性的多视角分割方法。我们还介绍了一种基于PanSt3R和Vanilla 3DGS的预测生成新视角预测的简单方法。总的来说，所提出的PanSt3R概念简单，但快速且可扩展，并在多个基准上实现了最先进的性能，同时比现有方法快了几个数量级。", "summary": "PanSt3R是一种新颖的3D场景多视角全景分割方法，旨在克服现有方法对2D分割的依赖和高昂的测试时优化成本。该方法在单次前向传播中联合预测3D几何和多视角全景分割，无需额外的优化步骤。PanSt3R基于MUSt3R并增强了语义感知能力，同时改进了多视角分割的后处理流程。它还提供了一种生成新视角预测的方法。PanSt3R在速度和可扩展性方面显著优于现有技术，并在多个基准测试中达到了最先进的性能。", "keywords": "全景分割, 3D重建, 多视角一致性, 深度学习, PanSt3R", "comments": "PanSt3R的创新之处在于其统一且集成的端到端方法，避免了传统方法中昂贵的测试时优化和对2D分割的次优依赖。通过将3D几何和多视角全景分割结合在单次前向传播中，该方法显著提高了效率和性能。其可扩展性以及在多个基准上达到SOTA性能，使其在3D场景理解领域具有重要意义。"}}
{"id": "2506.21356", "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models", "authors": ["Hongbo Liu", "Jingwen He", "Yi Jin", "Dian Zheng", "Yuhao Dong", "Fan Zhang", "Ziqi Huang", "Yinan He", "Yangguang Li", "Weichao Chen", "Yu Qiao", "Wanli Ouyang", "Shengjie Zhao", "Ziwei Liu"], "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\n\\textbf{ShotBench}, a comprehensive benchmark specifically designed for\ncinematic language understanding. It features over 3.5k expert-annotated QA\npairs from images and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60\\%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\n\\textbf{ShotQA}, a large-scale multimodal dataset comprising approximately 70k\ncinematic QA pairs. Leveraging ShotQA, we develop \\textbf{ShotVL} through\nsupervised fine-tuning and Group Relative Policy Optimization. ShotVL\nsignificantly outperforms all existing open-source and proprietary models on\nShotBench, establishing new \\textbf{state-of-the-art} performance. We\nopen-source our models, data, and code to foster rapid progress in this crucial\narea of AI-driven cinematic understanding and generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21356v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21356v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ShotBench：视觉语言模型中的专家级电影理解", "tldr": "本文介绍了ShotBench，一个用于评估视觉语言模型（VLMs）电影理解能力的基准，发现现有VLMs表现不佳。为解决此问题，构建了ShotQA数据集并开发了ShotVL模型，显著提升了电影理解的最新水平。", "motivation": "现有的视觉语言模型（VLMs）在理解电影中细致的电影语法方面能力不足且缺乏稳健的评估，这限制了细粒度视觉理解和AI辅助视频生成的精度。", "method": "引入了ShotBench，一个包含3.5k专家标注QA对的综合电影语言理解基准。构建了ShotQA，一个包含约70k电影QA对的大规模多模态数据集。利用ShotQA，通过监督微调和群组相对策略优化开发了ShotVL模型。", "result": "在ShotBench上评估了24个领先的VLM，发现它们存在显著局限性，即使表现最好的模型平均准确率也低于60%，尤其在细粒度视觉线索和复杂空间推理方面表现不佳。ShotVL在ShotBench上显著优于所有现有开源和专有模型，建立了新的最先进（SOTA）性能。", "conclusion": "现有VLMs在电影理解方面存在显著不足。本文引入的ShotBench、ShotQA和ShotVL为推动AI驱动的电影理解和生成领域的进步提供了关键的工具和方法。", "translation": "电影摄影作为电影最基本的视觉语言，对于传达叙事、情感和美学质量至关重要。尽管最近的视觉语言模型（VLMs）在通用视觉理解方面表现出强大的能力，但它们在理解单个镜头中嵌入的细致电影语法方面的熟练程度在很大程度上尚未被探索，并且缺乏稳健的评估。这一关键差距限制了细粒度的视觉理解以及AI辅助视频生成的精度。为了解决这个问题，我们引入了\\textbf{ShotBench}，一个专门为电影语言理解设计的综合基准。它包含来自图像和视频片段的3.5k多个专家标注的问答对，这些问答对经过精心策划，来源于200多部著名（主要是奥斯卡提名）电影，并涵盖了八个关键的电影摄影维度。我们对ShotBench上24个领先的VLM的评估揭示了它们存在的实质性局限性：即使是表现最好的模型也只达到了不到60%的平均准确率，尤其是在细粒度视觉线索和复杂空间推理方面表现挣扎。为了促进该领域的进步，我们构建了\\textbf{ShotQA}，一个包含大约70k电影问答对的大规模多模态数据集。利用ShotQA，我们通过监督微调和群组相对策略优化开发了\\textbf{ShotVL}。ShotVL在ShotBench上显著优于所有现有开源和专有模型，建立了新的\\textbf{最先进}性能。我们开源了我们的模型、数据和代码，以促进AI驱动的电影理解和生成这一关键领域的快速发展。", "summary": "本文介绍了ShotBench，一个用于评估视觉语言模型（VLMs）电影理解能力的综合基准，并揭示了现有VLMs在该领域的显著不足。为弥补这一差距，作者构建了大规模电影问答数据集ShotQA，并基于此开发了ShotVL模型。ShotVL通过监督微调和群组相对策略优化，在ShotBench上取得了超越现有模型的SOTA性能，为AI驱动的电影理解和生成提供了重要进展。", "keywords": "电影理解, 视觉语言模型, 基准测试, 数据集, 电影摄影", "comments": "本文通过引入专门的基准测试和大规模数据集，系统地揭示了现有视觉语言模型在电影理解这一特定且复杂的领域中的局限性。其创新点在于将电影摄影的专业知识量化为可评估的维度，并提出了一个有效提升模型性能的方法，对推动AI在电影内容创作和分析方面的应用具有重要意义。开源数据和模型的举措将极大促进该领域的研究进展。"}}
{"id": "2506.20697", "title": "scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection", "authors": ["Zhen Yuan", "Shaoqing Jiao", "Yihang Xiao", "Jiajie Peng"], "summary": "The advent of single-cell multi-omics technologies has enabled the\nsimultaneous profiling of diverse omics layers within individual cells.\nIntegrating such multimodal data provides unprecedented insights into cellular\nidentity, regulatory processes, and disease mechanisms. However, it remains\nchallenging, as current methods often rely on selecting highly variable genes\nor peaks during preprocessing, which may inadvertently discard crucial\nbiological information. Here, we present scMamba, a foundation model designed\nto integrate single-cell multi-omics data without the need for prior feature\nselection while preserving genomic positional information. scMamba introduces a\npatch-based cell tokenization strategy that treats genomics regions as words\n(tokens) and cells as sentences. Building upon the concept of state space\nduality, scMamba distills rich biological insights from high-dimensional,\nsparse single-cell multi-omics data. Additionally, our novel contrastive\nlearning approach, enhanced with cosine similarity regularization, enables\nsuperior alignment across omics layers compared to traditional methods.\nSystematic benchmarking across multiple datasets demonstrates that scMamba\nsignificantly outperforms state-of-the-art methods in preserving biological\nvariation, aligning omics layers, and enhancing key downstream tasks such as\nclustering, cell type annotation, and trajectory inference. Our findings\nposition scMamba as a powerful tool for large-scale single-cell multi-omics\nintegration, capable of handling large-scale atlases and advancing biological\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20697v1", "categories": ["q-bio.CB", "cs.LG"], "cate": "q-bio.CB", "url": "http://arxiv.org/abs/2506.20697v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "scMamba：一种超越高变特征选择的单细胞多组学整合的可扩展基础模型", "tldr": "scMamba是一个新的基础模型，用于整合单细胞多组学数据，无需特征选择，并能保留基因组位置信息，在多项任务中表现优于现有方法。", "motivation": "现有单细胞多组学整合方法通常依赖于高变基因或峰的选择，这可能会无意中丢弃关键的生物学信息，导致整合挑战。", "method": "scMamba引入了一种基于补丁的细胞标记化策略，将基因组区域视为词（tokens），细胞视为句子。它基于状态空间对偶性，并采用了一种新颖的对比学习方法，辅以余弦相似度正则化，以实现更好的组学层对齐。其核心在于无需预先进行特征选择，同时保留基因组位置信息。", "result": "scMamba在多个数据集上的系统基准测试表明，它在保留生物变异、对齐组学层以及增强下游任务（如聚类、细胞类型注释和轨迹推断）方面显著优于现有方法。", "conclusion": "scMamba被定位为一种强大的工具，适用于大规模单细胞多组学整合，能够处理大型图谱并推动生物学发现。", "translation": "单细胞多组学技术的出现使得在单个细胞内同时分析不同的组学层成为可能。整合此类多模态数据为细胞身份、调控过程和疾病机制提供了前所未有的见解。然而，这仍然具有挑战性，因为当前的方法在预处理过程中通常依赖于选择高变基因或峰，这可能会无意中丢弃关键的生物学信息。在此，我们提出了scMamba，一个旨在整合单细胞多组学数据的基础模型，无需预先进行特征选择，同时保留基因组位置信息。scMamba引入了一种基于补丁的细胞标记化策略，将基因组区域视为词（tokens），细胞视为句子。基于状态空间对偶性的概念，scMamba从高维、稀疏的单细胞多组学数据中提炼出丰富的生物学见解。此外，我们新颖的对比学习方法，通过余弦相似度正则化得到增强，与传统方法相比，能够实现卓越的组学层对齐。在多个数据集上的系统基准测试表明，scMamba在保留生物变异、对齐组学层以及增强关键下游任务（如聚类、细胞类型注释和轨迹推断）方面显著优于最先进的方法。我们的研究结果将scMamba定位为一种用于大规模单细胞多组学整合的强大工具，能够处理大型图谱并推动生物学发现。", "summary": "scMamba是一个新颖的基础模型，用于解决单细胞多组学数据整合中的挑战。它通过避免高变特征选择和保留基因组位置信息来克服传统方法的局限性。scMamba采用了一种独特的补丁式细胞标记化策略和增强的对比学习方法，能够从高维稀疏数据中提取生物学见解。基准测试显示，scMamba在数据整合、生物变异保留及下游任务表现上均优于现有方法，使其成为大规模单细胞多组学研究的有力工具。", "keywords": "单细胞多组学整合, 基础模型, 特征选择, 对比学习, scMamba", "comments": "scMamba的创新之处在于其无需高变特征选择的整合策略，这避免了信息丢失。其将基因组区域视为词、细胞视为句子的标记化方法，以及结合状态空间对偶性和对比学习的设计，为处理大规模、高维单细胞数据提供了新的范式。该模型有望显著提升单细胞多组学数据分析的准确性和效率，推动生物学发现。"}}
{"id": "2506.21357", "title": "CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations", "authors": ["Julian Lorenz", "Mrunmai Phatak", "Robin Schön", "Katja Ludwig", "Nico Hörmann", "Annemarie Friedrich", "Rainer Lienhart"], "summary": "2D scene graphs provide a structural and explainable framework for scene\nunderstanding. However, current work still struggles with the lack of accurate\nscene graph data. To overcome this data bottleneck, we present CoPa-SG, a\nsynthetic scene graph dataset with highly precise ground truth and exhaustive\nrelation annotations between all objects. Moreover, we introduce parametric and\nproto-relations, two new fundamental concepts for scene graphs. The former\nprovides a much more fine-grained representation than its traditional\ncounterpart by enriching relations with additional parameters such as angles or\ndistances. The latter encodes hypothetical relations in a scene graph and\ndescribes how relations would form if new objects are placed in the scene.\nUsing CoPa-SG, we compare the performance of various scene graph generation\nmodels. We demonstrate how our new relation types can be integrated in\ndownstream applications to enhance planning and reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21357v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21357v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CoPa-SG: 密集场景图与参数化和原型关系", "tldr": "CoPa-SG引入了一个新的合成场景图数据集，并提出了参数化关系和原型关系两种新概念，以解决现有场景图数据不足和表示粒度不够的问题，并增强下游应用能力。", "motivation": "现有2D场景图缺乏准确的数据，且传统关系表示粒度不够细致，无法充分支持场景理解、规划和推理。", "method": "1. 提出了CoPa-SG，一个合成场景图数据集，具有高精度真值和对象间详尽的关系标注。2. 引入了两种新的基本场景图概念：参数化关系（用角度、距离等参数丰富关系）和原型关系（编码假设关系，描述新对象放置时关系如何形成）。3. 使用CoPa-SG数据集比较了各种场景图生成模型的性能。4. 演示了如何将新关系类型集成到下游应用中以增强规划和推理能力。", "result": "1. 创建了一个具有高精度真值和详尽关系的合成场景图数据集CoPa-SG。2. 提出了参数化关系和原型关系，这两种新概念能够提供更细粒度的表示和编码假设关系。3. 使用CoPa-SG数据集比较了不同场景图生成模型的性能。4. 证明了新关系类型可以增强下游应用的规划和推理能力。", "conclusion": "CoPa-SG数据集和提出的参数化与原型关系概念，为解决现有场景图数据不足和表示能力限制提供了有效途径，并有望提升场景理解、规划和推理等下游应用的性能。", "translation": "2D场景图为场景理解提供了一个结构化且可解释的框架。然而，当前的工作仍在努力解决场景图数据缺乏准确性的问题。为了克服这一数据瓶颈，我们提出了CoPa-SG，一个合成场景图数据集，具有高度精确的地面真值和所有对象之间详尽的关系标注。此外，我们引入了参数化关系和原型关系，这是场景图的两个新的基本概念。前者通过用角度或距离等附加参数丰富关系，提供了比传统对应物更细粒度的表示。后者在场景图中编码假设关系，并描述了如果新对象放置在场景中，关系将如何形成。使用CoPa-SG，我们比较了各种场景图生成模型的性能。我们演示了如何将我们的新关系类型集成到下游应用中，以增强规划和推理能力。", "summary": "本文针对2D场景图数据不足和关系表示粒度不够的问题，提出了CoPa-SG数据集，该数据集包含高精度真值和详尽的关系标注。同时，引入了参数化关系和原型关系两种新概念：参数化关系通过添加角度、距离等参数提供更细粒度的关系表示；原型关系编码假设性关系，描述新对象加入时关系的变化。研究使用CoPa-SG比较了现有场景图生成模型，并展示了新关系类型如何提升下游应用的规划和推理能力。", "keywords": "场景图, CoPa-SG, 参数化关系, 原型关系, 数据集", "comments": "CoPa-SG通过提供一个高度精确且详尽标注的合成数据集，创新性地解决了场景图数据稀缺的瓶颈问题。更重要的是，引入参数化和原型关系的概念，极大地丰富了场景图的表达能力。参数化关系使得场景图能够捕获更细致的空间或语义信息，而原型关系则为场景图提供了预测和推理未来场景变化的能力，这对于增强规划和机器人等下游应用的智能至关重要。这项工作为未来场景图的研究和应用开辟了新的方向。"}}
{"id": "2506.20764", "title": "Control and optimization for Neural Partial Differential Equations in Supervised Learning", "authors": ["Alain Bensoussan", "Minh-Binh Tran", "Bangjie Wang"], "summary": "Although there is a substantial body of literature on control and\noptimization problems for parabolic and hyperbolic systems, the specific\nproblem of controlling and optimizing the coefficients of the associated\noperators within such systems has not yet been thoroughly explored. In this\nwork, we aim to initiate a line of research in control theory focused on\noptimizing and controlling the coefficients of these operators-a problem that\nnaturally arises in the context of neural networks and supervised learning.\n  In supervised learning, the primary objective is to transport initial data\ntoward target data through the layers of a neural network. We propose a novel\nperspective: neural networks can be interpreted as partial differential\nequations (PDEs). From this viewpoint, the control problem traditionally\nstudied in the context of ordinary differential equations (ODEs) is\nreformulated as a control problem for PDEs, specifically targeting the\noptimization and control of coefficients in parabolic and hyperbolic operators.\nTo the best of our knowledge, this specific problem has not yet been\nsystematically addressed in the control theory of PDEs.\n  To this end, we propose a dual system formulation for the control and\noptimization problem associated with parabolic PDEs, laying the groundwork for\nthe development of efficient numerical schemes in future research. We also\nprovide a theoretical proof showing that the control and optimization problem\nfor parabolic PDEs admits minimizers. Finally, we investigate the control\nproblem associated with hyperbolic PDEs and prove the existence of solutions\nfor a corresponding approximated control problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20764v1", "categories": ["math.OC", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20764v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "监督学习中神经偏微分方程的控制与优化", "tldr": "该论文探索了抛物线和双曲线偏微分方程系数的控制与优化，将神经网络解释为偏微分方程，并证明了这些控制问题的极小值解/解的存在性，这在偏微分方程控制理论中是新颖的。", "motivation": "尽管关于抛物线和双曲线系统的控制和优化问题已有大量文献，但尚未彻底探讨这些系统中相关算子系数的控制和优化这一具体问题。这个问题在神经网络和监督学习中自然产生，且在偏微分方程控制理论中尚未得到系统性解决。", "method": "作者提出将神经网络解释为偏微分方程（PDEs），并将传统的常微分方程（ODEs）控制问题重新表述为偏微分方程的控制问题，特别针对抛物线和双曲线算子中系数的优化和控制。对于抛物线偏微分方程，他们提出了对偶系统公式并证明了极小值解的存在性。对于双曲线偏微分方程，他们研究了控制问题并证明了相应近似控制问题解的存在性。", "result": "他们为抛物线偏微分方程提出了一个对偶系统公式，并证明了极小值解的存在性。他们还研究了双曲线偏微分方程，并证明了近似控制问题解的存在性。", "conclusion": "该论文为偏微分方程系数的控制和优化奠定了基础，特别是在将神经网络解释为偏微分方程的背景下，并为这些新颖的控制问题提供了极小值解/解存在性的理论证明。", "translation": "尽管关于抛物线和双曲线系统的控制和优化问题已有大量文献，但尚未彻底探讨这些系统中相关算子系数的控制和优化这一具体问题。在这项工作中，我们旨在开创控制理论中的一个研究方向，专注于优化和控制这些算子的系数——这是一个在神经网络和监督学习背景下自然产生的问题。\n在监督学习中，主要目标是通过神经网络的层将初始数据传输到目标数据。我们提出了一种新颖的视角：神经网络可以被解释为偏微分方程（PDEs）。从这个观点出发，传统上在常微分方程（ODEs）背景下研究的控制问题被重新表述为偏微分方程的控制问题，特别是针对抛物线和双曲线算子中系数的优化和控制。据我们所知，这个问题在偏微分方程控制理论中尚未得到系统性的解决。\n为此，我们提出了一个与抛物线偏微分方程相关的控制和优化问题的对偶系统公式，为未来研究中高效数值方案的开发奠定了基础。我们还提供了理论证明，表明抛物线偏微分方程的控制和优化问题存在极小值解。最后，我们研究了与双曲线偏微分方程相关的控制问题，并证明了相应近似控制问题的解的存在性。", "summary": "本文通过将神经网络解释为偏微分方程（PDEs），提出了一种新颖的神经网络控制和优化方法。它解决了在偏微分方程系统中控制和优化抛物线和双曲线算子系数这一未充分探索的问题，这与监督学习相关。作者为抛物线偏微分方程提出了一个对偶系统公式，并证明了极小值解的存在性；同时，也证明了涉及双曲线偏微分方程的近似控制问题解的存在性，为未来的数值方案奠定了理论基础。", "keywords": "神经网络, 偏微分方程, 控制理论, 优化, 监督学习", "comments": "这篇论文通过将神经网络与偏微分方程联系起来，提供了一个创新的视角，将监督学习中的核心问题（数据通过层传输）重新构建为偏微分方程控制问题。对算子系数优化的关注是控制理论中一个重要且未被充分探索的领域，尤其是在应用于神经网络时。极小值解和解存在性的理论证明是关键的基础步骤，尽管摘要暗示了未来对高效数值方案的工作，表明目前在实际实现上存在局限性。"}}
{"id": "2506.20779", "title": "Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon", "authors": ["Tongtong Liang", "Dan Qiao", "Yu-Xiang Wang", "Rahul Parhi"], "summary": "We study the implicit bias of flatness / low (loss) curvature and its effects\non generalization in two-layer overparameterized ReLU networks with\nmultivariate inputs -- a problem well motivated by the minima stability and\nedge-of-stability phenomena in gradient-descent training. Existing work either\nrequires interpolation or focuses only on univariate inputs. This paper\npresents new and somewhat surprising theoretical results for multivariate\ninputs. On two natural settings (1) generalization gap for flat solutions, and\n(2) mean-squared error (MSE) in nonparametric function estimation by stable\nminima, we prove upper and lower bounds, which establish that while flatness\ndoes imply generalization, the resulting rates of convergence necessarily\ndeteriorate exponentially as the input dimension grows. This gives an\nexponential separation between the flat solutions vis-\\`a-vis low-norm\nsolutions (i.e., weight decay), which knowingly do not suffer from the curse of\ndimensionality. In particular, our minimax lower bound construction, based on a\nnovel packing argument with boundary-localized ReLU neurons, reveals how flat\nsolutions can exploit a kind of ''neural shattering'' where neurons rarely\nactivate, but with high weight magnitudes. This leads to poor performance in\nhigh dimensions. We corroborate these theoretical findings with extensive\nnumerical simulations. To the best of our knowledge, our analysis provides the\nfirst systematic explanation for why flat minima may fail to generalize in high\ndimensions.", "comment": "Comments Welcome!", "pdf_url": "http://arxiv.org/pdf/2506.20779v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20779v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "ReLU神经网络的稳定最小值遭受维度诅咒：神经破碎现象", "tldr": "在ReLU网络中，平坦最小值虽然有助于泛化，但其收敛速度会随着输入维度的增加呈指数级恶化，这是一种“神经破碎”现象，与低范数解形成对比。", "motivation": "研究平坦度/低损失曲率的隐式偏差及其对具有多元输入的双层过参数ReLU网络泛化能力的影响，该问题受到梯度下降训练中最小值稳定性和边缘稳定性现象的启发。现有工作要么需要插值，要么只关注单变量输入。", "method": "通过理论分析，证明了平坦解的泛化差距和稳定最小值在非参数函数估计中均方误差的上下界。采用了一种基于边界局部化ReLU神经元的新颖填充论证来构建极小极大下界。通过大量的数值模拟来证实理论发现。", "result": "证明了虽然平坦度确实意味着泛化，但随着输入维度的增长，由此产生的收敛速度必然呈指数级恶化。揭示了平坦解与低范数解（即权重衰减）之间在维度诅咒方面的指数级分离。提出了“神经破碎”现象，即平坦解可以利用很少激活但具有高权重幅度的神经元，导致在高维度下性能不佳。", "conclusion": "本研究首次系统地解释了为什么平坦最小值在高维度下可能无法泛化，并将其与性能随维度呈指数级下降的“神经破碎”现象联系起来。", "translation": "我们研究了平坦度/低（损失）曲率的隐式偏差及其对具有多元输入的双层过参数ReLU网络泛化能力的影响——这个问题受到梯度下降训练中最小值稳定性和边缘稳定性现象的充分启发。现有工作要么需要插值，要么只关注单变量输入。本文提出了针对多元输入的一些新颖且有些令人惊讶的理论结果。在两种自然设置下：（1）平坦解的泛化差距，以及（2）稳定最小值在非参数函数估计中的均方误差（MSE），我们证明了上下界，这些界限表明，虽然平坦度确实意味着泛化，但随着输入维度的增长，由此产生的收敛速度必然呈指数级恶化。这导致了平坦解与低范数解（即权重衰减）之间的指数级分离，众所周知，低范数解不会遭受维度诅咒。特别是，我们的极小极大下界构造，基于一种新颖的边界局部化ReLU神经元的填充论证，揭示了平坦解如何利用一种“神经破碎”现象，即神经元很少激活，但具有高权重幅度。这导致在高维度下性能不佳。我们通过大量的数值模拟证实了这些理论发现。据我们所知，我们的分析首次系统地解释了为什么平坦最小值在高维度下可能无法泛化。", "summary": "本文研究了具有多元输入的双层过参数ReLU网络中平坦/低曲率最小值的泛化性能，该主题受到梯度下降现象的启发。理论上证明，虽然平坦度有助于泛化，但其收敛速度会随着输入维度的增加呈指数级恶化，这与低范数解形成鲜明对比。论文引入了“神经破碎”现象，这是一种在高维度下平坦解依赖于很少激活但具有大权重的神经元的机制，从而导致性能不佳。这些理论发现得到了数值模拟的支持，为高维度下平坦最小值泛化失败提供了系统性解释。", "keywords": "ReLU网络, 平坦最小值, 泛化, 维度诅咒, 神经破碎", "comments": "本文通过系统地解释了为何通常与良好泛化能力相关的平坦最小值，在ReLU网络的高维设置中可能出现反常的泛化失败，从而做出了重要的理论贡献。引入“神经破碎”现象为这种现象提供了一种新颖且直观的机制，区分了平坦解和低范数解。严谨的上下界证明与数值验证相结合，增强了研究结果的说服力，并填补了深度学习中隐式偏差理解的关键空白。"}}
{"id": "2506.21364", "title": "CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection", "authors": ["Zhixin Cheng", "Jiacheng Deng", "Xinjun Li", "Xiaotian Yin", "Bohao Liao", "Baoqun Yin", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Detection-free methods typically follow a coarse-to-fine pipeline, extracting\nimage and point cloud features for patch-level matching and refining dense\npixel-to-point correspondences. However, differences in feature channel\nattention between images and point clouds may lead to degraded matching\nresults, ultimately impairing registration accuracy. Furthermore, similar\nstructures in the scene could lead to redundant correspondences in cross-modal\nmatching. To address these issues, we propose Channel Adaptive Adjustment\nModule (CAA) and Global Optimal Selection Module (GOS). CAA enhances\nintra-modal features and suppresses cross-modal sensitivity, while GOS replaces\nlocal selection with global optimization. Experiments on RGB-D Scenes V2 and\n7-Scenes demonstrate the superiority of our method, achieving state-of-the-art\nperformance in image-to-point cloud registration.", "comment": "ICCV 2025 accepted", "pdf_url": "http://arxiv.org/pdf/2506.21364v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21364v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CA-I2P：通道自适应配准网络与全局最优选择", "tldr": "CA-I2P通过通道自适应调整和全局最优选择，解决了图像与点云配准中的特征通道注意力差异和冗余对应问题，达到了最先进的配准性能。", "motivation": "现有的检测无关方法在图像和点云之间特征通道注意力差异较大，以及场景中相似结构可能导致跨模态匹配中的冗余对应，从而影响配准精度。", "method": "本文提出了通道自适应调整模块（CAA）和全局最优选择模块（GOS）。CAA旨在增强模态内特征并抑制跨模态敏感性，而GOS则将局部选择替换为全局优化。", "result": "在RGB-D Scenes V2和7-Scenes数据集上的实验表明，CA-I2P方法具有优越性，并在图像到点云配准中达到了最先进的性能。", "conclusion": "CA-I2P通过引入通道自适应调整模块（CAA）和全局最优选择模块（GOS），有效地解决了图像到点云配准中存在的特征通道注意力差异和冗余对应问题，显著提高了配准精度。", "translation": "无检测方法通常遵循从粗到精的流程，提取图像和点云特征进行块级匹配并细化密集的像素到点对应。然而，图像和点云之间特征通道注意力的差异可能导致匹配结果退化，最终损害配准精度。此外，场景中相似的结构可能导致跨模态匹配中的冗余对应。为了解决这些问题，我们提出了通道自适应调整模块（CAA）和全局最优选择模块（GOS）。CAA增强了模态内特征并抑制了跨模态敏感性，而GOS则用全局优化取代了局部选择。在RGB-D Scenes V2和7-Scenes上的实验证明了我们方法的优越性，在图像到点云配准中达到了最先进的性能。", "summary": "本文提出了CA-I2P网络，旨在解决图像到点云配准中特征通道注意力差异和冗余对应问题。为此，引入了通道自适应调整模块（CAA）以增强模态内特征并抑制跨模态敏感性，以及全局最优选择模块（GOS）以取代局部选择实现全局优化。实验结果表明，CA-I2P在图像到点云配准任务上取得了最先进的性能。", "keywords": "图像到点云配准, 通道自适应, 全局最优选择, 跨模态匹配, CA-I2P", "comments": "本文通过引入通道自适应调整和全局最优选择机制，有效地解决了图像与点云配准中常见的特征差异和冗余对应问题，提升了跨模态配准的鲁棒性和准确性，具有重要的创新性。"}}
{"id": "2506.21369", "title": "GenFlow: Interactive Modular System for Image Generation", "authors": ["Duc-Hung Nguyen", "Huu-Phuc Huynh", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21369v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21369v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GenFlow：图像生成交互式模块化系统", "tldr": "GenFlow是一个交互式模块化系统，旨在通过简化工作流程和降低技术门槛，让所有人都能轻松生成图像。", "motivation": "现有的生成艺术工具需要高级的技术专业知识，限制了其广泛应用。GenFlow旨在弥合这一差距，让所有技能水平的用户都能轻松生成图像。", "method": "GenFlow是一个模块化框架，具有基于节点的编辑器用于定制，以及由自然语言处理提供支持的智能助手。它自动化部署过程并最小化技术障碍。", "result": "用户研究表明，GenFlow能够优化工作流程，缩短任务完成时间，并通过其直观的界面和自适应功能增强用户理解。", "conclusion": "GenFlow是一个突破性的解决方案，重新定义了生成艺术领域的可访问性和效率。", "translation": "生成艺术开启了无限的创意可能性，但由于高级架构概念和计算工作流程所需的技术专业知识，其全部潜力仍未被充分利用。为了弥合这一差距，我们推出了GenFlow，一个新颖的模块化框架，使所有技能水平的用户都能精确而轻松地生成图像。GenFlow拥有用于无缝定制的基于节点的编辑器和由自然语言处理提供支持的智能助手，将工作流程创建的复杂性转化为直观且易于访问的体验。通过自动化部署过程并最大程度地减少技术障碍，我们的框架使尖端的生成艺术工具可供所有人使用。一项用户研究表明，GenFlow通过其直观的界面和自适应功能，能够优化工作流程、缩短任务完成时间并增强用户理解。这些结果将GenFlow定位为一个突破性的解决方案，重新定义了生成艺术领域的可访问性和效率。", "summary": "GenFlow是一个创新的模块化系统，旨在通过提供直观的基于节点的编辑器和智能助手，降低生成艺术的技术门槛。它自动化复杂流程，使用户能够轻松高效地生成图像。用户研究证实了其在优化工作流程、提高效率和用户理解方面的有效性，使其成为生成艺术领域可访问性的重要进步。", "keywords": "生成艺术, 模块化系统, 图像生成, 用户体验, 可访问性", "comments": "GenFlow的创新之处在于其将复杂的生成艺术工作流程模块化和用户友好化，通过节点编辑器和NLP助手显著降低了技术门槛。这对于普及生成艺术工具，让更多非专业人士参与创作具有重要意义。其关注用户体验和可访问性是其关键优势。"}}
{"id": "2506.21398", "title": "FastRef:Fast Prototype Refinement for Few-Shot Industrial Anomaly Detection", "authors": ["Long Tian", "Yufei Li", "Yuyang Dai", "Wenchao Chen", "Xiyang Liu", "Bo Chen"], "summary": "Few-shot industrial anomaly detection (FS-IAD) presents a critical challenge\nfor practical automated inspection systems operating in data-scarce\nenvironments. While existing approaches predominantly focus on deriving\nprototypes from limited normal samples, they typically neglect to\nsystematically incorporate query image statistics to enhance prototype\nrepresentativeness. To address this issue, we propose FastRef, a novel and\nefficient prototype refinement framework for FS-IAD. Our method operates\nthrough an iterative two-stage process: (1) characteristic transfer from query\nfeatures to prototypes via an optimizable transformation matrix, and (2)\nanomaly suppression through prototype alignment. The characteristic transfer is\nachieved through linear reconstruction of query features from prototypes, while\nthe anomaly suppression addresses a key observation in FS-IAD that unlike\nconventional IAD with abundant normal prototypes, the limited-sample setting\nmakes anomaly reconstruction more probable. Therefore, we employ optimal\ntransport (OT) for non-Gaussian sampled features to measure and minimize the\ngap between prototypes and their refined counterparts for anomaly suppression.\nFor comprehensive evaluation, we integrate FastRef with three competitive\nprototype-based FS-IAD methods: PatchCore, FastRecon, WinCLIP, and AnomalyDINO.\nExtensive experiments across four benchmark datasets of MVTec, ViSA, MPDD and\nRealIAD demonstrate both the effectiveness and computational efficiency of our\napproach under 1/2/4-shots.", "comment": "18pages, 7figures, 6tables", "pdf_url": "http://arxiv.org/pdf/2506.21398v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21398v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FastRef：用于少样本工业异常检测的快速原型细化", "tldr": "FastRef提出了一种新颖高效的原型细化框架，通过两阶段迭代过程解决了少样本工业异常检测中原型代表性不足的问题，并表现出高效和有效性。", "motivation": "少样本工业异常检测（FS-IAD）在数据稀缺环境下对实际自动化检测系统构成严峻挑战。现有方法主要关注从有限正常样本中提取原型，但通常忽略系统地结合查询图像统计信息来增强原型代表性。", "method": "提出FastRef，一个新颖高效的FS-IAD原型细化框架。该方法通过迭代的两阶段过程运行：1) 通过可优化变换矩阵将查询特征的特性转移到原型中（通过原型对查询特征的线性重建实现）；2) 通过原型对齐抑制异常（利用最优传输OT测量并最小化原型与其细化对应物之间的差距）。FastRef可与PatchCore、FastRecon、WinCLIP和AnomalyDINO等方法集成。", "result": "在MVTec、ViSA、MPDD和RealIAD四个基准数据集上的大量实验证明了该方法在1/2/4-shot设置下的有效性和计算效率。", "conclusion": "论文提出了FastRef，一个新颖高效的FS-IAD原型细化框架，通过两阶段迭代过程（特征转移和异常抑制）解决了原型代表性问题，并在多个基准数据集上验证了其有效性和计算效率。", "translation": "少样本工业异常检测（FS-IAD）对在数据稀缺环境中运行的实际自动化检测系统提出了严峻挑战。现有方法主要侧重于从有限的正常样本中获取原型，但通常忽略系统地整合查询图像统计信息以增强原型的代表性。为了解决这个问题，我们提出了FastRef，一个新颖高效的FS-IAD原型细化框架。我们的方法通过一个迭代的两阶段过程运行：（1）通过可优化的变换矩阵将查询特征的特性转移到原型中，以及（2）通过原型对齐抑制异常。特性转移是通过原型对查询特征的线性重建实现的，而异常抑制解决了FS-IAD中的一个关键观察，即与具有丰富正常原型的传统IAD不同，有限样本设置使得异常重建的可能性更大。因此，我们对非高斯采样特征采用最优传输（OT）来衡量和最小化原型与其细化对应物之间的差距以抑制异常。为了进行全面评估，我们将FastRef与三种有竞争力的基于原型的FS-IAD方法：PatchCore、FastRecon、WinCLIP和AnomalyDINO进行了集成。在MVTec、ViSA、MPDD和RealIAD四个基准数据集上进行的广泛实验证明了我们方法在1/2/4-shot设置下的有效性和计算效率。", "summary": "本文提出了FastRef，一个用于少样本工业异常检测（FS-IAD）的新型高效原型细化框架。针对现有方法在数据稀缺环境下原型代表性不足的问题，FastRef采用迭代两阶段过程：首先通过可优化变换矩阵将查询特征的特性转移到原型，然后通过原型对齐抑制异常，其中利用最优传输（OT）处理非高斯特征。实验在MVTec、ViSA等四个基准数据集上验证了FastRef在1/2/4-shot设置下的有效性和计算效率，并可与现有SOTA方法集成。", "keywords": "少样本工业异常检测, 原型细化, 最优传输, 特征转移, 异常抑制", "comments": "FastRef的创新之处在于其提出的两阶段迭代原型细化过程，特别是利用最优传输（OT）来解决少样本设置下异常更容易被重建的问题。该方法不仅提高了原型代表性，还兼顾了计算效率，这对于实际工业应用至关重要。与现有方法的集成能力也增强了其普适性。"}}
{"id": "2506.21401", "title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction", "authors": ["Zhirui Gao. Renjiao Yi", "Yaqiao Dai", "Xuening Zhu", "Wei Chen", "Chenyang Zhu", "Kai Xu"], "summary": "This paper presents an end-to-end framework for reconstructing 3D parametric\ncurves directly from multi-view edge maps. Contrasting with existing two-stage\nmethods that follow a sequential ``edge point cloud reconstruction and\nparametric curve fitting'' pipeline, our one-stage approach optimizes 3D\nparametric curves directly from 2D edge maps, eliminating error accumulation\ncaused by the inherent optimization gap between disconnected stages. However,\nparametric curves inherently lack suitability for rendering-based multi-view\noptimization, necessitating a complementary representation that preserves their\ngeometric properties while enabling differentiable rendering. We propose a\nnovel bi-directional coupling mechanism between parametric curves and\nedge-oriented Gaussian components. This tight correspondence formulates a\ncurve-aware Gaussian representation, \\textbf{CurveGaussian}, that enables\ndifferentiable rendering of 3D curves, allowing direct optimization guided by\nmulti-view evidence. Furthermore, we introduce a dynamically adaptive topology\noptimization framework during training to refine curve structures through\nlinearization, merging, splitting, and pruning operations. Comprehensive\nevaluations on the ABC dataset and real-world benchmarks demonstrate our\none-stage method's superiority over two-stage alternatives, particularly in\nproducing cleaner and more robust reconstructions. Additionally, by directly\noptimizing parametric curves, our method significantly reduces the parameter\ncount during training, achieving both higher efficiency and superior\nperformance compared to existing approaches.", "comment": "Code: https://github.com/zhirui-gao/Curve-Gaussian Accepted by ICCV\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21401v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21401v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "曲线感知高斯泼溅用于三维参数曲线重建", "tldr": "该论文提出了一种名为CurveGaussian的单阶段端到端框架，用于直接从多视图边缘图重建三维参数曲线，通过新颖的可微分表示和自适应拓扑优化，克服了传统两阶段方法的误差累积问题，实现了更高效、更鲁棒的重建。", "motivation": "现有两阶段方法在“边缘点云重建和参数曲线拟合”的顺序管道中存在固有的优化差距，导致误差累积；同时，参数曲线本身不适合基于渲染的多视图优化。", "method": "提出了一种端到端单阶段框架。通过参数曲线和面向边缘的高斯分量之间的新型双向耦合机制，形成了“曲线感知高斯表示（CurveGaussian）”，实现了三维曲线的可微分渲染，从而可以直接通过多视图证据进行优化。此外，在训练过程中引入了动态自适应拓扑优化框架，通过线性化、合并、分裂和修剪操作来细化曲线结构。", "result": "在ABC数据集和真实世界基准测试中，该单阶段方法优于两阶段替代方案，尤其在生成更清晰、更鲁棒的重建方面表现出色。通过直接优化参数曲线，该方法显著减少了训练期间的参数数量，与现有方法相比，实现了更高的效率和卓越的性能。", "conclusion": "所提出的单阶段方法CurveGaussian通过直接从二维边缘图重建三维参数曲线，克服了两阶段方法的局限性，通过实现可微分优化，在效率和性能上均优于现有方法。", "translation": "本文提出了一种端到端框架，用于直接从多视图边缘图重建三维参数曲线。与现有遵循“边缘点云重建和参数曲线拟合”顺序管道的两阶段方法形成对比，我们的一阶段方法直接从二维边缘图优化三维参数曲线，消除了由不连贯阶段之间固有的优化差距引起的误差累积。然而，参数曲线本身不适合基于渲染的多视图优化，因此需要一种互补的表示，既能保留其几何特性，又能实现可微分渲染。我们提出了一种参数曲线和面向边缘的高斯分量之间的新型双向耦合机制。这种紧密的对应关系形成了曲线感知高斯表示，即\\textbf{CurveGaussian}，它能够实现三维曲线的可微分渲染，从而允许通过多视图证据直接进行优化。此外，我们在训练期间引入了动态自适应拓扑优化框架，通过线性化、合并、分裂和修剪操作来细化曲线结构。在ABC数据集和真实世界基准测试上的综合评估表明，我们的一阶段方法优于两阶段替代方案，特别是在生成更清晰、更鲁棒的重建方面。此外，通过直接优化参数曲线，我们的方法显著减少了训练期间的参数数量，与现有方法相比，实现了更高的效率和卓越的性能。", "summary": "本文提出了一种名为CurveGaussian的新型单阶段端到端框架，用于直接从多视图边缘图重建三维参数曲线。与传统的两阶段方法不同，它通过引入一种新颖的曲线感知高斯表示来实现可微分渲染，从而避免了误差累积，并允许直接优化曲线。该方法还结合了自适应拓扑优化，并在实验中证明其在重建质量、鲁棒性和效率方面均优于现有两阶段方法。", "keywords": "三维参数曲线重建, 高斯泼溅, 可微分渲染, 边缘图, 单阶段优化", "comments": "该论文的创新之处在于提出了一个端到端的单阶段三维参数曲线重建框架，通过引入CurveGaussian这一新颖的可微分表示和自适应拓扑优化，有效解决了传统两阶段方法中存在的误差累积和优化鸿沟问题。其重要性在于实现了更精确、更鲁棒且更高效的三维曲线重建。"}}
{"id": "2506.21278", "title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution", "authors": ["Lukas Sablica", "Kurt Hornik"], "summary": "We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21278v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21278v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用高效球面柯西分布的超球面变分自编码器", "tldr": "提出了一种使用球面柯西（spCauchy）潜在分布的新型变分自编码器（VAE）架构，它在超球面表示、避免过正则化、解决数值不稳定性和高效重参数化方面优于传统方法。", "motivation": "传统的变分自编码器（VAE）中使用高斯潜在空间或广泛使用的von Mises-Fisher（vMF）分布存在局限性，例如不能很好地捕获方向性数据、可能导致过正则化以及vMF在计算归一化常数时存在数值不稳定性。因此，需要一种更自然、更稳定、更高效的超球面潜在变量表示。", "method": "本研究提出了一种新型的变分自编码器（VAE）架构，该架构采用球面柯西（spCauchy）作为潜在分布。spCauchy通过莫比乌斯变换实现完全可微分且高效的重参数化技巧，并且其KL散度可以通过快速收敛的幂级数计算。", "result": "球面柯西（spCauchy）潜在分布能够提供更自然的超球面表示，更好地捕获方向性数据，同时保持灵活性。其重尾性质防止了过正则化，确保了潜在空间的有效利用，并提供了更具表达力的表示。此外，spCauchy避免了vMF固有的数值不稳定性，实现了稳定和可扩展的训练。KL散度计算消除了下溢或上溢的担忧。", "conclusion": "球面柯西（spCauchy）分布是变分自编码器（VAE）的一个引人注目的替代方案，在高维生成建模中提供了理论优势和实际效率。", "translation": "我们提出了一种新颖的变分自编码器（VAE）架构，该架构采用球面柯西（spCauchy）潜在分布。与传统的高斯潜在空间或广泛使用的von Mises-Fisher（vMF）分布不同，spCauchy提供了一种更自然的潜在变量超球面表示，能更好地捕获方向性数据，同时保持灵活性。其重尾性质防止了过正则化，确保了潜在空间的有效利用，同时提供了更具表达力的表示。此外，spCauchy避免了vMF固有的数值不稳定性，这些不稳定性源于计算涉及贝塞尔函数的归一化常数。相反，它通过莫比乌斯变换实现了完全可微分且高效的重参数化技巧，从而实现稳定和可扩展的训练。KL散度可以通过快速收敛的幂级数计算，消除了与超几何函数比值评估相关的下溢或上溢问题。这些特性使得spCauchy成为VAE的一个引人注目的替代方案，在高维生成建模中提供了理论优势和实际效率。", "summary": "该论文提出了一种新的变分自编码器（VAE）架构，其核心是使用球面柯西（spCauchy）作为潜在分布。这种方法相较于传统的高斯或von Mises-Fisher（vMF）分布，能更自然地表示超球面潜在变量，有效捕获方向性数据，并避免过正则化。spCauchy通过莫比乌斯变换实现高效且稳定的重参数化，解决了vMF的数值不稳定问题，并且KL散度计算更加鲁棒。这些特性使其成为高维生成建模中一个具有理论和实践优势的替代方案。", "keywords": "变分自编码器, 球面柯西分布, 超球面表示, 重参数化技巧, 方向性数据", "comments": "这项工作创新性地将球面柯西分布应用于变分自编码器（VAE）的潜在空间，解决了传统方法在处理超球面数据时的局限性和数值稳定性问题。其通过重尾特性防止过正则化，并利用莫比乌斯变换实现高效重参数化，显著提升了模型在方向性数据建模和高维生成任务中的性能和鲁棒性。"}}
{"id": "2506.20831", "title": "Efficacy of Temporal Fusion Transformers for Runoff Simulation", "authors": ["Sinan Rasiya Koya", "Tirthankar Roy"], "summary": "Combining attention with recurrence has shown to be valuable in sequence\nmodeling, including hydrological predictions. Here, we explore the strength of\nTemporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks\nin rainfall-runoff modeling. We train ten randomly initialized models, TFT and\nLSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five\nsubsets of the Caravan dataset, each representing catchments in the US,\nAustralia, Brazil, Great Britain, and Chile. Then, the performance of the\nmodels, their variability regarding the catchment attributes, and the\ndifference according to the datasets are assessed. Our findings show that TFT\nslightly outperforms LSTM, especially in simulating the midsection and peak of\nhydrographs. Furthermore, we show the ability of TFT to handle longer sequences\nand why it can be a better candidate for higher or larger catchments. Being an\nexplainable AI technique, TFT identifies the key dynamic and static variables,\nproviding valuable scientific insights. However, both TFT and LSTM exhibit a\nconsiderable drop in performance with the Caravan dataset, indicating possible\ndata quality issues. Overall, the study highlights the potential of TFT in\nimproving hydrological modeling and understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20831v1", "categories": ["physics.geo-ph", "cs.LG", "stat.AP"], "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.20831v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "径流模拟中时态融合变换器的有效性", "tldr": "时态融合变换器（TFT）在径流模拟中略优于长短期记忆网络（LSTM），特别是在处理峰值和长序列方面，且具有可解释性，尽管数据质量可能影响性能。", "motivation": "鉴于注意力机制与循环神经网络结合在序列建模（包括水文预测）中的价值，本研究旨在探讨时态融合变换器（TFTs）在降雨径流建模中相对于长短期记忆网络（LSTM）的优势。", "method": "研究在美国的531个CAMELS流域上训练了10个随机初始化的TFT和LSTM模型。随后，实验使用Caravan数据集的五个子集重复进行，这些子集分别代表美国、澳大利亚、巴西、英国和智利的流域。评估了模型的性能、其在流域属性方面的变异性以及不同数据集之间的差异。", "result": "研究发现，TFT略优于LSTM，尤其是在模拟水文过程线的中间部分和峰值方面。此外，TFT能够处理更长的序列，并且是更高或更大流域的更好选择。作为一种可解释的人工智能技术，TFT能够识别关键的动态和静态变量，提供有价值的科学见解。然而，TFT和LSTM在Caravan数据集上的性能均出现显著下降，这可能表明存在数据质量问题。", "conclusion": "这项研究突出了TFT在改进水文建模和理解方面的潜力。", "translation": "将注意力机制与循环神经网络相结合已被证明在序列建模（包括水文预测）中具有重要价值。本文探讨了时态融合变换器（TFTs）在降雨径流建模中相对于长短期记忆网络（LSTM）的优势。我们针对美国531个CAMELS流域，训练了10个随机初始化的TFT和LSTM模型。我们使用Caravan数据集的五个子集重复了实验，每个子集代表美国、澳大利亚、巴西、英国和智利的流域。然后，评估了模型的性能、它们在流域属性方面的变异性以及不同数据集之间的差异。我们的研究结果表明，TFT略优于LSTM，特别是在模拟水文过程线的中间部分和峰值方面。此外，我们展示了TFT处理更长序列的能力，以及为什么它可能成为更高或更大流域的更好选择。作为一种可解释的人工智能技术，TFT能够识别关键的动态和静态变量，提供有价值的科学见解。然而，TFT和LSTM在Caravan数据集上的性能均表现出相当大的下降，这表明可能存在数据质量问题。总的来说，这项研究强调了TFT在改进水文建模和理解方面的潜力。", "summary": "本文探讨了时态融合变换器（TFTs）在降雨径流模拟中相较于长短期记忆网络（LSTM）的有效性。研究在美国CAMELS和全球Caravan数据集上进行了实验，结果显示TFTs性能略优于LSTMs，尤其是在模拟水文过程线峰值和处理长序列方面，并且TFTs作为可解释AI能提供有价值的科学洞察。尽管TFTs展现出改进水文建模的潜力，但研究也指出在特定数据集上模型性能下降，可能与数据质量问题有关。", "keywords": "时态融合变换器, 径流模拟, 水文建模, LSTM, 可解释人工智能", "comments": "本文将时态融合变换器（TFTs）这一可解释的人工智能技术引入水文建模领域，并证明其在径流模拟中优于传统的LSTM模型，尤其在处理水文过程线峰值和长序列方面表现出色。TFT的可解释性为科学洞察提供了重要价值，是其创新之处。同时，研究指出了Caravan数据集可能存在的数据质量问题，这既是局限性，也为未来研究方向提供了线索。"}}
{"id": "2506.21416", "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation", "authors": ["Bowen Chen", "Mengyi Zhao", "Haomiao Sun", "Li Chen", "Xu Wang", "Kang Du", "Xinglong Wu"], "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.", "comment": "Project Page: https://bytedance.github.io/XVerse Github Link:\n  https://github.com/bytedance/XVerse", "pdf_url": "http://arxiv.org/pdf/2506.21416v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21416v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "XVerse：通过DiT调制实现身份和语义属性的多主体一致性控制", "tldr": "XVerse是一个新的多主体图像生成模型，它通过将参考图像转换为文本流调制偏移量，解决了现有方法在多主体文本到图像生成中控制主体身份和语义属性时出现的编辑性差和属性纠缠问题，从而实现了高保真、可编辑的多主体图像合成。", "motivation": "在文本到图像生成中，特别是在多主体场景下，对主体身份和语义属性（姿态、风格、光照）进行细粒度控制，往往会损害Diffusion Transformers（DiTs）的可编辑性和连贯性。许多现有方法会引入伪影或遭受属性纠缠。", "method": "我们提出了一种新颖的多主体受控生成模型XVerse。通过将参考图像转换为特定令牌文本流调制的偏移量，XVerse能够对特定主体进行精确和独立的控制，而不会破坏图像潜在特征或图像特征。", "result": "XVerse提供了高保真、可编辑的多主体图像合成，并能对个体主体特征和语义属性进行稳健控制。", "conclusion": "XVerse显著提高了个性化和复杂场景的生成能力，有效解决了多主体文本到图像生成中控制主体身份和语义属性的挑战。", "translation": "在文本到图像生成中，特别是对于多个主体，对主体身份和语义属性（姿态、风格、光照）进行细粒度控制，往往会损害Diffusion Transformers（DiTs）的可编辑性和连贯性。许多方法会引入伪影或遭受属性纠缠。为了克服这些挑战，我们提出了一种新颖的多主体受控生成模型XVerse。通过将参考图像转换为特定令牌文本流调制的偏移量，XVerse能够对特定主体进行精确和独立的控制，而不会破坏图像潜在特征或图像特征。因此，XVerse提供了高保真、可编辑的多主体图像合成，并能对个体主体特征和语义属性进行稳健控制。这项进展显著提高了个性化和复杂场景的生成能力。", "summary": "XVerse是一个创新的多主体文本到图像生成模型，旨在解决现有Diffusion Transformers在处理多主体时控制身份和语义属性（如姿态、风格、光照）的挑战，这些挑战通常导致编辑性差和属性纠缠。XVerse通过将参考图像转化为特定令牌文本流的调制偏移量，实现了对每个主体的精确独立控制，避免了对图像潜在特征的干扰。这使得XVerse能够生成高保真、可编辑的多主体图像，并对个体特征和语义属性具有强大的控制能力，从而显著提升了个性化和复杂场景的生成效果。", "keywords": "多主体控制, 文本到图像生成, Diffusion Transformers, 身份控制, 语义属性", "comments": "该论文提出了一种创新的DiT调制方法，通过将参考图像转换为文本流偏移量，实现了对多主体图像生成中身份和语义属性的精细且独立控制。其核心创新在于解决了传统方法中常见的编辑性受损和属性纠缠问题，为个性化和复杂场景的生成提供了更强大的能力。"}}
{"id": "2506.20839", "title": "Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys", "authors": ["Jing Luo", "Yejun Gu", "Yanfei Wang", "Xiaolong Ma", "Jaafar. A El-Awady"], "summary": "Machine learning has significantly advanced the understanding and application\nof structural materials, with an increasing emphasis on integrating existing\ndata and quantifying uncertainties in predictive modeling. This study presents\na comprehensive methodology utilizing a mixed density network (MDN) model,\ntrained on extensive experimental data from literature. This approach uniquely\npredicts the distribution of dislocation density, inferred as a latent\nvariable, and the resulting stress distribution at the grain level. The\nincorporation of statistical parameters of those predicted distributions into a\ndislocation-mediated plasticity model allows for accurate stress-strain\npredictions with explicit uncertainty quantification. This strategy not only\nimproves the accuracy and reliability of mechanical property predictions but\nalso plays a vital role in optimizing alloy design, thereby facilitating the\ndevelopment of new materials in a rapidly evolving industry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20839v1", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.20839v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "面向FCC合金位错塑性与应力-应变响应预测的不确定性感知机器学习框架", "tldr": "本研究提出了一种基于混合密度网络（MDN）的机器学习框架，能够预测FCC合金的位错塑性与应力-应变响应，并明确量化不确定性，从而提高材料性能预测的准确性和可靠性，并有助于合金设计优化。", "motivation": "机器学习在结构材料理解和应用方面取得了显著进展，但需要进一步整合现有数据并量化预测模型中的不确定性。本研究旨在解决这一需求，提高机械性能预测的准确性和可靠性，并优化合金设计。", "method": "本研究采用混合密度网络（MDN）模型，利用大量实验数据进行训练。该方法独特地预测位错密度（推断为潜在变量）及其产生的晶粒级应力分布。通过将这些预测分布的统计参数整合到位错介导的塑性模型中，实现了对力学性能的准确预测并明确量化了不确定性。", "result": "该方法不仅提高了机械性能预测的准确性和可靠性，而且在优化合金设计方面发挥了关键作用。", "conclusion": "所提出的不确定性感知机器学习框架显著提升了FCC合金机械性能预测的准确性和可靠性，并为优化合金设计和加速新材料开发提供了有效途径。", "translation": "机器学习在理解和应用结构材料方面取得了显著进展，越来越强调整合现有数据并量化预测模型中的不确定性。本研究提出了一种综合方法，利用混合密度网络（MDN）模型，并基于文献中的大量实验数据进行训练。这种方法独特地预测位错密度（被推断为潜在变量）的分布以及由此产生的晶粒级应力分布。将这些预测分布的统计参数纳入位错介导的塑性模型中，可以实现准确的应力-应变预测，并明确量化不确定性。该策略不仅提高了机械性能预测的准确性和可靠性，而且在优化合金设计方面发挥了至关重要的作用，从而促进了快速发展行业中新材料的开发。", "summary": "本研究开发了一个不确定性感知的机器学习框架，利用混合密度网络（MDN）模型，通过预测位错密度和晶粒级应力分布，并结合位错介导的塑性模型，实现了对FCC合金应力-应变响应的准确预测和明确的不确定性量化。该方法显著提升了机械性能预测的准确性和可靠性，对合金设计优化和新材料开发具有重要意义。", "keywords": "不确定性量化, 机器学习, 位错塑性, 应力-应变响应, FCC合金", "comments": "本文的创新点在于提出了一个能够量化不确定性的机器学习框架，这在材料科学领域尤为重要，因为材料性能预测常伴随固有的不确定性。通过结合MDN模型和位错介导的塑性理论，该研究提供了一个更可靠的工具来预测材料行为，并有望加速新材料的发现和设计过程。其对位错密度作为潜在变量的预测，是理解微观机制与宏观性能之间关系的关键一步。"}}
{"id": "2506.21430", "title": "HyperSORT: Self-Organising Robust Training with hyper-networks", "authors": ["Samuel Joutard", "Marijn Stollenga", "Marc Balle Sanchez", "Mohammad Farid Azampour", "Raphael Prevost"], "summary": "Medical imaging datasets often contain heterogeneous biases ranging from\nerroneous labels to inconsistent labeling styles. Such biases can negatively\nimpact deep segmentation networks performance. Yet, the identification and\ncharacterization of such biases is a particularly tedious and challenging task.\nIn this paper, we introduce HyperSORT, a framework using a hyper-network\npredicting UNets' parameters from latent vectors representing both the image\nand annotation variability. The hyper-network parameters and the latent vector\ncollection corresponding to each data sample from the training set are jointly\nlearned. Hence, instead of optimizing a single neural network to fit a dataset,\nHyperSORT learns a complex distribution of UNet parameters where low density\nareas can capture noise-specific patterns while larger modes robustly segment\norgans in differentiated but meaningful manners. We validate our method on two\n3D abdominal CT public datasets: first a synthetically perturbed version of the\nAMOS dataset, and TotalSegmentator, a large scale dataset containing real\nunknown biases and errors. Our experiments show that HyperSORT creates a\nstructured mapping of the dataset allowing the identification of relevant\nsystematic biases and erroneous samples. Latent space clusters yield UNet\nparameters performing the segmentation task in accordance with the underlying\nlearned systematic bias. The code and our analysis of the TotalSegmentator\ndataset are made available: https://github.com/ImFusionGmbH/HyperSORT", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21430v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21430v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HyperSORT: 自组织鲁棒训练与超网络", "tldr": "HyperSORT使用超网络预测UNet参数，通过学习数据变异性来处理医学图像数据集中的偏见和错误，从而实现鲁棒分割并识别数据集中的系统性偏见。", "motivation": "医学影像数据集常包含异质性偏见（如错误标签、不一致的标注风格），这些偏见会负面影响深度分割网络的性能。识别和表征这些偏见是特别繁琐和具有挑战性的任务。", "method": "本文引入了HyperSORT框架，该框架使用一个超网络从代表图像和标注变异性的潜在向量中预测UNet的参数。超网络参数和对应每个训练数据样本的潜在向量集合是联合学习的。因此，HyperSORT不是优化单个神经网络来拟合数据集，而是学习UNet参数的复杂分布，其中低密度区域可以捕获噪声特异性模式，而较大的模式则能以区分但有意义的方式鲁棒地分割器官。", "result": "该方法在两个3D腹部CT公共数据集（合成扰动的AMOS数据集和TotalSegmentator）上进行了验证。实验表明，HyperSORT创建了数据集的结构化映射，能够识别相关的系统性偏见和错误样本。潜在空间聚类产生的UNet参数能够根据学习到的潜在系统性偏见执行分割任务。", "conclusion": "HyperSORT能够有效处理医学影像数据集中的偏见和错误，通过学习复杂的UNet参数分布，不仅实现了鲁棒分割，还能识别并表征数据集中的系统性偏见和异常样本。", "translation": "医学影像数据集通常包含从错误标签到不一致标注风格的异质性偏见。这些偏见会对深度分割网络的性能产生负面影响。然而，识别和表征这些偏见是一项特别繁琐和具有挑战性的任务。在本文中，我们引入了HyperSORT，这是一个使用超网络根据代表图像和标注变异性的潜在向量预测UNets参数的框架。超网络参数和对应训练集中每个数据样本的潜在向量集合是联合学习的。因此，HyperSORT不是优化单个神经网络来拟合数据集，而是学习UNet参数的复杂分布，其中低密度区域可以捕获噪声特异性模式，而较大的模式则能以区分但有意义的方式鲁棒地分割器官。我们在两个3D腹部CT公共数据集上验证了我们的方法：首先是AMOS数据集的合成扰动版本，以及TotalSegmentator，一个包含真实未知偏见和错误的大规模数据集。我们的实验表明，HyperSORT创建了数据集的结构化映射，从而能够识别相关的系统性偏见和错误样本。潜在空间聚类产生了根据底层学习的系统性偏见执行分割任务的UNet参数。代码和我们对TotalSegmentator数据集的分析已公开：https://github.com/ImFusionGmbH/HyperSORT", "summary": "本文提出了HyperSORT框架，旨在解决医学影像数据集中存在的异质性偏见对深度分割网络性能的影响。HyperSORT利用超网络从潜在向量中预测UNet参数，这些潜在向量捕获了图像和标注的变异性。通过联合学习超网络参数和数据样本的潜在向量，HyperSORT能够学习UNet参数的复杂分布，从而在实现鲁棒器官分割的同时，识别并表征数据集中的系统性偏见和异常样本。该方法在合成扰动数据集和大规模真实数据集上均得到了有效验证。", "keywords": "医学影像, 数据偏见, 超网络, 鲁棒训练, 深度分割", "comments": "HyperSORT的创新之处在于其利用超网络和联合学习策略来处理医学影像数据中的异质性偏见，这不仅提升了分割的鲁棒性，更重要的是，它提供了一种结构化的方式来映射和理解数据集中的内在偏见和错误，这对于数据质量控制和模型可靠性评估具有重要意义。其能够识别系统性偏见和错误样本的能力是一个显著的优势。"}}
{"id": "2506.21444", "title": "Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation", "authors": ["Sweta Banerjee", "Viktoria Weiss", "Taryn A. Donovan", "Rutger A. Fick", "Thomas Conrad", "Jonas Ammeling", "Nils Porsche", "Robert Klopfleisch", "Christopher Kaltenecker", "Katharina Breininger", "Marc Aubreville", "Christof A. Bertram"], "summary": "Atypical mitoses mark a deviation in the cell division process that can be an\nindependent prognostically relevant marker for tumor malignancy. However, their\nidentification remains challenging due to low prevalence, at times subtle\nmorphological differences from normal mitoses, low inter-rater agreement among\npathologists, and class imbalance in datasets. Building on the Atypical Mitosis\ndataset for Breast Cancer (AMi-Br), this study presents a comprehensive\nbenchmark comparing deep learning approaches for automated atypical mitotic\nfigure (AMF) classification, including baseline models, foundation models with\nlinear probing, and foundation models fine-tuned with low-rank adaptation\n(LoRA). For rigorous evaluation, we further introduce two new hold-out AMF\ndatasets - AtNorM-Br, a dataset of mitoses from the The TCGA breast cancer\ncohort, and AtNorM-MD, a multi-domain dataset of mitoses from the MIDOG++\ntraining set. We found average balanced accuracy values of up to 0.8135,\n0.7696, and 0.7705 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and\nAtNorM-MD datasets, respectively, with the results being particularly good for\nLoRA-based adaptation of the Virchow-line of foundation models. Our work shows\nthat atypical mitosis classification, while being a challenging problem, can be\neffectively addressed through the use of recent advances in transfer learning\nand model fine-tuning techniques. We make available all code and data used in\nthis paper in this github repository:\nhttps://github.com/DeepMicroscopy/AMi-Br_Benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21444v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21444v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "深度学习和视觉基础模型在非典型与正常有丝分裂分类中的基准测试及跨数据集评估", "tldr": "该研究比较了深度学习和视觉基础模型在识别非典型有丝分裂方面的性能，并在多个数据集上进行了评估，发现LoRA微调的基础模型表现良好。", "motivation": "非典型有丝分裂是肿瘤恶性程度的独立预后相关标志物，但其识别具有挑战性，包括患病率低、形态差异细微、病理学家间一致性低以及数据集中类别不平衡。", "method": "本研究以乳腺癌非典型有丝分裂数据集（AMi-Br）为基础，全面比较了用于自动化非典型有丝分裂图像（AMF）分类的深度学习方法，包括基线模型、使用线性探测的基础模型以及使用低秩适应（LoRA）进行微调的基础模型。为了进行严格评估，引入了两个新的保留AMF数据集：AtNorM-Br（来自TCGA乳腺癌队列）和AtNorM-MD（来自MIDOG++训练集的多领域数据集）。", "result": "在域内AMi-Br数据集上，平均平衡准确率高达0.8135；在域外AtNorm-Br和AtNorM-MD数据集上，平均平衡准确率分别为0.7696和0.7705。特别是基于Virchow系列基础模型的LoRA适应性表现尤为出色。", "conclusion": "尽管非典型有丝分裂分类是一个具有挑战性的问题，但通过利用迁移学习和模型微调技术的最新进展可以有效解决。", "translation": "非典型有丝分裂标志着细胞分裂过程的偏差，其本身可以作为肿瘤恶性程度的独立预后相关标志物。然而，由于患病率低、有时与正常有丝分裂的形态差异细微、病理学家间判读一致性低以及数据集中类别不平衡，其识别仍然具有挑战性。本研究以乳腺癌非典型有丝分裂数据集（AMi-Br）为基础，提出了一个全面的基准测试，比较了用于自动化非典型有丝分裂图像（AMF）分类的深度学习方法，包括基线模型、使用线性探测的基础模型以及使用低秩适应（LoRA）进行微调的基础模型。为了进行严格评估，我们进一步引入了两个新的保留AMF数据集——AtNorM-Br（一个来自TCGA乳腺癌队列的有丝分裂数据集）和AtNorM-MD（一个来自MIDOG++训练集的多领域有丝分裂数据集）。我们发现，在域内AMi-Br数据集上，平均平衡准确率高达0.8135；在域外AtNorm-Br和AtNorM-MD数据集上，平均平衡准确率分别为0.7696和0.7705，其中基于Virchow系列基础模型的LoRA适应性表现尤为出色。我们的工作表明，非典型有丝分裂分类虽然是一个具有挑战性的问题，但可以通过利用迁移学习和模型微调技术的最新进展来有效解决。本论文中使用的所有代码和数据均可在以下GitHub仓库获取：https://github.com/DeepMicroscopy/AMi-Br_Benchmark。", "summary": "本研究旨在解决非典型有丝分裂识别的挑战，通过对深度学习和视觉基础模型进行基准测试。研究比较了基线模型、线性探测和LoRA微调的基础模型在AMi-Br、AtNorM-Br和AtNorM-MD三个数据集上的性能。结果显示，LoRA微调的基础模型，特别是Virchow系列，在非典型有丝分裂分类上取得了良好的平衡准确率，证明了迁移学习和微调在解决此挑战性问题中的有效性。", "keywords": "非典型有丝分裂分类, 深度学习, 视觉基础模型, 迁移学习, LoRA", "comments": "本文通过引入新的数据集和全面比较多种深度学习及基础模型，特别是在非典型有丝分裂分类这一病理诊断难题上，展示了迁移学习和LoRA微调的巨大潜力。其创新性在于对不同模型和适应策略进行了严格的跨数据集评估，为临床病理诊断的自动化提供了有价值的参考。"}}
{"id": "2506.20910", "title": "Faster Fixed-Point Methods for Multichain MDPs", "authors": ["Matthew Zurek", "Yudong Chen"], "summary": "We study value-iteration (VI) algorithms for solving general (a.k.a.\nmultichain) Markov decision processes (MDPs) under the average-reward\ncriterion, a fundamental but theoretically challenging setting. Beyond the\ndifficulties inherent to all average-reward problems posed by the lack of\ncontractivity and non-uniqueness of solutions to the Bellman operator, in the\nmultichain setting an optimal policy must solve the navigation subproblem of\nsteering towards the best connected component, in addition to optimizing\nlong-run performance within each component. We develop algorithms which better\nsolve this navigational subproblem in order to achieve faster convergence for\nmultichain MDPs, obtaining improved rates of convergence and sharper measures\nof complexity relative to prior work. Many key components of our results are of\npotential independent interest, including novel connections between\naverage-reward and discounted problems, optimal fixed-point methods for\ndiscounted VI which extend to general Banach spaces, new sublinear convergence\nrates for the discounted value error, and refined suboptimality decompositions\nfor multichain MDPs. Overall our results yield faster convergence rates for\ndiscounted and average-reward problems and expand the theoretical foundations\nof VI approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20910v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20910v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多链马尔可夫决策过程的更快不动点方法", "tldr": "该研究为平均奖励准则下的多链马尔可夫决策过程（MDPs）开发了更快的价值迭代算法，通过更好地解决导航子问题，实现了更快的收敛速度和更清晰的复杂性度量。", "motivation": "研究平均奖励准则下的多链马尔可夫决策过程（MDPs）是一个理论上具有挑战性的基本问题。除了所有平均奖励问题固有的非收缩性和贝尔曼算子解的非唯一性之外，多链设置中最佳策略还必须解决导航子问题，即引导至最佳连接组件，同时优化每个组件内的长期性能。", "method": "开发了新的算法，旨在更好地解决多链MDPs中的导航子问题。研究还包括平均奖励问题与折扣问题之间的新颖联系，扩展到一般巴拿赫空间的折扣价值迭代的最优不动点方法，折扣价值误差的新次线性收敛速率，以及多链MDPs的细化次优分解。", "result": "获得了比现有工作更快的收敛速度和更清晰的复杂性度量。整体结果提高了折扣和平均奖励问题的收敛速度。", "conclusion": "本研究的结果为折扣和平均奖励问题带来了更快的收敛速度，并扩展了价值迭代方法的理论基础。", "translation": "我们研究了在平均奖励准则下解决通用（又称多链）马尔可夫决策过程（MDPs）的价值迭代（VI）算法，这是一个基础但理论上具有挑战性的设置。除了所有平均奖励问题因缺乏收缩性和贝尔曼算子解的非唯一性而固有的困难之外，在多链设置中，最优策略除了优化每个组件内的长期性能外，还必须解决导航子问题，即引导至最佳连接组件。我们开发了能够更好地解决此导航子问题以实现多链MDPs更快收敛的算法，相对于先前的工作，获得了改进的收敛速度和更清晰的复杂性度量。我们结果的许多关键组成部分具有潜在的独立兴趣，包括平均奖励问题与折扣问题之间的新颖联系，扩展到一般巴拿赫空间的折扣VI的最优不动点方法，折扣价值误差的新次线性收敛速率，以及多链MDPs的细化次优分解。总的来说，我们的结果为折扣和平均奖励问题带来了更快的收敛速度，并扩展了VI方法的理论基础。", "summary": "本论文研究平均奖励准则下的多链马尔可夫决策过程（MDPs）的价值迭代算法。针对多链设置中导航子问题的挑战，作者开发了新算法，以实现更快的收敛速度和更精确的复杂性度量。研究还揭示了平均奖励与折扣问题的新联系，提出了适用于一般巴拿赫空间的最优不动点方法，并提供了折扣价值误差的次线性收敛率和多链MDPs的细化次优分解，从而提升了VI方法的理论基础。", "keywords": "多链MDPs, 平均奖励, 价值迭代, 不动点方法, 收敛速度", "comments": "本文通过解决多链MDPs中独特的导航子问题，创新性地提升了平均奖励准则下价值迭代算法的收敛速度和理论严谨性。其将平均奖励与折扣问题联系起来，并提出适用于更广泛空间的最优不动点方法，具有重要的理论和潜在实践价值。"}}
{"id": "2506.21446", "title": "Controllable 3D Placement of Objects with Scene-Aware Diffusion Models", "authors": ["Mohamed Omran", "Dimitris Kalatzis", "Jens Petersen", "Amirhossein Habibian", "Auke Wiggers"], "summary": "Image editing approaches have become more powerful and flexible with the\nadvent of powerful text-conditioned generative models. However, placing objects\nin an environment with a precise location and orientation still remains a\nchallenge, as this typically requires carefully crafted inpainting masks or\nprompts. In this work, we show that a carefully designed visual map, combined\nwith coarse object masks, is sufficient for high quality object placement. We\ndesign a conditioning signal that resolves ambiguities, while being flexible\nenough to allow for changing of shapes or object orientations. By building on\nan inpainting model, we leave the background intact by design, in contrast to\nmethods that model objects and background jointly. We demonstrate the\neffectiveness of our method in the automotive setting, where we compare\ndifferent conditioning signals in novel object placement tasks. These tasks are\ndesigned to measure edit quality not only in terms of appearance, but also in\nterms of pose and location accuracy, including cases that require non-trivial\nshape changes. Lastly, we show that fine location control can be combined with\nappearance control to place existing objects in precise locations in a scene.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21446v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21446v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于场景感知扩散模型的可控三维物体放置", "tldr": "该研究提出了一种利用精心设计的视觉地图和粗略物体掩码，通过场景感知扩散模型实现高精度、可控的三维物体放置方法，特别适用于汽车场景，并能保持背景不变。", "motivation": "现有图像编辑方法虽然强大，但精确控制物体在环境中的位置和方向仍然具有挑战性，通常需要精心制作的修复掩码或提示。", "method": "本研究通过设计一种结合视觉地图和粗略物体掩码的条件信号，实现了高质量的物体放置。该条件信号能够解决歧义，并灵活支持形状或方向的改变。方法基于修复模型构建，旨在保持背景完整。", "result": "该方法在汽车场景中展示了其有效性，通过比较不同条件信号在新型物体放置任务中的表现。这些任务不仅评估外观质量，还衡量姿态和位置的准确性，包括需要非平凡形状变化的情况。此外，该方法还能将精细位置控制与外观控制相结合，以在场景中精确放置现有物体。", "conclusion": "该研究成功展示了一种通过精心设计的视觉地图和粗略物体掩码，结合场景感知扩散模型，实现高精度、可控三维物体放置的有效方法，解决了现有图像编辑方法在精确位置和方向控制上的挑战，并能保持背景不变。", "translation": "随着强大的文本条件生成模型的出现，图像编辑方法变得更加强大和灵活。然而，将物体以精确的位置和方向放置在环境中仍然是一个挑战，因为这通常需要精心制作的修复掩码或提示。在这项工作中，我们展示了精心设计的视觉地图与粗略物体掩码相结合，足以实现高质量的物体放置。我们设计了一种条件信号，它能解决歧义，同时足够灵活以允许改变形状或物体方向。通过建立在修复模型的基础上，我们特意保持背景完整，这与联合建模物体和背景的方法形成对比。我们在汽车环境中展示了我们方法的有效性，比较了新型物体放置任务中不同的条件信号。这些任务旨在不仅从外观方面，而且从姿态和位置准确性方面衡量编辑质量，包括需要非平凡形状变化的情况。最后，我们展示了精细位置控制可以与外观控制相结合，以将现有物体精确放置在场景中的特定位置。", "summary": "本研究提出了一种利用场景感知扩散模型进行可控三维物体放置的新方法。通过结合精心设计的视觉地图和粗略物体掩码作为条件信号，该方法能够实现高精度的物体定位和姿态控制，同时保持背景不变。该方法在汽车场景中得到了验证，并能处理复杂的形状变化，展示了在精确位置和外观控制方面的强大能力。", "keywords": "三维物体放置, 扩散模型, 场景感知, 图像编辑, 可控生成", "comments": "这项工作在图像编辑领域，特别是在三维物体精确放置方面取得了重要进展。其创新点在于引入了结合视觉地图和粗略物体掩码的条件信号，有效解决了传统方法中对精确修复掩码或提示的依赖。通过基于修复模型的设计，确保了背景的完整性，这是一个重要的实际优势。该方法在汽车场景中的应用展示了其潜在的工业价值，并且对姿态和位置准确性的强调使其超越了单纯的外观生成。未来的工作可能探索其在更复杂、多样化场景中的泛化能力。"}}
{"id": "2506.21451", "title": "A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario", "authors": ["Cyrus Addy", "Ajay Kumar Gurumadaiah", "Yixiang Gao", "Kwame Awuah-Offei"], "summary": "Underground mining operations face significant safety challenges that make\nemergency response capabilities crucial. While robots have shown promise in\nassisting with search and rescue operations, their effectiveness depends on\nreliable miner detection capabilities. Deep learning algorithms offer potential\nsolutions for automated miner detection, but require comprehensive training\ndatasets, which are currently lacking for underground mining environments. This\npaper presents a novel thermal imaging dataset specifically designed to enable\nthe development and validation of miner detection systems for potential\nemergency applications. We systematically captured thermal imagery of various\nmining activities and scenarios to create a robust foundation for detection\nalgorithms. To establish baseline performance metrics, we evaluated several\nstate-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,\nand RT-DETR on our dataset. While not exhaustive of all possible emergency\nsituations, this dataset serves as a crucial first step toward developing\nreliable thermal-based miner detection systems that could eventually be\ndeployed in real emergency scenarios. This work demonstrates the feasibility of\nusing thermal imaging for miner detection and establishes a foundation for\nfuture research in this critical safety application.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21451v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21451v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "针对多样化场景的地下矿工检测综合数据集", "tldr": "本文提出了一个用于地下矿工热成像检测的综合数据集，以解决现有训练数据缺乏的问题，并为紧急情况下的矿工检测系统开发奠定基础。", "motivation": "地下采矿作业面临严峻的安全挑战，需要可靠的应急响应能力。机器人辅助救援需要可靠的矿工检测能力，而深度学习算法虽有潜力，但缺乏针对地下采矿环境的综合训练数据集。", "method": "本文提出了一个专门为地下矿工检测设计的新型热成像数据集。系统地捕获了各种采矿活动和场景的热图像。为了建立基线性能指标，在该数据集上评估了几种最先进的目标检测算法，包括YOLOv8、YOLOv10、YOLO11和RT-DETR。", "result": "该数据集是开发可靠的基于热成像的矿工检测系统的关键第一步。研究结果证明了使用热成像进行矿工检测的可行性。", "conclusion": "该工作为未来在这一关键安全应用领域的研究奠定了基础，并有望最终部署到真实的紧急情况中。", "translation": "地下采矿作业面临严峻的安全挑战，这使得应急响应能力至关重要。虽然机器人在搜索和救援行动中展现出前景，但其有效性取决于可靠的矿工检测能力。深度学习算法为自动化矿工检测提供了潜在解决方案，但需要全面的训练数据集，而目前地下采矿环境中缺乏此类数据。本文提出了一个新颖的热成像数据集，专门用于开发和验证针对潜在紧急应用的矿工检测系统。我们系统地捕获了各种采矿活动和场景的热图像，为检测算法创建了坚实的基础。为了建立基线性能指标，我们评估了数据集上的几种最先进的目标检测算法，包括YOLOv8、YOLOv10、YOLO11和RT-DETR。尽管该数据集并非涵盖所有可能的紧急情况，但它作为开发可靠的基于热成像的矿工检测系统的关键第一步，这些系统最终可能部署到真实的紧急场景中。这项工作证明了使用热成像进行矿工检测的可行性，并为未来在这一关键安全应用领域的研究奠定了基础。", "summary": "本文针对地下采矿环境中缺乏用于深度学习的矿工检测数据集的问题，提出了一个新颖的、全面的热成像数据集。该数据集通过系统地捕获各种采矿场景的热图像构建，旨在支持紧急情况下矿工检测系统的开发和验证。研究人员使用YOLOv8、YOLOv10、YOLO11和RT-DETR等先进目标检测算法在该数据集上进行了基线性能评估，验证了热成像在矿工检测中的可行性，并为该领域的未来研究奠定了基础。", "keywords": "地下矿工检测, 热成像, 数据集, 目标检测, 采矿安全", "comments": "这项工作通过构建专门针对地下采矿环境的热成像数据集，解决了深度学习在矿工检测应用中面临的关键数据稀缺问题，具有重要的实际意义。其创新之处在于专注于热成像数据，这对于在低光照或无光照的地下环境中进行检测至关重要。该数据集的发布及其基线评估为后续研究提供了宝贵的资源和起点，有望推动地下矿工安全检测技术的发展。"}}
{"id": "2506.20928", "title": "Active Learning for Manifold Gaussian Process Regression", "authors": ["Yuanxing Cheng", "Lulu Kang", "Yiwei Wang", "Chun Liu"], "summary": "This paper introduces an active learning framework for manifold Gaussian\nProcess (GP) regression, combining manifold learning with strategic data\nselection to improve accuracy in high-dimensional spaces. Our method jointly\noptimizes a neural network for dimensionality reduction and a Gaussian process\nregressor in the latent space, supervised by an active learning criterion that\nminimizes global prediction error. Experiments on synthetic data demonstrate\nsuperior performance over randomly sequential learning. The framework\nefficiently handles complex, discontinuous functions while preserving\ncomputational tractability, offering practical value for scientific and\nengineering applications. Future work will focus on scalability and\nuncertainty-aware manifold learning.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20928v1", "categories": ["stat.ML", "cs.LG", "62", "G.3"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20928v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "流形高斯过程回归的主动学习", "tldr": "本文提出了一种结合流形学习和主动学习的框架，用于高维空间中的高斯过程回归，能有效提高精度并处理复杂函数。", "motivation": "在高维空间中提高高斯过程回归的准确性。", "method": "本文提出了一种主动学习框架，结合流形学习和策略性数据选择，共同优化用于降维的神经网络和潜在空间中的高斯过程回归器，并通过最小化全局预测误差的主动学习准则进行监督。", "result": "在合成数据上的实验表明，该方法性能优于随机序列学习。该框架能有效处理复杂、不连续的函数，同时保持计算可行性。", "conclusion": "该框架为科学和工程应用提供了实用价值。未来的工作将侧重于可扩展性和不确定性感知的流形学习。", "translation": "本文介绍了一种用于流形高斯过程（GP）回归的主动学习框架，该框架结合了流形学习和策略性数据选择，以提高高维空间中的准确性。我们的方法共同优化了一个用于降维的神经网络和一个在潜在空间中的高斯过程回归器，并由一个最小化全局预测误差的主动学习准则进行监督。在合成数据上的实验表明，其性能优于随机序列学习。该框架能够高效处理复杂、不连续的函数，同时保持计算可行性，为科学和工程应用提供了实用价值。未来的工作将侧重于可扩展性和不确定性感知的流形学习。", "summary": "本文提出了一种针对流形高斯过程回归的主动学习框架。该框架通过结合流形学习和策略性数据选择，旨在提高高维空间中的预测精度。其方法是联合优化一个用于降维的神经网络和一个在潜在空间中的高斯过程回归器，并由一个最小化全局预测误差的主动学习准则进行监督。实验结果显示，该方法在处理复杂、不连续函数时表现出优于随机序列学习的性能，同时保持了计算效率，具有重要的实际应用价值。", "keywords": "主动学习, 流形高斯过程回归, 维度降低, 策略性数据选择, 高维空间", "comments": "该论文的创新点在于将流形学习与主动学习相结合，解决了高维空间中高斯过程回归的精度问题。通过联合优化降维网络和GP回归器，并在潜在空间中进行主动学习，提高了数据利用效率。其能够处理复杂不连续函数并保持计算可行性的特性，使其在实际工程和科学应用中具有重要意义。未来的可扩展性研究将进一步提升其应用潜力。"}}
{"id": "2506.21452", "title": "Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency", "authors": ["Kaiyu Song", "Hanjiang Lai"], "summary": "Classifier-free guidance (CFG) succeeds in condition diffusion models that\nuse a guidance scale to balance the influence of conditional and unconditional\nterms. A high guidance scale is used to enhance the performance of the\nconditional term. However, the high guidance scale often results in\noversaturation and unrealistic artifacts. In this paper, we introduce a new\nperspective based on low-frequency signals, identifying the accumulation of\nredundant information in these signals as the key factor behind oversaturation\nand unrealistic artifacts. Building on this insight, we propose low-frequency\nimproved classifier-free guidance (LF-CFG) to mitigate these issues.\nSpecifically, we introduce an adaptive threshold-based measurement to pinpoint\nthe locations of redundant information. We determine a reasonable threshold by\nanalyzing the change rate of low-frequency information between prior and\ncurrent steps. We then apply a down-weight strategy to reduce the impact of\nredundant information in the low-frequency signals. Experimental results\ndemonstrate that LF-CFG effectively alleviates oversaturation and unrealistic\nartifacts across various diffusion models, including Stable Diffusion-XL,\nStable Diffusion 2.1, 3.0, 3.5, and SiT-XL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21452v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21452v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过低频重新思考无分类器引导中的过饱和问题", "tldr": "本文提出了一种基于低频信号的新方法LF-CFG，以解决无分类器引导（CFG）中高引导尺度导致的过饱和和不真实伪影问题，通过识别并降低冗余低频信息的影响来改善扩散模型性能。", "motivation": "无分类器引导（CFG）在高引导尺度下常导致过饱和和不真实伪影，限制了其性能。", "method": "本文提出低频改进无分类器引导（LF-CFG）。通过分析先验步和当前步之间低频信息的变化率，引入自适应阈值测量来识别冗余信息的位置，并应用降权策略以减少低频信号中冗余信息的影响。", "result": "实验结果表明，LF-CFG有效缓解了包括Stable Diffusion-XL、Stable Diffusion 2.1、3.0、3.5和SiT-XL在内的各种扩散模型中的过饱和和不真实伪影问题。", "conclusion": "LF-CFG通过解决低频信号中的冗余信息积累问题，成功改善了无分类器引导在高引导尺度下的性能，有效减轻了过饱和和不真实伪影。", "translation": "无分类器引导（CFG）成功应用于条件扩散模型，它使用引导尺度来平衡条件项和无条件项的影响。高引导尺度用于增强条件项的性能。然而，高引导尺度常常导致过饱和和不真实的伪影。在本文中，我们引入了一种基于低频信号的新视角，将这些信号中冗余信息的积累确定为导致过饱和和不真实伪影的关键因素。基于这一见解，我们提出了低频改进无分类器引导（LF-CFG）来缓解这些问题。具体来说，我们引入了一种基于自适应阈值的测量方法来精确定位冗余信息的位置。我们通过分析先验步和当前步之间低频信息的变化率来确定合理的阈值。然后，我们应用降权策略来减少低频信号中冗余信息的影响。实验结果表明，LF-CFG有效缓解了包括Stable Diffusion-XL、Stable Diffusion 2.1、3.0、3.5和SiT-XL在内的各种扩散模型中的过饱和和不真实伪影。", "summary": "本文针对无分类器引导（CFG）在高引导尺度下产生的过饱和及不真实伪影问题，提出了一种名为低频改进无分类器引导（LF-CFG）的新方法。研究发现，低频信号中冗余信息的积累是导致这些问题的关键。LF-CFG通过引入自适应阈值测量来识别冗余信息，并采用降权策略来降低其影响。实验证明，LF-CFG在多种扩散模型上有效改善了图像质量，缓解了过饱和和伪影。", "keywords": "无分类器引导, 低频, 过饱和, 扩散模型, LF-CFG", "comments": "该论文创新性地从低频信号视角重新审视了无分类器引导中的过饱和问题，并提出了LF-CFG这一有效解决方案。其核心贡献在于识别了低频冗余信息积累作为问题根源，并设计了自适应阈值和降权策略来缓解。这为提升扩散模型在高引导尺度下的生成质量提供了新的思路。"}}
{"id": "2506.20930", "title": "Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "Ya-Chuan Chen"], "summary": "We propose a hybrid quantum-classical reinforcement learning framework for\nsector rotation in the Taiwan stock market. Our system employs Proximal Policy\nOptimization (PPO) as the backbone algorithm and integrates both classical\narchitectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,\nQASA) as policy and value networks. An automated feature engineering pipeline\nextracts financial indicators from capital share data to ensure consistent\nmodel input across all configurations. Empirical backtesting reveals a key\nfinding: although quantum-enhanced models consistently achieve higher training\nrewards, they underperform classical models in real-world investment metrics\nsuch as cumulative return and Sharpe ratio. This discrepancy highlights a core\nchallenge in applying reinforcement learning to financial domains -- namely,\nthe mismatch between proxy reward signals and true investment objectives. Our\nanalysis suggests that current reward designs may incentivize overfitting to\nshort-term volatility rather than optimizing risk-adjusted returns. This issue\nis compounded by the inherent expressiveness and optimization instability of\nquantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We\ndiscuss the implications of this reward-performance gap and propose directions\nfor future improvement, including reward shaping, model regularization, and\nvalidation-based early stopping. Our work offers a reproducible benchmark and\ncritical insights into the practical challenges of deploying quantum\nreinforcement learning in real-world finance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20930v1", "categories": ["quant-ph", "cs.LG", "q-fin.CP"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.20930v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "量子强化学习交易代理在台湾股市板块轮动中的应用", "tldr": "该研究提出了一种混合量子-经典强化学习框架用于台湾股市板块轮动，发现量子增强模型在训练奖励上表现更好，但在实际投资指标（如累积回报和夏普比率）上不如经典模型，这揭示了强化学习在金融领域应用中代理奖励与真实投资目标不匹配的核心挑战。", "motivation": "解决在台湾股市中进行板块轮动的问题，并探索混合量子-经典强化学习框架的应用潜力。同时，也旨在揭示强化学习在金融领域应用中面临的挑战，特别是代理奖励与真实投资目标之间的不匹配问题。", "method": "提出一个混合量子-经典强化学习框架，以PPO为核心算法，并结合经典架构（LSTM, Transformer）和量子增强模型（QNN, QRWKV, QASA）作为策略和价值网络。通过自动化特征工程提取金融指标。", "result": "经验回测显示，量子增强模型在训练奖励上持续获得更高分数，但在累积回报和夏普比率等实际投资指标上表现不如经典模型。这种差异表明代理奖励信号与真实投资目标之间存在不匹配。", "conclusion": "强化学习在金融领域应用中，当前奖励设计可能导致模型过拟合短期波动而非优化风险调整回报。NISQ约束下的量子电路固有的表达能力和优化不稳定性加剧了这一问题。研究强调了奖励-性能差距，并提出了未来改进方向，包括奖励塑造、模型正则化和基于验证的提前停止。", "translation": "我们提出了一种用于台湾股票市场板块轮动的混合量子-经典强化学习框架。我们的系统采用近端策略优化（PPO）作为核心算法，并集成了经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）作为策略和价值网络。一个自动化特征工程流程从资本份额数据中提取金融指标，以确保所有配置的模型输入一致。经验回测揭示了一个关键发现：尽管量子增强模型持续获得更高的训练奖励，但在累积回报和夏普比率等实际投资指标上，它们的表现不如经典模型。这种差异突出了将强化学习应用于金融领域的一个核心挑战——即代理奖励信号与真实投资目标之间的不匹配。我们的分析表明，当前的奖励设计可能激励模型过拟合短期波动，而不是优化风险调整后的回报。在噪声中等规模量子（NISQ）约束下，量子电路固有的表达能力和优化不稳定性加剧了这个问题。我们讨论了这种奖励-性能差距的影响，并提出了未来改进的方向，包括奖励塑造、模型正则化和基于验证的提前停止。我们的工作提供了一个可重现的基准，并为在实际金融中部署量子强化学习的实际挑战提供了关键见解。", "summary": "本研究提出了一个用于台湾股市板块轮动的混合量子-经典强化学习框架。该框架以PPO为基础，结合了经典（LSTM, Transformer）和量子增强（QNN, QRWKV, QASA）模型。回测结果显示，量子模型虽然训练奖励高，但在实际投资表现上逊于经典模型，揭示了金融RL中代理奖励与真实目标不匹配的问题。论文讨论了这种奖励-性能差距的含义，并提出了解决策略。", "keywords": "量子强化学习, 板块轮动, 台湾股市, 代理奖励, 金融科技", "comments": "该论文创新性地将量子强化学习应用于金融板块轮动，并提供了实证基准。其重要贡献在于揭示了量子增强模型在训练奖励与实际投资表现之间存在的“奖励-性能差距”，指出了当前强化学习在金融领域应用中，奖励设计可能导致过拟合短期波动而非优化风险调整回报的核心局限性。论文还讨论了NISQ环境下量子电路的挑战，并提出了未来改进方向，对该领域的研究具有重要的指导意义。"}}
{"id": "2506.21469", "title": "Evaluation of Traffic Signals for Daily Traffic Pattern", "authors": ["Mohammad Shokrolah Shirazi", "Hung-Fu Chang"], "summary": "The turning movement count data is crucial for traffic signal design,\nintersection geometry planning, traffic flow, and congestion analysis. This\nwork proposes three methods called dynamic, static, and hybrid configuration\nfor TMC-based traffic signals. A vision-based tracking system is developed to\nestimate the TMC of six intersections in Las Vegas using traffic cameras. The\nintersection design, route (e.g. vehicle movement directions), and signal\nconfiguration files with compatible formats are synthesized and imported into\nSimulation of Urban MObility for signal evaluation with realistic data. The\ninitial experimental results based on estimated waiting times indicate that the\ncycle time of 90 and 120 seconds works best for all intersections. In addition,\nfour intersections show better performance for dynamic signal timing\nconfiguration, and the other two with lower performance have a lower ratio of\ntotal vehicle count to total lanes of the intersection leg. Since daily traffic\nflow often exhibits a bimodal pattern, we propose a hybrid signal method that\nswitches between dynamic and static methods, adapting to peak and off-peak\ntraffic conditions for improved flow management. So, a built-in traffic\ngenerator module creates vehicle routes for 4 hours, including peak hours, and\na signal design module produces signal schedule cycles according to static,\ndynamic, and hybrid methods. Vehicle count distributions are weighted\ndifferently for each zone (i.e., West, North, East, South) to generate diverse\ntraffic patterns. The extended experimental results for 6 intersections with 4\nhours of simulation time imply that zone-based traffic pattern distributions\naffect signal design selection. Although the static method works great for\nevenly zone-based traffic distribution, the hybrid method works well for highly\nweighted traffic at intersection pairs of the West-East and North-South zones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21469v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21469v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "日常交通模式下交通信号灯评估", "tldr": "本研究评估了动态、静态和混合交通信号控制方法，利用真实交通数据和模拟，发现混合方法能更好地适应日常交通模式变化，有效管理高峰和非高峰时段的交通流。", "motivation": "转弯流量计数数据对交通信号设计和交通流分析至关重要。鉴于日常交通流常呈现双峰模式，本研究旨在提出并评估一种能适应高峰和非高峰交通状况的混合信号方法，以改进流量管理。", "method": "1. 提出动态、静态和混合三种基于TMC的交通信号配置方法。2. 开发基于视觉的跟踪系统，利用交通摄像头估算拉斯维加斯六个交叉口的TMC数据。3. 将交叉口设计、路线和信号配置文件导入SUMO进行信号评估。4. 提出混合信号方法，在动态和静态方法之间切换，以适应高峰和非高峰交通。5. 内置交通生成器模块创建4小时（含高峰时段）车辆路线，信号设计模块生成静态、动态和混合方法的信号调度周期。6. 对不同区域（西、北、东、南）的车辆计数分布进行加权，生成多样化的交通模式。", "result": "1. 初步实验结果表明，90秒和120秒的周期时间对所有交叉口效果最佳。2. 四个交叉口在动态信号配时下表现更好，另两个表现较差的交叉口其总车辆数与总车道数比率较低。3. 扩展实验结果显示，基于区域的交通模式分布影响信号设计选择。4. 静态方法适用于均匀的区域交通分布。5. 混合方法对西-东和北-南区域交叉口对的高权重交通效果良好。", "conclusion": "交通信号设计的选择受区域交通模式分布的影响，其中混合方法对于日常交通流中常见的不平衡交通模式表现良好，能够有效适应高峰和非高峰交通状况。", "translation": "转弯流量计数数据对于交通信号设计、交叉口几何规划、交通流和拥堵分析至关重要。这项工作提出了三种基于转弯流量计数（TMC）的交通信号方法：动态配置、静态配置和混合配置。开发了一个基于视觉的跟踪系统，利用交通摄像头估计拉斯维加斯六个交叉口的TMC。将交叉口设计、路线（例如车辆移动方向）和兼容格式的信号配置文件合成并导入到城市交通模拟器（Simulation of Urban MObility, SUMO）中，以使用真实数据进行信号评估。基于估计等待时间的初步实验结果表明，90秒和120秒的周期时间对所有交叉口效果最佳。此外，四个交叉口在动态信号配时配置下表现更好，而另外两个表现较差的交叉口，其总车辆数与交叉口支路总车道数的比率较低。由于日常交通流通常呈现双峰模式，我们提出了一种混合信号方法，在动态和静态方法之间切换，以适应高峰和非高峰交通状况，从而改进流量管理。因此，一个内置的交通生成器模块创建了4小时的车辆路线（包括高峰时段），一个信号设计模块根据静态、动态和混合方法生成信号调度周期。每个区域（即西、北、东、南）的车辆计数分布权重不同，以生成多样化的交通模式。针对6个交叉口进行4小时模拟时间的扩展实验结果表明，基于区域的交通模式分布会影响信号设计选择。尽管静态方法对于均匀的基于区域的交通分布效果很好，但混合方法对于西-东和北-南区域交叉口对的高权重交通效果良好。", "summary": "本论文评估了动态、静态和混合三种交通信号控制方法。研究利用基于视觉的跟踪系统获取拉斯维加斯交叉口的真实转弯流量计数（TMC）数据，并在SUMO中进行仿真，以评估这些方法在各种日常交通模式（包括高峰和非高峰）下的性能。初步结果确定了最佳周期时间，扩展模拟表明，虽然静态方法适用于均匀的交通分布，但本文提出的混合方法，通过在动态和静态控制之间切换以适应双峰交通模式，在不平衡的区域性交通流中表现出色。", "keywords": "交通信号, 转弯流量计数, SUMO, 混合方法, 交通模式", "comments": "本文通过将真实世界数据采集（基于视觉的TMC）与仿真（SUMO）相结合，并提出自适应的混合控制策略，为交通信号优化提供了一种实用方法。其创新之处在于混合方法能够响应日常交通的双峰特性，提供比纯静态或动态系统更灵活的解决方案。其重要性在于通过数据驱动和自适应的信号管理来解决现实世界的交通拥堵问题。一个潜在的局限性可能是研究结果的普遍性，因为该研究基于拉斯维加斯的特定交叉口，并且混合切换机制的实时实施可能存在复杂性。"}}
{"id": "2506.20933", "title": "Lower Bounds on the Size of Markov Equivalence Classes", "authors": ["Erik Jahn", "Frederick Eberhardt", "Leonard J. Schulman"], "summary": "Causal discovery algorithms typically recover causal graphs only up to their\nMarkov equivalence classes unless additional parametric assumptions are made.\nThe sizes of these equivalence classes reflect the limits of what can be\nlearned about the underlying causal graph from purely observational data. Under\nthe assumptions of acyclicity, causal sufficiency, and a uniform model prior,\nMarkov equivalence classes are known to be small on average. In this paper, we\nshow that this is no longer the case when any of these assumptions is relaxed.\nSpecifically, we prove exponentially large lower bounds for the expected size\nof Markov equivalence classes in three settings: sparse random directed acyclic\ngraphs, uniformly random acyclic directed mixed graphs, and uniformly random\ndirected cyclic graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20933v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20933v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "马尔可夫等价类大小的下界", "tldr": "当放松无环性、因果充分性或统一模型先验假设时，马尔可夫等价类的预期大小不再平均较小，而是呈指数级增长。", "motivation": "了解因果发现算法在纯观测数据下能学到什么，以及马尔可夫等价类的大小如何反映这些限制。先前研究表明在特定假设下等价类平均较小，但本文旨在探索放松这些假设后的情况。", "method": "通过证明在三种不同设置（稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图）下，马尔可夫等价类的预期大小呈指数级增长的下界。", "result": "证明了当放松无环性、因果充分性或统一模型先验假设中的任何一个时，马尔可夫等价类的预期大小不再平均较小，而是呈指数级大。", "conclusion": "放松因果发现的常见假设（如无环性、因果充分性）会导致马尔可夫等价类的大小呈指数级增长，这表明在这些情况下，从纯观测数据中学习底层因果图的限制会大大增加。", "translation": "因果发现算法通常只能恢复因果图到其马尔可夫等价类，除非做出额外的参数假设。这些等价类的大小反映了纯观测数据能从底层因果图中学习到的极限。在无环性、因果充分性和统一模型先验的假设下，已知马尔可夫等价类平均较小。在本文中，我们表明当放松这些假设中的任何一个时，情况不再如此。具体来说，我们在三种设置下证明了马尔可夫等价类预期大小的指数级大下界：稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图。", "summary": "本文研究了在放松因果发现的常见假设（如无环性、因果充分性和统一模型先验）时，马尔可夫等价类大小的变化。传统观点认为在这些假设下等价类平均较小，但作者证明了当任何一个假设被放松时，等价类的预期大小会呈指数级增长。具体地，研究在稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图三种情境下推导出了指数级下界，揭示了在更一般或复杂的因果模型中，仅凭观测数据推断因果结构的难度显著增加。", "keywords": "马尔可夫等价类, 因果发现, 下界, 有向无环图, 有向循环图", "comments": "该论文的创新之处在于挑战了马尔可夫等价类在一般情况下“平均较小”的传统认知。通过证明在放松关键假设（如无环性）时，等价类大小会呈指数级增长，它揭示了因果发现中基于纯观测数据的固有局限性在更广泛或复杂的模型设定下变得更加严峻。这对于理解因果推断的理论边界和指导实际算法设计具有重要意义，尤其是在处理包含循环或隐变量的复杂系统时。"}}
{"id": "2506.20935", "title": "Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics", "authors": ["Hsin-Hsiung Huang", "Hayden Hampton"], "summary": "Forecasting geopolitical conflict from data sources like the Global Database\nof Events, Language, and Tone (GDELT) is a critical challenge for national\nsecurity. The inherent sparsity, burstiness, and overdispersion of such data\ncause standard deep learning models, including the Temporal Fusion Transformer\n(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,\na hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)\ncompetition by overcoming these limitations. Designed to bridge this gap, our\nmodel employs a two-stage process: first, a TFT captures complex temporal\ndynamics to generate multi-quantile forecasts. These quantiles then serve as\ninformed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),\nwhich performs principled spatiotemporal smoothing and uncertainty\nquantification. In a case study forecasting conflict dynamics in the Middle\nEast and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,\nshowing a superior ability to predict the timing and magnitude of bursty event\nperiods, particularly at long-range horizons. This work offers a robust\nframework for generating more reliable and actionable intelligence from\nchallenging event data, with all code and workflows made publicly available to\nensure reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20935v1", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO", "37M10, 62M10, 62P25, 65Y20"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20935v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用稀疏时间融合转换器和高斯过程混合模型预测地缘政治事件：中东和美国冲突动态的案例研究", "tldr": "本文提出STFT-VNNGP，一种结合TFT和VNNGP的混合模型，通过准确预测稀疏、突发的地缘政治冲突（尤其是在长期预测方面）赢得了一项竞赛。", "motivation": "从GDELT等数据源预测地缘政治冲突对国家安全至关重要，但此类数据固有的稀疏性、突发性和过度分散性导致标准深度学习模型（如TFT）产生不可靠的长期预测。", "method": "本文引入STFT-VNNGP混合架构，采用两阶段过程：首先，TFT捕获复杂时间动态以生成多分位数预测；然后，这些分位数作为变分最近邻高斯过程（VNNGP）的输入，进行有原则的时空平滑和不确定性量化。", "result": "在中东和美国冲突动态的案例研究中，STFT-VNNGP持续优于独立的TFT模型，在预测突发事件的时间和幅度方面表现出卓越的能力，尤其是在长期预测方面。", "conclusion": "该工作提供了一个强大的框架，可以从具有挑战性的事件数据中生成更可靠和可操作的情报，并且所有代码和工作流程都已公开以确保可重现性。", "translation": "从全球事件、语言和语调数据库（GDELT）等数据源预测地缘政治冲突是国家安全面临的一项严峻挑战。此类数据固有的稀疏性、突发性和过度分散性导致包括时间融合转换器（TFT）在内的标准深度学习模型产生不可靠的长期预测。我们引入了STFT-VNNGP，这是一种混合架构，通过克服这些限制赢得了2023年威胁检测算法（ATD）竞赛。为了弥补这一差距，我们的模型采用两阶段过程：首先，TFT捕获复杂的时间动态以生成多分位数预测。然后，这些分位数作为变分最近邻高斯过程（VNNGP）的输入，后者执行有原则的时空平滑和不确定性量化。在中东和美国冲突动态预测的案例研究中，STFT-VNNGP持续优于独立的TFT模型，在预测突发事件的时间和幅度方面表现出卓越的能力，尤其是在长期预测方面。这项工作为从具有挑战性的事件数据中生成更可靠和可操作的情报提供了一个强大的框架，所有代码和工作流程都已公开，以确保可重现性。", "summary": "本文旨在解决稀疏、突发地缘政治冲突数据预测的挑战，这类数据常导致TFT等标准深度学习模型在长期预测中表现不佳。作者提出了STFT-VNNGP混合模型，该模型赢得了2023年ATD竞赛。它采用两阶段方法：TFT生成多分位数预测，然后这些预测作为VNNGP的输入，用于时空平滑和不确定性量化。在中东和美国冲突动态的案例研究中，STFT-VNNGP持续优于独立的TFT模型，尤其在长期突发事件预测方面表现出色，为可操作情报提供了强大框架。", "keywords": "地缘政治预测, 时间融合转换器, 高斯过程, 混合模型, 冲突动态", "comments": "该论文的创新之处在于其混合架构（TFT与VNNGP结合），专门设计用于处理稀疏、突发和过度分散的数据。其两阶段处理过程有效结合了两种模型的优势（TFT用于时间动态捕获，VNNGP用于平滑和不确定性量化）。该研究解决了国家安全领域的一个关键挑战，并在2023年ATD竞赛中获胜，证明了其有效性。代码和工作流程的公开性增强了研究的可复现性和潜在影响力。"}}
{"id": "2506.21476", "title": "Global and Local Entailment Learning for Natural World Imagery", "authors": ["Srikumar Sastry", "Aayush Dhakal", "Eric Xing", "Subash Khanal", "Nathan Jacobs"], "summary": "Learning the hierarchical structure of data in vision-language models is a\nsignificant challenge. Previous works have attempted to address this challenge\nby employing entailment learning. However, these approaches fail to model the\ntransitive nature of entailment explicitly, which establishes the relationship\nbetween order and semantics within a representation space. In this work, we\nintroduce Radial Cross-Modal Embeddings (RCME), a framework that enables the\nexplicit modeling of transitivity-enforced entailment. Our proposed framework\noptimizes for the partial order of concepts within vision-language models. By\nleveraging our framework, we develop a hierarchical vision-language foundation\nmodel capable of representing the hierarchy in the Tree of Life. Our\nexperiments on hierarchical species classification and hierarchical retrieval\ntasks demonstrate the enhanced performance of our models compared to the\nexisting state-of-the-art models. Our code and models are open-sourced at\nhttps://vishu26.github.io/RCME/index.html.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21476v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21476v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "自然世界图像的全局和局部蕴涵学习", "tldr": "本文提出了径向跨模态嵌入（RCME）框架，用于显式建模蕴涵的传递性，并在视觉-语言模型中优化概念的部分顺序，从而在分层物种分类和分层检索任务中实现了优于现有SOTA模型的性能。", "motivation": "在视觉-语言模型中学习数据的分层结构是一个重大挑战。现有方法未能明确建模蕴涵的传递性，而传递性建立了表示空间中顺序和语义之间的关系。", "method": "本文引入了径向跨模态嵌入（RCME）框架，该框架能够显式建模传递性强制的蕴涵。该框架优化了视觉-语言模型中概念的部分顺序。通过利用该框架，开发了一个能够表示生命之树中层次结构的分层视觉-语言基础模型。", "result": "在分层物种分类和分层检索任务上的实验表明，与现有最先进的模型相比，我们的模型性能有所增强。", "conclusion": "RCME框架通过显式建模蕴涵的传递性，有效提升了视觉-语言模型中分层结构的表示能力，并在相关任务中取得了显著的性能提升。", "translation": "在视觉-语言模型中学习数据的分层结构是一个重大挑战。以往的工作试图通过采用蕴涵学习来解决这一挑战。然而，这些方法未能明确建模蕴涵的传递性，而蕴涵的传递性建立了表示空间中顺序和语义之间的关系。在这项工作中，我们引入了径向跨模态嵌入（RCME），这是一个能够显式建模传递性强制蕴涵的框架。我们提出的框架优化了视觉-语言模型中概念的部分顺序。通过利用我们的框架，我们开发了一个能够表示生命之树中层次结构的分层视觉-语言基础模型。我们在分层物种分类和分层检索任务上的实验表明，与现有最先进的模型相比，我们的模型性能有所增强。我们的代码和模型已在 https://vishu26.github.io/RCME/index.html 开源。", "summary": "本文针对视觉-语言模型中学习数据分层结构的挑战，提出了径向跨模态嵌入（RCME）框架。该框架创新性地显式建模了蕴涵的传递性，并优化了概念的部分顺序，以更好地表示层次结构。RCME被应用于构建一个能够表示生命之树层次结构的分层视觉-语言基础模型。实验结果表明，RCME在分层物种分类和分层检索任务中均优于现有最先进的模型。", "keywords": "蕴涵学习, 视觉-语言模型, 分层结构, 传递性, RCME", "comments": "本文的创新点在于提出了RCME框架，显式地解决了先前蕴涵学习方法未能建模蕴涵传递性的问题，这对于在视觉-语言模型中准确表示数据层次结构至关重要。通过优化概念的部分顺序，RCME提供了一种更鲁棒和语义一致的方式来学习和表示分层数据，对于生物分类等需要复杂层次关系的领域具有重要意义。"}}
{"id": "2506.21484", "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning", "authors": ["Tajamul Ashraf", "Janibul Bashir"], "summary": "We focus on the source-free domain adaptive object detection (SF-DAOD)\nproblem when source data is unavailable during adaptation and the model must\nadapt to an unlabeled target domain. The majority of approaches for the problem\nemploy a self-supervised approach using a student-teacher (ST) framework where\npseudo-labels are generated via a source-pretrained model for further\nfine-tuning. We observe that the performance of a student model often degrades\ndrastically, due to the collapse of the teacher model, primarily caused by high\nnoise in pseudo-labels, resulting from domain bias, discrepancies, and a\nsignificant domain shift across domains. To obtain reliable pseudo-labels, we\npropose a Target-based Iterative Query-Token Adversarial Network (TITAN), which\nseparates the target images into two subsets: those similar to the source\n(easy) and those dissimilar (hard). We propose a strategy to estimate variance\nto partition the target domain. This approach leverages the insight that higher\ndetection variances correspond to higher recall and greater similarity to the\nsource domain. Also, we incorporate query-token-based adversarial modules into\na student-teacher baseline framework to reduce the domain gaps between two\nfeature representations. Experiments conducted on four natural imaging datasets\nand two challenging medical datasets have substantiated the superior\nperformance of TITAN compared to existing state-of-the-art (SOTA)\nmethodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7\npercent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,\nrespectively.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21484v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21484v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TITAN：基于查询令牌的域自适应对抗学习", "tldr": "TITAN提出了一种新的SF-DAOD方法，通过分离目标域的难易样本并结合查询令牌对抗模块，解决了伪标签噪声问题，显著提升了目标检测性能。", "motivation": "现有源数据不可用的域自适应目标检测（SF-DAOD）方法中，学生-教师框架生成的伪标签噪声过高（由于域偏差、差异和显著的域偏移），导致教师模型崩溃和学生模型性能急剧下降。", "method": "本文提出了一种名为Target-based Iterative Query-Token Adversarial Network (TITAN)的方法。该方法将目标图像分为与源域相似的（容易）和不相似的（困难）两个子集，并通过估计方差来划分目标域，利用高检测方差对应高召回率和与源域更高相似性的洞察。此外，TITAN将基于查询令牌的对抗模块整合到学生-教师基线框架中，以减少两种特征表示之间的域差距。", "result": "在四个自然图像数据集和两个具有挑战性的医学数据集上进行的实验表明，TITAN的性能优于现有最先进（SOTA）方法。具体地，在C2F、C2B、S2C和K2C基准上，mAP比当前SOTA分别提高了+22.7%、+22.2%、+21.1%和+3.7个百分点。", "conclusion": "TITAN通过有效处理伪标签噪声和减少域差距，显著提升了源数据不可用时的域自适应目标检测性能，超越了现有最先进方法。", "translation": "我们关注源数据在自适应期间不可用且模型必须适应未标记目标域的无源域自适应目标检测（SF-DAOD）问题。解决该问题的大多数方法都采用学生-教师（ST）框架下的自监督方法，其中通过源预训练模型生成伪标签以进行进一步微调。我们观察到，学生模型的性能通常会急剧下降，这是由于教师模型崩溃造成的，主要原因是伪标签中存在高噪声，而高噪声又源于域偏差、差异以及跨域的显著域偏移。为了获得可靠的伪标签，我们提出了一个基于目标的迭代查询令牌对抗网络（TITAN），它将目标图像分为两个子集：与源域相似的（容易）和不相似的（困难）。我们提出了一种估计方差的策略来划分目标域。这种方法利用了以下洞察：较高的检测方差对应较高的召回率和与源域的更大相似性。此外，我们将基于查询令牌的对抗模块整合到学生-教师基线框架中，以减少两种特征表示之间的域差距。在四个自然图像数据集和两个具有挑战性的医学数据集上进行的实验证明，TITAN的性能优于现有最先进（SOTA）方法。我们报告在C2F、C2B、S2C和K2C基准上，mAP比当前SOTA分别提高了+22.7、+22.2、+21.1和+3.7个百分点。", "summary": "本文提出了TITAN（Target-based Iterative Query-Token Adversarial Network），旨在解决源数据不可用时的域自适应目标检测（SF-DAOD）问题。针对现有学生-教师框架中伪标签噪声导致性能下降的问题，TITAN通过将目标图像划分为易学和难学子集（基于检测方差），并引入基于查询令牌的对抗模块来减少域差距，从而生成更可靠的伪标签。实验证明，TITAN在多个数据集上显著优于现有SOTA方法。", "keywords": "源自适应目标检测, 伪标签, 对抗学习, 查询令牌, 域适应", "comments": "TITAN的创新点在于其独特的两阶段目标域划分策略（基于方差估计的易难样本分离）和引入查询令牌对抗模块来弥补域差距。这有效解决了SF-DAOD中伪标签噪声和教师模型崩溃的核心问题，取得了显著的性能提升，尤其是在跨域检测方面表现出色。"}}
{"id": "2506.21486", "title": "Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection", "authors": ["Tobias J. Riedlinger", "Kira Maag", "Hanno Gottschalk"], "summary": "Deep neural networks have set the state-of-the-art in computer vision tasks\nsuch as bounding box detection and semantic segmentation. Object detectors and\nsegmentation models assign confidence scores to predictions, reflecting the\nmodel's uncertainty in object detection or pixel-wise classification. However,\nthese confidence estimates are often miscalibrated, as their architectures and\nloss functions are tailored to task performance rather than probabilistic\nfoundation. Even with well calibrated predictions, object detectors fail to\nquantify uncertainty outside detected bounding boxes, i.e., the model does not\nmake a probability assessment of whether an area without detected objects is\ntruly free of obstacles. This poses a safety risk in applications such as\nautomated driving, where uncertainty in empty areas remains unexplored. In this\nwork, we propose an object detection model grounded in spatial statistics.\nBounding box data matches realizations of a marked point process, commonly used\nto describe the probabilistic occurrence of spatial point events identified as\nbounding box centers, where marks are used to describe the spatial extension of\nbounding boxes and classes. Our statistical framework enables a\nlikelihood-based training and provides well-defined confidence estimates for\nwhether a region is drivable, i.e., free of objects. We demonstrate the\neffectiveness of our method through calibration assessments and evaluation of\nperformance.", "comment": "15 pages, 4 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21486v1", "categories": ["cs.CV", "cs.LG", "math.PR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21486v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "迈向可靠的空旷空间检测：用于目标检测的条件标记点过程", "tldr": "本文提出了一种基于空间统计学和条件标记点过程的目标检测模型，旨在解决现有深度学习目标检测器在未检测区域（空旷空间）的不确定性量化问题，并为区域是否可驾驶提供明确的置信度估计。", "motivation": "现有深度神经网络在目标检测中表现出色，但其置信度估计常有偏差，且无法量化未检测区域（空旷空间）的不确定性。这在自动驾驶等应用中存在安全隐患，因为模型未能评估无障碍区域是否真正安全。", "method": "本文提出了一种基于空间统计学的方法。将边界框数据视为标记点过程的实现，其中边界框中心是空间点事件，标记用于描述边界框的空间扩展和类别。该统计框架支持基于似然的训练，并为区域是否可驾驶（即无物体）提供明确定义的置信度估计。", "result": "通过校准评估和性能评估，证明了该方法的有效性。", "conclusion": "该研究提出了一种基于空间统计学的目标检测模型，它能为未检测区域提供明确定义的置信度估计，从而提高了空旷空间检测的可靠性，对于自动驾驶等安全关键应用具有重要意义。", "translation": "深度神经网络在边界框检测和语义分割等计算机视觉任务中取得了最先进的成果。目标检测器和分割模型为预测分配置信度分数，反映了模型在目标检测或像素级分类中的不确定性。然而，这些置信度估计通常是未校准的，因为其架构和损失函数是为任务性能而非概率基础量身定制的。即使预测经过良好校准，目标检测器也无法量化检测到的边界框之外的不确定性，即模型未对没有检测到物体的区域是否真正没有障碍物进行概率评估。这在自动驾驶等应用中带来了安全风险，因为空旷区域的不确定性尚未被探索。在这项工作中，我们提出了一种基于空间统计学原理的目标检测模型。边界框数据与标记点过程的实现相匹配，该过程常用于描述被识别为边界框中心的空间点事件的概率发生，其中标记用于描述边界框和类别的空间扩展。我们的统计框架支持基于似然的训练，并为区域是否可驾驶（即无物体）提供明确定义的置信度估计。我们通过校准评估和性能评估展示了我们方法的有效性。", "summary": "当前深度学习目标检测器在空旷区域的不确定性量化方面存在局限性，可能带来安全风险。本文提出了一种基于空间统计学的目标检测模型，利用条件标记点过程来建模边界框数据。该方法能够提供明确定义的置信度估计，判断一个区域是否无障碍物，并通过校准和性能评估验证了其有效性，旨在提高空旷空间检测的可靠性。", "keywords": "空旷空间检测, 条件标记点过程, 目标检测, 不确定性量化, 空间统计学", "comments": "该论文的创新点在于将空间统计学中的条件标记点过程引入到目标检测领域，以解决现有深度学习模型在量化未检测区域不确定性方面的不足。这对于自动驾驶等安全关键应用具有重要意义，因为它能提供对“空旷”区域的概率评估，从而提高系统的可靠性和安全性。这种将传统统计学方法与现代深度学习相结合的思路值得关注。"}}
{"id": "2506.21509", "title": "Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration", "authors": ["Jiahe Chen", "Jiaying He", "Qian Shao", "Qiyuan Chen", "Jiahe Ying", "Hongxia Xu", "Jintai Chen", "Jianwei Zheng", "Jian Wu"], "summary": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21509v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21509v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过动态Logits校准缓解大型视觉-语言模型的幻觉", "tldr": "本文提出了一种名为动态Logits校准（DLC）的新型免训练解码框架，用于在推理时动态地将文本生成与视觉证据对齐，有效缓解大型视觉-语言模型的幻觉。", "motivation": "大型视觉-语言模型（LVLM）在多模态理解方面取得了显著进展，但经常受到幻觉的困扰，即生成与视觉输入矛盾的文本。现有的免训练解码策略存在局限性，包括使用不适应生成过程中语义漂移的静态约束、需要多次前向传播导致的低效率以及由于过于僵硬的干预规则导致的细节退化。", "method": "本文引入了动态Logits校准（DLC），一种新型的免训练解码框架，旨在推理时动态地将文本生成与视觉证据对齐。在解码阶段，DLC逐步利用CLIP评估输入图像与生成的文本序列之间的语义对齐。然后，根据动态更新的上下文基线评估候选tokens的相对视觉优势（RVA），自适应地调整输出logits以偏向视觉上接地气的tokens。此外，一个由实时上下文对齐分数提供信息的自适应加权机制，在平衡视觉指导的同时，确保文本输出的整体质量。", "result": "在各种基准和不同的LVLM架构（如LLaVA、InstructBLIP和MiniGPT-4）上进行的广泛实验表明，DLC显著减少了幻觉，优于现有方法，并通过避免多次前向传播保持了高推理效率。", "conclusion": "本文提出了一种有效且高效的解码时解决方案来缓解幻觉，从而增强了LVLM在更多实践中的可靠性。", "translation": "大型视觉-语言模型（LVLM）在多模态理解方面取得了显著进展，但它们经常受到幻觉的困扰——生成与视觉输入矛盾的文本。现有的免训练解码策略表现出关键局限性，包括使用不适应生成过程中语义漂移的静态约束、需要多次前向传播导致的低效率以及由于过于僵硬的干预规则导致的细节退化。为了克服这些挑战，本文引入了动态Logits校准（DLC），一种新型的免训练解码框架，旨在推理时动态地将文本生成与视觉证据对齐。在解码阶段，DLC逐步利用CLIP评估输入图像与生成的文本序列之间的语义对齐。然后，根据动态更新的上下文基线评估候选tokens的相对视觉优势（RVA），自适应地调整输出logits以偏向视觉上接地气的tokens。此外，一个由实时上下文对齐分数提供信息的自适应加权机制，在平衡视觉指导的同时，确保文本输出的整体质量。在各种基准和不同的LVLM架构（如LLaVA、InstructBLIP和MiniGPT-4）上进行的广泛实验表明，DLC显著减少了幻觉，优于现有方法，并通过避免多次前向传播保持了高推理效率。总的来说，我们提出了一种有效且高效的解码时解决方案来缓解幻觉，从而增强了LVLM在更多实践中的可靠性。代码将在Github上发布。", "summary": "本文提出了一种名为动态Logits校准（DLC）的新型免训练解码框架，旨在解决大型视觉-语言模型（LVLM）中常见的幻觉问题。DLC通过在解码阶段动态地使用CLIP评估视觉与文本的语义对齐，并基于相对视觉优势（RVA）和自适应加权机制调整输出logits，从而在推理时将文本生成与视觉证据对齐。实验证明，DLC能显著减少幻觉，优于现有方法，并保持高推理效率。", "keywords": "大型视觉-语言模型, 幻觉缓解, 动态Logits校准, 免训练解码, 视觉-语言对齐", "comments": "DLC的创新之处在于其动态调整logits的机制，通过实时评估视觉与文本的语义对齐来指导生成过程，克服了现有方法中静态约束和效率低下的问题。其免训练的特性以及避免多次前向传播的设计也大大提升了实用性。"}}
{"id": "2506.21513", "title": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation", "authors": ["Wentao Hu", "Shunkai Li", "Ziqiao Peng", "Haoxian Zhang", "Fan Shi", "Xiaoqiang Liu", "Pengfei Wan", "Di Zhang", "Hui Tian"], "summary": "Creating high-quality, generalizable speech-driven 3D talking heads remains a\npersistent challenge. Previous methods achieve satisfactory results for fixed\nviewpoints and small-scale audio variations, but they struggle with large head\nrotations and out-of-distribution (OOD) audio. Moreover, they are constrained\nby the need for time-consuming, identity-specific training. We believe the core\nissue lies in the lack of sufficient 3D priors, which limits the extrapolation\ncapabilities of synthesized talking heads. To address this, we propose\nGGTalker, which synthesizes talking heads through a combination of\ngeneralizable priors and identity-specific adaptation. We introduce a two-stage\nPrior-Adaptation training strategy to learn Gaussian head priors and adapt to\nindividual characteristics. We train Audio-Expression and Expression-Visual\npriors to capture the universal patterns of lip movements and the general\ndistribution of head textures. During the Customized Adaptation, individual\nspeaking styles and texture details are precisely modeled. Additionally, we\nintroduce a color MLP to generate fine-grained, motion-aligned textures and a\nBody Inpainter to blend rendered results with the background, producing\nindistinguishable, photorealistic video frames. Comprehensive experiments show\nthat GGTalker achieves state-of-the-art performance in rendering quality, 3D\nconsistency, lip-sync accuracy, and training efficiency.", "comment": "ICCV 2025, Project page: https://vincenthu19.github.io/GGTalker/", "pdf_url": "http://arxiv.org/pdf/2506.21513v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21513v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GGTalker：基于可泛化高斯先验和身份特异性适应的说话人脸合成", "tldr": "GGTalker通过结合可泛化高斯先验和身份特异性适应，解决了高质量、可泛化3D说话人脸合成中现有方法在头部大旋转和OOD音频上的局限性，并提高了训练效率和效果。", "motivation": "创建高质量、可泛化的语音驱动3D说话人脸合成仍然是一个挑战。现有方法在固定视角和小规模音频变化下表现尚可，但在大头部旋转和域外音频（OOD）下表现不佳，且需要耗时的身份特异性训练。核心问题在于缺乏足够的3D先验知识，限制了合成说话人脸的外推能力。", "method": "提出GGTalker，通过结合可泛化先验和身份特异性适应来合成说话人脸。引入两阶段的“先验-适应”训练策略：学习高斯头部先验并适应个体特征。训练音频-表情先验和表情-视觉先验，以捕获唇部运动的通用模式和头部纹理的通用分布。在定制适应阶段，精确建模个体说话风格和纹理细节。引入颜色MLP生成精细的、与运动对齐的纹理，并引入身体修复器（Body Inpainter）将渲染结果与背景融合，生成逼真的视频帧。", "result": "GGTalker在渲染质量、3D一致性、唇形同步准确性和训练效率方面达到了最先进的性能。", "conclusion": "GGTalker通过结合可泛化高斯先验和身份特异性适应，有效解决了语音驱动3D说话人脸合成中的泛化性、鲁棒性及训练效率问题，并取得了SOTA表现。", "translation": "创建高质量、可泛化的语音驱动3D说话人脸仍然是一个持续的挑战。现有方法在固定视角和小规模音频变化下取得了令人满意的结果，但它们在大头部旋转和域外（OOD）音频方面表现不佳。此外，它们受限于耗时的身份特异性训练。我们认为核心问题在于缺乏足够的3D先验知识，这限制了合成说话人脸的外推能力。为了解决这个问题，我们提出了GGTalker，它通过结合可泛化先验和身份特异性适应来合成说话人脸。我们引入了一种两阶段的“先验-适应”训练策略，以学习高斯头部先验并适应个体特征。我们训练音频-表情先验和表情-视觉先验，以捕捉唇部运动的通用模式和头部纹理的通用分布。在定制适应阶段，精确建模了个体说话风格和纹理细节。此外，我们引入了颜色MLP来生成精细的、与运动对齐的纹理，并引入了身体修复器（Body Inpainter）将渲染结果与背景融合，从而产生难以区分的、逼真的视频帧。综合实验表明，GGTalker在渲染质量、3D一致性、唇形同步准确性和训练效率方面达到了最先进的性能。", "summary": "针对高质量、可泛化语音驱动3D说话人脸合成的挑战，特别是现有方法在大头部旋转和OOD音频下的不足以及耗时的身份特异性训练问题，本文提出了GGTalker。该方法结合了可泛化高斯先验和身份特异性适应，通过两阶段的“先验-适应”训练策略，学习通用模式并精细建模个体特征。GGTalker引入了颜色MLP和身体修复器以生成逼真视频帧，并在实验中展现出在渲染质量、3D一致性、唇形同步和训练效率方面的SOTA性能。", "keywords": "说话人脸合成, 3D先验, 高斯先验, 身份适应, 泛化性", "comments": "这项工作通过引入可泛化高斯先验和两阶段的适应策略，显著提升了3D说话人脸合成的泛化能力和鲁棒性，尤其是在处理大头部旋转和域外音频方面。其结合通用模式学习和个体细节建模的思路具有创新性，并且通过颜色MLP和身体修复器进一步增强了视觉真实感，为该领域树立了新的基准。"}}
{"id": "2506.21514", "title": "G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation", "authors": ["Mohammed Rakib", "Arunkumar Bagavathi"], "summary": "Multimodal learning aims to leverage information from diverse data modalities\nto achieve more comprehensive performance. However, conventional multimodal\nmodels often suffer from modality imbalance, where one or a few modalities\ndominate model optimization, leading to suboptimal feature representation and\nunderutilization of weak modalities. To address this challenge, we introduce\nGradient-Guided Distillation (G$^{2}$D), a knowledge distillation framework\nthat optimizes the multimodal model with a custom-built loss function that\nfuses both unimodal and multimodal objectives. G$^{2}$D further incorporates a\ndynamic sequential modality prioritization (SMP) technique in the learning\nprocess to ensure each modality leads the learning process, avoiding the\npitfall of stronger modalities overshadowing weaker ones. We validate G$^{2}$D\non multiple real-world datasets and show that G$^{2}$D amplifies the\nsignificance of weak modalities while training and outperforms state-of-the-art\nmethods in classification and regression tasks. Our code is available at\nhttps://github.com/rAIson-Lab/G2D.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21514v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21514v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "G$^{2}$D：通过梯度引导蒸馏提升多模态学习", "tldr": "G$^{2}$D通过梯度引导蒸馏，结合定制损失函数和动态模态优先级技术，解决了多模态学习中的模态不平衡问题，并显著提升了模型性能。", "motivation": "传统多模态模型常受模态不平衡困扰，导致特征表示不佳和弱模态未被充分利用，即一个或少数模态主导模型优化，进而影响整体性能。", "method": "本文引入了梯度引导蒸馏（G$^{2}$D），这是一个知识蒸馏框架，通过融合单模态和多模态目标的定制损失函数来优化多模态模型。G$^{2}$D还结合了动态序列模态优先级（SMP）技术，确保每个模态都能引导学习过程，避免强模态掩盖弱模态。", "result": "G$^{2}$D在多个真实世界数据集上进行了验证，结果显示它在训练过程中增强了弱模态的重要性，并在分类和回归任务中超越了现有最先进的方法。", "conclusion": "G$^{2}$D框架有效解决了多模态学习中的模态不平衡问题，通过其创新的蒸馏和模态优先级技术，显著提升了多模态模型的性能。", "translation": "多模态学习旨在利用来自不同数据模态的信息以实现更全面的性能。然而，传统的多模态模型常常受到模态不平衡的困扰，即一个或几个模态主导模型优化，导致次优的特征表示和弱模态的未充分利用。为了解决这一挑战，我们引入了梯度引导蒸馏（G$^{2}$D），这是一个知识蒸馏框架，通过定制的损失函数来优化多模态模型，该函数融合了单模态和多模态目标。G$^{2}$D在学习过程中进一步融入了动态序列模态优先级（SMP）技术，以确保每个模态都能引导学习过程，避免强模态掩盖弱模态的弊端。我们在多个真实世界数据集上验证了G$^{2}$D，结果表明G$^{2}$D在训练过程中增强了弱模态的重要性，并在分类和回归任务中超越了现有最先进的方法。我们的代码可在https://github.com/rAIson-Lab/G2D获取。", "summary": "本文提出了一种名为梯度引导蒸馏（G$^{2}$D）的新型知识蒸馏框架，旨在解决多模态学习中常见的模态不平衡问题。G$^{2}$D通过设计一个融合了单模态和多模态目标的定制损失函数来优化模型，并引入了动态序列模态优先级（SMP）技术，以确保弱模态在学习过程中获得足够重视。实验结果表明，G$^{2}$D能够有效增强弱模态的重要性，并在分类和回归任务中超越了现有先进方法，从而提升了多模态模型的整体性能。", "keywords": "多模态学习, 知识蒸馏, 模态不平衡, 梯度引导, 动态优先级", "comments": "G$^{2}$D的创新之处在于其结合了知识蒸馏、定制损失函数以及动态序列模态优先级（SMP）技术，以直接解决多模态学习中的核心挑战——模态不平衡问题。这种方法有效避免了强模态对弱模态的掩盖，从而实现了更全面的特征学习和性能提升，对于推动多模态模型在实际应用中的发展具有重要意义。"}}
{"id": "2506.21079", "title": "Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games", "authors": ["Yann Kerzreho"], "summary": "This paper introduces a new approach for approximating the learning dynamics\nof multiple reinforcement learning (RL) agents interacting in a finite-state\nMarkov game. The idea is to rescale the learning process by simultaneously\nreducing the learning rate and increasing the update frequency, effectively\ntreating the agent's parameters as a slow-evolving variable influenced by the\nfast-mixing game state. Under mild assumptions-ergodicity of the state process\nand continuity of the updates-we prove the convergence of this rescaled process\nto an ordinary differential equation (ODE). This ODE provides a tractable,\ndeterministic approximation of the agent's learning dynamics. An implementation\nof the framework is available at\\,:\nhttps://github.com/yannKerzreho/MarkovGameApproximation", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21079v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21079v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "有限状态马尔可夫博弈中多智能体学习动力学的均匀化", "tldr": "本文提出了一种新的方法，通过重缩放学习过程，将多智能体强化学习动力学近似为常微分方程（ODE），从而提供了一种可处理的确定性近似。", "motivation": "本文旨在为在有限状态马尔可夫博弈中交互的多个强化学习（RL）智能体的学习动力学提供一种新的近似方法。", "method": "该方法通过同时降低学习率和增加更新频率来重缩放学习过程，将智能体的参数视为受快速混合博弈状态影响的慢速演化变量。在状态过程遍历性和更新连续性的温和假设下，证明了重缩放过程收敛到一个常微分方程（ODE）。", "result": "重缩放过程收敛到一个常微分方程（ODE）。这个ODE为智能体的学习动力学提供了一个可处理的、确定性的近似。", "conclusion": "该研究成功地将多智能体学习动力学近似为常微分方程，为理解和分析复杂的RL系统提供了一种简化的确定性模型。", "translation": "本文介绍了一种新的方法，用于近似在有限状态马尔可夫博弈中交互的多个强化学习（RL）智能体的学习动力学。其思想是通过同时降低学习率和增加更新频率来重缩放学习过程，有效地将智能体的参数视为受快速混合博弈状态影响的慢速演化变量。在温和假设——状态过程的遍历性和更新的连续性——下，我们证明了该重缩放过程收敛到一个常微分方程（ODE）。这个ODE为智能体的学习动力学提供了一个可处理的、确定性的近似。该框架的实现可在以下网址获取：https://github.com/yannKerzreho/MarkovGameApproximation", "summary": "本文提出了一种新颖的方法，通过对学习过程进行重缩放，将有限状态马尔可夫博弈中多智能体强化学习的动力学近似为常微分方程（ODE）。通过降低学习率和提高更新频率，将智能体参数视为慢变量，并在温和假设下证明了该重缩放过程收敛到ODE。该ODE提供了一个可处理的、确定性的学习动力学近似。", "keywords": "多智能体学习动力学, 马尔可夫博弈, 均匀化, 常微分方程, 强化学习", "comments": "该论文的创新之处在于其提出的均匀化方法，通过将多智能体强化学习动力学近似为常微分方程，极大地简化了复杂系统的分析。这种方法提供了一个可处理的、确定性的模型，有望为多智能体RL理论和应用带来新的见解。其局限性可能在于对“温和假设”的具体要求以及该近似在实际复杂场景中的普适性。"}}
{"id": "2506.21520", "title": "MADrive: Memory-Augmented Driving Scene Modeling", "authors": ["Polina Karpikova", "Daniil Selikhanovych", "Kirill Struminsky", "Ruslan Musaev", "Maria Golitsyna", "Dmitry Baranchuk"], "summary": "Recent advances in scene reconstruction have pushed toward highly realistic\nmodeling of autonomous driving (AD) environments using 3D Gaussian splatting.\nHowever, the resulting reconstructions remain closely tied to the original\nobservations and struggle to support photorealistic synthesis of significantly\naltered or novel driving scenarios. This work introduces MADrive, a\nmemory-augmented reconstruction framework designed to extend the capabilities\nof existing scene reconstruction methods by replacing observed vehicles with\nvisually similar 3D assets retrieved from a large-scale external memory bank.\nSpecifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg}\ncar videos captured in the wild and present a retrieval module that finds the\nmost similar car instances in the memory bank, reconstructs the corresponding\n3D assets from video, and integrates them into the target scene through\norientation alignment and relighting. The resulting replacements provide\ncomplete multi-view representations of vehicles in the scene, enabling\nphotorealistic synthesis of substantially altered configurations, as\ndemonstrated in our experiments. Project page:\nhttps://yandex-research.github.io/madrive/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21520v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21520v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MADrive：记忆增强的驾驶场景建模", "tldr": "MADrive是一个记忆增强的重建框架，通过从外部大型车辆资产库中检索并替换现有场景中的车辆，实现了自动驾驶环境中显著改变或新颖驾驶场景的光真实感合成。", "motivation": "现有的3D高斯泼溅场景重建方法虽然能实现高真实感建模，但重建结果与原始观测紧密相关，难以支持显著改变或新颖驾驶场景的光真实感合成。", "method": "本研究引入了MADrive框架，通过将观测到的车辆替换为从大型外部记忆库中检索到的视觉相似的3D资产来扩展现有场景重建方法的能力。具体地，发布了MAD-Cars数据集（包含约7万个360度野外捕获的汽车视频），并提出了一个检索模块，该模块能找到最相似的汽车实例，从视频中重建相应的3D资产，并通过方向对齐和重新打光将其整合到目标场景中。", "result": "替换后的车辆提供了场景中车辆的完整多视图表示，使得能够光真实感地合成大幅改变的配置。实验证明了这一点。", "conclusion": "MADrive通过引入记忆增强的重建框架和大规模车辆资产库，成功克服了现有场景重建方法在支持复杂驾驶场景合成方面的局限性，实现了更高灵活度和真实感的场景建模。", "translation": "标题：MADrive：记忆增强的驾驶场景建模\n\n摘要：场景重建的最新进展推动了使用3D高斯泼溅技术对自动驾驶（AD）环境进行高度逼真的建模。然而，由此产生的重建结果仍然与原始观测紧密相关，难以支持显著改变或新颖驾驶场景的光真实感合成。这项工作引入了MADrive，一个记忆增强的重建框架，旨在通过用从大型外部记忆库中检索到的视觉相似的3D资产替换观察到的车辆来扩展现有场景重建方法的能力。具体来说，我们发布了MAD-Cars，一个精选的包含约7万个野外捕获的360度汽车视频的数据集，并提出了一个检索模块，该模块在记忆库中找到最相似的汽车实例，从视频中重建相应的3D资产，并通过方向对齐和重新打光将其整合到目标场景中。由此产生的替换提供了场景中车辆的完整多视图表示，使大幅改变配置的光真实感合成成为可能，这在我们的实验中得到了证明。项目页面：https://yandex-research.github.io/madrive/", "summary": "MADrive是一个记忆增强的框架，旨在解决现有3D场景重建方法在合成大幅改变或新颖驾驶场景时的局限性。它通过从包含7万个360度汽车视频的MAD-Cars数据集中检索视觉相似的3D车辆资产，并将其集成到目标场景中，从而实现对车辆的多视图、光真实感替换，极大地增强了自动驾驶环境的场景合成能力。", "keywords": "驾驶场景建模, 记忆增强, 3D高斯泼溅, 车辆合成, 自动驾驶", "comments": "MADrive的创新点在于引入了“记忆增强”的概念，通过外部大规模3D资产库来增强场景重建的灵活性和真实感。它解决了现有方法难以合成大幅改变场景的痛点，通过MAD-Cars数据集和智能检索-集成模块，为自动驾驶环境的模拟和测试提供了新的、更强大的工具。这项工作在实际应用中具有重要意义，尤其是在需要生成多样化和极端驾驶场景进行自动驾驶系统训练和验证的领域。"}}
{"id": "2506.21526", "title": "WAFT: Warping-Alone Field Transforms for Optical Flow", "authors": ["Yihan Wang", "Jia Deng"], "summary": "We introduce Warping-Alone Field Transforms (WAFT), a simple and effective\nmethod for optical flow. WAFT is similar to RAFT but replaces cost volume with\nhigh-resolution warping, achieving better accuracy with lower memory cost. This\ndesign challenges the conventional wisdom that constructing cost volumes is\nnecessary for strong performance. WAFT is a simple and flexible\nmeta-architecture with minimal inductive biases and reliance on custom designs.\nCompared with existing methods, WAFT ranks 1st on Spring and KITTI benchmarks,\nachieves the best zero-shot generalization on KITTI, while being up to 4.1x\nfaster than methods with similar performance. Code and model weights are\navailable at https://github.com/princeton-vl/WAFT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21526v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21526v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "WAFT：仅翘曲场变换实现光流估计", "tldr": "WAFT是一种简单有效的光流估计新方法，通过高分辨率翘曲取代成本体，实现了更高的精度、更低的内存消耗和更快的速度，并挑战了传统观念。", "motivation": "该研究的动机是挑战光流估计领域中“构建成本体对于获得强大性能是必要的”这一传统观念，旨在开发一种更简单、更高效的方法。", "method": "WAFT（Warping-Alone Field Transforms）是一种用于光流估计的元架构。它类似于RAFT，但用高分辨率翘曲替代了成本体，从而降低了内存成本并提高了精度。WAFT设计简单灵活，具有最小的归纳偏置和对自定义设计的依赖。", "result": "WAFT在Spring和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化能力，并且比性能相似的方法快4.1倍。", "conclusion": "WAFT证明了通过高分辨率翘曲替代成本体可以实现卓越的光流估计性能，这不仅挑战了领域内的传统认知，还提供了更高效、更通用的解决方案。", "translation": "我们引入了仅翘曲场变换（WAFT），这是一种简单有效的光流估计方法。WAFT类似于RAFT，但用高分辨率翘曲取代了成本体，以更低的内存成本实现了更高的精度。这种设计挑战了“构建成本体对于获得强大性能是必要的”这一传统观念。WAFT是一个简单灵活的元架构，具有最小的归纳偏置和对自定义设计的依赖。与现有方法相比，WAFT在Spring和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化能力，同时比性能相似的方法快4.1倍。代码和模型权重可在 https://github.com/princeton-vl/WAFT 获取。", "summary": "WAFT（Warping-Alone Field Transforms）是一种新颖且高效的光流估计方法。它通过用高分辨率翘曲取代传统的成本体，在保持甚至提升精度的同时显著降低了内存消耗。该方法挑战了构建成本体是高性能必要条件的传统观点。WAFT作为一种简单灵活的元架构，在Spring和KITTI基准测试中表现优异，实现了最佳的零样本泛化，并且比同类方法快达4.1倍。", "keywords": "光流, WAFT, 高分辨率翘曲, 成本体, 零样本泛化", "comments": "该论文通过提出WAFT，成功挑战了光流领域中长期以来关于成本体必要性的传统认知，这本身就是一项重要的创新。其“仅翘曲”的设计理念不仅简化了模型，还显著提升了效率和泛化能力，为光流估计方法的设计提供了新的方向。"}}
{"id": "2506.21541", "title": "StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning", "authors": ["Chuxin Wang", "Yixin Zha", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Recently, Mamba-based methods have demonstrated impressive performance in\npoint cloud representation learning by leveraging State Space Model (SSM) with\nthe efficient context modeling ability and linear complexity. However, these\nmethods still face two key issues that limit the potential of SSM: Destroying\nthe adjacency of 3D points during SSM processing and failing to retain\nlong-sequence memory as the input length increases in downstream tasks. To\naddress these issues, we propose StruMamba3D, a novel paradigm for\nself-supervised point cloud representation learning. It enjoys several merits.\nFirst, we design spatial states and use them as proxies to preserve spatial\ndependencies among points. Second, we enhance the SSM with a state-wise update\nstrategy and incorporate a lightweight convolution to facilitate interactions\nbetween spatial states for efficient structure modeling. Third, our method\nreduces the sensitivity of pre-trained Mamba-based models to varying input\nlengths by introducing a sequence length-adaptive strategy. Experimental\nresults across four downstream tasks showcase the superior performance of our\nmethod. In addition, our method attains the SOTA 95.1% accuracy on ModelNet40\nand 92.75% accuracy on the most challenging split of ScanObjectNN without\nvoting strategy.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21541v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21541v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "StruMamba3D：探索结构化Mamba用于自监督点云表示学习", "tldr": "StruMamba3D通过引入空间状态和序列长度自适应策略，解决了现有基于Mamba的点云表示学习方法中三维点邻接性破坏和长序列记忆保留不足的问题，并在多个下游任务中取得了优越的性能和最先进的结果。", "motivation": "现有的基于Mamba的点云表示学习方法存在两个主要问题：在SSM处理过程中破坏三维点的邻接性，以及在下游任务中随着输入长度增加未能保留长序列记忆。", "method": "本文提出了StruMamba3D，一种新的自监督点云表示学习范式。它通过设计空间状态作为代理来保留点之间的空间依赖性；通过状态感知更新策略增强SSM，并结合轻量级卷积促进空间状态间的交互以进行高效结构建模；通过引入序列长度自适应策略，降低预训练的Mamba模型对不同输入长度的敏感性。", "result": "StruMamba3D在四个下游任务中展示了优越的性能。在ModelNet40上达到了95.1%的最先进（SOTA）准确率，在ScanObjectNN最具挑战性的分割上（不使用投票策略）达到了92.75%的准确率。", "conclusion": "StruMamba3D有效解决了现有基于Mamba的点云表示学习方法的局限性，在多个基准数据集上展现了卓越的性能和最先进的准确率。", "translation": "最近，基于Mamba的方法通过利用状态空间模型（SSM）高效的上下文建模能力和线性复杂度，在点云表示学习中展示了令人印象深刻的性能。然而，这些方法仍然面临两个限制SSM潜力的关键问题：在SSM处理过程中破坏三维点的邻接性，以及在下游任务中随着输入长度的增加未能保留长序列记忆。为了解决这些问题，我们提出了StruMamba3D，一种用于自监督点云表示学习的新范式。它具有多项优点。首先，我们设计了空间状态并将其用作代理以保留点之间的空间依赖性。其次，我们通过状态感知更新策略增强了SSM，并结合了轻量级卷积以促进空间状态之间的交互，从而实现高效的结构建模。第三，我们的方法通过引入序列长度自适应策略，降低了预训练的基于Mamba的模型对不同输入长度的敏感性。在四个下游任务上的实验结果展示了我们方法的优越性能。此外，我们的方法在ModelNet40上达到了95.1%的最先进准确率，在ScanObjectNN最具挑战性的分割上（不使用投票策略）达到了92.75%的准确率。", "summary": "StruMamba3D提出了一种新颖的自监督点云表示学习范式，旨在解决现有基于Mamba方法在处理三维点云时面临的邻接性破坏和长序列记忆保留不足的问题。该方法通过引入空间状态来维护点间空间依赖，并结合状态感知更新策略和轻量级卷积来增强SSM的结构建模能力。此外，其序列长度自适应策略有效提升了模型对不同输入长度的鲁棒性。实验证明，StruMamba3D在多项下游任务中表现卓越，并在ModelNet40和ScanObjectNN数据集上取得了最先进的准确率。", "keywords": "点云, 自监督学习, Mamba, 状态空间模型, 表示学习", "comments": "该论文的创新点在于为Mamba模型在点云学习中引入了空间状态概念，并结合了状态感知更新和轻量级卷积来更好地处理三维结构，同时提出的序列长度自适应策略增强了模型的实用性。其重要性在于显著提升了Mamba在点云表示学习领域的性能，并在关键数据集上达到了SOTA水平。抽象中未明确提及局限性。"}}
{"id": "2506.21544", "title": "DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion", "authors": ["Yansong Qu", "Shaohui Dai", "Xinyang Li", "Yuze Wang", "You Shen", "Liujuan Cao", "Rongrong Ji"], "summary": "Reconstructing 3D objects from a single image is a long-standing challenge,\nespecially under real-world occlusions. While recent diffusion-based view\nsynthesis models can generate consistent novel views from a single RGB image,\nthey generally assume fully visible inputs and fail when parts of the object\nare occluded. This leads to inconsistent views and degraded 3D reconstruction\nquality. To overcome this limitation, we propose an end-to-end framework for\nocclusion-aware multi-view generation. Our method directly synthesizes six\nstructurally consistent novel views from a single partially occluded image,\nenabling downstream 3D reconstruction without requiring prior inpainting or\nmanual annotations. We construct a self-supervised training pipeline using the\nPix2Gestalt dataset, leveraging occluded-unoccluded image pairs and\npseudo-ground-truth views to teach the model structure-aware completion and\nview consistency. Without modifying the original architecture, we fully\nfine-tune the view synthesis model to jointly learn completion and multi-view\ngeneration. Additionally, we introduce the first benchmark for occlusion-aware\nreconstruction, encompassing diverse occlusion levels, object categories, and\nmask patterns. This benchmark provides a standardized protocol for evaluating\nfuture methods under partial occlusions. Our code is available at\nhttps://github.com/Quyans/DeOcc123.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21544v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21544v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DeOcc-1-to-3：基于自监督多视角扩散的单图像三维去遮挡", "tldr": "DeOcc-1-to-3提出一个自监督多视角扩散框架，从单张遮挡图像生成一致的新视角以进行三维重建，并引入了一个新基准。", "motivation": "从单张图像重建三维物体是一个长期存在的挑战，尤其是在真实世界的遮挡情况下。现有的基于扩散的视角合成模型在物体部分被遮挡时会失效，导致视角不一致和三维重建质量下降。", "method": "提出一个端到端、遮挡感知的多视角生成框架，直接从单张部分遮挡图像合成六个结构一致的新视角。该方法利用Pix2Gestalt数据集构建自监督训练流程，通过遮挡-非遮挡图像对和伪真值视角，训练模型学习结构感知补全和视角一致性。在不修改原始架构的情况下，全面微调视角合成模型以联合学习补全和多视角生成。此外，还引入了首个用于遮挡感知重建的基准。", "result": "该方法能够从单张部分遮挡图像直接合成六个结构一致的新视角，无需预先修复或手动标注即可进行后续的三维重建。同时，提出的基准为评估未来在部分遮挡下的方法提供了标准化协议。", "conclusion": "该论文提出了一个端到端的框架，用于从单张遮挡图像进行三维去遮挡和多视角生成，并通过自监督学习实现结构感知补全和视角一致性。同时，还引入了首个遮挡感知重建的基准，为该领域提供了标准化评估协议。", "translation": "从单张图像重建三维物体是一个长期存在的挑战，尤其是在真实世界的遮挡情况下。虽然最近基于扩散的视角合成模型可以从单张RGB图像生成一致的新视角，但它们通常假设输入是完全可见的，并且当物体部分被遮挡时会失效。这导致视角不一致和三维重建质量下降。为了克服这一限制，我们提出了一个端到端的遮挡感知多视角生成框架。我们的方法直接从单张部分遮挡图像合成六个结构一致的新视角，无需预先修复或手动标注即可实现后续的三维重建。我们利用Pix2Gestalt数据集构建了一个自监督训练流程，利用遮挡-非遮挡图像对和伪真值视角来教导模型进行结构感知补全和视角一致性。在不修改原始架构的情况下，我们完全微调了视角合成模型，以联合学习补全和多视角生成。此外，我们引入了首个用于遮挡感知重建的基准，其涵盖了不同的遮挡级别、物体类别和遮罩模式。该基准为评估未来在部分遮挡下的方法提供了标准化协议。我们的代码可在 https://github.com/Quyans/DeOcc123 获取。", "summary": "该论文提出了DeOcc-1-to-3，一个端到端的自监督多视角扩散框架，旨在解决从单张遮挡图像进行三维重建的挑战。针对现有扩散模型在遮挡场景下表现不佳的问题，该方法通过自监督学习，利用遮挡-非遮挡图像对和伪真值视角，从单张部分遮挡图像直接合成六个结构一致的新视角，从而实现高质量的三维重建，无需预先修复。此外，论文还首次引入了一个用于遮挡感知重建的基准，为该领域未来的研究提供了统一的评估标准。", "keywords": "三维去遮挡, 单图像重建, 自监督学习, 多视角扩散, 基准测试", "comments": "该论文的创新之处在于提出了一种自监督的多视角扩散框架，能够有效解决单图像三维重建中长期存在的遮挡问题。通过联合学习图像补全和多视角生成，显著提升了在部分遮挡条件下的三维重建质量。同时，引入首个遮挡感知重建基准，为该领域未来的研究提供了宝贵的评估工具和标准化协议，具有重要的实际应用价值和研究推动作用。"}}
{"id": "2506.21549", "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark", "authors": ["Alex Costanzino", "Pierluigi Zama Ramirez", "Luigi Lella", "Matteo Ragaglia", "Alessandro Oliva", "Giuseppe Lisanti", "Luigi Di Stefano"], "summary": "We propose SiM3D, the first benchmark considering the integration of\nmultiview and multimodal information for comprehensive 3D anomaly detection and\nsegmentation (ADS), where the task is to produce a voxel-based Anomaly Volume.\nMoreover, SiM3D focuses on a scenario of high interest in manufacturing:\nsingle-instance anomaly detection, where only one object, either real or\nsynthetic, is available for training. In this respect, SiM3D stands out as the\nfirst ADS benchmark that addresses the challenge of generalising from synthetic\ntraining data to real test data. SiM3D includes a novel multimodal multiview\ndataset acquired using top-tier industrial sensors and robots. The dataset\nfeatures multiview high-resolution images (12 Mpx) and point clouds (7M points)\nfor 333 instances of eight types of objects, alongside a CAD model for each\ntype. We also provide manually annotated 3D segmentation GTs for anomalous test\nsamples. To establish reference baselines for the proposed multiview 3D ADS\ntask, we adapt prominent singleview methods and assess their performance using\nnovel metrics that operate on Anomaly Volumes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21549v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21549v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SiM3D：单实例多视角多模态多设置三维异常检测基准", "tldr": "提出了SiM3D，首个整合多视角和多模态信息的3D异常检测与分割基准，专注于制造业中单实例异常检测，并解决合成数据到真实数据的泛化挑战。", "motivation": "现有的3D异常检测与分割缺乏整合多视角和多模态信息的基准。在制造业中，单实例异常检测（仅一个对象可用于训练）以及从合成数据到真实数据的泛化是重要的挑战。", "method": "提出了SiM3D，一个首次整合多视角和多模态信息的3D异常检测与分割基准。它包含一个使用顶级工业传感器和机器人获取的新颖多模态多视角数据集，该数据集包含333个实例的8种对象类型的高分辨率图像和点云，以及CAD模型和手动标注的3D分割真值。为了建立参考基线，论文还调整了著名的单视角方法，并使用新颖的异常体指标评估其性能。", "result": "SiM3D提供了一个用于3D异常检测和分割的综合基准，包含一个多模态多视角数据集（高分辨率图像和点云，共333个实例，8种对象类型），以及用于异常测试样本的手动标注3D分割真值。此外，还建立了基于调整后单视角方法的参考基线，并引入了新的异常体评估指标。", "conclusion": "SiM3D是首个将多视角和多模态信息整合到3D异常检测与分割中的基准，特别关注制造业中的单实例异常检测和合成到真实数据的泛化挑战，并提供了全面的数据集、真值和评估方法，为该领域的研究奠定了基础。", "translation": "我们提出了SiM3D，这是第一个考虑整合多视角和多模态信息以进行全面三维异常检测和分割（ADS）的基准，其任务是生成基于体素的异常体。此外，SiM3D专注于制造业中一个备受关注的场景：单实例异常检测，即训练时只有一个对象可用，无论是真实的还是合成的。在这方面，SiM3D作为第一个解决从合成训练数据泛化到真实测试数据挑战的ADS基准脱颖而出。SiM3D包含一个使用顶级工业传感器和机器人获取的新颖多模态多视角数据集。该数据集包含333个实例的八种对象类型的多视角高分辨率图像（12 Mpx）和点云（7M点），以及每种类型的CAD模型。我们还为异常测试样本提供了手动标注的三维分割真值。为了为所提出的多视角三维ADS任务建立参考基线，我们调整了著名的单视角方法，并使用在异常体上操作的新颖指标评估了它们的性能。", "summary": "本文提出了SiM3D，一个开创性的3D异常检测与分割基准，首次整合了多视角和多模态信息。该基准特别关注制造业中的单实例异常检测场景，并解决了从合成数据到真实数据泛化的挑战。SiM3D包含一个新颖的多模态多视角数据集，该数据集通过工业级传感器和机器人采集，包含高分辨率图像、点云和CAD模型，并提供手动标注的3D分割真值。同时，论文还为多视角3D ADS任务建立了基线，并引入了新的评估指标。", "keywords": "3D异常检测, 多模态, 多视角, 单实例学习, 基准数据集", "comments": "SiM3D的创新之处在于它是首个将多视角和多模态信息整合到3D异常检测与分割领域的基准，并且特别关注了制造业中极具挑战性的单实例训练和合成到真实数据泛化问题。其提供的高质量数据集和评估方法为该领域未来的研究提供了宝贵的资源和标准。"}}
{"id": "2506.21460", "title": "Wild refitting for black box prediction", "authors": ["Martin J. Wainwright"], "summary": "We describe and analyze a computionally efficient refitting procedure for\ncomputing high-probability upper bounds on the instance-wise mean-squared\nprediction error of penalized nonparametric estimates based on least-squares\nminimization. Requiring only a single dataset and black box access to the\nprediction method, it consists of three steps: computing suitable residuals,\nsymmetrizing and scaling them with a pre-factor $\\rho$, and using them to\ndefine and solve a modified prediction problem recentered at the current\nestimate. We refer to it as wild refitting, since it uses Rademacher residual\nsymmetrization as in a wild bootstrap variant. Under relatively mild conditions\nallowing for noise heterogeneity, we establish a high probability guarantee on\nits performance, showing that the wild refit with a suitably chosen wild noise\nscale $\\rho$ gives an upper bound on prediction error. This theoretical\nanalysis provides guidance into the design of such procedures, including how\nthe residuals should be formed, the amount of noise rescaling in the wild\nsub-problem needed for upper bounds, and the local stability properties of the\nblock-box procedure. We illustrate the applicability of this procedure to\nvarious problems, including non-rigid structure-from-motion recovery with\nstructured matrix penalties; plug-and-play image restoration with deep neural\nnetwork priors; and randomized sketching with kernel methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21460v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21460v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "黑盒预测的野性再拟合", "tldr": "本文提出了一种计算高效的“野性再拟合”过程，用于计算基于最小二乘法的惩罚非参数估计的实例级均方预测误差的高概率上限，适用于黑盒预测方法，并提供理论保证和实际应用示例。", "motivation": "需要为基于最小二乘法惩罚非参数估计的实例级均方预测误差计算高概率上限，尤其是在只能进行黑盒访问的预测方法下。", "method": "该方法被称为“野性再拟合”，计算高效，仅需单个数据集和对预测方法的黑盒访问。它包含三个步骤：计算合适的残差；使用预因子$\\rho$对残差进行对称化和缩放；使用这些残差定义和解决一个以当前估计为中心修改后的预测问题。它采用类似野性自举变体的Rademacher残差对称化。", "result": "在允许噪声异质性的相对温和条件下，该方法建立了其性能的高概率保证，表明通过适当选择的野性噪声尺度$\\rho$的野性再拟合可以给出预测误差的上限。", "conclusion": "这项理论分析为此类程序的设计提供了指导，包括如何形成残差、野性子问题中为获得上限所需的噪声重标量以及黑盒程序的局部稳定性特性。该程序适用于各种问题，包括带结构化矩阵惩罚的非刚性运动恢复、带深度神经网络先验的即插即用图像恢复以及带核方法的随机草图。", "translation": "我们描述并分析了一种计算高效的再拟合过程，用于计算基于最小二乘法惩罚非参数估计的实例级均方预测误差的高概率上限。该过程仅需要一个数据集和对预测方法的黑盒访问，包含三个步骤：计算合适的残差；使用预因子$\\rho$对其进行对称化和缩放；并使用它们定义和解决一个以当前估计为中心修改后的预测问题。我们称之为野性再拟合，因为它使用Rademacher残差对称化，类似于野性自举变体。在允许噪声异质性的相对温和条件下，我们建立了其性能的高概率保证，表明通过适当选择的野性噪声尺度$\\rho$的野性再拟合可以给出预测误差的上限。这项理论分析为此类程序的设计提供了指导，包括如何形成残差、野性子问题中为获得上限所需的噪声重标量以及黑盒程序的局部稳定性特性。我们通过各种问题说明了该过程的适用性，包括带结构化矩阵惩罚的非刚性运动恢复；带深度神经网络先验的即插即用图像恢复；以及带核方法的随机草图。", "summary": "本文介绍了一种名为“野性再拟合”的计算高效程序，用于为基于最小二乘法的惩罚非参数估计计算实例级均方预测误差的高概率上限，尤其适用于仅提供黑盒访问的预测方法。该方法通过三个核心步骤实现：计算残差、使用Rademacher对称化和缩放残差，并解决一个以当前估计为中心修改后的预测问题。理论分析表明，在存在噪声异质性的情况下，通过适当选择的噪声尺度，该方法能可靠地提供预测误差的上限，并为设计此类程序提供了有价值的指导。其适用性在非刚性结构恢复、图像修复和核方法等多个领域得到了验证。", "keywords": "黑盒预测, 野性再拟合, 预测误差界, 非参数估计, Rademacher对称化", "comments": "该论文提出了一种创新且实用的方法，通过提供高概率上限来量化黑盒预测的不确定性。其核心优势在于计算效率高、仅依赖单一数据集且与黑盒模型兼容，这使其具有极高的实用价值。利用Rademacher残差对称化是野性自举原理的巧妙应用。在噪声异质性条件下的理论保证增强了其鲁棒性，而从分析中得出的实际指导对于方法设计具有重要意义。"}}
{"id": "2506.21511", "title": "Gaussian Invariant Markov Chain Monte Carlo", "authors": ["Michalis K. Titsias", "Angelos Alexopoulos", "Siran Liu", "Petros Dellaportas"], "summary": "We develop sampling methods, which consist of Gaussian invariant versions of\nrandom walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and\nsecond order Hessian or Manifold MALA. Unlike standard RWM and MALA we show\nthat Gaussian invariant sampling can lead to ergodic estimators with improved\nstatistical efficiency. This is due to a remarkable property of Gaussian\ninvariance that allows us to obtain exact analytical solutions to the Poisson\nequation for Gaussian targets. These solutions can be used to construct\nefficient and easy to use control variates for variance reduction of estimators\nunder any intractable target. We demonstrate the new samplers and estimators in\nseveral examples, including high dimensional targets in latent Gaussian models\nwhere we compare against several advanced methods and obtain state-of-the-art\nresults. We also provide theoretical results regarding geometric ergodicity,\nand an optimal scaling analysis that shows the dependence of the optimal\nacceptance rate on the Gaussianity of the target.", "comment": "29, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21511v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21511v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "高斯不变马尔可夫链蒙特卡罗", "tldr": "本文开发了高斯不变采样方法，包括高斯不变RWM、MALA和二阶Hessian/Manifold MALA，这些方法在统计效率上有所提高，并通过控制变量法实现了方差减少，并在高维潜在高斯模型中取得了最先进的结果。", "motivation": "现有的随机游走 Metropolis (RWM) 和 Metropolis 调整 Langevin 算法 (MALA) 在统计效率方面可能存在局限性，特别是在处理复杂目标分布时。本文旨在开发能够提高统计效率的采样方法。", "method": "本文开发了高斯不变版本的随机游走 Metropolis (RWM)、Metropolis 调整 Langevin 算法 (MALA) 和二阶 Hessian 或 Manifold MALA 采样方法。这些方法利用高斯不变性，能够获得高斯目标泊松方程的精确解析解。这些解析解被用于构建高效且易于使用的控制变量，以减少在任何难以处理的目标下的估计器方差。文章还提供了关于几何遍历性的理论结果和最优尺度分析。", "result": "高斯不变采样方法与标准 RWM 和 MALA 相比，能够生成具有更高统计效率的遍历估计器。通过利用高斯不变性获得的泊松方程解析解，可以构建有效的控制变量来减少估计器的方差。在多个示例（包括高维潜在高斯模型）中，新采样器和估计器表现出色，并取得了与现有先进方法相比的最先进结果。理论分析揭示了几何遍历性，并且最优尺度分析表明最佳接受率依赖于目标的“高斯性”。", "conclusion": "本文开发的高斯不变采样方法在统计效率和方差减少方面优于传统方法，并在高维复杂模型中取得了最先进的性能。理论分析也支持了这些方法的有效性。", "translation": "我们开发了采样方法，包括随机游走 Metropolis (RWM)、Metropolis 调整 Langevin 算法 (MALA) 和二阶 Hessian 或 Manifold MALA 的高斯不变版本。与标准的 RWM 和 MALA 不同，我们证明了高斯不变采样可以产生具有改进统计效率的遍历估计器。这归因于高斯不变性的一项显著特性，它使我们能够获得高斯目标泊松方程的精确解析解。这些解析解可用于构建高效且易于使用的控制变量，以减少任何难以处理目标下估计器的方差。我们在几个示例中展示了新的采样器和估计器，包括潜在高斯模型中的高维目标，在这些模型中我们与几种先进方法进行了比较，并获得了最先进的结果。我们还提供了关于几何遍历性的理论结果，以及显示最优接受率对目标高斯性依赖关系的最优尺度分析。", "summary": "本文提出了一系列高斯不变的马尔可夫链蒙特卡罗（MCMC）采样方法，包括高斯不变的RWM、MALA及其高阶变体。研究表明，这些新方法相比传统RWM和MALA，能显著提高估计器的统计效率，主要得益于高斯不变性允许对高斯目标泊松方程进行精确解析求解。这些解析解被用于构建有效的控制变量，以降低复杂目标下估计器的方差。实验证明，在高维潜在高斯模型等应用中，新方法能达到最先进的性能。此外，论文还提供了关于几何遍历性和最优尺度分析的理论支撑。", "keywords": "高斯不变性, 马尔可夫链蒙特卡罗, 采样方法, 统计效率, 控制变量", "comments": "本文的创新之处在于引入了“高斯不变性”的概念，并将其应用于MCMC采样方法中，从而显著提高了采样效率和方差减少能力。通过利用高斯目标泊松方程的精确解析解来构建控制变量，为处理复杂和高维目标分布提供了强大的工具，尤其在潜在高斯模型中取得了突破性进展。这项工作对MCMC领域具有重要意义。"}}
{"id": "2506.21331", "title": "Automatic Reviewers Assignment to a Research Paper Based on Allied References and Publications Weight", "authors": ["Tamim Al Mahmud", "B M Mainul Hossain", "Dilshad Ara"], "summary": "Everyday, a vast stream of research documents is submitted to conferences,\nanthologies, journals, newsletters, annual reports, daily papers, and various\nperiodicals. Many such publications use independent external specialists to\nreview submissions. This process is called peer review, and the reviewers are\ncalled referees. However, it is not always possible to pick the best referee\nfor reviewing. Moreover, new research fields are emerging in every sector, and\nthe number of research papers is increasing dramatically. To review all these\npapers, every journal assigns a small team of referees who may not be experts\nin all areas. For example, a research paper in communication technology should\nbe reviewed by an expert from the same field. Thus, efficiently selecting the\nbest reviewer or referee for a research paper is a big challenge.\n  In this research, we propose and implement program that uses a new strategy\nto automatically select the best reviewers for a research paper. Every research\npaper contains references at the end, usually from the same area. First, we\ncollect the references and count authors who have at least one paper in the\nreferences. Then, we automatically browse the web to extract research topic\nkeywords. Next, we search for top researchers in the specific topic and count\ntheir h-index, i10-index, and citations for the first n authors. Afterward, we\nrank the top n authors based on a score and automatically browse their\nhomepages to retrieve email addresses. We also check their co-authors and\ncolleagues online and discard them from the list. The remaining top n authors,\ngenerally professors, are likely the best referees for reviewing the research\npaper.", "comment": "IEEE Conference Proceedings (5 Pages)", "pdf_url": "http://arxiv.org/pdf/2506.21331v1", "categories": ["cs.DL", "cs.CV"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.21331v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于相关参考文献和出版物权重的研究论文自动审稿人分配", "tldr": "该研究提出了一种基于参考文献、作者指标（h指数、i10指数、引用次数）和排除合著者来自动为研究论文分配最佳审稿人的系统。", "motivation": "由于研究论文数量的急剧增加和新研究领域的不断涌现，高效地为研究论文选择合适的专家审稿人成为一项重大挑战，当前的同行评审过程可能无法总是选出最合适的审稿人。", "method": "该方法首先收集研究论文的参考文献，识别参考文献中至少有一篇论文的作者。然后，自动从网络提取研究主题关键词，并搜索特定主题的顶尖研究人员，统计他们的h指数、i10指数和引用次数。接着，根据得分对前n位作者进行排名，并自动浏览其主页以获取电子邮件地址。同时，系统还会检查并剔除合著者和同事，最终推荐剩余的顶尖作者作为最佳审稿人。", "result": "Not mentioned in abstract", "conclusion": "本研究提出的策略通过利用作者的出版历史和指标，旨在识别并分配最适合研究论文的审稿人。", "translation": "每天，大量的研究文档被提交给会议、文集、期刊、通讯、年度报告、日报和各种期刊。许多此类出版物使用独立的外部专家来评审提交的稿件。这个过程被称为同行评审，审稿人被称为评审员。然而，并非总能为评审选择最佳评审员。此外，每个领域都在涌现新的研究领域，研究论文的数量急剧增加。为了评审所有这些论文，每本期刊都会分配一个小型评审团队，他们可能并非所有领域的专家。例如，一篇通信技术的研究论文应该由同一领域的专家评审。因此，高效地为研究论文选择最佳审稿人或评审员是一个巨大的挑战。\n在这项研究中，我们提出并实现了一个程序，该程序使用一种新策略来自动选择研究论文的最佳审稿人。每篇研究论文的末尾都包含参考文献，通常来自同一领域。首先，我们收集参考文献并统计在参考文献中至少有一篇论文的作者。然后，我们自动浏览网页以提取研究主题关键词。接下来，我们搜索特定主题的顶尖研究人员，并统计前n位作者的h指数、i10指数和引用次数。之后，我们根据得分对前n位作者进行排名，并自动浏览他们的主页以检索电子邮件地址。我们还会在线检查他们的合著者和同事，并将其从列表中剔除。剩下的前n位作者（通常是教授）很可能是评审该研究论文的最佳评审员。", "summary": "本研究旨在解决因研究量激增和专业领域细化而导致的论文审稿人高效分配难题。论文提出了一种自动化系统，通过分析论文参考文献、提取研究主题关键词，并基于h指数、i10指数和引用次数对顶尖研究人员进行排名来识别潜在审稿人。该系统还过滤掉合著者以确保评审独立性，旨在选出最合适的专家。", "keywords": "审稿人分配, 同行评审, H指数, i10指数, 文献计量学", "comments": "该论文提出了一种创新的自动化审稿人分配方法，鉴于研究的指数级增长，这一点至关重要。其新颖之处在于结合了参考文献分析和网络抓取，以获取作者指标（h指数、i10指数、引用次数），并排除了利益冲突（合著者）。这种方法有望显著提高同行评审的效率和质量。然而，摘要中未提及所提出系统的任何评估或性能指标，这对于评估其有效性至关重要。"}}
