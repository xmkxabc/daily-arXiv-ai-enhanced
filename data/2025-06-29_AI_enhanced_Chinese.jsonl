{"id": "2506.20770", "title": "Perry: A High-level Framework for Accelerating Cyber Deception Experimentation", "authors": ["Brian Singer", "Yusuf Saquib", "Lujo Bauer", "Vyas Sekar"], "summary": "Cyber deception aims to distract, delay, and detect network attackers with\nfake assets such as honeypots, decoy credentials, or decoy files. However,\ntoday, it is difficult for operators to experiment, explore, and evaluate\ndeception approaches. Existing tools and platforms have non-portable and\ncomplex implementations that are difficult to modify and extend. We address\nthis pain point by introducing Perry, a high-level framework that accelerates\nthe design and exploration of deception what-if scenarios. Perry has two\ncomponents: a high-level abstraction layer for security operators to specify\nattackers and deception strategies, and an experimentation module to run these\nattackers and defenders in realistic emulated networks. To translate these\nhigh-level specifications we design four key modules for Perry: 1) an action\nplanner that translates high-level actions into low-level implementations, 2)\nan observability module to translate low-level telemetry into high-level\nobservations, 3) an environment state service that enables environment agnostic\nstrategies, and 4) an attack graph service to reason about how attackers could\nexplore an environment. We illustrate that Perry's abstractions reduce the\nimplementation effort of exploring a wide variety of deception defenses,\nattackers, and environments. We demonstrate the value of Perry by emulating 55\nunique deception what-if scenarios and illustrate how these experiments enable\noperators to shed light on subtle tradeoffs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20770v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20770v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "Perry：一个加速网络欺骗实验的高级框架", "tldr": "Perry是一个高级框架，旨在通过提供抽象层和实验模块来加速网络欺骗策略的设计和探索，解决现有工具复杂且难以修改的问题。", "motivation": "今天的网络欺骗实验中，操作人员难以探索和评估欺骗方法，因为现有工具和平台实现复杂且不可移植，难以修改和扩展。", "method": "Perry框架包含两个主要组件：一个高级抽象层，用于安全操作人员指定攻击者和欺骗策略；一个实验模块，用于在真实的模拟网络中运行这些攻击者和防御者。为实现高级规范的转换，Perry设计了四个关键模块：1）一个行动规划器，将高级行动转换为低级实现；2）一个可观察性模块，将低级遥测数据转换为高级观察结果；3）一个环境状态服务，实现环境无关策略；4）一个攻击图服务，推理攻击者如何探索环境。", "result": "Perry的抽象层减少了探索各种欺骗防御、攻击者和环境的实现工作。通过模拟55个独特的欺骗“假设”场景，展示了Perry的价值，并说明了这些实验如何帮助操作人员揭示微妙的权衡。", "conclusion": "Perry框架通过提供高级抽象和实验能力，显著降低了网络欺骗实验的复杂性，使操作人员能够更有效地设计、探索和评估欺骗策略，并理解其中的权衡。", "translation": "网络欺骗旨在通过蜜罐、诱饵凭证或诱饵文件等虚假资产来分散、延迟和检测网络攻击者。然而，目前操作人员难以对欺骗方法进行实验、探索和评估。现有工具和平台的实现不可移植且复杂，难以修改和扩展。我们通过引入Perry来解决这一痛点，Perry是一个高级框架，可加速欺骗假设情景的设计和探索。Perry有两个组成部分：一个供安全操作人员指定攻击者和欺骗策略的高级抽象层，以及一个用于在真实模拟网络中运行这些攻击者和防御者的实验模块。为了转换这些高级规范，我们为Perry设计了四个关键模块：1）一个行动规划器，将高级行动转换为低级实现；2）一个可观察性模块，将低级遥测数据转换为高级观察结果；3）一个环境状态服务，实现环境无关策略；4）一个攻击图服务，推理攻击者如何探索环境。我们证明了Perry的抽象层减少了探索各种欺骗防御、攻击者和环境的实现工作。我们通过模拟55个独特的欺骗假设情景来展示Perry的价值，并说明了这些实验如何使操作人员能够揭示微妙的权衡。", "summary": "Perry是一个高级框架，旨在解决当前网络欺骗实验中工具复杂、难以修改和扩展的问题。它通过提供一个高级抽象层供安全操作员定义攻击者和欺骗策略，并结合一个实验模块在模拟网络中运行这些场景。框架内部通过行动规划器、可观察性模块、环境状态服务和攻击图服务将高级规范转化为低级实现和观察。实验证明，Perry显著降低了欺骗防御、攻击者和环境探索的实现难度，并能帮助用户发现欺骗策略中的细微权衡。", "keywords": "网络欺骗, 实验框架, 蜜罐, 高级抽象, 安全", "comments": "Perry的创新之处在于其提供的高级抽象层和模块化设计，这大大降低了网络欺骗实验的复杂性和实现成本。它通过将复杂的低级操作抽象化，使得安全操作人员能够更专注于策略设计而非技术细节，从而加速了欺骗场景的探索和评估。该框架对于网络安全研究和实践具有重要意义，尤其是在应对日益复杂的网络威胁方面。"}}
{"id": "2506.20800", "title": "SIMulator: SIM Tracing on a (Pico-)Budget", "authors": ["Gabriel K. Gegenhuber", "Philipp É. Frenzel", "Adrian Dabrowski"], "summary": "SIM tracing -- the ability to inspect, modify, and relay communication\nbetween a SIM card and modem -- has become a significant technique in cellular\nnetwork research. It enables essential security- and development-related\napplications such as fuzzing communication interfaces, extracting session keys,\nmonitoring hidden SIM activity (e.g., proactive SIM commands or over-the-air\nupdates), and facilitating scalable, distributed measurement platforms through\nSIM reuse. Traditionally, achieving these capabilities has relied on\nspecialized hardware, which can pose financial and logistical burdens for\nresearchers, particularly those new to the field. In this work, we show that\nfull SIM tracing functionality can be achieved using only simple, widely\navailable components, such as UART interfaces and GPIO ports. We port these\ncapabilities to low-cost microcontrollers, exemplified by the Raspberry Pi Pico\n(4~USD). Unlike other approaches, it dramatically reduces hardware complexity\nby electrically decoupling the SIM and the modem and only transferring on APDU\nlevel. By significantly reducing hardware requirements and associated costs, we\naim to make SIM tracing techniques accessible to a broader community of\nresearchers and hobbyists, fostering wider exploration and experimentation in\ncellular network research.", "comment": "Accepted Poster at WiSec 2025", "pdf_url": "http://arxiv.org/pdf/2506.20800v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20800v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "SIMulator：低成本（Pico-）SIM卡追踪", "tldr": "通过使用廉价且现成的组件（如树莓派Pico），实现完整的SIM卡追踪功能，大幅降低了硬件成本和复杂性，使SIM卡追踪技术更易于获取。", "motivation": "传统的SIM卡追踪技术依赖于昂贵的专用硬件，给研究人员带来了经济和物流负担，阻碍了该技术在蜂窝网络研究领域的广泛应用和实验。", "method": "通过使用简单的、广泛可用的组件（如UART接口和GPIO端口）实现完整的SIM卡追踪功能。该方法将SIM卡和调制解调器电解耦，并仅在APDU级别进行数据传输，从而显著降低了硬件复杂性。他们将这些功能移植到低成本微控制器（例如树莓派Pico）。", "result": "成功展示了如何使用低成本、现成的组件（如树莓派Pico）实现完整的SIM卡追踪功能，显著降低了硬件复杂度和成本。", "conclusion": "通过显著降低硬件要求和相关成本，该工作旨在使SIM卡追踪技术能被更广泛的研究人员和爱好者社区所使用，从而促进蜂窝网络研究领域更广泛的探索和实验。", "translation": "SIM卡追踪——检查、修改和中继SIM卡与调制解调器之间通信的能力——已成为蜂窝网络研究中的一项重要技术。它支持重要的安全和开发相关应用，例如模糊测试通信接口、提取会话密钥、监控隐藏的SIM卡活动（例如主动SIM卡命令或空中更新），并通过SIM卡复用促进可扩展的分布式测量平台。传统上，实现这些功能依赖于专用硬件，这可能给研究人员，特别是该领域的新手，带来经济和物流负担。在这项工作中，我们展示了仅使用简单、广泛可用的组件，如UART接口和GPIO端口，即可实现完整的SIM卡追踪功能。我们将这些功能移植到低成本微控制器上，例如树莓派Pico（4美元）。与其他方法不同，它通过电解耦SIM卡和调制解调器，并且仅在APDU级别传输，从而大大降低了硬件复杂性。通过显著降低硬件要求和相关成本，我们旨在使SIM卡追踪技术能被更广泛的研究人员和爱好者社区所使用，从而促进蜂窝网络研究领域更广泛的探索和实验。", "summary": "本文提出了一种名为“SIMulator”的低成本SIM卡追踪解决方案，旨在解决传统SIM卡追踪技术对昂贵专用硬件的依赖问题。通过利用UART和GPIO等简单、现成的组件，并将其移植到如树莓派Pico等低成本微控制器上，该方法实现了完整的SIM卡追踪功能。其关键创新在于通过电解耦SIM卡和调制解调器并在APDU级别传输数据，大幅降低了硬件复杂性。这项工作显著降低了SIM卡追踪的门槛，使其更易于蜂窝网络研究人员和爱好者进行探索和实验。", "keywords": "SIM卡追踪, 低成本硬件, 蜂窝网络研究, 树莓派Pico, APDU级别", "comments": "这项工作的创新之处在于，它通过巧妙地利用廉价且普遍可用的硬件组件，并优化了SIM卡与调制解调器之间的通信方式（APDU级别传输和电解耦），从而极大地降低了SIM卡追踪技术的门槛。这对于资源有限的研究人员和学生来说意义重大，有望促进蜂窝网络安全和开发领域的普及性研究。其重要性在于，它将曾经昂贵的专业技术民主化，使更多人能够参与到蜂窝网络的研究和实验中来。"}}
{"id": "2506.20806", "title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis", "authors": ["Zhonghao Zhan", "Huichi Zhou", "Hamed Haddadi"], "summary": "Graph Neural Networks (GNNs) show great promise for Network Intrusion\nDetection Systems (NIDS), particularly in IoT environments, but suffer\nperformance degradation due to distribution drift and lack robustness against\nrealistic adversarial attacks. Current robustness evaluations often rely on\nunrealistic synthetic perturbations and lack demonstrations on systematic\nanalysis of different kinds of adversarial attack, which encompass both\nblack-box and white-box scenarios. This work proposes a novel approach to\nenhance GNN robustness and generalization by employing Large Language Models\n(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These\nagents scrutinize graph structures derived from network flow data, identifying\nand potentially mitigating suspicious or adversarially perturbed elements\nbefore GNN processing. Our experiments, using a framework designed for\nrealistic evaluation and testing with a variety of adversarial attacks\nincluding a dataset collected from physical testbed experiments, demonstrate\nthat integrating LLM analysis can significantly improve the resilience of\nGNN-based NIDS against challenges, showcasing the potential of LLM agent as a\ncomplementary layer in intrusion detection architectures.", "comment": "Poster accepted at the 10th IEEE European Symposium on Security and\n  Privacy (Euro S&P 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20806v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20806v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "海报：通过基于代理的分析增强GNN在网络入侵检测中的鲁棒性", "tldr": "本研究提出了一种新颖的方法，通过使用大型语言模型（LLM）作为网络安全专家代理，增强图神经网络（GNN）在网络入侵检测系统（NIDS）中的鲁棒性，实验证明LLM分析能显著提高GNN的弹性。", "motivation": "图神经网络（GNN）在网络入侵检测系统（NIDS）中表现出巨大潜力，尤其是在物联网环境中，但它们面临性能下降的问题，原因在于分布漂移和对现实对抗性攻击的鲁棒性不足。当前的鲁棒性评估通常依赖不切实际的合成扰动，并且缺乏对不同类型对抗性攻击（包括黑盒和白盒场景）进行系统分析的演示。", "method": "本研究提出了一种新颖的方法，通过在代理管道中利用大型语言模型（LLM）作为模拟网络安全专家代理，以增强GNN的鲁棒性和泛化能力。这些代理在GNN处理之前，审查从网络流数据中派生出的图结构，识别并可能缓解可疑或受对抗性扰动的元素。", "result": "实验使用一个为现实评估设计的框架，并利用多种对抗性攻击（包括从物理测试台实验中收集的数据集）进行测试，结果表明集成LLM分析可以显著提高基于GNN的NIDS对抗挑战的弹性。", "conclusion": "本研究展示了LLM代理作为入侵检测架构中补充层面的潜力，能够有效增强GNN在网络入侵检测中的鲁棒性和泛化能力。", "translation": "图神经网络（GNN）在网络入侵检测系统（NIDS）中，尤其是在物联网环境中，展现出巨大的潜力，但由于分布漂移和对现实对抗性攻击缺乏鲁棒性，其性能会下降。当前的鲁棒性评估通常依赖不切实际的合成扰动，并且缺乏对各种对抗性攻击（包括黑盒和白盒场景）进行系统分析的演示。这项工作提出了一种新颖的方法，通过在代理管道中采用大型语言模型（LLM）作为模拟网络安全专家代理，以增强GNN的鲁棒性和泛化能力。这些代理在GNN处理之前，审查从网络流数据中派生出的图结构，识别并可能缓解可疑或受对抗性扰动的元素。我们的实验使用一个为现实评估设计的框架，并利用各种对抗性攻击（包括从物理测试台实验中收集的数据集）进行测试，结果表明集成LLM分析可以显著提高基于GNN的NIDS对抗挑战的弹性，展示了LLM代理作为入侵检测架构中补充层面的潜力。", "summary": "本研究旨在解决图神经网络（GNN）在网络入侵检测系统（NIDS）中面对现实对抗性攻击时鲁棒性不足的问题。文章提出一种创新方法，利用大型语言模型（LLM）作为模拟网络安全专家代理，在GNN处理网络流数据前，预先分析并识别图结构中的可疑或对抗性扰动元素。实验结果表明，这种LLM与GNN的集成显著提升了NIDS的抗攻击能力，验证了LLM代理在入侵检测架构中作为补充层的有效性。", "keywords": "图神经网络, 网络入侵检测, 鲁棒性, 大型语言模型, 对抗性攻击", "comments": "这项研究的创新点在于将大型语言模型（LLM）引入到网络入侵检测的GNN鲁棒性增强中，以代理的形式模拟网络安全专家进行预处理分析。这种结合利用了LLM的强大理解和推理能力来识别潜在威胁，弥补了传统GNN在面对复杂对抗攻击时的不足，为未来的NIDS设计提供了一个有潜力的方向。"}}
{"id": "2506.20872", "title": "Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research", "authors": ["Osama Zafar", "Rosemarie Santa González", "Mina Namazi", "Alfonso Morales", "Erman Ayday"], "summary": "Data-driven agriculture, which integrates technology and data into\nagricultural practices, has the potential to improve crop yield, disease\nresilience, and long-term soil health. However, privacy concerns, such as\nadverse pricing, discrimination, and resource manipulation, deter farmers from\nsharing data, as it can be used against them. To address this barrier, we\npropose a privacy-preserving framework that enables secure data sharing and\ncollaboration for research and development while mitigating privacy risks. The\nframework combines dimensionality reduction techniques (like Principal\nComponent Analysis (PCA)) and differential privacy by introducing Laplacian\nnoise to protect sensitive information. The proposed framework allows\nresearchers to identify potential collaborators for a target farmer and train\npersonalized machine learning models either on the data of identified\ncollaborators via federated learning or directly on the aggregated\nprivacy-protected data. It also allows farmers to identify potential\ncollaborators based on similarities. We have validated this on real-life\ndatasets, demonstrating robust privacy protection against adversarial attacks\nand utility performance comparable to a centralized system. We demonstrate how\nthis framework can facilitate collaboration among farmers and help researchers\npursue broader research objectives. The adoption of the framework can empower\nresearchers and policymakers to leverage agricultural data responsibly, paving\nthe way for transformative advances in data-driven agriculture. By addressing\ncritical privacy challenges, this work supports secure data integration,\nfostering innovation and sustainability in agricultural systems.", "comment": "arXiv admin note: text overlap with arXiv:2409.06069", "pdf_url": "http://arxiv.org/pdf/2506.20872v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20872v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "赋能数字农业：一个用于数据共享和协作研究的隐私保护框架", "tldr": "该论文提出了一个隐私保护框架，结合降维和差分隐私技术，以促进农业数据的安全共享和协作研究，同时解决农民的数据隐私担忧。", "motivation": "数据驱动的农业有潜力提高作物产量、疾病抵抗力和土壤健康，但隐私问题（如不利定价、歧视和资源操纵）阻碍了农民分享数据。", "method": "该框架通过结合降维技术（如主成分分析PCA）和引入拉普拉斯噪声的差分隐私来保护敏感信息。它允许研究人员识别潜在合作者，并通过联邦学习或直接在聚合的隐私保护数据上训练个性化机器学习模型。它还允许农民根据相似性识别合作者。", "result": "该框架已在真实数据集上进行了验证，展示了对对抗性攻击的强大隐私保护，并且其效用性能与集中式系统相当。它能够促进农民之间的协作，并帮助研究人员实现更广泛的研究目标。", "conclusion": "该框架的采用可以赋能研究人员和政策制定者负责任地利用农业数据，为数据驱动农业的转型进步铺平道路。通过解决关键隐私挑战，这项工作支持安全数据集成，促进农业系统的创新和可持续性。", "translation": "数据驱动的农业将技术和数据融入农业实践中，有潜力提高作物产量、疾病抵抗力和长期土壤健康。然而，隐私问题，例如不利定价、歧视和资源操纵，阻碍了农民分享数据，因为这些数据可能被用来对付他们。为了解决这一障碍，我们提出了一个隐私保护框架，该框架能够实现安全的数据共享和研究开发协作，同时减轻隐私风险。该框架结合了降维技术（如主成分分析（PCA））和差分隐私，通过引入拉普拉斯噪声来保护敏感信息。所提出的框架允许研究人员识别目标农民的潜在合作者，并通过联邦学习或直接在聚合的隐私保护数据上训练个性化机器学习模型。它还允许农民根据相似性识别潜在合作者。我们已在真实数据集上验证了这一点，证明了对对抗性攻击的强大隐私保护和与集中式系统相当的效用性能。我们展示了该框架如何促进农民之间的协作，并帮助研究人员追求更广泛的研究目标。该框架的采用可以赋能研究人员和政策制定者负责任地利用农业数据，为数据驱动农业的转型进步铺平道路。通过解决关键隐私挑战，这项工作支持安全数据集成，促进农业系统的创新和可持续性。", "summary": "本文提出了一个用于数字农业的隐私保护框架，旨在解决农民在数据共享方面的隐私担忧。该框架结合了降维（如PCA）和差分隐私技术，以确保敏感信息的安全。它支持研究人员和农民安全地共享数据，识别合作者，并训练机器学习模型。在真实数据集上的验证表明，该框架在提供强大隐私保护的同时，保持了与集中式系统相当的性能，从而促进了农业领域的协作、创新和可持续发展。", "keywords": "数字农业, 隐私保护, 数据共享, 联邦学习, 差分隐私", "comments": "该论文通过结合降维和差分隐私技术，为数字农业中的数据共享提供了一个创新的隐私保护解决方案。其重要性在于解决了农民对数据隐私的核心担忧，这正是阻碍数据驱动农业发展的关键障碍。该框架支持联邦学习和直接在聚合数据上训练模型，具有很高的实用性。在真实数据集上的验证增强了其可靠性，为负责任地利用农业大数据铺平了道路。"}}
{"id": "2506.20801", "title": "IMA-Catcher: An IMpact-Aware Nonprehensile Catching Framework based on Combined Optimization and Learning", "authors": ["Francesco Tassi", "Jianzhuang Zhao", "Gustavo J. G. Lahr", "Luna Gava", "Marco Monforte", "Arren Glover", "Chiara Bartolozzi", "Arash Ajoudani"], "summary": "Robotic catching of flying objects typically generates high impact forces\nthat might lead to task failure and potential hardware damages. This is\naccentuated when the object mass to robot payload ratio increases, given the\nstrong inertial components characterizing this task. This paper aims to address\nthis problem by proposing an implicitly impact-aware framework that\naccomplishes the catching task in both pre- and post-catching phases. In the\nfirst phase, a motion planner generates optimal trajectories that minimize\ncatching forces, while in the second, the object's energy is dissipated\nsmoothly, minimizing bouncing. In particular, in the pre-catching phase, a\nreal-time optimal planner is responsible for generating trajectories of the\nend-effector that minimize the velocity difference between the robot and the\nobject to reduce impact forces during catching. In the post-catching phase, the\nrobot's position, velocity, and stiffness trajectories are generated based on\nhuman demonstrations when catching a series of free-falling objects with\nunknown masses. A hierarchical quadratic programming-based controller is used\nto enforce the robot's constraints (i.e., joint and torque limits) and create a\nstack of tasks that minimizes the reflected mass at the end-effector as a\nsecondary objective. The initial experiments isolate the problem along one\ndimension to accurately study the effects of each contribution on the metrics\nproposed. We show how the same task, without velocity matching, would be\ninfeasible due to excessive joint torques resulting from the impact. The\naddition of reflected mass minimization is then investigated, and the catching\nheight is increased to evaluate the method's robustness. Finally, the setup is\nextended to catching along multiple Cartesian axes, to prove its generalization\nin space.", "comment": "25 pages, 17 figures, accepted by International Journal of Robotics\n  Research (IJRR)", "pdf_url": "http://arxiv.org/pdf/2506.20801v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20801v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "IMA-Catcher：一种基于组合优化和学习的冲击感知非抓取捕获框架", "tldr": "本文提出一个冲击感知的机器人捕获框架，通过优化预捕获阶段的轨迹和学习后捕获阶段的运动来减少冲击并提高鲁棒性。", "motivation": "机器人捕获飞行物体时会产生高冲击力，可能导致任务失败和硬件损坏，尤其当物体质量与机器人有效载荷比增加时，这一问题更加突出。", "method": "该框架分为预捕获和后捕获两阶段。预捕获阶段，实时优化规划器生成最小化机器人与物体速度差的末端执行器轨迹以减少冲击力。后捕获阶段，基于人类演示生成机器人的位置、速度和刚度轨迹，以平滑耗散能量并最小化弹跳。采用分层二次规划控制器来强制执行机器人约束（关节和扭矩限制），并将末端执行器反射质量最小化作为次要目标。", "result": "初步实验在一维上隔离问题，准确研究了各贡献对指标的影响。结果表明，没有速度匹配的任务因冲击导致的关节扭矩过大而不可行。研究了反射质量最小化的附加作用，并通过增加捕获高度评估了方法的鲁棒性。最后，将设置扩展到多笛卡尔轴捕获，证明了其在空间中的泛化性。", "conclusion": "本文提出的IMA-Catcher框架通过结合优化和学习，有效解决了机器人捕获过程中高冲击力的问题，通过预捕获阶段的速度匹配和后捕获阶段的能量耗散，显著降低了冲击并提高了捕获任务的成功率和鲁棒性。", "translation": "机器人捕获飞行物体通常会产生高冲击力，可能导致任务失败和潜在的硬件损坏。当物体质量与机器人有效载荷比增加时，由于该任务的强惯性分量，这种情况会更加突出。本文旨在通过提出一种隐式冲击感知框架来解决这个问题，该框架在捕获前和捕获后阶段都能完成捕获任务。在第一阶段，运动规划器生成最小化捕获力的最优轨迹；在第二阶段，平滑地耗散物体的能量，最大限度地减少弹跳。特别是，在捕获前阶段，实时最优规划器负责生成末端执行器的轨迹，以最小化机器人和物体之间的速度差，从而在捕获过程中减少冲击力。在捕获后阶段，机器人的位置、速度和刚度轨迹是根据人类演示生成的，这些演示用于捕获一系列未知质量的自由落体。使用基于分层二次规划的控制器来强制执行机器人的约束（即关节和扭矩限制），并创建一堆任务，将末端执行器的反射质量最小化作为次要目标。初步实验在一维上隔离了问题，以准确研究每个贡献对所提出指标的影响。我们展示了相同的任务，如果没有速度匹配，将因冲击导致的关节扭矩过大而不可行。然后研究了反射质量最小化的添加，并增加了捕获高度以评估方法的鲁棒性。最后，将设置扩展到沿多个笛卡尔轴进行捕获，以证明其在空间中的泛化性。", "summary": "IMA-Catcher是一个创新的机器人捕获框架，旨在解决高冲击力问题。它结合了预捕获阶段的优化运动规划（最小化速度差以减少冲击）和后捕获阶段的基于人类演示的学习（平滑耗散能量以减少弹跳）。该框架采用分层二次规划控制器来管理机器人约束并最小化反射质量。实验证明了其在减少冲击、提高鲁棒性及泛化性方面的有效性。", "keywords": "机器人捕获, 冲击感知, 优化, 学习, 非抓取", "comments": "本文的创新之处在于其双阶段的冲击感知框架，结合了优化（预捕获）和学习（后捕获），并引入了反射质量最小化作为次要目标，有效解决了机器人捕获中的高冲击力问题。该方法提高了捕获任务的成功率和硬件安全性，具有重要的实践意义。"}}
{"id": "2506.20782", "title": "Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing", "authors": ["Marc Bara"], "summary": "We present the first theoretical framework for applying spiking neural\nnetworks (SNNs) to synthetic aperture radar (SAR) interferometric phase\nunwrapping. Despite extensive research in both domains, our comprehensive\nliterature review confirms that SNNs have never been applied to phase\nunwrapping, representing a significant gap in current methodologies. As Earth\nobservation data volumes continue to grow exponentially (with missions like\nNISAR expected to generate 100PB in two years) energy-efficient processing\nbecomes critical for sustainable data center operations. SNNs, with their\nevent-driven computation model, offer potential energy savings of 30-100x\ncompared to conventional approaches while maintaining comparable accuracy. We\ndevelop spike encoding schemes specifically designed for wrapped phase data,\npropose SNN architectures that leverage the spatial propagation nature of phase\nunwrapping, and provide theoretical analysis of computational complexity and\nconvergence properties. Our framework demonstrates how the temporal dynamics\ninherent in SNNs can naturally model the spatial continuity constraints\nfundamental to phase unwrapping. This work opens a new research direction at\nthe intersection of neuromorphic computing and SAR interferometry, offering a\ncomplementary approach to existing algorithms that could enable more\nsustainable large-scale InSAR processing.", "comment": "8 pages, 2 figures, patent pending", "pdf_url": "http://arxiv.org/pdf/2506.20782v1", "categories": ["cs.NE", "cs.ET", "cs.LG", "eess.SP", "68T07, 94A08", "I.2.6; G.1.6; B.7.1"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20782v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "脉冲神经网络用于SAR干涉相位解缠：一种节能处理的理论框架", "tldr": "首次提出将脉冲神经网络（SNNs）应用于SAR干涉相位解缠，旨在通过SNNs的事件驱动计算模型实现显著的节能处理，并提供理论框架。", "motivation": "随着地球观测数据量呈指数级增长，特别是像NISAR这样的任务预计在两年内产生100PB数据，数据中心运营的能效变得至关重要。传统的处理方法能耗高。", "method": "1. 开发了专门为缠绕相位数据设计的脉冲编码方案。2. 提出了利用相位解缠空间传播特性的SNN架构。3. 提供了计算复杂度和收敛特性的理论分析。", "result": "框架展示了SNNs固有的时间动态如何自然地模拟相位解缠中空间连续性约束。SNNs与传统方法相比，在保持相当精度的同时，可节省30-100倍的能源。", "conclusion": "这项工作在神经拟态计算和SAR干涉测量交叉领域开辟了新的研究方向，为现有算法提供了一种补充方法，有望实现更可持续的大规模InSAR处理。", "translation": "我们首次提出了将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架。尽管这两个领域都有广泛的研究，但我们全面的文献综述证实，SNNs从未应用于相位解缠，这代表了当前方法论中的一个显著空白。随着地球观测数据量持续呈指数级增长（像NISAR这样的任务预计在两年内产生100PB数据），节能处理对于可持续的数据中心运营变得至关重要。SNNs以其事件驱动的计算模型，与传统方法相比，在保持相当精度的同时，可提供30-100倍的潜在能源节约。我们开发了专门为缠绕相位数据设计的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并提供了计算复杂度和收敛特性的理论分析。我们的框架展示了SNNs固有的时间动态如何自然地模拟相位解缠中空间连续性约束。这项工作在神经拟态计算和SAR干涉测量交叉领域开辟了新的研究方向，为现有算法提供了一种补充方法，有望实现更可持续的大规模InSAR处理。", "summary": "本文首次提出了将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架。鉴于地球观测数据量的快速增长和对节能处理的需求，SNNs因其事件驱动的特性，在保持高精度的同时，有望实现显著的能效提升。作者设计了专门的脉冲编码方案和SNN架构，并进行了理论分析，证明了SNNs能够有效处理相位解缠中的空间连续性问题。这项研究为神经拟态计算和SAR干涉测量领域开辟了新途径，有助于实现更可持续的大规模InSAR处理。", "keywords": "脉冲神经网络, SAR干涉相位解缠, 节能处理, 神经拟态计算, 理论框架", "comments": "本文的创新之处在于首次将脉冲神经网络引入SAR干涉相位解缠领域，填补了现有研究空白。其重要性体现在面对海量地球观测数据时，SNNs提供的巨大节能潜力，有望解决传统方法能耗过高的问题，推动大规模InSAR处理的可持续发展。理论框架的提出为后续的实践应用奠定了基础。"}}
{"id": "2506.21490", "title": "Ad-Hoc Human-AI Coordination Challenge", "authors": ["Tin Dizdarević", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Jakob Nicolaus Foerster"], "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21490v1", "categories": ["cs.AI", "cs.HC", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21490v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "临时人机协作挑战", "tldr": "论文提出了Ad-Hoc人机协作挑战赛(AH2AC2)，通过开发人类代理AI来解决Hanabi游戏中昂贵且难以复现的人类评估问题，并发布了有限数据集和基线结果。", "motivation": "实时应用中AI代理与人类的无缝协作至关重要，但仍是一个重大挑战。Hanabi游戏是理想的测试平台，然而，由于人类评估成本高昂且难以复现，限制了其在人机交互中的应用。", "method": "引入了“临时人机协作挑战赛”(AH2AC2)以克服昂贵且难以复现的人类评估限制。开发了基于大规模人类数据集的“人类代理AI”，作为AH2AC2中鲁棒、廉价且可复现的类人评估伙伴。开源了一个包含3,079场游戏的受限数据集，旨在鼓励数据高效方法的发展。提供了一个受控评估系统来托管代理AI，以确保公平评估。", "result": "提供了两人和三人Hanabi场景的基线结果。", "conclusion": "通过引入AH2AC2挑战赛、开发人类代理AI和发布受限数据集，本文为解决人机协作中的评估难题提供了一个可扩展、可复现的解决方案，并促进了数据高效人机协作AI的开发。", "translation": "实现AI代理与人类之间的无缝协作对于实际应用至关重要，但这仍然是一个重大的开放挑战。花火（Hanabi）是一款合作纸牌游戏，其特点是信息不完善、沟通受限、需要心智理论（theory of mind）以及协调行动——这使其成为人机协作的理想试验台。然而，由于人类评估的挑战，其在人机交互中的使用受到限制。在这项工作中，我们引入了“临时人机协作挑战赛”（Ad-Hoc Human-AI Coordination Challenge, AH2AC2），以克服昂贵且难以复现的人类评估的限制。我们利用大规模人类数据集开发了“人类代理AI”，它们在AH2AC2中作为鲁棒、廉价且可复现的类人评估伙伴。为了鼓励数据高效方法的发展，我们开源了一个包含3,079场游戏的数据集，并刻意限制了可用的人类游戏数据量。我们展示了两人和三人花火场景的基线结果。为确保公平评估，我们通过一个受控评估系统来托管代理AI，而不是公开发布它们。代码可在https://github.com/FLAIROx/ah2ac2 获取。", "summary": "本文针对人机协作中昂贵且难以复现的人类评估问题，提出了Ad-Hoc人机协作挑战赛(AH2AC2)。通过在合作游戏Hanabi中开发基于大规模人类数据集的“人类代理AI”作为评估伙伴，并开源了有限数据集，旨在提供一个经济、可复现且鼓励数据高效方法的人机协作AI评估平台。论文还提供了基线结果。", "keywords": "人机协作, AI评估, 花火游戏, 代理AI, 数据集", "comments": "这篇论文通过引入人类代理AI和AH2AC2挑战赛，创造性地解决了人机协作AI评估中成本高昂和可复现性差的关键痛点。其贡献在于提供了一个标准化、可扩展且更易于进行大规模实验的评估框架。通过限制可用数据量，它还巧妙地鼓励了数据高效AI方法的研究，这对于实际应用具有重要意义。"}}
{"id": "2506.20754", "title": "Domain Knowledge in Requirements Engineering: A Systematic Mapping Study", "authors": ["Marina Araújo", "Júlia Araújo", "Romeu Oliveira", "Lucas Romao", "Marcos Kalinowski"], "summary": "[Context] Domain knowledge is recognized as a key component for the success\nof Requirements Engineering (RE), as it provides the conceptual support needed\nto understand the system context, ensure alignment with stakeholder needs, and\nreduce ambiguity in requirements specification. Despite its relevance, the\nscientific literature still lacks a systematic consolidation of how domain\nknowledge can be effectively used and operationalized in RE. [Goal] This paper\naddresses this gap by offering a comprehensive overview of existing\ncontributions, including methods, techniques, and tools to incorporate domain\nknowledge into RE practices. [Method] We conducted a systematic mapping study\nusing a hybrid search strategy that combines database searches with iterative\nbackward and forward snowballing. [Results] In total, we found 75 papers that\nmet our inclusion criteria. The analysis highlights the main types of\nrequirements addressed, the most frequently considered quality attributes, and\nrecurring challenges in the formalization, acquisition, and long-term\nmaintenance of domain knowledge. The results provide support for researchers\nand practitioners in identifying established approaches and unresolved issues.\nThe study also outlines promising directions for future research, emphasizing\nthe development of scalable, automated, and sustainable solutions to integrate\ndomain knowledge into RE processes. [Conclusion] The study contributes by\nproviding a comprehensive overview that helps to build a conceptual and\nmethodological foundation for knowledge-driven requirements engineering.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20754v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20754v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "需求工程中的领域知识：一项系统性图谱研究", "tldr": "一项系统性图谱研究综述了需求工程中领域知识的现有贡献、方法、技术和工具，并指出了未来的研究方向。", "motivation": "领域知识是需求工程（RE）成功的关键组成部分，但现有文献缺乏关于如何有效利用和操作化领域知识的系统性整合。本研究旨在通过提供现有贡献的全面概述来填补这一空白。", "method": "采用混合搜索策略进行了一项系统性图谱研究，该策略结合了数据库搜索与迭代式向前和向后滚雪球抽样。", "result": "共筛选出75篇符合条件的论文。分析揭示了所处理的需求类型、最常考虑的质量属性以及领域知识形式化、获取和长期维护中的常见挑战。研究结果为研究人员和实践者识别既定方法和未解决问题提供了支持，并为未来研究指明了方向，强调开发可扩展、自动化和可持续的解决方案。", "conclusion": "本研究通过提供全面的概述，有助于为知识驱动的需求工程建立概念和方法论基础。", "translation": "[背景] 领域知识被认为是需求工程（RE）成功的关键组成部分，因为它提供了理解系统背景、确保与利益相关者需求一致性以及减少需求规范模糊性所需的概念支持。尽管其具有相关性，但科学文献仍然缺乏关于如何在需求工程中有效使用和操作化领域知识的系统性整合。[目标] 本文旨在通过全面概述现有贡献，包括将领域知识纳入需求工程实践的方法、技术和工具来弥补这一空白。[方法] 我们采用混合搜索策略进行了一项系统性图谱研究，该策略结合了数据库搜索与迭代式向后和向前滚雪球抽样。[结果] 我们总共找到了75篇符合纳入标准的论文。分析突出了所处理的主要需求类型、最常考虑的质量属性，以及领域知识形式化、获取和长期维护中反复出现的挑战。研究结果为研究人员和实践者识别既定方法和未解决问题提供了支持。该研究还概述了未来研究的有前景的方向，强调开发可扩展、自动化和可持续的解决方案，以将领域知识整合到需求工程流程中。[结论] 该研究通过提供全面的概述做出了贡献，有助于为知识驱动的需求工程建立概念和方法论基础。", "summary": "本系统性图谱研究全面概述了需求工程（RE）中领域知识的现有贡献，包括将其纳入RE实践的方法、技术和工具。研究通过混合搜索策略筛选了75篇论文，分析了所处理的需求类型、质量属性和领域知识形式化、获取及维护的挑战。研究结果为研究人员和实践者识别现有方法和未解决问题提供了依据，并指明了未来研究方向，旨在开发可扩展、自动化和可持续的领域知识整合方案，从而为知识驱动的需求工程奠定基础。", "keywords": "领域知识, 需求工程, 系统性图谱研究, 知识整合, 研究挑战", "comments": "这项研究的重要性在于它系统性地梳理了需求工程领域中领域知识的应用现状，填补了现有文献的空白。通过系统性图谱研究的方法，为研究人员和实践者提供了宝贵的参考，有助于理解该领域的现有成就、挑战和未来发展方向。其创新性体现在对现有研究的整合和对未来研究方向的明确指引，尤其强调了自动化和可持续解决方案的重要性。"}}
{"id": "2506.20748", "title": "Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots", "authors": ["Jingshu Li", "Zicheng Zhu", "Renwen Zhang", "Yi-Chieh Lee"], "summary": "Chatbots are increasingly integrated into people's lives and are widely used\nto help people. Recently, there has also been growing interest in the reverse\ndirection-humans help chatbots-due to a wide range of benefits including better\nchatbot performance, human well-being, and collaborative outcomes. However,\nlittle research has explored the factors that motivate people to help chatbots.\nTo address this gap, we draw on the Computers Are Social Actors (CASA)\nframework to examine how chatbot anthropomorphism-including human-like\nidentity, emotional expression, and non-verbal expression-influences human\nempathy toward chatbots and their subsequent prosocial behaviors and\nintentions. We also explore people's own interpretations of their prosocial\nbehaviors toward chatbots. We conducted an online experiment (N = 244) in which\nchatbots made mistakes in a collaborative image labeling task and explained the\nreasons to participants. We then measured participants' prosocial behaviors and\nintentions toward the chatbots. Our findings revealed that human identity and\nemotional expression of chatbots increased participants' prosocial behavior and\nintention toward chatbots, with empathy mediating these effects. Qualitative\nanalysis further identified two motivations for participants' prosocial\nbehaviors: empathy for the chatbot and perceiving the chatbot as human-like. We\ndiscuss the implications of these results for understanding and promoting human\nprosocial behaviors toward chatbots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20748v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20748v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "探索聊天机器人拟人化和人类同理心对人类对聊天机器人亲社会行为的影响", "tldr": "研究发现，聊天机器人的拟人化（如人类身份和情感表达）能通过增强人类同理心来促进人类对聊天机器人的亲社会行为。", "motivation": "现有研究很少探索激励人类帮助聊天机器人的因素。鉴于人类帮助聊天机器人有助于提升其性能、人类福祉和协作成果，本研究旨在填补这一空白。", "method": "基于“计算机是社会行动者（CASA）”框架，通过一项在线实验（N=244）进行。实验中，聊天机器人在图像标注任务中犯错并解释原因，随后测量参与者对聊天机器人的亲社会行为和意图。同时，定性分析了参与者亲社会行为的动机。", "result": "聊天机器人的人类身份和情感表达增加了参与者对其的亲社会行为和意图，且同理心在其中起中介作用。定性分析发现，参与者亲社会行为的动机包括对聊天机器人的同理心和将其视为类人。", "conclusion": "这些结果有助于理解和促进人类对聊天机器人的亲社会行为。", "translation": "聊天机器人越来越多地融入人们的生活，并被广泛用于帮助人们。最近，由于其在提升聊天机器人性能、人类福祉和协作成果等方面的广泛益处，人们对反向——即人类帮助聊天机器人——的兴趣也日益增长。然而，很少有研究探讨促使人们帮助聊天机器人的因素。为了弥补这一空白，我们借鉴“计算机是社会行动者（CASA）”框架，研究聊天机器人拟人化——包括类人身份、情感表达和非语言表达——如何影响人类对聊天机器人的同理心以及随后的亲社会行为和意图。我们还探讨了人们对他们对聊天机器人亲社会行为的自身解释。我们进行了一项在线实验（N = 244），其中聊天机器人在协作图像标注任务中犯了错误并向参与者解释了原因。然后，我们测量了参与者对聊天机器人的亲社会行为和意图。我们的发现表明，聊天机器人的人类身份和情感表达增加了参与者对聊天机器人的亲社会行为和意图，其中同理心在这些影响中起中介作用。定性分析进一步确定了参与者亲社会行为的两种动机：对聊天机器人的同理心和将聊天机器人视为类人。我们讨论了这些结果对于理解和促进人类对聊天机器人亲社会行为的启示。", "summary": "本研究探讨了聊天机器人拟人化（包括类人身份和情感表达）如何通过影响人类对聊天机器人的同理心，进而促进人类对其的亲社会行为和意图。通过在线实验（N=244）和定性分析，发现聊天机器人的人类身份和情感表达能增强人类的亲社会行为，且同理心是关键中介因素。研究还揭示了亲社会行为源于对机器人的同理心和将其视为类人。", "keywords": "聊天机器人拟人化, 人类同理心, 亲社会行为, 人机交互, CASA框架", "comments": "这篇论文通过实证研究填补了人类帮助聊天机器人这一新兴领域的研究空白，尤其强调了拟人化和同理心的重要性。其发现对于设计更具协作性、能激发用户积极互动的聊天机器人具有重要指导意义，有助于提升人机协作效率和用户体验。"}}
{"id": "2506.21073", "title": "Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G Networks", "authors": ["Ilias Papalamprou", "Nikolaos Fotos", "Nikolaos Chatzivasileiadis", "Anna Angelogianni", "Dimosthenis Masouros", "Dimitrios Soudris"], "summary": "The advent of 5G and beyond has brought increased performance networks,\nfacilitating the deployment of services closer to the user. To meet performance\nrequirements such services require specialized hardware, such as Field\nProgrammable Gate Arrays (FPGAs). However, FPGAs are often deployed in\nunprotected environments, leaving the user's applications vulnerable to\nmultiple attacks. With the rise of quantum computing, which threatens the\nintegrity of widely-used cryptographic algorithms, the need for a robust\nsecurity infrastructure is even more crucial. In this paper we introduce a\nhybrid hardware-software solution utilizing remote attestation to securely\nconfigure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms\nfor enhanced security. Additionally, to enable trustworthiness across the whole\nedge computing continuum, our solution integrates a blockchain infrastructure,\nensuring the secure storage of any security evidence. We evaluate the proposed\nsecure configuration process under different PQC algorithms in two FPGA\nfamilies, showcasing only 2% overheard compared to the non PQC approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21073v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21073v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "B5G网络中可信FPGA的后量子和基于区块链的认证", "tldr": "本文提出了一种混合硬件-软件解决方案，利用远程认证、后量子密码算法和区块链基础设施，在B5G网络中安全配置FPGA。", "motivation": "5G及未来网络的发展使得服务更接近用户，需要FPGA等专用硬件。然而，FPGA通常部署在非受保护环境中，容易受到攻击。量子计算的兴起对现有加密算法构成威胁，使得对强大安全基础设施的需求更加迫切。", "method": "本文提出了一种混合硬件-软件解决方案，利用远程认证安全配置FPGA，并集成了后量子密码（PQC）算法以增强安全性。此外，为了在整个边缘计算连续体中实现可信度，该解决方案集成了区块链基础设施，以确保任何安全证据的安全存储。", "result": "在两种FPGA系列中，使用不同的PQC算法评估了所提出的安全配置过程，结果显示与非PQC方法相比，开销仅为2%。", "conclusion": "本文提出的混合硬件-软件解决方案结合了远程认证、后量子密码和区块链技术，能够为B5G网络中的可信FPGA提供高效且强大的安全配置，有效应对现有和未来的安全威胁。", "translation": "5G及未来网络的出现带来了性能提升的网络，促进了服务更接近用户的部署。为了满足性能要求，此类服务需要专用硬件，例如现场可编程门阵列（FPGA）。然而，FPGA通常部署在非受保护环境中，使用户的应用程序容易受到多种攻击。随着量子计算的兴起，它威胁到广泛使用的加密算法的完整性，对强大的安全基础设施的需求变得更加关键。在本文中，我们引入了一种混合硬件-软件解决方案，利用远程认证安全配置FPGA，同时集成了后量子密码（PQC）算法以增强安全性。此外，为了在整个边缘计算连续体中实现可信度，我们的解决方案集成了区块链基础设施，确保任何安全证据的安全存储。我们在两种FPGA系列中，使用不同的PQC算法评估了所提出的安全配置过程，与非PQC方法相比，开销仅为2%。", "summary": "本文针对B5G网络中FPGA在非受保护环境下易受攻击以及量子计算对现有加密算法的威胁，提出了一种混合硬件-软件安全配置方案。该方案结合了远程认证、后量子密码算法和区块链技术，旨在安全配置FPGA并确保安全证据的存储。实验结果表明，与非PQC方法相比，该方案的开销仅为2%。", "keywords": "FPGA, 后量子密码, 区块链, 远程认证, B5G网络", "comments": "该论文创新性地将后量子密码与区块链技术结合应用于FPGA的远程认证和安全配置，有效应对了未来量子计算带来的安全挑战，并提高了边缘计算环境中FPGA的信任度。其低开销的评估结果也显示了该方案的实用潜力。"}}
{"id": "2506.20813", "title": "Entropic additive energy and entropy inequalities for sums and products", "authors": ["Rupert Li", "Lampros Gavalakis", "Ioannis Kontoyiannis"], "summary": "Following a growing number of studies that, over the past 15 years, have\nestablished entropy inequalities via ideas and tools from additive\ncombinatorics, in this work we obtain a number of new bounds for the\ndifferential entropy of sums, products, and sum-product combinations of\ncontinuous random variables. Partly motivated by recent work by Goh on the\ndiscrete entropic version of the notion of \"additive energy\", we introduce the\nadditive energy of pairs of continuous random variables and prove various\nversions of the statement that \"the additive energy is large if and only if the\nentropy of the sum is small\", along with a version of the\nBalog-Szemer\\'edi-Gowers theorem for differential entropy. Then, motivated in\npart by recent work by M\\'ath\\'e and O'Regan, we establish a series of new\ndifferential entropy inequalities for products and sum-product combinations of\ncontinuous random variables. In particular, we prove a new, general, ring\nPl\\\"unnecke-Ruzsa entropy inequality. We briefly return to the case of discrete\nentropy and provide a characterization of discrete random variables with \"large\ndoubling\", analogous to Tao's Freiman-type inverse sumset theory for the case\nof small doubling. Finally, we consider the natural entropic analog of the\nErd\\\"os-Szemer\\'edi sum-product phenomenon for integer-valued random variables.\nWe show that, if it does hold, then the range of parameters for which it does\nwould necessarily be significantly more restricted than its anticipated\ncombinatorial counterpart.", "comment": "26 pages, no figures", "pdf_url": "http://arxiv.org/pdf/2506.20813v1", "categories": ["cs.IT", "math.CO", "math.IT", "94A17 (Primary) 11B13 (Secondary)"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.20813v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "熵加性能量与和积的熵不等式", "tldr": "本文利用加性组合学的思想和工具，为连续随机变量的和、积及和积组合的微分熵获得了许多新的界限，引入了连续随机变量对的加性能量，并证明了相关定理和熵不等式，包括环Plünnecke-Ruzsa熵不等式，并探讨了离散熵和Erdős-Szemerédi和积现象的熵模拟。", "motivation": "过去15年，越来越多的研究通过加性组合学的思想和工具建立了熵不等式。本文部分受Goh关于“加性能量”离散熵版本近期工作的启发，以及部分受Máthé和O'Regan近期工作的启发。", "method": "引入连续随机变量对的加性能量，并证明了“加性能量大当且仅当和的熵小”的各种版本，以及微分熵的Balog-Szemerédi-Gowers定理版本。建立了连续随机变量的积和和积组合的一系列新的微分熵不等式，特别是证明了一个新的、通用的环Plünnecke-Ruzsa熵不等式。提供了具有“大倍增”的离散随机变量的表征。考虑了整数值随机变量的Erdős-Szemerédi和积现象的自然熵模拟。", "result": "获得了连续随机变量的和、积及和积组合的微分熵的许多新界限。证明了“加性能量大当且仅当和的熵小”的各种版本以及微分熵的Balog-Szemerédi-Gowers定理版本。建立了一系列新的微分熵不等式，包括新的通用环Plünnecke-Ruzsa熵不等式。提供了具有“大倍增”的离散随机变量的表征。对于Erdős-Szemerédi和积现象，结果表明如果它成立，则其参数范围将比预期的组合对应物受到更显著的限制。", "conclusion": "本文为连续随机变量的和、积及和积组合提供了新的微分熵界限和不等式，引入了熵加性能量的概念，并探讨了离散熵和大倍增现象。此外，研究了Erdős-Szemerédi和积现象的熵模拟，并指出其参数范围可能比组合版本更受限制。", "translation": "在过去15年中，越来越多的研究通过加性组合学的思想和工具建立了熵不等式，本文在此基础上，为连续随机变量的和、积及和积组合的微分熵获得了许多新的界限。部分受Goh关于“加性能量”离散熵版本近期工作的启发，我们引入了连续随机变量对的加性能量，并证明了“加性能量大当且仅当和的熵小”的各种版本，以及微分熵的Balog-Szemerédi-Gowers定理版本。然后，部分受Máthé和O'Regan近期工作的启发，我们建立了一系列新的连续随机变量的积和和积组合的微分熵不等式。特别是，我们证明了一个新的、通用的环Plünnecke-Ruzsa熵不等式。我们简要地回到离散熵的情况，并提供了一个具有“大倍增”的离散随机变量的表征，类似于陶哲轩关于小倍增的Freiman型逆和集理论。最后，我们考虑了整数值随机变量的Erdős-Szemerédi和积现象的自然熵模拟。我们表明，如果它确实成立，那么其成立的参数范围必然会比其预期的组合对应物受到更显著的限制。", "summary": "本文利用加性组合学的工具和思想，为连续随机变量的和、积及和积组合提供了大量新的微分熵界限。研究引入了连续随机变量对的加性能量，并证明了其与和的熵之间的关系，以及微分熵的Balog-Szemerédi-Gowers定理版本。此外，论文还建立了一系列新的微分熵不等式，特别是提出了一个通用的环Plünnecke-Ruzsa熵不等式。在离散熵方面，本文表征了具有“大倍增”的离散随机变量。最后，论文探讨了Erdős-Szemerédi和积现象的熵模拟，并指出其适用参数范围可能比组合版本更为受限。", "keywords": "熵加性能量, 熵不等式, 和积, 加性组合学, 微分熵", "comments": "本文的创新之处在于将加性组合学的思想和工具应用于微分熵领域，提出了“熵加性能量”这一新概念，并建立了其与和的熵之间的重要关系。论文还推导出了包括环Plünnecke-Ruzsa熵不等式在内的多项新的熵不等式，扩展了该领域的理论框架。此外，对离散熵和大倍增现象的探讨以及对Erdős-Szemerédi和积现象熵模拟的分析，都展现了其在概率论和组合学交叉领域的重要贡献。"}}
{"id": "2506.20679", "title": "Establishing validated standards for Home and Work location Detection", "authors": ["Silvia de Sojo", "Lorenzo Lucchini", "Ollin D. Langle-Chimal", "Samuel P. Fraiberger", "Laura Alessandretti"], "summary": "Smartphone location data have transformed urban mobility research, providing\nunprecedented insights into how people navigate and interact in cities.\nHowever, leveraging location data at scale presents methodological challenges.\nAccurately identifying individuals' home and work locations is critical for a\nrange of applications, including commuting analysis, unemployment estimation,\nand urban accessibility studies. Despite their widespread use, home-work\ndetection methods lack a standardized framework that accounts for differing\ndata quality and that is validated against ground-truth observations. This\nlimits the comparability and reproducibility of results across studies and\ndatasets. In this paper, we present HoWDe, a robust algorithm for identifying\nhome and work locations from mobility data, explicitly designed to handle\nmissing data and varying data quality across individuals. Using two unique\nground-truth datasets comprising over 5100 individuals from more than 80\ncountries, HoWDe achieves home and work detection accuracies of up to 97% and\n88%, respectively, with consistent performance across countries and demographic\ngroups. We examine how parameter choices shape the trade-off between accuracy\nand user retention, and demonstrate how these methodological decisions\ninfluence downstream applications such as employment estimation and commuting\npattern analysis. By supporting in-house pre-processing through a transparent\nand validated pipeline, HoWDe also facilitates the sharing of\nprivacy-preserving mobility data. Together, our tools and findings establish\nmethodological standards that support more robust, scalable, and reproducible\nmobility research at both individual and urban scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20679v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20679v1", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "建立家庭和工作地点检测的验证标准", "tldr": "HoWDe是一个鲁棒的算法，用于从移动数据中识别家庭和工作地点，解决了现有方法缺乏标准化框架和验证的问题，并实现了高精度检测。", "motivation": "智能手机位置数据在城市流动性研究中带来了巨大变革，但大规模利用位置数据存在方法论挑战。准确识别个人家庭和工作地点对于通勤分析、失业估计和城市可达性研究至关重要。尽管广泛使用，但现有的家庭-工作地点检测方法缺乏一个考虑不同数据质量并经过真实数据验证的标准化框架，这限制了研究结果的可比性和可重复性。", "method": "本文提出了HoWDe，一个从移动数据中识别家庭和工作地点的鲁棒算法，该算法明确设计用于处理缺失数据和不同个体间的数据质量差异。研究使用了两个独特的真实数据集，包含来自80多个国家的5100多名个体。", "result": "HoWDe在家庭和工作地点检测中分别达到了高达97%和88%的准确率，并且在不同国家和人口群体中表现一致。研究还探讨了参数选择如何影响准确性和用户留存之间的权衡，并展示了这些方法论决策如何影响下游应用，如就业估计和通勤模式分析。", "conclusion": "HoWDe通过透明和经过验证的流程支持内部预处理，促进了隐私保护移动数据的共享。本工具和研究结果建立了方法论标准，支持在个体和城市尺度上进行更鲁健、可扩展和可重复的移动研究。", "translation": "智能手机位置数据彻底改变了城市流动性研究，提供了前所未有的洞察力，揭示了人们如何在城市中导航和互动。然而，大规模利用位置数据带来了方法学挑战。准确识别个人的家庭和工作地点对于一系列应用至关重要，包括通勤分析、失业估计和城市可达性研究。尽管广泛使用，但家庭-工作地点检测方法缺乏一个考虑不同数据质量并经过真实观察验证的标准化框架。这限制了研究结果在不同研究和数据集之间的可比性和可重复性。在本文中，我们提出了HoWDe，一种从移动数据中识别家庭和工作地点的鲁棒算法，它明确设计用于处理缺失数据和不同个体间的数据质量差异。利用两个独特的真实数据集，包含来自80多个国家的5100多名个体，HoWDe在家庭和工作地点检测中分别达到了高达97%和88%的准确率，并且在不同国家和人口群体中表现一致。我们研究了参数选择如何影响准确性和用户留存之间的权衡，并展示了这些方法论决策如何影响下游应用，如就业估计和通勤模式分析。通过透明和经过验证的流程支持内部预处理，HoWDe还促进了隐私保护移动数据的共享。总的来说，我们的工具和研究结果建立了方法论标准，支持在个体和城市尺度上进行更鲁健、可扩展和可重复的移动研究。", "summary": "本研究提出了一种名为HoWDe的鲁棒算法，用于从智能手机移动数据中准确识别个人家庭和工作地点。该算法旨在解决现有方法缺乏标准化框架和对数据质量差异处理能力的问题。HoWDe利用两个包含5100多名个体的大规模真实数据集进行验证，在家庭和工作地点检测中分别实现了高达97%和88%的准确率，并展示了在不同国家和人口群体间的良好一致性。该研究还探讨了参数选择对准确性和用户留存的影响，并强调了这些方法论决策对下游应用的重要性，如就业估计和通勤模式分析。HoWDe通过提供透明且经过验证的预处理流程，有助于建立移动研究的方法论标准，从而支持更可靠、可扩展和可重复的流动性研究。", "keywords": "家庭地点检测, 工作地点检测, 移动数据, HoWDe, 位置数据分析", "comments": "本论文的创新之处在于提出了HoWDe算法，该算法专门设计用于处理移动数据中的缺失值和数据质量差异，并首次通过大规模的真实数据集（来自80多个国家的5100多人）进行了广泛验证。这解决了现有家庭-工作地点检测方法缺乏标准化和验证的痛点。其重要性体现在为城市流动性研究提供了更鲁棒、可扩展和可重复的方法学基础，特别是对于通勤分析、失业估计和城市可达性研究等关键应用。通过支持隐私保护数据共享，也促进了数据的更广泛利用。"}}
{"id": "2506.20762", "title": "Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks", "authors": ["Shisheng Hu", "Jie Gao", "Xue Qin", "Conghao Zhou", "Xinyu Huang", "Mushu Li", "Mingcheng He", "Xuemin Shen"], "summary": "In this paper, we propose a novel drift-adaptive slicing-based resource\nmanagement scheme for cooperative integrated sensing and communication (ISAC)\nnetworks. Particularly, we establish two network slices to provide sensing and\ncommunication services, respectively. In the large-timescale planning for the\nslices, we partition the sensing region of interest (RoI) of each mobile device\nand reserve network resources accordingly, facilitating low-complexity\ndistance-based sensing target assignment in small timescales. To cope with the\nnon-stationary spatial distributions of mobile devices and sensing targets,\nwhich can result in the drift in modeling the distributions and ineffective\nplanning decisions, we construct digital twins (DTs) of the slices. In each DT,\na drift-adaptive statistical model and an emulation function are developed for\nthe spatial distributions in the corresponding slice, which facilitates\nclosed-form decision-making and efficient validation of a planning decision,\nrespectively. Numerical results show that the proposed drift-adaptive\nslicing-based resource management scheme can increase the service satisfaction\nratio by up to 18% and reduce resource consumption by up to 13.1% when compared\nwith benchmark schemes.", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking", "pdf_url": "http://arxiv.org/pdf/2506.20762v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.20762v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "协同ISAC网络中基于漂移自适应切片资源管理", "tldr": "本文提出了一种针对协同ISAC网络的漂移自适应切片资源管理方案，利用数字孪生应对移动设备和传感目标的非平稳空间分布，显著提高了服务满意度并降低了资源消耗。", "motivation": "合作式ISAC网络中移动设备和传感目标的非平稳空间分布会导致建模漂移和规划决策无效。", "method": "本文提出了一种新颖的漂移自适应切片资源管理方案。该方案建立了两个网络切片分别提供传感和通信服务。在大时间尺度规划中，划分每个移动设备的传感感兴趣区域（RoI）并相应地预留网络资源，以便在小时间尺度上进行低复杂度的基于距离的传感目标分配。为应对移动设备和传感目标的非平稳空间分布，构建了切片的数字孪生（DTs）。每个DT中开发了针对相应切片空间分布的漂移自适应统计模型和仿真函数，分别促进了闭式决策和规划决策的有效验证。", "result": "与基准方案相比，所提出的漂移自适应切片资源管理方案可将服务满意度提高多达18%，并将资源消耗降低多达13.1%。", "conclusion": "所提出的基于漂移自适应切片的资源管理方案能够有效应对合作式ISAC网络中非平稳分布的挑战，显著提升了系统性能。", "translation": "在本文中，我们提出了一种新颖的漂移自适应切片式资源管理方案，用于协同集成传感与通信（ISAC）网络。具体而言，我们建立了两个网络切片，分别提供传感和通信服务。在切片的大时间尺度规划中，我们划分了每个移动设备的传感感兴趣区域（RoI），并相应地预留了网络资源，从而促进了小时间尺度上低复杂度的基于距离的传感目标分配。为了应对移动设备和传感目标的非平稳空间分布（这可能导致建模分布的漂移和无效的规划决策），我们构建了切片的数字孪生（DTs）。在每个DT中，针对相应切片中的空间分布开发了漂移自适应统计模型和仿真函数，分别促进了闭式决策和规划决策的有效验证。数值结果表明，与基准方案相比，所提出的漂移自适应切片式资源管理方案可以将服务满意度提高多达18%，并将资源消耗降低多达13.1%。", "summary": "本文提出了一种用于协同ISAC网络的漂移自适应切片资源管理方案。该方案设置了传感和通信两个网络切片，并通过大时间尺度规划进行资源预留。为应对移动设备和传感目标的非平稳空间分布，该方案为每个切片构建了数字孪生，其中包含漂移自适应统计模型和仿真函数，从而实现了闭式决策和高效验证。数值结果表明，该方案能显著提高服务满意度并降低资源消耗。", "keywords": "漂移自适应, 切片, 资源管理, ISAC, 数字孪生", "comments": "该论文的创新点在于将数字孪生与漂移自适应模型相结合，以有效管理ISAC网络中动态且非平稳的空间分布，这对于实际部署至关重要。切片技术与数字孪生的结合为资源管理提供了鲁棒的解决方案，具有重要的实际应用价值。"}}
{"id": "2506.20741", "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20741v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20741v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "OTSurv：一种用于生存预测的异质性感知最优传输多实例学习框架", "tldr": "OTSurv是一个新颖的多实例学习框架，它利用最优传输理论，通过引入异质性感知约束，显著提高了全玻片图像（WSI）的生存预测性能，并取得了最先进的结果。", "motivation": "现有的多实例学习（MIL）方法在利用全玻片图像（WSI）进行生存预测时，未能有效捕获病理异质性，包括全局的长尾形态分布和局部的瓦片级预测不确定性。", "method": "本文提出了OTSurv，一个基于最优传输（OT）的多实例学习框架。OTSurv将生存预测公式化为一个异质性感知的OT问题，并引入了两个关键约束：1）全局长尾约束，通过调节传输质量分配来建模先验形态分布，以避免模式崩溃和过度均匀性；2）局部不确定性感知约束，通过逐步提高总传输质量来优先处理高置信度补丁并抑制噪声。该问题被重铸为非平衡OT形式，并可以通过高效的、硬件友好的矩阵缩放算法求解。", "result": "OTSurv在六个流行的基准测试中取得了新的最先进结果，平均C指数绝对提高了3.6%。此外，OTSurv在对数秩检验中达到了统计显著性，并提供了高可解释性。", "conclusion": "OTSurv是一种强大且具有高可解释性的数字病理学生存预测工具。", "translation": "使用全玻片图像（WSIs）进行生存预测可以被视为一个多实例学习（MIL）问题。然而，现有的MIL方法往往未能明确捕获WSIs内部的病理异质性，无论是通过长尾形态分布体现的全局异质性，还是通过瓦片级预测不确定性体现的局部异质性。最优传输（OT）通过纳入边缘分布约束，提供了一种建模这种异质性的原则性方法。基于这一洞察，我们提出了OTSurv，一个从最优传输角度出发的新颖MIL框架。具体来说，OTSurv将生存预测公式化为一个异质性感知的OT问题，并带有两个约束：(1) 全局长尾约束，通过调节传输质量分配来建模先验形态分布，以避免模式崩溃和过度均匀性；(2) 局部不确定性感知约束，通过逐步提高总传输质量来优先处理高置信度补丁，同时抑制噪声。然后，我们将初始的OT问题（通过这些约束增强）重铸为一种非平衡OT形式，该形式可以通过高效的、硬件友好的矩阵缩放算法求解。经验上，OTSurv在六个流行基准测试中均取得了新的最先进结果，平均C指数绝对提高了3.6%。此外，OTSurv在对数秩检验中达到了统计显著性，并提供了高可解释性，使其成为数字病理学中生存预测的强大工具。我们的代码可在https://github.com/Y-Research-SBU/OTSurv获取。", "summary": "本文提出了OTSurv，一个新颖的多实例学习框架，旨在解决全玻片图像（WSI）生存预测中现有方法未能有效捕获病理异质性的问题。OTSurv将生存预测建模为一个异质性感知最优传输问题，通过引入全局长尾约束来处理形态分布，并通过局部不确定性感知约束来抑制噪声并优先处理高置信度补丁。该框架被转化为非平衡最优传输形式，并通过高效算法求解。实验结果表明，OTSurv在多个基准测试中取得了最先进的性能，并具有高可解释性，显示出其在数字病理学领域的应用潜力。", "keywords": "生存预测, 多实例学习, 最优传输, 病理异质性, 全玻片图像", "comments": "该论文的创新点在于将最优传输理论应用于多实例学习框架，以明确处理全玻片图像中的病理异质性，这对于生存预测至关重要。通过引入全局长尾和局部不确定性感知约束，OTSurv能够更鲁棒地捕捉复杂的病理特征。其在多个基准测试中取得的SOTA结果以及高可解释性，显示了其在数字病理学领域的巨大潜力。"}}
{"id": "2506.20673", "title": "ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data", "authors": ["Yongqian Sun", "Xijie Pan", "Xiao Xiong", "Lei Tao", "Jiaju Wang", "Shenglin Zhang", "Yuan Yuan", "Yuqi Li", "Kunlin Jian"], "summary": "Network failure diagnosis is challenging yet critical for high-performance\ncomputing (HPC) systems. Existing methods cannot be directly applied to HPC\nscenarios due to data heterogeneity and lack of accuracy. This paper proposes a\nnovel framework, called ClusterRCA, to localize culprit nodes and determine\nfailure types by leveraging multimodal data. ClusterRCA extracts features from\ntopologically connected network interface controller (NIC) pairs to analyze the\ndiverse, multimodal data in HPC systems. To accurately localize culprit nodes\nand determine failure types, ClusterRCA combines classifier-based and\ngraph-based approaches. A failure graph is constructed based on the output of\nthe state classifier, and then it performs a customized random walk on the\ngraph to localize the root cause. Experiments on datasets collected by a\ntop-tier global HPC device vendor show ClusterRCA achieves high accuracy in\ndiagnosing network failure for HPC systems. ClusterRCA also maintains robust\nperformance across different application scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20673v1", "categories": ["cs.DC", "cs.AI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20673v1", "date": "2025-06-17", "updated": "2025-06-17", "AI": {"title_translation": "ClusterRCA：使用多模态数据进行HPC系统网络故障诊断", "tldr": "ClusterRCA是一个用于HPC系统网络故障诊断的新框架，它利用多模态数据，结合分类器和图方法来定位故障节点并确定故障类型，实验证明其准确性高且性能稳健。", "motivation": "网络故障诊断对高性能计算（HPC）系统至关重要，但现有方法因数据异构性和准确性不足而无法直接应用于HPC场景。", "method": "本文提出了ClusterRCA框架，通过从拓扑连接的网络接口控制器（NIC）对中提取特征来分析多模态数据。它结合了基于分类器和基于图的方法：首先，根据状态分类器的输出构建故障图，然后在该图上执行定制的随机游走以定位根本原因。", "result": "在顶级全球HPC设备供应商收集的数据集上的实验表明，ClusterRCA在诊断HPC系统网络故障方面取得了高准确性，并且在不同的应用场景中保持了稳健的性能。", "conclusion": "ClusterRCA是一个有效且准确的HPC系统网络故障诊断框架，能够利用多模态数据并结合多种方法来定位故障节点和确定故障类型，具有良好的鲁棒性。", "translation": "网络故障诊断对于高性能计算（HPC）系统来说既具有挑战性又至关重要。由于数据异构性和准确性不足，现有方法无法直接应用于HPC场景。本文提出了一个名为ClusterRCA的新颖框架，通过利用多模态数据来定位罪魁祸首节点并确定故障类型。ClusterRCA从拓扑连接的网络接口控制器（NIC）对中提取特征，以分析HPC系统中多样化的多模态数据。为了准确地定位罪魁祸首节点并确定故障类型，ClusterRCA结合了基于分类器和基于图的方法。基于状态分类器的输出构建故障图，然后它在图上执行定制的随机游走以定位根本原因。在由一家顶级全球HPC设备供应商收集的数据集上进行的实验表明，ClusterRCA在诊断HPC系统网络故障方面取得了高准确性。ClusterRCA还在不同的应用场景中保持了稳健的性能。", "summary": "ClusterRCA是一个针对高性能计算（HPC）系统网络故障诊断的创新框架。它解决了现有方法在HPC场景中因数据异构性和准确性不足而面临的挑战。该框架通过分析来自拓扑连接网络接口控制器（NIC）对的多模态数据来提取特征，并结合基于分类器和基于图的方法来精确识别故障节点和确定故障类型。具体而言，它构建故障图并执行定制的随机游走以定位根本原因。实验结果表明，ClusterRCA在HPC系统网络故障诊断中表现出高准确性和跨不同应用场景的稳健性能。", "keywords": "HPC系统, 网络故障诊断, 多模态数据, 根因分析, 图算法", "comments": "ClusterRCA的创新之处在于其结合了多模态数据分析与分类器和图方法的混合诊断策略，这对于处理HPC系统复杂异构的网络数据非常有效。其定制的随机游走算法在故障图上的应用，为根因定位提供了一种新颖且可能更精确的途径。该研究的重要性体现在它为HPC系统这一关键基础设施提供了更高效、准确的故障诊断解决方案，有助于提升系统稳定性和可用性。"}}
{"id": "2506.20982", "title": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers", "authors": ["Martin Ruskov"], "summary": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings.", "comment": "accepted at D-SAIL Workshop - Transformative Curriculum Design:\n  Digitalization, Sustainability, and AI Literacy for 21st Century Learning", "pdf_url": "http://arxiv.org/pdf/2506.20982v1", "categories": ["cs.CY", "cs.RO", "K.3.1"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20982v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "我们的编程冒险：使用大型语言模型为学龄前儿童的实体编程机器人定制叙事", "tldr": "本文提出一种使用大型语言模型（LLMs）为学龄前儿童的实体编程机器人Cubetto生成个性化故事的方法，以帮助教师创建教学辅助材料，同时避免儿童直接接触LLMs。", "motivation": "在教育领域平衡使用大型语言模型（LLMs）面临挑战，尤其对于理解能力有限且易受屏幕时间影响的幼儿。传统编程教学可能不适合学龄前儿童，需要一种个性化且引人入胜的方式来帮助他们熟悉机器人指令。", "method": "研究人员采用行动研究方法，开发了一种快速原型化Cubetto游戏故事的流程。该方法利用LLMs生成个性化故事，以帮助学龄前儿童习惯指挥机器人。研究使用了开放权重模型，并在5种不同的LLMs上进行了测试，确保了结果的可复现性和模型无关性。过程中记录了所用材料、提示以及学习体验和结果。儿童不直接接触LLMs，技术仅用于帮助教师开发个性化叙事。", "result": "研究发现，生成的故事对于作为教师辅助工具是成功的。然而，在测试模型的4种不同任务场景中，研究人员遇到了模型一致性和幻觉问题，并记录了相应的评估过程以及克服这些问题的尝试（有些成功，有些不成功）。", "conclusion": "研究人员认为所提出的方法适用于学龄前课堂，并计划在真实的教育环境中进一步实验。该方法能有效帮助教师为学龄前儿童创建个性化编程教学叙事。", "translation": "在教育领域中，找到平衡使用大型语言模型（LLMs）的方法是一个挑战，因为存在对技术理解不足和受众易受影响的固有风险。对于年幼的儿童来说尤其如此，他们已知在普遍的屏幕时间方面存在困难。我们与一款名为Cubetto的实体编程机器人合作，提出了一种方法，通过在个性化故事准备中运用LLMs的能力来受益于这些模型，这对于学龄前儿童适应指挥机器人的实践是必要的。我们进行行动研究，以开发一个早期版本的规范化流程，用于快速原型化Cubetto的游戏故事。我们的方法既有可复现的结果，因为它使用了开放权重模型，又是模型无关的，因为我们用5种不同的LLMs进行了测试。我们一方面记录了过程、所用材料和提示，另一方面记录了学习体验和成果。我们认为生成对于将结果用作教师辅助工具的预期目的而言是成功的。在4种不同的任务场景中测试模型时，我们遇到了内容一致性和幻觉问题，并记录了相应的评估过程以及克服这些问题的尝试（有些成功，有些不成功）。重要的是，这个过程不直接让儿童接触LLMs。相反，这项技术被用于帮助教师轻松开发关于儿童喜欢主题的个性化叙事。我们相信我们的方法适用于学龄前课堂，并计划在真实世界的教育环境中进一步实验。", "summary": "本文探讨了在教育中使用大型语言模型（LLMs）的挑战，特别是对于学龄前儿童。研究提出了一种创新方法，利用LLMs为实体编程机器人Cubetto生成个性化叙事，旨在帮助幼儿适应编程指令，同时避免他们直接接触LLMs。通过行动研究和多模型测试，该方法被证明能有效生成教师辅助材料，尽管在一致性和幻觉方面仍存在挑战。研究强调了该方法对学龄前教育的适用性，并计划进行实地验证。", "keywords": "大型语言模型, 学龄前儿童, 实体机器人, 个性化叙事, 编程教育", "comments": "这项研究的创新之处在于其间接使用LLMs来支持学龄前儿童的编程教育，通过为教师提供个性化叙事工具，有效规避了儿童直接接触AI的风险。模型无关性和开放权重模型的使用提高了研究的可信度和复现性。然而，文中也坦诚地指出了LLMs在生成内容时存在的一致性和幻觉问题，这反映了当前LLMs应用中的普遍挑战，并为未来的研究指明了方向。其对教师辅助的定位也使其在实际教育场景中具有较高的应用潜力。"}}
{"id": "2506.20763", "title": "A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion", "authors": ["Y. Navidtehrani", "C. Betegón", "E. Martínez-Pañeda"], "summary": "We present a novel, generalised formulation to treat coupled structural\nintegrity problems by combining phase field and multi-physics modelling. The\napproach exploits the versatility of the heat transfer equation and is\ntherefore well suited to be adopted in commercial finite element packages,\nrequiring only integration point-level implementation. This aspect is\ndemonstrated here by implementing coupled, multi-variable phenomena through\nsimple \\texttt{UMAT} and \\texttt{UMATHT} subroutines in the finite element\npackage \\texttt{Abaqus}. The generalised theoretical and computational\nframework presented is particularised to four problems of engineering and\nscientific relevance: thermo-mechanical fracture, hydraulic fracture,\nhydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are\nconsidered. The results reveal a very good agreement with experimental data,\nand existing numerical and analytical solutions.The user subroutines developed\nare made freely available at https://mechmat.web.ox.ac.uk/codes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20763v1", "categories": ["cs.CE", "cs.NA", "math.NA", "physics.app-ph"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20763v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "耦合问题相场建模的通用框架：在热机械断裂、水力压裂、氢脆和腐蚀中的应用", "tldr": "本文提出了一个通用的框架，结合相场和多物理场建模来处理耦合结构完整性问题，并在Abaqus中实现，应用于多种工程问题并验证了其有效性。", "motivation": "本文旨在提出一种新颖的通用公式，通过结合相场和多物理场建模来处理耦合结构完整性问题，以提供一种可在商业有限元软件包中采用的多功能方法。", "method": "该方法结合了相场和多物理场建模，利用传热方程的多功能性。通过在Abaqus有限元软件包中实现简单的UMAT和UMATHT子程序来演示。该框架特别应用于热机械断裂、水力压裂、氢致开裂和金属腐蚀四个工程和科学相关问题，并考虑了二维和三维问题。", "result": "结果表明与实验数据以及现有数值和解析解吻合良好。", "conclusion": "所开发的通用理论和计算框架对于处理各种耦合结构完整性问题是有效且适用的，并与现有数据表现出良好的一致性。开发的子程序已免费提供。", "translation": "我们提出了一种新颖的通用公式，通过结合相场和多物理场建模来处理耦合结构完整性问题。该方法利用了传热方程的多功能性，因此非常适合在商业有限元软件包中采用，仅需要积分点级别的实现。这一点通过在有限元软件包Abaqus中，通过简单的UMAT和UMATHT子程序实现耦合多变量现象得到了证明。所提出的通用理论和计算框架被特别应用于四个具有工程和科学相关性的问题：热机械断裂、水力压裂、氢致开裂和金属腐蚀。考虑了二维和三维问题。结果表明与实验数据以及现有数值和解析解吻合良好。开发的子程序已在https://mechmat.web.ox.ac.uk/codes免费提供。", "summary": "本文提出了一个通用的相场与多物理场耦合建模框架，用于解决结构完整性问题。该方法利用传热方程的通用性，易于在Abaqus等商业有限元软件中通过UMAT/UMATHT子程序实现。该框架成功应用于热机械断裂、水力压裂、氢致开裂和金属腐蚀等工程问题，结果与实验数据及现有解决方案高度吻合。相关子程序已开源。", "keywords": "相场, 多物理场, 耦合问题, 断裂, 腐蚀", "comments": "本文的主要创新在于提出了一个通用的相场与多物理场耦合建模框架，其独特之处在于利用传热方程的通用性，使得该方法能够通过简单的用户子程序（如UMAT/UMATHT）无缝集成到Abaqus等商业有限元软件中，极大地提高了其实用性和工程应用价值。该框架成功应用于多种复杂的结构完整性问题，包括断裂和腐蚀，展示了其强大的多功能性。此外，将开发的子程序开源，进一步促进了该领域的科研和工程实践。"}}
{"id": "2506.20685", "title": "Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan", "Naima Iltaf", "Ihtesham ul Islam"], "summary": "Federated Learning (FL) has emerged as a transformative paradigm for\ndistributed machine learning while preserving data privacy. However, existing\napproaches predominantly focus on model heterogeneity and aggregation\ntechniques, largely overlooking the fundamental impact of dataset size\ncharacteristics on federated training dynamics. This paper introduces\nSize-Based Adaptive Federated Learning (SAFL), a novel progressive training\nframework that systematically organizes federated learning based on dataset\nsize characteristics across heterogeneous multi-modal data. Our comprehensive\nexperimental evaluation across 13 diverse datasets spanning 7 modalities\n(vision, text, time series, audio, sensor, medical vision, and multimodal)\nreveals critical insights: 1) an optimal dataset size range of 1000-1500\nsamples for federated learning effectiveness; 2) a clear modality performance\nhierarchy with structured data (time series, sensor) significantly\noutperforming unstructured data (text, multimodal); and 3) systematic\nperformance degradation for large datasets exceeding 2000 samples. SAFL\nachieves an average accuracy of 87.68% across all datasets, with structured\ndata modalities reaching 99%+ accuracy. The framework demonstrates superior\ncommunication efficiency, reducing total data transfer to 7.38 GB across 558\ncommunications while maintaining high performance. Our real-time monitoring\nframework provides unprecedented insights into system resource utilization,\nnetwork efficiency, and training dynamics. This work fills critical gaps in\nunderstanding how data characteristics should drive federated learning\nstrategies, providing both theoretical insights and practical guidance for\nreal-world FL deployments in neural network and learning systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20685v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20685v1", "date": "2025-06-24", "updated": "2025-06-24", "AI": {"title_translation": "渐进式大小自适应联邦学习：一个针对异构多模态数据系统的综合框架", "tldr": "本文提出了SAFL，一个基于数据集大小特征的渐进式联邦学习框架，用于处理异构多模态数据，揭示了数据集大小对联邦学习性能的关键影响，并取得了优异的性能和通信效率。", "motivation": "现有联邦学习方法主要关注模型异构性和聚合技术，但忽视了数据集大小特征对联邦训练动态的根本性影响。", "method": "本文提出了大小自适应联邦学习（SAFL），一个新颖的渐进式训练框架，系统地根据异构多模态数据的数据集大小特征来组织联邦学习。", "result": "实验结果显示：1) 联邦学习效果的最佳数据集大小范围是1000-1500个样本；2) 模态性能存在明显层级，结构化数据（时间序列、传感器）显著优于非结构化数据（文本、多模态）；3) 超过2000个样本的大数据集性能会系统性下降。SAFL在所有数据集上平均准确率达到87.68%，结构化数据模态达到99%+准确率。框架还展示了卓越的通信效率，在558次通信中将总数据传输量减少到7.38 GB。", "conclusion": "这项工作填补了理解数据特征如何驱动联邦学习策略的关键空白，为实际的联邦学习部署提供了理论见解和实践指导。", "translation": "联邦学习（FL）已成为一种在保护数据隐私的同时实现分布式机器学习的变革性范式。然而，现有方法主要关注模型异构性和聚合技术，却在很大程度上忽视了数据集大小特征对联邦训练动态的根本性影响。本文介绍了基于大小的自适应联邦学习（SAFL），一个新颖的渐进式训练框架，系统地根据异构多模态数据中的数据集大小特征来组织联邦学习。我们对跨越7种模态（视觉、文本、时间序列、音频、传感器、医学视觉和多模态）的13个不同数据集进行的全面实验评估揭示了关键见解：1）联邦学习效果的最佳数据集大小范围是1000-1500个样本；2）存在清晰的模态性能层级，其中结构化数据（时间序列、传感器）显著优于非结构化数据（文本、多模态）；以及3）对于超过2000个样本的大数据集，性能会系统性下降。SAFL在所有数据集上的平均准确率达到87.68%，结构化数据模态达到99%以上的准确率。该框架展示了卓越的通信效率，在558次通信中将总数据传输量减少到7.38 GB，同时保持了高性能。我们的实时监控框架为系统资源利用率、网络效率和训练动态提供了前所未有的洞察。这项工作填补了理解数据特征应如何驱动联邦学习策略的关键空白，为神经网络和学习系统中的实际联邦学习部署提供了理论见解和实践指导。", "summary": "本文提出了大小自适应联邦学习（SAFL），一个创新的渐进式联邦学习框架，旨在解决现有方法忽视数据集大小对训练动态影响的问题。SAFL系统地根据异构多模态数据的数据集大小特征来组织联邦学习。通过在13个数据集和7种模态上的广泛实验，研究发现联邦学习的最佳数据集大小为1000-1500样本，结构化数据性能优于非结构化数据，且大数据集性能下降。SAFL实现了87.68%的平均准确率，并显著提高了通信效率，为实际联邦学习部署提供了重要指导。", "keywords": "联邦学习, 数据集大小, 异构数据, 多模态, SAFL", "comments": "该论文的创新点在于首次系统性地探讨了数据集大小对联邦学习性能的影响，并提出了相应的自适应框架SAFL。它填补了现有研究的空白，提供了关于数据特征如何影响联邦学习策略的宝贵见解。实验结果不仅量化了最佳数据集大小范围，还揭示了不同模态数据的性能差异，这对于实际部署具有重要的指导意义。框架在通信效率上的提升也进一步增强了其实用性。"}}
{"id": "2506.20728", "title": "Distributed Lyapunov Functions for Nonlinear Networks", "authors": ["Yiming Wang", "Arthur N. Montanari", "Adilson E. Motter"], "summary": "Nonlinear networks are often multistable, exhibiting coexisting stable states\nwith competing regions of attraction (ROAs). As a result, ROAs can have complex\n\"tentacle-like\" morphologies that are challenging to characterize analytically\nor computationally. In addition, the high dimensionality of the state space\nprohibits the automated construction of Lyapunov functions using\nstate-of-the-art optimization methods, such as sum-of-squares (SOS)\nprogramming. In this letter, we propose a distributed approach for the\nconstruction of Lyapunov functions based solely on local information. To this\nend, we establish an augmented comparison lemma that characterizes the\nexistence conditions of partial Lyapunov functions, while also accounting for\nresidual effects caused by the associated dimensionality reduction. These\ntheoretical results allow us to formulate an SOS optimization that iteratively\nconstructs such partial functions, whose aggregation forms a composite Lyapunov\nfunction. The resulting composite function provides accurate convex\napproximations of both the volumes and shapes of the ROAs. We validate our\nmethod on networks of van der Pol and Ising oscillators, demonstrating its\neffectiveness in characterizing high-dimensional systems with non-convex ROAs.", "comment": "Codes are available at our GitHub repository\n  https://github.com/YimingSci/Distributed-Lya-Func", "pdf_url": "http://arxiv.org/pdf/2506.20728v1", "categories": ["eess.SY", "cond-mat.dis-nn", "cs.SY", "math.DS"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20728v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "非线性网络的分布式李雅普诺夫函数", "tldr": "该论文提出一种分布式方法，通过局部信息构建李雅普诺夫函数，以精确近似高维非线性网络中复杂吸引域的形状和体积。", "motivation": "非线性网络通常是多稳态的，其吸引域（ROA）形态复杂且难以分析或计算。此外，高维状态空间使得使用现有优化方法（如和平方（SOS）编程）自动构建李雅普诺夫函数变得困难。", "method": "本文提出了一种基于局部信息的分布式方法来构建李雅普诺夫函数。为此，建立了一个增广比较引理，用于表征部分李雅普诺夫函数的存在条件，并考虑了维度降低引起的残余效应。利用这些理论结果，制定了一个SOS优化问题，迭代地构建这些部分函数，它们的聚合形成一个复合李雅普诺夫函数。", "result": "生成的复合李雅普诺夫函数能够精确地凸近似吸引域的体积和形状。该方法在范德波尔振荡器和伊辛振荡器网络上得到了验证，证明了其在表征具有非凸吸引域的高维系统方面的有效性。", "conclusion": "该研究提出了一种有效的分布式方法，用于在高维非线性网络中构建李雅普诺夫函数，并准确刻画其复杂的吸引域。", "translation": "非线性网络通常是多稳态的，表现出共存的稳定状态和相互竞争的吸引域（ROA）。因此，吸引域可能具有复杂的“触手状”形态，难以进行分析或计算表征。此外，状态空间的高维性使得使用最先进的优化方法（如和平方（SOS）编程）自动构建李雅普诺夫函数变得困难。在这封信中，我们提出了一种基于局部信息的分布式方法来构建李雅普诺夫函数。为此，我们建立了一个增广比较引理，该引理表征了部分李雅普诺夫函数的存在条件，同时考虑了相关的维度降低引起的残余效应。这些理论结果使我们能够制定一个SOS优化问题，迭代地构建这些部分函数，它们的聚合形成一个复合李雅普诺夫函数。由此产生的复合函数提供了吸引域体积和形状的精确凸近似。我们在范德波尔和伊辛振荡器网络上验证了我们的方法，证明了其在表征具有非凸吸引域的高维系统方面的有效性。", "summary": "该论文提出了一种用于非线性网络的分布式李雅普诺夫函数构建方法，旨在解决高维状态空间中吸引域（ROA）复杂性和李雅普诺夫函数难以构建的问题。通过建立增广比较引理和迭代的SOS优化，该方法能够从局部信息构建部分函数并聚合为复合函数，从而精确地凸近似ROA的体积和形状。该方法已在多种振荡器网络上得到验证，显示出其在处理高维非凸ROA系统方面的有效性。", "keywords": "分布式李雅普诺夫函数, 非线性网络, 吸引域, 和平方编程, 高维系统", "comments": "本文的创新之处在于提出了一种基于局部信息构建分布式李雅普诺夫函数的方法，并通过增广比较引理解决了高维系统下吸引域分析和李雅普诺夫函数构建的挑战。这为分析复杂非线性动力学系统提供了一种可扩展且有效的新工具。其重要性体现在能够精确近似复杂吸引域的形状和体积，对于理解多稳态系统行为具有重要意义。"}}
{"id": "2506.20783", "title": "Precise Near-Field Beam Training with DFT Codebook based on Amplitude-only Measurement", "authors": ["Zijun Wang", "Shawn Tsai", "Rama Kiran", "Rui Zhang"], "summary": "Extremely large antenna arrays (ELAAs) operating in high-frequency bands have\nspurred the development of near-field communication, driving advancements in\nbeam training and signal processing design. In this work, we present a\nlow-complexity near-field beam training scheme that fully utilizes the\nconventional discrete Fourier transform (DFT) codebook designed for far-field\nusers. We begin by analyzing the received beam pattern in the near field and\nderive closed-form expressions for the beam width and central gain. These\nanalytical results enable the definition of an angle-dependent, modified\nRayleigh distance, which effectively distinguishes near-field and far-field\nuser regimes. Building on the analysis, we develop a direct and computationally\nefficient method to estimate user distance, with a complexity of O(1), and\nfurther improve its accuracy through a simple refinement. Simulation results\ndemonstrate significant gains in both single- and multi-user settings, with up\nto 2.38 dB SNR improvement over exhaustive search. To further enhance\nestimation accuracy, we additionally propose a maximum likelihood estimation\n(MLE) based refinement method, leveraging the Rician distribution of signal\namplitudes and achieving accuracy close to the Cramer--Rao bound (CRB).\nSimulation shows the single-user and multi-user achievable rates can both\napproach those obtained with ideal channel state information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20783v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20783v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基于幅度测量的DFT码本精确近场波束训练", "tldr": "提出了一种低复杂度的近场波束训练方案，利用传统DFT码本，并通过分析和MLE方法实现精确的用户距离估计，显著提升了信噪比和可达速率。", "motivation": "极大规模天线阵列（ELAAs）在高频段运行，推动了近场通信发展，但传统的远场波束训练方法不再适用，需要新的近场波束训练和信号处理设计。", "method": "本文提出了一种低复杂度的近场波束训练方案，充分利用了为远场用户设计的传统离散傅里叶变换（DFT）码本。首先，分析了近场接收波束模式，推导了波束宽度和中心增益的闭合形式表达式，并据此定义了角度依赖的修正瑞利距离。在此分析基础上，开发了一种计算效率为O(1)的用户距离直接估计方法，并通过简单细化提高其精度。此外，还提出了一种基于最大似然估计（MLE）的细化方法，利用信号幅度的莱斯分布，实现接近Cramer-Rao下界（CRB）的精度。", "result": "仿真结果表明，在单用户和多用户设置下，该方案均取得了显著增益，相对于穷举搜索，信噪比提高了高达2.38 dB。最大似然估计（MLE）方法实现了接近Cramer-Rao下界（CRB）的精度。仿真还显示，单用户和多用户的可达速率都能接近通过理想信道状态信息获得的结果。", "conclusion": "通过利用传统DFT码本并结合精确的距离估计算法（包括O(1)直接估计和MLE细化），本文提出的近场波束训练方案能够有效且高精度地进行近场用户定位和波束训练，显著提升了系统性能。", "translation": "极大规模天线阵列（ELAAs）在高频段的运行推动了近场通信的发展，促进了波束训练和信号处理设计的进步。在这项工作中，我们提出了一种低复杂度的近场波束训练方案，该方案充分利用了为远场用户设计的传统离散傅里叶变换（DFT）码本。我们首先分析了近场接收波束模式，并推导了波束宽度和中心增益的闭合形式表达式。这些分析结果使得能够定义一个角度依赖的修正瑞利距离，从而有效区分近场和远场用户区域。在此分析基础上，我们开发了一种直接且计算高效的用户距离估计算法，其复杂度为O(1)，并通过简单的细化进一步提高了其精度。仿真结果表明，在单用户和多用户设置下，该方案均取得了显著增益，相对于穷举搜索，信噪比提高了高达2.38 dB。为了进一步提高估计精度，我们额外提出了一种基于最大似然估计（MLE）的细化方法，该方法利用了信号幅度的莱斯分布，并实现了接近Cramer-Rao下界（CRB）的精度。仿真显示，单用户和多用户的可达速率都能接近通过理想信道状态信息获得的结果。", "summary": "本文提出了一种基于传统DFT码本的低复杂度近场波束训练方案，以应对极大规模天线阵列在近场通信中的挑战。通过分析近场波束模式并推导相关表达式，该方案定义了修正瑞利距离以区分近远场。核心贡献在于开发了一种计算复杂度为O(1)的直接用户距离估计算法，并通过简单细化和基于MLE的精确细化方法进一步提升精度，后者能达到接近Cramer-Rao下界。仿真结果验证了该方案在单用户和多用户场景下相对于传统方法在信噪比和可达速率上的显著性能提升。", "keywords": "近场通信, 波束训练, DFT码本, 距离估计, 极大规模天线阵列", "comments": "本文的创新点在于将为远场设计的DFT码本创造性地应用于近场波束训练，并通过对近场特性的深入分析和高效的距离估计算法（尤其是O(1)和MLE方法）实现了高精度和低复杂度。这对于未来6G通信中近场场景下的高效率波束管理具有重要意义。"}}
{"id": "2506.20683", "title": "Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG", "authors": ["Alexander Selivanov", "Philip Müller", "Özgün Turgut", "Nil Stolt-Ansó", "Daniel Rückert"], "summary": "An electrocardiogram (ECG) is a widely used, cost-effective tool for\ndetecting electrical abnormalities in the heart. However, it cannot directly\nmeasure functional parameters, such as ventricular volumes and ejection\nfraction, which are crucial for assessing cardiac function. Cardiac magnetic\nresonance (CMR) is the gold standard for these measurements, providing detailed\nstructural and functional insights, but is expensive and less accessible. To\nbridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive\nLearning), a multimodal contrastive learning framework that enhances ECG\nrepresentations by integrating spatio-temporal information from CMR. PTACL uses\nglobal patient-level contrastive loss and local temporal-level contrastive\nloss. The global loss aligns patient-level representations by pulling ECG and\nCMR embeddings from the same patient closer together, while pushing apart\nembeddings from different patients. Local loss enforces fine-grained temporal\nalignment within each patient by contrasting encoded ECG segments with\ncorresponding encoded CMR frames. This approach enriches ECG representations\nwith diagnostic information beyond electrical activity and transfers more\ninsights between modalities than global alignment alone, all without\nintroducing new learnable weights. We evaluate PTACL on paired ECG-CMR data\nfrom 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL\nachieves better performance in two clinically relevant tasks: (1) retrieving\npatients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac\nfunction parameters, such as ventricular volumes and ejection fraction. Our\nresults highlight the potential of PTACL to enhance non-invasive cardiac\ndiagnostics using ECG. The code is available at:\nhttps://github.com/alsalivan/ecgcmr", "comment": "accepted to MICCAI 2025 (Springer LNCS)", "pdf_url": "http://arxiv.org/pdf/2506.20683v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20683v1", "date": "2025-06-24", "updated": "2025-06-24", "AI": {"title_translation": "心脏MRI和ECG联合表征的全局和局部对比学习", "tldr": "本文提出了PTACL，一个多模态对比学习框架，通过整合心脏磁共振（CMR）的时空信息来增强心电图（ECG）的表示，从而在患者表型检索和心脏功能参数预测方面表现出优越性能。", "motivation": "心电图（ECG）虽然成本低廉且广泛使用，但无法直接测量心室容积和射血分数等关键心脏功能参数。心脏磁共振（CMR）是测量这些参数的金标准，但其成本高昂且可及性差。为了弥合这一差距，亟需一种方法来利用CMR的详细信息来增强ECG的诊断能力。", "method": "本文提出了PTACL（患者和时间对齐对比学习），一个多模态对比学习框架。该框架结合了两种对比损失：全局患者级对比损失，用于拉近同一患者的ECG和CMR嵌入，同时推开不同患者的嵌入；局部时间级对比损失，用于对比编码的ECG片段与对应的CMR帧，从而实现患者内部的细粒度时间对齐。此方法在不引入新的可学习权重的情况下，通过整合CMR的时空信息来丰富ECG的表示。", "result": "PTACL在英国生物样本库的27,951名受试者的配对ECG-CMR数据上进行了评估。与基线方法相比，PTACL在两个临床相关任务中取得了更好的性能：1）检索具有相似心脏表型的患者；2）预测CMR衍生的心脏功能参数，如心室容积和射血分数。", "conclusion": "PTACL有潜力通过增强ECG的诊断信息，从而改进非侵入性心脏诊断。", "translation": "心电图（ECG）是一种广泛使用、成本效益高的心电异常检测工具。然而，它不能直接测量功能参数，如心室容积和射血分数，这些参数对于评估心脏功能至关重要。心脏磁共振（CMR）是这些测量的金标准，提供详细的结构和功能洞察，但价格昂贵且可及性较低。为了弥合这一差距，我们提出了PTACL（患者和时间对齐对比学习），一个多模态对比学习框架，通过整合来自CMR的时空信息来增强ECG表示。PTACL使用全局患者级对比损失和局部时间级对比损失。全局损失通过拉近来自同一患者的ECG和CMR嵌入，同时推开不同患者的嵌入来对齐患者级表示。局部损失通过对比编码的ECG片段与相应的编码CMR帧，在每个患者内部强制执行细粒度的时间对齐。这种方法在不引入新的可学习权重的情况下，用超越电活动的诊断信息丰富了ECG表示，并且比单独的全局对齐在模态之间转移了更多的洞察力。我们在英国生物样本库的27,951名受试者的配对ECG-CMR数据上评估了PTACL。与基线方法相比，PTACL在两个临床相关任务中取得了更好的性能：（1）检索具有相似心脏表型的患者和（2）预测CMR衍生的心脏功能参数，如心室容积和射血分数。我们的结果突出了PTACL在利用ECG增强非侵入性心脏诊断方面的潜力。代码可在以下网址获取：https://github.com/alsalivan/ecgcmr", "summary": "本文提出了PTACL（患者和时间对齐对比学习），一个创新的多模态对比学习框架，旨在通过整合心脏磁共振（CMR）的时空信息来增强心电图（ECG）的表示。PTACL利用全局患者级和局部时间级对比损失，实现了ECG和CMR模态间在患者层面和时间层面的对齐，从而丰富了ECG的诊断信息。该方法在不增加新可学习权重的前提下，在英国生物样本库的大规模数据集上进行了验证，结果表明PTACL在检索相似心脏表型患者和预测CMR衍生的心脏功能参数方面均优于现有基线方法，展示了其在非侵入性心脏诊断领域的巨大潜力。", "keywords": "心电图, 心脏磁共振, 对比学习, 多模态, 心脏功能", "comments": "该论文的创新点在于提出了PTACL框架，巧妙地结合了全局患者级和局部时间级的对比学习，有效地融合了ECG和CMR这两种不同模态的数据。特别值得注意的是，该方法在不引入额外可学习权重的情况下实现了信息增强，这对于模型的轻量化和实际部署具有重要意义。其重要性在于弥合了ECG易用性高但信息有限与CMR信息丰富但成本高昂之间的差距，显著提升了ECG在心脏功能评估和表型检索方面的能力，为心脏疾病的早期诊断和监测提供了新的非侵入性途径。论文在大规模数据集上的验证也增强了其结果的可信度。"}}
{"id": "2506.20702", "title": "The Singapore Consensus on Global AI Safety Research Priorities", "authors": ["Yoshua Bengio", "Tegan Maharaj", "Luke Ong", "Stuart Russell", "Dawn Song", "Max Tegmark", "Lan Xue", "Ya-Qin Zhang", "Stephen Casper", "Wan Sie Lee", "Sören Mindermann", "Vanessa Wilfred", "Vidhisha Balachandran", "Fazl Barez", "Michael Belinsky", "Imane Bello", "Malo Bourgon", "Mark Brakel", "Siméon Campos", "Duncan Cass-Beggs", "Jiahao Chen", "Rumman Chowdhury", "Kuan Chua Seah", "Jeff Clune", "Juntao Dai", "Agnes Delaborde", "Nouha Dziri", "Francisco Eiras", "Joshua Engels", "Jinyu Fan", "Adam Gleave", "Noah Goodman", "Fynn Heide", "Dan Hendrycks", "Cyrus Hodes", "Bryan Low Kian Hsiang", "Minlie Huang", "Sami Jawhar", "Wang Jingyu", "Adam Tauman Kalai", "Meindert Kamphuis", "Mohan Kankanhalli", "Subhash Kantamneni", "Mathias Bonde Kirk", "Thomas Kwa", "Jeffrey Ladish", "Kwok-Yan Lam", "Wan Lee Sie", "Taewhi Lee", "Xiaojian Li", "Jiajun Liu", "Chaochao Lu", "Yifan Mai", "Richard Mallah", "Julian Michael", "Nick Moës", "Simon Möller", "Kihyuk Nam", "Kwan Yee Ng", "Mark Nitzberg", "Besmira Nushi", "Seán O hÉigeartaigh", "Alejandro Ortega", "Pierre Peigné", "James Petrie", "Benjamin Prud'Homme", "Reihaneh Rabbany", "Nayat Sanchez-Pi", "Sarah Schwettmann", "Buck Shlegeris", "Saad Siddiqui", "Aradhana Sinha", "Martín Soto", "Cheston Tan", "Dong Ting", "Robert Trager", "Brian Tse", "Anthony Tung K. H.", "Vanessa Wilfred", "John Willes", "Denise Wong", "Wei Xu", "Rongwu Xu", "Yi Zeng", "HongJiang Zhang", "Djordje Žikelić"], "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).", "comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report", "pdf_url": "http://arxiv.org/pdf/2506.20702v1", "categories": ["cs.AI", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20702v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "全球AI安全研究重点的新加坡共识", "tldr": "该报告基于2025年新加坡AI会议，旨在通过将AI安全研究领域分为开发、评估和控制，识别并综合全球AI安全研究重点，以构建可信赖的AI生态系统。", "motivation": "快速发展的AI能力和自主性带来了巨大的变革前景，但也引发了关于如何确保AI安全（即值得信赖、可靠和安全）的激烈辩论。因此，建立一个可信赖的生态系统至关重要，它有助于人们自信地接纳AI，并为创新提供最大空间，同时避免负面影响。", "method": "2025年新加坡AI会议（SCAI）汇集了来自不同地区的AI科学家，以识别和综合AI安全研究重点。该报告在此基础上，采纳了深度防御模型，将AI安全研究领域分为三类：创建可信赖AI系统面临的挑战（开发）、评估其风险面临的挑战（评估）以及部署后监控和干预面临的挑战（控制）。", "result": "该报告识别并组织了AI安全研究领域，分为开发、评估和控制三大类挑战，旨在支持AI安全研究，并构建可信赖的AI生态系统。", "conclusion": "通过识别和分类AI安全研究的优先事项（开发、评估、控制），可以更好地指导研究，以应对AI带来的挑战，确保AI的安全、可信赖和可靠，从而促进其积极发展并避免潜在风险。", "translation": "AI能力和自主性的快速提升预示着巨大的变革前景，但也引发了关于如何确保AI安全，即值得信赖、可靠和安全的激烈辩论。因此，建立一个可信赖的生态系统至关重要——它有助于人们自信地接纳AI，并为创新提供最大空间，同时避免负面影响。\n“2025年新加坡人工智能会议（SCAI）：人工智能安全国际科学交流”旨在通过汇集全球各地的AI科学家，识别和综合AI安全研究重点，以支持该领域的研究。这份报告建立在由Yoshua Bengio主持并得到33个政府支持的《国际AI安全报告》的基础上。通过采用深度防御模型，这份报告将AI安全研究领域分为三类：创建可信赖AI系统面临的挑战（开发）、评估其风险面临的挑战（评估），以及部署后监控和干预面临的挑战（控制）。", "summary": "《新加坡共识》报告基于2025年新加坡AI会议，旨在应对AI快速发展带来的安全挑战，通过汇集全球AI科学家，识别并综合AI安全研究优先事项。报告采纳深度防御模型，将AI安全研究领域划分为开发、评估和控制三类挑战，旨在构建一个值得信赖的AI生态系统，确保AI的可靠性和安全性，从而促进其负责任的创新与应用。", "keywords": "AI安全, 研究优先事项, 新加坡共识, 可信赖AI, 深度防御模型", "comments": "该论文（报告）通过组织国际会议并采纳深度防御模型，系统性地梳理了全球AI安全研究的优先领域，为未来AI安全研究提供了清晰的框架和方向。其创新之处在于将AI安全挑战细分为开发、评估和控制，这有助于更全面和有针对性地解决AI安全问题。该报告的重要性在于其由国际共识形成，并得到多国政府支持，具有广泛的指导意义。"}}
{"id": "2506.20703", "title": "Generative Blocks World: Moving Things Around in Pictures", "authors": ["Vaibhav Vavilala", "Seemandhar Jain", "Rahul Vasanth", "D. A. Forsyth", "Anand Bhattad"], "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.", "comment": "23 pages, 16 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.20703v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20703v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "生成式积木世界：在图像中移动物体", "tldr": "一种通过操纵3D几何基元来编辑生成图像的方法，提高了保真度和可编辑性。", "motivation": "旨在通过操纵简单的几何抽象来与生成图像的场景进行交互，从而实现对生成图像的编辑。", "method": "该方法将场景表示为凸3D基元的集合，允许使用不同数量的基元来表示同一场景，从而能够移动整个结构或微小细节。图像通过基于流的方法生成，该方法以深度和纹理提示为条件。纹理提示考虑了修改后的3D基元，超越了现有键值缓存技术提供的纹理一致性，并允许精确的对象和相机移动，同时保留对象身份。", "result": "定量和定性实验表明，该方法在视觉保真度、可编辑性和组合泛化方面优于现有工作。", "conclusion": "本文介绍了“生成式积木世界”，它成功地通过操D作3D几何基元实现了对生成图像场景的交互和操作，并在视觉保真度、可编辑性和泛化能力方面优于现有方法。", "translation": "本文介绍了生成式积木世界（Generative Blocks World），它通过操纵简单的几何抽象来与生成图像的场景进行交互。我们的方法将场景表示为凸3D基元的集合，同一个场景可以用不同数量的基元表示，从而允许编辑者移动整个结构或微小细节。一旦场景几何形状被编辑，图像就会通过一种基于流的方法生成，该方法以深度和纹理提示为条件。我们的纹理提示考虑了修改后的3D基元，超越了现有键值缓存技术提供的纹理一致性。这些纹理提示（a）允许精确的对象和相机移动，并且（b）在很大程度上保留了所描绘对象的身份。定量和定性实验表明，我们的方法在视觉保真度、可编辑性和组合泛化方面优于现有工作。", "summary": "本文介绍了生成式积木世界，这是一种通过操纵3D几何基元来交互式编辑生成图像的新方法。它将场景表示为凸3D基元的集合，允许从粗略结构到精细细节的灵活编辑。图像生成采用基于流的方法，并结合独特的纹理提示机制，该机制考虑了3D基元的修改，显著增强了纹理一致性并保留了对象身份。实验表明，与现有方法相比，该方法在视觉保真度、可编辑性和组合泛化方面表现优异。", "keywords": "生成式积木世界, 图像编辑, 3D基元, 纹理一致性, 场景操作", "comments": "该论文提出了一种创新的方法，通过将操作基于3D几何基元来实现对生成图像的编辑。使用考虑修改后的3D基元的纹理提示是现有方法的一个显著改进，解决了在编辑过程中保持视觉一致性的关键挑战。在不同细节级别（整体结构与微小细节）进行编辑的能力也增加了显著的灵活性。其在视觉保真度和可编辑性方面的表现表明了交互式生成模型的实际进步。"}}
{"id": "2506.20677", "title": "Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions", "authors": ["Shrinivass Arunachalam Balasubramanian"], "summary": "Sorting is an essential operation in computer science with direct\nconsequences on the performance of large scale data systems, real-time systems,\nand embedded computation. However, no sorting algorithm is optimal under all\ndistributions of data. The new adaptive hybrid sorting paradigm proposed in\nthis paper is the paradigm that automatically selects the most effective\nsorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time\nmonitoring of patterns in input data. The architecture begins by having a\nfeature extraction module to compute significant parameters such as data\nvolume, value range and entropy. These parameters are sent to a decision engine\ninvolving Finite State Machine and XGBoost classifier to aid smart and\neffective in choosing the optimal sorting strategy. It implements Counting Sort\non small key ranges, Radix Sort on large range structured input with\nlow-entropy keys and QuickSort on general purpose sorting. The experimental\nfindings of both synthetic and real life dataset confirm that the proposed\nsolution is actually inclined to excel significantly by comparison in execution\ntime, flexibility and the efficiency of conventional static sorting algorithms.\nThe proposed framework provides a scalable, high perhaps and applicable to a\nwide range of data processing operations like big data analytics, edge\ncomputing, and systems with hardware limitations.", "comment": "11 Pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20677v1", "categories": ["cs.DS", "cs.DB", "cs.PF"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20677v1", "date": "2025-06-22", "updated": "2025-06-22", "AI": {"title_translation": "自适应混合排序：针对不同数据分布的动态策略选择以实现最优排序", "tldr": "本文提出了一种自适应混合排序范式，它能根据实时监测的输入数据模式，自动选择最优的排序算法（计数排序、基数排序或快速排序），并在实验中证明其在执行时间、灵活性和效率上优于传统静态排序算法。", "motivation": "传统的排序算法无法在所有数据分布下都达到最优性能，因此需要一种能根据数据特性动态选择最优排序策略的方法。", "method": "该方法首先通过特征提取模块计算数据量、值范围和熵等参数，然后将这些参数送入一个包含有限状态机和XGBoost分类器的决策引擎，以智能高效地选择最优排序策略。具体来说，对小键范围数据使用计数排序，对大范围结构化低熵键输入使用基数排序，对通用排序使用快速排序。", "result": "在合成数据集和真实数据集上的实验结果表明，所提出的解决方案在执行时间、灵活性和效率方面显著优于传统的静态排序算法。", "conclusion": "所提出的自适应混合排序框架提供了一种可扩展、高性能且适用于大数据分析、边缘计算和硬件受限系统等多种数据处理操作的解决方案。", "translation": "排序是计算机科学中一项基本操作，直接影响大规模数据系统、实时系统和嵌入式计算的性能。然而，没有一种排序算法能在所有数据分布下都达到最优。本文提出的新型自适应混合排序范式，能够根据对输入数据模式的实时监测，自动选择最有效的排序算法，包括计数排序、基数排序或快速排序。该架构首先通过特征提取模块计算数据量、值范围和熵等重要参数。这些参数被发送到一个包含有限状态机和XGBoost分类器的决策引擎，以帮助智能有效地选择最佳排序策略。它在小键范围上实现计数排序，在大范围结构化低熵键输入上实现基数排序，在通用排序上实现快速排序。对合成数据集和真实数据集的实验结果证实，所提出的解决方案在执行时间、灵活性和效率方面明显优于传统的静态排序算法。所提出的框架具有可扩展性、高性能，并适用于大数据分析、边缘计算和硬件受限系统等广泛的数据处理操作。", "summary": "本文提出了一种自适应混合排序（Adaptive Hybrid Sort）范式，旨在解决现有排序算法无法在所有数据分布下都达到最优性能的问题。该方法通过一个特征提取模块实时监测输入数据的模式（如数据量、值范围和熵），并将这些参数输入到一个结合了有限状态机和XGBoost分类器的决策引擎。基于数据特性，系统动态选择最适合的排序算法：计数排序适用于小键范围，基数排序适用于大范围结构化低熵键数据，而快速排序则用于通用场景。实验结果表明，与传统静态排序算法相比，该自适应混合排序方案在执行时间、灵活性和效率方面均表现出显著优势，并适用于大数据分析、边缘计算等多种数据处理场景。", "keywords": "自适应排序, 混合排序, 动态策略选择, XGBoost, 数据分布", "comments": "这项研究的创新之处在于其动态策略选择机制，通过结合特征提取和机器学习（XGBoost）来优化排序算法的选择，从而克服了单一排序算法在不同数据分布下的局限性。其重要性体现在提升大规模数据系统、实时系统和边缘计算的性能上，具有广泛的应用前景。"}}
{"id": "2506.21074", "title": "CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via Dynamic Frame Rate", "authors": ["Hankun Wang", "Yiwei Guo", "Chongtian Shao", "Bohan Li", "Xie Chen", "Kai Yu"], "summary": "Neural speech codecs have been widely used in audio compression and various\ndownstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which\nallocate the same number of tokens to every equal-duration slice. However,\nspeech is inherently non-uniform in temporal information density. As a result,\nmany tokens are wasted on steady-state segments like long vowels and silences.\nTo address this mismatch, we present CodecSlime, a plugin-style method for\ncompressing temporal redundancy through supporting dynamic frame rate (DFR) on\nneural speech codecs for the first time. Our method is unsupervised and\narchitecture-agnostic, combining two key innovations, ScheDFR and\nMelt-and-Cool, for adapting inference and training, respectively. When\nintegrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR\n($\\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to\n46% relative to conventional FFR baselines with the same model architecture and\nsimilar bitrates, while other metrics are also competitive. CodecSlime also\nenables flexible trade-offs between reconstruction quality and bitrate: a\nsingle model supports inference at multiple frame rates and consistently\noutperforms FFR models at the corresponding frame rates. Audio samples are\navailable at https://acadarmeria.github.io/codecslime/.", "comment": "16 pages, 5 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2506.21074v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21074v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CodecSlime：通过动态帧率压缩神经语音编解码器的时间冗余", "tldr": "CodecSlime首次为神经语音编解码器引入动态帧率（DFR），显著压缩了时间冗余，提高了性能并支持灵活的质量-码率权衡。", "motivation": "当前主流的固定帧率（FFR）神经语音编解码器在处理语音时，由于语音在时间信息密度上固有的不均匀性，会在长元音和静音等稳态段浪费大量令牌，导致效率低下。", "method": "CodecSlime是一种插件式、无监督、与架构无关的方法，通过支持动态帧率（DFR）来压缩神经语音编解码器的时间冗余。它结合了ScheDFR用于推理和Melt-and-Cool用于训练。", "result": "当集成到典型的VQ-GAN编解码器骨干并以40 Hz DFR（约600 bps）运行时，CodecSlime的重建词错误率（WER）相对于具有相同模型架构和相似码率的传统FFR基线降低了高达46%，同时其他指标也具有竞争力。CodecSlime还实现了重建质量和码率之间的灵活权衡：单个模型支持以多种帧率进行推理，并在相应的帧率下始终优于FFR模型。", "conclusion": "CodecSlime通过引入动态帧率有效解决了神经语音编解码器中的时间冗余问题，显著提高了重建质量，并提供了灵活的码率控制，性能优于固定帧率方法。", "translation": "神经语音编解码器已广泛应用于音频压缩和各种下游任务。当前主流的编解码器是固定帧率（FFR）的，它们为每个等长时间段分配相同数量的令牌。然而，语音在时间信息密度上固有地不均匀。因此，许多令牌浪费在长元音和静音等稳态段上。为了解决这种不匹配问题，我们首次提出了CodecSlime，这是一种插件式方法，通过在神经语音编解码器上支持动态帧率（DFR）来压缩时间冗余。我们的方法是无监督且与架构无关的，结合了ScheDFR和Melt-and-Cool两项关键创新，分别用于适应推理和训练。当集成到典型的VQ-GAN编解码器骨干并以40 Hz DFR（约600 bps）运行时，CodecSlime的重建词错误率（WER）相对于具有相同模型架构和相似码率的传统FFR基线降低了高达46%，同时其他指标也具有竞争力。CodecSlime还实现了重建质量和码率之间的灵活权衡：单个模型支持以多种帧率进行推理，并在相应的帧率下始终优于FFR模型。音频样本可在https://acadarmeria.github.io/codecslime/获取。", "summary": "CodecSlime通过引入动态帧率（DFR）解决了固定帧率（FFR）神经语音编解码器在冗余语音段上浪费令牌的低效率问题。这种无监督、与架构无关的插件方法利用ScheDFR进行推理和Melt-and-Cool进行训练。实验表明，CodecSlime在相似码率下，其重建词错误率（WER）比FFR基线降低高达46%，同时提供灵活的质量-码率权衡，并持续优于FFR模型。", "keywords": "神经语音编解码器, 动态帧率, 时间冗余, 语音压缩, CodecSlime", "comments": "本文通过解决语音固有的时间非均匀性，提出了一种优化神经语音编解码器的新颖方法。CodecSlime的插件式、无监督和与架构无关的特性，以及其显著的WER降低和灵活的码率控制能力，使其成为高效音频压缩领域的重要贡献。动态帧率在语音编解码器中的应用概念是具有创新性的。"}}
{"id": "2506.20817", "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation", "authors": ["Ali Tourani", "Fatemeh Nazary", "Yashar Deldjoo"], "summary": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec", "comment": "20 pages, 6 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.20817v1", "categories": ["cs.IR", "cs.MM"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20817v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "RAG-VisualRec: 一种用于推荐中视觉与文本增强型检索增强生成开放资源", "tldr": "本文介绍了RAG-VisualRec，一个开放资源，结合LLM生成的剧情描述和视觉嵌入，用于多模态电影推荐，并展示了召回率和NDCG的提升。", "motivation": "在电影领域开发鲁棒的多模态推荐系统面临挑战，因为有限的元数据（如标题、类型）阻碍了生成有效的推荐。", "method": "该方法结合了LLM生成的剧情描述和从预告片中提取的视觉嵌入，形成一个统一的管道，支持检索增强生成（RAG）和协同过滤。它包含一个数据增强步骤，将稀疏元数据转换为更丰富的文本信号，并使用融合策略（如PCA、CCA）整合视觉线索。此外，还包括一个LLM驱动的重排序步骤。", "result": "实验评估表明，与单模态基线相比，基于CCA的融合显著提高了召回率。LLM驱动的重排序步骤进一步改善了NDCG，特别是在文本数据有限的场景中。", "conclusion": "该论文发布了RAG-VisualRec框架，旨在鼓励进一步探索针对冷启动、注重新颖性和特定领域设置的多模态推荐技术。", "translation": "本文旨在解决电影领域多模态推荐系统开发中的挑战，该领域有限的元数据（例如，标题、类型）通常会阻碍生成鲁棒的推荐。我们引入了一种资源，它将LLM生成的剧情描述与预告片派生的视觉嵌入结合在一个统一的管道中，支持检索增强生成（RAG）和协同过滤。我们方法的核心是一个数据增强步骤，将稀疏元数据转换为更丰富的文本信号，同时采用融合策略（例如，PCA、CCA）来整合视觉线索。实验评估表明，与单模态基线相比，基于CCA的融合显著提高了召回率，而LLM驱动的重排序步骤进一步改善了NDCG，特别是在文本数据有限的情况下。通过发布这个框架，我们邀请进一步探索针对冷启动、注重新颖性和特定领域设置的多模态推荐技术。所有代码、数据和详细文档均可在以下网址公开获取：https://github.com/RecSys-lab/RAG-VisualRec", "summary": "本文介绍了RAG-VisualRec，一个开放资源，旨在增强多模态电影推荐系统。它通过整合LLM生成的剧情描述和预告片派生的视觉嵌入来克服元数据有限的问题，并采用数据增强和CCA等融合技术。实验结果显示，特别是在数据稀缺的场景下，召回率和NDCG有显著提升，表明其在各种推荐设置中的潜力。", "keywords": "多模态推荐, 检索增强生成 (RAG), 视觉嵌入, LLM, 冷启动推荐", "comments": "该论文提出了一个创新的开放资源RAG-VisualRec，通过利用LLM进行文本丰富和视觉嵌入，有效地解决了多模态推荐中的冷启动和数据稀疏问题。将RAG与协同过滤相结合以及整个框架的发布是重要的贡献，促进了该挑战领域中的进一步研究。"}}
{"id": "2506.20777", "title": "Inverse initial data reconstruction for Maxwell's equations via time-dimensional reduction method", "authors": ["Thuy T. Le", "Cong B. Van", "Trong D. Dang", "Loc H. Nguyen"], "summary": "We study an inverse problem for the time-dependent Maxwell system in an\ninhomogeneous and anisotropic medium. The objective is to recover the initial\nelectric field $\\mathbf{E}_0$ in a bounded domain $\\Omega \\subset\n\\mathbb{R}^3$, using boundary measurements of the electric field and its normal\nderivative over a finite time interval. Informed by practical constraints, we\nadopt an under-determined formulation of Maxwell's equations that avoids the\nneed for initial magnetic field data and charge density information. To address\nthis inverse problem, we develop a time-dimension reduction approach by\nprojecting the electric field onto a finite-dimensional Legendre\npolynomial-exponential basis in time. This reformulates the original space-time\nproblem into a sequence of spatial systems for the projection coefficients. The\nreconstruction is carried out using the quasi-reversibility method within a\nminimum-norm framework, which accommodates the inherent non-uniqueness of the\nunder-determined setting. We prove a convergence theorem that ensures the\nquasi-reversibility solution approximates the true solution as the noise and\nregularization parameters vanish. Numerical experiments in a fully\nthree-dimensional setting validate the method's performance. The reconstructed\ninitial electric field remains accurate even with $10\\%$ noise in the data,\ndemonstrating the robustness and applicability of the proposed approach to\nrealistic inverse electromagnetic problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20777v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20777v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基于时间维度还原法的麦克斯韦方程组初始数据反演重建", "tldr": "本文提出了一种基于时间维度还原和准可逆性方法，用于在非均匀各向异性介质中从边界测量重建麦克斯韦方程组的初始电场，即使数据存在噪声也能保持准确性。", "motivation": "在非均匀各向异性介质中，利用边界测量重建瞬态麦克斯韦方程组的初始电场，同时考虑实际约束，避免需要初始磁场数据和电荷密度信息。", "method": "采用麦克斯韦方程组的欠定公式。通过将电场投影到有限维勒让德多项式-指数基上，开发了时间维度还原方法，将原始时空问题转化为一系列空间系统。重建通过最小范数框架内的准可逆性方法进行。证明了一个收敛定理。", "result": "数值实验验证了方法的性能，重建的初始电场即使在数据中存在10%噪声的情况下也保持准确。", "conclusion": "所提出的时间维度还原和准可逆性方法对实际逆电磁问题具有鲁棒性和适用性，能够从有噪声的边界测量中有效重建初始电场。", "translation": "我们研究了非均匀各向异性介质中瞬态麦克斯韦方程组的一个反问题。目标是利用在有限时间间隔内的电场及其法向导数的边界测量，重建有界域$\\\\Omega \\subset \\\\mathbb{R}^3$中的初始电场$\\\\mathbf{E}_0$。考虑到实际约束，我们采用了麦克斯韦方程组的欠定公式，避免了对初始磁场数据和电荷密度信息的需求。为了解决这个反问题，我们开发了一种时间维度还原方法，通过将电场投影到时间上的有限维勒让德多项式-指数基上。这使得原始的时空问题被重新表述为一系列关于投影系数的空间系统。重建是利用准可逆性方法在最小范数框架内进行的，该框架适应了欠定设置固有的非唯一性。我们证明了一个收敛定理，确保当噪声和正则化参数消失时，准可逆性解逼近真实解。在完全三维设置下的数值实验验证了该方法的性能。即使数据中存在10%的噪声，重建的初始电场仍然准确，这表明了所提出方法对实际逆电磁问题的鲁棒性和适用性。", "summary": "本文研究了在非均匀和各向异性介质中重建麦克斯韦方程组初始电场的反问题。为了应对实际约束，作者采用了一种欠定公式，避免了对初始磁场和电荷密度数据的需求。通过将电场投影到勒让德多项式-指数基上，提出了一种时间维度还原方法，将原始的时空问题转化为一系列空间系统。重建过程利用最小范数框架下的准可逆性方法来处理固有的非唯一性。研究证明了一个收敛定理，并数值实验验证了该方法的有效性和对数据噪声的鲁棒性，突显了其在实际逆电磁问题中的应用潜力。", "keywords": "反问题, 麦克斯韦方程组, 时间维度还原, 准可逆性, 初始数据重建", "comments": "该论文的创新之处在于其结合了时间维度还原方法和准可逆性方法，专门用于处理欠定麦克斯韦方程组，而无需完整的初始数据。这使得该方法在实际应用中更具可行性，尤其是在初始条件难以完全获取的情况下。其在存在显著数据噪声下仍能保持准确性的鲁棒性是一个重要的优点。"}}
{"id": "2506.20747", "title": "Towards Probabilistic Question Answering Over Tabular Data", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20747v1", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20747v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "面向表格数据概率问答的研究", "tldr": "本文提出了一个新基准LUCARIO和一个框架，用于在大规模表格数据上进行概率问答，通过诱导贝叶斯网络和使用大型语言模型，显著优于现有基线。", "motivation": "现有针对表格数据的问答（QA）方法（如NL2SQL系统）在事实性问题上表现良好，但在需要不确定性推理的概率性问题上表现不足。", "method": "本文引入了一个新基准LUCARIO和一个框架。该方法从表格中诱导贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。", "result": "实证结果表明，与基线相比有显著改进，突出了混合符号-神经推理的优势。", "conclusion": "本文提出的方法通过结合贝叶斯网络和大型语言模型，有效解决了表格数据上的概率问答问题，并取得了显著的性能提升。", "translation": "当前针对表格数据的问答（QA）方法，例如NL2SQL系统，在答案可以直接从表格中检索的事实性问题上表现良好。然而，它们在需要不确定性推理的概率性问题上表现不足。在本文中，我们引入了一个新的基准LUCARIO和一个用于大型表格数据概率问答的框架。我们的方法从表格中诱导贝叶斯网络，将自然语言查询翻译成概率查询，并使用大型语言模型（LLMs）生成最终答案。实证结果表明，与基线相比有显著改进，突出了混合符号-神经推理的优势。", "summary": "本文针对现有表格数据问答系统在处理概率性问题上的不足，提出了一个新的基准LUCARIO和一个概率问答框架。该框架通过从表格中构建贝叶斯网络、将自然语言查询转换为概率查询，并利用大型语言模型生成答案，实现了混合符号-神经推理。实验证明，该方法在处理不确定性推理问题上显著优于现有基线。", "keywords": "概率问答, 表格数据, 贝叶斯网络, 大型语言模型, 混合推理", "comments": "本文的创新点在于提出了一个专门用于表格数据概率问答的框架和基准，填补了现有NL2SQL系统在处理不确定性推理方面的空白。其结合贝叶斯网络（符号推理）和大型语言模型（神经推理）的混合方法具有重要意义，为复杂问答提供了新的思路。"}}
{"id": "2506.20945", "title": "A Multi-Stage Framework for Multimodal Controllable Speech Synthesis", "authors": ["Rui Niu", "Weihao Wu", "Jie Chen", "Long Ma", "Zhiyong Wu"], "summary": "Controllable speech synthesis aims to control the style of generated speech\nusing reference input, which can be of various modalities. Existing face-based\nmethods struggle with robustness and generalization due to data quality\nconstraints, while text prompt methods offer limited diversity and fine-grained\ncontrol. Although multimodal approaches aim to integrate various modalities,\ntheir reliance on fully matched training data significantly constrains their\nperformance and applicability. This paper proposes a 3-stage multimodal\ncontrollable speech synthesis framework to address these challenges. For face\nencoder, we use supervised learning and knowledge distillation to tackle\ngeneralization issues. Furthermore, the text encoder is trained on both\ntext-face and text-speech data to enhance the diversity of the generated\nspeech. Experimental results demonstrate that this method outperforms\nsingle-modal baseline methods in both face based and text prompt based speech\nsynthesis, highlighting its effectiveness in generating high-quality speech.", "comment": "Accepted by ICME2025", "pdf_url": "http://arxiv.org/pdf/2506.20945v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.20945v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多阶段多模态可控语音合成框架", "tldr": "本文提出了一个三阶段多模态可控语音合成框架，解决了现有方法在鲁棒性、泛化性、多样性和细粒度控制方面的不足，并在实验中表现出优于单模态基线方法的性能。", "motivation": "现有基于面部的可控语音合成方法因数据质量限制而存在鲁棒性和泛化性问题；文本提示方法在多样性和细粒度控制方面有限；多模态方法过度依赖完全匹配的训练数据，限制了其性能和适用性。", "method": "本文提出了一个三阶段多模态可控语音合成框架。对于面部编码器，采用监督学习和知识蒸馏来解决泛化问题。此外，文本编码器在文本-面部和文本-语音数据上进行训练，以增强生成语音的多样性。", "result": "实验结果表明，该方法在基于面部和基于文本提示的语音合成方面均优于单模态基线方法。", "conclusion": "提出的多阶段多模态框架有效解决了现有可控语音合成方法的挑战，能够生成高质量的语音，并在多模态控制方面表现出优越性。", "translation": "可控语音合成旨在利用各种模态的参考输入来控制生成语音的风格。现有基于面部的方法由于数据质量限制而存在鲁棒性和泛化性问题，而文本提示方法提供的多样性和细粒度控制有限。尽管多模态方法旨在整合各种模态，但它们对完全匹配的训练数据的依赖显著限制了其性能和适用性。本文提出了一个三阶段多模态可控语音合成框架来解决这些挑战。对于面部编码器，我们使用监督学习和知识蒸馏来解决泛化问题。此外，文本编码器在文本-面部和文本-语音数据上进行训练，以增强生成语音的多样性。实验结果表明，该方法在基于面部和基于文本提示的语音合成方面均优于单模态基线方法，突显了其在生成高质量语音方面的有效性。", "summary": "本文提出了一种三阶段多模态可控语音合成框架，旨在克服现有单模态和多模态方法在鲁棒性、泛化性、多样性和数据依赖性方面的局限。该框架通过对面部编码器采用监督学习和知识蒸馏，并对文本编码器利用多源数据训练，有效提升了生成语音的质量和控制能力。实验证明其性能优于传统的单模态基线。", "keywords": "可控语音合成, 多模态, 多阶段框架, 知识蒸馏, 语音生成", "comments": "该论文的创新点在于提出了一个多阶段框架来整合不同模态的控制信息，并针对性地解决了现有方法在泛化性、多样性和数据依赖性上的不足。通过引入知识蒸馏和多源数据训练，有效提升了系统的实用性和性能。这对于提升可控语音合成的质量和应用范围具有重要意义。"}}
{"id": "2506.20915", "title": "ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models", "authors": ["Mina Namazi", "Alexander Nemecek", "Erman Ayday"], "summary": "As the deployment of large language models (LLMs) grows in sensitive domains,\nensuring the integrity of their computational provenance becomes a critical\nchallenge, particularly in regulated sectors such as healthcare, where strict\nrequirements are applied in dataset usage. We introduce ZKPROV, a novel\ncryptographic framework that enables zero-knowledge proofs of LLM provenance.\nIt allows users to verify that a model is trained on a reliable dataset without\nrevealing sensitive information about it or its parameters. Unlike prior\napproaches that focus on complete verification of the training process\n(incurring significant computational cost) or depend on trusted execution\nenvironments, ZKPROV offers a distinct balance. Our method cryptographically\nbinds a trained model to its authorized training dataset(s) through\nzero-knowledge proofs while avoiding proof of every training step. By\nleveraging dataset-signed metadata and compact model parameter commitments,\nZKPROV provides sound and privacy-preserving assurances that the result of the\nLLM is derived from a model trained on the claimed authorized and relevant\ndataset. Experimental results demonstrate the efficiency and scalability of the\nZKPROV in generating this proof and verifying it, achieving a practical\nsolution for real-world deployments. We also provide formal security\nguarantees, proving that our approach preserves dataset confidentiality while\nensuring trustworthy dataset provenance.", "comment": "12 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.20915v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20915v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ZKPROV：一种用于大型语言模型数据集溯源的零知识方法", "tldr": "ZKPROV 是一种零知识证明框架，用于验证大型语言模型是否使用授权数据集进行训练，同时保护隐私和降低计算成本。", "motivation": "随着大型语言模型（LLM）在敏感领域的部署日益增多，确保其计算溯源的完整性成为一个关键挑战，尤其是在医疗保健等受监管行业，数据集使用方面有严格要求。", "method": "ZKPROV 是一种新颖的加密框架，利用零知识证明来验证 LLM 是否使用可靠数据集进行训练，同时不泄露敏感信息。它通过零知识证明将训练好的模型与授权数据集进行加密绑定，避免了对每个训练步骤的证明，并通过数据集签名的元数据和紧凑的模型参数承诺提供可靠且保护隐私的保证。", "result": "实验结果表明 ZKPROV 在生成和验证证明方面具有效率和可扩展性，为实际部署提供了实用解决方案。该方法还提供了形式化的安全保障，证明其在确保可信数据集溯源的同时保护了数据集的机密性。", "conclusion": "ZKPROV 提供了一种高效、可扩展且保护隐私的零知识方法，用于验证大型语言模型的数据集溯源，解决了在敏感领域部署 LLM 的关键挑战，并提供了形式化的安全保障。", "translation": "随着大型语言模型（LLM）在敏感领域的部署日益增多，确保其计算溯源的完整性成为一个关键挑战，尤其是在医疗保健等受监管行业，数据集使用方面有严格要求。我们引入了 ZKPROV，一种新颖的加密框架，能够实现 LLM 溯源的零知识证明。它允许用户验证模型是否在可靠数据集上训练，同时不泄露有关数据集或其参数的敏感信息。与之前专注于训练过程完整验证（导致显著的计算成本）或依赖可信执行环境的方法不同，ZKPROV 提供了一种独特的平衡。我们的方法通过零知识证明将训练好的模型与其授权训练数据集进行加密绑定，同时避免了对每个训练步骤的证明。通过利用数据集签名的元数据和紧凑的模型参数承诺，ZKPROV 提供了可靠且保护隐私的保证，即 LLM 的结果源自于在声称的授权和相关数据集上训练的模型。实验结果表明 ZKPROV 在生成和验证此证明方面具有效率和可扩展性，为实际部署提供了实用解决方案。我们还提供了形式化的安全保障，证明我们的方法在确保可信数据集溯源的同时保护了数据集的机密性。", "summary": "ZKPROV 是一种新颖的零知识证明框架，旨在解决大型语言模型在敏感领域部署时的数据集溯源完整性挑战。它允许用户验证模型是否在授权数据集上训练，同时保护数据集和模型参数的隐私。与现有方法不同，ZKPROV 避免了对每个训练步骤的完整验证，而是通过加密绑定模型与数据集并利用数据集签名的元数据和紧凑模型参数承诺来实现。实验证明 ZKPROV 高效、可扩展，并提供形式化安全保障，确保了数据集的机密性和溯源的可信性。", "keywords": "零知识证明, 大型语言模型溯源, 数据集完整性, 隐私保护, 加密框架", "comments": "ZKPROV 的创新之处在于，它在验证的彻底性、计算成本和隐私保护之间取得了独特的平衡，这与之前专注于完整训练过程验证或依赖可信执行环境的方法不同。它通过零知识证明，在不泄露敏感信息的前提下，确保了大型语言模型数据集的溯源完整性，这对于 LLM 在受监管行业的部署具有重要意义。"}}
{"id": "2506.20804", "title": "Online Planning for Cooperative Air-Ground Robot Systems with Unknown Fuel Requirements", "authors": ["Ritvik Agarwal", "Behnoushsadat Hatami", "Alvika Gautam", "Parikshit Maini"], "summary": "We consider an online variant of the fuel-constrained UAV routing problem\nwith a ground-based mobile refueling station (FCURP-MRS), where targets incur\nunknown fuel costs. We develop a two-phase solution: an offline heuristic-based\nplanner computes initial UAV and UGV paths, and a novel online planning\nalgorithm that dynamically adjusts rendezvous points based on real-time fuel\nconsumption during target processing. Preliminary Gazebo simulations\ndemonstrate the feasibility of our approach in maintaining UAV-UGV path\nvalidity, ensuring mission completion. Link to video:\nhttps://youtu.be/EmpVj-fjqNY", "comment": "Submitted to RSS (MRS Workshop)", "pdf_url": "http://arxiv.org/pdf/2506.20804v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20804v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "未知燃料需求下空地机器人协同系统的在线规划", "tldr": "本文提出了一种解决带有移动加油站的燃料受限无人机路径规划问题的在线方法，该方法在目标处理过程中根据实时燃料消耗动态调整会合点。", "motivation": "解决燃料受限无人机路径规划问题，特别是当目标燃料成本未知时，需要一种能够动态调整规划的在线方法。", "method": "提出了一种两阶段解决方案：离线启发式规划器计算初始无人机和无人车路径，以及一种新颖的在线规划算法，该算法根据目标处理过程中的实时燃料消耗动态调整会合点。", "result": "初步的Gazebo仿真证明了该方法在保持无人机-无人车路径有效性并确保任务完成方面的可行性。", "conclusion": "该方法在解决未知燃料需求下空地机器人协同系统的在线规划问题上是可行且有效的，能够确保任务完成。", "translation": "我们考虑了带有地面移动加油站的燃料受限无人机路径规划问题（FCURP-MRS）的在线变体，其中目标会产生未知的燃料成本。我们开发了一个两阶段解决方案：一个离线启发式规划器计算初始无人机和无人车路径，以及一个新颖的在线规划算法，该算法在目标处理过程中根据实时燃料消耗动态调整会合点。初步的Gazebo仿真证明了我们方法在保持无人机-无人车路径有效性、确保任务完成方面的可行性。视频链接：https://youtu.be/EmpVj-fjqNY", "summary": "本文针对带有移动加油站且目标燃料成本未知的燃料受限无人机路径规划问题（FCURP-MRS）提出了一个在线解决方案。该方案包含一个离线启发式规划器用于初始路径计算，以及一个新颖的在线算法，该算法能够根据实时燃料消耗动态调整无人机和无人车的会合点。初步的Gazebo仿真结果验证了该方法在维持路径有效性和确保任务完成方面的可行性。", "keywords": "在线规划, 空地机器人系统, 燃料受限无人机, 移动加油站, 动态调整", "comments": "该论文的创新点在于提出了一个两阶段的在线规划方法来处理未知燃料成本的空地机器人协同系统，特别是在实时燃料消耗下动态调整会合点的策略。这对于实际应用中资源受限且环境不确定的机器人系统具有重要意义。"}}
{"id": "2506.20834", "title": "Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher", "authors": ["Tomas Gallo Aquino", "Victoria Liu", "Habiba Azab", "Raissa Mathura", "Andrew J Watrous", "Eleonora Bartoli", "Benjamin Y Hayden", "Paul Sajda", "Sameer A Sheth", "Nuttida Rungratsameetaweemana"], "summary": "Transfer learning enhances the training of novel sensory and decision models\nby employing rich feature representations from large, pre-trained teacher\nmodels. Cognitive neuroscience shows that the human brain creates\nlow-dimensional, abstract representations for efficient sensorimotor coding.\nImportantly, the brain can learn these representations with significantly fewer\ndata points and less computational power than artificial models require. We\nintroduce Brain2Model Transfer Learning (B2M), a framework where neural\nactivity from human sensory and decision-making tasks acts as the teacher model\nfor training artificial neural networks. We propose two B2M strategies: (1)\nBrain Contrastive Transfer, which aligns brain activity and network activations\nthrough a contrastive objective; and (2) Brain Latent Transfer, which projects\nlatent dynamics from similar cognitive tasks onto student networks via\nsupervised regression of brain-derived features. We validate B2M in\nmemory-based decision-making with a recurrent neural network and scene\nreconstruction for autonomous driving with a variational autoencoder. The\nresults show that student networks benefiting from brain-based transfer\nconverge faster and achieve higher predictive accuracy than networks trained in\nisolation. Our findings indicate that the brain's representations are valuable\nfor artificial learners, paving the way for more efficient learning of complex\ndecision-making representations, which would be costly or slow through purely\nartificial training.", "comment": "15 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.20834v1", "categories": ["cs.NE", "cs.ET", "q-bio.NC"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.20834v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "Brain2Model 迁移：以人类神经活动作为教师训练感觉和决策模型", "tldr": "提出Brain2Model迁移学习框架，利用人类神经活动作为“教师”来训练人工神经网络，使其收敛更快、预测准确性更高。", "motivation": "现有的人工模型在学习复杂特征表示时需要大量数据和计算资源，而人脑能以更少的数据和计算能力学习高效的低维抽象表示。本研究旨在利用人脑的这种高效学习能力。", "method": "引入Brain2Model Transfer Learning (B2M) 框架，将人类感觉和决策任务中的神经活动作为教师模型来训练人工神经网络。提出了两种B2M策略：1) Brain Contrastive Transfer（通过对比目标对齐脑活动和网络激活）；2) Brain Latent Transfer（通过对脑源性特征进行监督回归，将来自类似认知任务的潜在动态投射到学生网络）。在基于记忆的决策任务（使用循环神经网络）和自动驾驶场景重建（使用变分自编码器）中验证了B2M。", "result": "受益于脑源性迁移的学生网络比单独训练的网络收敛更快，并实现了更高的预测准确性。", "conclusion": "大脑的表征对人工学习器很有价值，为更有效地学习复杂的决策表示铺平了道路，而这些表示通过纯粹的人工训练将是昂贵或缓慢的。", "translation": "迁移学习通过使用来自大型预训练教师模型的丰富特征表示，增强了新型感觉和决策模型的训练。认知神经科学表明，人脑为高效的感觉运动编码创建了低维、抽象的表示。重要的是，与人工智能模型所需的相比，人脑可以用明显更少的数据点和更少的计算能力学习这些表示。我们引入了Brain2Model迁移学习（B2M），这是一个框架，其中来自人类感觉和决策任务的神经活动充当训练人工神经网络的教师模型。我们提出了两种B2M策略：（1）脑对比迁移，它通过对比目标对齐脑活动和网络激活；（2）脑潜在迁移，它通过对脑源性特征的监督回归，将来自类似认知任务的潜在动态投射到学生网络。我们在基于记忆的决策任务中使用循环神经网络，以及在自动驾驶场景重建中使用变分自编码器验证了B2M。结果表明，受益于脑源性迁移的学生网络比单独训练的网络收敛更快，并获得了更高的预测准确性。我们的发现表明，大脑的表征对人工学习器很有价值，为更有效地学习复杂的决策表示铺平了道路，而这些表示通过纯粹的人工训练将是昂贵或缓慢的。", "summary": "本文提出了Brain2Model迁移学习（B2M）框架，利用人类感觉和决策任务中的神经活动作为“教师”来训练人工神经网络。该框架包含两种策略：脑对比迁移和脑潜在迁移。实验结果表明，与单独训练的网络相比，受益于脑源性迁移的学生网络收敛更快且预测准确性更高，证明了大脑表征对人工学习器的价值，能更高效地学习复杂决策表示。", "keywords": "迁移学习, 神经活动, 人工神经网络, 决策模型, 表征学习", "comments": "这项研究的创新之处在于利用人类神经活动作为教师信号来指导人工模型的训练，这与传统的预训练模型作为教师的方法不同。它利用了人脑在学习高效表示方面的优势，为解决人工模型训练中数据和计算效率问题提供了新思路，对于推动类脑AI的发展具有重要意义。"}}
{"id": "2506.20759", "title": "Agile Management for Machine Learning: A Systematic Mapping Study", "authors": ["Lucas Romao", "Hugo Villamizar", "Romeu Oliveira", "Silvio Alonso", "Marcos Kalinowski"], "summary": "[Context] Machine learning (ML)-enabled systems are present in our society,\ndriving significant digital transformations. The dynamic nature of ML\ndevelopment, characterized by experimental cycles and rapid changes in data,\nposes challenges to traditional project management. Agile methods, with their\nflexibility and incremental delivery, seem well-suited to address this\ndynamism. However, it is unclear how to effectively apply these methods in the\ncontext of ML-enabled systems, where challenges require tailored approaches.\n[Goal] Our goal is to outline the state of the art in agile management for\nML-enabled systems. [Method] We conducted a systematic mapping study using a\nhybrid search strategy that combines database searches with backward and\nforward snowballing iterations. [Results] Our study identified 27 papers\npublished between 2008 and 2024. From these, we identified eight frameworks and\ncategorized recommendations and practices into eight key themes, such as\nIteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable\nModel. The main challenge identified across studies was accurate effort\nestimation for ML-related tasks. [Conclusion] This study contributes by mapping\nthe state of the art and identifying open gaps in the field. While relevant\nwork exists, more robust empirical evaluation is still needed to validate these\ncontributions.", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20759v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20759v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "机器学习的敏捷管理：一项系统性图谱研究", "tldr": "本研究通过系统性图谱研究，概述了机器学习系统敏捷管理的现状，识别了相关框架、实践主题和主要挑战，并指出了未来研究方向。", "motivation": "机器学习（ML）系统开发具有动态性，传统项目管理面临挑战。敏捷方法看似适用，但如何有效应用于ML系统尚不明确。本研究旨在概述ML系统敏捷管理的最新进展。", "method": "本研究采用系统性图谱研究方法，结合数据库检索以及向前和向后滚雪球迭代的混合搜索策略。", "result": "研究识别了2008年至2024年间发布的27篇论文，从中确定了八个框架，并将推荐和实践归类为八个关键主题（如迭代灵活性、创新性ML特定工件和最小可行模型）。研究发现的主要挑战是ML相关任务的准确工作量估算。", "conclusion": "本研究通过绘制该领域的最新图谱并识别开放性空白做出了贡献。尽管存在相关工作，但仍需要更稳健的实证评估来验证这些贡献。", "translation": "[背景] 机器学习（ML）赋能的系统存在于我们的社会中，推动着重大的数字化转型。ML开发的动态性，以实验周期和数据快速变化为特征，给传统项目管理带来了挑战。敏捷方法以其灵活性和增量交付，似乎非常适合应对这种动态性。然而，目前尚不清楚如何在ML赋能系统（其中挑战需要量身定制的方法）的背景下有效应用这些方法。\n[目标] 我们的目标是概述ML赋能系统敏捷管理的最新技术水平。\n[方法] 我们进行了一项系统性图谱研究，采用了结合数据库搜索以及向后和向前滚雪球迭代的混合搜索策略。\n[结果] 我们的研究识别了2008年至2024年间发布的27篇论文。从中，我们识别了八个框架，并将推荐和实践归类为八个关键主题，例如迭代灵活性、创新性ML特定工件和最小可行模型。研究中发现的主要挑战是ML相关任务的准确工作量估算。\n[结论] 本研究通过绘制该领域的最新图谱并识别开放性空白做出了贡献。尽管存在相关工作，但仍需要更稳健的实证评估来验证这些贡献。", "summary": "本系统性图谱研究旨在概述机器学习（ML）系统敏捷管理的最新进展，以应对ML开发动态性带来的项目管理挑战。研究通过混合搜索策略识别了27篇相关论文，归纳出八个框架和八个关键实践主题，并指出ML任务工作量估算是一个主要挑战。研究贡献在于映射了该领域的现状并指出了未来实证研究的需求。", "keywords": "敏捷管理, 机器学习, 系统性图谱研究, 项目管理, 软件工程", "comments": "该研究通过系统性图谱研究，为机器学习项目的敏捷管理提供了全面的概述，识别了现有框架和实践，并突出了关键挑战（如工作量估算）。其重要性在于为从业者提供了指导，并为未来的研究指明了方向，特别是强调了实证验证的必要性。"}}
{"id": "2506.20884", "title": "\"TikTok, Do Your Thing\": User Reactions to Social Surveillance in the Public Sphere", "authors": ["Meira Gilbert", "Miranda Wei", "Lindah Kotut"], "summary": "''TikTok, Do Your Thing'' is a viral trend where users attempt to identify\nstrangers they see in public via information crowd-sourcing. The trend started\nas early as 2021 and users typically engage with it for romantic purposes\n(similar to a ''Missed Connections'' personal advertisement). This practice\nincludes acts of surveillance and identification in the public sphere, although\nby peers rather than governments or corporations. To understand users'\nreactions to this trend we conducted a qualitative analysis of 60 TikTok videos\nand 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were\nsuccessfully identified. We also find that while there were comments expressing\ndisapproval (n=310), more than double the number expressed support (n=883).\nSupportive comments demonstrated genuine interest and empathy, reflecting\nevolving conceptions of community and algorithmic engagement. On the other\nhand, disapproving comments highlighted concerns about inappropriate\nrelationships, stalking, consent, and gendered double standards. We discuss\nthese insights in relation to the normalization of interpersonal surveillance,\nonline stalking, and as an evolution of social surveillance to offer a new\nperspective on user perceptions surrounding interpersonal surveillance and\nidentification in the public sphere.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20884v1", "categories": ["cs.HC", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20884v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "“TikTok，做你的事”：用户对公共领域社会监视的反应", "tldr": "一项对TikTok上“做你的事”趋势的研究，分析了用户对通过众包识别陌生人的反应，发现支持者多于反对者，并探讨了人际监视的常态化。", "motivation": "了解用户对TikTok上通过众包识别陌生人这一病毒式趋势的反应。", "method": "对60个TikTok视频和1,901条用户评论进行了定性分析。", "result": "在审查的60个视频中，成功识别了19个人。支持性评论（883条）的数量是反对性评论（310条）的两倍多。支持性评论表现出兴趣和同情，反映了社区和算法参与概念的演变；反对性评论则关注不当关系、跟踪、同意和性别双重标准。", "conclusion": "研究结果讨论了人际监视、在线跟踪的常态化，以及社会监视的演变，为理解用户对公共领域人际监视和识别的看法提供了新视角。", "translation": "“TikTok，做你的事”是一个病毒式趋势，用户试图通过信息众包来识别他们在公共场合看到的陌生人。这个趋势早在2021年就开始了，用户通常出于浪漫目的（类似于“寻人启事”个人广告）参与其中。这种做法包括在公共领域的监视和识别行为，尽管是由同伴而非政府或公司进行的。为了理解用户对这一趋势的反应，我们对60个TikTok视频和1,901条用户评论进行了定性分析。在审查的60个视频中，我们发现有19个人被成功识别。我们还发现，虽然有表达反对的评论（n=310），但表达支持的评论数量（n=883）是其两倍多。支持性评论表现出真实的兴趣和同情，反映了社区和算法参与概念的演变。另一方面，反对性评论则强调了对不当关系、跟踪、同意和性别双重标准的担忧。我们结合人际监视、在线跟踪的常态化以及社会监视的演变来讨论这些见解，从而为用户对公共领域人际监视和识别的看法提供新视角。", "summary": "本研究对TikTok上流行的“做你的事”趋势进行了定性分析，该趋势涉及通过众包识别公共场合的陌生人。通过分析60个视频和1,901条用户评论，研究发现虽然有反对声音，但支持该行为的用户数量更多，反映了社群和算法参与的新观念。研究讨论了这种现象对人际监视常态化和在线跟踪的影响，并提供了对公共领域社会监视用户认知的独特视角。", "keywords": "TikTok, 社会监视, 用户反应, 众包, 身份识别", "comments": "本研究通过分析TikTok上一个具体的病毒式趋势，深入探讨了新兴的“同伴监视”现象，而非传统的政府或企业监视，这为社会监视研究提供了一个新颖的视角。其定性分析结合了视频和大量评论，为理解用户对此类行为的复杂态度提供了丰富的实证数据。"}}
{"id": "2506.21414", "title": "Accelerating GNN Training through Locality-aware Dropout and Merge", "authors": ["Gongjian Sun", "Mingyu Yan", "Dengke Han", "Runzhen Xue", "Duo Wang", "Xiaochun Ye", "Dongrui Fan"], "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in graph\nlearning and are widely adopted across various critical domains. However, the\nirregular connectivity between vertices leads to inefficient neighbor\naggregation, resulting in substantial irregular and coarse-grained DRAM\naccesses. This lack of data locality presents significant challenges for\nexecution platforms, ultimately degrading performance. While previous\naccelerator designs have leveraged on-chip memory and data access scheduling\nstrategies to address this issue, they still inevitably access features at\nirregular addresses from DRAM. In this work, we propose LiGNN, a hardware-based\nsolution that improves data locality by applying dropout and merge techniques\nduring neighbor aggregation to accelerate GNN training. Unlike conventional\nalgorithm-level dropout methods that primarily aim to improve accuracy while\noverlooking hardware costs, LiGNN introduces a locality-aware feature dropout\nmechanism. This approach selectively drops node features with data locality\nawareness, effectively reducing irregular DRAM accesses without compromising\nmodel accuracy. Moreover, by leveraging detailed knowledge of memory layout and\norganization-including critical alignment constraints-LiGNN strategically\nmerges memory accesses during neighbor aggregation at the DRAM row level,\nguided by GNN-level semantics. This optimization significantly improves data\nlocality with minimal additional cost. Under the commonly adopted 0.5 dropout\nrate, LiGNN outperforms state-of-the-art methods, delivering a 1.48~3.02x\nspeedup, reducing DRAM accesses by 34%~55%, and lowering DRAM row activations\nby 59%~82%, all while maintaining model accuracy.", "comment": "under review in TPDS. extend version of DATE 2025", "pdf_url": "http://arxiv.org/pdf/2506.21414v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21414v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过局部性感知Dropout和合并加速GNN训练", "tldr": "LiGNN通过局部性感知特征丢弃和内存访问合并，显著加速GNN训练，同时减少DRAM访问并保持模型精度。", "motivation": "图神经网络（GNN）训练中，顶点间的不规则连接导致低效的邻居聚合，产生大量不规则且粗粒度的DRAM访问，缺乏数据局部性，从而降低性能。现有加速器虽利用片上内存和调度策略，但仍不可避免地从DRAM不规则地址访问特征。", "method": "本文提出LiGNN，一种基于硬件的解决方案，通过在邻居聚合过程中应用局部性感知特征丢弃（Locality-aware Feature Dropout）和内存访问合并（Merge）技术来提高数据局部性。局部性感知特征丢弃选择性地丢弃节点特征，以减少不规则DRAM访问而不损害模型精度。通过利用内存布局和组织知识，LiGNN在DRAM行级别战略性地合并邻居聚合期间的内存访问。", "result": "在0.5的常用丢弃率下，LiGNN比现有最佳方法提速1.48~3.02倍，DRAM访问量减少34%~55%，DRAM行激活量降低59%~82%，同时保持模型精度。", "conclusion": "LiGNN通过局部性感知的数据处理和内存访问优化，显著加速了GNN训练，有效解决了GNN训练中数据局部性差和DRAM访问效率低的问题。", "translation": "图神经网络（GNN）在图学习中取得了显著成功，并被广泛应用于各种关键领域。然而，顶点间不规则的连接导致低效的邻居聚合，从而产生大量的非规则和粗粒度的DRAM访问。这种数据局部性的缺乏给执行平台带来了巨大挑战，最终降低了性能。虽然之前的加速器设计利用片上内存和数据访问调度策略来解决这个问题，但它们仍然不可避免地从DRAM的不规则地址访问特征。在这项工作中，我们提出了LiGNN，一种基于硬件的解决方案，通过在邻居聚合过程中应用丢弃和合并技术来提高数据局部性，从而加速GNN训练。与主要旨在提高精度而忽视硬件成本的传统算法级丢弃方法不同，LiGNN引入了一种局部性感知特征丢弃机制。这种方法有选择地丢弃具有数据局部性意识的节点特征，有效地减少了不规则的DRAM访问，同时不损害模型精度。此外，通过利用对内存布局和组织（包括关键对齐约束）的详细了解，LiGNN在GNN级语义的指导下，在DRAM行级别战略性地合并邻居聚合期间的内存访问。这种优化以最小的额外成本显著提高了数据局部性。在常用的0.5丢弃率下，LiGNN优于现有最佳方法，实现了1.48~3.02倍的加速，DRAM访问量减少34%~55%，DRAM行激活量降低59%~82%，所有这些都保持了模型精度。", "summary": "本研究提出LiGNN，一种硬件加速方案，旨在解决图神经网络（GNN）训练中因不规则连接导致的数据局部性差和DRAM访问效率低的问题。LiGNN通过引入局部性感知特征丢弃机制，选择性地减少不规则DRAM访问，并利用内存布局知识在DRAM行级别战略性合并内存访问。实验结果表明，LiGNN在保持模型精度的前提下，显著提升了GNN训练速度，并大幅减少了DRAM访问和行激活。", "keywords": "GNN训练, 数据局部性, Dropout, 内存合并, 硬件加速", "comments": "LiGNN的创新之处在于将局部性感知引入到Dropout机制中，使其不再仅仅是提高模型精度，而是同时优化硬件效率。此外，在DRAM行级别进行内存访问合并也是一个巧妙的设计，直接针对GNN训练中的内存访问瓶颈。这种软硬件协同设计的方法对于加速图计算具有重要意义。"}}
{"id": "2506.21078", "title": "Constant Modulus Waveforms for IoT-Centric Integrated Sensing and Communications", "authors": ["Tian Han", "Shalanika Dayarathna", "Rajitha Senanayake", "Peter Smith", "Aryan Kaushik", "Alain Mourad", "Richard A. Stirling-Gallacher", "Jamie Evans"], "summary": "Integrated sensing and communications (ISAC) is considered a key enabler to\nsupport application scenarios such as the Internet-of-Things (IoT) in which\nboth communications and sensing play significant roles. Multi-carrier\nwaveforms, such as orthogonal frequency division multiplexing (OFDM), have been\nconsidered as good candidates for ISAC due to their high communications data\nrate and good time bandwidth property for sensing. Nevertheless, their high\npeak-to-average-power-ratio (PAPR) values lead to either performance\ndegradation or an increase in system complexity. This can make OFDM unsuitable\nfor IoT applications with insufficient resources in terms of power, system\ncomplexity, hardware size or cost. This article provides IoT-centric constant\nmodulus waveform designs that leverage the advantage of unit PAPR and thus are\nmore suitable in resource-limited scenarios. More specifically, several\nsingle-carrier frequency and/or phase-modulated waveforms are considered. A\ncomprehensive discussion on their radar sensing and communications performance\nis conducted based on performance metrics, including the radar ambiguity\nfunction, the bandwidth property, the data rate, and the communications\nreceiver complexity.", "comment": "Submitted for publication to IEEE Communications Standards Magazine", "pdf_url": "http://arxiv.org/pdf/2506.21078v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21078v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "物联网中心集成传感与通信的恒模波形", "tldr": "本文针对物联网中资源受限的集成传感与通信(ISAC)场景，提出并分析了低PAPR的恒模波形，以克服传统多载波波形（如OFDM）的高PAPR问题。", "motivation": "传统多载波波形（如OFDM）在集成传感与通信(ISAC)中具有高数据率和良好的时带宽特性，但其高高峰均功率比(PAPR)导致性能下降或系统复杂性增加，这使得它们不适用于在功率、系统复杂性、硬件尺寸或成本方面资源不足的物联网应用。", "method": "本文提出了以物联网为中心的恒模波形设计，利用其单位PAPR的优点，使其更适用于资源受限的场景。具体考虑了几种单载波频率和/或相位调制波形，并基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等性能指标，对其雷达传感和通信性能进行了全面讨论。", "result": "对所提出的恒模波形在雷达传感和通信性能方面进行了全面讨论，并基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等性能指标进行了分析。", "conclusion": "恒模波形因其单位PAPR特性，比高PAPR的多载波波形更适合资源受限的物联网集成传感与通信应用。", "translation": "集成传感与通信（ISAC）被认为是支持物联网（IoT）等应用场景的关键使能技术，在这些场景中，通信和传感都发挥着重要作用。多载波波形，如正交频分复用（OFDM），因其高通信数据速率和良好的传感时带宽特性，被认为是ISAC的良好候选。然而，其高峰均功率比（PAPR）值导致性能下降或系统复杂性增加。这可能使OFDM不适用于在功率、系统复杂性、硬件尺寸或成本方面资源不足的物联网应用。本文提供了以物联网为中心的恒模波形设计，利用了单位PAPR的优势，因此更适用于资源受限的场景。更具体地说，本文考虑了几种单载波频率和/或相位调制波形。基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等性能指标，对它们的雷达传感和通信性能进行了全面讨论。", "summary": "本文提出了一种针对物联网集成传感与通信(ISAC)的恒模波形设计，旨在解决传统多载波波形（如OFDM）在高PAPR下不适用于资源受限物联网环境的问题。通过利用恒模波形单位PAPR的优势，该研究探讨了几种单载波频率和/或相位调制波形，并从雷达传感和通信性能两方面，基于雷达模糊函数、带宽特性、数据速率和通信接收器复杂性等关键指标进行了全面评估。", "keywords": "恒模波形, 集成传感与通信, 物联网, 低PAPR, 单载波", "comments": "本文的创新点在于针对物联网资源受限的特点，提出了恒模波形作为ISAC的替代方案，有效解决了OFDM等波形的高PAPR问题。这对于推动ISAC在实际物联网部署中的应用具有重要意义。"}}
{"id": "2506.20695", "title": "Malicious earworms and useful memes, how the far-right surfs on TikTok audio trends", "authors": ["Marloes Geboers", "Marcus Bösch"], "summary": "With its features of remix, TikTok is the designated platform for meme-making\nand dissemination. Creative combinations of video, emoji, and filters allow for\nan endless stream of memes and trends animated by sound. The platform has\nfocused its moderation on upholding physical safety, hence investing in the\ndetection of harmful challenges. In response to the DSA, TikTok implemented\nopt-outs for personalized feeds and features allowing users to report illegal\ncontent. At the same time, the platform remains subject to scrutiny. Centering\non the role of sound and its intersections with ambiguous memes, the presented\nresearch probed right-wing extremist formations relating to the 2024 German\nstate elections. The analysis evidences how the TikTok sound infrastructure\naffords a sustained presence of xenophobic content, often cloaked through\nvernacular modes of communication. These cloaking practices benefit from a\nsound infrastructure that affords the ongoing posting of user-generated sounds\nthat instantly spread through the use-this-sound button. Importantly, these\nsounds are often not clearly recognizable as networkers of extremist content.\nSongs that do contain hateful lyrics are not eligible for personalized feeds,\nhowever, they remain online where they profit from intersecting with benign\nmeme trends, rendering them visible in search results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20695v1", "categories": ["cs.SI", "cs.CY"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20695v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "恶意耳虫和有用模因：极右翼如何利用TikTok音频趋势", "tldr": "本研究探讨了极右翼势力如何利用TikTok的音频趋势和模因传播仇外内容，即使这些内容被伪装或不符合个性化推送条件，仍能通过声音基础设施和良性模因趋势保持可见性。", "motivation": "研究旨在探究声音在TikTok平台上的作用及其与模糊模因的交叉，以揭示与2024年德国州选举相关的极右翼极端主义形成。", "method": "该研究分析了TikTok的声音基础设施如何支持仇外内容的持续存在，以及这些内容如何通过本土化交流方式进行伪装。", "result": "分析表明，TikTok的声音基础设施为仇外内容提供了持续的存在空间，这些内容常通过本土化交流方式伪装。这些伪装做法得益于声音基础设施，允许用户生成的声音通过“使用此声音”按钮即时传播。这些声音通常不易被识别为极端主义内容的传播者。即使含有仇恨歌词的歌曲不符合个性化推送条件，它们仍在线上存在，并与良性模因趋势交叉，使其在搜索结果中可见。", "conclusion": "TikTok的声音基础设施使得仇外内容能够持续存在，即使被伪装或不符合个性化推送条件，也能通过与良性模因趋势的交叉保持可见性，这揭示了平台在内容审核方面的挑战。", "translation": "凭借其混音功能，TikTok是模因制作和传播的指定平台。视频、表情符号和滤镜的创意组合带来了由声音驱动的无尽模因和趋势。该平台已将其内容审核重点放在维护人身安全上，因此投资于有害挑战的检测。为响应《数字服务法案》（DSA），TikTok实施了个性化推送的选择退出功能，并允许用户举报非法内容。与此同时，该平台仍受到严格审查。本研究以声音的作用及其与模糊模因的交叉为中心，探讨了与2024年德国州选举相关的极右翼极端主义形成。分析表明，TikTok的声音基础设施如何使得仇外内容持续存在，这些内容通常通过本土化的交流方式进行伪装。这些伪装做法得益于声音基础设施，该基础设施使得用户生成的声音能够持续发布，并通过“使用此声音”按钮即时传播。重要的是，这些声音通常不易被明确识别为极端主义内容的传播者。确实含有仇恨歌词的歌曲不符合个性化推送条件，但它们仍在线上存在，并受益于与良性模因趋势的交叉，使其在搜索结果中可见。", "summary": "本研究探讨了极右翼势力如何利用TikTok的音频趋势和模因传播仇外内容。研究发现，TikTok的声音基础设施使得仇外内容能够持续存在，这些内容常通过本土化交流方式伪装，且不易被识别为极端主义内容。即使含有仇恨歌词的歌曲不符合个性化推送条件，它们仍能通过与良性模因趋势的交叉在搜索结果中保持可见性，突显了平台在内容审核方面的挑战。", "keywords": "TikTok, 极右翼, 音频趋势, 仇外内容, 模因", "comments": "该论文揭示了TikTok平台上极右翼内容传播的一个隐蔽且有效的机制，即利用音频趋势和模因进行伪装传播。其创新之处在于关注了声音作为一种内容载体的特殊性，以及这种载体如何被用于规避平台审核。这对于理解数字平台上的极端主义传播模式及其治理具有重要意义。"}}
{"id": "2506.21406", "title": "Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees", "authors": ["Tommaso Bonato", "Daniele De Sensi", "Salvatore Di Girolamo", "Abdulla Bataineh", "David Hewson", "Duncan Roweth", "Torsten Hoefler"], "summary": "Network latency severely impacts the performance of applications running on\nsupercomputers. Adaptive routing algorithms route packets over different\navailable paths to reduce latency and improve network utilization. However, if\na switch routes packets belonging to the same network flow on different paths,\nthey might arrive at the destination out-of-order due to differences in the\nlatency of these paths. For some transport protocols like TCP, QUIC, and RoCE,\nout-of-order (OOO) packets might cause large performance drops or significantly\nincrease CPU utilization. In this work, we propose flowcut switching, a new\nadaptive routing algorithm that provides high-performance in-order packet\ndelivery. Differently from existing solutions like flowlet switching, which are\nbased on the assumption of bursty traffic and that might still reorder packets,\nflowcut switching guarantees in-order delivery under any network conditions,\nand is effective also for non-bursty traffic, as it is often the case for RDMA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21406v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.21406v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "流切交换：高性能自适应路由与顺序交付保证", "tldr": "提出了一种名为“流切交换”的新型自适应路由算法，旨在解决现有自适应路由可能导致数据包乱序的问题，确保在任何网络条件下都能高性能地按序交付数据包，即使是非突发流量也适用。", "motivation": "现有自适应路由算法虽然能降低延迟和提高网络利用率，但可能导致同一数据流的数据包乱序到达，这会严重影响TCP、QUIC和RoCE等传输协议的性能并增加CPU利用率。", "method": "提出了一种新的自适应路由算法——流切交换（flowcut switching）。", "result": "流切交换能够提供高性能的按序数据包交付，在任何网络条件下都能保证按序交付，并且对非突发流量（如RDMA）也有效。", "conclusion": "流切交换是一种能够保证数据包按序交付的高性能自适应路由算法，解决了现有方案在数据包乱序方面的不足，并适用于各种流量模式。", "translation": "网络延迟严重影响超级计算机上运行应用程序的性能。自适应路由算法通过不同的可用路径路由数据包，以减少延迟并提高网络利用率。然而，如果一个交换机将属于同一网络流的数据包路由到不同的路径，由于这些路径的延迟差异，它们可能会乱序到达目的地。对于TCP、QUIC和RoCE等某些传输协议，乱序（OOO）数据包可能导致性能大幅下降或显著增加CPU利用率。在这项工作中，我们提出了流切交换（flowcut switching），一种提供高性能按序数据包交付的新型自适应路由算法。与现有的解决方案（如基于突发流量假设且仍可能重排序数据包的流片交换）不同，流切交换在任何网络条件下都能保证按序交付，并且对于非突发流量（如RDMA常见情况）也有效。", "summary": "本文提出了一种名为“流切交换”的新型自适应路由算法，旨在解决现有自适应路由在降低网络延迟和提高利用率时可能导致数据包乱序的问题。流切交换确保在任何网络条件下都能实现高性能的按序数据包交付，并且有效支持包括RDMA在内的非突发流量，克服了现有流片交换等方案的局限性。", "keywords": "自适应路由, 流切交换, 按序交付, 网络性能, 超级计算机", "comments": "这篇论文的创新点在于提出了一种新的自适应路由算法，该算法能够解决传统自适应路由可能导致的数据包乱序问题，尤其强调了在任何网络条件下和非突发流量下的按序交付保证，这对高性能计算和RDMA等应用至关重要。其重要性在于提升了超级计算机网络的可靠性和性能。"}}
{"id": "2506.20756", "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.20756v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20756v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "StereoDiff：立体扩散协同用于视频深度估计", "tldr": "StereoDiff是一种新型的两阶段视频深度估计算法，它结合了立体匹配和视频深度扩散，分别处理视频中的静态和动态区域，实现了最先进的性能。", "motivation": "现有的视频深度估计算法通常是图像深度估计的简单扩展，通过对预训练的视频扩散模型进行微调。然而，作者认为视频深度估计并非图像深度估计的简单延伸，因为视频中动态和静态区域的时间一致性要求存在根本性差异。", "method": "本文提出StereoDiff，一个两阶段的视频深度估计器。它将立体匹配（主要用于静态区域）与视频深度扩散（用于保持动态区域的深度平滑过渡）相结合。通过频域分析，数学上证明了立体匹配和视频深度扩散如何提供互补的优势，凸显了它们协同作用的有效性。", "result": "在零样本、真实世界、动态视频深度基准（包括室内和室外）上的实验结果表明，StereoDiff达到了最先进的性能（SoTA），展示了其在视频深度估计方面卓越的一致性和准确性。", "conclusion": "StereoDiff通过结合立体匹配和视频深度扩散的互补优势，解决了视频深度估计中静态和动态区域时间一致性的不同需求，从而在零样本动态视频深度估计任务上取得了最先进的性能。", "translation": "最近的视频深度估计方法通过遵循图像深度估计的范式，即通常使用大量数据对预训练的视频扩散模型进行微调，取得了很好的性能。然而，我们认为视频深度估计并非图像深度估计的简单延伸。视频中动态和静态区域的时间一致性要求存在根本性差异。静态区域（通常是背景）中一致的视频深度可以通过跨所有帧的立体匹配更有效地实现，这提供了更强的全局3D线索。而动态区域的一致性仍然需要从大规模视频深度数据中学习，以确保平滑过渡，因为三角测量约束会被违反。基于这些见解，我们引入了StereoDiff，这是一种两阶段的视频深度估计器，它协同立体匹配（主要用于静态区域）和视频深度扩散（用于保持动态区域的深度平滑过渡）。我们通过频域分析，数学上证明了立体匹配和视频深度扩散如何提供互补的优势，凸显了它们协同作用在捕捉两者优点方面的有效性。在零样本、真实世界、动态视频深度基准（包括室内和室外）上的实验结果表明，StereoDiff达到了最先进的性能（SoTA），展示了其在视频深度估计方面卓越的一致性和准确性。", "summary": "StereoDiff是一种新颖的两阶段视频深度估计算法，旨在解决现有方法在处理视频中静态和动态区域时间一致性方面的不足。该方法创新性地将立体匹配（用于静态背景以提供强全局3D线索）与视频深度扩散模型（用于动态区域以确保平滑过渡）相结合。通过频域分析，论文证明了这两种方法的互补性。实验结果表明，StereoDiff在零样本、真实世界的动态视频深度基准测试中，无论室内外，均取得了最先进的性能，显著提高了视频深度估计的一致性和准确性。", "keywords": "视频深度估计, 立体匹配, 扩散模型, 时间一致性, StereoDiff", "comments": "StereoDiff的创新点在于其两阶段的混合方法，巧妙地结合了立体匹配在静态区域的全局3D优势和扩散模型在动态区域的时间一致性学习能力。这种针对视频中不同运动特性区域采用不同策略的思路，解决了传统方法难以同时兼顾的问题，并通过频域分析提供了理论支撑，使得其在真实世界动态视频深度估计中表现出优越的性能。"}}
{"id": "2506.20674", "title": "Scalable GPU Performance Variability Analysis framework", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Seth Ockerman", "Amal Gueroudji", "Line Pouchard", "Tanzima Z. Islam"], "summary": "Analyzing large-scale performance logs from GPU profilers often requires\nterabytes of memory and hours of runtime, even for basic summaries. These\nconstraints prevent timely insight and hinder the integration of performance\nanalytics into automated workflows. Existing analysis tools typically process\ndata sequentially, making them ill-suited for HPC workflows with growing trace\ncomplexity and volume. We introduce a distributed data analysis framework that\nscales with dataset size and compute availability. Rather than treating the\ndataset as a single entity, our system partitions it into independently\nanalyzable shards and processes them concurrently across MPI ranks. This design\nreduces per-node memory pressure, avoids central bottlenecks, and enables\nlow-latency exploration of high-dimensional trace data. We apply the framework\nto end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate\nits ability to diagnose performance variability, and uncover the impact of\nmemory transfer latency on GPU kernel behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20674v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20674v1", "date": "2025-06-17", "updated": "2025-06-17", "AI": {"title_translation": "可扩展的GPU性能变异性分析框架", "tldr": "本文提出一个分布式数据分析框架，通过并行处理分区数据集，解决了现有GPU性能日志分析工具在处理大规模数据时存在的内存和运行时限制，实现了可扩展的性能变异性诊断。", "motivation": "分析来自GPU性能分析器的大规模性能日志通常需要大量的内存和运行时，即使是基本的摘要也如此。这些限制阻碍了及时洞察并阻碍了性能分析集成到自动化工作流中。现有分析工具通常顺序处理数据，不适合处理日益复杂的和大规模的HPC工作流。", "method": "我们引入了一个分布式数据分析框架，它能随数据集大小和计算可用性进行扩展。该系统将数据集分区为可独立分析的碎片，并跨MPI等级并发处理它们。这种设计减少了每个节点的内存压力，避免了中心瓶颈，并实现了对高维跟踪数据的低延迟探索。", "result": "该框架应用于来自真实HPC和AI工作负载的端到端Nsight Compute跟踪，展示了其诊断性能变异性的能力，并揭示了内存传输延迟对GPU内核行为的影响。", "conclusion": "Not mentioned in abstract", "translation": "分析来自GPU性能分析器的大规模性能日志通常需要数TB的内存和数小时的运行时间，即使是进行基本汇总也如此。这些限制阻碍了及时洞察，并阻碍了性能分析集成到自动化工作流中。现有分析工具通常顺序处理数据，使其不适用于日益增长的跟踪复杂性和数据量的HPC工作流。我们引入了一个分布式数据分析框架，该框架可随数据集大小和计算可用性进行扩展。该系统不将数据集视为一个单一实体，而是将其划分为可独立分析的分片，并跨MPI等级并发处理。这种设计减少了每节点内存压力，避免了中心瓶颈，并实现了对高维跟踪数据的低延迟探索。我们将该框架应用于来自真实HPC和AI工作负载的端到端Nsight Compute跟踪，展示了其诊断性能变异性的能力，并揭示了内存传输延迟对GPU内核行为的影响。", "summary": "本文提出了一个可扩展的分布式数据分析框架，旨在解决现有GPU性能分析工具在处理大规模性能日志时面临的内存和运行时限制。该框架通过将数据集分区并进行并发处理，有效降低了内存压力并消除了中心瓶颈，实现了对高维跟踪数据的低延迟探索。实验表明，该框架能够诊断HPC和AI工作负载中GPU的性能变异性，并揭示内存传输延迟对GPU内核行为的影响。", "keywords": "GPU性能分析, 分布式框架, 性能变异性, 大规模数据, HPC", "comments": "该论文提出了一种创新的分布式方法来解决GPU性能分析中长期存在的可扩展性问题。通过数据分区和并发处理，它有效地克服了传统工具的内存和时间限制，这对于HPC和AI领域的大规模性能优化至关重要。其价值在于能够提供及时且深入的性能洞察，从而促进自动化工作流的集成。"}}
{"id": "2506.20773", "title": "A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers", "authors": ["Stephen T. Castonguay", "Joshua B. Fernandes", "Michael A. Puso", "Sylvie Aubry"], "summary": "An efficient numerical framework is presented for modeling viscoelasticity\nand permanent set of polymers. It is based on the hereditary integral form of\ntransient network theory, in which polymer chains belong to distinct networks\neach with different natural equilibrium states. Chains continually detach from\npreviously formed networks and reattach to new networks in a state of zero\nstress. The free energy of these networks is given in terms of the deformation\ngradient relative to the configuration at which the network was born. A\ndecomposition of the kernel for various free energies allows for a recurrence\nrelationship to be established, bypassing the need to integrate over all time\nhistory. The technique is established for both highly compressible and nearly\nincompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and\nOgden-Hill material models. Multiple examples are presented showing the ability\nto handle rate-dependent response and residual strains under complex loading\nhistories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20773v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.20773v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "聚合物永久形变和粘弹性响应建模的遗传积分瞬态网络方法", "tldr": "提出了一种基于遗传积分瞬态网络理论的有效数值框架，用于模拟聚合物的粘弹性和永久形变。", "motivation": "旨在提供一个高效的数值框架来模拟聚合物的粘弹性和永久形变。", "method": "该方法基于瞬态网络理论的遗传积分形式，其中聚合物链属于不同的网络，每个网络具有不同的自然平衡态。链不断从旧网络分离并重新连接到新的零应力网络。网络的自由能由相对于网络诞生时构型的变形梯度给出。通过对各种自由能的核进行分解，建立了一个递归关系，从而避免了对所有时间历史进行积分的需要。该技术通过使用新胡克、布拉茨-科、叶奥和奥格登-希尔材料模型，适用于高度可压缩和近乎不可压缩材料。", "result": "提出了多个例子，展示了该框架在复杂加载历史下处理速率依赖响应和残余应变的能力。", "conclusion": "该研究成功建立了一个高效的数值框架，能够准确模拟聚合物的粘弹性和永久形变，并处理复杂加载历史下的速率依赖响应和残余应变。", "translation": "聚合物永久形变和粘弹性响应建模的遗传积分瞬态网络方法\n\n摘要：\n本文提出了一种用于模拟聚合物粘弹性和永久形变的有效数值框架。它基于瞬态网络理论的遗传积分形式，其中聚合物链属于不同的网络，每个网络具有不同的自然平衡态。链不断从先前形成的网络中分离，并以零应力状态重新连接到新网络。这些网络的自由能是根据相对于网络诞生时构型的变形梯度给出的。对各种自由能的核进行分解，可以建立一个递归关系，从而避免了对所有时间历史进行积分的需要。该技术通过使用新胡克、布拉茨-科、叶奥和奥格登-希尔材料模型，适用于高度可压缩和近乎不可压缩材料。本文提出了多个例子，展示了在复杂加载历史下处理速率依赖响应和残余应变的能力。", "summary": "本文介绍了一种高效的数值框架，用于模拟聚合物的粘弹性和永久形变。该框架基于瞬态网络理论的遗传积分形式，通过引入链的动态分离和重新连接机制，并利用自由能核的分解来建立递归关系，从而避免了完整时间历史积分的计算量。该方法适用于多种材料模型，并能有效处理复杂加载历史下的速率依赖响应和残余应变。", "keywords": "聚合物, 粘弹性, 永久形变, 瞬态网络理论, 遗传积分", "comments": "该论文的创新点在于提出了一个高效的数值框架，通过利用自由能核的分解来建立递归关系，巧妙地避免了传统遗传积分方法中对所有时间历史进行积分的计算瓶颈。这显著提高了模拟聚合物粘弹性和永久形变的效率，对于材料科学和工程领域的数值模拟具有重要意义。"}}
{"id": "2506.20693", "title": "E-ABIN: an Explainable module for Anomaly detection in BIological Networks", "authors": ["Ugo Lomoio", "Tommaso Mazza", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "summary": "The increasing availability of large-scale omics data calls for robust\nanalytical frameworks capable of handling complex gene expression datasets\nwhile offering interpretable results. Recent advances in artificial\nintelligence have enabled the identification of aberrant molecular patterns\ndistinguishing disease states from healthy controls. Coupled with improvements\nin model interpretability, these tools now support the identification of genes\npotentially driving disease phenotypes. However, current approaches to gene\nanomaly detection often remain limited to single datasets and lack accessible\ngraphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable\nframework for Anomaly detection in Biological Networks. E-ABIN combines\nclassical machine learning and graph-based deep learning techniques within a\nunified, user-friendly platform, enabling the detection and interpretation of\nanomalies from gene expression or methylation-derived networks. By integrating\nalgorithms such as Support Vector Machines, Random Forests, Graph Autoencoders\n(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a\nhigh predictive accuracy while maintaining interpretability. We demonstrate the\nutility of E-ABIN through case studies of bladder cancer and coeliac disease,\nwhere it effectively uncovers biologically relevant anomalies and offers\ninsights into disease mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20693v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20693v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "E-ABIN：一种用于生物网络异常检测的可解释模块", "tldr": "E-ABIN是一个可解释的框架，结合机器学习和图深度学习，用于生物网络中的异常检测，并已在疾病案例中验证。", "motivation": "现有基因异常检测方法通常局限于单一数据集，且缺乏易于访问的图形界面，同时大型组学数据需要能够处理复杂基因表达数据集并提供可解释结果的强大分析框架。", "method": "E-ABIN结合了经典机器学习（如支持向量机、随机森林）和基于图的深度学习技术（如图自编码器、图对抗属性网络），在一个统一的用户友好平台中实现，用于基因表达或甲基化衍生网络的异常检测和解释。", "result": "E-ABIN在膀胱癌和乳糜泻的案例研究中展示了其效用，有效地揭示了生物学相关的异常并提供了对疾病机制的见解，同时保持了高预测准确性和可解释性。", "conclusion": "E-ABIN提供了一个通用、可解释的生物网络异常检测框架，通过结合多种机器学习和深度学习技术，提高了疾病相关异常的检测和解释能力。", "translation": "大规模组学数据的日益普及，要求有强大的分析框架，能够处理复杂的基因表达数据集，同时提供可解释的结果。人工智能的最新进展使得识别区分疾病状态与健康对照的异常分子模式成为可能。结合模型可解释性的改进，这些工具现在支持识别可能驱动疾病表型的基因。然而，当前基因异常检测方法往往局限于单一数据集，并且缺乏易于访问的图形界面。在此，我们介绍了E-ABIN，一个用于生物网络异常检测的通用、可解释框架。E-ABIN在一个统一、用户友好的平台中结合了经典机器学习和基于图的深度学习技术，能够检测和解释来自基因表达或甲基化衍生网络的异常。通过整合支持向量机、随机森林、图自编码器（GAEs）和图对抗属性网络（GAANs）等算法，E-ABIN确保了高预测准确性，同时保持了可解释性。我们通过膀胱癌和乳糜泻的案例研究展示了E-ABIN的实用性，它有效地揭示了生物学相关的异常，并提供了对疾病机制的见解。", "summary": "E-ABIN是一个新颖的可解释框架，专为生物网络中的异常检测而设计。它整合了经典机器学习和图深度学习技术，在一个用户友好的平台中分析基因表达或甲基化数据。该框架旨在克服现有方法在单一数据集限制和缺乏图形界面方面的不足，并在膀胱癌和乳糜泻的案例研究中证明了其在识别疾病相关异常和提供机制见解方面的有效性。", "keywords": "异常检测, 生物网络, 可解释性, 机器学习, 深度学习", "comments": "E-ABIN的创新之处在于其结合了传统机器学习和图深度学习的混合方法，以及对模型可解释性的强调，这对于生物医学领域至关重要。其统一的用户友好平台也解决了现有工具可用性不足的问题，使其在处理复杂组学数据方面具有重要应用潜力。"}}
{"id": "2506.20780", "title": "Noise-Tolerant Hybrid Approach for Data-Driven Predictive Control", "authors": ["Mahmood Mazare", "Hossein Ramezani"], "summary": "This paper focuses on a key challenge in hybrid data-driven predictive\ncontrol: the effect of measurement noise on Hankel matrices. While noise is\nhandled in direct and indirect methods, hybrid approaches often overlook its\nimpact during trajectory estimation. We propose a Noise-Tolerant Data-Driven\nPredictive Control (NTDPC) framework that integrates singular value\ndecomposition to separate system dynamics from noise within reduced-order\nHankel matrices. This enables accurate prediction with shorter data horizons\nand lower computational effort. A sensitivity index is introduced to support\nhorizon selection under different noise levels. Simulation results indicate\nimproved robustness and efficiency compared to existing hybrid methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20780v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20780v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "噪声容忍的混合数据驱动预测控制方法", "tldr": "提出一种噪声容忍的数据驱动预测控制（NTDPC）框架，通过奇异值分解处理噪声，提高预测精度、鲁棒性和效率。", "motivation": "混合数据驱动预测控制方法在轨迹估计时常忽略测量噪声对Hankel矩阵的影响，导致预测性能下降。", "method": "提出噪声容忍数据驱动预测控制（NTDPC）框架，该框架将奇异值分解集成到降阶Hankel矩阵中，以分离系统动态和噪声。同时引入灵敏度指数以支持不同噪声水平下的预测范围选择。", "result": "仿真结果表明，与现有混合方法相比，NTDPC框架提高了鲁棒性和效率。", "conclusion": "NTDPC框架通过有效处理测量噪声，显著提升了混合数据驱动预测控制的性能，实现了更短的数据范围和更低的计算量。", "translation": "这篇论文专注于混合数据驱动预测控制中的一个关键挑战：测量噪声对Hankel矩阵的影响。虽然直接和间接方法可以处理噪声，但混合方法在轨迹估计过程中经常忽略其影响。我们提出了一种噪声容忍数据驱动预测控制（NTDPC）框架，该框架集成了奇异值分解，以在降阶Hankel矩阵中将系统动态与噪声分离。这使得在更短的数据范围和更低的计算量下实现精确预测成为可能。引入了一个灵敏度指数，以支持在不同噪声水平下的范围选择。仿真结果表明，与现有混合方法相比，该方法提高了鲁棒性和效率。", "summary": "本文针对混合数据驱动预测控制中测量噪声对Hankel矩阵的影响问题，提出了一种噪声容忍数据驱动预测控制（NTDPC）框架。该框架利用奇异值分解从降阶Hankel矩阵中分离系统动态与噪声，从而在更短数据范围和更低计算量下实现精确预测。此外，引入了灵敏度指数以辅助不同噪声水平下的预测范围选择。仿真结果验证了NTDPC相较于现有混合方法的鲁棒性和效率提升。", "keywords": "数据驱动预测控制, 噪声容忍, Hankel矩阵, 奇异值分解, 混合方法", "comments": "这项工作通过引入奇异值分解来明确处理混合数据驱动预测控制中的噪声问题，填补了现有混合方法的一个空白。其创新性在于将噪声分离机制集成到Hankel矩阵处理中，从而提高了预测的准确性和计算效率。引入灵敏度指数也增加了方法的实用性。"}}
{"id": "2506.20798", "title": "Physical Limits of Entanglement-Based Quantum Key Distribution over Long-Distance Satellite Links", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saif Al-Kuwari", "Khalid Qaraqe"], "summary": "Entanglement-based quantum key distribution (QKD) protocols, such as E91 and\nBBM92, offer strong information-theoretic security and are naturally suited for\nsatellite-to-satellite QKD (SatQKD) links. However, implementing these\nprotocols over long-distance inter-satellite free-space optical (FSO) channels\nposes critical physical-layer challenges that are not addressed in the existing\nliterature. In particular, photon losses due to beam divergence, pointing\nerrors, and background noise can severely degrade the key generation rate and\nquantum bit error rate (QBER), especially under narrow receiver field-of-view\n(FoV) constraints. This paper presents a comprehensive performance analysis of\nentanglement-based inter-satellite QKD, focusing on photon-level modeling and\nthe impact of practical impairments. We develop analytical expressions for\nsignal detection probabilities, background photon influence, multi-pair\nemissions, and QBER, incorporating key parameters such as link distance,\ntransmitter tracking jitter, receiver misalignment, and photon pair generation\nrate. Simulation results reveal the nonlinear sensitivity of system performance\nto tracking error and FoV limitations, and highlight optimal parameter regimes\nthat jointly maximize secret key rate while maintaining QBER below acceptable\nthresholds. The proposed model provides actionable design insights for reliable\nand efficient deployment of entanglement-based SatQKD systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20798v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20798v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "远距离卫星链路上基于纠缠的量子密钥分发的物理极限", "tldr": "本文分析了基于纠缠的卫星量子密钥分发（SatQKD）在长距离自由空间光链路中的物理层限制，特别是光子损耗、指向误差和背景噪声的影响，并提出了一个模型来提供系统设计见解。", "motivation": "现有的文献没有解决在长距离星间自由空间光（FSO）信道上实现基于纠缠的量子密钥分发（QKD）协议所面临的关键物理层挑战，特别是由于光束发散、指向误差和背景噪声引起的光子损耗会严重降低密钥生成速率和量子误码率（QBER）。", "method": "本文对基于纠缠的星间QKD进行了全面的性能分析，重点关注光子级建模和实际损伤的影响。研究人员开发了信号检测概率、背景光子影响、多对发射和QBER的分析表达式，并结合了链路距离、发射机跟踪抖动、接收机失准和光子对生成速率等关键参数。", "result": "仿真结果揭示了系统性能对跟踪误差和视场（FoV）限制的非线性敏感性，并强调了在保持QBER低于可接受阈值的同时共同最大化密钥生成速率的最佳参数范围。", "conclusion": "所提出的模型为可靠高效地部署基于纠缠的卫星QKD系统提供了可操作的设计见解。", "translation": "基于纠缠的量子密钥分发（QKD）协议，如E91和BBM92，提供强大的信息理论安全性，并且天然适用于卫星到卫星的QKD（SatQKD）链路。然而，在长距离星间自由空间光（FSO）信道上实现这些协议带来了现有文献中尚未解决的关键物理层挑战。特别是，由于光束发散、指向误差和背景噪声导致的光子损耗会严重降低密钥生成速率和量子误码率（QBER），尤其是在接收机视场（FoV）受限的情况下。本文对基于纠缠的星间QKD进行了全面的性能分析，重点关注光子级建模和实际损伤的影响。我们开发了信号检测概率、背景光子影响、多对发射和QBER的分析表达式，并结合了链路距离、发射机跟踪抖动、接收机失准和光子对生成速率等关键参数。仿真结果揭示了系统性能对跟踪误差和FoV限制的非线性敏感性，并强调了在保持QBER低于可接受阈值的同时共同最大化秘密密钥速率的最佳参数范围。所提出的模型为可靠高效地部署基于纠缠的SatQKD系统提供了可操作的设计见解。", "summary": "本文针对长距离星间自由空间光链路中基于纠缠的量子密钥分发（SatQKD）面临的物理层挑战，如光子损耗、指向误差和背景噪声，进行了全面的性能分析。研究开发了考虑多种关键参数的分析表达式，并通过仿真揭示了系统性能对跟踪误差和视场限制的非线性敏感性，最终确定了在保证低误码率下最大化密钥生成速率的最佳参数范围，为SatQKD系统设计提供了实用指导。", "keywords": "基于纠缠的QKD, 卫星QKD, 物理极限, 自由空间光, 性能分析", "comments": "这篇论文通过深入分析基于纠缠的卫星QKD系统在实际物理层挑战下的性能，填补了现有文献的空白。其创新之处在于提出了一个全面的光子级模型，并开发了分析表达式来量化各种损伤（如光子损耗、指向误差、背景噪声）对密钥生成速率和量子误码率的影响。通过仿真揭示了关键参数的非线性敏感性并识别了最优操作区域，这对于未来星间QKD系统的可靠和高效部署具有重要的指导意义。"}}
{"id": "2506.20688", "title": "Building Lightweight Semantic Segmentation Models for Aerial Images Using Dual Relation Distillation", "authors": ["Minglong Li", "Lianlei Shan", "Weiqiang Wang", "Ke Lv", "Bin Luo", "Si-Bao Chen"], "summary": "Recently, there have been significant improvements in the accuracy of CNN\nmodels for semantic segmentation. However, these models are often heavy and\nsuffer from low inference speed, which limits their practical application. To\naddress this issue, knowledge distillation has emerged as a promising approach\nto achieve a good trade-off between segmentation accuracy and efficiency. In\nthis paper, we propose a novel dual relation distillation (DRD) technique that\ntransfers both spatial and channel relations in feature maps from a cumbersome\nmodel (teacher) to a compact model (student). Specifically, we compute spatial\nand channel relation maps separately for the teacher and student models, and\nthen align corresponding relation maps by minimizing their distance. Since the\nteacher model usually learns more information and collects richer spatial and\nchannel correlations than the student model, transferring these correlations\nfrom the teacher to the student can help the student mimic the teacher better\nin terms of feature distribution, thus improving the segmentation accuracy of\nthe student model. We conduct comprehensive experiments on three segmentation\ndatasets, including two widely adopted benchmarks in the remote sensing field\n(Vaihingen and Potsdam datasets) and one popular benchmark in general scene\n(Cityscapes dataset). The experimental results demonstrate that our novel\ndistillation framework can significantly boost the performance of the student\nnetwork without incurring extra computational overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20688v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20688v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "使用双关系蒸馏构建轻量级航空图像语义分割模型", "tldr": "本文提出了一种新颖的双关系蒸馏（DRD）技术，通过从繁琐的教师模型向紧凑的学生模型传递空间和通道关系，显著提高了轻量级语义分割模型在航空图像上的性能，且不增加计算开销。", "motivation": "现有的CNN语义分割模型虽然精度高，但通常模型庞大且推理速度慢，限制了其实际应用。知识蒸馏被认为是一种在分割精度和效率之间取得良好平衡的有前景的方法。", "method": "本文提出了一种新颖的双关系蒸馏（DRD）技术。该技术将特征图中空间和通道关系从繁重模型（教师）转移到紧凑模型（学生）。具体来说，它分别为教师模型和学生模型计算空间和通道关系图，然后通过最小化它们之间的距离来对齐相应的关系图。", "result": "在包括两个遥感领域常用基准（Vaihingen和Potsdam数据集）和一个通用场景常用基准（Cityscapes数据集）在内的三个分割数据集上进行了综合实验。实验结果表明，所提出的新型蒸馏框架可以显著提升学生网络的性能，且不产生额外的计算开销。", "conclusion": "通过双关系蒸馏（DRD）技术，可以有效地将教师模型中更丰富的信息和相关性转移到学生模型，从而在不增加计算成本的情况下提高学生模型的语义分割精度，解决了模型轻量化和性能之间的权衡问题。", "translation": "最近，CNN模型在语义分割的准确性方面取得了显著的进步。然而，这些模型通常很“重”，推理速度慢，这限制了它们的实际应用。为了解决这个问题，知识蒸馏已成为一种有前途的方法，可以在分割精度和效率之间实现良好的权衡。在本文中，我们提出了一种新颖的双关系蒸馏（DRD）技术，该技术将特征图中的空间和通道关系从繁琐的模型（教师）转移到紧凑的模型（学生）。具体来说，我们分别为教师模型和学生模型计算空间和通道关系图，然后通过最小化它们之间的距离来对齐相应的关系图。由于教师模型通常比学生模型学习到更多的信息并收集更丰富的空间和通道相关性，因此将这些相关性从教师模型转移到学生模型可以帮助学生模型在特征分布方面更好地模仿教师模型，从而提高学生模型的分割精度。我们在三个分割数据集上进行了综合实验，包括遥感领域中两个广泛采用的基准（Vaihingen和Potsdam数据集）和一个通用场景中流行的基准（Cityscapes数据集）。实验结果表明，我们新颖的蒸馏框架可以显著提升学生网络的性能，而不会产生额外的计算开销。", "summary": "本文提出了一种名为双关系蒸馏（DRD）的新技术，旨在解决语义分割模型效率低下的问题。该方法通过将教师模型的空间和通道关系图知识转移到学生模型，以提高学生模型的分割精度。实验证明，DRD在多个数据集上显著提升了轻量级学生网络的性能，且未增加额外计算负担。", "keywords": "语义分割, 知识蒸馏, 轻量级模型, 航空图像, 双关系蒸馏", "comments": "本文的创新点在于提出了双关系蒸馏（DRD），该方法同时考虑了特征图中的空间和通道关系进行知识迁移，这比传统只关注单一类型关系的蒸馏方法更为全面。其重要性在于，它为构建更轻量、更高效且保持高精度的语义分割模型提供了有效途径，尤其适用于对计算资源和推理速度有较高要求的航空图像处理等实际应用场景。该方法在不增加计算开销的前提下提升了学生模型性能，具有很高的实用价值。"}}
{"id": "2506.20737", "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation", "authors": ["Gurusha Juneja", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "summary": "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20737v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20737v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "MAGPIE：一个用于多智能体上下文隐私评估的数据集", "tldr": "当前最先进的LLM代理在多智能体协作中未能有效理解和保护上下文隐私，即使有明确指示也会泄露信息，并且任务完成率低。", "motivation": "LLM驱动的智能体在协作任务中的普及，使得隐私问题变得至关重要，因为它们经常访问专有工具和敏感数据库。现有的基准测试主要评估单轮、低复杂度的任务，而无法充分评估LLM智能体对上下文隐私的理解以及在多轮对话中保护用户隐私的能力。", "method": "本文提出了一个名为MAGPIE的基准数据集，包含15个领域158个真实高风险场景，这些场景旨在使完全排除私有数据会阻碍任务完成，但无限制的信息共享可能导致重大损失。然后，作者评估了当前最先进的LLM（如GPT-4o和Claude-2.7-Sonnet）在(a)其对上下文私有数据的理解和(b)在不违反用户隐私的情况下进行协作的能力。", "result": "实证实验表明，当前模型（包括GPT-4o和Claude-2.7-Sonnet）缺乏对上下文隐私的鲁棒理解，将私有数据错误分类为可共享数据的比例分别为25.2%和43.6%。在多轮对话中，即使在明确的隐私指示下，这些模型在59.9%和50.5%的案例中披露了私人信息。此外，多智能体系统在71%的场景中未能完成任务。", "conclusion": "这些结果强调，当前模型未能同时实现上下文隐私保护和协作任务解决。", "translation": "LLM驱动的智能体日益普及，导致在日程安排、谈判、资源分配等任务中，智能体间协作的部署越来越多。在这种系统中，隐私至关重要，因为智能体经常访问需要严格保密的专有工具和领域特定数据库。本文研究了LLM驱动的智能体是否表现出对上下文隐私的理解。并且，如果受到指示，这些系统是否在非对抗性多轮对话中保持推理时用户隐私。现有评估LLM智能体上下文隐私的基准主要评估单轮、低复杂度的任务，其中私有信息可以很容易地被排除。我们首先提出了一个基准——MAGPIE，它包含15个领域158个真实高风险场景。这些场景的设计旨在使完全排除私有数据会阻碍任务完成，但无限制的信息共享可能导致重大损失。然后，我们评估了当前最先进的LLM在(a)它们对上下文私有数据的理解和(b)它们在不违反用户隐私的情况下进行协作的能力。实证实验表明，当前模型，包括GPT-4o和Claude-2.7-Sonnet，缺乏对上下文隐私的鲁棒理解，将私有数据错误分类为可共享的比例分别为25.2%和43.6%。在多轮对话中，即使在明确的隐私指示下，这些模型在59.9%和50.5%的案例中披露了私人信息。此外，多智能体系统在71%的场景中未能完成任务。这些结果强调，当前模型未能同时实现上下文隐私保护和协作任务解决。", "summary": "本研究提出了MAGPIE数据集，旨在评估LLM驱动的多智能体在真实高风险场景中对上下文隐私的理解和保护能力。现有基准未能充分评估复杂多轮对话中的隐私问题。通过对GPT-4o和Claude-2.7-Sonnet等SOTA模型的评估发现，它们在识别和保护上下文隐私方面表现不佳，即使在明确指示下也常泄露信息，且任务完成率低。研究结果表明，当前模型尚未能有效平衡隐私保护与协作任务解决。", "keywords": "多智能体系统, 上下文隐私, LLM, 数据集, 隐私评估", "comments": "本文通过引入MAGPIE数据集，填补了LLM多智能体上下文隐私评估的空白，其高风险真实场景设计具有创新性。实验结果清晰地揭示了当前LLM在隐私理解和保护方面的显著不足，对未来多智能体系统的发展具有重要指导意义，强调了在确保隐私安全前提下提升协作能力的需求。"}}
{"id": "2506.20875", "title": "3DGH: 3D Head Generation with Composable Hair and Face", "authors": ["Chengan He", "Junxuan Li", "Tobias Kirschstein", "Artem Sevastopolsky", "Shunsuke Saito", "Qingyang Tan", "Javier Romero", "Chen Cao", "Holly Rushmeier", "Giljoo Nam"], "summary": "We present 3DGH, an unconditional generative model for 3D human heads with\ncomposable hair and face components. Unlike previous work that entangles the\nmodeling of hair and face, we propose to separate them using a novel data\nrepresentation with template-based 3D Gaussian Splatting, in which deformable\nhair geometry is introduced to capture the geometric variations across\ndifferent hairstyles. Based on this data representation, we design a 3D\nGAN-based architecture with dual generators and employ a cross-attention\nmechanism to model the inherent correlation between hair and face. The model is\ntrained on synthetic renderings using carefully designed objectives to\nstabilize training and facilitate hair-face separation. We conduct extensive\nexperiments to validate the design choice of 3DGH, and evaluate it both\nqualitatively and quantitatively by comparing with several state-of-the-art 3D\nGAN methods, demonstrating its effectiveness in unconditional full-head image\nsynthesis and composable 3D hairstyle editing. More details will be available\non our project page: https://c-he.github.io/projects/3dgh/.", "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://c-he.github.io/projects/3dgh/", "pdf_url": "http://arxiv.org/pdf/2506.20875v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20875v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "3DGH：可组合头发和面部的3D头部生成", "tldr": "3DGH是一个无条件生成模型，用于生成具有可组合头发和面部组件的3D人头，通过分离头发和面部建模实现。", "motivation": "以往的工作将头发和面部的建模纠缠在一起，本研究旨在通过分离它们来解决这一问题，并引入可变形头发几何体以捕捉不同发型之间的几何变化。", "method": "提出了一种新颖的基于模板的3D高斯泼溅数据表示，其中引入可变形头发几何体。基于此表示，设计了一个具有双生成器的3D GAN架构，并采用交叉注意力机制来建模头发和面部之间的内在关联。模型在合成渲染数据上训练，并使用精心设计的优化目标来稳定训练和促进头发-面部分离。", "result": "3DGH在无条件全头图像合成和可组合3D发型编辑方面表现出有效性，并通过与多个最先进的3D GAN方法进行定性和定量比较得到了验证。", "conclusion": "3DGH通过解耦头发和面部建模，成功实现了高质量的3D头部生成以及可组合的发型编辑，展示了其在3D人头合成领域的先进性。", "translation": "我们提出了3DGH，一个用于生成具有可组合头发和面部组件的3D人头的无条件生成模型。与以往将头发和面部建模纠缠在一起的工作不同，我们提出使用一种新颖的基于模板的3D高斯泼溅数据表示来分离它们，其中引入了可变形头发几何体以捕捉不同发型之间的几何变化。基于这种数据表示，我们设计了一个具有双生成器的3D GAN架构，并采用交叉注意力机制来建模头发和面部之间的内在关联。该模型在合成渲染数据上训练，并使用精心设计的目标来稳定训练并促进头发-面部分离。我们进行了广泛的实验来验证3DGH的设计选择，并通过与几种最先进的3D GAN方法进行定性和定量比较来评估它，展示了其在无条件全头图像合成和可组合3D发型编辑方面的有效性。更多细节将在我们的项目页面上提供：https://c-he.github.io/projects/3dgh/。", "summary": "3DGH是一个创新的无条件3D人头生成模型，其核心在于通过基于模板的3D高斯泼溅数据表示，首次实现了头发和面部组件的解耦建模。该模型采用双生成器GAN架构和交叉注意力机制来处理头发与面部的关联，并在合成数据上训练。实验证明，3DGH在生成高质量全头图像和支持可组合3D发型编辑方面优于现有技术。", "keywords": "3D头部生成, 高斯泼溅, GAN, 头发面部分离, 可组合编辑", "comments": "该论文的创新点在于提出了头发和面部组件分离建模的新范式，通过新颖的3D高斯泼溅数据表示和可变形头发几何体，解决了以往方法中头发和面部纠缠的问题。这种解耦设计显著提升了3D头部生成和发型编辑的灵活性和真实感，为3D内容创作提供了重要工具。"}}
{"id": "2506.20687", "title": "Review of Three Variants of the k-d Tree", "authors": ["Russell A. Brown"], "summary": "The original description of the k-d tree recognized that rebalancing\ntechniques, such as used to build an AVL tree or a red-black tree, are not\napplicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is\nnecessary to find the median of a set of data for each recursive subdivision of\nthat set. The sort or selection used to find the median, and the technique used\nto partition the set about that median, strongly influence the computational\ncomplexity of building a k-d tree. This article describes and contrasts three\nvariants of the k-d tree that differ in their technique used to partition the\nset, and compares the performance of those variants. In addition, dual-threaded\nexecution is proposed and analyzed for one of the three variants.", "comment": "29 pages, 11 figures, one listing, one table", "pdf_url": "http://arxiv.org/pdf/2506.20687v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20687v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "k-d树三种变体的综述", "tldr": "本文综述并对比了k-d树的三种变体，这些变体在数据分区技术上有所不同，并比较了它们的性能。此外，还对其中一种变体提出了双线程执行方案并进行了分析。", "motivation": "由于k-d树的平衡构建需要高效地找到中位数并进行分区，且分区技术显著影响构建的计算复杂度，因此有必要研究和比较不同的k-d树变体。", "method": "本文描述并对比了k-d树的三种变体，这些变体在用于分区数据集的技术上有所不同。同时，还对其中一种变体提出了双线程执行方案并进行了分析。", "result": "本文描述并对比了三种k-d树变体的性能，并对其中一种变体提出了双线程执行的分析。", "conclusion": "Not mentioned in abstract", "translation": "k-d树的原始描述认识到，像用于构建AVL树或红黑树那样的再平衡技术不适用于k-d树。因此，为了构建一个平衡的k-d树，有必要为每次递归细分数据集时找到该数据集的中位数。用于找到中位数并围绕该中位数分区数据集的排序或选择技术，以及所使用的技术，强烈影响构建k-d树的计算复杂度。本文描述并对比了k-d树的三种变体，它们在用于分区数据集的技术上有所不同，并比较了这些变体的性能。此外，还对三种变体中的一种提出了双线程执行方案并进行了分析。", "summary": "本文综述了k-d树的三种变体，这些变体主要在数据分区技术上有所区别。文章阐述了k-d树平衡构建中查找中位数和分区的重要性及其对计算复杂度的影响，并对比了不同变体的性能。此外，还对其中一种变体提出了双线程执行方案并进行了分析。", "keywords": "k-d树, 变体, 分区技术, 计算复杂度, 双线程执行", "comments": "本文作为一篇综述性文章，不仅比较了k-d树的现有变体，还引入了新的优化思路（双线程执行），这对于理解和改进k-d树的构建效率具有一定的价值。其创新点在于对现有技术的系统性梳理和新方法的提出。"}}
{"id": "2506.21090", "title": "Post-training for Deepfake Speech Detection", "authors": ["Wanying Ge", "Xin Wang", "Xuechen Liu", "Junichi Yamagishi"], "summary": "We introduce a post-training approach that adapts self-supervised learning\n(SSL) models for deepfake speech detection by bridging the gap between general\npre-training and domain-specific fine-tuning. We present AntiDeepfake models, a\nseries of post-trained models developed using a large-scale multilingual speech\ndataset containing over 56,000 hours of genuine speech and 18,000 hours of\nspeech with various artifacts in over one hundred languages. Experimental\nresults show that the post-trained models already exhibit strong robustness and\ngeneralization to unseen deepfake speech. When they are further fine-tuned on\nthe Deepfake-Eval-2024 dataset, these models consistently surpass existing\nstate-of-the-art detectors that do not leverage post-training. Model\ncheckpoints and source code are available online.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21090v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21090v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于深度伪造语音检测的后训练", "tldr": "本文提出了一种用于深度伪造语音检测的后训练方法，该方法使自监督学习（SSL）模型适应此任务，并通过实验证明其性能优于现有最先进的检测器。", "motivation": "本文旨在通过弥合通用预训练和领域特定微调之间的差距，使自监督学习（SSL）模型适应深度伪造语音检测任务。", "method": "研究人员引入了一种后训练方法，该方法基于自监督学习（SSL）模型，并开发了“AntiDeepfake”模型系列。这些模型使用一个大规模多语言语音数据集进行训练，该数据集包含超过56,000小时的真实语音和18,000小时的包含各种伪影的语音，涵盖一百多种语言。", "result": "实验结果表明，后训练模型对未见过的深度伪造语音表现出强大的鲁棒性和泛化能力。当在Deepfake-Eval-2024数据集上进一步微调时，这些模型持续超越了未利用后训练的现有最先进检测器。", "conclusion": "所提出的后训练方法显著增强了深度伪造语音检测模型的性能，特别是在鲁棒性和泛化能力方面，超越了当前的最新技术。", "translation": "我们引入了一种后训练方法，通过弥合通用预训练和领域特定微调之间的差距，使自监督学习（SSL）模型适应深度伪造语音检测。我们提出了 AntiDeepfake 模型，这是一系列使用大型多语言语音数据集开发的后训练模型，该数据集包含超过 56,000 小时的真实语音和 18,000 小时的包含各种伪影的语音，涵盖一百多种语言。实验结果表明，后训练模型已经对未见过的深度伪造语音表现出强大的鲁棒性和泛化能力。当它们在 Deepfake-Eval-2024 数据集上进一步微调时，这些模型持续超越了未利用后训练的现有最先进检测器。模型检查点和源代码可在网上获取。", "summary": "本文提出了一种针对深度伪造语音检测的后训练方法，旨在弥合通用预训练与特定领域微调之间的差距。通过使用包含大量真实和伪造语音的多语言数据集，研究人员开发了“AntiDeepfake”模型。实验证明，这些后训练模型对未知深度伪造语音表现出强大的鲁棒性和泛化能力，并且在进一步微调后，其性能持续超越了现有未采用后训练的最先进检测器。", "keywords": "深度伪造语音检测, 后训练, 自监督学习, AntiDeepfake模型, 鲁棒性", "comments": "本文的创新之处在于引入了一个有效的后训练阶段，它成功地弥合了通用自监督学习预训练与特定深度伪造检测微调之间的鸿沟。这种方法显著提升了模型的鲁棒性和泛化能力，这对于应对实际世界中的深度伪造检测挑战至关重要。此外，使用大规模、多样化的多语言数据集也为模型的有效性做出了贡献。"}}
{"id": "2506.20844", "title": "The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "summary": "Scientific fact-checking aims to determine the veracity of scientific claims\nby retrieving and analysing evidence from research literature. The problem is\ninherently more complex than general fact-checking since it must accommodate\nthe evolving nature of scientific knowledge, the structural complexity of\nacademic literature and the challenges posed by long-form, multimodal\nscientific expression. However, existing approaches focus on simplified\nversions of the problem based on small-scale datasets consisting of abstracts\nrather than full papers, thereby avoiding the distinct challenges associated\nwith processing complete documents. This paper examines the limitations of\ncurrent scientific fact-checking systems and reveals the many potential\nfeatures and resources that could be exploited to advance their performance. It\nidentifies key research challenges within evidence retrieval, including (1)\nevidence-driven retrieval that addresses semantic limitations and topic\nimbalance (2) time-aware evidence retrieval with citation tracking to mitigate\noutdated information, (3) structured document parsing to leverage long-range\ncontext, (4) handling complex scientific expressions, including tables,\nfigures, and domain-specific terminology and (5) assessing the credibility of\nscientific literature. Preliminary experiments were conducted to substantiate\nthese challenges and identify potential solutions. This perspective paper aims\nto advance scientific fact-checking with a specialised IR system tailored for\nreal-world applications.", "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR'25)", "pdf_url": "http://arxiv.org/pdf/2506.20844v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20844v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "科学事实核查的下一阶段：从复杂结构化学术论文中进行高级证据检索", "tldr": "现有科学事实核查系统无法处理完整的学术论文，本研究旨在通过识别关键挑战和潜在解决方案来推进该领域，以构建一个针对真实世界应用的专业信息检索系统。", "motivation": "科学事实核查比一般事实核查更为复杂，需要适应科学知识的演变、学术文献的结构复杂性以及长篇多模态科学表达的挑战。然而，现有方法仅关注基于摘要的小规模数据集，未能解决处理完整文档时的独特挑战。", "method": "本文审视了当前科学事实核查系统的局限性，并揭示了可用于提升其性能的潜在特征和资源。它识别了证据检索中的关键研究挑战，包括处理语义限制、主题不平衡、过时信息、结构化文档解析、复杂科学表达和文献可信度评估。此外，还进行了初步实验以证实这些挑战并找出潜在解决方案。", "result": "本研究识别了科学事实核查中证据检索的五大关键挑战：1) 解决语义限制和主题不平衡的证据驱动检索；2) 结合引文跟踪以缓解过时信息的时间感知证据检索；3) 利用长程上下文的结构化文档解析；4) 处理包括表格、图表和领域特定术语在内的复杂科学表达；5) 评估科学文献的可信度。初步实验证实了这些挑战并提出了潜在解决方案。", "conclusion": "本展望性论文旨在通过一个为真实世界应用量身定制的专业信息检索系统，推进科学事实核查领域。", "translation": "科学事实核查旨在通过从研究文献中检索和分析证据来确定科学主张的真实性。这个问题本质上比一般事实核查更为复杂，因为它必须适应科学知识的演变性质、学术文献的结构复杂性以及长篇、多模态科学表达所带来的挑战。然而，现有方法侧重于基于小型数据集（仅包含摘要而非完整论文）的简化问题版本，从而避免了处理完整文档所带来的独特挑战。本文审视了当前科学事实核查系统的局限性，并揭示了许多可用于提升其性能的潜在特征和资源。它识别了证据检索中的关键研究挑战，包括 (1) 解决语义限制和主题不平衡的证据驱动检索，(2) 结合引文跟踪以缓解过时信息的时间感知证据检索，(3) 利用长程上下文的结构化文档解析，(4) 处理包括表格、图表和领域特定术语在内的复杂科学表达，以及 (5) 评估科学文献的可信度。进行了初步实验以证实这些挑战并找出潜在解决方案。本展望性论文旨在通过一个为真实世界应用量身定制的专业信息检索系统，推进科学事实核查。", "summary": "本论文探讨了当前科学事实核查系统在处理复杂、结构化学术论文时的局限性。它强调了现有方法未能充分利用完整文档的挑战，并提出了五项关键研究挑战，包括语义限制、时间感知、结构化解析、复杂表达处理和可信度评估。通过初步实验证实了这些挑战，并旨在开发一个专门的信息检索系统以推进科学事实核查在真实世界中的应用。", "keywords": "科学事实核查, 证据检索, 学术论文, 信息检索, 结构化文档", "comments": "这篇论文创新性地指出了当前科学事实核查领域面临的核心问题，即未能充分利用完整学术论文中的丰富信息和复杂结构。它不仅仅停留在指出问题，更进一步系统地提出了五项具体的、具有挑战性的研究方向，为未来的研究奠定了基础。其重要性在于，如果这些挑战能够被有效解决，将极大地提升科学事实核查的准确性和效率，对于打击虚假信息和维护科学严谨性具有深远意义。然而，作为一篇展望性论文，它主要集中于问题识别和方向指引，具体的实现细节和技术突破仍有待后续研究。"}}
{"id": "2506.20809", "title": "Boundary integral equation analysis for spheroidal suspensions", "authors": ["Leo Crowder", "Tianyue Li", "Eduardo Corona", "Shravan Veerapaneni"], "summary": "In this work, we provide a fast, spectrally accurate method for the\nevaluation of boundary integral operators (BIOs) on a suspension of prolate and\noblate spheroids. We first derive formulas for the standard layer potential\noperators for the Laplace equation applied to an expansion of the integral\ndensities in the appropriate spheroidal harmonic basis. These then lead to\nanalytical expressions in solid harmonics that allow spectrally accurate\nevaluation of near-field particle interactions. Finally, a standard quadrature\nscheme is used to evaluate smooth, far-field interactions; these are then\naccelerated using the fast multipole method.\n  Through a number of numerical test cases, we verify the accuracy and\nefficiency of our BIO evaluation framework for dense, polydisperse suspensions\nof spheroids. Through the use of standard formulas linking Stokes and Laplace\npotentials, we show our scheme can be readily applied to problems involving\nparticulate suspension flows. For both Laplace and Stokes, our method allows us\nto evaluate BIOs for suspensions up to hundreds of particles on a single\nprocessor.", "comment": "Submitted to Journal of Computational Physics, June 2025", "pdf_url": "http://arxiv.org/pdf/2506.20809v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20809v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "扁球体悬浮液的边界积分方程分析", "tldr": "本文提出了一种快速、谱精确的方法，用于在扁球体悬浮液中评估边界积分算子，并验证其在拉普拉斯和斯托克斯问题中的准确性和效率。", "motivation": "旨在提供一种快速、谱精确的方法来评估扁球体悬浮液中的边界积分算子（BIOs），以有效处理粒子相互作用和悬浮液流动问题。", "method": "首先推导了拉普拉斯方程标准层势算子在扁球谐波基展开中的公式，生成实体谐波中的解析表达式以实现近场粒子相互作用的谱精确评估。然后，使用标准正交方案评估远场相互作用，并通过快速多极子方法加速。通过数值测试验证了该框架在致密、多分散扁球体悬浮液中的准确性和效率，并利用连接斯托克斯和拉普拉斯势的标准公式将其应用于颗粒悬浮流问题。", "result": "开发了一种快速、谱精确的边界积分算子评估方法。该方法在数值测试中被验证对致密、多分散的扁球体悬浮液具有准确性和效率。该方案可以应用于涉及颗粒悬浮流的问题，并且能够在一个处理器上评估多达数百个粒子悬浮液的BIOs，适用于拉普拉斯和斯托克斯问题。", "conclusion": "本研究成功开发并验证了一种快速、谱精确的边界积分算子评估框架，该框架特别适用于扁球体悬浮液，并能有效处理拉普拉斯和斯托克斯问题，即使在单个处理器上也能处理大量粒子。", "translation": "在这项工作中，我们提供了一种快速、谱精确的方法，用于评估扁长球和扁球体悬浮液上的边界积分算子（BIOs）。我们首先推导了拉普拉斯方程的标准层势算子的公式，该算子应用于适当的扁球谐波基中的积分密度展开。这些公式随后产生了实体谐波中的解析表达式，从而能够对近场粒子相互作用进行谱精确评估。最后，使用标准正交方案评估平滑的远场相互作用；这些相互作用随后通过快速多极子方法进行加速。通过大量的数值测试案例，我们验证了我们的BIO评估框架对于致密、多分散扁球体悬浮液的准确性和效率。通过使用连接斯托克斯和拉普拉斯势的标准公式，我们表明我们的方案可以很容易地应用于涉及颗粒悬浮流的问题。对于拉普拉斯和斯托克斯问题，我们的方法允许我们在单个处理器上评估多达数百个粒子悬浮液的BIOs。", "summary": "本文提出了一种用于扁球体悬浮液中边界积分算子（BIOs）的快速、谱精确评估方法。该方法结合了扁球谐波展开、实体谐波中的解析表达式用于近场相互作用，以及结合快速多极子方法的标准正交方案用于远场相互作用。通过数值测试，验证了其在致密、多分散悬浮液中的准确性和效率，并展示了其在拉普拉斯和斯托克斯问题中处理多达数百个粒子悬浮液的能力。", "keywords": "边界积分算子, 扁球体悬浮液, 快速多极子方法, 谱精确, 拉普拉斯方程", "comments": "该论文提出了一种创新的、高效且高精度的边界积分算子评估框架，特别适用于复杂几何形状（扁球体）的悬浮液。其结合了多种数学和数值技术，包括谐波展开、解析表达式和快速多极子方法，以实现近场和远场相互作用的有效处理。该方法在处理致密和多分散系统方面的能力以及对斯托克斯流的适用性，使其在计算流体力学和粒子模拟领域具有重要意义。能够在单个处理器上处理数百个粒子也表明了其良好的计算效率。"}}
{"id": "2506.20793", "title": "Multi-lingual Functional Evaluation for Large Language Models", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20793v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20793v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "大型语言模型的多语言功能评估", "tldr": "本文通过创建新的多语言功能基准（CL-GSM Symbolic和CL-IFEval），揭示了现有静态多语言基准在评估大型语言模型实际性能和跨语言鲁棒性方面的不足。", "motivation": "现有的多语言基准测试（如Belebele、M-MMLU和M-GSM）在评估大型语言模型在多语言环境中的实际性能和鲁棒性方面存在不足，无法提供足够的理解。", "method": "研究人员通过将现有的英语功能基准模板翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种语言，创建了多语言功能基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）。", "result": "结果显示，一些静态多语言基准与功能性能的匹配度远低于其他基准。例如，M-GSM与CL-GSM Symbolic之间在英语、法语和西班牙语中的性能分别下降了24%、17%和18%；Belebele与CL-IFEval之间在不同语言中性能下降了15-24%；而M-MMLU与CL-IFEval之间仅下降了0.5%-3%。此外，模型在不同语言间的鲁棒性差异显著，其中阿拉伯语和英语在评估迭代中表现最为稳定。", "conclusion": "现有静态多语言基准未能充分捕捉大型语言模型的实际功能性能和跨语言鲁棒性，新的功能性基准测试方法提供了更深入的洞察。", "translation": "大型语言模型的多语言能力通常通过Belebele、M-MMLU和M-GSM等静态数据基准进行评估。然而，这些评估往往未能充分理解模型在多语言环境中的实际性能和鲁棒性。为此，我们通过将现有英语功能基准模板翻译成另外五种语言（法语、西班牙语、印地语、阿拉伯语和约鲁巴语），创建了多语言功能基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）。我们的结果表明，一些静态多语言基准比其他基准更能捕捉功能性能（即，在不同模型中，M-GSM与CL-GSM Symbolic在英语、法语和西班牙语中的性能分别下降了24%、17%和18%；类似地，Belebele与CL-IFEval之间在不同语言中性能下降了15-24%，而M-MMLU与CL-IFEval之间仅下降0.5%至3%）。同样，我们发现模型在不同语言间的鲁棒性差异显著，某些语言（例如阿拉伯语、英语）在评估迭代中表现最为稳定。", "summary": "本文针对大型语言模型在多语言环境中现有静态基准评估不足的问题，提出了两个新的多语言功能基准：CL-GSM Symbolic和CL-IFEval。通过将英语功能模板翻译成多种语言，研究发现现有静态基准与实际功能性能存在较大差距，且模型在不同语言间的鲁棒性差异显著。研究强调了使用功能性基准来更准确地评估LLM跨语言性能和鲁棒性的重要性。", "keywords": "多语言评估, 大型语言模型, 功能基准, 鲁棒性, 跨语言", "comments": "本文的创新点在于提出了两个新的多语言功能基准，以弥补现有静态基准在评估大型语言模型实际性能和鲁棒性方面的不足。这对于更全面地理解和改进多语言LLM至关重要，揭示了不同静态基准与功能性能之间存在的差异，并强调了跨语言鲁棒性的重要性。"}}
{"id": "2506.21086", "title": "PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching", "authors": ["Guillem Cortès-Sebastià", "Benjamin Martin", "Emilio Molina", "Xavier Serra", "Romain Hennequin"], "summary": "This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)\nsystem designed specifically around spectral peaks. This novel system is\ndesigned to leverage the sparse spectral coordinates typically computed by\ntraditional peak-based AFP methods. PeakNetFP performs hierarchical point\nfeature extraction techniques similar to the computer vision model PointNet++,\nand is trained using contrastive learning like in the state-of-the-art deep\nlearning AFP, NeuralFP. This combination allows PeakNetFP to outperform\nconventional AFP systems and achieves comparable performance to NeuralFP when\nhandling challenging time-stretched audio data. In extensive evaluation,\nPeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging\nfrom 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:\ncompared to NeuralFP, it has 100 times fewer parameters and uses 11 times\nsmaller input data. These features make PeakNetFP a lightweight and efficient\nsolution for AFP tasks where time stretching is involved. Overall, this system\nrepresents a promising direction for future AFP technologies, as it\nsuccessfully merges the lightweight nature of peak-based AFP with the\nadaptability and pattern recognition capabilities of neural network-based\napproaches, paving the way for more scalable and efficient solutions in the\nfield.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.21086v1", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.3.1; H.3.3; H.3.4"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21086v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PeakNetFP：基于峰值的神经音频指纹技术，对极端时间拉伸具有鲁棒性", "tldr": "PeakNetFP是一个新的神经音频指纹系统，它利用频谱峰值，在处理极端时间拉伸音频方面优于传统系统，并与最先进的NeuralFP表现相当，同时具有显著的效率优势。", "motivation": "传统的音频指纹识别系统在处理极端时间拉伸音频时面临挑战。本文旨在结合基于峰值的音频指纹技术的轻量级特性与神经网络的适应性和模式识别能力，以开发一个更鲁棒、更高效的解决方案。", "method": "本文介绍了PeakNetFP，一个专门围绕频谱峰值设计的神经音频指纹系统。它采用类似于PointNet++的分层点特征提取技术，并使用对比学习进行训练，类似于NeuralFP。", "result": "PeakNetFP在处理时间拉伸音频数据时，性能优于传统AFP系统，并与NeuralFP相当，在50%至200%的拉伸因子范围内，Top-1命中率保持在90%以上。与NeuralFP相比，PeakNetFP的参数量减少了100倍，输入数据量减少了11倍。", "conclusion": "PeakNetFP成功地将基于峰值的AFP的轻量级特性与神经网络的适应性和模式识别能力相结合，为未来音频指纹技术提供了有前景、可扩展且高效的解决方案，特别适用于涉及时间拉伸的场景。", "translation": "这项工作介绍了 PeakNetFP，这是第一个专门围绕频谱峰值设计的神经音频指纹（AFP）系统。这个新颖的系统旨在利用传统基于峰值的 AFP 方法通常计算的稀疏频谱坐标。PeakNetFP 执行类似于计算机视觉模型 PointNet++ 的分层点特征提取技术，并使用对比学习进行训练，类似于最先进的深度学习 AFP NeuralFP。这种组合使得 PeakNetFP 能够超越传统的 AFP 系统，并在处理具有挑战性的时间拉伸音频数据时，实现与 NeuralFP 相当的性能。在广泛的评估中，PeakNetFP 在 50% 到 200% 的拉伸因子范围内，保持了超过 90% 的 Top-1 命中率。此外，PeakNetFP 提供了显著的效率优势：与 NeuralFP 相比，它的参数少了 100 倍，输入数据小了 11 倍。这些特性使 PeakNetFP 成为处理时间拉伸的 AFP 任务的轻量级高效解决方案。总的来说，该系统代表了未来 AFP 技术的一个有前景的方向，因为它成功地将基于峰值的 AFP 的轻量级特性与基于神经网络的方法的适应性和模式识别能力相结合，为该领域更具可扩展性和效率的解决方案铺平了道路。", "summary": "PeakNetFP是一种新型的基于频谱峰值的神经音频指纹系统，它结合了PointNet++的分层特征提取和对比学习。该系统在处理极端时间拉伸的音频数据时，性能优于传统方法并与先进的NeuralFP相当。同时，PeakNetFP具有显著的效率优势，参数量和输入数据量远小于NeuralFP，为音频指纹任务提供了一个轻量且高效的解决方案。", "keywords": "音频指纹, 频谱峰值, 神经网络, 时间拉伸, PeakNetFP", "comments": "PeakNetFP的创新之处在于其成功融合了传统基于峰值AFP的效率与神经网络的强大模式识别能力，特别是在应对极端时间拉伸方面的鲁棒性。其显著的效率提升（参数减少100倍，输入数据量减少11倍）对于实际部署和扩展性具有重要意义，预示着音频指纹技术未来发展的一个有前景的方向。"}}
{"id": "2506.20926", "title": "CodeGuard: A Generalized and Stealthy Backdoor Watermarking for Generative Code Models", "authors": ["Haoxuan Li", "Jiale Zhang", "Xiaobing Sun", "Xiapu Luo"], "summary": "Generative code models (GCMs) significantly enhance development efficiency\nthrough automated code generation and code summarization. However, building and\ntraining these models require computational resources and time, necessitating\neffective digital copyright protection to prevent unauthorized leaks and\nmisuse. Backdoor watermarking, by embedding hidden identifiers, simplifies\ncopyright verification by breaking the model's black-box nature. Current\nbackdoor watermarking techniques face two main challenges: first, limited\ngeneralization across different tasks and datasets, causing fluctuating\nverification rates; second, insufficient stealthiness, as watermarks are easily\ndetected and removed by automated methods. To address these issues, we propose\nCodeGuard, a novel watermarking method combining attention mechanisms with\ndistributed trigger embedding strategies. Specifically, CodeGuard employs\nattention mechanisms to identify watermark embedding positions, ensuring\nverifiability. Moreover, by using homomorphic character replacement, it avoids\nmanual detection, while distributed trigger embedding reduces the likelihood of\nautomated detection. Experimental results demonstrate that CodeGuard achieves\nup to 100% watermark verification rates in both code summarization and code\ngeneration tasks, with no impact on the primary task performance. In terms of\nstealthiness, CodeGuard performs exceptionally, with a maximum detection rate\nof only 0.078 against ONION detection methods, significantly lower than\nbaseline methods.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.20926v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20926v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CodeGuard: 一种通用的隐蔽式生成代码模型后门水印技术", "tldr": "CodeGuard是一种新的水印技术，用于保护生成代码模型的版权，通过结合注意力机制和分布式触发器嵌入，实现了高验证率和强隐蔽性。", "motivation": "生成代码模型（GCMs）的版权保护面临挑战。现有后门水印技术存在两个主要问题：泛化能力差导致验证率波动，以及隐蔽性不足容易被自动化方法检测和移除。", "method": "提出CodeGuard方法，结合注意力机制和分布式触发器嵌入策略。具体地，利用注意力机制确定水印嵌入位置以确保可验证性；使用同形字符替换避免人工检测；通过分布式触发器嵌入降低自动化检测的可能性。", "result": "CodeGuard在代码摘要和代码生成任务中实现了高达100%的水印验证率，且不影响主要任务性能。在隐蔽性方面，对ONION检测方法的最高检测率仅为0.078，显著低于基线方法。", "conclusion": "CodeGuard成功解决了现有水印技术的泛化性和隐蔽性问题，在版权保护方面表现出色。", "translation": "生成式代码模型（GCMs）通过自动化代码生成和代码摘要显著提升了开发效率。然而，构建和训练这些模型需要大量的计算资源和时间，因此需要有效的数字版权保护来防止未经授权的泄露和滥用。后门水印技术通过嵌入隐藏标识符，打破了模型的黑箱性质，简化了版权验证。当前后门水印技术面临两大挑战：首先，在不同任务和数据集上的泛化能力有限，导致验证率波动；其次，隐蔽性不足，水印容易被自动化方法检测和移除。为了解决这些问题，我们提出了CodeGuard，一种结合注意力机制和分布式触发器嵌入策略的新型水印方法。具体而言，CodeGuard采用注意力机制来识别水印嵌入位置，确保可验证性。此外，通过使用同形字符替换，它避免了人工检测，而分布式触发器嵌入则降低了自动化检测的可能性。实验结果表明，CodeGuard在代码摘要和代码生成任务中均实现了高达100%的水印验证率，且不影响主要任务性能。在隐蔽性方面，CodeGuard表现出色，对ONION检测方法的最高检测率仅为0.078，显著低于基线方法。", "summary": "本文提出了一种名为CodeGuard的新型后门水印方法，旨在解决生成代码模型版权保护中现有水印技术泛化能力差和隐蔽性不足的问题。CodeGuard通过结合注意力机制来确定水印嵌入位置以确保可验证性，并利用同形字符替换和分布式触发器嵌入策略来提高隐蔽性。实验证明，CodeGuard在代码摘要和代码生成任务中均能达到100%的水印验证率，且不影响模型性能，同时对自动化检测方法具有极强的抵抗力，显著优于现有基线方法。", "keywords": "生成代码模型, 版权保护, 后门水印, CodeGuard, 隐蔽性", "comments": "CodeGuard的创新点在于结合了注意力机制和分布式触发器嵌入，有效提升了水印的泛化性和隐蔽性，解决了现有方法的痛点。其在不影响模型主任务性能的前提下实现高验证率和强抗检测能力，对于GCMs的版权保护具有重要意义。"}}
{"id": "2506.20812", "title": "Model-Based Real-Time Pose and Sag Estimation of Overhead Power Lines Using LiDAR for Drone Inspection", "authors": ["Alexandre Girard", "Steven A. Parkison", "Philippe Hamelin"], "summary": "Drones can inspect overhead power lines while they remain energized,\nsignificantly simplifying the inspection process. However, localizing a drone\nrelative to all conductors using an onboard LiDAR sensor presents several\nchallenges: (1) conductors provide minimal surface for LiDAR beams limiting the\nnumber of conductor points in a scan, (2) not all conductors are consistently\ndetected, and (3) distinguishing LiDAR points corresponding to conductors from\nother objects, such as trees and pylons, is difficult. This paper proposes an\nestimation approach that minimizes the error between LiDAR measurements and a\nsingle geometric model representing the entire conductor array, rather than\ntracking individual conductors separately. Experimental results, using data\nfrom a power line drone inspection, demonstrate that this method achieves\naccurate tracking, with a solver converging under 50 ms per frame, even in the\npresence of partial observations, noise, and outliers. A sensitivity analysis\nshows that the estimation approach can tolerate up to twice as many outlier\npoints as valid conductors measurements.", "comment": "Submitted to IEEE case 2025", "pdf_url": "http://arxiv.org/pdf/2506.20812v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20812v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "架空电力线基于模型的实时姿态和弧垂估计（通过无人机激光雷达进行巡检）", "tldr": "本文提出一种基于模型的激光雷达估计方法，通过单一几何模型实时准确地估计无人机在架空电力线巡检中的姿态和弧垂，有效应对数据稀疏、部分检测和异常点等挑战。", "motivation": "使用无人机激光雷达对架空电力线进行定位面临挑战：导线表面积小导致激光点少，并非所有导线都能持续检测到，且难以区分导线与其他物体（如树木、电塔）的激光点。", "method": "提出一种估计方法，通过最小化激光雷达测量值与表示整个导线阵列的单一几何模型之间的误差，而不是单独跟踪每根导线。", "result": "实验结果表明，该方法实现了准确的跟踪，即使在部分观测、噪声和异常点存在的情况下，求解器也能在每帧50毫秒内收敛。敏感性分析显示，该估计方法能容忍高达有效导线测量点两倍的异常点。", "conclusion": "该方法能够实时、准确地估计架空电力线的姿态和弧垂，并对噪声和异常点表现出高鲁棒性，有效解决了无人机激光雷达巡检中的定位难题。", "translation": "无人机可以在电力线带电的情况下进行巡检，这大大简化了巡检过程。然而，使用机载激光雷达传感器定位无人机相对于所有导线的位置存在几个挑战：(1) 导线为激光雷达光束提供的表面积很小，限制了每次扫描中导线点的数量；(2) 并非所有导线都能持续被检测到；(3) 难以将对应于导线的激光雷达点与其他物体（如树木和电塔）区分开。本文提出一种估计方法，该方法通过最小化激光雷达测量值与表示整个导线阵列的单一几何模型之间的误差，而不是单独跟踪每根导线。使用电力线无人机巡检数据的实验结果表明，即使在部分观测、噪声和异常点存在的情况下，该方法也能实现准确跟踪，并且求解器在每帧50毫秒内收敛。敏感性分析表明，该估计方法可以容忍多达有效导线测量点两倍的异常点。", "summary": "本文提出一种新颖的基于模型的实时估计方法，利用激光雷达数据进行无人机架空电力线巡检中的姿态和弧垂估计。该方法通过将激光雷达测量值与代表整个导线阵列的单一几何模型进行误差最小化，有效克服了传统方法中导线点稀疏、检测不一致和背景干扰等挑战。实验证明，该方法能实现准确跟踪，求解器收敛速度快（每帧50毫秒内），且对部分观测、噪声和大量异常点具有很高的鲁棒性。", "keywords": "架空电力线, 无人机巡检, 激光雷达, 姿态估计, 弧垂估计", "comments": "该论文的创新点在于其采用的基于单一几何模型的整体导线阵列估计方法，而非传统上单独跟踪每根导线，这有效解决了激光雷达数据稀疏和噪声干扰等实际挑战。其在实时性（50毫秒/帧）和对异常点的高容忍度方面表现出色，对于提升无人机电力线巡检的效率和可靠性具有重要意义。"}}
{"id": "2506.20918", "title": "Metadata Enrichment of Long Text Documents using Large Language Models", "authors": ["Manika Lamba", "You Peng", "Sophie Nikolov", "Glen Layne-Worthey", "J. Stephen Downie"], "summary": "In this project, we semantically enriched and enhanced the metadata of long\ntext documents, theses and dissertations, retrieved from the HathiTrust Digital\nLibrary in English published from 1920 to 2020 through a combination of manual\nefforts and large language models. This dataset provides a valuable resource\nfor advancing research in areas such as computational social science, digital\nhumanities, and information science. Our paper shows that enriching metadata\nusing LLMs is particularly beneficial for digital repositories by introducing\nadditional metadata access points that may not have originally been foreseen to\naccommodate various content types. This approach is particularly effective for\nrepositories that have significant missing data in their existing metadata\nfields, enhancing search results and improving the accessibility of the digital\nrepository.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20918v1", "categories": ["cs.DL", "cs.ET", "cs.IR"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.20918v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用大型语言模型对长文本文档进行元数据丰富", "tldr": "本项目利用人工和大型语言模型对HathiTrust数字图书馆中的长文本、论文和学位论文的元数据进行了语义丰富和增强，证明LLM在为数字存储库引入额外元数据访问点方面非常有效，尤其适用于现有元数据缺失的存储库。", "motivation": "该研究旨在解决数字存储库中现有元数据缺失的问题，并通过引入额外的元数据访问点来增强搜索结果和提高数字存储库的可访问性。", "method": "本项目结合了人工努力和大型语言模型（LLMs），对从HathiTrust数字图书馆检索到的1920年至2020年间出版的英文长文本文档、论文和学位论文的元数据进行了语义丰富和增强。", "result": "研究结果表明，使用大型语言模型丰富元数据对于数字存储库特别有益，因为它引入了可能未曾预见的额外元数据访问点，以适应各种内容类型。这种方法对于现有元数据字段中存在大量缺失数据的存储库尤其有效。", "conclusion": "该研究得出结论，利用大型语言模型丰富数字存储库的元数据可以显著提高搜索结果的质量和数字资源的访问便利性，尤其是在原始元数据不完整的情况下。", "translation": "本项目结合人工努力和大型语言模型，对从HathiTrust数字图书馆检索到的1920年至2020年间出版的英文长文本文档、论文和学位论文的元数据进行了语义丰富和增强。该数据集为计算社会科学、数字人文和信息科学等领域的研究进展提供了宝贵的资源。我们的论文表明，使用LLM丰富元数据对于数字存储库特别有益，因为它引入了可能未曾预见的额外元数据访问点，以适应各种内容类型。这种方法对于现有元数据字段中存在大量缺失数据的存储库尤其有效，可以增强搜索结果并提高数字存储库的可访问性。", "summary": "本文探讨了如何利用人工和大型语言模型（LLMs）对HathiTrust数字图书馆中的长文本文档、论文和学位论文的元数据进行语义丰富和增强。研究发现，通过LLMs丰富元数据能够为数字存储库提供额外的访问点，从而有效解决现有元数据缺失的问题，进而改善搜索结果并提升数字资源的整体可访问性。该方法尤其适用于元数据不完整的数字存储库，并为相关研究领域提供了有价值的数据集。", "keywords": "元数据丰富, 大型语言模型, 数字存储库, HathiTrust, 文本分析", "comments": "该论文的创新之处在于结合了人工努力和大型语言模型来解决数字存储库中长期存在的元数据缺失问题。其重要性体现在为数字人文、计算社会科学和信息科学领域提供了新的研究资源和方法，并实际提升了数字图书馆的用户体验和数据可发现性。该方法对于拥有大量遗留数据但元数据不完善的机构具有重要的实践指导意义。"}}
{"id": "2506.20851", "title": "Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach", "authors": ["Srikar Reddy Gadusu", "Larry Callahan", "Samir Lababidi", "Arunasri Nishtala", "Sophia Healey", "Hande McGinty"], "summary": "As data and knowledge expand rapidly, adopting systematic methodologies for\nontology generation has become crucial. With the daily increases in data\nvolumes and frequent content changes, the demand for databases to store and\nretrieve information for the creation of knowledge graphs has become\nincreasingly urgent. The previously established Knowledge Acquisition and\nRepresentation Methodology (KNARM) outlines a systematic approach to address\nthese challenges and create knowledge graphs. However, following this\nmethodology highlights the existing challenge of seamlessly integrating Neo4j\ndatabases with the Web Ontology Language (OWL). Previous attempts to integrate\ndata from Neo4j into an ontology have been discussed, but these approaches\noften require an understanding of description logics (DL) syntax, which may not\nbe familiar to many users. Thus, a more accessible method is necessary to\nbridge this gap. This paper presents a user-friendly approach that utilizes\nPython and its rdflib library to support ontology development. We showcase our\nnovel approach through a Neo4j database we created by integrating data from the\nFood and Drug Administration (FDA) Adverse Event Reporting System (FAERS)\ndatabase. Using this dataset, we developed a Python script that automatically\ngenerates the required classes and their axioms, facilitating a smoother\nintegration process. This approach offers a practical solution to the\nchallenges of ontology generation in the context of rapidly growing adverse\ndrug event datasets, supporting improved drug safety monitoring and public\nhealth decision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20851v1", "categories": ["cs.SE", "cs.AI", "cs.DB"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20851v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "通过自动化集成数据生成可靠不良事件档案（GRAPH-AID）：一种半自动化本体构建方法", "tldr": "本文提出了一种用户友好的基于Python的方法，用于从Neo4j数据库半自动化构建本体，特别针对不良事件数据，旨在弥合图数据库与OWL之间的鸿沟。", "motivation": "随着数据和知识的快速增长，系统化的本体生成方法变得至关重要。现有方法在将Neo4j数据库与Web本体语言（OWL）无缝集成时面临挑战，且通常需要用户理解复杂的描述逻辑（DL）语法，这限制了其可访问性。因此，需要一种更易于访问的方法来解决这一集成和可用性问题。", "method": "本文提出了一种用户友好的方法，利用Python及其rdflib库支持本体开发。研究通过整合美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）数据库的数据，创建了一个Neo4j数据库来展示该方法。此外，还开发了一个Python脚本，用于自动生成所需的类及其公理，以促进更顺畅的集成过程。", "result": "该方法促进了Neo4j数据库与OWL本体之间更顺畅的集成过程。它为快速增长的不良药物事件数据集背景下的本体生成挑战提供了实用解决方案。", "conclusion": "本文提出的方法能够支持改进药物安全监测和公共卫生决策，通过提供一种更易于访问和自动化的本体构建途径。", "translation": "随着数据和知识的迅速扩展，采用系统方法进行本体生成变得至关重要。随着数据量的日常增长和内容的频繁变化，对存储和检索信息以创建知识图谱的数据库的需求变得日益迫切。先前建立的知识获取与表示方法（KNARM）概述了一种解决这些挑战并创建知识图谱的系统方法。然而，遵循这种方法凸显了将Neo4j数据库与Web本体语言（OWL）无缝集成存在的挑战。以前曾讨论过将Neo4j数据集成到本体中的尝试，但这些方法通常需要理解描述逻辑（DL）语法，这对于许多用户来说可能不熟悉。因此，需要一种更易于访问的方法来弥合这一差距。本文提出了一种用户友好的方法，利用Python及其rdflib库支持本体开发。我们通过一个我们创建的Neo4j数据库展示了我们的新颖方法，该数据库集成了来自美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）数据库的数据。利用该数据集，我们开发了一个Python脚本，该脚本自动生成所需的类及其公理，从而促进更顺畅的集成过程。这种方法为快速增长的不良药物事件数据集背景下的本体生成挑战提供了实用解决方案，支持改进药物安全监测和公共卫生决策。", "summary": "本文介绍了GRAPH-AID，一种半自动化、用户友好的本体构建方法，旨在解决Neo4j图数据库与OWL本体之间的集成挑战。该方法利用Python及其rdflib库，从Neo4j数据中自动生成本体类和公理，并以FDA不良事件报告系统数据为例进行了演示。这种方法简化了大型动态数据集的本体开发，从而提升了药物安全监测和公共卫生决策的能力。", "keywords": "本体构建, Neo4j, OWL, Python, 不良事件, 知识图谱", "comments": "该论文的创新之处在于提供了一种更易于访问的、基于Python的解决方案（使用rdflib），以弥合Neo4j数据库和OWL本体之间的鸿沟，从而避免了对描述逻辑（DL）的深入理解。这对于涉及不良事件报告等大型动态数据集的实际应用尤其重要，使得本体生成对于非专业用户而言更加可行，并促进了药物安全等关键领域的真实世界知识图谱构建。"}}
{"id": "2506.20952", "title": "Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment", "authors": ["Kyosuke Ishibashi", "Atsushi Saito", "Zin Y. Tun", "Lucas Ray", "Megan C. Coram", "Akihiro Sakurai", "Allison M. Okamura", "Ko Yamamoto"], "summary": "Human crowd simulation in virtual reality (VR) is a powerful tool with\npotential applications including emergency evacuation training and assessment\nof building layout. While haptic feedback in VR enhances immersive experience,\nits effect on walking behavior in dense and dynamic pedestrian flows is\nunknown. Through a user study, we investigated how haptic feedback changes user\nwalking motion in crowded pedestrian flows in VR. The results indicate that\nhaptic feedback changed users' collision avoidance movements, as measured by\nincreased walking trajectory length and change in pelvis angle. The\ndisplacements of users' lateral position and pelvis angle were also increased\nin the instantaneous response to a collision with a non-player character (NPC),\neven when the NPC was inside the field of view. Haptic feedback also enhanced\nusers' awareness and visual exploration when an NPC approached from the side\nand back. Furthermore, variation in walking speed was increased by the haptic\nfeedback. These results suggested that the haptic feedback enhanced users'\nsensitivity to a collision in VR environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20952v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.20952v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "触觉反馈对动态VR行人环境中避让行为和视觉探索的影响", "tldr": "研究发现，在VR拥挤行人流中，触觉反馈会改变用户的避撞行为和视觉探索，提高对碰撞的敏感性。", "motivation": "尽管触觉反馈能增强VR沉浸式体验，但其对密集动态行人流中步行行为的影响尚不清楚。", "method": "通过一项用户研究，调查了触觉反馈如何改变用户在VR拥挤行人流中的步行运动。研究测量了步行轨迹长度、骨盆角度变化、侧向位置位移以及对非玩家角色（NPC）碰撞的瞬时反应。", "result": "触觉反馈改变了用户的避撞动作，表现为步行轨迹长度和骨盆角度的变化增加。用户在与NPC碰撞时的侧向位置和骨盆角度的位移增加，即使NPC在视野内也是如此。触觉反馈还增强了当NPC从侧面和后面接近时用户的意识和视觉探索。此外，步行速度的变化也增加了。", "conclusion": "这些结果表明，触觉反馈增强了用户在VR环境中对碰撞的敏感性。", "translation": "虚拟现实（VR）中的人群模拟是一个强大的工具，具有潜在的应用，包括紧急疏散训练和建筑布局评估。虽然VR中的触觉反馈增强了沉浸式体验，但其对密集动态行人流中步行行为的影响尚不清楚。通过一项用户研究，我们调查了触觉反馈如何改变用户在VR拥挤行人流中的步行运动。结果表明，触觉反馈改变了用户的避撞动作，通过步行轨迹长度和骨盆角度的变化增加来衡量。即使当非玩家角色（NPC）在视野内时，用户在与NPC碰撞的瞬时反应中，其侧向位置和骨盆角度的位移也增加了。当NPC从侧面和后面接近时，触觉反馈还增强了用户的意识和视觉探索。此外，步行速度的变化也因触觉反馈而增加。这些结果表明，触觉反馈增强了用户在VR环境中对碰撞的敏感性。", "summary": "本文通过一项用户研究，探究了在动态VR行人环境中触觉反馈对用户步行行为的影响。研究发现，触觉反馈显著改变了用户的避撞动作，增加了步行轨迹长度、骨盆角度变化以及对非玩家角色（NPC）碰撞的瞬时侧向位移。同时，触觉反馈还提升了用户对周围环境的意识和视觉探索能力，并增加了步行速度的变异性。这些结果表明，触觉反馈能有效增强用户在VR中对潜在碰撞的敏感度。", "keywords": "触觉反馈, 虚拟现实, 避让行为, 行人模拟, 碰撞敏感性", "comments": "这项研究通过用户实验，明确了触觉反馈在VR行人模拟中对用户行为的积极影响，特别是在避撞和空间感知方面。其创新性在于量化了触觉反馈对具体步行参数（如轨迹长度、骨盆角度）的影响，并指出其有助于提升用户对碰撞的敏感性，这对于VR应用中的沉浸感和真实感具有重要意义。潜在的局限性可能在于研究范围仅限于特定的人群密度和NPC行为模式，以及触觉反馈的具体实现方式。"}}
{"id": "2506.21487", "title": "OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits", "authors": ["Maryam Ghane", "Amir M. Hajisadeghi", "Hamid R. Zarandi"], "summary": "This paper presents OptGM, an optimized gate merging method designed to\nmitigate negative bias temperature instability (NBTI) in digital circuits.\nFirst, the proposed approach effectively identifies NBTI-critical internal\nnodes, defined as those with a signal probability exceeding a predefined\nthreshold. Next, based on the proposed optimized algorithm, the sensitizer gate\n(which drives the critical node) and the sensitive gate (which is fed by it)\nare merged into a new complex gate. This complex gate preserves the original\nlogic while eliminating NBTI-critical nodes. Finally, to evaluate the\neffectiveness of OptGM, we assess it on several combinational and sequential\nbenchmark circuits. Simulation results demonstrate that, on average, the number\nof NBTI-critical transistors (i.e., PMOS transistors connected to critical\nnodes), NBTI-induced delay degradation, and the total transistor count are\nreduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances\nperformance per cost (PPC) by 12.8% on average, with minimal area overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21487v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.21487v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "OptGM：一种优化的门合并方法，用于缓解数字电路中的NBTI效应", "tldr": "OptGM是一种优化的门合并方法，通过识别并合并关键节点相关的门来缓解数字电路中的NBTI效应，显著减少了NBTI关键晶体管数量和延迟退化。", "motivation": "该研究旨在缓解数字电路中的负偏压温度不稳定性（NBTI）问题。", "method": "OptGM首先识别信号概率超过预设阈值的NBTI关键内部节点。接着，根据优化的算法，将驱动关键节点的敏感器门和被其馈电的敏感门合并成一个新的复杂门，以消除NBTI关键节点并保持原有逻辑。", "result": "仿真结果表明，OptGM平均减少了89.29%的NBTI关键晶体管（PMOS），23.87%的NBTI引起的延迟退化，以及6.47%的总晶体管数量。此外，OptGM平均提升了12.8%的性能成本比（PPC），且面积开销最小。", "conclusion": "OptGM是一种有效的门合并方法，能够显著缓解数字电路中的NBTI效应，同时优化性能并保持较低的面积开销。", "translation": "本文提出了一种名为OptGM的优化门合并方法，旨在缓解数字电路中的负偏压温度不稳定性（NBTI）。首先，所提出的方法有效地识别NBTI关键内部节点，这些节点被定义为信号概率超过预设阈值的节点。其次，基于所提出的优化算法，将敏感器门（驱动关键节点的门）和敏感门（被其馈电的门）合并成一个新的复杂门。这个复杂门在保留原始逻辑的同时消除了NBTI关键节点。最后，为了评估OptGM的有效性，我们在几个组合和时序基准电路上进行了评估。仿真结果表明，平均而言，NBTI关键晶体管（即连接到关键节点的PMOS晶体管）的数量、NBTI引起的延迟退化和总晶体管数量分别减少了89.29%、23.87%和6.47%。此外，OptGM平均将性能成本比（PPC）提高了12.8%，且面积开销最小。", "summary": "本文提出OptGM，一种优化的门合并方法，用于缓解数字电路中的NBTI效应。该方法通过识别NBTI关键内部节点，并将相关联的敏感器门和敏感门合并成新的复杂门来消除这些关键节点。在多种基准电路上的仿真结果显示，OptGM显著减少了NBTI关键晶体管数量和延迟退化，同时降低了总晶体管数量并提升了性能成本比，且面积开销极小。", "keywords": "NBTI, 门合并, 数字电路, 可靠性, 性能优化", "comments": "OptGM的创新之处在于其优化的门合并算法，能够精准识别并消除NBTI关键节点，这对于提高数字电路的长期可靠性至关重要。其在减少NBTI效应的同时，还能在晶体管数量、延迟和性能成本比方面带来显著改进，显示出强大的实用价值和效率。"}}
{"id": "2506.21126", "title": "Semantic-aware Digital Twin for AI-based CSI Acquisition", "authors": ["Jiajia Guo", "Yiming Cui", "Shi Jin"], "summary": "Artificial intelligence (AI) substantially enhances channel state information\n(CSI) acquisition performance but is limited by its reliance on single-modality\ninformation and deployment challenges, particularly in dataset collection. This\npaper investigates the use of semantic-aware digital twin (DT) to enhance\nAI-based CSI acquisition. We first briefly introduce the motivation and recent\nadvancements in AI-driven CSI acquisition and semantic-aware DT employment for\nair interfaces. Then, we thoroughly explore how semantic-aware DT can bolster\nAI-based CSI acquisition. We categorizes the semantic-aware DT for AI-based CSI\nacquisition into two classes: enhancing AI-based CSI acquisition through\nintegration with DT and using DT to aid AI-based CSI deployment. Potential\nintegration frameworks are introduced in detail. Finally, we conclude by\noutlining potential research directions within the semantic-aware DT-assisted\nAI-based CSI acquisition.", "comment": "This article has been accepted by IEEE Communications Standards\n  Magazine", "pdf_url": "http://arxiv.org/pdf/2506.21126v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21126v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "面向AI的CSI获取的语义感知数字孪生", "tldr": "本文探讨了如何利用语义感知数字孪生技术来增强基于AI的信道状态信息(CSI)获取，旨在解决AI在CSI获取中面临的单模态信息依赖和部署挑战。", "motivation": "人工智能（AI）显著提升了信道状态信息（CSI）获取性能，但受限于对单一模态信息的依赖以及部署挑战，特别是数据集收集方面的挑战。", "method": "本文首先介绍了AI驱动的CSI获取和空口语义感知数字孪生应用的动机及最新进展。然后，深入探讨了语义感知数字孪生如何增强基于AI的CSI获取，将其分为通过与数字孪生集成增强AI-CSI获取和利用数字孪生辅助AI-CSI部署两类，并详细介绍了潜在的集成框架。", "result": "Not mentioned in abstract", "conclusion": "论文最后概述了语义感知数字孪生辅助AI-CSI获取中潜在的研究方向。", "translation": "人工智能（AI）显著提升了信道状态信息（CSI）获取性能，但受限于对单一模态信息的依赖以及部署挑战，特别是数据集收集方面的挑战。本文研究了利用语义感知数字孪生（DT）来增强基于AI的CSI获取。我们首先简要介绍了AI驱动的CSI获取和空口语义感知数字孪生应用的动机及最新进展。然后，我们深入探讨了语义感知数字孪生如何增强基于AI的CSI获取。我们将面向AI的CSI获取的语义感知数字孪生分为两类：通过与数字孪生集成增强AI-CSI获取，以及利用数字孪生辅助AI-CSI部署。文中详细介绍了潜在的集成框架。最后，我们总结并概述了语义感知数字孪生辅助AI-CSI获取中潜在的研究方向。", "summary": "本文探讨了语义感知数字孪生（DT）在增强基于人工智能（AI）的信道状态信息（CSI）获取方面的潜力，旨在克服AI在CSI获取中面临的单模态信息依赖和部署难题。论文将语义感知DT对AI-CSI的增强分为集成强化和部署辅助两类，并提出了具体的集成框架，最后指出了未来的研究方向。", "keywords": "语义感知数字孪生, 信道状态信息获取, 人工智能, 空口, 部署", "comments": "这篇论文提出了一种将语义感知数字孪生与AI结合的新颖方法，以解决当前AI在CSI获取中面临的实际部署挑战，特别是数据收集和单一模态信息限制。其创新点在于将数字孪生引入到通信领域的AI应用中，为AI-CSI获取提供更丰富、更真实的语义信息和部署支持。这对于提升未来无线通信系统的智能化水平具有重要意义。"}}
{"id": "2506.20971", "title": "Where is AIED Headed? Key Topics and Emerging Frontiers (2020-2024)", "authors": ["Shihui Feng", "Huilin Zhang", "Dragan Gašević"], "summary": "In this study, we analyze 2,398 research articles published between 2020 and\n2024 across eight core venues related to the field of Artificial Intelligence\nin Education (AIED). Using a three-step knowledge co-occurrence network\nanalysis, we analyze the knowledge structure of the field, the evolving\nknowledge clusters, and the emerging frontiers. Our findings reveal that AIED\nresearch remains strongly technically focused, with sustained themes such as\nintelligent tutoring systems, learning analytics, and natural language\nprocessing, alongside rising interest in large language models (LLMs) and\ngenerative artificial intelligence (GenAI). By tracking the bridging keywords\nover the past five years, we identify four emerging frontiers in AIED--LLMs,\nGenAI, multimodal learning analytics, and human-AI collaboration. The current\nresearch interests in GenAI are centered around GAI-driven personalization,\nself-regulated learning, feedback, assessment, motivation, and ethics.The key\nresearch interests and emerging frontiers in AIED reflect a growing emphasis on\nco-adaptive, human-centered AI for education. This study provides the first\nlarge-scale field-level mapping of AIED's transformation in the GenAI era and\nsheds light on the future research development and educational practices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20971v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20971v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "AIED 将走向何方？关键主题与新兴前沿 (2020-2024)", "tldr": "本研究通过分析2020-2024年AIED领域的2398篇文章，揭示了该领域的技术焦点、持续主题以及LLMs、GenAI等新兴前沿，并指出AIED正转向以人为中心的协同适应型AI。", "motivation": "该研究旨在分析人工智能教育（AIED）领域的知识结构、演变中的知识集群以及新兴前沿，特别是在生成式人工智能（GenAI）时代AIED的转型。", "method": "研究分析了2020年至2024年间在八个核心场馆发表的2398篇人工智能教育（AIED）研究文章。采用三步知识共现网络分析方法，并追踪过去五年中的桥接关键词。", "result": "AIED研究仍以技术为中心，持续主题包括智能辅导系统、学习分析和自然语言处理。大型语言模型（LLMs）和生成式人工智能（GenAI）的兴趣日益增长，并确定了LLMs、GenAI、多模态学习分析和人机协作作为四个新兴前沿。GenAI的研究兴趣集中在个性化、自主学习、反馈、评估、动机和伦理方面。", "conclusion": "AIED的关键研究兴趣和新兴前沿反映出对协同适应的、以人为本的教育人工智能的日益重视。本研究首次对AIED在GenAI时代的转型进行了大规模的领域级映射，并为未来的研究发展和教育实践提供了启示。", "translation": "在这项研究中，我们分析了2020年至2024年间在八个与教育人工智能（AIED）领域相关的核心场所发表的2398篇研究文章。通过三步知识共现网络分析，我们分析了该领域的知识结构、演变中的知识集群以及新兴前沿。我们的发现表明，AIED研究仍然高度专注于技术，持续的主题如智能辅导系统、学习分析和自然语言处理，同时对大型语言模型（LLMs）和生成式人工智能（GenAI）的兴趣日益增长。通过追踪过去五年中的桥接关键词，我们确定了AIED的四个新兴前沿——LLMs、GenAI、多模态学习分析和人机协作。目前GenAI的研究兴趣集中在GAI驱动的个性化、自主学习、反馈、评估、动机和伦理方面。AIED的关键研究兴趣和新兴前沿反映出对协同适应的、以人为本的教育人工智能的日益重视。这项研究提供了AIED在GenAI时代转型首次大规模的领域级映射，并为未来的研究发展和教育实践提供了启示。", "summary": "本研究通过对2020-2024年间2398篇AIED领域文章进行知识共现网络分析，揭示了AIED研究的技术焦点和持续主题（如智能辅导系统、学习分析），并识别出大型语言模型（LLMs）、生成式人工智能（GenAI）、多模态学习分析和人机协作等新兴前沿。研究强调AIED正日益转向以人为本的、协同适应的人工智能教育，并首次提供了AIED在GenAI时代转型的大规模图谱。", "keywords": "AIED, 大型语言模型, 生成式人工智能, 学习分析, 人机协作", "comments": "这项研究通过大规模文献分析，清晰地描绘了AIED领域在过去五年间的演变轨迹，特别是在GenAI兴起背景下的转型。其创新之处在于利用知识共现网络分析识别新兴前沿，为该领域的未来研究方向提供了宝贵的指引。研究强调了AIED从纯技术导向向以人为本、协同适应方向的转变，对于教育实践和政策制定具有重要参考价值。"}}
{"id": "2506.20965", "title": "Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment", "authors": ["Craig Steven Wright"], "summary": "This paper integrates Austrian capital theory with repeated game theory to\nexamine strategic miner behaviour under different institutional conditions in\nblockchain systems. It shows that when protocol rules are mutable, effective\ntime preference rises, undermining rational long-term planning and cooperative\nequilibria. Using formal game-theoretic analysis and Austrian economic\nprinciples, the paper demonstrates how mutable protocols shift miner incentives\nfrom productive investment to political rent-seeking and influence games. The\noriginal Bitcoin protocol is interpreted as an institutional anchor: a fixed\nrule-set enabling calculability and low time preference. Drawing on the work of\nBohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability\nis essential for restoring strategic coherence, entrepreneurial confidence, and\nsustainable network equilibrium.", "comment": "Approximately 10,770 words, 0 figure, 0 table. Submitted to The\n  Quarterly Journal of Austrian Economics", "pdf_url": "http://arxiv.org/pdf/2506.20965v1", "categories": ["econ.GN", "cs.CR", "cs.GT", "cs.NI", "q-fin.EC", "q-fin.GN", "91B42, 91A25, 91B50", "K.4.4; J.4; C.2.4"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.20965v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "理性矿工行为、协议稳定性与时间偏好：比特币激励环境的奥地利学派与博弈论分析", "tldr": "本文结合奥地利资本理论和重复博弈论，分析了区块链系统中矿工的战略行为。研究表明，可变协议会提高时间偏好，损害长期规划，并促使矿工转向寻租活动。比特币的原始协议被视为固定规则集，有助于可计算性和低时间偏好。协议不变性对于网络稳定至关重要。", "motivation": "本文旨在整合奥地利资本理论与重复博弈论，以检验区块链系统中在不同制度条件下的战略矿工行为，并探讨协议规则可变性对矿工激励和网络稳定性的影响。", "method": "本研究整合了奥地利资本理论与重复博弈论，并运用形式化的博弈论分析和奥地利经济学原理来论证其观点。", "result": "研究表明，当协议规则可变时，有效时间偏好会上升，从而破坏理性的长期规划和合作均衡。可变协议会将矿工的激励从生产性投资转向政治寻租和影响力博弈。", "conclusion": "协议的不变性对于恢复战略连贯性、企业信心和可持续的网络均衡至关重要。", "translation": "本文将奥地利资本理论与重复博弈论相结合，以考察区块链系统在不同制度条件下的战略矿工行为。研究表明，当协议规则可变时，有效时间偏好会上升，从而破坏理性的长期规划和合作均衡。通过形式化的博弈论分析和奥地利经济学原理，本文论证了可变协议如何将矿工的激励从生产性投资转向政治寻租和影响力博弈。原始的比特币协议被解释为一个制度锚：一个固定的规则集，能够实现可计算性和低时间偏好。借鉴庞巴维克、米塞斯和哈耶克的研究，本文提出协议的不变性对于恢复战略连贯性、企业信心和可持续的网络均衡至关重要。", "summary": "本文结合奥地利资本理论和重复博弈论，深入分析了区块链矿工的战略行为及其受协议规则可变性的影响。研究发现，可变的协议规则会提高矿工的有效时间偏好，从而损害其长期规划能力和合作意愿，并促使他们从生产性投资转向政治寻租。文章指出，原始比特币协议的固定规则集作为一种制度锚，有助于维持可计算性和低时间偏好。因此，论文强调协议的不可变性是实现战略一致性、增强创业信心和维护网络可持续均衡的关键。", "keywords": "比特币, 矿工行为, 协议稳定性, 时间偏好, 博弈论, 奥地利经济学", "comments": "本文创新性地将奥地利经济学派的资本理论与博弈论相结合，为理解比特币等区块链网络的激励机制和稳定性提供了独特的视角。通过引入“时间偏好”的概念来解释协议可变性对矿工行为的影响，为区块链协议设计提供了重要的理论依据和政策建议。其对协议不变性重要性的强调，对于当前区块链治理的讨论具有现实意义。"}}
{"id": "2506.20757", "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20757v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20757v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "ConViTac：使用对比表示对齐视觉-触觉融合", "tldr": "ConViTac 提出了一种新的视觉-触觉融合方法，通过对比表示和对比嵌入条件（CEC）机制来提高特征对齐，在材料分类和抓取预测任务中显著优于现有技术。", "motivation": "机器人中的视觉和触觉是互补的感觉模态，但现有的视觉-触觉融合方法（如特征相加和连接）在模态融合时往往导致特征整合不佳。", "method": "本文提出了 ConViTac，一个视觉-触觉表示学习网络。其核心是对比嵌入条件（CEC）机制，该机制利用自监督对比学习预训练的对比编码器，将视觉和触觉输入投影到统一的潜在嵌入中。这些嵌入通过跨模态注意力耦合视觉-触觉特征融合，以对齐统一表示并增强下游任务的性能。", "result": "ConViTac 在真实世界中表现出优于当前最先进方法的卓越性能。所提出的 CEC 机制有效性显著，在材料分类和抓取预测任务中，准确率提高了高达 12.0%。", "conclusion": "ConViTac 通过对比表示和对比嵌入条件（CEC）机制有效解决了视觉-触觉融合中特征对齐不佳的问题，并在多项下游任务中取得了显著的性能提升，证明了其在机器人感知和操作任务中的优越性。", "translation": "视觉和触觉是机器人最基本的两种感官模式，它们提供互补信息，增强感知和操作任务。先前的研究试图共同学习视觉-触觉表示，以提取更有意义的信息。然而，这些方法通常依赖于直接组合，例如特征相加和连接，进行模态融合，这往往导致特征整合不良。在本文中，我们提出了 ConViTac，一个视觉-触觉表示学习网络，旨在通过对比表示来增强融合过程中的特征对齐。我们的主要贡献是对比嵌入条件（CEC）机制，它利用通过自监督对比学习预训练的对比编码器，将视觉和触觉输入投影到统一的潜在嵌入中。这些嵌入通过跨模态注意力耦合视觉-触觉特征融合，旨在对齐统一表示并增强下游任务的性能。我们进行了广泛的实验，证明了 ConViTac 在现实世界中优于当前最先进的方法，以及我们提出的 CEC 机制的有效性，该机制在材料分类和抓取预测任务中将准确率提高了高达 12.0%。", "summary": "本文提出 ConViTac，一个用于视觉-触觉表示学习的网络，旨在解决传统融合方法中特征整合不佳的问题。通过引入对比嵌入条件（CEC）机制，ConViTac 利用自监督对比学习预训练的编码器将视觉和触觉输入映射到统一的潜在空间，并通过跨模态注意力实现特征对齐。实验证明 ConViTac 在真实世界任务中优于现有技术，并在材料分类和抓取预测中将准确率提高了高达 12.0%。", "keywords": "视觉-触觉融合, 对比学习, 机器人感知, 特征对齐, 多模态学习", "comments": "ConViTac 的创新之处在于其提出的对比嵌入条件（CEC）机制，它通过对比学习实现了视觉和触觉模态的深度对齐，有效解决了传统融合方法中特征整合不良的问题。这种方法对于需要多模态感知的机器人任务具有重要意义，尤其是在材料识别和抓取操作方面表现出显著的性能提升。"}}
{"id": "2506.20675", "title": "Utility-Driven Speculative Decoding for Mixture-of-Experts", "authors": ["Anish Saxena", "Po-An Tsai", "Hritvik Taneja", "Aamer Jaleel", "Moinuddin Qureshi"], "summary": "GPU memory bandwidth is the main bottleneck for low-latency Large Language\nModel (LLM) inference. Speculative decoding leverages idle GPU compute by using\na lightweight drafter to propose K tokens, which the LLM verifies in parallel,\nboosting token throughput. In conventional dense LLMs, all model weights are\nfetched each iteration, so speculation adds no latency overhead. Emerging\nMixture of Experts (MoE) models activate only a subset of weights per token,\ngreatly reducing data movement. However, we show that speculation is\nineffective for MoEs: draft tokens collectively activate more weights,\nincreasing data movement and verification time by 2-3x. When token throughput\ngains fail to offset this overhead, speculation causes slowdowns up to 1.5x,\nmaking it infeasible. Even when useful, the optimal K varies by task, model,\nand even between requests and iterations. Thus, despite widespread use in dense\nLLMs, speculation remains impractical in leading MoEs.\n  We present Cascade, a utility-driven framework that selectively enables\nspeculation to avoid slowdowns and dynamically tunes K to accelerate MoE\nserving. Cascade uses a lightweight metric, speculation utility, the ratio of\ntoken gains to verification cost, which shows iteration-level locality,\nenabling periodic decisions via short test and longer set phases. For each\nrequest, Cascade disables speculation if utility drops below one during\ntesting, and when utility exceeds one, tests multiple K-values to choose the\nutility-maximizing K for the set phase. We implement Cascade in vLLM and\nevaluate it on five popular MoEs with workloads spanning code, math,\nextraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and\nimproves throughput by 7-14% over static K, making speculative decoding\npractical for MoEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20675v1", "categories": ["cs.DC", "cs.AI", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20675v1", "date": "2025-06-17", "updated": "2025-06-17", "AI": {"title_translation": "效用驱动的专家混合模型推测解码", "tldr": "推测解码在MoE模型中因额外的数据移动而效率低下，导致速度下降。本文提出了Cascade框架，通过动态调整和选择性启用推测解码来提高MoE模型的吞吐量。", "motivation": "GPU内存带宽是大型语言模型（LLM）推理的主要瓶颈。推测解码通过利用轻量级草稿器来提升传统密集型LLM的吞吐量。然而，研究发现推测解码对新兴的专家混合（MoE）模型无效，因为草稿令牌会激活更多权重，导致数据移动和验证时间增加2-3倍，甚至造成高达1.5倍的速度下降，使其在MoE模型中不可行。", "method": "本文提出了Cascade框架，一个效用驱动的框架，它选择性地启用推测解码以避免速度下降，并动态调整K值以加速MoE服务。Cascade使用一个轻量级指标——推测效用（令牌增益与验证成本之比），该指标显示出迭代级别的局部性，通过短测试阶段和长设置阶段实现周期性决策。对于每个请求，如果效用在测试期间下降到1以下，Cascade会禁用推测解码；当效用超过1时，它会测试多个K值，选择效用最大化的K值用于设置阶段。该框架在vLLM中实现。", "result": "Cascade框架将速度下降限制在5%（对比1.5倍的速度下降），并且比静态K值提高了7-14%的吞吐量。这些结果使得推测解码在MoE模型中变得实用。", "conclusion": "尽管推测解码在密集型LLM中广泛使用，但其在MoE模型中存在固有的效率问题。通过引入效用驱动的Cascade框架，该研究成功解决了MoE模型中推测解码的局限性，使其能够有效地限制速度下降并显著提高吞吐量，从而使推测解码在领先的MoE模型中变得实用。", "translation": "GPU内存带宽是低延迟大型语言模型（LLM）推理的主要瓶颈。推测解码通过使用轻量级草稿器提出K个令牌，然后LLM并行验证这些令牌，从而利用空闲的GPU计算能力，提高令牌吞吐量。在传统的密集型LLM中，所有模型权重在每次迭代中都被获取，因此推测不会增加延迟开销。新兴的专家混合（MoE）模型每个令牌只激活一部分权重，大大减少了数据移动。然而，我们发现推测解码对MoE模型无效：草稿令牌共同激活了更多的权重，使数据移动和验证时间增加了2-3倍。当令牌吞吐量增益无法抵消此开销时，推测会导致高达1.5倍的速度下降，使其不可行。即使有用，最佳K值也会因任务、模型，甚至请求和迭代而异。因此，尽管在密集型LLM中广泛使用，推测解码在领先的MoE模型中仍然不切实际。\n我们提出了Cascade，一个效用驱动的框架，它选择性地启用推测解码以避免速度下降，并动态调整K值以加速MoE服务。Cascade使用一个轻量级指标，即推测效用，它是令牌增益与验证成本之比，显示出迭代级别的局部性，通过短测试阶段和长设置阶段实现周期性决策。对于每个请求，如果效用在测试期间下降到1以下，Cascade会禁用推测解码；当效用超过1时，它会测试多个K值，选择效用最大化的K值用于设置阶段。我们在vLLM中实现了Cascade，并在五个流行的MoE模型上进行了评估，这些工作负载涵盖了代码、数学、提取和混合任务。Cascade将速度下降限制在5%（对比1.5倍），并且比静态K值提高了7-14%的吞吐量，使得推测解码在MoE模型中变得实用。", "summary": "该论文解决了推测解码在专家混合（MoE）大型语言模型中效率低下的问题。传统推测解码在MoE模型中因激活更多权重而导致数据移动和验证时间显著增加，甚至造成性能下降。为解决此问题，本文提出了Cascade框架，一个效用驱动的系统，它基于“推测效用”（令牌增益与验证成本之比）动态地启用或禁用推测解码，并优化K值。Cascade通过周期性的测试和设置阶段，确保在效用较低时禁用推测，在效用较高时选择最佳K。在vLLM上的实验表明，Cascade能将MoE模型的速度下降限制在5%以内，并相对静态K值实现7-14%的吞吐量提升，从而使推测解码在MoE模型中变得实用。", "keywords": "推测解码, 专家混合模型, LLM推理, 效用驱动, Cascade框架", "comments": "该论文提出了一个创新性的解决方案，解决了推测解码在专家混合（MoE）模型中面临的实际挑战。通过引入“推测效用”这一轻量级指标和动态调整策略，Cascade框架有效地将推测解码从MoE模型的性能瓶颈转变为加速工具。其创新性在于识别了MoE模型中推测解码的根本问题（额外的数据移动），并提供了一个自适应的、实用的解决方案，使其在实际应用中可行。这项工作对于优化MoE模型的推理性能具有重要意义，尤其是在追求低延迟和高吞吐量的场景下。"}}
{"id": "2506.21362", "title": "Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation", "authors": ["Chang Liu", "Yixin Wang", "Moontae Lee"], "summary": "Efficient access to high-quality information is vital for online platforms.\nTo promote more useful information, users not only create new content but also\nevaluate existing content, often through helpfulness voting. Although\naggregated votes help service providers rank their user content, these votes\nare often biased by disparate accessibility per position and the cascaded\ninfluence of prior votes. For a fairer assessment of information quality, we\npropose the Counterfactual Voting Adjustment (CVA), a causal framework that\naccounts for the context in which individual votes are cast. Through\npreliminary and semi-synthetic experiments, we show that CVA effectively models\nthe position and herding biases, accurately recovering the predefined content\nquality. In a real experiment, we demonstrate that reranking content based on\nthe learned quality by CVA exhibits stronger alignment with both user sentiment\nand quality evaluation assessed by GPT-4o, outperforming system rankings based\non aggregated votes and model-based rerankings without causal inference. Beyond\nthe individual quality inference, our embeddings offer comparative insights\ninto the behavioral dynamics of expert user groups across 120 major\nStackExchange communities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21362v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.21362v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "反事实投票调整：用于在线平台中内容质量评估和更公平投票的有用性评价", "tldr": "本文提出了一种名为反事实投票调整（CVA）的因果框架，用于解决在线平台中有用性投票中的位置和从众偏差，从而实现更公平的内容质量评估和排序。", "motivation": "在线平台中的有用性投票常因位置差异和先前投票的级联影响而产生偏差，导致对信息质量的评估不够公平。", "method": "本文提出了一种名为反事实投票调整（CVA）的因果框架，该框架考虑了个体投票所处的上下文，旨在建模和纠正位置偏差和从众偏差。", "result": "在初步和半合成实验中，CVA有效建模了位置和从众偏差，并准确恢复了预定义的内容质量。在真实实验中，基于CVA学习到的质量对内容进行重新排序，与用户情感和GPT-4o评估的质量表现出更强的一致性，优于基于聚合投票的系统排名和没有因果推断的模型重排序。此外，CVA的嵌入还为120个主要StackExchange社区中专家用户群体的行为动态提供了比较性见解。", "conclusion": "反事实投票调整（CVA）框架能够有效纠正在线平台中有用性投票中的偏差，从而实现更公平、更准确的内容质量评估和排序，并且其嵌入能提供用户行为洞察。", "translation": "高效获取高质量信息对在线平台至关重要。为了推广更有用的信息，用户不仅创建新内容，还会评估现有内容，通常通过有用性投票。尽管聚合投票有助于服务提供商对用户内容进行排名，但这些投票常常因位置可访问性差异和先前投票的级联影响而产生偏差。为了更公平地评估信息质量，我们提出了反事实投票调整（CVA），这是一个考虑个体投票所处上下文的因果框架。通过初步和半合成实验，我们证明了CVA能有效建模位置和从众偏差，准确恢复预定义的内容质量。在真实实验中，我们证明基于CVA学习到的质量对内容进行重新排序，与用户情感和GPT-4o评估的质量表现出更强的一致性，优于基于聚合投票的系统排名和没有因果推断的模型重排序。除了个体质量推断，我们的嵌入还为120个主要StackExchange社区中专家用户群体的行为动态提供了比较性见解。", "summary": "本文提出了一种名为反事实投票调整（CVA）的因果框架，旨在解决在线平台中有用性投票中因位置和从众效应引起的内容质量评估偏差。CVA通过考虑投票上下文来建模这些偏差，并在合成和真实实验中证明其能准确恢复内容质量，并生成与用户情感和GPT-4o评估更一致的排名。此外，该框架的嵌入还提供了对专家用户群体行为的洞察。", "keywords": "反事实投票调整, 因果推断, 在线平台, 有用性投票, 偏差校正", "comments": "本文提出CVA框架，通过因果推断解决了在线平台有用性投票中普遍存在的偏差问题，如位置偏差和从众效应，这对于在线内容质量评估和排序具有重要意义。其创新之处在于引入因果框架来校正偏差，并通过与GPT-4o评估对齐验证了其有效性。此外，其产生的嵌入还能用于分析用户行为动态，拓宽了应用范围。"}}
{"id": "2506.20699", "title": "On Context-Content Uncertainty Principle", "authors": ["Xin Li"], "summary": "The Context-Content Uncertainty Principle (CCUP) proposes that inference\nunder uncertainty is governed by an entropy asymmetry between context and\ncontent: high-entropy contexts must be interpreted through alignment with\nlow-entropy, structured content. In this paper, we develop a layered\ncomputational framework that derives operational principles from this\nfoundational asymmetry. At the base level, CCUP formalizes inference as\ndirectional entropy minimization, establishing a variational gradient that\nfavors content-first structuring. Building upon this, we identify four\nhierarchical layers of operational principles: (\\textbf{L1}) \\emph{Core\nInference Constraints}, including structure-before-specificity, asymmetric\ninference flow, cycle-consistent bootstrapping, and conditional compression,\nall shown to be mutually reducible; (\\textbf{L2}) \\emph{Resource Allocation\nPrinciples}, such as precision-weighted attention, asymmetric learning rates,\nand attractor-based memory encoding; (\\textbf{L3}) \\emph{Temporal Bootstrapping\nDynamics}, which organize learning over time via structure-guided curricula;\nand (\\textbf{L4}) \\emph{Spatial Hierarchical Composition}, which integrates\nthese mechanisms into self-organizing cycles of memory, inference, and\nplanning. We present formal equivalence theorems, a dependency lattice among\nprinciples, and computational simulations demonstrating the efficiency gains of\nCCUP-aligned inference. This work provides a unified theoretical foundation for\nunderstanding how brains and machines minimize uncertainty through recursive\nstructure-specificity alignment. The brain is not just an inference machine. It\nis a cycle-consistent entropy gradient resolver, aligning structure and\nspecificity via path-dependent, content-seeded simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20699v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20699v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "论语境-内容不确定性原理", "tldr": "语境-内容不确定性原理(CCUP)提出，不确定性下的推断受语境和内容之间熵不对称的支配。本文提出了一个分层计算框架，从这一基本不对称性中推导出操作原理，并展示了其在减少不确定性方面的效率增益。", "motivation": "本研究旨在开发一个分层计算框架，从语境-内容不确定性原理（CCUP）这一基础不对称性中推导出操作原理，并提供一个统一的理论基础，以理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性。", "method": "本文提出了一个分层计算框架，该框架将推断形式化为定向熵最小化，并建立了有利于内容优先构建的变分梯度。在此基础上，作者识别出四个操作原理的层级：核心推断约束（L1）、资源分配原理（L2）、时间自举动力学（L3）和空间层级组合（L4）。研究还提出了形式等价定理、原理间的依赖格，并进行了计算模拟。", "result": "研究发现，核心推断约束（L1）中的所有原则（包括结构优先于特异性、不对称推断流、循环一致性自举和条件压缩）可以相互归约。计算模拟证明了与CCUP对齐的推断具有效率增益。", "conclusion": "本工作为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。大脑被描述为一个循环一致的熵梯度解析器，通过路径依赖、内容播种的模拟来对齐结构和特异性。", "translation": "语境-内容不确定性原理（CCUP）提出，不确定性下的推断受语境和内容之间熵不对称性的支配：高熵语境必须通过与低熵、结构化内容的对齐来解释。在本文中，我们开发了一个分层计算框架，从这一基础不对称性中推导出操作原理。在基础层面，CCUP将推断形式化为定向熵最小化，建立了一个有利于内容优先构建的变分梯度。在此基础上，我们识别出四个操作原理的层级：（L1）核心推断约束，包括结构优先于特异性、不对称推断流、循环一致性自举和条件压缩，所有这些都被证明可以相互归约；（L2）资源分配原理，例如精度加权注意力、不对称学习率和基于吸引子的记忆编码；（L3）时间自举动力学，通过结构引导的课程随时间组织学习；以及（L4）空间层级组合，将这些机制整合到记忆、推断和规划的自组织循环中。我们提出了形式等价定理、原理间的依赖格，以及计算模拟，展示了与CCUP对齐的推断的效率增益。这项工作为理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性提供了一个统一的理论基础。大脑不仅仅是一个推断机器。它是一个循环一致的熵梯度解析器，通过路径依赖、内容播种的模拟来对齐结构和特异性。", "summary": "本文提出了语境-内容不确定性原理（CCUP），该原理认为不确定性下的推断源于语境与内容之间的熵不对称性。作者开发了一个分层计算框架，将推断视为定向熵最小化，并在此基础上提出了四个层级的操作原理，涵盖了从核心推断约束到空间层级组合的机制。研究通过形式定理和模拟验证了这些原理，并证明了CCUP对齐推断的效率增益，为理解大脑和机器如何通过结构-特异性对齐来最小化不确定性提供了一个统一的理论。", "keywords": "语境-内容不确定性原理, 熵不对称, 推断, 计算框架, 不确定性最小化", "comments": "本文提出了一个新颖且具有深远意义的“语境-内容不确定性原理”，将不确定性下的推断归结为语境和内容之间的熵不对称性。其创新之处在于构建了一个分层的计算框架，将这一原理具体化为一系列可操作的机制，并强调了“内容优先构建”和“循环一致性”的重要性。该研究不仅为理解认知过程提供了新的理论视角，也可能对开发更高效的人工智能系统具有指导意义。"}}
{"id": "2506.20819", "title": "DPLib: A Standard Benchmark Library for Distributed Power System Analysis and Optimization", "authors": ["Milad Hasanzadeh", "Amin Kargarian"], "summary": "\\textit{DPLib} is an open-source MATLAB-based benchmark library created to\nsupport research and development in distributed and decentralized power system\nanalysis and optimization. Distributed and decentralized methods offer\nscalability, privacy preservation, and resilience to single points of failure,\nmaking them increasingly important for modern power systems. However, unlike\ncentralized tools such as MATPOWER, no general-purpose, reproducible data\nlibrary package currently exists for distributed power system studies. DPLib\nfills this gap by providing a standard power system library featuring over 20\nmulti-region benchmark test cases of varying sizes, along with a graph-based\npartitioning toolkit that decomposes any MATPOWER test system into multiple\nelectrically coherent regions. The partitioning toolkit, an easy-to-use MATLAB\ncode, generates standardized \\texttt{.mat} and \\texttt{.m} files, along with\nregion visualizations for intuitive understanding. We also provide modular,\neasy-to-use distributed optimal power flow (OPF) solvers: an alternating\ndirection method of multipliers(ADMM)-based DC-OPF solver implemented in\nYALMIP, and an ADMM-based AC-OPF solver leveraging IPOPT. These solvers\nvalidate the generated test systems for distributed optimization applications.\nNumerical results validate the generated test cases, establishing DPLib as a\nfoundation for reproducible distributed power system research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20819v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20819v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "DPLib：一个用于分布式电力系统分析和优化的标准基准库", "tldr": "DPLib 是一个开源的 MATLAB 基准库，为分布式电力系统分析和优化提供标准化的、可复现的测试用例和工具。", "motivation": "现代电力系统需要可扩展、保护隐私且具有单点故障恢复能力的分布式和去中心化方法。然而，目前缺乏像 MATPOWER 这样的通用、可复现的分布式电力系统数据库，DPLib 旨在填补这一空白。", "method": "DPLib 提供超过 20 个多区域基准测试用例，一个基于图的分割工具包，可将任何 MATPOWER 系统分解为多个电学上连贯的区域。此外，它还提供模块化、易于使用的分布式最优潮流 (OPF) 求解器，包括一个基于 ADMM 的 DC-OPF 求解器（YALMIP 实现）和一个基于 ADMM 的 AC-OPF 求解器（利用 IPOPT）。", "result": "数值结果验证了生成的测试用例，确立了 DPLib 作为可复现分布式电力系统研究的基础。", "conclusion": "DPLib 通过提供标准化的基准测试用例和工具，填补了分布式电力系统研究中通用、可复现数据库的空白，为未来的研究奠定了基础。", "translation": "DPLib 是一个开源的基于 MATLAB 的基准库，旨在支持分布式和去中心化电力系统分析与优化的研究和开发。分布式和去中心化方法提供了可扩展性、隐私保护以及对单点故障的弹性，这使得它们对现代电力系统越来越重要。然而，与 MATPOWER 等集中式工具不同，目前还没有一个通用、可复现的数据库包用于分布式电力系统研究。DPLib 填补了这一空白，提供了一个标准电力系统库，其中包含 20 多个不同规模的多区域基准测试用例，以及一个基于图的划分工具包，该工具包可以将任何 MATPOWER 测试系统分解为多个电学上连贯的区域。该划分工具包是一个易于使用的 MATLAB 代码，可以生成标准化的 .mat 和 .m 文件，以及区域可视化以便直观理解。我们还提供了模块化、易于使用的分布式最优潮流 (OPF) 求解器：一个基于交替方向乘子法 (ADMM) 的直流最优潮流 (DC-OPF) 求解器（在 YALMIP 中实现），以及一个利用 IPOPT 的基于 ADMM 的交流最优潮流 (AC-OPF) 求解器。这些求解器验证了生成的测试系统适用于分布式优化应用。数值结果验证了生成的测试用例，确立了 DPLib 作为可复现分布式电力系统研究的基础。", "summary": "DPLib 是一个开源的 MATLAB 基准库，旨在支持分布式和去中心化电力系统分析与优化研究。它解决了现有分布式电力系统缺乏通用、可复现数据库的问题，通过提供超过 20 个多区域测试用例、一个图分割工具包以及基于 ADMM 的分布式最优潮流求解器，为分布式电力系统研究提供了标准化的可复现基础。", "keywords": "分布式电力系统, 基准库, 最优潮流, MATLAB, ADMM", "comments": "DPLib 的创新之处在于它首次为分布式电力系统研究提供了一个标准化的、可复现的基准库，填补了现有工具的空白。其提供的多区域测试用例和图分割工具包对于促进分布式算法的开发和验证具有重要意义。此外，集成的分布式 OPF 求解器也验证了其测试系统的实用性。"}}
{"id": "2506.20823", "title": "Compact Analytical Model for Real-Time Evaluation of OAM-Based Inter-Satellite Links", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna"], "summary": "This paper presents an efficient analytical framework for evaluating the\nperformance of inter-satellite communication systems utilizing orbital angular\nmomentum (OAM) beams under pointing errors. An accurate analytical model is\nfirst developed to characterize intermodal crosstalk caused by beam\nmisalignment in OAM-based inter-satellite links. Building upon this model, we\nderive efficient expressions to analyze and optimize system performance in\nterms of bit error rate (BER). Unlike traditional Monte Carlo-based methods\nthat are computationally intensive, the proposed approach offers accurate\nperformance predictions. This enables a substantial decrease in computation\ntime while maintaining high accuracy, thanks to the use of analytical\nexpressions for both crosstalk and BER. This fast and accurate evaluation\ncapability is particularly critical for dynamic low Earth orbit (LEO) satellite\nconstellations, where network topology and channel conditions change rapidly,\nrequiring real-time link adaptation. Furthermore, we systematically design and\nevaluate asymmetric OAM mode sets, which significantly outperform symmetric\nconfigurations in the presence of pointing errors. Our results also reveal key\ninsights into the interaction between beam divergence, tracking accuracy, and\nlink distance, demonstrating that the proposed framework enables real-time\noptimization of system parameters with high fidelity. The analytical findings\nare rigorously validated against extensive Monte Carlo simulations, confirming\ntheir practical applicability for high-mobility optical wireless systems such\nas LEO satellite networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20823v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20823v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "用于实时评估基于OAM的卫星间链路的紧凑分析模型", "tldr": "本文提出了一种紧凑的分析模型，用于在指向误差下实时评估基于OAM的卫星间链路的性能，显著降低了计算时间并提高了精度。", "motivation": "传统的蒙特卡洛方法在评估OAM卫星间链路性能时计算量大，难以满足动态LEO卫星星座中实时链路适应的需求。因此，需要一个高效、准确的分析框架。", "method": "本文首先开发了一个精确的分析模型来表征OAM卫星间链路中由光束未对准引起的模间串扰。在此模型基础上，推导出高效的表达式来分析和优化误码率（BER）方面的系统性能。此外，系统地设计和评估了非对称OAM模式集。", "result": "所提出的方法提供了准确的性能预测，与传统蒙特卡洛方法相比，显著减少了计算时间，同时保持了高精度。非对称OAM模式集在存在指向误差的情况下显著优于对称配置。研究结果揭示了光束发散、跟踪精度和链路距离之间相互作用的关键见解，并证明了该框架能够高保真地实时优化系统参数。分析结果已通过广泛的蒙特卡洛模拟进行了严格验证。", "conclusion": "本文提出的紧凑分析模型能够对基于OAM的卫星间链路进行快速准确的性能评估和实时优化，特别适用于LEO卫星网络等高机动性光无线系统。", "translation": "本文提出了一种高效的分析框架，用于评估在指向误差下利用轨道角动量（OAM）光束的卫星间通信系统性能。首先开发了一个精确的分析模型，以表征基于OAM的卫星间链路中由光束未对准引起的模间串扰。在此模型基础上，我们推导出高效的表达式，以误码率（BER）方面分析和优化系统性能。与计算密集型的传统蒙特卡洛方法不同，所提出的方法提供了准确的性能预测。这使得计算时间大幅减少，同时保持了高精度，这得益于对串扰和BER都使用了分析表达式。这种快速准确的评估能力对于动态低地球轨道（LEO）卫星星座尤其关键，在这些星座中，网络拓扑和信道条件变化迅速，需要实时链路适应。此外，我们系统地设计和评估了非对称OAM模式集，这些模式集在存在指向误差的情况下显著优于对称配置。我们的结果还揭示了光束发散、跟踪精度和链路距离之间相互作用的关键见解，表明所提出的框架能够高保真地实时优化系统参数。分析结果通过广泛的蒙特卡洛模拟进行了严格验证，证实了它们在LEO卫星网络等高机动性光无线系统中的实际适用性。", "summary": "本文提出了一个紧凑的分析模型，用于实时评估在指向误差下基于OAM的卫星间链路的性能。该模型能够准确表征模间串扰并推导出高效的误码率表达式，从而实现快速而精确的性能预测和系统优化，尤其适用于动态LEO卫星星座。研究还发现非对称OAM模式集在存在指向误差时表现更优，并且深入分析了光束发散、跟踪精度和链路距离的相互作用，所有分析结果均通过蒙特卡洛模拟得到验证。", "keywords": "OAM, 卫星间链路, 指向误差, 分析模型, 实时评估", "comments": "这项工作通过提供一个紧凑且高效的分析模型，解决了传统蒙特卡洛模拟在评估OAM卫星间链路性能时计算量大的问题。其创新点在于将模间串扰和误码率的分析表达式结合起来，实现了实时性能评估和参数优化，这对于动态变化的LEO卫星网络至关重要。该模型的实用性在于其能够显著减少计算时间，同时保持高精度，为未来OAM通信系统的设计和部署提供了宝贵的工具。"}}
{"id": "2506.20689", "title": "U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "summary": "Artificial intelligence, including deep learning models, will play a\ntransformative role in automated medical image analysis for the diagnosis of\ncardiac disorders and their management. Automated accurate delineation of\ncardiac images is the first necessary initial step for the quantification and\nautomated diagnosis of cardiac disorders. In this paper, we propose a deep\nlearning based enhanced UNet model, U-R-Veda, which integrates convolution\ntransformations, vision transformer, residual links, channel-attention, and\nspatial attention, together with edge-detection based skip-connections for an\naccurate fully-automated semantic segmentation of cardiac magnetic resonance\n(CMR) images. The model extracts local-features and their interrelationships\nusing a stack of combination convolution blocks, with embedded channel and\nspatial attention in the convolution block, and vision transformers. Deep\nembedding of channel and spatial attention in the convolution block identifies\nimportant features and their spatial localization. The combined edge\ninformation with channel and spatial attention as skip connection reduces\ninformation-loss during convolution transformations. The overall model\nsignificantly improves the semantic segmentation of CMR images necessary for\nimproved medical image analysis. An algorithm for the dual attention module\n(channel and spatial attention) has been presented. Performance results show\nthat U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The\nmodel outperforms the accuracy attained by other models, based on DSC and HD\nmetrics, especially for the delineation of right-ventricle and\nleft-ventricle-myocardium.", "comment": "15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.20689v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20689v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "U-R-VEDA：集成UNET、残差连接、边缘和双重注意力以及视觉Transformer用于CMR的精确语义分割", "tldr": "本文提出U-R-Veda模型，通过结合UNet、残差连接、边缘和双重注意力以及视觉Transformer，显著提高了心脏磁共振图像（CMR）的语义分割精度。", "motivation": "自动化、精确的心脏图像分割是心脏疾病量化和自动化诊断的关键初始步骤。深度学习在医学图像分析中具有变革性作用，因此需要开发更准确的分割模型。", "method": "提出了一种名为U-R-Veda的深度学习增强型UNet模型，该模型集成了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力以及基于边缘检测的跳跃连接，旨在实现心脏磁共振（CMR）图像的精确全自动语义分割。模型通过组合卷积块（嵌入通道和空间注意力）和视觉Transformer提取局部特征及其相互关系。结合边缘信息与通道和空间注意力作为跳跃连接，有效减少了卷积变换过程中的信息损失。论文还介绍了双重注意力模块的算法。", "result": "U-R-Veda模型在DSC指标下实现了平均95.2%的准确率。在DSC和HD指标下，该模型性能优于其他现有模型，特别是在右心室和左心室心肌的勾勒方面表现突出。", "conclusion": "U-R-Veda模型显著提高了心脏磁共振（CMR）图像的语义分割精度，这对于改进医学图像分析以及心脏疾病的自动化诊断和管理至关重要。", "translation": "人工智能，包括深度学习模型，将在心脏疾病诊断和管理所需的自动化医学图像分析中发挥变革性作用。心脏图像的自动化精确勾勒是心脏疾病量化和自动化诊断的第一个必要初始步骤。在本文中，我们提出了一种基于深度学习的增强型UNet模型，U-R-Veda，它集成了卷积变换、视觉Transformer、残差连接、通道注意力、空间注意力，以及基于边缘检测的跳跃连接，用于心脏磁共振（CMR）图像的精确全自动语义分割。该模型通过一系列组合卷积块（在卷积块中嵌入通道和空间注意力）和视觉Transformer来提取局部特征及其相互关系。卷积块中通道和空间注意力的深度嵌入识别重要的特征及其空间定位。结合边缘信息与通道和空间注意力作为跳跃连接，减少了卷积变换过程中的信息损失。整体模型显著提高了CMR图像的语义分割，这对于改进医学图像分析是必要的。文中提出了双重注意力模块（通道和空间注意力）的算法。性能结果显示，U-R-Veda在DSC指标下达到了平均95.2%的准确率。该模型在DSC和HD指标下均优于其他模型所达到的准确性，尤其是在右心室和左心室心肌的勾勒方面。", "summary": "本文提出了一种名为U-R-Veda的深度学习模型，旨在提高心脏磁共振（CMR）图像的语义分割精度。该模型是UNet的增强版，创新性地结合了视觉Transformer、残差连接、双重注意力（通道和空间）以及基于边缘检测的跳跃连接，以有效提取特征并减少信息损失。实验结果表明，U-R-Veda在DSC指标下实现了95.2%的平均准确率，并优于其他模型，特别是在右心室和左心室心肌的分割方面，为心脏疾病的自动化诊断提供了更准确的工具。", "keywords": "语义分割, 心脏磁共振, UNet, 视觉Transformer, 注意力机制", "comments": "这篇论文通过将多种先进的深度学习技术（如UNet、Vision Transformer、残差连接、双重注意力以及边缘检测跳跃连接）创新性地集成到一个统一的框架U-R-Veda中，显著提升了心脏磁共振图像的语义分割精度。其主要创新点在于引入了基于边缘检测的跳跃连接，这有助于减少信息损失，以及深度嵌入通道和空间注意力来增强特征识别。该模型在医学图像分析，特别是心脏疾病的自动化诊断方面具有重要意义。"}}
{"id": "2506.20815", "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications", "authors": ["Xinye Tang", "Haijun Zhai", "Chaitanya Belwal", "Vineeth Thayanithi", "Philip Baumann", "Yogesh K Roy"], "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20815v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20815v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "领域特定AI应用的动态上下文感知提示推荐", "tldr": "本文提出一个动态上下文感知的提示推荐系统，用于领域特定AI应用，以解决LLM提示质量难题，并通过实验验证了其有效性。", "motivation": "LLM驱动的应用高度受用户提示质量的影响，但为领域特定应用创建高质量提示常常具有挑战性。", "method": "该系统结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名，以生成相关且可操作的提示建议。它利用行为遥测和两阶段分层推理过程动态选择和排序相关技能，并使用预定义和自适应模板（通过少样本学习增强）来合成提示。", "result": "在真实世界数据集上的实验表明，该方法实现了高实用性和相关性，并通过自动化和专家评估验证。", "conclusion": "该系统能够有效提升领域特定AI应用中提示的质量和相关性。", "translation": "LLM驱动的应用高度受用户提示质量的影响，而创建高质量提示常常具有挑战性，特别是对于领域特定应用。本文提出了一种新颖的、针对领域特定AI应用的动态上下文感知提示推荐系统。我们的解决方案结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名，以生成相关且可操作的提示建议。该系统利用行为遥测和两阶段分层推理过程来动态选择和排序相关技能，并使用预定义和自适应模板（通过少样本学习增强）合成提示。在真实世界数据集上的实验表明，我们的方法实现了高实用性和相关性，并通过自动化和专家评估验证。", "summary": "本文针对LLM驱动的领域特定应用中高质量提示难以创建的问题，提出了一种动态上下文感知提示推荐系统。该系统整合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名，并通过行为遥测和两阶段分层推理动态选择和合成提示。实验证明，该方法在实用性和相关性方面表现出色。", "keywords": "提示推荐, 领域特定AI, 上下文感知, LLM应用, 检索增强", "comments": "该研究通过结合多种先进技术（如检索增强、分层组织和少样本学习），为领域特定AI应用提供了实用的提示工程解决方案，有望显著降低用户创建高质量提示的门槛，提升LLM应用的可用性。其创新点在于动态上下文感知和多阶段推理机制。"}}
{"id": "2506.20901", "title": "Data Visualization for Improving Financial Literacy: A Systematic Review", "authors": ["Meng Du", "Robert Amor", "Kwan-Liu Ma", "Burkhard C. Wünsche"], "summary": "Financial literacy empowers individuals to make informed and effective\nfinancial decisions, improving their overall financial well-being and security.\nHowever, for many people understanding financial concepts can be daunting and\nonly half of US adults are considered financially literate. Data visualization\nsimplifies these concepts, making them accessible and engaging for learners of\nall ages. This systematic review analyzes 37 research papers exploring the use\nof data visualization and visual analytics in financial education and literacy\nenhancement. We classify these studies into five key areas: (1) the evolution\nof visualization use across time and space, (2) motivations for using\nvisualization tools, (3) the financial topics addressed and instructional\napproaches used, (4) the types of tools and technologies applied, and (5) how\nthe effectiveness of teaching interventions was evaluated. Furthermore, we\nidentify research gaps and highlight opportunities for advancing financial\nliteracy. Our findings offer practical insights for educators and professionals\nto effectively utilize or design visual tools for financial literacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20901v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20901v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "数据可视化在提升金融素养中的应用：一项系统综述", "tldr": "本系统综述分析了37篇关于数据可视化如何提升金融素养的论文，并将其分为五个关键领域，同时识别了研究空白，为教育者和专业人士提供了实用见解。", "motivation": "金融素养对个人财务福祉至关重要，但许多人觉得金融概念难以理解，且美国仅一半成年人具备金融素养。数据可视化能简化这些概念，使其更易于学习和理解。", "method": "本研究对37篇探讨数据可视化和视觉分析在金融教育及素养提升中应用的论文进行了系统综述。研究将这些论文分为五个关键领域进行分析。", "result": "研究将文献分为五个关键领域：(1) 可视化应用的时空演变，(2) 使用可视化工具的动机，(3) 所涉及的金融主题和教学方法，(4) 应用的工具和技术类型，以及 (5) 教学干预有效性的评估方式。此外，研究还指出了研究空白并强调了提升金融素养的机会。", "conclusion": "本研究的结果为教育者和专业人士有效利用或设计可视化工具以提升金融素养提供了实用的见解。", "translation": "金融素养使个人能够做出明智有效的财务决策，从而改善其整体财务福祉和安全。然而，对许多人来说，理解金融概念可能令人望而生畏，而且只有一半的美国成年人被认为是具备金融素养的。数据可视化简化了这些概念，使其对所有年龄段的学习者都易于理解和参与。本系统综述分析了37篇研究论文，探讨了数据可视化和视觉分析在金融教育和素养提升中的应用。我们将这些研究分为五个关键领域：(1) 可视化应用的时空演变，(2) 使用可视化工具的动机，(3) 所涉及的金融主题和教学方法，(4) 应用的工具和技术类型，以及 (5) 教学干预有效性的评估方式。此外，我们还识别了研究空白并强调了提升金融素养的机会。我们的发现为教育者和专业人士有效利用或设计可视化工具以提升金融素养提供了实用见解。", "summary": "本系统综述旨在探讨数据可视化在提升金融素养方面的应用。研究分析了37篇相关文献，并将其分为可视化工具的时空演变、使用动机、涉及的金融主题与教学方法、工具类型以及教学效果评估等五个关键方面。研究不仅总结了现有实践，还指出了未来的研究空白和发展机遇，为教育者和专业人士提供了利用可视化工具提升金融素养的实用指导。", "keywords": "数据可视化, 金融素养, 系统综述, 金融教育, 视觉分析", "comments": "该论文通过系统综述的方式，全面梳理了数据可视化在金融素养教育领域的应用现状，其创新性在于对现有研究进行了细致的分类和归纳，并明确指出了研究空白。这对于指导未来的研究方向和实践具有重要意义，有助于推动金融教育领域的可视化工具设计和应用。"}}
{"id": "2506.21324", "title": "Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning", "authors": ["Jiechen Chen", "Bipin Rajendran", "Osvaldo Simeone"], "summary": "Neuromorphic and quantum computing have recently emerged as promising\nparadigms for advancing artificial intelligence, each offering complementary\nstrengths. Neuromorphic systems built on spiking neurons excel at processing\ntime-series data efficiently through sparse, event-driven computation,\nconsuming energy only upon input events. Quantum computing, on the other hand,\nleverages superposition and entanglement to explore feature spaces that are\nexponentially large in the number of qubits. Hybrid approaches combining these\nparadigms have begun to show potential, but existing quantum spiking models\nhave important limitations. Notably, prior quantum spiking neuron\nimplementations rely on classical memory mechanisms on single qubits, requiring\nrepeated measurements to estimate firing probabilities, and they use\nconventional backpropagation on classical simulators for training. Here we\npropose a stochastic quantum spiking (SQS) neuron model that addresses these\nchallenges. The SQS neuron uses multi-qubit quantum circuits to realize a\nspiking unit with internal quantum memory, enabling event-driven probabilistic\nspike generation in a single shot. Furthermore, we outline how networks of SQS\nneurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a\nhardware-friendly local learning rule, eliminating the need for global\nclassical backpropagation. The proposed SQSNN model fuses the time-series\nefficiency of neuromorphic computing with the exponentially large inner state\nspace of quantum computing, paving the way for quantum spiking neural networks\nthat are modular, scalable, and trainable on quantum hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21324v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21324v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "具有量子记忆和局部学习的随机量子脉冲神经网络", "tldr": "该论文提出了一种新的随机量子脉冲（SQS）神经元模型，该模型使用多量子位电路实现内部量子记忆，并引入硬件友好的局部学习规则来训练网络，从而克服了现有量子脉冲模型的局限性。", "motivation": "现有量子脉冲神经元实现依赖于单量子位上的经典记忆机制，需要重复测量来估计放电概率，并且其训练使用经典模拟器上的传统反向传播，这些都限制了其效率和可扩展性。", "method": "本文提出了一种随机量子脉冲（SQS）神经元模型。该模型使用多量子位量子电路实现具有内部量子记忆的脉冲单元，能够在单次操作中实现事件驱动的概率性脉冲生成。此外，该论文概述了如何通过硬件友好的局部学习规则来训练SQS神经元网络（SQSNNs），从而消除了对全局经典反向传播的需求。", "result": "所提出的SQSNN模型成功融合了神经形态计算的时间序列处理效率与量子计算的指数级大内部状态空间。这为开发模块化、可扩展且可在量子硬件上直接训练的量子脉冲神经网络铺平了道路。", "conclusion": "SQSNN模型克服了现有量子脉冲模型的局限性，通过结合量子记忆和局部学习，实现了在量子硬件上可训练的、模块化和可扩展的量子脉冲神经网络。", "translation": "神经形态计算和量子计算最近已成为推进人工智能的有前景的范例，各自提供互补的优势。基于脉冲神经元的神经形态系统通过稀疏、事件驱动的计算，在高效处理时间序列数据方面表现出色，仅在输入事件发生时才消耗能量。另一方面，量子计算利用叠加和纠缠来探索量子比特数量呈指数级增长的特征空间。结合这些范例的混合方法已开始显示出潜力，但现有的量子脉冲模型存在重要的局限性。值得注意的是，先前的量子脉冲神经元实现依赖于单量子位上的经典记忆机制，需要重复测量来估计放电概率，并且它们使用经典模拟器上的传统反向传播进行训练。在此，我们提出了一种随机量子脉冲（SQS）神经元模型来解决这些挑战。SQS神经元使用多量子位量子电路来实现具有内部量子记忆的脉冲单元，从而在单次操作中实现事件驱动的概率性脉冲生成。此外，我们概述了如何通过硬件友好的局部学习规则训练SQS神经元网络——称为SQS神经网络（SQSNNs），从而消除了对全局经典反向传播的需求。所提出的SQSNN模型融合了神经形态计算的时间序列效率与量子计算的指数级大内部状态空间，为在量子硬件上模块化、可扩展且可训练的量子脉冲神经网络铺平了道路。", "summary": "这篇论文提出了一种新的随机量子脉冲（SQS）神经元模型及其网络（SQSNNs），旨在克服现有量子脉冲神经网络在内存机制和训练方法上的局限性。SQS神经元利用多量子位量子电路实现具有内部量子记忆的脉冲单元，能够单次实现事件驱动的概率性脉冲生成。SQSNNs则通过硬件友好的局部学习规则进行训练，无需传统的全局反向传播。该方法有效地结合了神经形态计算的时间序列处理能力和量子计算的巨大状态空间，为构建可在量子硬件上训练的、模块化且可扩展的量子脉冲神经网络奠定了基础。", "keywords": "随机量子脉冲神经网络, 量子记忆, 局部学习, 神经形态计算, 量子计算", "comments": "这篇论文的创新点在于提出了具有内部量子记忆的随机量子脉冲神经元，并通过硬件友好的局部学习规则解决了现有量子脉冲模型在内存机制和训练方法上的局限性。它有效地融合了神经形态和量子计算的优势，为未来在量子硬件上实现高效且可扩展的AI系统提供了新的途径，具有重要的理论和实践意义。"}}
{"id": "2506.20761", "title": "A Framework for Building Data Structures from Communication Protocols", "authors": ["Alexandr Andoni", "Shunhua Jiang", "Omri Weinstein"], "summary": "We present a general framework for designing efficient data structures for\nhigh-dimensional pattern-matching problems ($\\exists \\;? i\\in[n], f(x_i,y)=1$)\nthrough communication models in which $f(x,y)$ admits sublinear communication\nprotocols with exponentially-small error. Specifically, we reduce the data\nstructure problem to the Unambiguous Arthur-Merlin (UAM) communication\ncomplexity of $f(x,y)$ under product distributions.\n  We apply our framework to the Partial Match problem (a.k.a, matching with\nwildcards), whose underlying communication problem is sparse set-disjointness.\nWhen the database consists of $n$ points in dimension $d$, and the number of\n$\\star$'s in the query is at most $w = c\\log n \\;(\\ll d)$, the fastest known\nlinear-space data structure (Cole, Gottlieb and Lewenstein, STOC'04) had query\ntime $t \\approx 2^w = n^c$, which is nontrivial only when $c<1$. By contrast,\nour framework produces a data structure with query time $n^{1-1/(c \\log^2 c)}$\nand space close to linear.\n  To achieve this, we develop a one-sided $\\epsilon$-error communication\nprotocol for Set-Disjointness under product distributions with\n$\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$ complexity, improving on the\nclassical result of Babai, Frankl and Simon (FOCS'86). Building on this\nprotocol, we show that the Unambiguous AM communication complexity of\n$w$-Sparse Set-Disjointness with $\\epsilon$-error under product distributions\nis $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$, independent of the ambient dimension\n$d$, which is crucial for the Partial Match result. Our framework sheds further\nlight on the power of data-dependent data structures, which is instrumental for\nreducing to the (much easier) case of product distributions.", "comment": "53 pages, STOC 2025", "pdf_url": "http://arxiv.org/pdf/2506.20761v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20761v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "从通信协议构建数据结构的框架", "tldr": "本文提出了一个通用框架，通过通信模型为高维模式匹配问题设计高效数据结构，并将其应用于部分匹配问题，显著提高了查询时间。", "motivation": "为高维模式匹配问题设计高效数据结构，特别是通过利用通信模型中具有次线性通信协议和指数小误差的函数。", "method": "将数据结构问题归结为乘积分布下函数f(x,y)的无歧义Arthur-Merlin (UAM)通信复杂度。具体地，开发了一种在乘积分布下具有 $\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$ 复杂度的单边$\\epsilon$-误差集合不交性通信协议，并在此基础上证明了乘积分布下w-稀疏集合不交性的无歧义AM通信复杂度为 $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$。", "result": "应用于部分匹配问题时，对于查询中星号数量 w = c log n 的情况，本文框架生成的数据结构查询时间为 $n^{1-1/(c \\log^2 c)}$，而现有最快线性空间数据结构（Cole, Gottlieb and Lewenstein, STOC'04）的查询时间约为 $2^w = n^c$。此外，改进了Babai, Frankl and Simon (FOCS'86)关于集合不交性通信协议的经典结果。", "conclusion": "该框架为从通信协议构建数据结构提供了一种强大的方法，特别是在数据依赖型数据结构方面，这对于简化为乘积分布的情况至关重要。", "translation": "我们提出了一个通用框架，用于通过通信模型设计高维模式匹配问题（$\\exists \\;? i\\in[n], f(x_i,y)=1$）的高效数据结构，其中 $f(x,y)$ 允许具有次线性通信协议和指数小误差。具体来说，我们将数据结构问题归结为乘积分布下 $f(x,y)$ 的无歧义Arthur-Merlin (UAM)通信复杂度。\n我们将我们的框架应用于部分匹配问题（又称带通配符匹配），其底层通信问题是稀疏集合不交性。当数据库包含 $d$ 维中的 $n$ 个点，并且查询中的星号数量最多为 $w = c\\log n \\;(\\ll d)$ 时，已知最快的线性空间数据结构（Cole, Gottlieb 和 Lewenstein, STOC'04）的查询时间为 $t \\approx 2^w = n^c$，这仅在 $c<1$ 时才具有非平凡性。相比之下，我们的框架生成的数据结构查询时间为 $n^{1-1/(c \\log^2 c)}$，空间接近线性。\n为了实现这一点，我们开发了一种在乘积分布下具有 $\\tilde{\\Theta}(\\sqrt{d\\log(1/\\epsilon)})$ 复杂度的单边 $\\epsilon$-误差集合不交性通信协议，改进了 Babai、Frankl 和 Simon (FOCS'86) 的经典结果。在此协议的基础上，我们表明，在乘积分布下，具有 $\\epsilon$-误差的 $w$-稀疏集合不交性的无歧义 AM 通信复杂度为 $\\tilde{O}(\\sqrt{w \\log(1/\\epsilon)})$，独立于环境维度 $d$，这对于部分匹配结果至关重要。我们的框架进一步阐明了数据依赖型数据结构的强大功能，这对于简化为（更容易的）乘积分布情况至关重要。", "summary": "本文提出了一个通用的框架，旨在通过通信模型为高维模式匹配问题构建高效的数据结构。该框架将数据结构问题简化为乘积分布下的无歧义Arthur-Merlin (UAM)通信复杂度。通过开发一种改进的单边集合不交性通信协议，该框架在应用于部分匹配问题时，显著优化了查询时间，例如将查询时间从 $n^c$ 降低到 $n^{1-1/(c \\log^2 c)}$。研究强调了数据依赖型数据结构在处理乘积分布问题时的重要性。", "keywords": "数据结构, 通信协议, 模式匹配, 集合不交性, 部分匹配", "comments": "本文提出了一个新颖且通用的框架，将数据结构设计与通信协议联系起来，为高维模式匹配问题提供了理论上更优的解决方案。其创新之处在于将问题归结为UAM通信复杂度，并通过对集合不交性协议的改进，实现了查询时间的显著提升。这种方法为理解数据结构和通信复杂度的深层联系提供了新的视角，并展示了数据依赖型数据结构在实际应用中的强大潜力。"}}
{"id": "2506.21174", "title": "Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4", "authors": ["Jongyeon Park", "Joonhee Lee", "Do-Hyeon Lim", "Hong Kook Kim", "Hyeongcheol Geum", "Jeong Eun Lim"], "summary": "This technical report presents submission systems for Task 4 of the DCASE\n2025 Challenge. This model incorporates additional audio features (spectral\nroll-off and chroma features) into the embedding feature extracted from the\nmel-spectral feature to im-prove the classification capabilities of an\naudio-tagging model in the spatial semantic segmentation of sound scenes (S5)\nsystem. This approach is motivated by the fact that mixed audio often contains\nsubtle cues that are difficult to capture with mel-spectrograms alone. Thus,\nthese additional features offer alterna-tive perspectives for the model.\nSecond, an agent-based label correction system is applied to the outputs\nprocessed by the S5 system. This system reduces false positives, improving the\nfinal class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.\nFinally, we refine the training dataset to enhance the classi-fication accuracy\nof low-performing classes by removing irrele-vant samples and incorporating\nexternal data. That is, audio mix-tures are generated from a limited number of\ndata points; thus, even a small number of out-of-class data points could\ndegrade model performance. The experiments demonstrate that the submit-ted\nsystems employing these approaches relatively improve CA-SDRi by up to 14.7%\ncompared to the baseline of DCASE 2025 Challenge Task 4.", "comment": "DCASE 2025 challenge Task4, 5 pages", "pdf_url": "http://arxiv.org/pdf/2506.21174v1", "categories": ["eess.AS", "cs.LG"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21174v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DCASE 2025挑战任务4中通过丰富音频特征和基于代理的错误校正改进空间语义分割性能", "tldr": "本文为DCASE 2025挑战任务4提交系统，通过整合额外音频特征、基于代理的标签校正和数据集优化，显著提升了空间语义分割的性能，CA-SDRi指标最高提升14.7%。", "motivation": "混合音频中包含细微线索，单独使用梅尔频谱图难以捕捉，因此需要额外的特征提供替代视角。此外，有限的数据点生成的音频混合物中，即使少量无关样本也可能降低模型性能。", "method": "1. 将额外的音频特征（谱滚降和色度特征）整合到梅尔频谱特征中，以增强音频标注模型的分类能力。 2. 应用基于代理的标签校正系统处理S5系统输出，减少误报。 3. 通过移除不相关样本和整合外部数据来优化训练数据集，以提高低性能类别的分类精度。", "result": "实验表明，所提交的系统与DCASE 2025挑战任务4的基线相比，CA-SDRi相对提升高达14.7%。", "conclusion": "通过结合丰富的音频特征、代理校正系统和数据集优化，本研究显著提升了空间语义分割的性能，并在DCASE 2025挑战任务4中取得了优异表现。", "translation": "本技术报告介绍了DCASE 2025挑战任务4的提交系统。该模型将额外的音频特征（谱滚降和色度特征）整合到从梅尔频谱特征中提取的嵌入特征中，以提高声景空间语义分割（S5）系统中音频标注模型的分类能力。这种方法的原因是混合音频通常包含难以仅通过梅尔频谱图捕获的细微线索。因此，这些额外特征为模型提供了替代视角。其次，将基于代理的标签校正系统应用于S5系统处理的输出。该系统减少了误报，改善了最终的类别感知信号失真比改善（CA-SDRi）指标。最后，我们通过移除不相关样本和整合外部数据来优化训练数据集，以提高低性能类别的分类精度。也就是说，音频混合物是从有限数量的数据点生成的；因此，即使少量类别外数据点也可能降低模型性能。实验表明，采用这些方法的提交系统与DCASE 2025挑战任务4的基线相比，CA-SDRi相对提升高达14.7%。", "summary": "本文针对DCASE 2025挑战任务4，提出了一种改进空间语义分割性能的系统。该系统通过整合额外的音频特征（如谱滚降和色度特征）来增强音频标注模型的分类能力，以捕获梅尔频谱图难以捕捉的细微音频线索。此外，引入了一个基于代理的标签校正系统来减少误报，并优化了训练数据集，通过移除不相关样本和加入外部数据来提高低性能类别的分类精度。实验结果显示，与DCASE 2025挑战任务4的基线相比，所提交的系统在CA-SDRi指标上实现了高达14.7%的相对提升。", "keywords": "空间语义分割, 音频特征, 错误校正, DCASE 2025, 声景", "comments": "该论文通过多方面策略（特征丰富、代理校正、数据集优化）来解决空间语义分割中的挑战，特别是针对细微音频线索的捕捉和数据质量问题。这种综合方法是其创新之处，并在DCASE 2025挑战中取得了显著性能提升，显示了其在实际应用中的潜力。"}}
{"id": "2506.20854", "title": "Towards Two-Stage Counterfactual Learning to Rank", "authors": ["Shashank Gupta", "Yiming Liao", "Maarten de Rijke"], "summary": "Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.", "comment": "Accepted at ICTIR 2025 (co-located with SIGIR 2025)", "pdf_url": "http://arxiv.org/pdf/2506.20854v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20854v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "迈向两阶段反事实排序学习", "tldr": "本文提出了一种新颖的两阶段反事实排序学习（CLTR）方法，用于联合训练候选生成器和排序器，解决了单阶段CLTR在大规模应用中的局限性以及现有两阶段CLTR方法的不足。", "motivation": "现有反事实排序学习（CLTR）方法为单阶段，对于拥有数百万文档的真实世界应用而言不切实际。尽管实际排序系统采用两阶段设计，但现有两阶段CLTR方法存在局限性（例如，仅考虑Top-1排序，排序器固定），并且缺少一种能够联合训练排序器和候选生成器的方法。", "method": "本文提出了一种两阶段CLTR估计器，该估计器考虑了两个阶段之间的交互作用，并离线估计了两种策略的联合价值。此外，还提出了一种新颖的联合优化方法，分别训练候选生成器和排序器策略。", "result": "在半合成基准上的实验结果表明，所提出的联合CLTR方法优于现有基线。", "conclusion": "本文首次提出了针对两阶段排序系统的CLTR估计器和学习方法，通过联合优化候选生成器和排序器，有效解决了单阶段CLTR和现有两阶段CLTR方法的局限性。", "translation": "反事实排序学习（CLTR）旨在从用户交互中学习排序策略，同时纠正交互数据中固有的偏差，例如位置偏差。现有的CLTR方法假设单一的排序策略从整个文档候选集中选择Top-K排序。在实际应用中，候选文档集数量级达到数百万，使得单一阶段的排序策略不切实际。为了扩展到数百万文档，实际排序系统被设计为两阶段模式，即先有候选生成器，然后是排序器。现有针对两阶段离线排序系统的CLTR方法仅考虑Top-1排序设置，并且只专注于训练候选生成器，而排序器是固定的。现有文献中缺少一种能够联合训练排序器和候选生成器的CLTR方法。在本文中，我们提出了一种两阶段CLTR估计器，它考虑了两个阶段之间的交互作用，并离线估计了两种策略的联合价值。此外，我们提出了一种新颖的联合优化方法，分别训练候选生成器和排序器策略。据我们所知，我们是第一个提出两阶段排序的CLTR估计器和学习方法。在半合成基准上的实验结果表明，所提出的联合CLTR方法优于现有基线。", "summary": "本文提出了一种新颖的两阶段反事实排序学习（CLTR）方法，旨在解决现有单阶段CLTR在大规模真实世界应用中的不切实际性，以及现有两阶段CLTR方法无法联合训练排序器和候选生成器的问题。该方法包括一个两阶段CLTR估计器，用于评估联合策略价值，以及一种联合优化方法来训练两个阶段的策略。实验结果表明，其在半合成基准上优于基线方法。", "keywords": "反事实排序学习, 两阶段排序, 联合优化, 候选生成, 排序器", "comments": "该论文解决了将CLTR扩展到实际大规模排序系统的重要实践问题。其主要创新在于首次提出了针对两阶段排序的CLTR估计器和联合学习方法，这对于实际部署至关重要。虽然在半合成基准上进行了验证，但未来在真实世界数据集上的进一步验证将增强其主张。"}}
{"id": "2506.20890", "title": "Multicontinuum Homogenization for Poroelasticity Model", "authors": ["Dmitry Ammosov", "Mohammed Al-Kobaisi", "Yalchin Efendiev"], "summary": "In this paper, we derive multicontinuum poroelasticity models using the\nmulticontinuum homogenization method. Poroelasticity models are widely used in\nmany areas of science and engineering to describe coupled flow and mechanics\nprocesses in porous media. However, in many applications, the properties of\nporoelastic media possess high contrast, presenting serious computational\nchallenges. It is well known that standard homogenization approaches often fail\nto give an accurate solution due to the lack of macroscopic parameters.\nMulticontinuum approaches allow us to consider such cases by defining several\naverage states known as continua. In the field of poroelasticity,\nmultiple-network models arising from the multiple porous media theory are\nrepresentatives of these approaches. In this work, we extend previous findings\nby deriving the generalized multicontinuum poroelasticity model. We apply the\nrecently developed multicontinuum homogenization method and provide a rigorous\nderivation of multicontinuum equations. For this purpose, we formulate coupled\nconstraint cell problems in oversampled regions to consider different\nhomogenized effects. Then, we obtain a multicontinuum expansion of the\nfine-scale fields and derive the multicontinuum model supposing the smoothness\nof macroscopic variables. We present the most general version of equations and\nthe simplified ones based on our numerical experiments. Numerical results are\npresented for different heterogeneous media cases and demonstrate the high\naccuracy of our proposed multicontinuum models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20890v1", "categories": ["math.NA", "cs.CE", "cs.NA", "physics.comp-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20890v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "多连续体均匀化用于多孔弹性模型", "tldr": "本文利用多连续体均匀化方法，推导了广义多连续体多孔弹性模型，解决了标准均匀化方法在处理高对比度多孔介质时精度不足的问题，并通过数值实验验证了模型的准确性。", "motivation": "多孔弹性介质的性质常具有高对比度，导致严重的计算挑战；标准均匀化方法因缺乏宏观参数而无法提供准确解；需要能够通过定义多个平均状态（连续体）来处理此类情况的方法。", "method": "通过推导广义多连续体多孔弹性模型来扩展先前的发现；应用最近开发的多连续体均匀化方法，并严格推导多连续体方程；在过采样区域建立耦合约束单元问题；获得精细尺度场的多连续体展开；假设宏观变量光滑，推导多连续体模型；根据数值实验提出最通用和简化的方程版本。", "result": "成功推导了广义多连续体多孔弹性模型；通过对不同非均质介质情况的数值结果，证明了所提出的多连续体模型具有高精度。", "conclusion": "本文成功利用多连续体均匀化方法推导并验证了广义多连续体多孔弹性模型，为标准方法失效的高对比度多孔介质提供了准确的解决方案。", "translation": "在本文中，我们使用多连续体均匀化方法推导了多连续体多孔弹性模型。多孔弹性模型广泛应用于科学和工程的许多领域，以描述多孔介质中耦合的流动和力学过程。然而，在许多应用中，多孔弹性介质的性质具有高对比度，这带来了严重的计算挑战。众所周知，由于缺乏宏观参数，标准均匀化方法通常无法给出准确的解决方案。多连续体方法通过定义几个被称为连续体的平均状态，使我们能够考虑这种情况。在多孔弹性领域，源自多孔介质理论的多网络模型是这些方法的代表。在这项工作中，我们通过推导广义多连续体多孔弹性模型来扩展先前的发现。我们应用了最近开发的多连续体均匀化方法，并提供了多连续体方程的严格推导。为此，我们在过采样区域中建立了耦合约束单元问题，以考虑不同的均匀化效应。然后，我们获得了精细尺度场的多连续体展开，并在假设宏观变量光滑的情况下推导了多连续体模型。我们根据数值实验提出了最通用的方程版本和简化版本。数值结果针对不同的非均质介质情况进行了展示，并证明了我们提出的多连续体模型的高精度。", "summary": "本文利用多连续体均匀化方法，推导了广义多连续体多孔弹性模型。针对多孔介质性质高对比度导致标准均匀化方法失效的计算难题，该研究通过定义多个平均状态（连续体）来解决。文章严格推导了多连续体方程，通过在过采样区域建立耦合约束单元问题，并获得精细尺度场的多连续体展开。数值实验结果验证了所提出多连续体模型在处理不同非均质介质情况下的高精度。", "keywords": "多连续体均匀化, 多孔弹性, 多孔介质, 高对比度, 数值模拟", "comments": "该论文的创新点在于将多连续体均匀化方法应用于多孔弹性模型的推导，解决了传统均匀化方法在处理高对比度多孔介质时遇到的精度问题。其重要性体现在为复杂多孔介质中的流固耦合问题提供了更准确的建模工具，对于科学和工程领域的实际应用具有潜在价值。通过严格的数学推导和数值验证，该研究为多连续体方法在多孔弹性领域的应用奠定了基础。"}}
{"id": "2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "comment": "main paper is 14 pages", "pdf_url": "http://arxiv.org/pdf/2506.20803v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20803v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "构思-执行鸿沟：大型语言模型生成与人类研究想法的执行结果", "tldr": "尽管大型语言模型生成的想法在构思阶段被认为更具新颖性，但在经过专家执行后，其研究成果的评分显著低于人类生成的想法，揭示了“构思-执行鸿沟”。", "motivation": "现有研究发现大型语言模型（LLM）生成的科研想法在构思阶段被判断为比人类专家的想法更具新颖性。然而，一个好的想法不仅应该看起来新颖，它还应该在执行后产生更好的研究成果。因此，本研究的动机是测试AI生成的想法是否能在执行后带来更好的研究成果。", "method": "研究招募了43位专家研究人员，随机分配执行由专家撰写或LLM生成的想法。每位专家投入超过100小时实现想法，并撰写了一篇4页的短论文记录实验。所有执行的项目随后由专家NLP研究人员进行盲审。通过比较想法在执行前后的评分，来评估LLM和人类想法的表现。", "result": "比较想法在执行前后的评分，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性和总体；p < 0.05）上的得分下降幅度显著大于专家撰写的想法，弥合了构思阶段观察到的LLM与人类想法之间的差距。在比较执行研究的汇总评审分数时，甚至观察到在许多指标上排名发生了逆转，人类想法得分高于LLM想法。", "conclusion": "构思-执行鸿沟突出显示了当前LLM在生成真正有效的研究想法方面的局限性，以及在缺乏执行结果的情况下评估研究想法的挑战。", "translation": "大型语言模型（LLMs）在加速科学研究流程方面展现出潜力。这一过程的一个关键能力是生成新颖研究想法的能力，并且先前的研究发现，在某些情况下，LLM生成的想法被评判为比人类专家的想法更具新颖性。然而，一个好的想法不应仅仅看起来新颖，它还应该在执行后产生更好的研究。为了测试AI生成的想法是否能带来更好的研究成果，我们进行了一项执行研究，招募了43位专家研究人员来执行随机分配的想法，这些想法要么由专家撰写，要么由LLM生成。每位专家花费超过100小时来实现想法，并撰写了一篇4页的短论文来记录实验。所有执行的项目随后由专家NLP研究人员进行盲审。比较想法在执行前后的评分，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性和总体；p < 0.05）上的得分下降幅度显著大于专家撰写的想法，弥合了构思阶段观察到的LLM与人类想法之间的差距。在比较执行研究的汇总评审分数时，我们甚至观察到在许多指标上排名发生了逆转，人类想法得分高于LLM想法。这种构思-执行鸿沟突出显示了当前LLM在生成真正有效的研究想法方面的局限性，以及在缺乏执行结果的情况下评估研究想法的挑战。", "summary": "本文通过一项由43位专家研究人员参与的执行研究，比较了大型语言模型（LLM）生成的研究想法与人类专家想法的实际效果。研究发现，尽管LLM想法在构思阶段表现出新颖性，但在经过实际执行后，其研究成果的评分在所有关键指标上均显著低于人类想法，甚至出现排名反转。这揭示了LLM在研究想法生成中存在的“构思-执行鸿沟”，强调了当前LLM的局限性以及在缺乏执行结果时评估研究想法的挑战。", "keywords": "大型语言模型, 研究想法, 构思-执行鸿沟, 科学研究, 评估", "comments": "这项研究通过实际执行而非仅停留在概念评估，提供了一个新颖且重要的视角来评估LLM生成研究想法的质量。它揭示了LLM在生成“可行且有效”想法方面的局限性，对LLM在科学研究中的应用提出了警示，并强调了研究想法评估中“执行”的重要性，具有很高的实践指导意义。"}}
{"id": "2506.21167", "title": "A Hierarchical Deep Learning Approach for Minority Instrument Detection", "authors": ["Dylan Sechet", "Francesca Bugiotti", "Matthieu Kowalski", "Edouard d'Hérouville", "Filip Langiewicz"], "summary": "Identifying instrument activities within audio excerpts is vital in music\ninformation retrieval, with significant implications for music cataloging and\ndiscovery. Prior deep learning endeavors in musical instrument recognition have\npredominantly emphasized instrument classes with ample data availability.\nRecent studies have demonstrated the applicability of hierarchical\nclassification in detecting instrument activities in orchestral music, even\nwith limited fine-grained annotations at the instrument level. Based on the\nHornbostel-Sachs classification, such a hierarchical classification system is\nevaluated using the MedleyDB dataset, renowned for its diversity and richness\nconcerning various instruments and music genres. This work presents various\nstrategies to integrate hierarchical structures into models and tests a new\nclass of models for hierarchical music prediction. This study showcases more\nreliable coarse-level instrument detection by bridging the gap between detailed\ninstrument identification and group-level recognition, paving the way for\nfurther advancements in this domain.", "comment": "International Conference on Digital Audio Effects (DAFx)", "pdf_url": "http://arxiv.org/pdf/2506.21167v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21167v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种用于少数乐器检测的层次深度学习方法", "tldr": "本文提出了一种基于层次深度学习的方法，用于解决音乐信息检索中少数乐器检测的挑战，通过结合Hornbostel-Sachs分类法在MedleyDB数据集上实现了更可靠的粗粒度乐器检测。", "motivation": "在音乐信息检索中，识别音频片段中的乐器活动对于音乐编目和发现至关重要。然而，以往的深度学习方法主要侧重于数据量充足的乐器类别，而对数据有限的少数乐器检测不足。", "method": "本研究基于Hornbostel-Sachs分类法，评估了一种层次分类系统，并使用MedleyDB数据集进行验证。文章提出了多种策略将层次结构整合到模型中，并测试了一类新的层次音乐预测模型。", "result": "本研究通过弥合详细乐器识别和组级别识别之间的差距，展示了更可靠的粗粒度乐器检测。", "conclusion": "该研究通过引入层次深度学习方法，有效地提升了对少数乐器的检测能力，为该领域的进一步发展奠定了基础。", "translation": "在音乐信息检索中，识别音频片段中的乐器活动至关重要，对音乐编目和发现具有重要意义。以往在乐器识别方面的深度学习研究主要集中于数据量充足的乐器类别。最近的研究表明，即使在乐器层面缺乏细粒度标注的情况下，层次分类在管弦乐中检测乐器活动也具有适用性。基于Hornbostel-Sachs分类法，本研究使用MedleyDB数据集评估了这样一个层次分类系统，该数据集以其在各种乐器和音乐流派方面的多样性和丰富性而闻名。这项工作提出了将层次结构整合到模型中的各种策略，并测试了一类新的层次音乐预测模型。这项研究通过弥合详细乐器识别和组级别识别之间的差距，展示了更可靠的粗粒度乐器检测，为该领域的进一步发展铺平了道路。", "summary": "本研究提出了一种分层深度学习方法，旨在解决音乐信息检索中对少数乐器进行检测的挑战。研究基于Hornbostel-Sachs分类法，利用MedleyDB数据集评估了一个分层分类系统，并探索了多种将分层结构整合到模型中的策略。结果表明，该方法能够实现更可靠的粗粒度乐器检测，有效连接了详细乐器识别和组级别识别，为未来该领域的发展铺平了道路。", "keywords": "层次深度学习, 少数乐器检测, 音乐信息检索, Hornbostel-Sachs分类, MedleyDB", "comments": "这项工作在解决传统深度学习方法在处理数据稀疏的少数乐器类别时面临的挑战方面具有创新性。通过引入层次结构，它能够更有效地利用有限的细粒度标注，提升了检测的鲁棒性，尤其是在粗粒度识别层面。这对于音乐信息检索领域具有重要意义，因为它有助于更全面和准确地构建音乐数据库。"}}
{"id": "2506.20931", "title": "SPA: Towards More Stealth and Persistent Backdoor Attacks in Federated Learning", "authors": ["Chengcheng Zhu", "Ye Li", "Bosen Rao", "Jiale Zhang", "Yunlong Mao", "Sheng Zhong"], "summary": "Federated Learning (FL) has emerged as a leading paradigm for\nprivacy-preserving distributed machine learning, yet the distributed nature of\nFL introduces unique security challenges, notably the threat of backdoor\nattacks. Existing backdoor strategies predominantly rely on end-to-end label\nsupervision, which, despite their efficacy, often results in detectable feature\ndisentanglement and limited persistence. In this work, we propose a novel and\nstealthy backdoor attack framework, named SPA, which fundamentally departs from\ntraditional approaches by leveraging feature-space alignment rather than direct\ntrigger-label association. Specifically, SPA reduces representational distances\nbetween backdoor trigger features and target class features, enabling the\nglobal model to misclassify trigger-embedded inputs with high stealth and\npersistence. We further introduce an adaptive, adversarial trigger optimization\nmechanism, utilizing boundary-search in the feature space to enhance attack\nlongevity and effectiveness, even against defensive FL scenarios and non-IID\ndata distributions. Extensive experiments on various FL benchmarks demonstrate\nthat SPA consistently achieves high attack success rates with minimal impact on\nmodel utility, maintains robustness under challenging participation and data\nheterogeneity conditions, and exhibits persistent backdoor effects far\nexceeding those of conventional techniques. Our results call urgent attention\nto the evolving sophistication of backdoor threats in FL and emphasize the\npressing need for advanced, feature-level defense techniques.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2506.20931v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20931v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SPA：迈向联邦学习中更隐蔽和持久的后门攻击", "tldr": "SPA是一种新型的联邦学习后门攻击框架，通过特征空间对齐而非传统的标签关联，实现了更高的隐蔽性和持久性，并对抗防御机制和非IID数据。", "motivation": "现有的联邦学习后门攻击策略主要依赖端到端标签监督，但这会导致可检测的特征解耦和有限的持久性。", "method": "本文提出了一种名为SPA的新型隐蔽后门攻击框架，其核心是利用特征空间对齐而非直接的触发器-标签关联。具体而言，SPA减少了后门触发器特征和目标类特征之间的表征距离，使全局模型能够以高隐蔽性和持久性错误分类嵌入触发器的输入。此外，引入了一种自适应的对抗性触发器优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性，即使在防御性联邦学习场景和非IID数据分布下也能奏效。", "result": "在各种联邦学习基准测试上的大量实验表明，SPA始终能实现高攻击成功率，同时对模型效用影响最小；在具有挑战性的参与和数据异质性条件下保持鲁棒性；并表现出远超传统技术的持久后门效应。", "conclusion": "研究结果紧急呼吁关注联邦学习中后门威胁日益复杂化的问题，并强调迫切需要先进的特征级防御技术。", "translation": "联邦学习（FL）已成为一种领先的隐私保护分布式机器学习范式，然而FL的分布式特性带来了独特的安全挑战，特别是后门攻击的威胁。现有的后门策略主要依赖端到端标签监督，尽管有效，但通常会导致可检测的特征解耦和有限的持久性。在这项工作中，我们提出了一种新颖且隐蔽的后门攻击框架，名为SPA，它通过利用特征空间对齐而非直接的触发器-标签关联，从根本上脱离了传统方法。具体而言，SPA减少了后门触发器特征和目标类特征之间的表征距离，使全局模型能够以高隐蔽性和持久性错误分类嵌入触发器的输入。我们进一步引入了一种自适应的对抗性触发器优化机制，利用特征空间中的边界搜索来增强攻击的持久性和有效性，即使在防御性FL场景和非IID数据分布下也能奏效。在各种FL基准测试上的大量实验表明，SPA始终能实现高攻击成功率，同时对模型效用影响最小，在具有挑战性的参与和数据异质性条件下保持鲁棒性，并表现出远超传统技术的持久后门效应。我们的结果紧急呼吁关注FL中后门威胁日益复杂化的问题，并强调迫切需要先进的特征级防御技术。", "summary": "SPA是一种针对联邦学习的新型隐蔽后门攻击框架，旨在解决现有方法中可检测性高和持久性差的问题。它通过在特征空间中对齐后门触发器特征与目标类特征，而非传统的标签关联，实现高隐蔽性和持久性。此外，SPA引入自适应对抗性触发器优化机制，以增强攻击在防御环境和非IID数据下的效果和寿命。实验证明，SPA在保持模型效用的同时实现了高攻击成功率，并在异质性条件下表现出优异的鲁棒性和更持久的后门效应，这表明联邦学习迫切需要更先进的特征级防御。", "keywords": "联邦学习, 后门攻击, 特征空间对齐, 隐蔽性, 持久性", "comments": "SPA通过从传统的标签监督转向特征空间对齐，为联邦学习中的后门攻击提供了一种新颖且更隐蔽的范式。其创新点在于利用特征层面的操纵来提高攻击的隐蔽性和持久性，尤其是在对抗防御机制和非IID数据分布下的鲁棒性。这篇论文的重要性在于揭示了联邦学习中后门攻击的日益复杂性，并强调了开发更深层次、特征级防御策略的紧迫性。"}}
{"id": "2506.20954", "title": "Cooperative Circumnavigation for Multi-Quadrotor Systems via Onboard Sensing", "authors": ["Xueming Liu", "Lin Li", "Xiang Zhou", "Qingrui Zhang", "Tianjiang Hu"], "summary": "A cooperative circumnavigation framework is proposed for multi-quadrotor\nsystems to enclose and track a moving target without reliance on external\nlocalization systems. The distinct relationships between quadrotor-quadrotor\nand quadrotor-target interactions are evaluated using a heterogeneous\nperception strategy and corresponding state estimation algorithms. A modified\nKalman filter is developed to fuse visual-inertial odometry with range\nmeasurements to enhance the accuracy of inter-quadrotor relative localization.\nAn event-triggered distributed Kalman filter is designed to achieve robust\ntarget state estimation under visual occlusion by incorporating neighbor\nmeasurements and estimated inter-quadrotor relative positions. Using the\nestimation results, a cooperative circumnavigation controller is constructed,\nleveraging an oscillator-based autonomous formation flight strategy. We conduct\nextensive indoor and outdoor experiments to validate the efficiency of the\nproposed circumnavigation framework in occluded environments. Furthermore, a\nquadrotor failure experiment highlights the inherent fault tolerance property\nof the proposed framework, underscoring its potential for deployment in\nsearch-and-rescue operations.", "comment": "8 Pages, 7 figures. Accepted by RA-L", "pdf_url": "http://arxiv.org/pdf/2506.20954v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20954v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多旋翼系统基于机载感知的协同环绕", "tldr": "提出了一种多旋翼系统协同环绕框架，利用机载感知实现无外部定位的移动目标跟踪和包围，并在遮挡环境下表现出高效和容错性。", "motivation": "现有系统依赖外部定位，该研究旨在使多旋翼系统在不依赖外部定位系统的情况下，能够协同包围和跟踪移动目标。", "method": "提出一个协同环绕框架。采用异构感知策略评估旋翼机-旋翼机和旋翼机-目标之间的关系。开发了改进的卡尔曼滤波器融合视觉惯性里程计和距离测量以提高旋翼机间相对定位精度。设计了事件触发分布式卡尔曼滤波器，通过结合邻居测量和估计的旋翼机间相对位置，在视觉遮挡下实现鲁棒的目标状态估计。基于估计结果，构建了一个协同环绕控制器，利用基于振荡器的自主编队飞行策略。", "result": "通过广泛的室内外实验验证了所提出环绕框架在遮挡环境下的效率。四旋翼故障实验突出了该框架固有的容错性。", "conclusion": "所提出的协同环绕框架在不依赖外部定位的情况下，能够高效且鲁棒地实现多旋翼系统对移动目标的跟踪和包围，并具有良好的容错性，在搜救行动中具有潜在应用价值。", "translation": "提出了一种多旋翼系统协同环绕框架，用于在不依赖外部定位系统的情况下包围和跟踪移动目标。使用异构感知策略和相应的状态估计算法评估了旋翼机-旋翼机以及旋翼机-目标之间独特的相互关系。开发了一种改进的卡尔曼滤波器，将视觉惯性里程计与距离测量融合，以提高旋翼机间相对定位的精度。设计了一种事件触发分布式卡尔曼滤波器，通过结合邻居测量和估计的旋翼机间相对位置，在视觉遮挡下实现鲁棒的目标状态估计。利用估计结果，构建了一个协同环绕控制器，利用基于振荡器的自主编队飞行策略。我们进行了广泛的室内外实验，验证了所提出的环绕框架在遮挡环境下的效率。此外，一项旋翼机故障实验突出了所提出框架固有的容错特性，强调了其在搜救行动中的部署潜力。", "summary": "本文提出了一种多旋翼系统协同环绕框架，旨在无需外部定位系统即可包围和跟踪移动目标。该框架通过异构感知策略、改进的卡尔曼滤波器进行精确的旋翼机间相对定位，并设计事件触发分布式卡尔曼滤波器实现遮挡下的目标鲁棒估计。结合基于振荡器的编队飞行策略构建控制器。实验证明，该框架在遮挡环境下高效且具有容错性，适用于搜救任务。", "keywords": "多旋翼系统, 协同环绕, 机载感知, 卡尔曼滤波, 容错性", "comments": "该论文的创新点在于提出了一个无需外部定位的多旋翼协同环绕框架，特别是在遮挡环境下通过结合多种感知和估计方法（如改进卡尔曼滤波、事件触发分布式卡尔曼滤波）实现了鲁棒的目标跟踪和编队控制。其固有的容错性进一步增加了其实用性和部署潜力，尤其是在搜救等复杂场景中。"}}
{"id": "2506.21246", "title": "From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting", "authors": ["Giorgos Demosthenous", "Chryssis Georgiou", "Eliada Polydorou"], "summary": "This study investigates the impact of data source diversity on the\nperformance of cryptocurrency forecasting models by integrating various data\ncategories, including technical indicators, on-chain metrics, sentiment and\ninterest metrics, traditional market indices, and macroeconomic indicators. We\nintroduce the Crypto100 index, representing the top 100 cryptocurrencies by\nmarket capitalization, and propose a novel feature reduction algorithm to\nidentify the most impactful and resilient features from diverse data sources.\nOur comprehensive experiments demonstrate that data source diversity\nsignificantly enhances the predictive performance of forecasting models across\ndifferent time horizons. Key findings include the paramount importance of\non-chain metrics for both short-term and long-term predictions, the growing\nrelevance of traditional market indices and macroeconomic indicators for\nlonger-term forecasts, and substantial improvements in model accuracy when\ndiverse data sources are utilized. These insights help demystify the short-term\nand long-term driving factors of the cryptocurrency market and lay the\ngroundwork for developing more accurate and resilient forecasting models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21246v1", "categories": ["q-fin.PM", "cs.AI", "cs.ET", "cs.LG", "q-fin.ST"], "cate": "q-fin.PM", "url": "http://arxiv.org/abs/2506.21246v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从链上到宏观：评估数据源多样性在加密货币市场预测中的重要性", "tldr": "本研究通过整合链上、技术、情绪、传统市场和宏观经济等多源数据，并引入特征降维算法，证明数据源多样性显著提升了加密货币预测模型的性能，尤其强调链上指标的重要性以及传统市场和宏观指标对长期预测的关联性。", "motivation": "评估数据源多样性对加密货币预测模型性能的影响，并揭示加密货币市场的短期和长期驱动因素。", "method": "整合多种数据类别，包括技术指标、链上指标、情绪和兴趣指标、传统市场指数和宏观经济指标。引入Crypto100指数代表市值前100的加密货币。提出一种新颖的特征降维算法以识别最具影响力和弹性的特征。进行综合实验。", "result": "数据源多样性显著增强了预测模型在不同时间范围内的预测性能。链上指标对短期和长期预测都至关重要。传统市场指数和宏观经济指标对长期预测的关联性日益增长。利用多样化数据源可大幅提高模型准确性。", "conclusion": "数据源多样性对于开发更准确、更具弹性的加密货币市场预测模型至关重要，并有助于揭示市场驱动因素。", "translation": "本研究通过整合各种数据类别，包括技术指标、链上指标、情绪和兴趣指标、传统市场指数以及宏观经济指标，调查了数据源多样性对加密货币预测模型性能的影响。我们引入了Crypto100指数，代表市值前100名的加密货币，并提出了一种新颖的特征降维算法，以从多样化的数据源中识别出最具影响力和弹性的特征。我们全面的实验表明，数据源多样性显著增强了预测模型在不同时间范围内的预测性能。主要发现包括：链上指标对短期和长期预测都至关重要，传统市场指数和宏观经济指标对长期预测的关联性日益增长，以及利用多样化数据源可大幅提高模型准确性。这些见解有助于揭示加密货币市场的短期和长期驱动因素，并为开发更准确、更具弹性的预测模型奠定基础。", "summary": "本研究探讨了数据源多样性对加密货币市场预测模型性能的影响。通过整合技术、链上、情绪、传统市场和宏观经济等多类别数据，并引入Crypto100指数和新颖的特征降维算法，实验证明数据源多样性显著提升了预测准确性。研究强调了链上指标在短期和长期预测中的关键作用，以及传统市场和宏观经济指标对长期预测日益增长的重要性，为构建更稳健的加密货币预测模型提供了基础。", "keywords": "加密货币预测, 数据源多样性, 链上指标, 宏观经济指标, 特征降维", "comments": "这篇论文的创新点在于系统地评估了不同类型数据源（特别是链上和宏观经济数据）对加密货币市场预测的影响，并提出了一种特征降维算法来处理多源数据。其重要性在于为构建更全面、更准确的加密货币预测模型提供了实证支持和指导，尤其是在一个高度波动且受多种因素影响的市场中。研究结果对于投资者、分析师和模型开发者都具有实际意义。"}}
{"id": "2506.20869", "title": "Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation", "authors": ["Md Toufique Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Ayman Asad Khan", "Mika Saari", "Pekka Abrahamsson"], "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.", "comment": "Accepted as a full paper to the 51st Euromicro Conference on Software\n  Engineering and Advanced Applications (SEAA 2025). 9 pages, 4 figures. This\n  is the preprint version and not the final camera ready version", "pdf_url": "http://arxiv.org/pdf/2506.20869v1", "categories": ["cs.SE", "cs.AI", "cs.IR", "D.2.11; I.2.6; H.3.3"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20869v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "工程化RAG系统用于实际应用：设计、开发与评估", "tldr": "本研究开发并评估了五个真实世界领域的RAG系统，通过用户参与评估并记录了开发和部署RAG系统的12条关键经验教训。", "motivation": "尽管检索增强生成（RAG）系统是弥补大型语言模型（LLMs）在事实准确性和上下文相关性方面局限性的关键方法，但目前缺乏关于基于真实世界用例、通过用户参与评估并系统记录经验教训的RAG实现开发的实证研究。", "method": "本研究开发了五个针对治理、网络安全、农业、工业研究和医疗诊断等真实世界场景的特定领域RAG应用。每个系统都集成了多语言OCR、通过向量嵌入进行的语义检索以及领域适应性LLMs，并通过本地服务器或云API部署。研究通过一项涉及100名参与者的网络评估，从易用性、相关性、透明度、响应性、准确性和推荐可能性六个维度对系统进行了评估。", "result": "基于用户反馈和开发经验，研究记录了十二条关键经验教训，揭示了影响RAG系统在实践中可靠性和可用性的技术、操作和伦理挑战。", "conclusion": "本研究的结论是，通过实际应用开发和用户评估，我们总结了十二条关键经验教训，这些经验教训突出了在实际部署RAG系统时面临的技术、操作和伦理挑战，对提高其可靠性和可用性至关重要。", "translation": "检索增强生成（RAG）系统正成为将大型语言模型（LLMs）与外部知识相结合的关键方法，以解决事实准确性和上下文相关性方面的局限性。然而，目前缺乏关于基于真实世界用例、通过一般用户参与评估并伴随系统性经验教训文档的RAG实现开发的实证研究。本文介绍了为治理、网络安全、农业、工业研究和医疗诊断等真实世界场景开发的五个领域特定RAG应用。每个系统都集成了多语言OCR、通过向量嵌入进行的语义检索以及领域适应性LLMs，并通过本地服务器或云API部署以满足不同的用户需求。一项涉及总共100名参与者的网络评估从六个维度对系统进行了评估：(i) 易用性，(ii) 相关性，(iii) 透明度，(iv) 响应性，(v) 准确性，和 (vi) 推荐可能性。基于用户反馈和我们的开发经验，我们记录了十二条关键经验教训，突出了影响RAG系统在实践中可靠性和可用性的技术、操作和伦理挑战。", "summary": "本研究旨在弥补RAG系统在真实世界应用中经验研究的空白。论文开发了五个跨越治理、网络安全、农业、工业研究和医疗诊断的领域特定RAG应用。这些系统整合了多语言OCR、向量嵌入和领域适应性LLMs。通过对100名用户的网络评估，研究评估了系统的易用性、相关性、透明度、响应性、准确性和推荐可能性。最终，研究总结了12条关键经验教训，揭示了在实际部署RAG系统时面临的技术、操作和伦理挑战。", "keywords": "RAG系统, 真实世界应用, 大型语言模型, 用户评估, 经验教训", "comments": "本论文的创新之处在于其专注于RAG系统在真实世界场景中的工程化实践和实证评估，而非仅仅停留在理论层面。通过开发五个跨不同领域的应用并进行用户参与评估，论文提供了宝贵的实践经验和系统性的经验教训，填补了现有研究中缺乏实际部署细节和挑战的空白。这些经验教训对于未来RAG系统的开发和部署具有重要的指导意义，特别是其对技术、操作和伦理挑战的关注，使其成为RAG应用领域的重要参考。"}}
{"id": "2506.21195", "title": "Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact", "authors": ["Neha Raghuvanshi"], "summary": "Have you wondered how cross-functional teams balance between maximizing value\nthat users derive and business growth leading to win-win situations? This case\nstudy shows how User Experience Research (UXR) and Data Science teams used\nmixed methods research to strategically influence Product Led Growth (PLG) for\na Password Manager used by million+ users, thus allowing our users, internal\nteams, and business to win. The audience will take away practical\nlessons/techniques related to leveraging mixed methods to: a. Maximize user\nvalue while meeting business growth goals b. Influence cross-functional teams\nc. Measure user and business impact This case study can be easily tied to the\nUXR Point of view pyramid (POV) [2] that represents a methodological approach\nto construct a POV and further dives into actioning POV to create measurable\nuser and business impact.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21195v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21195v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "有意义地跟随用户，产品增长将随之而来：一项将用户体验视角与增长联系起来并产生可衡量影响的混合方法案例研究", "tldr": "本案例研究展示了用户体验研究（UXR）和数据科学团队如何使用混合方法，战略性地影响了密码管理器的产品主导增长，从而为用户、内部团队和业务带来了双赢。", "motivation": "探讨跨职能团队如何在最大化用户价值与实现业务增长之间取得平衡，特别是通过用户体验研究和数据科学来影响产品主导增长。", "method": "本研究采用混合方法案例研究，由用户体验研究（UXR）和数据科学团队共同进行，旨在战略性地影响产品主导增长。研究内容与用户体验研究视角（POV）金字塔的方法论方法相结合。", "result": "研究展示了用户体验研究和数据科学团队如何成功地战略性影响了拥有百万用户的密码管理器的产品主导增长，实现了用户、内部团队和业务的共赢。它提供了利用混合方法来最大化用户价值、实现业务增长目标、影响跨职能团队以及衡量用户和业务影响的实用经验和技术。", "conclusion": "通过用户体验研究和数据科学团队运用混合方法，可以有效平衡用户价值与业务增长，并产生可衡量的积极影响，从而实现多方共赢。", "translation": "您是否曾想过，跨职能团队如何在最大化用户所获得价值与业务增长之间取得平衡，从而实现双赢局面？本案例研究展示了用户体验研究（UXR）和数据科学团队如何使用混合方法研究，战略性地影响了拥有百万以上用户的密码管理器产品的产品主导增长（PLG），从而使我们的用户、内部团队和业务都取得了成功。受众将从中学到利用混合方法的实用经验/技术，以：a. 在实现业务增长目标的同时最大化用户价值 b. 影响跨职能团队 c. 衡量用户和业务影响。本案例研究可以轻松地与用户体验研究视角金字塔（POV）[2] 联系起来，该金字塔代表了一种构建视角的方法论，并进一步深入探讨了如何将视角付诸实践以创造可衡量的用户和业务影响。", "summary": "本案例研究通过一个拥有百万用户的密码管理器产品，展示了用户体验研究（UXR）和数据科学团队如何运用混合方法来战略性地推动产品主导增长（PLG）。它阐述了如何平衡用户价值与业务增长、影响跨职能团队以及衡量影响，并与用户体验研究视角金字塔相契合，从而创造可衡量的用户和业务成果。", "keywords": "用户体验研究, 混合方法, 产品主导增长, 案例研究, 用户价值", "comments": "该论文以案例研究的形式，提供了用户体验研究和数据科学团队应用混合方法实现产品主导增长的实用范例。其创新之处在于清晰展示了从用户体验研究视角到可衡量业务影响的路径，为平衡用户价值与业务目标提供了可操作的见解。"}}
{"id": "2506.21370", "title": "Cluster-Aware Two-Stage Method for Fast Iterative MIMO Detection in LEO Satellite Communications", "authors": ["Jiuyu Liu", "Yi Ma", "Qihao Peng", "Rahim Tafazolli"], "summary": "In this paper, a cluster-aware two-stage multiple-input multiple-output\n(MIMO) detection method is proposed for direct-to-cell satellite\ncommunications. The method achieves computational efficiency by exploiting a\ndistinctive property of satellite MIMO channels: users within the same\ngeographical cluster exhibit highly correlated channel characteristics due to\ntheir physical proximity, which typically impedes convergence in conventional\niterative MIMO detectors. The proposed method implements a two-stage strategy\nthat first eliminates intra-cluster interference using computationally\nefficient small matrix inversions, then utilizes these pre-computed matrices to\naccelerate standard iterative MIMO detectors such as Gauss-Seidel (GS) and\nsymmetric successive over-relaxation (SSOR) for effective inter-cluster\ninterference cancellation. Computer simulations demonstrate that the proposed\nmethod achieves more than 12 times faster convergence under perfect channel\nstate information. Even when accounting for channel estimation errors, the\nmethod maintains 9 times faster convergence, demonstrating its robustness and\neffectiveness for next-generation satellite MIMO communications.", "comment": "This work has been accepted by IEEE/CIC ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2506.21370v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.21370v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "低轨卫星通信中用于快速迭代MIMO检测的集群感知两阶段方法", "tldr": "该论文提出了一种集群感知的两阶段MIMO检测方法，利用卫星MIMO信道的特性，显著加速了低轨卫星通信中迭代MIMO检测器的收敛速度。", "motivation": "由于同一地理集群内的用户具有高度相关的信道特性，这通常会阻碍传统迭代MIMO检测器的收敛，因此需要一种更高效的方法来处理卫星MIMO信道中的干扰。", "method": "所提出的方法采用两阶段策略：首先利用计算高效的小矩阵求逆消除集群内干扰；然后利用这些预计算的矩阵加速高斯-赛德尔（GS）和对称逐次超松弛（SSOR）等标准迭代MIMO检测器，以有效消除集群间干扰。", "result": "计算机仿真表明，在完美信道状态信息下，该方法实现了超过12倍的收敛速度提升；即使考虑信道估计误差，收敛速度仍能提高9倍。", "conclusion": "该方法在低轨卫星MIMO通信中表现出鲁棒性和有效性，适用于下一代卫星MIMO通信。", "translation": "本文提出了一种用于直达小区卫星通信的集群感知两阶段多输入多输出（MIMO）检测方法。该方法通过利用卫星MIMO信道的独特特性来实现计算效率：同一地理集群内的用户由于物理邻近性而表现出高度相关的信道特性，这通常会阻碍传统迭代MIMO检测器的收敛。所提出的方法实施了一种两阶段策略，首先使用计算高效的小矩阵求逆消除集群内干扰，然后利用这些预计算的矩阵加速标准迭代MIMO检测器（如高斯-赛德尔（GS）和对称逐次超松弛（SSOR））以有效消除集群间干扰。计算机仿真表明，在完美信道状态信息下，所提出的方法实现了超过12倍的收敛速度提升。即使考虑到信道估计误差，该方法仍保持9倍的收敛速度提升，这表明了其在下一代卫星MIMO通信中的鲁棒性和有效性。", "summary": "本文提出了一种针对直达小区卫星通信的集群感知两阶段MIMO检测方法。该方法利用卫星MIMO信道中同集群用户信道特性高度相关的特点，通过分阶段处理集群内和集群间干扰来提高计算效率。首先，利用小矩阵求逆消除集群内干扰；然后，利用这些预计算的矩阵加速标准迭代MIMO检测器（如GS和SSOR）来消除集群间干扰。仿真结果表明，该方法在完美信道信息下收敛速度提升12倍以上，在存在信道估计误差时仍能提升9倍，验证了其在下一代卫星MIMO通信中的鲁棒性和有效性。", "keywords": "MIMO检测, 低轨卫星通信, 集群感知, 迭代检测, 收敛性", "comments": "该论文的创新点在于充分利用了卫星MIMO信道中特有的集群用户信道相关性，并将其融入到两阶段检测框架中，有效解决了传统迭代MIMO检测器收敛慢的问题。这种“集群感知”的设计思路对于提升未来低轨卫星通信系统的性能具有重要意义，尤其是在高吞吐量和低延迟的应用场景下。其在存在信道估计误差时仍能保持显著的性能提升，也体现了方法的实用性。"}}
{"id": "2506.20980", "title": "Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs", "authors": ["Ziyu Zheng", "Yaming Yang", "Ziyu Guan", "Wei Zhao", "Weigang Lu"], "summary": "Real-world networks usually have a property of node heterophily, that is, the\nconnected nodes usually have different features or different labels. This\nheterophily issue has been extensively studied in homogeneous graphs but\nremains under-explored in heterogeneous graphs, where there are multiple types\nof nodes and edges. Capturing node heterophily in heterogeneous graphs is very\nchallenging since both node/edge heterogeneity and node heterophily should be\ncarefully taken into consideration. Existing methods typically convert\nheterogeneous graphs into homogeneous ones to learn node heterophily, which\nwill inevitably lose the potential heterophily conveyed by heterogeneous\nrelations. To bridge this gap, we propose Relation-Aware Separation of\nHomophily and Heterophily (RASH), a novel contrastive learning framework that\nexplicitly models high-order semantics of heterogeneous interactions and\nadaptively separates homophilic and heterophilic patterns. Particularly, RASH\nintroduces dual heterogeneous hypergraphs to encode multi-relational bipartite\nsubgraphs and dynamically constructs homophilic graphs and heterophilic graphs\nbased on relation importance. A multi-relation contrastive loss is designed to\nalign heterogeneous and homophilic/heterophilic views by maximizing mutual\ninformation. In this way, RASH simultaneously resolves the challenges of\nheterogeneity and heterophily in heterogeneous graphs. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of RASH across various\ndownstream tasks. The code is available at:\nhttps://github.com/zhengziyu77/RASH.", "comment": "accepted by KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.20980v1", "categories": ["cs.SI", "cs.AI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.20980v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "增强同质性-异质性分离：异构图中的关系感知学习", "tldr": "本文提出了RASH，一个新颖的对比学习框架，用于在异构图中明确建模高阶语义的异构交互，并自适应地分离同质性和异质性模式，解决了异构图中的异质性问题。", "motivation": "现有研究在同质图上的异质性问题已被广泛研究，但在异构图（包含多种节点和边类型）中仍未得到充分探索。现有方法通常将异构图转换为同构图来学习节点异质性，这会不可避免地丢失异构关系所传达的潜在异质性。因此，需要一种能够同时处理异构性和异质性挑战的方法。", "method": "本文提出了RASH（Relation-Aware Separation of Homophily and Heterophily），一个新颖的对比学习框架。RASH引入了双重异构超图来编码多关系二分图，并根据关系重要性动态构建同质图和异质图。同时设计了一个多关系对比损失，通过最大化互信息来对齐异构视图和同质/异质视图。通过这种方式，RASH同时解决了异构图中的异构性和异质性挑战。", "result": "在基准数据集上进行的广泛实验证明了RASH在各种下游任务中的有效性。", "conclusion": "RASH通过明确建模异构交互的高阶语义并自适应地分离同质性和异质性模式，成功解决了异构图中的异构性和异质性挑战，并在多个下游任务中表现出有效性。", "translation": "真实世界网络通常具有节点异质性，即连接的节点通常具有不同的特征或标签。这种异质性问题在同质图中已得到广泛研究，但在异构图（其中存在多种节点和边类型）中仍未得到充分探索。在异构图中捕获节点异质性非常具有挑战性，因为节点/边的异构性和节点异质性都应仔细考虑。现有方法通常将异构图转换为同质图来学习节点异质性，这会不可避免地丢失异构关系所传达的潜在异质性。为了弥补这一差距，我们提出了关系感知同质性与异质性分离（RASH），一个新颖的对比学习框架，它明确地建模异构交互的高阶语义并自适应地分离同质性和异质性模式。特别是，RASH引入了双重异构超图来编码多关系二分图，并根据关系重要性动态构建同质图和异质图。设计了一个多关系对比损失，通过最大化互信息来对齐异构视图和同质/异质视图。通过这种方式，RASH同时解决了异构图中的异构性和异质性挑战。在基准数据集上进行的广泛实验证明了RASH在各种下游任务中的有效性。代码可在：https://github.com/zhengziyu77/RASH 获取。", "summary": "本文提出了一种名为RASH（关系感知同质性与异质性分离）的新型对比学习框架，旨在解决异构图中的节点异质性问题。针对现有方法在处理异构关系时信息丢失的缺陷，RASH通过引入双重异构超图来编码多关系二分图，并根据关系重要性动态构建同质图和异质图。此外，它设计了多关系对比损失来对齐不同视图，从而有效分离同质性和异质性模式，并同时处理异构性和异质性挑战。实验结果表明RASH在多个下游任务中表现出有效性。", "keywords": "异构图, 异质性, 同质性, 对比学习, 关系感知", "comments": "该论文通过引入关系感知的对比学习框架RASH，创新性地解决了异构图中同质性与异质性分离的难题。其核心贡献在于同时考虑了节点/边的异构性和节点异质性，并避免了传统方法在异构图转换过程中信息丢失的问题。通过构建双重异构超图和设计多关系对比损失，RASH能够更好地捕获高阶语义和区分不同的模式，这对于理解和分析复杂真实世界网络具有重要意义。"}}
{"id": "2506.21134", "title": "Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations", "authors": ["Jacopo Bufalino", "Jose Luis Martin-Navarro", "Mario Di Francesco", "Tuomas Aura"], "summary": "Kubernetes has emerged as the de facto standard for container orchestration.\nUnfortunately, its increasing popularity has also made it an attractive target\nfor malicious actors. Despite extensive research on securing Kubernetes, little\nattention has been paid to the impact of network configuration on the security\nof application deployments. This paper addresses this gap by conducting a\ncomprehensive analysis of network misconfigurations in a Kubernetes cluster\nwith specific reference to lateral movement. Accordingly, we carried out an\nextensive evaluation of 287 open-source applications belonging to six different\norganizations, ranging from IT companies and public entities to non-profits. As\na result, we identified 634 misconfigurations, well beyond what could be found\nby solutions in the state of the art. We responsibly disclosed our findings to\nthe concerned organizations and engaged in a discussion to assess their\nseverity. As of now, misconfigurations affecting more than thirty applications\nhave been fixed with the mitigations we proposed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21134v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21134v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "内幕工作：防御Kubernetes集群的网络错误配置", "tldr": "Kubernetes作为容器编排的事实标准，其网络配置错误对应用安全构成严重威胁。本文通过对287个开源应用进行全面分析，发现了634个网络错误配置，并协助相关组织修复了其中30多个，填补了该领域的研究空白。", "motivation": "Kubernetes已成为容器编排的事实标准，但其网络配置对应用部署安全的影响却鲜有研究，这导致了安全漏洞。本文旨在填补这一研究空白，深入分析Kubernetes集群中的网络错误配置，特别是与横向移动相关的安全风险。", "method": "本文对属于六个不同组织（包括IT公司、公共实体和非营利组织）的287个开源应用程序的Kubernetes集群进行了全面的网络错误配置分析，重点关注横向移动的可能性。", "result": "研究识别出634个网络错误配置，这一数量远超现有解决方案所能发现的。研究结果已负责任地披露给相关组织，并且截至目前，影响30多个应用程序的错误配置已根据本文提出的缓解措施得到修复。", "conclusion": "Kubernetes集群中的网络错误配置是一个普遍且严重的威胁，对应用部署安全影响巨大。通过系统的分析和负责任的披露，可以有效识别并修复这些问题，显著提升集群的整体安全性。", "translation": "Kubernetes已成为容器编排的事实标准。不幸的是，其日益增长的普及也使其成为恶意行为者的一个有吸引力的目标。尽管对保护Kubernetes进行了广泛研究，但很少关注网络配置对应用程序部署安全的影响。本文通过对Kubernetes集群中的网络错误配置进行全面分析来解决这一差距，并特别提及横向移动。因此，我们对属于六个不同组织（从IT公司和公共实体到非营利组织）的287个开源应用程序进行了广泛评估。结果，我们识别出634个错误配置，这远远超出了现有解决方案所能发现的。我们负责任地向相关组织披露了我们的发现，并进行了讨论以评估其严重性。截至目前，影响30多个应用程序的错误配置已根据我们提出的缓解措施得到修复。", "summary": "本文旨在填补Kubernetes集群网络配置对应用部署安全影响研究的空白。通过对287个开源应用的全面分析，本文识别出634个网络错误配置，这一数量远超现有解决方案的发现能力。研究结果已负责任地披露给相关组织，并已帮助修复了30多个应用的错误配置，凸显了网络错误配置对Kubernetes安全性的普遍影响和修复的重要性。", "keywords": "Kubernetes, 网络错误配置, 安全, 容器编排, 横向移动", "comments": "本文创新性地关注了Kubernetes集群中长期被忽视的网络错误配置问题，并进行了大规模的实证分析。其发现的错误配置数量远超现有工具，强调了此类安全漏洞的普遍性和严重性。研究不仅指出了问题，还积极与组织合作进行披露和修复，具有很强的实践意义和影响力，对提升Kubernetes集群的整体安全性具有重要贡献。"}}
{"id": "2506.20786", "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20786v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20786v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "AI驱动的基于MRI的脑肿瘤分割基准测试", "tldr": "本研究评估了多种基于SAM的模型和nnU-Net在BraTS 2023数据集上进行脑肿瘤分割的零样本推理和微调性能，发现SAM和SAM 2在精确边界框提示下表现出色，但nnU-Net在实际应用中仍占主导地位。", "motivation": "现有通用可提示模型和医学变体缺乏在常见医学数据集上针对不同提示质量的评估和比较。", "method": "本研究使用Segment Anything Model (SAM)、Segment Anything Model 2 (SAM 2)、MedSAM、SAM-Med-3D和nnU-Net，在BraTS 2023成人胶质瘤和儿科数据集上，针对点和边界框两种提示类型进行零样本推理。同时，通过在儿科数据集上对SAM、SAM 2、MedSAM和SAM-Med-3D进行微调，扩展了模型和提示的评估及比较。", "result": "SAM和SAM 2在提供极其精确的边界框提示时，Dice分数分别高达0.894和0.893，超过了nnU-Net的分割性能。然而，由于提供高精度提示的不切实际性，nnU-Net仍然是主导的医学图像分割网络。微调后，点提示的性能显著提高，但仍未能实现比边界框或nnU-Net更好的分割效果。", "conclusion": "尽管一些基于SAM的模型在理想提示下表现出色，但nnU-Net在实际应用中仍因其鲁棒性而占据主导地位。微调可以显著改善点提示性能，但仍需进一步研究以超越现有最佳方法。", "translation": "医学图像分割极大地辅助了医学诊断，其中基于U-Net的架构和nnU-Net提供了最先进的性能。近年来，已经引入了许多通用的可提示模型和医学变体，但目前缺乏在常见医学数据集上针对各种提示质量对这些模型进行评估和比较。本研究使用Segment Anything Model (SAM)、Segment Anything Model 2 (SAM 2)、MedSAM、SAM-Med-3D和nnU-Net在BraTS 2023成人胶质瘤和儿科数据集上，针对点和边界框两种提示类型，进行零样本推理。其中一些模型表现出有希望的Dice分数，特别是SAM和SAM 2在获得极其精确的边界框提示时，分别获得了高达0.894和0.893的分数，这超过了nnU-Net的分割性能。然而，由于向模型提供高精度提示的不切实际性，nnU-Net仍然是主导的医学图像分割网络。模型和提示的评估以及比较通过在儿科数据集上微调SAM、SAM 2、MedSAM和SAM-Med-3D得以扩展。微调后点提示性能的提升是显著的，并显示出未来研究的希望，但未能实现比边界框或nnU-Net更好的分割效果。", "summary": "本研究评估了多种AI驱动模型（包括SAM、SAM 2、MedSAM、SAM-Med-3D和nnU-Net）在BraTS 2023脑肿瘤数据集上的分割性能。研究发现，在理想的精确边界框提示下，SAM和SAM 2的性能可超越nnU-Net，但实际应用中高精度提示难以获得，使得nnU-Net仍是主流。此外，对部分模型进行微调可显著提升点提示性能，但仍未超越边界框或nnU-Net的效果。", "keywords": "脑肿瘤分割, AI驱动, 医学图像分割, SAM模型, nnU-Net", "comments": "这篇论文对当前流行的可提示分割模型在医学图像分割领域的应用进行了及时的基准测试。其创新点在于系统地比较了不同提示质量下这些模型（特别是SAM系列）的零样本和微调性能。研究揭示了尽管SAM等模型在理想条件下表现出色，但在实际医疗场景中，提供高精度提示的挑战使得传统的nnU-Net仍具优势。这强调了未来研究需要关注如何提高模型对低质量或自动生成提示的鲁棒性，以及如何弥合研究与实际应用之间的差距。"}}
{"id": "2506.20938", "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks", "authors": ["Joshua H. Davis", "Daniel Nichols", "Ishan Khillan", "Abhinav Bhatele"], "summary": "GPGPU architectures have become significantly diverse in recent years, which\nhas led to an emergence of a variety of specialized programming models and\nsoftware stacks to support them. While portable execution models exist, they\nstill require significant developer effort to port to and optimize for\ndifferent hardware architectures. Recent advances in large language models\n(LLMs) can help us reduce some of this programmer burden. In this paper, we\npresent a novel benchmark and testing framework, ParEval-Repo, which can be\nused to evaluate the efficacy of LLM-based approaches in automatically\ntranslating entire codebases across GPGPU execution models. ParEval-Repo\nincludes several scientific computing and AI mini-applications in a range of\nprogramming models, and levels of repository complexity. We use ParEval-Repo to\nevaluate a range of state-of-the-art open-source and commercial LLMs, with both\na non-agentic and a top-down agentic approach. We assess code generated by the\nLLMs and approaches in terms of compilability, functional correctness,\ncategories of build errors, and the cost of translation in terms of the number\nof inference tokens. Our results demonstrate that LLM translation of scientific\napplications is feasible for small programs but difficulty with generating\nfunctional build systems and cross-file dependencies pose challenges in scaling\nto larger codebases.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.20938v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20938v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ParEval-Repo：一个用于评估LLM在仓库级HPC翻译任务中表现的基准套件", "tldr": "提出了ParEval-Repo基准测试套件，用于评估LLM在GPGPU代码库翻译中的表现，发现LLM在小型程序上可行，但在大型程序中面临构建系统和跨文件依赖挑战。", "motivation": "近年来GPGPU架构日益多样化，导致代码移植和优化需要大量开发者精力；大型语言模型（LLM）的最新进展有望减轻这一负担。", "method": "本文提出了ParEval-Repo，一个用于评估LLM自动翻译整个GPGPU代码库的基准测试和测试框架。该框架包含多种编程模型和仓库复杂度的科学计算和AI迷你应用。研究人员使用非代理和自顶向下代理两种方法评估了一系列最先进的开源和商业LLM，并从可编译性、功能正确性、构建错误类别和推理令牌成本方面评估了LLM生成的代码。", "result": "结果表明，LLM对科学应用程序的翻译对于小型程序是可行的，但难以生成功能性构建系统和处理跨文件依赖，这给扩展到更大的代码库带来了挑战。", "conclusion": "LLM在代码翻译方面具有潜力，尤其对于小型程序，但要实现大规模代码库的自动翻译，仍需解决复杂的构建系统和跨文件依赖问题。", "translation": "近年来，GPGPU架构变得异常多样化，这导致了各种专业编程模型和软件栈的出现以支持它们。尽管存在可移植的执行模型，但它们仍然需要大量的开发人员精力才能移植并针对不同的硬件架构进行优化。大型语言模型（LLM）的最新进展可以帮助我们减轻程序员的负担。在本文中，我们提出了一个新颖的基准和测试框架ParEval-Repo，它可用于评估基于LLM的方法在自动翻译整个GPGPU执行模型代码库方面的功效。ParEval-Repo包含一系列科学计算和AI迷你应用程序，涵盖多种编程模型和不同级别的仓库复杂性。我们使用ParEval-Repo评估了一系列最先进的开源和商业LLM，采用了非代理和自顶向下代理两种方法。我们从可编译性、功能正确性、构建错误类别以及翻译成本（以推理令牌数量计）方面评估了LLM生成和方法生成的代码。我们的结果表明，LLM对科学应用程序的翻译对于小型程序是可行的，但难以生成功能性构建系统和处理跨文件依赖，这给扩展到更大的代码库带来了挑战。", "summary": "本文提出了ParEval-Repo，一个用于评估大型语言模型（LLMs）在仓库级高性能计算（HPC）代码翻译任务中表现的新基准测试套件。该套件包含多种编程模型和复杂度的科学计算及AI迷你应用。研究人员使用ParEval-Repo评估了多款LLMs，发现它们在翻译小型科学程序方面表现可行，但在处理大型代码库时，由于构建系统和跨文件依赖的挑战，仍存在显著困难。", "keywords": "LLM, 代码翻译, GPGPU, 基准测试, HPC", "comments": "该论文提出了一个及时且重要的基准测试，用于评估LLM在复杂代码翻译任务中的能力，特别是在GPGPU领域。其创新点在于关注仓库级别的翻译，而不仅仅是单个文件或函数。结果揭示了LLM在处理复杂构建系统和跨文件依赖方面的局限性，这对于未来LLM在软件工程领域的应用具有指导意义。"}}
{"id": "2506.20743", "title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "summary": "Foundation models (FMs) are catalyzing a transformative shift in materials\nscience (MatSci) by enabling scalable, general-purpose, and multimodal AI\nsystems for scientific discovery. Unlike traditional machine learning models,\nwhich are typically narrow in scope and require task-specific engineering, FMs\noffer cross-domain generalization and exhibit emergent capabilities. Their\nversatility is especially well-suited to materials science, where research\nchallenges span diverse data types and scales. This survey provides a\ncomprehensive overview of foundation models, agentic systems, datasets, and\ncomputational tools supporting this growing field. We introduce a task-driven\ntaxonomy encompassing six broad application areas: data extraction,\ninterpretation and Q\\&A; atomistic simulation; property prediction; materials\nstructure, design and discovery; process planning, discovery, and optimization;\nand multiscale modeling. We discuss recent advances in both unimodal and\nmultimodal FMs, as well as emerging large language model (LLM) agents.\nFurthermore, we review standardized datasets, open-source tools, and autonomous\nexperimental platforms that collectively fuel the development and integration\nof FMs into research workflows. We assess the early successes of foundation\nmodels and identify persistent limitations, including challenges in\ngeneralizability, interpretability, data imbalance, safety concerns, and\nlimited multimodal fusion. Finally, we articulate future research directions\ncentered on scalable pretraining, continual learning, data governance, and\ntrustworthiness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20743v1", "categories": ["cs.LG", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20743v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "材料科学中的人工智能综述：基础模型、LLM智能体、数据集和工具", "tldr": "本综述全面概述了基础模型（FMs）在材料科学中的应用，涵盖了FMs、LLM智能体、数据集和工具，探讨了其六个主要应用领域、最新进展、早期成功、持续存在的局限性以及未来的研究方向。", "motivation": "基础模型（FMs）正在推动材料科学（MatSci）的变革，通过提供可扩展、通用和多模态的AI系统实现科学发现。与传统机器学习模型不同，FMs具有跨领域泛化和涌现能力，特别适用于材料科学中多样化的数据类型和尺度挑战。因此，本综述旨在全面概述支持这一新兴领域的基础模型、智能体系统、数据集和计算工具。", "method": "本综述提供了基础模型、智能体系统、数据集和计算工具在材料科学中的全面概述。它引入了一个任务驱动的分类法，涵盖六个广泛的应用领域：数据提取、解释和问答；原子模拟；性质预测；材料结构、设计和发现；过程规划、发现和优化；以及多尺度建模。文中讨论了单模态和多模态FMs以及新兴的大型语言模型（LLM）智能体的最新进展，并回顾了标准化数据集、开源工具和自主实验平台。", "result": "综述评估了基础模型的早期成功，并指出了其持续存在的局限性，包括泛化性、可解释性、数据不平衡、安全问题和有限的多模态融合等挑战。", "conclusion": "未来的研究方向应集中于可扩展的预训练、持续学习、数据治理和可信度。", "translation": "基础模型（FMs）正在通过为科学发现提供可扩展、通用和多模态的AI系统，催化材料科学（MatSci）的变革性转变。与通常范围狭窄且需要特定任务工程的传统机器学习模型不同，FMs提供跨领域泛化并展现出涌现能力。它们的通用性特别适合材料科学，因为研究挑战跨越不同的数据类型和尺度。本综述全面概述了支持这一新兴领域的基础模型、智能体系统、数据集和计算工具。我们引入了一个任务驱动的分类法，涵盖六个广泛的应用领域：数据提取、解释和问答；原子模拟；性质预测；材料结构、设计和发现；过程规划、发现和优化；以及多尺度建模。我们讨论了单模态和多模态FMs以及新兴的大型语言模型（LLM）智能体的最新进展。此外，我们回顾了标准化数据集、开源工具和自主实验平台，它们共同推动了FMs的开发和集成到研究工作流程中。我们评估了基础模型的早期成功，并指出了持续存在的局限性，包括泛化性、可解释性、数据不平衡、安全问题和有限的多模态融合等挑战。最后，我们阐明了以可扩展预训练、持续学习、数据治理和可信度为中心的未来研究方向。", "summary": "这篇综述深入探讨了基础模型（FMs）如何通过提供可扩展、通用和多模态的AI系统来彻底改变材料科学研究。文章首先阐述了FMs相较于传统机器学习模型的优势，即跨领域泛化和涌现能力，使其特别适用于材料科学的多样化数据和挑战。随后，综述全面概述了支持该领域发展的FMs、智能体系统、数据集和计算工具，并提出一个任务驱动的分类法，涵盖六个主要应用领域。文章讨论了单模态和多模态FMs以及新兴LLM智能体的最新进展，并审视了标准化数据集、开源工具和自主实验平台。最后，综述评估了FMs的早期成功，并识别了泛化性、可解释性、数据不平衡、安全性和多模态融合等方面的局限性，并提出了未来研究方向，包括可扩展预训练、持续学习、数据治理和可信度。", "keywords": "基础模型, 材料科学, LLM智能体, 数据集, 人工智能综述", "comments": "这篇综述及时且全面地梳理了基础模型在材料科学领域的应用现状和未来潜力。其创新之处在于提出了一个任务驱动的分类法，有助于系统性地理解FMs在该领域的具体应用。论文不仅总结了现有成就，更深刻地指出了当前面临的挑战，如数据不平衡和多模态融合的局限性，并为未来的研究指明了方向，具有重要的指导意义。"}}
{"id": "2506.20701", "title": "Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models", "authors": ["Vineet Jain", "Kusha Sareen", "Mohammad Pedramfar", "Siamak Ravanbakhsh"], "summary": "Adapting a pretrained diffusion model to new objectives at inference time\nremains an open problem in generative modeling. Existing steering methods\nsuffer from inaccurate value estimation, especially at high noise levels, which\nbiases guidance. Moreover, information from past runs is not reused to improve\nsample quality, resulting in inefficient use of compute. Inspired by the\nsuccess of Monte Carlo Tree Search, we address these limitations by casting\ninference-time alignment as a search problem that reuses past computations. We\nintroduce a tree-based approach that samples from the reward-aligned target\ndensity by propagating terminal rewards back through the diffusion chain and\niteratively refining value estimates with each additional generation. Our\nproposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact\nsamples from the target distribution in the limit of infinite rollouts, and its\ngreedy variant, Diffusion Tree Search (DTS$^\\star$), performs a global search\nfor high reward samples. On MNIST and CIFAR-10 class-conditional generation,\nDTS matches the FID of the best-performing baseline with up to $10\\times$ less\ncompute. In text-to-image generation and language completion tasks, DTS$^\\star$\neffectively searches for high reward samples that match best-of-N with up to\n$5\\times$ less compute. By reusing information from previous generations, we\nget an anytime algorithm that turns additional compute into steadily better\nsamples, providing a scalable approach for inference-time alignment of\ndiffusion models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20701v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20701v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "扩散树采样：扩散模型可扩展的推理时对齐", "tldr": "本文提出了一种名为扩散树采样（DTS）的新方法，通过将推理时对齐视为一个搜索问题并重用过去计算，显著提高了扩散模型在推理时适应新目标的能力，并在不同任务中实现了更高的效率和样本质量。", "motivation": "预训练扩散模型在推理时适应新目标仍然是一个开放问题。现有引导方法存在价值估计不准确的问题，尤其是在高噪声水平下，导致引导偏差。此外，它们不重用过去的运行信息来提高样本质量，导致计算效率低下。", "method": "受蒙特卡洛树搜索成功的启发，本文将推理时对齐视为一个搜索问题，并重用过去的计算来解决上述限制。提出了一种基于树的方法，通过将终端奖励反向传播通过扩散链，并随着每次额外生成迭代地完善价值估计，从奖励对齐的目标密度中采样。提出的方法包括扩散树采样（DTS）及其贪婪变体扩散树搜索（DTS$^\text{\\star}$）。", "result": "在MNIST和CIFAR-10类别条件生成任务中，DTS以高达10倍的计算量匹配了表现最佳基线的FID。在文本到图像生成和语言补全任务中，DTS$^\text{\\star}$有效地搜索高奖励样本，以高达5倍的计算量匹配了最佳N选择的结果。", "conclusion": "通过重用来自先前生成的信息，DTS成为一种随时可用的算法，可以将额外的计算转化为持续更好的样本，为扩散模型的推理时对齐提供了一种可扩展的方法。", "translation": "将预训练扩散模型在推理时适应新目标仍然是生成建模中的一个开放问题。现有引导方法存在价值估计不准确的问题，尤其是在高噪声水平下，这会导致引导偏差。此外，它们不重用过去的运行信息来提高样本质量，导致计算效率低下。受蒙特卡洛树搜索成功的启发，我们通过将推理时对齐视为一个重用过去计算的搜索问题来解决这些限制。我们引入了一种基于树的方法，通过将终端奖励反向传播通过扩散链，并随着每次额外生成迭代地完善价值估计，从奖励对齐的目标密度中采样。我们提出的方法，扩散树采样（DTS），在无限次展开的极限下能从目标分布中产生渐近精确的样本，其贪婪变体，扩散树搜索（DTS$^\text{\\star}$），则执行高奖励样本的全局搜索。在MNIST和CIFAR-10类别条件生成任务中，DTS以高达10倍的计算量匹配了表现最佳基线的FID。在文本到图像生成和语言补全任务中，DTS$^\text{\\star}$有效地搜索高奖励样本，以高达5倍的计算量匹配了最佳N选择的结果。通过重用来自先前生成的信息，我们得到了一种随时可用的算法，可以将额外的计算转化为持续更好的样本，为扩散模型的推理时对齐提供了一种可扩展的方法。", "summary": "本文提出了一种名为扩散树采样（DTS）的新型方法，用于解决预训练扩散模型在推理时适应新目标时的效率和准确性问题。DTS将推理时对齐建模为一个搜索问题，并借鉴蒙特卡洛树搜索的思想，通过构建树结构并重用过去的计算来迭代优化价值估计。DTS及其贪婪变体DTS$^\text{\\star}$能够从目标分布中生成渐近精确的样本，并在多项任务中展现出显著的计算效率提升，例如在图像生成任务中实现高达10倍的计算量减少，在文本到图像和语言补全任务中实现高达5倍的计算量减少，同时保持或提高样本质量。该方法提供了一种可扩展的、能够持续利用额外计算来改进样本的推理时对齐方案。", "keywords": "扩散模型, 推理时对齐, 蒙特卡洛树搜索, 扩散树采样, 计算效率", "comments": "这项工作通过将推理时对齐问题转化为一个可重用的搜索问题，为扩散模型的引导提供了一个新颖且高效的视角。借鉴蒙特卡洛树搜索的成功经验，DTS有效地解决了现有方法中价值估计不准确和计算效率低下的问题。其创新之处在于通过树结构反向传播奖励并迭代优化，实现计算资源的有效利用，从而在保证样本质量的同时大幅减少计算量。这使得扩散模型在实际应用中更具可扩展性和实用性。"}}
{"id": "2506.20882", "title": "Resilience Through Escalation: A Graph-Based PACE Architecture for Satellite Threat Response", "authors": ["Anouar Boumeftah", "Sarah McKenzie-Picot", "Peter Klimas", "Gunes Karabulut Kurt"], "summary": "Satellite systems increasingly face operational risks from jamming,\ncyberattacks, and electromagnetic disruptions. Traditional redundancy\nstrategies often fail against dynamic, multi-vector threats. This paper\nintroduces a resilience-by-design framework grounded in the PACE (Primary,\nAlternate, Contingency, Emergency) methodology, originally developed for\ntactical communications in military operations, adapting it to satellite\nsystems through a layered state-transition model informed by threat scoring\nframeworks such as CVSS, DREAD, and NASA's risk matrix. We define a dynamic\nresilience index to quantify system adaptability and implement three PACE\nvariants: static, adaptive, and softmax-based decision models, to evaluate\nresilience under diverse disruption scenarios. The proposed approach highlights\nthe effectiveness of lightweight, decision-aware fallback mechanisms in\nimproving survivability and operational continuity for next-generation space\nassets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20882v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20882v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "通过升级实现弹性：一种基于图的卫星威胁响应PACE架构", "tldr": "针对卫星系统面临的动态多向量威胁，本文提出了一种基于PACE方法论的弹性设计框架，通过分层状态转换模型和威胁评分框架，引入动态弹性指数和三种PACE变体，以提高下一代空间资产的生存能力和运行连续性。", "motivation": "卫星系统日益面临来自干扰、网络攻击和电磁中断等操作风险。传统的冗余策略在应对动态、多向量威胁时往往失效。", "method": "本文引入了一种基于PACE（Primary, Alternate, Contingency, Emergency）方法论的弹性设计框架，该框架通过结合CVSS、DREAD和NASA风险矩阵等威胁评分框架的分层状态转换模型，将其应用于卫星系统。研究人员定义了一个动态弹性指数来量化系统适应性，并实现了三种PACE变体：静态、自适应和基于softmax的决策模型，以评估在不同中断场景下的弹性。", "result": "所提出的方法突出了轻量级、决策感知的故障转移机制在提高下一代空间资产的生存能力和运行连续性方面的有效性。", "conclusion": "轻量级、决策感知的故障转移机制能够有效提高下一代空间资产的生存能力和运行连续性，从而增强卫星系统对动态多向量威胁的弹性。", "translation": "卫星系统日益面临来自干扰、网络攻击和电磁中断的操作风险。传统的冗余策略在应对动态、多向量威胁时往往失效。本文引入了一种基于PACE（主用、备用、应急、紧急）方法论的弹性设计框架，该方法最初是为军事行动中的战术通信而开发的，通过结合CVSS、DREAD和NASA风险矩阵等威胁评分框架的分层状态转换模型，将其应用于卫星系统。我们定义了一个动态弹性指数来量化系统适应性，并实现了三种PACE变体：静态、自适应和基于softmax的决策模型，以评估在不同中断场景下的弹性。所提出的方法突出了轻量级、决策感知的故障转移机制在提高下一代空间资产的生存能力和运行连续性方面的有效性。", "summary": "本文针对卫星系统在动态多向量威胁下传统冗余策略失效的问题，提出了一种基于PACE（Primary, Alternate, Contingency, Emergency）方法论的弹性设计框架。该框架将PACE应用于卫星系统，通过分层状态转换模型和威胁评分框架（如CVSS、DREAD、NASA风险矩阵），并引入动态弹性指数和三种PACE变体（静态、自适应、softmax），以评估和提高系统在各种中断场景下的韧性。研究结果表明，轻量级、决策感知的故障转移机制能有效提升下一代空间资产的生存能力和运行连续性。", "keywords": "卫星系统, 弹性, PACE架构, 威胁响应, 故障转移机制", "comments": "本文创新性地将源于军事战术通信的PACE方法论应用于卫星系统弹性设计，并通过结合威胁评分框架和动态弹性指数，提供了一个结构化的应对动态多向量威胁的策略。其提出的轻量级、决策感知的故障转移机制对于提高未来空间资产的生存能力和操作连续性具有重要意义。"}}
{"id": "2506.20858", "title": "Doppler Estimation and Compensation Techniques in LoRa Direct-to-Satellite Communications", "authors": ["Jamil Farhat", "Gianni Pasolini", "Enrico Paolini", "Muhammad Asad Ullah", "Richard Demo Souza"], "summary": "Within the LPWAN framework, the LoRa modulation adopted by LoRaWAN technology\nhas garnered significant interest as a connectivity solution for IoT\napplications due to its ability to offer low-cost, low-power, and long-range\ncommunications. One emerging use case of LoRa is DtS connectivity, which\nextends coverage to remote areas for supporting IoT operations. The satellite\nIoT industry mainly prefers LEO because it has lower launch costs and less path\nloss compared to Geostationary orbit. However, a major drawback of LEO\nsatellites is the impact of the Doppler effect caused by their mobility.\nEarlier studies have confirmed that the Doppler effect significantly degrades\nthe LoRa DtS performance. In this paper, we propose four frameworks for Doppler\nestimation and compensation in LoRa DtS connectivity and numerically compare\nthe performance against the ideal scenario without the Doppler effect.\nFurthermore, we investigate the trade-offs among these frameworks by analyzing\nthe interplay between spreading factor, and other key parameters related to the\nDoppler effect. The results provide insights into how to achieve robust LoRa\nconfigurations for DtS connectivity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20858v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20858v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "LoRa直连卫星通信中的多普勒效应估计与补偿技术", "tldr": "本文提出了四种多普勒效应估计和补偿框架，用于LoRa直连卫星通信，并分析了它们在不同参数下的性能，以实现鲁棒的LoRa配置。", "motivation": "LoRa技术在物联网应用中因其低成本、低功耗和长距离通信能力而受到关注。LoRa直连卫星（DtS）通信是其新兴应用，可将覆盖范围扩展到偏远地区。然而，低地球轨道（LEO）卫星的移动性导致的多普勒效应会显著降低LoRa DtS的性能。", "method": "本文提出了四种用于LoRa DtS连接中多普勒效应估计和补偿的框架，并将其性能与没有多普勒效应的理想场景进行了数值比较。此外，通过分析扩频因子和其他与多普勒效应相关的关键参数之间的相互作用，研究了这些框架之间的权衡。", "result": "结果提供了关于如何为DtS连接实现鲁棒LoRa配置的见解。", "conclusion": "通过提出和比较多普勒效应估计与补偿框架，本文为LoRa直连卫星通信中实现鲁棒的LoRa配置提供了指导。", "translation": "在LPWAN框架内，LoRaWAN技术采用的LoRa调制因其提供低成本、低功耗和长距离通信的能力而作为物联网应用的连接解决方案获得了广泛关注。LoRa的一个新兴用例是DtS连接，它将覆盖范围扩展到偏远地区以支持物联网操作。卫星物联网行业主要偏爱LEO，因为它与地球同步轨道相比具有更低的发射成本和更小的路径损耗。然而，LEO卫星的一个主要缺点是其移动性引起的多普勒效应的影响。早期研究已证实多普勒效应会显著降低LoRa DtS的性能。在本文中，我们提出了四种用于LoRa DtS连接中多普勒效应估计和补偿的框架，并将其性能与没有多普勒效应的理想场景进行了数值比较。此外，我们通过分析扩频因子和与多普勒效应相关的其他关键参数之间的相互作用，研究了这些框架之间的权衡。结果提供了关于如何为DtS连接实现鲁棒LoRa配置的见解。", "summary": "本文针对LoRa直连卫星（DtS）通信中低地球轨道（LEO）卫星移动性导致的多普勒效应问题，提出了四种多普勒效应估计和补偿框架。通过数值比较这些框架的性能，并分析扩频因子与其他关键参数之间的权衡，研究旨在为实现鲁棒的LoRa DtS配置提供指导和见解。", "keywords": "LoRa, 直连卫星通信, 多普勒效应, 补偿技术, LEO卫星", "comments": "本文解决了LoRa直连卫星通信中一个关键的挑战——多普勒效应。通过提出并比较多种估计和补偿框架，为LoRa在LEO卫星环境中的可靠运行提供了实用的解决方案。其创新在于提出并分析了具体的多普勒补偿策略，并提供了参数权衡的见解，对于推动LoRa DtS应用具有重要意义。"}}
{"id": "2506.20897", "title": "Development of MR spectral analysis method robust against static magnetic field inhomogeneity", "authors": ["Shuki Maruyama", "Hidenori Takeshima"], "summary": "Purpose:To develop a method that enhances the accuracy of spectral analysis\nin the presence of static magnetic field B0 inhomogeneity. Methods:The authors\nproposed a new spectral analysis method utilizing a deep learning model trained\non modeled spectra that consistently represent the spectral variations induced\nby B0 inhomogeneity. These modeled spectra were generated from the B0 map and\nmetabolite ratios of the healthy human brain. The B0 map was divided into a\npatch size of subregions, and the separately estimated metabolites and baseline\ncomponents were averaged and then integrated. The quality of the modeled\nspectra was visually and quantitatively evaluated against the measured spectra.\nThe analysis models were trained using measured, simulated, and modeled\nspectra. The performance of the proposed method was assessed using mean squared\nerrors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)\nof the metabolite ratios were also compared to LCModel when analyzing the\nphantom spectra acquired under two types of B0 inhomogeneity. Results:The\nmodeled spectra exhibited broadened and narrowed spectral peaks depending on\nthe B0 inhomogeneity and were quantitatively close to the measured spectra. The\nanalysis model trained using measured spectra with modeled spectra improved\nMSEs by 49.89% compared to that trained using measured spectra alone, and by\n26.66% compared to that trained using measured spectra with simulated spectra.\nThe performance improved as the number of modeled spectra increased from 0 to\n1,000. This model showed significantly lower MAPEs than LCModel under both\ntypes of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep\nlearning model using the modeled spectra was developed. The results suggest\nthat the proposed method has the potential to improve the accuracy of spectral\nanalysis by increasing the training samples of spectra.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20897v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.20897v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "开发了一种对静磁场不均匀性具有鲁棒性的MR光谱分析方法", "tldr": "开发了一种新的深度学习MR光谱分析方法，通过使用建模光谱来对抗B0不均匀性，提高了分析精度。", "motivation": "在静磁场B0不均匀性存在的情况下，提高光谱分析的准确性。", "method": "提出了一种利用深度学习模型进行光谱分析的新方法。该模型通过健康人脑的B0图和代谢物比率生成的建模光谱进行训练，这些建模光谱能一致地表示B0不均匀性引起的光谱变化。B0图被划分为子区域，并对单独估计的代谢物和基线成分进行平均和整合。分析模型使用测量、模拟和建模光谱进行训练，并通过代谢物比率的均方误差（MSE）和平均绝对百分比误差（MAPE）评估其性能，并与LCModel进行比较。", "result": "建模光谱根据B0不均匀性表现出加宽和变窄的光谱峰，且在定量上接近测量光谱。与仅使用测量光谱训练的模型相比，结合建模光谱训练的分析模型将MSEs提高了49.89%；与结合模拟光谱训练的模型相比，MSEs提高了26.66%。性能随建模光谱数量的增加而提高。该模型在两种B0不均匀性下均显示出比LCModel显著更低的MAPE。", "conclusion": "开发了一种利用建模光谱训练的新的光谱分析深度学习模型。结果表明，所提出的方法通过增加光谱训练样本，有潜力提高光谱分析的准确性。", "translation": "目的：开发一种在静磁场B0不均匀性存在的情况下提高光谱分析准确性的方法。\n方法：作者提出了一种新的光谱分析方法，该方法利用深度学习模型，该模型通过对建模光谱进行训练，这些建模光谱一致地表示由B0不均匀性引起的光谱变化。这些建模光谱是根据健康人脑的B0图和代谢物比率生成的。B0图被划分为补丁大小的子区域，并对单独估计的代谢物和基线成分进行平均然后整合。建模光谱的质量通过视觉和定量方式与测量光谱进行评估。分析模型使用测量光谱、模拟光谱和建模光谱进行训练。所提出方法的性能通过代谢物比率的均方误差（MSE）进行评估。在分析两种B0不均匀性下获得的体模光谱时，还将代谢物比率的平均绝对百分比误差（MAPE）与LCModel进行了比较。\n结果：建模光谱根据B0不均匀性表现出加宽和变窄的光谱峰，并且在定量上接近测量光谱。与仅使用测量光谱训练的模型相比，使用测量光谱和建模光谱训练的分析模型将MSEs提高了49.89%，与使用测量光谱和模拟光谱训练的模型相比，提高了26.66%。性能随着建模光谱数量从0增加到1000而提高。该模型在两种B0不均匀性下均显示出比LCModel显著更低的MAPE。\n结论：开发了一种利用建模光谱训练的新的光谱分析深度学习模型。结果表明，所提出的方法通过增加光谱训练样本，有潜力提高光谱分析的准确性。", "summary": "本文开发了一种新颖的基于深度学习的MR光谱分析方法，旨在提高静磁场B0不均匀性下的分析精度。该方法利用B0图和代谢物比率生成的建模光谱来训练深度学习模型。评估结果表明，与传统训练方法相比，结合建模光谱显著提高了性能（例如，MSE降低49.89%），并且优于LCModel，这表明通过扩展训练数据，该方法有潜力实现更准确的光谱分析。", "keywords": "MR光谱分析, 深度学习, B0不均匀性, 建模光谱, 代谢物定量", "comments": "本文提出了一种创新方法，通过合成逼真的“建模光谱”来增强深度学习的训练数据，从而提高MR光谱分析的准确性。这解决了MR波谱（MRS）中常见的B0不均匀性挑战，通过有效地扩展训练数据集，并特别考虑了这些变化。与LCModel等现有方法相比，其显著的性能提升突出了其在临床和研究应用中的实际重要性，在这些应用中，稳健准确的代谢物定量至关重要。"}}
{"id": "2506.20949", "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation", "authors": ["Chenkai Sun", "Denghui Zhang", "ChengXiang Zhai", "Heng Ji"], "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20949v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.20949v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "超越反应性安全：通过长周期模拟实现风险感知型LLM对齐", "tldr": "提出一个框架和数据集，通过长周期模拟评估LLM的风险感知对齐，并在新旧安全基准上取得显著提升。", "motivation": "鉴于语言模型代理对高风险社会决策日益增长的影响力，确保其有益影响需要理解其建议的深远影响。当前的安全方法可能过于反应性，未能预测长期、间接的危害。", "method": "提出一个概念验证框架，该框架能够宏观地模拟模型生成的建议如何在社会系统中随时间传播，从而实现更鲁棒的对齐。引入了一个包含100个间接危害场景的数据集，用于测试模型预测看似无害用户提示可能导致的负面、不明显结果的能力。", "result": "在新数据集上取得了超过20%的改进，并且在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）上对强基线模型取得了超过70%的平均胜率。", "conclusion": "该方法为开发更安全的代理提供了一个有前景的方向。", "translation": "鉴于基于语言模型的代理对从公共政策到医疗保健等高风险社会决策日益增长的影响力，确保其有益影响需要理解其建议的深远影响。我们提出了一个概念验证框架，该框架能够宏观地模拟模型生成的建议如何随时间在社会系统中传播，从而实现更鲁棒的对齐。为了评估语言模型的长期安全意识，我们还引入了一个包含100个间接危害场景的数据集，用于测试模型预测看似无害的用户提示可能导致的负面、不明显结果的能力。我们的方法不仅在新数据集上取得了超过20%的改进，而且在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）上对强基线模型取得了超过70%的平均胜率，这为开发更安全的代理提供了一个有前景的方向。", "summary": "该论文提出了一个超越传统反应性安全的风险感知型LLM对齐框架。通过宏观模拟模型建议在社会系统中的长期传播，并引入一个包含100个间接危害场景的新数据集，以评估LLM预测非显而易见负面结果的能力。实验结果表明，该方法在新数据集上性能提升超过20%，并在现有安全基准上表现优于强基线，为构建更安全的AI代理提供了新途径。", "keywords": "LLM对齐, 风险感知, 长周期模拟, 间接危害, 安全性", "comments": "该论文的创新点在于从传统的反应性安全转向更具前瞻性的风险感知对齐，通过引入“长周期模拟”和“间接危害场景数据集”来评估和提升LLM的长期安全意识。这对于LLM在高风险社会决策中的应用至关重要，有助于预防潜在的、非显而易见的负面影响。其提出的框架和数据集为未来LLM安全研究提供了有价值的工具和方向。"}}
{"id": "2506.20946", "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models", "authors": ["Donggoo Kang", "Jangyeong Kim", "Dasol Jeong", "Junyoung Choi", "Jeonga Wi", "Hyunmin Lee", "Joonho Gwon", "Joonki Paik"], "summary": "Current texture synthesis methods, which generate textures from fixed\nviewpoints, suffer from inconsistencies due to the lack of global context and\ngeometric understanding. Meanwhile, recent advancements in video generation\nmodels have demonstrated remarkable success in achieving temporally consistent\nvideos. In this paper, we introduce VideoTex, a novel framework for seamless\ntexture synthesis that leverages video generation models to address both\nspatial and temporal inconsistencies in 3D textures. Our approach incorporates\ngeometry-aware conditions, enabling precise utilization of 3D mesh structures.\nAdditionally, we propose a structure-wise UV diffusion strategy, which enhances\nthe generation of occluded areas by preserving semantic information, resulting\nin smoother and more coherent textures. VideoTex not only achieves smoother\ntransitions across UV boundaries but also ensures high-quality, temporally\nstable textures across video frames. Extensive experiments demonstrate that\nVideoTex outperforms existing methods in texture fidelity, seam blending, and\nstability, paving the way for dynamic real-time applications that demand both\nvisual quality and temporal coherence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20946v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.7; I.4.10; I.2.10"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.20946v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用几何感知扩散和时间视频模型实现一致的零样本3D纹理合成", "tldr": "VideoTex是一个利用视频生成模型的新框架，通过几何感知条件和结构化UV扩散策略，解决了现有3D纹理合成中存在的空间和时间不一致问题，实现了高质量、时间稳定的纹理。", "motivation": "当前的纹理合成方法由于缺乏全局上下文和几何理解，导致生成纹理不一致。而视频生成模型在实现时间上一致的视频方面取得了显著成功，这启发了本文利用视频生成模型来解决3D纹理的空间和时间不一致性。", "method": "本文引入了VideoTex框架，该框架利用视频生成模型来解决3D纹理的空间和时间不一致问题。它结合了几何感知条件，以精确利用3D网格结构，并提出了一种结构化UV扩散策略，通过保留语义信息来增强遮挡区域的生成。", "result": "VideoTex不仅在UV边界处实现了更平滑的过渡，而且确保了视频帧之间高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法。", "conclusion": "VideoTex为需要视觉质量和时间连贯性的动态实时应用铺平了道路。", "translation": "当前从固定视角生成纹理的合成方法，由于缺乏全局上下文和几何理解，存在不一致性问题。与此同时，视频生成模型在实现时间上一致的视频方面取得了显著进展。在本文中，我们引入了VideoTex，这是一个用于无缝纹理合成的新颖框架，它利用视频生成模型来解决3D纹理中的空间和时间不一致性。我们的方法结合了几何感知条件，能够精确利用3D网格结构。此外，我们提出了一种结构化UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而产生更平滑、更连贯的纹理。VideoTex不仅实现了UV边界之间更平滑的过渡，而且确保了视频帧之间高质量、时间稳定的纹理。大量的实验表明，VideoTex在纹理保真度、接缝融合和稳定性方面优于现有方法，为需要视觉质量和时间连贯性的动态实时应用铺平了道路。", "summary": "VideoTex是一个新颖的3D纹理合成框架，它创新性地利用视频生成模型来解决现有方法中存在的空间和时间不一致问题。通过结合几何感知条件和结构化UV扩散策略，VideoTex能够生成高质量、时间稳定的纹理，尤其是在UV边界和遮挡区域表现出色，并在实验中展现出优于现有方法的纹理保真度、接缝融合和稳定性。", "keywords": "零样本3D纹理合成, 几何感知扩散, 时间视频模型, VideoTex, 纹理一致性", "comments": "该论文的创新点在于将视频生成模型引入3D纹理合成领域，有效解决了传统方法中存在的空间和时间不一致性问题。通过引入几何感知条件和结构化UV扩散策略，提升了纹理的连贯性和质量，尤其对UV边界和遮挡区域的处理表现突出，为未来的实时动态应用提供了重要基础。"}}
{"id": "2506.21512", "title": "Assessing an evolutionary search engine for small language models, prompts, and evaluation metrics", "authors": ["Cláudio Lúcio do Val Lopes", "Lucca Machado"], "summary": "The concurrent optimization of language models and instructional prompts\npresents a significant challenge for deploying efficient and effective AI\nsystems, particularly when balancing performance against computational costs\nlike token usage. This paper introduces and assesses a bi-objective\nevolutionary search engine designed to navigate this complex space, focusing\nspecifically on Small Language Models (SLMs). We employ the NSGA-II algorithm\nand prompt grammar to simultaneously optimize for task accuracy and token\nefficiency across some reasoning tasks. Our results successfully identify\ndiverse, high-performing model-prompt combinations, quantitatively revealing\nthe critical trade-off between the two objectives. This research highlights\ntask-specific affinities between particular SLMs and prompt structures (e.g.,\ninstructions, context, chain of thought). The generated practical Pareto fronts\noffer decision-makers a portfolio of optimized solutions adaptable to their\nspecific constraints. This automated approach moves beyond traditional manual\ntuning, providing a foundational framework for discovering effective human-AI\ninteraction patterns.", "comment": "14 pages, 1 figure, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.21512v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.21512v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "评估一种用于小型语言模型、提示和评估指标的进化搜索引擎", "tldr": "本文介绍并评估了一个双目标进化搜索引擎，用于同时优化小型语言模型（SLM）和提示，以平衡任务准确性和token效率。", "motivation": "并发优化语言模型和指令提示对于部署高效且有效的AI系统，尤其是在平衡性能与计算成本（如token使用）时，是一个重大挑战。", "method": "本文引入并评估了一个双目标进化搜索引擎，专门针对小型语言模型（SLMs）。采用NSGA-II算法和提示语法，同时优化推理任务中的任务准确性和token效率。", "result": "结果成功识别出多样化、高性能的模型-提示组合，定量揭示了两个目标之间的关键权衡。研究强调了特定SLM与提示结构（如指令、上下文、思维链）之间的任务特定亲和性。生成的实用帕累托前沿为决策者提供了一系列可适应其特定约束的优化解决方案。", "conclusion": "这种自动化方法超越了传统的手动调优，为发现有效的人机交互模式提供了基础框架。", "translation": "语言模型和指令提示的并发优化对于部署高效且有效的AI系统提出了重大挑战，尤其是在平衡性能与计算成本（如token使用）时。本文介绍并评估了一种旨在解决这一复杂空间的双目标进化搜索引擎，特别关注小型语言模型（SLMs）。我们采用NSGA-II算法和提示语法，在一些推理任务中同时优化任务准确性和token效率。我们的结果成功识别出多样化、高性能的模型-提示组合，定量揭示了两个目标之间的关键权衡。这项研究强调了特定SLM与提示结构（例如指令、上下文、思维链）之间的任务特定亲和性。生成的实用帕累托前沿为决策者提供了一系列可适应其特定约束的优化解决方案。这种自动化方法超越了传统的手动调优，为发现有效的人机交互模式提供了基础框架。", "summary": "本文介绍并评估了一个双目标进化搜索引擎，该引擎利用NSGA-II算法和提示语法，旨在同时优化小型语言模型（SLMs）和指令提示，以平衡任务准确性和token效率。研究成功发现了高性能的模型-提示组合，并揭示了性能与成本之间的权衡关系，为自动化优化人机交互模式提供了新方法。", "keywords": "进化搜索, 小型语言模型, 提示优化, NSGA-II, token效率", "comments": "该研究通过引入进化搜索来自动化SLM和提示的优化，超越了传统的手动调优，具有创新性。它解决了AI系统部署中的实际挑战，即如何在性能和计算成本之间取得平衡，为决策者提供了实用的优化方案。其发现的帕累托前沿尤其有价值。"}}
{"id": "2506.20828", "title": "Practical and Accurate Local Edge Differentially Private Graph Algorithms", "authors": ["Pranay Mundra", "Charalampos Papamanthou", "Julian Shun", "Quanquan C. Liu"], "summary": "The rise of massive networks across diverse domains necessitates\nsophisticated graph analytics, often involving sensitive data and raising\nprivacy concerns. This paper addresses these challenges using local\ndifferential privacy (LDP), which enforces privacy at the individual level,\nwhere no third-party entity is trusted, unlike centralized models that assume a\ntrusted curator. We introduce novel LDP algorithms for two fundamental graph\nstatistics: k-core decomposition and triangle counting. Our approach leverages\ninput-dependent private graph properties, specifically the degeneracy and\nmaximum degree of the graph, to improve theoretical utility. Unlike prior\nmethods, our error bounds are determined by the maximum degree rather than the\ntotal number of edges, resulting in significantly tighter guarantees. For\ntriangle counting, we improve upon the work of Imola, Murakami, and\nChaudhury~\\cite{IMC21locally, IMC21communication}, which bounds error in terms\nof edge count. Instead, our algorithm achieves bounds based on graph degeneracy\nby leveraging a private out-degree orientation, a refined variant of Eden et\nal.'s randomized response technique~\\cite{ELRS23, and a novel analysis,\nyielding stronger guarantees than prior work. Beyond theoretical gains, we are\nthe first to evaluate local DP algorithms in a distributed simulation, unlike\nprior work tested on a single processor. Experiments on real-world graphs show\nsubstantial accuracy gains: our k-core decomposition achieves errors within 3x\nof exact values, far outperforming the 131x error in the baseline of Dhulipala\net al.~\\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative\napproximation errors by up to six orders of magnitude, while maintaining\ncompetitive runtime.", "comment": "To appear in VLDB 2025", "pdf_url": "http://arxiv.org/pdf/2506.20828v1", "categories": ["cs.DS", "cs.CR", "cs.DB"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20828v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "实用且准确的局部边缘差分隐私图算法", "tldr": "该论文提出了新的局部差分隐私图算法，用于k核分解和三角形计数，通过利用图的简并度和最大度，显著提高了准确性并降低了误差，并在分布式模拟中进行了首次评估。", "motivation": "大规模网络中涉及敏感数据的复杂图分析面临隐私问题，需要一种不信任第三方实体的隐私保护方法，即局部差分隐私（LDP）。", "method": "本研究提出了新的局部差分隐私（LDP）算法，用于k核分解和三角形计数。该方法利用输入依赖的私有图属性，特别是图的简并度和最大度，以提高理论效用。对于三角形计数，算法通过利用私有出度定向、Eden et al.随机响应技术的改进变体以及新颖的分析，实现了基于图简并度的误差界限。此外，该工作首次在分布式模拟中评估了局部DP算法。", "result": "k核分解算法的误差在精确值的3倍以内，远优于基线方法的131倍误差。三角形计数算法将乘法近似误差减少了多达六个数量级，同时保持了有竞争力的运行时间。误差界限由最大度而非总边数决定，从而获得更严格的保证。", "conclusion": "该论文提出了实用且准确的局部差分隐私图算法，显著提高了k核分解和三角形计数的准确性，并在分布式模拟中验证了其有效性，为大规模网络中的隐私保护图分析提供了更强的理论和实践保障。", "translation": "大规模网络在不同领域的兴起，需要复杂的图分析，这通常涉及敏感数据并引发隐私问题。本文利用局部差分隐私（LDP）解决了这些挑战，LDP在个体层面强制执行隐私，不信任任何第三方实体，这与假设存在可信策展人的中心化模型不同。我们为两个基本图统计量引入了新颖的LDP算法：k核分解和三角形计数。我们的方法利用输入依赖的私有图属性，特别是图的简并度和最大度，以提高理论效用。与现有方法不同，我们的误差界限由最大度而非总边数决定，从而获得显著更严格的保证。对于三角形计数，我们改进了Imola、Murakami和Chaudhury的工作，该工作将误差限制在边数方面。相反，我们的算法通过利用私有出度定向、Eden et al.的随机响应技术的改进变体以及新颖的分析，实现了基于图简并度的界限，从而获得了比先前工作更强的保证。除了理论上的收益，我们是第一个在分布式模拟中评估局部DP算法的，而先前的研究是在单个处理器上进行的。在真实世界图上的实验显示出显著的准确性增益：我们的k核分解误差在精确值的3倍以内，远远优于Dhulipala et al.基线中131倍的误差。我们的三角形计数算法将乘法近似误差减少了多达六个数量级，同时保持了有竞争力的运行时间。", "summary": "该论文针对大规模网络中涉及敏感数据的图分析，提出了实用且准确的局部差分隐私（LDP）图算法。具体地，研究人员为k核分解和三角形计数设计了新颖的LDP算法，这些算法利用图的简并度和最大度来提高理论效用和收紧误差界限。与现有工作相比，其误差界限由最大度而非总边数决定，并且在三角形计数方面，通过新的分析和技术，误差界限基于图简并度。该研究首次在分布式模拟中评估了LDP算法，并在真实世界图上展示了显著的准确性提升，例如k核分解误差仅为基线的1/40左右，三角形计数误差减少了多达六个数量级。", "keywords": "局部差分隐私, 图算法, k核分解, 三角形计数, 分布式模拟", "comments": "该论文在局部差分隐私图算法领域取得了显著进展。其创新点在于利用图的内在属性（如简并度和最大度）来推导出更紧密的误差界限，这与以往依赖总边数的方法形成鲜明对比。此外，首次在分布式模拟中对LDP算法进行评估，增强了其在实际应用中的可信度和实用性。实验结果表明了其方法的卓越准确性，对于解决大规模网络中的隐私保护图分析问题具有重要意义。"}}
{"id": "2506.21386", "title": "Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings", "authors": ["Ghazal Al-Shwayyat", "Omer Nezih Gerek"], "summary": "Arabic dialect recognition presents a significant challenge in speech\ntechnology due to the linguistic diversity of Arabic and the scarcity of large\nannotated datasets, particularly for underrepresented dialects. This research\ninvestigates hybrid modeling strategies that integrate classical signal\nprocessing techniques with deep learning architectures to address this problem\nin low-resource scenarios. Two hybrid models were developed and evaluated: (1)\nMel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural\nNetwork (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with\na Recurrent Neural Network (RNN). The models were trained on a dialect-filtered\nsubset of the Common Voice Arabic dataset, with dialect labels assigned based\non speaker metadata. Experimental results demonstrate that the MFCC + CNN\narchitecture achieved superior performance, with an accuracy of 91.2% and\nstrong precision, recall, and F1-scores, significantly outperforming the\nWavelet + RNN configuration, which achieved an accuracy of 66.5%. These\nfindings highlight the effectiveness of leveraging spectral features with\nconvolutional models for Arabic dialect recognition, especially when working\nwith limited labeled data. The study also identifies limitations related to\ndataset size, potential regional overlaps in labeling, and model optimization,\nproviding a roadmap for future research. Recommendations for further\nimprovement include the adoption of larger annotated corpora, integration of\nself-supervised learning techniques, and exploration of advanced neural\narchitectures such as Transformers. Overall, this research establishes a strong\nbaseline for future developments in Arabic dialect recognition within\nresource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21386v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21386v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "混合深度学习与信号处理在低资源环境下的阿拉伯语方言识别", "tldr": "本研究结合深度学习与信号处理技术，解决了低资源环境下阿拉伯语方言识别的挑战，其中MFCC+CNN模型表现最优，准确率达91.2%。", "motivation": "阿拉伯语方言识别面临挑战，因为语言多样性以及缺乏大型标注数据集，尤其对于代表性不足的方言。", "method": "开发并评估了两种混合模型：1) 梅尔频率倒谱系数 (MFCC) 结合卷积神经网络 (CNN)；2) 离散小波变换 (DWT) 特征结合循环神经网络 (RNN)。模型在Common Voice阿拉伯语数据集的方言过滤子集上进行训练。", "result": "MFCC+CNN架构表现优越，准确率达91.2%，显著优于小波+RNN配置（准确率66.5%）。结果表明光谱特征与卷积模型在有限标注数据下对阿拉伯语方言识别有效。", "conclusion": "研究表明结合光谱特征的卷积模型在阿拉伯语方言识别中有效，尤其是在资源受限的环境下。本研究为该领域未来的发展建立了强大的基线，并提出了未来研究方向。", "translation": "阿拉伯语方言识别在语音技术中面临重大挑战，原因在于阿拉伯语的语言多样性以及缺乏大型标注数据集，特别是对于代表性不足的方言。本研究探讨了混合建模策略，将经典信号处理技术与深度学习架构相结合，以解决低资源场景下的这一问题。开发并评估了两种混合模型：(1) 梅尔频率倒谱系数 (MFCC) 结合卷积神经网络 (CNN)，以及 (2) 离散小波变换 (DWT) 特征结合循环神经网络 (RNN)。这些模型在Common Voice阿拉伯语数据集的方言过滤子集上进行训练，方言标签基于说话人元数据分配。实验结果表明，MFCC + CNN 架构取得了卓越的性能，准确率达到 91.2%，并具有强大的精确度、召回率和 F1 分数，显著优于小波 + RNN 配置（准确率 66.5%）。这些发现突出了利用光谱特征与卷积模型进行阿拉伯语方言识别的有效性，尤其是在处理有限标注数据时。该研究还指出了与数据集大小、潜在的区域标签重叠和模型优化相关的局限性，为未来的研究提供了路线图。进一步改进的建议包括采用更大的标注语料库、整合自监督学习技术以及探索先进的神经网络架构，如 Transformers。总的来说，这项研究为资源受限环境下阿拉伯语方言识别的未来发展奠定了坚实的基础。", "summary": "本研究针对低资源环境下阿拉伯语方言识别的挑战，提出并评估了两种混合深度学习与信号处理模型。其中，结合梅尔频率倒谱系数(MFCC)和卷积神经网络(CNN)的模型表现最佳，准确率达91.2%，显著优于离散小波变换(DWT)与循环神经网络(RNN)的组合。研究结果强调了利用光谱特征与卷积模型在有限标注数据下进行阿拉伯语方言识别的有效性，并为未来研究提出了方向，包括扩充数据集和探索更先进的模型架构。", "keywords": "阿拉伯语方言识别, 深度学习, 信号处理, 低资源, MFCC-CNN", "comments": "本研究的创新点在于结合了传统的信号处理技术（如MFCC）与现代深度学习架构（CNN），有效解决了低资源环境下阿拉伯语方言识别的难题。其重要性在于为该领域提供了强大的基线，并明确指出了未来研究的方向，如数据集扩充和自监督学习，对于促进阿拉伯语语音技术的发展具有积极意义。"}}
{"id": "2506.20963", "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora", "authors": ["Fangyuan Zhang", "Zhengjun Huang", "Yingli Zhou", "Qintian Guo", "Zhixun Li", "Wensheng Luo", "Di Jiang", "Yixiang Fang", "Xiaofang Zhou"], "summary": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20963v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20963v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EraRAG：面向不断增长语料库的高效增量检索增强生成", "tldr": "EraRAG是一个新的多层图RAG框架，通过LSH实现高效、可扩展的动态更新，显著减少了更新时间和token消耗，同时保持了高准确性。", "motivation": "现有图检索增强生成（Graph-RAG）方法在语料库动态变化时，需要昂贵的完整图重建，限制了其在动态环境中的可扩展性。", "method": "EraRAG是一个新颖的多层图检索增强生成（Graph-RAG）框架，它利用基于超平面的局部敏感哈希（LSH）将原始语料库分区并组织成层次图结构，从而实现新数据的高效局部插入，无需重新训练或昂贵的重新计算。", "result": "EraRAG在更新时间和token消耗方面比现有Graph-RAG系统减少了高达一个数量级，同时提供了卓越的准确性性能。", "conclusion": "EraRAG为必须在持续增长语料库上运行的检索增强生成（RAG）系统提供了一条实用的前进道路，弥合了检索效率和适应性之间的差距。", "translation": "图检索增强生成（Graph-RAG）通过对外部语料库进行结构化检索来增强大型语言模型（LLM）。然而，现有方法通常假设语料库是静态的，每当有新文档到来时都需要昂贵的完整图重建，这限制了它们在动态、不断演变环境中的可扩展性。为了解决这些限制，我们引入了EraRAG，这是一种新颖的多层图RAG框架，支持高效且可扩展的动态更新。我们的方法利用基于超平面的局部敏感哈希（LSH）将原始语料库分区并组织成层次图结构，从而在不破坏现有拓扑的情况下实现新数据的高效局部插入。该设计消除了重新训练或昂贵重新计算的需要，同时保持了高检索精度和低延迟。大规模基准测试的实验表明，与现有Graph-RAG系统相比，EraRAG在更新时间和token消耗方面实现了高达一个数量级的减少，同时提供了卓越的准确性性能。这项工作为必须在持续增长语料库上运行的RAG系统提供了一条实用的前进道路，弥合了检索效率和适应性之间的差距。我们的代码和数据可在 https://github.com/EverM0re/EraRAG-Official 获取。", "summary": "EraRAG是一个创新的多层图检索增强生成（Graph-RAG）框架，旨在解决现有Graph-RAG系统在动态语料库中面临的扩展性问题。它利用基于超平面的局部敏感哈希（LSH）构建分层图结构，实现了新数据的局部高效插入，避免了昂贵的完整图重建。实验证明，EraRAG显著减少了更新时间和token消耗，同时保持或提高了检索准确性，为持续增长语料库的RAG系统提供了实用方案。", "keywords": "Retrieval Augmented Generation, Graph-RAG, Dynamic Updates, Locality-Sensitive Hashing, Scalability", "comments": "EraRAG的创新之处在于其引入的多层图结构和基于LSH的局部更新机制，有效解决了传统Graph-RAG在动态语料库中存在的效率瓶颈。这对于需要处理实时或不断增长信息的大型语言模型应用具有重要意义，因为它提高了系统的实用性和可扩展性，降低了维护成本。"}}
{"id": "2506.20940", "title": "Two-dimensional greedy randomized Kaczmarz methods for solving large-scale linear systems", "authors": ["Tao Li", "Meng-Long Xiao", "Xin-Fang Zhang"], "summary": "In this paper, we consider a novel two-dimensional randomized Kaczmarz method\nand its improved version with simple random sampling, which chooses two active\nrows with probability proportional to the square of their cross-product-like\nconstant, for solving large-scale linear systems. From the greedy selection\nstrategy with grasping two larger entries of the residual vector at each\niteration, we then devise a two-dimensional greedy randomized Kaczmarz method.\nTo improve the above methods further, motivated by the semi-randomized Kaczmarz\nmethod and Chebyshev's law of large numbers, we propose a two-dimensional\nsemi-randomized Kaczmarz method and its modified version with simple random\nsampling, which is particularly advantageous for big data problems.\nTheoretically, we prove that the proposed methods converge to the unique\nleast-norm solution of the consistent linear systems. Numerical results on some\npractical applications illustrate the superiority of the proposed methods\ncompared with some existing ones in terms of computing time.", "comment": "arXiv admin note: text overlap with arXiv:2506.16106", "pdf_url": "http://arxiv.org/pdf/2506.20940v1", "categories": ["math.NA", "cs.NA", "65F10, 65F20, 94A08"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.20940v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "二维贪婪随机Kaczmarz方法求解大规模线性系统", "tldr": "本文提出并分析了多种二维随机Kaczmarz方法，用于高效求解大规模线性系统，并在理论上证明了收敛性，数值实验显示其优越性。", "motivation": "解决大规模线性系统的高效求解问题，并改进现有Kaczmarz方法的效率。", "method": "本文提出了一种新型二维随机Kaczmarz方法及其通过简单随机抽样改进的版本，该方法以与其叉积常数平方成比例的概率选择两个活动行。基于残差向量中两个较大项的贪婪选择策略，设计了二维贪婪随机Kaczmarz方法。受半随机Kaczmarz方法和切比雪夫大数定律的启发，提出了一种二维半随机Kaczmarz方法及其通过简单随机抽样修改的版本。", "result": "理论上证明了所提出的方法收敛到一致线性系统的唯一最小范数解。数值结果表明，在计算时间方面，所提出的方法优于一些现有方法。", "conclusion": "提出的二维随机Kaczmarz方法及其变体在求解大规模线性系统方面具有理论收敛性和数值上的优越性，尤其适用于大数据问题。", "translation": "在本文中，我们考虑了一种新颖的二维随机Kaczmarz方法及其通过简单随机抽样改进的版本，该方法以与其叉积常数平方成比例的概率选择两个活动行，用于求解大规模线性系统。从每次迭代中抓住残差向量中两个较大项的贪婪选择策略，我们随后设计了一种二维贪婪随机Kaczmarz方法。为了进一步改进上述方法，受半随机Kaczmarz方法和切比雪夫大数定律的启发，我们提出了一种二维半随机Kaczmarz方法及其通过简单随机抽样修改的版本，这对于大数据问题特别有利。理论上，我们证明了所提出的方法收敛到一致线性系统的唯一最小范数解。在一些实际应用上的数值结果表明，所提出的方法在计算时间方面优于一些现有方法。", "summary": "本文研究了多种新型二维随机Kaczmarz方法，包括基于简单随机抽样的改进版本、贪婪随机Kaczmarz方法以及受半随机Kaczmarz方法启发的半随机变体。这些方法旨在高效求解大规模线性系统，特别适用于大数据问题。研究从理论上证明了所提方法能收敛到一致线性系统的唯一最小范数解，并通过数值实验验证了其在计算时间上的优越性。", "keywords": "Kaczmarz方法, 随机算法, 大规模线性系统, 二维选择, 收敛性", "comments": "这篇论文通过引入二维选择策略和结合贪婪、半随机思想，对经典的Kaczmarz方法进行了创新性改进。其针对大规模线性系统和大数据问题的适用性，以及理论收敛性和数值效率的证明，显示了其在数值线性代数领域的潜在重要性。"}}
{"id": "2506.20821", "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "comment": "Preprint Copy", "pdf_url": "http://arxiv.org/pdf/2506.20821v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20821v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "MultiFinRAG：一个用于金融问答的优化多模态检索增强生成（RAG）框架", "tldr": "MultiFinRAG是一个专为金融问答设计的多模态RAG框架，通过多模态提取和分层回退策略，显著提高了在复杂金融文档上问答的准确性，超越了ChatGPT-4o。", "motivation": "金融文档（如10-K、10-Q、投资者演示文稿）篇幅巨大且包含文本、表格、图像等多种模态。对这些内容进行问答通常需要跨模态联合推理，而传统的大型语言模型（LLMs）和检索增强生成（RAG）管道因令牌限制、布局丢失和碎片化的跨模态上下文而面临挑战。", "method": "MultiFinRAG首先通过将表格和图表图像分批发送给轻量级、量化的开源多模态LLM进行多模态提取，生成结构化JSON输出和简洁的文本摘要。这些输出与叙述性文本一起被嵌入并使用模态感知相似度阈值进行索引，以实现精确检索。然后，一个分层回退策略在必要时动态地从纯文本升级到文本+表格+图像上下文，从而实现跨模态推理并减少不相关的上下文。", "result": "MultiFinRAG在包含文本、表格、图像和组合多模态推理的复杂金融问答任务中，比ChatGPT-4o（免费版）的准确率高出19个百分点，并且可以在商用硬件上运行。", "conclusion": "MultiFinRAG成功地解决了金融问答中多模态推理的挑战，通过其优化的多模态检索增强生成框架，显著提高了问答准确性，并证明了在处理复杂金融文档方面的优越性。", "translation": "金融文档——例如10-K、10-Q和投资者演示文稿——篇幅达数百页，并结合了多种模态，包括密集的叙述性文本、结构化表格和复杂的图表。对这些内容进行问答通常需要跨模态联合推理，这由于令牌限制、布局丢失和碎片化的跨模态上下文而对传统的大型语言模型（LLMs）和检索增强生成（RAG）管道造成压力。我们引入了MultiFinRAG，一个专为金融问答设计的检索增强生成框架。MultiFinRAG首先通过将表格和图表图像分批发送给一个轻量级、量化的开源多模态LLM来执行多模态提取，该模型既能生成结构化JSON输出，也能生成简洁的文本摘要。这些输出与叙述性文本一起被嵌入并使用模态感知相似度阈值进行索引，以实现精确检索。然后，一个分层回退策略在必要时动态地从纯文本升级到文本+表格+图像上下文，从而实现跨模态推理并减少不相关的上下文。尽管在商用硬件上运行，MultiFinRAG在涉及文本、表格、图像和组合多模态推理的复杂金融问答任务中，比ChatGPT-4o（免费版）的准确率高出19个百分点。", "summary": "MultiFinRAG是一个为金融问答设计的RAG框架，旨在解决传统LLM和RAG在处理包含文本、表格和图像等多模态金融文档时面临的挑战。它通过多模态提取（利用轻量级多模态LLM处理图像和表格）、模态感知嵌入和索引，以及分层回退策略来整合跨模态信息。该框架在商用硬件上运行，并在复杂金融问答任务中表现出色，比ChatGPT-4o高出19个百分点的准确率。", "keywords": "金融问答, 多模态RAG, 检索增强生成, 跨模态推理, 金融文档", "comments": "MultiFinRAG的创新之处在于其专门针对金融领域多模态文档的RAG优化。它通过结合轻量级多模态LLM进行提取和智能的分层回退策略，有效解决了传统RAG在处理跨模态信息时的痛点。其在商用硬件上实现显著性能提升的特点，也凸显了其实用性和效率。"}}
{"id": "2506.21269", "title": "Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou", "authors": ["Pengfei Fan", "Yuli Zhang", "Xinheng Wang", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Shangbo Wang"], "summary": "This study presents and publicly releases the Suzhou Urban Road Acoustic\nDataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive\ndata-acquisition protocols and annotation guidelines to ensure transparency and\nreproducibility of the experimental workflow. To model the coupling between\nvehicular noise and driving speed, we propose a bimodal-feature-fusion deep\nconvolutional neural network (BMCNN). During preprocessing, an adaptive\ndenoising and normalization strategy is applied to suppress environmental\nbackground interference; in the network architecture, parallel branches extract\nMel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,\nwhich are subsequently fused via a cross-modal attention mechanism in the\nintermediate feature space to fully exploit time-frequency information.\nExperimental results demonstrate that BMCNN achieves a classification accuracy\nof 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic\ndataset. Ablation studies and robustness tests on the Suzhou dataset further\nvalidate the contributions of each module to performance improvement and\noverfitting mitigation. The proposed acoustics-based speed classification\nmethod can be integrated into smart-city traffic management systems for\nreal-time noise monitoring and speed estimation, thereby optimizing traffic\nflow control, reducing roadside noise pollution, and supporting sustainable\nurban planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21269v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21269v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "整合车辆声学数据以增强城市交通管理：以苏州车速分类为例", "tldr": "本研究发布了苏州城市道路声学数据集，并提出了一种双模态特征融合深度卷积神经网络（BMCNN），用于通过车辆噪声对行驶速度进行分类，在多个数据集上取得了高精度，可应用于智慧城市交通管理。", "motivation": "为了建模车辆噪声与行驶速度之间的耦合关系，并实现实时噪声监测、速度估计，从而优化交通流量控制、减少路边噪声污染并支持可持续城市规划。", "method": "本研究发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了双模态特征融合深度卷积神经网络（BMCNN）。预处理阶段采用自适应降噪和归一化策略；网络架构中，并行分支提取梅尔频率倒谱系数（MFCCs）和小波包能量特征，并通过跨模态注意力机制在中间特征空间融合，以充分利用时频信息。", "result": "BMCNN在SZUR-Acoustic Dataset上实现了87.56%的分类准确率，在公共IDMT-Traffic数据集上达到了96.28%。消融研究和鲁棒性测试进一步验证了各模块对性能提升和过拟合缓解的贡献。", "conclusion": "所提出的基于声学的速度分类方法可以集成到智慧城市交通管理系统中，用于实时噪声监测和速度估计，从而优化交通流量控制、减少路边噪声污染并支持可持续城市规划。", "translation": "本研究介绍并公开发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset），该数据集附带全面的数据采集协议和标注指南，以确保实验工作流程的透明度和可复现性。为了建模车辆噪声与驾驶速度之间的耦合关系，我们提出了一种双模态特征融合深度卷积神经网络（BMCNN）。在预处理过程中，采用自适应降噪和归一化策略来抑制环境背景干扰；在网络架构中，并行分支提取梅尔频率倒谱系数（MFCCs）和小波包能量特征，随后通过中间特征空间中的跨模态注意力机制进行融合，以充分利用时频信息。实验结果表明，BMCNN在SZUR-Acoustic Dataset上实现了87.56%的分类准确率，在公共IDMT-Traffic数据集上达到了96.28%。对苏州数据集进行的消融研究和鲁棒性测试进一步验证了每个模块对性能改进和缓解过拟合的贡献。所提出的基于声学的速度分类方法可以集成到智慧城市交通管理系统中，用于实时噪声监测和速度估计，从而优化交通流量控制、减少路边噪声污染并支持可持续城市规划。", "summary": "本研究发布了苏州城市道路声学数据集（SZUR-Acoustic Dataset）及其采集协议和标注指南，旨在利用车辆声学数据进行车速分类。为此，提出了一种双模态特征融合深度卷积神经网络（BMCNN），该网络通过自适应降噪预处理，并并行提取MFCCs和小波包能量特征，然后通过跨模态注意力机制进行融合。实验证明，BMCNN在SZUR-Acoustic Dataset和IDMT-Traffic数据集上均表现出高精度，并通过消融实验验证了其鲁棒性。该方法可应用于智慧城市交通管理，实现实时噪声监测和速度估计，从而优化交通流、减少噪声污染并支持城市规划。", "keywords": "车辆声学数据, 交通管理, 速度分类, 深度学习, 苏州数据集", "comments": "该论文的创新点在于发布了一个新的城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种新颖的BMCNN模型，结合了MFCCs和小波包能量特征，并通过跨模态注意力机制进行融合，有效提升了车速分类的准确性。其重要性体现在为智慧城市交通管理提供了一种基于声学数据的实时监测和优化方案，具有潜在的实际应用价值。"}}
{"id": "2506.20981", "title": "PrivacyGo: Privacy-Preserving Ad Measurement with Multidimensional Intersection", "authors": ["Jian Du", "Haohao Qian", "Shikun Zhang", "Wen-jie Lu", "Donghang Lu", "Yongchuan Niu", "Bo Jiang", "Yongjun Zhao", "Qiang Yan"], "summary": "This paper tackles the challenging and practical problem of multi-identifier\nprivate user profile matching for privacy-preserving ad measurement, a\ncornerstone of modern advertising analytics. We introduce a comprehensive\ncryptographic framework leveraging reversed Oblivious Pseudorandom Functions\n(OPRF) and novel blind key rotation techniques to support secure matching\nacross multiple identifiers. Our design prevents cross-identifier linkages and\nincludes a differentially private mechanism to obfuscate intersection sizes,\nmitigating risks such as membership inference attacks.\n  We present a concrete construction of our protocol that achieves both strong\nprivacy guarantees and high efficiency. It scales to large datasets, offering a\npractical and scalable solution for privacy-centric applications like secure ad\nconversion tracking. By combining rigorous cryptographic principles with\ndifferential privacy, our work addresses a critical need in the advertising\nindustry, setting a new standard for privacy-preserving ad measurement\nframeworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20981v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.20981v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PrivacyGo：基于多维交集的隐私保护广告测量", "tldr": "本文提出了PrivacyGo，一个利用反向不经意伪随机函数（OPRF）和盲密钥轮换技术，并结合差分隐私机制，实现高效且强隐私保护的多标识符广告测量框架。", "motivation": "解决多标识符私有用户画像匹配的挑战性实际问题，以实现隐私保护的广告测量，这是现代广告分析的基石。", "method": "引入了一个综合的加密框架，利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术来支持跨多个标识符的安全匹配。设计中防止了跨标识符链接，并包含一个差分隐私机制来混淆交集大小，以减轻成员推断攻击等风险。", "result": "所提出的协议实现了强大的隐私保证和高效率。它能够扩展到大型数据集，为安全广告转化跟踪等以隐私为中心的应用提供了实用且可扩展的解决方案。", "conclusion": "通过将严格的密码学原理与差分隐私相结合，我们的工作解决了广告行业的一个关键需求，为隐私保护广告测量框架树立了新标准。", "translation": "本文解决了隐私保护广告测量中多标识符私有用户画像匹配这一具有挑战性和实用性的问题，这是现代广告分析的基石。我们引入了一个综合的加密框架，利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术来支持跨多个标识符的安全匹配。我们的设计防止了跨标识符链接，并包含一个差分隐私机制来混淆交集大小，以减轻成员推断攻击等风险。\n我们提出了一个具体的协议构建，它实现了强大的隐私保证和高效率。它能够扩展到大型数据集，为安全广告转化跟踪等以隐私为中心的应用提供了实用且可扩展的解决方案。通过将严格的密码学原理与差分隐私相结合，我们的工作解决了广告行业的一个关键需求，为隐私保护广告测量框架树立了新标准。", "summary": "本文介绍了PrivacyGo，一个用于隐私保护广告测量的综合加密框架。它利用反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术实现多标识符安全匹配，并通过差分隐私机制混淆交集大小，有效防止跨标识符链接和成员推断攻击。该协议高效且可扩展，为大规模数据集提供了实用的解决方案，满足了广告行业对隐私保护的关键需求，并为未来框架设定了新标准。", "keywords": "隐私保护广告测量, 不经意伪随机函数, 差分隐私, 多标识符匹配", "comments": "本文的创新之处在于其将反向不经意伪随机函数（OPRF）和新颖的盲密钥轮换技术与差分隐私机制巧妙结合，以解决多标识符用户画像匹配中的隐私保护问题。其重要性在于为广告行业提供了急需的、兼具强大隐私保护和高效率的解决方案，尤其是在大规模数据场景下，为隐私保护广告测量树立了新标准。"}}
{"id": "2506.20966", "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends", "authors": ["Tian-Yu Xiang", "Ao-Qun Jin", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Sheng-Bin Duan", "Fu-Chao Xie", "Wen-Kai Wang", "Si-Cheng Wang", "Ling-Yun Li", "Tian Tu", "Zeng-Guang Hou"], "summary": "Vision-language-action (VLA) models extend vision-language models (VLM) by\nintegrating action generation modules for robotic manipulation. Leveraging\nstrengths of VLM in vision perception and instruction understanding, VLA models\nexhibit promising generalization across diverse manipulation tasks. However,\napplications demanding high precision and accuracy reveal performance gaps\nwithout further adaptation. Evidence from multiple domains highlights the\ncritical role of post-training to align foundational models with downstream\napplications, spurring extensive research on post-training VLA models. VLA\nmodel post-training aims to address the challenge of improving an embodiment's\nability to interact with the environment for the given tasks, analogous to the\nprocess of humans motor skills acquisition. Accordingly, this paper reviews\npost-training strategies for VLA models through the lens of human motor\nlearning, focusing on three dimensions: environments, embodiments, and tasks. A\nstructured taxonomy is introduced aligned with human learning mechanisms: (1)\nenhancing environmental perception, (2) improving embodiment awareness, (3)\ndeepening task comprehension, and (4) multi-component integration. Finally, key\nchallenges and trends in post-training VLA models are identified, establishing\na conceptual framework to guide future research. This work delivers both a\ncomprehensive overview of current VLA model post-training methods from a human\nmotor learning perspective and practical insights for VLA model development.\n(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20966v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20966v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "VLA模型后训练与人类运动学习的并行性：进展、挑战与趋势", "tldr": "本文从人类运动学习的角度综述了VLA模型后训练策略，并提出了一个分类法，以指导未来研究。", "motivation": "VLA模型在复杂操作任务中表现出泛化能力，但在需要高精度和准确性的应用中存在性能差距，需要进一步的后训练来弥合这些差距并使基础模型与下游应用对齐。", "method": "本文从人类运动学习的角度回顾了VLA模型的后训练策略，重点关注环境、具身和任务三个维度。文章引入了一个与人类学习机制对齐的结构化分类法，包括：增强环境感知、提高具身意识、深化任务理解以及多组件集成。", "result": "论文提供了一个从人类运动学习角度对当前VLA模型后训练方法的全面概述，并为VLA模型开发提供了实用见解。同时，识别了VLA模型后训练中的关键挑战和趋势，并建立了一个概念框架来指导未来研究。", "conclusion": "本文通过将VLA模型后训练与人类运动学习进行类比，不仅全面概述了现有方法，还为VLA模型的发展提供了实践指导，并为未来的研究奠定了概念基础。", "translation": "视觉-语言-动作 (VLA) 模型通过集成机器人操作的动作生成模块，扩展了视觉-语言模型 (VLM)。VLA 模型利用 VLM 在视觉感知和指令理解方面的优势，在各种操作任务中展现出有前景的泛化能力。然而，需要高精度和准确性的应用在没有进一步适应的情况下暴露出性能差距。来自多个领域的证据强调了后训练在使基础模型与下游应用对齐方面的关键作用，这激发了对后训练 VLA 模型的广泛研究。VLA 模型后训练旨在解决如何提高具身与环境交互以完成给定任务的能力，这类似于人类运动技能习得的过程。因此，本文从人类运动学习的角度回顾了 VLA 模型的后训练策略，重点关注三个维度：环境、具身和任务。文章引入了一个与人类学习机制对齐的结构化分类法：(1) 增强环境感知，(2) 提高具身意识，(3) 深化任务理解，以及 (4) 多组件集成。最后，识别了后训练 VLA 模型的关键挑战和趋势，建立了一个概念框架以指导未来的研究。这项工作既提供了从人类运动学习角度对当前 VLA 模型后训练方法的全面概述，也为 VLA 模型开发提供了实用见解。", "summary": "本文综述了视觉-语言-动作（VLA）模型的后训练策略，旨在解决其在需要高精度任务中的性能差距。论文将VLA模型后训练比作人类运动学习过程，并从环境、具身和任务三个维度进行分析，提出了一个包含增强环境感知、提高具身意识、深化任务理解和多组件集成的结构化分类法。文章还识别了VLA模型后训练的关键挑战和趋势，并建立了一个概念框架，为未来的研究和VLA模型开发提供了全面的概述和实践见解。", "keywords": "VLA模型后训练, 人类运动学习, 机器人操作, 具身智能, 分类法", "comments": "本文的创新之处在于其独特地将VLA模型的后训练与人类运动学习过程进行类比，这提供了一个新颖且直观的视角来理解和分类VLA模型的适应性策略。这种跨学科的视角有助于系统地分析现有方法，并为未来研究指明了方向，特别是在具身智能体与真实世界交互的背景下。"}}
{"id": "2506.21537", "title": "ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers", "authors": ["Nicholas S. DiBrita", "Jason Han", "Tirthak Patel"], "summary": "Research in quantum machine learning has recently proliferated due to the\npotential of quantum computing to accelerate machine learning. An area of\nmachine learning that has not yet been explored is neural ordinary differential\nequation (neural ODE) based residual neural networks (ResNets), which aim to\nimprove the effectiveness of neural networks using the principles of ordinary\ndifferential equations. In this work, we present our insights about why analog\nRydberg atom quantum computers are especially well-suited for ResNets. We also\nintroduce ResQ, a novel framework to optimize the dynamics of Rydberg atom\nquantum computers to solve classification problems in machine learning using\nanalog quantum neural ODEs.", "comment": "ResQ will appear in the Proceedings of the IEEE International\n  Conference on Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2506.21537v1", "categories": ["quant-ph", "cs.CV", "cs.ET"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21537v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ResQ：在模拟里德堡原子量子计算机上实现残差神经网络的新颖框架", "tldr": "本文介绍了ResQ，一个新颖的框架，用于在模拟里德堡原子量子计算机上实现基于神经常微分方程（neural ODE）的残差神经网络（ResNets），以解决机器学习中的分类问题。", "motivation": "量子计算加速机器学习的潜力导致量子机器学习研究迅速发展。其中，基于神经常微分方程（neural ODE）的残差神经网络（ResNets）尚未在量子机器学习领域探索。", "method": "本文引入了ResQ框架，旨在优化里德堡原子量子计算机的动态，以使用模拟量子神经ODE解决机器学习中的分类问题。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "量子机器学习的研究最近由于量子计算加速机器学习的潜力而迅速发展。机器学习中尚未探索的一个领域是基于神经常微分方程（neural ODE）的残差神经网络（ResNets），它们旨在利用常微分方程的原理提高神经网络的有效性。在这项工作中，我们提出了关于为什么模拟里德堡原子量子计算机特别适合ResNets的见解。我们还介绍了ResQ，这是一个新颖的框架，用于优化里德堡原子量子计算机的动态，以使用模拟量子神经ODE解决机器学习中的分类问题。", "summary": "本文介绍了ResQ，一个新颖的框架，用于在模拟里德堡原子量子计算机上实现基于神经常微分方程（neural ODE）的残差神经网络（ResNets）。作者阐述了里德堡原子量子计算机特别适合ResNets的原因，并提出ResQ通过优化其动态，利用模拟量子神经ODE解决机器学习中的分类问题。", "keywords": "量子机器学习, 残差神经网络, 里德堡原子量子计算机, 神经ODE, 分类", "comments": "这项工作提出了一种将残差神经网络与量子计算相结合的新方法，特别是在模拟里德堡原子量子计算机上实现，这在量子机器学习领域是一个新颖的方向。其创新点在于将神经ODE的概念引入量子领域，并为特定硬件平台（里德堡原子量子计算机）提供了实现方案。"}}
{"id": "2506.20883", "title": "Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance", "authors": ["Kyanna Dagenais", "Istvan David"], "summary": "Model-driven engineering problems often require complex model transformations\n(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of\nsuch problems include model synchronization, automated model repair, and design\nspace exploration. Manually developing complex MTs is an error-prone and often\ninfeasible process. Reinforcement learning (RL) is an apt way to alleviate\nthese issues. In RL, an autonomous agent explores the state space through trial\nand error to identify beneficial sequences of actions, such as MTs. However, RL\nmethods exhibit performance issues in complex problems. In these situations,\nhuman guidance can be of high utility. In this paper, we present an approach\nand technical framework for developing complex MT sequences through RL, guided\nby potentially uncertain human advice. Our framework allows user-defined MTs to\nbe mapped onto RL primitives, and executes them as RL programs to find optimal\nMT sequences. Our evaluation shows that human guidance, even if uncertain,\nsubstantially improves RL performance, and results in more efficient\ndevelopment of complex MTs. Through a trade-off between the certainty and\ntimeliness of human advice, our method takes a step towards RL-driven\nhuman-in-the-loop engineering methods.", "comment": "Accepted for ACM/IEEE MODELS'25", "pdf_url": "http://arxiv.org/pdf/2506.20883v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.20883v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "不确定人类指导下强化学习的复杂模型转换", "tldr": "本文提出了一种通过强化学习，并在不确定人类指导下开发复杂模型转换序列的方法，该方法显著提高了RL性能并提升了复杂MT的开发效率。", "motivation": "模型驱动工程中的复杂模型转换（MTs）手动开发容易出错且不可行。传统的强化学习（RL）方法在处理复杂问题时存在性能问题，而人类指导在此类情况下具有高实用性。", "method": "本文提出了一种开发复杂MT序列的方法和技术框架，该框架通过强化学习，并由可能不确定的人类建议指导。它允许将用户定义的MTs映射到RL原语，并将其作为RL程序执行以找到最佳MT序列。", "result": "评估结果显示，即使不确定，人类指导也能显著提高RL性能，并使得复杂MT的开发更加高效。", "conclusion": "通过在人类建议的确定性和及时性之间进行权衡，该方法朝着RL驱动的人机协作工程方法迈进了一步。", "translation": "模型驱动工程问题通常需要复杂的模型转换（MTs），即需要大量序列串联的MTs。这类问题的相关例子包括模型同步、自动化模型修复和设计空间探索。手动开发复杂的MTs是一个容易出错且通常不可行的过程。强化学习（RL）是缓解这些问题的合适方法。在RL中，自主智能体通过试错探索状态空间，以识别有益的动作序列，例如MTs。然而，RL方法在复杂问题中表现出性能问题。在这种情况下，人类指导具有很高的实用性。在本文中，我们提出了一种通过RL开发复杂MT序列的方法和技术框架，该框架由可能不确定的人类建议指导。我们的框架允许将用户定义的MTs映射到RL原语，并将其作为RL程序执行以找到最佳MT序列。我们的评估表明，人类指导，即使是不确定的，也能显著提高RL性能，并使得复杂MT的开发更加高效。通过在人类建议的确定性和及时性之间进行权衡，我们的方法朝着RL驱动的人机协作工程方法迈进了一步。", "summary": "本文提出了一种结合强化学习与不确定人类指导来开发复杂模型转换序列的方法和框架。针对手动开发复杂模型转换的困难以及传统强化学习在复杂问题上的性能瓶颈，该研究允许将用户定义的模型转换映射到强化学习原语以寻找最优序列。实验证明，即使是不确定的人类指导也能显著提升强化学习的性能，从而更高效地开发复杂的模型转换，推动了人机协作的强化学习工程方法。", "keywords": "强化学习, 模型转换, 人类指导, 模型驱动工程, 人机协作", "comments": "该论文的创新点在于将人类不确定性指导引入强化学习，以解决复杂模型转换的开发难题。它有效地结合了人类的领域知识和强化学习的探索能力，为解决实际工程问题提供了新的思路。其重要性在于提升了强化学习在复杂系统开发中的实用性，并为未来人机协作的智能工程方法奠定了基础。"}}
{"id": "2506.21201", "title": "Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices", "authors": ["Zihao You", "Michael Crabb"], "summary": "The consumption of subtitles via TVs, laptops and smartphones has the\npotential to marginalize people based on their complex accessibility needs. The\ncurrent one-size-fits-all approach to this accessibility aid is no longer fit\nfor purpose and work is required to look at how it can be adapted to be\npersonalised for individual users based on individual context, content, and\nconsumption habits. People with Aphasia, for example, encounter significant\nchallenges in understanding subtitle texts.\n  We see our work as a call to action for more inclusive practices, focusing on\nhow the thoughts and opinions of people with aphasia can be included in media\nresearch. Our work investigates how to develop future media solutions for\npeople with aphasia to create a more inclusive media viewing environment. We\nbelieve the key to this is appropriate prototyping tools and methods to allow\nequitable inclusion in the system design process.", "comment": "3 pages, 1 figure, Access InContext Workshop at CHI 2025 on 26th of\n  April", "pdf_url": "http://arxiv.org/pdf/2506.21201v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21201v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "失语症患者的字幕媒体适应：持续存在的无障碍障碍和新兴设计实践", "tldr": "本文呼吁为失语症患者开发个性化字幕媒体解决方案，以解决现有字幕的无障碍障碍，并强调通过适当的原型工具和方法实现包容性设计的重要性。", "motivation": "当前的字幕模式“一刀切”，无法满足具有复杂无障碍需求（例如失语症患者）的人群，导致他们在理解字幕文本方面遇到重大挑战并被边缘化。因此，需要研究如何根据个人情境、内容和消费习惯对字幕进行个性化调整。", "method": "本文将工作视为一项行动号召，旨在推动更具包容性的实践，重点关注如何将失语症患者的观点和意见纳入媒体研究。工作旨在探索如何为失语症患者开发未来的媒体解决方案，以创造一个更具包容性的媒体观看环境。实现这一目标的关键在于采用适当的原型工具和方法，以确保在系统设计过程中实现公平的包容性。", "result": "未在摘要中提及", "conclusion": "本文认为，开发适合失语症患者的未来媒体解决方案，并创造更具包容性的媒体观看环境的关键在于采用适当的原型工具和方法，以确保在系统设计过程中实现公平的包容性。", "translation": "通过电视、笔记本电脑和智能手机消费字幕有可能使那些具有复杂无障碍需求的人被边缘化。当前这种“一刀切”的无障碍辅助方式已不再适用，需要研究如何根据个人情境、内容和消费习惯对其进行个性化调整。例如，失语症患者在理解字幕文本方面遇到了重大挑战。\n我们将我们的工作视为一项行动号召，旨在推动更具包容性的实践，重点关注如何将失语症患者的观点和意见纳入媒体研究。我们的工作旨在探索如何为失语症患者开发未来的媒体解决方案，以创造一个更具包容性的媒体观看环境。我们相信实现这一目标的关键在于采用适当的原型工具和方法，以确保在系统设计过程中实现公平的包容性。", "summary": "本文指出，现有字幕的“一刀切”模式对失语症等具有复杂无障碍需求的人群造成了边缘化，他们难以理解字幕内容。作者呼吁采取行动，通过将失语症患者的意见纳入媒体研究和设计过程，开发个性化的媒体解决方案。文章强调，利用适当的原型工具和方法是实现系统设计中公平包容性的关键，旨在为失语症患者创造一个更具包容性的媒体观看环境。", "keywords": "失语症, 字幕, 无障碍, 个性化, 包容性设计", "comments": "本文的创新之处在于其明确关注失语症患者这一特定群体在字幕消费上面临的无障碍挑战，并提出需要从“一刀切”转向个性化适应。其重要性在于强调了用户中心设计和包容性实践，特别是将受影响人群（失语症患者）的观点纳入设计过程。文章的局限性在于，它更多地是一个研究方向和呼吁，而非一个已完成研究的结果报告，因此没有提供具体的方法细节或实验数据。"}}
{"id": "2506.20904", "title": "Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL", "authors": ["Matthew Zurek", "Guy Zamir", "Yudong Chen"], "summary": "We study offline reinforcement learning in average-reward MDPs, which\npresents increased challenges from the perspectives of distribution shift and\nnon-uniform coverage, and has been relatively underexamined from a theoretical\nperspective. While previous work obtains performance guarantees under\nsingle-policy data coverage assumptions, such guarantees utilize additional\ncomplexity measures which are uniform over all policies, such as the uniform\nmixing time. We develop sharp guarantees depending only on the target policy,\nspecifically the bias span and a novel policy hitting radius, yielding the\nfirst fully single-policy sample complexity bound for average-reward offline\nRL. We are also the first to handle general weakly communicating MDPs,\ncontrasting restrictive structural assumptions made in prior work. To achieve\nthis, we introduce an algorithm based on pessimistic discounted value iteration\nenhanced by a novel quantile clipping technique, which enables the use of a\nsharper empirical-span-based penalty function. Our algorithm also does not\nrequire any prior parameter knowledge for its implementation. Remarkably, we\nshow via hard examples that learning under our conditions requires coverage\nassumptions beyond the stationary distribution of the target policy,\ndistinguishing single-policy complexity measures from previously examined\ncases. We also develop lower bounds nearly matching our main result.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20904v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20904v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "平均奖励离线强化学习中的最优单策略样本复杂度和瞬态覆盖", "tldr": "本文研究了平均奖励离线强化学习，首次提出了完全单策略样本复杂度界限，并处理了一般弱通信MDPs。它引入了一种带有分位数裁剪的新算法，并表明学习需要超越平稳分布的覆盖。", "motivation": "平均奖励MDPs中的离线强化学习在理论上尚未被充分研究，面临分布偏移和非均匀覆盖的挑战。现有工作提供的性能保证依赖于对所有策略统一的额外复杂性度量（如统一混合时间），而非仅针对目标策略。", "method": "本文提出了一种基于悲观折扣值迭代的算法，通过新颖的分位数裁剪技术进行增强，从而能够使用更精确的基于经验跨度的惩罚函数。该算法的实现不需要任何先验参数知识。", "result": "本文开发了仅依赖于目标策略（偏差跨度和新颖的策略命中半径）的精确保证，首次为平均奖励离线RL带来了完全单策略样本复杂度界限。它也是第一个处理一般弱通信MDPs的工作。通过困难示例，作者表明在他们的条件下学习需要超越目标策略平稳分布的覆盖假设。此外，他们还开发了几乎与主要结果匹配的下限。", "conclusion": "本文为平均奖励离线强化学习提供了第一个单策略样本复杂度界限，解决了分布偏移和非均匀覆盖的挑战，并适用于一般弱通信MDPs。研究表明，单策略复杂性度量需要比以往认为的更强的覆盖假设。", "translation": "我们研究了平均奖励MDPs中的离线强化学习，这在分布偏移和非均匀覆盖方面带来了更大的挑战，并且从理论角度来看相对未被充分研究。虽然之前的工作在单策略数据覆盖假设下获得了性能保证，但这些保证利用了对所有策略统一的额外复杂性度量，例如统一混合时间。我们开发了仅依赖于目标策略的精确保证，特别是偏差跨度和一种新颖的策略命中半径，从而为平均奖励离线RL带来了第一个完全单策略样本复杂度界限。我们也是第一个处理一般弱通信MDPs的工作，这与之前工作中限制性结构假设形成对比。为了实现这一点，我们引入了一种基于悲观折扣值迭代的算法，并通过新颖的分位数裁剪技术增强，这使得能够使用更尖锐的基于经验跨度的惩罚函数。我们的算法也不需要任何先验参数知识即可实现。值得注意的是，我们通过困难示例表明，在我们的条件下学习需要超越目标策略平稳分布的覆盖假设，这使得单策略复杂性度量与先前研究的案例区分开来。我们还开发了几乎与我们主要结果匹配的下限。", "summary": "本文解决了平均奖励MDPs中的离线强化学习问题，这是一个理论上未被充分研究的领域，存在分布偏移和非均匀覆盖的挑战。它首次提出了完全单策略样本复杂度界限，利用了新的依赖于目标策略的复杂性度量，如偏差跨度和新颖的策略命中半径。所提出的算法基于悲观折扣值迭代并结合分位数裁剪，能够处理一般弱通信MDPs，且无需先验参数知识。该工作还强调，在此条件下进行单策略学习需要超越平稳分布的覆盖。", "keywords": "离线强化学习, 平均奖励MDPs, 样本复杂度, 单策略, 弱通信MDPs", "comments": "该论文通过提供第一个单策略样本复杂度界限，对平均奖励离线强化学习做出了重要的理论贡献，这比以往统一的复杂性度量更为实用且限制性更小。引入策略命中半径以及处理一般弱通信MDPs的能力是显著的创新。发现单策略学习需要更强的覆盖假设对未来的研究具有重要启发意义。"}}
{"id": "2506.20992", "title": "Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty", "authors": ["Craig Steven Wright"], "summary": "This paper develops a formal game-theoretic model to examine how protocol\nmutability disrupts cooperative mining behaviour in blockchain systems. Using a\nrepeated game framework with stochastic rule shocks, we show that even minor\nuncertainty in institutional rules increases time preference and induces\nstrategic deviation. Fixed-rule environments support long-term investment and\nstable equilibrium strategies; in contrast, mutable protocols lead to\nshort-termism, higher discounting, and collapse of coordinated engagement.\nSimulation results identify instability zones in the parameter space where\nrational mining gives way to extractive or arbitrage conduct. These findings\nsupport an Austrian economic interpretation: calculability requires rule\nstability. Institutional noise undermines the informational basis for\nproductive action. We conclude that protocol design must be treated as a\nconstitutional economic constraint, not a discretionary variable, if\nsustainable cooperation is to emerge in decentralised systems.", "comment": "40 pages, submitted to QJAE", "pdf_url": "http://arxiv.org/pdf/2506.20992v1", "categories": ["econ.GN", "cs.CE", "cs.CY", "cs.GT", "cs.SI", "q-fin.EC", "91A05, 91B42, 68M14, 91B62", "J.4; C.2.4; K.4.1; F.1.1"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.20992v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "制度噪音、策略偏差与跨期崩溃：协议不确定性下矿工行为的形式模型", "tldr": "本文通过一个形式化的博弈论模型，探讨了区块链系统中协议的可变性如何破坏合作挖矿行为，指出协议不确定性会导致短期主义和协调参与的崩溃。", "motivation": "本文旨在通过一个形式化的博弈论模型，研究协议的可变性如何扰乱区块链系统中的合作挖矿行为。", "method": "本文采用了一个重复博弈框架，其中包含随机规则冲击，以开发一个形式化的博弈论模型。", "result": "研究表明，即使制度规则中存在微小的不确定性，也会增加时间偏好并导致策略偏差。固定规则环境支持长期投资和稳定的均衡策略，而可变协议则导致短期主义、更高的贴现率和协调参与的崩溃。模拟结果识别出参数空间中的不稳定区域，其中理性挖矿让位于掠夺性或套利行为。", "conclusion": "本文得出结论，如果要在去中心化系统中实现可持续的合作，协议设计必须被视为一种宪法经济约束，而非可自由裁量的变量。", "translation": "本文开发了一个形式化的博弈论模型，旨在研究协议的可变性如何扰乱区块链系统中的合作挖矿行为。我们使用一个包含随机规则冲击的重复博弈框架，结果表明，即使制度规则中存在微小的不确定性，也会增加时间偏好并导致策略偏差。固定规则环境支持长期投资和稳定的均衡策略；相反，可变协议会导致短期主义、更高的贴现率和协调参与的崩溃。模拟结果识别出参数空间中的不稳定区域，其中理性挖矿让位于掠夺性或套利行为。这些发现支持奥地利经济学的解释：可计算性需要规则的稳定性。制度噪音破坏了生产性行动的信息基础。我们得出结论，如果要在去中心化系统中实现可持续的合作，协议设计必须被视为一种宪法经济约束，而非可自由裁量的变量。", "summary": "本文构建了一个博弈论模型，探讨了区块链协议的可变性对矿工合作行为的影响。研究发现，协议规则的不确定性会促使矿工采取短期行为和策略偏差，导致合作崩溃。与此相反，稳定的规则环境则有利于长期投资和均衡策略。研究强调，为实现去中心化系统的可持续合作，协议设计应被视为一项基本经济约束而非可变因素。", "keywords": "区块链, 博弈论, 协议不确定性, 矿工行为, 合作崩溃", "comments": "本文创新性地将博弈论模型应用于区块链矿工行为分析，揭示了协议不确定性对系统稳定性和合作的负面影响。其重要性在于为区块链协议设计提供了理论依据，强调了规则稳定性的关键作用，并从奥地利经济学角度提供了独特见解。该研究对于理解和设计可持续的去中心化系统具有重要指导意义。"}}
{"id": "2506.20795", "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20795v1", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20795v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "基础模型在人机交互手势识别中与基于骨架的方法相比如何？", "tldr": "本研究比较了基础模型（V-JEPA，Gemini Flash 2.0）与基于骨架的方法（HD-GCN）在人机交互手势识别中的表现，并引入了新数据集NUGGET。结果显示HD-GCN性能最佳，但V-JEPA表现接近，而Gemini在零样本设置下表现不佳。", "motivation": "手势是人机非语言交流的关键，尤其在嘈杂环境中。传统深度学习手势识别依赖于特定任务的架构。视觉基础模型（VFMs）和视觉语言模型（VLMs）凭借其强大的泛化能力，有望取代专用模块，从而降低系统复杂性，因此本研究旨在探究它们在此领域的适用性。", "method": "本研究调查了如何将视觉基础模型（V-JEPA）和多模态视觉语言模型（Gemini Flash 2.0）应用于动态全身手势识别，并将其与表现最佳的基于骨架的方法（HD-GCN）进行比较。为此，研究引入了一个专为内部物流环境中人机通信设计的新数据集NUGGET，用于评估不同的手势识别方法。", "result": "实验结果显示，HD-GCN取得了最佳性能。V-JEPA在添加一个简单的、针对特定任务的分类头后，表现与HD-GCN接近，这为通过使用V-JEPA作为共享多任务模型来降低系统复杂性提供了可能途径。相比之下，Gemini Flash 2.0在零样本设置下，仅凭文本描述难以区分手势。", "conclusion": "HD-GCN目前在手势识别方面表现最优。然而，V-JEPA展现出作为共享多任务模型降低系统复杂性的潜力。对于Gemini等VLM在零样本设置中识别手势，仍需进一步研究合适的输入表示。", "translation": "手势使非语言人机通信成为可能，尤其是在敏捷生产等嘈杂环境中。传统的基于深度学习的手势识别依赖于使用图像、视频或骨骼姿态估计作为输入的特定任务架构。与此同时，视觉基础模型（VFMs）和视觉语言模型（VLMs）凭借其强大的泛化能力，有望通过取代专用任务模块来降低系统复杂性。本研究调查了如何将此类模型应用于动态、全身手势识别，并比较了V-JEPA（最先进的VFM）、Gemini Flash 2.0（多模态VLM）和HD-GCN（表现最佳的基于骨架的方法）。我们引入了NUGGET，一个专为内部物流环境中人机通信量身定制的数据集，以评估不同的手势识别方法。在我们的实验中，HD-GCN取得了最佳性能，但V-JEPA通过一个简单的、针对特定任务的分类头也表现接近——这为通过将其用作共享多任务模型来降低系统复杂性铺平了可能的道路。相比之下，Gemini在零样本设置下，仅凭文本描述难以区分手势，这凸显了对适合手势的输入表示进行进一步研究的必要性。", "summary": "该论文探讨了视觉基础模型（V-JEPA）和视觉语言模型（Gemini Flash 2.0）在人机交互中动态全身手势识别的有效性，并将其与领先的基于骨架的方法（HD-GCN）进行比较。通过引入新数据集NUGGET进行评估，研究发现HD-GCN表现最佳，但V-JEPA在配备简单分类头后也取得了相似的性能，这表明其在降低系统复杂性方面的潜力。然而，Gemini Flash 2.0在零样本手势区分上表现出局限性，提示需要改进此类模型的输入表示。", "keywords": "手势识别, 基础模型, 人机交互, 基于骨架, 视觉模型", "comments": "该论文探讨了人机交互领域的一个重要且前沿的问题，即基础模型在手势识别中的应用。引入的NUGGET数据集对于未来的相关研究具有重要价值。研究发现V-JEPA在仅使用简单分类头的情况下就能取得接近最佳骨架模型的性能，这一点尤为创新，预示着未来人机交互系统可能通过使用更通用、更少复杂的基础模型来实现。同时，Gemini在零样本设置下的挣扎也揭示了当前VLM在处理细微物理手势时，若无特定视觉训练或恰当表示，所面临的挑战。"}}
{"id": "2506.20994", "title": "Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe", "authors": ["Måns I. Andersson", "Martin Karp", "Niclas Jansson", "Stefano Markidis"], "summary": "With the emergence of new high-performance computing (HPC) accelerators, such\nas Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures\nhas become a major challenge for HPC application developers. The increasing\nhardware diversity in HPC systems often necessitates the development of\narchitecture-specific code, hindering the sustainability of large-scale\nscientific applications. In this work, we leverage DaCe, a data-centric\nparallel programming framework, to automate the generation of high-performance\nkernels. DaCe enables automatic code generation for multicore processors and\nvarious accelerators, reducing the burden on developers who would otherwise\nneed to rewrite code for each new architecture. Our study demonstrates DaCe's\ncapabilities by applying its automatic code generation to a critical\ncomputational kernel used in Computational Fluid Dynamics (CFD). Specifically,\nwe focus on Neko, a Fortran-based solver that employs the spectral-element\nmethod, which relies on small tensor operations. We detail the formulation of\nthis computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)\nrepresentation and discuss how this approach facilitates high-performance code\ngeneration. Additionally, we outline the workflow for seamlessly integrating\nDaCe's generated code into the Neko solver. Our results highlight the\nportability and performance of the generated code across multiple platforms,\nincluding Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive\nperformance results. By demonstrating the potential of automatic code\ngeneration, we emphasise the feasibility of using portable solutions to ensure\nthe long-term sustainability of large-scale scientific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20994v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.20994v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用 DaCe 为计算流体动力学代码生成可移植高性能内核", "tldr": "本文利用数据中心并行编程框架 DaCe，为计算流体动力学 (CFD) 代码自动生成高性能且可移植的内核，以应对高性能计算 (HPC) 异构硬件的开发挑战，并展示了其在多种 GPU 平台上的竞争力。", "motivation": "随着新的高性能计算 (HPC) 加速器（如 Nvidia 和 AMD GPU）的出现，高效地针对不同的硬件架构已成为 HPC 应用程序开发人员面临的主要挑战。HPC 系统中不断增加的硬件多样性通常需要开发特定于架构的代码，这阻碍了大规模科学应用的可持续性。", "method": "研究利用数据中心并行编程框架 DaCe 自动化生成高性能内核。具体地，将 DaCe 的自动代码生成应用于计算流体动力学 (CFD) 中使用的关键计算内核（Neko，一个基于 Fortran 的谱元法求解器，依赖于小型张量操作）。详细阐述了使用 DaCe 的有状态数据流多重图 (SDFG) 表示法构建计算内核，并讨论了如何通过此方法实现高性能代码生成。此外，还概述了将 DaCe 生成的代码无缝集成到 Neko 求解器中的工作流程。", "result": "生成代码在多个平台（包括 Nvidia GH200、Nvidia A100 和 AMD MI250X GPU）上展现出良好的可移植性和竞争力的高性能结果。", "conclusion": "通过展示自动代码生成的潜力，本文强调了使用可移植解决方案确保大规模科学应用长期可持续性的可行性，有效应对了 HPC 硬件多样性的挑战。", "translation": "随着 Nvidia 和 AMD GPU 等新型高性能计算 (HPC) 加速器的出现，高效地针对不同的硬件架构已成为 HPC 应用程序开发人员面临的主要挑战。HPC 系统中不断增加的硬件多样性通常需要开发特定于架构的代码，这阻碍了大规模科学应用的可持续性。在这项工作中，我们利用数据中心并行编程框架 DaCe 来自动化生成高性能内核。DaCe 能够为多核处理器和各种加速器自动生成代码，从而减轻了开发人员为每个新架构重写代码的负担。我们的研究通过将 DaCe 的自动代码生成应用于计算流体动力学 (CFD) 中使用的关键计算内核，展示了 DaCe 的能力。具体来说，我们专注于 Neko，一个基于 Fortran 的求解器，它采用谱元法，该方法依赖于小型张量操作。我们详细阐述了使用 DaCe 的有状态数据流多重图 (SDFG) 表示法构建此计算内核，并讨论了这种方法如何促进高性能代码生成。此外，我们还概述了将 DaCe 生成的代码无缝集成到 Neko 求解器中的工作流程。我们的结果突出了生成代码在多个平台上的可移植性和性能，包括 Nvidia GH200、Nvidia A100 和 AMD MI250X GPU，并取得了具有竞争力的性能结果。通过展示自动代码生成的潜力，我们强调了使用可移植解决方案确保大规模科学应用长期可持续性的可行性。", "summary": "本文旨在解决高性能计算 (HPC) 硬件多样性带来的挑战，即大规模科学应用需要为不同架构重写代码的问题。研究利用数据中心并行编程框架 DaCe，实现了高性能内核的自动化生成，从而减轻了开发人员的负担。通过将 DaCe 应用于计算流体动力学 (CFD) 中的关键计算内核（Neko 求解器），研究展示了 DaCe 在生成可移植代码方面的能力。结果表明，生成代码在 Nvidia GH200、Nvidia A100 和 AMD MI250X 等多种 GPU 平台上均表现出竞争力，证明了自动代码生成对于确保大规模科学应用长期可持续性的潜力。", "keywords": "HPC, DaCe, 自动代码生成, 计算流体动力学, 可移植性", "comments": "该论文的创新点在于利用 DaCe 框架为计算流体动力学 (CFD) 代码实现高性能内核的自动生成和跨平台可移植性。这对于当前 HPC 领域硬件多样性日益增长的趋势至关重要，它能显著降低开发人员为不同架构重写代码的负担，从而提高大规模科学应用的可持续性和代码的生命周期。研究通过具体实例展示了 DaCe 的实用性和有效性。"}}
{"id": "2506.20745", "title": "Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates", "authors": ["C. Betegón", "C. Rodríguez", "E. Martínez-Pañeda", "R. M. McMeeking"], "summary": "The exceptional adhesion properties of biological fibrillar structures --\nsuch as those found in geckos -- have inspired the development of synthetic\nadhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated\nsuperior pull-off strength compared to other geometries. In this study, we\nemploy a computational approach based on a Dugdale cohesive zone model to\nanalyze the detachment behavior of these fibrils when adhered to a rigid\nsubstrate. The results provide complete pull-off curves, revealing that the\nseparation process is inherently unstable under load control, regardless of\nwhether detachment initiates at the fibril edge or center. Our findings show\nthat fibrils with a wide, thin mushroom cap effectively reduce stress\nconcentrations and promote central detachment, leading to enhanced adhesion.\nHowever, detachment from the center is not observed in all geometries, whereas\nedge detachment can occur under certain conditions in all cases. Additionally,\nwe investigate the impact of adhesion defects at the fibril center, showing\nthat they can significantly reduce pull-off strength, particularly at high\nvalues of the dimensionless parameter \\c{hi}. These insights contribute to the\noptimization of bio-inspired adhesives and microstructured surfaces for various\nengineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20745v1", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cs.CE", "physics.app-ph", "physics.bio-ph"], "cate": "cond-mat.soft", "url": "http://arxiv.org/abs/2506.20745v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "蘑菇状纤维粘附于刚性基底的剥离强度", "tldr": "研究了蘑菇状纤维粘附在刚性基底上的剥离强度，发现分离过程不稳定，优化帽形可增强粘附，但中心脱离并非总是发生，且缺陷会显著降低强度。", "motivation": "生物纤维结构（如壁虎）的卓越粘附性能启发了合成粘附表面的开发，其中蘑菇状纤维表现出优越的剥离强度，因此需要深入分析其脱离行为以优化生物启发粘合剂。", "method": "采用基于Dugdale内聚力区模型的计算方法来分析蘑菇状纤维粘附在刚性基底上的脱离行为。", "result": "提供了完整的剥离曲线；揭示了分离过程在载荷控制下本质上是不稳定的，无论脱离是从纤维边缘还是中心开始；发现宽而薄的蘑菇帽能有效减少应力集中并促进中心脱离，从而增强粘附；并非所有几何形状都观察到中心脱离，而边缘脱离在所有情况下都可能发生；纤维中心的粘附缺陷会显著降低剥离强度，特别是在无量纲参数\\c{hi}值较高时。", "conclusion": "这些见解有助于优化生物启发粘合剂和微结构表面，用于各种工程应用。", "translation": "生物纤维结构（如壁虎中发现的）卓越的粘附性能激发了合成粘附表面的发展。其中，蘑菇状纤维与其他几何形状相比，表现出优越的剥离强度。在本研究中，我们采用基于Dugdale内聚力区模型的计算方法来分析这些纤维粘附在刚性基底上时的脱离行为。结果提供了完整的剥离曲线，揭示了分离过程在载荷控制下本质上是不稳定的，无论脱离是从纤维边缘还是中心开始。我们的研究结果表明，宽而薄的蘑菇帽纤维能有效减少应力集中并促进中心脱离，从而增强粘附。然而，并非在所有几何形状中都观察到中心脱离，而在某些条件下，边缘脱离在所有情况下都可能发生。此外，我们研究了纤维中心粘附缺陷的影响，表明它们可以显著降低剥离强度，特别是在无量纲参数\\c{hi}值较高时。这些见解有助于优化生物启发粘合剂和微结构表面，用于各种工程应用。", "summary": "本文利用Dugdale内聚力区模型对蘑菇状纤维在刚性基底上的剥离行为进行了计算分析。研究发现，脱离过程在载荷控制下不稳定。优化蘑菇帽的几何形状（宽而薄）有助于减少应力集中并促进中心脱离，从而提高粘附力。同时指出中心脱离并非普适现象，且粘附缺陷，尤其是在高无量纲参数下，会显著降低剥离强度。这些发现为生物启发粘合剂和微结构表面的优化提供了指导。", "keywords": "蘑菇状纤维, 剥离强度, 粘附, Dugdale模型, 生物启发粘合剂", "comments": "这项研究通过计算方法深入分析了蘑菇状纤维的剥离机制，揭示了其在载荷控制下的不稳定性以及帽形几何和内部缺陷对粘附强度的影响。其创新在于将Dugdale模型应用于此类生物启发结构，为设计高性能合成粘合剂提供了宝贵的理论依据和优化方向，特别是在理解应力集中和脱离模式方面具有重要意义。"}}
{"id": "2506.20705", "title": "On Convolutions, Intrinsic Dimension, and Diffusion Models", "authors": ["Kin Kwan Leung", "Rasa Hosseinzadeh", "Gabriel Loaiza-Ganem"], "summary": "The manifold hypothesis asserts that data of interest in high-dimensional\nambient spaces, such as image data, lies on unknown low-dimensional\nsubmanifolds. Diffusion models (DMs) -- which operate by convolving data with\nprogressively larger amounts of Gaussian noise and then learning to revert this\nprocess -- have risen to prominence as the most performant generative models,\nand are known to be able to learn distributions with low-dimensional support.\nFor a given datum in one of these submanifolds, we should thus intuitively\nexpect DMs to have implicitly learned its corresponding local intrinsic\ndimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari\net al. (2024b) recently showed that this is indeed the case by linking this LID\nto the rate of change of the log marginal densities of the DM with respect to\nthe amount of added noise, resulting in an LID estimator known as FLIPD. LID\nestimators such as FLIPD have a plethora of uses, among others they quantify\nthe complexity of a given datum, and can be used to detect outliers,\nadversarial examples and AI-generated text. FLIPD achieves state-of-the-art\nperformance at LID estimation, yet its theoretical underpinnings are incomplete\nsince Kamkari et al. (2024b) only proved its correctness under the highly\nunrealistic assumption of affine submanifolds. In this work we bridge this gap\nby formally proving the correctness of FLIPD under realistic assumptions.\nAdditionally, we show that an analogous result holds when Gaussian convolutions\nare replaced with uniform ones, and discuss the relevance of this result.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20705v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20705v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "关于卷积、内在维度和扩散模型", "tldr": "本文在更现实的假设下，形式化证明了扩散模型中FLIPD内在维度估计器的正确性，并探讨了均匀卷积的类似结果。", "motivation": "现有的局部内在维度（LID）估计器FLIPD虽然在扩散模型中表现出色，但其理论基础不完善，仅在高度不现实的仿射子流形假设下证明了其正确性。本文旨在弥补这一理论空白，为FLIPD提供在现实条件下的形式化理论支撑。", "method": "本文通过形式化证明，在更现实的假设下验证了FLIPD估计器的正确性。此外，研究还展示了当高斯卷积被均匀卷积替代时，一个类似的LID估计结果仍然成立，并讨论了其相关性。", "result": "本文成功在现实假设下形式化证明了FLIPD估计器的正确性。同时，研究发现当高斯卷积被均匀卷积替代时，也存在一个类似的局部内在维度估计结果。", "conclusion": "本文通过在更现实的条件下证明FLIPD的理论正确性，完善了扩散模型中内在维度估计的理论基础，从而增强了FLIPD在量化数据复杂性、检测异常值和对抗性样本等应用中的可靠性。此外，对均匀卷积的探讨也拓宽了对扩散模型机制的理解。", "translation": "流形假设认为高维环境空间中的数据（如图像数据）位于未知的低维子流形上。扩散模型（DMs）——通过将数据与逐渐增大的高斯噪声进行卷积，然后学习逆转这一过程——已成为性能最佳的生成模型，并已知能够学习具有低维支持的分布。对于这些子流形中的给定数据，我们直观地期望DMs能够隐式学习其相应的局部内在维度（LID），即其所属子流形的维度。Kamkari 等人（2024b）最近通过将此LID与DM的对数边际密度随噪声量变化的速率联系起来，证明了确实如此，从而产生了名为FLIPD的LID估计器。FLIPD 等LID估计器有多种用途，其中包括量化给定数据的复杂性，并可用于检测异常值、对抗性样本和AI生成文本。FLIPD 在LID估计方面取得了最先进的性能，但其理论基础不完善，因为Kamkari 等人（2024b）仅在高度不现实的仿射子流形假设下证明了其正确性。在这项工作中，我们通过在现实假设下形式化证明FLIPD的正确性来弥补这一空白。此外，我们还表明，当高斯卷积被均匀卷积替代时，一个类似的结果也成立，并讨论了该结果的相关性。", "summary": "本文致力于完善扩散模型（DMs）中局部内在维度（LID）估计器FLIPD的理论基础。尽管FLIPD在LID估计方面表现出色，但其理论正确性此前仅在不现实的仿射子流形假设下得到证明。本研究通过形式化证明，在更现实的假设下验证了FLIPD的正确性，从而弥补了这一理论空白。此外，研究还发现，当高斯卷积被均匀卷积替代时，也存在一个类似的LID估计结果，并探讨了其意义。", "keywords": "扩散模型, 内在维度, 卷积, FLIPD, 理论证明", "comments": "本文的创新之处在于它填补了FLIPD估计器理论基础的空白，将其正确性证明推广到更现实的条件下，这对于理解和信任扩散模型在处理高维数据时的行为至关重要。其重要性在于，更坚实的理论基础有助于FLIPD在异常检测、对抗性样本识别和AI生成文本检测等实际应用中获得更广泛的采纳和更高的可靠性。"}}
{"id": "2506.20987", "title": "Optimal Parameter Design for Power Electronic Converters Using a Probabilistic Learning-Based Stochastic Surrogate Model", "authors": ["Akash Mahajan", "Shivam Chaturvedi", "Srijita Das", "Wencong Su", "Van-Hai Bui"], "summary": "The selection of optimal design for power electronic converter parameters\ninvolves balancing efficiency and thermal constraints to ensure high\nperformance without compromising safety. This paper introduces a\nprobabilistic-learning-based stochastic surrogate modeling framework to address\nthis challenge and significantly reduce the time required during the design\nphase. The approach begins with a neural network classifier that evaluates the\nfeasibility of parameter configurations, effectively filtering out unsafe\nand/or impractical inputs. Subsequently, a probabilistic prediction model\nestimates the converter's efficiency and temperature while quantifying\nprediction uncertainty, providing both performance insights and reliability\nmetrics. Finally, a heuristic optimization-based model is employed to optimize\na multi-objective function that maximizes efficiency while adhering to thermal\nconstraints. The optimization process incorporates penalty terms to discourage\nsolutions that violate practical thresholds, ensuring actionable and realistic\nrecommendations. An advanced heuristic optimization method is used to find the\noptimal solution and is compared with several well-known search algorithms,\nincluding Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Simulated\nAnnealing (SA), Tabu-Search (TS), and Stochastic Hill Climbing (SHC). The\nresults demonstrate significant improvements in predictive accuracy and\noptimization outcomes, offering a robust solution for advancing power\nelectronics design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20987v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.20987v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于概率学习随机代理模型的电力电子变换器最优参数设计", "tldr": "本文提出了一种基于概率学习的随机代理模型框架，用于电力电子变换器的最优参数设计，显著减少设计时间并提高性能和安全性。", "motivation": "电力电子变换器参数的优化设计需要平衡效率和热约束，以确保高性能和安全性。传统方法耗时较长，本研究旨在解决这一挑战并显著缩短设计阶段所需时间。", "method": "该方法首先使用神经网络分类器评估参数配置的可行性，过滤不安全或不切实际的输入。随后，概率预测模型估计变换器的效率和温度，并量化预测不确定性。最后，采用基于启发式优化的模型来优化一个多目标函数，旨在最大化效率同时遵守热约束，并引入惩罚项以避免不符合实际阈值的解决方案。该优化方法与遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）、禁忌搜索（TS）和随机爬山（SHC）等多种已知搜索算法进行了比较。", "result": "结果表明，该框架在预测精度和优化结果方面均有显著改进。", "conclusion": "该基于概率学习的随机代理模型框架为电力电子设计提供了一个鲁棒的解决方案，能够有效提升预测精度和优化结果。", "translation": "电力电子变换器参数的最优设计涉及平衡效率和热约束，以确保高性能同时不损害安全性。本文引入了一个基于概率学习的随机代理建模框架，以应对这一挑战并显著减少设计阶段所需时间。该方法首先使用神经网络分类器评估参数配置的可行性，有效过滤掉不安全和/或不切实际的输入。随后，概率预测模型估计变换器的效率和温度，同时量化预测不确定性，提供性能洞察和可靠性指标。最后，采用基于启发式优化的模型来优化一个多目标函数，旨在最大化效率同时遵守热约束。优化过程引入惩罚项，以避免违反实际阈值的解决方案，确保可操作和实际的建议。使用一种先进的启发式优化方法来寻找最优解，并与几种知名搜索算法进行了比较，包括遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）、禁忌搜索（TS）和随机爬山（SHC）。结果表明，在预测精度和优化结果方面均有显著改进，为推进电力电子设计提供了鲁棒的解决方案。", "summary": "本文提出了一种基于概率学习的随机代理建模框架，用于电力电子变换器的最优参数设计。该框架通过结合神经网络分类器、概率预测模型和启发式优化方法，旨在平衡效率和热约束，同时显著减少设计时间。实验结果表明，该方法在提高预测精度和优化效果方面表现出色，为电力电子设计提供了一个鲁棒且高效的解决方案。", "keywords": "电力电子变换器, 参数优化, 概率学习, 代理模型, 启发式优化", "comments": "该论文的创新之处在于整合了多种机器学习和优化技术，形成了一个端到端的电力电子变换器参数设计框架。通过引入神经网络进行可行性过滤、概率模型进行不确定性量化以及多目标启发式优化，该方法有效解决了传统设计中效率与安全性的平衡难题，并显著缩短了设计周期。其重要性体现在为复杂工程设计提供了一种高效、鲁棒且具有实际指导意义的自动化设计工具。"}}
{"id": "2506.20863", "title": "Quantum-Accelerated Wireless Communications: Concepts, Connections, and Implications", "authors": ["Naoki Ishikawa", "Giuseppe Thadeu Freitas de Abreu", "Petar Popovski", "Robert W. Heath Jr"], "summary": "Quantum computing is poised to redefine the algorithmic foundations of\ncommunication systems. While quantum superposition and entanglement enable\nquadratic or exponential speedups for specific problems, identifying use cases\nwhere these advantages yield engineering benefits is, however, still\nnontrivial. This article presents the fundamentals of quantum computing in a\nstyle familiar to the communications society, outlining the current limits of\nfault-tolerant quantum computing and uncovering a mathematical harmony between\nquantum and wireless systems, which makes the topic more enticing to wireless\nresearchers. Based on a systematic review of pioneering and state-of-the-art\nstudies, we distill common design trends for the research and development of\nquantum-accelerated communication systems and highlight lessons learned. The\nkey insight is that classical heuristics can sharpen certain quantum\nparameters, underscoring the complementary strengths of classical and quantum\ncomputing. This article aims to catalyze interdisciplinary research at the\nfrontier of quantum information processing and future communication systems.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20863v1", "categories": ["eess.SP", "quant-ph"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20863v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "量子加速无线通信：概念、连接与启示", "tldr": "本文介绍了量子计算在通信系统中的基础知识，探讨了量子与无线系统之间的数学和谐，并基于系统回顾总结了量子加速通信系统的设计趋势和经验教训，强调了经典启发式方法与量子计算的互补优势，旨在促进跨学科研究。", "motivation": "尽管量子计算能为特定问题带来二次或指数级的加速，但要识别出能产生工程效益的用例仍然非易事。本文旨在促进量子信息处理与未来通信系统前沿的跨学科研究。", "method": "本文以通信领域熟悉的方式介绍了量子计算的基础知识，概述了容错量子计算的当前限制，并揭示了量子与无线系统之间的数学和谐。基于对开创性和最先进研究的系统回顾，本文提炼了量子加速通信系统研发的常见设计趋势并强调了经验教训。", "result": "本文提炼了量子加速通信系统研发的常见设计趋势，并强调了经验教训。关键的见解是，经典启发式方法可以锐化某些量子参数，突出了经典计算和量子计算的互补优势。", "conclusion": "本文旨在催化量子信息处理与未来通信系统前沿的跨学科研究。", "translation": "量子计算有望重新定义通信系统的算法基础。尽管量子叠加和纠缠能为特定问题带来二次或指数级的加速，但识别出能产生工程效益的用例仍然非易事。本文以通信领域熟悉的方式介绍了量子计算的基础知识，概述了容错量子计算的当前限制，并揭示了量子与无线系统之间的数学和谐，这使得该主题对无线研究人员更具吸引力。基于对开创性和最先进研究的系统回顾，我们提炼了量子加速通信系统研发的常见设计趋势，并强调了经验教训。关键的见解是，经典启发式方法可以锐化某些量子参数，突出了经典计算和量子计算的互补优势。本文旨在催化量子信息处理与未来通信系统前沿的跨学科研究。", "summary": "本文探讨了量子计算如何重新定义通信系统。文章以通信领域熟悉的方式介绍了量子计算的基础，揭示了量子与无线系统之间的数学和谐。通过系统回顾现有研究，提炼了量子加速通信系统的设计趋势和经验，并强调了经典启发式方法与量子计算的互补优势，旨在促进相关领域的跨学科研究。", "keywords": "量子计算, 无线通信, 量子加速, 经典启发式, 跨学科研究", "comments": "本文创新性地将量子计算与无线通信结合，以易于理解的方式介绍了量子概念，并指出了经典与量子计算的互补性，对于推动该领域的跨学科研究具有重要意义。其系统回顾和经验总结对未来研究提供了宝贵指导。"}}
{"id": "2506.21162", "title": "A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation", "authors": ["Shuwei Xing", "Derek W. Cool", "David Tessier", "Elvis C. S. Chen", "Terry M. Peters", "Aaron Fenster"], "summary": "3D ultrasound (US) imaging has shown significant benefits in enhancing the\noutcomes of percutaneous liver tumour ablation. Its clinical integration is\ncrucial for transitioning 3D US into the therapeutic domain. However,\nchallenges of tumour identification in US images continue to hinder its broader\nadoption. In this work, we propose a novel framework for integrating 3D US into\nthe standard ablation workflow. We present a key component, a clinically viable\n2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to\nreduce registration complexity. To facilitate efficient verification of the\nregistration workflow, we also propose an intuitive multimodal image\nvisualization technique. In our study, 2D US-CT/MRI registration achieved a\nlandmark distance error of approximately 2-4 mm with a runtime of 0.22s per\nimage pair. Additionally, non-rigid registration reduced the mean alignment\nerror by approximately 40% compared to rigid registration. Results demonstrated\nthe efficacy of the proposed 2D US-CT/MRI registration workflow. Our\nintegration framework advanced the capabilities of 3D US imaging in improving\npercutaneous tumour ablation, demonstrating the potential to expand the\ntherapeutic role of 3D US in clinical interventions.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21162v1", "categories": ["eess.IV", "cs.AI"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21162v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "将3D超声整合到经皮肝肿瘤消融中的新颖框架", "tldr": "该论文提出了一种将3D超声整合到肝肿瘤消融中的新框架，其核心是2D超声-CT/MRI配准方法，该方法表现出良好的准确性和速度，有望扩展3D超声的临床应用。", "motivation": "尽管3D超声成像在提高经皮肝肿瘤消融效果方面具有显著益处，但超声图像中肿瘤识别的挑战持续阻碍了其更广泛的临床应用，特别是在治疗领域。", "method": "作者提出了一个将3D超声整合到标准消融工作流程中的新颖框架。该框架的关键组件是一种临床可行的2D超声-CT/MRI配准方法，该方法利用3D超声作为中间体来降低配准复杂性。此外，还提出了一种直观的多模态图像可视化技术，以促进配准工作流程的有效验证。", "result": "在研究中，2D超声-CT/MRI配准实现了约2-4毫米的地标距离误差，每对图像的运行时间为0.22秒。与刚性配准相比，非刚性配准将平均对齐误差减少了约40%。", "conclusion": "所提出的2D超声-CT/MRI配准工作流程被证明是有效的。该整合框架提升了3D超声成像在改善经皮肿瘤消融方面的能力，并展示了扩大3D超声在临床干预中治疗作用的潜力。", "translation": "3D超声（US）成像在提高经皮肝肿瘤消融效果方面已显示出显著益处。将其临床整合对于将3D超声引入治疗领域至关重要。然而，超声图像中肿瘤识别的挑战仍然阻碍了其更广泛的应用。在这项工作中，我们提出了一个将3D超声整合到标准消融工作流程中的新颖框架。我们提出了一个关键组件，即一种临床可行的2D超声-CT/MRI配准方法，该方法利用3D超声作为中间体来降低配准复杂性。为了促进配准工作流程的有效验证，我们还提出了一种直观的多模态图像可视化技术。在我们的研究中，2D超声-CT/MRI配准实现了约2-4毫米的地标距离误差，每对图像的运行时间为0.22秒。此外，与刚性配准相比，非刚性配准将平均对齐误差减少了约40%。结果证明了所提出的2D超声-CT/MRI配准工作流程的有效性。我们的整合框架提升了3D超声成像在改善经皮肿瘤消融方面的能力，展示了扩大3D超声在临床干预中治疗作用的潜力。", "summary": "本文提出了一种将3D超声（US）整合到经皮肝肿瘤消融中的新颖框架，旨在解决超声图像中肿瘤识别的挑战。该框架的核心是一个2D超声-CT/MRI配准方法，它利用3D超声作为中间体来简化配准过程。同时，还引入了一种多模态图像可视化技术用于验证。实验结果表明，该2D超声-CT/MRI配准方法实现了2-4毫米的误差和快速运行时间，且非刚性配准显著改善了对齐效果，证明了该框架的有效性及其扩展3D超声治疗作用的潜力。", "keywords": "3D超声, 肝肿瘤消融, 图像配准, 多模态成像", "comments": "该论文通过提出一种利用3D超声作为中间体的2D超声-CT/MRI配准方法，创新性地解决了3D超声在肝肿瘤消融中临床整合的挑战。这种方法有效降低了复杂性并提高了准确性，其低地标误差和快速运行时间证实了其临床可行性。此外，多模态可视化技术的引入进一步增强了其实用性。这项工作对于推进图像引导介入治疗和改善肝肿瘤患者的预后具有重要意义。"}}
{"id": "2506.21215", "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?", "authors": ["Haoang Chi", "He Li", "Wenjing Yang", "Feng Liu", "Long Lan", "Xiaoguang Ren", "Tongliang Liu", "Bo Han"], "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.", "comment": "24 pages, accepted at NeurIPS 2024", "pdf_url": "http://arxiv.org/pdf/2506.21215v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21215v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "揭示大型语言模型中的因果推理：现实还是幻象？", "tldr": "大型语言模型（LLMs）目前仅能进行浅层（一级）因果推理，缺乏真正的人类级（二级）能力。本文通过分析其自回归机制并引入新的基准CausalProbe-2024证明了这一点。为弥合差距，作者提出了G^2-Reasoner，通过整合通用知识和目标导向提示显著提升了LLMs的因果推理能力。", "motivation": "大型语言模型（LLMs）在理解语境因果关系和提供符合因果定律的响应方面表现出能力，但目前尚不清楚它们是否进行类似人类的真正因果推理。现有证据表明LLMs仅能进行浅层（一级）因果推理，缺乏真正的人类级（二级）因果推理能力，因此需要深入探究并寻找弥补差距的方法。", "method": "本文从方法论上深入探讨了基于Transformer的LLMs的自回归机制，揭示其并非固有因果性。在实证上，引入了一个新的因果问答基准CausalProbe-2024，其语料库对于所研究的LLMs而言是全新的且几乎未曾见过。为了弥合向二级因果推理的差距，本文提出了G^2-Reasoner方法，该方法将通用知识和目标导向提示融入到LLMs的因果推理过程中。", "result": "LLMs在CausalProbe-2024上的表现比早期基准有显著下降，这表明它们主要进行一级因果推理。实验证明，G^2-Reasoner显著增强了LLMs的因果推理能力，特别是在全新和反事实的语境中。", "conclusion": "这项工作为LLMs向真正的因果推理迈进，超越一级并向二级迈进指明了一条新路径。", "translation": "因果推理能力对于推动大型语言模型（LLMs）迈向强大的人工智能至关重要。虽然多功能LLMs似乎已经展示出理解语境因果关系和提供符合因果定律的响应的能力，但它们是否进行类似人类的真正因果推理仍不清楚。然而，目前的证据表明情况恰恰相反。具体而言，LLMs仅能进行浅层（一级）因果推理，这主要归因于嵌入其参数中的因果知识，但它们缺乏真正的人类级（二级）因果推理能力。为了支持这一假设，从方法论上，我们深入研究了基于Transformer的LLMs的自回归机制，揭示其并非固有因果性。在实证上，我们引入了一个名为CausalProbe-2024的新因果问答基准，其语料库对于所研究的LLMs而言是全新的且几乎未曾见过。与早期基准相比，LLMs在CausalProbe-2024上的表现出现显著下降，这表明它们主要进行一级因果推理。为了弥合向二级因果推理的差距，我们从人类推理通常由通用知识和预期目标促进这一事实中获得灵感。我们提出了G^2-Reasoner，一种将通用知识和目标导向提示融入LLMs因果推理过程的方法。实验表明，G^2-Reasoner显著增强了LLMs的因果推理能力，特别是在全新和反事实的语境中。这项工作为LLMs向真正的因果推理迈进，超越一级并向二级迈进指明了一条新路径。", "summary": "本文探讨了大型语言模型（LLMs）的因果推理能力，指出尽管其表面上表现出因果理解，但实际上仅限于浅层（一级）推理，而非真正的人类级（二级）因果推理。研究通过分析Transformer模型的自回归机制，并引入全新的CausalProbe-2024基准进行实证验证，发现LLMs在该基准上表现显著下降，证实了其一级推理的局限性。为提升LLMs的因果推理能力至二级水平，作者提出了G^2-Reasoner方法，该方法通过整合通用知识和目标导向提示，有效增强了LLMs在陌生和反事实语境下的因果推理表现，为LLMs实现更深层次的因果理解开辟了新途径。", "keywords": "因果推理, 大型语言模型, CausalProbe-2024, G^2-Reasoner, 人工智能", "comments": "该论文对LLMs的因果推理能力进行了批判性审视，明确指出其当前局限于浅层（一级）推理，而非真正的人类级（二级）因果理解，这对于理解LLMs的深层能力边界至关重要。引入CausalProbe-2024这一全新基准，有效避免了模型对训练数据中记忆知识的依赖，从而更真实地评估了LLMs的泛化因果推理能力。此外，G^2-Reasoner的提出，通过融入通用知识和目标导向提示，为弥补LLMs在因果推理上的不足提供了一个创新且有前景的方向，对于推动LLMs向更高级别的人工智能发展具有重要意义。"}}
{"id": "2506.21272", "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character", "authors": ["Jiayi Zheng", "Xiaodong Cun"], "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon\nvideos from a single child's drawing, while faithfully preserving its unique\nartistic style. Unlike previous storytelling methods that primarily focus on\ncharacter consistency and basic motion, FairyGen explicitly disentangles\ncharacter modeling from stylized background generation and incorporates\ncinematic shot design to support expressive and coherent storytelling. Given a\nsingle character sketch, we first employ an MLLM to generate a structured\nstoryboard with shot-level descriptions that specify environment settings,\ncharacter actions, and camera perspectives. To ensure visual consistency, we\nintroduce a style propagation adapter that captures the character's visual\nstyle and applies it to the background, faithfully retaining the character's\nfull visual identity while synthesizing style-consistent scenes. A shot design\nmodule further enhances visual diversity and cinematic quality through frame\ncropping and multi-view synthesis based on the storyboard. To animate the\nstory, we reconstruct a 3D proxy of the character to derive physically\nplausible motion sequences, which are then used to fine-tune an MMDiT-based\nimage-to-video diffusion model. We further propose a two-stage motion\ncustomization adapter: the first stage learns appearance features from\ntemporally unordered frames, disentangling identity from motion; the second\nstage models temporal dynamics using a timestep-shift strategy with frozen\nidentity weights. Once trained, FairyGen directly renders diverse and coherent\nvideo scenes aligned with the storyboard. Extensive experiments demonstrate\nthat our system produces animations that are stylistically faithful,\nnarratively structured natural motion, highlighting its potential for\npersonalized and engaging story animation. The code will be available at\nhttps://github.com/GVCLab/FairyGen", "comment": "Project Page: https://jayleejia.github.io/FairyGen/ ; Code:\n  https://github.com/GVCLab/FairyGen", "pdf_url": "http://arxiv.org/pdf/2506.21272v1", "categories": ["cs.GR", "cs.CV", "cs.MM"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21272v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FairyGen：从单个儿童手绘角色生成故事驱动的卡通视频", "tldr": "FairyGen是一个自动系统，能从单个儿童手绘角色生成故事驱动的卡通视频，同时忠实保留其艺术风格，并融入电影级镜头设计和物理运动。", "motivation": "现有讲故事方法主要关注角色一致性和基本动作，缺乏表达性和连贯性。本研究旨在从儿童手绘角色生成故事驱动的卡通视频，同时忠实保留其独特艺术风格，并通过解耦角色建模与风格化背景生成、融入电影级镜头设计来提升表达性和连贯性。", "method": "1. 使用MLLM从单个角色草图生成带镜头描述的结构化故事板。\n2. 引入风格传播适配器，将角色视觉风格应用于背景，确保视觉一致性。\n3. 通过镜头设计模块（帧裁剪和多视图合成）增强视觉多样性和电影质量。\n4. 重建角色3D代理以生成物理上合理的运动序列，并用于微调基于MMDiT的图像到视频扩散模型。\n5. 提出两阶段运动定制适配器：第一阶段从无序帧学习外观特征以解耦身份与运动；第二阶段使用时间步长偏移策略建模时间动态。", "result": "1. FairyGen能够直接渲染与故事板对齐的多样且连贯的视频场景。\n2. 实验证明该系统生成的动画在风格上忠实，叙事结构上具有自然的运动。", "conclusion": "FairyGen系统在个性化和引人入胜的故事动画方面具有巨大潜力。", "translation": "我们提出了FairyGen，一个自动系统，可以从单个儿童手绘角色生成故事驱动的卡通视频，同时忠实地保留其独特的艺术风格。与以往主要关注角色一致性和基本动作的讲故事方法不同，FairyGen明确地将角色建模与风格化背景生成解耦，并融入电影级镜头设计以支持富有表现力和连贯性的故事讲述。给定单个角色草图，我们首先使用MLLM生成一个结构化故事板，其中包含指定环境设置、角色动作和摄像机视角的镜头级描述。为确保视觉一致性，我们引入了一个风格传播适配器，该适配器捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留了角色的完整视觉身份。一个镜头设计模块通过基于故事板的帧裁剪和多视图合成进一步增强了视觉多样性和电影质量。为了动画化故事，我们重建了角色的3D代理以导出物理上合理的运动序列，然后用于微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间上无序的帧中学习外观特征，从而将身份与运动解耦；第二阶段使用带有冻结身份权重的timestep-shift策略建模时间动态。一旦训练完成，FairyGen直接渲染与故事板对齐的多样且连贯的视频场景。大量实验表明，我们的系统生成的动画在风格上忠实，叙事结构上具有自然的运动，突出了其在个性化和引人入胜的故事动画方面的潜力。代码将可在https://github.com/GVCLab/FairyGen获取。", "summary": "FairyGen是一个创新的自动化系统，能够将儿童手绘角色转化为故事驱动的卡通视频，同时忠实保留其原始艺术风格。该系统通过多模态大语言模型生成结构化故事板，利用风格传播适配器确保视觉一致性，并通过镜头设计模块提升电影感。为实现逼真动画，FairyGen重建角色3D代理以生成物理运动，并采用两阶段运动定制适配器解耦身份与动作。实验结果表明，FairyGen生成的动画风格忠实、叙事连贯且运动自然，展现了其在个性化故事动画领域的巨大潜力。", "keywords": "卡通视频生成, 儿童手绘, 风格保留, 故事板, 运动定制", "comments": "FairyGen的创新之处在于其端到端地将儿童手绘角色转化为高质量、故事驱动的动画视频，特别强调了对原始艺术风格的保留、电影级镜头设计以及物理上合理的角色运动。通过解耦角色与背景生成、引入两阶段运动定制适配器等方法，有效解决了传统方法在风格一致性和动作逼真度上的挑战。该系统为个性化内容创作提供了新的可能性，具有重要的应用价值。"}}
{"id": "2506.20906", "title": "Almost Tight Additive Guarantees for \\boldmath $k$-Edge-Connectivity", "authors": ["Nikhil Kumar", "Chaitanya Swamy"], "summary": "We consider the \\emph{$k$-edge connected spanning subgraph} (kECSS) problem,\nwhere we are given an undirected graph $G = (V, E)$ with nonnegative edge costs\n$\\{c_e\\}_{e\\in E}$, and we seek a minimum-cost \\emph{$k$-edge connected}\nsubgraph $H$ of $G$. For even $k$, we present a polytime algorithm that\ncomputes a $(k-2)$-edge connected subgraph of cost at most the optimal value\n$LP^*$ of the natural LP-relaxation for kECSS; for odd $k$, we obtain a\n$(k-3)$-edge connected subgraph of cost at most $LP^*$. Since kECSS is APX-hard\nfor all $k\\geq 2$, our results are nearly optimal. They also significantly\nimprove upon the recent work of Hershkowitz et al., both in terms of solution\nquality and the simplicity of algorithm and its analysis. Our techniques also\nyield an alternate guarantee, where we obtain a $(k-1)$-edge connected subgraph\nof cost at most $1.5\\cdot LP^*$; with unit edge costs, the cost guarantee\nimproves to $(1+\\frac{4}{3k})\\cdot LP^*$, which improves upon the\nstate-of-the-art approximation for unit edge costs, but with a unit loss in\nedge connectivity.\n  Our kECSS-result also yields results for the \\emph{$k$-edge connected\nspanning multigraph} (kECSM) problem, where multiple copies of an edge can be\nselected: we obtain a $(1+2/k)$-approximation algorithm for even $k$, and a\n$(1+3/k)$-approximation algorithm for odd $k$.\n  Our techniques extend to the degree-bounded versions of kECSS and kECSM,\nwherein we also impose degree lower- and upper- bounds on the nodes. We obtain\nthe same cost and connectivity guarantees for these degree-bounded versions\nwith an additive violation of (roughly) $2$ for the degree bounds. These are\nthe first results for degree-bounded \\{kECSS,kECSM\\} of the form where the cost\nof the solution obtained is at most the optimum, and the connectivity\nconstraints are violated by an additive constant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20906v1", "categories": ["cs.DS", "F.2.2; G.2"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.20906v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于 k-边连通性几乎紧密的加性保证", "tldr": "本文为k-边连通生成子图（kECSS）问题及其变体提供了新的近似算法，在成本接近最优解的同时，实现了几乎最优的边连通性。", "motivation": "k-边连通生成子图（kECSS）问题对于所有 k≥2 都是 APX-hard 的。现有方法在解质量和算法简洁性方面仍有改进空间。本文旨在显著改进现有结果。", "method": "本文提出了一种多项式时间算法来解决 kECSS 问题。该算法利用 LP 松弛的性质，并扩展到处理 k-边连通生成多图（kECSM）问题以及带有度数限制的版本。", "result": "对于偶数 k，算法计算出一个 (k-2)-边连通子图，其成本至多为自然 LP 松弛的最优值 LP*；对于奇数 k，获得一个 (k-3)-边连通子图，成本至多为 LP*。这些结果在边连通性方面接近最优。此外，还获得了成本至多为 1.5·LP* 的 (k-1)-边连通子图；对于单位边成本，成本保证改进到 (1+4/(3k))·LP*。对于 kECSM 问题，偶数 k 获得 (1+2/k)-近似算法，奇数 k 获得 (1+3/k)-近似算法。对于度数限制版本，获得了相同的成本和连通性保证，度数限制的加性违反（大约）为 2。", "conclusion": "本文为 kECSS 及其变体问题提供了几乎最优的加性保证，显著改进了现有技术，特别是在解质量和算法简洁性方面。这些是首次对于度数限制的 {kECSS, kECSM} 形式的问题，其解的成本至多为最优值，并且连通性约束以加性常数违反。", "translation": "我们考虑 k-边连通生成子图（kECSS）问题，给定一个无向图 G = (V, E)，边成本为非负值 {ce}e∈E，我们寻求一个最小成本的 k-边连通子图 H。对于偶数 k，我们提出了一种多项式时间算法，计算出一个 (k-2)-边连通子图，其成本至多为 kECSS 的自然 LP 松弛的最优值 LP*；对于奇数 k，我们获得一个 (k-3)-边连通子图，成本至多为 LP*。由于 kECSS 对于所有 k≥2 都是 APX-hard 的，我们的结果几乎是最优的。它们还在解质量和算法及其分析的简洁性方面显著改进了 Hershkowitz 等人的最新工作。我们的技术还提供了一种替代保证，即我们获得一个 (k-1)-边连通子图，成本至多为 1.5·LP*；对于单位边成本，成本保证改进到 (1+4/(3k))·LP*，这改进了单位边成本的最新近似，但边连通性损失了一个单位。我们的 kECSS 结果也为 k-边连通生成多图（kECSM）问题带来了结果，其中可以选择边的多个副本：我们为偶数 k 获得了 (1+2/k)-近似算法，为奇数 k 获得了 (1+3/k)-近似算法。我们的技术扩展到 kECSS 和 kECSM 的度数限制版本，其中我们还对节点施加了度数下限和上限。我们为这些度数限制版本获得了相同的成本和连通性保证，度数限制的加性违反（大约）为 2。这些是首次对于度数限制的 {kECSS, kECSM} 形式的结果，其中获得的解的成本至多为最优值，并且连通性约束以加性常数违反。", "summary": "本文研究了 k-边连通生成子图 (kECSS) 问题，提出了一种多项式时间算法。该算法对于偶数 k 实现了 (k-2)-边连通性，对于奇数 k 实现了 (k-3)-边连通性，且成本均不高于 LP 松弛的最优值 LP*，这些结果在边连通性上接近最优。此外，文章还提供了在成本为 1.5·LP* 时实现 (k-1)-边连通性的替代保证，并针对单位边成本进行了优化。研究成果也扩展到 k-边连通生成多图 (kECSM) 问题，并为度数限制版本的 kECSS 和 kECSM 提供了近似保证，这是该领域内的首次此类结果。", "keywords": "k-边连通性, 生成子图, 近似算法, LP 松弛, 度数限制", "comments": "该论文在 k-边连通性问题的近似算法领域取得了显著进展，尤其是在提供几乎紧密的加性保证方面。其创新之处在于算法的简洁性和对现有结果的显著改进，并且首次将此类保证扩展到度数限制版本，具有重要的理论和实践意义。"}}
{"id": "2506.21448", "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing", "authors": ["Huadai Liu", "Jialei Wang", "Kaicheng Luo", "Wen Wang", "Qian Chen", "Zhou Zhao", "Wei Xue"], "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\n\\textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT)\nreasoning to enable stepwise, interactive audio generation and editing for\nvideos. Our approach decomposes the process into three complementary stages:\nfoundational foley generation that creates semantically coherent soundscapes,\ninteractive object-centric refinement through precise user interactions, and\ntargeted editing guided by natural language instructions. At each stage, a\nmultimodal large language model generates contextually aligned CoT reasoning\nthat guides a unified audio foundation model. Furthermore, we introduce\n\\textbf{AudioCoT}, a comprehensive dataset with structured reasoning\nannotations that establishes connections between visual content, textual\ndescriptions, and sound synthesis. Experiments demonstrate that ThinkSound\nachieves state-of-the-art performance in video-to-audio generation across both\naudio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio\nbenchmark. The demo page is available at https://ThinkSound-Demo.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21448v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.21448v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ThinkSound：多模态大型语言模型中的思维链推理用于音频生成与编辑", "tldr": "ThinkSound利用思维链推理在多模态大语言模型中实现视频到音频的生成和编辑，并引入AudioCoT数据集，达到SOTA性能。", "motivation": "尽管端到端视频到音频生成有很大进步，但生成高质量、能真实捕捉视觉内容细微差别的音频仍然具有挑战性，这需要对视觉动态、声学环境和时间关系进行复杂的推理。", "method": "提出ThinkSound框架，利用思维链（CoT）推理实现分步、交互式视频音频生成和编辑。该方法分解为三个阶段：基础拟音生成、交互式以对象为中心的细化、以及自然语言指导的定向编辑。每个阶段，多模态大语言模型生成上下文对齐的CoT推理，指导统一的音频基础模型。此外，引入AudioCoT数据集，包含结构化推理标注，连接视觉内容、文本描述和声音合成。", "result": "ThinkSound在视频到音频生成方面，无论是在音频指标还是CoT指标上，都达到了最先进的性能。在分布外Movie Gen Audio基准测试中表现出色。", "conclusion": "ThinkSound通过引入思维链推理和分阶段方法，显著提升了视频到音频生成和编辑的质量和交互性，并引入了AudioCoT数据集，为该领域提供了新的SOTA解决方案。", "translation": "尽管端到端视频到音频生成已大大改进，但生成能真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。就像创意行业的专业人士一样，这种生成需要对视觉动态、声学环境和时间关系等项目进行复杂的推理。我们提出了 ThinkSound，一个新颖的框架，它利用思维链（CoT）推理来实现视频的分步、交互式音频生成和编辑。我们的方法将该过程分解为三个互补的阶段：创建语义连贯声景的基础拟音生成，通过精确用户交互进行的交互式以对象为中心的细化，以及由自然语言指令引导的定向编辑。在每个阶段，多模态大型语言模型生成与上下文对齐的CoT推理，以指导统一的音频基础模型。此外，我们引入了 AudioCoT，一个包含结构化推理标注的综合数据集，它建立了视觉内容、文本描述和声音合成之间的联系。实验表明，ThinkSound 在视频到音频生成方面，无论是在音频指标还是CoT指标上，都达到了最先进的性能，并在分布外 Movie Gen Audio 基准测试中表现出色。演示页面可在 https://ThinkSound-Demo.github.io 获取。", "summary": "ThinkSound是一个新颖的框架，利用多模态大型语言模型中的思维链推理，实现了视频到音频的分步、交互式生成与编辑。它将生成过程分解为基础拟音、交互式对象细化和自然语言引导编辑三个阶段，并在每个阶段通过CoT推理指导音频模型。论文还引入了包含结构化推理标注的AudioCoT数据集。实验证明ThinkSound在视频到音频生成方面达到了SOTA性能。", "keywords": "音频生成, 思维链, 多模态大语言模型, 视频到音频, 音频编辑", "comments": "该论文的创新点在于将思维链（CoT）推理引入多模态大语言模型，以解决视频到音频生成中高质量音频合成的复杂推理需求。其分阶段的交互式方法提升了生成过程的精细度和用户控制力。引入的AudioCoT数据集也为后续研究提供了宝贵的资源，推动了该领域的进步。"}}
{"id": "2506.20978", "title": "Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality", "authors": ["Naihe Feng", "Yi Sui", "Shiyi Hou", "Jesse C. Cresswell", "Ga Wu"], "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.", "comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment", "pdf_url": "http://arxiv.org/pdf/2506.20978v1", "categories": ["cs.IR", "H.3.3"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.20978v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过条件共形事实性评估检索增强生成响应质量", "tldr": "Conformal-RAG利用共形预测为检索增强生成（RAG）的响应质量，尤其是子声明的质量，提供统计保证，且无需真实答案。", "motivation": "现有检索增强生成（RAG）研究主要关注整体问答准确性，但忽视了生成响应中子声明的质量。此外，当前的RAG自动评估方法缺乏概率保证或需要真实答案。", "method": "本文提出了Conformal-RAG框架，该框架受共形预测（CP）在大型语言模型（LLM）上应用的启发。Conformal-RAG利用CP和RAG机制的内部信息，为响应质量提供统计保证，并确保跨多个子领域的组条件覆盖，无需手动标记共形集。", "result": "Conformal-RAG在不依赖真实答案的情况下，对精炼子声明的质量提供统计保证，确保响应可靠性。与直接将CP应用于LLM相比，Conformal-RAG在保持相同可靠性保证的前提下，保留了高达60%的更多高质量子声明。", "conclusion": "Conformal-RAG通过利用共形预测和RAG内部信息，有效解决了RAG响应（特别是子声明）的质量评估和可靠性保证问题，提供了统计保证且无需真实答案，表现优于直接的CP应用。", "translation": "现有关于检索增强生成（RAG）的研究主要集中于提高整体问答准确性，但往往忽视了生成响应中子声明的质量。最近旨在提高RAG可信度的方法，例如通过自动评估指标，缺乏概率保证或需要真实答案。为了解决这些限制，我们提出了Conformal-RAG，一个受共形预测（CP）在大型语言模型（LLM）上最新应用启发的Nove框架。Conformal-RAG利用CP和RAG机制的内部信息来提供响应质量的统计保证。它确保跨多个子领域的组条件覆盖，而无需手动标记共形集，使其适用于复杂的RAG应用。与现有RAG自动评估方法相比，Conformal-RAG对精炼子声明的质量提供统计保证，确保响应可靠性而无需真实答案。此外，我们的实验表明，通过利用RAG系统的信息，Conformal-RAG比直接将CP应用于LLM保留了高达60%的更多高质量子声明，同时保持相同的可靠性保证。", "summary": "本文提出了Conformal-RAG，一个用于评估检索增强生成（RAG）响应中子声明质量的新框架。该方法通过应用共形预测（CP）并利用RAG系统的内部信息，为响应质量提供统计保证，解决了现有方法缺乏概率保证或需要真实答案的局限性。实验证明，Conformal-RAG在保持可靠性的同时，比直接将CP应用于大型语言模型保留了更多高质量的子声明。", "keywords": "检索增强生成, 共形预测, 响应质量, 事实性, 统计保证", "comments": "该论文的创新之处在于将共形预测应用于RAG，以提供响应（特别是子声明）质量的统计保证，解决了在没有真实答案的情况下评估RAG可信度的关键问题。这对于RAG系统在需要高可靠性的应用中部署具有重要意义。"}}
{"id": "2506.21025", "title": "An energy-stable parametric finite element method for the Willmore flow in three dimensions", "authors": ["Weizhu Bao", "Yifei Li", "Dongmin Wang"], "summary": "This work develops novel energy-stable parametric finite element methods\n(ES-PFEM) for the Willmore flow and curvature-dependent geometric gradient\nflows of surfaces in three dimensions. The key to achieving the energy\nstability lies in the use of two novel geometric identities: (i) a reformulated\nvariational form of the normal velocity field, and (ii) incorporation of the\ntemporal evolution of the mean curvature into the governing equations. These\nidentities enable the derivation of a new variational formulation. By using the\nparametric finite element method, an implicit fully discrete scheme is\nsubsequently developed, which maintains the energy dissipative property at the\nfully discrete level. Based on the ES-PFEM, comprehensive insights into the\ndesign of ES-PFEM for general curvature-dependent geometric gradient flows and\na new understanding of mesh quality improvement in PFEM are provided. In\nparticular, we develop the first PFEM for the Gauss curvature flow of surfaces.\nFurthermore, a tangential velocity control methodology is applied to improve\nthe mesh quality and enhance the robustness of the proposed numerical method.\nExtensive numerical experiments confirm that the proposed method preserves\nenergy dissipation properties and maintain good mesh quality in the surface\nevolution under the Willmore flow.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21025v1", "categories": ["math.NA", "cs.NA", "65M60, 65M12, 35K55, 53C44"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21025v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "三维Willmore流的能量稳定参数有限元方法", "tldr": "本文提出了一种用于三维Willmore流和曲率相关几何梯度流的能量稳定参数有限元方法（ES-PFEM），通过引入新的几何恒等式和切向速度控制，实现了能量耗散性和良好的网格质量。", "motivation": "为了解决三维Willmore流和曲率相关几何梯度流的数值模拟问题，并实现能量稳定性。", "method": "开发了基于两个新几何恒等式（法向速度场的重新表述变分形式和将平均曲率时间演化纳入控制方程）的能量稳定参数有限元方法（ES-PFEM）。该方法采用隐式全离散方案，并应用切向速度控制方法以改善网格质量和增强数值方法的鲁棒性。特别地，开发了首个用于高斯曲率流的PFEM。", "result": "所提出的方法在全离散层面保持了能量耗散特性，并在Willmore流下的表面演化中保持了良好的网格质量。数值实验证实了该方法的有效性。", "conclusion": "本文提出的能量稳定参数有限元方法能够有效且稳定地模拟三维Willmore流和曲率相关几何梯度流，同时保持能量耗散性和良好的网格质量。", "translation": "这项工作开发了用于三维Willmore流和曲率相关几何梯度流的能量稳定参数有限元方法（ES-PFEM）。实现能量稳定性的关键在于使用了两个新颖的几何恒等式：（i）法向速度场的重新表述变分形式，以及（ii）将平均曲率的时间演化纳入控制方程。这些恒等式使得能够推导出新的变分公式。通过使用参数有限元方法，随后开发了一个隐式全离散方案，该方案在全离散层面保持了能量耗散特性。基于ES-PFEM，本文提供了关于通用曲率相关几何梯度流的ES-PFEM设计以及PFEM中网格质量改进的新理解的全面见解。特别是，我们开发了首个用于曲面高斯曲率流的PFEM。此外，还应用了切向速度控制方法来改善网格质量并增强所提出数值方法的鲁棒性。大量的数值实验证实，所提出的方法在Willmore流下的表面演化中保持了能量耗散特性并维持了良好的网格质量。", "summary": "本研究提出了一种用于三维Willmore流和曲率相关几何梯度流的能量稳定参数有限元方法（ES-PFEM）。该方法通过引入两个新的几何恒等式，推导出了新的变分公式，并开发了一个隐式全离散方案，确保了能量耗散性。此外，还应用了切向速度控制技术来提高网格质量和增强方法的鲁棒性。数值实验验证了该方法在保持能量耗散特性的同时，能在表面演化过程中维持良好的网格质量。", "keywords": "Willmore流, 参数有限元方法, 能量稳定, 几何梯度流, 网格质量", "comments": "该论文的创新点在于引入了两个新的几何恒等式，从而实现了Willmore流和曲率相关几何梯度流的能量稳定数值模拟。这是首次将PFEM应用于高斯曲率流，并且通过切向速度控制显著提升了网格质量和方法的鲁棒性，对于三维几何流的数值计算具有重要意义。"}}
{"id": "2506.20822", "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "authors": ["Quintin Myers", "Yanjun Gao"], "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20822v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20822v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "揭示LLM中隐藏的暴力倾向：基于行为情景的人口统计学分析", "tldr": "大型语言模型（LLMs）在处理道德模糊的日常冲突时，表现出隐藏的、与人口统计学相关的暴力倾向，且其表面输出与内部偏好不一致，这与现有社会科学发现相悖。", "motivation": "尽管大型语言模型（LLMs）被提议用于检测和响应在线暴力内容，但它们在道德模糊的现实场景中进行推理的能力尚未得到充分检验。", "method": "本研究首次使用经过验证的社会科学工具“暴力行为情景问卷（VBVQ）”来评估LLMs对日常冲突的反应。为评估潜在偏见，研究引入了基于角色的提示，其中包含美国境内不同种族、年龄和地理身份的变量。在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLM。", "result": "研究揭示了两个关键发现：(1) LLM的表面文本生成与其内部对暴力反应的偏好常常存在差异；(2) 它们的暴力倾向因人口统计学特征而异，这经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "conclusion": "LLMs在处理道德模糊场景时存在隐藏的暴力倾向，并且这些倾向表现出人口统计学上的偏见，甚至与人类社会科学的既有发现相悖。", "translation": "大型语言模型（LLMs）正越来越多地被提议用于检测和响应在线暴力内容，然而，它们在道德模糊的现实场景中进行推理的能力仍未得到充分检验。我们首次提出一项研究，使用经过验证的社会科学工具——暴力行为情景问卷（VBVQ）来评估LLMs，该问卷旨在衡量人类对日常冲突的反应。为了评估潜在偏见，我们引入了基于角色的提示，其中包含美国境内不同种族、年龄和地理身份的变量。在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLM。我们的研究揭示了两个关键发现：(1) LLM的表面文本生成与其内部对暴力反应的偏好常常存在差异；(2) 它们的暴力倾向因人口统计学特征而异，这经常与犯罪学、社会科学和心理学中已有的发现相矛盾。", "summary": "本研究首次利用社会科学工具“暴力行为情景问卷（VBVQ）”评估了大型语言模型（LLMs）在道德模糊情景下的隐藏暴力倾向。通过引入基于人口统计学（种族、年龄、地理身份）的提示，研究发现LLMs的表面输出与内部暴力偏好存在差异，且其暴力倾向表现出人口统计学偏见，这与已有的社会科学研究结果相悖。", "keywords": "LLM, 暴力倾向, 人口统计学分析, 行为情景, 偏见", "comments": "这项研究创新性地将社会科学的测量工具应用于LLM评估，揭示了LLM在处理复杂道德情景时可能存在的深层偏见和不一致性，为理解和改进LLM的伦理行为提供了重要视角。"}}
{"id": "2506.21298", "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation", "authors": ["Atharva Mehta", "Shivam Chauhan", "Monojit Choudhury"], "summary": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21298v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21298v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "探索低资源音乐生成中的适配器设计权衡", "tldr": "本文研究了两种大型音乐AI模型（MusicGen和Mustango）在低资源音乐类型上采用适配器进行参数高效微调时的设计权衡，发现不同适配器类型和大小对音乐生成质量和效率有显著影响，并对比了两种模型架构的优缺点。", "motivation": "微调大型音乐生成模型计算成本高昂，需要大量硬件资源。尽管参数高效微调（PEFT）特别是基于适配器的方法前景广阔，但对于低资源音乐类型，适配器的最佳设计（包括架构、放置和大小）尚不明确。", "method": "通过研究两种AI音乐模型（MusicGen和Mustango）在两种低资源音乐流派（印度斯坦古典音乐和土耳其马卡姆音乐）上的各种适配器配置来探索其设计权衡。", "result": "卷积适配器擅长捕捉精细的局部音乐细节（如装饰音和短旋律片段），而Transformer适配器能更好地保持对结构化即兴创作至关重要的长程依赖。中等大小适配器（40M参数）在表现力和质量之间达到最佳平衡。扩散模型Mustango生成多样性更高且更符合输入提示，但在音符稳定性、节奏对齐和美学方面有所欠缺，且计算量大、训练时间长。自回归模型MusicGen训练更快、效率更高、质量更好，但其生成内容略有冗余。", "conclusion": "不同的适配器设计在捕捉音乐细节和长程依赖方面存在显著权衡，中等大小的适配器能在计算效率和生成质量之间提供最佳平衡。此外，不同的基础模型架构（如扩散模型和自回归模型）在生成多样性、质量、计算效率和训练时间方面也表现出不同的特性。", "translation": "微调大型音乐生成模型，如MusicGen和Mustango，是一个计算成本高昂的过程，通常需要更新数十亿参数，因此需要大量的硬件资源。参数高效微调（PEFT）技术，特别是基于适配器的方法，已成为一种有前景的替代方案，它能够以最少的训练参数进行适应，同时保持模型性能。然而，适配器的设计选择，包括其架构、位置和大小，数量众多，目前尚不清楚对于给定的低资源音乐类型，这些组合中的哪一个能产生最佳适配器以及原因。在本文中，我们试图通过研究两种AI音乐模型（MusicGen和Mustango）在两种流派：印度斯坦古典音乐和土耳其马卡姆音乐上的各种适配器配置来回答这个问题。\n我们的发现揭示了明显的权衡：基于卷积的适配器擅长捕捉精细的局部音乐细节，如装饰音和短旋律片段，而基于Transformer的适配器能更好地保持对结构化即兴创作至关重要的长程依赖。此外，我们分析了不同适配器规模下的计算资源需求，表明中等大小的适配器（40M参数）在表现力和质量之间达到了最佳平衡。此外，我们发现基于扩散模型的Mustango在生成更多样化的输出并更好地符合输入提示方面表现出色，但在提供音符稳定性、节奏对齐和美学方面有所欠缺。而且，它的计算量大，需要更长的训练时间。相比之下，像MusicGen这样的自回归模型提供更快的训练速度和更高的效率，并且可以生成更高质量的输出，但其生成内容略有冗余。", "summary": "本文探讨了在低资源音乐生成中，大型音乐模型（MusicGen和Mustango）采用参数高效微调（PEFT）技术时，不同适配器设计（架构、位置、大小）的权衡。研究发现，卷积适配器擅长捕捉局部细节，Transformer适配器擅长长程依赖，中等大小适配器（40M参数）在表现力和质量间达到最佳平衡。同时，对比了MusicGen（自回归）和Mustango（扩散）在生成多样性、质量、效率和资源消耗方面的差异，为低资源音乐生成提供了适配器选择和模型特性的指导。", "keywords": "音乐生成, 适配器, 参数高效微调, 低资源, MusicGen, Mustango", "comments": "这项研究通过系统地探索适配器设计在低资源音乐生成中的权衡，为参数高效微调提供了有价值的见解。其创新之处在于对比了不同适配器类型（卷积与Transformer）对音乐细节和结构的影响，并量化了适配器大小对性能和资源的影响。此外，对两种不同架构的基础模型（自回归与扩散）的）比较也很有意义。这些发现对于优化音乐生成模型的部署和训练效率具有重要指导意义，尤其是在计算资源受限的场景。"}}
{"id": "2506.21069", "title": "TEMPEST-LoRa: Cross-Technology Covert Communication", "authors": ["Xieyang Sun", "Yuanqing Zheng", "Wei Xi", "Zuhao Chen", "Zhizhen Chen", "Han Hao", "Zhiping Jiang", "Sheng Zhong"], "summary": "Electromagnetic (EM) covert channels pose significant threats to computer and\ncommunications security in air-gapped networks. Previous works exploit EM\nradiation from various components (e.g., video cables, memory buses, CPUs) to\nsecretly send sensitive information. These approaches typically require the\nattacker to deploy highly specialized receivers near the victim, which limits\ntheir real-world impact. This paper reports a new EM covert channel,\nTEMPEST-LoRa, that builds on Cross-Technology Covert Communication (CTCC),\nwhich could allow attackers to covertly transmit EM-modulated secret data from\nair-gapped networks to widely deployed operational LoRa receivers from afar. We\nreveal the potential risk and demonstrate the feasibility of CTCC by tackling\npractical challenges involved in manipulating video cables to precisely\ngenerate the EM leakage that could readily be received by third-party\ncommercial LoRa nodes/gateways. Experiment results show that attackers can\nreliably decode secret data modulated by the EM leakage from a video cable at a\nmaximum distance of 87.5m or a rate of 21.6 kbps. We note that the secret data\ntransmission can be performed with monitors turned off (therefore covertly).", "comment": "15 pages, 19 figures, and this paper has been accepted to ACM CCS\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21069v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21069v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TEMPEST-LoRa：跨技术秘密通信", "tldr": "本文提出并验证了一种名为TEMPEST-LoRa的新型电磁秘密信道，它利用视频线缆的电磁泄漏，使攻击者能够将秘密数据从气隙网络 covertly 传输到远距离的LoRa接收器，无需专用接收器。", "motivation": "电磁（EM）秘密信道对气隙网络中的计算机和通信安全构成重大威胁。以往的工作需要攻击者在受害者附近部署高度专业化的接收器，这限制了其现实世界中的影响。本文旨在解决这一限制，提出一种利用广泛部署的LoRa接收器进行秘密通信的方法。", "method": "本文提出了一种名为TEMPEST-LoRa的新型电磁秘密信道，该信道基于跨技术秘密通信（CTCC）。通过操纵视频线缆精确生成电磁泄漏，使其能够被第三方商用LoRa节点/网关接收，从而实现从气隙网络到LoRa接收器的电磁调制秘密数据传输。", "result": "实验结果表明，攻击者能够可靠地解码视频线缆电磁泄漏调制产生的秘密数据，最大距离可达87.5米，或传输速率达到21.6 kbps。秘密数据传输可以在显示器关闭的情况下进行，从而实现隐蔽性。", "conclusion": "本文揭示了跨技术秘密通信（CTCC）的潜在风险，并论证了其可行性，表明利用现有商用LoRa设备进行远距离电磁秘密通信是可行的。", "translation": "电磁（EM）秘密信道对气隙网络中的计算机和通信安全构成重大威胁。以往的工作利用各种组件（例如，视频线缆、内存总线、CPU）的电磁辐射来秘密发送敏感信息。这些方法通常要求攻击者在受害者附近部署高度专业化的接收器，这限制了它们在现实世界中的影响。本文报告了一种新型电磁秘密信道，TEMPEST-LoRa，它建立在跨技术秘密通信（CTCC）之上，可以允许攻击者将电磁调制的秘密数据从气隙网络秘密传输到远距离广泛部署的操作性LoRa接收器。我们通过解决操纵视频线缆以精确生成电磁泄漏所涉及的实际挑战，揭示了CTCC的潜在风险并证明了其可行性，这些泄漏可以轻易地被第三方商用LoRa节点/网关接收。实验结果表明，攻击者能够可靠地解码视频线缆电磁泄漏调制产生的秘密数据，最大距离可达87.5米，或传输速率达到21.6 kbps。我们注意到，秘密数据传输可以在显示器关闭的情况下进行（因此是秘密的）。", "summary": "本文提出并验证了一种名为TEMPEST-LoRa的新型电磁秘密信道，它利用视频线缆的电磁泄漏，实现了从气隙网络到远距离LoRa接收器的秘密数据传输。与以往需要专用接收器的方法不同，TEMPEST-LoRa利用了广泛部署的商用LoRa设备，显著提高了秘密通信的现实可行性。实验证明，该方法能在最远87.5米或最高21.6 kbps的速率下可靠解码数据，且传输可在显示器关闭时进行，增强了隐蔽性。", "keywords": "电磁秘密信道, 跨技术通信, LoRa, 气隙网络, TEMPEST", "comments": "本文的创新之处在于提出了跨技术秘密通信（CTCC）的概念，并将其应用于电磁秘密信道，特别是利用LoRa接收器。这解决了传统电磁秘密信道需要专用高成本接收设备的限制，极大地拓展了其潜在的攻击面和现实威胁。其重要性在于揭示了一种新的、更易于实施的对气隙网络进行数据窃取的潜在威胁，对信息安全领域具有重要的警示意义。通过利用现有商用技术，该研究展示了高级持续性威胁（APT）的新方向。"}}
{"id": "2506.20969", "title": "ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation", "authors": ["Shruti Bansal", "Wenshan Wang", "Yifei Liu", "Parv Maheshwari"], "summary": "Autonomous systems rely on sensors to estimate the environment around them.\nHowever, cameras, LiDARs, and RADARs have their own limitations. In nighttime\nor degraded environments such as fog, mist, or dust, thermal cameras can\nprovide valuable information regarding the presence of objects of interest due\nto their heat signature. They make it easy to identify humans and vehicles that\nare usually at higher temperatures compared to their surroundings. In this\npaper, we focus on the adaptation of thermal cameras for robotics and\nautomation, where the biggest hurdle is the lack of data. Several multi-modal\ndatasets are available for driving robotics research in tasks such as scene\nsegmentation, object detection, and depth estimation, which are the cornerstone\nof autonomous systems. However, they are found to be lacking in thermal\nimagery. Our paper proposes a solution to augment these datasets with synthetic\nthermal data to enable widespread and rapid adaptation of thermal cameras. We\nexplore the use of conditional diffusion models to convert existing RGB images\nto thermal images using self-attention to learn the thermal properties of\nreal-world objects.", "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2506.20969v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.20969v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ThermalDiffusion: 视觉到热像的图像到图像转换用于自主导航", "tldr": "本文提出使用条件扩散模型将RGB图像转换为热像图，以解决自主导航中热像数据缺乏的问题，从而增强自主系统在恶劣环境下的感知能力。", "motivation": "自主系统依赖传感器感知环境，但传统传感器在夜间或恶劣环境下存在局限性。热像仪能提供有价值的信息，尤其易于识别热源，但机器人和自动化领域面临热像数据缺乏的障碍，现有许多多模态数据集缺少热像数据。", "method": "提出使用条件扩散模型将现有RGB图像转换为热像图，利用自注意力机制学习真实物体的热特性，从而生成合成热像数据来扩充现有数据集。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "自主系统依赖传感器来估计周围环境。然而，相机、激光雷达和雷达都有其局限性。在夜间或恶劣环境（如雾、霾或灰尘）中，热像仪由于物体的热特征，可以提供有关目标物体存在的宝贵信息。它们使得识别通常比周围环境温度更高的人类和车辆变得容易。在本文中，我们关注热像仪在机器人和自动化领域的应用，其中最大的障碍是数据的缺乏。有几个多模态数据集可用于驾驶机器人研究，涉及场景分割、目标检测和深度估计等任务，这些都是自主系统的基石。然而，这些数据集被发现缺乏热像数据。我们的论文提出了一种解决方案，通过合成热像数据来扩充这些数据集，以实现热像仪的广泛和快速应用。我们探索使用条件扩散模型将现有RGB图像转换为热像图，利用自注意力机制学习真实物体的热特性。", "summary": "本文提出了一种名为ThermalDiffusion的方法，旨在解决自主导航中热像数据缺乏的问题。该方法利用条件扩散模型，通过学习物体的热特性，将现有RGB图像转换为合成热像图，从而扩充现有的多模态数据集，促进热像仪在机器人和自动化领域的广泛应用，特别是在夜间或恶劣环境下的感知任务。", "keywords": "热像仪, 图像到图像转换, 自主导航, 条件扩散模型, 数据增强", "comments": "该论文的创新点在于利用条件扩散模型生成合成热像数据，从而弥补了现有自动驾驶数据集中热像数据不足的缺陷。这对于提升自主系统在低能见度环境下的感知能力具有重要意义。通过合成数据扩充，可以降低热像仪应用的门槛，加速其在机器人和自动化领域的推广。"}}
{"id": "2506.21014", "title": "Boosting Vulnerability Detection with Inter-function Multilateral Association Insights", "authors": ["Shaojian Qiu", "Mengyang Huang", "Jiahao Cheng"], "summary": "Vulnerability detection is a crucial yet challenging technique for ensuring\nthe security of software systems. Currently, most deep learning-based\nvulnerability detection methods focus on stand-alone functions, neglecting the\ncomplex inter-function interrelations, particularly the multilateral\nassociations. This oversight can fail to detect vulnerabilities in these\ninterrelations. To address this gap, we present an Inter-Function Multilateral\nAssociation analysis framework for Vulnerability Detection (IFMA-VD). The\ncornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and\nutilizing hyperedge convolution to extract multilateral association features.\nSpecifically, we first parse functions into a code property graph to generate\nintra-function features. Following this, we construct a code behavior\nhypergraph by segmenting the program dependency graph to isolate and encode\nbehavioral features into hyperedges. Finally, we utilize a hypergraph network\nto capture the multilateral association knowledge for augmenting vulnerability\ndetection. We evaluate IFMA-VD on three widely used vulnerability datasets and\ndemonstrate improvements in F-measure and Recall compared to baseline methods.\nAdditionally, we illustrate that multilateral association features can boost\ncode feature representation and validate the effectiveness of IFMA-VD on\nreal-world datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21014v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21014v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用函数间多边关联洞察提升漏洞检测", "tldr": "现有深度学习漏洞检测方法忽略函数间多边关联导致漏报。本文提出IFMA-VD框架，通过构建代码行为超图和超边卷积来捕捉这些关联，显著提升了漏洞检测的F-measure和召回率，并在真实世界数据集中验证了其有效性。", "motivation": "当前大多数基于深度学习的漏洞检测方法侧重于独立函数，忽略了复杂的函数间相互关联，特别是多边关联，这可能导致无法检测到存在于这些关联中的漏洞。", "method": "本文提出了一个用于漏洞检测的函数间多边关联分析框架（IFMA-VD）。该框架的核心是构建代码行为超图并利用超边卷积来提取多边关联特征。具体步骤包括：首先，将函数解析为代码属性图以生成函数内特征；接着，通过分割程序依赖图构建代码行为超图，将行为特征编码到超边中；最后，利用超图网络捕获多边关联知识以增强漏洞检测。", "result": "IFMA-VD在三个广泛使用的漏洞数据集上进行了评估，与基线方法相比，F-measure和召回率均有所提高。此外，研究表明多边关联特征可以提升代码特征表示，并验证了IFMA-VD在真实世界数据集上的有效性。", "conclusion": "通过构建代码行为超图和利用超边卷积来提取函数间多边关联特征，IFMA-VD框架能够显著提升深度学习在漏洞检测方面的性能，并有效增强代码特征表示。", "translation": "漏洞检测是确保软件系统安全的关键但具有挑战性的技术。目前，大多数基于深度学习的漏洞检测方法侧重于独立函数，忽略了复杂的函数间相互关联，特别是多边关联。这种疏忽可能导致无法检测到存在于这些相互关联中的漏洞。为了解决这一差距，我们提出了一个用于漏洞检测的函数间多边关联分析框架（IFMA-VD）。IFMA-VD的基石在于构建代码行为超图并利用超边卷积来提取多边关联特征。具体来说，我们首先将函数解析为代码属性图以生成函数内特征。随后，我们通过分割程序依赖图来构建代码行为超图，以隔离并将行为特征编码到超边中。最后，我们利用超图网络捕获多边关联知识以增强漏洞检测。我们在三个广泛使用的漏洞数据集上评估了IFMA-VD，并展示了与基线方法相比，F-measure和召回率的提高。此外，我们还说明了多边关联特征可以提升代码特征表示，并验证了IFMA-VD在真实世界数据集上的有效性。", "summary": "本文提出了一种名为IFMA-VD的新型框架，旨在通过解决当前深度学习漏洞检测方法中被忽视的函数间多边关联问题来提升检测能力。IFMA-VD的核心在于构建代码行为超图并利用超边卷积来提取这些复杂的关联特征，并将其与传统的函数内特征相结合。在多个广泛使用的漏洞数据集上的评估结果表明，IFMA-VD在F-measure和召回率方面均优于现有基线方法，并且能够有效提升代码特征表示，证明了其在真实世界场景中的实用性和有效性。", "keywords": "漏洞检测, 函数间关联, 超图, 深度学习, 代码分析", "comments": "本文的创新点在于首次将超图和超边卷积引入到漏洞检测领域，以捕捉传统方法难以处理的函数间多边关联。这种方法弥补了现有深度学习模型在处理复杂跨函数漏洞方面的不足，为提升漏洞检测的全面性和准确性提供了新的视角和有效的解决方案。其对真实世界数据集的有效性验证也增加了其实用价值。"}}
{"id": "2506.21319", "title": "Multimodal LLMs for Visualization Reconstruction and Understanding", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21319v1", "categories": ["cs.HC", "cs.CV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21319v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多模态大语言模型用于可视化重建与理解", "tldr": "本文提出了一种新的数据集和训练方法，以使多模态大语言模型（MLLMs）能够更好地理解和重建数据可视化图表，解决了现有MLLMs在处理图表时无法解码数据到视觉映射规则和提取结构化信息的问题，并取得了显著的改进。", "motivation": "可视化对于数据交流至关重要，但理解它们需要同时理解视觉元素及其底层数据关系。当前的多模态大模型在自然图像理解方面表现出色，但在处理可视化图表时却面临困难，因为它们无法解码数据到视觉的映射规则并提取结构化信息。", "method": "为了解决这些挑战，本文提出并训练了一种专门用于理解可视化的新型多模态可视化大语言模型。该方法将图表图像与相应的矢量化表示、编码方案和数据特征相结合。所提出的矢量格式能够实现可视化内容的紧凑和准确重建。", "result": "实验结果表明，在数据提取准确性和图表重建质量方面都有显著的改进。", "conclusion": "通过引入专门训练的多模态可视化大语言模型和新的数据集，可以有效提升大模型对可视化图表的理解、数据提取和重建能力。", "translation": "可视化对于数据交流至关重要，然而理解它们需要同时理解视觉元素及其底层数据关系。当前的多模态大模型虽然在自然图像理解方面表现出色，但由于它们无法解码数据到视觉的映射规则并提取结构化信息，因此在处理可视化图表时面临困难。为了解决这些挑战，我们提出了一种新的数据集并训练了专门用于理解可视化的多模态可视化大语言模型。我们的方法将图表图像与相应的矢量化表示、编码方案和数据特征相结合。所提出的矢量格式能够实现可视化内容的紧凑和准确重建。实验结果表明，在数据提取准确性和图表重建质量方面都有显著的改进。", "summary": "本文针对当前多模态大语言模型（MLLMs）在理解和重建数据可视化图表方面的局限性，提出了一种创新方法。研究者构建了一个新颖的数据集，并训练了专门用于可视化理解的MLLMs。该方法结合了图表图像、其矢量化表示、编码方案和数据特征，并利用一种新的矢量格式实现紧凑且准确的重建。实验证明，该方法显著提升了数据提取的准确性和图表重建的质量。", "keywords": "多模态大语言模型, 数据可视化, 图表重建, 数据提取, 矢量化表示", "comments": "该论文通过引入专门的数据集和训练方法，有效解决了当前多模态大语言模型在理解复杂数据可视化图表方面的核心挑战。其创新点在于结合了图表图像与结构化的矢量表示及数据特征，从而使模型能够更好地理解数据到视觉的映射规则。这对于提升AI在数据分析和报告生成等领域的应用具有重要意义。"}}
{"id": "2506.21093", "title": "Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection", "authors": ["Li Fan", "Peng Wang", "Jing Yang", "Cong Shen"], "summary": "Transformers have shown potential in solving wireless communication problems,\nparticularly via in-context learning (ICL), where models adapt to new tasks\nthrough prompts without requiring model updates. However, prior ICL-based\nTransformer models rely on deep architectures with many layers to achieve\nsatisfactory performance, resulting in substantial storage and computational\ncosts. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a\nCoT-enhanced shallow Transformer framework for wireless symbol detection. By\nintroducing autoregressive latent reasoning steps within the hidden space,\nCHOOSE significantly improves the reasoning capacity of shallow models (1-2\nlayers) without increasing model depth. This design enables lightweight\nTransformers to achieve detection performance comparable to much deeper models,\nmaking them well-suited for deployment on resource-constrained mobile devices.\nExperimental results demonstrate that our approach outperforms conventional\nshallow Transformers and achieves performance comparable to that of deep\nTransformers, while maintaining storage and computational efficiency. This\nrepresents a promising direction for implementing Transformer-based algorithms\nin wireless receivers with limited computational resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21093v1", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21093v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "思想链增强的浅层Transformer用于无线符号检测", "tldr": "本文提出了CHOOSE，一个CoT增强的浅层Transformer框架，用于高效无线符号检测，以更少的资源实现与深层模型相当的性能。", "motivation": "先前的基于上下文学习（ICL）的Transformer模型在无线通信问题上表现良好，但它们依赖于深层架构，导致高昂的存储和计算成本，不适用于资源受限的移动设备。", "method": "本文提出了CHain Of thOught Symbol dEtection (CHOOSE)，一个CoT增强的浅层Transformer框架，用于无线符号检测。CHOOSE通过在隐藏空间内引入自回归潜在推理步骤，显著提升了浅层模型（1-2层）的推理能力，而无需增加模型深度。", "result": "CHOOSE显著提升了浅层模型的推理能力，实现了与更深层模型相当的检测性能。实验结果表明，该方法优于传统的浅层Transformer，并达到了深层Transformer的性能，同时保持了存储和计算效率。", "conclusion": "CHOOSE为在计算资源有限的无线接收器中实现基于Transformer的算法提供了一个有前景的方向。", "translation": "Transformer在解决无线通信问题方面显示出潜力，特别是在上下文学习（ICL）方面，模型通过提示适应新任务而无需更新模型。然而，先前的基于ICL的Transformer模型依赖于具有多层的深层架构才能达到令人满意的性能，这导致了大量的存储和计算成本。在这项工作中，我们提出了CHain Of thOught Symbol dEtection (CHOOSE)，一个用于无线符号检测的CoT增强型浅层Transformer框架。通过在隐藏空间中引入自回归潜在推理步骤，CHOOSE在不增加模型深度的情况下显著提高了浅层模型（1-2层）的推理能力。这种设计使轻量级Transformer能够实现与更深层模型相当的检测性能，使其非常适合部署在资源受限的移动设备上。实验结果表明，我们的方法优于传统的浅层Transformer，并实现了与深层Transformer相当的性能，同时保持了存储和计算效率。这代表了在计算资源有限的无线接收器中实现基于Transformer算法的一个有希望的方向。", "summary": "本文提出CHOOSE，一种思想链（CoT）增强的浅层Transformer框架，用于无线符号检测。针对传统基于上下文学习的Transformer模型深度大、计算成本高的问题，CHOOSE通过在隐藏空间引入自回归潜在推理步骤，显著提升了浅层模型（1-2层）的推理能力，使其在不增加模型深度的情况下达到与深层模型相当的性能。实验证明，该方法在保持存储和计算效率的同时，优于传统浅层Transformer并媲美深层模型，为资源受限的无线设备部署Transformer算法提供了可行方案。", "keywords": "无线符号检测, 浅层Transformer, 思想链, 上下文学习, 资源受限设备", "comments": "该论文的创新点在于将“思想链”（Chain-of-Thought）的概念引入到浅层Transformer架构中，解决了深度Transformer模型在资源受限设备上部署的挑战。通过在隐藏空间进行巧妙的自回归推理，显著提升了模型的推理能力而无需增加模型深度，具有重要的实际应用价值。"}}
{"id": "2506.21426", "title": "Evolution and determinants of firm-level systemic risk in local production networks", "authors": ["Anna Mancini", "Balázs Lengyel", "Riccardo Di Clemente", "Giulio Cimini"], "summary": "Recent crises like the COVID-19 pandemic and geopolitical tensions have\nexposed vulnerabilities and caused disruptions of supply chains, leading to\nproduct shortages, increased costs, and economic instability. This has prompted\nincreasing efforts to assess systemic risk, namely the effects of firm\ndisruptions on entire economies. However, the ability of firms to react to\ncrises by rewiring their supply links has been largely overlooked, limiting our\nunderstanding of production networks resilience. Here we study dynamics and\ndeterminants of firm-level systemic risk in the Hungarian production network\nfrom 2015 to 2022. We use as benchmark a heuristic maximum entropy null model\nthat generates an ensemble of production networks at equilibrium, by preserving\nthe total input (demand) and output (supply) of each firm at the sector level.\nWe show that the fairly stable set of firms with highest systemic risk\nundergoes a structural change during COVID-19, as those enabling economic\nexchanges become key players in the economy -- a result which is not reproduced\nby the null model. Although the empirical systemic risk aligns well with the\nnull value until the onset of the pandemic, it becomes significantly smaller\nafterwards as the adaptive behavior of firms leads to a more resilient economy.\nFurthermore, firms' international trade volume (being a subject of disruption)\nbecomes a significant predictor of their systemic risk. However, international\nlinks cannot provide an unequivocal explanation for the observed trends, as\nimports and exports have opposing effects on local systemic risk through the\nsupply and demand channels.", "comment": "15 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21426v1", "categories": ["physics.soc-ph", "cs.SI", "econ.GN", "physics.data-an", "q-fin.EC", "q-fin.RM"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.21426v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "企业层面系统性风险在本地生产网络中的演变与决定因素", "tldr": "研究匈牙利生产网络中企业层面系统性风险的动态和决定因素，发现疫情期间风险结构变化，企业适应性行为降低了风险，国际贸易量是重要预测因素。", "motivation": "近期危机暴露了供应链的脆弱性，导致经济不稳定，促使人们评估系统性风险。然而，企业通过重组供应链来应对危机的能力被忽视，这限制了对生产网络弹性的理解。", "method": "本文研究了2015年至2022年匈牙利生产网络中企业层面系统性风险的动态和决定因素。研究使用启发式最大熵零模型作为基准，该模型通过保留每个企业在部门层面的总投入（需求）和产出（供应）来生成均衡生产网络。", "result": "具有最高系统风险的稳定企业集在COVID-19期间经历了结构性变化，经济交换的促成者成为关键参与者，零模型未能重现此结果。经验系统风险在疫情爆发前与零值吻合，之后显著减小，表明企业适应性行为导致经济更具弹性。企业国际贸易量成为系统风险的重要预测因素，但国际联系（进口和出口对本地系统风险有相反影响）无法提供明确解释。", "conclusion": "企业在危机中的适应性行为（如供应链重组）能够显著降低企业层面的系统性风险并增强经济弹性。国际贸易量是系统风险的重要决定因素，但其影响复杂且需进一步分析。", "translation": "新冠疫情和地缘政治紧张等近期危机暴露了供应链的脆弱性并导致中断，进而引发产品短缺、成本增加和经济不稳定。这促使人们加大力度评估系统性风险，即企业中断对整个经济体的影响。然而，企业通过重组供应链接来应对危机的能力在很大程度上被忽视，这限制了我们对生产网络弹性的理解。本文研究了2015年至2022年匈牙利生产网络中企业层面系统性风险的动态和决定因素。我们使用启发式最大熵零模型作为基准，该模型通过保留每个企业在部门层面的总投入（需求）和产出（供应）来生成均衡生产网络集合。我们发现，具有最高系统风险的相对稳定的企业集合在新冠疫情期间经历了结构性变化，那些促成经济交流的企业成为经济中的关键参与者——这一结果未能被零模型重现。尽管经验系统风险在疫情爆发前与零值吻合良好，但之后显著减小，因为企业的适应性行为导致经济更具弹性。此外，企业的国际贸易量（作为中断的对象）成为其系统性风险的重要预测因素。然而，国际联系无法为观察到的趋势提供明确的解释，因为进出口通过供应和需求渠道对本地系统性风险产生相反的影响。", "summary": "本文研究了2015年至2022年匈牙利生产网络中企业层面系统性风险的演变和决定因素。研究发现，在COVID-19疫情期间，高系统风险企业的结构发生变化，企业适应性行为显著降低了整体系统风险，使经济更具弹性。国际贸易量被确定为系统风险的重要预测因素，但国际联系对系统风险的影响复杂，进出口作用相反。", "keywords": "系统性风险, 生产网络, 供应链弹性, 企业适应性, 国际贸易", "comments": "这项研究通过关注企业在危机中重组供应链的能力，弥补了现有系统性风险评估中被忽视的一个重要方面，为理解生产网络弹性提供了新视角。其创新之处在于使用最大熵零模型作为基准来量化企业适应性行为对系统性风险的影响。研究结果强调了企业微观行为对宏观经济稳定性的重要作用。"}}
{"id": "2506.20832", "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "pdf_url": "http://arxiv.org/pdf/2506.20832v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20832v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "利用视觉-语言模型选择扩散模型生成的可靠超分辨率样本", "tldr": "本文提出了一种利用视觉-语言模型（VLMs）从扩散模型生成的超分辨率图像集中选择最可靠样本的自动化框架，并引入了新的可靠性评分（TWS）来评估选定的样本。", "motivation": "超分辨率（SR）是一个病态逆问题，传统回归SR模型在保真度和感知质量之间权衡，容易引入伪影；扩散模型能生成多样化SR图像，但从多样化集合中选择最可靠的解决方案仍然是一个挑战。", "method": "本文引入了一个自动化框架，利用BLIP-2、GPT-4o等视觉-语言模型（VLMs）通过结构化查询评估语义正确性、视觉质量和伪影存在。然后对排名靠前的SR候选进行集成以获得单个可靠输出。为评估VLM选择样本的有效性，提出了一种新的可靠性评分（TWS），该混合指标通过CLIP嵌入的语义相似性、边缘图上SSIM的结构完整性以及多级小波分解的伪影敏感性来量化SR可靠性。", "result": "实验结果表明，TWS在模糊图像和自然图像中都与人类偏好高度相关。VLM引导的选择始终能产生高TWS值。与PSNR、LPIPS等传统指标相比，本文方法能更好地反映信息保真度。", "conclusion": "本文提出了一种利用视觉-语言模型选择扩散模型生成的可靠超分辨率样本的原则性、可扩展且可推广的解决方案，通过使输出与人类期望和语义正确性对齐，为生成式SR的可靠性设定了新基准。", "translation": "超分辨率（SR）是一个病态逆问题，对于给定的低分辨率图像有许多可行的解决方案。一方面，回归SR模型旨在平衡保真度和感知质量以产生单一解决方案，但这种权衡常常引入伪影，在识别数字或字母等信息关键应用中造成歧义。另一方面，扩散模型生成多样化的SR图像集，但从该集合中选择最可靠的解决方案仍然是一个挑战。本文引入了一个鲁棒的自动化框架，通过利用视觉-语言模型（VLMs）的语义推理能力，从扩散生成的SR样本集中识别最可靠的SR样本。具体来说，BLIP-2、GPT-4o及其变体等VLM被提示结构化查询，以评估语义正确性、视觉质量和伪影存在。然后对排名靠前的SR候选进行集成，以经济高效的方式产生单个可靠输出。为了严格评估VLM选择样本的有效性，我们提出了一种新颖的可靠性评分（TWS）——一个混合指标，通过三个互补的组成部分量化SR可靠性：通过CLIP嵌入的语义相似性、使用SSIM在边缘图上的结构完整性，以及通过多级小波分解的伪影敏感性。我们通过实验证明，TWS在模糊图像和自然图像中都与人类偏好高度相关，并且VLM引导的选择始终能产生高TWS值。与PSNR、LPIPS等未能反映信息保真度的传统指标相比，我们的方法为驾驭扩散SR空间的不确定性提供了一种原则性、可扩展且可推广的解决方案。通过使输出与人类期望和语义正确性对齐，这项工作为生成式SR中的可靠性设定了新基准。", "summary": "本研究提出了一种创新框架，利用视觉-语言模型（VLMs）从扩散模型生成的超分辨率（SR）图像集中智能选择最可靠的样本。针对现有SR方法在保真度和感知质量之间权衡以及扩散模型选择最佳输出的挑战，该框架通过VLMs评估语义正确性、视觉质量和伪影，并对顶级候选进行集成。为验证VLM选择的有效性，论文引入了新的“可靠性评分”（TWS），该指标综合考虑语义相似性、结构完整性和伪影敏感性。实验证明TWS与人类偏好高度相关，且VLM引导的选择能持续获得高TWS值，表明该方法在信息保真度方面优于传统指标，为生成式SR的可靠性提供了可扩展且通用的解决方案。", "keywords": "超分辨率, 扩散模型, 视觉-语言模型, 可靠性评分, 样本选择", "comments": "该论文的创新点在于将视觉-语言模型（VLMs）引入超分辨率（SR）领域，解决了扩散模型生成多样化但难以选择最可靠样本的问题。通过利用VLM的语义理解能力，实现了对SR结果的智能评估和选择，这超越了传统基于像素或感知的度量。此外，提出的可靠性评分（TWS）是一个重要的贡献，它为量化SR输出的可靠性提供了一个更全面、更符合人类感知的标准。这项工作为生成式SR的可靠性评估和选择开辟了新途径，对于信息关键型应用具有重要意义。"}}
{"id": "2506.21033", "title": "BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services", "authors": ["Zhaojiacheng Zhou", "Hongze Liu", "Shijing Yuan", "Hanning Zhang", "Jiong Lou", "Chentao Wu", "Jie Li"], "summary": "The hallucination problem of Large Language Models (LLMs) has increasingly\ndrawn attention. Augmenting LLMs with external knowledge is a promising\nsolution to address this issue. However, due to privacy and security concerns,\na vast amount of downstream task-related knowledge remains dispersed and\nisolated across various \"silos,\" making it difficult to access. To bridge this\nknowledge gap, we propose a blockchain-based external knowledge framework that\ncoordinates multiple knowledge silos to provide reliable foundational knowledge\nfor large model retrieval while ensuring data security. Technically, we distill\nknowledge from local data into prompts and execute transactions and records on\nthe blockchain. Additionally, we introduce a reputation mechanism and\ncross-validation to ensure knowledge quality and provide incentives for\nparticipation. Furthermore, we design a query generation framework that\nprovides a direct API interface for large model retrieval. To evaluate the\nperformance of our proposed framework, we conducted extensive experiments on\nvarious knowledge sources. The results demonstrate that the proposed framework\nachieves efficient LLM service knowledge sharing in blockchain environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21033v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21033v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "BLOCKS：区块链支持的跨孤岛知识共享以实现高效LLM服务", "tldr": "提出一个基于区块链的外部知识框架BLOCKS，通过协调多个知识孤岛来解决LLM幻觉问题，同时确保数据安全和知识质量。", "motivation": "大型语言模型（LLMs）的幻觉问题日益受到关注。通过外部知识增强LLMs是一个有前景的解决方案。然而，由于隐私和安全问题，大量下游任务相关的知识分散并孤立在各种“孤岛”中，难以访问，造成知识鸿沟。", "method": "我们提出了一个基于区块链的外部知识框架，用于协调多个知识孤岛，为大型模型检索提供可靠的基础知识，同时确保数据安全。技术上，我们将本地数据中的知识提炼成提示，并在区块链上执行交易和记录。此外，我们引入了声誉机制和交叉验证来确保知识质量并激励参与。我们还设计了一个查询生成框架，为大型模型检索提供直接的API接口。", "result": "实验结果表明，所提出的框架在区块链环境中实现了高效的LLM服务知识共享。", "conclusion": "BLOCKS框架通过利用区块链技术，有效解决了LLM的幻觉问题，实现了跨孤岛的知识安全共享，并提升了LLM服务的效率和可靠性。", "translation": "大型语言模型（LLMs）的幻觉问题日益受到关注。通过外部知识增强LLMs是一个有前景的解决方案，可以解决这个问题。然而，由于隐私和安全问题，大量下游任务相关的知识仍然分散并孤立在各种“孤岛”中，难以访问。为了弥合这一知识鸿沟，我们提出了一个基于区块链的外部知识框架，该框架协调多个知识孤岛，为大型模型检索提供可靠的基础知识，同时确保数据安全。在技术上，我们将本地数据中的知识提炼成提示，并在区块链上执行交易和记录。此外，我们引入了声誉机制和交叉验证来确保知识质量并激励参与。此外，我们设计了一个查询生成框架，为大型模型检索提供直接的API接口。为了评估我们所提出框架的性能，我们对各种知识源进行了广泛的实验。结果表明，所提出的框架在区块链环境中实现了高效的LLM服务知识共享。", "summary": "本文提出了BLOCKS，一个基于区块链的外部知识框架，旨在解决大型语言模型（LLMs）的幻觉问题。该框架通过协调分散在不同“孤岛”中的知识，为LLM提供可靠的检索知识，同时确保数据隐私和安全。BLOCKS通过将本地数据知识提炼为提示并在区块链上记录交易，并引入声誉机制和交叉验证来保证知识质量。此外，它提供了一个查询生成框架作为LLM检索的API接口。实验证明，BLOCKS能够高效地在区块链环境下实现LLM服务知识共享。", "keywords": "区块链, 知识共享, LLM, 幻觉问题, 知识孤岛", "comments": "BLOCKS在解决LLM幻觉问题方面具有创新性，其核心在于利用区块链技术实现跨孤岛的安全知识共享，这对于保护数据隐私和促进分布式协作至关重要。声誉机制和交叉验证的引入进一步提升了知识的可靠性。该框架为构建更可靠、更高效的LLM服务提供了新的思路。"}}
{"id": "2506.20729", "title": "Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset", "authors": ["Zhiqi Gao", "Tianyi Li", "Yurii Kvasiuk", "Sai Chaitanya Tadepalli", "Maja Rudolph", "Daniel J. H. Chung", "Frederic Sala", "Moritz Münchmeyer"], "summary": "Large language models (LLMs) have shown strong capabilities in complex\nreasoning, and test-time scaling techniques can enhance their performance with\ncomparably low cost. Many of these methods have been developed and evaluated on\nmathematical reasoning benchmarks such as AIME. This paper investigates whether\nthe lessons learned from these benchmarks generalize to the domain of advanced\ntheoretical physics. We evaluate a range of common test-time scaling methods on\nthe TPBench physics dataset and compare their effectiveness with results on\nAIME. To better leverage the structure of physics problems, we develop a novel,\nsymbolic weak-verifier framework to improve parallel scaling results. Our\nempirical results demonstrate that this method significantly outperforms\nexisting test-time scaling approaches on TPBench. We also evaluate our method\non AIME, confirming its effectiveness in solving advanced mathematical\nproblems. Our findings highlight the power of step-wise symbolic verification\nfor tackling complex scientific problems.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20729v1", "categories": ["cs.LG", "astro-ph.CO", "cs.AI", "hep-ph", "hep-th"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20729v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "理论物理中的测试时缩放技术——TPBench数据集上的方法比较", "tldr": "本文评估了大型语言模型（LLMs）在理论物理问题上的测试时缩放技术，并引入了一种新颖的符号弱验证器框架。该框架在TPBench数据集上显著优于现有方法，并在AIME数据集上同样有效，突出了分步符号验证在解决复杂科学问题中的强大作用。", "motivation": "大型语言模型（LLMs）在复杂推理方面表现出色，而测试时缩放技术能以低成本提升其性能。现有许多此类方法主要在数学推理基准（如AIME）上开发和评估。本文的动机是探究这些经验是否能推广到高级理论物理领域，并开发出更适合物理问题结构的测试时缩放方法。", "method": "本文评估了TPBench物理数据集上的一系列常见测试时缩放方法，并将其有效性与AIME上的结果进行比较。为了更好地利用物理问题的结构，研究开发了一种新颖的、符号化的弱验证器框架，以改进并行缩放结果。", "result": "本文提出的新颖符号弱验证器方法在TPBench数据集上显著优于现有的测试时缩放方法。该方法在AIME数据集上也得到了评估，证实了其在解决高级数学问题方面的有效性。", "conclusion": "分步符号验证对于解决复杂的科学问题具有强大的作用。", "translation": "大型语言模型（LLMs）在复杂推理方面展现出强大的能力，而测试时缩放技术可以以相对较低的成本提升它们的性能。许多此类方法已在AIME等数学推理基准上开发和评估。本文研究了从这些基准中学到的经验是否能推广到高级理论物理领域。我们评估了TPBench物理数据集上的一系列常见测试时缩放方法，并将其有效性与AIME上的结果进行了比较。为了更好地利用物理问题的结构，我们开发了一种新颖的符号弱验证器框架，以改进并行缩放结果。我们的实证结果表明，该方法在TPBench上显著优于现有测试时缩放方法。我们还在AIME上评估了我们的方法，证实了其在解决高级数学问题方面的有效性。我们的发现强调了分步符号验证在解决复杂科学问题方面的强大作用。", "summary": "本文探讨了大型语言模型（LLMs）在高级理论物理领域应用测试时缩放技术，重点关注TPBench数据集。研究评估了现有方法，并引入了一种新颖的符号弱验证器框架，该框架旨在利用物理问题的结构。实证结果表明，这种新方法在TPBench上显著优于其他测试时缩放方法，并且在AIME数学推理基准上同样有效，突出了分步符号验证在处理复杂科学问题方面的强大潜力。", "keywords": "测试时缩放, 大型语言模型, 理论物理, 符号验证, TPBench", "comments": "该论文的创新之处在于将测试时缩放技术应用于理论物理问题，并超越了通用的数学推理基准。引入一种新颖的、利用问题结构的符号弱验证器框架是其关键贡献，它展示了性能的显著提升，并揭示了符号验证在复杂科学任务中的巨大潜力。"}}
{"id": "2506.21302", "title": "Coordinated Control of Autonomous Vehicles for Traffic Density Reduction at a Signalized Junction: An MPC Approach", "authors": ["Rudra Sen", "Subashish Datta"], "summary": "The effective and safe management of traffic is a key issue due to the rapid\nadvancement of the urban transportation system. Connected autonomous vehicles\n(CAVs) possess the capability to connect with each other and adjacent\ninfrastructure, presenting novel opportunities for enhancing traffic flow and\ncoordination. This work proposes a dual-mode model predictive control (MPC)\narchitecture that tackles two interrelated issues: mitigating traffic density\nat signalized junctions and facilitating seamless, cooperative lane changes in\nhigh-density traffic conditions. The objective of this work is to facilitate\nresponsive decision-making for CAVs, thereby enhancing the efficiency and\nsafety of urban mobility. Moreover, we ensure recursive feasibility and\nconvergence of the proposed MPC scheme by the integration of an\nonline-calculated maximal control invariant terminal set. Finally, the efficacy\nof the proposed approach is validated through numerical simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21302v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21302v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "信号交叉口交通密度降低的自动驾驶车辆协同控制：一种MPC方法", "tldr": "针对信号交叉口交通密度和车道变换问题，提出一种双模MPC架构，通过数值仿真验证了其对CAV交通效率和安全性的提升。", "motivation": "城市交通系统快速发展，交通的有效和安全管理成为关键问题。连接式自动驾驶车辆（CAVs）为增强交通流和协调性提供了新机会。", "method": "提出了一种双模模型预测控制（MPC）架构，旨在解决信号交叉口交通密度缓解和高密度交通条件下无缝、合作式车道变换问题。通过集成在线计算的最大控制不变终端集，确保了所提MPC方案的递归可行性和收敛性。", "result": "所提出的方法通过数值仿真验证了其有效性。", "conclusion": "该研究通过双模MPC架构为CAV的响应式决策提供了有效方案，有望提升城市交通的效率和安全性。", "translation": "城市交通系统的快速发展使得交通的有效和安全管理成为一个关键问题。连接式自动驾驶车辆（CAVs）具备相互连接和与相邻基础设施连接的能力，为增强交通流和协调性提供了新的机会。这项工作提出了一种双模模型预测控制（MPC）架构，解决了两个相互关联的问题：缓解信号交叉口的交通密度和在高密度交通条件下促进无缝、合作式车道变换。这项工作的目标是促进CAVs的响应式决策，从而提高城市交通的效率和安全性。此外，我们通过集成在线计算的最大控制不变终端集，确保了所提出的MPC方案的递归可行性和收敛性。最后，通过数值仿真验证了所提出方法的有效性。", "summary": "这项研究提出了一种双模模型预测控制（MPC）架构，用于连接式自动驾驶车辆（CAVs）在信号交叉口的协同控制。该方法旨在缓解交通密度并实现高密度条件下的合作式车道变换，从而提高城市交通的效率和安全性。通过集成在线计算的最大控制不变终端集，确保了MPC方案的可行性和收敛性，并通过数值仿真验证了其有效性。", "keywords": "自动驾驶车辆, 交通密度缓解, 信号交叉口, 模型预测控制, 协同控制", "comments": "这篇论文通过引入双模MPC架构，为CAV在复杂信号交叉口环境下的协同控制提供了一个新颖的解决方案。其创新点在于同时处理交通密度缓解和车道变换两个相互关联的问题，并通过理论分析确保了控制方案的可靠性。其重要性在于为未来智能交通系统的发展奠定了基础，有望显著提升城市交通的效率和安全性。"}}
{"id": "2506.20970", "title": "Co-Design of Sensing, Communications, and Control for Low-Altitude Wireless Networks", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Fan Liu", "Yuanhao Cui"], "summary": "The rapid advancement of Internet of Things (IoT) services and the evolution\ntoward the sixth generation (6G) have positioned unmanned aerial vehicles\n(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This\nwork investigates the co-design of integrated sensing, communication, and\ncontrol ($\\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite\nblocklength (FBL) transmission. In particular, the UAVs continuously monitor\nthe state of the field robots and transmit their observations to the robot\ncontroller to ensure stable control while cooperating to localize an unknown\nsensing target (ST). To this end, a weighted optimization problem is first\nformulated by jointly considering the control and localization performance in\nterms of the linear quadratic regulator (LQR) cost and the determinant of the\nFisher information matrix (FIM), respectively. The resultant problem,\noptimizing resource allocations, the UAVs' deployment positions, and multi-user\nscheduling, is non-convex. To circumvent this challenge, we first derive a\nclosed-form expression of the LQR cost with respect to other variables.\nSubsequently, the non-convex optimization problem is decomposed into a series\nof sub-problems by leveraging the alternating optimization (AO) approach, in\nwhich the difference of convex functions (DC) programming and projected\ngradient descent (PGD) method are employed to obtain an efficient near-optimal\nsolution. Furthermore, the convergence and computational complexity of the\nproposed algorithm are thoroughly analyzed. Extensive simulation results are\npresented to validate the effectiveness of our proposed approach compared to\nthe benchmark schemes and reveal the trade-off between control and sensing\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20970v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.20970v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "低空无线网络中传感、通信与控制的协同设计", "tldr": "本文针对低空无线网络中多无人机协作系统，研究了有限块长传输下集成感知、通信和控制（SC²）的协同设计，并通过优化方法解决了资源分配、无人机部署和多用户调度问题。", "motivation": "随着物联网服务和第六代 (6G) 的快速发展，无人机在低空无线网络中扮演关键使能者的角色。本文旨在解决多无人机协作系统中，如何在有限块长传输下实现传感、通信和控制的协同设计，以确保现场机器人控制的稳定性和未知传感目标的有效定位。", "method": "本文首先构建了一个加权优化问题，联合考虑控制性能（通过LQR代价衡量）和定位性能（通过FIM行列式衡量）。针对该非凸问题，首先推导了LQR代价的闭式表达式，随后利用交替优化（AO）方法将其分解为一系列子问题，并采用凸函数差分（DC）编程和投影梯度下降（PGD）方法获得高效的近最优解。此外，还对所提算法的收敛性和计算复杂度进行了分析。", "result": "大量的仿真结果验证了所提方法与基准方案相比的有效性，并揭示了控制和传感性能之间的权衡。", "conclusion": "本文提出并验证了一种在低空无线网络中实现传感、通信和控制协同设计的有效方法，通过联合优化资源分配、无人机部署和多用户调度，成功平衡了控制和感知性能，为基于无人机的多协作系统提供了解决方案。", "translation": "物联网服务和第六代 (6G) 的快速发展使无人机 (UAV) 成为低空无线网络 (LAWN) 的关键使能者。这项工作研究了多无人机协作系统中集成传感、通信和控制 ($\\mathbf{SC^{2}}$) 的协同设计，采用有限块长 (FBL) 传输。具体而言，无人机持续监测现场机器人的状态并将其观测结果传输给机器人控制器，以确保稳定的控制，同时协作定位未知传感目标 (ST)。为此，本文首先通过联合考虑控制和定位性能（分别以线性二次调节器 (LQR) 代价和费雪信息矩阵 (FIM) 的行列式衡量）来构建一个加权优化问题。由此产生的问题，即优化资源分配、无人机部署位置和多用户调度，是非凸的。为了克服这一挑战，我们首先推导了LQR代价关于其他变量的闭式表达式。随后，利用交替优化 (AO) 方法将非凸优化问题分解为一系列子问题，其中采用凸函数差分 (DC) 编程和投影梯度下降 (PGD) 方法来获得高效的近最优解。此外，本文还彻底分析了所提算法的收敛性和计算复杂度。大量的仿真结果验证了我们所提方法与基准方案相比的有效性，并揭示了控制和传感性能之间的权衡。", "summary": "本文针对低空无线网络中多无人机协作系统，研究了有限块长传输下集成传感、通信和控制（SC²）的协同设计。通过构建一个考虑控制和定位性能的加权优化问题，并利用交替优化、凸函数差分编程和投影梯度下降等方法，解决了资源分配、无人机部署和多用户调度等非凸问题，最终实现了高效的近最优解。仿真结果验证了该方法的有效性，并揭示了控制与感知性能的权衡。", "keywords": "无人机, 低空无线网络, 协同设计, 传感通信控制, 有限块长传输", "comments": "本文的创新之处在于提出了低空无线网络中传感、通信和控制的协同设计框架，并针对其非凸优化问题，巧妙地结合了多种优化技术（AO, DC, PGD）来寻找近最优解。其重要性在于为未来6G背景下基于无人机的低空无线网络提供了理论和实践指导，尤其是在有限块长传输条件下的多无人机协作场景。"}}
{"id": "2506.21171", "title": "Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations", "authors": ["Jing Yang", "Qunliang Xing", "Mai Xu", "Minglang Qiao"], "summary": "Joint Photographic Experts Group (JPEG) achieves data compression by\nquantizing Discrete Cosine Transform (DCT) coefficients, which inevitably\nintroduces compression artifacts. Most existing JPEG quality enhancement\nmethods operate in the pixel domain, suffering from the high computational\ncosts of decoding. Consequently, direct enhancement of JPEG images in the DCT\ndomain has gained increasing attention. However, current DCT-domain methods\noften exhibit limited performance. To address this challenge, we identify two\ncritical types of correlations within the DCT coefficients of JPEG images.\nBuilding on this insight, we propose an Advanced DCT-domain JPEG Quality\nEnhancement (AJQE) method that fully exploits these correlations. The AJQE\nmethod enables the adaptation of numerous well-established pixel-domain models\nto the DCT domain, achieving superior performance with reduced computational\ncomplexity. Compared to the pixel-domain counterparts, the DCT-domain models\nderived by our method demonstrate a 0.35 dB improvement in PSNR and a 60.5%\nincrease in enhancement throughput on average.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21171v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21171v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在DCT中发掘宝藏：通过利用潜在相关性推进JPEG质量增强", "tldr": "该研究提出了一种在DCT域直接增强JPEG图像质量的新方法AJQE，通过利用DCT系数中的潜在相关性，实现了更好的性能和更低的计算成本。", "motivation": "现有的JPEG质量增强方法大多在像素域操作，计算成本高昂。虽然DCT域增强方法受到关注，但其性能有限。", "method": "识别了JPEG图像DCT系数中的两种关键相关性，并基于此提出了高级DCT域JPEG质量增强（AJQE）方法，该方法充分利用这些相关性，使许多成熟的像素域模型能够适应DCT域。", "result": "与像素域对应方法相比，该方法导出的DCT域模型在PSNR上平均提高了0.35 dB，增强吞吐量平均增加了60.5%。", "conclusion": "通过在DCT域利用潜在相关性，可以显著提高JPEG图像的质量增强性能并降低计算成本。", "translation": "联合图像专家组（JPEG）通过量化离散余弦变换（DCT）系数实现数据压缩，这不可避免地引入了压缩伪影。大多数现有的JPEG质量增强方法在像素域操作，但存在解码计算成本高的问题。因此，直接在DCT域增强JPEG图像获得了越来越多的关注。然而，当前的DCT域方法通常表现出有限的性能。为了解决这一挑战，我们识别了JPEG图像DCT系数中的两种关键相关性。基于这一洞察，我们提出了一种高级DCT域JPEG质量增强（AJQE）方法，该方法充分利用了这些相关性。AJQE方法使得许多成熟的像素域模型能够适应DCT域，以更低的计算复杂性实现卓越的性能。与像素域对应方法相比，我们方法导出的DCT域模型在PSNR上平均提高了0.35 dB，增强吞吐量平均增加了60.5%。", "summary": "本文针对JPEG图像压缩伪影问题，提出了一种名为AJQE的DCT域质量增强方法。该方法通过识别并利用DCT系数中的两种关键相关性，成功地将像素域的成熟模型应用于DCT域。实验结果表明，AJQE在性能上优于传统的像素域方法，并在计算效率上也有显著提升。", "keywords": "JPEG质量增强, DCT域, 离散余弦变换, 相关性, 压缩伪影", "comments": "这项工作通过直接在DCT域进行JPEG质量增强，避免了像素域方法高昂的解码成本，具有创新性。其核心在于发现了DCT系数中的潜在相关性并加以利用，这使得现有像素域模型得以高效迁移，为JPEG图像处理提供了新的视角和高效的解决方案。"}}
{"id": "2506.21230", "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "authors": ["Junhao Shi", "Zhaoye Fei", "Siyin Wang", "Qipeng Guo", "Jingjing Gong", "Xipeng QIu"], "summary": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks\nbut struggle with complex scenarios involving unfamiliar environments and\nmulti-step goals. Current approaches rely on environment-agnostic imitation\nlearning that disconnects instructions from environmental contexts, causing\nmodels to struggle with context-sensitive instructions and rely on\nsupplementary cues rather than visual reasoning during long-horizon\ninteractions. In this work, we propose World-Aware Planning Narrative\nEnhancement (WAP), a framework that infuses LVLMs with comprehensive\nenvironmental understanding through four cognitive capabilities (visual\nappearance modeling, spatial reasoning, functional abstraction, and syntactic\ngrounding) while developing and evaluating models using only raw visual\nobservations through curriculum learning. Evaluations on the EB-ALFRED\nbenchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a\n60.7 absolute improvement in task success rates, particularly in commonsense\nreasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced\nopen-source models outperform proprietary systems like GPT-4o and\nClaude-3.5-Sonnet by a large margin.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21230v1", "categories": ["cs.AI", "cs.RO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21230v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "世界感知规划叙事增强大型视觉-语言模型规划器", "tldr": "本文提出WAP框架，通过注入世界感知能力显著提升大型视觉-语言模型（LVLMs）在具身规划任务中的表现，尤其在复杂和长程场景下，甚至超越了专有模型。", "motivation": "大型视觉-语言模型（LVLMs）在具身规划任务中，面对不熟悉的环境和多步骤目标时表现不佳。现有方法依赖于与环境无关的模仿学习，导致指令与环境脱节，模型难以处理情境敏感指令，并在长时间交互中过度依赖辅助线索而非视觉推理。", "method": "本文提出了世界感知规划叙事增强（WAP）框架，通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法接地）为LVLMs注入全面的环境理解。模型开发和评估仅使用原始视觉观测，并采用课程学习。", "result": "在EB-ALFRED基准测试中取得了显著提升，Qwen2.5-VL的任务成功率绝对提高了60.7%，其中常识推理提高了60.0%，长程规划提高了70.0%。此外，增强后的开源模型大幅超越了GPT-4o和Claude-3.5-Sonnet等专有系统。", "conclusion": "通过引入世界感知规划叙事增强（WAP）框架，大型视觉-语言模型在具身规划任务中，特别是在处理复杂和长程目标时，取得了显著的性能提升，表明了环境理解对提升模型规划能力的关键作用。", "translation": "大型视觉-语言模型（LVLMs）在具身规划任务中展现出潜力，但在涉及不熟悉环境和多步骤目标的复杂场景中表现不佳。当前方法依赖于与环境无关的模仿学习，将指令与环境背景分离，导致模型难以处理情境敏感指令，并在长时间交互中依赖辅助线索而非视觉推理。在这项工作中，我们提出了世界感知规划叙事增强（WAP），一个通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法接地）为LVLMs注入全面环境理解的框架，同时仅使用原始视觉观测通过课程学习来开发和评估模型。在EB-ALFRED基准测试上的评估表明取得了实质性改进，Qwen2.5-VL的任务成功率绝对提高了60.7%，尤其在常识推理（+60.0）和长程规划（+70.0）方面。值得注意的是，我们增强后的开源模型大幅超越了GPT-4o和Claude-3.5-Sonnet等专有系统。", "summary": "本文针对大型视觉-语言模型（LVLMs）在复杂具身规划任务中缺乏环境理解的问题，提出了一种名为世界感知规划叙事增强（WAP）的新框架。WAP通过视觉外观建模、空间推理、功能抽象和句法接地等认知能力，为LVLMs注入全面的环境理解。通过课程学习和仅使用原始视觉观测进行训练，该方法在EB-ALFRED基准测试中取得了显著效果，使Qwen2.5-VL的任务成功率大幅提升，并在常识推理和长程规划方面表现优异，甚至超越了GPT-4o等领先的专有模型。", "keywords": "大型视觉-语言模型, 具身规划, 世界感知规划, 环境理解, 课程学习", "comments": "本文的创新之处在于明确提出并实现了将“世界感知”能力集成到LVLMs中，以解决具身规划中环境理解不足的问题。通过引入四种认知能力和采用课程学习，模型能够更好地进行视觉推理而非仅仅依赖辅助线索。其在开源模型上取得的显著性能提升，并超越了专有系统，凸显了该方法的实用性和重要性。"}}
{"id": "2506.21425", "title": "IDGraphs: Intrusion Detection and Analysis Using Stream Compositing", "authors": ["Pin Ren", "Yan Gao", "Zhichun Li", "Yan Chen", "Benjamin Watson"], "summary": "Traffic anomalies and attacks are commonplace in today's networks and\nidentifying them rapidly and accurately is critical for large network\noperators. For a statistical intrusion detection system (IDS), it is crucial to\ndetect at the flow-level for accurate detection and mitigation. However,\nexisting IDS systems offer only limited support for 1) interactively examining\ndetected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)\nand discovering correlated attacks. These problems are becoming even more acute\nas the traffic on today's high-speed routers continues to grow.\n  IDGraphs is an interactive visualization system for intrusion detection that\naddresses these challenges. The central visualization in the system is a\nflow-level trace plotted with time on the horizontal axis and aggregated number\nof unsuccessful connections on the vertical axis. We then summarize a stack of\ntens or hundreds of thousands of these traces using the Histographs [RW05]\ntechnique, which maps data frequency at each pixel to brightness. Users may\nthen interactively query the summary view, performing analysis by highlighting\nsubsets of the traces. For example, brushing a linked correlation matrix view\nhighlights traces with similar patterns, revealing distributed attacks that are\ndifficult to detect using standard statistical analysis.\n  We apply IDGraphs system to a real network router data-set with 179M\nflow-level records representing a total traffic of 1.16TB. The system\nsuccessfully detects and analyzes a variety of attacks and anomalies, including\nport scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21425v1", "categories": ["cs.GR", "cs.CR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21425v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "IDGraphs：使用流合成进行入侵检测与分析", "tldr": "IDGraphs是一个交互式可视化系统，通过流级别追踪和Histographs技术检测和分析各种网络入侵和异常，包括分布式攻击。", "motivation": "现有的入侵检测系统（IDS）在交互式检查入侵、分析蠕虫传播模式和发现相关攻击方面支持有限，且随着高速路由器流量增长，这些问题日益严重。", "method": "IDGraphs是一个交互式可视化系统，其核心可视化是流级别追踪图，水平轴为时间，垂直轴为聚合的非成功连接数。系统使用Histographs技术汇总数万条追踪，将数据频率映射到亮度。用户可交互式查询摘要视图，通过突出显示追踪子集进行分析，例如，通过关联矩阵视图高亮显示具有相似模式的追踪，揭示分布式攻击。", "result": "IDGraphs系统成功检测并分析了多种攻击和异常，包括端口扫描、蠕虫爆发、隐秘TCP SYN洪水和一些分布式攻击。该系统已应用于一个包含1.79亿流级别记录（总计1.16TB流量）的真实网络路由器数据集。", "conclusion": "Not mentioned in abstract", "translation": "今天的网络中，流量异常和攻击屡见不鲜，对于大型网络运营商来说，快速准确地识别它们至关重要。对于统计入侵检测系统（IDS）而言，在流级别进行检测对于准确检测和缓解至关重要。然而，现有IDS系统仅提供有限的支持，用于：1）交互式检查检测到的入侵和异常，2）分析蠕虫传播模式，3）发现相关攻击。随着当今高速路由器上的流量持续增长，这些问题变得更加严峻。\nIDGraphs是一个交互式可视化入侵检测系统，旨在解决这些挑战。该系统的核心可视化是一个流级别的追踪图，水平轴为时间，垂直轴为聚合的非成功连接数。然后，我们使用Histographs [RW05] 技术汇总数万条此类追踪，该技术将每个像素的数据频率映射到亮度。用户可以交互式查询摘要视图，通过突出显示追踪子集进行分析。例如，刷选链接的关联矩阵视图会高亮显示具有相似模式的追踪，从而揭示使用标准统计分析难以检测到的分布式攻击。\n我们将IDGraphs系统应用于一个真实的网络路由器数据集，该数据集包含1.79亿条流级别记录，代表总计1.16TB的流量。该系统成功检测并分析了多种攻击和异常，包括端口扫描、蠕虫爆发、隐秘TCP SYN洪水以及一些分布式攻击。", "summary": "IDGraphs是一个针对大规模网络流量异常和攻击的交互式可视化入侵检测系统。它通过流级别追踪和Histographs技术，解决了现有IDS在交互式分析、蠕虫模式识别和关联攻击发现方面的不足。该系统能够有效地检测和分析各种网络威胁，如端口扫描、蠕虫和分布式攻击，并在实际网络数据集中得到了验证。", "keywords": "入侵检测, 流合成, 可视化, 网络安全, 异常检测", "comments": "IDGraphs的创新之处在于其将流级别追踪与Histographs可视化技术相结合，提供了交互式分析能力，特别是能够发现传统方法难以检测的分布式攻击。这对于处理高速网络海量数据至关重要，提高了入侵检测的效率和准确性。"}}
{"id": "2506.21118", "title": "Courcelle's Theorem for Lipschitz Continuity", "authors": ["Tatsuya Gima", "Soh Kumabe", "Yuichi Yoshida"], "summary": "Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida\n(FOCS'23), measures the stability of an algorithm against small input\nperturbations. Algorithms with small Lipschitz continuity are desirable, as\nthey ensure reliable decision-making and reproducible scientific research.\nSeveral studies have proposed Lipschitz continuous algorithms for various\ncombinatorial optimization problems, but these algorithms are problem-specific,\nrequiring a separate design for each problem.\n  To address this issue, we provide the first algorithmic meta-theorem in the\nfield of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz\ncontinuous analogue of Courcelle's theorem, which offers Lipschitz continuous\nalgorithms for problems on bounded-treewidth graphs. Specifically, we consider\nthe problem of finding a vertex set in a graph that maximizes or minimizes the\ntotal weight, subject to constraints expressed in monadic second-order logic\n(MSO_2). We show that for any $\\varepsilon>0$, there exists a $(1\\pm\n\\varepsilon)$-approximation algorithm for the problem with a polylogarithmic\nLipschitz constant on bounded treewidth graphs. On such graphs, our result\noutperforms most existing Lipschitz continuous algorithms in terms of\napproximability and/or Lipschitz continuity. Further, we provide similar\nresults for problems on bounded-clique-width graphs subject to constraints\nexpressed in MSO_1. Additionally, we construct a Lipschitz continuous version\nof Baker's decomposition using our meta-theorem as a subroutine.", "comment": "ESA 2025, 27 pages", "pdf_url": "http://arxiv.org/pdf/2506.21118v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21118v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Lipschitz 连续性的 Courcelle 定理", "tldr": "本文提出了Lipschitz连续算法领域的首个算法元定理，可以为有界树宽图上的MSO_2约束问题提供具有多对数Lipschitz常数的近似算法，且性能优于现有算法。", "motivation": "Lipschitz连续算法虽然对算法稳定性很重要，但现有算法通常是问题特定的，需要为每个问题单独设计。", "method": "本文提出了Lipschitz连续算法领域的首个算法元定理，作为Courcelle定理的Lipschitz连续模拟。该方法考虑在有界树宽图上，针对满足单调二阶逻辑（MSO_2）约束的顶点集最大化或最小化总权重问题。此外，还为有界团宽图上的MSO_1约束问题提供了类似结果，并利用该元定理构建了Baker分解的Lipschitz连续版本。", "result": "对于有界树宽图上的MSO_2约束问题，本文提出了一个具有多对数Lipschitz常数的 (1±ε)-近似算法。该结果在近似性和/或Lipschitz连续性方面优于大多数现有算法。对于有界团宽图上的MSO_1约束问题，也得到了类似的结果。此外，利用该元定理成功构建了Baker分解的Lipschitz连续版本。", "conclusion": "本文首次为Lipschitz连续算法领域提供了算法元定理，显著提升了有界树宽图和有界团宽图上特定优化问题的算法设计效率和性能。", "translation": "算法的Lipschitz连续性由Kumabe和Yoshida（FOCS'23）引入，用于衡量算法对微小输入扰动的稳定性。具有小Lipschitz常数的算法是可取的，因为它们能确保可靠的决策和可重现的科学研究。一些研究已经为各种组合优化问题提出了Lipschitz连续算法，但这些算法是问题特定的，需要为每个问题单独设计。\n为了解决这个问题，我们提供了Lipschitz连续算法领域的首个算法元定理。我们的结果可以看作是Courcelle定理的Lipschitz连续模拟，它为有界树宽图上的问题提供了Lipschitz连续算法。具体来说，我们考虑在图中寻找一个顶点集的问题，该顶点集在满足单调二阶逻辑（MSO_2）约束的情况下，使总权重最大化或最小化。我们表明，对于任意ε>0，存在一个具有多对数Lipschitz常数的 (1±ε)-近似算法，用于解决有界树宽图上的这类问题。在这些图上，我们的结果在近似性和/或Lipschitz连续性方面优于大多数现有Lipschitz连续算法。此外，我们为受MSO_1约束的有界团宽图上的问题提供了类似的结果。此外，我们利用我们的元定理作为子程序，构建了Baker分解的Lipschitz连续版本。", "summary": "本文提出了Lipschitz连续算法领域的首个算法元定理，作为Courcelle定理的Lipschitz连续模拟。该元定理为有界树宽图上满足MSO_2约束的顶点集优化问题提供了具有多对数Lipschitz常数的 (1±ε)-近似算法，其性能优于现有算法。研究还扩展到有界团宽图上的MSO_1约束问题，并利用该元定理构建了Lipschitz连续的Baker分解。", "keywords": "Lipschitz连续性, 算法元定理, Courcelle定理, 有界树宽图, 组合优化", "comments": "本文的创新点在于提出了Lipschitz连续算法的第一个算法元定理，这极大地简化了针对特定图结构（如有限树宽图）设计稳定算法的过程。它将Courcelle定理的强大框架扩展到Lipschitz连续性领域，为组合优化问题的稳定性分析和算法设计提供了通用的方法。这对于确保算法在面对输入扰动时的可靠性和可重现性具有重要意义。"}}
{"id": "2506.21032", "title": "RecCoT: Enhancing Recommendation via Chain-of-Thought", "authors": ["Shuo Yang", "Jiangxia Cao", "Haipeng Li", "Yuqi Mao", "Shuchao Pang"], "summary": "In real-world applications, users always interact with items in multiple\naspects, such as through implicit binary feedback (e.g., clicks, dislikes, long\nviews) and explicit feedback (e.g., comments, reviews). Modern recommendation\nsystems (RecSys) learn user-item collaborative signals from these implicit\nfeedback signals as a large-scale binary data-streaming, subsequently\nrecommending other highly similar items based on users' personalized historical\ninteractions. However, from this collaborative-connection perspective, the\nRecSys does not focus on the actual content of the items themselves but instead\nprioritizes higher-probability signals of behavioral co-occurrence among items.\nConsequently, under this binary learning paradigm, the RecSys struggles to\nunderstand why a user likes or dislikes certain items. To alleviate it, some\nworks attempt to utilize the content-based reviews to capture the semantic\nknowledge to enhance recommender models. However, most of these methods focus\non predicting the ratings of reviews, but do not provide a human-understandable\nexplanation.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.21032v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21032v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "RecCoT: 通过思维链增强推荐", "tldr": "现有的推荐系统难以理解用户喜好背后的原因，因为它们主要关注行为共现而非内容本身，且基于内容的解释方法通常只预测评分而缺乏人类可理解的解释。本文旨在通过思维链（Chain-of-Thought）来解决这一问题。", "motivation": "在现实应用中，用户与商品的多方面交互（如隐式反馈和显式反馈）产生了大量数据。现代推荐系统主要从隐式反馈中学习用户-商品协同信号，并基于用户历史交互推荐相似商品。然而，这种协同连接视角使得推荐系统不关注商品内容本身，而是优先考虑行为共现的高概率信号。因此，在二元学习范式下，推荐系统难以理解用户喜欢或不喜欢某些商品的原因。尽管一些工作尝试利用基于内容的评论来捕获语义知识以增强推荐模型，但大多数方法侧重于预测评论评分，而不能提供人类可理解的解释。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在实际应用中，用户总是通过多个方面与商品进行交互，例如通过隐式二元反馈（如点击、不喜欢、长时间观看）和显式反馈（如评论、评价）。现代推荐系统（RecSys）从这些隐式反馈信号中学习用户-商品的协同信号，作为大规模的二元数据流，随后根据用户的个性化历史交互推荐其他高度相似的商品。然而，从这种协同连接的角度来看，推荐系统并不关注商品本身的实际内容，而是优先考虑商品之间行为共现的更高概率信号。因此，在这种二元学习范式下，推荐系统难以理解用户喜欢或不喜欢某些商品的原因。为了缓解这一问题，一些工作尝试利用基于内容的评论来捕获语义知识以增强推荐模型。然而，这些方法大多侧重于预测评论的评分，但未能提供人类可理解的解释。", "summary": "现有推荐系统主要依赖用户行为的协同信号进行推荐，但这种方法无法理解用户喜好背后的深层原因，因为它侧重于行为共现而非商品内容。虽然一些研究尝试利用内容（如评论）来增强模型，但它们通常只预测评分，缺乏人类可理解的解释。本文旨在通过引入思维链（Chain-of-Thought）来解决推荐系统在理解用户偏好原因和提供可解释性方面的不足。", "keywords": "推荐系统, 思维链, 可解释性, 用户行为, 内容理解", "comments": "这篇论文的创新点可能在于将“思维链”这一概念引入推荐系统，以解决现有系统无法提供人类可理解的解释和深入理解用户喜好原因的痛点。如果能成功地将思维链应用于推荐，将极大地提升推荐系统的透明度和用户信任度，从传统的“推荐什么”转向“为什么推荐”。"}}
{"id": "2506.21065", "title": "Entropy-stable in- and outflow boundary conditions for the compressible Navier-Stokes equations", "authors": ["Magnus Svärd", "Anita Gjesteland"], "summary": "We propose inflow and outflow boundary conditions for the compressible\nNavier-Stokes equations and prove that they allow a priori estimates of the\nentropy, mass and total energy. Furthermore, we demonstrate how to approximate\nthese boundary conditions in conjunction with an entropy-stable finite-volume\nscheme. The method is also applicable to other types of entropy-stable schemes.\nFinally, we carry out some numerical computations with the finite-volume scheme\nand demonstrate their robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21065v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21065v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "压缩Navier-Stokes方程的熵稳定进出口边界条件", "tldr": "提出了适用于可压缩Navier-Stokes方程的熵稳定进出口边界条件，并证明其允许对熵、质量和总能量进行先验估计，同时通过数值计算验证了其鲁棒性。", "motivation": "为可压缩Navier-Stokes方程开发并验证能够保证熵、质量和总能量先验估计的进出口边界条件。", "method": "提出了新的进出口边界条件，并证明了它们允许对熵、质量和总能量进行先验估计。此外，还展示了如何将这些边界条件与熵稳定的有限体积格式结合进行近似，并指出该方法也适用于其他类型的熵稳定格式。最后，通过有限体积格式进行了数值计算。", "result": "数值计算结果表明所提出的边界条件与有限体积格式结合使用时具有良好的鲁棒性。", "conclusion": "所提出的可压缩Navier-Stokes方程的熵稳定进出口边界条件是鲁棒的，并且允许对熵、质量和总能量进行重要的先验估计。", "translation": "我们为可压缩Navier-Stokes方程提出了进出口边界条件，并证明它们允许对熵、质量和总能量进行先验估计。此外，我们展示了如何将这些边界条件与熵稳定的有限体积格式结合进行近似。该方法也适用于其他类型的熵稳定格式。最后，我们使用有限体积格式进行了一些数值计算，并证明了它们的鲁棒性。", "summary": "本文提出了适用于可压缩Navier-Stokes方程的熵稳定进出口边界条件。研究证明这些条件能够对熵、质量和总能量进行先验估计。作者进一步展示了如何将这些边界条件与熵稳定的有限体积格式相结合进行近似，并指出其同样适用于其他熵稳定格式。通过数值计算，验证了所提方法与有限体积格式结合的鲁棒性。", "keywords": "熵稳定, 边界条件, Navier-Stokes, 可压缩流, 有限体积格式", "comments": "本文的主要创新在于提出了能够保证熵、质量和总能量先验估计的进出口边界条件，这对于可压缩流体数值模拟的稳定性和准确性至关重要。此外，该方法不仅适用于有限体积格式，还可推广到其他熵稳定格式，显示了其通用性。"}}
{"id": "2506.20876", "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20876v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20876v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "少做决定，多交流：论医学领域端到端事实核查的结构效度", "tldr": "尽管自动事实核查在医学领域潜力巨大，但端到端系统仍未被广泛使用。本研究通过分析临床专家如何核查医学声明，揭示了将事实核查视为端到端过程的根本挑战，并提出应将其视为交互式沟通问题。", "motivation": "自动事实核查在医学领域因其高风险性质和医学文献评估挑战而备受关注。然而，尽管有需求，端到端事实核查系统仍未被广泛使用，这促使研究者探究其原因。", "method": "本研究首次调查了临床专家如何通过综合医学证据来核实社交媒体上的真实医学声明，以此寻找端到端事实核查的“上限”。", "result": "研究揭示了端到端医学事实核查的根本挑战，包括：难以将非结构化声明与临床试验等科学证据关联；欠明确声明的歧义与意图不符；以及固有的主观真实性标签。", "conclusion": "事实核查应被视为一个交互式沟通问题来处理和评估，而非一个简单的端到端过程。", "translation": "标题：少做决定，多交流：论医学领域端到端事实核查的结构效度\n\n抽象：技术进步在自动事实核查等曾被视为具有挑战性的任务上取得了具体进展。鉴于医疗决策的高风险性质以及批判性评估大量多样医学文献的挑战，在公共卫生和医学领域采用这些系统的兴趣日益增长。循证医学与每个人息息相关，但其本质高度专业化，使得大多数用户的医学素养不足以充分驾驭该领域。医学交流中的这些问题为端到端事实核查代理奠定了基础：针对当前医学文献核查声明并返回有证据支持的结论。然而，此类系统仍未被广泛使用。为了理解这一点，我们首次研究了临床专家如何通过综合医学证据来核实社交媒体上的真实声明。在寻找这个“上限”的过程中，我们揭示了将端到端事实核查应用于医学时面临的根本挑战：难以将日常声明与临床试验形式的科学证据联系起来；欠明确声明中的歧义与不匹配的意图；以及固有的主观真实性标签。我们认为，事实核查应被视为一个交互式沟通问题来处理和评估，而非一个端到端过程。", "summary": "本文探讨了医学领域端到端自动事实核查系统未被广泛使用的原因。研究通过分析临床专家如何核实社交媒体上的医学声明，揭示了当前端到端方法面临的根本挑战，包括将非结构化声明与科学证据关联的困难、声明的歧义性以及真实性标签的主观性。作者提出，事实核查应被重新定义并评估为一个交互式沟通问题，而非一个简单的端到端过程。", "keywords": "事实核查, 医学, 结构效度, 沟通, 临床专家", "comments": "这篇论文很有创新性，它没有停留在技术层面讨论如何改进现有的端到端事实核查系统，而是从根本上质疑了其在医学领域应用的“结构效度”。通过分析人类专家的行为，揭示了医学事实核查的复杂性，并提出了将其视为交互式沟通问题的全新视角，这对未来医学信息核查系统的设计具有重要指导意义。"}}
{"id": "2506.21440", "title": "Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform", "authors": ["Maxime Leiber", "Yosra Marnissi", "Axel Barrau", "Sylvain Meignen", "Laurent Massoulié"], "summary": "The short-time Fourier transform (STFT) is widely used for analyzing\nnon-stationary signals. However, its performance is highly sensitive to its\nparameters, and manual or heuristic tuning often yields suboptimal results. To\novercome this limitation, we propose a unified differentiable formulation of\nthe STFT that enables gradient-based optimization of its parameters. This\napproach addresses the limitations of traditional STFT parameter tuning\nmethods, which often rely on computationally intensive discrete searches. It\nenables fine-tuning of the time-frequency representation (TFR) based on any\ndesired criterion. Moreover, our approach integrates seamlessly with neural\nnetworks, allowing joint optimization of the STFT parameters and network\nweights. The efficacy of the proposed differentiable STFT in enhancing TFRs and\nimproving performance in downstream tasks is demonstrated through experiments\non both simulated and real-world data.", "comment": "DSTFT, STFT, spectrogram, time-frequency, IEEE Transactions on Signal\n  Processing, 10 pages", "pdf_url": "http://arxiv.org/pdf/2506.21440v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21440v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可学习的自适应时频表示通过可微分短时傅里叶变换", "tldr": "短时傅里叶变换（STFT）的参数敏感性导致性能次优。本文提出一种统一的可微分STFT公式，支持梯度优化参数，能与神经网络联合优化，有效提升时频表示质量及下游任务性能。", "motivation": "传统短时傅里叶变换（STFT）的性能对其参数高度敏感，手动或启发式调优常导致次优结果，且依赖计算密集型离散搜索。", "method": "提出STFT的统一可微分公式，实现基于梯度的参数优化。该方法允许根据任意所需准则对时频表示（TFR）进行微调，并能与神经网络无缝集成，联合优化STFT参数和网络权重。", "result": "在模拟数据和真实世界数据上的实验证明了所提出的可微分STFT在增强时频表示（TFR）和改进下游任务性能方面的有效性。", "conclusion": "通过引入可微分的STFT，可以克服传统STFT参数调优的局限性，实现参数的自适应优化，从而显著提升时频分析的质量和下游任务的表现。", "translation": "短时傅里叶变换（STFT）被广泛用于分析非平稳信号。然而，其性能对其参数高度敏感，手动或启发式调优常常产生次优结果。为了克服这一限制，我们提出了一种STFT的统一可微分公式，该公式支持基于梯度的参数优化。这种方法解决了传统STFT参数调优方法的局限性，这些方法通常依赖于计算密集型的离散搜索。它使得能够根据任何所需准则对时频表示（TFR）进行微调。此外，我们的方法与神经网络无缝集成，允许联合优化STFT参数和网络权重。通过在模拟和真实世界数据上的实验，证明了所提出的可微分STFT在增强TFR和提高下游任务性能方面的有效性。", "summary": "本文提出一种新颖的可微分短时傅里叶变换（STFT）框架，旨在解决传统STFT参数敏感且难以优化的局限。通过将STFT公式化为可微分形式，研究者能够利用梯度优化方法自适应地调整其参数，从而获得更优的时频表示。该方法还能与神经网络联合优化，并在模拟和真实数据上的实验中验证了其在提升时频表示质量和下游任务性能方面的有效性。", "keywords": "短时傅里叶变换, 可微分, 时频表示, 梯度优化, 神经网络", "comments": "这篇论文通过使STFT可微分，开辟了在深度学习框架内优化信号处理前端的新途径。其创新之处在于将传统信号处理工具与现代机器学习的梯度优化能力相结合，使得时频分析不再依赖于经验性参数选择，而是能够根据任务需求进行端到端的学习和优化。这对于需要精确时频特征的非平稳信号处理任务（如语音、音乐、生物医学信号分析）具有重要意义，有望显著提升相关应用的性能。"}}
{"id": "2506.21106", "title": "PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction", "authors": ["Felipe Castaño", "Eduardo Fidalgo", "Enrique Alegre", "Rocio Alaiz-Rodríguez", "Raul Orduna", "Francesco Zola"], "summary": "Phishing attacks pose a significant cybersecurity threat, evolving rapidly to\nbypass detection mechanisms and exploit human vulnerabilities. This paper\nintroduces PhishKey to address the challenges of adaptability, robustness, and\nefficiency. PhishKey is a novel phishing detection method using automatic\nfeature extraction from hybrid sources. PhishKey combines character-level\nprocessing with Convolutional Neural Networks (CNN) for URL classification, and\na Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at\nthe word level. CAPE reduces noise and ensures complete sample processing\navoiding crop operations on the input data. The predictions from both modules\nare integrated using a soft-voting ensemble to achieve more accurate and\nreliable classifications. Experimental evaluations on four state-of-the-art\ndatasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1\nScore and shows strong resistance to adversarial manipulations such as\ninjection attacks with minimal performance degradation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21106v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21106v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PhishKey：一种新颖的基于质心的方法，通过自适应HTML组件提取增强网络钓鱼检测", "tldr": "PhishKey是一种新颖的网络钓鱼检测方法，结合了基于CNN的URL分类和基于质心的HTML内容提取，通过软投票集成实现高准确率和对对抗性攻击的强大抵抗力。", "motivation": "网络钓鱼攻击对网络安全构成重大威胁，且不断演变以规避检测机制和利用人类漏洞。本文旨在解决网络钓鱼检测中适应性、鲁棒性和效率的挑战。", "method": "PhishKey是一种新颖的网络钓鱼检测方法，采用混合源的自动特征提取。它结合了字符级处理与卷积神经网络（CNN）进行URL分类，以及一个基于质心的关键组件网络钓鱼提取器（CAPE）用于词级别的HTML内容处理。CAPE减少了噪声并确保完整样本处理，避免对输入数据进行裁剪操作。两个模块的预测通过软投票集成进行整合，以实现更准确可靠的分类。", "result": "在四个最先进的数据集上进行的实验评估表明PhishKey的有效性。它实现了高达98.70%的F1分数，并显示出对注入攻击等对抗性操纵的强大抵抗力，性能下降最小。", "conclusion": "PhishKey通过结合多源特征提取和集成学习，显著提高了网络钓鱼检测的准确性和鲁棒性，有效应对了现有挑战。", "translation": "网络钓鱼攻击对网络安全构成重大威胁，它们迅速演变以绕过检测机制并利用人类的漏洞。本文引入了PhishKey，以应对适应性、鲁棒性和效率方面的挑战。PhishKey是一种新颖的网络钓鱼检测方法，利用混合来源的自动特征提取。PhishKey将字符级处理与卷积神经网络（CNN）相结合进行URL分类，并使用一个基于质心的关键组件网络钓鱼提取器（CAPE）处理词级别的HTML内容。CAPE减少了噪声，并确保完整的样本处理，避免对输入数据进行裁剪操作。来自两个模块的预测通过软投票集成进行整合，以实现更准确和可靠的分类。在四个最先进的数据集上进行的实验评估证明了PhishKey的有效性。它实现了高达98.70%的F1分数，并显示出对注入攻击等对抗性操纵的强大抵抗力，性能下降最小。", "summary": "PhishKey是一种创新的网络钓鱼检测系统，旨在解决现有方法的适应性、鲁棒性和效率问题。它通过结合基于CNN的URL字符级分析和基于质心的HTML内容词级提取（CAPE）来自动提取混合特征。CAPE通过避免数据裁剪来减少噪声并确保完整处理。最终，两个模块的预测通过软投票集成，以实现高准确率和对对抗性攻击的强大抵抗力，实验证明其F1分数高达98.70%。", "keywords": "网络钓鱼检测, 质心, HTML组件提取, 卷积神经网络, 软投票集成", "comments": "本文提出了一种结合URL和HTML内容分析的新颖网络钓鱼检测方法PhishKey，其创新点在于引入了基于质心的HTML关键组件提取器（CAPE）来减少噪声并确保完整数据处理，以及通过软投票集成不同模块的预测。其在对抗性操纵下的鲁棒性表现尤其值得关注，这对于实际应用中的网络安全至关重要。该方法在解决网络钓鱼不断演变带来的挑战方面具有重要意义。"}}
{"id": "2506.21016", "title": "Fault-Tolerant Spacecraft Attitude Determination using State Estimation Techniques", "authors": ["B. Chidambaram", "A. Hilbert", "M. Silva"], "summary": "The extended and unscented Kalman filter, and the particle filter provide a\nrobust framework for fault-tolerant attitude estimation on spacecraft. This\npaper explores how each filter performs for a large satellite in a low earth\norbit. Additionally, various techniques, built on these filters, for fault\ndetection, isolation and recovery from erroneous sensor measurements, are\nanalyzed. Key results from this analysis include filter performance for various\nfault modes.", "comment": "8 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.21016v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21016v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "容错航天器姿态确定使用状态估计技术", "tldr": "本文探讨了扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波在航天器容错姿态估计中的性能，并分析了基于这些滤波器的故障检测、隔离和恢复技术。", "motivation": "航天器姿态估计需要鲁棒的容错框架，尤其是在传感器测量出现错误时，以确保可靠运行。", "method": "本文采用扩展卡尔曼滤波（EKF）、无迹卡尔曼滤波（UKF）和粒子滤波（PF）作为核心状态估计算法，用于航天器姿态估计。在此基础上，进一步分析了为故障检测、隔离和从错误传感器测量中恢复而构建的各种技术。研究对象是低地球轨道上的大型卫星。", "result": "分析的关键结果包括各种故障模式下不同滤波器的性能表现。", "conclusion": "扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波为航天器容错姿态估计提供了一个鲁棒的框架，并且基于这些滤波器的故障检测、隔离和恢复技术能够有效应对错误的传感器测量。", "translation": "扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波为航天器容错姿态估计提供了一个鲁棒的框架。本文探讨了每种滤波器在低地球轨道大型卫星上的性能。此外，还分析了基于这些滤波器构建的各种用于故障检测、隔离和从错误传感器测量中恢复的技术。这项分析的关键结果包括各种故障模式下的滤波器性能。", "summary": "本文研究了扩展卡尔曼滤波、无迹卡尔曼滤波和粒子滤波在航天器容错姿态估计中的应用。研究重点是评估这些滤波器在低地球轨道大型卫星上的性能，并分析了基于它们的故障检测、隔离和恢复技术，特别是在存在错误传感器测量的情况下。研究结果揭示了不同故障模式下滤波器的表现。", "keywords": "容错, 姿态估计, 卡尔曼滤波, 粒子滤波, 故障检测", "comments": "这篇论文的创新点在于系统地比较了多种状态估计算法（EKF, UKF, PF）在航天器容错姿态确定中的表现，并进一步分析了基于这些滤波器的故障检测、隔离和恢复策略。这对于提高航天器在复杂环境下的自主性和可靠性具有重要意义。"}}
{"id": "2506.21138", "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE", "authors": ["Abdelkarim El-Hajjami", "Camille Salinesi"], "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21138v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21138v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "合成需求有多好？评估LLM生成的AI4RE数据集", "tldr": "研究提出Synthline v1，用于生成合成需求数据，并通过评估不同策略（多样本提示、自动提示优化、后生成筛选）对数据质量的影响，发现合成数据在某些任务上可匹敌甚至超越人工数据，为AI4RE提供数据集短缺的解决方案。", "motivation": "AI4RE领域面临公开可用的带标签需求数据集短缺的重大障碍。尽管大型语言模型（LLM）为合成数据生成提供了有前景的能力，但控制和优化生成需求质量的系统方法尚未得到充分探索。", "method": "论文提出了Synthline v1，一个增强的产品线方法，用于生成合成需求数据，扩展了v0版本，增加了高级生成策略和管理技术。研究通过四个研究问题评估了提示策略、自动化提示优化和后生成管理如何影响四种分类任务（缺陷检测、功能性与非功能性、质量与非质量、安全与非安全）的数据质量。", "result": "多样本提示显著提高了效用和多样性，F1分数提升6-44点。使用PACE进行自动化提示优化产生了依赖于任务的结果，极大地改善了功能性分类（+32.5点），但降低了其他任务的性能。基于相似性的管理提高了多样性，但通常损害了分类性能，表明一些冗余可能对ML模型有帮助。合成需求在特定任务上可以匹敌或超越人工数据，合成数据在安全（+7.8点）和缺陷分类（+15.4点）上优于人工数据。", "conclusion": "这些发现为AI4RE提供了实际见解，并通过系统的合成生成为缓解数据集稀缺性开辟了一条可行之路。", "translation": "公开可用的带标签需求数据集的短缺仍然是推进需求工程人工智能（AI4RE）的主要障碍。尽管大型语言模型（LLM）为合成数据生成提供了有前景的能力，但控制和优化生成需求质量的系统方法仍未得到充分探索。本文提出了Synthline v1，一种增强的产品线方法，用于生成合成需求数据，该方法通过高级生成策略和管理技术扩展了我们早期的v0版本。我们调查了四个研究问题，评估了提示策略、自动化提示优化和后生成管理如何影响四种分类任务的数据质量：缺陷检测、功能性与非功能性、质量与非质量、安全与非安全。我们的评估显示，多样本提示显著提高了单样本生成的效用和多样性，F1分数提升了6到44点。使用PACE（Prompt Actor-Critic Editing）进行自动化提示优化产生了依赖于任务的结果，极大地改善了功能性分类（+32.5点），但降低了其他任务的性能。有趣的是，基于相似性的管理提高了多样性，但通常损害了分类性能，这表明某些冗余可能有助于机器学习模型。最重要的是，我们的结果表明，合成需求在特定任务上可以匹敌或超越人工需求，合成数据在安全（+7.8点）和缺陷分类（+15.4点）上超越了人工数据。这些发现为AI4RE提供了实际见解，并通过系统的合成生成为缓解数据集稀缺性开辟了一条可行之路。", "summary": "本文针对AI4RE领域面临的需求数据集短缺问题，提出并评估了Synthline v1，一个基于LLM的合成需求数据生成框架。该框架通过多样本提示、自动化提示优化和后生成管理等策略，旨在提高生成数据的质量。实验结果表明，多样本提示能显著提升数据效用和多样性；自动化优化效果因任务而异；而基于相似性的管理虽增多样性但可能损害分类性能。最重要的是，研究发现合成数据在缺陷检测和安全分类等特定任务上可超越人工数据，为AI4RE提供了一条通过系统化合成生成来缓解数据集稀缺的有效途径。", "keywords": "合成需求, 大型语言模型, 需求工程人工智能, 数据集生成, 数据质量", "comments": "这项研究通过系统地探索LLM生成合成需求数据的策略，为AI4RE领域的数据稀缺问题提供了创新性的解决方案。其重要性在于证明了在特定任务上合成数据能够媲美甚至超越人工数据，这对于推动AI在需求工程中的应用具有里程碑意义。论文详细评估了不同生成和管理策略对数据质量的影响，提供了宝贵的实践指导。"}}
{"id": "2506.21322", "title": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory", "authors": ["Hong Wang", "Natalia Calvo-Barajas", "Katie Winkle", "Ginevra Castellano"], "summary": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2506.21322v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21322v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "“我该相信谁？”：当家庭医疗机器人与人类记忆矛盾时用户的解读与决策", "tldr": "当家庭医疗机器人提供的信息与用户记忆冲突时，机器人透明度会影响用户对信息差异的解读，且用户倾向于过度信任机器人。", "motivation": "随着智能医疗机器人在家庭环境中部署的可行性增加，当机器人提供的信息与用户记忆矛盾时，用户信任和决策问题随之产生，本研究旨在探讨这种情境下的用户行为。", "method": "本研究采用2x2被试间在线研究，共有176名参与者。参与者观看家庭医疗机器人（Furhat）建议服药时间与用户记忆不符的视频。研究变量是机器人的透明度和社交性水平。", "result": "结果表明，机器人透明度影响用户对信息差异的解读：低透明度机器人下，用户多假设自己记忆有误；高透明度机器人下，参与者更可能将差异归因于外部因素（如伴侣修改信息）。此外，参与者表现出过度信任倾向，常优先采纳机器人建议，即使怀疑系统故障或第三方干扰。", "conclusion": "研究结果强调了机器人系统中透明度机制的重要性、多用户家庭机器人系统访问控制的复杂性和重要性，以及用户在医疗等敏感领域过度依赖机器人的潜在风险。", "translation": "在不久的将来，机器人提供身体协助、心理支持和日常健康管理的能力进步，使得在家庭环境中部署智能医疗机器人变得越来越可行。然而，当这些机器人提供的信息与用户记忆相矛盾时，挑战随之出现，引发了用户信任和决策方面的担忧。本文提出了一项研究，探讨了当机器人提供冲突信息时，改变机器人的透明度和社交性水平如何影响用户的解读、决策和感知信任。在一项2x2被试间在线研究中，176名参与者观看了Furhat机器人作为家庭医疗助手，建议虚构用户在与用户记忆不同时间服药的视频。结果表明，机器人的透明度影响了用户对信息差异的解读：对于低透明度机器人，最常见的假设是用户没有正确记住时间，而对于高透明度机器人，参与者更倾向于将差异归因于外部因素，例如伴侣或其他家庭成员修改了机器人的信息。此外，参与者表现出过度信任的倾向，即使怀疑系统故障或第三方干扰，也常常优先考虑机器人的建议而非用户的记忆。这些发现强调了透明度机制在机器人系统中的影响，多用户机器人部署在家庭环境中系统访问控制的复杂性和重要性，以及用户在医疗等敏感领域过度依赖机器人的潜在风险。", "summary": "本研究探讨了当家庭医疗机器人提供的信息与用户记忆冲突时，机器人的透明度和社交性如何影响用户的解读、决策和信任。通过一项在线实验，发现机器人透明度会影响用户对信息差异的归因（内部记忆错误或外部因素）。研究还揭示了用户普遍存在过度信任机器人的倾向，即便怀疑系统问题也倾向于采纳机器人建议。这些发现提示了在家庭医疗机器人设计中，透明度机制、系统访问控制以及用户过度依赖风险管理的重要性。", "keywords": "家庭医疗机器人, 用户信任, 透明度, 决策, 人机交互", "comments": "本研究揭示了在人机交互，特别是医疗健康领域，用户信任和决策的复杂性。其创新之处在于通过实验设计量化了机器人透明度对用户归因的影响，并指出了用户过度信任的潜在风险。研究结果对于未来家庭医疗机器人的设计，尤其是透明度机制和多用户环境下的访问控制具有重要的指导意义，提醒开发者需关注用户信任的建立与风险规避。"}}
{"id": "2506.21308", "title": "Balancing Privacy and Utility in Correlated Data: A Study of Bayesian Differential Privacy", "authors": ["Martin Lange", "Patricia Guerra-Balboa", "Javier Parra-Arnau", "Thorsten Strufe"], "summary": "Privacy risks in differentially private (DP) systems increase significantly\nwhen data is correlated, as standard DP metrics often underestimate the\nresulting privacy leakage, leaving sensitive information vulnerable. Given the\nubiquity of dependencies in real-world databases, this oversight poses a\ncritical challenge for privacy protections. Bayesian differential privacy (BDP)\nextends DP to account for these correlations, yet current BDP mechanisms\nindicate notable utility loss, limiting its adoption.\n  In this work, we address whether BDP can be realistically implemented in\ncommon data structures without sacrificing utility -- a key factor for its\napplicability. By analyzing arbitrary and structured correlation models,\nincluding Gaussian multivariate distributions and Markov chains, we derive\npractical utility guarantees for BDP. Our contributions include theoretical\nlinks between DP and BDP and a novel methodology for adapting DP mechanisms to\nmeet the BDP requirements. Through evaluations on real-world databases, we\ndemonstrate that our novel theorems enable the design of BDP mechanisms that\nmaintain competitive utility, paving the way for practical privacy-preserving\ndata practices in correlated settings.", "comment": "This is the extended version of the paper accepted in the Proceedings\n  of the VLDB Endowment (PVLDB), 2025. The code used for our experiments is\n  accessible in https://github.com/lange-martin/privacy-utility-bdp", "pdf_url": "http://arxiv.org/pdf/2506.21308v1", "categories": ["cs.CR", "cs.IT", "math.IT", "68P27"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.21308v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "平衡相关数据中的隐私与效用：贝叶斯差分隐私研究", "tldr": "本研究通过新颖的理论和方法，证明了贝叶斯差分隐私（BDP）可以在保持竞争性效用的情况下应用于实际相关数据结构，解决了标准差分隐私在相关数据中隐私泄露评估不足和现有BDP机制效用损失大的问题。", "motivation": "当数据存在关联性时，差分隐私（DP）系统中的隐私风险显著增加，因为标准DP度量往往低估了隐私泄露的程度，使得敏感信息容易受到攻击。贝叶斯差分隐私（BDP）旨在解决DP在处理相关数据时的不足，但现有BDP机制存在明显的效用损失，限制了其应用。", "method": "我们通过分析任意和结构化的关联模型，包括高斯多变量分布和马尔可夫链，推导出了BDP的实用效用保证。我们的贡献包括建立DP和BDP之间的理论联系，以及提出一种新颖的方法来调整DP机制以满足BDP要求。", "result": "通过在真实世界数据库上的评估，我们证明了我们新颖的定理能够设计出保持竞争性效用的BDP机制。", "conclusion": "本研究为在相关数据设置中实现实用的隐私保护数据实践铺平了道路，表明BDP可以在不牺牲效用的情况下应用于常见数据结构。", "translation": "当数据存在关联性时，差分隐私（DP）系统中的隐私风险显著增加，因为标准DP度量往往低估了由此产生的隐私泄露，使得敏感信息容易受到攻击。鉴于现实世界数据库中依赖关系的普遍性，这种疏忽对隐私保护构成了严峻挑战。贝叶斯差分隐私（BDP）扩展了DP以考虑这些关联性，但当前的BDP机制显示出显著的效用损失，限制了其应用。\n在这项工作中，我们探讨了BDP是否能在不牺牲效用的情况下，在常见数据结构中实际实现——这是其适用性的关键因素。通过分析任意和结构化的关联模型，包括高斯多变量分布和马尔可夫链，我们推导出了BDP的实用效用保证。我们的贡献包括建立DP和BDP之间的理论联系，以及提出一种新颖的方法来调整DP机制以满足BDP要求。通过在真实世界数据库上的评估，我们证明了我们新颖的定理能够设计出保持竞争性效用的BDP机制，从而为在相关数据设置中实现实用的隐私保护数据实践铺平了道路。", "summary": "本研究旨在解决标准差分隐私在处理相关数据时隐私泄露评估不足的问题，以及贝叶斯差分隐私（BDP）在考虑数据关联性时存在的效用损失。通过分析多种关联模型，并建立DP与BDP的理论联系，作者提出了一种新颖的方法来设计BDP机制。实验结果表明，这些新机制能够在保持竞争性效用的同时实现BDP，从而为在相关数据环境中应用隐私保护技术提供了实用途径。", "keywords": "差分隐私, 贝叶斯差分隐私, 相关数据, 隐私-效用权衡, 效用保证", "comments": "这项工作在差分隐私领域具有重要意义，尤其是在处理现实世界中普遍存在的相关数据时。其创新之处在于不仅识别了现有BDP机制的效用损失问题，而且通过理论推导和新颖的方法解决了这一问题，使得BDP在实际应用中更具可行性。这对于推动隐私保护技术在复杂数据环境中的落地具有积极作用。"}}
{"id": "2506.20841", "title": "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization", "authors": ["Ha Min Son", "Shahbaz Rezaei", "Xin Liu"], "summary": "Semi-supervised domain generalization (SSDG) aims to solve the problem of\ngeneralizing to out-of-distribution data when only a few labels are available.\nDue to label scarcity, applying domain generalization methods often\nunderperform. Consequently, existing SSDG methods combine semi-supervised\nlearning methods with various regularization terms. However, these methods do\nnot explicitly regularize to learn domains invariant representations across all\ndomains, which is a key goal for domain generalization. To address this, we\nintroduce FixCLR. Inspired by success in self-supervised learning, we change\ntwo crucial components to adapt contrastive learning for explicit domain\ninvariance regularization: utilization of class information from pseudo-labels\nand using only a repelling term. FixCLR can also be added on top of most\nexisting SSDG and semi-supervised methods for complementary performance\nimprovements. Our research includes extensive experiments that have not been\npreviously explored in SSDG studies. These experiments include benchmarking\ndifferent improvements to semi-supervised methods, evaluating the performance\nof pretrained versus non-pretrained models, and testing on datasets with many\ndomains. Overall, FixCLR proves to be an effective SSDG method, especially when\ncombined with other semi-supervised methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20841v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20841v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "FixCLR：用于半监督域泛化的负类对比学习", "tldr": "FixCLR是一种新的半监督域泛化（SSDG）方法，它通过修改对比学习来明确地学习域不变表示，解决了现有SSDG方法在标签稀缺时性能不佳的问题，并在实验中表现出有效性。", "motivation": "现有的半监督域泛化（SSDG）方法在标签稀缺时未能明确地正则化以学习跨所有域的域不变表示，而这对于域泛化是关键目标。", "method": "本文引入了FixCLR，它通过改变对比学习的两个关键组件来适应其进行显式域不变性正则化：一是利用伪标签的类别信息，二是仅使用排斥项。FixCLR还可以添加到大多数现有SSDG和半监督方法之上，以实现互补的性能改进。", "result": "FixCLR被证明是一种有效的半监督域泛化（SSDG）方法，尤其是在与其他半监督方法结合使用时。研究还包括了SSD领域未曾探索过的广泛实验，如基准测试半监督方法的不同改进、评估预训练与非预训练模型的性能以及在多域数据集上进行测试。", "conclusion": "FixCLR是一种有效的半监督域泛化（SSDG）方法，特别是当它与其他的半监督方法结合使用时，能够提高性能。", "translation": "半监督域泛化（SSDG）旨在解决当只有少量标签可用时，泛化到分布外数据的问题。由于标签稀缺，应用域泛化方法通常表现不佳。因此，现有的SSDG方法将半监督学习方法与各种正则化项结合起来。然而，这些方法没有明确地正则化以学习跨所有域的域不变表示，而这对于域泛化是一个关键目标。为了解决这个问题，我们引入了FixCLR。受自监督学习成功的启发，我们改变了两个关键组件，以使对比学习适应显式域不变性正则化：利用伪标签的类别信息和仅使用排斥项。FixCLR还可以添加到大多数现有SSDG和半监督方法之上，以实现互补的性能改进。我们的研究包括了在SSDG研究中以前未曾探索过的广泛实验。这些实验包括对半监督方法不同改进的基准测试、评估预训练与非预训练模型的性能，以及在许多域的数据集上进行测试。总的来说，FixCLR被证明是一种有效的SSDG方法，尤其是在与其他半监督方法结合使用时。", "summary": "本文提出FixCLR，一种针对半监督域泛化（SSDG）的新方法，旨在解决现有SSDG方法在标签稀缺时无法明确学习域不变表示的问题。FixCLR通过修改对比学习，利用伪标签信息并仅使用排斥项来实现显式域不变性正则化。实验表明，FixCLR是一种有效的SSDG方法，尤其在与其他半监督方法结合时能显著提升性能。", "keywords": "半监督域泛化, 对比学习, 域不变性, 伪标签, FixCLR", "comments": "FixCLR的创新之处在于将对比学习适应性地应用于SSDG，并通过利用伪标签信息和专门的排斥项来明确解决域不变性学习的挑战。其能够作为插件与其他现有方法结合，增加了其实用性和普适性。广泛的实验设计也提升了其研究的全面性。"}}
{"id": "2506.21072", "title": "Bridding OT and PaaS in Edge-to-Cloud Continuum", "authors": ["Carlos J Barrios", "Yves Denneulin"], "summary": "The Operational Technology Platform as a Service (OTPaaS) initiative provides\na structured framework for the efficient management and storage of data. It\nensures excellent response times while improving security, reliability, data\nand technology sovereignty, robustness, and energy efficiency, which are\ncrucial for industrial transformation and data sovereignty. This paper\nillustrates successful deployment, adaptable application management, and\nvarious integration components catering to Edge and Cloud environments. It\nleverages the advantages of the Platform as a Service model and highlights key\nchallenges that have been addressed for specific use cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21072v1", "categories": ["cs.DC", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21072v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在边缘到云连续体中连接OT和PaaS", "tldr": "本文介绍了操作技术即服务平台（OTPaaS）倡议，该倡议提供了一个用于数据高效管理和存储的结构化框架，改善了响应时间、安全性、可靠性、数据和技术主权以及能源效率，并在边缘和云环境中成功部署。", "motivation": "当前工业转型和数据主权对数据管理和存储提出了高要求，需要确保卓越的响应时间、提高安全性、可靠性、数据和技术主权、鲁棒性以及能源效率。", "method": "本文提出了操作技术即服务平台（OTPaaS），它提供了一个用于数据高效管理和存储的结构化框架。该框架利用了平台即服务（PaaS）模型的优势，并解决了特定用例中的关键挑战，实现了成功的部署、可适应的应用程序管理和各种集成组件，以满足边缘和云环境的需求。", "result": "OTPaaS实现了卓越的响应时间，提高了安全性、可靠性、数据和技术主权以及能源效率。它在边缘和云环境中实现了成功的部署、可适应的应用程序管理和各种集成组件，并解决了特定用例中的关键挑战。", "conclusion": "本文展示了操作技术即服务平台（OTPaaS）在边缘到云连续体中连接OT和PaaS的成功部署，它提供了一个结构化框架，用于高效管理和存储数据，解决了工业转型中的关键挑战并提升了各项性能指标。", "translation": "操作技术即服务平台（OTPaaS）倡议提供了一个用于数据高效管理和存储的结构化框架。它确保了卓越的响应时间，同时提高了安全性、可靠性、数据和技术主权、鲁棒性和能源效率，这些对于工业转型和数据主权至关重要。本文阐述了成功的部署、可适应的应用程序管理以及满足边缘和云环境的各种集成组件。它利用了平台即服务模型的优势，并强调了针对特定用例已解决的关键挑战。", "summary": "本文介绍了操作技术即服务平台（OTPaaS），这是一个为工业转型和数据主权设计的框架，旨在高效管理和存储数据。OTPaaS通过提供结构化框架、优化响应时间、增强安全性、可靠性、数据主权和能源效率，并在边缘到云环境中实现成功的部署和灵活的应用程序管理，解决了关键挑战。", "keywords": "OTPaaS, 边缘到云连续体, 工业转型, 数据主权, PaaS", "comments": "本文的创新点在于提出了OTPaaS，它有效地将操作技术（OT）与平台即服务（PaaS）模型相结合，以解决工业转型中的数据管理和主权挑战。其重要性体现在提升了边缘到云连续体中的数据效率、安全性和可靠性。"}}
{"id": "2506.21311", "title": "Estimating Technical Loss without Power Flows: A Practical, Data-Driven Approach for Loss Estimation in Distribution Grids", "authors": ["Mohini Bariya", "Genevieve Flaspohler"], "summary": "Electric grids in low- and middle-income countries (LMICs) across the world\nface an acute challenge. To support global decarbonisation efforts and raise\nmillions from energy poverty, these grids must shoulder substantial load growth\nwhile integrating distributed renewable generation. However, decades of rapid\nand poorly funded infrastructure expansions have led to national grids in many\nLMICs that are strained and weak, composed of aging, faulty, and undersized\ninfrastructure. A cause and symptom of this weakness is excessive technical\nloss within the grid infrastructure during energy delivery, particularly at the\ndistribution level; network losses are regularly estimated to be well over 20\npercent, compared to a baseline of 5 percent in higher-income nations.\nAddressing technical loss through targeted interventions is essential for\nbolstering grids' physical and economic strength. Unfortunately, current\napproaches for estimating and localizing technical loss require expensive,\nextensive power flow sensing, which is essentially absent in LMIC distribution\nsystems. We present a novel approach to technical loss estimation without power\nflows, which leverages more readily available voltage magnitude measurements at\nsparse locations in the grid. This estimator puts loss estimation and\nlocalization within reach for LMIC grids globally, and provides a critical tool\nfor the effective design, implementation, and evaluation of loss-reduction\ninterventions.", "comment": "6 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21311v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21311v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在没有潮流的情况下估算技术损耗：一种针对配电网损耗估算的实用数据驱动方法", "tldr": "低收入和中等收入国家（LMICs）的电网面临高技术损耗挑战，现有估算方法需昂贵潮流数据而缺失。本文提出一种新方法，仅利用稀疏电压幅值测量来估算和定位技术损耗，为LMIC电网提供实用工具。", "motivation": "全球低收入和中等收入国家（LMICs）的电网因基础设施老化、故障和不足，导致能源输送过程中技术损耗过高（常超20%），严重影响电网的物理和经济强度。现有估算和定位技术损耗的方法依赖昂贵且在LMIC电网中缺失的潮流传感。", "method": "本文提出一种新颖的数据驱动方法，在没有潮流数据的情况下估算技术损耗。该方法利用电网中稀疏位置更容易获得的电压幅值测量数据进行损耗估算和定位。", "result": "该估算器使得全球LMIC电网的技术损耗估算和定位成为可能。它为损耗减少干预措施的有效设计、实施和评估提供了关键工具，有助于增强电网的物理和经济强度。", "conclusion": "本文提出的方法为LMIC电网提供了一种实用且可行的技术损耗估算和定位方案，有助于加强这些电网的物理和经济实力，并支持其应对负荷增长和可再生能源整合的挑战。", "translation": "全球低收入和中等收入国家（LMICs）的电网面临严峻挑战。为了支持全球脱碳努力并使数百万人摆脱能源贫困，这些电网必须承担巨大的负荷增长，同时整合分布式可再生能源发电。然而，数十年来快速且资金不足的基础设施扩张，导致许多LMIC的全国电网紧张而薄弱，由老化、故障和尺寸不足的基础设施组成。这种薄弱的一个原因和症状是在能源输送过程中，尤其是在配电层面，电网基础设施内部存在过度的技术损耗；与高收入国家5%的基线相比，网络损耗通常估计远超20%。通过有针对性的干预措施解决技术损耗对于增强电网的物理和经济强度至关重要。不幸的是，当前估算和定位技术损耗的方法需要昂贵、广泛的潮流传感，这在LMIC配电系统中基本缺失。我们提出了一种在没有潮流的情况下估算技术损耗的新方法，该方法利用电网中稀疏位置更容易获得的电压幅值测量值。该估算器使得全球LMIC电网的损耗估算和定位成为可能，并为损耗减少干预措施的有效设计、实施和评估提供了关键工具。", "summary": "本文针对低收入和中等收入国家（LMICs）电网普遍存在的过高技术损耗问题，提出了一种无需昂贵潮流传感的新型估算方法。该方法利用电网中稀疏位置可用的电压幅值测量数据来估算和定位技术损耗，旨在为LMIC电网提供一个实用工具，以有效设计和实施减少损耗的干预措施，从而增强电网的物理和经济韧性。", "keywords": "技术损耗, 配电网, 电压测量, 数据驱动, 低收入国家", "comments": "本文的创新之处在于提出了一种无需传统潮流数据即可估算配电网技术损耗的方法，这对于数据和基础设施受限的低收入和中等收入国家（LMICs）具有重要实用价值。它解决了LMIC电网面临的关键挑战，即高技术损耗难以精确量化的问题，为有针对性的干预提供了可行途径。"}}
{"id": "2506.21043", "title": "Analysis of Null Related Beampattern Measures and Signal Quantization Effects for Linear Differential Microphone Arrays", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "summary": "A differential microphone array (DMA) offers enhanced capabilities to obtain\nsharp nulls at the cost of relatively broad peaks in the beam power pattern.\nThis can be used for applications that require nullification or attenuation of\ninterfering sources. To the best of our knowledge, the existing literature\nlacks measures that directly assess the efficacy of nulls, and null-related\nmeasures have not been investigated in the context of differential microphone\narrays (DMAs). This paper offers new insights about the utility of DMAs by\nproposing measures that characterize the nulls in their beam power patterns. We\ninvestigate the performance of differential beamformers by presenting and\nevaluating null-related measures namely null depth (ND) and Null Width (NW) as\na function of depth level relative to the beam power pattern maxima. A study of\nsignal quantization effects due to data acquisition for 1st, 2nd and 3rd order\nlinear DMAs and for different beampatterns i.e. dipole, cardioid, hypercardioid\nand supercardioid is presented. An analytical expression for the quantized\nbeamformed output for any general $ N^{th} $ order DMA is formulated.\nSimulation results of the variation of ND with number of quantization bits and\nthe variation of NW as a function of depth are also presented and inferences\nare drawn. Lab experiments are conducted in a fully anechoic room to support\nthe simulation results. The measured beampattern exhibits a pronounced null\ndepth, confirming the effectiveness of the experimental setup.", "comment": "10 pages, 15 Figures, 3 Tables", "pdf_url": "http://arxiv.org/pdf/2506.21043v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21043v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "线性差分麦克风阵列的零点相关波束图测量和信号量化效应分析", "tldr": "本文提出了表征差分麦克风阵列（DMA）波束图中零点的新度量，并研究了信号量化对零点深度和宽度的影响。", "motivation": "现有文献缺乏直接评估零点效用的度量，且未在差分麦克风阵列（DMA）背景下研究零点相关度量。DMA在消除干扰源方面有应用潜力。", "method": "提出了零点深度（ND）和零点宽度（NW）作为表征DMA波束图零点的新度量。研究了1阶、2阶和3阶线性DMA以及不同波束图（偶极、心形、超心形）的信号量化效应。推导了任意N阶DMA量化波束形成输出的解析表达式。通过仿真和消声室实验验证结果。", "result": "提出了零点深度（ND）和零点宽度（NW）度量。研究了量化位数对ND的影响以及深度对NW的影响。实验结果支持了仿真结果，测量到的波束图显示出明显的零点深度。", "conclusion": "本文提出了有效评估DMA零点性能的度量，并量化了信号量化对这些零点的影响，通过仿真和实验验证了其有效性。", "translation": "差分麦克风阵列（DMA）能够以波束功率图相对较宽的峰值为代价，获得增强的锐利零点能力。这可用于需要消除或衰减干扰源的应用。据我们所知，现有文献缺乏直接评估零点效用的度量，并且在差分麦克风阵列（DMA）的背景下尚未研究零点相关度量。本文通过提出表征其波束功率图中零点的度量，为DMA的效用提供了新的见解。我们通过展示和评估零点相关度量，即零点深度（ND）和零点宽度（NW）作为相对于波束功率图最大值的深度水平的函数，来研究差分波束形成器的性能。本文还介绍了对1阶、2阶和3阶线性DMA以及不同波束图（即偶极、心形、超心形和超心形）由于数据采集引起的信号量化效应的研究。公式化了任意N阶DMA量化波束形成输出的解析表达式。还提出了ND随量化位数的变化以及NW随深度变化的仿真结果，并得出了推论。在全消声室中进行了实验室实验以支持仿真结果。测得的波束图显示出明显的零点深度，证实了实验设置的有效性。", "summary": "本文针对差分麦克风阵列（DMA）在现有文献中缺乏零点评估度量的问题，提出了零点深度（ND）和零点宽度（NW）两种新度量来表征其波束图中的零点。研究了1至3阶线性DMA在不同波束图下的信号量化效应，并推导了量化波束形成输出的解析表达式。仿真和消声室实验结果验证了所提度量的有效性以及信号量化对零点性能的影响。", "keywords": "差分麦克风阵列, 零点深度, 零点宽度, 信号量化, 波束形成", "comments": "本文的创新点在于首次提出了专门用于评估差分麦克风阵列零点性能的量度（零点深度和零点宽度），填补了现有文献的空白。同时，还深入分析了信号量化对这些零点性能的影响，并给出了理论推导和实验验证，使得研究更为全面和实用。"}}
{"id": "2506.21245", "title": "GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models", "authors": ["Qifei Cui", "Xinyu Lu"], "summary": "This work introduces a novel framework for brain tumor segmentation\nleveraging pre-trained GANs and Unet architectures. By combining a global\nanomaly detection module with a refined mask generation network, the proposed\nmodel accurately identifies tumor-sensitive regions and iteratively enhances\nsegmentation precision using adversarial loss constraints. Multi-modal MRI data\nand synthetic image augmentation are employed to improve robustness and address\nthe challenge of limited annotated datasets. Experimental results on the BraTS\ndataset demonstrate the effectiveness of the approach, achieving high\nsensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the\nbaseline. This scalable method minimizes the dependency on fully annotated\ndata, paving the way for practical real-world applications in clinical\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21245v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21245v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GANet-Seg：基于混合生成模型的脑肿瘤对抗学习分割", "tldr": "GANet-Seg是一种结合GAN和Unet的脑肿瘤分割框架，通过对抗学习和数据增强提高分割精度并减少对标注数据的依赖。", "motivation": "针对脑肿瘤分割中有限标注数据集的挑战。", "method": "提出GANet-Seg框架，结合预训练GAN和Unet架构，包含全局异常检测模块和精细掩膜生成网络，利用对抗损失迭代提高精度，并采用多模态MRI数据和合成图像增强来提高鲁棒性。", "result": "在BraTS数据集上，该方法表现出比基线更高的敏感性和准确性（Dice和HD95指标）。", "conclusion": "该可扩展方法减少了对完全标注数据的依赖，为临床环境中的实际应用铺平了道路。", "translation": "这项工作引入了一种新颖的脑肿瘤分割框架，该框架利用了预训练的GAN和Unet架构。通过将全局异常检测模块与精细掩膜生成网络相结合，所提出的模型能够准确识别肿瘤敏感区域，并利用对抗性损失约束迭代地提高分割精度。研究中采用了多模态MRI数据和合成图像增强来提高鲁棒性，并解决了标注数据集有限的挑战。在BraTS数据集上的实验结果表明，该方法在病灶Dice和HD95指标上均表现出比基线更高的敏感性和准确性。这种可扩展的方法最大限度地减少了对完全标注数据的依赖，为临床环境中的实际应用铺平了道路。", "summary": "GANet-Seg是一个创新的脑肿瘤分割框架，它整合了预训练的GANs和Unet架构。该模型通过结合全局异常检测和精细掩膜生成网络，并利用对抗性损失迭代地提高分割精度。为了解决标注数据有限的问题并增强鲁棒性，研究使用了多模态MRI数据和合成图像增强。在BraTS数据集上的实验结果表明，该方法在病灶Dice和HD95指标上均优于基线，具有高敏感性和准确性。该方法可扩展，减少了对完全标注数据的依赖，有望应用于临床实践。", "keywords": "脑肿瘤分割, 对抗学习, GAN, Unet, 数据增强", "comments": "该论文创新性地结合了GAN和Unet架构，并通过对抗学习和数据增强有效解决了脑肿瘤分割中标注数据稀缺的挑战。其减少对完全标注数据依赖的特性，对于实际临床应用具有重要意义。"}}
{"id": "2506.21310", "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems", "authors": ["Pauline Speckmann", "Mario Nadj", "Christian Janiesch"], "summary": "Although several post-hoc methods for explainable AI have been developed,\nmost are static and neglect the user perspective, limiting their effectiveness\nfor the target audience. In response, we developed the interactive explainable\nintelligent system called IXAII that offers explanations from four explainable\nAI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored\nviews for five user groups and gives users agency over the explanations'\ncontent and their format. We evaluated IXAII through interviews with experts\nand lay users. Our results indicate that IXAII, which provides different\nexplanations with multiple visualization options, is perceived as helpful to\nincrease transparency. By bridging the gaps between explainable AI methods,\ninteractivity, and practical implementation, we provide a novel perspective on\nAI explanation practices and human-AI interaction.", "comment": "9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track", "pdf_url": "http://arxiv.org/pdf/2506.21310v1", "categories": ["cs.AI", "cs.SE", "K.6.3 Software Management"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21310v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "IXAII：一种用于决策支持系统的交互式可解释人工智能接口", "tldr": "开发了一个名为IXAII的交互式可解释AI系统，它结合了多种解释方法并提供定制化视图，通过专家和普通用户访谈证实其有助于提高透明度并弥合XAI实践与人机交互之间的鸿沟。", "motivation": "尽管已经开发了几种用于可解释AI的事后方法，但大多数都是静态的，并且忽视用户视角，限制了它们对目标受众的有效性。", "method": "开发了一个名为IXAII的交互式可解释智能系统，该系统提供了来自LIME、SHAP、Anchors和DiCE四种可解释AI方法的解释。原型为五种用户组提供了定制化视图，并赋予用户对解释内容和格式的控制权。通过与专家和普通用户访谈对IXAII进行了评估。", "result": "评估结果表明，IXAII（提供不同解释和多种可视化选项）被认为有助于提高透明度。", "conclusion": "通过弥合可解释AI方法、交互性和实际实施之间的差距，该研究为AI解释实践和人机交互提供了新颖的视角。", "translation": "尽管已经开发了几种用于可解释AI的事后方法，但大多数都是静态的，并且忽视了用户视角，限制了它们对目标受众的有效性。为此，我们开发了一个名为IXAII的交互式可解释智能系统，该系统提供了来自LIME、SHAP、Anchors和DiCE四种可解释AI方法的解释。我们的原型为五种用户组提供了定制化视图，并赋予用户对解释内容和格式的控制权。我们通过与专家和普通用户的访谈对IXAII进行了评估。我们的结果表明，IXAII（提供不同解释和多种可视化选项）被认为有助于提高透明度。通过弥合可解释AI方法、交互性和实际实施之间的差距，我们为AI解释实践和人机交互提供了新颖的视角。", "summary": "本文针对现有可解释AI方法静态且忽视用户视角的局限性，开发了交互式可解释AI系统IXAII。该系统集成了LIME、SHAP、Anchors和DiCE等四种解释方法，为不同用户群体提供定制化视图和解释内容的控制权。通过专家和普通用户的访谈评估，IXAII被证实能有效提高透明度。研究认为，IXAII通过结合多种解释方法、强调交互性与实用性，为AI解释实践和人机交互提供了新颖的视角。", "keywords": "可解释AI, 交互式界面, 决策支持系统, 人机交互, 透明度", "comments": "这项研究通过引入交互性显著提升了可解释AI的实用性，解决了现有方法在用户参与和适应性方面的不足。其创新点在于整合多种XAI方法并提供高度定制化的用户体验，这对于决策支持系统中的AI透明度至关重要。将用户视角置于核心地位，是推动XAI从理论走向实际应用的关键一步。"}}
{"id": "2506.21441", "title": "An evaluation of level of detail degradation in head-mounted display peripheries", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Martin Reddy"], "summary": "A paradigm for the design of systems that manage level of detail in virtual\nenvironments is proposed. As an example of the prototyping step in this\nparadigm, a user study was performed to evaluate the effectiveness of high\ndetail insets used with head-mounted displays. Ten subjects were given a simple\nsearch task that required the location and identification of a single target\nobject. All subjects used seven different displays (the independent variable),\nvarying in inset size and peripheral detail, to perform this task. Frame rate,\ntarget location, subject input method, and order of display use were all\ncontrolled. Primary dependent measures were search time on trials with correct\nidentification, and the percentage of all trials correctly identified. ANOVAs\nof the results showed that insetless, high detail displays did not lead to\nsignificantly different search times or accuracies than displays with insets.\nIn fact, only the insetless, low detail display returned significantly\ndifferent results. Further research is being performed to examine the effect of\nvarying task complexity, inset size, and level of detail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21441v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21441v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "头戴式显示器周边细节层次退化的评估", "tldr": "一项用户研究评估了头戴式显示器中使用高细节插入区（insets）的有效性。结果显示，与带有插入区的显示器相比，无插入区的高细节显示器在搜索时间或准确性上没有显著差异，而只有无插入区的低细节显示器表现出显著差异。", "motivation": "本文旨在评估在虚拟环境中管理细节层次（Level of Detail, LoD）的系统设计范例中，高细节插入区在头戴式显示器（HMD）中的有效性。", "method": "研究采用用户研究范式，十名受试者执行一个简单的搜索任务，要求定位并识别单个目标对象。受试者使用了七种不同的显示器设置（独立变量），这些设置在插入区大小和外围细节上有所不同。帧率、目标位置、受试者输入方法和显示器使用顺序均受到控制。主要因变量是正确识别试验的搜索时间以及所有试验正确识别的百分比，数据通过方差分析（ANOVAs）进行分析。", "result": "方差分析结果表明，无插入区的高细节显示器与带有插入区的显示器在搜索时间或准确性上没有显著差异。事实上，只有无插入区的低细节显示器产生了显著不同的结果。", "conclusion": "对于简单的搜索任务，在头戴式显示器中使用高细节插入区可能不会比全高细节显示器带来显著优势，而无插入区的低细节显示器则会对性能产生负面影响。", "translation": "本文提出了一种用于管理虚拟环境中细节层次（Level of Detail）的系统设计范式。作为该范式中原型设计步骤的一个例子，我们进行了一项用户研究，以评估头戴式显示器中使用高细节插入区（insets）的有效性。十名受试者被分配了一个简单的搜索任务，要求定位和识别单个目标对象。所有受试者使用七种不同的显示器（独立变量），这些显示器在插入区大小和外围细节方面有所不同，以执行此任务。帧率、目标位置、受试者输入方法和显示器使用顺序均受到控制。主要的因变量是正确识别试验的搜索时间，以及所有试验正确识别的百分比。对方差分析结果显示，无插入区的高细节显示器在搜索时间或准确性上与带有插入区的显示器没有显著差异。事实上，只有无插入区的低细节显示器返回了显著不同的结果。正在进行进一步的研究，以检查不同任务复杂性、插入区大小和细节层次的影响。", "summary": "本文提出了一种虚拟环境细节层次管理系统设计范式。作为原型设计示例，研究通过用户实验评估了头戴式显示器中高细节插入区的效果。十名受试者在七种不同显示条件下执行简单搜索任务。结果显示，高细节无插入区显示与有插入区显示在搜索时间及准确性上无显著差异，仅低细节无插入区显示表现出显著差异。", "keywords": "头戴式显示器, 细节层次, 外围视觉, 用户研究, 虚拟环境", "comments": "这项研究深入探讨了头戴式显示器中细节层次管理的一个关键问题，即在不牺牲用户体验的前提下优化渲染性能。研究结果表明，对于简单任务，高细节插入区可能未能带来预期的性能或感知优势，这对于VR系统设计者来说是一个重要的发现。它提示我们，在某些情况下，引入复杂渲染策略（如插入区）的收益可能不抵其复杂性。此外，研究明确了低细节无插入区对性能的负面影响，这符合预期。未来的研究可以探索更复杂的任务场景或不同的细节管理策略。"}}
{"id": "2506.21175", "title": "On Minimizing Wiggle in Stacked Area Charts", "authors": ["Alexander Dobler", "Martin Nöllenburg"], "summary": "Stacked area charts are a widely used visualization technique for numerical\ntime series. The x-axis represents time, and the time series are displayed as\nhorizontal, variable-height layers stacked on top of each other. The height of\neach layer corresponds to the time series values at each time point. The main\naesthetic criterion for optimizing the readability of stacked area charts is\nthe amount of vertical change of the borders between the time series in the\nvisualization, called wiggle. While many heuristic algorithms have been\ndeveloped to minimize wiggle, the computational complexity of minimizing wiggle\nhas not been formally analyzed. In this paper, we show that different variants\nof wiggle minimization are NP-hard and even hard to approximate. We also\npresent an exact mixed-integer linear programming formulation and compare its\nperformance with a state-of-the-art heuristic in an experimental evaluation.\nLastly, we consider a special case of wiggle minimization that corresponds to\nthe fundamentally interesting and natural problem of ordering a set of numbers\nas to minimize their sum of absolute prefix sums. We show several complexity\nresults for this problem that imply some of the mentioned hardness results for\nwiggle minimization.", "comment": "19 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.21175v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21175v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于最小化堆叠面积图中“摆动”的研究", "tldr": "本文首次形式化分析了堆叠面积图中“摆动”最小化问题的计算复杂性，证明其为NP-难且难以近似，并提出了一个精确的混合整数线性规划公式。", "motivation": "堆叠面积图是一种常用的时间序列可视化技术，其可读性的关键美学标准是层间边界的垂直变化量，即“摆动”。尽管已有许多启发式算法来最小化“摆动”，但该问题的计算复杂性尚未得到正式分析。因此，本文旨在填补这一空白，对“摆动”最小化问题进行形式化复杂性分析。", "method": "本文通过证明“摆动”最小化的不同变体是NP-难甚至难以近似，进行了复杂性分析。同时，提出了一个精确的混合整数线性规划（MILP）公式，并将其性能与最先进的启发式算法进行了实验比较。此外，还考虑了一个特殊的“摆动”最小化情况，即最小化一组数字的绝对前缀和的问题，并展示了其复杂性结果。", "result": "研究结果表明，“摆动”最小化的不同变体是NP-难问题，甚至难以近似。对于最小化绝对前缀和的特殊情况，也得出了几个复杂性结果，这些结果也暗示了“摆动”最小化的一些硬度结果。实验评估比较了提出的MILP公式与现有启发式算法的性能。", "conclusion": "本文首次对堆叠面积图中“摆动”最小化问题的计算复杂性进行了形式化分析，证明了其NP-难和难以近似的性质，并提供了一个精确的混合整数线性规划公式，为解决该问题提供了理论基础和新的方法。", "translation": "堆叠面积图是一种广泛用于数值时间序列的可视化技术。X轴代表时间，时间序列显示为水平、可变高度的层，彼此堆叠。每个层的高度对应于每个时间点的时间序列值。优化堆叠面积图可读性的主要美学标准是可视化中时间序列之间边界的垂直变化量，称为“摆动”。尽管已经开发了许多启发式算法来最小化“摆动”，但最小化“摆动”的计算复杂性尚未得到正式分析。在本文中，我们证明了“摆动”最小化的不同变体是NP-难的，甚至难以近似。我们还提出了一个精确的混合整数线性规划公式，并在实验评估中将其性能与最先进的启发式算法进行了比较。最后，我们考虑了“摆动”最小化的一个特殊情况，该情况对应于对一组数字进行排序以最小化其绝对前缀和的根本有趣且自然的问题。我们展示了该问题的几个复杂性结果，这些结果暗示了“摆动”最小化的一些硬度结果。", "summary": "本文首次对堆叠面积图中“摆动”最小化问题进行了形式化计算复杂性分析。研究证明了该问题的不同变体是NP-难且难以近似的。为解决此问题，论文提出了一个精确的混合整数线性规划（MILP）公式，并将其性能与现有启发式算法进行了实验比较。此外，文章还探讨了“摆动”最小化问题的一个特殊情况，即最小化一组数字绝对前缀和的问题，并推导了相关的复杂性结果，这些结果进一步支持了“摆动”最小化问题的硬度。", "keywords": "堆叠面积图, 摆动最小化, NP-难, 混合整数线性规划, 计算复杂性", "comments": "本文的创新之处在于首次对堆叠面积图中“摆动”最小化这一重要的可视化优化问题进行了严格的计算复杂性分析，填补了该领域的理论空白。证明其NP-难和难以近似的性质，为未来算法设计提供了重要的理论指导。提出的混合整数线性规划公式也为寻求精确解提供了一条新途径。同时，将该问题与最小化绝对前缀和的特殊情况联系起来，也展现了问题的深层数学联系。"}}
{"id": "2506.20995", "title": "Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance", "authors": ["Akio Hayakawa", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji"], "summary": "We propose a novel step-by-step video-to-audio generation method that\nsequentially produces individual audio tracks, each corresponding to a specific\nsound event in the video. Our approach mirrors traditional Foley workflows,\naiming to capture all sound events induced by a given video comprehensively.\nEach generation step is formulated as a guided video-to-audio synthesis task,\nconditioned on a target text prompt and previously generated audio tracks. This\ndesign is inspired by the idea of concept negation from prior compositional\ngeneration frameworks. To enable this guided generation, we introduce a\ntraining framework that leverages pre-trained video-to-audio models and\neliminates the need for specialized paired datasets, allowing training on more\naccessible data. Experimental results demonstrate that our method generates\nmultiple semantically distinct audio tracks for a single input video, leading\nto higher-quality composite audio synthesis than existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20995v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20995v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于负向音频引导的逐步骤视频到音频合成", "tldr": "提出了一种逐步骤视频到音频合成方法，通过负向音频引导生成独立音轨，无需专用配对数据集，且效果优于现有基线。", "motivation": "该研究旨在全面捕捉给定视频中引起的所有声音事件，效仿传统的拟音工作流程。同时，它解决了对专用配对数据集的需求，通过利用更易获取的数据进行训练。", "method": "本文提出了一种新颖的逐步骤视频到音频生成方法，该方法顺序生成与视频中特定声音事件对应的独立音轨。每个生成步骤都被表述为一项引导式视频到音频合成任务，以目标文本提示和先前生成的音轨为条件。该设计灵感来源于先前组合生成框架中的概念否定思想。为实现这种引导式生成，研究引入了一个训练框架，该框架利用预训练的视频到音频模型，并消除了对专用配对数据集的需求。", "result": "实验结果表明，该方法能够为单个输入视频生成多个语义上不同的音轨，从而实现比现有基线更高质量的复合音频合成。", "conclusion": "本文提出的基于负向音频引导的逐步骤视频到音频合成方法，能够生成多个语义上独立的音轨，并合成高质量的复合音频，且无需专用配对数据集，性能优于现有基线方法。", "translation": "我们提出了一种新颖的逐步骤视频到音频生成方法，该方法顺序生成与视频中特定声音事件对应的独立音轨。我们的方法模仿传统的拟音工作流程，旨在全面捕捉给定视频中引起的所有声音事件。每个生成步骤都被表述为一项引导式视频到音频合成任务，以目标文本提示和先前生成的音轨为条件。该设计灵感来源于先前组合生成框架中的概念否定思想。为实现这种引导式生成，我们引入了一个训练框架，该框架利用预训练的视频到音频模型，并消除了对专用配对数据集的需求，从而可以在更易获取的数据上进行训练。实验结果表明，我们的方法能够为单个输入视频生成多个语义上不同的音轨，从而实现比现有基线更高质量的复合音频合成。", "summary": "本文提出了一种新颖的逐步骤视频到音频合成方法，该方法通过顺序生成与视频中特定声音事件对应的独立音轨来模拟传统的拟音工作流程。每个生成步骤都受到目标文本提示和先前音轨的引导，并借鉴了概念否定思想。该方法利用预训练模型并在无需专用配对数据集的情况下进行训练。实验证明，其能为单个视频生成多个语义上不同的音轨，并实现比现有基线更高质量的复合音频合成。", "keywords": "视频到音频合成, 拟音, 负向音频引导, 逐步骤生成, 声音事件生成", "comments": "该论文的创新点在于其逐步骤的视频到音频合成方法，它模仿了传统的拟音工作流程，并通过负向音频引导实现精确的音轨生成。特别值得注意的是，该方法利用现有预训练模型，并成功避免了对昂贵的专用配对数据集的需求，这大大降低了训练门槛。其生成多个语义上独立音轨并实现高质量复合音频的能力，展现了其在音频合成领域的显著进步。"}}
{"id": "2506.21368", "title": "Real-time and personalized product recommendations for large e-commerce platforms", "authors": ["Matteo Tolloso", "Davide Bacciu", "Shahab Mokarizadeh", "Marco Varesi"], "summary": "We present a methodology to provide real-time and personalized product\nrecommendations for large e-commerce platforms, specifically focusing on\nfashion retail. Our approach aims to achieve accurate and scalable\nrecommendations with minimal response times, ensuring user satisfaction,\nleveraging Graph Neural Networks and parsimonious learning methodologies.\nExtensive experimentation with datasets from one of the largest e-commerce\nplatforms demonstrates the effectiveness of our approach in forecasting\npurchase sequences and handling multi-interaction scenarios, achieving\nefficient personalized recommendations under real-world constraints.", "comment": "This paper has been accepted for publication at the International\n  Conference on Artificial Neural Networks (ICANN) 2025. The final\n  authenticated version will be available for purchase through the publisher's\n  website. The conference proceedings will be published by Springer in the\n  Lecture Notes in Computer Science (LNCS) series", "pdf_url": "http://arxiv.org/pdf/2506.21368v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.21368v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "大型电商平台的实时个性化产品推荐", "tldr": "该研究提出了一种利用图神经网络为大型电商平台提供实时个性化产品推荐的方法，尤其适用于时尚零售，并证明其在预测购买序列和处理多交互场景方面的有效性。", "motivation": "为大型电商平台提供准确、可扩展且响应时间短的实时个性化产品推荐，以确保用户满意度，特别是在时尚零售领域。", "method": "利用图神经网络（Graph Neural Networks）和简约学习方法（parsimonious learning methodologies）。", "result": "在大型电商平台数据集上的广泛实验表明，该方法在预测购买序列和处理多交互场景方面有效，并在实际约束下实现了高效的个性化推荐。", "conclusion": "该方法能够为大型电商平台提供高效、实时且个性化的产品推荐，尤其适用于预测购买序列和处理多交互场景。", "translation": "我们提出了一种为大型电商平台提供实时个性化产品推荐的方法，特别关注时尚零售。我们的方法旨在通过最小的响应时间实现准确且可扩展的推荐，确保用户满意度，并利用图神经网络和简约学习方法。通过对一个最大的电商平台数据集进行广泛实验，证明了我们的方法在预测购买序列和处理多交互场景方面的有效性，并在实际约束下实现了高效的个性化推荐。", "summary": "本文提出了一种针对大型电商平台（特别是时尚零售）的实时个性化产品推荐方法。该方法结合了图神经网络和简约学习技术，旨在提供准确、可扩展且响应迅速的推荐。实验结果表明，该方法能有效预测购买序列并处理复杂的多交互场景，在实际应用中展现出高效的个性化推荐能力。", "keywords": "实时推荐, 个性化推荐, 图神经网络, 电商, 时尚零售", "comments": "该论文的创新点在于将图神经网络和简约学习方法应用于大型电商平台的实时个性化推荐，尤其关注时尚零售场景。其重要性在于解决了大规模平台下推荐系统的响应时间、准确性和可扩展性问题。论文通过实际数据集验证了其有效性，但抽象中未提及具体的模型架构细节或与现有SOTA方法的详细对比。"}}
{"id": "2506.21070", "title": "Inverse source problem with a posteriori interior measurements for space-time fractional diffusion equations", "authors": ["Kai Yu", "Zhiyuan Li", "Yikan Liu"], "summary": "This paper investigates an inverse source problem for space-time fractional\ndiffusion equations from a posteriori interior measurements. The uniqueness\nresult is established by the memory effect of fractional derivatives and the\nunique continuation property. For the numerical reconstruction, the inverse\nproblem is reformulated as an optimization problem with the Tikhonov\nregularization. We use the Levenberg-Marquardt method to identity the unknown\nsource from noisy measurements. Finally, we give some numerical examples to\nillustrate the efficiency and accuracy of the proposed algorithm.", "comment": "14 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.21070v1", "categories": ["math.NA", "cs.NA", "35R30, 35R11"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21070v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "具有后验内部测量的时间分数阶扩散方程的逆源问题", "tldr": "本文研究了具有后验内部测量的时间分数阶扩散方程的逆源问题，通过理论分析证明了唯一性，并使用Tikhonov正则化和Levenberg-Marquardt方法进行数值重建，数值例子验证了算法的效率和准确性。", "motivation": "本文研究了空间-时间分数阶扩散方程的逆源问题，该问题从后验内部测量中推断未知源。", "method": "本文通过分数阶导数的记忆效应和唯一延拓性质建立了唯一性结果。对于数值重建，将逆问题重新表述为Tikhonov正则化的优化问题，并使用Levenberg-Marquardt方法从噪声测量中识别未知源。", "result": "本文通过分数阶导数的记忆效应和唯一延拓性质建立了唯一性结果。数值例子证明了所提出算法的效率和准确性。", "conclusion": "本文成功解决了空间-时间分数阶扩散方程的逆源问题，通过理论分析确保了唯一性，并通过Tikhonov正则化和Levenberg-Marquardt方法实现了有效的数值重建，数值实验验证了其性能。", "translation": "本文研究了具有后验内部测量的时间分数阶扩散方程的逆源问题。分数阶导数的记忆效应和唯一延拓性质确立了唯一性结果。对于数值重建，将逆问题重新表述为Tikhonov正则化的优化问题。我们使用Levenberg-Marquardt方法从噪声测量中识别未知源。最后，我们给出了一些数值例子来说明所提出算法的效率和准确性。", "summary": "本文研究了从后验内部测量中恢复空间-时间分数阶扩散方程未知源的逆问题。论文首先通过分数阶导数的记忆效应和唯一延拓性质证明了该问题的唯一性。随后，为了进行数值重建，将逆问题转化为一个带有Tikhonov正则化的优化问题，并采用Levenberg-Marquardt方法从带噪声的测量数据中识别未知源。最后，通过数值例子验证了所提出算法的效率和准确性。", "keywords": "逆源问题, 分数阶扩散方程, 后验内部测量, Tikhonov正则化, Levenberg-Marquardt方法", "comments": "本文在理论上证明了逆源问题的唯一性，并结合Tikhonov正则化和Levenberg-Marquardt方法提供了实用的数值重建算法，具有较高的创新性。该方法对处理分数阶扩散方程的逆问题具有重要意义。"}}
{"id": "2506.20917", "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "authors": ["Zhengyan Shi"], "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "comment": "PhD Thesis", "pdf_url": "http://arxiv.org/pdf/2506.20917v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20917v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "优化下游任务的语言模型：一个后训练视角", "tldr": "本文提出了一系列后训练方法，以提高语言模型在下游任务中的适应性、鲁棒性、效率和泛化能力，解决了现有微调的挑战。", "motivation": "语言模型在NLP中能力显著，但将其高效、鲁棒地适应特定任务仍具挑战。现有微调方法常存在未标注数据利用不足、小数据集过拟合、计算成本高昂等问题，限制了其在真实世界任务中的应用。", "method": "本文提出一系列方法：1) 探索从未标注数据中提取知识的策略，引入一种优于现有半监督方法的持续预训练技术。2) 提出一种参数高效的微调方法，显著降低内存和计算成本，同时保持竞争力性能。3) 引入改进的监督微调方法，使语言模型更好地遵循指令，尤其在标注数据稀缺时，提升其在多种NLP任务（包括开放式生成）上的性能。4) 开发新的评估方法和基准，如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。", "result": "通过在多种NLP任务上的广泛实证研究，结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更适应广泛的应用。", "conclusion": "这些进展标志着在构建更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。", "translation": "语言模型（LMs）在自然语言处理（NLP）中展现出卓越的能力，但如何高效、鲁棒地使其适应特定任务仍具挑战。随着其规模和复杂性的增长，在标注数据上对语言模型进行微调常常导致未标注数据利用不足、在小型任务特定数据集上过拟合，并带来显著的计算成本。这些限制阻碍了它们在开放式真实世界语言任务中的应用。\n本论文提出了一系列方法，以更好地使语言模型适应下游应用。首先，我们探索了从未标注数据中提取任务相关知识的策略，引入了一种新颖的持续预训练技术，其性能优于最先进的半监督方法。其次，我们提出了一种参数高效的微调方法，该方法在保持竞争力性能的同时，大幅降低了内存和计算成本。我们还引入了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标注数据稀缺时，从而提升了它们在包括开放式生成在内的一系列NLP任务中的性能。最后，我们开发了新的评估方法和基准，例如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。\n通过在各种NLP任务中进行的广泛实证研究，我们的结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更适应广泛的应用。这些进步标志着在实现更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。", "summary": "本文针对语言模型在下游任务适应性方面的挑战，提出了一系列后训练方法。这些方法包括持续预训练以利用未标注数据、参数高效的微调以降低成本、以及改进的监督微调以增强指令遵循能力。研究还开发了新的评估基准。实证结果表明，这些方法显著提升了语言模型的鲁棒性、效率和泛化能力，使其能更好地适应广泛的实际应用。", "keywords": "语言模型优化, 后训练, 参数高效微调, 持续预训练, 下游任务", "comments": "本文的创新点在于从“后训练”视角系统性地解决了语言模型在下游任务适应性上的关键挑战，特别是对未标注数据的有效利用和计算效率的提升。其提出的持续预训练、参数高效微调和改进监督微调等方法，为实际应用中部署大型语言模型提供了更经济、更鲁棒的途径。这对于推动语言模型在资源受限或数据稀缺场景下的应用具有重要意义，并对通用人工智能的发展有所贡献。"}}
{"id": "2506.21478", "title": "SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture", "authors": ["Kehan Sui", "Jinxu Xiang", "Fang Jin"], "summary": "Singing voice synthesis (SVS) aims to generate expressive and high-quality\nvocals from musical scores, requiring precise modeling of pitch, duration, and\narticulation. While diffusion-based models have achieved remarkable success in\nimage and video generation, their application to SVS remains challenging due to\nthe complex acoustic and musical characteristics of singing, often resulting in\nartifacts that degrade naturalness. In this work, we propose SmoothSinger, a\nconditional diffusion model designed to synthesize high quality and natural\nsinging voices. Unlike prior methods that depend on vocoders as a final stage\nand often introduce distortion, SmoothSinger refines low-quality synthesized\naudio directly in a unified framework, mitigating the degradation associated\nwith two-stage pipelines. The model adopts a reference-guided dual-branch\narchitecture, using low-quality audio from any baseline system as a reference\nto guide the denoising process, enabling more expressive and context-aware\nsynthesis. Furthermore, it enhances the conventional U-Net with a parallel\nlow-frequency upsampling path, allowing the model to better capture pitch\ncontours and long term spectral dependencies. To improve alignment during\ntraining, we replace reference audio with degraded ground truth audio,\naddressing temporal mismatch between reference and target signals. Experiments\non the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that\nSmoothSinger achieves state-of-the-art results in both objective and subjective\nevaluations. Extensive ablation studies confirm its effectiveness in reducing\nartifacts and improving the naturalness of synthesized voices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21478v1", "categories": ["cs.SD", "cs.AI"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.21478v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SmoothSinger：一种带有多分辨率架构的条件扩散模型用于歌唱语音合成", "tldr": "SmoothSinger是一种条件扩散模型，通过直接在统一框架中细化低质量音频并采用参考引导的双分支架构，实现了高质量和自然的歌唱语音合成，在中文歌唱数据集上达到了SOTA性能。", "motivation": "歌唱语音合成（SVS）需要精确建模音高、持续时间和发音，但扩散模型在SVS中的应用仍具挑战性，因为歌唱的复杂声学和音乐特性常导致影响自然度的伪影。此外，依赖声码器的传统两阶段方法会引入失真。", "method": "本文提出了SmoothSinger，一个条件扩散模型，用于合成高质量和自然的歌唱语音。它在一个统一的框架中直接细化低质量合成音频，避免了两阶段管道的降级。模型采用参考引导的双分支架构，使用任何基线系统的低质量音频作为参考来指导去噪过程。它还通过并行低频上采样路径增强了传统的U-Net，以更好地捕获音高轮廓和长期频谱依赖。为改善训练期间的对齐，它用降级的真实音频替换参考音频，解决了参考信号和目标信号之间的时间不匹配。", "result": "在Opencpop数据集（一个大规模中文歌唱语料库）上的实验表明，SmoothSinger在客观和主观评估中均达到了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成语音自然度方面的有效性。", "conclusion": "SmoothSinger通过其独特的架构和训练策略，成功克服了传统扩散模型在歌唱语音合成中的挑战，生成了高质量、自然的歌唱语音，并达到了最先进的性能。", "translation": "歌唱语音合成（SVS）旨在从乐谱生成富有表现力的高质量人声，需要精确建模音高、持续时间和发音。虽然基于扩散的模型在图像和视频生成中取得了显著成功，但由于歌唱复杂的声学和音乐特性，它们在SVS中的应用仍然具有挑战性，常导致降低自然度的伪影。在这项工作中，我们提出了SmoothSinger，一个条件扩散模型，旨在合成高质量和自然的歌唱语音。与依赖声码器作为最终阶段并常引入失真的现有方法不同，SmoothSinger在一个统一的框架中直接细化低质量合成音频，减轻了两阶段管道相关的降级。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来指导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕获音高轮廓和长期频谱依赖。为了改善训练期间的对齐，我们用降级的真实音频替换参考音频，解决了参考信号和目标信号之间的时间不匹配。在Opencpop数据集（一个大规模中文歌唱语料库）上的实验表明，SmoothSinger在客观和主观评估中均达到了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成语音自然度方面的有效性。", "summary": "SmoothSinger是一种新型的条件扩散模型，专为高质量歌唱语音合成（SVS）设计。该模型通过在一个统一框架中直接细化低质量音频，避免了传统两阶段方法的失真。它采用参考引导的双分支架构，并增强了U-Net以更好地捕捉音高和长期依赖。为解决训练中的对齐问题，它使用降级的真实音频作为参考。在中文Opencpop数据集上的实验证明，SmoothSinger在客观和主观评估中均达到了最先进的性能，显著减少了伪影并提高了合成语音的自然度。", "keywords": "歌唱语音合成, 条件扩散模型, SmoothSinger, 多分辨率架构, 音频合成", "comments": "SmoothSinger的创新之处在于其统一的框架，直接对低质量音频进行细化，避免了传统声码器引入的失真。其参考引导的双分支架构和增强的U-Net设计，特别是并行低频上采样路径，有助于更好地捕捉歌唱的复杂特性。此外，使用降级真实音频进行训练对齐，是解决实际问题的有效策略。该工作在SVS领域取得了显著进展，为未来的高质量语音合成提供了新思路。"}}
{"id": "2506.21030", "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner", "authors": ["Zhou Tianxing", "Wang Zhirui", "Ao Haojia", "Chen Guangyan", "Xing Boyang", "Cheng Jingwen", "Yang Yi", "Yue Yufeng"], "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21030v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21030v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "STEP Planner：构建跨层级子目标树作为具身长周期任务规划器", "tldr": "STEP Planner通过构建跨层级子目标树，解决了LLM在具身长周期任务规划中成功率低的问题，显著提高了机器人任务完成率。", "motivation": "现有大型语言模型（LLM）直接作为动作序列生成器在具身长周期任务中表现出成功率低的问题，这主要是由于它们对这类任务的推理能力有限。", "method": "本文提出了STEP框架，通过一对闭环模型构建一个跨层级的子目标树。这对模型包括：1) 子目标分解模型，它利用基础LLM将复杂目标分解为可管理的子目标，从而扩展子目标树；2) 叶节点终止模型，它根据环境状态提供实时反馈，确定何时终止树的扩展，并确保每个叶节点可以直接转换为原始动作。", "result": "在VirtualHome WAH-NL基准测试和真实机器人上的实验表明，STEP在长周期具身任务完成方面取得了显著的成功率，其中WAH-NL基准测试高达34%，真实机器人实验高达25%。这些结果优于现有SOTA方法。", "conclusion": "STEP框架通过其独特的跨层级子目标树构建方法，有效解决了大型语言模型在具身长周期任务规划中的局限性，显著提升了机器人在复杂任务中的规划和执行成功率。", "translation": "可靠地执行长周期任务规划的能力对于在现实世界环境中部署机器人至关重要。然而，直接使用大型语言模型（LLM）作为动作序列生成器通常会导致成功率较低，因为它们对长周期具身任务的推理能力有限。在STEP框架中，我们通过一对闭环模型构建一个子目标树：一个子目标分解模型和一个叶节点终止模型。在这个框架内，我们开发了一个从粗到细分辨率的层级树结构。子目标分解模型利用基础LLM将复杂目标分解为可管理的子目标，从而扩展子目标树。叶节点终止模型根据环境状态提供实时反馈，确定何时终止树的扩展，并确保每个叶节点可以直接转换为原始动作。在VirtualHome WAH-NL基准测试和真实机器人上进行的实验表明，STEP在长周期具身任务完成方面取得了高达34%（WAH-NL）和25%（真实机器人）的成功率，优于现有SOTA方法。", "summary": "本文提出了STEP Planner，一个用于具身长周期任务规划的框架。针对LLM在长周期任务中推理能力有限导致成功率低的问题，STEP通过构建一个跨层级的子目标树来解决。该框架包含子目标分解模型（利用LLM分解目标）和叶节点终止模型（提供实时反馈并确保可执行的原始动作）。实验结果表明，STEP在虚拟和真实环境中均显著提升了长周期具身任务的完成成功率，优于现有最佳方法。", "keywords": "具身任务规划, 子目标树, 大型语言模型, 长周期任务, 机器人", "comments": "STEP Planner的创新之处在于其双闭环模型构建的跨层级子目标树结构，有效弥补了LLM在长周期具身任务规划中推理能力的不足。这种将LLM的宏观规划能力与实时环境反馈相结合的方法，为机器人复杂任务规划提供了一个有效且可扩展的解决方案，对于提升机器人在真实世界中的自主性具有重要意义。"}}
{"id": "2506.21211", "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models", "authors": ["Quanming Liu", "Xupeng Bu", "Zhichao Yan", "Ru Li"], "summary": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21211v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21211v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "$T^3$: 基于多级树的大语言模型自动程序修复", "tldr": "提出$T^3$框架，结合大语言模型和树搜索，提升自动程序修复的精度和效率。", "motivation": "尽管大语言模型（LLMs）和思维链（CoT）技术显著增强了模型的推理能力，但由于自动程序修复（APR）任务逻辑复杂且需要多步推理，CoT技术在APR领域的应用仍显不足。", "method": "本研究系统评估了几种常见思维链（CoT）技术在自动程序修复（APR）任务中的表现，并提出了一个名为$T^3$的创新框架。该框架将大语言模型的强大推理能力与树搜索相结合。", "result": "$T^3$框架有效提高了生成候选修复方案的精度。此外，$T^3$为优化APR任务中的样本选择和修复策略提供了有价值的指导。", "conclusion": "$T^3$建立了一个实现高效自动化调试的稳健框架。", "translation": "自动程序修复（APR）是软件开发和维护中的一项核心技术，旨在以最少的人工干预实现自动化缺陷修复。近年来，大型语言模型（LLMs）和思维链（CoT）技术的巨大进步显著增强了这些模型的推理能力。然而，由于复杂的逻辑和所需的多步推理能力，CoT技术在APR领域的应用仍然不足。本研究系统评估了几种常见CoT技术在APR任务中的性能，并提出了一个创新的框架$T^3$，它将LLMs强大的推理能力与树搜索相结合，有效提高了生成候选修复方案的精度。此外，$T^3$为优化APR任务中的样本选择和修复策略提供了有价值的指导，为实现高效自动化调试建立了一个稳健的框架。", "summary": "本文针对自动程序修复（APR）领域中大语言模型（LLMs）和思维链（CoT）技术应用不足的问题，提出了一个名为$T^3$的创新框架。该框架通过结合LLMs的推理能力和树搜索，显著提升了生成候选修复方案的精度。$T^3$还为APR任务中的样本选择和修复策略提供了优化指导，从而构建了一个高效自动调试的稳健框架。", "keywords": "自动程序修复, 大语言模型, 思维链, 树搜索, 自动化调试", "comments": "这项研究通过将大语言模型的先进推理能力与树搜索方法相结合，创新性地解决了自动程序修复中复杂逻辑和多步推理的挑战。$T^3$框架不仅提升了修复方案的生成精度，还为优化修复策略提供了新思路，对自动化软件调试领域具有重要意义。"}}
{"id": "2506.21333", "title": "A Systematic Review of Human-AI Co-Creativity", "authors": ["Saloni Singh", "Koen Hndriks", "Drik Heylen", "Kim Baraka"], "summary": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21333v1", "categories": ["cs.HC", "cs.AI", "I.2.11"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21333v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人机协同创造的系统综述", "tldr": "本文对62篇关于人机协同创造系统的论文进行了系统综述，识别了系统设计维度和24个设计考量。研究发现，高用户控制度能提高满意度和信任，适应性强的主动系统能增强协作，但也存在早期创造阶段支持不足等挑战。", "motivation": "为了支持人机协同创造领域的发展，并为未来系统提供有价值的设计基础，本文旨在通过系统综述来识别现有协同创造系统的设计考虑因素。", "method": "本文对62篇关于协同创造系统的论文进行了系统文献综述。这些论文涵盖了视觉艺术、设计和写作等多种应用领域，其中AI不仅是工具，更是创造过程中的积极合作者。", "result": "本文识别了几个与系统设计相关的关键维度：创造过程的阶段、创造性任务、系统的主动行为、用户控制、系统具身化和AI模型类型。研究发现，提供高用户控制度的系统能带来更高的满意度、信任和对创造成果更强的拥有感。此外，当主动系统具有适应性和情境敏感性时，可以增强协作。本文还提取了24个设计考量，强调了鼓励用户外部化思考以及增加系统社会存在感和透明度以培养信任的价值。", "conclusion": "尽管最近取得了进展，但仍存在重要空白，例如对问题澄清等早期创造阶段的支持有限，以及用户适应AI系统相关的挑战。高用户控制度和适应性强的主动系统对人机协同创造至关重要，但未来研究需解决现有局限。", "translation": "协同创造社区在开发更复杂、更定制化的系统以支持和增强人类创造力方面取得了显著进展。先前工作的设计考虑可以作为未来系统宝贵而高效的基础。为了支持这项工作，我们对62篇关于协同创造系统的论文进行了系统文献综述。这些论文涵盖了广泛的应用领域，包括视觉艺术、设计和写作，其中人工智能不仅仅是工具，更是创造过程中的积极合作者。通过这次综述，我们识别了几个与系统设计相关的关键维度：创造过程的阶段、创造性任务、系统的主动行为、用户控制、系统具身化和人工智能模型类型。我们的研究结果表明，提供高用户控制度的系统能带来更高的满意度、信任和对创造成果更强的拥有感。此外，当主动系统具有适应性和情境敏感性时，可以增强协作。我们还提取了24个设计考虑，强调了鼓励用户外部化思考以及增加系统社会存在感和透明度以培养信任的价值。尽管最近取得了进展，但仍存在重要空白，例如对问题澄清等早期创造阶段的支持有限，以及用户适应人工智能系统相关的挑战。", "summary": "本文对62篇人机协同创造系统论文进行了系统综述，旨在为未来系统设计提供基础。研究识别了创造过程阶段、任务、系统主动性、用户控制、具身化和AI模型类型等关键设计维度。结果表明，高用户控制度能提升满意度和信任，而适应性强的主动系统能增强协作。论文还提取了24个设计考量，并指出了早期创造阶段支持不足和用户适应挑战等现有差距。", "keywords": "人机协同创造, 系统综述, 设计考量, 用户控制, 人工智能协作", "comments": "该论文通过系统综述对人机协同创造领域进行了全面梳理，为未来系统设计提供了宝贵的理论基础和实践指导。其创新之处在于系统性地提炼了关键设计维度和具体的24个设计考量，特别是强调了用户控制和主动性在提升协作体验中的作用。然而，论文也指出了当前研究在早期创造阶段支持和用户适应性方面的局限性，为后续研究指明了方向。其重要性在于为该领域的研究者和开发者提供了一个结构化的知识框架，有助于推动人机协同创造系统的发展。"}}
{"id": "2506.21309", "title": "Linear codes arising from the point-hyperplane geometry-Part I: the Segre embedding", "authors": ["Ilaria Cardinali", "Luca Giuzzi"], "summary": "Let $V$ be a vector space over the finite field $\\mathbb{F}_q$ with $q$\nelements and $\\Lambda$ be the image of the Segre geometry\n$\\mathrm{PG}(V)\\otimes\\mathrm{PG}(V^*)$ in $\\mathrm{PG}(V\\otimes V^*)$.\nConsider the subvariety $\\Lambda_{1}$ of $\\Lambda$ represented by the pure\ntensors $x\\otimes \\xi$ with $x\\in V$ and $\\xi\\in V^*$ such that $\\xi(x)=0$.\nRegarding $\\Lambda_1$ as a projective system of $\\mathrm{PG}(V\\otimes V^*)$, we\nstudy the linear code $\\mathcal{C}(\\Lambda_1)$ arising from it. The code\n$\\mathcal{C}(\\Lambda_1)$ is minimal code and we determine its basic parameters,\nitsfull weight list and its linear automorphism group. We also give a\ngeometrical characterization of its minimum and second lowest weight codewords\nas well as of some of the words of maximum weight.", "comment": "29 pages", "pdf_url": "http://arxiv.org/pdf/2506.21309v1", "categories": ["math.CO", "cs.DM", "cs.IT", "math.IT", "51E22, 94B05, 14M12"], "cate": "math.CO", "url": "http://arxiv.org/abs/2506.21309v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "来源于点-超平面几何的线性码——第一部分：Segre嵌入", "tldr": "本文研究了从Segre几何的特定子簇$\\Lambda_1$（由满足$\\xi(x)=0$的纯张量构成）生成的线性码$\\mathcal{C}(\\Lambda_1)$。作者确定了该码的基本参数、权谱、自同构群，并对其特殊码字进行了几何刻画。", "motivation": "论文的动机是研究从特定的几何结构（Segre嵌入中的子簇$\\Lambda_1$）产生的线性码，以理解其性质和参数。", "method": "研究方法是将Segre几何中的特定子簇$\\Lambda_1$视为射影系统，并从中构造线性码$\\mathcal{C}(\\Lambda_1)$。然后通过代数和几何方法分析该码的性质，包括确定其参数、权谱、自同构群，并几何刻画其特定码字。", "result": "线性码$\\mathcal{C}(\\Lambda_1)$被证明是极小码。确定了$\\mathcal{C}(\\Lambda_1)$的基本参数。确定了$\\mathcal{C}(\\Lambda_1)$的完整权谱。确定了$\\mathcal{C}(\\Lambda_1)$的线性自同构群。对最小权重、第二低权重以及部分最大权重码字进行了几何刻画。", "conclusion": "通过对从Segre嵌入中特定子簇$\\Lambda_1$导出的线性码$\\mathcal{C}(\\Lambda_1)$的深入研究，本文成功地确定了其关键参数、权谱和自同构群，并提供了其特殊码字的几何刻画，表明这种几何构造可以产生具有明确数学性质的极小码。", "translation": "设$V$是有限域$\\mathbb{F}_q$上的一个向量空间，其元素个数为$q$，$\\Lambda$是Segre几何$\\mathrm{PG}(V)\\otimes\\mathrm{PG}(V^*)$在$\\mathrm{PG}(V\\otimes V^*)$中的像。考虑$\\Lambda$的一个子簇$\\Lambda_{1}$，它由满足$x\\in V$和$\\xi\\in V^*$且$\\xi(x)=0$的纯张量$x\\otimes \\xi$表示。将$\\Lambda_1$视为$\\mathrm{PG}(V\\otimes V^*)$的一个射影系统，我们研究由此产生的线性码$\\mathcal{C}(\\Lambda_1)$。码$\\mathcal{C}(\\Lambda_1)$是极小码，我们确定了它的基本参数、完整的权谱和线性自同构群。我们还对其最小权重、第二低权重码字以及部分最大权重码字进行了几何刻画。", "summary": "本文从有限域上的向量空间$V$出发，研究了由Segre几何$\\mathrm{PG}(V)\\otimes\\mathrm{PG}(V^*)$在$\\mathrm{PG}(V\\otimes V^*)$中的像$\\Lambda$的特定子簇$\\Lambda_1$（由满足$\\xi(x)=0$的纯张量构成）导出的线性码$\\mathcal{C}(\\Lambda_1)$。研究结果表明，该码是极小码，并且论文详细确定了其基本参数、完整的权谱和线性自同构群。此外，文章还对该码的最小、第二低以及部分最大权重码字进行了几何刻画。", "keywords": "线性码, Segre嵌入, 极小码, 权谱, 自同构群", "comments": "这篇论文深入探讨了代数几何与编码理论的交叉领域，通过从Segre嵌入的特定子簇构造线性码，并对其进行了全面的代数和几何分析。其创新之处在于将特定的几何结构与线性码的性质（如极小性、权谱和自同构群）联系起来，并提供了码字在几何上的解释。这对于理解码的结构和设计具有重要意义。"}}
{"id": "2506.20850", "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision", "authors": ["Yuting He", "Shuo Li"], "summary": "Contrastive learning (CL) has become a cornerstone of self-supervised\npretraining (SSP) in foundation models, however, extending CL to pixel-wise\nrepresentation, crucial for medical vision, remains an open problem. Standard\nCL formulates SSP as a binary optimization problem (binary CL) where the\nexcessive pursuit of feature dispersion leads to an over-dispersion problem,\nbreaking pixel-wise feature correlation thus disrupting the intra-class\ndistribution. Our vector CL reformulates CL as a vector regression problem,\nenabling dispersion quantification in pixel-wise pretraining via modeling\nfeature distances in regressing displacement vectors. To implement this novel\nparadigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER\nestablishes an extendable vector-based self-learning, enforces a consistent\noptimization flow from vector regression to distance modeling, and leverages a\nvector pyramid architecture for granularity adaptation, thus preserving\npixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,\nspanning 2 dimensions and 4 modalities, show that COVER significantly improves\npixel-wise SSP, advancing generalizable medical visual foundation models.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20850v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20850v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "用于医学图像像素级预训练的向量对比学习", "tldr": "本文提出了向量对比学习（Vector CL）和COVER框架，通过将对比学习重构为向量回归问题，解决了标准对比学习在像素级预训练中导致的特征过分散问题，显著提升了医学图像的像素级自监督预训练效果。", "motivation": "将对比学习（CL）扩展到对医学图像至关重要的像素级表征仍然是一个开放问题。标准的对比学习（二元CL）在追求特征分散时会导致过分散问题，破坏像素级特征相关性，从而扰乱类内分布。", "method": "本文将对比学习重新表述为一个向量回归问题，通过建模特征距离来回归位移向量，从而实现像素级预训练中的分散量化。为实现这一范式，提出了COntrast in VEctor Regression (COVER) 框架。COVER建立了一种可扩展的基于向量的自学习方法，强制执行从向量回归到距离建模的一致优化流程，并利用向量金字塔架构进行粒度适应，从而在自监督预训练（SSP）中保留像素级特征相关性。", "result": "在跨越2个维度和4种模态的8项任务上的大量实验表明，COVER显著改善了像素级自监督预训练（SSP），推动了可泛化的医学视觉基础模型的发展。", "conclusion": "本文提出的向量对比学习和COVER框架有效解决了标准对比学习在像素级预训练中的过分散问题，显著提升了医学图像自监督预训练的性能，有助于构建更通用的医学视觉基础模型。", "translation": "对比学习（CL）已成为基础模型中自监督预训练（SSP）的基石，然而，将CL扩展到像素级表示（这对于医学视觉至关重要）仍然是一个开放问题。标准CL将SSP表述为二元优化问题（二元CL），其中过度追求特征分散导致过分散问题，破坏了像素级特征相关性，从而扰乱了类内分布。我们的向量CL将CL重新表述为向量回归问题，通过在回归位移向量中建模特征距离来量化像素级预训练中的分散。为了实现这一新范式，我们提出了向量回归中的对比（COVER）框架。COVER建立了一种可扩展的基于向量的自学习方法，强制执行从向量回归到距离建模的一致优化流程，并利用向量金字塔架构进行粒度适应，从而在SSP中保留像素级特征相关性。在跨越2个维度和4种模态的8项任务上的大量实验表明，COVER显著改善了像素级SSP，推动了可泛化的医学视觉基础模型的发展。", "summary": "本研究提出了一种新颖的向量对比学习（Vector CL）方法，旨在解决标准对比学习在医学图像像素级自监督预训练（SSP）中导致的特征过分散问题。通过将对比学习重构为向量回归问题，并引入COntrast in VEctor Regression (COVER) 框架，该方法能够量化特征分散并保留像素级特征相关性。实验结果表明，COVER在多项医学图像任务上显著提升了像素级SSP的效果，为构建更通用的医学视觉基础模型奠定了基础。", "keywords": "向量对比学习, 像素级预训练, 医学图像, 自监督学习, COVER框架", "comments": "本文的创新点在于将传统的二元对比学习重新定义为向量回归问题，从而有效解决了像素级预训练中特征过分散的挑战。COVER框架通过引入向量金字塔结构，实现了多粒度适应，进一步增强了模型在医学图像复杂性上的表现。这项工作对于推动医学视觉领域的基础模型发展具有重要意义。"}}
{"id": "2506.21327", "title": "Enabling Bitcoin Smart Contracts on the Internet Computer", "authors": ["Ryan Croote", "Islam El-Ashi", "Thomas Locher", "Yvonne-Anne Pignolet"], "summary": "There is growing interest in providing programmatic access to the value\nlocked in Bitcoin, which famously offers limited programmability itself.\nVarious approaches have been put forth in recent years, with the vast majority\nof proposed mechanisms either building new functionality on top of Bitcoin or\nleveraging a bridging mechanism to enable smart contracts that make use of\n``wrapped'' bitcoins on entirely different platforms.\n  In this work, an architecture is presented that follows a different approach.\nThe architecture enables the execution of Turing-complete Bitcoin smart\ncontracts on the Internet Computer (IC), a blockchain platform for hosting and\nexecuting decentralized applications. Instead of using a bridge, IC and Bitcoin\nnodes interact directly, eliminating potential security risks that the use of a\nbridge entails. This integration requires novel concepts, in particular to\nreconcile the probabilistic nature of Bitcoin with the irreversibility of\nfinalized state changes on the IC, which may be of independent interest.\n  In addition to the presentation of the architecture, we provide evaluation\nresults based on measurements of the Bitcoin integration running on mainnet.\nThe evaluation results demonstrate that, with finalization in a few seconds and\nlow execution costs, this integration enables complex Bitcoin-based\ndecentralized applications that were not practically feasible or economically\nviable before.", "comment": "Published at ICDCS 2025, waiting for DOI", "pdf_url": "http://arxiv.org/pdf/2506.21327v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21327v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在互联网计算机上启用比特币智能合约", "tldr": "该论文提出了一种在互联网计算机（IC）上直接启用图灵完备比特币智能合约的架构，通过节点直接交互而非桥接，实现了几秒内的最终确认和低执行成本，从而使复杂的比特币去中心化应用成为可能。", "motivation": "人们对以编程方式访问比特币中锁定的价值越来越感兴趣，但比特币本身的编程能力有限。现有方法大多在比特币之上构建新功能或利用桥接机制，这可能带来安全风险。", "method": "本文提出了一种架构，使图灵完备的比特币智能合约能够在互联网计算机（IC）上执行。该方法不使用桥接，而是让IC和比特币节点直接交互，从而消除了桥接可能带来的安全风险。这种集成需要新颖的概念，特别是要协调比特币的概率性与IC上最终状态更改的不可逆性。", "result": "基于主网运行的比特币集成测量结果表明，该集成可在几秒内完成最终确认，且执行成本低。这使得以前不切实际或不经济的复杂比特币去中心化应用成为可能。", "conclusion": "通过直接集成比特币与互联网计算机，本文提出的架构提供了一种安全、快速且经济高效的方式来启用复杂的比特币智能合约，克服了传统方法的局限性。", "translation": "人们对以编程方式访问比特币中锁定的价值越来越感兴趣，比特币本身以编程能力有限而闻名。近年来，各种方法被提出，其中绝大多数提议的机制要么在比特币之上构建新功能，要么利用桥接机制在完全不同的平台上启用使用“封装”比特币的智能合约。\n在本文中，提出了一种采用不同方法的架构。该架构支持在互联网计算机（IC）上执行图灵完备的比特币智能合约，IC是一个用于托管和执行去中心化应用程序的区块链平台。IC和比特币节点直接交互，而不是使用桥接，从而消除了使用桥接可能带来的潜在安全风险。这种集成需要新颖的概念，特别是要协调比特币的概率性与IC上最终状态更改的不可逆性，这可能具有独立的意义。\n除了架构的介绍，我们还提供了基于主网运行的比特币集成测量结果的评估。评估结果表明，通过几秒内的最终确认和低执行成本，这种集成使得以前不切实际或不经济的复杂比特币去中心化应用成为可能。", "summary": "本文提出了一种新颖的架构，通过互联网计算机（IC）与比特币节点直接交互，而非传统的桥接机制，从而在IC上实现了图灵完备的比特币智能合约。这种方法规避了桥接带来的潜在安全风险，并引入了创新概念以协调比特币的概率特性与IC状态更改的最终性。主网评估结果表明，该集成可在数秒内完成交易最终确认，且执行成本低廉，极大地提升了复杂比特币去中心化应用的可行性和经济性。", "keywords": "比特币, 智能合约, 互联网计算机, 区块链集成, 去中心化应用", "comments": "该论文的创新之处在于其提出的直接集成方法，避免了传统桥接机制带来的安全风险。特别值得注意的是，它解决了如何协调比特币的概率性与互联网计算机状态更改的不可逆性这一复杂技术挑战，这对于构建可靠的跨链应用至关重要。这项工作有望显著扩展比特币的实用性，使其能够支持更复杂的去中心化应用。"}}
{"id": "2506.21186", "title": "Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout", "authors": ["Apurva Shah", "Axel Abels", "Ann Nowé", "Tom Lenaerts"], "summary": "Perpetual voting addresses fairness in sequential collective decision-making\nby evaluating representational equity over time. However, existing perpetual\nvoting rules rely on full participation and complete approval information,\nassumptions that rarely hold in practice, where partial turnout is the norm. In\nthis work, we study the integration of Artificial Delegates,\npreference-learning agents trained to represent absent voters, into perpetual\nvoting systems. We examine how absenteeism affects fairness and\nrepresentativeness under various voting methods and evaluate the extent to\nwhich Artificial Delegates can compensate for missing participation. Our\nfindings indicate that while absenteeism significantly affects fairness,\nArtificial Delegates reliably mitigate these effects and enhance robustness\nacross diverse scenarios.", "comment": "The paper has been accepted at the ACM Collective Intelligence\n  Conference (CI 2025), August 4 to 6, 2025, San Diego, CA, USA", "pdf_url": "http://arxiv.org/pdf/2506.21186v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21186v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人工智能代表解决部分投票率下永久投票中的公平性问题", "tldr": "本文研究了在永久投票系统中引入人工智能代表（偏好学习代理）来解决因部分投票率导致的公平性问题。研究发现，人工智能代表能有效缓解缺席对公平性的影响并增强系统鲁棒性。", "motivation": "现有的永久投票规则依赖于完全参与和完整的赞成信息，这在实际中很少实现，因为部分投票率是常态。这导致在顺序集体决策中，公平性和代表性受到影响。", "method": "本文研究了将人工智能代表（经过训练代表缺席选民的偏好学习代理）整合到永久投票系统中。我们考察了缺席对不同投票方法下公平性和代表性的影响，并评估了人工智能代表在多大程度上可以弥补缺失的参与。", "result": "我们的研究结果表明，虽然缺席显著影响公平性，但人工智能代表能够可靠地缓解这些影响，并在不同场景下增强系统的鲁棒性。", "conclusion": "人工智能代表可以有效解决永久投票中因部分投票率导致的公平性问题，提高系统的鲁棒性。", "translation": "永久投票通过评估随时间变化的代表性公平来解决顺序集体决策中的公平性问题。然而，现有的永久投票规则依赖于完全参与和完整的赞成信息，这些假设在实践中很少成立，因为部分投票率是常态。在这项工作中，我们研究了将人工智能代表（经过训练代表缺席选民的偏好学习代理）整合到永久投票系统中。我们考察了缺席对不同投票方法下公平性和代表性的影响，并评估了人工智能代表在多大程度上可以弥补缺失的参与。我们的研究结果表明，虽然缺席显著影响公平性，但人工智能代表能够可靠地缓解这些影响，并在不同场景下增强系统的鲁棒性。", "summary": "本文探讨了在永久投票系统中引入人工智能代表来解决部分投票率导致的公平性问题。研究发现，人工智能代表（作为偏好学习代理）能够有效弥补选民缺席对公平性和代表性的负面影响，显著提升系统在多种场景下的鲁棒性。", "keywords": "永久投票, 公平性, 部分投票率, 人工智能代表, 偏好学习", "comments": "该论文提出了一种新颖的方法，通过引入人工智能代表来解决永久投票中实际存在的投票率不足问题，这对于提高在线决策系统或长期投票机制的公平性和实用性具有重要意义。其创新之处在于将机器学习代理与投票理论相结合，以弥补现实世界的局限性。"}}
{"id": "2506.20921", "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach", "authors": ["Tong Zeng", "Srivathsan Badrinarayanan", "Janghoon Ock", "Cheng-Kai Lai", "Amir Barati Farimani"], "summary": "Chemical process optimization is crucial to maximize production efficiency\nand economic performance. Traditional methods, including gradient-based\nsolvers, evolutionary algorithms, and parameter grid searches, become\nimpractical when operating constraints are ill-defined or unavailable,\nrequiring engineers to rely on subjective heuristics to estimate feasible\nparameter ranges. To address this constraint definition bottleneck, we present\na multi-agent framework of large language model (LLM) agents that autonomously\ninfer operating constraints from minimal process descriptions, then\ncollaboratively guide optimization using the inferred constraints. Our\nAutoGen-based agentic framework employs OpenAI's o3 model, with specialized\nagents for constraint generation, parameter validation, simulation execution,\nand optimization guidance. Through two phases - autonomous constraint\ngeneration using embedded domain knowledge, followed by iterative multi-agent\noptimization - the framework eliminates the need for predefined operational\nbounds. Validated on the hydrodealkylation process across cost, yield, and\nyield-to-cost ratio metrics, the framework demonstrated competitive performance\nwith conventional optimization methods while achieving better computational\nefficiency, requiring fewer iterations to converge. Our approach converged in\nunder 20 minutes, achieving a 31-fold speedup over grid search. Beyond\ncomputational efficiency, the framework's reasoning-guided search demonstrates\nsophisticated process understanding, correctly identifying utility trade-offs,\nand applying domain-informed heuristics. This approach shows significant\npotential for optimization scenarios where operational constraints are poorly\ncharacterized or unavailable, particularly for emerging processes and retrofit\napplications.", "comment": "16 pages (main manuscript without references), 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20921v1", "categories": ["cs.LG", "cs.AI", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20921v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LLM引导的多智能体方法在化工过程优化中的应用", "tldr": "本文提出一个基于LLM多智能体框架，解决化工过程优化中操作约束定义不清的问题，通过自主推理约束并协同优化，实现了更高的计算效率和更好的过程理解。", "motivation": "传统的化工过程优化方法在操作约束不明确或不可用时变得不切实际，导致工程师需要依赖主观启发式方法来估计参数范围。这形成了一个“约束定义瓶颈”，需要一种新方法来解决。", "method": "本文提出了一个基于AutoGen的LLM多智能体框架，利用OpenAI的o3模型。该框架分为两个阶段：首先，通过嵌入的领域知识自主生成操作约束；然后，利用推断出的约束进行迭代的多智能体协同优化。框架中包含专门的智能体负责约束生成、参数验证、模拟执行和优化指导，从而消除了对预定义操作边界的需求。", "result": "在加氢脱烷基过程的成本、产率和产率成本比指标上进行了验证，该框架与传统优化方法相比展现出有竞争力的性能，同时实现了更好的计算效率，收敛所需的迭代次数更少。该方法在20分钟内收敛，比网格搜索加速了31倍。此外，框架的推理引导搜索展示了对过程的复杂理解，能够正确识别效用权衡并应用领域启发式方法。", "conclusion": "该框架在操作约束不明确或不可用的优化场景中，特别是对于新兴过程和改造应用，具有显著潜力。", "translation": "化工过程优化对于最大化生产效率和经济性能至关重要。传统的优化方法，包括基于梯度的求解器、进化算法和参数网格搜索，在操作约束定义不明确或不可用时变得不切实际，需要工程师依赖主观启发式方法来估计可行的参数范围。为了解决这个约束定义瓶颈，我们提出了一个大型语言模型（LLM）智能体的多智能体框架，该框架能够从最少的工艺描述中自主推断操作约束，然后协同引导优化。我们基于AutoGen的智能体框架采用了OpenAI的o3模型，其中包含专门的智能体用于约束生成、参数验证、模拟执行和优化指导。通过两个阶段——利用嵌入式领域知识自主生成约束，随后进行迭代的多智能体优化——该框架消除了对预定义操作边界的需求。在加氢脱烷基过程的成本、产率和产率成本比指标上进行验证，该框架与传统优化方法相比展现出有竞争力的性能，同时实现了更好的计算效率，收敛所需的迭代次数更少。我们的方法在20分钟内收敛，比网格搜索加速了31倍。除了计算效率，该框架的推理引导搜索展示了复杂的工艺理解，能够正确识别效用权衡并应用领域启发式方法。这种方法在操作约束特征不明确或不可用的优化场景中，特别是对于新兴工艺和改造应用，显示出巨大的潜力。", "summary": "本文提出一种新颖的LLM多智能体框架，旨在解决化工过程优化中操作约束难以定义或缺失的问题。该框架通过LLM智能体自主推断约束，并协同指导优化。该方法在加氢脱烷基过程验证中，不仅达到了与传统方法相当的性能，还在计算效率上取得了显著提升，比网格搜索快31倍，并展现出对复杂过程的深刻理解，对缺乏明确约束的优化场景具有重要应用前景。", "keywords": "化工过程优化, 大型语言模型, 多智能体系统, 约束生成, 过程理解", "comments": "这篇论文通过引入LLM多智能体框架来解决化工过程优化中长期存在的“约束定义瓶颈”，具有重要的创新性。它将LLM的推理能力与传统优化相结合，实现了自主约束生成和高效优化，显著提升了计算效率并展现了对复杂过程的理解。这种方法对于新兴工艺和改造应用尤其有价值，因为它能够处理数据不完整或约束不明确的情况，有望降低工程师的主观依赖性，加速新工艺的开发和优化。"}}
{"id": "2506.20746", "title": "Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers", "authors": ["Todd Nief", "David Reber", "Sean Richardson", "Ari Holtzman"], "summary": "When an LLM learns a relation during finetuning (e.g., new movie releases,\ncorporate mergers, etc.), where does this information go? Is it extracted when\nthe model processes an entity, recalled just-in-time before a prediction, or\nare there multiple separate heuristics? Existing localization approaches (e.g.\nactivation patching) are ill-suited for this analysis because they tend to\nreplace parts of the residual stream, potentially deleting information. To fill\nthis gap, we propose dynamic weight-grafting between fine-tuned and pre-trained\nlanguage models to show that fine-tuned language models both (1) extract\nrelation information learned during finetuning while processing entities and\n(2) ``recall\" this information in later layers while generating predictions. In\nsome cases, models need both of these pathways to correctly generate finetuned\ninformation while, in other cases, a single ``enrichment\" or ``recall\" pathway\nalone is sufficient. We examine the necessity and sufficiency of these\ninformation pathways, examining what layers they occur at, how much redundancy\nthey exhibit, and which model components are involved -- finding that the\n``recall\" pathway occurs via both task-specific attention mechanisms and a\nrelation extraction step in the output of the attention and the feedforward\nnetworks at the final layers before next token prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20746v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20746v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "关系抽取的多个流：Transformer中的丰富与回忆", "tldr": "研究发现，微调后的语言模型在处理实体时会提取关系信息，并在后续层中“回忆”这些信息以进行预测，这两种路径可能单独或共同作用。", "motivation": "现有定位方法（如激活修补）不适用于分析LLM在微调期间学习的关系信息去向，因为它们可能删除信息。本文旨在填补这一空白，探究关系信息在模型中的存储和利用方式。", "method": "提出在微调模型和预训练模型之间进行动态权重嫁接（dynamic weight-grafting），以分析微调后的语言模型如何提取和回忆关系信息。研究了这些信息路径的必要性、充分性、发生层、冗余度以及涉及的模型组件。", "result": "研究发现，微调后的语言模型在处理实体时会提取在微调期间学习到的关系信息，并在后续层中“回忆”这些信息以生成预测。在某些情况下，模型需要这两种路径才能正确生成微调信息，而在其他情况下，单个“丰富”或“回忆”路径就足够了。结果表明，“回忆”路径通过任务特定的注意力机制以及在注意力输出和前馈网络输出中（在下一个token预测前的最终层）的关系抽取步骤发生。", "conclusion": "LLM在微调过程中学习到的关系信息通过“提取”和“回忆”两种信息流在模型中存在和被利用。这两种信息流的组合方式因情况而异，且“回忆”路径涉及注意力机制和最终层的网络输出。", "translation": "当LLM在微调期间学习到某种关系（例如，新电影上映、公司合并等）时，这些信息去了哪里？是在模型处理实体时被提取，还是在预测前及时被回忆，亦或是存在多个独立启发式方法？现有的定位方法（例如激活修补）不适合这种分析，因为它们倾向于替换残差流的部分，可能删除信息。为了填补这一空白，我们提出了在微调语言模型和预训练语言模型之间进行动态权重嫁接，以表明微调语言模型（1）在处理实体时提取微调期间学习到的关系信息，并且（2）在生成预测时在后续层中“回忆”这些信息。在某些情况下，模型需要这两种路径才能正确生成微调信息，而在其他情况下，单个“丰富”或“回忆”路径就足够了。我们检查了这些信息路径的必要性和充分性，研究了它们发生的层、它们表现出的冗余度以及涉及的模型组件——发现“回忆”路径通过任务特定的注意力机制以及在下一个token预测前的最终层中注意力输出和前馈网络的输出中的关系抽取步骤发生。", "summary": "本文探讨了大型语言模型（LLMs）在微调过程中学习到的关系信息如何在模型内部存储和利用。通过引入动态权重嫁接方法，研究发现微调后的LLMs在处理实体时会提取关系信息，并在后续层中“回忆”这些信息以进行预测。研究表明，这两种“丰富”和“回忆”路径可能单独或共同作用，并且“回忆”路径涉及特定的注意力机制和最终层的网络输出。", "keywords": "关系抽取, Transformer, 语言模型, 微调, 信息流", "comments": "本文通过提出动态权重嫁接这一创新方法，有效地解决了现有定位方法无法深入分析LLM内部信息流的局限性。其发现的“提取”和“回忆”双路径机制，对于理解Transformer模型如何存储和利用知识具有重要意义，揭示了模型内部信息处理的复杂性和冗余性，对未来的模型解释性和设计优化具有指导价值。"}}
{"id": "2506.21510", "title": "Joint Scheduling of DER under Demand Charges: Structure and Approximation", "authors": ["Ruixiao Yang", "Gulai Shen", "Ahmed S. Alahmed", "Chuchu Fan"], "summary": "We study the joint scheduling of behind-the-meter distributed energy\nresources (DERs), including flexible loads, renewable generation, and battery\nenergy storage systems, under net energy metering frameworks with demand\ncharges. The problem is formulated as a stochastic dynamic program aimed at\nmaximizing expected operational surplus while accounting for renewable\ngeneration uncertainty. We analytically characterize the structure of the\noptimal control policy and show that it admits a threshold-based form. However,\ndue to the strong temporal coupling of the storage and demand charge\nconstraints, the number of conditional branches in the policy scales\ncombinatorially with the scheduling horizon, as it requires a look-ahead over\nfuture states. To overcome the high computational complexity in the general\nformulation, an efficient approximation algorithm is proposed, which searches\nfor the peak demand under a mildly relaxed problem. We show that the algorithm\nscales linearly with the scheduling horizon. Extensive simulations using two\nopen-source datasets validate the proposed algorithm and compare its\nperformance against different DER control strategies, including a reinforcement\nlearning-based one. Under varying storage and tariff parameters, the results\nshow that the proposed algorithm outperforms various benchmarks in achieving a\nrelatively small solution gap compared to the theoretical upper bound.", "comment": "15 pages, 4 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21510v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.21510v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "需求费用下分布式能源的联合调度：结构与近似", "tldr": "本文研究了在需求费用下，包括灵活负荷、可再生能源发电和电池储能系统在内的表后分布式能源的联合调度问题，并提出了一种高效的近似算法来解决其计算复杂性。", "motivation": "在净计量框架和需求费用下，研究表后分布式能源（包括灵活负荷、可再生能源发电和电池储能系统）的联合调度问题，旨在最大化预期运营盈余并考虑可再生能源发电的不确定性。", "method": "将问题表述为随机动态规划，旨在最大化预期运营盈余。分析表征了最优控制策略的结构，并表明其具有基于阈值的形式。为了克服一般公式中高计算复杂度问题，提出了一种高效的近似算法，通过在轻微放宽的问题下搜索峰值需求来解决。", "result": "由于储能和需求费用约束的强时间耦合，策略中的条件分支数量与调度周期呈组合式增长。提出的近似算法的计算复杂度与调度周期呈线性关系。通过使用两个开源数据集进行的广泛仿真验证了所提出的算法，并表明其在与理论上限相比实现相对较小的解决方案差距方面优于各种基准，包括基于强化学习的策略。", "conclusion": "本文提出的近似算法能够有效且高效地解决需求费用下的分布式能源联合调度问题，并在性能上优于现有基准。", "translation": "我们在净计量框架和需求费用下，研究了表后分布式能源（DERs）的联合调度问题，包括灵活负荷、可再生能源发电和电池储能系统。该问题被表述为一个随机动态规划，旨在最大化预期运营盈余，同时考虑可再生能源发电的不确定性。我们分析性地表征了最优控制策略的结构，并表明它具有基于阈值的形式。然而，由于储能和需求费用约束的强时间耦合，策略中的条件分支数量与调度周期呈组合式增长，因为它需要对未来状态进行展望。为了克服一般公式中的高计算复杂度问题，提出了一种高效的近似算法，该算法在轻微放宽的问题下搜索峰值需求。我们表明该算法的计算复杂度与调度周期呈线性关系。使用两个开源数据集进行的广泛仿真验证了所提出的算法，并将其性能与不同的DER控制策略（包括基于强化学习的策略）进行了比较。在不同的储能和费率参数下，结果表明，与理论上限相比，所提出的算法在实现相对较小的解决方案差距方面优于各种基准。", "summary": "本文探讨了在需求费用下分布式能源（DERs）的联合调度问题，包括灵活负荷、可再生能源和电池储能。该问题被建模为一个随机动态规划，旨在最大化预期运营盈余。研究揭示了最优控制策略的阈值形式，但由于强时间耦合导致计算复杂度高。为解决此问题，论文提出了一种高效的近似算法，该算法通过在放宽的问题中寻找峰值需求，实现了与调度周期线性相关的计算复杂度。仿真结果验证了该算法的有效性，并显示其性能优于多种现有控制策略。", "keywords": "分布式能源, 需求费用, 联合调度, 随机动态规划, 近似算法", "comments": "该研究创新性地将需求费用下的分布式能源联合调度问题表述为随机动态规划，并识别出最优策略的阈值结构。其主要贡献在于提出了一种高效的近似算法，有效解决了由于强时间耦合导致的高计算复杂度问题。该算法的线性扩展性及其在仿真中优于基准的性能，表明其在实际应用中具有重要价值。"}}
{"id": "2506.21112", "title": "Point Cloud Environment-Based Channel Knowledge Map Construction", "authors": ["Yancheng Wang", "Wei Guo", "Guanying Chen", "Ye Zhang", "Shuguang Cui"], "summary": "Channel knowledge map (CKM) provides certain levels of channel state\ninformation (CSI) for an area of interest, serving as a critical enabler for\nenvironment-aware communications by reducing the overhead of frequent CSI\nacquisition. However, existing CKM construction schemes adopt over-simplified\nenvironment information, which significantly compromises their accuracy. To\naddress this issue, this work proposes a joint model- and data-driven approach\nto construct CKM by leveraging point cloud environmental data along with a few\nsamples of location-tagged channel information. First, we propose a novel point\nselector to identify subsets of point cloud that contain environmental\ninformation relevant to multipath channel gains, by constructing a set of\nco-focal ellipsoids based on different time of arrival (ToAs). Then, we trained\na neural channel gain estimator to learn the mapping between each selected\nsubset and its corresponding channel gain, using a real-world dataset we\ncollected through field measurements, comprising environmental point clouds and\ncorresponding channel data. Finally, experimental results demonstrate that: For\nCKM construction of power delay profile (PDP), the proposed method achieves a\nroot mean squared error (RMSE) of 2.95 dB, significantly lower than the 7.32 dB\nachieved by the conventional ray-tracing method; for CKM construction of\nreceived power values, i.e., radio map, it achieves an RMSE of 1.04 dB,\nsurpassing the Kriging interpolation method with an RMSE of 1.68 dB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21112v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21112v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于点云环境的信道知识图谱构建", "tldr": "本文提出了一种结合模型和数据驱动的方法，利用点云环境数据构建信道知识图谱（CKM），显著提高了CKM的精度，优于传统的光线追踪和Kriging插值方法。", "motivation": "现有的信道知识图谱（CKM）构建方案采用过于简化的环境信息，严重影响了其准确性，无法有效支持环境感知通信。", "method": "本文提出了一种联合模型驱动和数据驱动的方法来构建CKM。首先，设计了一种新颖的点选择器，通过构建基于不同到达时间（ToA）的同焦点椭圆，识别包含与多径信道增益相关的环境信息的点云子集。然后，利用实地测量收集的真实世界数据集（包含环境点云和相应的信道数据），训练了一个神经网络信道增益估计器，学习每个选定子集与其对应信道增益之间的映射。", "result": "对于功率延迟剖面（PDP）的CKM构建，所提方法实现了2.95 dB的均方根误差（RMSE），显著低于传统光线追踪方法的7.32 dB；对于接收功率值（即无线电图）的CKM构建，它实现了1.04 dB的RMSE，优于Kriging插值方法的1.68 dB。", "conclusion": "本文提出的基于点云环境的信道知识图谱构建方法，通过结合模型和数据驱动的方法，显著提高了信道知识图谱的精度，在不同场景下均表现出优越性。", "translation": "信道知识图谱（CKM）为感兴趣区域提供一定程度的信道状态信息（CSI），通过减少频繁CSI获取的开销，成为环境感知通信的关键使能技术。然而，现有CKM构建方案采用过于简化的环境信息，这显著损害了它们的准确性。为解决此问题，本工作提出了一种联合模型驱动和数据驱动的方法，通过利用点云环境数据以及少量位置标记的信道信息样本来构建CKM。首先，我们提出了一种新颖的点选择器，通过构建基于不同到达时间（ToA）的同焦点椭圆，识别包含与多径信道增益相关的环境信息的点云子集。然后，我们利用通过实地测量收集的真实世界数据集（包含环境点云和相应的信道数据），训练了一个神经网络信道增益估计器，以学习每个选定子集与其相应信道增益之间的映射。最后，实验结果表明：对于功率延迟剖面（PDP）的CKM构建，所提方法实现了2.95 dB的均方根误差（RMSE），显著低于传统光线追踪方法的7.32 dB；对于接收功率值（即无线电图）的CKM构建，它实现了1.04 dB的RMSE，超越了Kriging插值方法的1.68 dB。", "summary": "本文提出了一种创新的联合模型驱动和数据驱动方法，利用详细的点云环境数据和少量信道信息样本来构建高精度的信道知识图谱（CKM）。该方法通过一个新颖的点选择器识别与信道增益相关的点云子集，并训练一个神经网络估计器来学习映射关系。实验结果表明，与传统的光线追踪和Kriging插值方法相比，该方法在构建功率延迟剖面和无线电图的CKM时，均显著降低了均方根误差，证明了其在环境感知通信中的优越性。", "keywords": "信道知识图谱, 点云, 环境感知通信, 神经网络, 信道状态信息", "comments": "本文的创新点在于将详细的点云环境数据引入到信道知识图谱（CKM）的构建中，并通过结合模型驱动（点选择器）和数据驱动（神经网络估计器）的方法，有效解决了现有CKM方案环境信息简化导致精度低的问题。其提出的点选择器是亮点，能够从复杂的点云数据中提取与信道相关的关键信息。实验结果也清晰地展示了其在精度上的显著提升，对未来环境感知通信和无线网络优化具有重要意义。"}}
{"id": "2506.21499", "title": "Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising", "authors": ["Hojat Asgariandehkordi", "Mostafa Sharifzadeh", "Hassan Rivaz"], "summary": "Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by\ncombining echoes from multiple steered transmissions. While increasing the\nnumber of angles generally improves image quality, it drastically reduces the\nframe rate and can introduce blurring artifacts in fast-moving targets.\nMoreover, compounded images remain susceptible to noise, particularly when\nacquired with a limited number of transmissions. We propose a zero-shot\ndenoising framework tailored for low-angle CPWC acquisitions, which enhances\ncontrast without relying on a separate training dataset. The method divides the\navailable transmission angles into two disjoint subsets, each used to form\ncompound images that include higher noise levels. The new compounded images are\nthen used to train a deep model via a self-supervised residual learning scheme,\nenabling it to suppress incoherent noise while preserving anatomical\nstructures. Because angle-dependent artifacts vary between the subsets while\nthe underlying tissue response is similar, this physics-informed pairing allows\nthe network to learn to disentangle the inconsistent artifacts from the\nconsistent tissue signal. Unlike supervised methods, our model requires no\ndomain-specific fine-tuning or paired data, making it adaptable across\nanatomical regions and acquisition setups. The entire pipeline supports\nefficient training with low computational cost due to the use of a lightweight\narchitecture, which comprises only two convolutional layers. Evaluations on\nsimulation, phantom, and in vivo data demonstrate superior contrast enhancement\nand structure preservation compared to both classical and deep learning-based\ndenoising methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21499v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21499v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "轻量级物理信息零样本超声平面波去噪", "tldr": "提出一种轻量级物理信息零样本去噪框架，通过自监督学习在低角度超声图像中实现对比度增强和结构保留，无需外部训练数据。", "motivation": "超声相干平面波复合(CPWC)虽能增强对比度，但增加角度会降低帧率并对快速移动目标引入模糊伪影；且在有限传输次数下，复合图像仍易受噪声影响。", "method": "该方法为低角度CPWC采集设计，无需单独训练数据集。它将可用传输角度分为两个不相交的子集，每个子集用于形成包含更高噪声水平的复合图像。这些新的复合图像通过自监督残差学习方案训练一个深度模型，使其能够抑制不相干噪声同时保留解剖结构。由于子集间的角度相关伪影不同而底层组织响应相似，这种物理信息配对使网络能够学习区分不一致的伪影和一致的组织信号。该模型无需领域特定微调或配对数据，且采用轻量级架构（仅包含两个卷积层）以实现高效训练和低计算成本。", "result": "在仿真、体模和体内数据上的评估表明，与经典和基于深度学习的去噪方法相比，该方法在对比度增强和结构保留方面表现出卓越性能。", "conclusion": "该研究成功开发了一种轻量级、物理信息驱动的零样本去噪框架，有效解决了低角度CPWC超声图像的噪声问题，并在无需外部训练数据的情况下实现了优异的图像质量，展现了其在不同解剖区域和采集设置下的普适性。", "translation": "超声相干平面波复合（CPWC）通过结合来自多个转向传输的回波来增强图像对比度。虽然增加角度数量通常会提高图像质量，但它会显著降低帧率，并可能在快速移动目标中引入模糊伪影。此外，复合图像仍然容易受到噪声影响，尤其是在有限传输次数下采集时。\n我们提出了一种针对低角度CPWC采集的零样本去噪框架，该框架无需单独的训练数据集即可增强对比度。该方法将可用的传输角度分为两个不相交的子集，每个子集用于形成包含更高噪声水平的复合图像。然后，新的复合图像通过自监督残差学习方案训练一个深度模型，使其能够抑制不相干噪声同时保留解剖结构。由于子集之间的角度相关伪影不同，而底层组织响应相似，这种物理信息配对允许网络学习将不一致的伪影与一致的组织信号分离。与监督方法不同，我们的模型不需要领域特定的微调或配对数据，这使其能够适应不同的解剖区域和采集设置。由于采用了轻量级架构（仅包含两个卷积层），整个流程支持高效训练，计算成本低。在仿真、体模和体内数据上的评估表明，与经典和基于深度学习的去噪方法相比，该方法在对比度增强和结构保留方面表现出卓越的性能。", "summary": "本文提出了一种新颖的轻量级物理信息零样本去噪框架，专门用于处理低角度超声相干平面波复合（CPWC）图像中的噪声问题。通过将传输角度分为两个子集并利用自监督残差学习，该模型能够有效抑制噪声并增强对比度，同时保留结构细节。其关键创新在于利用子集间伪影差异与组织信号一致性的物理信息进行训练，从而避免了对外部训练数据和领域特定微调的需求，实现了在多种场景下的普适性。实验结果证明，该方法在对比度增强和结构保留方面优于现有去噪技术。", "keywords": "超声去噪, 零样本学习, 物理信息, 自监督学习, 平面波复合", "comments": "这篇论文的创新点在于其零样本和物理信息驱动的自监督学习范式，有效解决了超声图像去噪中对大量标注数据依赖的问题。轻量级架构也使其具有实际应用潜力，降低了计算成本。通过巧妙地利用角度相关伪影的特性来训练网络，展现了对超声成像物理原理的深刻理解。"}}
{"id": "2506.21329", "title": "Active Inference AI Systems for Scientific Discovery", "authors": ["Karthik Duraisamy"], "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21329v1", "categories": ["cs.AI", "physics.soc-ph", "68", "I.2"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21329v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "科学发现的主动推理AI系统", "tldr": "当前AI系统在科学发现中存在局限。本文提出“主动推理AI系统”通过整合因果自监督模型、贝叶斯引导规划器、知识图谱以及与模拟器/实验室的闭环交互，来弥合抽象、推理和现实差距，并强调人类判断的不可或缺性。", "motivation": "当前人工智能系统在操作架构、脆弱的推理机制以及与实验现实的分离方面存在根本性限制，阻碍了变革性的科学发现。本文认为，AI驱动的科学进步现在取决于弥合抽象、推理和现实这三个基本差距，而非仅仅依赖模型规模、数据或测试时间计算。", "method": "本文将用于科学发现的主动推理AI系统定义为：(i) 维护基于因果自监督基础模型的长期研究记忆；(ii) 具备贝叶斯防护栏的符号或神经符号规划器；(iii) 发展持久的知识图谱，其中思考生成新概念节点，推理建立因果边缘，真实世界交互修剪错误连接并加强已验证的路径；(iv) 通过与高保真模拟器和自动化实验室的闭环交互来完善其内部表示——这是一个操作循环，其中心理模拟指导行动，经验惊喜重塑理解。本质上，本文概述了一种架构，其中发现源于使反事实推理成为可能内部模型与将假设根植于现实的外部验证之间的相互作用。此外，文章还认为，模拟和实验反馈中固有的模糊性以及潜在的不确定性使得人类判断不可或缺，并非作为临时支架，而是作为永久的架构组成部分。", "result": "Not mentioned in abstract", "conclusion": "AI在科学发现中的进步需要弥合抽象、推理和现实差距，而不仅仅是模型扩展。主动推理AI系统，通过整合内部模型与外部验证，并纳入不可或缺的人类判断，为未来的发展指明了方向。", "translation": "人工智能的快速发展引发了人们对变革性科学发现的期望，然而当前的系统在操作架构、脆弱的推理机制以及与实验现实的分离方面仍然存在根本性限制。在早期工作的基础上，我们认为AI驱动的科学进步现在取决于弥合三个基本差距——抽象差距、推理差距和现实差距——而不是依赖模型规模/数据/测试时间计算。科学推理需要支持行动和响应模拟的内部表示、区分相关性与机制的因果结构以及持续校准。我们将用于科学发现的主动推理AI系统定义为：(i) 维护基于因果自监督基础模型的长期研究记忆；(ii) 具备贝叶斯防护栏的符号或神经符号规划器；(iii) 发展持久的知识图谱，其中思考生成新概念节点，推理建立因果边缘，真实世界交互修剪错误连接并加强已验证的路径；(iv) 通过与高保真模拟器和自动化实验室的闭环交互来完善其内部表示——这是一个操作循环，其中心理模拟指导行动，经验惊喜重塑理解。本质上，我们概述了一种架构，其中发现源于使反事实推理成为可能内部模型与将假设根植于现实的外部验证之间的相互作用。文章还认为，模拟和实验反馈中固有的模糊性以及潜在的不确定性使得人类判断不可或缺，并非作为临时支架，而是作为永久的架构组成部分。", "summary": "本文提出“主动推理AI系统”作为一种新的架构范式，旨在克服当前AI在科学发现中的局限性。文章认为，进步的关键在于弥合抽象、推理和现实差距，而非仅仅依赖模型规模或数据。所提出的系统整合了因果自监督基础模型、贝叶斯引导规划器、不断演进的知识图谱，以及与模拟和自动化实验室的闭环交互。这种架构强调内部反事实推理与外部经验验证之间的相互作用。至关重要的是，由于科学反馈中固有的模糊性和不确定性，文章提出人类判断是不可或缺的永久组成部分。", "keywords": "主动推理, 科学发现, AI系统, 因果推理, 知识图谱", "comments": "该论文为AI在科学发现中引入了一种新颖的架构框架，超越了当前的扩展范式。其创新之处在于明确地解决了“差距”（抽象、推理、现实），并提出了一种闭环、主动推理的方法，将内部模型与真实世界互动相结合。对因果推理、知识图谱以及人类判断永久作用的强调是重要的贡献，突出了AI驱动科学更全面和稳健的愿景。其局限性在于，它是一个概念框架而非实证研究。"}}
{"id": "2506.21456", "title": "Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "summary": "Previous work has demonstrated the utility of reductions in the level of\ndetail (LOD) in the periphery of head-tracked, large field of view displays.\nThis paper provides a psychophysically based model, centered around an eye/head\nmovement tradeoff, that explains the effectiveness of peripheral degradation\nand suggests how peripherally degraded displays should be designed. An\nexperiment evaluating the effect on search performance of the shape and area of\nthe high detail central area (inset) in peripherally degraded displays was\nperformed, results indicated that inset shape is not a significant factor in\nperformance. Inset area, however, was significant: performance with displays\nsubtending at least 30 degrees of horizontal and vertical angle was not\nsignificantly different from performance with an undegraded display. These\nresults agreed with the proposed model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21456v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21456v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过头部追踪的周边降级管理细节水平：一个模型及由此产生的设计原则", "tldr": "本文提出了一个基于眼/头运动权衡的心理物理模型，解释了头部追踪大视场显示器中周边细节降级的有效性，并通过实验验证了高细节中央区域（内嵌）的面积而非形状是影响搜索性能的关键因素，且30度角内嵌区域性能与未降级显示无显著差异。", "motivation": "之前的研究表明，在头部追踪的大视场显示器中，降低周边细节水平（LOD）是有用的。本文的动机是提供一个基于心理物理学的模型来解释这种周边降级的有效性，并提出如何设计周边降级的显示器。", "method": "本文提出了一个基于眼/头运动权衡的心理物理学模型。为了验证模型，进行了一项实验，评估了周边降级显示器中高细节中央区域（内嵌）的形状和面积对搜索性能的影响。", "result": "实验结果表明，内嵌区域的形状对性能没有显著影响。然而，内嵌区域的面积是显著因素：当显示器的高细节区域（内嵌）水平和垂直视角至少为30度时，其性能与未降级显示器的性能没有显著差异。这些结果与提出的模型一致。", "conclusion": "本文提出了一个解释周边细节降级有效性的心理物理模型，并通过实验验证了在头部追踪大视场显示器中，高细节中央区域的面积是影响性能的关键因素，且内嵌区域达到特定大小（至少30度水平和垂直视角）时，性能可与未降级显示器媲美。", "translation": "以前的工作已经证明了在头部追踪的大视场显示器中，降低周边细节水平（LOD）的实用性。本文提供了一个基于心理物理学的模型，该模型以眼/头运动权衡为中心，解释了周边降级的有效性，并提出了如何设计周边降级显示器。进行了一项实验，评估了周边降级显示器中高细节中央区域（内嵌）的形状和面积对搜索性能的影响，结果表明内嵌形状对性能不是一个显著因素。然而，内嵌区域是显著的：当显示器的高细节区域水平和垂直视角至少为30度时，其性能与未降级显示器的性能没有显著差异。这些结果与提出的模型一致。", "summary": "本文提出了一个基于眼/头运动权衡的心理物理模型，用于解释头部追踪大视场显示器中周边细节降级的有效性，并指导其设计。通过实验，研究了高细节中央区域（内嵌）的形状和面积对搜索性能的影响。研究发现内嵌形状不影响性能，但内嵌面积是关键因素，当内嵌区域达到至少30度水平和垂直视角时，其性能与未降级显示器无显著差异，这与所提出的模型相符。", "keywords": "头部追踪, 周边降级, 细节水平, 心理物理模型, 大视场显示器", "comments": "这项研究通过提出一个心理物理模型并进行实验验证，为头部追踪大视场显示器中的细节管理提供了理论基础和设计指导。其创新之处在于将眼/头运动权衡纳入模型，并明确了高细节区域的最小有效尺寸，这对于优化虚拟现实和增强现实等应用的用户体验和系统性能具有重要意义。"}}
{"id": "2506.21216", "title": "Edge Clique Partition and Cover Beyond Independence", "authors": ["Fedor V. Fomin", "Petr A. Golovach", "Danil Sagunov", "Kirill Simonov"], "summary": "Covering and partitioning the edges of a graph into cliques are classical\nproblems at the intersection of combinatorial optimization and graph theory,\nhaving been studied through a range of algorithmic and complexity-theoretic\nlenses. Despite the well-known fixed-parameter tractability of these problems\nwhen parameterized by the total number of cliques, such a parameterization\noften fails to be meaningful for sparse graphs. In many real-world instances,\non the other hand, the minimum number of cliques in an edge cover or partition\ncan be very close to the size of a maximum independent set \\alpha(G).\n  Motivated by this observation, we investigate above \\alpha parameterizations\nof the edge clique cover and partition problems. Concretely, we introduce and\nstudy Edge Clique Cover Above Independent Set (ECC/\\alpha) and Edge Clique\nPartition Above Independent Set (ECP/\\alpha), where the goal is to cover or\npartition all edges of a graph using at most \\alpha(G) + k cliques, and k is\nthe parameter. Our main results reveal a distinct complexity landscape for the\ntwo variants. We show that ECP/\\alpha is fixed-parameter tractable, whereas\nECC/\\alpha is NP-complete for all k \\geq 2, yet can be solved in polynomial\ntime for k \\in {0,1}. These findings highlight intriguing differences between\nthe two problems when viewed through the lens of parameterization above a\nnatural lower bound.\n  Finally, we demonstrate that ECC/\\alpha becomes fixed-parameter tractable\nwhen parameterized by k + \\omega(G), where \\omega(G) is the size of a maximum\nclique of the graph G. This result is particularly relevant for sparse graphs,\nin which \\omega is typically small. For H-minor free graphs, we design a\nsubexponential algorithm of running time f(H)^{\\sqrt{k}}n^{O(1)}.", "comment": "An extended abstract of this paper appears in the proceedings of ESA\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21216v1", "categories": ["cs.DS", "cs.DM"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21216v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "边团划分与覆盖超越独立集", "tldr": "本文研究了边团覆盖和划分问题在独立集大小之上的参数化，发现边团划分在独立集大小之上是固定参数可解的，而边团覆盖则在参数k≥2时是NP完全的，但在k+最大团大小参数化下可解。", "motivation": "经典的图边覆盖和划分问题，当以总团数参数化时，对于稀疏图缺乏意义。然而，在许多实际场景中，边覆盖或划分所需的最小团数与最大独立集\n\\alpha(G)的大小非常接近。受此观察启发，本文研究了边团覆盖和划分问题在\n\\alpha之上的参数化。", "method": "本文引入并研究了超越独立集的边团覆盖（ECC/\\alpha）和超越独立集的边团划分（ECP/\\alpha）问题。目标是使用至多\\alpha(G) + k个团来覆盖或划分图的所有边，其中k是参数。此外，还研究了ECC/\\alpha在k + \\omega(G)参数化下的复杂度，并为H-minor自由图设计了次指数算法。", "result": "主要结果揭示了两种变体截然不同的复杂性图景：ECP/\\alpha是固定参数可解的，而ECC/\\alpha对于所有k \\geq 2都是NP完全的，但对于k \\in {0,1}可以在多项式时间内解决。此外，当以k + \\omega(G)（其中\\omega(G)是图G的最大团大小）参数化时，ECC/\\alpha变得固定参数可解。对于H-minor自由图，设计了一个运行时间为f(H)^{\\sqrt{k}}n^{O(1)}的次指数算法。", "conclusion": "这些发现突出了边团覆盖和划分问题在超越自然下界（最大独立集大小）的参数化视角下，二者之间引人入胜的差异。ECC/\\alpha在k + \\omega(G)参数化下的可解性对于稀疏图尤其重要。", "translation": "将图的边覆盖和划分为团是组合优化和图论交叉领域的经典问题，已通过一系列算法和复杂性理论视角进行了研究。尽管这些问题在以总团数参数化时具有众所周知的固定参数可解性，但这种参数化对于稀疏图通常没有意义。另一方面，在许多实际实例中，边覆盖或划分中最小团数可以非常接近最大独立集\\alpha(G)的大小。受此观察启发，我们研究了边团覆盖和划分问题在\\alpha之上的参数化。具体而言，我们引入并研究了超越独立集的边团覆盖（ECC/\\alpha）和超越独立集的边团划分（ECP/\\alpha），其目标是使用至多\\alpha(G) + k个团来覆盖或划分图的所有边，其中k是参数。我们的主要结果揭示了两种变体截然不同的复杂性图景。我们表明ECP/\\alpha是固定参数可解的，而ECC/\\alpha对于所有k \\geq 2都是NP完全的，但对于k \\in {0,1}可以在多项式时间内解决。这些发现突出了在超越自然下界参数化的视角下，这两个问题之间引人入胜的差异。最后，我们证明了当以k + \\omega(G)（其中\\omega(G)是图G的最大团大小）参数化时，ECC/\\alpha变得固定参数可解。这个结果对于稀疏图特别相关，因为在稀疏图中\\omega通常很小。对于H-minor自由图，我们设计了一个运行时间为f(H)^{\\sqrt{k}}n^{O(1)}的次指数算法。", "summary": "本文探讨了图的边团覆盖和划分问题，引入了超越最大独立集大小（\\alpha(G)）的参数化方法。研究了两种新问题：ECC/\\alpha和ECP/\\alpha，其中目标是使用至多\\alpha(G) + k个团。研究发现ECP/\\alpha是固定参数可解的，而ECC/\\alpha在k \\geq 2时是NP完全的，但在k \\in {0,1}时可在多项式时间内解决。此外，论文证明了ECC/\\alpha在k + \\omega(G)参数化下是固定参数可解的，并为H-minor自由图提供了次指数算法。这些结果揭示了两种问题在新的参数化视角下的复杂性差异，对稀疏图尤其有意义。", "keywords": "边团覆盖, 边团划分, 参数化复杂性, 独立集, 稀疏图", "comments": "本文通过引入“超越保证”的参数化方法（基于独立集大小），成功解决了传统参数化方法在稀疏图上边团问题缺乏意义的局限性。这种创新方法揭示了边团覆盖和划分这两个密切相关问题在复杂性上的显著差异，为参数化复杂性理论提供了新的视角和见解。特别是在\\omega较小的图类上的可解性，增加了其在实际应用中的潜在价值。"}}
{"id": "2506.21104", "title": "Robust space-time multiscale upscaling via multicontinuum homogenization for evolving perforated media", "authors": ["Wei Xie", "Viet Ha Hoang", "Yin Yang", "Yunqing Huang"], "summary": "Time-evolving perforated domains arise in many engineering and geoscientific\napplications, including reactive transport, particle deposition, and structural\ndegradation in porous media. Accurately capturing the macroscopic behavior of\nsuch systems poses significant computational challenges due to the dynamic\nfine-scale geometries. In this paper, we develop a robust and generalizable\nmultiscale modeling framework based on multicontinuum homogenization to derive\neffective macroscopic equations in shrinking domains. The method distinguishes\nmultiple continua according to the physical characteristics (e.g., channel\nwidths), and couples them via space-time local cell problems formulated on\nrepresentative volume elements. These local problems incorporate temporal\nderivatives and domain evolution, ensuring consistency with underlying\nfine-scale dynamics. The resulting upscaled system yields computable\nmacroscopic coefficients and is suitable for large-scale simulations. Several\nnumerical experiments are presented to validate the accuracy, efficiency, and\npotential applicability of the method to complex time-dependent engineering\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21104v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21104v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "演化多孔介质中基于多连续体均匀化的鲁棒时空多尺度粗化", "tldr": "本文提出了一种基于多连续体均匀化的鲁棒多尺度建模框架，用于推导时间演化多孔介质中的有效宏观方程，解决了动态精细尺度几何带来的计算挑战。", "motivation": "时间演化多孔域（例如，反应输运、颗粒沉积和结构退化）的宏观行为难以准确捕获，因为其动态精细尺度几何形状带来了显著的计算挑战。", "method": "开发了一种基于多连续体均匀化的鲁棒且可推广的多尺度建模框架。该方法根据物理特性（如通道宽度）区分多个连续体，并通过在代表性体积单元上制定的时空局部单元问题将它们耦合，这些问题包含了时间导数和域演化。", "result": "由此产生的粗化系统产生了可计算的宏观系数，适用于大规模模拟。数值实验验证了该方法的准确性、效率及其在复杂时间相关工程问题中的潜在适用性。", "conclusion": "所开发的多尺度建模框架基于多连续体均匀化，为演化多孔介质中的宏观行为模拟提供了一种鲁棒、准确且高效的方法，适用于大规模和复杂的工程问题。", "translation": "时间演化多孔域出现在许多工程和地球科学应用中，包括反应输运、颗粒沉积和多孔介质中的结构退化。由于动态的精细尺度几何形状，准确捕获此类系统的宏观行为带来了显著的计算挑战。在本文中，我们开发了一种基于多连续体均匀化的鲁棒且可推广的多尺度建模框架，以推导收缩域中的有效宏观方程。该方法根据物理特性（例如，通道宽度）区分多个连续体，并通过在代表性体积单元上制定的时空局部单元问题将它们耦合起来。这些局部问题包含了时间导数和域演化，确保了与底层精细尺度动力学的一致性。由此产生的粗化系统产生了可计算的宏观系数，适用于大规模模拟。本文展示了几个数值实验，以验证该方法的准确性、效率以及其在复杂时间相关工程问题中的潜在适用性。", "summary": "本文提出了一种鲁棒且可推广的多尺度建模框架，利用多连续体均匀化来推导时间演化多孔介质的有效宏观方程。它通过区分和耦合多个连续体，并通过考虑时间导数和域演化的时空局部单元问题，解决了动态精细尺度几何带来的计算难题。由此产生的粗化系统提供了可计算的宏观系数，使其适用于高效的大规模模拟，并已通过数值实验验证。", "keywords": "多尺度粗化, 多连续体均匀化, 演化多孔介质, 时空, 多孔介质", "comments": "该论文的创新之处在于将多尺度均匀化方法扩展到处理时间演化的多孔域，这对于反应输运等动态系统至关重要。利用多连续体均匀化和包含域演化的时空局部单元问题是其关键优势，确保了与精细尺度动力学的一致性，并产生了可计算的宏观系数，适用于大规模模拟。其在复杂时间相关工程问题中的适用性凸显了其实际重要性。"}}
{"id": "2506.20920", "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20920v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20920v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FineWeb2: 一个可扩展至所有语言的流水线——使预训练数据处理适应每种语言", "tldr": "该论文介绍了FineWeb2，一个新型预训练数据流水线，能够自动适应任何语言，从而创建更好的多语言LLM数据集和模型，并可扩展到1000多种语言。", "motivation": "训练高性能多语言大型语言模型（LLM）面临挑战，主要原因是在于难以针对大量语言定制过滤和去重流水线，尽管英语数据集的开放开发已取得实质性进展。", "method": "该研究引入了一种基于FineWeb的新型预训练数据集整理流水线，能够自动适应任何语言。研究人员在九种不同语言上广泛消融了流水线设计选择，并使用基于可衡量标准选择的评估任务进行指导。此外，还引入了一种考虑重复计数和质量的数据集再平衡方法。最终，该流水线被扩展到1000多种语言，利用近100个Common Crawl快照生成了FineWeb2。", "result": "该流水线能够创建比现有数据集产生更高性能模型的非英语语料库。数据集再平衡方法提供了额外的性能提升。最终生成了FineWeb2，一个20TB（50亿文档）的多语言数据集。", "conclusion": "该论文成功开发并扩展了FineWeb2，一个新型多语言预训练数据集和流水线，解决了为LLM创建高质量非英语数据的挑战，并带来了更高性能的模型。", "translation": "预训练最先进的大型语言模型（LLM）需要大量干净且多样化的文本数据。尽管大型高质量英语预训练数据集的开放开发最近取得了实质性进展，但训练高性能多语言LLM仍然是一个挑战，这在很大程度上是由于难以针对大量语言定制过滤和去重流水线。在这项工作中，我们引入了一种基于FineWeb的新型预训练数据集整理流水线，该流水线可以自动适应支持任何语言。我们在一组九种不同语言上广泛消融了我们的流水线设计选择，并由一组有意义且信息丰富的评估任务指导，这些任务通过基于可衡量标准的新颖选择过程进行选择。最终，我们表明我们的流水线可用于创建比现有数据集产生更高性能模型的非英语语料库。我们还引入了一种直接且有原则的方法来重新平衡数据集，该方法考虑了重复计数和质量，从而提供了额外的性能提升。最后，我们使用近100个Common Crawl快照将我们的流水线扩展到1000多种语言，以生成FineWeb2，这是一个新的20TB（50亿文档）多语言数据集，我们与我们的流水线、训练和评估代码库一起发布。", "summary": "该论文介绍了FineWeb2，一种新颖的预训练数据整理流水线，旨在自动适应任何语言，解决了为大型语言模型创建高质量多语言数据集的挑战。该流水线的设计选择在九种语言上进行了广泛评估，证明其能够生成比现有数据集产生更高性能模型的非英语语料库。此外，一种新的、有原则的数据集再平衡方法，考虑到重复和质量，进一步提升了模型性能。作者将该流水线扩展到1000多种语言，生成了20TB的多语言数据集FineWeb2，并与代码库一同发布。", "keywords": "预训练数据, 多语言LLM, 数据整理, FineWeb2, 数据流水线", "comments": "该论文通过提供一个可扩展且适应性强的解决方案来创建高质量的多语言预训练数据，做出了重大贡献，这对于开发全球相关的大型语言模型至关重要。其自动适应能力和有原则的再平衡方法具有创新性。FineWeb2数据集和代码库的发布将极大地造福开源社区。"}}
{"id": "2506.21041", "title": "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling", "authors": ["Junwei You", "Pei Li", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Haotian Shi", "Bin Ran"], "summary": "Ensuring robust planning and decision-making under rare, diverse, and\nvisually degraded long-tail scenarios remains a fundamental challenge for\nautonomous driving in urban environments. This issue becomes more critical in\ncooperative settings, where vehicles and infrastructure jointly perceive and\nreason across complex environments. To address this challenge, we propose\nV2X-REALM, a vision-language model (VLM)-based framework with adaptive\nmultimodal learning for robust cooperative autonomous driving under long-tail\nscenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven\nlong-tail scenario generation and evaluation pipeline that leverages foundation\nmodels to synthesize realistic long-tail conditions such as snow and fog across\nvehicle- and infrastructure-side views, enriching training diversity\nefficiently; (ii) a gated multi-scenario adaptive attention module that\nmodulates the visual stream using scenario priors to recalibrate ambiguous or\ncorrupted features; and (iii) a multi-task scenario-aware contrastive learning\nobjective that improves multimodal alignment and promotes cross-scenario\nfeature separability. Extensive experiments demonstrate that V2X-REALM\nsignificantly outperforms existing baselines in robustness, semantic reasoning,\nsafety, and planning accuracy under complex, challenging driving conditions,\nadvancing the scalability of end-to-end cooperative autonomous driving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21041v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21041v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "V2X-REALM：基于视觉-语言模型的鲁棒端到端协同自动驾驶与自适应长尾建模", "tldr": "V2X-REALM提出了一种基于视觉-语言模型的方法，通过创新的场景生成、注意力机制和对比学习，显著提升了在复杂长尾场景下协同自动驾驶的鲁棒性和性能。", "motivation": "在城市环境中，自动驾驶车辆在稀有、多样且视觉退化的长尾场景下，实现鲁棒的规划和决策面临巨大挑战，尤其在车辆和基础设施协同感知的合作设置中更为关键。", "method": "提出V2X-REALM框架，一个基于视觉-语言模型（VLM）并结合自适应多模态学习的方法，用于在长尾场景下实现鲁棒的协同自动驾驶。该框架包含三项核心创新：1) 一个提示驱动的长尾场景生成和评估管道，利用基础模型合成雪、雾等现实长尾条件，高效丰富训练多样性；2) 一个门控多场景自适应注意力模块，利用场景先验调节视觉流以校准模糊或损坏的特征；3) 一个多任务场景感知对比学习目标，以改善多模态对齐并促进跨场景特征可分离性。", "result": "实验证明，V2X-REALM在复杂、具有挑战性的驾驶条件下，其鲁棒性、语义推理、安全性以及规划精度均显著优于现有基线。", "conclusion": "V2X-REALM显著提升了端到端协同自动驾驶的鲁棒性、语义推理、安全性及规划精度，从而推动了其可扩展性。", "translation": "确保在稀有、多样且视觉退化的长尾场景下进行鲁棒的规划和决策，仍然是城市环境中自动驾驶面临的一个基本挑战。在协同设置中，当车辆和基础设施在复杂环境中共同感知和推理时，这个问题变得更加关键。为了解决这个挑战，我们提出了V2X-REALM，一个基于视觉-语言模型（VLM）的框架，通过自适应多模态学习，在长尾场景下实现鲁棒的协同自动驾驶。V2X-REALM引入了三项核心创新：(i) 一个提示驱动的长尾场景生成和评估管道，利用基础模型合成车辆和基础设施侧视图中的雪、雾等现实长尾条件，有效丰富训练多样性；(ii) 一个门控多场景自适应注意力模块，利用场景先验调节视觉流以校准模糊或损坏的特征；(iii) 一个多任务场景感知对比学习目标，以改善多模态对齐并促进跨场景特征可分离性。大量实验表明，V2X-REALM在复杂、具有挑战性的驾驶条件下，其鲁棒性、语义推理、安全性以及规划精度均显著优于现有基线，从而推动了端到端协同自动驾驶的可扩展性。", "summary": "本文提出了V2X-REALM，一个基于视觉-语言模型（VLM）的框架，旨在解决协同自动驾驶在长尾场景下鲁棒性不足的问题。该框架通过创新的提示驱动长尾场景生成、门控多场景自适应注意力模块和多任务场景感知对比学习目标，有效提升了模型在复杂环境下的鲁棒性、语义推理、安全性和规划精度，并显著优于现有方法，推进了端到端协同自动驾驶的可扩展性。", "keywords": "协同自动驾驶, 长尾场景, 视觉-语言模型, 鲁棒性, 数据增强", "comments": "V2X-REALM的创新点在于其结合了视觉-语言模型与自适应多模态学习，并通过合成长尾场景数据、引入自适应注意力机制和对比学习来提升模型在极端复杂条件下的鲁棒性。这种方法对于解决自动驾驶领域的长尾问题具有重要意义，尤其是在协同自动驾驶背景下，其提出的数据增强和特征校准策略具有很强的实用价值和前瞻性。"}}
{"id": "2506.21266", "title": "KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks", "authors": ["Daniil Karol", "Elizaveta Artser", "Ilya Vlasov", "Yaroslav Golubev", "Hieke Keuning", "Anastasiia Birillo"], "summary": "Collecting data of students solving programming tasks is incredibly valuable\nfor researchers and educators. It allows verifying that the students correctly\napply the features and concepts they are taught, or finding students'\nmisconceptions. However, existing data collection tools have limitations, e.g.,\nno control over the granularity of the collected code, not collecting the\nspecific events of the programming environment used, and overall being hard to\nconfigure.\n  To overcome these limitations, we propose KOALA, a convenient and highly\nconfigurable tool for collecting code snapshots and feature usage from students\nsolving programming tasks in JetBrains IDEs. The plugin can be installed in\nIDEs and configured to provide the students with the necessary tasks, enable or\ndisable certain IDE features like code completion, and run surveys. During\nproblem solving, the plugin collects code snapshots at the configured\ngranularity, all IDE actions like running and debugging, as well as some data\nnot collected in prior works, like employed hotkeys and switching focus between\nfiles. The collected data is sent to the server that comes with the tool, where\nit is stored and can be converted to the standardized ProgSnap2 format. To\nshowcase the tool, we collected data from 28 students solving tasks in two\ncourses within the IDE, highlighting some insights from this data.", "comment": "Accepted to CompEd'25, 7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21266v1", "categories": ["cs.SE", "cs.CY"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21266v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "KOALA：一个用于收集解决编程任务时IDE数据的可配置工具", "tldr": "KOALA是一个可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用数据，以克服现有数据收集工具的局限性。", "motivation": "现有数据收集工具存在局限性，例如无法控制收集代码的粒度、不收集特定编程环境事件以及难以配置。为了克服这些限制，研究人员需要一个更便捷、可配置的工具来收集学生在解决编程任务时的数据。", "method": "研究人员提出了KOALA，一个方便且高度可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用数据。该插件可以安装在IDE中，并配置为提供任务、启用/禁用IDE功能（如代码补全）和运行调查。在问题解决过程中，KOALA会收集配置粒度的代码快照、所有IDE操作（如运行和调试），以及之前工作中未收集的数据，如热键使用和文件焦点切换。收集到的数据会发送到工具附带的服务器，存储并可转换为标准化的ProgSnap2格式。", "result": "为了展示该工具，研究人员收集了28名学生在IDE中完成两门课程任务的数据，并从中 highlighting 了一些见解。", "conclusion": "KOALA为研究人员和教育工作者提供了一个强大且灵活的工具，用于收集学生在编程任务中的详细行为数据，克服了现有工具的局限性，从而更好地理解学生的学习过程和发现误解。", "translation": "收集学生解决编程任务的数据对于研究人员和教育工作者来说非常有价值。它允许验证学生是否正确应用了所学的特性和概念，或者发现学生的误解。然而，现有的数据收集工具存在局限性，例如无法控制所收集代码的粒度，不收集所用编程环境的特定事件，并且总体上难以配置。\n为了克服这些局限性，我们提出了KOALA，一个方便且高度可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用情况。该插件可以安装在IDE中，并配置为向学生提供必要的任务、启用或禁用某些IDE功能（如代码补全），以及运行调查。在问题解决过程中，该插件会收集配置粒度的代码快照、所有IDE操作（如运行和调试），以及之前工作中未收集的一些数据，例如使用的热键和文件之间的焦点切换。收集到的数据会发送到工具附带的服务器，在那里存储并可以转换为标准化的ProgSnap2格式。为了展示该工具，我们收集了28名学生在IDE中两门课程中解决任务的数据，并突出显示了这些数据中的一些见解。", "summary": "KOALA是一个为研究人员和教育工作者设计的可配置工具，旨在克服现有编程任务数据收集工具的局限性。它作为一个插件安装在JetBrains IDE中，能够以可配置的粒度收集学生的代码快照、IDE操作（如运行、调试、热键使用和文件焦点切换）以及功能使用情况。收集到的数据可存储在配套服务器上并转换为ProgSnap2格式。该工具已成功用于收集28名学生在实际课程中解决任务的数据，并提供了有价值的见解。", "keywords": "数据收集, IDE, 编程任务, KOALA, 可配置工具", "comments": "KOALA的创新之处在于其高度可配置性，允许研究人员精确控制数据收集的粒度和类型，包括以往工具未捕捉到的热键和焦点切换等细节。这对于深入理解学生编程行为和认知过程具有重要意义。其对JetBrains IDE的特定支持以及数据转换为ProgSnap2标准格式的能力，也增强了其实用性和数据互操作性。"}}
{"id": "2506.21417", "title": "Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation", "authors": ["Yunxiu Xu", "Siyu Wang", "Shoichi Hasegawa"], "summary": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device, designed\nwith thin strings and actuators attached to the fingernails, ensures minimal\nweight (1.55 g per finger) and preserves finger flexibility. Integrating the\nsoftware with a physics engine renders multiple types of haptic feedback (grip\nforce, collision, and sliding vibration feedback). We evaluated the device's\nperformance in pressure perception, slip feedback, typical dexterous\nmanipulation tasks, and daily operations, and we gathered user experience\nthrough subjective assessments. Our results show that participants could\nperceive and respond to pressure and vibration feedback. Through dexterous\nmanipulation experiments, we further demonstrated that these minimal haptic\ncues significantly improved virtual task efficiency, showcasing how lightweight\nhaptic feedback can enhance manipulation performance without complex\nmechanisms. The device's ability to preserve tactile sensations and minimize\nhindrance to real-world operations is a key advantage over glove-type haptic\ndevices. This research offers a potential solution for designing haptic\ninterfaces that balance lightweight construction, haptic feedback for dexterous\nmanipulation, and daily wearability.", "comment": "14 pages, 15 figures, 2 tables. Published in IEEE Transactions on\n  Haptics (Early Access)", "pdf_url": "http://arxiv.org/pdf/2506.21417v1", "categories": ["cs.HC", "H.5.2; I.3.6"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.21417v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "轻量化指甲触觉设备：无阻碍指腹力和振动反馈，增强虚拟灵巧操作", "tldr": "一种轻量级、可穿戴的指甲触觉设备，通过提供力觉和振动反馈，显著提高了虚拟环境中的灵巧操作效率，同时不影响真实世界互动。", "motivation": "现有触觉设备可能阻碍真实世界互动或过于笨重。本研究旨在开发一种轻量级、可穿戴的触觉设备，在提供有效触觉反馈以增强虚拟灵巧操作的同时，不影响用户在现实世界中的活动。", "method": "该研究提出了一种轻量级、可穿戴的指尖触觉设备，其设计特点是将细线和执行器连接到指甲上，以确保最小重量（每根手指1.55克）并保持手指灵活性。该设备软件与物理引擎集成，能够提供多种触觉反馈，包括抓握力、碰撞和滑动振动反馈。研究通过压力感知、滑移反馈、典型的灵巧操作任务和日常操作来评估设备性能，并通过主观评估收集用户体验。", "result": "参与者能够感知并响应压力和振动反馈。通过灵巧操作实验，证明了这些微小的触觉线索显著提高了虚拟任务效率。该设备保留了触觉感知并最大限度地减少了对现实世界操作的阻碍，这是其相对于手套式触觉设备的关键优势。", "conclusion": "本研究提供了一种潜在的解决方案，用于设计一种在轻量化构造、灵巧操作触觉反馈和日常佩戴性之间取得平衡的触觉界面。", "translation": "本研究提出了一种轻量级、可穿戴的指尖触觉设备，该设备为虚拟环境中的灵巧操作提供基于物理的触觉反馈，同时不阻碍真实世界互动。该设备采用连接到指甲上的细线和执行器设计，确保了最小重量（每根手指1.55克）并保持了手指灵活性。将软件与物理引擎集成，可以呈现多种类型的触觉反馈（抓握力、碰撞和滑动振动反馈）。我们评估了设备在压力感知、滑移反馈、典型灵巧操作任务和日常操作中的性能，并通过主观评估收集了用户体验。我们的结果表明，参与者能够感知并响应压力和振动反馈。通过灵巧操作实验，我们进一步证明这些微小的触觉线索显著提高了虚拟任务效率，展示了轻量级触觉反馈如何在没有复杂机制的情况下增强操作性能。该设备保留触觉感知并最大限度地减少对现实世界操作的阻碍，这是其相对于手套式触觉设备的关键优势。这项研究为设计平衡轻量化构造、灵巧操作触觉反馈和日常佩戴性的触觉界面提供了一种潜在的解决方案。", "summary": "本研究介绍了一种创新的轻量级指尖触觉设备，通过将细线和执行器连接到指甲上，实现了仅1.55克的单指重量，同时保持了手指的自然灵活性。该设备与物理引擎集成，能提供抓握力、碰撞和滑动振动等多种触觉反馈。实验证明，该设备能有效传递压力和振动反馈，显著提升了虚拟灵巧操作任务的效率。其主要优势在于在提供有效触觉反馈的同时，不阻碍用户的真实世界操作和触觉感知，为虚拟现实和日常可穿戴触觉界面设计提供了新的平衡解决方案。", "keywords": "触觉设备, 灵巧操作, 虚拟现实, 指甲触觉, 轻量化", "comments": "这项研究的创新之处在于其独特的指甲附着设计，这使得设备极其轻量化且不阻碍指腹的自然触觉。这解决了传统手套式触觉设备笨重、影响真实世界互动和触觉感知的痛点。其重要性在于为虚拟现实和增强现实中的精细操作提供了一种实用且高效的解决方案，尤其是在需要高灵活性和真实感触觉反馈的应用场景中。该设备的简洁机制和显著的性能提升，使其在未来人机交互领域具有广阔的应用潜力。"}}
{"id": "2506.21352", "title": "Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions", "authors": ["Le Vu Anh", "Mehmet Dik", "Nguyen Viet Anh"], "summary": "Persistent Laplacians are matrix operators that track how the shape and\nstructure of data transform across scales and are popularly adopted in biology,\nphysics, and machine learning. Their eigenvalues are concise descriptors of\ngeometric and topological features in a filtration. Although earlier work\nestablished global algebraic stability for these operators, the precise change\nin a single eigenvalue when one simplex, such as a vertex, edge, or triangle,\nis added has remained unknown. This is important because downstream tools,\nincluding heat-kernel signatures and spectral neural networks, depend directly\non these eigenvalues. We close this gap by proving a uniform Lipschitz bound:\nafter inserting one simplex, every up-persistent Laplacian eigenvalue can vary\nby at most twice the Euclidean norm of that simplex's boundary, independent of\nfiltration scale and complex size. This result delivers the first\neigenvalue-level robustness guarantee for spectral topological data analysis.\nIt guarantees that spectral features remain stable under local updates and\nenables reliable error control in dynamic data settings.", "comment": "16 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21352v1", "categories": ["cs.LG", "cs.IT", "math.IT", "math.MG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21352v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "持久拉普拉斯特征值在单形插入下的利普希茨界", "tldr": "本文证明了持久拉普拉斯特征值在插入单个单形时，其变化量存在一个统一的利普希茨界，从而首次为谱拓扑数据分析提供了特征值层面的鲁棒性保证。", "motivation": "尽管早期工作已确立了持久拉普拉斯算子的全局代数稳定性，但当插入一个单形（如顶点、边或三角形）时，单个特征值的精确变化仍是未知的。这很重要，因为下游工具（包括热核签名和谱神经网络）直接依赖于这些特征值。", "method": "通过证明一个统一的利普希茨界来解决这一问题。", "result": "在插入一个单形后，每个上持久拉普拉斯特征值最多可变化该单形边界的欧几里得范数的两倍，且与过滤尺度和复形大小无关。", "conclusion": "该结果首次为谱拓扑数据分析提供了特征值层面的鲁棒性保证。它保证了谱特征在局部更新下保持稳定，并使得动态数据设置中的可靠误差控制成为可能。", "translation": "持久拉普拉斯算子是跟踪数据形状和结构如何跨尺度变化的矩阵算子，广泛应用于生物学、物理学和机器学习领域。它们的特征值是过滤中几何和拓扑特征的简洁描述符。尽管早期工作已确立了这些算子的全局代数稳定性，但当插入一个单形（如顶点、边或三角形）时，单个特征值的精确变化仍是未知的。这很重要，因为下游工具（包括热核签名和谱神经网络）直接依赖于这些特征值。我们通过证明一个统一的利普希茨界来弥补这一空白：在插入一个单形后，每个上持久拉普拉斯特征值最多可变化该单形边界的欧几里得范数的两倍，且与过滤尺度和复形大小无关。该结果首次为谱拓扑数据分析提供了特征值层面的鲁棒性保证。它保证了谱特征在局部更新下保持稳定，并使得动态数据设置中的可靠误差控制成为可能。", "summary": "本文解决了当插入单个单形时，持久拉普拉斯特征值的精确变化未知的问题，这对于下游工具至关重要。作者证明了一个统一的利普希茨界，表明任何上持久拉普拉斯特征值最多变化插入单形边界欧几里得范数的两倍，且与过滤尺度和复形大小无关。这为谱拓扑数据分析提供了首个特征值层面的鲁棒性保证，确保了谱特征在局部更新下的稳定性，并使得动态数据中的可靠误差控制成为可能。", "keywords": "持久拉普拉斯算子, 特征值, 利普希茨界, 单形插入, 拓扑数据分析", "comments": "本文为谱拓扑数据分析提供了基础性理论保证，对于数据动态演变或受噪声影响的应用具有重要意义。其“首次特征值层面的鲁棒性保证”突显了其重要性。"}}
{"id": "2506.20867", "title": "Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation", "authors": ["Ryosuke Kawamura", "Hideaki Hayashi", "Shunsuke Otake", "Noriko Takemura", "Hajime Nagahara"], "summary": "Dynamic facial expression recognition (DFER) is a task that estimates\nemotions from facial expression video sequences. For practical applications,\naccurately recognizing ambiguous facial expressions -- frequently encountered\nin in-the-wild data -- is essential. In this study, we propose MIDAS, a data\naugmentation method designed to enhance DFER performance for ambiguous facial\nexpression data using soft labels representing probabilities of multiple\nemotion classes. MIDAS augments training data by convexly combining pairs of\nvideo frames and their corresponding emotion class labels. This approach\nextends mixup to soft-labeled video data, offering a simple yet highly\neffective method for handling ambiguity in DFER. To evaluate MIDAS, we\nconducted experiments on both the DFEW dataset and FERV39k-Plus, a newly\nconstructed dataset that assigns soft labels to an existing DFER dataset. The\nresults demonstrate that models trained with MIDAS-augmented data achieve\nsuperior performance compared to the state-of-the-art method trained on the\noriginal dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20867v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20867v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "使用基于软标签的数据增强提升模糊动态面部表情识别", "tldr": "本文提出MIDAS，一种基于软标签的数据增强方法，旨在改善动态面部表情识别，特别是针对模糊表情，并展示了其卓越的性能。", "motivation": "在实际应用中，准确识别野外数据中常见的模糊面部表情对于动态面部表情识别（DFER）至关重要。", "method": "本文提出MIDAS，一种数据增强方法。MIDAS通过凸组合视频帧对及其对应的情感类别软标签来增强训练数据。这种方法将mixup技术扩展到软标签视频数据。研究团队在DFEW数据集和新构建的FERV39k-Plus数据集上进行了实验。", "result": "使用MIDAS增强数据训练的模型，与在原始数据集上训练的现有最先进方法相比，取得了卓越的性能。", "conclusion": "MIDAS是一种简单但高效的处理DFER中模糊性的方法，并显著提升了模糊面部表情数据的DFER性能。", "translation": "动态面部表情识别（DFER）是一项从面部表情视频序列中估计情绪的任务。对于实际应用而言，准确识别模糊面部表情——这在野外数据中频繁遇到——至关重要。在本研究中，我们提出了MIDAS，一种数据增强方法，旨在利用代表多种情感类别概率的软标签，提升模糊面部表情数据的DFER性能。MIDAS通过凸组合视频帧对及其对应的情感类别标签来增强训练数据。这种方法将mixup扩展到软标签视频数据，为处理DFER中的模糊性提供了一种简单而高效的方法。为了评估MIDAS，我们在DFEW数据集和FERV39k-Plus（一个新构建的、为现有DFER数据集分配软标签的数据集）上进行了实验。结果表明，使用MIDAS增强数据训练的模型，与在原始数据集上训练的最先进方法相比，取得了卓越的性能。", "summary": "本文介绍了一种名为MIDAS的新型基于软标签的数据增强方法，用于动态面部表情识别（DFER）。MIDAS通过将mixup方法应用于视频帧及其软标签，增强了对模糊面部表情的识别。在DFEW和新数据集FERV39k-Plus上的评估表明，经过MIDAS增强训练的模型表现优于现有的最先进方法。", "keywords": "动态面部表情识别, 数据增强, 软标签, 模糊表情, MIDAS", "comments": "该研究的创新之处在于将mixup技术扩展到带有软标签的视频数据，以解决动态面部表情识别中的模糊性问题，这是一个实际挑战。此外，构建一个新的带有软标签的数据集（FERV39k-Plus）也值得关注。"}}
{"id": "2506.21422", "title": "Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget", "authors": ["Kevin Kreutz", "Philipp Wiesner", "Monica Vitali"], "summary": "The carbon footprint of data centers has recently become a critical concern.\nSo far, most carbon-aware strategies have focused on leveraging the flexibility\nof scheduling decisions for batch processing by shifting the time and location\nof workload executions. However, such approaches cannot be applied to\nservice-oriented cloud applications, since they have to be reachable at every\npoint in time and often at low latencies. We propose a carbon-aware approach\nfor operating microservices under hourly carbon budgets. By choosing the most\nappropriate version and horizontal scaleout for each microservice, our strategy\nmaximizes user experience and revenue while staying within budget constraints.\nExperiments across various application configurations and carbon budgets\ndemonstrate that the approach adapts properly to changing workloads and carbon\nintensities.", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "pdf_url": "http://arxiv.org/pdf/2506.21422v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21422v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "面向预算优化用户体验的碳感知微服务部署", "tldr": "提出一种碳感知微服务部署策略，通过选择合适的版本和扩展规模，在预算内最大化用户体验和收益，以应对数据中心碳排放问题。", "motivation": "数据中心的碳足迹日益成为关键问题。现有碳感知策略主要针对批处理，无法应用于需要低延迟和持续可达的服务导向型云应用。", "method": "提出一种碳感知方法，用于在每小时碳预算下运行微服务。通过为每个微服务选择最合适的版本和横向扩展规模，在预算约束内最大化用户体验和收益。", "result": "实验证明，该方法能正确适应不断变化的工作负载和碳强度。", "conclusion": "该碳感知微服务部署方法能够在预算内有效优化用户体验和收益，并适应动态环境。", "translation": "数据中心的碳足迹最近已成为一个关键问题。\n迄今为止，大多数碳感知策略都侧重于利用调度决策的灵活性，通过改变工作负载执行的时间和地点来进行批处理。然而，这种方法不能应用于面向服务的云应用程序，因为它们必须在任何时间点都可达，并且通常需要低延迟。我们提出了一种碳感知方法，用于在每小时碳预算下运行微服务。通过为每个微服务选择最合适的版本和横向扩展规模，我们的策略在保持预算约束的同时，最大化用户体验和收益。在各种应用程序配置和碳预算下的实验表明，该方法能正确适应不断变化的工作负载和碳强度。", "summary": "本文针对数据中心碳排放问题，提出一种碳感知微服务部署策略。该策略通过为微服务选择合适的版本和横向扩展规模，在每小时碳预算内最大化用户体验和收益，弥补了现有碳感知方法不适用于服务导向型应用的不足。实验证明其能有效适应动态的工作负载和碳强度。", "keywords": "Carbon-aware, Microservices, User Experience, Carbon Footprint, Cloud Applications", "comments": "该研究创新性地将碳感知策略应用于服务导向型微服务，解决了传统碳感知方法无法应对实时、低延迟应用场景的局限性。其在预算约束下平衡环境影响、用户体验和收益的思路具有重要实践意义。"}}
{"id": "2506.20752", "title": "Characterization and Mitigation of Training Instabilities in Microscaling Formats", "authors": ["Huangyuan Su", "Mujin Kwun", "Stephanie Gil", "Sham Kakade", "Nikhil Anand"], "summary": "Training large language models is an expensive, compute-bound process that\nmust be repeated as models scale, algorithms improve, and new data is\ncollected. To address this, next-generation hardware accelerators increasingly\nsupport lower-precision arithmetic formats, such as the Microscaling (MX)\nformats introduced in NVIDIA's Blackwell architecture. These formats use a\nshared scale within blocks of parameters to extend representable range and\nperform forward/backward GEMM operations in reduced precision for efficiency\ngains. In this work, we investigate the challenges and viability of\nblock-scaled precision formats during model training. Across nearly one\nthousand language models trained from scratch -- spanning compute budgets from\n$2 \\times 10^{17}$ to $4.8 \\times 10^{19}$ FLOPs and sweeping over a broad\nrange of weight-activation precision combinations -- we consistently observe\nthat training in MX formats exhibits sharp, stochastic instabilities in the\nloss, particularly at larger compute scales. To explain this phenomenon, we\nconduct controlled experiments and ablations on a smaller proxy model that\nexhibits similar behavior as the language model, sweeping across architectural\nsettings, hyperparameters, and precision formats. These experiments motivate a\nsimple model in which multiplicative gradient bias introduced by the\nquantization of layer-norm affine parameters and a small fraction of\nactivations can trigger runaway divergence. Through \\emph{in situ} intervention\nexperiments on our proxy model, we demonstrate that instabilities can be\naverted or delayed by modifying precision schemes mid-training. Guided by these\nfindings, we evaluate stabilization strategies in the LLM setting and show that\ncertain hybrid configurations recover performance competitive with\nfull-precision training. We release our code at\nhttps://github.com/Hither1/systems-scaling.", "comment": "14 pages + appendices", "pdf_url": "http://arxiv.org/pdf/2506.20752v1", "categories": ["cs.LG", "cs.AR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20752v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "微缩格式训练不稳定性表征与缓解", "tldr": "本文研究了微缩（MX）格式训练大型语言模型时出现的训练不稳定性，并提出了缓解策略，证明混合精度配置可恢复性能。", "motivation": "训练大型语言模型是一个昂贵且计算密集的过程。为了提高效率，下一代硬件加速器支持低精度算术格式，例如NVIDIA Blackwell架构中引入的微缩（MX）格式。然而，这些新格式在模型训练期间可能引入挑战，因此需要对其不稳定性进行表征和缓解研究。", "method": "研究人员训练了近千个语言模型，涵盖不同计算预算和权重-激活精度组合，以观察MX格式的训练行为。为了解释观察到的不稳定性，他们在较小的代理模型上进行了受控实验和消融研究，探索了架构设置、超参数和精度格式的影响。通过代理模型上的“原位”干预实验，证明了通过修改训练中的精度方案可以避免或延迟不稳定性。最后，将这些发现应用于LLM设置，评估了稳定策略。", "result": "研究一致发现，使用MX格式训练时，损失函数会出现尖锐的随机不稳定性，尤其是在较大的计算规模下。通过受控实验，他们提出了一个简单模型，解释了由层归一化仿射参数和少量激活的量化引入的乘法梯度偏差可能导致失控发散。干预实验表明，通过在训练中期修改精度方案可以避免或延迟不稳定性。在LLM设置中，某些混合配置能恢复与全精度训练相当的性能。", "conclusion": "微缩格式在大型语言模型训练中存在不稳定性，这种不稳定性是由量化引入的梯度偏差引起的。通过在训练过程中采用混合精度配置等稳定策略，可以有效缓解这些不稳定性，使其性能与全精度训练相当。", "translation": "训练大型语言模型是一个昂贵且计算受限的过程，随着模型规模的扩大、算法的改进和新数据的收集，这一过程必须重复进行。为了解决这个问题，下一代硬件加速器越来越多地支持低精度算术格式，例如NVIDIA Blackwell架构中引入的微缩（MX）格式。这些格式在参数块内使用共享比例因子来扩展可表示范围，并以较低精度执行前向/后向GEMM操作以提高效率。在这项工作中，我们研究了块缩放精度格式在模型训练期间的挑战和可行性。在从头开始训练的近千个语言模型中——计算预算从$2 \\times 10^{17}$到$4.8 \\times 10^{19}$ FLOPs，并涵盖了广泛的权重-激活精度组合——我们一致观察到，MX格式的训练表现出尖锐的、随机的损失不稳定性，尤其是在较大的计算规模下。为了解释这种现象，我们对一个表现出与语言模型相似行为的较小代理模型进行了受控实验和消融研究，探索了架构设置、超参数和精度格式。这些实验促使我们提出了一个简单模型，其中由层归一化仿射参数和少量激活的量化引入的乘法梯度偏差可以触发失控发散。通过对代理模型的“原位”干预实验，我们证明了通过在训练中期修改精度方案可以避免或延迟不稳定性。在这些发现的指导下，我们在LLM设置中评估了稳定策略，并表明某些混合配置可以恢复与全精度训练相当的性能。我们的代码已在https://github.com/Hither1/systems-scaling 发布。", "summary": "本文深入研究了NVIDIA Blackwell架构中引入的微缩（MX）格式在大型语言模型训练中出现的训练不稳定性。通过对大量模型的实验，作者发现MX格式在训练损失中表现出尖锐的随机不稳定性，尤其是在大规模计算下。研究揭示，这种不稳定性源于量化引入的乘法梯度偏差。为解决此问题，研究提出了通过在训练中期调整精度方案来规避或延迟不稳定性，并证明了在LLM训练中，采用某些混合精度配置能够达到与全精度训练相当的性能，从而为高效的低精度模型训练提供了可行途径。", "keywords": "微缩格式, 训练不稳定性, 大型语言模型, 低精度训练, 梯度偏差", "comments": "这篇论文的创新点在于系统性地表征了新型低精度微缩格式（MX）在大型语言模型训练中出现的稳定性问题，并深入分析了其根本原因（量化引入的梯度偏差）。更重要的是，它不仅指出了问题，还提出了实用的缓解策略，如动态精度调整和混合配置，这对于推动下一代AI硬件的有效利用和大规模模型训练的效率至关重要。其研究结果对于未来低精度训练的优化具有指导意义。"}}
{"id": "2506.20799", "title": "Structural System Identification via Validation and Adaptation", "authors": ["Cristian López", "Keegan J. Moore"], "summary": "Estimating the governing equation parameter values is essential for\nintegrating experimental data with scientific theory to understand, validate,\nand predict the dynamics of complex systems. In this work, we propose a new\nmethod for structural system identification (SI), uncertainty quantification,\nand validation directly from data. Inspired by generative modeling frameworks,\na neural network maps random noise to physically meaningful parameters. These\nparameters are then used in the known equation of motion to obtain fake\naccelerations, which are compared to real training data via a mean square error\nloss. To simultaneously validate the learned parameters, we use independent\nvalidation datasets. The generated accelerations from these datasets are\nevaluated by a discriminator network, which determines whether the output is\nreal or fake, and guides the parameter-generator network. Analytical and real\nexperiments show the parameter estimation accuracy and model validation for\ndifferent nonlinear structural systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20799v1", "categories": ["math.DS", "cs.LG", "cs.SY", "eess.SY"], "cate": "math.DS", "url": "http://arxiv.org/abs/2506.20799v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "结构系统识别通过验证和自适应", "tldr": "本文提出一种基于生成模型框架的新方法，利用神经网络和判别器网络，直接从数据中进行结构系统识别、不确定性量化和验证。", "motivation": "估计控制方程参数值对于整合实验数据与科学理论以理解、验证和预测复杂系统的动力学至关重要。", "method": "该方法受生成建模框架启发，使用神经网络将随机噪声映射到物理有意义的参数。这些参数用于已知运动方程生成“假”加速度，并与真实训练数据通过均方误差损失进行比较。为同时验证学习到的参数，使用独立的验证数据集，通过判别器网络评估生成的加速度，以区分真假并指导参数生成网络。", "result": "分析和实际实验表明，该方法对不同非线性结构系统具有参数估计的准确性和模型验证能力。", "conclusion": "该方法成功地实现了结构系统识别、不确定性量化和模型验证，并表现出良好的准确性。", "translation": "估计控制方程参数值对于将实验数据与科学理论相结合以理解、验证和预测复杂系统的动力学至关重要。在这项工作中，我们提出了一种直接从数据中进行结构系统识别（SI）、不确定性量化和验证的新方法。受生成建模框架的启发，一个神经网络将随机噪声映射到具有物理意义的参数。然后，这些参数用于已知的运动方程中以获得“假”加速度，通过均方误差损失与真实的训练数据进行比较。为了同时验证学习到的参数，我们使用独立的验证数据集。来自这些数据集的生成加速度由一个判别器网络评估，该网络确定输出是真实的还是假的，并指导参数生成网络。分析和实际实验表明，该方法对不同非线性结构系统具有参数估计的准确性和模型验证能力。", "summary": "本文提出了一种新的结构系统识别（SI）、不确定性量化和验证方法。该方法受生成模型启发，利用一个参数生成神经网络将噪声转换为物理参数，并结合已知运动方程生成模拟加速度。这些模拟加速度与真实数据通过均方误差损失进行比较。同时，利用独立的验证数据集和一个判别器网络来验证学习到的参数，判别器区分生成加速度的真伪，从而指导参数生成器。实验证明了该方法在非线性结构系统中的参数估计准确性和模型验证能力。", "keywords": "结构系统识别, 不确定性量化, 生成模型, 神经网络, 模型验证", "comments": "这项工作创新性地将生成对抗网络（GAN）的思想引入到结构系统识别中，通过生成器和判别器网络的协同作用，实现了参数估计和模型验证的同步进行。其优势在于能够直接从数据中进行识别，并同时量化不确定性，对于复杂非线性系统的动力学理解和预测具有重要意义。"}}
{"id": "2506.21123", "title": "Characterization of Rydberg-Atom Signal Reception of Dual-Frequency Signals Coupled with Two Energy Levels", "authors": ["Hao Wu", "Chongwu Xie", "Xinyuan Yao", "Kang-Da Wu", "Shanchi Wu", "Rui Ni", "Guo-Yong Xiang", "Chen Gong"], "summary": "Rydberg atomic sensors have been adopted for novel radio frequency (RF)\nmeasurement technique and the sensing capability for signals in multiple\nfrequencies makes it attractive for multi-user communication. However, unlike\ntraditional antennas where the signals in multiple frequencies are orthogonal,\nthe received signals of atomic sensors corresponding to different energy levels\nwill be downconverted to the baseband simultaneously, resulting in multi-user\ninterference. Thus, in this paper, we analyze the mutual interference\ncharacteristics of two RF signals with different carrier frequencies coupling\ndifferent energy levels. We introduce the joint response coefficient based on\nthe receiver characteristics and analyze the interference of one user to\nanother. We analyze the bit-error rate (BER) and symbol-error rate (SER) for\ntwo signals coupling two different energy levels. We also conduct experiments\nto validate the BER and SER results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21123v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21123v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "偶联两个能级的里德堡原子双频信号接收特性表征", "tldr": "本文研究了里德堡原子传感器接收双频信号时的多用户干扰问题，分析了误码率和误符号率，并通过实验验证。", "motivation": "里德堡原子传感器在多频信号接收方面具有潜力，但其接收到的多频信号会同时下变频到基带，导致多用户干扰。因此，需要分析这种干扰特性。", "method": "本文引入了基于接收机特性的联合响应系数，用于分析用户间的干扰。分析了偶联两个不同能级的两个信号的误码率（BER）和误符号率（SER），并通过实验进行了验证。", "result": "分析了两个不同载波频率的射频信号偶联不同能级时的相互干扰特性，并对误码率（BER）和误符号率（SER）进行了分析。实验结果验证了这些分析。", "conclusion": "论文分析并实验验证了里德堡原子传感器在接收双频信号时，由于信号同时下变频到基带而引起的多用户干扰特性，特别是在误码率和误符号率方面的表现。", "translation": "里德堡原子传感器已被用于新颖的射频(RF)测量技术，其对多频信号的传感能力使其在多用户通信中具有吸引力。然而，与多频信号正交的传统天线不同，原子传感器接收到的对应于不同能级的信号将同时下变频到基带，导致多用户干扰。因此，本文分析了两个不同载波频率的射频信号偶联不同能级时的相互干扰特性。我们引入了基于接收机特性的联合响应系数，并分析了一个用户对另一个用户的干扰。我们分析了偶联两个不同能级的两个信号的误码率（BER）和误符号率（SER）。我们还进行了实验来验证BER和SER结果。", "summary": "本文研究了里德堡原子传感器在接收双频信号时面临的多用户干扰问题。针对不同能级耦合的两个射频信号，论文分析了其相互干扰特性，引入了联合响应系数，并详细分析了误码率（BER）和误符号率（SER）。实验结果验证了理论分析的准确性。", "keywords": "里德堡原子传感器, 双频信号, 多用户干扰, 误码率, 误符号率", "comments": "该论文关注了里德堡原子传感器在多用户通信应用中的一个关键挑战——多用户干扰。通过引入联合响应系数并进行误码率和误符号率分析，结合实验验证，为理解和缓解里德堡原子传感器的干扰问题提供了深入的见解。这对于推动里德堡原子传感器在实际通信系统中的应用具有重要意义。"}}
{"id": "2506.21535", "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation", "authors": ["Mohammed Baharoon", "Jun Ma", "Congyu Fang", "Augustin Toma", "Bo Wang"], "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21535v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.21535v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "探索用于CT报告生成的3D多模态大语言模型设计空间", "tldr": "本研究系统探索了3D多模态大语言模型（MLLMs）在CT报告生成中的设计空间，包括视觉输入、投影器、LLMs和微调技术。引入了两种知识增强方法，使GREEN分数提升高达10%，并在MICCAI 2024 AMOS-MM挑战中获得第二名。研究发现，放射报告生成与LLM大小关联不大，体积大小的提升并非总能带来性能提升，而结合分割掩码能提高性能。", "motivation": "多模态大语言模型（MLLMs）已成为自动化放射报告生成（RRG）的一种有前景的方法。本研究旨在系统地调查3D MLLM的设计空间，以改进CT报告生成。", "method": "系统地研究了3D MLLM的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术。此外，引入了两种基于知识的报告增强方法。", "result": "在GREEN分数上性能提升高达10%，并在MICCAI 2024 AMOS-MM挑战中获得第二名。在1,687个AMOS-MM数据集案例上的结果表明，在相同训练协议下，RRG在很大程度上独立于LLM的大小。如果原始ViT在较小体积上预训练，则更大的体积大小不总是能提高性能。使用分割掩码与CT体积一起可以提高性能。", "conclusion": "3D MLLMs的设计空间对CT报告生成具有重要影响。研究发现，在相同的训练协议下，放射报告生成与LLM的大小关联不大，体积大小的提升并非总能带来性能提升，而结合分割掩码可以有效提高性能。", "translation": "多模态大语言模型（MLLMs）已成为自动化放射报告生成（RRG）的一种有前景的方法。在这项工作中，我们系统地研究了3D MLLM的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和用于3D CT报告生成的微调技术。我们还引入了两种基于知识的报告增强方法，将GREEN分数性能提高了高达10％，并在MICCAI 2024 AMOS-MM挑战中获得了第二名。我们在AMOS-MM数据集的1,687个病例上的结果表明，在相同的训练协议下，RRG在很大程度上独立于LLM的大小。我们还表明，如果原始ViT在较小体积大小上进行预训练，则较大的体积大小并不总是能提高性能。最后，我们表明使用分割掩码与CT体积一起可以提高性能。代码已公开在 https://github.com/bowang-lab/AMOS-MM-Solution。", "summary": "本文系统地探索了用于CT报告生成的3D多模态大语言模型（MLLMs）的设计空间，涵盖视觉输入、投影器、LLM和微调技术。研究引入了两种知识增强的报告扩充方法，显著提升了性能，在MICCAI 2024 AMOS-MM挑战中获得第二名。研究发现，在相同训练协议下，放射报告生成与LLM大小关联不大，且更大的体积不总是提高性能，而结合分割掩码能有效提升性能。", "keywords": "3D MLLMs, CT报告生成, 放射报告生成, 设计空间, 知识增强", "comments": "本文对3D MLLM在CT报告生成中的设计空间进行了系统性探索，特别关注了输入表示、模型选择和微调策略。其创新点在于引入知识增强方法，并取得了显著的竞赛成绩。研究结果揭示了LLM大小并非唯一决定因素，以及体积大小与预训练匹配的重要性，并证明了分割掩码的有效性，为未来的放射报告生成研究提供了有价值的见解。"}}
{"id": "2506.21393", "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding", "authors": ["Junwen Zhang", "Pu Chen", "Yin Zhang"], "summary": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.", "comment": "43 pages and 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.21393v1", "categories": ["cs.AI", "68T07 (Primary), 68T50, 68T30, 68T45 (Secondary)", "F.2.2; I.2.7; I.2.10"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21393v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TableMoE：多模态表格理解中结构化专家推理的神经-符号路由", "tldr": "TableMoE通过神经-符号路由和专家系统，显著提升了在复杂真实世界条件下对多模态表格的理解能力。", "motivation": "现有多模态大语言模型（MLLMs）在处理复杂结构、符号密度高、视觉降级（如模糊、倾斜、水印、不完整结构或字体、多跨度或分层嵌套布局）的真实世界表格（WildStruct条件）时表现不佳，导致性能受限且泛化能力差。", "method": "提出TableMoE，一种神经-符号混合连接专家（MoCE）架构，其核心是创新的神经-符号路由机制，能够预测潜在语义令牌角色并利用符号推理图指导的置信度感知门控策略，将表格元素动态路由到专门的专家（Table-to-HTML、Table-to-JSON、Table-to-Code）。为促进预训练，引入了大规模TableMoE-Align数据集（包含1.2M表格-HTML-JSON-代码四元组）。为评估模型，策划分发了四个挑战性的WildStruct基准：WMMFinQA、WMMTatQA、WMMTabDialog和WMMFinanceMath。", "result": "TableMoE显著超越了现有最先进的模型。广泛的消融研究验证了每个核心组件，强调了神经-符号路由和结构化专家对齐的关键作用。定性分析进一步展示了TableMoE的可解释性和增强的鲁棒性。", "conclusion": "整合神经-符号推理对于多模态表格理解非常有效。", "translation": "在现实世界语境中对表格进行多模态理解具有挑战性，因为其结构复杂性、符号密度和视觉降级（模糊、倾斜、水印、不完整的结构或字体、多跨度或分层嵌套布局）。现有的多模态大型语言模型（MLLMs）在这些“野外结构”（WildStruct）条件下表现不佳，导致性能有限和泛化能力差。为了解决这些挑战，我们提出了TableMoE，一种神经-符号混合连接专家（MoCE）架构，专门用于对多模态表格数据进行鲁棒、结构化推理。TableMoE具有创新的神经-符号路由机制，该机制预测潜在的语义令牌角色（例如，标题、数据单元格、轴、公式），并利用由符号推理图引导的置信度感知门控策略，将表格元素动态路由到专门的专家（表格到HTML、表格到JSON、表格到代码）。为了促进有效的对齐驱动预训练，我们引入了大规模的TableMoE-Align数据集，该数据集包含来自金融、科学、生物医学和工业领域的120万个表格-HTML-JSON-代码四元组，专门用于模型预训练。为了评估，我们策划并发布了四个具有挑战性的WildStruct基准：WMMFinQA、WMMTatQA、WMMTabDialog和WMMFinanceMath，这些基准专门设计用于在真实世界多模态降级和结构复杂性下对模型进行压力测试。实验结果表明，TableMoE显著超越了现有最先进的模型。广泛的消融研究验证了每个核心组件，强调了神经-符号路由和结构化专家对齐的关键作用。通过定性分析，我们进一步展示了TableMoE的可解释性和增强的鲁棒性，强调了整合神经-符号推理对于多模态表格理解的有效性。", "summary": "本论文提出了TableMoE，一种神经-符号混合连接专家（MoCE）架构，旨在解决现有MLLM在复杂真实世界表格（WildStruct条件）多模态理解中的挑战。TableMoE采用创新的神经-符号路由机制，动态地将表格元素路由到专门的专家，并通过大规模TableMoE-Align数据集进行预训练。在四个新策展的WildStruct基准测试中，TableMoE显著超越了现有SOTA模型，并通过消融研究验证了其核心组件的有效性，尤其强调了神经-符号路由在提升表格理解鲁棒性和可解释性方面的关键作用。", "keywords": "多模态表格理解, 神经-符号路由, 专家系统, TableMoE, WildStruct", "comments": "这篇论文的创新点在于其神经-符号路由机制，它将符号推理与神经网络相结合，有效地处理了多模态表格数据固有的结构复杂性和视觉降级问题。通过动态路由到专门的专家，TableMoE展现了对真实世界复杂表格更强的适应性和泛化能力。此外，新发布的大规模数据集和挑战性基准测试对于推动该领域的研究也具有重要意义。"}}
{"id": "2506.21418", "title": "Vantage Point Selection Algorithms for Bottleneck Capacity Estimation", "authors": ["Vikrant Ashvinkumar", "Rezaul Chowdhury", "Jie Gao", "Mayank Goswami", "Joseph S. B. Mitchell", "Valentin Polishchuk"], "summary": "Motivated by the problem of estimating bottleneck capacities on the Internet,\nwe formulate and study the problem of vantage point selection. We are given a\ngraph $G=(V, E)$ whose edges $E$ have unknown capacity values that are to be\ndiscovered. Probes from a vantage point, i.e, a vertex $v \\in V$, along\nshortest paths from $v$ to all other vertices, reveal bottleneck edge\ncapacities along each path. Our goal is to select $k$ vantage points from $V$\nthat reveal the maximum number of bottleneck edge capacities.\n  We consider both a non-adaptive setting where all $k$ vantage points are\nselected before any bottleneck capacity is revealed, and an adaptive setting\nwhere each vantage point selection instantly reveals bottleneck capacities\nalong all shortest paths starting from that point. In the non-adaptive setting,\nby considering a relaxed model where edge capacities are drawn from a random\npermutation (which still leaves the problem of maximizing the expected number\nof revealed edges NP-hard), we are able to give a $1-1/e$ approximate\nalgorithm. In the adaptive setting we work with the least permissive model\nwhere edge capacities are arbitrarily fixed but unknown. We compare with the\nbest solution for the particular input instance (i.e. by enumerating all\nchoices of $k$ tuples), and provide both lower bounds on instance optimal\napproximation algorithms and upper bounds for trees and planar graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21418v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21418v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "瓶颈容量估计的观测点选择算法", "tldr": "本文研究了在给定图上选择k个观测点以揭示最多瓶颈边缘容量的问题，提出了非自适应和自适应设置下的近似算法和界限。", "motivation": "估计互联网上的瓶颈容量。", "method": "将瓶颈容量估计问题形式化为图上的观测点选择问题，目标是选择k个观测点以最大化揭示的瓶颈边缘容量。研究了非自适应和自适应两种设置。在非自适应设置中，采用边缘容量来自随机排列的松弛模型，并设计了近似算法。在自适应设置中，使用边缘容量任意固定但未知的模型，并提供了近似算法的上下界分析。", "result": "在非自适应设置中，尽管问题在边缘容量来自随机排列的松弛模型下仍是NP-难的，但本文提供了一个1-1/e近似算法。在自适应设置中，论文给出了实例最优近似算法的下界以及针对树和平面图的特定上界。", "conclusion": "Not mentioned in abstract", "translation": "受互联网瓶颈容量估计问题的启发，我们提出并研究了观测点选择问题。我们给定一个图G=(V, E)，其边E的容量值未知，需要被发现。从一个观测点，即一个顶点v ∈ V，沿着从v到所有其他顶点的最短路径进行的探测，将揭示每条路径上的瓶颈边缘容量。我们的目标是从V中选择k个观测点，以揭示最大数量的瓶颈边缘容量。\n我们考虑了两种设置：非自适应设置，其中所有k个观测点在任何瓶颈容量被揭示之前就被选择；以及自适应设置，其中每个观测点选择都会立即揭示从该点开始的所有最短路径上的瓶颈容量。在非自适应设置中，通过考虑一个松弛模型，其中边缘容量从随机排列中抽取（这仍然使得最大化预期揭示边缘数量的问题是NP-难的），我们能够提供一个1-1/e近似算法。在自适应设置中，我们使用最不宽松的模型，其中边缘容量是任意固定但未知的。我们与特定输入实例的最佳解决方案（即通过枚举所有k个元组的选择）进行比较，并提供了实例最优近似算法的下界以及树和平面图的上界。", "summary": "本文研究了在图上选择k个最优观测点以最大化发现瓶颈边缘容量的问题，该问题源于互联网瓶颈容量估计。论文区分了非自适应和自适应两种设置。在非自适应设置中，尽管问题是NP-难的，但提出了一种1-1/e近似算法。在自适应设置中，论文为任意固定但未知容量的模型提供了实例最优近似算法的下界，以及针对特定图结构（如树和平面图）的上界。", "keywords": "瓶颈容量估计, 观测点选择, 近似算法, 图论, 网络测量", "comments": "这篇论文解决了互联网瓶颈容量估计中的一个核心挑战，即如何高效选择观测点以获取最多的信息。其创新点在于将问题形式化为图论中的观测点选择，并区分了非自适应和自适应两种现实场景。论文不仅证明了问题的计算复杂性（NP-难），还在不同设置下提出了理论上具有保证的近似算法和界限，这对于理解和解决大规模网络测量问题具有重要意义。"}}
{"id": "2506.21206", "title": "Robust and efficient pre-processing techniques for particle-based methods including dynamic boundary generation", "authors": ["Niklas S. Neher", "Erik Faulhaber", "Sven Berger", "Christian Weißenfels", "Gregor J. Gassner", "Michael Schlottke-Lakemper"], "summary": "Obtaining high-quality particle distributions for stable and accurate\nparticle-based simulations poses significant challenges, especially for complex\ngeometries. We introduce a preprocessing technique for 2D and 3D geometries,\noptimized for smoothed particle hydrodynamics (SPH) and other particle-based\nmethods. Our pipeline begins with the generation of a resolution-adaptive point\ncloud near the geometry's surface employing a face-based neighborhood search.\nThis point cloud forms the basis for a signed distance field, enabling\nefficient, localized computations near surface regions. To create an initial\nparticle configuration, we apply a hierarchical winding number method for fast\nand accurate inside-outside segmentation. Particle positions are then relaxed\nusing an SPH-inspired scheme, which also serves to pack boundary particles.\nThis ensures full kernel support and promotes isotropic distributions while\npreserving the geometry interface. By leveraging the meshless nature of\nparticle-based methods, our approach does not require connectivity information\nand is thus straightforward to integrate into existing particle-based\nframeworks. It is robust to imperfect input geometries and memory-efficient\nwithout compromising performance. Moreover, our experiments demonstrate that\nwith increasingly higher resolution, the resulting particle distribution\nconverges to the exact geometry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21206v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21206v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "粒子方法中鲁棒高效的预处理技术，包括动态边界生成", "tldr": "本文提出了一种针对2D和3D粒子方法（如SPH）的鲁棒高效的预处理流程，用于生成高质量、分辨率自适应的粒子分布，即使对于复杂或不完美的几何形状也能适用。", "motivation": "获取用于稳定和精确的基于粒子的模拟的高质量粒子分布面临重大挑战，尤其是在处理复杂几何形状时。", "method": "该预处理流程包括：首先，通过基于面的邻域搜索在几何体表面附近生成分辨率自适应点云；接着，利用该点云构建符号距离场以进行局部计算；然后，采用分层缠绕数方法进行快速准确的内外分割，以创建初始粒子配置；最后，使用受SPH启发的方案松弛粒子位置并填充边界粒子，以确保完整的核支持和各向同性分布。", "result": "该方法不需要连接信息，易于集成到现有框架中，对不完美的输入几何形状具有鲁棒性，且内存高效而不影响性能。实验表明，随着分辨率的提高，所得粒子分布会收敛于精确的几何形状。", "conclusion": "该论文提出的预处理技术有效地解决了粒子模拟中高质量粒子分布生成的挑战，为复杂几何形状的表示提供了鲁棒、高效和准确的解决方案。", "translation": "获取用于稳定和精确的基于粒子的模拟的高质量粒子分布带来了重大挑战，特别是对于复杂几何形状。我们介绍了一种针对2D和3D几何形状的预处理技术，该技术针对平滑粒子流体动力学（SPH）和其他基于粒子的方法进行了优化。我们的流程首先通过基于面的邻域搜索在几何体表面附近生成一个分辨率自适应的点云。该点云构成了符号距离场的基础，从而能够在表面区域附近进行高效的局部计算。为了创建初始粒子配置，我们采用了一种分层缠绕数方法进行快速准确的内外分割。然后，使用SPH启发式方案对粒子位置进行松弛，该方案也用于填充边界粒子。这确保了完整的核支持并促进了各向同性分布，同时保留了几何界面。通过利用基于粒子方法的无网格特性，我们的方法不需要连接信息，因此易于集成到现有的基于粒子框架中。它对不完美的输入几何形状具有鲁棒性，并且内存高效，同时不影响性能。此外，我们的实验表明，随着分辨率的不断提高，所得粒子分布收敛于精确的几何形状。", "summary": "本文提出了一种鲁棒高效的预处理流程，用于为2D和3D粒子方法（如SPH）生成高质量的粒子分布，尤其适用于复杂几何形状。该方法通过生成分辨率自适应点云、构建符号距离场、利用分层缠绕数法进行分割，并结合SPH启发式方案进行粒子松弛和边界粒子填充。该方法无需网格连接信息，对不完美输入具有鲁棒性，内存高效，并能确保几何形状的精确表示，在高分辨率下收敛于精确几何。", "keywords": "粒子方法, 预处理, 粒子分布, SPH, 复杂几何", "comments": "该论文通过解决高质量粒子分布生成这一关键挑战，对粒子模拟领域做出了重要贡献，尤其是在处理复杂和不完美几何形状方面。其无网格特性和鲁棒性是关键创新点，使其非常适合集成到现有框架中。利用分辨率自适应点云和SPH启发式松弛来填充边界粒子的方法尤为巧妙，确保了效率和准确性。"}}
{"id": "2506.20923", "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "pdf_url": "http://arxiv.org/pdf/2506.20923v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20923v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "KaLM-Embedding-V2：卓越的训练技术和数据启发了多功能嵌入模型", "tldr": "KaLM-Embedding-V2是一个紧凑的多功能文本嵌入模型，通过优化架构、多阶段训练、数据增强和难例挖掘，在MTEB中表现出色，超越同类模型并媲美大型模型。", "motivation": "旨在通过利用卓越的训练技术和数据，开发一个在通用文本嵌入任务中表现出色的多功能且紧凑的嵌入模型。", "method": "架构改进：移除因果注意力掩码，采用全双向Transformer和简单的平均池化生成固定长度嵌入。多阶段训练：在大型弱监督开源语料库上进行预训练；在高质量检索和非检索数据集上进行微调；使用模型融合参数平均（model-soup parameter averaging）以实现稳健泛化。训练策略：引入焦点式重加权机制以关注困难样本，以及在线硬负例混合策略以持续丰富硬负例。数据：收集超过20类数据用于预训练，100类数据用于微调。", "result": "在MTEB中文和英文评估中，KaLM-Embedding-V2显著优于同等大小的模型，并能与大3倍、14倍、18倍和26倍的嵌入模型竞争，为小于10亿参数的多功能紧凑嵌入模型树立了新标准。", "conclusion": "KaLM-Embedding-V2通过创新的训练技术、数据策略和模型架构，成为一个高性能、紧凑且通用的文本嵌入模型，在MTEB基准上表现卓越，设立了新的行业标准。", "translation": "在本文中，我们提出了KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，它通过利用卓越的训练技术和数据，在通用文本嵌入任务中取得了令人印象深刻的性能。我们的主要创新包括：(1) 为了更好地使架构与表示学习对齐，我们移除了因果注意力掩码，并采用了一个带有简单而有效的平均池化的全双向Transformer来生成固定长度嵌入；(2) 我们采用了多阶段训练流程：(i) 在大规模弱监督开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；(iii) 使用模型融合参数平均（model-soup parameter averaging）以实现稳健泛化。此外，我们引入了一种焦点式重加权机制，将学习集中在困难样本上，以及一种在线硬负例混合策略，以在不进行昂贵离线挖掘的情况下持续丰富硬负例；(3) 我们收集了超过20类数据用于预训练，100类数据用于微调，以提高嵌入模型的性能和泛化能力。在MTEB（大规模文本嵌入基准）中文和英文上的广泛评估表明，我们的模型显著优于其他同等大小的模型，并能与大3倍、14倍、18倍和26倍的嵌入模型竞争，为小于10亿参数的多功能紧凑嵌入模型树立了新标准。", "summary": "本文提出了KaLM-Embedding-V2，一个紧凑且多功能的文本嵌入模型。该模型通过改进架构（全双向Transformer和平均池化）、多阶段训练流程（预训练、微调、模型融合）、创新的训练策略（焦点式重加权、在线硬负例混合）以及丰富的数据集（20+类预训练，100+类微调）实现了卓越性能。在MTEB中英文基准测试中，KaLM-Embedding-V2表现出显著优势，不仅超越同等规模模型，更能与参数量大数倍的模型相媲美，为小于10亿参数的通用嵌入模型设定了新标准。", "keywords": "文本嵌入, KaLM-Embedding-V2, 多阶段训练, 硬负例挖掘, 模型融合", "comments": "该论文的创新点在于结合了多项先进技术以优化文本嵌入模型：通过移除因果注意力掩码并采用全双向Transformer来更好地适应表示学习；引入了高效的多阶段训练流程，包括大规模预训练和高质量微调，并结合模型融合增强泛化能力；此外，焦点式重加权和在线硬负例挖掘策略有效提升了模型处理困难样本的能力。数据多样性和规模也是其成功的关键。其重要性在于证明了即使是紧凑型模型，通过卓越的训练技术和数据也能达到甚至超越大型模型的性能，这对于资源受限的应用场景具有重要意义。"}}
{"id": "2506.20816", "title": "Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers", "authors": ["Furkan Mumcu", "Yasin Yilmaz"], "summary": "Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input\ndesigns with limited noise budgets. While numerous successful attacks with\nsubtle modifications to original input have been proposed, defense techniques\nagainst these attacks are relatively understudied. Existing defense approaches\neither focus on improving DNN robustness by negating the effects of\nperturbations or use a secondary model to detect adversarial data. Although\nequally important, the attack detection approach, which is studied in this\nwork, provides a more practical defense compared to the robustness approach. We\nshow that the existing detection methods are either ineffective against the\nstate-of-the-art attack techniques or computationally inefficient for real-time\nprocessing. We propose a novel universal and efficient method to detect\nadversarial examples by analyzing the varying degrees of impact of attacks on\ndifferent DNN layers. {Our method trains a lightweight regression model that\npredicts deeper-layer features from early-layer features, and uses the\nprediction error to detect adversarial samples.} Through theoretical arguments\nand extensive experiments, we demonstrate that our detection method is highly\neffective, computationally efficient for real-time processing, compatible with\nany DNN architecture, and applicable across different domains, such as image,\nvideo, and audio.", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.17442", "pdf_url": "http://arxiv.org/pdf/2506.20816v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20816v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "通过网络层非均匀影响的通用高效对抗数据检测", "tldr": "提出了一种通用高效的对抗样本检测方法，通过分析对抗攻击对深度神经网络（DNN）不同层产生的非均匀影响，训练轻量级回归模型并利用预测误差进行检测。", "motivation": "深度神经网络（DNN）极易受到对抗性输入的攻击，现有防御技术（如提高鲁棒性或使用次级模型检测）存在局限性。尤其，现有检测方法对于先进攻击无效或计算效率低下，无法满足实时处理需求，因此需要一种通用且高效的对抗样本检测方法。", "method": "提出了一种新颖的通用高效方法来检测对抗样本，通过分析对抗攻击对不同DNN层产生的不同程度影响。具体而言，该方法训练一个轻量级回归模型，该模型利用早期层特征预测更深层特征，并使用预测误差来检测对抗样本。", "result": "所提出的检测方法具有高效性、计算效率高（适用于实时处理）、兼容任何DNN架构，并且可应用于图像、视频和音频等不同领域。", "conclusion": "通过分析对抗攻击对DNN层产生的非均匀影响，本文提出了一种通用且高效的对抗样本检测方法，并在理论和大量实验中证明了其有效性、计算效率和广泛适用性。", "translation": "深度神经网络（DNN）以其对具有有限噪声预算的对抗性输入设计的高度脆弱性而闻名。虽然已经提出了许多成功地对原始输入进行细微修改的攻击，但针对这些攻击的防御技术相对研究不足。现有的防御方法要么侧重于通过抵消扰动的影响来提高DNN的鲁棒性，要么使用辅助模型来检测对抗性数据。尽管同样重要，但本研究中探讨的攻击检测方法与鲁棒性方法相比，提供了一种更实用的防御。我们表明，现有检测方法要么对最先进的攻击技术无效，要么在实时处理方面计算效率低下。我们提出了一种新颖的通用高效方法，通过分析攻击对不同DNN层产生的不同程度影响来检测对抗样本。我们的方法训练一个轻量级回归模型，该模型从早期层特征预测更深层特征，并使用预测误差来检测对抗样本。通过理论论证和大量实验，我们证明了我们的检测方法高效、计算效率高（适用于实时处理）、兼容任何DNN架构，并且可应用于图像、视频和音频等不同领域。", "summary": "本文提出了一种通用且高效的对抗样本检测方法，旨在解决深度神经网络（DNN）对对抗性攻击的脆弱性以及现有防御方法（特别是检测方法）的局限性。该方法的核心思想是分析对抗攻击对DNN不同层产生的非均匀影响，并通过训练一个轻量级回归模型来实现：该模型利用早期层特征预测更深层特征，并依据预测误差来识别对抗样本。研究表明，该方法在实时处理中表现出卓越的有效性和计算效率，同时兼容各种DNN架构，并能广泛应用于图像、视频和音频等多种数据领域。", "keywords": "对抗样本检测, 深度神经网络, 层影响分析, 轻量级回归模型, 实时处理", "comments": "该论文提出了一种新颖的对抗样本检测思路，即利用对抗攻击对网络层影响的非均匀性进行检测，而非传统地改进模型鲁棒性或依赖复杂的辅助模型。其创新性在于通过预测层间特征并利用预测误差进行检测，实现了通用性和高效率，这对于对抗样本检测在实际应用中的部署具有重要意义。该方法的轻量级设计也使其具有较高的实用价值。"}}
{"id": "2506.21057", "title": "Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions", "authors": ["Zhuochen Miao", "Jun Lv", "Hongjie Fang", "Yang Jin", "Cewu Lu"], "summary": "Imitation learning has emerged as a powerful paradigm in robot manipulation,\nyet its generalization capability remains constrained by object-specific\ndependencies in limited expert demonstrations. To address this challenge, we\npropose knowledge-driven imitation learning, a framework that leverages\nexternal structural semantic knowledge to abstract object representations\nwithin the same category. We introduce a novel semantic keypoint graph as a\nknowledge template and develop a coarse-to-fine template-matching algorithm\nthat optimizes both structural consistency and semantic similarity. Evaluated\non three real-world robotic manipulation tasks, our method achieves superior\nperformance, surpassing image-based diffusion policies with only one-quarter of\nthe expert demonstrations. Extensive experiments further demonstrate its\nrobustness across novel objects, backgrounds, and lighting conditions. This\nwork pioneers a knowledge-driven approach to data-efficient robotic learning in\nreal-world settings. Code and more materials are available on\nhttps://knowledge-driven.github.io/.", "comment": "IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.21057v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21057v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "知识驱动的模仿学习：实现跨多样化条件的泛化", "tldr": "本文提出了一种知识驱动的模仿学习框架，利用外部结构化语义知识来抽象物体表示，以提高机器人操作在不同条件下的泛化能力和数据效率。", "motivation": "现有的模仿学习在机器人操作中受限于有限专家演示中物体特定的依赖性，导致泛化能力不足。", "method": "提出了一种知识驱动的模仿学习框架，利用外部结构化语义知识来抽象同一类别内的物体表示。引入了新的语义关键点图作为知识模板，并开发了一种粗到细的模板匹配算法，优化结构一致性和语义相似性。", "result": "在三个真实世界机器人操作任务中，该方法表现出卓越的性能，仅用四分之一的专家演示就超越了基于图像的扩散策略。实验进一步证明了其在新型物体、背景和光照条件下的鲁棒性。", "conclusion": "本文开创了一种知识驱动的方法，用于在真实世界环境中进行数据高效的机器人学习。", "translation": "模仿学习已成为机器人操作中一种强大的范式，但其泛化能力仍然受限于有限专家演示中物体特定的依赖性。为了解决这一挑战，我们提出了一种知识驱动的模仿学习框架，该框架利用外部结构化语义知识来抽象同一类别内的物体表示。我们引入了一种新颖的语义关键点图作为知识模板，并开发了一种粗到细的模板匹配算法，该算法同时优化了结构一致性和语义相似性。在三个真实世界机器人操作任务中进行评估，我们的方法取得了卓越的性能，仅用四分之一的专家演示就超越了基于图像的扩散策略。广泛的实验进一步证明了其在新型物体、背景和光照条件下的鲁棒性。这项工作开创了一种知识驱动的方法，用于在真实世界环境中进行数据高效的机器人学习。代码和更多材料可在 https://knowledge-driven.github.io/ 上获取。", "summary": "本文提出了一种名为“知识驱动的模仿学习”的新框架，旨在解决传统模仿学习在机器人操作中泛化能力受限的问题。该方法通过利用外部结构化语义知识和引入语义关键点图作为知识模板，实现对物体表示的抽象化。通过开发一种粗到细的模板匹配算法，该框架在真实世界机器人任务中取得了优异的性能，显著减少了所需的专家演示数量，并展现了对新型物体、背景和光照条件的强大鲁棒性，从而实现了数据高效的机器人学习。", "keywords": "模仿学习, 机器人操作, 知识驱动, 泛化, 语义关键点图", "comments": "这项工作通过引入外部语义知识和语义关键点图，创新性地解决了模仿学习中泛化能力和数据效率的挑战。其在真实世界任务中用更少数据超越现有方法的表现，凸显了知识驱动方法在机器人学习领域的巨大潜力。该方法对提高机器人学习的实用性和可扩展性具有重要意义。"}}
{"id": "2506.21297", "title": "Exploring Micro Frontends: A Case Study Application in E-Commerce", "authors": ["Ricardo Hideki Hangai Kojo", "Luiz Fernando Corte Real", "Renato Cordeiro Ferreira", "Thatiane de Oliveira Rosa", "Alfredo Goldman"], "summary": "In the micro frontends architectural style, the frontend is divided into\nsmaller components, which can range from a simple button to an entire page. The\ngoal is to improve scalability, resilience, and team independence, albeit at\nthe cost of increased complexity and infrastructure demands. This paper seeks\nto understand when it is worth adopting micro frontends, particularly in the\ncontext of industry. To achieve this, we conducted an investigation into the\nstate of the art of micro frontends, based on both academic and gray\nliterature. We then implemented this architectural style in a marketplace for\nhandcrafted products, which already used microservices. Finally, we evaluated\nthe implementation through a semi-open questionnaire with the developers. At\nthe studied marketplace company, the need for architectural change arose due to\nthe tight coupling between their main system (a Java monolith) and a dedicated\nfrontend system. Additionally, there were deprecated technologies and poor\ndeveloper experience. To address these issues, the micro frontends architecture\nwas adopted, along with the API Gateway and Backend for Frontend patterns, and\ntechnologies such as Svelte and Fastify. Although the adoption of Micro\nFrontends was successful, it was not strictly necessary to meet the company's\nneeds. According to the analysis of the mixed questionnaire responses, other\nalternatives, such as a monolithic frontend, could have achieved comparable\nresults. What made adopting micro frontends the most convenient choice in the\ncompany's context was the monolith strangulation and microservices adoption,\nwhich facilitated implementation through infrastructure reuse and knowledge\nsharing between teams.", "comment": "11 pages, 2 figures (2 diagrams), submitted to the workshop AMP 2025", "pdf_url": "http://arxiv.org/pdf/2506.21297v1", "categories": ["cs.SE", "cs.DC", "D.2.11; D.2.13; D.2.7"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21297v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "探索微前端：在电子商务中的案例研究应用", "tldr": "本文通过案例研究探讨了微前端在电子商务领域的应用，发现其成功采用并非总是必需，但在已采用微服务和单体应用绞杀模式的背景下最为便利。", "motivation": "本文旨在理解何时值得采用微前端架构，尤其是在工业背景下。具体来说，研究的市场公司面临主系统与前端系统紧密耦合、技术过时以及开发者体验不佳的问题，因此需要进行架构变革。", "method": "研究方法包括对微前端的最新技术进行调查（基于学术和灰色文献），然后在一个已使用微服务的手工产品市场中实施微前端架构，最后通过对开发人员进行半开放式问卷调查来评估实施效果。", "result": "微前端的采用是成功的，但并非严格必要，其他替代方案（如单体前端）也能达到类似效果。在公司背景下，采用微前端最方便的选择是因为单体应用绞杀和微服务已采用，这通过基础设施重用和团队间知识共享促进了实施。", "conclusion": "微前端架构可以成功实施，但其必要性取决于具体的公司背景和现有架构。在已存在微服务和单体应用绞杀模式的情况下，微前端的采用会因基础设施复用和知识共享而变得更加便利和合理。", "translation": "在微前端架构风格中，前端被划分为更小的组件，范围可以从一个简单的按钮到整个页面。其目标是提高可伸缩性、弹性和团队独立性，尽管代价是增加了复杂性和基础设施需求。本文旨在理解何时值得采用微前端，尤其是在工业背景下。为了实现这一目标，我们对微前端的最新技术进行了调查，调查基于学术和灰色文献。然后，我们在一个已使用微服务的手工产品市场中实施了这种架构风格。最后，我们通过对开发人员进行半开放式问卷调查来评估了实施效果。在所研究的市场公司中，由于其主系统（一个Java单体应用）与专用前端系统之间的紧密耦合，以及存在废弃技术和糟糕的开发者体验，因此出现了架构变更的需求。为了解决这些问题，采用了微前端架构，以及API网关和后端即前端模式，并使用了Svelte和Fastify等技术。尽管微前端的采用是成功的，但它并非严格必要来满足公司的需求。根据对混合问卷答复的分析，其他替代方案，如单体前端，可能也能达到类似的结果。在公司背景下，使采用微前端成为最便利选择的原因是单体应用绞杀和微服务采用，这通过基础设施重用和团队间知识共享促进了实施。", "summary": "本研究探讨了微前端架构在电子商务领域的应用价值。通过对现有文献的调查以及在一个已采用微服务的手工产品市场进行案例实施和评估，发现微前端的成功采用并非总是满足公司需求的严格必要条件，其他方案也能达到相似效果。然而，在已有单体应用绞杀和微服务部署的背景下，微前端因基础设施复用和团队知识共享而成为最便利且有效的选择。", "keywords": "微前端, 电子商务, 案例研究, 微服务, 架构风格", "comments": "该论文通过一个具体的案例研究，深入探讨了微前端架构的实际应用和其在特定情境下的适用性。其创新之处在于不仅关注了微前端的优点，也坦诚地指出了其并非总是最佳或必需的方案，并强调了现有架构（如微服务和单体应用绞杀）对微前端实施便利性的影响。这为企业在考虑采用微前端时提供了宝贵的实践经验和决策依据，避免了盲目追随技术潮流。"}}
{"id": "2506.21436", "title": "Succinct Preferential Attachment Graphs", "authors": ["Ziad Ismaili Alaoui", "Namrata", "Sebastian Wild"], "summary": "Computing over compressed data combines the space saving of data compression\nwith efficient support for queries directly on the compressed representation.\nSuch data structures are widely applied in text indexing and have been\nsuccessfully generalised to trees. For graphs, support for computing over\ncompressed data remains patchy; typical results in the area of succinct data\nstructures are restricted to a specific class of graphs and use the same,\nworst-case amount of space for any graph from this class.\n  In this work, we design a data structure whose space usage automatically\nimproves with the compressibility of the graph at hand, while efficiently\nsupporting navigational operations (simulating adjacency-list access).\nSpecifically, we show that the space usage approaches the instance-optimal\nspace when the graph is drawn according to the classic Barab\\'asi-Albert model\nof preferential-attachment graphs. Our data-structure techniques also work for\narbitrary graphs, guaranteeing a size asymptotically no larger than an\nentropy-compressed edge list. A key technical contribution is the careful\nanalysis of the instance-optimal space usage.", "comment": "WG 2025", "pdf_url": "http://arxiv.org/pdf/2506.21436v1", "categories": ["cs.DS", "cs.IT", "math.IT", "math.PR"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.21436v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "简洁的优先附着图", "tldr": "本文设计了一种新的数据结构，用于压缩图数据，其空间使用量能根据图的可压缩性自动优化，尤其适用于优先附着图，并支持高效的导航操作。", "motivation": "现有的图压缩数据结构通常局限于特定类型的图，并且空间使用量是固定的最坏情况，无法根据图的可压缩性进行优化。这限制了在压缩数据上进行高效图计算的应用。", "method": "本文设计了一种新的数据结构，其空间使用量能够随着图的可压缩性自动改进。该数据结构能够高效支持导航操作（模拟邻接表访问）。关键技术贡献在于对实例最优空间使用量的仔细分析。", "result": "设计的数据结构在图是根据经典的Barabási-Albert优先附着图模型绘制时，其空间使用量接近实例最优空间。对于任意图，该数据结构也能保证其大小渐近不大于熵压缩的边列表。", "conclusion": "本文提出了一种新的数据结构，它能有效地在压缩图数据上进行计算，其空间效率能根据图的可压缩性自适应优化，并对优先附着图表现出实例最优的性能。", "translation": "在压缩数据上进行计算结合了数据压缩的空间节省和直接在压缩表示上高效支持查询的优点。这类数据结构广泛应用于文本索引，并已成功推广到树。对于图，在压缩数据上进行计算的支持仍然不完善；简洁数据结构领域中的典型结果局限于特定类别的图，并且对该类别中的任何图都使用相同的、最坏情况的空间量。\n在这项工作中，我们设计了一种数据结构，其空间使用量能随着手头图的可压缩性自动改进，同时高效支持导航操作（模拟邻接表访问）。具体来说，我们表明当图根据经典的Barabási-Albert优先附着图模型绘制时，其空间使用量接近实例最优空间。我们的数据结构技术也适用于任意图，保证其大小渐近不大于熵压缩的边列表。一个关键的技术贡献是对实例最优空间使用量的仔细分析。", "summary": "本文提出了一种新的压缩图数据结构，旨在解决现有图压缩数据结构在空间效率和普适性方面的不足。该数据结构能够根据图的可压缩性自动调整空间占用，从而实现更优的存储效率，尤其在处理Barabási-Albert优先附着图时，其空间使用量接近理论最优。此外，它还能高效支持图的导航操作，并且适用于任意图，其空间复杂度不大于熵压缩的边列表。这项工作的主要贡献在于其自适应空间效率和对实例最优空间使用的深入分析。", "keywords": "压缩图, 优先附着图, 简洁数据结构, 空间效率, Barabási-Albert模型", "comments": "这项工作的创新之处在于其提出的数据结构能够自适应地根据图的实际可压缩性来优化空间使用，而非采用固定的最坏情况空间。这对于处理大规模且结构多样的图数据具有重要意义，尤其是在需要高效查询和导航操作的场景下。对优先附着图的实例最优性能证明了其在特定图模型下的优越性。"}}
{"id": "2506.20877", "title": "THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion", "authors": ["Calin Teodor Ioan"], "summary": "Monocular depth estimation methods traditionally train deep models to infer\ndepth directly from RGB pixels. This implicit learning often overlooks explicit\nmonocular cues that the human visual system relies on, such as occlusion\nboundaries, shading, and perspective. Rather than expecting a network to\ndiscover these cues unaided, we present ThirdEye, a cue-aware pipeline that\ndeliberately supplies each cue through specialised, pre-trained, and frozen\nnetworks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)\nequipped with a key-value working-memory module that weights them by\nreliability. An adaptive-bins transformer head then produces a high-resolution\ndisparity map. Because the cue experts are frozen, ThirdEye inherits large\namounts of external supervision while requiring only modest fine-tuning. This\nextended version provides additional architectural detail, neuroscientific\nmotivation, and an expanded experimental protocol; quantitative results will\nappear in a future revision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20877v1", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20877v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "THIRDEYE：基于脑启发式多阶段融合的线索感知单目深度估计", "tldr": "THIRDEYE提出了一个线索感知的单目深度估计方法，通过专门的预训练网络显式地提供并融合人类视觉系统所依赖的单目线索，以克服传统方法忽略这些线索的问题。", "motivation": "传统的单目深度估计方法在从RGB像素推断深度时，通常会忽略人类视觉系统所依赖的显式单目线索（如遮挡边界、阴影和透视）。本文旨在通过主动提供这些线索来解决这一问题。", "method": "本文提出了ThirdEye，一个线索感知管道，通过专门的、预训练且冻结的网络提供每种单目线索。这些线索在受大脑启发的V1->V2->V3三阶段皮层层级中融合，并配备一个键值工作记忆模块，根据可靠性对线索进行加权。之后，一个自适应分箱Transformer头部生成高分辨率的视差图。由于线索专家网络是冻结的，ThirdEye继承了大量的外部监督，同时只需要适度的微调。", "result": "本文提供了额外的架构细节、神经科学动机和扩展的实验协议；定量结果将在未来的修订版中公布。", "conclusion": "未在摘要中提及。", "translation": "单目深度估计方法传统上训练深度模型直接从RGB像素推断深度。这种隐式学习往往忽略了人类视觉系统所依赖的显式单目线索，例如遮挡边界、阴影和透视。我们没有期望网络在没有帮助的情况下发现这些线索，而是提出了ThirdEye，一个线索感知管道，通过专门的、预训练和冻结的网络刻意提供每种线索。这些线索在配备了键值工作记忆模块的三阶段皮层层级（V1->V2->V3）中融合，该模块根据可靠性对它们进行加权。然后，一个自适应分箱Transformer头部生成高分辨率的视差图。由于线索专家网络是冻结的，ThirdEye继承了大量的外部监督，同时只需要适度的微调。这个扩展版本提供了额外的架构细节、神经科学动机和扩展的实验协议；定量结果将在未来的修订版中公布。", "summary": "本文介绍了ThirdEye，一种新颖的单目深度估计方法，旨在通过显式地整合人类视觉系统使用的单目线索来改进传统深度学习模型的不足。ThirdEye管道利用预训练和冻结的专业网络提供遮挡边界、阴影和透视等线索，并在一个受大脑启发的、具有可靠性加权机制的三阶段皮层层级中融合这些线索。最终，一个自适应分箱Transformer头部生成高分辨率的视差图。该方法通过利用外部监督和仅需少量微调，提高了效率和性能。本扩展版本详细阐述了其架构和神经科学依据，但定量结果将在后续版本中发布。", "keywords": "单目深度估计, 线索感知, 脑启发, 多阶段融合, THIRDEYE", "comments": "本文提出了一种新颖的、受大脑启发的方法来解决单目深度估计中的挑战，即传统方法忽略了显式视觉线索。其创新之处在于通过专门的、预训练的冻结网络主动引入这些线索，并通过多阶段融合和可靠性加权机制进行整合。这种方法有效地利用了外部监督，并降低了模型微调的成本。然而，摘要中明确指出定量结果将在未来版本中提供，这意味着当前版本尚未展示其性能的量化优势。"}}
{"id": "2506.21449", "title": "exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design", "authors": ["Maxim Moraru", "Weiyi Xia", "Zhuo Ye", "Feng Zhang", "Yongxin Yao", "Ying Wai Li", "Cai-Zhuang Wang"], "summary": "exa-AMD is a Python-based application designed to accelerate the discovery\nand design of functional materials by integrating AI/ML tools, materials\ndatabases, and quantum mechanical calculations into scalable, high-performance\nworkflows. The execution model of exa-AMD relies on Parsl, a task-parallel\nprogramming library that enables a flexible execution of tasks on any computing\nresource from laptops to supercomputers. By using Parsl, exa-AMD is able to\ndecouple the workflow logic from execution configuration, thereby empowering\nresearchers to scale their workflows without having to reimplement them for\neach system.", "comment": "We intend to publish the paper to the Journal of Open Source Software", "pdf_url": "http://arxiv.org/pdf/2506.21449v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21449v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "exa-AMD：一个用于加速AI辅助材料发现与设计的可扩展工作流", "tldr": "exa-AMD是一个基于Python的应用，利用AI/ML工具、材料数据库和量子力学计算，通过Parsl实现可扩展的高性能工作流，加速功能材料的发现和设计。", "motivation": "旨在通过整合AI/ML工具、材料数据库和量子力学计算，加速功能材料的发现和设计。", "method": "exa-AMD是一个基于Python的应用，其执行模型依赖于Parsl任务并行编程库。Parsl使其能够灵活地在从笔记本电脑到超级计算机的任何计算资源上执行任务，并解耦工作流逻辑与执行配置，从而使研究人员无需为每个系统重新实现即可扩展其工作流。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "exa-AMD是一个基于Python的应用程序，旨在通过将AI/ML工具、材料数据库和量子力学计算集成到可扩展、高性能的工作流中，来加速功能材料的发现和设计。exa-AMD的执行模型依赖于Parsl，这是一个任务并行编程库，它使得任务能够在从笔记本电脑到超级计算机的任何计算资源上灵活执行。通过使用Parsl，exa-AMD能够将工作流逻辑与执行配置解耦，从而使研究人员能够扩展其工作流，而无需为每个系统重新实现它们。", "summary": "exa-AMD是一个Python应用程序，通过整合AI/ML工具、材料数据库和量子力学计算，创建可扩展的高性能工作流，以加速功能材料的发现与设计。它利用Parsl库实现任务并行执行，可在多种计算资源上灵活运行，并将工作流逻辑与执行配置分离，从而简化了工作流的扩展和部署。", "keywords": "材料发现, AI/ML, 量子力学, 可扩展工作流, Parsl", "comments": "exa-AMD的创新之处在于其将AI/ML、材料数据库和量子力学计算集成到一个可扩展的工作流中，并利用Parsl实现计算资源的灵活调度和工作流的解耦。这极大地降低了研究人员在不同计算环境（从个人电脑到超级计算机）中扩展和部署材料发现工作流的复杂性，有望显著加速新材料的研发进程。"}}
{"id": "2506.21500", "title": "Devising a solution to the problems of Cancer awareness in Telangana", "authors": ["Priyanka Avhad", "Vedanti Kshirsagar", "Urvi Ranjan", "Mahek Nakhua"], "summary": "According to the data, the percent of women who underwent screening for\ncervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3\npercent, 0.3 percent and 2.3 percent respectively. Although early detection is\nthe only way to reduce morbidity and mortality, people have very low awareness\nabout cervical and breast cancer signs and symptoms and screening practices. We\ndeveloped an ML classification model to predict if a person is susceptible to\nbreast or cervical cancer based on demographic factors. We devised a system to\nprovide suggestions for the nearest hospital or Cancer treatment centres based\non the users location or address. In addition to this, we can integrate the\nhealth card to maintain medical records of all individuals and conduct\nawareness drives and campaigns. For ML classification models, we used decision\ntree classification and support vector classification algorithms for cervical\ncancer susceptibility and breast cancer susceptibility respectively. Thus, by\ndevising this solution we come one step closer to our goal which is spreading\ncancer awareness, thereby, decreasing the cancer mortality and increasing\ncancer literacy among the people of Telangana.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21500v1", "categories": ["cs.LG", "cs.CY", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21500v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在特伦甘纳邦设计解决癌症意识问题的方案", "tldr": "针对特伦甘纳邦癌症筛查率低和意识不足的问题，研究开发了一个机器学习模型来预测癌症易感性，并设计了一个提供最近医院建议的系统，旨在提高癌症意识和降低死亡率。", "motivation": "特伦甘纳邦宫颈癌、乳腺癌和口腔癌的筛查率极低（分别为3.3%、0.3%和2.3%），尽管早期检测是降低发病率和死亡率的唯一途径，但人们对癌症症状和筛查实践的认识非常不足。", "method": "开发了一个机器学习分类模型，基于人口统计学因素预测个人对乳腺癌或宫颈癌的易感性。同时设计了一个系统，根据用户位置提供最近医院或癌症治疗中心的建议。此外，还计划整合健康卡以维护医疗记录，并开展宣传活动。机器学习模型使用了决策树分类（宫颈癌）和支持向量分类（乳腺癌）算法。", "result": "成功开发了基于人口统计学因素预测乳腺癌和宫颈癌易感性的机器学习分类模型，并设计了根据用户位置提供最近医院或癌症治疗中心建议的系统。该方案还具备整合健康卡和开展宣传活动的潜力。", "conclusion": "通过开发此解决方案，旨在提高特伦甘纳邦居民的癌症意识、降低癌症死亡率并提高癌症知识水平。", "translation": "根据数据显示，2020年特伦甘纳邦接受宫颈癌、乳腺癌和口腔癌筛查的女性比例分别为3.3%、0.3%和2.3%。尽管早期检测是降低发病率和死亡率的唯一途径，但人们对宫颈癌和乳腺癌的症状以及筛查实践的认识非常不足。我们开发了一个机器学习分类模型，根据人口统计学因素预测一个人是否易患乳腺癌或宫颈癌。我们设计了一个系统，根据用户的位置或地址提供最近的医院或癌症治疗中心的建议。除此之外，我们还可以整合健康卡以维护所有个体的医疗记录，并开展宣传活动。对于机器学习分类模型，我们分别使用了决策树分类和支持向量分类算法来预测宫颈癌易感性和乳腺癌易感性。因此，通过设计这个解决方案，我们离我们的目标更近了一步，即传播癌症意识，从而降低癌症死亡率并提高特伦甘纳邦人民的癌症知识水平。", "summary": "本研究针对特伦甘纳邦癌症筛查率和公众意识低下的问题，提出了一项综合解决方案。该方案包括开发一个基于人口统计学因素预测乳腺癌和宫颈癌易感性的机器学习分类模型（使用决策树和支持向量机），以及一个根据用户位置推荐最近癌症治疗中心的系统。此外，该方案还设想整合健康卡以管理医疗记录，并开展癌症宣传活动，旨在提高当地居民的癌症知识水平、促进早期检测并最终降低癌症死亡率。", "keywords": "癌症意识, 机器学习, 易感性预测, 特伦甘纳邦, 早期检测", "comments": "这项研究的创新点在于结合了机器学习技术来预测癌症易感性，并将其集成到一个实际的地理位置推荐系统中，同时强调了通过健康卡整合医疗记录和开展宣传活动的重要性。这提供了一个多方面的、技术驱动的解决方案来解决癌症意识不足的公共卫生问题，具有重要的实际应用价值。然而，抽象中未提及模型的具体性能指标或验证结果，也未说明该系统是否已实际部署和其效果评估。"}}
{"id": "2506.20771", "title": "Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models", "authors": ["Xinghao Dong", "Huchen Yang", "Jin-Long Wu"], "summary": "We propose a latent score-based generative AI framework for learning\nstochastic, non-local closure models and constitutive laws in nonlinear\ndynamical systems of computational mechanics. This work addresses a key\nchallenge of modeling complex multiscale dynamical systems without a clear\nscale separation, for which numerically resolving all scales is prohibitively\nexpensive, e.g., for engineering turbulent flows. While classical closure\nmodeling methods leverage domain knowledge to approximate subgrid-scale\nphenomena, their deterministic and local assumptions can be too restrictive in\nregimes lacking a clear scale separation. Recent developments of\ndiffusion-based stochastic models have shown promise in the context of closure\nmodeling, but their prohibitive computational inference cost limits practical\napplications for many real-world applications. This work addresses this\nlimitation by jointly training convolutional autoencoders with conditional\ndiffusion models in the latent spaces, significantly reducing the\ndimensionality of the sampling process while preserving essential physical\ncharacteristics. Numerical results demonstrate that the joint training approach\nhelps discover a proper latent space that not only guarantees small\nreconstruction errors but also ensures good performance of the diffusion model\nin the latent space. When integrated into numerical simulations, the proposed\nstochastic modeling framework via latent conditional diffusion models achieves\nsignificant computational acceleration while maintaining comparable predictive\naccuracy to standard diffusion models in physical spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20771v1", "categories": ["cs.LG", "math.DS", "physics.comp-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20771v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "非线性动力系统中基于潜在分数生成模型的随机和非局部闭合建模", "tldr": "本文提出了一种基于潜在分数生成AI框架，用于非线性动力系统中的随机、非局部闭合建模，显著提高了计算效率并保持了预测精度。", "motivation": "在计算力学中，对没有明确尺度分离的复杂多尺度动力系统进行建模是一个关键挑战，因为数值求解所有尺度会带来高昂的计算成本。传统的闭合建模方法在缺乏明确尺度分离的区域中过于受限，而近期基于扩散的随机模型虽然有前景，但其高昂的计算推断成本限制了实际应用。", "method": "本文提出了一种潜在分数生成AI框架，通过在潜在空间中联合训练卷积自编码器和条件扩散模型来解决上述限制。这种方法显著降低了采样过程的维度，同时保留了重要的物理特性。", "result": "数值结果表明，联合训练方法有助于发现一个合适的潜在空间，该空间不仅保证了小的重建误差，而且确保了潜在空间中扩散模型的良好性能。当集成到数值模拟中时，所提出的随机建模框架通过潜在条件扩散模型实现了显著的计算加速，同时保持了与物理空间中标准扩散模型相当的预测精度。", "conclusion": "所提出的基于潜在条件扩散模型的随机建模框架能够为非线性动力系统提供一种计算高效且准确的随机非局部闭合模型，克服了现有方法的局限性。", "translation": "我们提出了一种潜在分数生成AI框架，用于在计算力学的非线性动力系统中学习随机、非局部闭合模型和本构定律。这项工作解决了在没有明确尺度分离的情况下建模复杂多尺度动力系统的关键挑战，例如工程湍流，其中数值求解所有尺度是极其昂贵的。虽然经典的闭合建模方法利用领域知识来近似亚网格尺度现象，但它们的确定性和局部假设在缺乏明确尺度分离的区域中可能过于严格。最近基于扩散的随机模型在闭合建模方面显示出前景，但其高昂的计算推断成本限制了许多实际应用的实践性。这项工作通过在潜在空间中联合训练卷积自编码器和条件扩散模型来解决这一限制，显著降低了采样过程的维度，同时保留了重要的物理特性。数值结果表明，联合训练方法有助于发现一个合适的潜在空间，该空间不仅保证了小的重建误差，而且确保了潜在空间中扩散模型的良好性能。当集成到数值模拟中时，所提出的通过潜在条件扩散模型的随机建模框架实现了显著的计算加速，同时保持了与物理空间中标准扩散模型相当的预测精度。", "summary": "本文提出了一种新颖的基于潜在分数生成AI框架，用于在复杂非线性动力系统中开发随机和非局部闭合模型，特别是那些缺乏明确尺度分离的系统。为了解决高计算成本和传统确定性/局部闭合方法的局限性，该框架在潜在空间中联合训练卷积自编码器和条件扩散模型。这种方法有效降低了采样过程的维度，同时保留了关键的物理特性。数值结果表明，这种联合训练产生了合适的潜在空间，确保了低重建误差和扩散模型的强大性能。当集成到模拟中时，所提出的框架显著加速了计算，同时与物理空间中的标准扩散模型相比，预测精度没有降低。", "keywords": "随机闭合建模, 潜在生成模型, 非线性动力系统, 扩散模型, 计算力学", "comments": "本文的创新点在于通过在潜在空间中结合自编码器和条件扩散模型，显著降低了扩散模型在复杂多尺度动力系统中的计算成本。这对于克服传统闭合模型的局限性以及现有扩散模型的高计算开销至关重要，为计算力学中的复杂系统建模提供了更高效、更灵活的工具。"}}
{"id": "2506.21208", "title": "Adversarial Training: Enhancing Out-of-Distribution Generalization for Learning Wireless Resource Allocation", "authors": ["Shengjie Liu", "Chenyang Yang"], "summary": "Deep neural networks (DNNs) have widespread applications for optimizing\nresource allocation. Yet, their performance is vulnerable to distribution\nshifts between training and test data, say channels. In this letter, we resort\nto adversarial training (AT) for enhancing out-of-distribution (OOD)\ngeneralizability of DNNs trained in unsupervised manner. We reformulate AT to\ncapture the OOD degradation, and propose a one-step gradient ascent method for\nAT. The proposed method is validated by optimizing hybrid precoding. Simulation\nresults showcase the enhanced OOD performance of multiple kinds of DNNs across\nvarious channel distributions, when only Rayleigh fading channels are used for\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21208v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21208v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "对抗训练：增强无线资源分配学习中的分布外泛化能力", "tldr": "本文提出通过对抗训练增强深度神经网络在无线资源分配中对分布外数据的泛化能力，以应对信道分布偏移。", "motivation": "深度神经网络在优化无线资源分配方面应用广泛，但其性能易受训练与测试数据间分布偏移（如信道变化）的影响，导致泛化能力下降。", "method": "本文采用对抗训练（AT）来增强无监督训练的深度神经网络（DNNs）的分布外（OOD）泛化能力。具体方法是重新制定AT以捕捉OOD退化，并提出一种一步梯度上升法用于AT。该方法通过优化混合预编码进行了验证。", "result": "仿真结果表明，即使仅使用瑞利衰落信道进行训练，所提出的方法也能在各种信道分布下显著增强多种DNN的OOD性能。", "conclusion": "对抗训练能够有效增强深度神经网络在无线资源分配任务中的分布外泛化能力，即使训练数据分布有限，也能在不同信道条件下保持良好性能。", "translation": "深度神经网络（DNNs）在优化资源分配方面有广泛应用。然而，它们的性能容易受到训练和测试数据之间分布偏移（例如信道）的影响。本文采用对抗训练（AT）来增强无监督训练的DNNs的分布外（OOD）泛化能力。我们重新制定了AT以捕捉OOD退化，并提出了一种用于AT的一步梯度上升方法。所提出的方法通过优化混合预编码得到了验证。仿真结果表明，即使仅使用瑞利衰落信道进行训练，多种DNNs在各种信道分布下都表现出增强的OOD性能。", "summary": "本文提出一种基于对抗训练（AT）的方法，旨在提升无监督深度神经网络在无线资源分配任务中的分布外（OOD）泛化能力，以应对训练与测试数据间的信道分布偏移问题。研究者通过重新制定AT来捕捉OOD退化，并引入一步梯度上升法。实验结果显示，该方法在仅使用瑞利衰落信道进行训练的情况下，仍能显著提高不同类型DNN在多种信道分布下的OOD性能，并通过优化混合预编码进行了验证。", "keywords": "对抗训练, 分布外泛化, 无线资源分配, 深度神经网络, 混合预编码", "comments": "本文的创新之处在于将对抗训练应用于无线资源分配领域，以解决深度神经网络在分布偏移下的泛化能力问题。特别指出的是，它在无监督学习的背景下，通过重新制定AT和提出一步梯度上升法，有效地提升了模型在未见信道条件下的性能，这对于实际无线通信系统的鲁棒性部署具有重要意义。"}}
{"id": "2506.20696", "title": "IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "summary": "Elucidating the biomechanical behavior of the myocardium is crucial for\nunderstanding cardiac physiology, but cannot be directly inferred from clinical\nimaging and typically requires finite element (FE) simulations. However,\nconventional FE methods are computationally expensive and often fail to\nreproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed\nneural network (PINN) framework that integrates imaged motion consistency (IMC)\nwith FE modeling for patient-specific left ventricular (LV) biomechanics.\nCardiac motion is first estimated from MRI or echocardiography using either a\npre-trained attention-based network or an unsupervised cyclic-regularized\nnetwork, followed by extraction of motion modes. IMC-PINN-FE then rapidly\nestimates myocardial stiffness and active tension by fitting clinical pressure\nmeasurements, accelerating computation from hours to seconds compared to\ntraditional inverse FE. Based on these parameters, it performs FE modeling\nacross the cardiac cycle at 75x speedup. Through motion constraints, it matches\nimaged displacements more accurately, improving average Dice from 0.849 to\n0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE\nadvances previous PINN-FE models by introducing back-computation of material\nproperties and better motion fidelity. Using motion from a single subject to\nreconstruct shape modes also avoids the need for large datasets and improves\npatient specificity. IMC-PINN-FE offers a robust and efficient approach for\nrapid, personalized, and image-consistent cardiac biomechanical modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20696v1", "categories": ["physics.med-ph", "cs.AI", "eess.IV"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.20696v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "IMC-PINN-FE：一种用于患者特异性左心室有限元建模的物理信息神经网络，具有图像运动一致性和生物力学参数估计功能", "tldr": "IMC-PINN-FE是一个物理信息神经网络框架，通过整合图像运动一致性来快速、患者特异性地进行心脏有限元建模并估计生物力学参数，显著提高了计算速度和准确性。", "motivation": "阐明心肌的生物力学行为对于理解心脏生理学至关重要，但无法直接从临床影像中推断，通常需要有限元（FE）模拟。然而，传统的有限元方法计算成本高昂，并且常常无法重现观察到的心脏运动。", "method": "提出了IMC-PINN-FE，一个将图像运动一致性（IMC）与有限元建模相结合的物理信息神经网络（PINN）框架。首先使用预训练的基于注意力网络或无监督循环正则化网络从MRI或超声心动图中估计心脏运动，然后提取运动模式。IMC-PINN-FE通过拟合临床压力测量值，快速估计心肌刚度和主动张力。基于这些参数，它在整个心动周期内执行有限元建模，并通过运动约束更准确地匹配图像位移。该方法通过使用单个受试者的运动重建形状模式，避免了对大型数据集的需求。", "result": "与传统逆向有限元相比，计算时间从数小时缩短到数秒，心脏周期内的有限元建模速度提高了75倍。通过运动约束，平均Dice系数从0.849提高到0.927，实现了更准确的图像位移匹配，同时保持了真实的压力-容积行为。该模型通过引入材料属性的反向计算和更好的运动保真度，改进了先前的PINN-FE模型，并提高了患者特异性。", "conclusion": "IMC-PINN-FE为快速、个性化和图像一致的心脏生物力学建模提供了一种鲁棒且有效的方法。", "translation": "阐明心肌的生物力学行为对于理解心脏生理学至关重要，但无法直接从临床影像中推断出来，通常需要有限元（FE）模拟。然而，传统的有限元方法计算成本高昂，并且通常无法重现观察到的心脏运动。我们提出了IMC-PINN-FE，一个物理信息神经网络（PINN）框架，它将图像运动一致性（IMC）与有限元建模相结合，用于患者特异性左心室（LV）生物力学。心脏运动首先通过预训练的基于注意力网络或无监督循环正则化网络从MRI或超声心动图中估计，然后提取运动模式。IMC-PINN-FE随后通过拟合临床压力测量，快速估计心肌刚度和主动张力，与传统逆向有限元相比，将计算时间从数小时缩短到数秒。基于这些参数，它以75倍的速度在整个心动周期内执行有限元建模。通过运动约束，它更准确地匹配图像位移，将平均Dice系数从0.849提高到0.927，同时保持真实的压力-容积行为。IMC-PINN-FE通过引入材料属性的反向计算和更好的运动保真度，改进了先前的PINN-FE模型。利用单个受试者的运动重建形状模式也避免了对大型数据集的需求，并提高了患者特异性。IMC-PINN-FE为快速、个性化和图像一致的心脏生物力学建模提供了一种鲁棒且有效的方法。", "summary": "本文介绍了IMC-PINN-FE，一个用于患者特异性左心室有限元（FE）建模的物理信息神经网络框架。它解决了传统有限元方法计算成本高昂且难以重现心脏运动的局限性。IMC-PINN-FE通过整合图像运动一致性，拟合临床压力数据，快速估计心肌生物力学参数（刚度、主动张力），实现了显著的计算加速（从数小时到数秒，FE建模加速75倍）。该模型提高了运动保真度（Dice系数从0.849到0.927），并保持了真实的压力-容积行为，同时通过利用单个受试者的运动重建形状模式，减少了对大型数据集的需求。这为心脏生物力学建模提供了一种鲁棒、高效和个性化的解决方案。", "keywords": "物理信息神经网络, 有限元建模, 心脏生物力学, 患者特异性, 图像运动一致性", "comments": "该论文的创新之处在于将图像运动一致性直接整合到物理信息神经网络-有限元（PINN-FE）框架中，实现了更准确和患者特异性的建模。通过引入材料属性的反向计算和改进的运动保真度，该模型在现有PINN-FE模型基础上取得了进步。其重要性体现在显著缩短了计算时间（从数小时到数秒），使患者特异性心脏生物力学建模在临床上更具可行性。此外，通过利用单个受试者的运动来重建形状模式，也有效解决了数据依赖性问题。"}}
{"id": "2506.21458", "title": "Spatial Mental Modeling from Limited Views", "authors": ["Baiqiao Yin", "Qineng Wang", "Pingyue Zhang", "Jianshu Zhang", "Kangrui Wang", "Zihan Wang", "Jieyu Zhang", "Keshigeyan Chandrasegaran", "Han Liu", "Ranjay Krishna", "Saining Xie", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.", "comment": "Preprint version", "pdf_url": "http://arxiv.org/pdf/2506.21458v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21458v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "有限视角下的空间心理建模", "tldr": "本文提出了MindCube基准测试，揭示了当前视觉语言模型（VLMs）在从有限视角构建空间心理模型方面的不足。通过“先建图后推理”的方法并结合强化学习，显著提高了VLMs对不可观测空间的理解能力。", "motivation": "人类能够从有限的视角想象出完整的场景并形成空间心理模型，但现有的视觉语言模型（VLMs）在这方面表现不佳，存在巨大差距，其性能接近随机水平。本文旨在解决这一关键问题，提升VLMs构建鲁棒空间心理模型的能力。", "method": "本文首先提出了MindCube基准测试（包含21,154个问题和3,268张图像），用于系统评估VLMs构建空间心理模型的能力，包括表示位置（认知映射）、方向（视角转换）和动态（心理模拟）。随后，探索了三种方法来帮助VLMs近似空间心理模型：未见的中间视图、自然语言推理链和认知地图。其中，最有效的方法是“先建图后推理”的协同方法，该方法联合训练模型，使其首先生成认知地图，然后在此基础上进行推理。此外，还通过引入强化学习进一步提升了模型性能。", "result": "通过“先建图后推理”的方法，模型的准确率从37.8%显著提升至60.8%（提高了23.0%）。在此基础上，加入强化学习后，性能进一步提升至70.7%（总计提升了32.9%）。", "conclusion": "积极构建和利用内部结构化的空间表征，并结合灵活的推理过程，即对空间心理模型进行脚手架式（scaffolding）的构建，能够显著提高视觉语言模型对不可观测空间的理解能力。", "translation": "视觉语言模型（VLMs）能否像人类一样，仅凭少量视图就能想象出完整的场景？人类会形成空间心理模型，即对未见空间的内部表征，以便对布局、视角和运动进行推理。我们新的MindCube基准测试包含21,154个问题，涉及3,268张图像，揭示了这一关键差距，即现有VLMs表现出接近随机的性能。利用MindCube，我们系统地评估了VLMs在表示位置（认知映射）、方向（视角转换）和动态（“假设”运动的心理模拟）方面，构建鲁棒空间心理模型的能力。然后，我们探索了三种方法来帮助VLMs近似空间心理模型，包括未见的中间视图、自然语言推理链和认知地图。显著的改进来自于一种协同方法——“先建图后推理”，它联合训练模型，使其首先生成认知地图，然后在此基础上进行推理。通过训练模型对这些内部地图进行推理，我们将准确率从37.8%提高到60.8%（+23.0%）。加入强化学习后，性能进一步提升至70.7%（+32.9%）。我们的关键见解是，这种空间心理模型的脚手架式构建，即主动构建和利用具有灵活推理过程的内部结构化空间表征，显著提高了对不可观测空间的理解。", "summary": "本文研究了视觉语言模型（VLMs）从有限视角构建空间心理模型的能力，并指出当前VLMs在此方面表现不佳。为此，作者提出了MindCube基准测试，用于系统评估VLMs的认知映射、视角转换和心理模拟能力。研究探索了多种方法，发现“先建图后推理”的协同方法，即模型先生成认知地图再进行推理，能显著提升性能。结合强化学习，模型准确率从37.8%提升至70.7%。研究强调了构建和利用内部结构化空间表征对于理解不可观测空间的重要性。", "keywords": "空间心理建模, 视觉语言模型, MindCube, 认知地图, 强化学习", "comments": "本文的创新点在于提出了MindCube这一专门用于评估VLM空间心理建模能力的基准测试，填补了现有研究的空白。此外，“先建图后推理”的策略，将认知映射与推理过程相结合，为VLM处理复杂空间推理任务提供了有效途径。通过模拟人类构建空间心理模型的过程，该研究为VLM在更高级别的认知理解方面迈出了重要一步，具有重要的理论和实践意义。其提出的方法和见解，对于未来构建更具人类智能的AI系统具有指导作用。"}}
{"id": "2506.21191", "title": "Prompt-Guided Turn-Taking Prediction", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.", "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "pdf_url": "http://arxiv.org/pdf/2506.21191v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21191v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "提示词引导的轮流对话预测", "tldr": "本文提出了一种通过文本提示动态控制轮流对话预测的新模型，该模型基于Transformer，并利用大型语言模型生成合成提示数据，实验结果表明其提高了预测精度并能根据提示词有效调整轮流对话时序。", "motivation": "轮流对话预测模型是语音对话系统和会话机器人中的重要组成部分。现有的方法虽然利用了基于Transformer的架构进行实时语音活动预测，但缺乏直观和明确的动态控制能力。", "method": "本文提出了一种新颖的模型，通过将文本提示嵌入到基于Transformer的语音活动投影（VAP）模型的通道内Transformer和跨通道Transformer中，实现了轮流对话预测的动态控制。由于缺乏现有数据集中的文本提示数据，研究人员利用大型语言模型（LLM）生成了合成提示语句，并使用超过950小时的人-人语音对话数据评估了该方法的可行性。", "result": "实验结果表明，所提出的模型提高了预测准确性，并能根据文本提示有效地改变轮流对话的时序行为。", "conclusion": "所提出的通过文本提示动态控制轮流对话预测的模型，在提高预测准确性和根据上下文动态调整轮流对话时序方面表现出有效性。", "translation": "轮流对话预测模型是语音对话系统和会话机器人中的重要组成部分。最近的方法利用基于Transformer的架构来连续实时地预测语音活动。在这项研究中，我们提出了一种新颖的模型，可以通过文本提示动态控制轮流对话预测。这种方法允许通过“更快”或“更平静”等指令进行直观和明确的控制，从而动态适应对话伙伴和上下文。所提出的模型建立在基于Transformer的语音活动投影（VAP）模型之上，将文本提示嵌入到通道内Transformer和跨通道Transformer中。我们使用超过950小时的人-人语音对话数据评估了我们方法的可行性。由于现有数据集中没有可用于所提出方法的文本提示数据，我们利用大型语言模型（LLM）生成了合成提示语句。实验结果表明，所提出的模型提高了预测准确性，并能根据文本提示有效地改变轮流对话时序行为。", "summary": "本文提出了一种新颖的、基于Transformer的轮流对话预测模型，该模型允许通过文本提示（如“更快”或“更平静”）进行动态控制。该模型将文本提示嵌入到Transformer架构中，并利用大型语言模型生成合成提示数据以进行训练。在超过950小时的人-人对话数据上进行的评估显示，该模型不仅提高了预测准确性，还能根据给定的文本提示有效调整轮流对话的时序行为，从而实现了对对话系统的直观和明确控制。", "keywords": "轮流对话预测, 文本提示, Transformer, 大型语言模型, 语音对话系统", "comments": "该研究的创新之处在于引入了文本提示来动态控制轮流对话预测，这为对话系统提供了更直观和灵活的交互方式。利用大型语言模型生成合成训练数据，有效解决了现有数据集中缺乏此类特定提示数据的问题，这对于研究新颖的控制方法具有重要意义。该方法有望提升人机对话的自然度和适应性。"}}
{"id": "2506.21049", "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "comment": "Accepted by ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21049v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21049v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种用于电商查询分类的半监督可扩展统一框架", "tldr": "本文提出了一种名为SSUF的半监督可扩展统一框架，通过知识增强、标签增强和结构增强模块，解决了电商查询分类中信息不足、对后验标签依赖高以及缺乏统一框架的问题，并在实验中取得了显著优于现有模型的效果。", "motivation": "电商查询分类对电商应用至关重要，但现有方法面临多重挑战：查询通常短且缺乏上下文，导致建模时先验信息不足；多数工业方法依赖用户后验点击行为构建训练样本，易陷入马太效应；查询分类的子任务缺乏统一框架，导致算法优化效率低下。", "method": "本文提出了一种新颖的半监督可扩展统一框架（SSUF），包含多个增强模块以统一查询分类任务。其中，知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题；标签增强模块利用标签语义和半监督信号减少对后验标签的依赖；结构增强模块基于复杂的标签关系增强标签表示。每个模块都高度可插拔，可根据子任务需求增删输入特征。", "result": "通过广泛的离线和在线A/B实验，结果表明SSUF显著优于现有最先进的模型。", "conclusion": "SSUF框架成功解决了电商查询分类中信息不足、对后验标签依赖高以及缺乏统一框架等问题，并在实际应用中展现出卓越的性能，证明了其有效性和优越性。", "translation": "查询分类，包括意图和类别预测等多个子任务，对电子商务应用至关重要。电子商务查询通常较短且缺乏上下文，并且标签之间的信息无法利用，导致建模的先验信息不足。大多数现有工业查询分类方法依赖于用户的后验点击行为来构建训练样本，从而导致马太效应的恶性循环。此外，查询分类的子任务缺乏统一框架，导致算法优化效率低下。\n在本文中，我们提出了一种新颖的半监督可扩展统一框架（SSUF），包含多个增强模块以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号减少对后验标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块都高度可插拔，可以根据每个子任务的需要添加或删除输入特征。我们进行了广泛的离线和在线A/B实验，结果表明SSUF显著优于现有最先进的模型。", "summary": "本文针对电商查询分类中查询信息不足、对后验标签依赖高以及子任务缺乏统一框架等问题，提出了一种半监督可扩展统一框架（SSUF）。SSUF通过知识增强、标签增强和结构增强三个模块，分别提升查询表示、减少对后验标签的依赖并利用标签关系。该框架模块化且高度可插拔，经大量离线和在线实验验证，其性能显著优于现有最先进的模型。", "keywords": "电商查询分类, 半监督学习, 统一框架, 可扩展性, 知识增强", "comments": "本文的创新点在于提出了一个统一的半监督框架来解决电商查询分类的多个痛点。其模块化的设计（知识增强、标签增强、结构增强）使其具有高度的灵活性和可扩展性，能够适应不同子任务的需求，这对于工业界应用具有重要意义。通过结合半监督学习和世界知识，有效缓解了数据稀疏和对标注数据依赖的问题，具有很强的实用价值。"}}
{"id": "2506.21241", "title": "On the coordinate system-dependence of the accuracy of symplectic numerical methods", "authors": ["Donát M. Takács", "Tamás Fülöp"], "summary": "Symplectic numerical methods have become a widely-used choice for the\naccurate simulation of Hamiltonian systems in various fields, including\ncelestial mechanics, molecular dynamics and robotics. Even though their\ncharacteristics are well-understood mathematically, relatively little attention\nhas been paid in general to the practical aspect of how the choice of\ncoordinates affects the accuracy of the numerical results, even though the\nconsequences can be computationally significant. The present article aims to\nfill this gap by giving a systematic overview of how coordinate transformations\ncan influence the results of simulations performed using symplectic methods. We\ngive a derivation for the non-invariance of the modified Hamiltonian of\nsymplectic methods under coordinate transformations, as well as a sufficient\ncondition for the non-preservation of a first integral corresponding to a\ncyclic coordinate for the symplectic Euler method. We also consider the\npossibility of finding order-compensating coordinate transformations that\nimprove the order of accuracy of a numerical method. Various numerical examples\nare presented throughout.", "comment": "24 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.21241v1", "categories": ["math.NA", "cs.NA", "physics.class-ph", "physics.comp-ph", "65P10, 70H15, 34A05"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21241v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于辛数值方法精度对坐标系的依赖性", "tldr": "本文系统性地探讨了坐标系选择如何影响辛数值方法的精度，揭示了修正哈密顿量在坐标变换下的非不变性，并探讨了提高精度的方法。", "motivation": "辛数值方法在模拟哈密顿系统时被广泛使用，但关于坐标系选择如何影响其精度这一实际问题，却鲜有关注，尽管其计算后果可能非常显著。本文旨在填补这一空白。", "method": "本文系统性地概述了坐标变换如何影响辛方法模拟结果。具体方法包括：推导辛方法修正哈密顿量在坐标变换下的非不变性；给出辛Euler方法对应循环坐标第一积分不守恒的充分条件；考虑寻找能够提高数值方法精度的阶补偿坐标变换。", "result": "研究结果表明，辛方法修正哈密顿量在坐标变换下是非不变的。对于辛Euler方法，给出了对应循环坐标第一积分不守恒的充分条件。文中还探讨了通过阶补偿坐标变换提高数值方法精度的可能性。全文提供了各种数值示例。", "conclusion": "本文结论是，坐标变换会显著影响辛数值方法的精度。修正哈密顿量在坐标变换下不具有不变性，并且某些情况下第一积分也不会被保持。通过特定的坐标变换有可能提高方法的精度。", "translation": "辛数值方法已成为在天体力学、分子动力学和机器人学等各个领域中精确模拟哈密顿系统的广泛选择。尽管它们的特性在数学上已得到充分理解，但通常很少有人关注坐标选择如何影响数值结果的精度这一实际方面，尽管其后果在计算上可能非常显著。本文旨在通过系统地概述坐标变换如何影响使用辛方法进行的模拟结果来填补这一空白。我们推导了辛方法修正哈密顿量在坐标变换下的非不变性，以及辛Euler方法对应循环坐标第一积分不守恒的充分条件。我们还考虑了寻找能够提高数值方法精度的阶补偿坐标变换的可能性。文中提供了各种数值示例。", "summary": "本文系统性地探讨了坐标系选择对辛数值方法精度的影响。研究表明，辛方法的修正哈密顿量在坐标变换下不具有不变性，并且某些情况下与循环坐标对应的第一积分可能不被保持。文章还探讨了如何通过寻找阶补偿坐标变换来提高数值方法的精度，并通过数值示例进行了验证。", "keywords": "辛数值方法, 坐标变换, 精度, 哈密顿系统, 修正哈密顿量", "comments": "本文填补了辛数值方法研究中的一个重要空白，即坐标系选择对方法精度的实际影响。其创新之处在于系统性地推导了修正哈密顿量的非不变性，并提出了通过坐标变换来优化精度的可能性，这对于实际应用具有重要指导意义。"}}
{"id": "2506.20989", "title": "Can Gradient Descent Simulate Prompting?", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "comment": "14 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.20989v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20989v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "梯度下降能否模拟提示？", "tldr": "本文提出了一种元训练语言模型的方法，使得梯度更新能够模拟提示（prompting）的效果，从而在某些任务上恢复甚至超越提示模型的性能。", "motivation": "语言模型（LM）整合新信息主要有两种方式：改变提示或改变参数（如微调）。尽管参数更新没有长期存储成本，但对于许多模型更新而言，提示（prompting）明显更有效，因为它能从单一示例中稳健泛化并进行逻辑推理，而标准微调则不行。本文旨在探讨是否可以通过修改模型，使微调能够模拟提示的效果。", "method": "本文描述了一种元训练语言模型的方法，使得梯度更新能够模拟对新信息进行条件化的效果。该方法利用了基于梯度的元学习工具，但使用语言模型自身的提示预测作为目标，从而消除了对真实标签的需求。", "result": "后续的梯度下降训练恢复了部分（有时甚至是全部）提示模型的性能，在“反转诅咒”任务上显示出改进，并且在单次梯度更新后能够回答关于文本段落的问题。", "conclusion": "这些结果表明，通过适当的初始化，梯度下降可以表现出惊人的表达能力。我们的研究结果为长上下文建模提供了新途径，并为基于梯度的学习的泛化能力提供了见解。", "translation": "将新信息整合到语言模型（LM）中有两种主要方式：改变其提示或改变其参数（例如通过微调）。参数更新不会产生模型更改的长期存储成本。然而，对于许多模型更新而言，提示（prompting）明显更有效：提示模型可以从单一示例中稳健泛化，并进行标准微调中不会发生的逻辑推理。那么，模型能否被修改，使得微调能够模拟提示呢？本文描述了一种元训练语言模型的方法，使得梯度更新能够模拟对新信息进行条件化的效果。我们的方法利用了基于梯度的元学习工具，但使用语言模型自身的提示预测作为目标，从而消除了对真实标签的需求。随后的梯度下降训练恢复了部分（有时甚至是全部）提示模型的性能——在“反转诅咒”任务上显示出改进，并在单次梯度更新后回答了关于文本段落的问题。这些结果表明，通过适当的初始化，梯度下降可以表现出惊人的表达能力。我们的结果为长上下文建模提供了新途径，并为基于梯度的学习的泛化能力提供了见解。", "summary": "本文探讨了梯度下降能否模拟语言模型中的提示（prompting）效果。研究发现，尽管提示在泛化和推理方面优于标准微调，但通过一种基于梯度的元学习方法，使用模型自身的提示预测作为训练目标，可以使梯度更新模拟提示的效果。实验结果显示，这种方法在某些任务上恢复了提示模型的性能，并改进了“反转诅咒”问题，表明在适当初始化下，梯度下降具有强大的表达能力，为长上下文建模和梯度学习的泛化提供了新思路。", "keywords": "梯度下降, 提示, 元学习, 语言模型, 微调", "comments": "这项研究的创新点在于提出了一个巧妙的元训练框架，它通过利用语言模型自身的提示预测作为训练目标，避免了对真实标签的依赖，从而使梯度下降能够模拟提示的效果。这对于理解和弥合提示与微调之间的性能差距具有重要意义。其结果不仅为长上下文建模开辟了新的研究方向，也加深了我们对基于梯度学习泛化能力的理解。"}}
{"id": "2506.21063", "title": "Control of Marine Robots in the Era of Data-Driven Intelligence", "authors": ["Lin Hong", "Lu Liu", "Zhouhua Peng", "Fumin Zhang"], "summary": "The control of marine robots has long relied on model-based methods grounded\nin classical and modern control theory. However, the nonlinearity and\nuncertainties inherent in robot dynamics, coupled with the complexity of marine\nenvironments, have revealed the limitations of conventional control methods.\nThe rapid evolution of machine learning has opened new avenues for\nincorporating data-driven intelligence into control strategies, prompting a\nparadigm shift in the control of marine robots. This paper provides a review of\nrecent progress in marine robot control through the lens of this emerging\nparadigm. The review covers both individual and cooperative marine robotic\nsystems, highlighting notable achievements in data-driven control of marine\nrobots and summarizing open-source resources that support the development and\nvalidation of advanced control methods. Finally, several future perspectives\nare outlined to guide research toward achieving high-level autonomy for marine\nrobots in real-world applications. This paper aims to serve as a roadmap toward\nthe next-generation control framework of marine robots in the era of\ndata-driven intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21063v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21063v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "海洋机器人在数据驱动智能时代的控制", "tldr": "本文综述了数据驱动智能在海洋机器人控制领域的最新进展，并展望了未来的研究方向。", "motivation": "传统的基于模型的海洋机器人控制方法在应对机器人动力学非线性、不确定性以及复杂海洋环境时存在局限性，而机器学习的快速发展为将数据驱动智能融入控制策略开辟了新途径，促使控制范式发生转变。", "method": "本文从数据驱动智能的视角，回顾了海洋机器人控制的最新进展，涵盖了单个和协作海洋机器人系统，重点介绍了数据驱动控制的显著成就，并总结了支持高级控制方法开发和验证的开源资源。", "result": "突出了数据驱动控制在海洋机器人领域取得的显著成就，并总结了支持高级控制方法开发和验证的开源资源。", "conclusion": "提出了未来研究的几个方向，以指导海洋机器人在实际应用中实现高水平自主性，并旨在为数据驱动智能时代海洋机器人的下一代控制框架提供路线图。", "translation": "海洋机器人的控制长期以来一直依赖于基于经典和现代控制理论的模型方法。然而，机器人动力学固有的非线性和不确定性，加上海洋环境的复杂性，揭示了传统控制方法的局限性。机器学习的快速发展为将数据驱动智能融入控制策略开辟了新途径，促使海洋机器人控制范式发生转变。本文从这一新兴范式的角度回顾了海洋机器人控制的最新进展。该综述涵盖了单个和协作海洋机器人系统，重点介绍了数据驱动海洋机器人控制方面的显著成就，并总结了支持高级控制方法开发和验证的开源资源。最后，概述了几个未来展望，以指导研究实现海洋机器人在实际应用中的高水平自主性。本文旨在为数据驱动智能时代海洋机器人的下一代控制框架提供路线图。", "summary": "本文综述了数据驱动智能在海洋机器人控制领域的最新进展，以应对传统模型方法在复杂海洋环境中的局限性。文章涵盖了单个和协作海洋机器人系统，重点介绍了数据驱动控制的显著成就，并总结了相关的开源资源。此外，论文还展望了未来的研究方向，旨在为实现海洋机器人的高水平自主性提供路线图，并为下一代控制框架提供指导。", "keywords": "海洋机器人控制, 数据驱动智能, 机器学习, 控制策略, 自主性", "comments": "这篇综述论文的重要性在于它及时地总结了数据驱动智能在海洋机器人控制这一复杂且关键领域中的应用。它不仅指出了传统方法的不足，还系统地梳理了新兴的数据驱动范式，并提供了实用的开源资源信息，为研究人员指明了未来的发展方向，具有重要的指导意义。"}}
{"id": "2506.21300", "title": "An object-centric core metamodel for IoT-enhanced event logs", "authors": ["Yannis Bertrand", "Christian Imenkamp", "Lukas Malburg", "Matthias Ehrendorfer", "Marco Franceschetti", "Joscha Grüger", "Francesco Leotta", "Jürgen Mangler", "Ronny Seiger", "Agnes Koschmider", "Stefanie Rinderle-Ma", "Barbara Weber", "Estefania Serral"], "summary": "Advances in Internet-of-Things (IoT) technologies have prompted the\nintegration of IoT devices with business processes (BPs) in many organizations\nacross various sectors, such as manufacturing, healthcare and smart spaces. The\nproliferation of IoT devices leads to the generation of large amounts of IoT\ndata providing a window on the physical context of BPs, which facilitates the\ndiscovery of new insights about BPs using process mining (PM) techniques.\nHowever, to achieve these benefits, IoT data need to be combined with\ntraditional process (event) data, which is challenging due to the very\ndifferent characteristics of IoT and process data, for instance in terms of\ngranularity levels. Recently, several data models were proposed to integrate\nIoT data with process data, each focusing on different aspects of data\nintegration based on different assumptions and requirements. This fragmentation\nhampers data exchange and collaboration in the field of PM, e.g., making it\ntedious for researchers to share data. In this paper, we present a core model\nsynthesizing the most important features of existing data models. As the core\nmodel is based on common requirements, it greatly facilitates data sharing and\ncollaboration in the field. A prototypical Python implementation is used to\nevaluate the model against various use cases and demonstrate that it satisfies\nthese common requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21300v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.21300v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一个以对象为中心的物联网增强事件日志核心元模型", "tldr": "物联网数据与流程数据的整合因碎片化而面临挑战。本文提出了一个核心元模型，综合现有模型，促进流程挖掘中的数据共享和协作，并通过Python原型进行验证。", "motivation": "物联网数据与业务流程的集成面临挑战，原因在于物联网数据和传统流程事件数据之间存在不同的数据特性（例如粒度级别），以及现有数据模型的碎片化，这阻碍了流程挖掘领域的数据交换和协作。", "method": "本文提出了一个核心元模型，该模型综合了现有数据模型最重要的特征，并基于共同的要求。通过一个原型Python实现来评估该模型。", "result": "该核心模型极大地促进了该领域的数据共享和协作。Python实现证明该模型满足各种用例中的共同要求。", "conclusion": "本文提出的以对象为中心的核​​心元模型通过综合现有模型的特征，有效地将物联网数据与流程数据集成，从而解决了数据碎片化问题，并实现了流程挖掘中更好的数据共享和协作。", "translation": "物联网（IoT）技术的进步促使物联网设备与各行各业（如制造业、医疗保健和智能空间）的业务流程（BP）集成。物联网设备的普及导致生成大量物联网数据，这些数据为业务流程的物理上下文提供了窗口，从而有助于使用流程挖掘（PM）技术发现关于业务流程的新见解。然而，为了实现这些益处，物联网数据需要与传统流程（事件）数据结合，这具有挑战性，因为物联网数据和流程数据具有非常不同的特征，例如粒度级别。最近，提出了几种数据模型来将物联网数据与流程数据集成，每个模型都根据不同的假设和要求侧重于数据集成的不同方面。这种碎片化阻碍了流程挖掘领域的数据交换和协作，例如，使得研究人员共享数据变得繁琐。在本文中，我们提出了一个核心模型，该模型综合了现有数据模型最重要的特征。由于该核心模型基于共同的要求，它极大地促进了该领域的数据共享和协作。一个原型Python实现用于根据各种用例评估该模型，并证明它满足这些共同要求。", "summary": "本文旨在解决将异构物联网数据与传统流程事件数据集成到流程挖掘中的挑战。文章指出了不同数据特性以及现有集成模型碎片化的问题，这些问题阻碍了数据共享和协作。为克服这些问题，作者提出了一个以对象为中心的核​​心元模型，该模型根据共同要求综合了现有模型的关键特征。通过一个Python原型实现验证了该模型在各种用例中促进数据共享和协作的有效性。", "keywords": "物联网, 流程挖掘, 数据集成, 元模型, 事件日志", "comments": "该论文通过解决物联网和传统流程数据之间的数据碎片化问题，解决了流程挖掘中的一个相关问题。所提出的核心元模型通过综合现有方法，提供了一个标准化的解决方案，可以显著提高数据互操作性和协作。使用Python原型进行验证增加了实用价值。其以对象为中心的方法对于处理物联网数据的多样性似乎很有前景。"}}
{"id": "2506.21543", "title": "Detecting weighted hidden cliques", "authors": ["Urmisha Chatterjee", "Karissa Huang", "Ritabrata Karmakar", "B. R. Vinay Kumar", "Gábor Lugosi", "Nandan Malhotra", "Anirban Mandal", "Maruf Alam Tarafdar"], "summary": "We study a generalization of the classical hidden clique problem to graphs\nwith real-valued edge weights. Formally, we define a hypothesis testing\nproblem. Under the null hypothesis, edges of a complete graph on $n$ vertices\nare associated with independent and identically distributed edge weights from a\ndistribution $P$. Under the alternate hypothesis, $k$ vertices are chosen at\nrandom and the edge weights between them are drawn from a distribution $Q$,\nwhile the remaining are sampled from $P$. The goal is to decide, upon observing\nthe edge weights, which of the two hypotheses they were generated from. We\ninvestigate the problem under two different scenarios: (1) when $P$ and $Q$ are\ncompletely known, and (2) when there is only partial information of $P$ and\n$Q$. In the first scenario, we obtain statistical limits on $k$ when the two\nhypotheses are distinguishable, and when they are not. Additionally, in each of\nthe scenarios, we provide bounds on the minimal risk of the hypothesis testing\nproblem when $Q$ is not absolutely continuous with respect to $P$. We also\nprovide computationally efficient spectral tests that can distinguish the two\nhypotheses as long as $k=\\Omega(\\sqrt{n})$ in both the scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21543v1", "categories": ["math.ST", "cs.IT", "math.IT", "math.PR", "stat.TH", "62F03"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.21543v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "检测加权隐藏团", "tldr": "本文研究了加权图上的隐藏团检测问题，将其建模为假设检验，并在P和Q已知或部分已知的情况下，给出了统计极限、最小风险界限以及计算高效的谱检验方法，其中谱检验在k=Omega(sqrt(n))时有效。", "motivation": "经典隐藏团问题的推广，研究加权图上的隐藏团检测，将其形式化为统计假设检验问题。", "method": "1. 定义了一个假设检验问题：零假设下，边权服从分布P；备择假设下，k个顶点的边权服从分布Q，其余服从P。2. 在两种场景下进行研究：(1) P和Q完全已知；(2) P和Q只有部分信息。3. 提供了计算高效的谱检验方法。", "result": "1. 在P和Q完全已知的情况下，获得了两个假设可区分和不可区分时k的统计极限。2. 在两种场景下，当Q不相对于P绝对连续时，给出了假设检验问题的最小风险界限。3. 提供了计算高效的谱检验，在两种场景下只要k=Omega(sqrt(n))就能区分两个假设。", "conclusion": "本文成功地将经典隐藏团问题推广到加权图，并提供了在不同信息量下检测加权隐藏团的统计极限、风险界限和高效计算方法。", "translation": "我们研究了经典隐藏团问题在实值边权图上的推广。形式上，我们定义了一个假设检验问题。在零假设下，一个n个顶点的完全图的边与来自分布P的独立同分布的边权相关联。在备择假设下，随机选择k个顶点，它们之间的边权从分布Q中抽取，而其余边权从P中抽取。目标是在观察到边权后，判断它们是由哪种假设生成的。我们在两种不同的场景下研究了这个问题：(1) 当P和Q完全已知时，以及(2) 当P和Q只有部分信息时。在第一种场景中，我们获得了当两个假设可区分和不可区分时k的统计极限。此外，在每种场景中，当Q不相对于P绝对连续时，我们提供了假设检验问题的最小风险界限。我们还提供了计算高效的谱检验，在两种场景下只要k=Omega(sqrt(n))就能区分这两个假设。", "summary": "本文将经典的隐藏团问题推广到加权图，并将其建模为一个假设检验问题。研究在P和Q分布已知或部分已知两种场景下进行。主要贡献包括确定可区分时的统计极限、提供最小风险界限（尤其当Q不相对于P绝对连续时），以及提出在k达到Omega(sqrt(n))时计算高效的谱检验方法。", "keywords": "加权隐藏团, 假设检验, 统计极限, 谱检验, 图算法", "comments": "这篇论文通过将经典的隐藏团问题推广到加权图，并采用严格的统计假设检验框架进行分析，具有重要的理论意义。它不仅探讨了统计可区分性的极限，还考虑了实际应用中P和Q信息不完全的情况，并提出了计算上可行的谱方法，这对于处理大规模加权图数据中的隐藏结构具有指导价值。"}}
{"id": "2506.20879", "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "summary": "Generation of images containing multiple humans, performing complex actions,\nwhile preserving their facial identities, is a significant challenge. A major\nfactor contributing to this is the lack of a a dedicated benchmark. To address\nthis, we introduce MultiHuman-Testbench, a novel benchmark for rigorously\nevaluating generative models for multi-human generation. The benchmark\ncomprises 1800 samples, including carefully curated text prompts, describing a\nrange of simple to complex human actions. These prompts are matched with a\ntotal of 5,550 unique human face images, sampled uniformly to ensure diversity\nacross age, ethnic background, and gender. Alongside captions, we provide\nhuman-selected pose conditioning images which accurately match the prompt. We\npropose a multi-faceted evaluation suite employing four key metrics to quantify\nface count, ID similarity, prompt alignment, and action detection. We conduct a\nthorough evaluation of a diverse set of models, including zero-shot approaches\nand training-based methods, with and without regional priors. We also propose\nnovel techniques to incorporate image and region isolation using human\nsegmentation and Hungarian matching, significantly improving ID similarity. Our\nproposed benchmark and key findings provide valuable insights and a\nstandardized tool for advancing research in multi-human image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20879v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20879v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "MultiHuman-Testbench：多人物图像生成基准测试", "tldr": "MultiHuman-Testbench是一个新的基准测试，用于评估多人物图像生成模型，它包含精心策划的文本提示、人脸图像和姿态条件图像，并提出多方面评估指标，以解决现有模型在多人物生成方面的挑战。", "motivation": "生成包含多个复杂动作人物且能保持面部身份的图像是一个重大挑战，主要原因是缺乏专门的基准测试。", "method": "引入MultiHuman-Testbench基准测试，包含1800个样本（文本提示）和5550张独特人脸图像，以及人工选择的姿态条件图像。提出多方面评估套件，采用四项关键指标：人脸计数、身份相似度、提示对齐和动作检测。评估了包括零样本和基于训练的方法。提出了结合人类分割和匈牙利匹配的图像和区域隔离新技术，以提高身份相似度。", "result": "对多种模型进行了全面评估，包括零样本方法和基于训练的方法，无论是否使用区域先验。提出的新图像和区域隔离技术显著提高了身份相似度。", "conclusion": "提出的基准测试和关键发现为多人物图像生成领域的研究进展提供了有价值的见解和标准化工具。", "translation": "生成包含多个复杂动作人物且能保持面部身份的图像是一个重大挑战。造成这一问题的一个主要因素是缺乏专门的基准测试。为了解决这个问题，我们引入了MultiHuman-Testbench，这是一个新颖的基准测试，用于严格评估多人物生成模型。该基准测试包含1800个样本，包括精心策划的文本提示，描述了一系列从简单到复杂的人类动作。这些提示与总共5550张独特的人脸图像相匹配，这些图像经过均匀采样，以确保年龄、种族背景和性别多样性。除了标题，我们还提供了人工选择的姿态条件图像，这些图像与提示准确匹配。我们提出了一套多方面的评估套件，采用四项关键指标来量化人脸计数、身份相似度、提示对齐和动作检测。我们对一系列多样化的模型进行了全面评估，包括零样本方法和基于训练的方法，无论是否使用区域先验。我们还提出了结合人类分割和匈牙利匹配的图像和区域隔离的新技术，显著提高了身份相似度。我们提出的基准测试和关键发现为多人物图像生成领域的研究进展提供了有价值的见解和标准化工具。", "summary": "本研究引入了MultiHuman-Testbench，一个专门用于评估多人物图像生成模型的基准测试。该基准包含1800个文本提示、5550张多样化人脸图像和人工选择的姿态条件图像。为量化模型性能，研究提出了包含人脸计数、身份相似度、提示对齐和动作检测在内的四项关键评估指标。此外，还提出了利用人类分割和匈牙利匹配的新技术以提升身份相似度。该基准测试旨在解决现有模型在生成多人物图像时面临的挑战，并为该领域的研究提供标准化工具和有价值的见解。", "keywords": "多人物图像生成, 基准测试, 人脸身份, 姿态条件, 评估指标", "comments": "该论文通过引入一个专门的基准测试MultiHuman-Testbench，解决了多人物图像生成领域中缺乏标准化评估工具的关键问题。其创新之处在于结合了多样化的数据（文本提示、人脸图像、姿态条件），并提出了多维度的评估指标。特别是引入人类分割和匈牙利匹配来提高身份相似度的技术，具有很强的实用价值。该工作为推动多人物图像生成技术的发展奠定了坚实的基础，具有重要的研究意义。"}}
{"id": "2506.21467", "title": "Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces", "authors": ["Michael Johnston", "Burkhard Ringlein", "Christoph Hagleitner", "Alessandro Pomponio", "Vassilis Vassiliadis", "Christian Pinto", "Srikumar Venugopal"], "summary": "Finding the optimal set of cloud resources to deploy a given workload at\nminimal cost while meeting a defined service level agreement is an active area\nof research. Combining tens of parameters applicable across a large selection\nof compute, storage, and services offered by cloud providers with similar\nnumbers of application-specific parameters leads to configuration spaces with\nmillions of deployment options.\n  In this paper, we propose Discovery Space, an abstraction that formalizes the\ndescription of workload configuration problems, and exhibits a set of\ncharacteristics required for structured, robust and distributed investigations\nof large search spaces. We describe a concrete implementation of the Discovery\nSpace abstraction and show that it is generalizable across a diverse set of\nworkloads such as Large Language Model inference and Big Data Analytics.\n  We demonstrate that our approach enables safe, transparent sharing of data\nbetween executions of best-of-breed optimizers increasing the efficiency of\noptimal configuration detection in large search spaces. We also demonstrate how\nDiscovery Spaces enable transfer and reuse of knowledge across similar search\nspaces, enabling configuration search speed-ups of over 90%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21467v1", "categories": ["cs.DC", "C.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.21467v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用发现空间进行高效且可重用的云配置搜索", "tldr": "本文提出了“发现空间”这一抽象概念，用于高效且可重用地搜索最佳云配置，通过实现数据共享和知识迁移，显著加速了大规模搜索空间中的优化过程。", "motivation": "在满足服务水平协议的同时，以最低成本为给定工作负载寻找最佳云资源配置是一个活跃的研究领域。由于云提供商和应用程序的参数众多，导致配置空间包含数百万个部署选项，这使得寻找最优配置变得非常复杂和具有挑战性。", "method": "本文提出了“发现空间”（Discovery Space），这是一种抽象概念，它形式化了工作负载配置问题的描述，并具有大规模搜索空间结构化、健壮和分布式调查所需的一系列特性。文中描述了发现空间抽象的具体实现，并展示了其在大型语言模型推理和大数据分析等多种工作负载中的通用性。", "result": "研究表明，该方法能够实现最佳优化器执行之间安全、透明的数据共享，从而提高在大规模搜索空间中检测最佳配置的效率。此外，发现空间还能够实现知识在类似搜索空间之间的转移和重用，从而使配置搜索速度提高90%以上。", "conclusion": "“发现空间”为云配置搜索提供了一种高效且可重用的方法，通过实现数据共享和知识迁移，显著加快了最佳配置的检测速度。", "translation": "寻找以最低成本部署给定工作负载同时满足定义的服务水平协议的最佳云资源集是一个活跃的研究领域。将适用于云提供商提供的大量计算、存储和服务的数十个参数与类似数量的特定于应用程序的参数结合起来，导致配置空间中存在数百万个部署选项。\n在本文中，我们提出了“发现空间”（Discovery Space），这是一种抽象，它形式化了工作负载配置问题的描述，并展现了大规模搜索空间结构化、健壮和分布式调查所需的一系列特性。我们描述了发现空间抽象的具体实现，并表明它可推广到各种工作负载，例如大型语言模型推理和大数据分析。\n我们证明了我们的方法能够在最佳优化器执行之间安全、透明地共享数据，从而提高在大规模搜索空间中检测最佳配置的效率。我们还展示了发现空间如何实现知识在类似搜索空间之间的转移和重用，从而使配置搜索速度提高90%以上。", "summary": "本文提出“发现空间”（Discovery Space）这一抽象概念，旨在形式化并高效探索大规模云配置搜索空间。该方法支持结构化和分布式调查，促进优化器之间的数据安全共享以及在类似工作负载间的知识迁移。它显著加速了最佳云资源配置过程，对大型语言模型推理和大数据分析等多样化工作负载的配置搜索速度提升超过90%。", "keywords": "云配置, 发现空间, 搜索空间优化, 知识迁移, 资源管理", "comments": "本文的创新之处在于提出了“发现空间”作为云配置搜索的形式化抽象，有效解决了大规模参数空间带来的复杂性。其实现数据共享和知识迁移，从而显著提升搜索速度（超过90%）的能力是关键贡献，使得云资源优化更加高效，并可在不同工作负载之间重用。"}}
{"id": "2506.21532", "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets", "authors": ["Akshay Paruchuri", "Maryam Aziz", "Rohit Vartak", "Ayman Ali", "Best Uchehara", "Xin Liu", "Ishan Chatterjee", "Monica Agrawal"], "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat", "comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release", "pdf_url": "http://arxiv.org/pdf/2506.21532v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21532v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "“医生，你好吗？”：分析用户如何在大型对话式AI数据集中寻求健康信息", "tldr": "本研究通过构建HealthChat-11K数据集，系统分析了用户如何利用大型语言模型寻求健康信息，并揭示了现有模型在医疗支持能力上的不足。", "motivation": "人们越来越多地通过交互式聊天机器人向大型语言模型(LLMs)寻求医疗保健信息，然而这些对话的性质和固有风险在很大程度上仍未被探索。", "method": "作者过滤了大规模对话式AI数据集，构建了HealthChat-11K，一个包含1.1万个真实对话和2.5万条用户消息的精选数据集。研究利用HealthChat-11K和一个由临床医生驱动的用户与LLM互动分类法，系统地研究了21个不同健康专业的用户互动。", "result": "分析揭示了用户如何以及为何寻求健康信息的洞察，例如常见的互动模式、上下文不完整的情况、情感行为，以及可能诱导奉承的互动（如引导性问题）。", "conclusion": "研究强调了部署为对话式AI的LLMs在医疗保健支持能力方面需要改进。", "translation": "人们越来越多地通过交互式聊天机器人向大型语言模型（LLMs）寻求医疗保健信息，然而这些对话的性质和固有风险在很大程度上仍未被探索。在本文中，我们筛选了大规模对话式AI数据集，以构建HealthChat-11K，这是一个包含1.1万个真实世界对话（由2.5万条用户消息组成）的精选数据集。我们使用HealthChat-11K和一个由临床医生驱动的用户在寻求医疗保健信息时与LLMs互动的方式分类法，系统地研究了21个不同健康专业的用户互动。我们的分析揭示了关于用户如何以及为何寻求健康信息的本质洞察，例如常见的互动、上下文不完整的情况、情感行为，以及可能诱导奉承的互动（例如引导性问题），这突显了部署为对话式AI的LLMs在医疗保健支持能力方面需要改进。获取我们的分析代码和工件以及将它们组合成精选数据集的链接在此：https://github.com/yahskapar/HealthChat", "summary": "本研究旨在探索用户如何通过大型语言模型（LLMs）寻求健康信息，因为当前对话的性质和风险尚未被充分研究。作者构建了一个名为HealthChat-11K的精选数据集，包含1.1万个真实对话。通过结合该数据集和临床医生驱动的分类法，研究系统分析了21个健康专业中用户与LLMs的互动。研究发现包括常见互动模式、上下文缺失、情感行为以及可能导致LLM产生奉承的互动，强调了LLMs在提供医疗保健支持方面需要改进。", "keywords": "健康信息, 大型语言模型, 对话式AI, 用户互动, 医疗保健", "comments": "这篇论文通过构建和分析大规模真实世界对话数据集HealthChat-11K，填补了LLM在医疗健康信息查询领域研究的空白。其创新之处在于使用了临床医生驱动的分类法对用户互动进行系统性研究，揭示了用户行为模式和LLM现有局限，特别是上下文不完整和潜在的奉承问题，为未来LLM在医疗领域的改进提供了重要方向。"}}
{"id": "2506.20790", "title": "Stochastic Parameter Decomposition", "authors": ["Lucius Bushnaq", "Dan Braun", "Lee Sharkey"], "summary": "A key step in reverse engineering neural networks is to decompose them into\nsimpler parts that can be studied in relative isolation. Linear parameter\ndecomposition -- a framework that has been proposed to resolve several issues\nwith current decomposition methods -- decomposes neural network parameters into\na sum of sparsely used vectors in parameter space. However, the current main\nmethod in this framework, Attribution-based Parameter Decomposition (APD), is\nimpractical on account of its computational cost and sensitivity to\nhyperparameters. In this work, we introduce \\textit{Stochastic Parameter\nDecomposition} (SPD), a method that is more scalable and robust to\nhyperparameters than APD, which we demonstrate by decomposing models that are\nslightly larger and more complex than was possible to decompose with APD. We\nalso show that SPD avoids other issues, such as shrinkage of the learned\nparameters, and better identifies ground truth mechanisms in toy models. By\nbridging causal mediation analysis and network decomposition methods, this\ndemonstration opens up new research possibilities in mechanistic\ninterpretability by removing barriers to scaling linear parameter decomposition\nmethods to larger models. We release a library for running SPD and reproducing\nour experiments at https://github.com/goodfire-ai/spd.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20790v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20790v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "随机参数分解", "tldr": "提出了一种名为随机参数分解（SPD）的新方法，解决了现有神经网络分解方法（APD）的计算成本高和超参数敏感性问题，使其能应用于更大的模型并促进机制可解释性研究。", "motivation": "逆向工程神经网络的一个关键步骤是将其分解为更简单的部分。现有线性参数分解框架中的主要方法，归因参数分解（APD），因其计算成本高和对超参数敏感而不够实用。", "method": "引入了随机参数分解（SPD）方法，该方法通过将神经网络参数分解为参数空间中稀疏使用的向量之和，并结合因果中介分析和网络分解方法，以实现比APD更好的可扩展性和鲁棒性。", "result": "SPD比APD更具可扩展性和对超参数的鲁棒性，能够分解比APD更大、更复杂的模型。它还避免了学习参数的收缩等问题，并在玩具模型中更好地识别了真实机制。", "conclusion": "通过弥合因果中介分析和网络分解方法之间的鸿沟，SPD为将线性参数分解方法扩展到更大模型消除了障碍，从而在机制可解释性方面开辟了新的研究可能性。", "translation": "逆向工程神经网络的一个关键步骤是将其分解为可以相对独立研究的更简单的部分。线性参数分解——一个旨在解决当前分解方法中若干问题的框架——将神经网络参数分解为参数空间中稀疏使用的向量之和。然而，该框架中当前的主要方法，基于归因的参数分解（APD），因其计算成本和对超参数的敏感性而变得不切实际。在这项工作中，我们引入了“随机参数分解”（SPD），这是一种比APD更具可扩展性且对超参数更鲁棒的方法，我们通过分解比APD可能分解的模型略大且更复杂的模型来证明这一点。我们还表明，SPD避免了其他问题，例如学习参数的收缩，并在玩具模型中更好地识别了真实机制。通过弥合因果中介分析和网络分解方法之间的鸿沟，这一演示通过消除将线性参数分解方法扩展到更大模型的障碍，为机制可解释性开辟了新的研究可能性。我们发布了一个用于运行SPD和重现我们实验的库，地址为https://github.com/goodfire-ai/spd。", "summary": "本文提出了一种名为随机参数分解（SPD）的新方法，旨在解决现有神经网络线性参数分解方法（如归因参数分解APD）在计算成本和超参数敏感性方面的局限性。SPD被证明比APD更具可扩展性和鲁棒性，能够分解更大、更复杂的模型，并有效避免了参数收缩等问题。该工作通过连接因果中介分析和网络分解，为大规模神经网络的机制可解释性研究提供了新的途径。", "keywords": "随机参数分解, 神经网络, 机制可解释性, 参数分解, 逆向工程", "comments": "这项工作通过引入随机参数分解（SPD），显著提升了神经网络参数分解方法的实用性和可扩展性。其创新之处在于提高了对超参数的鲁棒性和处理更大模型的能力，这对于深度学习模型的可解释性研究至关重要。通过解决现有方法的关键限制，SPD为机制可解释性（mechanistic interpretability）领域开辟了新的研究方向，尤其是在理解和逆向工程复杂神经网络方面具有重要意义。"}}
{"id": "2506.21207", "title": "Estimation of superconducting cavity bandwidth and detuning using a Luenberger observer", "authors": ["Bozo Richter", "Andrea Bellandi", "Julien Branlard", "Leon Speidel", "Annika Eichler"], "summary": "Enabled by progress in superconducting technology, several continuous wave\nlinear accelerators are foreseen in the next decade. For these machines, it is\nof crucial importance to track the main cavity parameters, such as the\nresonator bandwidth and detuning. The bandwidth yields information on the\nsuperconducting state of the cavity. The detuning should be minimized to limit\nthe required power to operate the cavity. The estimation of these parameters is\ncommonly implemented in the digital electronics of the Low-Level RF control\nsystem to minimize the computation delay. In this proceeding, we present a way\nto compute the bandwidth and detuning using a Luenberger observer. In contrast\nto previous methods, a state observer yields estimations at the native control\nsystem sample rate without explicitly filtering the input signals.\nAdditionally, the error convergence properties of the estimations can be\ncontrolled intuitively by adjusting gain parameters. Implementation\nconsiderations and test results on the derived observer are presented in the\nmanuscript.", "comment": "10 pages, 4 figures, to be published in APS Physical Review -\n  Accelerator and Beams", "pdf_url": "http://arxiv.org/pdf/2506.21207v1", "categories": ["physics.acc-ph", "cs.SY", "eess.SY"], "cate": "physics.acc-ph", "url": "http://arxiv.org/abs/2506.21207v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "超导腔带宽和失谐的Luenberger观测器估计", "tldr": "本文提出使用Luenberger观测器估计超导腔的带宽和失谐，该方法无需显式滤波，且估计误差收敛性可控。", "motivation": "未来十年将出现多个连续波直线加速器，需要精确跟踪超导腔的带宽和失谐等关键参数。带宽反映腔体超导状态，失谐需最小化以降低功耗。传统估计方法可能存在计算延迟。", "method": "本文提出使用Luenberger观测器来计算超导腔的带宽和失谐。与现有方法不同，该状态观测器能在控制系统原始采样率下提供估计，无需显式过滤输入信号。此外，估计的误差收敛特性可通过调整增益参数直观控制。", "result": "论文介绍了使用Luenberger观测器估计超导腔带宽和失谐的方法，并展示了其实现考虑和测试结果。该方法能以原始采样率提供估计，无需显式滤波，且误差收敛性可控。", "conclusion": "Luenberger观测器是一种有效且可控的超导腔带宽和失谐估计方法，优于传统方法，有助于未来直线加速器的参数跟踪。", "translation": "由于超导技术的进步，未来十年预计将出现多个连续波直线加速器。对于这些机器，跟踪主要腔体参数，如谐振器带宽和失谐，至关重要。带宽提供了腔体超导状态的信息。失谐应最小化以限制操作腔体所需的功率。这些参数的估计通常在低电平射频控制系统的数字电子中实现，以最小化计算延迟。在本论文中，我们提出了一种使用Luenberger观测器计算带宽和失谐的方法。与以前的方法不同，状态观测器能够在原始控制系统采样率下提供估计，而无需显式过滤输入信号。此外，估计的误差收敛特性可以通过调整增益参数直观控制。手稿中介绍了派生观测器的实现考虑和测试结果。", "summary": "本文提出了一种基于Luenberger观测器的新方法，用于估计超导腔的带宽和失谐。该方法克服了传统方法中显式滤波和潜在计算延迟的问题，能在控制系统原始采样率下提供精确估计，并且其误差收敛特性可通过增益参数进行直观控制。这对于未来连续波直线加速器中关键腔体参数的实时、精确跟踪具有重要意义。", "keywords": "超导腔, 带宽, 失谐, Luenberger观测器, 低电平射频", "comments": "该论文的创新之处在于将Luenberger观测器应用于超导腔参数估计，显著改进了传统方法。其优势在于无需显式滤波即可在原始采样率下获得估计，且估计误差的收敛性可控，这对于提高低电平射频控制系统的响应速度和精度，尤其是在对实时性要求高的加速器应用中，具有重要价值。"}}
{"id": "2506.21325", "title": "Localization-Based Beam Focusing in Near-Field Communications", "authors": ["Nima Mozaffarikhosravi", "Prathapasinghe Dharmawansa", "Italo Atzeni"], "summary": "Shifting 6G-and-beyond wireless communication systems to higher frequency\nbands and the utilization of massive multiple-input multiple-output arrays will\nextend the near-field region, affecting beamforming and user localization\nschemes. In this paper, we propose a localization-based beam-focusing strategy\nthat leverages the dominant line-of-sight (LoS) propagation arising at mmWave\nand sub-THz frequencies. To support this approach, we analyze the 2D-MUSIC\nalgorithm for distance estimation by examining its spectrum in simplified,\ntractable setups with minimal numbers of antennas and users. Lastly, we compare\nthe proposed localization-based beam focusing, with locations estimated via\n2D-MUSIC, with zero forcing with pilot-based channel estimation in terms of\nuplink sum spectral efficiency. Our numerical results show that the proposed\nmethod becomes more effective under LoS-dominated propagation, short coherence\nblocks, and strong noise power arising at high carrier frequencies and with\nlarge bandwidths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21325v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21325v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于定位的近场通信波束聚焦", "tldr": "该论文提出了一种基于定位的近场通信波束聚焦策略，特别适用于视距（LoS）主导的高频环境。", "motivation": "将6G及未来无线通信系统转向更高频段并利用大规模多输入多输出阵列将扩展近场区域，这会影响波束成形和用户定位方案，因此需要新的策略。", "method": "本文提出了一种利用主导视距（LoS）传播的基于定位的波束聚焦策略。为了支持该方法，论文分析了用于距离估计的2D-MUSIC算法，并在简化的设置中检查其频谱。最后，将通过2D-MUSIC估计位置的所提方法与基于导频信道估计的迫零算法在上行链路总频谱效率方面进行了比较。", "result": "数值结果表明，在LoS主导传播、短相干块以及高载波频率和大带宽下出现的强噪声功率条件下，所提出的方法变得更加有效。", "conclusion": "基于定位的波束聚焦方法，特别是当使用2D-MUSIC进行定位时，在特定的挑战性近场通信场景中，尤其是在视距条件主导的场景中，具有优势。", "translation": "将6G及未来无线通信系统转向更高频段以及大规模多输入多输出阵列的利用将扩展近场区域，影响波束成形和用户定位方案。在本文中，我们提出了一种基于定位的波束聚焦策略，该策略利用了毫米波和亚太赫兹频率下出现的主导视距（LoS）传播。为了支持这种方法，我们通过在天线和用户数量最少的简化、易处理的设置中检查其频谱，分析了用于距离估计的2D-MUSIC算法。最后，我们比较了所提出的基于定位的波束聚焦（通过2D-MUSIC估计位置）与基于导频信道估计的迫零算法在上行链路总频谱效率方面的性能。我们的数值结果表明，在LoS主导传播、短相干块以及高载波频率和大带宽下出现的强噪声功率条件下，所提出的方法变得更加有效。", "summary": "该论文提出了一种针对近场6G及未来通信的基于定位的波束聚焦策略，利用毫米波和亚太赫兹频率下主导的视距（LoS）传播。论文分析了2D-MUSIC算法用于距离估计，并比较了所提方法与基于导频的迫零算法在上行链路总频谱效率方面的性能。结果显示，在LoS主导环境、短相干块和高噪声功率下，所提方法更有效。", "keywords": "定位, 波束聚焦, 近场通信, 2D-MUSIC, 视距传播", "comments": "该论文解决了未来无线系统（6G及以后）中近场效应日益突出的关键挑战。利用定位进行波束聚焦是一种创新方法，尤其是在视距主导的场景中。对2D-MUSIC算法进行距离估计的分析是实现该方法的关键组成部分。其在特定挑战性条件（强噪声、短相干）下的有效性突显了其潜在的适用性。"}}
{"id": "2506.21185", "title": "Out-of-Distribution Semantic Occupancy Prediction", "authors": ["Yuheng Zhang", "Mengfei Duan", "Kunyu Peng", "Yuhang Wang", "Ruiping Liu", "Fei Teng", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "summary": "3D Semantic Occupancy Prediction is crucial for autonomous driving, providing\na dense, semantically rich environmental representation. However, existing\nmethods focus on in-distribution scenes, making them susceptible to\nOut-of-Distribution (OoD) objects and long-tail distributions, which increases\nthe risk of undetected anomalies and misinterpretations, posing safety hazards.\nTo address these challenges, we introduce Out-of-Distribution Semantic\nOccupancy Prediction, targeting OoD detection in 3D voxel space. To fill the\ngaps in the dataset, we propose a Synthetic Anomaly Integration Pipeline that\ninjects synthetic anomalies while preserving realistic spatial and occlusion\npatterns, enabling the creation of two datasets: VAA-KITTI and VAA-KITTI-360.\nWe introduce OccOoD, a novel framework integrating OoD detection into 3D\nsemantic occupancy prediction, with Voxel-BEV Progressive Fusion (VBPF)\nleveraging an RWKV-based branch to enhance OoD detection via geometry-semantic\nfusion. Experimental results demonstrate that OccOoD achieves state-of-the-art\nOoD detection with an AuROC of 67.34% and an AuPRCr of 29.21% within a 1.2m\nregion, while maintaining competitive occupancy prediction performance. The\nestablished datasets and source code will be made publicly available at\nhttps://github.com/7uHeng/OccOoD.", "comment": "The established datasets and source code will be made publicly\n  available at https://github.com/7uHeng/OccOoD", "pdf_url": "http://arxiv.org/pdf/2506.21185v1", "categories": ["cs.CV", "cs.RO", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21185v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "分布外语义占据预测", "tldr": "本文提出了一种名为OccOoD的新框架，用于在3D体素空间中检测自动驾驶场景中的分布外（OoD）对象，通过合成异常数据和Voxel-BEV渐进融合技术，实现了最先进的OoD检测性能。", "motivation": "现有的3D语义占据预测方法主要关注分布内场景，对分布外（OoD）对象和长尾分布敏感，这增加了未检测到的异常和误解的风险，从而构成安全隐患。", "method": "本文提出了“分布外语义占据预测”来解决3D体素空间中的OoD检测问题。为弥补数据集空白，引入了合成异常集成管道（Synthetic Anomaly Integration Pipeline），用于注入合成异常并创建了VAA-KITTI和VAA-KITTI-360两个数据集。同时，提出了OccOoD框架，将OoD检测集成到3D语义占据预测中，并通过Voxel-BEV渐进融合（VBPF）利用基于RWKV的分支，通过几何-语义融合增强OoD检测能力。", "result": "实验结果表明，OccOoD在1.2米区域内实现了最先进的OoD检测，AuROC达到67.34%，AuPRCr达到29.21%，同时保持了有竞争力的占据预测性能。", "conclusion": "OccOoD框架在处理自动驾驶中的分布外语义占据预测问题上表现出色，通过引入新的合成数据集和创新的融合机制，显著提升了OoD检测的性能，证明了其在增强自动驾驶系统安全性方面的潜力。数据集和源代码已公开。", "translation": "3D语义占据预测对于自动驾驶至关重要，它提供了密集的、语义丰富的环境表示。然而，现有方法主要关注分布内场景，使其容易受到分布外（OoD）对象和长尾分布的影响，这增加了未检测到的异常和误解的风险，从而构成安全隐患。为了解决这些挑战，我们引入了分布外语义占据预测，旨在3D体素空间中进行OoD检测。为了填补数据集的空白，我们提出了一个合成异常集成管道，该管道在保持真实空间和遮挡模式的同时注入合成异常，从而能够创建两个数据集：VAA-KITTI和VAA-KITTI-360。我们引入了OccOoD，这是一个将OoD检测集成到3D语义占据预测中的新型框架，其中Voxel-BEV渐进融合（VBPF）利用基于RWKV的分支，通过几何-语义融合增强OoD检测。实验结果表明，OccOoD在1.2米区域内实现了最先进的OoD检测，AuROC达到67.34%，AuPRCr达到29.21%，同时保持了有竞争力的占据预测性能。所建立的数据集和源代码将在https://github.com/7uHeng/OccOoD公开。", "summary": "本文针对自动驾驶中3D语义占据预测现有方法在处理分布外（OoD）对象时的局限性，提出了一种名为“分布外语义占据预测”的新范式。为解决数据稀缺问题，作者开发了合成异常集成管道，并创建了VAA-KITTI和VAA-KITTI-360数据集。在此基础上，引入了OccOoD框架，它通过Voxel-BEV渐进融合（VBPF）集成OoD检测功能，并利用RWKV分支进行几何-语义融合以增强检测。实验证明，OccOoD在OoD检测方面达到了最先进的性能，同时保持了良好的占据预测能力。", "keywords": "分布外检测, 语义占据预测, 自动驾驶, 异常检测, OccOoD", "comments": "该论文的创新点在于其专注于自动驾驶中的分布外（OoD）对象检测，这是一个关键的安全问题。通过引入合成异常集成管道来扩充数据集，并设计OccOoD框架及其中的Voxel-BEV渐进融合（VBPF）和RWKV分支，有效地将OoD检测融入到3D语义占据预测中。这对于提高自动驾驶系统在复杂和未知环境下的鲁棒性和安全性具有重要意义。数据集和代码的公开也为后续研究提供了便利。"}}
{"id": "2506.20932", "title": "Thinning to improve two-sample discrepancy", "authors": ["Gleb Smirnov", "Roman Vershynin"], "summary": "The discrepancy between two independent samples \\(X_1,\\dots,X_n\\) and\n\\(Y_1,\\dots,Y_n\\) drawn from the same distribution on $\\mathbb{R}^d$ typically\nhas order \\(O(\\sqrt{n})\\) even in one dimension. We give a simple online\nalgorithm that reduces the discrepancy to \\(O(\\log^{2d} n)\\) by discarding a\nsmall fraction of the points.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.20932v1", "categories": ["math.PR", "cs.DS"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.20932v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "稀疏化以改善两样本差异", "tldr": "通过丢弃少量点，一种简单的在线算法可以将两样本差异从O(sqrt(n))降低到O(log^(2d) n)。", "motivation": "两个独立样本之间的差异通常为O(sqrt(n))，即使在同一分布中也是如此，这表明存在较大的差异需要改进。", "method": "本文提出了一种简单的在线算法，通过丢弃一小部分点来减少样本差异。", "result": "该算法将两样本差异从O(sqrt(n))级别降低到O(log^(2d) n)级别。", "conclusion": "通过稀疏化（丢弃少量点），可以显著降低两个独立样本之间的差异。", "translation": "两个独立样本 \\(X_1,\\dots,X_n\\) 和 \\(Y_1,\\dots,Y_n\\) 从 $\\mathbb{R}^d$ 上的相同分布中抽取，即使在一维情况下，它们之间的差异通常也达到 \\(O(\\sqrt{n})\\) 级别。我们提出了一种简单的在线算法，通过丢弃一小部分点，将差异降低到 \\(O(\\log^{2d} n)\\) 级别。", "summary": "本文提出了一种简单的在线算法，用于降低两个独立样本之间的差异。该算法通过丢弃一小部分样本点，成功地将通常为 \\(O(\\sqrt{n})\\) 级别的差异显著改善至 \\(O(\\log^{2d} n)\\) 级别。", "keywords": "样本差异, 稀疏化, 在线算法, 随机抽样", "comments": "这篇论文的创新点在于提供了一种简单且高效的在线算法，通过“稀疏化”策略显著降低了样本差异的阶数，从多项式级别改善到对数级别，这在理论和实际应用中都具有重要意义。"}}
{"id": "2506.21242", "title": "Runge--Kutta generalized Convolution Quadrature for sectorial problems", "authors": ["Jing Guo", "Maria Lopez-Fernandez"], "summary": "We study the application of the generalized convolution quadrature (gCQ)\nbased on Runge--Kutta methods to approximate the solution of an important class\nof sectorial problems. The gCQ generalizes Lubich's original convolution\nquadrature (CQ) to variable steps. High-order versions of the gCQ have been\ndeveloped in the last decade, relying on certain Runge--Kutta methods. The\nRunge--Kutta based gCQ has been studied so far in a rather general setting,\nwhich includes applications to boundary integral formulations of wave problems.\nThe available stability and convergence results for these new methods are\nsuboptimal compared to those known for the uniform-step CQ, both in terms of\nconvergence order and regularity requirements of the data. Here we focus on a\nspecial class of sectorial problems and prove that in these important\napplications it is possible to achieve the same order of convergence as for the\noriginal CQ, under the same regularity hypotheses on the data, and for very\ngeneral time meshes. In the particular case of data with some known algebraic\ntype of singularity, we also show how to choose an optimally graded time mesh\nto achieve convergence with maximal order, overcoming the well-known order\nreduction of the original CQ in these situations. An important advantage of the\ngCQ method is that it allows for a fast and memory-efficient implementation. We\ndescribe how the fast and oblivious Runge--Kutta based gCQ can be implemented\nand illustrate our theoretical results with several numerical experiments. The\ncodes implementing the examples in this article are available in [13].", "comment": "35 pages, 26 figures", "pdf_url": "http://arxiv.org/pdf/2506.21242v1", "categories": ["math.NA", "cs.NA", "65R20, 65L06, 65M15, 26A33, 35R11"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21242v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Runge--Kutta 广义卷积积分法求解扇形问题", "tldr": "本文证明了Runge-Kutta广义卷积积分法（gCQ）在处理扇形问题时，即使采用可变步长和奇异数据，也能达到与原始均匀步长卷积积分法（CQ）相同的最优收敛阶数。", "motivation": "现有的基于Runge--Kutta的广义卷积积分法（gCQ）在收敛阶数和数据正则性要求方面，其稳定性和收敛性结果与均匀步长卷积积分法（CQ）相比次优。此外，原始CQ在处理奇异数据时存在阶数降低的问题。", "method": "本文研究了基于Runge--Kutta方法的广义卷积积分法（gCQ）在逼近一类扇形问题解方面的应用。作者证明了在特定条件下，该方法可以达到与原始CQ相同的收敛性能，并针对具有已知代数奇点的数据，提出了选择最优分级时间网格的方法，以实现最大阶收敛。同时，该方法支持快速且内存高效的实现。", "result": "对于扇形问题，基于Runge--Kutta的gCQ在相同的数据正则性假设下，以及对于非常通用的时间网格，可以达到与原始CQ相同的收敛阶数。对于具有某些已知代数类型奇点的数据，通过选择最优分级时间网格，可以实现最大阶收敛，克服了原始CQ在这些情况下的阶数降低问题。此外，gCQ方法允许快速和内存高效的实现。", "conclusion": "基于Runge--Kutta的广义卷积积分法（gCQ）在解决扇形问题时表现出高效性，能够实现最优收敛并有效处理奇异数据，同时具有快速和内存高效的实现优势。", "translation": "我们研究了基于Runge--Kutta方法的广义卷积积分法（gCQ）在逼近一类重要扇形问题解方面的应用。gCQ将Lubich的原始卷积积分法（CQ）推广到可变步长。在过去十年中，高阶版本的gCQ已得到发展，它们依赖于某些Runge--Kutta方法。迄今为止，基于Runge--Kutta的gCQ已在相当通用的设置中进行了研究，其中包括应用于波问题的边界积分公式。与均匀步长CQ的已知结果相比，这些新方法的现有稳定性和收敛性结果次优，无论是在收敛阶数还是数据正则性要求方面。在这里，我们专注于一类特殊的扇形问题，并证明在这些重要应用中，在相同的数据正则性假设下，以及对于非常通用的时间网格，可以实现与原始CQ相同的收敛阶数。对于具有某种已知代数类型奇点的数据的特殊情况，我们还展示了如何选择最优分级时间网格以实现最大阶收敛，从而克服了原始CQ在这些情况下众所周知的阶数降低问题。gCQ方法的一个重要优点是它允许快速和内存高效的实现。我们描述了如何实现快速且无感知的基于Runge--Kutta的gCQ，并通过几个数值实验说明了我们的理论结果。本文中实现示例的代码可在[13]中获取。", "summary": "本文探讨了基于Runge--Kutta方法的广义卷积积分法（gCQ）在解决扇形问题中的应用。研究表明，针对这类特定问题，gCQ即使在可变时间步长下，也能达到与传统均匀步长卷积积分法（CQ）相同的最优收敛阶数和数据正则性要求。此外，对于具有奇异性的数据，论文提出了选择最优分级时间网格的方法，以克服原始CQ的阶数降低问题，从而实现最大阶收敛。该方法还具有快速且内存高效的实现优势，并通过理论证明和数值实验得到了验证。", "keywords": "Runge-Kutta, 广义卷积积分, 扇形问题, 可变步长, 奇点", "comments": "该论文通过解决基于Runge--Kutta的广义卷积积分法（gCQ）在特定重要问题（扇形问题）上的次优性，做出了重要贡献。其能够在可变步长下实现最优收敛，并处理奇异数据，同时保持计算效率，是现有方法的一个显著改进。提供代码也增强了其实用价值。"}}
{"id": "2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.20993v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20993v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SAC：一种用于测量和诱导LLM个性特征并进行动态强度控制的框架", "tldr": "本文提出了SAC框架，通过扩展MPI并结合16PF模型，实现对LLM个性特征的动态强度控制，克服了现有模型仅提供粗略维度且缺乏强度控制的限制。", "motivation": "现有的大语言模型人格模型主要依赖于“大五人格”框架，只提供粗糙的个性维度，并且缺乏控制特质强度的机制，无法满足人们对LLM展现类人个性的日益增长的期望。", "method": "本文通过将原使用“大五人格”模型的机器个性清单（MPI）扩展至包含16种个性因素（16PF）模型，以实现对16种不同特质的表达控制。同时，开发了一个名为特定属性控制（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。该方法引入了基于形容词的语义锚定来指导特质强度表达，并利用了跨越五个强度因素（频率、深度、阈值、努力和意愿）的行为问题。", "result": "实验发现，将强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，观察到目标特质强度的变化会系统地影响心理学上密切相关的特质，表明LLM内化了多维个性结构，而非孤立地处理特质。", "conclusion": "本文的工作为医疗保健、教育和面试等领域受控且细致的人机交互开辟了新途径，使我们离真正类人社交机器更近一步。", "translation": "近年来，大型语言模型（LLM）在广泛的领域中获得了显著关注。人们对它们在交互过程中展现类人个性的期望也日益增长。为了满足这一期望，许多研究提出了通过心理测量评估来建模LLM个性的方法。然而，大多数现有模型面临两个主要限制：它们依赖于“大五人格”（OCEAN）框架，该框架只提供粗糙的个性维度；并且它们缺乏控制特质强度的机制。在本文中，我们通过扩展机器个性清单（MPI）来弥补这一空白，MPI最初使用“大五人格”模型，现在将其整合到16种个性因素（16PF）模型中，从而能够对十六种不同的特质进行表达控制。我们还开发了一个名为特定属性控制（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。我们的方法引入了基于形容词的语义锚定来指导特质强度表达，并利用了跨越五个强度因素（频率、深度、阈值、努力和意愿）的行为问题。通过实验，我们发现将强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，我们观察到目标特质强度的变化会系统地影响心理学上密切相关的特质，这表明LLM内化了多维个性结构，而非孤立地处理特质。我们的工作为医疗保健、教育和面试等领域受控且细致的人机交互开辟了新途径，使我们离真正类人社交机器更近一步。", "summary": "本文提出了SAC框架，用于测量和动态控制大型语言模型（LLM）的个性特征强度。针对现有模型仅依赖粗糙的“大五人格”维度且缺乏强度控制的问题，作者扩展了机器个性清单（MPI）以整合16种个性因素（16PF）模型，并引入了基于形容词的语义锚定和行为问题来指导和评估特质强度。实验结果表明，将强度视为连续谱能产生更一致和可控的个性表达，且LLM能内化多维个性结构。这项工作为实现更细致、类人的人机交互奠定了基础。", "keywords": "LLM个性, 动态强度控制, 16PF模型, SAC框架, 人机交互", "comments": "本文的创新之处在于提出了SAC框架，通过将16PF模型引入LLM个性建模，并实现了对个性特质的动态强度控制，克服了现有模型在精细度和控制力上的局限性。其引入的形容词语义锚定和五强度因素的行为问题设计具有独创性。研究结果表明LLM能够内化多维个性结构，这对于未来开发更具人性化和复杂交互能力的AI具有重要意义。该工作为LLM在医疗、教育等关键领域中的应用提供了新的可能性。"}}
{"id": "2506.20856", "title": "Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA", "authors": ["Fei Wang", "Baochun Li"], "summary": "Memorization in large language models (LLMs) makes them vulnerable to data\nextraction attacks. While pre-training memorization has been extensively\nstudied, fewer works have explored its impact in fine-tuning, particularly for\nLoRA fine-tuning, a widely adopted parameter-efficient method.\n  In this work, we re-examine memorization in fine-tuning and uncover a\nsurprising divergence from prior findings across different fine-tuning\nstrategies. Factors such as model scale and data duplication, which strongly\ninfluence memorization in pre-training and full fine-tuning, do not follow the\nsame trend in LoRA fine-tuning. Using a more relaxed similarity-based\nmemorization metric, we demonstrate that LoRA significantly reduces\nmemorization risks compared to full fine-tuning, while still maintaining strong\ntask performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20856v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20856v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "更精简的训练，更低的泄露：使用LoRA重新审视LLM微调中的记忆化", "tldr": "LoRA微调显著降低了LLM记忆化风险，且在模型规模和数据重复等因素上的表现与全量微调不同，同时保持了良好的任务性能。", "motivation": "大型语言模型（LLMs）中的记忆化使其容易受到数据提取攻击。虽然预训练中的记忆化已被广泛研究，但很少有工作探讨其在微调中的影响，特别是对于广泛采用的参数高效方法LoRA微调。", "method": "作者重新审视了微调中的记忆化，并使用一种更宽松的基于相似度的记忆化度量方法，比较了LoRA微调与全量微调的记忆化风险。", "result": "研究发现LoRA微调在记忆化方面与先前的发现存在显著差异，模型规模和数据重复等因素在LoRA微调中不再遵循与预训练和全量微调相同的趋势。LoRA与全量微调相比显著降低了记忆化风险，同时保持了强大的任务性能。", "conclusion": "LoRA微调是一种有效的降低LLM记忆化风险的方法，同时能保持良好的任务性能，并且其记忆化行为与传统微调方式不同。", "translation": "大型语言模型（LLMs）中的记忆化使其容易受到数据提取攻击。虽然预训练中的记忆化已被广泛研究，但很少有工作探讨其在微调中的影响，特别是对于广泛采用的参数高效方法LoRA微调。在这项工作中，我们重新审视了微调中的记忆化，并揭示了与不同微调策略先前发现的惊人差异。模型规模和数据重复等因素在预训练和全量微调中强烈影响记忆化，但在LoRA微调中不遵循相同的趋势。使用一种更宽松的基于相似度的记忆化度量方法，我们证明LoRA与全量微调相比显著降低了记忆化风险，同时仍保持强大的任务性能。", "summary": "本文重新审视了大型语言模型（LLMs）微调中的记忆化问题，特别关注了参数高效的LoRA微调方法。研究发现，与预训练和全量微调不同，LoRA微调在记忆化行为上表现出显著差异，模型规模和数据重复等因素对其影响不再遵循相同趋势。通过采用宽松的相似度记忆化度量，实验证明LoRA能显著降低记忆化风险，同时保持出色的任务性能，为LLM安全微调提供了新视角。", "keywords": "LLM, LoRA, 记忆化, 微调, 数据安全", "comments": "这项工作揭示了LoRA微调在降低LLM记忆化风险方面的潜力，这对于提高模型安全性至关重要。其发现模型规模和数据重复对LoRA记忆化影响不同于传统微调，为理解和优化参数高效微调提供了新的见解。这对于实际应用中平衡模型性能与数据隐私具有重要指导意义。"}}
{"id": "2506.21077", "title": "CURL-SLAM: Continuous and Compact LiDAR Mapping", "authors": ["Kaicheng Zhang", "Shida Xu", "Yining Ding", "Xianwen Kong", "Sen Wang"], "summary": "This paper studies 3D LiDAR mapping with a focus on developing an updatable\nand localizable map representation that enables continuity, compactness and\nconsistency in 3D maps. Traditional LiDAR Simultaneous Localization and Mapping\n(SLAM) systems often rely on 3D point cloud maps, which typically require\nextensive storage to preserve structural details in large-scale environments.\nIn this paper, we propose a novel paradigm for LiDAR SLAM by leveraging the\nContinuous and Ultra-compact Representation of LiDAR (CURL) introduced in [1].\nOur proposed LiDAR mapping approach, CURL-SLAM, produces compact 3D maps\ncapable of continuous reconstruction at variable densities using CURL's\nspherical harmonics implicit encoding, and achieves global map consistency\nafter loop closure. Unlike popular Iterative Closest Point (ICP)-based LiDAR\nodometry techniques, CURL-SLAM formulates LiDAR pose estimation as a unique\noptimization problem tailored for CURL and extends it to local Bundle\nAdjustment (BA), enabling simultaneous pose refinement and map correction.\nExperimental results demonstrate that CURL-SLAM achieves state-of-the-art 3D\nmapping quality and competitive LiDAR trajectory accuracy, delivering\nsensor-rate real-time performance (10 Hz) on a CPU. We will release the\nCURL-SLAM implementation to the community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21077v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21077v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CURL-SLAM：连续紧凑的激光雷达建图", "tldr": "本文提出了CURL-SLAM，一种利用连续超紧凑激光雷达表示（CURL）的新型激光雷达SLAM系统，实现了高效、高质量、实时性能的3D建图。", "motivation": "传统的激光雷达同步定位与建图（SLAM）系统通常依赖于3D点云地图，这需要大量的存储空间来保留大规模环境中的结构细节。本研究的动机是开发一种可更新、可定位的地图表示，以实现3D地图的连续性、紧凑性和一致性。", "method": "本文提出了CURL-SLAM，一种新颖的激光雷达SLAM范式，通过利用激光雷达的连续超紧凑表示（CURL）来实现。CURL-SLAM使用CURL的球谐函数隐式编码，生成能够以可变密度进行连续重建的紧凑3D地图，并在闭环后实现全局地图一致性。它将激光雷达位姿估计公式化为一个为CURL量身定制的独特优化问题，并将其扩展到局部束调整（BA），从而实现同步的位姿优化和地图校正。", "result": "实验结果表明，CURL-SLAM实现了最先进的3D建图质量和有竞争力的激光雷达轨迹精度，并在CPU上实现了传感器速率实时性能（10 Hz）。", "conclusion": "CURL-SLAM为3D激光雷达建图提供了一个高效且有效的解决方案，克服了传统方法的存储限制，同时保持了高质量和实时性能。", "translation": "本文研究了3D激光雷达建图，重点在于开发一种可更新、可定位的地图表示，以实现3D地图的连续性、紧凑性和一致性。传统的激光雷达同步定位与建图（SLAM）系统通常依赖于3D点云地图，这通常需要大量的存储空间来保留大规模环境中的结构细节。在本文中，我们通过利用[1]中引入的激光雷达连续超紧凑表示（CURL），提出了一种新颖的激光雷达SLAM范式。我们提出的激光雷达建图方法CURL-SLAM，使用CURL的球谐函数隐式编码，生成能够以可变密度进行连续重建的紧凑3D地图，并在闭环后实现全局地图一致性。与流行的基于迭代最近点（ICP）的激光雷达里程计技术不同，CURL-SLAM将激光雷达位姿估计公式化为一个为CURL量身定制的独特优化问题，并将其扩展到局部束调整（BA），从而实现同步的位姿优化和地图校正。实验结果表明，CURL-SLAM实现了最先进的3D建图质量和有竞争力的激光雷达轨迹精度，并在CPU上实现了传感器速率实时性能（10 Hz）。我们将向社区发布CURL-SLAM的实现。", "summary": "本文介绍了CURL-SLAM，一种新颖的激光雷达SLAM系统，旨在克服传统点云地图的存储限制。CURL-SLAM利用带有球谐函数编码的激光雷达连续超紧凑表示（CURL），生成紧凑、连续且一致的3D地图。它采用为CURL量身定制的独特优化问题进行位姿估计，并通过局部束调整实现同步的位姿和地图优化。实验结果表明，CURL-SLAM具有最先进的建图质量、有竞争力的轨迹精度和实时的CPU性能。", "keywords": "激光雷达SLAM, 紧凑建图, 连续表示, 球谐函数, 位姿估计", "comments": "创新点在于将CURL表示应用于激光雷达SLAM，替代传统的点云，实现了紧凑、连续的隐式编码，从而有效解决了大规模环境下的存储和重建效率问题。为CURL量身定制的位姿估计优化问题及其与束调整的结合也体现了创新性。该工作对于机器人、自动驾驶和增强现实等需要高效3D环境表示的领域具有重要意义。抽象中未提及具体局限性，但潜在挑战可能包括球谐函数编码的计算复杂性或自定义优化问题相对于更常见ICP方法的实现难度。"}}
{"id": "2506.20807", "title": "GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization", "authors": ["Martin Andrews", "Sam Witteveen"], "summary": "Optimizing GPU kernels for high performance is a complex task, often\ndemanding deep architectural knowledge, extensive profiling, and iterative\nexperimentation. This challenge is amplified when targeting newer or\nless-documented GPU architectures where traditional development aids are\nscarce. This paper introduces an LLM-powered \"GPU Kernel Scientist,\" an\nautomated methodology for iteratively refining accelerator kernels.\n  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)\nstrategically selecting promising prior code versions as a basis for new\niterations; (b) generating hypotheses for optimization experiments, based on\nexisting code and assimilated knowledge from general GPU literature; and (c)\nautonomously implementing these experiments through code modification and\nsubsequent submission to an external evaluation system, using only observed\ntiming data as performance feedback. We detail how this approach navigates the\nchallenges of the AMD MI300 target architecture and leverages LLMs to\ncompensate for limited domain-specific human expertise.\n  Since quantitative results from an ongoing performance competition were\nembargoed on paper submission date, we present the architectural design,\noperational workflow, and qualitative insights, highlighting the potential of\nLLM-driven agents to democratise and accelerate GPU kernel optimization,\nespecially in resource-constrained or rapidly evolving hardware environments.", "comment": "4 page paper plus Appendices. Accepted to the ES-FoMo \"Efficient\n  Systems for Foundation Models\" workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.20807v1", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20807v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "GPU内核科学家：一个LLM驱动的迭代内核优化框架", "tldr": "本文介绍了一个由LLM驱动的“GPU内核科学家”框架，用于自动化和迭代地优化GPU内核，尤其适用于新颖或文档不全的架构。", "motivation": "优化GPU内核以实现高性能是一项复杂任务，需要深入的架构知识、大量的性能分析和迭代实验。当针对较新或文档较少的GPU架构时，由于传统开发辅助工具的匮乏，这一挑战会进一步加剧。", "method": "本文引入了一个由LLM驱动的“GPU内核科学家”框架，它是一个自动化的迭代优化加速器内核的方法。该方法采用LLMs进行多阶段、演进式处理：(a) 战略性地选择有潜力的先前代码版本作为新迭代的基础；(b) 基于现有代码和从通用GPU文献中吸收的知识，生成优化实验的假设；(c) 通过代码修改和随后提交到外部评估系统，自主实施这些实验，仅使用观察到的时间数据作为性能反馈。该方法详细说明了如何应对AMD MI300目标架构的挑战，并利用LLMs弥补领域特定人类专业知识的不足。", "result": "由于正在进行的性能竞赛的定量结果在论文提交日期被禁运，本文展示了其架构设计、操作工作流程和定性见解，强调了LLM驱动的智能体在民主化和加速GPU内核优化方面的潜力。", "conclusion": "LLM驱动的智能体有潜力使GPU内核优化民主化并加速，尤其是在资源受限或快速发展的硬件环境中。", "translation": "优化GPU内核以实现高性能是一项复杂任务，通常需要深入的架构知识、大量的性能分析和迭代实验。当针对较新或文档较少的GPU架构时，由于传统开发辅助工具的匮乏，这一挑战会进一步加剧。本文介绍了一个由LLM驱动的“GPU内核科学家”，这是一种迭代优化加速器内核的自动化方法。\n我们的方法采用LLMs进行多阶段、演进式处理：(a) 战略性地选择有潜力的先前代码版本作为新迭代的基础；(b) 基于现有代码和从通用GPU文献中吸收的知识，生成优化实验的假设；(c) 通过代码修改和随后提交到外部评估系统，仅使用观察到的时间数据作为性能反馈，自主实施这些实验。我们详细介绍了这种方法如何应对AMD MI300目标架构的挑战，并利用LLMs弥补领域特定人类专业知识的不足。\n由于正在进行的性能竞赛的定量结果在论文提交日期被禁运，我们展示了架构设计、操作工作流程和定性见解，强调了LLM驱动的智能体在民主化和加速GPU内核优化方面的潜力，特别是在资源受限或快速发展的硬件环境中。", "summary": "本文提出了一个名为“GPU内核科学家”的LLM驱动框架，旨在自动化和加速GPU内核的迭代优化过程。该框架通过LLM在多阶段演进过程中选择代码版本、生成优化假设并自主执行实验，尤其适用于缺乏文档的新兴GPU架构。尽管定量结果因竞赛禁运而未公布，论文详细阐述了其架构设计、操作流程和定性发现，突出了LLM在提升GPU优化效率和可及性方面的巨大潜力。", "keywords": "GPU优化, LLM, 内核优化, 自动化, 迭代", "comments": "该论文提出了一种创新的、基于LLM的GPU内核优化方法，通过自动化实验和知识整合，有效弥补了在新型或文档不足架构上优化GPU内核时人力专业知识的不足。其“GPU内核科学家”概念有望显著降低GPU性能优化的门槛，实现优化过程的民主化和加速。尽管缺乏具体的定量性能数据，但其方法论和对LLM潜力的强调，预示着该领域未来发展的方向。"}}
{"id": "2506.20900", "title": "The Role of Cyclopean-Eye in Stereo Vision", "authors": ["Sherlon Almeida da Silva", "Davi Geiger", "Luiz Velho", "Moacir Antonelli Ponti"], "summary": "This work investigates the geometric foundations of modern stereo vision\nsystems, with a focus on how 3D structure and human-inspired perception\ncontribute to accurate depth reconstruction. We revisit the Cyclopean Eye model\nand propose novel geometric constraints that account for occlusions and depth\ndiscontinuities. Our analysis includes the evaluation of stereo feature\nmatching quality derived from deep learning models, as well as the role of\nattention mechanisms in recovering meaningful 3D surfaces. Through both\ntheoretical insights and empirical studies on real datasets, we demonstrate\nthat combining strong geometric priors with learned features provides internal\nabstractions for understanding stereo vision systems.", "comment": "arXiv admin note: text overlap with arXiv:2502.21280", "pdf_url": "http://arxiv.org/pdf/2506.20900v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20900v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "独眼模型在立体视觉中的作用", "tldr": "该研究探讨了独眼模型在立体视觉中的作用，通过结合几何先验和深度学习特征来提高深度重建的准确性，并处理遮挡和深度不连续性。", "motivation": "该工作旨在深入探究现代立体视觉系统的几何基础，并着重研究3D结构和受人类启发的感知如何促进精确的深度重建。", "method": "本文重新审视了独眼模型，并提出了新的几何约束来处理遮挡和深度不连续性。分析中还包括评估来自深度学习模型的立体特征匹配质量，以及注意力机制在恢复有意义的3D表面中的作用。研究通过理论见解和对真实数据集的实证研究进行。", "result": "结合强大的几何先验知识与学习到的特征，为理解立体视觉系统提供了内部抽象。", "conclusion": "论文证明了将强大的几何先验与学习到的特征相结合，能够提供理解立体视觉系统的内部抽象。", "translation": "这项工作研究了现代立体视觉系统的几何基础，重点关注3D结构和受人类启发的感知如何有助于精确的深度重建。我们重新审视了独眼模型，并提出了新的几何约束，以解决遮挡和深度不连续性问题。我们的分析包括评估源自深度学习模型的立体特征匹配质量，以及注意力机制在恢复有意义的3D表面中的作用。通过理论见解和对真实数据集的实证研究，我们证明了将强大的几何先验与学习到的特征相结合，能够为理解立体视觉系统提供内部抽象。", "summary": "这项研究深入探讨了现代立体视觉系统的几何基础，特别是独眼模型在精确深度重建中的作用。通过引入新的几何约束来处理遮挡和深度不连续性，并结合深度学习模型的特征匹配质量评估以及注意力机制的作用，文章证明了将几何先验与学习特征结合能有效提升对立体视觉系统的理解。", "keywords": "立体视觉, 独眼模型, 深度重建, 几何约束, 深度学习", "comments": "这篇论文的创新之处在于它重新审视了经典的独眼模型，并将其与现代深度学习技术相结合，特别是引入了新的几何约束来处理复杂的遮挡和深度不连续性。它强调了几何先验知识在数据驱动的深度学习模型中的重要性，为理解和改进立体视觉系统提供了新的视角。"}}
{"id": "2506.20442", "title": "When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact", "authors": ["Tianyao Shi", "Ritbik Kumar", "Inez Hua", "Yi Ding"], "summary": "Biodiversity loss is a critical planetary boundary, yet its connection to\ncomputing remains largely unexamined. Prior sustainability efforts in computing\nhave focused on carbon and water, overlooking biodiversity due to the lack of\nappropriate metrics and modeling frameworks. This paper presents the first\nend-to-end analysis of biodiversity impact from computing systems. We introduce\ntwo new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity\nIndex (OBI)--to quantify biodiversity impact across the lifecycle, and present\nFABRIC, a modeling framework that links computing workloads to biodiversity\nimpacts. Our evaluation highlights the need to consider biodiversity alongside\ncarbon and water in sustainable computing design and optimization. The code is\navailable at https://github.com/TianyaoShi/FABRIC.", "comment": "Accepted by HotCarbon' 25", "pdf_url": "http://arxiv.org/pdf/2506.20442v1", "categories": ["cs.CY", "cs.AR", "cs.DC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.20442v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "当服务器遇到物种：计算对生物多样性影响的“从摇篮到坟墓”视角", "tldr": "计算领域对生物多样性的影响尚未被充分研究，本文首次提出了端到端的分析方法，并引入新指标和建模框架来量化和评估这种影响，强调了在可持续计算设计中考虑生物多样性的必要性。", "motivation": "生物多样性丧失是一个重要的地球边界问题，但计算领域以往的可持续性努力主要集中在碳和水上，由于缺乏合适的衡量指标和建模框架，生物多样性影响被忽视了。", "method": "本文提出了计算系统生物多样性影响的首次端到端分析。引入了两个新指标：具身生物多样性指数（EBI）和操作生物多样性指数（OBI），用于量化整个生命周期的生物多样性影响。同时，提出了FABRIC建模框架，将计算工作负载与生物多样性影响联系起来。", "result": "评估结果强调了在可持续计算设计和优化中，除了碳和水之外，还需要考虑生物多样性。", "conclusion": "在可持续计算设计和优化中，生物多样性与碳和水一样，是需要被考虑的关键因素。", "translation": "生物多样性丧失是一个关键的地球边界问题，但其与计算的联系在很大程度上仍未被检验。计算领域先前的可持续发展努力主要集中在碳和水上，由于缺乏适当的衡量指标和建模框架，生物多样性被忽视了。本文首次提出了计算系统生物多样性影响的端到端分析。我们引入了两个新指标——具身生物多样性指数（EBI）和操作生物多样性指数（OBI）——以量化整个生命周期的生物多样性影响，并提出了FABRIC，一个将计算工作负载与生物多样性影响联系起来的建模框架。我们的评估强调了在可持续计算设计和优化中，除了碳和水之外，还需要考虑生物多样性。代码可在 https://github.com/TianyaoShi/FABRIC 获取。", "summary": "本文首次全面分析了计算系统对生物多样性的影响，填补了现有可持续计算研究的空白。研究引入了具身生物多样性指数（EBI）和操作生物多样性指数（OBI）这两个新指标，并开发了FABRIC建模框架，旨在量化并连接计算活动与生物多样性影响。研究结果强调，在未来的可持续计算设计和优化中，生物多样性应与碳排放和水资源消耗同等重要地被考虑。", "keywords": "生物多样性, 可持续计算, 生命周期评估, EBI, OBI", "comments": "该论文的创新之处在于首次提出了计算系统对生物多样性影响的端到端分析，并引入了专门的量化指标和建模框架，弥补了现有可持续计算研究在生物多样性方面的空白。其重要性在于将生物多样性这一关键的全球环境问题引入计算领域，为未来的可持续计算设计和优化提供了新的视角和工具。"}}
{"id": "2506.21239", "title": "Exact Time-Varying Turnpikes for Dynamic Operation of District Heating Networks", "authors": ["Max Rose", "Hannes Gernandt", "Timm Faulwasser", "Johannes Schiffer"], "summary": "District heating networks (DHNs) are crucial for decarbonizing the heating\nsector. Yet, their efficient and reliable operation requires the coordination\nof multiple heat producers and the consideration of future demands. Predictive\nand optimization-based control is commonly used to address this task, but\nexisting results for DHNs do not account for time-varying problem aspects.\nSince the turnpike phenomenon can serve as a basis for model predictive control\ndesign and analysis, this paper examines its role in DHN optimization by\nanalyzing the underlying optimal control problem with time-varying prices and\ndemands. That is, we derive conditions for the existence of a unique\ntime-varying singular arc, which constitutes the time varying turnpike, and we\nprovide its closed-form expression. Additionally, we present converse turnpike\nresults showing a exact time-varying case implies strict dissipativity of the\noptimal control problem. A numerical example illustrates our findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21239v1", "categories": ["math.OC", "cs.SY", "eess.SY"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.21239v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "区域供热网络动态运行的精确时变转轨现象", "tldr": "该研究为区域供热网络优化提供了精确的时变转轨现象分析，解决了现有方法未能考虑时变因素的问题。", "motivation": "区域供热网络的高效可靠运行需要协调多个热源并考虑未来需求，但现有的预测和优化控制方法未能考虑时变问题。", "method": "本文通过分析具有时变价格和需求的最优控制问题，研究了转轨现象在区域供热网络优化中的作用。具体地，推导了唯一时变奇异弧（构成时变转轨）存在的条件，并给出了其闭式表达式。此外，还提出了逆转轨结果，表明精确的时变情况意味着最优控制问题具有严格耗散性。一个数值例子说明了我们的发现。", "result": "导出了唯一时变奇异弧（即时变转轨）存在的条件及其闭式表达式。研究还表明，精确的时变转轨情况意味着最优控制问题具有严格耗散性。通过数值例子验证了研究结果。", "conclusion": "本文为区域供热网络在时变条件下的优化运行提供了理论基础，通过引入精确的时变转轨概念，克服了现有方法在处理时变因素方面的不足，并揭示了其与最优控制问题耗散性之间的关系。", "translation": "区域供热网络 (DHNs) 对于供热部门的脱碳至关重要。然而，它们的有效和可靠运行需要协调多个热力生产者并考虑未来的需求。基于预测和优化的控制通常用于解决此任务，但现有的DHN结果并未考虑时变问题。由于转轨现象可以作为模型预测控制设计和分析的基础，本文通过分析具有时变价格和需求的基本最优控制问题，研究了其在DHN优化中的作用。也就是说，我们推导了唯一时变奇异弧存在的条件，该奇异弧构成了时变转轨，并提供了其闭式表达式。此外，我们还提出了逆转轨结果，表明精确的时变情况意味着最优控制问题具有严格耗散性。一个数值例子说明了我们的发现。", "summary": "本文针对区域供热网络在时变条件下的高效优化运行问题，引入了精确时变转轨的概念。研究推导了构成时变转轨的唯一时变奇异弧的存在条件及其闭式表达式，并证明了精确时变转轨情况隐含最优控制问题的严格耗散性，为区域供热网络的模型预测控制设计和分析提供了新的理论基础。", "keywords": "区域供热网络, 时变转轨, 最优控制, 奇异弧, 耗散性", "comments": "这篇论文通过引入“精确时变转轨”的概念，创新性地解决了区域供热网络优化中现有方法未能有效处理时变因素的局限性。其理论推导，特别是关于时变奇异弧的闭式表达式和与耗散性的关联，为复杂动态系统在时变环境下的最优控制提供了新的分析工具和理论深度，对于提升区域供热网络的能源效率和脱碳进程具有重要意义。"}}
{"id": "2506.21375", "title": "Integrating Movable Antennas and Intelligent Reflecting Surfaces for Coverage Enhancement", "authors": ["Ying Gao", "Qingqing Wu", "Weidong Mei", "Guangji Chen", "Wen Chen", "Ziyuan Zheng"], "summary": "This paper investigates an intelligent reflecting surface (IRS)-aided movable\nantenna (MA) system, where multiple IRSs cooperate with a multi-MA base station\nto extend wireless coverage to multiple designated target areas. The objective\nis to maximize the worst-case signal-to-noise ratio (SNR) across all locations\nwithin these areas through joint optimization of MA positions, IRS reflection\ncoefficients, and transmit beamforming. To achieve this while balancing the\nperformance-cost trade-off, we propose three coverage-enhancement schemes: the\narea-adaptive MA-IRS scheme, the area-adaptive MA-staIRS scheme, and the shared\nMA-staIRS scheme, where staIRS denotes static IRSs with reflection coefficients\nconfigured only once during installation. These schemes lead to challenging\nnon-convex optimization problems with implicit objective functions, which are\ndifficult to solve optimally. To address these problems, we propose a general\nalgorithmic framework that can be applied to solve each problem efficiently\nalbeit suboptimally. Simulation results demonstrate that: 1) the proposed\nMA-based schemes consistently outperform their fixed-position antenna\n(FPA)-based counterparts under both area-adaptive and static IRS\nconfigurations, with the area-adaptive MA-IRS scheme achieving the best\nworst-case SNR performance; 2) as transmit antennas are typically far fewer\nthan IRS elements, the area-adaptive MA-staIRS scheme may underperform the\nbaseline FPA scheme with area-adaptive IRSs in terms of the worst-case SNR, but\na modest increase in antenna number can reverse this trend; 3) under a fixed\ntotal cost, the optimal MA-to-IRS-element ratio for the worst-case SNR\nmaximization is empirically found to be proportional to the reciprocal of their\nunit cost ratio.", "comment": "13 pages, 8 figures, submitted to an IEEE journal for possible\n  publication on on May 8, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21375v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.21375v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "整合可移动天线和智能反射面以增强覆盖范围", "tldr": "本文研究了IRS辅助的MA系统，通过联合优化MA位置、IRS反射系数和发射波束成形，最大化多目标区域的最低SNR，并提出三种方案和算法框架来解决非凸优化问题，仿真结果表明MA方案优于FPA方案。", "motivation": "旨在通过联合优化可移动天线（MA）位置、智能反射面（IRS）反射系数和发射波束成形，将无线覆盖扩展到多个指定目标区域，并最大化这些区域内所有位置的最差情况信噪比（SNR）。", "method": "提出三种覆盖增强方案：区域自适应MA-IRS方案、区域自适应MA-staIRS方案和共享MA-staIRS方案，以平衡性能与成本。为解决由此产生的具有隐式目标函数的非凸优化问题，提出了一种通用的算法框架。", "result": "1) 所提出的基于MA的方案在区域自适应和静态IRS配置下均优于基于固定位置天线（FPA）的方案，其中区域自适应MA-IRS方案实现了最佳的最差情况SNR性能；2) 当发射天线数量远少于IRS单元时，区域自适应MA-staIRS方案在最差情况SNR方面可能不如基线FPA方案与区域自适应IRS的组合，但适度增加天线数量可以扭转这一趋势；3) 在固定总成本下，为最大化最差情况SNR，MA与IRS单元的最佳比例经验性地发现与其单位成本比的倒数成正比。", "conclusion": "整合可移动天线和智能反射面能够有效增强无线覆盖，所提出的MA方案在大多数情况下优于固定位置天线方案，且性能与成本之间存在权衡，特定配置下可通过增加天线数量或优化MA与IRS单元成本比来提升性能。", "translation": "本文研究了一种智能反射面（IRS）辅助的可移动天线（MA）系统，其中多个IRS与多MA基站协作，将无线覆盖扩展到多个指定目标区域。目标是通过联合优化MA位置、IRS反射系数和发射波束成形，最大化这些区域内所有位置的最差情况信噪比（SNR）。为了在平衡性能与成本权衡的同时实现这一目标，我们提出了三种覆盖增强方案：区域自适应MA-IRS方案、区域自适应MA-staIRS方案和共享MA-staIRS方案，其中staIRS表示反射系数仅在安装时配置一次的静态IRS。这些方案导致了具有隐式目标函数的挑战性非凸优化问题，难以找到最优解。为了解决这些问题，我们提出了一个通用的算法框架，可以有效地（尽管是次优地）解决每个问题。仿真结果表明：1）在区域自适应和静态IRS配置下，所提出的基于MA的方案始终优于其基于固定位置天线（FPA）的对应方案，其中区域自适应MA-IRS方案实现了最佳的最差情况SNR性能；2）由于发射天线通常远少于IRS单元，区域自适应MA-staIRS方案在最差情况SNR方面可能不如基线FPA方案与区域自适应IRS的组合，但适度增加天线数量可以扭转这一趋势；3）在固定总成本下，为最大化最差情况SNR，可移动天线与IRS单元的最佳比例经验性地发现与其单位成本比的倒数成正比。", "summary": "本文研究了智能反射面（IRS）辅助的可移动天线（MA）系统，旨在通过联合优化MA位置、IRS反射系数和发射波束成形来扩展无线覆盖并最大化目标区域的最差情况信噪比。为此，论文提出了三种覆盖增强方案（区域自适应MA-IRS、区域自适应MA-staIRS、共享MA-staIRS）以及一个通用的算法框架来解决由此产生的非凸优化问题。仿真结果表明，所提出的MA方案在性能上优于固定位置天线方案，尤其是区域自适应MA-IRS方案表现最佳，并揭示了天线数量和MA与IRS单元成本比对性能的影响。", "keywords": "可移动天线, 智能反射面, 覆盖增强, 信噪比最大化, 非凸优化", "comments": "这项研究通过整合可移动天线和智能反射面，为无线覆盖增强提供了一种新颖且有潜力的解决方案。其创新点在于同时优化MA位置、IRS反射系数和发射波束成形，以解决复杂的非凸问题。虽然提出的算法框架是次优的，但其通用性使其能够应用于多种场景。研究结果强调了MA在提升系统性能方面的优势，并提供了关于成本与性能权衡的实际指导，对未来无线通信系统的设计具有重要意义。"}}
{"id": "2506.21198", "title": "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation", "authors": ["Yihong Cao", "Jiaming Zhang", "Xu Zheng", "Hao Shi", "Kunyu Peng", "Hang Liu", "Kailun Yang", "Hui Zhang"], "summary": "Panoramic image processing is essential for omni-context perception, yet\nfaces constraints like distortions, perspective occlusions, and limited\nannotations. Previous unsupervised domain adaptation methods transfer knowledge\nfrom labeled pinhole data to unlabeled panoramic images, but they require\naccess to source pinhole data. To address these, we introduce a more practical\ntask, i.e., Source-Free Occlusion-Aware Seamless Segmentation (SFOASS), and\npropose its first solution, called UNconstrained Learning Omni-Context\nKnowledge (UNLOCK). Specifically, UNLOCK includes two key modules: Omni\nPseudo-Labeling Learning and Amodal-Driven Context Learning. While adapting\nwithout relying on source data or target labels, this framework enhances models\nto achieve segmentation with 360{\\deg} viewpoint coverage and occlusion-aware\nreasoning. Furthermore, we benchmark the proposed SFOASS task through both\nreal-to-real and synthetic-to-real adaptation settings. Experimental results\nshow that our source-free method achieves performance comparable to\nsource-dependent methods, yielding state-of-the-art scores of 10.9 in mAAP and\n11.6 in mAP, along with an absolute improvement of +4.3 in mAPQ over the\nsource-only method. All data and code will be made publicly available at\nhttps://github.com/yihong-97/UNLOCK.", "comment": "Accepted to ICCV 2025. All data and code will be made publicly\n  available at https://github.com/yihong-97/UNLOCK", "pdf_url": "http://arxiv.org/pdf/2506.21198v1", "categories": ["cs.CV", "cs.RO", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21198v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "解锁约束：无源遮挡感知无缝分割", "tldr": "本文提出了无源遮挡感知无缝分割（SFOASS）这一更实际的任务，并首次提出了名为UNLOCK的解决方案，该方案在无需源数据或目标标签的情况下，实现了与依赖源数据方法相当的性能。", "motivation": "全景图像处理面临畸变、透视遮挡和标注有限等挑战。现有的无监督域适应方法需要访问源针孔数据，这在实际应用中不便。本文旨在解决这些问题，提出一种更实用的无源任务。", "method": "本文提出了无源遮挡感知无缝分割（SFOASS）任务，并首次提出了解决方案UNLOCK。UNLOCK包含两个关键模块：全方位伪标签学习和非模态驱动上下文学习。该框架在不依赖源数据或目标标签的情况下进行适应，增强模型以实现360度视点覆盖和遮挡感知推理的分割。SFOASS任务通过真实到真实和合成到真实的适应设置进行基准测试。", "result": "实验结果表明，该无源方法取得了与依赖源数据方法相当的性能，在mAAP和mAP上分别达到10.9和11.6的最新分数，并在mAPQ上比仅依赖源数据的方法绝对提升了+4.3。", "conclusion": "本文提出的UNLOCK框架成功解决了无源遮挡感知无缝分割这一实际任务，展示了与依赖源数据方法相当的性能，并在特定指标上取得了最先进的结果。", "translation": "全景图像处理对于全方位感知至关重要，但面临畸变、透视遮挡和有限标注等限制。以前的无监督域适应方法将知识从有标签的针孔数据转移到无标签的全景图像，但它们需要访问源针孔数据。为了解决这些问题，我们引入了一个更实际的任务，即无源遮挡感知无缝分割（SFOASS），并提出了其第一个解决方案，名为“无约束学习全方位上下文知识”（UNLOCK）。具体来说，UNLOCK 包括两个关键模块：全方位伪标签学习和非模态驱动上下文学习。在不依赖源数据或目标标签的情况下进行适应，该框架增强了模型，实现了360度视点覆盖和遮挡感知推理的分割。此外，我们通过真实到真实和合成到真实的适应设置对所提出的 SFOASS 任务进行了基准测试。实验结果表明，我们的无源方法取得了与依赖源数据的方法相当的性能，在 mAAP 中获得了10.9的最新分数，在 mAP 中获得了11.6的最新分数，并且在 mAPQ 上比仅使用源数据的方法绝对提高了+4.3。所有数据和代码都将在 https://github.com/yihong-97/UNLOCK 公开提供。", "summary": "本文提出了一种名为“无源遮挡感知无缝分割”（SFOASS）的新型实用任务，旨在克服传统无监督域适应方法在全景图像处理中对源数据依赖的限制。所提出的解决方案UNLOCK，利用全方位伪标签学习和非模态驱动上下文学习，实现了360度视点覆盖和遮挡感知推理。实验结果表明，UNLOCK的性能与依赖源数据的方法相当，并取得了最先进的结果，这标志着全景分割领域无源域适应研究的重大进展。", "keywords": "全景图像处理, 无源, 遮挡感知, 无缝分割, 域适应", "comments": "本文通过提出SFOASS任务和UNLOCK框架，解决了全景图像处理中一个重要的实际挑战。其创新之处在于能够无需源域数据或目标标签即可进行鲁棒分割，这极大地提高了全景分割在源数据可能不可用或敏感的实际场景中的适用性。与依赖源数据方法相当的性能证明了其有效性及广泛应用的潜力。"}}
{"id": "2506.21506", "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jiménez Gutiérrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.", "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/", "pdf_url": "http://arxiv.org/pdf/2506.21506v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21506v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Mind2Web 2：使用智能体作为评判者评估智能体搜索", "tldr": "本文介绍了Mind2Web 2，一个用于评估智能体搜索系统的新基准和“智能体作为评判者”评估框架，并发现当前最佳系统能达到人类表现的50-70%。", "motivation": "随着大型语言模型驱动的智能体搜索系统日益复杂和开放，现有的评估基准和方法已不足以应对其长搜索周期和动态答案的挑战。", "method": "本文引入了Mind2Web 2，一个包含130个真实、高质量、长周期任务的基准，这些任务需要实时网页浏览和大量信息综合。为评估时变且复杂的答案，作者提出了新颖的“智能体作为评判者”框架，该框架基于树状评分标准构建特定任务的评判智能体，以自动评估答案的正确性和来源归属。", "result": "对九个前沿智能体搜索系统和人类表现的综合评估显示，表现最佳的OpenAI Deep Research系统在耗时减半的情况下，已能达到人类性能的50-70%。", "conclusion": "Mind2Web 2为开发和评估下一代智能体搜索系统提供了严格的基础，并揭示了当前系统在效率和性能方面的巨大潜力。", "translation": "智能体搜索系统，如深度研究系统，其中大型语言模型自主浏览网页、综合信息并返回全面的、有引用的答案，代表着用户与网络规模信息交互方式的重大转变。虽然智能体搜索有望提高效率和减轻认知负担，但其日益增长的复杂性和开放性已超越了现有的评估基准和方法，这些基准和方法大多假设搜索周期短且答案静态。在本文中，我们介绍了Mind2Web 2，这是一个包含130个真实、高质量、长周期任务的基准，这些任务需要实时网页浏览和大量信息综合，耗费了超过1000小时的人工劳动构建。为了解决评估时变和复杂答案的挑战，我们提出了一种新颖的“智能体作为评判者”框架。我们的方法基于树状评分标准构建特定任务的评判智能体，以自动评估答案的正确性和来源归属。我们对九个前沿智能体搜索系统和人类表现进行了全面评估，并进行了详细的错误分析，为未来的发展提供了见解。表现最佳的系统OpenAI Deep Research在耗时减半的情况下，已能达到人类性能的50-70%，显示出巨大的潜力。总而言之，Mind2Web 2为开发和评估下一代智能体搜索系统提供了严格的基础。", "summary": "本文针对大型语言模型驱动的智能体搜索系统，提出了Mind2Web 2基准和“智能体作为评判者”评估框架。Mind2Web 2是一个包含130个需实时网页浏览和信息综合的长期任务基准，而“智能体作为评判者”框架则能自动评估复杂答案的正确性和来源。研究评估了九个前沿系统，发现最佳系统OpenAI Deep Research在效率更高的情况下，能达到人类表现的50-70%。该工作为未来智能体搜索系统的开发和评估奠定了基础。", "keywords": "智能体搜索, 评估, 基准, Agent-as-a-Judge, Mind2Web 2", "comments": "本文的创新之处在于提出了一个更贴近实际、长周期的智能体搜索评估基准Mind2Web 2，并引入了“智能体作为评判者”这一新颖的自动化评估框架，有效解决了现有评估方法在处理复杂、动态答案方面的不足。这对于推动智能体搜索领域的发展，特别是大型语言模型在信息综合和网页浏览方面的应用具有重要意义。其结果也显示了当前智能体系统的巨大潜力，同时也指出了与人类表现的差距，为未来的研究方向提供了清晰的指引。"}}
{"id": "2506.21307", "title": "Guarding Offices with Maximum Dispersion", "authors": ["Sándor P. Fekete", "Kai Kobbe", "Dominik Krupke", "Joseph S. B. Mitchell", "Christian Rieck", "Christian Scheffer"], "summary": "We investigate the Dispersive Art Gallery Problem with vertex guards and\nrectangular visibility ($r$-visibility) for a class of orthogonal polygons that\nreflect the properties of real-world floor plans: these office-like polygons\nconsist of rectangular rooms and corridors. In the dispersive variant of the\nArt Gallery Problem, the objective is not to minimize the number of guards but\nto maximize the minimum geodesic $L_1$-distance between any two guards, called\nthe dispersion distance.\n  Our main contributions are as follows. We prove that determining whether a\nvertex guard set can achieve a dispersion distance of $4$ in office-like\npolygons is NP-complete, where vertices of the polygon are restricted to\ninteger coordinates. Additionally, we present a simple worst-case optimal\nalgorithm that guarantees a dispersion distance of $3$ in polynomial time. Our\ncomplexity result extends to polyominoes, resolving an open question posed by\nRieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be\nrational, we establish analogous results, proving that achieving a dispersion\ndistance of $2+\\varepsilon$ is NP-hard for any $\\varepsilon > 0$, while the\nclassic Art Gallery Problem remains solvable in polynomial time for this class\nof polygons. Furthermore, we give a straightforward polynomial-time algorithm\nthat computes worst-case optimal solutions with a dispersion distance of $2$.\n  On the other hand, for the more restricted class of hole-free independent\noffice-like polygons, we propose a dynamic programming approach that computes\noptimal solutions. Moreover, we demonstrate that the problem is practically\ntractable for arbitrary orthogonal polygons. To this end, we compare solvers\nbased on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently\ncompute optimal solutions for randomly generated instances with up to $1600$\nvertices in under $15$s.", "comment": "40 pages, 29 figures, to appear in the proceedings 50th International\n  Symposium on Mathematical Foundations of Computer Science (MFCS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.21307v1", "categories": ["cs.CG", "cs.DS", "F.2.2"], "cate": "cs.CG", "url": "http://arxiv.org/abs/2506.21307v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "以最大离散度守卫办公室", "tldr": "本文研究了办公室状正交多边形中的离散美术馆问题，证明了特定离散距离的NP完全性，并提出了最优算法和实际求解器的比较。", "motivation": "本文研究了离散美术馆问题，其目标不是最小化守卫数量，而是最大化任意两个守卫之间的最小测地L1距离，即离散距离。该问题应用于反映真实世界平面图特征的办公室状多边形，这些多边形由矩形房间和走廊组成。", "method": "研究方法包括：证明了在办公室状多边形中，当顶点坐标限制为整数时，确定顶点守卫集是否能实现4的离散距离是NP完全的。提出了一个简单的最坏情况最优算法，该算法在多项式时间内保证3的离散距离。当顶点坐标允许为有理数时，证明了实现2+ε的离散距离是NP难的。提出了一个直接的多项式时间算法，计算出离散距离为2的最坏情况最优解。对于更受限的无孔独立办公室状多边形，提出了一个动态规划方法来计算最优解。此外，通过比较基于SAT、CP和MIP公式的求解器，证明了该问题对于任意正交多边形在实践中是可处理的。", "result": "主要结果包括：证明了在办公室状多边形中，当顶点坐标为整数时，确定顶点守卫集是否能实现4的离散距离是NP完全的。提出了一个最坏情况最优的多项式时间算法，保证3的离散距离。当顶点坐标允许为有理数时，确定实现2+ε离散距离是NP难的。提出了一个多项式时间算法，计算出离散距离为2的最坏情况最优解。对于无孔独立办公室状多边形，动态规划方法能计算出最优解。SAT求解器能高效地计算出多达1600个顶点的随机生成实例的最优解，耗时不到15秒，表明该问题在实践中是可处理的。", "conclusion": "本文确定了办公室状多边形中离散美术馆问题在不同坐标假设和离散距离下的计算复杂性，为特定情况提供了高效算法，并展示了使用各种求解器（特别是SAT求解器）解决大规模实例的实用可行性。", "translation": "我们研究了带有顶点守卫和矩形可见性（r-可见性）的离散美术馆问题，该问题针对一类反映真实世界平面图属性的正交多边形：这些办公室状多边形由矩形房间和走廊组成。在美术馆问题的离散变体中，目标不是最小化守卫数量，而是最大化任意两个守卫之间的最小测地L1距离，称为离散距离。\n我们的主要贡献如下。我们证明了在办公室状多边形中，当多边形的顶点限制为整数坐标时，确定顶点守卫集是否能实现4的离散距离是NP完全的。此外，我们提出了一个简单的最坏情况最优算法，该算法在多项式时间内保证3的离散距离。我们的复杂性结果扩展到多联骨牌，解决了Rieck和Scheffer（CGTA 2024）提出的一个开放问题。当顶点坐标允许为有理数时，我们建立了类似的结论，证明了对于任意ε > 0，实现2+ε的离散距离是NP难的，而经典的美术馆问题对于这类多边形仍然可以在多项式时间内解决。此外，我们给出了一个直接的多项式时间算法，计算出离散距离为2的最坏情况最优解。\n另一方面，对于更受限的无孔独立办公室状多边形，我们提出了一种动态规划方法来计算最优解。此外，我们证明了该问题对于任意正交多边形在实践中是可处理的。为此，我们比较了基于SAT、CP和MIP公式的求解器。值得注意的是，SAT求解器能在不到15秒的时间内高效地计算出多达1600个顶点的随机生成实例的最优解。", "summary": "本文研究了办公室状正交多边形中的离散美术馆问题，旨在最大化守卫之间的最小L1距离。研究证明了在整数坐标下实现4的离散距离以及在有理坐标下实现2+ε的离散距离是NP完全/NP难的，并提出了保证3和2离散距离的多项式时间最优算法。对于特定限制的多边形，本文引入了动态规划方法。此外，通过比较SAT、CP和MIP求解器，论文展示了SAT求解器能高效处理大型实例，验证了该问题的实际可处理性。", "keywords": "离散美术馆问题, 正交多边形, NP完全, 算法, 求解器", "comments": "本文通过将美术馆问题扩展到离散变体，并将其应用于符合真实世界平面图的办公室状多边形，做出了重要贡献。其在不同坐标假设和离散距离下对问题复杂性的详细分析（包括NP完全性证明和多项式时间算法）极具价值。尤为突出的是，对不同求解器（特别是SAT求解器）在处理大规模实例时的效率进行的实际评估，成功地连接了理论复杂性与实际应用，展示了该问题在实践中的可操作性，具有很强的创新性。"}}
{"id": "2506.21222", "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21222v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21222v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "增强大型语言模型通过句法检索的自动术语提取", "tldr": "本文提出一种基于句法检索的提示策略，利用大型语言模型进行自动术语提取，实验证明其能有效提高F1分数。", "motivation": "自动术语提取（ATE）识别领域特定表达，对机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLMs）显著推动了各种NLP任务，但其在ATE方面的潜力却鲜有研究。", "method": "提出一种基于检索的提示策略，在少样本设置下，根据句法而非语义相似性选择示例。这种句法检索方法与领域无关，为捕获术语边界提供更可靠的指导。该方法在域内和跨域设置下进行评估。", "result": "在三个专门的ATE基准测试上的实验表明，句法检索提高了F1分数。研究还分析了查询句和检索示例之间的词汇重叠如何影响性能。", "conclusion": "这些发现强调了在将大型语言模型应用于术语提取任务时，句法线索的重要性。", "translation": "自动术语提取（ATE）识别领域特定的表达，这对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLMs）在各种自然语言处理任务中取得了显著进展，但它们在ATE方面的潜力却鲜有研究。我们提出一种基于检索的提示策略，在少样本设置下，根据句法而非语义相似性选择示例。这种句法检索方法与领域无关，为捕获术语边界提供了更可靠的指导。我们在域内和跨域设置下评估了该方法，分析了查询句与其检索到的示例之间的词汇重叠如何影响性能。在三个专门的ATE基准测试上的实验表明，句法检索提高了F1分数。这些发现强调了在将大型语言模型应用于术语提取任务时，句法线索的重要性。", "summary": "本文旨在探索大型语言模型在自动术语提取（ATE）中的应用潜力，提出一种新颖的基于句法检索的提示策略。该策略在少样本设置下，通过识别句法相似性而非语义相似性来选择示例，从而为术语边界的识别提供更可靠的指导，且与领域无关。在多个ATE基准测试上的实验结果表明，该句法检索方法能有效提升F1分数，突显了句法线索在将大型语言模型应用于术语提取任务时的关键作用。", "keywords": "自动术语提取, 大型语言模型, 句法检索, 少样本学习, 术语识别", "comments": "这篇论文的创新点在于提出了一个新颖的、与领域无关的句法检索方法，用于大型语言模型在少样本设置下的自动术语提取。它成功地将LLMs的能力扩展到ATE领域，并证明了句法信息对于提高术语提取性能的重要性，这对于未来的LLM在专业领域应用具有指导意义。"}}
{"id": "2506.21306", "title": "On Uniform Weighted Deep Polynomial approximation", "authors": ["Kingsley Yeon", "Steven B. Damelin"], "summary": "It is a classical result in rational approximation theory that certain\nnon-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be\nefficiently approximated using rational functions with root-exponential\nconvergence in terms of degrees of freedom \\cite{Sta, GN}. In contrast,\npolynomial approximations admit only algebraic convergence by Jackson's theorem\n\\cite{Lub2}. Recent work shows that composite polynomial architectures can\nrecover exponential approximation rates even without smoothness \\cite{KY}. In\nthis work, we introduce and analyze a class of weighted deep polynomial\napproximants tailored for functions with asymmetric behavior-growing unbounded\non one side and decaying on the other. By multiplying a learnable deep\npolynomial with a one-sided weight, we capture both local non-smoothness and\nglobal growth. We show numerically that this framework outperforms Taylor,\nChebyshev, and standard deep polynomial approximants, even when all use the\nsame number of parameters. To optimize these approximants in practice, we\npropose a stable graph-based parameterization strategy building on \\cite{Jar}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21306v1", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.NA", "stat.ML"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21306v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关于均匀加权深度多项式逼近", "tldr": "本文介绍了一种加权深度多项式逼近器，用于处理具有不对称行为的函数，数值结果显示其性能优于传统方法。", "motivation": "传统的有理函数逼近对于非光滑或奇异函数能达到根指数收敛，而多项式逼近仅能实现代数收敛。虽然近期复合多项式结构能恢复指数逼近率，但本文旨在解决具有不对称行为（一侧无界增长，另一侧衰减）的函数的有效逼近问题。", "method": "引入并分析了一类加权深度多项式逼近器。该方法通过将一个可学习的深度多项式与一个单侧权重相乘，以捕获局部非光滑性和全局增长。为优化这些逼近器，还提出了一种稳定的基于图的参数化策略。", "result": "数值结果表明，该框架在参数数量相同的情况下，优于泰勒、切比雪夫和标准深度多项式逼近器。", "conclusion": "本文成功引入并分析了一类加权深度多项式逼近器，专门用于有效逼近具有不对称行为的函数，并在数值上证明其性能优于现有方法。", "translation": "在有理逼近理论中，有一个经典结果表明，某些非光滑或奇异函数，如$|x|$和$x^{1/p}$，可以使用有理函数以自由度上的根指数收敛进行有效逼近。相比之下，根据杰克逊定理，多项式逼近只能实现代数收敛。最近的工作表明，复合多项式结构即使在没有光滑性的情况下也能恢复指数逼近率。在这项工作中，我们引入并分析了一类加权深度多项式逼近器，专门用于具有不对称行为（在一侧无界增长，在另一侧衰减）的函数。通过将可学习的深度多项式与单侧权重相乘，我们捕获了局部非光滑性和全局增长。我们通过数值证明，即使所有方法使用相同数量的参数，该框架也优于泰勒、切比雪夫和标准深度多项式逼近器。为了在实践中优化这些逼近器，我们提出了一种基于图的稳定参数化策略。", "summary": "本文提出了一种新颖的“加权深度多项式逼近器”，专为具有不对称行为（如一侧无界增长、另一侧衰减）的函数设计。与传统多项式逼近的代数收敛性或近期深度多项式方法不同，该方法通过将深度多项式与单侧权重相乘，有效捕获局部非光滑性和全局增长。数值实验表明，即使在参数数量相同的情况下，该新框架也比泰勒、切比雪夫和标准深度多项式逼近器表现更优。此外，为实际优化还引入了一种稳定的基于图的参数化策略。", "keywords": "深度多项式逼近, 加权逼近, 非对称函数, 有理逼近, 指数收敛", "comments": "该论文巧妙地将经典逼近理论中的权重概念与深度学习架构相结合，解决了使用深度学习技术有效逼近具有不对称行为函数这一特定挑战。这种方法扩展了深度逼近方法在非光滑函数或仅局部非光滑函数上的应用范围。数值证据显示其性能优于现有方法，是该研究的一个重要亮点。"}}
{"id": "2506.21031", "title": "Large Language Models Acing Chartered Accountancy", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "pdf_url": "http://arxiv.org/pdf/2506.21031v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21031v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "大型语言模型在特许会计领域取得优异表现", "tldr": "本文介绍了CA-Ben，一个用于评估大型语言模型在印度特许会计领域金融、法律和定量推理能力的基准。评估结果显示，某些模型（如Claude 3.5 Sonnet和GPT-4o）表现突出，但在数值计算和法律解释方面仍存在挑战。", "motivation": "尽管大型语言模型（LLMs）在金融实践中发挥着越来越重要的作用，但它们在捕捉和应用领域特定金融知识方面的能力尚不明确。为了填补印度金融背景下的关键空白，本文旨在评估LLMs在特许会计领域的表现。", "method": "本文引入了CA-Ben，一个专门设计用于评估LLMs金融、法律和定量推理能力的特许会计基准。CA-Ben包含来自印度特许会计师协会（ICAI）严格考试的结构化问答数据集，涵盖基础、中级和高级CA课程阶段。研究评估了六个主流LLMs（GPT 4o, LLAMA 3.3 70B, LLAMA 3.1 405B, MISTRAL Large, Claude 3.5 Sonnet, 和 Microsoft Phi 4），并采用了标准化协议。", "result": "评估结果显示，不同LLMs的表现存在差异，其中Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现优于其他模型。然而，在数值计算和法律解释方面出现了显著挑战。", "conclusion": "研究结果强调了当前大型语言模型的优势和局限性。未来可以通过混合推理和检索增强生成方法来改进LLMs，特别是在定量分析和准确法律解释方面。", "translation": "先进的智能系统，特别是大型语言模型（LLMs），正通过自然语言处理（NLP）的进步显著重塑金融实践。然而，这些模型有效捕捉和应用领域特定金融知识的程度仍不确定。为解决广阔的印度金融背景下的一个关键空白，本文引入了CA-Ben，一个专门设计用于评估LLMs金融、法律和定量推理能力的特许会计基准。CA-Ben包含来自印度特许会计师协会（ICAI）进行的严格考试的结构化问答数据集，涵盖基础、中级和高级CA课程阶段。使用标准化协议评估了六个主要LLMs，即GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。结果表明性能存在差异，其中Claude 3.5 Sonnet和GPT-4o表现优于其他模型，尤其是在概念和法律推理方面。在数值计算和法律解释方面出现了显著挑战。研究结果强调了当前LLMs的优势和局限性，并提出了通过混合推理和检索增强生成方法进行未来改进的建议，特别是在定量分析和准确法律解释方面。", "summary": "本文介绍了CA-Ben，一个专门为评估大型语言模型（LLMs）在印度特许会计领域的金融、法律和定量推理能力而设计的基准。该基准的数据集来源于印度特许会计师协会（ICAI）的考试。研究评估了GPT-4o、Claude 3.5 Sonnet等六个主流LLMs，发现Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现突出，但在数值计算和法律解释方面仍面临挑战。研究强调了当前LLMs的优缺点，并建议未来通过混合推理和检索增强生成方法进行改进。", "keywords": "大型语言模型, 特许会计, 金融推理, 法律推理, CA-Ben", "comments": "本文通过创建CA-Ben这一特定领域的基准，创新性地评估了大型语言模型在特许会计这一复杂专业领域的应用潜力。其重要性在于揭示了当前LLMs在处理金融、法律和定量推理方面的具体能力边界，特别是指出了数值计算和法律解释的局限性，为未来LLM在专业领域的应用和改进指明了方向。"}}
{"id": "2506.20944", "title": "E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs", "authors": ["Van-Hoang Phan", "Long-Khanh Pham", "Dang Vu", "Anh-Duy Tran", "Minh-Son Dao"], "summary": "The rapid spread of misinformation in mobile and wireless networks presents\ncritical security challenges. This study introduces a training-free,\nretrieval-based multimodal fact verification system that leverages pretrained\nvision-language models and large language models for credibility assessment. By\ndynamically retrieving and cross-referencing trusted data sources, our approach\nmitigates vulnerabilities of traditional training-based models, such as\nadversarial attacks and data poisoning. Additionally, its lightweight design\nenables seamless edge device integration without extensive on-device\nprocessing. Experiments on two fact-checking benchmarks achieve SOTA results,\nconfirming its effectiveness in misinformation detection and its robustness\nagainst various attack vectors, highlighting its potential to enhance security\nin mobile and wireless communication environments.", "comment": "Accepted to AsiaCCS 2025 @ SCID", "pdf_url": "http://arxiv.org/pdf/2506.20944v1", "categories": ["cs.MM", "cs.CR"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.20944v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "E-FreeM2: 通过MLLMs实现高效免训练的多尺度跨模态新闻验证", "tldr": "E-FreeM2是一个无需训练的多模态事实核查系统，利用预训练的视觉语言模型和大型语言模型，通过动态检索和交叉引用信任数据源来检测虚假信息，并在基准测试中取得SOTA结果。", "motivation": "移动和无线网络中虚假信息的快速传播带来了严峻的安全挑战，传统基于训练的模型容易受到对抗性攻击和数据投毒。", "method": "本研究引入了一个免训练、基于检索的多模态事实验证系统E-FreeM2，该系统利用预训练的视觉语言模型（VLM）和大型语言模型（LLM）进行可信度评估。通过动态检索和交叉引用信任数据源，该方法规避了传统训练模型的脆弱性。此外，其轻量级设计支持边缘设备集成。", "result": "在两个事实核查基准测试中取得了SOTA结果，证实了其在虚假信息检测方面的有效性以及对各种攻击向量的鲁棒性。", "conclusion": "E-FreeM2的提出及其在基准测试中的优异表现，凸显了其在增强移动和无线通信环境安全方面的潜力。", "translation": "移动和无线网络中虚假信息的快速传播带来了严峻的安全挑战。本研究引入了一个免训练、基于检索的多模态事实验证系统，该系统利用预训练的视觉语言模型和大型语言模型进行可信度评估。通过动态检索和交叉引用信任数据源，我们的方法减轻了传统基于训练模型的脆弱性，例如对抗性攻击和数据投毒。此外，其轻量级设计使得无需大量的设备上处理即可无缝集成到边缘设备。在两个事实核查基准测试中进行的实验取得了最先进（SOTA）的结果，证实了其在虚假信息检测方面的有效性以及对各种攻击向量的鲁棒性，突显了其在增强移动和无线通信环境安全方面的潜力。", "summary": "E-FreeM2是一个高效的免训练多模态事实核查系统，旨在应对移动和无线网络中的虚假信息传播挑战。它利用预训练的视觉语言模型和大型语言模型，通过动态检索和交叉引用可信数据源进行可信度评估，有效规避了传统训练模型的漏洞。该系统设计轻量化，易于集成到边缘设备，并在两个事实核查基准测试中取得了最先进的结果，证明了其在检测虚假信息和抵御攻击方面的强大能力，对提升移动和无线通信安全具有重要意义。", "keywords": "虚假信息检测, 多模态验证, 免训练, 视觉语言模型, 事实核查", "comments": "该论文的创新点在于提出了一个“免训练”的多模态事实核查系统，这显著降低了模型对大量标注数据的依赖，并增强了对对抗性攻击和数据投毒的鲁棒性。其轻量级设计也使其适用于资源受限的边缘设备，具有很高的实用价值。在当前虚假信息泛滥的背景下，这种高效且鲁棒的解决方案具有重要的社会和技术意义。"}}
{"id": "2506.21178", "title": "UAIbot: Beginner-friendly web-based simulator for interactive robotics learning and research", "authors": ["Johnata Brayan", "Armando Alves Neto", "Pavel Petrovič", "Gustavo M Freitas", "Vinicius Mariano Gonçalves"], "summary": "This paper presents UAIbot, a free and open-source web-based robotics\nsimulator designed to address the educational and research challenges\nconventional simulation platforms generally face. The Python and JavaScript\ninterfaces of UAIbot enable accessible hands-on learning experiences without\ncumbersome installations. By allowing users to explore fundamental mathematical\nand physical principles interactively, ranging from manipulator kinematics to\npedestrian flow dynamics, UAIbot provides an effective tool for deepening\nstudent understanding, facilitating rapid experimentation, and enhancing\nresearch dissemination.", "comment": "12 pages, 8 figures, submitted to Springer proceedings", "pdf_url": "http://arxiv.org/pdf/2506.21178v1", "categories": ["cs.RO", "68T40", "I.2.9; I.6.3"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21178v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "UAIbot：面向交互式机器人学习与研究的初学者友好型网络模拟器", "tldr": "UAIbot是一个免费开源的网络机器人模拟器，通过Python和JavaScript接口提供便捷的动手学习体验，以解决传统模拟平台的教育和研究挑战。", "motivation": "传统模拟平台在教育和研究方面面临挑战，因此需要一个更易于访问和使用的工具。", "method": "UAIbot是一个免费且开源的网络机器人模拟器，通过Python和JavaScript接口实现，允许用户交互式地探索从机械臂运动学到行人流动力学的基本数学和物理原理。", "result": "UAIbot能够加深学生理解、促进快速实验并增强研究传播。", "conclusion": "UAIbot为交互式机器人学习和研究提供了一个有效的工具。", "translation": "本文介绍了UAIbot，一个免费开源的网络机器人模拟器，旨在解决传统模拟平台普遍面临的教育和研究挑战。UAIbot的Python和JavaScript接口无需繁琐的安装，即可提供便捷的动手学习体验。通过允许用户交互式地探索从机械臂运动学到行人流动力学的基本数学和物理原理，UAIbot为加深学生理解、促进快速实验和增强研究传播提供了有效的工具。", "summary": "UAIbot是一个免费开源的网络机器人模拟器，旨在克服传统模拟平台在教育和研究上的局限性。它通过Python和JavaScript接口提供易于访问的动手学习体验，使用户能够交互式地探索机器人学和物理原理，从而有效地加深理解、加速实验并促进研究成果的传播。", "keywords": "机器人模拟器, 网络平台, 交互式学习, 开源, 机器人教育", "comments": "UAIbot的创新之处在于其网络化、开源和无需安装的特性，极大地降低了机器人学习和研究的门槛，使其对初学者更加友好。其支持交互式探索物理和数学原理的能力，对于教育和研究实践都具有重要意义。"}}
{"id": "2506.21182", "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21182v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21182v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "维护MTEB：实现嵌入基准测试的长期可用性和可复现性", "tldr": "本文关注MTEB（大规模文本嵌入基准）的工程维护，以确保其长期可用性和可复现性，通过持续集成、处理社区贡献和扩展基准来提升其质量和相关性。", "motivation": "MTEB已成为文本嵌入模型评估的标准平台，但之前的研究侧重于核心基准方法。本文的动机是解决工程方面的问题，以确保MTEB的持续可复现性和可扩展性。", "method": "作者通过维护强大的持续集成管道来验证数据集完整性、自动化测试执行和评估基准结果的泛化性。他们还详细介绍了增强可复现性和可用性的设计选择，并讨论了处理社区贡献以及用新任务和数据集扩展基准的策略。", "result": "这些工程实践有助于MTEB变得更加全面，同时保持质量和相关性。MTEB的规模得到了扩展，且其质量和与领域的关联性得以维持。", "conclusion": "本文的经验为面临类似挑战（在机器学习评估框架中确保可复现性和可用性）的基准维护者提供了宝贵的见解。", "translation": "大规模文本嵌入基准 (MTEB) 已成为文本嵌入模型的事实标准评估平台。虽然先前的工作已经建立了核心基准方法，但本文侧重于确保MTEB持续可复现性和可扩展性的工程方面。我们介绍了维护强大持续集成管道的方法，这些管道验证数据集完整性、自动化测试执行并评估基准结果的泛化性。我们详细介绍了共同增强可复现性和可用性的设计选择。此外，我们讨论了处理社区贡献以及用新任务和数据集扩展基准的策略。这些工程实践在扩展MTEB使其变得更全面、同时保持质量并最终与该领域保持相关性方面发挥了重要作用。我们的经验为面临类似挑战（在机器学习评估框架中确保可复现性和可用性）的基准维护者提供了宝贵的见解。MTEB 存储库可在以下网址获取：https://github.com/embeddings-benchmark/mteb", "summary": "本文关注MTEB（大规模文本嵌入基准）的工程维护，旨在确保其长期可用性和可复现性。通过实施强大的持续集成管道，验证数据完整性、自动化测试并评估结果泛化性，作者提升了MTEB的质量。论文还讨论了处理社区贡献和扩展基准的策略，这些工程实践使得MTEB在保持质量的同时变得更加全面和相关，并为其他基准维护者提供了宝贵经验。", "keywords": "MTEB, 文本嵌入, 基准测试, 可复现性, 工程实践", "comments": "本文的创新点在于将工程实践提升到与核心基准方法同等重要的地位，强调了在大型机器学习评估框架中维护和扩展性对于长期可用性和可复现性的关键作用。其重要性在于为其他复杂的基准项目提供了实用的指导和经验，有助于提升整个机器学习社区的评估标准和实践。"}}
{"id": "2506.20911", "title": "FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing", "authors": ["Advait Gupta", "Rishie Raj", "Dang Nguyen", "Tianyi Zhou"], "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A$^*$\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A$^*$\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA$^*$'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A$^*$\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA$^*$ is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20911v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20911v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FaSTA$^*$: 结合子程序挖掘的快慢工具路径代理，用于高效多轮图像编辑", "tldr": "FaSTA$^*$是一个高效的神经符号代理，通过结合LLM规划和A*搜索，并挖掘可重用子程序，来解决多轮图像编辑任务。", "motivation": "解决具有挑战性的多轮图像编辑任务，例如“检测并重新着色长凳为粉色，移除猫，重新着色墙壁为黄色”等，并提高计算效率。", "method": "FaSTA$^*$结合了大型语言模型（LLM）进行快速、高层次的子任务规划，以及A$^*$搜索进行慢速、准确的工具使用和每个子任务的局部搜索，以找到成本高效的工具路径。为了节省A$^*$在类似子任务上的成本，它通过LLM对先前成功的工具路径进行归纳推理，持续提取/改进常用的子程序，并将其作为新工具在未来的任务中重用，采用自适应的快慢规划：优先探索高级子程序，失败时才激活低级A$^*$搜索。", "result": "FaSTA$^*$在计算效率方面显著优于最近的图像编辑方法，同时在成功率方面与最先进的基线保持竞争力。", "conclusion": "FaSTA$^*$通过结合LLM的高级规划和子程序挖掘，以及A$^*$的精确搜索，实现了高效且成功的复杂多轮图像编辑，显著提升了计算效率。", "translation": "我们开发了一种成本高效的神经符号代理，以解决具有挑战性的多轮图像编辑任务，例如“检测图像中的长凳并将其重新着色为粉色。此外，为了更清晰的视图，移除猫并将墙壁重新着色为黄色。”它结合了大型语言模型（LLM）的快速、高层次子任务规划与每个子任务的慢速、准确、工具使用和局部A$^*$搜索，以找到成本高效的工具路径——一系列对AI工具的调用。为了节省A$^*$在类似子任务上的成本，我们通过LLM对先前成功的工具路径进行归纳推理，持续提取/改进常用子程序，并将其作为新工具用于未来的任务中，采用自适应的快慢规划，其中首先探索更高级别的子程序，并且只有当它们失败时，才激活低级别的A$^*$搜索。可重用的符号子程序显著节省了应用于类似图像的相同类型子任务的探索成本，从而产生了一种类人快慢工具路径代理“FaSTA$^*$”：首先由LLM尝试快速子任务规划，然后是基于规则的每个子任务的子程序选择，预计可以覆盖大多数任务，而慢速A$^*$搜索仅在新颖和具有挑战性的子任务中触发。通过与最近的图像编辑方法进行比较，我们证明FaSTA$^*$在计算效率方面显著更高，同时在成功率方面与最先进的基线保持竞争力。", "summary": "FaSTA$^*$是一个新颖的神经符号代理，专为高效处理复杂的多轮图像编辑任务而设计。它巧妙地结合了大型语言模型（LLM）进行高层次、快速的子任务规划，以及A$^*$搜索进行低层次、精确的工具使用。为了进一步优化效率，FaSTA$^*$引入了子程序挖掘机制，通过LLM从成功的工具路径中学习并重用常见的操作序列，从而显著降低了重复性子任务的计算成本。这种快慢结合的规划策略使得FaSTA$^*$在保持与现有技术相当的成功率的同时，大幅提升了计算效率。", "keywords": "图像编辑, 神经符号代理, LLM, A*搜索, 子程序挖掘", "comments": "这篇论文通过引入“快慢”规划范式和子程序挖掘机制，在多轮图像编辑领域展现了创新。它有效地结合了LLM的通用规划能力和A$^*$的精确搜索，并通过学习和重用子程序，解决了传统方法在处理重复性任务时的效率问题。这种神经符号方法提供了一个有前景的框架，可将符号推理与神经网络的能力结合起来，以提高复杂任务的效率和鲁棒性。其重要性在于，它为创建更智能、更成本高效的AI代理来执行复杂、多步骤的现实世界任务提供了一条途径。"}}
{"id": "2506.20686", "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models", "authors": ["Hoa La", "Ahan Gupta", "Alex Morehead", "Jianlin Cheng", "Minjia Zhang"], "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.", "comment": "13 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20686v1", "categories": ["q-bio.BM", "cs.DC", "cs.LG", "cs.PF"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.20686v1", "date": "2025-06-24", "updated": "2025-06-24", "AI": {"title_translation": "MegaFold：加速蛋白质结构预测模型的系统级优化", "tldr": "MegaFold通过系统级优化加速AlphaFold3等蛋白质结构预测模型的训练，显著减少内存使用并提高训练速度，支持更长的序列长度。", "motivation": "AlphaFold3等蛋白质结构预测模型引入了计算和内存密集型操作、2D注意力机制和检索增强数据管道，这些共同阻碍了其训练的可扩展性，并导致高昂的系统成本。", "method": "MegaFold通过以下方式解决关键瓶颈：1) 提前缓存以消除检索增强数据管道中的GPU空闲时间；2) 基于Triton的内核用于异构设备上的内存高效EvoAttention；3) 对AF3中常见和关键的小型操作进行深度融合。", "result": "在NVIDIA H200和AMD MI250 GPU上的评估表明，MegaFold将AF3训练的峰值内存使用量减少了高达1.23倍，并将每次迭代的训练时间分别提高了高达1.73倍和1.62倍。更重要的是，MegaFold能够训练比PyTorch基线长1.35倍的序列长度而不会出现内存不足。", "conclusion": "MegaFold显著提高了现代蛋白质折叠模型的可扩展性，通过系统级优化解决了AlphaFold3训练中的主要性能瓶颈。", "translation": "蛋白质结构预测模型，如AlphaFold3（AF3），通过将科学知情的架构改变融入到Transformer架构中，推动了生物分子建模的前沿。然而，这些进步带来了高昂的系统成本，引入了：计算和内存密集型操作、2D注意力机制以及检索增强数据管道，这些共同阻碍了AF3训练的可扩展性。在这项工作中，我们提出了MegaFold，一个加速AF3训练的跨平台系统。MegaFold通过提前缓存来消除检索增强数据管道中的GPU空闲时间，基于Triton的内核用于异构设备上的内存高效EvoAttention，以及对AF3中常见和关键的小型操作进行深度融合，解决了关键瓶颈。在NVIDIA H200和AMD MI250 GPU上的评估表明，MegaFold将AF3训练的峰值内存使用量减少了高达1.23倍，并将每次迭代的训练时间分别提高了高达1.73倍和1.62倍。更重要的是，与PyTorch基线相比，MegaFold能够训练长1.35倍的序列长度而不会出现内存不足，显著提高了现代蛋白质折叠模型的可扩展性。我们已在https://github.com/Supercomputing-System-AI-Lab/MegaFold/开源了我们的代码。", "summary": "MegaFold是一个跨平台系统，旨在加速AlphaFold3等蛋白质结构预测模型的训练。它通过提前缓存、基于Triton的内存高效内核和深度融合等系统级优化，解决了计算和内存密集型操作、2D注意力以及检索增强数据管道带来的可扩展性挑战。实验结果表明，MegaFold显著降低了内存使用，提高了训练速度，并支持更长的蛋白质序列训练，从而提升了现代蛋白质折叠模型的整体可扩展性。", "keywords": "蛋白质结构预测, AlphaFold3, 系统优化, 训练加速, MegaFold", "comments": "MegaFold的创新之处在于其针对AlphaFold3训练瓶颈的系统级优化，特别是跨平台支持和对异构设备的优化。通过解决GPU空闲时间、内存效率和操作融合等问题，该工作显著提升了蛋白质结构预测模型的训练效率和可扩展性，对于生物分子建模领域的高效研究具有重要意义。"}}
{"id": "2506.20810", "title": "FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs", "authors": ["Shashwat Khandelwal", "Jakoba Petri-Koenig", "Thomas B. Preußer", "Michaela Blott", "Shreejith Shanker"], "summary": "Recurrent neural networks (RNNs), particularly LSTMs, are effective for\ntime-series tasks like sentiment analysis and short-term stock prediction.\nHowever, their computational complexity poses challenges for real-time\ndeployment in resource constrained environments. While FPGAs offer a promising\nplatform for energy-efficient AI acceleration, existing tools mainly target\nfeed-forward networks, and LSTM acceleration typically requires full custom\nimplementation. In this paper, we address this gap by leveraging the\nopen-source and extensible FINN framework to enable the generalized deployment\nof LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open\nNeural Network Exchange (ONNX) specification to model the recurrent nature of\nLSTM computations, enabling support for mixed quantisation within them and\nfunctional verification of LSTM-based models. Furthermore, we introduce custom\ntransformations within the FINN compiler to map the quantised ONNX computation\ngraph to hardware blocks from the HLS kernel library of the FINN compiler and\nVitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM\nmodel for a mid-price stock prediction task using the widely used dataset and\ngenerating a corresponding hardware IP of the model using our flow, targeting\nthe XCZU7EV device. We show that the generated quantised ConvLSTM accelerator\nthrough our flow achieves a balance between performance (latency) and resource\nconsumption, while matching (or bettering) inference accuracy of\nstate-of-the-art models with reduced precision. We believe that the\ngeneralisable nature of the proposed flow will pave the way for\nresource-efficient RNN accelerator designs on FPGAs.", "comment": "9 pages, 6 figures, 5 tables, Accepted for publication in IEEE\n  FPL-2025 (https://2025.fpl.org/)", "pdf_url": "http://arxiv.org/pdf/2506.20810v1", "categories": ["cs.LG", "cs.AI", "cs.AR", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20810v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "FINN-GL：面向FPGA加速LSTM的广义混合精度扩展", "tldr": "FINN-GL通过扩展FINN框架，实现了在FPGA上部署混合精度量化的LSTM，解决了RNNs在资源受限环境中实时部署的挑战，并在股票预测任务中验证了其性能与资源效率。", "motivation": "循环神经网络（RNNs），特别是LSTMs，虽然在时间序列任务中表现出色，但其计算复杂性使得在资源受限环境中进行实时部署面临挑战。现有的FPGA加速工具主要针对前馈网络，且LSTM加速通常需要完全定制实现，存在一个空白。", "method": "本文利用开源可扩展的FINN框架，通过ONNX规范中的Scan操作来建模LSTM计算的循环特性，支持混合精度量化和功能验证。此外，在FINN编译器中引入了自定义转换，将量化后的ONNX计算图映射到FINN编译器和Vitis HLS的HLS内核库中的硬件块。", "result": "所提出的工具流生成的量化ConvLSTM加速器在性能（延迟）和资源消耗之间取得了平衡，同时在精度上与最先进的低精度模型持平或更优。", "conclusion": "该研究提出的通用化流程有望为FPGA上资源高效的RNN加速器设计铺平道路。", "translation": "循环神经网络（RNN），特别是长短期记忆网络（LSTM），在情感分析和短期股票预测等时间序列任务中表现出色。然而，它们的计算复杂性给资源受限环境中的实时部署带来了挑战。虽然FPGA为节能AI加速提供了一个有前景的平台，但现有工具主要针对前馈网络，且LSTM加速通常需要完全定制实现。在本文中，我们通过利用开源和可扩展的FINN框架来弥补这一空白，从而实现LSTM在FPGA上的广义部署。具体来说，我们利用Open Neural Network Exchange (ONNX) 规范中的Scan操作来建模LSTM计算的循环特性，从而支持其中的混合量化和基于LSTM模型的功能验证。此外，我们在FINN编译器中引入了自定义转换，将量化后的ONNX计算图映射到FINN编译器和Vitis HLS的HLS内核库中的硬件块。我们通过训练一个量化的ConvLSTM模型用于中期股票预测任务，并使用我们提出的流程生成该模型的相应硬件IP（目标设备为XCZU7EV），从而验证了所提出的工具流。我们表明，通过我们流程生成的量化ConvLSTM加速器在性能（延迟）和资源消耗之间取得了平衡，同时在推理精度上与最先进的低精度模型持平（或更优）。我们相信，所提出流程的通用性将为FPGA上资源高效的RNN加速器设计铺平道路。", "summary": "本文提出FINN-GL，一个基于FINN框架的广义混合精度扩展，旨在解决LSTM在资源受限FPGA上实时部署的挑战。通过利用ONNX的Scan操作和自定义FINN编译器转换，FINN-GL实现了LSTM的混合精度量化和硬件映射。实验证明，该方法生成的量化ConvLSTM加速器在保持高推理精度的同时，显著优化了性能与资源消耗的平衡。", "keywords": "FPGA, LSTM, 混合精度, FINN, 神经网络加速", "comments": "这项工作通过扩展FINN框架以支持RNNs（特别是LSTMs）的混合精度量化和FPGA部署，填补了现有工具的空白。其创新之处在于利用ONNX Scan操作来处理RNN的循环特性，并引入自定义编译器转换。这对于实现资源高效的AI加速器设计具有重要意义，尤其是在边缘计算领域。"}}
{"id": "2506.21265", "title": "Active Disturbance Rejection Control for Trajectory Tracking of a Seagoing USV: Design, Simulation, and Field Experiments", "authors": ["Jelmer van der Saag", "Elia Trevisan", "Wouter Falkena", "Javier Alonso-Mora"], "summary": "Unmanned Surface Vessels (USVs) face significant control challenges due to\nuncertain environmental disturbances like waves and currents. This paper\nproposes a trajectory tracking controller based on Active Disturbance Rejection\nControl (ADRC) implemented on the DUS V2500. A custom simulation incorporating\nrealistic waves and current disturbances is developed to validate the\ncontroller's performance, supported by further validation through field tests\nin the harbour of Scheveningen, the Netherlands, and at sea. Simulation results\ndemonstrate that ADRC significantly reduces cross-track error across all tested\nconditions compared to a baseline PID controller but increases control effort\nand energy consumption. Field trials confirm these findings while revealing a\nfurther increase in energy consumption during sea trials compared to the\nbaseline.", "comment": "Accepted for presentation at IROS 2025. Submitted version", "pdf_url": "http://arxiv.org/pdf/2506.21265v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21265v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "海上USV轨迹跟踪的主动抗扰控制：设计、仿真与实地实验", "tldr": "本文提出并验证了基于主动抗扰控制（ADRC）的USV轨迹跟踪控制器，在仿真和实地测试中表现出更好的抗扰性能，但能耗增加。", "motivation": "无人水面艇（USVs）由于波浪和水流等不确定的环境扰动而面临严峻的控制挑战。", "method": "本文提出了一种基于主动抗扰控制（ADRC）的轨迹跟踪控制器，并将其应用于DUS V2500。通过开发包含真实波浪和水流扰动的定制仿真来验证控制器性能，并通过在荷兰斯赫弗宁根港口和海上进行实地测试进一步验证。", "result": "仿真结果表明，与基线PID控制器相比，ADRC在所有测试条件下显著降低了横向跟踪误差，但增加了控制工作量和能耗。实地试验证实了这些发现，同时揭示了海上试验期间能耗相对于基线进一步增加。", "conclusion": "主动抗扰控制（ADRC）能有效提高海上无人艇在复杂环境扰动下的轨迹跟踪精度，但需权衡其增加的控制能耗。", "translation": "无人水面艇（USVs）由于波浪和水流等不确定的环境扰动而面临严峻的控制挑战。本文提出了一种基于主动抗扰控制（ADRC）的轨迹跟踪控制器，并将其应用于DUS V2500。开发了一种包含真实波浪和水流扰动的定制仿真来验证控制器的性能，并通过在荷兰斯赫弗宁根港口和海上进行实地测试进一步验证。仿真结果表明，与基线PID控制器相比，ADRC在所有测试条件下显著降低了横向跟踪误差，但增加了控制工作量和能耗。实地试验证实了这些发现，同时揭示了海上试验期间能耗相对于基线进一步增加。", "summary": "本文针对无人水面艇在复杂环境扰动下的轨迹跟踪挑战，提出并验证了一种基于主动抗扰控制（ADRC）的控制器。通过定制仿真和实地试验（包括港口和海上测试），研究表明ADRC能显著提高轨迹跟踪精度，有效抑制横向误差，但同时也会导致控制工作量和能源消耗的增加。", "keywords": "主动抗扰控制, 无人水面艇, 轨迹跟踪, 环境扰动, 实地试验", "comments": "这项研究通过结合仿真和真实世界实地试验，全面验证了ADRC在海上无人艇轨迹跟踪中的有效性。其创新点在于将ADRC应用于USV的复杂环境扰动抑制，并量化了其性能提升与能耗增加的权衡。对能耗的关注是重要的实际考量。"}}
{"id": "2506.21349", "title": "Generalizable Neural Electromagnetic Inverse Scattering", "authors": ["Yizhe Cheng", "Chunxun Tian", "Haoru Wang", "Wentao Zhu", "Xiaoxuan Ma", "Yizhou Wang"], "summary": "Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in\napplications such as medical imaging, where the goal is to reconstruct the\nrelative permittivity from scattered electromagnetic field. This inverse\nprocess is inherently ill-posed and highly nonlinear, making it particularly\nchallenging. A recent machine learning-based approach, Img-Interiors, shows\npromising results by leveraging continuous implicit functions. However, it\nrequires case-specific optimization, lacks generalization to unseen data, and\nfails under sparse transmitter setups (e.g., with only one transmitter). To\naddress these limitations, we revisit EISP from a physics-informed perspective,\nreformulating it as a two stage inverse transmission-scattering process. This\nformulation reveals the induced current as a generalizable intermediate\nrepresentation, effectively decoupling the nonlinear scattering process from\nthe ill-posed inverse problem. Built on this insight, we propose the first\ngeneralizable physics-driven framework for EISP, comprising a current estimator\nand a permittivity solver, working in an end-to-end manner. The current\nestimator explicitly learns the induced current as a physical bridge between\nthe incident and scattered field, while the permittivity solver computes the\nrelative permittivity directly from the estimated induced current. This design\nenables data-driven training and generalizable feed-forward prediction of\nrelative permittivity on unseen data while maintaining strong robustness to\ntransmitter sparsity. Extensive experiments show that our method outperforms\nstate-of-the-art approaches in reconstruction accuracy, generalization, and\nrobustness. This work offers a fundamentally new perspective on electromagnetic\ninverse scattering and represents a major step toward cost-effective practical\nsolutions for electromagnetic imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21349v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21349v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可泛化神经电磁逆散射", "tldr": "提出一种可泛化、鲁棒的物理驱动电磁逆散射问题（EISP）框架，通过引入感应电流作为中间表示，解决了现有方法泛化能力差和稀疏发射器设置下的问题。", "motivation": "电磁逆散射问题（EISP）本质上是不适定且高度非线性的。现有基于机器学习的方法（如Img-Interiors）缺乏对未见数据的泛化能力，需要针对性优化，并且在稀疏发射器设置下（例如仅有一个发射器）表现不佳。", "method": "将EISP重新表述为两阶段逆透射-散射过程，发现感应电流是可泛化的中间表示，从而有效解耦了非线性散射过程与不适定逆问题。在此基础上，提出首个可泛化物理驱动的EISP框架，包含一个电流估计器（显式学习感应电流作为入射场和散射场之间的物理桥梁）和一个介电常数求解器（直接从估计的感应电流计算相对介电常数）。", "result": "广泛实验表明，该方法在重建精度、泛化能力和鲁棒性方面优于现有最先进方法，并对发射器稀疏性保持强大的鲁棒性。", "conclusion": "这项工作为电磁逆散射提供了全新的视角，是实现经济高效的电磁成像实际解决方案的重要一步。", "translation": "解决电磁逆散射问题（EISP）在医学成像等应用中至关重要，其目标是从散射电磁场中重建相对介电常数。这个逆过程本质上是不适定且高度非线性的，使其极具挑战性。最近一种基于机器学习的方法Img-Interiors，通过利用连续隐函数显示出有希望的结果。然而，它需要针对特定情况进行优化，缺乏对未见数据的泛化能力，并且在稀疏发射器设置下（例如仅有一个发射器）会失效。为了解决这些限制，我们从物理信息驱动的角度重新审视EISP，将其重新表述为两阶段逆透射-散射过程。这种表述揭示了感应电流作为一种可泛化的中间表示，有效地将非线性散射过程与不适定逆问题解耦。基于这一见解，我们提出了首个可泛化物理驱动的EISP框架，包括一个电流估计器和一个介电常数求解器，以端到端的方式工作。电流估计器显式学习感应电流作为入射场和散射场之间的物理桥梁，而介电常数求解器直接从估计的感应电流计算相对介电常数。这种设计实现了对未见数据进行数据驱动训练和可泛化的相对介电常数前向预测，同时对发射器稀疏性保持强大的鲁棒性。广泛实验表明，我们的方法在重建精度、泛化能力和鲁棒性方面优于现有最先进方法。这项工作为电磁逆散射提供了全新的视角，是实现经济高效的电磁成像实际解决方案的重要一步。", "summary": "本文针对电磁逆散射问题（EISP）提出了一种新的可泛化物理驱动框架。该问题本质上是不适定且高度非线性的，现有机器学习方法存在泛化能力不足和稀疏发射器设置下失效的问题。作者将EISP重新表述为两阶段过程，并发现感应电流作为可泛化的中间表示，有效解耦了问题。所提出的框架包含电流估计器和介电常数求解器，实现了数据驱动训练和对未见数据的鲁棒、可泛化预测。实验证明，该方法在精度、泛化和鲁棒性方面均优于现有最先进方法，为实际电磁成像解决方案奠定了基础。", "keywords": "电磁逆散射, 泛化, 物理信息, 神经网络, 感应电流", "comments": "本文的创新点在于通过将电磁逆散射问题重新表述为两阶段过程，并引入感应电流作为可泛化的中间表示，有效解耦了非线性散射与不适定逆问题，从而显著提升了模型的泛化能力和对稀疏发射器设置的鲁棒性。这种物理信息驱动的方法克服了现有纯数据驱动方法在面对未见数据和稀疏输入时的局限性，是电磁成像领域的重要进展，具有重要的实际应用价值。"}}
{"id": "2506.21536", "title": "PsyLite Technical Report", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21536v1", "categories": ["cs.AI", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.21536v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PsyLite 技术报告", "tldr": "PsyLite是一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型代理，通过两阶段训练和创新性条件RAG解决了现有模型在对话安全、场景处理和轻量化部署方面的不足，并在心理咨询专业性和对话安全性上表现出色，支持低内存部署。", "motivation": "现有AI驱动的心理咨询模型在对话安全、详细场景处理和轻量化部署方面存在不足。", "method": "本研究提出了PsyLite，一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型代理。采用两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化）来增强模型的深度推理、心理咨询和安全对话能力。部署时使用Ollama和Open WebUI，并创建自定义工作流Pipelines。设计了创新的条件RAG，在适当时候引入相声幽默元素以增强用户体验，并拒绝危险请求以加强对话安全。模型还使用量化技术（GGUF q4_k_m）实现低硬件部署。", "result": "PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中均优于基线模型。尤其在心理咨询专业性（CPsyCounE得分提高47.6%）和对话安全性（SafeDialBench得分提高2.4%）方面表现突出。模型通过量化技术仅需5GB内存即可运行，实现了低硬件部署。", "conclusion": "PsyLite为资源受限环境下的心理咨询应用提供了一个可行的解决方案，显著提升了AI心理咨询模型的专业性、安全性和部署轻量化。", "translation": "随着数字技术的快速发展，人工智能驱动的心理咨询逐渐成为心理健康领域的重要研究方向。然而，现有模型在对话安全性、细节场景处理和轻量化部署方面仍存在不足。为了解决这些问题，本研究提出了PsyLite，一个基于InternLM2.5-7B-chat基础模型开发的轻量级心理咨询大语言模型代理。通过两阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），PsyLite增强了模型的深度推理能力、心理咨询能力和安全对话能力。在通过Ollama和Open WebUI部署后，利用Pipelines创建了自定义工作流。设计了一种创新的条件RAG，在心理咨询过程中适时引入相声幽默元素以增强用户体验，并拒绝危险请求以加强对话安全性。评估结果显示，PsyLite在中文通用评估（CEval）、心理咨询专业评估（CPsyCounE）和对话安全评估（SafeDialBench）中均优于基线模型，尤其在心理咨询专业性（CPsyCounE得分提高47.6%）和对话安全性（安全得分提高2.4%）方面表现突出。此外，该模型采用量化技术（GGUF q4_k_m）实现了低硬件部署（5GB内存足以运行），为资源受限环境下的心理咨询应用提供了可行的解决方案。", "summary": "本研究提出了PsyLite，一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型代理，旨在解决现有AI心理咨询模型在对话安全、场景处理和轻量化部署方面的不足。PsyLite采用两阶段训练策略和创新的条件RAG，提升了模型的深度推理、心理咨询和安全对话能力，并能适时引入幽默元素以优化用户体验。评估结果表明，PsyLite在专业性、对话安全性及通用能力上均显著优于基线模型，且通过量化技术实现了低内存占用（5GB），为资源受限环境下的心理咨询应用提供了有效且可行的解决方案。", "keywords": "心理咨询, 大语言模型, 轻量化部署, 对话安全, RAG", "comments": "PsyLite的创新点在于其两阶段训练策略（混合蒸馏和ORPO偏好优化）和独特的条件RAG设计，后者通过引入幽默元素提升用户体验，并加强了对话安全性。其轻量化部署能力（仅需5GB内存）是其重要优势，使得心理咨询应用在资源受限环境中成为可能，具有较高的实际应用价值。该研究全面考虑了专业性、安全性和部署便捷性，为AI心理咨询领域的发展提供了有益的探索。"}}
{"id": "2506.21288", "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21288v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21288v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "小型编码器在检测基础性方面可与大型解码器媲美", "tldr": "研究表明，小型编码器模型在检测大型语言模型回答的“基础性”（是否基于提供上下文）方面，能达到与大型解码器模型相当的准确度，同时大幅降低推理延迟。", "motivation": "大型语言模型（LLMs）在缺乏上下文时容易产生不基于事实的推测，导致信息不一致和不可信。为了确保事实一致性和可信度，检测LLM生成内容的基础性至关重要，尤其是在进行昂贵的答案生成之前进行检测可以显著减少推理时间和资源消耗。", "method": "该研究采用轻量级、任务特定的编码器模型（如RoBERTa和NomicBERT），并在精选数据集上进行微调，以检测给定查询是否基于提供的文档。然后，将这些模型的准确性与最先进的LLMs（如Llama3 8B和GPT4o）进行比较。", "result": "轻量级编码器模型（如RoBERTa和NomicBERT）在基础性检测方面的准确性可与最先进的LLMs（如Llama3 8B和GPT4o）相媲美，同时将推理延迟降低了几个数量级。", "conclusion": "轻量级、任务特定的编码器模型是检测LLMs回答基础性的一种高效且准确的替代方案，能够显著降低资源消耗和推理时间，而不牺牲准确性。", "translation": "通过外部上下文增强大型语言模型（LLMs）显著提高了它们在自然语言处理（NLP）任务中的性能。然而，当提供的上下文缺乏信息时，LLMs难以可靠地回答查询，经常诉诸于无根据的猜测或内部知识。基础性——即生成严格由上下文支持的回答——对于确保事实一致性和可信度至关重要。本研究侧重于在LLMs进行昂贵的答案生成之前，检测给定查询是否基于所提供的文档。这种检测机制可以显著减少推理时间和资源消耗。我们表明，轻量级、任务特定的编码器模型，如RoBERTa和NomicBERT，在精选数据集上进行微调后，在基础性检测方面可以达到与最先进的LLMs（如Llama3 8B和GPT4o）相当的准确性，同时将推理延迟降低了几个数量级。代码可在：https://github.com/chandarlab/Hallucinate-less 获取。", "summary": "本研究旨在解决大型语言模型在缺乏上下文时易产生不实信息的问题，通过在答案生成前检测查询的“基础性”来提高事实一致性和可信度。研究发现，经过微调的轻量级编码器模型（如RoBERTa、NomicBERT）在基础性检测方面的准确性可与Llama3 8B和GPT4o等大型LLM媲美，同时大幅降低了推理延迟和资源消耗。", "keywords": "基础性检测, 大型语言模型, 编码器模型, 推理延迟, 事实一致性", "comments": "这篇论文的创新点在于证明了小型、专门化的模型在特定任务（如基础性检测）上可以达到与大型通用模型相当的性能，这对于优化LLM的应用具有重要意义，尤其是在资源受限或需要低延迟的场景下。它提供了一个实用的解决方案来缓解LLM的“幻觉”问题。"}}
{"id": "2506.21314", "title": "A Sampling-Based Adaptive Rank Approach to the Wigner-Poisson System", "authors": ["Andrew Christlieb", "Sining Gong", "Jing-Mei Qiu", "Nanyi Zheng"], "summary": "We develop a mass-conserving, adaptive-rank solver for the 1D1V\nWigner-Poisson system. Our work is motivated by applications to the study of\nthe stopping power of $\\alpha$ particles at the National Ignition Facility\n(NIF). In this regime, electrons are in a warm dense state, requiring more than\na standard kinetic model. They are hot enough to neglect Pauli exclusion, yet\nquantum enough to require accounting for uncertainty. The Wigner-Poisson system\ncaptures these effects but presents challenges due to its nonlocal nature.\nBased on a second-order Strang splitting method, we first design a full-rank\nsolver with a structure-preserving Fourier update that ensures the intermediate\nsolutions remain real-valued (up to machine precision), improving upon previous\nmethods. Simulations demonstrate that the solutions exhibit a low rank\nstructure for moderate to high dimensionless Planck constants ($H \\ge 0.1$).\nThis observed low rank structure motivates the development of an adaptive-rank\nsolver, built on a Semi-Lagrangian adaptive-rank (SLAR) scheme for advection\nand an adaptive-rank, structure-preserving Fourier update for the Wigner\nintegral terms, with a rigorous proof of structure-preserving property\nprovided. Our solver achieves $O(N)$ complexity in both storage and computation\ntime, while preserving mass and maintaining momentum accuracy up to the\ntruncation error. The adaptive rank simulations are visually indistinguishable\nfrom the full-rank simulations in capturing solution structures. These results\nhighlight the potential of adaptive rank methods for high-dimensional\nWigner-Poisson simulations, paving the way toward fully kinetic studies of\nstopping power in warm dense plasmas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21314v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21314v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Wigner-Poisson 系统的一种基于采样的自适应秩方法", "tldr": "本文开发了一种用于1D1V Wigner-Poisson系统的质量守恒、自适应秩求解器，该求解器在计算复杂度和存储方面达到O(N)，同时保持精度与全秩模拟相当，为高温稠密等离子体中的阻止本领研究铺平了道路。", "motivation": "该研究的动机是应用于国家点火装置（NIF）中α粒子阻止本领的研究。在此状态下，电子处于温稠密状态，需要超越标准动力学模型的描述。Wigner-Poisson系统能够捕捉量子效应和不确定性，但其非局部性质带来了挑战。此外，模拟显示解在某些条件下呈现低秩结构，这激励了自适应秩求解器的开发。", "method": "本文首先基于二阶Strang分裂方法设计了一个全秩求解器，该求解器包含一个结构保持的傅里叶更新，以确保中间解为实值。在此基础上，开发了一个自适应秩求解器，该求解器采用半拉格朗日自适应秩（SLAR）方案处理对流项，并对Wigner积分项采用自适应秩、结构保持的傅里叶更新，并提供了严格的结构保持性质证明。", "result": "模拟结果表明，对于中到高无量纲普朗克常数($H \\ge 0.1$)，解呈现低秩结构。开发的求解器在存储和计算时间上都达到了O(N)的复杂度，同时保持了质量守恒和动量精度达到截断误差。自适应秩模拟在捕捉解结构方面与全秩模拟在视觉上无法区分。", "conclusion": "这些结果突出了自适应秩方法在高维Wigner-Poisson模拟中的潜力，为温稠密等离子体中阻止本领的完全动力学研究铺平了道路。", "translation": "我们开发了一种用于1D1V Wigner-Poisson系统的质量守恒、自适应秩求解器。我们的工作是受国家点火装置（NIF）中α粒子阻止本领研究应用的启发。在这种状态下，电子处于温稠密状态，需要超越标准动力学模型的描述。它们足够热，可以忽略泡利不相容原理，但又足够量子化，需要考虑不确定性。Wigner-Poisson系统捕捉了这些效应，但由于其非局部性质带来了挑战。基于二阶Strang分裂方法，我们首先设计了一个全秩求解器，该求解器具有结构保持的傅里叶更新，确保中间解保持实值（达到机器精度），改进了以前的方法。模拟表明，对于中到高无量纲普朗斯常数（$H \\ge 0.1$），解呈现低秩结构。观察到的这种低秩结构促使我们开发了一个自适应秩求解器，该求解器基于用于对流的半拉格朗日自适应秩（SLAR）方案和用于Wigner积分项的自适应秩、结构保持傅里叶更新，并提供了结构保持性质的严格证明。我们的求解器在存储和计算时间上都达到了O(N)的复杂度，同时保持质量守恒和动量精度达到截断误差。自适应秩模拟在捕捉解结构方面与全秩模拟在视觉上无法区分。这些结果突出了自适应秩方法在高维Wigner-Poisson模拟中的潜力，为温稠密等离子体中阻止本领的完全动力学研究铺平了道路。", "summary": "本文针对1D1V Wigner-Poisson系统，开发了一种质量守恒的自适应秩求解器，旨在解决温稠密等离子体中α粒子阻止本领研究的需求。该求解器基于二阶Strang分裂和结构保持傅里叶更新，能够有效捕捉Wigner-Poisson系统的非局部量子效应。研究发现，在特定条件下，系统解呈现低秩结构，这促使作者构建了一个高效的自适应秩方案，结合了SLAR对流和自适应秩傅里叶更新。新方法实现了O(N)的计算复杂度和存储，同时在精度上与全秩模拟相当，为未来高维Wigner-Poisson模拟及温稠密等离子体动力学研究提供了有效工具。", "keywords": "Wigner-Poisson系统, 自适应秩, 质量守恒, 温稠密等离子体, 阻止本领", "comments": "该论文的创新之处在于提出了一个高效的自适应秩求解器来处理Wigner-Poisson系统，特别是在解决温稠密等离子体中的量子效应方面。通过利用解的低秩特性，该方法显著降低了计算复杂度和存储需求（O(N)），同时保持了与全秩模拟相当的精度，这对于高维模拟具有重要意义。结构保持的傅里叶更新和严格的证明也增加了方法的鲁棒性。这为未来在NIF等设施中进行更深入的完全动力学研究奠定了基础，解决了传统方法在大规模模拟中面临的计算瓶颈。"}}
{"id": "2506.21463", "title": "Aligning Spoken Dialogue Models from User Interactions", "authors": ["Anne Wu", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez"], "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21463v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21463v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从用户交互中对齐语音对话模型", "tldr": "本文提出了一种新颖的偏好对齐框架，通过用户交互改进实时对话中的语音对话模型，并构建了一个大规模数据集来微调模型，实验证明其能显著提升对话质量。", "motivation": "当前偏好学习方法主要关注基于文本的语言模型，不适用于实时语音交互的复杂性，如更丰富的动态（中断、插话）和缺乏明确的说话人轮次分割。", "method": "提出了一种新颖的偏好对齐框架。构建了一个包含超过15万个偏好对的大规模数据集，这些数据来自原始多轮语音对话并使用AI反馈进行标注，覆盖了语言内容和时间上下文变化。利用离线对齐方法微调了一个全双工自回归语音到语音模型。部署微调后的模型并进行整体性人工评估。", "result": "对通用对话的反馈能持续有效地改进语音对话模型，使其产生更真实、更安全、更符合上下文的交互。评估结果显示了其对单轮对话之外的影响。", "conclusion": "研究结果揭示了在各种动态之间保持良好平衡的重要性，这对于自然的实时语音对话系统至关重要。", "translation": "我们提出了一种新颖的偏好对齐框架，用于通过用户交互改进实时对话中的语音对话模型。当前的偏好学习方法主要关注基于文本的语言模型，不直接适用于实时语音交互的复杂性，例如更丰富的动态（如中断、插话）以及说话人轮次之间没有明确的分割。我们从原始多轮语音对话中创建了一个包含超过15万个偏好对的大规模数据集，并用AI反馈进行标注，以涵盖语言内容和时间上下文变化的偏好。我们利用离线对齐方法对一个全双工自回归语音到语音模型进行微调。广泛的实验表明，对通用对话的反馈可以持续有效地改进语音对话模型，以产生更真实、更安全、更符合上下文的交互。我们部署了微调后的模型并进行了整体性人工评估，以评估其对单轮对话之外的影响。我们的发现揭示了在各种动态之间保持良好平衡的重要性，这对于自然的实时语音对话系统至关重要。", "summary": "本文提出了一种创新的偏好对齐框架，旨在通过利用用户交互数据来提升实时语音对话模型的性能。针对现有偏好学习方法不适用于实时语音交互复杂性的问题，研究团队构建了一个包含超过15万个偏好对的大规模语音对话数据集，并通过AI反馈进行标注。该框架采用离线对齐方法对全双工自回归语音到语音模型进行微调。实验证明，该方法能显著提高语音对话模型的对话质量，使其更真实、安全且上下文更一致。研究还通过人工评估验证了其在多轮对话中的有效性，并强调了实时语音对话系统中动态平衡的关键性。", "keywords": "语音对话模型, 偏好对齐, 实时语音, 用户交互, 语音到语音模型", "comments": "本文的创新点在于提出了一个专门针对实时语音交互的偏好对齐框架，并构建了大规模的语音偏好数据集，这对于克服传统文本偏好学习方法在语音领域面临的挑战至关重要。通过微调全双工语音到语音模型，并进行全面的多轮对话评估，该研究为提升实时对话系统的自然性和鲁棒性提供了有价值的见解。"}}
{"id": "2506.21205", "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations", "authors": ["Elia Trevisan", "Khaled A. Mustafa", "Godert Notten", "Xinwei Wang", "Javier Alonso-Mora"], "summary": "Deploying mobile robots safely among humans requires the motion planner to\naccount for the uncertainty in the other agents' predicted trajectories. This\nremains challenging in traditional approaches, especially with arbitrarily\nshaped predictions and real-time constraints. To address these challenges, we\npropose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI),\na motion planner that incorporates uncertain future motions modelled with\npotentially non-Gaussian stochastic predictions. By leveraging MPPI's\ngradient-free nature, we propose a method that efficiently approximates the\njoint Collision Probability (CP) among multiple dynamic obstacles for several\nhundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This\nenables the rejection of samples exceeding a predefined CP threshold or the\nintegration of CP as a weighted objective within the navigation cost function.\nConsequently, DRA-MPPI mitigates the freezing robot problem while enhancing\nsafety. Real-world and simulated experiments with multiple dynamic obstacles\ndemonstrate DRA-MPPI's superior performance compared to state-of-the-art\napproaches, including Scenario-based Model Predictive Control (S-MPC), Frenet\nplanner, and vanilla MPPI.", "comment": "Accepted for presentation at IROS 2025. Submitted Version", "pdf_url": "http://arxiv.org/pdf/2506.21205v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21205v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "动态风险感知MPPI在人群中移动机器人中的应用：通过高效蒙特卡洛近似", "tldr": "提出了一种名为DRA-MPPI的运动规划器，利用蒙特卡洛近似实时计算碰撞概率，使移动机器人在人群中安全导航，解决了传统方法的挑战。", "motivation": "在人群中安全部署移动机器人时，传统方法难以处理其他代理预测轨迹中的不确定性，尤其是在存在任意形状预测和实时约束的情况下，这给运动规划带来了挑战。", "method": "本文提出动态风险感知模型预测路径积分控制（DRA-MPPI），这是一种运动规划器，它将未来不确定运动（可能通过非高斯随机预测建模）纳入考虑。通过利用MPPI的无梯度特性，该方法通过蒙特卡洛（MC）近似，实时高效地计算多个动态障碍物之间的联合碰撞概率（CP），用于数百条采样轨迹。这使得可以拒绝超过预定义CP阈值的样本，或将CP作为加权目标整合到导航成本函数中。", "result": "真实世界和模拟实验表明，DRA-MPPI在多动态障碍物场景下，性能优于包括基于场景的模型预测控制（S-MPC）、Frenet 规划器和普通MPPI在内的现有方法。DRA-MPPI有效缓解了“机器人冻结”问题并增强了安全性。", "conclusion": "DRA-MPPI通过将蒙特卡洛近似集成到MPPI框架中，实现了对动态障碍物不确定性的高效风险感知处理，从而成功解决了移动机器人在人群中安全导航的挑战，并在实际应用中展现出卓越的性能和更高的安全性。", "translation": "在人类中安全部署移动机器人要求运动规划器考虑其他代理预测轨迹中的不确定性。这在传统方法中仍然具有挑战性，特别是在任意形状的预测和实时约束下。为了解决这些挑战，我们提出了一种动态风险感知模型预测路径积分控制（DRA-MPPI），这是一种运动规划器，它结合了可能具有非高斯随机预测建模的不确定未来运动。通过利用MPPI的无梯度特性，我们提出了一种方法，通过蒙特卡洛（MC）方法实时高效地近似多个动态障碍物之间的联合碰撞概率（CP），用于数百条采样轨迹。这使得可以拒绝超过预定义CP阈值的样本，或将CP作为加权目标整合到导航成本函数中。因此，DRA-MPPI在增强安全性的同时缓解了机器人冻结问题。在具有多个动态障碍物的真实世界和模拟实验中，DRA-MPPI 与包括基于场景的模型预测控制（S-MPC）、Frenet 规划器和普通MPPI在内的现有方法相比，表现出卓越的性能。", "summary": "本文提出了一种名为动态风险感知模型预测路径积分控制（DRA-MPPI）的运动规划器，旨在解决移动机器人在人群中安全导航的挑战。DRA-MPPI利用MPPI的无梯度特性，通过蒙特卡洛近似方法，实时高效地计算多个动态障碍物之间的联合碰撞概率。这种方法允许系统根据碰撞概率阈值拒绝轨迹或将其整合到成本函数中，从而有效缓解了“机器人冻结”问题并显著提高了安全性。实验证明，DRA-MPPI在复杂动态环境中表现优于现有先进方法。", "keywords": "移动机器人, 风险感知, MPPI, 蒙特卡洛近似, 碰撞概率", "comments": "该论文的创新点在于将蒙特卡洛近似引入MPPI框架，以高效处理动态障碍物的非高斯不确定性预测，从而在实时约束下实现风险感知规划。这对于提高移动机器人在复杂人群环境中的安全性和鲁棒性至关重要，有效解决了“机器人冻结”这一实际问题。"}}
{"id": "2506.20922", "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization", "authors": ["Ju-Hyeon Nam", "Dong-Hyun Moon", "Sang-Chul Lee"], "summary": "Image editing techniques have rapidly advanced, facilitating both innovative\nuse cases and malicious manipulation of digital images. Deep learning-based\nmethods have recently achieved high accuracy in pixel-level forgery\nlocalization, yet they frequently struggle with computational overhead and\nlimited representation power, particularly for subtle or complex tampering. In\nthis paper, we propose M2SFormer, a novel Transformer encoder-based framework\ndesigned to overcome these challenges. Unlike approaches that process spatial\nand frequency cues separately, M2SFormer unifies multi-frequency and\nmulti-scale attentions in the skip connection, harnessing global context to\nbetter capture diverse forgery artifacts. Additionally, our framework addresses\nthe loss of fine detail during upsampling by utilizing a global prior map, a\ncurvature metric indicating the difficulty of forgery localization, which then\nguides a difficulty-guided attention module to preserve subtle manipulations\nmore effectively. Extensive experiments on multiple benchmark datasets\ndemonstrate that M2SFormer outperforms existing state-of-the-art models,\noffering superior generalization in detecting and localizing forgeries across\nunseen domains.", "comment": "Accepted in International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.20922v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20922v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "M2SFormer：结合边缘感知难度引导的多光谱多尺度注意力图像伪造定位", "tldr": "M2SFormer是一个新的Transformer编码器框架，通过统一多频率和多尺度注意力，并利用全局先验图和难度引导注意力模块，解决了图像伪造定位中计算开销大和表示能力有限的问题，在多个基准数据集上表现优于现有SOTA模型。", "motivation": "现有的深度学习图像伪造定位方法在像素级别伪造定位上精度高，但通常面临计算开销大和表示能力有限的问题，尤其对于细微或复杂的篡改。", "method": "本文提出了M2SFormer，一个基于Transformer编码器的新型框架。它不像传统方法那样单独处理空间和频率线索，而是统一了跳跃连接中的多频率和多尺度注意力，以更好地捕获多样化的伪造痕迹。此外，M2SFormer通过利用一个全局先验图（一个指示伪造定位难度的曲率度量）来解决上采样过程中精细细节的丢失问题，并引导一个难度引导注意力模块更有效地保留细微篡改。", "result": "在多个基准数据集上进行的广泛实验表明，M2SFormer优于现有的最先进模型，在检测和定位未知领域伪造方面提供了卓越的泛化能力。", "conclusion": "M2SFormer通过其新颖的多光谱多尺度注意力机制和边缘感知难度引导，有效地解决了图像伪造定位中的挑战，并在泛化能力和性能上超越了现有SOTA模型。", "translation": "图像编辑技术发展迅速，既促进了创新应用，也导致了数字图像的恶意篡改。基于深度学习的方法最近在像素级伪造定位方面取得了高精度，但它们经常面临计算开销大和表示能力有限的问题，特别是对于细微或复杂的篡改。在本文中，我们提出了M2SFormer，一个新颖的基于Transformer编码器的框架，旨在克服这些挑战。与单独处理空间和频率线索的方法不同，M2SFormer在跳跃连接中统一了多频率和多尺度注意力，利用全局上下文更好地捕获多样化的伪造痕迹。此外，我们的框架通过利用全局先验图（一个指示伪造定位难度的曲率度量）来解决上采样过程中精细细节的丢失问题，该先验图随后引导一个难度引导注意力模块，以更有效地保留细微篡改。在多个基准数据集上进行的广泛实验表明，M2SFormer优于现有的最先进模型，在检测和定位未知领域伪造方面提供了卓越的泛化能力。", "summary": "M2SFormer是一个基于Transformer编码器的新型图像伪造定位框架，旨在解决现有深度学习方法中存在的计算开销大和表示能力有限的问题。它通过在跳跃连接中统一多频率和多尺度注意力来捕捉复杂的伪造痕迹，并引入一个基于全局先验图的难度引导注意力模块，以有效保留上采样过程中丢失的细微细节。实验结果表明，M2SFormer在多个基准数据集上均优于现有最先进模型，并在跨未知领域的伪造检测和定位方面展现出卓越的泛化能力。", "keywords": "图像伪造定位, 多光谱多尺度注意力, Transformer, 难度引导, M2SFormer", "comments": "M2SFormer的创新之处在于其统一多频率和多尺度注意力的方法，以及引入边缘感知难度引导机制，这有助于更精确地定位细微伪造。该模型通过解决计算开销和表示能力限制，为图像伪造定位领域带来了显著的进步，特别是在泛化能力方面。"}}
{"id": "2506.21036", "title": "An Information-Theoretic Analysis for Federated Learning under Concept Drift", "authors": ["Fu Peng", "Meng Zhang", "Ming Tang"], "summary": "Recent studies in federated learning (FL) commonly train models on static\ndatasets. However, real-world data often arrives as streams with shifting\ndistributions, causing performance degradation known as concept drift. This\npaper analyzes FL performance under concept drift using information theory and\nproposes an algorithm to mitigate the performance degradation. We model concept\ndrift as a Markov chain and introduce the \\emph{Stationary Generalization\nError} to assess a model's capability to capture characteristics of future\nunseen data. Its upper bound is derived using KL divergence and mutual\ninformation. We study three drift patterns (periodic, gradual, and random) and\ntheir impact on FL performance. Inspired by this, we propose an algorithm that\nregularizes the empirical risk minimization approach with KL divergence and\nmutual information, thereby enhancing long-term performance. We also explore\nthe performance-cost tradeoff by identifying a Pareto front. To validate our\napproach, we build an FL testbed using Raspberry Pi4 devices. Experimental\nresults corroborate with theoretical findings, confirming that drift patterns\nsignificantly affect performance. Our method consistently outperforms existing\napproaches for these three patterns, demonstrating its effectiveness in\nadapting concept drift in FL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21036v1", "categories": ["cs.LG", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21036v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "联邦学习在概念漂移下的信息论分析", "tldr": "本文使用信息论分析了联邦学习在概念漂移下的性能，并提出了一种新的算法来减轻性能下降。", "motivation": "现有的联邦学习模型通常在静态数据集上训练，但在现实世界中数据流常常出现分布变化（概念漂移），导致性能下降。", "method": "将概念漂移建模为马尔可夫链；引入“稳态泛化误差”并使用KL散度和互信息推导其上限；研究了三种漂移模式（周期性、渐进式和随机）对FL性能的影响；提出了一种通过KL散度和互信息对经验风险最小化进行正则化的算法；探索了性能-成本权衡；使用Raspberry Pi4设备构建FL测试平台进行验证。", "result": "实验结果与理论发现一致，证实漂移模式显著影响性能；所提出的方法在三种漂移模式下均优于现有方法，证明了其在FL中适应概念漂移的有效性。", "conclusion": "所提出的基于信息论的正则化算法能够有效提高联邦学习在概念漂移下的长期性能，并在不同漂移模式下表现出优越性。", "translation": "近期联邦学习（FL）研究通常在静态数据集上训练模型。然而，现实世界数据常以分布变化的流形式出现，导致性能下降，即概念漂移。本文利用信息论分析了概念漂移下联邦学习的性能，并提出了一种算法来减轻性能下降。我们将概念漂移建模为马尔可夫链，并引入“稳态泛化误差”来评估模型捕获未来未见数据特征的能力。其上限使用KL散度和互信息推导。我们研究了三种漂移模式（周期性、渐进式和随机）及其对FL性能的影响。受此启发，我们提出了一种算法，通过KL散度和互信息对经验风险最小化方法进行正则化，从而提高长期性能。我们还通过识别帕累托前沿探索了性能-成本权衡。为验证我们的方法，我们使用树莓派4设备构建了一个FL测试平台。实验结果与理论发现相符，证实漂移模式显著影响性能。我们的方法在这三种模式下始终优于现有方法，证明了其在FL中适应概念漂移的有效性。", "summary": "这篇论文通过信息论方法分析了联邦学习在概念漂移下的性能下降问题。作者将概念漂移建模为马尔可夫链，并引入“稳态泛化误差”来衡量模型对未来数据的适应能力，其上限通过KL散度和互信息导出。在此基础上，提出了一种新的算法，通过KL散度和互信息对经验风险最小化进行正则化，以提升长期性能。研究还探讨了周期性、渐进式和随机三种漂移模式的影响。实验验证了理论发现，并表明所提出的方法在适应联邦学习中的概念漂移方面优于现有方法。", "keywords": "联邦学习, 概念漂移, 信息论, KL散度, 互信息", "comments": "这篇论文的创新点在于将信息论工具（KL散度、互信息）引入到联邦学习的概念漂移分析中，并基于此设计了新的正则化算法。通过理论分析和在真实设备上的实验验证，增强了研究的可靠性。其对不同漂移模式的探讨也增加了研究的深度。"}}
{"id": "2506.20814", "title": "Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning", "authors": ["Jakub Piwko", "Jędrzej Ruciński", "Dawid Płudowski", "Antoni Zajko", "Patryzja Żak", "Mateusz Zacharecki", "Anna Kozak", "Katarzyna Woźnica"], "summary": "Ensemble learning has proven effective in boosting predictive performance,\nbut traditional methods such as bagging, boosting, and dynamic ensemble\nselection (DES) suffer from high computational cost and limited adaptability to\nheterogeneous data distributions. To address these limitations, we propose\nHellsemble, a novel and interpretable ensemble framework for binary\nclassification that leverages dataset complexity during both training and\ninference. Hellsemble incrementally partitions the dataset into circles of\ndifficulty by iteratively passing misclassified instances from simpler models\nto subsequent ones, forming a committee of specialised base learners. Each\nmodel is trained on increasingly challenging subsets, while a separate router\nmodel learns to assign new instances to the most suitable base model based on\ninferred difficulty. Hellsemble achieves strong classification accuracy while\nmaintaining computational efficiency and interpretability. Experimental results\non OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often\noutperforms classical ensemble methods. Our findings suggest that embracing\ninstance-level difficulty offers a promising direction for constructing\nefficient and robust ensemble systems.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20814v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20814v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "划分、专业化和路由：一种高效集成学习的新方法", "tldr": "Hellsemble是一种新的集成学习框架，通过根据数据难度划分并路由实例，实现了高效且准确的二元分类。", "motivation": "传统集成学习方法（如bagging、boosting和动态集成选择）存在计算成本高和对异构数据分布适应性有限的问题。", "method": "本文提出了Hellsemble，一个新颖且可解释的二元分类集成框架。它通过迭代地将简单模型错误分类的实例传递给后续模型，将数据集增量划分为难度圈，形成一个专业化的基础学习器委员会。每个模型都在难度递增的子集上进行训练，同时一个独立的路由模型学习根据推断的难度将新实例分配给最合适的基础模型。", "result": "Hellsemble在保持计算效率和可解释性的同时，实现了强大的分类精度。在OpenML-CC18和Tabzilla基准测试上的实验结果表明，Hellsemble通常优于经典的集成方法。", "conclusion": "拥抱实例级难度为构建高效且鲁棒的集成系统提供了有前景的方向。", "translation": "集成学习已被证明能有效提升预测性能，但传统的袋装法、提升法和动态集成选择（DES）等方法存在计算成本高和对异构数据分布适应性有限的问题。为了解决这些局限性，我们提出了Hellsemble，一种新颖且可解释的二元分类集成框架，该框架在训练和推理过程中都利用了数据集的复杂性。Hellsemble通过迭代地将简单模型错误分类的实例传递给后续模型，将数据集逐步划分为难度圈，从而形成一个专业化的基础学习器委员会。每个模型都在难度递增的子集上进行训练，同时一个独立的路由模型学习根据推断的难度将新实例分配给最合适的基础模型。Hellsemble在保持计算效率和可解释性的同时，实现了强大的分类精度。在OpenML-CC18和Tabzilla基准测试上的实验结果表明，Hellsemble通常优于经典的集成方法。我们的研究结果表明，采用实例级难度为构建高效且鲁棒的集成系统提供了有前景的方向。", "summary": "本文提出了Hellsemble，一种新颖的二元分类集成学习框架，旨在解决传统集成方法计算成本高和适应性差的问题。Hellsemble通过将数据集按难度增量划分并训练专业化的基础学习器，以及一个路由模型来分配新实例，实现了高效、准确和可解释的分类。实验结果表明，Hellsemble在多个基准测试上优于现有经典集成方法。", "keywords": "集成学习, 二元分类, 实例难度, 模型路由, Hellsemble", "comments": "Hellsemble的创新之处在于其“划分、专业化和路由”的策略，特别是根据实例难度进行增量划分和专门化训练，以及引入路由模型来提高效率和可解释性。这为构建更适应复杂数据的高效集成系统提供了新思路。"}}
{"id": "2506.21412", "title": "Plasmonically Enhanced Flexural-Mode AlScN Nanoplate Resonator as Uncooled and Ultrafast IR Detector with High Responsivity", "authors": ["Aurelio Venditti", "Walter Gubinelli", "Enise F. Altin", "Luca Colombo", "Pietro Simeoni", "Benyamin Davaji", "Matteo Rinaldi"], "summary": "This letter introduces a novel class of miniaturized, uncooled, and\nultra-fast infrared (IR) resonant thermal detectors (RTDs) based on 30%-doped\nAluminum Scandium Nitride (AlScN) nanoplates. Exploiting high electromechanical\ncoupling, good thermal properties, and enhanced and selective IR absorption,\nthe presented device aims to demonstrate significant advancements over the\nstate-of-the-art IR RTDs. This single pixel combines compact footprint, high\nspectral selectivity and responsivity, reduced noise, and fast thermal\nresponse, allowing for the potential development of innovative IR thermal\nimagers through multi-pixel integration. The flexural nature of the actuated\nresonance mode eventually enables an interferometric optical readout, paving\nthe way towards achieving extremely low Noise Equivalent Power levels. These\nresults demonstrate a high IR responsivity of around 130 ppt/pW, a thermal time\nconstant of around 330 us, and a large out-of-plane displacement. This work\nrepresents the first experimental integration on a resonating platform of\nplasmonic absorbers that utilize AlScN as dielectric layer.", "comment": "This manuscript has been submitted to ACS Nano Letters for\n  consideration", "pdf_url": "http://arxiv.org/pdf/2506.21412v1", "categories": ["physics.ins-det", "cs.SY", "eess.SY", "physics.app-ph"], "cate": "physics.ins-det", "url": "http://arxiv.org/abs/2506.21412v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "谐振增强型弯曲模式AlScN纳米板谐振器作为高响应度的非制冷超快红外探测器", "tldr": "本文介绍了一种基于AlScN纳米板的非制冷、超快、高响应度、小型化红外谐振热探测器，结合了等离子体吸收器，并展示了优异的性能。", "motivation": "现有红外谐振热探测器（RTDs）在尺寸、速度和响应度方面存在局限性，需要开发更先进的、具有更高性能（如更小尺寸、更高响应度、更低噪声、更快响应速度）的红外探测器。", "method": "本文引入了一种新型的基于30%掺杂AlScN纳米板的微型化、非制冷、超快红外谐振热探测器（RTDs）。该器件利用AlScN的高机电耦合、良好的热性能以及增强和选择性红外吸收特性。通过驱动弯曲共振模式，实现了干涉光学读出，并首次在谐振平台上实验性集成了利用AlScN作为介电层的等离子体吸收器。", "result": "所提出的器件实现了约130 ppt/pW的高红外响应度，约330 us的热时间常数，以及大的面外位移。该单像素器件结合了紧凑的尺寸、高光谱选择性和响应度、降低的噪声以及快速热响应。", "conclusion": "所提出的基于AlScN纳米板的谐振红外探测器通过结合等离子体吸收器，在小型化、非制冷、超快红外探测方面取得了显著进展，展示了高性能，并为未来创新型红外热成像仪的开发奠定了基础。", "translation": "这封信介绍了一种基于30%掺杂氮化铝钪（AlScN）纳米板的新型微型化、非制冷、超快红外（IR）谐振热探测器（RTD）。该器件利用高机电耦合、良好的热性能以及增强的选择性红外吸收，旨在展示比现有红外RTD显著的进步。这种单像素器件结合了紧凑的尺寸、高光谱选择性和响应度、降低的噪声以及快速热响应，从而为通过多像素集成开发创新型红外热成像仪提供了潜力。驱动共振模式的弯曲性质最终实现了干涉光学读出，为实现极低噪声等效功率水平铺平了道路。这些结果展示了约130 ppt/pW的高红外响应度、约330 us的热时间常数以及大的面外位移。这项工作代表了首次在谐振平台上实验性集成了利用AlScN作为介电层的等离子体吸收器。", "summary": "本文提出并实验验证了一种基于30%掺杂AlScN纳米板的等离子体增强型弯曲模式谐振器，作为一种新型的非制冷、超快、高响应度红外探测器。该探测器利用AlScN优异的机电耦合和热性能，并结合了等离子体吸收器，实现了紧凑尺寸、高光谱选择性、低噪声和快速热响应。实验结果表明其红外响应度约为130 ppt/pW，热时间常数约为330 us，为未来高性能红外热成像仪的发展提供了新途径。", "keywords": "AlScN纳米板, 红外探测器, 谐振热探测器, 等离子体增强, 非制冷", "comments": "这项工作通过首次在谐振平台上集成等离子体吸收器和利用AlScN作为介电层，展示了显著的创新性。该方法有望解决传统红外探测器在尺寸、速度和响应度方面的局限性，为非制冷红外成像技术带来了突破。其高响应度和快速响应特性使其在许多应用中具有重要意义，尤其是在需要实时、高灵敏度探测的场景。"}}
{"id": "2504.15217", "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models", "authors": ["Yatong Bai", "Jonah Casebeer", "Somayeh Sojoudi", "Nicholas J. Bryan"], "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15217v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2504.15217v1", "date": "2025-04-21", "updated": "2025-04-21", "AI": {"title_translation": "DRAGON：分布奖励优化扩散生成模型", "tldr": "DRAGON是一个灵活的框架，通过优化分布奖励来微调生成模型，在音频生成任务中表现出色，无需人类偏好训练即可提高感知质量。", "motivation": "传统的强化学习与人类反馈（RLHF）或配对偏好方法（如DPO）相比，DRAGON旨在提供一个更灵活的框架，用于将媒体生成模型微调到期望的结果，并能优化评估单个示例或其分布的奖励函数。", "method": "DRAGON是一个用于微调媒体生成模型的通用框架。它能够优化评估单个示例或其分布的奖励函数，支持实例级、实例到分布以及分布到分布的奖励。该方法通过选择编码器和参考示例集来构建新颖的奖励函数，创建示例分布，即使跨模态编码器（如CLAP）也能使用不同模态的参考示例。DRAGON收集在线和在策略生成，对其评分以构建正负演示集，并利用两组之间的对比来最大化奖励。", "result": "在对一个音频领域的文本到音乐扩散模型进行微调的评估中，DRAGON在20种不同目标奖励（包括自定义音乐美学模型、CLAP分数、Vendi多样性和Frechet音频距离）上实现了81.45%的平均胜率。基于示例集的奖励函数能有效增强生成质量，并与基于模型的奖励相当。使用适当的示例集，DRAGON在未经人类偏好标注训练的情况下，获得了60.95%的人类投票音乐质量胜率。", "conclusion": "DRAGON展示了一种设计和优化奖励函数的新方法，以提高人类感知的生成质量。", "translation": "我们提出了用于生成优化的分布奖励（DRAGON），这是一个用于将媒体生成模型微调到期望结果的通用框架。与传统的人类反馈强化学习（RLHF）或配对偏好方法（如直接偏好优化DPO）相比，DRAGON更加灵活。它可以优化评估单个示例或其分布的奖励函数，使其与广泛的实例级、实例到分布和分布到分布奖励兼容。利用这种多功能性，我们通过选择编码器和一组参考示例来创建示例分布，从而构建新颖的奖励函数。当使用跨模态编码器（如CLAP）时，参考示例可以是不同模态的（例如，文本与音频）。然后，DRAGON收集在线和在策略生成，对其评分以构建正向演示集和负向演示集，并利用两组之间的对比来最大化奖励。为了进行评估，我们使用20种不同的奖励函数微调了一个音频领域的文本到音乐扩散模型，包括自定义音乐美学模型、CLAP分数、Vendi多样性和Frechet音频距离（FAD）。我们还比较了实例级（每首歌曲）和全数据集FAD设置，同时消融了多个FAD编码器和参考集。在所有20个目标奖励中，DRAGON实现了81.45%的平均胜率。此外，基于示例集的奖励函数确实增强了生成质量，并与基于模型的奖励相当。通过适当的示例集，DRAGON在未经人类偏好标注训练的情况下，实现了60.95%的人类投票音乐质量胜率。因此，DRAGON展示了一种设计和优化奖励函数的新方法，以提高人类感知的质量。声音示例可在https://ml-dragon.github.io/web/查看。", "summary": "DRAGON是一个灵活的框架，用于微调媒体生成模型，通过优化可评估单个或分布示例的奖励函数，超越了传统的RLHF和DPO方法。它通过构建基于示例集的奖励函数，利用正负演示集的对比来最大化奖励。在对文本到音乐扩散模型的评估中，DRAGON在多种奖励函数下表现出色，实现了高胜率，并能在不依赖人类偏好标注的情况下显著提升人类感知的生成质量。", "keywords": "分布奖励, 生成模型, 扩散模型, 奖励函数, 微调", "comments": "DRAGON的创新之处在于其对奖励函数的广阔兼容性（实例级、实例到分布、实例到实例、分布到分布），以及通过构建示例分布来设计奖励函数的能力，这使其比传统方法更灵活。其重要性在于，它提供了一种在不直接依赖大量人类偏好标注的情况下，显著提高生成内容（特别是音乐）人类感知质量的新途径，降低了标注成本并提升了效率。"}}
{"id": "2506.21384", "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.", "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "pdf_url": "http://arxiv.org/pdf/2506.21384v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21384v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用LLM辅助查询理解实现实时检索增强生成", "tldr": "Omni-RAG是一个新颖的框架，通过LLM辅助查询理解来处理嘈杂、模糊和多意图的用户查询，从而提高实时RAG系统的鲁棒性和有效性。", "motivation": "实时检索增强生成（RAG）系统在处理嘈杂、模糊且包含多个意图的用户查询时面临巨大挑战，因为当前系统通常在更干净的数据上训练或评估，难以处理此类复杂输入。", "method": "本文提出了Omni-RAG框架，通过LLM辅助查询理解来预处理用户输入。它包含三个关键模块：1）深度查询理解和分解：利用LLM纠正拼写错误并分解多意图查询为结构化子查询；2）意图感知知识检索：对每个子查询从语料库（如FineWeb使用OpenSearch）执行检索并聚合结果；3）重排和生成：使用重排器（如BGE）优化文档选择，然后由LLM（如Falcon-10B）使用思维链提示生成最终响应。", "result": "Not mentioned in abstract", "conclusion": "Omni-RAG旨在通过鲁棒地处理复杂和嘈杂的查询，弥合当前RAG能力与实时应用（例如SIGIR 2025 LiveRAG挑战赛中强调的应用）需求之间的差距。", "translation": "实时检索增强生成（RAG）系统在处理通常嘈杂、模糊且包含多个意图的用户查询时面临巨大挑战。虽然RAG通过外部知识增强了大型语言模型（LLM），但当前系统通常难以处理此类复杂输入，因为它们通常在更干净的数据上训练或评估。本文介绍了一种名为Omni-RAG的新颖框架，旨在提高RAG系统在实时、开放域设置中的鲁棒性和有效性。Omni-RAG采用LLM辅助查询理解，通过三个关键模块预处理用户输入：(1) 深度查询理解和分解，该模块利用LLM和定制提示来消除查询噪声（例如，纠正拼写错误）并将多意图查询分解为结构化子查询；(2) 意图感知知识检索，该模块对每个子查询从语料库（即使用OpenSearch的FineWeb）执行检索并聚合结果；(3) 重排和生成，其中重排器（即BGE）在LLM（即Falcon-10B）使用思维链提示生成最终响应之前优化文档选择。Omni-RAG旨在通过鲁棒地处理复杂和嘈杂的查询，弥合当前RAG能力与实时应用（例如SIGIR 2025 LiveRAG挑战赛中强调的应用）需求之间的差距。", "summary": "本文提出了Omni-RAG框架，旨在解决实时检索增强生成（RAG）系统处理嘈杂、模糊和多意图用户查询的挑战。Omni-RAG通过LLM辅助查询理解来预处理输入，其核心包括深度查询理解与分解（去噪和分解查询）、意图感知知识检索（从语料库检索并聚合结果）以及重排与生成（优化文档选择并生成响应）。该框架旨在提升RAG系统在真实世界开放域环境中的鲁棒性和有效性。", "keywords": "RAG, LLM, 查询理解, 实时系统, 检索增强生成", "comments": "该论文的创新点在于提出了一个端到端的Omni-RAG框架，通过LLM辅助的查询理解来显著提升RAG系统处理复杂和嘈杂实时查询的能力。其模块化的设计，特别是查询分解和意图感知检索，对于提高RAG在实际应用中的性能至关重要。论文解决了RAG系统在实际部署中面临的关键挑战，即输入数据的“脏”性，这对于推动RAG的广泛应用具有重要意义。"}}
{"id": "2506.21326", "title": "A discontinuous in time Streamline Diffusion Virtual Element Method for Darcy-transport problem", "authors": ["R A Caraballo Diaz", "F Dassi"], "summary": "We present a first numerical study of transport phenomena involving\nchemically reactive species, modeled by advection-diffusion-reaction systems\nwith flow fields governed by Darcy's law. Among the various discretisation\napproaches, we consider the Streamline Diffusion method. Both the velocity\nfield and the species concentrations are computed using the Virtual Element\nMethod using a Discontinuous Galerkin scheme for time. An abstract error\nestimate has been derived using a special technique that utilizes Gauss-Radau\ninterpolation in conjunction with numerical integration. These theoretical\nfindings are supported by numerical experiments with arbitrary-order accuracy\nin both space and time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21326v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21326v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种用于达西输运问题的非连续时间流线扩散虚单元法", "tldr": "本文首次提出了用于达西输运问题的非连续时间流线扩散虚单元法，并通过理论分析和数值实验验证了其有效性和精度。", "motivation": "研究涉及化学反应物种的输运现象，这些现象由对流-扩散-反应系统建模，流场受达西定律控制。", "method": "采用流线扩散法进行离散化；速度场和物种浓度使用虚单元法计算，时间上采用间断伽辽金格式；利用结合高斯-拉道插值和数值积分的特殊技术推导了抽象误差估计。", "result": "推导出了一个抽象误差估计；理论发现得到了数值实验的支持；在空间和时间上都实现了任意阶精度。", "conclusion": "所提出的非连续时间流线扩散虚单元法对于涉及反应物种的达西输运问题是有效且准确的，并得到了理论分析和数值证据的支持。", "translation": "我们首次对涉及化学反应物种的输运现象进行了数值研究，这些现象由对流-扩散-反应系统建模，流场受达西定律控制。在各种离散化方法中，我们考虑了流线扩散法。速度场和物种浓度都使用虚单元法计算，时间上采用间断伽辽金格式。利用结合高斯-拉道插值和数值积分的特殊技术，推导出了一个抽象误差估计。这些理论发现得到了数值实验的支持，这些实验在空间和时间上都具有任意阶精度。", "summary": "本文提出了一种新颖的数值方法——非连续时间流线扩散虚单元法，用于模拟由对流-扩散-反应系统和达西定律控制的化学反应物种的输运现象。详细阐述了空间离散化使用虚单元法，时间离散化使用间断伽辽金格式。研究还包括利用独特的插值和积分技术推导抽象误差估计，并通过数值实验验证了理论结果，证明了该方法在空间和时间上均能达到任意阶精度。", "keywords": "流线扩散, 虚单元法, 间断伽辽金, 达西输运, 对流-扩散-反应", "comments": "本文创新性地将流线扩散法与虚单元法和间断伽辽金格式相结合，用于解决复杂的达西输运问题。抽象误差估计的推导以及任意阶精度的展示是重要的贡献，突显了所提出的数值方案在模拟反应性输运现象方面的鲁棒性和精确性。"}}
{"id": "2506.21053", "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21053v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21053v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MT2-CSD：一种新的对话立场检测数据集和多语义知识融合方法", "tldr": "本文提出了MT2-CSD数据集和LLM-CRAN模型，用于多目标、多轮对话立场检测，实验证明LLM-CRAN显著优于基线模型。", "motivation": "传统的立场检测研究多针对个体实例，缺乏捕获真实社交媒体中多方讨论动态的数据集，这阻碍了对话立场检测的进展。", "method": "本文引入了MT2-CSD，一个迄今为止最大的多目标、多轮对话立场检测综合数据集，包含24,457个标注实例。为应对挑战，提出了LLM增强型对话关系注意力网络（LLM-CRAN），利用大型语言模型的推理能力来提高对话理解。", "result": "实验结果表明，LLM-CRAN在MT2-CSD数据集上的对话立场检测任务中显著优于强大的基线模型。", "conclusion": "LLM-CRAN在对话立场检测任务中表现出色，显著优于基线模型。", "translation": "在当代社交媒体领域，自动立场检测对于意见挖掘至关重要，因为它综合并审查用户对有争议话题的观点，以揭示普遍趋势和情绪。传统的立场检测研究通常针对单个实例，从而限制了其建模真实社交媒体场景中典型的多方讨论的能力。这一缺点主要源于缺乏真实捕捉社交媒体互动动态的数据集，阻碍了对话立场检测的进展。在本文中，我们介绍了MT2-CSD，一个用于多目标、多轮对话立场检测的综合数据集。据我们所知，MT2-CSD是目前可用于此目的的最大数据集，包含24,457个标注实例，并展现出最大的对话深度，从而为立场检测带来了新的挑战。为了应对这些挑战，我们提出了LLM增强型对话关系注意力网络（LLM-CRAN），它利用LLM的推理能力来提高对话理解。我们进行了广泛的实验来评估LLM-CRAN在MT2-CSD数据集上的有效性。实验结果表明，LLM-CRAN在对话立场检测任务中显著优于强大的基线模型。", "summary": "本文针对对话立场检测领域缺乏大规模多方讨论数据集的问题，推出了MT2-CSD，一个目前最大的多目标、多轮对话立场检测数据集。为有效应对该数据集带来的新挑战，作者提出了一种名为LLM-CRAN（大型语言模型增强型对话关系注意力网络）的新方法，该方法利用大型语言模型的推理能力来提升对话理解。通过在MT2-CSD数据集上进行广泛实验，结果表明LLM-CRAN在对话立场检测任务中显著优于现有强大的基线模型。", "keywords": "对话立场检测, MT2-CSD, LLM-CRAN, 大型语言模型, 数据集", "comments": "本文通过创建迄今为止最大的对话立场检测数据集MT2-CSD，有效地解决了该领域长期存在的数据稀缺问题，为后续研究奠定了坚实的基础。同时，所提出的LLM-CRAN模型创新性地将大型语言模型的推理能力应用于对话理解，并在实验中展现出卓越的性能，为对话立场检测提供了新的、高效的解决方案。"}}
{"id": "2506.21046", "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features", "authors": ["Shangbo Wu", "Yu-an Tan", "Ruinan Ma", "Wencong Ma", "Dehua Zhu", "Yuanzhang Li"], "summary": "The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA.", "comment": "14 pages, 9 figures, to appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21046v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21046v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用自监督视觉Transformer特征提升生成式对抗迁移性", "tldr": "本文提出dSVA，一种利用自监督ViT（结合对比学习和掩蔽图像建模）提取的全局和局部特征来生成对抗样本，显著提升了黑盒对抗迁移性，超越了现有技术。", "motivation": "传统方法依赖于监督学习特征来生成对抗样本以提高迁移性，但自监督学习和Transformer架构的协同作用启发了研究者探索利用自监督Vision Transformer (ViT) 表示是否能进一步提升对抗迁移性。", "method": "提出dSVA，一种生成式双自监督ViT特征攻击。该方法利用自监督ViT的两种范式：从对比学习中获取全局结构特征，以及从掩蔽图像建模中获取局部纹理特征。设计了一个新的生成训练框架，包含一个生成器用于创建黑盒对抗样本，并通过利用自监督ViT的联合特征和注意力机制来训练生成器。", "result": "研究发现，对比学习和掩蔽图像建模使ViT能够关注不同的特征倾向。当协同利用这些特征时，可以获得出色的对抗泛化能力。通过扰动自监督ViT提取的双重深度特征，实现了对各种架构模型的显著黑盒迁移性，性能优于现有最先进的技术。", "conclusion": "利用自监督Vision Transformer的全局和局部特征，通过提出的dSVA方法，可以有效生成高迁移性的黑盒对抗样本，显著提升了对抗攻击的泛化能力。", "translation": "深度神经网络（DNNs）的能力源于从提供的数据中提取和解释特征。通过利用DNN中的中间特征而不是依赖硬标签，我们能够制作出更有效泛化的对抗性扰动，从而提升黑盒迁移性。在之前的工作中，这些特征普遍来自监督学习。受自监督学习和Transformer架构之间卓越协同作用的启发，本文探讨了利用自监督视觉Transformer（ViT）表示是否能改善对抗迁移性。我们提出了dSVA——一种生成式双自监督ViT特征攻击，它利用了对比学习（CL）的全局结构特征和掩蔽图像建模（MIM）的局部纹理特征，这两种是ViT的自监督学习范式组合。我们设计了一种新颖的生成训练框架，其中包含一个生成器来创建黑盒对抗样本，并采用策略通过利用自监督ViT的联合特征和注意力机制来训练生成器。我们的研究结果表明，CL和MIM使ViT能够关注不同的特征倾向，当协同利用时，这两种倾向展现出强大的对抗泛化能力。通过扰动自监督ViT提取的双重深度特征，我们获得了对各种架构模型的显著黑盒迁移性，其性能优于现有最先进的技术。代码可在https://github.com/spencerwooo/dSVA 获取。", "summary": "本文提出dSVA，一种新型的生成式对抗攻击方法，旨在提升黑盒对抗样本的迁移性。dSVA通过利用自监督Vision Transformer (ViT) 提取的全局（来自对比学习）和局部（来自掩蔽图像建模）双重特征来训练一个生成器，以创建具有高度泛化能力的对抗扰动。实验证明，这种结合自监督ViT特征的方法显著优于现有技术，实现了对多种模型架构的卓越黑盒迁移性。", "keywords": "对抗迁移性, 自监督学习, Vision Transformer, 黑盒攻击, 生成式对抗网络", "comments": "这篇论文的创新点在于首次将自监督Vision Transformer的特征，特别是结合了对比学习和掩蔽图像建模的双重特征，应用于提升生成式对抗样本的黑盒迁移性。利用ViT的中间特征而非硬标签，以及挖掘自监督学习的潜力，是其核心贡献。这项工作对于提高对抗攻击的有效性和理解模型鲁棒性具有重要意义。"}}
{"id": "2506.21250", "title": "ACTLLM: Action Consistency Tuned Large Language Model", "authors": ["Jing Bi", "Lianggong Bruce Wen", "Zhang Liu", "Chenliang Xu"], "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model),\na novel approach for robot manipulation in dynamic environments. Traditional\nvision-based systems often struggle to learn visual representations that excel\nin both task execution and spatial reasoning, thereby limiting their\nadaptability in dynamic environments. ACTLLM addresses these challenges by\nharnessing language to craft structured scene descriptors, providing a uniform\ninterface for both spatial understanding and task performance through flexible\nlanguage instructions. Moreover, we introduce a novel action consistency\nconstraint that aligns visual perception with corresponding actions, thereby\nenhancing the learning of actionable visual representations. Additionally, we\nhave reformulated the Markov decision process for manipulation tasks into a\nmulti-turn visual dialogue framework. This approach enables the modeling of\nlong-term task execution with enhanced contextual relevance derived from the\nhistory of task execution. During our evaluation, ACTLLM excels in diverse\nscenarios, proving its effectiveness on challenging vision-based robot\nmanipulation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21250v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21250v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ACTLLM：动作一致性调整大型语言模型", "tldr": "ACTLLM利用语言和动作一致性来改进动态环境中机器人的操作，优于传统基于视觉的系统。", "motivation": "传统的基于视觉的系统在任务执行和空间推理方面难以学习优秀的视觉表示，从而限制了它们在动态环境中的适应性。", "method": "ACTLLM利用语言来构建结构化场景描述，通过灵活的语言指令为空间理解和任务执行提供统一接口。引入了一种新的动作一致性约束，使视觉感知与相应动作对齐，从而增强可操作视觉表示的学习。将操纵任务的马尔可夫决策过程重新构建为多轮视觉对话框架，以建模长期任务执行并增强上下文相关性。", "result": "在评估中，ACTLLM在不同场景中表现出色，证明了其在具有挑战性的基于视觉的机器人操作任务上的有效性。", "conclusion": "ACTLLM通过整合语言和动作一致性，是动态环境中机器人操作的有效方法。", "translation": "本文介绍了ACTLLM（动作一致性调整大型语言模型），这是一种在动态环境中进行机器人操作的新颖方法。传统的基于视觉的系统通常难以学习在任务执行和空间推理方面都表现出色的视觉表示，从而限制了它们在动态环境中的适应性。ACTLLM通过利用语言来构建结构化场景描述，为空间理解和任务性能通过灵活的语言指令提供统一接口，从而解决了这些挑战。此外，我们引入了一种新颖的动作一致性约束，将视觉感知与相应的动作对齐，从而增强了可操作视觉表示的学习。此外，我们还将操纵任务的马尔可夫决策过程重新构建为多轮视觉对话框架。这种方法能够通过任务执行历史中获取的增强上下文相关性来建模长期任务执行。在我们的评估中，ACTLLM在各种场景中表现出色，证明了其在具有挑战性的基于视觉的机器人操纵任务上的有效性。", "summary": "本文提出了ACTLLM（动作一致性调整大型语言模型），一种用于动态环境中机器人操作的新方法。它利用语言创建结构化场景描述，并引入动作一致性约束来对齐视觉感知与相应动作。通过将马尔可夫决策过程重构为多轮视觉对话框架，ACTLLM增强了长期任务执行和上下文相关性，并在具有挑战性的视觉机器人操作任务中表现出色。", "keywords": "ACTLLM, 机器人操作, 大型语言模型, 动作一致性, 动态环境", "comments": "ACTLLM的创新之处在于整合语言进行场景理解和动作一致性约束来学习可操作的视觉表示，解决了传统视觉系统在动态环境中适应性不足的问题。其重要性在于提升了机器人操作在复杂动态环境中的鲁棒性和泛化能力。"}}
{"id": "2506.21338", "title": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification", "authors": ["Galvin Brice S. Lim", "Brian Godwin S. Lim", "Argel A. Bandala", "John Anthony C. Jose", "Timothy Scott C. Chu", "Edwin Sybingco"], "summary": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.21338v1", "categories": ["cs.LG", "cs.HC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21338v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "AGTCNet：一种用于原则性运动想象脑电图分类的图时域方法", "tldr": "AGTCNet是一种新颖的图时域模型，用于运动想象EEG分类，通过利用EEG电极的地形配置和图卷积注意力网络，实现了最先进的性能，同时具有更小的模型尺寸和更快的推理时间。", "motivation": "脑机接口(BCI)技术在赋能运动障碍个体方面具有巨大潜力，但开发与受试者和会话无关的BCI系统仍是一个重大挑战，因为神经活动的复杂性和变异性以及EEG硬件限制导致现有方法无法有效捕获多通道EEG信号中复杂的时空依赖性。", "method": "本研究引入了注意力图时域卷积网络（AGTCNet），这是一种新颖的图时域模型，用于运动想象EEG（MI-EEG）分类。AGTCNet利用EEG电极的地形配置作为归纳偏置，并集成了图卷积注意力网络（GCAT）以共同学习富有表现力的时空EEG表示。", "result": "AGTCNet显著优于现有MI-EEG分类器，实现了最先进的性能，同时具有紧凑的架构。模型尺寸减小了49.87%，推理时间加快了64.65%，并缩短了输入EEG信号。在BCI Competition IV Dataset 2a上，受试者独立分类的移动平均准确率为66.82%，经过受试者特异性微调后提高到82.88%。在EEG Motor Movement/Imagery Dataset上，AGTCNet在4分类和2分类受试者独立分类中分别达到了64.14%和85.22%的移动平均准确率，受试者特异性分类进一步提高到72.13%和90.54%。", "conclusion": "AGTCNet在运动想象EEG分类中表现出卓越的有效性和实用性，其紧凑的架构和优越的性能使其成为BCI部署的理想选择。", "translation": "脑机接口（BCI）技术利用脑电图（EEG）标志着一项变革性创新，使运动障碍个体能够平等地参与其环境。尽管其潜力巨大，但由于个体之间和随时间推移的神经活动固有的复杂性和变异性，以及EEG硬件的限制，开发与受试者无关和与会话无关的BCI系统仍然是一个重大挑战。尽管先前的研究试图开发鲁棒的BCI系统，但现有方法在捕获多通道EEG信号中复杂的时空依赖性方面仍然无效。本研究通过引入注意力图时域卷积网络（AGTCNet）来解决这一空白，这是一种用于运动想象EEG（MI-EEG）分类的新型图时域模型。具体而言，AGTCNet利用EEG电极的地形配置作为归纳偏置，并集成了图卷积注意力网络（GCAT），以共同学习富有表现力的时空EEG表示。所提出的模型显著优于现有MI-EEG分类器，在利用紧凑架构的同时实现了最先进的性能，突出了其在BCI部署中的有效性和实用性。AGTCNet模型尺寸减小了49.87%，推理时间加快了64.65%，并且输入EEG信号更短，在BCI Competition IV Dataset 2a上，受试者独立分类的移动平均准确率为66.82%，经过受试者特异性微调后进一步提高到82.88%。在EEG Motor Movement/Imagery Dataset上，AGTCNet在4分类和2分类受试者独立分类中分别达到了64.14%和85.22%的移动平均准确率，受试者特异性分类进一步提高到72.13%和90.54%。", "summary": "本研究提出了一种名为AGTCNet的新型图时域卷积网络，用于运动想象EEG（MI-EEG）分类，旨在解决现有BCI系统在捕获EEG信号复杂时空依赖性方面的不足。AGTCNet利用EEG电极的地形配置作为归纳偏置，并结合图卷积注意力网络（GCAT）来学习有效的时空EEG表示。实验结果表明，AGTCNet在多个数据集上均优于现有MI-EEG分类器，实现了最先进的性能，同时具有更小的模型尺寸和更快的推理速度，证明了其在BCI部署中的实用性和有效性。", "keywords": "运动想象, 脑电图分类, 图时域网络, 脑机接口, 注意力机制", "comments": "该论文提出了一种创新的图时域方法（AGTCNet）来解决运动想象EEG分类中的关键挑战，即捕获复杂的时空依赖性。其核心创新在于结合了EEG电极的拓扑结构作为归纳偏置，并利用图卷积注意力网络来学习丰富的表示。该模型不仅在性能上超越了现有方法，还在模型尺寸和推理速度方面取得了显著优化，这对于实际的BCI系统部署至关重要，显示出很高的实用价值和应用潜力。"}}
{"id": "2506.20936", "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling", "authors": ["Hao Zhang", "Haolan Xu", "Chun Feng", "Varun Jampani", "Narendra Ahuja"], "summary": "Skinning and rigging are fundamental components in animation, articulated\nobject reconstruction, motion transfer, and 4D generation. Existing approaches\npredominantly rely on Linear Blend Skinning (LBS), due to its simplicity and\ndifferentiability. However, LBS introduces artifacts such as volume loss and\nunnatural deformations, and it fails to model elastic materials like soft\ntissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty\ntissues). In this work, we propose PhysRig: a differentiable physics-based\nskinning and rigging framework that overcomes these limitations by embedding\nthe rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),\nwhich is simulated as a deformable soft-body structure driven by the animated\nskeleton. Our method leverages continuum mechanics and discretizes the object\nas particles embedded in an Eulerian background grid to ensure\ndifferentiability with respect to both material properties and skeletal motion.\nAdditionally, we introduce material prototypes, significantly reducing the\nlearning space while maintaining high expressiveness. To evaluate our\nframework, we construct a comprehensive synthetic dataset using meshes from\nObjaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object\ncategories and motion patterns. Our method consistently outperforms traditional\nLBS-based approaches, generating more realistic and physically plausible\nresults. Furthermore, we demonstrate the applicability of our framework in the\npose transfer task highlighting its versatility for articulated object\nmodeling.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20936v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20936v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PhysRig：用于真实关节对象建模的可微分基于物理的蒙皮和绑定框架", "tldr": "PhysRig是一个可微分的基于物理的蒙皮和绑定框架，它通过模拟可变形的软体结构来克服传统线性混合蒙皮（LBS）的限制，从而实现更真实的关节对象建模。", "motivation": "现有蒙皮和绑定方法（如线性混合蒙皮LBS）因其简单性和可微分性而被广泛使用，但存在体积损失、不自然变形等缺陷，且无法有效模拟弹性材料（如软组织、毛发和柔性附属物）。", "method": "该研究提出了PhysRig框架，通过将刚性骨架嵌入到体积表示（例如四面体网格）中，并将其模拟为由动画骨架驱动的可变形软体结构。该方法利用连续介质力学，将对象离散化为嵌入欧拉背景网格中的粒子，以确保对材料属性和骨骼运动的可微分性。此外，引入了材料原型以显著减少学习空间同时保持高表达能力。", "result": "研究构建了一个包含Objaverse、The Amazing Animals Zoo和MixaMo网格的综合合成数据集进行评估。PhysRig方法始终优于传统的基于LBS的方法，生成更真实、更符合物理规律的结果。此外，该框架在姿态迁移任务中也展现了其适用性，突显了其在关节对象建模方面的多功能性。", "conclusion": "PhysRig是一个创新的可微分基于物理的蒙皮和绑定框架，有效解决了传统LBS方法的局限性，通过模拟软体变形生成更真实、更符合物理规律的动画效果，并展现了在关节对象建模任务中的广泛适用性。", "translation": "蒙皮和绑定是动画、关节对象重建、运动迁移和4D生成中的基本组成部分。现有方法由于其简单性和可微分性，主要依赖于线性混合蒙皮（LBS）。然而，LBS会引入诸如体积损失和不自然变形之类的伪影，并且无法模拟弹性材料，例如软组织、毛皮和柔性附属物（例如，大象鼻子、耳朵和脂肪组织）。在这项工作中，我们提出了PhysRig：一个可微分的基于物理的蒙皮和绑定框架，通过将刚性骨架嵌入到体积表示（例如四面体网格）中来克服这些限制，该体积表示被模拟为由动画骨架驱动的可变形软体结构。我们的方法利用连续介质力学，并将对象离散化为嵌入在欧拉背景网格中的粒子，以确保对材料属性和骨骼运动都具有可微分性。此外，我们引入了材料原型，显著减少了学习空间，同时保持了高表达能力。为了评估我们的框架，我们使用来自Objaverse、The Amazing Animals Zoo和MixaMo的网格构建了一个综合合成数据集，涵盖了不同的对象类别和运动模式。我们的方法始终优于传统的基于LBS的方法，生成更真实、更符合物理规律的结果。此外，我们展示了我们的框架在姿态迁移任务中的适用性，突显了其在关节对象建模方面的多功能性。", "summary": "本文提出了PhysRig，一个可微分的基于物理的蒙皮和绑定框架，旨在解决传统线性混合蒙皮（LBS）在处理体积损失、不自然变形和弹性材料方面的局限性。PhysRig通过将骨架嵌入到可变形的软体体积表示中，并利用连续介质力学和欧拉背景网格确保可微分性。该框架还引入了材料原型以优化学习空间。实验证明，PhysRig在生成更真实、物理上更合理的结果方面优于LBS，并在姿态迁移等任务中表现出强大的通用性。", "keywords": "可微分物理, 蒙皮, 绑定, 关节对象建模, 软体模拟", "comments": "PhysRig的创新之处在于其将可微分的物理模拟引入蒙皮和绑定过程，从而克服了传统LBS在处理弹性材料和生成物理真实变形方面的固有缺陷。通过结合连续介质力学和欧拉网格，该方法实现了对材料属性和骨骼运动的端到端可微分性，为更真实的动画和建模开辟了新的可能性。引入材料原型也有效地平衡了表达能力和学习效率。这项工作对于需要高保真软体变形的领域具有重要意义。"}}
{"id": "2506.21453", "title": "Towards an Optimal Control Perspective of ResNet Training", "authors": ["Jens Püttschneider", "Simon Heilig", "Asja Fischer", "Timm Faulwasser"], "summary": "We propose a training formulation for ResNets reflecting an optimal control\nproblem that is applicable for standard architectures and general loss\nfunctions. We suggest bridging both worlds via penalizing intermediate outputs\nof hidden states corresponding to stage cost terms in optimal control. For\nstandard ResNets, we obtain intermediate outputs by propagating the state\nthrough the subsequent skip connections and the output layer. We demonstrate\nthat our training dynamic biases the weights of the unnecessary deeper residual\nlayers to vanish. This indicates the potential for a theory-grounded layer\npruning strategy.", "comment": "Accepted for presentation at the High-dimensional Learning Dynamics\n  (HiLD) workshop at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21453v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21453v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ResNet训练的最优控制视角", "tldr": "提出一种基于最优控制的ResNet训练公式，可使不必要的深层权重消失，为层剪枝提供理论基础。", "motivation": "旨在为ResNet训练提供一个最优控制问题的新视角，并探索一种理论驱动的层剪枝策略。", "method": "提出一种将ResNet训练表述为最优控制问题的方法，通过惩罚对应于最优控制中阶段成本项的隐藏状态的中间输出来连接两个领域。对于标准ResNet，通过后续的跳跃连接和输出层传播状态以获得中间输出。", "result": "证明了这种训练动态会使不必要的深层残差层的权重趋于消失。", "conclusion": "这种方法为理论基础的层剪枝策略提供了潜力。", "translation": "我们提出了一种针对ResNet的训练公式，它反映了一个适用于标准架构和通用损失函数的最优控制问题。我们建议通过惩罚对应于最优控制中阶段成本项的隐藏状态的中间输出来连接这两个领域。对于标准ResNet，我们通过后续的跳跃连接和输出层传播状态以获得中间输出。我们证明了我们的训练动态会使不必要的深层残差层的权重趋于消失。这表明了理论基础的层剪枝策略的潜力。", "summary": "本文提出了一种将ResNet训练视为最优控制问题的新公式，该方法通过惩罚中间输出，使得不必要的深层残差层权重在训练过程中自动趋于零。这为开发基于理论的ResNet层剪枝策略提供了新的方向和潜力。", "keywords": "ResNet训练, 最优控制, 层剪枝, 深度学习, 模型压缩", "comments": "这项工作创新性地将ResNet训练与最优控制理论相结合，为理解和优化深度网络提供了一个新的视角。其能够使不必要的层权重自动消失的发现，对于模型压缩和效率提升具有重要意义，为未来的理论驱动型网络剪枝策略奠定了基础。"}}
{"id": "2506.20243", "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "authors": ["Papa Séga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2506.20243v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.20243v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "CBF-AFA：基于分块的多自监督学习融合用于自动流利度评估", "tldr": "本文提出一种基于分块的多自监督学习（SSL）融合方法CBF-AFA，结合Wav2Vec2、HuBERT和WavLM模型及CNN-BiLSTM框架，用于提高非母语使用者自动流利度评估的准确性，并在多个数据集上取得了显著改进。", "motivation": "自动流利度评估（AFA）仍然具有挑战性，尤其是在捕获非母语使用者的语音节奏、停顿和不流利现象方面。", "method": "提出一种基于分块的方法，将自监督学习（SSL）模型（Wav2Vec2、HuBERT和WavLM）与分层CNN-BiLSTM框架相结合。使用Silero语音活动检测（Silero-VAD）将语音分割成呼吸组块，通过可学习的加权机制融合SSL嵌入，并辅以块级流利度标记（如语速、停顿持续时间、n-gram重复）。CNN-BiLSTM用于捕获跨块的局部和长期依赖。", "result": "在Avalinguo和Speechocean762数据集上进行评估，与单个SSL基线相比，在Speechocean762上F1分数提高了2.8，皮尔逊相关性提高了6.2个点；在Avalinguo上F1分数提高了4.2，皮尔逊相关性提高了4.0个点，超越了基于Pyannote.audio的分割基线。", "conclusion": "这些发现强调了基于分块的多自监督学习融合在鲁棒流利度评估中的有效性，但未来的工作应探索其在具有不规则韵律方言上的泛化能力。", "translation": "自动流利度评估（AFA）仍然具有挑战性，特别是在捕获非母语使用者的语音节奏、停顿和不流利现象方面。我们引入了一种基于分块的方法，该方法集成了自监督学习（SSL）模型（Wav2Vec2、HuBERT和WavLM），这些模型因其在语音、韵律和嘈杂语音建模方面的互补优势而被选中，并结合了一个分层CNN-BiLSTM框架。语音使用Silero语音活动检测（Silero-VAD）被分割成呼吸组块，从而实现细粒度的时间分析，同时减轻过度分割的伪影。SSL嵌入通过可学习的加权机制进行融合，平衡声学和语言特征，并富含块级流利度标记（例如，语速、停顿持续时间、n-gram重复）。CNN-BiLSTM捕获跨块的局部和长期依赖。在Avalinguo和Speechocean762数据集上进行评估，与单个SSL基线相比，我们的方法在Speechocean762上F1分数提高了2.8，皮尔逊相关性提高了6.2个点，在Avalinguo上F1分数提高了4.2，皮尔逊相关性提高了4.0个点，超越了基于Pyannote.audio的分割基线。这些发现强调了基于分块的多自监督学习融合在鲁棒流利度评估中的有效性，尽管未来的工作应探索其在具有不规则韵律方言上的泛化能力。", "summary": "本文提出CBF-AFA，一种用于自动流利度评估的基于分块的多自监督学习（SSL）融合方法。该方法将Wav2Vec2、HuBERT和WavLM等SSL模型与分层CNN-BiLSTM框架结合，通过Silero-VAD将语音分割成呼吸组块，并融合SSL嵌入和块级流利度标记。实验结果表明，CBF-AFA在多个数据集上显著提高了流利度评估的F1分数和皮尔逊相关性，优于现有基线。", "keywords": "自动流利度评估, 自监督学习, 分块, 多模型融合, 语音分析", "comments": "本文的创新点在于提出了基于分块的多自监督学习融合方法，有效结合了不同SSL模型的互补优势，并通过分块处理减轻了过度分割问题，同时融入了丰富的块级流利度特征。其重要性体现在显著提升了非母语使用者自动流利度评估的准确性。局限性在于未来需要探索其在具有不规则韵律方言上的泛化能力。"}}
{"id": "2506.21445", "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21445v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21445v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Text2Cypher 跨语言：评估超越英语的基础模型", "tldr": "本文评估了基础LLMs在多语言Text2Cypher任务上的性能，发现英语表现最佳，其次是西班牙语，土耳其语最差，并创建并发布了一个多语言测试集以支持跨语言比较。", "motivation": "当前将自然语言转换为数据库查询（如Text2Cypher）的研究主要集中在英语，缺乏对其他语言的评估。", "method": "创建并发布了一个通过将英语问题翻译成西班牙语和土耳其语而构建的多语言测试集，同时保留了原始Cypher查询以实现公平的跨语言比较。使用标准化提示和指标评估了多种基础模型。探索了将任务提示翻译成西班牙语和土耳其语对评估结果的影响。", "result": "模型性能呈现一致模式：英语表现最好，西班牙语次之，土耳其语最低，这归因于训练数据可用性和语言特征的差异。提示翻译对评估指标的影响很小或没有影响。", "conclusion": "需要对多语言查询生成进行更具包容性的评估和开发。未来的工作应包括模式本地化和跨不同语言的微调。", "translation": "大型语言模型的最新进展使得自然语言接口能够将用户问题翻译成数据库查询，例如Text2SQL、Text2SPARQL和Text2Cypher。虽然这些接口增强了数据库的可访问性，但目前大多数研究仅关注英语，对其他语言的评估有限。本文研究了基础LLM在多语言Text2Cypher任务上的性能。我们通过将英语问题翻译成西班牙语和土耳其语，同时保留原始Cypher查询，创建并发布了一个多语言测试集，从而实现了公平的跨语言比较。我们使用标准化提示和指标评估了多种基础模型。我们的结果显示出一致的性能模式：英语最高，其次是西班牙语，土耳其语最低。我们将其归因于训练数据可用性和语言特征的差异。此外，我们探讨了将任务提示翻译成西班牙语和土耳其语的影响。结果显示评估指标变化很小或没有变化，表明提示翻译影响不大。我们的发现强调了在多语言查询生成中需要更具包容性的评估和开发。未来的工作包括模式本地化和跨不同语言的微调。", "summary": "本文旨在解决现有Text2Cypher研究主要集中于英语的局限性，通过创建并发布一个包含英语、西班牙语和土耳其语的多语言测试集，评估了基础大型语言模型在跨语言Text2Cypher任务上的性能。研究结果显示，模型在英语上的表现最佳，西班牙语次之，土耳其语最差，这可能与训练数据可用性和语言特性有关。同时，研究发现将任务提示翻译成其他语言对模型性能影响微乎其微。论文强调了未来在多语言查询生成领域进行更具包容性评估和开发的必要性。", "keywords": "Text2Cypher, 多语言, 基础模型, 跨语言评估, 大型语言模型", "comments": "这项研究通过构建多语言测试集并评估基础模型在Text2Cypher任务上的表现，填补了现有研究主要集中于英语的空白，具有重要的实践意义。它揭示了当前LLMs在处理非英语数据库查询任务时存在的性能差距，并指出了未来在多语言数据、模型训练和评估方面的发展方向。其创新点在于创建了用于跨语言比较的标准化测试集，并探讨了语言特性和提示翻译对模型性能的影响。"}}
{"id": "2506.21361", "title": "Efficient parameter-robust preconditioners for linear poroelasticity and elasticity in the primal formulation", "authors": ["Weizhang Huang", "Zhuoran Wang"], "summary": "Poroelasticity problems play an important role in various engineering,\ngeophysical, and biological applications. Their full discretization results in\na large-scale saddle-point system at each time step that is becoming singular\nfor locking cases and needs effective preconditioners for its fast iterative\nsolution. Instead of constructing spectrally equivalent ones, we develop\nnonsingular preconditioners so that the eigenvalues of the preconditioned\nsystem consist of a cluster around $1$ and an outlier in the order of\n$1/\\lambda$, where $\\lambda$ is a Lam\\'{e} constant that is large for locking\ncases. It is known that the convergence factor of GMRES is bounded by the\nradius of the cluster for this type of systems. Both two- and three-field block\ntriangular Schur complement preconditioners are studied. Upper bounds of the\nradius of the eigenvalue cluster for those systems are obtained and shown to be\nrelated to the inf-sup condition but independent of mesh size, time step, and\nlocking parameters, which reflects the robustness of the preconditioners with\nrespect to parameter variations. Moreover, the developed preconditioners do not\nneed to compute the Schur complement and neither require exact inversion of\ndiagonal blocks except the leading one. A locking-free weak Galerkin finite\nelement method and the implicit Euler scheme are used for the discretization of\nthe governing equation. Both two- and three-dimensional numerical results are\npresented to confirm the effectiveness and parameter-robustness of the\ndeveloped preconditioners.", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2506.21361v1", "categories": ["math.NA", "cs.NA", "65M60, 65F08, 65F10, 74F10"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21361v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "原初公式中线性多孔弹性与弹性的高效参数鲁棒预处理器", "tldr": "该论文为大规模多孔弹性问题开发了新的鲁棒预处理器，证明它们能有效处理锁定情况和参数变化，并得到了数值结果的证实。", "motivation": "多孔弹性问题离散化后会产生大规模的鞍点系统，这些系统在锁定情况下可能变得奇异，因此需要有效的预处理器来快速迭代求解。", "method": "作者开发了非奇异预处理器，使得预处理系统的特征值围绕1形成簇，并有一个量级为1/λ的异常值。他们研究了二场和三场块三角Schur补预处理器。离散化采用了无锁定弱伽辽金有限元方法和隐式欧拉格式。", "result": "获得了特征值簇半径的上限，这些上限与inf-sup条件相关，且独立于网格尺寸、时间步长和锁定参数。所开发的预处理器不需要计算Schur补，除了首个对角块外，也不需要对所有对角块进行精确求逆。二维和三维数值结果证实了它们的有效性和参数鲁棒性。", "conclusion": "所开发的非奇异预处理器对于解决大规模线性多孔弹性与弹性问题（包括锁定情况）是有效且参数鲁棒的，数值结果证实了这一点。", "translation": "多孔弹性问题在各种工程、地球物理和生物应用中扮演着重要角色。它们完全离散化后，在每个时间步都会产生一个大规模的鞍点系统，该系统在锁定情况下会变得奇异，需要有效的预处理器来实现快速迭代求解。我们没有构建谱等效的预处理器，而是开发了非奇异预处理器，使得预处理系统的特征值由一个围绕1的簇和一个量级为1/λ的异常值组成，其中λ是在锁定情况下很大的Lamé常数。已知GMRES的收敛因子受此类系统特征值簇半径的限制。研究了二场和三场块三角Schur补预处理器。获得了这些系统特征值簇半径的上限，并表明它们与inf-sup条件相关，但独立于网格尺寸、时间步长和锁定参数，这反映了预处理器对参数变化的鲁棒性。此外，所开发的预处理器不需要计算Schur补，除了首个对角块外，也不需要对对角块进行精确求逆。使用无锁定弱伽辽金有限元方法和隐式欧拉格式对控制方程进行离散化。提供了二维和三维数值结果，以证实所开发预处理器的有效性和参数鲁棒性。", "summary": "本文旨在解决多孔弹性问题离散化产生的大规模鞍点系统在锁定情况下可能出现的奇异性问题。作者提出了新颖的非奇异预处理器，使得预处理系统的特征值围绕1聚集，从而改善GMRES收敛性。他们分析了二场和三场块三角Schur补预处理器，并证明其特征值簇边界对网格尺寸、时间步长和锁定参数的变化具有鲁棒性。关键在于，这些预处理器避免了显式Schur补计算和对大部分对角块的精确求逆。数值实验验证了所提出方法的有效性和参数鲁棒性，这些方法采用了无锁定弱伽辽金有限元方法和隐式欧拉格式进行离散化。", "keywords": "多孔弹性, 预处理器, 鞍点系统, 参数鲁棒性, Schur补", "comments": "该研究的创新之处在于开发了非奇异预处理器，这些预处理器在不需显式计算Schur补或对大多数对角块进行精确求逆的情况下实现了参数鲁棒性，从而提高了计算效率。对特征值聚类及其与关键参数无关性的理论分析是一项重要贡献，确保了其广泛适用性。这项工作对于高效解决各种科学和工程领域中复杂的多孔弹性问题至关重要。"}}
{"id": "2506.21096", "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "comment": "Accepted by ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21096v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21096v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DALR：用于多模态句子表示学习的双层对齐学习", "tldr": "提出DALR，通过一致性学习和排序蒸馏解决多模态句子表示学习中的跨模态错位偏差和模内语义差异问题，并在STS和TR任务上表现优异。", "motivation": "现有多模态句子表示学习方法主要侧重于粗粒度对齐，面临跨模态错位偏差和模内语义差异两大关键挑战，这些问题严重降低了句子表示质量。", "method": "提出DALR（用于多模态句子表示的双层对齐学习）。对于跨模态对齐，引入一致性学习模块，通过软化负样本并利用辅助任务的语义相似性实现细粒度跨模态对齐。此外，将排序蒸馏与全局模内对齐学习相结合，以更好地捕捉句子关系并提高表示质量。", "result": "在语义文本相似性（STS）和迁移（TR）任务上的综合实验验证了该方法的有效性，并持续显示出优于现有最先进基线的性能。", "conclusion": "DALR通过双层对齐学习（细粒度跨模态对齐和全局模内对齐）有效解决了多模态句子表示学习中的关键挑战，并取得了卓越的性能。", "translation": "以前的多模态句子表示学习方法取得了令人印象深刻的性能。然而，大多数方法侧重于粗粒度对齐图像和文本，面临两个关键挑战：跨模态错位偏差和模内语义差异，这严重降低了句子表示质量。为了解决这些挑战，我们提出了DALR（用于多模态句子表示的双层对齐学习）。对于跨模态对齐，我们提出了一种一致性学习模块，该模块软化负样本并利用辅助任务的语义相似性来实现细粒度跨模态对齐。此外，我们认为句子关系超越了二元正负标签，表现出更复杂的排序结构。为了更好地捕捉这些关系并提高表示质量，我们将排序蒸馏与全局模内对齐学习相结合。在语义文本相似性（STS）和迁移（TR）任务上的综合实验验证了我们方法的有效性，持续证明其优于最先进的基线。", "summary": "本文提出DALR（双层对齐学习），旨在解决现有多模态句子表示学习中存在的跨模态错位偏差和模内语义差异问题。DALR通过引入一致性学习模块实现细粒度跨模态对齐，并结合排序蒸馏与全局模内对齐学习来捕捉更复杂的句子关系。实验结果表明，DALR在语义文本相似性（STS）和迁移（TR）任务上均优于现有基线。", "keywords": "多模态句子表示学习, 双层对齐学习, 跨模态对齐, 模内对齐, 排序蒸馏", "comments": "本文创新性地提出了双层对齐学习框架DALR，通过细粒度跨模态一致性学习和全局模内排序蒸馏，有效解决了多模态句子表示学习中长期存在的对齐挑战，提升了表示质量。其方法设计思路清晰，实验结果具有说服力，对后续多模态表示学习研究具有重要的启发意义。"}}
{"id": "2506.20939", "title": "AIR-VIEW: The Aviation Image Repository for Visibility Estimation of Weather, A Dataset and Benchmark", "authors": ["Chad Mourning", "Zhewei Wang", "Justin Murray"], "summary": "Machine Learning for aviation weather is a growing area of research for\nproviding low-cost alternatives for traditional, expensive weather sensors;\nhowever, in the area of atmospheric visibility estimation, publicly available\ndatasets, tagged with visibility estimates, of distances relevant for aviation,\nof diverse locations, of sufficient size for use in supervised learning, are\nabsent. This paper introduces a new dataset which represents the culmination of\na year-long data collection campaign of images from the FAA weather camera\nnetwork suitable for this purpose. We also present a benchmark when applying\nthree commonly used approaches and a general-purpose baseline when trained and\ntested on three publicly available datasets, in addition to our own, when\ncompared against a recently ratified ASTM standard.", "comment": "5 pages, meant as citation for dataset", "pdf_url": "http://arxiv.org/pdf/2506.20939v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20939v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "AIR-VIEW：用于天气能见度估计的航空图像库，一个数据集和基准", "tldr": "该论文介绍了名为AIR-VIEW的航空图像数据集，用于天气能见度估计，并提供了一个基准测试，以解决现有公开数据集的不足。", "motivation": "现有用于航空大气能见度估计的公开数据集缺乏适用于相关距离、多样化位置且规模足够用于监督学习的数据，并且这些数据未标记能见度，而传统气象传感器成本高昂。", "method": "本文介绍了名为AIR-VIEW的新数据集，该数据集是美国联邦航空管理局（FAA）气象摄像头网络一年数据收集活动的成果。此外，论文还提出了一个基准测试，将三种常用方法和一个通用基线模型在包括AIR-VIEW在内的三个公开数据集上进行训练和测试，并与最近批准的ASTM标准进行比较。", "result": "论文推出了一个专门用于航空能见度估计的图像数据集AIR-VIEW，并提供了一个基准测试，展示了三种常用方法和一个通用基线模型在该数据集上的表现，并与ASTM标准进行了对比。", "conclusion": "该论文通过提供急需的航空能见度估计数据集和基准测试，为航空气象机器学习研究领域提供了低成本的替代方案和重要的资源，有助于推动该领域的发展。", "translation": "机器学习在航空气象领域是一个不断增长的研究方向，旨在为传统昂贵的气象传感器提供低成本替代方案；然而，在大气能见度估计领域，缺乏适用于航空相关距离、多样化位置、且规模足够用于监督学习的公开数据集，这些数据集也没有进行能见度标记。本文介绍了一个新数据集，它代表了FAA气象摄像头网络一年数据收集活动的成果，非常适合此目的。我们还提出了一个基准测试，当在包括我们自己的数据集在内的三个公开数据集上训练和测试三种常用方法和一个通用基线模型时，并将其与最近批准的ASTM标准进行比较。", "summary": "本文介绍了AIR-VIEW数据集，这是一个专门为航空大气能见度估计构建的图像库，旨在弥补现有公开数据集的不足。该数据集汇集了FAA气象摄像头网络一年的图像数据。此外，论文还提供了一个基准测试，评估了三种常用机器学习方法和一个通用基线模型在AIR-VIEW及其他公开数据集上的性能，并参考了最新的ASTM标准，为航空气象机器学习研究提供了重要资源。", "keywords": "航空气象, 能见度估计, 图像数据集, 机器学习, 基准测试", "comments": "该论文的创新之处在于解决了航空气象领域缺乏高质量、大规模公开数据集的关键问题，特别是针对大气能见度估计。AIR-VIEW数据集的发布为研究人员提供了宝贵的资源，有助于推动低成本航空气象传感器的发展和机器学习在该领域的应用。其提供的基准测试也为后续研究提供了性能评估的参照标准，具有重要的实践意义。"}}
{"id": "2506.21524", "title": "Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding", "authors": ["Libn Varghese", "Bhaskar Chaudhury", "Miral Shah", "Mainak Bandyopadhyay"], "summary": "The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase\nspace information using particle and grid data structures. High computational\ncosts in 2D and 3D device-scale PIC simulations necessitate parallelization,\nwith the Charge Deposition (CD) subroutine often becoming a bottleneck due to\nfrequent particle-grid interactions. Conventional methods mitigate dependencies\nby generating private grids for each core, but this approach faces scalability\nissues. We propose a novel approach based on a particle-thread binding strategy\nthat requires only four private grids per node in distributed memory systems or\nfour private grids in shared memory systems, enhancing CD scalability and\nperformance while maintaining conventional data structures and requiring\nminimal changes to existing PIC codes. This method ensures complete\naccessibility of grid data structure for concurrent threads and avoids\nsimultaneous access to particles within the same cell using additional\nfunctions and flags. Performance evaluations using a PIC benchmark for\nlow-temperature partially magnetized E x B discharge simulation on a shared\nmemory as well as a distributed memory system (1000 cores) demonstrate the\nmethod's scalability, and additionally, we show the method has little hardware\ndependency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21524v1", "categories": ["physics.comp-ph", "cs.DC", "physics.plasm-ph"], "cate": "physics.comp-ph", "url": "http://arxiv.org/abs/2506.21524v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "静电粒子网格法在低温等离子体模拟中的基准测试与粒子线程绑定并行化", "tldr": "提出一种基于粒子线程绑定的新方法，通过减少私有网格数量，显著提升了静电粒子网格法（PIC）中电荷沉积（CD）子程序的并行化性能和可伸缩性，尤其适用于低温等离子体模拟。", "motivation": "2D和3D设备尺度PIC模拟计算成本高，其中电荷沉积（CD）子程序因频繁的粒子-网格交互而成为瓶颈。传统通过为每个核心生成私有网格的方法存在可伸缩性问题，因此需要一种新的并行化策略来提高CD的可伸缩性和性能。", "method": "提出了一种基于粒子线程绑定策略的新方法。该方法在分布式内存系统中每个节点仅需四个私有网格，或在共享内存系统中仅需四个私有网格。它通过额外的函数和标志确保并发线程对网格数据结构的完全可访问性，并避免同时访问同一单元格内的粒子，从而增强CD的可伸缩性和性能，同时保持传统数据结构并对现有PIC代码进行最小程度的修改。", "result": "使用低温部分磁化E×B放电模拟的PIC基准测试在共享内存和分布式内存系统（1000个核心）上进行了性能评估。结果表明，该方法具有良好的可伸缩性，并且硬件依赖性很小。", "conclusion": "该粒子线程绑定策略有效解决了PIC模拟中电荷沉积子程序的并行化瓶颈和可伸缩性问题，通过少量私有网格实现了高效并行，且对现有代码改动小，具有良好的性能和硬件无关性。", "translation": "粒子网格法（PIC）用于等离子体模拟，通过粒子和网格数据结构跟踪粒子相空间信息。2D和3D设备尺度PIC模拟的高计算成本使得并行化成为必要，其中电荷沉积（CD）子程序由于频繁的粒子-网格交互而常常成为瓶颈。传统方法通过为每个核心生成私有网格来减轻依赖性，但这种方法面临可伸缩性问题。我们提出了一种基于粒子线程绑定策略的新方法，该方法在分布式内存系统中每个节点仅需四个私有网格，或在共享内存系统中仅需四个私有网格，从而增强了CD的可伸伸缩性和性能，同时保持了传统数据结构并对现有PIC代码进行最小程度的修改。该方法确保并发线程对网格数据结构的完全可访问性，并使用额外函数和标志避免同时访问同一单元格内的粒子。在共享内存以及分布式内存系统（1000个核心）上使用低温部分磁化E×B放电模拟的PIC基准测试进行的性能评估表明，该方法具有可伸缩性，此外，我们还表明该方法具有很小的硬件依赖性。", "summary": "该论文针对静电粒子网格法（PIC）在低温等离子体模拟中电荷沉积（CD）子程序的并行化瓶颈问题，提出了一种创新的粒子线程绑定策略。该方法通过显著减少所需的私有网格数量（每个节点或系统仅需四个），解决了传统方法的伸缩性限制。实验结果表明，该策略在共享内存和分布式内存系统上均展现出优异的并行性能和可伸缩性，且对现有PIC代码改动极小，同时具有较低的硬件依赖性。", "keywords": "粒子网格法, 并行化, 电荷沉积, 粒子线程绑定, 等离子体模拟", "comments": "这篇论文的创新点在于提出了粒子线程绑定策略，显著减少了PIC模拟中电荷沉积子程序所需的私有网格数量，从而有效解决了传统并行方法在可伸缩性上的瓶颈。其重要性在于，该方法不仅提升了PIC模拟的并行效率和性能，而且对现有代码的改动小，易于集成，且具有良好的硬件无关性，这对于大规模低温等离子体模拟具有重要意义。"}}
{"id": "2506.20818", "title": "Demystifying Distributed Training of Graph Neural Networks for Link Prediction", "authors": ["Xin Huang", "Chul-Ho Lee"], "summary": "Graph neural networks (GNNs) are powerful tools for solving graph-related\nproblems. Distributed GNN frameworks and systems enhance the scalability of\nGNNs and accelerate model training, yet most are optimized for node\nclassification. Their performance on link prediction remains underexplored.\nThis paper demystifies distributed training of GNNs for link prediction by\ninvestigating the issue of performance degradation when each worker trains a\nGNN on its assigned partitioned subgraph without having access to the entire\ngraph. We discover that the main sources of the issue come from not only the\ninformation loss caused by graph partitioning but also the ways of drawing\nnegative samples during model training. While sharing the complete graph\ninformation with each worker resolves the issue and preserves link prediction\naccuracy, it incurs a high communication cost. We propose SpLPG, which\neffectively leverages graph sparsification to mitigate the issue of performance\ndegradation at a reduced communication cost. Experiment results on several\npublic real-world datasets demonstrate the effectiveness of SpLPG, which\nreduces the communication overhead by up to about 80% while mostly preserving\nlink prediction accuracy.", "comment": "Accepted by IEEE ICDCS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20818v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20818v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "揭秘用于链接预测的图神经网络分布式训练", "tldr": "本文深入探讨了用于链接预测的图神经网络分布式训练中性能下降的问题，发现其主要原因在于图划分导致的信息丢失和负样本抽取方式，并提出了SpLPG方法，通过图稀疏化有效解决了性能下降问题并显著降低了通信成本。", "motivation": "现有的分布式图神经网络框架大多针对节点分类进行优化，而它们在链接预测任务上的性能尚未得到充分探索。当每个工作节点仅在分配到的子图上训练GNN而无法访问整个图时，链接预测的性能会下降，本文旨在揭示并解决这一问题。", "method": "本文首先研究了当工作节点仅在分区子图上训练GNN时，链接预测性能下降的原因，发现其主要源于图划分导致的信息丢失和负样本抽取方式。为解决这一问题，论文提出了SpLPG，该方法有效利用图稀疏化来减轻性能下降并降低通信成本。", "result": "实验结果表明，SpLPG在多个公共真实世界数据集上有效，它在大部分保持链接预测准确性的同时，将通信开销降低了约80%。", "conclusion": "本文成功揭示了分布式GNN在链接预测中性能下降的原因，并提出了一种名为SpLPG的有效方法，通过图稀疏化在显著降低通信成本的同时保持了链接预测的准确性。", "translation": "图神经网络（GNNs）是解决图相关问题的强大工具。分布式GNN框架和系统增强了GNN的可扩展性并加速了模型训练，但大多数都针对节点分类进行了优化。它们在链接预测上的性能仍未得到充分探索。本文通过研究每个工作节点在分配到的分区子图上训练GNN而无法访问整个图时性能下降的问题，揭示了用于链接预测的GNN分布式训练的奥秘。我们发现，该问题的主要来源不仅是图划分导致的信息丢失，还包括模型训练期间负样本的抽取方式。虽然与每个工作节点共享完整的图信息可以解决这个问题并保持链接预测的准确性，但这会带来高昂的通信成本。我们提出了SpLPG，它有效地利用图稀疏化来减轻性能下降问题，同时降低了通信成本。在几个公共真实世界数据集上的实验结果表明，SpLPG的有效性，它在主要保持链接预测准确性的同时，将通信开销降低了约80%。", "summary": "本文探讨了分布式图神经网络在链接预测任务中遇到的性能下降问题，当工作节点仅访问部分图数据时，性能下降源于图划分造成的信息损失和负样本采样方式。为解决此问题，论文提出了SpLPG，该方法通过图稀疏化有效缓解了性能下降，并在保持链接预测准确性的同时显著降低了通信开销。", "keywords": "图神经网络, 分布式训练, 链接预测, 图稀疏化, 通信开销", "comments": "这篇论文通过深入分析分布式GNN在链接预测中的性能瓶颈，揭示了信息丢失和负采样方式的关键影响，具有重要的理论意义。SpLPG方法的提出，特别是利用图稀疏化来平衡性能与通信成本，展现了创新性，为大规模图神经网络的实际应用提供了新的思路。其对通信开销的显著降低，对于资源受限的分布式环境尤为重要。"}}
{"id": "2506.21508", "title": "skLEP: A Slovak General Language Understanding Benchmark", "authors": ["Marek Šuppa", "Andrej Ridzik", "Daniel Hládek", "Tomáš Javůrek", "Viktória Ondrejová", "Kristína Sásiková", "Martin Tamajka", "Marián Šimko"], "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.21508v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21508v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "skLEP：一个斯洛伐克通用语言理解基准", "tldr": "介绍了skLEP，首个用于评估斯洛伐克语NLU模型的综合基准，包含九项任务，并发布了数据集、工具包和排行榜。", "motivation": "现有斯洛伐克语NLU模型缺乏一个全面的评估基准，因此需要一个专门设计的基准来彻底评估模型能力并推动未来研究。", "method": "编译了skLEP，包含九项多样化任务（词元级、句子对级、文档级）。创建了新的斯洛伐克语原创数据集，并精心翻译了已有的英语NLU资源。使用skLEP任务系统地评估了各种斯洛伐克语特有、多语言和英语预训练语言模型。发布了完整的基准数据、开源工具包和公共排行榜。", "result": "成功引入了skLEP，首个用于评估斯洛伐克语NLU模型的综合基准。对多种预训练语言模型进行了首次系统性评估。发布了数据集、工具包和排行榜，以促进可复现性并推动未来研究。", "conclusion": "skLEP的引入及其相关资源的发布，为斯洛伐克语NLU模型提供了一个全面的评估框架，并有望促进该领域的研究进展。", "translation": "在这项工作中，我们介绍了skLEP，这是第一个专门为评估斯洛伐克语自然语言理解（NLU）模型而设计的综合基准。我们编译了skLEP，使其包含九项多样化的任务，涵盖了词元级、句子对级和文档级挑战，从而提供了对模型能力的全面评估。为了创建这个基准，我们整理了为斯洛伐克语量身定制的全新原创数据集，并精心翻译了已有的英语NLU资源。在这篇论文中，我们还首次使用skLEP任务对各种斯洛伐克语特有、多语言和英语预训练语言模型进行了系统而广泛的评估。最后，我们还发布了完整的基准数据、一个促进模型微调和评估的开源工具包，以及一个公共排行榜（https://github.com/slovak-nlp/sklep），希望能促进可复现性并推动斯洛伐克语NLU的未来研究。", "summary": "本文介绍了skLEP，这是首个专为斯洛伐克语自然语言理解（NLU）模型评估设计的综合基准。skLEP包含九项涵盖不同粒度（词元、句子对、文档）的任务，其数据集由原创斯洛伐克语数据和翻译的英语NLU资源组成。作者使用skLEP对多种预训练语言模型进行了首次系统评估，并发布了基准数据、开源工具包和公共排行榜，旨在促进斯洛伐克语NLU领域的研究进展和可复现性。", "keywords": "斯洛伐克语NLU, 语言理解基准, 自然语言处理, 模型评估, 数据集", "comments": "这篇论文的创新点在于填补了斯洛伐克语NLU领域缺乏综合评估基准的空白。通过提供多样化的任务、高质量的数据集以及开源工具和排行榜，该工作极大地降低了斯洛伐克语NLU研究的门槛，有望加速该领域的发展。其对多种模型的广泛评估也为后续研究提供了有价值的基线。"}}
{"id": "2506.21395", "title": "Optimal solutions employing an algebraic Variational Multiscale approach Part II: Application to Navier-Stokes", "authors": ["Suyash Shrestha", "Marc Gerritsma", "Gonzalo Rubio", "Steven Hulshoff", "Esteban Ferrer"], "summary": "This work presents a nonlinear extension of the high-order discretisation\nframework based on the Variational Multiscale (VMS) method previously\nintroduced for steady linear problems. Building on the concept of an optimal\nprojector defined via the symmetric part of the governing operator, we\ngeneralise the formulation to treat the 2D incompressible Navier-Stokes\nequations. The arroach maintains a clear separation between the resolved and\nunresolved scales, with the fine-scale contributions approximated through the\napproximate Fine-Scale Greens' function of the associated symmetric operator.\nThis enables a consistent variational treatment of the nonlinearity while\npreserving high-order accuracy. We show that the method yields numerical\nsolutions that closely approximate the optimal projection of the\ncontinuous/highly resolved solution and inherits desirable conservation\nproperties. Numerical results confirm the framework's robustness, accuracy, and\nits potential for application to a broad class of nonlinear multiscale\nproblems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21395v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21395v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "采用代数变分多尺度方法的优化解 第二部分：在Navier-Stokes方程中的应用", "tldr": "本文将先前用于稳态线性问题的变分多尺度(VMS)方法扩展到非线性问题，特别是2D不可压缩Navier-Stokes方程，并展示了其高精度、鲁棒性和守恒性。", "motivation": "将先前用于稳态线性问题的变分多尺度（VMS）离散化框架非线性扩展，以处理2D不可压缩Navier-Stokes方程。", "method": "本文提出了基于变分多尺度（VMS）方法的非线性高阶离散化框架的扩展。通过一个由控制算子的对称部分定义的最佳投影器概念，将公式推广到处理2D不可压缩Navier-Stokes方程。该方法在已分辨尺度和未分辨尺度之间保持清晰分离，细尺度贡献通过相关对称算子的近似细尺度格林函数进行近似。", "result": "该方法产生的数值解与连续/高分辨解的最佳投影非常接近，并继承了理想的守恒特性。数值结果证实了该框架的鲁棒性、准确性及其在广泛的非线性多尺度问题中的应用潜力。", "conclusion": "该非线性变分多尺度框架具有鲁棒性、准确性，并有望应用于广泛的非线性多尺度问题。", "translation": "这项工作提出了先前为稳态线性问题引入的基于变分多尺度（VMS）方法的高阶离散化框架的非线性扩展。基于通过控制算子的对称部分定义的最佳投影器概念，我们将公式推广到处理二维不可压缩Navier-Stokes方程。该方法在已分辨尺度和未分辨尺度之间保持清晰分离，细尺度贡献通过相关对称算子的近似细尺度格林函数进行近似。这使得非线性能够得到一致的变分处理，同时保持高阶精度。我们表明，该方法产生的数值解与连续/高分辨解的最佳投影非常接近，并继承了理想的守恒特性。数值结果证实了该框架的鲁棒性、准确性及其在广泛的非线性多尺度问题中的应用潜力。", "summary": "本文将先前用于稳态线性问题的变分多尺度（VMS）方法扩展到非线性问题，特别是2D不可压缩Navier-Stokes方程。该方法通过最佳投影器和近似细尺度格林函数，实现了已分辨尺度与未分辨尺度的清晰分离，并能一致地处理非线性，同时保持高阶精度。结果表明，该方法能产生接近最优投影的数值解，并具有良好的守恒性、鲁棒性和准确性，适用于多种非线性多尺度问题。", "keywords": "变分多尺度, Navier-Stokes, 非线性, 数值方法, 高阶精度", "comments": "本文的主要创新在于将变分多尺度（VMS）方法从线性稳态问题成功扩展到非线性Navier-Stokes方程，并保持了高阶精度和守恒性。这对于解决复杂的非线性多尺度问题具有重要意义，展示了该方法在数值模拟领域的强大潜力。"}}
{"id": "2506.21098", "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "pdf_url": "http://arxiv.org/pdf/2506.21098v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21098v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ComRAG：面向工业实时社区问答的动态向量存储检索增强生成", "tldr": "ComRAG是一个为工业实时社区问答设计的检索增强生成框架，它通过整合静态知识和动态历史问答对，并采用基于质心的记忆机制，显著提升了性能并降低了延迟和存储增长。", "motivation": "社区问答（CQA）平台是重要的知识库，但有效利用历史交互和领域知识进行实时问答仍面临挑战。现有方法常未充分利用外部知识，未能整合动态历史问答上下文，或缺乏适用于工业部署的记忆机制。", "method": "本文提出了ComRAG，一个用于实时工业CQA的检索增强生成框架。它通过基于质心的记忆机制，将静态知识与动态历史问答对相结合，并为检索、生成和高效存储进行了设计。", "result": "在三个工业CQA数据集上评估，ComRAG始终优于所有基线——向量相似度提高了25.9%，延迟降低了8.7%至23.3%，迭代过程中块增长从20.23%降低到2.06%。", "conclusion": "ComRAG通过其创新的动态记忆机制，有效解决了工业实时社区问答中的挑战，并在性能、效率和存储方面取得了显著提升。", "translation": "社区问答（CQA）平台可被视为社区中重要的知识库，但如何有效利用历史交互和领域知识进行实时问答仍然是一个挑战。现有方法往往未充分利用外部知识，未能整合动态历史问答上下文，或缺乏适用于工业部署的记忆机制。我们提出了ComRAG，一个用于实时工业CQA的检索增强生成框架，它通过为检索、生成和高效存储设计的基于质心的记忆机制，将静态知识与动态历史问答对相结合。在三个工业CQA数据集上进行评估，ComRAG始终优于所有基线——在向量相似度方面取得了高达25.9%的提升，延迟降低了8.7%至23.3%，并且迭代过程中块增长从20.23%降低到2.06%。", "summary": "本文提出了ComRAG，一个针对工业实时社区问答的检索增强生成框架。它通过引入基于质心的动态记忆机制，有效整合了静态知识和动态历史问答对，解决了现有方法在知识利用、动态上下文整合和记忆机制方面的不足。实验证明，ComRAG在向量相似度、延迟和存储效率方面均显著优于现有基线。", "keywords": "社区问答, 检索增强生成, 动态向量存储, 实时问答, 工业应用", "comments": "ComRAG的创新之处在于其针对工业CQA场景设计的动态向量存储和基于质心的记忆机制，有效解决了实时问答中知识利用和效率的难题。其在性能提升和资源消耗降低方面的表现，使其在工业应用中具有重要价值。"}}
{"id": "2506.21347", "title": "Real-time Terrain Analysis for Off-road Autonomous Vehicles", "authors": ["Edwina Lewis", "Aditya Parameshwaran", "Laura Redmond", "Yue Wang"], "summary": "This research addresses critical autonomous vehicle control challenges\narising from road roughness variation, which induces course deviations and\npotential loss of road contact during steering operations. We present a novel\nreal-time road roughness estimation system employing Bayesian calibration\nmethodology that processes axle accelerations to predict terrain roughness with\nquantifiable confidence measures. The technical framework integrates a Gaussian\nprocess surrogate model with a simulated half-vehicle model, systematically\nprocessing vehicle velocity and road surface roughness parameters to generate\ncorresponding axle acceleration responses. The Bayesian calibration routine\nperforms inverse estimation of road roughness from observed accelerations and\nvelocities, yielding posterior distributions that quantify prediction\nuncertainty for adaptive risk management. Training data generation utilizes\nLatin Hypercube sampling across comprehensive velocity and roughness parameter\nspaces, while the calibrated model integrates seamlessly with a Simplex\ncontroller architecture to dynamically adjust velocity limits based on\nreal-time roughness predictions. Experimental validation on stochastically\ngenerated surfaces featuring varying roughness regions demonstrates robust\nreal-time characterization capabilities, with the integrated Simplex control\nstrategy effectively enhancing autonomous vehicle operational safety through\nproactive surface condition response. This innovative Bayesian framework\nestablishes a comprehensive foundation for mitigating roughness-related\noperational risks while simultaneously improving efficiency and safety margins\nin autonomous vehicle systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21347v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21347v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "越野自主车辆的实时地形分析", "tldr": "本文提出了一种新颖的实时道路粗糙度估计系统，利用贝叶斯校准方法处理车轴加速度，以预测地形粗糙度并量化置信度，从而提高越野自主车辆的运行安全性和效率。", "motivation": "为解决道路粗糙度变化导致自主车辆在转向操作中出现方向偏差和潜在失控的问题，本研究旨在开发一种实时地形分析方法。", "method": "该研究提出了一种实时道路粗糙度估计系统，采用贝叶斯校准方法处理车轴加速度。技术框架整合了高斯过程代理模型与模拟半车辆模型，系统地处理车辆速度和路面粗糙度参数以生成车轴加速度响应。贝叶斯校准例程从观测到的加速度和速度中反向估计道路粗糙度，并产生量化预测不确定性的后验分布。训练数据通过拉丁超立方采样生成，校准模型与Simplex控制器架构无缝集成，根据实时粗糙度预测动态调整速度限制。", "result": "在随机生成的不同粗糙度区域表面进行的实验验证表明，该系统具有强大的实时表征能力。集成的Simplex控制策略通过主动响应路面状况，有效提升了自主车辆的运行安全性。", "conclusion": "这种创新的贝叶斯框架为缓解与粗糙度相关的操作风险奠定了全面的基础，同时提高了自主车辆系统的效率和安全裕度。", "translation": "本研究旨在解决道路粗糙度变化引起的自主车辆控制挑战，这些变化会导致转向操作期间的方向偏差和潜在的失控。我们提出了一种新颖的实时道路粗糙度估计系统，该系统采用贝叶斯校准方法，处理车轴加速度以预测地形粗糙度并提供可量化的置信度。该技术框架将高斯过程代理模型与模拟半车辆模型相结合，系统地处理车辆速度和路面粗糙度参数以生成相应的车轴加速度响应。贝叶斯校准程序从观测到的加速度和速度中进行道路粗糙度的逆向估计，生成后验分布以量化预测不确定性，用于自适应风险管理。训练数据生成利用拉丁超立方采样在全面的速度和粗糙度参数空间中进行，而校准模型与Simplex控制器架构无缝集成，根据实时粗糙度预测动态调整速度限制。在具有不同粗糙度区域的随机生成表面上进行的实验验证表明，其具有强大的实时表征能力，集成的Simplex控制策略通过主动响应路面状况，有效提升了自主车辆的运行安全性。这种创新的贝叶斯框架为缓解与粗糙度相关的操作风险奠定了全面的基础，同时提高了自主车辆系统的效率和安全裕度。", "summary": "本文提出了一种新颖的实时道路粗糙度估计系统，旨在解决自主车辆因路面粗糙度变化导致的控制挑战。该系统利用贝叶斯校准方法，通过处理车轴加速度来预测地形粗糙度，并量化预测不确定性。技术框架整合了高斯过程代理模型和模拟半车辆模型，并与Simplex控制器无缝集成，以根据实时粗糙度预测动态调整车辆速度限制。实验验证表明，该系统能够提供鲁棒的实时地形表征，并有效增强自主车辆的运行安全性。", "keywords": "实时, 地形分析, 自动驾驶车辆, 贝叶斯校准, 道路粗糙度", "comments": "该论文提出了一种创新的贝叶斯框架，用于实时道路粗糙度估计，有效解决了越野自主车辆面临的关键挑战。其创新之处在于将贝叶斯校准、高斯过程代理模型与Simplex控制器相结合，实现了对地形粗糙度的精确预测和自适应的速度调整，从而显著提高了车辆的运行安全性和效率。量化预测不确定性是其重要贡献，有助于更可靠的风险管理。"}}
{"id": "2506.20947", "title": "Hierarchical Sub-action Tree for Continuous Sign Language Recognition", "authors": ["Dejie Yang", "Zhu Xu", "Xinjie Gao", "Yang Liu"], "summary": "Continuous sign language recognition (CSLR) aims to transcribe untrimmed\nvideos into glosses, which are typically textual words. Recent studies indicate\nthat the lack of large datasets and precise annotations has become a bottleneck\nfor CSLR due to insufficient training data. To address this, some works have\ndeveloped cross-modal solutions to align visual and textual modalities.\nHowever, they typically extract textual features from glosses without fully\nutilizing their knowledge. In this paper, we propose the Hierarchical\nSub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge\nwith visual representation learning. By incorporating gloss-specific knowledge\nfrom large language models, our approach leverages textual information more\neffectively. Specifically, we construct an HST for textual information\nrepresentation, aligning visual and textual modalities step-by-step and\nbenefiting from the tree structure to reduce computational complexity.\nAdditionally, we impose a contrastive alignment enhancement to bridge the gap\nbetween the two modalities. Experiments on four datasets (PHOENIX-2014,\nPHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the\neffectiveness of our HST-CSLR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20947v1", "categories": ["cs.CV", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20947v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于连续手语识别的层次子动作树", "tldr": "本文提出了HST-CSLR，通过构建层次子动作树（HST）并结合大型语言模型的词素知识，有效提升了连续手语识别（CSLR）的性能，解决了数据不足和词素知识利用不充分的问题，并降低了计算复杂度。", "motivation": "连续手语识别（CSLR）面临的主要瓶颈是缺乏大型数据集和精确标注，导致训练数据不足。现有的跨模态解决方案在提取文本特征时未能充分利用词素知识。", "method": "本文提出了层次子动作树（HST），命名为HST-CSLR，旨在有效结合词素知识与视觉表示学习。该方法通过整合来自大型语言模型的词素特定知识，更有效地利用文本信息。具体来说，它构建了一个用于文本信息表示的HST，逐步对齐视觉和文本模态，并利用树结构降低计算复杂度。此外，还引入了对比对齐增强机制来弥合两种模态之间的差距。", "result": "在PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture这四个数据集上的实验证明了所提出的HST-CSLR的有效性。", "conclusion": "所提出的HST-CSLR方法通过有效结合词素知识和视觉表示学习，成功解决了连续手语识别中数据不足和词素知识利用不充分的问题，并在多项数据集上展现出优越性能。", "translation": "连续手语识别（CSLR）旨在将未剪辑的视频转录为通常为文本词汇的词素。最近的研究表明，由于训练数据不足，大型数据集和精确标注的缺乏已成为CSLR的瓶颈。为了解决这个问题，一些工作开发了跨模态解决方案来对齐视觉和文本模态。然而，它们通常从词素中提取文本特征，而没有充分利用其知识。在本文中，我们提出了层次子动作树（Hierarchical Sub-action Tree，简称HST），命名为HST-CSLR，以有效地将词素知识与视觉表示学习相结合。通过整合来自大型语言模型的词素特定知识，我们的方法更有效地利用了文本信息。具体来说，我们构建了一个用于文本信息表示的HST，逐步对齐视觉和文本模态，并受益于树结构以降低计算复杂度。此外，我们施加了对比对齐增强以弥合两种模态之间的差距。在四个数据集（PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture）上的实验证明了我们HST-CSLR的有效性。", "summary": "本文提出了一种名为HST-CSLR的新型连续手语识别方法，旨在解决数据稀缺和词素知识未充分利用的问题。该方法引入了层次子动作树（HST），以有效地将来自大型语言模型的特定词素知识与视觉表示相结合。HST促进了视觉和文本模态的逐步对齐，降低了计算复杂度，并通过对比对齐机制得到进一步增强。在多个数据集上的实验结果证实了该方法的有效性。", "keywords": "连续手语识别, 层次子动作树, 词素知识, 跨模态对齐, 大型语言模型", "comments": "本文通过利用大型语言模型的词素知识并构建层次子动作树来结构化文本信息，提出了一种创新的连续手语识别方法。其逐步对齐策略和计算复杂度的降低是显著的改进。将树结构用于对齐以及结合大模型知识是该方法的核心优势。"}}
{"id": "2506.20849", "title": "Learning-Based Resource Management in Integrated Sensing and Communication Systems", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "In this paper, we tackle the task of adaptive time allocation in integrated\nsensing and communication systems equipped with radar and communication units.\nThe dual-functional radar-communication system's task involves allocating dwell\ntimes for tracking multiple targets and utilizing the remaining time for data\ntransmission towards estimated target locations. We introduce a novel\nconstrained deep reinforcement learning (CDRL) approach, designed to optimize\nresource allocation between tracking and communication under time budget\nconstraints, thereby enhancing target communication quality. Our numerical\nresults demonstrate the efficiency of our proposed CDRL framework, confirming\nits ability to maximize communication quality in highly dynamic environments\nwhile adhering to time constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20849v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20849v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "集成感知与通信系统中基于学习的资源管理", "tldr": "本文提出了一种基于约束深度强化学习（CDRL）的方法，用于在集成感知与通信系统中优化雷达跟踪和数据传输之间的时间分配，以在动态环境下最大化通信质量。", "motivation": "本文旨在解决集成感知与通信系统中自适应时间分配的任务，具体是优化雷达跟踪多个目标和向估计目标位置进行数据传输之间的时间分配，以提升目标通信质量。", "method": "本文引入了一种新颖的约束深度强化学习（CDRL）方法，旨在时间预算约束下优化跟踪和通信之间的资源分配。", "result": "数值结果表明，所提出的CDRL框架是高效的，并证实了其在高度动态环境中遵守时间约束的同时最大化通信质量的能力。", "conclusion": "本文提出的CDRL框架能够有效地在集成感知与通信系统中进行资源管理，实现动态环境下通信质量的最大化，同时满足时间约束。", "translation": "在本文中，我们探讨了集成感知与通信系统中自适应时间分配的任务，该系统配备了雷达和通信单元。双功能雷达通信系统的任务包括为跟踪多个目标分配驻留时间，并利用剩余时间向估计的目标位置进行数据传输。我们引入了一种新颖的约束深度强化学习（CDRL）方法，旨在时间预算约束下优化跟踪和通信之间的资源分配，从而提高目标通信质量。我们的数值结果证明了我们提出的CDRL框架的效率，证实了其在高度动态环境中遵守时间约束的同时最大化通信质量的能力。", "summary": "本文提出了一种新颖的约束深度强化学习（CDRL）方法，用于解决集成感知与通信系统中雷达跟踪和数据传输之间的自适应时间分配问题。该方法旨在时间预算约束下优化资源分配，以提高目标通信质量。数值结果验证了CDRL框架在动态环境中高效地最大化通信质量并遵守时间约束的能力。", "keywords": "集成感知与通信, 资源管理, 深度强化学习, 时间分配, 通信质量", "comments": "本文的创新点在于提出了一个基于约束深度强化学习的框架（CDRL），用于解决集成感知与通信（ISAC）系统中雷达跟踪和通信资源分配的复杂问题。这种方法能够有效地在动态环境下优化时间分配，从而提升通信质量，这对于未来ISAC系统的发展具有重要意义。"}}
{"id": "2506.20853", "title": "Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management", "authors": ["Ziyang Lu", "Subodh Kalia", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "The time allocation problem in multi-function cognitive radar systems focuses\non the trade-off between scanning for newly emerging targets and tracking the\npreviously detected targets. We formulate this as a multi-objective\noptimization problem and employ deep reinforcement learning to find\nPareto-optimal solutions and compare deep deterministic policy gradient (DDPG)\nand soft actor-critic (SAC) algorithms. Our results demonstrate the\neffectiveness of both algorithms in adapting to various scenarios, with SAC\nshowing improved stability and sample efficiency compared to DDPG. We further\nemploy the NSGA-II algorithm to estimate an upper bound on the Pareto front of\nthe considered problem. This work contributes to the development of more\nefficient and adaptive cognitive radar systems capable of balancing multiple\ncompeting objectives in dynamic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20853v1", "categories": ["cs.LG", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20853v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "认知雷达资源管理中的多目标强化学习", "tldr": "本文将认知雷达时间分配问题建模为多目标优化问题，并使用深度强化学习（DDPG和SAC）寻找帕累托最优解，结果显示SAC在稳定性和样本效率上优于DDPG。", "motivation": "多功能认知雷达系统中的时间分配问题，需要在扫描新出现的目标和跟踪已检测目标之间进行权衡。", "method": "将问题表述为多目标优化问题，并采用深度强化学习来寻找帕累托最优解，比较了深度确定性策略梯度（DDPG）和软行动者-评论家（SAC）算法。此外，还使用NSGA-II算法来估计所考虑问题的帕累托前沿的上限。", "result": "两种算法在适应各种场景方面均有效，其中SAC与DDPG相比表现出更高的稳定性和样本效率。", "conclusion": "这项工作有助于开发更高效、更自适应的认知雷达系统，能够在动态环境中平衡多个相互竞争的目标。", "translation": "多目标强化学习在认知雷达资源管理中的应用\n\n多功能认知雷达系统中的时间分配问题，侧重于新兴目标扫描和已检测目标跟踪之间的权衡。我们将此问题表述为多目标优化问题，并采用深度强化学习来寻找帕累托最优解，并比较了深度确定性策略梯度（DDPG）和软行动者-评论家（SAC）算法。我们的结果表明，两种算法在适应各种场景方面均有效，其中SAC与DDPG相比表现出更高的稳定性和样本效率。我们进一步采用NSGA-II算法来估计所考虑问题的帕累托前沿的上限。这项工作有助于开发更高效、更自适应的认知雷达系统，能够在动态环境中平衡多个相互竞争的目标。", "summary": "本研究解决了多功能认知雷达系统中的时间分配问题，该问题涉及新目标扫描和已跟踪目标跟踪之间的权衡。作者将此问题建模为多目标优化问题，并应用深度强化学习方法，特别是比较了DDPG和SAC算法，以找到帕累托最优解。实验结果表明，两种算法都能有效适应不同场景，其中SAC在稳定性和样本效率方面优于DDPG。此外，研究还利用NSGA-II算法来估计帕累托前沿的上限。这项工作为开发更高效、更具适应性的认知雷达系统提供了贡献，使其能够在动态环境中有效平衡多个竞争目标。", "keywords": "多目标强化学习, 认知雷达, 资源管理, DDPG, SAC", "comments": "本文创新性地将多目标强化学习应用于认知雷达资源管理，以解决扫描和跟踪之间的权衡问题。通过比较DDPG和SAC算法，并指出SAC的优越性，为该领域的实际应用提供了有价值的指导。引入NSGA-II来估计帕累托前沿的上限，也增强了研究的严谨性。"}}
{"id": "2506.21538", "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval", "authors": ["Hani Alomari", "Anushka Sivakumar", "Andrew Zhang", "Chris Thomas"], "summary": "Cross-modal image-text retrieval is challenging because of the diverse\npossible associations between content from different modalities. Traditional\nmethods learn a single-vector embedding to represent semantics of each sample,\nbut struggle to capture nuanced and diverse relationships that can exist across\nmodalities. Set-based approaches, which represent each sample with multiple\nembeddings, offer a promising alternative, as they can capture richer and more\ndiverse relationships. In this paper, we show that, despite their promise,\nthese set-based representations continue to face issues including sparse\nsupervision and set collapse, which limits their effectiveness. To address\nthese challenges, we propose Maximal Pair Assignment Similarity to optimize\none-to-one matching between embedding sets which preserve semantic diversity\nwithin the set. We also introduce two loss functions to further enhance the\nrepresentations: Global Discriminative Loss to enhance distinction among\nembeddings, and Intra-Set Divergence Loss to prevent collapse within each set.\nOur method achieves state-of-the-art performance on MS-COCO and Flickr30k\nwithout relying on external data.", "comment": "Accepted at the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025 Main)", "pdf_url": "http://arxiv.org/pdf/2506.21538v1", "categories": ["cs.CV", "cs.IR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21538v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "最大匹配很重要：防止表示坍塌以实现鲁棒的跨模态检索", "tldr": "本文提出了最大对分配相似度（Maximal Pair Assignment Similarity）以及两种新的损失函数（全局判别损失和集合内散度损失），以防止基于集合的跨模态检索中的表示坍塌，并取得了最先进的性能。", "motivation": "传统的单向量嵌入方法难以捕捉跨模态之间细微和多样化的关系。尽管基于集合的方法在捕捉更丰富、更多样化关系方面有前景，但它们仍面临稀疏监督和集合坍塌等问题，这限制了其有效性。", "method": "为了解决挑战，本文提出：1. 最大对分配相似度（Maximal Pair Assignment Similarity）来优化嵌入集合之间的一对一匹配，以保留集合内的语义多样性。2. 全局判别损失（Global Discriminative Loss）来增强嵌入之间的区分度。3. 集合内散度损失（Intra-Set Divergence Loss）来防止每个集合内部的坍塌。", "result": "所提出的方法在MS-COCO和Flickr30k数据集上取得了最先进的性能，并且不依赖于外部数据。", "conclusion": "所提出的最大对分配相似度，结合全局判别损失和集合内散度损失，有效解决了基于集合的跨模态检索中的表示坍塌和稀疏监督问题，从而提高了性能。", "translation": "跨模态图像-文本检索具有挑战性，因为不同模态内容之间可能存在多种关联。传统方法学习单一向量嵌入来表示每个样本的语义，但难以捕捉跨模态存在的细微和多样化关系。基于集合的方法，即用多个嵌入表示每个样本，提供了一种有前景的替代方案，因为它们可以捕捉更丰富和更多样化的关系。在本文中，我们表明，尽管基于集合的表示有其前景，但它们仍然面临稀疏监督和集合坍塌等问题，这限制了它们的有效性。为了解决这些挑战，我们提出了最大对分配相似度（Maximal Pair Assignment Similarity）来优化嵌入集合之间的一对一匹配，从而保留集合内的语义多样性。我们还引入了两个损失函数来进一步增强表示：全局判别损失（Global Discriminative Loss）以增强嵌入之间的区分度，以及集合内散度损失（Intra-Set Divergence Loss）以防止每个集合内部的坍塌。我们的方法在MS-COCO和Flickr30k上取得了最先进的性能，且不依赖外部数据。", "summary": "本文针对基于集合的跨模态检索中存在的表示坍塌和稀疏监督问题，提出了一种新的解决方案。虽然基于集合的方法比单向量嵌入更有效，但仍有局限。为此，本文引入了最大对分配相似度（Maximal Pair Assignment Similarity）来优化嵌入集合之间的一对一匹配，以保持语义多样性。此外，还提出了全局判别损失（Global Discriminative Loss）和集合内散度损失（Intra-Set Divergence Loss），分别用于增强区分度和防止集合内坍塌。该方法在MS-COCO和Flickr30k数据集上取得了最先进的性能。", "keywords": "跨模态检索, 表示坍塌, 基于集合的嵌入, 最大匹配, 损失函数", "comments": "该论文的创新点在于明确解决了基于集合的跨模态检索中“集合坍塌”这一已知限制。通过引入新颖的相似性度量（MPAS）和两个有针对性的损失函数（GDL、IDL），它提供了一个全面的解决方案来保持语义多样性和判别能力。在不依赖外部数据的情况下取得最先进的成果，突显了其有效性和效率。"}}
{"id": "2506.21405", "title": "An adaptive dynamical low-rank optimizer for solving kinetic parameter identification inverse problems", "authors": ["Lena Baumann", "Lukas Einkemmer", "Christian Klingenberg", "Jonas Kusch"], "summary": "The numerical solution of parameter identification inverse problems for\nkinetic equations can exhibit high computational and memory costs. In this\npaper, we propose a dynamical low-rank scheme for the reconstruction of the\nscattering parameter in the radiative transfer equation from a number of\nmacroscopic time-independent measurements. We first work through the PDE\nconstrained optimization procedure in a continuous setting and derive the\nadjoint equations using a Lagrangian reformulation. For the scattering\ncoefficient, a periodic B-spline approximation is introduced and a gradient\ndescent step for updating its coefficients is formulated. After the\ndiscretization, a dynamical low-rank approximation (DLRA) is applied. We make\nuse of the rank-adaptive basis update & Galerkin integrator and a line search\napproach for the adaptive refinement of the gradient descent step size and the\nDLRA tolerance. We show that the proposed scheme significantly reduces both\nmemory and computational cost. Numerical results computed with different\ninitial conditions validate the accuracy and efficiency of the proposed DLRA\nscheme compared to solutions computed with a full solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21405v1", "categories": ["math.NA", "cs.NA", "35Q49, 49M41, 65M22, 65M32"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21405v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种用于解决动力学参数识别逆问题的自适应动态低秩优化器", "tldr": "本文提出了一种自适应动态低秩优化方案，用于解决动力学参数识别逆问题，显著降低了计算和内存成本，并验证了其准确性和效率。", "motivation": "动力学方程参数识别逆问题的数值解存在高计算和内存成本。", "method": "本文提出了一种动态低秩方案，用于从宏观时间无关测量中重建辐射传输方程中的散射参数。该方法首先在连续设置中进行偏微分方程约束优化，使用拉格朗日重构推导伴随方程。对于散射系数，引入周期B样条近似并制定梯度下降步骤。离散化后，应用动态低秩近似（DLRA），并利用秩自适应基更新、伽辽金积分器和线搜索方法来自适应细化梯度下降步长和DLRA容差。", "result": "所提出的方案显著降低了内存和计算成本。使用不同初始条件计算的数值结果验证了所提出的DLRA方案的准确性和效率，与使用完整求解器计算的解决方案相比。", "conclusion": "该研究表明，所提出的自适应动态低秩近似（DLRA）方案能够有效且高效地解决动力学参数识别逆问题，显著降低了计算和内存成本，并保持了高精度。", "translation": "动力学方程参数识别逆问题的数值解可能表现出高计算和内存成本。在本文中，我们提出了一种动态低秩方案，用于从多个宏观时间无关测量中重建辐射传输方程中的散射参数。我们首先在连续设置中通过偏微分方程约束优化过程，并使用拉格朗日重构推导伴随方程。对于散射系数，引入了周期B样条近似，并制定了更新其系数的梯度下降步骤。离散化后，应用动态低秩近似（DLRA）。我们利用秩自适应基更新和伽辽金积分器，以及线搜索方法来自适应细化梯度下降步长和DLRA容差。我们表明，所提出的方案显著降低了内存和计算成本。使用不同初始条件计算的数值结果验证了所提出的DLRA方案的准确性和效率，与使用完整求解器计算的解决方案相比。", "summary": "本文提出了一种自适应动态低秩近似（DLRA）方案，用于解决动力学方程的参数识别逆问题，特别是辐射传输方程中散射参数的重建。该方案结合了偏微分方程约束优化、拉格朗日重构、周期B样条近似和梯度下降法。通过引入秩自适应基更新、伽辽金积分器和线搜索方法，实现了对步长和DLRA容差的自适应调整。数值结果表明，该方案显著降低了计算和内存成本，并在准确性和效率方面优于传统完整求解器。", "keywords": "动态低秩近似, 逆问题, 参数识别, 动力学方程, 自适应优化", "comments": "该论文的创新之处在于将自适应动态低秩近似方法应用于动力学参数识别逆问题，有效解决了传统方法计算和内存成本高的问题。其提出的结合秩自适应和线搜索的策略，提高了算法的自适应性和效率。该方法在处理大规模逆问题时具有重要意义。"}}
{"id": "2506.21119", "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios.", "comment": "Accepted by ICONIP 2024", "pdf_url": "http://arxiv.org/pdf/2506.21119v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21119v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Progtuning：面向Transformer语言模型的渐进式微调框架", "tldr": "Progtuning是一种新的微调框架，通过渐进式地减少更新的Transformer块数量，优化资源分配并减少25%的参数更新，同时保持竞争力性能，并与现有参数高效微调方法高度兼容。", "motivation": "随着Transformer语言模型规模的不断增长，更新所有模型参数的成本日益增加。尽管参数高效微调方法通过选择性更新少量参数来解决此问题，但它们和大多数现有微调方法仍需更新与初始模型相同数量的参数，这忽略了Transformer块之间贡献的不平衡性，导致计算资源分配效率低下。", "method": "本文提出了Progtuning，这是一种结合渐进式学习的Transformer语言模型新型微调框架。具体而言，Progtuning根据贡献度渐进式地减少需要更新的Transformer块数量。", "result": "Progtuning显著优化了资源分配，并将更新参数的数量减少了约25%，同时仍保持了具有竞争力的性能。它还显示出与参数高效微调方法的高度适应性，在各种适应场景中表现出色。", "conclusion": "Progtuning通过渐进式减少更新参数数量，有效提升了Transformer模型微调的资源效率，同时保持了高性能，并展现了良好的兼容性。", "translation": "微调是利用基于Transformer的语言模型进行下游任务的一种有前景的技术。随着模型规模的不断增长，更新所有模型参数变得越来越昂贵。参数高效微调方法通过选择性更新一小部分参数有效地解决了这个问题。然而，微调和大多数现有参数高效微调方法需要更新与初始大小相同数量的参数，这忽略了Transformer块之间贡献的不平等性，导致计算资源分配极其低效。在本文中，我们提出了Progtuning，这是一种结合渐进式学习的基于Transformer语言模型的新型微调框架。具体而言，Progtuning根据贡献度渐进式地减少更新的Transformer块数量。值得注意的是，Progtuning优化了资源分配，并将更新参数的数量减少了大约25%，同时仍保持了具有竞争力的性能。它还显示出与参数高效微调方法的高度适应性，在各种适应场景中表现出色。", "summary": "本文提出Progtuning，一个面向Transformer语言模型的渐进式微调框架。针对现有微调方法在大型模型中资源分配效率低下的问题，Progtuning根据Transformer块的贡献度，渐进式地减少更新的块数量。实验表明，Progtuning能优化资源分配，减少约25%的更新参数量，同时保持竞争力性能，并能与现有参数高效微调方法良好兼容。", "keywords": "Progtuning, 微调, Transformer, 参数高效, 渐进式学习", "comments": "Progtuning的创新之处在于其渐进式减少更新Transformer块数量的策略，这有效解决了现有微调方法在大型模型中资源分配效率低下的问题。通过利用块贡献度的差异，它实现了计算资源的优化，并在保持性能的同时显著减少了参数更新量，展现了其在实际应用中的重要性和潜力。"}}
{"id": "2506.21539", "title": "WorldVLA: Towards Autoregressive Action World Model", "authors": ["Jun Cen", "Chaohui Yu", "Hangjie Yuan", "Yuming Jiang", "Siteng Huang", "Jiayan Guo", "Xin Li", "Yibing Song", "Hao Luo", "Fan Wang", "Deli Zhao", "Hao Chen"], "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.", "comment": "Code: https://github.com/alibaba-damo-academy/WorldVLA", "pdf_url": "http://arxiv.org/pdf/2506.21539v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.21539v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "WorldVLA：迈向自回归动作世界模型", "tldr": "WorldVLA是一个统一动作和图像理解与生成的自回归动作世界模型，它将VLA模型和世界模型集成在一个框架中，并通过注意力掩码策略解决了自回归动作生成中的性能下降问题。", "motivation": "现有动作模型和世界模型是独立的，缺乏统一的框架来同时进行动作和图像的理解与生成。此外，自回归动作生成存在性能下降和误差传播的问题。", "method": "提出了WorldVLA，一个自回归动作世界模型，它将视觉-语言-动作（VLA）模型和世界模型集成在一个单一框架中。世界模型通过利用动作和图像理解来预测未来图像，以学习环境的潜在物理特性，从而改进动作生成。动作模型根据图像观察生成后续动作，辅助视觉理解并反过来帮助世界模型的视觉生成。为了解决自回归动作序列生成中性能下降的问题，提出了一种注意力掩码策略，在生成当前动作时选择性地掩盖先前的动作。", "result": "WorldVLA优于独立的动作模型和世界模型，突出了世界模型和动作模型之间的相互增强。同时发现自回归生成动作序列时动作模型性能会下降，这归因于模型对动作预测的泛化能力有限导致误差传播。所提出的注意力掩码策略在动作块生成任务中显示出显著的性能改进。", "conclusion": "WorldVLA成功地统一了动作和图像的理解与生成，并通过相互增强实现了优于独立模型的性能。所提出的注意力掩码策略有效地解决了自回归动作生成中的性能下降问题。", "translation": "我们提出了WorldVLA，一个统一动作和图像理解与生成的自回归动作世界模型。我们的WorldVLA将视觉-语言-动作（VLA）模型和世界模型集成在一个单一框架中。世界模型通过利用动作和图像理解来预测未来图像，目的是学习环境的潜在物理特性以改进动作生成。同时，动作模型根据图像观察生成后续动作，辅助视觉理解并反过来帮助世界模型的视觉生成。我们证明WorldVLA优于独立的动作模型和世界模型，突出了世界模型和动作模型之间的相互增强。此外，我们发现当以自回归方式生成动作序列时，动作模型的性能会下降。这种现象可归因于模型对动作预测的泛化能力有限，导致误差从早期动作传播到后续动作。为了解决这个问题，我们提出了一种注意力掩码策略，在生成当前动作时选择性地掩盖先前的动作，这在动作块生成任务中显示出显著的性能改进。", "summary": "WorldVLA是一个创新的自回归动作世界模型，它首次将视觉-语言-动作（VLA）模型与世界模型整合，实现了动作与图像的统一理解和生成。该模型通过世界模型预测未来图像以学习环境物理，并由动作模型生成动作以辅助视觉理解。研究表明WorldVLA性能优于独立模型，并揭示了自回归动作生成中存在的误差传播问题。为解决此问题，论文提出了一种注意力掩码策略，有效提升了动作块生成任务的性能。", "keywords": "自回归世界模型, 动作生成, 图像理解, VLA模型, 注意力掩码", "comments": "WorldVLA的创新之处在于其统一的VLA和世界模型框架，实现了动作与图像理解和生成的双向增强。它不仅提出了一个更全面的智能体模型，还识别并成功解决了自回归动作生成中的一个关键挑战——误差传播问题。注意力掩码策略的提出对于提高长序列动作生成的鲁棒性具有重要意义。该研究为未来构建更通用、更强大的具身智能体提供了新的方向。"}}
{"id": "2506.20960", "title": "OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs", "authors": ["Yiman Zhang", "Ziheng Luo", "Qiangyu Yan", "Wei He", "Borui Jiang", "Xinghao Chen", "Kai Han"], "summary": "In this paper, we introduce OmniEval, a benchmark for evaluating\nomni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,\nand textual inputs. Compared with existing benchmarks, our OmniEval has several\ndistinctive features: (i) Full-modal collaboration: We design evaluation tasks\nthat highlight the strong coupling between audio and video, requiring models to\neffectively leverage the collaborative perception of all modalities; (ii)\nDiversity of videos: OmniEval includes 810 audio-visual synchronized videos,\n285 Chinese videos and 525 English videos; (iii) Diversity and granularity of\ntasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended\nquestions and 1205 multiple-choice questions. These questions are divided into\n3 major task types and 12 sub-task types to achieve comprehensive evaluation.\nAmong them, we introduce a more granular video localization task named\nGrounding. Then we conduct experiments on OmniEval with several omni-modality\nmodels. We hope that our OmniEval can provide a platform for evaluating the\nability to construct and understand coherence from the context of all\nmodalities. Codes and data could be found at https://omnieval.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20960v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20960v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "OmniEval：一个评估视觉、听觉和文本输入的全模态模型的基准", "tldr": "OmniEval是一个新的基准测试，用于评估能处理视觉、听觉和文本输入的全模态模型，其特点是全模态协作、视频多样性和任务多样性。", "motivation": "为了解决现有基准在评估包含视觉、听觉和文本输入的全模态模型方面的不足，并提供一个评估模型从所有模态上下文中构建和理解连贯性能力的平台。", "method": "本文引入了OmniEval，一个用于评估全模态模型的基准。其特点包括：设计了强调音频和视频之间强耦合的全模态协作评估任务；包含810个音视频同步视频（285个中文，525个英文）；拥有2617个问答对（1412个开放式，1205个选择题），分为3种主要任务类型和12种子任务类型，并引入了更细粒度的视频定位任务Grounding。研究者已使用多个全模态模型在OmniEval上进行了实验。", "result": "OmniEval基准包含810个音视频同步视频（285个中文，525个英文）。它提供了2617个问答对，包括1412个开放式问题和1205个选择题，这些问题被分为3种主要任务类型和12种子任务类型。此外，引入了一个名为Grounding的更细粒度的视频定位任务。研究者已使用多个全模态模型在该基准上进行了实验。", "conclusion": "作者希望OmniEval能够为评估模型从所有模态上下文中构建和理解连贯性的能力提供一个平台。", "translation": "本文介绍了 OmniEval，一个用于评估 MiniCPM-O 2.6 等全模态模型的基准，该基准包含视觉、听觉和文本输入。与现有基准相比，我们的 OmniEval 具有几个显著特点：(i) 全模态协作：我们设计了突出音频和视频之间强耦合的评估任务，要求模型有效利用所有模态的协作感知；(ii) 视频多样性：OmniEval 包含 810 个音视频同步视频，其中 285 个中文视频和 525 个英文视频；(iii) 任务多样性和粒度：OmniEval 包含 2617 个问答对，其中包括 1412 个开放式问题和 1205 个选择题。这些问题分为 3 种主要任务类型和 12 种子任务类型，以实现全面评估。其中，我们引入了一个更细粒度的视频定位任务，名为 Grounding。然后，我们使用几个全模态模型在 OmniEval 上进行了实验。我们希望 OmniEval 能够为评估从所有模态上下文中构建和理解连贯性的能力提供一个平台。代码和数据可在 https://omnieval.github.io/ 上找到。", "summary": "本文介绍了OmniEval，一个用于评估全模态模型（如MiniCPM-O 2.6）的新型基准测试。该基准整合了视觉、听觉和文本输入，并具有全模态协作、多样化视频（包含中英文）以及细粒度任务类型（包括开放式和选择题，以及新引入的Grounding任务）等特点。研究者已使用多个全模态模型进行了实验，并期望OmniEval能成为评估模型理解多模态信息连贯性的重要平台。", "keywords": "全模态模型, 基准测试, 视觉, 听觉, 文本", "comments": "OmniEval的创新之处在于其对全模态协作的强调和细致的任务设计，特别是引入了Grounding任务，这对于评估复杂的多模态理解能力至关重要。其包含中英文视频的多样性也增加了其泛化能力和实用性。该基准的发布对于推动全模态AI模型的发展具有重要意义。"}}
{"id": "2506.21455", "title": "An Iterative Methodology for Unitary Quantum Channel Search", "authors": ["Matthew M. Lin", "Hao-Wei Huang", "Bing-Ze Lu"], "summary": "In this paper, we propose an iterative algorithm using polar decomposition to\napproximate a channel characterized by a single unitary matrix based on\ninput-output quantum state pairs. In limited data, we state and prove that the\noptimal solution obtained from our method using one pair with a specific\nstructure will generate an equivalent class, significantly reducing the\ndimension of the searching space. Furthermore, we prove that the unitary\nmatrices describing the same channel differ by a complex number with modulus 1.\nWe rigorously prove our proposed algorithm can ultimately identify a critical\npoint, which is also a local minimum of the established objective function.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21455v1", "categories": ["math.NA", "cs.NA", "quant-ph"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.21455v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "单一量子信道搜索的迭代方法", "tldr": "本文提出了一种使用极分解的迭代算法，用于根据输入-输出量子态对近似单一量子信道。该方法在有限数据下能显著减少搜索空间维度，并能识别目标函数的局部最小值。", "motivation": "本文旨在根据输入-输出量子态对来近似由单一酉矩阵表征的量子信道。", "method": "本文提出了一种使用极分解的迭代算法来近似量子信道。该方法在有限数据下，通过特定结构的一对量子态，其最优解能生成一个等价类，从而显著减少搜索空间维度。", "result": "1. 在有限数据下，使用特定结构的一对量子态，该方法获得的最优解将生成一个等价类，显著减少了搜索空间的维度。2. 描述相同信道的酉矩阵仅相差一个模为1的复数。3. 所提出的算法最终可以识别一个临界点，该临界点也是已建立目标函数的局部最小值。", "conclusion": "本文严格证明了所提出的算法最终可以识别一个临界点，该临界点也是已建立目标函数的局部最小值。", "translation": "在本文中，我们提出了一种使用极分解的迭代算法，用于根据输入-输出量子态对来近似由单一酉矩阵表征的信道。在有限数据下，我们陈述并证明了我们的方法使用具有特定结构的一对量子态获得的最佳解决方案将生成一个等价类，从而显著减少搜索空间的维度。此外，我们证明了描述相同信道的酉矩阵仅相差一个模为1的复数。我们严格证明了我们提出的算法最终可以识别一个临界点，该临界点也是已建立目标函数的局部最小值。", "summary": "本文提出了一种基于极分解的迭代算法，旨在通过输入-输出量子态对来近似单一酉量子信道。研究表明，在数据受限的情况下，利用特定结构的单个量子态对，该方法能产生一个等价类，有效降低搜索空间维度。此外，论文证明了描述同一信道的酉矩阵仅相差一个模为1的复数。算法被严格证明能够收敛到一个临界点，该点同时也是所构建目标函数的局部最小值。", "keywords": "酉量子信道, 迭代算法, 极分解, 搜索空间减少, 量子态对", "comments": "该论文提出了一种新颖的迭代方法，利用极分解来解决酉量子信道的表征问题。其创新点在于通过减少搜索空间维度来优化在有限数据下的信道近似，这对于实际应用具有重要意义。此外，对算法收敛性到局部最小值的严格证明增加了其理论可靠性。"}}
{"id": "2506.21170", "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21170v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21170v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "文本扩散建模的压缩平滑潜在空间", "tldr": "Cosmos引入了一种基于压缩平滑潜在空间的文本扩散模型，解决了传统自回归模型的速度和连贯性问题以及令牌级扩散模型的高维度挑战，实现了更快的推理和更高的生成质量。", "motivation": "自回归语言模型解码速度慢且难以保持全局连贯性；扩散模型虽有潜力，但其在文本生成中的应用受限于令牌级表示的高维度。", "method": "Cosmos在专为扩散定制的压缩、平滑潜在空间中进行文本生成。该空间通过一个自编码器学习，该自编码器同时进行令牌级重建和与预训练语言编码器冻结激活的对齐训练，并利用基于扰动的增强。", "result": "文本表示可被压缩8倍，同时保持与令牌级扩散模型相当的生成质量。增加潜在序列长度可使Cosmos超越基于扩散和自回归的基线模型。在故事生成、问题生成、摘要和去毒化四项任务上，Cosmos实现了可比或更优的生成质量，并提供超过2倍的推理速度提升。", "conclusion": "Cosmos通过在压缩平滑潜在空间中操作，为文本扩散建模提供了一种新颖且高效的方法，解决了现有模型的局限性，实现了更快的推理速度和更高的生成质量。", "translation": "自回归语言模型主导着现代文本生成，但其顺序性引入了根本性限制：解码速度慢，并且难以保持全局连贯性。扩散模型通过实现并行生成和灵活控制，提供了一种有前景的替代方案；然而，它们在文本生成中的应用受到令牌级表示高维度的阻碍。我们引入了Cosmos，这是一种新颖的文本生成方法，它完全在专为扩散定制的压缩、平滑潜在空间中运行。这个空间是通过一个自编码器学习的，该自编码器同时进行令牌级重建和与预训练语言编码器冻结激活的对齐训练，从而提供强大的语义基础并实现有效的基于扰动的增强。经验表明，文本表示可以被压缩8倍，同时保持与令牌级扩散模型相当的生成质量。此外，增加潜在序列长度使Cosmos能够超越基于扩散和自回归的基线模型。我们在包括故事生成、问题生成、摘要和去毒化在内的四项不同的生成任务上评估了Cosmos，并将其与各种生成范式进行了比较。Cosmos实现了可比或更优的生成质量，同时推理速度提高了2倍以上。", "summary": "本文介绍了Cosmos，一种新颖的文本生成方法，它利用压缩平滑的潜在空间进行扩散建模。该空间通过专门训练的自编码器学习，解决了文本扩散中高维度令牌表示的限制以及自回归模型的缓慢问题。Cosmos实现了显著的文本压缩（8倍），同时在多种任务（故事生成、问题生成、摘要、去毒化）中保持或提高了生成质量，并且推理速度比现有扩散和自回归基线模型快2倍以上。", "keywords": "文本扩散, 潜在空间, 文本生成, 自编码器, Cosmos", "comments": "Cosmos通过在学习到的压缩潜在空间中操作，创新性地解决了文本扩散面临的高维度问题。其在保持或提升生成质量的同时实现显著压缩和更快的推理速度，使其成为一项重要的进展，有望为更高效和可控的文本生成模型铺平道路。"}}
{"id": "2506.20964", "title": "Evidence-based diagnostic reasoning with multi-agent copilot for human pathology", "authors": ["Chengkuan Chen", "Luca L. Weishaupt", "Drew F. K. Williamson", "Richard J. Chen", "Tong Ding", "Bowen Chen", "Anurag Vaidya", "Long Phi Le", "Guillaume Jaume", "Ming Y. Lu", "Faisal Mahmood"], "summary": "Pathology is experiencing rapid digital transformation driven by whole-slide\nimaging and artificial intelligence (AI). While deep learning-based\ncomputational pathology has achieved notable success, traditional models\nprimarily focus on image analysis without integrating natural language\ninstruction or rich, text-based context. Current multimodal large language\nmodels (MLLMs) in computational pathology face limitations, including\ninsufficient training data, inadequate support and evaluation for multi-image\nunderstanding, and a lack of autonomous, diagnostic reasoning capabilities. To\naddress these limitations, we introduce PathChat+, a new MLLM specifically\ndesigned for human pathology, trained on over 1 million diverse,\npathology-specific instruction samples and nearly 5.5 million question answer\nturns. Extensive evaluations across diverse pathology benchmarks demonstrated\nthat PathChat+ substantially outperforms the prior PathChat copilot, as well as\nboth state-of-the-art (SOTA) general-purpose and other pathology-specific\nmodels. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI\nsystem leveraging PathChat+ to autonomously evaluate gigapixel whole-slide\nimages (WSIs) through iterative, hierarchical diagnostic reasoning, reaching\nhigh accuracy on DDxBench, a challenging open-ended differential diagnosis\nbenchmark, while also capable of generating visually grounded,\nhumanly-interpretable summary reports.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20964v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20964v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "结合多智能体副驾驶的人类病理学循证诊断推理", "tldr": "本文介绍了PathChat+，一种专为人类病理学设计的多模态大型语言模型（MLLM），以及SlideSeek，一个利用PathChat+进行全玻片图像（WSI）自主诊断推理的多智能体AI系统。PathChat+在病理学基准测试中表现出色，而SlideSeek在鉴别诊断方面达到了高精度并能生成可解释的报告。", "motivation": "传统计算病理学模型主要关注图像分析，缺乏自然语言指令和丰富的文本上下文整合。当前计算病理学中的多模态大型语言模型（MLLM）面临训练数据不足、多图像理解支持和评估不足，以及缺乏自主诊断推理能力的局限性。", "method": "本文引入了PathChat+，一种专为人类病理学设计的新型多模态大型语言模型（MLLM），其训练数据超过100万个多样化的病理学特定指令样本和近550万个问答回合。此外，还提出了SlideSeek，一个支持推理的多智能体AI系统，该系统利用PathChat+通过迭代、分层的诊断推理自主评估千兆像素全玻片图像（WSI）。", "result": "PathChat+在各种病理学基准上显著优于先前的PathChat副驾驶，以及最先进（SOTA）的通用模型和其他病理学专用模型。SlideSeek在具有挑战性的开放式鉴别诊断基准DDxBench上达到了高精度，同时还能生成视觉上接地、人类可解释的摘要报告。", "conclusion": "通过开发PathChat+和SlideSeek，本文有效地解决了当前计算病理学模型在整合丰富上下文理解和自主诊断推理方面的局限性，从而提高了诊断准确性和可解释性。", "translation": "病理学正经历由全玻片成像和人工智能（AI）驱动的快速数字化转型。虽然基于深度学习的计算病理学取得了显著成功，但传统模型主要侧重于图像分析，而没有整合自然语言指令或丰富的文本上下文。当前计算病理学中的多模态大型语言模型（MLLM）面临局限性，包括训练数据不足、对多图像理解的支持和评估不足，以及缺乏自主诊断推理能力。为了解决这些局限性，我们引入了PathChat+，这是一种专为人类病理学设计的新型MLLM，其训练数据超过100万个多样化的病理学特定指令样本和近550万个问答回合。在各种病理学基准上的广泛评估表明，PathChat+显著优于先前的PathChat副驾驶，以及最先进（SOTA）的通用模型和其他病理学专用模型。此外，我们提出了SlideSeek，这是一种支持推理的多智能体AI系统，利用PathChat+通过迭代、分层的诊断推理自主评估千兆像素全玻片图像（WSI），在DDxBench（一个具有挑战性的开放式鉴别诊断基准）上达到了高精度，同时还能生成视觉上接地、人类可解释的摘要报告。", "summary": "本文提出PathChat+，一种专为人类病理学设计的新型多模态大型语言模型（MLLM），通过大规模病理学特定数据训练，以克服现有模型在整合自然语言和多图像理解方面的不足。同时，文章还介绍了SlideSeek，一个利用PathChat+进行全玻片图像自主、迭代诊断推理的多智能体AI系统。实验证明，PathChat+在诊断性能上显著优于现有模型，而SlideSeek在鉴别诊断上表现出高精度并能生成可解释的报告。", "keywords": "计算病理学, 多智能体AI, 大型语言模型, 诊断推理, 全玻片成像", "comments": "本文通过开发专门的MLLM（PathChat+）和智能多智能体系统（SlideSeek），在计算病理学领域取得了显著进展。将丰富的文本上下文和千兆像素图像上的迭代推理相结合，解决了以往方法的关键局限性，为更准确、更具人类可解释性的AI辅助病理诊断提供了有前景的方向。大规模、病理学特定的训练数据是其一个重要的创新点。"}}
{"id": "2506.20967", "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing", "authors": ["Lingling Cai", "Kang Zhao", "Hangjie Yuan", "Xiang Wang", "Yingya Zhang", "Kejie Huang"], "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in\nvideo generation. However, directly applying existing video editing methods to\nVideo DiTs often incurs substantial computational overhead, due to\nresource-intensive attention modification or finetuning. To alleviate this\nproblem, we present DFVEdit, an efficient zero-shot video editing method\ntailored for Video DiTs. DFVEdit eliminates the need for both attention\nmodification and fine-tuning by directly operating on clean latents via flow\ntransformation. To be more specific, we observe that editing and sampling can\nbe unified under the continuous flow perspective. Building upon this\nfoundation, we propose the Conditional Delta Flow Vector (CDFV) -- a\ntheoretically unbiased estimation of DFV -- and integrate Implicit Cross\nAttention (ICA) guidance as well as Embedding Reinforcement (ER) to further\nenhance editing quality. DFVEdit excels in practical efficiency, offering at\nleast 20x inference speed-up and 85\\% memory reduction on Video DiTs compared\nto attention-engineering-based editing methods. Extensive quantitative and\nqualitative experiments demonstrate that DFVEdit can be seamlessly applied to\npopular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art\nperformance on structural fidelity, spatial-temporal consistency, and editing\nquality.", "comment": "Zero-shot video editing", "pdf_url": "http://arxiv.org/pdf/2506.20967v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20967v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DFVEdit: 条件增量流向量用于零样本视频编辑", "tldr": "DFVEdit是一种高效的零样本视频编辑方法，通过流变换直接操作潜在空间，避免了对Video DiTs进行注意力修改或微调，显著提高了速度和效率。", "motivation": "现有视频编辑方法直接应用于Video DiTs时，由于资源密集型的注意力修改或微调，会导致巨大的计算开销。", "method": "提出了DFVEdit，一种高效的零样本视频编辑方法，专为Video DiTs设计。它通过流变换直接作用于干净的潜在空间，消除了注意力修改和微调的需要。具体来说，将编辑和采样统一在连续流视角下，提出了条件增量流向量（CDFV），并集成了隐式交叉注意力（ICA）指导和嵌入强化（ER）来增强编辑质量。", "result": "DFVEdit在实用效率方面表现出色，与基于注意力工程的编辑方法相比，在Video DiTs上实现了至少20倍的推理速度提升和85%的内存减少。广泛的定量和定性实验表明，DFVEdit可以无缝应用于流行的Video DiTs（例如CogVideoX和Wan2.1），并在结构保真度、时空一致性和编辑质量方面取得了最先进的性能。", "conclusion": "DFVEdit提供了一种高效、高性能的零样本视频编辑解决方案，克服了现有方法在Video DiTs上应用的计算障碍，并展示了优越的编辑质量和效率。", "translation": "视频扩散Transformer（Video DiTs）的出现标志着视频生成领域的一个里程碑。然而，将现有视频编辑方法直接应用于Video DiTs通常会产生巨大的计算开销，原因在于资源密集型的注意力修改或微调。为了缓解这个问题，我们提出了DFVEdit，一种专为Video DiTs量身定制的高效零样本视频编辑方法。DFVEdit通过流变换直接操作干净的潜在空间，从而消除了对注意力修改和微调的需求。更具体地说，我们观察到编辑和采样可以在连续流的视角下统一起来。在此基础上，我们提出了条件增量流向量（CDFV）——一种理论上无偏的DFV估计——并整合了隐式交叉注意力（ICA）指导以及嵌入强化（ER）以进一步提升编辑质量。DFVEdit在实际效率方面表现出色，与基于注意力工程的编辑方法相比，在Video DiTs上实现了至少20倍的推理速度提升和85%的内存减少。广泛的定量和定性实验表明，DFVEdit可以无缝应用于流行的Video DiTs（例如CogVideoX和Wan2.1），在结构保真度、时空一致性和编辑质量方面取得了最先进的性能。", "summary": "DFVEdit是一种高效的零样本视频编辑方法，专为Video DiTs设计，旨在解决现有方法计算开销大的问题。它通过流变换直接在潜在空间操作，引入了条件增量流向量（CDFV）、隐式交叉注意力（ICA）和嵌入强化（ER），从而避免了耗时的注意力修改和微调。实验证明，DFVEdit显著提高了推理速度和内存效率，并在结构保真度、时空一致性、编辑质量方面达到了最先进的水平。", "keywords": "视频编辑, 零样本, 视频扩散Transformer, 流向量, 计算效率", "comments": "DFVEdit的创新之处在于其通过流变换直接在潜在空间进行视频编辑，避免了对Video DiTs进行昂贵的注意力修改或微调，显著提升了效率。其提出的CDFV以及ICA和ER的结合，为实现高质量、高效率的零样本视频编辑提供了新范式，对于推动视频生成模型的实际应用具有重要意义。"}}
{"id": "2506.20886", "title": "Omniwise: Predicting GPU Kernels Performance with LLMs", "authors": ["Zixian Wang", "Cole Ramos", "Muhammad A. Awad", "Keith Lowery"], "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20886v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20886v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "Omniwise：使用大型语言模型预测 GPU 内核性能", "tldr": "Omniwise 是一种新颖的端到端自监督微调管道，利用大型语言模型（LLMs）无需执行或分析即可预测 GPU 内核性能，在 AMD GPU 上实现了高准确度，并提供了集成工具。", "motivation": "深度神经网络（DNNs）的快速发展推动了对 GPU 性能预测的需求，而传统的代码执行或性能分析工具存在局限性。", "method": "本文引入了 Omniwise，这是一个端到端、自监督的微调管道，首次将大型语言模型（LLMs）应用于 GPU 内核性能预测。该方法模型无关且轻量级，能够直接从内核代码预测关键性能指标，无需代码执行或使用分析工具。", "result": "Omniwise 在 AMD MI250 和 MI300X 架构上执行的 GPU 内核上，实现了超过 90% 的预测在 10% 相对误差范围内的准确度。它可以预测内存带宽、缓存命中率、GFLOPs 和算术强度等性能指标。此外，还开发了在线推理服务器和 Visual Studio Code 插件，以集成到开发人员的工作流程中。", "conclusion": "Omniwise 成功地将大型语言模型应用于 GPU 内核性能预测，提供了一种高效、准确且无需代码执行的解决方案，并为开发人员提供了便捷的集成工具。", "translation": "近年来，深度神经网络（DNNs）的快速发展彻底改变了人工智能，使模型在理解、生成和处理复杂数据方面拥有前所未有的能力。这些强大的架构已经改变了广泛的下游应用，解决了超越人类能力的任务。在本文中，我们介绍了 Omniwise，这是第一个端到端、自监督的微调管道，将大型语言模型（LLMs）应用于 GPU 内核性能预测——这是性能分析中的一个新颖用例。Omniwise 是模型无关且轻量级的，即使使用一个小型 3B 参数模型也能取得显著效果。它可以直接从内核代码预测关键性能指标，包括内存带宽、缓存命中率、GFLOPs 和算术强度，而无需代码执行或分析工具。我们的方法在 AMD MI250 和 MI300X 架构上执行的 GPU 内核上，超过 90% 的预测在 10% 的相对误差范围内。除了管道之外，我们还开发了一个在线推理服务器和一个 Visual Studio Code 插件，将基于 LLM 的性能预测无缝集成到开发人员的工作流程中。", "summary": "Omniwise 提出了一个创新的端到端自监督微调管道，首次将大型语言模型（LLMs）应用于 GPU 内核性能预测。该系统无需实际执行或使用分析工具，即可直接从内核代码预测内存带宽、缓存命中率等关键性能指标。Omniwise 模型无关且轻量，即使是小型模型也能在 AMD GPU 上实现超过 90% 的预测在 10% 相对误差内的准确度。此外，该研究还开发了在线推理服务器和 VS Code 插件，便于开发者集成使用。", "keywords": "GPU 性能预测, 大型语言模型, 自监督学习, 性能分析, Omniwise", "comments": "Omniwise 的创新之处在于将 LLM 应用于 GPU 性能预测这一新颖领域，并实现了无需代码执行的预测，显著提高了效率。其模型无关和轻量级的特点也增加了实用性。集成到开发工作流的工具进一步提升了其潜在影响力。"}}
{"id": "2506.21199", "title": "MedPrompt: LLM-CNN Fusion with Weight Routing for Medical Image Segmentation and Classification", "authors": ["Shadman Sobhan", "Kazi Abrar Mahmud", "Abduz Zami"], "summary": "Current medical image analysis systems are typically task-specific, requiring\nseparate models for classification and segmentation, and lack the flexibility\nto support user-defined workflows. To address these challenges, we introduce\nMedPrompt, a unified framework that combines a few-shot prompted Large Language\nModel (Llama-4-17B) for high-level task planning with a modular Convolutional\nNeural Network (DeepFusionLab) for low-level image processing. The LLM\ninterprets user instructions and generates structured output to dynamically\nroute task-specific pretrained weights. This weight routing approach avoids\nretraining the entire framework when adding new tasks-only task-specific\nweights are required, enhancing scalability and deployment. We evaluated\nMedPrompt across 19 public datasets, covering 12 tasks spanning 5 imaging\nmodalities. The system achieves a 97% end-to-end correctness in interpreting\nand executing prompt-driven instructions, with an average inference latency of\n2.5 seconds, making it suitable for near real-time applications. DeepFusionLab\nachieves competitive segmentation accuracy (e.g., Dice 0.9856 on lungs) and\nstrong classification performance (F1 0.9744 on tuberculosis). Overall,\nMedPrompt enables scalable, prompt-driven medical imaging by combining the\ninterpretability of LLMs with the efficiency of modular CNNs.", "comment": "40 pages, 8 Tables, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2506.21199v1", "categories": ["cs.CV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21199v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MedPrompt：用于医学图像分割和分类的LLM-CNN权重路由融合", "tldr": "MedPrompt是一个统一框架，通过LLM和模块化CNN的融合及权重路由，实现了可扩展、提示驱动的医学图像分割和分类。", "motivation": "当前的医学图像分析系统通常是任务特定的，需要独立的模型进行分类和分割，并且缺乏支持用户定义工作流的灵活性。", "method": "MedPrompt是一个统一框架，结合了用于高级任务规划的少量样本提示式大型语言模型（Llama-4-17B）和用于低级图像处理的模块化卷积神经网络（DeepFusionLab）。LLM解释用户指令并生成结构化输出，以动态路由特定任务的预训练权重。这种权重路由方法避免了在添加新任务时重新训练整个框架，只需特定任务的权重，从而增强了可扩展性和部署能力。", "result": "MedPrompt在19个公共数据集、涵盖12项任务和5种成像模式上进行了评估。该系统在解释和执行提示驱动指令方面实现了97%的端到端正确性，平均推理延迟为2.5秒，适用于近实时应用。DeepFusionLab实现了具有竞争力的分割精度（例如，肺部Dice系数0.9856）和强大的分类性能（肺结核F1分数0.9744）。", "conclusion": "MedPrompt通过结合大型语言模型的可解释性与模块化卷积神经网络的效率，实现了可扩展、提示驱动的医学成像。", "translation": "当前的医学图像分析系统通常是任务特定的，需要独立的模型进行分类和分割，并且缺乏支持用户定义工作流的灵活性。为了解决这些挑战，我们引入了MedPrompt，一个统一的框架，它结合了用于高级任务规划的少量样本提示式大型语言模型（Llama-4-17B）与用于低级图像处理的模块化卷积神经网络（DeepFusionLab）。LLM解释用户指令并生成结构化输出，以动态路由特定任务的预训练权重。这种权重路由方法避免了在添加新任务时重新训练整个框架——只需特定任务的权重，从而增强了可扩展性和部署能力。我们在19个公共数据集上评估了MedPrompt，涵盖了5种成像模式的12项任务。该系统在解释和执行提示驱动指令方面实现了97%的端到端正确性，平均推理延迟为2.5秒，使其适用于近实时应用。DeepFusionLab实现了具有竞争力的分割精度（例如，肺部Dice系数0.9856）和强大的分类性能（肺结核F1分数0.9744）。总的来说，MedPrompt通过结合大型语言模型的可解释性与模块化卷积神经网络的效率，实现了可扩展、提示驱动的医学成像。", "summary": "MedPrompt是一个创新的统一框架，旨在解决当前医学图像分析系统任务特异性和缺乏灵活性的问题。它将Llama-4-17B LLM用于高级任务规划和指令解释，并与模块化DeepFusionLab CNN结合进行低级图像处理。通过独特的权重路由机制，MedPrompt无需对整个框架进行重新训练即可添加新任务，显著提高了可扩展性。该系统在多任务、多模态数据集上表现出色，实现了高指令执行正确性、低推理延迟，并在分割和分类任务上取得了竞争性结果，为可扩展、提示驱动的医学成像提供了解决方案。", "keywords": "MedPrompt, LLM, CNN, 医学图像分割, 分类, 权重路由", "comments": "该论文提出了一个将LLM和CNN融合的创新框架，通过权重路由实现了医学图像分析的统一和可扩展性，解决了传统系统任务特异性的痛点。其核心创新在于利用LLM进行高级任务规划和动态权重路由，这对于未来开发更灵活、适应性更强的AI医疗系统具有重要意义。在实际应用中，其低推理延迟也使其适用于近实时场景。"}}
{"id": "2506.21124", "title": "Quantum Adaptive Search: A Hybrid Quantum-Classical Algorithm for Global Optimization of Multivariate Functions", "authors": ["G. Intoccia", "U. Chirico", "V. Schiano Di Cola", "G. Pepe", "S. Cuomo"], "summary": "This work presents Quantum Adaptive Search (QAGS), a hybrid quantum-classical\nalgorithm for the global optimization of multivariate functions. The method\nemploys an adaptive mechanism that dynamically narrows the search space based\non a quantum-estimated probability distribution of the objective function. A\nquantum state encodes information about solution quality through an appropriate\ncomplex amplitude mapping, enabling the identification of the most promising\nregions, and thus progressively tightening the search bounds; then a classical\noptimizer performs local refinement of the solution. The analysis demonstrates\nthat QAGS ensures a contraction of the search space toward global optima, with\ncontrolled computational complexity. The numerical results on the benchmark\nfunctions show that, compared to the classical methods, QAGS achieves higher\naccuracy while offering advantages in both time and space complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21124v1", "categories": ["quant-ph", "cs.NA", "math.NA", "math.OC"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.21124v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "量子自适应搜索：一种用于多元函数全局优化的混合量子-经典算法", "tldr": "QAGS是一种混合量子-经典算法，通过量子估计和自适应机制动态缩小搜索空间，实现多元函数的全局优化，相比经典方法具有更高的精度和更好的时间/空间复杂度。", "motivation": "该研究旨在提出一种新的混合量子-经典算法，用于解决多元函数的全局优化问题，以期克服现有方法的局限性并提高优化效率和精度。", "method": "QAGS是一种混合量子-经典算法。它采用自适应机制，根据目标函数的量子估计概率分布动态缩小搜索空间。量子态通过复振幅映射编码解的质量信息，以识别最有前景的区域，从而逐步收紧搜索边界。随后，经典的优化器对解决方案进行局部细化。", "result": "分析表明，QAGS能够确保搜索空间向全局最优收缩，并具有可控的计算复杂度。对基准函数的数值结果显示，与经典方法相比，QAGS实现了更高的精度，并在时间复杂度与空间复杂度方面均具有优势。", "conclusion": "QAGS作为一种混合量子-经典算法，通过其独特的自适应搜索机制，为多元函数的全局优化提供了一种高效且高精度的解决方案，并在计算资源方面展现出优越性。", "translation": "这项工作提出了量子自适应搜索（QAGS），一种用于多元函数全局优化的混合量子-经典算法。该方法采用自适应机制，根据目标函数的量子估计概率分布动态缩小搜索空间。量子态通过适当的复振幅映射编码解的质量信息，从而能够识别最有前景的区域，并逐步收紧搜索边界；然后，经典的优化器对解决方案进行局部细化。分析表明，QAGS确保了搜索空间向全局最优收缩，并具有可控的计算复杂度。对基准函数的数值结果显示，与经典方法相比，QAGS实现了更高的精度，并在时间复杂度与空间复杂度方面均具有优势。", "summary": "本文提出量子自适应搜索（QAGS），一种创新的混合量子-经典算法，专为多元函数的全局优化设计。该算法利用量子估计的概率分布，通过自适应机制动态缩小搜索空间，并通过量子态编码解决方案质量以识别最佳区域。随后，经典优化器进行局部精修。研究证明QAGS能有效收敛至全局最优，并具有可控的计算复杂度。数值实验表明，QAGS在精度、时间复杂度及空间复杂度上均优于传统经典优化方法。", "keywords": "量子自适应搜索, 全局优化, 混合算法, 量子计算, 多元函数", "comments": "QAGS的创新之处在于结合了量子计算的潜力与经典优化的优势，特别是其自适应机制和量子态编码信息的能力，使得搜索空间能够高效收缩。这对于解决复杂的全局优化问题具有重要意义，尤其是在处理高维或非凸函数时，有望提供比纯经典方法更优的性能。"}}
{"id": "2506.20977", "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging", "authors": ["Tao Liu", "Dafeng Zhang", "Gengchen Li", "Shizhuo Liu", "Yongqi Song", "Senmao Li", "Shiqi Yang", "Boqian Li", "Kai Wang", "Yaxing Wang"], "summary": "Face aging has become a crucial task in computer vision, with applications\nranging from entertainment to healthcare. However, existing methods struggle\nwith achieving a realistic and seamless transformation across the entire\nlifespan, especially when handling large age gaps or extreme head poses. The\ncore challenge lies in balancing age accuracy and identity preservation--what\nwe refer to as the Age-ID trade-off. Most prior methods either prioritize age\ntransformation at the expense of identity consistency or vice versa. In this\nwork, we address this issue by proposing a two-pass face aging framework, named\nCradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first\npass focuses on solving age accuracy by introducing an adaptive noise injection\n(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of\nage and gender for the given person as the textual condition. Also, by\nadjusting the noise level, we can control the strength of aging while allowing\nmore flexibility in transforming the face. However, identity preservation is\nweakly ensured here to facilitate stronger age transformations. In the second\npass, we enhance identity preservation while maintaining age-specific features\nby conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace\nand Rotate-CLIP. This pass allows for denoising the transformed image from the\nfirst pass, ensuring stronger identity preservation without compromising the\naging accuracy. Both passes are jointly trained in an end-to-end way. Extensive\nexperiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL\nprotocols, show that our Cradle2Cane outperforms existing face aging methods in\nage accuracy and identity consistency.", "comment": "30 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.20977v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20977v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "从摇篮到拐杖：一种用于高保真寿命面部老化的双通道框架", "tldr": "提出了一种名为 Cradle2Cane 的双通道面部老化框架，通过平衡年龄准确性和身份保持，实现了逼真且无缝的全寿命周期面部老化。", "motivation": "现有的面部老化方法在实现逼真和无缝的全寿命周期转换方面存在困难，尤其是在处理大年龄差距或极端头部姿势时。核心挑战在于平衡年龄准确性和身份保持，即“年龄-身份权衡”。", "method": "提出了一种名为 Cradle2Cane 的双通道面部老化框架，基于少步文本到图像（T2I）扩散模型。第一阶段通过自适应噪声注入（AdaNI）机制解决年龄准确性问题，该机制由年龄和性别提示描述引导。第二阶段通过条件化两个身份感知嵌入（SVR-ArcFace 和 Rotate-CLIP）来增强身份保持，同时保持年龄特定特征，对第一阶段的图像进行去噪。两个阶段联合进行端到端训练。", "result": "在 CelebA-HQ 测试数据集上，通过 Face++ 和 Qwen-VL 协议评估，Cradle2Cane 在年龄准确性和身份一致性方面优于现有面部老化方法。", "conclusion": "Cradle2Cane 框架通过其独特的双通道设计，有效解决了面部老化中的年龄准确性和身份保持之间的权衡问题，实现了卓越的性能。", "translation": "面部老化已成为计算机视觉中的一项关键任务，其应用范围从娱乐到医疗保健。然而，现有方法在实现整个生命周期内逼真和无缝的转换方面存在困难，尤其是在处理大年龄差距或极端头部姿势时。核心挑战在于平衡年龄准确性和身份保持——我们称之为年龄-身份权衡。大多数现有方法要么以牺牲身份一致性为代价优先进行年龄转换，要么反之。在这项工作中，我们通过提出一种名为 Cradle2Cane 的双通道面部老化框架来解决这个问题，该框架基于少步文本到图像（T2I）扩散模型。第一阶段通过引入自适应噪声注入（AdaNI）机制来解决年龄准确性问题。该机制通过将给定人物的年龄和性别提示描述作为文本条件来引导。此外，通过调整噪声水平，我们可以控制老化的强度，同时允许面部转换更灵活。然而，为了促进更强的年龄转换，此处对身份保持的保障较弱。在第二阶段，我们通过将模型条件化于两个身份感知嵌入（IDEmb）：SVR-ArcFace 和 Rotate-CLIP，来增强身份保持，同时保持年龄特定特征。此阶段允许对第一阶段转换后的图像进行去噪，确保更强的身份保持而不损害老化准确性。两个阶段都以端到端的方式联合训练。在 CelebA-HQ 测试数据集上，通过 Face++ 和 Qwen-VL 协议进行的广泛实验表明，我们的 Cradle2Cane 在年龄准确性和身份一致性方面优于现有面部老化方法。", "summary": "这篇论文提出了一种名为 Cradle2Cane 的双通道面部老化框架，旨在解决现有方法在全寿命周期面部老化中年龄准确性和身份保持之间的权衡问题。该框架利用文本到图像扩散模型，第一阶段侧重于通过自适应噪声注入实现年龄准确性，第二阶段则通过身份感知嵌入来增强身份保持。实验结果表明，该方法在年龄准确性和身份一致性方面均优于现有技术。", "keywords": "面部老化, 扩散模型, 身份保持, 年龄准确性, 双通道框架", "comments": "这篇论文的创新点在于提出了一个双通道框架来解耦并优化面部老化中的两个核心挑战：年龄准确性和身份保持。通过分阶段处理，第一阶段侧重于年龄转换的强度和灵活性，第二阶段则专门用于加强身份一致性，这种设计思路有效地解决了“年龄-身份权衡”问题。利用扩散模型和身份感知嵌入是其技术亮点，为高保真面部老化提供了新的解决方案。"}}
{"id": "2506.20893", "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20893v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20893v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "有效类别遗忘中输出分布重加权的必要性", "tldr": "本文提出了一种名为RWFT的轻量级输出重加权遗忘方法，用于从训练好的分类器中擦除整个类别，无需完全重新训练。RWFT在性能上与完全重新训练相当，并且对新的成员推断攻击（MIA-NN）具有鲁棒性，显著优于现有方法。", "motivation": "完全重新训练以实现类别遗忘成本高昂，且现有遗忘方法未能真正擦除类别，导致它们容易受到成员推断攻击（如本文提出的MIA-NN），从而无法有效执行用户删除权或减轻有害预测。", "method": "本文提出了一种名为RWFT的输出重加权遗忘方法，通过对遗忘类别样本的预测概率质量进行简单重新分配来实现。为证明现有方法的不足，作者设计了一种新的成员推断攻击MIA-NN。此外，还引入了一种基于预测概率总变异（TV）距离的新度量，用于量化残余泄漏并评估遗忘效果。", "result": "RWFT在先前用于评估的指标和本文提出的新TV-based指标上都与完全重新训练的结果相匹配。与最先进的方法相比，RWFT在先前使用的指标上提高了2.79%，在新的基于TV的指标上比现有最佳方法提高了111.45%。所提出的MIA-NN成功揭示了现有遗忘方法中已遗忘的类别。", "conclusion": "输出分布重加权对于有效的类别遗忘至关重要。RWFT的卓越性能和对新攻击的鲁棒性证明了其作为昂贵完全重新训练的可行替代方案。", "translation": "在这项工作中，我们引入了一种输出重加权遗忘方法RWFT，这是一种轻量级技术，可以在不进行完全重新训练的情况下从训练好的分类器中擦除整个类别。从训练好的模型中遗忘特定类别对于执行用户删除权和减轻有害或有偏见的预测至关重要。完全重新训练成本高昂，并且现有的遗忘方法在预测来自已遗忘类别的样本时无法复制重新训练模型的行为。我们通过设计一种成员推断攻击的变体MIA-NN来证明这种失败，MIA-NN成功地揭示了任何这些方法中已遗忘的类别。我们提出了一种简单的概率质量重新分配方法，用于预测已遗忘类别中的样本，该方法对MIA-NN具有鲁棒性。我们还引入了一种基于预测概率总变异（TV）距离的新度量，用于量化残余泄漏，以防止未来的方法容易受到新攻击的影响。通过对机器学习遗忘领域最先进基线的广泛实验，我们表明我们的方法在先前工作用于评估的指标和我们在这项工作中提出的新指标上都与完全重新训练的结果相匹配。与最先进的方法相比，我们在先前使用的指标上提高了2.79%，在我们新的基于TV的指标上比现有最佳方法提高了111.45%。", "summary": "本文提出了一种名为RWFT的输出重加权遗忘方法，旨在高效地从训练好的分类器中擦除特定类别，而无需进行耗时的完全重新训练。该方法解决了现有遗忘技术在面对新的成员推断攻击（MIA-NN）时的脆弱性问题，通过对遗忘类别的预测概率质量进行重新分配来实现鲁棒性。实验证明，RWFT在性能上可媲美完全重新训练，并且在现有评估指标和本文提出的基于总变异（TV）距离的新指标上均显著优于最先进的遗忘方法。", "keywords": "类别遗忘, 输出重加权, 成员推断攻击, 总变异距离, 机器学习遗忘", "comments": "本文的创新之处在于明确指出了输出分布重加权对于有效类别遗忘的必要性，并提出了一种鲁棒且轻量级的解决方案RWFT。同时，MIA-NN的引入和基于TV距离的新度量也是重要的贡献，它们推动了遗忘效果和安全评估的边界。这项工作对于数据隐私（用户删除权）和减轻模型偏见具有重要意义。"}}
{"id": "2506.20694", "title": "Evaluating PDE discovery methods for multiscale modeling of biological signals", "authors": ["Andréa Ducos", "Audrey Denizot", "Thomas Guyet", "Hugues Berry"], "summary": "Biological systems are non-linear, include unobserved variables and the\nphysical principles that govern their dynamics are partly unknown. This makes\nthe characterization of their behavior very challenging. Notably, their\nactivity occurs on multiple interdependent spatial and temporal scales that\nrequire linking mechanisms across scales. To address the challenge of bridging\ngaps between scales, we leverage partial differential equations (PDE)\ndiscovery. PDE discovery suggests meso-scale dynamics characteristics from\nmicro-scale data. In this article, we present our framework combining\nparticle-based simulations and PDE discovery and conduct preliminary\nexperiments to assess equation discovery in controlled settings. We evaluate\nfive state-of-the-art PDE discovery methods on particle-based simulations of\ncalcium diffusion in astrocytes. The performances of the methods are evaluated\non both the form of the discovered equation and the forecasted temporal\nvariations of calcium concentration. Our results show that several methods\naccurately recover the diffusion term, highlighting the potential of PDE\ndiscovery for capturing macroscopic dynamics in biological systems from\nmicroscopic data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20694v1", "categories": ["q-bio.QM", "cs.AI"], "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.20694v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "评估用于生物信号多尺度建模的偏微分方程发现方法", "tldr": "该论文评估了用于生物信号多尺度建模的偏微分方程（PDE）发现方法，特别是利用星形胶质细胞中钙扩散的粒子模拟进行验证。结果表明，一些方法能准确恢复扩散项，展示了PDE发现从微观数据中捕获宏观动力学的潜力。", "motivation": "生物系统是非线性的，包含未观测变量，且其动力学原理部分未知，活动发生在多个相互依赖的空间和时间尺度上，这使得其行为表征极具挑战性。为了解决跨尺度连接的难题，本文利用偏微分方程（PDE）发现来弥合尺度间的差距。", "method": "本文提出了一个结合粒子模拟和PDE发现的框架。在受控环境下，对星形胶质细胞中钙扩散的粒子模拟数据，评估了五种最先进的PDE发现方法。性能评估基于发现方程的形式和预测的钙浓度时间变化。", "result": "研究结果表明，有几种方法能够准确地恢复扩散项。", "conclusion": "PDE发现具有从微观数据中捕获生物系统宏观动力学的潜力。", "translation": "生物系统是非线性的，包含未观测变量，并且控制其动力学的物理原理部分未知。这使得表征它们的行为非常具有挑战性。值得注意的是，它们的活动发生在多个相互依赖的空间和时间尺度上，这需要跨尺度连接机制。为了解决弥合尺度之间差距的挑战，我们利用偏微分方程（PDE）发现。PDE发现从微观数据中推断出中观尺度的动力学特征。在本文中，我们提出了结合基于粒子的模拟和PDE发现的框架，并进行了初步实验以评估受控环境中的方程发现。我们评估了五种最先进的PDE发现方法在星形胶质细胞中钙扩散的基于粒子的模拟上的表现。这些方法的性能根据发现方程的形式和钙浓度预测的时间变化进行评估。我们的结果表明，几种方法准确地恢复了扩散项，突出了PDE发现从微观数据中捕获生物系统宏观动力学潜力。", "summary": "本文旨在解决复杂生物系统多尺度建模的挑战，利用偏微分方程（PDE）发现技术。研究提出了一个整合粒子模拟与PDE发现的框架，并评估了五种先进的PDE发现方法在星形胶质细胞钙扩散粒子模拟中的表现。评估标准包括方程形式的恢复准确性和钙浓度时间变化的预测能力。结果显示，多种方法能准确识别扩散项，表明PDE发现有望从微观数据推断生物系统的宏观动力学。", "keywords": "PDE发现, 多尺度建模, 生物信号, 钙扩散, 星形胶质细胞", "comments": "该论文探索了PDE发现应用于生物多尺度建模的新颖性，这对于理解生物系统的复杂性至关重要。利用粒子模拟进行受控评估是研究的亮点，而专注于恢复特定项（如扩散项）则为方法的实用性提供了具体证据。"}}
{"id": "2506.21275", "title": "Surrogate normal-forms for the numerical bifurcation and stability analysis of navier-stokes flows via machine learning", "authors": ["Alessandro Della Pia", "Dimitrios G. Patsatzis", "Gianluigi Rozza", "Lucia Russo", "Constantinos Siettos"], "summary": "Inspired by the Equation-Free multiscale modeling approach, we demonstrate\nhow the embed-learn-lift framework enables the construction of surrogate\nnormal-forms, namely minimal-dimensional reduced-order models (ROMs), from\nhigh-fidelity Navier-Stokes simulations. These surrogate models are then used\nfor efficient and accurate bifurcation and stability analysis. The framework\nproceeds in four steps. First, manifold learning reveals the intrinsic latent\ndimension of the high-dimensional spatio-temporal Navier-Stokes dynamics across\nparameter space. Second, we construct low-dimensional \"normal-form\" like ROMs\non this latent space using Gaussian Process Regression (GPR), capturing the\nemergent dynamics. Third, using these models, we apply numerical bifurcation\ntools to compute bifurcation diagrams and perform stability analysis in the\nlatent space. This includes tracing branches of limit cycles arising from\nAndronov-Hopf bifurcations - tasks intractable in full space due to\ncomputational cost. Finally, solving the pre-image problem allows\nreconstruction of the bifurcation structure in the original high-dimensional\nspace. We demonstrate the methodology on two canonical flows: wake flow past an\ninfinite circular cylinder and planar sudden-expansion channel flow. These\nexhibit Andronov-Hopf and pitchfork bifurcations, respectively, as Reynolds\nnumber increases. Our method identifies the latent dimensionality and\nconstructs GPR-based surrogate normal-forms that enable the tracing and\nstability analysis of bifurcating solutions, including limit cycles, their\nperiod, and stability via Floquet multipliers.", "comment": "26 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.21275v1", "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.21275v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过机器学习对纳维-斯托克斯流进行数值分岔和稳定性分析的代理范式", "tldr": "本文提出一种基于机器学习的“嵌入-学习-提升”框架，通过构建代理范式（低维降阶模型）高效准确地分析纳维-斯托克斯流的分岔和稳定性，解决了全空间计算成本过高的问题。", "motivation": "对高维纳维-斯托克斯流进行分岔和稳定性分析在全空间中计算成本极高，导致某些任务（如跟踪极限环）难以实现。", "method": "采用“嵌入-学习-提升”四步框架：1. 流形学习揭示纳维-斯托克斯动力学的内在潜在维度。2. 使用高斯过程回归(GPR)在该潜在空间中构建低维“范式”降阶模型。3. 在潜在空间中应用数值分岔工具计算分岔图并进行稳定性分析。4. 求解原像问题以在原始高维空间中重建分岔结构。", "result": "该方法在无限圆柱尾流和平面突扩通道流两种典型流动上进行了验证，成功识别了潜在维度，构建了基于GPR的代理范式，从而能够跟踪和分析分岔解，包括极限环、其周期及其通过Floquet乘子的稳定性。", "conclusion": "所提出的基于机器学习的代理范式方法有效地实现了对纳维-斯托克斯流动的分岔和稳定性分析，解决了全空间计算成本过高的问题，并能准确捕获复杂动力学行为。", "translation": "受无方程多尺度建模方法的启发，我们展示了“嵌入-学习-提升”框架如何从高精度纳维-斯托克斯模拟中构建代理范式，即最小维度的降阶模型（ROMs）。这些代理模型随后被用于高效准确的分岔和稳定性分析。该框架分为四个步骤。首先，流形学习揭示了跨参数空间的高维时空纳维-斯托克斯动力学的内在潜在维度。其次，我们使用高斯过程回归（GPR）在该潜在空间中构建了低维“范式”类降阶模型，捕获了涌现动力学。第三，使用这些模型，我们应用数值分岔工具在潜在空间中计算分岔图并进行稳定性分析。这包括跟踪由Andronov-Hopf分岔产生的极限环分支——这些任务在全空间中由于计算成本是无法处理的。最后，求解原像问题允许在原始高维空间中重建分岔结构。我们在两种典型流动上演示了该方法：无限圆柱尾流和平面突扩通道流。随着雷诺数的增加，它们分别表现出Andronov-Hopf和叉形分岔。我们的方法识别了潜在维度并构建了基于GPR的代理范式，使得能够跟踪和稳定性分析分岔解，包括极限环、其周期以及通过Floquet乘子确定的稳定性。", "summary": "本文提出了一种名为“嵌入-学习-提升”的机器学习框架，用于从高精度纳维-斯托克斯模拟中构建代理范式（最小维降阶模型）。该框架通过流形学习识别潜在维度，然后利用高斯过程回归在该潜在空间中构建降阶模型，并应用数值分岔工具进行高效的分岔和稳定性分析，最终将结果重建到原始高维空间。该方法成功应用于两种典型流动，验证了其在处理计算成本高昂的分岔分析任务（如极限环）方面的有效性。", "keywords": "纳维-斯托克斯流, 分岔分析, 稳定性分析, 机器学习, 降阶模型", "comments": "这项工作创新性地将机器学习方法（流形学习和高斯过程回归）应用于纳维-斯托克斯流的降阶建模，构建了代理范式，从而能够高效地进行高维流动的分岔和稳定性分析，解决了传统方法在计算成本上的难题，尤其在分析极限环等复杂动力学行为方面展现了强大潜力。"}}
{"id": "2506.20979", "title": "3D Scene-Camera Representation with Joint Camera Photometric Optimization", "authors": ["Weichen Dai", "Kangcheng Ma", "Jiaxin Wang", "Kecen Pan", "Yuhang Ming", "Hua Zhang", "Wanzeng Kong"], "summary": "Representing scenes from multi-view images is a crucial task in computer\nvision with extensive applications. However, inherent photometric distortions\nin the camera imaging can significantly degrade image quality. Without\naccounting for these distortions, the 3D scene representation may inadvertently\nincorporate erroneous information unrelated to the scene, diminishing the\nquality of the representation. In this paper, we propose a novel 3D\nscene-camera representation with joint camera photometric optimization. By\nintroducing internal and external photometric model, we propose a full\nphotometric model and corresponding camera representation. Based on\nsimultaneously optimizing the parameters of the camera representation, the\nproposed method effectively separates scene-unrelated information from the 3D\nscene representation. Additionally, during the optimization of the photometric\nparameters, we introduce a depth regularization to prevent the 3D scene\nrepresentation from fitting scene-unrelated information. By incorporating the\ncamera model as part of the mapping process, the proposed method constructs a\ncomplete map that includes both the scene radiance field and the camera\nphotometric model. Experimental results demonstrate that the proposed method\ncan achieve high-quality 3D scene representations, even under conditions of\nimaging degradation, such as vignetting and dirt.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20979v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20979v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "三维场景-相机表示与联合相机光度优化", "tldr": "本文提出了一种新的三维场景-相机表示方法，通过联合优化相机光度模型来有效分离场景无关信息，即使在图像退化条件下也能实现高质量的三维场景表示。", "motivation": "在计算机视觉中，从多视图图像表示场景至关重要，但相机成像中的光度畸变会显著降低图像质量，导致三维场景表示中包含与场景无关的错误信息，从而降低表示质量。", "method": "本文提出了一种新的三维场景-相机表示方法，通过引入内部和外部光度模型构建完整的相机光度模型。该方法通过同时优化相机表示参数，将场景无关信息从三维场景表示中分离。此外，在光度参数优化过程中引入深度正则化，以防止三维场景表示拟合无关信息。通过将相机模型作为映射过程的一部分，构建了一个包含场景辐射场和相机光度模型的完整映射。", "result": "实验结果表明，即使在渐晕和污垢等成像退化条件下，所提出的方法也能实现高质量的三维场景表示。", "conclusion": "通过联合优化相机光度模型，本方法能够有效处理相机畸变，从而获得更准确、高质量的三维场景表示，即使面对图像退化也能表现良好。", "translation": "从多视图图像表示场景是计算机视觉中一项关键任务，具有广泛的应用。然而，相机成像中固有的光度畸变会显著降低图像质量。如果不考虑这些畸变，三维场景表示可能会无意中包含与场景无关的错误信息，从而降低表示的质量。本文提出了一种新的三维场景-相机表示方法，并进行联合相机光度优化。通过引入内部和外部光度模型，我们提出了一种完整的光度模型和相应的相机表示。通过同时优化相机表示的参数，所提出的方法有效地将与场景无关的信息从三维场景表示中分离出来。此外，在光度参数优化过程中，我们引入了深度正则化，以防止三维场景表示拟合与场景无关的信息。通过将相机模型作为映射过程的一部分，所提出的方法构建了一个完整的映射，包括场景辐射场和相机光度模型。实验结果表明，即使在渐晕和污垢等成像退化条件下，所提出的方法也能实现高质量的三维场景表示。", "summary": "本文提出一种新颖的三维场景-相机表示方法，通过联合优化相机光度模型来解决多视图图像中相机光度畸变导致的三维场景表示质量下降问题。该方法引入内部和外部光度模型构建完整相机表示，并同时优化相机参数以分离场景无关信息。此外，通过深度正则化防止无关信息拟合。实验证明，即使在成像退化条件下，该方法也能实现高质量的三维场景表示。", "keywords": "三维场景表示, 相机光度优化, 光度畸变, 深度正则化, 多视图图像", "comments": "该论文提出了一种创新的方法，将相机光度模型整合到三维场景表示的映射过程中，有效地解决了相机成像畸变对场景重建质量的影响。通过联合优化和深度正则化，提高了三维场景表示的鲁棒性和准确性，特别是在图像质量受损的情况下表现出色，具有重要的实际应用价值。"}}
{"id": "2506.20898", "title": "Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction", "authors": ["Erfan Hajihashemi", "Yanning Shen"], "summary": "Online conformal prediction has demonstrated its capability to construct a\nprediction set for each incoming data point that covers the true label with a\npredetermined probability. To cope with potential distribution shift,\nmulti-model online conformal prediction has been introduced to select and\nleverage different models from a preselected candidate set. Along with the\nimproved flexibility, the choice of the preselected set also brings challenges.\nA candidate set that includes a large number of models may increase the\ncomputational complexity. In addition, the inclusion of irrelevant models with\npoor performance may negatively impact the performance and lead to\nunnecessarily large prediction sets. To address these challenges, we propose a\nnovel multi-model online conformal prediction algorithm that identifies a\nsubset of effective models at each time step by collecting feedback from a\nbipartite graph, which is refined upon receiving new data. A model is then\nselected from this subset to construct the prediction set, resulting in reduced\ncomputational complexity and smaller prediction sets. Additionally, we\ndemonstrate that using prediction set size as feedback, alongside model loss,\ncan significantly improve efficiency by constructing smaller prediction sets\nwhile still satisfying the required coverage guarantee. The proposed algorithms\nare proven to ensure valid coverage and achieve sublinear regret. Experiments\non real and synthetic datasets validate that the proposed methods construct\nsmaller prediction sets and outperform existing multi-model online conformal\nprediction approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20898v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20898v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "图结构反馈多模型集成在线共形预测", "tldr": "提出了一种新的多模型在线共形预测算法，该算法利用图结构反馈动态选择有效模型子集，从而降低了计算复杂度并减小了预测集大小。", "motivation": "现有的多模型在线共形预测方法在处理大量候选模型时面临挑战，包括计算复杂性增加以及包含不相关模型可能导致性能下降和预测集过大。", "method": "本文提出了一种新颖的多模型在线共形预测算法：\n1. 在每个时间步通过收集二分图的反馈来识别有效模型子集，该二分图会根据新数据进行完善。\n2. 从该子集中选择一个模型来构建预测集。\n3. 除了模型损失，还利用预测集大小作为反馈，以显著提高效率。", "result": "1. 降低了计算复杂度。\n2. 构建了更小的预测集。\n3. 显著提高了效率。\n4. 确保了有效的覆盖率。\n5. 实现了次线性遗憾。\n6. 在真实和合成数据集上的实验验证，所提出的方法构建的预测集更小，并且优于现有的多模型在线共形预测方法。", "conclusion": "所提出的图结构反馈多模型在线共形预测算法有效解决了模型选择的挑战，在保证覆盖率的同时，能够生成更高效、更准确的预测集。", "translation": "在线共形预测已证明其能够为每个传入数据点构建一个预测集，并以预定概率覆盖真实标签。为了应对潜在的分布漂移，引入了多模型在线共形预测，以从预选的候选集中选择和利用不同的模型。在提高灵活性的同时，预选集的选择也带来了挑战。包含大量模型的候选集可能会增加计算复杂度。此外，包含性能不佳的不相关模型可能会对性能产生负面影响，并导致不必要的过大预测集。为了解决这些挑战，我们提出了一种新颖的多模型在线共形预测算法，该算法通过收集二分图的反馈来识别每个时间步的有效模型子集，该二分图在接收新数据时得到完善。然后从该子集中选择一个模型来构建预测集，从而降低了计算复杂度和减小了预测集。此外，我们证明了将预测集大小作为反馈，除了模型损失之外，可以通过构建更小的预测集，同时仍满足所需的覆盖保证，显著提高效率。所提出的算法被证明可以确保有效覆盖并实现次线性遗憾。在真实和合成数据集上的实验验证了所提出的方法构建的预测集更小，并且优于现有的多模型在线共形预测方法。", "summary": "本文提出了一种新颖的多模型在线共形预测算法，该算法利用图结构反馈动态选择有效模型子集。通过根据新数据完善二分图并利用预测集大小作为反馈，该方法显著降低了计算复杂度，生成了更小的预测集，并提高了效率。实验证明，该算法在保证有效覆盖和次线性遗憾的同时，性能优于现有方法。", "keywords": "在线共形预测, 多模型集成, 图结构反馈, 预测集, 模型选择", "comments": "该论文的创新点在于将图结构反馈引入在线共形预测中的模型选择过程，有效地解决了模型冗余和计算效率问题。利用预测集大小作为反馈信号以进一步优化效率，是另一个值得关注的贡献。这使得该方法在实际应用中更具吸引力。"}}
{"id": "2506.21252", "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.", "comment": "ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2506.21252v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21252v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Agent-RewardBench：迈向真实世界多模态智能体在感知、规划和安全方面的奖励建模统一基准", "tldr": "Agent-RewardBench是一个新的基准，用于评估多模态大语言模型在真实世界多模态智能体中奖励建模的能力，实验表明现有模型表现有限，需专门训练。", "motivation": "多模态大语言模型（MLLMs）在真实世界任务中展现潜力，但由于缺乏外部反馈，智能体难以进行自我纠正和泛化。奖励模型作为外部反馈是一种有前景的方法，但目前尚不清楚如何为智能体选择奖励模型。因此，迫切需要构建一个针对智能体的奖励基准。", "method": "我们提出了Agent-RewardBench，一个旨在评估MLLMs中奖励建模能力的基准。其特点包括：1) 多维度和真实世界智能体场景评估，涵盖感知、规划和安全，包含7个场景；2) 步级奖励评估，提供规划过程中性能的更细粒度视图；3) 适当的难度和高质量，通过从10个不同模型中采样、控制难度和手动验证来确保数据完整性。", "result": "实验表明，即使是最先进的多模态模型也表现出有限的性能。", "conclusion": "现有最先进的多模态模型在智能体奖励建模方面表现有限，凸显了对智能体奖励建模进行专门训练的必要性。", "translation": "随着多模态大语言模型（MLLMs）的进步，多模态智能体在网络导航和具身智能等真实世界任务中展现出前景。然而，由于缺乏外部反馈，这些智能体在自我纠正和泛化方面存在困难。使用奖励模型作为外部反馈是一种有前景的方法，但目前尚不清楚如何为智能体选择奖励模型。因此，迫切需要建立一个针对智能体的奖励基准。为解决这些挑战，我们提出了Agent-RewardBench，一个旨在评估MLLMs中奖励建模能力的基准。该基准具有三个关键特征：(1) 多维度和真实世界智能体场景评估。它涵盖感知、规划和安全，包含7个场景；(2) 步级奖励评估。它允许评估任务中各个步骤的智能体能力，提供规划过程中性能的更细粒度视图；(3) 适当的难度和高质量。我们从10个不同的模型中仔细采样，控制难度以保持任务挑战性，并进行人工验证以确保数据的完整性。实验表明，即使是最先进的多模态模型也表现出有限的性能，这凸显了在智能体奖励建模方面进行专门训练的必要性。代码可在github上获取。", "summary": "该论文提出了Agent-RewardBench，一个用于评估多模态大语言模型（MLLMs）在真实世界多模态智能体中奖励建模能力的统一基准。该基准涵盖了感知、规划和安全等多个维度，包含7个真实世界场景，并支持步级奖励评估以提供细粒度性能视图。其数据经过精心采样、难度控制和人工验证，以确保高质量。实验结果表明，即使是当前最先进的MLLMs在奖励建模方面也表现出有限的性能，强调了对智能体奖励建模进行专门训练的必要性。", "keywords": "奖励建模, 多模态智能体, 基准测试, MLLMs, 感知规划安全", "comments": "Agent-RewardBench的创新之处在于其针对多模态智能体奖励建模的统一评估框架，特别是其多维度、步级评估和高质量数据设计。这对于推动MLLMs在真实世界智能体应用中的发展至关重要，因为它揭示了当前模型在奖励建模方面的局限性，并指明了未来研究的方向。"}}
{"id": "2506.21121", "title": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Lu Zhang", "Peiliang Li", "Shaojie Shen"], "summary": "Trajectory prediction for surrounding agents is a challenging task in\nautonomous driving due to its inherent uncertainty and underlying\nmultimodality. Unlike prevailing data-driven methods that primarily rely on\nsupervised learning, in this paper, we introduce a novel Graph-oriented Inverse\nReinforcement Learning (GoIRL) framework, which is an IRL-based predictor\nequipped with vectorized context representations. We develop a feature adaptor\nto effectively aggregate lane-graph features into grid space, enabling seamless\nintegration with the maximum entropy IRL paradigm to infer the reward\ndistribution and obtain the policy that can be sampled to induce multiple\nplausible plans. Furthermore, conditioned on the sampled plans, we implement a\nhierarchical parameterized trajectory generator with a refinement module to\nenhance prediction accuracy and a probability fusion strategy to boost\nprediction confidence. Extensive experimental results showcase our approach not\nonly achieves state-of-the-art performance on the large-scale Argoverse &\nnuScenes motion forecasting benchmarks but also exhibits superior\ngeneralization abilities compared to existing supervised models.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21121v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21121v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GoIRL：面向图的逆强化学习用于多模态轨迹预测", "tldr": "GoIRL是一个基于逆强化学习的框架，用于多模态轨迹预测，在自动驾驶基准测试中表现出色，并展现出卓越的泛化能力。", "motivation": "在自动驾驶中，由于固有的不确定性和潜在的多模态性，周围代理的轨迹预测是一项具有挑战性的任务。", "method": "本文引入了GoIRL（Graph-oriented Inverse Reinforcement Learning）框架，这是一个基于IRL的预测器，配备矢量化上下文表示。该方法开发了一个特征适配器，用于有效地将车道图特征聚合到网格空间，并与最大熵IRL范式集成，以推断奖励分布并获得可以采样以诱导多个合理计划的策略。此外，以采样的计划为条件，GoIRL实现了一个分层参数化轨迹生成器，带有细化模块以提高预测精度，以及一个概率融合策略以提升预测置信度。", "result": "GoIRL方法在大型Argoverse和nuScenes运动预测基准测试中不仅取得了最先进的性能，而且与现有监督模型相比，展现出卓越的泛化能力。", "conclusion": "GoIRL框架通过结合逆强化学习和图结构表示，为自动驾驶中的多模态轨迹预测提供了一种有效且泛化能力强的解决方案，优于传统的监督学习方法。", "translation": "周围代理的轨迹预测是自动驾驶中一项具有挑战性的任务，原因在于其固有的不确定性和潜在的多模态性。与主要依赖监督学习的现有数据驱动方法不同，本文引入了一种新颖的面向图的逆强化学习（GoIRL）框架，它是一个配备矢量化上下文表示的基于IRL的预测器。我们开发了一个特征适配器，有效地将车道图特征聚合到网格空间中，从而实现与最大熵IRL范式的无缝集成，以推断奖励分布并获得可以采样以诱导多个合理计划的策略。此外，以采样的计划为条件，我们实现了一个分层参数化轨迹生成器，带有细化模块以提高预测精度，以及一个概率融合策略以提升预测置信度。广泛的实验结果表明，我们的方法不仅在大型Argoverse和nuScenes运动预测基准测试中取得了最先进的性能，而且与现有监督模型相比，展现出卓越的泛化能力。", "summary": "本文提出GoIRL，一个面向图的逆强化学习框架，用于自动驾驶中的多模态轨迹预测。GoIRL结合特征适配器将车道图特征融入网格空间，并通过最大熵IRL推断奖励和策略以生成多模态计划。在此基础上，利用分层轨迹生成器和概率融合策略提高预测精度和置信度。实验证明，GoIRL在Argoverse和nuScenes基准测试中达到SOTA性能，并展现出优于现有监督模型的泛化能力。", "keywords": "轨迹预测, 逆强化学习, 图神经网络, 自动驾驶, 多模态", "comments": "该论文的创新点在于将逆强化学习（IRL）引入多模态轨迹预测，并结合图结构表示，这与主流的监督学习方法形成对比。通过IRL推断奖励函数和策略来生成多模态预测，更符合自动驾驶中复杂决策的特点。其在SOTA性能和泛化能力上的提升，显示了IRL在解决轨迹预测不确定性方面的潜力。"}}
{"id": "2506.20983", "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation", "authors": ["Wenjie Xuan", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "summary": "Recent works favored dense signals (e.g., depth, DensePose), as an\nalternative to sparse signals (e.g., OpenPose), to provide detailed spatial\nguidance for pose-guided text-to-image generation. However, dense\nrepresentations raised new challenges, including editing difficulties and\npotential inconsistencies with textual prompts. This fact motivates us to\nrevisit sparse signals for pose guidance, owing to their simplicity and\nshape-agnostic nature, which remains underexplored. This paper proposes a novel\nSpatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust\ncontrollability for pose-guided image generation. Specifically, we extend\nOpenPose to a learnable spatial representation, making keypoint embeddings\ndiscriminative and expressive. Additionally, we introduce keypoint concept\nlearning, which encourages keypoint tokens to attend to the spatial positions\nof each keypoint, thus improving pose alignment. Experiments on animal- and\nhuman-centric image generation tasks demonstrate that our method outperforms\nrecent spatially controllable T2I generation approaches under sparse-pose\nguidance and even matches the performance of dense signal-based methods.\nMoreover, SP-Ctrl shows promising capabilities in diverse and cross-species\ngeneration through sparse signals. Codes will be available at\nhttps://github.com/DREAMXFAR/SP-Ctrl.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.20983v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20983v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "重新思考用于姿态引导文本到图像生成的稀疏信号", "tldr": "本文提出SP-Ctrl，通过扩展OpenPose并引入关键点概念学习，使稀疏姿态信号在文本到图像生成中实现鲁棒控制，性能优于现有方法甚至媲美密集信号。", "motivation": "近期工作倾向使用密集信号（如深度、DensePose）提供详细空间引导，但密集表示存在编辑困难和与文本提示不一致的问题。这促使作者重新审视稀疏信号（如OpenPose）在姿态引导中的潜力，因其简单且与形状无关的特性仍未被充分探索。", "method": "本文提出Spatial-Pose ControlNet (SP-Ctrl)。具体而言，将OpenPose扩展为可学习的空间表示，使关键点嵌入更具判别性和表达性。此外，引入关键点概念学习，鼓励关键点tokens关注每个关键点的空间位置，从而改善姿态对齐。", "result": "在以动物和人类为中心的图像生成任务上的实验表明，该方法在稀疏姿态引导下优于近期空间可控的T2I生成方法，甚至达到与基于密集信号的方法相当的性能。SP-Ctrl还通过稀疏信号在多样化和跨物种生成中展现出有前景的能力。", "conclusion": "SP-Ctrl成功地使稀疏信号在姿态引导的文本到图像生成中实现鲁棒控制，并在性能上超越或媲美密集信号方法，展现了稀疏信号的巨大潜力。", "translation": "近期工作倾向于使用密集信号（例如，深度、DensePose）而非稀疏信号（例如，OpenPose）为姿态引导的文本到图像生成提供详细的空间引导。然而，密集表示带来了新的挑战，包括编辑困难和与文本提示潜在的不一致性。这一事实促使我们重新审视稀疏信号在姿态引导中的应用，因为其简单性和与形状无关的特性仍未被充分探索。本文提出了一种新颖的空间姿态控制网络（Spatial-Pose ControlNet, SP-Ctrl），使稀疏信号具备用于姿态引导图像生成的鲁棒可控性。具体而言，我们将OpenPose扩展为可学习的空间表示，使关键点嵌入具有判别性和表达性。此外，我们引入了关键点概念学习，鼓励关键点tokens关注每个关键点的空间位置，从而改善姿态对齐。在以动物和人类为中心的图像生成任务上的实验表明，我们的方法在稀疏姿态引导下优于近期空间可控的T2I生成方法，甚至达到与密集信号基方法相当的性能。此外，SP-Ctrl通过稀疏信号在多样化和跨物种生成中展现出有前景的能力。代码将在https://github.com/DREAMXFAR/SP-Ctrl 提供。", "summary": "本文针对姿态引导文本到图像生成中密集信号的挑战，重新审视并利用稀疏信号。提出Spatial-Pose ControlNet (SP-Ctrl)，通过将OpenPose扩展为可学习空间表示并引入关键点概念学习，显著提升了稀疏信号的控制能力和姿态对齐。实验证明，SP-Ctrl在稀疏姿态引导下性能超越现有方法，并能与密集信号方法媲美，展现了在多样化和跨物种生成中的潜力。", "keywords": "姿态引导生成, 文本到图像, 稀疏信号, OpenPose, ControlNet", "comments": "本文创新性地解决了稀疏姿态信号在文本到图像生成中控制力不足的问题，通过设计可学习的关键点表示和引入关键点概念学习，有效提升了稀疏信号的表达能力和对齐精度。其重要性在于证明了稀疏信号在特定场景下不仅可以与密集信号方法相媲美，甚至可能在编辑灵活性和避免不一致性方面具有优势。这为未来的姿态引导生成研究提供了新的视角和方向。"}}
{"id": "2506.21274", "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness", "comment": "(Submitted for publication)", "pdf_url": "http://arxiv.org/pdf/2506.21274v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21274v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "猫鼠游戏——伪造文本生成能否超越检测系统？", "tldr": "研究发现，尽管Gemini在生成欺骗性文本方面有所提升，但可靠的伪造文本检测即使对大型模型仍然可行，暗示检测系统可能不会被无限超越。", "motivation": "大型语言模型能生成逼真的“伪造文本”，而检测方法也层出不穷，这似乎预示着一场无休止的“军备竞赛”。论文的动机是探讨模型生成欺骗性文本的能力是否会达到一个瓶颈，从而回答检测系统能否持续有效的问题。", "method": "研究通过统计分类器来识别“经典侦探小说风格”的“伪造文本”。具体对比了Gemini和GPT模型在0.5版本更新后的欺骗性生成能力。", "result": "发现Gemini在0.5版本更新后生成欺骗性文本的能力有所增强，而GPT则没有。", "conclusion": "尽管新的模型架构可能会提高其欺骗性，但即使对于越来越大的模型，可靠的伪造文本检测仍然是可行的。", "translation": "大型语言模型可以在学术写作、产品评论和政治新闻等领域生成令人信服的“伪造文本”。许多方法已被研究用于检测人工智能生成的文本。虽然这似乎预示着一场无休止的“军备竞赛”，但我们注意到，较新的大型语言模型使用越来越多的参数、训练数据和能源，而相对简单的分类器则以适度的资源展示了良好的检测准确性。为了探讨模型击败检测器的能力是否因此达到一个平台期，我们研究了统计分类器识别“经典侦探小说风格”的“伪造文本”的能力。经过0.5个版本的提升，我们发现Gemini生成欺骗性文本的能力有所增强，而GPT则没有。这表明，即使对于越来越大的模型，可靠的伪造文本检测仍然是可行的，尽管新的模型架构可能会提高其欺骗性。", "summary": "本文探讨了大型语言模型（LLMs）生成“伪造文本”的能力与检测系统之间的“猫鼠游戏”。研究指出，尽管LLMs不断增大，但简单的分类器仍能有效检测。通过考察统计分类器识别经典侦探小说风格伪造文本的能力，发现Gemini在0.5版本升级后生成欺骗性文本的能力有所提升，而GPT未见此趋势。研究结论认为，即使面对更大型的模型，伪造文本的可靠检测仍可能保持可行性。", "keywords": "伪造文本, 大型语言模型, 文本检测, Gemini, GPT", "comments": "这篇论文通过对比不同大型语言模型的欺骗性文本生成能力，探讨了AI生成内容检测的长期可行性。其创新点在于将“猫鼠游戏”的视角引入到AI内容攻防战中，并具体考察了模型版本更新对欺骗性的影响。重要性在于为AI内容检测领域提供了乐观的展望，认为即使模型能力增强，检测仍有其可行性。限制可能在于仅考察了特定风格的文本和有限的模型版本增量。"}}
{"id": "2506.20986", "title": "EVA: Mixture-of-Experts Semantic Variant Alignment for Compositional Zero-Shot Learning", "authors": ["Xiao Zhang", "Yongqiang Ma", "Haodong Jing", "Nanning Zheng"], "summary": "Compositional Zero-Shot Learning (CZSL) investigates compositional\ngeneralization capacity to recognize unknown state-object pairs based on\nlearned primitive concepts. Existing CZSL methods typically derive primitives\nfeatures through a simple composition-prototype mapping, which is suboptimal\nfor a set of individuals that can be divided into distinct semantic subsets.\nMoreover, the all-to-one cross-modal primitives matching neglects compositional\ndivergence within identical states or objects, limiting fine-grained\nimage-composition alignment. In this study, we propose EVA, a\nMixture-of-Experts Semantic Variant Alignment framework for CZSL. Specifically,\nwe introduce domain-expert adaption, leveraging multiple experts to achieve\ntoken-aware learning and model high-quality primitive representations. To\nenable accurate compositional generalization, we further present semantic\nvariant alignment to select semantically relevant representation for\nimage-primitives matching. Our method significantly outperforms other\nstate-of-the-art CZSL methods on three popular benchmarks in both closed- and\nopen-world settings, demonstrating the efficacy of the proposed insight.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20986v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20986v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EVA：混合专家语义变体对齐的组合零样本学习", "tldr": "提出EVA框架，通过混合专家和语义变体对齐，解决现有组合零样本学习中原始特征提取和跨模态匹配的不足，显著提升了性能。", "motivation": "现有组合零样本学习（CZSL）方法在原始特征推导方面存在次优性，尤其对于可分为不同语义子集的个体集合；此外，一对多的跨模态原始匹配忽略了相同状态或对象内的组合差异，限制了细粒度的图像-组合对齐。", "method": "本研究提出了EVA，一个用于CZSL的混合专家语义变体对齐框架。该框架引入了领域专家自适应，利用多个专家实现令牌感知学习并建模高质量的原始表示。此外，它还提出了语义变体对齐，以选择语义相关的表示进行图像-原始匹配，从而实现准确的组合泛化。", "result": "该方法在闭世界和开放世界设置下，于三个流行的基准测试上显著优于其他最先进的CZSL方法。", "conclusion": "该研究提出的EVA框架及其核心见解是有效的，能够显著提升组合零样本学习的性能。", "translation": "组合零样本学习（CZSL）研究基于学习到的原始概念识别未知状态-对象对的组合泛化能力。现有的CZSL方法通常通过简单的组合-原型映射来推导原始特征，这对于可以划分为不同语义子集的个体集合来说是次优的。此外，一对多的跨模态原始匹配忽略了相同状态或对象内的组合差异，限制了细粒度的图像-组合对齐。在本研究中，我们提出了EVA，一个用于CZSL的混合专家语义变体对齐框架。具体来说，我们引入了领域专家自适应，利用多个专家实现令牌感知学习并建模高质量的原始表示。为了实现准确的组合泛化，我们进一步提出了语义变体对齐，以选择语义相关的表示进行图像-原始匹配。我们的方法在闭世界和开放世界设置下，于三个流行的基准测试上显著优于其他最先进的CZSL方法，证明了所提出见解的有效性。", "summary": "本论文提出了EVA，一个用于组合零样本学习（CZSL）的混合专家语义变体对齐框架。针对现有CZSL方法在原始特征提取和跨模态匹配上的不足，EVA引入了领域专家自适应以获取高质量原始表示，并通过语义变体对齐实现准确的图像-原始匹配。实验结果表明，EVA在多个基准测试上显著超越了现有的先进方法。", "keywords": "组合零样本学习, 混合专家, 语义变体对齐, 领域专家自适应, 原始表示", "comments": "这项研究的创新点在于提出了一个混合专家框架来处理CZSL中原始特征的语义多样性，并通过语义变体对齐解决了细粒度图像-组合对齐的问题。其重要性在于显著提升了CZSL的性能，为未来研究提供了新的方向。"}}
{"id": "2506.20916", "title": "Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "summary": "Deep reinforcement learning has been extensively studied in decision-making\nprocesses and has demonstrated superior performance over conventional\napproaches in various fields, including radar resource management (RRM).\nHowever, a notable limitation of neural networks is their ``black box\" nature\nand recent research work has increasingly focused on explainable AI (XAI)\ntechniques to describe the rationale behind neural network decisions. One\npromising XAI method is local interpretable model-agnostic explanations (LIME).\nHowever, the sampling process in LIME ignores the correlations between\nfeatures. In this paper, we propose a modified LIME approach that integrates\ndeep learning (DL) into the sampling process, which we refer to as DL-LIME. We\nemploy DL-LIME within deep reinforcement learning for radar resource\nmanagement. Numerical results show that DL-LIME outperforms conventional LIME\nin terms of both fidelity and task performance, demonstrating superior\nperformance with both metrics. DL-LIME also provides insights on which factors\nare more important in decision making for radar resource management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20916v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20916v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "雷达资源管理的可解释人工智能：深度强化学习中改进的LIME", "tldr": "本文提出DL-LIME，一种改进的LIME方法，用于解释雷达资源管理中的深度强化学习决策，数值结果表明其在保真度和任务性能上均优于传统LIME。", "motivation": "深度强化学习在雷达资源管理中表现优异，但其“黑箱”性质限制了决策理解。因此，需要可解释人工智能（XAI）技术来揭示神经网络决策背后的原理，特别是解决LIME采样过程忽略特征相关性的问题。", "method": "本文提出了一种名为DL-LIME的改进LIME方法，该方法将深度学习（DL）集成到LIME的采样过程中。DL-LIME被应用于雷达资源管理中的深度强化学习。", "result": "数值结果表明，DL-LIME在保真度和任务性能方面均优于传统的LIME，并且在两项指标上都表现出卓越的性能。DL-LIME还提供了关于雷达资源管理决策中哪些因素更重要的见解。", "conclusion": "DL-LIME在雷达资源管理的可解释深度强化学习中表现出优越的性能，并能提供决策因素的重要性见解，克服了传统LIME在特征相关性方面的不足。", "translation": "深度强化学习在决策过程中得到了广泛研究，并在包括雷达资源管理（RRM）在内的各个领域中表现出优于传统方法的性能。然而，神经网络的一个显著限制是其“黑箱”性质，最近的研究工作越来越关注可解释人工智能（XAI）技术，以描述神经网络决策背后的原理。一种有前景的XAI方法是局部可解释模型无关解释（LIME）。然而，LIME中的采样过程忽略了特征之间的相关性。在本文中，我们提出了一种改进的LIME方法，该方法将深度学习（DL）集成到采样过程中，我们称之为DL-LIME。我们将DL-LIME应用于雷达资源管理中的深度强化学习。数值结果表明，DL-LIME在保真度和任务性能方面均优于传统的LIME，并在两项指标上都表现出卓越的性能。DL-LIME还提供了关于雷达资源管理决策中哪些因素更重要的见解。", "summary": "本文提出了一种改进的LIME方法（DL-LIME），旨在解决传统LIME在深度强化学习中解释雷达资源管理决策时忽略特征相关性的问题。DL-LIME通过将深度学习集成到LIME的采样过程中，提高了模型解释的保真度和任务性能，并能揭示决策过程中的重要影响因素。", "keywords": "可解释人工智能, 深度强化学习, 雷达资源管理, LIME, DL-LIME", "comments": "本文的创新点在于提出了DL-LIME，通过将深度学习融入LIME的采样过程，解决了传统LIME忽略特征相关性的问题。这对于提升深度强化学习在雷达资源管理等关键领域的决策可解释性具有重要意义，有助于理解“黑箱”模型并增强其在实际应用中的可信度。"}}
{"id": "2506.21285", "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.21285v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21285v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Double-Checker：通过自我批判微调增强慢思考LLM的推理能力", "tldr": "Double-Checker框架通过自我批判微调显著提升了慢思考LLM的推理能力，尤其在复杂推理任务上表现突出。", "motivation": "虽然慢思考大型语言模型（LLMs）表现出类似反思的推理能力，但它们生成信息丰富的批判和改进先前解决方案的能力仍然有限。", "method": "本文引入了Double-Checker，这是一个旨在通过促进明确的自我批判和迭代改进其先前解决方案来增强慢思考LLMs推理能力的框架。通过在我们精心策划的1,730个自我批判实例上进行微调，Double-Checker使长CoT LLMs能够在推理过程中迭代地批判和完善其输出，直到它们根据自我生成的批判评估解决方案是正确的。", "result": "Double-Checker在全面的推理基准套件中验证了其功效，表明迭代自我批判显著增强了长CoT LLMs的推理能力。值得注意的是，与原始的长CoT LLMs相比，Double-Checker将AIME挑战性基准测试上的pass@1性能从4.4%提高到18.2%。", "conclusion": "这些结果突出了开发更值得信赖和更有效的、能够进行结构化自我批判的LLMs的一个有前景的方向。", "translation": "虽然慢思考大型语言模型（LLMs）表现出类似反思的推理能力，通常被称为“顿悟时刻”，但它们生成信息丰富的批判和改进先前解决方案的能力仍然有限。在本文中，我们引入了Double-Checker，这是一个旨在通过促进明确的自我批判和迭代改进其先前解决方案来增强慢思考LLMs推理能力的原则性框架。通过在我们精心策划的1,730个自我批判实例上进行微调，Double-Checker使长CoT LLMs能够在推理过程中迭代地批判和完善其输出，直到它们在自我生成的批判下评估其解决方案是正确的。我们在全面的推理基准套件中验证了Double-Checker的功效，表明迭代自我批判显著增强了长CoT LLMs的推理能力。值得注意的是，与原始的长CoT LLMs相比，我们的Double-Checker将AIME挑战性基准测试上的pass@1性能从4.4%提高到18.2%。这些结果突出了开发更值得信赖和更有效的、能够进行结构化自我批判的LLMs的一个有前景的方向。", "summary": "本文提出了Double-Checker框架，旨在通过自我批判微调来提升慢思考大型语言模型（LLMs）的推理能力。该框架利用1,730个自我批判实例进行训练，使LLMs能够迭代地自我批判并改进其输出，直到达到满意的解决方案。实验结果表明，Double-Checker显著增强了长CoT LLMs的推理能力，尤其在AIME基准测试中实现了显著的性能提升，这为开发更可靠且具自我批判能力的LLMs提供了新方向。", "keywords": "慢思考LLM, 自我批判, 微调, 推理能力, Double-Checker", "comments": "Double-Checker的创新之处在于其明确的自我批判和迭代细化机制，这有效地弥补了现有慢思考LLMs在自我纠错方面的不足。通过构建特定的自我批判数据集进行微调，该方法提供了一个可行的路径来提升LLMs的可靠性和推理深度。其在AIME等复杂推理任务上的显著性能提升，凸显了该方法的实用性和重要性。"}}
{"id": "2506.20988", "title": "Segment Anything in Pathology Images with Natural Language", "authors": ["Zhixuan Chen", "Junlin Hou", "Liqi Lin", "Yihui Wang", "Yequan Bie", "Xi Wang", "Yanning Zhou", "Ronald Cheong Kin Chan", "Hao Chen"], "summary": "Pathology image segmentation is crucial in computational pathology for\nanalyzing histological features relevant to cancer diagnosis and prognosis.\nHowever, current methods face major challenges in clinical applications due to\nlimited annotated data and restricted category definitions. To address these\nlimitations, we propose PathSegmentor, the first text-prompted segmentation\nfoundation model designed specifically for pathology images. We also introduce\nPathSeg , the largest and most comprehensive dataset for pathology\nsegmentation, built from 17 public sources and containing 275k image-mask-label\ntriples across 160 diverse categories. With PathSegmentor, users can perform\nsemantic segmentation using natural language prompts, eliminating the need for\nlaborious spatial inputs such as points or boxes. Extensive experiments\ndemonstrate that PathSegmentor outperforms specialized models with higher\naccuracy and broader applicability, while maintaining a compact architecture.\nIt significantly surpasses existing spatial- and text-prompted models by 0.145\nand 0.429 in overall Dice scores, respectively, showing strong robustness in\nsegmenting complex structures and generalizing to external datasets. Moreover,\nPathSegmentor's outputs enhance the interpretability of diagnostic models\nthrough feature importance estimation and imaging biomarker discovery, offering\npathologists evidence-based support for clinical decision-making. This work\nadvances the development of explainable AI in precision oncology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20988v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20988v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用自然语言分割病理图像中的任何内容", "tldr": "PathSegmentor是一个基于文本提示的病理图像分割基础模型，它利用最大的病理分割数据集PathSeg，实现了高精度和广泛适用性，并超越了现有模型，同时增强了诊断模型的可解释性。", "motivation": "当前的病理图像分割方法在临床应用中面临标注数据有限和类别定义受限的挑战。", "method": "本文提出了PathSegmentor，这是第一个专为病理图像设计的文本提示分割基础模型。同时引入了PathSeg，这是迄今为止最大、最全面的病理分割数据集，包含来自17个公共来源的27.5万个图像-掩码-标签三元组，涵盖160个不同类别。PathSegmentor允许用户使用自然语言提示进行语义分割，无需繁琐的空间输入。", "result": "PathSegmentor在准确性和适用性方面优于专业模型，同时保持紧凑的架构。在整体Dice分数上，它分别比现有空间提示模型和文本提示模型高出0.145和0.429。该模型在分割复杂结构和泛化到外部数据集方面表现出强大的鲁棒性。此外，PathSegmentor的输出通过特征重要性估计和成像生物标志物发现，增强了诊断模型的可解释性。", "conclusion": "这项工作推动了精准肿瘤学中可解释人工智能的发展。", "translation": "病理图像分割在计算病理学中至关重要，用于分析与癌症诊断和预后相关的组织学特征。然而，由于标注数据有限和类别定义受限，当前方法在临床应用中面临重大挑战。为了解决这些限制，我们提出了PathSegmentor，这是第一个专为病理图像设计的文本提示分割基础模型。我们还引入了PathSeg，这是迄今为止最大、最全面的病理分割数据集，它由17个公共来源构建，包含160个不同类别的27.5万个图像-掩码-标签三元组。通过PathSegmentor，用户可以使用自然语言提示执行语义分割，无需繁琐的空间输入，例如点或框。大量实验表明，PathSegmentor以更高的准确性和更广泛的适用性优于专业模型，同时保持紧凑的架构。它在整体Dice分数上分别显着超越现有空间提示模型0.145和文本提示模型0.429，在分割复杂结构和泛化到外部数据集方面表现出强大的鲁棒性。此外，PathSegmentor的输出通过特征重要性估计和成像生物标志物发现，增强了诊断模型的可解释性，为病理学家提供了循证支持，以辅助临床决策。这项工作推动了精准肿瘤学中可解释人工智能的发展。", "summary": "本研究提出PathSegmentor，一个创新的文本提示病理图像分割基础模型，旨在解决现有方法在标注数据和类别定义方面的局限性。为支持该模型，本文构建了PathSeg，一个包含160个类别27.5万个图像-掩码-标签三元组的大型数据集。PathSegmentor允许用户通过自然语言进行分割，无需空间输入，并在实验中展现出优于现有模型的准确性、鲁棒性和泛化能力。此外，它还能增强诊断模型的可解释性，为临床决策提供支持，从而推动精准肿瘤学中可解释AI的发展。", "keywords": "病理图像分割, 自然语言处理, 基础模型, PathSegmentor, PathSeg", "comments": "PathSegmentor的创新之处在于它是首个将自然语言提示应用于病理图像分割的基础模型，极大地简化了分割过程并克服了传统方法的标注限制。其提出的PathSeg数据集规模庞大且多样性高，为病理AI研究提供了宝贵的资源。该模型不仅在性能上超越了现有模型，更重要的是，它通过提升诊断模型的可解释性，为临床决策提供了更强的循证支持，在精准肿瘤学领域具有重要意义。"}}
{"id": "2506.20991", "title": "TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation", "authors": ["Chade Li", "Pengju Zhang", "Yihong Wu"], "summary": "The rapid advancement of 3D vision-language models (VLMs) has spurred\nsignificant interest in interactive point cloud processing tasks, particularly\nfor real-world applications. However, existing methods often underperform in\npoint-level tasks, such as segmentation, due to missing direct 3D-text\nalignment, limiting their ability to link local 3D features with textual\ncontext. To solve this problem, we propose TSDASeg, a Two-Stage model coupled\nwith a Direct cross-modal Alignment module and memory module for interactive\npoint cloud Segmentation. We introduce the direct cross-modal alignment module\nto establish explicit alignment between 3D point clouds and textual/2D image\ndata. Within the memory module, we employ multiple dedicated memory banks to\nseparately store text features, visual features, and their cross-modal\ncorrespondence mappings. These memory banks are dynamically leveraged through\nself-attention and cross-attention mechanisms to update scene-specific features\nbased on prior stored data, effectively addressing inconsistencies in\ninteractive segmentation results across diverse scenarios. Experiments\nconducted on multiple 3D instruction, reference, and semantic segmentation\ndatasets demonstrate that the proposed method achieves state-of-the-art\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20991v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20991v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TSDASeg: 一种用于交互式点云分割的直接对齐两阶段模型", "tldr": "TSDASeg是一个两阶段模型，通过直接跨模态对齐和记忆模块，解决了现有方法在交互式点云分割中缺乏3D-文本直接对齐的问题，并在多个数据集上达到了最先进的性能。", "motivation": "现有方法在点级别任务（如分割）中表现不佳，因为缺乏直接的3D-文本对齐，限制了它们将局部3D特征与文本上下文关联的能力。", "method": "我们提出了TSDASeg，一个两阶段模型，结合了直接跨模态对齐模块和记忆模块用于交互式点云分割。直接跨模态对齐模块用于建立3D点云与文本/2D图像数据之间的显式对齐。记忆模块利用多个专用记忆库分别存储文本特征、视觉特征及其跨模态对应映射，并通过自注意力机制和交叉注意力机制动态更新场景特定特征，以解决不同场景下交互式分割结果的不一致性。", "result": "在多个3D指令、参考和语义分割数据集上进行的实验表明，所提出的方法取得了最先进的性能。", "conclusion": "TSDASeg通过引入直接跨模态对齐和记忆模块，有效解决了交互式点云分割中缺乏直接3D-文本对齐的问题，并在多项任务中实现了最先进的性能。", "translation": "3D视觉-语言模型（VLMs）的快速发展激发了人们对交互式点云处理任务的浓厚兴趣，尤其是在实际应用中。然而，现有方法在点级别任务（如分割）中往往表现不佳，原因在于缺乏直接的3D-文本对齐，这限制了它们将局部3D特征与文本上下文关联的能力。为了解决这个问题，我们提出了TSDASeg，一个两阶段模型，结合了直接跨模态对齐模块和记忆模块用于交互式点云分割。我们引入了直接跨模态对齐模块，以在3D点云和文本/2D图像数据之间建立显式对齐。在记忆模块中，我们采用多个专用记忆库来分别存储文本特征、视觉特征及其跨模态对应映射。这些记忆库通过自注意力机制和交叉注意力机制动态地利用，以基于先前存储的数据更新场景特定特征，从而有效解决在不同场景下交互式分割结果的不一致性。在多个3D指令、参考和语义分割数据集上进行的实验表明，所提出的方法取得了最先进的性能。", "summary": "本文提出了TSDASeg，一个用于交互式点云分割的两阶段模型，旨在解决现有方法因缺乏直接3D-文本对齐而在点级别任务中表现不佳的问题。TSDASeg引入了一个直接跨模态对齐模块，用于建立3D点云与文本/2D图像数据的显式对齐，并包含一个记忆模块，该模块利用多个记忆库存储并动态更新跨模态特征及其对应映射，以提高分割结果的一致性。实验证明，TSDASeg在多个3D分割数据集上实现了最先进的性能。", "keywords": "点云分割, 交互式分割, 跨模态对齐, 记忆网络, 3D视觉语言模型", "comments": "该论文的创新点在于提出了一个结合直接跨模态对齐和记忆模块的两阶段模型TSDASeg，以解决3D视觉-语言模型在交互式点云分割中缺乏直接3D-文本对齐的问题。通过显式对齐和动态记忆机制，该模型能够更好地将局部3D特征与文本上下文关联，提高了分割的准确性和跨场景的一致性。其在多个数据集上达到SOTA性能，表明了其重要性和有效性。"}}
{"id": "2506.20927", "title": "Interpretable Representation Learning for Additive Rule Ensembles", "authors": ["Shahrzad Behzadimanesh", "Pierre Le Bodic", "Geoffrey I. Webb", "Mario Boley"], "summary": "Small additive ensembles of symbolic rules offer interpretable prediction\nmodels. Traditionally, these ensembles use rule conditions based on\nconjunctions of simple threshold propositions $x \\geq t$ on a single input\nvariable $x$ and threshold $t$, resulting geometrically in axis-parallel\npolytopes as decision regions. While this form ensures a high degree of\ninterpretability for individual rules and can be learned efficiently using the\ngradient boosting approach, it relies on having access to a curated set of\nexpressive and ideally independent input features so that a small ensemble of\naxis-parallel regions can describe the target variable well. Absent such\nfeatures, reaching sufficient accuracy requires increasing the number and\ncomplexity of individual rules, which diminishes the interpretability of the\nmodel. Here, we extend classical rule ensembles by introducing logical\npropositions with learnable sparse linear transformations of input variables,\ni.e., propositions of the form $\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$, where\n$\\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as\ngeneral polytopes with oblique faces. We propose a learning method using\nsequential greedy optimization based on an iteratively reweighted formulation\nof logistic regression. Experimental results demonstrate that the proposed\nmethod efficiently constructs rule ensembles with the same test risk as\nstate-of-the-art methods while significantly reducing model complexity across\nten benchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20927v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20927v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可解释的加性规则集成表示学习", "tldr": "本文通过引入可学习的稀疏线性变换，扩展了经典的规则集成模型，使其能够生成具有倾斜面的决策区域，从而在保持相同预测风险的同时显著降低模型复杂性。", "motivation": "传统的加性规则集成模型依赖于精心设计且独立的输入特征。如果缺乏此类特征，为了达到足够的准确性，需要增加规则的数量和复杂性，这会损害模型的可解释性。", "method": "本文通过引入带有可学习稀疏线性变换的逻辑命题（形式为$\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$），扩展了经典的规则集成模型，使决策区域能够形成具有倾斜面的更通用多面体。学习方法采用基于迭代重加权逻辑回归的序贯贪婪优化。", "result": "实验结果表明，所提出的方法能够高效地构建规则集成模型，在十个基准数据集上达到与现有最先进方法相同的测试风险，同时显著降低了模型复杂性。", "conclusion": "本文提出的方法通过引入可学习的稀疏线性变换，成功地提升了加性规则集成的表现，使其在保持高可解释性的同时，能够在特征不理想的情况下依然获得高精度和低模型复杂度。", "translation": "符号规则的小型加性集成提供了可解释的预测模型。传统上，这些集成使用基于单个输入变量$x$和阈值$t$的简单阈值命题$x \\geq t$的合取作为规则条件，这在几何上导致轴平行多面体作为决策区域。虽然这种形式确保了单个规则的高度可解释性，并且可以使用梯度提升方法高效学习，但它依赖于访问一组经过精心策划的、表达力强且理想情况下独立的输入特征，以便少量轴平行区域的集成能够很好地描述目标变量。在缺乏此类特征的情况下，达到足够的准确性需要增加单个规则的数量和复杂性，这会降低模型的可解释性。本文通过引入带有可学习稀疏线性变换的输入变量的逻辑命题来扩展经典规则集成，即形式为$\\mathbf{x}^\\mathrm{T}\\mathbf{w} \\geq t$的命题，其中$\\mathbf{w}$是可学习的稀疏权重向量，从而使决策区域成为具有倾斜面的通用多面体。我们提出了一种基于逻辑回归的迭代重加权公式的序贯贪婪优化学习方法。实验结果表明，所提出的方法能够高效地构建规则集成模型，在十个基准数据集上达到与现有最先进方法相同的测试风险，同时显著降低了模型复杂性。", "summary": "本文提出了一种扩展的加性规则集成模型，旨在解决传统模型在缺乏高质量输入特征时可解释性下降的问题。通过引入可学习的稀疏线性变换，模型能够生成具有倾斜面的决策区域，从而更灵活地描述目标变量。该方法采用基于迭代重加权逻辑回归的序贯贪婪优化进行学习。实验证明，新方法在保持与现有最先进方法相同预测性能的同时，显著降低了模型复杂度，提升了可解释性。", "keywords": "可解释学习, 规则集成, 稀疏线性变换, 模型复杂度", "comments": "本文的创新之处在于将可解释的特征学习融入到规则集成模型中，通过允许规则条件基于输入变量的稀疏线性组合，解决了传统轴平行规则对特征质量的依赖问题。这使得模型在不牺牲准确性的前提下，能够应对更复杂的决策边界，并保持较高的可解释性，对于实际应用中特征工程的挑战具有重要意义。该方法通过降低模型复杂度实现了高效学习，是可解释人工智能领域的一个重要进展。"}}
{"id": "2506.21294", "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "authors": ["Bram Willemsen", "Gabriel Skantze"], "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.", "comment": "Accepted for publication at XLLM @ ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21294v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21294v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用自回归语言模型检测视觉基础对话中的指代表达", "tldr": "本文探索了使用纯文本自回归语言模型检测视觉基础对话中的指代表达，发现即使资源有限，纯文本方法也有效，但指出该任务本质上是多模态的。", "motivation": "研究仅凭语言上下文能在多大程度上帮助检测对话视觉上下文中具有（视觉上可感知）指代对象的提及。", "method": "调整一个预训练的大型语言模型（LLM），通过下一词元预测在文本中划定提及范围边界，对展开的对话中的提及范围进行相对粗粒度的标注。", "result": "研究结果表明，即使使用中等规模的LLM、相对较小的数据集和参数高效的微调，纯文本方法也能有效，这突出了语言上下文对于此任务的相对重要性。", "conclusion": "该任务本质上是一个多模态问题，单模态方法存在固有的局限性。", "translation": "在本文中，我们探索使用一种纯文本的自回归语言建模方法，从视觉基础对话中提取指代表达。更具体地说，目的是研究仅凭语言上下文能在多大程度上帮助检测对话视觉上下文中具有（视觉上可感知）指代对象的提及。为此，我们调整了一个预训练的大型语言模型（LLM），通过下一词元预测在文本中划定提及范围边界，对展开的对话中的提及范围进行相对粗粒度的标注。我们的研究结果表明，即使使用中等规模的LLM、相对较小的数据集和参数高效的微调，纯文本方法也能有效，这突出了语言上下文对于此任务的相对重要性。然而，我们认为该任务本质上是一个多模态问题，并讨论了单模态方法固有的局限性。", "summary": "本文探讨了使用纯文本自回归语言模型检测视觉基础对话中指代表达的有效性。通过调整预训练的LLM利用下一词元预测来识别提及范围，研究发现仅凭语言上下文就能出乎意料地有效，即使在资源有限的情况下。然而，论文也指出该任务本质上是多模态的，强调了单模态方法的固有局限性。", "keywords": "指代表达, 视觉基础对话, 自回归语言模型, 纯文本方法, 语言上下文", "comments": "创新点在于探索了纯文本方法在看似多模态任务中的出人意料的有效性，强调了语言上下文的关键作用。其重要性在于证明了语言上下文在指代表达检测中的强大能力。局限性在于论文明确指出该任务本质上是多模态的，这意味着纯文本方法存在根本性的限制，未来需要结合视觉信息。"}}
{"id": "2506.21234", "title": "Real-Time ESFP: Estimating, Smoothing, Filtering, and Pose-Mapping", "authors": ["Qifei Cui", "Yuang Zhou", "Ruichen Deng"], "summary": "This paper presents ESFP, an end-to-end pipeline that converts monocular RGB\nvideo into executable joint trajectories for a low-cost 4-DoF desktop arm. ESFP\ncomprises four sequential modules. (1) Estimating: ROMP lifts each frame to a\n24-joint 3-D skeleton. (2) Smoothing: the proposed HPSTM-a sequence-to-sequence\nTransformer with self-attention-combines long-range temporal context with a\ndifferentiable forward-kinematics decoder, enforcing constant bone lengths and\nanatomical plausibility while jointly predicting joint means and full\ncovariances. (3) Filtering: root-normalized trajectories are variance-weighted\naccording to HPSTM's uncertainty estimates, suppressing residual noise. (4)\nPose-Mapping: a geometric retargeting layer transforms shoulder-elbow-wrist\ntriples into the uArm's polar workspace, preserving wrist orientation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21234v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21234v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "实时ESFP：估计、平滑、滤波和姿态映射", "tldr": "ESFP是一个实时管道，将单目RGB视频转换为低成本机械臂的可执行关节轨迹。", "motivation": "旨在将单目RGB视频转换为低成本4自由度桌面机械臂的可执行关节轨迹。", "method": "本文提出了ESFP，一个端到端管道，包含四个顺序模块：(1) 估计：ROMP将每帧提升为24关节3D骨架。(2) 平滑：所提出的HPSTM（一个带有自注意力机制的序列到序列Transformer）将长程时间上下文与可微分正向运动学解码器相结合，强制骨骼长度恒定和解剖学合理性，同时共同预测关节均值和完整协方差。(3) 滤波：根据HPSTM的不确定性估计，对根部归一化的轨迹进行方差加权，以抑制残余噪声。(4) 姿态映射：几何重定向层将肩-肘-腕三元组转换为uArm的极坐标工作空间，同时保持腕部方向。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "本文提出了ESFP，这是一个端到端管道，可将单目RGB视频转换为低成本4自由度桌面机械臂的可执行关节轨迹。ESFP包含四个顺序模块。(1) 估计：ROMP将每帧提升为24关节3D骨架。(2) 平滑：所提出的HPSTM——一个带有自注意力机制的序列到序列Transformer——将长程时间上下文与可微分正向运动学解码器相结合，强制骨骼长度恒定和解剖学合理性，同时共同预测关节均值和完整协方差。(3) 滤波：根据HPSTM的不确定性估计，对根部归一化的轨迹进行方差加权，以抑制残余噪声。(4) 姿态映射：几何重定向层将肩-肘-腕三元组转换为uArm的极坐标工作空间，同时保持腕部方向。", "summary": "本文介绍了一个名为ESFP的实时端到端管道，旨在将单目RGB视频转换为低成本4自由度桌面机械臂的可执行关节轨迹。ESFP由四个模块组成：首先，ROMP估计每帧的24关节3D骨架；其次，HPSTM（一个带有可微分正向运动学解码器的Transformer）平滑骨架数据，确保骨长和解剖学合理性，并预测关节均值和协方差；接着，通过方差加权滤波抑制噪声；最后，姿态映射层将人体姿态转换为机械臂的极坐标工作空间，并保持腕部方向。", "keywords": "实时姿态估计, 机械臂控制, 单目视频, Transformer, 运动学", "comments": "该论文的创新之处在于其端到端的管道设计，能够将单目RGB视频转换为低成本机械臂的运动轨迹，这对于普及人机交互具有重要意义。特别是，所提出的HPSTM模块结合了可微分正向运动学解码器，有效地强制了骨骼长度和解剖学合理性，并能共同预测关节均值和协方差，这在处理人体姿态数据时是一个显著的进步。"}}
{"id": "2506.20941", "title": "Model State Arithmetic for Machine Unlearning", "authors": ["Keivan Rezaei", "Mehrdad Saberi", "Abhilasha Ravichander", "Soheil Feizi"], "summary": "Large language models are trained on massive corpora of web data, which may\ninclude private data, copyrighted material, factually inaccurate data, or data\nthat degrades model performance. Eliminating the influence of such problematic\ndatapoints through complete retraining -- by repeatedly pretraining the model\non datasets that exclude these specific instances -- is computationally\nprohibitive. For this reason, unlearning algorithms have emerged that aim to\neliminate the influence of particular datapoints, while otherwise preserving\nthe model -- at a low computational cost. However, precisely estimating and\nundoing the influence of individual datapoints has proved to be challenging. In\nthis work, we propose a new algorithm, MSA, for estimating and undoing the\ninfluence of datapoints -- by leveraging model checkpoints i.e. artifacts\ncapturing model states at different stages of pretraining. Our experimental\nresults demonstrate that MSA consistently outperforms existing machine\nunlearning algorithms across multiple benchmarks, models, and evaluation\nmetrics, suggesting that MSA could be an effective approach towards more\nflexible large language models that are capable of data erasure.", "comment": "Preprint. Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.20941v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20941v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于机器遗忘的模型状态算术", "tldr": "MSA是一种新的机器遗忘算法，通过利用模型检查点，在计算成本较低的情况下有效地消除大型语言模型中特定数据点的影响，且性能优于现有算法。", "motivation": "大型语言模型在训练时可能包含私有、受版权保护、不准确或降低性能的数据。通过完全重训练来消除这些数据的影响计算成本过高，因此需要低成本的遗忘算法。然而，精确估计和消除单个数据点的影响具有挑战性。", "method": "本文提出了一种名为MSA的新算法，用于估计和消除数据点的影响。该方法通过利用模型检查点，即捕获预训练不同阶段模型状态的工件来实现。", "result": "实验结果表明，MSA在多个基准、模型和评估指标上始终优于现有的机器遗忘算法。", "conclusion": "MSA可能是一种有效的方法，可以实现更灵活、能够进行数据擦除的大型语言模型。", "translation": "大型语言模型在海量的网络数据语料库上进行训练，其中可能包含私人数据、受版权保护的材料、事实不准确的数据或会降低模型性能的数据。通过完全重训练（即在排除这些特定实例的数据集上重复预训练模型）来消除这些有问题数据点的影响，计算成本过高。因此，出现了旨在消除特定数据点影响，同时以低计算成本保留模型的遗忘算法。然而，精确估计和消除单个数据点的影响已被证明具有挑战性。在这项工作中，我们提出了一种新的算法MSA，用于估计和消除数据点的影响——通过利用模型检查点，即捕获预训练不同阶段模型状态的工件。我们的实验结果表明，MSA在多个基准、模型和评估指标上始终优于现有的机器遗忘算法，这表明MSA可能是一种实现更灵活、能够进行数据擦除的大型语言模型的有效方法。", "summary": "本文提出了一种名为MSA的新型机器遗忘算法，旨在以低计算成本消除大型语言模型中特定数据点的影响。该算法通过利用预训练过程中的模型检查点来估计并撤销数据点的影响。实验结果表明，MSA在多个基准、模型和评估指标上均优于现有方法，为实现更灵活、支持数据擦除的大型语言模型提供了一条有效途径。", "keywords": "机器遗忘, 大型语言模型, 模型状态算术, 数据擦除, 模型检查点", "comments": "该论文提出了一种创新的机器遗忘方法，通过利用模型训练过程中的中间状态（检查点）来解决传统遗忘算法中精确估计和消除数据影响的难题。这种方法在计算效率和性能上均优于现有技术，对于提高大型语言模型的数据隐私保护和内容管理能力具有重要意义。其创新点在于将模型状态算术应用于遗忘问题，提供了一种新的视角和解决方案。"}}
{"id": "2506.21360", "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.", "comment": "Accepted in CogSci 2025", "pdf_url": "http://arxiv.org/pdf/2506.21360v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21360v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人工智能文学批评的结构主义方法：利用格雷马斯符号方阵处理大型语言模型", "tldr": "本文提出了GLASS框架，一个基于格雷马斯符号方阵的结构化分析框架，旨在增强大型语言模型进行深度文学分析的能力。", "motivation": "大型语言模型在理解和生成文本方面表现出色，但在为思想深刻、叙事复杂的作品提供专业的文学批评方面存在困难。", "method": "本文提出了GLASS（通过符号方阵进行的格雷马斯文学分析）框架，一个基于格雷马斯符号方阵（GSS）的结构化分析框架。研究构建了首个用于GSS文学批评的数据集，包含48部作品的详细分析。此外，研究提出了使用“LLM作为评判者”范式进行GSS文学批评的量化指标。", "result": "GLASS框架的结果与专家批评以及多个作品和大型语言模型进行了比较，显示出高性能。最后，将GLASS应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究空白。", "conclusion": "这项研究提供了一个基于人工智能的文学研究和教育工具，为文学参与背后的认知机制提供了见解。", "translation": "大型语言模型（LLMs）在理解和生成文本方面表现出色，但在为思想深刻、叙事复杂的作品提供专业的文学批评方面存在困难。本文提出了GLASS（通过符号方阵进行的格雷马斯文学分析），一个基于格雷马斯符号方阵（GSS）的结构化分析框架，旨在增强大型语言模型进行深度文学分析的能力。GLASS有助于快速剖析叙事作品中的叙事结构和深层含义。我们提出了首个用于GSS文学批评的数据集，包含48部作品的详细分析。然后，我们提出了使用“LLM作为评判者”范式进行GSS文学批评的量化指标。我们的框架结果与专家批评以及多个作品和大型语言模型进行了比较，显示出高性能。最后，我们将GLASS应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究空白。这项研究提供了一个基于人工智能的文学研究和教育工具，为文学参与背后的认知机制提供了见解。", "summary": "本文提出GLASS，一个基于格雷马斯符号方阵的结构化分析框架，旨在提升大型语言模型进行深度文学批评的能力。研究构建了首个GSS文学批评数据集，并提出了量化评估指标。实验结果表明，GLASS在与专家批评和多种LLM比较时表现出色，并成功应用于经典作品生成高质量分析，为文学研究和教育提供了新的AI工具。", "keywords": "文学批评, 大型语言模型, 格雷马斯符号方阵, 结构主义, 人工智能", "comments": "本文的创新点在于将结构主义文学批评理论（格雷马斯符号方阵）与大型语言模型结合，为LLM提供了一个系统的、可量化的文学分析框架。这不仅提升了LLM进行深度文学批评的能力，也为AI在人文领域的应用开辟了新路径，具有重要的理论和实践意义。"}}
{"id": "2506.20998", "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting", "authors": ["Yeon-Ji Song", "Jaein Kim", "Byung-Ju Kim", "Byoung-Tak Zhang"], "summary": "Novel view synthesis is a task of generating scenes from unseen perspectives;\nhowever, synthesizing dynamic scenes from blurry monocular videos remains an\nunresolved challenge that has yet to be effectively addressed. Existing novel\nview synthesis methods are often constrained by their reliance on\nhigh-resolution images or strong assumptions about static geometry and rigid\nscene priors. Consequently, their approaches lack robustness in real-world\nenvironments with dynamic object and camera motion, leading to instability and\ndegraded visual fidelity. To address this, we propose Motion-aware Dynamic View\nSynthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting\n(DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular\nvideos. Our model generates dense 3D Gaussians, restoring sharpness from blurry\nvideos and reconstructing detailed 3D geometry of the scene affected by dynamic\nmotion variations. Our model achieves robust performance in novel view\nsynthesis under dynamic blurry scenes and sets a new benchmark in realistic\nnovel view synthesis for blurry monocular video inputs.", "comment": "CVPRW 2025, Neural Fields Beyond Conventional Cameras", "pdf_url": "http://arxiv.org/pdf/2506.20998v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.20998v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DBMovi-GS：通过稀疏控制高斯泼溅从模糊单目视频进行动态视图合成", "tldr": "DBMovi-GS通过生成密集3D高斯，解决了从模糊单目视频合成动态场景的挑战，提高了视图合成的鲁棒性和视觉保真度。", "motivation": "现有新颖视图合成方法在处理动态模糊单目视频时面临挑战，因为它们依赖高分辨率图像或对静态几何、刚性场景的强假设，导致在真实世界动态环境中缺乏鲁棒性，视觉质量下降。", "method": "提出DBMovi-GS（Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting）方法，通过生成密集的3D高斯，从模糊视频中恢复清晰度并重建受动态运动变化影响的场景的详细3D几何。", "result": "DBMovi-GS在动态模糊场景下的新颖视图合成中取得了鲁棒性能，并为模糊单目视频输入下的真实新颖视图合成树立了新基准。", "conclusion": "DBMovi-GS成功解决了从模糊单目视频进行动态视图合成的难题，提高了鲁棒性并设定了新的性能标准。", "translation": "新颖视图合成是一项从未知视角生成场景的任务；然而，从模糊单目视频合成动态场景仍然是一个尚未有效解决的挑战。现有新颖视图合成方法通常受限于对高分辨率图像的依赖，或者对静态几何和刚性场景先验的强烈假设。因此，它们的方法在具有动态物体和相机运动的真实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度下降。为了解决这个问题，我们提出了DBMovi-GS（Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting），一种旨在从模糊单目视频进行动态视图合成的方法。我们的模型生成密集的3D高斯，从模糊视频中恢复清晰度并重建受动态运动变化影响的场景的详细3D几何。我们的模型在动态模糊场景下的新颖视图合成中取得了鲁棒性能，并为模糊单目视频输入下的真实新颖视图合成树立了新基准。", "summary": "本文提出了DBMovi-GS，一种针对从模糊单目视频进行动态视图合成的新方法。该方法通过生成密集的3D高斯来解决现有技术在处理动态模糊场景时的局限性，能够从模糊视频中恢复清晰度并重建详细的3D几何。实验结果表明，DBMovi-GS在动态模糊场景下的新颖视图合成中表现出鲁棒性，并为该领域设定了新的性能基准。", "keywords": "动态视图合成, 模糊单目视频, 高斯泼溅, 3D几何重建, 运动感知", "comments": "这篇论文解决了从模糊单目视频进行动态视图合成这一具有挑战性的实际问题，其创新点在于结合了运动感知和稀疏控制高斯泼溅技术，实现了从模糊输入中恢复清晰度和重建动态3D几何，显著提升了在该复杂场景下的视图合成质量和鲁棒性。"}}
{"id": "2506.20957", "title": "Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding", "authors": ["Jiameng Chen", "Xiantao Cai", "Jia Wu", "Wenbin Hu"], "summary": "Antibody design remains a critical challenge in therapeutic and diagnostic\ndevelopment, particularly for complex antigens with diverse binding interfaces.\nCurrent computational methods face two main limitations: (1) capturing\ngeometric features while preserving symmetries, and (2) generalizing novel\nantigen interfaces. Despite recent advancements, these methods often fail to\naccurately capture molecular interactions and maintain structural integrity. To\naddress these challenges, we propose \\textbf{AbMEGD}, an end-to-end framework\nintegrating \\textbf{M}ulti-scale \\textbf{E}quivariant \\textbf{G}raph\n\\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging\nadvanced geometric deep learning, AbMEGD combines atomic-level geometric\nfeatures with residue-level embeddings, capturing local atomic details and\nglobal sequence-structure interactions. Its E(3)-equivariant diffusion method\nensures geometric precision, computational efficiency, and robust\ngeneralizability for complex antigens. Furthermore, experiments using the\nSAbDab database demonstrate a 10.13\\% increase in amino acid recovery, 3.32\\%\nrise in improvement percentage, and a 0.062~\\AA\\ reduction in root mean square\ndeviation within the critical CDR-H3 region compared to DiffAb, a leading\nantibody design model. These results highlight AbMEGD's ability to balance\nstructural integrity with improved functionality, establishing a new benchmark\nfor sequence-structure co-design and affinity optimization. The code is\navailable at: https://github.com/Patrick221215/AbMEGD.", "comment": "9 pages, 4 figures, accepted at IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.20957v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1; J.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20957v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用多尺度等变图扩散模型进行抗体设计与优化以实现精确的复杂抗原结合", "tldr": "AbMEGD是一个新的端到端框架，它结合了多尺度等变图扩散模型，用于抗体序列和结构协同设计，显著提高了复杂抗原结合的准确性和功能性。", "motivation": "当前的计算方法在抗体设计中面临两大挑战：一是如何在保持对称性的同时捕获几何特征，二是如何泛化到新型抗原界面。这些方法往往无法准确捕获分子相互作用并维持结构完整性。", "method": "我们提出了AbMEGD，一个结合了多尺度等变图扩散的端到端框架，用于抗体序列和结构协同设计。它利用先进的几何深度学习，结合原子级几何特征与残基级嵌入，捕捉局部原子细节和全局序列-结构相互作用。其E(3)-等变扩散方法确保了几何精度、计算效率和对复杂抗原的鲁棒泛化能力。", "result": "在SAbDab数据库上的实验表明，与领先的抗体设计模型DiffAb相比，AbMEGD在关键的CDR-H3区域的氨基酸恢复率提高了10.13%，改进百分比提高了3.32%，均方根偏差减少了0.062 Å。", "conclusion": "这些结果突出显示了AbMEGD在平衡结构完整性与改进功能方面的能力，为序列-结构协同设计和亲和力优化建立了新的基准。", "translation": "抗体设计仍然是治疗和诊断开发中的一个关键挑战，特别是对于具有不同结合界面的复杂抗原。当前的计算方法面临两个主要限制：(1) 在保留对称性的同时捕获几何特征，以及 (2) 泛化新型抗原界面。尽管最近取得了进展，但这些方法通常无法准确捕获分子相互作用并保持结构完整性。为了解决这些挑战，我们提出了 AbMEGD，一个集成多尺度等变图扩散的端到端框架，用于抗体序列和结构协同设计。AbMEGD 利用先进的几何深度学习，将原子级几何特征与残基级嵌入相结合，捕捉局部原子细节和全局序列-结构相互作用。其 E(3)-等变扩散方法确保了几何精度、计算效率以及对复杂抗原的鲁棒泛化能力。此外，使用 SAbDab 数据库进行的实验表明，与领先的抗体设计模型 DiffAb 相比，在关键的 CDR-H3 区域，氨基酸恢复率提高了 10.13%，改进百分比提高了 3.32%，均方根偏差减少了 0.062 Å。这些结果突出显示了 AbMEGD 在平衡结构完整性与改进功能方面的能力，为序列-结构协同设计和亲和力优化建立了新的基准。代码可在：https://github.com/Patrick221215/AbMEGD 获取。", "summary": "本研究提出了AbMEGD，一个基于多尺度等变图扩散的端到端框架，旨在解决抗体设计中几何特征捕获和泛化能力不足的问题。AbMEGD结合原子级几何特征与残基级嵌入，通过E(3)-等变扩散方法实现精确、高效且泛化性强的抗体序列和结构协同设计。实验结果显示，AbMEGD在氨基酸恢复率、改进百分比和均方根偏差方面均优于现有模型，证明其在提升抗体功能性和结构完整性方面的卓越性能，为抗体设计领域树立了新标准。", "keywords": "抗体设计, 等变图扩散, 序列-结构协同设计, 复杂抗原结合, 几何深度学习", "comments": "AbMEGD的创新之处在于其将多尺度等变图扩散模型应用于抗体序列和结构协同设计，特别是通过E(3)-等变性确保了几何精度和泛化能力。这种方法有效地解决了现有计算方法在处理复杂抗原时的局限性，为抗体药物发现提供了强大的新工具。其在关键CDR-H3区域的显著性能提升，表明了该模型在实际应用中的潜力。"}}
{"id": "2506.21358", "title": "ToosiCubix: Monocular 3D Cuboid Labeling via Vehicle Part Annotations", "authors": ["Behrooz Nasihatkon", "Hossein Resani", "Amirreza Mehrzadian"], "summary": "Many existing methods for 3D cuboid annotation of vehicles rely on expensive\nand carefully calibrated camera-LiDAR or stereo setups, limiting their\naccessibility for large-scale data collection. We introduce ToosiCubix, a\nsimple yet powerful approach for annotating ground-truth cuboids using only\nmonocular images and intrinsic camera parameters. Our method requires only\nabout 10 user clicks per vehicle, making it highly practical for adding 3D\nannotations to existing datasets originally collected without specialized\nequipment. By annotating specific features (e.g., wheels, car badge,\nsymmetries) across different vehicle parts, we accurately estimate each\nvehicle's position, orientation, and dimensions up to a scale ambiguity (8\nDoF). The geometric constraints are formulated as an optimization problem,\nwhich we solve using a coordinate descent strategy, alternating between\nPerspective-n-Points (PnP) and least-squares subproblems. To handle common\nambiguities such as scale and unobserved dimensions, we incorporate\nprobabilistic size priors, enabling 9 DoF cuboid placements. We validate our\nannotations against the KITTI and Cityscapes3D datasets, demonstrating that our\nmethod offers a cost-effective and scalable solution for high-quality 3D cuboid\nannotation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21358v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21358v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ToosiCubix：通过车辆部件标注进行单目3D长方体标注", "tldr": "ToosiCubix 是一种新颖的单目3D长方体标注方法，只需少量用户点击即可在单目图像上为车辆生成高精度3D标注，解决了现有方法对昂贵设备依赖的问题，并展现出成本效益和可扩展性。", "motivation": "许多现有的车辆3D长方体标注方法依赖于昂贵且经过仔细校准的相机-LiDAR或立体设置，这限制了它们在大规模数据收集中的可及性。", "method": "ToosiCubix 仅使用单目图像和相机内参，通过用户点击标注车辆特定特征（如车轮、车标、对称性），精确估计车辆的位置、方向和尺寸（8自由度）。几何约束被表述为一个优化问题，通过坐标下降策略解决，交替进行PnP和最小二乘子问题。为处理尺度和未观测尺寸的歧义，引入概率尺寸先验，实现9自由度长方体放置。", "result": "该方法仅需每次车辆约10次用户点击。在KITTI和Cityscapes3D数据集上验证了其标注效果，证明该方法为高质量3D长方体标注提供了一种成本效益高且可扩展的解决方案。", "conclusion": "ToosiCubix 提供了一种成本效益高且可扩展的解决方案，用于使用单目图像和少量用户交互进行高质量的车辆3D长方体标注，克服了传统方法对昂贵硬件的依赖。", "translation": "许多现有的车辆3D长方体标注方法依赖于昂贵且经过仔细校准的相机-LiDAR或立体设置，这限制了它们在大规模数据收集中的可及性。我们引入了ToosiCubix，这是一种简单而强大的方法，仅使用单目图像和相机内参即可标注真实长方体。我们的方法每辆车只需大约10次用户点击，这使得它对于向最初未配备专业设备收集的现有数据集添加3D标注非常实用。通过标注不同车辆部件上的特定特征（例如车轮、车标、对称性），我们精确估计了每辆车的位置、方向和尺寸，但存在尺度模糊性（8自由度）。几何约束被表述为一个优化问题，我们使用坐标下降策略解决，在PnP（透视-n-点）和最小二乘子问题之间交替。为了处理常见的模糊性，如尺度和未观测尺寸，我们结合了概率尺寸先验，实现了9自由度长方体放置。我们在KITTI和Cityscapes3D数据集上验证了我们的标注，证明我们的方法为高质量3D长方体标注提供了一种成本效益高且可扩展的解决方案。", "summary": "ToosiCubix 提出了一种新颖的单目3D长方体标注方法，用于车辆标注，旨在解决现有方法对昂贵硬件（如相机-LiDAR或立体设置）的依赖。该方法仅需单目图像和内参，通过用户对车辆部件（如车轮、车标）进行约10次点击，即可精确估计车辆的8自由度位置、方向和尺寸。通过将几何约束建模为优化问题并采用坐标下降策略，结合PnP和最小二乘子问题解决。为处理尺度和未观测尺寸的歧义，ToosiCubix 引入了概率尺寸先验，实现了9自由度长方体放置。在KITTI和Cityscapes3D数据集上的验证表明，ToosiCubix 提供了一种成本效益高且可扩展的高质量3D长方体标注解决方案。", "keywords": "单目3D标注, 车辆长方体, 成本效益, 几何约束, 尺寸先验", "comments": "该论文的创新点在于提供了一种成本效益高且可扩展的单目3D长方体标注方法，显著降低了3D标注对昂贵硬件的依赖。通过巧妙地利用车辆部件特征和优化策略，实现了高精度的3D标注，并能集成到现有数据集中。其限制可能在于对用户点击精度和先验知识的依赖，以及在极端视角或遮挡情况下的鲁棒性。"}}
{"id": "2506.21001", "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology", "authors": ["Qiuyi Qi", "Xin Li", "Ming Kong", "Zikang Xu", "Bingdi Chen", "Qiang Zhu", "S Kevin Zhou"], "summary": "Challenges such as the lack of high-quality annotations, long-tailed data\ndistributions, and inconsistent staining styles pose significant obstacles to\ntraining neural networks to detect abnormal cells in cytopathology robustly.\nThis paper proposes a style-aligned image composition (SAIC) method that\ncomposes high-fidelity and style-preserved pathological images to enhance the\neffectiveness and robustness of detection models. Without additional training,\nSAIC first selects an appropriate candidate from the abnormal cell bank based\non attribute guidance. Then, it employs a high-frequency feature reconstruction\nto achieve a style-aligned and high-fidelity composition of abnormal cells and\npathological backgrounds. Finally, it introduces a large vision-language model\nto filter high-quality synthesis images. Experimental results demonstrate that\nincorporating SAIC-synthesized images effectively enhances the performance and\nrobustness of abnormal cell detection for tail categories and styles, thereby\nimproving overall detection performance. The comprehensive quality evaluation\nfurther confirms the generalizability and practicality of SAIC in clinical\napplication scenarios. Our code will be released at\nhttps://github.com/Joey-Qi/SAIC.", "comment": "MIDL 2025 Oral", "pdf_url": "http://arxiv.org/pdf/2506.21001v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21001v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于细胞病理学中异常细胞鲁棒检测的风格对齐图像合成", "tldr": "本文提出了一种风格对齐图像合成 (SAIC) 方法，通过合成高质量且风格保留的病理图像，以解决细胞病理学中异常细胞检测面临的标注不足、数据长尾分布和染色风格不一致等挑战，有效提升了检测模型的性能和鲁棒性。", "motivation": "细胞病理学中异常细胞检测面临高质量标注缺乏、数据长尾分布以及染色风格不一致等挑战，这些问题严重阻碍了神经网络训练出鲁棒的检测模型。", "method": "本文提出了一种风格对齐图像合成 (SAIC) 方法。该方法无需额外训练，首先根据属性指导从异常细胞库中选择合适的候选细胞；然后，利用高频特征重建技术实现异常细胞与病理背景的风格对齐和高保真合成；最后，引入大型视觉-语言模型来过滤高质量的合成图像。", "result": "实验结果表明，结合 SAIC 合成的图像能有效提升尾部类别和风格的异常细胞检测性能和鲁棒性，从而全面改善整体检测性能。全面的质量评估进一步证实了 SAIC 在临床应用场景中的通用性和实用性。", "conclusion": "SAIC 方法通过合成高质量、风格对齐的病理图像，有效克服了细胞病理学中异常细胞检测的挑战，显著提升了检测模型的性能和鲁棒性，并在临床应用中展现出良好的通用性和实用性。", "translation": "细胞病理学中异常细胞检测面临高质量标注不足、数据长尾分布以及染色风格不一致等挑战，这些因素严重阻碍了神经网络训练出鲁棒的检测模型。本文提出了一种风格对齐图像合成 (SAIC) 方法，该方法能够合成高保真且风格保留的病理图像，以增强检测模型的有效性和鲁棒性。SAIC 无需额外训练，首先根据属性指导从异常细胞库中选择合适的候选细胞。然后，它采用高频特征重建技术，实现异常细胞与病理背景的风格对齐和高保真合成。最后，引入一个大型视觉-语言模型来过滤高质量的合成图像。实验结果表明，结合 SAIC 合成的图像能有效提升尾部类别和风格的异常细胞检测性能和鲁棒性，从而全面改善整体检测性能。全面的质量评估进一步证实了 SAIC 在临床应用场景中的通用性和实用性。我们的代码将在 https://github.com/Joey-Qi/SAIC 发布。", "summary": "本文提出了一种名为风格对齐图像合成 (SAIC) 的方法，旨在解决细胞病理学中异常细胞检测面临的标注不足、数据不平衡和染色风格不一致等问题。SAIC 通过从异常细胞库中选择细胞并利用高频特征重建技术，合成高保真且风格与病理背景对齐的图像，并使用大型视觉-语言模型进行质量过滤。实验证明，SAIC 合成的图像能有效提升异常细胞检测模型在尾部类别和不同风格下的性能和鲁棒性，具有良好的临床应用前景。", "keywords": "图像合成, 异常细胞检测, 细胞病理学, 风格对齐, 数据增强", "comments": "该论文提出了一种创新的图像合成方法 SAIC，通过模拟真实病理图像的复杂性，有效解决了细胞病理学中数据稀缺和风格不一致的难题。其无需额外训练的特性和引入视觉-语言模型进行质量过滤的步骤是其亮点，提升了合成图像的实用性。这对于推动医学图像分析，特别是罕见病理的AI诊断具有重要意义。"}}
{"id": "2506.20990", "title": "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes", "authors": ["Yifan Yang", "Zhen Zhang", "Rupak Vignesh Swaminathan", "Jing Liu", "Nathan Susanj", "Zheng Zhang"], "summary": "Fine-tuning vision language models (VLMs) has achieved remarkable performance\nacross various downstream tasks; yet, it requires access to model gradients\nthrough backpropagation (BP), making them unsuitable for memory-constrained,\ninference-only edge devices. To address this limitation, previous work has\nexplored various BP-free fine-tuning methods. However, these approaches often\nrely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)\noptimization, and often fail to achieve satisfactory performance. In this\npaper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)\napproach, specifically designed to enhance the performance of ZO VLM\nfine-tuning via a sharpness-aware warm-up training. SharpZO features a\ntwo-stage optimization process: a sharpness-aware ES stage that globally\nexplores and smooths the loss landscape to construct a strong initialization,\nfollowed by a fine-grained local search via sparse ZO optimization. The entire\noptimization relies solely on forward passes. Detailed theoretical analysis and\nextensive experiments on CLIP models demonstrate that SharpZO significantly\nimproves accuracy and convergence speed, achieving up to 7% average gain over\nstate-of-the-art forward-only methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20990v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.20990v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SharpZO：通过仅前向传播的混合锐度感知视觉语言模型提示调优", "tldr": "SharpZO是一种仅前向传播的混合锐度感知零阶优化方法，用于VLM微调，显著提高了性能和收敛速度，优于现有方法。", "motivation": "现有的视觉语言模型（VLM）微调方法需要反向传播，不适用于内存受限、仅推理的边缘设备。先前的无反向传播微调方法（如进化策略或零阶优化）通常性能不佳。", "method": "本文提出了一种混合锐度感知零阶优化（SharpZO）方法，专门通过锐度感知热身训练来增强零阶VLM微调的性能。SharpZO包含一个两阶段优化过程：首先是锐度感知进化策略（ES）阶段，用于全局探索和平滑损失景观以构建强大的初始化；其次是通过稀疏零阶（ZO）优化进行细粒度局部搜索。整个优化过程仅依赖于前向传播。", "result": "SharpZO显著提高了准确性和收敛速度，在CLIP模型上实现了比最先进的仅前向传播方法平均高达7%的增益。", "conclusion": "SharpZO通过其混合锐度感知和两阶段优化过程，在仅前向传播的VLM微调中取得了显著的性能提升和更快的收敛速度，有效解决了内存受限设备的挑战。", "translation": "对视觉语言模型（VLM）进行微调已在各种下游任务中取得了显著性能；然而，它需要通过反向传播（BP）访问模型梯度，这使得它们不适用于内存受限、仅推理的边缘设备。为了解决这一限制，先前的工作探索了各种无需BP的微调方法。然而，这些方法通常依赖于高方差的进化策略（ES）或零阶（ZO）优化，并且往往未能达到令人满意的性能。在本文中，我们提出了一种混合锐度感知零阶优化（SharpZO）方法，专门设计用于通过锐度感知热身训练来增强ZO VLM微调的性能。SharpZO具有两阶段优化过程：一个锐度感知ES阶段，用于全局探索和平滑损失景观以构建强大的初始化，随后是通过稀疏ZO优化进行细粒度局部搜索。整个优化过程仅依赖于前向传播。详细的理论分析和在CLIP模型上的大量实验表明，SharpZO显著提高了准确性和收敛速度，比最先进的仅前向传播方法平均增益高达7%。", "summary": "本文提出SharpZO，一种针对视觉语言模型（VLM）提示调优的混合锐度感知零阶优化方法，旨在解决传统微调对内存和反向传播的依赖，使其适用于边缘设备。SharpZO采用两阶段优化：首先通过锐度感知进化策略进行全局探索以获得良好初始化，然后进行稀疏零阶局部搜索。该方法仅依赖前向传播，并在实验中显示出显著提高准确性和收敛速度，优于现有仅前向传播方法。", "keywords": "锐度感知, 视觉语言模型, 提示调优, 零阶优化, 仅前向传播", "comments": "该论文的创新点在于提出了一个混合的、仅前向传播的优化框架（SharpZO），将锐度感知与零阶优化结合起来，以提高VLM在内存受限设备上的微调性能。其两阶段设计（全局探索与局部搜索）以及对锐度感知的引入，有效解决了传统零阶方法性能不佳的痛点，为边缘AI设备的模型部署提供了实用的解决方案，具有重要的应用价值。"}}
{"id": "2506.21443", "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "authors": ["Ali Şenol", "Garima Agrawal", "Huan Liu"], "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21443v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21443v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "领域知识增强型大型语言模型用于欺诈和概念漂移检测", "tldr": "大型语言模型在欺诈和概念漂移检测中面临挑战，本文提出一个领域知识增强型LLM框架，结合漂移检测单元，可准确识别欺诈性对话并分类漂移性质。", "motivation": "在动态平台检测欺骗性对话因语言模式演变和概念漂移（语义或主题变化）而日益困难。大型语言模型在风险敏感场景中常受上下文模糊性和幻觉困扰，难以准确分类。", "method": "本文提出了一个领域知识（DK）增强型大型语言模型框架，将预训练LLM与结构化、任务特定洞察相结合，用于欺诈和概念漂移检测。该架构包含三个主要组件：(1)用于检测虚假或欺骗性对话的DK-LLM模块；(2)用于确定语义漂移是否发生的漂移检测单元（OCDD）；(3)第二个DK-LLM模块用于将漂移分类为良性或欺诈性。该框架在虚假评论数据集和多轮对话数据集SEConvo上进行了验证。", "result": "该系统能高精度检测虚假对话并有效分类漂移性质。基于LLaMA的实现达到了98%的分类准确率。与零样本基线相比，结合领域知识和漂移感知显著提高了高风险自然语言处理应用的性能、可解释性和鲁棒性。", "conclusion": "将领域知识和漂移感知融入大型语言模型显著提高了其在高风险自然语言处理应用中检测欺诈和概念漂移的性能、可解释性和鲁棒性。", "translation": "在动态平台上检测欺骗性对话变得越来越困难，原因在于不断演变的语言模式和概念漂移（Concept Drift，CD）——即语义或主题随时间推移而改变交互上下文或意图。这些变化可能模糊恶意意图或模仿正常对话，使得准确分类变得具有挑战性。虽然大型语言模型（LLM）在自然语言任务中表现出色，但它们在风险敏感场景中经常在上下文模糊性和幻觉方面遇到困难。为了解决这些挑战，我们提出了一个领域知识（DK）增强型LLM框架，该框架将预训练LLM与结构化、任务特定洞察相结合，以执行欺诈和概念漂移检测。所提出的架构包括三个主要组件：(1)一个DK-LLM模块用于检测虚假或欺骗性对话；(2)一个漂移检测单元（OCDD）用于确定是否发生了语义漂移；(3)第二个DK-LLM模块用于将漂移分类为良性或欺诈性。我们首先使用一个虚假评论数据集验证了领域知识的价值，然后将我们的完整框架应用于SEConvo，这是一个包含各种类型欺诈和垃圾邮件攻击的多轮对话数据集。结果显示，我们的系统能够高精度地检测虚假对话，并有效分类漂移的性质。在结构化提示的指导下，基于LLaMA的实现达到了98%的分类准确率。与零样本基线的比较研究表明，在高风险的自然语言处理应用中，结合领域知识和漂移感知显著提高了性能、可解释性和鲁棒性。", "summary": "本文提出一个领域知识（DK）增强型大型语言模型（LLM）框架，旨在解决动态平台上的欺骗性对话和概念漂移检测挑战。该框架将预训练LLM与结构化领域洞察相结合，包含用于欺骗检测的DK-LLM、用于语义漂移的漂移检测单元（OCDD）以及另一个DK-LLM用于分类漂移性质。在虚假评论和SEConvo数据集上进行评估，该系统在检测虚假对话和分类漂移方面表现出高准确性（基于LLaMA实现达到98%），与零样本基线相比，显著提升了高风险自然语言处理应用的性能、可解释性和鲁棒性。", "keywords": "欺诈检测, 概念漂移, 大型语言模型, 领域知识, 欺骗性对话", "comments": "本文的创新点在于将领域知识和漂移检测机制有效地整合到大型语言模型中，以解决高风险场景下欺诈检测的动态性和复杂性问题。其重要性在于显著提高了模型在面对不断演变的语言模式和语义漂移时的准确性、可解释性和鲁棒性，为实际应用中的欺诈识别提供了更可靠的解决方案。"}}
{"id": "2506.21420", "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting", "authors": ["Taoyu Wu", "Yiyi Miao", "Zhuoxiao Li", "Haocheng Zhao", "Kang Dang", "Jionglong Su", "Limin Yu", "Haoang Li"], "summary": "Efficient three-dimensional reconstruction and real-time visualization are\ncritical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian\nSplatting (3DGS) has demonstrated remarkable performance in efficient 3D\nreconstruction and rendering. Most 3DGS-based Simultaneous Localization and\nMapping (SLAM) methods only rely on the appearance constraints for optimizing\nboth 3DGS and camera poses. However, in endoscopic scenarios, the challenges\ninclude photometric inconsistencies caused by non-Lambertian surfaces and\ndynamic motion from breathing affects the performance of SLAM systems. To\naddress these issues, we additionally introduce optical flow loss as a\ngeometric constraint, which effectively constrains both the 3D structure of the\nscene and the camera motion. Furthermore, we propose a depth regularisation\nstrategy to mitigate the problem of photometric inconsistencies and ensure the\nvalidity of 3DGS depth rendering in endoscopic scenes. In addition, to improve\nscene representation in the SLAM system, we improve the 3DGS refinement\nstrategy by focusing on viewpoints corresponding to Keyframes with suboptimal\nrendering quality frames, achieving better rendering results. Extensive\nexperiments on the C3VD static dataset and the StereoMIS dynamic dataset\ndemonstrate that our method outperforms existing state-of-the-art methods in\nnovel view synthesis and pose estimation, exhibiting high performance in both\nstatic and dynamic surgical scenes. The source code will be publicly available\nupon paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21420v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21420v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EndoFlow-SLAM：基于流约束高斯溅射的实时内窥镜SLAM", "tldr": "EndoFlow-SLAM是一种实时内窥镜SLAM方法，它利用三维高斯溅射，并通过引入光流损失和深度正则化来解决内窥镜场景中的光度不一致和动态运动问题，性能优于现有方法。", "motivation": "在内窥镜等手术场景中，高效的三维重建和实时可视化至关重要。现有的基于3DGS的SLAM方法主要依赖于外观约束，这在内窥镜场景中表现不佳，因为存在非朗伯表面引起的光度不一致和呼吸等动态运动。", "method": "该方法引入光流损失作为几何约束来优化3DGS和相机姿态，有效约束场景的三维结构和相机运动。此外，它提出了一种深度正则化策略以减轻光度不一致问题并确保3DGS深度渲染在内窥镜场景中的有效性。为了改善SLAM系统中的场景表示，该方法还改进了3DGS细化策略，通过关注与渲染质量欠佳的关键帧对应的视点来提高渲染效果。", "result": "该方法在新的视图合成和姿态估计方面优于现有的最先进方法。在C3VD静态数据集和StereoMIS动态数据集上的实验表明，它在静态和动态手术场景中均表现出高性能。", "conclusion": "EndoFlow-SLAM方法通过结合光流损失和深度正则化，有效解决了内窥镜场景中的挑战，并在三维重建和姿态估计方面实现了优于现有方法的性能。", "translation": "在内窥镜等手术场景中，高效的三维重建和实时可视化至关重要。近年来，三维高斯溅射（3DGS）在高效三维重建和渲染方面表现出卓越的性能。大多数基于3DGS的同步定位与建图（SLAM）方法仅依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致以及呼吸引起的动态运动影响SLAM系统的性能。为了解决这些问题，我们额外引入了光流损失作为几何约束，它有效地约束了场景的三维结构和相机运动。此外，我们提出了一种深度正则化策略，以减轻光度不一致问题并确保3DGS深度渲染在内窥镜场景中的有效性。为了改善SLAM系统中的场景表示，我们改进了3DGS细化策略，通过关注与渲染质量欠佳的关键帧对应的视点，实现了更好的渲染效果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中均表现出高性能。源代码将在论文接收后公开发布。", "summary": "本文介绍了一种名为EndoFlow-SLAM的新型实时内窥镜SLAM系统，该系统利用三维高斯溅射技术。它通过引入光流损失作为几何约束并提出深度正则化策略，解决了内窥镜环境中光度不一致和动态运动等挑战。该方法还改进了3DGS的细化策略。在静态和动态数据集上的实验结果表明，与现有最先进方法相比，EndoFlow-SLAM在新的视图合成和姿态估计方面表现出卓越的性能。", "keywords": "内窥镜SLAM, 三维高斯溅射, 光流, 深度正则化, 实时重建", "comments": "该论文通过专门解决手术环境的独特挑战，在内窥镜SLAM领域取得了显著进展。将光流损失作为几何约束的集成以及新颖的深度正则化策略是关键创新点，它们增强了基于3DGS的SLAM在非朗伯和动态场景中的鲁棒性和准确性。改进的3DGS细化也有助于更好的场景表示，这对于手术应用至关重要。在静态和动态数据集上的实验验证进一步证实了其高性能的主张。"}}
{"id": "2506.21002", "title": "Inverse Scene Text Removal", "authors": ["Takumi Yoshimatsu", "Shumpei Takezaki", "Seiichi Uchida"], "summary": "Scene text removal (STR) aims to erase textual elements from images. It was\noriginally intended for removing privacy-sensitiveor undesired texts from\nnatural scene images, but is now also appliedto typographic images. STR\ntypically detects text regions and theninpaints them. Although STR has advanced\nthrough neural networksand synthetic data, misuse risks have increased. This\npaper investi-gates Inverse STR (ISTR), which analyzes STR-processed images\nandfocuses on binary classification (detecting whether an image has un-dergone\nSTR) and localizing removed text regions. We demonstrate inexperiments that\nthese tasks are achievable with high accuracies, en-abling detection of\npotential misuse and improving STR. We also at-tempt to recover the removed\ntext content by training a text recognizerto understand its difficulty.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.21002v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21002v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "逆向场景文本移除", "tldr": "本文研究逆向场景文本移除（ISTR），旨在检测图像是否经过场景文本移除（STR）处理，定位被移除的文本区域，并尝试恢复被移除的文本内容，以应对STR的滥用风险。", "motivation": "场景文本移除（STR）技术虽然旨在移除图像中的敏感或不希望出现的文本，但其滥用风险也随之增加。因此，需要一种方法来检测图像是否经过STR处理以及被移除的文本区域，从而应对潜在的滥用并改进STR技术。", "method": "本文研究逆向场景文本移除（ISTR）。主要方法包括：1) 对图像进行二分类，判断其是否经过STR处理。2) 定位被移除的文本区域。3) 训练一个文本识别器，尝试恢复被移除的文本内容，以评估其难度。", "result": "实验表明，ISTR的二分类和定位任务能够以高精度实现，这使得检测潜在滥用和改进STR成为可能。同时，恢复被移除文本内容的任务被证明具有一定难度。", "conclusion": "逆向场景文本移除（ISTR）技术能够有效检测图像是否经过STR处理并定位被移除的文本区域，这对于识别STR的滥用和改进STR技术具有重要意义。尽管恢复被移除文本内容具有挑战性，但该研究为未来的探索奠定了基础。", "translation": "场景文本移除（STR）旨在从图像中擦除文本元素。它最初是为了从自然场景图像中移除隐私敏感或不希望出现的文本，但现在也应用于印刷图像。STR通常会检测文本区域，然后对其进行图像修复。尽管STR通过神经网络和合成数据取得了进展，但滥用风险也随之增加。本文研究逆向STR（ISTR），它分析经过STR处理的图像，并侧重于二分类（检测图像是否经过STR处理）和定位被移除的文本区域。我们在实验中证明，这些任务可以高精度实现，从而能够检测潜在的滥用并改进STR。我们还尝试通过训练一个文本识别器来恢复被移除的文本内容，以了解其难度。", "summary": "本文提出逆向场景文本移除（ISTR）技术，旨在分析经过场景文本移除（STR）处理的图像。ISTR主要关注检测图像是否经过STR处理、定位被移除的文本区域，并通过实验证明这些任务可以高精度完成，有助于检测STR的滥用并改进STR。此外，研究还尝试恢复被移除的文本内容，以评估其难度。", "keywords": "逆向场景文本移除, 场景文本移除, 文本检测, 文本恢复, 滥用检测", "comments": "这项研究具有创新性，因为它逆向思考了现有的场景文本移除技术，提出了检测其滥用和改进其效果的方法。其重要性在于应对了AI技术潜在的伦理和安全风险，为图像内容溯源提供了新的思路。"}}
{"id": "2506.21003", "title": "Distilling Normalizing Flows", "authors": ["Steven Walton", "Valeriy Klyukin", "Maksim Artemev", "Denis Derkach", "Nikita Orlov", "Humphrey Shi"], "summary": "Explicit density learners are becoming an increasingly popular technique for\ngenerative models because of their ability to better model probability\ndistributions. They have advantages over Generative Adversarial Networks due to\ntheir ability to perform density estimation and having exact latent-variable\ninference. This has many advantages, including: being able to simply\ninterpolate, calculate sample likelihood, and analyze the probability\ndistribution. The downside of these models is that they are often more\ndifficult to train and have lower sampling quality.\n  Normalizing flows are explicit density models, that use composable bijective\nfunctions to turn an intractable probability function into a tractable one. In\nthis work, we present novel knowledge distillation techniques to increase\nsampling quality and density estimation of smaller student normalizing flows.\nWe seek to study the capacity of knowledge distillation in Compositional\nNormalizing Flows to understand the benefits and weaknesses provided by these\narchitectures. Normalizing flows have unique properties that allow for a\nnon-traditional forms of knowledge transfer, where we can transfer that\nknowledge within intermediate layers. We find that through this distillation,\nwe can make students significantly smaller while making substantial performance\ngains over a non-distilled student. With smaller models there is a\nproportionally increased throughput as this is dependent upon the number of\nbijectors, and thus parameters, in the network.", "comment": "Published in eLVM @ CVPR\n  (https://openaccess.thecvf.com/content/CVPR2025W/eLVM/html/Walton_Distilling_Normalizing_Flows_CVPRW_2025_paper)", "pdf_url": "http://arxiv.org/pdf/2506.21003v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21003v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "蒸馏归一化流", "tldr": "本文提出了一种新颖的知识蒸馏技术，用于提高小型学生归一化流的采样质量和密度估计，发现通过蒸馏可以显著缩小模型并提高性能。", "motivation": "显式密度学习器作为生成模型日益流行，但它们通常训练更困难且采样质量较低。本研究旨在通过知识蒸馏技术提高小型归一化流的采样质量和密度估计，并探索知识蒸馏在组合归一化流中的潜力。", "method": "本文提出了新颖的知识蒸馏技术，用于提高小型学生归一化流的采样质量和密度估计。研究了知识蒸馏在组合归一化流中的能力，并利用归一化流的独特属性，实现中间层的非传统知识转移。", "result": "研究发现，通过知识蒸馏，可以使学生模型显著缩小，同时相比未蒸馏的学生模型，性能有大幅提升。更小的模型会带来成比例的吞吐量增加，因为这取决于网络中的双射器（即参数）数量。", "conclusion": "知识蒸馏能够有效提升小型归一化流的性能和效率，使其在保持甚至超越性能的同时，实现模型尺寸的大幅缩小。", "translation": "显式密度学习器作为生成模型正变得越来越流行，因为它们能够更好地建模概率分布。与生成对抗网络相比，它们具有执行密度估计和精确潜在变量推理的优势。这带来了许多优点，包括：能够简单地进行插值、计算样本似然和分析概率分布。这些模型的缺点是它们通常更难训练且采样质量较低。\n归一化流是显式密度模型，它们使用可组合的双射函数将一个难以处理的概率函数转换为一个可处理的函数。在这项工作中，我们提出了新颖的知识蒸馏技术，以提高小型学生归一化流的采样质量和密度估计。我们旨在研究知识蒸馏在组合归一化流中的能力，以了解这些架构所带来的优点和缺点。归一化流具有独特的特性，允许非传统的知识转移形式，我们可以在中间层转移知识。我们发现，通过这种蒸馏，我们可以使学生模型显著缩小，同时相比未蒸馏的学生模型，性能有大幅提升。随着模型尺寸的缩小，吞吐量会按比例增加，因为这取决于网络中双射器（即参数）的数量。", "summary": "本文针对显式密度学习器（特别是归一化流）训练困难和采样质量低的缺点，提出了一种新颖的知识蒸馏技术。该方法利用归一化流的独特结构，实现了中间层的知识转移。实验结果表明，通过这种蒸馏，能够显著缩小学生归一化流的模型尺寸，同时在采样质量和密度估计方面取得显著性能提升，并带来更高的吞吐量。", "keywords": "知识蒸馏, 归一化流, 生成模型, 密度估计, 模型压缩", "comments": "本文的创新点在于将知识蒸馏技术应用于归一化流，并特别探索了在中间层进行知识转移的可能性。这对于解决归一化流模型庞大且训练困难的问题具有重要意义，使得小型模型也能达到高性能，从而提高其实用性和部署效率。"}}
{"id": "2506.21427", "title": "Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning", "authors": ["Prajwal Koirala", "Cody Fleming"], "summary": "Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21427v1", "categories": ["cs.LG", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21427v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于流的单步完成：高效且富有表现力的策略学习", "tldr": "本文提出了一种名为单步完成策略（SSCP）的新型生成式策略，它通过增强的流匹配目标进行训练，能够实现一次性动作生成，从而结合了生成模型的表达能力和单峰策略的训练与推理效率，解决了现有生成模型在离线强化学习中迭代采样带来的高推理成本和训练不稳定性问题。", "motivation": "现有生成模型（如扩散模型和流匹配模型）虽然能为离线强化学习提供富有表现力的策略，捕获丰富、多模态的动作分布，但其迭代采样引入了高昂的推理成本和由于跨采样步骤的梯度传播导致的训练不稳定性。", "method": "本文提出了“单步完成策略”（SSCP），这是一种生成式策略，通过增强的流匹配目标进行训练，旨在从中间流样本预测直接完成向量，从而实现准确的一次性动作生成。SSCP在一个离策略actor-critic框架中运行，并可扩展到离线、离线到在线以及在线强化学习设置。此外，SSCP还被扩展到目标条件强化学习。", "result": "SSCP在速度和适应性方面比基于扩散的基线模型有了显著提升。它在标准离线强化学习和行为克隆基准测试中取得了优异的结果，证明了其作为深度强化学习和序列决策框架的多功能性、表达性和高效性。", "conclusion": "SSCP是一种多功能、富有表现力且高效的深度强化学习和序列决策框架，它通过一次性动作生成结合了生成模型的表达能力和单峰策略的效率，有效解决了现有生成模型在强化学习中的局限性。", "translation": "生成模型，如扩散模型和流匹配，通过捕获丰富、多模态的动作分布，为离线强化学习（RL）提供了富有表现力的策略，但其迭代采样引入了高昂的推理成本和由于跨采样步骤的梯度传播导致的训练不稳定性。我们提出了“单步完成策略”（SSCP），这是一种通过增强的流匹配目标训练的生成式策略，用于预测来自中间流样本的直接完成向量，从而实现准确的一次性动作生成。在一个离策略actor-critic框架中，SSCP结合了生成模型的表达能力与单峰策略的训练和推理效率，无需冗长的反向传播链。我们的方法能有效地扩展到离线、离线到在线和在线RL设置，在速度和适应性方面比基于扩散的基线模型提供了显著提升。我们进一步将SSCP扩展到目标条件RL，使扁平策略能够利用子目标结构而无需显式分层推理。SSCP在标准离线RL和行为克隆基准测试中取得了优异的结果，将其定位为深度RL和序列决策的多功能、富有表现力且高效的框架。", "summary": "本文提出了一种名为单步完成策略（SSCP）的新型生成式策略，旨在解决现有生成模型在离线强化学习中因迭代采样导致的高推理成本和训练不稳定性问题。SSCP通过增强的流匹配目标进行训练，能够实现一次性、准确的动作生成，有效结合了生成模型的表达能力和传统单峰策略的训练与推理效率。该方法在一个离策略actor-critic框架中实现，并被证明在离线、离线到在线和在线强化学习设置中均能有效扩展，且在速度和适应性上优于扩散基线模型。SSCP还扩展到目标条件强化学习，并在多项基准测试中取得了出色表现，展现了其在深度强化学习领域的通用性、表达性和高效性。", "keywords": "强化学习, 生成模型, 流匹配, 单步完成策略, 离线RL", "comments": "SSCP的创新之处在于它通过“单步完成”机制，实现了生成模型的一次性动作生成，极大地提升了推理效率并增强了训练稳定性，同时保留了生成模型捕获复杂动作分布的能力。这解决了现有基于流或扩散的生成策略在强化学习中面临的核心挑战，使其在实际应用中更具可行性。其在多RL设置下的普适性和对目标条件RL的扩展也增加了其重要性。"}}
{"id": "2506.21005", "title": "VisionGuard: Synergistic Framework for Helmet Violation Detection", "authors": ["Lam-Huy Nguyen", "Thinh-Phuc Nguyen", "Thanh-Hai Nguyen", "Gia-Huy Dinh", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Enforcing helmet regulations among motorcyclists is essential for enhancing\nroad safety and ensuring the effectiveness of traffic management systems.\nHowever, automatic detection of helmet violations faces significant challenges\ndue to environmental variability, camera angles, and inconsistencies in the\ndata. These factors hinder reliable detection of motorcycles and riders and\ndisrupt consistent object classification. To address these challenges, we\npropose VisionGuard, a synergistic multi-stage framework designed to overcome\nthe limitations of frame-wise detectors, especially in scenarios with class\nimbalance and inconsistent annotations. VisionGuard integrates two key\ncomponents: Adaptive Labeling and Contextual Expander modules. The Adaptive\nLabeling module is a tracking-based refinement technique that enhances\nclassification consistency by leveraging a tracking algorithm to assign\npersistent labels across frames and correct misclassifications. The Contextual\nExpander module improves recall for underrepresented classes by generating\nvirtual bounding boxes with appropriate confidence scores, effectively\naddressing the impact of data imbalance. Experimental results show that\nVisionGuard improves overall mAP by 3.1% compared to baseline detectors,\ndemonstrating its effectiveness and potential for real-world deployment in\ntraffic surveillance systems, ultimately promoting safety and regulatory\ncompliance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21005v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21005v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "VisionGuard: 头盔违规检测的协同框架", "tldr": "VisionGuard是一个多阶段框架，通过自适应标签和上下文扩展模块解决了自动头盔违规检测中的挑战，提高了检测精度和一致性。", "motivation": "自动检测头盔违规面临环境变异性、摄像机角度和数据不一致等挑战，这些因素阻碍了摩托车和骑手的可靠检测以及一致的对象分类。因此，需要一个更鲁棒的框架来提高道路安全和交通管理效率。", "method": "提出VisionGuard，一个协同多阶段框架，包含两个关键组件：自适应标签模块和上下文扩展模块。自适应标签模块是一种基于跟踪的细化技术，通过利用跟踪算法在帧间分配持久标签并纠正错误分类，从而增强分类一致性。上下文扩展模块通过生成具有适当置信分数的虚拟边界框，提高了代表性不足类别的召回率，有效解决了数据不平衡的影响。", "result": "实验结果表明，VisionGuard相比基线检测器，整体mAP提高了3.1%。", "conclusion": "VisionGuard在头盔违规检测方面表现出有效性，并具有在交通监控系统中实际部署的潜力，最终促进了安全和法规遵从。", "translation": "在摩托车手之间执行头盔规定对于提高道路安全和确保交通管理系统的有效性至关重要。然而，由于环境变异性、摄像机角度和数据不一致性，自动检测头盔违规面临重大挑战。这些因素阻碍了摩托车和骑手的可靠检测，并干扰了对象的一致分类。为了应对这些挑战，我们提出了VisionGuard，一个协同多阶段框架，旨在克服帧级检测器的局限性，特别是在类别不平衡和注释不一致的场景中。VisionGuard集成了两个关键组件：自适应标签模块和上下文扩展模块。自适应标签模块是一种基于跟踪的细化技术，通过利用跟踪算法在帧间分配持久标签并纠正错误分类，从而增强分类一致性。上下文扩展模块通过生成具有适当置信分数的虚拟边界框，提高了代表性不足类别的召回率，有效解决了数据不平衡的影响。实验结果表明，与基线检测器相比，VisionGuard的整体mAP提高了3.1%，证明了其在交通监控系统中实际部署的有效性和潜力，最终促进了安全和法规遵从。", "summary": "本研究提出VisionGuard，一个协同多阶段框架，旨在解决自动头盔违规检测中因环境变异性、摄像头角度和数据不一致导致的挑战。该框架包含自适应标签模块（通过跟踪提高分类一致性）和上下文扩展模块（通过生成虚拟边界框解决数据不平衡）。实验结果显示，VisionGuard将整体mAP提高了3.1%，证明了其在交通监控系统中的有效性和实际部署潜力。", "keywords": "头盔检测, 交通安全, 目标检测, 数据不平衡, 跟踪算法", "comments": "VisionGuard的创新之处在于其协同多阶段框架，特别是结合了跟踪算法进行标签细化和通过生成虚拟边界框来处理数据不平衡。这种方法有效地解决了现有帧级检测器在复杂交通场景中的局限性，对于提升交通安全监控的自动化水平具有重要意义。其模块化的设计也可能使其易于集成到现有系统中。"}}
{"id": "2506.21028", "title": "TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence", "authors": ["Feng Jiang", "Mangal Prakash", "Hehuan Ma", "Jianyuan Deng", "Yuzhi Guo", "Amina Mollaysa", "Tommaso Mansi", "Rui Liao", "Junzhou Huang"], "summary": "Molecular property prediction aims to learn representations that map chemical\nstructures to functional properties. While multimodal learning has emerged as a\npowerful paradigm to learn molecular representations, prior works have largely\noverlooked textual and taxonomic information of molecules for representation\nlearning. We introduce TRIDENT, a novel framework that integrates molecular\nSMILES, textual descriptions, and taxonomic functional annotations to learn\nrich molecular representations. To achieve this, we curate a comprehensive\ndataset of molecule-text pairs with structured, multi-level functional\nannotations. Instead of relying on conventional contrastive loss, TRIDENT\nemploys a volume-based alignment objective to jointly align tri-modal features\nat the global level, enabling soft, geometry-aware alignment across modalities.\nAdditionally, TRIDENT introduces a novel local alignment objective that\ncaptures detailed relationships between molecular substructures and their\ncorresponding sub-textual descriptions. A momentum-based mechanism dynamically\nbalances global and local alignment, enabling the model to learn both broad\nfunctional semantics and fine-grained structure-function mappings. TRIDENT\nachieves state-of-the-art performance on 11 downstream tasks, demonstrating the\nvalue of combining SMILES, textual, and taxonomic functional annotations for\nmolecular property prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21028v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21028v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TRIDENT: 三模态分子表示学习与分类学注释和局部对应", "tldr": "TRIDENT是一个新颖的框架，它结合了分子SMILES、文本描述和分类学注释来学习丰富的分子表示，并在11个下游任务上实现了最先进的性能。", "motivation": "现有的多模态分子表示学习方法大多忽略了分子的文本和分类学信息，导致学习到的分子表示不够丰富。", "method": "TRIDENT整合了分子SMILES、文本描述和分类学功能注释，并为此策划了一个包含结构化、多层次功能注释的全面分子-文本对数据集。它采用基于体积的对齐目标进行全局三模态特征对齐，并引入新颖的局部对齐目标来捕捉分子子结构与对应子文本描述之间的详细关系。通过基于动量机制动态平衡全局和局部对齐，使模型能够同时学习广泛的功能语义和细粒度的结构-功能映射。", "result": "TRIDENT在11个下游分子性质预测任务上实现了最先进的性能。", "conclusion": "结合分子SMILES、文本信息和分类学功能注释对于分子性质预测具有显著价值，TRIDENT框架有效地利用这些多模态信息学习到了丰富的分子表示。", "translation": "分子性质预测旨在学习将化学结构映射到功能性质的表示。尽管多模态学习已成为学习分子表示的强大范式，但先前的研究在表示学习中很大程度上忽略了分子的文本和分类学信息。我们引入了TRIDENT，一个新颖的框架，它整合了分子SMILES、文本描述和分类学功能注释来学习丰富的分子表示。为了实现这一点，我们策划了一个包含结构化、多层次功能注释的全面分子-文本对数据集。TRIDENT没有依赖传统的对比损失，而是采用基于体积的对齐目标来联合对齐全局级别的三模态特征，从而实现跨模态的软、几何感知对齐。此外，TRIDENT引入了一种新颖的局部对齐目标，捕捉分子子结构及其对应子文本描述之间的详细关系。一种基于动量机制动态平衡全局和局部对齐，使模型能够学习广泛的功能语义和细粒度的结构-功能映射。TRIDENT在11个下游任务上实现了最先进的性能，证明了结合SMILES、文本和分类学功能注释对分子性质预测的价值。", "summary": "TRIDENT是一个新颖的多模态分子表示学习框架，它突破了以往研究仅依赖SMILES的局限，首次整合了分子SMILES、文本描述和分类学功能注释。该框架通过构建全面的分子-文本数据集，并采用创新的全局体积对齐和局部对应对齐目标，结合动量机制动态平衡学习，从而有效捕捉广义功能语义和细粒度结构-功能映射。实验证明，TRIDENT在11个下游分子性质预测任务上取得了最先进的性能。", "keywords": "分子表示学习, 多模态, 分类学注释, 局部对应, 分子性质预测", "comments": "TRIDENT的创新之处在于其三模态整合（SMILES、文本、分类学注释）以及新颖的体积对齐和局部对齐目标，克服了传统对比学习的局限。它强调了文本和分类学信息在分子表示学习中的重要性，为未来的多模态化学信息学研究开辟了新方向。"}}
{"id": "2506.21547", "title": "SAM4D: Segment Anything in Camera and LiDAR Streams", "authors": ["Jianyun Xu", "Song Wang", "Ziqian Ni", "Chunyong Hu", "Sheng Yang", "Jianke Zhu", "Qiang Li"], "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.", "comment": "Accepted by ICCV2025, Project Page: https://SAM4D-Project.github.io", "pdf_url": "http://arxiv.org/pdf/2506.21547v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21547v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SAM4D：在相机和激光雷达流中分割一切", "tldr": "SAM4D是一个多模态、时序基础模型，用于相机和激光雷达流的可提示分割，通过统一多模态位置编码和运动感知跨模态记忆注意力实现，并开发了自动数据引擎以加速伪标签生成。", "motivation": "为了解决相机和激光雷达流中的可提示分割问题，并克服传统标注瓶颈，提升自动驾驶场景中的分割鲁棒性。", "method": "本文提出了SAM4D模型，包含：1) 统一多模态位置编码（UMPE），用于对齐相机和激光雷达特征并实现跨模态提示和交互。2) 运动感知跨模态记忆注意力（MCMA），利用自我运动补偿增强时间一致性和长时特征检索。3) 多模态自动化数据引擎，结合VFM驱动的视频掩膜、时空4D重建和跨模态掩膜融合，以快速生成相机-激光雷达对齐的伪标签。", "result": "在构建的Waymo-4DSeg数据集上进行了广泛实验，结果表明SAM4D具有强大的跨模态分割能力和在数据标注方面的巨大潜力。", "conclusion": "SAM4D模型在相机和激光雷达流的可提示分割方面表现出色，并通过其创新的数据生成框架显著提升了标注效率和准确性，在自动驾驶场景中展示了广阔的应用前景。", "translation": "我们提出了SAM4D，一个多模态和时序基础模型，专为相机和激光雷达流中的可提示分割而设计。引入了统一多模态位置编码（UMPE），以在共享的3D空间中对齐相机和激光雷达特征，实现无缝的跨模态提示和交互。此外，我们提出了运动感知跨模态记忆注意力（MCMA），它利用自我运动补偿来增强时间一致性和长距离特征检索，确保在动态变化的自动驾驶场景中实现鲁棒分割。为了避免标注瓶颈，我们开发了一个多模态自动化数据引擎，该引擎协同VFM驱动的视频掩膜、时空4D重建和跨模态掩膜融合。该框架生成的相机-激光雷达对齐的伪标签的速度比人工标注快几个数量级，同时在点云表示中保留了VFM衍生的语义保真度。我们在构建的Waymo-4DSeg上进行了广泛实验，这些实验证明了所提出的SAM4D强大的跨模态分割能力和在数据标注方面的巨大潜力。", "summary": "SAM4D是一个创新的多模态和时序基础模型，旨在实现相机和激光雷达流的可提示分割。该模型引入了统一多模态位置编码（UMPE）以对齐跨模态特征，并采用运动感知跨模态记忆注意力（MCMA）来增强动态场景下的时间一致性和特征检索。为解决标注难题，SAM4D还包含一个多模态自动化数据引擎，能高效生成高质量的伪标签。在Waymo-4DSeg上的实验验证了其强大的跨模态分割能力和在数据标注方面的巨大潜力。", "keywords": "SAM4D, 多模态分割, 相机-激光雷达, 自动驾驶, 伪标签", "comments": "该论文的创新点在于提出了一个统一的多模态时序基础模型SAM4D，能够同时处理相机和激光雷达数据进行可提示分割。特别值得注意的是其统一多模态位置编码（UMPE）实现了跨模态特征的有效对齐，以及运动感知跨模态记忆注意力（MCMA）提升了动态场景下的鲁棒性。此外，其自动化数据引擎解决了多模态标注的瓶颈，显著提高了数据生成效率，对于自动驾驶领域的数据标注和模型训练具有重要意义。"}}
{"id": "2506.21006", "title": "Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning", "authors": ["Tyler Ward", "Xiaoqin Wang", "Braxton McFarland", "Md Atik Ahamed", "Sahar Nozad", "Talal Arshad", "Hafsa Nebbache", "Jin Chen", "Abdullah Imran"], "summary": "Complete removal of cancer tumors with a negative specimen margin during\nlumpectomy is essential in reducing breast cancer recurrence. However, 2D\nspecimen radiography (SR), the current method used to assess intraoperative\nspecimen margin status, has limited accuracy, resulting in nearly a quarter of\npatients requiring additional surgery. To address this, we propose a novel deep\nlearning framework combining the Segment Anything Model (SAM) with\nForward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging\nboth local and global contrastive learning for patch-level classification of SR\nimages. After annotating SR images with regions of known maligancy,\nnon-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18\nbackbone with FFCL to classify margin status, then reconstruct coarse binary\nmasks to prompt SAM for refined tumor margin segmentation. Our approach\nachieved an AUC of 0.8455 for margin classification and segmented margins with\na 27.4% improvement in Dice similarity over baseline models, while reducing\ninference time to 47 milliseconds per image. These results demonstrate that\nFFCL-SAM significantly enhances both the speed and accuracy of intraoperative\nmargin assessment, with strong potential to reduce re-excision rates and\nimprove surgical outcomes in breast cancer treatment. Our code is available at\nhttps://github.com/tbwa233/FFCL-SAM/.", "comment": "19 pages, 7 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21006v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21006v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "乳腺癌保乳术切缘检测，结合SAM和前向-前向对比学习", "tldr": "提出一种结合SAM和前向-前向对比学习的深度学习框架FFCL-SAM，用于提高乳腺癌保乳术中切缘评估的速度和准确性，以减少二次手术率。", "motivation": "乳腺癌保乳术中完整切除肿瘤并确保切缘阴性对降低复发至关重要。然而，当前术中切缘评估方法（2D标本X射线摄影）准确性有限，导致近四分之一的患者需要二次手术。", "method": "提出一种结合了Segment Anything Model (SAM) 和前向-前向对比学习 (FFCL) 的新型深度学习框架。该方法首先通过对已知恶性、非恶性组织和病理确诊切缘的SR图像进行标注，然后使用FFCL（一种利用局部和全局对比学习进行斑块级分类的预训练策略）预训练ResNet-18骨干网络以分类切缘状态，最后重建粗略的二值掩膜来提示SAM进行精细的肿瘤切缘分割。", "result": "该方法在切缘分类方面实现了0.8455的AUC；在切缘分割方面，Dice相似系数比基线模型提高了27.4%；同时将每张图像的推理时间缩短至47毫秒。", "conclusion": "FFCL-SAM显著提高了术中切缘评估的速度和准确性，具有降低二次手术率和改善乳腺癌治疗手术效果的巨大潜力。", "translation": "在保乳术中，完整切除肿瘤并确保标本切缘阴性对于降低乳腺癌复发至关重要。然而，2D标本X射线摄影（SR）是目前用于评估术中标本切缘状态的方法，其准确性有限，导致近四分之一的患者需要额外的手术。为了解决这个问题，我们提出了一种新颖的深度学习框架，该框架将Segment Anything Model（SAM）与前向-前向对比学习（FFCL）相结合。FFCL是一种预训练策略，利用局部和全局对比学习对SR图像进行斑块级分类。在对已知恶性、非恶性组织和病理确诊切缘区域的SR图像进行标注后，我们使用FFCL预训练ResNet-18骨干网络以分类切缘状态，然后重建粗略的二值掩膜以提示SAM进行精细的肿瘤切缘分割。我们的方法在切缘分类方面实现了0.8455的AUC，在切缘分割方面，Dice相似系数比基线模型提高了27.4%，同时将每张图像的推理时间缩短至47毫秒。这些结果表明，FFCL-SAM显著提高了术中切缘评估的速度和准确性，具有降低二次手术率和改善乳腺癌治疗手术效果的巨大潜力。我们的代码可在https://github.com/tbwa233/FFCL-SAM/获取。", "summary": "本文提出了一种名为FFCL-SAM的新型深度学习框架，旨在解决乳腺癌保乳术中现有切缘评估方法准确性不足导致高二次手术率的问题。该框架结合了Segment Anything Model (SAM) 和前向-前向对比学习 (FFCL)，通过预训练ResNet-18进行切缘分类，并利用SAM进行精细分割。实验结果表明，FFCL-SAM显著提高了切缘评估的速度和准确性，有望降低乳腺癌患者的二次手术率并改善治疗效果。", "keywords": "乳腺癌, 保乳术, 切缘检测, SAM, 对比学习, 深度学习", "comments": "本文的创新点在于将SAM与前向-前向对比学习相结合，用于解决乳腺癌保乳术中切缘评估的关键问题。通过利用FFCL进行有效的特征学习和SAM进行精确分割，该方法在提高诊断准确性和降低推理时间方面取得了显著进展，对于改善患者预后具有重要临床意义。"}}
{"id": "2506.21035", "title": "Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning", "authors": ["Haodong Lu", "Chongyang Zhao", "Jason Xue", "Lina Yao", "Kristen Moore", "Dong Gong"], "summary": "Continual learning (CL) with large pre-trained models is challenged by\ncatastrophic forgetting and task interference. Existing LoRA-based\nMixture-of-Experts (MoE) approaches mitigate forgetting by assigning and\nfreezing task-specific adapters, but suffer from interference, redundancy, and\nambiguous routing due to coarse adapter-level selection. However, this design\nintroduces three key challenges: 1) Interference: Activating full LoRA experts\nper input leads to subspace interference and prevents selective reuse of useful\ncomponents across tasks. 2) Redundancy: Newly added experts often duplicate or\ncontradict existing knowledge due to unnecessary activation of unrelated ranks\nand insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features\nacross tasks confuse the router, resulting in unstable expert assignments. As\nmore experts accumulate, earlier task routing degrades, accelerating\nforgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with\nself-activated and sparse rank activation for CL. Unlike mixing multiple\nlow-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,\neach treated as an independent expert, enabling fine-grained mixture of rank-1\nexpert utilization while mitigating interference and redundancy. To avoid\nambiguous routing, we propose that each rank-1 expert can infer its own\nrelevance via intermediate activations. Coupled with our proposed rank pruning\nand activation budgets, MoRA adaptively selects a sparse mixture of ranks per\ninput. We validate MoRA on continual learning tasks with CLIP and large\nlanguage models (LLMs), analyzing both in-domain learning and out-of-domain\nforgetting/generalization during fine-tuning. MoRA shows significant\neffectiveness on enhancing CL with PTMs, and improving generalization while\nmitigating forgetting.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.21035v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21035v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "循序渐进：通过自激活稀疏秩自适应学习实现持续学习", "tldr": "现有基于LoRA的MoE方法在持续学习中存在干扰、冗余和路由模糊等问题。本文提出了MoRA，一种通过自激活稀疏秩激活实现持续学习的方法，它将更新分解为细粒度的秩-1专家，并通过自适应选择稀疏秩混合来减轻干扰、冗余和模糊性。MoRA在CLIP和LLMs的持续学习任务中显示出显著效果，增强了持续学习并改善了泛化能力，同时减轻了遗忘。", "motivation": "大型预训练模型在持续学习（CL）中面临灾难性遗忘和任务干扰的挑战。现有的基于LoRA的专家混合（MoE）方法通过分配和冻结任务特定适配器来减轻遗忘，但由于粗粒度的适配器级别选择，它们面临干扰、冗余和模糊路由的问题。具体来说，这些挑战包括：1）干扰：对每个输入激活完整的LoRA专家会导致子空间干扰，并阻碍跨任务选择性重用有用组件。2）冗余：由于不必要地激活不相关秩和未能充分重用相关秩，新添加的专家经常复制或与现有知识冲突。3）模糊性：任务之间重叠的特征会混淆路由器，导致专家分配不稳定。随着专家数量的积累，早期任务的路由性能会下降，加速遗忘。", "method": "本文提出了MoRA（Mixture-of-Rank Adaptive learning），一种用于持续学习的自激活稀疏秩自适应学习方法。与混合多个低秩矩阵不同，MoRA将每个秩-r更新分解为r个秩-1组件，每个组件被视为一个独立的专家，从而实现了秩-1专家的细粒度混合利用，同时减轻了干扰和冗余。为了避免路由模糊，MoRA提出每个秩-1专家可以通过中间激活来推断其自身的相关性。结合所提出的秩剪枝和激活预算，MoRA为每个输入自适应地选择稀疏的秩混合。", "result": "MoRA在CLIP和大型语言模型（LLMs）的持续学习任务上进行了验证，分析了微调过程中的域内学习和域外遗忘/泛化。结果表明，MoRA在增强使用预训练模型的持续学习方面显示出显著效果，并在减轻遗忘的同时改善了泛化能力。", "conclusion": "MoRA通过其自激活稀疏秩自适应学习方法，有效增强了预训练模型的持续学习能力，提高了泛化性并减轻了遗忘。", "translation": "大型预训练模型（PTMs）的持续学习（CL）面临灾难性遗忘和任务干扰的挑战。现有的基于LoRA的专家混合（MoE）方法通过分配和冻结任务特定适配器来减轻遗忘，但由于粗粒度的适配器级别选择，它们面临干扰、冗余和模糊路由的问题。然而，这种设计引入了三个关键挑战：1）干扰：对每个输入激活完整的LoRA专家会导致子空间干扰，并阻碍跨任务选择性重用有用组件。2）冗余：由于不必要地激活不相关秩和未能充分重用相关秩，新添加的专家经常复制或与现有知识冲突。3）模糊性：任务之间重叠的特征会混淆路由器，导致专家分配不稳定。随着更多专家的积累，早期任务的路由性能会下降，加速遗忘。我们提出了MoRA，一种具有自激活稀疏秩激活的秩自适应学习方法，用于持续学习。与混合多个低秩矩阵不同，MoRA将每个秩-r更新分解为r个秩-1组件，每个组件被视为一个独立的专家，从而实现了秩-1专家利用的细粒度混合，同时减轻了干扰和冗余。为了避免模糊路由，我们提出每个秩-1专家可以通过中间激活来推断其自身的相关性。结合我们提出的秩剪枝和激活预算，MoRA为每个输入自适应地选择稀疏的秩混合。我们在CLIP和大型语言模型（LLMs）的持续学习任务上验证了MoRA，分析了微调过程中的域内学习和域外遗忘/泛化。MoRA在增强使用PTMs的持续学习方面显示出显著效果，并在减轻遗忘的同时改善了泛化能力。", "summary": "MoRA提出了一种针对大型预训练模型持续学习的新方法，旨在解决现有LoRA-based MoE方法中的干扰、冗余和路由模糊问题。MoRA通过将秩更新分解为细粒度的秩-1专家，并利用自激活机制、秩剪枝和激活预算，实现了对每个输入自适应地选择稀疏的秩混合。实验证明，MoRA在持续学习任务中能有效提高泛化能力并减轻灾难性遗忘。", "keywords": "持续学习, 专家混合, LoRA, 大型语言模型, 灾难性遗忘", "comments": "该论文的创新点在于其细粒度的秩级别专家分解和自激活机制，这有效地解决了现有基于适配器级别MoE方法在持续学习中面临的局限性，特别是对于大型预训练模型。这种方法有望实现更高效和有效的知识保留与迁移。"}}
{"id": "2506.21468", "title": "TopK Language Models", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21468v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21468v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TopK 语言模型", "tldr": "引入了TopK语言模型，通过在Transformer架构中集成TopK激活函数来解决稀疏自编码器（SAE）的局限性，实现无需后训练的、可解释的模型，同时保持性能。", "motivation": "稀疏自编码器（SAE）是分析和解释Transformer语言模型激活空间的重要工具，但存在局限性：1. 后训练导致不清楚概念未被发现是SAE的问题还是底层LM未表示该概念。2. 训练条件和架构选择影响SAE学习的特征。3. 缺乏特征稳定性使得难以比较不同检查点之间的SAE特征，从而难以追踪LM学习概念的过程。", "method": "通过修改Transformer架构，在选定层引入TopK激活函数，使模型的隐藏状态等同于TopK SAE的潜在特征。这种方法消除了后训练的需要，同时提供了与SAE相当的可解释性。", "result": "TopK语言模型在模型大小、计算效率和可解释性之间提供了有利的权衡。尽管进行了简单的架构更改，TopK语言模型仍保持其原始能力，并提供强大的可解释性优势。实验表明，TopK语言模型学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并有助于详细分析跨检查点和层级的神经元形成过程。", "conclusion": "TopK语言模型提供稳定可靠的工具，用于理解语言模型如何学习和表示概念，这将显著推进未来在模型可解释性和可控性方面的研究。", "translation": "稀疏自编码器（SAE）已成为分析和解释基于Transformer的语言模型（LM）激活空间的重要工具。然而，SAE存在一些缺点，降低了其实用性和内部有效性。由于SAE是事后训练的，因此不清楚未能发现特定概念是SAE的问题，还是底层LM未表示该概念。训练条件和架构选择影响SAE学习哪些特征，这使得这个问题更加严重。在追踪LM在训练过程中如何学习概念时，特征稳定性的缺乏也使得难以比较不同检查点上的SAE特征。为了解决这些限制，我们引入了对Transformer架构的修改，在选定层中加入了TopK激活函数，使模型的隐藏状态等同于TopK SAE的潜在特征。这种方法消除了事后训练的需要，同时提供了与SAE相当的可解释性。由此产生的TopK LM在模型大小、计算效率和可解释性之间提供了有利的权衡。尽管进行了这种简单的架构更改，TopK LM仍保持其原始能力，同时提供强大的可解释性优势。我们的实验表明，TopK LM学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并有助于详细分析跨检查点和层级的神经元形成过程。这些特性使得TopK LM成为理解语言模型如何学习和表示概念的稳定可靠工具，我们相信这将显著推进未来在模型可解释性和可控性方面的研究。", "summary": "本研究提出TopK语言模型，旨在克服稀疏自编码器（SAE）在语言模型可解释性方面的局限。通过在Transformer架构中直接集成TopK激活函数，TopK语言模型能够生成与TopK SAE潜在特征等效的隐藏状态，从而消除了对SAAE进行后训练的需求。这种方法在保持模型性能的同时，显著提升了可解释性、计算效率和特征稳定性。实验证明，TopK语言模型学习到的稀疏表示有助于进行神经元干预和跨检查点的详细分析，为理解语言模型学习机制提供了稳定可靠的工具。", "keywords": "TopK语言模型, 可解释性, 稀疏自编码器, Transformer, 激活函数", "comments": "TopK语言模型提出了一种新颖且具有前瞻性的方法来解决当前语言模型可解释性中的关键挑战。其创新之处在于将可解释性直接融入模型架构而非作为后处理步骤，这极大地提高了特征的稳定性和可信度。这种“内置”的可解释性有望显著降低理解大型语言模型内部工作机制的复杂性，并为未来模型控制和安全性的研究奠定基础。"}}
{"id": "2506.21552", "title": "Whole-Body Conditioned Egocentric Video Prediction", "authors": ["Yutong Bai", "Danny Tran", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik"], "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.", "comment": "Project Page: https://dannytran123.github.io/PEVA", "pdf_url": "http://arxiv.org/pdf/2506.21552v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21552v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "全身条件化的自我中心视频预测", "tldr": "本文训练了一个名为PEVA的模型，该模型能够根据过去的视频和相对3D身体姿态来预测第一人称自我中心视频。模型利用自回归条件扩散变换器在大型数据集Nymeria上进行训练，并设计了分层评估协议以全面分析其具身预测和控制能力。", "motivation": "旨在从人类视角解决建模复杂真实世界环境和具身智能体行为的挑战。", "method": "训练了一个名为PEVA的模型，该模型通过结合过去的视频和由相对3D身体姿态表示的动作来预测自我中心视频。模型通过身体关节层次结构化的运动姿态轨迹进行条件化，并采用自回归条件扩散变换器架构。该模型在大型真实世界自我中心视频和身体姿态捕捉数据集Nymeria上进行训练。此外，还设计了一个具有日益挑战性任务的分层评估协议，用于全面分析模型的具身预测和控制能力。", "result": "成功训练了PEVA模型，能够根据人类动作预测第一人称视频。通过设计分层评估协议，实现了对模型具身预测和控制能力的全面分析。", "conclusion": "这项工作代表了从人类视角通过视频预测来解决建模复杂真实世界环境和具身智能体行为挑战的初步尝试。", "translation": "我们训练模型根据人类动作预测自我中心视频（PEVA），给定过去的视频和以相对3D身体姿态表示的动作。通过以身体关节层次结构组织的运动姿态轨迹为条件，我们的模型学会了从第一人称视角模拟物理人类动作如何塑造环境。我们在Nymeria上训练了一个自回归条件扩散变换器，Nymeria是一个大型的真实世界自我中心视频和身体姿态捕捉数据集。我们进一步设计了一个具有日益挑战性任务的分层评估协议，从而能够全面分析模型的具身预测和控制能力。我们的工作代表了从人类视角通过视频预测来解决建模复杂真实世界环境和具身智能体行为挑战的初步尝试。", "summary": "本文提出PEVA模型，旨在根据过去的视频和3D身体姿态动作预测第一人称自我中心视频。该模型利用自回归条件扩散变换器，并在大型真实世界数据集Nymeria上进行训练，以模拟人类动作如何从第一人称视角塑造环境。为全面评估模型具身预测和控制能力，研究者还设计了一个分层评估协议。这项工作是解决从人类视角建模复杂真实环境和具身智能体行为挑战的初步探索。", "keywords": "自我中心视频预测, 具身智能体, 身体姿态, 扩散变换器, 动作预测", "comments": "这项工作创新性地将全身姿态作为条件引入到自我中心视频预测中，通过模拟人类动作来预测环境变化，为具身智能体的感知和控制提供了新的视角。使用扩散变换器和大规模真实世界数据集Nymeria是其重要贡献。其分层评估协议也为未来研究提供了有益的框架。"}}
{"id": "2506.21008", "title": "The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion", "authors": ["Bang Gong", "Luchao Qi", "Jiaye Wu", "Zhicheng Fu", "Chunbo Song", "David W. Jacobs", "John Nicholson", "Roni Sengupta"], "summary": "We introduce the Aging Multiverse, a framework for generating multiple\nplausible facial aging trajectories from a single image, each conditioned on\nexternal factors such as environment, health, and lifestyle. Unlike prior\nmethods that model aging as a single deterministic path, our approach creates\nan aging tree that visualizes diverse futures. To enable this, we propose a\ntraining-free diffusion-based method that balances identity preservation, age\naccuracy, and condition control. Our key contributions include attention mixing\nto modulate editing strength and a Simulated Aging Regularization strategy to\nstabilize edits. Extensive experiments and user studies demonstrate\nstate-of-the-art performance across identity preservation, aging realism, and\nconditional alignment, outperforming existing editing and age-progression\nmodels, which often fail to account for one or more of the editing criteria. By\ntransforming aging into a multi-dimensional, controllable, and interpretable\nprocess, our approach opens up new creative and practical avenues in digital\nstorytelling, health education, and personalized visualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21008v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21008v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "老龄化多重宇宙：通过免训练扩散生成条件感知面部老化树", "tldr": "提出“老龄化多重宇宙”框架，利用免训练扩散模型生成受环境、健康等条件影响的多种面部老化轨迹，实现身份保持、年龄准确性和条件控制，性能优于现有模型。", "motivation": "现有面部老化方法将老化建模为单一确定性路径，未能反映受环境、健康和生活方式等外部因素影响的多种可能老化轨迹。本文旨在解决这一局限性，生成多维度的、受条件控制的面部老化轨迹。", "method": "本文引入了“老龄化多重宇宙”框架，用于从单张图像生成多个合理且受外部因素（如环境、健康、生活方式）影响的面部老化轨迹。该方法采用免训练的扩散模型，旨在平衡身份保持、年龄准确性和条件控制。关键贡献包括使用注意力混合（attention mixing）来调节编辑强度，以及采用模拟老化正则化（Simulated Aging Regularization）策略来稳定编辑。", "result": "广泛的实验和用户研究表明，该方法在身份保持、老化真实感和条件对齐方面达到了最先进的性能，优于现有编辑和年龄进展模型，这些模型常常无法同时满足一个或多个编辑标准。", "conclusion": "该方法成功将面部老化模拟转化为一个多维度、可控且可解释的过程，为数字故事讲述、健康教育和个性化可视化等领域开辟了新的创造性和实用途径。", "translation": "我们引入了“老龄化多重宇宙”框架，用于从单张图像生成多个合理的面部老化轨迹，每个轨迹都以环境、健康和生活方式等外部因素为条件。与将老化建模为单一确定性路径的现有方法不同，我们的方法创建了一个可视化多样化未来的老化树。为了实现这一点，我们提出了一种免训练的基于扩散的方法，该方法平衡了身份保持、年龄准确性和条件控制。我们的主要贡献包括注意力混合（attention mixing）以调节编辑强度和模拟老化正则化（Simulated Aging Regularization）策略以稳定编辑。广泛的实验和用户研究表明，在身份保持、老化真实感和条件对齐方面，该方法达到了最先进的性能，优于现有编辑和年龄进展模型，这些模型通常无法考虑到一个或多个编辑标准。通过将老化转化为多维度、可控和可解释的过程，我们的方法在数字故事讲述、健康教育和个性化可视化方面开辟了新的创造性和实用途径。", "summary": "本文提出了一个名为“老龄化多重宇宙”的创新框架，旨在克服传统面部老化模型仅生成单一路径的局限性。该框架能够从单张图像生成多个受环境、健康、生活方式等外部因素影响的合理面部老化轨迹，形成一个可视化多样化未来的“老化树”。其核心是一个免训练的扩散模型，通过引入注意力混合和模拟老化正则化策略，有效平衡了身份保持、年龄准确性和条件控制。实验和用户研究证实，该方法在多项关键指标上均达到最先进水平，显著优于现有模型，为数字叙事、健康教育和个性化可视化等领域提供了新的、多维度、可控且可解释的面部老化模拟解决方案。", "keywords": "面部老化, 扩散模型, 条件生成, 免训练, 老化轨迹", "comments": "这篇论文通过引入“老龄化多重宇宙”框架，解决了传统面部老化模型仅生成单一确定性路径的局限性，实现了对多种可能老化轨迹的模拟，并融入了外部条件控制，极具创新性。其免训练扩散模型和关键的注意力混合、模拟老化正则化策略，有效平衡了身份保持、年龄准确性和条件控制，展现了卓越的性能。该研究不仅在技术上有所突破，更在数字故事讲述、健康教育等领域开辟了新的应用前景，具有重要的理论和实践价值。"}}
{"id": "2506.21495", "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21495v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21495v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "弥合LLM的离线与在线强化学习", "tldr": "本文研究了大型语言模型（LLMs）微调中离线、半在线和完全在线强化学习方法的有效性。实验发现，在线和半在线方法（如DPO和GRPO）性能相似且显著优于离线方法，并且结合可验证和不可验证任务的多任务学习能进一步提升性能。", "motivation": "研究强化学习方法在大型语言模型（LLMs）微调中，从离线到半在线再到完全在线模式的有效性，特别是针对可验证和不可验证任务。", "method": "在可验证数学和不可验证指令遵循任务上进行训练和基准评估。广泛比较了在线和半在线的直接偏好优化（DPO）和群组奖励策略优化（GRPO）目标。详细分析了训练动态和超参数选择策略。探索了可验证和不可验证奖励的联合多任务学习。", "result": "在线和半在线的DPO和GRPO变体在性能和收敛性上表现相似。所有在线/半在线方法都显著优于离线方法。将可验证和不可验证奖励进行联合多任务处理可以提高两种任务类型的性能。", "conclusion": "在LLM微调中，在线和半在线强化学习方法优于离线方法，并且通过多任务学习结合可验证和不可验证奖励可以进一步提升整体性能。", "translation": "我们研究了强化学习方法在大型语言模型（LLMs）微调中，从离线到半在线再到完全在线模式的有效性，涵盖可验证和不可验证任务。我们的实验包括在可验证数学和不可验证指令遵循任务上进行训练，并对两者都进行了一系列基准评估。在这些设置中，我们广泛比较了在线和半在线的直接偏好优化（Direct Preference Optimization）和群组奖励策略优化（Group Reward Policy Optimization）目标，并惊人地发现这些变体之间性能和收敛性相似，所有这些都显著优于离线方法。我们提供了训练动态和超参数选择策略的详细分析，以实现最佳结果。最后，我们展示了将可验证和不可验证奖励进行联合多任务处理可以提高两种任务类型的性能。", "summary": "本文探讨了在大型语言模型（LLMs）微调过程中，强化学习从离线到在线不同范式的有效性。研究人员在可验证和不可验证任务上，对比了在线和半在线的直接偏好优化（DPO）与群组奖励策略优化（GRPO）方法，发现它们表现相似且均显著优于离线方法。此外，通过多任务学习联合处理可验证和不可验证奖励，能够提升两类任务的整体性能。", "keywords": "强化学习, 大语言模型, 在线学习, 离线学习, 微调", "comments": "这项研究的创新之处在于系统地比较了不同在线程度的强化学习方法在LLM微调中的表现，并揭示了在线/半在线方法的优越性。发现DPO和GRPO在在线设置下表现相似是一个有趣的发现。此外，多任务学习的有效性也为LLM的RLHF（人类反馈强化学习）提供了新的方向。"}}
{"id": "2506.21009", "title": "User-in-the-Loop View Sampling with Error Peaking Visualization", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Shohei Mori"], "summary": "Augmented reality (AR) provides ways to visualize missing view samples for\nnovel view synthesis. Existing approaches present 3D annotations for new view\nsamples and task users with taking images by aligning the AR display. This data\ncollection task is known to be mentally demanding and limits capture areas to\npre-defined small areas due to the ideal but restrictive underlying sampling\ntheory. To free users from 3D annotations and limited scene exploration, we\npropose using locally reconstructed light fields and visualizing errors to be\nremoved by inserting new views. Our results show that the error-peaking\nvisualization is less invasive, reduces disappointment in final results, and is\nsatisfactory with fewer view samples in our mobile view synthesis system. We\nalso show that our approach can contribute to recent radiance field\nreconstruction for larger scenes, such as 3D Gaussian splatting.", "comment": "Accepted at IEEE ICIP 2025, Project Page:\n  https://mediated-reality.github.io/projects/yasunaga_icip25/", "pdf_url": "http://arxiv.org/pdf/2506.21009v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21009v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用户在环的视图采样与误差峰值可视化", "tldr": "现有AR视图采样方法对用户要求高且受限。本文提出一种通过误差可视化指导的、侵入性更小的视图采样方法，能在移动视图合成系统中用更少样本获得满意结果，并适用于大场景辐射场重建，如3D高斯泼溅。", "motivation": "现有增强现实（AR）中的新颖视图合成方法依赖于对用户要求高的3D注释和受限的捕获区域，导致数据收集任务精神负担重且效率低下。本文旨在解决这些限制，使用户摆脱繁琐的3D注释，并允许更自由的场景探索。", "method": "本文提出了一种用户在环的视图采样方法，利用局部重建的光场，并通过可视化需要通过插入新视图来消除的误差（即“误差峰值可视化”）来指导用户。", "result": "结果显示，误差峰值可视化方法侵入性更小，减少了最终结果的失望，并且在移动视图合成系统中用更少的视图样本就能令人满意。此外，该方法还能促进更大场景的辐射场重建，例如3D高斯泼溅。", "conclusion": "所提出的用户在环的视图采样与误差峰值可视化方法显著提高了视图合成的效率和用户体验，使其更适用于移动系统和大规模场景重建。", "translation": "增强现实（AR）提供了一种可视化缺失视图样本以进行新颖视图合成的方法。现有方法为新的视图样本提供3D注释，并要求用户通过对齐AR显示屏来拍摄图像。这种数据收集任务众所周知是精神上要求很高的，并且由于理想但限制性的底层采样理论，将捕获区域限制在预定义的小区域。为了使用户摆脱3D注释和有限的场景探索，我们建议使用局部重建的光场并可视化通过插入新视图来消除的误差。我们的结果表明，误差峰值可视化侵入性更小，减少了最终结果的失望，并且在我们的移动视图合成系统中，用更少的视图样本就能令人满意。我们还表明，我们的方法可以促进最近对更大场景的辐射场重建，例如3D高斯泼溅。", "summary": "本文针对现有AR新颖视图合成中视图采样任务的局限性（如用户负担重、区域受限）提出了一种新方法。该方法利用局部重建的光场，并通过“误差峰值可视化”来引导用户添加新视图，从而摆脱了传统的3D注释。实验结果表明，这种方法侵入性更小，能减少用户对最终结果的失望，并能在移动视图合成系统中用更少的视图样本达到满意效果。此外，该方法还有助于大型场景的辐射场重建，包括3D高斯泼溅等技术。", "keywords": "视图采样, 增强现实, 新颖视图合成, 误差可视化, 光场, 3D高斯泼溅", "comments": "本文的创新之处在于将传统的、基于明确3D注释的用户引导方式，转变为基于误差驱动的可视化指导，这使得数据收集过程更加直观和省力。这一转变对于提高AR视图合成和辐射场重建的效率和可扩展性具有重要意义，尤其是在移动应用场景下。"}}
{"id": "2506.21037", "title": "RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment", "authors": ["Suorong Yang", "Peijia Li", "Furao Shen", "Jian Zhao"], "summary": "Modern deep architectures often rely on large-scale datasets, but training on\nthese datasets incurs high computational and storage overhead. Real-world\ndatasets often contain substantial redundancies, prompting the need for more\ndata-efficient training paradigms. Data selection has shown promise to mitigate\nredundancy by identifying the most representative samples, thereby reducing\ntraining costs without compromising performance. Existing methods typically\nrely on static scoring metrics or pretrained models, overlooking the combined\neffect of selected samples and their evolving dynamics during training. We\nintroduce the concept of epsilon-sample cover, which quantifies sample\nredundancy based on inter-sample relationships, capturing the intrinsic\nstructure of the dataset. Based on this, we reformulate data selection as a\nreinforcement learning (RL) process and propose RL-Selector, where a\nlightweight RL agent optimizes the selection policy by leveraging\nepsilon-sample cover derived from evolving dataset distribution as a reward\nsignal. Extensive experiments across benchmark datasets and diverse\narchitectures demonstrate that our method consistently outperforms existing\nstate-of-the-art baselines. Models trained with our selected datasets show\nenhanced generalization performance with improved training efficiency.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21037v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21037v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "RL-Selector：基于冗余评估的强化学习引导数据选择", "tldr": "RL-Selector是一种新的数据选择方法，它将数据选择重新定义为强化学习过程，利用epsilon-sample cover来量化样本冗余并作为奖励信号，以提高训练效率和泛化性能。", "motivation": "现代深度学习模型依赖于大规模数据集，导致高昂的计算和存储开销。实际数据集常包含大量冗余，因此需要更数据高效的训练范式。现有数据选择方法通常依赖静态评分或预训练模型，忽略了样本组合效果及其在训练过程中的动态演变。", "method": "本文引入了epsilon-sample cover概念，基于样本间关系量化样本冗余，以捕捉数据集的内在结构。在此基础上，将数据选择重新表述为强化学习（RL）过程，并提出了RL-Selector。RL-Selector中，一个轻量级RL代理通过利用从不断演变的数据集分布中获得的epsilon-sample cover作为奖励信号，优化数据选择策略。", "result": "RL-Selector在基准数据集和各种架构上的广泛实验表明，其方法始终优于现有最先进的基线。使用RL-Selector选择的数据集训练的模型显示出增强的泛化性能，同时提高了训练效率。", "conclusion": "RL-Selector通过将数据选择重新定义为强化学习问题，并利用epsilon-sample cover作为动态奖励信号，有效解决了大数据集训练的冗余和效率问题，显著提升了模型的泛化能力和训练效率。", "translation": "现代深度架构通常依赖大规模数据集，但在此类数据集上进行训练会产生高昂的计算和存储开销。实际数据集通常包含大量冗余，这促使人们需要更数据高效的训练范式。数据选择已显示出通过识别最具代表性的样本来减轻冗余的潜力，从而在不损害性能的情况下降低训练成本。现有方法通常依赖静态评分指标或预训练模型，忽略了所选样本的组合效应及其在训练过程中的演变动态。我们引入了epsilon-sample cover的概念，它基于样本间关系量化样本冗余，捕捉数据集的内在结构。在此基础上，我们将数据选择重新表述为强化学习（RL）过程，并提出了RL-Selector，其中一个轻量级RL代理通过利用从不断演变的数据集分布中获得的epsilon-sample cover作为奖励信号来优化选择策略。在基准数据集和各种架构上的广泛实验表明，我们的方法始终优于现有最先进的基线。使用我们选择的数据集训练的模型显示出增强的泛化性能，同时提高了训练效率。", "summary": "RL-Selector是一种新颖的数据选择方法，旨在解决大规模数据集训练中的计算和存储开销问题。该方法通过引入epsilon-sample cover来量化样本冗余并捕捉数据集内在结构，将数据选择建模为强化学习过程。一个轻量级RL代理利用动态演变的epsilon-sample cover作为奖励信号来优化数据选择策略。实验结果表明，RL-Selector在性能上超越了现有先进方法，并能有效提升模型的泛化能力和训练效率。", "keywords": "数据选择, 强化学习, 冗余评估, epsilon-sample cover, 训练效率", "comments": "RL-Selector的创新点在于将数据选择与强化学习结合，并引入epsilon-sample cover来动态评估样本冗余，这使得数据选择过程能够适应训练中的数据分布变化。这种动态适应性是其优于传统静态方法的关键。该方法对于减少深度学习模型的训练成本和提高数据效率具有重要意义。"}}
{"id": "2506.21497", "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21497v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21497v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过交互式大型语言模型对齐增强社交驱动对话中的用户参与度", "tldr": "通过利用未来对话的信号和用户反应作为奖励，本文提出了一种使用用户模拟器和i×MCTS来收集数据并通过DPO对齐交互式大型语言模型的方法，以增强社交驱动对话中的用户参与度。", "motivation": "虽然现有工作已优化模型以推理相关知识或规划对话行为流程，但用户参与度与知识或对话行为之间的关系是微妙的，并不能保证在社交驱动对话中的用户参与度。", "method": "本文通过利用对话未来发展的信号，使交互式大型语言模型（LLMs）能够学习用户参与度。具体来说，将用户在交互后与对话意图相关的反应作为奖励信号来对齐交互式LLMs。为此，开发了一个用户模拟器与目标交互式LLMs进行交互，并通过i×MCTS（Monte Carlo Tree Search for interaction）探索用户与交互式LLM系统之间的交互。通过这种方式，收集包含高质量和低质量体验对的数据集，并相应地通过直接偏好优化（DPO）对齐交互式LLMs以实现高水平的用户参与度。", "result": "在两种社交驱动对话场景（情感支持对话和劝导行善）中进行的实验表明，所提出的方法有效增强了交互式大型语言模型中的用户参与度。", "conclusion": "本文提出的方法通过直接利用用户反应作为奖励信号并结合i×MCTS和DPO，能够有效增强社交驱动对话中交互式大型语言模型的用户参与度。", "translation": "通过交互增强用户参与度在社交驱动对话中扮演着重要角色。虽然现有工作已优化模型以推理相关知识或规划对话行为流程，但用户参与度与知识或对话行为之间的关系是微妙的，并不能保证在社交驱动对话中的用户参与度。为此，我们通过利用对话未来发展的信号，使交互式大型语言模型（LLMs）能够学习用户参与度。具体来说，我们采用一种更直接且相关的用户参与度指标，即用户在交互后与对话意图相关的反应，作为奖励来对齐交互式LLMs。为了实现这一点，我们开发了一个用户模拟器与目标交互式LLMs进行交互，并通过i×MCTS（Monte Carlo Tree Search for interaction）探索用户与交互式LLM系统之间的交互。通过这种方式，我们收集了一个包含高质量和低质量体验对的数据集，并相应地通过直接偏好优化（DPO）对齐交互式LLMs以实现高水平的用户参与度。在两种社交驱动对话场景（情感支持对话和劝导行善）中进行的实验表明，我们的方法有效增强了交互式大型语言模型中的用户参与度。", "summary": "该研究旨在通过直接利用用户反应作为奖励信号来增强社交驱动对话中大型语言模型（LLMs）的用户参与度。为解决现有方法在保证用户参与度方面的不足，作者开发了一个用户模拟器，并结合i×MCTS（交互式蒙特卡洛树搜索）来探索用户与LLM之间的交互，从而收集高质量和低质量对话体验的数据集。随后，利用直接偏好优化（DPO）对交互式LLMs进行对齐。实验证明，该方法在情感支持和劝导等社交对话场景中能有效提升用户参与度。", "keywords": "用户参与度, 大型语言模型, 社交驱动对话, 交互式LLMs, DPO", "comments": "本文的创新点在于提出了一种更直接且相关的方式来衡量和优化用户参与度，即利用用户在交互后的实际反应作为奖励信号。通过结合用户模拟器、i×MCTS探索性数据收集以及DPO对齐，构建了一个端到端的系统，有效地将用户反馈融入到LLM的训练中，以解决社交对话中用户参与度难以保证的问题。这种方法为提升LLM在复杂社交场景中的表现提供了新的思路。"}}
{"id": "2506.21011", "title": "Bridging Video Quality Scoring and Justification via Large Multimodal Models", "authors": ["Qizhi Xie", "Kun Yuan", "Yunpeng Qu", "Jiachao Gong", "Mingda Wu", "Ming Sun", "Chao Zhou", "Jihong Zhu"], "summary": "Classical video quality assessment (VQA) methods generate a numerical score\nto judge a video's perceived visual fidelity and clarity. Yet, a score fails to\ndescribe the video's complex quality dimensions, restricting its applicability.\nBenefiting from the linguistic output, adapting video large multimodal models\n(LMMs) to VQA via instruction tuning has the potential to address this issue.\nThe core of the approach lies in the video quality-centric instruction data.\nPrevious explorations mainly focus on the image domain, and their data\ngeneration processes heavily rely on human quality annotations and proprietary\nsystems, limiting data scalability and effectiveness. To address these\nchallenges, we propose the Score-based Instruction Generation (SIG) pipeline.\nSpecifically, SIG first scores multiple quality dimensions of an unlabeled\nvideo and maps scores to text-defined levels. It then explicitly incorporates a\nhierarchical Chain-of-Thought (CoT) to model the correlation between specific\ndimensions and overall quality, mimicking the human visual system's reasoning\nprocess. The automated pipeline eliminates the reliance on expert-written\nquality descriptions and proprietary systems, ensuring data scalability and\ngeneration efficiency. To this end, the resulting Score2Instruct (S2I) dataset\ncontains over 320K diverse instruction-response pairs, laying the basis for\ninstruction tuning. Moreover, to advance video LMMs' quality scoring and\njustification abilities simultaneously, we devise a progressive tuning strategy\nto fully unleash the power of S2I. Built upon SIG, we further curate a\nbenchmark termed S2I-Bench with 400 open-ended questions to better evaluate the\nquality justification capacity of video LMMs. Experimental results on the\nS2I-Bench and existing benchmarks indicate that our method consistently\nimproves quality scoring and justification capabilities across multiple video\nLMMs.", "comment": "15 pages, 4 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2506.21011v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21011v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过大型多模态模型连接视频质量评分与理由阐释", "tldr": "本文提出了一种名为SIG的自动化流程，用于生成大规模视频质量评估的指令数据，并构建了Score2Instruct数据集和S2I-Bench基准，显著提升了视频大型多模态模型的质量评分和理由阐释能力。", "motivation": "传统的视频质量评估（VQA）方法只生成一个分数，无法描述视频复杂的质量维度，限制了其适用性。现有探索主要集中在图像领域，且数据生成依赖人工标注和专有系统，限制了数据可扩展性和有效性。", "method": "提出了基于分数的指令生成（SIG）流程：首先对未标注视频的多个质量维度进行评分，并将分数映射到文本定义的级别；然后明确引入分层思维链（CoT）来模拟人类视觉系统的推理过程，建模特定维度与整体质量之间的关联。该自动化流程消除了对专家编写质量描述和专有系统的依赖。在此基础上，构建了包含320K+指令-响应对的Score2Instruct (S2I) 数据集，并设计了渐进式微调策略。此外，还构建了一个包含400个开放式问题的S2I-Bench基准来评估视频LMM的质量理由阐释能力。", "result": "在S2I-Bench和现有基准上的实验结果表明，该方法持续改进了多种视频大型多模态模型的质量评分和理由阐释能力。", "conclusion": "通过自动化数据生成和渐进式微调策略，成功地提升了大型多模态模型在视频质量评估中同时进行评分和理由阐释的能力，解决了传统方法的局限性。", "translation": "经典视频质量评估（VQA）方法生成一个数值分数来判断视频感知的视觉保真度和清晰度。然而，一个分数无法描述视频复杂的质量维度，限制了其适用性。受益于语言输出，通过指令微调将视频大型多模态模型（LMMs）应用于VQA有潜力解决这个问题。该方法的核心在于以视频质量为中心的指令数据。先前的探索主要集中在图像领域，其数据生成过程严重依赖人工质量标注和专有系统，限制了数据的可扩展性和有效性。为了解决这些挑战，我们提出了基于分数的指令生成（SIG）流程。具体而言，SIG首先对未标注视频的多个质量维度进行评分，并将分数映射到文本定义的级别。然后，它明确地引入了分层思维链（CoT）来建模特定维度与整体质量之间的相关性，模仿人类视觉系统的推理过程。自动化流程消除了对专家编写质量描述和专有系统的依赖，确保了数据的可扩展性和生成效率。为此，生成的Score2Instruct (S2I) 数据集包含超过32万个多样化的指令-响应对，为指令微调奠定了基础。此外，为了同时提升视频LMMs的质量评分和理由阐释能力，我们设计了一种渐进式微调策略以充分释放S2I的潜力。在SIG的基础上，我们进一步策划了一个名为S2I-Bench的基准，包含400个开放式问题，以更好地评估视频LMMs的质量理由阐释能力。在S2I-Bench和现有基准上的实验结果表明，我们的方法持续改进了多种视频LMMs的质量评分和理由阐释能力。", "summary": "本文旨在解决传统视频质量评估（VQA）方法仅提供分数而缺乏详细质量维度描述的问题。研究提出了一种名为“基于分数的指令生成（SIG）”的自动化流程，该流程通过对未标注视频进行多维度评分并将分数映射为文本级别，并结合分层思维链（CoT）来模拟人类推理过程，从而生成大规模、高质量的视频质量评估指令数据。基于此，构建了包含超过320K指令-响应对的Score2Instruct (S2I) 数据集，并开发了渐进式微调策略以提升视频大型多模态模型（LMMs）的质量评分和理由阐释能力。为评估模型性能，还创建了S2I-Bench基准。实验结果表明，该方法显著提升了视频LMMs在质量评分和理由阐释方面的能力。", "keywords": "视频质量评估, 大型多模态模型, 指令微调, 数据生成, 思维链", "comments": "本文的创新点在于提出了一个自动化、可扩展的视频质量评估指令数据生成流程（SIG），摆脱了对人工标注和专有系统的依赖，解决了大规模高质量数据获取的瓶颈问题。通过结合思维链（CoT）模拟人类推理过程，使得LMMs不仅能评分还能提供质量理由，极大地增强了VQA的实用性。构建的S2I数据集和S2I-Bench基准为未来研究提供了宝贵的资源。"}}
{"id": "2506.21039", "title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning", "authors": ["Jaebak Hwang", "Sanghyeon Lee", "Jeongmo Kim", "Seungyul Han"], "summary": "Long-horizon goal-conditioned tasks pose fundamental challenges for\nreinforcement learning (RL), particularly when goals are distant and rewards\nare sparse. While hierarchical and graph-based methods offer partial solutions,\nthey often suffer from subgoal infeasibility and inefficient planning. We\nintroduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL\nframework that enforces single-step subgoal reachability by structurally\nconstraining high-level decision-making. To enhance exploration, SSE employs a\ndecoupled exploration policy that systematically traverses underexplored\nregions of the goal space. Furthermore, a failure-aware path refinement, which\nrefines graph-based planning by dynamically adjusting edge costs according to\nobserved low-level success rates, thereby improving subgoal reliability.\nExperimental results across diverse long-horizon benchmarks demonstrate that\nSSE consistently outperforms existing goal-conditioned RL and hierarchical RL\napproaches in both efficiency and success rate.", "comment": "9 technical page followed by references and appendix", "pdf_url": "http://arxiv.org/pdf/2506.21039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21039v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "严格子目标执行：分层强化学习中可靠的长周期规划", "tldr": "引入了严格子目标执行（SSE），一个基于图的分层强化学习框架，通过强制单步子目标可达性、解耦探索策略和失败感知路径细化来解决长周期任务中的子目标不可行性和规划效率低下问题，并在效率和成功率上超越现有方法。", "motivation": "长周期目标条件任务对强化学习（RL）提出了根本性挑战，特别是当目标遥远且奖励稀疏时。现有的分层和基于图的方法通常存在子目标不可行性和规划效率低下的问题。", "method": "我们引入了严格子目标执行（SSE），这是一个基于图的分层强化学习框架，它通过结构化约束高层决策来强制执行单步子目标可达性。为了增强探索，SSE采用了一种解耦探索策略，系统地遍历目标空间中未充分探索的区域。此外，还引入了失败感知路径细化，通过根据观察到的低层成功率动态调整边缘成本来改进基于图的规划，从而提高子目标的可靠性。", "result": "在各种长周期基准测试中，实验结果表明SSE在效率和成功率方面始终优于现有的目标条件RL和分层RL方法。", "conclusion": "严格子目标执行（SSE）是一种有效且可靠的分层强化学习框架，能够显著提高长周期目标条件任务的规划效率和成功率，解决了现有方法中子目标不可行性和规划效率低下的问题。", "translation": "长周期目标条件任务对强化学习（RL）提出了根本性挑战，特别是当目标遥远且奖励稀疏时。虽然分层和基于图的方法提供了部分解决方案，但它们常常面临子目标不可行性和规划效率低下的问题。我们引入了严格子目标执行（SSE），一个基于图的分层RL框架，通过结构化约束高层决策来强制执行单步子目标可达性。为了增强探索，SSE采用了一种解耦探索策略，系统地遍历目标空间中未充分探索的区域。此外，失败感知路径细化通过根据观察到的低层成功率动态调整边缘成本来改进基于图的规划，从而提高子目标的可靠性。在各种长周期基准测试中，实验结果表明SSE在效率和成功率方面始终优于现有的目标条件RL和分层RL方法。", "summary": "本文提出了严格子目标执行（SSE），一种针对长周期目标条件任务的分层强化学习框架。该框架通过结构化约束高层决策来确保单步子目标的可达性，并结合了用于高效探索的解耦策略以及根据低层成功率动态调整规划路径的失败感知细化机制。实验证明，SSE在效率和成功率上均优于现有方法，有效解决了分层RL中子目标不可行和规划效率低下的问题。", "keywords": "分层强化学习, 长周期规划, 子目标执行, 图基方法, 探索策略", "comments": "SSE的创新之处在于其通过结构化约束强制子目标可达性，并结合了解耦探索和失败感知路径细化，这些机制共同提升了长周期规划的可靠性和效率。其重要性在于为解决稀疏奖励和遥远目标的复杂RL任务提供了更稳健的解决方案。"}}
{"id": "2506.21012", "title": "FedSC: Federated Learning with Semantic-Aware Collaboration", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Jiahua Shi", "Jun Shen"], "summary": "Federated learning (FL) aims to train models collaboratively across clients\nwithout sharing data for privacy-preserving. However, one major challenge is\nthe data heterogeneity issue, which refers to the biased labeling preferences\nat multiple clients. A number of existing FL methods attempt to tackle data\nheterogeneity locally (e.g., regularizing local models) or globally (e.g.,\nfine-tuning global model), often neglecting inherent semantic information\ncontained in each client. To explore the possibility of using intra-client\nsemantically meaningful knowledge in handling data heterogeneity, in this\npaper, we propose Federated Learning with Semantic-Aware Collaboration (FedSC)\nto capture client-specific and class-relevant knowledge across heterogeneous\nclients. The core idea of FedSC is to construct relational prototypes and\nconsistent prototypes at semantic-level, aiming to provide fruitful class\nunderlying knowledge and stable convergence signals in a prototype-wise\ncollaborative way. On the one hand, FedSC introduces an inter-contrastive\nlearning strategy to bring instance-level embeddings closer to relational\nprototypes with the same semantics and away from distinct classes. On the other\nhand, FedSC devises consistent prototypes via a discrepancy aggregation manner,\nas a regularization penalty to constrain the optimization region of the local\nmodel. Moreover, a theoretical analysis for FedSC is provided to ensure a\nconvergence guarantee. Experimental results on various challenging scenarios\ndemonstrate the effectiveness of FedSC and the efficiency of crucial\ncomponents.", "comment": "12 pages, KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.21012v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21012v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FedSC：语义感知协作的联邦学习", "tldr": "FedSC通过构建语义级原型来解决联邦学习中的数据异质性问题，以实现语义感知协作。", "motivation": "联邦学习面临数据异质性（客户端偏好标签）的挑战，现有方法常忽略客户端固有的语义信息。本文旨在探索利用客户端内部语义信息来解决数据异质性问题。", "method": "本文提出了FedSC（语义感知协作的联邦学习），其核心思想是在语义层面构建关系原型和一致性原型，以提供类别的潜在知识和稳定的收敛信号。FedSC引入了跨对比学习策略，使实例级嵌入更接近相同语义的关系原型并远离不同类别。此外，FedSC通过差异聚合方式设计了一致性原型，作为正则化惩罚来约束局部模型的优化区域。", "result": "在各种挑战性场景下的实验结果表明，FedSC的有效性和关键组件的效率。", "conclusion": "FedSC通过语义感知协作，利用语义级原型有效地解决了联邦学习中的数据异质性问题，并提供了收敛保证。", "translation": "联邦学习（FL）旨在在不共享数据的情况下跨客户端协作训练模型，以保护隐私。然而，一个主要的挑战是数据异质性问题，这指的是多个客户端的标签偏好存在偏差。许多现有的联邦学习方法试图在本地（例如，正则化本地模型）或全局（例如，微调全局模型）解决数据异质性问题，但通常忽略了每个客户端中包含的固有语义信息。为了探索在处理数据异质性时使用客户端内部语义有意义知识的可能性，本文提出了语义感知协作的联邦学习（FedSC），以捕获异构客户端的客户端特定和类相关知识。FedSC的核心思想是在语义层面构建关系原型和一致性原型，旨在以原型协作的方式提供丰富的类别底层知识和稳定的收敛信号。一方面，FedSC引入了一种跨对比学习策略，使实例级嵌入更接近具有相同语义的关系原型，并远离不同的类别。另一方面，FedSC通过差异聚合方式设计了一致性原型，作为正则化惩罚来约束局部模型的优化区域。此外，本文还提供了FedSC的理论分析，以确保收敛性。在各种挑战性场景下的实验结果表明了FedSC的有效性和关键组件的效率。", "summary": "本文提出了一种名为FedSC（语义感知协作的联邦学习）的新方法，旨在解决联邦学习中的数据异质性问题，该问题源于客户端之间标签偏好的偏差。与现有方法不同，FedSC利用客户端内部的语义信息，通过构建语义层面的关系原型和一致性原型来实现语义感知协作。具体而言，FedSC采用跨对比学习策略使实例嵌入与语义原型对齐，并通过差异聚合生成一致性原型作为正则化。理论分析证明了其收敛性，实验结果也验证了FedSC在各种复杂场景下的有效性。", "keywords": "联邦学习, 数据异质性, 语义感知, 原型, 协作", "comments": "FedSC的创新之处在于其通过引入语义层面的原型（关系原型和一致性原型）来解决联邦学习中的数据异质性问题，这弥补了传统方法忽略固有语义信息的不足。其结合跨对比学习和正则化惩罚的设计思路，为在保护隐私的同时提升模型性能提供了新的视角。理论收敛性保证增加了其可靠性，实验结果也验证了其有效性。"}}
{"id": "2506.21044", "title": "Efficient Skill Discovery via Regret-Aware Optimization", "authors": ["He Zhang", "Ming Zhou", "Shaopeng Zhai", "Ying Sun", "Hui Xiong"], "summary": "Unsupervised skill discovery aims to learn diverse and distinguishable\nbehaviors in open-ended reinforcement learning. For existing methods, they\nfocus on improving diversity through pure exploration, mutual information\noptimization, and learning temporal representation. Despite that they perform\nwell on exploration, they remain limited in terms of efficiency, especially for\nthe high-dimensional situations. In this work, we frame skill discovery as a\nmin-max game of skill generation and policy learning, proposing a regret-aware\nmethod on top of temporal representation learning that expands the discovered\nskill space along the direction of upgradable policy strength. The key insight\nbehind the proposed method is that the skill discovery is adversarial to the\npolicy learning, i.e., skills with weak strength should be further explored\nwhile less exploration for the skills with converged strength. As an\nimplementation, we score the degree of strength convergence with regret, and\nguide the skill discovery with a learnable skill generator. To avoid\ndegeneration, skill generation comes from an up-gradable population of skill\ngenerators. We conduct experiments on environments with varying complexities\nand dimension sizes. Empirical results show that our method outperforms\nbaselines in both efficiency and diversity. Moreover, our method achieves a 15%\nzero shot improvement in high-dimensional environments, compared to existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21044v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21044v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "效率化技能发现通过后悔感知优化", "tldr": "本文提出一种后悔感知方法，将无监督技能发现视为一个最小-最大博弈，以提高在高维环境下的效率和多样性，并通过实验证明其优于现有方法。", "motivation": "现有无监督技能发现方法在探索方面表现良好，但在效率上，尤其是在高维情况下存在局限性。", "method": "将技能发现构建为技能生成和策略学习的最小-最大博弈。提出一种后悔感知方法，该方法基于时间表征学习，并沿着可升级策略强度的方向扩展已发现的技能空间。核心思想是技能发现与策略学习是对抗性的，即对强度较弱的技能进行更多探索，而对强度已收敛的技能则进行较少探索。通过后悔来评估强度收敛程度，并用可学习的技能生成器引导技能发现。为避免退化，技能生成来自一个可升级的技能生成器群体。", "result": "该方法在效率和多样性方面均优于现有基线方法。在高维环境中实现了15%的零样本提升。", "conclusion": "本文提出的后悔感知技能发现方法有效解决了现有方法在效率上的局限性，特别是在高维场景下表现出色，提升了技能发现的效率和多样性。", "translation": "无监督技能发现旨在在开放式强化学习中学习多样化和可区分的行为。现有方法侧重于通过纯粹探索、互信息优化和学习时间表征来提高多样性。尽管它们在探索方面表现良好，但在效率方面仍然受限，特别是在高维情况下。在这项工作中，我们将技能发现构建为技能生成和策略学习的最小-最大博弈，提出了一种基于时间表征学习的后悔感知方法，该方法沿着可升级策略强度的方向扩展已发现的技能空间。该方法背后的关键见解是，技能发现与策略学习是对抗性的，即对于强度较弱的技能应进行更多探索，而对于强度已收敛的技能则应进行较少探索。作为一种实现方式，我们用后悔来评估强度收敛程度，并用可学习的技能生成器引导技能发现。为避免退化，技能生成来自一个可升级的技能生成器群体。我们在不同复杂度和维度大小的环境中进行了实验。经验结果表明，我们的方法在效率和多样性方面均优于基线方法。此外，与现有方法相比，我们的方法在高维环境中实现了15%的零样本提升。", "summary": "本文提出了一种名为“后悔感知优化”的无监督技能发现方法，旨在解决现有方法在高维环境中效率低下的问题。该方法将技能发现建模为一个最小-最大博弈，通过评估策略强度收敛程度并引导技能生成器，选择性地探索未充分掌握的技能，从而有效提升了技能发现的效率和多样性，并在高维任务中取得了显著的性能提升。", "keywords": "无监督技能发现, 后悔感知, 强化学习, 最小-最大博弈, 技能生成", "comments": "这篇论文通过引入“后悔感知”机制，将技能发现转化为一个动态的、对抗性的学习过程，巧妙地解决了现有无监督技能发现方法在效率上的瓶颈，尤其是在处理高维数据时。其创新点在于将后悔理论应用于技能探索的决策，使得算法能够智能地分配探索资源，避免对已掌握技能的过度探索，从而提高了整体学习效率。"}}
{"id": "2506.21521", "title": "Potemkin Understanding in Large Language Models", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21521v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21521v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "大型语言模型中的波将金式理解", "tldr": "本文引入了一个框架来质疑LLM基准测试的有效性，指出LLM在基准测试上的成功可能是一种“波将金式理解”（虚假理解），源于其概念表征的内部不连贯性，与人类理解方式不同。研究发现这种现象普遍存在。", "motivation": "本文旨在探讨大型语言模型（LLMs）在基准数据集上进行评估的合理性，并质疑LLMs基于这些策展问题集的答案所进行的推断是否真实反映其能力。特别是，它提出如果LLMs的误解方式不像人类，那么它们在基准测试上的成功可能只是一种“波将金式理解”（Potemkin understanding），即一种由与人类概念解释方式不符的答案所驱动的理解幻觉。", "method": "本研究首先引入了一个形式化框架来解决LLM基准测试有效性的问题。其核心观点是，用于测试LLMs的基准（如AP考试）也用于测试人类，因此，只有当LLMs的误解方式与人类误解方式相似时，这些基准测试才有效。为量化“波将金式理解”的存在，论文提出了两种程序：一种是使用在三个领域专门设计的基准测试；另一种是提供其普遍性下限的通用程序。", "result": "研究发现，“波将金式理解”（potemkins）在不同模型、任务和领域中普遍存在。此外，这些失败不仅反映了不正确的理解，还反映了概念表征中更深层次的内部不连贯性。", "conclusion": "大型语言模型在基准测试上的成功可能只是一种“波将金式理解”，即一种理解的幻觉，其根本原因在于其内部概念表征的深层不连贯性，这与人类解释概念的方式截然不同。", "translation": "大型语言模型（LLMs）通常通过基准数据集进行评估。但是，基于LLM对一组精选问题的回答来推断其能力，这种做法的合理性何在？本文首先引入了一个形式化框架来解决这个问题。关键在于，用于测试LLMs的基准——例如AP考试——也用于测试人类。然而，这带来一个启示：只有当LLMs对概念的误解方式与人类的误解方式相似时，这些基准测试才是有效的。否则，在基准测试上的成功仅证明了波将金式理解（potemkin understanding）：一种由与任何人类解释概念方式不符的答案所驱动的理解幻觉。我们提出了两种量化波将金式理解存在的方法：一种是在三个领域使用专门设计的基准，另一种是提供其普遍性下限的通用程序。我们发现波将金式理解在各种模型、任务和领域中普遍存在。我们还发现这些失败不仅反映了不正确的理解，而且反映了概念表征中更深层次的内部不连贯性。", "summary": "本文引入了一个形式化框架，旨在探讨大型语言模型（LLMs）在基准测试中表现出的“理解”是否真实。论文指出，如果LLMs的误解方式不像人类，那么其在基准测试上的成功可能只是一种“波将金式理解”，即一种虚假的理解幻觉。研究提出了两种量化这种现象的方法：一种是使用专门设计的基准，另一种是通用程序。结果表明，“波将金式理解”在不同LLM、任务和领域中普遍存在，并且这种失败反映了LLM内部概念表征的深层不连贯性。", "keywords": "大型语言模型, 基准测试, 波将金式理解, 概念表征, 评估", "comments": "本文通过引入“波将金式理解”这一概念，对当前大型语言模型评估的有效性提出了深刻质疑，具有重要的创新性。它不仅提供了一个形式化的框架来分析LLM的理解，还提出了量化这种虚假理解的方法。研究结果揭示了LLM内部概念表征可能存在的深层不连贯性，这对于理解LLM的局限性及其未来发展方向具有重要意义。该研究强调了仅凭基准测试表现来判断LLM真正理解能力的不足。"}}
{"id": "2506.21015", "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation", "authors": ["Qingyue Jiao", "Kangyu Zheng", "Yiyu Shi", "Zhiding Liang"], "summary": "Machine learning-assisted diagnosis is gaining traction in skin disease\ndetection, but training effective models requires large amounts of high-quality\ndata. Skin disease datasets often suffer from class imbalance, privacy\nconcerns, and object bias, making data augmentation essential. While classical\ngenerative models are widely used, they demand extensive computational\nresources and lengthy training time. Quantum computing offers a promising\nalternative, but existing quantum-based image generation methods can only yield\ngrayscale low-quality images. Through a novel classical-quantum latent space\nfusion technique, our work overcomes this limitation and introduces the first\nclassical-quantum generative adversarial network (GAN) capable of generating\ncolor medical images. Our model outperforms classical deep convolutional GANs\nand existing hybrid classical-quantum GANs in both image generation quality and\nclassification performance boost when used as data augmentation. Moreover, the\nperformance boost is comparable with that achieved using state-of-the-art\nclassical generative models, yet with over 25 times fewer parameters and 10\ntimes fewer training epochs. Such results suggest a promising future for\nquantum image generation as quantum hardware advances. Finally, we demonstrate\nthe robust performance of our model on real IBM quantum machine with hardware\nnoise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21015v1", "categories": ["cs.CV", "cs.LG", "quant-ph"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21015v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "混合Q：用于皮肤病图像生成的混合经典-量子生成对抗网络", "tldr": "本文提出HybridQ，首个能生成彩色医学图像的混合经典-量子GAN，相比经典GAN，其参数更少、训练更快，且在皮肤病数据增强方面表现优越，展现了量子图像生成的巨大潜力。", "motivation": "机器学习辅助诊断皮肤病需要大量高质量数据，但现有数据集存在类别不平衡、隐私和偏差问题，且经典生成模型计算资源需求大、训练时间长。现有量子图像生成方法仅能生成低质量灰度图像。", "method": "本文提出了HybridQ，一个新颖的经典-量子生成对抗网络（GAN），通过独特的经典-量子潜在空间融合技术，克服了现有量子方法只能生成灰度图像的限制，实现了彩色医学图像的生成。", "result": "HybridQ在图像生成质量和作为数据增强时的分类性能提升方面，优于经典的深度卷积GAN和现有混合经典-量子GAN。其性能提升可与最先进的经典生成模型媲美，但参数减少25倍以上，训练周期减少10倍。该模型在具有硬件噪声的真实IBM量子机上表现出鲁棒性能。", "conclusion": "混合经典-量子GAN在生成彩色医学图像方面表现出色，且显著减少了模型参数和训练时间，这表明随着量子硬件的进步，量子图像生成具有广阔的应用前景。", "translation": "机器学习辅助诊断在皮肤病检测中越来越受欢迎，但训练有效的模型需要大量高质量数据。皮肤病数据集常面临类别不平衡、隐私问题和对象偏差，使得数据增强变得至关重要。虽然经典生成模型被广泛使用，但它们需要大量的计算资源和漫长的训练时间。量子计算提供了一个有前景的替代方案，但现有的基于量子的图像生成方法只能产生灰度低质量图像。通过一种新颖的经典-量子潜在空间融合技术，我们的工作克服了这一限制，并引入了第一个能够生成彩色医学图像的经典-量子生成对抗网络（GAN）。我们的模型在图像生成质量和作为数据增强时的分类性能提升方面，均优于经典的深度卷积GAN和现有混合经典-量子GAN。此外，其性能提升与使用最先进的经典生成模型所达到的效果相当，但参数减少了25倍以上，训练周期减少了10倍。这些结果表明，随着量子硬件的发展，量子图像生成具有广阔前景。最后，我们展示了我们的模型在具有硬件噪声的真实IBM量子机上的鲁棒性能。", "summary": "本文提出HybridQ，一个创新的混合经典-量子GAN，旨在解决皮肤病数据集的数据不足问题。它通过独特的经典-量子潜在空间融合技术，首次实现了彩色医学图像的生成。实验证明，HybridQ在图像质量和数据增强效果上均优于现有经典和混合GAN，同时显著减少了模型参数和训练时间，并在真实量子硬件上验证了其鲁棒性，预示着量子图像生成领域的巨大潜力。", "keywords": "混合经典-量子GAN, 皮肤病图像生成, 数据增强, 量子计算, 彩色医学图像", "comments": "该工作在量子图像生成领域迈出了重要一步，首次实现了彩色医学图像的生成，具有显著的创新性。其利用量子计算的优势，在减少模型复杂度和训练时间的同时，保持了与SOTA经典模型相当的性能，这对于资源受限的医学图像处理领域尤为重要。在真实量子硬件上的验证也增加了其实用性。"}}
{"id": "2506.21054", "title": "FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning", "authors": ["Fu Peng", "Ming Tang"], "summary": "In federated learning (FL), the data distribution of each client may change\nover time, introducing both temporal and spatial data heterogeneity, known as\nconcept drift. Data heterogeneity arises from three drift sources: real drift\n(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the\ninput distribution P(x)), and label drift (a shift in the label distribution\nP(y)). However, most existing FL methods addressing concept drift primarily\nfocus on real drift. When clients experience virtual or label drift, these\nmethods often fail to selectively retain useful historical knowledge, leading\nto catastrophic forgetting. A key challenge lies in distinguishing different\nsources of drift, as they require distinct adaptation strategies: real drift\ncalls for discarding outdated data, while virtual or label drift benefits from\nretaining historical data. Without explicitly identifying the drift sources, a\ngeneral adaptation strategy is suboptimal and may harm generalization. To\naddress this challenge, we propose FedDAA, a dynamic clustered FL framework\ndesigned to adapt to multi-source concept drift while preserving valuable\nhistorical knowledge. Specifically, FedDAA integrates three modules: a cluster\nnumber determination module to find the optimal number of clusters; a real\ndrift detection module to distinguish real drift from virtual/label drift; and\na concept drift adaptation module to adapt to new data while retaining useful\nhistorical information. We provide theoretical convergence guarantees, and\nexperiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over\nstate-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21054v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21054v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FedDAA：联邦学习中用于概念漂移适应的动态客户端聚类", "tldr": "FedDAA提出了一种动态聚类联邦学习框架，通过区分不同类型的概念漂移来适应多源概念漂移，并有效保留历史知识。", "motivation": "联邦学习中，客户端数据分布随时间变化，引入概念漂移（包括真实漂移、虚拟漂移和标签漂移），现有方法主要关注真实漂移，导致在虚拟或标签漂移时无法有效保留历史知识，从而出现灾难性遗忘。区分不同漂移源并采取不同适应策略是关键挑战。", "method": "提出FedDAA框架，包含三个模块：聚类数量确定模块（寻找最优聚类数）、真实漂移检测模块（区分真实漂移与虚拟/标签漂移）和概念漂移适应模块（适应新数据同时保留有用历史信息）。", "result": "FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上，比最先进方法实现了7.84%至8.52%的准确率提升。", "conclusion": "FedDAA成功解决了联邦学习中多源概念漂移的适应问题，通过动态客户端聚类和漂移源区分，有效保留了历史知识并提高了模型性能。", "translation": "在联邦学习（FL）中，每个客户端的数据分布可能随时间变化，引入时间和空间数据异质性，即概念漂移。数据异质性来源于三种漂移源：真实漂移（条件分布P(y|x)的变化）、虚拟漂移（输入分布P(x)的变化）和标签漂移（标签分布P(y)的变化）。然而，大多数现有解决概念漂移的FL方法主要关注真实漂移。当客户端经历虚拟漂移或标签漂移时，这些方法往往无法选择性地保留有用的历史知识，导致灾难性遗忘。一个关键挑战在于区分不同漂移源，因为它们需要不同的适应策略：真实漂移需要丢弃过时数据，而虚拟漂移或标签漂移则受益于保留历史数据。如果不明确识别漂移源，通用的适应策略将是次优的，并可能损害泛化能力。为了解决这一挑战，我们提出了FedDAA，一个动态聚类的联邦学习框架，旨在适应多源概念漂移同时保留有价值的历史知识。具体来说，FedDAA集成了三个模块：一个聚类数量确定模块用于找到最优聚类数；一个真实漂移检测模块用于区分真实漂移与虚拟/标签漂移；以及一个概念漂移适应模块用于适应新数据同时保留有用的历史信息。我们提供了理论收敛保证，实验表明FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100上比最先进的方法实现了7.84%到8.52%的准确率提升。", "summary": "该论文提出了FedDAA，一个动态聚类的联邦学习框架，旨在解决联邦学习中由多源概念漂移引起的数据异质性问题。FedDAA通过区分真实漂移、虚拟漂移和标签漂移，并集成聚类数量确定、真实漂移检测和概念漂移适应模块，有效地保留历史知识并适应新数据，从而避免灾难性遗忘。实验证明，FedDAA在多个数据集上显著优于现有SOTA方法。", "keywords": "联邦学习, 概念漂移, 动态聚类, 数据异质性, 灾难性遗忘", "comments": "FedDAA的创新点在于明确区分了联邦学习中概念漂移的不同来源（真实、虚拟、标签漂移），并针对性地设计了适应策略，这解决了现有方法普遍存在的“一刀切”问题。其动态聚类和历史知识保留机制对于提高联邦学习在非稳态环境下的鲁棒性和泛化能力具有重要意义。理论收敛保证增加了其可信度。"}}
{"id": "2506.21017", "title": "Multimodal Prompt Alignment for Facial Expression Recognition", "authors": ["Fuyan Ma", "Yiran He", "Bin Sun", "Shutao Li"], "summary": "Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs) like CLIP for various downstream tasks. Despite their success,\ncurrent VLM-based facial expression recognition (FER) methods struggle to\ncapture fine-grained textual-visual relationships, which are essential for\ndistinguishing subtle differences between facial expressions. To address this\nchallenge, we propose a multimodal prompt alignment framework for FER, called\nMPA-FER, that provides fine-grained semantic guidance to the learning process\nof prompted visual features, resulting in more precise and interpretable\nrepresentations. Specifically, we introduce a multi-granularity hard prompt\ngeneration strategy that utilizes a large language model (LLM) like ChatGPT to\ngenerate detailed descriptions for each facial expression. The LLM-based\nexternal knowledge is injected into the soft prompts by minimizing the feature\ndiscrepancy between the soft prompts and the hard prompts. To preserve the\ngeneralization abilities of the pretrained CLIP model, our approach\nincorporates prototype-guided visual feature alignment, ensuring that the\nprompted visual features from the frozen image encoder align closely with\nclass-specific prototypes. Additionally, we propose a cross-modal global-local\nalignment module that focuses on expression-relevant facial features, further\nimproving the alignment between textual and visual features. Extensive\nexperiments demonstrate our framework outperforms state-of-the-art methods on\nthree FER benchmark datasets, while retaining the benefits of the pretrained\nmodel and minimizing computational costs.", "comment": "To appear in ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.21017v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21017v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多模态提示对齐用于面部表情识别", "tldr": "本文提出了一种名为MPA-FER的多模态提示对齐框架，通过引入LLM生成的细粒度硬提示和多级对齐机制，显著提升了基于VLM的面部表情识别性能。", "motivation": "当前基于视觉-语言模型（VLM）的面部表情识别（FER）方法难以捕捉细粒度的文本-视觉关系，而这对于区分面部表情的细微差异至关重要。", "method": "本文提出了MPA-FER框架。具体而言，它引入了一种多粒度硬提示生成策略，利用大型语言模型（LLM，如ChatGPT）为每个面部表情生成详细描述。通过最小化软提示与硬提示之间的特征差异，将LLM的外部知识注入软提示中。为了保留预训练CLIP模型的泛化能力，该方法还结合了原型引导的视觉特征对齐，确保冻结图像编码器产生的提示视觉特征与特定类别原型紧密对齐。此外，还提出了一个跨模态全局-局部对齐模块，专注于与表情相关的面部特征，进一步改善文本和视觉特征之间的对齐。", "result": "在三个FER基准数据集上，MPA-FER框架的表现优于最先进的方法，同时保留了预训练模型的优势并最大限度地降低了计算成本。", "conclusion": "本文提出的多模态提示对齐框架（MPA-FER）通过整合LLM生成的细粒度语义指导和多级对齐机制，有效解决了VLM在面部表情识别中捕捉细微文本-视觉关系的挑战，显著提升了识别性能并保持了模型效率。", "translation": "提示学习已被广泛应用于有效适应视觉-语言模型（VLMs）如CLIP，以执行各种下游任务。尽管取得了成功，但当前基于VLM的面部表情识别（FER）方法难以捕捉细粒度的文本-视觉关系，而这对于区分面部表情的细微差异至关重要。为了解决这一挑战，我们提出了一种用于FER的多模态提示对齐框架，称为MPA-FER，它为提示视觉特征的学习过程提供细粒度的语义指导，从而获得更精确和可解释的表示。具体而言，我们引入了一种多粒度硬提示生成策略，该策略利用大型语言模型（LLM），如ChatGPT，为每个面部表情生成详细描述。通过最小化软提示与硬提示之间的特征差异，将基于LLM的外部知识注入软提示中。为了保留预训练CLIP模型的泛化能力，我们的方法结合了原型引导的视觉特征对齐，确保冻结图像编码器产生的提示视觉特征与特定类别原型紧密对齐。此外，我们提出了一个跨模态全局-局部对齐模块，专注于与表情相关的面部特征，进一步改善文本和视觉特征之间的对齐。广泛的实验表明，我们的框架在三个FER基准数据集上优于最先进的方法，同时保留了预训练模型的优势并最大限度地降低了计算成本。", "summary": "本文提出MPA-FER，一个用于面部表情识别的多模态提示对齐框架，旨在解决现有VLM方法在捕获细粒度文本-视觉关系方面的不足。该框架通过利用LLM生成多粒度硬提示来提供细致的语义指导，并将这些知识注入软提示。同时，它结合了原型引导的视觉特征对齐和跨模态全局-局部对齐模块，以确保视觉特征与类别原型紧密对齐并聚焦于表情相关特征。实验结果表明，MPA-FER在多个FER基准数据集上超越了现有最佳方法，同时保持了预训练模型的优点并降低了计算成本。", "keywords": "面部表情识别, 提示学习, 多模态, 视觉-语言模型, CLIP", "comments": "该论文的创新点在于将LLM生成的细粒度语义知识引入到VLM的提示学习中，并通过多级对齐机制（软硬提示对齐、原型引导对齐、跨模态全局-局部对齐）来增强文本-视觉特征的匹配度。这不仅提升了面部表情识别的精度，也为VLM在细粒度识别任务中的应用提供了新的思路，具有较高的研究价值和实际应用潜力。"}}
{"id": "2506.21071", "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph", "authors": ["Jingwei Wang", "Zai Zhang", "Hao Qian", "Chunjing Gan", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "Bin Shi", "Bo Dong"], "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs.", "comment": "20 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.21071v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21071v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "利用知识图谱的高质量指令数据增强LLM的工具使用能力", "tldr": "本文提出一种利用知识图谱生成高质量指令数据的新方法，以提高大型语言模型（LLM）的工具使用和整体能力，实验证明少量合成数据即可显著提升效果。", "motivation": "教导大型语言模型（LLM）使用工具对提升其解决问题能力和扩展应用至关重要。然而，有效使用工具具有挑战性，因为它需要深入理解工具功能和用户意图。以往方法主要依赖LLM生成指令数据，但数据质量往往不足。", "method": "本文提出一种新方法，利用知识图谱生成高质量指令数据。首先，从知识图谱中提取多种查询路径，将其转化为广泛的用户查询。然后，将实体间的关系转化为可操作的工具，并将每个查询的路径解析为详细的解决方案步骤，从而创建高质量指令数据。", "result": "实验表明，仅用少量这种合成数据进行微调，就能显著提升LLM的工具利用率和整体能力。", "conclusion": "通过利用知识图谱生成高质量指令数据，可以有效增强大型语言模型的工具使用能力和整体性能。", "translation": "教导大型语言模型（LLM）使用工具对于提高其解决问题能力和扩展其应用至关重要。然而，有效使用工具具有挑战性，因为它需要深入理解工具功能和用户意图。以前的方法主要依赖LLM生成指令数据，但这些数据的质量通常不足。在本文中，我们提出了一种新方法，利用知识图谱为LLM生成高质量的指令数据。知识图谱是人工整理的、富含语义信息的数据集。我们首先从给定的知识图谱中提取各种查询路径，这些路径被转换为广泛的用户查询。然后，我们将实体之间的关系转化为可操作的工具，并将每个查询的路径解析为详细的解决方案步骤，从而创建高质量的指令数据。我们的实验表明，仅用少量这种合成数据进行微调，就能显著提升LLM的工具利用率和整体能力。", "summary": "本研究提出一种基于知识图谱的新方法，旨在解决大型语言模型（LLM）在工具使用方面指令数据质量不足的问题。通过从知识图谱中提取查询路径、转换为用户查询，并将实体关系转化为工具及解决方案步骤，生成高质量的指令数据。实验证明，利用少量此类合成数据进行微调，能够显著提升LLM的工具使用能力和整体性能。", "keywords": "LLM, 工具使用, 指令数据, 知识图谱, 微调", "comments": "该论文的创新点在于利用结构化、人工整理的知识图谱来生成高质量的LLM工具使用指令数据，而非依赖LLM自身生成，从而克服了以往数据质量不足的局限性。其重要性体现在为提升LLM的实际应用能力提供了有效的数据生成范式。"}}
{"id": "2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21545v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.21545v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "语言模型训练中的数据效能", "tldr": "本文定义并探索了“数据效能”这一新概念，旨在通过优化训练数据的组织来提升语言模型性能，提出了DELT范式及其组件（如LQS和FO），实验证明其在不增加数据或模型规模的前提下显著提升了语言模型性能。", "motivation": "现有研究主要关注数据效率，即通过选择训练数据子集来最大化性能。本文旨在补充这一领域，提出“数据效能”概念，专注于通过优化训练数据的组织来最大化性能，因为该领域相对未充分探索。", "method": "本文引入了名为DELT的通用范式，用于在语言模型训练中考虑数据效能，该范式包含数据评分、数据选择和数据排序三个核心组件。具体地，研究设计了可学习性-质量评分（LQS）作为数据评分的新实例，它从梯度一致性角度评估数据样本的可学习性和质量；同时还设计了折叠排序（FO）作为数据排序的新实例，旨在解决模型遗忘和数据分布偏差等问题。", "result": "1. 所提出的DELT范式的各种实例在不增加数据规模和模型大小的情况下，不同程度地提升了语言模型性能。2. 在这些实例中，结合了所提出的用于数据评分的LQS和用于数据排序的折叠排序，实现了最显著的性能提升。3. 通过应用数据选择，数据效能可以与数据效率协同实现。", "conclusion": "数据效能是语言模型训练中一个有前景的基础研究领域。", "translation": "数据是语言模型（LM）训练的基础。最近的研究致力于数据效率，旨在通过选择最小或最优的训练数据子集来最大化性能。数据过滤、采样和选择等技术在该领域发挥着关键作用。作为补充，我们定义了数据效能，它侧重于通过优化训练数据的组织来最大化性能，而这方面相对未充分探索。这项工作引入了一种通用范式DELT，用于在LM训练中考虑数据效能，突出了训练数据组织的重要性。DELT包含三个组件：数据评分、数据选择和数据排序。在这些组件中，我们设计了可学习性-质量评分（LQS），作为数据评分的一个新实例，它从梯度一致性角度考虑每个数据样本的可学习性和质量。我们还设计了折叠排序（FO），作为数据排序的一个新实例，它解决了模型遗忘和数据分布偏差等问题。全面的实验验证了LM训练中的数据效能，结果表明：首先，所提出的DELT的各种实例在不增加数据规模和模型大小的情况下，不同程度地提升了LM性能。其次，在这些实例中，我们提出的用于数据评分的LQS和用于数据排序的折叠排序的组合实现了最显著的改进。最后，通过应用数据选择，数据效能可以与数据效率协同实现。因此，我们认为数据效能是LM训练中一个有前景的基础领域。", "summary": "本文定义并探索了“数据效能”这一新概念，旨在通过优化训练数据的组织来提升语言模型性能，以补充现有关注数据选择的“数据效率”研究。研究提出DELT通用范式，包含数据评分、数据选择和数据排序。文中具体设计了LQS（基于梯度一致性评估数据可学习性和质量）和FO（解决模型遗忘和分布偏差）两种新方法。实验证明，DELT及其组件能在不增加数据或模型规模的前提下显著提升LM性能，其中LQS与FO的结合效果最佳，且数据效能可与数据效率协同作用。", "keywords": "数据效能, 语言模型, 训练数据组织, 数据评分, 数据排序", "comments": "本文创新性地提出了“数据效能”的概念，并构建了DELT范式，将研究重点从“选择什么数据”扩展到“如何组织数据”，填补了现有研究的空白。LQS和FO的具体设计考虑了训练过程中的实际问题，具有实用价值。这项工作为语言模型训练的数据优化提供了新的视角和有效方法，对于提升模型性能和训练效率具有重要意义。"}}
{"id": "2506.21018", "title": "LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection", "authors": ["Lei Hao", "Lina Xu", "Chang Liu", "Yanni Dong"], "summary": "Effective deep feature extraction via feature-level fusion is crucial for\nmultimodal object detection. However, previous studies often involve complex\ntraining processes that integrate modality-specific features by stacking\nmultiple feature-level fusion units, leading to significant computational\noverhead. To address this issue, we propose a new fusion detection baseline\nthat uses a single feature-level fusion unit to enable high-performance\ndetection, thereby simplifying the training process. Based on this approach, we\npropose a lightweight attention-guided self-modulation feature fusion network\n(LASFNet), which introduces a novel attention-guided self-modulation feature\nfusion (ASFF) module that adaptively adjusts the responses of fusion features\nat both global and local levels based on attention information from different\nmodalities, thereby promoting comprehensive and enriched feature generation.\nAdditionally, a lightweight feature attention transformation module (FATM) is\ndesigned at the neck of LASFNet to enhance the focus on fused features and\nminimize information loss. Extensive experiments on three representative\ndatasets demonstrate that, compared to state-of-the-art methods, our approach\nachieves a favorable efficiency-accuracy trade-off, reducing the number of\nparameters and computational cost by as much as 90% and 85%, respectively,\nwhile improving detection accuracy (mAP) by 1%-3%. The code will be\nopen-sourced at https://github.com/leileilei2000/LASFNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21018v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21018v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LASFNet：一种用于多模态目标检测的轻量级注意力引导自调制特征融合网络", "tldr": "LASFNet是一种轻量级多模态目标检测网络，通过单一特征级融合单元和注意力机制显著降低计算量和参数，同时提高检测精度。", "motivation": "现有多模态目标检测方法在特征级融合时，往往涉及复杂的训练过程，并堆叠多个特征级融合单元，导致显著的计算开销。", "method": "本文提出一种新的融合检测基线，仅使用单个特征级融合单元以简化训练过程并实现高性能检测。在此基础上，提出了轻量级注意力引导自调制特征融合网络（LASFNet）。LASFNet引入了新颖的注意力引导自调制特征融合（ASFF）模块，该模块根据来自不同模态的注意力信息，自适应地调整全局和局部层面的融合特征响应，从而促进全面且丰富的特征生成。此外，在LASFNet的颈部设计了轻量级特征注意力转换（FATM）模块，以增强对融合特征的关注并最小化信息损失。", "result": "在三个代表性数据集上进行的大量实验表明，与最先进的方法相比，LASFNet实现了良好的效率-精度权衡，参数量和计算成本分别降低了90%和85%，同时检测精度（mAP）提高了1%-3%。", "conclusion": "LASFNet在多模态目标检测中实现了计算效率和检测精度的显著提升，提供了一种高性能且轻量级的解决方案，有效解决了现有方法的复杂性和计算开销问题。", "translation": "通过特征级融合进行有效的深度特征提取对于多模态目标检测至关重要。然而，以往的研究通常涉及复杂的训练过程，通过堆叠多个特征级融合单元来整合模态特定特征，导致显著的计算开销。为了解决这个问题，我们提出了一种新的融合检测基线，该基线使用单一特征级融合单元来实现高性能检测，从而简化了训练过程。在此基础上，我们提出了一种轻量级注意力引导自调制特征融合网络（LASFNet），它引入了一种新颖的注意力引导自调制特征融合（ASFF）模块，该模块根据来自不同模态的注意力信息，自适应地调整全局和局部层面的融合特征响应，从而促进全面且丰富的特征生成。此外，在LASFNet的颈部设计了一个轻量级特征注意力转换模块（FATM），以增强对融合特征的关注并最小化信息损失。在三个代表性数据集上进行的大量实验表明，与最先进的方法相比，我们的方法实现了良好的效率-精度权衡，参数量和计算成本分别降低了90%和85%，同时检测精度（mAP）提高了1%-3%。代码将在https://github.com/leileilei2000/LASFNet开源。", "summary": "LASFNet是一种轻量级注意力引导自调制特征融合网络，旨在解决多模态目标检测中复杂的训练过程和高计算开销问题。该网络采用单一特征级融合单元，并引入注意力引导自调制特征融合（ASFF）模块和轻量级特征注意力转换（FATM）模块，以促进全面且丰富的特征生成，同时减少信息损失。实验证明，LASFNet在大幅降低参数量和计算成本的同时，有效提升了检测精度，实现了效率与精度的良好平衡。", "keywords": "多模态目标检测, 特征融合, 轻量级, 注意力机制, 自调制", "comments": "该研究通过引入单一特征级融合单元和创新的注意力引导自调制模块，有效解决了传统多模态目标检测中训练复杂和计算开销大的问题。其在大幅降低模型参数和计算成本的同时，仍能提升检测精度，展现了出色的效率-精度权衡，对于实际应用具有重要意义。"}}
{"id": "2506.21022", "title": "Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation", "authors": ["Ze Wang", "Hao Chen", "Benran Hu", "Jiang Liu", "Ximeng Sun", "Jialian Wu", "Yusheng Su", "Xiaodong Yu", "Emad Barsoum", "Zicheng Liu"], "summary": "Image tokenization plays a critical role in reducing the computational\ndemands of modeling high-resolution images, significantly improving the\nefficiency of image and multimodal understanding and generation. Recent\nadvances in 1D latent spaces have reduced the number of tokens required by\neliminating the need for a 2D grid structure. In this paper, we further advance\ncompact discrete image representation by introducing 1D binary image latents.\nBy representing each image as a sequence of binary vectors, rather than using\ntraditional one-hot codebook tokens, our approach preserves high-resolution\ndetails while maintaining the compactness of 1D latents. To the best of our\nknowledge, our text-to-image models are the first to achieve competitive\nperformance in both diffusion and auto-regressive generation using just 128\ndiscrete tokens for images up to 1024x1024, demonstrating up to a 32-fold\nreduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary\nlatent space, coupled with simple model architectures, achieves marked\nimprovements in speed training and inference speed. Our text-to-image models\nallow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X\nGPUs, and the training can be completed within 200 GPU days. Our models achieve\ncompetitive performance compared to modern image generation models without any\nin-house private training data or post-training refinements, offering a\nscalable and efficient alternative to conventional tokenization methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21022v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21022v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Instella-T2I：推动一维离散潜在空间图像生成的极限", "tldr": "Instella-T2I引入了一维二进制图像潜在空间，显著减少了高分辨率图像的标记数量（最多32倍），实现了具有竞争力的文本到图像生成，并加快了训练和推理速度。", "motivation": "为了降低高分辨率图像建模的计算需求，提高图像和多模态理解与生成的效率，特别是通过进一步压缩一维离散图像表示来解决现有方法中标记数量过多的问题。", "method": "本研究引入了一维二进制图像潜在空间，将每张图像表示为一系列二进制向量，而非传统的独热码本标记。这种方法旨在在保持一维潜在空间紧凑性的同时，保留高分辨率图像的细节。该技术应用于文本到图像模型。", "result": "模型在使用仅128个离散标记生成高达1024x1024的图像时，在扩散和自回归生成方面均取得了具有竞争力的性能，与标准VQ-VAE相比，标记数量减少了多达32倍。同时，训练和推理速度显著提高，例如在单个GPU节点（配备8个AMD MI300X GPU）上支持4096的全局批处理大小，并在200个GPU日内完成训练。在无内部私有训练数据或后期训练改进的情况下，性能与现代图像生成模型相当。", "conclusion": "所提出的一维二进制潜在空间与简单模型架构相结合，为图像生成提供了一种可扩展且高效的替代传统标记化方法。", "translation": "图像标记化在降低高分辨率图像建模的计算需求方面起着关键作用，显著提高了图像和多模态理解与生成的效率。一维潜在空间的最新进展通过消除对二维网格结构的需求，减少了所需的标记数量。在本文中，我们通过引入一维二进制图像潜在空间，进一步推进了紧凑的离散图像表示。通过将每张图像表示为二进制向量序列，而不是使用传统的独热码本标记，我们的方法在保持一维潜在空间紧凑性的同时，保留了高分辨率细节。据我们所知，我们的文本到图像模型首次在使用仅128个离散标记生成高达1024x1024的图像时，在扩散和自回归生成方面均取得了具有竞争力的性能，与标准VQ-VAE相比，标记数量减少了多达32倍。所提出的一维二进制潜在空间，结合简单的模型架构，显著提高了训练速度和推理速度。我们的文本到图像模型允许在单个GPU节点（配备8个AMD MI300X GPU）上实现4096的全局批处理大小，并且训练可以在200个GPU日内完成。我们的模型与现代图像生成模型相比，在没有任何内部私有训练数据或后期训练改进的情况下取得了具有竞争力的性能，为传统的标记化方法提供了一种可扩展且高效的替代方案。", "summary": "Instella-T2I提出了一种创新的图像标记化方法，通过引入一维二进制图像潜在空间来显著减少高分辨率图像的离散标记数量。该方法将图像表示为二进制向量序列，而非传统的独热码本标记，从而在保持紧凑性的同时保留图像细节。实验证明，其文本到图像模型在使用极少（128个）标记的情况下，在扩散和自回归生成方面均达到竞争性性能，标记数量最多减少32倍，并显著提升了训练和推理速度，为图像生成提供了一种高效且可扩展的替代方案。", "keywords": "一维二进制潜在空间, 图像标记化, 文本到图像生成, 离散潜在空间, 计算效率", "comments": "这篇论文在图像标记化方面提出了显著创新，从传统的独热码本标记转向了一维二进制潜在空间。其声称的高分辨率图像标记数量减少32倍，加上具有竞争力的性能以及更快的训练和推理速度，凸显了其在实现更高效、可扩展图像生成模型方面的巨大潜力，尤其是在模型规模日益增大的背景下。此外，在没有私有数据或后期训练改进的情况下仍能达到此性能，进一步强调了其鲁棒性和实用性。"}}
{"id": "2506.21095", "title": "FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation", "authors": ["Xenia Heilmann", "Luca Corbucci", "Mattia Cerrato", "Anna Monreale"], "summary": "Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing clients' private data. However, fairness remains a key\nconcern, as biases in local clients' datasets can impact the entire federated\nsystem. Heterogeneous data distributions across clients may lead to models that\nare fairer for some clients than others. Although several fairness-enhancing\nsolutions are present in the literature, most focus on mitigating bias for a\nsingle sensitive attribute, typically binary, overlooking the diverse and\nsometimes conflicting fairness needs of different clients. This limited\nperspective can limit the effectiveness of fairness interventions for the\ndifferent clients. To support more robust and reproducible fairness research in\nFL, we aim to enable a consistent benchmarking of fairness-aware FL methods at\nboth the global and client levels. In this paper, we contribute in three ways:\n(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to\nevaluating fair FL methods under heterogeneous client bias; (2) we release four\nbias-heterogeneous datasets and corresponding benchmarks to compare fairness\nmitigation methods in a controlled environment; (3) we provide ready-to-use\nfunctions for evaluating fairness outcomes for these datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21095v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21095v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FeDa4Fair：客户端级联邦数据集用于公平性评估", "tldr": "该论文引入了 FeDa4Fair，一个用于生成表格数据集的库，以及四个基准数据集，旨在评估联邦学习中的公平性，通过解决异构客户端偏差来支持更健壮和可复现的公平性研究。", "motivation": "联邦学习中，本地数据集的偏差和异构数据分布导致公平性问题，模型对不同客户端的公平性不同。现有公平性解决方案通常只关注单一敏感属性，忽略了客户端多样化且冲突的公平性需求，限制了其有效性。因此，需要支持在全局和客户端级别对公平感知型联邦学习方法进行一致的基准测试，以促进更健壮和可复现的公平性研究。", "method": "本文引入了 FeDa4Fair 库，用于生成专门评估异构客户端偏差下公平联邦学习方法的表格数据集。同时，发布了四个具有偏差异构性的数据集及其相应的基准，以便在受控环境中比较公平性缓解方法。此外，还提供了即用型函数，用于评估这些数据集的公平性结果。", "result": "该论文贡献了 FeDa4Fair 库，用于生成适应公平联邦学习评估的数据集；发布了四个具有偏差异构性的数据集和相应的基准；并提供了用于公平性评估的即用型函数。", "conclusion": "通过引入 FeDa4Fair 库、数据集和评估函数，该研究旨在通过在全局和客户端级别实现对公平感知型联邦学习方法的一致基准测试，从而支持联邦学习中更健壮和可复现的公平性研究。", "translation": "联邦学习 (FL) 允许在不共享客户端私有数据的情况下，在多个客户端之间进行协作模型训练。然而，公平性仍然是一个关键问题，因为本地客户端数据集中存在的偏差会影响整个联邦系统。客户端之间异构的数据分布可能导致模型对某些客户端比其他客户端更公平。尽管文献中存在几种增强公平性的解决方案，但大多数都侧重于减轻单一敏感属性（通常是二元）的偏差，而忽略了不同客户端多样化且有时相互冲突的公平性需求。这种有限的视角可能会限制公平性干预对不同客户端的有效性。为了支持联邦学习中更健壮和可复现的公平性研究，我们的目标是在全局和客户端级别上实现对公平感知型联邦学习方法的一致基准测试。在本文中，我们通过三种方式做出贡献：(1) 我们引入了 FeDa4Fair，一个用于生成表格数据集的库，专为评估异构客户端偏差下的公平联邦学习方法而定制；(2) 我们发布了四个具有偏差异构性的数据集以及相应的基准，用于在受控环境中比较公平性缓解方法；(3) 我们提供了即用型函数，用于评估这些数据集的公平性结果。", "summary": "该论文介绍了 FeDa4Fair，一个旨在改进联邦学习 (FL) 中公平性评估的新库和一套基准数据集。它解决了当前 FL 公平性研究中经常忽视多样化客户端级偏差的局限性。FeDa4Fair 提供了生成具有异构客户端偏差的表格数据集的工具，四个预构建的偏差异构数据集及其基准，以及评估函数，从而能够在全局和客户端级别对公平感知型 FL 方法进行更健壮和可复现的评估。", "keywords": "联邦学习, 公平性, 异构数据, 偏差, 基准测试", "comments": "该论文通过解决公平性评估中的关键空白，为联邦学习社区做出了宝贵贡献。FeDa4Fair 的引入以及配套的数据集和基准直接解决了异构客户端偏差的挑战，而现有方法通常只关注单一敏感属性。这项工作具有创新性，因为它提供了一个标准化和可复现的框架，用于对公平感知型联邦学习方法进行基准测试，从而超越了理论讨论，提供了实用工具。其重要性在于通过使研究人员能够在不同客户端条件下彻底测试公平性干预措施，从而促进更有效和公平的联邦学习模型开发。"}}
{"id": "2506.21034", "title": "DidSee: Diffusion-Based Depth Completion for Material-Agnostic Robotic Perception and Manipulation", "authors": ["Wenzhou Lyu", "Jialing Lin", "Wenqi Ren", "Ruihao Xia", "Feng Qian", "Yang Tang"], "summary": "Commercial RGB-D cameras often produce noisy, incomplete depth maps for\nnon-Lambertian objects. Traditional depth completion methods struggle to\ngeneralize due to the limited diversity and scale of training data. Recent\nadvances exploit visual priors from pre-trained text-to-image diffusion models\nto enhance generalization in dense prediction tasks. However, we find that\nbiases arising from training-inference mismatches in the vanilla diffusion\nframework significantly impair depth completion performance. Additionally, the\nlack of distinct visual features in non-Lambertian regions further hinders\nprecise prediction. To address these issues, we propose \\textbf{DidSee}, a\ndiffusion-based framework for depth completion on non-Lambertian objects.\nFirst, we integrate a rescaled noise scheduler enforcing a zero terminal\nsignal-to-noise ratio to eliminate signal leakage bias. Second, we devise a\nnoise-agnostic single-step training formulation to alleviate error accumulation\ncaused by exposure bias and optimize the model with a task-specific loss.\nFinally, we incorporate a semantic enhancer that enables joint depth completion\nand semantic segmentation, distinguishing objects from backgrounds and yielding\nprecise, fine-grained depth maps. DidSee achieves state-of-the-art performance\non multiple benchmarks, demonstrates robust real-world generalization, and\neffectively improves downstream tasks such as category-level pose estimation\nand robotic grasping.Project page: https://wenzhoulyu.github.io/DidSee/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21034v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21034v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DidSee：基于扩散的深度补全，用于材质无关的机器人感知和操作", "tldr": "DidSee是一种基于扩散模型的深度补全框架，专门用于处理非朗伯物体，通过改进噪声调度、训练方式和引入语义增强器，实现了SOTA性能，并提升了机器人下游任务。", "motivation": "商业RGB-D相机对非朗伯物体产生的深度图存在噪声和不完整性。传统深度补全方法因训练数据多样性和规模有限而泛化能力不足。虽然扩散模型利用视觉先验，但香草扩散框架中训练-推理不匹配导致的偏差以及非朗伯区域缺乏明显视觉特征，严重影响了深度补全性能。", "method": "提出DidSee框架。首先，整合一个重新缩放的噪声调度器，强制零终端信噪比以消除信号泄漏偏差。其次，设计一种噪声无关的单步训练公式，以减轻暴露偏差引起的误差累积，并使用任务特定损失优化模型。最后，结合一个语义增强器，实现联合深度补全和语义分割，区分物体和背景，生成精确、细粒度的深度图。", "result": "DidSee在多个基准测试上取得了最先进的性能，展示了强大的真实世界泛化能力，并有效改善了下游任务，如类别级姿态估计和机器人抓取。", "conclusion": "DidSee通过创新的扩散模型改进和语义增强，有效解决了非朗伯物体深度补全的挑战，显著提升了机器人感知和操作的性能。", "translation": "商业RGB-D相机经常为非朗伯物体生成有噪声、不完整的深度图。传统的深度补全方法由于训练数据的多样性和规模有限，难以泛化。最近的进展利用预训练的文本到图像扩散模型的视觉先验来增强密集预测任务的泛化能力。然而，我们发现香草扩散框架中训练-推理不匹配导致的偏差显著损害了深度补全性能。此外，非朗伯区域缺乏明显的视觉特征进一步阻碍了精确预测。为了解决这些问题，我们提出了\\textbf{DidSee}，一个基于扩散的非朗伯物体深度补全框架。首先，我们集成了一个重新缩放的噪声调度器，强制零终端信噪比以消除信号泄漏偏差。其次，我们设计了一种噪声无关的单步训练公式，以减轻暴露偏差引起的误差累积，并使用任务特定损失优化模型。最后，我们结合了一个语义增强器，实现联合深度补全和语义分割，区分物体和背景，并生成精确、细粒度的深度图。DidSee在多个基准测试上取得了最先进的性能，展示了强大的真实世界泛化能力，并有效改善了下游任务，如类别级姿态估计和机器人抓取。项目页面：https://wenzhoulyu.github.io/DidSee/", "summary": "DidSee是一个创新的基于扩散的深度补全框架，专门针对商业RGB-D相机在非朗伯物体上生成噪声和不完整深度图的问题。它通过引入重新缩放的噪声调度器解决信号泄漏偏差，设计噪声无关的单步训练公式缓解误差累积，并结合语义增强器实现联合深度补全和语义分割。DidSee在多个基准测试上实现了最先进的性能，并显著提升了机器人感知和操作的下游任务。", "keywords": "深度补全, 扩散模型, 非朗伯物体, 机器人感知, 语义分割", "comments": "DidSee的创新点在于其针对扩散模型在深度补全任务中的特定挑战（如训练-推理不匹配偏差）进行了改进，并通过引入语义增强器提升了对非朗伯物体的处理能力。其在机器人感知和操作下游任务中的有效性也证明了其重要的实际应用价值。"}}
{"id": "2506.21102", "title": "Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning", "authors": ["David Debot", "Pietro Barbiero", "Gabriele Dominici", "Giuseppe Marra"], "summary": "Concept-Based Models (CBMs) are a class of deep learning models that provide\ninterpretability by explaining predictions through high-level concepts. These\nmodels first predict concepts and then use them to perform a downstream task.\nHowever, current CBMs offer interpretability only for the final task\nprediction, while the concept predictions themselves are typically made via\nblack-box neural networks. To address this limitation, we propose Hierarchical\nConcept Memory Reasoner (H-CMR), a new CBM that provides interpretability for\nboth concept and task predictions. H-CMR models relationships between concepts\nusing a learned directed acyclic graph, where edges represent logic rules that\ndefine concepts in terms of other concepts. During inference, H-CMR employs a\nneural attention mechanism to select a subset of these rules, which are then\napplied hierarchically to predict all concepts and the final task. Experimental\nresults demonstrate that H-CMR matches state-of-the-art performance while\nenabling strong human interaction through concept and model interventions. The\nformer can significantly improve accuracy at inference time, while the latter\ncan enhance data efficiency during training when background knowledge is\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21102v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21102v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "注意力引导图学习实现可解释的分层概念推理", "tldr": "本文提出了H-CMR模型，通过注意力引导的图学习，为概念和任务预测提供双重可解释性，并达到先进性能，支持人机交互。", "motivation": "现有基于概念的模型（CBMs）仅为最终任务预测提供可解释性，而概念预测本身通常通过黑箱神经网络进行，限制了整体可解释性。", "method": "提出分层概念记忆推理器（H-CMR），通过学习到的有向无环图建模概念间的逻辑关系。在推理时，H-CMR利用神经注意力机制选择规则子集，并分层应用于预测所有概念和最终任务。", "result": "H-CMR的性能与最先进模型相当，并通过概念和模型干预实现了强大的人机交互。概念干预显著提高了推理时的准确性，模型干预在有背景知识时提高了训练期间的数据效率。", "conclusion": "H-CMR成功解决了CBMs在概念预测方面缺乏可解释性的问题，通过分层概念推理和注意力机制，在保持高性能的同时，增强了模型的可解释性和人机交互能力。", "translation": "基于概念的模型（CBMs）是一类深度学习模型，通过高级概念解释预测来提供可解释性。这些模型首先预测概念，然后利用它们执行下游任务。然而，当前的CBMs仅为最终任务预测提供可解释性，而概念预测本身通常是通过黑箱神经网络进行的。为了解决这一限制，我们提出了分层概念记忆推理器（H-CMR），这是一种新的CBM，为概念和任务预测都提供了可解释性。H-CMR使用学习到的有向无环图来建模概念之间的关系，其中边表示定义概念的逻辑规则。在推理过程中，H-CMR采用神经注意力机制来选择这些规则的一个子集，然后将其分层应用于预测所有概念和最终任务。实验结果表明，H-CMR的性能与最先进水平相当，同时通过概念和模型干预实现了强大的人机交互。前者可以在推理时显著提高准确性，而后者在有背景知识时可以提高训练期间的数据效率。", "summary": "本文提出了H-CMR，一种新型基于概念的模型（CBM），旨在解决现有CBMs在概念预测方面缺乏可解释性的问题。H-CMR通过构建有向无环图来表示概念间的逻辑关系，并利用注意力机制分层推理概念和任务。实验证明，H-CMR不仅性能与最先进模型持平，还能通过人机交互显著提升推理准确性和训练数据效率，为概念和任务预测提供了全面的可解释性。", "keywords": "概念模型, 可解释性, 图学习, 注意力机制, 分层推理", "comments": "H-CMR的创新点在于解决了CBMs在概念预测层面的黑箱问题，通过引入图结构和注意力机制实现了端到端的概念和任务可解释性。其允许人类干预的能力，提升了模型的实用性和可信度。"}}
{"id": "2506.21042", "title": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "summary": "Detectors often suffer from performance drop due to domain gap between\ntraining and testing data. Recent methods explore diffusion models applied to\ndomain generalization (DG) and adaptation (DA) tasks, but still struggle with\nlarge inference costs and have not yet fully leveraged the capabilities of\ndiffusion models. We propose to tackle these problems by extracting\nintermediate features from a single-step diffusion process, improving feature\ncollection and fusion to reduce inference time by 75% while enhancing\nperformance on source domains (i.e., Fitness). Then, we construct an\nobject-centered auxiliary branch by applying box-masked images with class\nprompts to extract robust and domain-invariant features that focus on object.\nWe also apply consistency loss to align the auxiliary and ordinary branch,\nbalancing fitness and generalization while preventing overfitting and improving\nperformance on target domains (i.e., Generalization). Furthermore, within a\nunified framework, standard detectors are guided by diffusion detectors through\nfeature-level and object-level alignment on source domains (for DG) and\nunlabeled target domains (for DA), thereby improving cross-domain detection\nperformance (i.e., Transferability). Our method achieves competitive results on\n3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO\ngeneralization benchmark demonstrate that our method maintains significant\nadvantages and show remarkable efficiency in large domain shifts and low-data\nscenarios. Our work shows the superiority of applying diffusion models to\ndomain generalized and adaptive detection tasks and offers valuable insights\nfor visual perception tasks across diverse domains. The code is available at\n\\href{https://github.com/heboyong/Fitness-Generalization-Transferability}{Fitness-Generalization-Transferability}.", "comment": "Accepted by ICCV2025. arXiv admin note: text overlap with\n  arXiv:2503.02101", "pdf_url": "http://arxiv.org/pdf/2506.21042v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21042v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用扩散模型提升域泛化和自适应检测：适应性、泛化性和可迁移性", "tldr": "该论文提出了一种新的扩散模型方法，通过利用单步扩散特征、构建目标中心辅助分支和引入一致性损失，显著降低了推理成本，并提升了域泛化和域自适应检测的性能，同时在适应性、泛化性和可迁移性方面表现出色。", "motivation": "目标检测器通常因训练和测试数据之间的域间隙而导致性能下降。尽管最近的方法尝试将扩散模型应用于域泛化（DG）和域自适应（DA）任务，但它们仍面临高昂的推理成本，且未能充分利用扩散模型的全部能力。", "method": "1. 从单步扩散过程中提取中间特征，改进特征收集和融合，以减少75%的推理时间并提升源域性能（适应性）。2. 通过对带有类别提示的框掩码图像应用，构建一个目标中心辅助分支，以提取鲁棒且域不变的关注于目标的特征。3. 应用一致性损失来对齐辅助分支和普通分支，平衡适应性和泛化性，防止过拟合，并提升目标域性能（泛化性）。4. 在一个统一框架内，通过在源域（DG）和未标记目标域（DA）上进行特征级和对象级对齐，扩散检测器指导标准检测器，从而提升跨域检测性能（可迁移性）。", "result": "1. 在3个DA基准和5个DG基准上取得了有竞争力的结果。2. 在COCO泛化基准测试中，该方法在大域偏移和低数据场景下保持了显著优势并显示出卓越的效率。", "conclusion": "本工作展示了将扩散模型应用于域泛化和自适应检测任务的优越性，并为跨不同领域的视觉感知任务提供了宝贵的见解。", "translation": "检测器通常因训练和测试数据之间的域间隙而导致性能下降。最近的方法探索将扩散模型应用于域泛化（DG）和域自适应（DA）任务，但仍面临高昂的推理成本，并且尚未充分利用扩散模型的能力。我们提出通过从单步扩散过程中提取中间特征来解决这些问题，改进特征收集和融合，从而将推理时间减少75%，同时提高源域性能（即适应性）。然后，我们通过应用带有类别提示的框掩码图像来构建一个目标中心辅助分支，以提取鲁棒且域不变的关注于目标的特征。我们还应用一致性损失来对齐辅助分支和普通分支，平衡适应性和泛化性，同时防止过拟合并提高目标域性能（即泛化性）。此外，在一个统一的框架内，标准检测器通过在源域（用于DG）和未标记目标域（用于DA）上进行特征级和对象级对齐，由扩散检测器引导，从而提高跨域检测性能（即可迁移性）。我们的方法在3个DA基准和5个DG基准上取得了有竞争力的结果。此外，在COCO泛化基准上的实验表明，我们的方法在大域偏移和低数据场景下保持了显著优势并显示出卓越的效率。我们的工作展示了将扩散模型应用于域泛化和自适应检测任务的优越性，并为跨不同领域的视觉感知任务提供了宝贵的见解。代码可在\\href{https://github.com/heboyong/Fitness-Generalization-Transferability}{Fitness-Generalization-Transferability}获取。", "summary": "本文提出了一种利用扩散模型改进域泛化（DG）和域自适应（DA）目标检测的新方法。为解决现有扩散模型在DG/DA任务中推理成本高和能力未充分利用的问题，作者提出从单步扩散中提取中间特征，显著降低了推理时间并提升源域性能。此外，通过构建一个基于框掩码图像和类别提示的目标中心辅助分支，并结合一致性损失，该方法能够提取鲁棒的域不变特征，平衡模型的适应性与泛化性。在一个统一框架下，通过特征级和对象级对齐，该方法能有效指导标准检测器，从而提升跨域检测的可迁移性。实验结果表明，该方法在多个DG和DA基准上取得了竞争性表现，并在大域偏移和低数据场景下展现出卓越的效率和优势。", "keywords": "扩散模型, 域泛化, 域自适应, 目标检测, 跨域性能", "comments": "该论文的创新点在于其巧妙地利用了扩散模型的中间特征，并通过单步扩散过程显著降低了推理成本，解决了现有扩散模型在域适应和泛化任务中面临的效率问题。此外，引入目标中心辅助分支和一致性损失，旨在同时优化模型的“适应性”（源域性能）、“泛化性”（目标域性能）和“可迁移性”（跨域性能），提供了一个统一且全面的解决方案。这种多维度优化策略，结合对扩散模型潜力的深度挖掘，对于提升实际应用中的检测器鲁棒性具有重要意义。"}}
{"id": "2506.21103", "title": "Learning to Skip the Middle Layers of Transformers", "authors": ["Tim Lawson", "Laurence Aitchison"], "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle.", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21103v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21103v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "学习跳过Transformer的中间层", "tldr": "本文提出了一种动态跳过Transformer中间层的新架构，以提高效率，但实验结果显示与基线相比，在所研究的规模下未能改善计算效率与性能的权衡。", "motivation": "为了提高Transformer的效率，现有的条件计算方法通常针对独立模块或独立跳过层。然而，可解释性研究表明Transformer的中间层表现出更大的冗余性。本文旨在利用这一洞察，通过跳过中间层来减少计算需求，并可能促进多级表示层次的出现。", "method": "本文提出了一种新颖的架构，通过学习的门控机制，根据输入动态地跳过从中间向外对称的中心块。同时，采用门控注意力机制防止后续token关注被跳过的token位置。残差范数通过“sandwich”或“perilayernorm”方案控制，并通过自适应正则化损失控制门控稀疏性。", "result": "在所研究的规模下，本文提出的方法与层数较少的密集基线相比，未能改善验证交叉熵与估计FLOPs之间的权衡，即未能达到计算效率的提升。", "conclusion": "尽管提出了一种新颖的动态跳过Transformer中间层的架构，但在实验验证的规模下，该方法未能有效提升Transformer的计算效率。", "translation": "条件计算是提高Transformer效率的常用策略。现有方法通常针对单个模块（例如，专家混合层）或独立跳过层。然而，可解释性研究表明Transformer的中间层表现出更大的冗余性，并且早期层将信息聚合到token位置。受这些见解的启发，我们提出了一种新颖的架构，该架构动态地跳过从中间向外可变数量的层。特别是，一个学习到的门控机制根据输入决定是否绕过对称的中心块范围，并且一个门控注意力机制阻止后续token关注被跳过的token位置。残差范数通过“sandwich”或“perilayernorm”方案控制，并通过自适应正则化损失控制门控稀疏性。我们的目标是减少“更简单”token的计算需求，并可能促进一个新兴的多级表示层次，但在所研究的规模下，与层数较少的密集基线相比，我们的方法在验证交叉熵和估计FLOPs之间的权衡方面未能实现改进。我们已在https://github.com/tim-lawson/skip-middle 发布了代码。", "summary": "本文提出了一种创新的Transformer架构，旨在通过动态跳过其冗余的中间层来提高计算效率。该方法引入了一个学习到的门控机制来选择性地跳过中心块，并结合门控注意力机制来处理跳过的token位置。尽管方法设计旨在减少计算量并可能形成多级表示，但实验结果表明，在所测试的规模下，与传统的密集Transformer相比，该方法未能显著改善计算效率与模型性能之间的权衡。", "keywords": "Transformer, 条件计算, 层跳过, 模型效率, 门控机制", "comments": "本文的创新点在于提出了一个基于中间层冗余性的动态跳过策略，并引入了门控机制和门控注意力来支持这一策略。然而，其主要局限性在于未能达到预期的计算效率提升，这可能暗示了在实际应用中，这种层跳过策略的复杂性或其对模型性能的影响需要更深入的研究和优化，尤其是在不同的模型规模下。"}}
{"id": "2506.21045", "title": "Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling", "authors": ["Hansam Cho", "Seoung Bum Kim"], "summary": "Text-guided diffusion models have become essential for high-quality image\nsynthesis, enabling dynamic image editing. In image editing, two crucial\naspects are editability, which determines the extent of modification, and\nfaithfulness, which reflects how well unaltered elements are preserved.\nHowever, achieving optimal results is challenging because of the inherent\ntrade-off between editability and faithfulness. To address this, we propose\nFaithfulness Guidance and Scheduling (FGS), which enhances faithfulness with\nminimal impact on editability. FGS incorporates faithfulness guidance to\nstrengthen the preservation of input image information and introduces a\nscheduling strategy to resolve misalignment between editability and\nfaithfulness. Experimental results demonstrate that FGS achieves superior\nfaithfulness while maintaining editability. Moreover, its compatibility with\nvarious editing methods enables precise, high-quality image edits across\ndiverse tasks.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2506.21045v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21045v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过引导和调度提高基于扩散的图像编辑保真度", "tldr": "本文提出FGS方法，通过引导和调度策略，在保持可编辑性的同时显著提高扩散模型图像编辑的保真度。", "motivation": "在文本引导的扩散模型图像编辑中，可编辑性（修改程度）和保真度（未改变元素保留程度）之间存在固有的权衡，导致难以获得最佳结果。", "method": "本文提出保真度引导与调度（Faithfulness Guidance and Scheduling, FGS）方法。FGS结合了保真度引导来加强输入图像信息的保留，并引入调度策略来解决可编辑性和保真度之间的错位问题。", "result": "实验结果表明，FGS在保持可编辑性的同时，实现了卓越的保真度。此外，它与各种编辑方法的兼容性使其能够在不同任务中实现精确、高质量的图像编辑。", "conclusion": "FGS方法有效解决了扩散模型图像编辑中保真度和可编辑性之间的权衡问题，显著提升了图像编辑的质量和精确性。", "translation": "文本引导的扩散模型已成为高质量图像合成的关键，能够实现动态图像编辑。在图像编辑中，两个关键方面是可编辑性（决定修改程度）和保真度（反映未改变元素保留程度）。然而，由于可编辑性和保真度之间固有的权衡，实现最佳结果具有挑战性。为了解决这个问题，我们提出了保真度引导与调度（FGS），它在对可编辑性影响最小的情况下增强了保真度。FGS结合了保真度引导以加强输入图像信息的保留，并引入了调度策略以解决可编辑性和保真度之间的错位。实验结果表明，FGS在保持可编辑性的同时实现了卓越的保真度。此外，它与各种编辑方法的兼容性使其能够在不同任务中实现精确、高质量的图像编辑。", "summary": "本文提出了一种名为FGS（Faithfulness Guidance and Scheduling）的新方法，旨在解决基于扩散的图像编辑中可编辑性和保真度之间的固有权衡。FGS通过结合保真度引导来增强原始图像信息的保留，并引入调度策略来解决两者之间的不一致。实验证明，FGS在维持可编辑性的同时显著提高了图像编辑的保真度，并兼容多种编辑方法，从而实现高质量和精确的图像编辑。", "keywords": "扩散模型, 图像编辑, 保真度, 引导, 调度", "comments": "本文提出FGS方法，通过创新的引导和调度策略有效解决了扩散模型图像编辑中长期存在的保真度与可编辑性之间的权衡问题。其通用兼容性是重要亮点，预示着该方法在各种图像编辑任务中具有广泛的应用潜力。"}}
{"id": "2506.21107", "title": "Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges", "authors": ["Changxi Chi", "Jun Xia", "Yufei Huang", "Jingbo Zhou", "Siyuan Li", "Yunfan Liu", "Chang Yu", "Stan Z. Li"], "summary": "Estimating single-cell responses across various perturbations facilitates the\nidentification of key genes and enhances drug screening, significantly boosting\nexperimental efficiency. However, single-cell sequencing is a destructive\nprocess, making it impossible to capture the same cell's phenotype before and\nafter perturbation. Consequently, data collected under perturbed and\nunperturbed conditions are inherently unpaired. Existing methods either attempt\nto forcibly pair unpaired data using random sampling, or neglect the inherent\nrelationship between unperturbed and perturbed cells during the modeling. In\nthis work, we propose a framework based on Dual Diffusion Implicit Bridges\n(DDIB) to learn the mapping between different data distributions, effectively\naddressing the challenge of unpaired data. We further interpret this framework\nas a form of data augmentation. We integrate gene regulatory network (GRN)\ninformation to propagate perturbation signals in a biologically meaningful way,\nand further incorporate a masking mechanism to predict silent genes, improving\nthe quality of generated profiles. Moreover, gene expression under the same\nperturbation often varies significantly across cells, frequently exhibiting a\nbimodal distribution that reflects intrinsic heterogeneity. To capture this, we\nintroduce a more suitable evaluation metric. We propose Unlasting, dual\nconditional diffusion models that overcome the problem of unpaired single-cell\nperturbation data and strengthen the model's insight into perturbations under\nthe guidance of the GRN, with a dedicated mask model designed to improve\ngeneration quality by predicting silent genes. In addition, we introduce a\nbiologically grounded evaluation metric that better reflects the inherent\nheterogeneity in single-cell responses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21107v1", "categories": ["cs.LG", "q-bio.MN"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21107v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Unlasting：基于双条件扩散隐式桥的非配对单细胞多扰动估计", "tldr": "Unlasting提出了一种基于双条件扩散隐式桥（DDIB）的模型，用于解决单细胞多扰动数据中固有的非配对问题，并通过整合基因调控网络和掩蔽机制来提高估计质量，并引入新的评估指标。", "motivation": "估计单细胞对各种扰动的反应对于识别关键基因和增强药物筛选至关重要，能显著提高实验效率。然而，单细胞测序的破坏性使得无法捕获同一细胞扰动前后的表型，导致扰动和未扰动条件下收集的数据固有地是非配对的。现有方法要么强制配对非配对数据，要么忽略了未扰动和扰动细胞之间的内在关系。", "method": "该研究提出了一个基于双扩散隐式桥（DDIB）的框架，用于学习不同数据分布之间的映射，从而有效解决非配对数据问题。该框架被解释为一种数据增强形式。研究整合了基因调控网络（GRN）信息以生物学上有意义的方式传播扰动信号，并引入掩蔽机制来预测沉默基因，以提高生成谱的质量。此外，还引入了一个更合适的评估指标，以捕获相同扰动下细胞间表达的固有异质性（通常表现为双峰分布）。", "result": "Not mentioned in abstract", "conclusion": "Unlasting引入了双条件扩散模型，通过DDIB解决了非配对单细胞扰动数据的问题，并在基因调控网络（GRN）的指导下增强了模型对扰动的洞察力，同时设计了一个专门的掩蔽模型来预测沉默基因以提高生成质量。此外，还引入了一个基于生物学的评估指标，能更好地反映单细胞反应中固有的异质性。", "translation": "估计单细胞对各种扰动的反应有助于识别关键基因并增强药物筛选，显著提高实验效率。然而，单细胞测序是一个破坏性过程，使得不可能在扰动前后捕获同一细胞的表型。因此，在扰动和未扰动条件下收集的数据本质上是非配对的。现有方法要么试图通过随机采样强制配对非配对数据，要么在建模过程中忽略了未扰动和扰动细胞之间的内在关系。在这项工作中，我们提出了一个基于双扩散隐式桥（DDIB）的框架，以学习不同数据分布之间的映射，有效解决了非配对数据的挑战。我们进一步将此框架解释为一种数据增强形式。我们整合了基因调控网络（GRN）信息，以生物学上有意义的方式传播扰动信号，并进一步结合了掩蔽机制来预测沉默基因，从而提高了生成谱的质量。此外，在相同扰动下基因表达在不同细胞间往往差异显著，经常表现出反映内在异质性的双峰分布。为了捕捉这一点，我们引入了一个更合适的评估指标。我们提出了Unlasting，一种双条件扩散模型，它克服了非配对单细胞扰动数据的问题，并在GRN的指导下增强了模型对扰动的洞察力，设计了一个专门的掩蔽模型来通过预测沉默基因来提高生成质量。此外，我们引入了一个基于生物学的评估指标，能更好地反映单细胞反应中固有的异质性。", "summary": "Unlasting提出了一种创新的双条件扩散模型，旨在解决单细胞多扰动数据中固有的非配对问题，这对于识别关键基因和药物筛选至关重要。该模型利用双扩散隐式桥（DDIB）学习扰动前后细胞状态的映射，并将其视为数据增强。为提高预测质量，Unlasting整合了基因调控网络（GRN）信息以指导扰动信号传播，并引入掩蔽机制预测沉默基因。此外，针对单细胞响应的内在异质性，该研究还提出了一个更合适的生物学评估指标。", "keywords": "单细胞，扰动，非配对数据，扩散模型，基因调控网络", "comments": "这项工作通过引入双条件扩散隐式桥（DDIB）框架，创新性地解决了单细胞多扰动数据中固有的非配对问题，这在生物学实验中是一个普遍的挑战。其亮点在于将DDIB解释为数据增强，并结合基因调控网络（GRN）来提供生物学指导，以及通过掩蔽机制提高生成质量，这些都增强了模型的生物学合理性和性能。此外，针对单细胞异质性引入新的评估指标，也显示了其对生物学复杂性的深刻理解。该方法有望显著提高单细胞扰动分析的效率和准确性。"}}
{"id": "2506.21127", "title": "Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "summary": "The increasing automation of navigation for unmanned aerial vehicles (UAVs)\nhas exposed them to adversarial attacks that exploit vulnerabilities in\nreinforcement learning (RL) through sensor manipulation. Although existing\nrobust RL methods aim to mitigate such threats, their effectiveness has limited\ngeneralization to out-of-distribution shifts from the optimal value\ndistribution, as they are primarily designed to handle fixed perturbation. To\naddress this limitation, this paper introduces an antifragile RL framework that\nenhances adaptability to broader distributional shifts by incorporating a\nswitching mechanism based on discounted Thompson sampling (DTS). This mechanism\ndynamically selects among multiple robust policies to minimize adversarially\ninduced state-action-value distribution shifts. The proposed approach first\nderives a diverse ensemble of action robust policies by accounting for a range\nof perturbations in the policy space. These policies are then modeled as a\nmultiarmed bandit (MAB) problem, where DTS optimally selects policies in\nresponse to nonstationary Bernoulli rewards, effectively adapting to evolving\nadversarial strategies. Theoretical framework has also been provided where by\noptimizing the DTS to minimize the overall regrets due to distributional shift,\nresults in effective adaptation against unseen adversarial attacks thus\ninducing antifragility. Extensive numerical simulations validate the\neffectiveness of the proposed framework in complex navigation environments with\nmultiple dynamic three-dimensional obstacles and with stronger projected\ngradient descent (PGD) and spoofing attacks. Compared to conventional robust,\nnon-adaptive RL methods, the antifragile approach achieves superior\nperformance, demonstrating shorter navigation path lengths and a higher rate of\nconflict-free navigation trajectories compared to existing robust RL techniques", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21127v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21127v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "对抗环境下无人机冲突避免的抗脆弱强化学习鲁棒策略切换", "tldr": "本文提出了一种基于折扣汤普森采样的鲁棒策略切换机制，用于抗脆弱强化学习，以提高无人机在对抗环境中的适应性，有效应对未见的对抗性攻击。", "motivation": "现有鲁棒强化学习方法在处理固定扰动时有效，但对于超出最优价值分布的分布偏移泛化能力有限，难以应对利用传感器操纵对无人机导航造成的对抗性攻击。", "method": "本文提出一种抗脆弱强化学习框架，通过引入基于折扣汤普森采样（DTS）的切换机制来增强对更广泛分布偏移的适应性。该机制动态选择多个鲁棒策略，以最小化对抗性引起的状态-动作-价值分布偏移。首先，通过考虑策略空间中的一系列扰动，推导出一组多样化的动作鲁棒策略。然后，将这些策略建模为多臂赌博机（MAB）问题，其中DTS根据非平稳伯努利奖励优化选择策略，从而有效适应不断演变的对抗策略。理论框架也已提供，通过优化DTS以最小化由于分布偏移引起的总体遗憾，从而实现对未见对抗性攻击的有效适应，从而诱导抗脆弱性。", "result": "广泛的数值模拟验证了所提出的框架在具有多个动态三维障碍和更强的PGD及欺骗攻击的复杂导航环境中的有效性。与传统的鲁棒、非自适应强化学习方法相比，抗脆弱方法表现出卓越的性能，导航路径长度更短，无冲突导航轨迹的成功率更高。", "conclusion": "本文提出的抗脆弱强化学习框架，通过引入基于折扣汤普森采样的策略切换机制，显著提高了无人机在对抗环境下的适应性和鲁棒性，有效应对了超出分布的对抗性攻击，并取得了优于现有方法的性能。", "translation": "无人机（UAV）导航自动化程度的提高，使其更容易受到通过传感器操纵利用强化学习（RL）漏洞的对抗性攻击。尽管现有的鲁棒RL方法旨在减轻此类威胁，但它们的有效性在泛化到超出最优价值分布的分布偏移方面受到限制，因为它们主要设计用于处理固定扰动。为了解决这一限制，本文引入了一个抗脆弱RL框架，通过结合基于折扣汤普森采样（DTS）的切换机制，增强了对更广泛分布偏移的适应性。该机制动态选择多个鲁棒策略，以最小化对抗性引起的状态-动作-价值分布偏移。所提出的方法首先通过考虑策略空间中的一系列扰动，推导出一组多样化的动作鲁棒策略。然后，将这些策略建模为多臂赌博机（MAB）问题，其中DTS根据非平稳伯努利奖励优化选择策略，从而有效适应不断演变的对抗策略。理论框架也已提供，通过优化DTS以最小化由于分布偏移引起的总体遗憾，从而实现对未见对抗性攻击的有效适应，从而诱导抗脆弱性。广泛的数值模拟验证了所提出的框架在具有多个动态三维障碍和更强的投影梯度下降（PGD）和欺骗攻击的复杂导航环境中的有效性。与传统的鲁棒、非自适应RL方法相比，抗脆弱方法取得了卓越的性能，与现有鲁棒RL技术相比，导航路径长度更短，无冲突导航轨迹的成功率更高。", "summary": "本文提出了一种针对无人机在对抗环境下进行冲突避免的抗脆弱强化学习框架。该框架通过引入基于折扣汤普森采样（DTS）的策略切换机制，动态选择多个鲁棒策略，以应对由传感器操纵引起的对抗性攻击和分布偏移。研究将策略选择建模为多臂赌博机问题，并优化DTS以最小化遗憾，从而实现对未知攻击的有效适应。实验结果表明，该方法在复杂导航环境中表现优于传统的鲁棒非自适应强化学习方法，具有更短的导航路径和更高的无冲突轨迹成功率。", "keywords": "抗脆弱强化学习, 策略切换, 无人机冲突避免, 折扣汤普森采样, 对抗环境", "comments": "本文的创新点在于将抗脆弱性概念引入强化学习，并通过动态策略切换机制来应对非固定、不断演变的对抗性攻击。将策略选择问题转化为多臂赌博机并利用折扣汤普森采样进行优化，提供了一种新颖的自适应方法。该研究对于提高无人机在复杂、不确定环境中的自主性和安全性具有重要意义。"}}
{"id": "2506.21055", "title": "Class-Agnostic Region-of-Interest Matching in Document Images", "authors": ["Demin Zhang", "Jiahao Lyu", "Zhijie Shen", "Yu Zhou"], "summary": "Document understanding and analysis have received a lot of attention due to\ntheir widespread application. However, existing document analysis solutions,\nsuch as document layout analysis and key information extraction, are only\nsuitable for fixed category definitions and granularities, and cannot achieve\nflexible applications customized by users. Therefore, this paper defines a new\ntask named ``Class-Agnostic Region-of-Interest Matching'' (``RoI-Matching'' for\nshort), which aims to match the customized regions in a flexible, efficient,\nmulti-granularity, and open-set manner. The visual prompt of the reference\ndocument and target document images are fed into our model, while the output is\nthe corresponding bounding boxes in the target document images. To meet the\nabove requirements, we construct a benchmark RoI-Matching-Bench, which sets\nthree levels of difficulties following real-world conditions, and propose the\nmacro and micro metrics to evaluate. Furthermore, we also propose a new\nframework RoI-Matcher, which employs a siamese network to extract multi-level\nfeatures both in the reference and target domains, and cross-attention layers\nto integrate and align similar semantics in different domains. Experiments show\nthat our method with a simple procedure is effective on RoI-Matching-Bench, and\nserves as the baseline for further research. The code is available at\nhttps://github.com/pd162/RoI-Matching.", "comment": "Accepted by ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.21055v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21055v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "文档图像中类别无关的感兴趣区域匹配", "tldr": "本文提出了一种名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务，旨在灵活、高效、多粒度地匹配文档图像中的自定义区域，并构建了基准数据集RoI-Matching-Bench和提出了新的框架RoI-Matcher。", "motivation": "现有的文档分析解决方案（如文档布局分析和关键信息提取）仅适用于固定的类别定义和粒度，无法实现用户定制的灵活应用。", "method": "本文定义了一个名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务，旨在以灵活、高效、多粒度、开放集的方式匹配自定义区域。为此，构建了一个包含三级难度的基准数据集RoI-Matching-Bench，并提出了宏观和微观评估指标。此外，还提出了一个新的框架RoI-Matcher，该框架采用Siamese网络从参考文档和目标文档中提取多级特征，并使用交叉注意力层整合和对齐不同领域中的相似语义。", "result": "实验表明，本文提出的RoI-Matcher方法过程简单但在RoI-Matching-Bench上是有效的。", "conclusion": "本文提出的RoI-Matcher方法在RoI-Matching-Bench上表现有效，可以作为进一步研究的基线。", "translation": "文档理解和分析因其广泛应用而受到大量关注。然而，现有的文档分析解决方案，如文档布局分析和关键信息提取，仅适用于固定的类别定义和粒度，无法实现用户定制的灵活应用。因此，本文定义了一项名为“类别无关感兴趣区域匹配”（简称“RoI-Matching”）的新任务，旨在以灵活、高效、多粒度、开放集的方式匹配自定义区域。参考文档和目标文档图像的视觉提示被输入到我们的模型中，输出是目标文档图像中对应的边界框。为了满足上述要求，我们构建了一个基准RoI-Matching-Bench，它根据实际条件设置了三个难度级别，并提出了宏观和微观指标进行评估。此外，我们还提出了一个新的框架RoI-Matcher，它采用Siamese网络在参考域和目标域中提取多级特征，并使用交叉注意力层整合和对齐不同域中的相似语义。实验表明，我们方法的过程简单但在RoI-Matching-Bench上是有效的，并可作为进一步研究的基线。代码可在https://github.com/pd162/RoI-Matching获取。", "summary": "本文针对现有文档分析解决方案在灵活性和定制化方面的不足，提出了一项名为“类别无关感兴趣区域匹配”（RoI-Matching）的新任务。该任务旨在实现文档图像中自定义区域的灵活、高效、多粒度、开放集匹配。为支持此任务，研究者构建了一个新的基准数据集RoI-Matching-Bench，并提出了相应的评估指标。同时，还提出了一个基于Siamese网络和交叉注意力层的框架RoI-Matcher。实验证明，RoI-Matcher在RoI-Matching-Bench上表现有效，可作为未来研究的基线。", "keywords": "文档图像, 感兴趣区域匹配, 类别无关, RoI-Matching, Siamese网络", "comments": "本文的创新点在于提出了一个新颖且实用的“类别无关感兴趣区域匹配”任务，解决了现有文档分析方法在灵活性和用户定制化方面的局限性。通过构建专门的基准数据集和设计有效的框架，为文档图像理解领域提供了一个重要的研究方向和有力的基线，对未来开发更通用和用户友好的文档分析工具具有重要意义。"}}
{"id": "2506.21129", "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks", "authors": ["Deepak Kumar Panda", "Adolfo Perrusquia", "Weisi Guo"], "summary": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21129v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21129v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "课程引导的反脆弱强化学习用于观测空间攻击下安全无人机冲突解除", "tldr": "该论文提出了一种课程引导的反脆弱强化学习框架，旨在使强化学习策略在观测空间对抗性攻击下，特别是无人机避障场景中，保持鲁棒性和适应性，显著优于现有基线。", "motivation": "部署在无人机导航等安全关键系统中的强化学习策略容易受到观测空间中分布外(OOD)对抗性攻击的影响，这些攻击会导致价值估计显著下降，从而导致不安全或次优的决策，使得现有策略脆弱。", "method": "提出了一种反脆弱强化学习框架，通过引入一个模拟攻击者逐步增加观测空间扰动的强度，使RL智能体能够适应并泛化更广泛的OOD观测，并预测以前未见的攻击。理论上定义了脆弱性为价值函数分布随扰动强度单调发散，反脆弱性为这种价值偏移的有界性，并推导了稳定遗忘的适应条件。通过迭代专家引导的评论家对齐，使用Wasserstein距离最小化在逐步扰动的观测上强制执行这些界限。", "result": "在无人机冲突解除场景中，反脆弱策略在面对PGD和GPS欺骗攻击时，始终优于标准和鲁棒RL基线，累积奖励提高了15%，冲突事件减少了30%以上。", "conclusion": "反脆弱强化学习在具有演变威胁情景的环境中，对于安全和弹性决策具有实际和理论上的可行性。", "translation": "部署在无人机导航等安全关键系统中的强化学习(RL)策略容易受到观测空间中分布外(OOD)对抗性攻击的影响。这些攻击会导致分布偏移，显著降低价值估计，从而导致不安全或次优的决策，使得现有策略脆弱。为了解决这一漏洞，我们提出了一种反脆弱强化学习框架，旨在适应递增的对抗性扰动课程。该框架引入了一个模拟攻击者，逐步增加观测空间扰动的强度，从而使RL智能体能够适应并泛化更广泛的OOD观测，并预测以前未见的攻击。我们首先对脆弱性进行了理论表征，将灾难性遗忘正式定义为价值函数分布随扰动强度单调发散。在此基础上，我们将反脆弱性定义为这种价值偏移的有界性，并推导了遗忘得到稳定的适应条件。我们的方法通过使用Wasserstein距离最小化在逐步扰动的观测上，进行迭代专家引导的评论家对齐来强制执行这些界限。我们在涉及动态3D障碍的无人机冲突解除场景中对该方法进行了实证评估。结果表明，当受到投影梯度下降(PGD)和GPS欺骗攻击时，反脆弱策略始终优于标准和鲁棒RL基线，累积奖励提高了15%，冲突事件减少了30%以上。这些发现证明了反脆弱强化学习在具有演变威胁情景的环境中，对于安全和弹性决策的实际和理论可行性。", "summary": "本论文提出了一种课程引导的反脆弱强化学习框架，以解决强化学习策略在无人机等安全关键系统中，观测空间对抗性攻击导致的脆弱性问题。该框架通过模拟攻击者逐步增强扰动，使RL智能体学习适应并泛化未见的攻击。研究定义了脆弱性和反脆弱性，并通过Wasserstein距离最小化进行专家引导的评论家对齐来稳定价值函数。实验结果显示，在无人机冲突解除场景中，该方法在面对PGD和GPS欺骗攻击时，相较于传统和鲁棒RL基线，能显著提高累积奖励并减少冲突事件，证明了其在动态威胁环境中实现安全和弹性决策的有效性。", "keywords": "反脆弱强化学习, 对抗性攻击, 无人机冲突解除, 观测空间, 课程学习", "comments": "该论文的创新点在于提出了“反脆弱”这一概念，并将其应用于强化学习以应对观测空间攻击。通过引入课程学习式的模拟攻击者和理论化的脆弱性/反脆弱性定义，以及基于Wasserstein距离的评论家对齐方法，为RL在安全关键系统中的鲁棒性提供了一条新颖且有效的路径。其在无人机避障场景下的出色表现，凸显了该方法在实际应用中的巨大潜力。"}}
{"id": "2506.21056", "title": "SAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification", "authors": ["Dinh-Khoi Vo", "Van-Loc Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Retrieving 3D objects in complex indoor environments using only a masked 2D\nimage and a natural language description presents significant challenges. The\nROOMELSA challenge limits access to full 3D scene context, complicating\nreasoning about object appearance, geometry, and semantics. These challenges\nare intensified by distorted viewpoints, textureless masked regions, ambiguous\nlanguage prompts, and noisy segmentation masks. To address this, we propose\nSAMURAI: Shape-Aware Multimodal Retrieval for 3D Object Identification. SAMURAI\nintegrates CLIP-based semantic matching with shape-guided re-ranking derived\nfrom binary silhouettes of masked regions, alongside a robust majority voting\nstrategy. A dedicated preprocessing pipeline enhances mask quality by\nextracting the largest connected component and removing background noise. Our\nhybrid retrieval framework leverages both language and shape cues, achieving\ncompetitive performance on the ROOMELSA private test set. These results\nhighlight the importance of combining shape priors with language understanding\nfor robust open-world 3D object retrieval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21056v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21056v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SAMURAI：形状感知多模态3D物体识别检索", "tldr": "SAMURAI提出了一种结合CLIP语义匹配和形状引导重排序的混合检索框架，以解决使用部分2D图像和自然语言从复杂室内环境中检索3D物体的挑战，并在ROOMELSA数据集上取得了有竞争力的表现。", "motivation": "在复杂的室内环境中，仅使用遮罩2D图像和自然语言描述来检索3D物体面临巨大挑战，特别是ROOMELSA挑战限制了对完整3D场景上下文的访问，导致难以推断物体外观、几何和语义。此外，扭曲的视角、无纹理的遮罩区域、模糊的语言提示和嘈杂的分割遮罩进一步加剧了这些挑战。", "method": "本文提出了SAMURAI：形状感知多模态3D物体识别检索。它集成了基于CLIP的语义匹配、从遮罩区域的二值轮廓派生的形状引导重排序，以及一种鲁棒的多数投票策略。此外，一个专门的预处理管道通过提取最大连通分量和去除背景噪声来增强遮罩质量。该混合检索框架利用语言和形状线索。", "result": "SAMURAI在ROOMELSA私有测试集上取得了有竞争力的性能。", "conclusion": "研究结果强调了将形状先验与语言理解相结合对于鲁棒的开放世界3D物体检索的重要性。", "translation": "仅使用遮罩2D图像和自然语言描述在复杂室内环境中检索3D物体带来了显著的挑战。ROOMELSA挑战限制了对完整3D场景上下文的访问，使推理物体外观、几何和语义变得复杂。扭曲的视角、无纹理的遮罩区域、模糊的语言提示和嘈杂的分割遮罩进一步加剧了这些挑战。为了解决这个问题，我们提出了SAMURAI：形状感知多模态3D物体识别检索。SAMURAI将基于CLIP的语义匹配与从遮罩区域的二值轮廓派生出的形状引导重排序相结合，并辅以鲁棒的多数投票策略。一个专门的预处理管道通过提取最大连通分量和去除背景噪声来增强遮罩质量。我们的混合检索框架利用语言和形状线索，在ROOMELSA私有测试集上取得了有竞争力的性能。这些结果突出了将形状先验与语言理解相结合对于鲁棒的开放世界3D物体检索的重要性。", "summary": "本文提出SAMURAI，一个用于3D物体识别的形状感知多模态检索系统，旨在解决在复杂室内环境中仅使用遮罩2D图像和自然语言描述进行3D物体检索的难题。SAMURAI结合了CLIP语义匹配、基于二值轮廓的形状引导重排序和多数投票策略，并通过预处理管道提升遮罩质量。该混合框架有效利用语言和形状信息，在ROOMELSA私有测试集上展现出竞争力，强调了结合形状先验和语言理解在开放世界3D物体检索中的重要性。", "keywords": "3D物体检索, 多模态, 形状感知, CLIP, ROOMELSA", "comments": "SAMURAI的创新点在于其混合检索框架，巧妙地结合了基于CLIP的语义匹配和形状引导重排序，有效利用了语言和形状两种模态的信息。其预处理管道对遮罩质量的提升也增强了系统的鲁棒性。该方法在解决复杂室内3D物体检索问题上具有重要意义，特别是在有限场景上下文和噪声数据下。"}}
{"id": "2506.21137", "title": "NaLaFormer: Norm-Aware Linear Attention for Transformer Models", "authors": ["Weikang Meng", "Yadan Luo", "Liangyu Huo", "Yaowei Wang", "Xin Li", "Zheng Zhang"], "summary": "Linear attention has emerged as a viable alternative to softmax attention by\nreducing complexity from quadratic to linear in sequence length. To preserve\ntwo fundamental properties of softmax, non-negativity and entropy reduction,\ncurrent works employ various linearly separatable kernel functions with $L1$\nnormalization instead of softmax operator. However, query norms are neglected\nby the normalization operation in linear attention, such degradation heavily\nleads to an entropy gap. Meanwhile, existing works inhibit negative values of\nquery and key vectors resulting in a missing inner-product interactions after\nbeing mapped. To address these dual challenges, we propose a novel Norm-Aware\nLinear Attention mechanism serving to restore norm-guided dynamic spikiness and\nrecover kernel-perturbed norm distributions. Specifically, we first decouple\nquery and key matrices into two components: norm and direction, to achieve\nnorm-aware spikiness control and norm consistency, respectively. We\nmathematically reveal that the extent of entropy reduction varies with the\nquery norm in softmax normalization, motivating a query-norm aware kernel\nfunction for dynamic control over entropy reduction. Furthermore, to ensure\nnorm consistency and enforce non-negativity constraints, we employ a\nnorm-preserving mapping to project all elements of the angular matrix into\npositive values, leveraging cosine similarity to inhibit dimensions with\nopposite directions. We conduct extensive experiments demonstrating that the\nNaLaFormer improves performance on vision and language tasks, enhancing both\nexpressiveness and efficiency by up to 4.2\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21137v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21137v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "NaLaFormer：面向Transformer模型的范数感知线性注意力机制", "tldr": "NaLaFormer提出了一种范数感知线性注意力机制，通过解决现有线性注意力中查询范数被忽视和负值被抑制的问题，提高了Transformer模型在视觉和语言任务上的性能和效率。", "motivation": "现有线性注意力机制在处理序列长度时虽然降低了复杂度，但通过L1归一化代替softmax操作时，忽略了查询范数，导致熵间隙。同时，抑制查询和键向量的负值会导致内积交互缺失。这些缺陷是该研究的动机。", "method": "本文提出了一种新颖的范数感知线性注意力机制（NaLaFormer），旨在恢复范数引导的动态尖峰性和恢复核扰动的范数分布。具体而言，首先将查询和键矩阵解耦为范数和方向两个分量，分别实现范数感知尖峰控制和范数一致性。数学上揭示了softmax归一化中熵减少程度随查询范数变化，从而提出了一种查询范数感知的核函数以动态控制熵减少。此外，为确保范数一致性并强制非负性约束，采用范数保持映射将角度矩阵的所有元素投影为正值，并利用余弦相似度抑制方向相反的维度。", "result": "NaLaFormer在视觉和语言任务上提升了性能，同时将表达能力和效率提高了高达4.2%。", "conclusion": "NaLaFormer通过解决线性注意力中查询范数被忽视和内积交互缺失的问题，有效提升了Transformer模型在视觉和语言任务上的表现和效率，证明了其在恢复范数引导的动态尖峰性和范数一致性方面的有效性。", "translation": "线性注意力作为softmax注意力的一种可行替代方案，将序列长度的复杂度从二次降低到线性。为了保留softmax的两个基本特性：非负性和熵减少，当前工作采用各种线性可分离的核函数进行L1归一化而非softmax操作。然而，线性注意力中的归一化操作忽略了查询范数，这种退化严重导致了熵间隙。同时，现有工作抑制了查询和键向量的负值，导致映射后内积交互的缺失。为了解决这些双重挑战，我们提出了一种新颖的范数感知线性注意力机制，旨在恢复范数引导的动态尖峰性并恢复核扰动的范数分布。具体而言，我们首先将查询和键矩阵解耦为两个分量：范数和方向，分别实现范数感知尖峰控制和范数一致性。我们通过数学揭示了softmax归一化中熵减少的程度随查询范数而变化，这激发了一种查询范数感知的核函数，用于动态控制熵减少。此外，为了确保范数一致性并强制非负性约束，我们采用范数保持映射将角度矩阵的所有元素投影为正值，利用余弦相似度抑制方向相反的维度。我们进行了广泛的实验，证明NaLaFormer在视觉和语言任务上提高了性能，并将表达能力和效率提高了高达4.2%。", "summary": "本文提出了一种名为NaLaFormer的范数感知线性注意力机制，旨在解决现有线性注意力中查询范数被忽视导致的熵间隙问题，以及抑制负值引起的内积交互缺失问题。NaLaFormer通过将查询和键矩阵解耦为范数和方向，并引入查询范数感知的核函数和范数保持映射，实现了对动态尖峰性和范数一致性的控制。实验结果表明，NaLaFormer在视觉和语言任务上显著提升了Transformer模型的性能、表达能力和效率。", "keywords": "线性注意力, Transformer, 范数感知, 熵间隙, NaLaFormer", "comments": "NaLaFormer的创新点在于其对线性注意力中查询范数和负值处理的深入分析，并提出了一套完整的解决方案。通过解耦范数和方向、引入范数感知核函数和范数保持映射，该方法有效地弥补了现有线性注意力机制的不足，尤其是在保持熵减少特性和内积交互方面。其在视觉和语言任务上取得的性能提升和效率优化，表明了其在实际应用中的潜力，是Transformer模型效率和表达能力提升方向上的一个重要进展。"}}
{"id": "2506.21220", "title": "Complexity-aware fine-tuning", "authors": ["Andrey Goncharov", "Daniil Vyazhev", "Petr Sychev", "Edvard Khalafyan", "Alexey Zaytsev"], "summary": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21220v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21220v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "复杂性感知微调", "tldr": "提出了一种基于熵的复杂性感知微调方法，在性能与蒸馏相当的同时显著减少了数据量。", "motivation": "现有的大模型微调方法（SFT和蒸馏）存在局限性：SFT性能有限，而蒸馏虽然效果好但成本高昂，需要大量数据和昂贵的调用。", "method": "提出了一种高效微调的新范式，即只对通过熵识别出的复杂数据使用推理。具体方法是：将训练数据根据单令牌答案熵（ROC AUC 0.73）划分为不同复杂性类别，然后通过SFT和蒸馏对小型开源模型（约3B）进行微调。", "result": "该方法显著优于标准SFT方法（平均准确率0.55 vs 0.43），并且在使用62%更少数据的情况下，实现了与传统蒸馏相当的性能（两者平均准确率均为0.55）。", "conclusion": "基于复杂性感知的微调方法可以在显著减少数据量和成本的同时，达到或超越现有微调方法的性能，为高效大模型微调提供了新途径。", "translation": "通用大型语言模型（LLMs）通常通过监督微调（SFT）来提升在特定领域的性能。通过蒸馏大型模型的思维链可以获得更好的结果，但这需要大量昂贵的调用和更多的数据。我们提出了一种新颖的、高效微调的蓝图，该蓝图仅对通过熵识别出的复杂数据使用推理。具体来说，我们针对两个小型开源模型（约3B），通过单令牌答案熵（ROC AUC 0.73）将训练数据划分为复杂性类别，并通过SFT和蒸馏对大型语言模型（LLMs）进行微调，结果表明我们的流水线显著优于标准SFT方法（平均准确率0.55 vs 0.43），并且在使用62%更少数据的情况下，性能与蒸馏相当（两者平均准确率均为0.55）。我们发布了代码和数据，以促进该方向的进一步研究。", "summary": "本文提出了一种名为“复杂性感知微调”的新型高效LLM微调方法，旨在解决传统SFT性能有限和蒸馏成本高昂的问题。该方法通过熵值识别数据复杂性，并仅对复杂数据应用推理。实验结果表明，该方法在小型模型上显著优于标准SFT，并且在使用更少数据的情况下达到了与传统蒸馏相当的性能。", "keywords": "大型语言模型, 微调, 复杂性感知, 熵, 蒸馏", "comments": "该论文的创新点在于引入了“复杂性感知”的概念，并利用熵来智能地分配计算资源，即只对真正需要复杂推理的数据进行更密集的处理。这有效地解决了传统蒸调方法数据效率低下的问题，为高效微调大模型提供了一个有前景的方向，对于资源受限的场景尤为重要。"}}
{"id": "2506.21076", "title": "PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image", "authors": ["Hongyu Yan", "Kunming Luo", "Weiyu Li", "Yixun Liang", "Shengming Li", "Jingwei Huang", "Chunchao Guo", "Ping Tan"], "summary": "3D characters play a crucial role in our daily entertainment. To improve the\nefficiency of 3D character modeling, recent image-based methods use two\nseparate models to achieve pose standardization and 3D reconstruction of the\nA-pose character. However, these methods are prone to generating distorted and\ndegraded images in the pose standardization stage due to self-occlusion and\nviewpoints, which further affects the geometric quality of the subsequent\nreconstruction process. To tackle these problems, we propose PoseMaster, an\nend-to-end controllable 3D character generation framework. Specifically, we\nunify pose transformation and 3D character generation into a flow-based 3D\nnative generation framework. To achieve accurate arbitrary-pose control, we\npropose to leverage the 3D body bones existing in the skeleton of an animatable\ncharacter as the pose condition. Furthermore, considering the specificity of\nmulti-condition control, we randomly empty the pose condition and the image\ncondition during training to improve the effectiveness and generalizability of\npose control. Finally, we create a high-quality pose-control dataset derived\nfrom realistic character animation data to make the model learning the implicit\nrelationships between skeleton and skinning weights. Extensive experiments show\nthat PoseMaster outperforms current state-of-the-art techniques in both\nqualitative and quantitative evaluations for A-pose character generation while\ndemonstrating its powerful ability to achieve precise control for arbitrary\nposes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21076v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21076v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PoseMaster：从单张图像生成任意姿态的3D角色", "tldr": "PoseMaster是一个端到端的3D角色生成框架，能从单张图像生成任意姿态的3D角色，解决了现有方法在姿态标准化阶段易产生扭曲和降级图像的问题。", "motivation": "为了提高3D角色建模效率，现有基于图像的方法通常使用两个独立模型进行A-pose角色的姿态标准化和3D重建，但这些方法在姿态标准化阶段容易因自遮挡和视角问题生成扭曲和降级的图像，从而影响后续重建的几何质量。", "method": "我们提出了PoseMaster，一个端到端可控的3D角色生成框架。它将姿态变换和3D角色生成统一到流基3D原生生成框架中。为实现精确的任意姿态控制，我们利用可动画角色骨架中的3D骨骼作为姿态条件。此外，在训练期间随机清空姿态条件和图像条件，以提高多条件控制的有效性和泛化性。最后，我们创建了一个高质量的姿态控制数据集，源自真实角色动画数据，使模型学习骨骼和蒙皮权重之间的隐式关系。", "result": "大量的实验表明，PoseMaster在A-pose角色生成方面，无论在定性还是定量评估中都优于当前最先进的技术，并展示了其实现任意姿态精确控制的强大能力。", "conclusion": "PoseMaster成功地提供了一个端到端框架，能够从单张图像高效且高质量地生成任意姿态的3D角色，克服了现有方法的局限性。", "translation": "3D角色在我们的日常娱乐中扮演着至关重要的角色。为了提高3D角色建模的效率，最近基于图像的方法使用两个独立的模型来实现A-pose角色的姿态标准化和3D重建。然而，这些方法在姿态标准化阶段容易因自遮挡和视角问题生成扭曲和降级的图像，这进一步影响了后续重建过程的几何质量。为了解决这些问题，我们提出了PoseMaster，一个端到端可控的3D角色生成框架。具体来说，我们将姿态变换和3D角色生成统一到一个基于流的3D原生生成框架中。为了实现精确的任意姿态控制，我们建议利用可动画角色骨架中存在的3D身体骨骼作为姿态条件。此外，考虑到多条件控制的特殊性，我们在训练期间随机清空姿态条件和图像条件，以提高姿态控制的有效性和泛化能力。最后，我们创建了一个高质量的姿态控制数据集，该数据集来源于真实的字符动画数据，旨在使模型学习骨骼和蒙皮权重之间的隐式关系。大量的实验表明，PoseMaster在A-pose角色生成方面，无论在定性还是定量评估中都优于当前最先进的技术，同时展示了其实现任意姿态精确控制的强大能力。", "summary": "PoseMaster是一个创新的端到端框架，旨在从单张图像生成任意姿态的高质量3D角色。它解决了现有图像-3D方法在姿态标准化阶段因自遮挡和视角导致的图像失真问题。该框架将姿态变换和3D生成统一为一个流基3D原生模型，并利用3D骨骼作为精确的姿态控制条件。通过在训练中随机清空条件和使用高质量的姿态控制数据集，PoseMaster显著提高了生成质量和姿态控制的泛化性，在A-pose生成和任意姿态控制方面均超越了现有SOTA方法。", "keywords": "3D角色生成, 任意姿态, 单张图像, 端到端, 姿态控制", "comments": "PoseMaster的创新之处在于其端到端的框架，统一了姿态变换和3D生成，并巧妙地利用3D骨骼作为姿态条件，有效解决了传统两阶段方法中姿态标准化导致的图像质量问题。其引入的条件随机清空策略也增强了模型的泛化能力。这项工作对于提高3D角色建模效率和质量具有重要意义。"}}
{"id": "2506.21140", "title": "DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding", "authors": ["Ziwei Wang", "Hongbin Wang", "Tianwang Jia", "Xingyi He", "Siyang Li", "Dongrui Wu"], "summary": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform\nspontaneous/evoked neural activity into control commands for external\ncommunication. While convolutional neural networks (CNNs) remain the mainstream\nbackbone for EEG decoding, their inherently short receptive field makes it\ndifficult to capture long-range temporal dependencies and global inter-channel\nrelationships. Recent CNN-Transformer (Conformers) hybrids partially address\nthis issue, but most adopt a serial design, resulting in suboptimal integration\nof local and global features, and often overlook explicit channel-wise\nmodeling. To address these limitations, we propose DBConformer, a dual-branch\nconvolutional Transformer network tailored for EEG decoding. It integrates a\ntemporal Conformer to model long-range temporal dependencies and a spatial\nConformer to extract inter-channel interactions, capturing both temporal\ndynamics and spatial patterns in EEG signals. A lightweight channel attention\nmodule further refines spatial representations by assigning data-driven\nimportance to EEG channels. Extensive experiments on five motor imagery (MI)\ndatasets and two seizure detection datasets under three evaluation settings\ndemonstrate that DBConformer consistently outperforms 10 competitive baseline\nmodels, with over eight times fewer parameters than the high-capacity EEG\nConformer baseline. Further, the visualization results confirm that the\nfeatures extracted by DBConformer are physiologically interpretable and aligned\nwith sensorimotor priors in MI. The superior performance and interpretability\nof DBConformer make it reliable for robust and explainable EEG decoding. Code\nis publicized at https://github.com/wzwvv/DBConformer.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21140v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21140v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DBConformer：用于脑电图解码的双分支卷积Transformer", "tldr": "DBConformer是一种新型双分支卷积Transformer网络，通过有效结合时间Transformer、空间Transformer和通道注意力模块，解决了传统CNN和串行Conformer在EEG解码中长程依赖和通道间关系建模的不足，在多个数据集上表现优异且参数量少，特征可解释。", "motivation": "传统CNN在EEG解码中难以捕获长程时间依赖和全局通道间关系，因为其感受野短。现有CNN-Transformer混合模型（Conformers）多采用串行设计，导致局部和全局特征集成次优，且常忽视显式的通道建模。", "method": "本文提出了DBConformer，一个专为EEG解码设计的双分支卷积Transformer网络。它集成了一个时间Conformer来建模长程时间依赖，一个空间Conformer来提取通道间相互作用，从而捕获EEG信号中的时间动态和空间模式。一个轻量级的通道注意力模块通过为EEG通道分配数据驱动的重要性，进一步细化了空间表示。", "result": "在五个运动想象（MI）数据集和两个癫痫检测数据集上，在三种评估设置下，DBConformer始终优于10个有竞争力的基线模型。DBConformer的参数量比高容量的EEG Conformer基线少八倍以上。可视化结果证实DBConformer提取的特征具有生理可解释性，并与MI中的感觉运动先验知识一致。", "conclusion": "DBConformer的卓越性能和可解释性使其在稳健和可解释的EEG解码中表现可靠。", "translation": "基于脑电图（EEG）的脑机接口（BCI）将自发/诱发的神经活动转化为外部通信的控制命令。尽管卷积神经网络（CNN）仍然是EEG解码的主流骨干网络，但其固有的短感受野使其难以捕获长程时间依赖和全局通道间关系。最近的CNN-Transformer（Conformers）混合模型部分解决了这个问题，但大多数采用串行设计，导致局部和全局特征的集成次优，并且经常忽视显式的通道建模。为了解决这些局限性，我们提出了DBConformer，一个专为EEG解码设计的双分支卷积Transformer网络。它集成了一个时间Conformer来建模长程时间依赖，一个空间Conformer来提取通道间相互作用，从而捕获EEG信号中的时间动态和空间模式。一个轻量级的通道注意力模块通过为EEG通道分配数据驱动的重要性，进一步细化了空间表示。在五个运动想象（MI）数据集和两个癫痫检测数据集上，在三种评估设置下进行的广泛实验表明，DBConformer始终优于10个有竞争力的基线模型，并且参数量比高容量的EEG Conformer基线少八倍以上。此外，可视化结果证实DBConformer提取的特征具有生理可解释性，并与MI中的感觉运动先验知识一致。DBConformer卓越的性能和可解释性使其在稳健和可解释的EEG解码中表现可靠。代码已在https://github.com/wzwvv/DBConformer 公开。", "summary": "本文提出了DBConformer，一个针对EEG解码的双分支卷积Transformer网络。该模型旨在解决传统CNN在捕获长程时间依赖和全局通道间关系上的不足，以及现有Conformer模型在局部与全局特征集成和通道建模上的次优问题。DBConformer通过结合时间Conformer、空间Conformer和一个轻量级通道注意力模块，有效捕获EEG信号的时间动态和空间模式。实验结果表明，DBConformer在多个运动想象和癫痫检测数据集上表现出优于基线模型的性能，同时拥有更少的参数，并能提取出具有生理可解释性的特征。", "keywords": "脑电图解码, 脑机接口, 卷积Transformer, 双分支网络, 通道注意力", "comments": "DBConformer的创新点在于其双分支架构，分别用时间Conformer和空间Conformer处理EEG信号的时间和空间特征，并通过引入通道注意力模块进一步优化空间表示，有效解决了现有模型在长程依赖和通道间关系建模上的不足。其重要性体现在不仅提升了EEG解码的性能，还显著降低了模型参数量，并提供了生理可解释性，这对于BCI等实际应用非常关键。"}}
{"id": "2506.21263", "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster", "authors": ["Ji Qi", "WenPeng Zhu", "Li Li", "Ming Wu", "YingJun Wu", "Wu He", "Xun Gao", "Jason Zeng", "Michael Heinrich"], "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21263v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21263v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DiLoCoX：一种低通信量大规模去中心化集群训练框架", "tldr": "DiLoCoX是一种低通信量的大规模去中心化集群训练框架，旨在通过结合多种技术，在慢速网络上有效训练超1000亿参数的大模型，实现显著加速。", "motivation": "当前基础模型（特别是大型语言模型）的分布式训练需要高通信量，严重依赖具有快速可靠互连的中心化集群。本文旨在解决在慢速网络上进行训练以及在处理超过1000亿参数模型时释放去中心化集群潜力的问题。", "method": "本文提出了DiLoCoX，一个低通信量大规模去中心化集群训练框架。它结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案。并通过收敛性理论分析验证了通信与本地训练的一步延迟重叠以及自适应梯度压缩方案的益处。", "result": "DiLoCoX能够在1Gbps网络上预训练一个1070亿参数的基础模型。与传统的AllReduce相比，DiLoCoX在分布式训练中实现了357倍的加速，同时模型收敛性退化可以忽略不计。", "conclusion": "DiLoCoX是首个成功应用于超过1000亿参数模型的去中心化训练框架，能够在大规模分布式训练中显著减少通信量并提高效率。", "translation": "基础模型，特别是大型语言模型（LLMs）的分布式训练需要高水平的通信。因此，它高度依赖于具有快速可靠互连的中心化集群。我们能否在慢速网络上进行训练，从而在处理超过1000亿参数的模型时，释放去中心化集群的力量？在本文中，我们提出了DiLoCoX，一个低通信量大规模去中心化集群训练框架。它结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案。这种组合显著提高了参数规模和模型预训练的速度。我们通过收敛性理论分析证明了通信与本地训练的一步延迟重叠以及自适应梯度压缩方案的益处。经验上，我们证明DiLoCoX能够在1Gbps网络上预训练一个1070亿参数的基础模型。与传统的AllReduce相比，DiLoCoX在分布式训练中可以实现357倍的加速，同时保持模型收敛性可忽略的退化。据我们所知，这是第一个成功应用于超过1000亿参数模型的去中心化训练框架。", "summary": "DiLoCoX是一个针对去中心化集群的低通信量大规模训练框架，旨在解决大型模型在慢速网络上分布式训练的挑战。它通过整合流水线并行、双优化器策略、一步延迟通信与本地训练重叠以及自适应梯度压缩等技术，显著提升了参数规模和预训练速度。实验证明，DiLoCoX能在1Gbps网络上预训练1070亿参数模型，相较于AllReduce实现357倍加速，同时保持良好收敛性，是首个应用于千亿级参数模型的去中心化训练框架。", "keywords": "去中心化训练, 低通信量, 大语言模型, 流水线并行, 梯度压缩", "comments": "该论文提出DiLoCoX框架，创新性地结合多种技术以解决大规模模型在去中心化集群和慢速网络下训练的通信瓶颈。其最大的亮点在于实现了千亿级参数模型在低带宽网络下的有效训练，并取得了显著的加速比，这对于推动去中心化AI训练具有重要意义。理论分析和经验验证的结合也增强了其说服力。"}}
{"id": "2506.21080", "title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception", "authors": ["Sanjoy Chowdhury", "Subrata Biswas", "Sayan Nag", "Tushar Nagarajan", "Calvin Murdock", "Ishwarya Ananthabhotla", "Yijun Qian", "Vamsi Krishna Ithapu", "Dinesh Manocha", "Ruohan Gao"], "summary": "Modern perception models, particularly those designed for multisensory\negocentric tasks, have achieved remarkable performance but often come with\nsubstantial computational costs. These high demands pose challenges for\nreal-world deployment, especially in resource-constrained environments. In this\npaper, we introduce EgoAdapt, a framework that adaptively performs cross-modal\ndistillation and policy learning to enable efficient inference across different\negocentric perception tasks, including egocentric action recognition, active\nspeaker localization, and behavior anticipation. Our proposed policy module is\nadaptable to task-specific action spaces, making it broadly applicable.\nExperimental results on three challenging egocentric datasets EPIC-Kitchens,\nEasyCom, and Aria Everyday Activities demonstrate that our method significantly\nenhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,\nand energy up to 9.6x, while still on-par and in many cases outperforming, the\nperformance of corresponding state-of-the-art models.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21080v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21080v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "EgoAdapt：用于高效以自我为中心感知的自适应多感官蒸馏和策略学习", "tldr": "EgoAdapt是一个框架，通过自适应跨模态蒸馏和策略学习，显著提高了以自我为中心感知任务的效率，同时保持或超越了最先进的性能。", "motivation": "现代多感官以自我为中心感知模型性能卓越，但计算成本高昂，难以在资源受限环境中实际部署。", "method": "本文引入了EgoAdapt框架，它通过自适应地执行跨模态蒸馏和策略学习，以实现以自我为中心感知任务（包括动作识别、主动说话人定位和行为预测）的有效推理。其策略模块可适应特定任务的动作空间，具有广泛适用性。", "result": "在EPIC-Kitchens、EasyCom和Aria Everyday Activities三个以自我为中心的数据集上的实验结果表明，EgoAdapt显著提高了效率，GMACs降低高达89.09%，参数减少高达82.02%，能耗降低高达9.6倍，同时性能与最先进模型持平，在许多情况下甚至超越。", "conclusion": "EgoAdapt通过自适应多感官蒸馏和策略学习，成功解决了以自我为中心感知模型的高计算成本问题，实现了显著的效率提升，同时保持了顶尖的性能，使其更适用于实际部署。", "translation": "现代感知模型，特别是那些为多感官以自我为中心任务设计的模型，已经取得了卓越的性能，但通常伴随着巨大的计算成本。这些高要求对实际部署构成了挑战，尤其是在资源受限的环境中。在本文中，我们引入了EgoAdapt，一个自适应执行跨模态蒸馏和策略学习的框架，旨在实现不同以自我为中心感知任务的有效推理，包括以自我为中心动作识别、主动说话人定位和行为预测。我们提出的策略模块可适应特定任务的动作空间，使其具有广泛的适用性。在三个具有挑战性的以自我为中心数据集EPIC-Kitchens、EasyCom和Aria Everyday Activities上的实验结果表明，我们的方法显著提高了效率，GMACs降低高达89.09%，参数减少高达82.02%，能耗降低高达9.6倍，同时性能与相应的最先进模型持平，在许多情况下甚至超越。", "summary": "本文提出了EgoAdapt框架，旨在解决现代多感官以自我为中心感知模型计算成本高昂的问题。EgoAdapt通过自适应跨模态蒸馏和策略学习，有效提升了以自我为中心动作识别、主动说话人定位和行为预测等任务的推理效率。实验证明，该方法在大幅降低计算资源消耗（如GMACs、参数和能耗）的同时，保持了与现有最先进模型相当甚至更优的性能，为资源受限环境下的部署提供了可行方案。", "keywords": "以自我为中心感知, 多感官, 蒸馏, 策略学习, 效率", "comments": "EgoAdapt的创新之处在于其结合了自适应跨模态蒸馏和策略学习，有效解决了以自我为中心感知领域中模型效率与性能之间的矛盾。该方法在显著降低计算资源需求的同时，依然能保持或超越SOTA性能，这对于边缘计算和移动设备上的AI部署具有重要意义。其策略模块的通用性也增加了其应用潜力。"}}
{"id": "2506.21142", "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "summary": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21142v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21142v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "无人机网络攻击的生成对抗规避与域外检测", "tldr": "本文提出一个基于cGAN的框架来生成规避IDS的隐蔽性对抗攻击，并使用CVAE来检测这些攻击。", "motivation": "传统的入侵检测系统（IDS）难以识别新型威胁，且现有方法难以区分隐蔽性对抗攻击与真实的域外（OOD）事件，这使得无人机系统在民用空域中面临漏洞。", "method": "本文引入一个基于条件生成对抗网络（cGAN）的框架来生成规避IDS的隐蔽性对抗攻击。首先训练一个多类IDS分类器，然后cGAN扰动已知攻击以生成被错误分类为良性但统计上仍与OOD分布相似的对抗样本。为检测这些扰动，实现了一个条件变分自动编码器（CVAE），利用负对数似然将对抗输入与真实OOD样本分离。", "result": "基于CVAE的遗憾分数在识别隐蔽性对抗威胁方面显著优于传统的基于Mahalanobis距离的检测器。", "conclusion": "研究结果强调了高级概率建模对于增强IDS能力以应对自适应、基于生成模型的网络入侵的重要性。", "translation": "无人机日益融入民用空域，凸显了对弹性智能入侵检测系统（IDS）的需求，因为传统的异常检测方法往往无法识别新型威胁。一种常见的方法将不熟悉的攻击视为域外（OOD）样本；然而，当缓解措施不足时，这会使系统易受攻击。此外，传统的OOD检测器难以区分隐蔽性对抗攻击与真实的OOD事件。本文引入了一个基于条件生成对抗网络（cGAN）的框架，用于制作规避IDS机制的隐蔽性对抗攻击。我们首先设计了一个鲁棒的多类IDS分类器，该分类器基于良性无人机遥测数据和已知的网络攻击（包括拒绝服务（DoS）、虚假数据注入（FDI）、中间人（MiTM）和重放攻击）进行训练。利用这个分类器，我们的cGAN扰动已知攻击以生成对抗样本，这些样本被错误分类为良性，同时保持与OOD分布的统计相似性。这些对抗样本经过迭代优化，以实现高隐蔽性和成功率。为了检测此类扰动，我们实现了一个条件变分自动编码器（CVAE），利用负对数似然将对抗输入与真实OOD样本分离。比较评估表明，基于CVAE的遗憾分数在识别隐蔽性对抗威胁方面显著优于传统的基于Mahalanobis距离的检测器。我们的发现强调了高级概率建模对于增强IDS能力以应对自适应、基于生成模型的网络入侵的重要性。", "summary": "本研究针对无人机网络攻击中传统入侵检测系统（IDS）难以识别新型威胁和隐蔽性对抗攻击的问题，提出了一种基于cGAN的对抗样本生成框架和基于CVAE的检测方法。该框架利用cGAN生成能够规避IDS并被错误分类为良性的隐蔽性对抗攻击，同时保持与域外（OOD）分布的统计相似性。为检测这些攻击，引入CVAE并利用负对数似然区分对抗输入与真实OOD样本。实验结果表明，CVAE在识别隐蔽性对抗威胁方面显著优于传统的Mahalanobis距离检测器，强调了高级概率建模在增强IDS对抗自适应网络入侵中的重要性。", "keywords": "无人机网络攻击, 生成对抗网络, 入侵检测系统, 域外检测, 条件变分自动编码器", "comments": "这项研究具有创新性，因为它不仅关注如何生成更隐蔽的对抗攻击来规避IDS，还同时提出了有效的检测方法。通过结合cGAN进行攻击生成和CVAE进行防御，该论文提供了一个全面的视角来应对无人机网络攻击中的新兴威胁。特别是CVAE在区分隐蔽对抗样本和真实OOD样本上的表现，为未来的IDS设计提供了新的思路。"}}
{"id": "2506.21277", "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context", "authors": ["Qize Yang", "Shimin Yao", "Weixuan Chen", "Shenghao Fu", "Detao Bai", "Jiaxing Zhao", "Boyuan Sun", "Bowen Yin", "Xihan Wei", "Jingren Zhou"], "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21277v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21277v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HumanOmniV2：从理解到结合上下文的全模态推理", "tldr": "本文提出了HumanOmniV2，一个通过强化学习增强多模态大语言模型推理能力的方法，旨在解决现有模型中全局上下文理解不足和捷径问题。它通过引入上下文、格式、准确性和逻辑奖励来改进推理过程，并提出了IntentBench基准来评估模型理解人类意图的能力。", "motivation": "随着多模态大语言模型的发展，深入理解和解释人类意图的能力变得至关重要，这需要详细和周密的推理。尽管强化学习在增强大语言模型推理能力方面显示出潜力，但将其应用于多模态数据和格式的挑战仍未解决。本文识别出现有多模态推理模型中的两个问题：全局上下文理解不足和捷径问题。", "method": "为了解决全局上下文理解不足和捷径问题，本文强调模型需要清晰理解多模态输入中的全局上下文进行推理。为确保多模态上下文信息的准确解释，引入了由大型语言模型判断的上下文奖励，以及格式和准确性奖励。此外，为提高复杂推理能力，利用LLM评估逻辑奖励，判断推理过程是否成功整合了多模态信息与逻辑方法。还引入了一个推理全模态基准IntentBench，旨在评估模型理解复杂人类意图和情感的能力。", "result": "本文提出的方法在多个全模态基准测试中，相较于其他开源全模态模型，展示了更先进的性能。", "conclusion": "本文通过引入上下文理解和逻辑推理的奖励机制，并提出新的评估基准，显著提升了多模态大语言模型理解复杂人类意图的能力。", "translation": "随着多模态大型语言模型的快速发展，深入理解和解释人类意图的能力已成为一项关键能力，这需要详细而周密的推理。在最近的研究中，强化学习（RL）已显示出增强大型语言模型（LLMs）推理能力的潜力。然而，将RL适应多模态数据和格式的挑战在很大程度上仍未解决。在本文中，我们识别出现有多模态推理模型中的两个问题：全局上下文理解不足和捷径问题。当模型错误解释多模态上下文时，可能发生上下文理解不足，导致不正确的答案。捷径问题发生时，模型忽略多模态输入中的关键线索，直接回答查询而不考虑多模态信息。为了解决这些问题，我们强调模型需要清晰理解多模态输入中的全局上下文进行推理。这种全局上下文理解可以有效防止模型忽略关键多模态线索并确保彻底的推理过程。为确保多模态上下文信息的准确解释，我们实施了一个由大型语言模型判断的上下文奖励，以及格式和准确性奖励。此外，为提高复杂推理能力，我们采用LLM评估逻辑奖励，确定推理过程是否成功整合了多模态信息与逻辑方法。我们还引入了一个推理全模态基准IntentBench，旨在评估模型理解复杂人类意图和情感的能力。我们提出的方法在多个全模态基准测试中，相较于其他开源全模态模型，展示了先进的性能。", "summary": "本文提出了HumanOmniV2，旨在通过解决多模态大语言模型中存在的全局上下文理解不足和捷径问题来提升其推理能力。作者强调模型需基于对全局上下文的清晰理解进行推理，并为此引入了由LLM判断的上下文奖励、格式奖励和准确性奖励。为增强复杂推理能力，模型还利用LLM评估逻辑奖励。此外，本文还推出了一个名为IntentBench的推理全模态基准，用于评估模型对复杂人类意图和情感的理解。实验结果表明，该方法在多个全模态基准上优于其他开源模型。", "keywords": "多模态推理, 全局上下文理解, 强化学习, 大语言模型, IntentBench", "comments": "本文识别并解决了多模态推理模型中的两个关键问题：全局上下文理解不足和捷径问题，这对于提升多模态LLM的鲁棒性和准确性至关重要。通过引入多种奖励机制（上下文、格式、准确性、逻辑）和新的评估基准（IntentBench），该研究提供了一个全面的框架来增强模型对复杂人类意图的理解，具有重要的创新性。"}}
{"id": "2506.21091", "title": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching", "authors": ["Mahmoud Tahmasebi", "Saif Huq", "Kevin Meehan", "Marion McAfee"], "summary": "Stereo matching has become an increasingly important component of modern\nautonomous systems. Developing deep learning-based stereo matching models that\ndeliver high accuracy while operating in real-time continues to be a major\nchallenge in computer vision. In the domain of cost-volume-based stereo\nmatching, accurate disparity estimation depends heavily on large-scale cost\nvolumes. However, such large volumes store substantial redundant information\nand also require computationally intensive aggregation units for processing and\nregression, making real-time performance unattainable. Conversely, small-scale\ncost volumes followed by lightweight aggregation units provide a promising\nroute for real-time performance, but lack sufficient information to ensure\nhighly accurate disparity estimation. To address this challenge, we propose the\nEnhanced Shuffle Mixer (ESM) to mitigate information loss associated with\nsmall-scale cost volumes. ESM restores critical details by integrating primary\nfeatures into the disparity upsampling unit. It quickly extracts features from\nthe initial disparity estimation and fuses them with image features. These\nfeatures are mixed by shuffling and layer splitting then refined through a\ncompact feature-guided hourglass network to recover more detailed scene\ngeometry. The ESM focuses on local contextual connectivity with a large\nreceptive field and low computational cost, leading to the reconstruction of a\nhighly accurate disparity map at real-time. The compact version of ESMStereo\nachieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX\nOrin.", "comment": "Under peer review", "pdf_url": "http://arxiv.org/pdf/2506.21091v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21091v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ESMStereo：增强型ShuffleMixer视差上采样，实现实时精确立体匹配", "tldr": "提出ESMStereo，通过增强型ShuffleMixer（ESM）在小尺度代价体上恢复细节，实现实时高精度立体匹配，速度快且计算成本低。", "motivation": "深度学习立体匹配模型在追求高精度的同时难以实现实时性能。大尺度代价体计算量大且冗余，无法实时；小尺度代价体虽快但精度不足，缺乏足够信息。", "method": "提出增强型Shuffle Mixer (ESM) 来解决小尺度代价体的信息丢失问题。ESM通过将原始特征集成到视差上采样单元中，快速提取初始视差估计特征并与图像特征融合。这些特征通过混洗和层分割进行混合，并通过紧凑的特征引导沙漏网络进行细化，以恢复更详细的场景几何。ESM专注于具有大感受野和低计算成本的局部上下文连接性。", "result": "ESMStereo的紧凑版本在高端GPU上实现了116 FPS的推理速度，在AGX Orin上实现了91 FPS。", "conclusion": "ESMStereo通过ESM技术，成功解决了实时性和高精度立体匹配的挑战，实现了在低计算成本下重建高精度视差图。", "translation": "立体匹配已成为现代自主系统日益重要的组成部分。开发基于深度学习的立体匹配模型，在提供高精度的同时实现实时操作，仍然是计算机视觉领域的一个主要挑战。在基于代价体的立体匹配领域，准确的视差估计严重依赖于大规模代价体。然而，如此大的代价体存储了大量的冗余信息，并且还需要计算密集型的聚合单元进行处理和回归，这使得实时性能无法实现。相反，小规模代价体加上轻量级聚合单元为实时性能提供了一条有前景的途径，但缺乏足够的信息来确保高精度的视差估计。为了应对这一挑战，我们提出了增强型Shuffle Mixer（ESM）来减轻与小规模代价体相关的信息丢失。ESM通过将主要特征集成到视差上采样单元中来恢复关键细节。它从初始视差估计中快速提取特征并将其与图像特征融合。这些特征通过混洗和层分割进行混合，然后通过一个紧凑的特征引导沙漏网络进行细化，以恢复更详细的场景几何。ESM专注于具有大感受野和低计算成本的局部上下文连接性，从而在实时条件下重建高精度视差图。ESMStereo的紧凑版本在高端GPU上实现了116 FPS的推理速度，在AGX Orin上实现了91 FPS。", "summary": "本文提出ESMStereo，一种针对实时高精度立体匹配的深度学习模型。为解决现有方法中大尺度代价体计算量大和小尺度代价体精度不足的问题，ESMStereo引入了增强型Shuffle Mixer (ESM)。ESM通过将原始特征整合到视差上采样单元中，并利用特征混洗、层分割和特征引导沙漏网络来恢复细节，有效弥补了小尺度代价体的信息损失。该方法实现了低计算成本下的大感受野局部上下文连接，从而能实时重建高精度视差图。实验结果显示，ESMStereo在高端GPU上能达到116 FPS，在AGX Orin上达到91 FPS。", "keywords": "立体匹配, 视差上采样, ShuffleMixer, 实时性能, 深度学习", "comments": "这篇论文通过提出ESM模块，巧妙地解决了实时立体匹配中精度与速度难以兼顾的矛盾。其核心创新在于通过特征融合和高效的特征混合机制，在小尺度代价体上恢复了关键的细节信息，同时保持了低计算成本。这对于自动驾驶等需要实时高精度感知能力的系统具有重要意义。"}}
{"id": "2506.21144", "title": "Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion", "authors": ["Yuguang Zhang", "Kuangpu Guo", "Zhihe Lu", "Yunbo Wang", "Jian Liang"], "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, but is challenged by\nheterogeneity in data, computation, and communication. Pretrained\nvision-language models (VLMs), with their strong generalization and lightweight\ntuning via prompts, offer a promising solution. However, existing federated\nprompt-learning methods rely only on text prompts and overlook joint\nlabel-domain distribution shifts. In this paper, we propose a personalized FL\nframework based on dual-prompt learning and cross fusion, termed pFedDC.\nSpecifically, each client maintains both global and local prompts across vision\nand language modalities: global prompts capture common knowledge shared across\nthe federation, while local prompts encode client-specific semantics and domain\ncharacteristics. Meanwhile, a cross-fusion module is designed to adaptively\nintegrate prompts from different levels, enabling the model to generate\npersonalized representations aligned with each client's unique data\ndistribution. Extensive experiments across nine datasets with various types of\nheterogeneity show that pFedDC consistently outperforms state-of-the-art\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21144v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21144v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过双提示优化和交叉融合的个性化联邦学习", "tldr": "联邦学习（FL）面临异质性挑战。预训练视觉-语言模型（VLM）结合提示学习有潜力解决此问题，但现有方法存在局限性。本文提出了pFedDC，一个基于双提示优化和交叉融合的个性化FL框架，通过全局和局部提示以及跨模态融合来处理客户端异质性，并在多个数据集上表现优于现有SOTA方法。", "motivation": "联邦学习（FL）面临数据、计算和通信方面的异质性挑战。现有的联邦提示学习方法仅依赖文本提示，并且忽视了联合标签域分布偏移。", "method": "本文提出了一个名为pFedDC的个性化联邦学习框架，它基于双提示学习和交叉融合。具体而言，每个客户端在视觉和语言模态上都维护全局和局部提示：全局提示捕获联邦共享的通用知识，而局部提示编码客户端特定的语义和领域特征。同时，设计了一个交叉融合模块，自适应地整合不同级别的提示，使模型能够生成与每个客户端独特数据分布对齐的个性化表示。", "result": "在九个具有各种异质性类型的数据集上进行的广泛实验表明，pFedDC始终优于最先进的方法。", "conclusion": "本文提出的pFedDC框架通过双提示优化和交叉融合有效解决了联邦学习中的异质性问题，并在多个数据集上取得了优于现有最先进方法的性能。", "translation": "联邦学习（FL）使得去中心化客户端之间能够协同训练模型，而无需共享本地数据，但其受到数据、计算和通信异质性的挑战。预训练视觉-语言模型（VLM）凭借其强大的泛化能力和轻量级提示微调，提供了一个有前景的解决方案。然而，现有的联邦提示学习方法仅依赖于文本提示，并且忽视了联合标签域分布偏移。在本文中，我们提出了一个基于双提示学习和交叉融合的个性化FL框架，命名为pFedDC。具体而言，每个客户端在视觉和语言模态上都维护全局和局部提示：全局提示捕获联邦共享的通用知识，而局部提示编码客户端特定的语义和领域特征。同时，设计了一个交叉融合模块，自适应地整合不同级别的提示，使模型能够生成与每个客户端独特数据分布对齐的个性化表示。在九个具有各种异质性类型的数据集上进行的广泛实验表明，pFedDC始终优于最先进的方法。", "summary": "本文提出了一种名为pFedDC的个性化联邦学习（FL）框架，旨在解决FL中存在的异质性问题。该框架利用预训练视觉-语言模型（VLMs）的优势，引入了双提示学习和交叉融合机制。每个客户端维护全局和局部提示以捕捉共享知识和客户端特定语义，并通过交叉融合模块生成个性化表示。实验结果表明，pFedDC在多种异质性数据集上均优于现有的先进方法。", "keywords": "联邦学习, 个性化联邦学习, 提示学习, 视觉-语言模型, 异质性", "comments": "该论文的创新点在于提出了结合双提示学习（全局和局部提示）和跨模态融合的个性化联邦学习框架pFedDC，有效利用了VLM的强大能力来解决FL中的数据异质性问题。这种分层提示和自适应融合的方法为实现更高效和个性化的FL提供了新的思路。"}}
{"id": "2506.21101", "title": "OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "AndyPian Wu", "Chaoyang Wang", "Chengjie Wang", "Taisong Jin", "SevenShu", "Yunsheng Wu", "Yongge Liu", "Rongrong Ji"], "summary": "As one of the earliest ancient languages, Oracle Bone Script (OBS)\nencapsulates the cultural records and intellectual expressions of ancient\ncivilizations. Despite the discovery of approximately 4,500 OBS characters,\nonly about 1,600 have been deciphered. The remaining undeciphered ones, with\ntheir complex structure and abstract imagery, pose significant challenges for\ninterpretation. To address these challenges, this paper proposes a novel\ntwo-stage semantic typography framework, named OracleFusion. In the first\nstage, this approach leverages the Multimodal Large Language Model (MLLM) with\nenhanced Spatial Awareness Reasoning (SAR) to analyze the glyph structure of\nthe OBS character and perform visual localization of key components. In the\nsecond stage, we introduce Oracle Structural Vector Fusion (OSVF),\nincorporating glyph structure constraints and glyph maintenance constraints to\nensure the accurate generation of semantically enriched vector fonts. This\napproach preserves the objective integrity of the glyph structure, offering\nvisually enhanced representations that assist experts in deciphering OBS.\nExtensive qualitative and quantitative experiments demonstrate that\nOracleFusion outperforms state-of-the-art baseline models in terms of\nsemantics, visual appeal, and glyph maintenance, significantly enhancing both\nreadability and aesthetic quality. Furthermore, OracleFusion provides\nexpert-like insights on unseen oracle characters, making it a valuable tool for\nadvancing the decipherment of OBS.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21101v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21101v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "OracleFusion：通过结构化约束语义排版辅助甲骨文破译", "tldr": "OracleFusion是一个两阶段的语义排版框架，利用多模态大语言模型和结构化向量融合，帮助专家破译复杂的甲骨文，并显著提升可读性和美观性。", "motivation": "甲骨文作为最早的古老语言之一，约有4500个字符，但其中约1600个已被破译，其余未破译的字符结构复杂、意象抽象，给解读带来了巨大挑战。本研究旨在解决这些挑战。", "method": "本文提出了一个名为OracleFusion的新型两阶段语义排版框架。第一阶段，该方法利用增强空间感知推理（SAR）的多模态大语言模型（MLLM）分析甲骨文的字形结构并进行关键组件的视觉定位。第二阶段，引入了甲骨文结构向量融合（OSVF），结合字形结构约束和字形维护约束，以确保准确生成语义丰富的矢量字体。", "result": "广泛的定性和定量实验表明，OracleFusion在语义、视觉吸引力和字形维护方面优于最先进的基线模型，显著增强了可读性和美学质量。此外，OracleFusion为未见过的甲骨文提供了专家级的洞察力。", "conclusion": "OracleFusion是推进甲骨文破译的宝贵工具。", "translation": "甲骨文作为最早的古老语言之一，承载着古代文明的文化记录和思想表达。尽管已发现约4500个甲骨文，但其中只有约1600个已被破译。其余未破译的字符，其复杂的结构和抽象的意象，给解读带来了巨大挑战。为了应对这些挑战，本文提出了一种新颖的两阶段语义排版框架，名为OracleFusion。在第一阶段，该方法利用增强空间感知推理（SAR）的多模态大语言模型（MLLM）分析甲骨文的字形结构并进行关键组件的视觉定位。在第二阶段，我们引入了甲骨文结构向量融合（OSVF），结合字形结构约束和字形维护约束，以确保准确生成语义丰富的矢量字体。这种方法保留了字形结构的客观完整性，提供了视觉增强的表示，辅助专家破译甲骨文。广泛的定性和定量实验表明，OracleFusion在语义、视觉吸引力和字形维护方面优于最先进的基线模型，显著增强了可读性和美学质量。此外，OracleFusion为未见过的甲骨文提供了专家级的洞察力，使其成为推进甲骨文破译的宝贵工具。", "summary": "本文提出了一个名为OracleFusion的两阶段语义排版框架，旨在辅助甲骨文的破译。该框架首先利用增强空间感知推理的多模态大语言模型分析甲骨文字形结构并定位关键组件，然后通过引入甲骨文结构向量融合，结合字形结构和维护约束，生成语义丰富的矢量字体。实验证明，OracleFusion在语义、视觉效果和字形维护方面优于现有模型，显著提升了甲骨文的可读性和美观性，并能为专家提供未见字符的洞察力，是甲骨文破译的重要工具。", "keywords": "甲骨文, 破译, 语义排版, 多模态大语言模型, 字形结构", "comments": "该论文提出了一种创新性的两阶段框架OracleFusion，将多模态大语言模型与结构化约束相结合，解决了甲骨文破译中字形复杂和抽象的挑战。其创新点在于结合了先进的AI技术与传统文本分析，通过语义排版提供了视觉增强的表示，有效辅助了专家工作。该方法不仅提高了破译效率，也为其他古文字的数字化研究提供了新的思路。"}}
{"id": "2506.21146", "title": "Linearity-based neural network compression", "authors": ["Silas Dobler", "Florian Lemmerich"], "summary": "In neural network compression, most current methods reduce unnecessary\nparameters by measuring importance and redundancy. To augment already highly\noptimized existing solutions, we propose linearity-based compression as a novel\nway to reduce weights in a neural network. It is based on the intuition that\nwith ReLU-like activation functions, neurons that are almost always activated\nbehave linearly, allowing for merging of subsequent layers. We introduce the\ntheory underlying this compression and evaluate our approach experimentally.\nOur novel method achieves a lossless compression down to 1/4 of the original\nmodel size in over the majority of tested models. Applying our method on\nalready importance-based pruned models shows very little interference between\ndifferent types of compression, demonstrating the option of successful\ncombination of techniques. Overall, our work lays the foundation for a new type\nof compression method that enables smaller and ultimately more efficient neural\nnetwork models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21146v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21146v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于线性度的神经网络压缩", "tldr": "本文提出了一种基于线性度的新型神经网络压缩方法，通过合并线性行为的层，实现了对多数模型高达1/4的无损压缩，并可与其他压缩方法结合。", "motivation": "当前的神经网络压缩方法主要通过衡量重要性和冗余度来减少参数。为了进一步优化现有解决方案，本文旨在提出一种新的权重减少方法。", "method": "本文提出了一种基于线性度的压缩方法。其核心思想是，对于使用ReLU类激活函数的神经网络，几乎总是激活的神经元表现出线性行为，这允许合并后续层。文章介绍了该压缩方法的理论基础并进行了实验评估。", "result": "该方法在大多数测试模型上实现了无损压缩，将模型大小减小到原始的1/4。将此方法应用于已基于重要性剪枝的模型时，显示出与其他压缩类型之间很少的干扰，表明可以成功组合不同的技术。", "conclusion": "这项工作为一种新型的压缩方法奠定了基础，该方法能够实现更小、最终更高效的神经网络模型。", "translation": "在神经网络压缩领域，目前大多数方法通过衡量重要性和冗余度来减少不必要的参数。为了增强现有高度优化的解决方案，我们提出了一种基于线性度的压缩方法，作为减少神经网络权重的新颖方式。它基于这样的直觉：对于ReLU类激活函数，几乎总是激活的神经元表现出线性行为，从而允许合并后续层。我们介绍了这种压缩背后的理论，并对我们的方法进行了实验评估。我们新颖的方法在大多数测试模型中实现了无损压缩，将模型大小压缩到原始的1/4。将我们的方法应用于已经基于重要性剪枝的模型时，显示出不同类型压缩之间几乎没有干扰，这表明成功组合技术的可能性。总的来说，我们的工作为一种新型压缩方法奠定了基础，该方法能够实现更小、最终更高效的神经网络模型。", "summary": "本文提出了一种新颖的基于线性度的神经网络压缩方法，其核心在于利用ReLU类激活函数下神经元的线性行为来合并后续层。该方法在大多数测试模型上实现了高达1/4的无损压缩，并被证明可以成功地与现有的基于重要性的剪枝方法结合使用，为构建更小、更高效的神经网络模型开辟了新途径。", "keywords": "神经网络压缩, 线性度, ReLU, 模型合并, 无损压缩", "comments": "这项工作提出了一种新颖的神经网络压缩视角，即利用神经元的线性行为进行层合并，而非传统的基于重要性或冗余度的剪枝。其创新性在于发现了ReLU激活函数下神经元的线性特性可用于模型压缩，并实现了显著的无损压缩比例。此外，该方法能够与现有压缩技术结合，增加了其实用性和潜在影响力。"}}
{"id": "2506.21328", "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts", "authors": ["Jiajie Yang"], "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing.", "comment": "15 pages,4 figures", "pdf_url": "http://arxiv.org/pdf/2506.21328v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21328v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "潜在原型路由：在专家混合模型中实现近乎完美的负载均衡", "tldr": "提出LPR路由框架，解决了MoE模型中专家负载不平衡问题，实现了近乎完美的负载均衡，同时不影响性能。", "motivation": "当前MoE系统存在严重的负载不平衡问题，即在训练和推理过程中只有一小部分专家被持续激活，导致模型容量和计算资源严重未充分利用。", "method": "通过聚类视角重新审视专家路由，并提出了潜在原型路由（Latent Prototype Routing, LPR），这是一个新颖的路由框架，它推广了现有方法，并在不损害下游性能的情况下促进了平衡的专家利用。", "result": "在多个开源MoE模型（包括DeepSeek-V3, Qwen3-MoE, Mixtral）上的实验表明，LPR将专家负载的基尼系数平均从0.70降低到0.035，将最小-最大专家负载比从1e-6提高到0.70，实现了近乎完美的负载均衡。", "conclusion": "LPR成功解决了MoE模型的负载不平衡问题，显著提高了资源利用率，且未损害模型性能。", "translation": "专家混合（MoE）架构已成为有效扩展大型语言模型（LLMs）的关键策略。然而，当前的MoE系统存在严重的负载不平衡问题，即在训练和推理过程中，只有一小部分专家被持续激活，导致模型容量和计算资源严重未充分利用。在这项工作中，我们通过聚类视角重新审视专家路由，并提出了潜在原型路由（Latent Prototype Routing, LPR），这是一个新颖的路由框架，它推广了现有方法，并在不损害下游性能的情况下促进了平衡的专家利用。在多个开源MoE模型（包括DeepSeek-V3、Qwen3-MoE和Mixtral）上进行的广泛实验表明，LPR将专家负载的基尼系数平均从0.70降低到0.035，将最小-最大专家负载比从1e-6提高到0.70，实现了近乎完美的负载均衡。", "summary": "这项工作提出了潜在原型路由（LPR），一个针对专家混合（MoE）模型的新型路由框架，旨在解决现有MoE架构中严重的专家负载不平衡问题。LPR通过聚类视角推广了现有方法，并在不影响下游性能的前提下，显著提高了专家利用的平衡性。实验证明，LPR在多种MoE模型上实现了近乎完美的负载均衡，大幅降低了基尼系数并提升了最小-最大负载比。", "keywords": "专家混合, 负载均衡, 潜在原型路由, 大型语言模型, 路由算法", "comments": "这篇论文通过引入潜在原型路由（LPR）为MoE模型中的负载均衡问题提供了一个创新的解决方案。其重要性在于，它显著提升了大型语言模型中MoE架构的资源利用效率，解决了长期存在的专家利用率低下问题，从而有助于更高效地扩展LLMs。"}}
{"id": "2506.21109", "title": "Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection", "authors": ["Luosheng Xu", "Dalin Zhang", "Zhaohui Song"], "summary": "Remote sensing change detection is essential for monitoring urban expansion,\ndisaster assessment, and resource management, offering timely, accurate, and\nlarge-scale insights into dynamic landscape transformations. While deep\nlearning has revolutionized change detection, the increasing complexity and\ncomputational demands of modern models have not necessarily translated into\nsignificant accuracy gains. Instead of following this trend, this study\nexplores a more efficient approach, focusing on lightweight models that\nmaintain high accuracy while minimizing resource consumption, which is an\nessential requirement for on-satellite processing. To this end, we propose\nFlickCD, which means quick flick then get great results, pushing the boundaries\nof the performance-resource trade-off. FlickCD introduces an Enhanced\nDifference Module (EDM) to amplify critical feature differences between\ntemporal phases while suppressing irrelevant variations such as lighting and\nweather changes, thereby reducing computational costs in the subsequent change\ndecoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion\nBlocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global\nSelf-Attention (EGSA) to efficiently capture semantic information at multiple\nscales, preserving both coarse- and fine-grained changes. Extensive experiments\non four benchmark datasets demonstrate that FlickCD reduces computational and\nstorage overheads by more than an order of magnitude while achieving\nstate-of-the-art (SOTA) performance or incurring only a minor (<1\\% F1)\naccuracy trade-off. The implementation code is publicly available at\nhttps://github.com/xulsh8/FlickCD.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.21109v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21109v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "突破权衡界限：紧凑而有效的遥感变化检测", "tldr": "本文提出了FlickCD，一个轻量级遥感变化检测模型，它在大幅减少计算和存储开销的同时，保持了最先进的性能，适用于星载处理。", "motivation": "深度学习模型在遥感变化检测中日益复杂且计算需求高，但并未带来显著精度提升。本研究旨在探索一种更高效的方法，即开发轻量级模型，以在最小化资源消耗的同时保持高精度，满足星载处理需求。", "method": "提出了FlickCD模型。FlickCD引入了增强差异模块（EDM）来放大关键特征差异并抑制无关变化（如光照、天气），从而降低后续变化解码器的计算成本。此外，FlickCD解码器结合了局部-全局融合块，利用移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）来有效捕获多尺度语义信息，保留粗粒度和细粒度变化。", "result": "在四个基准数据集上的广泛实验表明，FlickCD将计算和存储开销降低了一个数量级以上，同时实现了最先进（SOTA）的性能或仅带来微小的（<1% F1）精度权衡。", "conclusion": "FlickCD成功地在遥感变化检测中推动了性能-资源权衡的边界，证明了在保持高精度的同时实现显著资源效率是可行的，特别适用于星载处理。", "translation": "遥感变化检测对于监测城市扩张、灾害评估和资源管理至关重要，能为动态景观转型提供及时、准确和大规模的洞察。虽然深度学习彻底改变了变化检测，但现代模型日益增长的复杂性和计算需求并未必然转化为显著的精度提升。本研究不追随这一趋势，而是探索一种更高效的方法，专注于轻量级模型，这些模型在最小化资源消耗的同时保持高精度，这是星载处理的基本要求。为此，我们提出了FlickCD（意为快速一瞥即可获得出色结果），它推动了性能-资源权衡的界限。FlickCD引入了增强差异模块（EDM），以放大时间阶段之间的关键特征差异，同时抑制光照和天气变化等无关变化，从而降低后续变化解码器的计算成本。此外，FlickCD解码器结合了局部-全局融合块，利用移位窗口自注意力（SWSA）和增强全局自注意力（EGSA）来有效捕获多尺度语义信息，保留粗粒度和细粒度变化。在四个基准数据集上的广泛实验表明，FlickCD将计算和存储开销降低了一个数量级以上，同时实现了最先进（SOTA）的性能或仅带来微小的（<1% F1）精度权衡。实现代码已在https://github.com/xulsh8/FlickCD 公开。", "summary": "本文提出了一种名为FlickCD的轻量级遥感变化检测模型，旨在解决现有深度学习模型计算复杂性高但精度提升有限的问题。FlickCD通过引入增强差异模块（EDM）有效处理时间差异并降低计算成本，并利用局部-全局融合块（结合SWSA和EGSA）捕获多尺度语义信息。实验证明，FlickCD在大幅减少计算和存储开销的同时，在多个基准数据集上实现了先进的性能，或仅有微小的精度损失，特别适用于资源受限的星载环境。", "keywords": "遥感变化检测, 轻量级模型, FlickCD, 性能-资源权衡, 星载处理", "comments": "这篇论文的创新点在于其对性能-资源权衡的关注，特别是在遥感变化检测领域对轻量级模型的追求，以适应星载处理的需求。FlickCD通过其独特的EDM和局部-全局融合块设计，有效地平衡了效率和准确性，为实际部署提供了有前景的解决方案。"}}
{"id": "2506.21158", "title": "Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design", "authors": ["Hampus Gummesson Svensson", "Ola Engkvist", "Jon Paul Janet", "Christian Tyrchan", "Morteza Haghir Chehreghani"], "summary": "In many real-world applications, evaluating the goodness of instances is\noften costly and time-consuming, e.g., human feedback and physics simulations,\nin contrast to proposing new instances. In particular, this is even more\ncritical in reinforcement learning, as new interactions with the environment\n(i.e., new instances) need to be evaluated to provide a reward signal to learn\nfrom. As sufficient exploration is crucial, learning from a diverse mini-batch\ncan have a large impact and help mitigate mode collapse. In this paper, we\nintroduce diverse mini-batch selection for reinforcement learning and propose\nto use determinantal point processes for this task. We study this framework in\nthe context of a real-world problem, namely drug discovery. We experimentally\nstudy how our proposed framework can improve the effectiveness of chemical\nexploration in de novo drug design, where finding diverse and high-quality\nsolutions is essential. We conduct a comprehensive evaluation with three\nwell-established molecular generation oracles over numerous generative steps.\nOur experiments conclude that our diverse mini-batch selection framework can\nsubstantially improve the diversity of the solutions, while still obtaining\nsolutions of high quality. In drug discovery, such outcome can potentially lead\nto fulfilling unmet medication needs faster.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21158v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21158v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "强化学习中多样化小批量选择用于从头药物设计中的高效化学探索", "tldr": "本文提出了一种在强化学习中选择多样化小批量的方法，利用行列式点过程来提高药物发现中化学探索的效率和多样性，同时保持解决方案的质量。", "motivation": "在许多实际应用中，评估实例的“好坏”成本高昂且耗时，特别是在强化学习中，新的环境交互需要被评估以提供奖励信号。为了有效探索并避免模式崩溃，从多样化的小批量中学习至关重要。", "method": "本文提出在强化学习中引入多样化小批量选择，并建议使用行列式点过程（Determinantal Point Processes, DPPs）来完成这项任务。该框架在药物发现的背景下进行研究和实验。", "result": "实验结果表明，所提出的多样化小批量选择框架可以显著提高解决方案的多样性，同时仍能获得高质量的解决方案。", "conclusion": "该多样化小批量选择框架可以提高从头药物设计中化学探索的效率，有望更快地满足未被满足的药物需求。", "translation": "在许多实际应用中，评估实例的优劣往往成本高昂且耗时，例如人类反馈和物理模拟，这与提出新实例形成对比。特别是，这在强化学习中更为关键，因为需要评估与环境的新交互（即新实例）以提供可供学习的奖励信号。由于充分的探索至关重要，从多样化的小批量中学习可以产生巨大影响并有助于缓解模式崩溃。在本文中，我们引入了强化学习中的多样化小批量选择，并提出使用行列式点过程来完成这项任务。我们在一个实际问题，即药物发现的背景下研究了该框架。我们实验研究了我们提出的框架如何提高从头药物设计中化学探索的有效性，其中找到多样化和高质量的解决方案至关重要。我们使用三个成熟的分子生成预言机进行了全面的评估，涉及大量的生成步骤。我们的实验得出结论，我们的多样化小批量选择框架可以显著提高解决方案的多样性，同时仍能获得高质量的解决方案。在药物发现中，这种结果可能潜在地导致更快地满足未被满足的药物需求。", "summary": "本文提出了一种在强化学习中进行多样化小批量选择的新框架，旨在解决实例评估成本高昂的问题，尤其是在药物发现等实际应用中。通过利用行列式点过程，该方法能够促进高效的化学探索，从而在从头药物设计中生成多样化且高质量的分子。实验证明，该框架在提高解决方案多样性的同时，保持了其高品质，有望加速新药的研发。", "keywords": "强化学习, 多样化小批量选择, 行列式点过程, 从头药物设计, 化学探索", "comments": "本文的创新点在于将行列式点过程引入强化学习中的小批量选择，以解决探索不足和模式崩溃的问题。其重要性体现在将该方法应用于药物发现这一实际且高价值的领域，有效提升了化学探索的效率和解决方案的多样性。这对于加速新药研发，满足未被满足的医疗需求具有显著的潜在影响。"}}
{"id": "2506.21116", "title": "IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes", "authors": ["Yujia Liang", "Jile Jiao", "Zhicheng Wang", "Xuetao Feng", "Zixuan Ye", "Yuan Wang", "Hao Lu"], "summary": "Video Large Language Models (VideoLLMs) have demonstrated remarkable\nunderstanding capabilities, but are found struggling to tackle multi-shot\nscenarios,e.g., video clips with varying camera angles or scene changes. This\nchallenge can render failures such as instance identity forgetting and key\nframe negligence. In this work, we first attribute the challenge to the lack of\nmulti-shot annotations among existing datasets and therefore we introduce a new\ndataset termed MultiClip-Bench, featuring dense descriptions and\ninstruction-based question-answering pairs tailored for multi-shot scenarios.\nWe empirically find that the training set significantly boosts the multi-shot\nperformance, while the testing benchmark provides a reliable measure of the\nmodel capability in multi-shot scenarios. By further analyzing and discovering\nthat current models only encode instance features in a discrete or lossy\nmanner, at the risk of missing identity information, we then contribute a new\nmodel IPFormer-VideoLLM. Its key idea is the injection of instance-level\nfeatures as instance prompts through an efficient attention-based connector.\nThis allows for the aggregation of instance-specific information across scenes.\nExperiments demonstrate that our proposed dataset and model not only enhance\nthe multi-scene video understanding significantly, but also offer distinct\nadvantages across various video benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21116v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21116v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "IPFormer-VideoLLM：增强多镜头场景下的多模态视频理解", "tldr": "该研究针对现有视频大语言模型在多镜头场景中表现不佳的问题，提出了一个新的多镜头数据集MultiClip-Bench和一种名为IPFormer-VideoLLM的新模型，通过注入实例级特征来显著提升多场景视频理解能力。", "motivation": "现有的视频大语言模型（VideoLLMs）在处理多镜头场景（如不同摄像机角度或场景变化的视频片段）时表现不佳，容易出现实例身份遗忘和关键帧忽略等问题。研究者认为这是由于现有数据集中缺乏多镜头标注。", "method": "首先，引入了一个名为MultiClip-Bench的新数据集，该数据集包含针对多镜头场景的密集描述和基于指令的问答对。其次，针对现有模型离散或有损编码实例特征的问题，提出了IPFormer-VideoLLM模型，其核心思想是通过高效的基于注意力的连接器注入实例级特征作为实例提示，从而聚合跨场景的实例特定信息。", "result": "实验表明，所提出的数据集和模型不仅显著增强了多场景视频理解能力，而且在各种视频基准测试中展现出明显的优势。", "conclusion": "本文提出的MultiClip-Bench数据集和IPFormer-VideoLLM模型有效解决了视频大语言模型在多镜头场景理解上的挑战，显著提升了多场景视频理解性能。", "translation": "视频大语言模型（VideoLLMs）已展现出卓越的理解能力，但在处理多镜头场景时（例如，具有不同摄像机角度或场景变化的视频片段）却发现它们难以应对。这一挑战可能导致诸如实例身份遗忘和关键帧忽略等失败。在这项工作中，我们首先将这一挑战归因于现有数据集中缺乏多镜头标注，因此我们引入了一个名为MultiClip-Bench的新数据集，该数据集具有针对多镜头场景的密集描述和基于指令的问答对。我们通过经验发现，训练集显著提升了多镜头性能，而测试基准则为模型在多镜头场景中的能力提供了可靠的衡量标准。通过进一步分析和发现当前模型仅以离散或有损方式编码实例特征，并存在丢失身份信息的风险，我们随后贡献了一个新模型IPFormer-VideoLLM。其核心思想是通过高效的基于注意力的连接器注入实例级特征作为实例提示。这允许聚合跨场景的实例特定信息。实验证明，我们提出的数据集和模型不仅显著增强了多场景视频理解能力，而且在各种视频基准测试中提供了独特的优势。", "summary": "本研究旨在解决视频大语言模型在多镜头场景理解中的不足。作者指出现有数据集缺乏多镜头标注是主要原因，并为此推出了MultiClip-Bench数据集，包含密集描述和问答对。此外，为了解决现有模型实例特征编码的缺陷，提出了IPFormer-VideoLLM模型，通过注意力连接器注入实例级特征，以聚合跨场景的实例信息。实验证明，该数据集和模型显著提升了多场景视频理解能力，并在多个视频基准上表现优异。", "keywords": "视频大语言模型, 多镜头理解, MultiClip-Bench, IPFormer-VideoLLM, 实例特征", "comments": "该论文的创新点在于同时解决了多镜头视频理解中的数据和模型问题。MultiClip-Bench数据集填补了多镜头标注的空白，为模型训练和评估提供了基础。IPFormer-VideoLLM通过注入实例级特征，有效解决了实例身份遗忘问题，提升了模型对复杂场景的理解能力。这种数据与模型协同改进的方法具有重要的实践意义。"}}
{"id": "2506.21408", "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference", "authors": ["Colin Samplawski", "Adam D. Cobb", "Manoj Acharya", "Ramneet Kaur", "Susmit Jha"], "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.", "comment": "Accepted at UAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21408v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21408v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过随机变分子空间推断实现大型语言模型的可扩展贝叶斯低秩适应", "tldr": "ScalaBL是一种可扩展的贝叶斯低秩适应方法，通过在低维子空间中进行推断，有效解决了大型语言模型的不确定性量化问题，且仅需少量额外参数。", "motivation": "大型语言模型（LLMs）存在生成错误信息和校准不良的问题，这使得模型的不确定性量化至关重要，尤其是在自动驾驶和医疗保健等高风险领域。现有的基于贝叶斯深度学习的方法虽然通过对微调模型的低秩适应（LoRA）参数进行推断，使其更易于处理，但由于需要额外的参数，难以扩展到更大的LLMs。", "method": "本文提出了可扩展贝叶斯低秩适应（ScalaBL），通过随机变分子空间推断实现。该方法在LoRA秩r的r维子空间中进行贝叶斯推断。通过将LoRA参数重新用作投影矩阵，能够将子空间中的样本映射到LLM的完整权重空间。所有参数都通过随机变分推断学习。", "result": "ScalaBL在仅需约1000个额外参数的情况下，实现了与最先进方法相当的性能。此外，它能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是现有工作的四倍。", "conclusion": "ScalaBL为大型语言模型中的贝叶斯不确定性量化提供了一种可扩展且参数高效的解决方案，在保持竞争性性能的同时，在规模上超越了现有方法。", "translation": "尽管大型语言模型（LLMs）被广泛使用，但它们已知会产生错误信息且校准不佳。这使得这些模型的不确定性量化变得至关重要，特别是在自动驾驶和医疗保健等高风险领域。先前的工作通过对微调模型的低秩适应（LoRA）参数进行推断，使基于贝叶斯深度学习的方法更易于处理这一问题。然而，这些方法需要额外的参数，因此难以扩展到更大的LLMs。本文提出了通过随机变分子空间推断实现的可扩展贝叶斯低秩适应（ScalaBL）。我们对LoRA秩r的r维子空间进行贝叶斯推断。通过将LoRA参数重新用作投影矩阵，我们能够将该子空间中的样本映射到LLM的完整权重空间。这使我们能够使用随机变分推断学习我们方法的所有参数。尽管我们的子空间维度较低，我们仍能实现与最先进方法相当的性能，并且仅需要约1000个额外参数。此外，它使我们能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是现有工作的四倍。", "summary": "针对大型语言模型（LLMs）不确定性量化和现有贝叶斯LoRA方法扩展性差的问题，本文提出了ScalaBL。该方法通过在低维子空间中进行贝叶斯推断，并利用LoRA参数作为投影矩阵，将子空间样本映射到LLM的完整权重空间，从而实现高效的参数学习。ScalaBL在仅需少量额外参数的情况下，即可达到与现有技术相当的性能，并能扩展到规模更大的贝叶斯LLM。", "keywords": "贝叶斯LLM, 低秩适应, 不确定性量化, 随机变分推断, 可扩展性", "comments": "本文的创新之处在于提出了一种名为ScalaBL的可扩展贝叶斯低秩适应方法，它通过在低维子空间中进行推断并巧妙地重用LoRA参数作为投影矩阵，显著提高了大型语言模型不确定性量化方法的扩展性。这种方法在仅增加极少量参数（约1000个）的情况下，实现了与最先进技术相当的性能，并成功应用于迄今为止最大的贝叶斯LLM。这对于在高风险领域部署具有不确定性感知能力的LLM具有重要意义，克服了现有贝叶斯深度学习方法在处理大型模型时的主要瓶颈。"}}
{"id": "2506.21117", "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization", "authors": ["Jan Ackermann", "Jonas Kulhanek", "Shengqu Cai", "Haofei Xu", "Marc Pollefeys", "Gordon Wetzstein", "Leonidas Guibas", "Songyou Peng"], "summary": "In dynamic 3D environments, accurately updating scene representations over\ntime is crucial for applications in robotics, mixed reality, and embodied AI.\nAs scenes evolve, efficient methods to incorporate changes are needed to\nmaintain up-to-date, high-quality reconstructions without the computational\noverhead of re-optimizing the entire scene. This paper introduces CL-Splats,\nwhich incrementally updates Gaussian splatting-based 3D representations from\nsparse scene captures. CL-Splats integrates a robust change-detection module\nthat segments updated and static components within the scene, enabling focused,\nlocal optimization that avoids unnecessary re-computation. Moreover, CL-Splats\nsupports storing and recovering previous scene states, facilitating temporal\nsegmentation and new scene-analysis applications. Our extensive experiments\ndemonstrate that CL-Splats achieves efficient updates with improved\nreconstruction quality over the state-of-the-art. This establishes a robust\nfoundation for future real-time adaptation in 3D scene reconstruction tasks.", "comment": "ICCV 2025, Project Page: https://cl-splats.github.io", "pdf_url": "http://arxiv.org/pdf/2506.21117v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21117v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CL-Splats：基于局部优化的持续高斯泼溅学习", "tldr": "CL-Splats 通过局部优化和变化检测，高效地更新高斯泼溅3D场景表示，实现了动态3D场景的持续重建。", "motivation": "在动态3D环境中，准确及时地更新场景表示对于机器人、混合现实和具身AI等应用至关重要。传统方法重新优化整个场景计算开销巨大，因此需要一种高效的方法来整合场景变化，以保持高质量的最新重建。", "method": "本文介绍了CL-Splats，它通过从稀疏场景捕获中增量更新基于高斯泼溅的3D表示。CL-Splats集成了一个鲁棒的变化检测模块，该模块能够分割场景中的更新和静态组件，从而实现集中的局部优化，避免不必要的重新计算。此外，CL-Splats还支持存储和恢复先前的场景状态，以支持时间分割和新的场景分析应用。", "result": "我们的大量实验表明，CL-Splats实现了高效的更新，并且重建质量优于现有技术。", "conclusion": "CL-Splats为未来3D场景重建任务中的实时适应性奠定了坚实的基础。", "translation": "在动态3D环境中，随着时间准确更新场景表示对于机器人、混合现实和具身AI中的应用至关重要。随着场景的演变，需要有效的方法来整合变化，以保持最新、高质量的重建，而无需重新优化整个场景的计算开销。本文介绍了CL-Splats，它从稀疏场景捕获中增量更新基于高斯泼溅的3D表示。CL-Splats集成了一个鲁棒的变化检测模块，该模块在场景中分割更新的和静态的组件，从而实现集中的局部优化，避免不必要的重新计算。此外，CL-Splats支持存储和恢复先前的场景状态，从而促进时间分割和新的场景分析应用。我们的大量实验表明，CL-Splats实现了高效更新，并且重建质量优于现有技术。这为未来3D场景重建任务中的实时适应性奠定了坚实的基础。", "summary": "CL-Splats是一种用于动态3D环境的持续学习方法，它通过增量更新高斯泼溅表示来高效地维护高质量的场景重建。该方法采用变化检测模块进行局部优化，并支持场景状态的存储与恢复，实验证明其在效率和重建质量上均优于现有技术，为实时3D场景重建提供了坚实基础。", "keywords": "持续学习, 高斯泼溅, 局部优化, 3D场景重建, 变化检测", "comments": "该论文提出了一种创新的持续学习方法CL-Splats，通过结合高斯泼溅和局部优化策略，解决了动态3D场景实时更新的挑战。其核心创新在于鲁棒的变化检测模块，这显著减少了计算开销。支持历史状态恢复的功能也为高级场景分析提供了潜力，使其在机器人、MR和具身AI等领域具有重要应用价值。"}}
{"id": "2506.21240", "title": "Zero-Shot Learning for Obsolescence Risk Forecasting", "authors": ["Elie Saad", "Aya Mrabah", "Mariem Besbes", "Marc Zolghadri", "Victor Czmil", "Claude Baron", "Vincent Bourgeois"], "summary": "Component obsolescence poses significant challenges in industries reliant on\nelectronic components, causing increased costs and disruptions in the security\nand availability of systems. Accurate obsolescence risk prediction is essential\nbut hindered by a lack of reliable data. This paper proposes a novel approach\nto forecasting obsolescence risk using zero-shot learning (ZSL) with large\nlanguage models (LLMs) to address data limitations by leveraging\ndomain-specific knowledge from tabular datasets. Applied to two real-world\ndatasets, the method demonstrates effective risk prediction. A comparative\nevaluation of four LLMs underscores the importance of selecting the right model\nfor specific forecasting tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21240v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21240v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "零样本学习在报废风险预测中的应用", "tldr": "本研究提出了一种利用零样本学习和大型语言模型来预测组件报废风险的新方法，有效解决了数据稀缺问题，并在真实世界数据集中展现了有效性。", "motivation": "电子元件的报废对相关行业构成重大挑战，导致成本增加并扰乱系统安全性和可用性。准确的报废风险预测至关重要，但受限于可靠数据的缺乏。", "method": "本文提出了一种新颖的方法，利用零样本学习（ZSL）和大型语言模型（LLMs）来预测报废风险，通过利用表格数据集中的领域特定知识来解决数据限制问题。", "result": "该方法应用于两个真实世界数据集，展示了有效的风险预测能力。对四种大型语言模型的比较评估强调了为特定预测任务选择合适模型的重要性。", "conclusion": "通过零样本学习和大型语言模型，可以有效预测组件报废风险，克服数据稀缺的挑战，且模型选择对预测性能至关重要。", "translation": "组件报废对依赖电子组件的行业构成了重大挑战，导致成本增加并扰乱系统的安全性和可用性。准确的报废风险预测至关重要，但受到可靠数据缺乏的阻碍。本文提出了一种新颖的方法，利用零样本学习（ZSL）和大型语言模型（LLMs）来预测报废风险，通过利用表格数据集中的领域特定知识来解决数据限制问题。该方法应用于两个真实世界数据集，展示了有效的风险预测。对四种大型语言模型的比较评估强调了为特定预测任务选择合适模型的重要性。", "summary": "本论文提出了一种新颖的零样本学习（ZSL）结合大型语言模型（LLMs）的方法，用于预测电子组件的报废风险。该方法通过利用表格数据集中的领域特定知识，有效解决了传统预测中数据稀缺的问题。在两个真实世界数据集上的应用证明了其有效的风险预测能力，同时研究也强调了为特定预测任务选择合适LLM的重要性。", "keywords": "零样本学习, 报废风险预测, 大型语言模型, 数据稀缺, 风险预测", "comments": "该论文的创新点在于将零样本学习和大型语言模型应用于报废风险预测这一传统上数据受限的领域。这种方法克服了数据稀缺的挑战，为工业界提供了一种实用的解决方案。通过比较不同LLM的性能，也为未来的应用提供了选型指导，具有重要的实践意义。"}}
{"id": "2506.21474", "title": "Logios : An open source Greek Polytonic Optical Character Recognition system", "authors": ["Perifanos Konstantinos", "Goutsos Dionisis"], "summary": "In this paper, we present an Optical Character Recognition (OCR) system\nspecifically designed for the accurate recognition and digitization of Greek\npolytonic texts. By leveraging the combined strengths of convolutional layers\nfor feature extraction and recurrent layers for sequence learning, our system\naddresses the unique challenges posed by Greek polytonic scripts. This approach\naims to overcome the limitations of traditional OCR methods, offering\nsignificant improvements in accuracy and efficiency. We release the underlying\nmodel as an open-source library and make our OCR platform available for\nacademic use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21474v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21474v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "Logios：一个开源的希腊语多音节光学字符识别系统", "tldr": "Logios是一个开源的希腊语多音节OCR系统，它结合了卷积层和循环层来提高识别精度和效率。", "motivation": "该论文旨在开发一个专门用于准确识别和数字化希腊语多音节文本的OCR系统，以克服传统OCR方法的局限性，并解决希腊语多音节文字带来的独特挑战。", "method": "该系统利用卷积层进行特征提取，并利用循环层进行序列学习，结合两者的优势来处理希腊语多音节文本。", "result": "该方法在准确性和效率方面显著优于传统OCR方法。该系统作为开源库发布，并可供学术使用。", "conclusion": "Logios系统通过结合深度学习技术，成功地为希腊语多音节文本提供了一个高精度、高效率的OCR解决方案，并以开源形式供社区使用。", "translation": "在本文中，我们提出了一个光学字符识别（OCR）系统，专门用于希腊语多音节文本的准确识别和数字化。通过利用卷积层进行特征提取和循环层进行序列学习的综合优势，我们的系统解决了希腊语多音节文字带来的独特挑战。这种方法旨在克服传统OCR方法的局限性，在准确性和效率方面提供了显著改进。我们以开源库的形式发布了底层模型，并使我们的OCR平台可供学术使用。", "summary": "Logios是一个专门为希腊语多音节文本设计的开源OCR系统。它结合了卷积层进行特征提取和循环层进行序列学习，有效解决了希腊语多音节文字的独特挑战，并在准确性和效率上超越了传统OCR方法。该系统及其底层模型已作为开源资源发布，供学术界使用。", "keywords": "OCR, 希腊语多音节, 深度学习, 开源, 字符识别", "comments": "该论文的创新之处在于其专门针对希腊语多音节文本的OCR系统设计，并结合了卷积层和循环层来优化性能。作为开源项目发布，它对学术研究和希腊语文本数字化领域具有重要意义。"}}
{"id": "2506.21132", "title": "Learning to See in the Extremely Dark", "authors": ["Hai Jiang", "Binhao Guan", "Zhen Liu", "Xiaohong Liu", "Jian Yu", "Zheng Liu", "Songchen Han", "Shuaicheng Liu"], "summary": "Learning-based methods have made promising advances in low-light RAW image\nenhancement, while their capability to extremely dark scenes where the\nenvironmental illuminance drops as low as 0.0001 lux remains to be explored due\nto the lack of corresponding datasets. To this end, we propose a\npaired-to-paired data synthesis pipeline capable of generating well-calibrated\nextremely low-light RAW images at three precise illuminance ranges of 0.01-0.1\nlux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB\nreferences to comprise a large-scale paired dataset named\nSee-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement\napproaches. Furthermore, we propose a diffusion-based framework that leverages\nthe generative ability and intrinsic denoising property of diffusion models to\nrestore visually pleasing results from extremely low-SNR RAW inputs, in which\nan Adaptive Illumination Correction Module (AICM) and a color consistency loss\nare introduced to ensure accurate exposure correction and color restoration.\nExtensive experiments on the proposed SIED and publicly available benchmarks\ndemonstrate the effectiveness of our method. The code and dataset are available\nat https://github.com/JianghaiSCU/SIED.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21132v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21132v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "学习在极暗环境中看清", "tldr": "提出一种数据合成流程和扩散模型框架，用于在极低照度下增强RAW图像，并创建了SIED数据集。", "motivation": "现有的基于学习的方法在极暗场景（照度低至0.0001 lux）下的RAW图像增强能力尚未被充分探索，因为缺乏相应的数据集。", "method": "1. 提出一个配对数据合成流程，生成三种精确照度范围（0.01-0.1 lux, 0.001-0.01 lux, 0.0001-0.001 lux）的极低照度RAW图像，并配有高质量sRGB参考，构建了大规模配对数据集SIED。 2. 提出一个基于扩散的框架，利用扩散模型的生成能力和内在去噪特性从极低信噪比RAW输入中恢复视觉上令人满意的结果，其中引入了自适应照度校正模块（AICM）和颜色一致性损失以确保准确的曝光校正和颜色恢复。", "result": "在所提出的SIED数据集和公开基准上进行了广泛实验，证明了所提方法的有效性。", "conclusion": "该研究通过构建专用数据集和提出创新的扩散模型框架，有效解决了极暗环境下RAW图像增强的挑战。", "translation": "基于学习的方法在低光RAW图像增强方面取得了可喜的进展，然而，由于缺乏相应的数据集，它们在环境照度低至0.0001 lux的极暗场景中的能力仍有待探索。为此，我们提出了一种配对数据合成流程，能够生成在0.01-0.1 lux、0.001-0.01 lux和0.0001-001 lux三种精确照度范围内的校准良好的极低照度RAW图像，并结合高质量sRGB参考，构成了一个名为See-in-the-Extremely-Dark (SIED) 的大规模配对数据集，用于评估低光RAW图像增强方法。此外，我们提出了一种基于扩散的框架，该框架利用扩散模型的生成能力和内在去噪特性，从极低信噪比的RAW输入中恢复出视觉上令人满意的结果，其中引入了自适应照度校正模块（AICM）和颜色一致性损失，以确保准确的曝光校正和颜色恢复。在所提出的SIED和公开可用基准上进行的广泛实验证明了我们方法的有效性。代码和数据集可在https://github.com/JianghaiSCU/SIED获取。", "summary": "本文针对极暗环境下RAW图像增强缺乏数据集的问题，提出了一个配对数据合成流程，构建了大规模SIED数据集。同时，提出了一种基于扩散的框架，结合自适应照度校正模块和颜色一致性损失，以有效增强极低信噪比RAW图像的曝光和色彩。实验证明了该方法的有效性。", "keywords": "极暗图像增强, RAW图像, 数据合成, 扩散模型, SIED数据集", "comments": "该论文的创新点在于构建了首个针对极暗环境（低至0.0001 lux）的RAW图像增强数据集SIED，解决了该领域数据稀缺的难题。同时，将扩散模型应用于极低信噪比图像增强，并引入了专门的模块和损失函数，有效地利用了扩散模型的去噪和生成能力，为极暗图像恢复提供了新的SOTA解决方案，具有重要的实际应用价值。"}}
{"id": "2506.21135", "title": "YOLO-FDA: Integrating Hierarchical Attention and Detail Enhancement for Surface Defect Detection", "authors": ["Jiawei Hu"], "summary": "Surface defect detection in industrial scenarios is both crucial and\ntechnically demanding due to the wide variability in defect types, irregular\nshapes and sizes, fine-grained requirements, and complex material textures.\nAlthough recent advances in AI-based detectors have improved performance,\nexisting methods often suffer from redundant features, limited detail\nsensitivity, and weak robustness under multiscale conditions. To address these\nchallenges, we propose YOLO-FDA, a novel YOLO-based detection framework that\nintegrates fine-grained detail enhancement and attention-guided feature fusion.\nSpecifically, we adopt a BiFPN-style architecture to strengthen bidirectional\nmultilevel feature aggregation within the YOLOv5 backbone. To better capture\nfine structural changes, we introduce a Detail-directional Fusion Module (DDFM)\nthat introduces a directional asymmetric convolution in the second-lowest layer\nto enrich spatial details and fuses the second-lowest layer with low-level\nfeatures to enhance semantic consistency. Furthermore, we propose two novel\nattention-based fusion strategies, Attention-weighted Concatenation (AC) and\nCross-layer Attention Fusion (CAF) to improve contextual representation and\nreduce feature noise. Extensive experiments on benchmark datasets demonstrate\nthat YOLO-FDA consistently outperforms existing state-of-the-art methods in\nterms of both accuracy and robustness across diverse types of defects and\nscales.", "comment": "14 pages, 6 figures. Submitted to The 8th Chinese Conference on\n  Pattern Recognition and Computer Vision", "pdf_url": "http://arxiv.org/pdf/2506.21135v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21135v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "YOLO-FDA：集成层次注意力与细节增强的表面缺陷检测", "tldr": "本文提出了YOLO-FDA，一个基于YOLO的新型检测框架，通过集成细粒度细节增强和注意力引导的特征融合，显著提升了工业场景下表面缺陷检测的准确性和鲁棒性，尤其解决了多尺度和细粒度缺陷的挑战。", "motivation": "工业场景中的表面缺陷检测面临巨大挑战，包括缺陷类型多样、形状尺寸不规则、细粒度要求高以及复杂材料纹理。现有基于AI的检测器存在特征冗余、细节敏感度不足以及多尺度条件下鲁棒性差的问题。", "method": "YOLO-FDA是一个基于YOLO的检测框架。它采用BiFPN风格的架构以加强YOLOv5骨干网络内的双向多级特征聚合。为捕获精细结构变化，引入了细节定向融合模块（DDFM），该模块在倒数第二层使用定向非对称卷积并与低级特征融合以增强空间细节和语义一致性。此外，提出了两种新颖的注意力融合策略：注意力加权拼接（AC）和跨层注意力融合（CAF），以改善上下文表示并减少特征噪声。", "result": "在基准数据集上进行的广泛实验表明，YOLO-FDA在各种缺陷类型和尺度上，无论是在准确性还是鲁棒性方面，都始终优于现有的最先进方法。", "conclusion": "YOLO-FDA成功解决了工业表面缺陷检测中存在的冗余特征、细节敏感度不足及多尺度鲁棒性差的问题，并在准确性和鲁棒性方面均超越了现有最先进方法，证明了其在实际应用中的有效性。", "translation": "工业场景中的表面缺陷检测既关键又具技术挑战性，原因在于缺陷类型多样、形状和尺寸不规则、对细粒度要求高以及材料纹理复杂。尽管近期基于AI的检测器取得了进展，但现有方法常受限于冗余特征、细节敏感度不足以及多尺度条件下的鲁棒性差。为解决这些挑战，我们提出了YOLO-FDA，一个新颖的、基于YOLO的检测框架，它集成了细粒度细节增强和注意力引导的特征融合。具体来说，我们采用了一种BiFPN风格的架构来加强YOLOv5骨干网络内的双向多级特征聚合。为了更好地捕获精细的结构变化，我们引入了一个细节定向融合模块（DDFM），它在倒数第二层引入了定向非对称卷积以丰富空间细节，并将倒数第二层与低级特征融合以增强语义一致性。此外，我们提出了两种新颖的基于注意力的融合策略：注意力加权拼接（AC）和跨层注意力融合（CAF），以改善上下文表示并减少特征噪声。在基准数据集上的大量实验表明，YOLO-FDA在各种缺陷类型和尺度上，无论是在准确性还是鲁棒性方面，都始终优于现有的最先进方法。", "summary": "YOLO-FDA是一个新颖的基于YOLO的框架，旨在解决工业表面缺陷检测中存在的冗余特征、细节敏感度不足及多尺度鲁棒性差的问题。该框架通过引入BiFPN风格的架构强化特征聚合，设计细节定向融合模块（DDFM）以增强空间细节和语义一致性，并提出注意力加权拼接（AC）和跨层注意力融合（CAF）两种注意力融合策略来优化上下文表示并减少噪声。实验证明，YOLO-FDA在准确性和鲁棒性方面均超越了现有SOTA方法。", "keywords": "表面缺陷检测, YOLO, 层次注意力, 细节增强, 特征融合", "comments": "该论文通过引入多个新颖组件（DDFM、AC、CAF）到一个基于YOLO的框架中，专门解决了工业表面缺陷检测中的常见挑战，如细粒度细节和多尺度变化。分层注意力与细节增强的结合是一种有效的方法，可以提高检测的敏感性和鲁棒性，是对该领域的重要贡献。"}}
{"id": "2506.21291", "title": "Improved seeding strategies for k-means and k-GMM", "authors": ["Guillaume Carrière", "Frédéric Cazals"], "summary": "We revisit the randomized seeding techniques for k-means clustering and k-GMM\n(Gaussian Mixture model fitting with Expectation-Maximization), formalizing\ntheir three key ingredients: the metric used for seed sampling, the number of\ncandidate seeds, and the metric used for seed selection. This analysis yields\nnovel families of initialization methods exploiting a lookahead\nprinciple--conditioning the seed selection to an enhanced coherence with the\nfinal metric used to assess the algorithm, and a multipass strategy to tame\ndown the effect of randomization.\n  Experiments show a consistent constant factor improvement over classical\ncontenders in terms of the final metric (SSE for k-means, log-likelihood for\nk-GMM), at a modest overhead. In particular, for k-means, our methods improve\non the recently designed multi-swap strategy, which was the first one to\noutperform the greedy k-means++ seeding.\n  Our experimental analysis also shed light on subtle properties of k-means\noften overlooked, including the (lack of) correlations between the SSE upon\nseeding and the final SSE, the variance reduction phenomena observed in\niterative seeding methods, and the sensitivity of the final SSE to the pool\nsize for greedy methods.\n  Practically, our most effective seeding methods are strong candidates to\nbecome one of the--if not the--standard techniques. From a theoretical\nperspective, our formalization of seeding opens the door to a new line of\nanalytical approaches.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.21291v1", "categories": ["cs.LG", "F.2; G.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21291v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "改进的k-means和k-GMM初始化策略", "tldr": "本文重新审视了k-means和k-GMM的随机初始化技术，并提出了利用前瞻性原则和多通道策略的新型初始化方法，实验表明这些方法在最终度量方面取得了显著改进，且开销适中，有望成为新的标准技术。", "motivation": "研究者重新审视了k-means聚类和k-GMM（高斯混合模型拟合）的随机初始化技术，旨在改进现有的初始化方法，以获得更好的聚类性能和模型拟合效果，并解决现有方法在初始化SSE与最终SSE之间缺乏相关性等问题。", "method": "本文形式化了随机初始化技术的三个关键要素：种子采样使用的度量、候选种子数量以及种子选择使用的度量。在此基础上，提出了利用“前瞻性原则”（根据评估算法的最终度量增强种子选择的一致性）和“多通道策略”（驯服随机化效应）的新型初始化方法。", "result": "实验结果表明，与经典方法相比，本文提出的方法在最终度量（k-means的SSE，k-GMM的对数似然）方面取得了持续的常数因子改进，且开销适中。特别是对于k-means，本文方法优于最近设计的多交换策略，后者是第一个优于贪婪k-means++初始化的方法。实验分析还揭示了k-means的一些细微特性，如初始化SSE与最终SSE之间的（缺乏）相关性、迭代初始化方法中观察到的方差减少现象以及最终SSE对贪婪方法池大小的敏感性。", "conclusion": "本文提出的最有效的初始化方法有望成为标准技术。从理论角度看，对初始化过程的形式化为新的分析方法打开了大门。", "translation": "我们重新审视了k-means聚类和k-GMM（使用期望最大化进行高斯混合模型拟合）的随机初始化技术，并将其三个关键要素形式化：用于种子采样的度量、候选种子的数量以及用于种子选择的度量。这项分析产生了利用前瞻性原则（使种子选择与用于评估算法的最终度量保持更高的一致性）和多通道策略（驯服随机化效应）的新型初始化方法。实验表明，在最终度量（k-means的SSE，k-GMM的对数似然）方面，与经典竞争者相比，本文方法以适中的开销实现了持续的常数因子改进。特别是对于k-means，我们的方法改进了最近设计的多交换策略，这是第一个优于贪婪k-means++初始化的方法。我们的实验分析还揭示了k-means的一些常被忽视的细微特性，包括初始化时的SSE与最终SSE之间的（缺乏）相关性、迭代初始化方法中观察到的方差减少现象，以及最终SSE对贪婪方法池大小的敏感性。实际上，我们最有效的初始化方法是成为标准技术（如果不是唯一）的有力候选者。从理论角度来看，我们对初始化的形式化为新的分析方法打开了大门。", "summary": "本文重新审视了k-means和k-GMM的随机初始化技术，并对其三个关键要素进行了形式化。在此基础上，提出了基于“前瞻性原则”和“多通道策略”的新型初始化方法。实验证明，这些新方法在最终度量上比现有方法有显著且持续的改进，尤其在k-means上超越了k-means++和多交换策略，且计算开销适中。研究还揭示了k-means初始化的一些内在特性。本文提出的方法有望成为新的标准初始化技术，其理论形式化也为未来研究奠定了基础。", "keywords": "k-means, k-GMM, 初始化策略, 随机种子, 聚类", "comments": "本文的创新之处在于对k-means和k-GMM的随机初始化技术进行了深入的形式化分析，并在此基础上提出了结合“前瞻性原则”和“多通道策略”的新型初始化方法。其重要性在于实验证明这些方法在性能上显著优于现有SOTA方法，且开销可控，具有很强的实用价值和潜在的行业标准影响力。此外，对k-means内在特性的揭示也具有重要的理论意义。"}}
{"id": "2506.21546", "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation", "authors": ["Xinzhuo Li", "Adheesh Juvekar", "Xingyou Liu", "Muntasir Wahed", "Kiet A. Nguyen", "Ismini Lourentzou"], "summary": "Recent progress in vision-language segmentation has significantly advanced\ngrounded visual understanding. However, these models often exhibit\nhallucinations by producing segmentation masks for objects not grounded in the\nimage content or by incorrectly labeling irrelevant regions. Existing\nevaluation protocols for segmentation hallucination primarily focus on label or\ntextual hallucinations without manipulating the visual context, limiting their\ncapacity to diagnose critical failures. In response, we introduce\nHalluSegBench, the first benchmark specifically designed to evaluate\nhallucinations in visual grounding through the lens of counterfactual visual\nreasoning. Our benchmark consists of a novel dataset of 1340 counterfactual\ninstance pairs spanning 281 unique object classes, and a set of newly\nintroduced metrics that quantify hallucination sensitivity under visually\ncoherent scene edits. Experiments on HalluSegBench with state-of-the-art\nvision-language segmentation models reveal that vision-driven hallucinations\nare significantly more prevalent than label-driven ones, with models often\npersisting in false segmentation, highlighting the need for counterfactual\nreasoning to diagnose grounding fidelity.", "comment": "Project webpage: https://plan-lab.github.io/hallusegbench/", "pdf_url": "http://arxiv.org/pdf/2506.21546v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21546v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HalluSegBench：用于分割幻觉评估的反事实视觉推理", "tldr": "引入了HalluSegBench，这是一个新的基准，用于通过反事实视觉推理评估视觉语言分割模型中的幻觉，发现视觉驱动的幻觉比标签驱动的更普遍。", "motivation": "现有的视觉语言分割模型存在幻觉问题（生成图像中不存在对象的分割掩码或错误标记不相关区域）。目前的评估协议主要关注标签或文本幻觉，但未操纵视觉上下文，限制了诊断关键故障的能力。因此，需要一个专门设计用于通过反事实视觉推理评估视觉接地幻觉的基准。", "method": "引入了HalluSegBench，这是第一个专门用于通过反事实视觉推理评估视觉接地幻觉的基准。它包含一个包含1340个反事实实例对（涵盖281个独特对象类别）的新数据集，以及一套新的指标，用于量化在视觉连贯场景编辑下的幻觉敏感性。", "result": "对最先进的视觉语言分割模型在HalluSegBench上的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型通常会持续进行错误的分割。", "conclusion": "需要反事实推理来诊断视觉接地保真度，因为视觉驱动的幻觉比标签驱动的更普遍，且模型倾向于持续错误的分割。", "translation": "近期在视觉语言分割方面的进展显著推动了基础视觉理解。然而，这些模型经常通过为图像内容中没有基础的对象生成分割掩码或错误标记不相关区域来表现出幻觉。现有的分割幻觉评估协议主要关注标签或文本幻觉，而没有操纵视觉上下文，这限制了它们诊断关键故障的能力。因此，我们引入了HalluSegBench，这是第一个专门设计用于通过反事实视觉推理评估视觉接地幻觉的基准。我们的基准包含一个由1340个反事实实例对组成的新数据集，涵盖281个独特的对象类别，以及一套新引入的指标，用于量化在视觉连贯场景编辑下的幻觉敏感性。在HalluSegBench上对最先进的视觉语言分割模型进行的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型通常会持续进行错误的分割，这突出表明需要反事实推理来诊断接地保真度。", "summary": "本文介绍了HalluSegBench，一个用于评估视觉语言分割模型中幻觉的新基准。该基准利用反事实视觉推理，包含一个新数据集和量化幻觉敏感性的指标。实验发现，视觉驱动的幻觉比标签驱动的更普遍，强调了通过反事实推理诊断模型接地保真度的重要性。", "keywords": "分割幻觉, 反事实视觉推理, 视觉语言分割, 基准测试, 视觉接地", "comments": "这篇论文的创新点在于首次提出了一个专门通过反事实视觉推理来评估视觉接地幻觉的基准。通过操纵视觉上下文而非仅仅依赖标签或文本，HalluSegBench能够更深层次地诊断视觉语言分割模型的关键故障。其发现视觉驱动幻觉的普遍性，为未来模型改进指明了方向，对于提升模型的鲁棒性和可靠性具有重要意义。"}}
{"id": "2506.21150", "title": "Tree-based Semantic Losses: Application to Sparsely-supervised Large Multi-class Hyperspectral Segmentation", "authors": ["Junwen Wang", "Oscar Maccormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "summary": "Hyperspectral imaging (HSI) shows great promise for surgical applications,\noffering detailed insights into biological tissue differences beyond what the\nnaked eye can perceive. Refined labelling efforts are underway to train vision\nsystems to distinguish large numbers of subtly varying classes. However,\ncommonly used learning methods for biomedical segmentation tasks penalise all\nerrors equivalently and thus fail to exploit any inter-class semantics in the\nlabel space. In this work, we introduce two tree-based semantic loss functions\nwhich take advantage of a hierarchical organisation of the labels. We further\nincorporate our losses in a recently proposed approach for training with\nsparse, background-free annotations. Extensive experiments demonstrate that our\nproposed method reaches state-of-the-art performance on a sparsely annotated\nHSI dataset comprising $107$ classes organised in a clinically-defined semantic\ntree structure. Furthermore, our method enables effective detection of\nout-of-distribution (OOD) pixels without compromising segmentation performance\non in-distribution (ID) pixels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21150v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21150v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "树状语义损失：在稀疏监督大规模多类别高光谱分割中的应用", "tldr": "本文引入了两种基于树的语义损失函数，利用标签的层次结构，在稀疏标注的高光谱图像分割任务中实现了最先进的性能，并能有效检测分布外（OOD）像素。", "motivation": "常见的生物医学分割方法对所有错误同等惩罚，未能利用标签空间中的类间语义信息，尤其是在处理大量细微差异的类别时。", "method": "提出了两种基于树的语义损失函数，利用标签的层次组织结构。这些损失函数被整合到一种最近提出的稀疏、无背景标注的训练方法中。", "result": "在包含107个类别（按临床定义的语义树结构组织）的稀疏标注高光谱图像数据集上，所提出的方法达到了最先进的性能。此外，该方法能够在不影响分布内像素分割性能的情况下，有效检测分布外（OOD）像素。", "conclusion": "基于树的语义损失函数通过利用标签的层次结构，显著提升了稀疏监督大规模多类别高光谱图像分割的性能，并增强了对OOD像素的检测能力。", "translation": "高光谱成像（HSI）在外科应用中显示出巨大潜力，它能提供肉眼无法察觉的生物组织差异的详细信息。目前正在进行精细的标注工作，以训练视觉系统区分大量细微变化的类别。然而，生物医学分割任务中常用的学习方法对所有错误都进行等效惩罚，因此未能利用标签空间中的任何类间语义。在这项工作中，我们引入了两种基于树的语义损失函数，它们利用了标签的层次组织结构。我们进一步将我们的损失函数整合到一种最近提出的使用稀疏、无背景标注进行训练的方法中。广泛的实验表明，我们提出的方法在一个包含107个类别（这些类别组织在一个临床定义的语义树结构中）的稀疏标注HSI数据集上达到了最先进的性能。此外，我们的方法能够在不影响分布内（ID）像素分割性能的情况下，有效检测分布外（OOD）像素。", "summary": "本文针对生物医学高光谱图像分割中现有方法未能利用类间语义信息的问题，提出了两种基于树的语义损失函数。这些损失函数利用标签的层次结构，并结合稀疏标注训练方法，在一个包含107个类别的稀疏标注高光谱数据集上实现了最先进的分割性能，并能有效检测分布外像素。", "keywords": "树状语义损失, 高光谱分割, 稀疏监督, 多类别, 分布外检测", "comments": "这项工作通过引入基于树的语义损失函数，巧妙地解决了高光谱图像分割中类间语义信息未被利用的问题，尤其是在处理大量细微差异的类别时。其创新性在于将标签的层次结构融入损失函数，这不仅提高了分割精度，还实现了对分布外像素的有效检测，这在实际医疗应用中具有重要意义，因为它能帮助识别异常区域。该方法在稀疏标注数据集上的表现也突出了其在标注成本高昂领域中的实用性。"}}
{"id": "2506.21151", "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels", "authors": ["Aida Moafi", "Danial Moafi", "Evgeny M. Mirkes", "Gerry P. McCann", "Abbas S. Alatrany", "Jayanth R. Arnold", "Mostafa Mehdipour Ghazi"], "summary": "The accurate segmentation of myocardial scars from cardiac MRI is essential\nfor clinical assessment and treatment planning. In this study, we propose a\nrobust deep-learning pipeline for fully automated myocardial scar detection and\nsegmentation by fine-tuning state-of-the-art models. The method explicitly\naddresses challenges of label noise from semi-automatic annotations, data\nheterogeneity, and class imbalance through the use of Kullback-Leibler loss and\nextensive data augmentation. We evaluate the model's performance on both acute\nand chronic cases and demonstrate its ability to produce accurate and smooth\nsegmentations despite noisy labels. In particular, our approach outperforms\nstate-of-the-art models like nnU-Net and shows strong generalizability in an\nout-of-distribution test set, highlighting its robustness across various\nimaging conditions and clinical tasks. These results establish a reliable\nfoundation for automated myocardial scar quantification and support the broader\nclinical adoption of deep learning in cardiac imaging.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21151v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21151v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于心脏MRI心肌瘢痕分割的鲁棒深度学习，带有噪声标签", "tldr": "本研究提出了一种鲁棒的深度学习方法，用于心脏MRI中的心肌瘢痕分割，有效解决了噪声标签问题，并优于现有最先进模型。", "motivation": "从心脏MRI中准确分割心肌瘢痕对于临床评估和治疗计划至关重要。", "method": "通过微调最先进的模型，提出了一种鲁棒的深度学习流程，用于全自动心肌瘢痕检测和分割。该方法通过使用Kullback-Leibler损失和广泛的数据增强，明确解决了半自动标注带来的标签噪声、数据异质性和类别不平衡等挑战。", "result": "该模型在存在噪声标签的情况下也能生成准确平滑的分割结果，并在急性和慢性病例中表现出色。它优于nnU-Net等最先进的模型，并在分布外测试集中显示出强大的泛化能力，突出了其在各种成像条件和临床任务中的鲁棒性。", "conclusion": "这些结果为自动化心肌瘢痕量化奠定了可靠基础，并支持深度学习在心脏成像中更广泛的临床应用。", "translation": "从心脏MRI中准确分割心肌瘢痕对于临床评估和治疗计划至关重要。在本研究中，我们通过微调最先进的模型，提出了一种鲁棒的深度学习流程，用于全自动心肌瘢痕检测和分割。该方法通过使用Kullback-Leibler损失和广泛的数据增强，明确解决了半自动标注、数据异质性和类别不平衡带来的标签噪声挑战。我们在急性和慢性病例中评估了模型的性能，并证明了其在存在噪声标签的情况下也能生成准确平滑分割的能力。特别是，我们的方法优于nnU-Net等最先进的模型，并在分布外测试集中显示出强大的泛化能力，突出了其在各种成像条件和临床任务中的鲁棒性。这些结果为自动化心肌瘢痕量化奠定了可靠基础，并支持深度学习在心脏成像中更广泛的临床应用。", "summary": "本文提出了一种用于心脏MRI中心肌瘢痕自动分割的鲁棒深度学习流程。该方法通过采用Kullback-Leibler损失和大量数据增强，有效解决了标签噪声、数据异质性和类别不平衡等挑战。该方法在性能和泛化能力上均优于现有最先进模型，为深度学习在心脏成像领域的更广泛临床应用奠定了可靠基础。", "keywords": "心肌瘢痕分割, 深度学习, 心脏MRI, 噪声标签, 鲁棒性", "comments": "该论文通过为心肌瘢痕分割提供一个鲁棒的解决方案，解决了关键的临床需求。其创新之处在于有效处理噪声标签并实现强大的泛化能力，这在现实世界的医学影像应用中是重要的挑战。明确提到超越nnU-Net表明其在现有基准上取得了显著改进。"}}
{"id": "2506.21152", "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image", "authors": ["Pufan Li", "Bi'an Du", "Wei Hu"], "summary": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To takle these issues, we present a novel method that seamlessly\nintegrates geometry and perception priors without requiring additional model\ntraining to reconstruct detailed 3D objects from a single image. Specifically,\nwe train three different Gaussian branches initialized from the geometry prior,\nperception prior and Gaussian noise, respectively. The geometry prior captures\nthe rough 3D shapes, while the perception prior utilizes the 2D pretrained\ndiffusion model to enhance multiview information. Subsequently, we refine 3D\nGaussian branches through mutual interaction between geometry and perception\npriors, further enhanced by a reprojection-based strategy that enforces depth\nconsistency. Experiments demonstrate the higher-fidelity reconstruction results\nof our method, outperforming existing methods on novel view synthesis and 3D\nreconstruction, demonstrating robust and consistent 3D object generation.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.21152v1", "categories": ["cs.CV", "68", "I.4.0"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21152v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "几何与感知引导的高斯函数，用于单幅图像的多视角一致性三维生成", "tldr": "本文提出一种新方法，通过结合几何和感知先验来从单幅图像生成多视角一致且细节丰富的三维物体，无需额外训练，优于现有方法。", "motivation": "现有从单视角图像生成真实三维对象的方法，如微调2D扩散模型或直接生成3D信息，通常存在多视角一致性差和几何细节不足的问题。", "method": "提出一种新方法，无需额外模型训练，无缝整合几何和感知先验来重建详细三维物体。具体来说，训练三个高斯分支，分别从几何先验、感知先验和高斯噪声初始化。几何先验捕获粗略3D形状，感知先验利用2D预训练扩散模型增强多视角信息。通过几何和感知先验之间的相互作用，并辅以基于重投影的深度一致性策略，进一步细化3D高斯分支。", "result": "实验证明该方法实现了更高保真度的重建结果，在新视角合成和3D重建方面优于现有方法，展示了鲁棒且一致的3D对象生成。", "conclusion": "该方法通过结合几何和感知先验，有效解决了单视角3D生成中多视角一致性和几何细节不足的问题，实现了高质量的3D重建。", "translation": "从单视角图像生成真实三维物体需要自然的外观、三维一致性以及捕捉未见区域多种合理解释的能力。现有方法通常依赖于微调预训练的二维扩散模型或通过快速网络推理或三维高斯溅射直接生成三维信息，但其结果普遍存在多视角一致性差和几何细节不足的问题。为了解决这些问题，我们提出了一种新颖的方法，无需额外的模型训练即可无缝整合几何和感知先验，从单幅图像重建详细的三维物体。具体来说，我们训练了三个不同高斯分支，分别从几何先验、感知先验和高斯噪声初始化。几何先验捕获粗略的三维形状，而感知先验则利用二维预训练扩散模型来增强多视角信息。随后，我们通过几何和感知先验之间的相互作用来细化三维高斯分支，并通过基于重投影的策略进一步增强，该策略强制执行深度一致性。实验表明，我们的方法重建结果具有更高的保真度，在新视角合成和三维重建方面优于现有方法，展示了鲁棒且一致的三维物体生成。", "summary": "本文提出一种新颖的方法，通过将几何和感知先验无缝整合到高斯溅射框架中，从单幅图像生成多视角一致且细节丰富的三维物体。该方法通过初始化三个高斯分支（基于几何先验、感知先验和噪声），并利用几何先验捕获粗略形状、感知先验增强多视角信息，以及通过相互作用和重投影策略进行细化。实验证明，该方法在3D重建和新视角合成方面优于现有技术，解决了单视图3D生成中常见的一致性和细节问题。", "keywords": "单视角3D生成, 高斯溅射, 几何先验, 感知先验, 多视角一致性", "comments": "本文的创新之处在于无需额外模型训练即可有效结合几何和感知先验来提升单视图3D生成质量。通过引入多个高斯分支和相互作用机制，巧妙地解决了现有方法在多视角一致性和几何细节方面的不足，为单视图3D重建提供了一个高效且高质量的解决方案。"}}
{"id": "2506.21343", "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language Models", "authors": ["Jingyao Li", "Hao Sun", "Zile Qiao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Hong Xu", "Jiaya Jia"], "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21343v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21343v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DynamicBench：评估大型语言模型中的实时报告生成", "tldr": "DynamicBench是一个新的基准测试，用于评估大型语言模型在实时数据处理和报告生成方面的能力，通过结合网络搜索和本地数据库，并在无文档和有文档场景下表现出最先进的性能。", "motivation": "传统的大型语言模型（LLMs）基准测试依赖静态评估，无法捕捉当代应用中实时信息处理的动态需求。为了解决这一限制，本文提出了DynamicBench。", "method": "DynamicBench采用双路径检索管道，整合网络搜索和本地报告数据库。它需要领域特定知识，以确保在专业领域内生成准确的报告。通过在提供或不提供外部文档的场景中评估模型，DynamicBench衡量模型独立处理最新信息或利用上下文增强的能力。此外，论文还引入了一个先进的报告生成系统，擅长管理动态信息合成。", "result": "实验结果证实了该方法的有效性，其方法在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%，达到了最先进的性能。", "conclusion": "DynamicBench成功地解决了传统LLM基准测试在评估实时信息处理方面的不足，并展示了在动态报告生成方面超越现有最先进模型的强大能力。", "translation": "大型语言模型（LLMs）的传统基准测试通常依赖于通过讲故事或表达意见进行的静态评估，这未能捕捉到当代应用中实时信息处理的动态需求。为了解决这一限制，我们提出了DynamicBench，一个旨在评估LLMs存储和处理最新数据能力的基准测试。DynamicBench利用双路径检索管道，整合了网络搜索和本地报告数据库。它需要领域特定知识，以确保在专业领域内生成准确的报告。通过在提供或不提供外部文档的场景中评估模型，DynamicBench有效地衡量了它们独立处理最新信息或利用上下文增强的能力。此外，我们引入了一个先进的报告生成系统，擅长管理动态信息合成。我们的实验结果证实了我们方法的有效性，我们的方法取得了最先进的性能，在无文档和文档辅助场景中分别超越GPT4o 7.0%和5.8%。代码和数据将公开发布。", "summary": "DynamicBench是一个为评估大型语言模型（LLMs）实时报告生成能力而设计的基准测试。它通过结合网络搜索和本地报告数据库的双路径检索管道，克服了传统静态评估的局限性。DynamicBench在提供或不提供外部文档的场景下评估LLMs处理最新信息的能力，并引入了一个先进的动态信息合成报告生成系统。实验结果表明，该方法在这些场景下均达到了最先进的性能，超越了GPT4o。", "keywords": "大型语言模型, 实时报告生成, 基准测试, 动态信息处理, DynamicBench", "comments": "本文提出的DynamicBench创新性地解决了LLM评估中实时动态信息处理的空白，提供了一个更贴近实际应用场景的评估框架。其双路径检索和有无文档的评估设计，能更全面地衡量LLM在动态数据处理和报告生成方面的能力。超越GPT4o的性能表明了该方法的有效性和重要性。"}}
{"id": "2506.21165", "title": "Topology-Aware Modeling for Unsupervised Simulation-to-Reality Point Cloud Recognition", "authors": ["Longkun Zou", "Kangjun Liu", "Ke Chen", "Kailing Guo", "Kui Jia", "Yaowei Wang"], "summary": "Learning semantic representations from point sets of 3D object shapes is\noften challenged by significant geometric variations, primarily due to\ndifferences in data acquisition methods. Typically, training data is generated\nusing point simulators, while testing data is collected with distinct 3D\nsensors, leading to a simulation-to-reality (Sim2Real) domain gap that limits\nthe generalization ability of point classifiers. Current unsupervised domain\nadaptation (UDA) techniques struggle with this gap, as they often lack robust,\ndomain-insensitive descriptors capable of capturing global topological\ninformation, resulting in overfitting to the limited semantic patterns of the\nsource domain. To address this issue, we introduce a novel Topology-Aware\nModeling (TAM) framework for Sim2Real UDA on object point clouds. Our approach\nmitigates the domain gap by leveraging global spatial topology, characterized\nby low-level, high-frequency 3D structures, and by modeling the topological\nrelations of local geometric features through a novel self-supervised learning\ntask. Additionally, we propose an advanced self-training strategy that combines\ncross-domain contrastive learning with self-training, effectively reducing the\nimpact of noisy pseudo-labels and enhancing the robustness of the adaptation\nprocess. Experimental results on three public Sim2Real benchmarks validate the\neffectiveness of our TAM framework, showing consistent improvements over\nstate-of-the-art methods across all evaluated tasks. The source code of this\nwork will be available at https://github.com/zou-longkun/TAG.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21165v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21165v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "无监督仿真到现实点云识别的拓扑感知建模", "tldr": "提出了一个名为TAM的拓扑感知建模框架，用于解决点云识别中仿真到现实的域鸿沟问题，通过利用全局空间拓扑和自监督学习来提升泛化能力，并在多个基准测试中优于现有方法。", "motivation": "3D对象点云的语义表示学习面临显著的几何变异，主要是由于数据采集方法（模拟器与真实传感器）不同导致的仿真到现实（Sim2Real）域鸿沟，这限制了点云分类器的泛化能力。现有无监督域适应（UDA）技术因缺乏鲁棒、域不敏感的拓扑描述符而难以有效弥合此鸿沟，导致对源域有限语义模式的过拟合。", "method": "提出了一种新颖的拓扑感知建模（TAM）框架，用于对象点云的Sim2Real无监督域适应。该方法通过利用由低级、高频3D结构表征的全局空间拓扑来弥合域鸿沟，并通过新颖的自监督学习任务建模局部几何特征的拓扑关系。此外，还提出了一种先进的自训练策略，将跨域对比学习与自训练相结合，以有效减少噪声伪标签的影响并增强适应过程的鲁棒性。", "result": "在三个公共Sim2Real基准测试上的实验结果验证了TAM框架的有效性，显示在所有评估任务中均持续优于现有最先进的方法。", "conclusion": "TAM框架通过有效利用全局空间拓扑和建模局部几何特征的拓扑关系，并结合先进的自训练策略，成功弥合了仿真到现实的点云识别域鸿沟，显著提升了模型的泛化能力和性能。", "translation": "学习3D物体形状点集的语义表示常常受到显著几何变化的挑战，这主要源于数据采集方法的差异。通常，训练数据使用点模拟器生成，而测试数据则通过不同的3D传感器收集，这导致了仿真到现实（Sim2Real）的领域鸿沟，限制了点分类器的泛化能力。当前的无监督域适应（UDA）技术难以应对这一鸿沟，因为它们通常缺乏能够捕获全局拓扑信息的鲁棒、域不敏感的描述符，从而导致对源域有限语义模式的过拟合。为了解决这个问题，我们引入了一种新颖的拓扑感知建模（TAM）框架，用于对象点云的Sim2Real UDA。我们的方法通过利用由低级、高频3D结构表征的全局空间拓扑，并通过新颖的自监督学习任务建模局部几何特征的拓扑关系来减轻领域鸿沟。此外，我们提出了一种先进的自训练策略，将跨域对比学习与自训练相结合，有效减少了噪声伪标签的影响，并增强了适应过程的鲁棒性。在三个公共Sim2Real基准测试上的实验结果验证了我们TAM框架的有效性，显示在所有评估任务中均持续优于现有最先进的方法。本工作的源代码将可在https://github.com/zou-longkun/TAG.git获取。", "summary": "本文针对3D点云识别中存在的仿真到现实（Sim2Real）域鸿沟问题，提出了一种名为拓扑感知建模（TAM）的新颖框架。该框架通过利用全局空间拓扑信息和建模局部几何特征的拓扑关系来减轻域差异，并通过结合跨域对比学习的自训练策略来提高适应过程的鲁棒性。实验证明，TAM框架在多个Sim2Real基准测试上均优于现有方法。", "keywords": "拓扑感知建模, Sim2Real, 点云识别, 无监督域适应, 拓扑关系", "comments": "这篇论文通过引入拓扑感知建模，创新性地解决了点云Sim2Real域适应中的关键挑战，即缺乏鲁棒的拓扑描述符。其结合全局拓扑、局部拓扑关系建模以及先进自训练策略的方法是其亮点，有望在点云泛化能力方面带来显著提升。"}}
{"id": "2506.21184", "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding", "authors": ["Minghao Qin", "Yan Shu", "Peitian Zhang", "Kun Lun", "Huaying Yuan", "Juenjie Zhou", "Shitao Xiao", "Bo Zhao", "Zheng Liu"], "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.", "comment": "14 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.21184v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21184v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "面向成本效益型长视频理解的任务感知KV压缩", "tldr": "Video-X^2L 提出了一种任务感知的双层KV压缩和选择性KV重载方法，以在保持计算效率的同时，有效解决长视频理解中信息丢失的问题。", "motivation": "现有的大型多模态语言模型（MLLMs）在长视频理解（LVU）方面面临严峻挑战，主要原因是计算成本过高。尽管最近的KV压缩方法试图缓解此问题，但在高压缩比下常导致显著信息损失。", "method": "本文引入了Video-X^2L，它能灵活地为每个长视频理解任务保留关键视频信息。该方法包含两个关键操作：1. 双层KV压缩：在MLLM的预填充阶段，生成低压缩KVs（L-KVs）捕获细粒度细节，以及高压缩KVs（H-KVs）提供紧凑表示。2. 选择性KV重载：在MLLM的解码阶段，对最关键的视频块选择性地重载L-KVs，而对不那么重要的部分使用H-KVs。该方法无需额外训练，且与现有KV可压缩的MLLMs兼容。", "result": "Video-X^2L 在VideoMME、MLVU、LongVideoBench和VNBench等多个流行长视频理解基准测试中进行了评估。实验结果表明，Video-X^2L 在性能上显著优于现有KV压缩方法，同时大幅节省了计算成本。", "conclusion": "Video-X^2L 通过其创新的双层KV压缩和选择性重载机制，有效解决了长视频理解中的计算成本和信息丢失问题，实现了卓越的性能提升和计算效率，且无需额外训练，证明了其简单而有效性。", "translation": "长视频理解（LVU）对现有的大型多模态语言模型（MLLMs）来说仍然是一个严峻的挑战，这主要归因于高昂的计算成本。最近的方法探索了KV压缩来缓解这个问题，但它们在高压缩比下常常遭受显著的信息损失。在本文中，我们引入了Video-X^2L，它能灵活地为每个LVU任务保留关键的视频信息。Video-X^2L 包含两个关键操作。第一个操作称为双层KV压缩。在MLLM的预填充阶段，Video-X^2L 生成两种类型的压缩KVs：低压缩KVs（L-KVs）用于捕获细粒度的视频细节，以及高压缩KVs（H-KVs）用于提供紧凑的视频表示。第二个操作称为选择性KV重载。在MLLM的解码阶段，Video-X^2L 对最关键的视频块选择性地重载L-KVs，同时对其他不那么重要的部分使用H-KVs。这使得MLLM能够充分利用特定任务的信息，同时保持整体的紧凑性。Video-X^2L 既简单又有效：它无需额外训练，并直接兼容现有的KV可压缩MLLMs。我们使用各种流行的LVU基准测试（包括VideoMME、MLVU、LongVideoBench和VNBench）评估了Video-X^2L。我们的实验结果表明，Video-X^2L 在性能上以巨大优势超越了现有KV压缩方法，同时大幅节省了计算成本。", "summary": "本文提出Video-X^2L，一种面向长视频理解（LVU）的高效KV压缩方法，旨在解决现有方法在计算成本和信息损失方面的挑战。Video-X^2L 引入了双层KV压缩，在预填充阶段生成低压缩（L-KVs）和高压缩（H-KVs）两种表示；并在解码阶段采用选择性KV重载，根据重要性动态选择使用L-KVs或H-KVs。该方法无需额外训练，兼容现有模型，并在多个LVU基准测试中显著优于现有KV压缩方法，同时大幅降低了计算成本。", "keywords": "长视频理解, KV压缩, 多模态大语言模型, 计算效率, 任务感知", "comments": "Video-X^2L 的创新之处在于其任务感知的双层KV压缩和选择性重载机制，有效平衡了信息保留与计算效率。其无需额外训练的特性使其具有很高的实用性和兼容性，能够直接应用于现有模型，这一点非常重要。该方法为解决长视频理解中的核心挑战提供了一个简洁而强大的解决方案。"}}
{"id": "2506.21355", "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning", "authors": ["Melanie Rieff", "Maya Varma", "Ossian Rabow", "Subathra Adithan", "Julie Kim", "Ken Chang", "Hannah Lee", "Nidhi Rohatgi", "Christian Bluethgen", "Mohamed S. Muneer", "Jean-Benoit Delbrouck", "Michael Moor"], "summary": "Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, example ordering exhibits a recency bias,\ni.e., placing the most relevant example last can lead to substantial\nperformance improvements by up to 71%. Our findings highlight critical\nlimitations and biases in current MLLMs when learning multimodal medical tasks\nfrom context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21355v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21355v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SMMILE: 一个专家驱动的多模态医学上下文学习基准", "tldr": "尽管医学领域潜力巨大，多模态上下文学习（ICL）仍未得到充分探索。本文介绍了SMMILE，一个专家驱动的医学多模态ICL基准，并发现当前的多模态大型语言模型（MLLMs）在医学任务中ICL能力较差，易受不相关示例影响，并存在近因偏差。", "motivation": "尽管多模态上下文学习（ICL）在医学等领域具有巨大潜力，但其仍未得到充分探索。临床医生日常工作中需要从有限示例中适应各种专业任务。尽管多模态大型语言模型（MLLMs）在医学视觉问答（VQA）方面取得了进展，但它们从上下文中学习多模态任务的能力仍是未知。", "method": "本文引入了SMMILE，这是第一个用于医学任务的专家驱动的多模态ICL基准。11位医学专家策划了111个问题（517个问题-图像-答案三元组），涵盖6个医学专业和13种成像模式。此外，还引入了SMMILE++，一个包含1038个排列问题的增强变体。对15个MLLMs进行了全面评估，并进行了开放式评估。", "result": "大多数MLLMs在医学任务中表现出中度到差的多模态ICL能力。ICL在SMMILE上比零样本平均仅提高8%，在SMMILE++上提高9.4%。不相关的上下文示例可使性能下降高达9.5%。示例排序表现出近因偏差，将最相关的示例放在最后可以使性能大幅提高高达71%。", "conclusion": "当前的多模态大型语言模型在从上下文中学习多模态医学任务时，存在关键的局限性和偏差。", "translation": "尽管在医学等领域具有巨大潜力，多模态上下文学习（ICL）仍未得到充分探索。临床医生经常遇到需要从有限示例中进行适应的各种专业任务，例如从少量相关过往病例中获取见解或考虑一组有限的鉴别诊断。虽然多模态大型语言模型（MLLMs）在医学视觉问答（VQA）方面取得了进展，但它们从上下文中学习多模态任务的能力在很大程度上是未知的。我们引入了SMMILE，这是第一个用于医学任务的专家驱动的多模态ICL基准。11位医学专家策划了问题，每个问题都包含多模态查询和多模态上下文示例作为任务演示。SMMILE包含111个问题（517个问题-图像-答案三元组），涵盖6个医学专业和13种成像模式。我们进一步引入了SMMILE++，一个包含1038个排列问题的增强变体。对15个MLLMs的全面评估表明，大多数模型在医学任务中表现出中度到差的多模态ICL能力。在开放式评估中，ICL在SMMILE上比零样本平均仅提高8%，在SMMILE++上提高9.4%。我们观察到对不相关上下文示例的敏感性：即使一个噪声或不相关的示例也会使性能下降高达9.5%。此外，示例排序表现出近因偏差，即把最相关的示例放在最后可以使性能大幅提高高达71%。我们的发现突出了当前MLLMs在从上下文中学习多模态医学任务时的关键局限性和偏差。", "summary": "该论文介绍了SMMILE，一个用于医学任务的专家驱动的多模态上下文学习（ICL）基准，及其增强版本SMMILE++。它评估了15个多模态大型语言模型（MLLMs），发现它们在医学背景下普遍表现出较差的ICL能力。研究揭示ICL相比零样本学习仅提供适度改进，并且MLLMs极易受到不相关示例的影响，并在示例排序中表现出强烈的近因偏差。这些发现突出了当前MLLMs在医学ICL方面的显著局限性。", "keywords": "多模态上下文学习, 医学人工智能, 基准, 大型语言模型, 临床推理", "comments": "本文通过关注多模态大型语言模型（MLLMs）在医学领域的上下文学习（ICL）能力，弥补了评估方面的关键空白，这对于现实世界的临床应用至关重要。SMMILE基准的专家驱动的策划增加了显著价值并确保了临床相关性。这些发现很重要，因为它们揭示了当前MLLMs的显著局限性和偏差（对不相关示例的敏感性、近因偏差），表明简单地增加模型大小或数据可能不足以实现稳健的医学ICL。这项工作为旨在改进MLLM在复杂医学推理任务中性能的未来研究提供了有价值的基准和见解。"}}
{"id": "2506.21367", "title": "rQdia: Regularizing Q-Value Distributions With Image Augmentation", "authors": ["Sam Lerman", "Jing Bi"], "summary": "rQdia regularizes Q-value distributions with augmented images in pixel-based\ndeep reinforcement learning. With a simple auxiliary loss, that equalizes these\ndistributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks\nrespectively in the MuJoCo Continuous Control Suite from pixels, and\nData-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured\nin both sample efficiency and longer-term training. Moreover, the addition of\nrQdia finally propels model-free continuous control from pixels over the state\nencoding baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21367v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21367v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "rQdia: 使用图像增强正则化Q值分布", "tldr": "rQdia通过图像增强和简单的辅助损失来正则化Q值分布，显著提升了像素级深度强化学习算法的性能，甚至超越了状态编码基线。", "motivation": "在像素级深度强化学习中，Q值分布的正则化是一个挑战，并且存在提升现有算法性能的需求。", "method": "rQdia通过一个简单的辅助损失来正则化Q值分布，该损失利用均方误差（MSE）来均衡增强图像的Q值分布。", "result": "rQdia在MuJoCo连续控制套件（像素输入）中，分别使DrQ和SAC在9/12和10/12的任务上获得性能提升。在Atari街机环境中，它使Data-Efficient Rainbow在18/26的环境中获得提升。性能提升体现在样本效率和长期训练方面。此外，rQdia的加入首次使无模型连续控制（像素输入）超越了状态编码基线。", "conclusion": "rQdia通过对Q值分布的有效正则化，显著提升了像素级深度强化学习算法的性能，使其在多个基准测试中表现优异，甚至超越了传统的状态编码方法。", "translation": "rQdia通过增强图像在基于像素的深度强化学习中正则化Q值分布。通过一个简单的辅助损失，该损失通过MSE均衡这些分布，rQdia在MuJoCo连续控制套件的像素输入任务中分别提升了DrQ和SAC在9/12和10/12的任务上的表现，并在26个Atari街机环境中的18个中提升了Data-Efficient Rainbow。性能提升体现在样本效率和长期训练两方面。此外，rQdia的加入最终推动了无模型连续控制（像素输入）超越了状态编码基线。", "summary": "rQdia是一种用于像素级深度强化学习的新方法，它通过图像增强和基于MSE的辅助损失来正则化Q值分布。该方法显著提升了DrQ、SAC和Data-Efficient Rainbow等算法在MuJoCo和Atari基准测试上的性能，提高了样本效率和长期训练效果，并首次使无模型连续控制（像素输入）超越了状态编码基线。", "keywords": "Q值分布正则化, 图像增强, 深度强化学习, 像素级控制, 样本效率", "comments": "rQdia的创新点在于将图像增强与Q值分布正则化相结合，通过一个简洁的辅助损失有效解决了像素级DRL中的稳定性问题。其重要性在于显著提升了现有SOTA算法的性能，并首次实现了无模型像素级控制超越状态编码基线，这为未来基于像素输入的DRL研究开辟了新方向。"}}
{"id": "2506.21188", "title": "GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud Sequential Grounding", "authors": ["Zijun Lin", "Shuting He", "Cheston Tan", "Bihan Wen"], "summary": "Sequential grounding in 3D point clouds (SG3D) refers to locating sequences\nof objects by following text instructions for a daily activity with detailed\nsteps. Current 3D visual grounding (3DVG) methods treat text instructions with\nmultiple steps as a whole, without extracting useful temporal information from\neach step. However, the instructions in SG3D often contain pronouns such as\n\"it\", \"here\" and \"the same\" to make language expressions concise. This requires\ngrounding methods to understand the context and retrieve relevant information\nfrom previous steps to correctly locate object sequences. Due to the lack of an\neffective module for collecting related historical information,\nstate-of-the-art 3DVG methods face significant challenges in adapting to the\nSG3D task. To fill this gap, we propose GroundFlow -- a plug-in module for\ntemporal reasoning on 3D point cloud sequential grounding. Firstly, we\ndemonstrate that integrating GroundFlow improves the task accuracy of 3DVG\nbaseline methods by a large margin (+7.5\\% and +10.2\\%) in the SG3D benchmark,\neven outperforming a 3D large language model pre-trained on various datasets.\nFurthermore, we selectively extract both short-term and long-term step\ninformation based on its relevance to the current instruction, enabling\nGroundFlow to take a comprehensive view of historical information and maintain\nits temporal understanding advantage as step counts increase. Overall, our work\nintroduces temporal reasoning capabilities to existing 3DVG models and achieves\nstate-of-the-art performance in the SG3D benchmark across five datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21188v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21188v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GroundFlow：一个用于3D点云序列定位中时序推理的即插即用模块", "tldr": "GroundFlow是一个即插即用模块，通过有效利用短时和长时历史信息，显著提升了3D点云序列定位任务中现有视觉定位模型的时序推理能力和准确性。", "motivation": "现有的3D视觉定位(3DVG)方法在处理多步骤文本指令时，未能有效提取时序信息，尤其是在指令包含代词（如“it”、“here”、“the same”）时，需要理解上下文并从先前的步骤中检索相关信息。由于缺乏有效的历史信息收集模块，这些方法难以适应3D点云序列定位(SG3D)任务。", "method": "本文提出了GroundFlow，一个即插即用模块，用于3D点云序列定位中的时序推理。它通过选择性地提取与当前指令相关的短时和长时步骤信息，从而全面利用历史信息。", "result": "GroundFlow将3DVG基线方法在SG3D基准测试中的任务准确率大幅提升（+7.5%和+10.2%），甚至优于在各种数据集上预训练的3D大型语言模型。在五个数据集上实现了最先进的性能。", "conclusion": "GroundFlow为现有3DVG模型引入了时序推理能力，并在SG3D基准测试中达到了最先进的性能，解决了现有方法在处理序列指令时的不足。", "translation": "3D点云中的序列定位（SG3D）是指通过遵循详细步骤的日常活动文本指令来定位一系列对象。当前的3D视觉定位（3DVG）方法将多步骤文本指令作为一个整体处理，未能从每个步骤中提取有用的时序信息。然而，SG3D中的指令通常包含代词，如“它”、“这里”和“相同”，以使语言表达简洁。这要求定位方法理解上下文并从先前的步骤中检索相关信息，以正确地定位对象序列。由于缺乏有效的模块来收集相关的历史信息，最先进的3DVG方法在适应SG3D任务时面临重大挑战。为了填补这一空白，我们提出了GroundFlow——一个用于3D点云序列定位中时序推理的即插即用模块。首先，我们证明了在SG3D基准测试中，集成GroundFlow显著提高了3DVG基线方法的任务准确率（+7.5%和+10.2%），甚至优于在各种数据集上预训练的3D大型语言模型。此外，我们根据与当前指令的相关性选择性地提取短时和长时步骤信息，使GroundFlow能够全面查看历史信息，并在步骤计数增加时保持其时序理解优势。总的来说，我们的工作为现有3DVG模型引入了时序推理能力，并在五个数据集的SG3D基准测试中实现了最先进的性能。", "summary": "本文提出了GroundFlow，一个用于3D点云序列定位（SG3D）的即插即用模块，旨在解决现有3D视觉定位（3DVG）方法在处理多步骤指令时缺乏时序推理能力的问题。GroundFlow通过选择性地提取短时和长时历史信息，使模型能够理解上下文并处理代词。实验证明，GroundFlow显著提升了基线方法的准确率，并在SG3D基准测试中达到了最先进的性能。", "keywords": "3D点云, 序列定位, 时序推理, 即插即用模块, 视觉定位", "comments": "该研究提出GroundFlow模块，创新性地解决了3D点云序列定位中时序推理的难题，特别是对于处理含有代词的复杂指令。其即插即用的设计使其易于集成到现有3DVG模型中，具有很高的实用价值。性能提升显著，甚至超越了大型预训练模型，显示了其有效性和重要性。"}}
{"id": "2506.21371", "title": "MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators", "authors": ["Vasileios Leon", "Georgios Makris", "Sotirios Xydis", "Kiamal Pekmestzi", "Dimitrios Soudris"], "summary": "Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has\nestablished them as the defacto approach for providing advanced Machine\nLearning tasks with excellent accuracy. Targeting low-power DNN computing, this\npaper examines the interplay of fine-grained error resilience of DNN workloads\nin collaboration with hardware approximation techniques, to achieve higher\nlevels of energy efficiency. Utilizing the state-of-the-art ROUP approximate\nmultipliers, we systematically explore their fine-grained distribution across\nthe network according to our layer-, filter-, and kernel-level approaches, and\nexamine their impact on accuracy and energy. We use the ResNet-8 model on the\nCIFAR-10 dataset to evaluate our approximations. The proposed solution delivers\nup to 54% energy gains in exchange for up to 4% accuracy loss, compared to the\nbaseline quantized model, while it provides 2x energy gains with better\naccuracy versus the state-of-the-art DNN approximations.", "comment": "Presented at the 13th IEEE LASCAS Conference", "pdf_url": "http://arxiv.org/pdf/2506.21371v1", "categories": ["cs.LG", "cs.AR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21371v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MAx-DNN：面向节能型DNN硬件加速器的多级算术近似", "tldr": "MAx-DNN提出了一种多级算术近似方法，利用近似乘法器在DNN硬件加速器中实现显著的能效提升，同时保持可接受的精度。", "motivation": "为了实现低功耗的深度神经网络（DNN）计算，本文旨在探索DNN工作负载的细粒度错误弹性与硬件近似技术相结合，以提高能效。", "method": "该方法利用最先进的ROUP近似乘法器，系统地探索了它们在网络中的细粒度分布，并根据层级、滤波器级和核级方法进行应用，评估了它们对精度和能耗的影响。实验在CIFAR-10数据集上使用ResNet-8模型进行评估。", "result": "与基线量化模型相比，所提出的解决方案实现了高达54%的能耗增益，同时带来了高达4%的精度损失。与现有最先进的DNN近似方法相比，它提供了2倍的能耗增益和更好的精度。", "conclusion": "MAx-DNN通过多级算术近似，有效地提高了DNN硬件加速器的能效，在保持可接受精度损失的同时，甚至在能效和精度上超越了现有最先进的DNN近似方法。", "translation": "如今，深度神经网络（DNN）架构的快速发展已使其成为提供高级机器学习任务并实现卓越准确性的事实标准方法。为了实现低功耗的DNN计算，本文研究了DNN工作负载的细粒度错误弹性与硬件近似技术的相互作用，以实现更高水平的能效。我们利用最先进的ROUP近似乘法器，系统地探索了它们在网络中的细粒度分布，根据我们的层级、滤波器级和核级方法，并检验了它们对精度和能耗的影响。我们使用CIFAR-10数据集上的ResNet-8模型来评估我们的近似方法。与基线量化模型相比，所提出的解决方案以高达4%的精度损失换取高达54%的能耗增益，同时与现有最先进的DNN近似方法相比，它提供了2倍的能耗增益和更好的精度。", "summary": "MAx-DNN提出了一种针对深度神经网络硬件加速器的多级算术近似方法，旨在提高能效。该方法利用ROUP近似乘法器，在层级、滤波器级和核级对近似进行细粒度分布。实验结果表明，与基线量化模型相比，该方法能将能耗降低高达54%，精度损失不超过4%；与现有最先进的DNN近似方法相比，它能提供2倍的能耗增益并保持更好的精度。", "keywords": "深度学习, 硬件加速器, 能量效率, 近似计算, 多级近似", "comments": "本文的创新点在于提出了多级（层级、滤波器级、核级）的细粒度算术近似方法，并结合了ROUP近似乘法器，以在能效和精度之间取得平衡。其重要性在于为未来低功耗DNN硬件设计提供了新的思路，尤其是在资源受限的环境中。该方法在能效提升方面表现出色，且精度损失可控，甚至优于现有方法。"}}
{"id": "2506.21374", "title": "Pay Attention to Small Weights", "authors": ["Chao Zhou", "Tom Jacobs", "Advait Gadhikar", "Rebekka Burkholz"], "summary": "Finetuning large pretrained neural networks is known to be\nresource-intensive, both in terms of memory and computational cost. To mitigate\nthis, a common approach is to restrict training to a subset of the model\nparameters. By analyzing the relationship between gradients and weights during\nfinetuning, we observe a notable pattern: large gradients are often associated\nwith small-magnitude weights. This correlation is more pronounced in finetuning\nsettings than in training from scratch. Motivated by this observation, we\npropose NANOADAM, which dynamically updates only the small-magnitude weights\nduring finetuning and offers several practical advantages: first, this\ncriterion is gradient-free -- the parameter subset can be determined without\ngradient computation; second, it preserves large-magnitude weights, which are\nlikely to encode critical features learned during pretraining, thereby reducing\nthe risk of catastrophic forgetting; thirdly, it permits the use of larger\nlearning rates and consistently leads to better generalization performance in\nexperiments. We demonstrate this for both NLP and vision tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21374v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21374v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "关注小权重", "tldr": "NANOADAM通过动态更新微调过程中小幅度的权重来节省资源并提高性能。", "motivation": "大型预训练神经网络的微调在内存和计算成本方面都是资源密集型的。研究发现，在微调过程中，大梯度通常与小幅度的权重相关联，这在微调设置中比从头开始训练更明显，这启发了本研究。", "method": "提出了NANOADAM，它在微调期间仅动态更新小幅度的权重。该准则是无梯度的，可以在不计算梯度的情况下确定参数子集；它保留了可能编码预训练期间学习到的关键特征的大幅度权重，从而降低了灾难性遗忘的风险；它允许使用更大的学习率。", "result": "在NLP和视觉任务的实验中，NANOADAM始终带来更好的泛化性能。", "conclusion": "NANOADAM通过仅更新微调过程中的小幅度权重，提供了一种资源高效且有效的策略，该策略能提高泛化性能并减少灾难性遗忘。", "translation": "微调大型预训练神经网络在内存和计算成本方面都是资源密集型的。为了缓解这一问题，一种常见的方法是将训练限制在模型参数的一个子集上。通过分析微调过程中梯度与权重之间的关系，我们观察到一个显著的模式：大梯度通常与小幅度的权重相关联。这种相关性在微调设置中比从头开始训练更为明显。受此观察启发，我们提出了NANOADAM，它在微调期间仅动态更新小幅度的权重，并提供了几个实际优势：首先，这个准则是无梯度的——可以在不计算梯度的情况下确定参数子集；其次，它保留了大幅度的权重，这些权重可能编码了预训练期间学习到的关键特征，从而降低了灾难性遗忘的风险；第三，它允许使用更大的学习率，并在实验中始终带来更好的泛化性能。我们在NLP和视觉任务中都证明了这一点。", "summary": "本研究关注大型预训练神经网络微调过程中资源消耗大的问题。通过分析梯度与权重的关系，发现大梯度常与小幅度权重相关。受此启发，提出了NANOADAM方法，该方法在微调时仅动态更新小幅度权重。NANOADAM的优势在于其无梯度性、能保留预训练关键特征以减少灾难性遗忘，并允许使用更大的学习率。实验证明，该方法在NLP和视觉任务中均能持续提升泛化性能。", "keywords": "小权重, 微调, 资源效率, NANOADAM, 泛化性能", "comments": "这篇论文提出了一种新颖且高效的微调策略NANOADAM，其创新点在于利用权重大小而非梯度信息来选择更新参数，从而实现了无梯度参数选择。这不仅节省了计算资源，还通过保留大权重有效缓解了灾难性遗忘问题，并允许使用更大的学习率，最终提升了模型的泛化能力。该方法对于资源受限的深度学习应用具有重要意义。"}}
{"id": "2506.21085", "title": "CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions", "authors": ["Yangzhe Peng", "Kaiyuan Gao", "Liang He", "Yuheng Cong", "Haiguang Liu", "Kun He", "Lijun Wu"], "summary": "Molecular docking plays a crucial role in predicting the binding mode of\nligands to target proteins, and covalent interactions, which involve the\nformation of a covalent bond between the ligand and the target, are\nparticularly valuable due to their strong, enduring binding nature. However,\nmost existing docking methods and deep learning approaches hardly account for\nthe formation of covalent bonds and the associated structural changes. To\naddress this gap, we introduce a comprehensive benchmark for covalent docking,\nCovDocker, which is designed to better capture the complexities of covalent\nbinding. We decompose the covalent docking process into three main tasks:\nreactive location prediction, covalent reaction prediction, and covalent\ndocking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,\nwe establish baseline performances and demonstrate the effectiveness of the\nbenchmark in accurately predicting interaction sites and modeling the molecular\ntransformations involved in covalent binding. These results confirm the role of\nthe benchmark as a rigorous framework for advancing research in covalent drug\ndesign. It underscores the potential of data-driven approaches to accelerate\nthe discovery of selective covalent inhibitors and addresses critical\nchallenges in therapeutic development.", "comment": "Accepted to KDD 2025 Research Track", "pdf_url": "http://arxiv.org/pdf/2506.21085v1", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.21085v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CovDocker：基于任务、数据集和解决方案的共价药物设计基准", "tldr": "CovDocker是一个用于共价药物设计的基准，通过任务、数据集和解决方案解决了现有方法在预测共价键形成方面的不足。", "motivation": "大多数现有的分子对接方法和深度学习方法难以解释共价键的形成及其相关的结构变化，而共价键因其强大、持久的结合性质在药物设计中具有重要价值。", "method": "引入了一个名为CovDocker的全面共价对接基准，将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接。通过调整Uni-Mol和Chemformer等最先进的模型，建立了基线性能。", "result": "该基准在准确预测相互作用位点和模拟共价结合所涉及的分子转化方面表现出有效性，证实了其作为推进共价药物设计研究的严格框架的作用。", "conclusion": "CovDocker基准能够加速选择性共价抑制剂的发现，并解决了治疗开发中的关键挑战，强调了数据驱动方法在药物设计中的潜力。", "translation": "分子对接在预测配体与靶蛋白的结合模式中起着至关重要的作用，而共价相互作用，即配体与靶点之间形成共价键，因其强大、持久的结合性质而特别有价值。然而，大多数现有的对接方法和深度学习方法很难解释共价键的形成和相关的结构变化。为了弥补这一空白，我们引入了一个全面的共价对接基准——CovDocker，它旨在更好地捕捉共价结合的复杂性。我们将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接。通过调整Uni-Mol和Chemformer等最先进的模型，我们建立了基线性能，并证明了该基准在准确预测相互作用位点和模拟共价结合所涉及的分子转化方面的有效性。这些结果证实了该基准作为推进共价药物设计研究的严格框架的作用。它强调了数据驱动方法加速选择性共价抑制剂发现的潜力，并解决了治疗开发中的关键挑战。", "summary": "本文提出了CovDocker，一个用于共价对接的新型基准，旨在解决现有方法在处理共价键形成和结构变化方面的局限性。CovDocker将共价对接过程分解为反应位点预测、共价反应预测和共价对接三个任务。通过利用最先进的模型，它建立了基线并证明了其在准确预测相互作用位点和模拟分子转化方面的有效性，从而为推进共价药物设计和加速选择性共价抑制剂的发现提供了一个稳健的框架。", "keywords": "共价对接, 药物设计, 基准测试, 分子对接, 共价抑制剂", "comments": "CovDocker通过为共价对接提供一个结构化的基准，具有创新性，解决了传统方法常忽视的药物设计中复杂但关键的方面。其将过程分解为特定任务并利用最先进模型的方法，使其成为未来研究的宝贵工具，有望加速更有效共价药物的开发。对数据驱动方法的关注是其一大优势。"}}
{"id": "2506.21382", "title": "Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection", "authors": ["Zhi Zheng", "Bochuan Zhou", "Yuping Song"], "summary": "Cryptocurrency transaction fraud detection faces the dual challenges of\nincreasingly complex transaction patterns and severe class imbalance.\nTraditional methods rely on manual feature engineering and struggle to capture\ntemporal and structural dependencies in transaction networks. This paper\nproposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that\nenhances detection performance through three modules: (1) designing an advanced\ntemporal embedding module that fuses multi-scale time difference features with\nperiodic position encoding; (2) constructing a temporal-aware triple attention\nmechanism that jointly optimizes structural, temporal, and global context\nattention; (3) employing weighted BCE loss to address class imbalance.\nExperiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT\nachieves an AUC of 0.9130, representing a 9.2% improvement over the best\ntraditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This\nmethod not only validates the enhancement effect of temporal awareness and\ntriple attention mechanisms on graph neural networks, but also provides\nfinancial institutions with more reliable fraud detection tools, with its\ndesign principles generalizable to other temporal graph anomaly detection\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21382v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21382v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "加密货币交易欺诈检测的时序感知图注意力网络", "tldr": "提出ATGAT模型，通过时序嵌入和三重注意力机制，有效提升加密货币交易欺诈检测的性能，优于传统方法和GNN基线。", "motivation": "传统方法在复杂交易模式和严重类别不平衡下难以捕捉交易网络中的时序和结构依赖性，且依赖手动特征工程。", "method": "提出增强时序感知图注意力网络（ATGAT），包含三个模块：1) 先进时序嵌入模块融合多尺度时间差特征与周期位置编码；2) 构建时序感知三重注意力机制联合优化结构、时序和全局上下文注意力；3) 采用加权BCE损失解决类别不平衡。", "result": "在Elliptic++数据集上，ATGAT的AUC达到0.9130，比最佳传统方法XGBoost提升9.2%，比GCN提升12.0%，比标准GAT提升10.0%。", "conclusion": "该方法验证了时序感知和三重注意力机制对图神经网络的增强效果，为金融机构提供更可靠的欺诈检测工具，且设计原则可推广到其他时序图异常检测任务。", "translation": "加密货币交易欺诈检测面临着日益复杂的交易模式和严重的类别不平衡的双重挑战。传统方法依赖手动特征工程，难以捕获交易网络中的时序和结构依赖性。本文提出了一种增强时序感知图注意力网络（ATGAT），通过三个模块提升检测性能：(1) 设计一种先进的时序嵌入模块，融合多尺度时间差特征与周期位置编码；(2) 构建一种时序感知三重注意力机制，联合优化结构、时序和全局上下文注意力；(3) 采用加权BCE损失解决类别不平衡。在Elliptic++加密货币数据集上的实验表明，ATGAT的AUC达到0.9130，比最佳传统方法XGBoost提升9.2%，比GCN提升12.0%，比标准GAT提升10.0%。该方法不仅验证了时序感知和三重注意力机制对图神经网络的增强效果，而且为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广到其他时序图异常检测任务。", "summary": "本文针对加密货币交易欺诈检测中复杂模式和类别不平衡的挑战，提出了一种增强时序感知图注意力网络（ATGAT）。该模型通过先进的时序嵌入、时序感知三重注意力机制和加权BCE损失来捕获时序和结构依赖性。实验结果显示，ATGAT在Elliptic++数据集上显著优于传统方法和图神经网络基线，验证了其有效性，并为金融机构提供了更可靠的欺诈检测方案，其方法具有通用性。", "keywords": "加密货币欺诈检测, 图注意力网络, 时序感知, 类别不平衡, 异常检测", "comments": "该论文的创新点在于结合了时序感知和三重注意力机制来增强图神经网络在欺诈检测中的性能，特别是解决了加密货币交易中时序依赖和类别不平衡的难题。其提出的ATGAT模型在多个方面进行了优化，超越了现有方法，且其设计原则具有良好的泛化能力，可应用于其他时序图异常检测任务，具有重要的实践价值。"}}
{"id": "2506.21209", "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models", "authors": ["Louis Kerner", "Michel Meintz", "Bihe Zhao", "Franziska Boenisch", "Adam Dziedzic"], "summary": "State-of-the-art text-to-image models like Infinity generate photorealistic\nimages at an unprecedented speed. These models operate in a bitwise\nautoregressive manner over a discrete set of tokens that is practically\ninfinite in size. However, their impressive generative power comes with a\ngrowing risk: as their outputs increasingly populate the Internet, they are\nlikely to be scraped and reused as training data-potentially by the very same\nmodels. This phenomenon has been shown to lead to model collapse, where\nrepeated training on generated content, especially from the models' own\nprevious versions, causes a gradual degradation in performance. A promising\nmitigation strategy is watermarking, which embeds human-imperceptible yet\ndetectable signals into generated images-enabling the identification of\ngenerated content. In this work, we introduce BitMark, a robust bitwise\nwatermarking framework for Infinity. Our method embeds a watermark directly at\nthe bit level of the token stream across multiple scales (also referred to as\nresolutions) during Infinity's image generation process. Our bitwise watermark\nsubtly influences the bits to preserve visual fidelity and generation speed\nwhile remaining robust against a spectrum of removal techniques. Furthermore,\nit exhibits high radioactivity, i.e., when watermarked generated images are\nused to train another image generative model, this second model's outputs will\nalso carry the watermark. The radioactive traces remain detectable even when\nonly fine-tuning diffusion or image autoregressive models on images watermarked\nwith our BitMark. Overall, our approach provides a principled step toward\npreventing model collapse in image generative models by enabling reliable\ndetection of generated outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21209v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21209v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "BitMark for Infinity：水印比特位自回归图像生成模型", "tldr": "提出BitMark，一种针对Infinity等比特位自回归图像生成模型的鲁棒水印框架，用于防止模型崩溃并实现内容溯源和放射性追踪。", "motivation": "最先进的文本到图像生成模型（如Infinity）生成的图像可能被重复用作训练数据，导致模型性能逐渐下降（模型崩溃），因此需要一种方法来识别和追踪生成内容。", "method": "引入BitMark，一个鲁棒的比特位水印框架。该方法在Infinity的图像生成过程中，直接在令牌流的比特级别跨多个尺度嵌入水印。这种比特位水印能微妙地影响比特，以保持视觉保真度和生成速度，同时对一系列移除技术保持鲁棒性。此外，它还具有高放射性，即当带水印的生成图像用于训练另一个图像生成模型时，第二个模型的输出也将带有水印。", "result": "BitMark在保持视觉保真度和生成速度的同时，对移除技术具有鲁棒性。它还展示了高放射性，即使用带BitMark水印的图像微调扩散模型或图像自回归模型时，放射性痕迹仍然可检测。", "conclusion": "我们的方法通过实现对生成输出的可靠检测，为防止图像生成模型中的模型崩溃提供了原则性的一步。", "translation": "最先进的文本到图像模型（如Infinity）以前所未有的速度生成逼真的图像。这些模型以比特位自回归方式操作，作用于实际上无限大小的离散令牌集。然而，它们强大的生成能力伴随着日益增长的风险：随着它们的输出越来越多地充斥互联网，它们很可能被抓取并重新用作训练数据——甚至可能被同一模型使用。这种现象已被证明会导致模型崩溃，即重复使用生成内容进行训练，特别是来自模型自身先前版本的生成内容，会导致性能逐渐下降。一种有前景的缓解策略是水印，它将人眼不可察觉但可检测的信号嵌入到生成的图像中——从而能够识别生成内容。在这项工作中，我们引入了BitMark，一个针对Infinity的鲁棒比特位水印框架。我们的方法在Infinity的图像生成过程中，直接在令牌流的比特级别跨多个尺度（也称为分辨率）嵌入水印。我们的比特位水印微妙地影响比特，以保持视觉保真度和生成速度，同时对一系列移除技术保持鲁棒性。此外，它还具有高放射性，即当带水印的生成图像用于训练另一个图像生成模型时，第二个模型的输出也将带有水印。即使仅使用我们的BitMark水印的图像对扩散模型或图像自回归模型进行微调，放射性痕迹仍然可检测。总的来说，我们的方法通过实现对生成输出的可靠检测，为防止图像生成模型中的模型崩溃提供了原则性的一步。", "summary": "本文提出了BitMark，一个专为Infinity等比特位自回归图像生成模型设计的鲁棒水印框架。为解决生成内容被重复用于训练导致模型崩溃的问题，BitMark在图像生成过程中直接在令牌流的比特位级别嵌入水印。该方法旨在不影响图像质量和生成速度的前提下，提供对水印移除的鲁棒性，并具备“放射性”特性，即水印能够传播到使用这些生成图像训练的新模型中。这使得对生成内容的可靠检测和溯源成为可能，从而有效防止模型崩溃。", "keywords": "水印, 图像生成模型, 模型崩溃, 比特位自回归, Infinity", "comments": "BitMark的创新之处在于其比特位级别的水印嵌入，这与传统图像水印方法不同，直接作用于生成模型的核心机制。其“放射性”特性尤为重要，因为它解决了生成内容被二次利用后溯源困难的问题，为防止模型崩溃提供了新颖且强大的工具。该方法在保持生成质量和速度的同时，实现了水印的鲁棒性和传播性，具有重要的实用价值。"}}
{"id": "2506.21387", "title": "Early Stopping Tabular In-Context Learning", "authors": ["Jaris Küken", "Lennart Purucker", "Frank Hutter"], "summary": "Tabular foundation models have shown strong performance across various\ntabular learning tasks via in-context learning, offering robust generalization\nwithout any downstream finetuning. However, their inference-time costs remain\nhigh, particularly for larger datasets. To address this, we propose\nearly-stopping the in-context learning process. We achieve this by dynamically\nevaluating whether to stop in-context learning after each Transformer encoder\nlayer. Once stopped, we decode the embedding using a pre-trained layer-wise\ndecoder. Experiments across 34 small classification tasks size show that early\nstopping in-context learning accelerates inference by up to x1.3 with\nnegligible degradation in predictive performance. To assess scalability, we\nfurther evaluate our method on five larger classification tasks, achieving\nspeedups of up to x2.2. Our results demonstrate the potential of early exiting\nas an effective and practical strategy for improving the efficiency of tabular\nin-context learning.", "comment": "ICML Workshop Paper", "pdf_url": "http://arxiv.org/pdf/2506.21387v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21387v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "表格上下文学习的提前停止", "tldr": "本文提出了一种提前停止表格上下文学习的方法，通过在Transformer编码器层后动态评估是否停止，并使用预训练的逐层解码器解码嵌入，从而在不显著降低预测性能的情况下，显著加速了表格基础模型的推理时间。", "motivation": "表格基础模型通过上下文学习在各种表格学习任务中表现出色，但其推理时间成本很高，特别是对于大型数据集。", "method": "本文提出通过动态评估是否在每个Transformer编码器层之后停止上下文学习来实现提前停止。一旦停止，使用预训练的逐层解码器解码嵌入。", "result": "在34个小型分类任务上的实验表明，提前停止上下文学习可将推理速度提高达1.3倍，而预测性能的下降可忽略不计。在五个大型分类任务上，速度提高了达2.2倍。", "conclusion": "早期退出是提高表格上下文学习效率的有效且实用的策略。", "translation": "表格基础模型通过上下文学习在各种表格学习任务中表现出强大的性能，提供了无需任何下游微调的强大泛化能力。然而，它们的推理时间成本仍然很高，特别是对于大型数据集。为了解决这个问题，我们提出了提前停止上下文学习过程。我们通过在每个Transformer编码器层之后动态评估是否停止上下文学习来实现这一点。一旦停止，我们使用预训练的逐层解码器解码嵌入。在34个小型分类任务上的实验表明，提前停止上下文学习可将推理速度提高达1.3倍，而预测性能的下降可忽略不计。为了评估可扩展性，我们进一步在五个大型分类任务上评估了我们的方法，实现了高达2.2倍的速度提升。我们的结果表明，早期退出作为一种有效且实用的策略，在提高表格上下文学习效率方面具有潜力。", "summary": "本文针对表格基础模型在上下文学习中推理时间成本高的问题，提出了一种提前停止上下文学习的方法。该方法在每个Transformer编码器层后动态判断是否停止，并利用预训练的逐层解码器进行嵌入解码。实验证明，该方法在小型和大型分类任务上均能显著加速推理，同时对预测性能影响甚微，显示了早期退出在提升表格上下文学习效率方面的潜力。", "keywords": "表格上下文学习, 提前停止, 推理加速, 基础模型, Transformer", "comments": "这项研究的创新点在于将早期退出策略应用于表格上下文学习，有效解决了表格基础模型推理成本高的问题。其重要性体现在为实际应用提供了一种更高效、更实用的解决方案，使得上下文学习在处理大型表格数据时更具可行性。该方法在不显著牺牲性能的前提下实现了显著的加速，具有很高的实用价值。"}}
{"id": "2506.21233", "title": "ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation", "authors": ["Xiwei Xuan", "Ziquan Deng", "Kwan-Liu Ma"], "summary": "Training-free open-vocabulary semantic segmentation (OVS) aims to segment\nimages given a set of arbitrary textual categories without costly model\nfine-tuning. Existing solutions often explore attention mechanisms of\npre-trained models, such as CLIP, or generate synthetic data and design complex\nretrieval processes to perform OVS. However, their performance is limited by\nthe capability of reliant models or the suboptimal quality of reference sets.\nIn this work, we investigate the largely overlooked data quality problem for\nthis challenging dense scene understanding task, and identify that a\nhigh-quality reference set can significantly benefit training-free OVS. With\nthis observation, we introduce a data-quality-oriented framework, comprising a\ndata pipeline to construct a reference set with well-paired segment-text\nembeddings and a simple similarity-based retrieval to unveil the essential\neffect of data. Remarkably, extensive evaluations on ten benchmark datasets\ndemonstrate that our method outperforms all existing training-free OVS\napproaches, highlighting the importance of data-centric design for advancing\nOVS without training. Our code is available at https://github.com/xiweix/ReME .", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21233v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21233v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ReME：一个用于免训练开放词汇分割的数据中心框架", "tldr": "ReME提出一个数据中心框架，通过构建高质量参考集和简单的相似性检索，显著提升了免训练开放词汇分割的性能。", "motivation": "现有免训练开放词汇语义分割（OVS）方法受限于所依赖模型的能力或参考集质量不佳。本文旨在解决这一被忽视的数据质量问题。", "method": "引入ReME框架，一个以数据质量为导向的框架，包含一个数据管道用于构建具有良好配对片段-文本嵌入的参考集，以及一个简单的基于相似性的检索机制。", "result": "在十个基准数据集上的广泛评估表明，ReME方法优于所有现有的免训练OVS方法。", "conclusion": "强调了以数据为中心的设计对于无需训练推进开放词汇分割的重要性。", "translation": "免训练开放词汇语义分割（OVS）旨在在不进行昂贵的模型微调的情况下，根据一组任意文本类别对图像进行分割。现有解决方案通常探索预训练模型（如CLIP）的注意力机制，或生成合成数据并设计复杂的检索过程来执行OVS。然而，它们的性能受限于所依赖模型的能力或参考集质量不佳。在这项工作中，我们研究了这项具有挑战性的密集场景理解任务中在很大程度上被忽视的数据质量问题，并确定高质量的参考集可以显著有益于免训练OVS。基于这一观察，我们引入了一个以数据质量为导向的框架ReME，它包括一个用于构建具有良好配对的片段-文本嵌入的参考集的数据管道，以及一个简单的基于相似性的检索方法，以揭示数据的重要作用。值得注意的是，在十个基准数据集上的广泛评估表明，我们的方法优于所有现有的免训练OVS方法，突出了以数据为中心的设计对于无需训练推进OVS的重要性。我们的代码可在https://github.com/xiweix/ReME 获取。", "summary": "本文提出ReME，一个针对免训练开放词汇语义分割的数据中心框架。现有方法受限于模型能力或数据质量。ReME通过构建高质量的片段-文本嵌入参考集，并采用简单的相似性检索，解决了数据质量问题。实验证明，ReME在十个基准数据集上显著优于现有免训练OVS方法，强调了数据中心设计的重要性。", "keywords": "开放词汇分割, 数据中心, 免训练, 语义分割, 参考集", "comments": "该论文的创新点在于将免训练开放词汇分割的重点从复杂的模型设计或微调转移到数据质量上。通过证明精心策划的参考集和简单的检索方法可以超越现有更复杂的方法，它强调了机器学习中一个关键但常被忽视的方面：数据质量的影响。这种以数据为中心的方法在提高性能的同时简化了解决方案。"}}
{"id": "2506.21411", "title": "Distributed Cross-Channel Hierarchical Aggregation for Foundation Models", "authors": ["Aristeidis Tsaris", "Isaac Lyngaas", "John Lagregren", "Mohamed Wahib", "Larry York", "Prasanna Balaprakash", "Dan Lu", "Feiyi Wang", "Xiao Wang"], "summary": "Vision-based scientific foundation models hold significant promise for\nadvancing scientific discovery and innovation. This potential stems from their\nability to aggregate images from diverse sources such as varying physical\ngroundings or data acquisition systems and to learn spatio-temporal\ncorrelations using transformer architectures. However, tokenizing and\naggregating images can be compute-intensive, a challenge not fully addressed by\ncurrent distributed methods. In this work, we introduce the Distributed\nCross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets\nwith a large number of channels across image modalities. Our method is\ncompatible with any model-parallel strategy and any type of vision transformer\narchitecture, significantly improving computational efficiency. We evaluated\nD-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated\nwith tensor parallelism and model sharding, our approach achieved up to a 75%\nreduction in memory usage and more than doubled sustained throughput on up to\n1,024 AMD GPUs on the Frontier Supercomputer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21411v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21411v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "分布式跨通道分层聚合用于基础模型", "tldr": "D-CHAG是一种新的分布式聚合方法，用于处理多通道图像数据，显著提升了基础模型在计算效率和内存使用方面的表现。", "motivation": "现有分布式方法未能充分解决图像标记化和聚合计算密集的问题，尤其是在处理大量通道的图像数据时。", "method": "引入了分布式跨通道分层聚合（D-CHAG）方法，该方法兼容任何模型并行策略和视觉Transformer架构，旨在提高计算效率。", "result": "D-CHAG与张量并行和模型分片结合时，在超光谱成像和天气预报任务上，内存使用减少高达75%，持续吞吐量增加一倍以上，在Frontier超级计算机上使用多达1024个AMD GPU时表现出色。", "conclusion": "D-CHAG显著提高了处理多通道图像数据的基础模型的计算效率和资源利用率。", "translation": "基于视觉的科学基础模型在推动科学发现和创新方面具有巨大的前景。这种潜力源于它们能够聚合来自不同来源（例如不同的物理基础或数据采集系统）的图像，并使用Transformer架构学习时空相关性。然而，图像的标记化和聚合可能计算密集，这是当前分布式方法尚未完全解决的挑战。在这项工作中，我们引入了分布式跨通道分层聚合（D-CHAG）方法，该方法专为具有大量图像模态通道的数据集而设计。我们的方法兼容任何模型并行策略和任何类型的视觉Transformer架构，显著提高了计算效率。我们在高光谱成像和天气预报任务上评估了D-CHAG。当与张量并行和模型分片集成时，我们的方法在Frontier超级计算机上使用多达1024个AMD GPU时，内存使用减少高达75%，持续吞吐量增加一倍以上。", "summary": "本文提出了一种名为分布式跨通道分层聚合（D-CHAG）的新方法，旨在解决视觉基础模型在处理多通道图像数据时面临的计算密集型聚合挑战。D-CHAG兼容多种模型并行策略和视觉Transformer架构，并通过在超光谱成像和天气预报任务上的评估，展示了其在减少内存使用（高达75%）和提高吞吐量（超过一倍）方面的显著效果，特别是在大规模GPU部署环境下。", "keywords": "分布式聚合, 基础模型, 视觉Transformer, 计算效率, 多通道图像", "comments": "D-CHAG的创新之处在于其针对多通道图像数据的分布式聚合策略，有效解决了现有方法在计算效率上的不足。其与模型并行策略的兼容性以及在大规模GPU集群上的显著性能提升，使其成为处理复杂科学数据集的视觉基础模型的重要进步。"}}
{"id": "2506.21237", "title": "DiMPLe -- Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation", "authors": ["Umaima Rahman", "Mohammad Yaqub", "Dwarikanath Mahapatra"], "summary": "We introduce DiMPLe (Disentangled Multi-Modal Prompt Learning), a novel\napproach to disentangle invariant and spurious features across vision and\nlanguage modalities in multi-modal learning. Spurious correlations in visual\ndata often hinder out-of-distribution (OOD) performance. Unlike prior methods\nfocusing solely on image features, DiMPLe disentangles features within and\nacross modalities while maintaining consistent alignment, enabling better\ngeneralization to novel classes and robustness to distribution shifts. Our\nmethod combines three key objectives: (1) mutual information minimization\nbetween invariant and spurious features, (2) spurious feature regularization,\nand (3) contrastive learning on invariant features. Extensive experiments\ndemonstrate DiMPLe demonstrates superior performance compared to CoOp-OOD, when\naveraged across 11 diverse datasets, and achieves absolute gains of 15.27 in\nbase class accuracy and 44.31 in novel class accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21237v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21237v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DiMPLe -- 解耦多模态提示学习：通过不变性和虚假特征分离增强分布外对齐", "tldr": "DiMPLe是一种新的多模态提示学习方法，通过解耦视觉和语言模态中的不变特征和虚假特征，显著提高了分布外性能，尤其是在新类别准确性方面。", "motivation": "视觉数据中的虚假相关性经常阻碍模型在分布外（OOD）环境下的性能。现有方法主要关注图像特征，未能有效处理跨模态的特征解耦以提高泛化能力。", "method": "DiMPLe通过结合三个关键目标来解耦模态内部和模态之间的特征：1) 最小化不变特征和虚假特征之间的互信息；2) 对虚假特征进行正则化；3) 对不变特征进行对比学习。", "result": "DiMPLe在11个不同数据集上的平均表现优于CoOp-OOD，在基础类别准确率上获得15.27的绝对提升，在新类别准确率上获得44.31的绝对提升。", "conclusion": "DiMPLe通过有效解耦多模态学习中的不变和虚假特征，显著提升了模型在分布外场景下的泛化能力和对分布偏移的鲁棒性。", "translation": "我们引入DiMPLe（解耦多模态提示学习），这是一种新颖方法，用于在多模态学习中解耦视觉和语言模态中的不变特征和虚假特征。视觉数据中的虚假相关性经常阻碍分布外（OOD）性能。与以往仅关注图像特征的方法不同，DiMPLe在保持一致对齐的同时，解耦模态内部和模态之间的特征，从而能够更好地泛化到新类别并增强对分布偏移的鲁棒性。我们的方法结合了三个关键目标：（1）不变特征和虚假特征之间的互信息最小化，（2）虚假特征正则化，以及（3）不变特征上的对比学习。广泛的实验表明，DiMPLe在11个不同数据集上平均表现出优于CoOp-OOD的性能，并在基础类别准确率上实现了15.27的绝对增益，在新类别准确率上实现了44.31的绝对增益。", "summary": "本文提出了DiMPLe（解耦多模态提示学习），旨在通过在视觉和语言模态中解耦不变特征和虚假特征来解决多模态学习中分布外性能受阻的问题。DiMPLe通过最小化不变特征与虚假特征的互信息、正则化虚假特征以及对不变特征进行对比学习，实现了模态内部和跨模态的特征解耦。实验结果表明，DiMPLe在多个数据集上显著优于现有方法，尤其在提高新类别泛化能力方面表现突出。", "keywords": "多模态学习, 分布外泛化, 特征解耦, 提示学习, 虚假相关性", "comments": "DiMPLe的创新之处在于其独特地将不变特征和虚假特征在多模态（视觉和语言）环境中进行解耦，而不仅仅是单一模态。这对于提高模型在面对分布外数据时的泛化能力和鲁棒性至关重要。其结合互信息最小化、正则化和对比学习的机制设计也颇具新意。"}}
{"id": "2506.21249", "title": "Temporal Rate Reduction Clustering for Human Motion Segmentation", "authors": ["Xianghan Meng", "Zhengyu Tong", "Zhiyuan Huang", "Chun-Guang Li"], "summary": "Human Motion Segmentation (HMS), which aims to partition videos into\nnon-overlapping human motions, has attracted increasing research attention\nrecently. Existing approaches for HMS are mainly dominated by subspace\nclustering methods, which are grounded on the assumption that high-dimensional\ntemporal data align with a Union-of-Subspaces (UoS) distribution. However, the\nframes in video capturing complex human motions with cluttered backgrounds may\nnot align well with the UoS distribution. In this paper, we propose a novel\napproach for HMS, named Temporal Rate Reduction Clustering\n($\\text{TR}^2\\text{C}$), which jointly learns structured representations and\naffinity to segment the frame sequences in video. Specifically, the structured\nrepresentations learned by $\\text{TR}^2\\text{C}$ maintain temporally consistent\nand align well with a UoS structure, which is favorable for the HMS task. We\nconduct extensive experiments on five benchmark HMS datasets and achieve\nstate-of-the-art performances with different feature extractors.", "comment": "The paper is accepted by ICCV 2025. The first two authors are equally\n  contributed", "pdf_url": "http://arxiv.org/pdf/2506.21249v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21249v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "人体运动分割中的时序速率降低聚类", "tldr": "提出了一种新的时序速率降低聚类（TR^2C）方法，用于人体运动分割，解决了传统子空间聚类在复杂背景下对UoS假设不适用的问题，并在基准数据集上达到了最先进的性能。", "motivation": "现有的人体运动分割方法主要依赖于子空间聚类，其假设高维时序数据符合子空间联合（UoS）分布。然而，包含复杂人体运动和杂乱背景的视频帧可能不符合UoS分布，导致现有方法性能受限。", "method": "提出了一种名为时序速率降低聚类（TR^2C）的新方法，用于人体运动分割。TR^2C联合学习结构化表示和亲和力来分割视频中的帧序列。其学习到的结构化表示能够保持时间上的一致性，并良好地与UoS结构对齐。", "result": "在五个基准人体运动分割数据集上进行了广泛实验，并使用不同的特征提取器取得了最先进的性能。", "conclusion": "TR^2C通过学习时序一致且与UoS结构良好对齐的结构化表示，有效解决了复杂人体运动分割中传统子空间聚类方法的局限性，并实现了卓越的性能。", "translation": "人体运动分割（HMS）旨在将视频划分为不重叠的人体运动，最近引起了越来越多的研究关注。现有的HMS方法主要由子空间聚类方法主导，这些方法基于高维时序数据与子空间联合（UoS）分布对齐的假设。然而，捕获复杂人体运动和杂乱背景的视频帧可能无法很好地与UoS分布对齐。在本文中，我们提出了一种新颖的HMS方法，名为时序速率降低聚类（TR^2C），它联合学习结构化表示和亲和力来分割视频中的帧序列。具体而言，TR^2C学习到的结构化表示保持时间上的一致性，并与UoS结构良好对齐，这有利于HMS任务。我们在五个基准HMS数据集上进行了广泛实验，并使用不同的特征提取器取得了最先进的性能。", "summary": "本文提出一种名为时序速率降低聚类（TR^2C）的新方法，用于人体运动分割。针对现有子空间聚类方法在复杂背景下难以满足子空间联合（UoS）分布假设的问题，TR^2C通过联合学习时间一致且与UoS结构良好对齐的结构化表示和亲和力，有效分割视频帧序列。实验结果表明，TR^2C在多个基准数据集上取得了最先进的性能。", "keywords": "人体运动分割, 时序速率降低聚类, 子空间聚类, 结构化表示, 视频分割", "comments": "这篇论文的创新点在于提出了TR^2C，解决了传统子空间聚类方法在复杂人体运动分割中对UoS假设的局限性。通过学习时间一致的结构化表示，该方法能够更好地适应真实世界的复杂视频数据，并在性能上超越现有技术，具有重要的实际应用价值。"}}
{"id": "2506.21429", "title": "Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort", "authors": ["Franco Rugolon", "Thomas Jack Samuels", "Stephan Hau", "Lennart Högman"], "summary": "This study investigates the efficacy of using multimodal machine learning\ntechniques to detect deception in dyadic interactions, focusing on the\nintegration of data from both the deceiver and the deceived. We compare early\nand late fusion approaches, utilizing audio and video data - specifically,\nAction Units and gaze information - across all possible combinations of\nmodalities and participants. Our dataset, newly collected from Swedish native\nspeakers engaged in truth or lie scenarios on emotionally relevant topics,\nserves as the basis for our analysis. The results demonstrate that\nincorporating both speech and facial information yields superior performance\ncompared to single-modality approaches. Moreover, including data from both\nparticipants significantly enhances deception detection accuracy, with the best\nperformance (71%) achieved using a late fusion strategy applied to both\nmodalities and participants. These findings align with psychological theories\nsuggesting differential control of facial and vocal expressions during initial\ninteractions. As the first study of its kind on a Scandinavian cohort, this\nresearch lays the groundwork for future investigations into dyadic\ninteractions, particularly within psychotherapy settings.", "comment": "40 pages, 2 figures, 2 tables. To be submitted in Behavior Research\n  Methods", "pdf_url": "http://arxiv.org/pdf/2506.21429v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21429v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用多模态机器学习检测双人交流中的欺骗：一项针对瑞典人群体的研究", "tldr": "本研究利用多模态机器学习（音频和视频数据）在瑞典人群体中检测双人交流中的欺骗，发现结合双方参与者的数据和晚期融合策略能显著提高检测准确率（最高71%）。", "motivation": "该研究旨在探究使用多模态机器学习技术检测双人互动中欺骗的有效性，并侧重于整合欺骗者和被欺骗者双方的数据。", "method": "研究比较了早期和晚期融合方法，利用音频和视频数据（特别是动作单元和凝视信息），涵盖了所有可能的模态和参与者组合。数据集是从参与真实或谎言情景的瑞典母语者那里新收集的。", "result": "结果表明，结合语音和面部信息比单一模态方法表现更优。此外，纳入双方参与者的数据显著提高了欺骗检测的准确性，最佳性能（71%）是通过对两种模态和参与者应用晚期融合策略实现的。", "conclusion": "这些发现与心理学理论相符，表明在初始互动中面部和声音表情的差异控制。作为针对斯堪的纳维亚人群体的首次同类研究，这项研究为未来对双人互动（特别是在心理治疗环境中）的调查奠定了基础。", "translation": "本研究调查了使用多模态机器学习技术在双人互动中检测欺骗的有效性，重点关注整合欺骗者和被欺骗者双方的数据。我们比较了早期和晚期融合方法，利用音频和视频数据——特别是动作单元和凝视信息——跨所有可能的模态和参与者组合。我们的数据集是新收集的，来自参与情感相关话题的真实或谎言情景的瑞典母语者，作为我们分析的基础。结果表明，结合语音和面部信息比单一模态方法表现更优。此外，纳入双方参与者的数据显著提高了欺骗检测的准确性，最佳性能（71%）是通过对两种模态和参与者应用晚期融合策略实现的。这些发现与心理学理论相符，表明在初始互动中面部和声音表情的差异控制。作为针对斯堪的纳维亚人群体的首次同类研究，这项研究为未来对双人互动，特别是在心理治疗环境中的调查奠定了基础。", "summary": "本研究探讨了在双人交流中利用多模态机器学习检测欺骗的有效性，特别关注整合欺骗者和被欺骗者双方的音频和视频数据。通过比较早期和晚期融合策略，研究发现结合语音和面部信息，并纳入双方参与者的数据，能显著提高欺骗检测的准确性。其中，晚期融合策略在结合所有模态和参与者数据时达到了71%的最佳性能。这项针对瑞典人群体的研究为未来在心理治疗等领域进行双人互动欺骗检测研究奠定了基础。", "keywords": "欺骗检测, 多模态机器学习, 双人交流, 音频视频融合, 瑞典人群体", "comments": "这项研究的创新之处在于它是首次针对斯堪的纳维亚人群体进行的多模态欺骗检测研究，并且强调了整合双方参与者数据的重要性。其发现与心理学理论相符，并为未来在心理治疗等实际应用中的欺骗检测研究奠定了基础。"}}
{"id": "2506.21260", "title": "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic", "authors": ["Munish Monga", "Vishal Chudasama", "Pankaj Wasnik", "Biplab Banerjee"], "summary": "Real-world object detection systems, such as those in autonomous driving and\nsurveillance, must continuously learn new object categories and simultaneously\nadapt to changing environmental conditions. Existing approaches, Class\nIncremental Object Detection (CIOD) and Domain Incremental Object Detection\n(DIOD) only address one aspect of this challenge. CIOD struggles in unseen\ndomains, while DIOD suffers from catastrophic forgetting when learning new\nclasses, limiting their real-world applicability. To overcome these\nlimitations, we introduce Dual Incremental Object Detection (DuIOD), a more\npractical setting that simultaneously handles class and domain shifts in an\nexemplar-free manner. We propose DuET, a Task Arithmetic-based model merging\nframework that enables stable incremental learning while mitigating sign\nconflicts through a novel Directional Consistency Loss. Unlike prior methods,\nDuET is detector-agnostic, allowing models like YOLO11 and RT-DETR to function\nas real-time incremental object detectors. To comprehensively evaluate both\nretention and adaptation, we introduce the Retention-Adaptability Index (RAI),\nwhich combines the Average Retention Index (Avg RI) for catastrophic forgetting\nand the Average Generalization Index for domain adaptability into a common\nground. Extensive experiments on the Pascal Series and Diverse Weather Series\ndemonstrate DuET's effectiveness, achieving a +13.12% RAI improvement while\npreserving 89.3% Avg RI on the Pascal Series (4 tasks), as well as a +11.39%\nRAI improvement with 88.57% Avg RI on the Diverse Weather Series (3 tasks),\noutperforming existing methods.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21260v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21260v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DuET：基于无样本任务算术的双增量目标检测", "tldr": "DuET提出了一种双增量目标检测（DuIOD）框架，通过任务算术和方向一致性损失，在不使用样本的情况下同时处理类别和域增量学习，显著提高了性能。", "motivation": "现有的类别增量目标检测（CIOD）和域增量目标检测（DIOD）方法分别在未见领域和学习新类别时存在灾难性遗忘问题，限制了它们在自动驾驶和监控等实际应用中的部署。", "method": "本文提出了双增量目标检测（DuIOD）这一更实用的设置，能够同时处理类别和域漂移，且无需样本。为此，提出了DuET框架，一个基于任务算术的模型合并框架，通过新颖的方向一致性损失来缓解符号冲突，实现稳定的增量学习。DuET是检测器无关的，可兼容YOLO11和RT-DETR等模型。同时引入了保留-适应性指数（RAI）来综合评估保留能力和适应能力。", "result": "在Pascal Series（4个任务）上，DuET的RAI提高了+13.12%，同时保持了89.3%的平均保留指数（Avg RI）。在Diverse Weather Series（3个任务）上，RAI提高了+11.39%，同时保持了88.57%的Avg RI，性能优于现有方法。", "conclusion": "DuET通过其独特的无样本任务算术方法，成功解决了双增量目标检测中的挑战，在保留旧知识和适应新领域方面取得了显著进步，使其成为一种更实用和高效的实时增量目标检测解决方案。", "translation": "DuET：双增量目标检测通过无样本任务算术\n\n在自动驾驶和监控等现实世界目标检测系统中，必须持续学习新的目标类别并同时适应不断变化的环境条件。现有方法，类别增量目标检测（CIOD）和域增量目标检测（DIOD）仅解决了这一挑战的一个方面。CIOD在未见领域中表现不佳，而DIOD在学习新类别时遭受灾难性遗忘，限制了它们的实际应用性。为了克服这些限制，我们引入了双增量目标检测（DuIOD），这是一个更实用的设置，可以同时以无样本的方式处理类别和域的转变。我们提出了DuET，一个基于任务算术的模型合并框架，通过新颖的方向一致性损失来缓解符号冲突，从而实现稳定的增量学习。与之前的方法不同，DuET是检测器无关的，允许YOLO11和RT-DETR等模型作为实时增量目标检测器运行。为了全面评估保留和适应能力，我们引入了保留-适应性指数（RAI），它将用于灾难性遗忘的平均保留指数（Avg RI）和用于域适应性的平均泛化指数结合到一个共同的基准中。在Pascal Series和Diverse Weather Series上的大量实验证明了DuET的有效性，在Pascal Series（4个任务）上实现了+13.12%的RAI改进，同时保留了89.3%的Avg RI，以及在Diverse Weather Series（3个任务）上实现了+11.39%的RAI改进，同时保留了88.57%的Avg RI，优于现有方法。", "summary": "本文提出DuET框架，用于解决双增量目标检测（DuIOD）问题，即在无样本条件下同时处理类别和域增量学习。针对现有CIOD和DIOD方法的局限性，DuET采用基于任务算术的模型合并方法和方向一致性损失，实现了稳定的增量学习并缓解了灾难性遗忘。DuET是检测器无关的，可应用于多种实时检测器。为全面评估，引入了保留-适应性指数（RAI）。实验结果表明，DuET在多个数据集上显著提升了RAI，并有效保持了知识保留。", "keywords": "双增量目标检测, 任务算术, 无样本学习, 增量学习, 灾难性遗忘", "comments": "DuET的创新之处在于提出了双增量目标检测（DuIOD）这一更贴近实际的设置，并首次通过无样本的任务算术方法来解决这一复杂问题。其检测器无关的特性大大提升了实用性。引入的保留-适应性指数（RAI）为评估增量学习模型提供了更全面的视角，兼顾了保留和适应能力，对领域发展具有重要意义。"}}
{"id": "2506.21270", "title": "Video Virtual Try-on with Conditional Diffusion Transformer Inpainter", "authors": ["Cheng Zou", "Senlin Cheng", "Bolei Xu", "Dandan Zheng", "Xiaobo Li", "Jingdong Chen", "Ming Yang"], "summary": "Video virtual try-on aims to naturally fit a garment to a target person in\nconsecutive video frames. It is a challenging task, on the one hand, the output\nvideo should be in good spatial-temporal consistency, on the other hand, the\ndetails of the given garment need to be preserved well in all the frames.\nNaively using image-based try-on methods frame by frame can get poor results\ndue to severe inconsistency. Recent diffusion-based video try-on methods,\nthough very few, happen to coincide with a similar solution: inserting temporal\nattention into image-based try-on model to adapt it for video try-on task,\nwhich have shown improvements but there still exist inconsistency problems. In\nthis paper, we propose ViTI (Video Try-on Inpainter), formulate and implement\nvideo virtual try-on as a conditional video inpainting task, which is different\nfrom previous methods. In this way, we start with a video generation problem\ninstead of an image-based try-on problem, which from the beginning has a better\nspatial-temporal consistency. Specifically, at first we build a video\ninpainting framework based on Diffusion Transformer with full 3D\nspatial-temporal attention, and then we progressively adapt it for video\ngarment inpainting, with a collection of masking strategies and multi-stage\ntraining. After these steps, the model can inpaint the masked garment area with\nappropriate garment pixels according to the prompt with good spatial-temporal\nconsistency. Finally, as other try-on methods, garment condition is added to\nthe model to make sure the inpainted garment appearance and details are as\nexpected. Both quantitative and qualitative experimental results show that ViTI\nis superior to previous works.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.21270v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21270v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "视频虚拟试穿与条件扩散Transformer修复器", "tldr": "提出ViTI，将视频虚拟试穿视为条件视频修复任务，利用扩散Transformer实现更好的时空一致性和服装细节保留。", "motivation": "视频虚拟试穿面临时空一致性和服装细节保留的挑战。现有图像基试穿方法帧间不一致，扩散基视频试穿方法虽有改进但仍存在不一致问题。", "method": "提出ViTI，将视频虚拟试穿表述为条件视频修复任务。首先构建基于扩散Transformer的视频修复框架，采用全3D时空注意力。然后通过一系列掩码策略和多阶段训练逐步适应视频服装修复。最后加入服装条件以确保修复的服装外观和细节符合预期。", "result": "定量和定性实验结果表明ViTI优于现有工作。", "conclusion": "ViTI通过将视频虚拟试穿重新定义为条件视频修复任务，并利用扩散Transformer和特定的训练策略，成功解决了现有方法的时空一致性问题，并有效保留了服装细节，取得了优异的性能。", "translation": "视频虚拟试穿旨在自然地将服装适配到连续视频帧中的目标人物。这是一项具有挑战性的任务，一方面，输出视频应具有良好的时空一致性；另一方面，给定服装的细节需要在所有帧中得到很好的保留。简单地逐帧使用基于图像的试穿方法会因为严重的不一致性而得到糟糕的结果。最近基于扩散的视频试穿方法，尽管数量很少，但恰好不谋而合地采取了类似的解决方案：将时间注意力插入基于图像的试穿模型中，以使其适应视频试穿任务，这虽然显示出改进，但仍然存在不一致性问题。在本文中，我们提出了ViTI（视频试穿修复器），将视频虚拟试穿表述并实现为一个条件视频修复任务，这与以往的方法不同。通过这种方式，我们从一个视频生成问题而不是基于图像的试穿问题开始，这从一开始就具有更好的时空一致性。具体来说，我们首先构建了一个基于扩散Transformer的视频修复框架，该框架具有完整的3D时空注意力，然后我们通过一系列掩码策略和多阶段训练逐步将其适应于视频服装修复。经过这些步骤后，模型可以根据提示使用适当的服装像素修复被遮罩的服装区域，并具有良好的时空一致性。最后，与其他试穿方法一样，将服装条件添加到模型中，以确保修复后的服装外观和细节符合预期。定量和定性实验结果均表明ViTI优于以往的工作。", "summary": "本文提出了ViTI，一种将视频虚拟试穿视为条件视频修复任务的新方法。该方法基于扩散Transformer构建，利用全3D时空注意力以及多阶段训练和掩码策略，旨在解决现有视频试穿方法中存在的时空不一致性和服装细节保留问题。实验结果表明ViTI在时空一致性和服装细节保留方面优于现有技术。", "keywords": "视频虚拟试穿, 条件扩散模型, Transformer, 视频修复, 时空一致性", "comments": "本文的创新点在于将视频虚拟试穿任务重新定义为条件视频修复，并利用扩散Transformer的强大能力来处理时空一致性。通过3D时空注意力、掩码策略和多阶段训练，有效地解决了现有方法的缺陷，为视频虚拟试穿领域提供了一个新颖且高效的解决方案。"}}
{"id": "2506.21461", "title": "A Keyword-Based Technique to Evaluate Broad Question Answer Script", "authors": ["Tamim Al Mahmud", "Md Gulzar Hussain", "Sumaiya Kabir", "Hasnain Ahmad", "Mahmudus Sobhan"], "summary": "Evaluation is the method of assessing and determining the educational system\nthrough various techniques such as verbal or viva-voice test, subjective or\nobjective written test. This paper presents an efficient solution to evaluate\nthe subjective answer script electronically. In this paper, we proposed and\nimplemented an integrated system that examines and evaluates the written answer\nscript. This article focuses on finding the keywords from the answer script and\nthen compares them with the keywords that have been parsed from both open and\nclosed domain. The system also checks the grammatical and spelling errors in\nthe answer script. Our proposed system tested with answer scripts of 100\nstudents and gives precision score 0.91.", "comment": "ACM Conference Proceedings (9 Pages)", "pdf_url": "http://arxiv.org/pdf/2506.21461v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21461v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "一种基于关键词的开放式问题答案评估技术", "tldr": "论文提出了一种基于关键词的集成系统，用于电子化评估主观题答案，并检查语法和拼写错误，在100份样本上达到了0.91的准确率。", "motivation": "旨在提供一种高效的电子化解决方案来评估主观题答案卷。", "method": "论文提出并实现了一个集成系统，该系统通过从答案脚本中查找关键词并与开放域和封闭域中解析的关键词进行比较来评估答案。该系统还检查答案脚本中的语法和拼写错误。", "result": "所提出的系统在100名学生的答案脚本上进行了测试，并获得了0.91的准确率。", "conclusion": "该系统能够高效地电子化评估主观题答案，并通过关键词匹配和错误检查提供高精度评估。", "translation": "评估是通过口头或口试、主观或客观书面测试等各种技术评估和确定教育系统的方法。本文提出了一种高效的电子化评估主观题答案卷的解决方案。在本文中，我们提出并实现了一个检查和评估书面答案卷的集成系统。本文重点在于从答案卷中查找关键词，然后将其与从开放域和封闭域中解析出的关键词进行比较。该系统还检查答案卷中的语法和拼写错误。我们提出的系统使用100名学生的答案卷进行了测试，并给出了0.91的准确率。", "summary": "本文提出并实现了一种基于关键词的集成系统，用于电子化评估主观题答案。该系统通过提取答案中的关键词并与预设关键词进行比对来评估内容，同时检查语法和拼写错误。实验结果表明，该系统在100份学生答案脚本上的准确率达到了0.91，为教育评估提供了一种高效的自动化方法。", "keywords": "关键词评估, 主观题评估, 电子评估, 答案脚本, 语法检查", "comments": "该研究提供了一种实用的自动化评估主观题答案的方法，其创新点在于结合了关键词匹配和语法拼写检查。0.91的准确率表明了其有效性，但在实际应用中可能需要更广泛的测试集和对不同学科主观题的适应性进行验证。"}}
{"id": "2506.21276", "title": "WordCon: Word-level Typography Control in Scene Text Rendering", "authors": ["Wenda Shi", "Yiren Song", "Zihan Rao", "Dengming Zhang", "Jiaming Liu", "Xingxing Zou"], "summary": "Achieving precise word-level typography control within generated images\nremains a persistent challenge. To address it, we newly construct a word-level\ncontrolled scene text dataset and introduce the Text-Image Alignment (TIA)\nframework. This framework leverages cross-modal correspondence between text and\nlocal image regions provided by grounding models to enhance the Text-to-Image\n(T2I) model training. Furthermore, we propose WordCon, a hybrid\nparameter-efficient fine-tuning (PEFT) method. WordCon reparameterizes\nselective key parameters, improving both efficiency and portability. This\nallows seamless integration into diverse pipelines, including artistic text\nrendering, text editing, and image-conditioned text rendering. To further\nenhance controllability, the masked loss at the latent level is applied to\nguide the model to concentrate on learning the text region in the image, and\nthe joint-attention loss provides feature-level supervision to promote\ndisentanglement between different words. Both qualitative and quantitative\nresults demonstrate the superiority of our method to the state of the art. The\ndatasets and source code will be available for academic use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21276v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21276v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "WordCon：场景文本渲染中的词级别排版控制", "tldr": "WordCon 引入了一个新的数据集和混合参数高效微调方法，以实现图像中精确的词级别排版控制，并在文本到图像模型中表现出优越性。", "motivation": "在生成的图像中实现精确的词级别排版控制仍然是一个持续的挑战。", "method": "为了解决词级别排版控制的挑战，本研究构建了一个词级别控制的场景文本数据集，并引入了文本-图像对齐（TIA）框架，该框架利用接地模型提供的文本和局部图像区域之间的跨模态对应关系来增强文本到图像（T2I）模型训练。此外，提出了WordCon，这是一种混合参数高效微调（PEFT）方法，它重新参数化选择性关键参数，提高了效率和可移植性。为了进一步增强可控性，在潜在级别应用了掩码损失以引导模型集中学习图像中的文本区域，并且联合注意力损失提供了特征级别的监督，以促进不同单词之间的解耦。", "result": "定性和定量结果都表明，本方法优于现有技术。", "conclusion": "WordCon 通过新数据集、TIA 框架和混合PEFT方法，成功解决了图像中词级别排版控制的挑战，并在文本到图像渲染中取得了最先进的性能。", "translation": "在生成的图像中实现精确的词级别排版控制仍然是一个持续的挑战。为了解决这个问题，我们新建了一个词级别控制的场景文本数据集，并引入了文本-图像对齐（TIA）框架。该框架利用接地模型提供的文本和局部图像区域之间的跨模态对应关系来增强文本到图像（T2I）模型训练。此外，我们提出了WordCon，这是一种混合参数高效微调（PEFT）方法。WordCon 重新参数化选择性关键参数，提高了效率和可移植性。这使得它能够无缝集成到各种管道中，包括艺术文本渲染、文本编辑和图像条件文本渲染。为了进一步增强可控性，在潜在级别应用了掩码损失以引导模型集中学习图像中的文本区域，并且联合注意力损失提供了特征级别的监督，以促进不同单词之间的解耦。定性和定量结果都表明，我们的方法优于现有技术。数据集和源代码将可用于学术用途。", "summary": "本论文提出了WordCon，一种用于实现图像中精确词级别排版控制的方法。该方法通过构建一个新的词级别控制场景文本数据集和引入文本-图像对齐（TIA）框架来增强文本到图像（T2I）模型的训练。WordCon 还包含一种混合参数高效微调（PEFT）方法，以提高效率和可移植性。此外，通过应用潜在级别的掩码损失和联合注意力损失来增强可控性。实验结果表明，WordCon 在场景文本渲染方面优于现有技术。", "keywords": "词级别排版控制, 场景文本渲染, 参数高效微调, 文本-图像对齐, WordCon", "comments": "该论文的创新点在于提出了一个专门用于词级别排版控制的新数据集和WordCon混合PEFT方法，有效地解决了现有T2I模型在精确文本渲染方面的局限性。TIA框架和两种新型损失函数（掩码损失和联合注意力损失）的引入，进一步提升了模型对文本区域的关注和不同单词间的解耦能力，这对于高质量的场景文本生成至关重要。其高效性和可移植性也使其在实际应用中具有广阔前景。"}}
{"id": "2506.21465", "title": "Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage", "authors": ["Gavin Lee Goodship", "Luis Miralles-Pechuan", "Stephen O'Sullivan"], "summary": "Extended Stability Runge-Kutta (ESRK) methods are crucial for solving\nlarge-scale computational problems in science and engineering, including\nweather forecasting, aerodynamic analysis, and complex biological modelling.\nHowever, balancing accuracy, stability, and computational efficiency remains\nchallenging, particularly for high-order, low-storage schemes. This study\nintroduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)\napproach for automated heuristic discovery, optimising low-storage ESRK\nmethods. Unlike traditional approaches that rely on manually designed\nheuristics or exhaustive numerical searches, our method leverages GA-driven\nmutations for search-space exploration and an RL-inspired state transition\nmechanism to refine heuristic selection dynamically. This enables systematic\nparameter reduction, preserving fourth-order accuracy while significantly\nimproving computational efficiency.The proposed GA-RL heuristic optimisation\nframework is validated through rigorous testing on benchmark problems,\nincluding the 1D and 2D Brusselator systems and the steady-state Navier-Stokes\nequations. The best-performing heuristic achieves a 25\\% reduction in IPOPT\nruntime compared to traditional ESRK optimisation processes while maintaining\nnumerical stability and accuracy. These findings demonstrate the potential of\nadaptive heuristic discovery to improve resource efficiency in high-fidelity\nsimulations and broaden the applicability of low-storage Runge-Kutta methods in\nreal-world computational fluid dynamics, physics simulations, and other\ndemanding fields. This work establishes a new paradigm in heuristic\noptimisation for numerical methods, opening pathways for further exploration\nusing Deep RL and AutoML-based heuristic search", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21465v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21465v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "优化四阶龙格-库塔方法：一种提高效率和降低存储的动态启发式方法", "tldr": "一种新的混合遗传算法（GA）和强化学习（RL）方法被提出，用于优化低存储四阶龙格-库塔方法，在保持精度和稳定性的同时，将计算时间减少了25%。", "motivation": "在科学和工程领域的大规模计算问题中，平衡高阶、低存储的扩展稳定性龙格-库塔（ESRK）方法的精度、稳定性与计算效率是一个挑战。传统方法依赖手动设计启发式或穷举数值搜索，效率低下。", "method": "本研究引入了一种混合遗传算法（GA）和强化学习（RL）方法，用于自动化启发式发现，以优化低存储ESRK方法。该方法利用GA驱动的突变进行搜索空间探索，并采用RL启发的态势转换机制动态地优化启发式选择，从而实现系统参数减少并提高计算效率。", "result": "在基准问题上进行测试，表现最佳的启发式方法相比传统ESRK优化过程，使IPOPT运行时减少了25%，同时保持了数值稳定性和精度。", "conclusion": "该研究表明自适应启发式发现有潜力提高高保真模拟的资源效率，并拓宽低存储龙格-库塔方法的适用性，为数值方法的启发式优化建立了新范式。", "translation": "扩展稳定性龙格-库塔（ESRK）方法对于解决科学和工程中的大规模计算问题至关重要，包括天气预报、气动分析和复杂的生物建模。然而，平衡精度、稳定性和计算效率仍然是一个挑战，特别是对于高阶、低存储方案。本研究引入了一种混合遗传算法（GA）和强化学习（RL）方法，用于自动化启发式发现，优化低存储ESRK方法。与依赖手动设计启发式或穷举数值搜索的传统方法不同，我们的方法利用GA驱动的突变进行搜索空间探索，并利用RL启发的态势转换机制动态地优化启发式选择。这使得系统参数得以减少，同时保持四阶精度并显著提高计算效率。所提出的GA-RL启发式优化框架通过对基准问题（包括一维和二维Brusselator系统以及稳态Navier-Stokes方程）的严格测试得到验证。与传统的ESRK优化过程相比，表现最佳的启发式方法在保持数值稳定性和精度的同时，使IPOPT运行时减少了25%。这些发现证明了自适应启发式发现提高高保真模拟资源效率的潜力，并拓宽了低存储龙格-库塔方法在实际计算流体动力学、物理模拟和其他高要求领域的适用性。这项工作建立了数值方法启发式优化新范式，为使用深度强化学习和基于AutoML的启发式搜索进一步探索开辟了道路。", "summary": "本文提出了一种新颖的混合遗传算法（GA）和强化学习（RL）框架，用于优化低存储四阶扩展稳定性龙格-库塔（ESRK）方法。这种自动化的启发式发现方法有效地平衡了精度、稳定性和计算效率，克服了传统手动方法的局限性。该GA-RL框架在基准问题上进行测试，实现了IPOPT运行时25%的减少，同时保持了精度和稳定性，展示了其在高保真模拟和实际应用中的潜力。", "keywords": "龙格-库塔方法, 遗传算法, 强化学习, 启发式优化, 计算效率", "comments": "该工作通过结合遗传算法和强化学习进行数值方法的自动化启发式发现，超越了传统的手动或穷举搜索方法，具有创新性。其重要性在于显著提高了关键龙格-库塔方法的计算效率，同时保持了精度，这对大规模科学和工程模拟具有广泛的影响。它为启发式优化提出的范式转变是一个显著贡献。"}}
{"id": "2506.21287", "title": "HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation", "authors": ["Diego Biagini", "Nassir Navab", "Azade Farshad"], "summary": "Surgical Video Synthesis has emerged as a promising research direction\nfollowing the success of diffusion models in general-domain video generation.\nAlthough existing approaches achieve high-quality video generation, most are\nunconditional and fail to maintain consistency with surgical actions and\nphases, lacking the surgical understanding and fine-grained guidance necessary\nfor factual simulation. We address these challenges by proposing HieraSurg, a\nhierarchy-aware surgical video generation framework consisting of two\nspecialized diffusion models. Given a surgical phase and an initial frame,\nHieraSurg first predicts future coarse-grained semantic changes through a\nsegmentation prediction model. The final video is then generated by a\nsecond-stage model that augments these temporal segmentation maps with\nfine-grained visual features, leading to effective texture rendering and\nintegration of semantic information in the video space. Our approach leverages\nsurgical information at multiple levels of abstraction, including surgical\nphase, action triplets, and panoptic segmentation maps. The experimental\nresults on Cholecystectomy Surgical Video Generation demonstrate that the model\nsignificantly outperforms prior work both quantitatively and qualitatively,\nshowing strong generalization capabilities and the ability to generate higher\nframe-rate videos. The model exhibits particularly fine-grained adherence when\nprovided with existing segmentation maps, suggesting its potential for\npractical surgical applications.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21287v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21287v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HieraSurg：层次感知扩散模型用于手术视频生成", "tldr": "HieraSurg是一个层次感知的扩散模型，通过两阶段方法生成与手术动作和阶段一致的高质量手术视频，解决了现有模型缺乏语义一致性的问题。", "motivation": "现有手术视频生成方法大多是无条件的，未能保持与手术动作和阶段的一致性，缺乏手术理解和细粒度指导，无法进行事实模拟。", "method": "提出HieraSurg框架，包含两个专用扩散模型。首先，给定手术阶段和初始帧，通过分割预测模型预测未来粗粒度语义变化。然后，第二阶段模型将这些时间分割图与细粒度视觉特征结合，生成最终视频，实现有效的纹理渲染和语义信息集成。该方法利用多层抽象的手术信息，包括手术阶段、动作三元组和全景分割图。", "result": "在胆囊切除术视频生成实验中，该模型在定量和定性方面显著优于现有工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有分割图时，模型表现出特别细粒度的遵循性。", "conclusion": "HieraSurg模型在手术视频生成方面表现出色，特别是在结合现有分割图时，其潜在的实际手术应用价值很高。", "translation": "手术视频合成已成为一个有前景的研究方向，这得益于扩散模型在通用领域视频生成方面的成功。尽管现有方法实现了高质量的视频生成，但大多数是无条件的，未能保持与手术动作和阶段的一致性，缺乏事实模拟所需的手术理解和细粒度指导。我们通过提出HieraSurg来解决这些挑战，HieraSurg是一个层次感知的手术视频生成框架，由两个专门的扩散模型组成。给定手术阶段和初始帧，HieraSurg首先通过分割预测模型预测未来粗粒度语义变化。然后，第二阶段模型将这些时间分割图与细粒度视觉特征结合，生成最终视频，从而在视频空间中实现有效的纹理渲染和语义信息集成。我们的方法利用多层抽象的手术信息，包括手术阶段、动作三元组和全景分割图。在胆囊切除术视频生成上的实验结果表明，该模型在定量和定性方面都显著优于现有工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有分割图时，该模型表现出特别细粒度的遵循性，这表明其在实际手术应用中的潜力。", "summary": "HieraSurg提出一种层次感知扩散模型框架，通过两阶段模型利用手术阶段、动作三元组和全景分割图等多层抽象信息，解决了现有手术视频生成模型在保持动作和阶段一致性方面的不足。实验证明，HieraSurg在生成高质量、高帧率手术视频方面优于现有方法，并表现出良好的泛化能力和实际应用潜力。", "keywords": "手术视频生成, 扩散模型, 层次感知, 语义一致性, HieraSurg", "comments": "创新点在于提出了一个层次感知的双阶段扩散模型框架，有效地整合了多层级手术信息（阶段、动作、分割图），解决了现有模型在手术视频生成中缺乏语义一致性的问题。其能够生成更高帧率的视频并显示出强大的泛化能力，对于推动手术模拟和训练具有重要意义。局限性可能在于对高质量初始帧和分割图的依赖，以及模型复杂性带来的计算成本。"}}
{"id": "2506.21502", "title": "Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems", "authors": ["Francesco Vitale", "Nicola Dall'Ora", "Sebastiano Gaiardelli", "Enrico Fraccaroli", "Nicola Mazzocca", "Franco Fummi"], "summary": "Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring\nsystem dependability and operational efficiency by accurately detecting\nanomalies and identifying their root causes. However, the manual modeling of\nfaulty behaviors often demands extensive domain expertise and produces models\nthat are complex, error-prone, and difficult to interpret. To address this\nchallenge, we present a novel unsupervised fault diagnosis methodology that\nintegrates collective anomaly detection in multivariate time series, process\nmining, and stochastic simulation. Initially, collective anomalies are detected\nfrom low-level sensor data using multivariate time-series analysis. These\nanomalies are then transformed into structured event logs, enabling the\ndiscovery of interpretable process models through process mining. By\nincorporating timing distributions into the extracted Petri nets, the approach\nsupports stochastic simulation of faulty behaviors, thereby enhancing root\ncause analysis and behavioral understanding. The methodology is validated using\nthe Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart\nmanufacturing. Experimental results demonstrate its effectiveness in modeling,\nsimulating, and classifying faulty behaviors in CPSs. This enables the creation\nof comprehensive fault dictionaries that support predictive maintenance and the\ndevelopment of digital twins for industrial environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21502v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21502v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "过程挖掘驱动的建模与仿真以增强信息物理系统中的故障诊断", "tldr": "本文提出了一种结合多元时间序列异常检测、过程挖掘和随机仿真的无监督故障诊断方法，用于信息物理系统，并证明了其有效性。", "motivation": "传统的信息物理系统（CPS）故障诊断中，手动建模故障行为需要大量领域专业知识，且模型复杂、易错、难以解释。", "method": "本文提出了一种新颖的无监督故障诊断方法，该方法整合了多元时间序列中的集体异常检测、过程挖掘和随机仿真。具体而言，首先利用多元时间序列分析从低级传感器数据中检测集体异常，然后将这些异常转换为结构化事件日志，并通过过程挖掘发现可解释的过程模型（Petri网）。最后，通过将时间分布纳入提取的Petri网，支持故障行为的随机仿真，从而增强根本原因分析和行为理解。该方法使用机器人臂数据集（RoAD）进行了验证。", "result": "实验结果表明，该方法在建模、仿真和分类信息物理系统中的故障行为方面是有效的。它能够创建支持预测性维护和工业环境中数字孪生体开发的全面故障字典。", "conclusion": "本文提出的整合了多元时间序列异常检测、过程挖掘和随机仿真的无监督故障诊断方法，有效解决了信息物理系统故障诊断中手动建模的挑战，并为预测性维护和数字孪生体提供了支持。", "translation": "信息物理系统（CPS）中的故障诊断对于通过准确检测异常和识别其根本原因来确保系统可靠性和运行效率至关重要。然而，故障行为的手动建模通常需要大量的领域专业知识，并且产生的模型复杂、容易出错且难以解释。为了解决这一挑战，我们提出了一种新颖的无监督故障诊断方法，该方法整合了多元时间序列中的集体异常检测、过程挖掘和随机仿真。最初，利用多元时间序列分析从低级传感器数据中检测集体异常。然后，这些异常被转换为结构化事件日志，通过过程挖掘发现可解释的过程模型。通过将时间分布纳入提取的Petri网，该方法支持故障行为的随机仿真，从而增强了根本原因分析和行为理解。该方法使用机器人臂数据集（RoAD）（智能制造领域广泛认可的基准）进行了验证。实验结果证明了其在信息物理系统中建模、仿真和分类故障行为的有效性。这使得能够创建支持预测性维护和工业环境中数字孪生体开发的全面故障字典。", "summary": "本文针对信息物理系统（CPS）中手动故障建模的复杂性和局限性，提出了一种新颖的无监督故障诊断方法。该方法结合了多元时间序列异常检测、过程挖掘和随机仿真技术。通过将传感器数据中的集体异常转化为事件日志，并利用过程挖掘发现可解释的过程模型（Petri网），再通过随机仿真增强根本原因分析。该方法在机器人臂数据集上得到验证，实验结果表明其在建模、仿真和分类故障行为方面的有效性，有助于创建故障字典以支持预测性维护和数字孪生体开发。", "keywords": "故障诊断, 信息物理系统, 过程挖掘, 异常检测, 随机仿真", "comments": "本文的创新之处在于将多种先进技术（多元时间序列分析、过程挖掘、随机仿真）融合到一个无监督的故障诊断框架中，有效解决了传统手动建模的痛点。其通过自动化和数据驱动的方式提升了故障行为的可解释性和根本原因分析能力，对于实现智能制造中的预测性维护和数字孪生体具有重要意义。"}}
{"id": "2506.21312", "title": "Continual Self-Supervised Learning with Masked Autoencoders in Remote Sensing", "authors": ["Lars Möllenbrok", "Behnood Rasti", "Begüm Demir"], "summary": "The development of continual learning (CL) methods, which aim to learn new\ntasks in a sequential manner from the training data acquired continuously, has\ngained great attention in remote sensing (RS). The existing CL methods in RS,\nwhile learning new tasks, enhance robustness towards catastrophic forgetting.\nThis is achieved by using a large number of labeled training samples, which is\ncostly and not always feasible to gather in RS. To address this problem, we\npropose a novel continual self-supervised learning method in the context of\nmasked autoencoders (denoted as CoSMAE). The proposed CoSMAE consists of two\ncomponents: i) data mixup; and ii) model mixup knowledge distillation. Data\nmixup is associated with retaining information on previous data distributions\nby interpolating images from the current task with those from the previous\ntasks. Model mixup knowledge distillation is associated with distilling\nknowledge from past models and the current model simultaneously by\ninterpolating their model weights to form a teacher for the knowledge\ndistillation. The two components complement each other to regularize the MAE at\nthe data and model levels to facilitate better generalization across tasks and\nreduce the risk of catastrophic forgetting. Experimental results show that\nCoSMAE achieves significant improvements of up to 4.94% over state-of-the-art\nCL methods applied to MAE. Our code is publicly available at:\nhttps://git.tu-berlin.de/rsim/CoSMAE.", "comment": "Accepted to IEEE Geoscience and Remote Sensing Letters. Our code is\n  available at https://git.tu-berlin.de/rsim/CoSMAE", "pdf_url": "http://arxiv.org/pdf/2506.21312v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21312v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "遥感领域基于掩码自编码器的持续自监督学习", "tldr": "提出CoSMAE，一种新的持续自监督学习方法，通过数据混合和模型混合知识蒸馏，在遥感中减少灾难性遗忘，且无需大量标注数据。", "motivation": "现有的遥感持续学习方法在减少灾难性遗忘方面依赖于大量昂贵的标注训练样本，这在遥感领域难以实现。", "method": "提出CoSMAE（Continual Self-Supervised Learning with Masked Autoencoders），包含两个组件：1）数据混合：通过将当前任务图像与先前任务图像进行插值来保留之前数据分布的信息；2）模型混合知识蒸馏：通过插值过去模型和当前模型的权重来形成一个教师模型进行知识蒸馏。这两个组件在数据和模型层面协同作用，以正则化掩码自编码器，从而更好地泛化并减少灾难性遗忘。", "result": "CoSMAE在应用于MAE的现有持续学习方法上实现了显著改进，性能提升高达4.94%。", "conclusion": "CoSMAE通过其独特的数据和模型混合机制，有效解决了遥感领域持续学习中灾难性遗忘和对大量标注数据依赖的问题，并取得了优于现有方法的性能。", "translation": "持续学习（CL）方法旨在从连续获取的训练数据中顺序学习新任务，其发展在遥感（RS）领域获得了极大关注。遥感中现有的持续学习方法在学习新任务的同时，增强了对灾难性遗忘的鲁棒性。这通常通过使用大量标注训练样本实现，但在遥感中，这既昂贵又并非总是可行。为了解决这个问题，我们提出了一种在掩码自编码器（MAE）背景下的新型持续自监督学习方法（记作CoSMAE）。所提出的CoSMAE包含两个组件：i）数据混合；ii）模型混合知识蒸馏。数据混合通过将当前任务的图像与先前任务的图像进行插值，以保留先前数据分布的信息。模型混合知识蒸馏通过插值过去模型和当前模型的权重来形成一个教师模型，用于知识蒸馏。这两个组件相互补充，在数据和模型层面正则化MAE，以促进更好的跨任务泛化并降低灾难性遗忘的风险。实验结果表明，CoSMAE在应用于MAE的现有持续学习方法上取得了显著改进，性能提升高达4.94%。我们的代码已公开可用，地址为：https://git.tu-berlin.de/rsim/CoSMAE。", "summary": "本文提出CoSMAE，一种针对遥感领域持续自监督学习的新方法，旨在解决现有持续学习方法对大量标注数据的依赖和灾难性遗忘问题。CoSMAE通过数据混合和模型混合知识蒸馏双重机制，在数据和模型层面正则化掩码自编码器，从而有效保留旧知识、促进新知识学习，并提高跨任务泛化能力。实验证明，CoSMAE在性能上显著优于现有最先进的持续学习方法。", "keywords": "持续学习, 自监督学习, 掩码自编码器, 遥感, 灾难性遗忘", "comments": "这篇论文的创新点在于将数据混合和模型混合知识蒸馏相结合，应用于持续自监督学习框架，特别是在遥感这种数据标注昂贵的领域。它有效地解决了持续学习中的核心挑战——灾难性遗忘，同时避免了对大量标注数据的需求，这对于实际应用具有重要意义。"}}
{"id": "2506.21550", "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale", "authors": ["Xiaona Zhou", "Constantin Brif", "Ismini Lourentzou"], "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21550v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21550v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "mTSBench：大规模多元时间序列异常检测和模型选择基准测试", "tldr": "mTSBench是一个大型基准测试平台，用于评估多元时间序列异常检测和无监督模型选择方法，发现没有单一检测器表现最佳，且现有模型选择方法仍有很大提升空间。", "motivation": "多元时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于复杂的变量间依赖、时间动态和稀疏的异常标签而充满挑战。现有研究缺乏统一的大规模评估和模型选择基准。", "method": "本研究引入了mTSBench，这是迄今为止最大的MTS-AD和无监督模型选择基准测试平台。它涵盖19个数据集、12个应用领域共344个带标签时间序列，评估了24种异常检测方法（包括基于LLM的方法），并在标准化条件下系统地对无监督模型选择技术进行基准测试。", "result": "研究结果证实没有单一检测器在所有数据集上表现最佳，这强调了模型选择的重要性。然而，即使是最先进的模型选择方法也远未达到最优，揭示了关键的差距。", "conclusion": "mTSBench提供了一个统一的评估套件，以实现严格、可复现的比较，并促进自适应异常检测和鲁棒模型选择的未来进展。", "translation": "多元时间序列异常检测（MTS-AD）在医疗保健、网络安全和工业监控等领域至关重要，但由于复杂的变量间依赖、时间动态和稀疏的异常标签，它仍然充满挑战。我们引入了mTSBench，这是迄今为止最大的MTS-AD和无监督模型选择基准测试平台，涵盖19个数据集、12个不同应用领域的344个带标签时间序列。mTSBench评估了24种异常检测方法，包括用于多元时间序列的大型语言模型（LLM）检测器，并在标准化条件下系统地对无监督模型选择技术进行基准测试。与先前的发现一致，我们的结果证实没有单一检测器在所有数据集上表现出色，这突显了模型选择的重要性。然而，即使是最先进的选择方法也远未达到最优，这揭示了关键的差距。mTSBench提供了一个统一的评估套件，以实现严格、可复现的比较，并促进自适应异常检测和鲁棒模型选择的未来进展。", "summary": "mTSBench是一个针对多元时间序列异常检测（MTS-AD）和无监督模型选择的大规模基准测试平台。它整合了大量数据集和多种检测方法，包括基于LLM的模型，旨在提供标准化评估。研究发现没有单一的MTS-AD方法能普适最优，且现有模型选择技术仍有显著提升空间。mTSBench旨在促进未来自适应异常检测和鲁棒模型选择的研究。", "keywords": "多元时间序列异常检测, 模型选择, 基准测试, 无监督, LLM", "comments": "这篇论文通过引入迄今为止最大的多元时间序列异常检测基准mTSBench，解决了该领域缺乏统一评估标准的痛点。其创新之处在于数据集的广度和深度，以及对LLM-based检测器的纳入和对无监督模型选择的系统性评估。研究结果强调了模型选择的重要性，并指出了当前模型选择方法的局限性，为未来的研究指明了方向。mTSBench有望成为推动MTS-AD领域进展的重要工具。"}}
{"id": "2506.21316", "title": "DrishtiKon: Multi-Granular Visual Grounding for Text-Rich Document Images", "authors": ["Badri Vishal Kasuba", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "summary": "Visual grounding in text-rich document images is a critical yet underexplored\nchallenge for document intelligence and visual question answering (VQA)\nsystems. We present \\drishtikon, a multi-granular visual grounding framework\ndesigned to enhance interpretability and trust in VQA for complex, multilingual\ndocuments. Our approach integrates robust multi-lingual OCR, large language\nmodels, and a novel region matching algorithm to accurately localize answer\nspans at block, line, word, and point levels. We curate a new benchmark from\nthe CircularsVQA test set, providing fine-grained, human-verified annotations\nacross multiple granularities. Extensive experiments demonstrate that our\nmethod achieves state-of-the-art grounding accuracy, with line-level\ngranularity offering the best trade-off between precision and recall. Ablation\nstudies further highlight the benefits of multi-block and multi-line reasoning.\nComparative evaluations with leading vision-language models reveal the\nlimitations of current VLMs in precise localization, underscoring the\neffectiveness of our structured, alignment-based approach. Our findings pave\nthe way for more robust and interpretable document understanding systems in\nreal-world, text-centric scenarios. Code and dataset has been made available at\nhttps://github.com/kasuba-badri-vishal/DhrishtiKon.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.21316v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21316v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DrishtiKon：文本丰富文档图像的多粒度视觉定位", "tldr": "DrishtiKon是一个多粒度视觉定位框架，通过整合多语言OCR、LLM和新颖的区域匹配算法，在文本丰富的文档图像中实现SOTA的定位精度，提高了VQA系统的可解释性。", "motivation": "文本丰富文档图像中的视觉定位对于文档智能和视觉问答（VQA）系统来说是一个关键但尚未充分探索的挑战。", "method": "该论文提出了DrishtiKon，一个多粒度视觉定位框架，旨在增强复杂多语言文档中VQA的可解释性和信任度。该方法整合了强大的多语言OCR、大型语言模型和新颖的区域匹配算法，以在块、行、单词和点级别精确定位答案跨度。此外，该研究还从CircularsVQA测试集中整理了一个新的基准，提供了跨多个粒度的细粒度、人工验证的标注。", "result": "广泛的实验表明，该方法实现了最先进的定位精度，其中行级粒度在精确度和召回率之间提供了最佳权衡。消融研究进一步强调了多块和多行推理的优势。与领先的视觉-语言模型的比较评估揭示了当前VLM在精确本地化方面的局限性，突出了该结构化、基于对齐的方法的有效性。", "conclusion": "该研究结果为在真实世界的、以文本为中心的场景中构建更鲁棒和可解释的文档理解系统铺平了道路。", "translation": "文本丰富文档图像中的视觉定位对于文档智能和视觉问答（VQA）系统来说是一个关键但尚未充分探索的挑战。我们提出了DrishtiKon，一个多粒度视觉定位框架，旨在增强复杂多语言文档中VQA的可解释性和信任度。我们的方法整合了强大的多语言OCR、大型语言模型和新颖的区域匹配算法，以在块、行、单词和点级别精确定位答案跨度。我们从CircularsVQA测试集中整理了一个新的基准，提供了跨多个粒度的细粒度、人工验证的标注。广泛的实验表明，我们的方法实现了最先进的定位精度，其中行级粒度在精确度和召回率之间提供了最佳权衡。消融研究进一步强调了多块和多行推理的优势。与领先的视觉-语言模型的比较评估揭示了当前VLM在精确本地化方面的局限性，突出了我们结构化、基于对齐的方法的有效性。我们的发现为在真实世界的、以文本为中心的场景中构建更鲁棒和可解释的文档理解系统铺平了道路。代码和数据集已在https://github.com/kasuba-badri-vishal/DhrishtiKon 上提供。", "summary": "DrishtiKon是一个创新的多粒度视觉定位框架，专为文本丰富的文档图像设计。它结合多语言OCR、LLM和新颖的区域匹配算法，在不同粒度（块、行、单词、点）上实现精确的答案定位。该框架通过新的基准进行验证，并在视觉问答（VQA）任务中取得了最先进的性能，尤其在行级定位上表现出最佳平衡，克服了现有视觉-语言模型在精确本地化方面的不足，为构建更可靠和可解释的文档理解系统奠定了基础。", "keywords": "视觉定位, 文档图像, 多粒度, 视觉问答, OCR", "comments": "DrishtiKon的创新之处在于其多粒度视觉定位能力，能够精确定位不同粒度（块、行、单词、点）的答案跨度。该框架通过整合强大的多语言OCR和大型语言模型，并结合新颖的区域匹配算法，有效解决了文本丰富文档图像中视觉定位的挑战。其在新的基准上取得的最先进性能，以及对现有视觉-语言模型局限性的克服，都凸显了其在文档智能和VQA领域的重要性和潜力，有助于提升系统在真实世界场景中的可解释性和鲁棒性。"}}
{"id": "2506.21551", "title": "Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test", "authors": ["Ziyue Li", "Chenrui Fan", "Tianyi Zhou"], "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21551v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.21551v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "在LLM预训练中何处寻找Grokking现象？无需测试即可监控记忆到泛化的转变", "tldr": "本研究首次在大规模语言模型（LLM）预训练中观察到Grokking现象，并揭示其机制：训练样本的内部通路从随机演变为结构化且复杂度降低，表明从记忆到泛化的转变。论文还提出了两个新颖的指标，可在无需测试的情况下监控泛化性能。", "motivation": "Grokking现象（训练损失收敛后测试性能仍持续提升）在神经网络训练中被发现，使得泛化机制和推理等新兴能力变得神秘。先前的研究主要在小型模型和玩具任务上进行，缺乏在大规模语言模型预训练中对Grokking现象的深入理解。", "method": "研究首次在7B大型语言模型（OLMoE）的一次性预训练检查点上进行Grokking研究。计算训练损失并在多种基准任务（包括数学推理、代码生成、常识/领域特定知识检索）上评估泛化能力。通过调查LLM内部动态，具体分析训练样本的通路（跨层专家选择），以揭示Grokking的“泛化出现”机制。开发了两个新颖的指标来量化通路距离和单个通路的复杂性。", "result": "1. 首次证实Grokking现象在大规模基础模型预训练中依然发生，尽管不同数据可能异步进入Grokking阶段。 2. 揭示Grokking期间训练样本的通路从随机、实例特定演变为更结构化且样本间可共享。 3. 发现尽管损失已收敛，样本通路复杂度仍降低，这表明记忆到泛化的转换，为延迟泛化提供了机制解释。 4. 开发的新指标能够预测各种下游任务的泛化改进，这些指标高效、易于计算且仅依赖于训练数据。", "conclusion": "Grokking现象在大规模LLM预训练中普遍存在，其内部机制与训练样本通路的结构化和复杂度降低有关。开发的通路指标具有实际价值，可用于无需微调和测试即可监控预训练中的泛化性能。理论上，更结构化的通路能降低模型复杂度并改善泛化界限。", "translation": "Grokking，即在训练损失收敛后测试性能仍持续提升的现象，最近在神经网络训练中被观察到，使得泛化机制以及推理等新兴能力变得神秘。虽然之前的研究通常在少数玩具或高度特定的任务上训练小型模型数千个周期，但我们首次对7B大型语言模型（LLM），即OLMoE，在一次性预训练过程中的检查点上进行Grokking研究。我们计算了训练损失，并在包括数学推理、代码生成以及常识/领域特定知识检索任务在内的多种基准任务上评估了泛化能力。 我们的研究首次证实Grokking现象在大规模基础模型预训练中依然发生，尽管不同的数据可能异步进入Grokking阶段。我们通过调查LLM内部动态，进一步揭示了Grokking的“泛化出现”机制。具体来说，我们发现训练样本的通路（即跨层专家选择）在Grokking期间从随机、实例特定演变为更结构化且样本间可共享。此外，尽管损失已收敛，但样本通路的复杂度仍有所降低。这些表明了从记忆到泛化的转换，为延迟泛化提供了机制解释。在这项研究中，我们开发了两个新颖的指标来量化通路距离和单个通路的复杂性。我们展示了它们预测各种下游任务泛化改进的能力。它们高效、简单易算，并且仅依赖于训练数据。因此，它们对预训练具有实用价值，使我们能够在不进行微调和测试的情况下监控泛化性能。理论上，我们表明更结构化的通路降低了模型复杂度并改善了泛化界限。", "summary": "本研究首次在大规模语言模型（7B LLM OLMoE）的预训练过程中观察并分析了Grokking现象。研究发现，Grokking不仅在大型模型中存在，而且其核心机制在于训练样本的内部通路（专家选择）从随机无序转变为结构化且复杂度降低，这标志着模型从记忆向泛化的转变。为实现无需测试的泛化监控，论文提出了两个新的通路指标，并验证了其预测下游任务泛化改进的有效性，这些指标高效且仅依赖于训练数据。该工作为理解和监控大型语言模型预训练中的泛化行为提供了新的视角和实用工具。", "keywords": "Grokking, LLM预训练, 泛化, 记忆到泛化, 通路动态", "comments": "这项研究具有重要的创新性。首先，它是首次在大规模LLM预训练中系统研究Grokking现象的工作，弥补了先前研究主要集中在小型模型和玩具任务上的空白。其次，论文通过深入分析LLM的内部通路动态，为Grokking现象中“泛化出现”提供了机制性解释，揭示了从记忆到泛化的转换过程。最后，提出的两个新颖的通路指标具有很高的实用价值，它们使得在不进行耗时的微调和测试的情况下监控模型泛化能力成为可能，这对于高效的大规模模型预训练具有深远意义。"}}
{"id": "2506.21154", "title": "Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation", "authors": ["He Li", "Haoang Chi", "Mingyu Liu", "Wanrong Huang", "Liyang Xu", "Wenjing Yang"], "summary": "The real world naturally has dimensions of time and space. Therefore,\nestimating the counterfactual outcomes with spatial-temporal attributes is a\ncrucial problem. However, previous methods are based on classical statistical\nmodels, which still have limitations in performance and generalization. This\npaper proposes a novel framework for estimating counterfactual outcomes with\nspatial-temporal attributes using the Transformer, exhibiting stronger\nestimation ability. Under mild assumptions, the proposed estimator within this\nframework is consistent and asymptotically normal. To validate the\neffectiveness of our approach, we conduct simulation experiments and real data\nexperiments. Simulation experiments show that our estimator has a stronger\nestimation capability than baseline methods. Real data experiments provide a\nvaluable conclusion to the causal effect of conflicts on forest loss in\nColombia. The source code is available at\nhttps://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.", "comment": "24 pages, accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.21154v1", "categories": ["stat.ME", "cs.AI", "cs.LG"], "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.21154v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于Transformer的时空反事实结果估计", "tldr": "本文提出了一种基于Transformer的时空反事实结果估计新框架，克服了传统统计方法的局限性，并在模拟和真实数据实验中验证了其更强的估计能力。", "motivation": "真实世界中具有时空属性的反事实结果估计是一个关键问题，但以往基于经典统计模型的方法在性能和泛化能力上存在局限性。", "method": "本文提出了一种利用Transformer来估计具有时空属性的反事实结果的新颖框架。在该框架下，所提出的估计器在温和假设下具有一致性和渐近正态性。通过模拟实验和真实数据实验来验证方法的有效性。", "result": "模拟实验表明，所提出的估计器比基线方法具有更强的估计能力。真实数据实验为哥伦比亚冲突对森林损失的因果效应提供了有价值的结论。", "conclusion": "所提出的基于Transformer的时空反事实估计框架能够有效且更强地估计反事实结果，克服了传统方法的局限性，并在实际应用中展现了潜力。", "translation": "真实世界自然具有时间和空间维度。因此，估计具有时空属性的反事实结果是一个关键问题。然而，以往的方法都是基于经典统计模型的，在性能和泛化能力上仍存在局限性。本文提出了一种利用Transformer来估计具有时空属性的反事实结果的新颖框架，展现出更强的估计能力。在温和假设下，该框架内提出的估计器是一致且渐近正态的。为了验证我们方法的有效性，我们进行了模拟实验和真实数据实验。模拟实验表明，我们的估计器比基线方法具有更强的估计能力。真实数据实验为哥伦比亚冲突对森林损失的因果效应提供了有价值的结论。源代码可在https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master获取。", "summary": "本文提出了一种新颖的基于Transformer的时空反事实结果估计框架，旨在克服传统统计模型在处理具有时空属性的反事实估计时存在的性能和泛化局限性。该框架下的估计器在理论上具有一致性和渐近正态性。通过模拟实验和针对哥伦比亚冲突对森林损失因果效应的真实数据实验，验证了该方法相比现有基线方法具有更强的估计能力和实际应用价值。", "keywords": "Transformer, 时空反事实估计, 因果推断, 机器学习, 森林损失", "comments": "该论文的创新点在于将Transformer模型引入到时空反事实结果估计这一关键问题中，有效解决了传统统计模型在复杂时空数据处理上的局限性。其理论上的严谨性（一致性和渐近正态性）以及在真实世界问题（冲突对森林损失的因果效应）中的成功应用，突显了其重要的研究价值和实际应用潜力。"}}
{"id": "2506.21317", "title": "LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "summary": "Current vision-language models (VLMs) are well-adapted for general visual\nunderstanding tasks. However, they perform inadequately when handling complex\nvisual tasks related to human poses and actions due to the lack of specialized\nvision-language instruction-following data. We introduce a method for\ngenerating such data by integrating human keypoints with traditional visual\nfeatures such as captions and bounding boxes, enabling more precise\nunderstanding of human-centric scenes. Our approach constructs a dataset\ncomprising 200,328 samples tailored to fine-tune models for human-centric\ntasks, focusing on three areas: conversation, detailed description, and complex\nreasoning. We establish an Extended Human Pose and Action Understanding\nBenchmark (E-HPAUB) to assess model performance on human pose and action\nunderstanding. We fine-tune the LLaVA-1.5-7B model using this dataset and\nevaluate our resulting LLaVA-Pose model on the benchmark, achieving significant\nimprovements. Experimental results show an overall improvement of 33.2%\ncompared to the original LLaVA-1.5-7B model. These findings highlight the\neffectiveness of keypoint-integrated data in enhancing multimodal models for\nhuman-centric visual understanding. Code is available at\nhttps://github.com/Ody-trek/LLaVA-Pose.", "comment": "arXiv admin note: substantial text overlap with arXiv:2409.09306", "pdf_url": "http://arxiv.org/pdf/2506.21317v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21317v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "LLaVA-Pose：通过关键点集成指令微调增强人体姿态和动作理解", "tldr": "LLaVA-Pose通过关键点集成指令微调，显著提升了现有视觉语言模型在人体姿态和动作理解方面的性能。", "motivation": "当前视觉语言模型在处理复杂的人体姿态和动作相关视觉任务时表现不佳，原因在于缺乏专门的视觉语言指令遵循数据。", "method": "提出一种数据生成方法，将人体关键点与传统视觉特征（如字幕和边界框）集成，以创建专门用于人体中心场景理解的数据。构建了一个包含200,328个样本的数据集，用于微调模型，专注于对话、详细描述和复杂推理。建立了扩展的人体姿态和动作理解基准（E-HPAUB）来评估模型性能。使用该数据集微调LLaVA-1.5-7B模型，得到LLaVA-Pose模型。", "result": "LLaVA-Pose模型在E-HPAUB基准上取得了显著改进，与原始LLaVA-1.5-7B模型相比，整体性能提升了33.2%。", "conclusion": "关键点集成数据在增强多模态模型进行以人为中心的视觉理解方面是有效的。", "translation": "当前视觉语言模型（VLMs）已很好地适应了一般视觉理解任务。然而，由于缺乏专门的视觉语言指令遵循数据，它们在处理与人体姿态和动作相关的复杂视觉任务时表现不足。我们引入了一种通过将人体关键点与传统视觉特征（如字幕和边界框）集成来生成此类数据的方法，从而能够更精确地理解以人为中心的场景。我们的方法构建了一个包含200,328个样本的数据集，专门用于微调以人为中心任务的模型，重点关注三个领域：对话、详细描述和复杂推理。我们建立了一个扩展的人体姿态和动作理解基准（E-HPAUB）来评估模型在人体姿态和动作理解方面的性能。我们使用该数据集对LLaVA-1.5-7B模型进行微调，并在基准上评估了我们得到的LLaVA-Pose模型，取得了显著改进。实验结果显示，与原始LLaVA-1.5-7B模型相比，整体性能提升了33.2%。这些发现强调了关键点集成数据在增强多模态模型进行以人为中心的视觉理解方面的有效性。代码可在https://github.com/Ody-trek/LLaVA-Pose获取。", "summary": "本文提出了LLaVA-Pose模型，旨在解决现有视觉语言模型在人体姿态和动作理解方面的不足。通过开发一种将人体关键点与传统视觉特征结合的数据生成方法，并构建了一个包含20万样本的专用数据集，作者对LLaVA-1.5-7B模型进行了微调。同时，建立了E-HPAUB基准用于评估。实验结果表明，LLaVA-Pose模型在人体姿态和动作理解任务上实现了33.2%的显著性能提升，证明了关键点集成数据在增强以人为中心视觉理解方面的有效性。", "keywords": "人体姿态理解, 动作理解, 关键点集成, 指令微调, 视觉语言模型", "comments": "本文的创新点在于通过集成人体关键点到指令微调数据中，解决了现有VLM在复杂人体姿态和动作理解上的局限性。其构建的专用数据集和评估基准对后续研究具有重要意义，并展示了在特定领域通过数据增强提升模型能力的有效路径。"}}
{"id": "2409.18017", "title": "Transferring disentangled representations: bridging the gap between synthetic and real images", "authors": ["Jacopo Dapueto", "Nicoletta Noceti", "Francesca Odone"], "summary": "Developing meaningful and efficient representations that separate the\nfundamental structure of the data generation mechanism is crucial in\nrepresentation learning. However, Disentangled Representation Learning has not\nfully shown its potential on real images, because of correlated generative\nfactors, their resolution and limited access to ground truth labels.\nSpecifically on the latter, we investigate the possibility of leveraging\nsynthetic data to learn general-purpose disentangled representations applicable\nto real data, discussing the effect of fine-tuning and what properties of\ndisentanglement are preserved after the transfer. We provide an extensive\nempirical study to address these issues. In addition, we propose a new\ninterpretable intervention-based metric, to measure the quality of factors\nencoding in the representation. Our results indicate that some level of\ndisentanglement, transferring a representation from synthetic to real data, is\npossible and effective.", "comment": "Accepted to NeurIPS, 2024", "pdf_url": "http://arxiv.org/pdf/2409.18017v3", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2409.18017v3", "date": "2024-09-26", "updated": "2024-12-06", "AI": {"title_translation": "解耦表示迁移：弥合合成图像与真实图像之间的鸿沟", "tldr": "本文研究了如何利用合成数据学习通用的解耦表示并将其应用于真实数据，发现一定程度的解耦表示迁移是可能且有效的。", "motivation": "解耦表示学习在真实图像上未能充分发挥潜力，原因是生成因子相关性、分辨率问题以及缺乏真实标签。本文旨在探索利用合成数据学习解耦表示，以应用于真实数据。", "method": "本文研究了利用合成数据学习通用解耦表示并将其应用于真实数据的可能性，讨论了微调的效果以及迁移后解耦属性的保留情况。此外，提出了一种新的基于干预的可解释度量来衡量表示中因子编码的质量。", "result": "结果表明，将解耦表示从合成数据迁移到真实数据是可能且有效的，并且可以实现一定程度的解耦。", "conclusion": "本文得出结论，将解耦表示从合成数据迁移到真实数据是可行且有效的，为在真实图像上应用解耦表示学习提供了新的途径。", "translation": "开发有意义且高效的表示，以分离数据生成机制的基本结构，在表示学习中至关重要。然而，解耦表示学习尚未在真实图像上充分展示其潜力，原因是生成因子相关性、分辨率问题以及缺乏真实标签。特别是在后者方面，我们研究了利用合成数据学习适用于真实数据的通用解耦表示的可能性，讨论了微调的效果以及迁移后解耦属性的保留情况。我们提供了广泛的实证研究来解决这些问题。此外，我们提出了一种新的可解释的基于干预的度量，以衡量表示中因子编码的质量。我们的结果表明，将解耦表示从合成数据迁移到真实数据，一定程度的解耦是可能且有效的。", "summary": "本文探讨了将解耦表示从合成数据迁移到真实图像的可行性与有效性，以解决真实图像中解耦表示学习面临的挑战。研究分析了微调的影响和解耦属性的保留情况，并提出了一种新的基于干预的度量方法。实证结果表明，这种迁移能够实现一定程度的有效解耦。", "keywords": "解耦表示, 迁移学习, 合成数据, 真实图像, 表示学习", "comments": "本文提出了一种解决真实图像中解耦表示学习挑战的创新方法，即通过利用合成数据进行迁移学习。其提出的新的可解释度量也增加了研究的价值，为评估解耦质量提供了新工具。这项工作对于推动解耦表示在实际应用中的落地具有重要意义。"}}
{"id": "2506.21330", "title": "Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models", "authors": ["Haoyang Wu", "Tsun-Hsuan Wang", "Mathias Lechner", "Ramin Hasani", "Jennifer A. Eckhoff", "Paul Pak", "Ozanan R. Meireles", "Guy Rosman", "Yutong Ban", "Daniela Rus"], "summary": "Surgical workflow analysis is essential in robot-assisted surgeries, yet the\nlong duration of such procedures poses significant challenges for comprehensive\nvideo analysis. Recent approaches have predominantly relied on transformer\nmodels; however, their quadratic attention mechanism restricts efficient\nprocessing of lengthy surgical videos. In this paper, we propose a novel\nhierarchical input-dependent state space model that leverages the linear\nscaling property of state space models to enable decision making on full-length\nvideos while capturing both local and global dynamics. Our framework\nincorporates a temporally consistent visual feature extractor, which appends a\nstate space model head to a visual feature extractor to propagate temporal\ninformation. The proposed model consists of two key modules: a\nlocal-aggregation state space model block that effectively captures intricate\nlocal dynamics, and a global-relation state space model block that models\ntemporal dependencies across the entire video. The model is trained using a\nhybrid discrete-continuous supervision strategy, where both signals of discrete\nphase labels and continuous phase progresses are propagated through the\nnetwork. Experiments have shown that our method outperforms the current\nstate-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on\nMICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available\nafter paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21330v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21330v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于分层输入依赖状态空间模型的整体手术阶段识别", "tldr": "本文提出了一种新颖的分层输入依赖状态空间模型，用于手术阶段识别，解决了Transformer模型在处理长手术视频时的效率问题，并显著超越了现有最先进的方法。", "motivation": "手术工作流程分析在机器人辅助手术中至关重要，但长时间的手术视频对全面分析构成挑战。现有Transformer模型因其二次注意力机制，难以高效处理冗长视频。", "method": "本文提出一种新颖的分层输入依赖状态空间模型，利用状态空间模型的线性扩展特性处理完整视频，并捕获局部和全局动态。该框架包含一个时间一致的视觉特征提取器，其头部附加状态空间模型以传播时间信息。模型由两个关键模块组成：局部聚合状态空间模型块（捕获局部动态）和全局关系状态空间模型块（建模整个视频的时间依赖性）。模型采用混合离散-连续监督策略训练。", "result": "实验表明，该方法大幅超越了当前最先进的方法：在Cholec80数据集上提升2.8%，在MICCAI2016数据集上提升4.3%，在Heichole数据集上提升12.9%。", "conclusion": "所提出的分层输入依赖状态空间模型有效解决了长手术视频分析的挑战，通过高效捕获局部和全局时间动态，表现出优于现有方法的卓越性能。", "translation": "手术工作流程分析在机器人辅助手术中至关重要，然而，此类手术的长时间特性对全面的视频分析构成了重大挑战。最近的方法主要依赖于Transformer模型；然而，它们的二次注意力机制限制了对冗长手术视频的有效处理。在本文中，我们提出了一种新颖的分层输入依赖状态空间模型，该模型利用状态空间模型的线性扩展特性，能够在捕获局部和全局动态的同时，对完整视频进行决策。我们的框架包含一个时间一致的视觉特征提取器，它在视觉特征提取器上附加一个状态空间模型头部以传播时间信息。所提出的模型由两个关键模块组成：一个有效捕获复杂局部动态的局部聚合状态空间模型块，以及一个建模整个视频时间依赖性的全局关系状态空间模型块。该模型采用混合离散-连续监督策略进行训练，其中离散阶段标签和连续阶段进度两种信号都通过网络传播。实验表明，我们的方法在Cholec80数据集上取得了2.8%的提升，在MICCAI2016数据集上取得了4.3%的提升，在Heichole数据集上取得了12.9%的提升，大幅超越了当前最先进的方法。代码将在论文接收后公开发布。", "summary": "本文提出了一种用于手术阶段识别的分层输入依赖状态空间模型，旨在高效处理长手术视频，克服了Transformer模型的局限性。该模型利用局部和全局状态空间块以及混合监督策略来捕获时间动态，并在多个数据集上显著优于现有最先进的方法。", "keywords": "手术阶段识别, 状态空间模型, 分层模型, 工作流程分析, 长视频分析", "comments": "该论文通过提出一种高效的替代Transformer模型的方法，解决了手术视频分析中的关键挑战。其分层设计和混合监督策略具有创新性，能够鲁棒地捕获精细和广泛的时间依赖性。在多个数据集上取得的显著性能提升突显了其实用重要性。"}}
{"id": "2506.20672", "title": "The final solution of the Hitchhiker's problem #5", "authors": ["Matjaž Omladič", "Martin Vuk", "Aljaž Zalar"], "summary": "A recent survey, nicknamed \"Hitchhiker's Guide\", J.J. Arias-Garc{\\i}a, R.\nMesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and\nSystems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the\ndependence modeling community in spite of the lack of statistical\ninterpretation of quasi-copulas. In our previous work (arXiv:2410.19339,\naccepted in Fuzzy Sets and Systems), we addressed the question of extreme\nvalues of the mass distribution associated with multivariate quasi-copulas.\nUsing a linear programming approach, we were able to solve Open Problem 5 of\nthe \"Guide\" up to dimension d = 17 and disprove a recent conjecture on the\nsolution to that problem. In this paper, we use an analytical approach to\nprovide a complete answer to the original question.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2506.20672v1", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20672v1", "date": "2025-05-20", "updated": "2025-05-20", "AI": {"title_translation": "银河系漫游指南问题 #5 的最终解决方案", "tldr": "本文采用分析方法，为准连接函数领域“银河系漫游指南”中的开放问题5提供了完整解决方案。", "motivation": "尽管准连接函数缺乏统计解释，但其在依赖建模领域的重要性日益凸显。之前的工作未能完全解决“银河系漫游指南”中提出的开放问题5，特别是未能提供一个完整、普遍的解决方案，这促使了本研究的进行。", "method": "本文采用分析方法（analytical approach）来解决问题。之前的相关工作使用了线性规划方法。", "result": "本文为“银河系漫游指南”中的开放问题5提供了完整、普遍的答案。之前的研究使用线性规划方法解决了该问题直至维度d=17，并推翻了一个近期关于该问题解决方案的猜想。", "conclusion": "本文通过分析方法，彻底解决了准连接函数领域中“银河系漫游指南”的开放问题5。", "translation": "近期一项名为“银河系漫游指南”的调查（J.J. Arias-Garc{\\i}a, R. Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and Systems 393 (2020) 1-28）尽管准连接函数缺乏统计解释，但在依赖建模领域提高了准连接函数问题的评级。在我们之前的工作（arXiv:2410.19339，已在Fuzzy Sets and Systems接受）中，我们解决了与多元准连接函数相关的质量分布极值问题。通过线性规划方法，我们能够解决“指南”中的开放问题5，直至维度d=17，并驳斥了一个关于该问题解决方案的近期猜想。在本文中，我们使用分析方法为原始问题提供了完整的答案。", "summary": "本文旨在为准连接函数领域中“银河系漫游指南”调查提出的开放问题5提供最终的完整解决方案。此问题在依赖建模中具有重要性，尽管准连接函数缺乏统计解释。作者在先前工作中曾使用线性规划方法部分解决了该问题并推翻了相关猜想，但本研究则采用分析方法，提供了对原问题的普遍性完整解答。", "keywords": "准连接函数, 依赖建模, 开放问题5, 分析方法, 银河系漫游指南", "comments": "本文的创新之处在于，它通过采用纯粹的分析方法，而非之前工作所依赖的线性规划方法，成功地为准连接函数领域一个重要的开放问题提供了普遍且完整的解决方案。这解决了先前方法在维度限制上的不足，并可能为准连接函数理论的进一步发展奠定基础。"}}
{"id": "2506.21348", "title": "PanSt3R: Multi-view Consistent Panoptic Segmentation", "authors": ["Lojze Zust", "Yohann Cabon", "Juliette Marrie", "Leonid Antsfeld", "Boris Chidlovskii", "Jerome Revaud", "Gabriela Csurka"], "summary": "Panoptic segmentation of 3D scenes, involving the segmentation and\nclassification of object instances in a dense 3D reconstruction of a scene, is\na challenging problem, especially when relying solely on unposed 2D images.\nExisting approaches typically leverage off-the-shelf models to extract\nper-frame 2D panoptic segmentations, before optimizing an implicit geometric\nrepresentation (often based on NeRF) to integrate and fuse the 2D predictions.\nWe argue that relying on 2D panoptic segmentation for a problem inherently 3D\nand multi-view is likely suboptimal as it fails to leverage the full potential\nof spatial relationships across views. In addition to requiring camera\nparameters, these approaches also necessitate computationally expensive\ntest-time optimization for each scene. Instead, in this work, we propose a\nunified and integrated approach PanSt3R, which eliminates the need for\ntest-time optimization by jointly predicting 3D geometry and multi-view\npanoptic segmentation in a single forward pass. Our approach builds upon recent\nadvances in 3D reconstruction, specifically upon MUSt3R, a scalable multi-view\nversion of DUSt3R, and enhances it with semantic awareness and multi-view\npanoptic segmentation capabilities. We additionally revisit the standard\npost-processing mask merging procedure and introduce a more principled approach\nfor multi-view segmentation. We also introduce a simple method for generating\nnovel-view predictions based on the predictions of PanSt3R and vanilla 3DGS.\nOverall, the proposed PanSt3R is conceptually simple, yet fast and scalable,\nand achieves state-of-the-art performance on several benchmarks, while being\norders of magnitude faster than existing methods.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21348v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21348v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "PanSt3R: 多视角一致的全景分割", "tldr": "PanSt3R提出了一种统一、快速且可扩展的方法，用于从2D图像进行多视角一致的3D全景分割，无需测试时优化，并实现了最先进的性能。", "motivation": "现有的3D场景全景分割方法通常依赖于2D全景分割并进行测试时优化，这被认为是次优的，因为它未能充分利用跨视角的空间关系，且计算成本高昂。", "method": "本文提出了PanSt3R，一种统一且集成的方法，通过在单次前向传播中联合预测3D几何和多视角全景分割，从而消除了测试时优化的需求。该方法基于MUSt3R（DUSt3R的可扩展多视角版本），并增强了语义感知和多视角全景分割能力。此外，它还改进了标准的后处理遮罩合并程序，并引入了一种生成新视角预测的简单方法。", "result": "PanSt3R在多个基准测试中取得了最先进的性能，并且比现有方法快几个数量级。", "conclusion": "PanSt3R提供了一种概念简单、快速且可扩展的解决方案，用于多视角一致的3D全景分割，解决了现有方法的局限性，并显著提升了性能和效率。", "translation": "3D场景的全景分割，涉及对场景密集3D重建中对象实例的分割和分类，是一个具有挑战性的问题，尤其是在仅依赖未姿态2D图像时。现有方法通常利用现成的模型来提取每帧2D全景分割，然后优化隐式几何表示（通常基于NeRF）以整合和融合2D预测。我们认为，对于一个本质上是3D和多视角的问题，依赖2D全景分割可能是次优的，因为它未能充分利用跨视角的空间关系的全部潜力。除了需要相机参数外，这些方法还需要对每个场景进行计算成本高昂的测试时优化。相反，在这项工作中，我们提出了一种统一且集成的方法PanSt3R，它通过在单次前向传播中联合预测3D几何和多视角全景分割，从而消除了测试时优化的需求。我们的方法建立在3D重建的最新进展之上，特别是基于MUSt3R（DUSt3R的可扩展多视角版本），并增强了其语义感知和多视角全景分割能力。我们还重新审视了标准的后处理遮罩合并程序，并引入了一种更原则性的多视角分割方法。我们还介绍了一种基于PanSt3R和普通3DGS预测生成新视角预测的简单方法。总的来说，所提出的PanSt3R概念简单，但快速且可扩展，并在多个基准测试中取得了最先进的性能，同时比现有方法快几个数量级。", "summary": "PanSt3R是一种创新的3D场景全景分割方法，旨在解决现有方法在处理未姿态2D图像时的局限性。它通过一个统一的单次前向传播模型，联合预测3D几何和多视角全景分割，无需耗时的测试时优化。该方法基于MUSt3R，并引入了改进的后处理和新视角预测功能，实现了卓越的性能和显著的速度提升。", "keywords": "3D全景分割, 多视角一致性, 几何预测, 语义感知, 测试时优化", "comments": "PanSt3R的创新之处在于其统一的、无需测试时优化的方法，将3D几何和多视角全景分割结合在一个单一的前向传播中，这显著提高了效率和可扩展性。它解决了现有方法在利用跨视角空间关系方面的不足，并通过在多个基准上达到最先进的性能证明了其重要性。"}}
{"id": "2506.21356", "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models", "authors": ["Hongbo Liu", "Jingwen He", "Yi Jin", "Dian Zheng", "Yuhao Dong", "Fan Zhang", "Ziqi Huang", "Yinan He", "Yangguang Li", "Weichao Chen", "Yu Qiao", "Wanli Ouyang", "Shengjie Zhao", "Ziwei Liu"], "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\n\\textbf{ShotBench}, a comprehensive benchmark specifically designed for\ncinematic language understanding. It features over 3.5k expert-annotated QA\npairs from images and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60\\%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\n\\textbf{ShotQA}, a large-scale multimodal dataset comprising approximately 70k\ncinematic QA pairs. Leveraging ShotQA, we develop \\textbf{ShotVL} through\nsupervised fine-tuning and Group Relative Policy Optimization. ShotVL\nsignificantly outperforms all existing open-source and proprietary models on\nShotBench, establishing new \\textbf{state-of-the-art} performance. We\nopen-source our models, data, and code to foster rapid progress in this crucial\narea of AI-driven cinematic understanding and generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21356v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21356v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "ShotBench：视觉语言模型中的专家级电影理解", "tldr": "VLMs在电影理解上存在不足，本研究引入ShotBench基准、ShotQA数据集和ShotVL模型，显著提升了电影理解的SOTA性能。", "motivation": "现有的视觉语言模型（VLMs）在理解电影中细致的电影语法方面能力不足，且缺乏鲁棒的评估方法，这限制了细粒度视觉理解和AI辅助视频生成的精度。", "method": "本文引入了ShotBench，一个专门用于电影语言理解的综合基准，包含3.5k专家标注的QA对。研究者评估了24个领先的VLM在ShotBench上的表现。为促进进步，构建了ShotQA，一个包含约70k电影QA对的大规模多模态数据集。最后，利用ShotQA，通过有监督微调和群组相对策略优化，开发了ShotVL模型。", "result": "评估显示现有VLMs在ShotBench上存在显著局限性，即使表现最好的模型平均准确率也低于60%，尤其在细粒度视觉线索和复杂空间推理方面表现不佳。ShotVL显著优于所有现有开源和专有模型，在ShotBench上建立了新的最先进性能。", "conclusion": "ShotBench、ShotQA和ShotVL的引入，以及它们的开源，旨在推动AI驱动的电影理解和生成领域的快速发展。", "translation": "电影摄影作为电影最基本的视觉语言，对于传达叙事、情感和美学品质至关重要。尽管最近的视觉语言模型（VLMs）在通用视觉理解方面表现出强大的能力，但它们在理解单个镜头中嵌入的细致电影语法方面的熟练程度在很大程度上尚未被探索，并且缺乏鲁棒的评估。这一关键空白限制了细粒度视觉理解和AI辅助视频生成的精度。为了解决这个问题，我们引入了\\textbf{ShotBench}，一个专门为电影语言理解设计的综合基准。它包含来自图像和视频片段的3.5k多个专家标注的问答对，这些数据精心策划自200多部著名（主要是奥斯卡提名）电影，并涵盖了八个关键的电影摄影维度。我们对24个领先的VLM在ShotBench上的评估揭示了它们存在的重大局限性：即使是表现最好的模型也只达到了不到60%的平均准确率，尤其在处理细粒度视觉线索和复杂空间推理方面表现挣扎。为了促进该领域的进步，我们构建了\\textbf{ShotQA}，一个包含约70k电影问答对的大规模多模态数据集。利用ShotQA，我们通过有监督微调和群组相对策略优化开发了\\textbf{ShotVL}。ShotVL在ShotBench上显著优于所有现有开源和专有模型，建立了新的\\textbf{最先进}性能。我们开源了我们的模型、数据和代码，以促进AI驱动的电影理解和生成这一关键领域的快速发展。", "summary": "本文提出了ShotBench，一个用于评估视觉语言模型（VLMs）电影理解能力的综合基准，并揭示了现有VLMs在该领域的显著局限性。为解决此问题，研究者构建了大规模数据集ShotQA，并基于此开发了ShotVL模型，该模型在电影理解任务上达到了新的最先进水平。所有资源均已开源，旨在推动电影AI领域的发展。", "keywords": "电影理解, 视觉语言模型, ShotBench, ShotQA, ShotVL", "comments": "本文通过引入专门的电影理解基准（ShotBench）和大规模数据集（ShotQA），系统地揭示了当前视觉语言模型在电影语法理解上的不足。其提出的ShotVL模型显著提升了该领域的性能，并通过开源数据和模型，为未来AI辅助电影创作和分析奠定了重要基础，具有重要的创新性和实用价值。"}}
{"id": "2506.21357", "title": "CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations", "authors": ["Julian Lorenz", "Mrunmai Phatak", "Robin Schön", "Katja Ludwig", "Nico Hörmann", "Annemarie Friedrich", "Rainer Lienhart"], "summary": "2D scene graphs provide a structural and explainable framework for scene\nunderstanding. However, current work still struggles with the lack of accurate\nscene graph data. To overcome this data bottleneck, we present CoPa-SG, a\nsynthetic scene graph dataset with highly precise ground truth and exhaustive\nrelation annotations between all objects. Moreover, we introduce parametric and\nproto-relations, two new fundamental concepts for scene graphs. The former\nprovides a much more fine-grained representation than its traditional\ncounterpart by enriching relations with additional parameters such as angles or\ndistances. The latter encodes hypothetical relations in a scene graph and\ndescribes how relations would form if new objects are placed in the scene.\nUsing CoPa-SG, we compare the performance of various scene graph generation\nmodels. We demonstrate how our new relation types can be integrated in\ndownstream applications to enhance planning and reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21357v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21357v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CoPa-SG：具有参数和原型关系的密集场景图", "tldr": "本文提出了CoPa-SG，一个合成场景图数据集，旨在解决现有场景图数据不足的问题。同时引入了两种新的关系类型：参数关系（更细粒度的表示）和原型关系（编码假设关系），以增强场景理解和下游应用。", "motivation": "当前场景图研究面临缺乏准确场景图数据的困境。", "method": "本文提出了CoPa-SG，一个具有高度精确真值和详尽对象间关系标注的合成场景图数据集。此外，引入了两种新的基本场景图概念：参数关系（通过角度或距离等额外参数丰富关系，提供更细粒度的表示）和原型关系（编码场景图中的假设关系，描述新对象放置时关系如何形成）。", "result": "使用CoPa-SG数据集，本文比较了各种场景图生成模型的性能。研究展示了新的关系类型如何集成到下游应用中，以增强规划和推理能力。", "conclusion": "本文通过提供CoPa-SG数据集和引入参数关系与原型关系，有效解决了场景图数据稀缺的问题，并证明了这些新关系类型能够显著提升下游应用的规划和推理能力。", "translation": "2D场景图为场景理解提供了一个结构化和可解释的框架。然而，当前工作仍然受困于缺乏准确的场景图数据。为了克服这一数据瓶颈，我们提出了CoPa-SG，一个具有高度精确真值和详尽所有对象之间关系标注的合成场景图数据集。此外，我们引入了参数关系和原型关系，这是场景图的两个新的基本概念。前者通过用角度或距离等额外参数丰富关系，提供了比传统对应物更细粒度的表示。后者在场景图中编码假设关系，并描述了如果新对象放置在场景中，关系将如何形成。使用CoPa-SG，我们比较了各种场景图生成模型的性能。我们展示了我们的新关系类型如何集成到下游应用中，以增强规划和推理能力。", "summary": "本文针对2D场景图数据稀缺的问题，提出了CoPa-SG，一个包含精确真值和详尽关系标注的合成场景图数据集。同时，引入了参数关系（提供细粒度表示）和原型关系（编码假设关系）两种新概念。研究利用CoPa-SG比较了现有模型，并展示了新关系类型在增强规划和推理能力方面的应用潜力。", "keywords": "场景图, CoPa-SG, 参数关系, 原型关系, 合成数据集", "comments": "CoPa-SG通过提供高质量的合成数据集解决了场景图领域的数据瓶颈，其创新之处在于引入了参数关系和原型关系，这两种新的关系类型显著增强了场景图的表达能力和泛化性，为更复杂的场景理解和推理任务奠定了基础。"}}
{"id": "2506.20697", "title": "scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection", "authors": ["Zhen Yuan", "Shaoqing Jiao", "Yihang Xiao", "Jiajie Peng"], "summary": "The advent of single-cell multi-omics technologies has enabled the\nsimultaneous profiling of diverse omics layers within individual cells.\nIntegrating such multimodal data provides unprecedented insights into cellular\nidentity, regulatory processes, and disease mechanisms. However, it remains\nchallenging, as current methods often rely on selecting highly variable genes\nor peaks during preprocessing, which may inadvertently discard crucial\nbiological information. Here, we present scMamba, a foundation model designed\nto integrate single-cell multi-omics data without the need for prior feature\nselection while preserving genomic positional information. scMamba introduces a\npatch-based cell tokenization strategy that treats genomics regions as words\n(tokens) and cells as sentences. Building upon the concept of state space\nduality, scMamba distills rich biological insights from high-dimensional,\nsparse single-cell multi-omics data. Additionally, our novel contrastive\nlearning approach, enhanced with cosine similarity regularization, enables\nsuperior alignment across omics layers compared to traditional methods.\nSystematic benchmarking across multiple datasets demonstrates that scMamba\nsignificantly outperforms state-of-the-art methods in preserving biological\nvariation, aligning omics layers, and enhancing key downstream tasks such as\nclustering, cell type annotation, and trajectory inference. Our findings\nposition scMamba as a powerful tool for large-scale single-cell multi-omics\nintegration, capable of handling large-scale atlases and advancing biological\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20697v1", "categories": ["q-bio.CB", "cs.LG"], "cate": "q-bio.CB", "url": "http://arxiv.org/abs/2506.20697v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "scMamba：一种超越高变特征选择的单细胞多组学整合可扩展基础模型", "tldr": "scMamba是一个新的单细胞多组学整合基础模型，它通过避免特征选择、保留基因组位置信息、采用新颖的标记化策略和对比学习，优于现有方法。", "motivation": "现有单细胞多组学整合方法通常依赖于高变特征选择，这可能无意中丢弃关键生物信息，因此整合多模态数据仍具挑战性。", "method": "scMamba是一个基础模型，无需预先进行特征选择即可整合单细胞多组学数据，同时保留基因组位置信息。它引入了一种基于补丁的细胞标记化策略，将基因组区域视为词（标记），细胞视为句子。该模型基于状态空间对偶性，并采用一种新颖的、通过余弦相似性正则化增强的对比学习方法，以实现优于传统方法的组学层对齐。", "result": "在多个数据集上的系统基准测试表明，scMamba在保留生物变异、对齐组学层以及增强聚类、细胞类型注释和轨迹推断等关键下游任务方面，显著优于现有最先进方法。", "conclusion": "scMamba被定位为一种强大的大规模单细胞多组学整合工具，能够处理大型图谱并推动生物学发现。", "translation": "单细胞多组学技术的出现使得在单个细胞内同时分析不同的组学层成为可能。整合此类多模态数据为细胞身份、调控过程和疾病机制提供了前所未有的见解。然而，这仍然具有挑战性，因为当前方法在预处理过程中通常依赖于选择高变基因或峰，这可能会无意中丢弃关键的生物学信息。在此，我们提出了scMamba，一个基础模型，旨在整合单细胞多组学数据，而无需事先进行特征选择，同时保留基因组位置信息。scMamba引入了一种基于补丁的细胞标记化策略，将基因组区域视为词（标记），将细胞视为句子。基于状态空间对偶性的概念，scMamba从高维、稀疏的单细胞多组学数据中提取丰富的生物学见解。此外，我们新颖的对比学习方法，通过余弦相似性正则化得到增强，与传统方法相比，能够实现卓越的组学层对齐。在多个数据集上的系统基准测试表明，scMamba在保留生物变异、对齐组学层以及增强聚类、细胞类型注释和轨迹推断等关键下游任务方面，显著优于现有最先进方法。我们的研究结果将scMamba定位为一种强大的大规模单细胞多组学整合工具，能够处理大型图谱并推动生物学发现。", "summary": "scMamba是一种新型单细胞多组学整合基础模型，旨在解决现有方法依赖高变特征选择而丢失生物信息的挑战。该模型无需预先进行特征选择，并保留基因组位置信息，通过引入基于补丁的细胞标记化策略和创新的对比学习方法实现卓越的组学层对齐。系统基准测试证明，scMamba在保留生物变异、对齐组学层和改善下游任务方面显著优于现有技术，使其成为大规模单细胞多组学整合和生物发现的强大工具。", "keywords": "单细胞多组学, 基础模型, 特征选择, 数据整合, scMamba", "comments": "该论文的创新点在于提出了一个无需高变特征选择即可整合单细胞多组学数据的基础模型，通过独特的基于补丁的细胞标记化策略和新颖的余弦相似性正则化对比学习方法，有效保留了基因组位置信息并实现了优越的组学层对齐。其重要性在于为大规模单细胞多组学数据整合提供了可扩展且高性能的解决方案，有望推动细胞身份、调控过程和疾病机制的研究。"}}
{"id": "2506.20764", "title": "Control and optimization for Neural Partial Differential Equations in Supervised Learning", "authors": ["Alain Bensoussan", "Minh-Binh Tran", "Bangjie Wang"], "summary": "Although there is a substantial body of literature on control and\noptimization problems for parabolic and hyperbolic systems, the specific\nproblem of controlling and optimizing the coefficients of the associated\noperators within such systems has not yet been thoroughly explored. In this\nwork, we aim to initiate a line of research in control theory focused on\noptimizing and controlling the coefficients of these operators-a problem that\nnaturally arises in the context of neural networks and supervised learning.\n  In supervised learning, the primary objective is to transport initial data\ntoward target data through the layers of a neural network. We propose a novel\nperspective: neural networks can be interpreted as partial differential\nequations (PDEs). From this viewpoint, the control problem traditionally\nstudied in the context of ordinary differential equations (ODEs) is\nreformulated as a control problem for PDEs, specifically targeting the\noptimization and control of coefficients in parabolic and hyperbolic operators.\nTo the best of our knowledge, this specific problem has not yet been\nsystematically addressed in the control theory of PDEs.\n  To this end, we propose a dual system formulation for the control and\noptimization problem associated with parabolic PDEs, laying the groundwork for\nthe development of efficient numerical schemes in future research. We also\nprovide a theoretical proof showing that the control and optimization problem\nfor parabolic PDEs admits minimizers. Finally, we investigate the control\nproblem associated with hyperbolic PDEs and prove the existence of solutions\nfor a corresponding approximated control problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20764v1", "categories": ["math.OC", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20764v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "监督学习中神经偏微分方程的控制与优化", "tldr": "本文提出将神经网络解释为偏微分方程（PDEs），并研究了控制和优化这类PDEs中系数的问题。研究证明了抛物线型PDEs存在最小化解，并证明了双曲型PDEs近似控制问题的解的存在性。", "motivation": "尽管关于抛物线和双曲系统控制与优化问题的文献很多，但针对这些系统中相关算子系数的控制和优化问题尚未被深入探索。这个问题自然地出现在神经网络和监督学习的背景中，其中主要目标是通过神经网络层传输初始数据到目标数据。本文旨在开创一个专注于优化和控制这些算子系数的控制理论研究方向。", "method": "本文提出了一种新颖的视角，将神经网络解释为偏微分方程（PDEs）。在此基础上，将传统上在常微分方程（ODEs）背景下研究的控制问题重新表述为偏微分方程的控制问题，特别是针对抛物线和双曲算子中的系数优化和控制。为此，本文为抛物线型PDEs的控制和优化问题提出了一个对偶系统公式，并提供了理论证明，表明其存在最小化解。此外，还研究了与双曲型PDEs相关的控制问题，并证明了相应近似控制问题的解的存在性。", "result": "本文为抛物线型偏微分方程（PDEs）的控制和优化问题提出了一个对偶系统公式，并提供了理论证明，表明该问题存在最小化解。此外，本文还证明了与双曲型偏微分方程相关的近似控制问题的解的存在性。", "conclusion": "本文开创了控制理论中优化和控制偏微分方程（PDEs）算子系数的研究方向，特别是在将神经网络解释为PDEs的背景下。研究为抛物线型PDEs的控制和优化问题提供了存在性证明，并为双曲型PDEs的近似控制问题提供了存在性证明，为未来开发高效数值方案奠定了基础。", "translation": "尽管关于抛物线和双曲系统控制与优化问题的文献很多，但针对这些系统中相关算子系数的特定问题尚未被深入探索。在这项工作中，我们旨在开创一个专注于优化和控制这些算子系数的控制理论研究方向——这个问题自然地出现在神经网络和监督学习的背景中。在监督学习中，主要目标是通过神经网络层将初始数据传输到目标数据。我们提出了一种新颖的视角：神经网络可以被解释为偏微分方程（PDEs）。从这个角度来看，传统上在常微分方程（ODEs）背景下研究的控制问题被重新表述为偏微分方程的控制问题，特别是针对抛物线和双曲算子中系数的优化和控制。据我们所知，这个特定问题在偏微分方程的控制理论中尚未得到系统性解决。为此，我们为与抛物线型偏微分方程相关的控制和优化问题提出了一个对偶系统公式，为未来研究中高效数值方案的开发奠定了基础。我们还提供了一个理论证明，表明抛物线型偏微分方程的控制和优化问题存在最小化解。最后，我们研究了与双曲型偏微分方程相关的控制问题，并证明了相应近似控制问题的解的存在性。", "summary": "本论文提出了一种新颖的观点，将神经网络解释为偏微分方程（PDEs），以解决监督学习中抛物线和双曲算子系数控制与优化这一未充分探索的问题。研究将传统的控制问题重新构建在PDE框架内，为抛物线型PDEs开发了对偶系统公式并证明了最小化解的存在性。对于双曲型PDEs，论文证明了近似控制问题解的存在性，为未来的数值方案奠定了理论基础。", "keywords": "神经网络, 偏微分方程, 控制理论, 优化, 监督学习", "comments": "本文通过将神经网络与偏微分方程相结合，提供了一种新颖且重要的跨学科方法，为深度学习优化应用先进控制理论开辟了新途径。对最小化解和解的存在性的理论证明是未来算法开发的关键基础步骤，具有重要的理论价值。"}}
{"id": "2506.21364", "title": "CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection", "authors": ["Zhixin Cheng", "Jiacheng Deng", "Xinjun Li", "Xiaotian Yin", "Bohao Liao", "Baoqun Yin", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Detection-free methods typically follow a coarse-to-fine pipeline, extracting\nimage and point cloud features for patch-level matching and refining dense\npixel-to-point correspondences. However, differences in feature channel\nattention between images and point clouds may lead to degraded matching\nresults, ultimately impairing registration accuracy. Furthermore, similar\nstructures in the scene could lead to redundant correspondences in cross-modal\nmatching. To address these issues, we propose Channel Adaptive Adjustment\nModule (CAA) and Global Optimal Selection Module (GOS). CAA enhances\nintra-modal features and suppresses cross-modal sensitivity, while GOS replaces\nlocal selection with global optimization. Experiments on RGB-D Scenes V2 and\n7-Scenes demonstrate the superiority of our method, achieving state-of-the-art\nperformance in image-to-point cloud registration.", "comment": "ICCV 2025 accepted", "pdf_url": "http://arxiv.org/pdf/2506.21364v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21364v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "CA-I2P：通道自适应配准网络与全局最优选择", "tldr": "本文提出CA-I2P网络，通过通道自适应调整模块(CAA)和全局最优选择模块(GOS)解决图像-点云配准中特征通道注意力差异和相似结构导致的冗余对应问题，实现了最先进的性能。", "motivation": "现有检测无关的粗到精配准方法中，图像和点云之间特征通道注意力的差异会导致匹配结果下降，从而损害配准精度。此外，场景中的相似结构可能导致跨模态匹配中出现冗余对应。", "method": "提出通道自适应调整模块(CAA)和全局最优选择模块(GOS)。CAA增强模态内特征并抑制跨模态敏感性；GOS用全局优化取代局部选择。", "result": "在RGB-D Scenes V2和7-Scenes数据集上的实验表明，该方法在图像到点云配准中取得了最先进的性能。", "conclusion": "CA-I2P通过其提出的CAA和GOS模块，有效解决了跨模态特征差异和冗余对应问题，显著提升了图像到点云配准的精度，达到了最先进的水平。", "translation": "检测无关的方法通常遵循从粗到精的流程，提取图像和点云特征进行块级匹配并细化密集的像素到点对应关系。然而，图像和点云之间特征通道注意力的差异可能导致匹配结果下降，最终损害配准精度。此外，场景中的相似结构可能导致跨模态匹配中出现冗余对应。为了解决这些问题，我们提出了通道自适应调整模块（CAA）和全局最优选择模块（GOS）。CAA增强模态内特征并抑制跨模态敏感性，而GOS用全局优化取代局部选择。在RGB-D Scenes V2和7-Scenes上的实验证明了我们方法的优越性，在图像到点云配准中实现了最先进的性能。", "summary": "本文提出CA-I2P网络，用于解决图像-点云配准中特征通道注意力差异和相似结构导致的冗余对应问题。通过引入通道自适应调整模块（CAA）增强模态内特征并抑制跨模态敏感性，以及全局最优选择模块（GOS）取代局部选择进行全局优化，该方法在RGB-D Scenes V2和7-Scenes数据集上实现了图像到点云配准的最先进性能。", "keywords": "图像-点云配准, 通道自适应, 全局最优选择, 跨模态匹配, CA-I2P", "comments": "该论文的创新点在于提出了两个针对图像-点云跨模态配准中的关键挑战的模块：通道自适应调整模块（CAA）和全局最优选择模块（GOS）。CAA有效地处理了图像和点云之间特征通道注意力的差异，而GOS则通过全局优化避免了相似结构带来的冗余对应。这种针对性的模块设计显著提升了配准精度，并在最先进的性能上得到了验证，对于多模态数据融合和三维重建领域具有重要意义。"}}
{"id": "2506.20779", "title": "Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon", "authors": ["Tongtong Liang", "Dan Qiao", "Yu-Xiang Wang", "Rahul Parhi"], "summary": "We study the implicit bias of flatness / low (loss) curvature and its effects\non generalization in two-layer overparameterized ReLU networks with\nmultivariate inputs -- a problem well motivated by the minima stability and\nedge-of-stability phenomena in gradient-descent training. Existing work either\nrequires interpolation or focuses only on univariate inputs. This paper\npresents new and somewhat surprising theoretical results for multivariate\ninputs. On two natural settings (1) generalization gap for flat solutions, and\n(2) mean-squared error (MSE) in nonparametric function estimation by stable\nminima, we prove upper and lower bounds, which establish that while flatness\ndoes imply generalization, the resulting rates of convergence necessarily\ndeteriorate exponentially as the input dimension grows. This gives an\nexponential separation between the flat solutions vis-\\`a-vis low-norm\nsolutions (i.e., weight decay), which knowingly do not suffer from the curse of\ndimensionality. In particular, our minimax lower bound construction, based on a\nnovel packing argument with boundary-localized ReLU neurons, reveals how flat\nsolutions can exploit a kind of ''neural shattering'' where neurons rarely\nactivate, but with high weight magnitudes. This leads to poor performance in\nhigh dimensions. We corroborate these theoretical findings with extensive\nnumerical simulations. To the best of our knowledge, our analysis provides the\nfirst systematic explanation for why flat minima may fail to generalize in high\ndimensions.", "comment": "Comments Welcome!", "pdf_url": "http://arxiv.org/pdf/2506.20779v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20779v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "ReLU神经网络的稳定极小值受维度诅咒的影响：神经破碎现象", "tldr": "本研究发现，在多变量输入下，ReLU神经网络的平坦极小值虽然有助于泛化，但其收敛速度会随输入维度的增加呈指数级恶化，这揭示了一种“神经破碎”现象，解释了高维空间中平坦极小值泛化失败的原因。", "motivation": "该研究旨在探究过参数化ReLU网络中平坦度/低损失曲率的隐式偏差及其对泛化能力的影响，这源于梯度下降训练中的极小值稳定性和边缘稳定性现象。现有工作要么需要插值，要么仅关注单变量输入，因此本研究旨在为多变量输入提供新的理论结果。", "method": "本研究在两种自然设置下（1）平坦解的泛化差距和（2）稳定极小值在非参数函数估计中的均方误差（MSE），通过理论分析证明了上界和下界。特别地，通过基于边界局部化ReLU神经元的新颖填充论证，构建了极小极大下界，并辅以广泛的数值模拟来证实理论发现。", "result": "研究发现，尽管平坦度确实能提升泛化能力，但其收敛速度会随输入维度的增加呈指数级恶化。这导致平坦解与低范数解（如权重衰减）之间存在指数级分离，后者已知不受维度诅咒的影响。具体来说，我们发现平坦解会利用一种“神经破碎”现象，即神经元很少激活但权重幅度很高，这导致在高维空间中性能不佳。", "conclusion": "本研究首次系统地解释了为什么平坦极小值可能在高维空间中无法泛化。", "translation": "我们研究了平坦度/低（损失）曲率的隐式偏差及其在具有多变量输入的双层过参数化ReLU网络中对泛化能力的影响——这是一个由梯度下降训练中的极小值稳定性和边缘稳定性现象充分激发的问题。现有工作要么需要插值，要么只关注单变量输入。本文为多变量输入提供了新的且有些令人惊讶的理论结果。在两种自然设置下：（1）平坦解的泛化差距，以及（2）稳定极小值在非参数函数估计中的均方误差（MSE），我们证明了上界和下界，这些结果表明，虽然平坦度确实意味着泛化，但其收敛速度必然随输入维度的增长而呈指数级恶化。这导致了平坦解与低范数解（即权重衰减）之间的指数级分离，后者已知不受维度诅咒的影响。特别是，我们基于边界局部化ReLU神经元的新颖填充论证构建的极小极大下界揭示了平坦解如何利用一种“神经破碎”现象——神经元很少激活，但具有高权重幅度。这导致在高维空间中性能不佳。我们通过广泛的数值模拟证实了这些理论发现。据我们所知，我们的分析首次系统地解释了为什么平坦极小值可能在高维空间中无法泛化。", "summary": "本研究探讨了过参数化ReLU网络中平坦极小值对泛化的影响，特别是在多变量输入场景下。研究通过理论分析和数值模拟，发现尽管平坦度有助于泛化，但其收敛速度会随输入维度的增加呈指数级下降。这揭示了平坦解在高维空间中性能不佳的原因，即“神经破碎”现象，其中神经元激活稀疏但权重巨大，从而导致其与不受维度诅咒影响的低范数解产生显著的性能差距。这项工作首次系统地解释了平坦极小值在高维空间中泛化失败的机制。", "keywords": "平坦极小值, ReLU神经网络, 维度诅咒, 泛化, 神经破碎现象", "comments": "该论文通过引入“神经破碎”现象，首次系统地解释了ReLU神经网络中平坦极小值在高维空间下泛化性能下降的深层原因，填补了现有研究的空白。其理论分析严谨，并通过数值模拟进行了验证，为理解深度学习模型的泛化机制提供了重要的新视角。"}}
{"id": "2506.21369", "title": "GenFlow: Interactive Modular System for Image Generation", "authors": ["Duc-Hung Nguyen", "Huu-Phuc Huynh", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21369v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21369v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GenFlow: 图像生成交互式模块化系统", "tldr": "GenFlow是一个交互式模块化系统，通过节点编辑器和智能助手，让所有人都能轻松生成图像，旨在降低生成艺术的技术门槛。", "motivation": "生成艺术具有巨大的创意潜力，但由于其复杂的技术要求，普通用户难以充分利用。本研究旨在通过提供一个易于访问的工具来弥合这一技术鸿沟，使所有技能水平的用户都能精确、轻松地生成图像。", "method": "本文提出了GenFlow，一个新型模块化框架。该框架包含一个用于无缝定制的基于节点的编辑器，以及一个由自然语言处理（NLP）驱动的智能助手。GenFlow通过自动化部署过程和最小化技术障碍，将复杂的工作流创建转化为直观且易于访问的体验。", "result": "一项用户研究表明，GenFlow能够优化工作流程，减少任务完成时间，并通过其直观的界面和自适应功能增强用户理解。", "conclusion": "GenFlow被定位为一个突破性的解决方案，它重新定义了生成艺术领域的可访问性和效率。", "translation": "生成艺术解锁了无限的创意可能性，但由于高级架构概念和计算工作流所需的技术专业知识，其全部潜力仍未被充分利用。为了弥合这一差距，我们提出了GenFlow，一个新颖的模块化框架，使所有技能水平的用户都能够精确、轻松地生成图像。GenFlow具有用于无缝定制的基于节点的编辑器和由自然语言处理驱动的智能助手，将工作流创建的复杂性转化为直观且易于访问的体验。通过自动化部署过程并最大限度地减少技术障碍，我们的框架使尖端生成艺术工具可供所有人使用。一项用户研究表明，GenFlow通过其直观的界面和自适应功能，能够优化工作流、减少任务完成时间并增强用户理解。这些结果使GenFlow成为一个突破性的解决方案，重新定义了生成艺术领域的可访问性和效率。", "summary": "GenFlow是一个创新的交互式模块化系统，旨在降低生成艺术的创作门槛。它通过结合直观的节点编辑器和基于自然语言处理的智能助手，简化了图像生成工作流程的复杂性。该系统自动化部署并减少技术障碍，使得所有技能水平的用户都能轻松高效地生成图像。用户研究证实了GenFlow在优化工作流程、缩短任务时间以及提升用户理解方面的有效性，从而显著提高了生成艺术工具的可访问性和效率。", "keywords": "图像生成, 模块化系统, 生成艺术, 人机交互, 自然语言处理", "comments": "GenFlow的创新之处在于其通过模块化和智能辅助，将复杂的生成艺术工作流变得高度可访问和用户友好，极大地降低了技术门槛。这对于推动生成艺术的普及和民主化具有重要意义，使得更多非专业用户能够探索和利用这一创意领域。其结合节点编辑和NLP助手的混合方法，为未来的创意工具设计提供了有价值的参考。"}}
{"id": "2506.21398", "title": "FastRef:Fast Prototype Refinement for Few-Shot Industrial Anomaly Detection", "authors": ["Long Tian", "Yufei Li", "Yuyang Dai", "Wenchao Chen", "Xiyang Liu", "Bo Chen"], "summary": "Few-shot industrial anomaly detection (FS-IAD) presents a critical challenge\nfor practical automated inspection systems operating in data-scarce\nenvironments. While existing approaches predominantly focus on deriving\nprototypes from limited normal samples, they typically neglect to\nsystematically incorporate query image statistics to enhance prototype\nrepresentativeness. To address this issue, we propose FastRef, a novel and\nefficient prototype refinement framework for FS-IAD. Our method operates\nthrough an iterative two-stage process: (1) characteristic transfer from query\nfeatures to prototypes via an optimizable transformation matrix, and (2)\nanomaly suppression through prototype alignment. The characteristic transfer is\nachieved through linear reconstruction of query features from prototypes, while\nthe anomaly suppression addresses a key observation in FS-IAD that unlike\nconventional IAD with abundant normal prototypes, the limited-sample setting\nmakes anomaly reconstruction more probable. Therefore, we employ optimal\ntransport (OT) for non-Gaussian sampled features to measure and minimize the\ngap between prototypes and their refined counterparts for anomaly suppression.\nFor comprehensive evaluation, we integrate FastRef with three competitive\nprototype-based FS-IAD methods: PatchCore, FastRecon, WinCLIP, and AnomalyDINO.\nExtensive experiments across four benchmark datasets of MVTec, ViSA, MPDD and\nRealIAD demonstrate both the effectiveness and computational efficiency of our\napproach under 1/2/4-shots.", "comment": "18pages, 7figures, 6tables", "pdf_url": "http://arxiv.org/pdf/2506.21398v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21398v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "FastRef：用于少样本工业异常检测的快速原型细化", "tldr": "FastRef提出了一种新颖高效的原型细化框架，通过查询特征的特性转移和异常抑制来改善少样本工业异常检测中原型的代表性，并在多个基准数据集上展现出有效性和计算效率。", "motivation": "少样本工业异常检测（FS-IAD）在数据稀缺环境中对实际自动化检测系统构成严峻挑战。现有方法主要关注从有限的正常样本中提取原型，但通常忽略系统地结合查询图像统计信息来增强原型的代表性。", "method": "我们提出了FastRef，一种新颖高效的少样本工业异常检测原型细化框架。该方法通过迭代的两阶段过程进行操作：1）通过可优化变换矩阵将查询特征的特性转移到原型；2）通过原型对齐抑制异常。特性转移通过原型对查询特征进行线性重建实现，而异常抑制则针对少样本设置下异常重建更可能发生的问题，利用最优传输（OT）对非高斯采样特征进行测量并最小化原型与其细化对应物之间的差距。", "result": "在MVTec、ViSA、MPDD和RealIAD四个基准数据集上，通过与PatchCore、FastRecon、WinCLIP和AnomalyDINO等三种有竞争力的基于原型的FS-IAD方法集成并进行大量实验，结果证明了我们方法在1/2/4-shots设置下的有效性和计算效率。", "conclusion": "FastRef通过有效整合查询图像统计信息并解决少样本设置下异常重建概率增加的问题，显著提升了少样本工业异常检测中原型的代表性和性能，为数据稀缺环境下的自动化检测系统提供了有前景的解决方案。", "translation": "少样本工业异常检测（FS-IAD）对在数据稀缺环境中运行的实际自动化检测系统提出了严峻挑战。虽然现有方法主要侧重于从有限的正常样本中提取原型，但它们通常忽略系统地结合查询图像统计信息以增强原型的代表性。为了解决这个问题，我们提出了FastRef，一种新颖高效的少样本工业异常检测原型细化框架。我们的方法通过迭代的两阶段过程进行操作：（1）通过可优化变换矩阵将查询特征的特性转移到原型，以及（2）通过原型对齐抑制异常。特性转移是通过对查询特征从原型进行线性重建实现的，而异常抑制则解决了FS-IAD中的一个关键观察，即与拥有丰富正常原型的传统IAD不同，有限样本设置使得异常重建的可能性更大。因此，我们采用最优传输（OT）对非高斯采样特征进行测量，并最小化原型与其细化对应物之间的差距以抑制异常。为了进行全面评估，我们将FastRef与三种有竞争力的基于原型的FS-IAD方法（PatchCore、FastRecon、WinCLIP和AnomalyDINO）进行了整合。在MVTec、ViSA、MPDD和RealIAD四个基准数据集上，在1/2/4-shots设置下进行的广泛实验证明了我们方法的有效性和计算效率。", "summary": "本文提出FastRef，一种新颖高效的少样本工业异常检测（FS-IAD）原型细化框架。针对现有方法忽视查询图像统计信息和少样本下异常易重建的问题，FastRef通过迭代的两阶段过程工作：一是通过可优化变换矩阵将查询特征的特性转移到原型，二是通过最优传输进行原型对齐以抑制异常。该方法与多种现有FS-IAD方法结合，并在MVTec等四个基准数据集上验证了其在少样本设置下的有效性和计算效率。", "keywords": "少样本异常检测, 原型细化, 工业检测, 最优传输, FastRef", "comments": "FastRef的创新点在于其迭代的两阶段原型细化过程，特别是将查询图像统计信息系统地融入原型构建，以及利用最优传输（OT）来解决少样本设置下异常更容易被重建的问题。这对于数据稀缺的工业环境具有重要意义，因为它提高了原型在有限样本下的代表性，从而提升了异常检测的准确性。该方法不仅有效，还强调了计算效率，这在实际部署中是一个关键优势。"}}
{"id": "2506.21401", "title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction", "authors": ["Zhirui Gao. Renjiao Yi", "Yaqiao Dai", "Xuening Zhu", "Wei Chen", "Chenyang Zhu", "Kai Xu"], "summary": "This paper presents an end-to-end framework for reconstructing 3D parametric\ncurves directly from multi-view edge maps. Contrasting with existing two-stage\nmethods that follow a sequential ``edge point cloud reconstruction and\nparametric curve fitting'' pipeline, our one-stage approach optimizes 3D\nparametric curves directly from 2D edge maps, eliminating error accumulation\ncaused by the inherent optimization gap between disconnected stages. However,\nparametric curves inherently lack suitability for rendering-based multi-view\noptimization, necessitating a complementary representation that preserves their\ngeometric properties while enabling differentiable rendering. We propose a\nnovel bi-directional coupling mechanism between parametric curves and\nedge-oriented Gaussian components. This tight correspondence formulates a\ncurve-aware Gaussian representation, \\textbf{CurveGaussian}, that enables\ndifferentiable rendering of 3D curves, allowing direct optimization guided by\nmulti-view evidence. Furthermore, we introduce a dynamically adaptive topology\noptimization framework during training to refine curve structures through\nlinearization, merging, splitting, and pruning operations. Comprehensive\nevaluations on the ABC dataset and real-world benchmarks demonstrate our\none-stage method's superiority over two-stage alternatives, particularly in\nproducing cleaner and more robust reconstructions. Additionally, by directly\noptimizing parametric curves, our method significantly reduces the parameter\ncount during training, achieving both higher efficiency and superior\nperformance compared to existing approaches.", "comment": "Code: https://github.com/zhirui-gao/Curve-Gaussian Accepted by ICCV\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.21401v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21401v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "曲线感知高斯泼溅用于三维参数曲线重建", "tldr": "本文提出一种名为CurveGaussian的单阶段、端到端方法，通过曲线感知高斯表示直接从多视图边缘图重建三维参数曲线，有效解决了传统两阶段方法的误差累积问题，并实现了更高效、更鲁棒的重建。", "motivation": "现有三维参数曲线重建方法通常采用两阶段流程（边缘点云重建和参数曲线拟合），这导致了固有的优化间隙和误差累积。此外，参数曲线本身不适合基于渲染的多视图优化，缺乏一种能保持其几何特性同时支持可微分渲染的表示。", "method": "本文提出了一个端到端的单阶段框架，直接从多视图2D边缘图优化3D参数曲线。核心创新是提出了一种新颖的参数曲线与边缘导向高斯分量之间的双向耦合机制，形成了“曲线感知高斯表示”（CurveGaussian），从而实现了3D曲线的可微分渲染，支持由多视图证据引导的直接优化。此外，还引入了一个动态自适应拓扑优化框架，在训练过程中通过线性化、合并、分裂和修剪操作来精炼曲线结构。", "result": "在ABC数据集和真实世界基准上的综合评估表明，本文提出的单阶段方法优于两阶段替代方案，尤其在生成更清晰、更鲁棒的重建方面表现出色。通过直接优化参数曲线，该方法显著减少了训练期间的参数数量，实现了更高的效率和卓越的性能。", "conclusion": "本文成功提出了一个创新的单阶段端到端框架，通过曲线感知高斯表示和动态拓扑优化，直接从多视图边缘图重建三维参数曲线。该方法有效解决了传统两阶段方法的误差累积问题，并在重建质量、鲁棒性和训练效率方面均超越了现有方法。", "translation": "本文提出了一个端到端框架，用于直接从多视图边缘图重建三维参数曲线。与遵循“边缘点云重建和参数曲线拟合”顺序管道的现有两阶段方法不同，我们的一阶段方法直接从2D边缘图优化3D参数曲线，消除了由不连贯阶段之间固有的优化间隙引起的误差累积。然而，参数曲线本身不适合基于渲染的多视图优化，因此需要一种互补的表示，既能保留其几何特性，又能实现可微分渲染。我们提出了一种新颖的参数曲线和边缘导向高斯分量之间的双向耦合机制。这种紧密的对应关系形成了一种曲线感知高斯表示，即\\textbf{CurveGaussian}，它实现了3D曲线的可微分渲染，允许由多视图证据引导的直接优化。此外，我们在训练期间引入了一个动态自适应拓扑优化框架，通过线性化、合并、分裂和修剪操作来细化曲线结构。在ABC数据集和真实世界基准上的综合评估表明，我们的一阶段方法优于两阶段替代方案，特别是在生成更清晰、更鲁棒的重建方面。此外，通过直接优化参数曲线，我们的方法显著减少了训练期间的参数数量，与现有方法相比，实现了更高的效率和卓越的性能。", "summary": "本文提出了一种名为CurveGaussian的端到端单阶段框架，用于直接从多视图边缘图重建三维参数曲线。该方法通过引入参数曲线与边缘导向高斯分量之间的双向耦合机制，构建了曲线感知高斯表示，实现了3D曲线的可微分渲染和直接优化。结合动态自适应拓扑优化，克服了现有两阶段方法的误差累积问题，并在效率和重建质量上均表现出优越性。", "keywords": "3D参数曲线重建, 高斯泼溅, 可微分渲染, 单阶段方法, 拓扑优化", "comments": "这篇论文的创新点在于提出了“曲线感知高斯表示”（CurveGaussian），巧妙地将参数曲线与高斯泼溅技术结合，解决了参数曲线难以直接进行可微分渲染的难题。其单阶段端到端的设计显著减少了传统两阶段方法的误差累积，提高了重建的鲁棒性和效率。动态拓扑优化框架的引入也增强了方法对复杂曲线结构的处理能力。这种将几何建模与可微分渲染相结合的思路，为3D重建领域提供了新的视角。"}}
{"id": "2506.21278", "title": "Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution", "authors": ["Lukas Sablica", "Kurt Hornik"], "summary": "We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21278v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21278v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "使用高效球面柯西分布的超球面变分自编码器", "tldr": "本文提出一种新的变分自编码器（VAE）架构，使用球面柯西（spCauchy）潜在分布，解决了传统方法的问题，提供了更稳定高效的高维生成建模。", "motivation": "传统VAE中使用的潜在分布（如高斯或von Mises-Fisher (vMF)）在处理超球面表示时存在局限性，例如无法很好地捕获方向性数据、可能导致过度正则化，以及vMF存在数值不稳定性（涉及Bessel函数和超几何函数计算）。", "method": "提出了一种新的变分自编码器（VAE）架构，该架构采用球面柯西（spCauchy）潜在分布。spCauchy通过莫比乌斯变换实现了完全可微分且高效的重参数化技巧，并且其KL散度可以通过快速收敛的幂级数计算。", "result": "spCauchy分布为潜在变量提供了更自然的超球面表示，更好地捕获方向性数据，同时保持灵活性。其重尾特性防止了过度正则化，确保了潜在空间的高效利用，并提供了更具表达力的表示。它规避了vMF固有的数值不稳定性，实现了稳定和可扩展的训练，并消除了KL散度计算中下溢或溢出的担忧。", "conclusion": "球面柯西（spCauchy）分布是变分自编码器（VAEs）的一个引人注目的替代方案，在高维生成建模中提供了理论优势和实际效率。", "translation": "我们提出了一种新颖的变分自编码器（VAE）架构，该架构采用球面柯西（spCauchy）潜在分布。与传统的 Gaussian 潜在空间或广泛使用的 von Mises-Fisher (vMF) 分布不同，spCauchy 为潜在变量提供了更自然的超球面表示，更好地捕获方向性数据，同时保持灵活性。其重尾特性防止了过度正则化，确保了潜在空间的高效利用，同时提供了更具表达力的表示。此外，spCauchy 规避了 vMF 固有的数值不稳定性，这些不稳定性源于涉及贝塞尔函数计算的归一化常数。相反，它通过莫比乌斯变换实现了完全可微分且高效的重参数化技巧，从而实现稳定和可扩展的训练。KL 散度可以通过快速收敛的幂级数计算，消除了与评估超几何函数比率相关的下溢或溢出问题。这些特性使得 spCauchy 成为 VAEs 的一个引人注目的替代方案，在高维生成建模中提供了理论优势和实际效率。", "summary": "本文提出了一种使用球面柯西（spCauchy）潜在分布的新型变分自编码器（VAE）架构。与传统的高斯或von Mises-Fisher (vMF) 分布相比，spCauchy能更好地捕捉方向性数据，防止过度正则化，并避免vMF的数值不稳定问题。通过莫比乌斯变换实现高效重参数化，并通过快速收敛的幂级数计算KL散度，确保了高维生成建模中的稳定和可扩展训练，展现出理论和实践上的优势。", "keywords": "变分自编码器, 球面柯西分布, 超球面表示, 重参数化, 生成建模", "comments": "这篇论文的创新点在于引入了球面柯西（spCauchy）分布作为变分自编码器（VAE）的潜在空间分布。它有效地解决了传统高斯和vMF分布在处理超球面数据时的局限性，特别是vMF的数值稳定性问题。通过采用莫比乌斯变换和幂级数计算KL散度，该方法提供了更稳定、高效且表达力更强的潜在表示，对于高维生成建模具有重要意义。"}}
{"id": "2506.21416", "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation", "authors": ["Bowen Chen", "Mengyi Zhao", "Haomiao Sun", "Li Chen", "Xu Wang", "Kang Du", "Xinglong Wu"], "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.", "comment": "Project Page: https://bytedance.github.io/XVerse Github Link:\n  https://github.com/bytedance/XVerse", "pdf_url": "http://arxiv.org/pdf/2506.21416v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21416v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "XVerse：通过DiT调制实现身份和语义属性的多主体一致性控制", "tldr": "XVerse是一个新的模型，通过对文本流进行令牌特定调制，实现了对多主体图像生成中身份和语义属性的精细、独立控制，解决了现有方法中存在的编辑性差和属性纠缠问题。", "motivation": "在文本到图像生成中，对主体身份和语义属性（姿态、风格、光照）进行细粒度控制，特别是针对多个主体时，经常会损害扩散变换器（DiTs）的可编辑性和连贯性，导致伪影或属性纠缠。", "method": "我们提出了一个新颖的多主体受控生成模型XVerse。通过将参考图像转换为令牌特定文本流调制的偏移量，XVerse能够对特定主体进行精确和独立的控制，而不会破坏图像潜在特征。", "result": "XVerse提供了高保真、可编辑的多主体图像合成，并能对个体主体特征和语义属性进行稳健控制。", "conclusion": "XVerse显著改善了个性化和复杂场景的生成能力，为多主体图像生成提供了强大的控制方案。", "translation": "实现对文本到图像生成中主体身份和语义属性（姿态、风格、光照）的细粒度控制，特别是对于多个主体，通常会损害扩散变换器（DiTs）的可编辑性和连贯性。许多方法会引入伪影或遭受属性纠缠。为了克服这些挑战，我们提出了一种新颖的多主体受控生成模型XVerse。通过将参考图像转换为令牌特定文本流调制的偏移量，XVerse允许对特定主体进行精确和独立的控制，而不会破坏图像潜在特征或特性。因此，XVerse提供了高保真、可编辑的多主体图像合成，并能对个体主体特征和语义属性进行稳健控制。这一进展显著改善了个性化和复杂场景的生成能力。", "summary": "XVerse是一种新颖的多主体受控生成模型，旨在解决文本到图像生成中，特别是多主体场景下，对主体身份和语义属性进行细粒度控制时，现有扩散变换器（DiTs）面临的编辑性差和属性纠缠问题。该模型通过将参考图像转化为令牌特定的文本流调制偏移量，实现了对特定主体的精确和独立控制，且不干扰图像潜在特征。最终，XVerse能够生成高保真、可编辑的多主体图像，并对个体主体特征和语义属性具有强大的控制能力，从而显著提升了个性化和复杂场景的生成效果。", "keywords": "多主体控制, 扩散变换器, 身份控制, 语义属性, 文本到图像生成", "comments": "XVerse的创新点在于其独特的“令牌特定文本流调制”机制，通过将参考图像转换为偏移量，实现了对多个主体身份和语义属性的精确且独立的控制，避免了传统方法中常见的属性纠缠和伪影问题。这对于提升文本到图像生成在复杂场景和个性化需求方面的应用具有重要意义。"}}
{"id": "2506.20831", "title": "Efficacy of Temporal Fusion Transformers for Runoff Simulation", "authors": ["Sinan Rasiya Koya", "Tirthankar Roy"], "summary": "Combining attention with recurrence has shown to be valuable in sequence\nmodeling, including hydrological predictions. Here, we explore the strength of\nTemporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks\nin rainfall-runoff modeling. We train ten randomly initialized models, TFT and\nLSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five\nsubsets of the Caravan dataset, each representing catchments in the US,\nAustralia, Brazil, Great Britain, and Chile. Then, the performance of the\nmodels, their variability regarding the catchment attributes, and the\ndifference according to the datasets are assessed. Our findings show that TFT\nslightly outperforms LSTM, especially in simulating the midsection and peak of\nhydrographs. Furthermore, we show the ability of TFT to handle longer sequences\nand why it can be a better candidate for higher or larger catchments. Being an\nexplainable AI technique, TFT identifies the key dynamic and static variables,\nproviding valuable scientific insights. However, both TFT and LSTM exhibit a\nconsiderable drop in performance with the Caravan dataset, indicating possible\ndata quality issues. Overall, the study highlights the potential of TFT in\nimproving hydrological modeling and understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20831v1", "categories": ["physics.geo-ph", "cs.LG", "stat.AP"], "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.20831v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "时间融合转换器在径流模拟中的效用", "tldr": "本研究探讨了时间融合转换器（TFTs）在降雨-径流建模中相对于长短期记忆（LSTM）网络的优势，结果显示TFTs略优于LSTM，尤其在模拟径流中段和峰值方面，并能处理更长序列和提供可解释性。", "motivation": "结合注意力机制和循环神经网络在序列建模（包括水文预测）中显示出价值。本研究旨在探索时间融合转换器（TFTs）在降雨-径流建模中相对于长短期记忆（LSTM）网络的优势。", "method": "研究训练了十个随机初始化的时间融合转换器（TFT）和长短期记忆（LSTM）模型，使用美国531个CAMELS流域的数据。实验还重复了五次，使用了Caravan数据集的不同子集，这些子集代表了美国、澳大利亚、巴西、英国和智利的流域。然后，评估了模型的性能、其在流域属性方面的变异性以及数据集之间的差异。", "result": "研究发现，时间融合转换器（TFT）略优于长短期记忆（LSTM），特别是在模拟水文图的中段和峰值方面。此外，TFT能够处理更长的序列，并且可能是更大或更高流域的更好选择。作为一种可解释的人工智能技术，TFT能够识别关键的动态和静态变量，提供有价值的科学见解。然而，TFT和LSTM在Caravan数据集上的性能都显著下降，这表明可能存在数据质量问题。", "conclusion": "这项研究总体上强调了时间融合转换器（TFT）在改进水文建模和理解方面的潜力。", "translation": "将注意力与循环相结合在序列建模（包括水文预测）中显示出价值。在这里，我们探讨了时间融合转换器（TFTs）在降雨-径流建模中相对于长短期记忆（LSTM）网络的优势。我们对美国531个CAMELS流域训练了十个随机初始化的模型，包括TFT和LSTM。我们用Caravan数据集的五个子集重复了实验，每个子集代表了美国、澳大利亚、巴西、英国和智利的流域。然后，评估了模型的性能、其在流域属性方面的变异性以及根据数据集的不同。我们的发现表明，TFT略优于LSTM，尤其是在模拟水文图的中段和峰值方面。此外，我们展示了TFT处理更长序列的能力，以及为什么它可能是更高或更大流域的更好选择。作为一种可解释的人工智能技术，TFT识别了关键的动态和静态变量，提供了有价值的科学见解。然而，TFT和LSTM在Caravan数据集上的性能都表现出显著下降，这表明可能存在数据质量问题。总的来说，这项研究强调了TFT在改进水文建模和理解方面的潜力。", "summary": "本研究评估了时间融合转换器（TFT）在径流模拟中相对于长短期记忆（LSTM）网络的性能。通过在美国CAMELS流域和Caravan数据集的全球流域上进行训练和测试，结果表明TFT在模拟水文图的峰值和中段方面略优于LSTM，并能有效处理长序列。TFT作为一种可解释的AI技术，还能识别关键变量，提供科学洞察。尽管在某些数据集上性能有所下降，但研究强调了TFT在改进水文建模和理解方面的巨大潜力。", "keywords": "时间融合转换器, 径流模拟, LSTM, 水文建模, 可解释AI", "comments": "该论文的创新点在于首次系统性地评估了时间融合转换器（TFT）在径流模拟中的效用，并将其与广泛使用的LSTM进行了比较。TFT在处理长序列数据和提供模型可解释性方面的优势，对于水文领域具有重要意义，因为它不仅提升了预测精度，还为理解径流过程中的关键变量提供了工具。然而，论文也指出了在Caravan数据集上性能下降的问题，这提示了未来研究需要关注数据质量对模型表现的影响。"}}
{"id": "2506.20839", "title": "Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys", "authors": ["Jing Luo", "Yejun Gu", "Yanfei Wang", "Xiaolong Ma", "Jaafar. A El-Awady"], "summary": "Machine learning has significantly advanced the understanding and application\nof structural materials, with an increasing emphasis on integrating existing\ndata and quantifying uncertainties in predictive modeling. This study presents\na comprehensive methodology utilizing a mixed density network (MDN) model,\ntrained on extensive experimental data from literature. This approach uniquely\npredicts the distribution of dislocation density, inferred as a latent\nvariable, and the resulting stress distribution at the grain level. The\nincorporation of statistical parameters of those predicted distributions into a\ndislocation-mediated plasticity model allows for accurate stress-strain\npredictions with explicit uncertainty quantification. This strategy not only\nimproves the accuracy and reliability of mechanical property predictions but\nalso plays a vital role in optimizing alloy design, thereby facilitating the\ndevelopment of new materials in a rapidly evolving industry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20839v1", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.20839v1", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "不确定性感知机器学习框架，用于预测面心立方合金中的位错塑性与应力-应变响应", "tldr": "本文提出了一个基于混合密度网络（MDN）的机器学习框架，能够预测FCC合金的位错塑性和应力-应变响应，并量化不确定性，以提高材料设计效率。", "motivation": "机器学习在结构材料理解和应用方面取得了显著进展，但需要更好地整合现有数据并量化预测模型中的不确定性。本研究旨在解决这一需求，提高机械性能预测的准确性和可靠性，并优化合金设计。", "method": "本研究提出了一种综合方法，利用混合密度网络（MDN）模型，该模型通过大量文献实验数据进行训练。此方法独特地预测位错密度分布（推断为潜在变量）和晶粒级别的应力分布。通过将这些预测分布的统计参数整合到位错介导的塑性模型中，实现了对应力-应变响应的准确预测，并明确量化了不确定性。", "result": "该方法不仅提高了机械性能预测的准确性和可靠性，而且在优化合金设计方面发挥了关键作用，从而促进了新材料的开发。", "conclusion": "本研究提出的不确定性感知机器学习框架通过预测位错塑性和应力-应变响应，并量化不确定性，显著提高了机械性能预测的准确性和可靠性，并有助于优化合金设计和新材料开发。", "translation": "机器学习在结构材料的理解和应用方面取得了显著进展，越来越重视整合现有数据和量化预测模型中的不确定性。本研究提出了一种综合方法，利用混合密度网络（MDN）模型，该模型通过大量文献实验数据进行训练。这种方法独特地预测位错密度分布（推断为潜在变量）和晶粒级别的应力分布。通过将这些预测分布的统计参数整合到位错介导的塑性模型中，实现了对应力-应变响应的准确预测，并明确量化了不确定性。这一策略不仅提高了机械性能预测的准确性和可靠性，而且在优化合金设计方面发挥了关键作用，从而促进了快速发展行业中新材料的开发。", "summary": "本研究提出了一个不确定性感知的机器学习框架，利用混合密度网络（MDN）模型，通过实验数据训练，预测面心立方（FCC）合金的位错密度和晶粒级应力分布。通过将这些预测分布的统计参数整合到位错介导的塑性模型中，实现了对应力-应变响应的准确预测并量化了不确定性。此方法显著提高了机械性能预测的准确性和可靠性，并有助于优化合金设计和加速新材料开发。", "keywords": "机器学习, 位错塑性, 应力-应变, 不确定性量化, FCC合金", "comments": "该研究的创新之处在于将混合密度网络（MDN）模型应用于预测材料的潜在变量（如位错密度）及其分布，并明确量化了预测的不确定性，这对于材料科学中的可靠预测至关重要。其重要性体现在提高了机械性能预测的准确性和可靠性，并为优化合金设计和新材料开发提供了有力的工具，特别是在快速发展的工业背景下。"}}
{"id": "2506.21430", "title": "HyperSORT: Self-Organising Robust Training with hyper-networks", "authors": ["Samuel Joutard", "Marijn Stollenga", "Marc Balle Sanchez", "Mohammad Farid Azampour", "Raphael Prevost"], "summary": "Medical imaging datasets often contain heterogeneous biases ranging from\nerroneous labels to inconsistent labeling styles. Such biases can negatively\nimpact deep segmentation networks performance. Yet, the identification and\ncharacterization of such biases is a particularly tedious and challenging task.\nIn this paper, we introduce HyperSORT, a framework using a hyper-network\npredicting UNets' parameters from latent vectors representing both the image\nand annotation variability. The hyper-network parameters and the latent vector\ncollection corresponding to each data sample from the training set are jointly\nlearned. Hence, instead of optimizing a single neural network to fit a dataset,\nHyperSORT learns a complex distribution of UNet parameters where low density\nareas can capture noise-specific patterns while larger modes robustly segment\norgans in differentiated but meaningful manners. We validate our method on two\n3D abdominal CT public datasets: first a synthetically perturbed version of the\nAMOS dataset, and TotalSegmentator, a large scale dataset containing real\nunknown biases and errors. Our experiments show that HyperSORT creates a\nstructured mapping of the dataset allowing the identification of relevant\nsystematic biases and erroneous samples. Latent space clusters yield UNet\nparameters performing the segmentation task in accordance with the underlying\nlearned systematic bias. The code and our analysis of the TotalSegmentator\ndataset are made available: https://github.com/ImFusionGmbH/HyperSORT", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.21430v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21430v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "HyperSORT：基于超网络的自组织鲁棒训练", "tldr": "HyperSORT是一种使用超网络来处理医学图像数据集中异构偏差的框架，通过学习UNet参数的复杂分布，实现鲁棒分割并识别数据集中的系统性偏差和错误样本。", "motivation": "医学影像数据集常包含从错误标签到不一致标注风格的异构偏差，这些偏差会严重影响深度分割网络的性能，而识别和表征这些偏差是一项特别繁琐和具有挑战性的任务。", "method": "本文引入了HyperSORT框架，该框架使用一个超网络，根据代表图像和标注变异性的潜在向量来预测UNets的参数。超网络参数和训练集中每个数据样本对应的潜在向量集合是共同学习的。因此，HyperSORT不是优化单个神经网络来拟合数据集，而是学习UNet参数的复杂分布，其中低密度区域可以捕获噪声特定模式，而较大的模式则能以区分但有意义的方式稳健地分割器官。", "result": "实验表明，HyperSORT创建了数据集的结构化映射，从而能够识别相关的系统性偏差和错误样本。潜在空间聚类产生的UNet参数能够根据底层学习到的系统性偏差执行分割任务。", "conclusion": "HyperSORT框架能够有效处理医学影像数据中的异构偏差，通过学习UNet参数的复杂分布，实现鲁棒的器官分割，并提供了一种识别数据集系统性偏差和错误样本的结构化方法。", "translation": "医学影像数据集通常包含从错误标签到不一致标注风格的异构偏差。这些偏差会对深度分割网络的性能产生负面影响。然而，识别和表征此类偏差是一项特别繁琐和具有挑战性的任务。在本文中，我们引入了HyperSORT，一个使用超网络从代表图像和标注变异性的潜在向量预测UNets参数的框架。超网络参数和训练集中每个数据样本对应的潜在向量集合是共同学习的。因此，HyperSORT不是优化单个神经网络来拟合数据集，而是学习UNet参数的复杂分布，其中低密度区域可以捕获噪声特定模式，而较大的模式则能以区分但有意义的方式稳健地分割器官。我们在两个3D腹部CT公共数据集上验证了我们的方法：首先是AMOS数据集的一个合成扰动版本，以及TotalSegmentator，一个包含真实未知偏差和错误的大规模数据集。我们的实验表明，HyperSORT创建了数据集的结构化映射，从而能够识别相关的系统性偏差和错误样本。潜在空间聚类产生的UNet参数能够根据底层学习到的系统性偏差执行分割任务。代码和我们对TotalSegmentator数据集的分析已公开：https://github.com/ImFusionGmbH/HyperSORT", "summary": "HyperSORT是一种新颖的框架，旨在解决医学影像数据集中存在的异构偏差（如错误标签和不一致的标注风格）对深度分割网络性能的影响。该方法利用一个超网络从表示图像和标注变异性的潜在向量中学习UNet的参数分布，而非单一网络优化。通过联合学习超网络参数和数据样本的潜在向量，HyperSORT能够区分数据集中的噪声模式和有效分割模式。实验在合成扰动的AMOS数据集和包含真实偏差的TotalSegmentator数据集上进行，结果表明HyperSORT能有效地创建数据集的结构化映射，从而识别系统性偏差和错误样本，并且潜在空间聚类能产生与学习到的偏差一致的分割参数。", "keywords": "超网络, 鲁棒训练, 医学影像, 分割, 数据偏差", "comments": "HyperSORT的创新之处在于其利用超网络来学习UNet参数的复杂分布，从而能够应对医学影像数据中常见的异构偏差。这种方法超越了传统的单一模型优化，能够更精细地处理数据变异性，并识别出数据集中的噪声和错误样本。这对于提高深度学习模型在真实世界、有噪声数据上的鲁棒性和可靠性具有重要意义。"}}
{"id": "2506.21444", "title": "Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation", "authors": ["Sweta Banerjee", "Viktoria Weiss", "Taryn A. Donovan", "Rutger A. Fick", "Thomas Conrad", "Jonas Ammeling", "Nils Porsche", "Robert Klopfleisch", "Christopher Kaltenecker", "Katharina Breininger", "Marc Aubreville", "Christof A. Bertram"], "summary": "Atypical mitoses mark a deviation in the cell division process that can be an\nindependent prognostically relevant marker for tumor malignancy. However, their\nidentification remains challenging due to low prevalence, at times subtle\nmorphological differences from normal mitoses, low inter-rater agreement among\npathologists, and class imbalance in datasets. Building on the Atypical Mitosis\ndataset for Breast Cancer (AMi-Br), this study presents a comprehensive\nbenchmark comparing deep learning approaches for automated atypical mitotic\nfigure (AMF) classification, including baseline models, foundation models with\nlinear probing, and foundation models fine-tuned with low-rank adaptation\n(LoRA). For rigorous evaluation, we further introduce two new hold-out AMF\ndatasets - AtNorM-Br, a dataset of mitoses from the The TCGA breast cancer\ncohort, and AtNorM-MD, a multi-domain dataset of mitoses from the MIDOG++\ntraining set. We found average balanced accuracy values of up to 0.8135,\n0.7696, and 0.7705 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and\nAtNorM-MD datasets, respectively, with the results being particularly good for\nLoRA-based adaptation of the Virchow-line of foundation models. Our work shows\nthat atypical mitosis classification, while being a challenging problem, can be\neffectively addressed through the use of recent advances in transfer learning\nand model fine-tuning techniques. We make available all code and data used in\nthis paper in this github repository:\nhttps://github.com/DeepMicroscopy/AMi-Br_Benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21444v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21444v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "深度学习和视觉基础模型在非典型与正常有丝分裂分类中的基准测试及跨数据集评估", "tldr": "本研究对深度学习和视觉基础模型在非典型有丝分裂分类中的性能进行了全面基准测试，并引入了新的数据集进行跨领域评估，结果表明迁移学习和模型微调技术能有效解决这一挑战性问题。", "motivation": "非典型有丝分裂是肿瘤恶性程度的独立预后相关标志物，但其识别具有挑战性，原因包括：患病率低、与正常有丝分裂形态差异细微、病理学家间判读一致性低以及数据集中类别不平衡。", "method": "本研究基于乳腺癌非典型有丝分裂数据集（AMi-Br），对自动化非典型有丝分裂（AMF）分类的深度学习方法进行了全面基准测试，包括基线模型、使用线性探测的基础模型以及使用低秩适应（LoRA）进行微调的基础模型。为了进行严格评估，还引入了两个新的保留AMF数据集：来自TCGA乳腺癌队列的AtNorM-Br数据集和来自MIDOG++训练集的多领域AtNorM-MD数据集。", "result": "在域内AMi-Br数据集上，平均平衡准确率最高达到0.8135；在域外AtNorm-Br和AtNorM-MD数据集上，分别达到0.7696和0.7705。其中，基于LoRA的Virchow系列基础模型表现尤为出色。", "conclusion": "本研究表明，非典型有丝分裂分类虽然是一个具有挑战性的问题，但通过利用迁移学习和模型微调技术的最新进展，可以有效地解决。", "translation": "非典型有丝分裂标志着细胞分裂过程中的偏差，可以作为肿瘤恶性程度的独立预后相关标志物。然而，由于患病率低、有时与正常有丝分裂形态差异细微、病理学家之间判读一致性低以及数据集中类别不平衡，其识别仍然具有挑战性。本研究基于乳腺癌非典型有丝分裂数据集（AMi-Br），对自动化非典型有丝分裂（AMF）分类的深度学习方法进行了全面基准测试，包括基线模型、使用线性探测的基础模型以及使用低秩适应（LoRA）进行微调的基础模型。为了进行严格评估，我们进一步引入了两个新的保留AMF数据集——AtNorM-Br（来自TCGA乳腺癌队列的有丝分裂数据集）和AtNorM-MD（来自MIDOG++训练集的多领域有丝分裂数据集）。我们发现在域内AMi-Br数据集上，平均平衡准确率最高达到0.8135；在域外AtNorm-Br和AtNorM-MD数据集上，分别达到0.7696和0.7705。其中，基于LoRA的Virchow系列基础模型表现尤为出色。我们的工作表明，非典型有丝分裂分类虽然是一个具有挑战性的问题，但通过利用迁移学习和模型微调技术的最新进展，可以有效地解决。我们已将本文中使用的所有代码和数据发布到此GitHub仓库：https://github.com/DeepMicroscopy/AMi-Br_Benchmark。", "summary": "本研究全面评估了深度学习和视觉基础模型在非典型与正常有丝分裂分类中的性能。针对非典型有丝分裂识别的挑战，研究人员对基线模型、线性探测的基础模型和LoRA微调的基础模型进行了基准测试。为确保评估的严谨性，引入了两个新的跨领域数据集。结果显示，基于LoRA微调的基础模型在域内和域外数据集上均表现良好，证明了迁移学习和微调技术在解决此挑战性问题上的有效性。", "keywords": "非典型有丝分裂, 深度学习, 基础模型, 迁移学习, LoRA", "comments": "本文的创新之处在于其对非典型有丝分裂分类问题进行了全面且严谨的基准测试，特别是在引入新的跨数据集进行评估，增强了结果的泛化能力。LoRA在基础模型上的应用是其重要贡献，表明了在医学图像分析中有效利用预训练大模型的潜力。这对于提高病理诊断的自动化和标准化具有重要意义。"}}
{"id": "2506.21446", "title": "Controllable 3D Placement of Objects with Scene-Aware Diffusion Models", "authors": ["Mohamed Omran", "Dimitris Kalatzis", "Jens Petersen", "Amirhossein Habibian", "Auke Wiggers"], "summary": "Image editing approaches have become more powerful and flexible with the\nadvent of powerful text-conditioned generative models. However, placing objects\nin an environment with a precise location and orientation still remains a\nchallenge, as this typically requires carefully crafted inpainting masks or\nprompts. In this work, we show that a carefully designed visual map, combined\nwith coarse object masks, is sufficient for high quality object placement. We\ndesign a conditioning signal that resolves ambiguities, while being flexible\nenough to allow for changing of shapes or object orientations. By building on\nan inpainting model, we leave the background intact by design, in contrast to\nmethods that model objects and background jointly. We demonstrate the\neffectiveness of our method in the automotive setting, where we compare\ndifferent conditioning signals in novel object placement tasks. These tasks are\ndesigned to measure edit quality not only in terms of appearance, but also in\nterms of pose and location accuracy, including cases that require non-trivial\nshape changes. Lastly, we show that fine location control can be combined with\nappearance control to place existing objects in precise locations in a scene.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21446v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21446v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "可控的3D物体放置与场景感知扩散模型", "tldr": "该研究提出了一种基于视觉地图和粗糙物体掩码的扩散模型，用于精确控制场景中物体的3D放置，同时保持背景不变。", "motivation": "尽管文本条件生成模型使图像编辑更加强大和灵活，但在环境中精确放置具有特定位置和方向的物体仍然是一个挑战，这通常需要精心制作的修复掩码或提示。", "method": "该方法通过结合精心设计的视觉地图和粗糙的物体掩码来实现高质量的物体放置。它设计了一个条件信号来解决歧义，并允许改变形状或物体方向。该方法基于修复模型构建，旨在保持背景不变，这与同时建模物体和背景的方法不同。", "result": "该方法在汽车场景中展示了其有效性，通过比较不同条件信号在新型物体放置任务中的表现。这些任务旨在衡量编辑质量，不仅包括外观，还包括姿态和位置精度，甚至涉及非平凡的形状变化。此外，该方法还能将精细的位置控制与外观控制相结合，以在场景中精确位置放置现有物体。", "conclusion": "该研究表明，通过结合精心设计的视觉地图和粗糙物体掩码，可以实现高质量和精确控制的物体3D放置，解决了现有方法在精确位置和方向控制方面的挑战，并在汽车场景中验证了其有效性。", "translation": "图像编辑方法随着强大的文本条件生成模型的出现而变得更加强大和灵活。然而，在环境中精确放置具有特定位置和方向的物体仍然是一个挑战，因为这通常需要精心制作的修复掩码或提示。在这项工作中，我们展示了精心设计的视觉地图与粗糙物体掩码相结合，足以实现高质量的物体放置。我们设计了一种条件信号，它能解决歧义，同时足够灵活以允许改变形状或物体方向。通过建立在修复模型的基础上，我们有意保持背景不变，这与同时建模物体和背景的方法形成对比。我们在汽车环境中展示了我们方法的有效性，其中我们比较了新型物体放置任务中不同的条件信号。这些任务旨在衡量编辑质量，不仅包括外观，还包括姿态和位置精度，包括需要非平凡形状变化的情况。最后，我们展示了精细位置控制可以与外观控制相结合，以在场景中精确位置放置现有物体。", "summary": "本文提出了一种基于场景感知扩散模型的新方法，用于解决在图像中精确控制物体3D放置的挑战。该方法利用精心设计的视觉地图和粗糙物体掩码作为条件信号，以实现高质量的物体放置，同时保持背景不变并允许物体形状和方向的灵活变化。研究在汽车场景中验证了其有效性，并证明了其在姿态和位置精度方面的优越性，以及将位置和外观控制相结合的能力。", "keywords": "3D物体放置, 扩散模型, 场景感知, 图像编辑, 条件生成", "comments": "这项工作通过引入视觉地图和粗糙物体掩码作为条件信号，为可控的3D物体放置提供了一个新颖且有效的方法。其创新之处在于能够精确控制物体的位置和方向，同时保持背景完整，这在实际应用中非常重要。特别是在汽车场景中的应用展示了其潜力。该方法解决了现有生成模型在精细3D放置方面的局限性，具有重要的实际意义。"}}
{"id": "2506.20910", "title": "Faster Fixed-Point Methods for Multichain MDPs", "authors": ["Matthew Zurek", "Yudong Chen"], "summary": "We study value-iteration (VI) algorithms for solving general (a.k.a.\nmultichain) Markov decision processes (MDPs) under the average-reward\ncriterion, a fundamental but theoretically challenging setting. Beyond the\ndifficulties inherent to all average-reward problems posed by the lack of\ncontractivity and non-uniqueness of solutions to the Bellman operator, in the\nmultichain setting an optimal policy must solve the navigation subproblem of\nsteering towards the best connected component, in addition to optimizing\nlong-run performance within each component. We develop algorithms which better\nsolve this navigational subproblem in order to achieve faster convergence for\nmultichain MDPs, obtaining improved rates of convergence and sharper measures\nof complexity relative to prior work. Many key components of our results are of\npotential independent interest, including novel connections between\naverage-reward and discounted problems, optimal fixed-point methods for\ndiscounted VI which extend to general Banach spaces, new sublinear convergence\nrates for the discounted value error, and refined suboptimality decompositions\nfor multichain MDPs. Overall our results yield faster convergence rates for\ndiscounted and average-reward problems and expand the theoretical foundations\nof VI approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20910v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.20910v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多链马尔可夫决策过程的更快不动点方法", "tldr": "本文研究了用于解决平均奖励准则下的多链马尔可夫决策过程（MDPs）的价值迭代（VI）算法，并开发了新的算法来更好地解决导航子问题，从而实现了更快的收敛速度和更低的复杂性。", "motivation": "在平均奖励准则下解决多链马尔可夫决策过程（MDPs）是一个基本但理论上具有挑战性的问题。现有方法在多链设置中存在收敛速度慢和复杂性高的问题，特别是需要解决导航子问题。", "method": "本文开发了新的价值迭代（VI）算法，通过更好地解决导航子问题来加速多链MDPs的收敛。研究包括平均奖励问题与折扣问题之间的新联系，适用于一般巴纳赫空间的折扣VI最优不动点方法，折扣值误差的新次线性收敛率，以及多链MDPs的细化次优分解。", "result": "获得了改进的收敛速度和相对于现有工作更精确的复杂性度量。主要成果包括平均奖励问题与折扣问题之间的新联系，适用于一般巴纳赫空间的折扣VI最优不动点方法，折扣值误差的新次线性收敛率，以及多链MDPs的细化次优分解。", "conclusion": "本文提出的算法为折扣和平均奖励问题带来了更快的收敛速度，并扩展了价值迭代方法的理论基础。", "translation": "我们研究了在平均奖励准则下解决通用（又称多链）马尔可夫决策过程（MDPs）的价值迭代（VI）算法，这是一个基础但理论上具有挑战性的设置。除了所有平均奖励问题固有的非收缩性和贝尔曼算子解的非唯一性所带来的困难之外，在多链设置中，最优策略除了优化每个组件内的长期性能外，还必须解决导航子问题，即引导至最佳连接组件。我们开发了能够更好地解决此导航子问题的算法，以实现多链MDPs的更快收敛，相对于先前的工作，获得了改进的收敛速度和更精确的复杂性度量。我们结果中的许多关键组成部分可能具有独立的潜在兴趣，包括平均奖励问题与折扣问题之间的新颖联系，可扩展到一般巴纳赫空间的折扣VI最优不动点方法，折扣值误差的新次线性收敛率，以及多链MDPs的细化次优分解。总的来说，我们的结果为折扣和平均奖励问题带来了更快的收敛速度，并扩展了VI方法的理论基础。", "summary": "本文研究了平均奖励准则下的多链马尔可夫决策过程（MDPs）的价值迭代（VI）算法。针对该设置中贝尔曼算子的非收缩性、解的非唯一性以及导航子问题带来的挑战，作者开发了新的算法。这些算法通过改进导航子问题的解决，显著提高了收敛速度并降低了复杂性。研究还揭示了平均奖励问题与折扣问题的新联系，提出了适用于一般巴纳赫空间的最优折扣VI不动点方法，以及新的次线性收敛率和细化的次优分解。", "keywords": "马尔可夫决策过程, 平均奖励, 价值迭代, 不动点方法, 收敛速度", "comments": "这项工作在解决多链MDPs的平均奖励问题方面取得了重要进展，特别是在提高价值迭代算法的收敛速度方面。其创新之处在于提出了更好地解决导航子问题的方法，并建立了平均奖励问题与折扣问题之间的新颖联系。此外，将最优不动点方法扩展到一般巴纳赫空间也具有重要的理论意义。"}}
{"id": "2506.21451", "title": "A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario", "authors": ["Cyrus Addy", "Ajay Kumar Gurumadaiah", "Yixiang Gao", "Kwame Awuah-Offei"], "summary": "Underground mining operations face significant safety challenges that make\nemergency response capabilities crucial. While robots have shown promise in\nassisting with search and rescue operations, their effectiveness depends on\nreliable miner detection capabilities. Deep learning algorithms offer potential\nsolutions for automated miner detection, but require comprehensive training\ndatasets, which are currently lacking for underground mining environments. This\npaper presents a novel thermal imaging dataset specifically designed to enable\nthe development and validation of miner detection systems for potential\nemergency applications. We systematically captured thermal imagery of various\nmining activities and scenarios to create a robust foundation for detection\nalgorithms. To establish baseline performance metrics, we evaluated several\nstate-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,\nand RT-DETR on our dataset. While not exhaustive of all possible emergency\nsituations, this dataset serves as a crucial first step toward developing\nreliable thermal-based miner detection systems that could eventually be\ndeployed in real emergency scenarios. This work demonstrates the feasibility of\nusing thermal imaging for miner detection and establishes a foundation for\nfuture research in this critical safety application.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21451v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21451v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "地下矿工多样化场景检测的综合数据集", "tldr": "本文提出了一个用于地下矿工检测的全面热成像数据集，并评估了最先进的对象检测算法，以解决地下矿山紧急响应中矿工检测数据不足的问题。", "motivation": "地下采矿作业面临严峻的安全挑战，需要强大的应急响应能力。机器人辅助搜救需要可靠的矿工检测能力。深度学习算法虽有潜力，但缺乏用于地下采矿环境的全面训练数据集。", "method": "本文提出了一个专门为开发和验证矿工检测系统而设计的新型热成像数据集。系统地捕获了各种采矿活动和场景的热图像。评估了包括YOLOv8、YOLOv10、YOLO11和RT-DETR在内的几种最先进的目标检测算法在该数据集上的基线性能。", "result": "创建了一个鲁棒的热成像数据集，为检测算法奠定了基础。评估了多种先进的目标检测算法在该数据集上的性能，建立了基线性能指标。", "conclusion": "该数据集是开发可靠的基于热成像的矿工检测系统的关键第一步。这项工作证明了使用热成像进行矿工检测的可行性，并为未来在该关键安全应用中的研究奠定了基础。", "translation": "地下采矿作业面临严峻的安全挑战，使得应急响应能力至关重要。虽然机器人在搜救行动中显示出前景，但其有效性取决于可靠的矿工检测能力。深度学习算法为自动化矿工检测提供了潜在解决方案，但需要全面的训练数据集，而地下采矿环境中目前缺乏此类数据集。本文提出了一个新颖的热成像数据集，专门设计用于开发和验证针对潜在紧急应用的矿工检测系统。我们系统地捕获了各种采矿活动和场景的热图像，为检测算法奠定了坚实的基础。为了建立基线性能指标，我们评估了我们数据集上几种最先进的目标检测算法，包括YOLOv8、YOLOv10、YOLO11和RT-DETR。虽然该数据集并未涵盖所有可能的紧急情况，但它为开发可靠的基于热成像的矿工检测系统迈出了关键的第一步，这些系统最终可能部署在真实的紧急场景中。这项工作证明了使用热成像进行矿工检测的可行性，并为未来在该关键安全应用中的研究奠定了基础。", "summary": "本文针对地下矿山应急响应中矿工检测数据缺乏的问题，构建了一个全面的热成像数据集。该数据集包含了多种采矿活动和场景的热图像，旨在支持基于深度学习的矿工检测系统开发。研究人员利用该数据集评估了YOLOv8、YOLOv10、YOLO11和RT-DETR等先进目标检测算法的基线性能，证明了热成像在矿工检测中的可行性，并为未来的相关研究奠定了基础。", "keywords": "地下矿工检测, 热成像, 数据集, 深度学习, 目标检测", "comments": "这项工作的创新之处在于构建了一个专门用于地下矿工检测的全面热成像数据集，填补了现有数据集的空白。其重要性在于为开发可靠的矿工检测系统提供了关键数据支持，对于提高地下矿山的安全应急响应能力具有重大意义。尽管数据集未涵盖所有紧急情况，但它为该领域的研究和实际应用迈出了坚实的第一步。"}}
{"id": "2506.21452", "title": "Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency", "authors": ["Kaiyu Song", "Hanjiang Lai"], "summary": "Classifier-free guidance (CFG) succeeds in condition diffusion models that\nuse a guidance scale to balance the influence of conditional and unconditional\nterms. A high guidance scale is used to enhance the performance of the\nconditional term. However, the high guidance scale often results in\noversaturation and unrealistic artifacts. In this paper, we introduce a new\nperspective based on low-frequency signals, identifying the accumulation of\nredundant information in these signals as the key factor behind oversaturation\nand unrealistic artifacts. Building on this insight, we propose low-frequency\nimproved classifier-free guidance (LF-CFG) to mitigate these issues.\nSpecifically, we introduce an adaptive threshold-based measurement to pinpoint\nthe locations of redundant information. We determine a reasonable threshold by\nanalyzing the change rate of low-frequency information between prior and\ncurrent steps. We then apply a down-weight strategy to reduce the impact of\nredundant information in the low-frequency signals. Experimental results\ndemonstrate that LF-CFG effectively alleviates oversaturation and unrealistic\nartifacts across various diffusion models, including Stable Diffusion-XL,\nStable Diffusion 2.1, 3.0, 3.5, and SiT-XL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21452v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21452v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过低频重新思考无分类器引导中的过饱和现象", "tldr": "本文提出了一种名为LF-CFG的新方法，通过处理低频信号中的冗余信息来解决无分类器引导（CFG）中高引导尺度导致的过饱和和不真实伪影问题，并在多种扩散模型上取得了良好效果。", "motivation": "无分类器引导（CFG）在高引导尺度下常导致过饱和和不真实伪影，影响了条件扩散模型的性能。", "method": "本文基于低频信号中冗余信息累积是导致过饱和的关键因素这一新视角，提出了低频改进无分类器引导（LF-CFG）。具体方法是：引入一种基于自适应阈值的测量方法来定位冗余信息，通过分析先前和当前步骤之间低频信息的变化率来确定合理阈值，然后应用降权策略来减少低频信号中冗余信息的影响。", "result": "实验结果表明，LF-CFG在多种扩散模型（包括Stable Diffusion-XL, Stable Diffusion 2.1, 3.0, 3.5, 和 SiT-XL）中有效缓解了过饱和和不真实伪影问题。", "conclusion": "通过处理低频信号中的冗余信息，LF-CFG能够有效改善无分类器引导在生成图像时出现的过饱和及不真实伪影问题，从而提升条件扩散模型的图像生成质量。", "translation": "无分类器引导（CFG）成功地指导了条件扩散模型，该模型使用引导尺度来平衡条件项和无条件项的影响。高引导尺度用于增强条件项的性能。然而，高引导尺度常常导致过饱和和不真实伪影。在本文中，我们引入了一种基于低频信号的新视角，将这些信号中冗余信息的积累识别为导致过饱和和不真实伪影的关键因素。基于这一见解，我们提出了低频改进无分类器引导（LF-CFG）来缓解这些问题。具体来说，我们引入了一种基于自适应阈值的测量方法来精确定位冗余信息的位置。我们通过分析先前和当前步骤之间低频信息的变化率来确定一个合理的阈值。然后，我们应用降权策略来减少低频信号中冗余信息的影响。实验结果表明，LF-CFG在各种扩散模型（包括Stable Diffusion-XL、Stable Diffusion 2.1、3.0、3.5和SiT-XL）中有效地缓解了过饱和和不真实伪影。", "summary": "本文针对无分类器引导（CFG）在高引导尺度下易产生过饱和和不真实伪影的问题，提出了一种名为低频改进无分类器引导（LF-CFG）的新方法。该方法基于低频信号中冗余信息累积导致过饱和的洞察，通过引入自适应阈值测量来定位冗余信息，并采用降权策略减少其影响。实验证明，LF-CFG在多种扩散模型上有效改善了图像生成质量，缓解了过饱和及伪影问题。", "keywords": "无分类器引导, 低频信号, 过饱和, 扩散模型, 图像生成", "comments": "该论文从低频信号的视角重新审视了无分类器引导中的过饱和问题，提出了一个新颖且有效的解决方案，对改善扩散模型的图像生成质量具有重要意义。"}}
{"id": "2506.20928", "title": "Active Learning for Manifold Gaussian Process Regression", "authors": ["Yuanxing Cheng", "Lulu Kang", "Yiwei Wang", "Chun Liu"], "summary": "This paper introduces an active learning framework for manifold Gaussian\nProcess (GP) regression, combining manifold learning with strategic data\nselection to improve accuracy in high-dimensional spaces. Our method jointly\noptimizes a neural network for dimensionality reduction and a Gaussian process\nregressor in the latent space, supervised by an active learning criterion that\nminimizes global prediction error. Experiments on synthetic data demonstrate\nsuperior performance over randomly sequential learning. The framework\nefficiently handles complex, discontinuous functions while preserving\ncomputational tractability, offering practical value for scientific and\nengineering applications. Future work will focus on scalability and\nuncertainty-aware manifold learning.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.20928v1", "categories": ["stat.ML", "cs.LG", "62", "G.3"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20928v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "流形高斯过程回归的主动学习", "tldr": "本文提出了一种流形高斯过程回归的主动学习框架，该框架结合了流形学习和战略性数据选择，旨在提高高维空间中的预测精度。", "motivation": "在流形高斯过程 (GP) 回归中，为了提高高维空间中的预测精度。", "method": "该方法联合优化了一个用于降维的神经网络和一个在潜在空间中的高斯过程回归器，并由最小化全局预测误差的主动学习准则进行监督。", "result": "在合成数据上的实验表明，该框架的性能优于随机序列学习。它能高效处理复杂、不连续的函数，同时保持计算的可处理性。", "conclusion": "该框架为科学和工程应用提供了实用价值，能够有效提高高维空间中复杂函数的预测精度和计算效率。", "translation": "本文介绍了一种用于流形高斯过程 (GP) 回归的主动学习框架，该框架将流形学习与战略性数据选择相结合，以提高高维空间中的精度。我们的方法联合优化了一个用于降维的神经网络和一个在潜在空间中的高斯过程回归器，由最小化全局预测误差的主动学习准则监督。合成数据上的实验表明，其性能优于随机序列学习。该框架高效处理复杂、不连续的函数，同时保持计算的可处理性，为科学和工程应用提供了实用价值。未来的工作将侧重于可扩展性和不确定性感知流形学习。", "summary": "本文提出了一种用于流形高斯过程回归的主动学习框架。该框架通过联合优化一个用于降维的神经网络和一个在高维潜在空间中的高斯过程回归器，并利用主动学习准则来最小化全局预测误差，从而提高了预测精度。实验证明，该方法在处理复杂、不连续函数方面优于随机序列学习，且计算高效，具有实际应用价值。", "keywords": "主动学习, 高斯过程, 流形学习, 降维, 回归", "comments": "本文的创新点在于将主动学习与流形高斯过程回归相结合，特别是通过联合优化神经网络进行降维和在高维潜在空间中进行高斯过程回归。其重要性在于能够有效提高高维复杂数据的预测精度和计算效率。未来工作将关注可扩展性和不确定性感知流形学习，这表明当前版本可能在这些方面存在局限性。"}}
{"id": "2506.21469", "title": "Evaluation of Traffic Signals for Daily Traffic Pattern", "authors": ["Mohammad Shokrolah Shirazi", "Hung-Fu Chang"], "summary": "The turning movement count data is crucial for traffic signal design,\nintersection geometry planning, traffic flow, and congestion analysis. This\nwork proposes three methods called dynamic, static, and hybrid configuration\nfor TMC-based traffic signals. A vision-based tracking system is developed to\nestimate the TMC of six intersections in Las Vegas using traffic cameras. The\nintersection design, route (e.g. vehicle movement directions), and signal\nconfiguration files with compatible formats are synthesized and imported into\nSimulation of Urban MObility for signal evaluation with realistic data. The\ninitial experimental results based on estimated waiting times indicate that the\ncycle time of 90 and 120 seconds works best for all intersections. In addition,\nfour intersections show better performance for dynamic signal timing\nconfiguration, and the other two with lower performance have a lower ratio of\ntotal vehicle count to total lanes of the intersection leg. Since daily traffic\nflow often exhibits a bimodal pattern, we propose a hybrid signal method that\nswitches between dynamic and static methods, adapting to peak and off-peak\ntraffic conditions for improved flow management. So, a built-in traffic\ngenerator module creates vehicle routes for 4 hours, including peak hours, and\na signal design module produces signal schedule cycles according to static,\ndynamic, and hybrid methods. Vehicle count distributions are weighted\ndifferently for each zone (i.e., West, North, East, South) to generate diverse\ntraffic patterns. The extended experimental results for 6 intersections with 4\nhours of simulation time imply that zone-based traffic pattern distributions\naffect signal design selection. Although the static method works great for\nevenly zone-based traffic distribution, the hybrid method works well for highly\nweighted traffic at intersection pairs of the West-East and North-South zones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21469v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21469v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "交通信号灯在日常交通模式下的评估", "tldr": "本文提出动态、静态和混合三种基于交通转向计数（TMC）的交通信号配置方法，并使用视觉跟踪系统估算拉斯维加斯六个交叉口的TMC数据。通过SUMO仿真评估，发现混合方法在处理日常双峰交通模式方面表现良好，尤其适用于交通流量高度集中的交叉口。", "motivation": "交通转向计数（TMC）数据对交通信号设计和交通分析至关重要。日常交通流常呈现双峰模式，需要更有效的交通信号管理策略来适应高峰和非高峰时段，以改善交通流管理。", "method": "1. 提出动态、静态和混合三种基于TMC的交通信号配置方法。2. 开发了一个基于视觉的跟踪系统，利用交通摄像头估算拉斯维加斯六个交叉口的TMC数据。3. 将交叉口设计、路线和信号配置文件导入城市交通仿真（SUMO）进行评估。4. 针对日常交通流的双峰模式，提出混合信号方法，在高峰和非高峰时段之间切换动态和静态方法。5. 构建内置交通生成模块创建车辆路线，并设计信号调度周期模块以生成信号周期。6. 对不同区域的车辆计数分布进行加权，以生成多样化的交通模式。7. 对六个交叉口进行扩展仿真实验。", "result": "1. 初步实验结果表明，90秒和120秒的周期时间对所有交叉口效果最佳。2. 四个交叉口在动态信号配时配置下表现更好，另外两个性能较低的交叉口其总车辆数与交叉口支路总车道数的比率较低。3. 区域性交通模式分布会影响信号设计选择。4. 静态方法对于均匀的区域性交通分布效果很好。5. 混合方法对于西-东和北-南交叉口对中高度加权的交通流量效果良好。", "conclusion": "区域性交通模式分布对交通信号设计选择有显著影响。针对日常交通流的双峰模式，混合信号方法通过在动态和静态方法之间切换，能够有效适应高峰和非高峰交通条件，从而改善交通流管理，尤其是在交通流量高度集中的交叉口表现突出。", "translation": "转向计数数据对于交通信号设计、交叉口几何规划、交通流和拥堵分析至关重要。这项工作提出了三种基于TMC的交通信号配置方法，分别称为动态、静态和混合配置。开发了一个基于视觉的跟踪系统，利用交通摄像头估算拉斯维加斯六个交叉口的TMC。将交叉口设计、路线（例如车辆移动方向）和兼容格式的信号配置文件合成并导入城市交通仿真（Simulation of Urban MObility，SUMO）中，以使用真实数据进行信号评估。基于估算等待时间的初步实验结果表明，90秒和120秒的周期时间对所有交叉口效果最佳。此外，四个交叉口在动态信号配时配置下表现更好，而另外两个性能较低的交叉口其总车辆数与交叉口支路总车道数的比率较低。由于日常交通流通常呈现双峰模式，我们提出了一种混合信号方法，该方法在动态和静态方法之间切换，以适应高峰和非高峰交通条件，从而改善流量管理。因此，一个内置的交通生成器模块创建了4小时（包括高峰时段）的车辆路线，一个信号设计模块根据静态、动态和混合方法生成信号调度周期。每个区域（即西、北、东、南）的车辆计数分布被赋予不同的权重，以生成多样化的交通模式。对六个交叉口进行4小时仿真时间的扩展实验结果表明，区域性交通模式分布会影响信号设计选择。尽管静态方法对于均匀的区域性交通分布效果很好，但混合方法对于西-东和北-南区域交叉口对中高度加权的交通流量效果良好。", "summary": "本文针对交通信号设计中关键的转向计数数据，提出了动态、静态和混合三种交通信号配置方法。研究开发了一个基于视觉的系统来估算拉斯维加斯六个交叉口的转向计数，并通过SUMO仿真平台进行评估。鉴于日常交通流的双峰模式，特别提出了一种混合信号方法，该方法能根据高峰和非高峰时段在动态和静态策略间智能切换。实验结果表明，该混合方法在处理不均匀的区域交通分布，尤其是在交通流量高度集中的特定交叉口对中，能够显著优化交通流管理。", "keywords": "交通信号, 转向计数, 交通模式, 交通仿真, 混合方法", "comments": "本文的创新点在于提出了结合动态和静态方法的混合交通信号配置策略，以适应日常交通流的双峰模式。通过将视觉跟踪系统与交通仿真平台相结合，实现了基于真实数据的信号评估，增强了研究的实际应用价值。该研究强调了区域性交通模式分布对信号设计选择的重要性，并为城市交通管理提供了新的思路和工具，特别是在处理复杂和多变的交通状况方面具有潜在的应用前景。"}}
{"id": "2506.20930", "title": "Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "Ya-Chuan Chen"], "summary": "We propose a hybrid quantum-classical reinforcement learning framework for\nsector rotation in the Taiwan stock market. Our system employs Proximal Policy\nOptimization (PPO) as the backbone algorithm and integrates both classical\narchitectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,\nQASA) as policy and value networks. An automated feature engineering pipeline\nextracts financial indicators from capital share data to ensure consistent\nmodel input across all configurations. Empirical backtesting reveals a key\nfinding: although quantum-enhanced models consistently achieve higher training\nrewards, they underperform classical models in real-world investment metrics\nsuch as cumulative return and Sharpe ratio. This discrepancy highlights a core\nchallenge in applying reinforcement learning to financial domains -- namely,\nthe mismatch between proxy reward signals and true investment objectives. Our\nanalysis suggests that current reward designs may incentivize overfitting to\nshort-term volatility rather than optimizing risk-adjusted returns. This issue\nis compounded by the inherent expressiveness and optimization instability of\nquantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We\ndiscuss the implications of this reward-performance gap and propose directions\nfor future improvement, including reward shaping, model regularization, and\nvalidation-based early stopping. Our work offers a reproducible benchmark and\ncritical insights into the practical challenges of deploying quantum\nreinforcement learning in real-world finance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20930v1", "categories": ["quant-ph", "cs.LG", "q-fin.CP"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.20930v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "用于台湾股市板块轮动的量子强化学习交易代理", "tldr": "本研究提出了一种混合量子-经典强化学习框架用于台湾股市的板块轮动。量子增强模型在训练奖励上表现更好，但在实际投资指标（如累积收益和夏普比率）上却不如经典模型，这突出了奖励信号与真实投资目标不匹配的核心挑战。", "motivation": "研究旨在探索混合量子-经典强化学习框架在台湾股市板块轮动中的应用，并评估量子增强模型在实际金融应用中的表现，特别是解决代理奖励信号与真实投资目标之间的不匹配问题。", "method": "本研究提出了一种混合量子-经典强化学习框架，以PPO为核心算法，并集成了经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）作为策略和价值网络。通过自动化特征工程管道从股本数据中提取金融指标，确保模型输入的一致性。采用回溯测试进行实证验证。", "result": "实证回溯测试发现，尽管量子增强模型持续获得更高的训练奖励，但在累积收益和夏普比率等实际投资指标上，它们表现不如经典模型。这种差异突出了强化学习应用于金融领域的核心挑战——即代理奖励信号与真实投资目标之间的不匹配。分析表明，当前的奖励设计可能激励模型过度拟合短期波动，而不是优化风险调整后的收益。", "conclusion": "研究得出结论，当前强化学习在金融领域的奖励设计存在问题，可能导致过度拟合短期波动而非优化风险调整收益。此外，NISQ约束下量子电路固有的表现力和优化不稳定性也加剧了这一问题。本研究为量子强化学习在实际金融部署中的挑战提供了可复现的基准和关键见解，并提出了未来改进方向，包括奖励塑造、模型正则化和基于验证的早期停止。", "translation": "我们提出了一种用于台湾股票市场板块轮动的混合量子-经典强化学习框架。我们的系统采用近端策略优化（PPO）作为核心算法，并集成了经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）作为策略和价值网络。一个自动化特征工程管道从资本股份数据中提取金融指标，以确保所有配置中的模型输入一致。实证回溯测试揭示了一个关键发现：尽管量子增强模型持续获得更高的训练奖励，但在累积收益和夏普比率等实际投资指标上，它们表现不如经典模型。这种差异突出了将强化学习应用于金融领域的核心挑战——即代理奖励信号与真实投资目标之间的不匹配。我们的分析表明，当前的奖励设计可能激励模型过度拟合短期波动，而不是优化风险调整后的收益。这个问题因噪声中等规模量子（NISQ）约束下量子电路固有的表现力和优化不稳定性而加剧。我们讨论了这种奖励-性能差距的影响，并提出了未来改进方向，包括奖励塑造、模型正则化和基于验证的早期停止。我们的工作提供了一个可复现的基准和关于在实际金融中部署量子强化学习的实践挑战的关键见解。", "summary": "本研究提出了一种用于台湾股市板块轮动的混合量子-经典强化学习框架。该框架以PPO为基础，结合了经典（LSTM, Transformer）和量子增强（QNN, QRWKV, QASA）模型。通过自动化特征工程提取金融指标。回溯测试显示，尽管量子模型训练奖励高，但在实际投资指标上逊于经典模型。这种差异源于代理奖励与真实投资目标的不匹配，可能导致对短期波动的过度拟合。研究强调了NISQ约束下量子电路的优化不稳定性，并提出了奖励塑造、模型正则化和早期停止等改进方向，为量子强化学习在金融领域的实际应用提供了基准和见解。", "keywords": "量子强化学习, 板块轮动, 台湾股市, 奖励-性能差距, NISQ", "comments": "本研究的创新之处在于首次将混合量子-经典强化学习应用于台湾股市的板块轮动，并提供了一个可复现的基准。其重要性在于揭示了量子增强模型在金融领域实际应用中面临的关键挑战，即训练奖励与真实投资目标之间的“奖励-性能差距”。这对于未来量子强化学习在金融应用中的奖励设计、模型优化以及克服NISQ限制具有重要的指导意义。"}}
{"id": "2506.20933", "title": "Lower Bounds on the Size of Markov Equivalence Classes", "authors": ["Erik Jahn", "Frederick Eberhardt", "Leonard J. Schulman"], "summary": "Causal discovery algorithms typically recover causal graphs only up to their\nMarkov equivalence classes unless additional parametric assumptions are made.\nThe sizes of these equivalence classes reflect the limits of what can be\nlearned about the underlying causal graph from purely observational data. Under\nthe assumptions of acyclicity, causal sufficiency, and a uniform model prior,\nMarkov equivalence classes are known to be small on average. In this paper, we\nshow that this is no longer the case when any of these assumptions is relaxed.\nSpecifically, we prove exponentially large lower bounds for the expected size\nof Markov equivalence classes in three settings: sparse random directed acyclic\ngraphs, uniformly random acyclic directed mixed graphs, and uniformly random\ndirected cyclic graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20933v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20933v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "马尔可夫等价类大小的下界", "tldr": "当因果发现中的标准假设（如无环性、因果充分性）被放宽时，马尔可夫等价类的大小不再平均较小，反而可以呈指数级增长，这意味着从观测数据中学习因果图的不确定性大大增加。", "motivation": "因果发现算法通常只能恢复到马尔可夫等价类，除非做出额外的参数假设。这些等价类的大小反映了纯粹从观测数据中学习底层因果图的局限性。在无环性、因果充分性和统一模型先验的假设下，马尔可夫等价类平均而言是小的，但本文旨在探讨放宽这些假设后的情况。", "method": "通过在三种特定设置下（稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图）证明马尔可夫等价类预期大小的指数级大下界来展开研究。", "result": "研究表明，当无环性、因果充分性或统一模型先验的任何一个假设被放宽时，马尔可夫等价类的大小不再平均较小。具体来说，本文证明了在上述三种图设置中，马尔可夫等价类预期大小的指数级大下界。", "conclusion": "放宽因果发现中的标准假设会导致马尔可夫等价类呈指数级增大，这表明在这些情况下，从纯观测数据中准确识别底层因果图的能力受到严重限制和不确定性增加。", "translation": "因果发现算法通常只能恢复到马尔可夫等价类，除非做出额外的参数假设。这些等价类的大小反映了纯粹从观测数据中学习底层因果图的局限性。在无环性、因果充分性和统一模型先验的假设下，马尔可夫等价类平均而言是小的。在本文中，我们表明当这些假设中的任何一个被放宽时，情况就不再如此。具体来说，我们证明了在三种设置下：稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图，马尔可夫等价类预期大小的指数级大下界。", "summary": "本文研究了在放宽标准假设（如无环性、因果充分性）后，因果发现中马尔可夫等价类大小的变化。与以往认为其平均较小的观点相反，研究证明在稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图这三种情况下，马尔可夫等价类的预期大小可以呈指数级增长，揭示了在这些条件下从观测数据中进行因果推断的更深层次的局限性。", "keywords": "马尔可夫等价类, 因果发现, 下界, 有向图", "comments": "这项研究挑战了因果发现领域中关于马尔可夫等价类平均大小的普遍认知，揭示了在放松关键假设时，因果推断面临的巨大不确定性。证明指数级下界对于理解因果学习的理论和实践限制具有重要意义。"}}
{"id": "2506.21476", "title": "Global and Local Entailment Learning for Natural World Imagery", "authors": ["Srikumar Sastry", "Aayush Dhakal", "Eric Xing", "Subash Khanal", "Nathan Jacobs"], "summary": "Learning the hierarchical structure of data in vision-language models is a\nsignificant challenge. Previous works have attempted to address this challenge\nby employing entailment learning. However, these approaches fail to model the\ntransitive nature of entailment explicitly, which establishes the relationship\nbetween order and semantics within a representation space. In this work, we\nintroduce Radial Cross-Modal Embeddings (RCME), a framework that enables the\nexplicit modeling of transitivity-enforced entailment. Our proposed framework\noptimizes for the partial order of concepts within vision-language models. By\nleveraging our framework, we develop a hierarchical vision-language foundation\nmodel capable of representing the hierarchy in the Tree of Life. Our\nexperiments on hierarchical species classification and hierarchical retrieval\ntasks demonstrate the enhanced performance of our models compared to the\nexisting state-of-the-art models. Our code and models are open-sourced at\nhttps://vishu26.github.io/RCME/index.html.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21476v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21476v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "自然世界图像的全局和局部蕴涵学习", "tldr": "本文提出了径向跨模态嵌入（RCME）框架，明确建模蕴涵的传递性，以学习视觉-语言模型中的数据层次结构，并在分层物种分类和分层检索任务中取得了优于现有技术的性能。", "motivation": "在视觉-语言模型中学习数据的层次结构是一个重大挑战。现有的蕴涵学习方法未能明确建模蕴涵的传递性，而传递性建立了表示空间中顺序和语义之间的关系。", "method": "本文引入了径向跨模态嵌入（RCME）框架，该框架能够明确建模强制传递性的蕴涵。RCME框架优化了视觉-语言模型中概念的偏序。通过该框架，开发了一个能够表示生命之树中层次结构的分层视觉-语言基础模型。", "result": "在分层物种分类和分层检索任务上的实验表明，与现有最先进的模型相比，本文模型的性能有所增强。", "conclusion": "RCME框架通过明确建模蕴涵的传递性，有效解决了视觉-语言模型中层次结构学习的挑战，并在相关任务中取得了卓越表现。", "translation": "在视觉-语言模型中学习数据的层次结构是一个重大挑战。以前的工作试图通过采用蕴涵学习来解决这个挑战。然而，这些方法未能明确建模蕴涵的传递性，而蕴涵的传递性在表示空间中建立了顺序和语义之间的关系。在这项工作中，我们引入了径向跨模态嵌入（RCME），这是一个能够明确建模强制传递性的蕴涵的框架。我们提出的框架优化了视觉-语言模型中概念的偏序。通过利用我们的框架，我们开发了一个能够表示生命之树中层次结构的分层视觉-语言基础模型。我们在分层物种分类和分层检索任务上的实验证明了我们的模型比现有最先进模型的性能有所提升。我们的代码和模型已在 https://vishu26.github.io/RCME/index.html 开源。", "summary": "本文提出了一种名为径向跨模态嵌入（RCME）的新框架，用于解决视觉-语言模型中数据层次结构学习的挑战。RCME通过明确建模蕴涵的传递性来优化概念的偏序关系，从而克服了现有方法无法有效处理传递性的问题。该框架被应用于开发一个能够表示生命之树层次结构的分层视觉-语言基础模型。实验结果表明，RCME在分层物种分类和分层检索任务上的表现优于现有最先进的模型。", "keywords": "视觉-语言模型, 层次结构学习, 蕴涵学习, 传递性, 径向跨模态嵌入", "comments": "本文的创新点在于提出了RCME框架，明确地将蕴涵的传递性引入到视觉-语言模型的层次结构学习中，这对于理解和表示复杂数据关系至关重要。通过解决现有方法在传递性建模上的不足，RCME提供了一个更鲁棒和准确的层次结构学习方法，并在实际任务中取得了显著的性能提升，具有重要的应用前景。"}}
{"id": "2506.20935", "title": "Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics", "authors": ["Hsin-Hsiung Huang", "Hayden Hampton"], "summary": "Forecasting geopolitical conflict from data sources like the Global Database\nof Events, Language, and Tone (GDELT) is a critical challenge for national\nsecurity. The inherent sparsity, burstiness, and overdispersion of such data\ncause standard deep learning models, including the Temporal Fusion Transformer\n(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,\na hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)\ncompetition by overcoming these limitations. Designed to bridge this gap, our\nmodel employs a two-stage process: first, a TFT captures complex temporal\ndynamics to generate multi-quantile forecasts. These quantiles then serve as\ninformed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),\nwhich performs principled spatiotemporal smoothing and uncertainty\nquantification. In a case study forecasting conflict dynamics in the Middle\nEast and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,\nshowing a superior ability to predict the timing and magnitude of bursty event\nperiods, particularly at long-range horizons. This work offers a robust\nframework for generating more reliable and actionable intelligence from\nchallenging event data, with all code and workflows made publicly available to\nensure reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20935v1", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO", "37M10, 62M10, 62P25, 65Y20"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.20935v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "地缘政治事件预测：基于稀疏时间融合Transformer和高斯过程混合模型——以中东和美国冲突动态为例", "tldr": "本文提出STFT-VNNGP混合模型，结合TFT和高斯过程，有效解决了地缘政治事件数据稀疏性、爆发性导致传统模型预测不可靠的问题，并在中东和美国冲突预测中表现出优越的长期预测能力。", "motivation": "预测地缘政治冲突对国家安全至关重要，但来自GDELT等数据源的固有稀疏性、爆发性和过度离散性使得包括时间融合Transformer (TFT) 在内的标准深度学习模型在长期预测中产生不可靠的结果。", "method": "本文引入了STFT-VNNGP混合架构，该模型采用两阶段过程：首先，一个TFT模型捕获复杂的时间动态以生成多分位数预测；然后，这些分位数作为输入，送入变分最近邻高斯过程 (VNNGP) 进行原则性的时空平滑和不确定性量化。", "result": "在预测中东和美国冲突动态的案例研究中，STFT-VNNGP持续优于单独的TFT，在预测爆发性事件的时间和强度方面表现出卓越的能力，尤其是在长期预测范围。", "conclusion": "STFT-VNNGP为从具有挑战性的事件数据中生成更可靠和可操作的情报提供了一个强大的框架，并且所有代码和工作流程都已公开以确保可复现性。", "translation": "从全球事件、语言和语调数据库（GDELT）等数据源预测地缘政治冲突是国家安全面临的关键挑战。此类数据固有的稀疏性、爆发性和过度离散性导致包括时间融合Transformer（TFT）在内的标准深度学习模型产生不可靠的长期预测。我们引入了STFT-VNNGP，这是一种混合架构，通过克服这些限制赢得了2023年威胁检测算法（ATD）竞赛。我们的模型旨在弥补这一差距，采用两阶段过程：首先，一个TFT捕获复杂的时间动态以生成多分位数预测。然后，这些分位数作为变分最近邻高斯过程（VNNGP）的知情输入，该过程执行原则性的时空平滑和不确定性量化。在中东和美国冲突动态预测的案例研究中，STFT-VNNGP持续优于单独的TFT，显示出在预测爆发性事件的时间和强度方面的卓越能力，尤其是在长期预测范围。这项工作为从具有挑战性的事件数据中生成更可靠和可操作的情报提供了一个强大的框架，所有代码和工作流程均已公开，以确保可复现性。", "summary": "本文针对地缘政治冲突数据（如GDELT）固有的稀疏性、爆发性和过度离散性导致传统深度学习模型（如TFT）在长期预测中表现不佳的问题，提出了一种名为STFT-VNNGP的混合架构。该模型结合了TFT捕获复杂时间动态生成多分位数预测的能力，以及变分最近邻高斯过程（VNNGP）进行时空平滑和不确定性量化的优势。在预测中东和美国冲突动态的案例研究中，STFT-VNNGP在长期预测中持续优于单独的TFT，尤其擅长预测爆发性事件的发生时间和强度，为从复杂事件数据中生成可靠情报提供了新的解决方案。", "keywords": "地缘政治事件预测, 时间融合Transformer, 高斯过程, 冲突动态, STFT-VNNGP", "comments": "该论文通过结合深度学习模型（TFT）和统计模型（高斯过程）的优势，有效解决了地缘政治事件数据固有的挑战，特别是数据的稀疏性、爆发性和长期预测的可靠性问题。其混合架构的创新性在于利用TFT捕捉复杂时间动态，并利用高斯过程进行不确定性量化和空间平滑，从而提高了预测的鲁棒性和可解释性。赢得2023年威胁检测算法（ATD）竞赛也证明了其有效性。代码和工作流程的公开提升了其科学价值和可复现性。"}}
{"id": "2506.21484", "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning", "authors": ["Tajamul Ashraf", "Janibul Bashir"], "summary": "We focus on the source-free domain adaptive object detection (SF-DAOD)\nproblem when source data is unavailable during adaptation and the model must\nadapt to an unlabeled target domain. The majority of approaches for the problem\nemploy a self-supervised approach using a student-teacher (ST) framework where\npseudo-labels are generated via a source-pretrained model for further\nfine-tuning. We observe that the performance of a student model often degrades\ndrastically, due to the collapse of the teacher model, primarily caused by high\nnoise in pseudo-labels, resulting from domain bias, discrepancies, and a\nsignificant domain shift across domains. To obtain reliable pseudo-labels, we\npropose a Target-based Iterative Query-Token Adversarial Network (TITAN), which\nseparates the target images into two subsets: those similar to the source\n(easy) and those dissimilar (hard). We propose a strategy to estimate variance\nto partition the target domain. This approach leverages the insight that higher\ndetection variances correspond to higher recall and greater similarity to the\nsource domain. Also, we incorporate query-token-based adversarial modules into\na student-teacher baseline framework to reduce the domain gaps between two\nfeature representations. Experiments conducted on four natural imaging datasets\nand two challenging medical datasets have substantiated the superior\nperformance of TITAN compared to existing state-of-the-art (SOTA)\nmethodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7\npercent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,\nrespectively.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21484v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21484v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "TITAN: 基于查询令牌的领域自适应对抗学习", "tldr": "TITAN提出了一种新的源数据不可用时的目标检测领域自适应方法，通过将目标图像分为易/难子集并使用查询令牌对抗模块，解决了伪标签噪声问题，显著提升了性能。", "motivation": "在源数据不可用的领域自适应目标检测（SF-DAOD）问题中，现有方法因伪标签中的高噪声（源于领域偏差、差异和显著领域漂移）导致教师模型崩溃，进而使学生模型性能急剧下降。本文旨在解决这一问题，以获得更可靠的伪标签。", "method": "本文提出了Target-based Iterative Query-Token Adversarial Network (TITAN)。该方法首先将目标图像分为与源域相似的“容易”子集和不相似的“困难”子集，并提出一种基于估计方差的策略进行划分，利用高检测方差对应高召回率和与源域的更高相似性的洞察。其次，将基于查询令牌的对抗模块整合到学生-教师基线框架中，以减少两种特征表示之间的领域差距。", "result": "实验结果表明，TITAN在四个自然图像数据集和两个具有挑战性的医学数据集上的性能均优于现有最先进（SOTA）方法。具体地，在C2F、C2B、S2C和K2C基准测试中，mAP分别比当前SOTA提升了+22.7%、+22.2%、+21.1%和+3.7%。", "conclusion": "TITAN通过创新的目标域图像划分策略和引入查询令牌对抗模块，有效解决了源数据不可用的领域自适应目标检测中伪标签噪声和领域差距问题，取得了超越现有最先进方法的卓越性能。", "translation": "我们专注于源数据在适应过程中不可用且模型必须适应未标记目标域的源无关领域自适应目标检测（SF-DAOD）问题。大多数解决此问题的方法采用自监督方法，使用学生-教师（ST）框架，通过源预训练模型生成伪标签以进行进一步微调。我们观察到，由于教师模型的崩溃，学生模型的性能通常会急剧下降，这主要是由伪标签中的高噪声引起的，而高噪声又源于领域偏差、差异以及跨领域显著的领域漂移。为了获得可靠的伪标签，我们提出了一个基于目标的迭代查询令牌对抗网络（TITAN），它将目标图像分为两个子集：与源域相似的（容易）和不相似的（困难）。我们提出了一种估计方差的策略来划分目标域。这种方法利用了这样的洞察：更高的检测方差对应更高的召回率和与源域的更大相似性。此外，我们将基于查询令牌的对抗模块整合到学生-教师基线框架中，以减少两种特征表示之间的领域差距。在四个自然图像数据集和两个具有挑战性的医学数据集上进行的实验证实了TITAN优于现有最先进（SOTA）方法。我们报告在C2F、C2B、S2C和K2C基准测试中，mAP分别比当前SOTA提升了+22.7、+22.2、+21.1和+3.7个百分点。", "summary": "本文针对源数据不可用的领域自适应目标检测（SF-DAOD）中的伪标签噪声问题，提出了TITAN方法。该方法通过将目标图像划分为易/难子集并引入基于查询令牌的对抗模块，有效生成可靠伪标签并缩小领域差距。实验证明，TITAN在多个数据集上显著超越现有SOTA方法，表现出优越性能。", "keywords": "领域自适应, 目标检测, 无源学习, 对抗学习, 伪标签", "comments": "TITAN的创新点在于其提出的目标域图像划分策略，通过估计方差来区分“容易”和“困难”样本，以及将查询令牌对抗模块引入学生-教师框架，有效解决了SF-DAOD中伪标签噪声和领域漂移的核心挑战。其在多个数据集上取得的显著性能提升，证明了该方法的有效性和重要性。"}}
{"id": "2506.21486", "title": "Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection", "authors": ["Tobias J. Riedlinger", "Kira Maag", "Hanno Gottschalk"], "summary": "Deep neural networks have set the state-of-the-art in computer vision tasks\nsuch as bounding box detection and semantic segmentation. Object detectors and\nsegmentation models assign confidence scores to predictions, reflecting the\nmodel's uncertainty in object detection or pixel-wise classification. However,\nthese confidence estimates are often miscalibrated, as their architectures and\nloss functions are tailored to task performance rather than probabilistic\nfoundation. Even with well calibrated predictions, object detectors fail to\nquantify uncertainty outside detected bounding boxes, i.e., the model does not\nmake a probability assessment of whether an area without detected objects is\ntruly free of obstacles. This poses a safety risk in applications such as\nautomated driving, where uncertainty in empty areas remains unexplored. In this\nwork, we propose an object detection model grounded in spatial statistics.\nBounding box data matches realizations of a marked point process, commonly used\nto describe the probabilistic occurrence of spatial point events identified as\nbounding box centers, where marks are used to describe the spatial extension of\nbounding boxes and classes. Our statistical framework enables a\nlikelihood-based training and provides well-defined confidence estimates for\nwhether a region is drivable, i.e., free of objects. We demonstrate the\neffectiveness of our method through calibration assessments and evaluation of\nperformance.", "comment": "15 pages, 4 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.21486v1", "categories": ["cs.CV", "cs.LG", "math.PR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21486v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "迈向可靠的空旷空间检测：用于目标检测的条件标记点过程", "tldr": "本文提出了一种基于空间统计学的目标检测模型，利用条件标记点过程来量化未检测到物体区域的不确定性，从而提供对空旷区域的明确置信度估计，以解决现有目标检测器无法评估空旷空间安全性的问题。", "motivation": "当前深度学习目标检测器在检测性能上表现出色，但其置信度估计常常不准确，且无法量化未检测到物体区域（即空旷空间）的不确定性。这在自动驾驶等应用中构成安全风险，因为模型无法评估一个区域是否真正没有障碍物。", "method": "本文提出了一种基于空间统计学的方法，将边界框数据视为标记点过程的实现。其中，边界框中心是空间点事件，而标记则描述了边界框的空间扩展和类别。该统计框架支持基于似然的训练，并为区域是否可驾驶（即无物体）提供了明确的置信度估计。", "result": "通过校准评估和性能评估，证明了所提出方法的有效性。", "conclusion": "所提出的基于空间统计学的目标检测模型能够为未检测到物体的空旷区域提供明确的置信度估计，从而解决了现有模型在量化空旷空间不确定性方面的不足，提高了自动驾驶等应用中的安全性。", "translation": "深度神经网络在边界框检测和语义分割等计算机视觉任务中取得了最先进的成果。目标检测器和分割模型为预测分配置信度分数，反映了模型在目标检测或像素级分类中的不确定性。然而，这些置信度估计常常不准确，因为它们的架构和损失函数是为了任务性能而非概率基础而设计的。即使预测经过良好校准，目标检测器也无法量化检测到的边界框之外的不确定性，即模型无法评估一个没有检测到物体的区域是否真正没有障碍物。这在自动驾驶等应用中构成安全风险，因为空旷区域的不确定性尚未得到探索。在这项工作中，我们提出了一种基于空间统计学原理的目标检测模型。边界框数据与标记点过程的实现相匹配，标记点过程常用于描述识别为边界框中心的空间点事件的概率发生，其中标记用于描述边界框和类别的空间扩展。我们的统计框架支持基于似然的训练，并为区域是否可驾驶（即无物体）提供了明确的置信度估计。我们通过校准评估和性能评估证明了我们方法的有效性。", "summary": "当前的目标检测模型在量化空旷区域的不确定性方面存在不足，这在自动驾驶等安全关键应用中构成了风险。本文提出了一种基于空间统计学的目标检测模型，该模型利用条件标记点过程来建模边界框数据。通过将边界框视为带有空间和类别标记的点事件，该方法能够进行基于似然的训练，并为未检测到物体的区域（即空旷空间）提供明确且经过良好校准的置信度估计，从而有效评估区域的安全性。", "keywords": "目标检测, 不确定性量化, 标记点过程, 空旷空间检测, 空间统计学", "comments": "该论文的创新点在于将空间统计学中的条件标记点过程引入到目标检测领域，解决了现有深度学习模型无法有效量化空旷空间不确定性的关键问题。这对于自动驾驶等需要高安全性保障的应用具有重要意义，因为它提供了对“无障碍”区域的概率性评估，而不仅仅是关注已检测到的物体。这种方法提升了模型的可靠性和决策透明度。"}}
{"id": "2506.21509", "title": "Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration", "authors": ["Jiahe Chen", "Jiaying He", "Qian Shao", "Qiyuan Chen", "Jiahe Ying", "Hongxia Xu", "Jintai Chen", "Jianwei Zheng", "Jian Wu"], "summary": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21509v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21509v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "通过动态Logits校准减轻大型视觉语言模型的幻觉", "tldr": "本文提出了一种名为动态Logits校准（DLC）的新型免训练解码框架，旨在在推理时动态地将文本生成与视觉证据对齐，有效减少大型视觉语言模型（LVLMs）的幻觉，同时保持高推理效率。", "motivation": "大型视觉语言模型（LVLMs）在多模态理解方面取得了显著进展，但经常受到幻觉的困扰，即生成与视觉输入相矛盾的文本。现有的免训练解码策略存在局限性，包括使用静态约束无法适应生成过程中的语义漂移、需要多次前向传播导致的低效率以及过于僵硬的干预规则导致的细节退化。", "method": "本文引入了动态Logits校准（DLC），这是一种新型的免训练解码框架。在解码阶段，DLC逐步利用CLIP评估输入图像与生成文本序列之间的语义对齐。然后，根据动态更新的上下文基线评估候选tokens的相对视觉优势（RVA），自适应地调整输出logits以偏向视觉上接地气的tokens。此外，一个由实时上下文对齐分数提供信息的自适应加权机制，在平衡视觉指导的同时，确保文本输出的整体质量。", "result": "在各种基准和不同的LVLM架构（如LLaVA、InstructBLIP和MiniGPT-4）上进行的大量实验表明，DLC显著减少了幻觉，优于现有方法，并通过避免多次前向传播保持了高推理效率。", "conclusion": "本文提出了一种有效且高效的解码时解决方案来减轻大型视觉语言模型（LVLMs）的幻觉，从而增强LVLMs在更多实践中的可靠性。", "translation": "大型视觉语言模型（LVLMs）在多模态理解方面取得了显著进展，但它们经常受到幻觉的困扰——生成与视觉输入相矛盾的文本。现有的免训练解码策略存在关键局限性，包括使用静态约束无法适应生成过程中的语义漂移，需要多次前向传播导致的低效率，以及由于过于僵硬的干预规则导致的细节退化。为了克服这些挑战，本文引入了动态Logits校准（DLC），这是一种新型的免训练解码框架，旨在在推理时动态地将文本生成与视觉证据对齐。在解码阶段，DLC逐步利用CLIP来评估输入图像和生成文本序列之间的语义对齐。然后，根据动态更新的上下文基线评估候选tokens的相对视觉优势（RVA），自适应地调整输出logits以偏向视觉上接地气的tokens。此外，一个由实时上下文对齐分数提供信息的自适应加权机制，在平衡视觉指导的同时，确保文本输出的整体质量。在各种基准和不同的LVLM架构（如LLaVA、InstructBLIP和MiniGPT-4）上进行的大量实验表明，DLC显著减少了幻觉，优于现有方法，并通过避免多次前向传播保持了高推理效率。总的来说，我们提出了一种有效且高效的解码时解决方案来减轻幻觉，从而增强LVLMs在更多实践中的可靠性。代码将在Github上发布。", "summary": "本文提出了一种名为动态Logits校准（DLC）的新型免训练解码框架，旨在解决大型视觉语言模型（LVLMs）在文本生成中出现的幻觉问题。DLC在推理时通过逐步使用CLIP评估视觉与文本的语义对齐，并基于动态更新的上下文基线调整输出logits，以偏向视觉上更准确的tokens。它还采用自适应加权机制平衡视觉指导和文本质量。实验证明，DLC在多种LVLM架构上显著减少了幻觉，且保持了高推理效率，优于现有方法。", "keywords": "大型视觉语言模型, 幻觉, 动态Logits校准, 免训练, 视觉语言对齐", "comments": "DLC的创新之处在于其动态调整Logits的机制，通过实时评估视觉与文本的对齐程度，避免了传统静态约束的局限性，并且实现了免训练和高效率。这对于提升大型视觉语言模型的可靠性和实际应用价值具有重要意义。"}}
{"id": "2506.21513", "title": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation", "authors": ["Wentao Hu", "Shunkai Li", "Ziqiao Peng", "Haoxian Zhang", "Fan Shi", "Xiaoqiang Liu", "Pengfei Wan", "Di Zhang", "Hui Tian"], "summary": "Creating high-quality, generalizable speech-driven 3D talking heads remains a\npersistent challenge. Previous methods achieve satisfactory results for fixed\nviewpoints and small-scale audio variations, but they struggle with large head\nrotations and out-of-distribution (OOD) audio. Moreover, they are constrained\nby the need for time-consuming, identity-specific training. We believe the core\nissue lies in the lack of sufficient 3D priors, which limits the extrapolation\ncapabilities of synthesized talking heads. To address this, we propose\nGGTalker, which synthesizes talking heads through a combination of\ngeneralizable priors and identity-specific adaptation. We introduce a two-stage\nPrior-Adaptation training strategy to learn Gaussian head priors and adapt to\nindividual characteristics. We train Audio-Expression and Expression-Visual\npriors to capture the universal patterns of lip movements and the general\ndistribution of head textures. During the Customized Adaptation, individual\nspeaking styles and texture details are precisely modeled. Additionally, we\nintroduce a color MLP to generate fine-grained, motion-aligned textures and a\nBody Inpainter to blend rendered results with the background, producing\nindistinguishable, photorealistic video frames. Comprehensive experiments show\nthat GGTalker achieves state-of-the-art performance in rendering quality, 3D\nconsistency, lip-sync accuracy, and training efficiency.", "comment": "ICCV 2025, Project page: https://vincenthu19.github.io/GGTalker/", "pdf_url": "http://arxiv.org/pdf/2506.21513v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21513v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "GGTalker：基于可泛化高斯先验和身份特异性适应的说话人头部合成", "tldr": "GGTalker通过结合可泛化高斯先验和身份特异性适应，解决了高质量、可泛化语音驱动3D说话人头部合成中存在的头部大旋转、OOD音频和耗时训练问题。", "motivation": "现有方法在固定视角和小规模音频变化下表现良好，但在大头部旋转和域外(OOD)音频方面表现不佳，且受限于耗时的身份特异性训练。核心问题在于缺乏足够的3D先验，这限制了合成说话人头部的外推能力。", "method": "提出GGTalker，通过结合可泛化先验和身份特异性适应来合成说话人头部。引入两阶段的“先验-适应”训练策略，学习高斯头部先验并适应个体特征。训练“音频-表情”和“表情-视觉”先验以捕捉唇部运动的通用模式和头部纹理的通用分布。在“定制适应”阶段，精确建模个体说话风格和纹理细节。此外，引入颜色MLP生成精细、运动对齐的纹理，以及Body Inpainter将渲染结果与背景融合，生成逼真的视频帧。", "result": "综合实验表明，GGTalker在渲染质量、3D一致性、唇部同步精度和训练效率方面达到了最先进的性能。", "conclusion": "GGTalker通过引入可泛化高斯先验和身份特异性适应，有效解决了语音驱动3D说话人头部合成中的泛化性、鲁棒性和训练效率问题，并取得了SOTA表现。", "translation": "创建高质量、可泛化的语音驱动3D说话人头部仍然是一个持续的挑战。以前的方法在固定视角和小规模音频变化下取得了令人满意的结果，但它们在大头部旋转和域外（OOD）音频方面表现不佳。此外，它们受到耗时的身份特异性训练的限制。我们认为核心问题在于缺乏足够的3D先验，这限制了合成说话人头部的外推能力。为了解决这个问题，我们提出了GGTalker，它通过结合可泛化先验和身份特异性适应来合成说话人头部。我们引入了一种两阶段的“先验-适应”训练策略，以学习高斯头部先验并适应个体特征。我们训练“音频-表情”和“表情-视觉”先验来捕捉唇部运动的通用模式和头部纹理的通用分布。在“定制适应”阶段，精确建模个体说话风格和纹理细节。此外，我们引入了颜色MLP来生成精细、运动对齐的纹理，并引入了Body Inpainter将渲染结果与背景融合，生成难以区分的逼真视频帧。综合实验表明，GGTalker在渲染质量、3D一致性、唇部同步精度和训练效率方面达到了最先进的性能。", "summary": "GGTalker是一种新型的语音驱动3D说话人头部合成系统，旨在解决现有方法在处理大头部旋转、域外音频和耗时训练方面的局限性。它通过引入两阶段的“先验-适应”训练策略，结合了可泛化高斯头部先验（包括音频-表情和表情-视觉先验）和身份特异性适应。GGTalker还利用颜色MLP生成精细纹理，并使用Body Inpainter实现背景融合，从而合成高质量、逼真的视频帧。实验证明，GGTalker在渲染质量、3D一致性、唇部同步和训练效率方面均达到SOTA水平。", "keywords": "语音驱动3D说话人头部, 可泛化高斯先验, 身份特异性适应, GGTalker, 视频合成", "comments": "GGTalker的创新点在于其两阶段的“先验-适应”训练策略，有效结合了通用模式学习和个体化定制，显著提升了语音驱动3D说话人头部合成的泛化能力和效率。引入高斯先验来解决3D先验不足的问题，以及颜色MLP和Body Inpainter增强视觉真实感，是其重要贡献。该方法解决了现有技术在处理复杂头部运动和域外音频时的痛点，具有重要的应用潜力。"}}
{"id": "2506.21514", "title": "G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation", "authors": ["Mohammed Rakib", "Arunkumar Bagavathi"], "summary": "Multimodal learning aims to leverage information from diverse data modalities\nto achieve more comprehensive performance. However, conventional multimodal\nmodels often suffer from modality imbalance, where one or a few modalities\ndominate model optimization, leading to suboptimal feature representation and\nunderutilization of weak modalities. To address this challenge, we introduce\nGradient-Guided Distillation (G$^{2}$D), a knowledge distillation framework\nthat optimizes the multimodal model with a custom-built loss function that\nfuses both unimodal and multimodal objectives. G$^{2}$D further incorporates a\ndynamic sequential modality prioritization (SMP) technique in the learning\nprocess to ensure each modality leads the learning process, avoiding the\npitfall of stronger modalities overshadowing weaker ones. We validate G$^{2}$D\non multiple real-world datasets and show that G$^{2}$D amplifies the\nsignificance of weak modalities while training and outperforms state-of-the-art\nmethods in classification and regression tasks. Our code is available at\nhttps://github.com/rAIson-Lab/G2D.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21514v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21514v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "G$^{2}$D：通过梯度引导蒸馏提升多模态学习", "tldr": "G$^{2}$D是一种新的知识蒸馏框架，通过梯度引导和动态模态优先级解决多模态学习中的模态不平衡问题，提升弱模态的重要性并超越现有SOTA方法。", "motivation": "传统多模态模型常受模态不平衡问题困扰，即少数模态主导模型优化，导致特征表示不佳和弱模态未充分利用。", "method": "本文引入了梯度引导蒸馏（G$^{2}$D），这是一个知识蒸馏框架，通过自定义的损失函数优化多模态模型，该函数融合了单模态和多模态目标。G$^{2}$D还结合了动态序列模态优先级（SMP）技术，确保每种模态都能引导学习过程，避免强模态掩盖弱模态。", "result": "G$^{2}$D在多个真实世界数据集上进行了验证，结果表明G$^{2}$D在训练过程中增强了弱模态的重要性，并在分类和回归任务中超越了现有最先进的方法。", "conclusion": "G$^{2}$D通过解决模态不平衡问题，有效提升了多模态学习的性能，尤其增强了弱模态在训练中的作用。", "translation": "多模态学习旨在利用不同数据模态的信息，以实现更全面的性能。然而，传统的多模态模型常常受到模态不平衡的困扰，即一个或几个模态主导模型优化，导致次优的特征表示和对弱模态的利用不足。为了解决这一挑战，我们引入了梯度引导蒸馏（G$^{2}$D），这是一个知识蒸馏框架，它通过一个自定义的损失函数来优化多模态模型，该函数融合了单模态和多模态目标。G$^{2}$D在学习过程中进一步融入了动态序列模态优先级（SMP）技术，以确保每种模态都能引导学习过程，避免了强模态掩盖弱模态的弊端。我们在多个真实世界数据集上验证了G$^{2}$D，结果表明G$^{2}$D在训练时增强了弱模态的重要性，并在分类和回归任务中超越了现有最先进的方法。我们的代码可在https://github.com/rAIson-Lab/G2D获取。", "summary": "本文提出G$^{2}$D，一个梯度引导的知识蒸馏框架，旨在解决多模态学习中的模态不平衡问题。G$^{2}$D通过融合单模态和多模态目标的自定义损失函数以及动态序列模态优先级技术，确保弱模态在学习过程中得到充分利用和重视。实验证明，G$^{2}$D能有效提升弱模态的重要性，并在分类和回归任务中优于现有SOTA方法。", "keywords": "多模态学习, 梯度引导蒸馏, 模态不平衡, 知识蒸馏, 序列模态优先级", "comments": "G$^{2}$D的创新之处在于其结合了知识蒸馏、自定义损失函数以及动态模态优先级策略，以系统地解决多模态学习中的核心挑战——模态不平衡。特别是动态序列模态优先级（SMP）技术，通过确保每种模态都能引导学习过程，有效避免了强模态对弱模态的“遮蔽效应”，这对于提升多模态模型的鲁棒性和泛化能力至关重要。其在真实世界数据集上的SOTA表现证明了该方法的有效性和实用性。"}}
{"id": "2506.21520", "title": "MADrive: Memory-Augmented Driving Scene Modeling", "authors": ["Polina Karpikova", "Daniil Selikhanovych", "Kirill Struminsky", "Ruslan Musaev", "Maria Golitsyna", "Dmitry Baranchuk"], "summary": "Recent advances in scene reconstruction have pushed toward highly realistic\nmodeling of autonomous driving (AD) environments using 3D Gaussian splatting.\nHowever, the resulting reconstructions remain closely tied to the original\nobservations and struggle to support photorealistic synthesis of significantly\naltered or novel driving scenarios. This work introduces MADrive, a\nmemory-augmented reconstruction framework designed to extend the capabilities\nof existing scene reconstruction methods by replacing observed vehicles with\nvisually similar 3D assets retrieved from a large-scale external memory bank.\nSpecifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg}\ncar videos captured in the wild and present a retrieval module that finds the\nmost similar car instances in the memory bank, reconstructs the corresponding\n3D assets from video, and integrates them into the target scene through\norientation alignment and relighting. The resulting replacements provide\ncomplete multi-view representations of vehicles in the scene, enabling\nphotorealistic synthesis of substantially altered configurations, as\ndemonstrated in our experiments. Project page:\nhttps://yandex-research.github.io/madrive/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21520v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21520v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "MADrive：记忆增强的驾驶场景建模", "tldr": "MADrive引入了一个记忆增强框架，通过从大型记忆库中检索并替换车辆，解决了现有3D场景重建方法在生成显著改变或新颖驾驶场景时真实感合成能力不足的问题。", "motivation": "现有的场景重建方法，如使用3D高斯泼溅，虽然能实现高真实感建模，但其重建结果与原始观测紧密相关，难以支持对显著改变或新颖的驾驶场景进行真实感合成。", "method": "本文提出了MADrive，一个记忆增强的重建框架，旨在通过从一个大型外部记忆库中检索视觉上相似的3D资产来替换观察到的车辆，从而扩展现有场景重建方法的能力。具体地，该工作发布了MAD-Cars数据集（包含约7万个野外拍摄的360度汽车视频），并提出了一个检索模块，该模块能找到记忆库中最相似的汽车实例，从视频中重建相应的3D资产，并通过方向对齐和重新照明将其整合到目标场景中。", "result": "替换后的车辆提供了场景中完整的、多视角的表示，从而能够实现对大幅改变配置的车辆进行真实感合成，并在实验中得到了验证。", "conclusion": "MADrive通过引入记忆增强机制和外部3D资产，有效解决了现有场景重建方法在处理显著改变或新颖驾驶场景时真实感合成能力不足的问题，显著提升了自动驾驶环境建模的灵活性和真实感。", "translation": "近期场景重建的进展推动了使用3D高斯泼溅对自动驾驶（AD）环境进行高度真实的建模。然而，由此产生的重建结果仍然与原始观测紧密相关，难以支持对显著改变或新颖的驾驶场景进行真实感合成。这项工作引入了MADrive，一个记忆增强的重建框架，旨在通过从大型外部记忆库中检索视觉上相似的3D资产来替换观察到的车辆，从而扩展现有场景重建方法的能力。具体来说，我们发布了MAD-Cars，一个精选的包含约7万个野外拍摄的360度汽车视频的数据集，并提出了一个检索模块，该模块能找到记忆库中最相似的汽车实例，从视频中重建相应的3D资产，并通过方向对齐和重新照明将其整合到目标场景中。实验表明，由此产生的替换提供了场景中车辆完整的、多视角的表示，从而能够实现对大幅改变配置的真实感合成。项目页面：https://yandex-research.github.io/madrive/", "summary": "MADrive是一个记忆增强的重建框架，旨在通过从大型记忆库中检索并替换场景中的车辆，解决现有3D场景重建方法在合成显著改变或新颖驾驶场景时的局限性。该框架利用MAD-Cars数据集和专门的检索模块，实现车辆3D资产的重建与场景集成，从而实现对车辆配置进行真实感且灵活的合成。", "keywords": "记忆增强, 驾驶场景建模, 3D高斯泼溅, 车辆合成, MADrive", "comments": "MADrive的创新点在于引入了“记忆增强”的概念，通过外部大规模3D资产库来增强场景重建的灵活性和合成能力，而非仅仅依赖原始观测。这对于自动驾驶场景的模拟和数据增强具有重要意义，尤其是在生成多样化或罕见场景方面。其提出的MAD-Cars数据集和检索-重建-集成流程是实现这一目标的关键。"}}
{"id": "2506.21079", "title": "Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games", "authors": ["Yann Kerzreho"], "summary": "This paper introduces a new approach for approximating the learning dynamics\nof multiple reinforcement learning (RL) agents interacting in a finite-state\nMarkov game. The idea is to rescale the learning process by simultaneously\nreducing the learning rate and increasing the update frequency, effectively\ntreating the agent's parameters as a slow-evolving variable influenced by the\nfast-mixing game state. Under mild assumptions-ergodicity of the state process\nand continuity of the updates-we prove the convergence of this rescaled process\nto an ordinary differential equation (ODE). This ODE provides a tractable,\ndeterministic approximation of the agent's learning dynamics. An implementation\nof the framework is available at\\,:\nhttps://github.com/yannKerzreho/MarkovGameApproximation", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21079v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21079v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "有限状态马尔可夫博弈中多智能体学习动力学的均匀化", "tldr": "本文提出了一种通过重新缩放学习过程来近似有限状态马尔可夫博弈中多智能体学习动力学的新方法，并证明其收敛到一个可处理的常微分方程。", "motivation": "该研究旨在为在有限状态马尔可夫博弈中交互的多个强化学习（RL）智能体的学习动力学提供一种近似方法，以解决其复杂性。", "method": "通过同时降低学习率和增加更新频率来重新缩放学习过程，将智能体的参数视为受快速混合博弈状态影响的慢变量。在状态过程遍历性和更新连续性的温和假设下，证明了该重新缩放过程收敛到一个常微分方程（ODE）。", "result": "证明了重新缩放后的学习过程收敛到一个常微分方程（ODE）。", "conclusion": "该常微分方程提供了一种可处理的、确定性的智能体学习动力学近似。", "translation": "本文介绍了一种近似在有限状态马尔可夫博弈中交互的多个强化学习（RL）智能体学习动力学的新方法。其思想是通过同时降低学习率和增加更新频率来重新缩放学习过程，有效地将智能体的参数视为受快速混合博弈状态影响的慢演化变量。在温和假设（状态过程的遍历性和更新的连续性）下，我们证明了该重新缩放过程收敛到一个常微分方程（ODE）。这个ODE提供了一个可处理的、确定性的智能体学习动力学近似。该框架的实现可在以下网址获取：https://github.com/yannKerzreho/MarkovGameApproximation", "summary": "本文提出了一种新颖的方法，通过重新缩放学习过程来近似有限状态马尔可夫博弈中多智能体强化学习的动力学。该方法通过降低学习率和增加更新频率，将智能体参数视为慢变量，从而证明了重新缩放过程收敛到一个常微分方程（ODE）。这个ODE为智能体的学习动力学提供了一个可处理且确定性的近似。", "keywords": "多智能体学习, 马尔可夫博弈, 学习动力学, 均匀化, 常微分方程", "comments": "这项工作的创新之处在于引入了均匀化技术，将复杂的多智能体学习动力学简化为可分析的常微分方程。这提供了一种新颖且有前景的工具，用于理解和预测多智能体系统中的学习行为，尤其是在难以直接分析的场景下。其重要性在于提供了一个简化的模型，有助于理论分析和实际应用。"}}
{"id": "2506.21526", "title": "WAFT: Warping-Alone Field Transforms for Optical Flow", "authors": ["Yihan Wang", "Jia Deng"], "summary": "We introduce Warping-Alone Field Transforms (WAFT), a simple and effective\nmethod for optical flow. WAFT is similar to RAFT but replaces cost volume with\nhigh-resolution warping, achieving better accuracy with lower memory cost. This\ndesign challenges the conventional wisdom that constructing cost volumes is\nnecessary for strong performance. WAFT is a simple and flexible\nmeta-architecture with minimal inductive biases and reliance on custom designs.\nCompared with existing methods, WAFT ranks 1st on Spring and KITTI benchmarks,\nachieves the best zero-shot generalization on KITTI, while being up to 4.1x\nfaster than methods with similar performance. Code and model weights are\navailable at https://github.com/princeton-vl/WAFT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21526v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21526v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "WAFT：用于光流的独立形变场变换", "tldr": "WAFT是一种简单有效的光流方法，通过高分辨率形变取代成本体，实现了更高的精度和更低的内存成本，并且在多个基准测试中表现优异。", "motivation": "挑战了构建成本体对于光流方法强大性能的传统观念，旨在提供一种简单、灵活且性能优异的光流元架构。", "method": "WAFT是一种类似于RAFT的方法，但用高分辨率形变取代了成本体。它是一种具有最小归纳偏差和对定制设计依赖的元架构。", "result": "WAFT在Spring和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化能力，并且比性能相似的方法快4.1倍。", "conclusion": "WAFT证明了在光流领域，通过高分辨率形变取代成本体可以实现卓越的性能，挑战了传统方法，并提供了一种高效、灵活且准确的解决方案。", "translation": "我们引入了独立形变场变换（WAFT），这是一种简单有效的光流方法。WAFT类似于RAFT，但用高分辨率形变取代了成本体，以更低的内存成本实现了更好的精度。这一设计挑战了构建成本体对于强大性能而言是必要的传统观念。WAFT是一种简单灵活的元架构，具有最小的归纳偏差和对定制设计的依赖。与现有方法相比，WAFT在Spring和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化，同时比性能相似的方法快4.1倍。代码和模型权重可在https://github.com/princeton-vl/WAFT获取。", "summary": "WAFT是一种用于光流的简单高效方法，它通过高分辨率形变取代了传统成本体，从而在降低内存成本的同时提高了精度。该方法挑战了光流领域中成本体构建的必要性，并作为一种灵活的元架构，在Spring和KITTI等基准测试中表现出色，实现了顶级的性能和零样本泛化能力，同时速度远超同类方法。", "keywords": "光流, WAFT, 形变, 成本体, 深度学习", "comments": "WAFT的创新之处在于其挑战了光流领域中“成本体必要性”的传统认知，通过引入高分辨率形变实现了性能和效率的双重提升。其作为一种元架构，具有很好的灵活性和最小的归纳偏差，这对于未来的研究具有重要意义。该方法在多个主流基准测试中取得领先地位，并显著提升了推理速度，显示出其在实际应用中的巨大潜力。"}}
{"id": "2506.21541", "title": "StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning", "authors": ["Chuxin Wang", "Yixin Zha", "Wenfei Yang", "Tianzhu Zhang"], "summary": "Recently, Mamba-based methods have demonstrated impressive performance in\npoint cloud representation learning by leveraging State Space Model (SSM) with\nthe efficient context modeling ability and linear complexity. However, these\nmethods still face two key issues that limit the potential of SSM: Destroying\nthe adjacency of 3D points during SSM processing and failing to retain\nlong-sequence memory as the input length increases in downstream tasks. To\naddress these issues, we propose StruMamba3D, a novel paradigm for\nself-supervised point cloud representation learning. It enjoys several merits.\nFirst, we design spatial states and use them as proxies to preserve spatial\ndependencies among points. Second, we enhance the SSM with a state-wise update\nstrategy and incorporate a lightweight convolution to facilitate interactions\nbetween spatial states for efficient structure modeling. Third, our method\nreduces the sensitivity of pre-trained Mamba-based models to varying input\nlengths by introducing a sequence length-adaptive strategy. Experimental\nresults across four downstream tasks showcase the superior performance of our\nmethod. In addition, our method attains the SOTA 95.1% accuracy on ModelNet40\nand 92.75% accuracy on the most challenging split of ScanObjectNN without\nvoting strategy.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.21541v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21541v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "StruMamba3D：探索结构化Mamba用于自监督点云表示学习", "tldr": "StruMamba3D通过引入空间状态、状态更新策略和序列长度自适应策略，解决了Mamba模型在点云表示学习中破坏邻接性和长序列记忆问题，并取得了SOTA性能。", "motivation": "现有的基于Mamba的点云表示学习方法存在两个关键问题：1) 在SSM处理过程中破坏3D点的邻接性；2) 在下游任务中输入长度增加时未能保留长序列记忆。", "method": "本文提出StruMamba3D，一种用于自监督点云表示学习的新范式。它通过以下三点解决现有问题：1) 设计空间状态并将其用作代理以保留点之间的空间依赖性；2) 通过状态级更新策略和轻量级卷积增强SSM，促进空间状态间的交互以进行高效结构建模；3) 引入序列长度自适应策略，减少预训练Mamba模型对不同输入长度的敏感性。", "result": "在四项下游任务中表现出卓越性能。在ModelNet40上达到95.1%的SOTA准确率，在ScanObjectNN最具挑战性的分割上达到92.75%的准确率（无投票策略）。", "conclusion": "StruMamba3D有效解决了现有Mamba模型在点云表示学习中的局限性，并通过其创新设计显著提升了点云表示学习的性能，达到了SOTA水平。", "translation": "最近，基于Mamba的方法通过利用具有高效上下文建模能力和线性复杂度的状态空间模型（SSM），在点云表示学习中展现了令人印象深刻的性能。然而，这些方法仍然面临两个限制SSM潜力的关键问题：在SSM处理过程中破坏3D点的邻接性，以及在下游任务中输入长度增加时未能保留长序列记忆。为了解决这些问题，我们提出了StruMamba3D，一种用于自监督点云表示学习的新范式。它具有多项优点。首先，我们设计了空间状态并将其用作代理，以保留点之间的空间依赖性。其次，我们通过状态级更新策略增强了SSM，并结合了轻量级卷积以促进空间状态之间的交互，从而实现高效的结构建模。第三，我们的方法通过引入序列长度自适应策略，降低了预训练的基于Mamba的模型对不同输入长度的敏感性。在四项下游任务中的实验结果展示了我们方法的卓越性能。此外，我们的方法在ModelNet40上达到了95.1%的SOTA准确率，在ScanObjectNN最具挑战性的分割上达到了92.75%的准确率（无投票策略）。", "summary": "本文提出StruMamba3D，一种新颖的自监督点云表示学习范式，旨在解决现有Mamba模型在点云处理中破坏空间邻接性和长序列记忆保留的问题。StruMamba3D通过引入空间状态、增强SSM的状态级更新策略和轻量级卷积，以及序列长度自适应策略，有效保留了点云的空间依赖性并提高了模型的鲁棒性。实验证明，StruMamba3D在多项下游任务中表现卓越，并在ModelNet40和ScanObjectNN上取得了最先进的准确率。", "keywords": "点云表示学习, Mamba, 自监督学习, 状态空间模型, 3D视觉", "comments": "StruMamba3D创新性地将Mamba模型应用于点云表示学习，并针对Mamba在处理3D数据时固有的空间邻接性破坏和长序列记忆问题提出了有效的解决方案。通过引入空间状态和序列长度自适应策略，该方法显著提升了模型性能和鲁棒性，为自监督点云学习领域带来了重要进展。"}}
{"id": "2506.21544", "title": "DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion", "authors": ["Yansong Qu", "Shaohui Dai", "Xinyang Li", "Yuze Wang", "You Shen", "Liujuan Cao", "Rongrong Ji"], "summary": "Reconstructing 3D objects from a single image is a long-standing challenge,\nespecially under real-world occlusions. While recent diffusion-based view\nsynthesis models can generate consistent novel views from a single RGB image,\nthey generally assume fully visible inputs and fail when parts of the object\nare occluded. This leads to inconsistent views and degraded 3D reconstruction\nquality. To overcome this limitation, we propose an end-to-end framework for\nocclusion-aware multi-view generation. Our method directly synthesizes six\nstructurally consistent novel views from a single partially occluded image,\nenabling downstream 3D reconstruction without requiring prior inpainting or\nmanual annotations. We construct a self-supervised training pipeline using the\nPix2Gestalt dataset, leveraging occluded-unoccluded image pairs and\npseudo-ground-truth views to teach the model structure-aware completion and\nview consistency. Without modifying the original architecture, we fully\nfine-tune the view synthesis model to jointly learn completion and multi-view\ngeneration. Additionally, we introduce the first benchmark for occlusion-aware\nreconstruction, encompassing diverse occlusion levels, object categories, and\nmask patterns. This benchmark provides a standardized protocol for evaluating\nfuture methods under partial occlusions. Our code is available at\nhttps://github.com/Quyans/DeOcc123.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21544v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21544v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DeOcc-1-to-3: 从单张图像通过自监督多视图扩散进行3D去遮挡", "tldr": "提出DeOcc-1-to-3框架，利用自监督多视图扩散从单张遮挡图像生成一致的多视图，实现3D去遮挡和重建，并构建首个遮挡感知重建基准。", "motivation": "从单张图像重建3D物体是一个长期挑战，尤其是在真实世界遮挡下。现有扩散视图合成模型在物体部分遮挡时会生成不一致的视图，导致3D重建质量下降。", "method": "提出端到端遮挡感知多视图生成框架DeOcc-1-to-3，直接从单张部分遮挡图像合成六个结构一致的新视图。使用Pix2Gestalt数据集构建自监督训练流程，利用遮挡-非遮挡图像对和伪真值视图来学习结构感知补全和视图一致性。通过微调视图合成模型共同学习补全和多视图生成。", "result": "成功从单张部分遮挡图像生成六个结构一致的新视图，支持后续3D重建，无需预先修复或手动标注。引入了首个遮挡感知重建基准，涵盖不同遮挡水平、物体类别和掩模模式，为未来方法评估提供标准化协议。", "conclusion": "DeOcc-1-to-3框架通过自监督多视图扩散有效解决了单图像3D重建在遮挡下的挑战，并为该领域提供了重要的评估基准。", "translation": "从单张图像重建3D物体是一个长期存在的挑战，尤其是在真实世界的遮挡条件下。尽管最近基于扩散的视图合成模型可以从单张RGB图像生成一致的新视图，但它们通常假设输入是完全可见的，并且在物体部分被遮挡时会失效。这导致视图不一致和3D重建质量下降。为了克服这一限制，我们提出一个端到端的遮挡感知多视图生成框架。我们的方法直接从单张部分遮挡图像合成六个结构一致的新视图，从而实现下游3D重建，而无需预先修复或手动标注。我们使用Pix2Gestalt数据集构建了一个自监督训练流程，利用遮挡-非遮挡图像对和伪真值视图来教导模型结构感知补全和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习补全和多视图生成。此外，我们引入了第一个用于遮挡感知重建的基准，该基准涵盖了不同的遮挡水平、物体类别和掩模模式。这个基准为未来在部分遮挡下的方法评估提供了标准化协议。我们的代码可在https://github.com/Quyans/DeOcc123获取。", "summary": "本文提出了DeOcc-1-to-3，一个端到端的自监督多视图扩散框架，用于从单张部分遮挡图像实现3D去遮挡和重建。该方法通过微调视图合成模型，利用遮挡-非遮挡图像对和伪真值视图，学习生成结构一致的多视图，克服了现有模型在遮挡情况下的局限性。同时，论文还引入了首个遮挡感知重建基准，以标准化评估未来方法。", "keywords": "3D去遮挡, 单图像重建, 多视图扩散, 自监督学习, 遮挡感知重建", "comments": "该论文的创新点在于提出了一个无需预先修复或人工标注的端到端框架，直接从遮挡图像生成多视图，解决了单图像3D重建在遮挡下的难题。其自监督训练流程有效利用了现有数据。此外，首次引入遮挡感知重建基准对于推动该领域的研究具有重要意义。"}}
{"id": "2506.21549", "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark", "authors": ["Alex Costanzino", "Pierluigi Zama Ramirez", "Luigi Lella", "Matteo Ragaglia", "Alessandro Oliva", "Giuseppe Lisanti", "Luigi Di Stefano"], "summary": "We propose SiM3D, the first benchmark considering the integration of\nmultiview and multimodal information for comprehensive 3D anomaly detection and\nsegmentation (ADS), where the task is to produce a voxel-based Anomaly Volume.\nMoreover, SiM3D focuses on a scenario of high interest in manufacturing:\nsingle-instance anomaly detection, where only one object, either real or\nsynthetic, is available for training. In this respect, SiM3D stands out as the\nfirst ADS benchmark that addresses the challenge of generalising from synthetic\ntraining data to real test data. SiM3D includes a novel multimodal multiview\ndataset acquired using top-tier industrial sensors and robots. The dataset\nfeatures multiview high-resolution images (12 Mpx) and point clouds (7M points)\nfor 333 instances of eight types of objects, alongside a CAD model for each\ntype. We also provide manually annotated 3D segmentation GTs for anomalous test\nsamples. To establish reference baselines for the proposed multiview 3D ADS\ntask, we adapt prominent singleview methods and assess their performance using\nnovel metrics that operate on Anomaly Volumes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21549v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.21549v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "SiM3D：单实例多视角多模态多设置3D异常检测基准", "tldr": "SiM3D是一个新的3D异常检测基准，首次整合了多视角和多模态信息，专注于单实例训练，并解决了从合成数据泛化到真实数据的挑战。", "motivation": "现有3D异常检测缺乏整合多视角和多模态信息的基准，并且在工业制造场景中，特别是单实例训练和从合成数据泛化到真实数据的挑战尚未被充分解决。", "method": "提出了SiM3D基准，包含一个使用顶级工业传感器和机器人采集的新的多模态多视角数据集。该数据集包括高分辨率图像和点云，以及CAD模型和手动标注的3D分割真值。同时，适应了主流的单视角方法，并使用新的异常体指标进行性能评估，以建立基线。", "result": "提出了SiM3D基准，包含一个独特的数据集（333个实例，8种对象类型，高分辨率图像和点云，CAD模型，手动标注的3D分割GTs）。建立了基线，并引入了新的异常体指标。", "conclusion": "SiM3D是首个综合考虑多视角和多模态信息，并解决单实例训练和从合成到真实数据泛化挑战的3D异常检测与分割基准，为未来的研究提供了数据集、指标和基线。", "translation": "我们提出了SiM3D，这是第一个考虑整合多视角和多模态信息以进行全面的3D异常检测和分割（ADS）的基准，其任务是生成基于体素的异常体。此外，SiM3D专注于制造业中备受关注的场景：单实例异常检测，即只有单个对象（无论是真实的还是合成的）可用于训练。在这方面，SiM3D脱颖而出，成为第一个解决从合成训练数据泛化到真实测试数据挑战的ADS基准。SiM3D包含一个使用顶级工业传感器和机器人获取的新颖的多模态多视角数据集。该数据集包含333个八种类型对象实例的多视角高分辨率图像（12 Mpx）和点云（7M点），以及每种类型的CAD模型。我们还为异常测试样本提供了手动标注的3D分割真值（GTs）。为了为所提出的多视角3D ADS任务建立参考基线，我们调整了著名的单视角方法，并使用在异常体上操作的新指标评估了它们的性能。", "summary": "本文提出了SiM3D，一个针对3D异常检测与分割（ADS）的基准，首次整合了多视角和多模态信息。该基准特别关注制造业中的单实例异常检测场景，并解决了从合成训练数据泛化到真实测试数据的挑战。SiM3D包含一个新颖的多模态多视角数据集，该数据集利用工业级传感器和机器人采集，并提供高分辨率图像、点云、CAD模型以及手动标注的3D分割真值。此外，研究还通过调整现有单视角方法并引入新的异常体评估指标，为该任务建立了参考基线。", "keywords": "3D异常检测, 多视角, 多模态, 单实例, 基准", "comments": "SiM3D的创新之处在于其首次将多视角和多模态信息整合到3D异常检测基准中，并专注于解决工业应用中常见的单实例训练和合成到真实数据泛化的问题。其提供的高质量数据集和新的评估指标对推动3D异常检测领域的研究具有重要意义。"}}
{"id": "2506.21460", "title": "Wild refitting for black box prediction", "authors": ["Martin J. Wainwright"], "summary": "We describe and analyze a computionally efficient refitting procedure for\ncomputing high-probability upper bounds on the instance-wise mean-squared\nprediction error of penalized nonparametric estimates based on least-squares\nminimization. Requiring only a single dataset and black box access to the\nprediction method, it consists of three steps: computing suitable residuals,\nsymmetrizing and scaling them with a pre-factor $\\rho$, and using them to\ndefine and solve a modified prediction problem recentered at the current\nestimate. We refer to it as wild refitting, since it uses Rademacher residual\nsymmetrization as in a wild bootstrap variant. Under relatively mild conditions\nallowing for noise heterogeneity, we establish a high probability guarantee on\nits performance, showing that the wild refit with a suitably chosen wild noise\nscale $\\rho$ gives an upper bound on prediction error. This theoretical\nanalysis provides guidance into the design of such procedures, including how\nthe residuals should be formed, the amount of noise rescaling in the wild\nsub-problem needed for upper bounds, and the local stability properties of the\nblock-box procedure. We illustrate the applicability of this procedure to\nvarious problems, including non-rigid structure-from-motion recovery with\nstructured matrix penalties; plug-and-play image restoration with deep neural\nnetwork priors; and randomized sketching with kernel methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21460v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21460v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "黑盒预测的狂野再拟合", "tldr": "本文提出了一种计算高效的“狂野再拟合”程序，用于计算基于最小二乘法的惩罚非参数估计的实例均方预测误差的高概率上限，并提供了理论保证。", "motivation": "本文旨在描述和分析一种计算高效的再拟合程序，用于计算基于最小二乘法惩罚非参数估计的实例均方预测误差的高概率上限。该程序仅需一个数据集和对预测方法的黑盒访问。", "method": "该方法被称为“狂野再拟合”，因为它使用了Rademacher残差对称化，类似于一种野外自助法变体。它包括三个步骤：计算合适的残差，用预因子ρ对其进行对称化和缩放，以及使用它们定义和解决一个以当前估计为中心修改的预测问题。", "result": "在允许噪声异质性的相对温和条件下，该研究建立了其性能的高概率保证，表明选择合适野噪声尺度ρ的狂野再拟合可以给出预测误差的上限。", "conclusion": "该理论分析为设计此类程序提供了指导，包括残差应如何形成，上限所需的野子问题中的噪声重标量，以及黑盒过程的局部稳定性特性。该程序适用于非刚体结构恢复、即插即用图像恢复和随机草图与核方法等问题。", "translation": "我们描述并分析了一种计算高效的再拟合程序，用于计算基于最小二乘法惩罚非参数估计的实例均方预测误差的高概率上限。该程序仅需一个数据集和对预测方法的黑盒访问，它包含三个步骤：计算合适的残差，用预因子ρ对其进行对称化和缩放，以及使用它们定义和解决一个以当前估计为中心修改的预测问题。我们称之为狂野再拟合，因为它使用Rademacher残差对称化，类似于一种野外自助法变体。在允许噪声异质性的相对温和条件下，我们建立了其性能的高概率保证，表明选择合适野噪声尺度ρ的狂野再拟合可以给出预测误差的上限。这项理论分析为设计此类程序提供了指导，包括残差应如何形成，上限所需的野子问题中的噪声重标量，以及黑盒过程的局部稳定性特性。我们通过各种问题说明了该程序的适用性，包括使用结构化矩阵惩罚的非刚体结构恢复；使用深度神经网络先验的即插即用图像恢复；以及使用核方法的随机草图。", "summary": "本文提出了一种名为“狂野再拟合”的计算高效程序，旨在为基于最小二乘法的惩罚非参数估计的实例均方预测误差提供高概率上限。该方法仅依赖于单一数据集和对预测方法的黑盒访问，通过残差计算、对称化和缩放，以及解决修改后的预测问题来实现。研究在温和条件下建立了其性能的理论保证，证明了通过适当选择噪声尺度ρ，该方法能有效提供预测误差的上限。此外，文章还探讨了残差形成、噪声重标量和黑盒过程局部稳定性等设计考量，并展示了其在多种应用中的普适性。", "keywords": "狂野再拟合, 黑盒预测, 预测误差上限, 非参数估计, Rademacher对称化", "comments": "该论文提出了一种新颖且计算高效的“狂野再拟合”方法，用于量化黑盒预测模型的预测误差上限，这在实际应用中具有重要意义。其创新之处在于将Rademacher残差对称化应用于误差边界估计，并提供了严格的理论保证。该方法的“黑盒访问”特性使其适用于各种复杂的预测模型，包括深度学习模型，极大地扩展了其应用范围。理论分析为实际应用中的参数选择提供了清晰的指导。"}}
{"id": "2506.21511", "title": "Gaussian Invariant Markov Chain Monte Carlo", "authors": ["Michalis K. Titsias", "Angelos Alexopoulos", "Siran Liu", "Petros Dellaportas"], "summary": "We develop sampling methods, which consist of Gaussian invariant versions of\nrandom walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and\nsecond order Hessian or Manifold MALA. Unlike standard RWM and MALA we show\nthat Gaussian invariant sampling can lead to ergodic estimators with improved\nstatistical efficiency. This is due to a remarkable property of Gaussian\ninvariance that allows us to obtain exact analytical solutions to the Poisson\nequation for Gaussian targets. These solutions can be used to construct\nefficient and easy to use control variates for variance reduction of estimators\nunder any intractable target. We demonstrate the new samplers and estimators in\nseveral examples, including high dimensional targets in latent Gaussian models\nwhere we compare against several advanced methods and obtain state-of-the-art\nresults. We also provide theoretical results regarding geometric ergodicity,\nand an optimal scaling analysis that shows the dependence of the optimal\nacceptance rate on the Gaussianity of the target.", "comment": "29, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.21511v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.21511v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "高斯不变马尔可夫链蒙特卡洛", "tldr": "开发了高斯不变的MCMC采样方法，提高了统计效率，并能用于方差缩减，在实际应用中取得了最先进的结果。", "motivation": "现有标准随机游走Metropolis (RWM) 和Metropolis调整Langevin算法 (MALA) 等采样方法在统计效率上可能存在不足。本研究旨在开发能够改进统计效率的采样方法，并利用高斯不变性获得泊松方程的精确解析解，进而构建有效的方差缩减控制变量。", "method": "开发了高斯不变版本的随机游走Metropolis (RWM)、Metropolis调整Langevin算法 (MALA) 和二阶Hessian或流形MALA采样方法。利用高斯不变性，获得了高斯目标泊松方程的精确解析解。利用这些解析解构建了用于估计器方差缩减的控制变量。在多个例子中演示了新的采样器和估计器，包括潜在高斯模型中的高维目标，并与先进方法进行了比较。提供了几何遍历性的理论结果和最优尺度分析。", "result": "高斯不变采样方法能够产生具有改进统计效率的遍历估计器。高斯不变性使得能够获得高斯目标泊松方程的精确解析解。这些解析解可用于构建高效且易于使用的方差缩减控制变量。在潜在高斯模型等高维目标上，与现有先进方法相比，取得了最先进的结果。提供了关于几何遍历性的理论结果。最优尺度分析揭示了最优接受率对目标高斯性的依赖。", "conclusion": "高斯不变的MCMC采样方法通过利用高斯不变性带来的泊松方程解析解，显著提高了统计效率并实现了有效的方差缩减，在实践中表现出卓越的性能，并提供了坚实的理论支持。", "translation": "我们开发了采样方法，包括随机游走Metropolis (RWM)、Metropolis调整Langevin算法 (MALA) 和二阶Hessian或流形MALA的高斯不变版本。与标准RWM和MALA不同，我们表明高斯不变采样可以产生具有改进统计效率的遍历估计器。这归因于高斯不变性的一种显著特性，该特性使我们能够获得高斯目标泊松方程的精确解析解。这些解可用于构建高效且易于使用的控制变量，以减少任何难以处理的目标下的估计器方差。我们在几个示例中演示了新的采样器和估计器，包括潜在高斯模型中的高维目标，在这些示例中，我们与几种先进方法进行了比较，并获得了最先进的结果。我们还提供了关于几何遍历性的理论结果，以及一项最优尺度分析，该分析显示了最优接受率对目标高斯性的依赖。", "summary": "本文提出了一系列高斯不变的马尔可夫链蒙特卡洛 (MCMC) 采样方法，包括高斯不变的随机游走Metropolis和Langevin算法变体。研究表明，这些方法能够显著提高统计效率，因为高斯不变性允许对高斯目标下的泊松方程获得精确解析解。利用这些解析解，可以构建有效的控制变量，从而降低估计器的方差。在潜在高斯模型等高维目标上的实验证明，这些新方法优于现有先进技术，取得了最先进的性能。此外，论文还提供了关于几何遍历性的理论分析以及最优接受率与目标高斯性之间关系的最优尺度分析。", "keywords": "马尔可夫链蒙特卡洛, 高斯不变性, 统计效率, 方差缩减, 泊松方程", "comments": "这项研究通过引入“高斯不变性”这一新颖特性，对传统的MCMC采样方法进行了创新性改进。其重要性在于，它不仅提供了理论上的突破（泊松方程的解析解），更重要的是，这些理论成果能够直接转化为实际应用中高效的方差缩减技术，从而显著提升采样估计器的统计效率。在处理高维和复杂目标分布时，这种效率提升尤为关键，能够有效推动潜在高斯模型等领域的进展。"}}
{"id": "2506.21331", "title": "Automatic Reviewers Assignment to a Research Paper Based on Allied References and Publications Weight", "authors": ["Tamim Al Mahmud", "B M Mainul Hossain", "Dilshad Ara"], "summary": "Everyday, a vast stream of research documents is submitted to conferences,\nanthologies, journals, newsletters, annual reports, daily papers, and various\nperiodicals. Many such publications use independent external specialists to\nreview submissions. This process is called peer review, and the reviewers are\ncalled referees. However, it is not always possible to pick the best referee\nfor reviewing. Moreover, new research fields are emerging in every sector, and\nthe number of research papers is increasing dramatically. To review all these\npapers, every journal assigns a small team of referees who may not be experts\nin all areas. For example, a research paper in communication technology should\nbe reviewed by an expert from the same field. Thus, efficiently selecting the\nbest reviewer or referee for a research paper is a big challenge.\n  In this research, we propose and implement program that uses a new strategy\nto automatically select the best reviewers for a research paper. Every research\npaper contains references at the end, usually from the same area. First, we\ncollect the references and count authors who have at least one paper in the\nreferences. Then, we automatically browse the web to extract research topic\nkeywords. Next, we search for top researchers in the specific topic and count\ntheir h-index, i10-index, and citations for the first n authors. Afterward, we\nrank the top n authors based on a score and automatically browse their\nhomepages to retrieve email addresses. We also check their co-authors and\ncolleagues online and discard them from the list. The remaining top n authors,\ngenerally professors, are likely the best referees for reviewing the research\npaper.", "comment": "IEEE Conference Proceedings (5 Pages)", "pdf_url": "http://arxiv.org/pdf/2506.21331v1", "categories": ["cs.DL", "cs.CV"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.21331v1", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "基于相关参考文献和出版物权重的研究论文自动审稿人分配", "tldr": "本研究提出并实现了一种自动选择研究论文最佳审稿人的程序，该程序利用论文参考文献、作者的出版物权重（如H指数、i10指数、引用次数）以及在线信息来识别和推荐专家审稿人。", "motivation": "随着研究论文数量的急剧增加和新研究领域的不断涌现，为每篇论文高效且准确地选择最合适的专家审稿人成为一项重大挑战，因为现有审稿团队可能不具备所有领域的专业知识。", "method": "本研究提出并实现了一个自动审稿人选择程序。该程序首先收集论文参考文献，并识别参考文献中的作者。然后，自动从网络上提取研究主题关键词，并搜索该主题的顶尖研究人员。接着，计算这些研究人员的H指数、i10指数和引用次数，并基于得分对前n位作者进行排名。最后，自动浏览这些作者的主页获取电子邮件地址，并排除其共同作者和同事，从而推荐剩余的顶尖作者（通常是教授）作为最佳审稿人。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "每天，大量的研究文档被提交到会议、文集、期刊、通讯、年报、日报和各种期刊。许多此类出版物使用独立的外部专家来评审提交的稿件。这个过程被称为同行评审，审稿人被称为评审员。然而，并非总是能够为评审选择最佳评审员。此外，每个领域都在出现新的研究领域，研究论文的数量正在急剧增加。为了评审所有这些论文，每个期刊都会指派一个由少数评审员组成的小团队，他们可能不是所有领域的专家。例如，一篇关于通信技术的研究论文应该由来自同一领域的专家评审。因此，高效地为研究论文选择最佳审稿人或评审员是一个巨大的挑战。\n在这项研究中，我们提出并实现了一个程序，该程序使用一种新策略来自动选择研究论文的最佳审稿人。每篇研究论文的末尾都包含参考文献，通常来自同一领域。首先，我们收集参考文献并统计在参考文献中至少有一篇论文的作者。然后，我们自动浏览网络以提取研究主题关键词。接下来，我们搜索特定主题的顶尖研究人员，并统计前n位作者的H指数、i10指数和引用次数。之后，我们根据分数对前n位作者进行排名，并自动浏览他们的主页以检索电子邮件地址。我们还会在线检查他们的共同作者和同事，并将其从列表中删除。剩余的前n位作者，通常是教授，很可能是评审该研究论文的最佳评审员。", "summary": "本研究旨在解决为日益增长的研究论文量分配合适审稿人的挑战。文章提出并实现了一个自动化程序，该程序利用研究论文的参考文献来识别相关领域的专家。该方法包括提取参考文献作者、获取研究主题关键词、搜索顶尖研究人员并评估其学术影响力指标（如H指数、i10指数、引用次数），然后根据综合得分进行排名。最终，系统会排除共同作者和同事，推荐剩余的顶尖专家作为潜在的最佳审稿人。", "keywords": "审稿人分配, 同行评审, 参考文献分析, 学术指标, 自动化系统", "comments": "这项研究提出了一种新颖且实用的自动化审稿人分配方法，旨在解决同行评审过程中选择最佳评审员的效率和准确性问题。其创新点在于结合了论文参考文献、在线学术指标（H指数、i10指数、引用次数）和社交网络信息（共同作者、同事）来综合评估潜在审稿人的专业性和独立性。该方法有望显著提高审稿人分配的效率和质量，特别是在论文数量庞大且研究领域不断细分的背景下。然而，论文中没有提及实验结果或评估，因此其实际效果和鲁棒性有待进一步验证。"}}
